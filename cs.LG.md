# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [High Performance Computing Applied to Logistic Regression: A CPU and GPU Implementation Comparison.](http://arxiv.org/abs/2308.10037) | 我们提出了一种多功能的基于GPU的并行逻辑回归算法，相比CPU实现在执行时间上具有更好的性能，适用于实时预测应用。 |
| [^2] | [Semi-Supervised Anomaly Detection for the Determination of Vehicle Hijacking Tweets.](http://arxiv.org/abs/2308.10036) | 本研究提出了一种新的半监督方法，使用推文来识别车辆劫持事件。通过应用无监督异常检测算法和比较评估，发现CBLOF方法在准确率和F1-Score等指标上稍微优于KNN方法，因此被选为首选方法。 |
| [^3] | [Effects of Convolutional Autoencoder Bottleneck Width on StarGAN-based Singing Technique Conversion.](http://arxiv.org/abs/2308.10021) | 本文研究了卷积自编码器瓶颈宽度对基于StarGAN的唱歌技巧转换合成质量的影响，并通过主观评估结果发现，更宽的瓶颈可以提高发音清晰度，但不一定能够增加与目标声音的相似度。 |
| [^4] | [Semi-Implicit Variational Inference via Score Matching.](http://arxiv.org/abs/2308.10014) | 本文提出了一种基于得分匹配的半隐式变分推断方法SIVI-SM，该方法利用了半隐式变分家族的层次结构，并通过处理不可计算的变分密度来实现与MCMC相当的准确性，在各种贝叶斯推断任务中优于基于ELBO的SIVI方法。 |
| [^5] | [Disposable Transfer Learning for Selective Source Task Unlearning.](http://arxiv.org/abs/2308.09971) | 本论文提出了一种称为可拆卸式迁移学习的新范式，通过引入梯度碰撞损失，该方法可以选择性地遗忘源任务而不降低目标任务的性能。 |
| [^6] | [Tackling Vision Language Tasks Through Learning Inner Monologues.](http://arxiv.org/abs/2308.09970) | 通过学习内心独白，提出了一种新方法（IMMO）来解决复杂的视觉语言任务，克服了混合融合和特征对齐方法所面临的优化和可解释性问题。 |
| [^7] | [Anomaly-Aware Semantic Segmentation via Style-Aligned OoD Augmentation.](http://arxiv.org/abs/2308.09965) | 在自动驾驶中遇到未知对象是不可避免的，为了解决这个问题，我们提出了一种基于风格对齐的畸变感知语义分割方法。通过减小OoD数据和驾驶场景之间的域差异，我们改进了OoD合成过程，并利用预训练模型生成“给定类别之外”的预测结果进行异常分割，从而提高了性能。 |
| [^8] | [Towards Self-Adaptive Machine Learning-Enabled Systems Through QoS-Aware Model Switching.](http://arxiv.org/abs/2308.09960) | 通过使用多个模型管理ML模型相关的不确定性，我们提出了一种自适应机器学习系统，该系统利用自适应技术和动态模型切换以提高整体服务质量（QoS）。 |
| [^9] | [A Comparison of Adversarial Learning Techniques for Malware Detection.](http://arxiv.org/abs/2308.09958) | 本文比较了针对恶意软件检测的对抗学习技术，通过生成对抗性恶意软件样本，并测试其对杀毒产品的检测性能。结果显示，对先前检测到的恶意软件应用优化修改可能导致错误分类，同时生成的恶意软件样本可成功用于其他检测模型。 |
| [^10] | [To prune or not to prune : A chaos-causality approach to principled pruning of dense neural networks.](http://arxiv.org/abs/2308.09955) | 这个论文提出了一种基于混沌因果性的稠密神经网络有原则修剪方法，通过选择特定权重来最小化错分类，修剪后的网络保持了原始性能和特征的可解释性。 |
| [^11] | [Finding emergence in data: causal emergence inspired dynamics learning.](http://arxiv.org/abs/2308.09952) | 本文引入了一种基于因果出现理论的机器学习框架，能够在数据中学习宏观动力学和量化出现程度。实验证明该框架能够成功捕捉出现模式，并学习粗粒化策略，具有广泛的适用性。 |
| [^12] | [Study on the effectiveness of AutoML in detecting cardiovascular disease.](http://arxiv.org/abs/2308.09947) | 本文研究了自动机器学习在检测心血管疾病中的有效性，提出了一个自动机器学习应用框架，并使用合并的数据集来进行研究和优化。 |
| [^13] | [Dual Branch Deep Learning Network for Detection and Stage Grading of Diabetic Retinopathy.](http://arxiv.org/abs/2308.09945) | 这项研究介绍了一种双支路深度学习网络用于检测和分级糖尿病视网膜病变，通过利用单个眼底视网膜图像进行早期诊断和成功治疗。所提出的模型利用迁移学习和预训练模型，在大型多中心数据集上进行了训练，取得了卓越的性能，优于已有文献。 |
| [^14] | [On the Robustness of Open-World Test-Time Training: Self-Training with Dynamic Prototype Expansion.](http://arxiv.org/abs/2308.09942) | 本研究提出了一种改进开放世界测试时间训练的方法，包括自适应强OOD修剪、动态原型扩展和分布对齐等技术。这些方法提高了模型在目标领域受到强OOD数据污染时的稳健性。 |
| [^15] | [Practical Anomaly Detection over Multivariate Monitoring Metrics for Online Services.](http://arxiv.org/abs/2308.09937) | 本论文提出了一个基于协同机制的多变量监测指标异常检测框架，能够高效捕捉不同指标之间的依赖关系及其历史模式，实现了在线服务的实用异常检测。 |
| [^16] | [BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions.](http://arxiv.org/abs/2308.09936) | BLIVA是一个简单的多模态LLM模型，用于更好地处理嵌入了文本的图像上下文场景。它通过引入InstructBLIP的查询嵌入和LLaVA的编码补丁嵌入技术来改进视觉语言模型，从而有效解决了文本丰富的视觉问题。 |
| [^17] | [East: Efficient and Accurate Secure Transformer Framework for Inference.](http://arxiv.org/abs/2308.09923) | 我们提出了一个名为East的框架，以实现高效准确的安全Transformer推理。我们通过设计新的忘却分段多项式求值算法来优化激活函数的运行时间和通信量。此外，我们还设计了安全协议来处理softmax和层归一化。 |
| [^18] | [Never Explore Repeatedly in Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2308.09909) | 该论文提出了一种动态奖励缩放方法，以解决多智能体强化学习中的回访问题。实验结果表明，在复杂环境中，特别是在稀疏奖励设置下，该方法能够提高性能。 |
| [^19] | [Imputing Brain Measurements Across Data Sets via Graph Neural Networks.](http://arxiv.org/abs/2308.09907) | 本研究提出了一种通过在另一个包含缺失测量值的数据集上进行预测任务，使用图神经网络填补跨数据集脑部测量值的方法。 |
| [^20] | [DPMAC: Differentially Private Communication for Cooperative Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2308.09902) | DPMAC算法在多代理强化学习中应用了差分隐私通信机制，通过为每个代理配备本地消息发送器并根据差分隐私要求调整消息分布，保护了个体代理的隐私信息。 |
| [^21] | [Contrastive Learning-based Imputation-Prediction Networks for In-hospital Mortality Risk Modeling using EHRs.](http://arxiv.org/abs/2308.09896) | 本文提出了一种基于对比学习的基于电子病历的填补预测网络，用于预测住院死亡风险。新方法通过引入基于图分析的患者分层建模，只使用相似患者的信息，填补缺失值和预测死亡风险。 |
| [^22] | [Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs.](http://arxiv.org/abs/2308.09895) | 本文介绍了一种有效的方法，通过使用半合成数据来提升代码LLMs在低资源语言上的性能。方法名为MultiPL-T，通过将高资源语言的训练数据转化为低资源语言的训练数据，生成高质量的低资源语言数据集。 |
| [^23] | [Utilizing Semantic Textual Similarity for Clinical Survey Data Feature Selection.](http://arxiv.org/abs/2308.09892) | 利用语义文本相似性对临床调查数据进行特征选择，通过评估特征名称与目标名称之间的关系来选择特征。 |
| [^24] | [Inductive-bias Learning: Generating Code Models with Large Language Model.](http://arxiv.org/abs/2308.09890) | 这篇论文提出了一种新的学习方法，称为归纳偏差学习（IBL），它将上下文学习（ICL）和代码生成相结合，通过输入训练数据到提示中，输出相应的代码。 |
| [^25] | [DUAW: Data-free Universal Adversarial Watermark against Stable Diffusion Customization.](http://arxiv.org/abs/2308.09889) | 这项研究提出了一种无数据通用对抗水印（DUAW），通过干扰定制过程中的变分自动编码器，保护各种版本的SD模型中的众多受版权保护的图像。这种方法避免了直接处理受版权保护的图像的必要性，保护图像的保密性。 |
| [^26] | [On Estimating the Gradient of the Expected Information Gain in Bayesian Experimental Design.](http://arxiv.org/abs/2308.09888) | 该论文提出了一种估计贝叶斯实验设计中期望信息增益梯度的方法，通过结合随机梯度下降算法，实现了高效优化。具体而言，通过后验期望表示来估计与设计变量相关的梯度，并提出了UEEG-MCMC和BEEG-AP两种估计方法。这些方法在不同的实验设计问题上都表现出良好的性能。 |
| [^27] | [Calibrating Uncertainty for Semi-Supervised Crowd Counting.](http://arxiv.org/abs/2308.09887) | 本文提出了一种新颖的方法来校准人群计数模型的不确定性，通过有监督的不确定性估计策略和匹配基准的替代函数，实现了在半监督人群计数中可靠的不确定性估计和高质量的伪标签生成，并取得了最先进的性能。 |
| [^28] | [A Transformer-based Framework For Multi-variate Time Series: A Remaining Useful Life Prediction Use Case.](http://arxiv.org/abs/2308.09884) | 这项研究提出了一种基于Transformer的框架，用于多元时间序列预测，以满足预测学应用案例中剩余寿命预测的需求。 |
| [^29] | [Flamingo: Multi-Round Single-Server Secure Aggregation with Applications to Private Federated Learning.](http://arxiv.org/abs/2308.09883) | Flamingo是一个用于实现跨大量客户端安全聚合的系统，在私有联邦学习中有广泛的应用。通过消除每轮设置和引入轻量级的丢失容忍协议，Flamingo解决了以往协议在多轮设置下的问题，并引入了新的本地选择客户端邻域的方式。 |
| [^30] | [Generative Adversarial Networks Unlearning.](http://arxiv.org/abs/2308.09881) | 本文介绍了一种生成对抗网络的反学习方法，通过引入替代机制和定义虚假标签来解决生成器反学习带来的连续性和完整性问题。 |
| [^31] | [DatasetEquity: Are All Samples Created Equal? In The Quest For Equity Within Datasets.](http://arxiv.org/abs/2308.09878) | 该论文提出了一种新的方法来解决机器学习中数据不平衡的问题，通过使用深层感知嵌入和聚类来计算样本的似然性，并使用广义聚焦损失函数在训练过程中对样本进行不同的加权。实验验证了该方法的有效性。 |
| [^32] | [Skill Transformer: A Monolithic Policy for Mobile Manipulation.](http://arxiv.org/abs/2308.09873) | Skill Transformer是一个解决长时间跨度机器人任务的方法，通过结合条件序列建模和技能模块化，它能够在新场景中实现强大的任务规划和低级控制能力，相比基准模型，成功率提高了2.5倍。 |
| [^33] | [Tensor-Compressed Back-Propagation-Free Training for (Physics-Informed) Neural Networks.](http://arxiv.org/abs/2308.09858) | 本文提出了一个完全无需反向传播的神经网络训练框架，并通过张量压缩的方差约减方法和混合梯度评估方法改进了优化和效率。同时，还扩展了框架用于物理信息的神经网络的估计。 |
| [^34] | [Backdoor Mitigation by Correcting the Distribution of Neural Activations.](http://arxiv.org/abs/2308.09850) | 本文揭示了后门攻击的一个重要特性：成功的攻击会改变内部层激活的分布，我们提出了一种通过纠正分布改变实现训练后的后门减缓的方法。 |
| [^35] | [Enumerating Safe Regions in Deep Neural Networks with Provable Probabilistic Guarantees.](http://arxiv.org/abs/2308.09842) | 通过epsilon-ProVe方法，我们提出了一种高效近似的方法来枚举深度神经网络中的安全区域，并提供了可证明概率保证的紧密下估计。 |
| [^36] | [Microscopy Image Segmentation via Point and Shape Regularized Data Synthesis.](http://arxiv.org/abs/2308.09835) | 本文提出了一种采用合成训练数据的统一流程，通过点注释和形状先验进行显微图像分割。该方法克服了标注成本高的问题，且仍能提供关键信息用于分割。该流程包括三个阶段：获取点注释并生成伪密集分割掩码，将伪掩码转化为真实显微图像，并通过对象级一致性进行正则化。 |
| [^37] | [Learning from A Single Graph is All You Need for Near-Shortest Path Routing in Wireless Networks.](http://arxiv.org/abs/2308.09829) | 本文提出了一种学习算法，通过训练DNNs学习本地路由策略，只需从单个图形获得少量数据样本，即可推广到所有随机图形的无线网络。实验表明，我们的算法在性能上能够与贪婪转发相匹配或超越，并且通过利用网络领域知识，选择适当的输入特征和图形子采样，实现了高效、可扩展和可推广的学习。 |
| [^38] | [VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity Control.](http://arxiv.org/abs/2308.09804) | 本文提出了一种名为VL-PET的视觉和语言参数高效调整框架，通过粒度控制机制对模块化修改进行有效控制，克服了现有技术在性能和功能差距方面的不足。 |
| [^39] | [An Efficient High-Dimensional Gene Selection Approach based on Binary Horse Herd Optimization Algorithm for Biological Data Classification.](http://arxiv.org/abs/2308.09791) | 本文提出了一种基于二进制Horse Herd优化算法的高维基因选择方法，通过引入新的混合特征选择框架和转换函数，能够更高效地选择相关且信息丰富的特征子集，并在微阵列数据集上取得了良好的效果。 |
| [^40] | [A Two-Part Machine Learning Approach to Characterizing Network Interference in A/B Testing.](http://arxiv.org/abs/2308.09790) | 本论文提出了一种两部分机器学习方法，用于识别和描述A/B测试中的网络干扰。通过考虑潜在的复杂网络结构和建立适合的曝光映射，该方法在合成实验和真实大规模测试中的模拟中表现优于传统方法。 |
| [^41] | [Event-based Dynamic Graph Representation Learning for Patent Application Trend Prediction.](http://arxiv.org/abs/2308.09780) | 本研究提出了一种基于事件的动态图学习框架，用于准确预测专利申请趋势。该方法利用公司和分类代码的可记忆表示，通过历史记忆和当前信息更新来捕捉语义接近性。 |
| [^42] | [Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models.](http://arxiv.org/abs/2308.09778) | 本文旨在研究多模态视觉语言模型在理解空间关系方面的能力，提出了细粒度组合的空间关系基础，并采用自底向上的方法评估空间关系推理任务的性能。 |
| [^43] | [Time Series Predictions in Unmonitored Sites: A Survey of Machine Learning Techniques in Water Resources.](http://arxiv.org/abs/2308.09766) | 本论文综述了机器学习在未监测站点的水资源预测中的应用，包括河流流量、水质等相关变量，并讨论了融合集水区特征的新方法，提升机器学习的使用。 |
| [^44] | [Taken by Surprise: Contrast effect for Similarity Scores.](http://arxiv.org/abs/2308.09765) | 提出了一种新的相似度度量方法，称为“惊喜分数”，该方法能够考虑对象的上下文信息并显著提高零样本和少样本文档分类任务的性能。 |
| [^45] | [The Impact of Background Removal on Performance of Neural Networks for Fashion Image Classification and Segmentation.](http://arxiv.org/abs/2308.09764) | 本研究通过去除时尚图像的背景，提高了数据质量和模型性能，在多个方面进行了广泛的比较实验，结果表明背景去除对于模型训练有积极的影响。 |
| [^46] | [Data Compression and Inference in Cosmology with Self-Supervised Machine Learning.](http://arxiv.org/abs/2308.09751) | 本研究利用自我监督机器学习的方法，通过模拟增广构建了宇宙学数据的代表性汇总，可以有效压缩数据并用于精确参数推断，为宇宙学数据的压缩和分析提供了一种有前途的新途径。 |
| [^47] | [Causal Interpretable Progression Trajectory Analysis of Chronic Disease.](http://arxiv.org/abs/2308.09735) | 提出了一种名为因果轨迹预测（CTP）的新模型，通过将轨迹预测和因果发现相结合，准确预测慢性疾病的进展轨迹并揭示特征间的因果关系。 |
| [^48] | [A Robust Policy Bootstrapping Algorithm for Multi-objective Reinforcement Learning in Non-stationary Environments.](http://arxiv.org/abs/2308.09734) | 提出了一种适用于非静态环境中多目标强化学习的稳健策略引导算法，能够在线演化一个凸覆盖策略集，同时满足目标偏好空间的探索需求。 |
| [^49] | [Intrinsically Motivated Hierarchical Policy Learning in Multi-objective Markov Decision Processes.](http://arxiv.org/abs/2308.09733) | 该论文提出了一种解决多目标马尔可夫决策过程中的问题的方法，通过学习通用的技能集，使得策略覆盖集能够在非稳态环境中持续演化，从而提高性能。 |
| [^50] | [Baird Counterexample Is Solved: with an example of How to Debug a Two-time-scale Algorithm.](http://arxiv.org/abs/2308.09732) | 这篇论文解决了Baird反例上的收敛问题，通过一种具有收敛保证的算法，实现了线性收敛速度。 |
| [^51] | [ChatGPT-HealthPrompt. Harnessing the Power of XAI in Prompt-Based Healthcare Decision Support using ChatGPT.](http://arxiv.org/abs/2308.09731) | 本研究介绍了一种创新的方法，利用ChatGPT在临床决策中应用大型语言模型，通过策略性设计上下文提示，并以领域知识为基础进行高质量的二元分类任务。通过将机器学习模型视为医疗专家，提取关键见解并辅助决策过程，这一领域知识和人工智能的结合在创建更具洞察力的诊断工具方面具有重要潜力。此外，研究还探索了基于ChatGPT的零样本和少样本提示学习的动态，并验证了ChatGPT在医疗决策支持中的优势。 |
| [^52] | [Data diversity and virtual imaging in AI-based diagnosis: A case study based on COVID-19.](http://arxiv.org/abs/2308.09730) | 本研究通过使用多样性的临床和虚拟生成的医学图像开发和评估了COVID-19诊断的AI模型，发现数据集特征对于AI性能具有重要影响，容易导致泛化能力较差，最高下降20％。 |
| [^53] | [MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models.](http://arxiv.org/abs/2308.09729) | 本论文通过使用知识图谱来激发大型语言模型，解决了整合新知识、产生幻觉和决策过程不透明等问题，并通过生成思维导图展示了模型的推理路径，实验证明这种方法可以取得显著的实证增益。 |
| [^54] | [Learning representations by forward-propagating errors.](http://arxiv.org/abs/2308.09728) | 本文提出了一种基于前向传播和代数几何中双数概念的快速学习算法，可以在CPU上实现与CUDA加速相媲美的性能。 |
| [^55] | [Cross-city Few-Shot Traffic Forecasting via Traffic Pattern Bank.](http://arxiv.org/abs/2308.09727) | 本文提出了一种通过交通模式存储库来进行跨城市少样本交通预测的框架。通过将来自数据丰富城市的原始交通数据投影到高维空间并生成交通模式存储库，可以改善数据稀缺城市的交通预测性能。 |
| [^56] | [Equitable Restless Multi-Armed Bandits: A General Framework Inspired By Digital Health.](http://arxiv.org/abs/2308.09726) | 这项研究提出了不躁动多臂赌博机的公平目标，并开发了相应的算法，证明在数字健康等领域中可以提高数倍的公平性。 |
| [^57] | [MoCLIM: Towards Accurate Cancer Subtyping via Multi-Omics Contrastive Learning with Omics-Inference Modeling.](http://arxiv.org/abs/2308.09725) | 本论文介绍了一种名为MoCLIM的多组学对比学习框架，能够在癌症亚型划分中利用多组学数据的潜力，显著提高了数据的拟合度和亚型划分性能。 |
| [^58] | [Knowledge-inspired Subdomain Adaptation for Cross-Domain Knowledge Transfer.](http://arxiv.org/abs/2308.09724) | 知识启发的子领域适应（KISA）框架在交叉领域知识传递中提供了细粒度的领域适应能力。 |
| [^59] | [FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization for LLMs.](http://arxiv.org/abs/2308.09723) | 该论文提出了一种针对LLMs的高效权重量化方法，通过减少内存消耗和加速推理，解决了大型语言模型在实际部署中遇到的挑战。研究者们通过引入一种简单而有效的启发式方法，仅利用模型的权重，而无需额外微调，成功降低了质量损失。 |
| [^60] | [A Trustable LSTM-Autoencoder Network for Cyberbullying Detection on Social Media Using Synthetic Data.](http://arxiv.org/abs/2308.09722) | 本研究提出了一种使用合成数据进行社交媒体网络欺凌检测的可信LSTM-Autoencoder网络。该方法解决了数据可用性困难的问题，并通过实验证明了在印地语、孟加拉语和英语数据集上的有效性。 |
| [^61] | [A new solution and concrete implementation steps for Artificial General Intelligence.](http://arxiv.org/abs/2308.09721) | 本研究提出了在人工通用智能领域中解决现有技术缺陷的新方案，并分析了大型模型技术路线的局限性。 |
| [^62] | [On the Unexpected Abilities of Large Language Models.](http://arxiv.org/abs/2308.09720) | 大型语言模型展示了与其训练任务不直接相关的广泛能力，通过间接获取过程使其拥有综合能力的发展，这些能力在一定程度上可预测，并且与人类认知有关。 |
| [^63] | [Graph of Thoughts: Solving Elaborate Problems with Large Language Models.](http://arxiv.org/abs/2308.09687) | 想法图（GoT）是一种新的框架，它超越了现有的提示范式，通过将大型语言模型（LLM）的信息建模为任意图形，将LLM想法组合成具有协同效应的结果，提炼整个思维网络的本质，或者使用反馈环路增强思维。GoT在不同任务上展示出优势，并可以通过新的想法转换进行扩展，使LLM的推理更接近人类思维。 |
| [^64] | [Generalized Sum Pooling for Metric Learning.](http://arxiv.org/abs/2308.09228) | 本论文提出了一种泛化求和池化方法（GSP）用于深度度量学习。GSP通过选择语义实体的子集，学习忽略无用信息，并学习每个实体的重要性权重，从而改进了全局平均池化（GAP）方法。 |
| [^65] | [Reinforced Self-Training (ReST) for Language Modeling.](http://arxiv.org/abs/2308.08998) | 本文提出了一种称为自学习增强 (ReST) 的算法，通过从人类反馈中进行强化学习来提高大型语言模型 (LLM) 的输出质量。在机器翻译任务上的实验结果表明，ReST能够以高效的方式显著提高翻译质量。 |
| [^66] | [Consciousness in Artificial Intelligence: Insights from the Science of Consciousness.](http://arxiv.org/abs/2308.08708) | 本论文提出了一种严谨的方法，通过对当前的人工智能系统进行详细评估来探讨人工智能的意识问题。研究中对几种科学意识理论进行概述，并通过计算方法推导出意识的“指示性特征”。分析结果表明目前的人工智能系统尚不具备意识，但建立具有意识的人工智能系统并无明显的障碍。 |
| [^67] | [Story Visualization by Online Text Augmentation with Context Memory.](http://arxiv.org/abs/2308.07575) | 本论文提出了一种通过在线文本增强和上下文记忆来进行故事可视化的方法。通过使用新颖的记忆架构和多个伪描述作为训练过程的补充监督，该方法在两个故事可视化基准测试中取得了显著优于现有方法的结果。 |
| [^68] | [AudioFormer: Audio Transformer learns audio feature representations from discrete acoustic codes.](http://arxiv.org/abs/2308.07221) | AudioFormer是一种学习音频特征表示的方法，通过生成离散的声学代码并利用它们来训练掩码语言模型，从而将音频分类任务视为自然语言理解的形式。此外，引入了多正样本对比学习方法，通过学习联合表示来捕捉音频中的相关性。 |
| [^69] | [Natural Language is All a Graph Needs.](http://arxiv.org/abs/2308.07134) | 本论文提出了一种名为InstructGLM的结构化语言模型算法，该算法将大型语言模型与图表学习问题相结合，旨在探索是否可以用语言模型取代图神经网络作为图表的基础模型。 |
| [^70] | [Detecting and Preventing Hallucinations in Large Vision Language Models.](http://arxiv.org/abs/2308.06394) | 本论文提出了一个用于训练和评估模型的多模态幻觉检测数据集，以解决大型视觉语言模型中存在的幻觉文本问题。这是第一个用于详细图像描述的全面多模态幻觉检测数据集。 |
| [^71] | [Classification of Blood Cells Using Deep Learning Models.](http://arxiv.org/abs/2308.06300) | 这项研究使用深度学习模型通过图像分类对人类血细胞进行了分类和识别，为诊断疾病提供了重要的帮助。 |
| [^72] | [A Review on Classification of White Blood Cells Using Machine Learning Models.](http://arxiv.org/abs/2308.06296) | 本综述系统分析了在医学图像分析领域中应用的机器学习技术，为白细胞分类提供了有价值的洞察力和最佳方法。 |
| [^73] | [Adaptive SGD with Polyak stepsize and Line-search: Robust Convergence and Variance Reduction.](http://arxiv.org/abs/2308.06058) | 本文提出了AdaSPS和AdaSLS两种新的变种算法，用于解决SGD在非插值环境下的收敛问题，并在训练超参数模型时保持线性和亚线性的收敛速度。 |
| [^74] | [AdaER: An Adaptive Experience Replay Approach for Continual Lifelong Learning.](http://arxiv.org/abs/2308.03810) | AdaER是一种自适应经验重播方法，用于解决连续终身学习中的灾难性遗忘问题。它采用上下文提示的记忆回忆策略，选择性地重播最冲突的记忆。 |
| [^75] | [G-Mix: A Generalized Mixup Learning Framework Towards Flat Minima.](http://arxiv.org/abs/2308.03236) | G-Mix是一种通用的混合学习框架，通过结合Mixup和锐度感知极小化（SAM）方法的优点，来增强深度神经网络（DNN）的泛化能力。本文还介绍了两种新的算法：二进制G-Mix和分解G-Mix，用于进一步优化DNN性能。 |
| [^76] | [Source-free Domain Adaptive Human Pose Estimation.](http://arxiv.org/abs/2308.03202) | 提出了无源域自适应的人体姿势估计任务，旨在解决在适应过程中无法访问源数据的跨域学习挑战。通过提出的新框架，源保护模块更有效地保留源信息并抵抗噪声。 |
| [^77] | [An Empirical Study of AI-based Smart Contract Creation.](http://arxiv.org/abs/2308.02955) | 本研究通过评估LLMs生成的智能合约代码的质量，发现了安全漏洞的重要证据，并评估了输入参数对LLMs的影响。 |
| [^78] | [NeRFs: The Search for the Best 3D Representation.](http://arxiv.org/abs/2308.02751) | NeRFs是视图合成和相关问题中寻找最佳3D表示的结果，该方法利用神经网络查询获取体积参数来描述连续体积场景。 |
| [^79] | [Adaptive Preferential Attached kNN Graph With Distribution-Awareness.](http://arxiv.org/abs/2308.02442) | 本文提出了一种名为paNNG的算法，它结合了自适应kNN和基于分布的图构建。通过包含分布信息，paNNG能够有效提升模糊样本的性能，并实现更好的准确性和泛化能力。 |
| [^80] | [Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology.](http://arxiv.org/abs/2308.02180) | 本文研究了使用大型语言模型（LLMs）扩展临床试验匹配的方法，并以肿瘤学为案例研究。研究结果显示，先进的LLMs能够处理临床试验的复杂条件和匹配逻辑，相较于之前的方法，性能显著提升，并可作为人工辅助筛选患者-试验候选人的初步解决方案。 |
| [^81] | [FLARE: Fingerprinting Deep Reinforcement Learning Agents using Universal Adversarial Masks.](http://arxiv.org/abs/2307.14751) | FLARE是第一个用于验证疑似深度强化学习策略是否是另一个策略的非法副本的指纹机制，通过使用通用对抗性掩码作为指纹，并测量动作一致性值来验证被盗策略的真实所有权。 |
| [^82] | [An Empirical Evaluation of Temporal Graph Benchmark.](http://arxiv.org/abs/2307.12510) | 本文对Temporal Graph Benchmark进行了实证评估，通过扩展动态图库(DyGLib)到TGB，并使用十一种动态图学习方法进行全面比较。实验发现不同模型在不同数据集上表现出不同的性能，一些基线方法的性能可以通过使用DyGLib显著提高。 |
| [^83] | [Rethinking Data Distillation: Do Not Overlook Calibration.](http://arxiv.org/abs/2307.12463) | 本文指出经过蒸馏的数据无法很好地进行校准，因为在这种情况下，网络的 logits 分布更加集中，并且语义明确但与分类任务无关的信息会丢失。为了解决这个问题，我们提出了遮蔽温度缩放 (MTS) 和遮蔽蒸馏训练 (MDT) 方法，以获得更好的校准结果。 |
| [^84] | [Hindsight-DICE: Stable Credit Assignment for Deep Reinforcement Learning.](http://arxiv.org/abs/2307.11897) | 本研究提出了Hindsight-DICE算法，利用重要抽样比率估计技术改善了深度强化学习中的信用分配问题。 |
| [^85] | [Scalable Multi-agent Skill Discovery based on Kronecker Graphs.](http://arxiv.org/abs/2307.11629) | 本文提出了一种基于Kronecker图的可扩展多智能体技能发现方法，通过连接状态转换图的Fiedler向量，直接计算具有协作探索行为的多智能体技能。 |
| [^86] | [Efficient selective attention LSTM for well log curve synthesis.](http://arxiv.org/abs/2307.10253) | 本文提出了一种高效的选择性注意力LSTM方法，用于预测缺失的井曲线。通过实验证实了该方法的有效性和可行性。 |
| [^87] | [Efficient Guided Generation for LLMs.](http://arxiv.org/abs/2307.09702) | 本文描述了一种使用正则表达式和上下文无关文法来引导语言模型文本生成的高效方法。 |
| [^88] | [Handwritten and Printed Text Segmentation: A Signature Case Study.](http://arxiv.org/abs/2307.07887) | 本研究旨在解决手写和打印文本分割的挑战，并提出了一种新的方法来完整地恢复不同类别的文本，特别是在重叠部分提高分割性能。同时，还引入了一个新的数据集SignaTR6K，用于支持该任务。 |
| [^89] | [Large Language Models as Superpositions of Cultural Perspectives.](http://arxiv.org/abs/2307.07870) | 大型语言模型被认为是具有个性或一套价值观的，但实际上它可以看作是具有不同价值观和个性特征的角度的叠加。通过角度可控性的概念，我们研究了大型语言模型在不同角度下展示的价值观和个性特征的变化。实验结果表明，即使在没有明显提示的情况下，大型语言模型也会表达出不同的价值观。 |
| [^90] | [Improving Fairness of Graph Neural Networks: A Graph Counterfactual Perspective.](http://arxiv.org/abs/2307.04937) | 这项研究提出了以因果视角看待公平图学习问题的框架CAF，通过选择训练数据中的反事实来避免图神经网络中的偏见。 |
| [^91] | [Continual Learning as Computationally Constrained Reinforcement Learning.](http://arxiv.org/abs/2307.04345) | 本文研究了连续学习作为计算受限的强化学习的主题，提出了一个框架和一套工具来解决人工智能领域长期以来的挑战并促进进一步的研究。 |
| [^92] | [Optimal Bandwidth Selection for DENCLUE.](http://arxiv.org/abs/2307.03206) | 本文提出了一种计算DENCLUE算法最优参数的新方法，并在实验部分讨论了其性能。 |
| [^93] | [Stability of Q-Learning Through Design and Optimism.](http://arxiv.org/abs/2307.02632) | 本文主要介绍了Q-learning在强化学习中的重要性，以及使用乐观性训练和修改后的策略解决Q-learning的稳定性问题和算法收敛加速问题的方法。 |
| [^94] | [Rapid-INR: Storage Efficient CPU-free DNN Training Using Implicit Neural Representation.](http://arxiv.org/abs/2306.16699) | 本文提出了一种使用隐式神经表示进行高效的无CPU深度神经网络训练的新方法，通过在GPU上直接存储整个数据集以INR格式，减少了数据传输开销，从而加速训练过程。同时，采用高度并行化和实时执行的解码过程，进一步提升了压缩效果。 |
| [^95] | [Maintaining Plasticity in Deep Continual Learning.](http://arxiv.org/abs/2306.13812) | 持续性学习中，深度学习系统可能会失去适应新数据的能力，我们提出了一种名为对比可塑性的方法来解决这个问题。 |
| [^96] | [TACOformer:Token-channel compounded Cross Attention for Multimodal Emotion Recognition.](http://arxiv.org/abs/2306.13592) | TACOformer 提出了一种综合性的多模态融合视角，利用 Token-chAnnel COmpound（TACO）Cross Attention 模块，同时建模通道级别和令牌级别的跨模态交互，实现了在多模态情感识别任务上具有先进性能。 |
| [^97] | [Towards Balanced Active Learning for Multimodal Classification.](http://arxiv.org/abs/2306.08306) | 本研究针对多模态分类中的不公平问题，提出了三个设计准则，并提出了一种新的方法来实现更公平的数据选择。研究结果表明，该方法可以实现更平衡的多模态学习。 |
| [^98] | [AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks.](http://arxiv.org/abs/2306.08107) | 论文探讨了AutoML和LLMs之间的共生关系，并指出这两个领域的融合有望颠覆NLP和AutoML两个领域，同时也存在风险。 |
| [^99] | [Diffusion Models for Black-Box Optimization.](http://arxiv.org/abs/2306.07180) | 提出了一种基于扩散模型的离线黑盒优化的反向方法，称为去噪扩散优化模型（DDOM）。DDOM通过学习黑盒函数值条件下的生成模型来优化黑盒函数，克服了数据集质量和高维度下一对多映射困难的限制。 |
| [^100] | [iPLAN: Intent-Aware Planning in Heterogeneous Traffic via Distributed Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2306.06236) | 论文提出了基于MARL算法的iPLAN方法，可在高密度且异构交通场景下进行意图感知规划，使智能体能够从局部观测中推断附近驾驶者的意图，并通过行为或瞬时激励进行决策，实现自主导航。 |
| [^101] | [Task Relation-aware Continual User Representation Learning.](http://arxiv.org/abs/2306.01792) | 本文提出了一种新的持续用户表示学习方法TERACON，它能够学习通用的用户表示，而不是为每个任务学习任务特定的用户表示，具有很强的实用性和学习能力。 |
| [^102] | [Inverse Approximation Theory for Nonlinear Recurrent Neural Networks.](http://arxiv.org/abs/2305.19190) | 该论文证明了使用RNNs逼近非线性序列关系的逆近似定理，进一步将先前在线性RNNs中识别出的记忆难题推广到了一般的非线性情况，并提出了一个有原则的重新参数化方法来克服这些限制。 |
| [^103] | [Towards Certification of Machine Learning-Based Distributed Systems.](http://arxiv.org/abs/2305.16822) | 认证技术对于机器学习驱动的分布式系统验证不适用，本文提出了第一个ML-based分布式系统认证方案，以验证其非功能属性。 |
| [^104] | [Beyond Individual Input for Deep Anomaly Detection on Tabular Data.](http://arxiv.org/abs/2305.15121) | 本文提出了一种采用非参数转换器的深度异常检测方法，能够捕捉表格数据中特征与特征之间以及样本与样本之间的依赖关系，并在广泛的表格数据集上取得了比现有最先进方法更优的性能。 |
| [^105] | [GraphFC: Customs Fraud Detection with Label Scarcity.](http://arxiv.org/abs/2305.11377) | GraphFC是一个模型不可知、领域特定、半监督图神经网络框架，用于少量标签数据的海关欺诈检测。它利用了图神经网络和半监督学习，有效地结合了标记和未标记的数据，提高欺诈检测的性能。 |
| [^106] | [Federated learning for secure development of AI models for Parkinson's disease detection using speech from different languages.](http://arxiv.org/abs/2305.11284) | 本论文利用联邦学习方法，无需共享患者数据，实现在德语、西班牙语和捷克语三种语言数据集上进行帕金森病检测，取得了优于本地模型的诊断准确性。 |
| [^107] | [MPI-rical: Data-Driven MPI Distributed Parallelism Assistance with Transformers.](http://arxiv.org/abs/2305.09438) | 本文提出了一种基于Transformer模型的新方法MPI-rical，通过对大量代码片段进行训练实现自动化MPI代码生成，使并行化成为可能。 |
| [^108] | [IVP-VAE: Modeling EHR Time Series with Initial Value Problem Solvers.](http://arxiv.org/abs/2305.06741) | 本文提出了一种新的方法，在建模电子病历时间序列时，利用直接近似IVP的过程来消除递归计算，从而提高计算效率和训练速度。与目前基于IVP求解器和递归神经网络方法相比，本方法可以达到类似的分类和预测性能。 |
| [^109] | [Learning Decision Trees with Gradient Descent.](http://arxiv.org/abs/2305.03515) | 本文提出了一种使用梯度下降学习决策树的新方法，可以联合优化所有树的参数，从而避免了贪心算法造成次优解的问题。该方法在二分类任务上表现优异，并在多类任务中达到有竞争力的结果。 |
| [^110] | [FedAVO: Improving Communication Efficiency in Federated Learning with African Vultures Optimizer.](http://arxiv.org/abs/2305.01154) | 本文介绍了一种名为FedAVO的算法，利用非洲秃鹫优化器选择最佳超参数来提高联邦学习中的通信效率，显著降低了与FL操作相关的通信成本。 |
| [^111] | [SelfDocSeg: A Self-Supervised vision-based Approach towards Document Segmentation.](http://arxiv.org/abs/2305.00795) | SelfDocSeg提出了一种完全基于视觉的自我监督文档分割方法，通过生成伪布局训练图像编码器学习文档对象的表示和定位，克服了标注数据稀缺的挑战。 |
| [^112] | [Architectures of Topological Deep Learning: A Survey on Topological Neural Networks.](http://arxiv.org/abs/2304.10031) | 拓扑深度学习框架提供了从复杂系统相关数据中提取知识的全面架构，通过解决现有工作的符号和术语不一致问题，有望在应用科学和其他领域开拓新局面。 |
| [^113] | [Tetra-NeRF: Representing Neural Radiance Fields Using Tetrahedra.](http://arxiv.org/abs/2304.09987) | 本文提出了一种基于四面体和 Delaunay 表示的自适应表示方法，用于神经辐射场，可实现高效的训练和最先进的结果，同时提供了更多接近表面的细节，并且实现了比基于点的表示更好的性能。 |
| [^114] | [Improving Performance of Private Federated Models in Medical Image Analysis.](http://arxiv.org/abs/2304.05127) | 本论文通过本地步骤和调整功能进一步提高医学影像分析中的联邦学习模型性能。 |
| [^115] | [Learning Rate Schedules in the Presence of Distribution Shift.](http://arxiv.org/abs/2303.15634) | 该论文提出了一种学习速率表，以在数据分布发生变化时最小化SGD在线学习的后悔，能够对分布转移具有鲁棒性，同时适用于凸损失函数和非凸损失函数。最优学习速率表通常会在数据分布转移的情况下增加，能够用于高维回归模型和神经网络。 |
| [^116] | [Denoising Diffusion Autoencoders are Unified Self-supervised Learners.](http://arxiv.org/abs/2303.09769) | 本文研究了去噪扩散自编码器 (DDAE) 是否能通过无条件图像生成训练获取强有力的线性可分表示，结果表明DDAE是一个统一的自监督学习器，对于自监督生成和辨别性学习是通用的方法。在多类数据集上实现了95.9％和50.0％的线性探测精度，与掩码自编码器和对比学习相当。 |
| [^117] | [Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement.](http://arxiv.org/abs/2303.08983) | 提出了一种名为数据集增强的策略，一次性改进数据集，从而提高任何经过增强的数据集训练的模型的准确性、鲁棒性和校准性。例如，使用ImageDataNet+训练的ResNet-50在ImageNet验证集上的准确率提高了1.7％，在ImageNetV2上提高了3.5％，在ImageNet-R上提高了10.0％。 |
| [^118] | [Exploiting 4D CT Perfusion for segmenting infarcted areas in patients with suspected acute ischemic stroke.](http://arxiv.org/abs/2303.08757) | 该研究提出了一种新颖的方法，通过利用四维CTP全面利用时空信息，以分割疑似急性缺血性卒中患者的梗死区。实验证明，该方法明显优于现有的最先进方法，有潜力为改善AIS患者的早期诊断和治疗规划的准确性和效率做出贡献。 |
| [^119] | [Scalable Stochastic Gradient Riemannian Langevin Dynamics in Non-Diagonal Metrics.](http://arxiv.org/abs/2303.05101) | 本文提出了两种非对角度量，可以在随机梯度采样中使用，以改善神经网络模型的贝叶斯推断性能，尤其对于具有稀疏诱导先验的全连接神经网络和具有相关先验的卷积神经网络效果显著。 |
| [^120] | [Efficient Sensor Placement from Regression with Sparse Gaussian Processes in Continuous and Discrete Spaces.](http://arxiv.org/abs/2303.00028) | 本文提出了一种用稀疏高斯过程解决连续空间下传感器放置问题的方法，避免离散化环境并降低了计算复杂度，可通过贪婪算法找到好的解决方案。 |
| [^121] | [Meta-Learning with Adaptive Weighted Loss for Imbalanced Cold-Start Recommendation.](http://arxiv.org/abs/2302.14640) | 该论文提出了一种用于解决不平衡冷启动推荐问题的元学习算法，通过自适应加权损失来适应用户的个性化需求。 |
| [^122] | [Low Rank Matrix Completion via Robust Alternating Minimization in Nearly Linear Time.](http://arxiv.org/abs/2302.11068) | 本论文提出了一种在几乎线性时间内用鲁棒交替最小化方法完成低秩矩阵补全的方法，并证明了观察几乎线性数量的条目即可恢复矩阵$M$，此方法克服了交替最小化方法需要精确计算的限制，更符合实际实现中对效率的要求。 |
| [^123] | [HLSDataset: Open-Source Dataset for ML-Assisted FPGA Design using High Level Synthesis.](http://arxiv.org/abs/2302.10977) | 本文介绍了HLSDataset，这是一个用于使用高级综合的机器学习辅助FPGA设计的开源数据集。该数据集包含大量高质量的训练样本，并通过案例研究证明了其有效性。 |
| [^124] | [Feature Affinity Assisted Knowledge Distillation and Quantization of Deep Neural Networks on Label-Free Data.](http://arxiv.org/abs/2302.10899) | 本文提出了一种特征亲和力辅助的知识蒸馏方法，通过结合logit损失和特征亲和力损失，可以在无标签数据上压缩深度神经网络模型。 |
| [^125] | [Non-separable Covariance Kernels for Spatiotemporal Gaussian Processes based on a Hybrid Spectral Method and the Harmonic Oscillator.](http://arxiv.org/abs/2302.09580) | 该论文提出了一种基于混合谱方法和谐振子的非可分离协方差核的时空高斯过程研究，通过物理论证推导出一类新型的非可分离协方差核，能更好地捕捉观测到的时空相关性。 |
| [^126] | [Brainomaly: Unsupervised Neurologic Disease Detection Utilizing Unannotated T1-weighted Brain MR Images.](http://arxiv.org/abs/2302.09200) | 本研究提出了Brainomaly，一种基于GAN的图像翻译方法，特别设计用于神经疾病检测。与现有方法不同的是，Brainomaly利用未标注的脑部MR图像，具有更好的无监督疾病检测性能。 |
| [^127] | [A deep complementary energy method for solid mechanics using minimum complementary energy principle.](http://arxiv.org/abs/2302.01538) | 本文提出了一种名为深度互补能量方法(DCEM)的新方法，它基于最小互补能量原理，在固体力学中解决偏微分方程(PDE)，并扩展为DCEM-P以满足更多方程的要求。 |
| [^128] | [NeSyFOLD: Extracting Logic Programs from Convolutional Neural Networks.](http://arxiv.org/abs/2301.12667) | NeSyFOLD是一种神经符号框架，可以从CNN中提取逻辑规则并生成可解释的分类模型。它使用基于规则的FOLD-SE-M机器学习算法和自动映射算法来将CNN核映射到语义概念，并产生可解释的规则集。 |
| [^129] | [The Impossibility of Parallelizing Boosting.](http://arxiv.org/abs/2301.09627) | Boosting算法无法进行有效的并行化，需要指数级别的计算资源，否则并行化的效果并不显著。 |
| [^130] | [A Survey of Self-supervised Learning from Multiple Perspectives: Algorithms, Applications and Future Trends.](http://arxiv.org/abs/2301.05712) | 本综述论文从算法、应用和趋势的角度概述了自监督学习的多维视角。它介绍了SSL算法的动机、共性和差异，以及在图像处理、计算机视觉和自然语言处理等领域中的典型应用。 |
| [^131] | [Towards Explainable Land Cover Mapping: a Counterfactual-based Strategy.](http://arxiv.org/abs/2301.01520) | 本论文提出了一种基于反事实的策略，用于土地覆盖分类任务中的卫星图像时间序列。该方法具有灵活性，能发现土地覆盖类别之间的有趣信息关系，同时通过鼓励时间连续的扰动来得到更稀疏且可解释的解决方案。 |
| [^132] | [Data Augmentation using Transformers and Similarity Measures for Improving Arabic Text Classification.](http://arxiv.org/abs/2212.13939) | 本论文提出一种使用Transformer和相似度度量进行数据增强的方法，以改进阿拉伯文本分类，该方法利用AraGPT-2进行增强，并使用Euclidean、cosine、Jaccard和BLEU距离评估生成的句子。 |
| [^133] | [Development and Evaluation of a Learning-based Model for Real-time Haptic Texture Rendering.](http://arxiv.org/abs/2212.13332) | 该论文开发了一个基于学习的模型，用于实时触觉质地渲染，在多个用户的研究中评估了其感知性能。这个模型可以推广到各种质地和用户交互的变化，并使用视觉触觉传感器的数据进行实时渲染。 |
| [^134] | [Do DALL-E and Flamingo Understand Each Other?.](http://arxiv.org/abs/2212.12249) | DALL-E 和 Flamingo 通过重建图像和生成文本的任务来理解彼此。研究发现，在生成的图像与原始图像相似的情况下，图像描述和文本生成质量较高。 |
| [^135] | [Learning Latent Representations to Co-Adapt to Humans.](http://arxiv.org/abs/2212.09586) | 机器人学习者在与非稳态人类共同互动时面临挑战，本文通过学习和推理人类策略和策略动态的高级表示，提出了一种算法形式以实现机器人与动态人类共适应。 |
| [^136] | [Accelerating Antimicrobial Peptide Discovery with Latent Structure.](http://arxiv.org/abs/2212.09450) | 本研究提出了一种使用潜在结构的方法来加速抗菌肽的发现，该方法能够同时生成具有理想序列属性和二级结构的肽链，并通过湿实验验证了其中两个候选肽链的强大抗菌活性。 |
| [^137] | [SPARF: Large-Scale Learning of 3D Sparse Radiance Fields from Few Input Images.](http://arxiv.org/abs/2212.09100) | 提出了一个大规模的 ShapeNet 合成数据集 SPARF，包括超过 100 万个有多个体素分辨率的 3D 优化的辐射场，用于新视角合成。同时提出了一种新颖的管线 SuRFNet，通过学习少量视图生成稀疏体素辐射场。 |
| [^138] | [Low-Variance Forward Gradients using Direct Feedback Alignment and Momentum.](http://arxiv.org/abs/2212.07282) | 本文提出了一种前向直接反馈对齐算法（FDFA），结合了Activity-Perturbed前向梯度和动量法，用于计算DNN中的低方差梯度估计值。 |
| [^139] | [Scalable Dynamic Mixture Model with Full Covariance for Probabilistic Traffic Forecasting.](http://arxiv.org/abs/2212.06653) | 本文提出了一种扩展性强、适用于复杂概率交通预测的动态混合模型，通过模拟复杂的时变分布以更准确预测交通情况，具有高效性、灵活性和可扩展性。 |
| [^140] | [Federated Few-Shot Learning for Mobile NLP.](http://arxiv.org/abs/2212.05974) | 本研究首次探索了联邦学习在移动少样本自然语言处理中的应用，通过使用伪标签和提示学习算法，实现了仅有少量标记数据时的竞争性准确性。同时，通过创新的设计解决了高执行成本的问题。 |
| [^141] | [DeepCut: Unsupervised Segmentation using Graph Neural Networks Clustering.](http://arxiv.org/abs/2212.05853) | 本研究引入了一种轻量级的图神经网络（GNN）来解决图像分割中特征降维的问题，并采用无监督方法进行分割。与传统方法相比，该方法在构建图时同时使用局部特征和原始特征，从而能更好地进行聚类和分类分割。 |
| [^142] | [P{\O}DA: Prompt-driven Zero-shot Domain Adaptation.](http://arxiv.org/abs/2212.03241) | 本文提出了一种基于提示的零样本领域自适应方法，通过利用预训练的对比视觉语言模型（CLIP）来优化源特征的仿射变换，将其引导到目标文本嵌入中，同时保留其内容和语义。实验表明，该方法在几个数据集上显著优于基于CLIP的风格转移基线，用于下游任务。 |
| [^143] | [Beyond Object Recognition: A New Benchmark towards Object Concept Learning.](http://arxiv.org/abs/2212.02710) | 本文提出了一个挑战性的 Object Concept Learning (OCL) 任务，涉及对象属性、作用及其因果关系。作者构建了密集注释的知识库以支持 OCL，提出了 Object Concept Reasoning Network (OCRN) 作为基线，提升了对象认知的发展。 |
| [^144] | [PhysDiff: Physics-Guided Human Motion Diffusion Model.](http://arxiv.org/abs/2212.02500) | PhysDiff是一种物理引导的人体运动扩散模型，通过将物理约束融入扩散过程中，能够生成多样而逼真的人体动作，并解决了现有模型中存在的物理缺陷问题。 |
| [^145] | [Towards Practical Few-shot Federated NLP.](http://arxiv.org/abs/2212.00192) | 本论文介绍了一个用于处理联邦自然语言处理中少样本问题的方法，通过引入数据生成器和基于提示的联邦学习系统，能够在有限的标记数据下实现与完整微调相媲美的性能。然而，这种性能要求付出显著的系统成本。 |
| [^146] | [PCT-CycleGAN: Paired Complementary Temporal Cycle-Consistent Adversarial Networks for Radar-Based Precipitation Nowcasting.](http://arxiv.org/abs/2211.15046) | 本文提出了一种基于配对互补时间循环一致对抗网络的雷达降水预测方法，该方法包括两个生成器网络和循环一致性损失和对抗性损失。实验证明，该方法在准确性和推广能力方面优于现有的技术方法。 |
| [^147] | [Neural Architecture for Online Ensemble Continual Learning.](http://arxiv.org/abs/2211.14963) | 提出了一种完全可微的在线集成持续学习方法，实验结果表明在没有内存缓冲区的情况下取得了最先进的结果，并且可以通过较少的分类器获得较高的分类准确性。 |
| [^148] | [MPCViT: Searching for Accurate and Efficient MPC-Friendly Vision Transformer with Heterogeneous Attention.](http://arxiv.org/abs/2211.13955) | 本文提出了一种MPC友好的视觉Transformer模型MPCViT，该模型通过异构注意力搜索来实现准确且高效的ViT推理，并通过简单而有效的神经架构搜索算法来进一步提高推理效率。 |
| [^149] | [Can lies be faked? Comparing low-stakes and high-stakes deception video datasets from a Machine Learning perspective.](http://arxiv.org/abs/2211.13035) | 该论文从机器学习的角度比较了低风险和高风险的欺骗视频数据集，并探究了它们在实际应用中的区别和可行性。 |
| [^150] | [A Two-Stage Active Learning Algorithm for $k$-Nearest Neighbors.](http://arxiv.org/abs/2211.10773) | 本文提出了一种用于训练 $k$ 最近邻分类器的简单直观的主动学习算法，首次保持了 $k$ 最近邻投票概念的预测时间，并提供了一致性保证。 |
| [^151] | [Graph Neural Networks on SPD Manifolds for Motor Imagery Classification: A Perspective from the Time-Frequency Analysis.](http://arxiv.org/abs/2211.02641) | 本文介绍了一种基于SPD流形的图神经网络用于运动想象分类，利用EEG的二阶统计量，相比传统方法具有更好的性能。 |
| [^152] | [A Profit-Maximizing Strategy for Advertising on the e-Commerce Platforms.](http://arxiv.org/abs/2211.01160) | 本文提出了一种针对在线广告定向选项的最大化利润策略，通过将优化问题重新定义为多选背包问题，找到最佳特征组合以增加将定向受众转化为实际购买者的概率。 |
| [^153] | [Fant\^omas: Understanding Face Anonymization Reversibility.](http://arxiv.org/abs/2210.10651) | 本文对脸部匿名逆转现象进行了全面的研究，发现11种脸部匿名化方法至少部分可逆，并强调了重构和反演实现可逆性的机制。 |
| [^154] | [Bayesian Prompt Learning for Image-Language Model Generalization.](http://arxiv.org/abs/2210.02390) | 本文提出了一种基于贝叶斯方法的提示学习框架，对提示空间进行正则化，提高了对未见提示的泛化能力。 |
| [^155] | [Bias and Extrapolation in Markovian Linear Stochastic Approximation with Constant Stepsizes.](http://arxiv.org/abs/2210.00953) | 本研究研究了线性随机逼近中的偏差和外推问题。我们证明了在恒定步长和Markov数据的情况下，LSA迭代会收敛到唯一的极限和稳定分布，并建立了非渐进的几何收敛速度。我们还发现，这个极限的偏差与步长成比例，直至更高阶项。在可逆链的情况下，我们还探讨了偏差与Markov数据的混合时间之间的关系。 |
| [^156] | [Dataset Distillation Using Parameter Pruning.](http://arxiv.org/abs/2209.14609) | 本文提出了一种使用参数修剪的数据集蒸馏方法，该方法可以在蒸馏过程中修剪难以匹配的参数，提高蒸馏性能。 |
| [^157] | [Learning to Exploit Elastic Actuators for Quadruped Locomotion.](http://arxiv.org/abs/2209.07171) | 本文提出了一种直接在真实机器人上学习无模型控制器的方法，利用弹性执行器的动力学性质优化动态运动控制，学习到的控制器可实现高性能的四足运动控制。 |
| [^158] | [Multipoint-BAX: A New Approach for Efficiently Tuning Particle Accelerator Emittance via Virtual Objectives.](http://arxiv.org/abs/2209.04587) | 本论文提出了一种名为多点-BAX的新方法，通过虚拟目标来高效调整粒子加速器的发射度。该方法避免了使用传统的黑盒优化器进行缓慢而低效的多点查询，并通过快速学习模型计算发射度目标。该方法在Linac相干光源(LCLS)和Facility for Adv中最小化发射度。 |
| [^159] | [Towards Top-Down Automated Development in Limited Scopes: A Neuro-Symbolic Framework from Expressibles to Executables.](http://arxiv.org/abs/2209.01566) | 本研究提出了一个神经符号框架，从可表达性到可执行性实现了自顶向下的自动化开发。通过建立代码分类法和使用语义金字塔来关联文本数据和代码数据，我们可以改进深度代码生成的效果。 |
| [^160] | [Some Supervision Required: Incorporating Oracle Policies in Reinforcement Learning via Epistemic Uncertainty Metrics.](http://arxiv.org/abs/2208.10533) | 本文提出一种名为评判置信度引导探索的方法，用于将现有的神谕策略纳入标准的演员-评论家强化学习算法中，以提高探索效率。在不确定性高时，该方法会将神谕策略的行动作为建议纳入学习方案中，而在不确定性低时忽略它。 |
| [^161] | [Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks.](http://arxiv.org/abs/2207.13243) | 这篇综述调查了深度神经网络内部结构内部解释方法，并提出了一种分析方法的分类。这些解释方法对于帮助构建更可信赖的AI是至关重要的。 |
| [^162] | [Mitigating the Performance Sacrifice in DP-Satisfied Federated Settings through Graph Contrastive Learning.](http://arxiv.org/abs/2207.11836) | 本文研究了如何在联邦图学习中实现差分隐私，并观察到性能下降。图边上的差分隐私引入的噪声扰乱了图的相似性，限制了图学习模型的性能。 |
| [^163] | [The prediction of the quality of results in Logic Synthesis using Transformer and Graph Neural Networks.](http://arxiv.org/abs/2207.11437) | 该论文介绍了一种使用Transformer和图神经网络预测逻辑综合结果质量的深度学习方法，通过将结构转换表示为向量并提取优化序列的特征，以及利用图神经网络学习电路的图表示和预测QoR。 |
| [^164] | [Generative Pretraining for Black-Box Optimization.](http://arxiv.org/abs/2206.10786) | 该论文提出了一种使用离线数据预训练黑盒优化器的生成框架BONET，使用自回归模型和样本策略合成轨迹以帮助在高维空间中优化昂贵的黑盒函数。 |
| [^165] | [StudioGAN: A Taxonomy and Benchmark of GANs for Image Synthesis.](http://arxiv.org/abs/2206.09479) | StudioGAN提供了一个分类和基准库，支持多种GAN架构、条件方法、对抗损失、正则化模块、可微增强方法、评估指标和评估骨架。这个基准库可以用于训练和评估各种GAN模型，提供了大规模的基准数据集和评估方案。 |
| [^166] | [A Control Theoretic Framework for Adaptive Gradient Optimizers in Machine Learning.](http://arxiv.org/abs/2206.02034) | 本文提出了一个通用的自适应梯度方法框架，用于解决非凸优化问题。文章将自适应梯度方法建模为状态空间框架，并利用经典控制理论中的传递函数范式提出了一种新的Adam变体，称为AdamSSM。该算法在基准机器学习任务上表现出较好的性能优势。 |
| [^167] | [Hybrid Models for Mixed Variables in Bayesian Optimization.](http://arxiv.org/abs/2206.01409) | 本文提出了一种新型的混合模型，用于混合变量贝叶斯优化，并且在搜索和代理模型阶段都具有创新之处。数值实验证明了混合模型的优越性。 |
| [^168] | [Polynomial-Time Algorithms for Counting and Sampling Markov Equivalent DAGs with Applications.](http://arxiv.org/abs/2205.02654) | 本文提出了一种多项式时间算法，用于在马尔可夫等价类中计数和采样有向无环图。该算法解决了这一领域的长期未解决问题，并且在实验中得到验证，可以在活跃学习因果结构和因果效应识别方面实际应用。 |
| [^169] | [ConceptEvo: Interpreting Concept Evolution in Deep Learning Training.](http://arxiv.org/abs/2203.16475) | ConceptEvo是一个统一的深度神经网络解释框架，可以在训练过程中揭示概念的产生和演变，并通过人机评估和实验证明其发现对模型和预测具有重要意义。 |
| [^170] | [An Introduction to Neural Data Compression.](http://arxiv.org/abs/2202.06533) | 本文介绍了神经数据压缩领域，其中应用了神经网络和其他机器学习方法来进行数据压缩。通过使用生成模型，如标准化流、变分自编码器、扩散概率模型和生成对抗网络，可以从数据中端到端地学习压缩算法。本文提供了对必要背景知识（如信息论和计算机视觉）的回顾，并对目前文献中的基本思想和方法进行了精选指南。 |
| [^171] | [Neural Implicit Surface Evolution.](http://arxiv.org/abs/2201.09636) | 本文研究了神经网络在建模隐式曲面动态变化方面的应用，将其扩展到时空维度，实现了连续几何变换。通过考虑数据拟合和水平集方程的约束，网络能够快速收敛并逼近底层几何演化。 |
| [^172] | [Investigating Conversion from Mild Cognitive Impairment to Alzheimer's Disease using Latent Space Manipulation.](http://arxiv.org/abs/2111.08794) | 本研究提出了一个深度学习框架，利用潜空间调整来发现轻度认知障碍（MCI）转化为阿尔茨海默病的重要属性并解析其行为。 |
| [^173] | [Towards Designing Optimal Sensing Matrices for Generalized Linear Inverse Problems.](http://arxiv.org/abs/2111.03237) | 本文研究了设计最优感知矩阵对广义线性逆问题的重构性能的影响。通过对期望传播算法的性能进行研究，我们发现感知矩阵的频谱尖锐度对恢复性能很重要，并且尖锐度的影响取决于非线性函数$f$。根据我们的框架，我们提出了一种自适应的感知矩阵优化算法来最大化期望传播算法的性能。 |
| [^174] | [MMD Aggregated Two-Sample Test.](http://arxiv.org/abs/2110.15073) | 本文提出了两种新颖的基于最大均值差异（MMD）的非参数双样本核检验，并构造了一种自适应平均测试，称为MMDAgg，以解决平滑参数未知的问题。 |
| [^175] | [CausalAF: Causal Autoregressive Flow for Safety-Critical Driving Scenario Generation.](http://arxiv.org/abs/2110.13939) | CausalAF是一种用于安全关键驾驶场景生成的流模型，通过引入因果关系先验和新颖的因果掩蔽操作，将生成模型从仅从数据中学习相关性的情况扩展到学习生成场景如何引起风险情况的因果机制，以提高系统鲁棒性评估的多样性和效率。 |
| [^176] | [Learning Domain Invariant Representations by Joint Wasserstein Distance Minimization.](http://arxiv.org/abs/2106.04923) | 本文通过研究监督机器学习损失与联合空间的Wasserstein距离之间的关系，提出了一种学习域不变表示的方法。 |
| [^177] | [Discriminative Bayesian filtering lends momentum to the stochastic Newton method for minimizing log-convex functions.](http://arxiv.org/abs/2104.12949) | 该论文提出了一种判别贝叶斯滤波的方法，为随机牛顿法在最小化对数凸函数中提供了动力。通过考虑整个历史信息形成更新，该方法能够在迭代开始时减弱旧观测的影响。 |
| [^178] | [Walking Out of the Weisfeiler Leman Hierarchy: Graph Learning Beyond Message Passing.](http://arxiv.org/abs/2102.08786) | 我们提出了一种名为CRaWl的图学习神经网络架构，它通过利用随机游走过程中出现的子图来提取和聚合信息，从而检测长程相互作用并计算非局部特征。我们证明了CRaWl的表达能力与Weisfeiler Leman算法和图神经网络不可比较，实验证明它在多个基准数据集上与最先进的GNN架构相匹配。 |
| [^179] | [Deep Reinforcement Learning for Active High Frequency Trading.](http://arxiv.org/abs/2101.07107) | 本文介绍了一个基于深度强化学习的框架，用于在股票市场中进行活跃的高频交易。通过训练DRL代理来交易股票，并使用Proximal Policy Optimization算法进行优化。通过仅选择具有最大价格变动的训练样本来提高训练数据的信噪比。通过实验证明，代理能够创建对底层环境的动态表示，并能够识别偶尔出现的规律。 |
| [^180] | [A method to integrate and classify normal distributions.](http://arxiv.org/abs/2012.14331) | 本文介绍了一种可以对任意参数维度下的任意域内正态分布进行积分的方法，提供了法向向量函数的相关概率密度和统计指标，同时还提供了可以对任意数量正态分布进行分类的方法和维度降低和可视化的技术。 |
| [^181] | [One-Vote Veto: Semi-Supervised Learning for Low-Shot Glaucoma Diagnosis.](http://arxiv.org/abs/2012.04841) | 本文扩展了孪生网络，提出了一种可在有限标记数据和不平衡情况下进行低样本学习的训练方法，并引入了半监督学习策略，利用额外的未标记数据来提高准确性。 |
| [^182] | [Gradient Descent for Deep Matrix Factorization: Dynamics and Implicit Bias towards Low Rank.](http://arxiv.org/abs/2011.13772) | 通过分析梯度下降在线性网络和估计问题中的动力学，揭示了深度学习中的隐含偏好现象。 |
| [^183] | [Logarithmic Regret in Multisecretary and Online Linear Programming Problems with Continuous Valuations.](http://arxiv.org/abs/1912.08917) | 本文研究了一个收益管理问题，其中顾客的到达是连续的，有限均值连续分布的效用值和有界离散或连续分布的库存量。研究发现，如果初始库存量与顾客数成线性比例，随着顾客数的增加，预期的遗憾将以对数的速度增长。 |
| [^184] | [Generalization in Deep Learning.](http://arxiv.org/abs/1710.05468) | 本文从理论上解释了为什么以及如何深度学习能够在容量大、复杂性高、可能存在算法不稳定性、非鲁棒性和尖锐极小值的情况下实现良好的泛化，提出了一些新的开放问题，并讨论了研究结果的局限性。 |

# 详细

[^1]: 应用于逻辑回归的高性能计算：CPU和GPU实现的比较

    High Performance Computing Applied to Logistic Regression: A CPU and GPU Implementation Comparison. (arXiv:2308.10037v1 [cs.LG])

    [http://arxiv.org/abs/2308.10037](http://arxiv.org/abs/2308.10037)

    我们提出了一种多功能的基于GPU的并行逻辑回归算法，相比CPU实现在执行时间上具有更好的性能，适用于实时预测应用。

    

    我们提出了一种多功能的基于GPU的并行逻辑回归（LR）算法，旨在应对大数据集中对更快算法的需求。我们的实现是对X. Zou等人提出的并行梯度下降逻辑回归算法的直接翻译。我们的实验证明，相比现有的基于CPU的实现，我们基于GPU的LR在执行时间上表现更好，同时保持可比较的f1分数。对于实时预测应用，如图像识别、垃圾邮件检测和欺诈检测，我们的方法在处理大型数据集时具有显著的加速性能。我们的算法已经在一个可直接使用的Python库中实现，可在以下链接中获取：https://github.com/NechbaMohammed/SwiftLogisticReg

    We present a versatile GPU-based parallel version of Logistic Regression (LR), aiming to address the increasing demand for faster algorithms in binary classification due to large data sets. Our implementation is a direct translation of the parallel Gradient Descent Logistic Regression algorithm proposed by X. Zou et al. [12]. Our experiments demonstrate that our GPU-based LR outperforms existing CPU-based implementations in terms of execution time while maintaining comparable f1 score. The significant acceleration of processing large datasets makes our method particularly advantageous for real-time prediction applications like image recognition, spam detection, and fraud detection. Our algorithm is implemented in a ready-to-use Python library available at : https://github.com/NechbaMohammed/SwiftLogisticReg
    
[^2]: 用于确定车辆劫持推文的半监督异常检测方法

    Semi-Supervised Anomaly Detection for the Determination of Vehicle Hijacking Tweets. (arXiv:2308.10036v1 [cs.LG])

    [http://arxiv.org/abs/2308.10036](http://arxiv.org/abs/2308.10036)

    本研究提出了一种新的半监督方法，使用推文来识别车辆劫持事件。通过应用无监督异常检测算法和比较评估，发现CBLOF方法在准确率和F1-Score等指标上稍微优于KNN方法，因此被选为首选方法。

    

    在南非，车辆劫持问题日益严重。这导致旅行者时刻担心成为这种事件的受害者。本研究提出了一种新的半监督方法，使用推文来识别劫持事件，采用无监督异常检测算法。获取包含关键字"劫持"的推文，并使用词频-逆文档频率（TF-IDF）进行存储和处理，然后通过使用两种异常检测算法进行进一步分析：1）K最近邻（KNN）；2）基于聚类的异常因子（CBLOF）。比较评估结果表明，KNN方法的准确率为89%，而CBLOF的准确率为90%。CBLOF方法还能够获得0.8的F1-Score，而KNN的得分为0.78。因此，这两种方法有轻微差异，有利于CBLOF，被选为确定相关劫持事件的首选无监督方法。

    In South Africa, there is an ever-growing issue of vehicle hijackings. This leads to travellers constantly being in fear of becoming a victim to such an incident. This work presents a new semi-supervised approach to using tweets to identify hijacking incidents by using unsupervised anomaly detection algorithms. Tweets consisting of the keyword "hijacking" are obtained, stored, and processed using the term frequency-inverse document frequency (TF-IDF) and further analyzed by using two anomaly detection algorithms: 1) K-Nearest Neighbour (KNN); 2) Cluster Based Outlier Factor (CBLOF). The comparative evaluation showed that the KNN method produced an accuracy of 89%, whereas the CBLOF produced an accuracy of 90%. The CBLOF method was also able to obtain a F1-Score of 0.8, whereas the KNN produced a 0.78. Therefore, there is a slight difference between the two approaches, in favour of CBLOF, which has been selected as a preferred unsupervised method for the determination of relevant hijack
    
[^3]: 基于卷积自编码器瓶颈宽度的基于StarGAN的唱歌技巧转换的影响

    Effects of Convolutional Autoencoder Bottleneck Width on StarGAN-based Singing Technique Conversion. (arXiv:2308.10021v1 [eess.AS])

    [http://arxiv.org/abs/2308.10021](http://arxiv.org/abs/2308.10021)

    本文研究了卷积自编码器瓶颈宽度对基于StarGAN的唱歌技巧转换合成质量的影响，并通过主观评估结果发现，更宽的瓶颈可以提高发音清晰度，但不一定能够增加与目标声音的相似度。

    

    唱歌技巧转换（STC）是指在保持原始歌手身份、旋律和语言组成不变的情况下，将一种声乐技巧转换为另一种声乐技巧的任务。先前的STC研究以及一般的歌声转换研究都使用了卷积自编码器（CAE）进行转换，但CAE的瓶颈宽度对合成质量的影响尚未得到彻底评估。为此，我们构建了一个基于GAN的多域STC系统，该系统利用了WORLD声码器表征和CAE架构。我们改变了CAE的瓶颈宽度，并主观评估了转换结果。该模型在一个包括四位歌手和四种唱歌技巧（胸声、假声、嘶哑声和哨音声）的普通话数据集上进行了训练。结果表明，更宽的瓶颈对于发音清晰度更好，但不一定导致更接近目标声音的相似度。

    Singing technique conversion (STC) refers to the task of converting from one voice technique to another while leaving the original singer identity, melody, and linguistic components intact. Previous STC studies, as well as singing voice conversion research in general, have utilized convolutional autoencoders (CAEs) for conversion, but how the bottleneck width of the CAE affects the synthesis quality has not been thoroughly evaluated. To this end, we constructed a GAN-based multi-domain STC system which took advantage of the WORLD vocoder representation and the CAE architecture. We varied the bottleneck width of the CAE, and evaluated the conversion results subjectively. The model was trained on a Mandarin dataset which features four singers and four singing techniques: the chest voice, the falsetto, the raspy voice, and the whistle voice. The results show that a wider bottleneck corresponds to better articulation clarity but does not necessarily lead to higher likeness to the target te
    
[^4]: 通过得分匹配实现的半隐式变分推断

    Semi-Implicit Variational Inference via Score Matching. (arXiv:2308.10014v1 [stat.ML])

    [http://arxiv.org/abs/2308.10014](http://arxiv.org/abs/2308.10014)

    本文提出了一种基于得分匹配的半隐式变分推断方法SIVI-SM，该方法利用了半隐式变分家族的层次结构，并通过处理不可计算的变分密度来实现与MCMC相当的准确性，在各种贝叶斯推断任务中优于基于ELBO的SIVI方法。

    

    半隐式变分推断（SIVI）通过考虑以层次方式定义的隐式变分分布，极大地丰富了变分家族的表达能力。然而，由于变分分布的不可计算密度，当前的SIVI方法通常使用替代证据下界（ELBO）或使用昂贵的内循环MCMC运行以进行无偏ELBO的训练。在本文中，我们提出了SIVI-SM，一种基于得分匹配的替代训练目标的SIVI新方法。利用半隐式变分家族的层次结构，得分匹配目标允许使用去噪得分匹配自然处理不可计算的变分密度的最小最大形式。我们表明，SIVI-SM在各种贝叶斯推断任务中几乎与MCMC的准确性一致，并且优于基于ELBO的SIVI方法。

    Semi-implicit variational inference (SIVI) greatly enriches the expressiveness of variational families by considering implicit variational distributions defined in a hierarchical manner. However, due to the intractable densities of variational distributions, current SIVI approaches often use surrogate evidence lower bounds (ELBOs) or employ expensive inner-loop MCMC runs for unbiased ELBOs for training. In this paper, we propose SIVI-SM, a new method for SIVI based on an alternative training objective via score matching. Leveraging the hierarchical structure of semi-implicit variational families, the score matching objective allows a minimax formulation where the intractable variational densities can be naturally handled with denoising score matching. We show that SIVI-SM closely matches the accuracy of MCMC and outperforms ELBO-based SIVI methods in a variety of Bayesian inference tasks.
    
[^5]: 可拆卸式迁移学习用于选择性源任务遗忘

    Disposable Transfer Learning for Selective Source Task Unlearning. (arXiv:2308.09971v1 [cs.LG])

    [http://arxiv.org/abs/2308.09971](http://arxiv.org/abs/2308.09971)

    本论文提出了一种称为可拆卸式迁移学习的新范式，通过引入梯度碰撞损失，该方法可以选择性地遗忘源任务而不降低目标任务的性能。

    

    迁移学习被广泛用于训练深度神经网络(DNN)来构建强大的表示。即使在预训练模型适应目标任务后，特征提取器的表示性能仍然保留在一定程度上。由于预训练模型的性能可以被视为所有者的私有财产，自然而然地寻求预训练权重的广义性能的独占权是合理的。为了解决这个问题，我们提出了一种称为可拆卸式迁移学习(DTL)的新范式，它只处理源任务而不降低目标任务的性能。为了实现知识的遗忘，我们提出了一种新的损失函数，称为梯度碰撞损失(GC损失)。GC损失通过引导不同mini-batches的梯度向量朝不同方向前进，选择性地遗忘源知识。模型是否成功遗忘源任务通过“背驮式学习准确性”(PL准确性)来衡量。

    Transfer learning is widely used for training deep neural networks (DNN) for building a powerful representation. Even after the pre-trained model is adapted for the target task, the representation performance of the feature extractor is retained to some extent. As the performance of the pre-trained model can be considered the private property of the owner, it is natural to seek the exclusive right of the generalized performance of the pre-trained weight. To address this issue, we suggest a new paradigm of transfer learning called disposable transfer learning (DTL), which disposes of only the source task without degrading the performance of the target task. To achieve knowledge disposal, we propose a novel loss named Gradient Collision loss (GC loss). GC loss selectively unlearns the source knowledge by leading the gradient vectors of mini-batches in different directions. Whether the model successfully unlearns the source task is measured by piggyback learning accuracy (PL accuracy). PL
    
[^6]: 通过学习内心独白解决视觉语言任务

    Tackling Vision Language Tasks Through Learning Inner Monologues. (arXiv:2308.09970v1 [cs.CL])

    [http://arxiv.org/abs/2308.09970](http://arxiv.org/abs/2308.09970)

    通过学习内心独白，提出了一种新方法（IMMO）来解决复杂的视觉语言任务，克服了混合融合和特征对齐方法所面临的优化和可解释性问题。

    

    视觉语言任务需要AI模型对视觉和文本内容进行理解和推理。基于大型语言模型（LLM）的强大力量，出现了两种主要方法：（1）LLM和视觉-语言模型（VLM）之间的混合融合，其中视觉输入首先被VLM转化为语言描述，成为LLM生成最终答案的输入；（2）语言空间中的视觉特征对齐，其中视觉输入被编码为嵌入向量，并通过进一步的监督微调将其投影到LLM的语言空间中。第一种方法具有轻量级的训练成本和可解释性，但很难以端到端的方式进行优化。第二种方法具有相当的性能，但特征对齐通常需要大量的训练数据，并且缺乏可解释性。为了解决这个困境，我们提出了一种新的方法，即内心独白多模态优化（IMMO），通过模拟思维过程来解决复杂的视觉语言问题。

    Visual language tasks require AI models to comprehend and reason with both visual and textual content. Driven by the power of Large Language Models (LLMs), two prominent methods have emerged: (1) the hybrid integration between LLMs and Vision-Language Models (VLMs), where visual inputs are firstly converted into language descriptions by VLMs, serving as inputs for LLMs to generate final answer(s); (2) visual feature alignment in language space, where visual inputs are encoded as embeddings and projected to LLMs' language space via further supervised fine-tuning. The first approach provides light training costs and interpretability but is hard to be optimized in an end-to-end fashion. The second approach presents decent performance, but feature alignment usually requires large amounts of training data and lacks interpretability. To tackle this dilemma, we propose a novel approach, Inner Monologue Multi-Modal Optimization (IMMO), to solve complex vision language problems by simulating in
    
[^7]: 基于风格对齐的畸变感知语义分割

    Anomaly-Aware Semantic Segmentation via Style-Aligned OoD Augmentation. (arXiv:2308.09965v1 [cs.CV])

    [http://arxiv.org/abs/2308.09965](http://arxiv.org/abs/2308.09965)

    在自动驾驶中遇到未知对象是不可避免的，为了解决这个问题，我们提出了一种基于风格对齐的畸变感知语义分割方法。通过减小OoD数据和驾驶场景之间的域差异，我们改进了OoD合成过程，并利用预训练模型生成“给定类别之外”的预测结果进行异常分割，从而提高了性能。

    

    在自动驾驶的背景下，在开放世界中遇到未知对象成为不可避免的。因此，将标准的语义分割模型配备异常感知是至关重要的。许多先前的方法利用合成的离群分布（OoD）数据增强来解决这个问题。本文通过减小OoD数据和驾驶场景之间的域差异，有效减少了培训过程中可能作为明显捷径的风格差异，从而改进了OoD合成过程。此外，我们提出了一种简单的微调损失函数，有效地导致预训练的语义分割模型生成“给定类别之外”的预测结果，利用每个像素的OoD分数进行异常分割。通过最小的微调工作，我们的流水线使得预训练模型能够用于异常分割，同时保持原任务的性能。

    Within the context of autonomous driving, encountering unknown objects becomes inevitable during deployment in the open world. Therefore, it is crucial to equip standard semantic segmentation models with anomaly awareness. Many previous approaches have utilized synthetic out-of-distribution (OoD) data augmentation to tackle this problem. In this work, we advance the OoD synthesis process by reducing the domain gap between the OoD data and driving scenes, effectively mitigating the style difference that might otherwise act as an obvious shortcut during training. Additionally, we propose a simple fine-tuning loss that effectively induces a pre-trained semantic segmentation model to generate a ``none of the given classes" prediction, leveraging per-pixel OoD scores for anomaly segmentation. With minimal fine-tuning effort, our pipeline enables the use of pre-trained models for anomaly segmentation while maintaining the performance on the original task.
    
[^8]: 通过QoS感知的模型切换实现自适应机器学习系统

    Towards Self-Adaptive Machine Learning-Enabled Systems Through QoS-Aware Model Switching. (arXiv:2308.09960v1 [cs.SE])

    [http://arxiv.org/abs/2308.09960](http://arxiv.org/abs/2308.09960)

    通过使用多个模型管理ML模型相关的不确定性，我们提出了一种自适应机器学习系统，该系统利用自适应技术和动态模型切换以提高整体服务质量（QoS）。

    

    机器学习（ML），特别是深度学习，取得了巨大的进展，导致了机器学习驱动的系统（MLS）的兴起。然而，在推进这些MLS进入生产中仍然存在许多软件工程挑战，主要是由于影响整体服务质量（QoS）的各种运行时不确定性。这些不确定性来自ML模型、软件组件和环境因素。自适应技术在管理运行时不确定性方面具有潜力，但在MLS中的应用仍然较少探索。作为解决方案，我们提出了机器学习模型平衡器的概念，重点解决与ML模型相关的不确定性通过使用多个模型进行管理。随后，我们引入了AdaMLS，一种新颖的自适应方法，它利用这一概念并扩展了传统的MAPE-K循环以实现持续的MLS自适应。AdaMLS采用轻量级无监督学习用于动态模型切换，从而确保一致的Qo

    Machine Learning (ML), particularly deep learning, has seen vast advancements, leading to the rise of Machine Learning-Enabled Systems (MLS). However, numerous software engineering challenges persist in propelling these MLS into production, largely due to various run-time uncertainties that impact the overall Quality of Service (QoS). These uncertainties emanate from ML models, software components, and environmental factors. Self-adaptation techniques present potential in managing run-time uncertainties, but their application in MLS remains largely unexplored. As a solution, we propose the concept of a Machine Learning Model Balancer, focusing on managing uncertainties related to ML models by using multiple models. Subsequently, we introduce AdaMLS, a novel self-adaptation approach that leverages this concept and extends the traditional MAPE-K loop for continuous MLS adaptation. AdaMLS employs lightweight unsupervised learning for dynamic model switching, thereby ensuring consistent Qo
    
[^9]: 恶意软件检测的对抗学习技术比较

    A Comparison of Adversarial Learning Techniques for Malware Detection. (arXiv:2308.09958v1 [cs.CR])

    [http://arxiv.org/abs/2308.09958](http://arxiv.org/abs/2308.09958)

    本文比较了针对恶意软件检测的对抗学习技术，通过生成对抗性恶意软件样本，并测试其对杀毒产品的检测性能。结果显示，对先前检测到的恶意软件应用优化修改可能导致错误分类，同时生成的恶意软件样本可成功用于其他检测模型。

    

    机器学习已被证明是自动化恶意软件检测的有用工具，但机器学习模型也显示出对抗攻击的脆弱性。本文针对生成对抗性恶意软件样本，特别是恶意的Windows可执行文件进行了研究。我们总结和比较了关于对抗机器学习用于恶意软件检测的工作。我们使用基于梯度、基于进化算法和基于强化学习的方法生成对抗样本，并对生成的样本应用于选择的杀毒产品进行测试。我们比较了这些方法在准确性和实际应用性方面的性能。结果显示，对先前检测到的恶意软件应用优化修改可能导致将文件错误分类为良性。已知生成的恶意软件样本可成功用于其他检测模型，并且利用基因组合可以增加生成的样本的对抗效果。

    Machine learning has proven to be a useful tool for automated malware detection, but machine learning models have also been shown to be vulnerable to adversarial attacks. This article addresses the problem of generating adversarial malware samples, specifically malicious Windows Portable Executable files. We summarize and compare work that has focused on adversarial machine learning for malware detection. We use gradient-based, evolutionary algorithm-based, and reinforcement-based methods to generate adversarial samples, and then test the generated samples against selected antivirus products. We compare the selected methods in terms of accuracy and practical applicability. The results show that applying optimized modifications to previously detected malware can lead to incorrect classification of the file as benign. It is also known that generated malware samples can be successfully used against detection models other than those used to generate them and that using combinations of gene
    
[^10]: 是修剪还是不修剪：一种基于混沌因果性的稠密神经网络有原则修剪方法

    To prune or not to prune : A chaos-causality approach to principled pruning of dense neural networks. (arXiv:2308.09955v1 [cs.LG])

    [http://arxiv.org/abs/2308.09955](http://arxiv.org/abs/2308.09955)

    这个论文提出了一种基于混沌因果性的稠密神经网络有原则修剪方法，通过选择特定权重来最小化错分类，修剪后的网络保持了原始性能和特征的可解释性。

    

    在资源受限设备上通过去除权重而不影响性能来减小神经网络的大小（修剪），是一个重要的问题。过去，修剪通常是通过基于大小等标准对权重进行排序或惩罚，并在重新训练剩余权重之前去除低排名权重来实现的。修剪策略也可以涉及从网络中删除神经元，以实现所需的网络尺寸减小。我们将修剪问题定式为优化问题，目标是通过选择特定权重来最小化错分类。为实现这一目标，我们引入了学习中的混沌概念（李亚普诺夫指数），通过权重更新并利用因果关系来识别造成错分类的因果性权重。这样修剪后的网络保持了原始性能并保留了特征的可解释性。

    Reducing the size of a neural network (pruning) by removing weights without impacting its performance is an important problem for resource-constrained devices. In the past, pruning was typically accomplished by ranking or penalizing weights based on criteria like magnitude and removing low-ranked weights before retraining the remaining ones. Pruning strategies may also involve removing neurons from the network in order to achieve the desired reduction in network size. We formulate pruning as an optimization problem with the objective of minimizing misclassifications by selecting specific weights. To accomplish this, we have introduced the concept of chaos in learning (Lyapunov exponents) via weight updates and exploiting causality to identify the causal weights responsible for misclassification. Such a pruned network maintains the original performance and retains feature explainability.
    
[^11]: 在数据中寻找出现：启发于因果出现的动力学学习

    Finding emergence in data: causal emergence inspired dynamics learning. (arXiv:2308.09952v1 [physics.soc-ph])

    [http://arxiv.org/abs/2308.09952](http://arxiv.org/abs/2308.09952)

    本文引入了一种基于因果出现理论的机器学习框架，能够在数据中学习宏观动力学和量化出现程度。实验证明该框架能够成功捕捉出现模式，并学习粗粒化策略，具有广泛的适用性。

    

    以数据驱动的方式对复杂动态系统建模具有挑战性，因为微观层面的观测数据无法直接捕捉到出现行为和属性。因此，开发一个能够有效捕捉宏观层面出现动力学并根据可用数据量化出现的模型至关重要。受因果出现理论的启发，本文引入了一种机器学习框架，旨在学习一个包含出现潜在空间的宏观动力学。该框架通过最大化有效信息（EI）来获得一个具有更强因果效果的宏观动力学模型。对模拟和真实数据的实验结果表明了所提框架的有效性。它不仅成功捕捉到出现模式，还学习了粗粒化策略并量化了数据中的因果出现程度。此外，对不同环境进行的实验表明了框架在建模各种复杂动态系统中的适用性。

    Modelling complex dynamical systems in a data-driven manner is challenging due to the presence of emergent behaviors and properties that cannot be directly captured by micro-level observational data. Therefore, it is crucial to develop a model that can effectively capture emergent dynamics at the macro-level and quantify emergence based on the available data. Drawing inspiration from the theory of causal emergence, this paper introduces a machine learning framework aimed at learning macro-dynamics within an emergent latent space. The framework achieves this by maximizing the effective information (EI) to obtain a macro-dynamics model with stronger causal effects. Experimental results on both simulated and real data demonstrate the effectiveness of the proposed framework. Not only does it successfully capture emergent patterns, but it also learns the coarse-graining strategy and quantifies the degree of causal emergence in the data. Furthermore, experiments conducted on environments dif
    
[^12]: 自动机器学习在检测心血管疾病中的有效性研究

    Study on the effectiveness of AutoML in detecting cardiovascular disease. (arXiv:2308.09947v1 [cs.LG])

    [http://arxiv.org/abs/2308.09947](http://arxiv.org/abs/2308.09947)

    本文研究了自动机器学习在检测心血管疾病中的有效性，提出了一个自动机器学习应用框架，并使用合并的数据集来进行研究和优化。

    

    心血管疾病在患有慢性非传染性疾病的患者中广泛存在，并且是死亡的主要原因之一，包括在劳动年龄段。本文提出了发展和应用以患者为导向的系统的相关性，其中机器学习是一种有前景的技术，可以预测心血管疾病。自动化机器学习（AutoML）可以简化和加速AI/ML应用程序的开发过程，在患者导向系统的开发中是关键的，特别是对医疗专家的应用用户。作者提出了一个自动机器学习应用框架和三种场景，允许将来自UCI机器学习仓库的五个心血管疾病指标数据集进行合并，在检测这类疾病的有效性方面进行研究。研究中调查了一个使用和优化了超参数的AutoML模型。

    Cardiovascular diseases are widespread among patients with chronic noncommunicable diseases and are one of the leading causes of death, including in the working age. The article presents the relevance of the development and application of patient-oriented systems, in which machine learning (ML) is a promising technology that allows predicting cardiovascular diseases. Automated machine learning (AutoML) makes it possible to simplify and speed up the process of developing AI/ML applications, which is key in the development of patient-oriented systems by application users, in particular medical specialists. The authors propose a framework for the application of automatic machine learning and three scenarios that allowed for data combining five data sets of cardiovascular disease indicators from the UCI Machine Learning Repository to investigate the effectiveness in detecting this class of diseases. The study investigated one AutoML model that used and optimized the hyperparameters of thir
    
[^13]: 双支路深度学习网络用于糖尿病视网膜病变的检测和分级

    Dual Branch Deep Learning Network for Detection and Stage Grading of Diabetic Retinopathy. (arXiv:2308.09945v1 [eess.IV])

    [http://arxiv.org/abs/2308.09945](http://arxiv.org/abs/2308.09945)

    这项研究介绍了一种双支路深度学习网络用于检测和分级糖尿病视网膜病变，通过利用单个眼底视网膜图像进行早期诊断和成功治疗。所提出的模型利用迁移学习和预训练模型，在大型多中心数据集上进行了训练，取得了卓越的性能，优于已有文献。

    

    糖尿病视网膜病变是糖尿病的严重并发症，如果不及时治疗可能导致永久性失明。对该疾病的早期和准确诊断对于成功治疗至关重要。本文介绍了一种深度学习方法，用于利用单个眼底视网膜图像检测和分级糖尿病视网膜病变。我们的模型采用迁移学习，使用两个最先进的预训练模型作为特征提取器，并在新的数据集上进行微调。所提出的模型在包括从公开来源获得的APTOS 2019数据集在内的大型多中心数据集上进行训练。在APTOS 2019上，它在糖尿病视网膜病变的检测和分级中表现出色，优于已有的文献。在二分类中，所提出的方法的准确率达到98.50％，敏感性达到99.46％，特异性达到97.51％。在分级中，它达到了93.00％的平方加权Kappa值，准确率还是很高的。

    Diabetic retinopathy is a severe complication of diabetes that can lead to permanent blindness if not treated promptly. Early and accurate diagnosis of the disease is essential for successful treatment. This paper introduces a deep learning method for the detection and stage grading of diabetic retinopathy, using a single fundus retinal image. Our model utilizes transfer learning, employing two state-of-the-art pre-trained models as feature extractors and fine-tuning them on a new dataset. The proposed model is trained on a large multi-center dataset, including the APTOS 2019 dataset, obtained from publicly available sources. It achieves remarkable performance in diabetic retinopathy detection and stage classification on the APTOS 2019, outperforming the established literature. For binary classification, the proposed approach achieves an accuracy of 98.50%, a sensitivity of 99.46%, and a specificity of 97.51%. In stage grading, it achieves a quadratic weighted kappa of 93.00%, an accur
    
[^14]: 关于开放世界测试时间训练的稳健性：自我训练与动态原型扩展

    On the Robustness of Open-World Test-Time Training: Self-Training with Dynamic Prototype Expansion. (arXiv:2308.09942v1 [cs.CV])

    [http://arxiv.org/abs/2308.09942](http://arxiv.org/abs/2308.09942)

    本研究提出了一种改进开放世界测试时间训练的方法，包括自适应强OOD修剪、动态原型扩展和分布对齐等技术。这些方法提高了模型在目标领域受到强OOD数据污染时的稳健性。

    

    将深度学习模型泛化到未知的目标领域分布与低延迟的问题促使了对测试时间训练/适应（TTT/TTA）的研究。现有的方法通常注重提高在经过精心策划的目标领域数据下的测试时间训练性能。在这项研究中发现，许多最先进的方法在目标领域受到强有力的外部分布（OOD）数据污染时无法保持性能，即开放世界测试时间训练（OWTTT）。这种失败主要是由于无法区分强ODD样本和常规弱OOD样本。为了提高OWTTT的稳健性，我们首先开发了自适应的强OOD修剪方法，以提高自我训练TTT方法的效能。我们进一步提出了一种动态扩展原型的方式，以表示强OOD样本，以改善弱/强OOD数据的分离。最后，我们用分布对齐来规范化自我训练，并将其与动态原型扩展相结合，实现了最先进的技术。

    Generalizing deep learning models to unknown target domain distribution with low latency has motivated research into test-time training/adaptation (TTT/TTA). Existing approaches often focus on improving test-time training performance under well-curated target domain data. As figured out in this work, many state-of-the-art methods fail to maintain the performance when the target domain is contaminated with strong out-of-distribution (OOD) data, a.k.a. open-world test-time training (OWTTT). The failure is mainly due to the inability to distinguish strong OOD samples from regular weak OOD samples. To improve the robustness of OWTTT we first develop an adaptive strong OOD pruning which improves the efficacy of the self-training TTT method. We further propose a way to dynamically expand the prototypes to represent strong OOD samples for an improved weak/strong OOD data separation. Finally, we regularize self-training with distribution alignment and the combination yields the state-of-the-ar
    
[^15]: 在线服务的多变量监测指标实用异常检测

    Practical Anomaly Detection over Multivariate Monitoring Metrics for Online Services. (arXiv:2308.09937v1 [cs.SE])

    [http://arxiv.org/abs/2308.09937](http://arxiv.org/abs/2308.09937)

    本论文提出了一个基于协同机制的多变量监测指标异常检测框架，能够高效捕捉不同指标之间的依赖关系及其历史模式，实现了在线服务的实用异常检测。

    

    随着现代软件系统在复杂性和数据量方面不断增长，对多变量监测指标进行异常检测变得越来越重要和具有挑战性。特别是不同指标之间的依赖关系及其历史模式在追求及时准确的异常检测中起着关键作用。现有方法无法高效捕捉这种信息，无法满足工业需求。为了填补这一重要差距，本文提出了一种基于协同机制的多变量监测指标异常检测框架CMAnomaly。所提出的协同机制是一种能够以线性时间复杂度捕捉特征和时间维度上的成对交互的机制。然后可以使用成本效益模型来利用监测指标之间的依赖关系和其历史模式进行异常检测。该提出的框架得到了广泛应用。

    As modern software systems continue to grow in terms of complexity and volume, anomaly detection on multivariate monitoring metrics, which profile systems' health status, becomes more and more critical and challenging. In particular, the dependency between different metrics and their historical patterns plays a critical role in pursuing prompt and accurate anomaly detection. Existing approaches fall short of industrial needs for being unable to capture such information efficiently. To fill this significant gap, in this paper, we propose CMAnomaly, an anomaly detection framework on multivariate monitoring metrics based on collaborative machine. The proposed collaborative machine is a mechanism to capture the pairwise interactions along with feature and temporal dimensions with linear time complexity. Cost-effective models can then be employed to leverage both the dependency between monitoring metrics and their historical patterns for anomaly detection. The proposed framework is extensiv
    
[^16]: BLIVA: 一个简单的多模态LLM用于更好地处理文本丰富的视觉问题

    BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions. (arXiv:2308.09936v1 [cs.CV])

    [http://arxiv.org/abs/2308.09936](http://arxiv.org/abs/2308.09936)

    BLIVA是一个简单的多模态LLM模型，用于更好地处理嵌入了文本的图像上下文场景。它通过引入InstructBLIP的查询嵌入和LLaVA的编码补丁嵌入技术来改进视觉语言模型，从而有效解决了文本丰富的视觉问题。

    

    视觉语言模型（VLM）通过整合视觉理解能力扩展了大规模语言模型（LLM），在解决开放式视觉问答（VQA）任务方面取得了显著进展。然而，这些模型无法准确解释嵌入文本的图像，这在现实场景中经常发生。从图像中提取信息的标准流程通常涉及学习一组固定的查询嵌入。这些嵌入被设计为封装图像上下文，并随后用作LLM中的软提示输入。然而，这个过程受令牌数量的限制，可能限制对文本丰富的上下文场景的识别。为了改进这一点，本研究引入了BLIVA：InstructBLIP with Visual Assistant的增强版本。BLIVA集成了来自InstructBLIP的查询嵌入，并将编码的补丁嵌入直接投影到LLM中，这是受到LLaVA的启发的一种技术。这种方法有助于处理文本丰富的视觉问题。

    Vision Language Models (VLMs), which extend Large Language Models (LLM) by incorporating visual understanding capability, have demonstrated significant advancements in addressing open-ended visual question-answering (VQA) tasks. However, these models cannot accurately interpret images infused with text, a common occurrence in real-world scenarios. Standard procedures for extracting information from images often involve learning a fixed set of query embeddings. These embeddings are designed to encapsulate image contexts and are later used as soft prompt inputs in LLMs. Yet, this process is limited to the token count, potentially curtailing the recognition of scenes with text-rich context. To improve upon them, the present study introduces BLIVA: an augmented version of InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings from InstructBLIP and also directly projects encoded patch embeddings into the LLM, a technique inspired by LLaVA. This approach assists the mode
    
[^17]: East: 高效准确的安全Transformer推理框架

    East: Efficient and Accurate Secure Transformer Framework for Inference. (arXiv:2308.09923v1 [cs.CR])

    [http://arxiv.org/abs/2308.09923](http://arxiv.org/abs/2308.09923)

    我们提出了一个名为East的框架，以实现高效准确的安全Transformer推理。我们通过设计新的忘却分段多项式求值算法来优化激活函数的运行时间和通信量。此外，我们还设计了安全协议来处理softmax和层归一化。

    

    Transformer已经成功应用于实际应用中，例如ChatGPT，由于其强大的优势。然而，在服务过程中，用户的输入会泄漏给模型提供商。随着人们对隐私的关注，隐私保护的Transformer推理在这类服务中需求量大。对于隐私保护的Transformer推理来说，非线性函数的安全协议至关重要，但研究得不多。因此，设计实用的非线性函数安全协议对于模型性能而言很困难但很重要。在这项工作中，我们提出了一个名为\emph{East}的框架，以实现高效准确的安全Transformer推理。首先，我们提出了一个新的忘却分段多项式求值算法，并应用于激活函数，与之前的方法相比，它将GELU的运行时间和通信量分别减少了1.5倍和2.5倍以上。其次，我们精心设计了用于softmax和层归一化的安全协议，以忠实地保持模型的性能。

    Transformer has been successfully used in practical applications, such as ChatGPT, due to its powerful advantages. However, users' input is leaked to the model provider during the service. With people's attention to privacy, privacy-preserving Transformer inference is on the demand of such services. Secure protocols for non-linear functions are crucial in privacy-preserving Transformer inference, which are not well studied. Thus, designing practical secure protocols for non-linear functions is hard but significant to model performance. In this work, we propose a framework \emph{East} to enable efficient and accurate secure Transformer inference. Firstly, we propose a new oblivious piecewise polynomial evaluation algorithm and apply it to the activation functions, which reduces the runtime and communication of GELU by over 1.5$\times$ and 2.5$\times$, compared to prior arts. Secondly, the secure protocols for softmax and layer normalization are carefully designed to faithfully maintain 
    
[^18]: 不再重复探索的多智能体强化学习方法

    Never Explore Repeatedly in Multi-Agent Reinforcement Learning. (arXiv:2308.09909v1 [cs.LG])

    [http://arxiv.org/abs/2308.09909](http://arxiv.org/abs/2308.09909)

    该论文提出了一种动态奖励缩放方法，以解决多智能体强化学习中的回访问题。实验结果表明，在复杂环境中，特别是在稀疏奖励设置下，该方法能够提高性能。

    

    在多智能体强化学习领域中，内在动机已经成为探索的重要工具。尽管计算许多内在奖励依赖于使用神经网络逼近器估计变分后验，但由于这些神经统计逼近器的表达能力有限，出现了一个明显的挑战。我们将这个挑战定为"回访"问题，即智能体反复探索任务空间中的有限区域。为了解决这个问题，我们提出了一种动态奖励缩放方法。这种方法旨在稳定先前探索区域内的内在奖励的明显波动，并促进更广泛的探索，有效地抑制回访现象。我们的实验结果强调了我们方法的有效性，在像Google Research Football和StarCraft II微操作任务这样的复杂环境中表现出了更好的性能，特别是在稀疏奖励设置下。

    In the realm of multi-agent reinforcement learning, intrinsic motivations have emerged as a pivotal tool for exploration. While the computation of many intrinsic rewards relies on estimating variational posteriors using neural network approximators, a notable challenge has surfaced due to the limited expressive capability of these neural statistics approximators. We pinpoint this challenge as the "revisitation" issue, where agents recurrently explore confined areas of the task space. To combat this, we propose a dynamic reward scaling approach. This method is crafted to stabilize the significant fluctuations in intrinsic rewards in previously explored areas and promote broader exploration, effectively curbing the revisitation phenomenon. Our experimental findings underscore the efficacy of our approach, showcasing enhanced performance in demanding environments like Google Research Football and StarCraft II micromanagement tasks, especially in sparse reward settings.
    
[^19]: 通过图神经网络在数据集之间填补脑部测量值

    Imputing Brain Measurements Across Data Sets via Graph Neural Networks. (arXiv:2308.09907v1 [cs.LG])

    [http://arxiv.org/abs/2308.09907](http://arxiv.org/abs/2308.09907)

    本研究提出了一种通过在另一个包含缺失测量值的数据集上进行预测任务，使用图神经网络填补跨数据集脑部测量值的方法。

    

    公开可用的结构性MRI数据集可能不包含用于训练机器学习模型的重要脑部感兴趣区域（ROI）的具体测量值。本文提出了使用深度学习方法填补缺失测量值的替代方案，将插补问题重新定义为在包含缺失测量值并与感兴趣的数据集共享某些ROI测量的另一个（公开）数据集上的预测任务。通过训练深度学习模型从共享的测量中预测缺失的测量。

    Publicly available data sets of structural MRIs might not contain specific measurements of brain Regions of Interests (ROIs) that are important for training machine learning models. For example, the curvature scores computed by Freesurfer are not released by the Adolescent Brain Cognitive Development (ABCD) Study. One can address this issue by simply reapplying Freesurfer to the data set. However, this approach is generally computationally and labor intensive (e.g., requiring quality control). An alternative is to impute the missing measurements via a deep learning approach. However, the state-of-the-art is designed to estimate randomly missing values rather than entire measurements. We therefore propose to re-frame the imputation problem as a prediction task on another (public) data set that contains the missing measurements and shares some ROI measurements with the data sets of interest. A deep learning model is then trained to predict the missing measurements from the shared ones an
    
[^20]: DPMAC：差分隐私通信在多代理强化学习中的应用

    DPMAC: Differentially Private Communication for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2308.09902v1 [cs.LG])

    [http://arxiv.org/abs/2308.09902](http://arxiv.org/abs/2308.09902)

    DPMAC算法在多代理强化学习中应用了差分隐私通信机制，通过为每个代理配备本地消息发送器并根据差分隐私要求调整消息分布，保护了个体代理的隐私信息。

    

    通信对于人类社会和多代理强化学习(MARL)中的合作至关重要。人类在与他人交流时也希望保护自己的隐私，然而这方面的考虑在现有的MARL研究中并未得到充分考虑。因此，我们提出了差分隐私多代理通信(DPMAC)算法，通过为每个代理配备具有严格(ε,δ)-差分隐私(DP)保证的本地消息发送器来保护个体代理的敏感信息。与在隐私保护场景中常用的直接给消息加入预定义的DP噪声不同，我们为每个代理采用了一个随机消息发送器，并将DP要求融入发送器中，自动调整学习到的消息分布，从而减轻DP噪声带来的不稳定性。此外，我们证明了在具有隐私保护的合作MARL中存在纳什均衡。

    Communication lays the foundation for cooperation in human society and in multi-agent reinforcement learning (MARL). Humans also desire to maintain their privacy when communicating with others, yet such privacy concern has not been considered in existing works in MARL. To this end, we propose the \textit{differentially private multi-agent communication} (DPMAC) algorithm, which protects the sensitive information of individual agents by equipping each agent with a local message sender with rigorous $(\epsilon, \delta)$-differential privacy (DP) guarantee. In contrast to directly perturbing the messages with predefined DP noise as commonly done in privacy-preserving scenarios, we adopt a stochastic message sender for each agent respectively and incorporate the DP requirement into the sender, which automatically adjusts the learned message distribution to alleviate the instability caused by DP noise. Further, we prove the existence of a Nash equilibrium in cooperative MARL with privacy-pr
    
[^21]: 基于对比学习的基于电子病历的入院死亡风险建模网络的填补预测

    Contrastive Learning-based Imputation-Prediction Networks for In-hospital Mortality Risk Modeling using EHRs. (arXiv:2308.09896v1 [cs.LG])

    [http://arxiv.org/abs/2308.09896](http://arxiv.org/abs/2308.09896)

    本文提出了一种基于对比学习的基于电子病历的填补预测网络，用于预测住院死亡风险。新方法通过引入基于图分析的患者分层建模，只使用相似患者的信息，填补缺失值和预测死亡风险。

    

    从电子病历（EHRs）预测住院死亡风险受到了很大的关注。这样的预测将为医疗专业人员提供患者健康状况的早期警告，以便及时采取干预措施。这个预测任务具有挑战性，因为EHR数据本质上是不规则的，不仅存在许多缺失值，而且医疗记录之间的时间间隔也不一致。现有方法主要集中于利用患者医疗记录中的变量相关性来填补缺失值，并建立时间衰减机制来处理这种不规则性。本文提出了一种新颖的基于对比学习的基于电子病历的填补预测网络，用于预测住院死亡风险。我们的方法在填补过程中引入了基于图分析的患者分层建模，以对相似患者进行分组。这使得只使用相似患者的信息，除了个人情境信息外。

    Predicting the risk of in-hospital mortality from electronic health records (EHRs) has received considerable attention. Such predictions will provide early warning of a patient's health condition to healthcare professionals so that timely interventions can be taken. This prediction task is challenging since EHR data are intrinsically irregular, with not only many missing values but also varying time intervals between medical records. Existing approaches focus on exploiting the variable correlations in patient medical records to impute missing values and establishing time-decay mechanisms to deal with such irregularity. This paper presents a novel contrastive learning-based imputation-prediction network for predicting in-hospital mortality risks using EHR data. Our approach introduces graph analysis-based patient stratification modeling in the imputation process to group similar patients. This allows information of similar patients only to be used, in addition to personal contextual inf
    
[^22]: 从高资源语言到低资源编程语言的知识转移用于代码LLMs

    Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs. (arXiv:2308.09895v1 [cs.PL])

    [http://arxiv.org/abs/2308.09895](http://arxiv.org/abs/2308.09895)

    本文介绍了一种有效的方法，通过使用半合成数据来提升代码LLMs在低资源语言上的性能。方法名为MultiPL-T，通过将高资源语言的训练数据转化为低资源语言的训练数据，生成高质量的低资源语言数据集。

    

    在过去几年中，代码LLMs（大规模语言模型）开始对编程实践产生重大影响。代码LLMs还成为编程语言和软件工程研究的重要组成部分。然而，代码LLMs生成的代码质量在不同编程语言之间存在显著差异。代码LLMs对训练数据充分的编程语言（如Java、Python或JavaScript）产生令人印象深刻的结果，但在像OCaml和Racket这样的低资源语言上表现困难。本文提出了一种有效的方法，通过使用半合成数据提高代码LLMs在低资源语言上的性能。我们的方法生成了高质量的低资源语言数据集，并可用于微调任何预训练的代码LLMs。我们的方法称为MultiPL-T，它将高资源语言的训练数据转化为低资源语言的训练数据。我们将该方法应用于生成十个数据集。

    Over the past few years, Large Language Models of Code (Code LLMs) have started to have a significant impact on programming practice. Code LLMs are also emerging as a building block for research in programming languages and software engineering. However, the quality of code produced by a Code LLM varies significantly by programming languages. Code LLMs produce impressive results on programming languages that are well represented in their training data (e.g., Java, Python, or JavaScript), but struggle with low-resource languages, like OCaml and Racket.  This paper presents an effective approach for boosting the performance of Code LLMs on low-resource languages using semi-synthetic data. Our approach generates high-quality datasets for low-resource languages, which can then be used to fine-tune any pretrained Code LLM. Our approach, called MultiPL-T, translates training data from high-resource languages into training data for low-resource languages. We apply our approach to generate ten
    
[^23]: 利用语义文本相似性进行临床调查数据特征选择

    Utilizing Semantic Textual Similarity for Clinical Survey Data Feature Selection. (arXiv:2308.09892v1 [cs.CL])

    [http://arxiv.org/abs/2308.09892](http://arxiv.org/abs/2308.09892)

    利用语义文本相似性对临床调查数据进行特征选择，通过评估特征名称与目标名称之间的关系来选择特征。

    

    调查数据可能包含大量特征，而示例数量相对较低。在这种情况下，试图从调查数据预测结果的机器学习模型可能会过拟合，导致泛化能力较差。解决这个问题的一种方法是特征选择，它试图选择一个最佳的特征子集进行学习。特征选择过程中一个相对未被探索的信息来源是特征的文本名称，这可能在语义上指示哪些特征与目标结果相关。可以使用语言模型（LMs）评估特征名称和目标名称之间的关系，以生成语义文本相似性（STS）分数，然后可以使用这些分数来选择特征。我们研究了使用STS直接选择特征和在最小冗余最大相关（mRMR）算法中的性能。根据初步调查的结果评估了STS作为特征选择指标的性能。

    Survey data can contain a high number of features while having a comparatively low quantity of examples. Machine learning models that attempt to predict outcomes from survey data under these conditions can overfit and result in poor generalizability. One remedy to this issue is feature selection, which attempts to select an optimal subset of features to learn upon. A relatively unexplored source of information in the feature selection process is the usage of textual names of features, which may be semantically indicative of which features are relevant to a target outcome. The relationships between feature names and target names can be evaluated using language models (LMs) to produce semantic textual similarity (STS) scores, which can then be used to select features. We examine the performance using STS to select features directly and in the minimal-redundancy-maximal-relevance (mRMR) algorithm. The performance of STS as a feature selection metric is evaluated against preliminary survey
    
[^24]: 归纳偏差学习: 用大语言模型生成代码模型

    Inductive-bias Learning: Generating Code Models with Large Language Model. (arXiv:2308.09890v1 [cs.LG])

    [http://arxiv.org/abs/2308.09890](http://arxiv.org/abs/2308.09890)

    这篇论文提出了一种新的学习方法，称为归纳偏差学习（IBL），它将上下文学习（ICL）和代码生成相结合，通过输入训练数据到提示中，输出相应的代码。

    

    大型语言模型(LLMs)因其在上下文学习(ICL)方面的能力而受到关注。ICL技术在不更新LLM参数的情况下，仅通过输入训练数据到提示中即可实现基于规则的高准确性推理。虽然ICL是一个发展中的领域，还有许多未解答的问题，但LLMs本身作为推理模型似乎实现了不需要明确指出"归纳偏差"的推理。另一方面，代码生成也是LLMs的一项重要应用。代码生成的准确性大大提高，使得即使非工程师也可以通过精心设计的提示来生成执行所需任务的代码。本文提出了一种新颖的“学习”方法，称为“归纳偏差学习（IBL）”，它结合了ICL和代码生成的技术。IBL的思想很直观。与ICL类似，IBL将训练数据输入到提示中，并输出相应的代码。

    Large Language Models(LLMs) have been attracting attention due to a ability called in-context learning(ICL). ICL, without updating the parameters of a LLM, it is possible to achieve highly accurate inference based on rules ``in the context'' by merely inputting a training data into the prompt. Although ICL is a developing field with many unanswered questions, LLMs themselves serves as a inference model, seemingly realizing inference without explicitly indicate ``inductive bias''. On the other hand, a code generation is also a highlighted application of LLMs. The accuracy of code generation has dramatically improved, enabling even non-engineers to generate code to perform the desired tasks by crafting appropriate prompts. In this paper, we propose a novel ``learning'' method called an ``Inductive-Bias Learning (IBL)'', which combines the techniques of ICL and code generation. An idea of IBL is straightforward. Like ICL, IBL inputs a training data into the prompt and outputs a code with 
    
[^25]: DUAW：针对稳定扩散定制的无数据通用对抗水印

    DUAW: Data-free Universal Adversarial Watermark against Stable Diffusion Customization. (arXiv:2308.09889v1 [cs.CV])

    [http://arxiv.org/abs/2308.09889](http://arxiv.org/abs/2308.09889)

    这项研究提出了一种无数据通用对抗水印（DUAW），通过干扰定制过程中的变分自动编码器，保护各种版本的SD模型中的众多受版权保护的图像。这种方法避免了直接处理受版权保护的图像的必要性，保护图像的保密性。

    

    稳定扩散（SD）定制方法使用户能够个性化SD模型的输出，极大增强了AI艺术的灵活性和多样性。然而，它们也允许个人从受版权保护的图像中抄袭特定的风格或主题，这引起了对潜在版权侵权的重大关注。为了解决这个问题，我们提出了一种无形的无数据通用对抗水印（DUAW），旨在保护各种版本的SD模型中的众多受版权保护的图像。首先，DUAW被设计用于干扰SD定制过程中的变分自动编码器。其次，DUAW在无数据的环境下操作，它在大型语言模型（LLM）和预训练的SD模型产生的合成图像上进行训练。这种方法避免了直接处理受版权保护的图像的必要性，从而保护它们的保密性。一旦制作完成，DUAW可以无法察觉地集成到大量的受版权保护的图像中。

    Stable Diffusion (SD) customization approaches enable users to personalize SD model outputs, greatly enhancing the flexibility and diversity of AI art. However, they also allow individuals to plagiarize specific styles or subjects from copyrighted images, which raises significant concerns about potential copyright infringement. To address this issue, we propose an invisible data-free universal adversarial watermark (DUAW), aiming to protect a myriad of copyrighted images from different customization approaches across various versions of SD models. First, DUAW is designed to disrupt the variational autoencoder during SD customization. Second, DUAW operates in a data-free context, where it is trained on synthetic images produced by a Large Language Model (LLM) and a pretrained SD model. This approach circumvents the necessity of directly handling copyrighted images, thereby preserving their confidentiality. Once crafted, DUAW can be imperceptibly integrated into massive copyrighted image
    
[^26]: 关于贝叶斯实验设计中期望信息增益梯度的估计

    On Estimating the Gradient of the Expected Information Gain in Bayesian Experimental Design. (arXiv:2308.09888v1 [stat.ML])

    [http://arxiv.org/abs/2308.09888](http://arxiv.org/abs/2308.09888)

    该论文提出了一种估计贝叶斯实验设计中期望信息增益梯度的方法，通过结合随机梯度下降算法，实现了高效优化。具体而言，通过后验期望表示来估计与设计变量相关的梯度，并提出了UEEG-MCMC和BEEG-AP两种估计方法。这些方法在不同的实验设计问题上都表现出良好的性能。

    

    贝叶斯实验设计旨在找到贝叶斯推断的最佳实验条件，通常被描述为优化期望信息增益（EIG）。为了高效地优化EIG，往往需要梯度信息，因此估计EIG的梯度能力对于贝叶斯实验设计问题至关重要。该工作的主要目标是开发估计EIG梯度的方法，结合随机梯度下降算法，实现EIG的高效优化。具体而言，我们首先介绍了与设计变量相关的EIG梯度的后验期望表示。基于此，我们提出了两种估计EIG梯度的方法，UEEG-MCMC利用通过马尔科夫链蒙特卡洛（MCMC）生成的后验样本来估计EIG梯度，而BEEG-AP则专注于通过反复使用参数样本来实现高模拟效率。理论分析和数值实验表明，我们的方法在不同的实验设计问题上都能获得较好的性能。

    Bayesian Experimental Design (BED), which aims to find the optimal experimental conditions for Bayesian inference, is usually posed as to optimize the expected information gain (EIG). The gradient information is often needed for efficient EIG optimization, and as a result the ability to estimate the gradient of EIG is essential for BED problems. The primary goal of this work is to develop methods for estimating the gradient of EIG, which, combined with the stochastic gradient descent algorithms, result in efficient optimization of EIG. Specifically, we first introduce a posterior expected representation of the EIG gradient with respect to the design variables. Based on this, we propose two methods for estimating the EIG gradient, UEEG-MCMC that leverages posterior samples generated through Markov Chain Monte Carlo (MCMC) to estimate the EIG gradient, and BEEG-AP that focuses on achieving high simulation efficiency by repeatedly using parameter samples. Theoretical analysis and numerica
    
[^27]: 半监督人群计数中的不确定性校准

    Calibrating Uncertainty for Semi-Supervised Crowd Counting. (arXiv:2308.09887v1 [cs.CV])

    [http://arxiv.org/abs/2308.09887](http://arxiv.org/abs/2308.09887)

    本文提出了一种新颖的方法来校准人群计数模型的不确定性，通过有监督的不确定性估计策略和匹配基准的替代函数，实现了在半监督人群计数中可靠的不确定性估计和高质量的伪标签生成，并取得了最先进的性能。

    

    半监督人群计数是一项重要而具有挑战性的任务。一种常用的方法是通过迭代生成伪标签并将其添加到训练集中来进行学习。关键是使用不确定性来选择可靠的伪标签。在本文中，我们提出了一种新颖的方法来校准人群计数模型的不确定性。我们的方法采用有监督的不确定性估计策略通过一个替代函数来训练模型。这确保了在整个训练过程中不确定性得到良好的控制。我们提出了一种基于匹配的以补丁为单位的替代函数，以更好地近似人群计数任务的不确定性。所提出的方法注重细节，同时保持适当的细粒度。总的来说，我们的方法能够生成可靠的不确定性估计、高质量的伪标签，并在半监督人群计数中取得最先进的性能。

    Semi-supervised crowd counting is an important yet challenging task. A popular approach is to iteratively generate pseudo-labels for unlabeled data and add them to the training set. The key is to use uncertainty to select reliable pseudo-labels. In this paper, we propose a novel method to calibrate model uncertainty for crowd counting. Our method takes a supervised uncertainty estimation strategy to train the model through a surrogate function. This ensures the uncertainty is well controlled throughout the training. We propose a matching-based patch-wise surrogate function to better approximate uncertainty for crowd counting tasks. The proposed method pays a sufficient amount of attention to details, while maintaining a proper granularity. Altogether our method is able to generate reliable uncertainty estimation, high quality pseudolabels, and achieve state-of-the-art performance in semisupervised crowd counting.
    
[^28]: 基于Transformer的多元时间序列框架：剩余寿命预测应用案例

    A Transformer-based Framework For Multi-variate Time Series: A Remaining Useful Life Prediction Use Case. (arXiv:2308.09884v1 [cs.LG])

    [http://arxiv.org/abs/2308.09884](http://arxiv.org/abs/2308.09884)

    这项研究提出了一种基于Transformer的框架，用于多元时间序列预测，以满足预测学应用案例中剩余寿命预测的需求。

    

    最近，大型语言模型（LLMs）吸引了全球关注，并彻底改变了自然语言处理领域。LLMs的有效性可归因于其用于训练的模型架构，即transformers。Transformer模型在捕捉顺序数据中的上下文特征方面表现出色，由于时间序列数据是顺序的，可以利用Transformer模型实现更有效的时间序列数据预测。预测学领域对系统健康管理和适当的维护计划至关重要。对机器剩余可用寿命（RUL）进行可靠估计具有节省成本的潜力。这包括避免机器突然故障，最大限度地利用设备，并作为决策支持系统（DSS）。这项工作提出了一种基于编码器-Transformer架构的多元时间序列预测框架，用于预测学应用案例。我们验证了该框架的有效性。

    In recent times, Large Language Models (LLMs) have captured a global spotlight and revolutionized the field of Natural Language Processing. One of the factors attributed to the effectiveness of LLMs is the model architecture used for training, transformers. Transformer models excel at capturing contextual features in sequential data since time series data are sequential, transformer models can be leveraged for more efficient time series data prediction. The field of prognostics is vital to system health management and proper maintenance planning. A reliable estimation of the remaining useful life (RUL) of machines holds the potential for substantial cost savings. This includes avoiding abrupt machine failures, maximizing equipment usage, and serving as a decision support system (DSS). This work proposed an encoder-transformer architecture-based framework for multivariate time series prediction for a prognostics use case. We validated the effectiveness of the proposed framework on all f
    
[^29]: Flamingo: 多轮单服务器安全聚合及其在私有联邦学习中的应用

    Flamingo: Multi-Round Single-Server Secure Aggregation with Applications to Private Federated Learning. (arXiv:2308.09883v1 [cs.CR])

    [http://arxiv.org/abs/2308.09883](http://arxiv.org/abs/2308.09883)

    Flamingo是一个用于实现跨大量客户端安全聚合的系统，在私有联邦学习中有广泛的应用。通过消除每轮设置和引入轻量级的丢失容忍协议，Flamingo解决了以往协议在多轮设置下的问题，并引入了新的本地选择客户端邻域的方式。

    

    本文介绍了Flamingo，这是一个用于跨大量客户端安全聚合数据的系统。在安全聚合中，服务器对客户端的私有输入进行求和，并在不了解个体输入的情况下得到结果，仅能推断出最终总和。Flamingo专注于联邦学习中的多轮设置，其中执行多个连续的模型权重求和（平均），以得到一个良好的模型。之前的协议（例如Bell等人的CCS '20）仅适用于单轮，并通过多次重复该协议来适应联邦学习的设置。Flamingo消除了之前协议每轮设置的需求，并引入了一种新的轻量级的丢失容忍协议，以确保如果客户端在求和过程中离开，服务器仍然可以获得有意义的结果。此外，Flamingo还引入了一种新的本地选择所谓的客户端邻域的方式，此概念由Bell等人提出。

    This paper introduces Flamingo, a system for secure aggregation of data across a large set of clients. In secure aggregation, a server sums up the private inputs of clients and obtains the result without learning anything about the individual inputs beyond what is implied by the final sum. Flamingo focuses on the multi-round setting found in federated learning in which many consecutive summations (averages) of model weights are performed to derive a good model. Previous protocols, such as Bell et al. (CCS '20), have been designed for a single round and are adapted to the federated learning setting by repeating the protocol multiple times. Flamingo eliminates the need for the per-round setup of previous protocols, and has a new lightweight dropout resilience protocol to ensure that if clients leave in the middle of a sum the server can still obtain a meaningful result. Furthermore, Flamingo introduces a new way to locally choose the so-called client neighborhood introduced by Bell et al
    
[^30]: 生成对抗网络的反学习

    Generative Adversarial Networks Unlearning. (arXiv:2308.09881v1 [cs.LG])

    [http://arxiv.org/abs/2308.09881](http://arxiv.org/abs/2308.09881)

    本文介绍了一种生成对抗网络的反学习方法，通过引入替代机制和定义虚假标签来解决生成器反学习带来的连续性和完整性问题。

    

    随着机器学习的不断发展，以及数据滥用丑闻变得更加普遍，人们对个人信息变得越来越关注，并倡导有权利删除自己的数据。机器反学习已经成为从经过训练的机器学习模型中擦除训练数据的解决方案。尽管在分类器方面取得了成功，但由于其包括生成器和判别器的独特架构，对生成对抗网络（GAN）的研究还存在限制。其中一个挑战是生成器的反学习，因为该过程可能会扰乱潜在空间的连续性和完整性。这种扰乱可能会在反学习后降低模型的有效性。另一个挑战是如何为反学习图像定义判别器应执行的标准。在本文中，我们引入了一种替代机制，并定义了一个虚假标签来有效缓解这些挑战。

    As machine learning continues to develop, and data misuse scandals become more prevalent, individuals are becoming increasingly concerned about their personal information and are advocating for the right to remove their data. Machine unlearning has emerged as a solution to erase training data from trained machine learning models. Despite its success in classifiers, research on Generative Adversarial Networks (GANs) is limited due to their unique architecture, including a generator and a discriminator. One challenge pertains to generator unlearning, as the process could potentially disrupt the continuity and completeness of the latent space. This disruption might consequently diminish the model's effectiveness after unlearning. Another challenge is how to define a criterion that the discriminator should perform for the unlearning images. In this paper, we introduce a substitution mechanism and define a fake label to effectively mitigate these challenges. Based on the substitution mechan
    
[^31]: DatasetEquity: 所有样本是否平等？在数据集中追求公平性。

    DatasetEquity: Are All Samples Created Equal? In The Quest For Equity Within Datasets. (arXiv:2308.09878v1 [cs.CV])

    [http://arxiv.org/abs/2308.09878](http://arxiv.org/abs/2308.09878)

    该论文提出了一种新的方法来解决机器学习中数据不平衡的问题，通过使用深层感知嵌入和聚类来计算样本的似然性，并使用广义聚焦损失函数在训练过程中对样本进行不同的加权。实验验证了该方法的有效性。

    

    数据不平衡是机器学习领域中一个众所周知的问题，归因于数据收集的成本、标签的困难以及数据的地理分布。在计算机视觉中，图像外观引起的数据分布偏差仍未得到很好的研究。与使用类别标签的分类分布相比，图像外观展示了超出类别标签所提供的对象之间的复杂关系。通过对从原始像素中提取的深层感知特征进行聚类，可以获得更丰富的数据表示。本文提出了一种新的解决机器学习中数据不平衡问题的方法。该方法使用基于图像外观的深层感知嵌入和聚类计算样本的似然性，然后利用这些似然性在训练过程中对样本进行不同的加权，使用提出的\textbf{广义聚焦损失}函数。该损失函数可以很容易地与深度学习算法集成。实验验证了该方法的有效性。

    Data imbalance is a well-known issue in the field of machine learning, attributable to the cost of data collection, the difficulty of labeling, and the geographical distribution of the data. In computer vision, bias in data distribution caused by image appearance remains highly unexplored. Compared to categorical distributions using class labels, image appearance reveals complex relationships between objects beyond what class labels provide. Clustering deep perceptual features extracted from raw pixels gives a richer representation of the data. This paper presents a novel method for addressing data imbalance in machine learning. The method computes sample likelihoods based on image appearance using deep perceptual embeddings and clustering. It then uses these likelihoods to weigh samples differently during training with a proposed \textbf{Generalized Focal Loss} function. This loss can be easily integrated with deep learning algorithms. Experiments validate the method's effectiveness a
    
[^32]: Skill Transformer: 移动操作的单体策略

    Skill Transformer: A Monolithic Policy for Mobile Manipulation. (arXiv:2308.09873v1 [cs.LG])

    [http://arxiv.org/abs/2308.09873](http://arxiv.org/abs/2308.09873)

    Skill Transformer是一个解决长时间跨度机器人任务的方法，通过结合条件序列建模和技能模块化，它能够在新场景中实现强大的任务规划和低级控制能力，相比基准模型，成功率提高了2.5倍。

    

    我们提出了Skill Transformer，一种通过结合条件序列建模和技能模块化来解决长时间跨度机器人任务的方法。在机器人的自我中心和感知观察的条件下，Skill Transformer被端到端地训练以预测高级技能（如导航、拾取、放置）和整体低级动作（如底盘和手臂运动），采用transformer架构和解决完整任务的示范轨迹。通过技能预测模块，它保持了整体任务的组合性和模块性，同时推理低级动作并避免了模块方法中常见的交接错误。我们在一个基于实体的重新排列基准测试上对Skill Transformer进行了测试，并发现它在新场景中表现出强大的任务规划和低级控制能力，在困难的重新排列问题上的成功率比基准模型高出2.5倍。

    We present Skill Transformer, an approach for solving long-horizon robotic tasks by combining conditional sequence modeling and skill modularity. Conditioned on egocentric and proprioceptive observations of a robot, Skill Transformer is trained end-to-end to predict both a high-level skill (e.g., navigation, picking, placing), and a whole-body low-level action (e.g., base and arm motion), using a transformer architecture and demonstration trajectories that solve the full task. It retains the composability and modularity of the overall task through a skill predictor module while reasoning about low-level actions and avoiding hand-off errors, common in modular approaches. We test Skill Transformer on an embodied rearrangement benchmark and find it performs robust task planning and low-level control in new scenarios, achieving a 2.5x higher success rate than baselines in hard rearrangement problems.
    
[^33]: 张量压缩的反向传播免费训练（物理信息）的神经网络

    Tensor-Compressed Back-Propagation-Free Training for (Physics-Informed) Neural Networks. (arXiv:2308.09858v1 [cs.LG])

    [http://arxiv.org/abs/2308.09858](http://arxiv.org/abs/2308.09858)

    本文提出了一个完全无需反向传播的神经网络训练框架，并通过张量压缩的方差约减方法和混合梯度评估方法改进了优化和效率。同时，还扩展了框架用于物理信息的神经网络的估计。

    

    反向传播（BP）被广泛用于神经网络训练中计算梯度。然而，由于缺乏硬件和软件资源来支持自动微分，在边缘设备上实现BP是困难的。这大大增加了设备上训练加速器的设计复杂性和上市时间。本文提出了一个完全无需BP的框架，只需要前向传播就可以训练实际的神经网络。我们的技术贡献有三个方面。首先，我们提出了一种张量压缩的方差约减方法，极大提高了零阶（ZO）优化的可扩展性，使其能够处理大于以前ZO方法能力的网络尺寸。其次，我们提出了一种混合梯度评估方法，提高了ZO训练的效率。最后，我们通过提出一种稀疏格方法来扩展我们的BP-free训练框架，用于物理信息的神经网络（PINNs）的估计。

    Backward propagation (BP) is widely used to compute the gradients in neural network training. However, it is hard to implement BP on edge devices due to the lack of hardware and software resources to support automatic differentiation. This has tremendously increased the design complexity and time-to-market of on-device training accelerators. This paper presents a completely BP-free framework that only requires forward propagation to train realistic neural networks. Our technical contributions are three-fold. Firstly, we present a tensor-compressed variance reduction approach to greatly improve the scalability of zeroth-order (ZO) optimization, making it feasible to handle a network size that is beyond the capability of previous ZO approaches. Secondly, we present a hybrid gradient evaluation approach to improve the efficiency of ZO training. Finally, we extend our BP-free training framework to physics-informed neural networks (PINNs) by proposing a sparse-grid approach to estimate the 
    
[^34]: 通过修复神经激活的分布来减轻后门攻击

    Backdoor Mitigation by Correcting the Distribution of Neural Activations. (arXiv:2308.09850v1 [cs.LG])

    [http://arxiv.org/abs/2308.09850](http://arxiv.org/abs/2308.09850)

    本文揭示了后门攻击的一个重要特性：成功的攻击会改变内部层激活的分布，我们提出了一种通过纠正分布改变实现训练后的后门减缓的方法。

    

    后门（木马）攻击是针对深度神经网络的一种重要的对抗性攻击方式，当攻击者的后门触发器出现时，测试实例会被（错误）分类为攻击者的目标类。在本文中，我们揭示并分析了后门攻击的一个重要特性：成功的攻击会导致后门触发实例内部层激活的分布发生改变，与干净实例的分布不同。更重要的是，我们发现如果纠正了这种分布改变，带有后门触发器的实例将正确分类为它们原始的源类。基于我们的观察，我们提出了一种高效有效的方法，通过使用反向工程的触发器来通过纠正分布改变实现训练后的后门减轻。值得注意的是，我们的方法不改变DNN的任何可训练参数，但是相比现有方法，具有更好的减轻性能。

    Backdoor (Trojan) attacks are an important type of adversarial exploit against deep neural networks (DNNs), wherein a test instance is (mis)classified to the attacker's target class whenever the attacker's backdoor trigger is present. In this paper, we reveal and analyze an important property of backdoor attacks: a successful attack causes an alteration in the distribution of internal layer activations for backdoor-trigger instances, compared to that for clean instances. Even more importantly, we find that instances with the backdoor trigger will be correctly classified to their original source classes if this distribution alteration is corrected. Based on our observations, we propose an efficient and effective method that achieves post-training backdoor mitigation by correcting the distribution alteration using reverse-engineered triggers. Notably, our method does not change any trainable parameters of the DNN, but achieves generally better mitigation performance than existing methods
    
[^35]: 用可证明概率保证在深度神经网络中枚举安全区域

    Enumerating Safe Regions in Deep Neural Networks with Provable Probabilistic Guarantees. (arXiv:2308.09842v1 [cs.LG])

    [http://arxiv.org/abs/2308.09842](http://arxiv.org/abs/2308.09842)

    通过epsilon-ProVe方法，我们提出了一种高效近似的方法来枚举深度神经网络中的安全区域，并提供了可证明概率保证的紧密下估计。

    

    识别安全区域是保证基于深度神经网络（DNNs）系统的信任的关键点。为此，我们引入了AllDNN-Verification问题：给定一个安全属性和一个DNN，枚举属性输入域的所有安全区域，即属性成立的区域。由于问题的#P难度，我们提出了一种高效的近似方法叫做epsilon-ProVe。我们的方法通过统计预测容限限制获得可控低估的输出可达集，并能够提供一个具有可证明概率保证的安全区域的紧密下估计。我们在不同的标准基准测试上进行的实证评估显示了我们方法的可扩展性和有效性，为这种新型的DNN验证提供了有价值的见解。

    Identifying safe areas is a key point to guarantee trust for systems that are based on Deep Neural Networks (DNNs). To this end, we introduce the AllDNN-Verification problem: given a safety property and a DNN, enumerate the set of all the regions of the property input domain which are safe, i.e., where the property does hold. Due to the #P-hardness of the problem, we propose an efficient approximation method called epsilon-ProVe. Our approach exploits a controllable underestimation of the output reachable sets obtained via statistical prediction of tolerance limits, and can provide a tight (with provable probabilistic guarantees) lower estimate of the safe areas. Our empirical evaluation on different standard benchmarks shows the scalability and effectiveness of our method, offering valuable insights for this new type of verification of DNNs.
    
[^36]: 通过点和形状正则化的数据合成进行显微图像分割

    Microscopy Image Segmentation via Point and Shape Regularized Data Synthesis. (arXiv:2308.09835v1 [cs.CV])

    [http://arxiv.org/abs/2308.09835](http://arxiv.org/abs/2308.09835)

    本文提出了一种采用合成训练数据的统一流程，通过点注释和形状先验进行显微图像分割。该方法克服了标注成本高的问题，且仍能提供关键信息用于分割。该流程包括三个阶段：获取点注释并生成伪密集分割掩码，将伪掩码转化为真实显微图像，并通过对象级一致性进行正则化。

    

    当前基于深度学习的显微图像分割方法严重依赖于大量需要密集注释的训练数据，在实践中成本高且劳动密集。与完整标注所描述的对象的完整轮廓相比，点注释，特别是对象质心，更容易获取，并且仍然为后续分割提供关键信息。本文假设仅在训练期间有点注释，并开发了一个使用合成训练数据的统一流程进行显微图像分割的框架。我们的框架包括三个阶段：（1）获取点注释并使用形状先验约束采样一个伪密集分割掩码；（2）通过以非配对的方式训练的图像生成模型，将伪掩码转化为真实显微镜图像，并通过对象级一致性进行正则化；（3）伪掩码和合成图像共同构成了训练集。

    Current deep learning-based approaches for the segmentation of microscopy images heavily rely on large amount of training data with dense annotation, which is highly costly and laborious in practice. Compared to full annotation where the complete contour of objects is depicted, point annotations, specifically object centroids, are much easier to acquire and still provide crucial information about the objects for subsequent segmentation. In this paper, we assume access to point annotations only during training and develop a unified pipeline for microscopy image segmentation using synthetically generated training data. Our framework includes three stages: (1) it takes point annotations and samples a pseudo dense segmentation mask constrained with shape priors; (2) with an image generative model trained in an unpaired manner, it translates the mask to a realistic microscopy image regularized by object level consistency; (3) the pseudo masks along with the synthetic images then constitute 
    
[^37]: 学习自单一图形足以在无线网络中实现近最短路径路由

    Learning from A Single Graph is All You Need for Near-Shortest Path Routing in Wireless Networks. (arXiv:2308.09829v1 [cs.LG])

    [http://arxiv.org/abs/2308.09829](http://arxiv.org/abs/2308.09829)

    本文提出了一种学习算法，通过训练DNNs学习本地路由策略，只需从单个图形获得少量数据样本，即可推广到所有随机图形的无线网络。实验表明，我们的算法在性能上能够与贪婪转发相匹配或超越，并且通过利用网络领域知识，选择适当的输入特征和图形子采样，实现了高效、可扩展和可推广的学习。

    

    我们提出了一种学习算法，用于本地路由策略，它只需要从单个图形中获得的少量数据样本，同时能够推广到标准模型中的所有随机图形的无线网络。因此，我们通过训练深度神经网络（DNNs）来解决全局近最短路径问题，这些DNNs能够高效且可扩展地学习仅考虑节点状态和相邻节点状态的本地路由策略。值得注意的是，我们训练的其中一个DNN学习的策略完全符合贪婪转发的性能，另一个则普遍优于贪婪转发。我们的算法设计从多个方面充分利用了网络领域知识：首先，在选择输入特征时；其次，在选择“种子图”及其最短路径的子采样时。领域知识的利用为学习提供了理论解释，说明了为什么种子图和节点子采样足以实现有效、可扩展和可推广的学习。模拟实验验证了我们算法的性能，并与其他方法进行了比较。

    We propose a learning algorithm for local routing policies that needs only a few data samples obtained from a single graph while generalizing to all random graphs in a standard model of wireless networks. We thus solve the all-pairs near-shortest path problem by training deep neural networks (DNNs) that efficiently and scalably learn routing policies that are local, i.e., they only consider node states and the states of neighboring nodes. Remarkably, one of these DNNs we train learns a policy that exactly matches the performance of greedy forwarding; another generally outperforms greedy forwarding. Our algorithm design exploits network domain knowledge in several ways: First, in the selection of input features and, second, in the selection of a ``seed graph'' and subsamples from its shortest paths. The leverage of domain knowledge provides theoretical explainability of why the seed graph and node subsampling suffice for learning that is efficient, scalable, and generalizable. Simulatio
    
[^38]: VL-PET：通过粒度控制实现视觉和语言参数高效调整

    VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity Control. (arXiv:2308.09804v1 [cs.CV])

    [http://arxiv.org/abs/2308.09804](http://arxiv.org/abs/2308.09804)

    本文提出了一种名为VL-PET的视觉和语言参数高效调整框架，通过粒度控制机制对模块化修改进行有效控制，克服了现有技术在性能和功能差距方面的不足。

    

    随着预训练语言模型（PLM）的模型规模迅速增长，全面微调在模型训练和存储方面变得代价高昂。在视觉与语言（VL）中，提出了参数高效调整（PET）技术，将模块化修改（例如Adapter和LoRA）集成到编码器-解码器PLMs中。通过调整一小组可训练参数，这些技术的性能与全面微调相当。然而，过度的模块化修改和忽视编码器和解码器之间的功能差距可能导致性能降低，而现有的PET技术（例如VL-Adapter）忽视了这些关键问题。在本文中，我们提出了一种名为Vision-and-Language Parameter-Efficient Tuning（VL-PET）的框架，通过一种新颖的粒度控制机制对模块化修改进行有效控制。通过考虑由这种机制生成的不同粒度控制矩阵，可以实例化多种与模型无关的VL-PET模块。

    As the model size of pre-trained language models (PLMs) grows rapidly, full fine-tuning becomes prohibitively expensive for model training and storage. In vision-and-language (VL), parameter-efficient tuning (PET) techniques are proposed to integrate modular modifications (e.g., Adapter and LoRA) into encoder-decoder PLMs. By tuning a small set of trainable parameters, these techniques perform on par with full fine-tuning. However, excessive modular modifications and neglecting the functionality gap between the encoders and decoders can lead to performance degradation, while existing PET techniques (e.g., VL-Adapter) overlook these critical issues. In this paper, we propose a Vision-and-Language Parameter-Efficient Tuning (VL-PET) framework to impose effective control over modular modifications via a novel granularity-controlled mechanism. Considering different granularity-controlled matrices generated by this mechanism, a variety of model-agnostic VL-PET modules can be instantiated fr
    
[^39]: 一种基于二进制Horse Herd优化算法的高维基因选择方法用于生物数据分类

    An Efficient High-Dimensional Gene Selection Approach based on Binary Horse Herd Optimization Algorithm for Biological Data Classification. (arXiv:2308.09791v1 [cs.LG])

    [http://arxiv.org/abs/2308.09791](http://arxiv.org/abs/2308.09791)

    本文提出了一种基于二进制Horse Herd优化算法的高维基因选择方法，通过引入新的混合特征选择框架和转换函数，能够更高效地选择相关且信息丰富的特征子集，并在微阵列数据集上取得了良好的效果。

    

    Horse Herd优化算法是一种基于不同年龄的马的行为的新型元启发式算法。最近，HOA被引入来解决复杂和高维问题。本文提出了Horse Herd优化算法的二进制版本（BHOA），以解决离散问题和选择显著特征子集。此外，本研究提供了一种基于BHOA和最小冗余最大相关（MRMR）过滤方法的新型混合特征选择框架。这种更高效的混合特征选择能够产生一组相关且信息丰富的有益特征子集。由于特征选择是一个二进制问题，我们应用了一种新的转换函数（TF），称为X形状TF，将连续问题转换为二进制搜索空间。此外，支持向量机（SVM）被用于检验所提出方法在十个微阵列数据集（即淋巴瘤、前列腺等）上的效率。

    The Horse Herd Optimization Algorithm (HOA) is a new meta-heuristic algorithm based on the behaviors of horses at different ages. The HOA was introduced recently to solve complex and high-dimensional problems. This paper proposes a binary version of the Horse Herd Optimization Algorithm (BHOA) in order to solve discrete problems and select prominent feature subsets. Moreover, this study provides a novel hybrid feature selection framework based on the BHOA and a minimum Redundancy Maximum Relevance (MRMR) filter method. This hybrid feature selection, which is more computationally efficient, produces a beneficial subset of relevant and informative features. Since feature selection is a binary problem, we have applied a new Transfer Function (TF), called X-shape TF, which transforms continuous problems into binary search spaces. Furthermore, the Support Vector Machine (SVM) is utilized to examine the efficiency of the proposed method on ten microarray datasets, namely Lymphoma, Prostate, 
    
[^40]: 一种用于描述A/B测试中网络干扰的两部分机器学习方法

    A Two-Part Machine Learning Approach to Characterizing Network Interference in A/B Testing. (arXiv:2308.09790v1 [stat.ML])

    [http://arxiv.org/abs/2308.09790](http://arxiv.org/abs/2308.09790)

    本论文提出了一种两部分机器学习方法，用于识别和描述A/B测试中的网络干扰。通过考虑潜在的复杂网络结构和建立适合的曝光映射，该方法在合成实验和真实大规模测试中的模拟中表现优于传统方法。

    

    受网络干扰现象的影响，控制实验或"A/B测试"的可靠性通常会受到损害。为了解决这个问题，我们提出了一种基于机器学习的方法来识别和描述异质网络干扰。我们的方法考虑了潜在的复杂网络结构，并自动化了"曝光映射"确定的任务，从而解决了现有文献中的两个主要限制。我们引入了"因果网络模式"，并采用透明的机器学习模型来建立最适合反映潜在网络干扰模式的曝光映射。我们的方法通过在两个合成实验和一个涉及100-200万Instagram用户的真实大规模测试中的模拟得到了验证，表现优于传统方法，如基于设计的集群随机化和基于分析的邻域曝光映射。

    The reliability of controlled experiments, or "A/B tests," can often be compromised due to the phenomenon of network interference, wherein the outcome for one unit is influenced by other units. To tackle this challenge, we propose a machine learning-based method to identify and characterize heterogeneous network interference. Our approach accounts for latent complex network structures and automates the task of "exposure mapping'' determination, which addresses the two major limitations in the existing literature. We introduce "causal network motifs'' and employ transparent machine learning models to establish the most suitable exposure mapping that reflects underlying network interference patterns. Our method's efficacy has been validated through simulations on two synthetic experiments and a real-world, large-scale test involving 1-2 million Instagram users, outperforming conventional methods such as design-based cluster randomization and analysis-based neighborhood exposure mapping. 
    
[^41]: 基于事件的动态图表示学习在专利申请趋势预测中的应用

    Event-based Dynamic Graph Representation Learning for Patent Application Trend Prediction. (arXiv:2308.09780v1 [cs.AI])

    [http://arxiv.org/abs/2308.09780](http://arxiv.org/abs/2308.09780)

    本研究提出了一种基于事件的动态图学习框架，用于准确预测专利申请趋势。该方法利用公司和分类代码的可记忆表示，通过历史记忆和当前信息更新来捕捉语义接近性。

    

    准确预测公司在未来一段时间内将申请哪些类型的专利能够揭示出它们的发展战略，并帮助其提前发现潜在的合作伙伴或竞争对手。然而，由于对公司不断变化的偏好和对分类代码的语义关联的建模困难，这个问题在之前的研究中鲜有涉及。为了弥补这一空白，我们提出了一种基于事件的动态图学习框架，用于预测专利申请趋势。具体而言，我们的方法建立在公司和专利分类代码的可记忆表示基础上。当观察到一个新的专利时，相关公司和分类代码的表示根据历史记忆和当前编码的信息进行更新。此外，提供了一个层次化消息传递机制，以捕捉专利分类代码的语义接近性。

    Accurate prediction of what types of patents that companies will apply for in the next period of time can figure out their development strategies and help them discover potential partners or competitors in advance. Although important, this problem has been rarely studied in previous research due to the challenges in modelling companies' continuously evolving preferences and capturing the semantic correlations of classification codes. To fill in this gap, we propose an event-based dynamic graph learning framework for patent application trend prediction. In particular, our method is founded on the memorable representations of both companies and patent classification codes. When a new patent is observed, the representations of the related companies and classification codes are updated according to the historical memories and the currently encoded messages. Moreover, a hierarchical message passing mechanism is provided to capture the semantic proximities of patent classification codes by u
    
[^42]: 追求多模态视觉语言模型中的基于实际的视觉空间推理

    Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models. (arXiv:2308.09778v1 [cs.CV])

    [http://arxiv.org/abs/2308.09778](http://arxiv.org/abs/2308.09778)

    本文旨在研究多模态视觉语言模型在理解空间关系方面的能力，提出了细粒度组合的空间关系基础，并采用自底向上的方法评估空间关系推理任务的性能。

    

    随着大规模视觉和语言模型（VLMs）的进展，评估它们在各种视觉推理任务（如计数、指涉表达和一般的视觉问题回答）上的表现变得越来越重要。本文的重点是研究这些模型理解空间关系的能力。先前，人们尝试使用图像-文本匹配（Liu, Emerson, and Collier 2022) 或视觉问题回答任务来处理此问题，但都表现出性能不佳并且与人类性能存在较大差距。为了更好地理解差距，我们提出了细粒度组合的空间关系基础，并提出了一种自底向上的方法来对空间从句进行排名并评估空间关系推理任务的性能。我们建议通过结合和地面化物体对应的名词短语和它们的位置的证据来计算空间从句的最终排名。我们在代表性的视觉语言模型上展示了这种方法。

    With the advances in large scale vision-and-language models (VLMs) it is of interest to assess their performance on various visual reasoning tasks such as counting, referring expressions and general visual question answering. The focus of this work is to study the ability of these models to understanding spatial relations. Previously, this has been tackled using image-text matching (Liu, Emerson, and Collier 2022) or visual question answering task, both showing poor performance and a large gap compared to human performance. To better understand the gap, we present fine-grained compositional grounding of spatial relationships and propose a bottom up approach for ranking spatial clauses and evaluating the performance of spatial relationship reasoning task. We propose to combine the evidence from grounding noun phrases corresponding to objects and their locations to compute the final rank of the spatial clause. We demonstrate the approach on representative vision-language models (Tan and 
    
[^43]: 未监测站点中的时间序列预测：水资源中机器学习技术的综述

    Time Series Predictions in Unmonitored Sites: A Survey of Machine Learning Techniques in Water Resources. (arXiv:2308.09766v1 [cs.LG])

    [http://arxiv.org/abs/2308.09766](http://arxiv.org/abs/2308.09766)

    本论文综述了机器学习在未监测站点的水资源预测中的应用，包括河流流量、水质等相关变量，并讨论了融合集水区特征的新方法，提升机器学习的使用。

    

    针对未监测站点中动态环境变量的预测仍然是水资源科学中长期存在的挑战。世界上大部分的淡水资源没有适当的监测关键环境变量的能力，而对河流流量和水质等水文变量进行广泛预测的需求由于气候和土地利用变化在过去几十年中越来越迫切，并影响着水资源。现代机器学习方法能够从大规模、多样化的数据集中提取信息，相对于基于过程和经验模型的方法在水文时间序列预测方面表现越来越优越。我们回顾了机器学习在河流流量、水质和其他水资源预测中的相关最新应用，并讨论了利用新兴方法改进机器学习在集水区特征融合方面的机会。

    Prediction of dynamic environmental variables in unmonitored sites remains a long-standing challenge for water resources science. The majority of the world's freshwater resources have inadequate monitoring of critical environmental variables needed for management. Yet, the need to have widespread predictions of hydrological variables such as river flow and water quality has become increasingly urgent due to climate and land use change over the past decades, and their associated impacts on water resources. Modern machine learning methods increasingly outperform their process-based and empirical model counterparts for hydrologic time series prediction with their ability to extract information from large, diverse data sets. We review relevant state-of-the art applications of machine learning for streamflow, water quality, and other water resources prediction and discuss opportunities to improve the use of machine learning with emerging methods for incorporating watershed characteristics i
    
[^44]: 受冷落: 相似度分数的反差效应

    Taken by Surprise: Contrast effect for Similarity Scores. (arXiv:2308.09765v1 [cs.CL])

    [http://arxiv.org/abs/2308.09765](http://arxiv.org/abs/2308.09765)

    提出了一种新的相似度度量方法，称为“惊喜分数”，该方法能够考虑对象的上下文信息并显著提高零样本和少样本文档分类任务的性能。

    

    准确评估物体向量嵌入的相似度对于自然语言处理、信息检索和分类任务至关重要。流行的相似度分数（如余弦相似度）基于嵌入向量对，并忽略了从中提取对象的分布。人类对物体相似度的感知显著取决于对象出现的上下文。在这项工作中，我们提出了“惊喜分数”，这是一个对整体进行归一化的相似度度量，包括了人类感知的反差效应，并显著提高了零样本和少样本文档分类任务的性能。此分数量化了在两个元素之间找到给定相似度的惊喜，相对于成对的整体相似度。我们在零样本/少样本分类和聚类任务上评估了这个度量，通常发现与原始余弦相似度相比，性能提高了10-15\%。我们的代码...

    Accurately evaluating the similarity of object vector embeddings is of critical importance for natural language processing, information retrieval and classification tasks. Popular similarity scores (e.g cosine similarity) are based on pairs of embedding vectors and disregard the distribution of the ensemble from which objects are drawn. Human perception of object similarity significantly depends on the context in which the objects appear. In this work we propose the \emph{surprise score}, an ensemble-normalized similarity metric that encapsulates the contrast effect of human perception and significantly improves the classification performance on zero- and few-shot document classification tasks. This score quantifies the surprise to find a given similarity between two elements relative to the pairwise ensemble similarities. We evaluate this metric on zero/few shot classification and clustering tasks and typically find 10-15\% better performance compared to raw cosine similarity. Our cod
    
[^45]: 去除背景对于神经网络在时尚图像分类和分割中的性能影响

    The Impact of Background Removal on Performance of Neural Networks for Fashion Image Classification and Segmentation. (arXiv:2308.09764v1 [cs.CV])

    [http://arxiv.org/abs/2308.09764](http://arxiv.org/abs/2308.09764)

    本研究通过去除时尚图像的背景，提高了数据质量和模型性能，在多个方面进行了广泛的比较实验，结果表明背景去除对于模型训练有积极的影响。

    

    时尚理解是计算机视觉中的热门话题，在市场上具有很大的商业价值。由于服装的巨大多样性以及各种场景和背景的存在，时尚理解对于计算机视觉仍然是一个很大的挑战。在这项工作中，我们尝试去除时尚图像中的背景，以提高数据质量并提高模型性能。通过利用显著性物体检测，我们可以对时尚数据进行背景去除。被去除背景的时尚图像与时尚数据集中的原始图像形成对比。我们对这两种类型的图像进行了广泛的比较实验，包括模型架构、模型初始化、与其他训练技巧和数据增强的兼容性以及目标任务类型。实验证明，背景去除对于模型训练在多个方面都有影响。

    Fashion understanding is a hot topic in computer vision, with many applications having great business value in the market. Fashion understanding remains a difficult challenge for computer vision due to the immense diversity of garments and various scenes and backgrounds. In this work, we try removing the background from fashion images to boost data quality and increase model performance. Having fashion images of evident persons in fully visible garments, we can utilize Salient Object Detection to achieve the background removal of fashion data to our expectations. A fashion image with the background removed is claimed as the "rembg" image, contrasting with the original one in the fashion dataset. We conducted extensive comparative experiments with these two types of images on multiple aspects of model training, including model architectures, model initialization, compatibility with other training tricks and data augmentations, and target task types. Our experiments show that background 
    
[^46]: 宇宙学中的数据压缩与推断：自我监督机器学习的应用

    Data Compression and Inference in Cosmology with Self-Supervised Machine Learning. (arXiv:2308.09751v1 [astro-ph.CO])

    [http://arxiv.org/abs/2308.09751](http://arxiv.org/abs/2308.09751)

    本研究利用自我监督机器学习的方法，通过模拟增广构建了宇宙学数据的代表性汇总，可以有效压缩数据并用于精确参数推断，为宇宙学数据的压缩和分析提供了一种有前途的新途径。

    

    当前和即将到来的宇宙学调查所产生的海量数据，需要能够以最小的信息损失有效地汇总数据的压缩方案。我们介绍了一种利用自我监督机器学习范例以新颖方式构建代表性数据汇总的方法，通过基于模拟的增广技术。在流体力学宇宙学模拟数据上使用这种方法，我们展示了它能够提供高度信息丰富的数据汇总，可以用于各种下游任务，包括精确和准确的参数推断。我们还展示了该范例如何构建对预定系统效应不敏感的汇总表示，例如弥散物理的影响。我们的结果表明，自我监督机器学习技术为宇宙学数据的压缩和分析提供了一种有前途的新途径。

    The influx of massive amounts of data from current and upcoming cosmological surveys necessitates compression schemes that can efficiently summarize the data with minimal loss of information. We introduce a method that leverages the paradigm of self-supervised machine learning in a novel manner to construct representative summaries of massive datasets using simulation-based augmentations. Deploying the method on hydrodynamical cosmological simulations, we show that it can deliver highly informative summaries, which can be used for a variety of downstream tasks, including precise and accurate parameter inference. We demonstrate how this paradigm can be used to construct summary representations that are insensitive to prescribed systematic effects, such as the influence of baryonic physics. Our results indicate that self-supervised machine learning techniques offer a promising new approach for compression of cosmological data as well its analysis.
    
[^47]: 慢性疾病的因果可解释性进展轨迹分析

    Causal Interpretable Progression Trajectory Analysis of Chronic Disease. (arXiv:2308.09735v1 [cs.LG])

    [http://arxiv.org/abs/2308.09735](http://arxiv.org/abs/2308.09735)

    提出了一种名为因果轨迹预测（CTP）的新模型，通过将轨迹预测和因果发现相结合，准确预测慢性疾病的进展轨迹并揭示特征间的因果关系。

    

    慢性疾病是导致死亡的主要原因，强调了准确预测疾病进展轨迹和知情临床决策的需求。机器学习模型通过捕捉患者特征中的非线性模式，在这个领域显示出了潜力。然而，现有的基于机器学习的模型缺乏提供因果可解释性预测和评估治疗效果的能力，限制了其决策辅助的角度。在本研究中，我们提出了一种名为因果轨迹预测（CTP）的新模型来解决这一限制。CTP模型将轨迹预测和因果发现相结合，以实现准确预测疾病进展轨迹和揭示特征间的因果关系。通过将因果图结合到预测过程中，CTP确保祖先特征不受对后代特征的治疗影响，从而增强了模型的可解释性。

    Chronic disease is the leading cause of death, emphasizing the need for accurate prediction of disease progression trajectories and informed clinical decision-making. Machine learning (ML) models have shown promise in this domain by capturing non-linear patterns within patient features. However, existing ML-based models lack the ability to provide causal interpretable predictions and estimate treatment effects, limiting their decision-assisting perspective. In this study, we propose a novel model called causal trajectory prediction (CTP) to tackle the limitation. The CTP model combines trajectory prediction and causal discovery to enable accurate prediction of disease progression trajectories and uncovering causal relationships between features. By incorporating a causal graph into the prediction process, CTP ensures that ancestor features are not influenced by treatment on descendant features, thereby enhancing the interpretability of the model. By estimating the bounds of treatment e
    
[^48]: 一种适用于非静态环境中多目标强化学习的稳健策略引导算法

    A Robust Policy Bootstrapping Algorithm for Multi-objective Reinforcement Learning in Non-stationary Environments. (arXiv:2308.09734v1 [cs.LG])

    [http://arxiv.org/abs/2308.09734](http://arxiv.org/abs/2308.09734)

    提出了一种适用于非静态环境中多目标强化学习的稳健策略引导算法，能够在线演化一个凸覆盖策略集，同时满足目标偏好空间的探索需求。

    

    多目标马尔可夫决策过程是一类需要满足马尔可夫随机过程特性的序贯决策问题。多目标强化学习方法通过融合强化学习和多目标优化技术来解决这个问题。然而，这些方法在处理非静态环境时适应性不足。本文引入了一种发展性优化方法，可以在线探索定义的目标偏好空间，同时演化策略覆盖集。我们提出了一种新颖的多目标强化学习算法，可以在非静态环境中稳健地在线演化一个凸覆盖策略集。

    Multi-objective Markov decision processes are a special kind of multi-objective optimization problem that involves sequential decision making while satisfying the Markov property of stochastic processes. Multi-objective reinforcement learning methods address this problem by fusing the reinforcement learning paradigm with multi-objective optimization techniques. One major drawback of these methods is the lack of adaptability to non-stationary dynamics in the environment. This is because they adopt optimization procedures that assume stationarity to evolve a coverage set of policies that can solve the problem. This paper introduces a developmental optimization approach that can evolve the policy coverage set while exploring the preference space over the defined objectives in an online manner. We propose a novel multi-objective reinforcement learning algorithm that can robustly evolve a convex coverage set of policies in an online manner in non-stationary environments. We compare the prop
    
[^49]: 多目标马尔可夫决策过程中的内在动机层次策略学习

    Intrinsically Motivated Hierarchical Policy Learning in Multi-objective Markov Decision Processes. (arXiv:2308.09733v1 [cs.LG])

    [http://arxiv.org/abs/2308.09733](http://arxiv.org/abs/2308.09733)

    该论文提出了一种解决多目标马尔可夫决策过程中的问题的方法，通过学习通用的技能集，使得策略覆盖集能够在非稳态环境中持续演化，从而提高性能。

    

    多目标马尔可夫决策过程是涉及多个相互冲突的奖励函数的顺序决策问题，这些函数无法在没有妥协的情况下同时进行优化。这种问题无法像传统情况下那样通过单个最优策略来解决。相反，多目标强化学习方法发展了一个可以满足解决问题中所有可能偏好的最优策略覆盖集。然而，许多这些方法无法将其覆盖集推广到在非稳态环境中工作。在这些环境中，状态转移和奖励分布的参数随时间变化。这限制导致了进化策略集的性能严重下降。为了克服这个限制，需要学习一组通用的技能，可以在环境动态变化时引导策略覆盖集的演变，从而促进持续学习

    Multi-objective Markov decision processes are sequential decision-making problems that involve multiple conflicting reward functions that cannot be optimized simultaneously without a compromise. This type of problems cannot be solved by a single optimal policy as in the conventional case. Alternatively, multi-objective reinforcement learning methods evolve a coverage set of optimal policies that can satisfy all possible preferences in solving the problem. However, many of these methods cannot generalize their coverage sets to work in non-stationary environments. In these environments, the parameters of the state transition and reward distribution vary over time. This limitation results in significant performance degradation for the evolved policy sets. In order to overcome this limitation, there is a need to learn a generic skill set that can bootstrap the evolution of the policy coverage set for each shift in the environment dynamics therefore, it can facilitate a continuous learning 
    
[^50]: Baird反例已解决：以调试两个时间尺度算法的示例。

    Baird Counterexample Is Solved: with an example of How to Debug a Two-time-scale Algorithm. (arXiv:2308.09732v1 [cs.LG])

    [http://arxiv.org/abs/2308.09732](http://arxiv.org/abs/2308.09732)

    这篇论文解决了Baird反例上的收敛问题，通过一种具有收敛保证的算法，实现了线性收敛速度。

    

    Baird反例是由Leemon Baird在1995年提出的，首先用于证明Temporal Difference (TD(0))算法在这个例子上发散。从那时起，它经常被用来测试和比较离策略学习算法。梯度TD算法解决了TD在Baird反例上的发散问题。然而，它们在这个例子上的收敛仍然非常缓慢，而且缓慢的本质还不被很好地理解。本文旨在特别理解为什么TDC在这个例子上慢，并提供调试分析来理解这种行为。我们的调试技术可以用来研究两个时间尺度随机逼近算法的收敛行为。我们还提供了最近的Impression GTD算法在这个例子上的实证结果，表明收敛非常快，事实上是线性的。我们得出结论，Baird反例通过一种具有收敛保证的算法解决了，该算法收敛到TD解决方案。

    Baird counterexample was proposed by Leemon Baird in 1995, first used to show that the Temporal Difference (TD(0)) algorithm diverges on this example. Since then, it is often used to test and compare off-policy learning algorithms. Gradient TD algorithms solved the divergence issue of TD on Baird counterexample. However, their convergence on this example is still very slow, and the nature of the slowness is not well understood, e.g., see (Sutton and Barto 2018).  This note is to understand in particular, why TDC is slow on this example, and provide debugging analysis to understand this behavior. Our debugging technique can be used to study the convergence behavior of two-time-scale stochastic approximation algorithms. We also provide empirical results of the recent Impression GTD algorithm on this example, showing the convergence is very fast, in fact, in a linear rate. We conclude that Baird counterexample is solved, by an algorithm with convergence guarantee to the TD solution in gen
    
[^51]: ChatGPT-HealthPrompt. 利用ChatGPT在基于提示的医疗决策支持中发挥可解释人工智能的力量

    ChatGPT-HealthPrompt. Harnessing the Power of XAI in Prompt-Based Healthcare Decision Support using ChatGPT. (arXiv:2308.09731v1 [cs.AI])

    [http://arxiv.org/abs/2308.09731](http://arxiv.org/abs/2308.09731)

    本研究介绍了一种创新的方法，利用ChatGPT在临床决策中应用大型语言模型，通过策略性设计上下文提示，并以领域知识为基础进行高质量的二元分类任务。通过将机器学习模型视为医疗专家，提取关键见解并辅助决策过程，这一领域知识和人工智能的结合在创建更具洞察力的诊断工具方面具有重要潜力。此外，研究还探索了基于ChatGPT的零样本和少样本提示学习的动态，并验证了ChatGPT在医疗决策支持中的优势。

    

    本研究提出了一种创新的方法，将大型语言模型（LLMs）应用于临床决策，重点关注OpenAI的ChatGPT。我们的方法引入了上下文提示的使用，策略性地设计包括任务描述、特征描述，并且关键地整合领域知识，以便在数据稀缺的情况下进行高质量的二元分类任务。我们工作的创新之处在于利用从高性能可解释机器学习模型获得的领域知识，并将其无缝地融入到提示设计中。通过将这些机器学习模型视为医疗专家，我们提取了关于特征重要性的关键见解，以帮助决策过程。领域知识和人工智能的相互作用在创建更具洞察力的诊断工具方面具有重要的潜力。此外，我们的研究探讨了基于LLMs的零样本和少样本提示学习的动态。通过比较OpenAI的ChatGPT与传统的supervised学习方法的表现，我们证明了ChatGPT在医疗决策支持方面的优势。

    This study presents an innovative approach to the application of large language models (LLMs) in clinical decision-making, focusing on OpenAI's ChatGPT. Our approach introduces the use of contextual prompts-strategically designed to include task description, feature description, and crucially, integration of domain knowledge-for high-quality binary classification tasks even in data-scarce scenarios. The novelty of our work lies in the utilization of domain knowledge, obtained from high-performing interpretable ML models, and its seamless incorporation into prompt design. By viewing these ML models as medical experts, we extract key insights on feature importance to aid in decision-making processes. This interplay of domain knowledge and AI holds significant promise in creating a more insightful diagnostic tool.  Additionally, our research explores the dynamics of zero-shot and few-shot prompt learning based on LLMs. By comparing the performance of OpenAI's ChatGPT with traditional supe
    
[^52]: 基于COVID-19的数据多样性和虚拟成像的AI诊断：以病例研究为基础

    Data diversity and virtual imaging in AI-based diagnosis: A case study based on COVID-19. (arXiv:2308.09730v1 [eess.IV])

    [http://arxiv.org/abs/2308.09730](http://arxiv.org/abs/2308.09730)

    本研究通过使用多样性的临床和虚拟生成的医学图像开发和评估了COVID-19诊断的AI模型，发现数据集特征对于AI性能具有重要影响，容易导致泛化能力较差，最高下降20％。

    

    许多研究已经调查了基于深度学习的人工智能（AI）模型在新型冠状病毒（COVID-19）的医学影像诊断中的应用，许多报道称其性能几乎完美。然而，性能的变异性和潜在的数据偏差引发了对临床适用性的担忧。本回顾性研究涉及使用临床多样性和虚拟生成的医学图像开发和评估COVID-19诊断的人工智能（AI）模型。此外，我们进行了一次虚拟成像试验，以评估AI性能受疾病范围、辐射剂量和计算机断层扫描（CT）和胸部放射摄影（CXR）成像模态等几个患者和物理性因素的影响。数据集特征（包括数量、多样性和患病率）强烈影响了AI的性能，导致接收者操作特征曲线下面积下降了高达20％，且泛化能力差。

    Many studies have investigated deep-learning-based artificial intelligence (AI) models for medical imaging diagnosis of the novel coronavirus (COVID-19), with many reports of near-perfect performance. However, variability in performance and underlying data biases raise concerns about clinical generalizability. This retrospective study involved the development and evaluation of artificial intelligence (AI) models for COVID-19 diagnosis using both diverse clinical and virtually generated medical images. In addition, we conducted a virtual imaging trial to assess how AI performance is affected by several patient- and physics-based factors, including the extent of disease, radiation dose, and imaging modality of computed tomography (CT) and chest radiography (CXR). AI performance was strongly influenced by dataset characteristics including quantity, diversity, and prevalence, leading to poor generalization with up to 20% drop in receiver operating characteristic area under the curve. Model
    
[^53]: MindMap：知识图谱激发大型语言模型的思维图思考方法

    MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models. (arXiv:2308.09729v1 [cs.AI])

    [http://arxiv.org/abs/2308.09729](http://arxiv.org/abs/2308.09729)

    本论文通过使用知识图谱来激发大型语言模型，解决了整合新知识、产生幻觉和决策过程不透明等问题，并通过生成思维导图展示了模型的推理路径，实验证明这种方法可以取得显著的实证增益。

    

    通常，大型语言模型存在无法整合新知识、产生幻觉和决策过程不透明等限制。本文探讨了如何利用知识图谱（KG）来激发大型语言模型，以解决整合最新知识和引发模型思维路径的问题。具体来说，我们构建了一个提示管道，使大型语言模型能够理解KG输入并利用隐含知识和检索到的外部知识进行推理。此外，我们研究了引发大型语言模型执行推理和生成答案的思维导图。研究发现，生成的思维导图基于知识的本体论，展示了大型语言模型的推理路径，从而为生产环境中的推理提供了探索和评估的可能性。对三个问答数据集的实验证明，MindMap提示方法带来了显著的实证增益。

    LLMs usually exhibit limitations in their ability to incorporate new knowledge, the generation of hallucinations, and the transparency of their decision-making process. In this paper, we explore how to prompt LLMs with knowledge graphs (KG), working as a remedy to engage LLMs with up-to-date knowledge and elicit the reasoning pathways from LLMs. Specifically, we build a prompting pipeline that endows LLMs with the capability of comprehending KG inputs and inferring with a combined implicit knowledge and the retrieved external knowledge. In addition, we investigate eliciting the mind map on which LLMs perform the reasoning and generate the answers. It is identified that the produced mind map exhibits the reasoning pathways of LLMs grounded on the ontology of knowledge, hence bringing the prospects of probing and gauging LLM inference in production. The experiments on three question & answering datasets also show that MindMap prompting leads to a striking empirical gain. For instance, pr
    
[^54]: 通过前向传播误差学习表示

    Learning representations by forward-propagating errors. (arXiv:2308.09728v1 [cs.LG])

    [http://arxiv.org/abs/2308.09728](http://arxiv.org/abs/2308.09728)

    本文提出了一种基于前向传播和代数几何中双数概念的快速学习算法，可以在CPU上实现与CUDA加速相媲美的性能。

    

    反向传播（BP）是神经网络优化中广泛使用的学习算法。然而，BP需要巨大的计算成本，对于在中央处理单元（CPU）上进行训练来说太慢。因此，当前的神经网络优化是在图形处理单元（GPU）上进行，使用计算统一设备架构（CUDA）编程。在本文中，我们提出了一种基于在CPU上快速实现CUDA加速的轻量级快速学习算法。该算法基于在代数几何中使用双数概念的前向传播方法。

    Back-propagation (BP) is widely used learning algorithm for neural network optimization. However, BP requires enormous computation cost and is too slow to train in central processing unit (CPU). Therefore current neural network optimizaiton is performed in graphical processing unit (GPU) with compute unified device architecture (CUDA) programming. In this paper, we propose a light, fast learning algorithm on CPU that is fast as CUDA acceleration on GPU. This algorithm is based on forward-propagating method, using concept of dual number in algebraic geometry.
    
[^55]: 跨城市少样本交通预测：通过交通模式存储库

    Cross-city Few-Shot Traffic Forecasting via Traffic Pattern Bank. (arXiv:2308.09727v1 [cs.LG])

    [http://arxiv.org/abs/2308.09727](http://arxiv.org/abs/2308.09727)

    本文提出了一种通过交通模式存储库来进行跨城市少样本交通预测的框架。通过将来自数据丰富城市的原始交通数据投影到高维空间并生成交通模式存储库，可以改善数据稀缺城市的交通预测性能。

    

    交通预测是智能交通系统中的重要服务。利用深度模型解决这一任务，严重依赖于交通传感器或车辆设备的数据，然而一些城市可能缺乏设备支持，因此可用的数据较少。因此，有必要从数据丰富的城市中学习，并将知识转移给数据稀缺的城市，以提高交通预测的性能。为了解决这个问题，我们提出了一种通过交通模式存储库来进行跨城市少样本交通预测的框架，因为交通模式在不同城市间是相似的。交通模式存储库利用预训练的交通补丁编码器将数据丰富的城市的原始交通数据投影到高维空间，通过聚类生成交通模式存储库。然后，数据稀缺城市的交通数据可以查询交通模式存储库，并构建它们之间的显式关系。基于这些关系，元知识被聚合起来。

    Traffic forecasting is a critical service in Intelligent Transportation Systems (ITS). Utilizing deep models to tackle this task relies heavily on data from traffic sensors or vehicle devices, while some cities might lack device support and thus have few available data. So, it is necessary to learn from data-rich cities and transfer the knowledge to data-scarce cities in order to improve the performance of traffic forecasting. To address this problem, we propose a cross-city few-shot traffic forecasting framework via Traffic Pattern Bank (TPB) due to that the traffic patterns are similar across cities. TPB utilizes a pre-trained traffic patch encoder to project raw traffic data from data-rich cities into high-dimensional space, from which a traffic pattern bank is generated through clustering. Then, the traffic data of the data-scarce city could query the traffic pattern bank and explicit relations between them are constructed. The metaknowledge is aggregated based on these relations a
    
[^56]: 公平的不躁动多臂赌博机：受数字健康启发的通用框架

    Equitable Restless Multi-Armed Bandits: A General Framework Inspired By Digital Health. (arXiv:2308.09726v1 [cs.LG])

    [http://arxiv.org/abs/2308.09726](http://arxiv.org/abs/2308.09726)

    这项研究提出了不躁动多臂赌博机的公平目标，并开发了相应的算法，证明在数字健康等领域中可以提高数倍的公平性。

    

    不躁动多臂赌博机（RMABs）是一种在资源有限的连续环境中进行算法决策的流行框架。RMABs越来越被用于公共卫生、治疗安排、反偷猎等敏感决策，而本研究的动机正是数字健康。在这些高风险环境中，决策必须改善结果并避免不同群体之间的差距（例如，确保健康公平）。我们首次研究了RMABs的公平目标（ERMABs）。我们考虑了公平文献中两种公平性相关的目标，最小化最大化奖励和最大化纳什福利。我们为解决这两个目标开发了高效的算法——对于前者使用了水位填充算法，对于后者使用了理论上有动机的贪心算法来平衡不同群体大小。最后，我们通过三个模拟领域（包括一个新的数字健康模型）的实验证明，我们的方法在公平性方面可以比当前方法提高数倍。

    Restless multi-armed bandits (RMABs) are a popular framework for algorithmic decision making in sequential settings with limited resources. RMABs are increasingly being used for sensitive decisions such as in public health, treatment scheduling, anti-poaching, and -- the motivation for this work -digital health. For such high stakes settings, decisions must both improve outcomes and prevent disparities between groups (e.g., ensure health equity). We study equitable objectives for RMABs (ERMABs) for the first time. We consider two equity-aligned objectives from the fairness literature, minimax reward and max Nash welfare. We develop efficient algorithms for solving each -- a water filling algorithm for the former, and a greedy algorithm with theoretically motivated nuance to balance disparate group sizes for the latter. Finally, we demonstrate across three simulation domains, including a new digital health model, that our approaches can be multiple times more equitable than the curren
    
[^57]: MoCLIM: 用多组学对比学习和组学推理建模实现准确的癌症亚型划分

    MoCLIM: Towards Accurate Cancer Subtyping via Multi-Omics Contrastive Learning with Omics-Inference Modeling. (arXiv:2308.09725v1 [q-bio.GN])

    [http://arxiv.org/abs/2308.09725](http://arxiv.org/abs/2308.09725)

    本论文介绍了一种名为MoCLIM的多组学对比学习框架，能够在癌症亚型划分中利用多组学数据的潜力，显著提高了数据的拟合度和亚型划分性能。

    

    精准医学的目标是建立癌症亚型的生化机制与疾病之间的因果关系。基于组学的癌症亚型划分已经成为一种革命性的方法，因为不同级别的组学记录了癌症中多步骤过程的生化产物。本文旨在充分利用多组学数据的潜力来改善癌症亚型划分结果，因此开发了MoCLIM，一种表示学习框架。MoCLIM独立地从不同的组学模式中提取有信息量的特征。通过对不同组学模式之间的对比学习所得到的统一表示，在给定癌症情况下，我们能够将亚型很好地聚类到较低的潜空间中。这种对比可以被解释为在生物网络中观察到的组际推理的投影。在六个癌症数据集上的实验结果表明，我们的方法在较少的高维癌症数据拟合和亚型划分性能方面显著改善。

    Precision medicine fundamentally aims to establish causality between dysregulated biochemical mechanisms and cancer subtypes. Omics-based cancer subtyping has emerged as a revolutionary approach, as different level of omics records the biochemical products of multistep processes in cancers. This paper focuses on fully exploiting the potential of multi-omics data to improve cancer subtyping outcomes, and hence developed MoCLIM, a representation learning framework. MoCLIM independently extracts the informative features from distinct omics modalities. Using a unified representation informed by contrastive learning of different omics modalities, we can well-cluster the subtypes, given cancer, into a lower latent space. This contrast can be interpreted as a projection of inter-omics inference observed in biological networks. Experimental results on six cancer datasets demonstrate that our approach significantly improves data fit and subtyping performance in fewer high-dimensional cancer ins
    
[^58]: 知识启发的子领域适应用于交叉领域知识传递

    Knowledge-inspired Subdomain Adaptation for Cross-Domain Knowledge Transfer. (arXiv:2308.09724v1 [cs.LG])

    [http://arxiv.org/abs/2308.09724](http://arxiv.org/abs/2308.09724)

    知识启发的子领域适应（KISA）框架在交叉领域知识传递中提供了细粒度的领域适应能力。

    

    大多数最先进的深度领域适应技术以全局方式对齐源域和目标域样本。也就是说，对齐后，每个源样本都期望与任何目标样本相似。然而，在实践中，全局对齐可能并不总是最优或必要的。为了实现这种细粒度的领域适应，我们提出了一种新颖的知识启发的子领域适应（KISA）框架。具体来说，（1）我们提供了KISA最小化共享预期损失的理论洞见，这是领域适应方法成功的前提。（2）我们提出了知识启发的子领域划分问题，这在细粒度的领域适应中起着重要作用。

    Most state-of-the-art deep domain adaptation techniques align source and target samples in a global fashion. That is, after alignment, each source sample is expected to become similar to any target sample. However, global alignment may not always be optimal or necessary in practice. For example, consider cross-domain fraud detection, where there are two types of transactions: credit and non-credit. Aligning credit and non-credit transactions separately may yield better performance than global alignment, as credit transactions are unlikely to exhibit patterns similar to non-credit transactions. To enable such fine-grained domain adaption, we propose a novel Knowledge-Inspired Subdomain Adaptation (KISA) framework. In particular, (1) We provide the theoretical insight that KISA minimizes the shared expected loss which is the premise for the success of domain adaptation methods. (2) We propose the knowledge-inspired subdomain division problem that plays a crucial role in fine-grained doma
    
[^59]: FineQuant: 利用细粒度的权重量化为LLMs解锁效率

    FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization for LLMs. (arXiv:2308.09723v1 [cs.LG])

    [http://arxiv.org/abs/2308.09723](http://arxiv.org/abs/2308.09723)

    该论文提出了一种针对LLMs的高效权重量化方法，通过减少内存消耗和加速推理，解决了大型语言模型在实际部署中遇到的挑战。研究者们通过引入一种简单而有效的启发式方法，仅利用模型的权重，而无需额外微调，成功降低了质量损失。

    

    大型语言模型（LLMs）在各种语言任务中取得了最先进的性能，但由于其大量的内存需求，对于实际部署提出了挑战。此外，最新的生成模型由于自回归解码过程中的内存带宽瓶颈导致推理成本高。为了解决这些问题，我们提出了一种高效的仅基于权重量化方法，以减少LLMs的内存消耗并加速推理。为了确保质量降低最小化，我们引入了一种简单而有效的启发式方法，仅利用预训练模型的模型权重。这种方法适用于无需额外微调的Mixture-of-Experts（MoE）和密集模型。为了展示我们提出的方法的有效性，我们首先分析与LLMs量化相关的挑战和问题。随后，我们提出了自适应找到权重细粒度的启发式方法。

    Large Language Models (LLMs) have achieved state-of-the-art performance across various language tasks but pose challenges for practical deployment due to their substantial memory requirements. Furthermore, the latest generative models suffer from high inference costs caused by the memory bandwidth bottleneck in the auto-regressive decoding process. To address these issues, we propose an efficient weight-only quantization method that reduces memory consumption and accelerates inference for LLMs. To ensure minimal quality degradation, we introduce a simple and effective heuristic approach that utilizes only the model weights of a pre-trained model. This approach is applicable to both Mixture-of-Experts (MoE) and dense models without requiring additional fine-tuning. To demonstrate the effectiveness of our proposed method, we first analyze the challenges and issues associated with LLM quantization. Subsequently, we present our heuristic approach, which adaptively finds the granularity of 
    
[^60]: 一个可信的基于LSTM-Autoencoder网络的社交媒体网络欺凌检测方法：使用合成数据

    A Trustable LSTM-Autoencoder Network for Cyberbullying Detection on Social Media Using Synthetic Data. (arXiv:2308.09722v1 [cs.LG])

    [http://arxiv.org/abs/2308.09722](http://arxiv.org/abs/2308.09722)

    本研究提出了一种使用合成数据进行社交媒体网络欺凌检测的可信LSTM-Autoencoder网络。该方法解决了数据可用性困难的问题，并通过实验证明了在印地语、孟加拉语和英语数据集上的有效性。

    

    社交媒体网络欺凌对人类生活有害。随着在线社交网络的不断增长，仇恨言论的数量也在增加。这些可怕的内容可能导致抑郁和与自杀有关的行为。本文提出了一种使用合成数据进行社交媒体网络欺凌检测的可信LSTM-Autoencoder网络。我们通过生成机器翻译数据展示了一种解决数据可用性困难的前沿方法。然而，印地语和孟加拉语等几种语言由于缺乏数据集的原因，仍然缺乏足够的研究。我们使用提出的模型和传统模型（包括长短期记忆网络（LSTM），双向长短期记忆网络（BiLSTM），LSTM-Autoencoder，Word2vec，双向编码器表示转换（BERT）和生成预训练转换器2（GPT-2）模型）对印地语、孟加拉语和英语数据集进行了实验性的侵犯评论识别。我们采用了评估指标来评估模型的性能。

    Social media cyberbullying has a detrimental effect on human life. As online social networking grows daily, the amount of hate speech also increases. Such terrible content can cause depression and actions related to suicide. This paper proposes a trustable LSTM-Autoencoder Network for cyberbullying detection on social media using synthetic data. We have demonstrated a cutting-edge method to address data availability difficulties by producing machine-translated data. However, several languages such as Hindi and Bangla still lack adequate investigations due to a lack of datasets. We carried out experimental identification of aggressive comments on Hindi, Bangla, and English datasets using the proposed model and traditional models, including Long Short-Term Memory (LSTM), Bidirectional Long Short-Term Memory (BiLSTM), LSTM-Autoencoder, Word2vec, Bidirectional Encoder Representations from Transformers (BERT), and Generative Pre-trained Transformer 2 (GPT-2) models. We employed evaluation m
    
[^61]: 人工通用智能的新解决方案和具体实施步骤

    A new solution and concrete implementation steps for Artificial General Intelligence. (arXiv:2308.09721v1 [cs.LG])

    [http://arxiv.org/abs/2308.09721](http://arxiv.org/abs/2308.09721)

    本研究提出了在人工通用智能领域中解决现有技术缺陷的新方案，并分析了大型模型技术路线的局限性。

    

    目前，主流人工智能通常采用“注意机制+深度学习”+“强化学习”的技术路径。在人工智能生成内容（AIGC）领域取得了重大进展，掀起了大模型的技术浪潮。但在涉及与实际环境交互的领域，如养老护理、家庭保姆、农业生产和车辆驾驶等，试错成本很高，需要大量试错的强化学习过程很难实现。因此，为了实现可应用于任何领域的人工通用智能（AGI），我们需要同时利用现有技术并解决现有技术的缺陷，从而进一步发展人工智能的技术浪潮。本文分析了大型模型技术路线的局限性，并通过解决这些局限性，提出解决方案，从而解决了这一问题。

    At present, the mainstream artificial intelligence generally adopts the technical path of "attention mechanism + deep learning" + "reinforcement learning". It has made great progress in the field of AIGC (Artificial Intelligence Generated Content), setting off the technical wave of big models[ 2][13 ]. But in areas that need to interact with the actual environment, such as elderly care, home nanny, agricultural production, and vehicle driving, trial and error are expensive and a reinforcement learning process that requires much trial and error is difficult to achieve. Therefore, in order to achieve Artificial General Intelligence(AGI) that can be applied to any field, we need to use both existing technologies and solve the defects of existing technologies, so as to further develop the technological wave of artificial intelligence. In this paper, we analyze the limitations of the technical route of large models, and by addressing these limitations, we propose solutions, thus solving the
    
[^62]: 关于大型语言模型的意想不到能力

    On the Unexpected Abilities of Large Language Models. (arXiv:2308.09720v1 [cs.AI])

    [http://arxiv.org/abs/2308.09720](http://arxiv.org/abs/2308.09720)

    大型语言模型展示了与其训练任务不直接相关的广泛能力，通过间接获取过程使其拥有综合能力的发展，这些能力在一定程度上可预测，并且与人类认知有关。

    

    大型语言模型能展示出与其训练任务（预测人类书写文本的下一个单词）不直接相关的广泛能力。本文讨论了这种间接获取过程的性质及其与其他已知间接过程的关系。文章主张这种间接获取的一个重要副作用是综合能力的发展。本文还讨论了大型语言模型所开发的能力在多大程度上是可预测的。最后，文章简要讨论了这些系统所获得的认知技能与人类认知之间的关系。

    Large language models are capable of displaying a wide range of abilities that are not directly connected with the task for which they are trained: predicting the next words of human-written texts. In this article, I discuss the nature of this indirect acquisition process and its relation to other known indirect processes. I argue that an important side effect of such indirect acquisition is the development of integrated abilities. I discuss the extent to which the abilities developed by large language models are predictable. Finally, I briefly discuss the relation between the cognitive skills acquired by these systems and human cognition.
    
[^63]: 想法图：用大型语言模型解决复杂问题

    Graph of Thoughts: Solving Elaborate Problems with Large Language Models. (arXiv:2308.09687v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.09687](http://arxiv.org/abs/2308.09687)

    想法图（GoT）是一种新的框架，它超越了现有的提示范式，通过将大型语言模型（LLM）的信息建模为任意图形，将LLM想法组合成具有协同效应的结果，提炼整个思维网络的本质，或者使用反馈环路增强思维。GoT在不同任务上展示出优势，并可以通过新的想法转换进行扩展，使LLM的推理更接近人类思维。

    

    我们介绍了一种名为想法图（Graph of Thoughts，GoT）的框架，它在大型语言模型（LLM）的提示能力上超越了Chain-of-Thought或Tree of Thoughts（ToT）等范式。GoT的关键思想和主要优势在于能够将LLM生成的信息建模为任意图形，其中信息单元（"LLM想法"）是顶点，边表示这些顶点之间的依赖关系。这种方法使得将任意LLM想法组合成具有协同效应的结果、提炼整个思维网络的本质或者使用反馈环路增强思维成为可能。我们证明GoT在不同任务上比最先进的方法有优势，例如在排序任务上质量提高了62%，同时成本降低了超过31%。我们确保GoT能够通过新的想法转换进行扩展，从而可以用于开创新的提示方案。这项工作使得LLM的推理更接近人类思维。

    We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ("LLM thoughts") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62% over ToT, while simultaneously reducing costs by >31%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinki
    
[^64]: 泛化的求和池化用于度量学习

    Generalized Sum Pooling for Metric Learning. (arXiv:2308.09228v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2308.09228](http://arxiv.org/abs/2308.09228)

    本论文提出了一种泛化求和池化方法（GSP）用于深度度量学习。GSP通过选择语义实体的子集，学习忽略无用信息，并学习每个实体的重要性权重，从而改进了全局平均池化（GAP）方法。

    

    深度度量学习的常见架构选择是卷积神经网络后跟全局平均池化（GAP）。尽管简单，GAP是一种高度有效的信息聚合方式。对于GAP的有效性，一种可能的解释是将每个特征向量视为表示不同语义实体的集合，而GAP则是它们的凸组合。在这个视角下，我们泛化了GAP并提出了一种可学习的泛化求和池化方法（GSP）。GSP通过两种不同的能力改进了GAP：i）能够选择语义实体的子集，从而有效地学习忽略无用信息；ii）学习与每个实体的重要性对应的权重。形式上，我们提出了一个熵平滑的最优传输问题，并展示了它是GAP的严格泛化，即问题的一个特定实现会得到GAP。我们证明了这个优化问题具有解析梯度，使我们能够将其作为直接学习的方法。

    A common architectural choice for deep metric learning is a convolutional neural network followed by global average pooling (GAP). Albeit simple, GAP is a highly effective way to aggregate information. One possible explanation for the effectiveness of GAP is considering each feature vector as representing a different semantic entity and GAP as a convex combination of them. Following this perspective, we generalize GAP and propose a learnable generalized sum pooling method (GSP). GSP improves GAP with two distinct abilities: i) the ability to choose a subset of semantic entities, effectively learning to ignore nuisance information, and ii) learning the weights corresponding to the importance of each entity. Formally, we propose an entropy-smoothed optimal transport problem and show that it is a strict generalization of GAP, i.e., a specific realization of the problem gives back GAP. We show that this optimization problem enjoys analytical gradients enabling us to use it as a direct lear
    
[^65]: 自学习增强 (ReST) 用于语言模型的强化学习

    Reinforced Self-Training (ReST) for Language Modeling. (arXiv:2308.08998v1 [cs.CL])

    [http://arxiv.org/abs/2308.08998](http://arxiv.org/abs/2308.08998)

    本文提出了一种称为自学习增强 (ReST) 的算法，通过从人类反馈中进行强化学习来提高大型语言模型 (LLM) 的输出质量。在机器翻译任务上的实验结果表明，ReST能够以高效的方式显著提高翻译质量。

    

    通过从人类反馈中进行强化学习 (RLHF)，可以通过与人类偏好对齐来提高大型语言模型 (LLM) 的输出质量。我们提出了一种简单的算法，通过增长批量强化学习 (RL) 来与人类偏好对齐 LLM，我们称之为增强自学习 (ReST)。给定初始的LLM策略，ReST通过从策略中生成样本来产生一个数据集，然后使用离线强化学习算法改进LLM策略。ReST比典型的在线RLHF方法更高效，因为训练数据集是离线生成的，可以重复使用数据。虽然ReST是适用于所有生成学习设置的通用方法，但我们将重点放在其在机器翻译中的应用上。我们的结果表明，ReST可以以计算和采样高效的方式显著提高翻译质量，通过自动化指标和人工评估在机器翻译基准上测量。

    Reinforcement learning from human feedback (RLHF) can improve the quality of large language model's (LLM) outputs by aligning them with human preferences. We propose a simple algorithm for aligning LLMs with human preferences inspired by growing batch reinforcement learning (RL), which we call Reinforced Self-Training (ReST). Given an initial LLM policy, ReST produces a dataset by generating samples from the policy, which are then used to improve the LLM policy using offline RL algorithms. ReST is more efficient than typical online RLHF methods because the training dataset is produced offline, which allows data reuse. While ReST is a general approach applicable to all generative learning settings, we focus on its application to machine translation. Our results show that ReST can substantially improve translation quality, as measured by automated metrics and human evaluation on machine translation benchmarks in a compute and sample-efficient manner.
    
[^66]: 人工智能中的意识：来自意识科学的洞见

    Consciousness in Artificial Intelligence: Insights from the Science of Consciousness. (arXiv:2308.08708v1 [cs.AI])

    [http://arxiv.org/abs/2308.08708](http://arxiv.org/abs/2308.08708)

    本论文提出了一种严谨的方法，通过对当前的人工智能系统进行详细评估来探讨人工智能的意识问题。研究中对几种科学意识理论进行概述，并通过计算方法推导出意识的“指示性特征”。分析结果表明目前的人工智能系统尚不具备意识，但建立具有意识的人工智能系统并无明显的障碍。

    

    当前或近期的人工智能系统是否能具有意识成为科学界关注的话题，也引起了公众的担忧。本报告提出并举例了一种严谨且经验基础的人工智能意识方法：根据我们目前最可信的神经科学理论对现有的人工智能系统进行详细评估。我们概述了几种广泛认可的科学意识理论，包括循环处理理论、全局工作空间理论、高阶理论、预测处理理论和注意模式理论。从这些理论中，我们推导出一些意识的“指示性特征”，并通过计算方法来评估人工智能系统是否具备这些特征。我们利用这些指示性特征来评估了几个近期的人工智能系统，并讨论了未来系统如何实现这些特征。我们的分析表明，目前没有现有的人工智能系统具有意识，但同时也显示出没有明显的建立具有意识的人工智能系统的障碍。

    Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern. This report argues for, and exemplifies, a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of our best-supported neuroscientific theories of consciousness. We survey several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higher-order theories, predictive processing, and attention schema theory. From these theories we derive "indicator properties" of consciousness, elucidated in computational terms that allow us to assess AI systems for these properties. We use these indicator properties to assess several recent AI systems, and we discuss how future systems might implement them. Our analysis suggests that no current AI systems are conscious, but also shows that there are no obvious barriers to building conscious AI systems.
    
[^67]: 通过在线文本增强和上下文记忆的方式进行故事可视化

    Story Visualization by Online Text Augmentation with Context Memory. (arXiv:2308.07575v1 [cs.CV])

    [http://arxiv.org/abs/2308.07575](http://arxiv.org/abs/2308.07575)

    本论文提出了一种通过在线文本增强和上下文记忆来进行故事可视化的方法。通过使用新颖的记忆架构和多个伪描述作为训练过程的补充监督，该方法在两个故事可视化基准测试中取得了显著优于现有方法的结果。

    

    故事可视化是一个具有挑战性的文本到图像生成任务，难点在于不仅需要从文本描述中呈现视觉细节，还需要对跨多个句子的长期上下文进行编码。以往的工作主要关注为每个句子生成语义相关的图像，但在给定段落中编码上下文以生成具有上下文说服力的图像（例如，正确的角色或适当的场景背景）仍然是一个挑战。为此，我们提出了一种新颖的记忆架构，用于双向Transformer，并通过在线文本增强生成多个伪描述作为训练过程中的补充监督，以更好地适应推理中的语言变化。在两个流行的故事可视化基准测试中进行了大量实验证明，即Pororo-SV和Flintstones-SV，所提出的方法在包括FID、字符...

    Story visualization (SV) is a challenging text-to-image generation task for the difficulty of not only rendering visual details from the text descriptions but also encoding a long-term context across multiple sentences. While prior efforts mostly focus on generating a semantically relevant image for each sentence, encoding a context spread across the given paragraph to generate contextually convincing images (e.g., with a correct character or with a proper background of the scene) remains a challenge. To this end, we propose a novel memory architecture for the Bi-directional Transformers with an online text augmentation that generates multiple pseudo-descriptions as supplementary supervision during training, for better generalization to the language variation at inference. In extensive experiments on the two popular SV benchmarks, i.e., the Pororo-SV and Flintstones-SV, the proposed method significantly outperforms the state of the arts in various evaluation metrics including FID, char
    
[^68]: AudioFormer: 通过离散的声学代码学习音频特征表示的音频变换器

    AudioFormer: Audio Transformer learns audio feature representations from discrete acoustic codes. (arXiv:2308.07221v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2308.07221](http://arxiv.org/abs/2308.07221)

    AudioFormer是一种学习音频特征表示的方法，通过生成离散的声学代码并利用它们来训练掩码语言模型，从而将音频分类任务视为自然语言理解的形式。此外，引入了多正样本对比学习方法，通过学习联合表示来捕捉音频中的相关性。

    

    我们提出了一种名为AudioFormer的方法，通过获取离散的声学代码来学习音频特征表示，并随后对其进行微调以用于音频分类任务。我们首先将音频分类任务视为一种自然语言理解 (NLU) 的形式，借助现有的神经音频编解码模型，我们生成了离散的声学代码，并利用它们来训练一个掩码语言模型 (MLM)，从而获得音频特征表示。此外，我们首创了一种多正样本对比 (MPC) 学习方法的整合，该方法能够学习同一音频输入中多个离散声学代码间的联合表示。在实验中，我们将离散的声学代码视为文本数据，并使用类似填空题的方法训练一个掩码语言模型，最终得到高质量的音频表示。值得注意的是，MPC学习技术能够有效捕捉到音频中的相关性。

    We propose a method named AudioFormer,which learns audio feature representations through the acquisition of discrete acoustic codes and subsequently fine-tunes them for audio classification tasks. Initially,we introduce a novel perspective by considering the audio classification task as a form of natural language understanding (NLU). Leveraging an existing neural audio codec model,we generate discrete acoustic codes and utilize them to train a masked language model (MLM),thereby obtaining audio feature representations. Furthermore,we pioneer the integration of a Multi-Positive sample Contrastive (MPC) learning approach. This method enables the learning of joint representations among multiple discrete acoustic codes within the same audio input. In our experiments,we treat discrete acoustic codes as textual data and train a masked language model using a cloze-like methodology,ultimately deriving high-quality audio representations. Notably,the MPC learning technique effectively captures c
    
[^69]: 自然语言是图表所需要的全部内容

    Natural Language is All a Graph Needs. (arXiv:2308.07134v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.07134](http://arxiv.org/abs/2308.07134)

    本论文提出了一种名为InstructGLM的结构化语言模型算法，该算法将大型语言模型与图表学习问题相结合，旨在探索是否可以用语言模型取代图神经网络作为图表的基础模型。

    

    大规模预训练语言模型的出现，如ChatGPT，已经在人工智能的各个研究领域中引起了革命。基于Transformer的大型语言模型（LLMs）逐渐取代了CNN和RNN，将计算机视觉和自然语言处理领域统一起来。与相对独立存在的数据（如图像、视频或文本）相比，图表是一种包含丰富结构和关系信息的数据类型。同时，作为最具表现力的媒介之一，自然语言在描述复杂结构方面表现出色。然而，将图表学习问题纳入生成式语言建模框架的现有工作仍然非常有限。随着大型语言模型的重要性不断增长，探索LLMs是否也可以替代GNNs成为图表的基础模型变得至关重要。在本文中，我们提出了InstructGLM（结构化语言模型）算法，系统地设计高度可扩展的模型来处理图表学习问题。

    The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scal
    
[^70]: 在大型视觉语言模型中检测和预防幻觉

    Detecting and Preventing Hallucinations in Large Vision Language Models. (arXiv:2308.06394v1 [cs.CV])

    [http://arxiv.org/abs/2308.06394](http://arxiv.org/abs/2308.06394)

    本论文提出了一个用于训练和评估模型的多模态幻觉检测数据集，以解决大型视觉语言模型中存在的幻觉文本问题。这是第一个用于详细图像描述的全面多模态幻觉检测数据集。

    

    经过调整的大型视觉语言模型（LVLMs）在泛化跨多种多模态任务方面取得了显著进展，特别是在视觉问答（VQA）方面。然而，为这些模型生成与视觉相关的详细回答仍然是一个具有挑战性的任务。我们发现，即使是当前最先进的LVLM（InstructBLIP）仍然存在着惊人的30%的幻觉文本，包括不存在的对象、不忠实的描述和不准确的关系。为了解决这个问题，我们引入了M-HalDetect，这是一个用于训练和评估幻觉检测和预防模型的多模态幻觉检测数据集。M-HalDetect包含了16k个细粒度的VQA示例标签，是第一个用于详细图像描述的全面多模态幻觉检测数据集。与之前只考虑对象幻觉的工作不同，我们还注释了实体描述。

    Instruction tuned Large Vision Language Models (LVLMs) have made significant advancements in generalizing across a diverse set of multimodal tasks, especially for Visual Question Answering (VQA). However, generating detailed responses that are visually grounded is still a challenging task for these models. We find that even the current state-of-the-art LVLMs (InstructBLIP) still contain a staggering 30 percent of hallucinatory text in the form of non-existent objects, unfaithful descriptions, and inaccurate relationships. To address this, we introduce M-HalDetect, a {M}ultimodal {Hal}lucination {Detect}ion Dataset that can be used to train and benchmark models for hallucination detection and prevention. M-HalDetect consists of 16k fine-grained labels on VQA examples, making it the first comprehensive multi-modal hallucination detection dataset for detailed image descriptions. Unlike previous work that only consider object hallucination, we additionally annotate both entity descriptions
    
[^71]: 使用深度学习模型进行血细胞分类

    Classification of Blood Cells Using Deep Learning Models. (arXiv:2308.06300v1 [eess.IV])

    [http://arxiv.org/abs/2308.06300](http://arxiv.org/abs/2308.06300)

    这项研究使用深度学习模型通过图像分类对人类血细胞进行了分类和识别，为诊断疾病提供了重要的帮助。

    

    人类血液主要包括血浆、红细胞、白细胞和血小板。血细胞为身体细胞提供氧气，滋养它们，保护它们免受感染，增强免疫力并促进凝血。人的健康状况可以从血细胞中反映出来。一个人被诊断出某种疾病的机会很大程度上受其血细胞类型和计数的影响。因此，血细胞分类非常重要，它可以帮助识别疾病，包括癌症、骨髓损伤、良性肿瘤和它们的生长。这种分类可以帮助血液学家区分不同的血细胞片段，以便确定疾病的原因。卷积神经网络是一种深度学习技术，它将人类血细胞（红细胞、白细胞和血小板）的图像分类为它们的亚型。在这项研究中，使用迁移学习将不同的CNN预训练模型应用于血细胞分类。

    Human blood mainly comprises plasma, red blood cells, white blood cells, and platelets. The blood cells provide the body's cells oxygen to nourish them, shield them from infections, boost immunity, and aid in clotting. Human health is reflected in blood cells. The chances that a human being can be diagnosed with a disease are significantly influenced by their blood cell type and count. Therefore, blood cell classification is crucial because it helps identify diseases, including cancer, damaged bone marrow, benign tumors, and their growth. This classification allows hematologists to distinguish between different blood cell fragments so that the cause of diseases can be identified. Convolution neural networks are a deep learning technique that classifies images of human blood cells (RBCs, WBCs, and platelets) into their subtypes. For this study, transfer learning is used to apply different CNN pre-trained models, including VGG16, VGG19, ResNet-50, ResNet-101, ResNet-152, InceptionV3 Mobi
    
[^72]: 通过机器学习模型对白细胞进行分类的综述

    A Review on Classification of White Blood Cells Using Machine Learning Models. (arXiv:2308.06296v1 [eess.IV])

    [http://arxiv.org/abs/2308.06296](http://arxiv.org/abs/2308.06296)

    本综述系统分析了在医学图像分析领域中应用的机器学习技术，为白细胞分类提供了有价值的洞察力和最佳方法。

    

    机器学习（ML）和深度学习（DL）模型在医学图像分析方面做出了重要贡献。这些模型通过预测和分类提高了预测的准确性，帮助血液学家根据计算和事实来诊断血液癌症和脑肿瘤。本综述主要关注于对白细胞分类的医学图像分析领域中应用的现代技术进行深入分析。针对本综述，讨论了使用血涂片图像、磁共振成像（MRI）、X射线和类似医学影像领域的方法。本综述的主要影响在于对应用于白细胞分类的机器学习技术进行详细分析。这种分析提供了有价值的洞察力，例如最常使用的技术和最佳表现的白细胞分类方法。最近几十年的研究表明，研究人员一直在使用ML和DL方法。

    The machine learning (ML) and deep learning (DL) models contribute to exceptional medical image analysis improvement. The models enhance the prediction and improve the accuracy by prediction and classification. It helps the hematologist to diagnose the blood cancer and brain tumor based on calculations and facts. This review focuses on an in-depth analysis of modern techniques applied in the domain of medical image analysis of white blood cell classification. For this review, the methodologies are discussed that have used blood smear images, magnetic resonance imaging (MRI), X-rays, and similar medical imaging domains. The main impact of this review is to present a detailed analysis of machine learning techniques applied for the classification of white blood cells (WBCs). This analysis provides valuable insight, such as the most widely used techniques and best-performing white blood cell classification methods. It was found that in recent decades researchers have been using ML and DL f
    
[^73]: 带有Polyak步长和线性搜索的自适应SGD: 鲁棒收敛和方差减小

    Adaptive SGD with Polyak stepsize and Line-search: Robust Convergence and Variance Reduction. (arXiv:2308.06058v1 [cs.LG])

    [http://arxiv.org/abs/2308.06058](http://arxiv.org/abs/2308.06058)

    本文提出了AdaSPS和AdaSLS两种新的变种算法，用于解决SGD在非插值环境下的收敛问题，并在训练超参数模型时保持线性和亚线性的收敛速度。

    

    最近提出的随机Polyak步长 (SPS) 和随机线性搜索 (SLS) 在训练超参数模型时显示出了显著的有效性。然而，在非插值环境下，这两种算法只能保证收敛到一个解的邻域，可能导致比初始猜测更差的输出结果。尽管已经提出了人为减小自适应步长的方法来解决这个问题 (Orvieto et al. [2022])，但这种方法会导致凸函数和超参数模型的收敛速度变慢。在本文中，我们做出了两个贡献：首先，我们提出了两种新的SPS和SLS变种，分别称为AdaSPS和AdaSLS，它们在非插值环境中保证收敛，并且在训练超参数模型时保持凸函数和强凸函数的亚线性和线性收敛速度。AdaSLS不需要对问题相关参数的了解，而AdaSPS只需要最优函数值的下界。

    The recently proposed stochastic Polyak stepsize (SPS) and stochastic line-search (SLS) for SGD have shown remarkable effectiveness when training over-parameterized models. However, in non-interpolation settings, both algorithms only guarantee convergence to a neighborhood of a solution which may result in a worse output than the initial guess. While artificially decreasing the adaptive stepsize has been proposed to address this issue (Orvieto et al. [2022]), this approach results in slower convergence rates for convex and over-parameterized models. In this work, we make two contributions: Firstly, we propose two new variants of SPS and SLS, called AdaSPS and AdaSLS, which guarantee convergence in non-interpolation settings and maintain sub-linear and linear convergence rates for convex and strongly convex functions when training over-parameterized models. AdaSLS requires no knowledge of problem-dependent parameters, and AdaSPS requires only a lower bound of the optimal function value 
    
[^74]: AdaER: 一种用于连续终身学习的自适应经验重播方法

    AdaER: An Adaptive Experience Replay Approach for Continual Lifelong Learning. (arXiv:2308.03810v1 [cs.LG])

    [http://arxiv.org/abs/2308.03810](http://arxiv.org/abs/2308.03810)

    AdaER是一种自适应经验重播方法，用于解决连续终身学习中的灾难性遗忘问题。它采用上下文提示的记忆回忆策略，选择性地重播最冲突的记忆。

    

    连续终身学习是一种受人类学习启发的机器学习框架，学习者会以顺序方式持续获取新知识。然而，流式训练数据的非稳态性给这个过程带来了一个重要挑战，即灾难性遗忘，指的是在引入新任务时快速遗忘先前学习的知识。虽然一些方法，如经验重播(ER)，已被提出来缓解这个问题，但它们的性能仍然有限，尤其在增量分类场景中，这被认为是自然而极具挑战性的。在本文中，我们提出了一种新颖的算法，称为自适应经验重播(AdaER)，以解决连续终身学习的挑战。AdaER包括两个阶段：记忆重播和记忆更新。在记忆重播阶段，AdaER引入了一种上下文提示的记忆回忆策略(C-CMR)，选择性地重播最冲突的记忆。

    Continual lifelong learning is an machine learning framework inspired by human learning, where learners are trained to continuously acquire new knowledge in a sequential manner. However, the non-stationary nature of streaming training data poses a significant challenge known as catastrophic forgetting, which refers to the rapid forgetting of previously learned knowledge when new tasks are introduced. While some approaches, such as experience replay (ER), have been proposed to mitigate this issue, their performance remains limited, particularly in the class-incremental scenario which is considered natural and highly challenging. In this paper, we present a novel algorithm, called adaptive-experience replay (AdaER), to address the challenge of continual lifelong learning. AdaER consists of two stages: memory replay and memory update. In the memory replay stage, AdaER introduces a contextually-cued memory recall (C-CMR) strategy, which selectively replays memories that are most conflictin
    
[^75]: G-Mix: 一种通用的混合学习框架，用于平面极小值

    G-Mix: A Generalized Mixup Learning Framework Towards Flat Minima. (arXiv:2308.03236v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.03236](http://arxiv.org/abs/2308.03236)

    G-Mix是一种通用的混合学习框架，通过结合Mixup和锐度感知极小化（SAM）方法的优点，来增强深度神经网络（DNN）的泛化能力。本文还介绍了两种新的算法：二进制G-Mix和分解G-Mix，用于进一步优化DNN性能。

    

    深度神经网络(DNNs)在各种复杂任务中取得了良好的结果。然而，在有限的训练数据可用时，当前的DNNs在过参数化方面面临挑战。为了增强DNNs的泛化能力，Mixup技术变得越来越受欢迎。然而，它仍然产生了次优的结果。受到成功的锐度感知极小化(SAM)方法的启发，该方法建立了训练损失平面的锐度与模型泛化之间的联系，我们提出了一种新的学习框架称为通用混合(G-Mix)，它结合了Mixup和SAM的优点来训练DNN模型。提供的理论分析证明了开发的G-Mix框架如何增强泛化能力。此外，为了进一步优化DNN性能与G-Mix框架，我们介绍了两种新算法：二进制G-Mix和分解G-Mix。这些算法根据训练数据将其分成两个子集。

    Deep neural networks (DNNs) have demonstrated promising results in various complex tasks. However, current DNNs encounter challenges with over-parameterization, especially when there is limited training data available. To enhance the generalization capability of DNNs, the Mixup technique has gained popularity. Nevertheless, it still produces suboptimal outcomes. Inspired by the successful Sharpness-Aware Minimization (SAM) approach, which establishes a connection between the sharpness of the training loss landscape and model generalization, we propose a new learning framework called Generalized-Mixup, which combines the strengths of Mixup and SAM for training DNN models. The theoretical analysis provided demonstrates how the developed G-Mix framework enhances generalization. Additionally, to further optimize DNN performance with the G-Mix framework, we introduce two novel algorithms: Binary G-Mix and Decomposed G-Mix. These algorithms partition the training data into two subsets based 
    
[^76]: 无源域自适应的人体姿势估计

    Source-free Domain Adaptive Human Pose Estimation. (arXiv:2308.03202v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2308.03202](http://arxiv.org/abs/2308.03202)

    提出了无源域自适应的人体姿势估计任务，旨在解决在适应过程中无法访问源数据的跨域学习挑战。通过提出的新框架，源保护模块更有效地保留源信息并抵抗噪声。

    

    人体姿势估计广泛应用于运动分析、医疗保健和虚拟现实等领域。然而，标注实际场景的数据集的巨大开销对姿势估计构成了重要挑战。为了解决这个问题，一种方法是在合成数据集上训练姿势估计模型，然后在真实世界数据上进行域自适应(DA)。然而，现有的HPE的DA方法在适应过程中忽略了数据隐私和安全问题，因为使用了源数据和目标数据。为此，我们提出了一种新的任务，名为无源域自适应的HPE，旨在解决在适应过程中无法访问源数据的HPE的跨域学习挑战。我们进一步提出了一个由三个模型组成的新框架：源模型、中间模型和目标模型，从源数据和目标数据的角度探索该任务。源保护模块更有效地保留源信息并抵抗噪声。

    Human Pose Estimation (HPE) is widely used in various fields, including motion analysis, healthcare, and virtual reality. However, the great expenses of labeled real-world datasets present a significant challenge for HPE. To overcome this, one approach is to train HPE models on synthetic datasets and then perform domain adaptation (DA) on real-world data. Unfortunately, existing DA methods for HPE neglect data privacy and security by using both source and target data in the adaptation process. To this end, we propose a new task, named source-free domain adaptive HPE, which aims to address the challenges of cross-domain learning of HPE without access to source data during the adaptation process. We further propose a novel framework that consists of three models: source model, intermediate model, and target model, which explores the task from both source-protect and target-relevant perspectives. The source-protect module preserves source information more effectively while resisting noise
    
[^77]: 基于AI的智能合约创建的实证研究

    An Empirical Study of AI-based Smart Contract Creation. (arXiv:2308.02955v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2308.02955](http://arxiv.org/abs/2308.02955)

    本研究通过评估LLMs生成的智能合约代码的质量，发现了安全漏洞的重要证据，并评估了输入参数对LLMs的影响。

    

    引入了大型语言模型（LLMs）如ChatGPT和Google Palm2用于智能合约生成似乎是第一个成熟的AI对程序员的实例。LLMs可以访问大量开源智能合约，使它们能够在Solidity中利用比其他代码生成工具更多的代码。虽然对LLMs用于智能合约生成的最初和非正式评估令人充满希望，但需要进行系统评估以探索这些模型的限制和优势。本研究的主要目标是评估LLMs生成的智能合约代码的质量。我们还旨在评估输入参数的质量和多样性对LLMs的影响。为了实现这个目标，我们创建了一个实验设置来评估生成的代码的有效性，正确性和效率。我们的研究发现在生成的智能合约中引入了关键的安全漏洞，同时也影响了整体质量。

    The introduction of large language models (LLMs) like ChatGPT and Google Palm2 for smart contract generation seems to be the first well-established instance of an AI pair programmer. LLMs have access to a large number of open-source smart contracts, enabling them to utilize more extensive code in Solidity than other code generation tools. Although the initial and informal assessments of LLMs for smart contract generation are promising, a systematic evaluation is needed to explore the limits and benefits of these models. The main objective of this study is to assess the quality of generated code provided by LLMs for smart contracts. We also aim to evaluate the impact of the quality and variety of input parameters fed to LLMs. To achieve this aim, we created an experimental setup for evaluating the generated code in terms of validity, correctness, and efficiency. Our study finds crucial evidence of security bugs getting introduced in the generated smart contracts as well as the overall q
    
[^78]: NeRFs: 寻找最佳3D表示的研究

    NeRFs: The Search for the Best 3D Representation. (arXiv:2308.02751v1 [cs.CV])

    [http://arxiv.org/abs/2308.02751](http://arxiv.org/abs/2308.02751)

    NeRFs是视图合成和相关问题中寻找最佳3D表示的结果，该方法利用神经网络查询获取体积参数来描述连续体积场景。

    

    神经辐射场（NeRFs）已成为视图合成或基于图像渲染等问题的首选表示方法，也应用于计算机图形学和计算机视觉等多个领域。NeRFs通过查询神经网络获得视图相关辐射和体积密度等体积参数，将场景表示为连续的体积。该表示方法已广泛应用，每年有数千篇论文在其基础上扩展或相关研究，多位作者和网站提供概述和调研，并有众多工业应用和创业公司。本文简要回顾了NeRFs的表示方法，并描述了长达三十年的寻找最佳3D表示方法以及最终引出NeRFs论文的过程。

    Neural Radiance Fields or NeRFs have become the representation of choice for problems in view synthesis or image-based rendering, as well as in many other applications across computer graphics and vision, and beyond. At their core, NeRFs describe a new representation of 3D scenes or 3D geometry. Instead of meshes, disparity maps, multiplane images or even voxel grids, they represent the scene as a continuous volume, with volumetric parameters like view-dependent radiance and volume density obtained by querying a neural network. The NeRF representation has now been widely used, with thousands of papers extending or building on it every year, multiple authors and websites providing overviews and surveys, and numerous industrial applications and startup companies. In this article, we briefly review the NeRF representation, and describe the three decades-long quest to find the best 3D representation for view synthesis and related problems, culminating in the NeRF papers. We then describe n
    
[^79]: 使用分布感知的自适应优先级附加kNN图

    Adaptive Preferential Attached kNN Graph With Distribution-Awareness. (arXiv:2308.02442v1 [cs.LG])

    [http://arxiv.org/abs/2308.02442](http://arxiv.org/abs/2308.02442)

    本文提出了一种名为paNNG的算法，它结合了自适应kNN和基于分布的图构建。通过包含分布信息，paNNG能够有效提升模糊样本的性能，并实现更好的准确性和泛化能力。

    

    基于图的kNN算法因其简单性和有效性在机器学习任务中广受欢迎。然而，传统的kNN图对于k值的固定依赖可能会影响其性能，特别是在涉及复杂数据分布的情况下。此外，与其他分类模型类似，决策边界上存在的模糊样本常常是一个挑战，因为它们更容易被错误分类。为了解决这些问题，我们提出了优先级附加k-最近邻图（paNNG），它将自适应的kNN与基于分布的图构建相结合。通过结合分布信息，paNNG可以显著提高模糊样本的性能，通过“拉”它们回到原始类别，从而实现改进的整体准确性和泛化能力。通过在多样化的基准数据集上进行严格评估，paNNG的性能超越了现有算法，展示了它的优越性。

    Graph-based kNN algorithms have garnered widespread popularity for machine learning tasks, due to their simplicity and effectiveness. However, the conventional kNN graph's reliance on a fixed value of k can hinder its performance, especially in scenarios involving complex data distributions. Moreover, like other classification models, the presence of ambiguous samples along decision boundaries often presents a challenge, as they are more prone to incorrect classification. To address these issues, we propose the Preferential Attached k-Nearest Neighbors Graph (paNNG), which combines adaptive kNN with distribution-based graph construction. By incorporating distribution information, paNNG can significantly improve performance for ambiguous samples by "pulling" them towards their original classes and hence enable enhanced overall accuracy and generalization capability. Through rigorous evaluations on diverse benchmark datasets, paNNG outperforms state-of-the-art algorithms, showcasing its 
    
[^80]: 通过大型语言模型扩展临床试验匹配：以肿瘤学为案例研究

    Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology. (arXiv:2308.02180v1 [cs.CL])

    [http://arxiv.org/abs/2308.02180](http://arxiv.org/abs/2308.02180)

    本文研究了使用大型语言模型（LLMs）扩展临床试验匹配的方法，并以肿瘤学为案例研究。研究结果显示，先进的LLMs能够处理临床试验的复杂条件和匹配逻辑，相较于之前的方法，性能显著提升，并可作为人工辅助筛选患者-试验候选人的初步解决方案。

    

    临床试验匹配是医疗传递和发现中的关键过程。实际上，由于庞大的非结构化数据和不可扩展的手动处理，该过程存在问题。本文通过以肿瘤学为重点领域，对使用大型语言模型（LLM）扩展临床试验匹配进行了系统研究。我们的研究基于一个正在美国一个大型医疗网络进行测试部署的临床试验匹配系统。初步结果令人鼓舞：先进的LLM（如GPT-4）可以立即连接临床试验的复杂的合格条件，并提取复杂的匹配逻辑（例如嵌套的AND/OR/NOT）。虽然仍不完美，LLM在性能上显著优于以前的强基准线，并可能作为在人与人之间进行候选患者-试验划分的初步解决方案。我们的研究还揭示了一些应用LLM进行端到端临床试验匹配的重要增长领域，例如上下文限制和准确性。

    Clinical trial matching is a key process in health delivery and discovery. In practice, it is plagued by overwhelming unstructured data and unscalable manual processing. In this paper, we conduct a systematic study on scaling clinical trial matching using large language models (LLMs), with oncology as the focus area. Our study is grounded in a clinical trial matching system currently in test deployment at a large U.S. health network. Initial findings are promising: out of box, cutting-edge LLMs, such as GPT-4, can already structure elaborate eligibility criteria of clinical trials and extract complex matching logic (e.g., nested AND/OR/NOT). While still far from perfect, LLMs substantially outperform prior strong baselines and may serve as a preliminary solution to help triage patient-trial candidates with humans in the loop. Our study also reveals a few significant growth areas for applying LLMs to end-to-end clinical trial matching, such as context limitation and accuracy, especially
    
[^81]: FLARE: 使用通用对抗性掩码对指纹深度强化学习智能体进行识别

    FLARE: Fingerprinting Deep Reinforcement Learning Agents using Universal Adversarial Masks. (arXiv:2307.14751v1 [cs.LG])

    [http://arxiv.org/abs/2307.14751](http://arxiv.org/abs/2307.14751)

    FLARE是第一个用于验证疑似深度强化学习策略是否是另一个策略的非法副本的指纹机制，通过使用通用对抗性掩码作为指纹，并测量动作一致性值来验证被盗策略的真实所有权。

    

    我们提出了FLARE，这是第一个用于验证疑似深度强化学习(DRL)策略是否是另一个（受害）策略的非法副本的指纹机制。我们首先展示了通过找到不可传递的、通用的对抗性掩码，即扰动，可以生成成功地从受害策略传递到其修改版本但不能传递到独立训练的策略的对抗性样本。FLARE利用这些掩码作为指纹，通过对通过掩码扰动的状态上的动作一致性值进行测量来验证被盗的DRL策略的真实所有权。我们的实证评估表明，FLARE是有效的（对于被盗副本具有100%的动作一致性），并且不会错误地指控独立策略（无误报）。FLARE还对模型修改攻击具有鲁棒性，并且不容易被更明智的对手规避而对智能体性能产生负面影响。我们还表明，并非所有的通用对抗性掩码都是适用的。

    We propose FLARE, the first fingerprinting mechanism to verify whether a suspected Deep Reinforcement Learning (DRL) policy is an illegitimate copy of another (victim) policy. We first show that it is possible to find non-transferable, universal adversarial masks, i.e., perturbations, to generate adversarial examples that can successfully transfer from a victim policy to its modified versions but not to independently trained policies. FLARE employs these masks as fingerprints to verify the true ownership of stolen DRL policies by measuring an action agreement value over states perturbed via such masks. Our empirical evaluations show that FLARE is effective (100% action agreement on stolen copies) and does not falsely accuse independent policies (no false positives). FLARE is also robust to model modification attacks and cannot be easily evaded by more informed adversaries without negatively impacting agent performance. We also show that not all universal adversarial masks are suitable 
    
[^82]: Temporal Graph Benchmark的实证评估

    An Empirical Evaluation of Temporal Graph Benchmark. (arXiv:2307.12510v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.12510](http://arxiv.org/abs/2307.12510)

    本文对Temporal Graph Benchmark进行了实证评估，通过扩展动态图库(DyGLib)到TGB，并使用十一种动态图学习方法进行全面比较。实验发现不同模型在不同数据集上表现出不同的性能，一些基线方法的性能可以通过使用DyGLib显著提高。

    

    本文通过将我们的动态图库(DyGLib)扩展到Temporal Graph Benchmark (TGB)，对TGB进行了实证评估。与TGB相比，我们包括了十一种流行的动态图学习方法进行更全面的比较。通过实验，我们发现：（1）不同模型在不同数据集上表现出不同的性能，这与之前的观察一致；（2）使用DyGLib时，一些基线方法的性能可以显著提高。本工作旨在方便研究人员在TGB上评估各种动态图学习方法，并试图提供可直接参考的结果供后续研究使用。本项目中使用的所有资源均可在https://github.com/yule-BUAA/DyGLib_TGB上公开获取。本工作正在进行中，欢迎社区提供反馈以进行改进。

    In this paper, we conduct an empirical evaluation of Temporal Graph Benchmark (TGB) by extending our Dynamic Graph Library (DyGLib) to TGB. Compared with TGB, we include eleven popular dynamic graph learning methods for more exhaustive comparisons. Through the experiments, we find that (1) different models depict varying performance across various datasets, which is in line with previous observations; (2) the performance of some baselines can be significantly improved over the reported results in TGB when using DyGLib. This work aims to ease the researchers' efforts in evaluating various dynamic graph learning methods on TGB and attempts to offer results that can be directly referenced in the follow-up research. All the used resources in this project are publicly available at https://github.com/yule-BUAA/DyGLib_TGB. This work is in progress, and feedback from the community is welcomed for improvements.
    
[^83]: 重新思考数据蒸馏：不要忽视校准

    Rethinking Data Distillation: Do Not Overlook Calibration. (arXiv:2307.12463v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.12463](http://arxiv.org/abs/2307.12463)

    本文指出经过蒸馏的数据无法很好地进行校准，因为在这种情况下，网络的 logits 分布更加集中，并且语义明确但与分类任务无关的信息会丢失。为了解决这个问题，我们提出了遮蔽温度缩放 (MTS) 和遮蔽蒸馏训练 (MDT) 方法，以获得更好的校准结果。

    

    在经过蒸馏的数据上训练的神经网络经常产生过于自信的输出，并需要通过校准方法进行修正。现有的校准方法，如温度缩放和混合训练，在原始的大规模数据上训练的神经网络上效果良好。然而，我们发现这些方法无法对从大源数据集蒸馏出的数据进行校准。本文中，我们展示了蒸馏数据会导致网络无法校准，原因是（i）最大logit分布更为集中，以及（ii）在分类任务无关但语义意义明确的信息损失。为了解决这个问题，我们提出了遮蔽温度缩放（MTS）和遮蔽蒸馏训练（MDT）方法，以减轻蒸馏数据的限制，并在保持数据蒸馏效率的同时实现更好的校准结果。

    Neural networks trained on distilled data often produce over-confident output and require correction by calibration methods. Existing calibration methods such as temperature scaling and mixup work well for networks trained on original large-scale data. However, we find that these methods fail to calibrate networks trained on data distilled from large source datasets. In this paper, we show that distilled data lead to networks that are not calibratable due to (i) a more concentrated distribution of the maximum logits and (ii) the loss of information that is semantically meaningful but unrelated to classification tasks. To address this problem, we propose Masked Temperature Scaling (MTS) and Masked Distillation Training (MDT) which mitigate the limitations of distilled data and achieve better calibration results while maintaining the efficiency of dataset distillation.
    
[^84]: Hindsight-DICE：稳定信用分配用于深度强化学习

    Hindsight-DICE: Stable Credit Assignment for Deep Reinforcement Learning. (arXiv:2307.11897v1 [cs.LG])

    [http://arxiv.org/abs/2307.11897](http://arxiv.org/abs/2307.11897)

    本研究提出了Hindsight-DICE算法，利用重要抽样比率估计技术改善了深度强化学习中的信用分配问题。

    

    在顺序决策问题中，环境往往提供很少的评估反馈来指导强化学习代理。在极端情况下，行为的长时间轨迹仅以一个终止信号标记，导致观察到非平凡奖励和触发此类反馈的个体步骤之间存在显著的时间延迟。解决这种信用分配挑战是强化学习的重要特征之一，本研究利用现有的重要抽样比率估计技术来显著改善策略梯度方法中的信用分配处理。虽然使用所谓的事后策略为观察到的轨迹返回返回数据重新加权提供了一个有原则的机制，但是简单地应用重要抽样会导致不稳定或过度滞后的学习。

    Oftentimes, environments for sequential decision-making problems can be quite sparse in the provision of evaluative feedback to guide reinforcement-learning agents. In the extreme case, long trajectories of behavior are merely punctuated with a single terminal feedback signal, engendering a significant temporal delay between the observation of non-trivial reward and the individual steps of behavior culpable for eliciting such feedback. Coping with such a credit assignment challenge is one of the hallmark characteristics of reinforcement learning and, in this work, we capitalize on existing importance-sampling ratio estimation techniques for off-policy evaluation to drastically improve the handling of credit assignment with policy-gradient methods. While the use of so-called hindsight policies offers a principled mechanism for reweighting on-policy data by saliency to the observed trajectory return, naively applying importance sampling results in unstable or excessively lagged learning.
    
[^85]: 基于Kronecker图的可扩展多智能体技能发现

    Scalable Multi-agent Skill Discovery based on Kronecker Graphs. (arXiv:2307.11629v1 [cs.LG])

    [http://arxiv.org/abs/2307.11629](http://arxiv.org/abs/2307.11629)

    本文提出了一种基于Kronecker图的可扩展多智能体技能发现方法，通过连接状态转换图的Fiedler向量，直接计算具有协作探索行为的多智能体技能。

    

    技能发现在强化学习中已经用于改善单智能体场景下稀疏奖励信号的探索，通过连接状态转换图的Fiedler向量所提供的嵌入空间中最远的状态。然而，在多智能体系统中，由于联合状态空间的指数增长，现有研究仍依赖于单智能体技能发现，要么变得难以实现，要么无法直接发现改善联合状态空间连通性的联合技能。本文展示了如何直接计算具有协作探索行为的多智能体技能，同时享受到分解的便利性。我们的关键思想是将联合状态空间近似为一个Kronecker图，基于此我们可以使用各个智能体的转换图的拉普拉斯谱直接估计其Fiedler向量。进一步地，考虑到直接计算拉普拉斯谱在计算上的复杂性，

    Covering skill (a.k.a., option) discovery has been developed to improve the exploration of RL in single-agent scenarios with sparse reward signals, through connecting the most distant states in the embedding space provided by the Fiedler vector of the state transition graph. Given that joint state space grows exponentially with the number of agents in multi-agent systems, existing researches still relying on single-agent option discovery either become prohibitive or fail to directly discover joint options that improve the connectivity of the joint state space. In this paper, we show how to directly compute multi-agent options with collaborative exploratory behaviors while still enjoying the ease of decomposition. Our key idea is to approximate the joint state space as a Kronecker graph, based on which we can directly estimate its Fiedler vector using the Laplacian spectrum of individual agents' transition graphs. Further, considering that directly computing the Laplacian spectrum is in
    
[^86]: 高效的选择性注意力LSTM用于井曲线合成

    Efficient selective attention LSTM for well log curve synthesis. (arXiv:2307.10253v1 [cs.LG])

    [http://arxiv.org/abs/2307.10253](http://arxiv.org/abs/2307.10253)

    本文提出了一种高效的选择性注意力LSTM方法，用于预测缺失的井曲线。通过实验证实了该方法的有效性和可行性。

    

    非核心钻井逐渐成为地质工程中的主要勘探方法，井曲线作为地质信息的主要载体日益重要。然而，地质环境、测井设备、钻孔质量和突发事件等因素都会影响井曲线的质量。以往的重新测井或手工修正方法成本高效率低。本文提出了一种利用现有数据预测缺失井曲线的机器学习方法，并通过实验证实了其有效性和可行性。所提方法在传统的长短期记忆（LSTM）神经网络基础上加入了自注意机制来分析数据的空间依赖性。它有选择地将LSTM中的主导计算结果包括在内，将计算复杂度从O(n^2)降至O(nlogn)，从而提高了计算效率

    Non-core drilling has gradually become the primary exploration method in geological engineering, and well logging curves have increasingly gained importance as the main carriers of geological information. However, factors such as geological environment, logging equipment, borehole quality, and unexpected events can all impact the quality of well logging curves. Previous methods of re-logging or manual corrections have been associated with high costs and low efficiency. This paper proposes a machine learning method that utilizes existing data to predict missing well logging curves, and its effectiveness and feasibility have been validated through experiments. The proposed method builds upon the traditional Long Short-Term Memory (LSTM) neural network by incorporating a self-attention mechanism to analyze the spatial dependencies of the data. It selectively includes the dominant computational results in the LSTM, reducing the computational complexity from O(n^2) to O(nlogn) and improving
    
[^87]: 高效的LLM引导生成

    Efficient Guided Generation for LLMs. (arXiv:2307.09702v1 [cs.CL])

    [http://arxiv.org/abs/2307.09702](http://arxiv.org/abs/2307.09702)

    本文描述了一种使用正则表达式和上下文无关文法来引导语言模型文本生成的高效方法。

    

    在本文中，我们描述了一种使用正则表达式和上下文无关文法来引导语言模型文本生成的高效方法。我们的方法在标记序列生成过程中几乎不增加任何开销，并使得引导生成在实际中可行。在开源Python库Outlines中提供了一个实现。

    In this article we describe an efficient approach to guiding language model text generation with regular expressions and context-free grammars. Our approach adds little to no overhead to the token sequence generation process, and makes guided generation feasible in practice. An implementation is provided in the open source Python library Outlines.
    
[^88]: 手写和打印文本分割：一个签名案例研究

    Handwritten and Printed Text Segmentation: A Signature Case Study. (arXiv:2307.07887v1 [cs.CV])

    [http://arxiv.org/abs/2307.07887](http://arxiv.org/abs/2307.07887)

    本研究旨在解决手写和打印文本分割的挑战，并提出了一种新的方法来完整地恢复不同类别的文本，特别是在重叠部分提高分割性能。同时，还引入了一个新的数据集SignaTR6K，用于支持该任务。

    

    在分析扫描文档时，手写文本可能覆盖打印文本。这在文档的光学字符识别（OCR）和数字化过程中造成困难，并且进而影响到下游的自然语言处理（NLP）任务。之前的研究要么仅关注手写文本的二分类，要么进行三类文档的分割，即手写、打印和背景像素的识别。这导致手写和打印重叠的像素只被分配到一个类别中，因此在另一个类别中不被考虑。因此，在这项研究中，我们开发了新的方法来解决手写和打印文本分割的挑战，目标是完整地恢复不同类别的文本，特别是提高重叠部分的分割性能。为了促进这项任务，我们介绍了一个新的数据集SignaTR6K，该数据集收集自真实的法律文件。

    While analyzing scanned documents, handwritten text can overlay printed text. This causes difficulties during the optical character recognition (OCR) and digitization process of documents, and subsequently, hurts downstream NLP tasks. Prior research either focuses only on the binary classification of handwritten text, or performs a three-class segmentation of the document, i.e., recognition of handwritten, printed, and background pixels. This results in the assignment of the handwritten and printed overlapping pixels to only one of the classes, and thus, they are not accounted for in the other class. Thus, in this research, we develop novel approaches for addressing the challenges of handwritten and printed text segmentation with the goal of recovering text in different classes in whole, especially improving the segmentation performance on the overlapping parts. As such, to facilitate with this task, we introduce a new dataset, SignaTR6K, collected from real legal documents, as well as
    
[^89]: 大型语言模型作为文化角度的叠加

    Large Language Models as Superpositions of Cultural Perspectives. (arXiv:2307.07870v1 [cs.CL])

    [http://arxiv.org/abs/2307.07870](http://arxiv.org/abs/2307.07870)

    大型语言模型被认为是具有个性或一套价值观的，但实际上它可以看作是具有不同价值观和个性特征的角度的叠加。通过角度可控性的概念，我们研究了大型语言模型在不同角度下展示的价值观和个性特征的变化。实验结果表明，即使在没有明显提示的情况下，大型语言模型也会表达出不同的价值观。

    

    大型语言模型（LLMs）常常被错误地认为具有个性或一套价值观。我们认为LLMs可以看作是具有不同价值观和个性特征的角度叠加。LLMs表现出依赖于上下文的价值观和个性特征，这些特征基于产生的角度而改变（与人类相反，人类在不同情境下通常具有更一致的价值观和个性特征）。我们引入了“角度可控性”的概念，指的是模型采用不同具有不同价值观和个性特征的角度的能力。在我们的实验中，我们使用心理学问卷（PVQ、VSM、IPIP）来研究展示的价值观和个性特征如何基于不同角度而改变。通过定性实验，我们展示了当提示中（隐式或显式）暗示了某些价值观时，LLMs表达出不同的价值观，即使在没有明显暗示的情况下，LLMs也会表达出不同的价值观。

    Large Language Models (LLMs) are often misleadingly recognized as having a personality or a set of values. We argue that an LLM can be seen as a superposition of perspectives with different values and personality traits. LLMs exhibit context-dependent values and personality traits that change based on the induced perspective (as opposed to humans, who tend to have more coherent values and personality traits across contexts). We introduce the concept of perspective controllability, which refers to a model's affordance to adopt various perspectives with differing values and personality traits. In our experiments, we use questionnaires from psychology (PVQ, VSM, IPIP) to study how exhibited values and personality traits change based on different perspectives. Through qualitative experiments, we show that LLMs express different values when those are (implicitly or explicitly) implied in the prompt, and that LLMs express different values even when those are not obviously implied (demonstrat
    
[^90]: 提高图神经网络公平性：一种图反事实角度

    Improving Fairness of Graph Neural Networks: A Graph Counterfactual Perspective. (arXiv:2307.04937v1 [cs.LG])

    [http://arxiv.org/abs/2307.04937](http://arxiv.org/abs/2307.04937)

    这项研究提出了以因果视角看待公平图学习问题的框架CAF，通过选择训练数据中的反事实来避免图神经网络中的偏见。

    

    图神经网络在图的表示学习中展现了出色的能力，促进了各种任务的进行。尽管它们在建模图中表现出色，但最近的研究表明，GNN倾向于从训练数据中继承和放大偏见，引起了在高风险场景中使用GNN的担忧。因此，已经做出了许多努力来实现公平感知的GNN。然而，大多数现有的公平GNN通过采用统计公平概念来学习公平节点表示，但在统计异常存在的情况下，这种方法可能无法减轻偏见。受因果理论的启发，有几种方法利用图反事实公平性来减轻不公平的根本原因。然而，这些方法会受到通过扰动或生成获得的非现实反事实的影响。在本文中，我们以因果视角看待公平图学习问题。在因果分析的指导下，我们提出了一种新的框架CAF，它可以从训练数据中选择反事实以避免偏见。

    Graph neural networks have shown great ability in representation (GNNs) learning on graphs, facilitating various tasks. Despite their great performance in modeling graphs, recent works show that GNNs tend to inherit and amplify the bias from training data, causing concerns of the adoption of GNNs in high-stake scenarios. Hence, many efforts have been taken for fairness-aware GNNs. However, most existing fair GNNs learn fair node representations by adopting statistical fairness notions, which may fail to alleviate bias in the presence of statistical anomalies. Motivated by causal theory, there are several attempts utilizing graph counterfactual fairness to mitigate root causes of unfairness. However, these methods suffer from non-realistic counterfactuals obtained by perturbation or generation. In this paper, we take a causal view on fair graph learning problem. Guided by the casual analysis, we propose a novel framework CAF, which can select counterfactuals from training data to avoid 
    
[^91]: 连续学习作为计算受限的强化学习

    Continual Learning as Computationally Constrained Reinforcement Learning. (arXiv:2307.04345v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.04345](http://arxiv.org/abs/2307.04345)

    本文研究了连续学习作为计算受限的强化学习的主题，提出了一个框架和一套工具来解决人工智能领域长期以来的挑战并促进进一步的研究。

    

    一种能够在漫长的生命周期内高效积累知识并发展越来越复杂技能的智能体可以推动人工智能能力的前沿。连续学习这一长期以来一直是人工智能领域的挑战，本文介绍了关于连续学习的概念并提出了一个框架和一套工具，以促进进一步的研究。

    An agent that efficiently accumulates knowledge to develop increasingly sophisticated skills over a long lifetime could advance the frontier of artificial intelligence capabilities. The design of such agents, which remains a long-standing challenge of artificial intelligence, is addressed by the subject of continual learning. This monograph clarifies and formalizes concepts of continual learning, introducing a framework and set of tools to stimulate further research.
    
[^92]: DENCLUE的最优带宽选择

    Optimal Bandwidth Selection for DENCLUE. (arXiv:2307.03206v1 [cs.LG])

    [http://arxiv.org/abs/2307.03206](http://arxiv.org/abs/2307.03206)

    本文提出了一种计算DENCLUE算法最优参数的新方法，并在实验部分讨论了其性能。

    

    在现代工业中，聚类算法是算法工程师的日常工作。尽管在2010年之前，聚类算法经历了快速增长，但在深度学习成为机器学习应用的实际工业标准之后，与该研究主题相关的创新停滞不前。2007年，提出了一种名为DENCLUE的基于密度的聚类算法，用于解决非线性数据结构的聚类问题。然而，直到2011年，该算法的参数选择问题仍然被大部分忽视。本文提出了一种计算DENCLUE算法最优参数的新方法，并在实验部分讨论了其性能。

    In modern day industry, clustering algorithms are daily routines of algorithm engineers. Although clustering algorithms experienced rapid growth before 2010. Innovation related to the research topic has stagnated after deep learning became the de facto industrial standard for machine learning applications. In 2007, a density-based clustering algorithm named DENCLUE was invented to solve clustering problem for nonlinear data structures. However, its parameter selection problem was largely neglected until 2011. In this paper, we propose a new approach to compute the optimal parameters for the DENCLUE algorithm, and discuss its performance in the experiment section.
    
[^93]: Q-Learning 的稳定性通过设计和乐观性

    Stability of Q-Learning Through Design and Optimism. (arXiv:2307.02632v1 [cs.LG])

    [http://arxiv.org/abs/2307.02632](http://arxiv.org/abs/2307.02632)

    本文主要介绍了Q-learning在强化学习中的重要性，以及使用乐观性训练和修改后的策略解决Q-learning的稳定性问题和算法收敛加速问题的方法。

    

    自从20世纪80年代Chris Watkins的论文中介绍以来，Q-learning已成为强化学习工具包中的重要组成部分。本文部分是关于随机逼近和Q-learning的教程，提供了关于INFORMS APS发布的第一届应用概率信托全体大会的详细信息。该论文还提出了确保这些算法的稳定性和可能加速收敛的新方法，以及其他设置中的随机逼近。两个贡献是全新的：1. Q-learning在线性函数逼近下的稳定性一直是一个有待研究的话题。结果表明，通过适当的乐观训练和修改后的Gibbs策略，可以存在满足投影Bellman方程的解，并且该算法是稳定的（参数估计有界）。收敛性仍然是众多待研究的问题之一。2. 新的优化方法在小批量执行中改善了逼近算法的迭代速度。

    Q-learning has become an important part of the reinforcement learning toolkit since its introduction in the dissertation of Chris Watkins in the 1980s. The purpose of this paper is in part a tutorial on stochastic approximation and Q-learning, providing details regarding the INFORMS APS inaugural Applied Probability Trust Plenary Lecture, presented in Nancy France, June 2023.  The paper also presents new approaches to ensure stability and potentially accelerated convergence for these algorithms, and stochastic approximation in other settings. Two contributions are entirely new:  1. Stability of Q-learning with linear function approximation has been an open topic for research for over three decades. It is shown that with appropriate optimistic training in the form of a modified Gibbs policy, there exists a solution to the projected Bellman equation, and the algorithm is stable (in terms of bounded parameter estimates). Convergence remains one of many open topics for research.  2. The ne
    
[^94]: 快速-INR: 使用隐式神经表示进行效率高的无CPU深度神经网络训练

    Rapid-INR: Storage Efficient CPU-free DNN Training Using Implicit Neural Representation. (arXiv:2306.16699v1 [cs.CV])

    [http://arxiv.org/abs/2306.16699](http://arxiv.org/abs/2306.16699)

    本文提出了一种使用隐式神经表示进行高效的无CPU深度神经网络训练的新方法，通过在GPU上直接存储整个数据集以INR格式，减少了数据传输开销，从而加速训练过程。同时，采用高度并行化和实时执行的解码过程，进一步提升了压缩效果。

    

    隐式神经表示(INR)是一种创新方法，用于表示复杂的形状或对象，而无需明确定义它们的几何形状或表面结构。相反，INR将对象表示为连续函数。先前的研究已经证明了将神经网络用作INR进行图像压缩的有效性，展示了与传统方法（如JPEG）相当的性能。然而，INR在图像压缩之外还具有各种应用潜力。本文介绍了Rapid-INR，一种利用INR对图像进行编码和压缩的新方法，从而加速计算机视觉任务中的神经网络训练。我们的方法在GPU上直接以INR格式存储整个数据集，减少了训练过程中CPU和GPU之间的数据传输开销。此外，从INR到RGB格式的解码过程高度并行化并实时执行。为了进一步提高压缩效果，我们提出了一种迭代的图像压缩算法。

    Implicit Neural Representation (INR) is an innovative approach for representing complex shapes or objects without explicitly defining their geometry or surface structure. Instead, INR represents objects as continuous functions. Previous research has demonstrated the effectiveness of using neural networks as INR for image compression, showcasing comparable performance to traditional methods such as JPEG. However, INR holds potential for various applications beyond image compression. This paper introduces Rapid-INR, a novel approach that utilizes INR for encoding and compressing images, thereby accelerating neural network training in computer vision tasks. Our methodology involves storing the whole dataset directly in INR format on a GPU, mitigating the significant data communication overhead between the CPU and GPU during training. Additionally, the decoding process from INR to RGB format is highly parallelized and executed on-the-fly. To further enhance compression, we propose iterativ
    
[^95]: 持续性深度学习中的可塑性维护

    Maintaining Plasticity in Deep Continual Learning. (arXiv:2306.13812v1 [cs.LG])

    [http://arxiv.org/abs/2306.13812](http://arxiv.org/abs/2306.13812)

    持续性学习中，深度学习系统可能会失去适应新数据的能力，我们提出了一种名为对比可塑性的方法来解决这个问题。

    

    现代深度学习系统专门用于一次性训练，而不是持续性学习，如果将深度学习系统应用于持续性学习中，则众所周知它们可能在记住早期的例子方面遭遇失败。更为基本但不为人知的是，它们也可能失去适应新数据的能力，这种现象被称为“可塑性丧失”。我们展示了使用MNIST和ImageNet数据集重构为一系列任务的持续学习中的可塑性丧失。在ImageNet中，二元分类的性能从一个早期任务的89％正确下降到77％，或者大约等于线性网络的水平。这种可塑性的丧失发生在各种深层网络架构，优化器和激活函数范围内，并且不会因批量归一化或放弃而得到缓解。在我们的实验中，通过我们提出的方法Contrastive Plasticity，可以缓解可塑性的丧失，该方法学习适应新的数据同时保留记住旧数据的能力。Contrastive Plasticity可以添加到任何神经网络中，而无需修改网络的架构，并带来非常少的计算开销。

    Modern deep-learning systems are specialized to problem settings in which training occurs once and then never again, as opposed to continual-learning settings in which training occurs continually. If deep-learning systems are applied in a continual learning setting, then it is well known that they may fail catastrophically to remember earlier examples. More fundamental, but less well known, is that they may also lose their ability to adapt to new data, a phenomenon called \textit{loss of plasticity}. We show loss of plasticity using the MNIST and ImageNet datasets repurposed for continual learning as sequences of tasks. In ImageNet, binary classification performance dropped from 89% correct on an early task down to 77%, or to about the level of a linear network, on the 2000th task. Such loss of plasticity occurred with a wide range of deep network architectures, optimizers, and activation functions, and was not eased by batch normalization or dropout. In our experiments, loss of plasti
    
[^96]: TACOformer: 多模态情感识别中的令牌通道复合交叉注意力模型

    TACOformer:Token-channel compounded Cross Attention for Multimodal Emotion Recognition. (arXiv:2306.13592v1 [cs.MM])

    [http://arxiv.org/abs/2306.13592](http://arxiv.org/abs/2306.13592)

    TACOformer 提出了一种综合性的多模态融合视角，利用 Token-chAnnel COmpound（TACO）Cross Attention 模块，同时建模通道级别和令牌级别的跨模态交互，实现了在多模态情感识别任务上具有先进性能。

    

    最近，基于生理信号的情感识别成为了一个进行深入研究的领域。利用多模态、多通道的生理信号显著提高了情感识别系统的性能，因为它们具有互补性。然而，有效地整合来自不同模态的与情感相关的语义信息并捕获跨模态的依赖关系仍然是一个具有挑战性的问题。许多现有的多模态融合方法忽略了多个通道信号之间的令牌到令牌或通道到通道的相关性，这在一定程度上限制了模型的分类能力。在本文中，我们提出了一种综合性的多模态融合视角，它整合了通道级别和令牌级别的跨模态交互，采用复合机制进行跨通道处理。特别是，我们引入了一种统一的交叉注意力模块，称为 Token-chAnnel COmpound（TACO）Cross Attention，用于执行多模态融合。实验结果表明，TACOformer 方法在多模态情感识别任务上具有先进性能。

    Recently, emotion recognition based on physiological signals has emerged as a field with intensive research. The utilization of multi-modal, multi-channel physiological signals has significantly improved the performance of emotion recognition systems, due to their complementarity. However, effectively integrating emotion-related semantic information from different modalities and capturing inter-modal dependencies remains a challenging issue. Many existing multimodal fusion methods ignore either token-to-token or channel-to-channel correlations of multichannel signals from different modalities, which limits the classification capability of the models to some extent. In this paper, we propose a comprehensive perspective of multimodal fusion that integrates channel-level and token-level cross-modal interactions. Specifically, we introduce a unified cross attention module called Token-chAnnel COmpound (TACO) Cross Attention to perform multimodal fusion, which simultaneously models channel-
    
[^97]: 迈向平衡多模态分类的主动学习

    Towards Balanced Active Learning for Multimodal Classification. (arXiv:2306.08306v2 [cs.MM] UPDATED)

    [http://arxiv.org/abs/2306.08306](http://arxiv.org/abs/2306.08306)

    本研究针对多模态分类中的不公平问题，提出了三个设计准则，并提出了一种新的方法来实现更公平的数据选择。研究结果表明，该方法可以实现更平衡的多模态学习。

    

    由于其较大的参数空间，训练多模态网络需要大量的数据。主动学习是一种广泛使用的技术，通过选择只对改善模型性能有贡献的样本，来减少数据注释成本。然而，当前的主动学习策略主要针对单模态任务设计，当应用于多模态数据时，往往会导致从主导模态中选择样本的偏见。这种不公平阻碍了平衡的多模态学习，这对于实现最佳性能至关重要。为了解决这个问题，我们提出了三个设计更平衡的多模态主动学习策略的准则。在遵循这些准则的基础上，我们提出了一种新的方法，通过在模态之间调节梯度嵌入和主导程度，实现更公平的数据选择。我们的研究表明，所提出的方法通过避免贪婪地选择样本，实现了更平衡的多模态学习。

    Training multimodal networks requires a vast amount of data due to their larger parameter space compared to unimodal networks. Active learning is a widely used technique for reducing data annotation costs by selecting only those samples that could contribute to improving model performance. However, current active learning strategies are mostly designed for unimodal tasks, and when applied to multimodal data, they often result in biased sample selection from the dominant modality. This unfairness hinders balanced multimodal learning, which is crucial for achieving optimal performance. To address this issue, we propose three guidelines for designing a more balanced multimodal active learning strategy. Following these guidelines, a novel approach is proposed to achieve more fair data selection by modulating the gradient embedding with the dominance degree among modalities. Our studies demonstrate that the proposed method achieves more balanced multimodal learning by avoiding greedy sample
    
[^98]: 巨型语言模型时代的AutoML：当前挑战，未来机遇和风险。

    AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks. (arXiv:2306.08107v1 [cs.LG])

    [http://arxiv.org/abs/2306.08107](http://arxiv.org/abs/2306.08107)

    论文探讨了AutoML和LLMs之间的共生关系，并指出这两个领域的融合有望颠覆NLP和AutoML两个领域，同时也存在风险。

    

    在过去的几年中，自然语言处理（NLP）和自动化机器学习（AutoML）领域取得了显著的成果。特别是在NLP领域，巨型语言模型（LLMs）最近经历了一系列突破。我们设想，两个领域通过紧密的融合可以彼此推动极限。为了展示这一愿景，我们探索了AutoML和LLMs之间的共生关系潜力，着重探讨了它们如何互相受益。我们特别研究了从不同角度增强LLMs的AutoML方法的机会以及利用AutoML进一步改进LLMs的挑战。为此，我们调查了现有工作，并对其中的风险进行了批判性评估。我们坚信，两个领域的融合有可能颠覆NLP和AutoML两个领域。通过强调可想象的协同作用和风险，我们旨在促进在交叉点的进一步探索。

    The fields of both Natural Language Processing (NLP) and Automated Machine Learning (AutoML) have achieved remarkable results over the past years. In NLP, especially Large Language Models (LLMs) have experienced a rapid series of breakthroughs very recently. We envision that the two fields can radically push the boundaries of each other through tight integration. To showcase this vision, we explore the potential of a symbiotic relationship between AutoML and LLMs, shedding light on how they can benefit each other. In particular, we investigate both the opportunities to enhance AutoML approaches with LLMs from different perspectives and the challenges of leveraging AutoML to further improve LLMs. To this end, we survey existing work, and we critically assess risks. We strongly believe that the integration of the two fields has the potential to disrupt both fields, NLP and AutoML. By highlighting conceivable synergies, but also risks, we aim to foster further exploration at the intersect
    
[^99]: 黑盒优化的扩散模型

    Diffusion Models for Black-Box Optimization. (arXiv:2306.07180v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.07180](http://arxiv.org/abs/2306.07180)

    提出了一种基于扩散模型的离线黑盒优化的反向方法，称为去噪扩散优化模型（DDOM）。DDOM通过学习黑盒函数值条件下的生成模型来优化黑盒函数，克服了数据集质量和高维度下一对多映射困难的限制。

    

    离线黑盒优化的目标是利用固定的函数评估数据集来优化一个昂贵的黑盒函数。过去的工作考虑了前向方法和反向方法。前向方法学习黑盒函数的替代模型，而反向方法直接将函数值映射到输入域的相应点。然而，这些方法受限于离线数据集的质量和在高维度中学习一对多映射的困难。我们提出了一种基于扩散模型的离线黑盒优化的新的反向方法，称为去噪扩散优化模型（DDOM）。给定一个离线数据集，DDOM学习一个基于黑盒函数域上的条件生成模型，该模型以函数值为条件。我们研究了DDOM中的几种设计选择，例如对数据集进行重新加权以便重点关注高函数值，以及在测试时使用无分类器指导以实现泛化能力。

    The goal of offline black-box optimization (BBO) is to optimize an expensive black-box function using a fixed dataset of function evaluations. Prior works consider forward approaches that learn surrogates to the black-box function and inverse approaches that directly map function values to corresponding points in the input domain of the black-box function. These approaches are limited by the quality of the offline dataset and the difficulty in learning one-to-many mappings in high dimensions, respectively. We propose Denoising Diffusion Optimization Models (DDOM), a new inverse approach for offline black-box optimization based on diffusion models. Given an offline dataset, DDOM learns a conditional generative model over the domain of the black-box function conditioned on the function values. We investigate several design choices in DDOM, such as re-weighting the dataset to focus on high function values and the use of classifier-free guidance at test-time to enable generalization to fun
    
[^100]: 基于分布式多智能体强化学习的异构交通意图感知规划算法iPLAN

    iPLAN: Intent-Aware Planning in Heterogeneous Traffic via Distributed Multi-Agent Reinforcement Learning. (arXiv:2306.06236v1 [cs.MA])

    [http://arxiv.org/abs/2306.06236](http://arxiv.org/abs/2306.06236)

    论文提出了基于MARL算法的iPLAN方法，可在高密度且异构交通场景下进行意图感知规划，使智能体能够从局部观测中推断附近驾驶者的意图，并通过行为或瞬时激励进行决策，实现自主导航。

    

    在高密度和异构交通场景中保障自动驾驶汽车（AVs）的安全和效率面临较大挑战，因为它们无法推断附近驾驶者的行为或意图。本文提出了一种具有轨迹和意图预测的分布式多智能体强化学习（MARL）算法，用于在高密度和异构交通场景中进行意图感知规划。我们的iPLAN方法使智能体仅从其本地观测中推断附近驾驶者的意图。我们模拟了两个不同的激励因素：行为激励用于智能体的长期规划，基于它们的驾驶行为或个性；瞬时激励用于智能体的短期规划，以基于当前交通状态进行碰撞避免。我们设计了一个双流推理模块，使智能体能够推断对手的激励并将其推断信息纳入决策。我们在两个模拟环境中进行了实验。

    Navigating safely and efficiently in dense and heterogeneous traffic scenarios is challenging for autonomous vehicles (AVs) due to their inability to infer the behaviors or intentions of nearby drivers. In this work, we propose a distributed multi-agent reinforcement learning (MARL) algorithm with trajectory and intent prediction in dense and heterogeneous traffic scenarios. Our approach for intent-aware planning, iPLAN, allows agents to infer nearby drivers' intents solely from their local observations. We model two distinct incentives for agents' strategies: Behavioral incentives for agents' long-term planning based on their driving behavior or personality; Instant incentives for agents' short-term planning for collision avoidance based on the current traffic state. We design a two-stream inference module that allows agents to infer their opponents' incentives and incorporate their inferred information into decision-making. We perform experiments on two simulation environments, Non-C
    
[^101]: 任务关系感知的持续用户表示学习

    Task Relation-aware Continual User Representation Learning. (arXiv:2306.01792v1 [cs.IR])

    [http://arxiv.org/abs/2306.01792](http://arxiv.org/abs/2306.01792)

    本文提出了一种新的持续用户表示学习方法TERACON，它能够学习通用的用户表示，而不是为每个任务学习任务特定的用户表示，具有很强的实用性和学习能力。

    

    用户建模是基于其过去行为学习将用户表示为低维表示空间的方法，它受到了工业界提供个性化服务的兴趣激增。以往的用户建模工作主要集中在学习为单一任务而设计的任务特定用户表示上。然而，由于为每个任务学习任务特定用户表示是不可行的，因此最近的研究引入了通用用户表示的概念，即与多种任务相关的更广义用户表示。尽管这些方法非常有效，但由于数据需求、灾难性遗忘以及为持续添加的任务提供有限的学习能力，现有的学习通用用户表示的方法在实际应用中是不切实际的。本文提出了一种新颖的持续用户表示学习方法TERACON，其学习能力不受任务数量限制。

    User modeling, which learns to represent users into a low-dimensional representation space based on their past behaviors, got a surge of interest from the industry for providing personalized services to users. Previous efforts in user modeling mainly focus on learning a task-specific user representation that is designed for a single task. However, since learning task-specific user representations for every task is infeasible, recent studies introduce the concept of universal user representation, which is a more generalized representation of a user that is relevant to a variety of tasks. Despite their effectiveness, existing approaches for learning universal user representations are impractical in real-world applications due to the data requirement, catastrophic forgetting and the limited learning capability for continually added tasks. In this paper, we propose a novel continual user representation learning method, called TERACON, whose learning capability is not limited as the number 
    
[^102]: 非线性循环神经网络的逆近似理论

    Inverse Approximation Theory for Nonlinear Recurrent Neural Networks. (arXiv:2305.19190v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19190](http://arxiv.org/abs/2305.19190)

    该论文证明了使用RNNs逼近非线性序列关系的逆近似定理，进一步将先前在线性RNNs中识别出的记忆难题推广到了一般的非线性情况，并提出了一个有原则的重新参数化方法来克服这些限制。

    

    我们证明了使用RNNs来逼近非线性序列关系的逆近似定理。这是近似理论中的一种称为Bernstein型结果的结果，它在假设目标函数可以通过假设空间有效逼近的条件下推导出目标函数的属性。特别地，我们展示了非线性序列关系可以被具有hardtanh/tanh激活函数的RNNs稳定逼近的时候，必须具有一个指数衰减的记忆结构--这个概念可以被明确定义。这将先前在线性RNNs中识别出的记忆难题推广到了一般的非线性情况，并量化了RNN架构在学习具有长期记忆的序列关系时的重要限制。基于分析，我们提出了一个有原则的重新参数化方法来克服这些限制。我们的理论结果通过数值实验进行了确认。

    We prove an inverse approximation theorem for the approximation of nonlinear sequence-to-sequence relationships using RNNs. This is a so-called Bernstein-type result in approximation theory, which deduces properties of a target function under the assumption that it can be effectively approximated by a hypothesis space. In particular, we show that nonlinear sequence relationships, viewed as functional sequences, that can be stably approximated by RNNs with hardtanh/tanh activations must have an exponential decaying memory structure -- a notion that can be made precise. This extends the previously identified curse of memory in linear RNNs into the general nonlinear setting, and quantifies the essential limitations of the RNN architecture for learning sequential relationships with long-term memory. Based on the analysis, we propose a principled reparameterization method to overcome the limitations. Our theoretical results are confirmed by numerical experiments.
    
[^103]: 机器学习驱动的分布式系统认证之路

    Towards Certification of Machine Learning-Based Distributed Systems. (arXiv:2305.16822v1 [cs.LG])

    [http://arxiv.org/abs/2305.16822](http://arxiv.org/abs/2305.16822)

    认证技术对于机器学习驱动的分布式系统验证不适用，本文提出了第一个ML-based分布式系统认证方案，以验证其非功能属性。

    

    机器学习（ML）日益被用于驱动部署在5G云边缘连续体上的复杂分布式系统的运行。相应地，分布式系统的行为变得更具非确定性。这种分布式系统的演化需要定义新的保证方法来验证非功能属性。认证作为系统和软件验证的最流行的保证技术，不能立即适用于其行为由基于机器学习的推理决定的系统。然而，政策制定者、监管机构和产业利益相关者越来越推崇定义ML的非功能属性（如公平性、鲁棒性、隐私）的认证技术。本文分析了当前认证方案的挑战和不足之处，讨论了开放的研究问题，并提出了第一个ML-based分布式系统认证方案。

    Machine Learning (ML) is increasingly used to drive the operation of complex distributed systems deployed on the cloud-edge continuum enabled by 5G. Correspondingly, distributed systems' behavior is becoming more non-deterministic in nature. This evolution of distributed systems requires the definition of new assurance approaches for the verification of non-functional properties. Certification, the most popular assurance technique for system and software verification, is not immediately applicable to systems whose behavior is determined by Machine Learning-based inference. However, there is an increasing push from policy makers, regulators, and industrial stakeholders towards the definition of techniques for the certification of non-functional properties (e.g., fairness, robustness, privacy) of ML. This article analyzes the challenges and deficiencies of current certification schemes, discusses open research issues and proposes a first certification scheme for ML-based distributed syst
    
[^104]: 在表格数据上进行深度异常检测的超越个体输入

    Beyond Individual Input for Deep Anomaly Detection on Tabular Data. (arXiv:2305.15121v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.15121](http://arxiv.org/abs/2305.15121)

    本文提出了一种采用非参数转换器的深度异常检测方法，能够捕捉表格数据中特征与特征之间以及样本与样本之间的依赖关系，并在广泛的表格数据集上取得了比现有最先进方法更优的性能。

    

    异常检测在金融、医疗和网络安全等各个领域都至关重要。在本文中，我们提出了一种新颖的基于非参数转换器（NPTs）的表格数据深度异常检测方法，以捕捉特征与特征之间以及样本与样本之间的依赖关系。在基于重构的框架中，我们训练NPT来重构正常样本的遮蔽特征。以非参数化方式，在推理过程中利用整个训练集，并利用模型在生成异常得分时重构遮蔽特征的能力。据我们所知，我们提出的方法是首个成功结合特征之间和样本之间依赖关系进行表格数据异常检测的方法。我们在31个表格数据集的广泛基准测试中评估了我们的方法，并证明我们的方法在F1得分和AUROC方面优于现有的最先进方法。

    Anomaly detection is crucial in various domains, such as finance, healthcare, and cybersecurity. In this paper, we propose a novel deep anomaly detection method for tabular data that leverages Non-Parametric Transformers (NPTs), a model initially proposed for supervised tasks, to capture both feature-feature and sample-sample dependencies. In a reconstruction-based framework, we train the NPT to reconstruct masked features of normal samples. In a non-parametric fashion, we leverage the whole training set during inference and use the model's ability to reconstruct the masked features during to generate an anomaly score. To the best of our knowledge, our proposed method is the first to successfully combine feature-feature and sample-sample dependencies for anomaly detection on tabular datasets. We evaluate our method on an extensive benchmark of 31 tabular datasets and demonstrate that our approach outperforms existing state-of-the-art methods based on the F1-score and AUROC by a signifi
    
[^105]: GraphFC: 带有少量标签数据的海关欺诈检测

    GraphFC: Customs Fraud Detection with Label Scarcity. (arXiv:2305.11377v1 [cs.LG])

    [http://arxiv.org/abs/2305.11377](http://arxiv.org/abs/2305.11377)

    GraphFC是一个模型不可知、领域特定、半监督图神经网络框架，用于少量标签数据的海关欺诈检测。它利用了图神经网络和半监督学习，有效地结合了标记和未标记的数据，提高欺诈检测的性能。

    

    全球的海关官员每年面对着海量的交易。随着连通性和全球化的增加，海关交易持续增长。与海关交易相关的是海关欺诈——即有意修改货物申报以避免税款和关税。由于人手不足，海关办公室只能对有限数量的申报进行手动检查。这需要通过机器学习（ML）技术自动化海关欺诈检测。然而，由于新进申报检查的标签数据受限，ML方法应具有稳健的性能。本文提出了$\textbf{GraphFC}$（海关欺诈$\textbf{GNN}$（$\textbf{G}$raph $\textbf{N}$eural $\textbf{N}$etworks）），这是一个模型不可知、领域特定、半监督图神经网络框架，用于少量标签数据的欺诈检测。$\textbf{GraphFC}$利用图神经网络和半监督学习的最新进展，有效地结合标记和未标记数据，提高了欺诈检测的性能。我们在实际的海关交易数据集上评估了我们的提议框架，并展示了它在标签数据稀缺的情况下检测海关欺诈的有效性。

    Custom officials across the world encounter huge volumes of transactions. With increased connectivity and globalization, the customs transactions continue to grow every year. Associated with customs transactions is the customs fraud - the intentional manipulation of goods declarations to avoid the taxes and duties. With limited manpower, the custom offices can only undertake manual inspection of a limited number of declarations. This necessitates the need for automating the customs fraud detection by machine learning (ML) techniques. Due the limited manual inspection for labeling the new-incoming declarations, the ML approach should have robust performance subject to the scarcity of labeled data. However, current approaches for customs fraud detection are not well suited and designed for this real-world setting. In this work, we propose $\textbf{GraphFC}$ ($\textbf{Graph}$ neural networks for $\textbf{C}$ustoms $\textbf{F}$raud), a model-agnostic, domain-specific, semi-supervised graph
    
[^106]: 基于联邦学习的多语言帕金森病检测模型的安全开发

    Federated learning for secure development of AI models for Parkinson's disease detection using speech from different languages. (arXiv:2305.11284v1 [eess.AS])

    [http://arxiv.org/abs/2305.11284](http://arxiv.org/abs/2305.11284)

    本论文利用联邦学习方法，无需共享患者数据，实现在德语、西班牙语和捷克语三种语言数据集上进行帕金森病检测，取得了优于本地模型的诊断准确性。

    

    帕金森病是一种影响人类说话的神经系统疾病。深度学习模型在自动化帕金森病评估中表现出了出色的性能，但严格的患者数据隐私法规阻碍了机构间共享数据。本文在不共享患者数据的前提下，利用联邦学习在德语、西班牙语和捷克语等三种不同语言的真实数据集上进行了帕金森病检测，并取得了优于本地模型的诊断准确性。

    Parkinson's disease (PD) is a neurological disorder impacting a person's speech. Among automatic PD assessment methods, deep learning models have gained particular interest. Recently, the community has explored cross-pathology and cross-language models which can improve diagnostic accuracy even further. However, strict patient data privacy regulations largely prevent institutions from sharing patient speech data with each other. In this paper, we employ federated learning (FL) for PD detection using speech signals from 3 real-world language corpora of German, Spanish, and Czech, each from a separate institution. Our results indicate that the FL model outperforms all the local models in terms of diagnostic accuracy, while not performing very differently from the model based on centrally combined training sets, with the advantage of not requiring any data sharing among collaborators. This will simplify inter-institutional collaborations, resulting in enhancement of patient outcomes.
    
[^107]: MPI-rical：基于Transformer的数据驱动MPI分布式并行辅助

    MPI-rical: Data-Driven MPI Distributed Parallelism Assistance with Transformers. (arXiv:2305.09438v1 [cs.DC])

    [http://arxiv.org/abs/2305.09438](http://arxiv.org/abs/2305.09438)

    本文提出了一种基于Transformer模型的新方法MPI-rical，通过对大量代码片段进行训练实现自动化MPI代码生成，使并行化成为可能。

    

    在高性能计算中，将串行代码自动并行化以支持共享内存和分布式内存系统是一项具有挑战性的任务。虽然许多尝试将串行代码转换为共享内存环境的并行代码（通常使用OpenMP），但没有任何一项尝试成功将其转化为分布式内存环境。本文提出了一种称为MPI-rical的新方法，通过基于Transformer模型对大约25,000个串行代码片段及其对应的并行MPI代码进行训练，从我们的语料库（MPICodeCorpus）的50,000多个代码片段中生成自动化MPI代码。为了评估模型的性能，我们首先将串行代码转换为基于MPI的并行代码翻译问题分解为两个子问题，并制定两个研究目标：代码补全，即在给定源代码中的某个位置，预测该位置的MPI函数；代码翻译，即预测一个MPI函数。

    Automatic source-to-source parallelization of serial code for shared and distributed memory systems is a challenging task in high-performance computing. While many attempts were made to translate serial code into parallel code for a shared memory environment (usually using OpenMP), none has managed to do so for a distributed memory environment. In this paper, we propose a novel approach, called MPI-rical, for automated MPI code generation using a transformer-based model trained on approximately 25,000 serial code snippets and their corresponding parallelized MPI code out of more than 50,000 code snippets in our corpus (MPICodeCorpus). To evaluate the performance of the model, we first break down the serial code to MPI-based parallel code translation problem into two sub-problems and develop two research objectives: code completion defined as given a location in the source code, predict the MPI function for that location, and code translation defined as predicting an MPI function as wel
    
[^108]: IVP-VAE: 利用初值问题求解器对电子病历时间序列进行建模

    IVP-VAE: Modeling EHR Time Series with Initial Value Problem Solvers. (arXiv:2305.06741v1 [cs.LG])

    [http://arxiv.org/abs/2305.06741](http://arxiv.org/abs/2305.06741)

    本文提出了一种新的方法，在建模电子病历时间序列时，利用直接近似IVP的过程来消除递归计算，从而提高计算效率和训练速度。与目前基于IVP求解器和递归神经网络方法相比，本方法可以达到类似的分类和预测性能。

    

    连续时间模型（例如神经ODE和神经流量）在分析电子病历中常见的不规则采样时间序列方面显示出有希望的结果。 基于这些模型，时间序列通常在变分自动编码器架构中通过初值问题（IVP）求解器和递归神经网络的混合处理。 顺序求解IVP使得这样的模型在计算效率上不够高。 本文提出了一种纯粹使用连续过程对时间序列进行建模的方法，其状态演变可以通过IVP直接近似。 这消除了递归计算的需要，并允许多个状态并行演变。 我们进一步通过一种基于其可逆性的IVP求解器融合编码器和解码器，这导致参数更少，收敛更快。 在三个真实世界的数据集上进行的实验表明，所提出的方法在获得更快的训练速度的同时，仍然可以获得较高的分类性能和预测性能。

    Continuous-time models such as Neural ODEs and Neural Flows have shown promising results in analyzing irregularly sampled time series frequently encountered in electronic health records. Based on these models, time series are typically processed with a hybrid of an initial value problem (IVP) solver and a recurrent neural network within the variational autoencoder architecture. Sequentially solving IVPs makes such models computationally less efficient. In this paper, we propose to model time series purely with continuous processes whose state evolution can be approximated directly by IVPs. This eliminates the need for recurrent computation and enables multiple states to evolve in parallel. We further fuse the encoder and decoder with one IVP solver based on its invertibility, which leads to fewer parameters and faster convergence. Experiments on three real-world datasets show that the proposed approach achieves comparable extrapolation and classification performance while gaining more 
    
[^109]: 使用梯度下降学习决策树

    Learning Decision Trees with Gradient Descent. (arXiv:2305.03515v1 [cs.LG])

    [http://arxiv.org/abs/2305.03515](http://arxiv.org/abs/2305.03515)

    本文提出了一种使用梯度下降学习决策树的新方法，可以联合优化所有树的参数，从而避免了贪心算法造成次优解的问题。该方法在二分类任务上表现优异，并在多类任务中达到有竞争力的结果。

    

    决策树是用于许多机器学习任务的常见工具，因为它们具有高度的解释性。然而，从数据中学习决策树是一个困难的优化问题，因为它是非凸和非可微的。因此，通常的方法是使用一种贪婪生长算法来学习决策树，在每个内部节点上局部最小化不纯度。不幸的是，这种贪心过程可能会导致次优的决策树。在本文中，我们提出了一种使用梯度下降学习难以处理的轴对齐决策树的新方法。所提出的方法使用反向传播和直通算子在密集的决策树表示上联合优化所有树的参数。我们的方法在二分类基准测试上优于现有方法，并在多类任务中实现了有竞争力的结果。

    Decision Trees (DTs) are commonly used for many machine learning tasks due to their high degree of interpretability. However, learning a DT from data is a difficult optimization problem, as it is non-convex and non-differentiable. Therefore, common approaches learn DTs using a greedy growth algorithm that minimizes the impurity locally at each internal node. Unfortunately, this greedy procedure can lead to suboptimal trees. In this paper, we present a novel approach for learning hard, axis-aligned DTs with gradient descent. The proposed method uses backpropagation with a straight-through operator on a dense DT representation to jointly optimize all tree parameters. Our approach outperforms existing methods on binary classification benchmarks and achieves competitive results for multi-class tasks.
    
[^110]: FedAVO：利用非洲秃鹫优化器提高联邦学习中的通信效率

    FedAVO: Improving Communication Efficiency in Federated Learning with African Vultures Optimizer. (arXiv:2305.01154v1 [cs.LG])

    [http://arxiv.org/abs/2305.01154](http://arxiv.org/abs/2305.01154)

    本文介绍了一种名为FedAVO的算法，利用非洲秃鹫优化器选择最佳超参数来提高联邦学习中的通信效率，显著降低了与FL操作相关的通信成本。

    

    近年来，分布式机器学习技术联邦学习（FL）由于强调用户数据隐私保护而变得越来越受欢迎。然而，FL的分布式计算可能导致通信受限并且学习过程变得拖延，需要对客户-服务器通信成本进行优化。选择的客户比例和本地训练循环次数是对FL性能有重大影响的两个超参数。由于在各种应用程序中存在不同的训练偏好，因此FL从业者很难手动选择这些超参数。在我们的研究论文中，我们介绍了FedAVO，这是一种新的FL算法，通过利用非洲秃鹫优化器（AVO）选择最佳超参数来增强通信效率。我们的研究表明，采用AVO进行FL超参数调整可以大大减少与FL操作相关的通信成本。

    Federated Learning (FL), a distributed machine learning technique has recently experienced tremendous growth in popularity due to its emphasis on user data privacy. However, the distributed computations of FL can result in constrained communication and drawn-out learning processes, necessitating the client-server communication cost optimization. The ratio of chosen clients and the quantity of local training passes are two hyperparameters that have a significant impact on FL performance. Due to different training preferences across various applications, it can be difficult for FL practitioners to manually select such hyperparameters. In our research paper, we introduce FedAVO, a novel FL algorithm that enhances communication effectiveness by selecting the best hyperparameters leveraging the African Vulture Optimizer (AVO). Our research demonstrates that the communication costs associated with FL operations can be substantially reduced by adopting AVO for FL hyperparameter adjustment. Th
    
[^111]: SelfDocSeg: 一种自我监督视觉文档分割方法

    SelfDocSeg: A Self-Supervised vision-based Approach towards Document Segmentation. (arXiv:2305.00795v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.00795](http://arxiv.org/abs/2305.00795)

    SelfDocSeg提出了一种完全基于视觉的自我监督文档分割方法，通过生成伪布局训练图像编码器学习文档对象的表示和定位，克服了标注数据稀缺的挑战。

    

    文档布局分析是一个众所周知的问题，已经被广泛探索，涵盖了从文本挖掘、识别到基于图形的表示、视觉特征提取等多种解决方案。然而，现有的大部分工作都忽略了标注数据的稀缺性这一关键事实。我们使用自我监督来解决这一挑战，并且与现有的使用文本挖掘和文本标签的自我监督文档分割方法不同，我们使用了完全基于视觉的方法在预训练中生成伪布局，以在没有任何真实标签或其导出物的情况下训练图像编码器，以自我监督的框架中学习文档对象的表示和定位。

    Document layout analysis is a known problem to the documents research community and has been vastly explored yielding a multitude of solutions ranging from text mining, and recognition to graph-based representation, visual feature extraction, etc. However, most of the existing works have ignored the crucial fact regarding the scarcity of labeled data. With growing internet connectivity to personal life, an enormous amount of documents had been available in the public domain and thus making data annotation a tedious task. We address this challenge using self-supervision and unlike, the few existing self-supervised document segmentation approaches which use text mining and textual labels, we use a complete vision-based approach in pre-training without any ground-truth label or its derivative. Instead, we generate pseudo-layouts from the document images to pre-train an image encoder to learn the document object representation and localization in a self-supervised framework before fine-tun
    
[^112]: 拓扑深度学习的架构：拓扑神经网络综述

    Architectures of Topological Deep Learning: A Survey on Topological Neural Networks. (arXiv:2304.10031v1 [cs.LG])

    [http://arxiv.org/abs/2304.10031](http://arxiv.org/abs/2304.10031)

    拓扑深度学习框架提供了从复杂系统相关数据中提取知识的全面架构，通过解决现有工作的符号和术语不一致问题，有望在应用科学和其他领域开拓新局面。

    

    自然界中充满了复杂的系统，其组成部分之间存在错综复杂的关系：从社交网络中个体之间的社交互动到蛋白质中原子之间的静电相互作用。拓扑深度学习（TDL）提供了一个全面的框架来处理和从这些系统相关的数据中提取知识，如预测一个人属于哪个社区或预测一个蛋白质是否可以成为合理的药物开发靶点。TDL已经证明拥有理论和实践上的优势，这为在应用科学和其他领域开拓新局面提供了希望。然而，TDL文献的快速增长也导致了拓扑神经网络（TNN）体系结构符号和术语上的不一致。这对于建立在现有工作基础上和将TNN部署到新的现实问题中都是一个真正的障碍。为了解决这个问题，我们提供了一个易于理解的综述。

    The natural world is full of complex systems characterized by intricate relations between their components: from social interactions between individuals in a social network to electrostatic interactions between atoms in a protein. Topological Deep Learning (TDL) provides a comprehensive framework to process and extract knowledge from data associated with these systems, such as predicting the social community to which an individual belongs or predicting whether a protein can be a reasonable target for drug development. TDL has demonstrated theoretical and practical advantages that hold the promise of breaking ground in the applied sciences and beyond. However, the rapid growth of the TDL literature has also led to a lack of unification in notation and language across Topological Neural Network (TNN) architectures. This presents a real obstacle for building upon existing works and for deploying TNNs to new real-world problems. To address this issue, we provide an accessible introduction 
    
[^113]: Tetra-NeRF：使用四面体表示的神经辐射场

    Tetra-NeRF: Representing Neural Radiance Fields Using Tetrahedra. (arXiv:2304.09987v1 [cs.CV])

    [http://arxiv.org/abs/2304.09987](http://arxiv.org/abs/2304.09987)

    本文提出了一种基于四面体和 Delaunay 表示的自适应表示方法，用于神经辐射场，可实现高效的训练和最先进的结果，同时提供了更多接近表面的细节，并且实现了比基于点的表示更好的性能。

    

    神经辐射场 (NeRF) 是一种非常流行的方法，用于新视角合成和三维重构问题。NeRF 常用的场景表示是将场景的一致的基于体素的细分与 MLP 结合起来。本文根据观察到的场景的（稀疏）点云提出了一种基于 Delaunay 表示的自适应表示，而非一致的细分或基于点的表示。我们证明了这种表示可以实现高效的训练，获得最先进的结果。我们的方法巧妙地结合了三维几何处理、三角形渲染和现代神经辐射场的概念。与基于体素的表示相比，我们的方法提供了更多接近表面的场景细节。与基于点的表示相比，我们的方法实现了更好的性能。

    Neural Radiance Fields (NeRFs) are a very recent and very popular approach for the problems of novel view synthesis and 3D reconstruction. A popular scene representation used by NeRFs is to combine a uniform, voxel-based subdivision of the scene with an MLP. Based on the observation that a (sparse) point cloud of the scene is often available, this paper proposes to use an adaptive representation based on tetrahedra and a Delaunay representation instead of the uniform subdivision or point-based representations. We show that such a representation enables efficient training and leads to state-of-the-art results. Our approach elegantly combines concepts from 3D geometry processing, triangle-based rendering, and modern neural radiance fields. Compared to voxel-based representations, ours provides more detail around parts of the scene likely to be close to the surface. Compared to point-based representations, our approach achieves better performance.
    
[^114]: 提高医学图像分析中私有联邦模型的性能

    Improving Performance of Private Federated Models in Medical Image Analysis. (arXiv:2304.05127v1 [cs.CR])

    [http://arxiv.org/abs/2304.05127](http://arxiv.org/abs/2304.05127)

    本论文通过本地步骤和调整功能进一步提高医学影像分析中的联邦学习模型性能。

    

    联邦学习（FL）是一种分布式机器学习（ML）方法，允许在不集中数据的情况下进行训练。这种方法对医学应用特别有益，因为它解决了与医疗数据相关的一些关键挑战，如隐私、安全和数据所有权。此外，FL可以提高用于医学应用中的ML模型的质量。医疗数据通常是多样化的，并且可以根据病人群体而有很大变化，这使得开发准确且具有一般性的ML模型具有挑战性。FL允许使用来自多个源的医疗数据，这有助于提高ML模型的质量和一般化能力。差分隐私（DP）是保护该过程安全和私密性的工具。在这项工作中，我们展示了通过采用本地步骤（一种提高FL通信效率的常用方法）和调整通信次数的数量，可以进一步提高模型性能。

    Federated learning (FL) is a distributed machine learning (ML) approach that allows data to be trained without being centralized. This approach is particularly beneficial for medical applications because it addresses some key challenges associated with medical data, such as privacy, security, and data ownership. On top of that, FL can improve the quality of ML models used in medical applications. Medical data is often diverse and can vary significantly depending on the patient population, making it challenging to develop ML models that are accurate and generalizable. FL allows medical data to be used from multiple sources, which can help to improve the quality and generalizability of ML models. Differential privacy (DP) is a go-to algorithmic tool to make this process secure and private. In this work, we show that the model performance can be further improved by employing local steps, a popular approach to improving the communication efficiency of FL, and tuning the number of communica
    
[^115]: 学习速率表在分布转移条件下的应用

    Learning Rate Schedules in the Presence of Distribution Shift. (arXiv:2303.15634v1 [cs.LG])

    [http://arxiv.org/abs/2303.15634](http://arxiv.org/abs/2303.15634)

    该论文提出了一种学习速率表，以在数据分布发生变化时最小化SGD在线学习的后悔，能够对分布转移具有鲁棒性，同时适用于凸损失函数和非凸损失函数。最优学习速率表通常会在数据分布转移的情况下增加，能够用于高维回归模型和神经网络。

    

    我们设计了学习速率表，以在数据分布发生变化时最小化SGD在线学习的后悔。我们通过随机微分方程的新颖分析，完全表征了在线线性回归的最优学习速率表。对于一般的凸损失函数，我们提出了新的学习速率表，对分布转移具有鲁棒性，我们给出了只有常数差异的后悔上下界。对于非凸损失函数，我们基于估计模型的梯度范数定义了一种后悔概念，并提出了一种学习时间表，以最小化总预期后悔的上限。直观地说，我们预计损失领域的变化需要更多的探索，我们证实了最优学习速率表通常会在数据分布转移的情况下增加。最后，我们提供了针对高维回归模型和神经网络的实验，以说明这些学习速率表的应用。

    We design learning rate schedules that minimize regret for SGD-based online learning in the presence of a changing data distribution. We fully characterize the optimal learning rate schedule for online linear regression via a novel analysis with stochastic differential equations. For general convex loss functions, we propose new learning rate schedules that are robust to distribution shift, and we give upper and lower bounds for the regret that only differ by constants. For non-convex loss functions, we define a notion of regret based on the gradient norm of the estimated models and propose a learning schedule that minimizes an upper bound on the total expected regret. Intuitively, one expects changing loss landscapes to require more exploration, and we confirm that optimal learning rate schedules typically increase in the presence of distribution shift. Finally, we provide experiments for high-dimensional regression models and neural networks to illustrate these learning rate schedule
    
[^116]: 去噪扩散自编码器是统一自监督学习器

    Denoising Diffusion Autoencoders are Unified Self-supervised Learners. (arXiv:2303.09769v1 [cs.CV])

    [http://arxiv.org/abs/2303.09769](http://arxiv.org/abs/2303.09769)

    本文研究了去噪扩散自编码器 (DDAE) 是否能通过无条件图像生成训练获取强有力的线性可分表示，结果表明DDAE是一个统一的自监督学习器，对于自监督生成和辨别性学习是通用的方法。在多类数据集上实现了95.9％和50.0％的线性探测精度，与掩码自编码器和对比学习相当。

    

    受扩散模型最近的进展的启发，这些模型类似于去噪自编码器，我们研究它们是否可以通过生成预训练获取分类的辨别性表示。本文展示了扩散模型中的网络，即去噪扩散自编码器(DDAE)是统一的自监督学习器:通过在无条件图像生成上进行预训练，DDAE已经在中间层学习到了强有力的线性可分表示，而无需辅助编码器，从而使扩散预训练成为自监督生成和辨别性学习的通用方法。为了验证这一点，我们在多类数据集上执行线性探测和微调评估。我们基于扩散的方法，在CIFAR-10和Tiny-ImageNet上分别实现了95.9％和50.0％的线性探测精度，与掩码自编码器和对比学习首次可比较。此外，从Image上的转移学习

    Inspired by recent advances in diffusion models, which are reminiscent of denoising autoencoders, we investigate whether they can acquire discriminative representations for classification via generative pre-training. This paper shows that the networks in diffusion models, namely denoising diffusion autoencoders (DDAE), are unified self-supervised learners: by pre-training on unconditional image generation, DDAE has already learned strongly linear-separable representations at its intermediate layers without auxiliary encoders, thus making diffusion pre-training emerge as a general approach for self-supervised generative and discriminative learning. To verify this, we perform linear probe and fine-tuning evaluations on multi-class datasets. Our diffusion-based approach achieves 95.9% and 50.0% linear probe accuracies on CIFAR-10 and Tiny-ImageNet, respectively, and is comparable to masked autoencoders and contrastive learning for the first time. Additionally, transfer learning from Image
    
[^117]: 数据集增强：提高模型准确性和鲁棒性

    Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement. (arXiv:2303.08983v1 [cs.CV])

    [http://arxiv.org/abs/2303.08983](http://arxiv.org/abs/2303.08983)

    提出了一种名为数据集增强的策略，一次性改进数据集，从而提高任何经过增强的数据集训练的模型的准确性、鲁棒性和校准性。例如，使用ImageDataNet+训练的ResNet-50在ImageNet验证集上的准确率提高了1.7％，在ImageNetV2上提高了3.5％，在ImageNet-R上提高了10.0％。

    

    我们提出了一种名为数据集增强的策略，一次性改进数据集，从而提高任何经过增强的数据集训练的模型的准确性，对用户没有额外的训练成本。我们提出了一种基于数据增强和知识蒸馏的数据集增强策略。我们的通用策略是基于广泛的CNN和基于transformer的模型的分析，以及对带有各种数据增强的最先进模型进行大规模的蒸馏研究。我们创建了ImageDataNet+的增强版本，以及增强的数据集CIFAR-100+，Flowers-102+和Food-101+。使用ImageDataNet+训练的模型更准确、更有鲁棒性和校准性，并且对下游任务（例如分割和检测）具有很好的迁移能力。例如，ResNet-50在ImageNet验证集上的准确率提高了1.7％，在ImageNetV2上提高了3.5％，在ImageNet-R上提高了10.0％。在ImageDataNet+上测量的Expected Calibration Error（ECE）也有显著改进。

    We propose Dataset Reinforcement, a strategy to improve a dataset once such that the accuracy of any model architecture trained on the reinforced dataset is improved at no additional training cost for users. We propose a Dataset Reinforcement strategy based on data augmentation and knowledge distillation. Our generic strategy is designed based on extensive analysis across CNN- and transformer-based models and performing large-scale study of distillation with state-of-the-art models with various data augmentations. We create a reinforced version of the ImageNet training dataset, called ImageNet+, as well as reinforced datasets CIFAR-100+, Flowers-102+, and Food-101+. Models trained with ImageNet+ are more accurate, robust, and calibrated, and transfer well to downstream tasks (e.g., segmentation and detection). As an example, the accuracy of ResNet-50 improves by 1.7% on the ImageNet validation set, 3.5% on ImageNetV2, and 10.0% on ImageNet-R. Expected Calibration Error (ECE) on the Ima
    
[^118]: 利用四维CT灌注成像对疑似急性缺血性卒中患者的梗死区进行分割

    Exploiting 4D CT Perfusion for segmenting infarcted areas in patients with suspected acute ischemic stroke. (arXiv:2303.08757v1 [eess.IV])

    [http://arxiv.org/abs/2303.08757](http://arxiv.org/abs/2303.08757)

    该研究提出了一种新颖的方法，通过利用四维CTP全面利用时空信息，以分割疑似急性缺血性卒中患者的梗死区。实验证明，该方法明显优于现有的最先进方法，有潜力为改善AIS患者的早期诊断和治疗规划的准确性和效率做出贡献。

    

    精确、快速的急性缺血性卒中（AIS）患者缺血区（核心和半影区）预测方法对于改进诊断和治疗规划具有重要的临床意义。计算机断层扫描（CT）是疑似AIS患者早期评估的主要模式之一。CT灌注成像（CTP）通常用作主要评估手段，以确定卒中位置、严重程度和缺血性病灶体积。目前，大多数CTP自动分割方法都使用已经处理过的三维彩色地图作为放射科医师常规视觉评估的输入。或者，基于切片的二维+时间输入使用原始CTP数据，其中忽略了在体积上的空间信息。在本文中，我们研究不同方法来利用整个四维CTP作为输入，以充分利用时空信息。这使我们提出了一种新颖的4D卷积层。我们在大型数据集上进行的全面实验表明，所提出的方法明显优于现有的最先进方法。该方法有潜力为改善AIS患者的早期诊断和治疗规划的准确性和效率做出贡献。

    Precise and fast prediction methods for ischemic areas (core and penumbra) in acute ischemic stroke (AIS) patients are of significant clinical interest: they play an essential role in improving diagnosis and treatment planning. Computed Tomography (CT) scan is one of the primary modalities for early assessment in patients with suspected AIS. CT Perfusion (CTP) is often used as a primary assessment to determine stroke location, severity, and volume of ischemic lesions. Current automatic segmentation methods for CTP mostly use already processed 3D color maps conventionally used for visual assessment by radiologists as input. Alternatively, the raw CTP data is used on a slice-by-slice basis as 2D+time input, where the spatial information over the volume is ignored. In this paper, we investigate different methods to utilize the entire 4D CTP as input to fully exploit the spatio-temporal information. This leads us to propose a novel 4D convolution layer. Our comprehensive experiments on a l
    
[^119]: 非对角度量中的可扩展随机梯度里曼动力学

    Scalable Stochastic Gradient Riemannian Langevin Dynamics in Non-Diagonal Metrics. (arXiv:2303.05101v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.05101](http://arxiv.org/abs/2303.05101)

    本文提出了两种非对角度量，可以在随机梯度采样中使用，以改善神经网络模型的贝叶斯推断性能，尤其对于具有稀疏诱导先验的全连接神经网络和具有相关先验的卷积神经网络效果显著。

    

    随机梯度采样方法通常用于对神经网络进行贝叶斯推断。观察到，包含微分几何概念的方法往往具有更好的性能，里曼度量通过考虑局部曲率来改善后验探索。然而，现有方法往往采用简单的对角度量以保持计算效率，这会损失一些性能。我们提出了两种非对角度量，可以在随机梯度采样中使用，以改善收敛性和探索性，在对比对角度量只有轻微的计算开销。我们展示了对于具有稀疏诱导先验的全连接神经网络和具有相关先验的卷积神经网络，使用这些度量可以提供改进。对于其他一些选择，后验分布在简单度量下也足够容易。

    Stochastic-gradient sampling methods are often used to perform Bayesian inference on neural networks. It has been observed that the methods in which notions of differential geometry are included tend to have better performances, with the Riemannian metric improving posterior exploration by accounting for the local curvature. However, the existing methods often resort to simple diagonal metrics to remain computationally efficient. This loses some of the gains. We propose two non-diagonal metrics that can be used in stochastic-gradient samplers to improve convergence and exploration but have only a minor computational overhead over diagonal metrics. We show that for fully connected neural networks (NNs) with sparsity-inducing priors and convolutional NNs with correlated priors, using these metrics can provide improvements. For some other choices the posterior is sufficiently easy also for the simpler metrics.
    
[^120]: 基于稀疏高斯过程的连续和离散空间的回归传感器放置优化

    Efficient Sensor Placement from Regression with Sparse Gaussian Processes in Continuous and Discrete Spaces. (arXiv:2303.00028v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2303.00028](http://arxiv.org/abs/2303.00028)

    本文提出了一种用稀疏高斯过程解决连续空间下传感器放置问题的方法，避免离散化环境并降低了计算复杂度，可通过贪婪算法找到好的解决方案。

    

    本文提出了一种基于稀疏高斯过程方法的传感器放置方案，用于监测温度、降水等空间（或时空）相关现象。与现有的基于高斯过程的传感器放置方法不同，我们将已知内核函数参数的稀疏高斯过程拟合到环境中随机采样的未标记位置，并通过学习得到的诱导点来解决连续空间的传感器放置问题。使用稀疏高斯过程避免了对环境进行离散化，并将计算复杂度从立方级别降低到线性级别。在候选传感器放置点集合的限制下，我们可以使用贪婪顺序选择算法来找到较好的解决方案。

    We present a novel approach based on sparse Gaussian processes (SGPs) to address the sensor placement problem for monitoring spatially (or spatiotemporally) correlated phenomena such as temperature and precipitation. Existing Gaussian process (GP) based sensor placement approaches use GPs with known kernel function parameters to model a phenomenon and subsequently optimize the sensor locations in a discretized representation of the environment. In our approach, we fit an SGP with known kernel function parameters to randomly sampled unlabeled locations in the environment and show that the learned inducing points of the SGP inherently solve the sensor placement problem in continuous spaces. Using SGPs avoids discretizing the environment and reduces the computation cost from cubic to linear complexity. When restricted to a candidate set of sensor placement locations, we can use greedy sequential selection algorithms on the SGP's optimization bound to find good solutions. We also present a
    
[^121]: 用自适应加权损失进行元学习，解决不平衡的冷启动推荐问题

    Meta-Learning with Adaptive Weighted Loss for Imbalanced Cold-Start Recommendation. (arXiv:2302.14640v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2302.14640](http://arxiv.org/abs/2302.14640)

    该论文提出了一种用于解决不平衡冷启动推荐问题的元学习算法，通过自适应加权损失来适应用户的个性化需求。

    

    顺序推荐系统在捕捉用户喜好方面取得了重大突破。然而，冷启动推荐仍然是一个基本挑战，因为它们通常涉及有限的用户-物品交互进行个性化。最近，基于梯度的元学习方法在顺序推荐领域中出现，因为它们具有快速适应和易于集成的能力。元学习算法将冷启动推荐描述为一个少样本学习问题，其中每个用户都被表示为需要适应的任务。然而，元学习算法通常假设任务样本在类别或值上均匀分布，而实际应用中的用户-物品交互并不符合这样的分布（例如，多次观看喜欢的视频，只留下正面评分而没有负面评分）。因此，占据任务训练数据大部分的不平衡用户反馈可能主导着用户的适应过程。

    Sequential recommenders have made great strides in capturing a user's preferences. Nevertheless, the cold-start recommendation remains a fundamental challenge as they typically involve limited user-item interactions for personalization. Recently, gradient-based meta-learning approaches have emerged in the sequential recommendation field due to their fast adaptation and easy-to-integrate abilities. The meta-learning algorithms formulate the cold-start recommendation as a few-shot learning problem, where each user is represented as a task to be adapted. While meta-learning algorithms generally assume that task-wise samples are evenly distributed over classes or values, user-item interactions in real-world applications do not conform to such a distribution (e.g., watching favorite videos multiple times, leaving only positive ratings without any negative ones). Consequently, imbalanced user feedback, which accounts for the majority of task training data, may dominate the user adaptation pr
    
[^122]: 用鲁棒交替最小化方法在几乎线性时间内完成低秩矩阵补全

    Low Rank Matrix Completion via Robust Alternating Minimization in Nearly Linear Time. (arXiv:2302.11068v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11068](http://arxiv.org/abs/2302.11068)

    本论文提出了一种在几乎线性时间内用鲁棒交替最小化方法完成低秩矩阵补全的方法，并证明了观察几乎线性数量的条目即可恢复矩阵$M$，此方法克服了交替最小化方法需要精确计算的限制，更符合实际实现中对效率的要求。

    

    给定一个矩阵$M\in \mathbb{R}^{m\times n}$，低秩矩阵补全问题要求我们通过只观察一组指定的条目$\Omega\subseteq [m]\times [n]$来找到$M$的秩为$k$的近似$UV^\top$，其中$U\in \mathbb{R}^{m\times k}$，$V\in \mathbb{R}^{n\times k}$。本文主要研究了一种被广泛使用的方法--交替最小化框架。Jain、Netrapalli和Sanghavi~\cite{jns13}证明了如果$M$的行和列是不相干的，那么交替最小化方法可以通过观察几乎线性数量的条目可靠地恢复矩阵$M$。虽然样本复杂度之后被改进~\cite{glz17}，但交替最小化步骤要求精确计算。这阻碍了更高效算法的开发，并未描述交替最小化的实际实现，其中更新通常是近似执行，以提高效率。

    Given a matrix $M\in \mathbb{R}^{m\times n}$, the low rank matrix completion problem asks us to find a rank-$k$ approximation of $M$ as $UV^\top$ for $U\in \mathbb{R}^{m\times k}$ and $V\in \mathbb{R}^{n\times k}$ by only observing a few entries specified by a set of entries $\Omega\subseteq [m]\times [n]$. In particular, we examine an approach that is widely used in practice -- the alternating minimization framework. Jain, Netrapalli and Sanghavi~\cite{jns13} showed that if $M$ has incoherent rows and columns, then alternating minimization provably recovers the matrix $M$ by observing a nearly linear in $n$ number of entries. While the sample complexity has been subsequently improved~\cite{glz17}, alternating minimization steps are required to be computed exactly. This hinders the development of more efficient algorithms and fails to depict the practical implementation of alternating minimization, where the updates are usually performed approximately in favor of efficiency.  In this p
    
[^123]: HLSDataset: 用于使用高级综合的机器学习辅助FPGA设计的开源数据集

    HLSDataset: Open-Source Dataset for ML-Assisted FPGA Design using High Level Synthesis. (arXiv:2302.10977v2 [cs.AR] UPDATED)

    [http://arxiv.org/abs/2302.10977](http://arxiv.org/abs/2302.10977)

    本文介绍了HLSDataset，这是一个用于使用高级综合的机器学习辅助FPGA设计的开源数据集。该数据集包含大量高质量的训练样本，并通过案例研究证明了其有效性。

    

    机器学习已被广泛应用于使用高级综合进行设计探索，以在FPGA设计的早期阶段提供更好、更快的性能以及资源和功耗估计。为了准确预测，需要高质量和大容量的训练数据集。本文介绍了一个用于使用高级综合的机器学习辅助FPGA设计的数据集，称为HLSDataset。该数据集是从常用的HLS C基准（Polybench、Machsuite、CHStone和Rossetta）生成的。生成的Verilog样本包含了多种指令，包括循环展开、循环流水线和数组划分，以确保覆盖了优化和现实设计。每种FPGA类型生成的Verilog样本总数近9000个。为了演示我们的数据集的有效性，我们进行了案例研究，使用经过我们数据集训练的机器学习模型进行功耗估计和资源使用估计。所有代码和数据集都在公共平台上公开。

    Machine Learning (ML) has been widely adopted in design exploration using high level synthesis (HLS) to give a better and faster performance, and resource and power estimation at very early stages for FPGA-based design. To perform prediction accurately, high-quality and large-volume datasets are required for training ML models.This paper presents a dataset for ML-assisted FPGA design using HLS, called HLSDataset. The dataset is generated from widely used HLS C benchmarks including Polybench, Machsuite, CHStone and Rossetta. The Verilog samples are generated with a variety of directives including loop unroll, loop pipeline and array partition to make sure optimized and realistic designs are covered. The total number of generated Verilog samples is nearly 9,000 per FPGA type. To demonstrate the effectiveness of our dataset, we undertake case studies to perform power estimation and resource usage estimation with ML models trained with our dataset. All the codes and dataset are public at t
    
[^124]: 特征亲和力辅助的知识蒸馏和深度神经网络无标签数据的量化

    Feature Affinity Assisted Knowledge Distillation and Quantization of Deep Neural Networks on Label-Free Data. (arXiv:2302.10899v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10899](http://arxiv.org/abs/2302.10899)

    本文提出了一种特征亲和力辅助的知识蒸馏方法，通过结合logit损失和特征亲和力损失，可以在无标签数据上压缩深度神经网络模型。

    

    本文提出了一种特征亲和力（FA）辅助的知识蒸馏（KD）方法，以改进深度神经网络（DNN）的量化感知训练。DNN的中间特征图上的FA损失起到了将中间步骤的解决方案教给学生的作用，而不仅仅是在传统的KD中作用于网络输出级别的logits损失。将logit损失和FA损失结合起来，我们发现量化的学生网络得到的监督比来自标记地面真实数据的监督更强。所得到的FAQD能够在无标签数据上压缩模型，这带来了即时的实际效益，因为预先训练好的教师模型是随时可用的，而无标签数据又是丰富的。相反，数据标记通常是费时费力的。最后，我们提出了一种快速特征亲和力（FFA）损失，它以较低的计算复杂度准确近似FA损失，有助于加快高分辨率训练的速度。

    In this paper, we propose a feature affinity (FA) assisted knowledge distillation (KD) method to improve quantization-aware training of deep neural networks (DNN). The FA loss on intermediate feature maps of DNNs plays the role of teaching middle steps of a solution to a student instead of only giving final answers in the conventional KD where the loss acts on the network logits at the output level. Combining logit loss and FA loss, we found that the quantized student network receives stronger supervision than from the labeled ground-truth data. The resulting FAQD is capable of compressing model on label-free data, which brings immediate practical benefits as pre-trained teacher models are readily available and unlabeled data are abundant. In contrast, data labeling is often laborious and expensive. Finally, we propose a fast feature affinity (FFA) loss that accurately approximates FA loss with a lower order of computational complexity, which helps speed up training for high resolution
    
[^125]: 基于混合谱方法和谐振子的非可分离协方差核的时空高斯过程研究

    Non-separable Covariance Kernels for Spatiotemporal Gaussian Processes based on a Hybrid Spectral Method and the Harmonic Oscillator. (arXiv:2302.09580v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.09580](http://arxiv.org/abs/2302.09580)

    该论文提出了一种基于混合谱方法和谐振子的非可分离协方差核的时空高斯过程研究，通过物理论证推导出一类新型的非可分离协方差核，能更好地捕捉观测到的时空相关性。

    

    高斯过程提供了一个灵活的非参数框架，用于近似高维空间中的函数。协方差核是高斯过程的主要引擎，包含了预测分布的相关性。对于具有时空数据集的应用，合适的核应该建模联合的时空依赖关系。可分离的时空协方差核提供了简单和计算效率较高的方案。然而，非可分离核包含了更好地捕捉观测到的相关性的时空交互作用。大多数具有显式表达式的非可分离核是基于数学考虑（可允许条件）而非基于第一原理导出的。我们提出了一种基于物理论证的混合谱方法来生成协方差核。我们使用这种方法推导了一类新型的物理动机的非可分离协方差核，它们的根源来自随机线性...

    Gaussian processes provide a flexible, non-parametric framework for the approximation of functions in high-dimensional spaces. The covariance kernel is the main engine of Gaussian processes, incorporating correlations that underpin the predictive distribution. For applications with spatiotemporal datasets, suitable kernels should model joint spatial and temporal dependence. Separable space-time covariance kernels offer simplicity and computational efficiency. However, non-separable kernels include space-time interactions that better capture observed correlations. Most non-separable kernels that admit explicit expressions are based on mathematical considerations (admissibility conditions) rather than first-principles derivations. We present a hybrid spectral approach for generating covariance kernels which is based on physical arguments. We use this approach to derive a new class of physically motivated, non-separable covariance kernels which have their roots in the stochastic, linear, 
    
[^126]: Brainomaly:利用未标注的T1加权脑部MR图像进行无监督的神经疾病检测

    Brainomaly: Unsupervised Neurologic Disease Detection Utilizing Unannotated T1-weighted Brain MR Images. (arXiv:2302.09200v3 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2302.09200](http://arxiv.org/abs/2302.09200)

    本研究提出了Brainomaly，一种基于GAN的图像翻译方法，特别设计用于神经疾病检测。与现有方法不同的是，Brainomaly利用未标注的脑部MR图像，具有更好的无监督疾病检测性能。

    

    在医学影像领域利用深度神经网络的能力存在挑战，主要是由于获取大规模标注数据集的困难，特别是对于罕见疾病来说，注释需求的成本、时间和工作量很高。无监督疾病检测方法，如异常检测，可以大大减少这些场景下的人力投入。虽然异常检测通常是专注于仅从健康受试者的图像中进行学习，但现实情况下常常存在包含同时健康和患病受试者的未标注数据集。最近的研究表明，利用这些未标注的图像可以改善无监督疾病和异常检测。然而，这些方法并未利用特定于脑图像的知识，从而在神经疾病检测方面表现不佳。为了解决这个限制，我们提出了Brainomaly，一种专门用于神经疾病检测的基于GAN的图像翻译方法。

    Harnessing the power of deep neural networks in the medical imaging domain is challenging due to the difficulties in acquiring large annotated datasets, especially for rare diseases, which involve high costs, time, and effort for annotation. Unsupervised disease detection methods, such as anomaly detection, can significantly reduce human effort in these scenarios. While anomaly detection typically focuses on learning from images of healthy subjects only, real-world situations often present unannotated datasets with a mixture of healthy and diseased subjects. Recent studies have demonstrated that utilizing such unannotated images can improve unsupervised disease and anomaly detection. However, these methods do not utilize knowledge specific to registered neuroimages, resulting in a subpar performance in neurologic disease detection. To address this limitation, we propose Brainomaly, a GAN-based image-to-image translation method specifically designed for neurologic disease detection. Bra
    
[^127]: 一种使用最小互补能量原理的深度互补能量方法应用于固体力学的研究

    A deep complementary energy method for solid mechanics using minimum complementary energy principle. (arXiv:2302.01538v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01538](http://arxiv.org/abs/2302.01538)

    本文提出了一种名为深度互补能量方法(DCEM)的新方法，它基于最小互补能量原理，在固体力学中解决偏微分方程(PDE)，并扩展为DCEM-P以满足更多方程的要求。

    

    近年来，深度学习的快速发展在各个领域，特别是在固体力学中解决偏微分方程(PDE)方面产生了显著影响，极大地受益于神经网络的优异逼近能力。物理启发神经网络(PINNs)和深度能量方法(DEM)在解决PDE方面受到了广泛关注。最小势能原理和互补能量原理是固体力学中两个重要的变分原理。然而，DEM是基于最小势能原理，但它缺乏最小互补能量的重要形式。为了弥补这一差距，我们提出了基于最小互补能量原理的深度互补能量方法(DCEM)。DCEM的输出函数是应力函数。我们将DCEM扩展到DCEM-Plus (DCEM-P)，添加满足偏微分方程的项。此外，我们还提出了一种深度互补能量算子的方法。

    In recent years, the rapid advancement of deep learning has significantly impacted various fields, particularly in solving partial differential equations (PDEs) in solid mechanics, benefiting greatly from the remarkable approximation capabilities of neural networks. In solving PDEs, Physics-Informed Neural Networks (PINNs) and the Deep Energy Method (DEM) have garnered substantial attention. The principle of minimum potential energy and complementary energy are two important variational principles in solid mechanics. However,DEM is based on the principle of minimum potential energy, but it lacks the important form of minimum complementary energy. To bridge this gap, we propose the deep complementary energy method (DCEM) based on the principle of minimum complementary energy. The output function of DCEM is the stress function. We extend DCEM to DCEM-Plus (DCEM-P), adding terms that satisfy partial differential equations. Furthermore, we propose a deep complementary energy operator metho
    
[^128]: NeSyFOLD: 从卷积神经网络中提取逻辑程序

    NeSyFOLD: Extracting Logic Programs from Convolutional Neural Networks. (arXiv:2301.12667v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12667](http://arxiv.org/abs/2301.12667)

    NeSyFOLD是一种神经符号框架，可以从CNN中提取逻辑规则并生成可解释的分类模型。它使用基于规则的FOLD-SE-M机器学习算法和自动映射算法来将CNN核映射到语义概念，并产生可解释的规则集。

    

    我们提出了一种新的神经符号框架NeSyFOLD，从CNN中提取逻辑规则并创建一个NeSyFOLD模型来对图像进行分类。NeSyFOLD的学习流程如下：（i）我们首先在输入图像数据集上预训练CNN，并提取最后一层核的激活作为二进制值；（ii）接下来，我们使用基于规则的FOLD-SE-M机器学习算法生成能够分类图像的逻辑程序——表示为每个核对应的二进制激活向量，同时产生逻辑解释。由FOLD-SE-M算法生成的规则具有核编号作为谓词。我们设计了一种新的算法，用于自动将CNN核映射到图像中的语义概念。这个映射被用来将规则集中的谓词名（核编号）替换为对应的语义概念标签。结果产生了可解释的规则集，可以被人类直观地理解。我们将我们的NeSyFOLD框架与最先进的方法进行了比较，并表明它可以实现竞争性的分类性能，同时提供可解释的和解释性的知识。

    We present a novel neurosymbolic framework called NeSyFOLD to extract logic rules from a CNN and create a NeSyFOLD model to classify images. NeSyFOLD's learning pipeline is as follows: (i) We first pre-train a CNN on the input image dataset and extract activations of the last layer kernels as binary values; (ii) Next, we use the FOLD-SE-M rule-based machine learning algorithm to generate a logic program that can classify an image -- represented as a vector of binary activations corresponding to each kernel -- while producing a logical explanation. The rules generated by the FOLD-SE-M algorithm have kernel numbers as predicates. We have devised a novel algorithm for automatically mapping the CNN kernels to semantic concepts in the images. This mapping is used to replace predicate names (kernel numbers) in the rule-set with corresponding semantic concept labels. The resulting rule-set is interpretable, and can be intuitively understood by humans. We compare our NeSyFOLD framework with th
    
[^129]: Boosting算法的并行化不可能性研究

    The Impossibility of Parallelizing Boosting. (arXiv:2301.09627v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.09627](http://arxiv.org/abs/2301.09627)

    Boosting算法无法进行有效的并行化，需要指数级别的计算资源，否则并行化的效果并不显著。

    

    Boosting算法的目标是将一系列弱学习器集成成一个强学习器。然而，这些方法都是完全顺序的。本文研究了Boosting算法的并行化可能性，发现了一个强烈的负面结果，即显著的并行化需要指数级别的计算资源来完成训练。

    The aim of boosting is to convert a sequence of weak learners into a strong learner. At their heart, these methods are fully sequential. In this paper, we investigate the possibility of parallelizing boosting. Our main contribution is a strong negative result, implying that significant parallelization of boosting requires an exponential blow-up in the total computing resources needed for training.
    
[^130]: 自监督学习的多维视角综述: 算法、应用和未来趋势

    A Survey of Self-supervised Learning from Multiple Perspectives: Algorithms, Applications and Future Trends. (arXiv:2301.05712v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.05712](http://arxiv.org/abs/2301.05712)

    本综述论文从算法、应用和趋势的角度概述了自监督学习的多维视角。它介绍了SSL算法的动机、共性和差异，以及在图像处理、计算机视觉和自然语言处理等领域中的典型应用。

    

    深度监督学习算法通常需要大量标记的样本来达到令人满意的性能。然而，收集和标记过多的样本可能成本高昂且耗时。作为无监督学习的子集，自监督学习（SSL）旨在从未标记的样本中学习有用的特征，而无需任何人工标注的标签。SSL最近引起了广泛关注，并且已经开发了许多相关算法。然而，目前很少有综述研究来解释不同的SSL变体之间的关系和演变。本文从算法、应用、三个主要趋势和待解问题的视角综述了各种SSL方法。首先，详细介绍了大多数SSL算法的动机，并比较了它们的共性和差异。其次，概述了SSL在图像处理和计算机视觉（CV）以及自然语言处理（NLP）等领域中的典型应用。

    Deep supervised learning algorithms generally require large numbers of labeled examples to achieve satisfactory performance. However, collecting and labeling too many examples can be costly and time-consuming. As a subset of unsupervised learning, self-supervised learning (SSL) aims to learn useful features from unlabeled examples without any human-annotated labels. SSL has recently attracted much attention and many related algorithms have been developed. However, there are few comprehensive studies that explain the connections and evolution of different SSL variants. In this paper, we provide a review of various SSL methods from the perspectives of algorithms, applications, three main trends, and open questions. First, the motivations of most SSL algorithms are introduced in detail, and their commonalities and differences are compared. Second, typical applications of SSL in domains such as image processing and computer vision (CV), as well as natural language processing (NLP), are dis
    
[^131]: 探索可解释的土地覆盖映射：一种基于反事实的策略

    Towards Explainable Land Cover Mapping: a Counterfactual-based Strategy. (arXiv:2301.01520v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.01520](http://arxiv.org/abs/2301.01520)

    本论文提出了一种基于反事实的策略，用于土地覆盖分类任务中的卫星图像时间序列。该方法具有灵活性，能发现土地覆盖类别之间的有趣信息关系，同时通过鼓励时间连续的扰动来得到更稀疏且可解释的解决方案。

    

    反事实解释是提高深度学习模型解释性的新兴工具。在给定样本的情况下，这些方法寻找并向用户显示决策边界上类似的样本。在本文中，我们提出了一种用于土地覆盖分类任务的多类别设置中的卫星图像时间序列的对抗生成反事实方法。该方法的一个独特特点是在给定反事实解释的情况下对目标类别没有先验假设。这种固有的灵活性允许发现土地覆盖类别之间的有趣信息关系。另一个特点是鼓励反事实与原始样本之间仅在一个小而紧凑的时间段内有所不同。这些时间连续的扰动允许得到更稀疏且因此更可解释的解决方案。此外，通过强制生成的反事实解释的合理性/真实性。

    Counterfactual explanations are an emerging tool to enhance interpretability of deep learning models. Given a sample, these methods seek to find and display to the user similar samples across the decision boundary. In this paper, we propose a generative adversarial counterfactual approach for satellite image time series in a multi-class setting for the land cover classification task. One of the distinctive features of the proposed approach is the lack of prior assumption on the targeted class for a given counterfactual explanation. This inherent flexibility allows for the discovery of interesting information on the relationship between land cover classes. The other feature consists of encouraging the counterfactual to differ from the original sample only in a small and compact temporal segment. These time-contiguous perturbations allow for a much sparser and, thus, interpretable solution. Furthermore, plausibility/realism of the generated counterfactual explanations is enforced via the
    
[^132]: 使用Transformer和相似度度量改进阿拉伯文本分类的数据增强

    Data Augmentation using Transformers and Similarity Measures for Improving Arabic Text Classification. (arXiv:2212.13939v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.13939](http://arxiv.org/abs/2212.13939)

    本论文提出一种使用Transformer和相似度度量进行数据增强的方法，以改进阿拉伯文本分类，该方法利用AraGPT-2进行增强，并使用Euclidean、cosine、Jaccard和BLEU距离评估生成的句子。

    

    学习模型的性能很大程度上依赖于训练数据的可用性和充足性。为了解决数据集充足性问题，研究人员广泛探索了数据增强（DA）作为一种有前景的方法。DA通过对现有数据应用转换来生成新的数据实例，从而增加数据集的大小和变化性。这种方法提高了模型的性能和准确性，尤其是在解决分类任务中的类别不平衡问题方面。然而，很少有研究探讨阿拉伯语的DA，而是依赖于传统的方法，如释义或基于噪声的技术。在本文中，我们提出了一种新的阿拉伯语DA方法，采用了最近强大的建模技术AraGPT-2来进行增强过程。利用欧氏距离、余弦距离、Jaccard距离和BLEU距离对生成的句子进行了上下文、语义、多样性和新颖性的评估。最后，使用AraBERT transformer对情感进行了预测。

    The performance of learning models heavily relies on the availability and adequacy of training data. To address the dataset adequacy issue, researchers have extensively explored data augmentation (DA) as a promising approach. DA generates new data instances through transformations applied to the available data, thereby increasing dataset size and variability. This approach has enhanced model performance and accuracy, particularly in addressing class imbalance problems in classification tasks. However, few studies have explored DA for the Arabic language, relying on traditional approaches such as paraphrasing or noising-based techniques. In this paper, we propose a new Arabic DA method that employs the recent powerful modeling technique, namely the AraGPT-2, for the augmentation process. The generated sentences are evaluated in terms of context, semantics, diversity, and novelty using the Euclidean, cosine, Jaccard, and BLEU distances. Finally, the AraBERT transformer is used on sentime
    
[^133]: 实时触觉质地渲染的基于学习的模型的开发和评估

    Development and Evaluation of a Learning-based Model for Real-time Haptic Texture Rendering. (arXiv:2212.13332v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2212.13332](http://arxiv.org/abs/2212.13332)

    该论文开发了一个基于学习的模型，用于实时触觉质地渲染，在多个用户的研究中评估了其感知性能。这个模型可以推广到各种质地和用户交互的变化，并使用视觉触觉传感器的数据进行实时渲染。

    

    当前的虚拟现实（VR）环境缺乏人类在现实生活中的交互中经历到的丰富触觉信号，例如在表面上的横向移动中感受到的质地感。在VR环境中添加逼真的触觉质地需要一个能够推广到用户交互的变化和世界上各种各样的质地的模型。目前存在用于触觉质地渲染的方法，但通常需要为每种质地开发一个模型，导致可扩展性较低。我们提出了一个基于深度学习的动作条件模型，用于触觉质地渲染，并通过多个用户的感知性能评估来呈现逼真的质地振动。该模型统一适用于所有材料，使用来自基于视觉的触觉传感器（GelSight）的数据，在实时条件下呈现适当的表面。在质地渲染方面，我们使用一个高带宽的振触觉传感器连接到一个3D系统。

    Current Virtual Reality (VR) environments lack the rich haptic signals that humans experience during real-life interactions, such as the sensation of texture during lateral movement on a surface. Adding realistic haptic textures to VR environments requires a model that generalizes to variations of a user's interaction and to the wide variety of existing textures in the world. Current methodologies for haptic texture rendering exist, but they usually develop one model per texture, resulting in low scalability. We present a deep learning-based action-conditional model for haptic texture rendering and evaluate its perceptual performance in rendering realistic texture vibrations through a multi part human user study. This model is unified over all materials and uses data from a vision-based tactile sensor (GelSight) to render the appropriate surface conditioned on the user's action in real time. For rendering texture, we use a high-bandwidth vibrotactile transducer attached to a 3D Systems
    
[^134]: DALL-E 和 Flamingo 互相理解吗？

    Do DALL-E and Flamingo Understand Each Other?. (arXiv:2212.12249v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.12249](http://arxiv.org/abs/2212.12249)

    DALL-E 和 Flamingo 通过重建图像和生成文本的任务来理解彼此。研究发现，在生成的图像与原始图像相似的情况下，图像描述和文本生成质量较高。

    

    在理解和创建图像和文本的多模态研究领域取得了显著的进展。这一进展体现在专注于规模化图像描述的复杂模型的出现上，如著名的 Flamingo 模型和用于文本到图像生成的 DALL-E 模型。在这个领域中，一个值得探究的有趣问题是 Flamingo 和 DALL-E 是否理解彼此。为了研究这个问题，我们提出了一个重建任务，其中 Flamingo 为给定的图像生成一个描述，然后 DALL-E 使用这个描述作为输入来合成一张新的图像。我们认为，如果生成的图像与给定的图像相似，那么这些模型就互相理解。具体来说，我们研究了图像重建质量和文本生成质量之间的关系。我们发现，最佳的图像描述是那些能够生成与原始图像相似的图像。

    The field of multimodal research focusing on the comprehension and creation of both images and text has witnessed significant strides. This progress is exemplified by the emergence of sophisticated models dedicated to image captioning at scale, such as the notable Flamingo model and text-to-image generative models, with DALL-E serving as a prominent example. An interesting question worth exploring in this domain is whether Flamingo and DALL-E understand each other. To study this question, we propose a reconstruction task where Flamingo generates a description for a given image and DALL-E uses this description as input to synthesize a new image. We argue that these models understand each other if the generated image is similar to the given image. Specifically, we study the relationship between the quality of the image reconstruction and that of the text generation. We find that an optimal description of an image is one that gives rise to a generated image similar to the original one. Th
    
[^135]: 学习潜在表示以与人类共适应

    Learning Latent Representations to Co-Adapt to Humans. (arXiv:2212.09586v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2212.09586](http://arxiv.org/abs/2212.09586)

    机器人学习者在与非稳态人类共同互动时面临挑战，本文通过学习和推理人类策略和策略动态的高级表示，提出了一种算法形式以实现机器人与动态人类共适应。

    

    当机器人在家庭、道路或工厂中与人类互动时，人类的行为通常会因为机器人而改变。对于机器人学习者来说，非稳态的人类是一个挑战：机器人已经学会与原始人类协调一起执行的动作可能在人类适应机器人后失败。在本文中，我们提出了一个算法形式，使得机器人（即自我代理）能够与动态人类（即其他代理）共同适应，只利用机器人的低级状态、动作和奖励。一个核心挑战是，人类不仅会对机器人的行为做出反应，而且人类的反应方式随着时间和用户之间的变化而变化。为了应对这个挑战，我们的洞察力是，机器人不需要建立人类的精确模型，而是可以学习和推理人类策略和策略动态的高级表示。基于这个洞察，我们开发了RILI：稳健地影响潜在意图。RILI首先将低级机器人观察嵌入到高级表示中...

    When robots interact with humans in homes, roads, or factories the human's behavior often changes in response to the robot. Non-stationary humans are challenging for robot learners: actions the robot has learned to coordinate with the original human may fail after the human adapts to the robot. In this paper we introduce an algorithmic formalism that enables robots (i.e., ego agents) to co-adapt alongside dynamic humans (i.e., other agents) using only the robot's low-level states, actions, and rewards. A core challenge is that humans not only react to the robot's behavior, but the way in which humans react inevitably changes both over time and between users. To deal with this challenge, our insight is that -- instead of building an exact model of the human -- robots can learn and reason over high-level representations of the human's policy and policy dynamics. Applying this insight we develop RILI: Robustly Influencing Latent Intent. RILI first embeds low-level robot observations into 
    
[^136]: 使用潜在结构加速抗菌肽的发现

    Accelerating Antimicrobial Peptide Discovery with Latent Structure. (arXiv:2212.09450v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2212.09450](http://arxiv.org/abs/2212.09450)

    本研究提出了一种使用潜在结构的方法来加速抗菌肽的发现，该方法能够同时生成具有理想序列属性和二级结构的肽链，并通过湿实验验证了其中两个候选肽链的强大抗菌活性。

    

    抗菌肽（AMPs）是对抗耐药病原体的有希望的治疗方法。最近，深度生成模型被用于发现新的AMPs。然而，先前的研究主要关注肽序列属性，并没有考虑到重要的结构信息。在本文中，我们提出了一种用于设计AMPs的潜在序列-结构模型（LSSAMP）。LSSAMP利用潜在空间中的多尺度向量量化来表示二级结构（例如α螺旋和β折叠）。通过在潜在空间中对样本进行采样，LSSAMP能够同时生成具有理想序列属性和二级结构的肽链。实验结果表明，由LSSAMP生成的肽链具有很高的抗菌活性概率。我们的湿实验验证了其中两个候选肽链具有强大的抗菌活性。代码已在https://github.com/dqwang122/LSSAMP上发布。

    Antimicrobial peptides (AMPs) are promising therapeutic approaches against drug-resistant pathogens. Recently, deep generative models are used to discover new AMPs. However, previous studies mainly focus on peptide sequence attributes and do not consider crucial structure information. In this paper, we propose a latent sequence-structure model for designing AMPs (LSSAMP). LSSAMP exploits multi-scale vector quantization in the latent space to represent secondary structures (e.g. alpha helix and beta sheet). By sampling in the latent space, LSSAMP can simultaneously generate peptides with ideal sequence attributes and secondary structures. Experimental results show that the peptides generated by LSSAMP have a high probability of antimicrobial activity. Our wet laboratory experiments verified that two of the 21 candidates exhibit strong antimicrobial activity. The code is released at https://github.com/dqwang122/LSSAMP.
    
[^137]: SPARF：从少量输入图像中学习大规模的 3D 稀疏辐射场

    SPARF: Large-Scale Learning of 3D Sparse Radiance Fields from Few Input Images. (arXiv:2212.09100v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.09100](http://arxiv.org/abs/2212.09100)

    提出了一个大规模的 ShapeNet 合成数据集 SPARF，包括超过 100 万个有多个体素分辨率的 3D 优化的辐射场，用于新视角合成。同时提出了一种新颖的管线 SuRFNet，通过学习少量视图生成稀疏体素辐射场。

    

    最近神经辐射场 (NeRFs) 的进展将新视角合成问题看作是稀疏辐射场 (SRF) 优化，使用稀疏体素进行高效快速渲染 (plenoxels, InstantNGP)。为了利用机器学习和采用 SRF 作为 3D 表示，我们提出了 SPARF，这是一个基于 ShapeNet 的大规模合成数据集，用于新视角合成，由 $\sim$ 17 百万张图像组成，从近 40,000 个高分辨率形状渲染而来 (400 X 400 像素)。该数据集比现有的用于新视角合成的合成数据集大几个数量级，包括超过一百万个具有多个体素分辨率的 3D 优化过的辐射场。此外，我们提出了一种新颖的管线（SuRFNet），它从少量视图中学习生成稀疏体素辐射场。这是通过使用密集收集的 SPARF 数据集和 3D 稀疏卷积来实现的。SuRFNet 使用少量/单个图像的部分 SRF 和特定的 SRF 损失来学习生成稀疏体素辐射场。

    Recent advances in Neural Radiance Fields (NeRFs) treat the problem of novel view synthesis as Sparse Radiance Field (SRF) optimization using sparse voxels for efficient and fast rendering (plenoxels,InstantNGP). In order to leverage machine learning and adoption of SRFs as a 3D representation, we present SPARF, a large-scale ShapeNet-based synthetic dataset for novel view synthesis consisting of $\sim$ 17 million images rendered from nearly 40,000 shapes at high resolution (400 X 400 pixels). The dataset is orders of magnitude larger than existing synthetic datasets for novel view synthesis and includes more than one million 3D-optimized radiance fields with multiple voxel resolutions. Furthermore, we propose a novel pipeline (SuRFNet) that learns to generate sparse voxel radiance fields from only few views. This is done by using the densely collected SPARF dataset and 3D sparse convolutions. SuRFNet employs partial SRFs from few/one images and a specialized SRF loss to learn to gener
    
[^138]: 低方差前向梯度算法：直接反馈对齐结合动量法

    Low-Variance Forward Gradients using Direct Feedback Alignment and Momentum. (arXiv:2212.07282v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.07282](http://arxiv.org/abs/2212.07282)

    本文提出了一种前向直接反馈对齐算法（FDFA），结合了Activity-Perturbed前向梯度和动量法，用于计算DNN中的低方差梯度估计值。

    

    深度神经网络（DNN）的监督学习通常使用误差反向传播（BP）算法进行。但是，反向传播期间的错误顺序传播和权重传输限制了其效率和可扩展性，因此人们越来越感兴趣寻找BP的本地替代方法。本文提出了一种前向直接反馈对齐（FDFA）算法，它结合了Activity-Perturbed前向梯度，直接反馈对齐和动量法来计算DNN中的低方差梯度估计值。

    Supervised learning in Deep Neural Networks (DNNs) is commonly performed using the error Backpropagation (BP) algorithm. The sequential propagation of errors and the transport of weights during the backward pass limits its efficiency and scalability. Therefore, there is growing interest in finding local alternatives to BP. Recently, methods based on Forward-Mode Automatic Differentiation have been proposed, such as the Forward Gradient algorithm and its variants. However, Forward Gradients suffer from high variance in large DNNs, which affects convergence. In this paper, we address the large variance of Forward Gradients and propose the Forward Direct Feedback Alignment (FDFA) algorithm that combines Activity-Perturbed Forward Gradients with Direct Feedback Alignment and momentum to compute low-variance gradient estimates in DNNs. Our results provides both theoretical proof and empirical evidence that our proposed method achieves lower variance compared to previous Forward Gradient tec
    
[^139]: 可扩展的完整协方差动态混合模型用于概率交通预测

    Scalable Dynamic Mixture Model with Full Covariance for Probabilistic Traffic Forecasting. (arXiv:2212.06653v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.06653](http://arxiv.org/abs/2212.06653)

    本文提出了一种扩展性强、适用于复杂概率交通预测的动态混合模型，通过模拟复杂的时变分布以更准确预测交通情况，具有高效性、灵活性和可扩展性。

    

    基于深度学习的多元多步骤交通预测模型通常在序列到序列的设置中使用均方误差（MSE）或平均绝对误差（MAE）作为损失函数，简单地假设误差遵循独立且各向同性的高斯或拉普拉斯分布。然而，在现实世界的交通预测任务中，这样的假设往往是不切实际的，因为时空预测的概率分布非常复杂，并且在时间上存在强烈的同时相关性，涉及传感器和预测时间跨度。在本文中，我们将矩阵变量误差过程的时变分布建模为零均值高斯分布的动态混合模型。为了实现高效性、灵活性和可扩展性，我们使用矩阵正态分布参数化每个混合成分，并允许混合权重随时间变化和可预测。所提出的方法可以无缝集成

    Deep learning-based multivariate and multistep-ahead traffic forecasting models are typically trained with the mean squared error (MSE) or mean absolute error (MAE) as the loss function in a sequence-to-sequence setting, simply assuming that the errors follow an independent and isotropic Gaussian or Laplacian distributions. However, such assumptions are often unrealistic for real-world traffic forecasting tasks, where the probabilistic distribution of spatiotemporal forecasting is very complex with strong concurrent correlations across both sensors and forecasting horizons in a time-varying manner. In this paper, we model the time-varying distribution for the matrix-variate error process as a dynamic mixture of zero-mean Gaussian distributions. To achieve efficiency, flexibility, and scalability, we parameterize each mixture component using a matrix normal distribution and allow the mixture weight to change and be predictable over time. The proposed method can be seamlessly integrated 
    
[^140]: 基于联邦学习的移动自然语言处理中的少样本学习

    Federated Few-Shot Learning for Mobile NLP. (arXiv:2212.05974v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.05974](http://arxiv.org/abs/2212.05974)

    本研究首次探索了联邦学习在移动少样本自然语言处理中的应用，通过使用伪标签和提示学习算法，实现了仅有少量标记数据时的竞争性准确性。同时，通过创新的设计解决了高执行成本的问题。

    

    自然语言处理（NLP）在移动应用中得到广泛应用。为了支持各种语言理解任务，通常需要在联邦隐私保护环境中对基础NLP模型进行微调。这个过程通常依赖于至少数十万个来自移动客户端的带标签训练样本；然而移动用户通常缺乏标记数据的意愿或知识。这种数据标签的不足被称为少样本场景，它成为移动NLP应用的主要障碍。本研究首次探究了少样本场景下的联邦NLP（FedFSL）。通过结合伪标签和提示学习等算法进展，我们首先建立了一个训练流程，在仅有0.05%（少于100个）的训练数据被标记，其余数据未标记的情况下，实现了竞争性的准确性。为了具体实施这个工作流程，我们进一步提出了一个名为FeS的系统，通过创新设计解决了高执行成本的问题。其中包括课程进度控制，目标网络和验证器网络的结构等方面的创新。

    Natural language processing (NLP) sees rich mobile applications. To support various language understanding tasks, a foundation NLP model is often fine-tuned in a federated, privacy-preserving setting (FL). This process currently relies on at least hundreds of thousands of labeled training samples from mobile clients; yet mobile users often lack willingness or knowledge to label their data. Such an inadequacy of data labels is known as a few-shot scenario; it becomes the key blocker for mobile NLP applications.  For the first time, this work investigates federated NLP in the few-shot scenario (FedFSL). By retrofitting algorithmic advances of pseudo labeling and prompt learning, we first establish a training pipeline that delivers competitive accuracy when only 0.05% (fewer than 100) of the training data is labeled and the remaining is unlabeled. To instantiate the workflow, we further present a system FeS, addressing the high execution cost with novel designs. (1) Curriculum pacing, whi
    
[^141]: DeepCut: 使用图神经网络聚类的无监督分割技术

    DeepCut: Unsupervised Segmentation using Graph Neural Networks Clustering. (arXiv:2212.05853v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.05853](http://arxiv.org/abs/2212.05853)

    本研究引入了一种轻量级的图神经网络（GNN）来解决图像分割中特征降维的问题，并采用无监督方法进行分割。与传统方法相比，该方法在构建图时同时使用局部特征和原始特征，从而能更好地进行聚类和分类分割。

    

    图像分割是计算机视觉中的基本任务。为了训练监督方法需要进行数据注释，这是一项耗时耗力的工作，因此激励了无监督方法的发展。当前的方法通常依赖于从预训练网络中提取深度特征来构建图，然后再应用经典的聚类方法如k-means和归一化割作为后处理步骤。然而，这种方法将特征中的高维信息降维为一对一的标量亲和力。为了解决这个限制，本研究引入了一种轻量级的图神经网络（GNN），用它来替代经典的聚类方法，并在优化相同的聚类目标函数的同时。与现有的方法不同，我们的GNN同时接受局部图像特征之间的一对一亲和力和原始特征作为输入。原始特征与聚类目标之间的直接连接使得我们能够隐式地对不同图之间的聚类进行分类，从而获得更好的分割效果。

    Image segmentation is a fundamental task in computer vision. Data annotation for training supervised methods can be labor-intensive, motivating unsupervised methods. Current approaches often rely on extracting deep features from pre-trained networks to construct a graph, and classical clustering methods like k-means and normalized-cuts are then applied as a post-processing step. However, this approach reduces the high-dimensional information encoded in the features to pair-wise scalar affinities. To address this limitation, this study introduces a lightweight Graph Neural Network (GNN) to replace classical clustering methods while optimizing for the same clustering objective function. Unlike existing methods, our GNN takes both the pair-wise affinities between local image features and the raw features as input. This direct connection between the raw features and the clustering objective enables us to implicitly perform classification of the clusters between different graphs, resulting 
    
[^142]: P{\O}DA: 基于提示的零样本领域自适应

    P{\O}DA: Prompt-driven Zero-shot Domain Adaptation. (arXiv:2212.03241v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.03241](http://arxiv.org/abs/2212.03241)

    本文提出了一种基于提示的零样本领域自适应方法，通过利用预训练的对比视觉语言模型（CLIP）来优化源特征的仿射变换，将其引导到目标文本嵌入中，同时保留其内容和语义。实验表明，该方法在几个数据集上显著优于基于CLIP的风格转移基线，用于下游任务。

    This paper proposes a prompt-driven zero-shot domain adaptation method, which leverages a pretrained contrastive vision-language model (CLIP) to optimize affine transformations of source features, steering them towards target text embeddings, while preserving their content and semantics. Experiments demonstrate that the method significantly outperforms CLIP-based style transfer baselines on several datasets for the downstream task at hand.

    领域自适应在计算机视觉领域得到了广泛的研究，但仍需要在训练时访问目标图像，这在某些不常见的情况下可能是不可行的。本文提出了“基于提示的零样本领域自适应”任务，其中我们仅使用目标域的单个通用文本描述（即提示）来调整在源域上训练的模型。首先，我们利用预训练的对比视觉语言模型（CLIP）来优化源特征的仿射变换，将其引导到目标文本嵌入中，同时保留其内容和语义。其次，我们展示了增强的特征可以用于执行语义分割的零样本领域自适应。实验表明，我们的方法在几个数据集上显著优于基于CLIP的风格转移基线，用于下游任务。我们的基于提示的方法甚至在某些数据集上优于一次性无监督领域自适应，并且gi

    Domain adaptation has been vastly investigated in computer vision but still requires access to target images at train time, which might be intractable in some uncommon conditions. In this paper, we propose the task of `Prompt-driven Zero-shot Domain Adaptation', where we adapt a model trained on a source domain using only a single general textual description of the target domain, i.e., a prompt. First, we leverage a pretrained contrastive vision-language model (CLIP) to optimize affine transformations of source features, steering them towards target text embeddings, while preserving their content and semantics. Second, we show that augmented features can be used to perform zero-shot domain adaptation for semantic segmentation. Experiments demonstrate that our method significantly outperforms CLIP-based style transfer baselines on several datasets for the downstream task at hand. Our prompt-driven approach even outperforms one-shot unsupervised domain adaptation on some datasets, and gi
    
[^143]: 超越对象识别：面向对象概念学习的新基准

    Beyond Object Recognition: A New Benchmark towards Object Concept Learning. (arXiv:2212.02710v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.02710](http://arxiv.org/abs/2212.02710)

    本文提出了一个挑战性的 Object Concept Learning (OCL) 任务，涉及对象属性、作用及其因果关系。作者构建了密集注释的知识库以支持 OCL，提出了 Object Concept Reasoning Network (OCRN) 作为基线，提升了对象认知的发展。

    

    理解对象是人工智能的核心，特别是对于具有体验的人工智能而言。虽然深度学习在对象识别方面表现出色，但当前机器仍然难以学习更高层次的知识，例如对象具有哪些属性，以及我们能够使用对象做什么。在本文中，我们提出了一项具有挑战性的 Object Concept Learning (OCL) 任务，以推动对象理解的发展。它要求机器推理出对象的作用，并同时给出原因：是哪些属性使得一个对象具有这些作用。为了支持 OCL，我们构建了一个密集注释的知识库，包括三个层次的对象概念（类别、属性、作用），以及三个层次的因果关系。通过分析 OCL 的因果结构，我们提出了一种基线：Object Concept Reasoning Network (OCRN)。它利用因果干预和概念实例化来推断三个层次，遵循它们之间的因果关系。

    Understanding objects is a central building block of artificial intelligence, especially for embodied AI. Even though object recognition excels with deep learning, current machines still struggle to learn higher-level knowledge, e.g., what attributes an object has, and what can we do with an object. In this work, we propose a challenging Object Concept Learning (OCL) task to push the envelope of object understanding. It requires machines to reason out object affordances and simultaneously give the reason: what attributes make an object possesses these affordances. To support OCL, we build a densely annotated knowledge base including extensive labels for three levels of object concept (category, attribute, affordance), and the causal relations of three levels. By analyzing the causal structure of OCL, we present a baseline, Object Concept Reasoning Network (OCRN). It leverages causal intervention and concept instantiation to infer the three levels following their causal relations. In ex
    
[^144]: PhysDiff: 物理引导的人体运动扩散模型

    PhysDiff: Physics-Guided Human Motion Diffusion Model. (arXiv:2212.02500v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.02500](http://arxiv.org/abs/2212.02500)

    PhysDiff是一种物理引导的人体运动扩散模型，通过将物理约束融入扩散过程中，能够生成多样而逼真的人体动作，并解决了现有模型中存在的物理缺陷问题。

    

    降噪扩散模型为生成多样而逼真的人体动作提供了巨大的潜力。然而，现有的运动扩散模型在扩散过程中往往忽视了物理定律，并且经常生成出具有明显缺陷的不符合物理规律的动作，如浮空、滑脚和透地。这严重影响了生成动作的质量并限制了它们在现实世界中的应用。为了解决这个问题，我们提出了一种新的物理引导的运动扩散模型（PhysDiff），它将物理约束融入到扩散过程中。具体地，我们提出了一个基于物理模拟器中运动模仿的物理运动投影模块，用于将扩散步骤的去噪动作投影为符合物理规律的动作。投影后的动作进一步用于下一个扩散步骤，以引导去噪扩散过程。直观地说，我们模型中使用物理定律使得动作逐步趋向于符合物理规律。

    Denoising diffusion models hold great promise for generating diverse and realistic human motions. However, existing motion diffusion models largely disregard the laws of physics in the diffusion process and often generate physically-implausible motions with pronounced artifacts such as floating, foot sliding, and ground penetration. This seriously impacts the quality of generated motions and limits their real-world application. To address this issue, we present a novel physics-guided motion diffusion model (PhysDiff), which incorporates physical constraints into the diffusion process. Specifically, we propose a physics-based motion projection module that uses motion imitation in a physics simulator to project the denoised motion of a diffusion step to a physically-plausible motion. The projected motion is further used in the next diffusion step to guide the denoising diffusion process. Intuitively, the use of physics in our model iteratively pulls the motion toward a physically-plausib
    
[^145]: 迈向实用的少样本联邦自然语言处理

    Towards Practical Few-shot Federated NLP. (arXiv:2212.00192v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.00192](http://arxiv.org/abs/2212.00192)

    本论文介绍了一个用于处理联邦自然语言处理中少样本问题的方法，通过引入数据生成器和基于提示的联邦学习系统，能够在有限的标记数据下实现与完整微调相媲美的性能。然而，这种性能要求付出显著的系统成本。

    

    基于Transformer的预训练模型已成为自然语言处理(NLP)的主流解决方案。为了在下游任务中对这些预训练模型进行微调，通常需要大量标注的私有数据。实际上，私有数据通常分布在异构的移动设备上，并且可能被禁止上传。此外，精心策划的标记数据通常很稀缺，这增加了另一个挑战。为了解决这些问题，我们首先引入了一个用于联邦少样本学习任务的数据生成器，它模拟了现实情况下稀缺标记数据的数量和偏斜性。随后，我们提出了AUG-FedPrompt，这是一个基于提示的联邦学习系统，利用大量无标签数据进行数据增强。我们的实验证明，在有限量的标记数据下，AUG-FedPrompt能够与完整微调相媲美。然而，这种竞争性能是以显著的系统成本为代价的。

    Transformer-based pre-trained models have emerged as the predominant solution for natural language processing (NLP). Fine-tuning such pre-trained models for downstream tasks often requires a considerable amount of labeled private data. In practice, private data is often distributed across heterogeneous mobile devices and may be prohibited from being uploaded. Moreover, well-curated labeled data is often scarce, presenting an additional challenge. To address these challenges, we first introduce a data generator for federated few-shot learning tasks, which encompasses the quantity and skewness of scarce labeled data in a realistic setting. Subsequently, we propose AUG-FedPrompt, a prompt-based federated learning system that exploits abundant unlabeled data for data augmentation. Our experiments indicate that AUG-FedPrompt can perform on par with full-set fine-tuning with a limited amount of labeled data. However, such competitive performance comes at a significant system cost.
    
[^146]: 基于配对互补时间循环一致对抗网络的雷达降水预测方法

    PCT-CycleGAN: Paired Complementary Temporal Cycle-Consistent Adversarial Networks for Radar-Based Precipitation Nowcasting. (arXiv:2211.15046v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15046](http://arxiv.org/abs/2211.15046)

    本文提出了一种基于配对互补时间循环一致对抗网络的雷达降水预测方法，该方法包括两个生成器网络和循环一致性损失和对抗性损失。实验证明，该方法在准确性和推广能力方面优于现有的技术方法。

    

    降水预测方法已经在过去几个世纪里得到了很好的发展，因为雨水对人类生活有着至关重要的影响。现有的降水预测模型包括定量降水预测 (QPF) 模型、卷积长短期记忆 (ConvLSTM) 模型以及最新的 MetNet-2 等多种复杂的方法。本文提出了基于配对互补时间循环一致对抗网络 (PCT-CycleGAN) 的雷达降水预测方法，受对抗生成网络 (CycleGAN) 强大的图像转换性能启发。PCT-CycleGAN 使用两个具有向前和向后时间动态的生成器网络生成时序性，每个生成器网络学习一个庞大的一对一映射，以逼近表示每个方向上的时间动态的映射函数。为了创建配对互补循环之间的强健时间因果关系，我们应用了循环一致性损失和对抗性损失。广泛的实验证明，PCT-CycleGAN 在准确性和推广能力方面优于现有的技术方法。

    The precipitation nowcasting methods have been elaborated over the centuries because rain has a crucial impact on human life. Not only quantitative precipitation forecast (QPF) models and convolutional long short-term memory (ConvLSTM), but also various sophisticated methods such as the latest MetNet-2 are emerging. In this paper, we propose a paired complementary temporal cycle-consistent adversarial networks (PCT-CycleGAN) for radar-based precipitation nowcasting, inspired by cycle-consistent adversarial networks (CycleGAN), which shows strong performance in image-to-image translation. PCT-CycleGAN generates temporal causality using two generator networks with forward and backward temporal dynamics in paired complementary cycles. Each generator network learns a huge number of one-to-one mappings about time-dependent radar-based precipitation data to approximate a mapping function representing the temporal dynamics in each direction. To create robust temporal causality between paired 
    
[^147]: 在线集成持续学习的神经结构

    Neural Architecture for Online Ensemble Continual Learning. (arXiv:2211.14963v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.14963](http://arxiv.org/abs/2211.14963)

    提出了一种完全可微的在线集成持续学习方法，实验结果表明在没有内存缓冲区的情况下取得了最先进的结果，并且可以通过较少的分类器获得较高的分类准确性。

    

    随着类别数量的增加，持续学习变得越来越具有挑战性。当每个示例只出现一次时，学习变得更加困难，这要求模型能够在线学习。最近的方法在这种设置下往往面临困难，或者存在无法微分的组件或者内存缓冲区的限制。因此，我们提出了完全可微的集成方法，可以在端到端的方式下高效训练神经网络的集成模型。所提出的技术在没有内存缓冲区的情况下取得了最先进的结果，并明显优于参考方法。进行的实验还表明，对于小型的集成模型，性能有显著提高，这表明可以通过更少的分类器获得相对较高的分类准确性。

    Continual learning with an increasing number of classes is a challenging task. The difficulty rises when each example is presented exactly once, which requires the model to learn online. Recent methods with classic parameter optimization procedures have been shown to struggle in such setups or have limitations like non-differentiable components or memory buffers. For this reason, we present the fully differentiable ensemble method that allows us to efficiently train an ensemble of neural networks in the end-to-end regime. The proposed technique achieves SOTA results without a memory buffer and clearly outperforms the reference methods. The conducted experiments have also shown a significant increase in the performance for small ensembles, which demonstrates the capability of obtaining relatively high classification accuracy with a reduced number of classifiers.
    
[^148]: MPCViT：使用异构注意力搜索精准高效的MPC友好视觉Transformer

    MPCViT: Searching for Accurate and Efficient MPC-Friendly Vision Transformer with Heterogeneous Attention. (arXiv:2211.13955v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2211.13955](http://arxiv.org/abs/2211.13955)

    本文提出了一种MPC友好的视觉Transformer模型MPCViT，该模型通过异构注意力搜索来实现准确且高效的ViT推理，并通过简单而有效的神经架构搜索算法来进一步提高推理效率。

    

    安全多方计算(MPC)可以直接在加密数据上进行计算，并在深度学习推理中保护数据与模型的隐私。然而，现有的神经网络架构，包括Vision Transformers(ViTs)，并未专为MPC设计或优化，因此产生了显著的延迟开销。本文观察到Softmax是主要的延迟瓶颈，由于高通信复杂性，可有选择地替换或线性化而不影响模型准确性。因此，我们提出了MPC友好的ViT，称为MPCViT，在MPC中实现准确而高效的ViT推理。基于对Softmax注意力和其他注意力变体的系统延迟和精度评估，我们提出了一个异构注意力优化空间。我们还开发了一种简单而有效的MPC感知神经架构搜索算法，用于快速帕累托优化。为了进一步提高推理效率，我们提出了MPCViT+，以联合优化Softmax替换和模型压缩。

    Secure multi-party computation (MPC) enables computation directly on encrypted data and protects both data and model privacy in deep learning inference. However, existing neural network architectures, including Vision Transformers (ViTs), are not designed or optimized for MPC and incur significant latency overhead. We observe Softmax accounts for the major latency bottleneck due to a high communication complexity, but can be selectively replaced or linearized without compromising the model accuracy. Hence, in this paper, we propose an MPC-friendly ViT, dubbed MPCViT, to enable accurate yet efficient ViT inference in MPC. Based on a systematic latency and accuracy evaluation of the Softmax attention and other attention variants, we propose a heterogeneous attention optimization space. We also develop a simple yet effective MPC-aware neural architecture search algorithm for fast Pareto optimization. To further boost the inference efficiency, we propose MPCViT+, to jointly optimize the So
    
[^149]: 谎言可以被伪造吗？从机器学习的角度比较低风险和高风险的欺骗视频数据集

    Can lies be faked? Comparing low-stakes and high-stakes deception video datasets from a Machine Learning perspective. (arXiv:2211.13035v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.13035](http://arxiv.org/abs/2211.13035)

    该论文从机器学习的角度比较了低风险和高风险的欺骗视频数据集，并探究了它们在实际应用中的区别和可行性。

    

    尽管谎言对人类社会有着巨大影响，并且欺骗检测的人类准确率仅为54％，但机器学习系统仍然无法在现实应用中有效执行自动化的欺骗检测，原因在于数据稀缺。公开的欺骗检测数据集很少，并且由于低风险和高风险谎言之间的概念区别，创造新的数据集变得困难。理论上，这两种谎言是如此不同，以至于一个种类的数据集不能用于另一个种类的应用。尽管获得低风险欺骗的数据更容易，因为它可以在受控环境中进行模拟（伪造），但这些谎言不具有与真实的高风险谎言相同的重要性或深度，而真实的高风险谎言很难获得，并且对于自动化欺骗检测系统具有实际兴趣。为了从实践角度探究这个区别是否成立，我们设计了几个实验，比较了一个高风险的欺骗检测数据集和一个低风险数据集。

    Despite the great impact of lies in human societies and a meager 54% human accuracy for Deception Detection (DD), Machine Learning systems that perform automated DD are still not viable for proper application in real-life settings due to data scarcity. Few publicly available DD datasets exist and the creation of new datasets is hindered by the conceptual distinction between low-stakes and high-stakes lies. Theoretically, the two kinds of lies are so distinct that a dataset of one kind could not be used for applications for the other kind. Even though it is easier to acquire data on low-stakes deception since it can be simulated (faked) in controlled settings, these lies do not hold the same significance or depth as genuine high-stakes lies, which are much harder to obtain and hold the practical interest of automated DD systems. To investigate whether this distinction holds true from a practical perspective, we design several experiments comparing a high-stakes DD dataset and a low-stak
    
[^150]: 一种 $k$ 最近邻的两阶段主动学习算法

    A Two-Stage Active Learning Algorithm for $k$-Nearest Neighbors. (arXiv:2211.10773v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.10773](http://arxiv.org/abs/2211.10773)

    本文提出了一种用于训练 $k$ 最近邻分类器的简单直观的主动学习算法，首次保持了 $k$ 最近邻投票概念的预测时间，并提供了一致性保证。

    

    $k$ 最近邻分类是一种流行的非参数方法，因为它具有自动适应分布比例变化等优点。然而，迄今为止，针对自然保留这些优秀特性的本地投票分类器的训练，设计主动学习策略一直是困难的，因此 $k$ 最近邻分类的主动学习策略在文献中一直缺失。在本文中，我们介绍了一种简单直观的主动学习算法，用于训练 $k$ 最近邻分类器，这是文献中第一次保持了 $k$ 最近邻投票概念的预测时间。我们为通过我们方案获取的样本提供了一种修改的 $k$ 最近邻分类器的一致性保证，并且当条件概率函数 $\mathbb{P}(Y=y|X=x)$ 足够平滑并且 Tsybakov 噪声条件成立时，我们的主动训练的方法表现良好。

    $k$-nearest neighbor classification is a popular non-parametric method because of desirable properties like automatic adaption to distributional scale changes. Unfortunately, it has thus far proved difficult to design active learning strategies for the training of local voting-based classifiers that naturally retain these desirable properties, and hence active learning strategies for $k$-nearest neighbor classification have been conspicuously missing from the literature. In this work, we introduce a simple and intuitive active learning algorithm for the training of $k$-nearest neighbor classifiers, the first in the literature which retains the concept of the $k$-nearest neighbor vote at prediction time. We provide consistency guarantees for a modified $k$-nearest neighbors classifier trained on samples acquired via our scheme, and show that when the conditional probability function $\mathbb{P}(Y=y|X=x)$ is sufficiently smooth and the Tsybakov noise condition holds, our actively trained
    
[^151]: 基于SPD流形的图神经网络用于运动想象分类：来自时频分析的视角

    Graph Neural Networks on SPD Manifolds for Motor Imagery Classification: A Perspective from the Time-Frequency Analysis. (arXiv:2211.02641v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2211.02641](http://arxiv.org/abs/2211.02641)

    本文介绍了一种基于SPD流形的图神经网络用于运动想象分类，利用EEG的二阶统计量，相比传统方法具有更好的性能。

    This paper introduces a graph neural network based on SPD manifolds for motor imagery classification, which utilizes second-order statistics of EEG signals and outperforms traditional methods.

    运动想象（MI）的分类是脑电图（EEG）基础脑机接口（BCI）领域中备受追捧的研究课题，具有巨大的商业价值。过去二十年，MI-EEG分类器的趋势发生了根本性的转变，其性能逐渐提高。 Tensor-CSPNet的出现是BCI研究中第一个几何深度学习（GDL）框架的必要性，其归因于信号的非欧几里德性质的特征化。从根本上讲，Tensor-CSPNet是一种基于深度学习的分类器，利用EEG的二阶统计量。与利用EEG信号的一阶统计量的传统方法相比，利用这些二阶统计量代表了经典的处理方法。这些统计量提供了足够的区分信息，使它们适用于MI-EEG分类。在本研究中，我们介绍了另一种GDL分类器，

    The classification of motor imagery (MI) is a highly sought-after research topic in the field of Electroencephalography (EEG)-based brain-computer interfaces (BCIs), with immense commercial value. Over the past two decades, there has been a fundamental shift in the trend of MI-EEG classifiers, resulting in a gradual increase in their performance. The emergence of Tensor-CSPNet, the first geometric deep learning (GDL) framework in BCI research, is attributed to the imperative of characterizing the non-Euclidean nature of signals. Fundamentally, Tensor-CSPNet is a deep learning-based classifier that capitalizes on the second-order statistics of EEGs. In contrast to the conventional approach of utilizing first-order statistics for EEG signals, the utilization of these second-order statistics represents the classical treatment. These statistics provide adequate discriminative information, rendering them suitable for MI-EEG classification. In this study, we introduce another GDL classifier,
    
[^152]: 电子商务平台上的广告最大化利润策略

    A Profit-Maximizing Strategy for Advertising on the e-Commerce Platforms. (arXiv:2211.01160v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2211.01160](http://arxiv.org/abs/2211.01160)

    本文提出了一种针对在线广告定向选项的最大化利润策略，通过将优化问题重新定义为多选背包问题，找到最佳特征组合以增加将定向受众转化为实际购买者的概率。

    

    在线广告管理平台已经越来越受到电子商务卖家/广告商的欢迎，提供了一个简化的方法来吸引目标客户。尽管有其优势，但对于资源有限的在线卖家来说，正确配置广告策略仍然是一个挑战。无效的策略往往会导致大量的无效点击，从而导致广告费用与销售增长不成比例。在本文中，我们提出了一种针对在线广告定向选项的新颖的最大化利润策略。所提出的模型旨在找到最佳的特征组合，以最大化将定向受众转化为实际购买者的概率。我们通过将优化挑战重新定义为多选背包问题（MCKP）来解决优化问题。我们进行了一项实证研究，使用来自天猫的真实数据，证明了我们提出的方法可以有效地优化广告策略。

    The online advertising management platform has become increasingly popular among e-commerce vendors/advertisers, offering a streamlined approach to reach target customers. Despite its advantages, configuring advertising strategies correctly remains a challenge for online vendors, particularly those with limited resources. Ineffective strategies often result in a surge of unproductive ``just looking'' clicks, leading to disproportionately high advertising expenses comparing to the growth of sales. In this paper, we present a novel profit-maximing strategy for targeting options of online advertising. The proposed model aims to find the optimal set of features to maximize the probability of converting targeted audiences into actual buyers. We address the optimization challenge by reformulating it as a multiple-choice knapsack problem (MCKP). We conduct an empirical study featuring real-world data from Tmall to show that our proposed method can effectively optimize the advertising strategy
    
[^153]: 理解脸部匿名逆转：Fant\^omas研究

    Fant\^omas: Understanding Face Anonymization Reversibility. (arXiv:2210.10651v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2210.10651](http://arxiv.org/abs/2210.10651)

    本文对脸部匿名逆转现象进行了全面的研究，发现11种脸部匿名化方法至少部分可逆，并强调了重构和反演实现可逆性的机制。

    

    脸部图像是一个丰富的信息源，可以用来识别个人并推断他们的私人信息。为了减轻这种隐私风险，匿名化方法使用对清晰图像进行转换以混淆敏感信息，同时保留一定的实用性。然而，虽然它们以令人印象深刻的声明发表，但有时并未经过令人信服的方法评估。将匿名化图像逆转回仿真其真实输入的程度，甚至能被人脸识别方法识别，这对于匿名化的缺陷是最强有力的指标。最近的一些研究结果的确表明，对于某些方法而言这是可能的。然而，目前对于哪些方法是可逆的，以及为什么可逆还不太清楚。在本文中，我们对脸部匿名逆转现象进行了全面的研究。我们发现，在经过测试的15种方法中，有11种至少部分可逆，并重点介绍了重构和反演是如何实现可逆性的。

    Face images are a rich source of information that can be used to identify individuals and infer private information about them. To mitigate this privacy risk, anonymizations employ transformations on clear images to obfuscate sensitive information, all while retaining some utility. Albeit published with impressive claims, they sometimes are not evaluated with convincing methodology.  Reversing anonymized images to resemble their real input -- and even be identified by face recognition approaches -- represents the strongest indicator for flawed anonymization. Some recent results indeed indicate that this is possible for some approaches. It is, however, not well understood, which approaches are reversible, and why. In this paper, we provide an exhaustive investigation in the phenomenon of face anonymization reversibility. Among other things, we find that 11 out of 15 tested face anonymizations are at least partially reversible and highlight how both reconstruction and inversion are the u
    
[^154]: 基于贝叶斯的提示学习用于图像-语言模型泛化

    Bayesian Prompt Learning for Image-Language Model Generalization. (arXiv:2210.02390v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.02390](http://arxiv.org/abs/2210.02390)

    本文提出了一种基于贝叶斯方法的提示学习框架，对提示空间进行正则化，提高了对未见提示的泛化能力。

    

    基础的图像-语言模型因其高效的适应下游任务的提示学习而引起了广泛关注。提示学习将语言模型输入的一部分视为可训练的，同时冻结其余部分，并优化经验风险最小化目标。然而，经验风险最小化已知受到分布偏移的影响，这影响了对训练过程中未见提示的泛化能力。通过利用贝叶斯方法的正则化能力，我们从贝叶斯角度考虑提示学习，并将其制定为变分推断问题。我们的方法对提示空间进行正则化，减少对已见提示的过度拟合，并提高了对未见提示的提示泛化能力。我们的框架通过以概率的方式对输入提示空间进行建模，作为先验分布，使我们的提议与基于图像无条件或有条件的提示学习方法兼容。我们进行了实验证明了本提出的方法的有效性。

    Foundational image-language models have generated considerable interest due to their efficient adaptation to downstream tasks by prompt learning. Prompt learning treats part of the language model input as trainable while freezing the rest, and optimizes an Empirical Risk Minimization objective. However, Empirical Risk Minimization is known to suffer from distributional shifts which hurt generalizability to prompts unseen during training. By leveraging the regularization ability of Bayesian methods, we frame prompt learning from the Bayesian perspective and formulate it as a variational inference problem. Our approach regularizes the prompt space, reduces overfitting to the seen prompts and improves the prompt generalization on unseen prompts. Our framework is implemented by modeling the input prompt space in a probabilistic manner, as an a priori distribution which makes our proposal compatible with prompt learning approaches that are unconditional or conditional on the image. We demon
    
[^155]: Markov线性随机逼近中的偏差和外推问题

    Bias and Extrapolation in Markovian Linear Stochastic Approximation with Constant Stepsizes. (arXiv:2210.00953v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.00953](http://arxiv.org/abs/2210.00953)

    本研究研究了线性随机逼近中的偏差和外推问题。我们证明了在恒定步长和Markov数据的情况下，LSA迭代会收敛到唯一的极限和稳定分布，并建立了非渐进的几何收敛速度。我们还发现，这个极限的偏差与步长成比例，直至更高阶项。在可逆链的情况下，我们还探讨了偏差与Markov数据的混合时间之间的关系。

    

    我们考虑了具有恒定步长和Markov数据的线性随机逼近（LSA）。将数据和LSA迭代的联合过程视为时间齐次Markov链，我们证明其在Wasserstein距离下收敛到唯一的极限和稳定分布，并建立了非渐进的几何收敛速度。此外，我们表明，该极限的偏差向量可以通过步长展开为无限级数。因此，偏差与步长成比例，直至更高阶项。这个结果与i.i.d.数据下的LSA形成对比，其中偏差为零。在可逆链设置下，我们提供了偏差与Markov数据的混合时间之间关系的一般特征，建立了它们大致成正比的结论。虽然Polyak-Ruppert尾平均减少了LSA迭代的方差，但并不影响偏差。以上特征使我们能够展示

    We consider Linear Stochastic Approximation (LSA) with a constant stepsize and Markovian data. Viewing the joint process of the data and LSA iterate as a time-homogeneous Markov chain, we prove its convergence to a unique limiting and stationary distribution in Wasserstein distance and establish non-asymptotic, geometric convergence rates. Furthermore, we show that the bias vector of this limit admits an infinite series expansion with respect to the stepsize. Consequently, the bias is proportional to the stepsize up to higher order terms. This result stands in contrast with LSA under i.i.d. data, for which the bias vanishes. In the reversible chain setting, we provide a general characterization of the relationship between the bias and the mixing time of the Markovian data, establishing that they are roughly proportional to each other.  While Polyak-Ruppert tail-averaging reduces the variance of the LSA iterates, it does not affect the bias. The above characterization allows us to show 
    
[^156]: 参数修剪的数据集蒸馏方法

    Dataset Distillation Using Parameter Pruning. (arXiv:2209.14609v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.14609](http://arxiv.org/abs/2209.14609)

    本文提出了一种使用参数修剪的数据集蒸馏方法，该方法可以在蒸馏过程中修剪难以匹配的参数，提高蒸馏性能。

    

    在许多领域中，获得先进模型的方法取决于大型数据集，这使得数据存储和模型训练变得昂贵。作为解决方案，数据集蒸馏可以合成保留原始大型数据集大多数信息的小型数据集。最近提出的匹配网络参数的数据集蒸馏方法已被证明在几个数据集上有效。然而，网络参数的维度通常很大。此外，一些参数在蒸馏过程中难以匹配，降低了蒸馏性能。基于这个观察，本研究提出了一种基于参数修剪的新型数据集蒸馏方法来解决这个问题。该方法可以在蒸馏过程中修剪难以匹配的参数，从而合成更加稳健的蒸馏数据集并提高蒸馏性能。在三个数据集上的实验结果表明，该方法优于其他最先进的数据集蒸馏方法。

    In many fields, the acquisition of advanced models depends on large datasets, making data storage and model training expensive. As a solution, dataset distillation can synthesize a small dataset that preserves most information of the original large dataset. The recently proposed dataset distillation method by matching network parameters has been proven effective for several datasets. However, the dimensions of network parameters are typically large. Furthermore, some parameters are difficult to match during the distillation process, degrading distillation performance. Based on this observation, this study proposes a novel dataset distillation method based on parameter pruning that solves the problem. The proposed method can synthesize more robust distilled datasets and improve distillation performance by pruning difficult-to-match parameters during the distillation process. Experimental results on three datasets show that the proposed method outperforms other state-of-the-art dataset d
    
[^157]: 学习利用弹性执行器进行四足动物的运动控制

    Learning to Exploit Elastic Actuators for Quadruped Locomotion. (arXiv:2209.07171v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2209.07171](http://arxiv.org/abs/2209.07171)

    本文提出了一种直接在真实机器人上学习无模型控制器的方法，利用弹性执行器的动力学性质优化动态运动控制，学习到的控制器可实现高性能的四足运动控制。

    

    在四足运动中，基于弹簧的执行器可以提高能效和性能，但也增加了控制器设计的难度。以往的工作主要是通过建模与仿真来寻找最优的控制器，而本文提出直接在真实机器人上学习无模型控制器。我们的方法是首先由中央模式发生器（CPGs）合成步态，通过优化这些参数得到一种能够实现高效运动的开环控制器。然后，为了使控制器更加稳健并进一步提高性能，本文采用强化学习来闭环控制，学习在CPGs之上的矫正动作。我们在DLR弹性四足机器人上评估了所提出的方法。学习到的慢跑和跳跃步态表明，在优化动态运动时利用弹性执行器动力学是自然发生的，尽管无模型控制器，但仍可实现高性能的运动控制。

    Spring-based actuators in legged locomotion provide energy-efficiency and improved performance, but increase the difficulty of controller design. While previous work has focused on extensive modeling and simulation to find optimal controllers for such systems, we propose to learn model-free controllers directly on the real robot. In our approach, gaits are first synthesized by central pattern generators (CPGs), whose parameters are optimized to quickly obtain an open-loop controller that achieves efficient locomotion. Then, to make this controller more robust and further improve the performance, we use reinforcement learning to close the loop, to learn corrective actions on top of the CPGs. We evaluate the proposed approach on the DLR elastic quadruped bert. Our results in learning trotting and pronking gaits show that exploitation of the spring actuator dynamics emerges naturally from optimizing for dynamic motions, yielding high-performing locomotion despite being model-free. The who
    
[^158]: 多点-BAX: 一种通过虚拟目标高效调整粒子加速器发射度的新方法

    Multipoint-BAX: A New Approach for Efficiently Tuning Particle Accelerator Emittance via Virtual Objectives. (arXiv:2209.04587v4 [physics.acc-ph] UPDATED)

    [http://arxiv.org/abs/2209.04587](http://arxiv.org/abs/2209.04587)

    本论文提出了一种名为多点-BAX的新方法，通过虚拟目标来高效调整粒子加速器的发射度。该方法避免了使用传统的黑盒优化器进行缓慢而低效的多点查询，并通过快速学习模型计算发射度目标。该方法在Linac相干光源(LCLS)和Facility for Adv中最小化发射度。

    

    尽管束发射度对于高亮度加速器的性能至关重要，但优化通常会受到时间限制，因为发射度计算通常是通过四极扫描完成的，而四极扫描通常较慢。这种计算是一种多点查询，即每个查询都需要多个辅助测量。传统的黑盒优化器，如贝叶斯优化，在处理这样的目标时速度慢且效率低下，因为它们必须获取完整的测量序列，但每个查询仅返回发射度。我们提出将贝叶斯算法执行(BAX)应用于查询和建模单个束流尺寸测量。BAX通过使用快速学习模型而不是直接从加速器中获取发射度指标来避免在加速器上进行缓慢的多点查询。在这里，我们使用BAX来最小化Linac相干光源(LCLS)和Facility for Adv的发射度。

    Although beam emittance is critical for the performance of high-brightness accelerators, optimization is often time limited as emittance calculations, commonly done via quadrupole scans, are typically slow. Such calculations are a type of $\textit{multi-point query}$, i.e. each query requires multiple secondary measurements. Traditional black-box optimizers such as Bayesian optimization are slow and inefficient when dealing with such objectives as they must acquire the full series of measurements, but return only the emittance, with each query. We propose applying Bayesian Algorithm Execution (BAX) to instead query and model individual beam-size measurements. BAX avoids the slow multi-point query on the accelerator by acquiring points through a $\textit{virtual objective}$, i.e. calculating the emittance objective from a fast learned model rather than directly from the accelerator. Here, we use BAX to minimize emittance at the Linac Coherent Light Source (LCLS) and the Facility for Adv
    
[^159]: 从可表达性到可执行性：在有限范围内实现自顶向下的自动化开发的神经符号框架

    Towards Top-Down Automated Development in Limited Scopes: A Neuro-Symbolic Framework from Expressibles to Executables. (arXiv:2209.01566v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2209.01566](http://arxiv.org/abs/2209.01566)

    本研究提出了一个神经符号框架，从可表达性到可执行性实现了自顶向下的自动化开发。通过建立代码分类法和使用语义金字塔来关联文本数据和代码数据，我们可以改进深度代码生成的效果。

    

    深度代码生成是软件工程中深度学习的一个主题，采用神经模型为预期功能生成代码。由于端到端神经方法缺乏领域知识和软件层次意识，它们在项目级任务上表现不佳。为了系统地探索代码生成的潜在改进，我们让其参与从“可表达性”到“可执行性”的自顶向下开发，这在有限的范围内是可能的。在这个过程中，它从大量的样本、特征和知识中受益。作为基础，我们建议在代码数据上建立一个分类法，即代码分类法，利用代码信息的分类。此外，我们引入了一个三层语义金字塔(SP)来关联文本数据和代码数据。它识别不同抽象层次的信息，从而引入了关于开发的领域知识，并揭示了软件的层次结构。

    Deep code generation is a topic of deep learning for software engineering (DL4SE), which adopts neural models to generate code for the intended functions. Since end-to-end neural methods lack domain knowledge and software hierarchy awareness, they tend to perform poorly w.r.t project-level tasks. To systematically explore the potential improvements of code generation, we let it participate in the whole top-down development from \emph{expressibles} to \emph{executables}, which is possible in limited scopes. In the process, it benefits from massive samples, features, and knowledge. As the foundation, we suggest building a taxonomy on code data, namely code taxonomy, leveraging the categorization of code information. Moreover, we introduce a three-layer semantic pyramid (SP) to associate text data and code data. It identifies the information of different abstraction levels, and thus introduces the domain knowledge on development and reveals the hierarchy of software. Furthermore, we propo
    
[^160]: 一些监督是必须的：通过认知不确定性度量在强化学习中引入神谕策略

    Some Supervision Required: Incorporating Oracle Policies in Reinforcement Learning via Epistemic Uncertainty Metrics. (arXiv:2208.10533v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.10533](http://arxiv.org/abs/2208.10533)

    本文提出一种名为评判置信度引导探索的方法，用于将现有的神谕策略纳入标准的演员-评论家强化学习算法中，以提高探索效率。在不确定性高时，该方法会将神谕策略的行动作为建议纳入学习方案中，而在不确定性低时忽略它。

    

    强化学习的固有问题是通过随机行动探索环境，其中很大一部分可能是无效的。相反，可以通过使用现有的（先前学习的或硬编码的）神谕策略、离线数据或演示来改善探索。但在使用神谕策略的情况下，如何最大化学习样本效率地将神谕经验融入到学习策略中可能不清楚。本文提出了一种名为评判置信度引导探索（Critic Confidence Guided Exploration，CCGE）的方法，用于将这样的神谕策略纳入标准的演员-评论家强化学习算法中。具体而言，当不确定性高时，CCGE以神谕策略的行动为建议，并将此信息纳入学习方案中，而当不确定性低时忽略它。CCGE对不确定性估计方法不加区分，并且我们证明它与现有算法相当。

    An inherent problem of reinforcement learning is performing exploration of an environment through random actions, of which a large portion can be unproductive. Instead, exploration can be improved by initializing the learning policy with an existing (previously learned or hard-coded) oracle policy, offline data, or demonstrations. In the case of using an oracle policy, it can be unclear how best to incorporate the oracle policy's experience into the learning policy in a way that maximizes learning sample efficiency. In this paper, we propose a method termed Critic Confidence Guided Exploration (CCGE) for incorporating such an oracle policy into standard actor-critic reinforcement learning algorithms. More specifically, CCGE takes in the oracle policy's actions as suggestions and incorporates this information into the learning scheme when uncertainty is high, while ignoring it when the uncertainty is low. CCGE is agnostic to methods of estimating uncertainty, and we show that it is equa
    
[^161]: 走向透明AI: 对深度神经网络内部结构的解释的调查

    Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks. (arXiv:2207.13243v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.13243](http://arxiv.org/abs/2207.13243)

    这篇综述调查了深度神经网络内部结构内部解释方法，并提出了一种分析方法的分类。这些解释方法对于帮助构建更可信赖的AI是至关重要的。

    

    过去十年的机器学习取得了巨大的规模和能力的增长，深度神经网络(DNNs)越来越多地被部署在现实世界中。然而，它们很难分析，这引发了对在不彻底理解其工作原理的情况下使用它们的担忧。解释它们的有效工具将对构建更可信赖的AI非常重要，通过帮助识别问题、修复错误和增进基本理解。特别是，"内部"可解释性技术，它们专注于解释DNNs的内部组件，非常适合于开发机械理解、指导手动修改和逆向工程解决方案。最近的研究主要集中在DNN可解释性上，迅速取得的进展使得对方法进行彻底系统化的困难。在这篇调查中，我们回顾了300多篇作品，重点关注内部可解释性工具。我们引入了一种分类方法，将方法按网络的哪个部分进行分类。

    The last decade of machine learning has seen drastic increases in scale and capabilities. Deep neural networks (DNNs) are increasingly being deployed in the real world. However, they are difficult to analyze, raising concerns about using them without a rigorous understanding of how they function. Effective tools for interpreting them will be important for building more trustworthy AI by helping to identify problems, fix bugs, and improve basic understanding. In particular, "inner" interpretability techniques, which focus on explaining the internal components of DNNs, are well-suited for developing a mechanistic understanding, guiding manual modifications, and reverse engineering solutions.  Much recent work has focused on DNN interpretability, and rapid progress has thus far made a thorough systematization of methods difficult. In this survey, we review over 300 works with a focus on inner interpretability tools. We introduce a taxonomy that classifies methods by what part of the netwo
    
[^162]: 通过图对比学习减轻DP满足的联邦设置中的性能损失

    Mitigating the Performance Sacrifice in DP-Satisfied Federated Settings through Graph Contrastive Learning. (arXiv:2207.11836v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.11836](http://arxiv.org/abs/2207.11836)

    本文研究了如何在联邦图学习中实现差分隐私，并观察到性能下降。图边上的差分隐私引入的噪声扰乱了图的相似性，限制了图学习模型的性能。

    

    目前，图学习模型是帮助研究人员探索图结构化数据的不可或缺的工具。在学术界，使用足够的训练数据在单个设备上优化图模型是训练强大图学习模型的典型方法。然而，由于隐私问题，在实际场景中这样做是不可行的。联邦学习通过引入各种隐私保护机制（例如基于图边的差分隐私），提供了解决这个限制的实际方法。然而，尽管联邦图学习中的差分隐私能确保表示在图中的敏感信息的安全，但通常会导致图学习模型的性能下降。在本文中，我们研究了如何在图边上实现差分隐私，并在实验中观察到性能下降。此外，我们注意到图边上的差分隐私引入噪声扰乱了图的相似性，这是图对比增强的一种方式。

    Currently, graph learning models are indispensable tools to help researchers explore graph-structured data. In academia, using sufficient training data to optimize a graph model on a single device is a typical approach for training a capable graph learning model. Due to privacy concerns, however, it is infeasible to do so in real-world scenarios. Federated learning provides a practical means of addressing this limitation by introducing various privacy-preserving mechanisms, such as differential privacy (DP) on the graph edges. However, although DP in federated graph learning can ensure the security of sensitive information represented in graphs, it usually causes the performance of graph learning models to degrade. In this paper, we investigate how DP can be implemented on graph edges and observe a performance decrease in our experiments. In addition, we note that DP on graph edges introduces noise that perturbs graph proximity, which is one of the graph augmentations in graph contrast
    
[^163]: 使用Transformer和图神经网络预测逻辑综合结果的质量

    The prediction of the quality of results in Logic Synthesis using Transformer and Graph Neural Networks. (arXiv:2207.11437v2 [cs.AR] UPDATED)

    [http://arxiv.org/abs/2207.11437](http://arxiv.org/abs/2207.11437)

    该论文介绍了一种使用Transformer和图神经网络预测逻辑综合结果质量的深度学习方法，通过将结构转换表示为向量并提取优化序列的特征，以及利用图神经网络学习电路的图表示和预测QoR。

    

    在逻辑综合阶段，综合工具中的结构转换需要与优化序列结合，并作用于电路，以满足指定的电路面积和延迟。然而，逻辑综合优化序列的运行时间较长，为电路对综合优化序列的结果质量（QoR）进行预测可以帮助工程师更快地找到更好的优化序列。在这项工作中，我们提出了一种深度学习方法，用于预测未见过的电路-优化序列对的QoR。具体而言，通过嵌入方法将结构转换转化为向量，并利用先进的自然语言处理（NLP）技术（Transformer）提取优化序列的特征。此外，为了使模型的预测过程能够从电路泛化到电路，电路的图表示被表示为邻接矩阵和特征矩阵。图神经网络被用于学习电路的图表示和预测QoR。

    In the logic synthesis stage, structure transformations in the synthesis tool need to be combined into optimization sequences and act on the circuit to meet the specified circuit area and delay. However, logic synthesis optimization sequences are time-consuming to run, and predicting the quality of the results (QoR) against the synthesis optimization sequence for a circuit can help engineers find a better optimization sequence faster. In this work, we propose a deep learning method to predict the QoR of unseen circuit-optimization sequences pairs. Specifically, the structure transformations are translated into vectors by embedding methods and advanced natural language processing (NLP) technology (Transformer) is used to extract the features of the optimization sequences. In addition, to enable the prediction process of the model to be generalized from circuit to circuit, the graph representation of the circuit is represented as an adjacency matrix and a feature matrix. Graph neural net
    
[^164]: 用于黑盒优化的生成预训练

    Generative Pretraining for Black-Box Optimization. (arXiv:2206.10786v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.10786](http://arxiv.org/abs/2206.10786)

    该论文提出了一种使用离线数据预训练黑盒优化器的生成框架BONET，使用自回归模型和样本策略合成轨迹以帮助在高维空间中优化昂贵的黑盒函数。

    

    科学和工程中的许多问题涉及在高维空间中优化昂贵的黑盒函数。对于这样的黑盒优化 (BBO) 问题，我们通常假设在线函数评估的预算很小，但往往可以访问用于预训练的固定离线数据集。之前的方法试图利用离线数据来逼近函数或其反函数，但在离数据分布较远时不够精确。我们提出了BONET，这是一个利用离线数据集预训练黑盒优化器的生成框架。在BONET中，我们对来自离线数据集的定长轨迹训练一个自回归模型。我们设计了一种采样策略，使用从低保真度样本到高保真度样本的单调转换的简单启发式来合成来自离线数据的轨迹。在Design-Bench上使用被因果掩蔽的Transformer实例化BONET，并进行评估，我们在平均排名上排名第一。

    Many problems in science and engineering involve optimizing an expensive black-box function over a high-dimensional space. For such black-box optimization (BBO) problems, we typically assume a small budget for online function evaluations, but also often have access to a fixed, offline dataset for pretraining. Prior approaches seek to utilize the offline data to approximate the function or its inverse but are not sufficiently accurate far from the data distribution. We propose BONET, a generative framework for pretraining a novel black-box optimizer using offline datasets. In BONET, we train an autoregressive model on fixed-length trajectories derived from an offline dataset. We design a sampling strategy to synthesize trajectories from offline data using a simple heuristic of rolling out monotonic transitions from low-fidelity to high-fidelity samples. Empirically, we instantiate BONET using a causally masked Transformer and evaluate it on Design-Bench, where we rank the best on averag
    
[^165]: StudioGAN: GAN图像合成的分类和基准库

    StudioGAN: A Taxonomy and Benchmark of GANs for Image Synthesis. (arXiv:2206.09479v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.09479](http://arxiv.org/abs/2206.09479)

    StudioGAN提供了一个分类和基准库，支持多种GAN架构、条件方法、对抗损失、正则化模块、可微增强方法、评估指标和评估骨架。这个基准库可以用于训练和评估各种GAN模型，提供了大规模的基准数据集和评估方案。

    

    生成对抗网络(GAN)是实现逼真图像合成的最先进的生成模型之一。然而，当前的GAN研究领域并没有提供可靠的基准库，从而无法进行一致且公平的评估。另外，由于缺乏验证的GAN实现，研究者们花费大量时间复现基准结果。我们研究了GAN方法的分类体系，并提出了一个名为StudioGAN的新的开源库。StudioGAN支持7种GAN架构、9种条件方法、4种对抗损失、12种正则化模块、3种可微增强方法、7种评估指标和5种评估骨架。通过我们的训练和评估方案，我们使用各种数据集(CIFAR10、ImageNet、AFHQv2、FFHQ和Baby/Papa/Granpa-ImageNet)以及3种不同的评估骨架(InceptionV3、SwAV和Swin Transformer)提出了一个大规模的基准库。

    Generative Adversarial Network (GAN) is one of the state-of-the-art generative models for realistic image synthesis. While training and evaluating GAN becomes increasingly important, the current GAN research ecosystem does not provide reliable benchmarks for which the evaluation is conducted consistently and fairly. Furthermore, because there are few validated GAN implementations, researchers devote considerable time to reproducing baselines. We study the taxonomy of GAN approaches and present a new open-source library named StudioGAN. StudioGAN supports 7 GAN architectures, 9 conditioning methods, 4 adversarial losses, 12 regularization modules, 3 differentiable augmentations, 7 evaluation metrics, and 5 evaluation backbones. With our training and evaluation protocol, we present a large-scale benchmark using various datasets (CIFAR10, ImageNet, AFHQv2, FFHQ, and Baby/Papa/Granpa-ImageNet) and 3 different evaluation backbones (InceptionV3, SwAV, and Swin Transformer). Unlike other benc
    
[^166]: 一个控制理论框架用于机器学习中的自适应梯度优化器

    A Control Theoretic Framework for Adaptive Gradient Optimizers in Machine Learning. (arXiv:2206.02034v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.02034](http://arxiv.org/abs/2206.02034)

    本文提出了一个通用的自适应梯度方法框架，用于解决非凸优化问题。文章将自适应梯度方法建模为状态空间框架，并利用经典控制理论中的传递函数范式提出了一种新的Adam变体，称为AdamSSM。该算法在基准机器学习任务上表现出较好的性能优势。

    

    自适应梯度方法已经在优化深度神经网络中变得流行起来，最近的例子包括AdaGrad和Adam。虽然Adam通常收敛更快，但是Adam的一些变体，比如AdaBelief算法，已经被提出来增强Adam与经典随机梯度方法相比的泛化能力较差的问题。本文提出了一个通用的自适应梯度方法框架用于解决非凸优化问题。我们首先在状态空间框架中建模自适应梯度方法，这使得我们能够简化自适应优化器（如AdaGrad、Adam和AdaBelief）的收敛证明。然后，我们利用经典控制理论中的传递函数范式，提出了一种新的Adam变体，称为AdamSSM。我们在传递函数中从平方梯度到二阶矩估计中添加了一个合适的极点-零点对。我们证明了所提出的AdamSSM算法的收敛性。在基准机器学习任务上的应用展示了该算法的性能优势。

    Adaptive gradient methods have become popular in optimizing deep neural networks; recent examples include AdaGrad and Adam. Although Adam usually converges faster, variations of Adam, for instance, the AdaBelief algorithm, have been proposed to enhance Adam's poor generalization ability compared to the classical stochastic gradient method. This paper develops a generic framework for adaptive gradient methods that solve non-convex optimization problems. We first model the adaptive gradient methods in a state-space framework, which allows us to present simpler convergence proofs of adaptive optimizers such as AdaGrad, Adam, and AdaBelief. We then utilize the transfer function paradigm from classical control theory to propose a new variant of Adam, coined AdamSSM. We add an appropriate pole-zero pair in the transfer function from squared gradients to the second moment estimate. We prove the convergence of the proposed AdamSSM algorithm. Applications on benchmark machine learning tasks of 
    
[^167]: 混合变量贝叶斯优化中的混合模型

    Hybrid Models for Mixed Variables in Bayesian Optimization. (arXiv:2206.01409v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.01409](http://arxiv.org/abs/2206.01409)

    本文提出了一种新型的混合模型，用于混合变量贝叶斯优化，并且在搜索和代理模型阶段都具有创新之处。数值实验证明了混合模型的优越性。

    

    本文提出了一种新型的混合模型，用于处理混合变量贝叶斯优化中的定量（连续和整数）和定性（分类）类型。我们的混合模型将蒙特卡洛树搜索结构（MCTS）用于分类变量，并将高斯过程（GP）用于连续变量。在搜索阶段中，我们将频率派的上置信度树搜索（UCTS）和贝叶斯狄利克雷搜索策略进行对比，展示了树结构在贝叶斯优化中的融合。在代理模型阶段，我们的创新之处在于针对混合变量贝叶斯优化的在线核选择。我们的创新，包括动态核选择、独特的UCTS（hybridM）和贝叶斯更新策略（hybridD），将我们的混合模型定位为混合变量代理模型的进步。数值实验凸显了混合模型的优越性，凸显了它们的潜力。

    This paper presents a new type of hybrid models for Bayesian optimization (BO) adept at managing mixed variables, encompassing both quantitative (continuous and integer) and qualitative (categorical) types. Our proposed new hybrid models merge Monte Carlo Tree Search structure (MCTS) for categorical variables with Gaussian Processes (GP) for continuous ones. Addressing efficiency in searching phase, we juxtapose the original (frequentist) upper confidence bound tree search (UCTS) and the Bayesian Dirichlet search strategies, showcasing the tree architecture's integration into Bayesian optimization. Central to our innovation in surrogate modeling phase is online kernel selection for mixed-variable BO. Our innovations, including dynamic kernel selection, unique UCTS (hybridM) and Bayesian update strategies (hybridD), position our hybrid models as an advancement in mixed-variable surrogate models. Numerical experiments underscore the hybrid models' superiority, highlighting their potentia
    
[^168]: 多项式时间算法在计数和采样马尔可夫等价的有向无环图中的应用

    Polynomial-Time Algorithms for Counting and Sampling Markov Equivalent DAGs with Applications. (arXiv:2205.02654v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.02654](http://arxiv.org/abs/2205.02654)

    本文提出了一种多项式时间算法，用于在马尔可夫等价类中计数和采样有向无环图。该算法解决了这一领域的长期未解决问题，并且在实验中得到验证，可以在活跃学习因果结构和因果效应识别方面实际应用。

    

    在图形因果分析中，从马尔可夫等价类中计数和采样有向无环图是基本任务。本文展示了这些任务可以在多项式时间内完成，解决了这一领域的长期未解决问题。我们的算法有效且易于实现。正如我们在实验中展示的那样，这些突破使得在活跃学习因果结构和因果效应识别方面，对于马尔可夫等价类，原本认为不可行的策略实际可应用。

    Counting and sampling directed acyclic graphs from a Markov equivalence class are fundamental tasks in graphical causal analysis. In this paper we show that these tasks can be performed in polynomial time, solving a long-standing open problem in this area. Our algorithms are effective and easily implementable. As we show in experiments, these breakthroughs make thought-to-be-infeasible strategies in active learning of causal structures and causal effect identification with regard to a Markov equivalence class practically applicable.
    
[^169]: ConceptEvo：解读深度学习训练中的概念演变

    ConceptEvo: Interpreting Concept Evolution in Deep Learning Training. (arXiv:2203.16475v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.16475](http://arxiv.org/abs/2203.16475)

    ConceptEvo是一个统一的深度神经网络解释框架，可以在训练过程中揭示概念的产生和演变，并通过人机评估和实验证明其发现对模型和预测具有重要意义。

    

    本文提出了ConceptEvo，一个用于深度神经网络（DNNs）的统一解释框架，可以揭示训练过程中学习概念的产生和演变。我们的工作填补了DNN解释研究中的一个关键空白，因为现有方法仅关注训练后的解释。ConceptEvo提出了两个新颖的技术贡献：（1）一种生成统一语义空间的算法，可以在训练过程中进行不同模型的并行比较；（2）一种发现和量化类别预测中重要概念演变的算法。通过与260名参与者进行大规模人机评估和定量实验，我们展示了ConceptEvo可以发现不同模型之间有意义且对预测重要的演变。ConceptEvo适用于现代（ConvNeXt）和经典的DNNs（例如VGGs，InceptionV3）。

    We present ConceptEvo, a unified interpretation framework for deep neural networks (DNNs) that reveals the inception and evolution of learned concepts during training. Our work fills a critical gap in DNN interpretation research, as existing methods focus on post-hoc interpretation after training. ConceptEvo presents two novel technical contributions: (1) an algorithm that generates a unified semantic space that enables side-by-side comparison of different models during training; and (2) an algorithm that discovers and quantifies important concept evolutions for class predictions. Through a large-scale human evaluation with 260 participants and quantitative experiments, we show that ConceptEvo discovers evolutions across different models that are meaningful to humans and important for predictions. ConceptEvo works for both modern (ConvNeXt) and classic DNNs (e.g., VGGs, InceptionV3).
    
[^170]: 神经数据压缩导论

    An Introduction to Neural Data Compression. (arXiv:2202.06533v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.06533](http://arxiv.org/abs/2202.06533)

    本文介绍了神经数据压缩领域，其中应用了神经网络和其他机器学习方法来进行数据压缩。通过使用生成模型，如标准化流、变分自编码器、扩散概率模型和生成对抗网络，可以从数据中端到端地学习压缩算法。本文提供了对必要背景知识（如信息论和计算机视觉）的回顾，并对目前文献中的基本思想和方法进行了精选指南。

    

    神经数据压缩是将神经网络和其他机器学习方法应用于数据压缩的领域。最近在统计机器学习方面的进展为数据压缩带来了新的可能性，通过使用强大的生成模型，如标准化流、变分自编码器、扩散概率模型和生成对抗网络，可以从数据中端到端地学习压缩算法。本文旨在通过回顾信息论（例如熵编码、率失真理论）和计算机视觉（例如图像质量评估、感知度量）等必要的背景知识，并提供对目前文献中基本思想和方法的精选指南，将这个研究领域介绍给更广泛的机器学习受众。

    Neural compression is the application of neural networks and other machine learning methods to data compression. Recent advances in statistical machine learning have opened up new possibilities for data compression, allowing compression algorithms to be learned end-to-end from data using powerful generative models such as normalizing flows, variational autoencoders, diffusion probabilistic models, and generative adversarial networks. The present article aims to introduce this field of research to a broader machine learning audience by reviewing the necessary background in information theory (e.g., entropy coding, rate-distortion theory) and computer vision (e.g., image quality assessment, perceptual metrics), and providing a curated guide through the essential ideas and methods in the literature thus far.
    
[^171]: 神经隐式曲面演变

    Neural Implicit Surface Evolution. (arXiv:2201.09636v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.09636](http://arxiv.org/abs/2201.09636)

    本文研究了神经网络在建模隐式曲面动态变化方面的应用，将其扩展到时空维度，实现了连续几何变换。通过考虑数据拟合和水平集方程的约束，网络能够快速收敛并逼近底层几何演化。

    

    本文研究使用平滑神经网络来建模隐式曲面在水平集方程下的动态变化。为此，它将神经隐式曲面的表示扩展到时空 $\mathbb{R}^3\times \mathbb{R}$，从而为连续几何变换提供了机制。例子包括将初始曲面演化为一般的向量场，使用平均曲率方程进行平滑和尖锐化，以及使用初始条件进行插值。网络训练考虑两个约束。数据项负责将初始条件适配到相应的时间点，通常为 $\mathbb{R}^3 \times \{0\}$。然后，水平集方程项迫使网络逼近水平集方程给出的底层几何演化，无需任何监督。网络还可以基于先前训练过的初始条件进行初始化，与标准方法相比，收敛速度更快。

    This work investigates the use of smooth neural networks for modeling dynamic variations of implicit surfaces under the level set equation (LSE). For this, it extends the representation of neural implicit surfaces to the space-time $\mathbb{R}^3\times \mathbb{R}$, which opens up mechanisms for continuous geometric transformations. Examples include evolving an initial surface towards general vector fields, smoothing and sharpening using the mean curvature equation, and interpolations of initial conditions.  The network training considers two constraints. A data term is responsible for fitting the initial condition to the corresponding time instant, usually $\mathbb{R}^3 \times \{0\}$. Then, a LSE term forces the network to approximate the underlying geometric evolution given by the LSE, without any supervision. The network can also be initialized based on previously trained initial conditions, resulting in faster convergence compared to the standard approach.
    
[^172]: 使用潜空间调整研究从轻度认知障碍到阿尔茨海默病的转化

    Investigating Conversion from Mild Cognitive Impairment to Alzheimer's Disease using Latent Space Manipulation. (arXiv:2111.08794v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.08794](http://arxiv.org/abs/2111.08794)

    本研究提出了一个深度学习框架，利用潜空间调整来发现轻度认知障碍（MCI）转化为阿尔茨海默病的重要属性并解析其行为。

    

    阿尔茨海默病是全球影响数百万人的最常见的痴呆病原因。研究阿尔茨海默病的潜在原因和风险因素对于预防其发展至关重要。轻度认知障碍（MCI）被认为是阿尔茨海默病之前的一个中间阶段。早期预测从MCI转化为阿尔茨海默病对于采取必要的预防措施以减缓病情进展和开发合适的治疗方法至关重要。在本研究中，我们提出了一个深度学习框架，用于发现作为MCI转化为阿尔茨海默病的标志性变量。特别地，我们通过操纵MCI和阿尔茨海默病患者训练的变分自编码器网络的潜空间，获取这些重要属性并解读它们导致MCI转化为阿尔茨海默病的行为。通过利用一个生成解码器和导致阿尔茨海默病诊断的维度，我们生成...

    Alzheimer's disease is the most common cause of dementia that affects millions of lives worldwide. Investigating the underlying causes and risk factors of Alzheimer's disease is essential to prevent its progression. Mild Cognitive Impairment (MCI) is considered an intermediate stage before Alzheimer's disease. Early prediction of the conversion from the MCI to Alzheimer's is crucial to take necessary precautions for decelerating the progression and developing suitable treatments. In this study, we propose a deep learning framework to discover the variables which are identifiers of the conversion from MCI to Alzheimer's disease. In particular, the latent space of a variational auto-encoder network trained with the MCI and Alzheimer's patients is manipulated to obtain the significant attributes and decipher their behavior that leads to the conversion from MCI to Alzheimer's disease. By utilizing a generative decoder and the dimensions that lead to the Alzheimer's diagnosis, we generate s
    
[^173]: 面向广义线性逆问题设计最优感知矩阵的研究

    Towards Designing Optimal Sensing Matrices for Generalized Linear Inverse Problems. (arXiv:2111.03237v3 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2111.03237](http://arxiv.org/abs/2111.03237)

    本文研究了设计最优感知矩阵对广义线性逆问题的重构性能的影响。通过对期望传播算法的性能进行研究，我们发现感知矩阵的频谱尖锐度对恢复性能很重要，并且尖锐度的影响取决于非线性函数$f$。根据我们的框架，我们提出了一种自适应的感知矩阵优化算法来最大化期望传播算法的性能。

    

    本文考虑一个逆问题$\mathbf{y}= f(\mathbf{Ax})$，其中$\mathbf{x}\in\mathbb{R}^n$是感兴趣信号，$\mathbf{A}$是感知矩阵，$f$是非线性函数，$\mathbf{y} \in \mathbb{R}^m$是测量向量。在许多应用中，我们可以自由设计感知矩阵$\mathbf{A}$，在这种情况下，我们可以优化$\mathbf{A}$以获得更好的重构性能。作为优化设计的第一步，了解感知矩阵对从$\mathbf{y}$恢复$\mathbf{x}$的难度的影响至关重要。本文研究了其中一种最成功的恢复方法，即期望传播(EP)算法的性能。我们定义了一种用于$\mathbf{A}$频谱的尖锐度概念，并展示了此度量对EP性能的重要性。我们证明了更尖锐的频谱可能会损害或改善恢复性能，这取决于$f$。根据我们的框架，我们可以设计出一种自适应的感知矩阵优化算法，该算法最大化了EP的性能。

    We consider an inverse problem $\mathbf{y}= f(\mathbf{Ax})$, where $\mathbf{x}\in\mathbb{R}^n$ is the signal of interest, $\mathbf{A}$ is the sensing matrix, $f$ is a nonlinear function and $\mathbf{y} \in \mathbb{R}^m$ is the measurement vector. In many applications, we have some level of freedom to design the sensing matrix $\mathbf{A}$, and in such circumstances we could optimize $\mathbf{A}$ to achieve better reconstruction performance. As a first step towards optimal design, it is important to understand the impact of the sensing matrix on the difficulty of recovering $\mathbf{x}$ from $\mathbf{y}$.  In this paper, we study the performance of one of the most successful recovery methods, i.e., the expectation propagation (EP) algorithm. We define a notion of spikiness for the spectrum of $\bmmathbfA}$ and show the importance of this measure for the performance of EP. We show that whether a spikier spectrum can hurt or help the recovery performance depends on $f$. Based on our frame
    
[^174]: MMD聚合双样本检验

    MMD Aggregated Two-Sample Test. (arXiv:2110.15073v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2110.15073](http://arxiv.org/abs/2110.15073)

    本文提出了两种新颖的基于最大均值差异（MMD）的非参数双样本核检验，并构造了一种自适应平均测试，称为MMDAgg，以解决平滑参数未知的问题。

    

    我们提出了两种新颖的基于最大均值差异（MMD）的非参数双样本核检验。首先，对于固定的核，我们使用排列或野蛮自举（wild bootstrap）构造了一个MMD检验，这两种流行的数值程序可确定测试阈值。我们证明这个测试可以在非渐近情况下控制I型错误的概率。因此，即使在小样本情况下，它仍然保持良好的校准性，这与以前的MMD测试不同，前者只能在渐近意义下保证正确的测试水平。当密度差异在Sobolev球中时，我们证明了我们的MMD检验在特定的核函数下是最优的，该核函数依赖于Sobolev球的平滑参数。在实践中，这个参数是未知的，因此不能使用具有特定核的最优MMD检验。为了解决这个问题，我们构造了一个自适应平均测试，称为MMDAgg。测试功率在Sobolev球的平滑参数上最大化。

    We propose two novel nonparametric two-sample kernel tests based on the Maximum Mean Discrepancy (MMD). First, for a fixed kernel, we construct an MMD test using either permutations or a wild bootstrap, two popular numerical procedures to determine the test threshold. We prove that this test controls the probability of type I error non-asymptotically. Hence, it can be used reliably even in settings with small sample sizes as it remains well-calibrated, which differs from previous MMD tests which only guarantee correct test level asymptotically. When the difference in densities lies in a Sobolev ball, we prove minimax optimality of our MMD test with a specific kernel depending on the smoothness parameter of the Sobolev ball. In practice, this parameter is unknown and, hence, the optimal MMD test with this particular kernel cannot be used. To overcome this issue, we construct an aggregated test, called MMDAgg, which is adaptive to the smoothness parameter. The test power is maximised ove
    
[^175]: CausalAF: 用于安全关键驾驶场景生成的因果自回归流

    CausalAF: Causal Autoregressive Flow for Safety-Critical Driving Scenario Generation. (arXiv:2110.13939v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2110.13939](http://arxiv.org/abs/2110.13939)

    CausalAF是一种用于安全关键驾驶场景生成的流模型，通过引入因果关系先验和新颖的因果掩蔽操作，将生成模型从仅从数据中学习相关性的情况扩展到学习生成场景如何引起风险情况的因果机制，以提高系统鲁棒性评估的多样性和效率。

    

    生成关键安全场景是评估自动驾驶系统鲁棒性的有效方式，但是场景的多样性和生成方法的效率受限于关键安全场景的稀缺性和结构。因此，仅从观测数据估计分布的现有生成模型不能很好地解决这个问题。在本文中，我们将因果关系作为先验融入到场景生成中，提出了一种基于流的生成框架——因果自回归流（CausalAF）。CausalAF通过新颖的因果掩蔽操作，鼓励生成模型揭示和遵循生成对象之间的因果关系，而不仅仅是从观测数据中随机采样。通过学习生成场景如何引起风险情况的因果机制，而不仅仅是从数据中学习相关性，CausalAF显著提高了生成模型的能力。

    Generating safety-critical scenarios, which are crucial yet difficult to collect, provides an effective way to evaluate the robustness of autonomous driving systems. However, the diversity of scenarios and efficiency of generation methods are heavily restricted by the rareness and structure of safety-critical scenarios. Therefore, existing generative models that only estimate distributions from observational data are not satisfying to solve this problem. In this paper, we integrate causality as a prior into the scenario generation and propose a flow-based generative framework, Causal Autoregressive Flow (CausalAF). CausalAF encourages the generative model to uncover and follow the causal relationship among generated objects via novel causal masking operations instead of searching the sample only from observational data. By learning the cause-and-effect mechanism of how the generated scenario causes risk situations rather than just learning correlations from data, CausalAF significantly
    
[^176]: 通过联合Wasserstein距离最小化学习域不变表示

    Learning Domain Invariant Representations by Joint Wasserstein Distance Minimization. (arXiv:2106.04923v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2106.04923](http://arxiv.org/abs/2106.04923)

    本文通过研究监督机器学习损失与联合空间的Wasserstein距离之间的关系，提出了一种学习域不变表示的方法。

    

    实际应用中，训练数据中的领域偏移很常见，例如数据来自不同的来源。理想情况下，机器学习模型应该在不考虑这些领域偏移的情况下表现良好，例如通过学习一个域不变表示。然而，常见的机器学习损失函数对模型在不同领域上的一致表现没有强有力的保障，特别是模型在某个领域上的表现是否是以损害其他领域表现为代价的。本文建立了这个问题的新的理论基础，通过提出经典监督机器学习损失和联合空间（表示空间和输出空间）中的Wasserstein距离之间的一组数学关系。我们证明分类或回归损失与GAN类型的领域判别器结合时形成了真实Wasserstein距离的上界。这意味着更不变的表示形式。

    Domain shifts in the training data are common in practical applications of machine learning; they occur for instance when the data is coming from different sources. Ideally, a ML model should work well independently of these shifts, for example, by learning a domain-invariant representation. However, common ML losses do not give strong guarantees on how consistently the ML model performs for different domains, in particular, whether the model performs well on a domain at the expense of its performance on another domain. In this paper, we build new theoretical foundations for this problem, by contributing a set of mathematical relations between classical losses for supervised ML and the Wasserstein distance in joint space (i.e. representation and output space). We show that classification or regression losses, when combined with a GAN-type discriminator between domains, form an upper-bound to the true Wasserstein distance between domains. This implies a more invariant representation and
    
[^177]: 判别贝叶斯滤波为随机牛顿法在最小化对数凸函数中提供动力

    Discriminative Bayesian filtering lends momentum to the stochastic Newton method for minimizing log-convex functions. (arXiv:2104.12949v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2104.12949](http://arxiv.org/abs/2104.12949)

    该论文提出了一种判别贝叶斯滤波的方法，为随机牛顿法在最小化对数凸函数中提供了动力。通过考虑整个历史信息形成更新，该方法能够在迭代开始时减弱旧观测的影响。

    

    为了最小化一组对数凸函数的平均值，随机牛顿法通过对完整目标函数的梯度和海森矩阵进行子采样版本的迭代更新其估计值。我们将这个优化问题置于一种具有判别性观测过程的潜在状态空间模型的顺序贝叶斯推断背景中。应用贝叶斯滤波可以得到一种新的优化算法，在形成更新时考虑了梯度和海森矩阵的整个历史。我们建立了基于矩阵的条件，在这些条件下，旧观测的影响随时间减弱，类似于Polyak的重球动力。通过一个示例展示了我们方法的各个方面，并回顾了随机牛顿法的其他相关创新。

    To minimize the average of a set of log-convex functions, the stochastic Newton method iteratively updates its estimate using subsampled versions of the full objective's gradient and Hessian. We contextualize this optimization problem as sequential Bayesian inference on a latent state-space model with a discriminatively-specified observation process. Applying Bayesian filtering then yields a novel optimization algorithm that considers the entire history of gradients and Hessians when forming an update. We establish matrix-based conditions under which the effect of older observations diminishes over time, in a manner analogous to Polyak's heavy ball momentum. We illustrate various aspects of our approach with an example and review other relevant innovations for the stochastic Newton method.
    
[^178]: 走出Weisfeiler Leman层次结构：超越消息传递的图学习

    Walking Out of the Weisfeiler Leman Hierarchy: Graph Learning Beyond Message Passing. (arXiv:2102.08786v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2102.08786](http://arxiv.org/abs/2102.08786)

    我们提出了一种名为CRaWl的图学习神经网络架构，它通过利用随机游走过程中出现的子图来提取和聚合信息，从而检测长程相互作用并计算非局部特征。我们证明了CRaWl的表达能力与Weisfeiler Leman算法和图神经网络不可比较，实验证明它在多个基准数据集上与最先进的GNN架构相匹配。

    

    我们提出了一种新颖的用于图学习的神经网络架构CRaWl。与图神经网络类似，CRaWl层在图上更新节点特征，因此可以自由地与GNN层进行组合或交错使用。然而，CRaWl与消息传递图神经网络在本质上有着不同的操作方式。CRaWl层利用一维卷积从通过图中的随机游走出现的子图中提取和聚合信息，从而检测长程相互作用并计算非局部特征。作为我们方法的理论基础，我们证明了一个定理，即CRaWl的表达能力与Weisfeiler Leman算法以及图神经网络是无法比较的。也就是说，CRaWl能够表达的函数，图神经网络无法表达，反之亦然。此结果可以扩展到Weisfeiler Leman层次的更高级别，从而也适用于高阶GNN。实验证明，CRaWl在多个基准数据集上与最先进的GNN架构相匹配。

    We propose CRaWl, a novel neural network architecture for graph learning. Like graph neural networks, CRaWl layers update node features on a graph and thus can freely be combined or interleaved with GNN layers. Yet CRaWl operates fundamentally different from message passing graph neural networks. CRaWl layers extract and aggregate information on subgraphs appearing along random walks through a graph using 1D Convolutions. Thereby it detects long range interactions and computes non-local features. As the theoretical basis for our approach, we prove a theorem stating that the expressiveness of CRaWl is incomparable with that of the Weisfeiler Leman algorithm and hence with graph neural networks. That is, there are functions expressible by CRaWl, but not by GNNs and vice versa. This result extends to higher levels of the Weisfeiler Leman hierarchy and thus to higher-order GNNs. Empirically, we show that CRaWl matches state-of-the-art GNN architectures across a multitude of benchmark datas
    
[^179]: 高频交易中的深度强化学习

    Deep Reinforcement Learning for Active High Frequency Trading. (arXiv:2101.07107v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2101.07107](http://arxiv.org/abs/2101.07107)

    本文介绍了一个基于深度强化学习的框架，用于在股票市场中进行活跃的高频交易。通过训练DRL代理来交易股票，并使用Proximal Policy Optimization算法进行优化。通过仅选择具有最大价格变动的训练样本来提高训练数据的信噪比。通过实验证明，代理能够创建对底层环境的动态表示，并能够识别偶尔出现的规律。

    

    我们介绍了第一个基于端到端深度强化学习（DRL）的框架，用于在股票市场中进行活跃的高频交易。我们训练DRL代理使用Proximal Policy Optimization算法来交易一单位的英特尔公司股票。训练是在连续三个月的高频限价委托簿数据上进行的，其中最后一个月是验证数据。为了最大化训练数据中的信噪比，我们通过仅选择具有最大价格变动的训练样本来组成后者。然后在接下来的一个月的数据上进行测试。使用顺序模型优化技术进行超参数调优。我们考虑了三种不同的状态特征化方式，它们在基于LOB的元特征上有所不同。通过分析代理在测试数据上的表现，我们认为代理能够创建对底层环境的动态表示。它们能够识别偶尔出现的规律。

    We introduce the first end-to-end Deep Reinforcement Learning (DRL) based framework for active high frequency trading in the stock market. We train DRL agents to trade one unit of Intel Corporation stock by employing the Proximal Policy Optimization algorithm. The training is performed on three contiguous months of high frequency Limit Order Book data, of which the last month constitutes the validation data. In order to maximise the signal to noise ratio in the training data, we compose the latter by only selecting training samples with largest price changes. The test is then carried out on the following month of data. Hyperparameters are tuned using the Sequential Model Based Optimization technique. We consider three different state characterizations, which differ in their LOB-based meta-features. Analysing the agents' performances on test data, we argue that the agents are able to create a dynamic representation of the underlying environment. They identify occasional regularities pre
    
[^180]: 一种整合和分类正态分布的方法

    A method to integrate and classify normal distributions. (arXiv:2012.14331v8 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2012.14331](http://arxiv.org/abs/2012.14331)

    本文介绍了一种可以对任意参数维度下的任意域内正态分布进行积分的方法，提供了法向向量函数的相关概率密度和统计指标，同时还提供了可以对任意数量正态分布进行分类的方法和维度降低和可视化的技术。

    

    单变量和多变量正态概率分布在模拟不确定性决策中被广泛使用。计算这些模型的性能需要在特定区域内对这些分布进行积分，这在不同的模型中可以有很大的差异。除了一些特殊情况，目前不存在通用的分析表达式、标准数值方法或软件来计算这些积分。本文提供了数学结果和开源软件，可以提供以下内容：（i）任意参数维度下任意域内法向的概率，（ii）法向向量函数的概率密度、累积分布和逆累积分布，（iii）任意数量正态分布之间的分类误差、贝叶斯最优辨别指数以及其与工作特征曲线的关系，（iv）此类问题的维度降低和可视化，以及（v）对于给定数据这些方法的可靠性测试。我们通过几个具体的例子，包括金融、生物和心理学来演示这些功能。

    Univariate and multivariate normal probability distributions are widely used when modeling decisions under uncertainty. Computing the performance of such models requires integrating these distributions over specific domains, which can vary widely across models. Besides some special cases, there exist no general analytical expressions, standard numerical methods or software for these integrals. Here we present mathematical results and open-source software that provide (i) the probability in any domain of a normal in any dimensions with any parameters, (ii) the probability density, cumulative distribution, and inverse cumulative distribution of any function of a normal vector, (iii) the classification errors among any number of normal distributions, the Bayes-optimal discriminability index and relation to the operating characteristic, (iv) dimension reduction and visualizations for such problems, and (v) tests for how reliably these methods may be used on given data. We demonstrate these
    
[^181]: 一票否决权：用于低样本青光眼诊断的半监督学习

    One-Vote Veto: Semi-Supervised Learning for Low-Shot Glaucoma Diagnosis. (arXiv:2012.04841v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2012.04841](http://arxiv.org/abs/2012.04841)

    本文扩展了孪生网络，提出了一种可在有限标记数据和不平衡情况下进行低样本学习的训练方法，并引入了半监督学习策略，利用额外的未标记数据来提高准确性。

    

    卷积神经网络（CNN）是一种有前景的技术，用于自动诊断眼底图像中的青光眼，这些图像通常在眼科检查中获取。然而，CNN通常需要大量标记良好的数据进行训练，而在许多生物医学图像分类应用中可能无法获得足够的数据，特别是在罕见疾病或专家标记成本高昂的情况下。本文有两个贡献来解决这个问题：（1）它扩展了传统的孪生网络，引入一种在标记数据有限且不平衡的情况下进行小样本学习的训练方法；（2）它引入了一种新颖的半监督学习策略，利用额外的未标记训练数据来提高准确性。我们提出的多任务孪生网络（MTSN）可以使用任何骨干CNN，并且我们通过四个骨干CNN的实验证明，其在有限的训练数据上的准确性接近经过训练的骨干CNN的准确性。

    Convolutional neural networks (CNNs) are a promising technique for automated glaucoma diagnosis from images of the fundus, and these images are routinely acquired as part of an ophthalmic exam. Nevertheless, CNNs typically require a large amount of well-labeled data for training, which may not be available in many biomedical image classification applications, especially when diseases are rare and where labeling by experts is costly. This article makes two contributions to address this issue: (1) It extends the conventional Siamese network and introduces a training method for low-shot learning when labeled data are limited and imbalanced, and (2) it introduces a novel semi-supervised learning strategy that uses additional unlabeled training data to achieve greater accuracy. Our proposed multi-task Siamese network (MTSN) can employ any backbone CNN, and we demonstrate with four backbone CNNs that its accuracy with limited training data approaches the accuracy of backbone CNNs trained wit
    
[^182]: 深度矩阵分解的梯度下降：动力学和对低秩的隐含偏好

    Gradient Descent for Deep Matrix Factorization: Dynamics and Implicit Bias towards Low Rank. (arXiv:2011.13772v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2011.13772](http://arxiv.org/abs/2011.13772)

    通过分析梯度下降在线性网络和估计问题中的动力学，揭示了深度学习中的隐含偏好现象。

    

    在深度学习中，常常使用比训练数据点更多的网络参数。在这种超参数化的情况下，通常有多个网络可以实现零训练误差，因此训练算法对计算解决方案具有隐含偏好。在实践中，（随机）梯度下降往往更喜欢具有良好泛化能力的解决方案，这可以解释深度学习的成功。在本文中，我们分析了梯度下降在线性网络和估计问题的简化环境中的动力学。虽然我们不在超参数化的情况下，但我们的分析仍然揭示了隐含偏好的现象。事实上，我们对香草梯度下降的动力学收敛进行了严格分析，并表征了频谱的动态收敛。我们能够准确地确定迭代的有效秩接近于低秩投影的有效秩的时间间隔。

    In deep learning, it is common to use more network parameters than training points. In such scenarioof over-parameterization, there are usually multiple networks that achieve zero training error so that thetraining algorithm induces an implicit bias on the computed solution. In practice, (stochastic) gradientdescent tends to prefer solutions which generalize well, which provides a possible explanation of thesuccess of deep learning. In this paper we analyze the dynamics of gradient descent in the simplifiedsetting of linear networks and of an estimation problem. Although we are not in an overparameterizedscenario, our analysis nevertheless provides insights into the phenomenon of implicit bias. In fact, wederive a rigorous analysis of the dynamics of vanilla gradient descent, and characterize the dynamicalconvergence of the spectrum. We are able to accurately locate time intervals where the effective rankof the iterates is close to the effective rank of a low-rank projection of the gro
    
[^183]: 在多秘书问题和连续估值的在线线性规划问题中的对数遗憾

    Logarithmic Regret in Multisecretary and Online Linear Programming Problems with Continuous Valuations. (arXiv:1912.08917v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1912.08917](http://arxiv.org/abs/1912.08917)

    本文研究了一个收益管理问题，其中顾客的到达是连续的，有限均值连续分布的效用值和有界离散或连续分布的库存量。研究发现，如果初始库存量与顾客数成线性比例，随着顾客数的增加，预期的遗憾将以对数的速度增长。

    

    我研究了一个一般的收益管理问题，其中$n$个顾客在$n$个时期内顺序到达，您必须动态决定要满足哪个顾客。满足第$t$个时期的顾客可以获得效用$u_{t}\in \mathbb{R}_{+}$，并减少您的库存量$A_{t}\in \mathbb{R}_{+}^{M}$。顾客向量$(u_{t}, A_{t}')'$是独立同分布的，其中$u_{t}$是从有限均值连续分布中抽取的，$A_{t}$是从有界离散或连续分布中抽取的。我研究了该系统的遗憾，即如果您不需要即时决策，您可以获得的额外效用。我展示了如果您的初始库存资产与$n$成线性比例，那么当$n \rightarrow \infty$时，您的预期遗憾是$ \Theta(\log(n)) $。我提供了一种简单的策略，实现了$ \Theta(\log(n)) $的遗憾率。最后，我将这个结果扩展到Arlotto和Gurich（2019）的多秘书问题，其中秘书估值是均匀分布的。

    I study a general revenue management problem in which $ n $ customers arrive sequentially over $ n $ periods, and you must dynamically decide which to satisfy. Satisfying the period-$ t $ customer yields utility $ u_{t} \in \mathbb{R}_{+} $ and decreases your inventory holdings by $ A_{t} \in \mathbb{R}_{+}^{M} $. The customer vectors, $ (u_{t}, A_{t}')' $, are i.i.d., with $ u_{t} $ drawn from a finite-mean continuous distribution and $ A_{t} $ drawn from a bounded discrete or continuous distribution. I study this system's regret, which is the additional utility you could get if you didn't have to make decisions on the fly. I show that if your initial inventory endowment scales linearly with $ n $ then your expected regret is $ \Theta(\log(n)) $ as $ n \rightarrow \infty $. I provide a simple policy that achieves this $ \Theta(\log(n)) $ regret rate. Finally, I extend this result to Arlotto and Gurich's (2019) multisecretary problem with uniformly distributed secretary valuations.
    
[^184]: 深度学习的泛化问题

    Generalization in Deep Learning. (arXiv:1710.05468v8 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1710.05468](http://arxiv.org/abs/1710.05468)

    本文从理论上解释了为什么以及如何深度学习能够在容量大、复杂性高、可能存在算法不稳定性、非鲁棒性和尖锐极小值的情况下实现良好的泛化，提出了一些新的开放问题，并讨论了研究结果的局限性。

    

    本文从理论上解释了为什么以及如何深度学习能够在容量大、复杂性高、可能存在算法不稳定性、非鲁棒性和尖锐极小值的情况下实现良好的泛化，回应了文献中的一个开放问题。我们还讨论了提供深度学习非虚空泛化保证的方法。基于理论观察，我们提出了一些新的开放问题，并讨论了我们研究结果的局限性。

    This paper provides theoretical insights into why and how deep learning can generalize well, despite its large capacity, complexity, possible algorithmic instability, nonrobustness, and sharp minima, responding to an open question in the literature. We also discuss approaches to provide non-vacuous generalization guarantees for deep learning. Based on theoretical observations, we propose new open problems and discuss the limitations of our results.
    

