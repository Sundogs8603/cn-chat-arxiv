# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Just One Byte (per gradient): A Note on Low-Bandwidth Decentralized Language Model Finetuning Using Shared Randomness.](http://arxiv.org/abs/2306.10015) | 本文介绍了一种通过使用共享随机性来进行低带宽分散式微调的方法，该方法可以通过每台机器生成不同的随机扰动来更新每个模型，从而具有高度通信效率，并且具有隐私保护的优势。 |
| [^2] | [CLIP2Protect: Protecting Facial Privacy using Text-Guided Makeup via Adversarial Latent Search.](http://arxiv.org/abs/2306.10008) | 该论文提出了一种两步法面部隐私保护方法CLIP2Protect，使用对抗性潜在搜索结合文本引导化妆，生成高质量面部图像，从而保护面部隐私。 |
| [^3] | [Robot Learning with Sensorimotor Pre-training.](http://arxiv.org/abs/2306.10007) | 本论文介绍了一种针对机器人学习的自监督感觉运动预训练方法，使用 Transformer 模型在视觉表示上进行操作，通过 20,000 条真实世界轨迹数据集的预训练可以使机器人在堆积方块任务中性能提高 2 倍，并使其能够更快地学习新任务。 |
| [^4] | [Unsupervised Learning of Style-Aware Facial Animation from Real Acting Performances.](http://arxiv.org/abs/2306.10006) | 本文提出了一种非监督学习的动画方法，可以通过文本或语音输入，实现基于真实动作表演的面部动画，并且可以不同程度地学习并合成不同的表演风格。 |
| [^5] | [SLACK: Stable Learning of Augmentations with Cold-start and KL regularization.](http://arxiv.org/abs/2306.09998) | 该论文提出了一种在不依赖先前知识的情况下直接学习数据增强策略的方法。 |
| [^6] | [Fairness in Preference-based Reinforcement Learning.](http://arxiv.org/abs/2306.09995) | 该论文提出了一种名为FPbRL的新的公平偏好强化学习方法，旨在通过广义Gini福利函数最大化策略学习来实现多目标优化并处理每个目标的公平性。 |
| [^7] | [Ensemble Framework for Cardiovascular Disease Prediction.](http://arxiv.org/abs/2306.09989) | 本文介绍了一个使用机器学习方法预测心血管疾病的集成框架，采用了IEEE Data Port 数据集，旨在早期准确诊断心脏问题，提高生命几率。 |
| [^8] | [Transforming Observations of Ocean Temperature with a Deep Convolutional Residual Regressive Neural Network.](http://arxiv.org/abs/2306.09987) | 本文利用深度卷积残差回归神经网络来将海洋温度观测转换，提高了分辨率和覆盖云间隙，有效评估了模型性能。 |
| [^9] | [Evaluating Superhuman Models with Consistency Checks.](http://arxiv.org/abs/2306.09983) | 本文提出了一个用一致性检查评估超人模型的框架，可以发现决策制定中的逻辑不一致性，即使对于超人模型的决策正确性可能是不可能评估的情况。 |
| [^10] | [Creating Multi-Level Skill Hierarchies in Reinforcement Learning.](http://arxiv.org/abs/2306.09980) | 本文提出了一种基于代理人与环境交互的图形结构的答案，使用分层图划分产生具有多个抽象层次的技能层次结构。技能能将代理人移动到状态空间中互相连接紧密但相互连接较弱的区域，有效提高了强化学习的效率。 |
| [^11] | [Adversarially robust clustering with optimality guarantees.](http://arxiv.org/abs/2306.09977) | 本文提出了一种简单的算法，即使在存在对抗性的异常值的情况下，也能获得最优的错标率。在没有异常值的情况下，该算法能够实现与洛伊德算法类似的理论保证. |
| [^12] | [Enhancing Fault Resilience of QNNs by Selective Neuron Splitting.](http://arxiv.org/abs/2306.09973) | 本文提出一种选择性神经元分裂方法，增强QNN的容错性，可以在加速器中设计轻量级纠错单元，相对于选择性三重模块冗余具有更小的开销，同时实现了类似的故障容错水平。 |
| [^13] | [HePCo: Data-Free Heterogeneous Prompt Consolidation for Continual Federated Learning.](http://arxiv.org/abs/2306.09970) | 本文提出了一种名为HePCo的轻量级提示合并算法，解决了在连续联邦学习中的数据异构和遗忘问题，并在不共享或存储任何数据的情况下最小化了通信开销。在真实数据集和合成数据集上实现了最先进的结果，并且保持了数据隐私。 |
| [^14] | [The Evolution theory of Learning: From Natural Selection to Reinforcement Learning.](http://arxiv.org/abs/2306.09961) | 本文探讨了强化学习和进化这两个领域之间的联系以及可能的启示，揭示了强化学习原则能够提高我们对进化和进化系统中反馈作用的理解。 |
| [^15] | [Training shallow ReLU networks on noisy data using hinge loss: when do we overfit and is it benign?.](http://arxiv.org/abs/2306.09955) | 本论文研究了二层ReLU网络在使用梯度下降和合页损失处理噪声数据进行二分类中的良性过拟合，通过对干净数据余量的条件的确定，得出了三种不同的训练结果，能够在训练过程中对神经元动态变化做出精细描述，并发现了两个不同的训练阶段。 |
| [^16] | [You Don't Need Robust Machine Learning to Manage Adversarial Attack Risks.](http://arxiv.org/abs/2306.09951) | 本文对现代机器学习中的鲁棒性问题和对抗攻击进行了调查，发现不必要采用高成本的强鲁棒机器学习，可以通过其他设计选择来缓解对抗攻击的风险。 |
| [^17] | [Nearly-Optimal Hierarchical Clustering for Well-Clustered Graphs.](http://arxiv.org/abs/2306.09950) | 本文提出了两种高效分层聚类算法，针对任何具有清晰聚类结构的输入图，其能在近线性时间内生成一个关于Dasgupta代价函数的O（1）近似的HC树，性能优于先前技术。 |
| [^18] | [Vehicle Occurrence-based Parking Space Detection.](http://arxiv.org/abs/2306.09940) | 本文提出了一种利用车辆出现信息生成热力图的自动检测停车位方法，并在PKLot和CNRPark-EXT数据集上取得了较高的准确率。 |
| [^19] | [Towards Better Orthogonality Regularization with Disentangled Norm in Training Deep CNNs.](http://arxiv.org/abs/2306.09939) | 本文提出一种新的通过解离规范优化实现更好的滤波器正交性的方法，在实现滤波器之间的严格正交性原则下，所采用的模型比以前的规范化方法在近正交性方面表现更好。 |
| [^20] | [Drag-guided diffusion models for vehicle image generation.](http://arxiv.org/abs/2306.09935) | 本文提出了一种基于物理的引导方法，在稳定扩散的基础上添加拖引导，生成能够同时最小化车辆预测阻力系数的图像。 |
| [^21] | [A Metaheuristic-based Machine Learning Approach for Energy Prediction in Mobile App Development.](http://arxiv.org/abs/2306.09931) | 本文提出了一种基于元启发式机器学习的直方图梯度提升分类机器（HGBC），用于移动应用中能耗的预测，并解决了特征选择和超参数调优两个问题。 |
| [^22] | [Friend or Foe? Exploring the Implications of Large Language Models on the Science System.](http://arxiv.org/abs/2306.09928) | LLMs有潜力在行政、创造性和分析任务方面对科学做出变革，但需要通过积极的监管和科学教育来解决与偏见、错误信息和质量保证有关的风险。 |
| [^23] | [Trained Transformers Learn Linear Models In-Context.](http://arxiv.org/abs/2306.09927) | 本文研究了Transformer在具有单层线性自注意层的线性回归任务上通过梯度流进行训练的ICL机制，揭示了梯度流具有找到目标函数全局最小值的能力。 |
| [^24] | [Learning to Summarize and Answer Questions about a Virtual Robot's Past Actions.](http://arxiv.org/abs/2306.09922) | 本论文介绍了一种学习总结和回答机器人动作历史的方法，能够通过一种语言模型同时完成总结和回答任务，并提供了自动生成问题和答案的方法来进行训练。此方法能够实现从问题回答中学习对象表示的零-shot转移。 |
| [^25] | [Feeding control and water quality monitoring in aquaculture systems: Opportunities and challenges.](http://arxiv.org/abs/2306.09920) | 本文讨论了控制饲料和监测水质对于水产养殖系统中平衡鱼类生产力和健康至关重要的问题，并强调了开发可靠的控制策略的必要性。 |
| [^26] | [Towards Quantum Federated Learning.](http://arxiv.org/abs/2306.09912) | 量子联邦学习通过将量子计算和联邦学习原理相结合，旨在利用量子技术增强学习过程中的隐私、安全和效率，并通过独特的分类法分类总结了这一快速发展领域的技术特点和未来研究方向。 |
| [^27] | [LabelBench: A Comprehensive Framework for Benchmarking Label-Efficient Learning.](http://arxiv.org/abs/2306.09910) | 本论文介绍了一个新的综合性标签高效学习基准评估框架LabelBench，并通过引入一种新的与半监督学习相结合的主动学习方法的基准测试，证明了在相对较少的标记示例下实现更好的标签效率。 |
| [^28] | [Correlation Clustering of Bird Sounds.](http://arxiv.org/abs/2306.09906) | 本文研究鸟声聚类问题，提出了一种通过学习训练集中记录对相关概率后，应用相关聚类于测试集进行聚类的方法，并比较了这种方法与测试集分类的准确性和相关性。同时，还探究了这种方法在应用于训练期间未听到的鸟类物种的记录以及分离鸟声和环境噪声方面的效果。 |
| [^29] | [Studying Generalization on Memory-Based Methods in Continual Learning.](http://arxiv.org/abs/2306.09890) | 本文研究了连续学习中基于记忆的方法的泛化性能，发现虽然这些方法可以帮助传统的内分布泛化，但它们可以通过学习虚假的特征和相关性来大大损害超出分布泛化，尤其是在线性分类器中。 |
| [^30] | [CANDID: Correspondence AligNment for Deep-burst Image Denoising.](http://arxiv.org/abs/2306.09887) | 本文提出了 CANDID，是一种针对深度爆发图像去噪的对齐算法，其中利用了光流的对应估计模块来对齐输入的所有图像，并通过建立的对应关系在预处理和原始图像中预测像素级可变滤波器核，从而实现最先进的结果。 |
| [^31] | [Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments in JAX.](http://arxiv.org/abs/2306.09884) | Jumanji是JAX中一套可扩展的强化学习系统，提供了一系列高度可定制的环境，具有快速、灵活、可扩展和模块化特点，利用硬件加速器赋能更有能力的代理人。 |
| [^32] | [Uncertainty Quantification via Spatial-Temporal Tweedie Model for Zero-inflated and Long-tail Travel Demand Prediction.](http://arxiv.org/abs/2306.09882) | 本文提出了一种新型的时空Tweedie模型STTD，旨在解决高分辨率OD矩阵中稀疏和长尾特征的问题，并成功量化预测不确定性，具有很高的应用前景。 |
| [^33] | [Generalizable One-shot Rope Manipulation with Parameter-Aware Policy.](http://arxiv.org/abs/2306.09872) | GenORM通过增加可变形绳索参数和使用各种可变形绳索的模拟训练操作策略，实现利用一次真实演示处理不同可形变绳索，从而节省演示时间和提高适用性。 |
| [^34] | [Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models.](http://arxiv.org/abs/2306.09869) | 本论文提出了能量交叉注意力的EBM框架，通过更新和转移上下文向量，隐式最小化能量函数的嵌套层次，优化文本到图像扩散模型的语义对齐问题，实现零样本组合生成。 |
| [^35] | [Transferability of Winning Lottery Tickets in Neural Network Differential Equation Solvers.](http://arxiv.org/abs/2306.09863) | 本文研究了神经网络微分方程求解器中中奖彩票的可转移性，发现哈密顿神经网络系统之间具有可转移性，并用重整化群理论分析了这两个系统的普遍性。 |
| [^36] | [DoubleAdapt: A Meta-learning Approach to Incremental Learning for Stock Trend Forecasting.](http://arxiv.org/abs/2306.09862) | DoubleAdapt是一个增量学习的方法，用于股票趋势预测。它利用元学习技术自动学习如何将股票数据适应到本地平稳分布空间中，从而有效地适应数据和模型，减轻分布漂移的影响。 |
| [^37] | [Practical Sharpness-Aware Minimization Cannot Converge All the Way to Optima.](http://arxiv.org/abs/2306.09850) | 该研究揭示了实用的锐度感知优化算法在某些情况下不能够全程向最优点收敛。 |
| [^38] | [Wasserstein distributional robustness of neural networks.](http://arxiv.org/abs/2306.09844) | 本研究探讨了基于Wasserstein分布鲁棒性的神经网络对抗攻击，提出了一组分布威胁模型，并使用一阶对抗攻击算法及其多步版本对其进行了评估。 |
| [^39] | [Subset Selection Based On Multiple Rankings in the Presence of Bias: Effectiveness of Fairness Constraints for Multiwinner Voting Score Functions.](http://arxiv.org/abs/2306.09835) | 本文研究了存在偏差时的子集选择问题，提出了在兼顾公平的前提下，使所选的子集满足群体公平性约束来提高选择质量，不同的多赢家评分函数对于公平性的依赖性可能不同。 |
| [^40] | [Building Blocks for a Complex-Valued Transformer Architecture.](http://arxiv.org/abs/2306.09827) | 该论文提出了复数变压器架构的构建块，以便将深度学习应用到处理复数信号或图像上，在不使用投影到 $ \mathbb {R} ^2 $ 的情况下实现。该方法通过提供多个版本的复数缩放点积注意机制和复数层归一化，测试结果显示其在MusicNet数据集上的分类和序列生成任务上表现优秀，具有更好的鲁棒性和相当的性能。 |
| [^41] | [FALL-E: A Foley Sound Synthesis Model and Strategies.](http://arxiv.org/abs/2306.09807) | 本文介绍了一种名为“FALL-E”的佛利音效合成系统及其策略。该模型通过条件训练使其能够根据文本输入了解声音质量和录音环境，并在DCASE 2023挑战赛中表现良好，尤其在多样性上得分最高。 |
| [^42] | [Sample-Efficient On-Policy Imitation Learning from Observations.](http://arxiv.org/abs/2306.09805) | 提出了一种称为SEILO的算法，该算法结合了标准的对抗模仿学习和逆动力学建模，实现了从无专家数据的观测中的样本高效策略模仿学习，成功地减少了与环境的交互并实现了专家水平的表现。 |
| [^43] | [Framework and Benchmarks for Combinatorial and Mixed-variable Bayesian Optimization.](http://arxiv.org/abs/2306.09803) | 本文介绍了一个模块化框架和基准，用于组合和混合变量贝叶斯优化，并提供多样的合成和真实世界基准测试。通过此框架，作者展示了4种常见的MCBO技术。 |
| [^44] | [$\pi2\text{vec}$: Policy Representations with Successor Features.](http://arxiv.org/abs/2306.09800) | 本文提出了$\pi2\text{vec}$方法，它可以将黑盒策略行为表示为特征向量，并可以用于离线策略选择。该方法为现代研究方向中的离线策略评估、基础模型状态表示和资源受限制的策略选择提供了重要的支持。 |
| [^45] | [GPINN: Physics-informed Neural Network with Graph Embedding.](http://arxiv.org/abs/2306.09792) | GPINN是一种物理信息神经网络框架，能够在图形结构中执行PINN，将拓扑数据融入神经网络的计算中进行问题求解，利用图嵌入技术注入额外的维度，以封装图的空间特征并保留原始空间的属性。GPINN比传统的PINN性能更好，特别是在捕捉解的物理特征方面表现更加优异。 |
| [^46] | [The Information Bottleneck's Ordinary Differential Equation: First-Order Root-Tracking for the IB.](http://arxiv.org/abs/2306.09790) | 本文重新导出了信息瓶颈的一阶ODR，描述了其最优权衡曲线的动态，揭示了最优编码的动态过程。 |
| [^47] | [Dynamic Decision Tree Ensembles for Energy-Efficient Inference on IoT Edge Nodes.](http://arxiv.org/abs/2306.09789) | 本文提出了一种在IoT边缘节点上非常适用的动态决策树集成算法，该算法可以自动调整执行树的数量，以权衡计算成本和精度，从而提高能源效率。 |
| [^48] | [Understanding Deep Generative Models with Generalized Empirical Likelihoods.](http://arxiv.org/abs/2306.09780) | 本文展示了广义经验似然（GEL）方法提供了一系列诊断工具来识别深度生成模型的许多缺陷，并结合最大均值差异和广义经验似然的技术，创造了保留每个样本可解释性的分布测试，还包括标签信息的指标。这些测试可以预测模式降低的程度。 |
| [^49] | [Gradient is All You Need?.](http://arxiv.org/abs/2306.09778) | 本文提供了一种新的角度分析了基于梯度的学习算法，将一种新的多粒子无导数优化方法解释为梯度下降的随机松弛方法。此优化方法证明了零阶方法并不一定低效或不具备泛化能力，并且可以在丰富类别的非光滑和非凸目标函数下全局收敛于全局最小值。 |
| [^50] | [Using Machine Learning Methods for Automation of Size Grid Building and Management.](http://arxiv.org/abs/2306.09775) | 本研究使用机器学习方法解决时装公司尺码选择的问题，创造出更为自动化的流程，降低团队工作量，具有实际应用价值。 |
| [^51] | [Politeness Stereotypes and Attack Vectors: Gender Stereotypes in Japanese and Korean Language Models.](http://arxiv.org/abs/2306.09752) | 中文总结该论文主要研究了日语和韩语语言模型中与礼貌水平相关的语法性别偏见，发现礼貌水平是网络欺凌检测模型中的攻击向量。 |
| [^52] | [Fedstellar: A Platform for Decentralized Federated Learning.](http://arxiv.org/abs/2306.09750) | Fedstellar是一个联邦学习平台，支持物理或虚拟设备的去中心化、半去中心化和中心化的方式训练模型，旨在解决现有平台在处理异构联盟网络拓扑等问题时的挑战。 |
| [^53] | [Temporal Difference Learning with Experience Replay.](http://arxiv.org/abs/2306.09746) | 本文提出具有经验回放的TD学习，在马尔科夫观测模型下，通过对噪声项的分解，提供了有限时间误差界限，可以通过调整回放缓冲区和小批量的大小来控制误差。 |
| [^54] | [Automatic Trade-off Adaptation in Offline RL.](http://arxiv.org/abs/2306.09744) | 研究提出了一种名为AutoLION的离线强化学习算法，通过自动权衡自适应来减少专家时间成本。 |
| [^55] | [Stabilized Neural Differential Equations for Learning Constrained Dynamics.](http://arxiv.org/abs/2306.09739) | 本文提出了一种稳定神经微分方程（SNDEs）的方法，可以强制使用任意流形约束。该方法通过添加稳定项使约束流形成为渐进稳定的，并且在实验中表现优于现有方法。 |
| [^56] | [Semi-Offline Reinforcement Learning for Optimized Text Generation.](http://arxiv.org/abs/2306.09712) | 该研究提出了一种半离线强化学习范式，该范式平衡了探索能力和培训成本，提供了一个理论基础来比较不同的强化学习设置，并在优化成本、渐近误差和过度拟合误差界方面实现了最优的RL设置。实验结果表明，该方法高效且性能优异。 |
| [^57] | [Representation and decomposition of functions in DAG-DNNs and structural network pruning.](http://arxiv.org/abs/2306.09707) | 本研究提出了一种新的DNN表示方法DAG-DNN，利用下三角矩阵分解的方法对其进行结构网络剪枝，同时证明DAG-DNN可以推导出DNN各个子架构上定义的所有函数。 |
| [^58] | [Reducing Computational Costs in Sentiment Analysis: Tensorized Recurrent Networks vs. Recurrent Networks.](http://arxiv.org/abs/2306.09705) | 张量循环网络是一种降低情感分析计算成本的潜在解决方案 |
| [^59] | [A Hierarchical Bayesian Model for Deep Few-Shot Meta Learning.](http://arxiv.org/abs/2306.09702) | 我们提出了适用于少样本元学习问题的层级贝叶斯模型，并通过引入全局变量，以帮助记忆历史情节的重要信息，同时控制模型对新情节适应的程度。我们提出了一种在线变分贝叶斯方法，通过实验证明该模型在少样本分类任务上优于现有方法。 |
| [^60] | [Linear convergence of Nesterov-1983 with the strong convexity.](http://arxiv.org/abs/2306.09694) | 本文使用高分辨率微分方程框架回答了Nesterov-1983和FISTA是否在强凸函数上线性收敛的问题，并指出线性收敛性不依赖于强凸性条件。 |
| [^61] | [Magnetic Resonance Spectroscopy Quantification Aided by Deep Estimations of Imperfection Factors and Overall Macromolecular Signal.](http://arxiv.org/abs/2306.09681) | 本论文通过深度学习方法提高了磁共振波谱技术中代谢物信号定量精度和稳定性。 |
| [^62] | [Multi-View Class Incremental Learning.](http://arxiv.org/abs/2306.09675) | 本文提出了一种名为多视角分类增量学习（MVCIL）的新模型，该模型使用随机化的表示学习技术进行特征提取，并提出正交融合子空间和选择性权重合并来解决增量学习中遗忘旧信息和学习新概念的挑战。实验结果表明该方法相比最新方法有效性更高。 |
| [^63] | [Multi-Classification using One-versus-One Deep Learning Strategy with Joint Probability Estimates.](http://arxiv.org/abs/2306.09668) | 本文提出了一种基于联合概率估计的一对一深度学习多分类模型，该模型通过特定的距离度量来校准二分类器的概率输出，并通过联合概率的距离最小化来获得对主体的类别概率估计。实验结果表明，该模型在多个应用中都具有更高的分类精度。 |
| [^64] | [A Smooth Binary Mechanism for Efficient Private Continual Observation.](http://arxiv.org/abs/2306.09666) | 本文提出了一种可微且凸的替代二进制机制的方法，用于解决隐私问题中发布私有前缀求和题的效率问题。该方法比标志性的二进制机制具有更低的方差和更好的观测时间保证，并且在计算和空间要求方面更加有效。 |
| [^65] | [Cooperative Multi-Objective Reinforcement Learning for Traffic Signal Control and Carbon Emission Reduction.](http://arxiv.org/abs/2306.09662) | 本文提出了一种合作的多目标架构，称为MOMA-DDPG，用于交通信号控制和碳减排问题。该方法涉及两种类型的智能体：一个专注于优化每个路口的本地交通，而另一个旨在优化全局交通吞吐量。结果显示，该方法优于现有最先进的方法，并解决了等待时间和碳排放量两个问题。 |
| [^66] | [Temporal Causal Mediation through a Point Process: Direct and Indirect Effects of Healthcare Interventions.](http://arxiv.org/abs/2306.09656) | 该论文介绍了一个新的非参数的中介变量-结果模型，用于医疗干预的因果分析，可以区分出干预的直接和间接效应，并考虑长程中介变量-结果交互作用。 |
| [^67] | [Learning CO$_2$ plume migration in faulted reservoirs with Graph Neural Networks.](http://arxiv.org/abs/2306.09648) | 提出一种基于图神经网络的模型，准确预测了非结构化合成储层中CO$_2$气云的时间演化，相比标准模型具有更好的准确性和减少的时间误差积累。 |
| [^68] | [Vacant Holes for Unsupervised Detection of the Outliers in Compact Latent Representation.](http://arxiv.org/abs/2306.09646) | 本文研究了基于变分自编码器（VAE）的无监督异常检测方法，通过引入图像的紧性特征来纠正VAE模型中的理论缺陷并缩小内点与离群点之间的距离，同时结合建模技术和结构知识提出了一种无监督异常检测算法。 |
| [^69] | [BISCUIT: Causal Representation Learning from Binary Interactions.](http://arxiv.org/abs/2306.09643) | 本文提出了一种名为BISCUIT的方法，可以在许多常见的设置中确定因果变量，并在机器人启发的数据集上进行了测试，表现良好。 |
| [^70] | [Cross-Domain Toxic Spans Detection.](http://arxiv.org/abs/2306.09642) | 本研究评估了三种跨域条件下检测毒性片段的方法，结果表明使用现成的词典的简单方法在跨域设置中表现最佳，而用于领域内的语言模型容易出现某些类型的假阳性。 |
| [^71] | [CLIPSonic: Text-to-Audio Synthesis with Unlabeled Videos and Pretrained Language-Vision Models.](http://arxiv.org/abs/2306.09635) | 本文提出了一种使用未标注视频和预训练语言视觉模型进行文本合成音频的方法。通过利用视觉模态作为桥梁来学习文本-音频对应关系，提出了有条件的扩散模型，生成视频的音轨。使用CLIP图像查询条件进行零样本模态转换。并采用预训练的扩散先验模型，生成对应于CLIP文本嵌入的CLIP图像嵌入。实验证明了该方法的有效性。 |
| [^72] | [The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement.](http://arxiv.org/abs/2306.09633) | 谷歌2021年在《自然》杂志上发表的一篇论文声称其使用强化学习在芯片设计领域进行了创新，但两项独立的评估表明，谷歌的方法不如人类设计师、不如一个众所周知的算法（模拟退火），并且也不如普遍可用的商业软件，文章的完整性也遭到了严重的损害。 |
| [^73] | [Power-law Dynamic arising from machine learning.](http://arxiv.org/abs/2306.09624) | 该论文研究了机器学习中的幂律动态，证明了其具有唯一平稳分布且可以通过比较连续和离散化幂律动态的出现时间来指导机器学习算法。 |
| [^74] | [From Hypergraph Energy Functions to Hypergraph Neural Networks.](http://arxiv.org/abs/2306.09623) | 本文提出了一种新的基于超图能量函数的节点嵌入方法，可以通过双层优化实现节点分类任务，相比传统GNN模型有更好的表现。 |
| [^75] | [Emergent Asymmetry of Precision and Recall for Measuring Fidelity and Diversity of Generative Models in High Dimensions.](http://arxiv.org/abs/2306.09618) | 本研究发现在高维生成模型测度中使用的精度和召回指标存在不对称性，可能会导致误导性结论。我们提出了一些方法来修正这种错误。 |
| [^76] | [HomoGCL: Rethinking Homophily in Graph Contrastive Learning.](http://arxiv.org/abs/2306.09614) | HomoGCL是一个模型无关的图形对比学习框架，利用同质性原则扩展正集，从而进一步提高图形对比学习的性能。 |
| [^77] | [GraphSHA: Synthesizing Harder Samples for Class-Imbalanced Node Classification.](http://arxiv.org/abs/2306.09612) | GraphSHA是一种能够缓解类不平衡问题的通用框架，在综合更难的较小类别样本以扩大较小类别的决策边界的同时，还提出了SemiMixup模块以避免扩大的边界违反邻居类别的子空间。 |
| [^78] | [CHORUS: Foundation Models for Unified Data Discovery and Exploration.](http://arxiv.org/abs/2306.09610) | 研究者探索将大型语言模型应用于数据发现和探索任务中，证明这些模型在表格类检测、列类型注释和联接列预测中具有优越性能，并有望将不同的数据管理任务统一在基础模型下。 |
| [^79] | [Structured Cooperative Learning with Graphical Model Priors.](http://arxiv.org/abs/2306.09595) | 本文提出了结构化协作学习算法，在不同设备之间通过协作完成分散任务。通过图模型先验生成的协作图，算法可以自动捕捉设备之间的跨任务相关性。 |
| [^80] | [FewSAR: A Few-shot SAR Image Classification Benchmark.](http://arxiv.org/abs/2306.09592) | 为解决SAR目标图像分类缺乏统一基准问题，我们提出了FewSAR，一个包括15个经典方法的Python代码库可用于小样本SAR图像分类任务。 |
| [^81] | [Understanding the Role of Feedback in Online Learning with Switching Costs.](http://arxiv.org/abs/2306.09588) | 本文研究了反馈在在线学习中的作用，发现在带有额外观测的情况下，当额外观测的数量超过一定阈值时，能减少后悔的最小值。 |
| [^82] | [Is the Volume of a Credal Set a Good Measure for Epistemic Uncertainty?.](http://arxiv.org/abs/2306.09586) | 本文探讨了将不确定性表示为一个置信集而非单一概率分布的方法。并发现，在二元分类中，信任集的体积是一种有意义的衡量认知不确定性的方法，但在多类分类中则没有这种效果。 |
| [^83] | [Fuzzy Feature Selection with Key-based Cryptographic Transformations.](http://arxiv.org/abs/2306.09583) | 本文提出了一种基于模糊逻辑的模糊特征选择方法，可用于选择最有效地完成密码学变换过程的特征子集，旨在提高密码系统的安全性和效率。 |
| [^84] | [Low-Switching Policy Gradient with Exploration via Online Sensitivity Sampling.](http://arxiv.org/abs/2306.09554) | 本文提出了一个具有探索和样本复杂度低的策略优化算法LPO，使用有界eluder-维数和在线灵敏度抽样来适用于非线性参数化的策略，并且在较少的样本量下获得了近似最优策略。 |
| [^85] | [QH9: A Quantum Hamiltonian Prediction Benchmark for QM9 Molecules.](http://arxiv.org/abs/2306.09549) | 该论文提出了一种新的量子哈密顿数据集QH9，用于为各种分子提供精确的哈密顿矩阵。通过设计基准任务，展示了当前机器学习模型有能力预测任意分子的哈密顿矩阵。 |
| [^86] | [Online Heavy-tailed Change-point detection.](http://arxiv.org/abs/2306.09548) | 本文提出了一种在线变点检测算法，可以应对重尾分布且保证有限的假阳性率。 |
| [^87] | [Training generative models from privatized data.](http://arxiv.org/abs/2306.09547) | 介绍了一种在隐私化数据上训练GAN的框架，使用熵正则化Wasserstein距离去噪可以缓解正则化偏差和隐私化噪声的影响，提高模型有效性。 |
| [^88] | [Block-State Transformer.](http://arxiv.org/abs/2306.09539) | 本文提出了一种名为块状态变换器（BST）的混合神经网络层，结合了状态空间模型和块变换器，旨在在语言建模任务中提高性能和可扩展性。实验证明，该模型在语言建模困惑度上优于类似的基于Transformer的架构，并且可以推广到更长的序列。此外，在层级别上具有超过十倍的速度提升。 |
| [^89] | [Residual Q-Learning: Offline and Online Policy Customization without Value.](http://arxiv.org/abs/2306.09526) | 该研究提出了一种新方法，使用动态控制残差的 Q 学习来进行离线和在线的策略定制，无需使用价值函数。 |
| [^90] | [Tighter Prediction Intervals for Causal Outcomes Under Hidden Confounding.](http://arxiv.org/abs/2306.09520) | 本文提出了一种名为Caus-Modens的算法，通过调制集合来描述因果结果区间，相比符合性预测方法，能够在实践中给出更紧密的结果区间。 |
| [^91] | [Relation-Aware Network with Attention-Based Loss for Few-Shot Knowledge Graph Completion.](http://arxiv.org/abs/2306.09519) | 本文提出了一种新颖的RANA框架，利用有策略地选择相关负样本和设计基于注意力机制的损失函数来更好地利用负样本并缓解零损失问题，同时设计了一种动态的关系感知实体编码来捕获不同关系下实体的不同表示。 |
| [^92] | [A Hybrid Feature Selection and Construction Method for Detection of Wind Turbine Generator Heating Faults.](http://arxiv.org/abs/2306.09491) | 该论文提出了一种用于检测风力涡轮机发电机加热故障的混合特征选取和构造方法，旨在通过特征构造和选取提高分类精度和降低计算负担。 |
| [^93] | [Attention-based Open RAN Slice Management using Deep Reinforcement Learning.](http://arxiv.org/abs/2306.09490) | 本文提出了一种创新的基于注意力的深度强化学习技术，利用O-RAN分离模块和分布式代理合作来优化网络切片管理。 |
| [^94] | [FedMultimodal: A Benchmark For Multimodal Federated Learning.](http://arxiv.org/abs/2306.09486) | FedMultimodal 是面向多模态联邦学习的基准测试，为此我们提出了第一个覆盖五种不同应用领域和十个社区的FL基准测试，以便促进多模态FL研究。 |
| [^95] | [R2-Diff: Denoising by diffusion as a refinement of retrieved motion for image-based motion prediction.](http://arxiv.org/abs/2306.09483) | R2-Diff 是基于图像相似度检索运动后，采用扩散模型进行降噪优化的基于图像的运动预测方法，可以更好地预测上下文相关的运动。 |
| [^96] | [Leveraging Residue Number System for Designing High-Precision Analog Deep Neural Network Accelerators.](http://arxiv.org/abs/2306.09481) | 本文提出使用余数系统（RNS）来消除ADC有限精度带来的信息丢失，从多个低精度运算组成高精度运算，实现高准确性和良好能源效率的模拟DNN加速器，并建议使用冗余的RNS来实现容错性。同时RNS相对于常规定点方法能够将数据转换器的能耗降低数个数量级。 |
| [^97] | [Understanding and Mitigating Extrapolation Failures in Physics-Informed Neural Networks.](http://arxiv.org/abs/2306.09478) | 本文研究了物理感知神经网络（PINNs）在外推场景下的行为，并提供了降低外推误差的策略。研究结果表明，一旦模型内插误差为零，进一步增加架构大小或采样点数量对于外推行为没有影响。 |
| [^98] | [FFB: A Fair Fairness Benchmark for In-Processing Group Fairness Methods.](http://arxiv.org/abs/2306.09468) | 本文提出了针对处理中组公平方法的公平公正基准框架（FFB），并进行了全面分析。该工作的关键贡献包括提供灵活、可扩展、极简和面向研究的开源代码；建立统一的公平方法基准测试流水线；进行广泛的基准测试，从 $\mathbf{45,079}$ 个实验中获取关键见解。 |
| [^99] | [AQuA: A Benchmarking Tool for Label Quality Assessment.](http://arxiv.org/abs/2306.09467) | AQuA是一款用于标签质量评估的基准测试工具，目的是评估标签噪声存在下的机器学习方法。该基准测试环境包括数据模拟、质量不同的真实数据集和几种最先进的标签噪声消除方法。 |
| [^100] | [Simplified Temporal Consistency Reinforcement Learning.](http://arxiv.org/abs/2306.09466) | 本文表明，一种仅依靠潜在动力学模型的潜在时间一致性训练的简单表示学习方法，可以实现高性能的强化学习，同时在纯规划过程中和模型无关的强化学习中都适用。在实验中，该方法解决了挑战性的高维奔跑任务，并且训练速度比基于集合的方法要快4.1倍。 |
| [^101] | [Kriging Convolutional Networks.](http://arxiv.org/abs/2306.09463) | 该研究介绍了一种新的Kriging卷积网络方法，结合了Kriging和图卷积网络的优点，并在多个应用中表现出了更好的性能。 |
| [^102] | [Motion Comfort Optimization for Autonomous Vehicles: Concepts, Methods, and Techniques.](http://arxiv.org/abs/2306.09462) | 本文介绍了针对人类舒适度的自动驾驶架构和与其相关的补充框架。讨论了自动驾驶舒适性、响应时间、运动晕车和优化技术等方面的技术细节和挑战。 |
| [^103] | [Hierarchical confusion matrix for classification performance evaluation.](http://arxiv.org/abs/2306.09461) | 提出了一种分级混淆矩阵用于分类性能评价，该方法考虑了分级分类问题的特殊性并可适用于多种类型的问题。实验证明该方法的合理性及其对于分级分类问题的实用性。 |
| [^104] | [Recurrent Memory Decision Transformer.](http://arxiv.org/abs/2306.09459) | 本文提出了循环记忆决策变压器（RMDT）模型，用于处理强化学习中的长序列问题。在Atari游戏和MoJoCo控制问题上的实验表明，采用循环记忆机制的RMDT模型显着优于其没有循环记忆机制的对应模型。 |
| [^105] | [Prevention of cyberattacks in WSN and packet drop by CI framework and information processing protocol using AI and Big Data.](http://arxiv.org/abs/2306.09448) | 本论文介绍了一个利用CI框架和信息处理协议以及人工智能和大数据分析预防WSN中的网络攻击和数据丢失的新方法，该框架通过动态响应威胁场景、采用人工智能算法和异常检测算法来实现网络安全性，并提供了一个专注于数据传输完整性的信息处理协议。 |
| [^106] | [Large-Scale Quantum Separability Through a Reproducible Machine Learning Lens.](http://arxiv.org/abs/2306.09444) | 本研究提出了一个机器学习管道用于大规模场景下量子可分性的近似解，通过有效算法近似查找最近的可分离密度矩阵，并将量子可分性视为分类问题，对任何二维混合状态都适用。 |
| [^107] | [Explore, Establish, Exploit: Red Teaming Language Models from Scratch.](http://arxiv.org/abs/2306.09442) | 本文提出了一种新的红队行动，通过从高层次、抽象的规范出发来考虑语言模型的行为，以探究模型的创新和贡献。 |
| [^108] | [Unsupervised Anomaly Detection via Nonlinear Manifold Learning.](http://arxiv.org/abs/2306.09441) | 该篇论文提出了一种基于非线性流形学习的无监督异常检测方法，可以鲁棒、高效、可解释地检测数据中的异常样本。 |
| [^109] | [Towards Practical Federated Causal Structure Learning.](http://arxiv.org/abs/2306.09433) | 为了解决联邦学习条件下的因果结构学习难题，提出了一种基于联邦条件独立性检验的因果结构学习方案FedC2SL，无需收集原始数据且对数据变异具有更强的抵抗力。 |
| [^110] | [Deep learning techniques for blind image super-resolution: A high-scale multi-domain perspective evaluation.](http://arxiv.org/abs/2306.09426) | 本文介绍了五种深度学习技术，通过对14个来自不同领域的小型数据集进行高比例尺度（8x）的控制实验，评估了这些技术在盲目图像超分辨率方面的效果，其中BlindSR取得了最佳的性能和计算成本之间的平衡，而APA在性能和复杂性之间取得了显著的性能，是在不需要实时计算的应用程序的良好选择。 |
| [^111] | [Arbitrariness Lies Beyond the Fairness-Accuracy Frontier.](http://arxiv.org/abs/2306.09425) | 研究表明，仅考虑组公平性和准确性的机器学习公平性干预可能加剧预测多样性。因此，应考虑第三个“任意性”轴，提出了一种集成算法可提供更一致的预测。 |
| [^112] | [SSL4EO-L: Datasets and Foundation Models for Landsat Imagery.](http://arxiv.org/abs/2306.09424) | 本文介绍了第一个专为Landsat系列卫星设计的自监督学习数据集SSL4EO-L，这也是历史上最大的Landsat数据集，用于预训练基础模型，并在多个下游任务上实现了最先进的性能。 |
| [^113] | [Diff-TTSG: Denoising probabilistic integrated speech and gesture synthesis.](http://arxiv.org/abs/2306.09417) | Diff-TTSG是第一个基于扩散的概率模型，用于联合学习合成语音和手势，相比于先前最新技术的非概率方法，它可以更好地捕捉人类讲话和运动的变化，产生更逼真和多样化的集成语音和手势合成。 |
| [^114] | [Non-Asymptotic Performance of Social Machine Learning Under Limited Data.](http://arxiv.org/abs/2306.09397) | 本文研究了限制数据下社交机器学习的概率误差问题，并提出了一种更强的一致性训练条件，推导出了两种任务的概率误差上界。 |
| [^115] | [Private Federated Frequency Estimation: Adapting to the Hardness of the Instance.](http://arxiv.org/abs/2306.09396) | 本论文提出了一种新的混合sketching算法并解决了一个基本问题：如何根据底层问题的难度设置sketch size。 |
| [^116] | [Multi-omics Prediction from High-content Cellular Imaging with Deep Learning.](http://arxiv.org/abs/2306.09391) | 本研究使用深度学习方法，从高内容细胞成像中直接预测细胞群体的多组学。实验结果表明，该方法能够在多种刺激条件下实现显著成果，为细胞组学领域提供了新的方法。 |
| [^117] | [ChatGPT for Suicide Risk Assessment on Social Media: Quantitative Evaluation of Model Performance, Potentials and Limitations.](http://arxiv.org/abs/2306.09390) | 本文量化评估了基于社交媒体的 ChatGPT 模型在自杀倾向评估方面的表现，比较了其结果与两个微调模型，并探讨了模型响应生成的最佳温度，研究结果显示，以人工标注数据集为基础微调的 Transformer 模型表现更佳，这篇论文为心理健康专家提供了重要的模型评估和参数优化建议。 |
| [^118] | [ST-PINN: A Self-Training Physics-Informed Neural Network for Partial Differential Equations.](http://arxiv.org/abs/2306.09389) | 提出了一种自训练物理知识神经网络ST-PINN。在训练期间引入基于伪标签的自学习算法，以提高现有PINN的精度和收敛性。实验结果表明， ST-PINN 可以学习更多的物理知识并受益于更好的收敛性。 |
| [^119] | [Adaptive Hierarchical SpatioTemporal Network for Traffic Forecasting.](http://arxiv.org/abs/2306.09386) | 本文提出了一种自适应分层时空网络（AHSTN），通过利用空间层次结构和建模多尺度空间相关性促进交通预测，AHSTN在节点级别的基础上引入了自适应的时空块，来自适应地处理不同层次之间的相关性，同时使用分层注意机制来选择性地聚合不同尺度的信息，具有优越性。 |
| [^120] | [Employing Multimodal Machine Learning for Stress Detection.](http://arxiv.org/abs/2306.09385) | 本文提出了一个基于多模态AI的框架，旨在通过融合多种信息来精确监测人的工作行为和压力水平。该框架使用卷积神经网络、时延神经网络和多层感知器等深度学习技术，能够高精度地检测高压力和低压力状态。 |
| [^121] | [Sound Demixing Challenge 2023 -- Music Demixing Track Technical Report.](http://arxiv.org/abs/2306.09382) | 本文介绍了2023年声音分离挑战赛音乐分离赛道的两个有效方法，分别是时效高的源分离网络和适用于噪声鲁棒性源分离的损失掩模方法。 |
| [^122] | [Spatiotemporal-Augmented Graph Neural Networks for Human Mobility Simulation.](http://arxiv.org/abs/2306.09381) | STAR框架通过设计多种时空图来捕捉位置的动态时空效应，为人类移动模拟任务提供新的方法。 |
| [^123] | [Understanding Parameter Sharing in Transformers.](http://arxiv.org/abs/2306.09380) | 本文从模型复杂度和梯度范围两个角度研究了Transformer中参数共享的有效性，发现提高训练收敛性是其中主要原因，模型复杂度只有一小部分贡献。 |
| [^124] | [Language Aligned Visual Representations Predict Human Behavior in Naturalistic Learning Tasks.](http://arxiv.org/abs/2306.09377) | 语言对齐的视觉表示方式比纯视觉表示方式更有效地预测人类在自然学习任务中的行为。 |
| [^125] | [Modularizing while Training: a New Paradigm for Modularizing DNN Models.](http://arxiv.org/abs/2306.09376) | 本文提出了一种新方法，将模块化纳入模型训练过程中，即在训练时模块化(MwT)，通过两个损失函数实现模型结构上的模块化，进而实现模块的重用，能够在较短的训练时间内达到可比较的模型精度，并且相对于最先进的训练后模块化方法需要更少的参数。 |
| [^126] | [Symmetry-Informed Geometric Representation for Molecules, Proteins, and Crystalline Materials.](http://arxiv.org/abs/2306.09375) | 本文提出了一个对称性识别的几何表示方法，称之为Geom3D，可以用于分子、蛋白质和晶体材料的基准测评。经过实验证明，该方法在各种数据集上实现了最先进的性能。 |
| [^127] | [Equitable Multi-task Learning.](http://arxiv.org/abs/2306.09373) | 该论文提出了一种名为EMTL的多任务优化方法，以实现公平的多任务学习。通过规范化不同任务的相对贡献，可以提高MTL的泛化性能，并利用方差正则化和高效的优化算法保证收敛。实验证明，该方法在合成和真实数据集上均表现出了更好的性能。 |
| [^128] | [Warpformer: A Multi-scale Modeling Approach for Irregular Clinical Time Series.](http://arxiv.org/abs/2306.09368) | Warpformer是一种能够完整考虑序列内不规则性和序列间差异性的多尺度建模方法。 |
| [^129] | [Fault Detection in Induction Motors using Functional Dimensionality Reduction Methods.](http://arxiv.org/abs/2306.09365) | 本研究提出了一种采用函数降维方法结合电机电流特征分析策略的故障检测方法，能够实时检测感应电动机中的故障，并且可以通过离线分析识别更多类型的故障。 |
| [^130] | [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting.](http://arxiv.org/abs/2306.09364) | TSMixer是一种用于多元时间序列预测的轻量级MLP-Mixer模型，可以有效地捕捉时间序列属性并在准确性方面超越了Transformers的方法。 |
| [^131] | [A Simple Data Augmentation for Feature Distribution Skewed Federated Learning.](http://arxiv.org/abs/2306.09363) | 本文针对特征分布偏斜的联邦学习提出了FedRDN方法，在输入层级上实现了数据增强，将整个联邦数据集的统计信息注入到本地客户端数据中，以缓解特征漂移问题。 |
| [^132] | [Deep Learning-Based Spatiotemporal Multi-Event Reconstruction for Delay Line Detectors.](http://arxiv.org/abs/2306.09359) | 本文提出了基于深度学习的延迟线探测器时空多事件重构方法，可以成功地重构多个粒子的空间和时间坐标，相比传统方法更加高效和准确。 |
| [^133] | [Datasets and Benchmarks for Offline Safe Reinforcement Learning.](http://arxiv.org/abs/2306.09303) | 该论文提出了一个专门针对离线安全强化学习的基准套件，包含了安全策略、数据集和高质量RL算法实现。作者还提供了一种数据收集流程，利用先进算法生成多样性数据集，用于38个受欢迎的安全RL任务。该套件可加速该领域的研究进展。 |
| [^134] | [Probabilistic Learning of Multivariate Time Series with Temporal Irregularity.](http://arxiv.org/abs/2306.09147) | 本文提出了一种针对具有时间不规则性的多元时间序列的概率学习方法，通过允许观察到达时间在模型构建中发挥核心作用，使用新颖的非参数先验模型明确融入时间不规则性。 |
| [^135] | [Enhanced Sampling with Machine Learning: A Review.](http://arxiv.org/abs/2306.09111) | 本文综述了机器学习与增强分子动力学方法的融合，这些技术能够解决分子动力学的时间尺度限制，成功的策略包括降维、强化学习和基于流的方法。 |
| [^136] | [ClimSim: An open large-scale dataset for training high-resolution physics emulators in hybrid multi-scale climate simulators.](http://arxiv.org/abs/2306.08754) | 这是一个用于训练高分辨率物理仿真器的气候数据集。该数据集包含了5.7亿个多变量输入和输出矢量对，用于隔离本地嵌套的高分辨率、高保真度物理的影响。 |
| [^137] | [Langevin Monte Carlo for strongly log-concave distributions: Randomized midpoint revisited.](http://arxiv.org/abs/2306.08494) | 本文提出了一种针对具有平滑且强对数凹密度的目标分布的采样方法，并给出了该方法的误差上限。 |
| [^138] | [Skill-Critic: Refining Learned Skills for Reinforcement Learning.](http://arxiv.org/abs/2306.08388) | 基于技能筛选与优化的Skill-Critic算法能够提高稀疏奖励环境下强化学习中低层策略的可靠性，并显著提高了性能。 |
| [^139] | [MMASD: A Multimodal Dataset for Autism Intervention Analysis.](http://arxiv.org/abs/2306.08243) | 提出了一个名为MMASD的自闭症多模态数据集，收集自治疗干预。它包括从32名自闭症患儿的干预录音中分段的1,315个数据样本，每个样本包含四种隐私保护模式的数据。 |
| [^140] | [h2oGPT: Democratizing Large Language Models.](http://arxiv.org/abs/2306.08161) | 本文介绍了h2oGPT，这是一套开源代码库，用于创建和使用基于GPTs的大语言模型（LLMs），包括100％私有文档搜索。目标是创建真正开源的替代封闭源GPTs，提高人工智能的开发和可靠性。 |
| [^141] | [Differentiating Metropolis-Hastings to Optimize Intractable Densities.](http://arxiv.org/abs/2306.07961) | 本文通过基于互联马尔科夫链的不偏微分，开发出一种无偏、低方差和自动的方法对复杂密度进行生成，从而实现对 MH 采样器的优化。 |
| [^142] | [Large Language Models Sometimes Generate Purely Negatively-Reinforced Text.](http://arxiv.org/abs/2306.07567) | 大型语言模型有时会从仅包含负奖励的例子中学习，导致生成类似泄漏密码或安全漏洞等敏感信息的文本 |
| [^143] | [Error Feedback Can Accurately Compress Preconditioners.](http://arxiv.org/abs/2306.06098) | 本论文提出一种错误反馈技术，可以通过在馈入预处理器之前对梯度信息进行压缩（稀疏化或低秩压缩），将预处理器的存储成本压缩多达两个数量级，而不会丢失收敛性。 |
| [^144] | [SMRVIS: Point cloud extraction from 3-D ultrasound for non-destructive testing.](http://arxiv.org/abs/2306.04668) | 提出了将超声体积点云提取作为图像分割问题的解决方案，并通过快速原型开发验证效果。 |
| [^145] | [Hiding in Plain Sight: Disguising Data Stealing Attacks in Federated Learning.](http://arxiv.org/abs/2306.03013) | 该研究提出了一个新的恶意服务器攻击框架SEER，可以在联邦学习中窃取用户数据，而且不易被客户端侦测出来。 |
| [^146] | [Graph-Based Model-Agnostic Data Subsampling for Recommendation Systems.](http://arxiv.org/abs/2305.16391) | 本文提出了一种基于图结构的无模型数据子采样方法，通过研究用户-物品图的拓扑结构来估计每个用户-物品交互的重要性，并在网络上进行传播来平滑估计值。该方法结合了无模型和基于模型的子采样方法的优点，在多个基准数据集上表现出较好的实验结果。 |
| [^147] | [Do Not Train It: A Linear Neural Architecture Search of Graph Neural Networks.](http://arxiv.org/abs/2305.14065) | 本文提出了一种新的图神经网络结构搜索方法——神经结构编码（NAC），它通过稀疏编码寻找最优结构参数，无需训练就能发挥表现力，在多个基准数据集上实现了最先进性能，并且运算速度比强基线方法快了200倍，精度提高了18.8％。 |
| [^148] | [Efficient Learning of Quantum States Prepared With Few Non-Clifford Gates.](http://arxiv.org/abs/2305.13409) | 该研究提出了一种能有效学习几个非克利福德门制备的量子状态的算法，并给出了一个随着稳定维数增大而学习所有状态的算法。 |
| [^149] | [Dirichlet Diffusion Score Model for Biological Sequence Generation.](http://arxiv.org/abs/2305.10699) | 本文介绍了一种针对离散数据，使用概率单纯形空间中的扩散过程进行建模的生成SDE模型。称之为Dirchlet扩散分数模型。模型可以生成满足严格限制的样本，且适用于生成生物序列。 |
| [^150] | [CLCIFAR: CIFAR-Derived Benchmark Datasets with Human Annotated Complementary Labels.](http://arxiv.org/abs/2305.08295) | 本研究开发了由人类标注的互补标签，创造了两个真实世界的CLL数据集，进一步揭示了现实表现下CLL算法的性能，为这一领域的研究提供了更实际的评估标准。 |
| [^151] | [An Algorithm For Adversary Aware Decentralized Networked MARL.](http://arxiv.org/abs/2305.05573) | 本文提出了一种对抗感知的去中心化网络多智能体强化学习算法，该算法允许非对抗性智能体在对抗方存在的情况下达成共识。 |
| [^152] | [Learning Neural Constitutive Laws From Motion Observations for Generalizable PDE Dynamics.](http://arxiv.org/abs/2304.14369) | 本文提出了一种混合神经网络和偏微分方程的方法，用于从运动观测中学习可推广的PDE动力学，并介绍了一种新框架"神经本构法"，该框架利用了一种严格保证标准本构先验的网络架构，以此来学习本构模型。 |
| [^153] | [Speed Is All You Need: On-Device Acceleration of Large Diffusion Models via GPU-Aware Optimizations.](http://arxiv.org/abs/2304.11267) | 该研究提出一系列通过GPU-aware优化大型扩散模型的方法，实现在配备GPU的移动设备上极快的推论延迟，扩大了生成性人工智能的适用范围并改善了用户体验。 |
| [^154] | [On Data Sampling Strategies for Training Neural Network Speech Separation Models.](http://arxiv.org/abs/2304.07142) | 本文研究了训练神经网络语音分离模型的数据采样策略对模型性能的影响。研究表明，对于特定的信号长度分布，采用特定的训练信号长度限制可以获得更好的性能。 |
| [^155] | [Deep Learning for Opinion Mining and Topic Classification of Course Reviews.](http://arxiv.org/abs/2304.03394) | 本文利用自然语言处理和深度学习技术，通过比较传统方法和现代机器学习方法，展示了如何处理大量课程评论，进行情感极性分析和主题分类。 |
| [^156] | [Self-Supervised Video Similarity Learning.](http://arxiv.org/abs/2304.03378) | 本文提出了自监督视频相似性学习的方法S$^2$VS，该方法通过学习实例区分解决多个检索和检测任务，无需用到标注数据，并在各个任务上都达到了最新的性能。 |
| [^157] | [On the Prime Number Divisibility by Deep Learning.](http://arxiv.org/abs/2304.01333) | 本文提出了使用深度学习判断质数整除性的方法，并发现关键在于提供给深度学习模型的特征空间。此外，商业可用的自动化机器学习管道无法解决此问题，需要提供适当的特征工程来解决。研究者还提出了一个封闭式解决方案。 |
| [^158] | [Artificial Intelligence in Ovarian Cancer Histopathology: A Systematic Review.](http://arxiv.org/abs/2303.18005) | 通过对36篇文章的综述，该研究发现人工智能模型在卵巢癌的诊断和预后中显示出有希望的结果，但现有研究受到小样本量，潜在偏见和缺乏外部验证的限制。 |
| [^159] | [Iterative Partial Fulfillment of Counterfactual Explanations: Benefits and Risks.](http://arxiv.org/abs/2303.11111) | 迭代部分满足的反事实解释被广泛用于解释机器学习模型在高风险领域中的推理，我们提出了一个新颖的属性：这种解释在迭代部分履行下的行为。主体可以在接收的解释中部分地履行请求一个新的预测和新的解释，重复此过程，直到预测为正面。 |
| [^160] | [Enhancing Activity Prediction Models in Drug Discovery with the Ability to Understand Human Language.](http://arxiv.org/abs/2303.03363) | 本文提出了一种新型活性预测模型，能够通过理解描述任务的文本信息来适应推理时的新预测任务，并在少样本学习基准和药物研发中的零数据问题上都能取得更好的预测性能。 |
| [^161] | [Seq-HyGAN: Sequence Classification via Hypergraph Attention Network.](http://arxiv.org/abs/2303.02393) | 本文提出了一种基于超图注意力网络的序列分类模型Seq-HyGAN，通过创建超图和引入注意力机制来处理序列数据中的复杂结构相似性，从而提高分类准确率。 |
| [^162] | [Representation Disentaglement via Regularization by Identification.](http://arxiv.org/abs/2303.00128) | 本文研究了从观测数据中学习解耦表示的问题，提出通过鉴别性正则化来实现表征解耦，解决了现代深度表征学习模型中出现的纠缠偏差行为问题。 |
| [^163] | [On the Training Instability of Shuffling SGD with Batch Normalization.](http://arxiv.org/abs/2302.12444) | 本文研究了随机梯度下降算法与批量归一化的相互作用，在特定网络中表现为单次重排与随机重排收敛到不同的扭曲全局最优点，建议使用随机重排。 |
| [^164] | [Diffusion Probabilistic Models for Structured Node Classification.](http://arxiv.org/abs/2302.10506) | 本文提出了一种新颖的DPM-SNC框架，通过反向扩散过程和流形约束的采样方法实现基于图结构的结构化节点分类，并设计了一种新的训练算法来应用DPMs，最大化一个新的变分下界。实验证明DPMs可以提高GNN的表达能力，提高节点分类的效果。 |
| [^165] | [Fairness in Matching under Uncertainty.](http://arxiv.org/abs/2302.03810) | 论文研究了算法两边市场中的公平性问题，提出了考虑不确定性因素的个体公平性概念，并设计了一个基于线性规划的模型来优化公平的分配 |
| [^166] | [Augmenting Rule-based DNS Censorship Detection at Scale with Machine Learning.](http://arxiv.org/abs/2302.02031) | 本文探讨了如何使用机器学习模型来帮助简化DNS审查检测过程，提高检测的可靠性，并发现启发式方法所错过的新审查实例和阻止标志。 |
| [^167] | [On Consistency and Asymptotic Normality of Least Absolute Deviation Estimators for 2-dimensional Sinusoidal Model.](http://arxiv.org/abs/2301.03229) | 本文提出了一种鲁棒的最小绝对偏差估计器，用于2维正弦模型参数估计，在数据存在异常值或重尾噪声时具有优越性并得到强一致性和渐近正态性的保证。 |
| [^168] | [Bagging is an Optimal PAC Learner.](http://arxiv.org/abs/2212.02264) | Bagging是一种最优的PAC学习器,具有可证明最优的样本复杂度。相对于Hanneke的算法，Bagging更为高效，且只需要logarithmic数量的子样本。 |
| [^169] | [PASTA: Proportional Amplitude Spectrum Training Augmentation for Syn-to-Real Domain Generalization.](http://arxiv.org/abs/2212.00979) | 本文提出了一种基于比例幅度谱训练增强的方法 PASTA，可有效提高合成数据到真实数据的泛化性能，在多个 Syn-to-Real 任务上均具有优越性能。 |
| [^170] | [Learning Combinatorial Structures via Markov Random Fields with Sampling through Lov\'asz Local Lemma.](http://arxiv.org/abs/2212.00296) | Nelson是一种基于神经网络和Lov\'asz Local Lemma的方法，使用约束的马尔可夫随机场模型生成满足组合约束条件的样本。 |
| [^171] | [Adversarial Cheap Talk.](http://arxiv.org/abs/2211.11030) | 本文提出了一种新型对抗性设置，在其中对手只能将信息附加到受害者的观察中，从而产生最小的影响范围，并提出对抗性廉价交流（ACT）算法进行对手训练。在高度受限的情况下，使用ACT训练的对手仍会对受害者的训练和测试表现产生显著影响，揭示了强化学习算法中的一种新的攻击向量。 |
| [^172] | [Efficient Video Representation Learning via Motion-Aware Token Selection.](http://arxiv.org/abs/2211.10636) | 该论文提出了一种新的运动感知标记选择方法，针对视频中不同补丁的信息密度，选择包含丰富动态特性的标记，放弃无效的标记，从而大大降低计算和存储需求，实现了在单台机器上进行预训练和微调而不影响性能。 |
| [^173] | [Explainable Action Advising for Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2211.07882) | 引入可解释行为建议的多智能体强化学习框架，使得学生可以理解所学的内容并进行推理从而提高样本效率和学习效果 |
| [^174] | [Training Debiased Subnetworks with Contrastive Weight Pruning.](http://arxiv.org/abs/2210.05247) | 本文探讨了在存在强假相关的偏置网络中提取最优无偏子网络的问题，并提出了使用对比剪枝权重训练实现去偏置子网络的算法 DCWP，在多个应用中都有良好的效果。 |
| [^175] | [Less is More: SlimG for Accurate, Robust, and Interpretable Graph Mining.](http://arxiv.org/abs/2210.04081) | SlimG算法是一个简单而有效的图挖掘算法，它基于小心的简单原则，能够在准确性、鲁棒性、可扩展性和可解释性等方面都表现出色，在各种图场景中优于最先进的GNN算法，是一个有前途的实用算法。 |
| [^176] | [Fact-Saboteurs: A Taxonomy of Evidence Manipulation Attacks against Fact-Verification Systems.](http://arxiv.org/abs/2209.03755) | 本研究提出了一种分类法，涵盖针对在线证据的伪装和虚假信息等两种攻击目标和不同的威胁模型维度。我们设计并提出了几种可能的攻击方法，展示了在多种情况下，可以对证据中与事实相关的部分进行微小的修改，并生成无法被自动识别的虚假信息。 |
| [^177] | [Data-Driven Influence Functions for Optimization-Based Causal Inference.](http://arxiv.org/abs/2208.13701) | 本文提出了一种利用有限差分逼近统计泛函Gateaux导数的构造算法，并研究了从数据中进行概率分布估计的情况下的Gateaux导数估计。研究结果为因果推断和动态治疗方案等问题提供了解决方案。 |
| [^178] | [Finding Reusable Machine Learning Components to Build Programming Language Processing Pipelines.](http://arxiv.org/abs/2208.05596) | 该论文提出一种寻找可重用的机器学习组件以构建编程语言处理流水线的方法，并通过收集、分析代表性论文来鉴定和描述关键概念，从而改善机器学习组件的可发现性、可访问性、互操作性和可重用性。 |
| [^179] | [Resolving the Human Subjects Status of Machine Learning's Crowdworkers.](http://arxiv.org/abs/2206.04039) | 机器学习在研究中使用的众包工作者问题引起了对其受试者身份的争议与监管合规性，本文针对该问题进行研究，重点关注了自然语言处理领域中的研究监管挑战。 |
| [^180] | [Calibrate and Debias Layer-wise Sampling for Graph Convolutional Networks.](http://arxiv.org/abs/2206.00583) | 本文根据矩阵近似的角度重新审视了GCN节点嵌入聚合的逐层采样方法，并提出了解决次优采样概率和估计偏差问题的两种新方案。实验证明这些改进是有效的。 |
| [^181] | [Temporal Detection of Anomalies via Actor-Critic Based Controlled Sensing.](http://arxiv.org/abs/2201.00879) | 本研究提出了一种基于Actor-Critic的控制感知方法，并使用贝叶斯公式学习过程状态的后验概率，以解决二元随机过程的异常检测问题。其能够优于现有方法，在低信噪比情况下有效地检测时序数据中的异常情况。 |
| [^182] | [Dynamic treatment effects: high-dimensional inference under model misspecification.](http://arxiv.org/abs/2111.06818) | 本文提出了一种新的鲁棒估计方法来解决动态治疗效应估计中的挑战，提高了在模型错误下的高维环境中的估计鲁棒性和可靠性。 |
| [^183] | [ARFED: Attack-Resistant Federated averaging based on outlier elimination.](http://arxiv.org/abs/2111.04550) | ARFED是一种防御算法，能够消除模型更新中的离群值，不需要假设数据分布、更新相似性或恶意参与者比率，实现了在基准联邦学习数据集上的最先进的防攻击鲁棒性能。 |
| [^184] | [Knowledge-driven Active Learning.](http://arxiv.org/abs/2110.08265) | 本文提出了基于知识的主动学习(KAL)框架，将通用领域知识转换为逻辑约束，作为样本选择的指南，使非专业用户能够用更少的样本训练模型，并在降低标记数据需求量的同时保持出色性能。 |
| [^185] | [Adversarial Image Color Transformations in Explicit Color Filter Space.](http://arxiv.org/abs/2011.06690) | 本论文提出了一种新型颜色变换攻击方法AdvCF，它通过简单色彩滤镜参数空间中的梯度信息进行优化，具有可区分但不引人注意的特点，并提供了对模型抵抗对抗性颜色变换的系统分析。 |
| [^186] | [A Unified Approach to Synchronization Problems over Subgroups of the Orthogonal Group.](http://arxiv.org/abs/2009.07514) | 本文提出了一个统一方法来解决正交群子群同步问题，该方法通过广义幂方法的迭代修正和恰当的初始步骤可以在某些假设条件下获得较强的理论保证，并且在计算机视觉和传感器网络等领域有实际应用。 |
| [^187] | [Boosting Simple Learners.](http://arxiv.org/abs/2001.11704) | 本论文探讨的是提升学习器的方法，关注弱学习器属于一个容量受限的类的假设，并重点关注需要多少个弱学习器才能生成准确的假设。通过设计新颖的算法，只需要约$\tilde{O}({1}/{\gamma})$个弱假设就能够规避经典下界。 |

# 详细

[^1]: 仅使用一字节（每梯度）：关于使用共享随机性进行低带宽分散式语言模型微调的简要说明

    Just One Byte (per gradient): A Note on Low-Bandwidth Decentralized Language Model Finetuning Using Shared Randomness. (arXiv:2306.10015v1 [cs.LG])

    [http://arxiv.org/abs/2306.10015](http://arxiv.org/abs/2306.10015)

    本文介绍了一种通过使用共享随机性来进行低带宽分散式微调的方法，该方法可以通过每台机器生成不同的随机扰动来更新每个模型，从而具有高度通信效率，并且具有隐私保护的优势。

    

    分布式模型训练受到梯度交换的通信成本的限制。本文通过使用共享随机性来进行低带宽的分散式微调，扩展了Malladi等人2023年的最新工作。该方法是对具有记忆效率的同时扰动随机逼近（SPSA）的自然分散式扩展。在每次迭代中，每台机器都使用随机数生成器（RNG）来对模型权重进行局部可重现的扰动，并计算和交换标量投影梯度，然后用于更新每个模型。通过将（机器，样本）标识符用作随机种子，每个模型可以重新生成彼此的扰动。由于机器只交换单字节的投影梯度，因此这是高度通信效率的。此外，还存在潜在的隐私优势，因为投影梯度可以在不同的训练数据上计算，而模型从不访问其他数据。我们的方法不仅在具有低通信成本的同时，也在模型的准确性和训练速度方面展现出优势。

    Language model training in distributed settings is limited by the communication cost of gradient exchanges. In this short note, we extend recent work from Malladi et al. (2023), using shared randomness to perform distributed fine-tuning with low bandwidth. The method is a natural decentralized extension of memory-efficient Simultaneous Perturbation Stochastic Approximation (SPSA). Each iteration, each machine seeds a Random Number Generator (RNG) to perform local reproducible perturbations on model weights and calculate and exchange scalar projected gradients, which are then used to update each model. By using a (machine, sample) identifier as the random seed, each model can regenerate one another's perturbations. As machines only exchange single-byte projected gradients, this is highly communication efficient. There are also potential privacy benefits, as projected gradients may be calculated on different training data, and models never access the other's data. Our approach not only d
    
[^2]: CLIP2Protect：使用对抗性潜在搜索的文本引导化妆来保护面部隐私

    CLIP2Protect: Protecting Facial Privacy using Text-Guided Makeup via Adversarial Latent Search. (arXiv:2306.10008v1 [cs.CV])

    [http://arxiv.org/abs/2306.10008](http://arxiv.org/abs/2306.10008)

    该论文提出了一种两步法面部隐私保护方法CLIP2Protect，使用对抗性潜在搜索结合文本引导化妆，生成高质量面部图像，从而保护面部隐私。

    

    基于深度学习的面部识别系统的成功已经引起了严重的隐私问题，因为它们能够在数字世界中启用未授权的用户跟踪。现有的隐私增强方法无法生成自然主义图像，既能保护面部隐私又不会损害用户体验。我们提出了一种新的面部隐私保护方法，采用两步法，依靠在预训练生成模型的低维流形中找到对抗性潜在编码。第一步将给定的面部图像反演成潜在空间中的编码，并微调生成模型，以从其潜在代码准确地重构给定的图像。这一步产生了一个良好的初始化，有助于生成类似于给定身份的高质量面部。随后，使用用户定义的化妆文本提示和保持身份的规范化来指导在潜在空间中寻找对抗性代码的搜索。广泛的实验表明，我们的方法能够生成具有高质量化妆的面部图像，可有效地保护面部隐私。

    The success of deep learning based face recognition systems has given rise to serious privacy concerns due to their ability to enable unauthorized tracking of users in the digital world. Existing methods for enhancing privacy fail to generate naturalistic images that can protect facial privacy without compromising user experience. We propose a novel two-step approach for facial privacy protection that relies on finding adversarial latent codes in the low-dimensional manifold of a pretrained generative model. The first step inverts the given face image into the latent space and finetunes the generative model to achieve an accurate reconstruction of the given image from its latent code. This step produces a good initialization, aiding the generation of high-quality faces that resemble the given identity. Subsequently, user-defined makeup text prompts and identity-preserving regularization are used to guide the search for adversarial codes in the latent space. Extensive experiments demons
    
[^3]: 具有感觉运动预训练的机器人学习

    Robot Learning with Sensorimotor Pre-training. (arXiv:2306.10007v1 [cs.RO])

    [http://arxiv.org/abs/2306.10007](http://arxiv.org/abs/2306.10007)

    本论文介绍了一种针对机器人学习的自监督感觉运动预训练方法，使用 Transformer 模型在视觉表示上进行操作，通过 20,000 条真实世界轨迹数据集的预训练可以使机器人在堆积方块任务中性能提高 2 倍，并使其能够更快地学习新任务。

    

    我们提出了一种自监督的感觉运动预训练方法，用于机器人学习。我们的模型称为 RPT，是一种 Transformer，它对感觉运动令牌序列进行操作。给定一系列相机图像、本体感觉机器人状态和过去的动作，我们将交错的序列编码为令牌，掩模出随机子集，并训练模型来预测掩模的内容。我们假设如果机器人能够预测缺失的内容，它已经获得了一个可以使其行动的物理世界的良好模型。RPT 的设计是在潜在的视觉表示上进行操作，从而使预测变得可行，能够实现 10 倍的模型扩展，并能在实际机器人上进行每秒 10 次的推理。为了评估我们的方法，我们使用运动规划和基于模型的抓取算法，收集了 9 个月内的 20,000 条真实世界轨迹数据集。我们发现，对这些数据进行预训练始终优于从头开始训练，在堆积方块任务中导致 2 倍的性能提高，并使机器人能够更快地学习新任务。

    We present a self-supervised sensorimotor pre-training approach for robotics. Our model, called RPT, is a Transformer that operates on sequences of sensorimotor tokens. Given a sequence of camera images, proprioceptive robot states, and past actions, we encode the interleaved sequence into tokens, mask out a random subset, and train a model to predict the masked-out content. We hypothesize that if the robot can predict the missing content it has acquired a good model of the physical world that can enable it to act. RPT is designed to operate on latent visual representations which makes prediction tractable, enables scaling to 10x larger models, and 10 Hz inference on a real robot. To evaluate our approach, we collect a dataset of 20,000 real-world trajectories over 9 months using a combination of motion planning and model-based grasping algorithms. We find that pre-training on this data consistently outperforms training from scratch, leads to 2x improvements in the block stacking task,
    
[^4]: 基于现实表演的面部动画风格感知非监督学习

    Unsupervised Learning of Style-Aware Facial Animation from Real Acting Performances. (arXiv:2306.10006v1 [cs.CV])

    [http://arxiv.org/abs/2306.10006](http://arxiv.org/abs/2306.10006)

    本文提出了一种非监督学习的动画方法，可以通过文本或语音输入，实现基于真实动作表演的面部动画，并且可以不同程度地学习并合成不同的表演风格。

    

    本文提出了一种新的方法，基于混合形状几何、动态纹理和神经渲染，用于从真实动作表演中驱动面部模型的文本/语音动画。通过训练包括形状和纹理的VAE，我们得到了一个参数化模型，以精确捕捉和逼真合成潜在特征向量中的面部表情。我们的动画方法基于条件卷积神经网络，将文本或语音转换为一系列动画参数。与以往的方法不同，我们的动画模型以非监督的方式学习区分和合成不同的表演风格，只需要用于描述训练序列内容的语音标签。为了实现逼真的实时渲染，我们训练了一个U-Net，通过计算改进的像素颜色和前景遮罩来改善栅格化渲染。我们定性/定量地将我们的框架与最近的头部建模方法以及面部动画方法进行比较，并评估感知渲染/动画效果。

    This paper presents a novel approach for text/speech-driven animation of a photo-realistic head model based on blend-shape geometry, dynamic textures, and neural rendering. Training a VAE for geometry and texture yields a parametric model for accurate capturing and realistic synthesis of facial expressions from a latent feature vector. Our animation method is based on a conditional CNN that transforms text or speech into a sequence of animation parameters. In contrast to previous approaches, our animation model learns disentangling/synthesizing different acting-styles in an unsupervised manner, requiring only phonetic labels that describe the content of training sequences. For realistic real-time rendering, we train a U-Net that refines rasterization-based renderings by computing improved pixel colors and a foreground matte. We compare our framework qualitatively/quantitatively against recent methods for head modeling as well as facial animation and evaluate the perceived rendering/ani
    
[^5]: SLACK: 冷启动和KL正则化的稳定数据增强学习

    SLACK: Stable Learning of Augmentations with Cold-start and KL regularization. (arXiv:2306.09998v1 [cs.CV])

    [http://arxiv.org/abs/2306.09998](http://arxiv.org/abs/2306.09998)

    该论文提出了一种在不依赖先前知识的情况下直接学习数据增强策略的方法。

    

    数据增强已被证明可以提高神经网络的泛化能力，前提是要精心选择一组变换。自动数据增强旨在自动化这个过程。然而，大多数最近的方法仍然依赖于一些先前的信息。本文提出了一种在不依赖先前知识的情况下直接学习数据增强策略的方法。

    Data augmentation is known to improve the generalization capabilities of neural networks, provided that the set of transformations is chosen with care, a selection often performed manually. Automatic data augmentation aims at automating this process. However, most recent approaches still rely on some prior information; they start from a small pool of manually-selected default transformations that are either used to pretrain the network or forced to be part of the policy learned by the automatic data augmentation algorithm. In this paper, we propose to directly learn the augmentation policy without leveraging such prior knowledge. The resulting bilevel optimization problem becomes more challenging due to the larger search space and the inherent instability of bilevel optimization algorithms. To mitigate these issues (i) we follow a successive cold-start strategy with a Kullback-Leibler regularization, and (ii) we parameterize magnitudes as continuous distributions. Our approach leads to
    
[^6]: 基于偏好的强化学习中的公平性

    Fairness in Preference-based Reinforcement Learning. (arXiv:2306.09995v1 [cs.LG])

    [http://arxiv.org/abs/2306.09995](http://arxiv.org/abs/2306.09995)

    该论文提出了一种名为FPbRL的新的公平偏好强化学习方法，旨在通过广义Gini福利函数最大化策略学习来实现多目标优化并处理每个目标的公平性。

    

    本文研究了在多目标情况下偏好强化学习(PbRL)中的公平性问题。主要目标是设计控制策略，既能够优化多个目标，又能够公平地处理每个目标。为实现这一目标，我们设计了一种新的公平偏好强化学习(FPbRL)方法。FPbRL的主要思想是通过新的福利偏好而不是PbRL中的基于奖励的偏好来学习与多目标关联的向量奖励函数，并通过最大化广义Gini福利函数进行策略学习。最后，在三个不同的环境上进行实验研究，展示了所提出的FPbRL方法能够实现有效和公平的控制策略的学习。

    In this paper, we address the issue of fairness in preference-based reinforcement learning (PbRL) in the presence of multiple objectives. The main objective is to design control policies that can optimize multiple objectives while treating each objective fairly. Toward this objective, we design a new fairness-induced preference-based reinforcement learning or FPbRL. The main idea of FPbRL is to learn vector reward functions associated with multiple objectives via new welfare-based preferences rather than reward-based preference in PbRL, coupled with policy learning via maximizing a generalized Gini welfare function. Finally, we provide experiment studies on three different environments to show that the proposed FPbRL approach can achieve both efficiency and equity for learning effective and fair policies.
    
[^7]: 心血管疾病预测的集成框架

    Ensemble Framework for Cardiovascular Disease Prediction. (arXiv:2306.09989v1 [cs.LG])

    [http://arxiv.org/abs/2306.09989](http://arxiv.org/abs/2306.09989)

    本文介绍了一个使用机器学习方法预测心血管疾病的集成框架，采用了IEEE Data Port 数据集，旨在早期准确诊断心脏问题，提高生命几率。

    

    心脏病是全球非传染性和缄默死亡的主要原因。心脏疾病或心血管疾病分为四种类型：冠状动脉心脏病、心力衰竭、先天性心脏病和心肌病。早期准确诊断心脏疾病对于避免进一步损伤并挽救患者的生命至关重要。因此，我们需要一个能够在心血管疾病变成危机之前预测其发生的系统。机器学习引起了医学科学研究人员的兴趣。针对心脏疾病预测，研究人员实施了各种机器学习方法和方法。在此工作中，我们采用了IEEE Data Port数据集，这是一种在线可用于心血管疾病个体的最大数据集之一。该数据集是由匈牙利，克里夫兰，长滩VA，瑞士和Statlog数据集组成，具有重要特征，如最大心率

    Heart disease is the major cause of non-communicable and silent death worldwide. Heart diseases or cardiovascular diseases are classified into four types: coronary heart disease, heart failure, congenital heart disease, and cardiomyopathy. It is vital to diagnose heart disease early and accurately in order to avoid further injury and save patients' lives. As a result, we need a system that can predict cardiovascular disease before it becomes a critical situation. Machine learning has piqued the interest of researchers in the field of medical sciences. For heart disease prediction, researchers implement a variety of machine learning methods and approaches. In this work, to the best of our knowledge, we have used the dataset from IEEE Data Port which is one of the online available largest datasets for cardiovascular diseases individuals. The dataset isa combination of Hungarian, Cleveland, Long Beach VA, Switzerland & Statlog datasets with important features such as Maximum Heart Rate Ac
    
[^8]: 采用深度卷积残差回归神经网络来转换海洋温度观测

    Transforming Observations of Ocean Temperature with a Deep Convolutional Residual Regressive Neural Network. (arXiv:2306.09987v1 [physics.ao-ph])

    [http://arxiv.org/abs/2306.09987](http://arxiv.org/abs/2306.09987)

    本文利用深度卷积残差回归神经网络来将海洋温度观测转换，提高了分辨率和覆盖云间隙，有效评估了模型性能。

    

    海表面温度 (SST) 是一项重要的气候变量，可以通过实测、遥感或混合模型方法进行测量。本文利用20世纪末和21世纪初的一些相关技术进步，庆祝了SST监测方面的进展。我们进一步发展了现有的水循环观测框架Flux to Flow（F2F），将AMSR-E和MODIS沉积物融合成一个更高分辨率的产品，以捕捉梯度并填补否则无法获得的云间隙。我们的神经网络架构限制在深度卷积残差回归神经网络上，使用2010年12个月SST的三组快照来衡量性能和成功度。平台的性能和这种方法的成功是通过均方根误差（RMSE）指标来评估的。

    Sea surface temperature (SST) is an essential climate variable that can be measured via ground truth, remote sensing, or hybrid model methodologies. Here, we celebrate SST surveillance progress via the application of a few relevant technological advances from the late 20th and early 21st century. We further develop our existing water cycle observation framework, Flux to Flow (F2F), to fuse AMSR-E and MODIS into a higher resolution product with the goal of capturing gradients and filling cloud gaps that are otherwise unavailable. Our neural network architecture is constrained to a deep convolutional residual regressive neural network. We utilize three snapshots of twelve monthly SST measurements in 2010 as measured by the passive microwave radiometer AMSR-E, the visible and infrared monitoring MODIS instrument, and the in situ Argo dataset ISAS. The performance of the platform and success of this approach is evaluated using the root mean squared error (RMSE) metric. We determine that th
    
[^9]: 用一致性检查评估超人模型

    Evaluating Superhuman Models with Consistency Checks. (arXiv:2306.09983v1 [cs.LG])

    [http://arxiv.org/abs/2306.09983](http://arxiv.org/abs/2306.09983)

    本文提出了一个用一致性检查评估超人模型的框架，可以发现决策制定中的逻辑不一致性，即使对于超人模型的决策正确性可能是不可能评估的情况。

    

    如果机器学习模型在各种推理或决策任务上实现了超人能力，那么我们该如何评估这些模型，考虑到人类代理会产生偏差? 在本文中，我们提出了一个用一致性检查评估超人模型的框架。我们的前提是，虽然评估超人决策的正确性可能是不可能的，但是如果模型的决策未能满足某些逻辑上、可解释的规则，我们仍然可以发现错误。我们将我们的框架实现在三个任务上，这些任务的决策正确性由于超人模型能力或其他缺乏基本事实而难以评估：评估国际象棋局面、预测未来事件和作出法律判断。我们表明，无论模型在这些任务上的表现如何(可能是超人的)，我们都能发现决策制定中的逻辑不一致性。例如：国际象棋引擎给出对局中棋子相对估值的不同排列。

    If machine learning models were to achieve superhuman abilities at various reasoning or decision-making tasks, how would we go about evaluating such models, given that humans would necessarily be poor proxies for ground truth? In this paper, we propose a framework for evaluating superhuman models via consistency checks. Our premise is that while the correctness of superhuman decisions may be impossible to evaluate, we can still surface mistakes if the model's decisions fail to satisfy certain logical, human-interpretable rules. We instantiate our framework on three tasks where correctness of decisions is hard to evaluate due to either superhuman model abilities, or to otherwise missing ground truth: evaluating chess positions, forecasting future events, and making legal judgments. We show that regardless of a model's (possibly superhuman) performance on these tasks, we can discover logical inconsistencies in decision making. For example: a chess engine assigning opposing valuations to 
    
[^10]: 论文标题：在强化学习中创建多级技能层次结构。

    Creating Multi-Level Skill Hierarchies in Reinforcement Learning. (arXiv:2306.09980v1 [cs.LG])

    [http://arxiv.org/abs/2306.09980](http://arxiv.org/abs/2306.09980)

    本文提出了一种基于代理人与环境交互的图形结构的答案，使用分层图划分产生具有多个抽象层次的技能层次结构。技能能将代理人移动到状态空间中互相连接紧密但相互连接较弱的区域，有效提高了强化学习的效率。

    

    摘要：什么样的技能层次结构对于自主代理人是有用的？我们提出了一种基于代理人与环境交互的图形结构的答案。我们的方法使用分层图划分来揭示图在不同时间尺度上的结构，从而产生具有多个抽象层次的技能层次结构。在层次结构的每个层次上，技能将代理人移动到状态空间中互相连接紧密但相互连接较弱的区域。我们在强化学习的广泛领域中展示了所提出的技能层次结构的效用。

    What is a useful skill hierarchy for an autonomous agent? We propose an answer based on the graphical structure of an agent's interaction with its environment. Our approach uses hierarchical graph partitioning to expose the structure of the graph at varying timescales, producing a skill hierarchy with multiple levels of abstraction. At each level of the hierarchy, skills move the agent between regions of the state space that are well connected within themselves but weakly connected to each other. We illustrate the utility of the proposed skill hierarchy in a wide variety of domains in the context of reinforcement learning.
    
[^11]: 带有最优性保证的对抗鲁棒聚类

    Adversarially robust clustering with optimality guarantees. (arXiv:2306.09977v1 [math.ST])

    [http://arxiv.org/abs/2306.09977](http://arxiv.org/abs/2306.09977)

    本文提出了一种简单的算法，即使在存在对抗性的异常值的情况下，也能获得最优的错标率。在没有异常值的情况下，该算法能够实现与洛伊德算法类似的理论保证.

    

    我们考虑对来自亚高斯混合的数据点进行聚类的问题。现有的可证明达到最优错标率的方法，如洛伊德算法，通常容易受到异常值的影响。相反，似乎对对抗性扰动具有鲁棒性的聚类方法不知道是否满足最优的统计保证。我们提出了一种简单的算法，即使允许出现对抗性的异常值，也能获得最优的错标率。当满足弱初始化条件时，我们的算法在常数次迭代中实现最优误差率。在没有异常值的情况下，在固定维度上，我们的理论保证与洛伊德算法类似。在各种模拟数据集上进行了广泛的实验，以支持我们的方法的理论保证。

    We consider the problem of clustering data points coming from sub-Gaussian mixtures. Existing methods that provably achieve the optimal mislabeling error, such as the Lloyd algorithm, are usually vulnerable to outliers. In contrast, clustering methods seemingly robust to adversarial perturbations are not known to satisfy the optimal statistical guarantees. We propose a simple algorithm that obtains the optimal mislabeling rate even when we allow adversarial outliers to be present. Our algorithm achieves the optimal error rate in constant iterations when a weak initialization condition is satisfied. In the absence of outliers, in fixed dimensions, our theoretical guarantees are similar to that of the Lloyd algorithm. Extensive experiments on various simulated data sets are conducted to support the theoretical guarantees of our method.
    
[^12]: 通过选择性神经元分裂增强QNN的容错性

    Enhancing Fault Resilience of QNNs by Selective Neuron Splitting. (arXiv:2306.09973v1 [cs.LG])

    [http://arxiv.org/abs/2306.09973](http://arxiv.org/abs/2306.09973)

    本文提出一种选择性神经元分裂方法，增强QNN的容错性，可以在加速器中设计轻量级纠错单元，相对于选择性三重模块冗余具有更小的开销，同时实现了类似的故障容错水平。

    

    深度神经网络（DNN）的出色性能导致它们在人类生活的各个方面得到应用。安全关键的应用程序也不例外，并对DNN提出了严格的可靠性要求。Quantized Neural Networks（QNN）已经出现，以应对DNN加速器的复杂性，然而，它们更容易出现可靠性问题。本文将最近的分析容错评估方法用于QNN，基于神经元易损因子（NVF）识别关键神经元。然后提出了一种新方法来分裂关键神经元，从而在加速器中设计轻量级纠错单元（LCU）而不需要重新设计其计算部分。该方法通过不同的QNN和数据集的实验进行了验证。结果表明，相对于选择性三重模块冗余（TMR），所提出的故障修正方法具有两倍小的开销，同时实现了类似的故障容错水平。

    The superior performance of Deep Neural Networks (DNNs) has led to their application in various aspects of human life. Safety-critical applications are no exception and impose rigorous reliability requirements on DNNs. Quantized Neural Networks (QNNs) have emerged to tackle the complexity of DNN accelerators, however, they are more prone to reliability issues.  In this paper, a recent analytical resilience assessment method is adapted for QNNs to identify critical neurons based on a Neuron Vulnerability Factor (NVF). Thereafter, a novel method for splitting the critical neurons is proposed that enables the design of a Lightweight Correction Unit (LCU) in the accelerator without redesigning its computational part.  The method is validated by experiments on different QNNs and datasets. The results demonstrate that the proposed method for correcting the faults has a twice smaller overhead than a selective Triple Modular Redundancy (TMR) while achieving a similar level of fault resiliency.
    
[^13]: HePCo：用于连续联邦学习的无数据异构提示合并方法

    HePCo: Data-Free Heterogeneous Prompt Consolidation for Continual Federated Learning. (arXiv:2306.09970v1 [cs.CV])

    [http://arxiv.org/abs/2306.09970](http://arxiv.org/abs/2306.09970)

    本文提出了一种名为HePCo的轻量级提示合并算法，解决了在连续联邦学习中的数据异构和遗忘问题，并在不共享或存储任何数据的情况下最小化了通信开销。在真实数据集和合成数据集上实现了最先进的结果，并且保持了数据隐私。

    

    本文研究了连续联邦学习的重要但鲜为人知的问题。在这种情况下，服务器与一组客户端通信，以逐步学习新的概念，同时不共享或存储任何数据。由于来自连续和联邦学习角度的挑战，此问题的复杂性受到了加剧。本文尝试在不需要访问任何存储数据的情况下解决遗忘和异构问题，同时最小化开销。我们通过采用一种基于提示的方法并提出一种名为HePCo的新颖轻量级提示合并算法，实现了此目标。我们的方法在真实数据集和合成数据集上均能取得最先进的结果并保持低通信开销，同时不影响数据隐私。

    In this paper, we focus on the important yet understudied problem of Continual Federated Learning (CFL), where a server communicates with a set of clients to incrementally learn new concepts over time without sharing or storing any data. The complexity of this problem is compounded by challenges from both the Continual and Federated Learning perspectives. Specifically, models trained in a CFL setup suffer from catastrophic forgetting which is exacerbated by data heterogeneity across clients. Existing attempts at this problem tend to impose large overheads on clients and communication channels or require access to stored data which renders them unsuitable for real-world use due to privacy. In this paper, we attempt to tackle forgetting and heterogeneity while minimizing overhead costs and without requiring access to any stored data. We achieve this by leveraging a prompting based approach (such that only prompts and classifier heads have to be communicated) and proposing a novel and lig
    
[^14]: 学习的进化论：从自然选择到强化学习

    The Evolution theory of Learning: From Natural Selection to Reinforcement Learning. (arXiv:2306.09961v1 [cs.NE])

    [http://arxiv.org/abs/2306.09961](http://arxiv.org/abs/2306.09961)

    本文探讨了强化学习和进化这两个领域之间的联系以及可能的启示，揭示了强化学习原则能够提高我们对进化和进化系统中反馈作用的理解。

    

    进化是塑造我们居住的生物世界的基本过程，强化学习是人工智能中用于开发可以从环境中学习的智能代理的强有力工具。近年来，研究人员探索了这两个看似独立领域之间的联系，并发现了令人信服的证据表明它们比以前想象的更密切相关。本文检验了这些联系及其影响，强调了强化学习原则提升我们对于进化和反馈在进化系统中作用的理解的潜力。

    Evolution is a fundamental process that shapes the biological world we inhabit, and reinforcement learning is a powerful tool used in artificial intelligence to develop intelligent agents that learn from their environment. In recent years, researchers have explored the connections between these two seemingly distinct fields, and have found compelling evidence that they are more closely related than previously thought. This paper examines these connections and their implications, highlighting the potential for reinforcement learning principles to enhance our understanding of evolution and the role of feedback in evolutionary systems.
    
[^15]: 使用合页损失在噪声数据上训练浅层ReLU网络：我们何时过度拟合且其是否良性？

    Training shallow ReLU networks on noisy data using hinge loss: when do we overfit and is it benign?. (arXiv:2306.09955v1 [cs.LG])

    [http://arxiv.org/abs/2306.09955](http://arxiv.org/abs/2306.09955)

    本论文研究了二层ReLU网络在使用梯度下降和合页损失处理噪声数据进行二分类中的良性过拟合，通过对干净数据余量的条件的确定，得出了三种不同的训练结果，能够在训练过程中对神经元动态变化做出精细描述，并发现了两个不同的训练阶段。

    

    我们研究了使用梯度下降和合页损失在噪声数据上训练的二层ReLU网络在二分类中的良性过拟合现象。我们特别考虑了线性可分数据，其中相对较小比例的标签被损坏或翻转。我们确定了干净数据余量的条件，产生了三种不同的训练结果：良性过拟合，在这种情况下将达到零损失，并且具有很高的概率测试数据被正确分类；过拟合，在这种情况下将达到零损失，但测试数据被错误分类的概率受到常数下限的约束；以及不过拟合，在这种情况下干净的点可以达到零损失，并且具有很高的概率测试数据被正确分类，但是不干净的点无法做出同样的预测。我们的分析提供了对神经元在训练过程中动态变化的一种精细描述，并揭示了两个不同的阶段：在第一个阶段中，干净点接近达到零损失，在第二个阶段中，干净点会振荡。

    We study benign overfitting in two-layer ReLU networks trained using gradient descent and hinge loss on noisy data for binary classification. In particular, we consider linearly separable data for which a relatively small proportion of labels are corrupted or flipped. We identify conditions on the margin of the clean data that give rise to three distinct training outcomes: benign overfitting, in which zero loss is achieved and with high probability test data is classified correctly; overfitting, in which zero loss is achieved but test data is misclassified with probability lower bounded by a constant; and non-overfitting, in which clean points, but not corrupt points, achieve zero loss and again with high probability test data is classified correctly. Our analysis provides a fine-grained description of the dynamics of neurons throughout training and reveals two distinct phases: in the first phase clean points achieve close to zero loss, in the second phase clean points oscillate on the
    
[^16]: 不需要强鲁棒机器学习来管理对抗攻击风险

    You Don't Need Robust Machine Learning to Manage Adversarial Attack Risks. (arXiv:2306.09951v1 [cs.LG])

    [http://arxiv.org/abs/2306.09951](http://arxiv.org/abs/2306.09951)

    本文对现代机器学习中的鲁棒性问题和对抗攻击进行了调查，发现不必要采用高成本的强鲁棒机器学习，可以通过其他设计选择来缓解对抗攻击的风险。

    

    现代机器学习（ML）模型的鲁棒性已成为社区内日益关注的问题。能够通过对输入进行貌似无关的更改来破坏模型，从而导致错误预测的能力令人震惊，而我们在构建具有鲁棒性的模型方面的成效也不容乐观。现有研究取得了一定的进展，但当前的缓解措施带来了很高的成本，同时也降低了模型的准确性。然而，当存在其他设计选择可以避免这种风险时，这样的权衡可能并不必要。在本调查中，我们通过眼光关注实践中如何缓解这些攻击，生产部署的风险以及管理这些风险。在此过程中，我们阐明了许多AML威胁不足以证明这种成本和权衡的必要性。

    The robustness of modern machine learning (ML) models has become an increasing concern within the community. The ability to subvert a model into making errant predictions using seemingly inconsequential changes to input is startling, as is our lack of success in building models robust to this concern. Existing research shows progress, but current mitigations come with a high cost and simultaneously reduce the model's accuracy. However, such trade-offs may not be necessary when other design choices could subvert the risk. In this survey we review the current literature on attacks and their real-world occurrences, or limited evidence thereof, to critically evaluate the real-world risks of adversarial machine learning (AML) for the average entity. This is done with an eye toward how one would then mitigate these attacks in practice, the risks for production deployment, and how those risks could be managed. In doing so we elucidate that many AML threats do not warrant the cost and trade-of
    
[^17]: 针对良好聚类的近最优分层聚类算法

    Nearly-Optimal Hierarchical Clustering for Well-Clustered Graphs. (arXiv:2306.09950v1 [cs.DS])

    [http://arxiv.org/abs/2306.09950](http://arxiv.org/abs/2306.09950)

    本文提出了两种高效分层聚类算法，针对任何具有清晰聚类结构的输入图，其能在近线性时间内生成一个关于Dasgupta代价函数的O（1）近似的HC树，性能优于先前技术。

    

    本文提出了两种针对Dasgupta代价函数的高效分层聚类算法。对于任何具有清晰聚类结构的输入图G，我们设计的算法在G的输入规模的近线性时间内运行，并返回一个关于Dasgupta代价函数的O（1）近似的HC树。我们将我们的算法与先前的最先进技术在合成和实际数据集上的性能进行比较，并表明我们设计的算法在更短的运行时间内产生可比或更好的HC树。

    This paper presents two efficient hierarchical clustering (HC) algorithms with respect to Dasgupta's cost function. For any input graph $G$ with a clear cluster-structure, our designed algorithms run in nearly-linear time in the input size of $G$, and return an $O(1)$-approximate HC tree with respect to Dasgupta's cost function. We compare the performance of our algorithm against the previous state-of-the-art on synthetic and real-world datasets and show that our designed algorithm produces comparable or better HC trees with much lower running time.
    
[^18]: 基于车辆出现的停车位检测

    Vehicle Occurrence-based Parking Space Detection. (arXiv:2306.09940v1 [cs.CV])

    [http://arxiv.org/abs/2306.09940](http://arxiv.org/abs/2306.09940)

    本文提出了一种利用车辆出现信息生成热力图的自动检测停车位方法，并在PKLot和CNRPark-EXT数据集上取得了较高的准确率。

    

    智能停车解决方案利用传感器、相机和数据分析提高停车效率、减少交通拥堵。计算机视觉方法近年来广泛用于停车场管理问题的解决，但大多数作品假定停车位是手动标注的，影响部署的成本和可行性。为了填补这一空白，本研究提出了一种自动检测停车位的方法，该方法接收一个停车场图像序列，并返回一个标识检测到的停车位的坐标列表。所提出的方法采用实例分割来识别汽车，并使用车辆出现来生成停车位的热力图。来自PKLot和CNRPark-EXT停车场数据集的12个不同子集的结果表明，该方法实现了最高95.60％的AP25分数和最高79.90％的AP50分数。

    Smart-parking solutions use sensors, cameras, and data analysis to improve parking efficiency and reduce traffic congestion. Computer vision-based methods have been used extensively in recent years to tackle the problem of parking lot management, but most of the works assume that the parking spots are manually labeled, impacting the cost and feasibility of deployment. To fill this gap, this work presents an automatic parking space detection method, which receives a sequence of images of a parking lot and returns a list of coordinates identifying the detected parking spaces. The proposed method employs instance segmentation to identify cars and, using vehicle occurrence, generate a heat map of parking spaces. The results using twelve different subsets from the PKLot and CNRPark-EXT parking lot datasets show that the method achieved an AP25 score up to 95.60\% and AP50 score up to 79.90\%.
    
[^19]: 通过解离规范优化实现更好的卷积神经网络训练

    Towards Better Orthogonality Regularization with Disentangled Norm in Training Deep CNNs. (arXiv:2306.09939v1 [cs.CV])

    [http://arxiv.org/abs/2306.09939](http://arxiv.org/abs/2306.09939)

    本文提出一种新的通过解离规范优化实现更好的滤波器正交性的方法，在实现滤波器之间的严格正交性原则下，所采用的模型比以前的规范化方法在近正交性方面表现更好。

    

    为了防止深度卷积神经网络训练的不稳定和特征冗余，人们开发了正交规范化。在现有的提议中，核正交规范化通过最小化由卷积过滤器形成的格拉姆矩阵与正交矩阵之间的残差来强制实现正交性。本文提出了一种新的度量方法来实现更好的滤波器正交性，该方法从残差中解离出对角线和相关信息。在实现滤波器之间的严格正交性原则下，所采用的模型比以前的规范化方法在近正交性方面表现更好。此外，我们观察到改进后的严格滤波器正交性在相对较浅的模型中的优点，但随着模型深度的增加，采用严格核正交性的模型的性能提升急剧下降。基于观察到的严格核正交性和不断增加的模型容Capacity中的潜在冲突，我们提出了一个新的正交规范化方法。

    Orthogonality regularization has been developed to prevent deep CNNs from training instability and feature redundancy. Among existing proposals, kernel orthogonality regularization enforces orthogonality by minimizing the residual between the Gram matrix formed by convolutional filters and the orthogonality matrix.  We propose a novel measure for achieving better orthogonality among filters, which disentangles diagonal and correlation information from the residual. The model equipped with the measure under the principle of imposing strict orthogonality between filters surpasses previous regularization methods in near-orthogonality. Moreover, we observe the benefits of improved strict filter orthogonality in relatively shallow models, but as model depth increases, the performance gains in models employing strict kernel orthogonality decrease sharply.  Furthermore, based on the observation of the potential conflict between strict kernel orthogonality and growing model capacity, we propos
    
[^20]: 车辆图像生成的拖曳引导扩散模型

    Drag-guided diffusion models for vehicle image generation. (arXiv:2306.09935v1 [cs.LG])

    [http://arxiv.org/abs/2306.09935](http://arxiv.org/abs/2306.09935)

    本文提出了一种基于物理的引导方法，在稳定扩散的基础上添加拖引导，生成能够同时最小化车辆预测阻力系数的图像。

    

    在网络规模训练的去噪扩散模型已经彻底改变了图像生成的方式。将这些工具应用于工程设计是一种有趣的可能性，但目前还不能解析并强制执行具体的工程约束条件。本文通过提出基于物理的引导，实现了在生成过程中优化性能度量（由代理模型预测），向这个目标迈进了一步。作为概念验证，在稳定扩散中添加拖引导，使该工具能够生成新车辆的图像，同时最小化它们的预测阻力系数。

    Denoising diffusion models trained at web-scale have revolutionized image generation. The application of these tools to engineering design is an intriguing possibility, but is currently limited by their inability to parse and enforce concrete engineering constraints. In this paper, we take a step towards this goal by proposing physics-based guidance, which enables optimization of a performance metric (as predicted by a surrogate model) during the generation process. As a proof-of-concept, we add drag guidance to Stable Diffusion, which allows this tool to generate images of novel vehicles while simultaneously minimizing their predicted drag coefficients.
    
[^21]: 基于元启发式机器学习的移动应用开发中能耗预测方法

    A Metaheuristic-based Machine Learning Approach for Energy Prediction in Mobile App Development. (arXiv:2306.09931v1 [cs.NE])

    [http://arxiv.org/abs/2306.09931](http://arxiv.org/abs/2306.09931)

    本文提出了一种基于元启发式机器学习的直方图梯度提升分类机器（HGBC），用于移动应用中能耗的预测，并解决了特征选择和超参数调优两个问题。

    

    能耗在移动应用开发和使用中扮演着重要的角色，是用户购买智能手机时考虑的最重要因素之一。考虑到可持续发展，必须找到能够降低移动设备能耗的方法，因为全球数十亿部智能手机的广泛使用对环境有着重要影响。本文提出了一种基于元启发式的直方图梯度提升分类机器（HGBC）方法，用于移动应用中能耗的预测。我们的元启发式方法解决了两个问题。首先，它在性能未有显著变化的情况下发现了多余和无关特征；其次，它执行了超参数调优。

    Energy consumption plays a vital role in mobile App development for developers and end-users, and it is considered one of the most crucial factors for purchasing a smartphone. In addition, in terms of sustainability, it is essential to find methods to reduce the energy consumption of mobile devices since the extensive use of billions of smartphones worldwide significantly impacts the environment. Despite the existence of several energy-efficient programming practices in Android, the leading mobile ecosystem, machine learning-based energy prediction algorithms for mobile App development have yet to be reported. Therefore, this paper proposes a histogram-based gradient boosting classification machine (HGBC), boosted by a metaheuristic approach, for energy prediction in mobile App development. Our metaheuristic approach is responsible for two issues. First, it finds redundant and irrelevant features without any noticeable change in performance. Second, it performs a hyper-parameter tuning
    
[^22]: 是友还是敌？探讨大型语言模型对科学系统的影响。

    Friend or Foe? Exploring the Implications of Large Language Models on the Science System. (arXiv:2306.09928v1 [cs.CY])

    [http://arxiv.org/abs/2306.09928](http://arxiv.org/abs/2306.09928)

    LLMs有潜力在行政、创造性和分析任务方面对科学做出变革，但需要通过积极的监管和科学教育来解决与偏见、错误信息和质量保证有关的风险。

    

    OpenAI开发的ChatGPT的出现引起了广泛的讨论，特别是对于它对科学和高等教育的潜在影响。虽然对教育的影响一直是主要关注的焦点，但对大型语言模型（LLMs）和基于LLMs的聊天机器人对科学和科学实践的影响的实证研究有限。为了进一步调查这个问题，我们进行了一个Delphi研究，涉及72位专门从事研究和人工智能的专家。该研究重点关注LLMs的应用和限制，以及它们对科学系统、伦理和法律考虑因素的影响，以及其有效使用所需的能力。我们的发现突出了LLMs在科学中的变革潜力，特别是在行政、创造性和分析任务方面。然而，与偏见、错误信息和质量保证有关的风险需要通过积极的监管和科学教育加以解决。这项研究为有关生成性人工智能在科学和高等教育中的影响的知情讨论做出了贡献。

    The advent of ChatGPT by OpenAI has prompted extensive discourse on its potential implications for science and higher education. While the impact on education has been a primary focus, there is limited empirical research on the effects of large language models (LLMs) and LLM-based chatbots on science and scientific practice. To investigate this further, we conducted a Delphi study involving 72 experts specialising in research and AI. The study focused on applications and limitations of LLMs, their effects on the science system, ethical and legal considerations, and the required competencies for their effective use. Our findings highlight the transformative potential of LLMs in science, particularly in administrative, creative, and analytical tasks. However, risks related to bias, misinformation, and quality assurance need to be addressed through proactive regulation and science education. This research contributes to informed discussions on the impact of generative AI in science and he
    
[^23]: 训练好的Transformer在上下文中学习线性模型

    Trained Transformers Learn Linear Models In-Context. (arXiv:2306.09927v1 [stat.ML])

    [http://arxiv.org/abs/2306.09927](http://arxiv.org/abs/2306.09927)

    本文研究了Transformer在具有单层线性自注意层的线性回归任务上通过梯度流进行训练的ICL机制，揭示了梯度流具有找到目标函数全局最小值的能力。

    

    基于注意力的神经网络，例如Transformers，在上下文学习（ICL）方面表现出了非凡的能力：给定一个来自未见过的任务的短语序列的提示，它们可以制定相关的每个令牌和下一个令牌的预测，而不需要任何参数更新。通过将标记的训练数据和未标记的测试数据序列嵌入到提示中，这使得Transformer表现得像有监督学习算法。事实上，最近的工作表明，在随机实例上训练Transformer体系结构的线性回归问题时，这些模型的预测会模仿普通最小二乘法的预测。

    Attention-based neural networks such as transformers have demonstrated a remarkable ability to exhibit in-context learning (ICL): Given a short prompt sequence of tokens from an unseen task, they can formulate relevant per-token and next-token predictions without any parameter updates. By embedding a sequence of labeled training data and unlabeled test data as a prompt, this allows for transformers to behave like supervised learning algorithms. Indeed, recent work has shown that when training transformer architectures over random instances of linear regression problems, these models' predictions mimic those of ordinary least squares.  Towards understanding the mechanisms underlying this phenomenon, we investigate the dynamics of ICL in transformers with a single linear self-attention layer trained by gradient flow on linear regression tasks. We show that despite non-convexity, gradient flow with a suitable random initialization finds a global minimum of the objective function. At this 
    
[^24]: 学习总结和回答与虚拟机器人过去动作相关的问题

    Learning to Summarize and Answer Questions about a Virtual Robot's Past Actions. (arXiv:2306.09922v1 [cs.RO])

    [http://arxiv.org/abs/2306.09922](http://arxiv.org/abs/2306.09922)

    本论文介绍了一种学习总结和回答机器人动作历史的方法，能够通过一种语言模型同时完成总结和回答任务，并提供了自动生成问题和答案的方法来进行训练。此方法能够实现从问题回答中学习对象表示的零-shot转移。

    

    当机器人执行长序列的动作时，用户需要轻松、可靠地了解它们所做的事情。因此，我们演示了使用自然语言学习总结和回答关于机器人代理过去动作的问题的任务。一个核心为大型语言模型的单一系统被训练用于总结和回答关于虚拟机器人的自我中心视频帧和问题提示的动作序列。为了实现问题回答的训练，我们开发了一种方法来自动生成关于对象、动作和在虚拟环境中机器人动作序列期间动作发生的时间顺序的英文问题和答案。将一个模型用于总结和回答问题使得从问题回答中学习的对象表示的零-shot转移能够提高动作总结的能力，包括在训练中未见过的对象。

    When robots perform long action sequences, users will want to easily and reliably find out what they have done. We therefore demonstrate the task of learning to summarize and answer questions about a robot agent's past actions using natural language alone. A single system with a large language model at its core is trained to both summarize and answer questions about action sequences given ego-centric video frames of a virtual robot and a question prompt. To enable training of question answering, we develop a method to automatically generate English-language questions and answers about objects, actions, and the temporal order in which actions occurred during episodes of robot action in the virtual environment. Training one model to both summarize and answer questions enables zero-shot transfer of representations of objects learned through question answering to improved action summarization. % involving objects not seen in training to summarize.
    
[^25]: 水产养殖系统中的饲料控制和水质监测：机遇和挑战

    Feeding control and water quality monitoring in aquaculture systems: Opportunities and challenges. (arXiv:2306.09920v1 [eess.SY])

    [http://arxiv.org/abs/2306.09920](http://arxiv.org/abs/2306.09920)

    本文讨论了控制饲料和监测水质对于水产养殖系统中平衡鱼类生产力和健康至关重要的问题，并强调了开发可靠的控制策略的必要性。

    

    水产养殖系统可以从先进的控制策略的最新发展中受益，以减少运营成本和鱼类损失，增加生产效率，从而提高鱼类福利和健康。监测水质和控制饲料是平衡鱼类生产力并塑造鱼类生长过程的基本要素。目前，大多数鱼类饲养过程在不同阶段手动进行，并依赖于耗时和具有挑战性的人工区分。饲料控制方法通过饲料转化率影响鱼类生长和繁殖，因此控制这些饲料参数对于增强鱼类福利和最小化一般渔业成本至关重要。环境因素（如高氨浓度和pH）的高浓度会影响水质和鱼类存活。因此，有必要开发控制策略来确定在水产养殖系统中最佳、高效和可靠的饲料过程和水质监测。

    Aquaculture systems can benefit from the recent development of advanced control strategies to reduce operating costs and fish loss and increase growth production efficiency, resulting in fish welfare and health. Monitoring the water quality and controlling feeding are fundamental elements of balancing fish productivity and shaping the fish growth process. Currently, most fish-feeding processes are conducted manually in different phases and rely on time-consuming and challenging artificial discrimination. The feeding control approach influences fish growth and breeding through the feed conversion rate; hence, controlling these feeding parameters is crucial for enhancing fish welfare and minimizing general fishery costs. The high concentration of environmental factors, such as a high ammonia concentration and pH, affect the water quality and fish survival. Therefore, there is a critical need to develop control strategies to determine optimal, efficient, and reliable feeding processes and
    
[^26]: 走向量子联邦学习

    Towards Quantum Federated Learning. (arXiv:2306.09912v1 [cs.LG])

    [http://arxiv.org/abs/2306.09912](http://arxiv.org/abs/2306.09912)

    量子联邦学习通过将量子计算和联邦学习原理相结合，旨在利用量子技术增强学习过程中的隐私、安全和效率，并通过独特的分类法分类总结了这一快速发展领域的技术特点和未来研究方向。

    

    量子联邦学习是一个新兴的交叉学科领域，将量子计算和联邦学习的原理相结合，旨在利用量子技术增强学习过程中的隐私、安全和效率。目前尚无关于该交叉学科领域的全面调查。本文对量子联邦学习进行了全面细致的探讨。我们旨在提供对量子联邦学习的原理、技术以及新兴应用的全面理解。我们讨论了这一快速发展领域的现状，确定了整合这些技术所面临的挑战和机遇，并概述了未来的方向和开放性研究问题。我们提出了一种独特的分类法，将量子联邦学习技术按其特征和所采用的量子技术分类。随着量子联邦学习领域的不断发展，我们可以预计将在各个行业实现更多的突破和应用。

    Quantum Federated Learning (QFL) is an emerging interdisciplinary field that merges the principles of Quantum Computing (QC) and Federated Learning (FL), with the goal of leveraging quantum technologies to enhance privacy, security, and efficiency in the learning process. Currently, there is no comprehensive survey for this interdisciplinary field. This review offers a thorough, holistic examination of QFL. We aim to provide a comprehensive understanding of the principles, techniques, and emerging applications of QFL. We discuss the current state of research in this rapidly evolving field, identify challenges and opportunities associated with integrating these technologies, and outline future directions and open research questions. We propose a unique taxonomy of QFL techniques, categorized according to their characteristics and the quantum techniques employed. As the field of QFL continues to progress, we can anticipate further breakthroughs and applications across various industries,
    
[^27]: LabelBench：基于综合框架的标签高效学习基准评估

    LabelBench: A Comprehensive Framework for Benchmarking Label-Efficient Learning. (arXiv:2306.09910v1 [cs.LG])

    [http://arxiv.org/abs/2306.09910](http://arxiv.org/abs/2306.09910)

    本论文介绍了一个新的综合性标签高效学习基准评估框架LabelBench，并通过引入一种新的与半监督学习相结合的主动学习方法的基准测试，证明了在相对较少的标记示例下实现更好的标签效率。

    

    标记数据是现代机器学习应用程序的关键，但获取标记可能很昂贵。为了减缓这一成本，机器学习方法（如迁移学习、半监督学习和主动学习）旨在实现标签高效性：从相对较少的标记示例中实现高预测性能。虽然在实践中获得最佳的标签效率通常需要这些技术的组合，但现有的基准评估框架并没有捕捉到所有这些技术的协同组合。本文通过引入LabelBench解决了这个缺陷，这是一个新的计算效率高的综合性框架，用于联合评估多个标签高效学习技术。作为LabelBench的一个应用，我们引入了一种与半监督学习一起使用的最新主动学习方法的新基准，用于微调预训练的视觉转换器。我们的基准证明了比先前报告的更好的标签效率。

    Labeled data are critical to modern machine learning applications, but obtaining labels can be expensive. To mitigate this cost, machine learning methods, such as transfer learning, semi-supervised learning and active learning, aim to be label-efficient: achieving high predictive performance from relatively few labeled examples. While obtaining the best label-efficiency in practice often requires combinations of these techniques, existing benchmark and evaluation frameworks do not capture a concerted combination of all such techniques. This paper addresses this deficiency by introducing LabelBench, a new computationally-efficient framework for joint evaluation of multiple label-efficient learning techniques. As an application of LabelBench, we introduce a novel benchmark of state-of-the-art active learning methods in combination with semi-supervised learning for fine-tuning pretrained vision transformers. Our benchmark demonstrates better label-efficiencies than previously reported in 
    
[^28]: 鸟鸣声的相关聚类

    Correlation Clustering of Bird Sounds. (arXiv:2306.09906v1 [cs.SD])

    [http://arxiv.org/abs/2306.09906](http://arxiv.org/abs/2306.09906)

    本文研究鸟声聚类问题，提出了一种通过学习训练集中记录对相关概率后，应用相关聚类于测试集进行聚类的方法，并比较了这种方法与测试集分类的准确性和相关性。同时，还探究了这种方法在应用于训练期间未听到的鸟类物种的记录以及分离鸟声和环境噪声方面的效果。

    

    鸟类声音分类是将任何声音记录与可以在记录中听到的鸟类物种相关联的任务。在这里，我们研究鸟声聚类，即决定任何一对声音记录是否可以听到相同的鸟类物种。我们首先从训练集中学习记录对按这种方式相关的概率，然后通过相关聚类来推断测试集的最大可能分区。我们解决以下问题：与测试集的分类相比，这种聚类的准确性如何？从分类获得的聚类如何与因此推断得出的聚类相关？在应用于训练期间未听到的鸟类物种的记录时，这种聚类的准确性如何？这种聚类在分离鸟鸣声和训练期间未听到的环境噪声方面有多有效？

    Bird sound classification is the task of relating any sound recording to those species of bird that can be heard in the recording. Here, we study bird sound clustering, the task of deciding for any pair of sound recordings whether the same species of bird can be heard in both. We address this problem by first learning, from a training set, probabilities of pairs of recordings being related in this way, and then inferring a maximally probable partition of a test set by correlation clustering. We address the following questions: How accurate is this clustering, compared to a classification of the test set? How do the clusters thus inferred relate to the clusters obtained by classification? How accurate is this clustering when applied to recordings of bird species not heard during training? How effective is this clustering in separating, from bird sounds, environmental noise not heard during training?
    
[^29]: 连续学习中基于记忆的方法的泛化性研究

    Studying Generalization on Memory-Based Methods in Continual Learning. (arXiv:2306.09890v1 [cs.LG])

    [http://arxiv.org/abs/2306.09890](http://arxiv.org/abs/2306.09890)

    本文研究了连续学习中基于记忆的方法的泛化性能，发现虽然这些方法可以帮助传统的内分布泛化，但它们可以通过学习虚假的特征和相关性来大大损害超出分布泛化，尤其是在线性分类器中。

    

    连续学习的目标之一是在一系列经验中不断学习新的概念，同时避免灾难性遗忘。为了减轻完全知识覆盖的问题，基于记忆的方法会存储一定比例的先前数据分布，在训练中使用。虽然这些方法产生了良好的结果，但很少有研究测试它们的超出分布泛化性能以及这些方法是否过度拟合重放记忆。在这项工作中，我们展示了尽管这些方法可以帮助传统的内分布泛化，但它们可以通过学习虚假的特征和相关性来大大损害超出分布泛化。使用控制环境，我们使用Synbol基准生成器（Lacoste等人，2020）展示了这种缺乏超出分布泛化主要出现在线性分类器中。

    One of the objectives of Continual Learning is to learn new concepts continually over a stream of experiences and at the same time avoid catastrophic forgetting. To mitigate complete knowledge overwriting, memory-based methods store a percentage of previous data distributions to be used during training. Although these methods produce good results, few studies have tested their out-of-distribution generalization properties, as well as whether these methods overfit the replay memory. In this work, we show that although these methods can help in traditional in-distribution generalization, they can strongly impair out-of-distribution generalization by learning spurious features and correlations. Using a controlled environment, the Synbol benchmark generator (Lacoste et al., 2020), we demonstrate that this lack of out-of-distribution generalization mainly occurs in the linear classifier.
    
[^30]: CANDID：深度爆发图像去噪的对齐算法

    CANDID: Correspondence AligNment for Deep-burst Image Denoising. (arXiv:2306.09887v1 [cs.CV])

    [http://arxiv.org/abs/2306.09887](http://arxiv.org/abs/2306.09887)

    本文提出了 CANDID，是一种针对深度爆发图像去噪的对齐算法，其中利用了光流的对应估计模块来对齐输入的所有图像，并通过建立的对应关系在预处理和原始图像中预测像素级可变滤波器核，从而实现最先进的结果。

    

    随着移动手机摄影和点拍相机的出现，深度爆发成像被广泛用于各种摄影效果，例如景深、超分辨率、运动去模糊和图像去噪。本文提出了一种包含基于光流的对应估计模块的解决方案，以解决深度爆发图像去噪问题。为了处理不同的噪声水平，对每个图像进行不同的预过滤设置。利用建立的对应关系，一个网络块可以预测每个图像的像素级可变滤波器核，以平滑原始和预处理的爆发图像，然后融合所有图像生成最终去噪输出。该流程通过组合爆发提供的所有信息，实现了最先进的结果。

    With the advent of mobile phone photography and point-and-shoot cameras, deep-burst imaging is widely used for a number of photographic effects such as depth of field, super-resolution, motion deblurring, and image denoising. In this work, we propose to solve the problem of deep-burst image denoising by including an optical flow-based correspondence estimation module which aligns all the input burst images with respect to a reference frame. In order to deal with varying noise levels the individual burst images are pre-filtered with different settings. Exploiting the established correspondences one network block predicts a pixel-wise spatially-varying filter kernel to smooth each image in the original and prefiltered bursts before fusing all images to generate the final denoised output. The resulting pipeline achieves state-of-the-art results by combining all available information provided by the burst.
    
[^31]: Jumanji: JAX中一套多样化可扩展的强化学习环境

    Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments in JAX. (arXiv:2306.09884v1 [cs.LG])

    [http://arxiv.org/abs/2306.09884](http://arxiv.org/abs/2306.09884)

    Jumanji是JAX中一套可扩展的强化学习系统，提供了一系列高度可定制的环境，具有快速、灵活、可扩展和模块化特点，利用硬件加速器赋能更有能力的代理人。

    

    开源强化学习环境在推动AI算法的发展方面起到了关键作用。现代强化学习研究需要模拟环境具备性能、可扩展性和模块化特点，以扩展其在更广泛的实际应用中的可用性。因此，我们提出了Jumanji，这是一套设计用于快速、灵活和可扩展的不同RL环境的强化学习系统。Jumanji提供了一系列的环境，专注于工业中经常遇到的组合问题，以及挑战性的一般决策任务。通过利用JAX和GPU、TPU等硬件加速器的效率，Jumanji能够迅速迭代研究思路和大规模实验，最终赋能更有能力的代理人。与现有的强化学习环境套件不同，Jumanji具有高度可定制性，允许用户根据其需求调整初始状态分布和问题复杂度。

    Open-source reinforcement learning (RL) environments have played a crucial role in driving progress in the development of AI algorithms. In modern RL research, there is a need for simulated environments that are performant, scalable, and modular to enable their utilization in a wider range of potential real-world applications. Therefore, we present Jumanji, a suite of diverse RL environments specifically designed to be fast, flexible, and scalable. Jumanji provides a suite of environments focusing on combinatorial problems frequently encountered in industry, as well as challenging general decision-making tasks. By leveraging the efficiency of JAX and hardware accelerators like GPUs and TPUs, Jumanji enables rapid iteration of research ideas and large-scale experimentation, ultimately empowering more capable agents. Unlike existing RL environment suites, Jumanji is highly customizable, allowing users to tailor the initial state distribution and problem complexity to their needs. Further
    
[^32]: 时空Tweedie模型在预测存在零膨胀和长尾旅行需求中的应用及不确定性量化

    Uncertainty Quantification via Spatial-Temporal Tweedie Model for Zero-inflated and Long-tail Travel Demand Prediction. (arXiv:2306.09882v1 [cs.LG])

    [http://arxiv.org/abs/2306.09882](http://arxiv.org/abs/2306.09882)

    本文提出了一种新型的时空Tweedie模型STTD，旨在解决高分辨率OD矩阵中稀疏和长尾特征的问题，并成功量化预测不确定性，具有很高的应用前景。

    

    传统的时空深度学习模型难以解决高分辨率OD矩阵中稀疏和长尾特征的问题，从而难以量化预测不确定性，而这对于交通管理至关重要。为了解决这些挑战，本文提出了一种新颖的方法：空间-Tweedie图神经网络（STTD）。STTD将Tweedie分布作为传统的“零膨胀”模型的有力替代品，并利用空间和时间嵌入来参数化旅行需求分布。我们使用真实世界的数据集进行评估，结果表明STTD在高分辨率场景下提供了准确的预测和精确的置信区间，具有优越性。

    crucial for transportation management. However, traditional spatial-temporal deep learning models grapple with addressing the sparse and long-tail characteristics in high-resolution O-D matrices and quantifying prediction uncertainty. This dilemma arises from the numerous zeros and over-dispersed demand patterns within these matrices, which challenge the Gaussian assumption inherent to deterministic deep learning models. To address these challenges, we propose a novel approach: the Spatial-Temporal Tweedie Graph Neural Network (STTD). The STTD introduces the Tweedie distribution as a compelling alternative to the traditional 'zero-inflated' model and leverages spatial and temporal embeddings to parameterize travel demand distributions. Our evaluations using real-world datasets highlight STTD's superiority in providing accurate predictions and precise confidence intervals, particularly in high-resolution scenarios.
    
[^33]: 可泛化的一次性绳索操作策略及其参数感知性。

    Generalizable One-shot Rope Manipulation with Parameter-Aware Policy. (arXiv:2306.09872v1 [cs.LG])

    [http://arxiv.org/abs/2306.09872](http://arxiv.org/abs/2306.09872)

    GenORM通过增加可变形绳索参数和使用各种可变形绳索的模拟训练操作策略，实现利用一次真实演示处理不同可形变绳索，从而节省演示时间和提高适用性。

    

    以绳索在运动过程中的固有不确定性为因素，以往绳索操作方法往往需要数百次真实演示来为每个绳索训练操作策略，即使是简单的“到达目标”任务，这限制了它们在我们不断变化的世界中的应用。为了解决这个问题，我们介绍了GenORM，一个框架，它可以让操作策略通过一次真实演示就可以处理不同可形变的绳索。我们通过在策略上增加可变形绳索参数并使用各种模拟可变形绳索来训练它，使策略能够根据不同的绳索参数调整行动。在推断时，GenORM通过最小化真实演示和模拟点云的网格密度差异来估计可变形绳索参数。通过可微分物理模拟器的帮助，我们仅需要一次演示数据就可以处理不同的绳索。

    Due to the inherent uncertainty in their deformability during motion, previous methods in rope manipulation often require hundreds of real-world demonstrations to train a manipulation policy for each rope, even for simple tasks such as rope goal reaching, which hinder their applications in our ever-changing world. To address this issue, we introduce GenORM, a framework that allows the manipulation policy to handle different deformable ropes with a single real-world demonstration. To achieve this, we augment the policy by conditioning it on deformable rope parameters and training it with a diverse range of simulated deformable ropes so that the policy can adjust actions based on different rope parameters. At the time of inference, given a new rope, GenORM estimates the deformable rope parameters by minimizing the disparity between the grid density of point clouds of real-world demonstrations and simulations. With the help of a differentiable physics simulator, we require only a single r
    
[^34]: 文本到图像扩散模型中的能量交叉注意力用于贝叶斯上下文更新

    Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models. (arXiv:2306.09869v1 [cs.CV])

    [http://arxiv.org/abs/2306.09869](http://arxiv.org/abs/2306.09869)

    本论文提出了能量交叉注意力的EBM框架，通过更新和转移上下文向量，隐式最小化能量函数的嵌套层次，优化文本到图像扩散模型的语义对齐问题，实现零样本组合生成。

    

    尽管文本到图像扩散模型在图像生成任务中表现出色，但最近的研究提出了一个问题，即生成的图像有时无法捕捉到文本提示的预期语义内容，这种现象通常被称为语义错位。为了解决这个问题，我们提出了一种新颖的基于能量的模型（EBM）框架。具体而言，我们首先在去噪自编码器的每个交叉注意力层中制定潜在图像表示和文本嵌入的EBM。然后，我们获得上下文向量的对数后验梯度，可以更新和转移到后续的交叉注意力层，从而隐式地最小化嵌套层次的能量函数。我们的潜在EBMs还允许零样本组合生成，即通过不同上下文的交叉注意力输出的线性组合。通过大量实验，我们证明了所提出的方法在处理各种图像生成任务方面非常有效，并可以显著降低文本提示和生成图像之间的语义错位现象。

    Despite the remarkable performance of text-to-image diffusion models in image generation tasks, recent studies have raised the issue that generated images sometimes cannot capture the intended semantic contents of the text prompts, which phenomenon is often called semantic misalignment. To address this, here we present a novel energy-based model (EBM) framework. Specifically, we first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder. Then, we obtain the gradient of the log posterior of context vectors, which can be updated and transferred to the subsequent cross-attention layer, thereby implicitly minimizing a nested hierarchy of energy functions. Our latent EBMs further allow zero-shot compositional generation as a linear combination of cross-attention outputs from different contexts. Using extensive experiments, we demonstrate that the proposed method is highly effective in handling various image generation 
    
[^35]: 神经网络微分方程求解器中中奖彩票的可转移性研究

    Transferability of Winning Lottery Tickets in Neural Network Differential Equation Solvers. (arXiv:2306.09863v1 [cs.LG])

    [http://arxiv.org/abs/2306.09863](http://arxiv.org/abs/2306.09863)

    本文研究了神经网络微分方程求解器中中奖彩票的可转移性，发现哈密顿神经网络系统之间具有可转移性，并用重整化群理论分析了这两个系统的普遍性。

    

    最近的研究表明，重整化群理论是通过迭代幅值剪枝来描述神经网络剪枝过程的有用框架。本文正式描述了RG理论与IMP之间的联系，并将之前关于彩票假设和弹性彩票假设的结果扩展到用于求解微分方程的哈密顿神经网络。我们找到了两个哈密顿神经网络的中奖彩票，并证明了这两个系统之间的可转移性，其中准确性取决于积分时间。然后使用RG的工具分析了这两个系统的普遍性。

    Recent work has shown that renormalisation group theory is a useful framework with which to describe the process of pruning neural networks via iterative magnitude pruning. This report formally describes the link between RG theory and IMP and extends previous results around the Lottery Ticket Hypothesis and Elastic Lottery Hypothesis to Hamiltonian Neural Networks for solving differential equations. We find lottery tickets for two Hamiltonian Neural Networks and demonstrate transferability between the two systems, with accuracy being dependent on integration times. The universality of the two systems is then analysed using tools from an RG perspective.
    
[^36]: DoubleAdapt：一种用于股票趋势预测的增量学习元学习方法

    DoubleAdapt: A Meta-learning Approach to Incremental Learning for Stock Trend Forecasting. (arXiv:2306.09862v1 [q-fin.ST])

    [http://arxiv.org/abs/2306.09862](http://arxiv.org/abs/2306.09862)

    DoubleAdapt是一个增量学习的方法，用于股票趋势预测。它利用元学习技术自动学习如何将股票数据适应到本地平稳分布空间中，从而有效地适应数据和模型，减轻分布漂移的影响。

    

    股票趋势预测是量化投资的基本任务之一，准确预测价格趋势是不可或缺的。作为一项在线服务，股票数据随时随地持续到达。使用最新数据对预测模型进行增量更新是实用而高效的，因为这些新数据可能揭示了未来股票市场中会重复出现的一些新模式。然而，由于分布漂移（即概念漂移）的挑战，股票趋势预测的增量学习仍然没有得到充分探索。随着股票市场动态演变，未来数据的分布可能会与增量数据稍微或显着地不同，从而阻碍增量更新的有效性。为了解决这一挑战，我们提出了一个利用两个适配器的端到端框架——DoubleAdapt，可以有效地适应数据和模型，以减轻分布漂移的影响。我们的关键洞察力是利用元学习技术自动学习如何将股票数据适应到本地平稳分布空间中。

    Stock trend forecasting is a fundamental task of quantitative investment where precise predictions of price trends are indispensable. As an online service, stock data continuously arrive over time. It is practical and efficient to incrementally update the forecast model with the latest data which may reveal some new patterns recurring in the future stock market. However, incremental learning for stock trend forecasting still remains under-explored due to the challenge of distribution shifts (a.k.a. concept drifts). With the stock market dynamically evolving, the distribution of future data can slightly or significantly differ from incremental data, hindering the effectiveness of incremental updates. To address this challenge, we propose DoubleAdapt, an end-to-end framework with two adapters, which can effectively adapt the data and the model to mitigate the effects of distribution shifts. Our key insight is to automatically learn how to adapt stock data into a locally stationary distri
    
[^37]: 实用的锐度感知优化算法不能全程向最优点收敛

    Practical Sharpness-Aware Minimization Cannot Converge All the Way to Optima. (arXiv:2306.09850v1 [cs.LG])

    [http://arxiv.org/abs/2306.09850](http://arxiv.org/abs/2306.09850)

    该研究揭示了实用的锐度感知优化算法在某些情况下不能够全程向最优点收敛。

    

    锐度感知优化(SAM)是一种优化器，它基于当前点$x_t$的梯度，在扰动$y_t=x_t+\rho\frac{\nabla f(x_t)}{\lVert\nabla f(x_t)\rVert}$处进行下降。现有研究证明了SAM对于平滑函数的收敛性，但是它们假设扰动的大小$\rho$逐渐衰减和/或在$y_t$中没有梯度归一化，这与实践不符。为了弥补这一差距，我们研究了具有实用配置（即常数$\rho$和$y_t$中的梯度归一化）的确定性/随机版本的SAM，并探讨了它们在具有（非）凸性假设的平滑函数上的收敛性质。令人惊讶的是，在许多情况下，我们发现SAM在收敛到全局最小值或稳定点方面具有有限的能力。对于平滑强凸函数，我们展示了确定性SAM具有严格的全局收敛率为$\tilde\Theta(\frac{1}{T^2})$，而随机SAM的收敛界则受到噪声水平降低的影响，这表明了平面目标表面的尖锐度和平缓性之间平衡的挑战。

    Sharpness-Aware Minimization (SAM) is an optimizer that takes a descent step based on the gradient at a perturbation $y_t = x_t + \rho \frac{\nabla f(x_t)}{\lVert \nabla f(x_t) \rVert}$ of the current point $x_t$. Existing studies prove convergence of SAM for smooth functions, but they do so by assuming decaying perturbation size $\rho$ and/or no gradient normalization in $y_t$, which is detached from practice. To address this gap, we study deterministic/stochastic versions of SAM with practical configurations (i.e., constant $\rho$ and gradient normalization in $y_t$) and explore their convergence properties on smooth functions with (non)convexity assumptions. Perhaps surprisingly, in many scenarios, we find out that SAM has limited capability to converge to global minima or stationary points. For smooth strongly convex functions, we show that while deterministic SAM enjoys tight global convergence rates of $\tilde \Theta(\frac{1}{T^2})$, the convergence bound of stochastic SAM suffer
    
[^38]: 神经网络的Wasserstein分布鲁棒性

    Wasserstein distributional robustness of neural networks. (arXiv:2306.09844v1 [cs.LG])

    [http://arxiv.org/abs/2306.09844](http://arxiv.org/abs/2306.09844)

    本研究探讨了基于Wasserstein分布鲁棒性的神经网络对抗攻击，提出了一组分布威胁模型，并使用一阶对抗攻击算法及其多步版本对其进行了评估。

    

    深度神经网络已知易受到对抗攻击(Adversarial Attacks, AA)的威胁。对于图像识别任务而言，这意味着对原始图像进行微小扰动就有可能导致其被错误分类。因此，设计对抗攻击以及对抗训练的方法是研究的焦点。本研究使用Wasserstein分布鲁棒性优化技术重新构思该问题，并利用最近的DRO敏感性分析的新见解进行了新的贡献。我们考虑一组分布威胁模型。与传统的点对点攻击不同的是，分布威胁模型允许攻击者以非均匀的方式扰动输入。我们将这些更一般的攻击与样本外性能和Knightian不确定性问题联系起来。为了评估神经网络的分布鲁棒性，我们提出了一种一阶AA算法及其多步版本。

    Deep neural networks are known to be vulnerable to adversarial attacks (AA). For an image recognition task, this means that a small perturbation of the original can result in the image being misclassified. Design of such attacks as well as methods of adversarial training against them are subject of intense research. We re-cast the problem using techniques of Wasserstein distributionally robust optimization (DRO) and obtain novel contributions leveraging recent insights from DRO sensitivity analysis. We consider a set of distributional threat models. Unlike the traditional pointwise attacks, which assume a uniform bound on perturbation of each input data point, distributional threat models allow attackers to perturb inputs in a non-uniform way. We link these more general attacks with questions of out-of-sample performance and Knightian uncertainty. To evaluate the distributional robustness of neural networks, we propose a first-order AA algorithm and its multi-step version. Our attack a
    
[^39]: 存在偏差时基于多个排名的子集选择：多赢家投票评分函数的公平限制的有效性

    Subset Selection Based On Multiple Rankings in the Presence of Bias: Effectiveness of Fairness Constraints for Multiwinner Voting Score Functions. (arXiv:2306.09835v1 [cs.CY])

    [http://arxiv.org/abs/2306.09835](http://arxiv.org/abs/2306.09835)

    本文研究了存在偏差时的子集选择问题，提出了在兼顾公平的前提下，使所选的子集满足群体公平性约束来提高选择质量，不同的多赢家评分函数对于公平性的依赖性可能不同。

    

    本文考虑了子集选择问题，其中给定多个项目的排名，目标是选择最高“质量”的子集。来自多赢家投票文献的评分函数已用于将排名聚合为子集的质量得分。我们研究了在此设置下的子集选择问题，当排名可能包含对一组项目的系统性或无意识的偏见时，我们的目标是在兼顾公平的前提下，提高选择质量。对于输入排名和偏见的一般模型，我们表明要求所选的子集满足群体公平性约束，可以提高选择在没有偏见的排名中的质量。重要的是，我们表明，对于公平性限制要有效，不同的多赢家评分函数可能需要极其不同数量的排名：对于一些函数，公平性限制需要指数级数量的排名才能恢复接近最优解，而对于其他函数，这种依赖性仅为多项式。这个结果依赖于一个新颖的概念。

    We consider the problem of subset selection where one is given multiple rankings of items and the goal is to select the highest ``quality'' subset. Score functions from the multiwinner voting literature have been used to aggregate rankings into quality scores for subsets. We study this setting of subset selection problems when, in addition, rankings may contain systemic or unconscious biases toward a group of items. For a general model of input rankings and biases, we show that requiring the selected subset to satisfy group fairness constraints can improve the quality of the selection with respect to unbiased rankings. Importantly, we show that for fairness constraints to be effective, different multiwinner score functions may require a drastically different number of rankings: While for some functions, fairness constraints need an exponential number of rankings to recover a close-to-optimal solution, for others, this dependency is only polynomial. This result relies on a novel notion 
    
[^40]: 复数变压器体系结构的构建块

    Building Blocks for a Complex-Valued Transformer Architecture. (arXiv:2306.09827v1 [cs.LG])

    [http://arxiv.org/abs/2306.09827](http://arxiv.org/abs/2306.09827)

    该论文提出了复数变压器架构的构建块，以便将深度学习应用到处理复数信号或图像上，在不使用投影到 $ \mathbb {R} ^2 $ 的情况下实现。该方法通过提供多个版本的复数缩放点积注意机制和复数层归一化，测试结果显示其在MusicNet数据集上的分类和序列生成任务上表现优秀，具有更好的鲁棒性和相当的性能。

    

    大多数深度学习流程都是建立在处理实值输入（如图像、语音或音乐信号）的实值操作之上。然而，许多应用程序自然使用复值信号或图像，例如MRI或遥感。此外，信号的傅里叶变换是复值的，并且具有许多应用。我们旨在使深度学习直接适用于这些复值信号，而不使用对 $ \mathbb {R} ^2 $ 的投影。因此，我们通过提供建立块，将变压器架构转换到复数域中，来增加复值神经网络的最近发展。我们提出了多个版本的复值缩放点积注意机制，以及复值层归一化。我们在MusicNet数据集上进行了分类和序列生成任务的测试，并表现出了对过拟合的改进稳健性，同时与实值变压器体系结构相比保持了同等的性能。

    Most deep learning pipelines are built on real-valued operations to deal with real-valued inputs such as images, speech or music signals. However, a lot of applications naturally make use of complex-valued signals or images, such as MRI or remote sensing. Additionally the Fourier transform of signals is complex-valued and has numerous applications. We aim to make deep learning directly applicable to these complex-valued signals without using projections into $\mathbb{R}^2$. Thus we add to the recent developments of complex-valued neural networks by presenting building blocks to transfer the transformer architecture to the complex domain. We present multiple versions of a complex-valued Scaled Dot-Product Attention mechanism as well as a complex-valued layer normalization. We test on a classification and a sequence generation task on the MusicNet dataset and show improved robustness to overfitting while maintaining on-par performance when compared to the real-valued transformer architec
    
[^41]: FALL-E：一种佛利音效合成模型及其策略

    FALL-E: A Foley Sound Synthesis Model and Strategies. (arXiv:2306.09807v1 [eess.AS])

    [http://arxiv.org/abs/2306.09807](http://arxiv.org/abs/2306.09807)

    本文介绍了一种名为“FALL-E”的佛利音效合成系统及其策略。该模型通过条件训练使其能够根据文本输入了解声音质量和录音环境，并在DCASE 2023挑战赛中表现良好，尤其在多样性上得分最高。

    

    本文介绍了一种名为“FALL-E”的佛利音效合成系统及其训练/推断策略。FALL-E模型采用级联方法，包括低分辨率频谱图生成、频谱图超分辨率和声码器。我们从头开始使用大量数据集训练了每个与声音相关的模型，并使用预训练的语言模型。我们使用数据集特定的文本将模型进行条件训练，使其能够根据文本输入了解声音质量和录音环境。此外，我们利用外部语言模型改善了我们的数据集的文本描述，并进行了质量、连贯性和多样性的提示工程。FALL-E在DCASE 2023挑战赛任务7中进行了客观评估和听力测试。提交结果在平均得分上获得第二名，同时在多样性上得分最高，在音频质量上获得第二名，在类适应性上获得第三名。

    This paper introduces FALL-E, a foley synthesis system and its training/inference strategies. The FALL-E model employs a cascaded approach comprising low-resolution spectrogram generation, spectrogram super-resolution, and a vocoder. We trained every sound-related model from scratch using our extensive datasets, and utilized a pre-trained language model. We conditioned the model with dataset-specific texts, enabling it to learn sound quality and recording environment based on text input. Moreover, we leveraged external language models to improve text descriptions of our datasets and performed prompt engineering for quality, coherence, and diversity. FALL-E was evaluated by an objective measure as well as listening tests in the DCASE 2023 challenge Task 7. The submission achieved the second place on average, while achieving the best score for diversity, second place for audio quality, and third place for class fitness.
    
[^42]: 观测中的样本高效策略模仿学习

    Sample-Efficient On-Policy Imitation Learning from Observations. (arXiv:2306.09805v1 [cs.LG])

    [http://arxiv.org/abs/2306.09805](http://arxiv.org/abs/2306.09805)

    提出了一种称为SEILO的算法，该算法结合了标准的对抗模仿学习和逆动力学建模，实现了从无专家数据的观测中的样本高效策略模仿学习，成功地减少了与环境的交互并实现了专家水平的表现。

    

    通过使用专家演示，模仿学习 (ILD) 旨在通过消除强化学习的许多缺点来帮助学习输出更好的策略。然而，在大多数真实世界的应用中，缺乏专家行动指导，因此无法使用ILD。相反，我们考虑观测中的模仿学习 (ILO)，其中没有提供专家动作，使其成为更具挑战性的问题。现有方法通常使用策略学习，这是众所周知的成本昂贵的。本文提出了 SEILO，一种新颖的样本高效策略算法，用于 ILO，将标准的对抗模仿学习与逆动力学建模相结合。这种方法使代理能够从对抗程序和行为克隆损失中获得反馈。我们实验证明，与其他最先进的策略 ILO 和 ILD 方法相比，我们提出的算法需要较少的与环境的交互来实现专家性能。

    Imitation learning from demonstrations (ILD) aims to alleviate numerous shortcomings of reinforcement learning through the use of demonstrations. However, in most real-world applications, expert action guidance is absent, making the use of ILD impossible. Instead, we consider imitation learning from observations (ILO), where no expert actions are provided, making it a significantly more challenging problem to address. Existing methods often employ on-policy learning, which is known to be sample-costly. This paper presents SEILO, a novel sample-efficient on-policy algorithm for ILO, that combines standard adversarial imitation learning with inverse dynamics modeling. This approach enables the agent to receive feedback from both the adversarial procedure and a behavior cloning loss. We empirically demonstrate that our proposed algorithm requires fewer interactions with the environment to achieve expert performance compared to other state-of-the-art on-policy ILO and ILD methods.
    
[^43]: 组合和混合变量贝叶斯优化的框架和基准。 (arXiv:2306.09803v1 [cs.LG])

    Framework and Benchmarks for Combinatorial and Mixed-variable Bayesian Optimization. (arXiv:2306.09803v1 [cs.LG])

    [http://arxiv.org/abs/2306.09803](http://arxiv.org/abs/2306.09803)

    本文介绍了一个模块化框架和基准，用于组合和混合变量贝叶斯优化，并提供多样的合成和真实世界基准测试。通过此框架，作者展示了4种常见的MCBO技术。

    

    本文介绍了一种模块化框架，用于混合变量和组合贝叶斯优化(MCBO)来解决领域中缺乏系统化基准和标准化评估的问题。目前的MCBO论文通常引入非多样性或非标准基准来评估其方法，阻碍了不同MCBO原语及其组合的正确评估。此外，介绍单个MCBO原语的论文通常省略了针对使用相同方法进行剩余原语的基线进行基准测试。这种省略主要是由于涉及的实现工作量非常大，导致缺乏控制评估并无法有效展示贡献的优点。为了克服这些挑战，我们提出的框架使贝叶斯优化组件的组合轻松易行，并提供了多样的合成和真实世界的基准测试任务。利用这种灵活性，我们实现了4种常见的MCBO技术，并在各种合成和真实基准测试中进行了评估。

    This paper introduces a modular framework for Mixed-variable and Combinatorial Bayesian Optimization (MCBO) to address the lack of systematic benchmarking and standardized evaluation in the field. Current MCBO papers often introduce non-diverse or non-standard benchmarks to evaluate their methods, impeding the proper assessment of different MCBO primitives and their combinations. Additionally, papers introducing a solution for a single MCBO primitive often omit benchmarking against baselines that utilize the same methods for the remaining primitives. This omission is primarily due to the significant implementation overhead involved, resulting in a lack of controlled assessments and an inability to showcase the merits of a contribution effectively. To overcome these challenges, our proposed framework enables an effortless combination of Bayesian Optimization components, and provides a diverse set of synthetic and real-world benchmarking tasks. Leveraging this flexibility, we implement 4
    
[^44]: $\pi2\text{vec}$：基于继承特征的策略表示方法

    $\pi2\text{vec}$: Policy Representations with Successor Features. (arXiv:2306.09800v1 [cs.LG])

    [http://arxiv.org/abs/2306.09800](http://arxiv.org/abs/2306.09800)

    本文提出了$\pi2\text{vec}$方法，它可以将黑盒策略行为表示为特征向量，并可以用于离线策略选择。该方法为现代研究方向中的离线策略评估、基础模型状态表示和资源受限制的策略选择提供了重要的支持。

    

    本文描述了$\pi2\text{vec}$，一种将黑盒策略行为表示为特征向量的方法。策略表示以任务无关的方式捕捉了基础模型特征统计数据在策略行为响应中的变化，并可以从离线数据中进行训练，从而可以用于离线策略选择。本研究为融合三个现代研究方向提供了重要的支持：作为离线强化学习的补充的离线策略评估，作为通用强大状态表示的基础模型，以及在资源受限制的环境中进行有效策略选择的方法。

    This paper describes $\pi2\text{vec}$, a method for representing behaviors of black box policies as feature vectors. The policy representations capture how the statistics of foundation model features change in response to the policy behavior in a task agnostic way, and can be trained from offline data, allowing them to be used in offline policy selection. This work provides a key piece of a recipe for fusing together three modern lines of research: Offline policy evaluation as a counterpart to offline RL, foundation models as generic and powerful state representations, and efficient policy selection in resource constrained environments.
    
[^45]: GPINN: 带有图嵌入的物理信息神经网络

    GPINN: Physics-informed Neural Network with Graph Embedding. (arXiv:2306.09792v1 [cs.LG])

    [http://arxiv.org/abs/2306.09792](http://arxiv.org/abs/2306.09792)

    GPINN是一种物理信息神经网络框架，能够在图形结构中执行PINN，将拓扑数据融入神经网络的计算中进行问题求解，利用图嵌入技术注入额外的维度，以封装图的空间特征并保留原始空间的属性。GPINN比传统的PINN性能更好，特别是在捕捉解的物理特征方面表现更加优异。

    

    本文提出一种带有图嵌入的物理信息神经网络框架（GPINN），用于在图形结构中执行物理信息神经网络（PINN）。该方法将拓扑数据融入到神经网络的计算中，从而显著提高了 PINN 的性能。图嵌入技术在输入空间中注入了额外的维度，以封装图的空间特征并保留原始空间的属性。这些额外的维度的选择由 Fiedler 向量引导，提供了图的最优异路径符号。进行了两个案例研究，结果显示 GPINN 在性能上相比传统的 PINN 有了显著提升，特别是在捕捉解的物理特征方面更具优势。

    This work proposes a Physics-informed Neural Network framework with Graph Embedding (GPINN) to perform PINN in graph, i.e. topological space instead of traditional Euclidean space, for improved problem-solving efficiency. The method integrates topological data into the neural network's computations, which significantly boosts the performance of the Physics-Informed Neural Network (PINN). The graph embedding technique infuses extra dimensions into the input space to encapsulate the spatial characteristics of a graph while preserving the properties of the original space. The selection of these extra dimensions is guided by the Fiedler vector, offering an optimised pathologic notation of the graph. Two case studies are conducted, which demonstrate significant improvement in the performance of GPINN in comparison to traditional PINN, particularly in its superior ability to capture physical features of the solution.
    
[^46]: 信息瓶颈的常微分方程：基于一阶根轨迹的信息瓶颈研究

    The Information Bottleneck's Ordinary Differential Equation: First-Order Root-Tracking for the IB. (arXiv:2306.09790v1 [cs.IT])

    [http://arxiv.org/abs/2306.09790](http://arxiv.org/abs/2306.09790)

    本文重新导出了信息瓶颈的一阶ODR，描述了其最优权衡曲线的动态，揭示了最优编码的动态过程。

    

    信息瓶颈是一种有损压缩方法，其速率-失真曲线描述了输入压缩和相关信息保留之间的基本权衡。然而，它掩盖了最优输入编码的基本动态。我们认为，随着输入信息的压缩，这些动态通常遵循分段光滑的轨迹，如最近在RD中所示。这些光滑的动态在最优编码发生定性变化（在分叉处）时会中断。通过利用信息瓶颈与RD的密切关系，可以看到次优解在那里发生碰撞或交换最优性。尽管信息瓶颈及其应用已被接受，但惊人地缺乏解决该问题的数值技术，即使对于已知分布的有限问题。我们重新导出了信息瓶颈的一阶常微分方程，描述了其最优权衡曲线的基本动态。

    The Information Bottleneck (IB) is a method of lossy compression. Its rate-distortion (RD) curve describes the fundamental tradeoff between input compression and the preservation of relevant information. However, it conceals the underlying dynamics of optimal input encodings. We argue that these typically follow a piecewise smooth trajectory as the input information is being compressed, as recently shown in RD. These smooth dynamics are interrupted when an optimal encoding changes qualitatively, at a bifurcation. By leveraging the IB's intimate relations with RD, sub-optimal solutions can be seen to collide or exchange optimality there.  Despite the acceptance of the IB and its applications, there are surprisingly few techniques to solve it numerically, even for finite problems whose distribution is known. We derive anew the IB's first-order Ordinary Differential Equation, which describes the dynamics underlying its optimal tradeoff curve. To exploit these dynamics, one needs not only 
    
[^47]: IoT边缘节点上的能源高效推理的动态决策树集成

    Dynamic Decision Tree Ensembles for Energy-Efficient Inference on IoT Edge Nodes. (arXiv:2306.09789v1 [cs.LG])

    [http://arxiv.org/abs/2306.09789](http://arxiv.org/abs/2306.09789)

    本文提出了一种在IoT边缘节点上非常适用的动态决策树集成算法，该算法可以自动调整执行树的数量，以权衡计算成本和精度，从而提高能源效率。

    

    随着物联网(IoT)设备的普及，需要能够在受限制的边缘节点上运行的能源高效机器学习(ML)模型。决策树集成，如随机森林(RFs)和梯度提升(GBTs)，在这个任务中特别适合，因为它们相对于其他选择具有较低的复杂性。然而，它们的推理时间和能量成本对于边缘硬件仍然很大。鉴于这种成本随着合奏规模的线性增长，本文提出使用动态合奏，根据延迟/能量目标和处理的输入的复杂性调整执行树的数量，以权衡计算成本和精度。我们专注于在多核低功耗IoT设备上部署这些算法，设计了一种工具，可将Python合奏自动转换为优化的C代码，并探索了几种优化策略，考虑了这些设备中可用的并行性和存储结构。

    With the increasing popularity of Internet of Things (IoT) devices, there is a growing need for energy-efficient Machine Learning (ML) models that can run on constrained edge nodes. Decision tree ensembles, such as Random Forests (RFs) and Gradient Boosting (GBTs), are particularly suited for this task, given their relatively low complexity compared to other alternatives. However, their inference time and energy costs are still significant for edge hardware. Given that said costs grow linearly with the ensemble size, this paper proposes the use of dynamic ensembles, that adjust the number of executed trees based both on a latency/energy target and on the complexity of the processed input, to trade-off computational cost and accuracy. We focus on deploying these algorithms on multi-core low-power IoT devices, designing a tool that automatically converts a Python ensemble into optimized C code, and exploring several optimizations that account for the available parallelism and memory hier
    
[^48]: 利用广义经验似然方法理解深度生成模型

    Understanding Deep Generative Models with Generalized Empirical Likelihoods. (arXiv:2306.09780v1 [cs.LG])

    [http://arxiv.org/abs/2306.09780](http://arxiv.org/abs/2306.09780)

    本文展示了广义经验似然（GEL）方法提供了一系列诊断工具来识别深度生成模型的许多缺陷，并结合最大均值差异和广义经验似然的技术，创造了保留每个样本可解释性的分布测试，还包括标签信息的指标。这些测试可以预测模式降低的程度。

    

    理解深度生成模型如何捕获高维数据分布仍然是一个重要的挑战。对于某些模型类别，比如生成对抗网络和扩散模型等不允许精确似然度量的模型，尤其困难。本文展示了广义经验似然（GEL）方法提供了一系列诊断工具来识别深度生成模型的许多缺陷。通过适当的矩条件规定，我们展示了本文提出的方法可以识别哪些模式被删除、DGM存在的模式不平衡程度以及DGM是否足够捕获类内多样性。我们展示了如何结合最大均值差异和广义经验似然的技术，不仅创造了保留每个样本可解释性的分布测试，还包括标签信息的指标。我们发现这些测试可以预测模式降低的程度。

    Understanding how well a deep generative model captures a distribution of high-dimensional data remains an important open challenge. It is especially difficult for certain model classes, such as Generative Adversarial Networks and Diffusion Models, whose models do not admit exact likelihoods. In this work, we demonstrate that generalized empirical likelihood (GEL) methods offer a family of diagnostic tools that can identify many deficiencies of deep generative models (DGMs). We show, with appropriate specification of moment conditions, that the proposed method can identify which modes have been dropped, the degree to which DGMs are mode imbalanced, and whether DGMs sufficiently capture intra-class diversity. We show how to combine techniques from Maximum Mean Discrepancy and Generalized Empirical Likelihood to create not only distribution tests that retain per-sample interpretability, but also metrics that include label information. We find that such tests predict the degree of mode dr
    
[^49]: 梯度真的是你所需要的一切吗？

    Gradient is All You Need?. (arXiv:2306.09778v1 [cs.LG])

    [http://arxiv.org/abs/2306.09778](http://arxiv.org/abs/2306.09778)

    本文提供了一种新的角度分析了基于梯度的学习算法，将一种新的多粒子无导数优化方法解释为梯度下降的随机松弛方法。此优化方法证明了零阶方法并不一定低效或不具备泛化能力，并且可以在丰富类别的非光滑和非凸目标函数下全局收敛于全局最小值。

    

    本文提供了一种新的分析方法，通过将一种新的多粒子无导数优化方法结合梯度下降看作随机松弛方法，来解释基于梯度的学习算法的理论理解。通过粒子之间的通讯，这种优化方法表现出类似于随机梯度下降的行为，证明了零阶方法并不一定低效或不具备泛化能力，并且可以在非光滑和非凸目标函数的丰富类别下全局收敛于全局最小值。

    In this paper we provide a novel analytical perspective on the theoretical understanding of gradient-based learning algorithms by interpreting consensus-based optimization (CBO), a recently proposed multi-particle derivative-free optimization method, as a stochastic relaxation of gradient descent. Remarkably, we observe that through communication of the particles, CBO exhibits a stochastic gradient descent (SGD)-like behavior despite solely relying on evaluations of the objective function. The fundamental value of such link between CBO and SGD lies in the fact that CBO is provably globally convergent to global minimizers for ample classes of nonsmooth and nonconvex objective functions, hence, on the one side, offering a novel explanation for the success of stochastic relaxations of gradient descent. On the other side, contrary to the conventional wisdom for which zero-order methods ought to be inefficient or not to possess generalization abilities, our results unveil an intrinsic gradi
    
[^50]: 使用机器学习方法自动化尺码表建立和管理

    Using Machine Learning Methods for Automation of Size Grid Building and Management. (arXiv:2306.09775v1 [cs.LG])

    [http://arxiv.org/abs/2306.09775](http://arxiv.org/abs/2306.09775)

    本研究使用机器学习方法解决时装公司尺码选择的问题，创造出更为自动化的流程，降低团队工作量，具有实际应用价值。

    

    时装服装公司需要提前一年规划下一个季度以进行供应链管理。该研究关注李维斯的尺码选择决策。目前，地区和计划小组级别的尺码表是手工建立和管理的，这给尺码、商家和计划团队带来了工作量。本研究旨在回答两个研究问题:“下一个季度每个尺码表名称下规划人员应该有哪些尺码可用？”和“下一个季度每个规划小组应采用哪些尺码？”我们使用一种分类模型来解决问题，这是机器学习中常用的模型之一。通过这项研究，使用机器学习技术创建了更为自动化的流程。预计在实践中投入使用后，公司团队的工作量将会减少。与时尚服装行业的许多研究不同，本研究聚焦于尺码问题。

    Fashion apparel companies require planning for the next season, a year in advance for supply chain management. This study focuses on size selection decision making for Levi Strauss. Currently, the region and planning group level size grids are built and managed manually. The company suffers from the workload it creates for sizing, merchant and planning teams. This research is aiming to answer two research questions: "Which sizes should be available to the planners under each size grid name for the next season(s)?" and "Which sizes should be adopted for each planning group for the next season(s)?". We approach to the problem with a classification model, which is one of the popular models used in machine learning. With this research, a more automated process was created by using machine learning techniques. A decrease in workload of the teams in the company is expected after it is put into practice. Unlike many studies in the state of art for fashion and apparel industry, this study focu
    
[^51]: 礼貌刻板印象和攻击向量：日韩语言模型中的性别刻板印象

    Politeness Stereotypes and Attack Vectors: Gender Stereotypes in Japanese and Korean Language Models. (arXiv:2306.09752v1 [cs.CL])

    [http://arxiv.org/abs/2306.09752](http://arxiv.org/abs/2306.09752)

    中文总结该论文主要研究了日语和韩语语言模型中与礼貌水平相关的语法性别偏见，发现礼貌水平是网络欺凌检测模型中的攻击向量。

    

    为了跟上大型语言模型的快速发展和使用，性别偏见研究在自然语言处理中变得越来越普遍。然而，非英语偏见研究还处于起步阶段，大多数工作都集中在英语上。在我们的工作中，我们研究了与礼貌水平相关的语法性别偏见在日语和韩语语言模型中的表现。这些语言的语言学研究已经确定了性别偏见和礼貌水平之间的联系，但尚不清楚语言模型是否会复制这些偏见。我们通过模板分析男性和女性语法性别的相对预测概率，并发现非正式礼貌语言最能表现出女性语法性别，而粗鲁和正式语言最能表现出男性语法性别。此外，我们发现礼貌水平是网络欺凌检测模型中的一种攻击向量，可以通过简单的技巧回避检测。

    In efforts to keep up with the rapid progress and use of large language models, gender bias research is becoming more prevalent in NLP. Non-English bias research, however, is still in its infancy with most work focusing on English. In our work, we study how grammatical gender bias relating to politeness levels manifests in Japanese and Korean language models. Linguistic studies in these languages have identified a connection between gender bias and politeness levels, however it is not yet known if language models reproduce these biases. We analyze relative prediction probabilities of the male and female grammatical genders using templates and find that informal polite speech is most indicative of the female grammatical gender, while rude and formal speech is most indicative of the male grammatical gender. Further, we find politeness levels to be an attack vector for allocational gender bias in cyberbullying detection models. Cyberbullies can evade detection through simple techniques ab
    
[^52]: Fedstellar：一个去中心化联邦学习平台

    Fedstellar: A Platform for Decentralized Federated Learning. (arXiv:2306.09750v1 [cs.LG])

    [http://arxiv.org/abs/2306.09750](http://arxiv.org/abs/2306.09750)

    Fedstellar是一个联邦学习平台，支持物理或虚拟设备的去中心化、半去中心化和中心化的方式训练模型，旨在解决现有平台在处理异构联盟网络拓扑等问题时的挑战。

    

    2016年，谷歌提出了联邦学习（FL）作为一种新的范式，可以在保护数据隐私的同时跨联盟参与者训练机器学习（ML）模型。虽然中心化联邦学习（CFL）是最常用的方法，但它存在通信瓶颈、单点故障和对中央服务器的依赖等局限。去中心化联邦学习（DFL）通过实现去中心化模型聚合和最小化对中央实体的依赖，来解决这些问题。然而，目前训练DFL模型的平台在处理异构联盟网络拓扑等关键问题方面存在困难。为了克服这些挑战，本文提出了Fedstellar，这是一个新型的平台，旨在在物理或虚拟设备的不同联盟中以去中心化、半去中心化和中心化的方式训练FL模型。

    In 2016, Google proposed Federated Learning (FL) as a novel paradigm to train Machine Learning (ML) models across the participants of a federation while preserving data privacy. Since its birth, Centralized FL (CFL) has been the most used approach, where a central entity aggregates participants' models to create a global one. However, CFL presents limitations such as communication bottlenecks, single point of failure, and reliance on a central server. Decentralized Federated Learning (DFL) addresses these issues by enabling decentralized model aggregation and minimizing dependency on a central entity. Despite these advances, current platforms training DFL models struggle with key issues such as managing heterogeneous federation network topologies. To overcome these challenges, this paper presents Fedstellar, a novel platform designed to train FL models in a decentralized, semi-decentralized, and centralized fashion across diverse federations of physical or virtualized devices. The Feds
    
[^53]: 《具有经验回放的时序差分学习》

    Temporal Difference Learning with Experience Replay. (arXiv:2306.09746v1 [cs.LG])

    [http://arxiv.org/abs/2306.09746](http://arxiv.org/abs/2306.09746)

    本文提出具有经验回放的TD学习，在马尔科夫观测模型下，通过对噪声项的分解，提供了有限时间误差界限，可以通过调整回放缓冲区和小批量的大小来控制误差。

    

    时序差分学习被普遍认为是强化学习领域中最受欢迎的算法之一。本文研究了其有限时间行为，包括均方误差和样本复杂度的有限时间界限。在经验方面，经验回放是深度强化学习算法成功的关键因素之一，但其在强化学习中的理论效应尚未被完全理解。本文提出了马尔科夫噪声项的简单分解，并为具有经验回放的TD学习提供了有限时间误差界限。具体而言，在马尔科夫观测模型下，我们证明了对于平均迭代和最终迭代情况下，常数步长引起的误差术语可以通过回放缓冲区的大小和从经验回放缓冲区中抽样的小批量来有效控制。

    Temporal-difference (TD) learning is widely regarded as one of the most popular algorithms in reinforcement learning (RL). Despite its widespread use, it has only been recently that researchers have begun to actively study its finite time behavior, including the finite time bound on mean squared error and sample complexity. On the empirical side, experience replay has been a key ingredient in the success of deep RL algorithms, but its theoretical effects on RL have yet to be fully understood. In this paper, we present a simple decomposition of the Markovian noise terms and provide finite-time error bounds for TD-learning with experience replay. Specifically, under the Markovian observation model, we demonstrate that for both the averaged iterate and final iterate cases, the error term induced by a constant step-size can be effectively controlled by the size of the replay buffer and the mini-batch sampled from the experience replay buffer.
    
[^54]: 离线强化学习中的自动权衡自适应

    Automatic Trade-off Adaptation in Offline RL. (arXiv:2306.09744v1 [cs.LG])

    [http://arxiv.org/abs/2306.09744](http://arxiv.org/abs/2306.09744)

    研究提出了一种名为AutoLION的离线强化学习算法，通过自动权衡自适应来减少专家时间成本。

    

    最近，出现了一些可在运行时保持自适应性的离线强化学习算法。例如，LION算法提供了一个用户界面，使用户能够在运行时设置基于估计回报的行为克隆和最优性之间的权衡。专家可以使用此界面根据其偏好调整策略行为，并在保守性和性能优化之间找到良好的权衡。由于专家时间宝贵，因此我们通过自动驾驶来扩展方法，以自动找到权衡的正确参数化，从而得到称为AutoLION的新算法。

    Recently, offline RL algorithms have been proposed that remain adaptive at runtime. For example, the LION algorithm \cite{lion} provides the user with an interface to set the trade-off between behavior cloning and optimality w.r.t. the estimated return at runtime. Experts can then use this interface to adapt the policy behavior according to their preferences and find a good trade-off between conservatism and performance optimization. Since expert time is precious, we extend the methodology with an autopilot that automatically finds the correct parameterization of the trade-off, yielding a new algorithm which we term AutoLION.
    
[^55]: 学习受限动力学的稳定神经微分方程

    Stabilized Neural Differential Equations for Learning Constrained Dynamics. (arXiv:2306.09739v1 [cs.LG])

    [http://arxiv.org/abs/2306.09739](http://arxiv.org/abs/2306.09739)

    本文提出了一种稳定神经微分方程（SNDEs）的方法，可以强制使用任意流形约束。该方法通过添加稳定项使约束流形成为渐进稳定的，并且在实验中表现优于现有方法。

    

    最近出现了许多成功的从数据学习动态系统的方法。然而，确保推断出的动态系统保留已知约束条件（例如守恒定律或对允许的系统状态的限制）仍然具有挑战性。我们提出了稳定神经微分方程（SNDEs）的方法，这是一种用于神经微分方程强制使用任意流形约束的方法。我们的方法基于一个稳定项，当添加到原始动态系统中时，可以将约束流形成为渐进稳定的。由于其简单性，我们的方法与所有常见的神经常微分方程（NODE）模型兼容并广泛适用。在广泛的经验评估中，我们证明SNDE在扩展可纳入NODE训练的约束类型方面胜过现有方法。

    Many successful methods to learn dynamical systems from data have recently been introduced. However, assuring that the inferred dynamics preserve known constraints, such as conservation laws or restrictions on the allowed system states, remains challenging. We propose stabilized neural differential equations (SNDEs), a method to enforce arbitrary manifold constraints for neural differential equations. Our approach is based on a stabilization term that, when added to the original dynamics, renders the constraint manifold provably asymptotically stable. Due to its simplicity, our method is compatible with all common neural ordinary differential equation (NODE) models and broadly applicable. In extensive empirical evaluations, we demonstrate that SNDEs outperform existing methods while extending the scope of which types of constraints can be incorporated into NODE training.
    
[^56]: 半离线强化学习用于优化文本生成

    Semi-Offline Reinforcement Learning for Optimized Text Generation. (arXiv:2306.09712v1 [cs.LG])

    [http://arxiv.org/abs/2306.09712](http://arxiv.org/abs/2306.09712)

    该研究提出了一种半离线强化学习范式，该范式平衡了探索能力和培训成本，提供了一个理论基础来比较不同的强化学习设置，并在优化成本、渐近误差和过度拟合误差界方面实现了最优的RL设置。实验结果表明，该方法高效且性能优异。

    

    在强化学习中，与环境交互有两种主要方式：在线和离线。在线方法探索环境所需时间较长，而离线方法通过牺牲探索能力有效地获得奖励信号。我们提出了半离线RL，一种新的范式，可以平滑地从离线转换到在线设置，平衡探索能力和培训成本，并为比较不同RL设置提供理论基础。基于半离线公式，我们提出了在优化成本、渐近误差和过度拟合误差界方面最优的RL设置。广泛的实验表明，我们的半离线方法效率高，与最先进的方法相比具有可比性或更好的性能。

    In reinforcement learning (RL), there are two major settings for interacting with the environment: online and offline. Online methods explore the environment at significant time cost, and offline methods efficiently obtain reward signals by sacrificing exploration capability. We propose semi-offline RL, a novel paradigm that smoothly transits from offline to online settings, balances exploration capability and training cost, and provides a theoretical foundation for comparing different RL settings. Based on the semi-offline formulation, we present the RL setting that is optimal in terms of optimization cost, asymptotic error, and overfitting error bound. Extensive experiments show that our semi-offline approach is efficient and yields comparable or often better performance compared with state-of-the-art methods.
    
[^57]: DAG-DNN中函数的表示和分解以及结构网络剪枝

    Representation and decomposition of functions in DAG-DNNs and structural network pruning. (arXiv:2306.09707v1 [cs.LG])

    [http://arxiv.org/abs/2306.09707](http://arxiv.org/abs/2306.09707)

    本研究提出了一种新的DNN表示方法DAG-DNN，利用下三角矩阵分解的方法对其进行结构网络剪枝，同时证明DAG-DNN可以推导出DNN各个子架构上定义的所有函数。

    

    必须仔细检查深度神经网络（DNN）提供的结论是普适的还是依赖于架构的。术语DAG-DNN指的是用直接无环图（DAG）表示架构的DNN的图形表示，其中弧与函数相关联。节点的级别表示输入节点和感兴趣节点之间的最大跳数。在当前研究中，我们演示了DAG-DNN可用于推导在DNN各个子架构上定义的所有函数。我们还演示了在DAG-DNN中定义的函数可以通过一系列下三角矩阵导出，其中每个矩阵提供了将子图中定义的函数过渡到指定级别节点的转换。与下三角矩阵相关联的lifting结构使得可以以系统化的方式执行网络的结构剪枝。分解是普遍适用的事实。

    The conclusions provided by deep neural networks (DNNs) must be carefully scrutinized to determine whether they are universal or architecture dependent. The term DAG-DNN refers to a graphical representation of a DNN in which the architecture is expressed as a direct-acyclic graph (DAG), on which arcs are associated with functions. The level of a node denotes the maximum number of hops between the input node and the node of interest. In the current study, we demonstrate that DAG-DNNs can be used to derive all functions defined on various sub-architectures of the DNN. We also demonstrate that the functions defined in a DAG-DNN can be derived via a sequence of lower-triangular matrices, each of which provides the transition of functions defined in sub-graphs up to nodes at a specified level. The lifting structure associated with lower-triangular matrices makes it possible to perform the structural pruning of a network in a systematic manner. The fact that decomposition is universally appl
    
[^58]: 降低情感分析中的计算成本：张量循环网络与循环网络的比较

    Reducing Computational Costs in Sentiment Analysis: Tensorized Recurrent Networks vs. Recurrent Networks. (arXiv:2306.09705v1 [cs.LG])

    [http://arxiv.org/abs/2306.09705](http://arxiv.org/abs/2306.09705)

    张量循环网络是一种降低情感分析计算成本的潜在解决方案

    

    预测观众对某一文本的反应对于政治、研究和商业行业等多个方面至关重要。情感分析（SA）是一种有用的自然语言处理（NLP）技术，利用词汇/统计和深度学习方法来确定不同大小的文本是否表现出积极、消极或中性的情感。循环网络在机器学习社区中广泛用于处理序列数据问题。然而，基于长短期记忆网络和门控循环单元的模型的一个缺点是参数数量显著高，因此这些模型的计算成本很高。当可用数据有限时，这种缺点甚至更为显著。此外，这些模型需要显着的过度参数化和正则化才能达到最佳性能。张量化模型代表了一个潜在的解决方案。本文将对一些社交媒体帖子进行情感分类。

    Anticipating audience reaction towards a certain text is integral to several facets of society ranging from politics, research, and commercial industries. Sentiment analysis (SA) is a useful natural language processing (NLP) technique that utilizes lexical/statistical and deep learning methods to determine whether different-sized texts exhibit positive, negative, or neutral emotions. Recurrent networks are widely used in machine-learning communities for problems with sequential data. However, a drawback of models based on Long-Short Term Memory networks and Gated Recurrent Units is the significantly high number of parameters, and thus, such models are computationally expensive. This drawback is even more significant when the available data are limited. Also, such models require significant over-parameterization and regularization to achieve optimal performance. Tensorized models represent a potential solution. In this paper, we classify the sentiment of some social media posts. We comp
    
[^59]: 一种适用于深度少样本元学习的层级贝叶斯模型

    A Hierarchical Bayesian Model for Deep Few-Shot Meta Learning. (arXiv:2306.09702v1 [cs.LG])

    [http://arxiv.org/abs/2306.09702](http://arxiv.org/abs/2306.09702)

    我们提出了适用于少样本元学习问题的层级贝叶斯模型，并通过引入全局变量，以帮助记忆历史情节的重要信息，同时控制模型对新情节适应的程度。我们提出了一种在线变分贝叶斯方法，通过实验证明该模型在少样本分类任务上优于现有方法。

    

    我们提出了一种新颖的层级贝叶斯模型，适用于具有大规模（可能是无限的）任务/情节的学习，很好地适应了少样本元学习问题。我们考虑以情节为基础的随机变量来模拟情节特定的目标生成过程，其中这些本地随机变量由更高级别的全局随机变量管理。全局变量有助于记住历史情节中的重要信息，同时以基于贝叶斯的原则控制模型需要适应新情节的程度。在我们的模型框架内，对于新的情节/任务的预测可以看作是一个贝叶斯推断问题。然而，使用大规模/无限数量的局部随机变量进行在线学习的一个主要障碍是，不能存储当前局部随机变量的后验分布以进行频繁的未来更新，而这在传统变分推断中很常见。我们需要能够将每个本地变量视为一次性学习问题，同时保持全局变量的影响，这使得使用在线推断成为必要。我们提出了一种高效的在线变分贝叶斯方法，具有计算时间和易于实现性的优点。通过实验，我们证明了我们的模型在少样本分类任务上优于几种现有方法。

    We propose a novel hierarchical Bayesian model for learning with a large (possibly infinite) number of tasks/episodes, which suits well the few-shot meta learning problem. We consider episode-wise random variables to model episode-specific target generative processes, where these local random variables are governed by a higher-level global random variate. The global variable helps memorize the important information from historic episodes while controlling how much the model needs to be adapted to new episodes in a principled Bayesian manner. Within our model framework, the prediction on a novel episode/task can be seen as a Bayesian inference problem. However, a main obstacle in learning with a large/infinite number of local random variables in online nature, is that one is not allowed to store the posterior distribution of the current local random variable for frequent future updates, typical in conventional variational inference. We need to be able to treat each local variable as a o
    
[^60]: 具有强凸性的 Nesterov-1983 的线性收敛性

    Linear convergence of Nesterov-1983 with the strong convexity. (arXiv:2306.09694v1 [math.OC])

    [http://arxiv.org/abs/2306.09694](http://arxiv.org/abs/2306.09694)

    本文使用高分辨率微分方程框架回答了Nesterov-1983和FISTA是否在强凸函数上线性收敛的问题，并指出线性收敛性不依赖于强凸性条件。

    

    对于现代基于梯度的优化，Nesterov 的加速梯度下降法是一个开创性里程碑，该方法在[Nesterov，1983]中提出，简称为Nesterov-1983。此后，重要的进展之一是它的近端推广，名为快速迭代收缩阈值算法（FISTA），广泛应用于图像科学和工程。然而，目前仍未知道Nesterov-1983和FISTA是否在强凸函数上线性收敛，而这已被列为综合评审[Chambolle和Pock，2016，附录B]中的未解决问题。本文通过使用高分辨率微分方程框架来回答这个问题。与先前采用的相空间表示一起，构造Lyapunov函数的关键区别在于动能的系数随迭代而变化。此外，我们指出，上述两种算法的线性收敛性没有依赖于强凸函数的条件。

    For modern gradient-based optimization, a developmental landmark is Nesterov's accelerated gradient descent method, which is proposed in [Nesterov, 1983], so shorten as Nesterov-1983. Afterward, one of the important progresses is its proximal generalization, named the fast iterative shrinkage-thresholding algorithm (FISTA), which is widely used in image science and engineering. However, it is unknown whether both Nesterov-1983 and FISTA converge linearly on the strongly convex function, which has been listed as the open problem in the comprehensive review [Chambolle and Pock, 2016, Appendix B]. In this paper, we answer this question by the use of the high-resolution differential equation framework. Along with the phase-space representation previously adopted, the key difference here in constructing the Lyapunov function is that the coefficient of the kinetic energy varies with the iteration. Furthermore, we point out that the linear convergence of both the two algorithms above has no d
    
[^61]: 基于深度误差因素与总体高分子信号估计的磁共振波谱定量技术

    Magnetic Resonance Spectroscopy Quantification Aided by Deep Estimations of Imperfection Factors and Overall Macromolecular Signal. (arXiv:2306.09681v1 [physics.med-ph])

    [http://arxiv.org/abs/2306.09681](http://arxiv.org/abs/2306.09681)

    本论文通过深度学习方法提高了磁共振波谱技术中代谢物信号定量精度和稳定性。

    

    磁共振波谱（MRS）是一种非侵入性的生物医学检测技术，由于代谢物信号的严重重叠、非理想采集条件引起的信号失真以及与强背景信号包括高分子信号的干扰，使用质子MRS精确量化代谢物仍然存在挑战。在本研究中，将深度学习引入到MRS量化中，通过引入正则化项、非理想采集条件的误差因素以及设计几种经验先验如诸如代谢物和高分子的基集，进一步提高了方法的精度和稳定性。

    Magnetic Resonance Spectroscopy (MRS) is an important non-invasive technique for in vivo biomedical detection. However, it is still challenging to accurately quantify metabolites with proton MRS due to three problems: Serious overlaps of metabolite signals, signal distortions due to non-ideal acquisition conditions and interference with strong background signals including macromolecule signals. The most popular software, LCModel, adopts the non-linear least square to quantify metabolites and addresses these problems by introducing regularization terms, imperfection factors of non-ideal acquisition conditions, and designing several empirical priors such as basissets of both metabolites and macromolecules. However, solving such a large non-linear quantitative problem is complicated. Moreover, when the signal-to-noise ratio of an input MRS signal is low, the solution may have a large deviation. In this work, deep learning is introduced to reduce the complexity of solving this overall quan
    
[^62]: 多视角分类增量学习

    Multi-View Class Incremental Learning. (arXiv:2306.09675v1 [cs.LG])

    [http://arxiv.org/abs/2306.09675](http://arxiv.org/abs/2306.09675)

    本文提出了一种名为多视角分类增量学习（MVCIL）的新模型，该模型使用随机化的表示学习技术进行特征提取，并提出正交融合子空间和选择性权重合并来解决增量学习中遗忘旧信息和学习新概念的挑战。实验结果表明该方法相比最新方法有效性更高。

    

    多视角学习（MVL）在整合数据集的多个视角以提高下游任务性能方面取得了巨大成功。为了使MVL方法在开放式环境中更实用，本文研究了一种新的范例，称为多视角分类增量学习（MVCIL），其中单个模型从连续的视图流中逐步分类新类，不需要访问早期数据的视图。但是，MVCIL面临着老信息的灾难性遗忘和学习新概念的干扰。为了解决这个问题，我们首先开发了一种基于随机化的表示学习技术，用于特征提取，以保证它们在工作状态下的分离视图最优，其中属于类的多个视图按顺序呈现；然后，我们将它们逐个集成到由提取的特征跨越的正交融合子空间中；最后，我们介绍选择性权重合并，以保留旧类的知识。基准数据集上的实验结果证明了我们提出的方法相对于最新方法的有效性。

    Multi-view learning (MVL) has gained great success in integrating information from multiple perspectives of a dataset to improve downstream task performance. To make MVL methods more practical in an open-ended environment, this paper investigates a novel paradigm called multi-view class incremental learning (MVCIL), where a single model incrementally classifies new classes from a continual stream of views, requiring no access to earlier views of data. However, MVCIL is challenged by the catastrophic forgetting of old information and the interference with learning new concepts. To address this, we first develop a randomization-based representation learning technique serving for feature extraction to guarantee their separate view-optimal working states, during which multiple views belonging to a class are presented sequentially; Then, we integrate them one by one in the orthogonality fusion subspace spanned by the extracted features; Finally, we introduce selective weight consolidation f
    
[^63]: 一种基于联合概率估计的一对一深度学习多分类模型

    Multi-Classification using One-versus-One Deep Learning Strategy with Joint Probability Estimates. (arXiv:2306.09668v1 [cs.LG])

    [http://arxiv.org/abs/2306.09668](http://arxiv.org/abs/2306.09668)

    本文提出了一种基于联合概率估计的一对一深度学习多分类模型，该模型通过特定的距离度量来校准二分类器的概率输出，并通过联合概率的距离最小化来获得对主体的类别概率估计。实验结果表明，该模型在多个应用中都具有更高的分类精度。

    

    One-versus-One（OvO）策略是一种多分类模型，它侧重于训练每一对类之间的二分类器。本文提出了一种新的OvO多分类模型，该模型采用联合概率估计方法，通过深度学习框架校准二分类器的概率输出，并通过联合概率的距离最小化来获得对主体的类别概率估计。实验表明，相对于传统方法，该模型具有更高的分类精度。

    The One-versus-One (OvO) strategy is an approach of multi-classification models which focuses on training binary classifiers between each pair of classes. While the OvO strategy takes advantage of balanced training data, the classification accuracy is usually hindered by the voting mechanism to combine all binary classifiers. In this paper, a novel OvO multi-classification model incorporating a joint probability measure is proposed under the deep learning framework. In the proposed model, a two-stage algorithm is developed to estimate the class probability from the pairwise binary classifiers. Given the binary classifiers, the pairwise probability estimate is calibrated by a distance measure on the separating feature hyperplane. From that, the class probability of the subject is estimated by solving a joint probability-based distance minimization problem. Numerical experiments in different applications show that the proposed model achieves generally higher classification accuracy than 
    
[^64]: 一种高效的私有连续观测平滑二元机制

    A Smooth Binary Mechanism for Efficient Private Continual Observation. (arXiv:2306.09666v1 [cs.LG])

    [http://arxiv.org/abs/2306.09666](http://arxiv.org/abs/2306.09666)

    本文提出了一种可微且凸的替代二进制机制的方法，用于解决隐私问题中发布私有前缀求和题的效率问题。该方法比标志性的二进制机制具有更低的方差和更好的观测时间保证，并且在计算和空间要求方面更加有效。

    

    在不断观测的隐私问题中，我们研究如何基于随时间演变的数据集发布差分隐私估计。发布私有前缀求和$x_1,x_2,x_3,\dots\in\{0,1\}$问题是一个特别好研究的问题，并且在私有随机梯度下降的最新方法中使用了一个广义形式。标志性的二进制机制自动加入的噪声的方差是多对数的。最近，Henzinger et al.和Denisov et al.表示，可以通过两种方式提高二进制机制：噪声的方差可以减小一个较大的常数因子，并且在时间步长内也可以更加平均。但是，他们生成噪声分布的算法在计算时间和（特别是）空间方面都不如人意。我们通过提供一种可微且凸的简单替代二进制机制的方法来解决效率问题，从而使我们能够相对于添加的噪声量对机制进行优化。我们的机制比标志性的二进制机制具有更低的方差和更好的观测时间保证，并且在计算和空间要求方面比以前的改进更有效。

    In privacy under continual observation we study how to release differentially private estimates based on a dataset that evolves over time. The problem of releasing private prefix sums of $x_1,x_2,x_3,\dots \in\{0,1\}$ (where the value of each $x_i$ is to be private) is particularly well-studied, and a generalized form is used in state-of-the-art methods for private stochastic gradient descent (SGD). The seminal binary mechanism privately releases the first $t$ prefix sums with noise of variance polylogarithmic in $t$. Recently, Henzinger et al. and Denisov et al. showed that it is possible to improve on the binary mechanism in two ways: The variance of the noise can be reduced by a (large) constant factor, and also made more even across time steps. However, their algorithms for generating the noise distribution are not as efficient as one would like in terms of computation time and (in particular) space. We address the efficiency problem by presenting a simple alternative to the binary
    
[^65]: 合作多目标强化学习用于交通信号控制和碳减排

    Cooperative Multi-Objective Reinforcement Learning for Traffic Signal Control and Carbon Emission Reduction. (arXiv:2306.09662v1 [cs.LG])

    [http://arxiv.org/abs/2306.09662](http://arxiv.org/abs/2306.09662)

    本文提出了一种合作的多目标架构，称为MOMA-DDPG，用于交通信号控制和碳减排问题。该方法涉及两种类型的智能体：一个专注于优化每个路口的本地交通，而另一个旨在优化全局交通吞吐量。结果显示，该方法优于现有最先进的方法，并解决了等待时间和碳排放量两个问题。

    

    现有的交通信号控制系统依赖于过于简化的基于规则的方法，甚至基于强化学习的方法也经常是次优的和不稳定的。为了解决这个问题，我们提出了一个合作的多目标架构，称为多目标多智能体深度确定性策略梯度（MOMA-DDPG），使用衰减权重来估计交通信号控制优化的多个奖励项。我们的方法涉及两种类型的智能体：一个专注于优化每个路口的本地交通，而另一个旨在优化全局交通吞吐量。我们使用从一个亚洲国家的交通摄像头收集到的真实世界交通数据来评估我们的方法。尽管包含了一个全局智能体，但我们的解决方案仍然是分散的，因为这个智能体在推理阶段不再是必要的。我们的结果证明了MOMA-DDPG的有效性，在所有性能指标上优于最先进的方法。此外，我们提出的系统最小化了等待时间和碳排放量两方面的问题。

    Existing traffic signal control systems rely on oversimplified rule-based methods, and even RL-based methods are often suboptimal and unstable. To address this, we propose a cooperative multi-objective architecture called Multi-Objective Multi-Agent Deep Deterministic Policy Gradient (MOMA-DDPG), which estimates multiple reward terms for traffic signal control optimization using age-decaying weights. Our approach involves two types of agents: one focuses on optimizing local traffic at each intersection, while the other aims to optimize global traffic throughput. We evaluate our method using real-world traffic data collected from an Asian country's traffic cameras. Despite the inclusion of a global agent, our solution remains decentralized as this agent is no longer necessary during the inference stage. Our results demonstrate the effectiveness of MOMA-DDPG, outperforming state-of-the-art methods across all performance metrics. Additionally, our proposed system minimizes both waiting ti
    
[^66]: 基于点过程的时间因果中介：医疗干预的直接和间接效应

    Temporal Causal Mediation through a Point Process: Direct and Indirect Effects of Healthcare Interventions. (arXiv:2306.09656v1 [cs.LG])

    [http://arxiv.org/abs/2306.09656](http://arxiv.org/abs/2306.09656)

    该论文介绍了一个新的非参数的中介变量-结果模型，用于医疗干预的因果分析，可以区分出干预的直接和间接效应，并考虑长程中介变量-结果交互作用。

    

    为决定适当的干预措施，需要建立一个治疗、结果和潜在中介变量的因果模型。因果中介分析可以区分干预的直接和间接效应，但多数研究都在静态环境下进行。在医疗领域，数据以复杂的、不规则的时间序列形式呈现，而且治疗、结果和中介变量在时间上有动态相互依赖关系。现有的动态因果中介分析方法局限于规则的测量间隔、简单的参数模型，并忽略了中介变量-结果之间的长程交互作用。为解决这些限制，我们提出了一个非参数的中介变量-结果模型，其中中介变量被假定为与结果过程互动的时间点过程。使用这个模型，我们估计了对结果的外部干预的直接和间接效应，显示了每种效应如何影响整个未来轨迹。我们在半月板数据集上进行了演示。

    Deciding on an appropriate intervention requires a causal model of a treatment, the outcome, and potential mediators. Causal mediation analysis lets us distinguish between direct and indirect effects of the intervention, but has mostly been studied in a static setting. In healthcare, data come in the form of complex, irregularly sampled time-series, with dynamic interdependencies between a treatment, outcomes, and mediators across time. Existing approaches to dynamic causal mediation analysis are limited to regular measurement intervals, simple parametric models, and disregard long-range mediator--outcome interactions. To address these limitations, we propose a non-parametric mediator--outcome model where the mediator is assumed to be a temporal point process that interacts with the outcome process. With this model, we estimate the direct and indirect effects of an external intervention on the outcome, showing how each of these affects the whole future trajectory. We demonstrate on sem
    
[^67]: 基于图神经网络学习断层储层中CO$_2$气云迁移

    Learning CO$_2$ plume migration in faulted reservoirs with Graph Neural Networks. (arXiv:2306.09648v1 [cs.LG])

    [http://arxiv.org/abs/2306.09648](http://arxiv.org/abs/2306.09648)

    提出一种基于图神经网络的模型，准确预测了非结构化合成储层中CO$_2$气云的时间演化，相比标准模型具有更好的准确性和减少的时间误差积累。

    

    基于深度学习的代理模型是地下流动问题（如CO$_2$地质储存）的数值模拟的有效补充。然而，如何准确地捕捉断层对CO$_2$气云迁移的影响，仍是许多现有基于卷积神经网络（CNN）或神经算子(Neural Operators)深度学习代理模型的挑战。我们提出了一种基于图神经网络（GNN）领域最新发展的图卷积长时记忆（GConvLSTM）和一个单步GNN模型（MeshGraphNet(MGN)）相结合的图神经模型，以对复杂的非结构化网格进行操作，限制时间误差累计。在一个具有不透水断层的合成储层中，我们证明了我们的方法可以准确预测气体饱和度和孔隙压力的时间演化。与标准MGN模型相比，我们的结果表现出更好的准确性和减少的时间误差积累。

    Deep-learning-based surrogate models provide an efficient complement to numerical simulations for subsurface flow problems such as CO$_2$ geological storage. Accurately capturing the impact of faults on CO$_2$ plume migration remains a challenge for many existing deep learning surrogate models based on Convolutional Neural Networks (CNNs) or Neural Operators. We address this challenge with a graph-based neural model leveraging recent developments in the field of Graph Neural Networks (GNNs). Our model combines graph-based convolution Long-Short-Term-Memory (GConvLSTM) with a one-step GNN model, MeshGraphNet (MGN), to operate on complex unstructured meshes and limit temporal error accumulation. We demonstrate that our approach can accurately predict the temporal evolution of gas saturation and pore pressure in a synthetic reservoir with impermeable faults. Our results exhibit a better accuracy and a reduced temporal error accumulation compared to the standard MGN model. We also show the
    
[^68]: 用于压缩潜在表示的无监督异常检测方法

    Vacant Holes for Unsupervised Detection of the Outliers in Compact Latent Representation. (arXiv:2306.09646v1 [stat.ML])

    [http://arxiv.org/abs/2306.09646](http://arxiv.org/abs/2306.09646)

    本文研究了基于变分自编码器（VAE）的无监督异常检测方法，通过引入图像的紧性特征来纠正VAE模型中的理论缺陷并缩小内点与离群点之间的距离，同时结合建模技术和结构知识提出了一种无监督异常检测算法。

    

    在真实世界中，对于任何机器学习模型的部署和操作，检测异常值至关重要。对于深度神经网络而言，这一点尤为重要，因为这些网络对于此类输入显示出过度自信。此外，即使是允许估计输入概率密度的深度生成模型也难以完成此任务。本文主要集中于这类模型中的一种：变分自编码器（VAE）。首先，我们揭示了经典VAE模型假设中的一个重大理论缺陷。其次，我们通过引入紧性作为从深度神经映射到潜在空间的图像的拓扑特征来纠正这一缺陷，并获得将图像压缩在确定限制内的可证界限来同时压缩内点和离群点的手段。我们采用两种方法实现紧性：（i）亚历山大夫扩展和（ii）对VAE编码器的映射进行固定的Lipschitz连续性常数。最后但也最重要的是，我们提出了一种基于利用已有建模技术和结构知识的无监督异常检测算法。

    Detection of the outliers is pivotal for any machine learning model deployed and operated in real-world. It is essential for the Deep Neural Networks that were shown to be overconfident with such inputs. Moreover, even deep generative models that allow estimation of the probability density of the input fail in achieving this task. In this work, we concentrate on the specific type of these models: Variational Autoencoders (VAEs). First, we unveil a significant theoretical flaw in the assumption of the classical VAE model. Second, we enforce an accommodating topological property to the image of the deep neural mapping to the latent space: compactness to alleviate the flaw and obtain the means to provably bound the image within the determined limits by squeezing both inliers and outliers together. We enforce compactness using two approaches: (i) Alexandroff extension and (ii) fixed Lipschitz continuity constant on the mapping of the encoder of the VAEs. Finally and most importantly, we di
    
[^69]: BISCUIT: 从二进制交互中学习因果关系表征

    BISCUIT: Causal Representation Learning from Binary Interactions. (arXiv:2306.09643v1 [cs.LG])

    [http://arxiv.org/abs/2306.09643](http://arxiv.org/abs/2306.09643)

    本文提出了一种名为BISCUIT的方法，可以在许多常见的设置中确定因果变量，并在机器人启发的数据集上进行了测试，表现良好。

    

    在机器人和实体AI等应用中，识别环境中的因果变量以及如何对它们进行干预具有核心价值。本文提出了一种方法，即在许多常见的设置中，例如加性高斯噪声模型中仍然可以确定因果变量，如果智能体与因果变量的交互可以用未知的二进制变量描述。我们通过这一可识别性结果提出BISCUIT，一种同时学习因果变量及其对应二进制交互变量的方法。在三个机器人启发的数据集上，BISCUIT准确地识别出因果变量，甚至可以扩展到复杂的、逼真的实体AI环境中。

    Identifying the causal variables of an environment and how to intervene on them is of core value in applications such as robotics and embodied AI. While an agent can commonly interact with the environment and may implicitly perturb the behavior of some of these causal variables, often the targets it affects remain unknown. In this paper, we show that causal variables can still be identified for many common setups, e.g., additive Gaussian noise models, if the agent's interactions with a causal variable can be described by an unknown binary variable. This happens when each causal variable has two different mechanisms, e.g., an observational and an interventional one. Using this identifiability result, we propose BISCUIT, a method for simultaneously learning causal variables and their corresponding binary interaction variables. On three robotic-inspired datasets, BISCUIT accurately identifies causal variables and can even be scaled to complex, realistic environments for embodied AI.
    
[^70]: 跨域毒性片段检测

    Cross-Domain Toxic Spans Detection. (arXiv:2306.09642v1 [cs.CL])

    [http://arxiv.org/abs/2306.09642](http://arxiv.org/abs/2306.09642)

    本研究评估了三种跨域条件下检测毒性片段的方法，结果表明使用现成的词典的简单方法在跨域设置中表现最佳，而用于领域内的语言模型容易出现某些类型的假阳性。

    

    鉴于毒性语言的动态性，自动检测毒性片段的方法可能会遭遇分布转移。为了探索这种现象，我们评估了三种检测跨域条件下毒性片段的方法：基于词典、根因抽取和微调语言模型。我们的研究发现，使用现成的词典的简单方法在跨域设置中表现最佳。跨领域误差分析表明：（1）根因提取方法容易出现假负结果，而（2）语言模型尽管在领域内的情况下表现最佳，但其明确提取毒性单词的数量比词典少，并且容易出现某些类型的假阳性。我们的代码公开在https://github.com/sfschouten/toxic-cross-domain 。

    Given the dynamic nature of toxic language use, automated methods for detecting toxic spans are likely to encounter distributional shift. To explore this phenomenon, we evaluate three approaches for detecting toxic spans under cross-domain conditions: lexicon-based, rationale extraction, and fine-tuned language models. Our findings indicate that a simple method using off-the-shelf lexicons performs best in the cross-domain setup. The cross-domain error analysis suggests that (1) rationale extraction methods are prone to false negatives, while (2) language models, despite performing best for the in-domain case, recall fewer explicitly toxic words than lexicons and are prone to certain types of false positives. Our code is publicly available at: https://github.com/sfschouten/toxic-cross-domain.
    
[^71]: CLIPSonic：使用未标注视频和预训练语言视觉模型的文本合成音频

    CLIPSonic: Text-to-Audio Synthesis with Unlabeled Videos and Pretrained Language-Vision Models. (arXiv:2306.09635v1 [cs.SD])

    [http://arxiv.org/abs/2306.09635](http://arxiv.org/abs/2306.09635)

    本文提出了一种使用未标注视频和预训练语言视觉模型进行文本合成音频的方法。通过利用视觉模态作为桥梁来学习文本-音频对应关系，提出了有条件的扩散模型，生成视频的音轨。使用CLIP图像查询条件进行零样本模态转换。并采用预训练的扩散先验模型，生成对应于CLIP文本嵌入的CLIP图像嵌入。实验证明了该方法的有效性。

    

    最近的研究探讨了使用大量成对的文本和音频数据进行文本合成音频。 然而，带有高质量文本注释的音频录音可能难以获取。 在本文中，我们使用未标记的视频和预训练语言视觉模型来进行文本合成音频的研究。 我们建议利用视觉模态作为桥梁来学习所需的文本-音频对应关系。 我们训练一个有条件的扩散模型，以生成视频的音频轨道，给定由预训练的对比语言图像预训练（CLIP）模型编码的视频帧。 在测试时间，我们首先探索执行零样本模态转换，并使用CLIP编码的文本查询条件扩散模型。 但是，我们观察到与图像查询相比存在明显的性能下降。 为了弥合这一差距，我们进一步采用预训练的扩散先验模型，生成给定CLIP文本嵌入的CLIP图像嵌入。 我们的结果显示了所提出方法的有效性，并且

    Recent work has studied text-to-audio synthesis using large amounts of paired text-audio data. However, audio recordings with high-quality text annotations can be difficult to acquire. In this work, we approach text-to-audio synthesis using unlabeled videos and pretrained language-vision models. We propose to learn the desired text-audio correspondence by leveraging the visual modality as a bridge. We train a conditional diffusion model to generate the audio track of a video, given a video frame encoded by a pretrained contrastive language-image pretraining (CLIP) model. At test time, we first explore performing a zero-shot modality transfer and condition the diffusion model with a CLIP-encoded text query. However, we observe a noticeable performance drop with respect to image queries. To close this gap, we further adopt a pretrained diffusion prior model to generate a CLIP image embedding given a CLIP text embedding. Our results show the effectiveness of the proposed method, and that 
    
[^72]: 虚假黎明：重新评估谷歌强化学习在芯片宏观布局中的应用

    The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement. (arXiv:2306.09633v1 [cs.LG])

    [http://arxiv.org/abs/2306.09633](http://arxiv.org/abs/2306.09633)

    谷歌2021年在《自然》杂志上发表的一篇论文声称其使用强化学习在芯片设计领域进行了创新，但两项独立的评估表明，谷歌的方法不如人类设计师、不如一个众所周知的算法（模拟退火），并且也不如普遍可用的商业软件，文章的完整性也遭到了严重的损害。

    

    谷歌2021年在《自然》杂志上发表的有关使用强化学习设计芯片的论文，因为所声称的结果缺乏充分的文件记录和关键步骤的说明，引发争议并受到媒体的批评报道。 而两项独立的评估填补了空白，证明谷歌强化学习落后于人类设计师、落后于一种众所周知的算法（模拟退火），并且还落后于普遍可用的商业软件。交叉检查的数据表明，由于行为、分析和报告中的错误，该《自然》文章的完整性受到了严重的损害。

    Reinforcement learning (RL) for physical design of silicon chips in a Google 2021 Nature paper stirred controversy due to poorly documented claims that raised eyebrows and attracted critical media coverage. The Nature paper withheld most inputs needed to produce reported results and some critical steps in the methodology. But two independent evaluations filled in the gaps and demonstrated that Google RL lags behind human designers, behind a well-known algorithm (Simulated Annealing), and also behind generally-available commercial software. Crosschecked data indicate that the integrity of the Nature paper is substantially undermined owing to errors in the conduct, analysis and reporting.
    
[^73]: 机器学习中出现的幂律动态研究

    Power-law Dynamic arising from machine learning. (arXiv:2306.09624v1 [stat.ML])

    [http://arxiv.org/abs/2306.09624](http://arxiv.org/abs/2306.09624)

    该论文研究了机器学习中的幂律动态，证明了其具有唯一平稳分布且可以通过比较连续和离散化幂律动态的出现时间来指导机器学习算法。

    

    我们研究了一种新的随机微分方程，这种方程起源于机器学习中的优化研究，我们称之为幂律动态，因为其平稳分布不能具有亚高斯尾部并服从幂律。我们证明，只要学习率足够小，幂律动态是遍历的且具有唯一的平稳分布。我们调查了它的首次存在时间。特别是，我们比较了连续的幂律动态及其离散化在退出时间上的差异。这种比较可以帮助指导机器学习算法。

    We study a kind of new SDE that was arisen from the research on optimization in machine learning, we call it power-law dynamic because its stationary distribution cannot have sub-Gaussian tail and obeys power-law. We prove that the power-law dynamic is ergodic with unique stationary distribution, provided the learning rate is small enough. We investigate its first exist time. In particular, we compare the exit times of the (continuous) power-law dynamic and its discretization. The comparison can help guide machine learning algorithm.
    
[^74]: 从超图能量函数到超图神经网络

    From Hypergraph Energy Functions to Hypergraph Neural Networks. (arXiv:2306.09623v1 [cs.LG])

    [http://arxiv.org/abs/2306.09623](http://arxiv.org/abs/2306.09623)

    本文提出了一种新的基于超图能量函数的节点嵌入方法，可以通过双层优化实现节点分类任务，相比传统GNN模型有更好的表现。

    

    超图是表示实体之间高阶交互的强大抽象模型。为了在实现下游预测的过程中利用这些关系，最近提出了多种超图神经网络架构，这些架构在很大程度上是建立在传统图神经网络（GNN）文献的先驱上的。在这篇论文中，我们首先介绍了一类具有表达能力的参数化超图正则化能量函数。然后，我们演示了如何将这些能量的极小值有效地作为节点嵌入器，再配合一个参数化分类器进行端到端训练，通过一个监督的双层优化过程实现。之后，我们发现了建议的双层超图优化中出现的预测模型的隐式架构和常用GNN架构之间的相似之处。在实证方面，我们在各种超图节点分类任务中展示了最先进的结果。

    Hypergraphs are a powerful abstraction for representing higher-order interactions between entities of interest. To exploit these relationships in making downstream predictions, a variety of hypergraph neural network architectures have recently been proposed, in large part building upon precursors from the more traditional graph neural network (GNN) literature. Somewhat differently, in this paper we begin by presenting an expressive family of parameterized, hypergraph-regularized energy functions. We then demonstrate how minimizers of these energies effectively serve as node embeddings that, when paired with a parameterized classifier, can be trained end-to-end via a supervised bilevel optimization process. Later, we draw parallels between the implicit architecture of the predictive models emerging from the proposed bilevel hypergraph optimization, and existing GNN architectures in common use. Empirically, we demonstrate state-of-the-art results on various hypergraph node classification
    
[^75]: 高维生成模型测度中的精度和召回的不对称性：衡量保真度和多样性的两个重要指标

    Emergent Asymmetry of Precision and Recall for Measuring Fidelity and Diversity of Generative Models in High Dimensions. (arXiv:2306.09618v1 [cs.LG])

    [http://arxiv.org/abs/2306.09618](http://arxiv.org/abs/2306.09618)

    本研究发现在高维生成模型测度中使用的精度和召回指标存在不对称性，可能会导致误导性结论。我们提出了一些方法来修正这种错误。

    

    精度和召回是衡量生成模型性能的两个重要指标，它们分别被用来测量模型的保真度和多样性。然而，本研究发现通过使用k近邻的常见逼近方法，这些指标在高维空间中容易出现误导性结论。具体来说，我们理论上和实验上证明了，在高维空间中，两个与真实分布的支持等距离的模型分布可以具有非常不同的精度和召回，从而导致不对称性。针对我们的理论发现，我们提出了一些简单的修正方法来消除这种错误结果。

    Precision and Recall are two prominent metrics of generative performance, which were proposed to separately measure the fidelity and diversity of generative models. Given their central role in comparing and improving generative models, understanding their limitations are crucially important. To that end, in this work, we identify a critical flaw in the common approximation of these metrics using k-nearest-neighbors, namely, that the very interpretations of fidelity and diversity that are assigned to Precision and Recall can fail in high dimensions, resulting in very misleading conclusions. Specifically, we empirically and theoretically show that as the number of dimensions grows, two model distributions with supports at equal point-wise distance from the support of the real distribution, can have vastly different Precision and Recall regardless of their respective distributions, hence an emergent asymmetry in high dimensions. Based on our theoretical insights, we then provide simple ye
    
[^76]: HomoGCL：重新思考图形对比学习中的同质性问题

    HomoGCL: Rethinking Homophily in Graph Contrastive Learning. (arXiv:2306.09614v1 [cs.LG])

    [http://arxiv.org/abs/2306.09614](http://arxiv.org/abs/2306.09614)

    HomoGCL是一个模型无关的图形对比学习框架，利用同质性原则扩展正集，从而进一步提高图形对比学习的性能。

    

    对比学习 (CL) 已成为自监督学习图形中的学习范式，通常遵循“增强-对比”学习方案。然而，我们观察到与计算机视觉领域中的对比学习不同，图形领域的对比学习即使没有数据增强也能表现良好。我们对这一现象进行了系统分析，并认为同质性即“物以类聚”的原则在图形对比学习的成功中起着关键作用。受到这一性质的启发，我们提出了HomoGCL，这是一个模型无关的框架，通过使用具有邻居特定重要性的邻居节点来扩展正集。理论上讲，HomoGCL引入了原始节点特征和节点嵌入在增强视图中的互信息的更严格的下限。此外，HomoGCL可以与现有的图形对比学习模型以插件方式组合起来，而只有轻微额外的计算开销。大量实验表明，HomoGCL产生了多种比现有方法更好的性能。

    Contrastive learning (CL) has become the de-facto learning paradigm in self-supervised learning on graphs, which generally follows the "augmenting-contrasting" learning scheme. However, we observe that unlike CL in computer vision domain, CL in graph domain performs decently even without augmentation. We conduct a systematic analysis of this phenomenon and argue that homophily, i.e., the principle that "like attracts like", plays a key role in the success of graph CL. Inspired to leverage this property explicitly, we propose HomoGCL, a model-agnostic framework to expand the positive set using neighbor nodes with neighbor-specific significances. Theoretically, HomoGCL introduces a stricter lower bound of the mutual information between raw node features and node embeddings in augmented views. Furthermore, HomoGCL can be combined with existing graph CL models in a plug-and-play way with light extra computational overhead. Extensive experiments demonstrate that HomoGCL yields multiple stat
    
[^77]: GraphSHA：用于类不平衡节点分类的更难样本综合。

    GraphSHA: Synthesizing Harder Samples for Class-Imbalanced Node Classification. (arXiv:2306.09612v1 [cs.LG])

    [http://arxiv.org/abs/2306.09612](http://arxiv.org/abs/2306.09612)

    GraphSHA是一种能够缓解类不平衡问题的通用框架，在综合更难的较小类别样本以扩大较小类别的决策边界的同时，还提出了SemiMixup模块以避免扩大的边界违反邻居类别的子空间。

    

    类不平衡是指某些类别的实例数量比其他类别少得多的现象，在真实世界的图结构场景中普遍存在。最近的研究发现，现有的图神经网络（GNNs）会低估较小类别的样本。我们调查了这种现象，并发现主类别的子空间挤压较小类别的子空间是这种失败的主要原因。我们自然地受到启发，以扩大较小类别的决策边界为目标，提出了通过综合更难的较小类别样本的通用框架GraphSHA。此外，为了避免扩大的较小边界违反邻居类别的子空间，我们还提出了一个称为SemiMixup的模块，将扩大的边界信息传递到较小类的内部，同时阻止从较小类到邻居类的信息传播。经验证明，GraphSHA在扩大较小类别的决策边界方面表现出了它的有效性。

    Class imbalance is the phenomenon that some classes have much fewer instances than others, which is ubiquitous in real-world graph-structured scenarios. Recent studies find that off-the-shelf Graph Neural Networks (GNNs) would under-represent minor class samples. We investigate this phenomenon and discover that the subspaces of minor classes being squeezed by those of the major ones in the latent space is the main cause of this failure. We are naturally inspired to enlarge the decision boundaries of minor classes and propose a general framework GraphSHA by Synthesizing HArder minor samples. Furthermore, to avoid the enlarged minor boundary violating the subspaces of neighbor classes, we also propose a module called SemiMixup to transmit enlarged boundary information to the interior of the minor classes while blocking information propagation from minor classes to neighbor classes. Empirically, GraphSHA shows its effectiveness in enlarging the decision boundaries of minor classes, as it 
    
[^78]: CHORUS: 统一数据发现和探索的基础模型。

    CHORUS: Foundation Models for Unified Data Discovery and Exploration. (arXiv:2306.09610v1 [cs.DB])

    [http://arxiv.org/abs/2306.09610](http://arxiv.org/abs/2306.09610)

    研究者探索将大型语言模型应用于数据发现和探索任务中，证明这些模型在表格类检测、列类型注释和联接列预测中具有优越性能，并有望将不同的数据管理任务统一在基础模型下。

    

    我们探索了将基础模型应用于数据发现和探索任务中。基础模型是一种大型语言模型 (LLMs)，在各种与其训练无关的不同任务上表现出了良好的性能。我们表明这些模型在数据发现和数据探索领域非常适用。在谨慎使用的情况下，它们具有优越的能力，可以优化表格类检测、列类型注释和联接列预测这三种代表性任务。在这三个任务上，我们展示了基于基础模型的方法优于任务特定的模型和最先进的技术。此外，我们的方法通常超过人类专家的任务表现。这表明了将不同的数据管理任务统一在基础模型下的未来方向。

    We explore the application of foundation models to data discovery and exploration tasks. Foundation models are large language models (LLMs) that show promising performance on a range of diverse tasks unrelated to their training. We show that these models are highly applicable to the data discovery and data exploration domain. When carefully used, they have superior capability on three representative tasks: table-class detection, column-type annotation and join-column prediction. On all three tasks, we show that a foundation-model-based approach outperforms the task-specific models and so the state of the art. Further, our approach often surpasses human-expert task performance. This suggests a future direction in which disparate data management tasks can be unified under foundation models.
    
[^79]: 结构化图模型先验下的协作学习

    Structured Cooperative Learning with Graphical Model Priors. (arXiv:2306.09595v1 [cs.LG])

    [http://arxiv.org/abs/2306.09595](http://arxiv.org/abs/2306.09595)

    本文提出了结构化协作学习算法，在不同设备之间通过协作完成分散任务。通过图模型先验生成的协作图，算法可以自动捕捉设备之间的跨任务相关性。

    

    我们研究了如何在分散设备上对不同任务进行个性化建模，这些设备的局部数据受限。我们提出了“结构化协作学习（SCooL）”，其中一个跨设备的协作图由图模型先验生成，以自动协调设备之间的相互学习。通过选择施加不同结构的图模型，我们可以通过变分推断推导出一类丰富的现有和新型去中心化学习算法。特别地，我们展示了三种 SCooL 的示例，在其中以 Dirac 分布、随机块模型（SBM）和注意力作为生成协作图的先验。这些 EM 类型的算法通过更新协作图和协同本地模型学习之间进行交替，仅通过监视模型更新来优化协作图，从而可以自动捕捉设备之间的跨任务相关性。

    We study how to train personalized models for different tasks on decentralized devices with limited local data. We propose "Structured Cooperative Learning (SCooL)", in which a cooperation graph across devices is generated by a graphical model prior to automatically coordinate mutual learning between devices. By choosing graphical models enforcing different structures, we can derive a rich class of existing and novel decentralized learning algorithms via variational inference. In particular, we show three instantiations of SCooL that adopt Dirac distribution, stochastic block model (SBM), and attention as the prior generating cooperation graphs. These EM-type algorithms alternate between updating the cooperation graph and cooperative learning of local models. They can automatically capture the cross-task correlations among devices by only monitoring their model updating in order to optimize the cooperation graph. We evaluate SCooL and compare it with existing decentralized learning met
    
[^80]: FewSAR: 一种小样本合成孔径雷达图像分类基准

    FewSAR: A Few-shot SAR Image Classification Benchmark. (arXiv:2306.09592v1 [cs.CV])

    [http://arxiv.org/abs/2306.09592](http://arxiv.org/abs/2306.09592)

    为解决SAR目标图像分类缺乏统一基准问题，我们提出了FewSAR，一个包括15个经典方法的Python代码库可用于小样本SAR图像分类任务。

    

    小样本学习是图像分类领域中重要且困难的问题之一。然而，与可见光数据集的快速发展相比，合成孔径雷达（SAR）目标图像分类的进展要慢得多。缺乏统一的基准是这种现象的一个关键原因，在当前文献中可能被严重忽视。为了解决这个问题，我们提出了一种新颖的小样本SAR图像分类基准（FewSAR）。FewSAR包括一个由15个经典方法组成的Python代码库，并分为三个类别以用于小样本SAR图像分类。它为不同的小样本SAR图像分类任务提供了一个可访问和可定制的测试平台。

    Few-shot learning (FSL) is one of the significant and hard problems in the field of image classification. However, in contrast to the rapid development of the visible light dataset, the progress in SAR target image classification is much slower. The lack of unified benchmark is a key reason for this phenomenon, which may be severely overlooked by the current literature. The researchers of SAR target image classification always report their new results on their own datasets and experimental setup. It leads to inefficiency in result comparison and impedes the further progress of this area. Motivated by this observation, we propose a novel few-shot SAR image classification benchmark (FewSAR) to address this issue. FewSAR consists of an open-source Python code library of 15 classic methods in three categories for few-shot SAR image classification. It provides an accessible and customizable testbed for different few-shot SAR image classification task. To further understanding the performanc
    
[^81]: 理解在线学习中反馈的作用及其切换成本(arXiv:2306.09588v1 [cs.LG])

    Understanding the Role of Feedback in Online Learning with Switching Costs. (arXiv:2306.09588v1 [cs.LG])

    [http://arxiv.org/abs/2306.09588](http://arxiv.org/abs/2306.09588)

    本文研究了反馈在在线学习中的作用，发现在带有额外观测的情况下，当额外观测的数量超过一定阈值时，能减少后悔的最小值。

    

    本文研究了在线学习中反馈对切换成本的作用。已经证明，在赌博反馈下，极小化最大后悔为$\widetilde{\Theta}(T^{2/3})$，并在完全信息反馈下提高到$\widetilde{\Theta}(\sqrt{T})$，其中$T$为时间跨度的长度。然而，目前仍然不知道反馈的数量和类型通常如何影响后悔。为此，我们首先考虑了带有额外观测的赌博学习环境；也就是说，除了典型的赌博反馈外，学习者可以自由地进行总共$B_{ex}$次额外的观测。我们完全确定了这种情况下的最小后悔，它表现出有趣的相变现象：当$B_{ex}=O(T^{2/3})$时，后悔仍然为$\widetilde{\Theta}(T^{2/3})$，但当$B_{ex}=\Omega(T^{2/3})$时，它变为$\widetilde{\Theta}(T/\sqrt{B_{\mathrm{ex}}})$，随着预算$B_{ex}$的增加而得到改善。

    In this paper, we study the role of feedback in online learning with switching costs. It has been shown that the minimax regret is $\widetilde{\Theta}(T^{2/3})$ under bandit feedback and improves to $\widetilde{\Theta}(\sqrt{T})$ under full-information feedback, where $T$ is the length of the time horizon. However, it remains largely unknown how the amount and type of feedback generally impact regret. To this end, we first consider the setting of bandit learning with extra observations; that is, in addition to the typical bandit feedback, the learner can freely make a total of $B_{\mathrm{ex}}$ extra observations. We fully characterize the minimax regret in this setting, which exhibits an interesting phase-transition phenomenon: when $B_{\mathrm{ex}} = O(T^{2/3})$, the regret remains $\widetilde{\Theta}(T^{2/3})$, but when $B_{\mathrm{ex}} = \Omega(T^{2/3})$, it becomes $\widetilde{\Theta}(T/\sqrt{B_{\mathrm{ex}}})$, which improves as the budget $B_{\mathrm{ex}}$ increases. To design a
    
[^82]: 一个置信集的数量是否是一种衡量认知不确定性的好方法？

    Is the Volume of a Credal Set a Good Measure for Epistemic Uncertainty?. (arXiv:2306.09586v1 [cs.LG])

    [http://arxiv.org/abs/2306.09586](http://arxiv.org/abs/2306.09586)

    本文探讨了将不确定性表示为一个置信集而非单一概率分布的方法。并发现，在二元分类中，信任集的体积是一种有意义的衡量认知不确定性的方法，但在多类分类中则没有这种效果。

    

    充分的不确定性表示和量化在各种科学学科中变得非常重要，特别是在机器学习和人工智能领域。作为表示不确定性的一种替代方法，我们考虑信任集（一组概率分布的凸集）。信任集的几何表示作为$d$维多面体意味着对（认知）不确定性的几何直觉。在本文中，我们展示了在二元分类的情况下，信任集的几何表示的体积是认知不确定性的一种有意义的度量方法，但在多类分类时则不那么有效。我们的理论发现强调了在机器学习中指定和使用正确的不确定性度量方法以及意识到可能的风险的关键作用。

    Adequate uncertainty representation and quantification have become imperative in various scientific disciplines, especially in machine learning and artificial intelligence. As an alternative to representing uncertainty via one single probability measure, we consider credal sets (convex sets of probability measures). The geometric representation of credal sets as $d$-dimensional polytopes implies a geometric intuition about (epistemic) uncertainty. In this paper, we show that the volume of the geometric representation of a credal set is a meaningful measure of epistemic uncertainty in the case of binary classification, but less so for multi-class classification. Our theoretical findings highlight the crucial role of specifying and employing uncertainty measures in machine learning in an appropriate way, and for being aware of possible pitfalls.
    
[^83]: 基于模糊逻辑的密钥密码学变换模糊特征选择

    Fuzzy Feature Selection with Key-based Cryptographic Transformations. (arXiv:2306.09583v1 [cs.CR])

    [http://arxiv.org/abs/2306.09583](http://arxiv.org/abs/2306.09583)

    本文提出了一种基于模糊逻辑的模糊特征选择方法，可用于选择最有效地完成密码学变换过程的特征子集，旨在提高密码系统的安全性和效率。

    

    在密码学领域中，选择相关特征在增强密码算法安全性和效率方面扮演着至关重要的角色。本文提出了一种新颖的方法，将模糊特征选择应用于基于密钥的密码学变换。提出的模糊特征选择利用模糊逻辑的优势来识别和选择最有效地contributi完成密码学变换过程的特征子集。通过将模糊特征选择纳入基于密钥的密码学变换中，该研究旨在提高抵抗攻击的能力并增强密码系统的整体性能。实验评估可以证明所提出的方法在选择安全密钥特征时具有最小的计算开销。本文强调了模糊特征选择作为设计和优化基于密钥的密码算法的有价值工具的潜力。

    In the field of cryptography, the selection of relevant features plays a crucial role in enhancing the security and efficiency of cryptographic algorithms. This paper presents a novel approach of applying fuzzy feature selection to key-based cryptographic transformations. The proposed fuzzy feature selection leverages the power of fuzzy logic to identify and select optimal subsets of features that contribute most effectively to the cryptographic transformation process. By incorporating fuzzy feature selection into key-based cryptographic transformations, this research aims to improve the resistance against attacks and enhance the overall performance of cryptographic systems. Experimental evaluations may demonstrate the effectiveness of the proposed approach in selecting secure key features with minimal computational overhead. This paper highlights the potential of fuzzy feature selection as a valuable tool in the design and optimization of key-based cryptographic algorithms, contributi
    
[^84]: 通过在线灵敏度抽样实现具探索性的低切换策略梯度

    Low-Switching Policy Gradient with Exploration via Online Sensitivity Sampling. (arXiv:2306.09554v1 [cs.LG])

    [http://arxiv.org/abs/2306.09554](http://arxiv.org/abs/2306.09554)

    本文提出了一个具有探索和样本复杂度低的策略优化算法LPO，使用有界eluder-维数和在线灵敏度抽样来适用于非线性参数化的策略，并且在较少的样本量下获得了近似最优策略。

    

    策略优化方法是增强学习中强大的算法之一，其具备对策略参数化的灵活性和处理模型错误的能力。然而，这些方法通常存在收敛速度缓慢，样本复杂度低的缺点。因此，设计可证明的策略优化算法对于提高样本效率至关重要。而针对这个问题的最新进展只适用于表格和线性设置，且这些成果的良好结构不能推广到非线性参数化的策略。本文利用最近价值算法包括有界eluder-维数和在线灵敏度抽样等的进展，针对一般非线性函数逼近设计低切换样本有效的策略优化算法LPO。我们证明了我们的算法只需$\widetilde{O}(\frac{\text{poly}(d)}{\varepsilon^3})$个样本即可获得$\varepsilon$-最优策略。

    Policy optimization methods are powerful algorithms in Reinforcement Learning (RL) for their flexibility to deal with policy parameterization and ability to handle model misspecification. However, these methods usually suffer from slow convergence rates and poor sample complexity. Hence it is important to design provably sample efficient algorithms for policy optimization. Yet, recent advances for this problems have only been successful in tabular and linear setting, whose benign structures cannot be generalized to non-linearly parameterized policies. In this paper, we address this problem by leveraging recent advances in value-based algorithms, including bounded eluder-dimension and online sensitivity sampling, to design a low-switching sample-efficient policy optimization algorithm, LPO, with general non-linear function approximation. We show that, our algorithm obtains an $\varepsilon$-optimal policy with only $\widetilde{O}(\frac{\text{poly}(d)}{\varepsilon^3})$ samples, where $\va
    
[^85]: QH9：QM9分子的量子哈密顿预测基准测试

    QH9: A Quantum Hamiltonian Prediction Benchmark for QM9 Molecules. (arXiv:2306.09549v1 [physics.chem-ph])

    [http://arxiv.org/abs/2306.09549](http://arxiv.org/abs/2306.09549)

    该论文提出了一种新的量子哈密顿数据集QH9，用于为各种分子提供精确的哈密顿矩阵。通过设计基准任务，展示了当前机器学习模型有能力预测任意分子的哈密顿矩阵。

    

    监督式机器学习方法越来越被用于加速电子结构预测，作为第一性原理计算方法（如密度泛函理论（DFT））的替代品。虽然许多量子化学数据集侧重于化学性质和原子力，但准确且高效地预测哈密顿矩阵的能力是非常重要和基本的物理量，它确定了物理系统和化学性质的量子状态。在这项工作中，我们生成了一个新的量子哈密顿数据集，命名为QH9，基于QM9数据集为2,399个分子动力学轨迹和130,831个稳定分子几何形态提供精确的哈密顿矩阵。通过设计各种分子的基准任务，我们展示了当前机器学习模型有能力预测任意分子的哈密顿矩阵。QH9数据集和基准模型都提供。

    Supervised machine learning approaches have been increasingly used in accelerating electronic structure prediction as surrogates of first-principle computational methods, such as density functional theory (DFT). While numerous quantum chemistry datasets focus on chemical properties and atomic forces, the ability to achieve accurate and efficient prediction of the Hamiltonian matrix is highly desired, as it is the most important and fundamental physical quantity that determines the quantum states of physical systems and chemical properties. In this work, we generate a new Quantum Hamiltonian dataset, named as QH9, to provide precise Hamiltonian matrices for 2,399 molecular dynamics trajectories and 130,831 stable molecular geometries, based on the QM9 dataset. By designing benchmark tasks with various molecules, we show that current machine learning models have the capacity to predict Hamiltonian matrices for arbitrary molecules. Both the QH9 dataset and the baseline models are provided
    
[^86]: 在线重尾变点检测

    Online Heavy-tailed Change-point detection. (arXiv:2306.09548v1 [stat.ML])

    [http://arxiv.org/abs/2306.09548](http://arxiv.org/abs/2306.09548)

    本文提出了一种在线变点检测算法，可以应对重尾分布且保证有限的假阳性率。

    

    我们研究了在线变点检测 (OCPD) 的算法，其中样本可能是重尾分布，一个接一个地呈现，并且必须尽早检测到底层均值的变化。我们提出了一种基于裁剪随机梯度下降 (SGD) 的算法，即使我们仅假定数据生成过程的第二阶矩有界，该算法也能正常工作。我们派生了在所有具有有界第二矩的分布族中最坏情况下的有限样本假阳性率 (FPR) 的保证。因此，我们的方法是第一个保证有限样本 FPR 的 OCPD 算法，即使数据是高维的，底层分布是重尾的。我们论文的技术贡献是展示了裁剪 SGD 可以估计随机向量的均值并同时在所有置信度值上提供置信度界限。我们将这个稳健的估计与并集边界论证相结合，构建一个有限的顺序变点算法。

    We study algorithms for online change-point detection (OCPD), where samples that are potentially heavy-tailed, are presented one at a time and a change in the underlying mean must be detected as early as possible. We present an algorithm based on clipped Stochastic Gradient Descent (SGD), that works even if we only assume that the second moment of the data generating process is bounded. We derive guarantees on worst-case, finite-sample false-positive rate (FPR) over the family of all distributions with bounded second moment. Thus, our method is the first OCPD algorithm that guarantees finite-sample FPR, even if the data is high dimensional and the underlying distributions are heavy-tailed. The technical contribution of our paper is to show that clipped-SGD can estimate the mean of a random vector and simultaneously provide confidence bounds at all confidence values. We combine this robust estimate with a union bound argument and construct a sequential change-point algorithm with finite
    
[^87]: 从隐私化数据中训练生成模型

    Training generative models from privatized data. (arXiv:2306.09547v1 [cs.LG])

    [http://arxiv.org/abs/2306.09547](http://arxiv.org/abs/2306.09547)

    介绍了一种在隐私化数据上训练GAN的框架，使用熵正则化Wasserstein距离去噪可以缓解正则化偏差和隐私化噪声的影响，提高模型有效性。

    

    本文介绍了一种在差分隐私化数据上训练生成式对抗网络（GAN）的框架，其中采用了常见的加性噪声机制（如拉普拉斯噪声和高斯噪声）对数据进行差分隐私保护。我们展示了当使用熵正则化Wasserstein距离来去噪数据分布时，这种方法可以唯一地缓解正则化偏差和隐私化噪声的影响，从而提高模型的有效性。同时，我们还分析了所提出的方法并提供了样本复杂度结果和实验证据以支持其可靠性。

    Local differential privacy (LDP) is a powerful method for privacy-preserving data collection. In this paper, we develop a framework for training Generative Adversarial Networks (GAN) on differentially privatized data. We show that entropic regularization of the Wasserstein distance -- a popular regularization method in the literature that has been often leveraged for its computational benefits -- can be used to denoise the data distribution when data is privatized by common additive noise mechanisms, such as Laplace and Gaussian. This combination uniquely enables the mitigation of both the regularization bias and the effects of privatization noise, thereby enhancing the overall efficacy of the model. We analyse the proposed method, provide sample complexity results and experimental evidence to support its efficacy.
    
[^88]: 块状态变换器

    Block-State Transformer. (arXiv:2306.09539v1 [cs.CL])

    [http://arxiv.org/abs/2306.09539](http://arxiv.org/abs/2306.09539)

    本文提出了一种名为块状态变换器（BST）的混合神经网络层，结合了状态空间模型和块变换器，旨在在语言建模任务中提高性能和可扩展性。实验证明，该模型在语言建模困惑度上优于类似的基于Transformer的架构，并且可以推广到更长的序列。此外，在层级别上具有超过十倍的速度提升。

    

    状态空间模型（SSM）在需要建模长期依赖性并且需要高效扩展到长序列的任务中显示出了惊人的效果。尽管最初是为连续信号设计的，但SSM在视觉和音频等许多任务中表现出了优异的性能；然而，在语言建模任务中，SSM仍然落后于Transformers的性能。在这项工作中，我们提出了一个名为块状态变换器（BST）的混合层，它在内部组合了一个用于长距离上下文化的SSM子层和一个用于短期序列表示的块变换器子层。我们研究了三种不同的、完全可并行的集成SSM和块注意力的变体。我们证明了我们的模型在语言建模的困惑度上优于类似的基于Transformer的架构，并且可以推广到更长的序列。此外，块状态变换器在层级别上具有超过十倍的速度提升。

    State space models (SSMs) have shown impressive results on tasks that require modeling long-range dependencies and efficiently scale to long sequences owing to their subquadratic runtime complexity. Originally designed for continuous signals, SSMs have shown superior performance on a plethora of tasks, in vision and audio; however, SSMs still lag Transformer performance in Language Modeling tasks. In this work, we propose a hybrid layer named Block-State Transformer (BST), that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sublayer for short-term representation of sequences. We study three different, and completely parallelizable, variants that integrate SSMs and block-wise attention. We show that our model outperforms similar Transformer-based architectures on language modeling perplexity and generalizes to longer sequences. In addition, the Block-State Transformer demonstrates more than tenfold increase in speed at the layer level compa
    
[^89]: 残差 Q 学习：无需价值的在线和离线策略定制

    Residual Q-Learning: Offline and Online Policy Customization without Value. (arXiv:2306.09526v1 [cs.LG])

    [http://arxiv.org/abs/2306.09526](http://arxiv.org/abs/2306.09526)

    该研究提出了一种新方法，使用动态控制残差的 Q 学习来进行离线和在线的策略定制，无需使用价值函数。

    

    模仿学习是一种广泛使用的框架，适用于从演示中学习模仿行为。当手工制作奖励函数困难或目标是模仿人类专家行为时，这种方法特别有吸引力。但是，学习的模仿策略只能遵循演示中的行为。在应用模仿策略时，我们可能需要根据不同的下游任务要求定制策略行为。同时，我们仍希望定制的策略保持其模仿性质。为此，我们提出了一种新的问题设置，称为策略定制。它将学习任务定义为训练一种策略，该策略继承先前策略的特性，同时满足目标下游任务强加的一些附加要求。我们提出了一种新颖和有原则的方法来解释和确定两个任务目标之间的权衡。具体而言，我们制定了一种动态控制残差的 Q 学习方法，该方法可以在不使用价值函数的情况下进行在线和离线策略定制。

    Imitation Learning (IL) is a widely used framework for learning imitative behavior from demonstrations. It is especially appealing for solving complex real-world tasks where handcrafting reward function is difficult, or when the goal is to mimic human expert behavior. However, the learned imitative policy can only follow the behavior in the demonstration. When applying the imitative policy, we may need to customize the policy behavior to meet different requirements coming from diverse downstream tasks. Meanwhile, we still want the customized policy to maintain its imitative nature. To this end, we formulate a new problem setting called policy customization. It defines the learning task as training a policy that inherits the characteristics of the prior policy while satisfying some additional requirements imposed by a target downstream task. We propose a novel and principled approach to interpret and determine the trade-off between the two task objectives. Specifically, we formulate the
    
[^90]: 针对潜在混淆下的因果结果的更紧密预测区间

    Tighter Prediction Intervals for Causal Outcomes Under Hidden Confounding. (arXiv:2306.09520v1 [cs.LG])

    [http://arxiv.org/abs/2306.09520](http://arxiv.org/abs/2306.09520)

    本文提出了一种名为Caus-Modens的算法，通过调制集合来描述因果结果区间，相比符合性预测方法，能够在实践中给出更紧密的结果区间。

    

    在存在隐藏混淆因素的情况下进行确切个体治疗结果的因果推断很少可能。因此，最近的研究改进了符合性预测方法，以产生结果区间。不幸的是，这类方法往往过于保守，有时会给出无信息量的区间。我们介绍了一种另类方法Caus-Modens，用于通过调制集合来描述因果结果区间。受到贝叶斯统计和集成不确定性量化的启发，Caus-Modens在实践中给出更紧密的结果区间，并通过三个分离基准测试的必要区间大小来实现足够的覆盖率。最后一个基准是使用未知但可探明的基础事实开展观察实验的GPT-4的新型用途。

    Causal inference of exact individual treatment outcomes in the presence of hidden confounders is rarely possible. Instead, recent work has adapted conformal prediction to produce outcome intervals. Unfortunately this family of methods tends to be overly conservative, sometimes giving uninformative intervals. We introduce an alternative approach termed Caus-Modens, for characterizing causal outcome intervals by modulated ensembles. Motivated from Bayesian statistics and ensembled uncertainty quantification, Caus-Modens gives tighter outcome intervals in practice, measured by the necessary interval size to achieve sufficient coverage on three separate benchmarks. The last benchmark is a novel usage of GPT-4 for observational experiments with unknown but probeable ground truth.
    
[^91]: 关系感知网络基于注意力损失的小样本知识图谱补全

    Relation-Aware Network with Attention-Based Loss for Few-Shot Knowledge Graph Completion. (arXiv:2306.09519v1 [cs.CL])

    [http://arxiv.org/abs/2306.09519](http://arxiv.org/abs/2306.09519)

    本文提出了一种新颖的RANA框架，利用有策略地选择相关负样本和设计基于注意力机制的损失函数来更好地利用负样本并缓解零损失问题，同时设计了一种动态的关系感知实体编码来捕获不同关系下实体的不同表示。

    

    小样本知识图谱补全旨在利用少量参考实体对预测关系的未见事实。现有方法随机选择一个负采样来最小化基于边界的排名损失，但这容易导致零损失问题。此外，实体在不同的上下文中应该具有不同的表征。为了解决这些问题，我们提出了一种新颖的关系感知网络基于注意力损失的框架。具体而言，我们通过有策略地选择相关负样本和设计基于注意力机制的损失函数来更好地利用丰富的负样本并缓解零损失问题。直觉上，与正样本更相似的负样本将对模型贡献更大。此外，我们设计了一种动态的关系感知实体编码来捕捉不同关系下实体的不同表示。三个基准数据集上的实验结果表明，相比最先进的方法，所提出的RANA框架的有效性。

    Few-shot knowledge graph completion (FKGC) task aims to predict unseen facts of a relation with few-shot reference entity pairs. Current approaches randomly select one negative sample for each reference entity pair to minimize a margin-based ranking loss, which easily leads to a zero-loss problem if the negative sample is far away from the positive sample and then out of the margin. Moreover, the entity should have a different representation under a different context. To tackle these issues, we propose a novel Relation-Aware Network with Attention-Based Loss (RANA) framework. Specifically, to better utilize the plentiful negative samples and alleviate the zero-loss issue, we strategically select relevant negative samples and design an attention-based loss function to further differentiate the importance of each negative sample. The intuition is that negative samples more similar to positive samples will contribute more to the model. Further, we design a dynamic relation-aware entity en
    
[^92]: 风力涡轮机发电机加热故障检测的混合特征选取和构造方法

    A Hybrid Feature Selection and Construction Method for Detection of Wind Turbine Generator Heating Faults. (arXiv:2306.09491v1 [cs.LG])

    [http://arxiv.org/abs/2306.09491](http://arxiv.org/abs/2306.09491)

    该论文提出了一种用于检测风力涡轮机发电机加热故障的混合特征选取和构造方法，旨在通过特征构造和选取提高分类精度和降低计算负担。

    

    信息预处理是机器学习应用有效设计的关键步骤。特征构造和选取是实现这一目的的强大技术。本文提出了一种用于检测风力涡轮机发电机加热故障的特征选取和构造方法。该方法通过从监控控制和数据采集（SCADA）系统收集数据，建立包含风力特征、操作数据、温度测量和状态信息的原始特征，并在特征构造步骤中创建新特征以获得更有力的故障指示信息。构造新特征后，采用混合特征选取技术在整个特征集中找出最相关的特征，以提高分类精度和降低计算负担。

    Preprocessing of information is an essential step for the effective design of machine learning applications. Feature construction and selection are powerful techniques used for this aim. In this paper, a feature selection and construction approach is presented for the detection of wind turbine generator heating faults. Data were collected from Supervisory Control and Data Acquisition (SCADA) system of a wind turbine. The original features directly collected from the data collection system consist of wind characteristics, operational data, temperature measurements and status information. In addition to these original features, new features were created in the feature construction step to obtain information that can be more powerful indications of the faults. After the construction of new features, a hybrid feature selection technique was implemented to find out the most relevant features in the overall set to increase the classification accuracy and decrease the computational burden. Fe
    
[^93]: 基于深度强化学习的注意力式Open RAN切片管理

    Attention-based Open RAN Slice Management using Deep Reinforcement Learning. (arXiv:2306.09490v1 [cs.DC])

    [http://arxiv.org/abs/2306.09490](http://arxiv.org/abs/2306.09490)

    本文提出了一种创新的基于注意力的深度强化学习技术，利用O-RAN分离模块和分布式代理合作来优化网络切片管理。

    

    随着Open Radio Access Network（O-RAN）和5G等新兴网络的不断发展，对具有不同需求的各种服务的需求也在增加。网络切片已经成为解决不同服务需求的潜在方案。然而，在动态环境下管理网络切片并保持服务质量（QoS）是一项具有挑战性的任务。利用机器学习（ML）方法进行动态网络的最优控制，可以通过防止服务级别协议（SLA）违规，提高网络性能，这对于可靠的决策和满足新兴网络的需求非常重要。虽然基于RL的控制方法可以有效地实现网络QoS的实时监控和控制，但需要泛化来提高决策的可靠性。本文介绍了一种创新的基于注意力的深度强化学习（ADRL）技术，利用O-RAN分离模块和分布式代理合作来实现更好的切片管理。

    As emerging networks such as Open Radio Access Networks (O-RAN) and 5G continue to grow, the demand for various services with different requirements is increasing. Network slicing has emerged as a potential solution to address the different service requirements. However, managing network slices while maintaining quality of services (QoS) in dynamic environments is a challenging task. Utilizing machine learning (ML) approaches for optimal control of dynamic networks can enhance network performance by preventing Service Level Agreement (SLA) violations. This is critical for dependable decision-making and satisfying the needs of emerging networks. Although RL-based control methods are effective for real-time monitoring and controlling network QoS, generalization is necessary to improve decision-making reliability. This paper introduces an innovative attention-based deep RL (ADRL) technique that leverages the O-RAN disaggregated modules and distributed agent cooperation to achieve better p
    
[^94]: FedMultimodal：面向多模态联邦学习的基准测试

    FedMultimodal: A Benchmark For Multimodal Federated Learning. (arXiv:2306.09486v1 [cs.DC])

    [http://arxiv.org/abs/2306.09486](http://arxiv.org/abs/2306.09486)

    FedMultimodal 是面向多模态联邦学习的基准测试，为此我们提出了第一个覆盖五种不同应用领域和十个社区的FL基准测试，以便促进多模态FL研究。

    

    近几年来，联邦学习（FL）已成为一种应对数据隐私挑战的新兴机器学习技术。在FL算法中，客户端提交本地训练模型，服务器将这些参数进行聚合直至收敛。尽管在计算机视觉、音频和自然语言处理等领域，对FL进行了重大努力，但是利用多模态数据流的FL应用仍然没有得到广泛探索。众所周知，多模态学习在情感识别、医疗保健、多媒体和社交媒体等领域具有广泛的现实应用，而用户隐私仍然是一个重要问题。具体来说，目前还没有针对多模态应用或相关任务的现有FL基准测试。为了促进多模态FL研究，我们介绍了FedMultimodal，这是第一个覆盖十个社区中的五个代表性多模态应用的FL基准测试。

    Over the past few years, Federated Learning (FL) has become an emerging machine learning technique to tackle data privacy challenges through collaborative training. In the Federated Learning algorithm, the clients submit a locally trained model, and the server aggregates these parameters until convergence. Despite significant efforts that have been made to FL in fields like computer vision, audio, and natural language processing, the FL applications utilizing multimodal data streams remain largely unexplored. It is known that multimodal learning has broad real-world applications in emotion recognition, healthcare, multimedia, and social media, while user privacy persists as a critical concern. Specifically, there are no existing FL benchmarks targeting multimodal applications or related tasks. In order to facilitate the research in multimodal FL, we introduce FedMultimodal, the first FL benchmark for multimodal learning covering five representative multimodal applications from ten comm
    
[^95]: R2-Diff: 基于图像的运动预测中一种以扩散为基础的，以检索运动为优化的降噪方法

    R2-Diff: Denoising by diffusion as a refinement of retrieved motion for image-based motion prediction. (arXiv:2306.09483v1 [cs.CV])

    [http://arxiv.org/abs/2306.09483](http://arxiv.org/abs/2306.09483)

    R2-Diff 是基于图像相似度检索运动后，采用扩散模型进行降噪优化的基于图像的运动预测方法，可以更好地预测上下文相关的运动。

    

    基于图像的运动预测是机器人操作中必不可少的技术之一。在各种预测模型中，我们将重点放在了扩散模型上，因为它们在各种应用中都取得了最先进的性能。扩散模型通过逐渐去噪图像上下文中的随机高斯噪声，以随机方式预测上下文相关的运动。虽然扩散模型能够通过改变随机噪声预测各种各样的运动，但有时它们不能基于图像上下文预测出恰当的运动，因为随机噪声是独立于图像上下文采样的。为了解决这个问题，我们提出了 R2-Diff。在 R2-Diff 中，从基于图像相似度的数据集中检索出一种运动，将其输入到扩散模型而不是随机噪声中。然后，经过扩散模型的去噪流程对检索到的运动进行改进。由于检索到的运动几乎适合于上下文，所以它变得更容易理解。

    Image-based motion prediction is one of the essential techniques for robot manipulation. Among the various prediction models, we focus on diffusion models because they have achieved state-of-the-art performance in various applications. In image-based motion prediction, diffusion models stochastically predict contextually appropriate motion by gradually denoising random Gaussian noise based on the image context. While diffusion models are able to predict various motions by changing the random noise, they sometimes fail to predict a contextually appropriate motion based on the image because the random noise is sampled independently of the image context. To solve this problem, we propose R2-Diff. In R2-Diff, a motion retrieved from a dataset based on image similarity is fed into a diffusion model instead of random noise. Then, the retrieved motion is refined through the denoising process of the diffusion model. Since the retrieved motion is almost appropriate to the context, it becomes ea
    
[^96]: 基于余数系统设计高精度模拟深度神经网络加速器的研究

    Leveraging Residue Number System for Designing High-Precision Analog Deep Neural Network Accelerators. (arXiv:2306.09481v1 [cs.AR])

    [http://arxiv.org/abs/2306.09481](http://arxiv.org/abs/2306.09481)

    本文提出使用余数系统（RNS）来消除ADC有限精度带来的信息丢失，从多个低精度运算组成高精度运算，实现高准确性和良好能源效率的模拟DNN加速器，并建议使用冗余的RNS来实现容错性。同时RNS相对于常规定点方法能够将数据转换器的能耗降低数个数量级。

    

    模拟DNN加速器的高精度数据转换器成本高，同时要保持良好的能源效率和高准确性是具有挑战性的。本文利用余数系统（RNS）从多个低精度运算组成高精度运算，克服了这一挑战。这使我们能够消除由ADC有限精度引起的信息丢失。我们的研究表明，RNS可使用仅具有6比特精度的数据转换器来实现最新DNN推理的99％FP32准确度，并建议使用冗余的RNS来实现容错的模拟加速器。此外，我们还显示RNS相对于常规的定点方法，可以将模拟加速器中数据转换器的能耗降低数个数量级。

    Achieving high accuracy, while maintaining good energy efficiency, in analog DNN accelerators is challenging as high-precision data converters are expensive. In this paper, we overcome this challenge by using the residue number system (RNS) to compose high-precision operations from multiple low-precision operations. This enables us to eliminate the information loss caused by the limited precision of the ADCs. Our study shows that RNS can achieve 99% FP32 accuracy for state-of-the-art DNN inference using data converters with only $6$-bit precision. We propose using redundant RNS to achieve a fault-tolerant analog accelerator. In addition, we show that RNS can reduce the energy consumption of the data converters within an analog accelerator by several orders of magnitude compared to a regular fixed-point approach.
    
[^97]: 理解和减轻物理感知神经网络的外推失效

    Understanding and Mitigating Extrapolation Failures in Physics-Informed Neural Networks. (arXiv:2306.09478v1 [cs.LG])

    [http://arxiv.org/abs/2306.09478](http://arxiv.org/abs/2306.09478)

    本文研究了物理感知神经网络（PINNs）在外推场景下的行为，并提供了降低外推误差的策略。研究结果表明，一旦模型内插误差为零，进一步增加架构大小或采样点数量对于外推行为没有影响。

    

    由于其使用深度神经网络有效逼近偏微分方程（PDE）的能力，物理感知神经网络（PINN）最近在科学界备受青睐。然而，它们的应用通常被局限于内插场景，其中预测依赖于在训练集支持内的输入。在现实世界的应用中，通常需要外推，但是PINNs的域外行为尚未得到充分研究。在本文中，我们对PINNs的外推行为进行了详细研究，并提供了针对几个先前的假设的证据：我们研究了不同模型选择对外推的影响，并发现一旦模型能够达到零内插误差，进一步增加架构大小或采样点数量对外推行为没有影响。我们还展示了对于一些PDE，PINNs在外推中的表现几乎与内插一样好。通过分析数据的Fourier和Laplace谱，我们提供了对观察到行为的解释。最后，我们提出并评估了不同的策略来减少外推误差，其中最有效的策略涉及修改训练损失函数以强调外推准确性。

    Physics-informed Neural Networks (PINNs) have recently gained popularity in the scientific community due to their effective approximation of partial differential equations (PDEs) using deep neural networks. However, their application has been generally limited to interpolation scenarios, where predictions rely on inputs within the support of the training set. In real-world applications, extrapolation is often required, but the out of domain behavior of PINNs is understudied. In this paper, we provide a detailed investigation of PINNs' extrapolation behavior and provide evidence against several previously held assumptions: we study the effects of different model choices on extrapolation and find that once the model can achieve zero interpolation error, further increases in architecture size or in the number of points sampled have no effect on extrapolation behavior. We also show that for some PDEs, PINNs perform nearly as well in extrapolation as in interpolation. By analyzing the Fouri
    
[^98]: FFB:面向处理组公平方法的公平公正基准

    FFB: A Fair Fairness Benchmark for In-Processing Group Fairness Methods. (arXiv:2306.09468v1 [cs.LG])

    [http://arxiv.org/abs/2306.09468](http://arxiv.org/abs/2306.09468)

    本文提出了针对处理中组公平方法的公平公正基准框架（FFB），并进行了全面分析。该工作的关键贡献包括提供灵活、可扩展、极简和面向研究的开源代码；建立统一的公平方法基准测试流水线；进行广泛的基准测试，从 $\mathbf{45,079}$ 个实验中获取关键见解。

    

    本文介绍了公平公正基准（FFB），这是一种针对处理中组公平方法的基准框架。确保机器学习的公平性对于符合道德和法律要求至关重要。然而，由于实验设置的不一致，缺乏易于访问的算法实现以及当前公平度量工具的有限可扩展性，存在比较和开发公平度量方法的挑战。为了解决这些问题，我们介绍了一个开源、标准化的基准，用于评估处理中的组公平方法，并提供了对确保不同民族/种族群体公平的最先进方法的全面分析。该工作提供了以下关键贡献：提供灵活、可扩展、极简和面向研究的开源代码；建立统一的公平方法基准测试流水线；进行广泛的基准测试，从 $\mathbf{45,079}$ 个实验中获取关键见解。

    This paper introduces the Fair Fairness Benchmark (\textsf{FFB}), a benchmarking framework for in-processing group fairness methods. Ensuring fairness in machine learning is critical for ethical and legal compliance. However, there exist challenges in comparing and developing of fairness methods due to inconsistencies in experimental settings, lack of accessible algorithmic implementations, and limited extensibility of current fairness packages and tools. To address these issues, we introduce an open-source, standardized benchmark for evaluating in-processing group fairness methods and provide a comprehensive analysis of state-of-the-art methods to ensure different notions of group fairness. This work offers the following key contributions: the provision of flexible, extensible, minimalistic, and research-oriented open-source code; the establishment of unified fairness method benchmarking pipelines; and extensive benchmarking, which yields key insights from $\mathbf{45,079}$ experiment
    
[^99]: AQuA：一种用于标签质量评估的基准测试工具

    AQuA: A Benchmarking Tool for Label Quality Assessment. (arXiv:2306.09467v1 [cs.LG])

    [http://arxiv.org/abs/2306.09467](http://arxiv.org/abs/2306.09467)

    AQuA是一款用于标签质量评估的基准测试工具，目的是评估标签噪声存在下的机器学习方法。该基准测试环境包括数据模拟、质量不同的真实数据集和几种最先进的标签噪声消除方法。

    

    机器学习模型的好坏取决于用于训练它们的数据。然而，最近的研究发现，被广泛用于训练和评估机器学习模型的数据集，例如ImageNet，存在普遍的标注错误。训练集中的错误标签会削弱机器学习模型的泛化性能，并影响使用测试集进行模型选择和评估。因此，在存在标签误差的情况下学习是研究的一个活跃领域，但该领域缺乏一个全面的基准来评估这些方法。为此，我们提出了AQuA这个基准测试环境，以严格评估在标签噪声存在的情况下实现机器学习的方法。

    Machine learning (ML) models are only as good as the data they are trained on. But recent studies have found datasets widely used to train and evaluate ML models, e.g. ImageNet, to have pervasive labeling errors. Erroneous labels on the train set hurt ML models' ability to generalize, and they impact evaluation and model selection using the test set. Consequently, learning in the presence of labeling errors is an active area of research, yet this field lacks a comprehensive benchmark to evaluate these methods. Most of these methods are evaluated on a few computer vision datasets with significant variance in the experimental protocols. With such a large pool of methods and inconsistent evaluation, it is also unclear how ML practitioners can choose the right models to assess label quality in their data. To this end, we propose a benchmarking environment AQuA to rigorously evaluate methods that enable machine learning in the presence of label noise. We also introduce a design space to del
    
[^100]: 简化后的时态一致性强化学习

    Simplified Temporal Consistency Reinforcement Learning. (arXiv:2306.09466v1 [cs.LG])

    [http://arxiv.org/abs/2306.09466](http://arxiv.org/abs/2306.09466)

    本文表明，一种仅依靠潜在动力学模型的潜在时间一致性训练的简单表示学习方法，可以实现高性能的强化学习，同时在纯规划过程中和模型无关的强化学习中都适用。在实验中，该方法解决了挑战性的高维奔跑任务，并且训练速度比基于集合的方法要快4.1倍。

    

    强化学习能够解决复杂的序贯决策任务，但目前仍存在样本效率和所需计算量等限制。为提高样本效率，最近的工作集中于基于模型的强化学习，其将模型学习与规划交错进行。最近的一些方法进一步利用策略学习、价值估计和自监督学习作为辅助目标。本文表明，令人惊讶的是，一种只依靠通过潜在时间一致性训练的潜在动力学模型的简单表示学习方法就足以实现高性能的强化学习。这适用于纯规划过程中利用被表示条件的动力学模型，同时也适用于在模型无关的强化学习中利用该表示作为策略和价值函数特征。在实验中，我们的方法通过在线规划器学习了准确的动力学模型，解决了挑战性的高维奔跑任务，而其训练速度比基于集合的方法要快4.1倍。

    Reinforcement learning is able to solve complex sequential decision-making tasks but is currently limited by sample efficiency and required computation. To improve sample efficiency, recent work focuses on model-based RL which interleaves model learning with planning. Recent methods further utilize policy learning, value estimation, and, self-supervised learning as auxiliary objectives. In this paper we show that, surprisingly, a simple representation learning approach relying only on a latent dynamics model trained by latent temporal consistency is sufficient for high-performance RL. This applies when using pure planning with a dynamics model conditioned on the representation, but, also when utilizing the representation as policy and value function features in model-free RL. In experiments, our approach learns an accurate dynamics model to solve challenging high-dimensional locomotion tasks with online planners while being 4.1 times faster to train compared to ensemble-based methods. 
    
[^101]: Kriging卷积网络

    Kriging Convolutional Networks. (arXiv:2306.09463v1 [cs.LG])

    [http://arxiv.org/abs/2306.09463](http://arxiv.org/abs/2306.09463)

    该研究介绍了一种新的Kriging卷积网络方法，结合了Kriging和图卷积网络的优点，并在多个应用中表现出了更好的性能。

    

    空间插值是一类估计问题，其中已知值的位置用于估计其他位置的值，着重于利用空间局部性和趋势。传统的Kriging方法具有强烈的高斯假设，因此常常无法捕捉数据内部的复杂性。受最近图神经网络进展的启发，我们介绍了Kriging卷积网络（KCN），它将图卷积网络（GCN）和Kriging的优点相结合。与标准GCN相比，KCN在产生预测时直接利用相邻观测值。此外，KCN还将Kriging方法作为特定配置包含在内。我们通过添加注意力来进一步提高模型的性能。通过实验证明，这种模型在几个应用中的性能优于GCN和Kriging。使用PyTorch实现的KCN在GitHub存储库上公开：https://github.com/tufts-ml/kcn-torch。

    Spatial interpolation is a class of estimation problems where locations with known values are used to estimate values at other locations, with an emphasis on harnessing spatial locality and trends. Traditional Kriging methods have strong Gaussian assumptions, and as a result, often fail to capture complexities within the data. Inspired by the recent progress of graph neural networks, we introduce Kriging Convolutional Networks (KCN), a method of combining the advantages of Graph Convolutional Networks (GCN) and Kriging. Compared to standard GCNs, KCNs make direct use of neighboring observations when generating predictions. KCNs also contain the Kriging method as a specific configuration. We further improve the model's performance by adding attention. Empirically, we show that this model outperforms GCNs and Kriging in several applications. The implementation of KCN using PyTorch is publicized at the GitHub repository: https://github.com/tufts-ml/kcn-torch.
    
[^102]: 自动驾驶车辆的行驶舒适优化：概念、方法和技术

    Motion Comfort Optimization for Autonomous Vehicles: Concepts, Methods, and Techniques. (arXiv:2306.09462v1 [cs.RO])

    [http://arxiv.org/abs/2306.09462](http://arxiv.org/abs/2306.09462)

    本文介绍了针对人类舒适度的自动驾驶架构和与其相关的补充框架。讨论了自动驾驶舒适性、响应时间、运动晕车和优化技术等方面的技术细节和挑战。

    

    本文从人类舒适性的角度概述了自动驾驶的架构和相关补充框架。介绍了衡量自动驾驶用户舒适性和心理分析的技术元素。同时，本文介绍了与自动驾驶结构和反应时间相关的技术。我们还讨论了自动驾驶舒适系统、AV驾驶员响应时间、AV舒适水平、运动晕车以及相关优化技术的技术细节。传感器的功能受到各种因素的影响。由于自动驾驶的传感器主要感知车辆周围的环境，包括“天气”，因此在不同的天气条件下，二手传感器在自动驾驶汽车中存在挑战和局限性。自动驾驶的舒适性和安全性也是影响自主发展的因素。

    This article outlines the architecture of autonomous driving and related complementary frameworks from the perspective of human comfort. The technical elements for measuring Autonomous Vehicle (AV) user comfort and psychoanalysis are listed here. At the same time, this article introduces the technology related to the structure of automatic driving and the reaction time of automatic driving. We also discuss the technical details related to the automatic driving comfort system, the response time of the AV driver, the comfort level of the AV, motion sickness, and related optimization technologies. The function of the sensor is affected by various factors. Since the sensor of automatic driving mainly senses the environment around a vehicle, including "the weather" which introduces the challenges and limitations of second-hand sensors in autonomous vehicles under different weather conditions. The comfort and safety of autonomous driving are also factors that affect the development of autono
    
[^103]: 分级混淆矩阵用于分类性能评价

    Hierarchical confusion matrix for classification performance evaluation. (arXiv:2306.09461v1 [cs.LG])

    [http://arxiv.org/abs/2306.09461](http://arxiv.org/abs/2306.09461)

    提出了一种分级混淆矩阵用于分类性能评价，该方法考虑了分级分类问题的特殊性并可适用于多种类型的问题。实验证明该方法的合理性及其对于分级分类问题的实用性。

    

    本文提出了一个新颖的分级混淆矩阵的概念，为基于混淆矩阵的（平面）二分类问题评估措施，同时考虑了分级分类问题的特殊性。我们将该概念发展到一个广义形式，并证明其适用于所有类型的分级分类问题，包括有向无环图、多路径标记和非强制叶节点预测。最后，我们使用基于新混淆矩阵的度量方法来评估三个真实世界的分级分类应用程序中的模型，并将结果与已建立的评估度量方法进行比较。结果概述了这种方法的合理性及其用于评估分级分类问题的实用性。分级混淆矩阵的实现可在GitHub上获得。

    In this work we propose a novel concept of a hierarchical confusion matrix, opening the door for popular confusion matrix based (flat) evaluation measures from binary classification problems, while considering the peculiarities of hierarchical classification problems. We develop the concept to a generalized form and prove its applicability to all types of hierarchical classification problems including directed acyclic graphs, multi path labelling, and non mandatory leaf node prediction. Finally, we use measures based on the novel confusion matrix to evaluate models within a benchmark for three real world hierarchical classification applications and compare the results to established evaluation measures. The results outline the reasonability of this approach and its usefulness to evaluate hierarchical classification problems. The implementation of hierarchical confusion matrix is available on GitHub.
    
[^104]: 循环记忆决策变压器

    Recurrent Memory Decision Transformer. (arXiv:2306.09459v1 [cs.LG])

    [http://arxiv.org/abs/2306.09459](http://arxiv.org/abs/2306.09459)

    本文提出了循环记忆决策变压器（RMDT）模型，用于处理强化学习中的长序列问题。在Atari游戏和MoJoCo控制问题上的实验表明，采用循环记忆机制的RMDT模型显着优于其没有循环记忆机制的对应模型。

    

    变革性模型最初是为自然语言问题而开发的，最近在离线强化学习任务中得到广泛应用。这是因为代理的历史可以表示为序列，并且整个任务可以缩减为序列建模任务。然而，变压器操作的二次复杂性限制了上下文的潜在增加。因此，为了在自然语言中处理长序列，使用了不同版本的记忆机制。在本文中，我们提出了循环记忆决策变压器（RMDT），这是一种在强化学习问题中使用循环记忆机制的模型。我们在Atari游戏和MoJoCo控制问题上进行了彻底的实验，并表明我们提出的模型在Atari游戏上显着优于没有循环记忆机制的对应模型。我们还仔细研究了记忆对所提出的模型绩效的影响。这些发现为开发更高效和更有效的处理长序列的强化学习模型提供了启示。

    Transformative models, originally developed for natural language problems, have recently been widely used in offline reinforcement learning tasks. This is due to the fact that the agent's history can be represented as a sequence, and the whole task can be reduced to the sequence modeling task. However, the quadratic complexity of the transformer operation limits the potential increase in context. Therefore, to work with long sequences in a natural language, different versions of the memory mechanism are used. In this paper, we propose the Recurrent Memory Decision Transformer (RMDT), a model that uses a recurrent memory mechanism for reinforcement learning problems. We conduct thorough experiments on Atari games and MoJoCo control problems, and show that our proposed model is significantly superior to its counterparts without the recurrent memory mechanism on Atari games. We also carefully study the effect of memory on the performance of the proposed model. These findings shed light on
    
[^105]: 利用CI框架和信息处理协议，运用人工智能和大数据分析预防WSN中的网络攻击和数据丢失

    Prevention of cyberattacks in WSN and packet drop by CI framework and information processing protocol using AI and Big Data. (arXiv:2306.09448v1 [cs.CR])

    [http://arxiv.org/abs/2306.09448](http://arxiv.org/abs/2306.09448)

    本论文介绍了一个利用CI框架和信息处理协议以及人工智能和大数据分析预防WSN中的网络攻击和数据丢失的新方法，该框架通过动态响应威胁场景、采用人工智能算法和异常检测算法来实现网络安全性，并提供了一个专注于数据传输完整性的信息处理协议。

    

    随着对无线传感器网络（WSN）的依赖在众多领域中的增加，网络攻击预防和数据传输完整性变得至关重要。本研究提供了一个完整的框架来解决这些问题，通过整合认知智能（CI）框架、信息处理协议和复杂的人工智能（AI）和大数据分析方法。CI架构旨在通过动态响应不断发展的威胁场景来提高WSN安全性。它采用人工智能算法不断监视和分析网络行为，实时识别和缓解任何入侵事件。异常检测算法也包含在框架中以识别由攻击或网络拥塞引起的数据包丢失情况。为支持CI架构，引入了一个专注于WSN内高效安全数据传输的信息处理协议。该协议关注加密和认证技术以保护数据完整性和防止未经许可的人员访问网络。最后，采用人工智能和大数据分析来开发预测模型，可以预测未来的网络攻击场景。通过模拟实验验证了所提出的框架，证明其在检测和预防WSN网络攻击方面的有效性。

    As the reliance on wireless sensor networks (WSNs) rises in numerous sectors, cyberattack prevention and data transmission integrity become essential problems. This study provides a complete framework to handle these difficulties by integrating a cognitive intelligence (CI) framework, an information processing protocol, and sophisticated artificial intelligence (AI) and big data analytics approaches. The CI architecture is intended to improve WSN security by dynamically reacting to an evolving threat scenario. It employs artificial intelligence algorithms to continuously monitor and analyze network behavior, identifying and mitigating any intrusions in real time. Anomaly detection algorithms are also included in the framework to identify packet drop instances caused by attacks or network congestion. To support the CI architecture, an information processing protocol focusing on efficient and secure data transfer within the WSN is introduced. To protect data integrity and prevent unwante
    
[^106]: 基于可复制的机器学习方法的大规模量子可分性研究

    Large-Scale Quantum Separability Through a Reproducible Machine Learning Lens. (arXiv:2306.09444v1 [quant-ph])

    [http://arxiv.org/abs/2306.09444](http://arxiv.org/abs/2306.09444)

    本研究提出了一个机器学习管道用于大规模场景下量子可分性的近似解，通过有效算法近似查找最近的可分离密度矩阵，并将量子可分性视为分类问题，对任何二维混合状态都适用。

    

    量子可分性问题是指如何判断一个二分体密度矩阵是纠缠的还是可分的。我们提出了一种机器学习管道，用于在大规模场景下找到此NP-难问题的近似解。我们提供了一种基于Frank-Wolfe的有效算法来近似查找最近的可分离密度矩阵，并推导了一种系统的方法将密度矩阵标记为可分离的或纠缠的，使我们能够将量子可分性视为分类问题。我们的方法适用于任何二维混合状态。对3-和7维度中的量子态进行的数值实验验证了所提出的程序的效率，并证明它可以扩展到上千个密度矩阵，并具有高量子纠缠检测精度。这一进展有助于基准测试量子可分性，并支持更强大的纠缠检测技术的发展。

    The quantum separability problem consists in deciding whether a bipartite density matrix is entangled or separable. In this work, we propose a machine learning pipeline for finding approximate solutions for this NP-hard problem in large-scale scenarios. We provide an efficient Frank-Wolfe-based algorithm to approximately seek the nearest separable density matrix and derive a systematic way for labeling density matrices as separable or entangled, allowing us to treat quantum separability as a classification problem. Our method is applicable to any two-qudit mixed states. Numerical experiments with quantum states of 3- and 7-dimensional qudits validate the efficiency of the proposed procedure, and demonstrate that it scales up to thousands of density matrices with a high quantum entanglement detection accuracy. This takes a step towards benchmarking quantum separability to support the development of more powerful entanglement detection techniques.
    
[^107]: 从零开始实现红队对抗语言模型的探索与建立

    Explore, Establish, Exploit: Red Teaming Language Models from Scratch. (arXiv:2306.09442v1 [cs.CL])

    [http://arxiv.org/abs/2306.09442](http://arxiv.org/abs/2306.09442)

    本文提出了一种新的红队行动，通过从高层次、抽象的规范出发来考虑语言模型的行为，以探究模型的创新和贡献。

    

    部署大型语言模型（LLMs）可能会产生有害输出，例如有毒或不诚实陈述。先前的研究已经引入了工具以调查有害输出，以识别和减轻这些风险。虽然这是确保语言模型安全的有价值步骤，但这些方法通常依赖于现有的针对不希望的输出的分类器。这限制了它们在只有预先知道有害行为类型的情况下的应用。然而，这跳过了红队行动的核心挑战：开发模型可能展示的行为的上下文理解。此外，当这样的分类器已经存在时，红队行动的边际价值有限，因为分类器可以用于过滤训练数据或模型输出。本文考虑在假设对手从高级、抽象的不良行为规范出发的情况下进行红队行动。红队应该在精化/扩展此规范的同时对抗该模型。

    Deploying Large language models (LLMs) can pose hazards from harmful outputs such as toxic or dishonest speech. Prior work has introduced tools that elicit harmful outputs in order to identify and mitigate these risks. While this is a valuable step toward securing language models, these approaches typically rely on a pre-existing classifier for undesired outputs. This limits their application to situations where the type of harmful behavior is known with precision beforehand. However, this skips a central challenge of red teaming: developing a contextual understanding of the behaviors that a model can exhibit. Furthermore, when such a classifier already exists, red teaming has limited marginal value because the classifier could simply be used to filter training data or model outputs. In this work, we consider red teaming under the assumption that the adversary is working from a high-level, abstract specification of undesired behavior. The red team is expected to refine/extend this spec
    
[^108]: 无监督非线性流形学习进行异常检测

    Unsupervised Anomaly Detection via Nonlinear Manifold Learning. (arXiv:2306.09441v1 [stat.ML])

    [http://arxiv.org/abs/2306.09441](http://arxiv.org/abs/2306.09441)

    该篇论文提出了一种基于非线性流形学习的无监督异常检测方法，可以鲁棒、高效、可解释地检测数据中的异常样本。

    

    异常样本指的是与其他数据显著偏离的样本，其检测在构建可靠的机器学习模型中起着重要作用。现有的大多数异常检测方法要么仅适用于（半）监督设置，要么在没有带标记异常样本的无监督应用中表现很差。为了填补这一研究空白，我们介绍了一种基于非线性流形学习的鲁棒、高效且可解释的方法，用于在无监督设置下检测异常。

    Anomalies are samples that significantly deviate from the rest of the data and their detection plays a major role in building machine learning models that can be reliably used in applications such as data-driven design and novelty detection. The majority of existing anomaly detection methods either are exclusively developed for (semi) supervised settings, or provide poor performance in unsupervised applications where there is no training data with labeled anomalous samples. To bridge this research gap, we introduce a robust, efficient, and interpretable methodology based on nonlinear manifold learning to detect anomalies in unsupervised settings. The essence of our approach is to learn a low-dimensional and interpretable latent representation (aka manifold) for all the data points such that normal samples are automatically clustered together and hence can be easily and robustly identified. We learn this low-dimensional manifold by designing a learning algorithm that leverages either a 
    
[^109]: 实用联邦因果结构学习之路

    Towards Practical Federated Causal Structure Learning. (arXiv:2306.09433v1 [cs.LG])

    [http://arxiv.org/abs/2306.09433](http://arxiv.org/abs/2306.09433)

    为了解决联邦学习条件下的因果结构学习难题，提出了一种基于联邦条件独立性检验的因果结构学习方案FedC2SL，无需收集原始数据且对数据变异具有更强的抵抗力。

    

    理解因果关系对于科学发现至关重要。因果结构学习的过程涉及从观测数据中识别因果图以理解这种关系。通常，一个中央服务器执行此任务，但与服务器共享数据会带来隐私风险。联邦学习可以解决这个问题，但现有的联邦因果结构学习解决方案对数据做出了不切实际的假设，并缺乏收敛保证。FedC2SL是一种联邦基于约束的因果结构学习方案，它使用联邦条件独立性检验来学习因果图，该检验在不收集客户端原始数据的情况下检查两个变量在一组条件下的条件独立性。FedC2SL对数据做出了更弱和更现实的假设，并更强地抵御了客户端之间的数据变异。FedPC和FedFCI是FedC2SL的两个变体，用于因果充分性和因果不充分性情况下的因果结构学习。

    Understanding causal relations is vital in scientific discovery. The process of causal structure learning involves identifying causal graphs from observational data to understand such relations. Usually, a central server performs this task, but sharing data with the server poses privacy risks. Federated learning can solve this problem, but existing solutions for federated causal structure learning make unrealistic assumptions about data and lack convergence guarantees. FedC2SL is a federated constraint-based causal structure learning scheme that learns causal graphs using a federated conditional independence test, which examines conditional independence between two variables under a condition set without collecting raw data from clients. FedC2SL requires weaker and more realistic assumptions about data and offers stronger resistance to data variability among clients. FedPC and FedFCI are the two variants of FedC2SL for causal structure learning in causal sufficiency and causal insuffic
    
[^110]: 盲目图像超分辨率的深度学习技术：高规模多领域视角评估

    Deep learning techniques for blind image super-resolution: A high-scale multi-domain perspective evaluation. (arXiv:2306.09426v1 [eess.IV])

    [http://arxiv.org/abs/2306.09426](http://arxiv.org/abs/2306.09426)

    本文介绍了五种深度学习技术，通过对14个来自不同领域的小型数据集进行高比例尺度（8x）的控制实验，评估了这些技术在盲目图像超分辨率方面的效果，其中BlindSR取得了最佳的性能和计算成本之间的平衡，而APA在性能和复杂性之间取得了显著的性能，是在不需要实时计算的应用程序的良好选择。

    

    尽管最近已经进行了几项解决图像超分辨率的解决方案和实验，这些方案和实验通常不会设计出高比例尺度，仅限于2倍或4倍。此外，数据集通常是基准测试集，不能真正涵盖多种领域的显著多样性以正确评估技术。还值得注意的是，盲目的SR对于真实世界的情景很有吸引力，因为它是基于降级过程是未知的想法，因此在这种情况下的技术基本上依赖于低分辨率（LR）图像。本文介绍了一个高比例尺度（8x）的受控实验，评估了五种最近专为盲目图像SR量身定制的DL技术：自适应伪增强（APA）、具有空间变化劣化的盲SR（BlindSR）、深度交替网络（DAN）、FastGAN和专家超分辨率混合（MoESR）。我们考虑了来自五个不同领域的14个小型数据集，包括医学、自然、城市和艺术图像。我们的结果表明，BlindSR在性能和计算成本之间取得了最好的平衡，而APA在最少的复杂性下获得了显著的性能，是在不需要实时计算的应用程序的良好选择。

    Despite several solutions and experiments have been conducted recently addressing image super-resolution (SR), boosted by deep learning (DL) techniques, they do not usually design evaluations with high scaling factors, capping it at 2x or 4x. Moreover, the datasets are generally benchmarks which do not truly encompass significant diversity of domains to proper evaluate the techniques. It is also interesting to remark that blind SR is attractive for real-world scenarios since it is based on the idea that the degradation process is unknown, and hence techniques in this context rely basically on low-resolution (LR) images. In this article, we present a high-scale (8x) controlled experiment which evaluates five recent DL techniques tailored for blind image SR: Adaptive Pseudo Augmentation (APA), Blind Image SR with Spatially Variant Degradations (BlindSR), Deep Alternating Network (DAN), FastGAN, and Mixture of Experts Super-Resolution (MoESR). We consider 14 small datasets from five diffe
    
[^111]: 公平性-准确性前沿之外存在任意性

    Arbitrariness Lies Beyond the Fairness-Accuracy Frontier. (arXiv:2306.09425v1 [cs.LG])

    [http://arxiv.org/abs/2306.09425](http://arxiv.org/abs/2306.09425)

    研究表明，仅考虑组公平性和准确性的机器学习公平性干预可能加剧预测多样性。因此，应考虑第三个“任意性”轴，提出了一种集成算法可提供更一致的预测。

    

    机器学习任务可能会有多个具有类似表现但产生不同输出的竞争模型，这被称为预测多样性。我们证明了仅针对组公平性和准确性进行优化的机器学习公平性干预可能会加剧预测多样性。因此，最先进的公平性干预可以掩盖高预测多样性背后的有利组公平性和准确性度量。我们认为，在部署模型以协助个体级影响应用程序时，应考虑第三个“任意性”轴。为了解决这一挑战，我们提出了一种适用于任何公平性干预的集成算法，可证明提供更一致的预测。

    Machine learning tasks may admit multiple competing models that achieve similar performance yet produce conflicting outputs for individual samples -- a phenomenon known as predictive multiplicity. We demonstrate that fairness interventions in machine learning optimized solely for group fairness and accuracy can exacerbate predictive multiplicity. Consequently, state-of-the-art fairness interventions can mask high predictive multiplicity behind favorable group fairness and accuracy metrics. We argue that a third axis of ``arbitrariness'' should be considered when deploying models to aid decision-making in applications of individual-level impact. To address this challenge, we propose an ensemble algorithm applicable to any fairness intervention that provably ensures more consistent predictions.
    
[^112]: Landsat影像数据集和基础模型的SSL4EO-L：深度自学习的首个卫星数据集

    SSL4EO-L: Datasets and Foundation Models for Landsat Imagery. (arXiv:2306.09424v1 [cs.LG])

    [http://arxiv.org/abs/2306.09424](http://arxiv.org/abs/2306.09424)

    本文介绍了第一个专为Landsat系列卫星设计的自监督学习数据集SSL4EO-L，这也是历史上最大的Landsat数据集，用于预训练基础模型，并在多个下游任务上实现了最先进的性能。

    

    Landsat计划是有史以来运行时间最长的地球观测计划，通过8个卫星的数据获取，已经有50多年的历史。这些卫星传感器捕获的多光谱图像对多个科学领域至关重要。尽管深度学习和遥感技术越来越受欢迎，但由于小型标记数据集的流行和缺乏基础模型，大多数研究人员仍使用决策树和随机森林进行Landsat图像分析。本文介绍了SSL4EO-L，这是第一个专为Landsat系列卫星（包括3个传感器和2个产品级别）进行自监督学习设计的数据集，也是历史上最大的Landsat数据集（500万图像块）。此外，我们还更新了L7 Irish和L8 Biome云检测数据集，介绍了Landsats 4-5 TM和Landsat 7 ETM + SR的第一个机器学习基准数据集。最后，我们使用SSL4EO-L预训练了第一个Landsat数据分析基础模型，在多个下游任务上实现了最先进的性能。

    The Landsat program is the longest-running Earth observation program in history, with 50+ years of data acquisition by 8 satellites. The multispectral imagery captured by sensors onboard these satellites is critical for a wide range of scientific fields. Despite the increasing popularity of deep learning and remote sensing, the majority of researchers still use decision trees and random forests for Landsat image analysis due to the prevalence of small labeled datasets and lack of foundation models. In this paper, we introduce SSL4EO-L, the first ever dataset designed for Self-Supervised Learning for Earth Observation for the Landsat family of satellites (including 3 sensors and 2 product levels) and the largest Landsat dataset in history (5M image patches). Additionally, we modernize and re-release the L7 Irish and L8 Biome cloud detection datasets, and introduce the first ML benchmark datasets for Landsats 4-5 TM and Landsat 7 ETM+ SR. Finally, we pre-train the first foundation models
    
[^113]: Diff-TTSG: 去噪概率集成语音和手势合成

    Diff-TTSG: Denoising probabilistic integrated speech and gesture synthesis. (arXiv:2306.09417v1 [eess.AS])

    [http://arxiv.org/abs/2306.09417](http://arxiv.org/abs/2306.09417)

    Diff-TTSG是第一个基于扩散的概率模型，用于联合学习合成语音和手势，相比于先前最新技术的非概率方法，它可以更好地捕捉人类讲话和运动的变化，产生更逼真和多样化的集成语音和手势合成。

    

    随着朗读语音合成实现高自然度评分，越来越多的研究开始关注合成自然言语。然而，人类面对面的自发对话既有口头的，也有非语言的（例如，共同言语手势）。最近才开始研究联合合成这两种模态在一个单一的系统中的好处。先前的最新技术使用非概率方法，无法捕捉人类讲话和运动的变化，并可能产生过度平滑的伪影和次优的合成质量。我们提出了第一个基于扩散的概率模型，称为 Diff-TTSG，共同学习合成语音和手势。我们的方法可以从头开始使用小型数据集进行训练。此外，我们描述了一组小心的单模态和多模态主观测试，用于评估集成语音和手势合成系统，并用它们来验证我们提出的方法。对于合成的样例而言，Diff-TTSG优于先前的最新技术，产生更逼真和多样化的集成语音和手势合成。

    With read-aloud speech synthesis achieving high naturalness scores, there is a growing research interest in synthesising spontaneous speech. However, human spontaneous face-to-face conversation has both spoken and non-verbal aspects (here, co-speech gestures). Only recently has research begun to explore the benefits of jointly synthesising these two modalities in a single system. The previous state of the art used non-probabilistic methods, which fail to capture the variability of human speech and motion, and risk producing oversmoothing artefacts and sub-optimal synthesis quality. We present the first diffusion-based probabilistic model, called Diff-TTSG, that jointly learns to synthesise speech and gestures together. Our method can be trained on small datasets from scratch. Furthermore, we describe a set of careful uni- and multi-modal subjective tests for evaluating integrated speech and gesture synthesis systems, and use them to validate our proposed approach. For synthesised examp
    
[^114]: 限制数据下社交机器学习的非渐进性能研究

    Non-Asymptotic Performance of Social Machine Learning Under Limited Data. (arXiv:2306.09397v1 [cs.LG])

    [http://arxiv.org/abs/2306.09397](http://arxiv.org/abs/2306.09397)

    本文研究了限制数据下社交机器学习的概率误差问题，并提出了一种更强的一致性训练条件，推导出了两种任务的概率误差上界。

    

    本文研究了社交机器学习框架中与错误概率相关的问题，该框架涉及独立的训练阶段，随后在图上进行协作决策阶段。该框架解决了分布式分类未标记数据的问题。我们考虑了两种分类任务，分别为统计分类和单样本分类，同时考虑了预测阶段数据观测受限的情况。对于每个任务，我们描述了分布式学习规则，并相应分析了错误概率。为此，我们首先引入了一个更强的一致性训练条件，该条件涉及训练分类器生成的边缘分布。基于此条件，我们为两种任务推导出了概率误差的上界，该上界取决于数据的统计特性和用于组合分布式分类器的组合策略。

    This paper studies the probability of error associated with the social machine learning framework, which involves an independent training phase followed by a cooperative decision-making phase over a graph. This framework addresses the problem of classifying a stream of unlabeled data in a distributed manner. We consider two kinds of classification tasks with limited observations in the prediction phase, namely, the statistical classification task and the single-sample classification task. For each task, we describe the distributed learning rule and analyze the probability of error accordingly. To do so, we first introduce a stronger consistent training condition that involves the margin distributions generated by the trained classifiers. Based on this condition, we derive an upper bound on the probability of error for both tasks, which depends on the statistical properties of the data and the combination policy used to combine the distributed classifiers. For the statistical classifica
    
[^115]: 私有联邦频率估计：适应实例难度的算法

    Private Federated Frequency Estimation: Adapting to the Hardness of the Instance. (arXiv:2306.09396v1 [cs.DS])

    [http://arxiv.org/abs/2306.09396](http://arxiv.org/abs/2306.09396)

    本论文提出了一种新的混合sketching算法并解决了一个基本问题：如何根据底层问题的难度设置sketch size。

    

    在联邦频率估计（FFE）中，多个客户端使用遵守Secure Summation(SecSum)隐私约束的服务器协议通信，共同估计其数据的频率。对于单轮FFE，研究者已知使用count sketching算法几乎可以达到基本的精度-通信复杂度平衡。但是在更实际的多轮FEE设置中，我们发现简单的count sketching算法是严格次优的。我们提出了一种新的混合sketching算法，并证明其更精确。此外，我们回答了一个基本问题：为了适应底层问题的难度，从业者应该如何设置sketch size？ 我们提出了一个两阶段的方法来解决这个问题。

    In federated frequency estimation (FFE), multiple clients work together to estimate the frequencies of their collective data by communicating with a server that respects the privacy constraints of Secure Summation (SecSum), a cryptographic multi-party computation protocol that ensures that the server can only access the sum of client-held vectors. For single-round FFE, it is known that count sketching is nearly information-theoretically optimal for achieving the fundamental accuracy-communication trade-offs [Chen et al., 2022]. However, we show that under the more practical multi-round FEE setting, simple adaptations of count sketching are strictly sub-optimal, and we propose a novel hybrid sketching algorithm that is provably more accurate. We also address the following fundamental question: how should a practitioner set the sketch size in a way that adapts to the hardness of the underlying problem? We propose a two-phase approach that allows for the use of a smaller sketch size for s
    
[^116]: 基于深度学习的高内容细胞成像多组学预测

    Multi-omics Prediction from High-content Cellular Imaging with Deep Learning. (arXiv:2306.09391v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.09391](http://arxiv.org/abs/2306.09391)

    本研究使用深度学习方法，从高内容细胞成像中直接预测细胞群体的多组学。实验结果表明，该方法能够在多种刺激条件下实现显著成果，为细胞组学领域提供了新的方法。

    

    高内容细胞成像、转录组学和蛋白质组学数据为影响细胞状态和功能的生物分子层提供了丰富和互补的视角。但是，尚未系统地探讨多组学测量值影响细胞形态的生物学决定因素，因此目前尚不清楚细胞成像是否能够直接预测多组学。在这里，我们探讨了使用Image2Omics——一种深度学习方法——直接从用多重荧光染料染色的高内容图像中预测细胞群体的多组学是否可能。我们在多种刺激条件下的人类诱导多能干细胞（hiPSC）衍生的基因编辑巨噬细胞中进行实验评估，并证明Image2Omics取得了显著的成果。

    High-content cellular imaging, transcriptomics, and proteomics data provide rich and complementary views on the molecular layers of biology that influence cellular states and function. However, the biological determinants through which changes in multi-omics measurements influence cellular morphology have not yet been systematically explored, and the degree to which cell imaging could potentially enable the prediction of multi-omics directly from cell imaging data is therefore currently unclear. Here, we address the question of whether it is possible to predict bulk multi-omics measurements directly from cell images using Image2Omics -- a deep learning approach that predicts multi-omics in a cell population directly from high-content images stained with multiplexed fluorescent dyes. We perform an experimental evaluation in gene-edited macrophages derived from human induced pluripotent stem cell (hiPSC) under multiple stimulation conditions and demonstrate that Image2Omics achieves sign
    
[^117]: 基于社交媒体的 ChatGPT 自杀风险评估：模型性能、潜力和限制的量化评估

    ChatGPT for Suicide Risk Assessment on Social Media: Quantitative Evaluation of Model Performance, Potentials and Limitations. (arXiv:2306.09390v1 [cs.CL])

    [http://arxiv.org/abs/2306.09390](http://arxiv.org/abs/2306.09390)

    本文量化评估了基于社交媒体的 ChatGPT 模型在自杀倾向评估方面的表现，比较了其结果与两个微调模型，并探讨了模型响应生成的最佳温度，研究结果显示，以人工标注数据集为基础微调的 Transformer 模型表现更佳，这篇论文为心理健康专家提供了重要的模型评估和参数优化建议。

    

    本文提出了一个新的框架来量化评估交互式 ChatGPT 模型在社交媒体帖子中进行自杀倾向评估的能力，利用马里兰大学 Reddit 自杀倾向数据集。我们使用零样本和少样本实验来对 ChatGPT 在这个任务中的表现进行技术评估，并将其结果与两个基于 transformer 的微调模型进行比较。此外，我们调查不同温度参数对 ChatGPT 响应生成的影响，并讨论基于 ChatGPT 不确定性率的最佳温度参数。我们的研究结果表明，虽然 ChatGPT 在这个任务中具有相当高的准确性，但以人工标注数据集为基础微调的 Transformer 模型表现更优。此外，我们的分析还阐明了如何通过调整 ChatGPT 的超参数来提高其在这一关键任务中帮助心理健康专家的能力。

    This paper presents a novel framework for quantitatively evaluating the interactive ChatGPT model in the context of suicidality assessment from social media posts, utilizing the University of Maryland Reddit suicidality dataset. We conduct a technical evaluation of ChatGPT's performance on this task using Zero-Shot and Few-Shot experiments and compare its results with those of two fine-tuned transformer-based models. Additionally, we investigate the impact of different temperature parameters on ChatGPT's response generation and discuss the optimal temperature based on the inconclusiveness rate of ChatGPT. Our results indicate that while ChatGPT attains considerable accuracy in this task, transformer-based models fine-tuned on human-annotated datasets exhibit superior performance. Moreover, our analysis sheds light on how adjusting the ChatGPT's hyperparameters can improve its ability to assist mental health professionals in this critical task.
    
[^118]: ST-PINN: 一种用于偏微分方程的自训练物理知识神经网络

    ST-PINN: A Self-Training Physics-Informed Neural Network for Partial Differential Equations. (arXiv:2306.09389v1 [cs.LG])

    [http://arxiv.org/abs/2306.09389](http://arxiv.org/abs/2306.09389)

    提出了一种自训练物理知识神经网络ST-PINN。在训练期间引入基于伪标签的自学习算法，以提高现有PINN的精度和收敛性。实验结果表明， ST-PINN 可以学习更多的物理知识并受益于更好的收敛性。

    

    偏微分方程是物理和工程计算核心。随着深度学习的发展，无网格方法的物理知识神经网络（PINN）在各种应用中展示出了快速求解PDE的巨大潜力。我们提出了一种自训练物理知识神经网络ST-PINN，来解决现有PINN的低精度和收敛问题。具体而言，ST-PINN在训练期间引入了基于伪标签的自学习算法。它将控制方程作为伪标记评估指标，并从样本点中选择最高置信度的示例来附加伪标签。我们相信我们是首先将自我训练机制引入物理信息学习的人。我们在不同领域和场景中对五个PDE进行了实验。结果表明，所提出的方法使网络可以学习更多物理信息并受益于收敛。

    Partial differential equations (PDEs) are an essential computational kernel in physics and engineering. With the advance of deep learning, physics-informed neural networks (PINNs), as a mesh-free method, have shown great potential for fast PDE solving in various applications. To address the issue of low accuracy and convergence problems of existing PINNs, we propose a self-training physics-informed neural network, ST-PINN. Specifically, ST-PINN introduces a pseudo label based self-learning algorithm during training. It employs governing equation as the pseudo-labeled evaluation index and selects the highest confidence examples from the sample points to attach the pseudo labels. To our best knowledge, we are the first to incorporate a self-training mechanism into physics-informed learning. We conduct experiments on five PDE problems in different fields and scenarios. The results demonstrate that the proposed method allows the network to learn more physical information and benefit conver
    
[^119]: 自适应分层时空网络用于交通预测。

    Adaptive Hierarchical SpatioTemporal Network for Traffic Forecasting. (arXiv:2306.09386v1 [cs.LG])

    [http://arxiv.org/abs/2306.09386](http://arxiv.org/abs/2306.09386)

    本文提出了一种自适应分层时空网络（AHSTN），通过利用空间层次结构和建模多尺度空间相关性促进交通预测，AHSTN在节点级别的基础上引入了自适应的时空块，来自适应地处理不同层次之间的相关性，同时使用分层注意机制来选择性地聚合不同尺度的信息，具有优越性。

    

    精确的交通预测对于智能交通系统至关重要，该系统被广泛采用以解决城市交通问题。现有的交通预测研究着重于建模交通数据中的空间 - 时间动态，其中，图卷积网络（GCN）是利用路网格中嵌入的空间依赖性的核心。然而，这些基于GCN的方法仅在节点级别（例如道路和交叉口）上运行，而忽略了整个城市的空间层次结构。诸如交叉口和道路段之类的节点可以形成簇（例如区域），这些节点在更高的层次上还可以相互作用并共享相似之处。在这项工作中，我们提出了一种自适应分层空间时间网络（AHSTN），通过利用空间层次结构和建模多尺度空间相关性促进交通预测。除了节点级别的时空块，AHSTN引入了自适应时空块，分别捕捉本地、区域和全球的依赖关系，以自适应地建模不同层次的相关性。此外，我们提出了一种分层注意机制，以选择性地聚合来自不同尺度的信息。对真实世界数据集的广泛实验表明，与最先进的方法相比，提出的AHSTN具有优越性。

    Accurate traffic forecasting is vital to intelligent transportation systems, which are widely adopted to solve urban traffic issues. Existing traffic forecasting studies focus on modeling spatial-temporal dynamics in traffic data, among which the graph convolution network (GCN) is at the center for exploiting the spatial dependency embedded in the road network graphs. However, these GCN-based methods operate intrinsically on the node level (e.g., road and intersection) only whereas overlooking the spatial hierarchy of the whole city. Nodes such as intersections and road segments can form clusters (e.g., regions), which could also have interactions with each other and share similarities at a higher level. In this work, we propose an Adaptive Hierarchical SpatioTemporal Network (AHSTN) to promote traffic forecasting by exploiting the spatial hierarchy and modeling multi-scale spatial correlations. Apart from the node-level spatiotemporal blocks, AHSTN introduces the adaptive spatiotempor
    
[^120]: 应用多模态机器学习进行压力检测

    Employing Multimodal Machine Learning for Stress Detection. (arXiv:2306.09385v1 [cs.LG])

    [http://arxiv.org/abs/2306.09385](http://arxiv.org/abs/2306.09385)

    本文提出了一个基于多模态AI的框架，旨在通过融合多种信息来精确监测人的工作行为和压力水平。该框架使用卷积神经网络、时延神经网络和多层感知器等深度学习技术，能够高精度地检测高压力和低压力状态。

    

    在当前时代，人类的生活方式越来越注重知识，导致久坐的工作方式变得更加普遍。这导致许多健康和心理障碍。心理健康是当今世界上最被忽视但也是最关键的方面之一。心理健康问题会直接或间接地影响到人体其他部分，并妨碍个人的日常活动和表现。然而，确定压力并找出导致严重心理疾病的压力趋势对于个人来说是具有挑战性的，并涉及多个因素。通过融合由行为模式产生的多个模态（由于各种因素）可以准确地实现这种识别。文献中已经确定了一些技术来实现这个目的。然而，为此目的提出的基于机器学习的多模态融合方法非常少。本文提出了一个基于多模态AI的框架，通过融合面部表情、语音模式、生理信号和自报数据，准确监测人的工作行为和压力水平。所提出的框架采用了卷积神经网络(Convolutional Neural Networks, CNNs)、时延神经网络(Time-delay Neural Networks, TDNNs)和多层感知器(Multi-Layer Perceptron, MLP)等先进的深度学习技术，能够高精度地分类高压力和低压力状态。在公共压力数据集和从医疗保健专业人员那里收集的数据集上获得的结果展示了所提出的框架在准确检测压力方面的有效性。

    In the current age, human lifestyle has become more knowledge oriented leading to generation of sedentary employment. This has given rise to a number of health and mental disorders. Mental wellness is one of the most neglected but crucial aspects of today's world. Mental health issues can, both directly and indirectly, affect other sections of human physiology and impede an individual's day-to-day activities and performance. However, identifying the stress and finding the stress trend for an individual leading to serious mental ailments is challenging and involves multiple factors. Such identification can be achieved accurately by fusing these multiple modalities (due to various factors) arising from behavioral patterns. Certain techniques are identified in the literature for this purpose; however, very few machine learning-based methods are proposed for such multimodal fusion tasks. In this work, a multimodal AI-based framework is proposed to monitor a person's working behavior and st
    
[^121]: 2023年声音分离挑战赛——音乐分离赛道技术报告

    Sound Demixing Challenge 2023 -- Music Demixing Track Technical Report. (arXiv:2306.09382v1 [cs.SD])

    [http://arxiv.org/abs/2306.09382](http://arxiv.org/abs/2306.09382)

    本文介绍了2023年声音分离挑战赛音乐分离赛道的两个有效方法，分别是时效高的源分离网络和适用于噪声鲁棒性源分离的损失掩模方法。

    

    本文介绍了我们在2023年声音分离挑战赛音乐分离赛道中获奖的解决方案。我们专注于两种在此挑战中设计的方法：一种时效高的源分离网络，在MUSDB基准测试上实现了最先进的结果；一种适用于噪声鲁棒性源分离的损失掩模方法。在github.com/kuielab/sdx23上提供了模型训练和最终提交的代码。

    In this report, we present our award-winning solutions for the Music Demixing Track of Sound Demixing Challenge 2023. We focus on two methods designed for this challenge: a time-efficient source separation network that achieves state-of-the-art results on the MUSDB benchmark and a loss masking method for noise-robust source separation. Code for reproducing model training and final submissions is available at github.com/kuielab/sdx23.
    
[^122]: 基于时空扩展图神经网络的人类移动模拟

    Spatiotemporal-Augmented Graph Neural Networks for Human Mobility Simulation. (arXiv:2306.09381v1 [cs.LG])

    [http://arxiv.org/abs/2306.09381](http://arxiv.org/abs/2306.09381)

    STAR框架通过设计多种时空图来捕捉位置的动态时空效应，为人类移动模拟任务提供新的方法。

    

    人类移动模式在政策决策和经济行为研究中有着重要的应用。人类移动模拟任务旨在给定一小组轨迹数据生成人类移动轨迹，但由于人类移动数据的稀缺性和稀疏性，引起了广泛关注。现有方法大多依赖于地点之间的静态关系，而很大程度上忽略了位置的动态时空效应。因此，我们提出了一种新的框架，即SpatioTemporal-Augmented gRaph神经网络（STAR），来模拟位置的动态时空效应。

    Human mobility patterns have shown significant applications in policy-decision scenarios and economic behavior researches. The human mobility simulation task aims to generate human mobility trajectories given a small set of trajectory data, which have aroused much concern due to the scarcity and sparsity of human mobility data. Existing methods mostly rely on the static relationships of locations, while largely neglect the dynamic spatiotemporal effects of locations. On the one hand, spatiotemporal correspondences of visit distributions reveal the spatial proximity and the functionality similarity of locations. On the other hand, the varying durations in different locations hinder the iterative generation process of the mobility trajectory. Therefore, we propose a novel framework to model the dynamic spatiotemporal effects of locations, namely SpatioTemporal-Augmented gRaph neural networks (STAR). The STAR framework designs various spatiotemporal graphs to capture the spatiotemporal co
    
[^123]: 理解Transformer中的参数共享

    Understanding Parameter Sharing in Transformers. (arXiv:2306.09380v1 [cs.LG])

    [http://arxiv.org/abs/2306.09380](http://arxiv.org/abs/2306.09380)

    本文从模型复杂度和梯度范围两个角度研究了Transformer中参数共享的有效性，发现提高训练收敛性是其中主要原因，模型复杂度只有一小部分贡献。

    

    参数共享已经被证明是一种高效的方法。在Transformer上的先前工作集中在在不同层次上共享参数，这可以通过增加模型深度来提高有限参数模型的性能。在本文中，我们从两个角度研究为什么此方法有效。首先，增加模型深度会使模型更加复杂，我们假设原因与模型复杂度（指FLOPs）有关。其次，由于每个共享参数在向前传播中会参与网络计算多次，其对应的梯度值与原模型的梯度值范围不同，这将影响模型的收敛性。基于此，我们假设训练收敛性也是其中之一的原因。通过进一步的分析，我们显示此方法的成功很大程度上归功于更好的收敛性，只有一小部分归因于增加的模型复杂度。这启发了我们进行更深层次的研究。

    Parameter sharing has proven to be a parameter-efficient approach. Previous work on Transformers has focused on sharing parameters in different layers, which can improve the performance of models with limited parameters by increasing model depth. In this paper, we study why this approach works from two perspectives. First, increasing model depth makes the model more complex, and we hypothesize that the reason is related to model complexity (referring to FLOPs). Secondly, since each shared parameter will participate in the network computation several times in forward propagation, its corresponding gradient will have a different range of values from the original model, which will affect the model convergence. Based on this, we hypothesize that training convergence may also be one of the reasons. Through further analysis, we show that the success of this approach can be largely attributed to better convergence, with only a small part due to the increased model complexity. Inspired by this
    
[^124]: 对齐语言的视觉表示预测人类在自然学习任务中的行为

    Language Aligned Visual Representations Predict Human Behavior in Naturalistic Learning Tasks. (arXiv:2306.09377v1 [cs.LG])

    [http://arxiv.org/abs/2306.09377](http://arxiv.org/abs/2306.09377)

    语言对齐的视觉表示方式比纯视觉表示方式更有效地预测人类在自然学习任务中的行为。

    

    人类具备识别和概括自然物体相关特征的能力，在各种情境中有所帮助。为了研究这种现象并确定最有效的表示方式以预测人类行为，我们进行了两个涉及类别学习和奖励学习的实验。我们的实验使用逼真的图像作为刺激物，并要求参与者基于所有试验的新型刺激物作出准确的决策，因此需要泛化。在两个任务中，底层规则是使用人类相似性判断提取的刺激维度生成的简单线性函数。值得注意的是，参与者在几次试验内就成功地确定了相关的刺激特征，证明了有效的泛化。我们进行了广泛的模型比较，评估了各种深度学习模型的表示对人类选择的逐次预测准确性。有趣的是，自然语言处理任务（如语言建模和机器翻译）训练的模型表示优于视觉任务训练的模型表示，表明对齐语言的视觉表示可能更有效地预测人类在自然学习任务中的行为。

    Humans possess the ability to identify and generalize relevant features of natural objects, which aids them in various situations. To investigate this phenomenon and determine the most effective representations for predicting human behavior, we conducted two experiments involving category learning and reward learning. Our experiments used realistic images as stimuli, and participants were tasked with making accurate decisions based on novel stimuli for all trials, thereby necessitating generalization. In both tasks, the underlying rules were generated as simple linear functions using stimulus dimensions extracted from human similarity judgments. Notably, participants successfully identified the relevant stimulus features within a few trials, demonstrating effective generalization. We performed an extensive model comparison, evaluating the trial-by-trial predictive accuracy of diverse deep learning models' representations of human choices. Intriguingly, representations from models train
    
[^125]: 模型训练中的模块化：一种新的模块化深度神经网络的范式

    Modularizing while Training: a New Paradigm for Modularizing DNN Models. (arXiv:2306.09376v1 [cs.LG])

    [http://arxiv.org/abs/2306.09376](http://arxiv.org/abs/2306.09376)

    本文提出了一种新方法，将模块化纳入模型训练过程中，即在训练时模块化(MwT)，通过两个损失函数实现模型结构上的模块化，进而实现模块的重用，能够在较短的训练时间内达到可比较的模型精度，并且相对于最先进的训练后模块化方法需要更少的参数。

    

    深度神经网络(DNN)模型已成为智能软件系统中越来越关键的组成部分。然而，训练DNN模型通常在时间和成本方面都很昂贵。为了解决这个问题，研究人员最近开始关注重用现有的DNN模型-借鉴软件工程中的代码重用思想。但是，重用整个模型可能会造成额外的开销或从不需要的功能中继承弱点。因此，现有的工作提出将已经训练好的模型分解成模块，即训练后的模块化，并实现模块的重用。但是，由于已经训练好的模型并不是为了模块化而构建的，所以训练后的模块化会导致巨大的开销和模型精度损失。本文提出了一种新方法，将模块化纳入模型训练过程中，即在训练时模块化（MwT）。我们通过两个损失函数在模型训练过程中使模型具有结构上的模块化能力，这两个损失函数同时优化模块内的内聚性和模块之间的独立性，从而得到一个真正的模块化模型。我们展示了我们的方法可以在较短的训练时间内达到可比较的模型精度，并且相对于最先进的训练后模块化方法需要更少的参数。

    Deep neural network (DNN) models have become increasingly crucial components in intelligent software systems. However, training a DNN model is typically expensive in terms of both time and money. To address this issue, researchers have recently focused on reusing existing DNN models - borrowing the idea of code reuse in software engineering. However, reusing an entire model could cause extra overhead or inherits the weakness from the undesired functionalities. Hence, existing work proposes to decompose an already trained model into modules, i.e., modularizing-after-training, and enable module reuse. Since trained models are not built for modularization, modularizing-after-training incurs huge overhead and model accuracy loss. In this paper, we propose a novel approach that incorporates modularization into the model training process, i.e., modularizing-while-training (MwT). We train a model to be structurally modular through two loss functions that optimize intra-module cohesion and int
    
[^126]: 分子、蛋白质和晶体材料的对称性识别几何表示

    Symmetry-Informed Geometric Representation for Molecules, Proteins, and Crystalline Materials. (arXiv:2306.09375v1 [cs.LG])

    [http://arxiv.org/abs/2306.09375](http://arxiv.org/abs/2306.09375)

    本文提出了一个对称性识别的几何表示方法，称之为Geom3D，可以用于分子、蛋白质和晶体材料的基准测评。经过实验证明，该方法在各种数据集上实现了最先进的性能。

    

    最近人工智能在化学、生物学和材料发现等领域引起了机器学习和科学界的极大关注。然而，由于这个领域的快速发展和科学界（如物理、化学和生物学）与机器学习界之间的知识差距，还没有进行过关于这些数据的几何表示的基准研究。为了解决这个问题，本文首先提供了当前对称性几何方法的统一视角，将其分类为三类：不变性、具有球形框架基础的等变性和具有向量框架基础的等变性。然后，我们提出了一个名为Geom3D的平台，可以对分子、蛋白质和晶体材料的不同几何表示进行系统的基准测评。利用Geom3D，我们进行了大量实验，评估了现有几何方法的性能，并提出了一种新的方法，它结合了球形框架基础和向量框架基础的等变性优点。我们的方法在各种数据集上实现了最先进的性能，证明了所提出的方法的有效性。

    Artificial intelligence for scientific discovery has recently generated significant interest within the machine learning and scientific communities, particularly in the domains of chemistry, biology, and material discovery. For these scientific problems, molecules serve as the fundamental building blocks, and machine learning has emerged as a highly effective and powerful tool for modeling their geometric structures. Nevertheless, due to the rapidly evolving process of the field and the knowledge gap between science (e.g., physics, chemistry, & biology) and machine learning communities, a benchmarking study on geometrical representation for such data has not been conducted. To address such an issue, in this paper, we first provide a unified view of the current symmetry-informed geometric methods, classifying them into three main categories: invariance, equivariance with spherical frame basis, and equivariance with vector frame basis. Then we propose a platform, coined Geom3D, which ena
    
[^127]: 公平的多任务学习

    Equitable Multi-task Learning. (arXiv:2306.09373v1 [cs.LG])

    [http://arxiv.org/abs/2306.09373](http://arxiv.org/abs/2306.09373)

    该论文提出了一种名为EMTL的多任务优化方法，以实现公平的多任务学习。通过规范化不同任务的相对贡献，可以提高MTL的泛化性能，并利用方差正则化和高效的优化算法保证收敛。实验证明，该方法在合成和真实数据集上均表现出了更好的性能。

    

    多任务学习（MTL）在各个研究领域（如计算机视觉、自然语言处理和信息检索等）中取得了巨大成功。但是，由于任务之间存在复杂且相互竞争的相关性，单纯地训练所有任务可能会导致不公平的学习，即一些任务被很好地学习，而其他任务则被忽视。多任务优化（MTO）旨在同时提高所有任务的表现，但传统方法往往在任务损失规模或梯度范数差异较大的情况下表现不佳。为了解决这个问题，我们深入研究了MTL的公平性问题，并发现在更新共享参数时，规范化不同任务的相对贡献（即任务特定损失值除以其原始梯度范数的值）可以提高MTL的泛化性能。基于我们的理论分析，我们提出了一种新的多任务优化方法，名为EMTL，以实现公平的MTL。具体来说，我们有效地添加了方差正则化，使不同任务的相对贡献更具可比性，并开发了一种高效的优化算法来保证收敛。我们在合成和真实数据集上进行了大量实验，结果表明了我们方法的有效性和优越性。

    Multi-task learning (MTL) has achieved great success in various research domains, such as CV, NLP and IR etc. Due to the complex and competing task correlation, na\"ive training all tasks may lead to inequitable learning, \textit{i.e.} some tasks are learned well while others are overlooked. Multi-task optimization (MTO) aims to improve all tasks at same time, but conventional methods often perform poor when tasks with large loss scale or gradient norm magnitude difference. To solve the issue, we in-depth investigate the equity problem for MTL and find that regularizing relative contribution of different tasks (\textit{i.e.} value of task-specific loss divides its raw gradient norm) in updating shared parameter can improve generalization performance of MTL. Based on our theoretical analysis, we propose a novel multi-task optimization method, named \textit{EMTL}, to achieve equitable MTL. Specifically, we efficiently add variance regularization to make different tasks' relative contribu
    
[^128]: Warpformer: 一种用于不规则临床时间序列建模的多尺度方法

    Warpformer: A Multi-scale Modeling Approach for Irregular Clinical Time Series. (arXiv:2306.09368v1 [cs.LG])

    [http://arxiv.org/abs/2306.09368](http://arxiv.org/abs/2306.09368)

    Warpformer是一种能够完整考虑序列内不规则性和序列间差异性的多尺度建模方法。

    

    不规则采样的多变量时间序列在各个领域中都很常见，特别是在医疗保健领域，呈现出序列内不规则性和序列间差异性。序列内不规则性指时间序列信号通常在不规则的时间间隔内记录，而序列间差异性则指不同序列之间的采样率显著变化。然而，最近关于不规则时间序列的研究主要集中在解决序列内不规则性问题，而忽略了序列间差异性问题。为了填补这一空白，我们提出了一种新方法：Warpformer，它充分考虑了这两种特征。

    Irregularly sampled multivariate time series are ubiquitous in various fields, particularly in healthcare, and exhibit two key characteristics: intra-series irregularity and inter-series discrepancy. Intra-series irregularity refers to the fact that time-series signals are often recorded at irregular intervals, while inter-series discrepancy refers to the significant variability in sampling rates among diverse series. However, recent advances in irregular time series have primarily focused on addressing intra-series irregularity, overlooking the issue of inter-series discrepancy. To bridge this gap, we present Warpformer, a novel approach that fully considers these two characteristics. In a nutshell, Warpformer has several crucial designs, including a specific input representation that explicitly characterizes both intra-series irregularity and inter-series discrepancy, a warping module that adaptively unifies irregular time series in a given scale, and a customized attention module fo
    
[^129]: 采用函数降维方法进行感应电动机故障检测

    Fault Detection in Induction Motors using Functional Dimensionality Reduction Methods. (arXiv:2306.09365v1 [eess.SY])

    [http://arxiv.org/abs/2306.09365](http://arxiv.org/abs/2306.09365)

    本研究提出了一种采用函数降维方法结合电机电流特征分析策略的故障检测方法，能够实时检测感应电动机中的故障，并且可以通过离线分析识别更多类型的故障。

    

    在旋转电机上实施故障检测和诊断策略对于现代工业系统的可靠性和安全性至关重要。本文的贡献是提出了一种方法，将传统的电机电流特征分析策略与函数降维方法（即函数主成分分析和函数扩散映射）相结合，以检测和分类感应电动机中的故障条件。所提出的方案获得了非常鼓舞人心的结果，不仅可以实时检测感应电动机中故障的存在，而且还可以通过离线分析识别更多类型的故障。

    The implementation of strategies for fault detection and diagnosis on rotating electrical machines is crucial for the reliability and safety of modern industrial systems. The contribution of this work is a methodology that combines conventional strategy of Motor Current Signature Analysis with functional dimensionality reduction methods, namely Functional Principal Components Analysis and Functional Diffusion Maps, for detecting and classifying fault conditions in induction motors. The results obtained from the proposed scheme are very encouraging, revealing a potential use in the future not only for real-time detection of the presence of a fault in an induction motor, but also in the identification of a greater number of types of faults present through an offline analysis.
    
[^130]: TSMixer: 用于多元时间序列预测的轻量级MLP-Mixer模型

    TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting. (arXiv:2306.09364v1 [cs.LG])

    [http://arxiv.org/abs/2306.09364](http://arxiv.org/abs/2306.09364)

    TSMixer是一种用于多元时间序列预测的轻量级MLP-Mixer模型，可以有效地捕捉时间序列属性并在准确性方面超越了Transformers的方法。

    

    Transformers因其能够捕捉长序列交互而在时间序列预测中备受青睐。然而，其内存和计算要求高的问题对长期预测构成了严重瓶颈。为了解决这一问题，我们提出了TSMixer，这是一种轻量级神经架构，专为多元预测和补丁时间序列表示学习而设计，是Transformers的有效替代。我们的模型借鉴了MLP-Mixer模型在计算机视觉中的成功经验。我们展示了将视觉MLP-Mixer适应于时间序列的挑战，并引入了经过实验证实的组件以提高准确性。这包括一种新的设计范式，即将在线协调头附加到MLP-Mixer骨干上，以显式地建模时间序列的属性，如层次结构和通道相关性。我们还提出了一种混合通道建模方法，平衡了编码多个时间序列通道和保留单个通道信息之间的权衡。我们的实验表明，TSMixer在一元和多元时间序列预测任务中均实现了最先进的性能，同时需要比基于Transformers的方法少得多的参数。

    Transformers have gained popularity in time series forecasting for their ability to capture long-sequence interactions. However, their high memory and computing requirements pose a critical bottleneck for long-term forecasting. To address this, we propose TSMixer, a lightweight neural architecture exclusively composed of multi-layer perceptron (MLP) modules. TSMixer is designed for multivariate forecasting and representation learning on patched time series, providing an efficient alternative to Transformers. Our model draws inspiration from the success of MLP-Mixer models in computer vision. We demonstrate the challenges involved in adapting Vision MLP-Mixer for time series and introduce empirically validated components to enhance accuracy. This includes a novel design paradigm of attaching online reconciliation heads to the MLP-Mixer backbone, for explicitly modeling the time-series properties such as hierarchy and channel-correlations. We also propose a Hybrid channel modeling approa
    
[^131]: 一种简单的面向特征分布偏斜联邦学习的数据增强方法

    A Simple Data Augmentation for Feature Distribution Skewed Federated Learning. (arXiv:2306.09363v1 [cs.LG])

    [http://arxiv.org/abs/2306.09363](http://arxiv.org/abs/2306.09363)

    本文针对特征分布偏斜的联邦学习提出了FedRDN方法，在输入层级上实现了数据增强，将整个联邦数据集的统计信息注入到本地客户端数据中，以缓解特征漂移问题。

    

    联邦学习（FL）是一种分布式协作学习方法，可以确保隐私保护。然而，由于数据异构性（即非独立同分布数据），它的性能必然受到影响。本文针对特征分布偏斜的FL场景展开研究，提出了一种通用的数据增强方法，以减轻由本地数据集之间潜在分布不同导致的特征漂移问题。

    Federated learning (FL) facilitates collaborative learning among multiple clients in a distributed manner, while ensuring privacy protection. However, its performance is inevitably degraded as suffering data heterogeneity, i.e., non-IID data. In this paper, we focus on the feature distribution skewed FL scenario, which is widespread in real-world applications. The main challenge lies in the feature shift caused by the different underlying distributions of local datasets. While the previous attempts achieved progress, few studies pay attention to the data itself, the root of this issue. Therefore, the primary goal of this paper is to develop a general data augmentation technique at the input level, to mitigate the feature shift. To achieve this goal, we propose FedRDN, a simple yet remarkably effective data augmentation method for feature distribution skewed FL, which randomly injects the statistics of the dataset from the entire federation into the client's data. By this, our method ca
    
[^132]: 基于深度学习的延迟线探测器时空多事件重构

    Deep Learning-Based Spatiotemporal Multi-Event Reconstruction for Delay Line Detectors. (arXiv:2306.09359v1 [physics.ins-det])

    [http://arxiv.org/abs/2306.09359](http://arxiv.org/abs/2306.09359)

    本文提出了基于深度学习的延迟线探测器时空多事件重构方法，可以成功地重构多个粒子的空间和时间坐标，相比传统方法更加高效和准确。

    

    现代物理学中，观察非常短的时间窗口内两个或更多粒子的位置一直是一项挑战。对于低能电子，可以使用微通道板和延迟线的组合构成延迟线探测器，以读出入射粒子的位置。这种方法可以完全重构多个粒子的空间和时间坐标。然而，对于多个接近的粒子，传统的峰值查找算法无法有效识别和重构事件。为了解决这个问题，我们提出了一个新的时空机器学习模型，使用卷积和循环神经网络在短时间窗口内提取和组合多个粒子的空间和时间特征，成功地重建了包含多达四个粒子的事件。

    Accurate observation of two or more particles within a very narrow time window has always been a challenge in modern physics. It creates the possibility of correlation experiments, such as the ground-breaking Hanbury Brown-Twiss experiment, leading to new physical insights. For low-energy electrons, one possibility is to use a microchannel plate with subsequent delay lines for the readout of the incident particle hits, a setup called a Delay Line Detector. The spatial and temporal coordinates of more than one particle can be fully reconstructed outside a region called the dead radius. For interesting events, where two electrons are close in space and time, the determination of the individual positions of the electrons requires elaborate peak finding algorithms. While classical methods work well with single particle hits, they fail to identify and reconstruct events caused by multiple nearby particles. To address this challenge, we present a new spatiotemporal machine learning model to 
    
[^133]: 离线安全强化学习的数据集和基准

    Datasets and Benchmarks for Offline Safe Reinforcement Learning. (arXiv:2306.09303v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.09303](http://arxiv.org/abs/2306.09303)

    该论文提出了一个专门针对离线安全强化学习的基准套件，包含了安全策略、数据集和高质量RL算法实现。作者还提供了一种数据收集流程，利用先进算法生成多样性数据集，用于38个受欢迎的安全RL任务。该套件可加速该领域的研究进展。

    

    本文提出了一个专门针对离线安全强化学习（RL）挑战的综合基准套件，旨在促进开发和评估训练和部署阶段中的安全学习算法的进展。我们的基准套件包含三个组件：1）专家制作的安全策略，2）D4RL样式的数据集以及环境包装器，以及3）高质量的离线安全RL基准实现。我们提供了一种有条理的数据收集流程，由先进的安全RL算法支持，可以促进在38个受欢迎的安全RL任务中生成各种数据集，从机器人控制到自动驾驶。我们还引入了一系列数据后处理过滤器，能够修改每个数据集的多样性，从而模拟各种数据收集条件。此外，我们提供了流行的离线安全RL算法的精美且可扩展的实现，以加速该领域的研究。 通过广泛的实验

    This paper presents a comprehensive benchmarking suite tailored to offline safe reinforcement learning (RL) challenges, aiming to foster progress in the development and evaluation of safe learning algorithms in both the training and deployment phases. Our benchmark suite contains three packages: 1) expertly crafted safe policies, 2) D4RL-styled datasets along with environment wrappers, and 3) high-quality offline safe RL baseline implementations. We feature a methodical data collection pipeline powered by advanced safe RL algorithms, which facilitates the generation of diverse datasets across 38 popular safe RL tasks, from robot control to autonomous driving. We further introduce an array of data post-processing filters, capable of modifying each dataset's diversity, thereby simulating various data collection conditions. Additionally, we provide elegant and extensible implementations of prevalent offline safe RL algorithms to accelerate research in this area. Through extensive experime
    
[^134]: 具有时间不规则性的多元时间序列的概率学习

    Probabilistic Learning of Multivariate Time Series with Temporal Irregularity. (arXiv:2306.09147v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.09147](http://arxiv.org/abs/2306.09147)

    本文提出了一种针对具有时间不规则性的多元时间序列的概率学习方法，通过允许观察到达时间在模型构建中发挥核心作用，使用新颖的非参数先验模型明确融入时间不规则性。

    

    在实践中收集的多元序列数据经常表现出时间的不规则性，包括非均匀时间间隔和组件错位。然而，如果不均匀的间距和异步是数据内生特征而不是不足观察的结果，则这些不规则性的信息内容在表征多元依赖结构时发挥决定性作用。现有的概率预测方法要么忽略了由此产生的统计异质性，要么易受到插补偏差的影响，要么将参数假设强加于数据分布上。本文提出了一种端到端解决方案，通过允许观察到达时间在模型构建中发挥核心作用来克服这些限制，这是时间不规则性的核心。为了认识到时间的不规则性，我们首先为组件启用唯一的隐藏状态，以便到达时间可以指导何时、如何和哪个隐藏状态应该更新。然后，我们通过一种新颖的非参数先验模型明确地融入了时间不规则性，该模型联合建模观测和隐藏状态，并自适应地捕获了跨组件的时间异质性和条件依赖性。我们使用合成数据和实际数据集展示了我们方法的优越性。

    Multivariate sequential data collected in practice often exhibit temporal irregularities, including nonuniform time intervals and component misalignment. However, if uneven spacing and asynchrony are endogenous characteristics of the data rather than a result of insufficient observation, the information content of these irregularities plays a defining role in characterizing the multivariate dependence structure. Existing approaches for probabilistic forecasting either overlook the resulting statistical heterogeneities, are susceptible to imputation biases, or impose parametric assumptions on the data distribution. This paper proposes an end-to-end solution that overcomes these limitations by allowing the observation arrival times to play the central role of model construction, which is at the core of temporal irregularities. To acknowledge temporal irregularities, we first enable unique hidden states for components so that the arrival times can dictate when, how, and which hidden state
    
[^135]: 机器学习增强采样：一项综述

    Enhanced Sampling with Machine Learning: A Review. (arXiv:2306.09111v2 [cond-mat.stat-mech] UPDATED)

    [http://arxiv.org/abs/2306.09111](http://arxiv.org/abs/2306.09111)

    本文综述了机器学习与增强分子动力学方法的融合，这些技术能够解决分子动力学的时间尺度限制，成功的策略包括降维、强化学习和基于流的方法。

    

    分子动力学 (MD) 可以在空间和时间上精确地研究物理系统，但是存在着严重的时间尺度限制。为了改善配置空间探索能力，已经开发了增强采样方法。然而，实施这些方法具有挑战性并且需要专业领域知识。近年来，在不同领域中使用机器学习 (ML) 技术已经显示出了潜力，因此在增强采样中采用了这些技术。尽管 ML 主要由于其数据驱动的本质而被广泛应用于各个领域，但其与增强采样的集成更自然，共享许多共同的协同作用。本综述通过呈现不同的共享观点，探索了ML和增强MD的融合。它提供了这个快速发展领域的全面概述，这可能难以保持最新。我们突出了像降维、强化学习和基于流的方法等成功策略。

    Molecular dynamics (MD) enables the study of physical systems with excellent spatiotemporal resolution but suffers from severe time-scale limitations. To address this, enhanced sampling methods have been developed to improve exploration of configurational space. However, implementing these is challenging and requires domain expertise. In recent years, integration of machine learning (ML) techniques in different domains has shown promise, prompting their adoption in enhanced sampling as well. Although ML is often employed in various fields primarily due to its data-driven nature, its integration with enhanced sampling is more natural with many common underlying synergies. This review explores the merging of ML and enhanced MD by presenting different shared viewpoints. It offers a comprehensive overview of this rapidly evolving field, which can be difficult to stay updated on. We highlight successful strategies like dimensionality reduction, reinforcement learning, and flow-based methods
    
[^136]: ClimSim：用于在混合多尺度气候模拟器中训练高分辨率物理仿真器的开源大规模数据集

    ClimSim: An open large-scale dataset for training high-resolution physics emulators in hybrid multi-scale climate simulators. (arXiv:2306.08754v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.08754](http://arxiv.org/abs/2306.08754)

    这是一个用于训练高分辨率物理仿真器的气候数据集。该数据集包含了5.7亿个多变量输入和输出矢量对，用于隔离本地嵌套的高分辨率、高保真度物理的影响。

    

    现代气候预测由于计算限制缺乏足够的空间和时间分辨率。一个后果是对关键过程（如暴风雨）的预测不准确和不精确。将物理和机器学习（ML）相结合的混合模式引入了新一代更高保真度的气候模拟器，通过将计算密集型、短、高分辨率的模拟委托给ML仿真器，可以避免摩尔定律问题。然而，这种混合的ML-物理仿真方法需要领域特定的处理，并且由于缺乏培训数据和相关的易于使用的工作流程，一直无法访问ML专家。我们提出了 ClimSim，这是迄今为止为混合ML-物理研究而设计的最大数据集。它由气候科学家和ML研究人员联合开发的多尺度气候模拟组成，包括57亿个多变量输入和输出矢量对，隔离了本地嵌套的高分辨率和高保真度物理学的影响。

    Modern climate projections lack adequate spatial and temporal resolution due to computational constraints. A consequence is inaccurate and imprecise prediction of critical processes such as storms. Hybrid methods that combine physics with machine learning (ML) have introduced a new generation of higher fidelity climate simulators that can sidestep Moore's Law by outsourcing compute-hungry, short, high-resolution simulations to ML emulators. However, this hybrid ML-physics simulation approach requires domain-specific treatment and has been inaccessible to ML experts because of lack of training data and relevant, easy-to-use workflows. We present ClimSim, the largest-ever dataset designed for hybrid ML-physics research. It comprises multi-scale climate simulations, developed by a consortium of climate scientists and ML researchers. It consists of 5.7 billion pairs of multivariate input and output vectors that isolate the influence of locally-nested, high-resolution, high-fidelity physics
    
[^137]: 针对强对数凹分布的 Langevin Monte Carlo：随机中点方法再探讨

    Langevin Monte Carlo for strongly log-concave distributions: Randomized midpoint revisited. (arXiv:2306.08494v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2306.08494](http://arxiv.org/abs/2306.08494)

    本文提出了一种针对具有平滑且强对数凹密度的目标分布的采样方法，并给出了该方法的误差上限。

    

    我们重新审视了从一个具有 $\mathbb R^p$ 中任意位置的平滑且强对数凹密度目标分布中采样的问题。在这种情况下，如果没有额外的密度信息可用，随机中点划分对于动力学 Langevin 扩散是已知的最可扩展的高维方法，具有大的条件数。我们的主要结果是对这种方法的Wasserstein-2误差的一种非渐进且易于计算的上限。为了更全面地解释建立可计算的上限的方法，我们对香农熵和中点划分进行了分析，以便阐明基本原理并提供有价值的见解，从而确立了与中点划分的动力学 Langevin 进程有关的改进上限。此外，通过应用这些技术，我们为采用欧拉划分的动力学 Langevin 进程建立了新的保证。

    We revisit the problem of sampling from a target distribution that has a smooth strongly log-concave density everywhere in $\mathbb R^p$. In this context, if no additional density information is available, the randomized midpoint discretization for the kinetic Langevin diffusion is known to be the most scalable method in high dimensions with large condition numbers. Our main result is a nonasymptotic and easy to compute upper bound on the Wasserstein-2 error of this method. To provide a more thorough explanation of our method for establishing the computable upper bound, we conduct an analysis of the midpoint discretization for the vanilla Langevin process. This analysis helps to clarify the underlying principles and provides valuable insights that we use to establish an improved upper bound for the kinetic Langevin process with the midpoint discretization. Furthermore, by applying these techniques we establish new guarantees for the kinetic Langevin process with Euler discretization, w
    
[^138]: Skill-Critic: 用于强化学习中学习技能的筛选与优化

    Skill-Critic: Refining Learned Skills for Reinforcement Learning. (arXiv:2306.08388v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.08388](http://arxiv.org/abs/2306.08388)

    基于技能筛选与优化的Skill-Critic算法能够提高稀疏奖励环境下强化学习中低层策略的可靠性，并显著提高了性能。

    

    分层强化学习可以通过时间抽象将一个策略分为多个层次，加快长期决策的速度。在稀疏奖励的环境中，技能即原始动作的序列，已经取得了有望的结果。通常情况下，技能的潜在空间和策略是从离线数据中发现的，但由于演示覆盖范围低或分布转移，所得到的低层策略可能不可靠。因此，我们提出了一种Fine-tuning低层策略与高层技能选择相结合的解决方案。我们的Skill-Critic算法优化了低层和高层策略，并通过从离线演示数据中学习的潜在空间进行初始化和规范化，以引导联合策略优化。我们在多个稀疏强化学习环境中验证了我们的方法，包括Gran Turismo Sport中新的稀疏奖励自主赛车任务。实验表明，Skill-Critic的低层策略Fine-tuning和演示引导策略初始化显著提高了性能。

    Hierarchical reinforcement learning (RL) can accelerate long-horizon decision-making by temporally abstracting a policy into multiple levels. Promising results in sparse reward environments have been seen with skills, i.e. sequences of primitive actions. Typically, a skill latent space and policy are discovered from offline data, but the resulting low-level policy can be unreliable due to low-coverage demonstrations or distribution shifts. As a solution, we propose fine-tuning the low-level policy in conjunction with high-level skill selection. Our Skill-Critic algorithm optimizes both the low and high-level policies; these policies are also initialized and regularized by the latent space learned from offline demonstrations to guide the joint policy optimization. We validate our approach in multiple sparse RL environments, including a new sparse reward autonomous racing task in Gran Turismo Sport. The experiments show that Skill-Critic's low-level policy fine-tuning and demonstration-g
    
[^139]: 一种用于自闭症干预分析的多模态数据集MMASD。

    MMASD: A Multimodal Dataset for Autism Intervention Analysis. (arXiv:2306.08243v1 [cs.CV])

    [http://arxiv.org/abs/2306.08243](http://arxiv.org/abs/2306.08243)

    提出了一个名为MMASD的自闭症多模态数据集，收集自治疗干预。它包括从32名自闭症患儿的干预录音中分段的1,315个数据样本，每个样本包含四种隐私保护模式的数据。

    

    自闭症谱系障碍（ASD）是一种发育性疾病，其特征是重大的社交沟通障碍和困难的知觉和表达沟通提示。机器学习技术已广泛应用于促进自闭症研究和评估。然而，计算模型主要集中于特定分析，并在自闭症社区的私有数据集上进行验证，这限制了由于数据共享复杂性而跨模型的比较。本研究提出了一种新的隐私保护开源数据集MMASD作为多模式ASD基准数据集，收集自患有自闭症儿童的游戏治疗干预。MMASD包括32名患有ASD的儿童的数据，以及从100多小时的干预录音中分段的1,315个数据样本。为促进公共访问，每个数据样本包含四种隐私保护模式的数据：（1）光流，（2）2D骨架，（3）3D骨架和（4）临床医生ASD评估。

    Autism spectrum disorder (ASD) is a developmental disorder characterized by significant social communication impairments and difficulties perceiving and presenting communication cues. Machine learning techniques have been broadly adopted to facilitate autism studies and assessments. However, computational models are primarily concentrated on specific analysis and validated on private datasets in the autism community, which limits comparisons across models due to privacy-preserving data sharing complications. This work presents a novel privacy-preserving open-source dataset, MMASD as a MultiModal ASD benchmark dataset, collected from play therapy interventions of children with Autism. MMASD includes data from 32 children with ASD, and 1,315 data samples segmented from over 100 hours of intervention recordings. To promote public access, each data sample consists of four privacy-preserving modalities of data: (1) optical flow, (2) 2D skeleton, (3) 3D skeleton, and (4) clinician ASD evalua
    
[^140]: h2oGPT：民主化大语言模型

    h2oGPT: Democratizing Large Language Models. (arXiv:2306.08161v1 [cs.CL])

    [http://arxiv.org/abs/2306.08161](http://arxiv.org/abs/2306.08161)

    本文介绍了h2oGPT，这是一套开源代码库，用于创建和使用基于GPTs的大语言模型（LLMs），包括100％私有文档搜索。目标是创建真正开源的替代封闭源GPTs，提高人工智能的开发和可靠性。

    

    基于生成预训练变压器（GPTs），大语言模型（LLMs）如GPT-4因其在自然语言处理方面的现实应用而成为人工智能革命的一部分。然而，它们也带来了许多重大的风险，如存在有偏见、私人或有害文本和未经授权的版权材料。本文介绍了h2oGPT，这是一套开源代码库，用于创建和使用基于GPTs的大语言模型（LLMs）。该项目的目标是创建世界上最好的真正开源的替代封闭源GPTs。与开源社区合作，作为其一部分，我们开源了几个LLM，其参数从7亿到400亿，可在完全自由的Apache 2.0许可下商用。我们的发布包括使用自然语言的100％私有文档搜索。开源语言模型有助于促进人工智能的发展并使其更加可靠。

    Foundation Large Language Models (LLMs) such as GPT-4 represent a revolution in AI due to their real-world applications though natural language processing. However, they also pose many significant risks such as the presence of biased, private, or harmful text, and the unauthorized inclusion of copyrighted material.  We introduce h2oGPT, a suite of open-source code repositories for the creation and use of Large Language Models (LLMs) based on Generative Pretrained Transformers (GPTs). The goal of this project is to create the world's best truly open-source alternative to closed-source GPTs. In collaboration with and as part of the incredible and unstoppable open-source community, we open-source several fine-tuned h2oGPT models from 7 to 40 Billion parameters, ready for commercial use under fully permissive Apache 2.0 licenses. Included in our release is 100% private document search using natural language.  Open-source language models help boost AI development and make it more accessible
    
[^141]: 通过不偏微分对抗复杂密度生成，基于互联马尔科夫链不偏微分优化 MH 采样方法

    Differentiating Metropolis-Hastings to Optimize Intractable Densities. (arXiv:2306.07961v1 [stat.ML])

    [http://arxiv.org/abs/2306.07961](http://arxiv.org/abs/2306.07961)

    本文通过基于互联马尔科夫链的不偏微分，开发出一种无偏、低方差和自动的方法对复杂密度进行生成，从而实现对 MH 采样器的优化。

    

    在概率模型推理中，目标密度函数通常变得难以计算，需要使用 Monte Carlo 计算。本文开发了一种不偏微分 Metropolis-Hastings 采样器的方法，使我们可以通过概率推理来进行微分。通过将随机微分的最新进展与 Markov 链耦合方法相结合，可以实现无偏，低方差和自动的程序。这使我们能够将基于梯度的优化应用于由于繁琐的目标密度导致期望的情况下。我们通过在高斯混合模型中找到一个模棱两可的观察和在 Ising 模型中最大化比热来演示了我们的方法。

    When performing inference on probabilistic models, target densities often become intractable, necessitating the use of Monte Carlo samplers. We develop a methodology for unbiased differentiation of the Metropolis-Hastings sampler, allowing us to differentiate through probabilistic inference. By fusing recent advances in stochastic differentiation with Markov chain coupling schemes, the procedure can be made unbiased, low-variance, and automatic. This allows us to apply gradient-based optimization to objectives expressed as expectations over intractable target densities. We demonstrate our approach by finding an ambiguous observation in a Gaussian mixture model and by maximizing the specific heat in an Ising model.
    
[^142]: 大型语言模型有时生成纯负反馈文本

    Large Language Models Sometimes Generate Purely Negatively-Reinforced Text. (arXiv:2306.07567v1 [cs.LG])

    [http://arxiv.org/abs/2306.07567](http://arxiv.org/abs/2306.07567)

    大型语言模型有时会从仅包含负奖励的例子中学习，导致生成类似泄漏密码或安全漏洞等敏感信息的文本

    

    在使用对抗性训练时，通常会训练反对最严重失败的案例。然而，这可能会意味着使用包含敏感信息（例如泄露的密码或安全漏洞）的案例作为训练数据。我们可能会认为使用梯度下降算法训练的语言模型永远不会生成仅在与最低奖励相关联的示例中出现的文本片段。本文表明这种假设是错误的：在某些情况下，大型语言模型确实从这种纯负反馈的示例中学习到了东西。我们提出了一种特定的训练设置，使得Pythia-160M能够生成密码的概率略高于随机，尽管仅在对模型不输出这些密码的示例中展示了这些密码。我们的代码可在https://github.com/FabienRoger/Learning-From-Negative-Examples上找到。

    When using adversarial training, it is common practice to train against the most egregious failures. However, this might imply using examples with sensitive information (such as leaked passwords or security vulnerabilities) as training data. One might assume that language models trained with gradient descent never generate text snippets which were only present in examples associated with the lowest possible reward. In this paper, we show that this assumption is wrong: in some situations, large language models do learn from such negatively-reinforced examples. We present a specific training setup that enables Pythia-160M to generate passwords with a probability slightly greater than chance, despite only showing it these passwords on examples where the model is incentivized to not output these passwords. Our code is available at https://github.com/FabienRoger/Learning-From-Negative-Examples
    
[^143]: 错误反馈可以准确地压缩预处理器。

    Error Feedback Can Accurately Compress Preconditioners. (arXiv:2306.06098v1 [cs.LG])

    [http://arxiv.org/abs/2306.06098](http://arxiv.org/abs/2306.06098)

    本论文提出一种错误反馈技术，可以通过在馈入预处理器之前对梯度信息进行压缩（稀疏化或低秩压缩），将预处理器的存储成本压缩多达两个数量级，而不会丢失收敛性。

    

    利用深度网络规模的二阶信息是改进当前用于深度学习优化器性能的主要途径之一。然而，现有的精确全矩阵预处理方法，如全矩阵Adagrad（GGT）或无矩阵近似曲率（M-FAC），即使应用于中等规模模型，也会遇到巨大的存储成本问题，因为它们必须存储梯度的滑动窗口，其存储需求在模型维度中是成倍增加的。本文通过一种高效且易于实现的错误反馈技术来解决这个问题，该技术可以在实践中将预处理器压缩多达两个数量级，而不会丢失收敛性。具体而言，我们的方法在将梯度信息馈入预处理器之前通过稀疏化或低秩压缩压缩梯度信息，将压缩误差反馈到未来的迭代中。对深度神经网络进行了大量实验。

    Leveraging second-order information at the scale of deep networks is one of the main lines of approach for improving the performance of current optimizers for deep learning. Yet, existing approaches for accurate full-matrix preconditioning, such as Full-Matrix Adagrad (GGT) or Matrix-Free Approximate Curvature (M-FAC) suffer from massive storage costs when applied even to medium-scale models, as they must store a sliding window of gradients, whose memory requirements are multiplicative in the model dimension. In this paper, we address this issue via an efficient and simple-to-implement error-feedback technique that can be applied to compress preconditioners by up to two orders of magnitude in practice, without loss of convergence. Specifically, our approach compresses the gradient information via sparsification or low-rank compression \emph{before} it is fed into the preconditioner, feeding the compression error back into future iterations. Extensive experiments on deep neural networks
    
[^144]: SMRVIS：用于非破坏性测试的三维超声点云提取

    SMRVIS: Point cloud extraction from 3-D ultrasound for non-destructive testing. (arXiv:2306.04668v1 [eess.IV])

    [http://arxiv.org/abs/2306.04668](http://arxiv.org/abs/2306.04668)

    提出了将超声体积点云提取作为图像分割问题的解决方案，并通过快速原型开发验证效果。

    

    我们提出了将超声体积的点云提取作为图像分割问题的解决方案。通过这个便捷的公式，我们开发了一个快速原型来探索U-Net架构的各种变体，并对其进行了评估。本报告记录了使用5个标记超声体积和84个未标记体积的训练数据集完成的实验结果，这个工作是作为一个名为“超声图像分析的深度学习”开放挑战的提交的一部分。源代码已在Github上\url{https://github.com/lisatwyw/smrvis}公开共享给研究社区。

    We propose to formulate point cloud extraction from ultrasound volumes as an image segmentation problem. Through this convenient formulation, a quick prototype exploring various variants of the U-Net architecture was developed and evaluated. This report documents the experimental results compiled using a training dataset of 5 labelled ultrasound volumes and 84 unlabelled volumes that got completed in a two-week period as part of a challenge submission to an open challenge entitled ``Deep Learning in Ultrasound Image Analysis''. Source code is shared with the research community at this GitHub URL \url{https://github.com/lisatwyw/smrvis}.
    
[^145]: 明藏暗窃：伪装数据窃取攻击在联邦学习中的应用

    Hiding in Plain Sight: Disguising Data Stealing Attacks in Federated Learning. (arXiv:2306.03013v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2306.03013](http://arxiv.org/abs/2306.03013)

    该研究提出了一个新的恶意服务器攻击框架SEER，可以在联邦学习中窃取用户数据，而且不易被客户端侦测出来。

    

    恶意服务器攻击已经使得联邦学习中的数据窃取在大批量和安全聚合等之前被视为私密的设置中变得可行。然而，许多关于恶意服务器攻击客户端侦测性的疑虑被提出，这使得它们在被公开知晓后的实用性受到质疑。在本研究中，我们首次全面研究了客户端侦测的问题。我们证明了许多基于两个关键原则的恶意服务器攻击都可以通过合理的客户端检查来检测出来。此外，我们制定了实用恶意服务器攻击的理想要求，并提出了SEER攻击框架，它满足所有理想要求，可以从现实网络的梯度中窃取用户数据，即使是在大批量(我们的实验中最大可达512)和安全聚合的情况下。SEER攻击的关键是使用秘密解码器，在共享模型上进行联合训练。我们的工作是迈向实用恶意服务器攻击的有前途的第一步。

    Malicious server (MS) attacks have enabled the scaling of data stealing in federated learning to large batch sizes and secure aggregation, settings previously considered private. However, many concerns regarding client-side detectability of MS attacks were raised, questioning their practicality once they are publicly known. In this work, for the first time, we thoroughly study the problem of client-side detectability.We demonstrate that most prior MS attacks, which fundamentally rely on one of two key principles, are detectable by principled client-side checks. Further, we formulate desiderata for practical MS attacks and propose SEER, a novel attack framework that satisfies all desiderata, while stealing user data from gradients of realistic networks, even for large batch sizes (up to 512 in our experiments) and under secure aggregation. The key insight of SEER is the use of a secret decoder, which is jointly trained with the shared model. Our work represents a promising first step to
    
[^146]: 基于图的无模型数据子采样在推荐系统中的应用研究

    Graph-Based Model-Agnostic Data Subsampling for Recommendation Systems. (arXiv:2305.16391v1 [cs.IR])

    [http://arxiv.org/abs/2305.16391](http://arxiv.org/abs/2305.16391)

    本文提出了一种基于图结构的无模型数据子采样方法，通过研究用户-物品图的拓扑结构来估计每个用户-物品交互的重要性，并在网络上进行传播来平滑估计值。该方法结合了无模型和基于模型的子采样方法的优点，在多个基准数据集上表现出较好的实验结果。

    

    数据子采样广泛用于加速训练大规模推荐系统。大多数子采样方法是基于模型的，常常需要一个预训练的试验模型来通过样本难度等方式测量数据重要性。然而，当试验模型被错误指定时，基于模型的子采样方法将会恶化。鉴于试验模型的错误指定在真实的推荐系统中普遍存在，我们提出了基于数据结构，即图形来探索的无模型数据子采样方法。具体地，我们研究用户-物品图的拓扑结构，通过图导电性来估计每个用户-物品交互（即用户-物品图中的一条边）的重要性，并在网络上进行传播步骤，平滑估计的重要性值。由于我们提出的方法是无模型的，因此我们可以将无模型和基于模型的子采样方法的优点结合起来。我们的实证研究表明，将这两种方法组合使用，在多个基准数据集上均比任何单一方法都要好。

    Data subsampling is widely used to speed up the training of large-scale recommendation systems. Most subsampling methods are model-based and often require a pre-trained pilot model to measure data importance via e.g. sample hardness. However, when the pilot model is misspecified, model-based subsampling methods deteriorate. Since model misspecification is persistent in real recommendation systems, we instead propose model-agnostic data subsampling methods by only exploring input data structure represented by graphs. Specifically, we study the topology of the user-item graph to estimate the importance of each user-item interaction (an edge in the user-item graph) via graph conductance, followed by a propagation step on the network to smooth out the estimated importance value.  Since our proposed method is model-agnostic, we can marry the merits of both model-agnostic and model-based subsampling methods. Empirically, we show that combing the two consistently improves over any single meth
    
[^147]: 不要训练它：图神经网络的线性神经架构搜索

    Do Not Train It: A Linear Neural Architecture Search of Graph Neural Networks. (arXiv:2305.14065v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.14065](http://arxiv.org/abs/2305.14065)

    本文提出了一种新的图神经网络结构搜索方法——神经结构编码（NAC），它通过稀疏编码寻找最优结构参数，无需训练就能发挥表现力，在多个基准数据集上实现了最先进性能，并且运算速度比强基线方法快了200倍，精度提高了18.8％。

    

    图神经网络的神经架构搜索（NAS-GNN）已经显著地提高了手动设计的图神经网络的性能。然而，这些方法继承了传统NAS方法的问题，如高计算成本和优化难度。更重要的是，以前的NAS方法忽视了GNN的独特性，即GNN具有无需训练就具有表现力的特点。采用随机初始化的权重，我们可以通过稀疏编码目标寻找最优的架构参数，并得出一种新的NAS-GNN方法，即神经结构编码（NAC）。因此，我们的NAC在GNN上实现了无更新方案，可以在线性时间内高效计算。在多个GNN基准数据集上的实证评估表明，我们的方法导致了最先进的性能，比强基线方法快200倍，精度提高了18.8％。

    Neural architecture search (NAS) for Graph neural networks (GNNs), called NAS-GNNs, has achieved significant performance over manually designed GNN architectures. However, these methods inherit issues from the conventional NAS methods, such as high computational cost and optimization difficulty. More importantly, previous NAS methods have ignored the uniqueness of GNNs, where GNNs possess expressive power without training. With the randomly-initialized weights, we can then seek the optimal architecture parameters via the sparse coding objective and derive a novel NAS-GNNs method, namely neural architecture coding (NAC). Consequently, our NAC holds a no-update scheme on GNNs and can efficiently compute in linear time. Empirical evaluations on multiple GNN benchmark datasets demonstrate that our approach leads to state-of-the-art performance, which is up to $200\times$ faster and $18.8\%$ more accurate than the strong baselines.
    
[^148]: 几个非克利福德门制备的量子状态的有效学习

    Efficient Learning of Quantum States Prepared With Few Non-Clifford Gates. (arXiv:2305.13409v1 [quant-ph])

    [http://arxiv.org/abs/2305.13409](http://arxiv.org/abs/2305.13409)

    该研究提出了一种能有效学习几个非克利福德门制备的量子状态的算法，并给出了一个随着稳定维数增大而学习所有状态的算法。

    

    我们提出了一种算法，可以有效地学习通过克利福德门和$O(\log(n))$个非克利福德门制备的量子状态。具体而言，对于最多使用$t$个非克利福德门制备的$n$量子比特状态$|\psi\rangle$，我们证明可以用$\mathsf{poly}(n,2^t,1/\epsilon)$时间和$|\psi\rangle$的复制来学习$|\psi\rangle$，使其跟真实状态的距离不超过$\epsilon$。该结果是一个稳定维数较大的状态学习算法的特例，当一个量子状态的稳定子维数为$k$，表示它被一个由$2^k$个Pauli算子的Abel群稳定。

    We give an algorithm that efficiently learns a quantum state prepared by Clifford gates and $O(\log(n))$ non-Clifford gates. Specifically, for an $n$-qubit state $\lvert \psi \rangle$ prepared with at most $t$ non-Clifford gates, we show that $\mathsf{poly}(n,2^t,1/\epsilon)$ time and copies of $\lvert \psi \rangle$ suffice to learn $\lvert \psi \rangle$ to trace distance at most $\epsilon$. This result follows as a special case of an algorithm for learning states with large stabilizer dimension, where a quantum state has stabilizer dimension $k$ if it is stabilized by an abelian group of $2^k$ Pauli operators. We also develop an efficient property testing algorithm for stabilizer dimension, which may be of independent interest.
    
[^149]: 生物序列生成的Dirichlet扩散分数模型

    Dirichlet Diffusion Score Model for Biological Sequence Generation. (arXiv:2305.10699v1 [cs.LG])

    [http://arxiv.org/abs/2305.10699](http://arxiv.org/abs/2305.10699)

    本文介绍了一种针对离散数据，使用概率单纯形空间中的扩散过程进行建模的生成SDE模型。称之为Dirchlet扩散分数模型。模型可以生成满足严格限制的样本，且适用于生成生物序列。

    

    设计生物序列是一个重要的挑战，需要满足复杂的限制，因此使用深度生成模型来解决是很自然的问题。扩散生成模型在许多应用中取得了相当大的成功。基于分数的生成随机微分方程（SDE）模型是一种连续时间扩散模型框架，具有许多优点，但最初提出的SDE不是自然地用于建模离散数据。为了开发适用于离散数据（例如生物序列）的生成SDE模型，我们在概率单纯形空间中引入了一种扩散过程，其中随机分布的平稳分布是Dirichlet分布。这使得在连续空间中进行扩散对于建模离散数据是自然的。我们称这种方法为Dirchlet扩散分数模型。我们通过数独生成任务证明了这种技术可以生成满足严格限制的样本。这种生成模型也可以用于生成生物序列。

    Designing biological sequences is an important challenge that requires satisfying complex constraints and thus is a natural problem to address with deep generative modeling. Diffusion generative models have achieved considerable success in many applications. Score-based generative stochastic differential equations (SDE) model is a continuous-time diffusion model framework that enjoys many benefits, but the originally proposed SDEs are not naturally designed for modeling discrete data. To develop generative SDE models for discrete data such as biological sequences, here we introduce a diffusion process defined in the probability simplex space with stationary distribution being the Dirichlet distribution. This makes diffusion in continuous space natural for modeling discrete data. We refer to this approach as Dirchlet diffusion score model. We demonstrate that this technique can generate samples that satisfy hard constraints using a Sudoku generation task. This generative model can also 
    
[^150]: CLCIFAR：带人类标注互补标签的CIFAR派生基准数据集

    CLCIFAR: CIFAR-Derived Benchmark Datasets with Human Annotated Complementary Labels. (arXiv:2305.08295v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.08295](http://arxiv.org/abs/2305.08295)

    本研究开发了由人类标注的互补标签，创造了两个真实世界的CLL数据集，进一步揭示了现实表现下CLL算法的性能，为这一领域的研究提供了更实际的评估标准。

    

    互补标签学习（CLL）是一种弱监督学习范式，旨在仅使用互补标签（标示实例不属于哪些类别）来训练多类分类器。尽管已经提出了多种CLL算法，但由于两个原因，它们的实际表现仍不清楚。首先，这些算法通常依赖于对互补标签生成的假设。其次，它们的评估仅限于合成数据集。为了获取有关CLL算法的真实世界表现的见解，我们开发了一种协议来收集由人类注释者注释的互补标签。这一努力导致创建了两个数据集，CLCIFAR10和CLCIFAR20，分别由CIFAR10和CIFAR100派生而来。这些数据集在https://github.com/ntucllab/complementary_cifar上公开发布，代表了第一个真实世界的CLL数据集。通过广泛的基准实验，我们发现相较于合成数据集，当使用人类注释的互补标签时，性能有明显下降。但是，我们也观察到，真实世界的CLL数据集使得在更接近实际应用条件下评估算法成为可能，从而更真实地评估其性能。

    Complementary-label learning (CLL) is a weakly-supervised learning paradigm that aims to train a multi-class classifier using only complementary labels, which indicate classes to which an instance does not belong. Despite numerous algorithmic proposals for CLL, their practical performance remains unclear for two reasons. Firstly, these algorithms often rely on assumptions about the generation of complementary labels. Secondly, their evaluation has been limited to synthetic datasets. To gain insights into the real-world performance of CLL algorithms, we developed a protocol to collect complementary labels annotated by human annotators. This effort resulted in the creation of two datasets, CLCIFAR10 and CLCIFAR20, derived from CIFAR10 and CIFAR100, respectively. These datasets, publicly released at https://github.com/ntucllab/complementary_cifar, represent the very first real-world CLL datasets. Through extensive benchmark experiments, we discovered a notable decline in performance when 
    
[^151]: 一种对抗感知的去中心化网络多智能体强化学习算法

    An Algorithm For Adversary Aware Decentralized Networked MARL. (arXiv:2305.05573v1 [cs.LG])

    [http://arxiv.org/abs/2305.05573](http://arxiv.org/abs/2305.05573)

    本文提出了一种对抗感知的去中心化网络多智能体强化学习算法，该算法允许非对抗性智能体在对抗方存在的情况下达成共识。

    

    去中心化的多智能体强化学习算法已经在文献中变得流行，因为它允许异构体拥有自己的奖励函数，相对于假定所有智能体拥有共同奖励函数的经典多智能体马尔可夫决策过程(MDP)设置。在本文中，我们遵循现有合作MARL的工作，在一个连接的时变网络中，智能体可以相互交换信息以达成共识。我们在共识更新中引入漏洞，在现有的MARL算法中，智能体可以偏离其常规的共识更新，我们称之为对抗性智能体。然后，我们提供了一种算法，使非对抗性智能体可以在受到约束条件限制的情况下，在对抗性存在的情况下达成共识。

    Decentralized multi-agent reinforcement learning (MARL) algorithms have become popular in the literature since it allows heterogeneous agents to have their own reward functions as opposed to canonical multi-agent Markov Decision Process (MDP) settings which assume common reward functions over all agents. In this work, we follow the existing work on collaborative MARL where agents in a connected time varying network can exchange information among each other in order to reach a consensus. We introduce vulnerabilities in the consensus updates of existing MARL algorithms where agents can deviate from their usual consensus update, who we term as adversarial agents. We then proceed to provide an algorithm that allows non-adversarial agents to reach a consensus in the presence of adversaries under a constrained setting.
    
[^152]: 从运动观测中学习神经本构法来实现可推广的PDE动力学研究

    Learning Neural Constitutive Laws From Motion Observations for Generalizable PDE Dynamics. (arXiv:2304.14369v1 [cs.LG])

    [http://arxiv.org/abs/2304.14369](http://arxiv.org/abs/2304.14369)

    本文提出了一种混合神经网络和偏微分方程的方法，用于从运动观测中学习可推广的PDE动力学，并介绍了一种新框架"神经本构法"，该框架利用了一种严格保证标准本构先验的网络架构，以此来学习本构模型。

    

    我们提出了一种混合神经网络(NN)和偏微分方程(PDE)方法,用于从运动观测中学习可推广的PDE动力学。许多NN方法学习端到端的模型,隐含地建模了所建立的PDE和本构模型(或材料模型)。这些方法没有明确的PDE知识,不能保证物理正确性,具有有限的广泛适用性。我们认为,所建立的PDEs通常是众所周知的,应该明确强制执行,而不是学习。相反,本构模型由于其数据拟合性质,特别适合于学习。为此,我们介绍了一种称为“神经本构法”(NCLaw)的新框架,它利用了一种严格保证标准本构先验的网络架构,包括旋转等变性和未变形状态平衡。我们将这个网络嵌入可微分的模拟中,通过最小化基于模拟和m之间的差异的损失函数来训练模型

    We propose a hybrid neural network (NN) and PDE approach for learning generalizable PDE dynamics from motion observations. Many NN approaches learn an end-to-end model that implicitly models both the governing PDE and constitutive models (or material models). Without explicit PDE knowledge, these approaches cannot guarantee physical correctness and have limited generalizability. We argue that the governing PDEs are often well-known and should be explicitly enforced rather than learned. Instead, constitutive models are particularly suitable for learning due to their data-fitting nature. To this end, we introduce a new framework termed "Neural Constitutive Laws" (NCLaw), which utilizes a network architecture that strictly guarantees standard constitutive priors, including rotation equivariance and undeformed state equilibrium. We embed this network inside a differentiable simulation and train the model by minimizing a loss function based on the difference between the simulation and the m
    
[^153]: 速度即一切：通过GPU-aware优化在设备上加速大型扩散模型

    Speed Is All You Need: On-Device Acceleration of Large Diffusion Models via GPU-Aware Optimizations. (arXiv:2304.11267v1 [cs.CV])

    [http://arxiv.org/abs/2304.11267](http://arxiv.org/abs/2304.11267)

    该研究提出一系列通过GPU-aware优化大型扩散模型的方法，实现在配备GPU的移动设备上极快的推论延迟，扩大了生成性人工智能的适用范围并改善了用户体验。

    

    基础模型的快速发展和应用已经彻底改变了人工智能领域。大型扩散模型因其生成逼真图像和支持各种任务的能力而受到了重视。这些模型在设备上的部署带来了许多好处，比如降低服务器成本、离线功能和改善用户隐私。然而，在设备上共同的大型扩散模型具有超过10亿的参数，由于设备的受限计算和内存资源存在挑战。我们提出了一系列大型扩散模型实现优化，以在配备GPU的移动设备上实现迄今为止最快的推论延迟（对于一个512x512的图像，在三星S23 Ultra上的"稳定扩散1.4"下，进行20次迭代的情况下，无需int8量化，推论的延迟小于12秒）。这些优化扩大了生成性人工智能的适用范围并改善了各种设备上的整体用户体验。

    The rapid development and application of foundation models have revolutionized the field of artificial intelligence. Large diffusion models have gained significant attention for their ability to generate photorealistic images and support various tasks. On-device deployment of these models provides benefits such as lower server costs, offline functionality, and improved user privacy. However, common large diffusion models have over 1 billion parameters and pose challenges due to restricted computational and memory resources on devices. We present a series of implementation optimizations for large diffusion models that achieve the fastest reported inference latency to-date (under 12 seconds for Stable Diffusion 1.4 without int8 quantization on Samsung S23 Ultra for a 512x512 image with 20 iterations) on GPU-equipped mobile devices. These enhancements broaden the applicability of generative AI and improve the overall user experience across a wide range of devices.
    
[^154]: 训练神经网络语音分离模型的数据采样策略研究

    On Data Sampling Strategies for Training Neural Network Speech Separation Models. (arXiv:2304.07142v1 [cs.SD])

    [http://arxiv.org/abs/2304.07142](http://arxiv.org/abs/2304.07142)

    本文研究了训练神经网络语音分离模型的数据采样策略对模型性能的影响。研究表明，对于特定的信号长度分布，采用特定的训练信号长度限制可以获得更好的性能。

    

    语音分离仍然是多说话信号处理的重要领域。深度神经网络（DNN）模型在许多语音分离基准上取得了最佳性能。一些模型需要较长的训练时间和较高的内存需求。以前的研究提出了缩短训练示例以解决这些问题，但这对模型性能的影响尚不清楚。本文分析了应用这些训练信号长度（TSL）限制对两个语音分离模型（SepFormer，一个变换器模型，和Conv-TasNet，一个卷积模型）的影响。使用WJS0-2Mix，WHAMR和Libri2Mix数据集来分析信号长度分布及其对训练效率的影响。研究表明，对于特定的分布，应用特定的TSL限制可以获得更好的性能。这主要是由于对波形起始索引进行随机采样导致更多独特的示例用于训练。

    Speech separation remains an important area of multi-speaker signal processing. Deep neural network (DNN) models have attained the best performance on many speech separation benchmarks. Some of these models can take significant time to train and have high memory requirements. Previous work has proposed shortening training examples to address these issues but the impact of this on model performance is not yet well understood. In this work, the impact of applying these training signal length (TSL) limits is analysed for two speech separation models: SepFormer, a transformer model, and Conv-TasNet, a convolutional model. The WJS0-2Mix, WHAMR and Libri2Mix datasets are analysed in terms of signal length distribution and its impact on training efficiency. It is demonstrated that, for specific distributions, applying specific TSL limits results in better performance. This is shown to be mainly due to randomly sampling the start index of the waveforms resulting in more unique examples for tra
    
[^155]: 深度学习应用于课程评论的观点挖掘和主题分类

    Deep Learning for Opinion Mining and Topic Classification of Course Reviews. (arXiv:2304.03394v1 [cs.CL])

    [http://arxiv.org/abs/2304.03394](http://arxiv.org/abs/2304.03394)

    本文利用自然语言处理和深度学习技术，通过比较传统方法和现代机器学习方法，展示了如何处理大量课程评论，进行情感极性分析和主题分类。

    

    对于教育工作者和管理者来说，学生对课程的反馈意见非常重要，无论课程的类型或机构如何。在机构级别或在线论坛上处理大量的开放反馈变得不可行。在本文中，我们收集和预处理了大量公开可用的课程评论。我们应用机器学习技术，目的是了解学生的情感和主题。具体而言，我们利用了当前的自然语言处理技术，如词嵌入和深度神经网络，并采用最先进的BERT（双向编码器表示来自变压器）、RoBERTa（经过优化的BERT方法）和XLNet（广义自回归预训练）技术。我们进行了广泛的实验，比较了这些技术与传统方法的差异。这项比较研究展示了如何应用现代机器学习方法进行情感极性分析和主题分类。

    Student opinions for a course are important to educators and administrators, regardless of the type of the course or the institution. Reading and manually analyzing open-ended feedback becomes infeasible for massive volumes of comments at institution level or online forums. In this paper, we collected and pre-processed a large number of course reviews publicly available online. We applied machine learning techniques with the goal to gain insight into student sentiments and topics. Specifically, we utilized current Natural Language Processing (NLP) techniques, such as word embeddings and deep neural networks, and state-of-the-art BERT (Bidirectional Encoder Representations from Transformers), RoBERTa (Robustly optimized BERT approach) and XLNet (Generalized Auto-regression Pre-training). We performed extensive experimentation to compare these techniques versus traditional approaches. This comparative study demonstrates how to apply modern machine learning approaches for sentiment polari
    
[^156]: 自监督视频相似性学习

    Self-Supervised Video Similarity Learning. (arXiv:2304.03378v1 [cs.CV])

    [http://arxiv.org/abs/2304.03378](http://arxiv.org/abs/2304.03378)

    本文提出了自监督视频相似性学习的方法S$^2$VS，该方法通过学习实例区分解决多个检索和检测任务，无需用到标注数据，并在各个任务上都达到了最新的性能。

    

    本文介绍了一种基于自监督学习的视频相似性学习方法S$^2$VS。与以往研究不同，这种方法使用自监督学习来实现视频相似性学习，并一次性解决多个检索和检测任务，而不需要使用带标签的数据。通过使用任务定制的增强和InfoNCE损失函数以及在自我相似性和硬负相似性上同时操作的附加损失函数，通过学习实例区分来实现。我们的方法在不同粒度下定义视频相关性的任务上进行了基准测试，涵盖了复制视频到描述相同事件的视频。我们学习了一个单一的通用模型，能够在所有任务上获得最新的表现，超越了以前使用标记数据的方法。代码和预训练模型是公开可用的。

    We introduce S$^2$VS, a video similarity learning approach with self-supervision. Self-Supervised Learning (SSL) is typically used to train deep models on a proxy task so as to have strong transferability on target tasks after fine-tuning. Here, in contrast to prior work, SSL is used to perform video similarity learning and address multiple retrieval and detection tasks at once with no use of labeled data. This is achieved by learning via instance-discrimination with task-tailored augmentations and the widely used InfoNCE loss together with an additional loss operating jointly on self-similarity and hard-negative similarity. We benchmark our method on tasks where video relevance is defined with varying granularity, ranging from video copies to videos depicting the same incident or event. We learn a single universal model that achieves state-of-the-art performance on all tasks, surpassing previously proposed methods that use labeled data. The code and pretrained models are publicly avai
    
[^157]: 关于使用深度学习判断质数整除性

    On the Prime Number Divisibility by Deep Learning. (arXiv:2304.01333v1 [cs.LG])

    [http://arxiv.org/abs/2304.01333](http://arxiv.org/abs/2304.01333)

    本文提出了使用深度学习判断质数整除性的方法，并发现关键在于提供给深度学习模型的特征空间。此外，商业可用的自动化机器学习管道无法解决此问题，需要提供适当的特征工程来解决。研究者还提出了一个封闭式解决方案。

    

    对于确定一个整数是否能够被2、3或其他质数整除这样的任务，对于人类来说可能很简单，但在没有预先指定算法的情况下，这对于计算机来说可能并不容易。本文测试了多个深度学习体系结构和特征工程方法，并评估了在确定大有限整数（高达$2^{32}$）是否能够被小质数整除的情况下，各种框架和网络结构（CNN、RNN、Transformer等）的能力。结果表明，预测质数整除性的能力极大地取决于提供给深度学习模型的特征空间，而不是网络框架或网络结构的复杂性（CNN、RNN、Transformer等）。我们还评估了来自亚马逊、谷歌和微软的商业可用的自动化机器学习（AutoML）管道，并证明除非提供适当的特征工程，否则它们无法解决此问题。我们进一步提出了一个封闭式解决方案来解决这个问题。

    Certain tasks such as determining whether a given integer can be divided by 2, 3, or other prime numbers may be trivial for human beings, but can be less straightforward for computers in the absence of pre-specified algorithms. In this paper, we tested multiple deep learning architectures and feature engineering approaches, and evaluated the scenario of determining divisibility of large finite integers (up to $2^{32}$) by small prime numbers. It turns out that, regardless of the network frameworks or the complexity of the network structures (CNN, RNN, Transformer, etc.), the ability to predict the prime number divisibility critically depends on the feature space fed into the deep learning models. We also evaluated commercially available Automated Machine Learning (AutoML) pipelines from Amazon, Google and Microsoft, and demonstrated that they failed to address this issue unless appropriately engineered features were provided. We further proposed a closed form solution to the problem us
    
[^158]: 卵巢癌组织病理学中的人工智能：一项系统综述

    Artificial Intelligence in Ovarian Cancer Histopathology: A Systematic Review. (arXiv:2303.18005v1 [eess.IV])

    [http://arxiv.org/abs/2303.18005](http://arxiv.org/abs/2303.18005)

    通过对36篇文章的综述，该研究发现人工智能模型在卵巢癌的诊断和预后中显示出有希望的结果，但现有研究受到小样本量，潜在偏见和缺乏外部验证的限制。

    

    目的-特征化和评估已发表的研究，评估利用组织病理学数据进行卵巢癌诊断或预后的人工智能（AI）方法的质量。方法-在2022年1月12日之前，对5个来源进行搜索。包括标准要求研究评估AI在卵巢癌的组织病理学图像上，对卵巢癌，包括输卵管卵巢和腹膜肿瘤的诊断或预后推断。排除评论和非英语文章。对每个包含的模型使用PROBAST评估偏倚风险。结果-共发现1434篇研究文章，其中36篇符合纳入标准。这些研究报告了62个感兴趣的模型，其中包括35个分类器，14个生存预测模型，7个分割模型和6个回归模型。使用1-1375张从1-664个卵巢癌患者中得到的幻灯片开发了这些模型。预测了广泛的结果，包括总体生存（9/62），组织学亚型（7/62）和淋巴结状态（6/62）。结论-基于可用的文献，AI模型在卵巢癌组织病理学的诊断和预后中显示出有希望的结果。但是，现有的研究受到样本量小、潜在的偏见和缺乏外部验证的限制。

    Purpose - To characterise and assess the quality of published research evaluating artificial intelligence (AI) methods for ovarian cancer diagnosis or prognosis using histopathology data. Methods - A search of 5 sources was conducted up to 01/12/2022. The inclusion criteria required that research evaluated AI on histopathology images for diagnostic or prognostic inferences in ovarian cancer, including tubo-ovarian and peritoneal tumours. Reviews and non-English language articles were excluded. The risk of bias was assessed for every included model using PROBAST. Results - A total of 1434 research articles were identified, of which 36 were eligible for inclusion. These studies reported 62 models of interest, including 35 classifiers, 14 survival prediction models, 7 segmentation models, and 6 regression models. Models were developed using 1-1375 slides from 1-664 ovarian cancer patients. A wide array of outcomes were predicted, including overall survival (9/62), histological subtypes (7
    
[^159]: 迭代部分满足的反事实解释：益处和风险

    Iterative Partial Fulfillment of Counterfactual Explanations: Benefits and Risks. (arXiv:2303.11111v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.11111](http://arxiv.org/abs/2303.11111)

    迭代部分满足的反事实解释被广泛用于解释机器学习模型在高风险领域中的推理，我们提出了一个新颖的属性：这种解释在迭代部分履行下的行为。主体可以在接收的解释中部分地履行请求一个新的预测和新的解释，重复此过程，直到预测为正面。

    

    反事实（CF）解释，也称为对比解释和算法回归，被广泛用于解释高风险领域中的机器学习模型。针对一个受到负面预测的主体（例如，拒绝房贷申请），CF解释是相似的情况，但预测结果为正面，这告知主体改进的方法。尽管它们的各种属性已经被研究，例如有效性和稳定性，但我们提供了一种新颖的属性：在迭代部分履行（IPF）下的行为。具体地，在接收到CF解释后，主体可能只能部分地履行它，然后请求一个新的预测及新的解释，重复此过程直到预测为正面。这种部分履行可能是由于主体的能力有限（例如，此时只能支付四张信用卡帐户中的两张）或试图冒险（例如，押注800美元的月薪增长足够）。

    Counterfactual (CF) explanations, also known as contrastive explanations and algorithmic recourses, are popular for explaining machine learning models in high-stakes domains. For a subject that receives a negative model prediction (e.g., mortgage application denial), the CF explanations are similar instances but with positive predictions, which informs the subject of ways to improve. While their various properties have been studied, such as validity and stability, we contribute a novel one: their behaviors under iterative partial fulfillment (IPF). Specifically, upon receiving a CF explanation, the subject may only partially fulfill it before requesting a new prediction with a new explanation, and repeat until the prediction is positive. Such partial fulfillment could be due to the subject's limited capability (e.g., can only pay down two out of four credit card accounts at this moment) or an attempt to take the chance (e.g., betting that a monthly salary increase of $800 is enough eve
    
[^160]: 利用人类语言理解能力增强药物研发中的活性预测模型

    Enhancing Activity Prediction Models in Drug Discovery with the Ability to Understand Human Language. (arXiv:2303.03363v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2303.03363](http://arxiv.org/abs/2303.03363)

    本文提出了一种新型活性预测模型，能够通过理解描述任务的文本信息来适应推理时的新预测任务，并在少样本学习基准和药物研发中的零数据问题上都能取得更好的预测性能。

    

    活性和性质预测模型是药物研发和材料科学中的核心工作，但目前它们必须经过训练或微调才能适应新任务。科学语言模型具有零数据和少数据样本的能力，因此对于此类低数据任务，无需训练或微调即可使用。然而，它们在活性预测方面的预测质量不足。本文提出一种新型活性预测模型，能够通过理解描述任务的文本信息来适应推理时的新预测任务。为此，我们提出了一种新的结构，具有化学和自然语言输入的分离模块，以及大型生物化学数据库中的对比预训练目标。通过大量实验证明，我们的方法CLAMP在少样本学习基准和药物研发中的零数据问题上都能取得更好的预测性能。我们认为我们的方法的进展归因于情境感知模型和对比学习策略的结合，以及用于结合数据源的对抗性自编码器。

    Activity and property prediction models are the central workhorses in drug discovery and materials sciences, but currently they have to be trained or fine-tuned for new tasks. Without training or fine-tuning, scientific language models could be used for such low-data tasks through their announced zero- and few-shot capabilities. However, their predictive quality at activity prediction is lacking. In this work, we envision a novel type of activity prediction model that is able to adapt to new prediction tasks at inference time, via understanding textual information describing the task. To this end, we propose a new architecture with separate modules for chemical and natural language inputs, and a contrastive pre-training objective on data from large biochemical databases. In extensive experiments, we show that our method CLAMP yields improved predictive performance on few-shot learning benchmarks and zero-shot problems in drug discovery. We attribute the advances of our method to the mo
    
[^161]: Seq-HyGAN: 基于超图注意力网络的序列分类

    Seq-HyGAN: Sequence Classification via Hypergraph Attention Network. (arXiv:2303.02393v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02393](http://arxiv.org/abs/2303.02393)

    本文提出了一种基于超图注意力网络的序列分类模型Seq-HyGAN，通过创建超图和引入注意力机制来处理序列数据中的复杂结构相似性，从而提高分类准确率。

    

    序列分类在不同领域有广泛的实际应用，例如在健康领域中的基因组分类和在商业领域的异常检测。然而，序列数据中缺乏显式的特征，这使得机器学习模型难以处理。虽然神经网络模型通过自动学习特征来解决这个问题，但它们仅限于捕获相邻结构连接并忽略序列之间的全局、高阶信息。为了解决序列分类问题中的这些挑战，我们提出了一种新的超图注意力网络模型——Seq-HyGAN。为了捕捉序列数据之间的复杂结构相似性，我们首先创建一个超图，其中序列被描绘为超边，从序列中提取的子序列被描绘为节点。此外，我们引入了基于注意力的超图神经网络模型，它利用了双层注意力机制。该模型生成一个序列表示

    Sequence classification has a wide range of real-world applications in different domains, such as genome classification in health and anomaly detection in business. However, the lack of explicit features in sequence data makes it difficult for machine learning models. While Neural Network (NN) models address this with learning features automatically, they are limited to capturing adjacent structural connections and ignore global, higher-order information between the sequences. To address these challenges in the sequence classification problems, we propose a novel Hypergraph Attention Network model, namely Seq-HyGAN. To capture the complex structural similarity between sequence data, we first create a hypergraph where the sequences are depicted as hyperedges and subsequences extracted from sequences are depicted as nodes. Additionally, we introduce an attention-based Hypergraph Neural Network model that utilizes a two-level attention mechanism. This model generates a sequence representa
    
[^162]: 通过鉴别性正则化来实现表征解耦

    Representation Disentaglement via Regularization by Identification. (arXiv:2303.00128v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00128](http://arxiv.org/abs/2303.00128)

    本文研究了从观测数据中学习解耦表示的问题，提出通过鉴别性正则化来实现表征解耦，解决了现代深度表征学习模型中出现的纠缠偏差行为问题。

    

    本文研究了从观测数据中学习解耦表示的问题。给定从$p(\mathbf{x}|\mathbf{y})$中生成的具有各自生成变量$\mathbf{y}_c$分解的分布$p(\mathbf{y}) = \prod_{c} p(\mathbf{y}_c )$的观测值${\mathbf{x}^{(i)}}$，我们尝试学习与每个$c$的后验分布$p(\mathbf{z}| \mathbf{x}, \hat{\mathbf{y}}_c)$匹配的解耦表示是否可行。我们认为现代深度表征学习模型无法解决与生成变量之间出现的纠缠偏差行为问题，这种行为上产生偏见。在因果推理的框架下，我们证明了可以在可识别性的条件下对这个问题进行解释和调和，这一点可以在监督或弱监督的条件下实现。因此，我们提出了一种通过鉴别性正则化（ReI）的模块化重新调整方法。

    This work focuses on the problem of learning disentangled representations from observational data. Given observations ${\mathbf{x}^{(i)}}$ for $i=1,...,N $ drawn from $p(\mathbf{x}|\mathbf{y})$ with generative variables $\mathbf{y}$ admitting the distribution factorization $p(\mathbf{y}) = \prod_{c} p(\mathbf{y}_c )$, we ask whether learning disentangled representations matching the space of observations with identification guarantees on the posterior $p(\mathbf{z}| \mathbf{x}, \hat{\mathbf{y}}_c)$ for each $c$, is plausible. We argue modern deep representation learning models of data matching the distributed factorization property are ill-posed with collider bias behaviour; a source of bias producing entanglement between generating variables. Under the rubric of causality, we show this issue can be explained and reconciled under the condition of identifiability; attainable under supervision or a weak-form of it. For this, we propose regularization by identification (ReI), a modular re
    
[^163]: 关于带批量归一化的随机梯度下降训练不稳定性的研究

    On the Training Instability of Shuffling SGD with Batch Normalization. (arXiv:2302.12444v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12444](http://arxiv.org/abs/2302.12444)

    本文研究了随机梯度下降算法与批量归一化的相互作用，在特定网络中表现为单次重排与随机重排收敛到不同的扭曲全局最优点，建议使用随机重排。

    

    本论文研究了随机梯度下降算法与批量归一化的相互作用，发现常用的单次重排和随机重排这两种 SGD 方式的训练稳定性差异非常大。实验证明，在使用批量归一化的线性网络回归问题中，单次重排和随机重排会分别收敛到扭曲的全局最优点，而在分类问题中，我们进一步探究了它们的训练是否会发散，并提出了实证验证结果，建议在使用 SGD 与批量归一化时，优先考虑随机重排。

    We uncover how SGD interacts with batch normalization and can exhibit undesirable training dynamics such as divergence. More precisely, we study how Single Shuffle (SS) and Random Reshuffle (RR) -- two widely used variants of SGD -- interact surprisingly differently in the presence of batch normalization: RR leads to much more stable evolution of training loss than SS. As a concrete example, for regression using a linear network with batch normalization, we prove that SS and RR converge to distinct global optima that are "distorted" away from gradient descent. Thereafter, for classification we characterize conditions under which training divergence for SS and RR can, and cannot occur. We present explicit constructions to show how SS leads to distorted optima in regression and divergence for classification, whereas RR avoids both distortion and divergence. We validate our results by confirming them empirically in realistic settings, and conclude that the separation between SS and RR use
    
[^164]: 基于扩散概率模型的结构化节点分类研究

    Diffusion Probabilistic Models for Structured Node Classification. (arXiv:2302.10506v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10506](http://arxiv.org/abs/2302.10506)

    本文提出了一种新颖的DPM-SNC框架，通过反向扩散过程和流形约束的采样方法实现基于图结构的结构化节点分类，并设计了一种新的训练算法来应用DPMs，最大化一个新的变分下界。实验证明DPMs可以提高GNN的表达能力，提高节点分类的效果。

    

    本文研究了基于图结构的结构化节点分类问题，其中节点标签之间存在依赖关系。特别地，我们集中研究了部分标记图的分类问题，需要将已知标记的信息纳入未知标签的预测中。为解决这个问题，我们提出了一种新颖的框架，利用扩散概率模型进行结构化节点分类(DPM-SNC)。我们的框架的核心是DPM-SNC的出色能力，即(a)使用具有表达性的反向扩散过程学习标签的联合分布，(b)利用流形约束的采样方法在已知标签的条件下进行预测。由于DPMs缺乏部分标记数据的训练算法，因此我们设计了一种新的训练算法，应用DPMs，最大化一个新的变分下界。我们还从理论上分析了如何通过增强GNN的表达能力来提高DPMs对节点分类的效果。

    This paper studies structured node classification on graphs, where the predictions should consider dependencies between the node labels. In particular, we focus on solving the problem for partially labeled graphs where it is essential to incorporate the information in the known label for predicting the unknown labels. To address this issue, we propose a novel framework leveraging the diffusion probabilistic model for structured node classification (DPM-SNC). At the heart of our framework is the extraordinary capability of DPM-SNC to (a) learn a joint distribution over the labels with an expressive reverse diffusion process and (b) make predictions conditioned on the known labels utilizing manifold-constrained sampling. Since the DPMs lack training algorithms for partially labeled data, we design a novel training algorithm to apply DPMs, maximizing a new variational lower bound. We also theoretically analyze how DPMs benefit node classification by enhancing the expressive power of GNNs 
    
[^165]: 不确定条件下的匹配公平性研究

    Fairness in Matching under Uncertainty. (arXiv:2302.03810v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03810](http://arxiv.org/abs/2302.03810)

    论文研究了算法两边市场中的公平性问题，提出了考虑不确定性因素的个体公平性概念，并设计了一个基于线性规划的模型来优化公平的分配

    

    算法在将学生分配到学校、用户分配给广告商以及应聘者安排工作面试时被使用，如今这种算法两边的市场普及程度越来越高，这也引起了人们对市场公平性问题的关注。在这种情境下，匹配的决策应该尊重被匹配个体的偏好，同时也需要在保证公平性的前提下考虑个体的品质以及未来表现。由于这些品质往往是通过机器学习算法从可观察特征中推断出来的，所以其判定是不确定的。本文的主要贡献是，在两边市场的框架下，我们对个体公平性的概念进行了严谨的公理化阐述，从而考虑到了这种不确定性对公平性的潜在影响，也在推断过程中尝试解决了这些影响。同时，我们还设计了一个线性规划模型，来实现对分配的公平性和效用最大化的均衡。

    The prevalence and importance of algorithmic two-sided marketplaces has drawn attention to the issue of fairness in such settings. Algorithmic decisions are used in assigning students to schools, users to advertisers, and applicants to job interviews. These decisions should heed the preferences of individuals, and simultaneously be fair with respect to their merits (synonymous with fit, future performance, or need). Merits conditioned on observable features are always \emph{uncertain}, a fact that is exacerbated by the widespread use of machine learning algorithms to infer merit from the observables. As our key contribution, we carefully axiomatize a notion of individual fairness in the two-sided marketplace setting which respects the uncertainty in the merits; indeed, it simultaneously recognizes uncertainty as the primary potential cause of unfairness and an approach to address it. We design a linear programming framework to find fair utility-maximizing distributions over allocations
    
[^166]: 用机器学习扩展规则型域名系统DNS审查检测的规模

    Augmenting Rule-based DNS Censorship Detection at Scale with Machine Learning. (arXiv:2302.02031v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02031](http://arxiv.org/abs/2302.02031)

    本文探讨了如何使用机器学习模型来帮助简化DNS审查检测过程，提高检测的可靠性，并发现启发式方法所错过的新审查实例和阻止标志。

    

    全球审查的增加导致了大量监测和曝光的测量平台的发展。域名系统（DNS）的审查是不同国家使用的关键机制，目前通过对特定目的地的DNS查询和响应（探针）样本应用启发式方法来检测。然而，这些启发式方法既与平台特定相关，也发现当审查者改变其阻止行为时的脆弱性，需要更可靠的自动化过程来检测审查。在本文中，我们探讨了机器学习（ML）模型如何（1）帮助简化检测过程，（2）提高使用大规模数据集进行审查检测的潜力，（3）发现现有启发式方法所错过的新审查实例和阻止标志。我们的研究表明，经过专家派生标签训练的监督模型可以学习检测审查和异常的已知实例。

    The proliferation of global censorship has led to the development of a plethora of measurement platforms to monitor and expose it. Censorship of the domain name system (DNS) is a key mechanism used across different countries. It is currently detected by applying heuristics to samples of DNS queries and responses (probes) for specific destinations. These heuristics, however, are both platform-specific and have been found to be brittle when censors change their blocking behavior, necessitating a more reliable automated process for detecting censorship.  In this paper, we explore how machine learning (ML) models can (1) help streamline the detection process, (2) improve the potential of using large-scale datasets for censorship detection, and (3) discover new censorship instances and blocking signatures missed by existing heuristic methods. Our study shows that supervised models, trained using expert-derived labels on instances of known anomalies and possible censorship, can learn the det
    
[^167]: 关于两维正弦模型最小绝对偏差估计的一致性和渐近正态性

    On Consistency and Asymptotic Normality of Least Absolute Deviation Estimators for 2-dimensional Sinusoidal Model. (arXiv:2301.03229v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2301.03229](http://arxiv.org/abs/2301.03229)

    本文提出了一种鲁棒的最小绝对偏差估计器，用于2维正弦模型参数估计，在数据存在异常值或重尾噪声时具有优越性并得到强一致性和渐近正态性的保证。

    

    在数字信号处理和时间序列分析中，2维正弦模型参数估计是一个基本问题。本文提出了一种鲁棒的最小绝对偏差 (LAD) 估计器，用于参数估计。该方法提供了一种抗干扰的替代方法，可以应对数据中存在异常值或重尾噪声等非鲁棒估计技术无法处理的情况。我们研究了LAD估计器的重要渐近性质，并证明了2维正弦模型信号参数的LAD估计器具有强一致性和渐近正态性。通过大量的模拟研究，我们进一步说明了使用LAD估计器优于最小二乘估计器的优势。对2维纹理数据的数据分析表明了所提出的LAD方法的实际应用性。

    Estimation of the parameters of a 2-dimensional sinusoidal model is a fundamental problem in digital signal processing and time series analysis. In this paper, we propose a robust least absolute deviation (LAD) estimators for parameter estimation. The proposed methodology provides a robust alternative to non-robust estimation techniques like the least squares estimators, in situations where outliers are present in the data or in the presence of heavy tailed noise. We study important asymptotic properties of the LAD estimators and establish the strong consistency and asymptotic normality of the LAD estimators of the signal parameters of a 2-dimensional sinusoidal model. We further illustrate the advantage of using LAD estimators over least squares estimators through extensive simulation studies. Data analysis of a 2-dimensional texture data indicates practical applicability of the proposed LAD approach.
    
[^168]: Bagging是一种最优的PAC学习器

    Bagging is an Optimal PAC Learner. (arXiv:2212.02264v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.02264](http://arxiv.org/abs/2212.02264)

    Bagging是一种最优的PAC学习器,具有可证明最优的样本复杂度。相对于Hanneke的算法，Bagging更为高效，且只需要logarithmic数量的子样本。

    

    在可实现的情况下确定PAC学习的最优样本复杂度是学习理论几十年来的中心难题。Han口的开创性工作给出了一个具有可证明最优样本复杂度的算法。他的算法基于对训练数据的谨慎和结构化子采样，然后返回在每个子样本上训练的假设的多数投票。在这项工作中，我们证明了实用且经典的heuristic bagging (即自助聚合)，是一种最优的PAC学习器。我们的主要技术贡献是对Bagging的Rademacher复杂度的紧密分析。

    Determining the optimal sample complexity of PAC learning in the realizable setting was a central open problem in learning theory for decades. Finally, the seminal work by Hanneke (2016) gave an algorithm with a provably optimal sample complexity. His algorithm is based on a careful and structured sub-sampling of the training data and then returning a majority vote among hypotheses trained on each of the sub-samples. While being a very exciting theoretical result, it has not had much impact in practice, in part due to inefficiency, since it constructs a polynomial number of sub-samples of the training data, each of linear size.  In this work, we prove the surprising result that the practical and classic heuristic bagging (a.k.a. bootstrap aggregation), due to Breiman (1996), is in fact also an optimal PAC learner. Bagging pre-dates Hanneke's algorithm by twenty years and is taught in most undergraduate machine learning courses. Moreover, we show that it only requires a logarithmic numb
    
[^169]: PASTA：比例幅度谱训练增强用于 Syn-to-Real 领域泛化

    PASTA: Proportional Amplitude Spectrum Training Augmentation for Syn-to-Real Domain Generalization. (arXiv:2212.00979v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.00979](http://arxiv.org/abs/2212.00979)

    本文提出了一种基于比例幅度谱训练增强的方法 PASTA，可有效提高合成数据到真实数据的泛化性能，在多个 Syn-to-Real 任务上均具有优越性能。

    

    合成数据可以提供廉价且丰富的训练数据，适用于真实世界数据稀缺的情况。然而，在真实世界数据上评估的模型在合成数据上训练时表现显著不佳。在本文中，我们提出了 Proportional Amplitude Spectrum Training Augmentation (PASTA)，一种简单而有效的增强策略，可提高合成到真实（Syn-to-Real）泛化性能。 PASTA 在 Fourier 领域中扰动合成图像的幅度谱以生成增强视图。具体而言，使用 PASTA，我们提出了一种结构化扰动策略，其中高频分量相对于低频分量更容易受到扰动。对于语义分割（GTAV-to-Real），目标检测（Sim10K-to-Real）和对象识别（VisDA-C Syn-to-Real）任务，在总共5个 Syn-to-Real 转移中，我们发现 PASTA 的性能优于更复杂的最先进的泛化方法，同时具有互补性。

    Synthetic data offers the promise of cheap and bountiful training data for settings where labeled real-world data is scarce. However, models trained on synthetic data significantly underperform when evaluated on real-world data. In this paper, we propose Proportional Amplitude Spectrum Training Augmentation (PASTA), a simple and effective augmentation strategy to improve out-of-the-box synthetic-to-real (syn-to-real) generalization performance. PASTA perturbs the amplitude spectra of synthetic images in the Fourier domain to generate augmented views. Specifically, with PASTA we propose a structured perturbation strategy where high-frequency components are perturbed relatively more than the low-frequency ones. For the tasks of semantic segmentation (GTAV-to-Real), object detection (Sim10K-to-Real), and object recognition (VisDA-C Syn-to-Real), across a total of 5 syn-to-real shifts, we find that PASTA outperforms more complex state-of-the-art generalization methods while being complemen
    
[^170]: 通过 Lov\'asz Local Lemma 进行采样的马尔可夫随机场学习组合结构

    Learning Combinatorial Structures via Markov Random Fields with Sampling through Lov\'asz Local Lemma. (arXiv:2212.00296v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.00296](http://arxiv.org/abs/2212.00296)

    Nelson是一种基于神经网络和Lov\'asz Local Lemma的方法，使用约束的马尔可夫随机场模型生成满足组合约束条件的样本。

    

    学习组合结构的生成模型在许多应用中具有革命性的影响，但现有方法无法提供高效且准确的学习结果，由于学习目标受到组合约束条件的制约，其梯度估计非常复杂。我们开发了基于 Lov\'asz Local Lemma 的神经网络（Nelson），它能够从约束的马尔可夫随机场模型的分布中生成满足组合约束条件的样本。

    Generative models for learning combinatorial structures have transformative impacts in many applications. However, existing approaches fail to offer efficient and accurate learning results. Because of the highly intractable nature of the gradient estimation of the learning objective subject to combinatorial constraints. Existing gradient estimation methods would easily run into exponential time/memory space, or incur huge estimation errors due to improper approximation. We develop NEural Lovasz Sampler (Nelson), a neural network based on Lov\'asz Local Lemma (LLL). We show it guarantees to generate samples satisfying combinatorial constraints from the distribution of the constrained Markov Random Fields model (MRF) under certain conditions. We further present a fully differentiable contrastive-divergence-based learning framework on constrained MRF (Nelson-CD). Meanwhile, Nelson-CD being fully differentiable allows us to take advantage of the parallel computing power of GPUs, resulting 
    
[^171]: 对抗性廉价交流

    Adversarial Cheap Talk. (arXiv:2211.11030v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.11030](http://arxiv.org/abs/2211.11030)

    本文提出了一种新型对抗性设置，在其中对手只能将信息附加到受害者的观察中，从而产生最小的影响范围，并提出对抗性廉价交流（ACT）算法进行对手训练。在高度受限的情况下，使用ACT训练的对手仍会对受害者的训练和测试表现产生显著影响，揭示了强化学习算法中的一种新的攻击向量。

    

    强化学习中的对抗性攻击通常假定攻击者可以高度特权地访问受害者的参数、环境或数据。本文提出了一种称为廉价交流MDP的新型对抗性设置，其中对手只能将确定性信息附加到受害者的观察中，从而产生最小的影响范围。对手不能掩盖地面事实，影响基本环境动态或奖励信号，引入不稳定性，增加随机性，看到受害者的动作或访问他们的参数。此外，我们提出了一种简单的元学习算法，称为对抗性廉价交流（ACT），在这种设置中对对手进行训练。我们证明，即使在高度受限的情况下，使用ACT训练的对手仍会显着影响受害者的训练和测试表现。影响训练时间表现揭示了一种新的攻击向量，并为现有强化学习算法的成功和失败模式提供了见解。

    Adversarial attacks in reinforcement learning (RL) often assume highly-privileged access to the victim's parameters, environment, or data. Instead, this paper proposes a novel adversarial setting called a Cheap Talk MDP in which an Adversary can merely append deterministic messages to the Victim's observation, resulting in a minimal range of influence. The Adversary cannot occlude ground truth, influence underlying environment dynamics or reward signals, introduce non-stationarity, add stochasticity, see the Victim's actions, or access their parameters. Additionally, we present a simple meta-learning algorithm called Adversarial Cheap Talk (ACT) to train Adversaries in this setting. We demonstrate that an Adversary trained with ACT still significantly influences the Victim's training and testing performance, despite the highly constrained setting. Affecting train-time performance reveals a new attack vector and provides insight into the success and failure modes of existing RL algorith
    
[^172]: 运动感知标记选择实现高效视频表征学习

    Efficient Video Representation Learning via Motion-Aware Token Selection. (arXiv:2211.10636v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.10636](http://arxiv.org/abs/2211.10636)

    该论文提出了一种新的运动感知标记选择方法，针对视频中不同补丁的信息密度，选择包含丰富动态特性的标记，放弃无效的标记，从而大大降低计算和存储需求，实现了在单台机器上进行预训练和微调而不影响性能。

    

    最近出现的蒙版视频建模技术通过在视频的自我监督学习中获得了显着的优势。然而，由于随机蒙版策略导致预测无效的标记/帧，这些技术需要大量的计算和存储，需要昂贵的计算机和大量显卡进行训练。我们利用视频补丁中的不均匀信息密度，并提出一种新的标记选择方法：MATS：运动感知标记选择，在自监督预训练和微调过程中找到包含丰富动态特性的标记，并放弃无效的标记，我们还提出了自适应帧选择策略，使模型能够关注最重要和因果性的帧，并使计算和存储需求得到显着降低，使得在单台机器上进行预训练和微调而不影响性能。

    Recently emerged Masked Video Modeling techniques demonstrated their potential by significantly outperforming previous methods in self-supervised learning for video. However, they require an excessive amount of computations and memory while predicting uninformative tokens/frames due to random masking strategies, requiring excessive computing power for training. (e.g., over 16 nodes with 128 NVIDIA A100 GPUs). To resolve this issue, we exploit the unequal information density among the patches in videos and propose a new token selection method, MATS: Motion-Aware Token Selection, that finds tokens containing rich motion features and drops uninformative ones during both self-supervised pre-training and fine-tuning. We further present an adaptive frame selection strategy that allows the model to focus on informative and causal frames with minimal redundancy. Our method significantly reduces computation and memory requirements, enabling the pre-training and fine-tuning on a single machine w
    
[^173]: 可解释的多智能体强化学习中的行为建议

    Explainable Action Advising for Multi-Agent Reinforcement Learning. (arXiv:2211.07882v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.07882](http://arxiv.org/abs/2211.07882)

    引入可解释行为建议的多智能体强化学习框架，使得学生可以理解所学的内容并进行推理从而提高样本效率和学习效果

    

    行为建议是一种基于师生范式的强化学习知识转移技术。专家老师在训练期间提供建议，以提高学生的样本效率和策略表现。这种建议通常以状态-动作对的形式给出。然而，这使得学生难以推理和应用于新颖状态。我们引入了可解释的行为建议，其中老师提供行为建议和相关的解释，说明为什么选取该行为.这允许学生自我反思所学的内容，实现建议的泛化，并导致学习效率的提高——即使在老师不理想的情况下，也可以有效地应用于单智能体和多智能体场景中。我们通过实验证明，与最先进的方法相比，我们的框架可以产生更好的策略回报和收敛速率。

    Action advising is a knowledge transfer technique for reinforcement learning based on the teacher-student paradigm. An expert teacher provides advice to a student during training in order to improve the student's sample efficiency and policy performance. Such advice is commonly given in the form of state-action pairs. However, it makes it difficult for the student to reason with and apply to novel states. We introduce Explainable Action Advising, in which the teacher provides action advice as well as associated explanations indicating why the action was chosen. This allows the student to self-reflect on what it has learned, enabling advice generalization and leading to improved sample efficiency and learning performance - even in environments where the teacher is sub-optimal. We empirically show that our framework is effective in both single-agent and multi-agent scenarios, yielding improved policy returns and convergence rates when compared to state-of-the-art methods
    
[^174]: 使用对比剪枝权重训练去偏置子网络

    Training Debiased Subnetworks with Contrastive Weight Pruning. (arXiv:2210.05247v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.05247](http://arxiv.org/abs/2210.05247)

    本文探讨了在存在强假相关的偏置网络中提取最优无偏子网络的问题，并提出了使用对比剪枝权重训练实现去偏置子网络的算法 DCWP，在多个应用中都有良好的效果。

    

    神经网络通常存在偏置性，导致提供具有误导性的统计证据，不能很好地推广。因此，提出了在偏置网络中提取最优无偏功能子网络的问题。本文首先提出了现有算法在探索具有强假相关性的无偏子网络存在限制的理论洞见，然后进一步阐明了偏差冲突样本对结构学习的重要性，并基于学习的（伪）无偏样本和选择性偏差冲突样本，提出了去偏置对比剪枝（DCWP）算法。在图像分类、语言模型和强化学习等各种应用中验证了 DCWP 的有效性。

    Neural networks are often biased to spuriously correlated features that provide misleading statistical evidence that does not generalize. This raises an interesting question: ``Does an optimal unbiased functional subnetwork exist in a severely biased network? If so, how to extract such subnetwork?" While empirical evidence has been accumulated about the existence of such unbiased subnetworks, these observations are mainly based on the guidance of ground-truth unbiased samples. Thus, it is unexplored how to discover the optimal subnetworks with biased training datasets in practice. To address this, here we first present our theoretical insight that alerts potential limitations of existing algorithms in exploring unbiased subnetworks in the presence of strong spurious correlations. We then further elucidate the importance of bias-conflicting samples on structure learning. Motivated by these observations, we propose a Debiased Contrastive Weight Pruning (DCWP) algorithm, which probes unbi
    
[^175]: 简即是美：针对准确、鲁棒和可解释的图挖掘的SlimG算法

    Less is More: SlimG for Accurate, Robust, and Interpretable Graph Mining. (arXiv:2210.04081v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.04081](http://arxiv.org/abs/2210.04081)

    SlimG算法是一个简单而有效的图挖掘算法，它基于小心的简单原则，能够在准确性、鲁棒性、可扩展性和可解释性等方面都表现出色，在各种图场景中优于最先进的GNN算法，是一个有前途的实用算法。

    

    在各种可能带有噪声特征和结构的图中，我们如何解决半监督节点分类问题？图神经网络（GNNs）在许多图挖掘任务中取得了成功，但由于训练难度、超参数调整和模型自身选择的困难，它们对于各种图方案的普适性有限。爱因斯坦说过：“让一切尽可能简单，但不要过于简单。”我们重新诠释了这个原则，提出了“小心的简单原则”：一个精心设计的简单模型可以在现实世界的图中超越复杂的模型。基于这一原则，我们提出了SlimG算法，用于半监督节点分类，表现出四个理想的性质：（a）准确，在13个真实数据集中赢得或并列第一；（b）鲁棒，在处理所有图数据场景（同质性、异质性、随机结构、噪声特征等）方面是唯一的；（c）快速可扩展，在百万级图中训练速度最快18倍；（d）可解释，提供它是如何工作的洞察。SlimG算法简单而有效，因为它基于一个直观的归纳偏置，但具有精心设计的消息传递机制。实验结果表明，在各种图场景中，SlimG算法优于最先进的GNN算法，是一个有前途的实用候选算法。

    How can we solve semi-supervised node classification in various graphs possibly with noisy features and structures? Graph neural networks (GNNs) have succeeded in many graph mining tasks, but their generalizability to various graph scenarios is limited due to the difficulty of training, hyperparameter tuning, and the selection of a model itself. Einstein said that we should "make everything as simple as possible, but not simpler." We rephrase it into the careful simplicity principle: a carefully-designed simple model can surpass sophisticated ones in real-world graphs. Based on the principle, we propose SlimG for semi-supervised node classification, which exhibits four desirable properties: It is (a) accurate, winning or tying on 10 out of 13 real-world datasets; (b) robust, being the only one that handles all scenarios of graph data (homophily, heterophily, random structure, noisy features, etc.); (c) fast and scalable, showing up to 18 times faster training in million-scale graphs; a
    
[^176]: 事实破坏者：针对事实验证系统的证据操纵攻击分类法

    Fact-Saboteurs: A Taxonomy of Evidence Manipulation Attacks against Fact-Verification Systems. (arXiv:2209.03755v4 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2209.03755](http://arxiv.org/abs/2209.03755)

    本研究提出了一种分类法，涵盖针对在线证据的伪装和虚假信息等两种攻击目标和不同的威胁模型维度。我们设计并提出了几种可能的攻击方法，展示了在多种情况下，可以对证据中与事实相关的部分进行微小的修改，并生成无法被自动识别的虚假信息。

    

    误导和虚假信息对我们的安全和稳定构成了严重的全球威胁。为了应对在线虚假信息规模的挑战，研究人员一直在努力通过检索和验证相关证据来自动进行事实核查。然而，尽管有许多进展，但对此类系统可能面临的攻击向量的全面评估仍然缺乏。本研究假设存在一个敌对方，通过伪装相关证据或者提供具有误导性的信息，自动干扰在线证据，以破坏事实验证模型。我们首先提出一个分类法，涵盖这两个目标和不同的威胁模型维度。基于此，我们设计并提出了几种可能的攻击方法。我们展示了在多种情况下，可以对证据中与事实相关的部分进行微小的修改，并生成与原始证据脱节的虚假信息，从而破坏事实验证结果，使其无法被自动识别。

    Mis- and disinformation are a substantial global threat to our security and safety. To cope with the scale of online misinformation, researchers have been working on automating fact-checking by retrieving and verifying against relevant evidence. However, despite many advances, a comprehensive evaluation of the possible attack vectors against such systems is still lacking. Particularly, the automated fact-verification process might be vulnerable to the exact disinformation campaigns it is trying to combat. In this work, we assume an adversary that automatically tampers with the online evidence in order to disrupt the fact-checking model via camouflaging the relevant evidence or planting a misleading one. We first propose an exploratory taxonomy that spans these two targets and the different threat model dimensions. Guided by this, we design and propose several potential attack methods. We show that it is possible to subtly modify claim-salient snippets in the evidence and generate diver
    
[^177]: 基于数据驱动的最优化因果推断影响函数

    Data-Driven Influence Functions for Optimization-Based Causal Inference. (arXiv:2208.13701v4 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2208.13701](http://arxiv.org/abs/2208.13701)

    本文提出了一种利用有限差分逼近统计泛函Gateaux导数的构造算法，并研究了从数据中进行概率分布估计的情况下的Gateaux导数估计。研究结果为因果推断和动态治疗方案等问题提供了解决方案。

    

    本研究探讨了一种利用有限差分逼近统计泛函Gateaux导数的构造算法，重点研究了在因果推断中出现的泛函。我们研究了概率分布未知但需要从数据中进行估计的情况。这些估计分布引导了经验Gateaux导数，我们研究了经验、数值和解析Gateaux导数之间的关系。从干预均值（平均潜在结果）的案例入手，我们勾勒了有限差分和解析Gateaux导数之间的关系。然后，我们得出了关于扰动和平滑的数值逼近速率要求，以保持单步调整的统计优势，例如速率双重强健性。接下来，我们研究了更复杂的泛函，如动态治疗方案、无限时段Markov决策中策略优化的线性规划形式。

    We study a constructive algorithm that approximates Gateaux derivatives for statistical functionals by finite differencing, with a focus on functionals that arise in  causal inference. We study the case where probability distributions are not known a priori but need to be estimated from data. These estimated distributions lead to empirical Gateaux derivatives, and we study the relationships between empirical, numerical, and analytical Gateaux derivatives. Starting with a case study of the interventional mean (average potential outcome), we delineate the relationship between finite differences and the analytical Gateaux derivative. We then derive requirements on the rates of numerical approximation in perturbation and smoothing that preserve the statistical benefits of one-step adjustments, such as rate double robustness. We then study more complicated functionals such as dynamic treatment regimes, the linear-programming formulation for policy optimization in infinite-horizon Markov dec
    
[^178]: 寻找可重用的机器学习组件以构建编程语言处理流水线

    Finding Reusable Machine Learning Components to Build Programming Language Processing Pipelines. (arXiv:2208.05596v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.05596](http://arxiv.org/abs/2208.05596)

    该论文提出一种寻找可重用的机器学习组件以构建编程语言处理流水线的方法，并通过收集、分析代表性论文来鉴定和描述关键概念，从而改善机器学习组件的可发现性、可访问性、互操作性和可重用性。

    

    机器学习在编程语言处理方面取得了巨大的进展，越来越多的人对这一领域表现出浓厚的兴趣。然而，对于新的研究和开发人员来说，由于需要解决各种不同的编程语言处理任务、发布的数据集和模型数量众多以及涉及的编译器或工具集复杂等因素，很难找到构建自己的机器学习流水线所需的正确组件。为了改善机器学习组件的可发现性、可访问性、互操作性和可重用性，我们收集并分析了一组代表性的基于机器学习的编程语言处理论文。然后，我们鉴定和描述了关键概念，包括编程语言处理任务、模型架构和支持工具。最后，我们展示了一些应用示例，说明如何利用可重用组件构建机器学习流水线来解决一组编程语言处理任务。

    Programming Language Processing (PLP) using machine learning has made vast improvements in the past few years. Increasingly more people are interested in exploring this promising field. However, it is challenging for new researchers and developers to find the right components to construct their own machine learning pipelines, given the diverse PLP tasks to be solved, the large number of datasets and models being released, and the set of complex compilers or tools involved. To improve the findability, accessibility, interoperability and reusability (FAIRness) of machine learning components, we collect and analyze a set of representative papers in the domain of machine learning-based PLP. We then identify and characterize key concepts including PLP tasks, model architectures and supportive tools. Finally, we show some example use cases of leveraging the reusable components to construct machine learning pipelines to solve a set of PLP tasks.
    
[^179]: 解决机器学习众包工作者的人类受试者身份问题

    Resolving the Human Subjects Status of Machine Learning's Crowdworkers. (arXiv:2206.04039v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2206.04039](http://arxiv.org/abs/2206.04039)

    机器学习在研究中使用的众包工作者问题引起了对其受试者身份的争议与监管合规性，本文针对该问题进行研究，重点关注了自然语言处理领域中的研究监管挑战。

    

    近年来，机器学习(Machine Learning, ML)在构建数据集和解决需要人类交互或判断的研究问题方面，已经严重依赖于众包工作者。由于执行的任务多样化和数据用途的多样性，很难确定何时将众包工作者视为工人(而非人类受试者)。这些困难加剧了政策的冲突，一些机构和研究人员将所有ML众包工作者视为人类受试者，而其他人则认为它们很少构成人类受试者。值得注意的是，包括众包工作的鲜有ML论文提到IRB的监督，引发了违反道德和法规要求的可能性。我们研究了ML众包研究的适当划定，并关注自然语言处理领域暴露出的独特研究监督挑战。至关重要的是，在美国公共规则下，这些判断取决于关于问题的确定，涉及谁(或什么)的问题。

    In recent years, machine learning (ML) has relied heavily on crowdworkers both for building datasets and for addressing research questions requiring human interaction or judgment. The diverse tasks performed and uses of the data produced render it difficult to determine when crowdworkers are best thought of as workers (versus human subjects). These difficulties are compounded by conflicting policies, with some institutions and researchers regarding all ML crowdworkers as human subjects and others holding that they rarely constitute human subjects. Notably few ML papers involving crowdwork mention IRB oversight, raising the prospect of non-compliance with ethical and regulatory requirements. We investigate the appropriate designation of ML crowdsourcing studies, focusing our inquiry on natural language processing to expose unique challenges for research oversight. Crucially, under the U.S. Common Rule, these judgments hinge on determinations of aboutness, concerning both whom (or what) 
    
[^180]: 校准和去偏图卷积网络的逐层采样

    Calibrate and Debias Layer-wise Sampling for Graph Convolutional Networks. (arXiv:2206.00583v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.00583](http://arxiv.org/abs/2206.00583)

    本文根据矩阵近似的角度重新审视了GCN节点嵌入聚合的逐层采样方法，并提出了解决次优采样概率和估计偏差问题的两种新方案。实验证明这些改进是有效的。

    

    已经开发了多种基于采样的方法来近似和加速图卷积网络（GCN）训练中的节点嵌入聚合。其中，逐层方法递归地执行重要性采样，共同选择每层中现有节点的邻居。本文从矩阵近似的角度重新审视了该方法，并确定了现有逐层采样方法中的两个问题：次优采样概率和无重复采样引起的估计偏差。为了解决这些问题，我们相应地提出了两种解决方案：构建采样概率的新原则和一种高效的去偏算法。通过广泛的估计方差分析和常见基准实验，证明了这些改进。代码和算法实现可在 https://github.com/ychen-stat-ml/GCN-layer-wise-sampling 上公开获取。

    Multiple sampling-based methods have been developed for approximating and accelerating node embedding aggregation in graph convolutional networks (GCNs) training. Among them, a layer-wise approach recursively performs importance sampling to select neighbors jointly for existing nodes in each layer. This paper revisits the approach from a matrix approximation perspective, and identifies two issues in the existing layer-wise sampling methods: suboptimal sampling probabilities and estimation biases induced by sampling without replacement. To address these issues, we accordingly propose two remedies: a new principle for constructing sampling probabilities and an efficient debiasing algorithm. The improvements are demonstrated by extensive analyses of estimation variance and experiments on common benchmarks. Code and algorithm implementations are publicly available at https://github.com/ychen-stat-ml/GCN-layer-wise-sampling .
    
[^181]: 基于Actor-Critic的控制感知方法用于时序异常检测

    Temporal Detection of Anomalies via Actor-Critic Based Controlled Sensing. (arXiv:2201.00879v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.00879](http://arxiv.org/abs/2201.00879)

    本研究提出了一种基于Actor-Critic的控制感知方法，并使用贝叶斯公式学习过程状态的后验概率，以解决二元随机过程的异常检测问题。其能够优于现有方法，在低信噪比情况下有效地检测时序数据中的异常情况。

    

    本文讨论了如何在监测一组二元随机过程时，当其中异常个数超过一个阈值时发出警报。为此，决策者选择和检测一组子过程，以获取它们状态（正常或非正常）的噪声估计。基于接受到的观测，决策者首先确定是否宣布异常数超过阈值，或继续观察。当决策是继续时，它会决定是否在下一个时刻收集观测，还是将其推迟到以后的某个时刻。如果它选择收集观测，它还将确定需侦测的子过程组。为了设计这种三步顺序决策过程，我们使用贝叶斯公式学习过程状态的后验概率。使用后验概率，我们构造了一个马尔可夫决策过程，并使用深度的Actor-Critic强化学习方法来解决它。我们提出的方法被称为AC-CST(基于Actor-Critic的控制感知方法用于时序异常检测)，在检测时间数据中的异常情况方面的性能优于现有方法，特别是在信噪比低的情况下。

    We address the problem of monitoring a set of binary stochastic processes and generating an alert when the number of anomalies among them exceeds a threshold. For this, the decision-maker selects and probes a subset of the processes to obtain noisy estimates of their states (normal or anomalous). Based on the received observations, the decisionmaker first determines whether to declare that the number of anomalies has exceeded the threshold or to continue taking observations. When the decision is to continue, it then decides whether to collect observations at the next time instant or defer it to a later time. If it chooses to collect observations, it further determines the subset of processes to be probed. To devise this three-step sequential decision-making process, we use a Bayesian formulation wherein we learn the posterior probability on the states of the processes. Using the posterior probability, we construct a Markov decision process and solve it using deep actor-critic reinforce
    
[^182]: 动态治疗效应：模型错误下的高维推断

    Dynamic treatment effects: high-dimensional inference under model misspecification. (arXiv:2111.06818v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2111.06818](http://arxiv.org/abs/2111.06818)

    本文提出了一种新的鲁棒估计方法来解决动态治疗效应估计中的挑战，提高了在模型错误下的高维环境中的估计鲁棒性和可靠性。

    

    估计动态治疗效应在各个学科中都是至关重要的，可以提供有关干预的时变因果影响的微妙见解。然而，由于“维数灾难”和时变混杂的存在，这种估计存在着挑战，可能导致估计偏误。此外，正确地规定日益增多的治疗分配和多重暴露的结果模型似乎过于复杂。鉴于这些挑战，双重鲁棒性的概念，在允许模型错误的情况下，是非常有价值的，然而在实际应用中并没有实现。本文通过提出新的鲁棒估计方法来解决这个问题，同时对治疗分配和结果模型进行鲁棒估计。我们提出了一种“序列模型双重鲁棒性”的解决方案，证明了当每个时间暴露都是双重鲁棒性的时，可以在多个时间点上实现双重鲁棒性。这种方法提高了高维环境下动态治疗效应估计的鲁棒性和可靠性。

    Estimating dynamic treatment effects is essential across various disciplines, offering nuanced insights into the time-dependent causal impact of interventions. However, this estimation presents challenges due to the "curse of dimensionality" and time-varying confounding, which can lead to biased estimates. Additionally, correctly specifying the growing number of treatment assignments and outcome models with multiple exposures seems overly complex. Given these challenges, the concept of double robustness, where model misspecification is permitted, is extremely valuable, yet unachieved in practical applications. This paper introduces a new approach by proposing novel, robust estimators for both treatment assignments and outcome models. We present a "sequential model double robust" solution, demonstrating that double robustness over multiple time points can be achieved when each time exposure is doubly robust. This approach improves the robustness and reliability of dynamic treatment effe
    
[^183]: 基于异常值消除的防攻击联邦学习算法ARFED

    ARFED: Attack-Resistant Federated averaging based on outlier elimination. (arXiv:2111.04550v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.04550](http://arxiv.org/abs/2111.04550)

    ARFED是一种防御算法，能够消除模型更新中的离群值，不需要假设数据分布、更新相似性或恶意参与者比率，实现了在基准联邦学习数据集上的最先进的防攻击鲁棒性能。

    

    在联邦学习中，每个参与者使用自己的数据训练本地模型，而全局模型是由集成来自这些参与者的模型更新的可信服务器形成的。由于服务器对参与者的训练过程没有影响与可见性以确保隐私，全局模型容易受到数据污染和模型污染等攻击。虽然最近提出了许多用于解决这些攻击的防御算法，但它们经常做出与联邦学习性质不符的强假设，例如假设非IID数据集。此外，它们大多缺乏综合实验分析。在本研究中，提出了一种名为ARFED的防御算法，它不对数据分布、参与者更新的相似性或恶意参与者的比率做任何假设。ARFED主要考虑模型体系结构每一层参与者更新的离群状态，基于到转换版本更新的距离，消除离群值。我们针对包括数据污染、模型污染和混合攻击在内的各种攻击，在基准联邦学习数据集上评估了ARFED。结果表明，ARFED在保持非IID和IID数据集高准确性的同时实现了最先进的防攻击鲁棒性能。

    In federated learning, each participant trains its local model with its own data and a global model is formed at a trusted server by aggregating model updates coming from these participants. Since the server has no effect and visibility on the training procedure of the participants to ensure privacy, the global model becomes vulnerable to attacks such as data poisoning and model poisoning. Although many defense algorithms have recently been proposed to address these attacks, they often make strong assumptions that do not agree with the nature of federated learning, such as assuming Non-IID datasets. Moreover, they mostly lack comprehensive experimental analyses. In this work, we propose a defense algorithm called ARFED that does not make any assumptions about data distribution, update similarity of participants, or the ratio of the malicious participants. ARFED mainly considers the outlier status of participant updates for each layer of the model architecture based on the distance to t
    
[^184]: 基于知识的主动学习

    Knowledge-driven Active Learning. (arXiv:2110.08265v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.08265](http://arxiv.org/abs/2110.08265)

    本文提出了基于知识的主动学习(KAL)框架，将通用领域知识转换为逻辑约束，作为样本选择的指南，使非专业用户能够用更少的样本训练模型，并在降低标记数据需求量的同时保持出色性能。

    

    深度学习模型的部署仍然受限于有限的监督数据问题。为了解决这个问题，主动学习策略旨在最小化训练深度学习模型所需的标记数据量。大多数主动学习策略基于不确定性样本选择，通常仅限于位于决策边界附近的样本。这些技术在理论上是可行的，但基于内容对所选样本的理解并不直观，这进一步导致非专业人士将深度学习视为黑盒子。在这里，我们首次提出考虑通用领域知识，并使非专业用户能够用更少的样本来训练模型。在我们的基于知识的主动学习(KAL)框架中，基于规则的知识被转换为逻辑约束，并检查其违反作为样本选择的自然指南。我们展示了即使是数据和输出类别之间的简单关系也可以提供出色的性能，同时降低标记数据需求量。

    The deployment of Deep Learning (DL) models is still precluded in those contexts where the amount of supervised data is limited. To answer this issue, active learning strategies aim at minimizing the amount of labelled data required to train a DL model. Most active strategies are based on uncertain sample selection, and even often restricted to samples lying close to the decision boundary. These techniques are theoretically sound, but an understanding of the selected samples based on their content is not straightforward, further driving non-experts to consider DL as a black-box. For the first time, here we propose to take into consideration common domain-knowledge and enable non-expert users to train a model with fewer samples. In our Knowledge-driven Active Learning (KAL) framework, rule-based knowledge is converted into logic constraints and their violation is checked as a natural guide for sample selection. We show that even simple relationships among data and output classes offer a
    
[^185]: 显式色彩滤镜空间中的对抗性图像颜色变换

    Adversarial Image Color Transformations in Explicit Color Filter Space. (arXiv:2011.06690v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2011.06690](http://arxiv.org/abs/2011.06690)

    本论文提出了一种新型颜色变换攻击方法AdvCF，它通过简单色彩滤镜参数空间中的梯度信息进行优化，具有可区分但不引人注意的特点，并提供了对模型抵抗对抗性颜色变换的系统分析。

    

    深度神经网络已被证明容易受到对抗性图像的攻击。传统攻击旨在产生无法区分的对抗性图像，但扰动受到严格限制。近期，研究者开始探索可区分但不引人注意的对抗性图像，并证明颜色变换攻击是有效的。在本论文中，我们提出了Adversarial Color Filter (AdvCF)，这是一种通过简单色彩滤镜参数空间中的梯度信息来优化的新型颜色变换攻击方法。特别地，我们明确指定了色彩滤镜空间，以便可以提供对模型抵抗对抗性颜色变换的系统分析，从攻击和防御的角度分析。相比之下，现有的颜色变换攻击由于缺乏这样明确的空间而无法提供系统分析机会。我们进一步证明了我们的AdvCF在愚弄图像分类器方面的有效性。

    Deep Neural Networks have been shown to be vulnerable to adversarial images. Conventional attacks strive for indistinguishable adversarial images with strictly restricted perturbations. Recently, researchers have moved to explore distinguishable yet non-suspicious adversarial images and demonstrated that color transformation attacks are effective. In this work, we propose Adversarial Color Filter (AdvCF), a novel color transformation attack that is optimized with gradient information in the parameter space of a simple color filter. In particular, our color filter space is explicitly specified so that we are able to provide a systematic analysis of model robustness against adversarial color transformations, from both the attack and defense perspectives. In contrast, existing color transformation attacks do not offer the opportunity for systematic analysis due to the lack of such an explicit space. We further demonstrate the effectiveness of our AdvCF in fooling image classifiers and als
    
[^186]: 正交群子群同步问题的统一方法

    A Unified Approach to Synchronization Problems over Subgroups of the Orthogonal Group. (arXiv:2009.07514v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2009.07514](http://arxiv.org/abs/2009.07514)

    本文提出了一个统一方法来解决正交群子群同步问题，该方法通过广义幂方法的迭代修正和恰当的初始步骤可以在某些假设条件下获得较强的理论保证，并且在计算机视觉和传感器网络等领域有实际应用。

    

    同步问题旨在估计一组群元素$G^*_1,\dots,G^*_n\in\mathcal{G}$,基于形如$G^*_i{G^*_j}^{-1}$的所有成对比率子集的嘈杂观测。本文考虑群为正交群的同步问题。我们提出了一种统一的解决方案，它由适当的初始化步骤和基于广义幂法的迭代细化步骤组成，并表明它在对群、测量图、噪声和初始化的某些假设下具有强大的理论保证。

    The problem of synchronization over a group $\mathcal{G}$ aims to estimate a collection of group elements $G^*_1, \dots, G^*_n \in \mathcal{G}$ based on noisy observations of a subset of all pairwise ratios of the form $G^*_i {G^*_j}^{-1}$. Such a problem has gained much attention recently and finds many applications across a wide range of scientific and engineering areas. In this paper, we consider the class of synchronization problems in which the group is a closed subgroup of the orthogonal group. This class covers many group synchronization problems that arise in practice. Our contribution is fivefold. First, we propose a unified approach for solving this class of group synchronization problems, which consists of a suitable initialization step and an iterative refinement step based on the generalized power method, and show that it enjoys a strong theoretical guarantee on the estimation error under certain assumptions on the group, measurement graph, noise, and initialization. Secon
    
[^187]: 提升简单学习器的能力

    Boosting Simple Learners. (arXiv:2001.11704v8 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2001.11704](http://arxiv.org/abs/2001.11704)

    本论文探讨的是提升学习器的方法，关注弱学习器属于一个容量受限的类的假设，并重点关注需要多少个弱学习器才能生成准确的假设。通过设计新颖的算法，只需要约$\tilde{O}({1}/{\gamma})$个弱假设就能够规避经典下界。

    

    提升是一种著名的机器学习方法，基于结合弱且具有适度错误的假设，以生成强且准确的假设。本研究探讨一种假设：弱学习器属于一个有边界容量的类。该假设来自于常见的传统做法，即将弱学习器视为“规则”或“基础类”中的“经验法则”（Schapire和Freund'12、Shalev-Shwartz和Ben-David'14）。具体而言，我们假设弱学习器的VC维度受到限制。我们主要关注两个问题：（i）Oracle复杂度：需要多少个弱学习器才能生成准确的假设？我们设计了一种新颖的提升算法，并证明它规避了Freund和Schapire的经典下界（'95，'12）。尽管下界表明有时需要具有$\gamma$-间隙的$\Omega({1}/{\gamma^2})$个弱假设，但我们的新方法只需要提供约$\tilde{O}({1}/{\gamma})$个弱假设，假设其

    Boosting is a celebrated machine learning approach which is based on the idea of combining weak and moderately inaccurate hypotheses to a strong and accurate one. We study boosting under the assumption that the weak hypotheses belong to a class of bounded capacity. This assumption is inspired by the common convention that weak hypotheses are "rules-of-thumbs" from an "easy-to-learn class". (Schapire and Freund~'12, Shalev-Shwartz and Ben-David '14.) Formally, we assume the class of weak hypotheses has a bounded VC dimension. We focus on two main questions: (i) Oracle Complexity: How many weak hypotheses are needed to produce an accurate hypothesis? We design a novel boosting algorithm and demonstrate that it circumvents a classical lower bound by Freund and Schapire ('95, '12). Whereas the lower bound shows that $\Omega({1}/{\gamma^2})$ weak hypotheses with $\gamma$-margin are sometimes necessary, our new method requires only $\tilde{O}({1}/{\gamma})$ weak hypothesis, provided that the
    

