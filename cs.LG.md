# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [PET Tracer Conversion among Brain PET via Variable Augmented Invertible Network.](http://arxiv.org/abs/2311.00735) | 本研究开发了一种示踪剂转化可逆神经网络（TC-INN），通过深度学习将FDG图像映射到DOPA图像，以解决DOPA在PET成像中广泛应用的挑战。 |
| [^2] | [Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation.](http://arxiv.org/abs/2311.00684) | 本研究提出了两种通过温度缩放实现的注意力对齐策略，通过改善T5在长序列处理中的注意力分布问题，提高了其在语言建模、检索和多文档问答等任务中的长上下文利用能力。 |
| [^3] | [Supervised and Penalized Baseline Correction.](http://arxiv.org/abs/2310.18306) | 本研究改进了受罚基线校正方法，通过利用先验分析物浓度来改善光谱预测性能，并在两个近红外数据集上进行了评估。 |
| [^4] | [Is Channel Independent strategy optimal for Time Series Forecasting?.](http://arxiv.org/abs/2310.17658) | 本文重新考虑了当前通道独立策略在时间序列预测中是否是最佳解决方案，并提出了一种称为CSC的通道自聚类策略来增强性能并减小参数大小。 |
| [^5] | [Fixed-Budget Real-Valued Combinatorial Pure Exploration of Multi-Armed Bandit.](http://arxiv.org/abs/2310.15681) | 本文研究了固定预算下多臂老虎机问题的实数组合纯探索。首先，我们引入了CSA算法，可在臂数指数增长的情况下找到最佳动作；然后，我们提出了Minimax-CombSAR算法，适用于动作类大小为多项式情况，并证明了它的最优性。最后，在实验中与之前的方法进行比较，结果表明我们的算法性能更好。 |
| [^6] | [Machine Translation for Nko: Tools, Corpora and Baseline Results.](http://arxiv.org/abs/2310.15612) | 该论文提出了针对Nko语（一种在多个西非国家使用的语言）开发可用的机器翻译系统的一套工具、资源和基准结果，包括新颖的协作平行文本整理软件、扩展的语料库和基线神经机器翻译结果。 |
| [^7] | [On existence, uniqueness and scalability of adversarial robustness measures for AI classifiers.](http://arxiv.org/abs/2310.14421) | 本文研究了针对AI分类器的对抗鲁棒性度量的存在性、唯一性和可扩展性，提出了可以验证的数学条件，并在合成基准测试和生物医学应用中进行了实际计算和解释。 |
| [^8] | [Adaptive, Doubly Optimal No-Regret Learning in Strongly Monotone and Exp-Concave Games with Gradient Feedback.](http://arxiv.org/abs/2310.14085) | 本文提出了一个自适应的OGD算法\textsf{AdaOGD}，在强凸性下实现了$ O(\log^2(T)) $的后悔，并且在强单调博弈中使得联合行动最后一次收敛到唯一的纳什均衡。 |
| [^9] | [Compositional Abilities Emerge Multiplicatively: Exploring Diffusion Models on a Synthetic Task.](http://arxiv.org/abs/2310.09336) | 组合能力以乘法方式出现：研究了条件扩散模型在合成任务中的组合泛化能力，结果显示这种能力受到底层数据生成过程的结构影响，且模型在学习到更高级的组合时存在困难。 |
| [^10] | [Synthesizing Missing MRI Sequences from Available Modalities using Generative Adversarial Networks in BraTS Dataset.](http://arxiv.org/abs/2310.07250) | 本文提出了一种利用生成对抗网络从已有的模态生成缺失的MRI序列的方法，在BraTS数据集上取得了有希望的结果。 |
| [^11] | [AdaptNet: Policy Adaptation for Physics-Based Character Control.](http://arxiv.org/abs/2310.00239) | AdaptNet是一种基于物理的角色控制的策略调整方法，通过修改现有策略的潜在空间，可以从类似任务中快速学习到新的行为，显著提高训练效率。 |
| [^12] | [Graph-based Neural Weather Prediction for Limited Area Modeling.](http://arxiv.org/abs/2309.17370) | 该论文提出了一种基于图像神经网络的有限区域天气预测方法，并通过使用北欧地区的本地模型进行了验证。 |
| [^13] | [Are Deep Learning Classification Results Obtained on CT Scans Fair and Interpretable?.](http://arxiv.org/abs/2309.12632) | 深度学习在CT扫描分类中的结果往往只关注准确性，而忽视了公正性和解释性，导致模型不可信和不适用于真实场景。 |
| [^14] | [Towards Last-layer Retraining for Group Robustness with Fewer Annotations.](http://arxiv.org/abs/2309.08534) | 本研究发现，在没有群体标注和只有少量类别标注的情况下，最后一层再训练仍然可以有效提高最差群体准确性，从而优于经验风险最小化在整个数据集上的表现，可以实现群体鲁棒性的提升。 |
| [^15] | [When Do Program-of-Thoughts Work for Reasoning?.](http://arxiv.org/abs/2308.15452) | 提出了复杂性影响推理分数（CIRS）来衡量编程语言对推理能力的影响，发现并非所有复杂性的代码数据都可以被学习或理解，适当的复杂性水平对于改善推理能力至关重要。 |
| [^16] | [Thompson Sampling for Real-Valued Combinatorial Pure Exploration of Multi-Armed Bandit.](http://arxiv.org/abs/2308.10238) | 这篇论文介绍了一种名为广义汤普森抽样探索算法，能够解决多臂老虎机实值组合纯探索问题中动作集合大小为指数级别的情况。 |
| [^17] | [The Bias Amplification Paradox in Text-to-Image Generation.](http://arxiv.org/abs/2308.00755) | 本文研究了文本到图像生成中的偏见放大现象，并发现其主要原因是训练数据和模型提示之间的差异。一旦考虑到各种分布差异，偏见放大现象显著减少。 |
| [^18] | [On (Normalised) Discounted Cumulative Gain as an Offline Evaluation Metric for Top-$n$ Recommendation.](http://arxiv.org/abs/2307.15053) | 本文批判性审视了(Normalised) Discounted Cumulative Gain作为Top-n推荐离线评估指标的方法，并研究了何时可以期望这些指标逼近在线实验的金标准结果。 |
| [^19] | [Counterfactual Explanation via Search in Gaussian Mixture Distributed Latent Space.](http://arxiv.org/abs/2307.13390) | 本文介绍了一种通过在自编码器的潜空间中进行高斯混合分布搜索来生成反事实解释的方法。 |
| [^20] | [From random-walks to graph-sprints: a low-latency node embedding framework on continuous-time dynamic graphs.](http://arxiv.org/abs/2307.08433) | 这篇论文介绍了一种在连续时间动态图上具有低延迟的节点嵌入框架，通过提出流式低延迟的近似随机游走特征，计算时间感知节点嵌入以总结多跳信息。 |
| [^21] | [Resetting the Optimizer in Deep RL: An Empirical Study.](http://arxiv.org/abs/2306.17833) | 在深度强化学习中，当优化问题的风景在不同迭代中差异较大时，重置优化器的内部参数可以避免污染和提高性能。 |
| [^22] | [ManimML: Communicating Machine Learning Architectures with Animation.](http://arxiv.org/abs/2306.17108) | ManimML是一个开源Python库，通过动画演示自动生成的ML算法，为机器学习从业者提供了一种简单且熟悉的方式来沟通和可视化ML算法。 |
| [^23] | [Latent SDEs on Homogeneous Spaces.](http://arxiv.org/abs/2306.16248) | 这篇论文研究了在齐次空间上的潜在SDE，通过使用单位球上的SDE进行变分推断，提出了一种简单且直观的表达式来计算近似后验和先验过程之间的KL散度。 |
| [^24] | [PriorBand: Practical Hyperparameter Optimization in the Age of Deep Learning.](http://arxiv.org/abs/2306.12370) | 提出了一种针对深度学习量身定制的超参数优化算法PriorBand，能够同时利用专家信念和廉价的代理任务，具有高效性和抗干扰能力。 |
| [^25] | [AdCraft: An Advanced Reinforcement Learning Benchmark Environment for Search Engine Marketing Optimization.](http://arxiv.org/abs/2306.11971) | AdCraft是一种高级强化学习基准环境，用于模拟出价和预算变化的搜索引擎营销(SEM)活动，可用于评估和提高SEM出价和预算管理相关的RL算法的鲁棒性。 |
| [^26] | [$FPDM$: Domain-Specific Fast Pre-training Technique using Document-Level Metadata.](http://arxiv.org/abs/2306.06190) | 本文提出了$FPDM$，使用文档元数据和领域特定分类作为监督信号，对领域特定语料库进行transformer编码器的预训练。$FPDM$通过句子级别的输入预训练开放领域的编码器，在微调时使用词汇级别的输入，性能优于其他基于transformer的模型。 |
| [^27] | [BackpropTools: A Fast, Portable Deep Reinforcement Learning Library for Continuous Control.](http://arxiv.org/abs/2306.03530) | BackpropTools是一款快速、可移植的连续控制深度强化学习库，它通过模板元编程提供紧密集成的可组合组件，并在异构平台集合上无缝使用，同时在连续控制问题的深度RL代理高效可扩展训练方面具有优势。由于其可移植性和实时保证，它成为了在嵌入式设备上部署学来的策略的有价值的工具。 |
| [^28] | [MiniSUPERB: Lightweight Benchmark for Self-supervised Speech Models.](http://arxiv.org/abs/2305.19011) | 这篇论文提出了一个名为MiniSUPERB的轻量级基准测试，可以有效地评估自监督语音模型的性能，并在计算成本上比SUPERB更低。同时，该论文还研究了在少样本情况下评估SSL语音模型的性能变化。 |
| [^29] | [Error Bounds for Learning with Vector-Valued Random Features.](http://arxiv.org/abs/2305.17170) | 本文提供了对向量值随机特征学习的完整误差分析，包括在模型错误说明下向量值RF估计器的强一致性和在良好规定的情况下极小化最优收敛速率。 |
| [^30] | [Unified Embedding: Battle-Tested Feature Representations for Web-Scale ML Systems.](http://arxiv.org/abs/2305.12102) | 本文介绍了一种名为“特征复用”的框架，它使用单一的表示空间 能够高效有效地学习高质量的特征嵌入，同时区分不同的分类特征。通过在多个公共数据集和新数据集“Web-Available Image Search (WAIS)”上的测试，我们展示了这种方法的优于现有技术的表现。 |
| [^31] | [Generalizing to new calorimeter geometries with Geometry-Aware Autoregressive Models (GAAMs) for fast calorimeter simulation.](http://arxiv.org/abs/2305.11531) | 基于几何感知的自回归模型能够学习电磁量计响应如何随几何形状变化，能够快速有效地模拟非环形的电磁量计。 |
| [^32] | [Explaining black box text modules in natural language with language models.](http://arxiv.org/abs/2305.09863) | 本文介绍了一种名为Summarize and Score（SASC）的方法，该方法可以自动获取黑盒文本模块的自然语言解释以及解释可靠程度的分数。研究者们已经在合成模块和BERT模型中使用SASC，让我们可以解释模块的选择性，这对于增强大型语言模型的可解释性非常重要。 |
| [^33] | [Supervised learning with probabilistic morphisms and kernel mean embeddings.](http://arxiv.org/abs/2305.06348) | 本文提出了监督学习中正确损失函数的概念，其通过概率测度的条件正则概率测度解决线性算子方程的问题得到定义，适用于可测空间的输入空间和标签空间。 |
| [^34] | [Extracting Diagnosis Pathways from Electronic Health Records Using Deep Reinforcement Learning.](http://arxiv.org/abs/2305.06295) | 本文采用深度强化学习算法，基于电子病历来学习获得正确诊断所需的观察序列的最优顺序。因为诊断指南的缺陷，尤其对罕见病或患有多种病的患者，DRL算法具有重要现实意义。 |
| [^35] | [$\beta$-Variational autoencoders and transformers for reduced-order modelling of fluid flows.](http://arxiv.org/abs/2304.03571) | 本文提出了一种使用$\beta$-VAE和Transformer相结合的方法来学习紧凑且近似正交的ROMs，该方法可用于混沌流体流动的降阶建模，并在准确性方面优于其他预测模型。 |
| [^36] | [SS-shapelets: Semi-supervised Clustering of Time Series Using Representative Shapelets.](http://arxiv.org/abs/2304.03292) | 本论文提出了一种名为SS-shapelets的半监督时间序列聚类方法，通过使用少量标记的和传播的伪标记时间序列来发现代表性形状子序列，从而提高聚类准确性。 |
| [^37] | [Towards Multi-User Activity Recognition through Facilitated Training Data and Deep Learning for Human-Robot Collaboration Applications.](http://arxiv.org/abs/2302.05763) | 本研究通过收集单个用户数据并在后处理中合并数据的方法，实现了多用户活动的识别，有望用于人机协作领域。 |
| [^38] | [Improving the Accuracy-Robustness Trade-Off of Classifiers via Adaptive Smoothing.](http://arxiv.org/abs/2301.12554) | 本文研究通过混合标准分类器和鲁棒模型的输出概率来减轻准确性和鲁棒性之间的权衡问题，进而提高分类器的鲁棒性。同时提出了一种自适应平滑的方法，可以降低实现鲁棒性的准确度惩罚。 |
| [^39] | [Is My Prediction Arbitrary? Measuring Self-Consistency in Fair Classification.](http://arxiv.org/abs/2301.11562) | 在公平分类中，模型的预测方差是一个重要但鲜为人知的误差来源问题。作者提出了一个自洽性标准来衡量测量和减少随意性。作者还开发了一个算法来处理随意性预测，并通过实证研究揭示了当前模型无法处理某些类型数据的问题。 |
| [^40] | [Editing Language Model-based Knowledge Graph Embeddings.](http://arxiv.org/abs/2301.10405) | 本文提出了一种新的任务——编辑基于语言模型的知识图谱嵌入，旨在实现对KG嵌入的数据高效和快速更新。针对这一任务，提出了一个简单而强大的方案——KGEditor，可以更好地更新特定事实而不影响其余部分的性能。 |
| [^41] | [Optimal Approximation Rates for Deep ReLU Neural Networks on Sobolev and Besov Spaces.](http://arxiv.org/abs/2211.14400) | 该论文研究了在Sobolev和Besov空间中，使用ReLU激活函数的深度神经网络能够以怎样的参数效率逼近函数，包括$L_p(\Omega)$范数下的误差度量。我们提供了所有$1\leq p,q \leq \infty$和$s>0$的完整解决方案，并引入了一种新的位提取技术来获得尖锐的上界。 |
| [^42] | [Reward is not Necessary: How to Create a Compositional Self-Preserving Agent for Life-Long Learning.](http://arxiv.org/abs/2211.10851) | 这项研究表明，我们可以使用内在动机衡量标准而不依赖于奖励来创建一个具有自我保护能力的智能体。 |
| [^43] | [An Analysis of Abstracted Model-Based Reinforcement Learning.](http://arxiv.org/abs/2208.14407) | 本论文分析了抽象模型驱动的强化学习中的问题，揭示抽象状态会引入样本相关性，而使用鞅不等式可以解决这个问题，从而将现有MBRL算法的保证扩展到带有抽象的设置。 |
| [^44] | [On the Need and Applicability of Causality for Fair Machine Learning.](http://arxiv.org/abs/2207.04053) | 本论文探讨了因果关系在公平机器学习中的必要性和适用性，强调了非因果预测的社会影响和法律反歧视过程依赖于因果主张。同时讨论了在实际场景中应用因果关系所面临的挑战和限制，并提出了可能的解决方案。 |

# 详细

[^1]: PET通过可变增强可逆网络在脑PET之间进行示踪剂转化

    PET Tracer Conversion among Brain PET via Variable Augmented Invertible Network. (arXiv:2311.00735v1 [cs.LG])

    [http://arxiv.org/abs/2311.00735](http://arxiv.org/abs/2311.00735)

    本研究开发了一种示踪剂转化可逆神经网络（TC-INN），通过深度学习将FDG图像映射到DOPA图像，以解决DOPA在PET成像中广泛应用的挑战。

    

    正电子发射断层成像（PET）作为一种具有高生化敏感性的成像技术，已被广泛应用于脑疾病诊断和脑科学研究。由于不同示踪剂在同一区域呈现出不同效果，示踪剂的选择对PET成像变得越来越重要。目前，在神经精神治疗中广泛应用PET成像，发现6-18F-氟-3,4-二羟基-L-苯丙氨酸（DOPA）在这一领域比18F标记的氟代脱氧葡萄糖（FDG）更有效。然而，由于制备复杂性以及其他限制，DOPA远不如FDG被广泛使用。为解决这个问题，提出了一种示踪剂转化可逆神经网络（TC-INN）用于图像投影，通过深度学习将FDG图像映射到DOPA图像。通过从FDG到DOPA生成PET图像，获得更多的诊断信息。

    Positron emission tomography (PET), as an imaging technique with high biochemical sensitivity, has been widely used in diagnosis of encephalopathy and brain science research used in brain disease diagnosis and brain science research. Since different tracers present different effects on the same focal area, the choice of tracers is getting more significant for PET imaging. Nowadays, with the wide application of PET imaging in neuropsychiatric treatment, 6-18F-fluoro-3, 4-dihydroxy-L-phenylalanine (DOPA) has been found to be more effective than 18F-labeled fluorine-2-deoxyglucose (FDG) in this field. However, due to the complexity of its preparation and other limitations, DOPA is far less widely used than FDG. To address this issue, a tracer conversion invertible neural network (TC-INN) for image projection is developed to map FDG images to DOPA images through deep learning. More diagnostic information is obtained by generating PET images from FDG to DOPA. Specifically, the proposed TC-I
    
[^2]: 注意力对齐和灵活的位置嵌入提高了Transformer长度外推能力

    Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation. (arXiv:2311.00684v1 [cs.CL])

    [http://arxiv.org/abs/2311.00684](http://arxiv.org/abs/2311.00684)

    本研究提出了两种通过温度缩放实现的注意力对齐策略，通过改善T5在长序列处理中的注意力分布问题，提高了其在语言建模、检索和多文档问答等任务中的长上下文利用能力。

    

    理想的长度可外推的Transformer语言模型可以处理比训练长度更长的序列而不需要进行长序列微调。这种长上下文利用能力高度依赖于灵活的位置嵌入设计。在调查现有大型预训练Transformer语言模型的灵活性时，我们发现T5系列值得更仔细研究，因为它的位置嵌入捕捉到了丰富而灵活的注意力模式。然而，T5存在着分散的注意力问题：输入序列越长，注意力分布就越平坦。为了缓解这个问题，我们提出了两种通过温度缩放实现的注意力对齐策略。我们的研究结果提高了T5在语言建模、检索和多文档问答方面的长上下文利用能力，而且不需要进行任何微调，这表明灵活的位置嵌入设计和注意力对齐对于Transformer长度外推至关重要。

    An ideal length-extrapolatable Transformer language model can handle sequences longer than the training length without any long sequence fine-tuning. Such long-context utilization capability highly relies on a flexible positional embedding design. Upon investigating the flexibility of existing large pre-trained Transformer language models, we find that the T5 family deserves a closer look, as its positional embeddings capture rich and flexible attention patterns. However, T5 suffers from the dispersed attention issue: the longer the input sequence, the flatter the attention distribution. To alleviate the issue, we propose two attention alignment strategies via temperature scaling. Our findings improve the long-context utilization capability of T5 on language modeling, retrieval, and multi-document question answering without any fine-tuning, suggesting that a flexible positional embedding design and attention alignment go a long way toward Transformer length extrapolation.\footnote{\url
    
[^3]: 监督和受罚基线校正

    Supervised and Penalized Baseline Correction. (arXiv:2310.18306v1 [stat.ML])

    [http://arxiv.org/abs/2310.18306](http://arxiv.org/abs/2310.18306)

    本研究改进了受罚基线校正方法，通过利用先验分析物浓度来改善光谱预测性能，并在两个近红外数据集上进行了评估。

    

    光谱测量可以显示由吸收和散射成分混合引起的扭曲光谱形状。这些扭曲（或基线）通常表现为非恒定偏移或低频振荡。因此，这些基线可能对分析和定量结果产生不利影响。基线校正是一个涵盖了预处理方法的总称，通过获取基线光谱（不需要的扭曲）并通过差异化去除扭曲。然而，当前最先进的基线校正方法即使可用分析物浓度或者它们对观察到的光谱变异有重要贡献，也没有利用它们。我们研究了一类最先进的方法（受罚基线校正）并对其进行修改，使其能够适应先验分析物浓度，从而提高预测性能。将在两个近红外数据集上评估性能，包括经典受罚方法。

    Spectroscopic measurements can show distorted spectra shapes arising from a mixture of absorbing and scattering contributions. These distortions (or baselines) often manifest themselves as non-constant offsets or low-frequency oscillations. As a result, these baselines can adversely affect analytical and quantitative results. Baseline correction is an umbrella term where one applies pre-processing methods to obtain baseline spectra (the unwanted distortions) and then remove the distortions by differencing. However, current state-of-the art baseline correction methods do not utilize analyte concentrations even if they are available, or even if they contribute significantly to the observed spectral variability. We examine a class of state-of-the-art methods (penalized baseline correction) and modify them such that they can accommodate a priori analyte concentration such that prediction can be enhanced. Performance will be access on two near infra-red data sets across both classical penal
    
[^4]: 通道独立策略是否是时间序列预测的最佳解？

    Is Channel Independent strategy optimal for Time Series Forecasting?. (arXiv:2310.17658v1 [cs.LG])

    [http://arxiv.org/abs/2310.17658](http://arxiv.org/abs/2310.17658)

    本文重新考虑了当前通道独立策略在时间序列预测中是否是最佳解决方案，并提出了一种称为CSC的通道自聚类策略来增强性能并减小参数大小。

    

    近年来出现了许多用于长期时间序列预测的模型。最近的研究表明，使用单一线性层的通道相关(CD)或通道独立(CI)建模，甚至可以超过许多复杂模型的性能。然而，当前的研究主要将CD和CI视为两种互补但互斥的方法，无法同时利用这两个极端。而且，CD和CI都是静态策略，无法在没有大量实验的情况下确定是特定数据集的最佳策略。在本文中，我们重新考虑了当前CI策略是否是时间序列预测的最佳解决方案。首先，我们提出了一种简单而有效的策略，称为CSC（通道自聚类策略），用于线性模型。我们的通道自聚类策略增强了CI策略的性能改进，并减小了参数大小。

    There has been an emergence of various models for long-term time series forecasting. Recent studies have demonstrated that a single linear layer, using Channel Dependent (CD) or Channel Independent (CI) modeling, can even outperform a large number of sophisticated models. However, current research primarily considers CD and CI as two complementary yet mutually exclusive approaches, unable to harness these two extremes simultaneously. And it is also a challenging issue that both CD and CI are static strategies that cannot be determined to be optimal for a specific dataset without extensive experiments. In this paper, we reconsider whether the current CI strategy is the best solution for time series forecasting. First, we propose a simple yet effective strategy called CSC, which stands for $\mathbf{C}$hannel $\mathbf{S}$elf-$\mathbf{C}$lustering strategy, for linear models. Our Channel Self-Clustering (CSC) enhances CI strategy's performance improvements while reducing parameter size, fo
    
[^5]: 固定预算下多臂老虎机问题的实数组合纯探索

    Fixed-Budget Real-Valued Combinatorial Pure Exploration of Multi-Armed Bandit. (arXiv:2310.15681v1 [cs.LG])

    [http://arxiv.org/abs/2310.15681](http://arxiv.org/abs/2310.15681)

    本文研究了固定预算下多臂老虎机问题的实数组合纯探索。首先，我们引入了CSA算法，可在臂数指数增长的情况下找到最佳动作；然后，我们提出了Minimax-CombSAR算法，适用于动作类大小为多项式情况，并证明了它的最优性。最后，在实验中与之前的方法进行比较，结果表明我们的算法性能更好。

    

    我们研究了在固定预算设置下多臂老虎机问题的实数组合纯探索。首先，我们引入了连续组合成功分配（CSA）算法，这是第一个能够在动作类大小相对于臂数呈指数增长的情况下确定最佳动作的算法。我们证明了CSA算法的误差概率上界与下界在指数的对数因子内相匹配。然后，我们介绍了另一个算法，名为极小化组合连续接受与拒绝（Minimax-CombSAR）算法，用于动作类大小为多项式情况，并证明它是最优的，与下界相匹配。最后，我们通过实验比较了这些算法与之前的方法，并展示了我们的算法表现更好。

    We study the real-valued combinatorial pure exploration of the multi-armed bandit in the fixed-budget setting. We first introduce the Combinatorial Successive Asign (CSA) algorithm, which is the first algorithm that can identify the best action even when the size of the action class is exponentially large with respect to the number of arms. We show that the upper bound of the probability of error of the CSA algorithm matches a lower bound up to a logarithmic factor in the exponent. Then, we introduce another algorithm named the Minimax Combinatorial Successive Accepts and Rejects (Minimax-CombSAR) algorithm for the case where the size of the action class is polynomial, and show that it is optimal, which matches a lower bound. Finally, we experimentally compare the algorithms with previous methods and show that our algorithm performs better.
    
[^6]: Nko语的机器翻译：工具、语料库和基准结果

    Machine Translation for Nko: Tools, Corpora and Baseline Results. (arXiv:2310.15612v1 [cs.CL])

    [http://arxiv.org/abs/2310.15612](http://arxiv.org/abs/2310.15612)

    该论文提出了针对Nko语（一种在多个西非国家使用的语言）开发可用的机器翻译系统的一套工具、资源和基准结果，包括新颖的协作平行文本整理软件、扩展的语料库和基线神经机器翻译结果。

    

    目前，尼科语（一种在多个西非国家使用的语言）没有可用的机器翻译系统，但它在文化和教育价值上具有重要意义。为了解决这个问题，我们提出了一套工具、资源和基准结果，旨在开发可用的尼科语和其他当前没有足够大的平行文本语料库的语言的机器翻译系统。具体包括：(1) Friallel：一种新颖的协作平行文本整理软件，通过基于副本编辑的工作流程实现质量控制。(2) 扩展了FLoRes-200和NLLB-Seed语料库，从其他语言中与尼科语平行翻译了2,009和6,193个高质量的文本。(3) nicolingua-0005：包含130,850个平行片段的三语和双语语料库，以及超过3百万尼科语单语言语料库。(4) 基线双语和多语言神经机器翻译结果与b...

    Currently, there is no usable machine translation system for Nko, a language spoken by tens of millions of people across multiple West African countries, which holds significant cultural and educational value. To address this issue, we present a set of tools, resources, and baseline results aimed towards the development of usable machine translation systems for Nko and other languages that do not currently have sufficiently large parallel text corpora available. (1) Friallel: A novel collaborative parallel text curation software that incorporates quality control through copyedit-based workflows. (2) Expansion of the FLoRes-200 and NLLB-Seed corpora with 2,009 and 6,193 high-quality Nko translations in parallel with 204 and 40 other languages. (3) nicolingua-0005: A collection of trilingual and bilingual corpora with 130,850 parallel segments and monolingual corpora containing over 3 million Nko words. (4) Baseline bilingual and multilingual neural machine translation results with the b
    
[^7]: 对AI分类器的对抗鲁棒性度量的存在性，唯一性和可扩展性研究

    On existence, uniqueness and scalability of adversarial robustness measures for AI classifiers. (arXiv:2310.14421v1 [stat.ML])

    [http://arxiv.org/abs/2310.14421](http://arxiv.org/abs/2310.14421)

    本文研究了针对AI分类器的对抗鲁棒性度量的存在性、唯一性和可扩展性，提出了可以验证的数学条件，并在合成基准测试和生物医学应用中进行了实际计算和解释。

    

    本文提出并证明了针对（局部）唯一可逆分类器、广义线性模型（GLM）和熵AI（EAI）具有最小对抗路径（MAP）和最小对抗距离（MAD）的存在性、唯一性和明确的分析计算的简单可验证的数学条件。在常见的合成基准测试数据集上，针对神经网络、提升随机森林、GLM和EAI等各类AI工具进行MAP和MAD的实际计算、比较和解释，包括双卷状螺旋线及其扩展以及两个生物医学数据问题（用于健康保险理赔预测和心脏病发作致死率分类）。在生物医学应用中，展示了MAP如何在预定义的可访问控制变量子集中提供唯一的最小患者特定风险缓解干预措施。

    Simply-verifiable mathematical conditions for existence, uniqueness and explicit analytical computation of minimal adversarial paths (MAP) and minimal adversarial distances (MAD) for (locally) uniquely-invertible classifiers, for generalized linear models (GLM), and for entropic AI (EAI) are formulated and proven. Practical computation of MAP and MAD, their comparison and interpretations for various classes of AI tools (for neuronal networks, boosted random forests, GLM and EAI) are demonstrated on the common synthetic benchmarks: on a double Swiss roll spiral and its extensions, as well as on the two biomedical data problems (for the health insurance claim predictions, and for the heart attack lethality classification). On biomedical applications it is demonstrated how MAP provides unique minimal patient-specific risk-mitigating interventions in the predefined subsets of accessible control variables.
    
[^8]: 在具有梯度反馈的强单调和指数凸博弈中的自适应、双重最优无悔学习

    Adaptive, Doubly Optimal No-Regret Learning in Strongly Monotone and Exp-Concave Games with Gradient Feedback. (arXiv:2310.14085v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2310.14085](http://arxiv.org/abs/2310.14085)

    本文提出了一个自适应的OGD算法\textsf{AdaOGD}，在强凸性下实现了$ O(\log^2(T)) $的后悔，并且在强单调博弈中使得联合行动最后一次收敛到唯一的纳什均衡。

    

    在强凸性或单调性假设下，网上梯度下降（OGD）被广泛认为是双重最优的：（1）在单个代理设置中，对于强凸成本函数，它实现了$ \Theta(\log T) $的最优后悔；（2）在具有强单调性的多代理博弈的情况下，每个代理使用OGD，我们获得了关于联合行动的最后一次收敛到唯一纳什均衡的最优速率$ \Theta(\frac{1}{T}) $。尽管这些有限时间的保证突出了其优点，但OGD的缺点是需要知道强凸性/单调性的参数。在本文中，我们设计了一个完全自适应的OGD算法\textsf{AdaOGD}，它不需要先验的知识这些参数。在单个代理设置中，我们的算法在强凸性下实现了$ O(\log^2(T)) $的后悔，这是最优的除了一个对数因子。此外，如果在强单调博弈中每个代理都使用\textsf{AdaOGD}，则联合行动收敛到最后一个迭代时的一次。

    Online gradient descent (OGD) is well known to be doubly optimal under strong convexity or monotonicity assumptions: (1) in the single-agent setting, it achieves an optimal regret of $\Theta(\log T)$ for strongly convex cost functions; and (2) in the multi-agent setting of strongly monotone games, with each agent employing OGD, we obtain last-iterate convergence of the joint action to a unique Nash equilibrium at an optimal rate of $\Theta(\frac{1}{T})$. While these finite-time guarantees highlight its merits, OGD has the drawback that it requires knowing the strong convexity/monotonicity parameters. In this paper, we design a fully adaptive OGD algorithm, \textsf{AdaOGD}, that does not require a priori knowledge of these parameters. In the single-agent setting, our algorithm achieves $O(\log^2(T))$ regret under strong convexity, which is optimal up to a log factor. Further, if each agent employs \textsf{AdaOGD} in strongly monotone games, the joint action converges in a last-iterate s
    
[^9]: 组合能力以乘法方式出现：在合成任务中探索扩散模型

    Compositional Abilities Emerge Multiplicatively: Exploring Diffusion Models on a Synthetic Task. (arXiv:2310.09336v1 [cs.LG])

    [http://arxiv.org/abs/2310.09336](http://arxiv.org/abs/2310.09336)

    组合能力以乘法方式出现：研究了条件扩散模型在合成任务中的组合泛化能力，结果显示这种能力受到底层数据生成过程的结构影响，且模型在学习到更高级的组合时存在困难。

    

    现代生成模型展示出了产生极为逼真数据的前所未有的能力。然而，考虑到现实世界的自然组合性，这些模型在实际应用中可靠使用需要展示出能够组合新的概念集合以生成训练数据集中未见的输出的能力。先前的研究表明，最近的扩散模型确实表现出了有趣的组合泛化能力，但它们也会出现无法预测的失败。受此启发，我们在合成环境中进行了有控制性的研究，以了解条件扩散模型的组合泛化能力，我们变化了训练数据的不同属性并测量了模型生成越界样本的能力。我们的结果显示：（i）从一个概念生成样本的能力和将它们组合起来的能力的出现顺序受到了底层数据生成过程的结构的影响；（ii）在组合任务上的表现表明模型在学习到更高级的组合时存在困难。

    Modern generative models exhibit unprecedented capabilities to generate extremely realistic data. However, given the inherent compositionality of the real world, reliable use of these models in practical applications requires that they exhibit the capability to compose a novel set of concepts to generate outputs not seen in the training data set. Prior work demonstrates that recent diffusion models do exhibit intriguing compositional generalization abilities, but also fail unpredictably. Motivated by this, we perform a controlled study for understanding compositional generalization in conditional diffusion models in a synthetic setting, varying different attributes of the training data and measuring the model's ability to generate samples out-of-distribution. Our results show: (i) the order in which the ability to generate samples from a concept and compose them emerges is governed by the structure of the underlying data-generating process; (ii) performance on compositional tasks exhib
    
[^10]: 在BraTS数据集中使用生成对抗网络从已有的模态生成缺失的MRI序列

    Synthesizing Missing MRI Sequences from Available Modalities using Generative Adversarial Networks in BraTS Dataset. (arXiv:2310.07250v1 [q-bio.QM])

    [http://arxiv.org/abs/2310.07250](http://arxiv.org/abs/2310.07250)

    本文提出了一种利用生成对抗网络从已有的模态生成缺失的MRI序列的方法，在BraTS数据集上取得了有希望的结果。

    

    高度侵袭性和致命的胶质母细胞瘤是一种常见的脑癌。磁共振成像（MRI）由于其无创和无辐射性质，在胶质母细胞瘤患者的诊断、治疗计划和随访中发挥着重要作用。国际脑肿瘤分割（BraTS）挑战为利用四种结构性MRI扫描（T1、T1Gd、T2、T2-FLAIR）准确高效地分割胶质母细胞瘤亚区域提供了许多人工智能算法。然而，这四个MRI序列不总是可用的。为解决这个问题，可以利用生成对抗网络（GANs）合成缺失的MRI序列。在本文中，我们实现并利用开源的GAN方法，以任三个MRI序列作为输入生成缺失的第四个结构序列。我们的方法贡献给了社区驱动的通用深度学习框架（GaNDLF），并在合成缺失的MRI序列方面取得了有希望的结果。

    Glioblastoma is a highly aggressive and lethal form of brain cancer. Magnetic resonance imaging (MRI) plays a significant role in the diagnosis, treatment planning, and follow-up of glioblastoma patients due to its non-invasive and radiation-free nature. The International Brain Tumor Segmentation (BraTS) challenge has contributed to generating numerous AI algorithms to accurately and efficiently segment glioblastoma sub-compartments using four structural (T1, T1Gd, T2, T2-FLAIR) MRI scans. However, these four MRI sequences may not always be available. To address this issue, Generative Adversarial Networks (GANs) can be used to synthesize the missing MRI sequences. In this paper, we implement and utilize an open-source GAN approach that takes any three MRI sequences as input to generate the missing fourth structural sequence. Our proposed approach is contributed to the community-driven generally nuanced deep learning framework (GaNDLF) and demonstrates promising results in synthesizing 
    
[^11]: AdaptNet: 基于物理的角色控制的策略调整

    AdaptNet: Policy Adaptation for Physics-Based Character Control. (arXiv:2310.00239v2 [cs.GR] UPDATED)

    [http://arxiv.org/abs/2310.00239](http://arxiv.org/abs/2310.00239)

    AdaptNet是一种基于物理的角色控制的策略调整方法，通过修改现有策略的潜在空间，可以从类似任务中快速学习到新的行为，显著提高训练效率。

    

    受到人类在学习新技能时能够适应现有技能的能力的启发，本文提出了一种名为AdaptNet的方法，该方法可以修改现有策略的潜在空间，使其能够从类似任务中快速学习到新的行为，相比从头开始学习。AdaptNet在给定的强化学习控制器基础上构建了一个两层次结构，通过增加原始状态嵌入来支持行为的适度变化，并进一步修改策略网络层来实现更深远的变化。该技术被证明可以有效地适应现有的基于物理的控制器以适应广泛的新的运动风格、新的任务目标、角色形态的变化以及环境的广泛变化。此外，与从头开始训练或使用其他修改现有策略的方法相比，它显示出显著提高的学习效率，表现为大大缩短的训练时间。代码可在https://motion-上获得。

    Motivated by humans' ability to adapt skills in the learning of new ones, this paper presents AdaptNet, an approach for modifying the latent space of existing policies to allow new behaviors to be quickly learned from like tasks in comparison to learning from scratch. Building on top of a given reinforcement learning controller, AdaptNet uses a two-tier hierarchy that augments the original state embedding to support modest changes in a behavior and further modifies the policy network layers to make more substantive changes. The technique is shown to be effective for adapting existing physics-based controllers to a wide range of new styles for locomotion, new task targets, changes in character morphology and extensive changes in environment. Furthermore, it exhibits significant increase in learning efficiency, as indicated by greatly reduced training times when compared to training from scratch or using other approaches that modify existing policies. Code is available at https://motion-
    
[^12]: 基于图像神经网络的有限区域天气预测

    Graph-based Neural Weather Prediction for Limited Area Modeling. (arXiv:2309.17370v1 [cs.LG])

    [http://arxiv.org/abs/2309.17370](http://arxiv.org/abs/2309.17370)

    该论文提出了一种基于图像神经网络的有限区域天气预测方法，并通过使用北欧地区的本地模型进行了验证。

    

    高精度的机器学习方法在天气预报领域的应用为模拟大气的可能性带来了新的变革。在气候变化时代，获取像这样的高分辨率预报模型的能力变得越来越重要。虽然大多数现有的神经网络天气预报方法都是针对全球预测，但如何将这些技术应用于有限区域建模是一个重要问题。本文将基于图像的神经网络天气预测方法应用于有限区域，并提出了多尺度分层模型扩展。通过使用北欧地区的本地模型进行实验证实了我们的方法的有效性。

    The rise of accurate machine learning methods for weather forecasting is creating radical new possibilities for modeling the atmosphere. In the time of climate change, having access to high-resolution forecasts from models like these is also becoming increasingly vital. While most existing Neural Weather Prediction (NeurWP) methods focus on global forecasting, an important question is how these techniques can be applied to limited area modeling. In this work we adapt the graph-based NeurWP approach to the limited area setting and propose a multi-scale hierarchical model extension. Our approach is validated by experiments with a local model for the Nordic region.
    
[^13]: 深度学习在CT扫描分类中的结果是否公正可解释？

    Are Deep Learning Classification Results Obtained on CT Scans Fair and Interpretable?. (arXiv:2309.12632v1 [cs.LG])

    [http://arxiv.org/abs/2309.12632](http://arxiv.org/abs/2309.12632)

    深度学习在CT扫描分类中的结果往往只关注准确性，而忽视了公正性和解释性，导致模型不可信和不适用于真实场景。

    

    鉴于深度学习方法在图像和物体分类中的巨大成功，生物医学图像处理领域也面临着深度学习应用于各种自动诊断案例的压力。不幸的是，文献中大多数基于深度学习的分类尝试仅仅关注极高的准确性，而不考虑可解释性或者患者训练和测试数据的分离。例如，大部分使用深度学习的肺结节分类论文会对数据进行随机洗牌，并将其分为训练、验证和测试集，导致一个人的CT扫描图像中的某些图像位于训练集中，而其他图像则位于验证或测试图像集中。这可能导致误导性的准确率报告和学习到的无关特征，最终降低了这些模型在实际应用中的可用性。

    Following the great success of various deep learning methods in image and object classification, the biomedical image processing society is also overwhelmed with their applications to various automatic diagnosis cases. Unfortunately, most of the deep learning-based classification attempts in the literature solely focus on the aim of extreme accuracy scores, without considering interpretability, or patient-wise separation of training and test data. For example, most lung nodule classification papers using deep learning randomly shuffle data and split it into training, validation, and test sets, causing certain images from the CT scan of a person to be in the training set, while other images of the exact same person to be in the validation or testing image sets. This can result in reporting misleading accuracy rates and the learning of irrelevant features, ultimately reducing the real-life usability of these models. When the deep neural networks trained on the traditional, unfair data sh
    
[^14]: 朝着使用更少标注实现群体鲁棒性的最后一层再训练

    Towards Last-layer Retraining for Group Robustness with Fewer Annotations. (arXiv:2309.08534v1 [cs.LG])

    [http://arxiv.org/abs/2309.08534](http://arxiv.org/abs/2309.08534)

    本研究发现，在没有群体标注和只有少量类别标注的情况下，最后一层再训练仍然可以有效提高最差群体准确性，从而优于经验风险最小化在整个数据集上的表现，可以实现群体鲁棒性的提升。

    

    神经网络的经验风险最小化(ERM)容易过度依赖虚假相关性，并在少数群体上具有较差的泛化性能。最近的深度特征再赋权(DFR)技术通过简单的最后一层再训练实现了最先进的群体保护性能，但它需要保留群体和类别的标注，并构建一个群体平衡的再赋权数据集。在这项工作中，我们研究了这个不切实际的要求，并发现即使没有群体标注（除了模型选择），只有少量的类别标注，最后一层再训练仍然可以出人意料地有效。我们首先证明了即使再赋权数据集中仅有一小部分最差群体数据，最后一层再训练仍然可以显著提高最差群体准确性。这意味着通过保留一部分训练数据来重新训练最后一层，可以在没有额外数据或标注的情况下，显著优于对整个数据集进行ERM。为了进一步提高群体鲁棒性，我们...

    Empirical risk minimization (ERM) of neural networks is prone to over-reliance on spurious correlations and poor generalization on minority groups. The recent deep feature reweighting (DFR) technique achieves state-of-the-art group robustness via simple last-layer retraining, but it requires held-out group and class annotations to construct a group-balanced reweighting dataset. In this work, we examine this impractical requirement and find that last-layer retraining can be surprisingly effective with no group annotations (other than for model selection) and only a handful of class annotations. We first show that last-layer retraining can greatly improve worst-group accuracy even when the reweighting dataset has only a small proportion of worst-group data. This implies a "free lunch" where holding out a subset of training data to retrain the last layer can substantially outperform ERM on the entire dataset with no additional data or annotations. To further improve group robustness, we i
    
[^15]: 什么时候编程思维对推理起作用?

    When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v1 [cs.CL])

    [http://arxiv.org/abs/2308.15452](http://arxiv.org/abs/2308.15452)

    提出了复杂性影响推理分数（CIRS）来衡量编程语言对推理能力的影响，发现并非所有复杂性的代码数据都可以被学习或理解，适当的复杂性水平对于改善推理能力至关重要。

    

    大型语言模型（LLM）的推理能力在体现出人工智能领域中起着关键作用。尽管像编程思维提示这样的方法对于使用编程语言来解决复杂推理任务的LLM非常有效，但代码数据对推理能力的具体影响仍未充分探索。为了填补这一空白，我们提出了复杂性影响推理分数（CIRS），它结合了结构和逻辑属性，以衡量代码和推理能力之间的相关性。具体而言，我们使用抽象语法树来编码结构信息，并通过考虑难度和圈复杂度来计算逻辑复杂性。通过实证分析，我们发现并非所有复杂性的代码数据都可以被LLM学习或理解。最佳复杂性水平对于通过编程辅助提示改善推理能力至关重要。然后我们设计了一个自动合成的方法...

    The reasoning capabilities of Large Language Models (LLMs) play a pivotal role in the realm of embodied artificial intelligence. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing
    
[^16]: Thompson Sampling用于多臂老虎机的实值组合纯探索

    Thompson Sampling for Real-Valued Combinatorial Pure Exploration of Multi-Armed Bandit. (arXiv:2308.10238v1 [cs.LG])

    [http://arxiv.org/abs/2308.10238](http://arxiv.org/abs/2308.10238)

    这篇论文介绍了一种名为广义汤普森抽样探索算法，能够解决多臂老虎机实值组合纯探索问题中动作集合大小为指数级别的情况。

    

    我们研究了多臂老虎机的实值组合纯探索（R-CPE-MAB）问题。在R-CPE-MAB中，玩家从给定的d个随机臂中选择一个，每个臂s的奖励遵循未知分布，其平均值为μs。在每个时间步骤中，玩家拉动一个臂并观察其奖励。玩家的目标是以尽可能少的臂拉动次数来确定最优动作π* = argmaxπ∈A μTπ，其中A是有限大小的实值动作集合。之前的方法假设动作集合A的大小在d的多项式级别上。我们引入了一种名为广义汤普森抽样探索（GenTS-Explore）算法，它是第一个可以解决动作集合大小在d的指数级别上的算法。同时，我们还引入了一种新的问题相关的样本复杂性。

    We study the real-valued combinatorial pure exploration of the multi-armed bandit (R-CPE-MAB) problem. In R-CPE-MAB, a player is given $d$ stochastic arms, and the reward of each arm $s\in\{1, \ldots, d\}$ follows an unknown distribution with mean $\mu_s$. In each time step, a player pulls a single arm and observes its reward. The player's goal is to identify the optimal \emph{action} $\boldsymbol{\pi}^{*} = \argmax_{\boldsymbol{\pi} \in \mathcal{A}} \boldsymbol{\mu}^{\top}\boldsymbol{\pi}$ from a finite-sized real-valued \emph{action set} $\mathcal{A}\subset \mathbb{R}^{d}$ with as few arm pulls as possible. Previous methods in the R-CPE-MAB assume that the size of the action set $\mathcal{A}$ is polynomial in $d$. We introduce an algorithm named the Generalized Thompson Sampling Explore (GenTS-Explore) algorithm, which is the first algorithm that can work even when the size of the action set is exponentially large in $d$. We also introduce a novel problem-dependent sample complexity 
    
[^17]: 文本到图像生成中的偏见放大悖论

    The Bias Amplification Paradox in Text-to-Image Generation. (arXiv:2308.00755v1 [cs.LG])

    [http://arxiv.org/abs/2308.00755](http://arxiv.org/abs/2308.00755)

    本文研究了文本到图像生成中的偏见放大现象，并发现其主要原因是训练数据和模型提示之间的差异。一旦考虑到各种分布差异，偏见放大现象显著减少。

    

    偏见放大是一种模型增加训练数据中不平衡的现象。本文通过使用稳定扩散来比较训练数据与生成图像中的性别比例，研究了文本到图像领域中的偏见放大现象。我们发现模型似乎放大了训练数据中存在的性别-职业偏见。然而，我们发现放大很大程度上可以归因于训练数据和模型提示之间的差异。例如，训练数据中的标题通常包含明确的性别信息，而我们使用的提示则不包含，这导致了分布的偏移，从而影响了偏见度量。一旦我们考虑到训练和生成时使用的文本之间的各种分布差异，我们观察到放大现象大大减少。我们的发现说明了比较模型和它们所训练的数据中的偏见所面临的挑战，并且强调了混淆因素。

    Bias amplification is a phenomenon in which models increase imbalances present in the training data. In this paper, we study bias amplification in the text-to-image domain using Stable Diffusion by comparing gender ratios in training vs. generated images. We find that the model appears to amplify gender-occupation biases found in the training data (LAION). However, we discover that amplification can largely be attributed to discrepancies between training captions and model prompts. For example, an inherent difference is that captions from the training data often contain explicit gender information while the prompts we use do not, which leads to a distribution shift and consequently impacts bias measures. Once we account for various distributional differences between texts used for training and generation, we observe that amplification decreases considerably. Our findings illustrate the challenges of comparing biases in models and the data they are trained on, and highlight confounding 
    
[^18]: 关于(Normalised) Discounted Cumulative Gain作为Top-n推荐的离线评估指标的论文翻译

    On (Normalised) Discounted Cumulative Gain as an Offline Evaluation Metric for Top-$n$ Recommendation. (arXiv:2307.15053v1 [cs.IR])

    [http://arxiv.org/abs/2307.15053](http://arxiv.org/abs/2307.15053)

    本文批判性审视了(Normalised) Discounted Cumulative Gain作为Top-n推荐离线评估指标的方法，并研究了何时可以期望这些指标逼近在线实验的金标准结果。

    

    推荐方法通常通过两种方式进行评估：(1) 通过(模拟)在线实验，通常被视为金标准，或者(2) 通过一些离线评估程序，目标是近似在线实验的结果。文献中采用了几种离线评估指标，受信息检索领域中常见的排名指标的启发。(Normalised) Discounted Cumulative Gain (nDCG)是其中一种广泛采用的度量标准，在很多年里，更高的(n)DCG值被用来展示新方法在Top-n推荐中的最新进展。我们的工作对这种方法进行了批判性的审视，并研究了我们何时可以期望这些指标逼近在线实验的金标准结果。我们从第一原理上正式提出了DCG被认为是在线奖励的无偏估计的假设，并给出了这个指标的推导。

    Approaches to recommendation are typically evaluated in one of two ways: (1) via a (simulated) online experiment, often seen as the gold standard, or (2) via some offline evaluation procedure, where the goal is to approximate the outcome of an online experiment. Several offline evaluation metrics have been adopted in the literature, inspired by ranking metrics prevalent in the field of Information Retrieval. (Normalised) Discounted Cumulative Gain (nDCG) is one such metric that has seen widespread adoption in empirical studies, and higher (n)DCG values have been used to present new methods as the state-of-the-art in top-$n$ recommendation for many years.  Our work takes a critical look at this approach, and investigates when we can expect such metrics to approximate the gold standard outcome of an online experiment. We formally present the assumptions that are necessary to consider DCG an unbiased estimator of online reward and provide a derivation for this metric from first principles
    
[^19]: 通过高斯混合分布潜空间的搜索生成反事实解释

    Counterfactual Explanation via Search in Gaussian Mixture Distributed Latent Space. (arXiv:2307.13390v1 [cs.LG])

    [http://arxiv.org/abs/2307.13390](http://arxiv.org/abs/2307.13390)

    本文介绍了一种通过在自编码器的潜空间中进行高斯混合分布搜索来生成反事实解释的方法。

    

    反事实解释（CEs）是用于解决算法补救中的两个问题的重要工具：1. 是什么关键因素导致了自动预测/决策？2. 如何改变这些因素以从用户角度获得更有利的结果？因此，通过提供易于理解的解释和易于实现的可行变化来引导用户与AI系统的交互对于可信赖的采用和长期接受AI系统是至关重要的。在文献中，已经提出了各种方法来生成CEs，并建议使用不同的质量度量来评估这些方法。然而，CEs的生成通常需要大量计算，并且生成的建议是不切实际的，因此不可操作。在本文中，我们介绍了一种新的方法，通过首先将自编码器的潜空间形成为高斯分布的混合，为预先训练的二分类器生成CEs。

    Counterfactual Explanations (CEs) are an important tool in Algorithmic Recourse for addressing two questions: 1. What are the crucial factors that led to an automated prediction/decision? 2. How can these factors be changed to achieve a more favorable outcome from a user's perspective? Thus, guiding the user's interaction with AI systems by proposing easy-to-understand explanations and easy-to-attain feasible changes is essential for the trustworthy adoption and long-term acceptance of AI systems. In the literature, various methods have been proposed to generate CEs, and different quality measures have been suggested to evaluate these methods. However, the generation of CEs is usually computationally expensive, and the resulting suggestions are unrealistic and thus non-actionable. In this paper, we introduce a new method to generate CEs for a pre-trained binary classifier by first shaping the latent space of an autoencoder to be a mixture of Gaussian distributions. CEs are then generat
    
[^20]: 从随机游走到图形快跑：一种在连续时间动态图上具有低延迟的节点嵌入框架

    From random-walks to graph-sprints: a low-latency node embedding framework on continuous-time dynamic graphs. (arXiv:2307.08433v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.08433](http://arxiv.org/abs/2307.08433)

    这篇论文介绍了一种在连续时间动态图上具有低延迟的节点嵌入框架，通过提出流式低延迟的近似随机游走特征，计算时间感知节点嵌入以总结多跳信息。

    

    许多真实世界的数据集具有基础的动态图结构，其中实体和它们的相互作用随时间演变。机器学习模型应考虑这些动态因素，以在下游任务中充分发挥其潜力。以前用于图表示学习的方法要么侧重于抽样k-跳邻域，类似于广度优先搜索，要么侧重于随机游走，类似于深度优先搜索。然而，这些方法在实时动态图上进行低延迟推断是计算上昂贵且不适用的。为了克服这些限制，我们提出了图形快跑，这是一个适用于连续时间动态图（CTDGs）的通用特征提取框架，具有低延迟，并且与高延迟模型相比具有竞争力。为了实现这一点，我们提出了一种流式、低延迟的近似随机游走特征。在我们的框架中，使用仅单跳操作计算总结多跳信息的时间感知节点嵌入。

    Many real-world datasets have an underlying dynamic graph structure, where entities and their interactions evolve over time. Machine learning models should consider these dynamics in order to harness their full potential in downstream tasks. Previous approaches for graph representation learning have focused on either sampling k-hop neighborhoods, akin to breadth-first search, or random walks, akin to depth-first search. However, these methods are computationally expensive and unsuitable for real-time, low-latency inference on dynamic graphs. To overcome these limitations, we propose graph-sprints a general purpose feature extraction framework for continuous-time-dynamic-graphs (CTDGs) that has low latency and is competitive with state-of-the-art, higher latency models. To achieve this, a streaming, low latency approximation to the random-walk based features is proposed. In our framework, time-aware node embeddings summarizing multi-hop information are computed using only single-hop ope
    
[^21]: 重置深度强化学习中的优化器：一个实证研究

    Resetting the Optimizer in Deep RL: An Empirical Study. (arXiv:2306.17833v1 [cs.LG])

    [http://arxiv.org/abs/2306.17833](http://arxiv.org/abs/2306.17833)

    在深度强化学习中，当优化问题的风景在不同迭代中差异较大时，重置优化器的内部参数可以避免污染和提高性能。

    

    我们关注的是在深度强化学习中近似计算最优值函数的任务。这个迭代过程包括在每个迭代中解决一系列不同迭代中目标函数可能改变的优化问题。解决这个问题的常见方法是使用现代变种的随机梯度下降算法，如Adam。这些优化器保持自己的内部参数，如梯度的一阶和二阶矩估计，并随时间更新这些参数。因此，之前迭代的信息被用来在当前迭代中解决优化问题。我们假设在之前迭代的优化风景与当前迭代相差较大的情况下，这可能会污染所使用优化器的内部参数。为了避免这种影响，一个简单的想法是在开始新的迭代时重置优化器的内部参数。

    We focus on the task of approximating the optimal value function in deep reinforcement learning. This iterative process is comprised of approximately solving a sequence of optimization problems where the objective function can change per iteration. The common approach to solving the problem is to employ modern variants of the stochastic gradient descent algorithm such as Adam. These optimizers maintain their own internal parameters such as estimates of the first and the second moment of the gradient, and update these parameters over time. Therefore, information obtained in previous iterations is being used to solve the optimization problem in the current iteration. We hypothesize that this can contaminate the internal parameters of the employed optimizer in situations where the optimization landscape of the previous iterations is quite different from the current iteration. To hedge against this effect, a simple idea is to reset the internal parameters of the optimizer when starting a n
    
[^22]: ManimML：用动画演示机器学习架构

    ManimML: Communicating Machine Learning Architectures with Animation. (arXiv:2306.17108v1 [cs.LG])

    [http://arxiv.org/abs/2306.17108](http://arxiv.org/abs/2306.17108)

    ManimML是一个开源Python库，通过动画演示自动生成的ML算法，为机器学习从业者提供了一种简单且熟悉的方式来沟通和可视化ML算法。

    

    近年来，由于机器学习在科学和工程领域的应用，对机器学习（ML）的兴趣激增。然而，随着ML技术的发展，解释和可视化新颖的ML算法的工具还远远落后。动画已被证明是一种强大的工具，可以制作出随时间动态变化的系统的吸引人可视化效果，非常适合用于沟通ML算法的任务。然而，目前动画化ML算法的方法是手工制作突出特定算法或使用复杂的通用动画软件。我们开发了ManimML，这是一个开源的Python库，可以直接从代码中轻松生成ML算法的动画。我们旨在利用ML从业者对编程的现有知识，而不是要求他们学习复杂的动画软件。ManimML具有熟悉的语法，用于指定模仿流行的深度学习框架如Pytorch的神经网络。

    There has been an explosion in interest in machine learning (ML) in recent years due to its applications to science and engineering. However, as ML techniques have advanced, tools for explaining and visualizing novel ML algorithms have lagged behind. Animation has been shown to be a powerful tool for making engaging visualizations of systems that dynamically change over time, which makes it well suited to the task of communicating ML algorithms. However, the current approach to animating ML algorithms is to handcraft applications that highlight specific algorithms or use complex generalized animation software. We developed ManimML, an open-source Python library for easily generating animations of ML algorithms directly from code. We sought to leverage ML practitioners' preexisting knowledge of programming rather than requiring them to learn complex animation software. ManimML has a familiar syntax for specifying neural networks that mimics popular deep learning frameworks like Pytorch.
    
[^23]: 齐次空间上的潜在SDE

    Latent SDEs on Homogeneous Spaces. (arXiv:2306.16248v1 [cs.LG])

    [http://arxiv.org/abs/2306.16248](http://arxiv.org/abs/2306.16248)

    这篇论文研究了在齐次空间上的潜在SDE，通过使用单位球上的SDE进行变分推断，提出了一种简单且直观的表达式来计算近似后验和先验过程之间的KL散度。

    

    我们考虑在潜在变量模型中的变分贝叶斯推断问题，其中一个（可能是复杂的）观测随机过程由潜在随机微分方程（SDE）的解决方案所驱动。受到学习大规模数据中（几乎任意）潜在神经SDE时所面临的挑战的启发，例如效率梯度计算，我们退一步并研究了一个特定的子类。在我们的情况下，SDE在一个齐次潜在空间上演变，并由相应（矩阵）Lie群的随机动力学所诱导。在学习问题中，单位$n$-球上的SDE可以说是这一设置中最相关的。值得注意的是，在变分推断中，单位球不仅有助于使用真正无信息的先验SDE，而且我们还获得了关于近似后验和先验过程之间的Kullback-Leibler散度的特别简单和直观的表达式，这在证据下界中至关重要。实验证明了我们的方法的性能优势。

    We consider the problem of variational Bayesian inference in a latent variable model where a (possibly complex) observed stochastic process is governed by the solution of a latent stochastic differential equation (SDE). Motivated by the challenges that arise when trying to learn an (almost arbitrary) latent neural SDE from large-scale data, such as efficient gradient computation, we take a step back and study a specific subclass instead. In our case, the SDE evolves on a homogeneous latent space and is induced by stochastic dynamics of the corresponding (matrix) Lie group. In learning problems, SDEs on the unit $n$-sphere are arguably the most relevant incarnation of this setup. Notably, for variational inference, the sphere not only facilitates using a truly uninformative prior SDE, but we also obtain a particularly simple and intuitive expression for the Kullback-Leibler divergence between the approximate posterior and prior process in the evidence lower bound. Experiments demonstrat
    
[^24]: PriorBand: 深度学习下的实用超参数优化方法。

    PriorBand: Practical Hyperparameter Optimization in the Age of Deep Learning. (arXiv:2306.12370v1 [cs.LG])

    [http://arxiv.org/abs/2306.12370](http://arxiv.org/abs/2306.12370)

    提出了一种针对深度学习量身定制的超参数优化算法PriorBand，能够同时利用专家信念和廉价的代理任务，具有高效性和抗干扰能力。

    

    深度学习（DL）流程中的超参数对其下游性能至关重要。尽管已经开发了许多超参数优化方法，但其代价往往对现代深度学习不可行。因此，手动实验仍是优化超参数的主要方法，依赖于研究人员的直觉、领域知识和廉价的初步探索。为了解决HPO算法和DL研究人员之间的这种不匹配，我们提出了PriorBand，这是一种针对DL量身定制的HPO算法，能够利用专家信念和廉价的代理任务。实验证明，PriorBand在一系列DL基准测试中的效率，以及在提供有效专家输入和抗击不良专家信念方面的收益。

    Hyperparameters of Deep Learning (DL) pipelines are crucial for their downstream performance. While a large number of methods for Hyperparameter Optimization (HPO) have been developed, their incurred costs are often untenable for modern DL. Consequently, manual experimentation is still the most prevalent approach to optimize hyperparameters, relying on the researcher's intuition, domain knowledge, and cheap preliminary explorations. To resolve this misalignment between HPO algorithms and DL researchers, we propose PriorBand, an HPO algorithm tailored to DL, able to utilize both expert beliefs and cheap proxy tasks. Empirically, we demonstrate PriorBand's efficiency across a range of DL benchmarks and show its gains under informative expert input and robustness against poor expert beliefs
    
[^25]: AdCraft：一种用于搜索引擎营销优化的高级强化学习基准环境

    AdCraft: An Advanced Reinforcement Learning Benchmark Environment for Search Engine Marketing Optimization. (arXiv:2306.11971v1 [cs.LG])

    [http://arxiv.org/abs/2306.11971](http://arxiv.org/abs/2306.11971)

    AdCraft是一种高级强化学习基准环境，用于模拟出价和预算变化的搜索引擎营销(SEM)活动，可用于评估和提高SEM出价和预算管理相关的RL算法的鲁棒性。

    

    本文介绍了一种新的强化学习基准环境—— AdCraft，其具有随机和非静态特性。该环境模拟了搜索引擎营销中出价和预算的动态变化。SEM是一种利用付费广告来增加网站在搜索引擎结果页面上的可见性的数字营销技术。SEM广告活动的表现取决于多个因素，包括关键字选择、广告设计、出价管理、预算调整和表现监控。深度强化学习最近被认为是一种优化SEM广告投放活动的潜在策略，但需要大量数据，在实践中可能成本高昂或不可行。我们的可定制环境使从业者能够评估和提高与SEM出价和预算管理相关的RL算法的鲁棒性，而无需付出这些成本。通过在AdCraft环境下进行一系列实验，

    We introduce \env{}, a novel benchmark environment for the Reinforcement Learning (RL) community distinguished by its stochastic and non-stationary properties. The environment simulates bidding and budgeting dynamics within Search Engine Marketing (SEM), a digital marketing technique utilizing paid advertising to enhance the visibility of websites on search engine results pages (SERPs). The performance of SEM advertisement campaigns depends on several factors, including keyword selection, ad design, bid management, budget adjustments, and performance monitoring. Deep RL recently emerged as a potential strategy to optimize campaign profitability within the complex and dynamic landscape of SEM but it requires substantial data, which may be costly or infeasible to acquire in practice. Our customizable environment enables practitioners to assess and enhance the robustness of RL algorithms pertinent to SEM bid and budget management without such costs. Through a series of experiments within 
    
[^26]: 使用文档级元数据的领域特定快速预训练技术$FPDM$

    $FPDM$: Domain-Specific Fast Pre-training Technique using Document-Level Metadata. (arXiv:2306.06190v1 [cs.CL])

    [http://arxiv.org/abs/2306.06190](http://arxiv.org/abs/2306.06190)

    本文提出了$FPDM$，使用文档元数据和领域特定分类作为监督信号，对领域特定语料库进行transformer编码器的预训练。$FPDM$通过句子级别的输入预训练开放领域的编码器，在微调时使用词汇级别的输入，性能优于其他基于transformer的模型。

    

    在各种领域的预训练已显示出在开放领域和领域特定下游任务上具有良好的结果。然而，最先进的transformers需要大量的预训练数据和计算资源。在本文中，我们提出了$FPDM$（Fast Pre-training Technique using Document Level Metadata），这是一个新颖、计算效率高的框架，利用文档元数据和领域特定的分类作为监督信号，对领域特定语料库进行transformer编码器的预训练。最主要的创新在于，在领域特定的预训练过程中，使用句子级别的嵌入作为输入，持续对开放领域的编码器进行预训练（以适应长文档），但在对该编码器进行微调时，则使用词汇级别嵌入作为输入。实验表明，$FPDM$在客户支持、科学和法律等领域的字符级F1分数和其他自动化指标方面优于几种基于transformer的基准，且在下游任务微调后性能下降可以忽略不计。

    Pre-training Transformers has shown promising results on open-domain and domain-specific downstream tasks. However, state-of-the-art Transformers require an unreasonably large amount of pre-training data and compute. In this paper, we propose $FPDM$ (Fast Pre-training Technique using Document Level Metadata), a novel, compute-efficient framework that utilizes Document metadata and Domain-Specific Taxonomy as supervision signals to pre-train transformer encoder on a domain-specific corpus. The main innovation is that during domain-specific pretraining, an open-domain encoder is continually pre-trained using sentence-level embeddings as inputs (to accommodate long documents), however, fine-tuning is done with token-level embeddings as inputs to this encoder. We show that $FPDM$ outperforms several transformer-based baselines in terms of character-level F1 scores and other automated metrics in the Customer Support, Scientific, and Legal Domains, and shows a negligible drop in performance 
    
[^27]: BackpropTools: 一款快速、可移植的连续控制深度强化学习库

    BackpropTools: A Fast, Portable Deep Reinforcement Learning Library for Continuous Control. (arXiv:2306.03530v1 [cs.LG])

    [http://arxiv.org/abs/2306.03530](http://arxiv.org/abs/2306.03530)

    BackpropTools是一款快速、可移植的连续控制深度强化学习库，它通过模板元编程提供紧密集成的可组合组件，并在异构平台集合上无缝使用，同时在连续控制问题的深度RL代理高效可扩展训练方面具有优势。由于其可移植性和实时保证，它成为了在嵌入式设备上部署学来的策略的有价值的工具。

    

    深度强化学习在许多领域中已被证明可以产生出具有能力的代理和控制策略，但常常受到训练时间过长的困扰。此外，在连续控制问题的情况下，现有深度学习库的实时性和可移植性的缺乏限制了学习策略在实际嵌入式设备上的应用。为了解决这些问题，我们提出了BackpropTools，一种依赖性-free、header-only、pure C++的深度监督和强化学习库。利用最近C++标准的模板元编程能力，我们提供了可以由编译器紧密集成的可组合组件。其新颖的架构允许BackpropTools在异构平台集合上无缝使用，从HPC集群、工作站和笔记本电脑到智能手机、智能手表和微控制器。具体来说，由于RL算法与模拟环境的紧密集成，BackpropTools在连续控制问题的深度RL代理的高效可扩展训练方面具有优势。此外，它的可移植性和实时保证使其成为在嵌入式设备上部署学来的策略的有价值的工具。

    Deep Reinforcement Learning (RL) has been demonstrated to yield capable agents and control policies in several domains but is commonly plagued by prohibitively long training times. Additionally, in the case of continuous control problems, the applicability of learned policies on real-world embedded devices is limited due to the lack of real-time guarantees and portability of existing deep learning libraries. To address these challenges, we present BackpropTools, a dependency-free, header-only, pure C++ library for deep supervised and reinforcement learning. Leveraging the template meta-programming capabilities of recent C++ standards, we provide composable components that can be tightly integrated by the compiler. Its novel architecture allows BackpropTools to be used seamlessly on a heterogeneous set of platforms, from HPC clusters over workstations and laptops to smartphones, smartwatches, and microcontrollers. Specifically, due to the tight integration of the RL algorithms with simu
    
[^28]: MiniSUPERB:轻量级自监督语音模型基准测试

    MiniSUPERB: Lightweight Benchmark for Self-supervised Speech Models. (arXiv:2305.19011v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2305.19011](http://arxiv.org/abs/2305.19011)

    这篇论文提出了一个名为MiniSUPERB的轻量级基准测试，可以有效地评估自监督语音模型的性能，并在计算成本上比SUPERB更低。同时，该论文还研究了在少样本情况下评估SSL语音模型的性能变化。

    

    SUPERB被提出用于评估自监督学习（SSL）语音模型在各种任务上的泛化能力。然而，由于大型数据集和多样化任务，它导致了高计算成本。在本文中，我们引入了MiniSUPERB，一个轻量级基准测试，它以明显更低的计算成本有效地评估SSL语音模型并且结果可与SUPERB相比。我们精选代表性任务，采样数据集，并离线提取模型表示。我们的方法与SUPERB Paper和SUPERB Challenge分别达到0.954和0.982的斯皮尔曼等级相关系数。此外，我们在乘-累积操作（MACs）方面减少了97％的计算成本。此外，我们在少样本情况下评估SSL语音模型，并观察到其性能有显著变化。据我们所知，这是第一项研究同时考虑模型本身的计算成本和在基准测试上评估的成本。

    SUPERB was proposed to evaluate the generalizability of self-supervised learning (SSL) speech models across various tasks. However, it incurs high computational costs due to the large datasets and diverse tasks. In this paper, we introduce MiniSUPERB, a lightweight benchmark that efficiently evaluates SSL speech models with comparable results to SUPERB but lower computational costs significantly. We carefully select representative tasks, sample datasets, and extract model representations offline. Our approach achieves a Spearman's rank correlation of 0.954 and 0.982 with SUPERB Paper and SUPERB Challenge, respectively. Additionally, we reduce the computational cost by 97% in terms of Multiply-ACcumulate operations (MACs). Furthermore, we evaluate SSL speech models in few-shot scenarios and observe significant variations in their performance. To our knowledge, this is the first study to examine both the computational cost of the model itself and the cost of evaluating it on a benchmark.
    
[^29]: 向量值随机特征学习的误差界分析

    Error Bounds for Learning with Vector-Valued Random Features. (arXiv:2305.17170v1 [stat.ML])

    [http://arxiv.org/abs/2305.17170](http://arxiv.org/abs/2305.17170)

    本文提供了对向量值随机特征学习的完整误差分析，包括在模型错误说明下向量值RF估计器的强一致性和在良好规定的情况下极小化最优收敛速率。

    

    本文提供了对于向量值随机特征学习的完整误差分析。该理论是针对完全通用的无限维度输入-输出设定中的RF Ridge回归而开发的，但仍适用于并改进了现有的有限维度分析。与文献中其他类似的工作相比，本文提出的方法依赖于底层风险函数的直接分析，完全避免了基于随机矩阵的显式RF Ridge回归解决方案公式的使用。这消除了随机矩阵理论中的浓度结果或其对随机算子的推广的需求。本文建立的主要结果包括在模型错误说明下向量值RF估计器的强一致性和在良好规定的情况下极小化最优收敛速率。实现这些收敛速率所需的参数复杂度(随机特征数量)和样本复杂度(标记数据数量)与

    This paper provides a comprehensive error analysis of learning with vector-valued random features (RF). The theory is developed for RF ridge regression in a fully general infinite-dimensional input-output setting, but nonetheless applies to and improves existing finite-dimensional analyses. In contrast to comparable work in the literature, the approach proposed here relies on a direct analysis of the underlying risk functional and completely avoids the explicit RF ridge regression solution formula in terms of random matrices. This removes the need for concentration results in random matrix theory or their generalizations to random operators. The main results established in this paper include strong consistency of vector-valued RF estimators under model misspecification and minimax optimal convergence rates in the well-specified setting. The parameter complexity (number of random features) and sample complexity (number of labeled data) required to achieve such rates are comparable with 
    
[^30]: 统一嵌入：面向 Web 规模 ML 系统的经过验证的特征表示

    Unified Embedding: Battle-Tested Feature Representations for Web-Scale ML Systems. (arXiv:2305.12102v1 [cs.LG])

    [http://arxiv.org/abs/2305.12102](http://arxiv.org/abs/2305.12102)

    本文介绍了一种名为“特征复用”的框架，它使用单一的表示空间 能够高效有效地学习高质量的特征嵌入，同时区分不同的分类特征。通过在多个公共数据集和新数据集“Web-Available Image Search (WAIS)”上的测试，我们展示了这种方法的优于现有技术的表现。

    

    高效、有效地学习高质量的特征嵌入对于 Web 规模的机器学习系统的性能至关重要。标准方法是将每个特征值表示为一个 d 维嵌入，引入数百亿个参数，而这些特征的基数非常高。这个瓶颈导致了备选嵌入算法的重大进展。本文介绍了一个简单但非常有效的框架，即“特征复用”，在许多不同的分类特征之间使用一个单一的表示空间。我们的理论和实证分析表明，复用的嵌入可以分解为每个组成特征的组件，使得模型可以区分特征。我们展示了复用的嵌入在几个公共数据集上优于现有技术。此外，我们引入了一个名为“Web-Available Image Search (WAIS)”的新数据集，以严格评估 Web 规模下的新嵌入算法。我们邀请社区通过提出可以准确、高效地将数百万张图像嵌入和分类到成千上万个类别的新模型来贡献 WAIS 挑战。

    Learning high-quality feature embeddings efficiently and effectively is critical for the performance of web-scale machine learning systems. A typical model ingests hundreds of features with vocabularies on the order of millions to billions of tokens. The standard approach is to represent each feature value as a d-dimensional embedding, introducing hundreds of billions of parameters for extremely high-cardinality features. This bottleneck has led to substantial progress in alternative embedding algorithms. Many of these methods, however, make the assumption that each feature uses an independent embedding table. This work introduces a simple yet highly effective framework, Feature Multiplexing, where one single representation space is used across many different categorical features. Our theoretical and empirical analysis reveals that multiplexed embeddings can be decomposed into components from each constituent feature, allowing models to distinguish between features. We show that multip
    
[^31]: 基于几何感知的自回归模型用于新型电磁量计几何模拟的泛化研究

    Generalizing to new calorimeter geometries with Geometry-Aware Autoregressive Models (GAAMs) for fast calorimeter simulation. (arXiv:2305.11531v1 [physics.ins-det])

    [http://arxiv.org/abs/2305.11531](http://arxiv.org/abs/2305.11531)

    基于几何感知的自回归模型能够学习电磁量计响应如何随几何形状变化，能够快速有效地模拟非环形的电磁量计。

    

    在粒子物理数据分析中，生成对撞产物的模拟探测器响应至关重要，但计算非常昂贵。其中一个子探测器，电磁量计由于其单元格的高粒度和复杂的相互作用而占据了计算时间的主导地位。生成模型可以提供更快的样本生成，但目前需要大量努力来优化特定探测器几何形状的性能，通常需要许多网络来描述不同的单元格大小和排列方式，这些模型不能推广到其他几何形状。我们开发了一种“几何感知”自回归模型，学习电磁量计响应如何随几何形状变化，能够生成看不见的几何形状的模拟响应而无需其他训练。该几何感知模型在涉及关键响应的生成和真实分布之间的Wasserstein距离等指标上比基线模型优越50％。我们通过在二维空间中模拟具有前所未有的细粒度，并扩展到非平面几何形状，展示了该方法的可行性和速度。

    Generation of simulated detector response to collision products is crucial to data analysis in particle physics, but computationally very expensive. One subdetector, the calorimeter, dominates the computational time due to the high granularity of its cells and complexity of the interaction. Generative models can provide more rapid sample production, but currently require significant effort to optimize performance for specific detector geometries, often requiring many networks to describe the varying cell sizes and arrangements, which do not generalize to other geometries. We develop a {\it geometry-aware} autoregressive model, which learns how the calorimeter response varies with geometry, and is capable of generating simulated responses to unseen geometries without additional training. The geometry-aware model outperforms a baseline, unaware model by 50\% in metrics such as the Wasserstein distance between generated and true distributions of key quantities which summarize the simulate
    
[^32]: 利用语言模型用自然语言解释黑盒文本模块

    Explaining black box text modules in natural language with language models. (arXiv:2305.09863v1 [cs.AI])

    [http://arxiv.org/abs/2305.09863](http://arxiv.org/abs/2305.09863)

    本文介绍了一种名为Summarize and Score（SASC）的方法，该方法可以自动获取黑盒文本模块的自然语言解释以及解释可靠程度的分数。研究者们已经在合成模块和BERT模型中使用SASC，让我们可以解释模块的选择性，这对于增强大型语言模型的可解释性非常重要。

    

    大型语言模型已经证明在各种任务中具有出色的预测性能。然而，它们的快速增长和不透明性已经引起了对可解释性的需求。本文询问是否可以自动获取黑盒文本模块的自然语言解释。一个“文本模块”是将文本映射到标量连续值的任何函数，例如LLM内的子模块或大脑区域的拟合模型。“黑盒”表示我们只能访问模块的输入/输出。我们引入了Summarize and Score（SASC）方法，它接受文本模块并返回模块选择性的自然语言解释以及解释可靠程度的分数。我们在三个上下文中研究SASC。首先，我们在合成模块上评估SASC，并发现它经常恢复基本真相说明。其次，我们使用SASC来解释预训练BERT模型中的模块，使得检查BERT的模块成为可能。

    Large language models (LLMs) have demonstrated remarkable prediction performance for a growing array of tasks. However, their rapid proliferation and increasing opaqueness have created a growing need for interpretability. Here, we ask whether we can automatically obtain natural language explanations for black box text modules. A "text module" is any function that maps text to a scalar continuous value, such as a submodule within an LLM or a fitted model of a brain region. "Black box" indicates that we only have access to the module's inputs/outputs.  We introduce Summarize and Score (SASC), a method that takes in a text module and returns a natural language explanation of the module's selectivity along with a score for how reliable the explanation is. We study SASC in 3 contexts. First, we evaluate SASC on synthetic modules and find that it often recovers ground truth explanations. Second, we use SASC to explain modules found within a pre-trained BERT model, enabling inspection of the 
    
[^33]: 带概率态射和核平均嵌入的监督学习

    Supervised learning with probabilistic morphisms and kernel mean embeddings. (arXiv:2305.06348v1 [math.ST])

    [http://arxiv.org/abs/2305.06348](http://arxiv.org/abs/2305.06348)

    本文提出了监督学习中正确损失函数的概念，其通过概率测度的条件正则概率测度解决线性算子方程的问题得到定义，适用于可测空间的输入空间和标签空间。

    

    本文提出了一个监督学习的生成模型中正确损失函数的概念，适用于可测空间的输入空间X和标签空间Y。 生成模型中的正确损失函数必须正确地度量可能预测器的假设空间H中的元素与监管运算符之间的差异，而监管运算符可能不属于H。 为了定义正确的损失函数，本文提出了一个关于概率测度μ在投影ΠX：X×Y→X相对于概率测度μ𝑋×𝑌的条件正则概率测度μY| X的特殊性质的表征方法，作为线性算子方程的解决方案。 如果Y是一个具有Borel σ-代数 BY的可分的可度量化拓扑空间，则提出了关于概率测度μ相对于投影ΠX的条件正则概率测度μY| X的另一种特殊性质的表征方法。

    In this paper I propose a concept of a correct loss function in a generative model of supervised learning for an input space $\mathcal{X}$ and a label space $\mathcal{Y}$, which are measurable spaces. A correct loss function in a generative model of supervised learning must correctly measure the discrepancy between elements of a hypothesis space $\mathcal{H}$ of possible predictors and the supervisor operator, which may not belong to $\mathcal{H}$. To define correct loss functions, I propose a characterization of a regular conditional probability measure $\mu_{\mathcal{Y}|\mathcal{X}}$ for a probability measure $\mu$ on $\mathcal{X} \times \mathcal{Y}$ relative to the projection $\Pi_{\mathcal{X}}: \mathcal{X}\times\mathcal{Y}\to \mathcal{X}$ as a solution of a linear operator equation. If $\mathcal{Y}$ is a separable metrizable topological space with the Borel $\sigma$-algebra $ \mathcal{B} (\mathcal{Y})$, I propose another characterization of a regular conditional probability measure
    
[^34]: 使用深度强化学习从电子病历中提取诊断路径

    Extracting Diagnosis Pathways from Electronic Health Records Using Deep Reinforcement Learning. (arXiv:2305.06295v1 [cs.LG])

    [http://arxiv.org/abs/2305.06295](http://arxiv.org/abs/2305.06295)

    本文采用深度强化学习算法，基于电子病历来学习获得正确诊断所需的观察序列的最优顺序。因为诊断指南的缺陷，尤其对罕见病或患有多种病的患者，DRL算法具有重要现实意义。

    

    临床诊断指南旨在说明可能导致诊断的步骤。指南能够理性地规范化临床决策，但由于它们的建立是为了覆盖大多数人群，因此在指导罕见病或患有多种病的患者获得正确诊断方面，存在缺陷。本文受指南启发，将诊断任务形式化为序列决策问题，并研究了使用电子病历(EHRs)训练的深度强化学习算法来学习获得正确诊断所需的观察序列的最优顺序。由于DRL算法的多样性和对上下文的敏感性，我们考虑了几种方法和设置，并将它们与彼此和经典分类器进行了比较。我们在一个合成但逼真的数据集上进行了实验。

    Clinical diagnosis guidelines aim at specifying the steps that may lead to a diagnosis. Guidelines enable rationalizing and normalizing clinical decisions but suffer drawbacks as they are built to cover the majority of the population and may fail in guiding to the right diagnosis for patients with uncommon conditions or multiple pathologies. Moreover, their updates are long and expensive, making them unsuitable to emerging practices. Inspired by guidelines, we formulate the task of diagnosis as a sequential decision-making problem and study the use of Deep Reinforcement Learning (DRL) algorithms trained on Electronic Health Records (EHRs) to learn the optimal sequence of observations to perform in order to obtain a correct diagnosis. Because of the variety of DRL algorithms and of their sensitivity to the context, we considered several approaches and settings that we compared to each other, and to classical classifiers. We experimented on a synthetic but realistic dataset to differenti
    
[^35]: $\beta$-变分自编码器和Transformer用于流体流动的降阶建模

    $\beta$-Variational autoencoders and transformers for reduced-order modelling of fluid flows. (arXiv:2304.03571v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2304.03571](http://arxiv.org/abs/2304.03571)

    本文提出了一种使用$\beta$-VAE和Transformer相结合的方法来学习紧凑且近似正交的ROMs，该方法可用于混沌流体流动的降阶建模，并在准确性方面优于其他预测模型。

    

    变分自编码器（VAE）架构有潜力开发混沌流体流动的降阶模型（ROMs）。我们提出一种使用$\beta$-VAE和Transformer相结合的方法来学习紧凑且近似正交的ROMs，同时在二维粘性流体流动的周期和混沌状态下进行数值测试。$\beta$-VAE被训练为学习流速的紧凑潜在表示形式，而Transformer则被训练为在潜在空间中预测时间动态。通过使用$\beta$-VAE来学习分离表示形式，我们获得了更可解释的流动模型，其特征类似于观察到的适当正交分解，但表示更高效。使用Poincaré图，结果表明我们的方法可以捕捉流体流动的基本动力学，优于其他预测模型。该方法在天气预报、地层流和气候模型等其他领域具有潜在应用。

    Variational autoencoder (VAE) architectures have the potential to develop reduced-order models (ROMs) for chaotic fluid flows. We propose a method for learning compact and near-orthogonal ROMs using a combination of a $\beta$-VAE and a transformer, tested on numerical data from a two-dimensional viscous flow in both periodic and chaotic regimes. The $\beta$-VAE is trained to learn a compact latent representation of the flow velocity, and the transformer is trained to predict the temporal dynamics in latent space. Using the $\beta$-VAE to learn disentangled representations in latent-space, we obtain a more interpretable flow model with features that resemble those observed in the proper orthogonal decomposition, but with a more efficient representation. Using Poincar\'e maps, the results show that our method can capture the underlying dynamics of the flow outperforming other prediction models. The proposed method has potential applications in other fields such as weather forecasting, st
    
[^36]: SS-shapelets: 代表形状子序列的半监督时间序列聚类方法

    SS-shapelets: Semi-supervised Clustering of Time Series Using Representative Shapelets. (arXiv:2304.03292v1 [cs.LG])

    [http://arxiv.org/abs/2304.03292](http://arxiv.org/abs/2304.03292)

    本论文提出了一种名为SS-shapelets的半监督时间序列聚类方法，通过使用少量标记的和传播的伪标记时间序列来发现代表性形状子序列，从而提高聚类准确性。

    

    形状子序列是使用本地特征（子序列）鉴别时间序列的有前途的时间序列聚类方法。现有的时间序列聚类方法可能无法捕获代表性形状子序列，因为它们从大量无信息的子序列中发现形状子序列，因此聚类准确性较低。本文提出了一种使用少量标记的和传播的伪标记时间序列来帮助发现代表性形状子序列，从而提高聚类准确性的半监督时间序列聚类方法（SS-Shapelets）。在SS-Shapelets中，我们提出了两种技术来发现代表性形状子序列，以便有效地聚类时间序列。1）一个“显著子序列链”（$SSC$），可以从标记/伪标记的时间序列中提取显著子序列（作为候选形状子序列），从而有助于从池中删除大量无信息的子序列。2）一种“线性判别选择形状子序列”（$LDSS$），它选择在多个时间序列之间具有区别性的形状子序列。

    Shapelets that discriminate time series using local features (subsequences) are promising for time series clustering. Existing time series clustering methods may fail to capture representative shapelets because they discover shapelets from a large pool of uninformative subsequences, and thus result in low clustering accuracy. This paper proposes a Semi-supervised Clustering of Time Series Using Representative Shapelets (SS-Shapelets) method, which utilizes a small number of labeled and propagated pseudo-labeled time series to help discover representative shapelets, thereby improving the clustering accuracy. In SS-Shapelets, we propose two techniques to discover representative shapelets for the effective clustering of time series. 1) A \textit{salient subsequence chain} ($SSC$) that can extract salient subsequences (as candidate shapelets) of a labeled/pseudo-labeled time series, which helps remove massive uninformative subsequences from the pool. 2) A \textit{linear discriminant select
    
[^37]: 基于深度学习和辅助训练数据的多用户活动识别实现人机协作

    Towards Multi-User Activity Recognition through Facilitated Training Data and Deep Learning for Human-Robot Collaboration Applications. (arXiv:2302.05763v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05763](http://arxiv.org/abs/2302.05763)

    本研究通过收集单个用户数据并在后处理中合并数据的方法，实现了多用户活动的识别，有望用于人机协作领域。

    

    人机交互研究逐渐关注多方面场景，即机器人与多个人用户同时交互的场景。 然而，在人机协作方面，研究仍处于早期阶段。处理此类合作的机器学习技术需要的数据比典型的人机交互设置中更不可行。本研究提出了非二元人机协作应用的并行任务场景，并提议一种替代方法来收集与多用户活动相关的数据，即收集与单个用户相关的数据并在后处理中合并它们，以减少产生成双设置录制的努力。收集了单个用户的活动三维骨架姿势并将它们合并成一对来验证该语句，随后，这些数据点被用于分别训练由LSTM网络和VAE 混合而成的模型。

    Human-robot interaction (HRI) research is progressively addressing multi-party scenarios, where a robot interacts with more than one human user at the same time. Conversely, research is still at an early stage for human-robot collaboration. The use of machine learning techniques to handle such type of collaboration requires data that are less feasible to produce than in a typical HRC setup. This work outlines scenarios of concurrent tasks for non-dyadic HRC applications. Based upon these concepts, this study also proposes an alternative way of gathering data regarding multi-user activity, by collecting data related to single users and merging them in post-processing, to reduce the effort involved in producing recordings of pair settings. To validate this statement, 3D skeleton poses of activity of single users were collected and merged in pairs. After this, such datapoints were used to separately train a long short-term memory (LSTM) network and a variational autoencoder (VAE) composed
    
[^38]: 通过自适应平滑改善分类器的准确性-鲁棒性平衡

    Improving the Accuracy-Robustness Trade-Off of Classifiers via Adaptive Smoothing. (arXiv:2301.12554v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12554](http://arxiv.org/abs/2301.12554)

    本文研究通过混合标准分类器和鲁棒模型的输出概率来减轻准确性和鲁棒性之间的权衡问题，进而提高分类器的鲁棒性。同时提出了一种自适应平滑的方法，可以降低实现鲁棒性的准确度惩罚。

    

    尽管以前的研究提出了大量增强神经分类器对抗鲁棒性的方法，但由于在清晰度方面存在不可接受的严重惩罚，实践者仍然不愿采用这些技术。本文表明，通过混合标准分类器和强鲁棒模型的输出概率，其中标准网络优化清晰度而不是一般的鲁棒性，可以显着减轻这种准确性-鲁棒性权衡问题。我们显示出基于鲁棒性的基本分类器的正确和不正确示例的置信度差异是这种改善的关键因素。除提供直观和经验证据外，我们还在现实假设下理论上证明了混合分类器的鲁棒性。此外，我们还将一个对抗性输入检测器适应为混合网络，自适应地调整两个基本模型的混合，从而进一步减少实现鲁棒性的准确性惩罚。

    While prior research has proposed a plethora of methods that enhance the adversarial robustness of neural classifiers, practitioners are still reluctant to adopt these techniques due to their unacceptably severe penalties in clean accuracy. This paper shows that by mixing the output probabilities of a standard classifier and a robust model, where the standard network is optimized for clean accuracy and is not robust in general, this accuracy-robustness trade-off can be significantly alleviated. We show that the robust base classifier's confidence difference for correct and incorrect examples is the key ingredient of this improvement. In addition to providing intuitive and empirical evidence, we also theoretically certify the robustness of the mixed classifier under realistic assumptions. Furthermore, we adapt an adversarial input detector into a mixing network that adaptively adjusts the mixture of the two base models, further reducing the accuracy penalty of achieving robustness. The 
    
[^39]: 预测是否随意？在公平分类中评估自洽性

    Is My Prediction Arbitrary? Measuring Self-Consistency in Fair Classification. (arXiv:2301.11562v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11562](http://arxiv.org/abs/2301.11562)

    在公平分类中，模型的预测方差是一个重要但鲜为人知的误差来源问题。作者提出了一个自洽性标准来衡量测量和减少随意性。作者还开发了一个算法来处理随意性预测，并通过实证研究揭示了当前模型无法处理某些类型数据的问题。

    

    在公平分类中，不同经过训练的模型之间的预测方差是一个重要但鲜为人知的误差来源问题。 实证表明，某些情况下，预测的方差差异非常大，以至于决策实际上是随意的。 为了研究这个问题，我们进行了大规模的实证研究，并做出了四个总体贡献：我们1）定义了一种基于方差的度量标准，称为自洽性，在测量和减少随意性时使用； 2）开发了一种合理的算法，当预测无法做出决策时，可以放弃分类； 3）进行了迄今为止有关公平分类中方差（相对于自洽性和随意性）作用的最大规模实证研究； 4）推出了一个工具包，使美国住房抵押贷款披露法案（HMDA）数据集易于用于未来研究。 总的来说，我们的实证结果揭示了关于可重复性的令人震惊的见解。当考虑到方差和随意预测的可能性时，大多数公平分类基准接近公平。 但是，一小部分实例显示出极大的随意性水平，这表明当前的模型可能无法处理某些类型的数据。

    Variance in predictions across different trained models is a significant, under-explored source of error in fair classification. Empirically, the variance on some instances is so large that decisions can be effectively arbitrary. To study this problem, we perform a large-scale empirical study and make four overarching contributions: We 1) Define a metric called self-consistency, derived from variance, which we use as a proxy for measuring and reducing arbitrariness; 2) Develop an ensembling algorithm that abstains from classification when a prediction would be arbitrary; 3) Conduct the largest to-date empirical study of the role of variance (vis-a-vis self-consistency and arbitrariness) in fair classification; and, 4) Release a toolkit that makes the US Home Mortgage Disclosure Act (HMDA) datasets easily usable for future research. Altogether, our empirical results reveal shocking insights about reproducibility. Most fairness classification benchmarks are close-to-fair when taking into
    
[^40]: 基于语言模型的知识图谱嵌入编辑

    Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.10405](http://arxiv.org/abs/2301.10405)

    本文提出了一种新的任务——编辑基于语言模型的知识图谱嵌入，旨在实现对KG嵌入的数据高效和快速更新。针对这一任务，提出了一个简单而强大的方案——KGEditor，可以更好地更新特定事实而不影响其余部分的性能。

    

    近几十年来，使用语言模型进行知识图谱（KG）嵌入已经取得了实证成功。但是，基于语言模型的KG嵌入通常作为静态工件部署，修改起来具有挑战性，需要重新训练。为了解决这个问题，本文提出了一种新的任务，即编辑基于语言模型的KG嵌入。该任务旨在实现对KG嵌入的数据高效和快速更新，而不影响其余部分的性能。我们构建了四个新数据集：E-FB15k237、A-FB15k237、E-WN18RR 和 A-WN18RR，并评估了几种知识编辑基线，证明了之前的模型处理该任务的能力有限。我们进一步提出了一个简单但强大的基线——KGEditor，它利用超网络的附加参数层来编辑/添加事实。全面的实验结果表明，当更新特定事实而不影响其余部分的性能时，KGEditor 的表现更好。

    Recently decades have witnessed the empirical success of framing Knowledge Graph (KG) embeddings via language models. However, language model-based KG embeddings are usually deployed as static artifacts, which are challenging to modify without re-training after deployment. To address this issue, we propose a new task of editing language model-based KG embeddings in this paper. The proposed task aims to enable data-efficient and fast updates to KG embeddings without damaging the performance of the rest. We build four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and evaluate several knowledge editing baselines demonstrating the limited ability of previous models to handle the proposed challenging task. We further propose a simple yet strong baseline dubbed KGEditor, which utilizes additional parametric layers of the hyper network to edit/add facts. Comprehensive experimental results demonstrate that KGEditor can perform better when updating specific facts while not affec
    
[^41]: 在Sobolev和Besov空间上，关于深度ReLU神经网络的最佳逼近速率研究

    Optimal Approximation Rates for Deep ReLU Neural Networks on Sobolev and Besov Spaces. (arXiv:2211.14400v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.14400](http://arxiv.org/abs/2211.14400)

    该论文研究了在Sobolev和Besov空间中，使用ReLU激活函数的深度神经网络能够以怎样的参数效率逼近函数，包括$L_p(\Omega)$范数下的误差度量。我们提供了所有$1\leq p,q \leq \infty$和$s>0$的完整解决方案，并引入了一种新的位提取技术来获得尖锐的上界。

    

    本文研究了使用ReLU激活函数的深度神经网络在Sobolev空间$W^s(L_q(\Omega))$和Besov空间$B^s_r(L_q(\Omega))$中以$L_p(\Omega)$范数度量误差的参数效率问题。我们的研究对于在科学计算和信号处理等领域中应用神经网络非常重要，在过去只有当$p=q=\infty$时才完全解决。我们的贡献是提供了所有$1\leq p,q\leq \infty$和$s>0$的完整解决方案，包括渐近匹配的上下界。关键的技术工具是一种新的位提取技术，它提供了稀疏向量的最佳编码。这使我们能够在$p>q$的非线性区域获得尖锐的上界。我们还提供了一种基于的$L_p$逼近下界推导的新方法。

    Let $\Omega = [0,1]^d$ be the unit cube in $\mathbb{R}^d$. We study the problem of how efficiently, in terms of the number of parameters, deep neural networks with the ReLU activation function can approximate functions in the Sobolev spaces $W^s(L_q(\Omega))$ and Besov spaces $B^s_r(L_q(\Omega))$, with error measured in the $L_p(\Omega)$ norm. This problem is important when studying the application of neural networks in a variety of fields, including scientific computing and signal processing, and has previously been completely solved only when $p=q=\infty$. Our contribution is to provide a complete solution for all $1\leq p,q\leq \infty$ and $s > 0$, including asymptotically matching upper and lower bounds. The key technical tool is a novel bit-extraction technique which gives an optimal encoding of sparse vectors. This enables us to obtain sharp upper bounds in the non-linear regime where $p > q$. We also provide a novel method for deriving $L_p$-approximation lower bounds based upon
    
[^42]: 奖励并非必要：如何为终身学习创建一个组合性自我保护智能体

    Reward is not Necessary: How to Create a Compositional Self-Preserving Agent for Life-Long Learning. (arXiv:2211.10851v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.10851](http://arxiv.org/abs/2211.10851)

    这项研究表明，我们可以使用内在动机衡量标准而不依赖于奖励来创建一个具有自我保护能力的智能体。

    

    强化学习认为最大化奖励和避免惩罚是解释目标导向行为的核心。然而，在一生中，生物需要学习关于世界结构的许多不同方面：世界状态和状态转移动力学。随着智能体融入新知识，状态组合的数量以指数级增长，并且对于给定的状态组合，没有明显定义的预设奖励或成本的加权组合，因为这样的加权需要在智能体在世界中的经验之前对好的和坏的组合进行编码。因此，我们必须在大状态空间中开发更自然的行为和动机模型。我们展示了仅使用内在动机衡量标准（即赋予能力）是可能的，该标准衡量智能体在转移操作者下实现许多可能未来的能力。我们建议将赋予能力扩展到分层状态空间中。

    Reinforcement Learning views the maximization of rewards and avoidance of punishments as central to explaining goal-directed behavior. However, over a life, organisms will need to learn about many different aspects of the world's structure: the states of the world and state-vector transition dynamics. The number of combinations of states grows exponentially as an agent incorporates new knowledge, and there is no obvious weighted combination of pre-existing rewards or costs defined for a given combination of states, as such a weighting would need to encode information about good and bad combinations prior to an agent's experience in the world. Therefore, we must develop more naturalistic accounts of behavior and motivation in large state-spaces. We show that it is possible to use only the intrinsic motivation metric of empowerment, which measures the agent's capacity to realize many possible futures under a transition operator. We propose to scale empowerment to hierarchical state-space
    
[^43]: 抽象模型驱动的强化学习的分析

    An Analysis of Abstracted Model-Based Reinforcement Learning. (arXiv:2208.14407v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.14407](http://arxiv.org/abs/2208.14407)

    本论文分析了抽象模型驱动的强化学习中的问题，揭示抽象状态会引入样本相关性，而使用鞅不等式可以解决这个问题，从而将现有MBRL算法的保证扩展到带有抽象的设置。

    

    在马尔可夫决策过程中，许多模型驱动的强化学习（MBRL）方法都能够提供对模型准确性和学习效率的保证。同时，状态抽象技术可以在保持与原问题有界损失的同时减少MDP的大小。然而，当将这两种技术结合起来时，即MBRL仅仅观察抽象状态时，却没有相应的保证可用。我们的理论分析表明，抽象可以引入在线采集样本之间的相关性（例如在真实世界中采集的样本）。这意味着，如果不考虑这种相关性，MBRL的结果不能直接推广到这个设置中。我们的结果表明，我们可以使用鞅不等式来克服这个问题。这个结果使得将现有MBRL算法的保证扩展到带有抽象的设置成为可能。

    Many methods for Model-based Reinforcement learning (MBRL) in Markov decision processes (MDPs) provide guarantees for both the accuracy of the model they can deliver and the learning efficiency. At the same time, state abstraction techniques allow for a reduction of the size of an MDP while maintaining a bounded loss with respect to the original problem. Therefore, it may come as a surprise that no such guarantees are available when combining both techniques, i.e., where MBRL merely observes abstract states. Our theoretical analysis shows that abstraction can introduce a dependence between samples collected online (e.g., in the real world). That means that, without taking this dependence into account, results for MBRL do not directly extend to this setting. Our result shows that we can use concentration inequalities for martingales to overcome this problem. This result makes it possible to extend the guarantees of existing MBRL algorithms to the setting with abstraction. We illustrate 
    
[^44]: 论公平机器学习中因果关系的必要性和适用性

    On the Need and Applicability of Causality for Fair Machine Learning. (arXiv:2207.04053v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.04053](http://arxiv.org/abs/2207.04053)

    本论文探讨了因果关系在公平机器学习中的必要性和适用性，强调了非因果预测的社会影响和法律反歧视过程依赖于因果主张。同时讨论了在实际场景中应用因果关系所面临的挑战和限制，并提出了可能的解决方案。

    

    除了在流行病学、政治和社会科学中的常见应用案例外，事实证明因果关系在评估自动决策的公正性方面十分重要，无论是在法律上还是日常生活中。我们提供了关于为何因果关系对公平性评估尤为重要的论点和示例。特别是，我们指出了非因果预测的社会影响以及依赖因果主张的法律反歧视过程。我们最后讨论了应用因果关系在实际场景中的挑战和局限性，以及可能的解决方案。

    Besides its common use cases in epidemiology, political, and social sciences, causality turns out to be crucial in evaluating the fairness of automated decisions, both in a legal and everyday sense. We provide arguments and examples, of why causality is particularly important for fairness evaluation. In particular, we point out the social impact of non-causal predictions and the legal anti-discrimination process that relies on causal claims. We conclude with a discussion about the challenges and limitations of applying causality in practical scenarios as well as possible solutions.
    

