# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Disentangled Latent Spaces Facilitate Data-Driven Auxiliary Learning.](http://arxiv.org/abs/2310.09278) | 本论文提出了一个新的框架，通过解耦过程来发现可以与主任务一起利用的不相关的分类任务和相关标签，从而在深度学习中促进辅助学习。 |
| [^2] | [A Hybrid Approach for Depression Classification: Random Forest-ANN Ensemble on Motor Activity Signals.](http://arxiv.org/abs/2310.09277) | 这篇论文介绍了一种名为混合随机森林-神经网络的新算法，该算法通过评估抑郁患者的传感器数据来分类抑郁症，准确率达到80%。 |
| [^3] | [Retro-fallback: retrosynthetic planning in an uncertain world.](http://arxiv.org/abs/2310.09270) | 本文针对逆合成任务在实验室执行可行性的不确定性问题，通过引入随机过程的表述，提出了一种名为 Retro-fallback 的贪婪算法，该算法能够最大化实验室可执行的合成计划的概率。 |
| [^4] | [Genetic algorithms are strong baselines for molecule generation.](http://arxiv.org/abs/2310.09267) | 本文研究表明，遗传算法是分子生成领域的强大基准线，并提出了GA准则，要求新的算法必须明显优于遗传算法。 |
| [^5] | [User Inference Attacks on Large Language Models.](http://arxiv.org/abs/2310.09266) | 本论文研究了在大型语言模型上的用户推理攻击，发现LLMs对于各种微调数据集都很容易受到攻击，尤其是对于离群用户和贡献大量数据的用户。这对保护用户隐私具有重要意义。 |
| [^6] | [PromptRE: Weakly-Supervised Document-Level Relation Extraction via Prompting-Based Data Programming.](http://arxiv.org/abs/2310.09265) | PromptRE是一种弱监督文档级别关系抽取方法，通过结合基于提示的技术，解决了“没有关系”的实例数量不平衡和直接使用预训练模型进行文档关系抽取的问题。 |
| [^7] | [Towards End-to-end 4-Bit Inference on Generative Large Language Models.](http://arxiv.org/abs/2310.09259) | 本论文介绍了一种使用名为QUIK的混合量化策略，在保持良好精度的同时实现大型生成模型的实际速度提升，通过将权重和激活值转换为4位，并提供高效率的逐层运行时GPU内核，实现了高达3.1倍的实际端到端吞吐量提升。 |
| [^8] | [Generative Entropic Neural Optimal Transport To Map Within and Across Spaces.](http://arxiv.org/abs/2310.09254) | 该论文介绍了生成熵神经最优传输在测度到测度映射中的应用，解决了处理非平方欧氏距离成本、确定性蒙格映射、映射跨不可比较空间和质量守恒约束等实际挑战。 |
| [^9] | [It's an Alignment, Not a Trade-off: Revisiting Bias and Variance in Deep Models.](http://arxiv.org/abs/2310.09250) | 在基于深度学习的分类模型集合中，对于正确分类的样本点，偏差和方差在样本级别上是对齐的。 |
| [^10] | [Hypernymy Understanding Evaluation of Text-to-Image Models via WordNet Hierarchy.](http://arxiv.org/abs/2310.09247) | 通过基于WordNet层次结构的方法，我们评估了流行的文本到图像模型对于上义词关系的理解能力。我们提出了两个自动度量标准，能够定量比较不同模型的语言能力，并发现了一些困难的词汇。我们全面评估了一些流行的文本到图像模型，包括GLIDE、Latent Diffusion和Stable Diffusion。 |
| [^11] | [Time CNN and Graph Convolution Network for Epileptic Spike Detection in MEG Data.](http://arxiv.org/abs/2310.09236) | 提出了一种使用1D时间卷积神经网络(Time CNN)和图卷积网络(GCN)的方法来检测MEG数据中的癫痫尖峰。相较于其他方法，我们的模型具有更少的参数，并且使用GCN来考虑MEG传感器的空间关系。在临床数据集和高度不平衡的数据集上，我们的模型都取得了优于深度学习方法的分类性能。 |
| [^12] | [Insuring Smiles: Predicting routine dental coverage using Spark ML.](http://arxiv.org/abs/2310.09229) | 本文利用机器学习算法分析健康保险计划的类型、区域和费用等因素，预测其是否覆盖成年人的常规牙科服务，旨在为个人和家庭提供选择最合适保险的临床策略。 |
| [^13] | [Fast & Efficient Learning of Bayesian Networks from Data: Knowledge Discovery and Causality.](http://arxiv.org/abs/2310.09222) | 本论文提出了两个新算法FSBN和SSBN，它们利用局部搜索和条件独立性测试从数据中学习贝叶斯网络的因果结构。实验结果表明，这两个算法在降低计算成本的同时保持了高质量的归纳，提供了可解释性和适应性。 |
| [^14] | [Unseen Image Synthesis with Diffusion Models.](http://arxiv.org/abs/2310.09213) | 本论文提出使用预训练的扩散模型在未见过的领域合成图像的方法，并理论上和经验上证明了这种方法的有效性。 |
| [^15] | [Regularization-Based Methods for Ordinal Quantification.](http://arxiv.org/abs/2310.09210) | 本文研究了序数量化问题，提出了两个新的数据集用于研究，实验比较了已有算法，并提出了一种性能更好的正则化OQ算法类别。 |
| [^16] | [SiamAF: Learning Shared Information from ECG and PPG Signals for Robust Atrial Fibrillation Detection.](http://arxiv.org/abs/2310.09203) | 提出了一种名为SiamAF的新方法，利用心电图和光电脉搏图信号的共享信息，通过Siamese网络和联合学习实现强健的心房颤动（AF）检测。 |
| [^17] | [Graph Condensation via Eigenbasis Matching.](http://arxiv.org/abs/2310.09202) | 本论文提出了基于特征匹配的无谱图压缩方法，以解决现有方法在泛化能力上存在的问题。通过详细分析发现，GNN注入合成图中的谱偏差导致了性能差异，我们的方法可以缓解这个问题。 |
| [^18] | [A 4-approximation algorithm for min max correlation clustering.](http://arxiv.org/abs/2310.09196) | 本论文引入了一种新的下界技术，并提出了一个组合算法来求解最小最大相关聚类问题，该算法能够在完全图上达到4的近似结果。通过贪婪联合启发式算法的扩展和实验证明，在多个基准数据集上，该算法在解决方案质量和运行时间上均有显著改进。 |
| [^19] | [Variational autoencoder with weighted samples for high-dimensional non-parametric adaptive importance sampling.](http://arxiv.org/abs/2310.09194) | 本文提出了一种使用变分自动编码器和加权样本的方法来近似高维非参数自适应重要性采样中的目标分布。所得到的分布族具有与非参数模型相当的表达能力，并且在高维情况下比传统的高斯或高斯混合族更高效。同时，我们还引入了可学习的先验分布以增加模型的灵活性和学习多模态分布的能力。 |
| [^20] | [Does Graph Distillation See Like Vision Dataset Counterpart?.](http://arxiv.org/abs/2310.09192) | 本文研究了图压缩方法中对结构信息的忽视问题，并提出了一种新的结构广播图数据集压缩方案（SGDD），该方案能够保留原始图的结构信息，并在跨架构泛化和特定任务中取得了优秀的性能。 |
| [^21] | [PRIOR: Personalized Prior for Reactivating the Information Overlooked in Federated Learning.](http://arxiv.org/abs/2310.09183) | 本研究提出了一种名为PRIOR的个性化先验方案，用于解决个性化联邦学习中由于全局模型忽视特定信息而导致的不完整信息问题。该方案使用带Bregman散度的PFL框架将个性化先验与本地目标函数解耦，以提高适应性和性能。 |
| [^22] | [A Deep Neural Network -- Mechanistic Hybrid Model to Predict Pharmacokinetics in Rat.](http://arxiv.org/abs/2310.09167) | 该论文提出了一种深度神经网络和机制混合模型，用于预测大鼠的药代动力学。通过训练更大的数据集、改进网络架构和参数化机制模型，成功减小了口服和静脉给药的误差，并将这种方法扩展到预测更多终点和处理不同的协变量。 |
| [^23] | [Jointly-Learned Exit and Inference for a Dynamic Neural Network : JEI-DNN.](http://arxiv.org/abs/2310.09163) | JEI-DNN是一种联合学习退出和推断的动态神经网络架构，通过允许模型在中间层进行部分预测，解决了大型预训练模型每次推断所需大量资源的问题。 |
| [^24] | [Quantum Machine Learning in Climate Change and Sustainability: a Review.](http://arxiv.org/abs/2310.09162) | 量子机器学习在解决气候变化和可持续发展问题方面具有潜力，可以应用于能源系统、气候数据预测、气候监测和危险事件预测等领域。该综述调查了目前将量子机器学习应用于这些问题的现有研究，并讨论了挑战、局限以及潜在机会和未来工作。 |
| [^25] | [The Computational Complexity of Finding Stationary Points in Non-Convex Optimization.](http://arxiv.org/abs/2310.09157) | 本文研究了在非凸优化中找到光滑目标函数的近似稳定点的计算复杂性和查询复杂性，并给出了相应的结果。对于$d=2$的情况，提供了一种零阶算法，只需要少量的函数值查询即可找到$\varepsilon$-近似稳定点。 |
| [^26] | [Lattice Approximations in Wasserstein Space.](http://arxiv.org/abs/2310.09149) | 本论文研究了在Wasserstein空间中通过离散和分段常数测度进行的结构逼近方法。结果表明，对于满秩的格点按比例缩放后得到的Voronoi分割逼近的测度误差是$O(h)$，逼近的$N$项误差为$O(N^{-\frac1d})$，并且可以推广到非紧支撑测度。 |
| [^27] | [Goodhart's Law in Reinforcement Learning.](http://arxiv.org/abs/2310.09144) | 该论文通过研究古哈特定律在强化学习中的现象，提出了一种衡量效应程度的方法，并实证表明在广泛的环境和奖励函数范围内，对不完美代理奖励的过度优化会导致降低在真实目标上的性能。然后，通过几何解释马尔可夫决策过程中古哈特定律的发生，提出了一种可避免陷阱的最优早停止方法，并推导了方法的理论遗憾界限。此外，还提出了一种逐渐过渡到真实奖励的训练方法。 |
| [^28] | [The Consensus Game: Language Model Generation via Equilibrium Search.](http://arxiv.org/abs/2310.09139) | 这篇论文介绍了一种新的语言模型解码方法，将其视为规范化的不完美信息序列信号博弈，并通过找到近似均衡点得到了一个解码算法。这种方法可以应用于问答和其他文本生成任务中。 |
| [^29] | [Computing Marginal and Conditional Divergences between Decomposable Models with Applications.](http://arxiv.org/abs/2310.09129) | 提出了一种计算可分解模型之间边际和条件差异的方法，能够在高维分布中精确计算差异，具有广泛的应用价值。 |
| [^30] | [On Generalization Bounds for Projective Clustering.](http://arxiv.org/abs/2310.09127) | 本文研究了投影聚类的学习界限问题，给出了几个近乎最优的结果，并且对于基于中心的目标，展示了收敛速率为O(sqrt(k/n))。 |
| [^31] | [Physics-guided Noise Neural Proxy for Low-light Raw Image Denoising.](http://arxiv.org/abs/2310.09126) | 本文提出了一种新的物理引导噪声神经代理（PNNP）用于准确噪声建模和低光原始图像去噪，集成了物理引导噪声解耦、物理引导代理模型和可微分分布导向损失等高效技术。 |
| [^32] | [Training and Predicting Visual Error for Real-Time Applications.](http://arxiv.org/abs/2310.09125) | 本论文研究了卷积神经网络在实时应用中训练和预测视觉误差的能力，通过预测重用着色或使用降低着色率导致的视觉误差，避免了对参考或渲染图像的依赖。 |
| [^33] | [Automatic Music Playlist Generation via Simulation-based Reinforcement Learning.](http://arxiv.org/abs/2310.09123) | 本文提出了一个使用模拟环境的强化学习框架，通过直接优化用户满意度指标，实现个性化音乐播放列表的自动生成。我们使用修改版的深度Q网络训练出的策略能够从大型和动态的候选项目集中进行推荐，以最大化消费指标。 |
| [^34] | [DSG: An End-to-End Document Structure Generator.](http://arxiv.org/abs/2310.09118) | DSG是一种端到端训练的文档解析系统，能够将渲染文档映射到结构化的层次格式，在实际应用中具有高效和灵活的特点。 |
| [^35] | [Insightful analysis of historical sources at scales beyond human capabilities using unsupervised Machine Learning and XAI.](http://arxiv.org/abs/2310.09091) | 本研究使用创新的机器学习技术对大规模历史文献进行分析，并重点研究了“Sacrobosco Collection”中知识的演变。通过这一研究，我们得出了一些重要的历史洞察。 |
| [^36] | [Online Relocating and Matching of Ride-Hailing Services: A Model-Based Modular Approach.](http://arxiv.org/abs/2310.09071) | 本研究提出了一种创新的基于模型的模块化方法，用于动态优化拼车平台中的订单匹配和车辆重定位。该方法通过两层和模块化的建模结构进行车流转移模式和快速匹配重定位，并证明了在特定网络中能够达到全局最优解，实验结果显示该方法具有优越的系统性能。 |
| [^37] | [KCTS: Knowledge-Constrained Tree Search Decoding with Token-Level Hallucination Detection.](http://arxiv.org/abs/2310.09044) | 提出了一种名为KCTS的知识约束树搜索解码方法，利用知识分类器和MCTS指导冻结的LM生成与参考知识对齐的文本，同时引入了一种新颖的令牌级幻觉检测方法RIPA。 |
| [^38] | [Optimal Scheduling of Electric Vehicle Charging with Deep Reinforcement Learning considering End Users Flexibility.](http://arxiv.org/abs/2310.09040) | 本研究使用深度强化学习方法，针对时间段电价计划，通过优化调度电动车充电过程来降低用户家庭的成本。 |
| [^39] | [MINDE: Mutual Information Neural Diffusion Estimation.](http://arxiv.org/abs/2310.09031) | MINDE是一种基于得分函数扩散模型的新方法，用于估计随机变量之间的互信息。该方法在准确性和自一致性测试方面优于其他文献中的主要替代方法。 |
| [^40] | [Subspace Adaptation Prior for Few-Shot Learning.](http://arxiv.org/abs/2310.09028) | 提出了少样本学习的子空间适应先验算法，通过同时学习初始化参数和参数子空间，可以基于任务分布决定使用梯度下降调整哪些操作子集，从而提高学习效率并降低过拟合风险。 |
| [^41] | [Federated Meta-Learning for Few-Shot Fault Diagnosis with Representation Encoding.](http://arxiv.org/abs/2310.09002) | 本文提出了一种基于表示编码的联邦元学习框架 (REFML) 用于少样本故障诊断，通过利用训练客户端之间的异质性并使用自适应插值方法来提高模型的泛化能力。 |
| [^42] | [Measuring the Stability of Process Outcome Predictions in Online Settings.](http://arxiv.org/abs/2310.09000) | 该论文提出了一个评估框架，用于衡量在线预测性过程监控模型的稳定性。该框架引入了四个性能元度量：显著性能下降的频率、这些下降的幅度、恢复率和性能的波动。 |
| [^43] | [Reroute Prediction Service.](http://arxiv.org/abs/2310.08988) | 该论文介绍了一种新颖的数据分析和机器学习系统，旨在通过积极支持重新规划决策来减少航班延误。该系统使用历史重新规划数据和天气数据预测未来几天是否会发布重新规划建议，以减少影响航线的因素。 |
| [^44] | [PAGE: Equilibrate Personalization and Generalization in Federated Learning.](http://arxiv.org/abs/2310.08961) | 本文提出了一种使用博弈论平衡个性化和泛化的联邦学习算法PAGE，通过将联邦学习视为客户和服务器之间的合作竞争博弈，并结合马尔科夫决策过程和强化学习算法来寻找平衡点。 |
| [^45] | [CAMELL: Confidence-based Acquisition Model for Efficient Self-supervised Active Learning with Label Validation.](http://arxiv.org/abs/2310.08944) | CAMELL是一个适用于序列多输出问题的主动学习框架，通过仅需专家标注序列的一小部分、自监督和标签验证机制来解决监督神经方法对大规模标注数据集的依赖限制。 |
| [^46] | [LLaMA Rider: Spurring Large Language Models to Explore the Open World.](http://arxiv.org/abs/2310.08922) | 本文提出了一种方法，通过多轮反馈-修订机制和子任务重标记，推动大型语言模型在开放世界中探索并提高其任务解决能力。 |
| [^47] | [Embarrassingly Simple Text Watermarks.](http://arxiv.org/abs/2310.08920) | 我们提出了一种极其简单但非常有效的文本水印方法Easymark，它可以在不改变文本含义的情况下注入水印，并且能够高度可信地检测出文本是否由采用该方法的系统生成。这种方法不需要访问大型语言模型，并且具有更高的检测准确度和BLEU分数。 |
| [^48] | [Relation-aware Ensemble Learning for Knowledge Graph Embedding.](http://arxiv.org/abs/2310.08917) | 本论文提出了一种关系感知集成学习方法，用于知识图谱嵌入任务，并通过分割搜索合并的算法在搜索关系感知集成权重方面取得了显著性能提升。 |
| [^49] | [Scalarization for Multi-Task and Multi-Domain Learning at Scale.](http://arxiv.org/abs/2310.08910) | 通过标量化方法在多任务和多领域学习中训练单个模型可以提高模型效率和准确性，并实现跨任务/领域的正向知识转移。该研究通过大规模分析多领域和多任务学习的动态性来更好地理解标量化方法的训练动态。 |
| [^50] | [Community Membership Hiding as Counterfactual Graph Search via Deep Reinforcement Learning.](http://arxiv.org/abs/2310.08909) | 这项研究通过深度强化学习的方式解决了社区成员隐藏的挑战，通过战略地改变网络图的结构属性，防止节点被社区检测算法识别出来，并验证了方法的有效性。 |
| [^51] | [Self supervised convolutional kernel based handcrafted feature harmonization: Enhanced left ventricle hypertension disease phenotyping on echocardiography.](http://arxiv.org/abs/2310.08897) | 本研究提出了一种自监督卷积核手工特征融合方法，用于增强超声心动图左室高血压病变的识别。通过将卷积滤波器应用于自监督学习预处理中，将图像转换为特征图，实现了在不同成像设备和协议下的一致特征提取。 |
| [^52] | [EHI: End-to-end Learning of Hierarchical Index for Efficient Dense Retrieval.](http://arxiv.org/abs/2310.08891) | EHI是一种端到端学习的层次索引方法，用于高效密集检索。它同时学习嵌入和ANNS结构，通过使用密集路径嵌入来捕获索引的语义信息，以优化检索性能。 |
| [^53] | [METRA: Scalable Unsupervised RL with Metric-Aware Abstraction.](http://arxiv.org/abs/2310.08887) | METRA提出了一种新的无监督强化学习目标，旨在使其在复杂的高维环境中可扩展。这个目标解决了纯探索方法在大状态空间环境中的困难以及互信息技能学习方法中缺乏激励而无法探索环境的问题。 |
| [^54] | [Gesture Recognition for FMCW Radar on the Edge.](http://arxiv.org/abs/2310.08876) | 本文介绍了一种基于60 GHz FMCW雷达的轻量级手势识别系统，通过使用一组五个特征和精简的处理算法，可以在嵌入式平台上高效识别手势，同时具有较低的内存、计算和功耗要求。 |
| [^55] | [A Survey of Methods for Handling Disk Data Imbalance.](http://arxiv.org/abs/2310.08867) | 本论文总结了处理磁盘数据不平衡问题的方法，包括数据层、算法层和混合方法，并讨论了不平衡数据分类的挑战及应对策略。 |
| [^56] | [Adaptivity and Modularity for Efficient Generalization Over Task Complexity.](http://arxiv.org/abs/2310.08866) | 引入一种新的任务来评估变压器在处理不同难度示例的问题上的泛化能力，并提出使用自适应和模块化计算机制的变压器架构，该架构在泛化到更高数量的计算时显示出更高的准确性和更公平的计算资源分配。 |
| [^57] | [In-Context Learning for Few-Shot Molecular Property Prediction.](http://arxiv.org/abs/2310.08863) | 本文将上下文学习的概念应用于少样本分子性质预测，通过学习从（分子，性质测量）对的上下文中预测分子性质，并在新的性质上快速适应，该方法在小支持集大小上超过了最近的元学习算法的性能，并在大支持集大小上与最佳方法竞争。 |
| [^58] | [Adam-family Methods with Decoupled Weight Decay in Deep Learning.](http://arxiv.org/abs/2310.08858) | 本文研究了一类广泛的Adam-family方法在训练非光滑神经网络中的收敛性质，提出了一种使用分离权重衰减的新框架，并证明了其收敛性。该框架包含了许多已知的Adam-family方法，并对这些方法在训练非光滑神经网络时提供了收敛性保证。 |
| [^59] | [Overcoming Recency Bias of Normalization Statistics in Continual Learning: Balance and Adaptation.](http://arxiv.org/abs/2310.08855) | 连续学习中的归一化统计存在近期偏差问题，我们提出了自适应BN的平衡策略AdaB$^2$N来解决这个问题。 |
| [^60] | [Rank-DETR for High Quality Object Detection.](http://arxiv.org/abs/2310.08854) | Rank-DETR是一种高质量目标检测方法，通过引入排名导向的设计，包括架构设计和损失函数设计，实现了性能卓越的基于DETR的目标检测器。 |
| [^61] | [Semi-Supervised End-To-End Contrastive Learning For Time Series Classification.](http://arxiv.org/abs/2310.08848) | 本文提出了一个名为SLOTS的半监督端到端模型，用于时间序列分类。它通过接收半标记数据集，在无监督预训练和下游微调中综合利用对比损失和分类损失，解决了现有方法中的缺点。 |
| [^62] | [On the Over-Memorization During Natural, Robust and Catastrophic Overfitting.](http://arxiv.org/abs/2310.08847) | 本论文研究了深度神经网络中的过度记忆问题，发现其会损害泛化能力，并提出了方法综合性地减轻不同类型的过拟合。 |
| [^63] | [A Framework for Few-Shot Policy Transfer through Observation Mapping and Behavior Cloning.](http://arxiv.org/abs/2310.08836) | 本文提出了一个通过观察映射和行为克隆进行少样本策略转移的框架，通过使用生成对抗网络和循环一致性损失来映射源领域和目标领域之间的观察，并利用这个映射来克隆源任务的成功策略。 |
| [^64] | [Optimal Sample Complexity for Average Reward Markov Decision Processes.](http://arxiv.org/abs/2310.08833) | 本论文解决了对于均匀收敛的马尔可夫决策过程的长期平均奖励最大化策略学习的样本复杂度问题，并建立了一个样本复杂度为$\widetilde O(|S||A|t_{\text{mix}}\epsilon^{-2})$的优化策略估计器。 |
| [^65] | [Distance-rank Aware Sequential Reward Learning for Inverse Reinforcement Learning with Sub-optimal Demonstrations.](http://arxiv.org/abs/2310.08823) | 提出了Distance-rank感知顺序奖励学习（DRASRL）框架，旨在解决逆强化学习中轨迹排名模糊和奖励模糊的问题。 |
| [^66] | [Exploring the relationship between response time sequence in scale answering process and severity of insomnia: a machine learning approach.](http://arxiv.org/abs/2310.08817) | 该研究发现量表答题过程中的反应时间与失眠严重程度之间存在关系，并开发了一种机器学习模型，可以根据反应时间数据预测参与者是否存在失眠。 |
| [^67] | [A Nonlinear Method for time series forecasting using VMD-GARCH-LSTM model.](http://arxiv.org/abs/2310.08812) | 该研究提出了一个新的时间序列预测方法，使用VMD-GARCH-LSTM模型来捕捉复杂时间序列中的局部特征和波动性信息，并通过集成各个子模态的预测结果来提高预测准确性。 |
| [^68] | [DDMT: Denoising Diffusion Mask Transformer Models for Multivariate Time Series Anomaly Detection.](http://arxiv.org/abs/2310.08800) | 本论文提出了一种名为DDMT的新框架，用于解决多变量时间序列异常检测中的噪声和弱身份映射问题。通过引入自适应动态邻居掩膜机制(ADNM)以及集成Transformer和去噪扩散模型，该框架可以有效地检测时间序列数据中的异常情况。 |
| [^69] | [Mitigating Bias for Question Answering Models by Tracking Bias Influence.](http://arxiv.org/abs/2310.08795) | 本论文提出了一种名为BMBI的方法来减轻多选问题回答模型的偏见。通过观察一个查询实例对另一个实例的影响，测量查询实例的偏见程度，并将其作为优化目标，形成一个多任务学习设置。同时引入新的偏见评估指标以量化偏见。 |
| [^70] | [Analysis of Weather and Time Features in Machine Learning-aided ERCOT Load Forecasting.](http://arxiv.org/abs/2310.08793) | 本研究开发了机器学习模型，利用时间和天气信息预测短期系统总负荷，发现最大程度利用各种相关特征不一定能达到最佳预测效果。 |
| [^71] | [Incentive Mechanism Design for Distributed Ensemble Learning.](http://arxiv.org/abs/2310.08792) | 本研究针对分布式集成学习提出了激励机制设计。通过指定学习者的训练数据量和奖励金额，解决了自利学习者不愿参与的问题。此外，通过拆解集成准确性为多样性-精确性权衡，指导了机制设计。通过新的快速求解算法，准确估计了每个学习者的贡献。 |
| [^72] | [Price of Stability in Quality-Aware Federated Learning.](http://arxiv.org/abs/2310.08790) | 本论文提出了一种质量感知的联邦学习方案，研究了标签噪声对系统性能的影响，并设计了一种游戏模型来理解客户端的行为。研究结果表明均衡结果的全局模型准确度低于社会最优解，并提出了一种高效算法来计算。 |
| [^73] | [Selectivity Drives Productivity: Efficient Dataset Pruning for Enhanced Transfer Learning.](http://arxiv.org/abs/2310.08782) | 本论文提出了一种解决迁移学习中数据集修剪的方法，通过集成数据集修剪和迁移学习的观点，发现现有的方法不适用于迁移学习范式，并提出了标签映射和特征映射这两种新的数据集修剪方法。 |
| [^74] | [When Machine Learning Models Leak: An Exploration of Synthetic Training Data.](http://arxiv.org/abs/2310.08775) | 本论文研究了针对一个预测人员或家庭是否会在接下来的两年内搬迁的机器学习模型的攻击，攻击者利用模型的预测以及公开的训练数据边际分布来推断目标个体的敏感属性值，同时探讨了用合成数据替代原始数据训练模型对攻击的影响。 |
| [^75] | [PhyloGFN: Phylogenetic inference with generative flow networks.](http://arxiv.org/abs/2310.08774) | PhyloGFN是一种基于生成流网络的系统发育推断方法，通过采样复杂的组合结构，能够产生多样且高质量的进化假设，并在边缘似然估计方面具有竞争力。 |
| [^76] | [Modeling Fission Gas Release at the Mesoscale using Multiscale DenseNet Regression with Attention Mechanism and Inception Blocks.](http://arxiv.org/abs/2310.08767) | 这项研究使用深度学习方法，基于2D核燃料微观结构图像，通过训练具有多尺度回归的神经网络模型来预测中子材料中裂变气的瞬时释放通量，并且取得了很高的预测精度。 |
| [^77] | [Calibrating Likelihoods towards Consistency in Summarization Models.](http://arxiv.org/abs/2310.08764) | 通过校准模型生成的序列的似然性，使其更好地与通过自然语言推理（NLI）模型测量的一致性度量相一致，从而提高摘要模型的一致性和质量。 |
| [^78] | [Stabilizing Subject Transfer in EEG Classification with Divergence Estimation.](http://arxiv.org/abs/2310.08762) | 本研究在EEG分类中使用新的正则化技术减少了在未见测试对象上的性能下降。通过设计正则化惩罚和发散估计算法，我们成功地在模型训练中强制执行了统计关系，并在大量计算实验中验证了我们的方法的有效性。 |
| [^79] | [Question Answering for Electronic Health Records: A Scoping Review of datasets and models.](http://arxiv.org/abs/2310.08759) | 本文对电子健康记录（EHR）中的问题回答进行了范围回顾。与其他医学QA任务不同，EHR QA通过从患者的医疗记录中获取答案。这项研究为现有的EHR QA作品提供了方法论回顾。 |
| [^80] | [Detection and prediction of clopidogrel treatment failures using longitudinal structured electronic health records.](http://arxiv.org/abs/2310.08757) | 本研究提出了使用机器学习算法基于长期结构化电子健康记录，自动检测和预测氯吡格雷治疗失败的方法。研究利用英国生物库的数据构建了模型，通过整理诊断、处方和过程记录来进行预测和检测。 |
| [^81] | [Tokenizer Choice For LLM Training: Negligible or Crucial?.](http://arxiv.org/abs/2310.08754) | 在LLM训练中，分词器的选择对模型的后续性能、成本有着显著影响，常见的分词器评估指标不一定预测模型的性能。 |
| [^82] | [Constrained Bayesian Optimization with Adaptive Active Learning of Unknown Constraints.](http://arxiv.org/abs/2310.08751) | 本文探讨了约束贝叶斯优化的理论和实际问题，提出了一种带有自适应主动学习未知约束的方法，在处理复杂约束场景时具有理论保证。 |
| [^83] | [Search-Adaptor: Text Embedding Customization for Information Retrieval.](http://arxiv.org/abs/2310.08750) | 本文提出了一种名为Search-Adaptor的方法，用于定制化预训练的大型语言模型以改善信息检索和搜索的性能。通过修改文本嵌入，Search-Adaptor在多个真实世界数据集上展现出了稳定且显著的性能提升。 |
| [^84] | [Evolutionary Dynamic Optimization and Machine Learning.](http://arxiv.org/abs/2310.08748) | 进化计算和机器学习的结合为优化复杂的机器学习任务提供了有价值的机会，并通过利用进化计算算法生成的数据来提供对搜索空间和种群动态的洞察。 |
| [^85] | [Robustness to Multi-Modal Environment Uncertainty in MARL using Curriculum Learning.](http://arxiv.org/abs/2310.08746) | 本文首次提出了在多模态环境不确定性中提高MARL稳健性的广义问题，并通过课程学习技术提出了通用的稳健训练方法。 |
| [^86] | [Circuit Component Reuse Across Tasks in Transformer Language Models.](http://arxiv.org/abs/2310.08744) | 这项工作证明了在Transformer语言模型中，电路组件可以在不同任务之间复用并产生相似的功能，为更高级的模型理解做出贡献。 |
| [^87] | [Development and Validation of a Deep Learning-Based Microsatellite Instability Predictor from Prostate Cancer Whole-Slide Images.](http://arxiv.org/abs/2310.08743) | 本研究开发和验证了一种基于深度学习的微卫星不稳定性预测器，可以从前列腺癌全切片图像中预测MSI状态，有助于识别最有可能受益于免疫治疗的患者。 |
| [^88] | [Splicing Up Your Predictions with RNA Contrastive Learning.](http://arxiv.org/abs/2310.08738) | 本研究将对比学习技术扩展到基因组数据，利用选择性剪接和基因复制产生的序列之间的功能相似性，学习到广义RNA同位素表示。我们的预训练策略在RNA半衰期和平均核糖体负载预测等任务上取得了竞争性的结果，在低数据条件下皮尔逊相关性增加了多达两倍。 |
| [^89] | [Provably Robust Cost-Sensitive Learning via Randomized Smoothing.](http://arxiv.org/abs/2310.08732) | 本研究通过随机平滑认证框架，为成本敏感的稳健分类器提供了严格的稳健性保证，并通过优化方案针对不同数据子组设计了细粒度认证半径，取得了优越的性能。 |
| [^90] | [A Simple Way to Incorporate Novelty Detection in World Models.](http://arxiv.org/abs/2310.08731) | 本文提出了一个简单的方法，通过利用世界模型幻觉状态和真实观察状态的不匹配性作为异常分数，将新颖性检测纳入世界模型强化学习代理中。在新环境中与传统方法相比，我们的工作具有优势。 |
| [^91] | [Heterophily-Based Graph Neural Network for Imbalanced Classification.](http://arxiv.org/abs/2310.08725) | 我们提出了一种基于异质性的图神经网络方法用于解决不平衡分类问题，通过考虑图的异质性以及类别不平衡之间的关系，我们提出了一种高效的Fast Im-GBK方法，可以有效地解决不平衡图上的分类问题。 |
| [^92] | [Designing Observables for Measurements with Deep Learning.](http://arxiv.org/abs/2310.08717) | 该研究提出使用机器学习来设计最优的可观测量，通过神经网络输出的展开的微分截面包含了关于感兴趣参数的最多信息，并且可以通过构造方法进行很好的测量。 |
| [^93] | [Transformer Choice Net: A Transformer Neural Network for Choice Prediction.](http://arxiv.org/abs/2310.08716) | 本论文介绍了一种适用于预测多个选择的Transformer神经网络架构，通过考虑顾客和项目的特征，以及上下文信息（如可选项的范围和定制需求），解决了离散选择模型在顾客选择多个项目时的挑战。 |
| [^94] | [Waymax: An Accelerated, Data-Driven Simulator for Large-Scale Autonomous Driving Research.](http://arxiv.org/abs/2310.08710) | Waymax是一种加速、数据驱动的模拟器，用于大规模自主驾驶研究。它使用真实世界驾驶数据来创建真实的多智能体交互场景，并支持大规模、分布式机器学习工作流程。通过学习和硬编码的行为模型，Waymax可以提供逼真的交互体验。 |
| [^95] | [Polynomial Time Cryptanalytic Extraction of Neural Network Models.](http://arxiv.org/abs/2310.08708) | 本文提出了几种新技术，以多项式数量的查询和多项式数量的时间从黑盒实现的ReLU-based深度神经网络中提取出所有实值参数。 |
| [^96] | [ELDEN: Exploration via Local Dependencies.](http://arxiv.org/abs/2310.08702) | 在具有复杂依赖关系的环境中，我们提出了ELDEN，一种新的内在奖励，通过使用代理对实体之间的相互影响的不确定性，鼓励有效地探索状态空间。 |
| [^97] | [Kernel-Elastic Autoencoder for Molecular Design.](http://arxiv.org/abs/2310.08685) | 核-弹性自编码器（KAE）是一种在分子设计领域具有增强性能的自监督生成模型。KAE实现了有效生成和准确重构的挑战，具有显著的多样性和最先进的性能。此外，KAE还可以根据特定条件生成分子，并在分子对接应用中表现出优秀的性能。 |
| [^98] | [Virtual Augmented Reality for Atari Reinforcement Learning.](http://arxiv.org/abs/2310.08683) | 本文研究了使用最先进的图像分割模型提高Atari强化学习智能体性能的方法 |
| [^99] | [GDL-DS: A Benchmark for Geometric Deep Learning under Distribution Shifts.](http://arxiv.org/abs/2310.08677) | GDL-DS是一个基准测试，用于评估几何深度学习模型在具有分布转换的场景中的性能。它包括多个科学领域的评估数据集，并研究了不同级别的超出分布特征的信息访问。 |
| [^100] | [Machine Learning Who to Nudge: Causal vs Predictive Targeting in a Field Experiment on Student Financial Aid Renewal.](http://arxiv.org/abs/2310.08672) | 通过对学生进行因果与预测性定位的比较，研究探讨了在学生助学金续签领域实验中鼓励对象选择的价值。这项大规模实地实验揭示了定位干预对于不同学生的效果，为干预策略的优化提供了参考。 |
| [^101] | [Every Parameter Matters: Ensuring the Convergence of Federated Learning with Dynamic Heterogeneous Models Reduction.](http://arxiv.org/abs/2310.08670) | 这项研究提出了一个统一的异构联邦学习算法框架，并证明了在一定条件下，这些算法对于不同类型的数据都能收敛到标准联邦学习的一个稳定点。此外，研究还揭示了两个关键因素：模型提取噪声和最小覆盖指数，提倡了本地模型选择和全局模型选择的联合设计。 |
| [^102] | [Counting and Algorithmic Generalization with Transformers.](http://arxiv.org/abs/2310.08661) | 这项研究分析了在计数任务中的算法推广，并证明了标准的Transformer的架构决策会阻碍其在超出分布任务上的性能。通过消除问题操作，修改后的Transformer在计数方面展示出了良好的算法推广性能。 |
| [^103] | [Learning RL-Policies for Joint Beamforming Without Exploration: A Batch Constrained Off-Policy Approach.](http://arxiv.org/abs/2310.08660) | 本研究提出了一种批量约束的离策略方法，用于解决联合波束成形的参数优化问题。通过使用深度强化学习技术，克服了传统方法中计算复杂度高和无法准确建模的难题。 |
| [^104] | [LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models.](http://arxiv.org/abs/2310.08659) | 本论文提出了LoftQ：一种针对大型语言模型的LoRA精调感知量化框架。该框架同时对LLM进行量化，并为LoRA精调找到适当的低秩初始化，以缓解量化模型和全精度模型之间的差异，并显著提高了下游任务的泛化能力。 |
| [^105] | [SplitBeam: Effective and Efficient Beamforming in Wi-Fi Networks Through Split Computing.](http://arxiv.org/abs/2310.08656) | SplitBeam是一个新的框架，通过分割计算，在Wi-Fi网络中实现了有效且高效的波束成形。 |
| [^106] | [Analyzing Textual Data for Fatality Classification in Afghanistan's Armed Conflicts: A BERT Approach.](http://arxiv.org/abs/2310.08653) | 这项研究利用BERT模型分析阿富汗武装冲突的文本数据，通过对事件的描述进行分类，判断其是否致命。模型在测试中表现出色。 |
| [^107] | [Electrical Grid Anomaly Detection via Tensor Decomposition.](http://arxiv.org/abs/2310.08650) | 本论文提出了一种通过非负张量分解实现电网异常检测的方法。与之前的降维方法不同，该方法能够更准确地对SCADA系统进行建模，并检测出其中的异常行为。 |
| [^108] | [Time-vectorized numerical integration for systems of ODEs.](http://arxiv.org/abs/2310.08649) | 本文介绍了一种时间向量化的数值积分方法，用于积分刚性常微分方程组，并通过伴随方法计算参数梯度。该方法在独立时间序列和连续时间步骤的批次上进行向量化，提供了更高的计算带宽，能够充分利用现代GPU并实现超过100倍的加速。 |
| [^109] | [Defect Analysis of 3D Printed Cylinder Object Using Transfer Learning Approaches.](http://arxiv.org/abs/2310.08645) | 本研究通过使用迁移学习模型分析了3D打印圆柱体中的缺陷，结果发现MobileNetV2在分类性能方面表现最好。 |
| [^110] | [A Mass-Conserving-Perceptron for Machine Learning-Based Modeling of Geoscientific Systems.](http://arxiv.org/abs/2310.08644) | 这篇论文提出了一种质量保持感知器（MCP）用于将物理-概念模型和机器学习模型结合起来建模地球科学系统，通过利用机器学习技术从数据中学习物理过程的功能性和质量保持性。 |
| [^111] | [Divorce Prediction with Machine Learning: Insights and LIME Interpretability.](http://arxiv.org/abs/2310.08620) | 通过评估名为“离婚预测数据集”的数据集，使用六种不同的机器学习算法，成功准确分类婚姻和离婚人群，具有98.5%的准确率。 |
| [^112] | [Safe Deep Policy Adaptation.](http://arxiv.org/abs/2310.08602) | 该论文提出了SafeDPA，一种新颖的强化学习和控制框架，用于同时解决策略适应和安全强化学习的问题。SafeDPA在仿真环境中联合学习自适应策略和动力学模型，并使用少量真实数据进行微调。在真实世界部署过程中，通过引入基于控制屏障函数的安全过滤器，确保了SafeDPA的安全性。 |
| [^113] | [Unit Commitment Predictor With a Performance Guarantee: A Support Vector Machine Classifier.](http://arxiv.org/abs/2310.08601) | 本文提出了一个带有性能保证的机组启停预测器，通过学习和预测常规机组的启停决策，系统运营商可以在求解器中使用预热启动并显著加速计算。对于预测，使用了适当正则化的核化支持向量机分类器，能将计算时间减少1.7倍。 |
| [^114] | [Deep Reinforcement Learning for Autonomous Vehicle Intersection Navigation.](http://arxiv.org/abs/2310.08595) | 本研究通过使用基于TD3算法的单智能体方法，在CARLA模拟平台中展示了在复杂T型路口导航中稳定收敛和改进安全性能，并在行程延误、碰撞减少和总体成本等方面优于先前的方法。 |
| [^115] | [Can We Edit Multimodal Large Language Models?.](http://arxiv.org/abs/2310.08475) | 本文提出了编辑多模式大型语言模型（MLLMs）的挑战，并构建了一个新的基准用于评估和比较不同编辑方法的效果。实验结果表明，编辑多模式LLMs仍然存在困难，但这项工作为NLP社区提供了宝贵的见解。 |
| [^116] | [Towards Causal Deep Learning for Vulnerability Detection.](http://arxiv.org/abs/2310.07958) | 本文提出了一种针对漏洞检测的因果深度学习方法CausalVul，通过引入因果性，并设计新的扰动，解决了深度学习漏洞检测中模型不稳定和泛化性能差的问题。 |
| [^117] | [Generative Modeling with Phase Stochastic Bridges.](http://arxiv.org/abs/2310.07805) | 通过在相位空间中构建路径测度，我们提出了一种新颖的生成建模框架，可以在动力传播的早期阶段生成逼真的数据点，并利用额外的速度信息实现高效的数据生成。 |
| [^118] | [Linear Latent World Models in Simple Transformers: A Case Study on Othello-GPT.](http://arxiv.org/abs/2310.07582) | 本研究通过案例研究奥赛罗-GPT，发现其具有线性表示对立棋子的世界模型，并揭示了线性世界表示和因果决策之间的相互作用及其与层深度和模型复杂度的依赖关系。 |
| [^119] | [On the Impact of Cross-Domain Data on German Language Models.](http://arxiv.org/abs/2310.07321) | 本研究通过对德语语言模型进行实验，发现将数据多样性置于数据质量之上的交叉领域数据集训练方法，可以显著提高模型的性能，并超过了之前的最先进模型。 |
| [^120] | [State of the Art on Diffusion Models for Visual Computing.](http://arxiv.org/abs/2310.07204) | 这篇论文旨在介绍最新的视觉计算领域中扩散模型的发展和应用，涵盖了生成人工智能的核心概念和实现细节，并总结了个人化、条件约束和反演等重要方面。 |
| [^121] | [Federated Generalization via Information-Theoretic Distribution Diversification.](http://arxiv.org/abs/2310.07171) | 该论文研究了联邦学习中泛化能力的挑战，特别关注训练分布和测试分布的不匹配。提出了一种信息论的泛化方法来解决这个问题。 |
| [^122] | [Prompt-augmented Temporal Point Process for Streaming Event Sequence.](http://arxiv.org/abs/2310.04993) | 提出了一种基于提示增强的时态点过程（PromptTPP）方法，用于解决流式事件序列学习的挑战。 |
| [^123] | [Label-free Node Classification on Graphs with Large Language Models (LLMS).](http://arxiv.org/abs/2310.04668) | 本文介绍了一种使用大型语言模型（LLMs）对图中节点进行无标签分类的方法，即LLM-GNN。它利用LLMs对一小部分节点进行注释，然后通过对LLMs的注释进行训练，使得GNN能够对其余大部分节点进行预测。这种方法充分发挥了GNNs和LLMs的优势，同时解决了它们在处理结构化数据方面的限制。 |
| [^124] | [Robust Multimodal Learning with Missing Modalities via Parameter-Efficient Adaptation.](http://arxiv.org/abs/2310.03986) | 通过低秩适应和中间特征的调制，我们提出了针对预训练多模态网络的参数高效适应程序，以实现对缺失模态的鲁棒性，并在某些情况下胜过独立的专门网络。 |
| [^125] | [SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks.](http://arxiv.org/abs/2310.03684) | SmoothLLM是第一个用于减轻大型语言模型上越狱攻击的算法，通过在输入提示上随机扰动并汇总预测结果来检测对抗性输入，将攻击成功率降低至不到一个百分点，并提供了可证明的保证。 |
| [^126] | [BioBridge: Bridging Biomedical Foundation Models via Knowledge Graph.](http://arxiv.org/abs/2310.03320) | BioBridge是一种通过知识图谱桥接单模态生物医学基础模型的参数高效学习框架。实验证明，BioBridge在跨模态检索任务中胜过最佳基线KG嵌入方法，具有泛化能力。 |
| [^127] | [LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving.](http://arxiv.org/abs/2310.03026) | 本文研究将大型语言模型（LLMs）作为复杂自动驾驶场景的决策组件，通过认知路径和算法来实现全面推理和可执行驾驶指令的转化。实验证明，LLMs能够在单车任务和复杂驾驶行为中表现出优越性能，这是因为其具有常识推理能力。 |
| [^128] | [H-InDex: Visual Reinforcement Learning with Hand-Informed Representations for Dexterous Manipulation.](http://arxiv.org/abs/2310.01404) | 这项工作提出了一种基于人体手部信息的视觉表示学习框架H-InDex，通过强化学习解决困难的巧妙操纵任务。实证研究表明，H-InDex明显优于强基线方法和最近的视觉基础模型。 |
| [^129] | [Dynamic DAG Discovery for Interpretable Imitation Learning.](http://arxiv.org/abs/2310.00489) | 提出了一种用于解释模仿学习中神经代理的动态DAG发现方法，通过有向无环因果图展现其捕获的知识，以增加透明度和可解释性。 |
| [^130] | [Model-Free, Regret-Optimal Best Policy Identification in Online CMDPs.](http://arxiv.org/abs/2309.15395) | 本文提出了一种无模型的算法，名为PRI，用于在线CMDPs中的最佳策略识别问题。该算法基于CMDPs的有限随机性属性，能够以低遗憾并以高概率识别出最优策略。 |
| [^131] | [Guided Online Distillation: Promoting Safe Reinforcement Learning by Offline Demonstration.](http://arxiv.org/abs/2309.09408) | 本研究提出了Guided Online Distillation (GOLD)方法，通过从离线演示数据中提取专家策略来引导在线探索，解决了安全强化学习中保守性问题，并通过决策转换器模型对离线策略学习进行有效的高容量建模。 |
| [^132] | [Answering Layer 3 queries with DiscoSCMs.](http://arxiv.org/abs/2309.09323) | 本文介绍了DiscoSCMs，一种用于解决因果查询的模型。它通过扩展结构因果模型和潜在结果框架来解决一致性规则引发的退化问题，并在分析个性化激励场景中的潜在结果时展示了其有效性。通过引入独立潜在噪声条件，可以提高解决Layer 3查询的准确性和可解释性。 |
| [^133] | [Pure Monte Carlo Counterfactual Regret Minimization.](http://arxiv.org/abs/2309.03084) | 纯蒙特卡洛反事实遗憾最小化算法（PCFR）是一种结合了反事实遗憾最小化（CFR）和虚拟游戏（FP）概念的新算法，能够与各种CFR变体相结合，包括蒙特卡洛CFR（MCCFR）。PCFR具有更好的性能和较快的收敛速度，同时降低了时间和空间复杂度。 |
| [^134] | [Theoretical Explanation of Activation Sparsity through Flat Minima and Adversarial Robustness.](http://arxiv.org/abs/2309.03004) | 提出了一个理论解释，将梯度稀疏性和激活稀疏性解释为对抗性鲁棒性的必要步骤，以隐藏特征和参数而言，这大致等于对学习良好模型的极小值平坦性。 |
| [^135] | [FRGNN: Mitigating the Impact of Distribution Shift on Graph Neural Networks via Test-Time Feature Reconstruction.](http://arxiv.org/abs/2308.09259) | FRGNN是一个通用框架，通过测试时间特征重构，减轻分布偏移对图神经网络的影响，不需要重新训练模型，保留了原始特征的关键信息来改善性能。 |
| [^136] | [Locally Adaptive and Differentiable Regression.](http://arxiv.org/abs/2308.07418) | 本文提出了一种本地自适应可微回归模型，通过对局部学习模型进行加权平均，在不同本地区域处理数据时具有竞争力，并在理论上实现更快的统计收敛以及在实际应用中改善了性能。 |
| [^137] | [MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities.](http://arxiv.org/abs/2308.02490) | MM-Vet是一个评估标准，用于评估大型多模态模型在复杂任务上的综合能力。该标准解决了如何结构化和评估复杂多模态任务、设计适用于不同问题和回答类型的评估指标以及如何提供模型洞察的问题。通过整合不同的核心视觉-语言能力，MM-Vet展示了有趣的能力和解决复杂任务的方法。 |
| [^138] | [Local-Global Temporal Fusion Network with an Attention Mechanism for Multiple and Multiclass Arrhythmia Classification.](http://arxiv.org/abs/2308.02416) | 本文提出了一个本地-全局时间融合网络结合注意机制的框架，用于针对心律失常的检测和分类任务。该方法通过提取本地时间信息和全局模式，并使用注意力机制进行本地-全局信息融合，以处理长度变化的心律失常数据。实验结果表明，此方法在心律失常分类任务上的性能优于现有方法。 |
| [^139] | [AutoML4ETC: Automated Neural Architecture Search for Real-World Encrypted Traffic Classification.](http://arxiv.org/abs/2308.02182) | AutoML4ETC是一个自动设计高效且高性能神经架构的工具，用于加密流量分类。其通过定义新颖的搜索空间和使用不同的搜索策略，在多个数据集上优于当前最先进的加密流量分类器。 |
| [^140] | [Explainable Equivariant Neural Networks for Particle Physics: PELICAN.](http://arxiv.org/abs/2307.16506) | PELICAN是一种可解释的等变神经网络，应用于粒子物理问题中。相比于其他方法，PELICAN的优势在于它采用了基于对称群的架构，具有降低复杂性、增加可解释性和提高性能的特点。它在标记和重构动量增强的顶夸克，并在密集环境中特别识别和测量W玻色子，以及识别不同类型的喷注等任务方面展示了出色的表现。 |
| [^141] | [On the learning Dynamics of Attention Networks.](http://arxiv.org/abs/2307.13421) | 本研究分析了软注意力、硬注意力和潜变量边际似然（LVML）注意力三种注意力模型的学习动态，发现了它们在所选择的片段聚合方式上的显著差异，并解释了分类模型在梯度下降下的演化对最终结果的影响。 |
| [^142] | [Graph Neural Networks based Log Anomaly Detection and Explanation.](http://arxiv.org/abs/2307.00527) | 提出了一种基于图神经网络的无监督日志异常检测方法，该方法将事件日志转换为带属性、有向和加权的图，并利用图神经网络进行图级别的异常检测。引入了一种新的图神经网络模型OCDiGCN来检测一组带属性、有向和加权的图中的图级别异常，并提供对异常的解释能力。 |
| [^143] | [Provable Robust Watermarking for AI-Generated Text.](http://arxiv.org/abs/2306.17439) | GPTWatermark是一种针对性模型水印技术，通过固定分组设计和强大的可证明保证，提供了对AI生成文本的鲁棒性检测和安全性防御。实验证明了其在检测准确性和生成质量方面的优越性，推动了LLMs负责任使用的进步。 |
| [^144] | [End-to-end Reinforcement Learning for Online Coverage Path Planning in Unknown Environments.](http://arxiv.org/abs/2306.16978) | 本文提出了一种基于端到端强化学习的在线覆盖路径规划方法，能处理未知环境并结合全局地图和局部感知输入，同时考虑长期路径规划和短期障碍物检测。 |
| [^145] | [SIMF: Semantics-aware Interactive Motion Forecasting for Autonomous Driving.](http://arxiv.org/abs/2306.14941) | 本文提出了一种名为SIMF的方法，用于自动驾驶车辆中语义感知的交互式运动预测。该方法通过实现基于语义的行为体选择和注意力机制提取全局编码，能够捕捉空间信息和语义信息，并优选相关的行为体进行运动预测。 |
| [^146] | [OpenDataVal: a Unified Benchmark for Data Valuation.](http://arxiv.org/abs/2306.10577) | 本文介绍了一种名为OpenDataVal的基准测试框架，该框架整合了多种数据集和九种最先进的数据估值算法实现，并提供了四个下游机器学习任务来评估数据价值的质量。 |
| [^147] | [A Universal Semantic-Geometric Representation for Robotic Manipulation.](http://arxiv.org/abs/2306.10474) | 这篇论文提出了一种通用的机器人感知模块，称为语义几何表示（SGR），该模块结合了大规模预训练的2D模型的丰富语义信息和3D空间推理的优势，能够在各种模拟和真实世界的机器人操纵任务中胜过最先进的方法。 |
| [^148] | [Multi-View Class Incremental Learning.](http://arxiv.org/abs/2306.09675) | 本文提出了一种名为多视角分类增量学习（MVCIL）的新模型，该模型使用随机化的表示学习技术进行特征提取，并提出正交融合子空间和选择性权重合并来解决增量学习中遗忘旧信息和学习新概念的挑战。实验结果表明该方法相比最新方法有效性更高。 |
| [^149] | [Unprocessing Seven Years of Algorithmic Fairness.](http://arxiv.org/abs/2306.07261) | 该论文取消了算法公平性中的后处理方法，并发现后处理实现的公平性-准确性Pareto边界包含了可评估的所有其他方法。 |
| [^150] | [Time Series Continuous Modeling for Imputation and Forecasting with Implicit Neural Representations.](http://arxiv.org/abs/2306.05880) | 该论文提出了基于INR的时间序列连续建模方法，解决了处理缺失数据、不规则采样和多传感器不对准观测等重复建模问题，并在预测和插值任务中取得了最新的性能表现，具有很好的泛化能力。 |
| [^151] | [Online Learning under Adversarial Nonlinear Constraints.](http://arxiv.org/abs/2306.03655) | 提出了一种在线学习算法CVV-Pro，可以处理对抗性的时变和非线性约束，只依赖于局部稀疏线性逼近，达到了$\sqrt{T}$遗憾率和$1/\sqrt{T}$的收敛速度。 |
| [^152] | [Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic.](http://arxiv.org/abs/2306.02865) | 该论文提出了 BEE 操作符，通过充分利用过去的成功经验，并保持探索乐观性，解决了离线策略演员-评论家中 Q 值高估与低估问题，提高了策略学习和样本效率。 |
| [^153] | [Bottleneck Structure in Learned Features: Low-Dimension vs Regularity Tradeoff.](http://arxiv.org/abs/2305.19008) | 本研究揭示了深度学习神经网路学习输入低维度表示和最小化特征映射中的复杂性/不规则性之间的权衡，控制了规律性，并利用理论工具证明了瓶颈结构的存在。 |
| [^154] | [Cross-Domain Policy Adaptation via Value-Guided Data Filtering.](http://arxiv.org/abs/2305.17625) | 通过价值目标的一致性，我们提出了一种价值导向的数据过滤算法(VGDF)，用于解决在不同领域之间进行策略适应的问题。 |
| [^155] | [Generating Images with Multimodal Language Models.](http://arxiv.org/abs/2305.17216) | 该论文提出了一种方法，将大型语言模型与预训练的图像编码器和解码器模型进行融合，能生成具有连贯性的图像输出，同时也能进行图像检索和多模态对话。 |
| [^156] | [A Mechanism for Solving Relational Tasks in Transformer Language Models.](http://arxiv.org/abs/2305.16130) | 这篇论文研究了在Transformer语言模型中解决关系任务的机制，并发现这些模型利用简单的线性更新来处理关系任务，并以内容无关的方式促进关系的输出。 |
| [^157] | [Counterfactual Generative Models for Time-Varying Treatments.](http://arxiv.org/abs/2305.15742) | 本文研究了时间变量处理情况下的反事实生成模型，能够捕捉整个反事实分布，并且能够有效推断反事实分布的某些统计量，适用于医疗保健和公共政策制定领域。 |
| [^158] | [The student becomes the master: Matching GPT3 on Scientific Factual Error Correction.](http://arxiv.org/abs/2305.14707) | 本文提出了一种不需要验证者且不做领域假设的主张校正系统，能够显著提高科学事实错误校正任务的性能，并通过使用LLM的提示方法和主张感知的解码过程来提高校正质量。 |
| [^159] | [Newton-Cotes Graph Neural Networks: On the Time Evolution of Dynamic Systems.](http://arxiv.org/abs/2305.14642) | 本文提出了基于Newton-Cotes公式的方法来预测动态系统的时间演化，与现有最先进的方法相比较， 实验结果表明该方法有着显著的改进。 |
| [^160] | [Learning Semantic Role Labeling from Compatible Label Sequences.](http://arxiv.org/abs/2305.14600) | 该论文探讨了如何从不相交的兼容标签序列中高效地学习，将此运用于语义角色标注任务，提出了联合处理VerbNet和PropBank标签的方法，并验证了其有效性。 |
| [^161] | [Training Neural Networks without Backpropagation: A Deeper Dive into the Likelihood Ratio Method.](http://arxiv.org/abs/2305.08960) | 提出一种新的似然比方法来训练神经网络，无需使用递归梯度计算，并在多种神经网络架构上有效地减少了对抗性攻击对模型造成的影响。 |
| [^162] | [DOCTOR: A Multi-Disease Detection Continual Learning Framework Based on Wearable Medical Sensors.](http://arxiv.org/abs/2305.05738) | DOCTOR是一种基于可穿戴医疗传感器的多疾病检测持续学习框架，采用了多头深度神经网络和Exemplar-replay风格的CL算法。它可以不断地学习新任务，并在内存使用、电池消耗和检测复杂度方面优于传统的ML驱动疾病检测方法。 |
| [^163] | [Generative modeling of living cells with SO(3)-equivariant implicit neural representations.](http://arxiv.org/abs/2304.08960) | 本文提出了使用有符号距离函数作为形状表示，通过神经网络计算所得，对旋转具有等变性，来生成逼真的活细胞模型，为生物医学成像中的数据驱动细胞跟踪和分割方法提供高质量训练数据集。 |
| [^164] | [The expressive power of pooling in Graph Neural Networks.](http://arxiv.org/abs/2304.01575) | 本文研究了池化算子在图神经网络中的表达能力，并提供了一个通用标准来选择或设计池化算子。 |
| [^165] | [Topological Reconstruction of Particle Physics Processes using Graph Neural Networks.](http://arxiv.org/abs/2303.13937) | Topograph是一种利用图神经网络和粒子衰变自然规律的拓扑结构重建方法，不仅解决了观测到的末态对象组合指派问题，还预测了中间粒子的性质及其后续衰变，比标准方法效果更好，与现代机器学习技术表现相当。 |
| [^166] | [A New Policy Iteration Algorithm For Reinforcement Learning in Zero-Sum Markov Games.](http://arxiv.org/abs/2303.09716) | 本文提出了一种适用于零和马尔可夫博弈的简单但有效的策略迭代算法。 |
| [^167] | [From Images to Features: Unbiased Morphology Classification via Variational Auto-Encoders and Domain Adaptation.](http://arxiv.org/abs/2303.08627) | 本研究提出了一种无偏态的星系形态分类方法，利用变分自编码器和域适应实现低维度表示，40维潜在变量能够有效再现星系图像中的大多数形态特征，并通过经典随机森林分类器实现了详细的形态特征分类。此外，该方法可以无偏地应用于两个不同的星系调查中。 |
| [^168] | [Kernel Density Bayesian Inverse Reinforcement Learning.](http://arxiv.org/abs/2303.06827) | KD-BIRL是一种核密度贝叶斯逆强化学习方法，通过直接逼近似然函数来学习代理的奖励函数，克服了学习点估计的缺点，并适用于复杂和无限环境。 |
| [^169] | [DP-Fast MH: Private, Fast, and Accurate Metropolis-Hastings for Large-Scale Bayesian Inference.](http://arxiv.org/abs/2303.06171) | 本文提出了一种新的DP-Fast MH算法，用于大规模贝叶斯推断，具有精确、快速和隐私保护的特点。 |
| [^170] | [Single-Cell Multimodal Prediction via Transformers.](http://arxiv.org/abs/2303.00233) | 本研究研究了如何利用Transformer模型在端到端的方式上处理多模态单细胞数据，并利用下游任务信息，提出了一个名为scMoFormer的框架 |
| [^171] | [Statistical Learning under Heterogenous Distribution Shift.](http://arxiv.org/abs/2302.13934) | 本文研究了异质分布偏移下的统计学习问题，通过研究经验风险最小化(ERM)在不同类别的复杂性下的表现，我们发现当类别$F$相比类别$G$更“简单”时，我们的预测器对于协变量偏移具有更强的鲁棒性，尤其在$\textbf{y}$的偏移远小于$\textbf{x}$的情况下。同时，我们发现ERM的行为与正交机器学习具有类似的特性。 |
| [^172] | [Learning Graph ARMA Processes from Time-Vertex Spectra.](http://arxiv.org/abs/2302.06887) | 本研究提出了一种基于学习过程谱密度的算法，用于推断缺失的信号值和进行信号插值，实验结果显示其在时间-顶点信号估计问题中具有高准确性。 |
| [^173] | [Knowledge is a Region in Weight Space for Fine-tuned Language Models.](http://arxiv.org/abs/2302.04863) | 研究探讨了不同模型在权重空间中的位置与性能的关联，发现微调语言模型在权重空间中有明确定义的区域，且这些区域中的模型表现出高性能。此外，通过绕过这些区域，可以得到性能相当甚至更好的新模型。 |
| [^174] | [Machine Learning for Synthetic Data Generation: A Review.](http://arxiv.org/abs/2302.04062) | 机器学习用于合成数据生成的综述，探讨了合成数据生成的应用（计算机视觉、语音、自然语言、医疗保健和商业）、机器学习方法（神经网络架构和深度生成模型）以及隐私和公平问题，并提出了未来的研究方向。 |
| [^175] | [Self-supervised Learning for Segmentation and Quantification of Dopamine Neurons in Parkinson's Disease.](http://arxiv.org/abs/2301.08141) | 这项研究介绍了一种自监督学习方法，用于数字病理学图像中多巴胺神经元的分割和计量。这种方法可以减少人工操作的主观性和时间消耗，提供可靠和无偏见的自动化系统。 |
| [^176] | [A Policy Optimization Method Towards Optimal-time Stability.](http://arxiv.org/abs/2301.00521) | 本文提出了一种面向最优时间稳定性的策略优化方法，将基于采样的李雅普诺夫稳定性与Actor-Critic框架相结合，发展出了自适应李雅普诺夫Actor-Critic（ALAC）算法。通过对十个机器人任务的评估，该方法在引导系统生成稳定模式方面表现优异。 |
| [^177] | [Learning from Guided Play: Improving Exploration for Adversarial Imitation Learning with Simple Auxiliary Tasks.](http://arxiv.org/abs/2301.00051) | 本研究提出了从引导式游戏学习（LfGP）的框架，通过引入多个辅助任务和主任务的专家演示，提升对抗性模仿学习（AIL）中的探索能力，并解决了传统AIL在学习操作任务时可能陷入次优解的问题。 |
| [^178] | [Dataless Knowledge Fusion by Merging Weights of Language Models.](http://arxiv.org/abs/2212.09849) | 本文提出了一种无数据知识融合方法，可以合并在不同训练数据集上建立的单个模型，以得到一个在所有数据集领域上表现良好且可以推广到域外数据的单一模型。 |
| [^179] | [Visual Dexterity: In-hand Dexterous Manipulation from Depth.](http://arxiv.org/abs/2211.11744) | 通过使用深度相机的读数，我们提出了一种通用物体重新定向控制器，可以实时、动态地重新定向复杂和新颖的物体形状，中位数重新定向时间接近于七秒。该控制器经过强化学习在仿真环境中训练，并在实际世界中对未用于训练的新物体形状进行了评估。 |
| [^180] | [ViNL: Visual Navigation and Locomotion Over Obstacles.](http://arxiv.org/abs/2210.14791) | ViNL是通过视觉导航和足球术在未知室内环境中实现机器人导航和足球术运动的方法。它包括无模型的视觉导航策略和视觉运动策略，通过端到端训练实现，并能够避免踩到小障碍物。ViNL能够实现高效且稳定的导航和足球术运动，无需环境先验知识。 |
| [^181] | [A Generalist Framework for Panoptic Segmentation of Images and Videos.](http://arxiv.org/abs/2210.06366) | 这个论文提出了一个通用框架，用于图像和视频的全景分割。他们将全景分割问题定义为离散数据生成问题，并提出了一个简单的扩散模型来建模全景掩码。他们的方法能够在流式设置中建模视频，并自动学习跟踪对象实例，并在实验中展现出与最先进的专家方法竞争的能力。 |
| [^182] | [A Logic for Expressing Log-Precision Transformers.](http://arxiv.org/abs/2210.02671) | 本研究分析了一种对数精度transformer，证明了任何对数精度transformer都可以等效地表示为一阶逻辑句子，扩展了对transformer语言模型的解释。 |
| [^183] | [SpeedLimit: Neural Architecture Search for Quantized Transformer Models.](http://arxiv.org/abs/2209.12127) | 本文介绍了SpeedLimit——一种新的神经架构搜索技术，通过在量化的Transformer模型中添加上限延迟约束，优化准确性。该方法比当前最先进的技术表现更好，为在延迟敏感的环境中使用Transformer模型提供了新的可能性。 |
| [^184] | [DataPerf: Benchmarks for Data-Centric AI Development.](http://arxiv.org/abs/2207.10062) | DataPerf是一个由社区主导的基准测试套件，旨在通过竞争、可比性和可重复性促进数据中心人工智能的创新。 |
| [^185] | [Learning Counterfactually Invariant Predictors.](http://arxiv.org/abs/2207.09768) | 通过提出图形标准和模型无关框架CIP，我们能够学习反事实不变的预测器，以实现在现实世界中的公平性、强健性和普适性。 |
| [^186] | [Hidden Parameter Recurrent State Space Models For Changing Dynamics Scenarios.](http://arxiv.org/abs/2206.14697) | 本论文提出了隐性参数的递归状态空间模型(HiP-RSSMs)，用于建模在真实世界中常见但动力学不完全相同的任务。该模型通过将一系列相关动力系统参数化为低维潜在因子，实现了简单而有效的学习和推断方法。实验证明，HiP-RSSMs在多个机器人基准测试中表现出色，比RSSMs和其他多任务模型更具优势。 |
| [^187] | [ZSON: Zero-Shot Object-Goal Navigation using Multimodal Goal Embeddings.](http://arxiv.org/abs/2206.12403) | 本文提出了一种使用多模式目标嵌入进行零样本目标导航的方法，通过在未标注的3D环境中训练语义目标导航代理，将目标图片编码成多模式的语义嵌入，实现了在开放世界中找到物体的能力。 |
| [^188] | [Structured Prediction Problem Archive.](http://arxiv.org/abs/2202.03574) | 该论文介绍了一个结构化预测问题的存档，集中收集了各种问题类别的数据集，并提供对问题的描述、格式和特性进行总结，以及列举了相关算法。该存档旨在方便进行基准测试和与已有工作的比较，并欢迎提交新的数据集和算法。 |
| [^189] | [DIT4BEARs Smart Roads Internship.](http://arxiv.org/abs/2107.06755) | 本研究实习项目旨在通过开发深度学习模型来改善北极地区的道路维护。实习团队成功地开发出一个天气预报应用程序，该应用程序可以准确预测道路的状态，从而提高道路安全性。 |
| [^190] | [Revisiting minimum description length complexity in overparameterized models.](http://arxiv.org/abs/2006.10189) | 本文重审了超参数模型中的最小描述长度复杂度。通过定义一个新的基于MDL的复杂度度量，我们发现复杂度不仅取决于参数数量，还与设计矩阵或核矩阵的奇异值和信噪比有关。 |

# 详细

[^1]: 解耦潜在空间促进数据驱动的辅助学习

    Disentangled Latent Spaces Facilitate Data-Driven Auxiliary Learning. (arXiv:2310.09278v1 [cs.LG])

    [http://arxiv.org/abs/2310.09278](http://arxiv.org/abs/2310.09278)

    本论文提出了一个新的框架，通过解耦过程来发现可以与主任务一起利用的不相关的分类任务和相关标签，从而在深度学习中促进辅助学习。

    

    在深度学习中，辅助目标常常被用来在数据稀缺或者主要任务非常复杂的情况下促进学习。这个想法主要受到同时解决多个任务带来的改进泛化能力的启发，从而产生更强大的共享表示。然而，找到能产生期望改进的最优辅助任务是一个关键问题，通常需要手动设计的技巧或者昂贵的元学习方法。本文提出了一个新颖的框架，称为Detaux，通过弱监督的解耦过程在任何多任务学习（MTL）模型中发现可以与主要任务一起利用的不相关的分类任务和相关标签。解耦过程在表示层面工作，将与主要任务相关的一个子空间与任意数量的正交子空间分离开来。

    In deep learning, auxiliary objectives are often used to facilitate learning in situations where data is scarce, or the principal task is extremely complex. This idea is primarily inspired by the improved generalization capability induced by solving multiple tasks simultaneously, which leads to a more robust shared representation. Nevertheless, finding optimal auxiliary tasks that give rise to the desired improvement is a crucial problem that often requires hand-crafted solutions or expensive meta-learning approaches. In this paper, we propose a novel framework, dubbed Detaux, whereby a weakly supervised disentanglement procedure is used to discover new unrelated classification tasks and the associated labels that can be exploited with the principal task in any Multi-Task Learning (MTL) model. The disentanglement procedure works at a representation level, isolating a subspace related to the principal task, plus an arbitrary number of orthogonal subspaces. In the most disentangled subsp
    
[^2]: 一种用于抑郁症分类的混合方法：基于运动活动信号的随机森林-神经网络集成

    A Hybrid Approach for Depression Classification: Random Forest-ANN Ensemble on Motor Activity Signals. (arXiv:2310.09277v1 [cs.LG])

    [http://arxiv.org/abs/2310.09277](http://arxiv.org/abs/2310.09277)

    这篇论文介绍了一种名为混合随机森林-神经网络的新算法，该算法通过评估抑郁患者的传感器数据来分类抑郁症，准确率达到80%。

    

    鉴于当今社会患有心理健康疾病的人数不断增加，心理健康的重要性不言而喻。可穿戴传感器，这些传感器越来越普遍，提供了一种追踪和理解心理健康问题的潜在途径。这些设备不仅监测日常活动，还持续记录诸如心率等生命体征，可能提供有关一个人心理状态的信息。最近的研究使用这些传感器与机器学习方法结合，识别与不同心理健康状况相关的模式，突显了这些数据在简单活动监测以外的巨大潜力。在该研究中，我们提出了一种新颖的算法，称为混合随机森林 - 神经网络，该算法针对抑郁患者的传感器数据进行评估。我们的方法在一个特殊数据集上进行评估时，在包括单相和双相抑郁患者的情况下达到了80%的显著准确率。

    Regarding the rising number of people suffering from mental health illnesses in today's society, the importance of mental health cannot be overstated. Wearable sensors, which are increasingly widely available, provide a potential way to track and comprehend mental health issues. These gadgets not only monitor everyday activities but also continuously record vital signs like heart rate, perhaps providing information on a person's mental state. Recent research has used these sensors in conjunction with machine learning methods to identify patterns relating to different mental health conditions, highlighting the immense potential of this data beyond simple activity monitoring. In this research, we present a novel algorithm called the Hybrid Random forest - Neural network that has been tailored to evaluate sensor data from depressed patients. Our method has a noteworthy accuracy of 80\% when evaluated on a special dataset that included both unipolar and bipolar depressive patients as well 
    
[^3]: Retro-fallback: 面向不确定世界的逆合成规划

    Retro-fallback: retrosynthetic planning in an uncertain world. (arXiv:2310.09270v1 [cs.AI])

    [http://arxiv.org/abs/2310.09270](http://arxiv.org/abs/2310.09270)

    本文针对逆合成任务在实验室执行可行性的不确定性问题，通过引入随机过程的表述，提出了一种名为 Retro-fallback 的贪婪算法，该算法能够最大化实验室可执行的合成计划的概率。

    

    逆合成是通过提出一系列化学反应从更简单、可购买的分子创建所需分子的任务。虽然先前的研究提出了一些算法来寻找一系列度量指标（例如最短路径、最低成本）的最优解，但这些研究通常忽视了我们对可能反应空间的不完全了解，这意味着算法生成的计划可能在实验室中无法实施。在本文中，我们提出了一种基于随机过程的逆合成新颖表述，以考虑这种不确定性。然后，我们提出了一种新颖的贪婪算法称为 Retro-fallback，最大化至少有一种合成计划能在实验室中执行的概率。使用仿真基准测试，我们证明 Retro-fallback 通常生成比流行的 MCTS 和 retro* 算法更好的一组合成计划。

    Retrosynthesis is the task of proposing a series of chemical reactions to create a desired molecule from simpler, buyable molecules. While previous works have proposed algorithms to find optimal solutions for a range of metrics (e.g. shortest, lowest-cost), these works generally overlook the fact that we have imperfect knowledge of the space of possible reactions, meaning plans created by the algorithm may not work in a laboratory. In this paper we propose a novel formulation of retrosynthesis in terms of stochastic processes to account for this uncertainty. We then propose a novel greedy algorithm called retro-fallback which maximizes the probability that at least one synthesis plan can be executed in the lab. Using in-silico benchmarks we demonstrate that retro-fallback generally produces better sets of synthesis plans than the popular MCTS and retro* algorithms.
    
[^4]: 遗传算法是分子生成的强大基准线

    Genetic algorithms are strong baselines for molecule generation. (arXiv:2310.09267v1 [cs.NE])

    [http://arxiv.org/abs/2310.09267](http://arxiv.org/abs/2310.09267)

    本文研究表明，遗传算法是分子生成领域的强大基准线，并提出了GA准则，要求新的算法必须明显优于遗传算法。

    

    生成分子，无论是有向还是无向的方式，是药物研发流程中的重要部分。遗传算法（GAs）通过随机修改已知分子来生成分子。本文通过实验证明，对于这种任务，GAs是非常强大的算法，胜过许多复杂的机器学习方法，这一结果可能让很多研究人员感到惊讶。因此，我们建议在同行评审过程中坚持要求新的算法必须明显优于GAs，我们将其称为GA准则。最终，我们的工作表明，许多关于分子生成的研究应该重新评估。

    Generating molecules, both in a directed and undirected fashion, is a huge part of the drug discovery pipeline. Genetic algorithms (GAs) generate molecules by randomly modifying known molecules. In this paper we show that GAs are very strong algorithms for such tasks, outperforming many complicated machine learning methods: a result which many researchers may find surprising. We therefore propose insisting during peer review that new algorithms must have some clear advantage over GAs, which we call the GA criterion. Ultimately our work suggests that a lot of research in molecule generation should be re-assessed.
    
[^5]: 大型语言模型上的用户推理攻击

    User Inference Attacks on Large Language Models. (arXiv:2310.09266v1 [cs.CR])

    [http://arxiv.org/abs/2310.09266](http://arxiv.org/abs/2310.09266)

    本论文研究了在大型语言模型上的用户推理攻击，发现LLMs对于各种微调数据集都很容易受到攻击，尤其是对于离群用户和贡献大量数据的用户。这对保护用户隐私具有重要意义。

    

    微调是将大型语言模型（LLMs）定制为专业任务和应用的常见有效方法。本文研究了在用户数据上微调LLMs的隐私问题。为此，我们定义了一个称为用户推理的现实威胁模型，其中攻击者推断出用户的数据是否被用于微调。我们实现了这种威胁模型的攻击，只需要从用户那里获取一小组样本（可能与用于训练的样本不同）和对微调LLM的黑盒访问权限。我们发现，LLMs在各种微调数据集上易受用户推理攻击的影响，有时攻击成功率接近完美。此外，我们调查了哪些特性使用户容易受到用户推理的攻击，发现离群用户（即数据分布与其他用户明显不同）和贡献大量数据的用户更容易受到攻击。最后，我们探索了解决这种攻击的方案。

    Fine-tuning is a common and effective method for tailoring large language models (LLMs) to specialized tasks and applications. In this paper, we study the privacy implications of fine-tuning LLMs on user data. To this end, we define a realistic threat model, called user inference, wherein an attacker infers whether or not a user's data was used for fine-tuning. We implement attacks for this threat model that require only a small set of samples from a user (possibly different from the samples used for training) and black-box access to the fine-tuned LLM. We find that LLMs are susceptible to user inference attacks across a variety of fine-tuning datasets, at times with near perfect attack success rates. Further, we investigate which properties make users vulnerable to user inference, finding that outlier users (i.e. those with data distributions sufficiently different from other users) and users who contribute large quantities of data are most susceptible to attack. Finally, we explore s
    
[^6]: PromptRE: 基于提示的数据编程的弱监督文本级关系抽取

    PromptRE: Weakly-Supervised Document-Level Relation Extraction via Prompting-Based Data Programming. (arXiv:2310.09265v1 [cs.CL])

    [http://arxiv.org/abs/2310.09265](http://arxiv.org/abs/2310.09265)

    PromptRE是一种弱监督文档级别关系抽取方法，通过结合基于提示的技术，解决了“没有关系”的实例数量不平衡和直接使用预训练模型进行文档关系抽取的问题。

    

    关系抽取旨在将两个实体之间的关系分类到预定义的类别中。尽管先前的研究主要集中在句级别的关系抽取上，但最近的研究将范围扩大到文档级别的关系抽取。传统的关系抽取方法严重依赖于人工标注的训练数据，这是一项耗时且劳动密集的任务。为了减少对手动标注的需求，最近已经开发了一些弱监督方法用于句级别的关系抽取，但在文档级别的关系抽取方面仍然有限的工作。弱监督的文档级别关系抽取面临着一些挑战，比如“没有关系”的实例数量不平衡，以及直接使用预训练的大语言模型进行文档关系抽取的失败。为了应对这些挑战，我们提出了PromptRE，一种新颖的基于提示的弱监督文档级别关系抽取方法，结合了基于提示的技术。

    Relation extraction aims to classify the relationships between two entities into pre-defined categories. While previous research has mainly focused on sentence-level relation extraction, recent studies have expanded the scope to document-level relation extraction. Traditional relation extraction methods heavily rely on human-annotated training data, which is time-consuming and labor-intensive. To mitigate the need for manual annotation, recent weakly-supervised approaches have been developed for sentence-level relation extraction while limited work has been done on document-level relation extraction. Weakly-supervised document-level relation extraction faces significant challenges due to an imbalanced number "no relation" instances and the failure of directly probing pretrained large language models for document relation extraction. To address these challenges, we propose PromptRE, a novel weakly-supervised document-level relation extraction method that combines prompting-based techniq
    
[^7]: 迈向端到端的基于生成大型语言模型的4位推理

    Towards End-to-end 4-Bit Inference on Generative Large Language Models. (arXiv:2310.09259v1 [cs.LG])

    [http://arxiv.org/abs/2310.09259](http://arxiv.org/abs/2310.09259)

    本论文介绍了一种使用名为QUIK的混合量化策略，在保持良好精度的同时实现大型生成模型的实际速度提升，通过将权重和激活值转换为4位，并提供高效率的逐层运行时GPU内核，实现了高达3.1倍的实际端到端吞吐量提升。

    

    我们展示了对于像LLaMA和OPT这样的大型生成模型，大多数推理计算可以通过将权重和激活值都转换为4位来完成，这种方式可以在保持良好精度的同时实现实际速度提升。我们通过一种名为QUIK的混合量化策略实现了这一目标，该策略将大部分权重和激活值压缩为4位，同时保留一些离群权重和激活值的较高精度。关键是，我们的方案考虑到了计算效率：我们提供了高效率的逐层运行时GPU内核，相对于FP16执行可以实现高达3.1倍的实际端到端吞吐量提升。我们在https://github.com/IST-DASLab/QUIK上提供了代码和模型。

    We show that the majority of the inference computations for large generative models such as LLaMA and OPT can be performed with both weights and activations being cast to 4 bits, in a way that leads to practical speedups while at the same time maintaining good accuracy. We achieve this via a hybrid quantization strategy called QUIK, which compresses most of the weights and activations to 4-bit, while keeping some outlier weights and activations in higher-precision. Crucially, our scheme is designed with computational efficiency in mind: we provide GPU kernels with highly-efficient layer-wise runtimes, which lead to practical end-to-end throughput improvements of up to 3.1x relative to FP16 execution. Code and models are provided at https://github.com/IST-DASLab/QUIK.
    
[^8]: 生成熵神经最优传输在空间内外映射中的应用

    Generative Entropic Neural Optimal Transport To Map Within and Across Spaces. (arXiv:2310.09254v1 [stat.ML])

    [http://arxiv.org/abs/2310.09254](http://arxiv.org/abs/2310.09254)

    该论文介绍了生成熵神经最优传输在测度到测度映射中的应用，解决了处理非平方欧氏距离成本、确定性蒙格映射、映射跨不可比较空间和质量守恒约束等实际挑战。

    

    学习测度到测度的映射是机器学习中的一个关键任务，尤其在生成建模中占据重要地位。近年来，受最优传输理论启发的技术不断涌现。结合神经网络模型，这些方法统称为"神经最优传输"，将最优传输作为归纳偏好：这些映射应该针对给定的成本函数是最优的，能以节约的方式（通过最小化位移）在空间内或空间间移动点。这一原则在直观上是合理的，但往往面临几个实际挑战，需要调整最优传输工具箱：处理其他非平方欧氏距离成本的挑战，确定性状况下的蒙格映射公式会限制灵活性，映射在不可比较的空间中会带来多个挑战，最优传输固有的质量守恒约束可能对异常数据给予过多的重视。

    Learning measure-to-measure mappings is a crucial task in machine learning, featured prominently in generative modeling. Recent years have witnessed a surge of techniques that draw inspiration from optimal transport (OT) theory. Combined with neural network models, these methods collectively known as \textit{Neural OT} use optimal transport as an inductive bias: such mappings should be optimal w.r.t. a given cost function, in the sense that they are able to move points in a thrifty way, within (by minimizing displacements) or across spaces (by being isometric). This principle, while intuitive, is often confronted with several practical challenges that require adapting the OT toolbox: cost functions other than the squared-Euclidean cost can be challenging to handle, the deterministic formulation of Monge maps leaves little flexibility, mapping across incomparable spaces raises multiple challenges, while the mass conservation constraint inherent to OT can provide too much credit to outli
    
[^9]: 它是一种对齐，而不是权衡：重新审视深度模型中的偏差和方差

    It's an Alignment, Not a Trade-off: Revisiting Bias and Variance in Deep Models. (arXiv:2310.09250v1 [cs.LG])

    [http://arxiv.org/abs/2310.09250](http://arxiv.org/abs/2310.09250)

    在基于深度学习的分类模型集合中，对于正确分类的样本点，偏差和方差在样本级别上是对齐的。

    

    传统的机器学习智慧认为泛化误差可以分解为偏差和方差，并且这两个术语之间存在着"权衡"。然而，在本文中，我们展示了在基于深度学习的分类模型集合中，偏差和方差在样本级别上是"对齐"的，其中对于正确分类的样本点，均方偏差大约等于方差。我们提供了经验证据来证实这一现象在各种深度学习模型和数据集中存在。此外，我们从校准和神经崩溃的两个理论视角研究了该现象。首先，我们理论上证明在模型良好校准的假设下，我们可以观察到偏差-方差的对齐。其次，在神经崩溃理论提供的图景下，我们展示了偏差和方差之间的近似相关性。

    Classical wisdom in machine learning holds that the generalization error can be decomposed into bias and variance, and these two terms exhibit a \emph{trade-off}. However, in this paper, we show that for an ensemble of deep learning based classification models, bias and variance are \emph{aligned} at a sample level, where squared bias is approximately \emph{equal} to variance for correctly classified sample points. We present empirical evidence confirming this phenomenon in a variety of deep learning models and datasets. Moreover, we study this phenomenon from two theoretical perspectives: calibration and neural collapse. We first show theoretically that under the assumption that the models are well calibrated, we can observe the bias-variance alignment. Second, starting from the picture provided by the neural collapse theory, we show an approximate correlation between bias and variance.
    
[^10]: 通过WordNet层次结构对文本到图像模型进行上义词理解评估

    Hypernymy Understanding Evaluation of Text-to-Image Models via WordNet Hierarchy. (arXiv:2310.09247v1 [cs.CV])

    [http://arxiv.org/abs/2310.09247](http://arxiv.org/abs/2310.09247)

    通过基于WordNet层次结构的方法，我们评估了流行的文本到图像模型对于上义词关系的理解能力。我们提出了两个自动度量标准，能够定量比较不同模型的语言能力，并发现了一些困难的词汇。我们全面评估了一些流行的文本到图像模型，包括GLIDE、Latent Diffusion和Stable Diffusion。

    

    最近，由于质量不断提高和众多实际应用，文本到图像合成引起了广泛关注。然而，对于文本到图像模型的语言理解能力仍然知之甚少，这使得难以推理出给定模型能够理解的提示表达。在这项工作中，我们衡量了流行的文本到图像模型对于上义词（或“是一个”关系）的理解能力。我们设计了两个基于WordNet语义层次和在ImageNet上预训练的现有图像分类器的自动度量标准。这两个度量标准均能够对文本到图像模型的语言能力进行广泛的定量比较，并提供了一种找到细粒度定性差异的方法，例如对于模型来说未知的单词，因此很难绘制。我们全面评估了流行的文本到图像模型，包括GLIDE、Latent Diffusion和Stable Diffusion，展示了它们的能力和局限。

    Text-to-image synthesis has recently attracted widespread attention due to rapidly improving quality and numerous practical applications. However, the language understanding capabilities of text-to-image models are still poorly understood, which makes it difficult to reason about prompt formulations that a given model would understand well. In this work, we measure the capability of popular text-to-image models to understand $\textit{hypernymy}$, or the "is-a" relation between words. We design two automatic metrics based on the WordNet semantic hierarchy and existing image classifiers pretrained on ImageNet. These metrics both enable broad quantitative comparison of linguistic capabilities for text-to-image models and offer a way of finding fine-grained qualitative differences, such as words that are unknown to models and thus are difficult for them to draw. We comprehensively evaluate popular text-to-image models, including GLIDE, Latent Diffusion, and Stable Diffusion, showing how ou
    
[^11]: 时间卷积神经网络和图卷积网络在MEG数据中的癫痫尖峰检测中的应用

    Time CNN and Graph Convolution Network for Epileptic Spike Detection in MEG Data. (arXiv:2310.09236v1 [cs.CV])

    [http://arxiv.org/abs/2310.09236](http://arxiv.org/abs/2310.09236)

    提出了一种使用1D时间卷积神经网络(Time CNN)和图卷积网络(GCN)的方法来检测MEG数据中的癫痫尖峰。相较于其他方法，我们的模型具有更少的参数，并且使用GCN来考虑MEG传感器的空间关系。在临床数据集和高度不平衡的数据集上，我们的模型都取得了优于深度学习方法的分类性能。

    

    癫痫患者的脑磁图(MEG)记录显示出尖峰，这是病理学的典型生物标志。检测这些尖峰可以准确地定位引发癫痫的脑区。目前尖峰检测通常是手动完成的，但由于MEG数据的复杂性，这是一项繁重且容易出错的任务。为了解决这个问题，我们提出了一种1D时间卷积神经网络(Time CNN)结合图卷积网络(GCN)的方法，将MEG记录的短时间框架分类为包含尖峰或不包含尖峰。与其他最近的方法相比，我们的模型需要训练的参数更少，并且我们提出使用GCN来考虑MEG传感器之间的空间关系。我们的模型产生了临床相关的结果，并且在平衡数据集上达到了76.7%的分类F1分数，在现实中高度不平衡的数据集上达到了25.5%的分类F1分数，针对尖峰类别。

    Magnetoencephalography (MEG) recordings of patients with epilepsy exhibit spikes, a typical biomarker of the pathology. Detecting those spikes allows accurate localization of brain regions triggering seizures. Spike detection is often performed manually. However, it is a burdensome and error prone task due to the complexity of MEG data. To address this problem, we propose a 1D temporal convolutional neural network (Time CNN) coupled with a graph convolutional network (GCN) to classify short time frames of MEG recording as containing a spike or not. Compared to other recent approaches, our models have fewer parameters to train and we propose to use a GCN to account for MEG sensors spatial relationships. Our models produce clinically relevant results and outperform deep learning-based state-of-the-art methods reaching a classification f1-score of 76.7% on a balanced dataset and of 25.5% on a realistic, highly imbalanced dataset, for the spike class.
    
[^12]: 保障微笑：利用Spark ML预测常规牙科保险覆盖

    Insuring Smiles: Predicting routine dental coverage using Spark ML. (arXiv:2310.09229v1 [cs.LG])

    [http://arxiv.org/abs/2310.09229](http://arxiv.org/abs/2310.09229)

    本文利用机器学习算法分析健康保险计划的类型、区域和费用等因素，预测其是否覆盖成年人的常规牙科服务，旨在为个人和家庭提供选择最合适保险的临床策略。

    

    在美国，个人和小型企业往往难以找到合适的健康保险覆盖。CMS提供的健康保险交流公共使用文件（Exchange PUFs）数据集提供了有关健康和牙科政策的重要信息。本文利用机器学习算法，通过分析计划类型、区域、免赔额、自付费用上限和共付金额等因素，预测健康保险计划是否覆盖成年人的常规牙科服务。我们的目标是为个人和家庭提供基于收入和费用的最合适的保险选择的临床策略。

    Finding suitable health insurance coverage can be challenging for individuals and small enterprises in the USA. The Health Insurance Exchange Public Use Files (Exchange PUFs) dataset provided by CMS offers valuable information on health and dental policies [1]. In this paper, we leverage machine learning algorithms to predict if a health insurance plan covers routine dental services for adults. By analyzing plan type, region, deductibles, out-of-pocket maximums, and copayments, we employ Logistic Regression, Decision Tree, Random Forest, Gradient Boost, Factorization Model and Support Vector Machine algorithms. Our goal is to provide a clinical strategy for individuals and families to select the most suitable insurance plan based on income and expenses.
    
[^13]: 从数据中快速和高效地学习贝叶斯网络：知识发现与因果关系

    Fast & Efficient Learning of Bayesian Networks from Data: Knowledge Discovery and Causality. (arXiv:2310.09222v1 [cs.LG])

    [http://arxiv.org/abs/2310.09222](http://arxiv.org/abs/2310.09222)

    本论文提出了两个新算法FSBN和SSBN，它们利用局部搜索和条件独立性测试从数据中学习贝叶斯网络的因果结构。实验结果表明，这两个算法在降低计算成本的同时保持了高质量的归纳，提供了可解释性和适应性。

    

    结构学习对贝叶斯网络（BNs）至关重要，它可以揭示因果关系，并能在不确定性下进行知识发现、预测、推理和决策。基于PC算法的两个新算法FSBN和SSBN，采用局部搜索策略和条件独立性测试，从数据中学习因果网络结构。它们利用d-分离来推断额外的拓扑信息，优先考虑条件集，并高效地终止搜索。FSBN实现了高达52%的计算成本减少，而SSBN在200个节点的网络中超过了它，减少了72%的计算成本。由于智能策略，SSBN展示了进一步的效率提升。实验研究表明，这两个算法在显著降低计算成本的同时，与PC算法的归纳质量相匹配。这使得它们在减轻计算负担的同时提供了可解释性和适应性，使它们具有很高的价值。

    Structure learning is essential for Bayesian networks (BNs) as it uncovers causal relationships, and enables knowledge discovery, predictions, inferences, and decision-making under uncertainty. Two novel algorithms, FSBN and SSBN, based on the PC algorithm, employ local search strategy and conditional independence tests to learn the causal network structure from data. They incorporate d-separation to infer additional topology information, prioritize conditioning sets, and terminate the search immediately and efficiently. FSBN achieves up to 52% computation cost reduction, while SSBN surpasses it with a remarkable 72% reduction for a 200-node network. SSBN demonstrates further efficiency gains due to its intelligent strategy. Experimental studies show that both algorithms match the induction quality of the PC algorithm while significantly reducing computation costs. This enables them to offer interpretability and adaptability while reducing the computational burden, making them valuable
    
[^14]: 使用扩散模型合成未见过的图像

    Unseen Image Synthesis with Diffusion Models. (arXiv:2310.09213v1 [cs.LG])

    [http://arxiv.org/abs/2310.09213](http://arxiv.org/abs/2310.09213)

    本论文提出使用预训练的扩散模型在未见过的领域合成图像的方法，并理论上和经验上证明了这种方法的有效性。

    

    当前生成领域的趋势是通过扩大模型规模和增加训练数据来实现通用领域表示，而我们在这项工作中选择相反的方向，通过使用预训练和冻结的去噪扩散概率模型（DDPMs）在单领域数据集上进行潜在采样和几何优化来合成未见过的领域图像。我们的关键观察是，即使是仅在单领域图像上进行预训练的DDPMs已经具备了足够的表示能力，可以通过反转潜在编码，并经过双向确定性扩散和去噪轨迹重构任意图像。这促使我们研究未见过图像领域中的潜在空间中沿去噪链的OOD样本的统计和几何行为。值得注意的是，我们在理论上和经验上都表明，反转的OOD样本也建立了高斯分布。

    While the current trend in the generative field is scaling up towards larger models and more training data for generalized domain representations, we go the opposite direction in this work by synthesizing unseen domain images without additional training. We do so via latent sampling and geometric optimization using pre-trained and frozen Denoising Diffusion Probabilistic Models (DDPMs) on single-domain datasets. Our key observation is that DDPMs pre-trained even just on single-domain images are already equipped with sufficient representation abilities to reconstruct arbitrary images from the inverted latent encoding following bi-directional deterministic diffusion and denoising trajectories. This motivates us to investigate the statistical and geometric behaviors of the Out-Of-Distribution (OOD) samples from unseen image domains in the latent spaces along the denoising chain. Notably, we theoretically and empirically show that the inverted OOD samples also establish Gaussians that are 
    
[^15]: 对序数量化问题的基于正则化的方法

    Regularization-Based Methods for Ordinal Quantification. (arXiv:2310.09210v1 [cs.LG])

    [http://arxiv.org/abs/2310.09210](http://arxiv.org/abs/2310.09210)

    本文研究了序数量化问题，提出了两个新的数据集用于研究，实验比较了已有算法，并提出了一种性能更好的正则化OQ算法类别。

    

    最近几年来，量化问题——即在未标记的数据项集中训练预测器的类别普遍性值——受到了越来越多的关注。然而，大多数量化研究集中在二分类和多分类问题的算法开发上，其中类别没有被排序。在本文中，我们研究了序数情况，即在n>2个类别的集合上定义了一个完全的排序。我们对这个领域做出了三个主要贡献。首先，我们创建并提供了两个序数计量（OQ）研究的数据集，克服了以前可用数据集的不足之处。其次，我们实验比较了迄今为止文献中提出的最重要的OQ算法。为此，我们将来自不同研究领域的作者提出的算法进行比较，如数据挖掘和天体物理学，这些作者对彼此的研究进展并不知晓。第三，我们提出了一种新颖的正则化OQ算法类别，它胜过了其它算法。

    Quantification, i.e., the task of training predictors of the class prevalence values in sets of unlabeled data items, has received increased attention in recent years. However, most quantification research has concentrated on developing algorithms for binary and multiclass problems in which the classes are not ordered. Here, we study the ordinal case, i.e., the case in which a total order is defined on the set of n>2 classes. We give three main contributions to this field. First, we create and make available two datasets for ordinal quantification (OQ) research that overcome the inadequacies of the previously available ones. Second, we experimentally compare the most important OQ algorithms proposed in the literature so far. To this end, we bring together algorithms proposed by authors from very different research fields, such as data mining and astrophysics, who were unaware of each others' developments. Third, we propose a novel class of regularized OQ algorithms, which outperforms e
    
[^16]: SiamAF: 学习心电图和光电脉搏图信号的共享信息用于强健的心房颤动检测

    SiamAF: Learning Shared Information from ECG and PPG Signals for Robust Atrial Fibrillation Detection. (arXiv:2310.09203v1 [cs.LG])

    [http://arxiv.org/abs/2310.09203](http://arxiv.org/abs/2310.09203)

    提出了一种名为SiamAF的新方法，利用心电图和光电脉搏图信号的共享信息，通过Siamese网络和联合学习实现强健的心房颤动（AF）检测。

    

    心房颤动（AF）是最常见的心脏心律失常类型，与中风、心力衰竭和其他心血管并发症的风险增加有关，但可以临床上无声。佩戴式设备进行被动性的AF监测可能有助于减少与AF相关的不良临床结果。在嘈杂的佩戴式数据中检测AF面临重大挑战，引发了各种不同的深度学习技术。先前的深度学习模型从单一形态学习，要么是心电图（ECG），要么是光电脉搏图（PPG）信号。然而，深度学习模型往往难以学习可泛化的特征，并依赖于更容易受到噪声损坏的特征，在某些场景中导致次优的性能，特别是在低质量信号的情况下。鉴于佩戴式设备和床边监护仪上ECG和PPG信号配对的日益丰富，我们提出了一种新的方法SiamAF，利用一种新颖的Siamese网络结构和联合学习的方法。

    Atrial fibrillation (AF) is the most common type of cardiac arrhythmia. It is associated with an increased risk of stroke, heart failure, and other cardiovascular complications, but can be clinically silent. Passive AF monitoring with wearables may help reduce adverse clinical outcomes related to AF. Detecting AF in noisy wearable data poses a significant challenge, leading to the emergence of various deep learning techniques. Previous deep learning models learn from a single modality, either electrocardiogram (ECG) or photoplethysmography (PPG) signals. However, deep learning models often struggle to learn generalizable features and rely on features that are more susceptible to corruption from noise, leading to sub-optimal performances in certain scenarios, especially with low-quality signals. Given the increasing availability of ECG and PPG signal pairs from wearables and bedside monitors, we propose a new approach, SiamAF, leveraging a novel Siamese network architecture and joint le
    
[^17]: 图嵌入与特征匹配的图压缩

    Graph Condensation via Eigenbasis Matching. (arXiv:2310.09202v1 [cs.LG])

    [http://arxiv.org/abs/2310.09202](http://arxiv.org/abs/2310.09202)

    本论文提出了基于特征匹配的无谱图压缩方法，以解决现有方法在泛化能力上存在的问题。通过详细分析发现，GNN注入合成图中的谱偏差导致了性能差异，我们的方法可以缓解这个问题。

    

    随着图数据量的增加，要求图神经网络（GNN）在各种图相关应用中提高效率和可伸缩性。最近，新兴的图压缩（GC）从数据角度降低了GNN的计算成本。它旨在用一个明显较小的合成图替代真实的大型图，使得在这两个图上训练的GNN表现出可比较的性能。然而，我们的实证研究发现，现有的GC方法在泛化能力上存在问题，即在同一个合成图上训练的不同GNN性能存在明显差异。是什么因素阻碍了GC的泛化能力，我们如何缓解这个问题？为了回答这个问题，我们进行了详细分析，发现GNN会将谱偏差注入合成图中，导致分布偏移。为解决这个问题，我们提出了基于特征匹配的无谱图压缩，称之为...

    The increasing amount of graph data places requirements on the efficiency and scalability of graph neural networks (GNNs), despite their effectiveness in various graph-related applications. Recently, the emerging graph condensation (GC) sheds light on reducing the computational cost of GNNs from a data perspective. It aims to replace the real large graph with a significantly smaller synthetic graph so that GNNs trained on both graphs exhibit comparable performance. However, our empirical investigation reveals that existing GC methods suffer from poor generalization, i.e., different GNNs trained on the same synthetic graph have obvious performance gaps. What factors hinder the generalization of GC and how can we mitigate it? To answer this question, we commence with a detailed analysis and observe that GNNs will inject spectrum bias into the synthetic graph, resulting in a distribution shift. To tackle this issue, we propose eigenbasis matching for spectrum-free graph condensation, name
    
[^18]: 一个求解最小最大相关聚类问题的4近似算法

    A 4-approximation algorithm for min max correlation clustering. (arXiv:2310.09196v1 [cs.DS])

    [http://arxiv.org/abs/2310.09196](http://arxiv.org/abs/2310.09196)

    本论文引入了一种新的下界技术，并提出了一个组合算法来求解最小最大相关聚类问题，该算法能够在完全图上达到4的近似结果。通过贪婪联合启发式算法的扩展和实验证明，在多个基准数据集上，该算法在解决方案质量和运行时间上均有显著改进。

    

    我们引入了一种求解最小最大相关聚类问题的下界技术，并基于此技术，提出了一个针对完全图的组合4近似算法。这改进了之前使用线性规划公式（Kalhan等，2019）获得的近似保证为5和使用组合算法（Davies等，2023）获得的近似保证为4的最佳已知结果。我们通过贪婪联合启发式算法扩展了该算法，并通过实验证明，在几个基准数据集上，它提高了解决方案质量和运行时间的技术水平。

    We introduce a lower bounding technique for the min max correlation clustering problem and, based on this technique, a combinatorial 4-approximation algorithm for complete graphs. This improves upon the previous best known approximation guarantees of 5, using a linear program formulation (Kalhan et al., 2019), and 4, for a combinatorial algorithm (Davies et al., 2023). We extend this algorithm by a greedy joining heuristic and show empirically that it improves the state of the art in solution quality and runtime on several benchmark datasets.
    
[^19]: 变分自动编码器与加权样本在高维非参数自适应重要性采样中的应用

    Variational autoencoder with weighted samples for high-dimensional non-parametric adaptive importance sampling. (arXiv:2310.09194v1 [cs.LG])

    [http://arxiv.org/abs/2310.09194](http://arxiv.org/abs/2310.09194)

    本文提出了一种使用变分自动编码器和加权样本的方法来近似高维非参数自适应重要性采样中的目标分布。所得到的分布族具有与非参数模型相当的表达能力，并且在高维情况下比传统的高斯或高斯混合族更高效。同时，我们还引入了可学习的先验分布以增加模型的灵活性和学习多模态分布的能力。

    

    采用加权样本的概率密度函数估计是所有自适应重要性采样算法的基础。传统上，目标分布要么通过非参数模型进行近似，要么在参数化族中进行近似。然而，这些模型要么面临维度灾难，要么缺乏灵活性。在本文中，我们建议使用由变分自动编码器参数化的分布作为近似模型。我们通过引入新的目标函数将现有框架扩展到加权样本的情况。所得到的分布族的灵活性使其与非参数模型一样表达能力强，尽管参数估计的数量非常高，但在高维情况下，这个分布族比传统的高斯或高斯混合族要更高效。此外，为了增加模型的灵活性和学习多模态分布的能力，我们考虑了可学习的先验分布。

    Probability density function estimation with weighted samples is the main foundation of all adaptive importance sampling algorithms. Classically, a target distribution is approximated either by a non-parametric model or within a parametric family. However, these models suffer from the curse of dimensionality or from their lack of flexibility. In this contribution, we suggest to use as the approximating model a distribution parameterised by a variational autoencoder. We extend the existing framework to the case of weighted samples by introducing a new objective function. The flexibility of the obtained family of distributions makes it as expressive as a non-parametric model, and despite the very high number of parameters to estimate, this family is much more efficient in high dimension than the classical Gaussian or Gaussian mixture families. Moreover, in order to add flexibility to the model and to be able to learn multimodal distributions, we consider a learnable prior distribution fo
    
[^20]: 图状压缩：是否看起来像视觉数据集？(arXiv:2310.09192v1 [cs.LG])

    Does Graph Distillation See Like Vision Dataset Counterpart?. (arXiv:2310.09192v1 [cs.LG])

    [http://arxiv.org/abs/2310.09192](http://arxiv.org/abs/2310.09192)

    本文研究了图压缩方法中对结构信息的忽视问题，并提出了一种新的结构广播图数据集压缩方案（SGDD），该方案能够保留原始图的结构信息，并在跨架构泛化和特定任务中取得了优秀的性能。

    

    在图表示学习中，对大规模图进行训练取得了显著的结果，但其成本和存储引起了越来越多的关注。现有的图压缩方法主要关注优化压缩图的特征矩阵，而忽视了原始图的结构信息的影响。为了调查结构信息的影响，我们从谱域进行分析，并经验性地确定了先前工作中的重要的拉普拉斯能量分布（LED）的偏移。这种偏移导致在跨架构泛化和特定任务（包括异常检测和链接预测）中的性能较差。在本文中，我们提出了一种新颖的结构广播图数据集压缩（SGDD）方案，将原始结构信息广播到合成图的生成过程中，显式地防止忽视原始结构信息。从理论上讲，SGDD生成的合成图具有保留原始图的结构信息的能力。

    Training on large-scale graphs has achieved remarkable results in graph representation learning, but its cost and storage have attracted increasing concerns. Existing graph condensation methods primarily focus on optimizing the feature matrices of condensed graphs while overlooking the impact of the structure information from the original graphs. To investigate the impact of the structure information, we conduct analysis from the spectral domain and empirically identify substantial Laplacian Energy Distribution (LED) shifts in previous works. Such shifts lead to poor performance in cross-architecture generalization and specific tasks, including anomaly detection and link prediction. In this paper, we propose a novel Structure-broadcasting Graph Dataset Distillation (SGDD) scheme for broadcasting the original structure information to the generation of the synthetic one, which explicitly prevents overlooking the original structure information. Theoretically, the synthetic graphs by SGDD 
    
[^21]: PRIOR: 个性化先验用于重新激活联邦学习中被忽视的信息

    PRIOR: Personalized Prior for Reactivating the Information Overlooked in Federated Learning. (arXiv:2310.09183v1 [cs.LG])

    [http://arxiv.org/abs/2310.09183](http://arxiv.org/abs/2310.09183)

    本研究提出了一种名为PRIOR的个性化先验方案，用于解决个性化联邦学习中由于全局模型忽视特定信息而导致的不完整信息问题。该方案使用带Bregman散度的PFL框架将个性化先验与本地目标函数解耦，以提高适应性和性能。

    

    传统的联邦学习（FL）通过在保护隐私的前提下训练机器学习模型，而异质数据特性降低了局部模型的性能。个性化联邦学习（PFL）通过从全局模型中合成个性化模型来解决这个问题，在本地数据上进行训练。然而，这样的全局模型可能忽视了客户端被采样的特定信息。本文提出了一种新的方案，通过将个性化先验知识注入到每个客户端的全局模型中，试图减轻PFL中引入的不完整信息问题。我们提出的方法的核心是一种框架，即带Bregman散度（pFedBreD）的PFL，通过在个性化场景中使用Bregman散度正则化的本地目标函数，使个性化先验与之解耦，具有更强的适应性。我们还放松了镜像下降（RMD），以显式地提取先验知识，提供可选择的策略。

    Classical federated learning (FL) enables training machine learning models without sharing data for privacy preservation, but heterogeneous data characteristic degrades the performance of the localized model. Personalized FL (PFL) addresses this by synthesizing personalized models from a global model via training on local data. Such a global model may overlook the specific information that the clients have been sampled. In this paper, we propose a novel scheme to inject personalized prior knowledge into the global model in each client, which attempts to mitigate the introduced incomplete information problem in PFL. At the heart of our proposed approach is a framework, the PFL with Bregman Divergence (pFedBreD), decoupling the personalized prior from the local objective function regularized by Bregman divergence for greater adaptability in personalized scenarios. We also relax the mirror descent (RMD) to extract the prior explicitly to provide optional strategies. Additionally, our pFed
    
[^22]: 一种深度神经网络和机制混合模型用于预测大鼠的药代动力学

    A Deep Neural Network -- Mechanistic Hybrid Model to Predict Pharmacokinetics in Rat. (arXiv:2310.09167v1 [q-bio.QM])

    [http://arxiv.org/abs/2310.09167](http://arxiv.org/abs/2310.09167)

    该论文提出了一种深度神经网络和机制混合模型，用于预测大鼠的药代动力学。通过训练更大的数据集、改进网络架构和参数化机制模型，成功减小了口服和静脉给药的误差，并将这种方法扩展到预测更多终点和处理不同的协变量。

    

    在小分子药物或农药的研发中，重要的一个方面就是它们在静脉和口服给药后的全身可用性。从候选化合物的化学结构预测全身可用性非常有价值，因为它可以让药物或农药的研发集中在具有良好动力学特性的化合物上。然而，这样的预测具有挑战性，因为可用性是分子性质、生物学和生理学之间复杂相互作用的结果，而训练数据非常稀缺。在这项工作中，我们改进了先前开发的混合模型[34]。我们将总的口服暴露的中值折叠误差从2.85降低到2.35，将静脉给药的误差从1.95降低到1.62。这是通过在更大的数据集上进行训练，改进神经网络架构以及机制模型的参数化实现的。此外，我们扩展了我们的方法来预测其他终点和处理不同的协变量。

    An important aspect in the development of small molecules as drugs or agro-chemicals is their systemic availability after intravenous and oral administration.The prediction of the systemic availability from the chemical structure of a poten-tial candidate is highly desirable, as it allows to focus the drug or agrochemicaldevelopment on compounds with a favorable kinetic profile. However, such pre-dictions are challenging as the availability is the result of the complex interplaybetween molecular properties, biology and physiology and training data is rare.In this work we improve the hybrid model developed earlier [34]. We reducethe median fold change error for the total oral exposure from 2.85 to 2.35 andfor intravenous administration from 1.95 to 1.62. This is achieved by trainingon a larger data set, improving the neural network architecture as well as theparametrization of mechanistic model. Further, we extend our approach to predictadditional endpoints and to handle different covar
    
[^23]: 动态神经网络的联合学习退出和推断：JEI-DNN

    Jointly-Learned Exit and Inference for a Dynamic Neural Network : JEI-DNN. (arXiv:2310.09163v1 [cs.LG])

    [http://arxiv.org/abs/2310.09163](http://arxiv.org/abs/2310.09163)

    JEI-DNN是一种联合学习退出和推断的动态神经网络架构，通过允许模型在中间层进行部分预测，解决了大型预训练模型每次推断所需大量资源的问题。

    

    大型预训练模型结合微调已成为主导的机器学习架构。尽管这些模型表现出令人印象深刻的性能，但它们的实际应用通常受到每次推断所需的巨大资源的限制。早期退出动态神经网络（EDNN）通过允许模型从中间层进行部分预测（即早期退出）来绕过这个问题。训练EDNN架构具有挑战性，因为它包括两个相互交织的组件：控制早期退出决策的门控机制（GM）和执行中间表示推断的中间推断模块（IMs）。因此，大多数现有方法依赖于门控机制的阈值置信度度量，并努力改进基本的骨干网络和推断模块。尽管取得了成功，但这种方法有两个基本缺点：1）门控机制和中间推断模块不能共同学习和优化，2）GM在一定程度上依赖于IMS和骨干网络的质量，导致模型性能上的损失。

    Large pretrained models, coupled with fine-tuning, are slowly becoming established as the dominant architecture in machine learning. Even though these models offer impressive performance, their practical application is often limited by the prohibitive amount of resources required for every inference. Early-exiting dynamic neural networks (EDNN) circumvent this issue by allowing a model to make some of its predictions from intermediate layers (i.e., early-exit). Training an EDNN architecture is challenging as it consists of two intertwined components: the gating mechanism (GM) that controls early-exiting decisions and the intermediate inference modules (IMs) that perform inference from intermediate representations. As a result, most existing approaches rely on thresholding confidence metrics for the gating mechanism and strive to improve the underlying backbone network and the inference modules. Although successful, this approach has two fundamental shortcomings: 1) the GMs and the IMs 
    
[^24]: 量子机器学习在气候变化和可持续发展中的应用：一项综述

    Quantum Machine Learning in Climate Change and Sustainability: a Review. (arXiv:2310.09162v1 [cs.LG])

    [http://arxiv.org/abs/2310.09162](http://arxiv.org/abs/2310.09162)

    量子机器学习在解决气候变化和可持续发展问题方面具有潜力，可以应用于能源系统、气候数据预测、气候监测和危险事件预测等领域。该综述调查了目前将量子机器学习应用于这些问题的现有研究，并讨论了挑战、局限以及潜在机会和未来工作。

    

    气候变化及其对全球可持续发展的影响是关键挑战，需要结合尖端技术和科学见解的创新解决方案。量子机器学习（QML）已经成为一种有前景的范式，利用量子计算的力量来解决包括气候变化和可持续发展在内的复杂问题。本研究调查了将量子机器学习应用于解决气候变化和可持续发展问题的现有文献。我们回顾了有潜力加速减碳的QML方法，包括能源系统、气候数据预测、气候监测和危险事件预测。我们讨论了量子机器学习方法的挑战和目前的局限，并概述了在气候变化研究这一重要领域利用基于QML的方法的潜在机会和未来工作。

    Climate change and its impact on global sustainability are critical challenges, demanding innovative solutions that combine cutting-edge technologies and scientific insights. Quantum machine learning (QML) has emerged as a promising paradigm that harnesses the power of quantum computing to address complex problems in various domains including climate change and sustainability. In this work, we survey existing literature that applies quantum machine learning to solve climate change and sustainability-related problems. We review promising QML methodologies that have the potential to accelerate decarbonization including energy systems, climate data forecasting, climate monitoring, and hazardous events predictions. We discuss the challenges and current limitations of quantum machine learning approaches and provide an overview of potential opportunities and future work to leverage QML-based methods in the important area of climate change research.
    
[^25]: 寻找非凸优化中的稳定点的计算复杂性

    The Computational Complexity of Finding Stationary Points in Non-Convex Optimization. (arXiv:2310.09157v1 [math.OC])

    [http://arxiv.org/abs/2310.09157](http://arxiv.org/abs/2310.09157)

    本文研究了在非凸优化中找到光滑目标函数的近似稳定点的计算复杂性和查询复杂性，并给出了相应的结果。对于$d=2$的情况，提供了一种零阶算法，只需要少量的函数值查询即可找到$\varepsilon$-近似稳定点。

    

    寻找非凸但光滑目标函数$f$在无限制的$d$维域上的近似稳定点，即梯度近似为零的点，是经典非凸优化中最基本的问题之一。然而，当问题的维度$d$与近似误差独立时，这个问题的计算复杂性和查询复杂性仍不十分清楚。在本文中，我们展示了以下计算复杂性和查询复杂性结果：1.在无限制的域中寻找近似稳定点的问题是PLS完全问题。2.对于$d=2$，我们提供了一种零阶算法，用于寻找$\varepsilon$-近似稳定点，只需要对目标函数进行最多$O(1/\varepsilon)$次函数值查询。3.我们证明当$d=2$时，任何算法至少需要$\Omega(1/\varepsilon)$次对目标函数和/或梯度的查询来找到$\varepsilon$-近似稳定点。

    Finding approximate stationary points, i.e., points where the gradient is approximately zero, of non-convex but smooth objective functions $f$ over unrestricted $d$-dimensional domains is one of the most fundamental problems in classical non-convex optimization. Nevertheless, the computational and query complexity of this problem are still not well understood when the dimension $d$ of the problem is independent of the approximation error. In this paper, we show the following computational and query complexity results:  1. The problem of finding approximate stationary points over unrestricted domains is PLS-complete.  2. For $d = 2$, we provide a zero-order algorithm for finding $\varepsilon$-approximate stationary points that requires at most $O(1/\varepsilon)$ value queries to the objective function.  3. We show that any algorithm needs at least $\Omega(1/\varepsilon)$ queries to the objective function and/or its gradient to find $\varepsilon$-approximate stationary points when $d=2$.
    
[^26]: 微分水平空间中的格点逼近

    Lattice Approximations in Wasserstein Space. (arXiv:2310.09149v1 [stat.ML])

    [http://arxiv.org/abs/2310.09149](http://arxiv.org/abs/2310.09149)

    本论文研究了在Wasserstein空间中通过离散和分段常数测度进行的结构逼近方法。结果表明，对于满秩的格点按比例缩放后得到的Voronoi分割逼近的测度误差是$O(h)$，逼近的$N$项误差为$O(N^{-\frac1d})$，并且可以推广到非紧支撑测度。

    

    我们考虑在Wasserstein空间$W_p(\mathbb{R}^d)$中通过离散和分段常数测度来对测度进行结构逼近。我们证明，如果一个满秩的格点$\Lambda$按照$h\in(0,1]$的比例进行缩放，那么基于$h\Lambda$的Voronoi分割得到的测度逼近是$O(h)$，不论$d$或$p$的取值。之后，我们使用覆盖论证证明，对于紧支撑的测度的$N$项逼近是$O(N^{-\frac1d})$，这与最优量化器和经验测度逼近在大多数情况下已知的速率相匹配。最后，我们将这些结果推广到非紧支撑测度，要求其具有足够的衰减性质。

    We consider structured approximation of measures in Wasserstein space $W_p(\mathbb{R}^d)$ for $p\in[1,\infty)$ by discrete and piecewise constant measures based on a scaled Voronoi partition of $\mathbb{R}^d$. We show that if a full rank lattice $\Lambda$ is scaled by a factor of $h\in(0,1]$, then approximation of a measure based on the Voronoi partition of $h\Lambda$ is $O(h)$ regardless of $d$ or $p$. We then use a covering argument to show that $N$-term approximations of compactly supported measures is $O(N^{-\frac1d})$ which matches known rates for optimal quantizers and empirical measure approximation in most instances. Finally, we extend these results to noncompactly supported measures with sufficient decay.
    
[^27]: 强化学习中的古哈特定律

    Goodhart's Law in Reinforcement Learning. (arXiv:2310.09144v1 [cs.LG])

    [http://arxiv.org/abs/2310.09144](http://arxiv.org/abs/2310.09144)

    该论文通过研究古哈特定律在强化学习中的现象，提出了一种衡量效应程度的方法，并实证表明在广泛的环境和奖励函数范围内，对不完美代理奖励的过度优化会导致降低在真实目标上的性能。然后，通过几何解释马尔可夫决策过程中古哈特定律的发生，提出了一种可避免陷阱的最优早停止方法，并推导了方法的理论遗憾界限。此外，还提出了一种逐渐过渡到真实奖励的训练方法。

    

    在现实世界中，实现完全捕捉复杂任务的奖励函数是不切实际的。因此，把奖励函数视为真实目标的代理而非定义是合理的。我们通过古哈特定律的视角研究了这一现象，该定律预测在某一临界点之后，对不完美代理奖励的过度优化会降低在真实目标上的性能。首先，我们提出一种衡量该效应程度的方法，并实证表明，在广泛的环境和奖励函数范围内，对不完美代理奖励进行优化常常会导致古哈特定律所预测的行为。然后，我们提供了一个几何解释，说明为什么在马尔可夫决策过程中发生古哈特定律。我们利用这些理论洞察为该问题提出了一种可避免陷阱的最优早停止方法，并推导了该方法的理论遗憾界限。此外，我们提出了一种训练方法，可以使用代理奖励进行优化，并且逐步过渡到真实奖励。

    Implementing a reward function that perfectly captures a complex task in the real world is impractical. As a result, it is often appropriate to think of the reward function as a proxy for the true objective rather than as its definition. We study this phenomenon through the lens of Goodhart's law, which predicts that increasing optimisation of an imperfect proxy beyond some critical point decreases performance on the true objective. First, we propose a way to quantify the magnitude of this effect and show empirically that optimising an imperfect proxy reward often leads to the behaviour predicted by Goodhart's law for a wide range of environments and reward functions. We then provide a geometric explanation for why Goodhart's law occurs in Markov decision processes. We use these theoretical insights to propose an optimal early stopping method that provably avoids the aforementioned pitfall and derive theoretical regret bounds for this method. Moreover, we derive a training method that 
    
[^28]: 共识游戏：通过均衡搜索生成语言模型

    The Consensus Game: Language Model Generation via Equilibrium Search. (arXiv:2310.09139v1 [cs.GT])

    [http://arxiv.org/abs/2310.09139](http://arxiv.org/abs/2310.09139)

    这篇论文介绍了一种新的语言模型解码方法，将其视为规范化的不完美信息序列信号博弈，并通过找到近似均衡点得到了一个解码算法。这种方法可以应用于问答和其他文本生成任务中。

    

    当应用于问答和其他文本生成任务时，语言模型（LMs）可以通过生成式查询（通过从其输出分布中抽样答案）或判别式查询（通过使用它们对一组候选输出进行评分或排序）进行查询。这些过程有时会产生非常不同的预测。我们如何调和互不相容的评分过程以获得连贯的LM预测呢？我们引入一种新的、无需训练的、博弈论过程用于语言模型解码。我们的方法将语言模型解码视为一种规范化的不完美信息序列信号博弈 - 称为共识游戏 - 在该博弈中，一个生成器试图用自然语言句子传达一个抽象的正确性参数给一个判别器。我们开发了计算程序来找到这个博弈的近似均衡点，从而得到了一个我们称之为EQUILIBRIUM-RANKING的解码算法。应用于大量任务（包括阅读理解，常识）

    When applied to question answering and other text generation tasks, language models (LMs) may be queried generatively (by sampling answers from their output distribution) or discriminatively (by using them to score or rank a set of candidate outputs). These procedures sometimes yield very different predictions. How do we reconcile mutually incompatible scoring procedures to obtain coherent LM predictions? We introduce a new, a training-free, game-theoretic procedure for language model decoding. Our approach casts language model decoding as a regularized imperfect-information sequential signaling game - which we term the CONSENSUS GAME - in which a GENERATOR seeks to communicate an abstract correctness parameter using natural language sentences to a DISCRIMINATOR. We develop computational procedures for finding approximate equilibria of this game, resulting in a decoding algorithm we call EQUILIBRIUM-RANKING. Applied to a large number of tasks (including reading comprehension, commonsen
    
[^29]: 计算可分解模型之间的边际和条件差异及其应用

    Computing Marginal and Conditional Divergences between Decomposable Models with Applications. (arXiv:2310.09129v1 [cs.LG])

    [http://arxiv.org/abs/2310.09129](http://arxiv.org/abs/2310.09129)

    提出了一种计算可分解模型之间边际和条件差异的方法，能够在高维分布中精确计算差异，具有广泛的应用价值。

    

    在许多应用中，计算两个高维分布之间的精确差异是有用的，但直接计算是不可行的。对于两个可分解模型（即弦图马尔可夫网络）的联合分布计算α-β差异（包括Kullback-Leibler差异和Hellinger距离）可以在指数时间内成为这些模型的树宽度。然而，将两个高维对象之间的不相似性减少为单个标量值可能是不具有信息性的。此外，在诸如监督学习的应用中，对于条件分布的差异可能更有兴趣。因此，我们提出了一种方法来计算两个可分解模型的任何边际或条件分布之间的精确α-β差异。以可行的方式进行此计算是非平凡的，因为我们需要对这些分布之间的差异进行分解，因此需要对条件分布进行分解。

    The ability to compute the exact divergence between two high-dimensional distributions is useful in many applications but doing so naively is intractable. Computing the alpha-beta divergence -- a family of divergences that includes the Kullback-Leibler divergence and Hellinger distance -- between the joint distribution of two decomposable models, i.e chordal Markov networks, can be done in time exponential in the treewidth of these models. However, reducing the dissimilarity between two high-dimensional objects to a single scalar value can be uninformative. Furthermore, in applications such as supervised learning, the divergence over a conditional distribution might be of more interest. Therefore, we propose an approach to compute the exact alpha-beta divergence between any marginal or conditional distribution of two decomposable models. Doing so tractably is non-trivial as we need to decompose the divergence between these distributions and therefore, require a decomposition over the m
    
[^30]: 关于投影聚类的泛化界限

    On Generalization Bounds for Projective Clustering. (arXiv:2310.09127v1 [cs.LG])

    [http://arxiv.org/abs/2310.09127](http://arxiv.org/abs/2310.09127)

    本文研究了投影聚类的学习界限问题，给出了几个近乎最优的结果，并且对于基于中心的目标，展示了收敛速率为O(sqrt(k/n))。

    

    给定一组点，聚类是将点集分成k个簇的过程，使得每个点被分配到的中心尽可能接近。最常见的是将中心点设置为点本身，这导致了著名的k-median和k-means目标。也可以选择将中心点设置为j维子空间，从而产生子空间聚类。本文考虑了这些问题的学习界限。也就是说，给定一个从某个未知但固定分布D中独立抽取的n个样本P集合，P上计算的解如何快速收敛到D的最佳聚类？我们给出了几个近乎最优的结果。特别地，对于基于中心的目标，我们展示了收敛速率为O(sqrt(k/n))，与已知的最优界限[Fefferman, Mitter, and Narayanan, Journal of the Mathematical Society 2016]和[Bartlett, Linder, and Lugosi, IEEE Trans. Inf. Theory 1998]相一致。

    Given a set of points, clustering consists of finding a partition of a point set into $k$ clusters such that the center to which a point is assigned is as close as possible. Most commonly, centers are points themselves, which leads to the famous $k$-median and $k$-means objectives. One may also choose centers to be $j$ dimensional subspaces, which gives rise to subspace clustering. In this paper, we consider learning bounds for these problems. That is, given a set of $n$ samples $P$ drawn independently from some unknown, but fixed distribution $\mathcal{D}$, how quickly does a solution computed on $P$ converge to the optimal clustering of $\mathcal{D}$? We give several near optimal results. In particular,  For center-based objectives, we show a convergence rate of $\tilde{O}\left(\sqrt{{k}/{n}}\right)$. This matches the known optimal bounds of [Fefferman, Mitter, and Narayanan, Journal of the Mathematical Society 2016] and [Bartlett, Linder, and Lugosi, IEEE Trans. Inf. Theory 1998] fo
    
[^31]: 物理引导的噪声神经代理用于低光原始图像去噪

    Physics-guided Noise Neural Proxy for Low-light Raw Image Denoising. (arXiv:2310.09126v1 [eess.IV])

    [http://arxiv.org/abs/2310.09126](http://arxiv.org/abs/2310.09126)

    本文提出了一种新的物理引导噪声神经代理（PNNP）用于准确噪声建模和低光原始图像去噪，集成了物理引导噪声解耦、物理引导代理模型和可微分分布导向损失等高效技术。

    

    低光原始图像去噪在移动摄影中起着至关重要的作用，学习方法已成为主流方法。使用合成数据训练学习方法成为替代对应真实数据的高效实用方法。然而，合成数据的质量受噪声模型精度的限制，降低了低光原始图像去噪的性能。本文提出了一个新颖的准确噪声建模框架，学习一个从暗场中获得的物理引导噪声神经代理（PNNP）。PNNP集成了三种高效技术：物理引导噪声解耦（PND），物理引导代理模型（PPM）和可微分分布导向损失（DDL）。PND将暗场解耦为不同的组分，并以灵活的方式处理不同水平的噪声，降低了噪声神经代理的复杂度。PPM通过引入物理先验有效地约束生成的噪声。

    Low-light raw image denoising plays a crucial role in mobile photography, and learning-based methods have become the mainstream approach. Training the learning-based methods with synthetic data emerges as an efficient and practical alternative to paired real data. However, the quality of synthetic data is inherently limited by the low accuracy of the noise model, which decreases the performance of low-light raw image denoising. In this paper, we develop a novel framework for accurate noise modeling that learns a physics-guided noise neural proxy (PNNP) from dark frames. PNNP integrates three efficient techniques: physics-guided noise decoupling (PND), physics-guided proxy model (PPM), and differentiable distribution-oriented loss (DDL). The PND decouples the dark frame into different components and handles different levels of noise in a flexible manner, which reduces the complexity of the noise neural proxy. The PPM incorporates physical priors to effectively constrain the generated no
    
[^32]: 实时应用中的训练和预测视觉误差

    Training and Predicting Visual Error for Real-Time Applications. (arXiv:2310.09125v1 [cs.GR])

    [http://arxiv.org/abs/2310.09125](http://arxiv.org/abs/2310.09125)

    本论文研究了卷积神经网络在实时应用中训练和预测视觉误差的能力，通过预测重用着色或使用降低着色率导致的视觉误差，避免了对参考或渲染图像的依赖。

    

    视觉误差度量在量化感知图像相似性中起着基本作用。最近，在实时应用中出现了它们的使用案例，如内容自适应着色和着色重用以提高性能和改善效率。已经建立了各种不同的度量标准，其中最复杂的能够捕捉到人类视觉系统的感知特征。然而，它们的复杂性、计算成本和对参考图像或渲染图像的依赖限制了它们在实时场景中的广泛使用，使得这些应用只能使用最简单的度量标准。在这项工作中，我们探索了卷积神经网络在不需要参考或渲染图像的情况下预测各种视觉度量的能力。具体而言，我们训练和部署了一个神经网络来估计重用着色或使用降低着色率导致的视觉误差。最终模型可以准确地考虑到这些因素。

    Visual error metrics play a fundamental role in the quantification of perceived image similarity. Most recently, use cases for them in real-time applications have emerged, such as content-adaptive shading and shading reuse to increase performance and improve efficiency. A wide range of different metrics has been established, with the most sophisticated being capable of capturing the perceptual characteristics of the human visual system. However, their complexity, computational expense, and reliance on reference images to compare against prevent their generalized use in real-time, restricting such applications to using only the simplest available metrics. In this work, we explore the abilities of convolutional neural networks to predict a variety of visual metrics without requiring either reference or rendered images. Specifically, we train and deploy a neural network to estimate the visual error resulting from reusing shading or using reduced shading rates. The resulting models account
    
[^33]: 基于模拟的强化学习自动生成音乐播放列表

    Automatic Music Playlist Generation via Simulation-based Reinforcement Learning. (arXiv:2310.09123v1 [stat.ML])

    [http://arxiv.org/abs/2310.09123](http://arxiv.org/abs/2310.09123)

    本文提出了一个使用模拟环境的强化学习框架，通过直接优化用户满意度指标，实现个性化音乐播放列表的自动生成。我们使用修改版的深度Q网络训练出的策略能够从大型和动态的候选项目集中进行推荐，以最大化消费指标。

    

    个性化音乐播放列表是音乐流媒体服务中常见的功能，但传统的技术，如协同过滤，依赖于对内容质量的明确假设，以学习如何进行推荐。这些假设往往导致离线模型目标和在线用户满意度指标之间的不一致。在本文中，我们提出了一个强化学习框架，通过使用模拟的播放列表生成环境直接优化用户满意度指标，解决了这些限制。我们使用这个模拟器开发和训练了一个修改版的深度Q网络，称为AH-DQN，在处理我们的强化学习问题的大状态和动作空间时能够解决挑战。由此产生的策略能够从大型和动态的候选项目集中进行推荐，以最大化消费指标。我们通过使用环境模拟进行离线评估和分析代理。

    Personalization of playlists is a common feature in music streaming services, but conventional techniques, such as collaborative filtering, rely on explicit assumptions regarding content quality to learn how to make recommendations. Such assumptions often result in misalignment between offline model objectives and online user satisfaction metrics. In this paper, we present a reinforcement learning framework that solves for such limitations by directly optimizing for user satisfaction metrics via the use of a simulated playlist-generation environment. Using this simulator we develop and train a modified Deep Q-Network, the action head DQN (AH-DQN), in a manner that addresses the challenges imposed by the large state and action space of our RL formulation. The resulting policy is capable of making recommendations from large and dynamic sets of candidate items with the expectation of maximizing consumption metrics. We analyze and evaluate agents offline via simulations that use environmen
    
[^34]: DSG: 一种端到端文档结构生成器

    DSG: An End-to-End Document Structure Generator. (arXiv:2310.09118v1 [cs.LG])

    [http://arxiv.org/abs/2310.09118](http://arxiv.org/abs/2310.09118)

    DSG是一种端到端训练的文档解析系统，能够将渲染文档映射到结构化的层次格式，在实际应用中具有高效和灵活的特点。

    

    工业、研究和公共部门的信息通常存储为渲染文档（如PDF文件、扫描件）。因此，为了实现下游任务，需要将渲染文档映射到结构化的层次格式。然而，现有的系统在这一任务上被启发式方法所限制，且无法进行端到端的训练。在本文中，我们引入了一种名为 Document Structure Generator (DSG) 的新型文档解析系统，该系统可以进行完全端到端的训练。DSG结合了深度神经网络来解析（i）文档中的实体（如图像、文本块、标题等）和（ii）捕捉实体之间顺序和嵌套结构的关系。与依赖启发式方法的现有系统不同，我们的DSG进行了端到端的训练，使其在现实世界的应用中高效且灵活。我们还贡献了一个名为E-Periodica的新的大规模数据集，其中包含了具有复杂文档结构的实际杂志，用于评估。我们的结果表明...

    Information in industry, research, and the public sector is widely stored as rendered documents (e.g., PDF files, scans). Hence, to enable downstream tasks, systems are needed that map rendered documents onto a structured hierarchical format. However, existing systems for this task are limited by heuristics and are not end-to-end trainable. In this work, we introduce the Document Structure Generator (DSG), a novel system for document parsing that is fully end-to-end trainable. DSG combines a deep neural network for parsing (i) entities in documents (e.g., figures, text blocks, headers, etc.) and (ii) relations that capture the sequence and nested structure between entities. Unlike existing systems that rely on heuristics, our DSG is trained end-to-end, making it effective and flexible for real-world applications. We further contribute a new, large-scale dataset called E-Periodica comprising real-world magazines with complex document structures for evaluation. Our results demonstrate th
    
[^35]: 无监督机器学习和XAI在超越人类能力的规模上洞察历史来源的分析

    Insightful analysis of historical sources at scales beyond human capabilities using unsupervised Machine Learning and XAI. (arXiv:2310.09091v1 [cs.LG])

    [http://arxiv.org/abs/2310.09091](http://arxiv.org/abs/2310.09091)

    本研究使用创新的机器学习技术对大规模历史文献进行分析，并重点研究了“Sacrobosco Collection”中知识的演变。通过这一研究，我们得出了一些重要的历史洞察。

    

    历史资料丰富，但如何将人类知识的演变和传播进行整合，无论是时间上还是空间上的，都是一个具有挑战性的问题，目前只能进行有限的选择性研究。巨大的历史资料量使得全面研究成为不可能，因为人类专家的数量有限。然而，随着大量历史资料以数字形式可获得，AI辅助历史分析有了希望。在这项工作中，我们采用创新的机器学习技术对大规模历史文献进行分析，实现了深入的历史洞察。我们的研究重点是“Sacrobosco Collection”中的知识演变，这是一个包含359个早期现代印刷版天文学教科书的数字化收藏品，这些教科书在1472年至1650年间在欧洲大学使用，大约有76,000页，其中许多包含天文和计算表格。通过机器学习分析这一收藏品，我们发现了一些重要的洞察。

    Historical materials are abundant. Yet, piecing together how human knowledge has evolved and spread both diachronically and synchronically remains a challenge that can so far only be very selectively addressed. The vast volume of materials precludes comprehensive studies, given the restricted number of human specialists. However, as large amounts of historical materials are now available in digital form there is a promising opportunity for AI-assisted historical analysis. In this work, we take a pivotal step towards analyzing vast historical corpora by employing innovative machine learning (ML) techniques, enabling in-depth historical insights on a grand scale. Our study centers on the evolution of knowledge within the `Sacrobosco Collection' -- a digitized collection of 359 early modern printed editions of textbooks on astronomy used at European universities between 1472 and 1650 -- roughly 76,000 pages, many of which contain astronomic, computational tables. An ML based analysis of t
    
[^36]: 在线的拼车服务重定位和匹配: 一种基于模型的模块化方法

    Online Relocating and Matching of Ride-Hailing Services: A Model-Based Modular Approach. (arXiv:2310.09071v1 [cs.LG])

    [http://arxiv.org/abs/2310.09071](http://arxiv.org/abs/2310.09071)

    本研究提出了一种创新的基于模型的模块化方法，用于动态优化拼车平台中的订单匹配和车辆重定位。该方法通过两层和模块化的建模结构进行车流转移模式和快速匹配重定位，并证明了在特定网络中能够达到全局最优解，实验结果显示该方法具有优越的系统性能。

    

    本研究提出了一种创新的基于模型的模块化方法(MMA)，用于在拼车平台中动态优化订单匹配和车辆重定位。MMA利用了一个两层和模块化的建模结构。上层确定系统内车流的空间转移模式，以最大化当前和未来阶段的总收入。在上层的指导下，下层进行了快速的车辆-订单匹配和车辆重定位。MMA具有可解释性，并配备了定制的多项式时间算法，作为一种在线订单匹配和车辆重定位算法，可以扩展到数千辆车辆。我们在简化网络上理论上证明了所提出的算法能够实现全局最优解，而基于玩具网络和实际数据集的数值实验表明，与批量匹配和强化匹配相比，MMA能够实现更优异的系统性能。

    This study proposes an innovative model-based modular approach (MMA) to dynamically optimize order matching and vehicle relocation in a ride-hailing platform. MMA utilizes a two-layer and modular modeling structure. The upper layer determines the spatial transfer patterns of vehicle flow within the system to maximize the total revenue of the current and future stages. With the guidance provided by the upper layer, the lower layer performs rapid vehicle-to-order matching and vehicle relocation. MMA is interpretable, and equipped with the customized and polynomial-time algorithm, which, as an online order-matching and vehicle-relocation algorithm, can scale past thousands of vehicles. We theoretically prove that the proposed algorithm can achieve the global optimum in stylized networks, while the numerical experiments based on both the toy network and realistic dataset demonstrate that MMA is capable of achieving superior systematic performance compared to batch matching and reinforcemen
    
[^37]: KCTS：带有令牌级幻觉检测的知识约束树搜索解码

    KCTS: Knowledge-Constrained Tree Search Decoding with Token-Level Hallucination Detection. (arXiv:2310.09044v1 [cs.CL])

    [http://arxiv.org/abs/2310.09044](http://arxiv.org/abs/2310.09044)

    提出了一种名为KCTS的知识约束树搜索解码方法，利用知识分类器和MCTS指导冻结的LM生成与参考知识对齐的文本，同时引入了一种新颖的令牌级幻觉检测方法RIPA。

    

    大型语言模型（LLM）展示了卓越的人类级自然语言生成能力。然而，它们产生错误信息的潜力，即所谓的幻觉问题，对其部署构成重大风险。解决这个问题的一种常见方法是检索相关知识，并使用输入中的知识对LLM进行精细调节。不幸的是，这种方法会引起高训练成本，并可能对多任务模型造成灾难性遗忘。为了克服这些局限性，我们提出了一种称为KCTS（知识约束树搜索）的知识约束解码方法，它使用知识分类器得分和MCTS（蒙特卡罗树搜索）来指导冻结的LM在每个解码步骤中生成与参考知识对齐的文本。为了将序列级知识分类器适应令牌级指导，我们还提出了一种新颖的令牌级幻觉检测方法，称为RIPA（奖励拐点近似）。

    Large Language Models (LLMs) have demonstrated remarkable human-level natural language generation capabilities. However, their potential to generate misinformation, often called the hallucination problem, poses a significant risk to their deployment. A common approach to address this issue is to retrieve relevant knowledge and fine-tune the LLM with the knowledge in its input. Unfortunately, this method incurs high training costs and may cause catastrophic forgetting for multi-tasking models. To overcome these limitations, we propose a knowledge-constrained decoding method called KCTS (Knowledge-Constrained Tree Search), which guides a frozen LM to generate text aligned with the reference knowledge at each decoding step using a knowledge classifier score and MCTS (Monte-Carlo Tree Search). To adapt the sequence-level knowledge classifier to token-level guidance, we also propose a novel token-level hallucination detection method called RIPA (Reward Inflection Point Approximation). Our e
    
[^38]: 考虑终端用户灵活性的深度强化学习电动车充电优化调度

    Optimal Scheduling of Electric Vehicle Charging with Deep Reinforcement Learning considering End Users Flexibility. (arXiv:2310.09040v1 [cs.LG])

    [http://arxiv.org/abs/2310.09040](http://arxiv.org/abs/2310.09040)

    本研究使用深度强化学习方法，针对时间段电价计划，通过优化调度电动车充电过程来降低用户家庭的成本。

    

    分布式能源资源（特别是电动汽车）的快速增长预计在未来十年内将大幅增加对现有电力分配网络的压力，增加了对更高系统可靠性和灵活性的需求。为了避免不必要的网络投资并增加对分配网络的可控性，网络运营商开展需求响应（DR）计划，以鼓励终端用户在回报经济或其他利益的前提下转移其用电消耗。人工智能（AI）方法在住宅负荷调度应用领域处于研究前沿，主要由于其高精度性、高计算速度和较低对于正在开发的模型的物理特性的依赖性。本研究的目标是在时间段电价计划下，利用深度强化学习方法确定用户家庭电动车减少成本的充电策略。

    The rapid growth of decentralized energy resources and especially Electric Vehicles (EV), that are expected to increase sharply over the next decade, will put further stress on existing power distribution networks, increasing the need for higher system reliability and flexibility. In an attempt to avoid unnecessary network investments and to increase the controllability over distribution networks, network operators develop demand response (DR) programs that incentivize end users to shift their consumption in return for financial or other benefits. Artificial intelligence (AI) methods are in the research forefront for residential load scheduling applications, mainly due to their high accuracy, high computational speed and lower dependence on the physical characteristics of the models under development. The aim of this work is to identify households' EV cost-reducing charging policy under a Time-of-Use tariff scheme, with the use of Deep Reinforcement Learning, and more specifically Deep
    
[^39]: MINDE: 互信息神经扩散估计

    MINDE: Mutual Information Neural Diffusion Estimation. (arXiv:2310.09031v1 [cs.LG])

    [http://arxiv.org/abs/2310.09031](http://arxiv.org/abs/2310.09031)

    MINDE是一种基于得分函数扩散模型的新方法，用于估计随机变量之间的互信息。该方法在准确性和自一致性测试方面优于其他文献中的主要替代方法。

    

    在这项工作中，我们提出了一种估计随机变量之间互信息（MI）的新方法。我们的方法基于Girsanov定理的原创解释，允许我们使用基于得分的扩散模型来估计两个密度函数之间的Kullback-Leibler散度，该估计是它们得分函数之间的差异。作为副产品，我们的方法还能够估计随机变量的熵。借助这样的构建模块，我们提出了一种通用的测量MI的方法，该方法分为两个方向展开：一个使用条件扩散过程，另一个使用联合扩散过程，可以同时对两个随机变量进行建模。我们的结果来自于对我们方法的各种变体进行彻底的实验协议，表明我们的方法比文献中的主要替代方法更准确，尤其是对于具有挑战性的分布。此外，我们的方法通过了MI自一致性测试，包括...

    In this work we present a new method for the estimation of Mutual Information (MI) between random variables. Our approach is based on an original interpretation of the Girsanov theorem, which allows us to use score-based diffusion models to estimate the Kullback Leibler divergence between two densities as a difference between their score functions. As a by-product, our method also enables the estimation of the entropy of random variables. Armed with such building blocks, we present a general recipe to measure MI, which unfolds in two directions: one uses conditional diffusion process, whereas the other uses joint diffusion processes that allow simultaneous modelling of two random variables. Our results, which derive from a thorough experimental protocol over all the variants of our approach, indicate that our method is more accurate than the main alternatives from the literature, especially for challenging distributions. Furthermore, our methods pass MI self-consistency tests, includin
    
[^40]: 少样本学习的子空间适应先验

    Subspace Adaptation Prior for Few-Shot Learning. (arXiv:2310.09028v1 [cs.LG])

    [http://arxiv.org/abs/2310.09028](http://arxiv.org/abs/2310.09028)

    提出了少样本学习的子空间适应先验算法，通过同时学习初始化参数和参数子空间，可以基于任务分布决定使用梯度下降调整哪些操作子集，从而提高学习效率并降低过拟合风险。

    

    梯度基于的元学习技术旨在从一系列训练任务中提取有用的先验知识，以便使用梯度下降更高效地学习新任务。尽管这些方法在各种情况下取得了成功，但它们通常在学习新任务时适应可训练层的所有参数。这忽略了对于给定任务分布来说可能更高效的学习策略，并且可能容易过拟合，特别是在少样本学习中，其中必须从有限数量的示例中学习任务。为了解决这些问题，我们提出了子空间适应先验(SAP)，这是一种新颖的基于梯度的元学习算法，它同时学习良好的初始化参数(先验知识)和参数子空间，以操作子集的形式表示应该是可适应的。通过这种方式，SAP可以根据潜在的任务分布学习应该使用梯度下降调整的操作子集，同时降低过拟合的风险。

    Gradient-based meta-learning techniques aim to distill useful prior knowledge from a set of training tasks such that new tasks can be learned more efficiently with gradient descent. While these methods have achieved successes in various scenarios, they commonly adapt all parameters of trainable layers when learning new tasks. This neglects potentially more efficient learning strategies for a given task distribution and may be susceptible to overfitting, especially in few-shot learning where tasks must be learned from a limited number of examples. To address these issues, we propose Subspace Adaptation Prior (SAP), a novel gradient-based meta-learning algorithm that jointly learns good initialization parameters (prior knowledge) and layer-wise parameter subspaces in the form of operation subsets that should be adaptable. In this way, SAP can learn which operation subsets to adjust with gradient descent based on the underlying task distribution, simultaneously decreasing the risk of over
    
[^41]: 基于表示编码的联邦元学习在少样本故障诊断中的应用

    Federated Meta-Learning for Few-Shot Fault Diagnosis with Representation Encoding. (arXiv:2310.09002v1 [cs.LG])

    [http://arxiv.org/abs/2310.09002](http://arxiv.org/abs/2310.09002)

    本文提出了一种基于表示编码的联邦元学习框架 (REFML) 用于少样本故障诊断，通过利用训练客户端之间的异质性并使用自适应插值方法来提高模型的泛化能力。

    

    基于深度学习的故障诊断方法需要大量的训练数据，然而由于分布在不同实体中，这些数据很难获取。联邦学习可以保障数据隐私的前提下，使多个客户端 collaboratively 训练共享模型。然而，客户端之间的领域差异和数据稀缺问题会降低全局联邦学习模型的性能。为了解决这些问题，我们提出了一种新颖的基于表示编码的联邦元学习 (REFML) 框架用于少样本故障诊断。首先，我们开发了一种基于表示编码和元学习的新型训练策略，它充分利用了训练客户端之间的异质性，将其有效地转化为对未见工况或设备类型的越域泛化的优势。此外，我们还提出了一种自适应插值方法，用于计算局部模型和全局模型的最优组合，作为局部训练的初始化。

    Deep learning-based fault diagnosis (FD) approaches require a large amount of training data, which are difficult to obtain since they are located across different entities. Federated learning (FL) enables multiple clients to collaboratively train a shared model with data privacy guaranteed. However, the domain discrepancy and data scarcity problems among clients deteriorate the performance of the global FL model. To tackle these issues, we propose a novel framework called representation encoding-based federated meta-learning (REFML) for few-shot FD. First, a novel training strategy based on representation encoding and meta-learning is developed. It harnesses the inherent heterogeneity among training clients, effectively transforming it into an advantage for out-of-distribution generalization on unseen working conditions or equipment types. Additionally, an adaptive interpolation method that calculates the optimal combination of local and global models as the initialization of local tra
    
[^42]: 在在线环境中衡量过程结果预测的稳定性

    Measuring the Stability of Process Outcome Predictions in Online Settings. (arXiv:2310.09000v1 [cs.LG])

    [http://arxiv.org/abs/2310.09000](http://arxiv.org/abs/2310.09000)

    该论文提出了一个评估框架，用于衡量在线预测性过程监控模型的稳定性。该框架引入了四个性能元度量：显著性能下降的频率、这些下降的幅度、恢复率和性能的波动。

    

    预测性过程监控旨在使用历史事件数据预测过程实例的未来进展。随着预测性过程监控在在线环境中越来越多地应用于及时干预，评估底层模型的性能变得至关重要，以确保其随时间的一致性和可靠性。这在高风险的商业场景中尤为重要，因为错误的预测可能会产生严重后果。然而，目前通常使用单个汇总值或时间序列可视化来评估预测模型，这使得评估其性能特别是随时间的稳定性变得具有挑战性。本文提出了一个评估框架，用于评估在线预测性过程监控模型的稳定性。该框架引入了四个性能元度量：显著性能下降的频率，这些下降的幅度，恢复率和性能的波动。

    Predictive Process Monitoring aims to forecast the future progress of process instances using historical event data. As predictive process monitoring is increasingly applied in online settings to enable timely interventions, evaluating the performance of the underlying models becomes crucial for ensuring their consistency and reliability over time. This is especially important in high risk business scenarios where incorrect predictions may have severe consequences. However, predictive models are currently usually evaluated using a single, aggregated value or a time-series visualization, which makes it challenging to assess their performance and, specifically, their stability over time. This paper proposes an evaluation framework for assessing the stability of models for online predictive process monitoring. The framework introduces four performance meta-measures: the frequency of significant performance drops, the magnitude of such drops, the recovery rate, and the volatility of perfor
    
[^43]: 路线重新规划预测服务

    Reroute Prediction Service. (arXiv:2310.08988v1 [cs.LG])

    [http://arxiv.org/abs/2310.08988](http://arxiv.org/abs/2310.08988)

    该论文介绍了一种新颖的数据分析和机器学习系统，旨在通过积极支持重新规划决策来减少航班延误。该系统使用历史重新规划数据和天气数据预测未来几天是否会发布重新规划建议，以减少影响航线的因素。

    

    仅在2019年，美国国家航空空间系统的延误成本就估计为330亿美元，这是过去几年增长趋势的峰值。为了解决这种巨大的低效率问题，我们设计和开发了一种新的数据分析和机器学习系统，旨在通过积极支持重新规划决策来减少延误。该系统在未来几天的时间范围内，预测某个空中管制区域或某个指定的咨询标识是否会发布重新规划建议，从而可能影响相关路线。为了提供这些预测，该系统使用从FAA提供的系统范围信息管理（SWIM）数据服务和美国国家环境预测中心（NCEP）提供的天气数据收集的历史重新规划数据。这些数据量庞大，包含许多以高速率流式传输的不相关和噪声数据。系统持续处理进入的原始数据。

    The cost of delays was estimated as 33 billion US dollars only in 2019 for the US National Airspace System, a peak value following a growth trend in past years. Aiming to address this huge inefficiency, we designed and developed a novel Data Analytics and Machine Learning system, which aims at reducing delays by proactively supporting re-routing decisions.  Given a time interval up to a few days in the future, the system predicts if a reroute advisory for a certain Air Route Traffic Control Center or for a certain advisory identifier will be issued, which may impact the pertinent routes. To deliver such predictions, the system uses historical reroute data, collected from the System Wide Information Management (SWIM) data services provided by the FAA, and weather data, provided by the US National Centers for Environmental Prediction (NCEP). The data is huge in volume, and has many items streamed at high velocity, uncorrelated and noisy. The system continuously processes the incoming raw
    
[^44]: PAGE: 在联邦学习中平衡个性化和泛化

    PAGE: Equilibrate Personalization and Generalization in Federated Learning. (arXiv:2310.08961v1 [cs.LG])

    [http://arxiv.org/abs/2310.08961](http://arxiv.org/abs/2310.08961)

    本文提出了一种使用博弈论平衡个性化和泛化的联邦学习算法PAGE，通过将联邦学习视为客户和服务器之间的合作竞争博弈，并结合马尔科夫决策过程和强化学习算法来寻找平衡点。

    

    联邦学习（FL）正在成为机器学习作为服务的主要推动力，其中客户（客户端）在服务提供商（服务器）的协调下共同从共享的本地更新中受益。考虑到数据异质性的负面效应，研究人员分别调查了客户的当前需求和服务器的未来需求，即本地模型个性化和全局模型泛化，而不是一个取代另一个。然而，这两个看似竞争的目标都同样重要，而不是黑白问题，应该同时实现。在本文中，我们提出了基于博弈论的第一个平衡个性化和泛化的算法，称为PAGE。为了探索均衡，PAGE将博弈进一步形式化为马尔科夫决策过程，并利用强化学习算法简化了解决方法。

    Federated learning (FL) is becoming a major driving force behind machine learning as a service, where customers (clients) collaboratively benefit from shared local updates under the orchestration of the service provider (server). Representing clients' current demands and the server's future demand, local model personalization and global model generalization are separately investigated, as the ill-effects of data heterogeneity enforce the community to focus on one over the other. However, these two seemingly competing goals are of equal importance rather than black and white issues, and should be achieved simultaneously. In this paper, we propose the first algorithm to balance personalization and generalization on top of game theory, dubbed PAGE, which reshapes FL as a co-opetition game between clients and the server. To explore the equilibrium, PAGE further formulates the game as Markov decision processes, and leverages the reinforcement learning algorithm, which simplifies the solving
    
[^45]: CAMELL：基于置信度的高效自监督主动学习与标签验证获取模型

    CAMELL: Confidence-based Acquisition Model for Efficient Self-supervised Active Learning with Label Validation. (arXiv:2310.08944v1 [cs.CL])

    [http://arxiv.org/abs/2310.08944](http://arxiv.org/abs/2310.08944)

    CAMELL是一个适用于序列多输出问题的主动学习框架，通过仅需专家标注序列的一小部分、自监督和标签验证机制来解决监督神经方法对大规模标注数据集的依赖限制。

    

    在序列任务中，受大规模且精确标注数据集的依赖限制，监督神经方法受到阻碍。标注质量随着从专家标注向众包标注的转变而逐渐恶化。为了解决这些挑战，我们提出了CAMELL（Confidence-based Acquisition Model for Efficient self-supervised active Learning with Label validation），这是一个针对序列多输出问题量身定制的基于池化的主动学习框架。CAMELL具有三个核心特点：(1)仅要求专家标注所选序列的一小部分，(2)为其余序列提供自监督，(3)采用标签验证机制，防止错误标签污染数据集并影响模型性能。我们在序列任务中对CAMELL进行了评估，特别强调对话信念跟踪，这是一个受限制的任务。

    Supervised neural approaches are hindered by their dependence on large, meticulously annotated datasets, a requirement that is particularly cumbersome for sequential tasks. The quality of annotations tends to deteriorate with the transition from expert-based to crowd-sourced labelling. To address these challenges, we present \textbf{CAMELL} (Confidence-based Acquisition Model for Efficient self-supervised active Learning with Label validation), a pool-based active learning framework tailored for sequential multi-output problems. CAMELL possesses three core features: (1) it requires expert annotators to label only a fraction of a chosen sequence, (2) it facilitates self-supervision for the remainder of the sequence, and (3) it employs a label validation mechanism to prevent erroneous labels from contaminating the dataset and harming model performance. We evaluate CAMELL on sequential tasks, with a special emphasis on dialogue belief tracking, a task plagued by the constraints of limited
    
[^46]: LLaMA Rider：推动大型语言模型探索开放世界

    LLaMA Rider: Spurring Large Language Models to Explore the Open World. (arXiv:2310.08922v1 [cs.LG])

    [http://arxiv.org/abs/2310.08922](http://arxiv.org/abs/2310.08922)

    本文提出了一种方法，通过多轮反馈-修订机制和子任务重标记，推动大型语言模型在开放世界中探索并提高其任务解决能力。

    

    近期，多项研究利用大型语言模型(LLMs)在环境中帮助决策和规划，并尝试将LLMs的知识与世界条件对齐。然而，LLMs在持续获取环境知识并在开放世界中适应的能力仍然不确定。在本文中，我们提出了一种方法来推动LLMs探索开放世界，收集经验，并学习提高其任务解决能力。在这种方法中，利用多轮反馈-修订机制，鼓励LLMs根据环境的反馈信息主动选择适当的修订操作，促进探索并增强模型的性能。此外，我们整合了子任务重标记，以帮助LLMs在子任务规划中保持一致性，并帮助模型学习任务之间的组合性质，使其能够通过基于所获得的探索经验的训练完成更广泛的任务。

    Recently, various studies have leveraged Large Language Models (LLMs) to help decision-making and planning in environments, and try to align the LLMs' knowledge with the world conditions. Nonetheless, the capacity of LLMs to continuously acquire environmental knowledge and adapt in an open world remains uncertain. In this paper, we propose an approach to spur LLMs to explore the open world, gather experiences, and learn to improve their task-solving capabilities. In this approach, a multi-round feedback-revision mechanism is utilized to encourage LLMs to actively select appropriate revision actions guided by feedback information from the environment. This facilitates exploration and enhances the model's performance. Besides, we integrate sub-task relabeling to assist LLMs in maintaining consistency in sub-task planning and help the model learn the combinatorial nature between tasks, enabling it to complete a wider range of tasks through training based on the acquired exploration experi
    
[^47]: 羞人简单的文字水印

    Embarrassingly Simple Text Watermarks. (arXiv:2310.08920v1 [cs.LG])

    [http://arxiv.org/abs/2310.08920](http://arxiv.org/abs/2310.08920)

    我们提出了一种极其简单但非常有效的文本水印方法Easymark，它可以在不改变文本含义的情况下注入水印，并且能够高度可信地检测出文本是否由采用该方法的系统生成。这种方法不需要访问大型语言模型，并且具有更高的检测准确度和BLEU分数。

    

    我们提出了Easymark，这是一种极其简单但非常有效的水印方法。随着大型语言模型（LLM）的出现，文本水印变得越来越重要。LLM可以生成与人类写作的文本无法区分的文本，这对文本的可信度是一个严重的问题。Easymark是这个问题的一个简单而有效的解决方案。Easymark可以在不改变文本含义的情况下注入水印，而验证器可以高度可信地检测出文本是否由采用Easymark的系统生成。Easymark非常容易实现，只需要几行代码。Easymark不需要访问LLMs，因此当LLM提供者不提供带水印的LLM时，可以在用户端实施。尽管它很简单，但它能够实现比最先进的文本水印方法更高的检测准确度和BLEU分数。我们还证明了完美水印的不可能定理。

    We propose Easymark, a family of embarrassingly simple yet effective watermarks. Text watermarking is becoming increasingly important with the advent of Large Language Models (LLM). LLMs can generate texts that cannot be distinguished from human-written texts. This is a serious problem for the credibility of the text. Easymark is a simple yet effective solution to this problem. Easymark can inject a watermark without changing the meaning of the text at all while a validator can detect if a text was generated from a system that adopted Easymark or not with high credibility. Easymark is extremely easy to implement so that it only requires a few lines of code. Easymark does not require access to LLMs, so it can be implemented on the user-side when the LLM providers do not offer watermarked LLMs. In spite of its simplicity, it achieves higher detection accuracy and BLEU scores than the state-of-the-art text watermarking methods. We also prove the impossibility theorem of perfect watermarki
    
[^48]: 知识图谱嵌入的关系感知集成学习

    Relation-aware Ensemble Learning for Knowledge Graph Embedding. (arXiv:2310.08917v1 [cs.LG])

    [http://arxiv.org/abs/2310.08917](http://arxiv.org/abs/2310.08917)

    本论文提出了一种关系感知集成学习方法，用于知识图谱嵌入任务，并通过分割搜索合并的算法在搜索关系感知集成权重方面取得了显著性能提升。

    

    知识图谱嵌入是自然语言处理中的基础任务，已经提出了各种方法来探索不同方式的语义模式。本文提出了一种关系感知集成学习方法，通过利用现有方法来学习一个集成模型。然而，使用关系感知集成探索这些语义会导致比一般集成方法更大的搜索空间。为了解决这个问题，我们提出了一个分割搜索合并的算法RelEns-DSC，它独立地搜索关系感知集成的权重。该算法具有与一般集成方法相同的计算成本，但性能更好。在基准数据集上的实验结果表明了所提方法在高效搜索关系感知集成权重和达到最先进的嵌入性能方面的有效性。代码公开在https://github.com/LARS-research/RelEns。

    Knowledge graph (KG) embedding is a fundamental task in natural language processing, and various methods have been proposed to explore semantic patterns in distinctive ways. In this paper, we propose to learn an ensemble by leveraging existing methods in a relation-aware manner. However, exploring these semantics using relation-aware ensemble leads to a much larger search space than general ensemble methods. To address this issue, we propose a divide-search-combine algorithm RelEns-DSC that searches the relation-wise ensemble weights independently. This algorithm has the same computation cost as general ensemble methods but with much better performance. Experimental results on benchmark datasets demonstrate the effectiveness of the proposed method in efficiently searching relation-aware ensemble weights and achieving state-of-the-art embedding performance. The code is public at https://github.com/LARS-research/RelEns.
    
[^49]: 在规模化的多任务和多领域学习中的标量化方法

    Scalarization for Multi-Task and Multi-Domain Learning at Scale. (arXiv:2310.08910v1 [cs.LG])

    [http://arxiv.org/abs/2310.08910](http://arxiv.org/abs/2310.08910)

    通过标量化方法在多任务和多领域学习中训练单个模型可以提高模型效率和准确性，并实现跨任务/领域的正向知识转移。该研究通过大规模分析多领域和多任务学习的动态性来更好地理解标量化方法的训练动态。

    

    在多个输入领域和/或输出任务上训练单个模型可以将多个信息源压缩为统一的主干，从而提高模型效率。它还可以实现任务/领域之间的正向知识转移，从而提高准确性和数据有效性。然而，优化这样的网络是一个挑战，特别是由于不同任务或领域之间的差异：尽管多年来提出了几个假设和解决方案，但最近的研究表明，统一的标量化训练，即简单地最小化任务损失的平均值，与更昂贵的最新优化方法相比，性能相当。这引发了我们对多任务和多领域网络的训练动态理解程度的问题。在这项工作中，我们首先设计了一个大规模的统一分析多领域和多任务学习，以更好地理解标量化在各种任务/领域组合中的动态性。

    Training a single model on multiple input domains and/or output tasks allows for compressing information from multiple sources into a unified backbone hence improves model efficiency. It also enables potential positive knowledge transfer across tasks/domains, leading to improved accuracy and data-efficient training. However, optimizing such networks is a challenge, in particular due to discrepancies between the different tasks or domains: Despite several hypotheses and solutions proposed over the years, recent work has shown that uniform scalarization training, i.e., simply minimizing the average of the task losses, yields on-par performance with more costly SotA optimization methods. This raises the issue of how well we understand the training dynamics of multi-task and multi-domain networks. In this work, we first devise a large-scale unified analysis of multi-domain and multi-task learning to better understand the dynamics of scalarization across varied task/domain combinations and 
    
[^50]: 通过深度强化学习将社区成员隐藏作为反事实图搜索

    Community Membership Hiding as Counterfactual Graph Search via Deep Reinforcement Learning. (arXiv:2310.08909v1 [cs.SI])

    [http://arxiv.org/abs/2310.08909](http://arxiv.org/abs/2310.08909)

    这项研究通过深度强化学习的方式解决了社区成员隐藏的挑战，通过战略地改变网络图的结构属性，防止节点被社区检测算法识别出来，并验证了方法的有效性。

    

    社区检测是社交媒体平台发现彼此紧密联系的用户群体的有用工具，他们共享共同的兴趣。然而，这种功能往往会以可能暴露个人隐私为代价，无意中透露他们的品味或偏好。因此，一些用户可能希望保护他们的匿名性，并出于各种原因选择退出社区检测，例如与政治或宗教组织的关联。在这项研究中，我们解决了社区成员隐藏的挑战，它涉及战略性地改变网络图的结构属性，以防止一个或多个节点被给定的社区检测算法识别出来。我们通过制定一个受限的反事实图目标，并通过深度强化学习来解决这个问题。我们通过两个不同的任务来验证我们方法的有效性：节点和社区欺骗。

    Community detection techniques are useful tools for social media platforms to discover tightly connected groups of users who share common interests. However, this functionality often comes at the expense of potentially exposing individuals to privacy breaches by inadvertently revealing their tastes or preferences. Therefore, some users may wish to safeguard their anonymity and opt out of community detection for various reasons, such as affiliation with political or religious organizations.  In this study, we address the challenge of community membership hiding, which involves strategically altering the structural properties of a network graph to prevent one or more nodes from being identified by a given community detection algorithm. We tackle this problem by formulating it as a constrained counterfactual graph objective, and we solve it via deep reinforcement learning. We validate the effectiveness of our method through two distinct tasks: node and community deception. Extensive exper
    
[^51]: 自监督卷积核手工特征的特色融合：增强超声心动图左室高血压病变表型的识别

    Self supervised convolutional kernel based handcrafted feature harmonization: Enhanced left ventricle hypertension disease phenotyping on echocardiography. (arXiv:2310.08897v1 [eess.IV])

    [http://arxiv.org/abs/2310.08897](http://arxiv.org/abs/2310.08897)

    本研究提出了一种自监督卷积核手工特征融合方法，用于增强超声心动图左室高血压病变的识别。通过将卷积滤波器应用于自监督学习预处理中，将图像转换为特征图，实现了在不同成像设备和协议下的一致特征提取。

    

    放射学特征学是一种通过图像提取定量手工特征来预测疾病的医学成像技术。在这些特征中进行融合，可以确保在不同的成像设备和协议中进行一致的特征提取。融合的方法包括标准化成像协议、统计调整和评估特征的稳健性。通过超声心动图可以诊断心肌疾病，如左室肥厚(LVH)和高血压心脏病(HHD)，但不同的成像设置会带来挑战。在这种情况下，特征融合技术对于在疾病诊断中应用手工特征至关重要。自监督学习(SSl)通过限制的数据集增强数据理解，并适应多样的数据设置。ConvNeXt-V2将卷积层集成到SSL中，在各种任务中展现出优越的性能。本研究侧重于SSL中的卷积滤波器，将它们用作预处理，将图像转换为特征图。

    Radiomics, a medical imaging technique, extracts quantitative handcrafted features from images to predict diseases. Harmonization in those features ensures consistent feature extraction across various imaging devices and protocols. Methods for harmonization include standardized imaging protocols, statistical adjustments, and evaluating feature robustness. Myocardial diseases such as Left Ventricular Hypertrophy (LVH) and Hypertensive Heart Disease (HHD) are diagnosed via echocardiography, but variable imaging settings pose challenges. Harmonization techniques are crucial for applying handcrafted features in disease diagnosis in such scenario. Self-supervised learning (SSL) enhances data understanding within limited datasets and adapts to diverse data settings. ConvNeXt-V2 integrates convolutional layers into SSL, displaying superior performance in various tasks. This study focuses on convolutional filters within SSL, using them as preprocessing to convert images into feature maps for h
    
[^52]: EHI: 高效密集检索的层次索引的端到端学习

    EHI: End-to-end Learning of Hierarchical Index for Efficient Dense Retrieval. (arXiv:2310.08891v1 [cs.LG])

    [http://arxiv.org/abs/2310.08891](http://arxiv.org/abs/2310.08891)

    EHI是一种端到端学习的层次索引方法，用于高效密集检索。它同时学习嵌入和ANNS结构，通过使用密集路径嵌入来捕获索引的语义信息，以优化检索性能。

    

    密集嵌入式检索现已成为语义搜索和排名问题的行业标准，如获取给定查询的相关网络文档。这些技术使用了两个阶段的过程：(a)对比学习来训练双编码器以嵌入查询和文档，以及(b)近似最近邻搜索(ANNS)以查找给定查询的相似文档。这两个阶段是不相交的；学得的嵌入可能不适合ANNS方法，反之亦然，导致性能不佳。在这项工作中，我们提出了一种名为端到端层次索引(EHI)的方法，它同时学习嵌入和ANNS结构以优化检索性能。EHI使用标准的双编码器模型来嵌入查询和文档，同时学习一个倒排文件索引(IVF)风格的树状结构以实现高效的ANNS。为了确保离散基于树的ANNS结构的稳定和高效学习，EHI引入了密集路径嵌入的概念，用来捕获索引的语义信息。

    Dense embedding-based retrieval is now the industry standard for semantic search and ranking problems, like obtaining relevant web documents for a given query. Such techniques use a two-stage process: (a) contrastive learning to train a dual encoder to embed both the query and documents and (b) approximate nearest neighbor search (ANNS) for finding similar documents for a given query. These two stages are disjoint; the learned embeddings might be ill-suited for the ANNS method and vice-versa, leading to suboptimal performance. In this work, we propose End-to-end Hierarchical Indexing -- EHI -- that jointly learns both the embeddings and the ANNS structure to optimize retrieval performance. EHI uses a standard dual encoder model for embedding queries and documents while learning an inverted file index (IVF) style tree structure for efficient ANNS. To ensure stable and efficient learning of discrete tree-based ANNS structure, EHI introduces the notion of dense path embedding that capture
    
[^53]: METRA:具有度量感知抽象的可扩展无监督强化学习

    METRA: Scalable Unsupervised RL with Metric-Aware Abstraction. (arXiv:2310.08887v1 [cs.LG])

    [http://arxiv.org/abs/2310.08887](http://arxiv.org/abs/2310.08887)

    METRA提出了一种新的无监督强化学习目标，旨在使其在复杂的高维环境中可扩展。这个目标解决了纯探索方法在大状态空间环境中的困难以及互信息技能学习方法中缺乏激励而无法探索环境的问题。

    

    无监督预训练策略在自然语言处理和计算机视觉领域证明了其高效性。同样，无监督强化学习（RL）有望发现各种潜在有用的行为，可以加速学习各种下游任务。然而，尽管之前的尝试，使无监督RL真正可扩展仍然是一个重大的挑战：在具有大状态空间的复杂环境中，纯探索方法可能会面临困难，因为覆盖每个可能的转换是不可行的；而互信息技能学习方法可能由于缺乏激励而完全无法探索环境。为了使无监督RL在复杂的高维环境中可扩展，我们提出了一种新的无监督RL目标，称为度量感知抽象（METRA）。

    Unsupervised pre-training strategies have proven to be highly effective in natural language processing and computer vision. Likewise, unsupervised reinforcement learning (RL) holds the promise of discovering a variety of potentially useful behaviors that can accelerate the learning of a wide array of downstream tasks. Previous unsupervised RL approaches have mainly focused on pure exploration and mutual information skill learning. However, despite the previous attempts, making unsupervised RL truly scalable still remains a major open challenge: pure exploration approaches might struggle in complex environments with large state spaces, where covering every possible transition is infeasible, and mutual information skill learning approaches might completely fail to explore the environment due to the lack of incentives. To make unsupervised RL scalable to complex, high-dimensional environments, we propose a novel unsupervised RL objective, which we call Metric-Aware Abstraction (METRA). Ou
    
[^54]: 边缘FMCW雷达的手势识别

    Gesture Recognition for FMCW Radar on the Edge. (arXiv:2310.08876v1 [cs.LG])

    [http://arxiv.org/abs/2310.08876](http://arxiv.org/abs/2310.08876)

    本文介绍了一种基于60 GHz FMCW雷达的轻量级手势识别系统，通过使用一组五个特征和精简的处理算法，可以在嵌入式平台上高效识别手势，同时具有较低的内存、计算和功耗要求。

    

    本文介绍了一种基于60 GHz调频连续波(FMCW)雷达的轻量级手势识别系统。我们展示了手势可以通过一组五个特征有效地进行特征化，并提出了一种精简的雷达处理算法来提取这些特征。与之前的方法相比，我们避免了繁重的二维处理，即距离-多普勒成像，并改为进行早期目标检测-这使得我们能够将系统移植到具有内存、计算和功耗严格限制的完全嵌入式平台上。基于循环神经网络(RNN)的架构利用这些特征共同检测和分类五种不同的手势。所提出的系统在我们的保留测试数据集上以98.4%的F1分数识别手势，它在一个Arm Cortex-M4微控制器上运行，需要不到280 kB的闪存存储器，120 kB的RAM，并消耗75 mW的功耗。

    This paper introduces a lightweight gesture recognition system based on 60 GHz frequency modulated continuous wave (FMCW) radar. We show that gestures can be characterized efficiently by a set of five features, and propose a slim radar processing algorithm to extract these features. In contrast to previous approaches, we avoid heavy 2D processing, i.e. range-Doppler imaging, and perform instead an early target detection - this allows us to port the system to fully embedded platforms with tight constraints on memory, compute and power consumption. A recurrent neural network (RNN) based architecture exploits these features to jointly detect and classify five different gestures. The proposed system recognizes gestures with an F1 score of 98.4% on our hold-out test dataset, it runs on an Arm Cortex-M4 microcontroller requiring less than 280 kB of flash memory, 120 kB of RAM, and consuming 75 mW of power.
    
[^55]: 处理磁盘数据不平衡的方法综述

    A Survey of Methods for Handling Disk Data Imbalance. (arXiv:2310.08867v1 [cs.LG])

    [http://arxiv.org/abs/2310.08867](http://arxiv.org/abs/2310.08867)

    本论文总结了处理磁盘数据不平衡问题的方法，包括数据层、算法层和混合方法，并讨论了不平衡数据分类的挑战及应对策略。

    

    分类问题中存在类别不平衡的情况，数据设计为准确性，而数据类别不平衡可能导致部分类别的错误分类成本较高。Backblaze数据集是与硬盘相关的广泛使用的数据集，它的失败数据量较少，健康数据量较大，存在严重的类别不平衡。本文对数据不平衡分类领域的研究进行了综述。讨论分为数据层方法、算法层方法和混合方法三个主要方面。对于每种方法类型，我们总结和分析了现有问题、算法思想、优势和缺点。此外，讨论了不平衡数据分类的挑战以及应对策略。研究人员可以根据自己的需求方便地选择适当的方法。

    Class imbalance exists in many classification problems, and since the data is designed for accuracy, imbalance in data classes can lead to classification challenges with a few classes having higher misclassification costs. The Backblaze dataset, a widely used dataset related to hard discs, has a small amount of failure data and a large amount of health data, which exhibits a serious class imbalance. This paper provides a comprehensive overview of research in the field of imbalanced data classification. The discussion is organized into three main aspects: data-level methods, algorithmic-level methods, and hybrid methods. For each type of method, we summarize and analyze the existing problems, algorithmic ideas, strengths, and weaknesses. Additionally, the challenges of unbalanced data classification are discussed, along with strategies to address them. It is convenient for researchers to choose the appropriate method according to their needs.
    
[^56]: 强适应性和模块化用于有效地在任务复杂性上进行泛化

    Adaptivity and Modularity for Efficient Generalization Over Task Complexity. (arXiv:2310.08866v1 [cs.LG])

    [http://arxiv.org/abs/2310.08866](http://arxiv.org/abs/2310.08866)

    引入一种新的任务来评估变压器在处理不同难度示例的问题上的泛化能力，并提出使用自适应和模块化计算机制的变压器架构，该架构在泛化到更高数量的计算时显示出更高的准确性和更公平的计算资源分配。

    

    变压器是否能够在需要处理不同难度示例的问题上进行有效的泛化？我们引入了一个新的任务，旨在评估对不同复杂度的泛化，并提出的结果表明标准的变压器在解决这些任务时面临挑战。这些任务是由Zhang等人在2021年提出的指针值检索的变体。我们研究了变压器中自适应和模块化计算机制的使用如何促进学习需要在顺序计算步骤数量（即计算图的深度）上进行泛化的任务。基于我们的观察，我们提出了一种基于变压器的架构称为Hyper-UT，它结合了来自超网络的动态函数生成和来自通用变压器的自适应深度。该模型在泛化到更高数量的计算时显示出更高的准确性和更公平的计算资源分配。

    Can transformers generalize efficiently on problems that require dealing with examples with different levels of difficulty? We introduce a new task tailored to assess generalization over different complexities and present results that indicate that standard transformers face challenges in solving these tasks. These tasks are variations of pointer value retrieval previously introduced by Zhang et al. (2021). We investigate how the use of a mechanism for adaptive and modular computation in transformers facilitates the learning of tasks that demand generalization over the number of sequential computation steps (i.e., the depth of the computation graph). Based on our observations, we propose a transformer-based architecture called Hyper-UT, which combines dynamic function generation from hyper networks with adaptive depth from Universal Transformers. This model demonstrates higher accuracy and a fairer allocation of computational resources when generalizing to higher numbers of computation
    
[^57]: 用于少样本分子性质预测的上下文学习

    In-Context Learning for Few-Shot Molecular Property Prediction. (arXiv:2310.08863v1 [cs.LG])

    [http://arxiv.org/abs/2310.08863](http://arxiv.org/abs/2310.08863)

    本文将上下文学习的概念应用于少样本分子性质预测，通过学习从（分子，性质测量）对的上下文中预测分子性质，并在新的性质上快速适应，该方法在小支持集大小上超过了最近的元学习算法的性能，并在大支持集大小上与最佳方法竞争。

    

    上下文学习已成为大型语言模型中少样本学习的重要方法，因其能够快速适应新任务，而无需微调模型参数。然而，它仅适用于自然语言应用，不适用于其他领域。本文将上下文学习的概念运用到少样本分子性质预测中，开发了一种新算法。我们的方法从（分子，性质测量）对的上下文中学习预测分子性质，并快速适应新的性质而无需微调。在FS-Mol和BACE分子性质预测基准测试中，我们发现该方法在小支持集大小上超过了最近的元学习算法的性能，并在大支持集大小上与最佳方法竞争。

    In-context learning has become an important approach for few-shot learning in Large Language Models because of its ability to rapidly adapt to new tasks without fine-tuning model parameters. However, it is restricted to applications in natural language and inapplicable to other domains. In this paper, we adapt the concepts underpinning in-context learning to develop a new algorithm for few-shot molecular property prediction. Our approach learns to predict molecular properties from a context of (molecule, property measurement) pairs and rapidly adapts to new properties without fine-tuning. On the FS-Mol and BACE molecular property prediction benchmarks, we find this method surpasses the performance of recent meta-learning algorithms at small support sizes and is competitive with the best methods at large support sizes.
    
[^58]: 使用分离权重衰减的Adam-family方法在深度学习中

    Adam-family Methods with Decoupled Weight Decay in Deep Learning. (arXiv:2310.08858v1 [math.OC])

    [http://arxiv.org/abs/2310.08858](http://arxiv.org/abs/2310.08858)

    本文研究了一类广泛的Adam-family方法在训练非光滑神经网络中的收敛性质，提出了一种使用分离权重衰减的新框架，并证明了其收敛性。该框架包含了许多已知的Adam-family方法，并对这些方法在训练非光滑神经网络时提供了收敛性保证。

    

    本文研究了一类广泛的Adam-family方法在最小化二次正则化非光滑非凸优化问题中的收敛性质，特别是在训练具有权重衰减的非光滑神经网络的情况下。受到AdamW方法的启发，我们提出了一种使用分离权重衰减的Adam-family方法的新框架。在我们的框架内，随机子梯度的一阶和二阶矩估计分别独立于权重衰减项进行更新。在合理的假设下，并且在更新主要优化变量时采用非递减步长，我们证明了我们提出的框架的收敛性质。此外，我们还展示了我们提出的框架包含了许多众所周知的Adam-family方法，从而为这些方法在训练非光滑神经网络时提供了收敛性保证。更重要的是，我们还展示了我们提出的框架渐近近似了一类次优点。

    In this paper, we investigate the convergence properties of a wide class of Adam-family methods for minimizing quadratically regularized nonsmooth nonconvex optimization problems, especially in the context of training nonsmooth neural networks with weight decay. Motivated by the AdamW method, we propose a novel framework for Adam-family methods with decoupled weight decay. Within our framework, the estimators for the first-order and second-order moments of stochastic subgradients are updated independently of the weight decay term. Under mild assumptions and with non-diminishing stepsizes for updating the primary optimization variables, we establish the convergence properties of our proposed framework. In addition, we show that our proposed framework encompasses a wide variety of well-known Adam-family methods, hence offering convergence guarantees for these methods in the training of nonsmooth neural networks. More importantly, we show that our proposed framework asymptotically approxi
    
[^59]: 克服连续学习中归一化统计的近期偏差：平衡和适应

    Overcoming Recency Bias of Normalization Statistics in Continual Learning: Balance and Adaptation. (arXiv:2310.08855v1 [cs.LG])

    [http://arxiv.org/abs/2310.08855](http://arxiv.org/abs/2310.08855)

    连续学习中的归一化统计存在近期偏差问题，我们提出了自适应BN的平衡策略AdaB$^2$N来解决这个问题。

    

    连续学习涉及学习一系列任务并适当平衡它们的知识。在深度神经网络中，由于对旧任务的训练样本有限，目前的研究主要关注于在基于梯度的优化中克服旧任务的灾难性遗忘。然而，归一化层提供了一种例外，因为它们通过梯度和当前观察到的训练样本的统计信息进行相互依赖的更新，这需要专门的策略来减轻近期偏差。在这项工作中，我们重点研究了最流行的批归一化（BN）并对其在连续学习中的次优性进行了深入的理论分析。我们的分析展示了BN统计的平衡和适应之间的困境，这可能影响训练的稳定性和推广能力。针对这些特定挑战，我们提出了自适应BN的平衡策略（AdaB$^2$N），它适当地将贝叶斯策略纳入其中，以更好地适应增量任务。

    Continual learning entails learning a sequence of tasks and balancing their knowledge appropriately. With limited access to old training samples, much of the current work in deep neural networks has focused on overcoming catastrophic forgetting of old tasks in gradient-based optimization. However, the normalization layers provide an exception, as they are updated interdependently by the gradient and statistics of currently observed training samples, which require specialized strategies to mitigate recency bias. In this work, we focus on the most popular Batch Normalization (BN) and provide an in-depth theoretical analysis of its sub-optimality in continual learning. Our analysis demonstrates the dilemma between balance and adaptation of BN statistics for incremental tasks, which potentially affects training stability and generalization. Targeting on these particular challenges, we propose Adaptive Balance of BN (AdaB$^2$N), which incorporates appropriately a Bayesian-based strategy to 
    
[^60]: 高质量目标检测的Rank-DETR方法

    Rank-DETR for High Quality Object Detection. (arXiv:2310.08854v1 [cs.CV])

    [http://arxiv.org/abs/2310.08854](http://arxiv.org/abs/2310.08854)

    Rank-DETR是一种高质量目标检测方法，通过引入排名导向的设计，包括架构设计和损失函数设计，实现了性能卓越的基于DETR的目标检测器。

    

    现代检测变换器（DETR）使用一组对象查询来预测边界框列表，通过将其分类置信度得分进行排序，并选择排名靠前的预测结果作为给定输入图像的最终检测结果。性能卓越的目标检测器需要对边界框预测进行准确的排序。对于基于DETR的检测器，排名靠前的边界框由于分类得分与定位准确性之间的不对齐而导致定位质量较差，从而阻碍了高质量检测器的构建。在这项工作中，我们通过提出一系列面向排名的设计，共同称为Rank-DETR，引入了一个简单且性能卓越的基于DETR的目标检测器。我们的主要贡献包括：（i）一个面向排名的架构设计，可以促进正面预测并抑制负面预测，以确保更低的假阳性率，以及（ii）一个面向排名的损失函数和匹配成本设计。

    Modern detection transformers (DETRs) use a set of object queries to predict a list of bounding boxes, sort them by their classification confidence scores, and select the top-ranked predictions as the final detection results for the given input image. A highly performant object detector requires accurate ranking for the bounding box predictions. For DETR-based detectors, the top-ranked bounding boxes suffer from less accurate localization quality due to the misalignment between classification scores and localization accuracy, thus impeding the construction of high-quality detectors. In this work, we introduce a simple and highly performant DETR-based object detector by proposing a series of rank-oriented designs, combinedly called Rank-DETR. Our key contributions include: (i) a rank-oriented architecture design that can prompt positive predictions and suppress the negative ones to ensure lower false positive rates, as well as (ii) a rank-oriented loss function and matching cost design 
    
[^61]: 半监督端到端对比学习用于时间序列分类

    Semi-Supervised End-To-End Contrastive Learning For Time Series Classification. (arXiv:2310.08848v1 [cs.LG])

    [http://arxiv.org/abs/2310.08848](http://arxiv.org/abs/2310.08848)

    本文提出了一个名为SLOTS的半监督端到端模型，用于时间序列分类。它通过接收半标记数据集，在无监督预训练和下游微调中综合利用对比损失和分类损失，解决了现有方法中的缺点。

    

    时间序列分类是金融、医疗和传感器数据分析等各种领域中的关键任务。无监督对比学习在使用有限标签的时间序列数据中学习有效表示方面引起了极大的兴趣。现有对比学习方法中普遍的方法包括两个独立的阶段：在无标签数据集上进行预训练编码器，然后在小规模标记数据集上对经过良好训练的模型进行微调。然而，这种两阶段方法存在一些缺点，例如无监督预训练对比损失不能直接影响下游微调分类器，以及缺乏利用由有价值的真实标签引导的分类损失。在本文中，我们提出了一个名为SLOTS（Semi-supervised Learning fOr Time clasSification）的端到端模型。SLOTS接收半标记数据集，其中包括大量无标签样本和少量标记样本。

    Time series classification is a critical task in various domains, such as finance, healthcare, and sensor data analysis. Unsupervised contrastive learning has garnered significant interest in learning effective representations from time series data with limited labels. The prevalent approach in existing contrastive learning methods consists of two separate stages: pre-training the encoder on unlabeled datasets and fine-tuning the well-trained model on a small-scale labeled dataset. However, such two-stage approaches suffer from several shortcomings, such as the inability of unsupervised pre-training contrastive loss to directly affect downstream fine-tuning classifiers, and the lack of exploiting the classification loss which is guided by valuable ground truth. In this paper, we propose an end-to-end model called SLOTS (Semi-supervised Learning fOr Time clasSification). SLOTS receives semi-labeled datasets, comprising a large number of unlabeled samples and a small proportion of labele
    
[^62]: 关于自然、鲁棒和灾难性过拟合中的过度记忆问题

    On the Over-Memorization During Natural, Robust and Catastrophic Overfitting. (arXiv:2310.08847v1 [cs.LG])

    [http://arxiv.org/abs/2310.08847](http://arxiv.org/abs/2310.08847)

    本论文研究了深度神经网络中的过度记忆问题，发现其会损害泛化能力，并提出了方法综合性地减轻不同类型的过拟合。

    

    过拟合对深度神经网络（DNN）的泛化能力产生了负面影响，无论是在自然训练还是对抗性训练中。现有的方法难以一致地解决不同类型的过拟合，通常设计了针对自然模式或对抗模式的策略。在本工作中，我们采用统一的视角，仅关注自然模式，去探索不同类型的过拟合。具体而言，我们研究了DNN中的记忆效应，并揭示了一种称为过度记忆的共同行为，这会损害它们的泛化能力。这种行为表现为DNN突然对某些训练模式产生高置信度的预测，并对其保持持久记忆。此外，当DNN过度记忆一种对抗模式时，它们往往同时展现出对应自然模式的高置信度预测。这些发现激励我们综合性地减轻不同类型的过拟合，阻碍过度记忆行为的发生。

    Overfitting negatively impacts the generalization ability of deep neural networks (DNNs) in both natural and adversarial training. Existing methods struggle to consistently address different types of overfitting, typically designing strategies that focus separately on either natural or adversarial patterns. In this work, we adopt a unified perspective by solely focusing on natural patterns to explore different types of overfitting. Specifically, we examine the memorization effect in DNNs and reveal a shared behaviour termed over-memorization, which impairs their generalization capacity. This behaviour manifests as DNNs suddenly becoming high-confidence in predicting certain training patterns and retaining a persistent memory for them. Furthermore, when DNNs over-memorize an adversarial pattern, they tend to simultaneously exhibit high-confidence prediction for the corresponding natural pattern. These findings motivate us to holistically mitigate different types of overfitting by hinder
    
[^63]: 通过观察映射和行为克隆进行少样本策略转移的框架

    A Framework for Few-Shot Policy Transfer through Observation Mapping and Behavior Cloning. (arXiv:2310.08836v1 [cs.RO])

    [http://arxiv.org/abs/2310.08836](http://arxiv.org/abs/2310.08836)

    本文提出了一个通过观察映射和行为克隆进行少样本策略转移的框架，通过使用生成对抗网络和循环一致性损失来映射源领域和目标领域之间的观察，并利用这个映射来克隆源任务的成功策略。

    

    尽管强化学习在机器人应用中取得了一些进展，但由于昂贵的交互成本，许多任务仍然难以解决。迁移学习通过将在源领域学到的知识转移到目标领域，有助于减少目标领域的训练时间。Sim2Real迁移有助于将在模拟机器人领域学到的知识转移到物理目标领域。知识迁移减少了在交互成本高的物理世界中训练任务所需的时间。然而，大多数现有方法假设两个领域的任务结构和物理属性完全对应。本文提出了一个通过观察映射和行为克隆进行少样本策略转移的框架。我们使用生成对抗网络（GANs）和循环一致性损失来映射源领域和目标领域之间的观察，并且后续使用这个学到的映射来克隆源任务的成功策略。

    Despite recent progress in Reinforcement Learning for robotics applications, many tasks remain prohibitively difficult to solve because of the expensive interaction cost. Transfer learning helps reduce the training time in the target domain by transferring knowledge learned in a source domain. Sim2Real transfer helps transfer knowledge from a simulated robotic domain to a physical target domain. Knowledge transfer reduces the time required to train a task in the physical world, where the cost of interactions is high. However, most existing approaches assume exact correspondence in the task structure and the physical properties of the two domains. This work proposes a framework for Few-Shot Policy Transfer between two domains through Observation Mapping and Behavior Cloning. We use Generative Adversarial Networks (GANs) along with a cycle-consistency loss to map the observations between the source and target domains and later use this learned mapping to clone the successful source task 
    
[^64]: 平均奖励马尔可夫决策过程的最优样本复杂度

    Optimal Sample Complexity for Average Reward Markov Decision Processes. (arXiv:2310.08833v1 [cs.LG])

    [http://arxiv.org/abs/2310.08833](http://arxiv.org/abs/2310.08833)

    本论文解决了对于均匀收敛的马尔可夫决策过程的长期平均奖励最大化策略学习的样本复杂度问题，并建立了一个样本复杂度为$\widetilde O(|S||A|t_{\text{mix}}\epsilon^{-2})$的优化策略估计器。

    

    我们在假设有一个生成模型的情况下，解决了与均匀收敛的马尔可夫决策过程相关的长期平均奖励的策略学习的样本复杂性问题。在这个背景下，现有的文献提供了一个样本复杂度的上界，$ \widetilde O(|S||A|t_{\text{mix}}^2 \epsilon^{-2})$，和一个下界，$\Omega(|S||A|t_{\text{mix}} \epsilon^{-2})$。在这些表达式中，$|S|$和$|A|$分别表示状态空间和动作空间的势，$t_{\text{mix}}$作为总变异混合时间的统一上限，$\epsilon$表示误差容忍度。因此，$t_{\text{mix}}$仍然存在一个显着的差距需要填补。我们的主要贡献是建立一个优化策略的估计器，其样本复杂度为$\widetilde O(|S||A|t_{\text{mix}}\epsilon^{-2})$，有效地达到了文献中的下界。这是通过结合算法思想实现的。

    We settle the sample complexity of policy learning for the maximization of the long run average reward associated with a uniformly ergodic Markov decision process (MDP), assuming a generative model. In this context, the existing literature provides a sample complexity upper bound of $\widetilde O(|S||A|t_{\text{mix}}^2 \epsilon^{-2})$ and a lower bound of $\Omega(|S||A|t_{\text{mix}} \epsilon^{-2})$. In these expressions, $|S|$ and $|A|$ denote the cardinalities of the state and action spaces respectively, $t_{\text{mix}}$ serves as a uniform upper limit for the total variation mixing times, and $\epsilon$ signifies the error tolerance. Therefore, a notable gap of $t_{\text{mix}}$ still remains to be bridged. Our primary contribution is to establish an estimator for the optimal policy of average reward MDPs with a sample complexity of $\widetilde O(|S||A|t_{\text{mix}}\epsilon^{-2})$, effectively reaching the lower bound in the literature. This is achieved by combining algorithmic idea
    
[^65]: Distance-rank感知顺序奖励学习用于具有次优演示的逆强化学习

    Distance-rank Aware Sequential Reward Learning for Inverse Reinforcement Learning with Sub-optimal Demonstrations. (arXiv:2310.08823v1 [cs.LG])

    [http://arxiv.org/abs/2310.08823](http://arxiv.org/abs/2310.08823)

    提出了Distance-rank感知顺序奖励学习（DRASRL）框架，旨在解决逆强化学习中轨迹排名模糊和奖励模糊的问题。

    

    逆强化学习（IRL）旨在基于收集到的专家演示明确推断出潜在的奖励函数。考虑到获取专家演示可能是昂贵的，当前IRL技术的重点是使用从次优演示中导出的奖励函数学习一个优于演示者的策略。然而，现有的IRL算法主要解决了在学习奖励函数时的轨迹排名模糊的挑战，却忽视了在进一步消除奖励模糊性方面，考虑轨迹之间的回报差异程度的关键作用。此外，需要注意的是，单个转换的奖励受到轨迹中的上下文信息的重要影响。为了解决这些问题，我们引入了Distance-rank感知顺序奖励学习（DRASRL）框架。与现有方法不同，DRASRL同时考虑了轨迹的排名和回报之间的差异度。

    Inverse reinforcement learning (IRL) aims to explicitly infer an underlying reward function based on collected expert demonstrations. Considering that obtaining expert demonstrations can be costly, the focus of current IRL techniques is on learning a better-than-demonstrator policy using a reward function derived from sub-optimal demonstrations. However, existing IRL algorithms primarily tackle the challenge of trajectory ranking ambiguity when learning the reward function. They overlook the crucial role of considering the degree of difference between trajectories in terms of their returns, which is essential for further removing reward ambiguity. Additionally, it is important to note that the reward of a single transition is heavily influenced by the context information within the trajectory. To address these issues, we introduce the Distance-rank Aware Sequential Reward Learning (DRASRL) framework. Unlike existing approaches, DRASRL takes into account both the ranking of trajectories
    
[^66]: 探索量表答题过程中反应时间序列与失眠严重程度之间的关系：一种机器学习方法

    Exploring the relationship between response time sequence in scale answering process and severity of insomnia: a machine learning approach. (arXiv:2310.08817v1 [cs.LG])

    [http://arxiv.org/abs/2310.08817](http://arxiv.org/abs/2310.08817)

    该研究发现量表答题过程中的反应时间与失眠严重程度之间存在关系，并开发了一种机器学习模型，可以根据反应时间数据预测参与者是否存在失眠。

    

    本研究旨在调查失眠与反应时间之间的关系，并利用反应时间数据开发一种机器学习模型来预测参与者是否存在失眠。通过设计一个移动应用程序，从2729名参与者那里收集到量表测试和反应时间数据。研究发现失眠症状的参与者与非失眠症状的参与者在总反应时间上存在显著差异（p <0.001）。在个体问题水平上观察到特定失眠方面的严重程度和反应时间之间的相关性。该机器学习模型在基于反应时间数据预测失眠症状方面表现出较高的预测准确度（0.743）。

    Objectives: The study aims to investigate the relationship between insomnia and response time. Additionally, it aims to develop a machine learning model to predict the presence of insomnia in participants using response time data. Methods: A mobile application was designed to administer scale tests and collect response time data from 2729 participants. The relationship between symptom severity and response time was explored, and a machine learning model was developed to predict the presence of insomnia. Results: The result revealed a statistically significant difference (p<.001) in the total response time between participants with or without insomnia symptoms. A correlation was observed between the severity of specific insomnia aspects and response times at the individual questions level. The machine learning model demonstrated a high predictive accuracy of 0.743 in predicting insomnia symptoms based on response time data. Conclusions: These findings highlight the potential utility of 
    
[^67]: 使用VMD-GARCH-LSTM模型的非线性时间序列预测方法

    A Nonlinear Method for time series forecasting using VMD-GARCH-LSTM model. (arXiv:2310.08812v1 [stat.ME])

    [http://arxiv.org/abs/2310.08812](http://arxiv.org/abs/2310.08812)

    该研究提出了一个新的时间序列预测方法，使用VMD-GARCH-LSTM模型来捕捉复杂时间序列中的局部特征和波动性信息，并通过集成各个子模态的预测结果来提高预测准确性。

    

    时间序列预测在各个领域都是一个重要而具有挑战性的任务。最近，基于模态分解的方法在复杂时间序列的预测中占据主导地位，因为它能够捕捉局部特征并从数据中提取内在模态的优势。然而，大多数模型无法捕捉包含重要信息的暗含波动性。为了提高当前、快速演变和波动性时间序列的预测能力，我们提出了一种新颖的分解-集成范式，即VMD-LSTM-GARCH模型。该模型使用变分模态分解算法将时间序列分解为K个子模态。随后，GARCH模型从这些子模态中提取波动性信息，这些信息作为LSTM的输入。每个子模态的数值和波动性信息被用来训练一个长短期记忆网络。该网络预测子模态，然后我们将所有子模态的预测结果进行聚合。

    Time series forecasting represents a significant and challenging task across various fields. Recently, methods based on mode decomposition have dominated the forecasting of complex time series because of the advantages of capturing local characteristics and extracting intrinsic modes from data. Unfortunately, most models fail to capture the implied volatilities that contain significant information. To enhance the forecasting of current, rapidly evolving, and volatile time series, we propose a novel decomposition-ensemble paradigm, the VMD-LSTM-GARCH model. The Variational Mode Decomposition algorithm is employed to decompose the time series into K sub-modes. Subsequently, the GARCH model extracts the volatility information from these sub-modes, which serve as the input for the LSTM. The numerical and volatility information of each sub-mode is utilized to train a Long Short-Term Memory network. This network predicts the sub-mode, and then we aggregate the predictions from all sub-modes 
    
[^68]: DDMT: 用于多变量时间序列异常检测的去噪扩散掩膜Transformer模型

    DDMT: Denoising Diffusion Mask Transformer Models for Multivariate Time Series Anomaly Detection. (arXiv:2310.08800v1 [cs.LG])

    [http://arxiv.org/abs/2310.08800](http://arxiv.org/abs/2310.08800)

    本论文提出了一种名为DDMT的新框架，用于解决多变量时间序列异常检测中的噪声和弱身份映射问题。通过引入自适应动态邻居掩膜机制(ADNM)以及集成Transformer和去噪扩散模型，该框架可以有效地检测时间序列数据中的异常情况。

    

    多变量时间序列中的异常检测已经成为时间序列研究中的一个重要挑战，在欺诈检测、故障诊断和系统状态估计等各个领域具有重要的研究影响。近年来，基于重建的模型在检测时间序列数据中的异常方面显示出了很大的潜力。然而，由于数据规模和维度的快速增加，时间序列重建过程中的噪声和弱身份映射问题越来越突出。为了解决这个问题，我们引入了一种新的自适应动态邻居掩膜机制(ADNM)，将其与Transformer和去噪扩散模型相结合，创建了一种名为Denoising Diffusion Mask Transformer (DDMT) 的新框架，用于多变量时间序列异常检测。ADNM模块用于减轻数据重建过程中输入和输出特征之间的信息泄漏问题，从而缓解了弱身份映射问题。

    Anomaly detection in multivariate time series has emerged as a crucial challenge in time series research, with significant research implications in various fields such as fraud detection, fault diagnosis, and system state estimation. Reconstruction-based models have shown promising potential in recent years for detecting anomalies in time series data. However, due to the rapid increase in data scale and dimensionality, the issues of noise and Weak Identity Mapping (WIM) during time series reconstruction have become increasingly pronounced. To address this, we introduce a novel Adaptive Dynamic Neighbor Mask (ADNM) mechanism and integrate it with the Transformer and Denoising Diffusion Model, creating a new framework for multivariate time series anomaly detection, named Denoising Diffusion Mask Transformer (DDMT). The ADNM module is introduced to mitigate information leakage between input and output features during data reconstruction, thereby alleviating the problem of WIM during recon
    
[^69]: 通过追踪偏见影响来减轻问题回答模型的偏见

    Mitigating Bias for Question Answering Models by Tracking Bias Influence. (arXiv:2310.08795v1 [cs.CL])

    [http://arxiv.org/abs/2310.08795](http://arxiv.org/abs/2310.08795)

    本论文提出了一种名为BMBI的方法来减轻多选问题回答模型的偏见。通过观察一个查询实例对另一个实例的影响，测量查询实例的偏见程度，并将其作为优化目标，形成一个多任务学习设置。同时引入新的偏见评估指标以量化偏见。

    

    已经证明各种NLP任务的模型存在刻板印象，而问题回答（QA）模型中的偏见尤其有害，因为输出的答案可能直接被最终用户使用。已经有数据集用于评估QA模型中的偏见，但是对于QA模型的偏见缓解技术仍处于探索阶段。本工作中，我们提出了一种名为BMBI的方法，用于缓解多选问题回答模型的偏见。基于一个直觉，即如果一个模型从一个有偏见的例子中学到了东西，它可能更容易出现偏见，我们通过观察一个查询实例对另一个实例的影响来衡量查询实例的偏见程度。如果受到影响的实例更偏见，我们认为查询实例是有偏见的。我们使用检测到的偏见程度作为优化目标，形成一个多任务学习设置，除了原来的QA任务。我们还引入了一种新的偏见评估指标，以全面而敏感的方式量化偏见。我们展示了我们的方法可以应用于减轻QA模型的偏见。

    Models of various NLP tasks have been shown to exhibit stereotypes, and the bias in the question answering (QA) models is especially harmful as the output answers might be directly consumed by the end users. There have been datasets to evaluate bias in QA models, while bias mitigation technique for the QA models is still under-explored. In this work, we propose BMBI, an approach to mitigate the bias of multiple-choice QA models. Based on the intuition that a model would lean to be more biased if it learns from a biased example, we measure the bias level of a query instance by observing its influence on another instance. If the influenced instance is more biased, we derive that the query instance is biased. We then use the bias level detected as an optimization objective to form a multi-task learning setting in addition to the original QA task. We further introduce a new bias evaluation metric to quantify bias in a comprehensive and sensitive way. We show that our method could be applie
    
[^70]: 机器学习辅助ERCOT负荷预测中的天气和时间特征分析

    Analysis of Weather and Time Features in Machine Learning-aided ERCOT Load Forecasting. (arXiv:2310.08793v1 [cs.LG])

    [http://arxiv.org/abs/2310.08793](http://arxiv.org/abs/2310.08793)

    本研究开发了机器学习模型，利用时间和天气信息预测短期系统总负荷，发现最大程度利用各种相关特征不一定能达到最佳预测效果。

    

    准确的负荷预测对于电力系统的高效可靠运行至关重要。电力消耗的很大一部分受天气条件的影响，因此天气信息是影响电力使用的重要因素。个人电器和工业设备也以其时间模式显著贡献于电力需求，因此时间也是负荷预测中需要考虑的一个有用因素。本研究开发了几种机器学习模型，其中将各种时间和天气信息作为输入特征，用于预测短期系统总负荷。还进行了消融研究，以研究和比较不同天气因素对预测准确性的影响。使用实际负荷和历史天气数据，训练了机器学习模型。有趣的是观察到，最大程度地利用所有可能与负荷相关的特征，不太可能达到最佳预测效果。

    Accurate load forecasting is critical for efficient and reliable operations of the electric power system. A large part of electricity consumption is affected by weather conditions, making weather information an important determinant of electricity usage. Personal appliances and industry equipment also contribute significantly to electricity demand with temporal patterns, making time a useful factor to consider in load forecasting. This work develops several machine learning (ML) models that take various time and weather information as part of the input features to predict the short-term system-wide total load. Ablation studies were also performed to investigate and compare the impacts of different weather factors on the prediction accuracy. Actual load and historical weather data for the same region were processed and then used to train the ML models. It is interesting to observe that using all available features, each of which may be correlated to the load, is unlikely to achieve the 
    
[^71]: 分布式集成学习的激励机制设计

    Incentive Mechanism Design for Distributed Ensemble Learning. (arXiv:2310.08792v1 [cs.GT])

    [http://arxiv.org/abs/2310.08792](http://arxiv.org/abs/2310.08792)

    本研究针对分布式集成学习提出了激励机制设计。通过指定学习者的训练数据量和奖励金额，解决了自利学习者不愿参与的问题。此外，通过拆解集成准确性为多样性-精确性权衡，指导了机制设计。通过新的快速求解算法，准确估计了每个学习者的贡献。

    

    分布式集成学习（DEL）涉及在分布式学习者中训练多个模型，然后结合它们的预测以提高性能。现有的相关研究集中在DEL算法设计和优化上，但忽视了激励这一重要问题，没有激励机制，自利学习者可能不愿参与DEL。我们旨在填补这一空白，提出了关于DEL激励机制设计的首个研究。我们提出的机制规定了具有异构计算和通信成本的学习者的训练数据量和奖励金额。一个设计挑战是准确理解学习者的多样性（即训练数据方面）如何影响集成准确性。为此，我们将集成准确性拆解为多样性-精确性权衡，以指导机制设计。另一个挑战是机制设计涉及解决具有大搜索空间的混合整数规划问题。为此，我们提出了一种新的快速求解算法，该算法通过同时考虑数据分布和通信成本来准确估计每个学习者的贡献。

    Distributed ensemble learning (DEL) involves training multiple models at distributed learners, and then combining their predictions to improve performance. Existing related studies focus on DEL algorithm design and optimization but ignore the important issue of incentives, without which self-interested learners may be unwilling to participate in DEL. We aim to fill this gap by presenting a first study on the incentive mechanism design for DEL. Our proposed mechanism specifies both the amount of training data and reward for learners with heterogeneous computation and communication costs. One design challenge is to have an accurate understanding regarding how learners' diversity (in terms of training data) affects the ensemble accuracy. To this end, we decompose the ensemble accuracy into a diversity-precision tradeoff to guide the mechanism design. Another challenge is that the mechanism design involves solving a mixed-integer program with a large search space. To this end, we propose a
    
[^72]: 质量感知联邦学习中的稳定成本

    Price of Stability in Quality-Aware Federated Learning. (arXiv:2310.08790v1 [cs.LG])

    [http://arxiv.org/abs/2310.08790](http://arxiv.org/abs/2310.08790)

    本论文提出了一种质量感知的联邦学习方案，研究了标签噪声对系统性能的影响，并设计了一种游戏模型来理解客户端的行为。研究结果表明均衡结果的全局模型准确度低于社会最优解，并提出了一种高效算法来计算。

    

    联邦学习是一种分布式机器学习方案，可以使客户端在不交换本地数据的情况下训练一个共享的全局模型。标签噪声的存在会严重影响联邦学习的性能，一些现有研究已经关注了用于标签去噪的算法设计。然而，它们忽视了一个重要问题，即由于自利性和对联邦学习性能的异质估值，客户端可能不会应用昂贵的标签去噪策略。为了填补这一空白，我们将客户端之间的互动建模为一种新颖的标签去噪博弈，并确定其均衡状态。我们还分析了稳定成本，该成本用来衡量均衡结果与社会最优解之间的系统性能差异（例如，全局模型准确度、社会福利）。我们证明了均衡结果的全局模型准确度始终低于社会最优解。我们进一步设计了一种高效算法来计算。

    Federated Learning (FL) is a distributed machine learning scheme that enables clients to train a shared global model without exchanging local data. The presence of label noise can severely degrade the FL performance, and some existing studies have focused on algorithm design for label denoising. However, they ignored the important issue that clients may not apply costly label denoising strategies due to them being self-interested and having heterogeneous valuations on the FL performance. To fill this gap, we model the clients' interactions as a novel label denoising game and characterize its equilibrium. We also analyze the price of stability, which quantifies the difference in the system performance (e.g., global model accuracy, social welfare) between the equilibrium outcome and the socially optimal solution. We prove that the equilibrium outcome always leads to a lower global model accuracy than the socially optimal solution does. We further design an efficient algorithm to compute 
    
[^73]: 选择性驱动生产力：增强迁移学习的高效数据集修剪

    Selectivity Drives Productivity: Efficient Dataset Pruning for Enhanced Transfer Learning. (arXiv:2310.08782v1 [cs.LG])

    [http://arxiv.org/abs/2310.08782](http://arxiv.org/abs/2310.08782)

    本论文提出了一种解决迁移学习中数据集修剪的方法，通过集成数据集修剪和迁移学习的观点，发现现有的方法不适用于迁移学习范式，并提出了标签映射和特征映射这两种新的数据集修剪方法。

    

    大规模数据通常被认为是深度学习应用的必要条件，但同时也会带来巨大的计算和基础设施成本。因此，数据集修剪（DP）作为一种有效的方法出现，通过识别和删除冗余的训练样本来提高数据效率，而不会影响性能。在这项工作中，我们旨在解决迁移学习中的DP问题，即如何在下游目标任务中提高预训练效率和完整微调准确性的同时修剪源数据集。据我们所知，迁移学习的DP问题仍然未解决，因为先前的研究主要将DP和迁移学习视为独立的问题。相反，我们建立了一个统一的视角，将DP与迁移学习相结合，并发现现有的DP方法不适用于迁移学习范式。然后，我们提出了两种新的DP方法，即标签映射和特征映射，用于监督和自监督的预训练设置。

    Massive data is often considered essential for deep learning applications, but it also incurs significant computational and infrastructural costs. Therefore, dataset pruning (DP) has emerged as an effective way to improve data efficiency by identifying and removing redundant training samples without sacrificing performance. In this work, we aim to address the problem of DP for transfer learning, i.e., how to prune a source dataset for improved pretraining efficiency and lossless finetuning accuracy on downstream target tasks. To our best knowledge, the problem of DP for transfer learning remains open, as previous studies have primarily addressed DP and transfer learning as separate problems. By contrast, we establish a unified viewpoint to integrate DP with transfer learning and find that existing DP methods are not suitable for the transfer learning paradigm. We then propose two new DP methods, label mapping and feature mapping, for supervised and self-supervised pretraining settings 
    
[^74]: 当机器学习模型泄漏：合成训练数据的探索

    When Machine Learning Models Leak: An Exploration of Synthetic Training Data. (arXiv:2310.08775v1 [cs.LG])

    [http://arxiv.org/abs/2310.08775](http://arxiv.org/abs/2310.08775)

    本论文研究了针对一个预测人员或家庭是否会在接下来的两年内搬迁的机器学习模型的攻击，攻击者利用模型的预测以及公开的训练数据边际分布来推断目标个体的敏感属性值，同时探讨了用合成数据替代原始数据训练模型对攻击的影响。

    

    我们研究了对一个机器学习模型的攻击，该模型用于预测一个人或家庭在接下来的两年内是否会搬迁，即迁移倾向分类器。攻击假设攻击者可以查询模型以获取预测，并且模型训练时使用的数据的边际分布是公开可用的。攻击还假设攻击者已经获取了一定数量目标个体的非敏感属性值。攻击的目标是推断这些目标个体的敏感属性值。我们探讨了在训练模型时用合成数据替代原始数据对攻击者成功推断敏感属性的影响。

    We investigate an attack on a machine learning model that predicts whether a person or household will relocate in the next two years, i.e., a propensity-to-move classifier. The attack assumes that the attacker can query the model to obtain predictions and that the marginal distribution of the data on which the model was trained is publicly available. The attack also assumes that the attacker has obtained the values of non-sensitive attributes for a certain number of target individuals. The objective of the attack is to infer the values of sensitive attributes for these target individuals. We explore how replacing the original data with synthetic data when training the model impacts how successfully the attacker can infer sensitive attributes.\footnote{Original paper published at PSD 2022. The paper was subsequently updated.}
    
[^75]: PhyloGFN: 基于生成流网络的系统发育推断

    PhyloGFN: Phylogenetic inference with generative flow networks. (arXiv:2310.08774v1 [q-bio.PE])

    [http://arxiv.org/abs/2310.08774](http://arxiv.org/abs/2310.08774)

    PhyloGFN是一种基于生成流网络的系统发育推断方法，通过采样复杂的组合结构，能够产生多样且高质量的进化假设，并在边缘似然估计方面具有竞争力。

    

    系统发育学是计算生物学的一个分支，研究生物实体之间的进化关系。尽管有着悠久的历史和众多应用，但从序列数据推断系统发育树仍然具有挑战性：树空间的高复杂性对当前的组合和概率技术构成了重要障碍。在本文中，我们采用生成流网络（GFlowNets）的框架来解决系统发育学中的两个核心问题：基于最简原则的和贝叶斯的系统发育推断。由于GFlowNets适用于采样复杂的组合结构，它们是探索和采样树拓扑和进化距离的多模态后验分布的自然选择。我们证明了我们的摊还后验采样器PhyloGFN在真实基准数据集上产生多样且高质量的进化假设。PhyloGFN在边缘似然估计方面与之前的工作相比具有竞争力。

    Phylogenetics is a branch of computational biology that studies the evolutionary relationships among biological entities. Its long history and numerous applications notwithstanding, inference of phylogenetic trees from sequence data remains challenging: the high complexity of tree space poses a significant obstacle for the current combinatorial and probabilistic techniques. In this paper, we adopt the framework of generative flow networks (GFlowNets) to tackle two core problems in phylogenetics: parsimony-based and Bayesian phylogenetic inference. Because GFlowNets are well-suited for sampling complex combinatorial structures, they are a natural choice for exploring and sampling from the multimodal posterior distribution over tree topologies and evolutionary distances. We demonstrate that our amortized posterior sampler, PhyloGFN, produces diverse and high-quality evolutionary hypotheses on real benchmark datasets. PhyloGFN is competitive with prior works in marginal likelihood estimat
    
[^76]: 使用多尺度密集回归神经网络和注意力机制以及Inception块模型对中子材料中的裂变气释放进行中尺度建模

    Modeling Fission Gas Release at the Mesoscale using Multiscale DenseNet Regression with Attention Mechanism and Inception Blocks. (arXiv:2310.08767v1 [cond-mat.mes-hall])

    [http://arxiv.org/abs/2310.08767](http://arxiv.org/abs/2310.08767)

    这项研究使用深度学习方法，基于2D核燃料微观结构图像，通过训练具有多尺度回归的神经网络模型来预测中子材料中裂变气的瞬时释放通量，并且取得了很高的预测精度。

    

    中尺度核燃料中的裂变气释放（FGR）的模拟是理解微观结构演变对FGR的影响的强大工具，但需要大量计算资源。在本研究中，我们提出了一种替代的、基于数据驱动的方法，使用深度学习从2D核燃料微观结构图像中预测瞬时FGR通量。我们训练和评估了四个具有多尺度回归的卷积神经网络（CNN）架构，这些网络使用一个混合相场/聚类动力学模型生成的模拟FGR数据。所有四个网络都具有很高的预测能力，$R^{2}$值均在98%以上。表现最好的网络结合了卷积块注意力模块（CBAM）和InceptionNet机制，提供了更高的准确性（平均绝对百分比误差为4.4%）、训练稳定性和对非常低的瞬时FGR通量的鲁棒性。

    Mesoscale simulations of fission gas release (FGR) in nuclear fuel provide a powerful tool for understanding how microstructure evolution impacts FGR, but they are computationally intensive. In this study, we present an alternate, data-driven approach, using deep learning to predict instantaneous FGR flux from 2D nuclear fuel microstructure images. Four convolutional neural network (CNN) architectures with multiscale regression are trained and evaluated on simulated FGR data generated using a hybrid phase field/cluster dynamics model. All four networks show high predictive power, with $R^{2}$ values above 98%. The best performing network combine a Convolutional Block Attention Module (CBAM) and InceptionNet mechanisms to provide superior accuracy (mean absolute percentage error of 4.4%), training stability, and robustness on very low instantaneous FGR flux values.
    
[^77]: 校准使得摘要模型在一致性方面更为准确

    Calibrating Likelihoods towards Consistency in Summarization Models. (arXiv:2310.08764v1 [cs.CL])

    [http://arxiv.org/abs/2310.08764](http://arxiv.org/abs/2310.08764)

    通过校准模型生成的序列的似然性，使其更好地与通过自然语言推理（NLI）模型测量的一致性度量相一致，从而提高摘要模型的一致性和质量。

    

    尽管抽象化文本摘要取得了一些新的进展，但目前的摘要模型仍然存在生成事实不一致的摘要的问题，这减弱了它们在实际应用中的效用。我们认为造成这种行为的主要原因是由于基于最大似然目标训练的摘要模型在给定上下文时赋予可能序列高概率，但它们往往不能准确地根据其一致性排名序列。在这项工作中，我们通过校准模型生成的序列的似然性，使其更好地与通过自然语言推理（NLI）模型测量的一致性度量相一致来解决这个问题。人类评估研究和自动指标表明，经过校准的模型生成更一致且更高质量的摘要。我们还展示了使用我们方法训练的模型返回的概率与NLI得分更为对齐，这显著提高了摘要模型的可靠性。

    Despite the recent advances in abstractive text summarization, current summarization models still suffer from generating factually inconsistent summaries, reducing their utility for real-world application. We argue that the main reason for such behavior is that the summarization models trained with maximum likelihood objective assign high probability to plausible sequences given the context, but they often do not accurately rank sequences by their consistency. In this work, we solve this problem by calibrating the likelihood of model generated sequences to better align with a consistency metric measured by natural language inference (NLI) models. The human evaluation study and automatic metrics show that the calibrated models generate more consistent and higher-quality summaries. We also show that the models trained using our method return probabilities that are better aligned with the NLI scores, which significantly increase reliability of summarization models.
    
[^78]: EEG分类中采用发散估计稳定主体迁移

    Stabilizing Subject Transfer in EEG Classification with Divergence Estimation. (arXiv:2310.08762v1 [cs.LG])

    [http://arxiv.org/abs/2310.08762](http://arxiv.org/abs/2310.08762)

    本研究在EEG分类中使用新的正则化技术减少了在未见测试对象上的性能下降。通过设计正则化惩罚和发散估计算法，我们成功地在模型训练中强制执行了统计关系，并在大量计算实验中验证了我们的方法的有效性。

    

    电encephalogram(EEG)数据的分类模型在未见测试对象上的表现大幅下降。我们通过新的正则化技术减少性能下降问题。我们提出了几个图模型来描述EEG分类任务。从每个模型中，我们确定了在理想的训练场景下（具有无限数据和全局最优模型），应该成立但在实践中可能不成立的统计关系。我们设计了正则化惩罚来强制执行这些关系。首先，我们确定了适用作为代理数量的合适的发散（如互信息和Wasserstein-1），可以用来测量统计独立和相关关系。其次，我们提供了使用二次神经网络模型在训练过程中高效估计这些数量的算法。我们使用大型基准EEG数据集进行了大量的计算实验，比较了我们的方法与其他方法。

    Classification models for electroencephalogram (EEG) data show a large decrease in performance when evaluated on unseen test sub jects. We reduce this performance decrease using new regularization techniques during model training. We propose several graphical models to describe an EEG classification task. From each model, we identify statistical relationships that should hold true in an idealized training scenario (with infinite data and a globally-optimal model) but that may not hold in practice. We design regularization penalties to enforce these relationships in two stages. First, we identify suitable proxy quantities (divergences such as Mutual Information and Wasserstein-1) that can be used to measure statistical independence and dependence relationships. Second, we provide algorithms to efficiently estimate these quantities during training using secondary neural network models. We conduct extensive computational experiments using a large benchmark EEG dataset, comparing our propo
    
[^79]: 电子健康记录的问题回答：数据集和模型的范围回顾

    Question Answering for Electronic Health Records: A Scoping Review of datasets and models. (arXiv:2310.08759v1 [cs.LG])

    [http://arxiv.org/abs/2310.08759](http://arxiv.org/abs/2310.08759)

    本文对电子健康记录（EHR）中的问题回答进行了范围回顾。与其他医学QA任务不同，EHR QA通过从患者的医疗记录中获取答案。这项研究为现有的EHR QA作品提供了方法论回顾。

    

    与患者相关的问题回答（QA）系统可以帮助临床医生和患者。它们可以帮助临床医生做决策，并使患者更好地了解他们的病历。大量的患者数据存储在电子健康记录（EHR）中，使得EHR QA成为一个重要的研究领域。在EHR QA中，答案是从患者的医疗记录中获得的。由于数据格式和模式的差异，这与其他使用医学网站或科学论文检索答案的医学QA任务有很大的不同，这使得研究EHR问题回答变得至关重要。本研究旨在对现有关于EHR QA的作品进行方法论回顾。我们在包括Google Scholar、ACL Anthology、ACM Digital Library和PubMed在内的四个数字资源中搜索了从2005年1月1日到2023年9月30日的文章，以收集有关EHR QA的相关出版物。共发现了4111篇论文。

    Question Answering (QA) systems on patient-related data can assist both clinicians and patients. They can, for example, assist clinicians in decision-making and enable patients to have a better understanding of their medical history. Significant amounts of patient data are stored in Electronic Health Records (EHRs), making EHR QA an important research area. In EHR QA, the answer is obtained from the medical record of the patient. Because of the differences in data format and modality, this differs greatly from other medical QA tasks that employ medical websites or scientific papers to retrieve answers, making it critical to research EHR question answering. This study aimed to provide a methodological review of existing works on QA over EHRs. We searched for articles from January 1st, 2005 to September 30th, 2023 in four digital sources including Google Scholar, ACL Anthology, ACM Digital Library, and PubMed to collect relevant publications on EHR QA. 4111 papers were identified for our
    
[^80]: 使用长期结构化电子健康记录检测和预测氯吡格雷治疗失败

    Detection and prediction of clopidogrel treatment failures using longitudinal structured electronic health records. (arXiv:2310.08757v1 [cs.LG])

    [http://arxiv.org/abs/2310.08757](http://arxiv.org/abs/2310.08757)

    本研究提出了使用机器学习算法基于长期结构化电子健康记录，自动检测和预测氯吡格雷治疗失败的方法。研究利用英国生物库的数据构建了模型，通过整理诊断、处方和过程记录来进行预测和检测。

    

    我们提出了机器学习算法，利用长期结构化电子健康记录（EHR）自动检测和预测氯吡格雷治疗失败。通过将自然语言与结构化EHR进行类比，我们引入了用于自然语言处理（NLP）应用的各种机器学习算法，构建治疗失败检测和预测模型。我们从英国生物库（UK Biobank）中生成了一个服用氯吡格雷处方的患者队列，并标注了这些患者在第一次氯吡格雷处方后一年内是否发生了治疗失败事件；在502,527名患者中，发现了1,824例治疗失败病例和6,859例对照病例。我们按患者整理了诊断、处方和过程记录，并按照相同日期组织成访问记录，以构建模型。这些模型分别用于检测和预测两个不同的任务。

    We propose machine learning algorithms to automatically detect and predict clopidogrel treatment failure using longitudinal structured electronic health records (EHR). By drawing analogies between natural language and structured EHR, we introduce various machine learning algorithms used in natural language processing (NLP) applications to build models for treatment failure detection and prediction. In this regard, we generated a cohort of patients with clopidogrel prescriptions from UK Biobank and annotated if the patients had treatment failure events within one year of the first clopidogrel prescription; out of 502,527 patients, 1,824 patients were identified as treatment failure cases, and 6,859 patients were considered as control cases. From the dataset, we gathered diagnoses, prescriptions, and procedure records together per patient and organized them into visits with the same date to build models. The models were built for two different tasks, i.e., detection and prediction, and t
    
[^81]: LLM训练中的分词选择：微不足道还是至关重要？

    Tokenizer Choice For LLM Training: Negligible or Crucial?. (arXiv:2310.08754v1 [cs.LG])

    [http://arxiv.org/abs/2310.08754](http://arxiv.org/abs/2310.08754)

    在LLM训练中，分词器的选择对模型的后续性能、成本有着显著影响，常见的分词器评估指标不一定预测模型的性能。

    

    近期LLM的成功主要是由于策划训练数据集、扩展模型架构和数据集规模，以及预训练目标的进步，而分词器的影响则是一个盲点。通过对24个单语和多语言LLM进行训练，并对不同的分词器算法和参数进行大范围实验，我们对分词器选择对LLM的后续性能、训练和推理成本的影响进行了全面研究。我们的研究表明，分词器选择对模型的后续性能、训练和推理成本有着显著影响。特别是，我们发现常见的分词器评估指标（如丰富度和平等性）并不总是对模型的后续性能具有预测能力，这使得这些指标成为对分词器评估的可疑选择。此外，我们还展示了针对五种最常见的欧洲语言训练的多语言分词器需要词汇表的大小。

    The recent success of LLMs has been predominantly driven by curating the training dataset composition, scaling of model architectures and dataset sizes and advancements in pretraining objectives, leaving tokenizer influence as a blind spot. Shedding light on this underexplored area, we conduct a comprehensive study on the influence of tokenizer choice on LLM downstream performance by training 24 mono- and multilingual LLMs at a 2.6B parameter scale, ablating different tokenizer algorithms and parameterizations. Our studies highlight that the tokenizer choice can significantly impact the model's downstream performance, training and inference costs. In particular, we find that the common tokenizer evaluation metrics fertility and parity are not always predictive of model downstream performance, rendering these metrics a questionable choice for tokenizer evaluation. Furthermore, we show that multilingual tokenizers trained on the five most frequent European languages require vocabulary si
    
[^82]: 带有自适应主动学习未知约束的约束贝叶斯优化

    Constrained Bayesian Optimization with Adaptive Active Learning of Unknown Constraints. (arXiv:2310.08751v1 [cs.LG])

    [http://arxiv.org/abs/2310.08751](http://arxiv.org/abs/2310.08751)

    本文探讨了约束贝叶斯优化的理论和实际问题，提出了一种带有自适应主动学习未知约束的方法，在处理复杂约束场景时具有理论保证。

    

    在现实世界中，如科学实验设计、医学治疗设计和工业过程优化等领域，优化目标在约束条件下的情况是常见的，其中目标和约束都是黑盒函数。处理这些复杂场景的一种常用方法是贝叶斯优化（BO）。在理论行为方面，BO在无约束设置下相对较为理解，其原则已经被广泛探索和验证。然而，对于约束贝叶斯优化（CBO）来说，现有框架往往依赖于启发式方法或近似方法，没有同样程度的理论保证。在本文中，我们深入研究了约束贝叶斯优化的理论和实际问题，其中目标和约束可以独立评估且受到噪声的影响。通过认识到目标和约束都可以帮助识别高置信度的兴趣区域，

    Optimizing objectives under constraints, where both the objectives and constraints are black box functions, is a common scenario in real-world applications such as scientific experimental design, design of medical therapies, and industrial process optimization. One popular approach to handling these complex scenarios is Bayesian Optimization (BO). In terms of theoretical behavior, BO is relatively well understood in the unconstrained setting, where its principles have been well explored and validated. However, when it comes to constrained Bayesian optimization (CBO), the existing framework often relies on heuristics or approximations without the same level of theoretical guarantees.  In this paper, we delve into the theoretical and practical aspects of constrained Bayesian optimization, where the objective and constraints can be independently evaluated and are subject to noise. By recognizing that both the objective and constraints can help identify high-confidence regions of interest 
    
[^83]: Search-Adaptor: 用于信息检索的文本嵌入个性化定制

    Search-Adaptor: Text Embedding Customization for Information Retrieval. (arXiv:2310.08750v1 [cs.LG])

    [http://arxiv.org/abs/2310.08750](http://arxiv.org/abs/2310.08750)

    本文提出了一种名为Search-Adaptor的方法，用于定制化预训练的大型语言模型以改善信息检索和搜索的性能。通过修改文本嵌入，Search-Adaptor在多个真实世界数据集上展现出了稳定且显著的性能提升。

    

    由预训练的大型语言模型提取的文本嵌入具有显著的改善信息检索和搜索的潜力。除了一直以来常规使用的零样本设置外，利用相关查询-语料库配对数据的信息能力进一步提升了大型语言模型的能力。在本文中，我们提出了一种新的方法，名为Search-Adaptor，以便以高效且稳健的方式定制化预训练的大型语言模型进行信息检索。Search-Adaptor可以修改预训练的大型语言模型生成的原始文本嵌入，可以与任何大型语言模型进行集成，包括只能通过API访问的模型。在多个真实世界的英文和多语言检索数据集中，我们展示了Search-Adaptor的一致且显著的性能提升--例如，在13个BEIR数据集上，nDCG@10相对于Google Embedding APIs平均提高了5.2%以上。

    Text embeddings extracted by pre-trained Large Language Models (LLMs) have significant potential to improve information retrieval and search. Beyond the zero-shot setup in which they are being conventionally used, being able to take advantage of the information from the relevant query-corpus paired data has the power to further boost the LLM capabilities. In this paper, we propose a novel method, Search-Adaptor, for customizing LLMs for information retrieval in an efficient and robust way. Search-Adaptor modifies the original text embedding generated by pre-trained LLMs, and can be integrated with any LLM, including those only available via APIs. On multiple real-world English and multilingual retrieval datasets, we show consistent and significant performance benefits for Search-Adaptor -- e.g., more than 5.2% improvements over the Google Embedding APIs in nDCG@10 averaged over 13 BEIR datasets.
    
[^84]: 进化动态优化与机器学习

    Evolutionary Dynamic Optimization and Machine Learning. (arXiv:2310.08748v1 [cs.NE])

    [http://arxiv.org/abs/2310.08748](http://arxiv.org/abs/2310.08748)

    进化计算和机器学习的结合为优化复杂的机器学习任务提供了有价值的机会，并通过利用进化计算算法生成的数据来提供对搜索空间和种群动态的洞察。

    

    进化计算(EC)作为一种受自然渐进发展机制启发的强大的人工智能领域已经出现。然而，EC方法常常面临停滞、多样性损失、计算复杂性、种群初始化和过早收敛等挑战。为了克服这些限制，研究人员将学习算法与进化技术相结合。这种集成利用了EC算法在迭代搜索过程中生成的有价值的数据，提供了对搜索空间和种群动态的洞察。类似地，进化算法与机器学习(ML)之间的关系是相互的，因为EC方法为优化噪声、不准确和动态目标函数所描述的复杂ML任务提供了极好的机会。这些混合技术被称为进化机器学习(EML)，已在ML过程的各个阶段应用。

    Evolutionary Computation (EC) has emerged as a powerful field of Artificial Intelligence, inspired by nature's mechanisms of gradual development. However, EC approaches often face challenges such as stagnation, diversity loss, computational complexity, population initialization, and premature convergence. To overcome these limitations, researchers have integrated learning algorithms with evolutionary techniques. This integration harnesses the valuable data generated by EC algorithms during iterative searches, providing insights into the search space and population dynamics. Similarly, the relationship between evolutionary algorithms and Machine Learning (ML) is reciprocal, as EC methods offer exceptional opportunities for optimizing complex ML tasks characterized by noisy, inaccurate, and dynamic objective functions. These hybrid techniques, known as Evolutionary Machine Learning (EML), have been applied at various stages of the ML process. EC techniques play a vital role in tasks such
    
[^85]: 在多模态环境不确定性中，使用课程学习提高MARL的稳健性

    Robustness to Multi-Modal Environment Uncertainty in MARL using Curriculum Learning. (arXiv:2310.08746v1 [cs.LG])

    [http://arxiv.org/abs/2310.08746](http://arxiv.org/abs/2310.08746)

    本文首次提出了在多模态环境不确定性中提高MARL稳健性的广义问题，并通过课程学习技术提出了通用的稳健训练方法。

    

    多智能体强化学习（MARL）在解决现实世界的挑战中起着重要作用。然而，将训练好的策略从仿真环境无缝过渡到现实世界需要它对各种环境不确定性具有稳健性。现有的工作集中在在单个环境变量（即行动、状态或奖励）的不确定性下找到纳什均衡或最优策略。这是因为多智能体系统本身非常复杂和非平稳。然而，在现实世界中，不确定性可能同时发生在多个环境变量中。本研究首次提出了在MARL中针对多模态环境不确定性的广义问题。为此，我们提出了一种基于课程学习技术的多模态不确定性的通用稳健训练方法。我们同时处理两种不同的环境不确定性，并在合作和竞争的MARL环境中呈现了广泛的结果，证明了我们的方法的有效性。

    Multi-agent reinforcement learning (MARL) plays a pivotal role in tackling real-world challenges. However, the seamless transition of trained policies from simulations to real-world requires it to be robust to various environmental uncertainties. Existing works focus on finding Nash Equilibrium or the optimal policy under uncertainty in one environment variable (i.e. action, state or reward). This is because a multi-agent system itself is highly complex and unstationary. However, in real-world situation uncertainty can occur in multiple environment variables simultaneously. This work is the first to formulate the generalised problem of robustness to multi-modal environment uncertainty in MARL. To this end, we propose a general robust training approach for multi-modal uncertainty based on curriculum learning techniques. We handle two distinct environmental uncertainty simultaneously and present extensive results across both cooperative and competitive MARL environments, demonstrating th
    
[^86]: Transformer语言模型中跨任务的电路组件复用

    Circuit Component Reuse Across Tasks in Transformer Language Models. (arXiv:2310.08744v1 [cs.CL])

    [http://arxiv.org/abs/2310.08744](http://arxiv.org/abs/2310.08744)

    这项工作证明了在Transformer语言模型中，电路组件可以在不同任务之间复用并产生相似的功能，为更高级的模型理解做出贡献。

    

    最近在机制可解释性方面的研究表明，通过电路分析可以成功地逆向工程语言模型的行为。然而，一个常见的批评是每个电路都是任务特定的，因此这样的分析不能为更高级的理解模型做出贡献。在这项工作中，我们提出证据表明洞察力（关于特定头部的低级发现和关于一般算法的高级发现）确实可以在任务之间进行泛化。具体而言，我们研究了Wang等人（2022）在间接宾语识别任务（IOI）中发现的电路，并展示了这个电路在更大的GPT2模型上的重现，以及在看似不同的任务中大部分被复用来解决问题：彩色物体（Ippolito和Callison-Burch，2023）。我们提供证据表明两个任务底层的过程在功能上非常相似，并且在电路中的注意力头部之间有大约78％的重叠。我们进一步展示了一个概念验证干预实验

    Recent work in mechanistic interpretability has shown that behaviors in language models can be successfully reverse-engineered through circuit analysis. A common criticism, however, is that each circuit is task-specific, and thus such analysis cannot contribute to understanding the models at a higher level. In this work, we present evidence that insights (both low-level findings about specific heads and higher-level findings about general algorithms) can indeed generalize across tasks. Specifically, we study the circuit discovered in Wang et al. (2022) for the Indirect Object Identification (IOI) task and 1.) show that it reproduces on a larger GPT2 model, and 2.) that it is mostly reused to solve a seemingly different task: Colored Objects (Ippolito & Callison-Burch, 2023). We provide evidence that the process underlying both tasks is functionally very similar, and contains about a 78% overlap in in-circuit attention heads. We further present a proof-of-concept intervention experiment
    
[^87]: 基于深度学习的微卫星不稳定性预测器的开发和验证，来自于前列腺癌全切片图像

    Development and Validation of a Deep Learning-Based Microsatellite Instability Predictor from Prostate Cancer Whole-Slide Images. (arXiv:2310.08743v1 [cs.CV])

    [http://arxiv.org/abs/2310.08743](http://arxiv.org/abs/2310.08743)

    本研究开发和验证了一种基于深度学习的微卫星不稳定性预测器，可以从前列腺癌全切片图像中预测MSI状态，有助于识别最有可能受益于免疫治疗的患者。

    

    微卫星不稳定性高（MSI-H）是免疫检查点抑制剂疗法的一种肿瘤不可知标志物。然而，在前列腺癌中，由于低患病率和检测成本较高，MSI状态通常不进行常规测试。因此，从血液染色剂靛蓝和伊红（H&E）染色的全切片图像（WSIs）中预测MSI状态，可以识别出最有可能受益于确认测试并有资格接受免疫治疗的前列腺癌患者。分析了连续转诊至我们机构的前列腺癌患者的去匿名记录中的前列腺活检和手术切除标本。通过下一代测序确定了他们的MSI状态。在截止日期之前的患者分为算法开发集（n=4015, MSI-H 1.8%）和配对验证集（n=173, MSI-H 19.7%），每个样本由两个连续的切片组成，一个内部染色和扫描，另一个在外部某个地点。截止日期之后的患者形成了时间验证集。

    Microsatellite instability-high (MSI-H) is a tumor agnostic biomarker for immune checkpoint inhibitor therapy. However, MSI status is not routinely tested in prostate cancer, in part due to low prevalence and assay cost. As such, prediction of MSI status from hematoxylin and eosin (H&E) stained whole-slide images (WSIs) could identify prostate cancer patients most likely to benefit from confirmatory testing and becoming eligible for immunotherapy. Prostate biopsies and surgical resections from de-identified records of consecutive prostate cancer patients referred to our institution were analyzed. Their MSI status was determined by next generation sequencing. Patients before a cutoff date were split into an algorithm development set (n=4015, MSI-H 1.8%) and a paired validation set (n=173, MSI-H 19.7%) that consisted of two serial sections from each sample, one stained and scanned internally and the other at an external site. Patients after the cutoff date formed the temporal validation 
    
[^88]: 用RNA对比学习改进预测准确性

    Splicing Up Your Predictions with RNA Contrastive Learning. (arXiv:2310.08738v1 [cs.LG])

    [http://arxiv.org/abs/2310.08738](http://arxiv.org/abs/2310.08738)

    本研究将对比学习技术扩展到基因组数据，利用选择性剪接和基因复制产生的序列之间的功能相似性，学习到广义RNA同位素表示。我们的预训练策略在RNA半衰期和平均核糖体负载预测等任务上取得了竞争性的结果，在低数据条件下皮尔逊相关性增加了多达两倍。

    

    鉴于基因组数据迅速积累，我们对RNA调控代码的理解尚不完整。近期在其他领域的自我监督方法已经证明了学习数据生成过程中的规则（例如语言中的句子结构）的能力。受此启发，我们通过利用通过选择性剪接和基因复制产生的序列之间的功能相似性，将对比学习技术扩展到基因组数据上。我们的新颖数据集和对比目标使得学习到广义RNA同位素表示。我们在RNA半衰期和平均核糖体负载预测等下游任务上证明了它们的实用性。我们的预训练策略在两项任务的线性探索中取得了竞争性的结果，并在低数据条件下皮尔逊相关性增加了多达两倍。重要的是，我们对学习的潜在空间的探索揭示了我们对比目标的语义有意义的表示。

    In the face of rapidly accumulating genomic data, our understanding of the RNA regulatory code remains incomplete. Recent self-supervised methods in other domains have demonstrated the ability to learn rules underlying the data-generating process such as sentence structure in language. Inspired by this, we extend contrastive learning techniques to genomic data by utilizing functional similarities between sequences generated through alternative splicing and gene duplication. Our novel dataset and contrastive objective enable the learning of generalized RNA isoform representations. We validate their utility on downstream tasks such as RNA half-life and mean ribosome load prediction. Our pre-training strategy yields competitive results using linear probing on both tasks, along with up to a two-fold increase in Pearson correlation in low-data conditions. Importantly, our exploration of the learned latent space reveals that our contrastive objective yields semantically meaningful representa
    
[^89]: 可证保健康商务字体尹包通过随机平滑(译注)水。

    Provably Robust Cost-Sensitive Learning via Randomized Smoothing. (arXiv:2310.08732v1 [cs.LG])

    [http://arxiv.org/abs/2310.08732](http://arxiv.org/abs/2310.08732)

    本研究通过随机平滑认证框架，为成本敏感的稳健分类器提供了严格的稳健性保证，并通过优化方案针对不同数据子组设计了细粒度认证半径，取得了优越的性能。

    

    我们关注于在成本敏感的情景下学习对抗性稳健分类器，在这种情况下，不同类别的对抗性变换的潜在危害被编码在一个二进制成本矩阵中。现有的方法要么是经验性的，无法证明稳健性，要么存在固有的可扩展性问题。在这项工作中，我们研究了随机平滑，一种更可扩展的稳健性认证框架，是否可以用于证明成本敏感的稳健性。建立在一种成本敏感认证半径的概念之上，我们展示了如何调整标准的随机平滑认证流程，为任何成本矩阵产生严格的稳健性保证。此外，通过针对不同数据子组设计的细粒度认证半径优化方案，我们提出了一种算法，用于训练针对成本敏感稳健性优化的平滑分类器。在图像基准测试和真实的医学数据集上进行了大量实验，证明了我们方法的优越性。

    We focus on learning adversarially robust classifiers under a cost-sensitive scenario, where the potential harm of different classwise adversarial transformations is encoded in a binary cost matrix. Existing methods are either empirical that cannot certify robustness or suffer from inherent scalability issues. In this work, we study whether randomized smoothing, a more scalable robustness certification framework, can be leveraged to certify cost-sensitive robustness. Built upon a notion of cost-sensitive certified radius, we show how to adapt the standard randomized smoothing certification pipeline to produce tight robustness guarantees for any cost matrix. In addition, with fine-grained certified radius optimization schemes specifically designed for different data subgroups, we propose an algorithm to train smoothed classifiers that are optimized for cost-sensitive robustness. Extensive experiments on image benchmarks and a real-world medical dataset demonstrate the superiority of our
    
[^90]: 一个简单的方法在世界模型中实现新颖性检测

    A Simple Way to Incorporate Novelty Detection in World Models. (arXiv:2310.08731v1 [cs.AI])

    [http://arxiv.org/abs/2310.08731](http://arxiv.org/abs/2310.08731)

    本文提出了一个简单的方法，通过利用世界模型幻觉状态和真实观察状态的不匹配性作为异常分数，将新颖性检测纳入世界模型强化学习代理中。在新环境中与传统方法相比，我们的工作具有优势。

    

    使用世界模型进行强化学习已经取得了显著的成功。然而，当世界机制或属性发生突然变化时，代理的性能和可靠性可能会显著下降。我们将视觉属性或状态转换的突变称为“新颖性”。在生成的世界模型框架中实施新颖性检测是保护部署时代理的关键任务。在本文中，我们提出了一种简单的边界方法，用于将新颖性检测纳入世界模型强化学习代理中，通过利用世界模型幻觉状态和真实观察状态的不匹配性作为异常分数。首先，我们提供了与序列决策相关的新颖性检测本体论，然后我们提供了在代理在世界模型中学习的转换分布中检测新颖性的有效方法。最后，我们展示了我们的工作在新环境中与传统方法相比的优势。

    Reinforcement learning (RL) using world models has found significant recent successes. However, when a sudden change to world mechanics or properties occurs then agent performance and reliability can dramatically decline. We refer to the sudden change in visual properties or state transitions as {\em novelties}. Implementing novelty detection within generated world model frameworks is a crucial task for protecting the agent when deployed. In this paper, we propose straightforward bounding approaches to incorporate novelty detection into world model RL agents, by utilizing the misalignment of the world model's hallucinated states and the true observed states as an anomaly score. We first provide an ontology of novelty detection relevant to sequential decision making, then we provide effective approaches to detecting novelties in a distribution of transitions learned by an agent in a world model. Finally, we show the advantage of our work in a novel environment compared to traditional ma
    
[^91]: 基于异质性的图神经网络用于不平衡分类

    Heterophily-Based Graph Neural Network for Imbalanced Classification. (arXiv:2310.08725v1 [cs.LG])

    [http://arxiv.org/abs/2310.08725](http://arxiv.org/abs/2310.08725)

    我们提出了一种基于异质性的图神经网络方法用于解决不平衡分类问题，通过考虑图的异质性以及类别不平衡之间的关系，我们提出了一种高效的Fast Im-GBK方法，可以有效地解决不平衡图上的分类问题。

    

    图神经网络（GNN）已经在解决图相关问题，包括节点分类方面显示出了潜力。然而，传统的GNN假设数据在各个类别之间均匀分布，而在现实世界的情境中，某些类别往往严重不平衡。这导致标准GNN在不平衡图上性能不佳。本文通过考虑图的异质性，引入了一种独特的方法来处理图上的不平衡分类问题。我们研究了类别不平衡与图的异质性之间的复杂关系，揭示了少数类不仅表现出样本稀缺，还表现出较低的同质性水平，从而促进了错误信息在相邻节点之间的传播。基于这一洞察，我们提出了一种高效的方法，称为Fast Im-GBK，它将不平衡分类策略与异质性感知的GNN有效地结合起来，以解决类别不平衡问题。

    Graph neural networks (GNNs) have shown promise in addressing graph-related problems, including node classification. However, conventional GNNs assume an even distribution of data across classes, which is often not the case in real-world scenarios, where certain classes are severely underrepresented. This leads to suboptimal performance of standard GNNs on imbalanced graphs. In this paper, we introduce a unique approach that tackles imbalanced classification on graphs by considering graph heterophily. We investigate the intricate relationship between class imbalance and graph heterophily, revealing that minority classes not only exhibit a scarcity of samples but also manifest lower levels of homophily, facilitating the propagation of erroneous information among neighboring nodes. Drawing upon this insight, we propose an efficient method, called Fast Im-GBK, which integrates an imbalance classification strategy with heterophily-aware GNNs to effectively address the class imbalance probl
    
[^92]: 使用深度学习为测量设计可观测量的研究

    Designing Observables for Measurements with Deep Learning. (arXiv:2310.08717v1 [physics.data-an])

    [http://arxiv.org/abs/2310.08717](http://arxiv.org/abs/2310.08717)

    该研究提出使用机器学习来设计最优的可观测量，通过神经网络输出的展开的微分截面包含了关于感兴趣参数的最多信息，并且可以通过构造方法进行很好的测量。

    

    在粒子物理和核物理的许多分析中，使用模拟来推断底层物理模型的基本、有效或现象学参数。当使用展开的截面进行推断时，可观测量是通过物理直觉和经验法则进行设计的。我们提出使用机器学习来设计最优的可观测量。通过神经网络输出的展开的微分截面包含了关于感兴趣参数的最多信息，并且可以通过构造方法进行很好的测量。我们使用两个物理模型来演示这个想法，这两个模型用于深度无弹性散射中的包含性测量。

    Many analyses in particle and nuclear physics use simulations to infer fundamental, effective, or phenomenological parameters of the underlying physics models. When the inference is performed with unfolded cross sections, the observables are designed using physics intuition and heuristics. We propose to design optimal observables with machine learning. Unfolded, differential cross sections in a neural network output contain the most information about parameters of interest and can be well-measured by construction. We demonstrate this idea using two physics models for inclusive measurements in deep inelastic scattering.
    
[^93]: Transformer Choice Net: 一种用于选择预测的Transformer神经网络

    Transformer Choice Net: A Transformer Neural Network for Choice Prediction. (arXiv:2310.08716v1 [cs.LG])

    [http://arxiv.org/abs/2310.08716](http://arxiv.org/abs/2310.08716)

    本论文介绍了一种适用于预测多个选择的Transformer神经网络架构，通过考虑顾客和项目的特征，以及上下文信息（如可选项的范围和定制需求），解决了离散选择模型在顾客选择多个项目时的挑战。

    

    在市场营销、经济学和运筹学中，离散选择模型，如多项式逻辑模型、Probit模型或混合逻辑模型广泛使用：给定一组可选项，顾客被模拟为选择其中之一以最大化（潜在的）效用函数。然而，将这些模型推广到顾客选择多个项目的情况（如电子商务购物）却存在问题。虽然可以建立合理的顾客行为模型，但由于项目子集的组合爆炸，对于这些模型的估计变得非常具有挑战性。在本文中，我们开发了一种有助于预测多个选择的Transformer神经网络架构，即Transformer Choice Net。事实证明，Transformer网络对于这个任务特别适用，因为它们不仅考虑了顾客和项目的特征，还考虑了上下文，即可选项的范围以及定制化需求。

    Discrete-choice models, such as Multinomial Logit, Probit, or Mixed-Logit, are widely used in Marketing, Economics, and Operations Research: given a set of alternatives, the customer is modeled as choosing one of the alternatives to maximize a (latent) utility function. However, extending such models to situations where the customer chooses more than one item (such as in e-commerce shopping) has proven problematic. While one can construct reasonable models of the customer's behavior, estimating such models becomes very challenging because of the combinatorial explosion in the number of possible subsets of items. In this paper we develop a transformer neural network architecture, the Transformer Choice Net, that is suitable for predicting multiple choices. Transformer networks turn out to be especially suitable for this task as they take into account not only the features of the customer and the items but also the context, which in this case could be the assortment as well as the custom
    
[^94]: Waymax: 一种用于大规模自主驾驶研究的加速、数据驱动模拟器

    Waymax: An Accelerated, Data-Driven Simulator for Large-Scale Autonomous Driving Research. (arXiv:2310.08710v1 [cs.RO])

    [http://arxiv.org/abs/2310.08710](http://arxiv.org/abs/2310.08710)

    Waymax是一种加速、数据驱动的模拟器，用于大规模自主驾驶研究。它使用真实世界驾驶数据来创建真实的多智能体交互场景，并支持大规模、分布式机器学习工作流程。通过学习和硬编码的行为模型，Waymax可以提供逼真的交互体验。

    

    模拟是以安全和成本效益的方式开发和评估自主车规划软件的重要工具。然而，真实的模拟需要精确建模复杂的多智能体交互行为。为了应对这些挑战，我们介绍了Waymax，一种新的多智能体场景下用于自主驾驶的数据驱动模拟器，专为大规模模拟和测试设计。Waymax使用公开发布的真实世界驾驶数据（例如Waymo开放运动数据集）来初始化或回放多样化的多智能体模拟场景。它完全运行在硬件加速器（如TPUs/GPUs）上，并支持图内模拟以进行训练，适用于现代大规模、分布式机器学习工作流程。为了支持在线训练和评估，Waymax包含了几个学习的和硬编码的行为模型，可以在模拟中进行真实的交互。为了补充Waymax，我们还对一套流行的仿真算法进行了基准测试。

    Simulation is an essential tool to develop and benchmark autonomous vehicle planning software in a safe and cost-effective manner. However, realistic simulation requires accurate modeling of nuanced and complex multi-agent interactive behaviors. To address these challenges, we introduce Waymax, a new data-driven simulator for autonomous driving in multi-agent scenes, designed for large-scale simulation and testing. Waymax uses publicly-released, real-world driving data (e.g., the Waymo Open Motion Dataset) to initialize or play back a diverse set of multi-agent simulated scenarios. It runs entirely on hardware accelerators such as TPUs/GPUs and supports in-graph simulation for training, making it suitable for modern large-scale, distributed machine learning workflows. To support online training and evaluation, Waymax includes several learned and hard-coded behavior models that allow for realistic interaction within simulation. To supplement Waymax, we benchmark a suite of popular imita
    
[^95]: 多项式时间的密码分析提取神经网络模型

    Polynomial Time Cryptanalytic Extraction of Neural Network Models. (arXiv:2310.08708v1 [cs.LG])

    [http://arxiv.org/abs/2310.08708](http://arxiv.org/abs/2310.08708)

    本文提出了几种新技术，以多项式数量的查询和多项式数量的时间从黑盒实现的ReLU-based深度神经网络中提取出所有实值参数。

    

    目前在训练深度神经网络（DNN）上花费了数十亿美元和无数GPU小时，因此，确定在给定对其黑盒实现的访问权限时，提取此类神经网络的所有参数的难度至关重要。在过去30年中，研究了许多版本的这个问题，而对ReLU-based深度神经网络的现有最佳攻击是由Carlini、Jagielski和Mironov在2020年的加密研讨会上提出的。这类似于对加密系统的差分选择明文攻击，其黑盒实现中嵌入了一个密钥，并且需要多项式数量的查询，但指数数量的时间（作为神经元数量的函数）。在本文中，我们通过开发几种新技术来改进这种攻击，从而使我们能够使用多项式数量的查询和多项式数量的时间提取具有任意高精度的ReLU-based DNN的所有实值参数。

    Billions of dollars and countless GPU hours are currently spent on training Deep Neural Networks (DNNs) for a variety of tasks. Thus, it is essential to determine the difficulty of extracting all the parameters of such neural networks when given access to their black-box implementations. Many versions of this problem have been studied over the last 30 years, and the best current attack on ReLU-based deep neural networks was presented at Crypto 2020 by Carlini, Jagielski, and Mironov. It resembles a differential chosen plaintext attack on a cryptosystem, which has a secret key embedded in its black-box implementation and requires a polynomial number of queries but an exponential amount of time (as a function of the number of neurons). In this paper, we improve this attack by developing several new techniques that enable us to extract with arbitrarily high precision all the real-valued parameters of a ReLU-based DNN using a polynomial number of queries and a polynomial amount of time. We
    
[^96]: ELDEN: 基于本地依赖的探索

    ELDEN: Exploration via Local Dependencies. (arXiv:2310.08702v1 [cs.LG])

    [http://arxiv.org/abs/2310.08702](http://arxiv.org/abs/2310.08702)

    在具有复杂依赖关系的环境中，我们提出了ELDEN，一种新的内在奖励，通过使用代理对实体之间的相互影响的不确定性，鼓励有效地探索状态空间。

    

    对于具有大状态空间和稀疏奖励的任务，强化学习一直面临着困难。在这些任务中，代理需要有效地探索状态空间，直到找到奖励。为了解决这个问题，社区提出了使用内在奖励来增强奖励函数的方法，内在奖励是一种鼓励代理访问有趣状态的奖励信号。在本文中，我们提出了一种新的方式来定义具有因式状态空间和复杂依赖关系的环境中的有趣状态，其中代理的动作可能会改变一个实体的值，进而可能影响另一个实体的值。我们的观点是，在这些环境中，探索有趣状态的关键是代理不确定实体（如代理或物体）是否相互影响，而不仅仅是如何相互影响。我们提出了"ELDEN: 基于本地依赖的探索"，这是一种新颖的内在奖励，促进了对新的相互作用的发现。

    Tasks with large state space and sparse rewards present a longstanding challenge to reinforcement learning. In these tasks, an agent needs to explore the state space efficiently until it finds a reward. To deal with this problem, the community has proposed to augment the reward function with intrinsic reward, a bonus signal that encourages the agent to visit interesting states. In this work, we propose a new way of defining interesting states for environments with factored state spaces and complex chained dependencies, where an agent's actions may change the value of one entity that, in order, may affect the value of another entity. Our insight is that, in these environments, interesting states for exploration are states where the agent is uncertain whether (as opposed to how) entities such as the agent or objects have some influence on each other. We present ELDEN, Exploration via Local DepENdencies, a novel intrinsic reward that encourages the discovery of new interactions between en
    
[^97]: 分子设计的核-弹性自编码器

    Kernel-Elastic Autoencoder for Molecular Design. (arXiv:2310.08685v1 [cs.LG])

    [http://arxiv.org/abs/2310.08685](http://arxiv.org/abs/2310.08685)

    核-弹性自编码器（KAE）是一种在分子设计领域具有增强性能的自监督生成模型。KAE实现了有效生成和准确重构的挑战，具有显著的多样性和最先进的性能。此外，KAE还可以根据特定条件生成分子，并在分子对接应用中表现出优秀的性能。

    

    我们引入了核-弹性自编码器（KAE），这是一种基于变压器架构的自监督生成模型，具有增强的分子设计性能。KAE基于两种新的损失函数进行建模：修改的最大均匀位移和加权重构。KAE解决了同时实现有效生成和准确重构的长期挑战。KAE在分子生成中实现了显著的多样性，并在独立测试数据集上保持了近乎完美的重构，超越了先前的分子生成模型。KAE实现了条件生成，并允许基于波束搜索进行解码，从而在受限优化中实现了最先进的性能。此外，KAE可以根据分子对接应用中的有利结合亲和力进行条件生成，并通过AutoDock Vina和Glide得分进行确认，胜过训练数据集中的所有现有候选分子。除了分子设计，我们预期KAE可以应用于其他领域。

    We introduce the Kernel-Elastic Autoencoder (KAE), a self-supervised generative model based on the transformer architecture with enhanced performance for molecular design. KAE is formulated based on two novel loss functions: modified maximum mean discrepancy and weighted reconstruction. KAE addresses the long-standing challenge of achieving valid generation and accurate reconstruction at the same time. KAE achieves remarkable diversity in molecule generation while maintaining near-perfect reconstructions on the independent testing dataset, surpassing previous molecule-generating models. KAE enables conditional generation and allows for decoding based on beam search resulting in state-of-the-art performance in constrained optimizations. Furthermore, KAE can generate molecules conditional to favorable binding affinities in docking applications as confirmed by AutoDock Vina and Glide scores, outperforming all existing candidates from the training dataset. Beyond molecular design, we antic
    
[^98]: Atari强化学习的虚拟增强现实技术

    Virtual Augmented Reality for Atari Reinforcement Learning. (arXiv:2310.08683v1 [cs.LG])

    [http://arxiv.org/abs/2310.08683](http://arxiv.org/abs/2310.08683)

    本文研究了使用最先进的图像分割模型提高Atari强化学习智能体性能的方法

    

    强化学习在游戏领域取得了重大突破，尤其是谷歌DeepMind的AlphaGo击败了人类围棋冠军柯洁。这一胜利也得益于Atari学习环境(ALE)，ALE在RL研究中具有重要意义，促进了AlphaGo和其他RL算法的发展。当前的Atari视频游戏RL研究中，RL智能体对环境的感知基于Atari视频游戏屏幕的原始像素数据，且图像预处理很少。相反，最先进的ML研究在增强图像感知方面取得了突破，其中一个显著的例子是Meta Research的“任何物体”的分割模型(SAM)，它是一个能够在没有训练的情况下分割图像的基础模型。本文探讨一个新颖的方法论问题：像SAM这样的最先进图像分割模型能否提高RL智能体在玩Atari视频游戏时的性能。

    Reinforcement Learning (RL) has achieved significant milestones in the gaming domain, most notably Google DeepMind's AlphaGo defeating human Go champion Ken Jie. This victory was also made possible through the Atari Learning Environment (ALE): The ALE has been foundational in RL research, facilitating significant RL algorithm developments such as AlphaGo and others. In current Atari video game RL research, RL agents' perceptions of its environment is based on raw pixel data from the Atari video game screen with minimal image preprocessing. Contrarily, cutting-edge ML research, external to the Atari video game RL research domain, is focusing on enhancing image perception. A notable example is Meta Research's "Segment Anything Model" (SAM), a foundation model capable of segmenting images without prior training (zero-shot). This paper addresses a novel methodical question: Can state-of-the-art image segmentation models such as SAM improve the performance of RL agents playing Atari video g
    
[^99]: GDL-DS: 分布转换下几何深度学习的基准测试

    GDL-DS: A Benchmark for Geometric Deep Learning under Distribution Shifts. (arXiv:2310.08677v1 [cs.LG])

    [http://arxiv.org/abs/2310.08677](http://arxiv.org/abs/2310.08677)

    GDL-DS是一个基准测试，用于评估几何深度学习模型在具有分布转换的场景中的性能。它包括多个科学领域的评估数据集，并研究了不同级别的超出分布特征的信息访问。

    

    几何深度学习(GDL)在各个科学领域引起了广泛关注，主要是因为其擅长对具有复杂几何结构的数据进行建模。然而，很少有研究探索其在处理分布转换问题上的能力，这是许多相关应用中常见的挑战。为了弥补这一空白，我们提出了GDL-DS，这是一个全面的基准测试，旨在评估GDL模型在具有分布转换的场景中的性能。我们的评估数据集涵盖了从粒子物理学和材料科学到生物化学的不同科学领域，并包括各种分布转换，包括条件、协变和概念转换。此外，我们研究了来自超出分布的测试数据的信息访问的三个级别，包括没有超出分布的信息、只有带标签的超出分布特征和带有少数标签的超出分布特征。总体而言，我们的基准测试涉及30个不同的实验设置，并评估3种信息访问水平。

    Geometric deep learning (GDL) has gained significant attention in various scientific fields, chiefly for its proficiency in modeling data with intricate geometric structures. Yet, very few works have delved into its capability of tackling the distribution shift problem, a prevalent challenge in many relevant applications. To bridge this gap, we propose GDL-DS, a comprehensive benchmark designed for evaluating the performance of GDL models in scenarios with distribution shifts. Our evaluation datasets cover diverse scientific domains from particle physics and materials science to biochemistry, and encapsulate a broad spectrum of distribution shifts including conditional, covariate, and concept shifts. Furthermore, we study three levels of information access from the out-of-distribution (OOD) testing data, including no OOD information, only OOD features without labels, and OOD features with a few labels. Overall, our benchmark results in 30 different experiment settings, and evaluates 3 
    
[^100]: 机器学习中的鼓励对象选择：在学生助学金续签领域实验中的因果与预测性目标定位

    Machine Learning Who to Nudge: Causal vs Predictive Targeting in a Field Experiment on Student Financial Aid Renewal. (arXiv:2310.08672v1 [econ.EM])

    [http://arxiv.org/abs/2310.08672](http://arxiv.org/abs/2310.08672)

    通过对学生进行因果与预测性定位的比较，研究探讨了在学生助学金续签领域实验中鼓励对象选择的价值。这项大规模实地实验揭示了定位干预对于不同学生的效果，为干预策略的优化提供了参考。

    

    在许多情境下，干预可能对某些人比其他人更有效，因此定位干预可能是有益的。我们通过一个规模庞大的实地实验（超过53,000名大学生）来分析在学生助学金续签前使用“鼓励”策略的价值。我们首先使用基线方法进行定位。首先，我们基于一个估计异质处理效应的因果森林进行定位，并根据估计出的拥有最高处理效应的学生来进行处理。接下来，我们评估两种替代的定位策略，一种是针对在没有干预的情况下预测到低助学金续签概率的学生，另一种是针对预测到高概率的学生。预测的基线结果并不是定位的理想标准，而且在先验上也不清楚是优先考虑低、高还是中间的预测。

    In many settings, interventions may be more effective for some individuals than others, so that targeting interventions may be beneficial. We analyze the value of targeting in the context of a large-scale field experiment with over 53,000 college students, where the goal was to use "nudges" to encourage students to renew their financial-aid applications before a non-binding deadline. We begin with baseline approaches to targeting. First, we target based on a causal forest that estimates heterogeneous treatment effects and then assigns students to treatment according to those estimated to have the highest treatment effects. Next, we evaluate two alternative targeting policies, one targeting students with low predicted probability of renewing financial aid in the absence of the treatment, the other targeting those with high probability. The predicted baseline outcome is not the ideal criterion for targeting, nor is it a priori clear whether to prioritize low, high, or intermediate predic
    
[^101]: 每个参数都很重要：确保具有动态异构模型缩减的联邦学习的收敛性

    Every Parameter Matters: Ensuring the Convergence of Federated Learning with Dynamic Heterogeneous Models Reduction. (arXiv:2310.08670v1 [cs.LG])

    [http://arxiv.org/abs/2310.08670](http://arxiv.org/abs/2310.08670)

    这项研究提出了一个统一的异构联邦学习算法框架，并证明了在一定条件下，这些算法对于不同类型的数据都能收敛到标准联邦学习的一个稳定点。此外，研究还揭示了两个关键因素：模型提取噪声和最小覆盖指数，提倡了本地模型选择和全局模型选择的联合设计。

    

    在跨设备的联邦学习中，由于底端设备存在资源瓶颈，那些可能提供独特贡献的低端设备被排除在训练大模型之外，这给联邦学习带来了重大挑战。最近的研究工作集中在异构模型的联邦学习上，通过从全局模型中提取缩小尺寸的模型，并将其应用于本地设备。尽管实证成功，但对于该方法的收敛性的一般理论保证仍然是一个未解决的问题。在本文中，我们提出了一个统一的异构联邦学习算法框架，并提供了一种通用的收敛性分析。特别地，我们证明了在某些充分条件下，对于IID和非IID数据，这些算法收敛到标准联邦学习的一个稳定点，适用于一般的平滑成本函数。此外，我们揭示了影响其收敛性的两个关键因素：模型提取噪声和最小覆盖指数，并主张了本地模型选择和全局模型选择的联合设计。

    Cross-device Federated Learning (FL) faces significant challenges where low-end clients that could potentially make unique contributions are excluded from training large models due to their resource bottlenecks. Recent research efforts have focused on model-heterogeneous FL, by extracting reduced-size models from the global model and applying them to local clients accordingly. Despite the empirical success, general theoretical guarantees of convergence on this method remain an open question. In this paper, we present a unifying framework for heterogeneous FL algorithms with online model extraction and provide a general convergence analysis. In particular, we prove that under certain sufficient conditions and for both IID and non-IID data, these algorithms converge to a stationary point of standard FL for general smooth cost functions. Moreover, we illuminate two key factors impacting its convergence: model-extraction noise and minimum coverage index, advocating a joint design of local 
    
[^102]: 使用Transformer进行计数和算法推广

    Counting and Algorithmic Generalization with Transformers. (arXiv:2310.08661v1 [cs.LG])

    [http://arxiv.org/abs/2310.08661](http://arxiv.org/abs/2310.08661)

    这项研究分析了在计数任务中的算法推广，并证明了标准的Transformer的架构决策会阻碍其在超出分布任务上的性能。通过消除问题操作，修改后的Transformer在计数方面展示出了良好的算法推广性能。

    

    机器学习中的算法推广是指学习生成数据的底层算法，以一种对超出分布的方式进行泛化的能力。这对大多数机器学习算法来说通常是一个困难的任务。在这里，我们分析了在计数时需要的算法推广，无论是隐式还是显式。我们表明标准的Transformer是基于阻碍这类任务的架构决策。特别是，我们讨论了使用层归一化和通过softmax归一化注意力权重的后果。通过消除这些问题操作，我们证明修改后的Transformer可以在计数方面展示出良好的算法推广性能，同时采用非常轻量级的架构。

    Algorithmic generalization in machine learning refers to the ability to learn the underlying algorithm that generates data in a way that generalizes out-of-distribution. This is generally considered a difficult task for most machine learning algorithms. Here, we analyze algorithmic generalization when counting is required, either implicitly or explicitly. We show that standard Transformers are based on architectural decisions that hinder out-of-distribution performance for such tasks. In particular, we discuss the consequences of using layer normalization and of normalizing the attention weights via softmax. With ablation of the problematic operations, we demonstrate that a modified transformer can exhibit a good algorithmic generalization performance on counting while using a very lightweight architecture.
    
[^103]: 在不探索的情况下学习联合波束成形的RL策略：一种批量约束的离策略方法

    Learning RL-Policies for Joint Beamforming Without Exploration: A Batch Constrained Off-Policy Approach. (arXiv:2310.08660v1 [cs.LG])

    [http://arxiv.org/abs/2310.08660](http://arxiv.org/abs/2310.08660)

    本研究提出了一种批量约束的离策略方法，用于解决联合波束成形的参数优化问题。通过使用深度强化学习技术，克服了传统方法中计算复杂度高和无法准确建模的难题。

    

    在这个项目中，我们考虑了网络参数优化的问题，以实现速率最大化。我们将其构建为功率控制、波束成形和干扰消除的联合优化问题。我们考虑了多个基站与多个用户设备通信的情况。由于穷举搜索的指数计算复杂度，我们使用深度强化学习（RL）技术来解决该非凸优化问题。现代通信系统以其难以准确建模的行为而臭名昭著。这限制了我们使用基于RL的算法，因为需要与环境进行交互以便代理能够高效地探索和学习。此外，由于失败的高成本，将算法部署在现实世界中进行探索和学习是不明智的。与先前提出的基于RL的解决方案（如基于深度Q网络（DQN）的控制）相比，我们提出采用一种离策略方法

    In this project, we consider the problem of network parameter optimization for rate maximization. We frame this as a joint optimization problem of power control, beam forming, and interference cancellation. We consider the setting where multiple Base Stations (BSs) are communicating with multiple user equipments (UEs). Because of the exponential computational complexity of brute force search, we instead solve this non-convex optimization problem using deep reinforcement learning (RL) techniques. The modern communication systems are notorious for their difficulty in exactly modeling their behaviour. This limits us in using RL based algorithms as interaction with the environment is needed for the agent to explore and learn efficiently. Further, it is ill advised to deploy the algorithm in real world for exploration and learning because of the high cost of failure. In contrast to the previous RL-based solutions proposed, such as deep-Q network (DQN) based control, we propose taking an off
    
[^104]: LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models

    LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models. (arXiv:2310.08659v1 [cs.CL])

    [http://arxiv.org/abs/2310.08659](http://arxiv.org/abs/2310.08659)

    本论文提出了LoftQ：一种针对大型语言模型的LoRA精调感知量化框架。该框架同时对LLM进行量化，并为LoRA精调找到适当的低秩初始化，以缓解量化模型和全精度模型之间的差异，并显著提高了下游任务的泛化能力。

    

    量化是为大型语言模型提供服务的不可或缺的技术，并最近被应用于LoRA精调中。本文关注在预训练模型上同时应用量化和LoRA精调的场景。在这种情况下，常常观察到完整精调和量化加LoRA精调方法之间在下游任务表现上存在一致的差距。为了解决这个问题，我们提出了LoftQ（LoRA-Fine-Tuning-aware Quantization）——一种新的量化框架，用于同时对LLM进行量化，并找到适当的低秩初始化来进行LoRA精调。这种初始化减轻了量化模型和全精度模型之间的差异，并显著提高了下游任务的泛化能力。我们在自然语言理解、问答、摘要和自然语言生成任务上评估了我们的方法。实验证明，我们的方法非常有效，在性能上优于现有的方法。

    Quantization is an indispensable technique for serving Large Language Models (LLMs) and has recently found its way into LoRA fine-tuning. In this work we focus on the scenario where quantization and LoRA fine-tuning are applied together on a pre-trained model. In such cases it is common to observe a consistent gap in the performance on downstream tasks between full fine-tuning and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ (LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that simultaneously quantizes an LLM and finds a proper low-rank initialization for LoRA fine-tuning. Such an initialization alleviates the discrepancy between the quantized and full-precision model and significantly improves the generalization in downstream tasks. We evaluate our method on natural language understanding, question answering, summarization, and natural language generation tasks. Experiments show that our method is highly effective and outperforms exis
    
[^105]: SplitBeam: 通过分割计算在Wi-Fi网络中实现有效且高效的波束成形

    SplitBeam: Effective and Efficient Beamforming in Wi-Fi Networks Through Split Computing. (arXiv:2310.08656v1 [cs.NI])

    [http://arxiv.org/abs/2310.08656](http://arxiv.org/abs/2310.08656)

    SplitBeam是一个新的框架，通过分割计算，在Wi-Fi网络中实现了有效且高效的波束成形。

    

    现代IEEE 802.11（Wi-Fi）网络广泛依赖多输入多输出（MIMO）来显著提高吞吐量。为了正确进行MIMO传输的波束成形，接入点需要频繁地从每个连接的站点获取波束成形矩阵（BM）。然而，矩阵的大小随着天线和子载波的数量增加而增大，导致空间中的开销和站点的计算负载不断增加。传统方法要么计算负载过大，要么波束成形精度不高。因此，我们提出了SplitBeam，一个新的框架，在该框架中，我们训练一个分割的深度神经网络（DNN），直接根据输入的信道状态信息（CSI）矩阵输出BM。我们制定并解决了一个瓶颈优化问题（BOP），以保持计算、空间中的开销和误码率（BER）低于应用要求。我们使用现成的Wi-Fi设备进行了广泛的实验性CSI收集，在两个不同的环境中收集数据。

    Modern IEEE 802.11 (Wi-Fi) networks extensively rely on multiple-input multiple-output (MIMO) to significantly improve throughput. To correctly beamform MIMO transmissions, the access point needs to frequently acquire a beamforming matrix (BM) from each connected station. However, the size of the matrix grows with the number of antennas and subcarriers, resulting in an increasing amount of airtime overhead and computational load at the station. Conventional approaches come with either excessive computational load or loss of beamforming precision. For this reason, we propose SplitBeam, a new framework where we train a split deep neural network (DNN) to directly output the BM given the channel state information (CSI) matrix as input. We formulate and solve a bottleneck optimization problem (BOP) to keep computation, airtime overhead, and bit error rate (BER) below application requirements. We perform extensive experimental CSI collection with off-the-shelf Wi-Fi devices in two distinct e
    
[^106]: 在阿富汗武装冲突中通过文本数据进行死亡分类的分析：一种BERT方法。

    Analyzing Textual Data for Fatality Classification in Afghanistan's Armed Conflicts: A BERT Approach. (arXiv:2310.08653v1 [cs.LG])

    [http://arxiv.org/abs/2310.08653](http://arxiv.org/abs/2310.08653)

    这项研究利用BERT模型分析阿富汗武装冲突的文本数据，通过对事件的描述进行分类，判断其是否致命。模型在测试中表现出色。

    

    阿富汗在历史上经历了许多武装冲突，尤其是在过去的20年中；这些事件对人类生活产生了重大影响，包括军事人员和平民，可能导致死亡。在这项研究中，我们旨在利用最先进的机器学习技术，根据由武装冲突位置和事件数据项目（ACLED）数据集提供的文本描述，对阿富汗武装冲突的结果进行分类，判断其是否致命。该数据集包含从2021年8月至2023年3月在阿富汗发生的武装冲突的详细描述。提出的方法利用了BERT（双向编码器转换器的双向编码器表示），这是自然语言处理中的先进语言表示模型。分类器利用事件的原始文本描述来估计事件导致死亡的可能性。该模型在测试集上取得了令人印象深刻的性能。

    Afghanistan has witnessed many armed conflicts throughout history, especially in the past 20 years; these events have had a significant impact on human lives, including military and civilians, with potential fatalities. In this research, we aim to leverage state-of-the-art machine learning techniques to classify the outcomes of Afghanistan armed conflicts to either fatal or non-fatal based on their textual descriptions provided by the Armed Conflict Location & Event Data Project (ACLED) dataset. The dataset contains comprehensive descriptions of armed conflicts in Afghanistan that took place from August 2021 to March 2023. The proposed approach leverages the power of BERT (Bidirectional Encoder Representations from Transformers), a cutting-edge language representation model in natural language processing. The classifier utilizes the raw textual description of an event to estimate the likelihood of the event resulting in a fatality. The model achieved impressive performance on the test 
    
[^107]: 通过张量分解实现电网异常检测

    Electrical Grid Anomaly Detection via Tensor Decomposition. (arXiv:2310.08650v1 [cs.LG])

    [http://arxiv.org/abs/2310.08650](http://arxiv.org/abs/2310.08650)

    本论文提出了一种通过非负张量分解实现电网异常检测的方法。与之前的降维方法不同，该方法能够更准确地对SCADA系统进行建模，并检测出其中的异常行为。

    

    监控与数据采集系统（SCADA）经常作为电网分站的神经系统。这些系统实现了实时监控、数据采集、设备控制，并确保分站及其连接设备的平稳高效运行。之前的研究已经证明，基于降维的方法，如主成分分析（PCA），可以准确识别SCADA系统中的异常。虽然非负矩阵分解（NMF）并没有专门应用于SCADA，但它在无线传感器网络中检测异常方面表现出了良好的结果。这些无监督方法通过对正常或预期行为进行建模，识别偏离预期行为的事件，从而检测未知类型的攻击或异常。然而，这些方法没有对SCADA系统中自然存在的复杂多维交互进行建模。相比之下，非负张量分解方法能够对SCADA系统进行更准确的建模和异常检测。

    Supervisory Control and Data Acquisition (SCADA) systems often serve as the nervous system for substations within power grids. These systems facilitate real-time monitoring, data acquisition, control of equipment, and ensure smooth and efficient operation of the substation and its connected devices. Previous work has shown that dimensionality reduction-based approaches, such as Principal Component Analysis (PCA), can be used for accurate identification of anomalies in SCADA systems. While not specifically applied to SCADA, non-negative matrix factorization (NMF) has shown strong results at detecting anomalies in wireless sensor networks. These unsupervised approaches model the normal or expected behavior and detect the unseen types of attacks or anomalies by identifying the events that deviate from the expected behavior. These approaches; however, do not model the complex and multi-dimensional interactions that are naturally present in SCADA systems. Differently, non-negative tensor de
    
[^108]: 时间向量化数值积分用于ODE系统

    Time-vectorized numerical integration for systems of ODEs. (arXiv:2310.08649v1 [math.NA])

    [http://arxiv.org/abs/2310.08649](http://arxiv.org/abs/2310.08649)

    本文介绍了一种时间向量化的数值积分方法，用于积分刚性常微分方程组，并通过伴随方法计算参数梯度。该方法在独立时间序列和连续时间步骤的批次上进行向量化，提供了更高的计算带宽，能够充分利用现代GPU并实现超过100倍的加速。

    

    在科学问题中，刚性的常微分方程组和稀疏训练数据是很常见的。本文描述了用于积分刚性常微分方程组的高效隐式向量化方法，并通过伴随方法计算参数梯度。主要创新是在独立时间序列的数量和连续时间步骤的批次或“块”上向量化问题，从而有效地向量化隐式ODE系统的组装。后向Euler方法的线性化隐式系统的块双对角结构允许进一步使用并行循环约化（PCR）进行向量化。在输入数据的两个轴上进行向量化，为计算设备提供了更高的计算带宽，使得即使是相对稀疏的问题也能充分利用现代GPU，并实现了超过100倍的加速，与标准的顺序时间积分相比。

    Stiff systems of ordinary differential equations (ODEs) and sparse training data are common in scientific problems. This paper describes efficient, implicit, vectorized methods for integrating stiff systems of ordinary differential equations through time and calculating parameter gradients with the adjoint method. The main innovation is to vectorize the problem both over the number of independent times series and over a batch or "chunk" of sequential time steps, effectively vectorizing the assembly of the implicit system of ODEs. The block-bidiagonal structure of the linearized implicit system for the backward Euler method allows for further vectorization using parallel cyclic reduction (PCR). Vectorizing over both axes of the input data provides a higher bandwidth of calculations to the computing device, allowing even problems with comparatively sparse data to fully utilize modern GPUs and achieving speed ups of greater than 100x, compared to standard, sequential time integration. We 
    
[^109]: 使用迁移学习方法对3D打印圆柱物体的缺陷进行分析

    Defect Analysis of 3D Printed Cylinder Object Using Transfer Learning Approaches. (arXiv:2310.08645v1 [cs.CV])

    [http://arxiv.org/abs/2310.08645](http://arxiv.org/abs/2310.08645)

    本研究通过使用迁移学习模型分析了3D打印圆柱体中的缺陷，结果发现MobileNetV2在分类性能方面表现最好。

    

    添加制造（AM）在医疗保健、航空航天和汽车等各个行业引起了关注。然而，早期识别AM过程中的缺陷可以降低生产成本并提高生产效率-这是一个关键挑战。本研究探讨了机器学习（ML）方法，特别是迁移学习（TL）模型在3D打印圆柱体缺陷检测中的有效性。使用包括VGG16、VGG19、ResNet50、ResNet101、InceptionResNetV2和MobileNetV2等模型对圆柱体的图像进行分析。通过准确率、精确率、召回率和F1-score指标比较了两个数据集的性能。在第一项研究中，VGG16、InceptionResNetV2和MobileNetV2达到了完美的得分。相反，ResNet50的性能最差，平均F1-score为0.32。同样地，在第二项研究中，MobileNetV2正确分类了所有实例，而ResNet50则出现了更多的假阳性和较少的真阳性，导致F1-score为0.75。

    Additive manufacturing (AM) is gaining attention across various industries like healthcare, aerospace, and automotive. However, identifying defects early in the AM process can reduce production costs and improve productivity - a key challenge. This study explored the effectiveness of machine learning (ML) approaches, specifically transfer learning (TL) models, for defect detection in 3D-printed cylinders. Images of cylinders were analyzed using models including VGG16, VGG19, ResNet50, ResNet101, InceptionResNetV2, and MobileNetV2. Performance was compared across two datasets using accuracy, precision, recall, and F1-score metrics. In the first study, VGG16, InceptionResNetV2, and MobileNetV2 achieved perfect scores. In contrast, ResNet50 had the lowest performance, with an average F1-score of 0.32. Similarly, in the second study, MobileNetV2 correctly classified all instances, while ResNet50 struggled with more false positives and fewer true positives, resulting in an F1-score of 0.75.
    
[^110]: 机器学习模型地球科学系统建模中的质量保持感知器

    A Mass-Conserving-Perceptron for Machine Learning-Based Modeling of Geoscientific Systems. (arXiv:2310.08644v1 [cs.LG])

    [http://arxiv.org/abs/2310.08644](http://arxiv.org/abs/2310.08644)

    这篇论文提出了一种质量保持感知器（MCP）用于将物理-概念模型和机器学习模型结合起来建模地球科学系统，通过利用机器学习技术从数据中学习物理过程的功能性和质量保持性。

    

    虽然数十年来致力于构建用于预测地球科学系统时间序列演化的物理-概念 (PC) 模型，但最近的研究表明，基于机器学习 (ML) 的门控循环神经网络技术可以用于开发更准确的模型。然而，从ML基础模型中提取物理理解的困难使得其在增强对系统结构和功能的科学知识方面的应用变得复杂。在这里，我们提出了一个理解物理性的质量保持感知器 (MCP) 作为弥合PC模型和ML模型的方法。MCP利用PC模型和GRNNs背后的有向图结构的内在同构性，以可解释的方式明确表示物理过程的质量保持性质，同时利用现有数据和现成的ML技术直接学习这种过程的功能性（可解释性）。

    Although decades of effort have been devoted to building Physical-Conceptual (PC) models for predicting the time-series evolution of geoscientific systems, recent work shows that Machine Learning (ML) based Gated Recurrent Neural Network technology can be used to develop models that are much more accurate. However, the difficulty of extracting physical understanding from ML-based models complicates their utility for enhancing scientific knowledge regarding system structure and function. Here, we propose a physically-interpretable Mass Conserving Perceptron (MCP) as a way to bridge the gap between PC-based and ML-based modeling approaches. The MCP exploits the inherent isomorphism between the directed graph structures underlying both PC models and GRNNs to explicitly represent the mass-conserving nature of physical processes while enabling the functional nature of such processes to be directly learned (in an interpretable manner) from available data using off-the-shelf ML technology. As
    
[^111]: 用机器学习预测离婚：洞察和LIME可解释性

    Divorce Prediction with Machine Learning: Insights and LIME Interpretability. (arXiv:2310.08620v1 [cs.LG])

    [http://arxiv.org/abs/2310.08620](http://arxiv.org/abs/2310.08620)

    通过评估名为“离婚预测数据集”的数据集，使用六种不同的机器学习算法，成功准确分类婚姻和离婚人群，具有98.5%的准确率。

    

    离婚是像美国这样的发达国家中最常见的社会问题之一。最近的婚姻中有近50％转变为非自愿的离婚或分居。尽管人们在很大程度上会发生变化，甚至随着时间的推移，但像离婚这样的事件并没有打断个人的日常活动；然而，离婚对个人的心理健康和个人生活产生了严重影响。在这项研究的范围内，通过评估名为“离婚预测数据集”的数据集，使用六种不同的机器学习算法（逻辑回归（LR）、线性判别分析（LDA）、K-最近邻（KNN）、分类回归树（CART）、高斯朴素贝叶斯（NB）和支持向量机（SVM））准确分类婚姻和离婚人群。初步的计算结果表明，SVM、KNN和LDA等算法可以以98.5的准确率执行这个任务。

    Divorce is one of the most common social issues in developed countries like in the United States. Almost 50% of the recent marriages turn into an involuntary divorce or separation. While it is evident that people vary to a different extent, and even over time, an incident like Divorce does not interrupt the individual's daily activities; still, Divorce has a severe effect on the individual's mental health, and personal life. Within the scope of this research, the divorce prediction was carried out by evaluating a dataset named by the 'divorce predictor dataset' to correctly classify between married and Divorce people using six different machine learning algorithms- Logistic Regression (LR), Linear Discriminant Analysis (LDA), K-Nearest Neighbors (KNN), Classification and Regression Trees (CART), Gaussian Na\"ive Bayes (NB), and, Support Vector Machines (SVM). Preliminary computational results show that algorithms such as SVM, KNN, and LDA, can perform that task with an accuracy of 98.5
    
[^112]: 安全深度策略适应

    Safe Deep Policy Adaptation. (arXiv:2310.08602v1 [cs.RO])

    [http://arxiv.org/abs/2310.08602](http://arxiv.org/abs/2310.08602)

    该论文提出了SafeDPA，一种新颖的强化学习和控制框架，用于同时解决策略适应和安全强化学习的问题。SafeDPA在仿真环境中联合学习自适应策略和动力学模型，并使用少量真实数据进行微调。在真实世界部署过程中，通过引入基于控制屏障函数的安全过滤器，确保了SafeDPA的安全性。

    

    自主和人工智能的一个重要目标是使自主机器人能够在动态和不确定的环境中快速适应。经典的自适应控制和安全控制提供了稳定性和安全性保证，但仅限于特定的系统类别。相比之下，基于强化学习（RL）的策略适应提供了通用性和泛化性，但同时也带来了安全性和稳健性的挑战。我们提出了SafeDPA，一种新颖的RL和控制框架，同时解决了策略适应和安全强化学习的问题。SafeDPA在仿真环境中联合学习自适应策略和动力学模型，预测环境配置，并使用少量真实数据对动力学模型进行微调。在RL策略之上引入基于控制屏障函数（CBF）的安全过滤器，以确保在真实世界部署过程中的安全性。我们提供了SafeDPA的理论安全性保证，并展示了SafeDPA对学习误差的稳健性。

    A critical goal of autonomy and artificial intelligence is enabling autonomous robots to rapidly adapt in dynamic and uncertain environments. Classic adaptive control and safe control provide stability and safety guarantees but are limited to specific system classes. In contrast, policy adaptation based on reinforcement learning (RL) offers versatility and generalizability but presents safety and robustness challenges. We propose SafeDPA, a novel RL and control framework that simultaneously tackles the problems of policy adaptation and safe reinforcement learning. SafeDPA jointly learns adaptive policy and dynamics models in simulation, predicts environment configurations, and fine-tunes dynamics models with few-shot real-world data. A safety filter based on the Control Barrier Function (CBF) on top of the RL policy is introduced to ensure safety during real-world deployment. We provide theoretical safety guarantees of SafeDPA and show the robustness of SafeDPA against learning errors 
    
[^113]: 带有性能保证的机组启停预测器：支持向量机分类器

    Unit Commitment Predictor With a Performance Guarantee: A Support Vector Machine Classifier. (arXiv:2310.08601v1 [math.OC])

    [http://arxiv.org/abs/2310.08601](http://arxiv.org/abs/2310.08601)

    本文提出了一个带有性能保证的机组启停预测器，通过学习和预测常规机组的启停决策，系统运营商可以在求解器中使用预热启动并显著加速计算。对于预测，使用了适当正则化的核化支持向量机分类器，能将计算时间减少1.7倍。

    

    系统运营商通常需要在有限的时间内解决大规模的机组启停问题进行计算。本文提供了一种实用的解决方案，展示了通过学习和预测常规机组的启停决策，系统运营商有可能在求解器中使用预热启动并显著加速计算。对于预测，我们训练了线性和核化支持向量机分类器，提供了一种样本外性能保证（如果适当正则化），转化为分布鲁棒分类器。对于机组启停问题，我们求解了一个混合整数二阶锥问题。基于IEEE 6节点和118节点测试系统的结果表明，适当正则化的核化支持向量机优于其他分类器，将计算时间减少了1.7倍。此外，如果存在严格的计算限制，没有预热启动的机组启停问题与最优解的距离较远。

    The system operators usually need to solve large-scale unit commitment problems within limited time frame for computation. This paper provides a pragmatic solution, showing how by learning and predicting the on/off commitment decisions of conventional units, there is a potential for system operators to warm start their solver and speed up their computation significantly. For the prediction, we train linear and kernelized support vector machine classifiers, providing an out-of-sample performance guarantee if properly regularized, converting to distributionally robust classifiers. For the unit commitment problem, we solve a mixed-integer second-order cone problem. Our results based on the IEEE 6-bus and 118-bus test systems show that the kernelized SVM with proper regularization outperforms other classifiers, reducing the computational time by a factor of 1.7. In addition, if there is a tight computational limit, while the unit commitment problem without warm start is far away from the o
    
[^114]: 自主驾驶车辆交叉路口导航的深度强化学习

    Deep Reinforcement Learning for Autonomous Vehicle Intersection Navigation. (arXiv:2310.08595v1 [cs.RO])

    [http://arxiv.org/abs/2310.08595](http://arxiv.org/abs/2310.08595)

    本研究通过使用基于TD3算法的单智能体方法，在CARLA模拟平台中展示了在复杂T型路口导航中稳定收敛和改进安全性能，并在行程延误、碰撞减少和总体成本等方面优于先前的方法。

    

    本文探讨了在密集交通场景中，自主驾驶车辆（AVs）在复杂T型路口导航中面临的挑战。强化学习算法已经成为一种有希望的方法，可以通过实时地使AVs做出安全高效的决策来应对这些挑战。在本文中，我们使用一种基于双延迟深度确定性策略梯度（TD3）强化学习算法的低成本、单智能体方法来解决在T型路口上的高效安全导航问题。我们展示了当我们在CARLA模拟平台上对我们的TD3方法进行训练和测试时，该方法呈现出稳定的收敛性和改进的安全性能，适用于各种交通密度。我们的结果表明，所提出的方法使AV能够有效地导航T型路口，在行程延误、碰撞减少和总体成本方面优于先前的方法。本研究对强化学习在自主驾驶车辆领域的应用贡献了新的知识。

    In this paper, we explore the challenges associated with navigating complex T-intersections in dense traffic scenarios for autonomous vehicles (AVs). Reinforcement learning algorithms have emerged as a promising approach to address these challenges by enabling AVs to make safe and efficient decisions in real-time. Here, we address the problem of efficiently and safely navigating T-intersections using a lower-cost, single-agent approach based on the Twin Delayed Deep Deterministic Policy Gradient (TD3) reinforcement learning algorithm. We show that our TD3-based method, when trained and tested in the CARLA simulation platform, demonstrates stable convergence and improved safety performance in various traffic densities. Our results reveal that the proposed approach enables the AV to effectively navigate T-intersections, outperforming previous methods in terms of travel delays, collision minimization, and overall cost. This study contributes to the growing body of knowledge on reinforceme
    
[^115]: 我们能编辑多模式大型语言模型吗？

    Can We Edit Multimodal Large Language Models?. (arXiv:2310.08475v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.08475](http://arxiv.org/abs/2310.08475)

    本文提出了编辑多模式大型语言模型（MLLMs）的挑战，并构建了一个新的基准用于评估和比较不同编辑方法的效果。实验结果表明，编辑多模式LLMs仍然存在困难，但这项工作为NLP社区提供了宝贵的见解。

    

    本文关注编辑多模式大型语言模型（MLLMs）。与编辑单模式LLMs相比，多模式模型的编辑更具挑战性，需要更高级别的审查和慎重考虑。为了促进这一领域的研究，我们构建了一个新的基准，称为MMEdit，用于编辑多模式LLMs，并建立了一套创新的度量标准进行评估。我们进行了包括各种模型编辑基线的综合实验，并分析了编辑多模式LLMs的不同组件的影响。根据经验，我们发现之前的基线在某种程度上可以实现编辑多模式LLMs，但效果仍然不理想，表明这个任务可能存在的困难。我们希望我们的工作能为NLP社区提供见解。代码和数据集可在https://github.com/zjunlp/EasyEdit获取。

    In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.
    
[^116]: 迈向针对漏洞检测的因果深度学习

    Towards Causal Deep Learning for Vulnerability Detection. (arXiv:2310.07958v1 [cs.SE])

    [http://arxiv.org/abs/2310.07958](http://arxiv.org/abs/2310.07958)

    本文提出了一种针对漏洞检测的因果深度学习方法CausalVul，通过引入因果性，并设计新的扰动，解决了深度学习漏洞检测中模型不稳定和泛化性能差的问题。

    

    近年来，深度学习的漏洞检测取得了有希望的成果。然而，一个阻碍其在实践中非常有用的重要挑战是模型在扰动下不稳定，并且不能很好地泛化到超出分布（OOD）的数据，例如，在真实世界中将训练好的模型应用到未见过的项目上。我们假设这是因为模型学习到了非稳定的特征，例如变量名，与标签具有虚假相关性。当扰动和OOD数据集不再具有相同的虚假特征时，模型预测失败。为了解决这个挑战，在本文中，我们将因果性引入了深度学习漏洞检测中。我们的方法CausalVul分为两个阶段。首先，我们设计了新的扰动来发现模型可能用于进行预测的虚假特征。其次，我们在现有的深度学习模型之上应用了因果学习算法，特别是do-计算，来解决这个问题。

    Deep learning vulnerability detection has shown promising results in recent years. However, an important challenge that still blocks it from being very useful in practice is that the model is not robust under perturbation and it cannot generalize well over the out-of-distribution (OOD) data, e.g., applying a trained model to unseen projects in real world. We hypothesize that this is because the model learned non-robust features, e.g., variable names, that have spurious correlations with labels. When the perturbed and OOD datasets no longer have the same spurious features, the model prediction fails. To address the challenge, in this paper, we introduced causality into deep learning vulnerability detection. Our approach CausalVul consists of two phases. First, we designed novel perturbations to discover spurious features that the model may use to make predictions. Second, we applied the causal learning algorithms, specifically, do-calculus, on top of existing deep learning models to sys
    
[^117]: 具有相位随机桥的生成建模

    Generative Modeling with Phase Stochastic Bridges. (arXiv:2310.07805v1 [cs.LG])

    [http://arxiv.org/abs/2310.07805](http://arxiv.org/abs/2310.07805)

    通过在相位空间中构建路径测度，我们提出了一种新颖的生成建模框架，可以在动力传播的早期阶段生成逼真的数据点，并利用额外的速度信息实现高效的数据生成。

    

    扩散模型（DMs）是用于连续输入的最先进的生成模型。DMs通过在输入空间（即位置空间）中构建随机微分方程（SDE），并使用神经网络进行反演来工作。在这项工作中，我们介绍了一种基于相位空间动力学的新型生成建模框架，其中相位空间被定义为一个包括位置和速度的增强空间。利用随机最优控制的洞察力，我们构建了相位空间中的路径测度，实现了高效的采样。与DMs相比，我们的框架在动力传播的早期阶段就能够生成逼真的数据点。这种早期预测为通过沿轨迹利用额外的速度信息实现高效的数据生成奠定了基础。在标准图像生成基准测试中，我们的模型在小函数评估数量的范围内表现出优秀的性能。

    Diffusion models (DMs) represent state-of-the-art generative models for continuous inputs. DMs work by constructing a Stochastic Differential Equation (SDE) in the input space (ie, position space), and using a neural network to reverse it. In this work, we introduce a novel generative modeling framework grounded in \textbf{phase space dynamics}, where a phase space is defined as {an augmented space encompassing both position and velocity.} Leveraging insights from Stochastic Optimal Control, we construct a path measure in the phase space that enables efficient sampling. {In contrast to DMs, our framework demonstrates the capability to generate realistic data points at an early stage of dynamics propagation.} This early prediction sets the stage for efficient data generation by leveraging additional velocity information along the trajectory. On standard image generation benchmarks, our model yields favorable performance over baselines in the regime of small Number of Function Evaluation
    
[^118]: 简单变压器中的线性潜在世界模型: 奥赛罗-GPT案例研究

    Linear Latent World Models in Simple Transformers: A Case Study on Othello-GPT. (arXiv:2310.07582v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.07582](http://arxiv.org/abs/2310.07582)

    本研究通过案例研究奥赛罗-GPT，发现其具有线性表示对立棋子的世界模型，并揭示了线性世界表示和因果决策之间的相互作用及其与层深度和模型复杂度的依赖关系。

    

    基础模型在决策和逻辑推理方面表现出显著的能力。然而，关于它们对世界的真正理解与纯粹的随机模仿相对的讨论持续存在。本文详细研究了在奥赛罗中训练的简单变压器，扩展了先前的研究，以增强对奥赛罗-GPT新兴世界模型的理解。研究发现，奥赛罗-GPT包含了对立棋子的线性表示，这一因素在驱动其决策过程中起到因果性的作用。本文进一步阐明了线性世界表示与因果决策之间的相互作用，以及它们对层深度和模型复杂度的依赖关系。我们已经将代码公开。

    Foundation models exhibit significant capabilities in decision-making and logical deductions. Nonetheless, a continuing discourse persists regarding their genuine understanding of the world as opposed to mere stochastic mimicry. This paper meticulously examines a simple transformer trained for Othello, extending prior research to enhance comprehension of the emergent world model of Othello-GPT. The investigation reveals that Othello-GPT encapsulates a linear representation of opposing pieces, a factor that causally steers its decision-making process. This paper further elucidates the interplay between the linear world representation and causal decision-making, and their dependence on layer depth and model complexity. We have made the code public.
    
[^119]: 关于交叉领域数据对德语语言模型的影响

    On the Impact of Cross-Domain Data on German Language Models. (arXiv:2310.07321v1 [cs.CL])

    [http://arxiv.org/abs/2310.07321](http://arxiv.org/abs/2310.07321)

    本研究通过对德语语言模型进行实验，发现将数据多样性置于数据质量之上的交叉领域数据集训练方法，可以显著提高模型的性能，并超过了之前的最先进模型。

    

    传统上，大型语言模型要么在通用网络抓取数据上训练，要么在特定领域的数据上。然而，生成型大型语言模型的最近成功突显了交叉领域数据集的好处。为了考察数据多样性高于质量的重要性，我们提出了一个包含五个领域文本的德语数据集，以及一个旨在包含高质量数据的数据集。通过在这两个数据集上训练参数范围从122M到750M的一系列模型，我们对多个下游任务进行了全面评估。我们的研究结果表明，使用交叉领域数据集训练的模型优于仅使用质量数据训练的模型，在先前最先进结果上提出了高达4.45%的改进。这些模型可在https://huggingface.co/ikim-uk-essen上找到。

    Traditionally, large language models have been either trained on general web crawls or domain-specific data. However, recent successes of generative large language models, have shed light on the benefits of cross-domain datasets. To examine the significance of prioritizing data diversity over quality, we present a German dataset comprising texts from five domains, along with another dataset aimed at containing high-quality data. Through training a series of models ranging between 122M and 750M parameters on both datasets, we conduct a comprehensive benchmark on multiple downstream tasks. Our findings demonstrate that the models trained on the cross-domain dataset outperform those trained on quality data alone, leading to improvements up to $4.45\%$ over the previous state-of-the-art. The models are available at https://huggingface.co/ikim-uk-essen
    
[^120]: 视觉计算扩散模型的最新进展

    State of the Art on Diffusion Models for Visual Computing. (arXiv:2310.07204v1 [cs.AI])

    [http://arxiv.org/abs/2310.07204](http://arxiv.org/abs/2310.07204)

    这篇论文旨在介绍最新的视觉计算领域中扩散模型的发展和应用，涵盖了生成人工智能的核心概念和实现细节，并总结了个人化、条件约束和反演等重要方面。

    

    随着生成人工智能的出现，视觉计算领域正在迅速发展，它为图像、视频和3D场景的生成、编辑和重建提供了前所未有的能力。在这些领域中，扩散模型是生成人工智能的首选架构。仅在过去一年中，基于扩散的工具和应用的文献数量呈指数增长，并且涉及计算机图形学、计算机视觉和人工智能社区的相关论文每天都在arXiv上发表。这个领域的快速增长使得跟上所有最新发展变得困难。这份最新技术报告（STAR）的目标是介绍扩散模型的基本数学概念、稳定扩散模型的实现细节和设计选择，以及概述这些生成人工智能工具的重要方面，包括个性化、条件约束、反演等。

    The field of visual computing is rapidly advancing due to the emergence of generative artificial intelligence (AI), which unlocks unprecedented capabilities for the generation, editing, and reconstruction of images, videos, and 3D scenes. In these domains, diffusion models are the generative AI architecture of choice. Within the last year alone, the literature on diffusion-based tools and applications has seen exponential growth and relevant papers are published across the computer graphics, computer vision, and AI communities with new works appearing daily on arXiv. This rapid growth of the field makes it difficult to keep up with all recent developments. The goal of this state-of-the-art report (STAR) is to introduce the basic mathematical concepts of diffusion models, implementation details and design choices of the popular Stable Diffusion model, as well as overview important aspects of these generative AI tools, including personalization, conditioning, inversion, among others. Mor
    
[^121]: 通过信息论分布多样化实现联邦泛化能力

    Federated Generalization via Information-Theoretic Distribution Diversification. (arXiv:2310.07171v1 [cs.LG])

    [http://arxiv.org/abs/2310.07171](http://arxiv.org/abs/2310.07171)

    该论文研究了联邦学习中泛化能力的挑战，特别关注训练分布和测试分布的不匹配。提出了一种信息论的泛化方法来解决这个问题。

    

    联邦学习（FL）因其在无需直接数据共享的情况下进行协同模型训练的能力而日益突出。然而，客户端之间本地数据分布的巨大差异，通常被称为非独立同分布（non-IID）挑战，对FL的泛化能力构成了重大障碍。当并非所有客户端都参与训练过程时，情况变得更加复杂，这是由于不稳定的网络连接或有限的计算能力而常见。这可能极大地复杂化了对训练模型的泛化能力的评估。尽管最近的大量研究集中在涉及具有不同分布的参与客户端的未见数据的泛化差距问题上，但参与客户端的训练分布和非参与客户端的测试分布之间的差异却被大部分忽视了。为此，我们的论文揭示了一种基于信息论的泛化方法。

    Federated Learning (FL) has surged in prominence due to its capability of collaborative model training without direct data sharing. However, the vast disparity in local data distributions among clients, often termed the non-Independent Identically Distributed (non-IID) challenge, poses a significant hurdle to FL's generalization efficacy. The scenario becomes even more complex when not all clients participate in the training process, a common occurrence due to unstable network connections or limited computational capacities. This can greatly complicate the assessment of the trained models' generalization abilities. While a plethora of recent studies has centered on the generalization gap pertaining to unseen data from participating clients with diverse distributions, the divergence between the training distributions of participating clients and the testing distributions of non-participating ones has been largely overlooked. In response, our paper unveils an information-theoretic genera
    
[^122]: 基于提示增强的时态点过程用于流式事件序列

    Prompt-augmented Temporal Point Process for Streaming Event Sequence. (arXiv:2310.04993v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.04993](http://arxiv.org/abs/2310.04993)

    提出了一种基于提示增强的时态点过程（PromptTPP）方法，用于解决流式事件序列学习的挑战。

    

    神经时态点过程（TPP）是建模连续时间事件序列（如网络用户活动和金融交易）的主要范例。在现实世界的应用中，事件数据通常以流式方式接收，模式的分布可能随时间变化。此外，隐私和内存限制在实际场景中也很常见，进一步增加了挑战。因此，连续监测TPP以学习流式事件序列是一个重要但未充分研究的问题。我们的工作通过采用持续学习（CL）来解决这个挑战，使模型能够在现实约束下连续学习一系列任务而不会发生灾难性遗忘。相应地，我们提出了一个简单而有效的框架PromptTPP，通过将基本TPP与一个提示机制进行整合，来解决这个问题。

    Neural Temporal Point Processes (TPPs) are the prevalent paradigm for modeling continuous-time event sequences, such as user activities on the web and financial transactions. In real-world applications, event data is typically received in a \emph{streaming} manner, where the distribution of patterns may shift over time. Additionally, \emph{privacy and memory constraints} are commonly observed in practical scenarios, further compounding the challenges. Therefore, the continuous monitoring of a TPP to learn the streaming event sequence is an important yet under-explored problem. Our work paper addresses this challenge by adopting Continual Learning (CL), which makes the model capable of continuously learning a sequence of tasks without catastrophic forgetting under realistic constraints. Correspondingly, we propose a simple yet effective framework, PromptTPP\footnote{Our code is available at {\small \url{ https://github.com/yanyanSann/PromptTPP}}}, by integrating the base TPP with a cont
    
[^123]: 使用大型语言模型（LLMS）对图中的节点进行无标签分类

    Label-free Node Classification on Graphs with Large Language Models (LLMS). (arXiv:2310.04668v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.04668](http://arxiv.org/abs/2310.04668)

    本文介绍了一种使用大型语言模型（LLMs）对图中节点进行无标签分类的方法，即LLM-GNN。它利用LLMs对一小部分节点进行注释，然后通过对LLMs的注释进行训练，使得GNN能够对其余大部分节点进行预测。这种方法充分发挥了GNNs和LLMs的优势，同时解决了它们在处理结构化数据方面的限制。

    

    近年来，图神经网络（Graph Neural Networks，GNNs）在节点分类方面取得了显著的进展。然而，为了确保良好的性能，它们需要大量高质量的标签。相比之下，大型语言模型（Large Language Models，LLMs）在文本属性图上展现出了令人印象深刻的零样学习能力。然而，它们在高效处理结构化数据方面面临挑战，并且推理成本较高。鉴于这些观察结果，本文引入了一种基于LLMs的无标签图节点分类方法，命名为LLM-GNN。它集成了GNNs和LLMs的优势，同时减轻了它们的限制。具体而言，LLMs被用来注释一小部分节点，然后通过对LLMs的注释进行训练，使GNNs能够预测其余大部分节点。LLM-GNN的实现面临一个独特的挑战：我们如何主动选择要由LLMs注释的节点，从而增强GNN的训练？我们如何利用LLMs来优化结构化数据的处理？

    In recent years, there have been remarkable advancements in node classification achieved by Graph Neural Networks (GNNs). However, they necessitate abundant high-quality labels to ensure promising performance. In contrast, Large Language Models (LLMs) exhibit impressive zero-shot proficiency on text-attributed graphs. Yet, they face challenges in efficiently processing structural data and suffer from high inference costs. In light of these observations, this work introduces a label-free node classification on graphs with LLMs pipeline, LLM-GNN. It amalgamates the strengths of both GNNs and LLMs while mitigating their limitations. Specifically, LLMs are leveraged to annotate a small portion of nodes and then GNNs are trained on LLMs' annotations to make predictions for the remaining large portion of nodes. The implementation of LLM-GNN faces a unique challenge: how can we actively select nodes for LLMs to annotate and consequently enhance the GNN training? How can we leverage LLMs to ob
    
[^124]: 通过参数高效适应，实现对缺失模态的鲁棒多模态学习

    Robust Multimodal Learning with Missing Modalities via Parameter-Efficient Adaptation. (arXiv:2310.03986v1 [cs.CV])

    [http://arxiv.org/abs/2310.03986](http://arxiv.org/abs/2310.03986)

    通过低秩适应和中间特征的调制，我们提出了针对预训练多模态网络的参数高效适应程序，以实现对缺失模态的鲁棒性，并在某些情况下胜过独立的专门网络。

    

    多模态学习旨在利用多个数据源来提高下游任务的整体性能。在一些相关的模态中观察到，如果在测试时间缺少一个或多个模态，现有的多模态网络的性能会显著下降。为了实现对缺失模态的鲁棒性，我们提出了预训练的多模态网络的简单和参数高效的适应程序。特别地，我们利用低秩适应和中间特征的调制来补偿缺失的模态。我们证明，这种适应可以部分弥补由于缺失模态而导致的性能下降，并在某些情况下胜过针对可用模态组合进行训练的独立的、专门的网络。所提出的适应所需的参数非常少（例如，少于）

    Multimodal learning seeks to utilize data from multiple sources to improve the overall performance of downstream tasks. It is desirable for redundancies in the data to make multimodal systems robust to missing or corrupted observations in some correlated modalities. However, we observe that the performance of several existing multimodal networks significantly deteriorates if one or multiple modalities are absent at test time. To enable robustness to missing modalities, we propose simple and parameter-efficient adaptation procedures for pretrained multimodal networks. In particular, we exploit low-rank adaptation and modulation of intermediate features to compensate for the missing modalities. We demonstrate that such adaptation can partially bridge performance drop due to missing modalities and outperform independent, dedicated networks trained for the available modality combinations in some cases. The proposed adaptation requires extremely small number of parameters (e.g., fewer than 
    
[^125]: SmoothLLM：防御大型语言模型免受越狱攻击

    SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks. (arXiv:2310.03684v1 [cs.LG])

    [http://arxiv.org/abs/2310.03684](http://arxiv.org/abs/2310.03684)

    SmoothLLM是第一个用于减轻大型语言模型上越狱攻击的算法，通过在输入提示上随机扰动并汇总预测结果来检测对抗性输入，将攻击成功率降低至不到一个百分点，并提供了可证明的保证。

    

    尽管努力将大型语言模型（LLM）与人类价值观保持一致，但广泛使用的LLM（如GPT、Llama、Claude和PaLM）仍然容易受到越狱攻击，即对目标LLM进行欺骗，以生成不合适的内容。为了解决这个漏洞，我们提出了SmoothLLM，这是第一个旨在减轻LLM上的越狱攻击的算法。基于我们的发现，对抗性生成的提示对字符级别的改变很脆弱，我们的防御首先随机扰动给定输入提示的多个副本，然后汇总相应的预测结果来检测对抗性输入。SmoothLLM将众多热门LLM的攻击成功率降低至不到一个百分点，避免了不必要的保守性，并对攻击缓解提供了可证明的保证。此外，我们的防御使用的查询数量比现有的攻击方法少得多，并且与任何LLM兼容。

    Despite efforts to align large language models (LLMs) with human values, widely-used LLMs such as GPT, Llama, Claude, and PaLM are susceptible to jailbreaking attacks, wherein an adversary fools a targeted LLM into generating objectionable content. To address this vulnerability, we propose SmoothLLM, the first algorithm designed to mitigate jailbreaking attacks on LLMs. Based on our finding that adversarially-generated prompts are brittle to character-level changes, our defense first randomly perturbs multiple copies of a given input prompt, and then aggregates the corresponding predictions to detect adversarial inputs. SmoothLLM reduces the attack success rate on numerous popular LLMs to below one percentage point, avoids unnecessary conservatism, and admits provable guarantees on attack mitigation. Moreover, our defense uses exponentially fewer queries than existing attacks and is compatible with any LLM.
    
[^126]: BioBridge: 通过知识图谱桥接生物医学基础模型

    BioBridge: Bridging Biomedical Foundation Models via Knowledge Graph. (arXiv:2310.03320v1 [cs.LG])

    [http://arxiv.org/abs/2310.03320](http://arxiv.org/abs/2310.03320)

    BioBridge是一种通过知识图谱桥接单模态生物医学基础模型的参数高效学习框架。实验证明，BioBridge在跨模态检索任务中胜过最佳基线KG嵌入方法，具有泛化能力。

    

    基础模型(FMs)能够利用大量的无标签数据，在各种任务上展现出优秀的性能。然而，用于生物医学领域的FMs主要仍处于单模态状态，即独立训练并用于处理蛋白质序列、小分子结构或临床数据等单一任务。为了克服生物医学FMs的这种局限性，我们提出了一种新颖的参数高效学习框架BioBridge，通过利用知识图谱(KG)来学习不需要微调任何底层单模态FMs的转换，从而桥接独立训练的单模态FMs以建立多模态行为。我们的实证结果表明，BioBridge在跨模态检索任务中可以击败最佳基线KG嵌入方法（平均提高约76.3%）。我们还发现，BioBridge表现出领域外的泛化能力，可以推广到未见的模态或关系中。

    Foundation models (FMs) are able to leverage large volumes of unlabeled data to demonstrate superior performance across a wide range of tasks. However, FMs developed for biomedical domains have largely remained unimodal, i.e., independently trained and used for tasks on protein sequences alone, small molecule structures alone, or clinical data alone. To overcome this limitation of biomedical FMs, we present BioBridge, a novel parameter-efficient learning framework, to bridge independently trained unimodal FMs to establish multimodal behavior. BioBridge achieves it by utilizing Knowledge Graphs (KG) to learn transformations between one unimodal FM and another without fine-tuning any underlying unimodal FMs. Our empirical results demonstrate that BioBridge can beat the best baseline KG embedding methods (on average by around 76.3%) in cross-modal retrieval tasks. We also identify BioBridge demonstrates out-of-domain generalization ability by extrapolating to unseen modalities or relation
    
[^127]: LanguageMPC：基于大型语言模型的自动驾驶决策者

    LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving. (arXiv:2310.03026v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2310.03026](http://arxiv.org/abs/2310.03026)

    本文研究将大型语言模型（LLMs）作为复杂自动驾驶场景的决策组件，通过认知路径和算法来实现全面推理和可执行驾驶指令的转化。实验证明，LLMs能够在单车任务和复杂驾驶行为中表现出优越性能，这是因为其具有常识推理能力。

    

    现有基于学习的自动驾驶系统在理解高级信息、推广罕见事件和提供可解释性方面面临挑战。为解决这些问题，本研究将大型语言模型（LLMs）作为复杂自动驾驶场景的决策组件，需要人类常识理解。我们设计了认知路径，使LLMs能够进行全面推理，并开发了将LLM决策转化为可执行驾驶指令的算法。通过这种方式，LLM决策通过引导参数矩阵适应与低级控制器无缝集成。大量实验表明，我们提出的方法不仅在单车任务中始终超越基线方法，而且还能处理复杂的驾驶行为，甚至多车协调，这要归功于LLMs的常识推理能力。本文介绍了将LLMs作为有效决策者的初步步骤。

    Existing learning-based autonomous driving (AD) systems face challenges in comprehending high-level information, generalizing to rare events, and providing interpretability. To address these problems, this work employs Large Language Models (LLMs) as a decision-making component for complex AD scenarios that require human commonsense understanding. We devise cognitive pathways to enable comprehensive reasoning with LLMs, and develop algorithms for translating LLM decisions into actionable driving commands. Through this approach, LLM decisions are seamlessly integrated with low-level controllers by guided parameter matrix adaptation. Extensive experiments demonstrate that our proposed method not only consistently surpasses baseline approaches in single-vehicle tasks, but also helps handle complex driving behaviors even multi-vehicle coordination, thanks to the commonsense reasoning capabilities of LLMs. This paper presents an initial step toward leveraging LLMs as effective decision-make
    
[^128]: H-InDex：基于手部信息的视觉强化学习在巧妙操纵中的应用

    H-InDex: Visual Reinforcement Learning with Hand-Informed Representations for Dexterous Manipulation. (arXiv:2310.01404v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.01404](http://arxiv.org/abs/2310.01404)

    这项工作提出了一种基于人体手部信息的视觉表示学习框架H-InDex，通过强化学习解决困难的巧妙操纵任务。实证研究表明，H-InDex明显优于强基线方法和最近的视觉基础模型。

    

    人的手具有卓越的灵巧性，长期以来一直是机器人操纵的灵感来源。在这项工作中，我们提出了一种基于人体$\textbf{H}$and$\textbf{-In}$formed视觉表示学习框架，通过强化学习解决困难的$\textbf{Dex}$terous操纵任务（$\textbf{H-InDex}$）。我们的框架包括三个阶段：（i）使用3D人手姿势估计进行预训练表示，（ii）使用自监督关键点检测进行离线自适应表示，和（iii）使用指数加权移动平均BatchNorm进行强化学习。后两个阶段仅修改预训练表示的总参数的 $0.36\%$，确保保留了来自预训练的知识。我们在12个具有挑战性的巧妙操纵任务上进行了实证研究，发现H-InDex明显优于强基线方法和最近的用于运动控制的视觉基础模型。 代码位于https://yanjieze.com/H

    Human hands possess remarkable dexterity and have long served as a source of inspiration for robotic manipulation. In this work, we propose a human $\textbf{H}$and$\textbf{-In}$formed visual representation learning framework to solve difficult $\textbf{Dex}$terous manipulation tasks ($\textbf{H-InDex}$) with reinforcement learning. Our framework consists of three stages: (i) pre-training representations with 3D human hand pose estimation, (ii) offline adapting representations with self-supervised keypoint detection, and (iii) reinforcement learning with exponential moving average BatchNorm. The last two stages only modify $0.36\%$ parameters of the pre-trained representation in total, ensuring the knowledge from pre-training is maintained to the full extent. We empirically study 12 challenging dexterous manipulation tasks and find that H-InDex largely surpasses strong baseline methods and the recent visual foundation models for motor control. Code is available at https://yanjieze.com/H
    
[^129]: 可解释性模仿学习的动态DAG发现

    Dynamic DAG Discovery for Interpretable Imitation Learning. (arXiv:2310.00489v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.00489](http://arxiv.org/abs/2310.00489)

    提出了一种用于解释模仿学习中神经代理的动态DAG发现方法，通过有向无环因果图展现其捕获的知识，以增加透明度和可解释性。

    

    模仿学习通过模仿专家的示范来学习代理策略，在医疗治疗方案和自动驾驶等许多应用中显示出了有希望的结果。然而，解释代理学习到的控制策略仍然是一个困难的任务。困难主要来自两个方面：1）模仿学习中的代理通常实现为深度神经网络，这些模型是黑盒模型，缺乏可解释性；2）代理决策背后的潜在因果机制可能随着轨迹而变化，而不是在整个时间步骤中保持静态不变。为了增加神经代理的透明度和提供更好的可解释性，我们提出以有向无环因果图的形式展示其所捕获的知识，其中节点是动作和状态变量，边表示预测背后的因果关系。此外，我们设计这个因果发现过程是依赖状态的，使其能够对潜在因果图中的动态进行建模。

    Imitation learning, which learns agent policy by mimicking expert demonstration, has shown promising results in many applications such as medical treatment regimes and self-driving vehicles. However, it remains a difficult task to interpret control policies learned by the agent. Difficulties mainly come from two aspects: 1) agents in imitation learning are usually implemented as deep neural networks, which are black-box models and lack interpretability; 2) the latent causal mechanism behind agents' decisions may vary along the trajectory, rather than staying static throughout time steps. To increase transparency and offer better interpretability of the neural agent, we propose to expose its captured knowledge in the form of a directed acyclic causal graph, with nodes being action and state variables and edges denoting the causal relations behind predictions. Furthermore, we design this causal discovery process to be state-dependent, enabling it to model the dynamics in latent causal gr
    
[^130]: 在在线CMDPs中，无模型、遗憾最优的最佳策略识别

    Model-Free, Regret-Optimal Best Policy Identification in Online CMDPs. (arXiv:2309.15395v1 [cs.LG])

    [http://arxiv.org/abs/2309.15395](http://arxiv.org/abs/2309.15395)

    本文提出了一种无模型的算法，名为PRI，用于在线CMDPs中的最佳策略识别问题。该算法基于CMDPs的有限随机性属性，能够以低遗憾并以高概率识别出最优策略。

    

    本文考虑了在线约束马尔科夫决策过程（CMDPs）中的最佳策略识别（BPI）问题。我们对具有低遗憾并且以高概率识别最优策略的无模型算法感兴趣。现有的在线CMDPs的无模型算法在次线性遗憾和违约时没有提供任何对最优策略的收敛保证，并且只在从以前使用的策略中随机均匀抽样时提供平均性能保证。本文提出了一种新的算法，名为PRUNING-REFINEMENT-IDENTIFICATION（PRI），基于我们发现的CMDPs的一个基本结构性质，称为有限随机性。该属性表明对于具有N约束的CMDP，存在一个最优策略，其中至多有N个随机决策。所提出的算法首先识别出在哪个步骤和哪个状态需要进行随机决策，然后对这些决策的分布进行微调。

    This paper considers the best policy identification (BPI) problem in online Constrained Markov Decision Processes (CMDPs). We are interested in algorithms that are model-free, have low regret, and identify an optimal policy with a high probability. Existing model-free algorithms for online CMDPs with sublinear regret and constraint violation do not provide any convergence guarantee to an optimal policy and provide only average performance guarantees when a policy is uniformly sampled at random from all previously used policies. In this paper, we develop a new algorithm, named Pruning-Refinement-Identification (PRI), based on a fundamental structural property of CMDPs we discover, called limited stochasticity. The property says for a CMDP with $N$ constraints, there exists an optimal policy with at most $N$ stochastic decisions.  The proposed algorithm first identifies at which step and in which state a stochastic decision has to be taken and then fine-tunes the distributions of these s
    
[^131]: 引导在线蒸馏：通过离线演示来促进安全强化学习

    Guided Online Distillation: Promoting Safe Reinforcement Learning by Offline Demonstration. (arXiv:2309.09408v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2309.09408](http://arxiv.org/abs/2309.09408)

    本研究提出了Guided Online Distillation (GOLD)方法，通过从离线演示数据中提取专家策略来引导在线探索，解决了安全强化学习中保守性问题，并通过决策转换器模型对离线策略学习进行有效的高容量建模。

    

    安全强化学习旨在找到满足成本约束的高奖励策略。从零开始进行学习时，安全强化学习代理往往过于保守，这阻碍了探索并限制了整体性能。在许多现实任务中，例如自动驾驶，可以获得大规模的专家演示数据。我们认为从离线数据中提取专家策略来引导在线探索是解决保守性问题的一种有希望的解决方案。大容量模型，例如决策转换器（DT），已被证明在离线策略学习中表现出色。然而，在真实场景中收集的数据很少包含危险情况（例如碰撞），这使得策略很难学习安全概念。此外，这些大规模策略网络无法满足像自动驾驶这样的真实任务推理时间的计算速度要求。为此，我们提出了引导在线蒸馏（GOLD）的方法。

    Safe Reinforcement Learning (RL) aims to find a policy that achieves high rewards while satisfying cost constraints. When learning from scratch, safe RL agents tend to be overly conservative, which impedes exploration and restrains the overall performance. In many realistic tasks, e.g. autonomous driving, large-scale expert demonstration data are available. We argue that extracting expert policy from offline data to guide online exploration is a promising solution to mitigate the conserveness issue. Large-capacity models, e.g. decision transformers (DT), have been proven to be competent in offline policy learning. However, data collected in real-world scenarios rarely contain dangerous cases (e.g., collisions), which makes it prohibitive for the policies to learn safety concepts. Besides, these bulk policy networks cannot meet the computation speed requirements at inference time on real-world tasks such as autonomous driving. To this end, we propose Guided Online Distillation (GOLD), a
    
[^132]: 用DiscoSCMs回答Layer 3查询

    Answering Layer 3 queries with DiscoSCMs. (arXiv:2309.09323v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2309.09323](http://arxiv.org/abs/2309.09323)

    本文介绍了DiscoSCMs，一种用于解决因果查询的模型。它通过扩展结构因果模型和潜在结果框架来解决一致性规则引发的退化问题，并在分析个性化激励场景中的潜在结果时展示了其有效性。通过引入独立潜在噪声条件，可以提高解决Layer 3查询的准确性和可解释性。

    

    在当代因果推断研究中，解决Pearl因果层次（PCH）下的关联、干预和反事实的因果查询是一个核心任务。本文针对一致性规则引发的退化问题，引入了分布一致性结构因果模型（DiscoSCMs），扩展了结构因果模型（SCM）和潜在结果框架。以个性化激励场景中潜在结果的相关模式$P(y_x, y'_{x'})$为案例研究。尽管反事实不再退化，但仍无法确定。因此，将独立潜在噪声条件纳入DiscoSCM。发现通过适应分布的嵌入式推断，可以极大地提高解决Layer 3查询的准确性和可解释性。

    Addressing causal queries across the Pearl Causal Hierarchy (PCH) (i.e., associational, interventional and counterfactual), which is formalized as \Layer{} Valuations, is a central task in contemporary causal inference research. Counterfactual questions, in particular, pose a significant challenge as they often necessitate a complete knowledge of structural equations. This paper identifies \textbf{the degeneracy problem} caused by the consistency rule. To tackle this, the \textit{Distribution-consistency Structural Causal Models} (DiscoSCMs) is introduced, which extends both the structural causal models (SCM) and the potential outcome framework. The correlation pattern of potential outcomes in personalized incentive scenarios, described by $P(y_x, y'_{x'})$, is used as a case study for elucidation. Although counterfactuals are no longer degenerate, they remain indeterminable. As a result, the condition of independent potential noise is incorporated into DiscoSCM. It is found that by ad
    
[^133]: 纯蒙特卡洛反事实遗憾最小化

    Pure Monte Carlo Counterfactual Regret Minimization. (arXiv:2309.03084v1 [cs.AI])

    [http://arxiv.org/abs/2309.03084](http://arxiv.org/abs/2309.03084)

    纯蒙特卡洛反事实遗憾最小化算法（PCFR）是一种结合了反事实遗憾最小化（CFR）和虚拟游戏（FP）概念的新算法，能够与各种CFR变体相结合，包括蒙特卡洛CFR（MCCFR）。PCFR具有更好的性能和较快的收敛速度，同时降低了时间和空间复杂度。

    

    反事实遗憾最小化（CFR）及其变体是目前解决大规模不完全信息博弈的最佳算法。本文在CFR的基础上提出了一种名为纯CFR（PCFR）的新算法，以实现更好的性能。PCFR可以看作是CFR和虚拟游戏（FP）的结合，继承了CFR的反事实遗憾（值）的概念，并在下一次迭代中使用最佳响应策略而不是遗憾匹配策略。我们的理论证明了PCFR可以实现Blackwell可达性，使PCFR能够与包括蒙特卡洛CFR（MCCFR）在内的任何CFR变体相结合。由此产生的纯MCCFR（PMCCFR）可以大大降低时间和空间复杂度。特别地，PMCCFR的收敛速度至少比MCCFR快三倍。此外，由于PMCCFR不通过严格被支配策略的路径，我们开发了一种新的启动算法，受到了严格被支配策略的启示。

    Counterfactual Regret Minimization (CFR) and its variants are the best algorithms so far for solving large-scale incomplete information games. Building upon CFR, this paper proposes a new algorithm named Pure CFR (PCFR) for achieving better performance. PCFR can be seen as a combination of CFR and Fictitious Play (FP), inheriting the concept of counterfactual regret (value) from CFR, and using the best response strategy instead of the regret matching strategy for the next iteration. Our theoretical proof that PCFR can achieve Blackwell approachability enables PCFR's ability to combine with any CFR variant including Monte Carlo CFR (MCCFR). The resultant Pure MCCFR (PMCCFR) can significantly reduce time and space complexity. Particularly, the convergence speed of PMCCFR is at least three times more than that of MCCFR. In addition, since PMCCFR does not pass through the path of strictly dominated strategies, we developed a new warm-start algorithm inspired by the strictly dominated strat
    
[^134]: 通过平坦极小值和对抗鲁棒性解释激活稀疏性的理论解释

    Theoretical Explanation of Activation Sparsity through Flat Minima and Adversarial Robustness. (arXiv:2309.03004v1 [cs.LG])

    [http://arxiv.org/abs/2309.03004](http://arxiv.org/abs/2309.03004)

    提出了一个理论解释，将梯度稀疏性和激活稀疏性解释为对抗性鲁棒性的必要步骤，以隐藏特征和参数而言，这大致等于对学习良好模型的极小值平坦性。

    

    最近对MLP层中的激活稀疏性的实证观察为大幅降低计算成本提供了机会。尽管有几项研究将其归因于训练动力学，但激活稀疏性的理论解释仅限于浅层网络、小训练步长以及修改的训练，尽管这种稀疏性已在通过vanilla协议进行大步骤训练的深层模型中被发现。为了填补这三个差距，我们提出了梯度稀疏性的概念作为激活稀疏性的源头，并基于此提出了一个理论解释，该解释将梯度稀疏性和激活稀疏性解释为对抗性鲁棒性的必要步骤，以隐藏特征和参数而言，这大致等于对学习良好模型的极小值平坦性。这个理论适用于经过LayerNorm标准训练的纯MLP，并且如果在训练过程中给权重添加噪声，还适用于Transformers或其他架构。为了消除其他来源的激活稀疏性，我们还进行了进一步的实证研究。

    A recent empirical observation of activation sparsity in MLP layers offers an opportunity to drastically reduce computation costs for free. Despite several works attributing it to training dynamics, the theoretical explanation of activation sparsity's emergence is restricted to shallow networks, small training steps well as modified training, even though the sparsity has been found in deep models trained by vanilla protocols for large steps. To fill the three gaps, we propose the notion of gradient sparsity as the source of activation sparsity and a theoretical explanation based on it that explains gradient sparsity and then activation sparsity as necessary steps to adversarial robustness w.r.t. hidden features and parameters, which is approximately the flatness of minima for well-learned models. The theory applies to standardly trained LayerNorm-ed pure MLPs, and further to Transformers or other architectures if noises are added to weights during training. To eliminate other sources o
    
[^135]: FRGNN:通过测试时间特征重构减轻分布偏移对图神经网络的影响

    FRGNN: Mitigating the Impact of Distribution Shift on Graph Neural Networks via Test-Time Feature Reconstruction. (arXiv:2308.09259v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.09259](http://arxiv.org/abs/2308.09259)

    FRGNN是一个通用框架，通过测试时间特征重构，减轻分布偏移对图神经网络的影响，不需要重新训练模型，保留了原始特征的关键信息来改善性能。

    

    由于不合适的样本选择和有限的训练数据，训练集和测试集之间经常存在分布偏移。这种偏移可能对图神经网络（GNNs）的测试性能产生不利影响。现有方法通过增强GNNs对分布偏移的鲁棒性或减小偏移本身来缓解这个问题。然而，这两种方法都需要重新训练模型，当无法访问模型结构和参数时，这变得不可行。为了解决这个挑战，我们提出了FR-GNN，一种用于GNNs进行特征重构的通用框架。FRGNN构建了一个从经过良好训练的GNN的输出到输入的映射关系，以获得类别代表性嵌入，然后使用这些嵌入来重构标记节点的特征。然后，这些重构的特征被合并到GNNs的消息传递机制中，以影响测试时间未标记节点的预测。值得注意的是，这些重构的特征保留了原始特征的关键信息，以便在没有可训练模型的情况下改善GNNs的性能。

    Due to inappropriate sample selection and limited training data, a distribution shift often exists between the training and test sets. This shift can adversely affect the test performance of Graph Neural Networks (GNNs). Existing approaches mitigate this issue by either enhancing the robustness of GNNs to distribution shift or reducing the shift itself. However, both approaches necessitate retraining the model, which becomes unfeasible when the model structure and parameters are inaccessible. To address this challenge, we propose FR-GNN, a general framework for GNNs to conduct feature reconstruction. FRGNN constructs a mapping relationship between the output and input of a well-trained GNN to obtain class representative embeddings and then uses these embeddings to reconstruct the features of labeled nodes. These reconstructed features are then incorporated into the message passing mechanism of GNNs to influence the predictions of unlabeled nodes at test time. Notably, the reconstructed
    
[^136]: 本地自适应可微回归

    Locally Adaptive and Differentiable Regression. (arXiv:2308.07418v1 [cs.LG])

    [http://arxiv.org/abs/2308.07418](http://arxiv.org/abs/2308.07418)

    本文提出了一种本地自适应可微回归模型，通过对局部学习模型进行加权平均，在不同本地区域处理数据时具有竞争力，并在理论上实现更快的统计收敛以及在实际应用中改善了性能。

    

    过度参数化模型，如深度神经网络和随机森林，在机器学习中变得非常受欢迎。然而，在现代超参数化的本地自适应模型中，常见的连续性和可微性目标往往被忽视。我们提出了一个通用框架，通过在对应的本地区域中对局部学习模型进行加权平均来构建全局连续可微模型。该模型在处理具有不同密度或不同本地区域中的函数值尺度的数据时具有竞争力。我们证明，当我们在本地模型中混合使用核岭和多项式回归项，并对它们进行连续拼接时，在理论上实现更快的统计收敛，并在各种实际环境中实现改进的性能。

    Over-parameterized models like deep nets and random forests have become very popular in machine learning. However, the natural goals of continuity and differentiability, common in regression models, are now often ignored in modern overparametrized, locally-adaptive models. We propose a general framework to construct a global continuous and differentiable model based on a weighted average of locally learned models in corresponding local regions. This model is competitive in dealing with data with different densities or scales of function values in different local regions. We demonstrate that when we mix kernel ridge and polynomial regression terms in the local models, and stitch them together continuously, we achieve faster statistical convergence in theory and improved performance in various practical settings.
    
[^137]: MM-Vet: 评估大型多模态模型的综合能力

    MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities. (arXiv:2308.02490v1 [cs.AI])

    [http://arxiv.org/abs/2308.02490](http://arxiv.org/abs/2308.02490)

    MM-Vet是一个评估标准，用于评估大型多模态模型在复杂任务上的综合能力。该标准解决了如何结构化和评估复杂多模态任务、设计适用于不同问题和回答类型的评估指标以及如何提供模型洞察的问题。通过整合不同的核心视觉-语言能力，MM-Vet展示了有趣的能力和解决复杂任务的方法。

    

    我们提出了MM-Vet，一个评估标准，用于检查在复杂多模态任务上的大型多模态模型（LMM）的表现。最近的LMM展示了各种有趣的能力，例如解决书写在黑板上的数学问题，推理新闻图片中的事件和名人，以及解释视觉笑话。快速的模型进步给评估标准的开发带来了挑战。问题包括：（1）如何系统地构建和评估复杂的多模态任务；（2）如何设计适用于不同类型问题和回答的评估指标；（3）如何给出超出简单性能排名的模型洞察。为此，我们提出了MM-Vet，基于这样一个洞察：解决复杂任务的有趣能力通常通过一种通才模型能够整合不同的核心视觉-语言（VL）能力来实现。MM-Vet定义了6个核心VL能力，并检查了从这些能力组合中得出的16种有趣的整合方式。

    We propose MM-Vet, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MM-Vet, designed based on the insight that the intriguing ability to solve complicated tasks is often achieved by a generalist model being able to integrate different core vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from the capability combin
    
[^138]: 本地-全局时间融合网络结合注意机制用于多类和多种心律失常分类

    Local-Global Temporal Fusion Network with an Attention Mechanism for Multiple and Multiclass Arrhythmia Classification. (arXiv:2308.02416v1 [eess.SP])

    [http://arxiv.org/abs/2308.02416](http://arxiv.org/abs/2308.02416)

    本文提出了一个本地-全局时间融合网络结合注意机制的框架，用于针对心律失常的检测和分类任务。该方法通过提取本地时间信息和全局模式，并使用注意力机制进行本地-全局信息融合，以处理长度变化的心律失常数据。实验结果表明，此方法在心律失常分类任务上的性能优于现有方法。

    

    临床决策支持系统（CDSSs）已被广泛应用于支持心电图（ECGs）中心血管科医生检测和分类心律失常时所做的决策。然而，由于心律失常长度的变化，形成一个针对心律失常分类任务的CDSS是具有挑战性的。虽然心律失常的发作时间是变化的，但之前开发的方法没有考虑到这些条件。因此，我们提出了一个框架，包括（i）本地时间信息提取，（ii）全局模式提取，和（iii）带有注意力的本地-全局信息融合，以实现对有限输入长度的心律失常检测和分类。我们的方法通过检测心律失常起始和结束时间作为一个事件，以及基于MIT-BIH心律失常数据库（MITDB）和MIT-BIH房颤数据库（AFDB）的心律失常持续时间来评估10类和4类表现。结果在统计上优于现有方法。

    Clinical decision support systems (CDSSs) have been widely utilized to support the decisions made by cardiologists when detecting and classifying arrhythmia from electrocardiograms (ECGs). However, forming a CDSS for the arrhythmia classification task is challenging due to the varying lengths of arrhythmias. Although the onset time of arrhythmia varies, previously developed methods have not considered such conditions. Thus, we propose a framework that consists of (i) local temporal information extraction, (ii) global pattern extraction, and (iii) local-global information fusion with attention to perform arrhythmia detection and classification with a constrained input length. The 10-class and 4-class performances of our approach were assessed by detecting the onset and offset of arrhythmia as an episode and the duration of arrhythmia based on the MIT-BIH arrhythmia database (MITDB) and MIT-BIH atrial fibrillation database (AFDB), respectively. The results were statistically superior to 
    
[^139]: AutoML4ETC: 自动化神经架构搜索实现现实世界加密流量分类

    AutoML4ETC: Automated Neural Architecture Search for Real-World Encrypted Traffic Classification. (arXiv:2308.02182v1 [cs.NI])

    [http://arxiv.org/abs/2308.02182](http://arxiv.org/abs/2308.02182)

    AutoML4ETC是一个自动设计高效且高性能神经架构的工具，用于加密流量分类。其通过定义新颖的搜索空间和使用不同的搜索策略，在多个数据集上优于当前最先进的加密流量分类器。

    

    在实验环境中，深度学习（DL）已成功应用于加密网络流量分类。然而，在实际应用中，DL分类器的性能随时间不可避免地下降。仅仅对新数据集进行模型重新训练只能部分提高其性能。手动调整模型架构以满足新数据集上的性能期望耗时且需要领域专业知识。本文提出了一种新颖的工具AutoML4ETC，用于自动设计高效且高性能的神经架构以进行加密流量分类。我们定义了一个新颖而强大的搜索空间，专门针对使用数据包头字节进行近实时加密流量分类。通过在搜索空间上使用不同的搜索策略，我们展示了AutoML4ETC生成的神经架构在多个数据集上均优于当前最先进的加密流量分类器，包括公共基准数据集。

    Deep learning (DL) has been successfully applied to encrypted network traffic classification in experimental settings. However, in production use, it has been shown that a DL classifier's performance inevitably decays over time. Re-training the model on newer datasets has been shown to only partially improve its performance. Manually re-tuning the model architecture to meet the performance expectations on newer datasets is time-consuming and requires domain expertise. We propose AutoML4ETC, a novel tool to automatically design efficient and high-performing neural architectures for encrypted traffic classification. We define a novel, powerful search space tailored specifically for the near real-time classification of encrypted traffic using packet header bytes. We show that with different search strategies over our search space, AutoML4ETC generates neural architectures that outperform the state-of-the-art encrypted traffic classifiers on several datasets, including public benchmark dat
    
[^140]: 可解释的等变神经网络在粒子物理中的应用：PELICAN

    Explainable Equivariant Neural Networks for Particle Physics: PELICAN. (arXiv:2307.16506v2 [hep-ph] UPDATED)

    [http://arxiv.org/abs/2307.16506](http://arxiv.org/abs/2307.16506)

    PELICAN是一种可解释的等变神经网络，应用于粒子物理问题中。相比于其他方法，PELICAN的优势在于它采用了基于对称群的架构，具有降低复杂性、增加可解释性和提高性能的特点。它在标记和重构动量增强的顶夸克，并在密集环境中特别识别和测量W玻色子，以及识别不同类型的喷注等任务方面展示了出色的表现。

    

    PELICAN是一种新颖的置换等变且洛伦兹不变或协变的聚合网络，旨在克服应用于粒子物理问题的常见限制。与许多使用非专用架构的方法相比，PELICAN采用基于对称群的架构，体现了复杂度降低、可解释性增强和性能提升等优势，而非以庞大的参数为代价。我们在标记（分类）和重构（回归）动量增强的顶夸克的背景下对PELICAN算法架构进行全面研究，包括在洛伦兹增强的顶夸克强子末态的密集环境中特别识别和测量W玻色子的困难任务。我们还将PELICAN应用于识别夸克-引发与胶子-引发喷注以及多类别分类任务。

    PELICAN is a novel permutation equivariant and Lorentz invariant or covariant aggregator network designed to overcome common limitations found in architectures applied to particle physics problems. Compared to many approaches that use non-specialized architectures that neglect underlying physics principles and require very large numbers of parameters, PELICAN employs a fundamentally symmetry group-based architecture that demonstrates benefits in terms of reduced complexity, increased interpretability, and raw performance. We present a comprehensive study of the PELICAN algorithm architecture in the context of both tagging (classification) and reconstructing (regression) Lorentz-boosted top quarks, including the difficult task of specifically identifying and measuring the $W$-boson inside the dense environment of the Lorentz-boosted top-quark hadronic final state. We also extend the application of PELICAN to the tasks of identifying quark-initiated vs.~gluon-initiated jets, and a multi-
    
[^141]: 关于注意力网络学习动态的研究

    On the learning Dynamics of Attention Networks. (arXiv:2307.13421v1 [cs.LG])

    [http://arxiv.org/abs/2307.13421](http://arxiv.org/abs/2307.13421)

    本研究分析了软注意力、硬注意力和潜变量边际似然（LVML）注意力三种注意力模型的学习动态，发现了它们在所选择的片段聚合方式上的显著差异，并解释了分类模型在梯度下降下的演化对最终结果的影响。

    

    注意力模型通常通过优化三个标准损失函数之一来学习，分别称为软注意力、硬注意力和潜变量边际似然（LVML）注意力。这三种范式都是为了达到相同的目标，即找到两个模型：一个“焦点”模型，用于“选择”输入中的正确“片段”，和一个“分类”模型，用于将选定的片段处理成目标标签。然而，它们在所选择的片段聚合方式上存在显著差异，导致了不同的动态和最终结果。我们观察到使用这些范式学习的模型具有独特的特征，并将其解释为在焦点模型固定时，分类模型在梯度下降下的演化所致。我们还在一个简单的设置中分析了这些范式，并推导出梯度流下参数轨迹的闭式表达式。在软注意力损失下，焦点模型在初始化阶段快速改善。

    Attention models are typically learned by optimizing one of three standard loss functions that are variously called -- soft attention, hard attention, and latent variable marginal likelihood (LVML) attention. All three paradigms are motivated by the same goal of finding two models -- a `focus' model that `selects' the right \textit{segment} of the input and a `classification' model that processes the selected segment into the target label. However, they differ significantly in the way the selected segments are aggregated, resulting in distinct dynamics and final results. We observe a unique signature of models learned using these paradigms and explain this as a consequence of the evolution of the classification model under gradient descent when the focus model is fixed. We also analyze these paradigms in a simple setting and derive closed-form expressions for the parameter trajectory under gradient flow. With the soft attention loss, the focus model improves quickly at initialization a
    
[^142]: 基于图神经网络的日志异常检测与解释

    Graph Neural Networks based Log Anomaly Detection and Explanation. (arXiv:2307.00527v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2307.00527](http://arxiv.org/abs/2307.00527)

    提出了一种基于图神经网络的无监督日志异常检测方法，该方法将事件日志转换为带属性、有向和加权的图，并利用图神经网络进行图级别的异常检测。引入了一种新的图神经网络模型OCDiGCN来检测一组带属性、有向和加权的图中的图级别异常，并提供对异常的解释能力。

    

    事件日志被广泛用于记录高科技系统的状态，因此日志异常检测对于监控这些系统非常重要。大多数现有的日志异常检测方法将日志事件计数矩阵或日志事件序列作为输入，利用日志事件之间的定量和/或顺序关系来检测异常。然而，仅考虑定量或顺序关系可能导致检测准确性较低。为了缓解这个问题，我们提出了一种基于图的无监督日志异常检测方法，称为Logs2Graphs，它首先将事件日志转换为带属性、有向和加权的图，然后利用图神经网络进行图级别的异常检测。具体而言，我们提出了一种全新的图神经网络模型One-Class Digraph Inception Convolutional Networks（OCDiGCN），用于在一组带属性、有向和加权的图中检测图级别的异常。通过将图表示与属性表示耦合起来，在图级别上进行异常检测的同时提供了对异常的解释能力。

    Event logs are widely used to record the status of high-tech systems, making log anomaly detection important for monitoring those systems. Most existing log anomaly detection methods take a log event count matrix or log event sequences as input, exploiting quantitative and/or sequential relationships between log events to detect anomalies. Unfortunately, only considering quantitative or sequential relationships may result in low detection accuracy. To alleviate this problem, we propose a graph-based method for unsupervised log anomaly detection, dubbed Logs2Graphs, which first converts event logs into attributed, directed, and weighted graphs, and then leverages graph neural networks to perform graph-level anomaly detection. Specifically, we introduce One-Class Digraph Inception Convolutional Networks, abbreviated as OCDiGCN, a novel graph neural network model for detecting graph-level anomalies in a collection of attributed, directed, and weighted graphs. By coupling the graph represe
    
[^143]: 可证明的针对AI生成文本的鲁棒水印技术

    Provable Robust Watermarking for AI-Generated Text. (arXiv:2306.17439v1 [cs.CL])

    [http://arxiv.org/abs/2306.17439](http://arxiv.org/abs/2306.17439)

    GPTWatermark是一种针对性模型水印技术，通过固定分组设计和强大的可证明保证，提供了对AI生成文本的鲁棒性检测和安全性防御。实验证明了其在检测准确性和生成质量方面的优越性，推动了LLMs负责任使用的进步。

    

    随着AI生成的文本越来越接近人类撰写的内容，检测机器生成的文本的能力变得至关重要。为了应对这一挑战，我们提出了GPTWatermark，一种强大且高质量的解决方案，用于确定一段文本是否来自特定模型。我们的方法扩展了现有的水印策略，并采用了一种固定的分组设计，以增强对编辑和改写攻击的鲁棒性。我们展示了我们的带水印语言模型在生成质量、检测正确性和对抗规避攻击的安全性方面具有强大的可证明保证。在各种大型语言模型（LLMs）和多样化数据集上的实验结果表明，我们的方法在检测准确性方面达到了优越的表现，并且与生成质量在困惑度方面相当，从而促进了LLMs的负责任使用。

    As AI-generated text increasingly resembles human-written content, the ability to detect machine-generated text becomes crucial. To address this challenge, we present GPTWatermark, a robust and high-quality solution designed to ascertain whether a piece of text originates from a specific model. Our approach extends existing watermarking strategies and employs a fixed group design to enhance robustness against editing and paraphrasing attacks. We show that our watermarked language model enjoys strong provable guarantees on generation quality, correctness in detection, and security against evasion attacks. Experimental results on various large language models (LLMs) and diverse datasets demonstrate that our method achieves superior detection accuracy and comparable generation quality in perplexity, thus promoting the responsible use of LLMs.
    
[^144]: 未知环境中的在线覆盖路径规划的端到端强化学习

    End-to-end Reinforcement Learning for Online Coverage Path Planning in Unknown Environments. (arXiv:2306.16978v1 [cs.RO])

    [http://arxiv.org/abs/2306.16978](http://arxiv.org/abs/2306.16978)

    本文提出了一种基于端到端强化学习的在线覆盖路径规划方法，能处理未知环境并结合全局地图和局部感知输入，同时考虑长期路径规划和短期障碍物检测。

    

    覆盖路径规划是寻找覆盖给定封闭区域整个自由空间的最短路径的问题，应用范围从机器人割草和吸尘到地雷清除和搜救任务。虽然离线方法可以为已知环境找到可证明完备且在某些情况下是最优的路径，但在在线场景下，环境事先未知，特别是在存在非静态障碍物的情况下，其价值有限。我们提出了一种基于连续状态和动作空间的端到端强化学习方法，用于处理未知环境的在线覆盖路径规划问题。我们从全局地图和局部感知输入构建观察空间，使代理能够规划长期路径，并同时对短期障碍物进行行动。为了考虑大规模环境，我们提出使用多尺度地图输入表示。此外，我们提出了一种新颖的总变差正则化方法以减少路径偏离问题。

    Coverage path planning is the problem of finding the shortest path that covers the entire free space of a given confined area, with applications ranging from robotic lawn mowing and vacuum cleaning, to demining and search-and-rescue tasks. While offline methods can find provably complete, and in some cases optimal, paths for known environments, their value is limited in online scenarios where the environment is not known beforehand, especially in the presence of non-static obstacles. We propose an end-to-end reinforcement learning-based approach in continuous state and action space, for the online coverage path planning problem that can handle unknown environments. We construct the observation space from both global maps and local sensory inputs, allowing the agent to plan a long-term path, and simultaneously act on short-term obstacle detections. To account for large-scale environments, we propose to use a multi-scale map input representation. Furthermore, we propose a novel total var
    
[^145]: SIMF: 自动驾驶的语义感知交互式运动预测

    SIMF: Semantics-aware Interactive Motion Forecasting for Autonomous Driving. (arXiv:2306.14941v1 [cs.CV])

    [http://arxiv.org/abs/2306.14941](http://arxiv.org/abs/2306.14941)

    本文提出了一种名为SIMF的方法，用于自动驾驶车辆中语义感知的交互式运动预测。该方法通过实现基于语义的行为体选择和注意力机制提取全局编码，能够捕捉空间信息和语义信息，并优选相关的行为体进行运动预测。

    

    自动驾驶车辆需要对周围多个行为体（行人和车辆）进行运动预测，以做出最优导航决策。现有的方法主要关注如何利用这些行为体的位置和速度，并未能捕捉到场景中的语义信息。此外，为了减少与场景中行为体数量增加相关的计算复杂度，一些方法利用欧氏距离来剪枝远离的行为体。然而，仅仅基于距离的度量无法选择相关的行为体并准确进行预测。为了解决这些问题，我们提出了一种称为SIMF的方法，用于捕捉空间信息以及语义信息，并优选相关的行为体进行运动预测。具体而言，我们通过实现一种基于语义的行为体选择方法，将其通过注意力机制传递，以提取全局编码。

    Autonomous vehicles require motion forecasting of their surrounding multi-agents (pedestrians and vehicles) to make optimal decisions for navigation. The existing methods focus on techniques to utilize the positions and velocities of these agents and fail to capture semantic information from the scene. Moreover, to mitigate the increase in computational complexity associated with the number of agents in the scene, some works leverage Euclidean distance to prune far-away agents. However, distance-based metric alone is insufficient to select relevant agents and accurately perform their predictions. To resolve these issues, we propose Semantics-aware Interactive Motion Forecasting (SIMF) method to capture semantics along with spatial information, and optimally select relevant agents for motion prediction. Specifically, we achieve this by implementing a semantic-aware selection of relevant agents from the scene and passing them through an attention mechanism to extract global encodings. Th
    
[^146]: OpenDataVal：一种数据价值评估的统一基准测试

    OpenDataVal: a Unified Benchmark for Data Valuation. (arXiv:2306.10577v1 [cs.LG])

    [http://arxiv.org/abs/2306.10577](http://arxiv.org/abs/2306.10577)

    本文介绍了一种名为OpenDataVal的基准测试框架，该框架整合了多种数据集和九种最先进的数据估值算法实现，并提供了四个下游机器学习任务来评估数据价值的质量。

    

    评估单个数据点的质量和影响对于提高模型性能和减轻训练数据集中不良偏差至关重要。尽管已经提出了几个数据估值算法来量化数据质量，但还缺乏一个系统化和标准化的数据估值基准测试系统。本文介绍了OpenDataVal，一种易于使用和统一的基准测试框架，使研究人员和从业者能够应用和比较各种数据估值算法。OpenDataVal提供了一个综合环境，包括（i）各种图像，自然语言和表格数据集，（ii）九种不同的最先进的数据估值算法的实现，以及（iii）可以导入任何scikit-learn模型的预测模型API。此外，我们提出了四个下游机器学习任务，用于评估数据值的质量。我们使用OpenDataVal进行基准测试分析，量化并比较不同数据估值算法在不同数据集上的表现。

    Assessing the quality and impact of individual data points is critical for improving model performance and mitigating undesirable biases within the training dataset. Several data valuation algorithms have been proposed to quantify data quality, however, there lacks a systemic and standardized benchmarking system for data valuation. In this paper, we introduce OpenDataVal, an easy-to-use and unified benchmark framework that empowers researchers and practitioners to apply and compare various data valuation algorithms. OpenDataVal provides an integrated environment that includes (i) a diverse collection of image, natural language, and tabular datasets, (ii) implementations of nine different state-of-the-art data valuation algorithms, and (iii) a prediction model API that can import any models in scikit-learn. Furthermore, we propose four downstream machine learning tasks for evaluating the quality of data values. We perform benchmarking analysis using OpenDataVal, quantifying and comparin
    
[^147]: 机器人操纵的通用语义几何表示

    A Universal Semantic-Geometric Representation for Robotic Manipulation. (arXiv:2306.10474v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2306.10474](http://arxiv.org/abs/2306.10474)

    这篇论文提出了一种通用的机器人感知模块，称为语义几何表示（SGR），该模块结合了大规模预训练的2D模型的丰富语义信息和3D空间推理的优势，能够在各种模拟和真实世界的机器人操纵任务中胜过最先进的方法。

    

    机器人在感知和与世界互动时 heavily relies 传感器，特别是RGB和深度相机。RGB相机记录了具有丰富语义信息的2D图像，但缺乏精确的空间信息。另一方面，深度相机提供了关键的3D几何数据，但捕捉到的语义有限。因此，整合两种模态对于学习机器人感知和控制的表示是至关重要的。然而，当前的研究主要集中在其中一种模态上，并忽略了结合两者的好处。为此，我们提出了$\textbf{语义几何表示} (\textbf{SGR})$，这是一个用于机器人的通用感知模块，它利用了大规模预训练的2D模型的丰富语义信息，并承继了3D空间推理的优点。我们的实验表明，SGR使机器人能够成功完成各种模拟和真实世界的机器人操纵任务，胜过了最先进的方法。

    Robots rely heavily on sensors, especially RGB and depth cameras, to perceive and interact with the world. RGB cameras record 2D images with rich semantic information while missing precise spatial information. On the other side, depth cameras offer critical 3D geometry data but capture limited semantics. Therefore, integrating both modalities is crucial for learning representations for robotic perception and control. However, current research predominantly focuses on only one of these modalities, neglecting the benefits of incorporating both. To this end, we present $\textbf{Semantic-Geometric Representation} (\textbf{SGR})$, a universal perception module for robotics that leverages the rich semantic information of large-scale pre-trained 2D models and inherits the merits of 3D spatial reasoning. Our experiments demonstrate that SGR empowers the agent to successfully complete a diverse range of simulated and real-world robotic manipulation tasks, outperforming state-of-the-art methods 
    
[^148]: 多视角分类增量学习

    Multi-View Class Incremental Learning. (arXiv:2306.09675v1 [cs.LG])

    [http://arxiv.org/abs/2306.09675](http://arxiv.org/abs/2306.09675)

    本文提出了一种名为多视角分类增量学习（MVCIL）的新模型，该模型使用随机化的表示学习技术进行特征提取，并提出正交融合子空间和选择性权重合并来解决增量学习中遗忘旧信息和学习新概念的挑战。实验结果表明该方法相比最新方法有效性更高。

    

    多视角学习（MVL）在整合数据集的多个视角以提高下游任务性能方面取得了巨大成功。为了使MVL方法在开放式环境中更实用，本文研究了一种新的范例，称为多视角分类增量学习（MVCIL），其中单个模型从连续的视图流中逐步分类新类，不需要访问早期数据的视图。但是，MVCIL面临着老信息的灾难性遗忘和学习新概念的干扰。为了解决这个问题，我们首先开发了一种基于随机化的表示学习技术，用于特征提取，以保证它们在工作状态下的分离视图最优，其中属于类的多个视图按顺序呈现；然后，我们将它们逐个集成到由提取的特征跨越的正交融合子空间中；最后，我们介绍选择性权重合并，以保留旧类的知识。基准数据集上的实验结果证明了我们提出的方法相对于最新方法的有效性。

    Multi-view learning (MVL) has gained great success in integrating information from multiple perspectives of a dataset to improve downstream task performance. To make MVL methods more practical in an open-ended environment, this paper investigates a novel paradigm called multi-view class incremental learning (MVCIL), where a single model incrementally classifies new classes from a continual stream of views, requiring no access to earlier views of data. However, MVCIL is challenged by the catastrophic forgetting of old information and the interference with learning new concepts. To address this, we first develop a randomization-based representation learning technique serving for feature extraction to guarantee their separate view-optimal working states, during which multiple views belonging to a class are presented sequentially; Then, we integrate them one by one in the orthogonality fusion subspace spanned by the extracted features; Finally, we introduce selective weight consolidation f
    
[^149]: 取消七年的算法公平性后处理

    Unprocessing Seven Years of Algorithmic Fairness. (arXiv:2306.07261v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.07261](http://arxiv.org/abs/2306.07261)

    该论文取消了算法公平性中的后处理方法，并发现后处理实现的公平性-准确性Pareto边界包含了可评估的所有其他方法。

    

    七年前，研究人员提出了一种后处理方法，以使模型在不同人口群体中的误差率相等。这项工作启动了数百篇论文，声称能够改进后处理基线。我们通过对几个表格数据集上数千个模型评估的实证评估来评估这些声明。我们发现，后处理实现的公平性-准确性Pareto边界包含我们可以评估的所有其他方法。这样做，我们解决了两个常见的方法论错误，这些错误困扰了以前的观察结果。一个与使用不同的无约束基础模型比较方法有关。另一个涉及实现不同的约束放松水平的方法。我们研究的核心是一种简单的想法，我们称之为取消处理，大致对应于后处理的反演。取消处理允许直接比较使用不同基础模型和放松级别的方法。解读我们的发现。

    Seven years ago, researchers proposed a postprocessing method to equalize the error rates of a model across different demographic groups. The work launched hundreds of papers purporting to improve over the postprocessing baseline. We empirically evaluate these claims through thousands of model evaluations on several tabular datasets. We find that the fairness-accuracy Pareto frontier achieved by postprocessing contains all other methods we were feasibly able to evaluate. In doing so, we address two common methodological errors that have confounded previous observations. One relates to the comparison of methods with different unconstrained base models. The other concerns methods achieving different levels of constraint relaxation. At the heart of our study is a simple idea we call unprocessing that roughly corresponds to the inverse of postprocessing. Unprocessing allows for a direct comparison of methods using different underlying models and levels of relaxation. Interpreting our findi
    
[^150]: 基于Implicit Neural Representations的时间序列连续建模用于插值和预测

    Time Series Continuous Modeling for Imputation and Forecasting with Implicit Neural Representations. (arXiv:2306.05880v1 [cs.LG])

    [http://arxiv.org/abs/2306.05880](http://arxiv.org/abs/2306.05880)

    该论文提出了基于INR的时间序列连续建模方法，解决了处理缺失数据、不规则采样和多传感器不对准观测等重复建模问题，并在预测和插值任务中取得了最新的性能表现，具有很好的泛化能力。

    

    尽管时间序列建模已被广泛探索，但在面对真实世界的数据时仍面临重大挑战。我们提出了一种新颖的建模方法，利用Implicit Neural Representations (INR)。该方法使我们能够有效地捕捉时间序列的连续性，并提供了自然的解决方案，以处理缺失数据、处理不规则采样或来自多个传感器的不对准观测等重复建模问题。通过引入条件调制INR参数并利用元学习技术，我们解决了模型泛化到未见样本和时间窗口移位的问题。通过大量实验，我们的模型展示了在预测和插值任务中领先的性能，同时在处理许多竞争模型无法处理的各种具有挑战性的场景方面展现了灵活性。

    Although widely explored, time series modeling continues to encounter significant challenges when confronted with real-world data. We propose a novel modeling approach leveraging Implicit Neural Representations (INR). This approach enables us to effectively capture the continuous aspect of time series and provides a natural solution to recurring modeling issues such as handling missing data, dealing with irregular sampling, or unaligned observations from multiple sensors. By introducing conditional modulation of INR parameters and leveraging meta-learning techniques, we address the issue of generalization to both unseen samples and time window shifts. Through extensive experimentation, our model demonstrates state-of-the-art performance in forecasting and imputation tasks, while exhibiting flexibility in handling a wide range of challenging scenarios that competing models cannot.
    
[^151]: 对抗性非线性约束下的在线学习

    Online Learning under Adversarial Nonlinear Constraints. (arXiv:2306.03655v1 [cs.LG])

    [http://arxiv.org/abs/2306.03655](http://arxiv.org/abs/2306.03655)

    提出了一种在线学习算法CVV-Pro，可以处理对抗性的时变和非线性约束，只依赖于局部稀疏线性逼近，达到了$\sqrt{T}$遗憾率和$1/\sqrt{T}$的收敛速度。

    

    在许多应用程序中，学习系统需要处理连续的非稳态数据流。我们在在线学习框架中研究了这个问题，并提出了一种算法，可以处理对抗性的时变和非线性约束。正如我们在这项工作中所展示的那样，这个名为Constraint Violation Velocity Projection (CVV-Pro)的算法达到了$\sqrt{T}$的遗憾率，并以$1/\sqrt{T}$的速度收敛于可行集，尽管可行集缓慢地随时间变化且不为学习者所知。CVV-Pro仅依赖于可行集的局部稀疏线性逼近，因此避免了在每次迭代中优化整个集合，这与投影梯度或Frank-Wolfe方法形成鲜明对比。我们还在两个玩家游戏中对算法进行了实证评估，其中玩家受到共享约束的限制。

    In many applications, learning systems are required to process continuous non-stationary data streams. We study this problem in an online learning framework and propose an algorithm that can deal with adversarial time-varying and nonlinear constraints. As we show in our work, the algorithm called Constraint Violation Velocity Projection (CVV-Pro) achieves $\sqrt{T}$ regret and converges to the feasible set at a rate of $1/\sqrt{T}$, despite the fact that the feasible set is slowly time-varying and a priori unknown to the learner. CVV-Pro only relies on local sparse linear approximations of the feasible set and therefore avoids optimizing over the entire set at each iteration, which is in sharp contrast to projected gradients or Frank-Wolfe methods. We also empirically evaluate our algorithm on two-player games, where the players are subjected to a shared constraint.
    
[^152]: 抓住意外收获：在离线策略演员-评论家中利用过去成功的价值(arXiv:2306.02865v2 [cs.LG]已更新)

    Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic. (arXiv:2306.02865v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02865](http://arxiv.org/abs/2306.02865)

    该论文提出了 BEE 操作符，通过充分利用过去的成功经验，并保持探索乐观性，解决了离线策略演员-评论家中 Q 值高估与低估问题，提高了策略学习和样本效率。

    

    学习高质量的 Q 值函数在许多现代离线深度强化学习 (RL) 算法的成功中起着关键作用。之前的研究集中解决采用函数逼近器和离线学习所导致的值过高的问题。与这种普遍观点不同，我们观察到 Q 值在 RL 训练过程的后期实际上被低估了，主要是由于贝尔曼更新中，当前策略使用比回放缓冲区中更优的动作样本差。我们假设这个长期被忽视的现象可能阻碍了策略学习，降低了样本效率。我们的想法是在保持探索乐观性的同时，结合充分利用过去成功的经验。我们提出了混合利用和探索 (BEE) 操作符，这是一种简单而有效的方法，使用历史上表现最佳的动作和当前策略生成的动作来更新 Q 值。

    Learning high-quality Q-value functions plays a key role in the success of many modern off-policy deep reinforcement learning (RL) algorithms. Previous works focus on addressing the value overestimation issue, an outcome of adopting function approximators and off-policy learning. Deviating from the common viewpoint, we observe that Q-values are indeed underestimated in the latter stage of the RL training process, primarily related to the use of inferior actions from the current policy in Bellman updates as compared to the more optimal action samples in the replay buffer. We hypothesize that this long-neglected phenomenon potentially hinders policy learning and reduces sample efficiency. Our insight to address this issue is to incorporate sufficient exploitation of past successes while maintaining exploration optimism. We propose the Blended Exploitation and Exploration (BEE) operator, a simple yet effective approach that updates Q-value using both historical best-performing actions and
    
[^153]: 学习特征中的瓶颈结构：低维度与规律性的权衡

    Bottleneck Structure in Learned Features: Low-Dimension vs Regularity Tradeoff. (arXiv:2305.19008v1 [cs.LG])

    [http://arxiv.org/abs/2305.19008](http://arxiv.org/abs/2305.19008)

    本研究揭示了深度学习神经网路学习输入低维度表示和最小化特征映射中的复杂性/不规则性之间的权衡，控制了规律性，并利用理论工具证明了瓶颈结构的存在。

    

    先前研究表明，具有大深度$L$和$L_{2}$正则化的DNN偏向于学习输入的低维表示，可以解释为最小化学习函数$f$的秩$R^{(0)}(f)$的概念，其被推测为瓶颈秩。我们计算了这个结果的有限深度修正，揭示了一个度量$R^{(1)}$的规律性，它控制了雅可比矩阵$\left|Jf(x)\right|_{+}$的伪行列式并在组合和加法下是次可加的。这使得网络可以在学习低维表示和最小化特征映射中的复杂性/不规则性之间保持平衡，从而学习“正确”的内部尺寸。我们还展示了大学习速率如何控制学习函数的规律性。最后，我们使用这些理论工具证明了瓶颈结构在$L\to\infty$时在学习特征中的猜想：对于大深度，几乎所有的隐藏表示都集中在...

    Previous work has shown that DNNs with large depth $L$ and $L_{2}$-regularization are biased towards learning low-dimensional representations of the inputs, which can be interpreted as minimizing a notion of rank $R^{(0)}(f)$ of the learned function $f$, conjectured to be the Bottleneck rank. We compute finite depth corrections to this result, revealing a measure $R^{(1)}$ of regularity which bounds the pseudo-determinant of the Jacobian $\left|Jf(x)\right|_{+}$ and is subadditive under composition and addition. This formalizes a balance between learning low-dimensional representations and minimizing complexity/irregularity in the feature maps, allowing the network to learn the `right' inner dimension. We also show how large learning rates also control the regularity of the learned function. Finally, we use these theoretical tools to prove the conjectured bottleneck structure in the learned features as $L\to\infty$: for large depths, almost all hidden representations concentrates aroun
    
[^154]: 跨领域策略适应性通过价值导向数据过滤

    Cross-Domain Policy Adaptation via Value-Guided Data Filtering. (arXiv:2305.17625v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.17625](http://arxiv.org/abs/2305.17625)

    通过价值目标的一致性，我们提出了一种价值导向的数据过滤算法(VGDF)，用于解决在不同领域之间进行策略适应的问题。

    

    在强化学习中，不同领域之间的策略泛化存在着动力学不匹配的重大挑战。例如，一个机器人在模拟器中学习策略，但当它在真实世界中部署时，环境的动力学可能会有所不同。针对动力学不匹配的源域和目标域，我们考虑在线动力学适应问题，在这种情况下，智能体可以访问足够的源域数据，而与目标域的在线交互是有限的。现有的研究试图从动力学差异的角度解决这个问题。在本文中，我们揭示了这些方法的局限性，并通过对领域之间价值的一种新的洞察，从价值差异的角度探索问题。具体而言，我们提出了基于价值目标对两个领域之间的配对值的距离的Value-Guided Data Filtering (VGDF)算法，该算法有选择性地共享来自源域的转换。Empir。

    Generalizing policies across different domains with dynamics mismatch poses a significant challenge in reinforcement learning. For example, a robot learns the policy in a simulator, but when it is deployed in the real world, the dynamics of the environment may be different. Given the source and target domain with dynamics mismatch, we consider the online dynamics adaptation problem, in which case the agent can access sufficient source domain data while online interactions with the target domain are limited. Existing research has attempted to solve the problem from the dynamics discrepancy perspective. In this work, we reveal the limitations of these methods and explore the problem from the value difference perspective via a novel insight on the value consistency across domains. Specifically, we present the Value-Guided Data Filtering (VGDF) algorithm, which selectively shares transitions from the source domain based on the proximity of paired value targets across the two domains. Empir
    
[^155]: 用多模态语言模型生成图片

    Generating Images with Multimodal Language Models. (arXiv:2305.17216v1 [cs.CL])

    [http://arxiv.org/abs/2305.17216](http://arxiv.org/abs/2305.17216)

    该论文提出了一种方法，将大型语言模型与预训练的图像编码器和解码器模型进行融合，能生成具有连贯性的图像输出，同时也能进行图像检索和多模态对话。

    

    我们提出了一种方法，将仅包含文本的大型语言模型（LLMs）与预训练的图像编码器和解码器模型进行融合，通过映射它们的嵌入空间。我们的模型展示了广泛的多模态能力：图像检索、新颖图像生成和多模态对话。这是第一种能够在任意交错的图像和文本输入之间进行条件调节，生成连贯图像（和文本）输出的方法。为了在图像生成任务中取得强大的性能，我们提出了一种有效的映射网络，将LLM基于现成的文本到图像生成模型，将文本的隐藏表示转换为视觉模型的嵌入空间，利用LLM强大的文本表示来生成视觉输出。我们的方法在长且复杂语言的任务上优于基准生成模型。除了新颖图像生成之外，我们的模型还能够从文本描述中检索图像，并进行多模态对话。

    We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespe
    
[^156]: 在Transformer语言模型中解决关系任务的机制

    A Mechanism for Solving Relational Tasks in Transformer Language Models. (arXiv:2305.16130v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.16130](http://arxiv.org/abs/2305.16130)

    这篇论文研究了在Transformer语言模型中解决关系任务的机制，并发现这些模型利用简单的线性更新来处理关系任务，并以内容无关的方式促进关系的输出。

    

    这篇论文提供了证据表明，尽管语言模型（LMs）的规模和复杂性，它们有时候利用一个简单的计算机制来解决一对一的关系任务（例如 capital_of(Poland)=Warsaw）。我们在上下文学习环境中研究了一系列语言模型的大小（从124M参数到176B参数），并发现对于多种任务（涉及首都、大写和过去时态等），机制的关键部分可以简化为前馈（FFN）网络通常应用的简单线性更新。这些更新也倾向于以内容无关的方式促进关系的输出（例如对编码 Poland:Warsaw::China:Beijing），揭示了这些模型在解决这些任务中的可预测模式。我们进一步显示这个机制是特定于需要从预训练存储器中检索而不是从局部上下文检索的任务。我们的结果为解决关系任务的语言模型的机制做出了贡献。

    A primary criticism towards language models (LMs) is their inscrutability. This paper presents evidence that, despite their size and complexity, LMs sometimes exploit a simple computational mechanism to solve one-to-one relational tasks (e.g., capital_of(Poland)=Warsaw). We investigate a range of language model sizes (from 124M parameters to 176B parameters) in an in-context learning setting, and find that for a variety of tasks (involving capital cities, upper-casing, and past-tensing) a key part of the mechanism reduces to a simple linear update typically applied by the feedforward (FFN) networks. These updates also tend to promote the output of the relation in a content-independent way (e.g., encoding Poland:Warsaw::China:Beijing), revealing a predictable pattern that these models take in solving these tasks. We further show that this mechanism is specific to tasks that require retrieval from pretraining memory, rather than retrieval from local context. Our results contribute to a g
    
[^157]: 时间变化处理的反事实生成模型

    Counterfactual Generative Models for Time-Varying Treatments. (arXiv:2305.15742v1 [stat.ML])

    [http://arxiv.org/abs/2305.15742](http://arxiv.org/abs/2305.15742)

    本文研究了时间变量处理情况下的反事实生成模型，能够捕捉整个反事实分布，并且能够有效推断反事实分布的某些统计量，适用于医疗保健和公共政策制定领域。

    

    估计平均因果效应是测试新疗法的常用做法。然而，平均效应会掩盖反事实分布中重要的个体特征，可能会引起安全、公平和道德方面的担忧。这个问题在时间设置中更加严重，因为处理是时序的和时变的，对反事实分布产生了错综复杂的影响。本文提出了一种新的条件生成建模方法，以捕获整个反事实分布，允许对反事实分布的某些统计量进行有效推断。这使得所提出的方法尤其适用于医疗保健和公共政策制定领域。我们的生成建模方法通过边际结构模型谨慎地解决了观察数据和目标反事实分布之间的分布不匹配。在合成和真实数据上，我们的方法优于现有的基线方法。

    Estimating average causal effects is a common practice to test new treatments. However, the average effect ''masks'' important individual characteristics in the counterfactual distribution, which may lead to safety, fairness, and ethical concerns. This issue is exacerbated in the temporal setting, where the treatment is sequential and time-varying, leading to an intricate influence on the counterfactual distribution. In this paper, we propose a novel conditional generative modeling approach to capture the whole counterfactual distribution, allowing efficient inference on certain statistics of the counterfactual distribution. This makes the proposed approach particularly suitable for healthcare and public policy making. Our generative modeling approach carefully tackles the distribution mismatch in the observed data and the targeted counterfactual distribution via a marginal structural model. Our method outperforms state-of-the-art baselines on both synthetic and real data.
    
[^158]: 学生超越了大师：基于GPT-3的科学事实错误校正方法的匹配

    The student becomes the master: Matching GPT3 on Scientific Factual Error Correction. (arXiv:2305.14707v1 [cs.CL])

    [http://arxiv.org/abs/2305.14707](http://arxiv.org/abs/2305.14707)

    本文提出了一种不需要验证者且不做领域假设的主张校正系统，能够显著提高科学事实错误校正任务的性能，并通过使用LLM的提示方法和主张感知的解码过程来提高校正质量。

    

    由于创建错误校正数据集的成本极高，大多数事实主张校正方法依赖于强大的验证模型来指导校正过程。这导致在科学事实校正等领域性能显著下降，因为好的验证模型并不总是存在。在本研究中，我们介绍了一种不做领域假设且不需要验证者的主张校正系统，但能够比现有方法提高一个数量级的性能 - 在SciFact数据集上实现94％的修正准确性，在SciFact-Open数据集上实现62.5％的修正准确性，分别比下一个最好的方法高出0.5％和1.50％。我们的方法利用LLMs中的提示功能，在训练期间创建一个丰富注释的数据集，可用于完全监督的训练和正则化。我们还使用主张感知的解码过程来提高纠正主张的质量。我们的方法与用于创建数据集的LLM相竞争，证明了利用基于LLM的训练提高科学主张校正任务性能的可能性。

    Due to the prohibitively high cost of creating error correction datasets, most Factual Claim Correction methods rely on a powerful verification model to guide the correction process. This leads to a significant drop in performance in domains like Scientific Claim Correction, where good verification models do not always exist. In this work, we introduce a claim correction system that makes no domain assumptions and does not require a verifier but is able to outperform existing methods by an order of magnitude -- achieving 94% correction accuracy on the SciFact dataset, and 62.5% on the SciFact-Open dataset, compared to the next best methods 0.5% and 1.50% respectively. Our method leverages the power of prompting with LLMs during training to create a richly annotated dataset that can be used for fully supervised training and regularization. We additionally use a claim-aware decoding procedure to improve the quality of corrected claims. Our method is competitive with the very LLM that was
    
[^159]: Newton-Cotes图神经网络：论动态系统的时间演化

    Newton-Cotes Graph Neural Networks: On the Time Evolution of Dynamic Systems. (arXiv:2305.14642v1 [cs.LG])

    [http://arxiv.org/abs/2305.14642](http://arxiv.org/abs/2305.14642)

    本文提出了基于Newton-Cotes公式的方法来预测动态系统的时间演化，与现有最先进的方法相比较， 实验结果表明该方法有着显著的改进。

    

    研究系统动态性是许多科学研究中最重要的分析方法之一。使用系统的初始状态作为输入，最近基于图神经网络（GNNs）的方法能够高精度地预测远离初始状态的未来状态。虽然这些方法在建模系统的坐标和相互作用力方面有不同的设计，但我们发现它们实际上共享一种常数的积分学习范例， 能够在初始和终端坐标之间的时间间隔内预测速度的积分。受此观察的启发，我们提出了一种新的方法，基于用Newton-Cotes公式估算的若干速度估计来预测积分，并理论上证明其有效性。 在几个基准实验上，我们发现该方法与现有最先进的方法相比有着持续且显著的改进。

    Reasoning system dynamics is one of the most important analytical approaches for many scientific studies. With the initial state of a system as input, the recent graph neural networks (GNNs)-based methods are capable of predicting the future state distant in time with high accuracy. Although these methods have diverse designs in modeling the coordinates and interacting forces of the system, we show that they actually share a common paradigm that learns the integration of the velocity over the interval between the initial and terminal coordinates. However, their integrand is constant w.r.t. time. Inspired by this observation, we propose a new approach to predict the integration based on several velocity estimations with Newton-Cotes formulas and prove its effectiveness theoretically. Extensive experiments on several benchmarks empirically demonstrate consistent and significant improvement compared with the state-of-the-art methods.
    
[^160]: 学习从兼容标签序列中的语义角色标注

    Learning Semantic Role Labeling from Compatible Label Sequences. (arXiv:2305.14600v1 [cs.CL])

    [http://arxiv.org/abs/2305.14600](http://arxiv.org/abs/2305.14600)

    该论文探讨了如何从不相交的兼容标签序列中高效地学习，将此运用于语义角色标注任务，提出了联合处理VerbNet和PropBank标签的方法，并验证了其有效性。

    

    本文探讨了如何高效地学习从不相交的兼容标签序列中标注的问题。我们认为，不相交标签集之间的兼容结构有助于模型的学习和推理。我们在语义角色标注（SRL）任务中验证了这一假设，具体地，标记具有两个角色序列的句子：VerbNet参数和PropBank参数。先前的研究已经表明跨任务交互可以提高性能。但是，这两个任务仍然是分别解码的，存在生成结构不一致的标签序列 (在像SEMLINK的词典中)的风险。为了消除这个问题，我们首先提出了一个简单而有效的设置，联合处理VerbNet和PropBank标签作为一个序列。通过这个设置，我们证明了在解码过程中强制执行SEMLINK约束不断提高总F1值。通过特殊的输入构造，我们的联合模型可以以超过99%的准确性从PropBank参数中推断出VerbNet参数。我们还提出了一种co

    This paper addresses the question of how to efficiently learn from disjoint, compatible label sequences. We argue that the compatible structures between disjoint label sets help model learning and inference. We verify this hypothesis on the task of semantic role labeling (SRL), specifically, tagging a sentence with two role sequences: VerbNet arguments and PropBank arguments. Prior work has shown that cross-task interaction improves performance. However, the two tasks are still separately decoded, running the risk of generating structurally inconsistent label sequences (as per lexicons like SEMLINK). To eliminate this issue, we first propose a simple and effective setup that jointly handles VerbNet and PropBank labels as one sequence. With this setup, we show that enforcing SEMLINK constraints during decoding constantly improves the overall F1. With special input constructions, our joint model infers VerbNet arguments from PropBank arguments with over 99% accuracy. We also propose a co
    
[^161]: 不使用反向传播训练神经网络：深入探究似然比方法

    Training Neural Networks without Backpropagation: A Deeper Dive into the Likelihood Ratio Method. (arXiv:2305.08960v1 [cs.LG])

    [http://arxiv.org/abs/2305.08960](http://arxiv.org/abs/2305.08960)

    提出一种新的似然比方法来训练神经网络，无需使用递归梯度计算，并在多种神经网络架构上有效地减少了对抗性攻击对模型造成的影响。

    

    反向传播是深度学习中训练神经网络的最重要的梯度估计方法。然而，文献表明，通过反向传播训练的神经网络容易受到对抗性攻击。我们开发了似然比方法，这是一种新的梯度估计方法，可以训练广泛的神经网络架构，包括卷积神经网络、循环神经网络、图神经网络和脉冲神经网络，而无需递归梯度计算。我们提出了三种方法来有效地减少神经网络训练过程中梯度估计的方差。我们的实验在多个数据集上训练不同的神经网络，并得到了数值结果。所有结果都表明，相对于反向传播方法，似然比方法对抗性攻击下有效地训练了各种神经网络，并显着提高了神经网络的鲁棒性。

    Backpropagation (BP) is the most important gradient estimation method for training neural networks in deep learning. However, the literature shows that neural networks trained by BP are vulnerable to adversarial attacks. We develop the likelihood ratio (LR) method, a new gradient estimation method, for training a broad range of neural network architectures, including convolutional neural networks, recurrent neural networks, graph neural networks, and spiking neural networks, without recursive gradient computation. We propose three methods to efficiently reduce the variance of the gradient estimation in the neural network training process. Our experiments yield numerical results for training different neural networks on several datasets. All results demonstrate that the LR method is effective for training various neural networks and significantly improves the robustness of the neural networks under adversarial attacks relative to the BP method.
    
[^162]: DOCTOR：基于可穿戴医疗传感器的多疾病检测持续学习框架

    DOCTOR: A Multi-Disease Detection Continual Learning Framework Based on Wearable Medical Sensors. (arXiv:2305.05738v1 [cs.LG])

    [http://arxiv.org/abs/2305.05738](http://arxiv.org/abs/2305.05738)

    DOCTOR是一种基于可穿戴医疗传感器的多疾病检测持续学习框架，采用了多头深度神经网络和Exemplar-replay风格的CL算法。它可以不断地学习新任务，并在内存使用、电池消耗和检测复杂度方面优于传统的ML驱动疾病检测方法。

    

    现代机器学习（ML）和边缘设备中的可穿戴医疗传感器（WMS）的进步使得智能医疗的ML驱动疾病检测成为可能。传统的ML驱动疾病检测方法依赖于为每种疾病和相应的WMS数据定制个别模型。然而，这种方法缺乏对分布变化和新任务分类的适应性。同时，为了检测每个新疾病，需要从头开始重新构建和训练模型。针对这些挑战，我们提出了基于WMS的多疾病检测持续学习框架DOCTOR。它采用了多头深度神经网络（DNN）和一种Exemplar-replay风格的CL算法。CL算法使得框架能够不断地学习新任务，其中涉及不同的数据分布、分类类别和疾病检测任务。DOCTOR在使用来自实际WMS的公共数据集进行四种常见疾病检测方面取得了最先进的性能。同时，在内存使用、电池消耗和检测复杂度方面，DOCTOR也优于基线方法。

    Modern advances in machine learning (ML) and wearable medical sensors (WMSs) in edge devices have enabled ML-driven disease detection for smart healthcare. Conventional ML-driven disease detection methods rely on customizing individual models for each disease and its corresponding WMS data. However, such methods lack adaptability to distribution shifts and new task classification classes. Also, they need to be rearchitected and retrained from scratch for each new disease. Moreover, installing multiple ML models in an edge device consumes excessive memory, drains the battery faster, and complicates the detection process. To address these challenges, we propose DOCTOR, a multi-disease detection continual learning (CL) framework based on WMSs. It employs a multi-headed deep neural network (DNN) and an exemplar-replay-style CL algorithm. The CL algorithm enables the framework to continually learn new missions where different data distributions, classification classes, and disease detection
    
[^163]: 使用SO(3)-等变隐式神经表示生成活细胞模型

    Generative modeling of living cells with SO(3)-equivariant implicit neural representations. (arXiv:2304.08960v1 [cs.CV])

    [http://arxiv.org/abs/2304.08960](http://arxiv.org/abs/2304.08960)

    本文提出了使用有符号距离函数作为形状表示，通过神经网络计算所得，对旋转具有等变性，来生成逼真的活细胞模型，为生物医学成像中的数据驱动细胞跟踪和分割方法提供高质量训练数据集。

    

    生物医学成像中基于数据的细胞跟踪和分割方法需要多样化和信息丰富的训练数据。当训练样本数量有限时，可以使用合成的计算机生成数据集来提高这些方法的准确性。这需要使用生成模型合成细胞形状以及相应的显微镜图像。为了合成逼真的活细胞形态，生成模型使用的形状表示应能够准确表示细节和拓扑变化，这在细胞中很常见。这些要求并不适用于3D体素掩模，因为它们有分辨率限制，也不适用于多边形网格，因为无法易于模拟细胞增长和有丝分裂等过程。在本文中，我们提出使用有符号距离函数（SDFs）的水平集来表示活细胞形状，这些水平集由神经网络估计得出，而且对旋转具有等变性。我们还介绍了一种将此表示转换为网格和RGB图像以进行可视化和用于下游计算机视觉任务的方法。

    Data-driven cell tracking and segmentation methods in biomedical imaging require diverse and information-rich training data. In cases where the number of training samples is limited, synthetic computer-generated data sets can be used to improve these methods. This requires the synthesis of cell shapes as well as corresponding microscopy images using generative models. To synthesize realistic living cell shapes, the shape representation used by the generative model should be able to accurately represent fine details and changes in topology, which are common in cells. These requirements are not met by 3D voxel masks, which are restricted in resolution, and polygon meshes, which do not easily model processes like cell growth and mitosis. In this work, we propose to represent living cell shapes as level sets of signed distance functions (SDFs) which are estimated by neural networks. We optimize a fully-connected neural network to provide an implicit representation of the SDF value at any p
    
[^164]: 图神经网络中池化的表达能力

    The expressive power of pooling in Graph Neural Networks. (arXiv:2304.01575v1 [cs.LG])

    [http://arxiv.org/abs/2304.01575](http://arxiv.org/abs/2304.01575)

    本文研究了池化算子在图神经网络中的表达能力，并提供了一个通用标准来选择或设计池化算子。

    

    在图神经网络（GNNs）中，分层池化算子通过创建图结构和其顶点特征的本地摘要来生成输入数据的更粗糙的表示。虽然已经致力于研究GNN中消息传递（MP）层的表达能力，但缺乏关于池化算子如何影响GNN表达能力的研究。此外，尽管最近在有效池化算子的设计方面取得了进展，但没有一个原则性的标准来比较它们。我们的工作旨在通过提供足够的条件使池化算子在其之前的MP层中完全保留表达能力来填补这一空白。这些条件作为选择现有池化算子或设计新的池化算子的通用和理论基础的标准。基于我们的理论发现，我们审查了几个现有的池化算子，并确定了那些不能满足表达性假设的算子。

    In Graph Neural Networks (GNNs), hierarchical pooling operators generate a coarser representation of the input data by creating local summaries of the graph structure and its vertex features. Considerable attention has been devoted to studying the expressive power of message-passing (MP) layers in GNNs, while a study on how pooling operators affect the expressivity of a GNN is still lacking. Additionally, despite the recent advances in the design of effective pooling operators, there is not a principled criterion to compare them. Our work aims to fill this gap by providing sufficient conditions for a pooling operator to fully preserve the expressive power of the MP layers before it. These conditions serve as a universal and theoretically-grounded criterion for choosing among existing pooling operators or designing new ones. Based on our theoretical findings, we reviewed several existing pooling operators and identified those that fail to satisfy the expressiveness assumptions. Finally,
    
[^165]: 使用图神经网络重建粒子物理过程的拓扑结构

    Topological Reconstruction of Particle Physics Processes using Graph Neural Networks. (arXiv:2303.13937v1 [hep-ph])

    [http://arxiv.org/abs/2303.13937](http://arxiv.org/abs/2303.13937)

    Topograph是一种利用图神经网络和粒子衰变自然规律的拓扑结构重建方法，不仅解决了观测到的末态对象组合指派问题，还预测了中间粒子的性质及其后续衰变，比标准方法效果更好，与现代机器学习技术表现相当。

    

    我们提出了一种新的方法，称为Topograph，它利用粒子物理衰变的本质和信息传递图神经网络的灵活性，重建了包括中介粒子在内的底层物理过程。Topograph不仅解决了观测到的末态对象的组合指派问题，将它们与它们原来的母粒子关联起来，而且直接预测了硬散射过程中中间粒子的性质及其后续衰变。与标准的组合方法或现代图神经网络方法相比，它的复杂度与重构对象的数量成线性关系。我们应用Topograph于全强子衰变模式下的顶夸克对产生问题，相对标准方法，我们的方法表现更优，与最先进的机器学习技术相当。

    We present a new approach, the Topograph, which reconstructs underlying physics processes, including the intermediary particles, by leveraging underlying priors from the nature of particle physics decays and the flexibility of message passing graph neural networks. The Topograph not only solves the combinatoric assignment of observed final state objects, associating them to their original mother particles, but directly predicts the properties of intermediate particles in hard scatter processes and their subsequent decays. In comparison to standard combinatoric approaches or modern approaches using graph neural networks, which scale exponentially or quadratically, the complexity of Topographs scales linearly with the number of reconstructed objects.  We apply Topographs to top quark pair production in the all hadronic decay channel, where we outperform the standard approach and match the performance of the state-of-the-art machine learning technique.
    
[^166]: 零和马尔可夫博弈中强化学习的新政策迭代算法

    A New Policy Iteration Algorithm For Reinforcement Learning in Zero-Sum Markov Games. (arXiv:2303.09716v1 [cs.LG])

    [http://arxiv.org/abs/2303.09716](http://arxiv.org/abs/2303.09716)

    本文提出了一种适用于零和马尔可夫博弈的简单但有效的策略迭代算法。

    

    许多基于模型的强化学习算法可以被视为具有两个阶段: 学习阶段和规划阶段。在标准MDPs情况下，可以使用价值迭代或策略迭代来解决学习问题。但在零和马尔可夫博弈的情况下，没有有效的策略迭代算法，以前的尝试都有局限性。本文提出了一种简单的策略迭代变体，能够有效地解决这个问题。

    Many model-based reinforcement learning (RL) algorithms can be viewed as having two phases that are iteratively implemented: a learning phase where the model is approximately learned and a planning phase where the learned model is used to derive a policy. In the case of standard MDPs, the learning problem can be solved using either value iteration or policy iteration. However, in the case of zero-sum Markov games, there is no efficient policy iteration algorithm; e.g., it has been shown in Hansen et al. (2013) that one has to solve Omega(1/(1-alpha)) MDPs, where alpha is the discount factor, to implement the only known convergent version of policy iteration. Another algorithm for Markov zero-sum games, called naive policy iteration, is easy to implement but is only provably convergent under very restrictive assumptions. Prior attempts to fix naive policy iteration algorithm have several limitations. Here, we show that a simple variant of naive policy iteration for games converges, and 
    
[^167]: 通过变分自编码器和域适应实现无偏态形态分类

    From Images to Features: Unbiased Morphology Classification via Variational Auto-Encoders and Domain Adaptation. (arXiv:2303.08627v1 [astro-ph.GA])

    [http://arxiv.org/abs/2303.08627](http://arxiv.org/abs/2303.08627)

    本研究提出了一种无偏态的星系形态分类方法，利用变分自编码器和域适应实现低维度表示，40维潜在变量能够有效再现星系图像中的大多数形态特征，并通过经典随机森林分类器实现了详细的形态特征分类。此外，该方法可以无偏地应用于两个不同的星系调查中。

    

    我们提出了一种新的方法，通过利用变分自编码器（VAE）和域适应（DA）的组合来降低星系图像的维度。我们使用Galaxy-Zoo DECaLS项目中具有详细形态类型标签的低红移星系的样本，证明了该方法的有效性。我们证明了40维潜在变量能够有效地再现星系图像中的大多数形态特征。为了进一步验证我们方法的有效性，我们利用40维潜在变量上的经典随机森林（RF）分类器来实现详细的形态特征分类。这种方法与直接应用神经网络分类效果相似。我们进一步通过使用DECaLS和BASS+MzLS重叠区域中的星系来调整VAE网络，从而增强了我们的模型，使其能够无偏地应用于这两个调查中的星系图像。我们观察到，在DA期间，噪声得到了有效抑制。

    We present a novel approach for the dimensionality reduction of galaxy images by leveraging a combination of variational auto-encoders (VAE) and domain adaptation (DA). We demonstrate the effectiveness of this approach using a sample of low redshift galaxies with detailed morphological type labels from the Galaxy-Zoo DECaLS project. We show that 40-dimensional latent variables can effectively reproduce most morphological features in galaxy images. To further validate the effectiveness of our approach, we utilised a classical random forest (RF) classifier on the 40-dimensional latent variables to make detailed morphology feature classifications. This approach performs similarly to a direct neural network application on galaxy images. We further enhance our model by tuning the VAE network via DA using galaxies in the overlapping footprint of DECaLS and BASS+MzLS, enabling the unbiased application of our model to galaxy images in both surveys. We observed that noise suppression during DA 
    
[^168]: 核密度贝叶斯逆强化学习

    Kernel Density Bayesian Inverse Reinforcement Learning. (arXiv:2303.06827v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06827](http://arxiv.org/abs/2303.06827)

    KD-BIRL是一种核密度贝叶斯逆强化学习方法，通过直接逼近似然函数来学习代理的奖励函数，克服了学习点估计的缺点，并适用于复杂和无限环境。

    

    逆强化学习（IRL）是一种通过观察代理行为来推断其奖励函数的强大框架，但学习奖励函数的点估计可能会误导，因为可能有多个函数能够很好地描述代理的行为。贝叶斯逆强化学习采用贝叶斯方法模拟候选奖励函数的分布，克服了学习点估计的缺点。然而，一些贝叶斯逆强化学习算法使用Q值函数代替似然函数。由此得到的后验计算量大，理论保证少，并且Q值函数通常对似然函数的逼近效果较差。我们引入了核密度贝叶斯逆强化学习（KD-BIRL），该方法使用条件核密度估计直接逼近似然函数，提供了一个高效的框架，在经过改进的奖励函数参数化下，适用于具有复杂和无限的环境。

    Inverse reinforcement learning~(IRL) is a powerful framework to infer an agent's reward function by observing its behavior, but IRL algorithms that learn point estimates of the reward function can be misleading because there may be several functions that describe an agent's behavior equally well. A Bayesian approach to IRL models a distribution over candidate reward functions, alleviating the shortcomings of learning a point estimate. However, several Bayesian IRL algorithms use a $Q$-value function in place of the likelihood function. The resulting posterior is computationally intensive to calculate, has few theoretical guarantees, and the $Q$-value function is often a poor approximation for the likelihood. We introduce kernel density Bayesian IRL (KD-BIRL), which uses conditional kernel density estimation to directly approximate the likelihood, providing an efficient framework that, with a modified reward function parameterization, is applicable to environments with complex and infin
    
[^169]: DP-Fast MH: 大规模贝叶斯推断的私有、快速、准确的Metropolis-Hastings算法

    DP-Fast MH: Private, Fast, and Accurate Metropolis-Hastings for Large-Scale Bayesian Inference. (arXiv:2303.06171v1 [cs.LG])

    [http://arxiv.org/abs/2303.06171](http://arxiv.org/abs/2303.06171)

    本文提出了一种新的DP-Fast MH算法，用于大规模贝叶斯推断，具有精确、快速和隐私保护的特点。

    This paper proposes a new DP-Fast MH algorithm for large-scale Bayesian inference, which is accurate, fast, and privacy-preserving.

    贝叶斯推断提供了一个从复杂数据中学习和在不确定性下推理的原则性框架。它已经广泛应用于机器学习任务，如医学诊断、药物设计和政策制定。在这些常见应用中，数据可能非常敏感。差分隐私（DP）提供了具有强大最坏情况隐私保证的数据分析工具，并已发展成为隐私保护数据分析的主要方法。在本文中，我们研究了Metropolis-Hastings（MH）算法，这是最基本的MCMC方法之一，用于差分隐私下的大规模贝叶斯推断。虽然大多数现有的私有MCMC算法为了获得隐私而牺牲了准确性和效率，但我们提供了第一个精确且快速的DP MH算法，大多数迭代中仅使用一个小批量的数据。我们进一步揭示了隐私、可扩展性（即批量大小）和效率（即收敛速度）之间的三重权衡，从理论上说明了这一点。

    Bayesian inference provides a principled framework for learning from complex data and reasoning under uncertainty. It has been widely applied in machine learning tasks such as medical diagnosis, drug design, and policymaking. In these common applications, the data can be highly sensitive. Differential privacy (DP) offers data analysis tools with powerful worst-case privacy guarantees and has been developed as the leading approach in privacy-preserving data analysis. In this paper, we study Metropolis-Hastings (MH), one of the most fundamental MCMC methods, for large-scale Bayesian inference under differential privacy. While most existing private MCMC algorithms sacrifice accuracy and efficiency to obtain privacy, we provide the first exact and fast DP MH algorithm, using only a minibatch of data in most iterations. We further reveal, for the first time, a three-way trade-off among privacy, scalability (i.e. the batch size), and efficiency (i.e. the convergence rate), theoretically char
    
[^170]: 单细胞多模态预测的Transformer研究

    Single-Cell Multimodal Prediction via Transformers. (arXiv:2303.00233v2 [q-bio.GN] UPDATED)

    [http://arxiv.org/abs/2303.00233](http://arxiv.org/abs/2303.00233)

    本研究研究了如何利用Transformer模型在端到端的方式上处理多模态单细胞数据，并利用下游任务信息，提出了一个名为scMoFormer的框架

    

    最近的多模态单细胞技术的发展使得从单个细胞中获取多个组学数据成为可能，从而实现对细胞状态和动态的更深入理解。然而，多模态单细胞数据的激增也带来了建模不同模态之间复杂相互作用的巨大挑战。最近的先进方法侧重于构建静态交互图，并应用图神经网络(GNNs)从多模态数据中学习。然而，这样的静态图可能不尽如人意，因为它们没有利用下游任务信息；而且，当深度堆叠GNN层时，GNNs也有一些固有的局限性。为了解决这些问题，本文研究了如何利用Transformer模型在端到端的方式上处理多模态单细胞数据，并利用下游任务信息。具体而言，我们提出了一个名为scMoFormer的框架，它可以轻松地整合外部的d

    The recent development of multimodal single-cell technology has made the possibility of acquiring multiple omics data from individual cells, thereby enabling a deeper understanding of cellular states and dynamics. Nevertheless, the proliferation of multimodal single-cell data also introduces tremendous challenges in modeling the complex interactions among different modalities. The recently advanced methods focus on constructing static interaction graphs and applying graph neural networks (GNNs) to learn from multimodal data. However, such static graphs can be suboptimal as they do not take advantage of the downstream task information; meanwhile GNNs also have some inherent limitations when deeply stacking GNN layers. To tackle these issues, in this work, we investigate how to leverage transformers for multimodal single-cell data in an end-to-end manner while exploiting downstream task information. In particular, we propose a scMoFormer framework which can readily incorporate external d
    
[^171]: 异质分布偏移下的统计学习

    Statistical Learning under Heterogenous Distribution Shift. (arXiv:2302.13934v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13934](http://arxiv.org/abs/2302.13934)

    本文研究了异质分布偏移下的统计学习问题，通过研究经验风险最小化(ERM)在不同类别的复杂性下的表现，我们发现当类别$F$相比类别$G$更“简单”时，我们的预测器对于协变量偏移具有更强的鲁棒性，尤其在$\textbf{y}$的偏移远小于$\textbf{x}$的情况下。同时，我们发现ERM的行为与正交机器学习具有类似的特性。

    

    本文研究了从随机变量对$(\mathbf{x},\mathbf{y})$中预测目标$\mathbf{z}$, 其中真实的预测器是加法的$\mathbb{E}[\mathbf{z} \mid \mathbf{x},\mathbf{y}] = f_\star(\mathbf{x}) +g_{\star}(\mathbf{y})$。我们研究了在给定训练分布上拟合的函数$f+g$, $f \in F$和$g \in G$上的经验风险最小化(ERM)在表现上的差异，但在测试分布上得到评估时会显示出协变量偏移。我们的研究表明，当类别$F$比$G$更“简单”（例如，以度量熵为衡量标准）时，我们的预测器对于协变量偏移的抗干扰能力更强，其中$\textbf{y}$的偏移要远小于$\textbf{x}$的偏移。我们的分析表明，ERM的行为与正交机器学习$\textbf{ qualitatively similarly}$：ERM恢复预测器中的$f$成分的速率仅对于类别$G$的复杂性具有较低阶的依赖性，调整后...

    This paper studies the prediction of a target $\mathbf{z}$ from a pair of random variables $(\mathbf{x},\mathbf{y})$, where the ground-truth predictor is additive $\mathbb{E}[\mathbf{z} \mid \mathbf{x},\mathbf{y}] = f_\star(\mathbf{x}) +g_{\star}(\mathbf{y})$. We study the performance of empirical risk minimization (ERM) over functions $f+g$, $f \in F$ and $g \in G$, fit on a given training distribution, but evaluated on a test distribution which exhibits covariate shift. We show that, when the class $F$ is "simpler" than $G$ (measured, e.g., in terms of its metric entropy), our predictor is more resilient to $\textbf{heterogenous covariate shifts}$ in which the shift in $\mathbf{x}$ is much greater than that in $\mathbf{y}$. Our analysis proceeds by demonstrating that ERM behaves $\textbf{qualitatively similarly to orthogonal machine learning}$: the rate at which ERM recovers the $f$-component of the predictor has only a lower-order dependence on the complexity of the class $G$, adjus
    
[^172]: 从时间-顶点谱学习图ARMA过程

    Learning Graph ARMA Processes from Time-Vertex Spectra. (arXiv:2302.06887v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.06887](http://arxiv.org/abs/2302.06887)

    本研究提出了一种基于学习过程谱密度的算法，用于推断缺失的信号值和进行信号插值，实验结果显示其在时间-顶点信号估计问题中具有高准确性。

    

    将时间变化的图信号建模为稳态时间-顶点随机过程，可以通过有效地利用过程在不同图节点和时间瞬间之间的相关性模式来推断缺失的信号值。在这项研究中，我们提出了一种算法，用于基于学习过程的不完整实现的联合时间-顶点功率谱密度来计算图自回归移动平均（图ARMA）过程，以用于信号插值任务。我们的解决方案首先通过部分观察到的实现粗略估计过程的联合谱，然后通过凸松弛将其投影到图ARMA过程的谱流形上来改进这个估计。然后，基于学习的模型估计最初缺失的信号值。实验结果表明，所提出的方法在时间-顶点信号估计问题中达到了很高的准确度。

    The modeling of time-varying graph signals as stationary time-vertex stochastic processes permits the inference of missing signal values by efficiently employing the correlation patterns of the process across different graph nodes and time instants. In this study, we propose an algorithm for computing graph autoregressive moving average (graph ARMA) processes based on learning the joint time-vertex power spectral density of the process from its incomplete realizations for the task of signal interpolation. Our solution relies on first roughly estimating the joint spectrum of the process from partially observed realizations and then refining this estimate by projecting it onto the spectrum manifold of the graph ARMA process through convex relaxations. The initially missing signal values are then estimated based on the learnt model. Experimental results show that the proposed approach achieves high accuracy in time-vertex signal estimation problems.
    
[^173]: 知识是微调语言模型中权重空间的一个区域

    Knowledge is a Region in Weight Space for Fine-tuned Language Models. (arXiv:2302.04863v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04863](http://arxiv.org/abs/2302.04863)

    研究探讨了不同模型在权重空间中的位置与性能的关联，发现微调语言模型在权重空间中有明确定义的区域，且这些区域中的模型表现出高性能。此外，通过绕过这些区域，可以得到性能相当甚至更好的新模型。

    

    神经网络研究一直专注于理解单个模型在单个数据集上的训练结果。然而，对于不同模型之间的关系，特别是那些在不同数据集上进行训练或测试的模型之间的关系，我们了解甚少。我们通过研究不同模型的权重空间和潜在的损失地形之间的相互关系来解决这个问题。具体而言，我们证明了为高性能而进行微调优化的模型存在于权重空间中定义明确的区域中，反之亦然——任何在这些区域中的模型都表现出高性能。值得注意的是，我们展示了在相同数据集上进行微调的语言模型在权重空间中形成一个紧密的聚类，而在相同基础任务下从不同数据集进行微调的模型则形成一个较松散的聚类。此外，绕过模型之间的区域会生成性能相当甚至更好的新模型，甚至在进行微调的情况下也是如此。

    Research on neural networks has focused on understanding a single model trained on a single dataset. However, relatively little is known about the relationships between different models, particularly those trained or tested on different datasets. We address this by studying how the weight space and the underlying loss landscape of different models are interconnected.  Specifically, we demonstrate that finetuned models that were optimized for high performance, reside in well-defined regions in weight space, and vice versa -- that any model that resides anywhere in those regions also exhibits high performance. Notably, we show that language models that have been finetuned on the same dataset form a tight cluster in the weight space, while models finetuned on different datasets from the same underlying task form a looser cluster. Moreover, traversing around the region between the models leads to new models that perform comparably or even better than models obtained via finetuning, even on
    
[^174]: 机器学习用于合成数据生成的综述

    Machine Learning for Synthetic Data Generation: A Review. (arXiv:2302.04062v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04062](http://arxiv.org/abs/2302.04062)

    机器学习用于合成数据生成的综述，探讨了合成数据生成的应用（计算机视觉、语音、自然语言、医疗保健和商业）、机器学习方法（神经网络架构和深度生成模型）以及隐私和公平问题，并提出了未来的研究方向。

    

    数据在机器学习中发挥着关键作用。然而，在实际应用中，数据存在多种问题，如数据质量低，有限的数据点导致机器学习模型欠拟合，由于隐私、安全和监管问题难以访问数据。合成数据生成提供了一种有前途的新途径，因为它可以以真实世界数据无法做到的方式进行共享和使用。本文系统地回顾了利用机器学习模型进行合成数据生成的现有工作。具体而言，我们从以下几个方面讨论合成数据生成的工作：（i）应用，包括计算机视觉、语音、自然语言、医疗保健和商业；（ii）机器学习方法，特别是神经网络架构和深度生成模型；（iii）隐私和公平问题。此外，我们还确定了这一新兴领域的挑战和机遇，并提出了未来的研究方向。

    Data plays a crucial role in machine learning. However, in real-world applications, there are several problems with data, e.g., data are of low quality; a limited number of data points lead to under-fitting of the machine learning model; it is hard to access the data due to privacy, safety and regulatory concerns. Synthetic data generation offers a promising new avenue, as it can be shared and used in ways that real-world data cannot. This paper systematically reviews the existing works that leverage machine learning models for synthetic data generation. Specifically, we discuss the synthetic data generation works from several perspectives: (i) applications, including computer vision, speech, natural language, healthcare, and business; (ii) machine learning methods, particularly neural network architectures and deep generative models; (iii) privacy and fairness issue. In addition, we identify the challenges and opportunities in this emerging field and suggest future research directions
    
[^175]: 自监督学习用于帕金森病中多巴胺神经元的分割和计量

    Self-supervised Learning for Segmentation and Quantification of Dopamine Neurons in Parkinson's Disease. (arXiv:2301.08141v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.08141](http://arxiv.org/abs/2301.08141)

    这项研究介绍了一种自监督学习方法，用于数字病理学图像中多巴胺神经元的分割和计量。这种方法可以减少人工操作的主观性和时间消耗，提供可靠和无偏见的自动化系统。

    

    帕金森病是人类中第二常见的神经退行性疾病，其特点是大脑黑质中多巴胺神经元逐渐丧失。在评估帕金森病动物模型的药物疗效时，计数大脑黑质中的多巴胺神经元是最重要的指标之一。目前，通过数字病理学图像分析人工进行多巴胺神经元的分析和计量，这种方法耗时费力且高度主观。因此，需要一种可靠且无偏见的自动化系统来对数字病理学图像中的多巴胺神经元进行计量。近年来，在医学图像处理中普遍采用深度学习解决方案。然而，开发高性能的深度学习模型取决于大规模、高质量的标注数据的可用性，这在数字病理学图像分析等应用中可能成本高昂。为此，我们提出了一种新的自监督学习方法，以克服数据标注的困难并实现对数字病理学图像中多巴胺神经元的准确分割和计量。

    Parkinson's Disease (PD) is the second most common neurodegenerative disease in humans. PD is characterized by the gradual loss of dopaminergic neurons in the Substantia Nigra (SN). Counting the number of dopaminergic neurons in the SN is one of the most important indexes in evaluating drug efficacy in PD animal models. Currently, analyzing and quantifying dopaminergic neurons is conducted manually by experts through analysis of digital pathology images which is laborious, time-consuming, and highly subjective. As such, a reliable and unbiased automated system is demanded for the quantification of dopaminergic neurons in digital pathology images. Recent years have seen a surge in adopting deep learning solutions in medical image processing. However, developing high-performing deep learning models hinges on the availability of large-scale, high-quality annotated data, which can be expensive to acquire, especially in applications like digital pathology image analysis. To this end, we pro
    
[^176]: 一种面向最优时间稳定性的策略优化方法

    A Policy Optimization Method Towards Optimal-time Stability. (arXiv:2301.00521v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2301.00521](http://arxiv.org/abs/2301.00521)

    本文提出了一种面向最优时间稳定性的策略优化方法，将基于采样的李雅普诺夫稳定性与Actor-Critic框架相结合，发展出了自适应李雅普诺夫Actor-Critic（ALAC）算法。通过对十个机器人任务的评估，该方法在引导系统生成稳定模式方面表现优异。

    

    在当前的无模型强化学习算法中，基于采样方法的稳定性标准常用于指导策略优化。然而，这些标准仅保证系统状态无限时间收敛到一个平衡点，从而导致策略的次优性。在本文中，我们提出了一种将基于采样的李雅普诺夫稳定性纳入策略优化的技术。我们的方法使系统状态能够在最优时间内达到平衡点，并保持稳定性，即"最优时间稳定性"。为实现这一目标，我们将优化方法整合到Actor-Critic框架中，从而开发出自适应李雅普诺夫Actor-Critic（ALAC）算法。通过对十个机器人任务进行评估，我们的方法明显优于先前的研究，在引导系统生成稳定模式方面表现出很好的效果。

    In current model-free reinforcement learning (RL) algorithms, stability criteria based on sampling methods are commonly utilized to guide policy optimization. However, these criteria only guarantee the infinite-time convergence of the system's state to an equilibrium point, which leads to sub-optimality of the policy. In this paper, we propose a policy optimization technique incorporating sampling-based Lyapunov stability. Our approach enables the system's state to reach an equilibrium point within an optimal time and maintain stability thereafter, referred to as "optimal-time stability". To achieve this, we integrate the optimization method into the Actor-Critic framework, resulting in the development of the Adaptive Lyapunov-based Actor-Critic (ALAC) algorithm. Through evaluations conducted on ten robotic tasks, our approach outperforms previous studies significantly, effectively guiding the system to generate stable patterns.
    
[^177]: 从引导式游戏学习中提升对抗性模仿学习中的探索能力

    Learning from Guided Play: Improving Exploration for Adversarial Imitation Learning with Simple Auxiliary Tasks. (arXiv:2301.00051v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.00051](http://arxiv.org/abs/2301.00051)

    本研究提出了从引导式游戏学习（LfGP）的框架，通过引入多个辅助任务和主任务的专家演示，提升对抗性模仿学习（AIL）中的探索能力，并解决了传统AIL在学习操作任务时可能陷入次优解的问题。

    

    对抗性模仿学习（AIL）已成为减少监督模仿学习的分布偏移的流行替代方法。然而，在在线强化学习阶段，AIL需要有效的探索。在这项工作中，我们展示了标准的、天真的探索方法，如果使用AIL学习的策略与专家分布足够匹配但没有完全学会所需的任务，可能会表现为一个次优的局部最大值。这对于操作任务来说可能尤为灾难，因为专家和非专家的状态-动作对之间的差别通常是微妙的。我们提出了从引导游戏中学习（LfGP）的框架，其中我们利用了多个探索性辅助任务的专家演示，除了一个主任务。这些辅助任务的添加强制代理人探索标准AIL可能学会忽视的状态和动作。此外，这种特定的公式允许辅助任务的可重复使用性

    Adversarial imitation learning (AIL) has become a popular alternative to supervised imitation learning that reduces the distribution shift suffered by the latter. However, AIL requires effective exploration during an online reinforcement learning phase. In this work, we show that the standard, naive approach to exploration can manifest as a suboptimal local maximum if a policy learned with AIL sufficiently matches the expert distribution without fully learning the desired task. This can be particularly catastrophic for manipulation tasks, where the difference between an expert and a non-expert state-action pair is often subtle. We present Learning from Guided Play (LfGP), a framework in which we leverage expert demonstrations of multiple exploratory, auxiliary tasks in addition to a main task. The addition of these auxiliary tasks forces the agent to explore states and actions that standard AIL may learn to ignore. Additionally, this particular formulation allows for the reusability of
    
[^178]: 通过合并语言模型的权重实现无数据知识融合

    Dataless Knowledge Fusion by Merging Weights of Language Models. (arXiv:2212.09849v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09849](http://arxiv.org/abs/2212.09849)

    本文提出了一种无数据知识融合方法，可以合并在不同训练数据集上建立的单个模型，以得到一个在所有数据集领域上表现良好且可以推广到域外数据的单一模型。

    

    微调预训练语言模型已成为构建下游NLP模型的流行范式。通常情况下，经过微调的模型已经可用，但其训练数据不可用，由于数据隐私或知识产权问题。这就造成了跨模型融合知识以产生更好的单一模型的障碍。在本文中，我们研究了建立在不同训练数据集上的单个模型之间合并的问题，以得到一个在所有数据集领域上表现良好且可以推广到域外数据的单一模型。我们提出了一种无数据知识融合方法，该方法在参数空间中合并模型，由权重引导，以最小化合并模型和单个模型之间的预测差异。在一系列评估设置中，我们展示了该方法显著优于如Fisher加权平均或模型集成等基线。此外，我们发现我们的方法是一个有前途的多语言微调替代方案，因为它可以在不需要任何额外注释数据的情况下实现可比的性能。

    Fine-tuning pre-trained language models has become the prevalent paradigm for building downstream NLP models. Oftentimes fine-tuned models are readily available but their training data is not, due to data privacy or intellectual property concerns. This creates a barrier to fusing knowledge across individual models to yield a better single model. In this paper, we study the problem of merging individual models built on different training data sets to obtain a single model that performs well both across all data set domains and can generalize on out-of-domain data. We propose a dataless knowledge fusion method that merges models in their parameter space, guided by weights that minimize prediction differences between the merged model and the individual models. Over a battery of evaluation settings, we show that the proposed method significantly outperforms baselines such as Fisher-weighted averaging or model ensembling. Further, we find that our method is a promising alternative to multi-
    
[^179]: 通过深度感知实现手持灵巧操作

    Visual Dexterity: In-hand Dexterous Manipulation from Depth. (arXiv:2211.11744v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2211.11744](http://arxiv.org/abs/2211.11744)

    通过使用深度相机的读数，我们提出了一种通用物体重新定向控制器，可以实时、动态地重新定向复杂和新颖的物体形状，中位数重新定向时间接近于七秒。该控制器经过强化学习在仿真环境中训练，并在实际世界中对未用于训练的新物体形状进行了评估。

    

    手持物体的重新定向对于执行许多灵巧操作任务非常必要，例如在当前机器人无法触及的结构不太完善的环境中使用工具。之前的研究建立了重新定向系统，假设以下情况之一或多种情况同时存在：仅重新定向具有简单形状的特定物体、重新定向范围有限、慢速或准静态操作、仅模拟结果、需要专用且昂贵的传感器套件以及其他不适用于实际部署的限制。我们提出了一种不做这些假设的通用物体重新定向控制器。它使用来自单个普通深度摄像机的读数，以实时方式通过任意旋转动态重新定向复杂且新颖的物体形状，中位数重新定向时间接近于七秒。该控制器经过强化学习在仿真环境中进行训练，并在未用于训练的新物体形状上在实际世界中进行评估，包括 ...

    In-hand object reorientation is necessary for performing many dexterous manipulation tasks, such as tool use in less structured environments that remain beyond the reach of current robots. Prior works built reorientation systems assuming one or many of the following: reorienting only specific objects with simple shapes, limited range of reorientation, slow or quasistatic manipulation, simulation-only results, the need for specialized and costly sensor suites, and other constraints which make the system infeasible for real-world deployment. We present a general object reorientation controller that does not make these assumptions. It uses readings from a single commodity depth camera to dynamically reorient complex and new object shapes by any rotation in real-time, with the median reorientation time being close to seven seconds. The controller is trained using reinforcement learning in simulation and evaluated in the real world on new object shapes not used for training, including the m
    
[^180]: ViNL：通过视觉导航和足球术行走避免障碍物

    ViNL: Visual Navigation and Locomotion Over Obstacles. (arXiv:2210.14791v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.14791](http://arxiv.org/abs/2210.14791)

    ViNL是通过视觉导航和足球术在未知室内环境中实现机器人导航和足球术运动的方法。它包括无模型的视觉导航策略和视觉运动策略，通过端到端训练实现，并能够避免踩到小障碍物。ViNL能够实现高效且稳定的导航和足球术运动，无需环境先验知识。

    

    我们提出了一种名为ViNL（Visual Navigation and Locomotion）的方法，它使四足机器人能够在路径上跨过小障碍物（例如鞋子、玩具、电缆），类似于人和宠物在行走时抬起脚步超过物体。ViNL包括：（1）一个视觉导航策略，输出线性和角速度命令，指导机器人在陌生的室内环境中达到目标坐标；（2）一个视觉运动策略，通过控制机器人的关节，避免踩到障碍物，并按照提供的速度命令移动。这两个策略都是完全“无模型”的，即通过端到端训练的传感器到行动神经网络进行训练。这两者在两个完全不同的模拟器中独立训练，然后通过将导航器的速度命令输入到定位器中协同部署，完全“零训练”。虽然之前的研究已经开发出了用于视觉导航的学习方法，但通常需要模型先验和/或人工特征工程。ViNL与这些方法不同，能够在没有环境先验知识的情况下，在未知的室内环境中实现高效且稳定的导航和足球术运动。

    We present Visual Navigation and Locomotion over obstacles (ViNL), which enables a quadrupedal robot to navigate unseen apartments while stepping over small obstacles that lie in its path (e.g., shoes, toys, cables), similar to how humans and pets lift their feet over objects as they walk. ViNL consists of: (1) a visual navigation policy that outputs linear and angular velocity commands that guides the robot to a goal coordinate in unfamiliar indoor environments; and (2) a visual locomotion policy that controls the robot's joints to avoid stepping on obstacles while following provided velocity commands. Both the policies are entirely "model-free", i.e. sensors-to-actions neural networks trained end-to-end. The two are trained independently in two entirely different simulators and then seamlessly co-deployed by feeding the velocity commands from the navigator to the locomotor, entirely "zero-shot" (without any co-training). While prior works have developed learning methods for visual na
    
[^181]: 图像和视频的全景分割的通用框架

    A Generalist Framework for Panoptic Segmentation of Images and Videos. (arXiv:2210.06366v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.06366](http://arxiv.org/abs/2210.06366)

    这个论文提出了一个通用框架，用于图像和视频的全景分割。他们将全景分割问题定义为离散数据生成问题，并提出了一个简单的扩散模型来建模全景掩码。他们的方法能够在流式设置中建模视频，并自动学习跟踪对象实例，并在实验中展现出与最先进的专家方法竞争的能力。

    

    全景分割为图像的每个像素分配语义和实例ID标签。由于实例ID的排列也是有效的解决方案，该任务需要学习高维度的一对多映射。因此，最先进的方法使用定制的架构和任务特定的损失函数。我们将全景分割问题定义为离散数据生成问题，不依赖任务的归纳偏差。我们提出了一个扩散模型来建模全景掩码，具有简单的架构和通用的损失函数。通过将过去的预测作为条件信号添加，我们的方法能够在流式设置中建模视频，并自动学习跟踪对象实例。通过大量实验证明，我们的简单方法在类似的设置下能够与最先进的专家方法竞争。

    Panoptic segmentation assigns semantic and instance ID labels to every pixel of an image. As permutations of instance IDs are also valid solutions, the task requires learning of high-dimensional one-to-many mapping. As a result, state-of-the-art approaches use customized architectures and task-specific loss functions. We formulate panoptic segmentation as a discrete data generation problem, without relying on inductive bias of the task. A diffusion model is proposed to model panoptic masks, with a simple architecture and generic loss function. By simply adding past predictions as a conditioning signal, our method is capable of modeling video (in a streaming setting) and thereby learns to track object instances automatically. With extensive experiments, we demonstrate that our simple approach can perform competitively to state-of-the-art specialist methods in similar settings.
    
[^182]: 表达对数精度变换器的逻辑

    A Logic for Expressing Log-Precision Transformers. (arXiv:2210.02671v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.02671](http://arxiv.org/abs/2210.02671)

    本研究分析了一种对数精度transformer，证明了任何对数精度transformer都可以等效地表示为一阶逻辑句子，扩展了对transformer语言模型的解释。

    

    解释基于transformer的语言模型推理能力的一种方法是描述它们可以在某些输入文本上解决的逻辑规则类型。我们分析了一种在长度为n的语境上计算前向传递的对数精度transformer，证明了任何对数精度transformer都可以等效地表示为一阶逻辑句子，而不仅仅是标准的全称和存在量词。

    One way to interpret the reasoning power of transformer-based language models is to describe the types of logical rules they can resolve over some input text. Recently, Chiang et al. (2023) showed that finite-precision transformers can be equivalently expressed in a generalization of first-order logic. However, finite-precision transformers are a weak transformer variant because, as we show, a single head can only attend to a constant number of tokens and, in particular, cannot represent uniform attention. Since attending broadly is a core capability for transformers, we ask whether a minimally more expressive model that can attend universally can also be characterized in logic. To this end, we analyze transformers whose forward pass is computed in $\log n$ precision on contexts of length $n$. We prove that any log-precision transformer can be equivalently expressed as a first-order logic sentence that, in addition to standard universal and existential quantifiers, may also contain maj
    
[^183]: SpeedLimit：量化Transformer模型的神经架构搜索

    SpeedLimit: Neural Architecture Search for Quantized Transformer Models. (arXiv:2209.12127v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.12127](http://arxiv.org/abs/2209.12127)

    本文介绍了SpeedLimit——一种新的神经架构搜索技术，通过在量化的Transformer模型中添加上限延迟约束，优化准确性。该方法比当前最先进的技术表现更好，为在延迟敏感的环境中使用Transformer模型提供了新的可能性。

    

    尽管Transformer模型的研究主要集中在提高诸如准确性和复杂度这样的性能指标上，但实际应用通常需要严格考虑推理延迟约束。为了解决这个挑战，我们引入了SpeedLimit，一种新的神经架构搜索技术，它在保持上限延迟约束的前提下优化准确性。我们的方法在搜索过程中结合了8位整数量化，超过了当前最先进的技术。我们的结果强调了在性能和延迟之间寻求最佳平衡的可行性和有效性，为在延迟敏感的环境中部署最先进的Transformer模型提供了新的方法。

    While research in the field of transformer models has primarily focused on enhancing performance metrics such as accuracy and perplexity, practical applications in industry often necessitate a rigorous consideration of inference latency constraints. Addressing this challenge, we introduce SpeedLimit, a novel Neural Architecture Search (NAS) technique that optimizes accuracy whilst adhering to an upper-bound latency constraint. Our method incorporates 8-bit integer quantization in the search process to outperform the current state-of-the-art technique. Our results underline the feasibility and efficacy of seeking an optimal balance between performance and latency, providing new avenues for deploying state-of-the-art transformer models in latency-sensitive environments.
    
[^184]: DataPerf：数据中心人工智能开发的基准测试

    DataPerf: Benchmarks for Data-Centric AI Development. (arXiv:2207.10062v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.10062](http://arxiv.org/abs/2207.10062)

    DataPerf是一个由社区主导的基准测试套件，旨在通过竞争、可比性和可重复性促进数据中心人工智能的创新。

    

    长期以来，机器学习研究一直注重模型而不是数据集，并且著名数据集被用于常见的机器学习任务，而忽视了底层问题的广度、难度和准确性。忽视数据的重要性导致了现实应用中的不准确性、偏见和脆弱性，并且现有的数据集基准测试已经达到了饱和状态，阻碍了研究的进展。为此，我们提出了DataPerf，这是一个由社区主导的评估机器学习数据集和数据中心算法的基准测试套件。我们旨在通过竞争、可比性和可重复性促进数据中心人工智能的创新。我们使机器学习社区能够迭代数据集，而不仅仅是架构，并提供一个开放的在线平台，以支持这种迭代开发。DataPerf的第一个迭代包含了五个基准测试，涵盖了视觉、语音、获取等多种数据中心技术、任务和模态。

    Machine learning research has long focused on models rather than datasets, and prominent datasets are used for common ML tasks without regard to the breadth, difficulty, and faithfulness of the underlying problems. Neglecting the fundamental importance of data has given rise to inaccuracy, bias, and fragility in real-world applications, and research is hindered by saturation across existing dataset benchmarks. In response, we present DataPerf, a community-led benchmark suite for evaluating ML datasets and data-centric algorithms. We aim to foster innovation in data-centric AI through competition, comparability, and reproducibility. We enable the ML community to iterate on datasets, instead of just architectures, and we provide an open, online platform with multiple rounds of challenges to support this iterative development. The first iteration of DataPerf contains five benchmarks covering a wide spectrum of data-centric techniques, tasks, and modalities in vision, speech, acquisition, 
    
[^185]: 学习反事实不变的预测器

    Learning Counterfactually Invariant Predictors. (arXiv:2207.09768v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.09768](http://arxiv.org/abs/2207.09768)

    通过提出图形标准和模型无关框架CIP，我们能够学习反事实不变的预测器，以实现在现实世界中的公平性、强健性和普适性。

    

    反事实不变性（CI）的概念对于在现实世界中公平、强健和具有普适性的预测器至关重要。我们提出了一种图形标准，它以观测分布的条件独立性作为预测器反事实不变的充分条件。为了学习这样的预测器，我们提出了一个称为Counterfactually Invariant Prediction（CIP）的模型无关框架，基于Hilbert-Schmidt条件独立准则（HSCIC），一种基于核的条件依赖度量。我们的实验结果在包括标量和多变量设置在内的各种模拟和真实世界数据集上证明了CIP在强制反事实不变性方面的有效性。

    Notions of counterfactual invariance (CI) have proven essential for predictors that are fair, robust, and generalizable in the real world. We propose graphical criteria that yield a sufficient condition for a predictor to be counterfactually invariant in terms of a conditional independence in the observational distribution. In order to learn such predictors, we propose a model-agnostic framework, called Counterfactually Invariant Prediction (CIP), building on the Hilbert-Schmidt Conditional Independence Criterion (HSCIC), a kernel-based conditional dependence measure. Our experimental results demonstrate the effectiveness of CIP in enforcing counterfactual invariance across various simulated and real-world datasets including scalar and multi-variate settings.
    
[^186]: 隐性参数的递归状态空间模型用于变化动态场景研究

    Hidden Parameter Recurrent State Space Models For Changing Dynamics Scenarios. (arXiv:2206.14697v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.14697](http://arxiv.org/abs/2206.14697)

    本论文提出了隐性参数的递归状态空间模型(HiP-RSSMs)，用于建模在真实世界中常见但动力学不完全相同的任务。该模型通过将一系列相关动力系统参数化为低维潜在因子，实现了简单而有效的学习和推断方法。实验证明，HiP-RSSMs在多个机器人基准测试中表现出色，比RSSMs和其他多任务模型更具优势。

    

    递归状态空间模型(RSSMs)是一种高度表达性的模型，用于学习时间序列数据和系统识别中的模式。然而，这些模型假设动力学是固定且不变的，而这在真实世界的场景中很少见。许多控制应用通常展示出具有类似但非完全相同的动力学的任务，可以将其建模为潜在变量。我们引入了隐性参数的递归状态空间模型(HiP-RSSMs)，这是一种将一系列相关动力系统参数化为低维潜在因子的框架。我们提出了一种简单有效的学习和推断高斯图模型的方法，避免了类似变分推断的近似方法。我们展示了HiP-RSSMs在多个具有挑战性的机器人基准测试中，无论是在真实系统还是模拟中，都优于RSSMs和竞争的多任务模型。

    Recurrent State-space models (RSSMs) are highly expressive models for learning patterns in time series data and system identification. However, these models assume that the dynamics are fixed and unchanging, which is rarely the case in real-world scenarios. Many control applications often exhibit tasks with similar but not identical dynamics which can be modeled as a latent variable. We introduce the Hidden Parameter Recurrent State Space Models (HiP-RSSMs), a framework that parametrizes a family of related dynamical systems with a low-dimensional set of latent factors. We present a simple and effective way of learning and performing inference over this Gaussian graphical model that avoids approximations like variational inference. We show that HiP-RSSMs outperforms RSSMs and competing multi-task models on several challenging robotic benchmarks both on real-world systems and simulations.
    
[^187]: ZSON: 使用多模式目标嵌入进行零样本目标导航

    ZSON: Zero-Shot Object-Goal Navigation using Multimodal Goal Embeddings. (arXiv:2206.12403v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.12403](http://arxiv.org/abs/2206.12403)

    本文提出了一种使用多模式目标嵌入进行零样本目标导航的方法，通过在未标注的3D环境中训练语义目标导航代理，将目标图片编码成多模式的语义嵌入，实现了在开放世界中找到物体的能力。

    

    我们提出了一种可扩展的方法来学习开放世界的物体目标导航(ObjectNav) -- 即让虚拟机器人(智能体)在未探索的环境中找到任何一个物体实例(例如，“找到一个水槽”的任务)。我们的方法完全是零样本的 -- 即不需要ObjectNav的奖励或任何形式的示范。相反，我们在图像目标导航(ImageNav)任务上进行训练，即使代理找到一个图片(即目标图片)被拍摄的位置。具体地说，我们将目标图片编码到一个多模式的语义嵌入空间中，以实现在未标注的3D环境(例如HM3D)中训练语义目标导航(SemanticNav)代理的规模扩展。训练后，可以指示SemanticNav代理根据自由形式的自然语言(例如，“水槽”，“浴室水槽”等)来查找物体，通过将语言目标映射到相同的多模式语义嵌入空间。因此，我们的方法实现了开放世界的ObjectNav。我们对代理进行了广泛的评估。

    We present a scalable approach for learning open-world object-goal navigation (ObjectNav) -- the task of asking a virtual robot (agent) to find any instance of an object in an unexplored environment (e.g., "find a sink"). Our approach is entirely zero-shot -- i.e., it does not require ObjectNav rewards or demonstrations of any kind. Instead, we train on the image-goal navigation (ImageNav) task, in which agents find the location where a picture (i.e., goal image) was captured. Specifically, we encode goal images into a multimodal, semantic embedding space to enable training semantic-goal navigation (SemanticNav) agents at scale in unannotated 3D environments (e.g., HM3D). After training, SemanticNav agents can be instructed to find objects described in free-form natural language (e.g., "sink", "bathroom sink", etc.) by projecting language goals into the same multimodal, semantic embedding space. As a result, our approach enables open-world ObjectNav. We extensively evaluate our agents 
    
[^188]: 结构化预测问题存档

    Structured Prediction Problem Archive. (arXiv:2202.03574v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.03574](http://arxiv.org/abs/2202.03574)

    该论文介绍了一个结构化预测问题的存档，集中收集了各种问题类别的数据集，并提供对问题的描述、格式和特性进行总结，以及列举了相关算法。该存档旨在方便进行基准测试和与已有工作的比较，并欢迎提交新的数据集和算法。

    

    结构化预测问题是机器学习中的基本工具之一。为了便于算法开发，我们在一个地方收集了大量易于阅读的数据集，涵盖了各种问题类别的问题。我们提供数据集的存档链接，问题描述和问题格式，以及问题特性的简要总结，包括大小，实例数量等。为了参考，我们还列举了文献中提出的一些算法，用于解决这些问题。我们希望这个中央存储库能够更容易地进行基准测试和与已有工作的比较。我们欢迎提交有趣的新数据集和算法，以便将其纳入我们的存档。

    Structured prediction problems are one of the fundamental tools in machine learning. In order to facilitate algorithm development for their numerical solution, we collect in one place a large number of datasets in easy to read formats for a diverse set of problem classes. We provide archival links to datasets, description of the considered problems and problem formats, and a short summary of problem characteristics including size, number of instances etc. For reference we also give a non-exhaustive selection of algorithms proposed in the literature for their solution. We hope that this central repository will make benchmarking and comparison to established works easier. We welcome submission of interesting new datasets and algorithms for inclusion in our archive.
    
[^189]: DIT4BEARs智能道路实习（arXiv：2107.06755v2 [cs.LG]更新）

    DIT4BEARs Smart Roads Internship. (arXiv:2107.06755v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.06755](http://arxiv.org/abs/2107.06755)

    本研究实习项目旨在通过开发深度学习模型来改善北极地区的道路维护。实习团队成功地开发出一个天气预报应用程序，该应用程序可以准确预测道路的状态，从而提高道路安全性。

    

    这项研究实习是由挪威北极大学（UiT）为我们团队提供的，我们是“智能道路-冬季道路维护2021”黑客马拉松的获胜者。实习从2021年5月3日开始，于2021年5月21日结束，每周进行两次会议。尽管我们实习生有不同的国籍和教育背景，但我们尽可能地作为一个团队合作。最吸引人的部分是我们意识到北极人民面临的严峻条件，从我们目前居住的地方很难获得这样独特的经验。我们开发和实施了几个深度学习模型来对状态（干燥、湿润、潮湿、结冰、积雪、泥泞）进行分类。根据最佳模型，天气预报应用程序将考虑Ta、Tsurf、Height、Speed、Water等因素来预测状态。关键部分是定义一个安全度量标准，该标准是基于摩擦力和事故率的乘积。

    The research internship at UiT - The Arctic University of Norway was offered for our team being the winner of the 'Smart Roads - Winter Road Maintenance 2021' Hackathon. The internship commenced on 3 May 2021 and ended on 21 May 2021 with meetings happening twice each week. In spite of having different nationalities and educational backgrounds, we both interns tried to collaborate as a team as much as possible. The most alluring part was working on this project made us realize the critical conditions faced by the arctic people, where it was hard to gain such a unique experience from our residence. We developed and implemented several deep learning models to classify the states (dry, moist, wet, icy, snowy, slushy). Depending upon the best model, the weather forecast app will predict the state taking the Ta, Tsurf, Height, Speed, Water, etc. into consideration. The crucial part was to define a safety metric which is the product of the accident rates based on friction and the accident ra
    
[^190]: 重审超参数模型中的最小描述长度复杂度

    Revisiting minimum description length complexity in overparameterized models. (arXiv:2006.10189v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2006.10189](http://arxiv.org/abs/2006.10189)

    本文重审了超参数模型中的最小描述长度复杂度。通过定义一个新的基于MDL的复杂度度量，我们发现复杂度不仅取决于参数数量，还与设计矩阵或核矩阵的奇异值和信噪比有关。

    

    复杂度是统计学习理论中的一个基本概念，旨在提供有关泛化性能的信息。在低维度情况下，参数数量在一定程度上是成功的，但在超参数模型中，当参数数量超过训练样本数量时，其合理性不足。我们重新审视了基于Rissanen最小描述长度（MDL）原理的复杂度度量，并定义了一种新的适用于超参数模型的基于MDL的复杂度（MDL-COMP）。MDL-COMP通过对一个良好的Ridge估计类所引起的编码而定义出来的最优性准则。我们对线性模型和核方法的MDL-COMP进行了广泛的理论刻画，并表明它不仅是参数数量的函数，而是设计或核矩阵的奇异值和信噪比的函数。对于具有n个观测值，d个参数和独立同分布的高斯预测因子的线性模型，MDL-COMP的尺度是线性的。

    Complexity is a fundamental concept underlying statistical learning theory that aims to inform generalization performance. Parameter count, while successful in low-dimensional settings, is not well-justified for overparameterized settings when the number of parameters is more than the number of training samples. We revisit complexity measures based on Rissanen's principle of minimum description length (MDL) and define a novel MDL-based complexity (MDL-COMP) that remains valid for overparameterized models. MDL-COMP is defined via an optimality criterion over the encodings induced by a good Ridge estimator class. We provide an extensive theoretical characterization of MDL-COMP for linear models and kernel methods and show that it is not just a function of parameter count, but rather a function of the singular values of the design or the kernel matrix and the signal-to-noise ratio. For a linear model with $n$ observations, $d$ parameters, and i.i.d. Gaussian predictors, MDL-COMP scales li
    

