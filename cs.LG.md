# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [SECAD-Net: Self-Supervised CAD Reconstruction by Learning Sketch-Extrude Operations.](http://arxiv.org/abs/2303.10613) | 本论文提出了一种名为SECAD-Net的自我监督神经网络，旨在以紧凑且易于编辑的方式从原始输入中重建高质量的CAD模型。其利用学习2D草图和3D挤压参数以创建一组挤压筒，并结合布尔操作来逼近目标几何形状。使用隐式场表示草图，允许通过在草图潜在空间中插值潜在代码来创建CAD变体。在实验中，SECAD-Net超越了现有的最先进方法，证明了其在重建质量和推断速度方面的卓越表现。 |
| [^2] | [Bangla Grammatical Error Detection Using T5 Transformer Model.](http://arxiv.org/abs/2303.10612) | 本文使用T5 Transformer模型成功地在孟加拉语中检测语法错误，并对模型检测到的错误进行了详细分析，同时探讨了将翻译模型适应语法检测任务的挑战。 |
| [^3] | [A model is worth tens of thousands of examples.](http://arxiv.org/abs/2303.10608) | 研究了神经网络与传统基于数学数据生成模型的信号处理方法的数据需求差异，并在两个简单的问题上得到了实证结果。 |
| [^4] | [Automatic pain recognition from Blood Volume Pulse (BVP) signal using machine learning techniques.](http://arxiv.org/abs/2303.10607) | 本研究利用机器学习技术和血容积脉搏信号设计了一种非侵入式的疼痛感知方式，并建立了模型来检测疼痛的存在和强度。 |
| [^5] | [AdaptGuard: Defending Against Universal Attacks for Model Adaptation.](http://arxiv.org/abs/2303.10594) | 本文研究了模型适应中存在的通用攻击，并提出了一个名为AdaptGuard的模型预处理框架，通过知识蒸馏和伪对抗样本加强目标模型的鲁棒性，提高模型适应算法的安全性。 |
| [^6] | [Hierarchical Personalized Federated Learning Over Massive Mobile Edge Computing Networks.](http://arxiv.org/abs/2303.10580) | 本篇论文提出了一种分层个性化联邦学习算法，能够在巨型移动边缘计算网络中处理移动用户设备的异构性，并且实现了训练损失最小化和轮延迟降低的目标权衡。 |
| [^7] | [Efficiently Counting Substructures by Subgraph GNNs without Running GNN on Subgraphs.](http://arxiv.org/abs/2303.10576) | 本文提出了一种使用结构嵌入和预计算的方法，以减少计算和内存成本，并实现了在子图 GNN 上高效计数子结构的目的。 |
| [^8] | [Instance-dependent Sample Complexity Bounds for Zero-sum Matrix Games.](http://arxiv.org/abs/2303.10565) | 本文提出了一种实例相关的界限，定义了一个游戏矩阵排序，可以捕捉到某些游戏动态比其他游戏更快收敛的直觉，从而限定了玩家达到近似均衡所需的最小回合数。 |
| [^9] | [Revisiting LiDAR Spoofing Attack Capabilities against Object Detection: Improvements, Measurement, and New Attack.](http://arxiv.org/abs/2303.10555) | 本文针对 LiDAR 欺骗攻击在目标检测领域存在的研究差距进行了实证研究，使用9种 LiDAR 和3种目标检测器进行了大规模测量，展示了一种新的攻击模式，提出了安全的 LiDAR 设计和评估方法。 |
| [^10] | [Elastic Interaction Energy-Based Generative Model: Approximation in Feature Space.](http://arxiv.org/abs/2303.10553) | 本文提出了一种利用弹性相互作用能的生成模型，采用稳定项解决了GAN训练不稳定的问题，并通过映射数据到特征空间逼近其分布。 |
| [^11] | [Unsupervised Learning for Solving the Travelling Salesman Problem.](http://arxiv.org/abs/2303.10538) | 无监督学习框架UTSP能够对旅行商问题进行求解，它使用图神经网络作为基础模型，在保证路径为哈密顿循环的前提下，能够找到最短路径。相较于其他方法，UTSP在训练样本与参数数量上占用更少的资源，且性能更佳。 |
| [^12] | [LNO: Laplace Neural Operator for Solving Differential Equations.](http://arxiv.org/abs/2303.10528) | LNO是一种用于解微分方程的算法，相比其他算法（如FNO）具有更好的逼近精度并适用于非周期性信号和瞬态响应，能够更好地解释模型并改进泛化能力。 |
| [^13] | [Unsupervised Interpretable Basis Extraction for Concept-Based Visual Explanations.](http://arxiv.org/abs/2303.10523) | 本文提出了一种无监督的方法，通过对CNN进行转换，从而更好地解释中间层的表示，提取了一个可解释性欠完备基础，并证明该方法在各种网络结构和训练数据集上都很有效。 |
| [^14] | [Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning.](http://arxiv.org/abs/2303.10512) | AdaLoRA是一种自适应预算分配方法，用于参数效率微调。将增量更新的预算根据权重矩阵的重要性分数进行自适应分配，通过奇异值分解的形式，实现了微调表现的优化。 |
| [^15] | [A Deep Learning System for Domain-specific speech Recognition.](http://arxiv.org/abs/2303.10510) | 本文提出了一个使用半监督学习注释领域特定数据，基于预训练的声学模型进行微调的ASR系统，并在领域特定上取得了优于商业ASR系统的性能。 |
| [^16] | [Neural Operators of Backstepping Controller and Observer Gain Functions for Reaction-Diffusion PDEs.](http://arxiv.org/abs/2303.10506) | 本文提出了一种使用神经算子学习和逼近设计映射的方法，以消除解决PDE的需要。 |
| [^17] | [Revisiting the Plastic Surgery Hypothesis via Large Language Models.](http://arxiv.org/abs/2303.10494) | 本论文重新审视了自动程序修复中的整形手术假设，并提出使用大型语言模型进行APR的新方法，主要解决了传统APR工具在不同项目中无法产生多样化修补程序的问题。 |
| [^18] | [Practical and Matching Gradient Variance Bounds for Black-Box Variational Bayesian Inference.](http://arxiv.org/abs/2303.10472) | 本文表明黑盒变分推理（BBVI）满足SGD文献中的ABC条件，该结果适用于平滑和二次增长的对数似然函数，同时我们的结果推广到广泛应用于BBVI实践中的非线性协方差参数化。 |
| [^19] | [SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models.](http://arxiv.org/abs/2303.10464) | 本文提出了SPDF算法来实现大规模语言模型的高效训练。通过非结构化权重稀疏性来进行预训练，可以降低计算成本，而密集微调则可以保证高性能的表现。 |
| [^20] | [Machine learning with data assimilation and uncertainty quantification for dynamical systems: a review.](http://arxiv.org/abs/2303.10462) | 本文综述了最新的交叉学科研究进展，涵盖了数据同化、不确定性量化和机器学习技术在高维动态系统中的广泛应用。这篇论文对于希望应用DA和UQ技术的机器学习科学家是一份全面的指南。 |
| [^21] | [Learn, Unlearn and Relearn: An Online Learning Paradigm for Deep Neural Networks.](http://arxiv.org/abs/2303.10455) | 学习、遗忘和重学（LURE）是一种深度神经网络的在线学习范式，它在遗忘阶段和重学阶段之间进行交替，通过有选择地遗忘模型中的不良信息和对泛化性的学习来在效率和泛化之间取得平衡。 |
| [^22] | [A Content Adaptive Learnable Time-Frequency Representation For Audio Signal Processing.](http://arxiv.org/abs/2303.10446) | 该论文提出了一种用于音频信号处理的内容自适应可学习时频表示法，通过学习卷积滤波器与变换器架构来将小的波形块投影到小的潜在维度上。 |
| [^23] | [EarCough: Enabling Continuous Subject Cough Event Detection on Hearables.](http://arxiv.org/abs/2303.10445) | 本文提出了一种名为EarCough的轻量级神经网络模型，可以在Hearables上实现连续主题咳嗽事件检测，准确度高达95.4％，F1分数为92.9％，仅需385 KB的空间。这将成为未来Hearables的低成本增值功能，为咳嗽监测技术提供了新的可能性。 |
| [^24] | [Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images.](http://arxiv.org/abs/2303.10435) | 本文研究了利用低分辨率图像传感器的计算机视觉系统中隐私保护与活动识别之间的权衡，通过对图像分辨率的调整和现代图像超分辨技术的应用，帮助设计更有效的隐私保护计算机视觉系统。 |
| [^25] | [NoisyHate: Benchmarking Content Moderation Machine Learning Models with Human-Written Perturbations Online.](http://arxiv.org/abs/2303.10430) | 本文提出了一个包含人类编写的在线扰动的测试集，用于毒性言论检测模型的评估。 |
| [^26] | [Protein Sequence Design with Batch Bayesian Optimisation.](http://arxiv.org/abs/2303.10429) | 本文提出了一种基于批量贝叶斯优化的蛋白质序列设计新方法，能更明智地决定选择哪些序列进行人工进化，实现更好的性能和更快的收敛。 |
| [^27] | [Discovering Predictable Latent Factors for Time Series Forecasting.](http://arxiv.org/abs/2303.10426) | 本文提出了一种基于推断可观察时间序列所暗示的固有潜在因素来进行信号成分分解并重构未来的时间序列预测算法。该算法不仅能够实现长期效率的稀疏关系推理，而且还能够解决时间序列在缺乏足够变量下的建模不稳定和不可预测的问题。 |
| [^28] | [ExplainFix: Explainable Spatially Fixed Deep Networks.](http://arxiv.org/abs/2303.10408) | ExplainFix采用固定滤波器和精简的网络参数，提高了深度神经网络的可解释性和训练速度。 |
| [^29] | [3DQD: Generalized Deep 3D Shape Prior via Part-Discretized Diffusion Process.](http://arxiv.org/abs/2303.10406) | 该论文提出了一种广义的深度三维形状先验模型，可以用于多种三维任务，其使用向量量化变分自编码器和离散扩散生成器来精确捕获局部精细的形状信息，并引入多频率融合模块来抑制高频形状特征波动，广泛实验表明其在各种三维形状生成任务上表现出优异的性能。 |
| [^30] | [Smart ROI Detection for Alzheimer's disease prediction using explainable AI.](http://arxiv.org/abs/2303.10401) | 本论文提出了一种基于可解释AI和Grad-Cam技术的智能方法，可以自动检测ROI以预测阿尔茨海默病。在176名MCI患者上实现并与最先进的方法进行比较，取得了显著的结果。 |
| [^31] | [Energy-Efficient Cellular-Connected UAV Swarm Control Optimization.](http://arxiv.org/abs/2303.10398) | 本文提出了一种基于蜂窝网络连接的无人机群控制方案，通过两阶段命令与控制（C&C）传输方案和设备对设备（D2D）通信来提高通信可靠性和能源效率。 |
| [^32] | [A Graph-Guided Reasoning Approach for Open-ended Commonsense Question Answering.](http://arxiv.org/abs/2303.10395) | 提出了一种基于图表推理的开放式常识问题回答方法，该方法能够处理常识问题的隐式多跳推理并构建与问题相关的开放式知识图，有望在没有预定义答案选项的真实情境应用中发挥作用。 |
| [^33] | [Powerful and Extensible WFST Framework for RNN-Transducer Losses.](http://arxiv.org/abs/2303.10384) | 本文提出了一个基于WFST框架的的RNN-Transducer Losses强大且可扩展的实现，“Compose-Transducer”和“Grid-Transducer”，并引入了新的W-Transducer Loss来展示组件的易扩展性。在实验中，W-Transducer（W-RNNT）表现出比标准RNN-T更好的性能。 |
| [^34] | [Interpretable Reinforcement Learning via Neural Additive Models for Inventory Management.](http://arxiv.org/abs/2303.10382) | 本文提出了一种基于神经加性模型的可解释强化学习方法，用于开发多级别供应链的动态库存订购策略，在三级供应链仿真测试中证明了实现与最先进深度强化学习方法相当的性能表现，同时具备传统策略的可解释性。 |
| [^35] | [Client Selection for Generalization in Accelerated Federated Learning: A Multi-Armed Bandit Approach.](http://arxiv.org/abs/2303.10373) | 本文提出了一种基于多臂老虎机的加速联邦学习中客户端选择方法，通过使用Thompson采样和同步更新实现更快的收敛速度和良好的泛化性能。 |
| [^36] | [UNREAL:Unlabeled Nodes Retrieval and Labeling for Heavily-imbalanced Node Classification.](http://arxiv.org/abs/2303.10371) | 本文提出了一种用于重度不平衡节点分类的迭代过采样方法UNREAL，通过添加未标记节点而不是合成节点，解决了特征和邻域生成的难题，并利用节点嵌入空间中的无监督学习进行几何排名来有效地校准伪标签分配。 |
| [^37] | [CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label Learning.](http://arxiv.org/abs/2303.10365) | CroSel是一种处理伪标签噪声的新方法，通过利用历史预测信息和一致性正则化项来准确识别部分标签数据的真实标签。 |
| [^38] | [DC-CCL: Device-Cloud Collaborative Controlled Learning for Large Vision Models.](http://arxiv.org/abs/2303.10361) | 提出了一种名为DC-CCL的设备-云协同控制学习框架，使云端无法直接部署在移动设备上的大型视觉模型仍然可以从设备端局部样本中受益。 |
| [^39] | [Neural Frailty Machine: Beyond proportional hazard assumption in neural survival regressions.](http://arxiv.org/abs/2303.10358) | 提出神经衰弱机器（NFM）框架用于生存回归，利用多重衰弱的经典思想来捕捉个体间未观察到的异质性，并能够处理非线性协变量依赖性。两个具体模型下扩展了神经比例危险模型和非参数危险回归模型，结论获得了统计保证。 |
| [^40] | [LossMix: Simplify and Generalize Mixup for Object Detection and Beyond.](http://arxiv.org/abs/2303.10343) | 本论文提出了一种称为 Supervision Interpolation 的新概念框架，通过放松和推广 Mixup 提供了一种全新的插值增强视角，并在此基础上提出了一种名为 LossMix 的简单而多功能的正则化方法，能够增强物体检测器的性能和鲁棒性，或者说LossMix 在目标检测和其他领域中表现出色。 |
| [^41] | [Recognizing Complex Gestures on Minimalistic Knitted Sensors: Toward Real-World Interactive Systems.](http://arxiv.org/abs/2303.10336) | 本文介绍了一种新型的数字编织电容式活动传感器，使用神经网络实现对12种复杂手势类的准确分类，并展示了该系统的鲁棒性和适用性。 |
| [^42] | [Hybrid Systems Neural Control with Region-of-Attraction Planner.](http://arxiv.org/abs/2303.10327) | 本文提出了一种基于神经网络的分层方法来控制混合系统，并利用吸引子区域估计器和规划器来确保系统的稳定性，相比传统方法，该方法训练时间更短，性能相同或更好。 |
| [^43] | [Pseudo Supervised Metrics: Evaluating Unsupervised Image to Image Translation Models In Unsupervised Cross-Domain Classification Frameworks.](http://arxiv.org/abs/2303.10310) | 本文提出了一种新方法——伪监督度量，用于评估无监督图片到图片翻译模型在无监督跨域分类框架中的性能，并在多个基准数据集上进行了实验。 |
| [^44] | [The Challenge of Differentially Private Screening Rules.](http://arxiv.org/abs/2303.10303) | 本文提出了第一个差分隐私筛选规则，同时发现了由于添加到确保隐私性的噪声量，使用有用的私有筛选规则的任务存在困难。 |
| [^45] | [Forecasting COVID-19 Case Counts Based on 2020 Ontario Data.](http://arxiv.org/abs/2303.10294) | 该论文开发了基于机器学习的模型，可以根据过去14天的环境和移动数据预测COVID-19的每日病例数，并在安大略省的四个县进行了测试，最佳模型可以以90.7%的准确率预测明天的每日COVID案例计数，以98.1%的准确率预测7天滚动平均COVID案例计数。 |
| [^46] | [Fixed Design Analysis of Regularization-Based Continual Learning.](http://arxiv.org/abs/2303.10263) | 该论文分析了固定设计下的正则化连续学习问题，提出算法在遗忘和不妥协之间存在权衡，可能会存在灾难性遗忘问题。 |
| [^47] | [Recent Developments in Machine Learning Methods for Stochastic Control and Games.](http://arxiv.org/abs/2303.10257) | 本文回顾了基于机器学习的随机控制问题和博弈的计算方法，特别是着重介绍了使用深度学习算法解决高维度和非常复杂结构情况下问题的新方法。 |
| [^48] | [Solving Differential-Algebraic Equations in Power Systems Dynamics with Neural Networks and Spatial Decomposition.](http://arxiv.org/abs/2303.10256) | 本文提出了一种使用神经网络和空间分解来近似电力系统动力学微分代数方程的方法，旨在加速仿真，提高数值稳定性和精度。 |
| [^49] | [Multi-Task Model Personalization for Federated Supervised SVM in Heterogeneous Networks.](http://arxiv.org/abs/2303.10254) | 本文提出了一种用于异构联邦网络中的高效分布式迭代学习方法，通过支持向量机实现对联邦分类和回归任务的处理，并支持个性化的学习模型。为了保护隐私，引入了一种随机掩码过程。研究结果表明，所提出的方法对于解决异构网络中联邦学习任务是有效的。 |
| [^50] | [Conformal Generative Modeling on Triangulated Surfaces.](http://arxiv.org/abs/2303.10251) | 提出了一种在三角面上进行生成模型的内在对称性生成模型，使用离散共形几何的进展将面网格映射到球形，可以通过任何已经开发为简单流形的生成模型方法进行训练。演示了这种方法在多个流形和多个生成模型子例程上的有效性。 |
| [^51] | [LSwinSR: UAV Imagery Super-Resolution based on Linear Swin Transformer.](http://arxiv.org/abs/2303.10232) | 本文提出一种新型LSwinSR超分辨率算法，基于最先进的Swin Transformer，应用于UAV图像超分辨率处理，同时引入更全面的评估指标F1得分，实验结果证明其比其他最先进算法更具优越性能。 |
| [^52] | [Robust Mode Connectivity-Oriented Adversarial Defense: Enhancing Neural Network Robustness Against Diversified $\ell_p$ Attacks.](http://arxiv.org/abs/2303.10225) | 本文提出一种新颖的鲁棒模态连接导向的对抗性防御，实现神经网络对多样化$\ell_p$攻击的鲁棒性，其中包括两个基于种群学习的学习阶段。 |
| [^53] | [CerviFormer: A Pap-smear based cervical cancer classification method using cross attention and latent transformer.](http://arxiv.org/abs/2303.10222) | 本文提出了一种基于交叉注意力和Transfomer的方法，可靠地将PAP检查图像分类为颈癌。对两个公开可用的PAP检查数据集上的分类效果分别为93.70％和97.15％。 |
| [^54] | [An Empirical Evaluation of Federated Contextual Bandit Algorithms.](http://arxiv.org/abs/2303.10218) | 本文通过联邦情境赌博算法对隐式信号进行学习，避免了访问难以获得的显式标签，并发现了简单而常用的softmax启发式在平衡算法方面的惊人有效性。 |
| [^55] | [Approximation of group explainers with coalition structure using Monte Carlo sampling on the product space of coalitions and features.](http://arxiv.org/abs/2303.10216) | 本文研究了使用蒙特卡罗采样算法逼近联盟结构群体解释器的复杂问题，提出一种快速、易于实现且与特定模型无关的新型算法，并提供了严密的统计分析及误差界的证明。 |
| [^56] | [Exploring contrast generalisation in deep learning-based brain MRI-to-CT synthesis.](http://arxiv.org/abs/2303.10202) | 本文研究了领域随机化（DR）来提高DL模型在生成基于MRI的脑部sCT的泛化能力。 |
| [^57] | [Unsupervised Domain Transfer with Conditional Invertible Neural Networks.](http://arxiv.org/abs/2303.10191) | 本研究提出了一种基于条件可逆神经网络的无监督域转移方法，通过可逆架构保证循环一致性，避免了模式崩溃问题，在高光谱成像和光声成像方面表现优于最先进的方法。 |
| [^58] | [A machine learning and feature engineering approach for the prediction of the uncontrolled re-entry of space objects.](http://arxiv.org/abs/2303.10183) | 本论文利用深度学习模型对低轨道下未经控制的太空物体进行再入预测，避免了物理建模误差的影响，具有较高的预测精度。 |
| [^59] | [SFE: A Simple, Fast and Efficient Feature Selection Algorithm for High-Dimensional Data.](http://arxiv.org/abs/2303.10182) | 提出一种针对高维数据集的特征选择算法SFE，由搜索代理和两个算子执行探索和开发阶段，对于高维度数据集表现良好，但在降维后性能不能提高 |
| [^60] | [Operating critical machine learning models in resource constrained regimes.](http://arxiv.org/abs/2303.10181) | 本文分享了在关键场景下使用机器学习模型时资源消耗和性能之间的权衡方法。考虑到模型在全球诊所中的部署，机器学习界正在为改进模型效率而努力。 |
| [^61] | [Towards Safe Propofol Dosing during General Anesthesia Using Deep Offline Reinforcement Learning.](http://arxiv.org/abs/2303.10180) | 本文提出了一种基于真实临床数据集的数据驱动强化学习算法Policy Constraint Q-Learning(PCQL)来实现全麻药物剂量控制，添加了保守Q-Learning方法和策略约束项以确保智能体做出更安全的决策。 |
| [^62] | [QUBO-inspired Molecular Fingerprint for Chemical Property Prediction.](http://arxiv.org/abs/2303.10179) | 本研究提出了一种基于QUBO的分子指纹，使用二次无约束二进制优化寻找更有效的交互指纹，并在QM9数据集上取得了成功。 |
| [^63] | [Deep Nonparametric Estimation of Intrinsic Data Structures by Chart Autoencoders: Generalization Error and Robustness.](http://arxiv.org/abs/2303.09863) | 本文介绍了一种图表自编码器用于深度非参数估计内部数据结构，并证明了其广义误差保证和去噪能力。 |
| [^64] | [SemDeDup: Data-efficient learning at web-scale through semantic deduplication.](http://arxiv.org/abs/2303.09540) | SemDeDup是一种利用预训练模型的嵌入来识别和删除语义重复项的方法。通过对LAION的子集进行分析，SemDeDup可以最小化性能损失的同时删除50%的数据，实际上将训练时间减半。此外，SemDeDup在提供效率收益的同时改进了先前的方法。 |
| [^65] | [Enhanced detection of the presence and severity of COVID-19 from CT scans using lung segmentation.](http://arxiv.org/abs/2303.09440) | 本论文介绍了一个深度学习模型，该模型利用肺部分割进行预处理，其验证 F1 分数在预测 CT 扫描中 COVID-19 的存在和严重程度方面显著超过基线水平。 |
| [^66] | [Generative Adversarial Network for Personalized Art Therapy in Melanoma Disease Management.](http://arxiv.org/abs/2303.09232) | 该论文提出了一种基于生成对抗网络的黑色素瘤个性化艺术治疗方法，该方法快速生成出个性化的艺术治疗内容，有效减轻患者心理压力。 |
| [^67] | [Watch or Listen: Robust Audio-Visual Speech Recognition with Visual Corruption Modeling and Reliability Scoring.](http://arxiv.org/abs/2303.08536) | 本论文研究了音视频语音识别在多模态输入损坏情况下的问题，并设计了音视频可靠性评分模块来提高模型的韧性。 |
| [^68] | [R^2: Range Regularization for Model Compression and Quantization.](http://arxiv.org/abs/2303.08253) | R^2提出了一种基于区间正则化的新方法，利用有效的最小值和最大值调整权重分布，从而使模型压缩和量化技术能够更好地利用其数值表示能力。该方法可以提高模型优化的质量，尤其是在较低位上。 |
| [^69] | [DBSCAN of Multi-Slice Clustering for three-order Tensor.](http://arxiv.org/abs/2303.07768) | 本文提出了 MSC-DBSCAN扩展算法，可以在三元聚类中从数据中提取不同子空间的不同切片聚类，并可以获得与 MSC 算法在处理秩一张量数据时相同的解决方案。 |
| [^70] | [Self-Supervised One-Shot Learning for Automatic Segmentation of StyleGAN Images.](http://arxiv.org/abs/2303.05639) | 本文提出了一种基于Self-Supervised对比聚类算法的自动分割方法，该方法可以利用StyleGAN生成器中的多尺度隐藏特征对图像进行快速分割，优于半监督基准方法的平均wIoU值1.02%，并提高了推理速度达4.5倍，同时提出了一种通用的一次学习方案，适用于其他GAN生成的图像。 |
| [^71] | [Computably Continuous Reinforcement-Learning Objectives are PAC-learnable.](http://arxiv.org/abs/2303.05518) | 本研究证明了计算连续的强化学习目标是可PAC学习的，这对于设计高效的强化学习算法打开了新的突破口。 |
| [^72] | [Group conditional validity via multi-group learning.](http://arxiv.org/abs/2303.03995) | 本文提出了一种实现群组条件有效性的方法，通过将问题规约为多组学习问题，从而解决了现有方法中受限于特定分组结构或分布假设等问题。同时，利用一种新的多组学习算法，进一步提高了样本复杂度保证和简化预测结构。 |
| [^73] | [Local Environment Poisoning Attacks on Federated Reinforcement Learning.](http://arxiv.org/abs/2303.02725) | 本文提出了第一个广义框架，将FRL的毒化攻击视为限制预算的优化问题，并设计了一个可以应用于基于策略的FRL的毒化攻击协议，通过训练一对私人评价者和公共评价者将其扩展到使用演员-评论家作为本地RL算法的FRL。 |
| [^74] | [Seq-HyGAN: Sequence Classification via Hypergraph Attention Network.](http://arxiv.org/abs/2303.02393) | 本文提出了一种基于超图注意力网络的序列分类模型Seq-HyGAN，通过创建超图和引入注意力机制来处理序列数据中的复杂结构相似性，从而提高分类准确率。 |
| [^75] | [Hierarchical Training of Deep Neural Networks Using Early Exiting.](http://arxiv.org/abs/2303.02384) | 本文提出了一种使用早期退出的分层训练方法，将深度神经网络分为边缘和云工作者，以减少通信成本、训练运行时间和隐私问题。 |
| [^76] | [Discovery and Recognition of Formula Concepts using Machine Learning.](http://arxiv.org/abs/2303.01994) | 本文提出了一种使用机器学习来自动识别和发现科学文献中数学概念的方法，包括公式概念发现和公式概念识别两个子任务，通过对arXiv子集中1.8M个公式出现进行实验，结果显示该方法优于强基线。 |
| [^77] | [ABAW: Valence-Arousal Estimation, Expression Recognition, Action Unit Detection & Emotional Reaction Intensity Estimation Challenges.](http://arxiv.org/abs/2303.01498) | 本比赛提供两个数据集，包含视频和声音数据，针对表情情感估计、表情识别、肌肉动作检测和情感反应强度估计四个任务进行挑战。参赛者需要提供能在复杂、不受控制的环境下运行的有效模型。 |
| [^78] | [AugGPT: Leveraging ChatGPT for Text Data Augmentation.](http://arxiv.org/abs/2302.13007) | AugGPT提出了一种基于ChatGPT的文本数据增强方法，该方法能够更忠实地保留正确标记的生成数据并提供足够的多样性，从而有效地缓解了自然语言处理任务中的限制样本量的问题。 |
| [^79] | [FrankenSplit: Saliency Guided Neural Feature Compression with Shallow Variational Bottleneck Injection.](http://arxiv.org/abs/2302.10681) | 本文提出了一种基于显著性指导的神经特征压缩与浅层变分瓶颈注入的新的资源意识压缩模型的框架，实现了比最先进的SC方法低60％的比特率，并且比现有的编解码标准的下放快16倍。 |
| [^80] | [SurvLIMEpy: A Python package implementing SurvLIME.](http://arxiv.org/abs/2302.10571) | 本文介绍了一个名为SurvLIMEpy的Python包，它实现了一种算法，可以计算适用于建模生存分析数据的机器学习算法的局部特征重要性，并支持各种生存模型。 |
| [^81] | [Stochastic Generative Flow Networks.](http://arxiv.org/abs/2302.09465) | Stochastic GFlowNets通过学习动态模型来扩展了GFlowNets的适用性，解决了原模型只适用于确定性环境的问题，并在具有随机动态的各种标准基准测试中表现良好，具有显著优势。 |
| [^82] | [T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models.](http://arxiv.org/abs/2302.08453) | 该论文提出利用T2I模型隐含学习的知识来更加精确地控制生成结果。通过训练简单轻量级的T2I适配器来对齐内部知识与外部控制信号，实现在生成结果的颜色和结构方面丰富的控制和编辑效果。 |
| [^83] | [Spatially heterogeneous learning by a deep student machine.](http://arxiv.org/abs/2302.07419) | 本论文研究了一种深度学生机器的教师-学生设置，通过学生机器的集合来研究由具有大量可调参数的DNN的监督学习。研究表明DNN的学习在网络空间中相当异质。 |
| [^84] | [An Optical XNOR-Bitcount Based Accelerator for Efficient Inference of Binary Neural Networks.](http://arxiv.org/abs/2302.06405) | 本论文提出了一种基于光学XNOR门和光电荷累加器的BNN加速器设计，相比现有的PICs加速器，该设计在面积、能效和吞吐量等方面获得了显着的改进。 |
| [^85] | [Learning from Noisy Crowd Labels with Logics.](http://arxiv.org/abs/2302.06337) | 这篇论文介绍了一种利用逻辑引导的从嘈杂的众包标签中学习的框架，能够改进文本分类和命名实体识别等任务中，学习从嘈杂数据中提取有效信息的方法，这种框架能够融合逻辑知识，提高现有技术水平。 |
| [^86] | [A Theoretical Understanding of Shallow Vision Transformers: Learning, Generalization, and Sample Complexity.](http://arxiv.org/abs/2302.06015) | 本文提供了第一份对于浅层ViT进行训练的理论分析，证明了使用SGD训练会产生稀疏的注意力图，目前的样本复杂度与标记相关令牌的分数倒数、标记级别的令牌噪声水平和初始模型错误呈正相关关系。 |
| [^87] | [Cross-Modal Fine-Tuning: Align then Refine.](http://arxiv.org/abs/2302.05738) | ORCA是一个通用的跨模态微调框架，它将单个大规模预训练模型的适用性扩展到多种模态，通过先对齐再微调的方法使跨模态微调成为可能，实验结果优于广泛范围的方法。 |
| [^88] | [IoT Botnet Detection Using an Economic Deep Learning Model.](http://arxiv.org/abs/2302.02013) | 本文提出了一种经济深度学习模型来检测物联网僵尸网络攻击以及不同类型的攻击。该模型能够在较小的预算下加速训练和检测过程，并且具有比最先进的检测模型更高的准确性。 |
| [^89] | [Contrast and Clustering: Learning Neighborhood Pair Representation for Source-free Domain Adaptation.](http://arxiv.org/abs/2301.13428) | 本文提出了一个无源域适应的实用且具有挑战性的方案，通过在原始特征空间聚类，构建真正困难的负对，结合噪声对比估计理论，学习一个域不变的特征来解决域上的差异问题，能够在三个常见的基准数据集上实现有效结果。 |
| [^90] | [Principled Reinforcement Learning with Human Feedback from Pairwise or $K$-wise Comparisons.](http://arxiv.org/abs/2301.11270) | 该论文提供了带有人类反馈强化学习问题的理论框架，证明了最大似然估计在Bradley-Terry-Luce和Plackett-Luce模型下收敛。此外，提出了在一定的覆盖假设下，基于悲观估计的MLE提供了性能更好的策略。在证明了真实MLE和以成对比较形式替代的备选MLE都可以在PL模型下收敛的同时，也表明了真实MLE的高效性。这些结果为RLHF算法提供了新的见解，并统一了RLHF问题和IRL问题。 |
| [^91] | [Learning Gradients of Convex Functions with Monotone Gradient Networks.](http://arxiv.org/abs/2301.10862) | 本文提出了 C-MGN 和 M-MGN 两种单调梯度神经网络结构，用于直接学习凸函数的梯度，并显示出比现有的方法更好的效果。 |
| [^92] | [Towards an AI-enabled Connected Industry: AGV Communication and Sensor Measurement Datasets.](http://arxiv.org/abs/2301.03364) | 本文介绍了两个无线测量活动所提供的数据集，并将其与机器学习结合起来用于指纹识别、视线检测、服务质量预测或链路选择等任务。 |
| [^93] | [Pseudo-Inverted Bottleneck Convolution for DARTS Search Space.](http://arxiv.org/abs/2301.01286) | 本文增加了ConvNeXt的微小设计变化来扩充DARTS搜索空间，提出了PIBConv块来减少计算占用，我们的架构在层数仅为2时优于一个具有类似规模的DARTS网络。 |
| [^94] | [KoopmanLab: machine learning for solving complex physics equations.](http://arxiv.org/abs/2301.01104) | KoopmanLab 提出了一种机器学习模块，名为 Koopman neural operator (KNO), 用于在没有解析解或闭合形式的条件下学习 PDEs，解决了传统数值方法精度和效率平衡的问题，并且适用于未知潜在 PDEs 生成的经验数据。 |
| [^95] | [Approaching Peak Ground Truth.](http://arxiv.org/abs/2301.00243) | 介绍了PGT理论概念和近似PGT的定量技术，为评估和提高机器学习模型在生物医学领域中的性能提供了策略。 |
| [^96] | [MN-DS: A Multilabeled News Dataset for News Articles Hierarchical Classification.](http://arxiv.org/abs/2212.12061) | 本文介绍了一个包含10,917篇新闻文章的多标签数据集，可用于训练机器学习模型自动按主题对新闻文章进行分类，对新闻结构、分类和预测未来事件的研究人员非常有帮助。 |
| [^97] | [On Calibrating Semantic Segmentation Models: Analyses and An Algorithm.](http://arxiv.org/abs/2212.12053) | 本文系统研究了语义分割模型的校准问题，提出了一种简单而有效的方法——选择性缩放，通过将正确/错误预测分开进行缩放，并更加关注错误预测的逻辑平滑，此方法在语义分割校准上取得了良好效果。 |
| [^98] | [Few-shot human motion prediction for heterogeneous sensors.](http://arxiv.org/abs/2212.11771) | 本研究提出了一种扩展时间序列预测方法来考虑异构传感器对人体运动预测的影响，并成功将其应用于不同传感器情境下的少样本有限运动任务中。 |
| [^99] | [Learning for Vehicle-to-Vehicle Cooperative Perception under Lossy Communication.](http://arxiv.org/abs/2212.08273) | 本文研究了复杂驾驶场景下可能存在的损失共享特征的情况对车辆协同感知的影响，并提出了一种新的损失通信感知特征融合方法，通过损失通信感知修复网络缓解了损失通信的副作用并增强了感知性能。 |
| [^100] | [Non-IID Transfer Learning on Graphs.](http://arxiv.org/abs/2212.08174) | 本文提出了一种用于图上跨网络迁移学习的新方法，在利用Weisfeiler-Lehman图同构测试的基础上，提出了图子树差异度量算法来衡量源目标之间的图分布差异，进而推导出跨网络迁移学习的一般化误差界限，同时还开发了GDSL算法用于对齐图子树，实验结果表明我们的方法优于现有的最先进方法。 |
| [^101] | [Non-equispaced Fourier Neural Solvers for PDEs.](http://arxiv.org/abs/2212.04689) | 提出了一种名为\textsc {NFS}的非等间隔傅里叶PDE求解器，能够在处理实际系统的空间域中非等间隔的数据情况下保持较高精度，并且具有网格不变推断能力。 |
| [^102] | [DRIP: Domain Refinement Iteration with Polytopes for Backward Reachability Analysis of Neural Feedback Loops.](http://arxiv.org/abs/2212.04646) | 本文提出了 DRIP 算法，使用多面体域精炼迭代来提高神经回路反向可达性分析的精度，进一步收紧了反投影集合的边缘。同时，还引入了一个新的公式来表示多面体边缘，以加强边缘限制。 |
| [^103] | [LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models.](http://arxiv.org/abs/2212.04088) | 本研究提出了一种新颖的方法LLM-Planner，利用大型语言模型为实体代理进行少样本规划，以实体代理目前所在的环境为基础，增强LLMs生成和更新计划，实验表明其在多任务和快速学习新任务的通用代理的开发中具有很好的表现。 |
| [^104] | [PCT-CycleGAN: Paired Complementary Temporal Cycle-Consistent Adversarial Networks for Radar-Based Precipitation Nowcasting.](http://arxiv.org/abs/2211.15046) | 本文提出了一种基于配对互补时间循环一致对抗网络的雷达降水预测方法，该方法包括两个生成器网络和循环一致性损失和对抗性损失。实验证明，该方法在准确性和推广能力方面优于现有的技术方法。 |
| [^105] | [SpaText: Spatio-Textual Representation for Controllable Image Generation.](http://arxiv.org/abs/2211.14305) | SpaText提出了一种新方法，利用开放词汇场景以及基于CLIP的空间文本表示进行文本到图像生成，能够实现对不同区域/对象的形状或布局的细粒度控制。 |
| [^106] | [Shape, Pose, and Appearance from a Single Image via Bootstrapped Radiance Field Inversion.](http://arxiv.org/abs/2211.11674) | 本文提出了一个自回归光场反演框架，可以从单个图像中恢复 SDF 参数化的 3D 形状、姿态和外观，而无需准确的真实姿势，且速度快、计算有效。 |
| [^107] | [Efficient Estimation for Longitudinal Network via Adaptive Merging.](http://arxiv.org/abs/2211.07866) | 本文提出了一个有效的纵向网络估计框架，利用自适应合并、张量分解和点过程等方法来减少估计偏差和方差。 |
| [^108] | [The Sample Complexity of Online Contract Design.](http://arxiv.org/abs/2211.05732) | 本文解决了在线合同设计中一个悬而未决的问题，证明了指数级的$m$个样本就足以学习一个近乎最优的合同。 |
| [^109] | [NESTER: An Adaptive Neurosymbolic Method for Treatment Effect Estimation.](http://arxiv.org/abs/2211.04370) | NESTER是一种自适应的神经符号化方法进行治疗效果评估，将治疗效果估计的所有要求集成到一个框架中，该方法比现有最先进的方法在多个基准数据集上性能更好。 |
| [^110] | [A Meta-GNN approach to personalized seizure detection and classification.](http://arxiv.org/abs/2211.02642) | 本文提出了一种个性化癫痫检测和分类的元图神经网络方法，通过将图神经网络和元学习相结合，能够在有限的样本中快速适应特定患者，并在 TUSZ 数据集上取得了优于基线的结果。 |
| [^111] | [Fully Bayesian inference for latent variable Gaussian process models.](http://arxiv.org/abs/2211.02218) | 隐变量高斯过程模型通过将定性因素映射到底层隐变量的方式解决了标准高斯过程无法适应定性输入的问题。本文提出了一种考虑隐变量估计不确定性的完全贝叶斯方法，支持通过隐变量可视化定性输入的效果。 |
| [^112] | [An Empirical Bayes Analysis of Object Trajectory Representation Models.](http://arxiv.org/abs/2211.01696) | 本论文对对象轨迹表示模型的复杂度和拟合误差之间的权衡进行了经验分析，发现简单的线性模型就能够高度重现真实世界的轨迹，通过使用经验贝叶斯方法可以为轨迹跟踪问题中必要的运动模型提供信息，并可以帮助规范预测模型。 |
| [^113] | [Interpretable estimation of the risk of heart failure hospitalization from a 30-second electrocardiogram.](http://arxiv.org/abs/2211.00819) | 本论文研究展示了使用机器学习方法可以通过30秒单导联心电图信号估计心力衰竭住院，并提供了临床有意义的解释。通过训练极限梯度提升加速故障时间模型，该模型在测试集上取得了较高的预测能力，可以在定位和治疗中发挥作用。 |
| [^114] | [Towards Better Out-of-Distribution Generalization of Neural Algorithmic Reasoning Tasks.](http://arxiv.org/abs/2211.00692) | 本文研究神经算法推理任务的非分布式泛化能力，分析当前基准测试的挑战并提出了解决方案。 |
| [^115] | [Convergence Rates of Stochastic Zeroth-order Gradient Descent for \L ojasiewicz Functions.](http://arxiv.org/abs/2210.16997) | 该论文证明了在Lojasiewicz函数上，随机零阶梯度下降算法具有收敛速率，且比 $\{ \|\mathbf{x}_t-\mathbf{x}_\infty\| \}_{t \in \mathbb{N}}$更快，无论$f$是平滑还是非平滑的。 |
| [^116] | [Broken Neural Scaling Laws.](http://arxiv.org/abs/2210.14891) | 本文提出了一个平滑破碎的幂律函数形式，可以准确地模拟和外推深度神经网络的缩放行为，适用于各种架构和大量不同任务，包括视觉、语言、音频、视频、生成建模、对比学习、机器人、不确定性估计/校准、对抗鲁棒性、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。 |
| [^117] | [Combined Data and Deep Learning Model Uncertainties: An Application to the Measurement of Solid Fuel Regression Rate.](http://arxiv.org/abs/2210.14287) | 本文研究了固体燃料回归速率测量中的不确定性问题，并提出了一种可用于处理多个不确定性来源的不确定性量化过程。 |
| [^118] | [Highly Efficient Real-Time Streaming and Fully On-Device Speaker Diarization with Multi-Stage Clustering.](http://arxiv.org/abs/2210.13690) | 本文提出了一种多阶段聚类策略，可以解决流式嵌入式演讲者分离系统的多方面挑战，从而提高效率。此策略使用不同的聚类算法处理不同长度的输入，并可适应不同资源约束的设备。 |
| [^119] | [Symmetric (Optimistic) Natural Policy Gradient for Multi-agent Learning with Parameter Convergence.](http://arxiv.org/abs/2210.12812) | 本文研究了多智能体学习中的机器人学习算法，指出了香草自然策略梯度算法可能因具有不收敛的参数而存在学习不稳定性问题，并提出了新的NPG算法变种以解决此问题。 |
| [^120] | [MTEB: Massive Text Embedding Benchmark.](http://arxiv.org/abs/2210.07316) | 本文提出了一个大规模文本嵌入基准测试(MTEB)，该基准测试涵盖了8个嵌入任务、58个数据集和112种语言，以解决文本嵌入在不同任务中表现差异的问题。通过33个模型的测试，作者发现该领域尚未收敛于一种通用的文本嵌入方法， |
| [^121] | [Predictive Inference with Feature Conformal Prediction.](http://arxiv.org/abs/2210.00173) | 本文提出基于特征符合预测的预测推断方法，通过利用深度表示学习的归纳偏置，扩展了符合预测到语义特征空间。从理论和实验结果来看，该方法优于常规符合预测，并在大规模任务上展现了最先进性能。 |
| [^122] | [Safe Exploration Method for Reinforcement Learning under Existence of Disturbance.](http://arxiv.org/abs/2209.15452) | 本文提出了一种能够在存在扰动情况下安全进行强化学习探索的方法，该方法利用了所控制对象和扰动的部分先验知识，可以保证以预先指定的概率满足显式状态约束。 |
| [^123] | [Construction and Applications of Billion-Scale Pre-Trained Multimodal Business Knowledge Graph.](http://arxiv.org/abs/2209.15214) | 本文介绍了一个基于阿里巴巴集团的空前规模的 OpenBG 商业知识图谱，包含超过 88 百万实体和 26 亿三元组。它具有精细的分类和多模态事实，有助于推动商业智能化的发展。 |
| [^124] | [Minimizing Human Assistance: Augmenting a Single Demonstration for Deep Reinforcement Learning.](http://arxiv.org/abs/2209.11275) | 本文通过使用虚拟现实模拟收集的单个人类示例辅助RL训练，从而在最小化人类干预的同时保留了使用人类示范的性能优势。这种方法使用单个示例生成的多个演示可显著提高训练速度，并使智能体能够解决复杂任务。 |
| [^125] | [Multi-armed Bandit Learning on a Graph.](http://arxiv.org/abs/2209.09419) | 本文提出了一种基于乐观原则和离线图形规划算法的学习算法G-UCB，能够平衡长期探索利用，用于解决一种名为图赌博机的MAB扩展，从而获得最大化的收益。 |
| [^126] | [EcoFormer: Energy-Saving Attention with Linear Complexity.](http://arxiv.org/abs/2209.09004) | EcoFormer是一种通过核哈希技术进行高维softmax注意力二值化的新方法，实现了具有线性时间复杂度的高效注意力模块，从而显著降低了计算和能量开销，同时在多个任务上取得了高性能。 |
| [^127] | [Bispectral Neural Networks.](http://arxiv.org/abs/2209.03416) | 本文提出了一种称为双谱神经网络的模型，能够从数据的隐含对称性中学习群、不可约表示和对应的完全不变映射，具有强大的基于不变性的对抗鲁棒性。 |
| [^128] | [Scalable Multi-Agent Lab Framework for Lab Optimization.](http://arxiv.org/abs/2208.09099) | 本文介绍了一种可扩展的多智能体实验室优化框架，能够有效地控制设备使用，让具有不同学习能力和目标的机器学习智能体协同运行研究活动。 |
| [^129] | [S-Prompts Learning with Pre-trained Transformers: An Occam's Razor for Domain Incremental Learning.](http://arxiv.org/abs/2207.12819) | 该论文提出了一种名为"S-Prompting"的学习范例，使用预先训练的变压器独立地跨越不同领域学习提示，大幅度减少连续学习中的遗忘问题，并具有出色的可适应性。 |
| [^130] | [GraphCFC: A Directed Graph based Cross-modal Feature Complementation Approach for Multimodal Conversational Emotion Recognition.](http://arxiv.org/abs/2207.12261) | 本文提出了一种基于有向图的跨模态特征补充方法，可以提取多模态上下文信息和交互信息，缓解了多模态融合中的异构性差距问题。 |
| [^131] | [Learning an Adaptive Forwarding Strategy for Mobile Wireless Networks: Resource Usage vs. Latency.](http://arxiv.org/abs/2207.11386) | 本文利用深度强化学习为移动无线网络设计了一种可扩展和普适的单副本路由策略，通过权衡竞争的网络目标，设计了一种具有奖励函数的路由策略，并提出了一组新颖的关系领域、路径和上下文特征来描述设备移动情况，同时采用灵活的训练方法来训练单个DeepRL代理。 |
| [^132] | [Active Exploration for Inverse Reinforcement Learning.](http://arxiv.org/abs/2207.08645) | AceIRL提出了一种新的逆强化学习算法，通过主动探索来学习奖励函数和策略，在不需要环境生成模型的情况下，能够确定可行奖励函数的置信区间，并找到侧重于环境中最有信息的区域的探索策略。 |
| [^133] | [Multitrack Music Transformer.](http://arxiv.org/abs/2207.06983) | 这篇论文提出了一种新的多轨音乐表示方法，可以支持各种不同的乐器，在短的序列长度下实现了性能上的显著提升，同时提出了一种新方法用于分析音乐自我关注，并验证了模型更关注与当前音符形成和谐跨度和位于同一八度的音符。 |
| [^134] | [Learning Continuous Grasping Function with a Dexterous Hand from Human Demonstrations.](http://arxiv.org/abs/2207.05053) | 该论文提出了一种利用隐式函数进行连续抓取规划的方法，名为连续抓取功能(CG)，通过人类示范学习，可在操纵多个物体时获得更高的成功率，并在真实机器人上得到验证，成功率显著提高。 |
| [^135] | [Parallel Conformal Hyperparameter Optimization.](http://arxiv.org/abs/2207.03017) | 本文提出了一种基于一致置信区间上限抽样的优化框架，其假设具有交换性，能够提供更多的搜索模型架构选择，并且在超参数调整时具有优异的性能。 |
| [^136] | [Ask-AC: An Initiative Advisor-in-the-Loop Actor-Critic Framework.](http://arxiv.org/abs/2207.01955) | 本文提出了一种新颖的主动顾问演员-评论家框架，Ask-AC，它替换了传统的被动监督信号机制，实现了定制化和高效的信息交换，其中的两个互补组件允许代理主动寻求顾问干预和识别漏掉的不稳定状态。 |
| [^137] | [Beyond mAP: Towards better evaluation of instance segmentation.](http://arxiv.org/abs/2207.01614) | 本文提出了两个新的度量标准以明确衡量空间和分类重复预测的数量，研究人员还提出了一种Semantic Sorting and NMS模块以消除这些重复预测，以超越传统的平均精度度量标准。 |
| [^138] | [Concentration inequalities and optimal number of layers for stochastic deep neural networks.](http://arxiv.org/abs/2206.11241) | 该论文提出了随机深度神经网络的浓度不等式，通过期望分类器给出了分类误差的概率上界，并确定了最优层数。 |
| [^139] | [Merak: An Efficient Distributed DNN Training Framework with Automated 3D Parallelism for Giant Foundation Models.](http://arxiv.org/abs/2206.04959) | Merak是一个自动化的三维并行深度学习训练框架，它解决了当前其他框架中需要手动修改模型才可并行的问题，并具有较高的计算、GPU内存和网络带宽利用率。 |
| [^140] | [Detecting Anomalous Cryptocurrency Transactions: an AML/CFT Application of Machine Learning-based Forensics.](http://arxiv.org/abs/2206.04803) | 本研究探讨机器学习和交易图分析方法在反洗钱和打击恐怖主义领域的应用，特别关注这些技术在社会技术生态系统中部署所产生的影响。 |
| [^141] | [Diffusion probabilistic modeling of protein backbones in 3D for the motif-scaffolding problem.](http://arxiv.org/abs/2206.04119) | 本研究提出了一种新的方法来解决蛋白质脚手架问题，采用E(3)-等变图神经网络来学习多样和长的蛋白质主干结构的分布，并用SMCDiff算法从分布中有效地采样出符合给定模体条件的脚手架，这一算法在大计算极限中可以理论上保证条件样本的采样；研究结果表明，我们的方法可以生成长达80个残基的脚手架，并可以为固定模体实现结构多样的脚手架。 |
| [^142] | [Robust Adversarial Attacks Detection based on Explainable Deep Reinforcement Learning For UAV Guidance and Planning.](http://arxiv.org/abs/2206.02670) | 本文提出了一种基于可解释的深度学习方法的创新性检测方案，以保护采用这些方法的无人机免受对抗攻击。 |
| [^143] | [Principal Component Analysis based frameworks for efficient missing data imputation algorithms.](http://arxiv.org/abs/2205.15150) | 本文提出了基于主成分分析的缺失数据插补框架，可适用于高维数据，能够显著优于基线方法，同时在分类任务中实现可比较或更好的分类准确性。 |
| [^144] | [Group-level Brain Decoding with Deep Learning.](http://arxiv.org/abs/2205.14102) | 本文提出了一种深度学习方法，利用主体嵌入技术解决个体差异带来的群体级脑解码问题，并在脑磁图像数据上进行了验证。 |
| [^145] | [Byzantine-Robust Federated Learning with Optimal Statistical Rates and Privacy Guarantees.](http://arxiv.org/abs/2205.11765) | 本文提出了一种具有近乎最优统计率的拜占庭鲁棒联邦学习协议，并展示了其与竞争协议相比的经验优越性，协议通过分桶可以结合隐私保障程序以对半诚实服务器进行安全保障。 |
| [^146] | [GANs as Gradient Flows that Converge.](http://arxiv.org/abs/2205.02910) | 本文通过在概率密度函数空间中的梯度下降方式来解决无监督学习问题，证明了GAN的训练过程可以被视为一种分布依赖的ODE的模拟。GAN算法最小化了两组样本之间的均方误差，并且只有在判别器足够强大时才能真正收敛。 |
| [^147] | [Reward Reports for Reinforcement Learning.](http://arxiv.org/abs/2204.10817) | 本文提出一种名为奖励报告的框架，用于记录已部署和迭代更新的学习系统。通过追踪设计选择和假设，帮助透明地传达系统目标，并跟踪目标的演变。 |
| [^148] | [Causal Confusion and Reward Misidentification in Preference-Based Reward Learning.](http://arxiv.org/abs/2204.06601) | 基于偏好的奖励学习中存在因果混淆和奖励误识别，非因果分散特征、偏好中的噪声以及状态的局部可观察性可能加剧奖励误识别，必须注意保证学习到的奖励的可识别性和因果性。 |
| [^149] | [Forecasting Cryptocurrency Returns from Sentiment Signals: An Analysis of BERT Classifiers and Weak Supervision.](http://arxiv.org/abs/2204.05781) | 本文主要研究了使用情感信号预测加密货币回报的方法，提出了一种弱监督学习的NLP方法来解决文本数据无标签的问题，并证实它能提高预测价值和准确性。 |
| [^150] | [On the Neural Tangent Kernel Analysis of Randomly Pruned Neural Networks.](http://arxiv.org/abs/2203.14328) | 本文研究了预剪枝在神经网络的神经切向核中的应用，并得出了两种情况下的结论。 |
| [^151] | [Sionna: An Open-Source Library for Next-Generation Physical Layer Research.](http://arxiv.org/abs/2203.11854) | Sionna是一个GPU加速的基于TensorFlow的开源库，实现了最先进算法，可以用于基准测试和端到端性能评估。它能够提供对神经网络的原生支持，使得研究人员能够更加专注于自己的研究。 |
| [^152] | [Representation Bias in Data: A Survey on Identification and Resolution Techniques.](http://arxiv.org/abs/2203.11852) | 本文综述了识别和解决数据表征偏差的文献，提出基于多个因素的分类法，该偏差可能影响人工智能应用的公平性和社会影响。 |
| [^153] | [Calibration of Derivative Pricing Models: a Multi-Agent Reinforcement Learning Perspective.](http://arxiv.org/abs/2203.06865) | 本文利用多智能体强化学习提出校准衍生品定价模型问题的博弈论解决方案，并希望该方法可用于解决其他金融领域的问题。实验证明，该算法能够学习局部波动率以及最小化百慕大期权价格所需的路径依赖性。 |
| [^154] | [NELA-GT-2022: A Large Multi-Labelled News Dataset for The Study of Misinformation in News Articles.](http://arxiv.org/abs/2203.05659) | NELA-GT-2022是一份包含361个来源的1,778,361篇文章的多标签新闻数据集，可用于研究新闻报道中的误导信息。 |
| [^155] | [New Projection-free Algorithms for Online Convex Optimization with Adaptive Regret Guarantees.](http://arxiv.org/abs/2202.04721) | 本文提出了无需投影的新型在线凸优化算法，基于在线梯度下降算法和高效的“不可行投影”计算方法，实现了自适应遗憾保证，并达到了$O(T^{3/4})$的定常性收敛速度。 |
| [^156] | [Message Passing Neural PDE Solvers.](http://arxiv.org/abs/2202.03376) | 本文提出了一种基于神经消息传递的求解器，用反向传播优化的神经函数逼近器取代计算图中所有启发式设计的组件，并建立了一种带有完全状态更新的消息传递方法，可推广到任何具有经典解结构的偏微分方程中。 |
| [^157] | [SimGRACE: A Simple Framework for Graph Contrastive Learning without Data Augmentation.](http://arxiv.org/abs/2202.03104) | SimGRACE是一种简单的图形对比学习框架，它不需要数据增强就能在几个基准数据集上实现最新的结果，并采用了一种高效的对比损失函数、负采样策略、以及一些附加项来增强性能。 |
| [^158] | [L-SVRG and L-Katyusha with Adaptive Sampling.](http://arxiv.org/abs/2201.13387) | 本文提出了一种自适应采样策略来提高L-SVRG和L-Katyusha优化方法在训练机器学习模型中的性能表现，可以在少量计算开销内实现采样分布的学习，同时不需要先验知识，并证明了其收敛性保证。 |
| [^159] | [Tiny, always-on and fragile: Bias propagation through design choices in on-device machine learning workflows.](http://arxiv.org/abs/2201.07677) | 本研究调查了边缘端机器学习工作流程中偏见传递的设计选择。结果表明，在模型训练过程中的技术设计选择，如模型架构和优化算法，会放大和传播可靠性偏见。 |
| [^160] | [Fighting Money Laundering with Statistics and Machine Learning: An Introduction and Review.](http://arxiv.org/abs/2201.04207) | 本文介绍了银行反洗钱的统计和机器学习方法，并提出客户风险评估和可疑行为标识两个核心要素。未来的研究方向包括生成合成数据、半监督和深度学习、可解释性以及结果的公平性。 |
| [^161] | [Augmenting astrophysical scaling relations with machine learning: application to reducing the Sunyaev-Zeldovich flux-mass scatter.](http://arxiv.org/abs/2201.01305) | 本文使用符号回归的机器学习工具，在Sunyaev-Zeldovich 荧光$-$星团质量关系中找到了一个新的代理变量，结合了$Y_\mathrm{SZ}$和电离气体浓度($c_\mathrm{gas}$)，极大地减小了该关系中的散射，提高了宇宙学分析的灵敏度。 |
| [^162] | [Learning-Augmented Algorithms for Online Steiner Tree.](http://arxiv.org/abs/2112.05353) | 本文提出了一种集成机器学习预测与在线算法设计的超越最坏情况算法分析模型，并基于该模型研究了有向图和无向图的在线Steiner树问题。通过预测在线终端节点，算法在有良好预测的情况下打破在线下界，并且竞争比例会优雅地降低。 |
| [^163] | [A generalization gap estimation for overparameterized models via the Langevin functional variance.](http://arxiv.org/abs/2112.03660) | 本文提出了一种函数方差的Langevin估计方法，用于高效计算超参数模型的泛化缺口，实现与基于梯度的优化算法一致。 |
| [^164] | [Geometry-aware Bayesian Optimization in Robotics using Riemannian Mat\'ern Kernels.](http://arxiv.org/abs/2111.01460) | 本文探讨了基于Riemannian Mat\'ern kernels技术在机器人学中的实现，并在一组基准函数上展示了其性能。该技术可以有效地处理非欧几里得域上定义的函数。 |
| [^165] | [Knowledge Sheaves: A Sheaf-Theoretic Framework for Knowledge Graph Embedding.](http://arxiv.org/abs/2110.03789) | 本研究提出了一种“知识花扭”方法来描述知识图谱嵌入模型，并可以表达广泛的嵌入先验约束，可轻松应对复合关系推理。 |
| [^166] | [Approximate Newton policy gradient algorithms.](http://arxiv.org/abs/2110.02398) | 本文提出了一种用于政策梯度算法的近似牛顿方法，包括自然策略梯度算法和全新的策略梯度算法，具有快速收敛的优势。 |
| [^167] | [Bilevel Imaging Learning Problems as Mathematical Programs with Complementarity Constraints: Reformulation and Theory.](http://arxiv.org/abs/2110.02273) | 本论文提出了一种新的数学规划问题（MPCC）的方法用于解决双层成像学习问题。该方法在满足一定条件下，可以得出稳定条件、最优性条件和局部唯一性结果，可用于函数空间中的问题，并且可以加入对梯度的约束。 |
| [^168] | [Long-Range Transformers for Dynamic Spatiotemporal Forecasting.](http://arxiv.org/abs/2109.12218) | 本研究提出了一种名为Spacetimeformer的方法，将多元时间序列预测转化为“时空序列”形式进行建模，实现了对变量之间动态空间关系的学习，同时在多个基准测试上取得了最先进的结果。 |
| [^169] | [Data-Driven Constitutive Relation Reveals Scaling Law for Hydrodynamic Transport Coefficients.](http://arxiv.org/abs/2108.00413) | 本文研究了基于数据的模型中导数选择及其局限性，认为这些模型相当于非线性长度尺度比例定律，建模比例定律可避免实际困难。 |
| [^170] | [Who Is the Strongest Enemy? Towards Optimal and Efficient Evasion Attacks in Deep RL.](http://arxiv.org/abs/2106.05087) | 本文提出了一种新的攻击方法，通过一个名为“演员”的设计函数和名为“导演”的基于RL的学习器之间的协作，最新方法可以找到最优攻击，提高了关于RL代理鲁棒性的理解。 |
| [^171] | [Towards Teachable Autotelic Agents.](http://arxiv.org/abs/2105.11977) | 本文提出了可教学自主智能体的概念，它们可以从内部和教学信号中同步学习，从而提高技能习得的效率，此举是构建具有人类级智能的代理的关键步骤。 |
| [^172] | [Disentangling Geometric Deformation Spaces in Generative Latent Shape Models.](http://arxiv.org/abs/2103.00142) | 研究改进了先前的生成模型，通过将物体几何空间分解为刚性方向、非刚性姿态和固有形状，并使用谱几何和概率分离的组合进行训练，赋予了对对象变形空间的可解释性描述。同时改进包括更复杂的旋转不变处理和使用流形变形网络来连接潜在和谱空间，可以更好地控制生成形状。 |
| [^173] | [Is Q-Learning Minimax Optimal? A Tight Sample Complexity Analysis.](http://arxiv.org/abs/2102.06548) | 本文通过紧密的样本复杂度分析回答了Q学习是否是极小极大最优的问题。 |
| [^174] | [Falsification-Based Robust Adversarial Reinforcement Learning.](http://arxiv.org/abs/2007.00691) | 本论文提出了基于虚假测试的鲁棒性对抗学习框架，通过使用虚假测试方法提高了强化学习算法在测试场景下的准确性和鲁棒性，得到了显著的实验结果。 |

# 详细

[^1]: SECAD-Net：通过学习草图挤压操作进行自我监督的CAD重建

    SECAD-Net: Self-Supervised CAD Reconstruction by Learning Sketch-Extrude Operations. (arXiv:2303.10613v1 [cs.CV])

    [http://arxiv.org/abs/2303.10613](http://arxiv.org/abs/2303.10613)

    本论文提出了一种名为SECAD-Net的自我监督神经网络，旨在以紧凑且易于编辑的方式从原始输入中重建高质量的CAD模型。其利用学习2D草图和3D挤压参数以创建一组挤压筒，并结合布尔操作来逼近目标几何形状。使用隐式场表示草图，允许通过在草图潜在空间中插值潜在代码来创建CAD变体。在实验中，SECAD-Net超越了现有的最先进方法，证明了其在重建质量和推断速度方面的卓越表现。

    

    从原始几何形状中反向工程CAD模型是一个经典但困难的研究问题。以往基于学习的方法由于受监督的设计模式或重建不易编辑的CAD形状而大量依赖标签。在这项工作中，我们介绍了SECAD-Net，这是一个端到端的神经网络，旨在以自我监督的方式重建紧凑且易于编辑的CAD模型。从现代CAD软件中最常使用的建模语言中汲取灵感，我们提出从原始形状中学习2D草图和3D挤压参数，从而通过将每个草图从2D平面挤压到3D体来生成一组挤压筒。通过融合布尔操作（即联合），这些筒可以组合在一起，以密切逼近目标几何形状。我们倡导使用隐式场来表示草图，这允许通过在草图潜在空间中插值潜在代码来创建CAD变体。广泛的实验结果表明，SECAD-Net可以从原始输入中重建高质量的CAD模型，在重建质量和推断速度方面超越了现有的最先进方法。

    Reverse engineering CAD models from raw geometry is a classic but strenuous research problem. Previous learning-based methods rely heavily on labels due to the supervised design patterns or reconstruct CAD shapes that are not easily editable. In this work, we introduce SECAD-Net, an end-to-end neural network aimed at reconstructing compact and easy-to-edit CAD models in a self-supervised manner. Drawing inspiration from the modeling language that is most commonly used in modern CAD software, we propose to learn 2D sketches and 3D extrusion parameters from raw shapes, from which a set of extrusion cylinders can be generated by extruding each sketch from a 2D plane into a 3D body. By incorporating the Boolean operation (i.e., union), these cylinders can be combined to closely approximate the target geometry. We advocate the use of implicit fields for sketch representation, which allows for creating CAD variations by interpolating latent codes in the sketch latent space. Extensive experim
    
[^2]: 使用T5 Transformer模型的孟加拉语语法错误检测

    Bangla Grammatical Error Detection Using T5 Transformer Model. (arXiv:2303.10612v1 [cs.CL])

    [http://arxiv.org/abs/2303.10612](http://arxiv.org/abs/2303.10612)

    本文使用T5 Transformer模型成功地在孟加拉语中检测语法错误，并对模型检测到的错误进行了详细分析，同时探讨了将翻译模型适应语法检测任务的挑战。

    

    本文提出了一种使用基于文本的转换变压器（T5）语言模型检测孟加拉语语法错误的方法，使用精细调整过的BanglaT5的小变种，在一个包含9385个句子的语料库上进行训练，其中错误被专用分界符括起来。T5模型主要设计用于翻译，而不是特别为这个任务设计的，因此需要进行广泛的后处理来使其适应错误检测任务。我们的实验表明，T5模型在检测孟加拉语语法错误时可以实现较低的Levenshtein距离，但是必须进行后处理才能达到最佳性能。在对精细调整的模型输出进行后处理后，最终测试集的平均Levenshtein距离为1.0394。本文还对模型检测到的错误进行了详细分析，并讨论了将翻译模型适应语法检测任务的挑战。我们的方法可以扩展到其他语言，这在成功检测孟加拉语语法错误时得到了证明。

    This paper presents a method for detecting grammatical errors in Bangla using a Text-to-Text Transfer Transformer (T5) Language Model, using the small variant of BanglaT5, fine-tuned on a corpus of 9385 sentences where errors were bracketed by the dedicated demarcation symbol. The T5 model was primarily designed for translation and is not specifically designed for this task, so extensive post-processing was necessary to adapt it to the task of error detection. Our experiments show that the T5 model can achieve low Levenshtein Distance in detecting grammatical errors in Bangla, but post-processing is essential to achieve optimal performance. The final average Levenshtein Distance after post-processing the output of the fine-tuned model was 1.0394 on a test set of 5000 sentences. This paper also presents a detailed analysis of the errors detected by the model and discusses the challenges of adapting a translation model for grammar. Our approach can be extended to other languages, demonst
    
[^3]: 一种模型胜过上万个样本

    A model is worth tens of thousands of examples. (arXiv:2303.10608v1 [cs.LG])

    [http://arxiv.org/abs/2303.10608](http://arxiv.org/abs/2303.10608)

    研究了神经网络与传统基于数学数据生成模型的信号处理方法的数据需求差异，并在两个简单的问题上得到了实证结果。

    

    传统的基于数学数据生成模型的信号处理方法已被深度神经网络所取代，深度神经网络需要大量的数据。由于理论样本复杂度几乎无法评估，这些样本量通常使用粗略的经验法则进行估计。然而，这些规则仅建议网络应该何时起作用，但并不涉及传统方法。特别是，一个有趣的问题是：神经网络需要多少数据才能与传统的基于模型的方法保持同步或者，如果可能的话，超越它们？在本文中，我们通过两个简单的示例来从经验上研究这个问题，其中数据是根据精确定义的数学模型生成的，并且已知最优或最先进的数学数据无关解决方案。

    Traditional signal processing methods relying on mathematical data generation models have been cast aside in favour of deep neural networks, which require vast amounts of data. Since the theoretical sample complexity is nearly impossible to evaluate, these amounts of examples are usually estimated with crude rules of thumb. However, these rules only suggest when the networks should work, but do not relate to the traditional methods. In particular, an interesting question is: how much data is required for neural networks to be on par or outperform, if possible, the traditional model-based methods? In this work, we empirically investigate this question in two simple examples, where the data is generated according to precisely defined mathematical models, and where well-understood optimal or state-of-the-art mathematical data-agnostic solutions are known. A first problem is deconvolving one-dimensional Gaussian signals and a second one is estimating a circle's radius and location in rando
    
[^4]: 利用机器学习技术从血容积脉搏信号自动识别疼痛

    Automatic pain recognition from Blood Volume Pulse (BVP) signal using machine learning techniques. (arXiv:2303.10607v1 [cs.LG])

    [http://arxiv.org/abs/2303.10607](http://arxiv.org/abs/2303.10607)

    本研究利用机器学习技术和血容积脉搏信号设计了一种非侵入式的疼痛感知方式，并建立了模型来检测疼痛的存在和强度。

    

    针对发展自动化疼痛识别感知系统，研究者越来越关注疼痛的生理反应。虽然较少被探索，但血容积脉搏（BVP）是一个可能有助于客观疼痛评估的生理测量候选。本研究中，我们对BVP信号应用机器学习技术，设想了一种非侵入式的疼痛感知方式。32名健康受试者参与了本研究。首先，我们研究了一组新颖的时间域、频率域和非线性动力学特征，这些特征潜在地对疼痛具有敏感性。这些包括来自BVP信号的24个特征和从同一BVP信号中导出的20个额外的心跳间隔（IBI）特征。利用这些特征，我们构建了机器学习模型来检测疼痛的存在和其强度。我们探索了不同的机器学习模型，包括逻辑回归、随机森林、支持向量机、自适应提升（AdaBoost）。

    Physiological responses to pain have received increasing attention among researchers for developing an automated pain recognition sensing system. Though less explored, Blood Volume Pulse (BVP) is one of the candidate physiological measures that could help objective pain assessment. In this study, we applied machine learning techniques on BVP signals to device a non-invasive modality for pain sensing. Thirty-two healthy subjects participated in this study. First, we investigated a novel set of time-domain, frequency-domain and nonlinear dynamics features that could potentially be sensitive to pain. These include 24 features from BVP signals and 20 additional features from Inter-beat Intervals (IBIs) derived from the same BVP signals. Utilizing these features, we built machine learning models for detecting the presence of pain and its intensity. We explored different machine learning models, including Logistic Regression, Random Forest, Support Vector Machines, Adaptive Boosting (AdaBoos
    
[^5]: AdaptGuard: 针对模型适应中的通用攻击进行防御

    AdaptGuard: Defending Against Universal Attacks for Model Adaptation. (arXiv:2303.10594v1 [cs.CR])

    [http://arxiv.org/abs/2303.10594](http://arxiv.org/abs/2303.10594)

    本文研究了模型适应中存在的通用攻击，并提出了一个名为AdaptGuard的模型预处理框架，通过知识蒸馏和伪对抗样本加强目标模型的鲁棒性，提高模型适应算法的安全性。

    

    模型适应旨在解决在仅访问已预训练源模型的约束下的域转移问题。随着对数据隐私和传输效率的越来越多关注，这种范式近年来变得越来越流行。本文研究了在模型适应算法中由于恶意提供方的存在而转移自源域的通用攻击的脆弱性。我们探讨了通用对抗扰动和后门攻击作为源侧漏洞的问题，并发现它们在适应后的目标模型中仍然存在。为了解决这个问题，我们提出了一个名为AdaptGuard的模型预处理框架，以提高模型适应算法的安全性。AdaptGuard通过知识蒸馏避免直接使用风险源参数，并利用调整半径下的伪对抗样本来增强鲁棒性。AdaptGuard是一个即插即用的模块，不需要修改现有的模型适应算法。

    Model adaptation aims at solving the domain transfer problem under the constraint of only accessing the pretrained source models. With the increasing considerations of data privacy and transmission efficiency, this paradigm has been gaining recent popularity. This paper studies the vulnerability to universal attacks transferred from the source domain during model adaptation algorithms due to the existence of the malicious providers. We explore both universal adversarial perturbations and backdoor attacks as loopholes on the source side and discover that they still survive in the target models after adaptation. To address this issue, we propose a model preprocessing framework, named AdaptGuard, to improve the security of model adaptation algorithms. AdaptGuard avoids direct use of the risky source parameters through knowledge distillation and utilizes the pseudo adversarial samples under adjusted radius to enhance the robustness. AdaptGuard is a plug-and-play module that requires neithe
    
[^6]: 巨型移动边缘计算网络中的分层个性化联邦学习

    Hierarchical Personalized Federated Learning Over Massive Mobile Edge Computing Networks. (arXiv:2303.10580v1 [cs.LG])

    [http://arxiv.org/abs/2303.10580](http://arxiv.org/abs/2303.10580)

    本篇论文提出了一种分层个性化联邦学习算法，能够在巨型移动边缘计算网络中处理移动用户设备的异构性，并且实现了训练损失最小化和轮延迟降低的目标权衡。

    

    个性化联邦学习（PFL）是一种新的联邦学习范 paradigm，特别是针对移动边缘计算（MEC）网络中各种移动用户设备（UE）带来的异构问题。但是，由于UE数量不断增加以及带来的复杂行政工作，将PFL算法从其传统的双层框架切换到多层框架是可行的。本文提出了分层个性化联邦学习（HPFL），一种用于在巨型MEC网络中部署PFL的算法。在HPFL中，UE被划分为多个集群，每个集群中的UE同步将其本地更新转发到边缘服务器（ES）进行边缘模型聚合，而ES半异步地将其边缘模型转发到云服务器进行全局模型聚合。上述训练方式在训练损失和轮延迟之间达到了一个权衡。HPFL以分层方式结合了训练损失最小化和轮延迟降低的目标，从而在培训效率和通信开销方面比传统的PFL算法实现了更好的性能。

    Personalized Federated Learning (PFL) is a new Federated Learning (FL) paradigm, particularly tackling the heterogeneity issues brought by various mobile user equipments (UEs) in mobile edge computing (MEC) networks. However, due to the ever-increasing number of UEs and the complicated administrative work it brings, it is desirable to switch the PFL algorithm from its conventional two-layer framework to a multiple-layer one. In this paper, we propose hierarchical PFL (HPFL), an algorithm for deploying PFL over massive MEC networks. The UEs in HPFL are divided into multiple clusters, and the UEs in each cluster forward their local updates to the edge server (ES) synchronously for edge model aggregation, while the ESs forward their edge models to the cloud server semi-asynchronously for global model aggregation. The above training manner leads to a tradeoff between the training loss in each round and the round latency. HPFL combines the objectives of training loss minimization and round 
    
[^7]: 不需要在子图上运行 GNN，使用子图 GNN 高效计数子结构

    Efficiently Counting Substructures by Subgraph GNNs without Running GNN on Subgraphs. (arXiv:2303.10576v1 [cs.LG])

    [http://arxiv.org/abs/2303.10576](http://arxiv.org/abs/2303.10576)

    本文提出了一种使用结构嵌入和预计算的方法，以减少计算和内存成本，并实现了在子图 GNN 上高效计数子结构的目的。

    

    近来，在图学习中使用图神经网络 (GNN) 来近似计算特定函数，如计数图的子结构，是一个热门趋势。在这些工作中，一种常用的方法是使用子图 GNN，将输入图分解为一系列子图，并通过对每个子图应用 GNN 来增强图的表示。尽管子图 GNN 能够计数复杂的子结构，但它们会遭受高计算和内存成本的困扰。本文提出了一个非常规的问题：我们是否能够使用 GNN 高效地计数子结构？为了回答这个问题，我们首先理论上证明，在子图中到根节点的距离是提高子图 GNN 计数能力的关键。然后，我们将这种信息编码为结构嵌入，并预先计算这些嵌入，以避免通过 GNN 反复提取所有子图中的信息。在各种基准测试上的实验表明，所提出的模型可以保持子图 GNN 的计数能力，同时显著降低计算和内存成本。

    Using graph neural networks (GNNs) to approximate specific functions such as counting graph substructures is a recent trend in graph learning. Among these works, a popular way is to use subgraph GNNs, which decompose the input graph into a collection of subgraphs and enhance the representation of the graph by applying GNN to individual subgraphs. Although subgraph GNNs are able to count complicated substructures, they suffer from high computational and memory costs. In this paper, we address a non-trivial question: can we count substructures efficiently with GNNs? To answer the question, we first theoretically show that the distance to the rooted nodes within subgraphs is key to boosting the counting power of subgraph GNNs. We then encode such information into structural embeddings, and precompute the embeddings to avoid extracting information over all subgraphs via GNNs repeatedly. Experiments on various benchmarks show that the proposed model can preserve the counting power of subgra
    
[^8]: 零和矩阵博弈中实例相关样本复杂度的界限

    Instance-dependent Sample Complexity Bounds for Zero-sum Matrix Games. (arXiv:2303.10565v1 [cs.GT])

    [http://arxiv.org/abs/2303.10565](http://arxiv.org/abs/2303.10565)

    本文提出了一种实例相关的界限，定义了一个游戏矩阵排序，可以捕捉到某些游戏动态比其他游戏更快收敛的直觉，从而限定了玩家达到近似均衡所需的最小回合数。

    

    本文研究了识别两人零和 $n\times 2$ 矩阵博弈近似均衡的样本复杂度。即，在一系列重复博弈中，两位玩家必须进行多少回合才能达到近似均衡（例如 Nash 平衡）。我们推导出一种实例相关的界限，定义了一个游戏矩阵排序，捕捉某些游戏动态比其他游戏更快收敛的直觉。具体而言，我们考虑一个随机观察模型，使得当两位玩家分别选择行动 $i$ 和 $j$ 时，他们都能观察到对方的行动以及随机观察 $X_{ij}$，使得 $\mathbb E[ X_{ij}] = A_{ij}$。据我们所知，这是首次以实例相关的方式给出了两位玩家在达到近似均衡之前必须玩的回合数下限的情况，其中回合数取决于游戏矩阵 $A$ 的特定属性以及所需的准确度。我们还p

    We study the sample complexity of identifying an approximate equilibrium for two-player zero-sum $n\times 2$ matrix games. That is, in a sequence of repeated game plays, how many rounds must the two players play before reaching an approximate equilibrium (e.g., Nash)? We derive instance-dependent bounds that define an ordering over game matrices that captures the intuition that the dynamics of some games converge faster than others. Specifically, we consider a stochastic observation model such that when the two players choose actions $i$ and $j$, respectively, they both observe each other's played actions and a stochastic observation $X_{ij}$ such that $\mathbb E[ X_{ij}] = A_{ij}$. To our knowledge, our work is the first case of instance-dependent lower bounds on the number of rounds the players must play before reaching an approximate equilibrium in the sense that the number of rounds depends on the specific properties of the game matrix $A$ as well as the desired accuracy. We also p
    
[^9]: 重新审视 LiDAR 欺骗攻击对于目标检测的能力：改进、测量和新攻击

    Revisiting LiDAR Spoofing Attack Capabilities against Object Detection: Improvements, Measurement, and New Attack. (arXiv:2303.10555v1 [cs.CR])

    [http://arxiv.org/abs/2303.10555](http://arxiv.org/abs/2303.10555)

    本文针对 LiDAR 欺骗攻击在目标检测领域存在的研究差距进行了实证研究，使用9种 LiDAR 和3种目标检测器进行了大规模测量，展示了一种新的攻击模式，提出了安全的 LiDAR 设计和评估方法。

    

    LiDAR（光学遥感）是进行精确长距离和宽范围 3D 感应的不可或缺的传感器，直接造福于自动驾驶技术的快速推广。同时，这种安全关键的应用强烈推动了对其安全性的研究。最近的研究表明，攻击者可以通过向 LiDAR 发送恶意激光来操纵 LiDAR 点云并欺骗目标检测。然而，这些研究面临三个关键的研究难点：（1）仅评估特定的 LiDAR（VLP-16）；（2）假设攻击能力未被验证；以及（3）评估受限的数据集上训练的模型。为了填补这些关键的研究难点，我们对总共9种流行的 LiDAR 和3种主要类型的目标检测器进行了第一次大规模的 LiDAR 欺骗攻击能力测量研究。为了进行这个测量，我们通过更加小心的光学和功能电子学，显著提高了 LiDAR 欺骗能力，展示了一种新的攻击模式，可以甚至规避最先进的防御机制。我们的结果提供了关于如何在不同的实际设置中实际进行 LiDAR 欺骗攻击的见解，并呼吁安全的 LiDAR 设计和评估方法。

    LiDAR (Light Detection And Ranging) is an indispensable sensor for precise long- and wide-range 3D sensing, which directly benefited the recent rapid deployment of autonomous driving (AD). Meanwhile, such a safety-critical application strongly motivates its security research. A recent line of research demonstrates that one can manipulate the LiDAR point cloud and fool object detection by firing malicious lasers against LiDAR. However, these efforts face 3 critical research gaps: (1) evaluating only on a specific LiDAR (VLP-16); (2) assuming unvalidated attack capabilities; and (3) evaluating with models trained on limited datasets.  To fill these critical research gaps, we conduct the first large-scale measurement study on LiDAR spoofing attack capabilities on object detectors with 9 popular LiDARs in total and 3 major types of object detectors. To perform this measurement, we significantly improved the LiDAR spoofing capability with more careful optics and functional electronics, whic
    
[^10]: 基于弹性相互作用能的生成模型：特征空间逼近

    Elastic Interaction Energy-Based Generative Model: Approximation in Feature Space. (arXiv:2303.10553v1 [cs.LG])

    [http://arxiv.org/abs/2303.10553](http://arxiv.org/abs/2303.10553)

    本文提出了一种利用弹性相互作用能的生成模型，采用稳定项解决了GAN训练不稳定的问题，并通过映射数据到特征空间逼近其分布。

    

    本文提出了一种新颖的生成建模方法，使用基于弹性相互作用能（EIE）的损失函数，受晶体缺陷之间的弹性相互作用启发。利用基于EIE的度量具有长程性质，能够考虑分布中的全局信息。此外，它包含一个自交互项，有助于防止模式崩溃并捕获所有分布的模式。为了克服高维数据的相对分散分布的困难，我们首先将数据映射到潜在特征空间中，并逼近特征分布而不是数据分布。我们采用 GAN 框架，并用特征转换网络替换判别器将数据映射到潜在空间。我们还在特征转换网络的损失函数中添加了一个稳定项，有效解决了 GAN 训练不稳定的问题。

    In this paper, we propose a novel approach to generative modeling using a loss function based on elastic interaction energy (EIE), which is inspired by the elastic interaction between defects in crystals. The utilization of the EIE-based metric presents several advantages, including its long range property that enables consideration of global information in the distribution. Moreover, its inclusion of a self-interaction term helps to prevent mode collapse and captures all modes of distribution. To overcome the difficulty of the relatively scattered distribution of high-dimensional data, we first map the data into a latent feature space and approximate the feature distribution instead of the data distribution. We adopt the GAN framework and replace the discriminator with a feature transformation network to map the data into a latent space. We also add a stabilizing term to the loss of the feature transformation network, which effectively addresses the issue of unstable training in GAN-b
    
[^11]: 无监督学习求解旅行商问题

    Unsupervised Learning for Solving the Travelling Salesman Problem. (arXiv:2303.10538v1 [cs.AI])

    [http://arxiv.org/abs/2303.10538](http://arxiv.org/abs/2303.10538)

    无监督学习框架UTSP能够对旅行商问题进行求解，它使用图神经网络作为基础模型，在保证路径为哈密顿循环的前提下，能够找到最短路径。相较于其他方法，UTSP在训练样本与参数数量上占用更少的资源，且性能更佳。

    

    我们提出了UTSP，一种利用无监督学习框架求解旅行商问题（TSP）的方法。我们使用替代损失训练图神经网络（GNN）。GNN输出一个热力图表示每个边成为最优路径的概率。然后，我们应用局部搜索根据热力图生成最终预测。我们的损失函数由两部分组成：一部分推动模型找到最短的路径，另一部分作为约束条件，确保路径形成哈密顿循环。实验结果表明，UTSP优于现有的数据驱动TSP启发式算法。我们的方法参数效率和数据效率均较高：与强化学习或监督学习方法相比，该模型仅占用约10％的参数和约0.2％的训练样本。

    We propose UTSP, an unsupervised learning (UL) framework for solving the Travelling Salesman Problem (TSP). We train a Graph Neural Network (GNN) using a surrogate loss. The GNN outputs a heat map representing the probability for each edge to be part of the optimal path. We then apply local search to generate our final prediction based on the heat map. Our loss function consists of two parts: one pushes the model to find the shortest path and the other serves as a surrogate for the constraint that the route should form a Hamiltonian Cycle. Experimental results show that UTSP outperforms the existing data-driven TSP heuristics. Our approach is parameter efficient as well as data efficient: the model takes $\sim$ 10\% of the number of parameters and $\sim$ 0.2\% of training samples compared with reinforcement learning or supervised learning methods.
    
[^12]: LNO: 用于解微分方程的拉普拉斯神经算子

    LNO: Laplace Neural Operator for Solving Differential Equations. (arXiv:2303.10528v1 [cs.LG])

    [http://arxiv.org/abs/2303.10528](http://arxiv.org/abs/2303.10528)

    LNO是一种用于解微分方程的算法，相比其他算法（如FNO）具有更好的逼近精度并适用于非周期性信号和瞬态响应，能够更好地解释模型并改进泛化能力。

    

    我们引入了拉普拉斯神经算子（LNO），利用拉普拉斯变换对输入空间进行分解。与傅里叶神经算子（FNO）不同，LNO可以处理非周期性信号，考虑瞬态响应，并呈指数收敛。LNO结合了输入和输出空间之间的极点-残差关系，具有更强的可解释性和改进的泛化能力。我们展示了单个拉普拉斯层在逼近三个ODE（Duffing振子、驱动引力摆和Lorenz系统）和三个PDE（Euler-Bernoulli梁、扩散方程和反应扩散系统）的解时，比FNO中的四个傅里叶模块具有更好的逼近精度。值得注意的是，在未阻尼情况下，LNO在捕捉瞬态响应方面优于FNO。对于线性欧拉-伯努利梁和扩散方程， LNO对极点-残差公式的精确表示比FNO产生了显着更好的结果。

    We introduce the Laplace neural operator (LNO), which leverages the Laplace transform to decompose the input space. Unlike the Fourier Neural Operator (FNO), LNO can handle non-periodic signals, account for transient responses, and exhibit exponential convergence. LNO incorporates the pole-residue relationship between the input and the output space, enabling greater interpretability and improved generalization ability. Herein, we demonstrate the superior approximation accuracy of a single Laplace layer in LNO over four Fourier modules in FNO in approximating the solutions of three ODEs (Duffing oscillator, driven gravity pendulum, and Lorenz system) and three PDEs (Euler-Bernoulli beam, diffusion equation, and reaction-diffusion system). Notably, LNO outperforms FNO in capturing transient responses in undamped scenarios. For the linear Euler-Bernoulli beam and diffusion equation, LNO's exact representation of the pole-residue formulation yields significantly better results than FNO. Fo
    
[^13]: 无监督解释性基础抽取用于基于概念的视觉解释

    Unsupervised Interpretable Basis Extraction for Concept-Based Visual Explanations. (arXiv:2303.10523v1 [cs.CV])

    [http://arxiv.org/abs/2303.10523](http://arxiv.org/abs/2303.10523)

    本文提出了一种无监督的方法，通过对CNN进行转换，从而更好地解释中间层的表示，提取了一个可解释性欠完备基础，并证明该方法在各种网络结构和训练数据集上都很有效。

    

    研究人员尝试用人类可以理解的概念来解释CNN图像分类器预测和中间层表示。本文提出了一种无监督后处理方法，通过查找解释像素激活的稀疏二值化转换表示的特征空间旋转来提取解释性欠完备基础。我们对现有的流行CNN进行了实验，并证明了我们方法在网络架构和训练数据集上提取解释性基础的有效性。最后，我们扩展了文献中的基础可解释性度量，并表明，当中间层表示被转换为我们方法提取的基础时，它们变得更易解释。

    An important line of research attempts to explain CNN image classifier predictions and intermediate layer representations in terms of human understandable concepts. In this work, we expand on previous works in the literature that use annotated concept datasets to extract interpretable feature space directions and propose an unsupervised post-hoc method to extract a disentangling interpretable basis by looking for the rotation of the feature space that explains sparse one-hot thresholded transformed representations of pixel activations. We do experimentation with existing popular CNNs and demonstrate the effectiveness of our method in extracting an interpretable basis across network architectures and training datasets. We make extensions to the existing basis interpretability metrics found in the literature and show that, intermediate layer representations become more interpretable when transformed to the bases extracted with our method. Finally, using the basis interpretability metrics
    
[^14]: 参数效率微调的自适应预算分配

    Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning. (arXiv:2303.10512v1 [cs.CL])

    [http://arxiv.org/abs/2303.10512](http://arxiv.org/abs/2303.10512)

    AdaLoRA是一种自适应预算分配方法，用于参数效率微调。将增量更新的预算根据权重矩阵的重要性分数进行自适应分配，通过奇异值分解的形式，实现了微调表现的优化。

    

    在自然语言处理中，对预训练的大型语言模型进行微调已经成为了一种重要的范式。然而，通常的做法是微调预训练模型中的所有参数，当存在大量下游任务时，这种方法变得不切实际。因此，许多微调方法被提出来以以参数有效的方式学习预训练加权的增量更新，例如低秩增量。这些方法通常将增量更新的预算均匀分配到所有预训练的权重矩阵上，忽略了不同权重参数的不同重要性。结果，微调的表现是次优的。为弥补这一差距，我们提出了AdaLoRA，根据它们的重要性分数自适应分配权重矩阵的参数预算。特别地，AdaLoRA将增量更新的参数化为奇异值分解的形式。这种新颖的方法使我们可以有效地剪枝奇异值。

    Fine-tuning large pre-trained language models on downstream tasks has become an important paradigm in NLP. However, common practice fine-tunes all of the parameters in a pre-trained model, which becomes prohibitive when a large number of downstream tasks are present. Therefore, many fine-tuning methods are proposed to learn incremental updates of pre-trained weights in a parameter efficient way, e.g., low-rank increments. These methods often evenly distribute the budget of incremental updates across all pre-trained weight matrices, and overlook the varying importance of different weight parameters. As a consequence, the fine-tuning performance is suboptimal. To bridge this gap, we propose AdaLoRA, which adaptively allocates the parameter budget among weight matrices according to their importance score. In particular, AdaLoRA parameterizes the incremental updates in the form of singular value decomposition. Such a novel approach allows us to effectively prune the singular values of unim
    
[^15]: 面向领域特定语音识别的深度学习系统

    A Deep Learning System for Domain-specific speech Recognition. (arXiv:2303.10510v1 [cs.CL])

    [http://arxiv.org/abs/2303.10510](http://arxiv.org/abs/2303.10510)

    本文提出了一个使用半监督学习注释领域特定数据，基于预训练的声学模型进行微调的ASR系统，并在领域特定上取得了优于商业ASR系统的性能。

    

    随着人机语音接口越来越便捷，许多最先进的自动语音识别（ASR）系统被提出。然而，商业ASR系统通常在领域特定语音，特别是在低资源情况下的表现较差。作者使用预训练的DeepSpeech2和Wav2Vec2声学模型，开发了受益特定的ASR系统。使用半监督学习注释领域特定数据，只需少量人工干预即可。最佳性能来自一种经过微调的Wav2Vec2-Large-LV60声学模型，带有外部KenLM，在受益特定语音上超越了Google和AWS ASR系统。还研究了将容易出错的ASR转录作为口语理解（SLU）的一部分的可行性。受益特定自然语言理解（NLU）任务的结果表明，领域特定微调的ASR系统可以超越商业ASR系统并提高NLU任务的准确性。

    As human-machine voice interfaces provide easy access to increasingly intelligent machines, many state-of-the-art automatic speech recognition (ASR) systems are proposed. However, commercial ASR systems usually have poor performance on domain-specific speech especially under low-resource settings. The author works with pre-trained DeepSpeech2 and Wav2Vec2 acoustic models to develop benefit-specific ASR systems. The domain-specific data are collected using proposed semi-supervised learning annotation with little human intervention. The best performance comes from a fine-tuned Wav2Vec2-Large-LV60 acoustic model with an external KenLM, which surpasses the Google and AWS ASR systems on benefit-specific speech. The viability of using error prone ASR transcriptions as part of spoken language understanding (SLU) is also investigated. Results of a benefit-specific natural language understanding (NLU) task show that the domain-specific fine-tuned ASR system can outperform the commercial ASR sys
    
[^16]: 反应扩散PDE的反步控制器与观测器增益函数的神经算子

    Neural Operators of Backstepping Controller and Observer Gain Functions for Reaction-Diffusion PDEs. (arXiv:2303.10506v1 [eess.SY])

    [http://arxiv.org/abs/2303.10506](http://arxiv.org/abs/2303.10506)

    本文提出了一种使用神经算子学习和逼近设计映射的方法，以消除解决PDE的需要。

    

    与ODE的模型涉及系统矩阵、控制器涉及向量或矩阵增益不同，PDE模型涉及函数系数和空间变量相关的增益函数。PDE反步控制器和观测器的设计是将系统模型函数映射到增益函数的非线性算子。通过学习和逼近这种设计映射的神经算子，可以消除解决PDE的需要。学习神经算子需要足够数量的设计PDE的先前解，以及操作员的训练。

    Unlike ODEs, whose models involve system matrices and whose controllers involve vector or matrix gains, PDE models involve functions in those roles functional coefficients, dependent on the spatial variables, and gain functions dependent on space as well. The designs of gains for controllers and observers for PDEs, such as PDE backstepping, are mappings of system model functions into gain functions. These infinite dimensional nonlinear operators are given in an implicit form through PDEs, in spatial variables, which need to be solved to determine the gain function for each new functional coefficient of the PDE. The need for solving such PDEs can be eliminated by learning and approximating the said design mapping in the form of a neural operator. Learning the neural operator requires a sufficient number of prior solutions for the design PDEs, offline, as well as the training of the operator. In recent work, we developed the neural operators for PDE backstepping designs for first order h
    
[^17]: 通过大型语言模型重新审视整形手术假设

    Revisiting the Plastic Surgery Hypothesis via Large Language Models. (arXiv:2303.10494v1 [cs.SE])

    [http://arxiv.org/abs/2303.10494](http://arxiv.org/abs/2303.10494)

    本论文重新审视了自动程序修复中的整形手术假设，并提出使用大型语言模型进行APR的新方法，主要解决了传统APR工具在不同项目中无法产生多样化修补程序的问题。

    

    自动化程序修复（APR）旨在自动生成输入错误程序的补丁。传统APR工具通常专注于特定的错误类型和修复方式，通过使用模板、启发式和正式规范。然而，这些技术在错误类型和修补程序的多样化方面存在限制。因此，研究人员设计了各种基于学习的APR工具，最近的工作集中在直接使用大型语言模型（LLMs）进行APR。虽然基于LLM的APR工具能够在许多修复数据集上实现最先进的性能，但用于直接修复的LLMs并没有完全了解项目特定信息，如独特的变量或方法名称。整形手术假设是APR的一个著名的见解，它指出修复错误的代码部分通常已经存在于同一项目中。传统的APR工具主要通过设计手动或基于启发的方法来利用整形手术假设。

    Automated Program Repair (APR) aspires to automatically generate patches for an input buggy program. Traditional APR tools typically focus on specific bug types and fixes through the use of templates, heuristics, and formal specifications. However, these techniques are limited in terms of the bug types and patch variety they can produce. As such, researchers have designed various learning-based APR tools with recent work focused on directly using Large Language Models (LLMs) for APR. While LLM-based APR tools are able to achieve state-of-the-art performance on many repair datasets, the LLMs used for direct repair are not fully aware of the project-specific information such as unique variable or method names.  The plastic surgery hypothesis is a well-known insight for APR, which states that the code ingredients to fix the bug usually already exist within the same project. Traditional APR tools have largely leveraged the plastic surgery hypothesis by designing manual or heuristic-based a
    
[^18]: 黑盒变分贝叶斯推理的实用匹配梯度方差界限

    Practical and Matching Gradient Variance Bounds for Black-Box Variational Bayesian Inference. (arXiv:2303.10472v1 [cs.LG])

    [http://arxiv.org/abs/2303.10472](http://arxiv.org/abs/2303.10472)

    本文表明黑盒变分推理（BBVI）满足SGD文献中的ABC条件，该结果适用于平滑和二次增长的对数似然函数，同时我们的结果推广到广泛应用于BBVI实践中的非线性协方差参数化。

    

    理解黑盒变分推理（BBVI）的梯度方差是建立其收敛性和算法改进的关键一步。然而，现有研究尚未表明BBVI的梯度方差满足用于研究随机梯度下降（SGD）收敛的条件。在本文中，我们展示了当应用于平滑和二次增长的对数似然函数时，BBVI满足与SGD文献中使用的ABC条件相匹配的界限。我们的结果推广到广泛应用于BBVI实践中的非线性协方差参数化。此外，我们表明，平均场参数化的方差具有经过验证的优越维度依赖性。

    Understanding the gradient variance of black-box variational inference (BBVI) is a crucial step for establishing its convergence and developing algorithmic improvements. However, existing studies have yet to show that the gradient variance of BBVI satisfies the conditions used to study the convergence of stochastic gradient descent (SGD), the workhorse of BBVI. In this work, we show that BBVI satisfies a matching bound corresponding to the $ABC$ condition used in the SGD literature when applied to smooth and quadratically-growing log-likelihoods. Our results generalize to nonlinear covariance parameterizations widely used in the practice of BBVI. Furthermore, we show that the variance of the mean-field parameterization has provably superior dimensional dependence.
    
[^19]: SPDF：大规模语言模型的稀疏预训练和密集微调

    SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models. (arXiv:2303.10464v1 [cs.LG])

    [http://arxiv.org/abs/2303.10464](http://arxiv.org/abs/2303.10464)

    本文提出了SPDF算法来实现大规模语言模型的高效训练。通过非结构化权重稀疏性来进行预训练，可以降低计算成本，而密集微调则可以保证高性能的表现。

    

    预训练和微调范式为自然语言处理（NLP）的多项突破做出了贡献。语言模型首先在大型数据集上进行跨域知识的预训练（例如，Pile、MassiveText等），然后在特定任务的数据上进行微调（例如，自然语言生成、文本摘要等）。虽然扩大模型和数据集大小有助于提高LLM性能，但这也带来了极为禁止性的计算成本。预训练LLMs通常需要比微调演习更多的FLOPs，两个阶段之间的模型容量通常保持不变。为了实现相对于训练FLOPs的训练效率，我们建议在两个阶段之间解耦模型容量，并引入稀疏预训练和密集微调（SPDF）。在这项工作中，我们展示了使用非结构化权重稀疏性来仅训练子集权重的好处。

    The pre-training and fine-tuning paradigm has contributed to a number of breakthroughs in Natural Language Processing (NLP). Instead of directly training on a downstream task, language models are first pre-trained on large datasets with cross-domain knowledge (e.g., Pile, MassiveText, etc.) and then fine-tuned on task-specific data (e.g., natural language generation, text summarization, etc.). Scaling the model and dataset size has helped improve the performance of LLMs, but unfortunately, this also leads to highly prohibitive computational costs. Pre-training LLMs often require orders of magnitude more FLOPs than fine-tuning and the model capacity often remains the same between the two phases. To achieve training efficiency w.r.t training FLOPs, we propose to decouple the model capacity between the two phases and introduce Sparse Pre-training and Dense Fine-tuning (SPDF). In this work, we show the benefits of using unstructured weight sparsity to train only a subset of weights during 
    
[^20]: 数据同化和不确定性量化在动态系统中的机器学习：一篇综述（arXiv：2303.10462v1 [cs.LG]）

    Machine learning with data assimilation and uncertainty quantification for dynamical systems: a review. (arXiv:2303.10462v1 [cs.LG])

    [http://arxiv.org/abs/2303.10462](http://arxiv.org/abs/2303.10462)

    本文综述了最新的交叉学科研究进展，涵盖了数据同化、不确定性量化和机器学习技术在高维动态系统中的广泛应用。这篇论文对于希望应用DA和UQ技术的机器学习科学家是一份全面的指南。

    

    数据同化（DA）和不确定性量化（UQ）被广泛应用于分析和减少高维空间 - 时间动态中的误差传播。典型应用范围从计算流体力学（CFD）到地球科学和气候系统。最近，很多工作致力于将DA，UQ和机器学习（ML）技术结合起来。这些研究努力解决高维动态系统中的一些关键挑战，包括但不限于动态系统识别，降阶替代建模，误差协方差规范和模型误差校正。许多开发的技术和方法学表现出广泛的适用性，涵盖了许多领域，因此需要一份综合性的指南。本文提供了这一交叉学科领域最新研究的综述，涵盖了广泛的应用范围。该综述旨在面向试图应用DA和UQ技术的机器学习科学家。

    Data Assimilation (DA) and Uncertainty quantification (UQ) are extensively used in analysing and reducing error propagation in high-dimensional spatial-temporal dynamics. Typical applications span from computational fluid dynamics (CFD) to geoscience and climate systems. Recently, much effort has been given in combining DA, UQ and machine learning (ML) techniques. These research efforts seek to address some critical challenges in high-dimensional dynamical systems, including but not limited to dynamical system identification, reduced order surrogate modelling, error covariance specification and model error correction. A large number of developed techniques and methodologies exhibit a broad applicability across numerous domains, resulting in the necessity for a comprehensive guide. This paper provides the first overview of the state-of-the-art researches in this interdisciplinary field, covering a wide range of applications. This review aims at ML scientists who attempt to apply DA and 
    
[^21]: 学习、遗忘和重学：深度神经网络的在线学习范式

    Learn, Unlearn and Relearn: An Online Learning Paradigm for Deep Neural Networks. (arXiv:2303.10455v1 [cs.LG])

    [http://arxiv.org/abs/2303.10455](http://arxiv.org/abs/2303.10455)

    学习、遗忘和重学（LURE）是一种深度神经网络的在线学习范式，它在遗忘阶段和重学阶段之间进行交替，通过有选择地遗忘模型中的不良信息和对泛化性的学习来在效率和泛化之间取得平衡。

    

    深度神经网络（DNN）通常是在提前提供完整的训练数据集的前提下进行训练的。然而，在现实世界的情况下，数据经常会随着时间的推移而以块的形式出现。这就引发了关于训练DNN的最佳策略的重要考虑，例如是否在每个新数据块到达时使用微调的方法（温启动）或者每当有新的数据块可用时从头开始重新训练它们。虽然后者用于训练可以消耗更多的资源，但最近的研究指出了温启动模型的泛化能力缺乏。因此，为了在效率和泛化之间取得平衡，我们介绍了一种深度神经网络的在线学习范式——学习、遗忘和重学（LURE）。LURE在遗忘阶段和重学阶段之间进行交替，遗忘阶段通过数据依赖方式的权重重新初始化有选择地遗忘模型中的不良信息，而重学阶段则强调对泛化性的学习。

    Deep neural networks (DNNs) are often trained on the premise that the complete training data set is provided ahead of time. However, in real-world scenarios, data often arrive in chunks over time. This leads to important considerations about the optimal strategy for training DNNs, such as whether to fine-tune them with each chunk of incoming data (warm-start) or to retrain them from scratch with the entire corpus of data whenever a new chunk is available. While employing the latter for training can be resource-intensive, recent work has pointed out the lack of generalization in warm-start models. Therefore, to strike a balance between efficiency and generalization, we introduce Learn, Unlearn, and Relearn (LURE) an online learning paradigm for DNNs. LURE interchanges between the unlearning phase, which selectively forgets the undesirable information in the model through weight reinitialization in a data-dependent manner, and the relearning phase, which emphasizes learning on generaliza
    
[^22]: 一种用于音频信号处理的内容自适应可学习时频表示法

    A Content Adaptive Learnable Time-Frequency Representation For Audio Signal Processing. (arXiv:2303.10446v1 [cs.SD])

    [http://arxiv.org/abs/2303.10446](http://arxiv.org/abs/2303.10446)

    该论文提出了一种用于音频信号处理的内容自适应可学习时频表示法，通过学习卷积滤波器与变换器架构来将小的波形块投影到小的潜在维度上。

    

    我们提出了一个可学习的内容自适应前端，用于音频信号处理。在深度学习的现代出现之前，我们使用固定表示的、不可学习的前端，如谱图或梅尔谱图，带/不带神经结构。随着卷积架构支持ASR和声学场景理解等各种应用，转向可学习前端，即从头开始学习和优化特定任务所需的基础函数和权重。在没有卷积块的变形器架构中，线性层将小的波形块投影到小的潜在维度上，然后将它们馈送到变形器架构中。在这项工作中，我们提出了一种计算内容自适应学习时频表示的方法。

    We propose a learnable content adaptive front end for audio signal processing. Before the modern advent of deep learning, we used fixed representation non-learnable front-ends like spectrogram or mel-spectrogram with/without neural architectures. With convolutional architectures supporting various applications such as ASR and acoustic scene understanding, a shift to a learnable front ends occurred in which both the type of basis functions and the weight were learned from scratch and optimized for the particular task of interest. With the shift to transformer-based architectures with no convolutional blocks present, a linear layer projects small waveform patches onto a small latent dimension before feeding them to a transformer architecture. In this work, we propose a way of computing a content-adaptive learnable time-frequency representation. We pass each audio signal through a bank of convolutional filters, each giving a fixed-dimensional vector. It is akin to learning a bank of finit
    
[^23]: EarCough：在Hearables上实现连续主题咳嗽事件检测

    EarCough: Enabling Continuous Subject Cough Event Detection on Hearables. (arXiv:2303.10445v1 [cs.SD])

    [http://arxiv.org/abs/2303.10445](http://arxiv.org/abs/2303.10445)

    本文提出了一种名为EarCough的轻量级神经网络模型，可以在Hearables上实现连续主题咳嗽事件检测，准确度高达95.4％，F1分数为92.9％，仅需385 KB的空间。这将成为未来Hearables的低成本增值功能，为咳嗽监测技术提供了新的可能性。

    

    咳嗽监测可以实现新的个体肺健康应用。主题咳嗽事件检测是连续咳嗽监测的基础。近年来，智能可穿戴设备的快速增长为这种需求开辟了新的机会。本文提出了EarCough，通过利用始终处于活动噪声抵消（ANC）模式下的麦克风，使Hearables上实现了连续主题咳嗽事件检测。具体而言，我们提出了一个轻量级的端到端神经网络模型-EarCoughNet。为了评估我们方法的有效性，我们通过用户研究构建了一个同步的运动和音频数据集。结果表明，EarCough在仅需要385 kB的空间要求下，实现了95.4％的准确度和92.9％的F1分数。我们预计EarCough将成为未来Hearables的低成本增值功能，实现连续主题咳嗽事件检测。

    Cough monitoring can enable new individual pulmonary health applications. Subject cough event detection is the foundation for continuous cough monitoring. Recently, the rapid growth in smart hearables has opened new opportunities for such needs. This paper proposes EarCough, which enables continuous subject cough event detection on edge computing hearables by leveraging the always-on active noise cancellation (ANC) microphones. Specifically, we proposed a lightweight end-to-end neural network model -EarCoughNet. To evaluate the effectiveness of our method, we constructed a synchronous motion and audio dataset through a user study. Results show that EarCough achieved an accuracy of 95.4% and an F1-score of 92.9% with a space requirement of only 385 kB. We envision EarCough as a low-cost add-on for future hearables to enable continuous subject cough event detection.
    
[^24]: 低分辨率图像隐私保护与活动识别的权衡建模

    Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images. (arXiv:2303.10435v1 [cs.HC])

    [http://arxiv.org/abs/2303.10435](http://arxiv.org/abs/2303.10435)

    本文研究了利用低分辨率图像传感器的计算机视觉系统中隐私保护与活动识别之间的权衡，通过对图像分辨率的调整和现代图像超分辨技术的应用，帮助设计更有效的隐私保护计算机视觉系统。

    

    利用低分辨率图像传感器的计算机视觉系统可以提供智能服务（例如活动识别），同时从硬件级别保留不必要的视觉隐私信息。然而，保护视觉隐私和实现准确的机器识别对图像分辨率有对抗性的需求。本文通过模拟日常生活活动情境，首先通过用户调查获取了最重要的视觉隐私特征。然后量化和分析了图像分辨率对人和机器识别性能以及隐私意识任务中的影响，并调查了现代图像超分辨技术如何影响这些效果。根据结果，我们提出了一种建模低分辨率图像隐私保护与活动识别权衡的方法，可以帮助设计更有效的隐私保护计算机视觉系统。

    A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy 
    
[^25]: NoisyHate：在人类编写的在线扰动下对内容审核机器学习模型进行基准测试

    NoisyHate: Benchmarking Content Moderation Machine Learning Models with Human-Written Perturbations Online. (arXiv:2303.10430v1 [cs.LG])

    [http://arxiv.org/abs/2303.10430](http://arxiv.org/abs/2303.10430)

    本文提出了一个包含人类编写的在线扰动的测试集，用于毒性言论检测模型的评估。

    

    在社交媒体上，具有有害内容的在线文本是一种威胁，可能会引起网络骚扰。尽管许多平台采取了措施，例如基于机器学习的仇恨言论检测系统来减少其影响，但那些有害内容发布者仍然可以通过修改有害词汇的拼写来逃避系统。这些修改后的单词也称为人类编写的文本扰动。许多研究开发了一定的技术来生成对抗样本，以帮助机器学习模型获得识别这些扰动的能力。然而，机器生成的扰动与人类编写的扰动之间仍存在差距。在本文中，我们介绍了一个包含人类编写的在线扰动的基准测试集，用于毒性言论检测模型。我们还招募了一组工人来评估此测试集的质量并删除低质量的样本。同时，为了检查我们的扰动是否可以归一化为其干净版本，我们还创建了一个相关的测试集。

    Online texts with toxic content are a threat in social media that might cause cyber harassment. Although many platforms applied measures, such as machine learning-based hate-speech detection systems, to diminish their effect, those toxic content publishers can still evade the system by modifying the spelling of toxic words. Those modified words are also known as human-written text perturbations. Many research works developed certain techniques to generate adversarial samples to help the machine learning models obtain the ability to recognize those perturbations. However, there is still a gap between those machine-generated perturbations and human-written perturbations. In this paper, we introduce a benchmark test set containing human-written perturbations online for toxic speech detection models. We also recruited a group of workers to evaluate the quality of this test set and dropped low-quality samples. Meanwhile, to check if our perturbation can be normalized to its clean version, w
    
[^26]: 批量贝叶斯优化在蛋白质序列设计中的应用

    Protein Sequence Design with Batch Bayesian Optimisation. (arXiv:2303.10429v1 [cs.LG])

    [http://arxiv.org/abs/2303.10429](http://arxiv.org/abs/2303.10429)

    本文提出了一种基于批量贝叶斯优化的蛋白质序列设计新方法，能更明智地决定选择哪些序列进行人工进化，实现更好的性能和更快的收敛。

    

    蛋白质序列设计是蛋白工程中具有挑战性的问题，旨在发现具有有用生物功能的新型蛋白质。本文提出了一种基于批量贝叶斯优化（Batch BO）的蛋白质序列设计新方法。通过将Batch BO集成到定向进化过程中，我们的方法能够更明智地决定选择哪些序列进行人工进化，从而实现更好的性能和更快的收敛。我们在一组虚拟蛋白质序列上评估了我们的方法。

    Protein sequence design is a challenging problem in protein engineering, which aims to discover novel proteins with useful biological functions. Directed evolution is a widely-used approach for protein sequence design, which mimics the evolution cycle in a laboratory environment and conducts an iterative protocol. However, the burden of laboratory experiments can be reduced by using machine learning approaches to build a surrogate model of the protein landscape and conducting in-silico population selection through model-based fitness prediction. In this paper, we propose a new method based on Batch Bayesian Optimization (Batch BO), a well-established optimization method, for protein sequence design. By incorporating Batch BO into the directed evolution process, our method is able to make more informed decisions about which sequences to select for artificial evolution, leading to improved performance and faster convergence. We evaluate our method on a suite of in-silico protein sequence
    
[^27]: 发现时间序列预测中可预测的潜在因素

    Discovering Predictable Latent Factors for Time Series Forecasting. (arXiv:2303.10426v1 [cs.LG])

    [http://arxiv.org/abs/2303.10426](http://arxiv.org/abs/2303.10426)

    本文提出了一种基于推断可观察时间序列所暗示的固有潜在因素来进行信号成分分解并重构未来的时间序列预测算法。该算法不仅能够实现长期效率的稀疏关系推理，而且还能够解决时间序列在缺乏足够变量下的建模不稳定和不可预测的问题。

    

    现代时间序列预测方法，如Transformer及其变种，已经在顺序数据建模方面展现出了强大的能力。为了实现高性能，它们通常依靠冗余的或不可解释的结构来建模变量之间复杂的关系，并使用大规模数据调整参数。然而，许多现实世界的数据挖掘任务缺乏足够的变量来进行关系推理，因此这些方法可能无法正确处理这种预测问题。由于缺乏数据，时间序列似乎受到许多外生变量的影响，因此建模变得不稳定和不可预测。为了解决这个关键问题，在本文中，我们开发了一种新的算法框架来推断可观察时间序列所暗示的固有潜在因素。推断出的因素被用来形成多个独立可预测的信号成分，这不仅能够实现长期效率的稀疏关系推理，而且还能重构未来。

    Modern time series forecasting methods, such as Transformer and its variants, have shown strong ability in sequential data modeling. To achieve high performance, they usually rely on redundant or unexplainable structures to model complex relations between variables and tune the parameters with large-scale data. Many real-world data mining tasks, however, lack sufficient variables for relation reasoning, and therefore these methods may not properly handle such forecasting problems. With insufficient data, time series appear to be affected by many exogenous variables, and thus, the modeling becomes unstable and unpredictable. To tackle this critical issue, in this paper, we develop a novel algorithmic framework for inferring the intrinsic latent factors implied by the observable time series. The inferred factors are used to form multiple independent and predictable signal components that enable not only sparse relation reasoning for long-term efficiency but also reconstructing the future
    
[^28]: ExplainFix: 可解释的固定空间深度神经网络

    ExplainFix: Explainable Spatially Fixed Deep Networks. (arXiv:2303.10408v1 [cs.CV])

    [http://arxiv.org/abs/2303.10408](http://arxiv.org/abs/2303.10408)

    ExplainFix采用固定滤波器和精简的网络参数，提高了深度神经网络的可解释性和训练速度。

    

    本研究提出了两个设计原则：卷积神经网络的所有空间滤波器权重可以在初始化时固定，而不必学习；只需很少的网络参数即可。本研究通过可视化模型解释、速度和准确性提升以及新的深度卷积神经网络工具做出了贡献。ExplainFix模型比完全学习的模型少了多达100倍的空间滤波器核，而匹配或提高了准确性。

    Is there an initialization for deep networks that requires no learning? ExplainFix adopts two design principles: the "fixed filters" principle that all spatial filter weights of convolutional neural networks can be fixed at initialization and never learned, and the "nimbleness" principle that only few network parameters suffice. We contribute (a) visual model-based explanations, (b) speed and accuracy gains, and (c) novel tools for deep convolutional neural networks. ExplainFix gives key insights that spatially fixed networks should have a steered initialization, that spatial convolution layers tend to prioritize low frequencies, and that most network parameters are not necessary in spatially fixed models. ExplainFix models have up to 100x fewer spatial filter kernels than fully learned models and matching or improved accuracy. Our extensive empirical analysis confirms that ExplainFix guarantees nimbler models (train up to 17\% faster with channel pruning), matching or improved predict
    
[^29]: 3DQD: 基于部分离散扩散过程的广义深度三维形状先验模型

    3DQD: Generalized Deep 3D Shape Prior via Part-Discretized Diffusion Process. (arXiv:2303.10406v1 [cs.CV])

    [http://arxiv.org/abs/2303.10406](http://arxiv.org/abs/2303.10406)

    该论文提出了一种广义的深度三维形状先验模型，可以用于多种三维任务，其使用向量量化变分自编码器和离散扩散生成器来精确捕获局部精细的形状信息，并引入多频率融合模块来抑制高频形状特征波动，广泛实验表明其在各种三维形状生成任务上表现出优异的性能。

    

    我们开发了一个广义的三维形状生成先验模型，专门针对多种三维任务，包括无条件形状生成、点云完成和跨模态形状生成等。一方面，为了精确捕获局部精细的形状信息，我们使用向量量化变分自编码器（VQ-VAE）来基于广泛的任务训练数据学习紧凑的码本并索引本地几何信息。另一方面，我们引入了离散扩散生成器来建模不同标记之间固有的结构依赖性。同时，我们开发了一个多频率融合模块（MFM），以多频率上下文信息为指导，抑制高频形状特征的波动。上述设计共同配备了我们提出的三维形状先验模型高保真度、多样性特征和跨模态对齐的能力，广泛的实验表明，在各种三维形状生成任务上表现出优异的性能。

    We develop a generalized 3D shape generation prior model, tailored for multiple 3D tasks including unconditional shape generation, point cloud completion, and cross-modality shape generation, etc. On one hand, to precisely capture local fine detailed shape information, a vector quantized variational autoencoder (VQ-VAE) is utilized to index local geometry from a compactly learned codebook based on a broad set of task training data. On the other hand, a discrete diffusion generator is introduced to model the inherent structural dependencies among different tokens. In the meantime, a multi-frequency fusion module (MFM) is developed to suppress high-frequency shape feature fluctuations, guided by multi-frequency contextual information. The above designs jointly equip our proposed 3D shape prior model with high-fidelity, diverse features as well as the capability of cross-modality alignment, and extensive experiments have demonstrated superior performances on various 3D shape generation ta
    
[^30]: 基于可解释AI的智能ROI检测用于阿尔茨海默病预测

    Smart ROI Detection for Alzheimer's disease prediction using explainable AI. (arXiv:2303.10401v1 [eess.IV])

    [http://arxiv.org/abs/2303.10401](http://arxiv.org/abs/2303.10401)

    本论文提出了一种基于可解释AI和Grad-Cam技术的智能方法，可以自动检测ROI以预测阿尔茨海默病。在176名MCI患者上实现并与最先进的方法进行比较，取得了显著的结果。

    

    预测MCI患者发展成阿尔茨海默病是减缓该病进程的重要步骤。因此，许多基于深度学习的方法已被引入到这项任务中。在这些方法中，基于ROI的方法在准确性和复杂性方面处于良好的地位。为了克服这些限制，我们提出了一种基于可解释AI的智能方法来自动检测ROI，并使用提取的ROI进行阿尔茨海默病的预测。

    Purpose Predicting the progression of MCI to Alzheimer's disease is an important step in reducing the progression of the disease. Therefore, many methods have been introduced for this task based on deep learning. Among these approaches, the methods based on ROIs are in a good position in terms of accuracy and complexity. In these techniques, some specific parts of the brain are extracted as ROI manually for all of the patients. Extracting ROI manually is time-consuming and its results depend on human expertness and precision. Method To overcome these limitations, we propose a novel smart method for detecting ROIs automatically based on Explainable AI using Grad-Cam and a 3DCNN model that extracts ROIs per patient. After extracting the ROIs automatically, Alzheimer's disease is predicted using extracted ROI-based 3D CNN. Results We implement our method on 176 MCI patients of the famous ADNI dataset and obtain remarkable results compared to the state-of-the-art methods. The accuracy acqu
    
[^31]: 能源高效的基于蜂窝网络连接的无人机群控制优化

    Energy-Efficient Cellular-Connected UAV Swarm Control Optimization. (arXiv:2303.10398v1 [cs.NI])

    [http://arxiv.org/abs/2303.10398](http://arxiv.org/abs/2303.10398)

    本文提出了一种基于蜂窝网络连接的无人机群控制方案，通过两阶段命令与控制（C&C）传输方案和设备对设备（D2D）通信来提高通信可靠性和能源效率。

    

    基于蜂窝网络连接的无人机群体是一种广泛应用于货运和交通管理等领域的有前途的解决方案。然而，如何高可靠、低延迟、高能效地通信和控制无人机群体仍然是一项挑战。本文在基于蜂窝网络连接的无人机群体中提出了一个两阶段命令与控制（C&C）传输方案，其中地面基站（GBS）在第一阶段广播常规C&C消息。在第二阶段，已成功解码C&C消息的无人机将通过设备对设备（D2D）通信以广播或单播模式将消息中继到其余无人机，以满足延迟和能量限制下尽可能多的无人机成功接收消息。为了找到最优策略使成功接收消息的无人机数最大化，在限制条件下，我们将问题制定为约束马尔可夫决策过程。为了解决此问题，我们提出了一个分散的有约束Q学习算法来学习最优策略。仿真结果表明，我们提出的方案在能源效率和可靠性方面优于现有方案。

    Cellular-connected unmanned aerial vehicle (UAV) swarm is a promising solution for diverse applications, including cargo delivery and traffic control. However, it is still challenging to communicate with and control the UAV swarm with high reliability, low latency, and high energy efficiency. In this paper, we propose a two-phase command and control (C&C) transmission scheme in a cellular-connected UAV swarm network, where the ground base station (GBS) broadcasts the common C&C message in Phase I. In Phase II, the UAVs that have successfully decoded the C&C message will relay the message to the rest of UAVs via device-to-device (D2D) communications in either broadcast or unicast mode, under latency and energy constraints. To maximize the number of UAVs that receive the message successfully within the latency and energy constraints, we formulate the problem as a Constrained Markov Decision Process to find the optimal policy. To address this problem, we propose a decentralized constraine
    
[^32]: 一种基于图表推理的开放式常识问题回答方法

    A Graph-Guided Reasoning Approach for Open-ended Commonsense Question Answering. (arXiv:2303.10395v1 [cs.CL])

    [http://arxiv.org/abs/2303.10395](http://arxiv.org/abs/2303.10395)

    提出了一种基于图表推理的开放式常识问题回答方法，该方法能够处理常识问题的隐式多跳推理并构建与问题相关的开放式知识图，有望在没有预定义答案选项的真实情境应用中发挥作用。

    

    最近，针对多项选择的常识问题回答的端到端训练模型取得了良好的结果。然而，在现实场景中，无法直接应用此类问答系统，因为没有提供答案候选人。因此，最近发布了一个新的基准挑战集（OpenCSR）用于开放式常识推理，其中包含自然科学问题而没有预定义的选项。在OpenCSR挑战集中，许多问题需要隐含的多跳推理并且决策空间很大，反映了这项任务的困难性质。现有的OpenCSR工作仅着眼于改进检索过程，从文本知识库中提取相关事实句子，而将重要且非平凡的推理任务超出范围。在本工作中，我们扩展范围，包括一个基于检索支撑事实构建问题相关开放知识图的推理器，并采用顺序

    Recently, end-to-end trained models for multiple-choice commonsense question answering (QA) have delivered promising results. However, such question-answering systems cannot be directly applied in real-world scenarios where answer candidates are not provided. Hence, a new benchmark challenge set for open-ended commonsense reasoning (OpenCSR) has been recently released, which contains natural science questions without any predefined choices. On the OpenCSR challenge set, many questions require implicit multi-hop reasoning and have a large decision space, reflecting the difficult nature of this task. Existing work on OpenCSR sorely focuses on improving the retrieval process, which extracts relevant factual sentences from a textual knowledge base, leaving the important and non-trivial reasoning task outside the scope. In this work, we extend the scope to include a reasoner that constructs a question-dependent open knowledge graph based on retrieved supporting facts and employs a sequentia
    
[^33]: 基于WFST框架的RNN-Transducer Losses强大且可扩展的实现

    Powerful and Extensible WFST Framework for RNN-Transducer Losses. (arXiv:2303.10384v1 [eess.AS])

    [http://arxiv.org/abs/2303.10384](http://arxiv.org/abs/2303.10384)

    本文提出了一个基于WFST框架的的RNN-Transducer Losses强大且可扩展的实现，“Compose-Transducer”和“Grid-Transducer”，并引入了新的W-Transducer Loss来展示组件的易扩展性。在实验中，W-Transducer（W-RNNT）表现出比标准RNN-T更好的性能。

    

    本文提出了一个基于加权有限状态转移器（WFST）的框架，以简化对RNN-Transducer（RNN-T） Losses的修改开发。现有的RNN-T实现使用与CUDA相关的代码，难以扩展和调试。WFST易于构建和扩展，并允许通过可视化进行调试。我们介绍了两个基于WFST的RNN-T实现：（1）“Compose-Transducer”，它基于声学和文本架构的WFST图组合，计算效率高和易于修改；（2）“Grid-Transducer”，直接构建晶格用于进一步计算，最紧凑和计算效率最高。我们通过引入新的W-Transducer Loss，即Connectionist Temporal Classification with Wild Cards的适应性，展示了组件的易扩展性。在缺少转录开头部分的弱监督数据设置中，W-Transducer（W-RNNT）始终优于标准RNN-T。

    This paper presents a framework based on Weighted Finite-State Transducers (WFST) to simplify the development of modifications for RNN-Transducer (RNN-T) loss. Existing implementations of RNN-T use CUDA-related code, which is hard to extend and debug. WFSTs are easy to construct and extend, and allow debugging through visualization. We introduce two WFST-powered RNN-T implementations: (1) "Compose-Transducer", based on a composition of the WFST graphs from acoustic and textual schema -- computationally competitive and easy to modify; (2) "Grid-Transducer", which constructs the lattice directly for further computations -- most compact, and computationally efficient. We illustrate the ease of extensibility through introduction of a new W-Transducer loss -- the adaptation of the Connectionist Temporal Classification with Wild Cards. W-Transducer (W-RNNT) consistently outperforms the standard RNN-T in a weakly-supervised data setup with missing parts of transcriptions at the beginning and 
    
[^34]: 基于神经加性模型的可解释强化学习在库存管理中的应用

    Interpretable Reinforcement Learning via Neural Additive Models for Inventory Management. (arXiv:2303.10382v1 [cs.LG])

    [http://arxiv.org/abs/2303.10382](http://arxiv.org/abs/2303.10382)

    本文提出了一种基于神经加性模型的可解释强化学习方法，用于开发多级别供应链的动态库存订购策略，在三级供应链仿真测试中证明了实现与最先进深度强化学习方法相当的性能表现，同时具备传统策略的可解释性。

    

    COVID-19疫情彰显了供应链的重要性和数字化管理在应对环境的动态变化中的作用。本文着重于为多级别即多阶段的供应链开发动态库存订购策略。传统的库存优化方法旨在确定静态订购策略，这些策略不能适应如COVID-19危机中观察到的动态变化。然而，传统策略具有可解释性的优势，这是供应链管理者沟通决策与相关方需要具备的关键特征。为了解决这一限制，我们提出了一种可解释性的强化学习方法，既具有传统静态策略的可解释性，又具有其他深度强化学习方法的灵活性和环境无关性。我们建议使用神经加性模型作为库存订购策略的解释函数逼近器。我们的方法在三级供应链仿真中进行了测试，并与传统库存策略以及其他强化学习方法进行了比较。结果表明，我们的方法优于传统策略，并在具有可解释性的同时，实现了与最先进的深度强化学习方法相当的性能。

    The COVID-19 pandemic has highlighted the importance of supply chains and the role of digital management to react to dynamic changes in the environment. In this work, we focus on developing dynamic inventory ordering policies for a multi-echelon, i.e. multi-stage, supply chain. Traditional inventory optimization methods aim to determine a static reordering policy. Thus, these policies are not able to adjust to dynamic changes such as those observed during the COVID-19 crisis. On the other hand, conventional strategies offer the advantage of being interpretable, which is a crucial feature for supply chain managers in order to communicate decisions to their stakeholders. To address this limitation, we propose an interpretable reinforcement learning approach that aims to be as interpretable as the traditional static policies while being as flexible and environment-agnostic as other deep learning-based reinforcement learning solutions. We propose to use Neural Additive Models as an interpr
    
[^35]: 一种加速联邦学习中的客户端选择方法：基于多臂老虎机的方法

    Client Selection for Generalization in Accelerated Federated Learning: A Multi-Armed Bandit Approach. (arXiv:2303.10373v1 [cs.LG])

    [http://arxiv.org/abs/2303.10373](http://arxiv.org/abs/2303.10373)

    本文提出了一种基于多臂老虎机的加速联邦学习中客户端选择方法，通过使用Thompson采样和同步更新实现更快的收敛速度和良好的泛化性能。

    

    联邦学习是一种新兴的机器学习范式，用于跨多个节点（即客户端）在本地数据集中进行模型训练，而无需明确交换数据。 近年来，由于其在隐私考虑和通信资源方面的优势，联邦学习受到了越来越多的关注。 在联邦学习中，所选客户端训练其本地模型并将模型的函数发送到服务器，后者消耗随机的处理和传输时间。 服务器更新全局模型并将其广播回客户端。 联邦学习中的客户端选择问题是在每个给定时间安排一组客户端进行训练和传输，以优化学习性能。本文提出了一种新颖的基于多臂老虎机的客户端选择方法，旨在通过Thompson采样和同步更新的思想来实现更快的收敛速度，同时保持良好的泛化性能，即为新观察结果提供可靠的预测结果。 我们开发了一种名为AGCS（Accelerated Generalized Client Selection）的新算法，该算法在合成数据集和真实数据集上的实验结果证明了我们的方法比现有的最先进方法更有效。

    Federated learning (FL) is an emerging machine learning (ML) paradigm used to train models across multiple nodes (i.e., clients) holding local data sets, without explicitly exchanging the data. It has attracted a growing interest in recent years due to its advantages in terms of privacy considerations, and communication resources. In FL, selected clients train their local models and send a function of the models to the server, which consumes a random processing and transmission time. The server updates the global model and broadcasts it back to the clients. The client selection problem in FL is to schedule a subset of the clients for training and transmission at each given time so as to optimize the learning performance. In this paper, we present a novel multi-armed bandit (MAB)-based approach for client selection to minimize the training latency without harming the ability of the model to generalize, that is, to provide reliable predictions for new observations. We develop a novel alg
    
[^36]: UNREAL: 用于重度不平衡节点分类的未标记节点检索和标记方法

    UNREAL:Unlabeled Nodes Retrieval and Labeling for Heavily-imbalanced Node Classification. (arXiv:2303.10371v1 [cs.LG])

    [http://arxiv.org/abs/2303.10371](http://arxiv.org/abs/2303.10371)

    本文提出了一种用于重度不平衡节点分类的迭代过采样方法UNREAL，通过添加未标记节点而不是合成节点，解决了特征和邻域生成的难题，并利用节点嵌入空间中的无监督学习进行几何排名来有效地校准伪标签分配。

    

    在现实世界的节点分类任务中，极度倾斜的标签分布很常见。如果不合适地处理，这对少数类别的GNNs性能会有极大的影响。由于其实用性，最近一系列的研究都致力于解决这个难题。现有的过采样方法通过产生“假”的少数节点和合成其特征和局部拓扑来平滑标签分布，这在很大程度上忽略了图上未标记节点的丰富信息。在本文中，我们提出了UNREAL，一种迭代过采样方法。第一个关键区别在于，我们只添加未标记节点而不是合成节点，这消除了特征和邻域生成的挑战。为了选择要添加的未标记节点，我们提出了几何排名来对未标记节点进行排名。几何排名利用节点嵌入空间中的无监督学习来有效地校准伪标签分配。最后，我们确定了问题。

    Extremely skewed label distributions are common in real-world node classification tasks. If not dealt with appropriately, it significantly hurts the performance of GNNs in minority classes. Due to its practical importance, there have been a series of recent research devoted to this challenge. Existing over-sampling techniques smooth the label distribution by generating ``fake'' minority nodes and synthesizing their features and local topology, which largely ignore the rich information of unlabeled nodes on graphs. In this paper, we propose UNREAL, an iterative over-sampling method. The first key difference is that we only add unlabeled nodes instead of synthetic nodes, which eliminates the challenge of feature and neighborhood generation. To select which unlabeled nodes to add, we propose geometric ranking to rank unlabeled nodes. Geometric ranking exploits unsupervised learning in the node embedding space to effectively calibrates pseudo-label assignment. Finally, we identify the issu
    
[^37]: CroSel: 用于部分标签学习的自信伪标签的跨选择

    CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label Learning. (arXiv:2303.10365v1 [cs.LG])

    [http://arxiv.org/abs/2303.10365](http://arxiv.org/abs/2303.10365)

    CroSel是一种处理伪标签噪声的新方法，通过利用历史预测信息和一致性正则化项来准确识别部分标签数据的真实标签。

    

    部分标签学习(PLL)是一个重要的弱监督学习问题，它允许每个训练示例有一个候选标签集，而不是一个单一的ground-truth标签。已经广泛探索了基于识别的方法来解决PLL中的标签歧义问题，这些方法将真实标签视为要识别的潜在变量。然而，准确和完整地识别真实标签仍然具有挑战性，这会在模型训练过程中导致伪标签中的噪声。本文提出了一种名为CroSel的新方法，该方法利用模型的历史预测信息来识别大多数训练示例的真实标签。首先，我们引入了一种交叉选择策略，使得两个深度模型可以相互选择部分标记数据的真实标签。此外，我们提出了一种新颖的一致性正则化项co-mix，以避免因虚假选择而引起的样本浪费和微小噪声。通过这种方式，CroSel能够挑选出大多数示例的真实标签。

    Partial-label learning (PLL) is an important weakly supervised learning problem, which allows each training example to have a candidate label set instead of a single ground-truth label. Identification-based methods have been widely explored to tackle label ambiguity issues in PLL, which regard the true label as a latent variable to be identified. However, identifying the true labels accurately and completely remains challenging, causing noise in pseudo labels during model training. In this paper, we propose a new method called CroSel, which leverages historical prediction information from models to identify true labels for most training examples. First, we introduce a cross selection strategy, which enables two deep models to select true labels of partially labeled data for each other. Besides, we propose a novel consistent regularization term called co-mix to avoid sample waste and tiny noise caused by false selection. In this way, CroSel can pick out the true labels of most examples 
    
[^38]: DC-CCL: 设备-云协同控制学习在大型视觉模型中的应用

    DC-CCL: Device-Cloud Collaborative Controlled Learning for Large Vision Models. (arXiv:2303.10361v1 [cs.LG])

    [http://arxiv.org/abs/2303.10361](http://arxiv.org/abs/2303.10361)

    提出了一种名为DC-CCL的设备-云协同控制学习框架，使云端无法直接部署在移动设备上的大型视觉模型仍然可以从设备端局部样本中受益。

    

    许多大型视觉模型已经部署在云端用于实时服务。同时，现场设备不断生成新的样本。如何利用设备端的样本来改进云端的大型模型成为实际需求，但陷入了没有原始样本上行和没有大型模型下行的困境中。本文提出了一种设备-云协同控制学习框架，名为DC-CCL，使云端无法直接部署在移动设备上的大型视觉模型仍然可以从设备端局部样本中受益。

    Many large vision models have been deployed on the cloud for real-time services. Meanwhile, fresh samples are continuously generated on the served mobile device. How to leverage the device-side samples to improve the cloud-side large model becomes a practical requirement, but falls into the dilemma of no raw sample up-link and no large model down-link. Specifically, the user may opt out of sharing raw samples with the cloud due to the concern of privacy or communication overhead, while the size of some large vision models far exceeds the mobile device's runtime capacity. In this work, we propose a device-cloud collaborative controlled learning framework, called DC-CCL, enabling a cloud-side large vision model that cannot be directly deployed on the mobile device to still benefit from the device-side local samples. In particular, DC-CCL vertically splits the base model into two submodels, one large submodel for learning from the cloud-side samples and the other small submodel for learni
    
[^39]: 神经衰弱机器：神经生存回归中超比例危险假设的突破

    Neural Frailty Machine: Beyond proportional hazard assumption in neural survival regressions. (arXiv:2303.10358v1 [cs.LG])

    [http://arxiv.org/abs/2303.10358](http://arxiv.org/abs/2303.10358)

    提出神经衰弱机器（NFM）框架用于生存回归，利用多重衰弱的经典思想来捕捉个体间未观察到的异质性，并能够处理非线性协变量依赖性。两个具体模型下扩展了神经比例危险模型和非参数危险回归模型，结论获得了统计保证。

    

    本文提出神经衰弱机器（NFM），这是一个强大灵活的神经建模框架，用于生存回归。NFM框架利用生存分析中的多重衰弱的经典思想来捕捉个体间未观察到的异质性，并能够利用神经结构的强大逼近能力处理非线性协变量依赖性。框架下推导出两个具体模型，它们分别扩展了神经比例危险模型和非参数危险回归模型。这两个模型都允许在似然目标下进行高效训练。理论上，对于两个提出的模型，我们通过表征其收敛速率，建立了神经函数逼近非参数组件的统计保证。在实证上，我们提供了合成实验来验证我们的理论陈述。我们还在不同规模的6个基准数据集上进行了实验评估，显示出所提出的方法的优越性。

    We present neural frailty machine (NFM), a powerful and flexible neural modeling framework for survival regressions. The NFM framework utilizes the classical idea of multiplicative frailty in survival analysis to capture unobserved heterogeneity among individuals, at the same time being able to leverage the strong approximation power of neural architectures for handling nonlinear covariate dependence. Two concrete models are derived under the framework that extends neural proportional hazard models and nonparametric hazard regression models. Both models allow efficient training under the likelihood objective. Theoretically, for both proposed models, we establish statistical guarantees of neural function approximation with respect to nonparametric components via characterizing their rate of convergence. Empirically, we provide synthetic experiments that verify our theoretical statements. We also conduct experimental evaluations over $6$ benchmark datasets of different scales, showing th
    
[^40]: LossMix：简化和广泛应用 Mixup 于目标检测和更多领域

    LossMix: Simplify and Generalize Mixup for Object Detection and Beyond. (arXiv:2303.10343v1 [cs.CV])

    [http://arxiv.org/abs/2303.10343](http://arxiv.org/abs/2303.10343)

    本论文提出了一种称为 Supervision Interpolation 的新概念框架，通过放松和推广 Mixup 提供了一种全新的插值增强视角，并在此基础上提出了一种名为 LossMix 的简单而多功能的正则化方法，能够增强物体检测器的性能和鲁棒性，或者说LossMix 在目标检测和其他领域中表现出色。

    

    数据混合增强广泛应用于图像分类任务中，但由于空间错位、前景/背景区分以及多个实例的挑战，这些技术不易应用于目标检测。本文提出一种称为监督插值的新概念框架，通过放松和推广 Mixup 提供了一种全新的插值增强视角，然后在这个框架的基础上，提出了 LossMix，这是一种简单而多功能的正则化方法，能够增强物体检测器的性能和鲁棒性。我们的关键insight是，通过插值损失误差来调整训练可以有效规范混合数据的训练，而不是使用ground truth标签。在PASCAL VOC和MS COCO数据集上的实证结果表明，LossMix始终优于当前流行的混合策略，并且我们设计了一种两阶段领域m...

    The success of data mixing augmentations in image classification tasks has been well-received. However, these techniques cannot be readily applied to object detection due to challenges such as spatial misalignment, foreground/background distinction, and plurality of instances. To tackle these issues, we first introduce a novel conceptual framework called Supervision Interpolation, which offers a fresh perspective on interpolation-based augmentations by relaxing and generalizing Mixup. Building on this framework, we propose LossMix, a simple yet versatile and effective regularization that enhances the performance and robustness of object detectors and more. Our key insight is that we can effectively regularize the training on mixed data by interpolating their loss errors instead of ground truth labels. Empirical results on the PASCAL VOC and MS COCO datasets demonstrate that LossMix consistently outperforms currently popular mixing strategies. Furthermore, we design a two-stage domain m
    
[^41]: 最小化编织传感器上的复杂手势识别：迈向真实世界交互系统

    Recognizing Complex Gestures on Minimalistic Knitted Sensors: Toward Real-World Interactive Systems. (arXiv:2303.10336v1 [cs.HC])

    [http://arxiv.org/abs/2303.10336](http://arxiv.org/abs/2303.10336)

    本文介绍了一种新型的数字编织电容式活动传感器，使用神经网络实现对12种复杂手势类的准确分类，并展示了该系统的鲁棒性和适用性。

    

    触控纺织品的发展带来了许多新颖的交互技术和应用。我们的数字编织电容式活动传感器可以在很少的人工干预下大规模制造。它们的敏感区域由单个导电纱线创建，并且只需要与外部硬件进行少量连接。这种技术提高了它们的鲁棒性和可用性，同时将实现交互性的复杂性从硬件转移到了计算模型。本文通过创建一个交互式手势识别系统的基础，提高了这种传感器的功能。它使用了一种新颖的传感器设计和基于神经网络的识别模型，可以对12种相对复杂、单触点手势类进行89.8%的准确分类，展示了未来令人兴奋的应用可能性。我们还通过其佩戴时的性能和洗涤的影响展示了该系统在实际环境中的适用性和鲁棒性。

    Developments in touch-sensitive textiles have enabled many novel interactive techniques and applications. Our digitally-knitted capacitive active sensors can be manufactured at scale with little human intervention. Their sensitive areas are created from a single conductive yarn, and they require only few connections to external hardware. This technique increases their robustness and usability, while shifting the complexity of enabling interactivity from the hardware to computational models. This work advances the capabilities of such sensors by creating the foundation for an interactive gesture recognition system. It uses a novel sensor design, and a neural network-based recognition model to classify 12 relatively complex, single touch point gesture classes with 89.8% accuracy, unfolding many possibilities for future applications. We also demonstrate the system's applicability and robustness to real-world conditions through its performance while being worn and the impact of washing and
    
[^42]: 具有吸引子区域规划器的混合系统神经控制

    Hybrid Systems Neural Control with Region-of-Attraction Planner. (arXiv:2303.10327v1 [cs.RO])

    [http://arxiv.org/abs/2303.10327](http://arxiv.org/abs/2303.10327)

    本文提出了一种基于神经网络的分层方法来控制混合系统，并利用吸引子区域估计器和规划器来确保系统的稳定性，相比传统方法，该方法训练时间更短，性能相同或更好。

    

    混合系统在机器人技术中广泛应用。然而，由于连续和离散动态的复杂性，确保混合系统的稳定性具有挑战性。即使一个具有所有系统模式稳定的系统仍然可能是不稳定的。因此，在模式切换时需要特殊处理以稳定系统。在这项工作中，我们提出了一种分层的、基于神经网络（NN）的控制一般混合系统的方法。对于每个系统模式，我们首先学习NN Lyapunov函数和NN控制器，以确保在吸引子区域（RoA）内的状态可以被稳定地控制。然后在不同模式之间学习RoA NN估计器。在模式切换时，我们提出了一个可微的规划器，以确保切换后的状态可以落入下一个模式的RoA中，从而稳定混合系统。我们提供了新颖的理论稳定性保证，并在汽车跟踪控制、pogobot导航和双足行走机器人运动中进行实验。与传统方法相比，我们的方法只需要0.25倍的训练时间即可找到Lyapunov函数，并且可以实现类似或更好的性能。

    Hybrid systems are prevalent in robotics. However, ensuring the stability of hybrid systems is challenging due to sophisticated continuous and discrete dynamics. A system with all its system modes stable can still be unstable. Hence special treatments are required at mode switchings to stabilize the system. In this work, we propose a hierarchical, neural network (NN)-based method to control general hybrid systems. For each system mode, we first learn an NN Lyapunov function and an NN controller to ensure the states within the region of attraction (RoA) can be stabilized. Then an RoA NN estimator is learned across different modes. Upon mode switching, we propose a differentiable planner to ensure the states after switching can land in next mode's RoA, hence stabilizing the hybrid system. We provide novel theoretical stability guarantees and conduct experiments in car tracking control, pogobot navigation, and bipedal walker locomotion. Our method only requires 0.25X of the training time 
    
[^43]: 伪监督度量：在无监督跨域分类框架中评估无监督图像翻译模型

    Pseudo Supervised Metrics: Evaluating Unsupervised Image to Image Translation Models In Unsupervised Cross-Domain Classification Frameworks. (arXiv:2303.10310v1 [cs.CV])

    [http://arxiv.org/abs/2303.10310](http://arxiv.org/abs/2303.10310)

    本文提出了一种新方法——伪监督度量，用于评估无监督图片到图片翻译模型在无监督跨域分类框架中的性能，并在多个基准数据集上进行了实验。

    

    图像分类的准确性和高效性取决于访问大型标记数据集并在模型训练的相同领域上测试数据。当处理来自不同领域的新数据时，分类变得更加具有挑战性，因为收集大型标记数据集并从头训练新分类器耗时、昂贵，有时是不可行或不可能的。跨域分类框架通过利用无监督图像对图像 (UI2I) 翻译模型将输入图像从未标记的域转换为标记域来处理这个数据域漂移问题。这些无监督模型的问题在于它们是无监督的。由于缺少注释，无法使用传统的监督度量来评估这些翻译模型以选择最佳的检查点模型。在本文中，我们介绍了一种新的方法，称为伪监督度量，专门用于评估无监督跨域分类框架中 UI2I 翻译模型的性能。我们通过对几个基准数据集的实验证明了我们的方法的有效性。

    The ability to classify images accurately and efficiently is dependent on having access to large labeled datasets and testing on data from the same domain that the model is trained on. Classification becomes more challenging when dealing with new data from a different domain, where collecting a large labeled dataset and training a new classifier from scratch is time-consuming, expensive, and sometimes infeasible or impossible. Cross-domain classification frameworks were developed to handle this data domain shift problem by utilizing unsupervised image-to-image (UI2I) translation models to translate an input image from the unlabeled domain to the labeled domain. The problem with these unsupervised models lies in their unsupervised nature. For lack of annotations, it is not possible to use the traditional supervised metrics to evaluate these translation models to pick the best-saved checkpoint model. In this paper, we introduce a new method called Pseudo Supervised Metrics that was desig
    
[^44]: 差分隐私筛选规则的挑战

    The Challenge of Differentially Private Screening Rules. (arXiv:2303.10303v1 [cs.LG])

    [http://arxiv.org/abs/2303.10303](http://arxiv.org/abs/2303.10303)

    本文提出了第一个差分隐私筛选规则，同时发现了由于添加到确保隐私性的噪声量，使用有用的私有筛选规则的任务存在困难。

    

    线性$L_1$正则化模型一直是数据分析中最简单和最有效的工具之一，特别是在信息检索问题中，文本$ n $-gram与TF-IDF或Okapi特征值是一个强大且易于比较的基准。在过去的十年中，筛选规则已经越来越受欢迎，作为减少生成$L_1$模型的稀疏回归权重的运行时间的一种方法。然而，尽管信息检索中需要越来越多的保护隐私的模型，但据我们所知，不存在差分隐私筛选规则。在本文中，我们为线性和逻辑回归开发了第一个差分隐私筛选规则。在这样做的过程中，我们发现由于添加到确保隐私性的噪声量，使用有用的私有筛选规则的任务存在困难。我们提供理论证据和实验证据，表明这种困难源于筛选步骤本身，而不是私有优化器。基于我们的结果，

    Linear $L_1$-regularized models have remained one of the simplest and most effective tools in data analysis, especially in information retrieval problems where n-grams over text with TF-IDF or Okapi feature values are a strong and easy baseline. Over the past decade, screening rules have risen in popularity as a way to reduce the runtime for producing the sparse regression weights of $L_1$ models. However, despite the increasing need of privacy-preserving models in information retrieval, to the best of our knoweledge, no differentially private screening rule exists. In this paper, we develop the first differentially private screening rule for linear and logistic regression. In doing so, we discover difficulties in the task of making a useful private screening rule due to the amount of noise added to ensure privacy. We provide theoretical arguments and experimental evidence that this difficulty arises from the screening step itself and not the private optimizer. Based on our results, we
    
[^45]: 基于2020年安大略省数据的COVID-19病例数量预测

    Forecasting COVID-19 Case Counts Based on 2020 Ontario Data. (arXiv:2303.10294v1 [cs.LG])

    [http://arxiv.org/abs/2303.10294](http://arxiv.org/abs/2303.10294)

    该论文开发了基于机器学习的模型，可以根据过去14天的环境和移动数据预测COVID-19的每日病例数，并在安大略省的四个县进行了测试，最佳模型可以以90.7%的准确率预测明天的每日COVID案例计数，以98.1%的准确率预测7天滚动平均COVID案例计数。

    

    目的：开发可以根据过去14天的环境和移动数据预测每天COVID-19病例数的机器学习模型。 方法：使用来自多伦多周围四个县的COVID-19数据。 数据准备为每天的记录，包含新COVID病例计数、患者人口统计数据、室外天气变量、室内环境因素和基于蜂窝移动和公共卫生限制的人类活动。对这些数据进行分析，以确定最重要的变量及其交互作用。使用CNN和LSTM深度神经网络方法开发预测模型。采用5倍时间交叉验证方法使用从2020年3月1日至2020年10月14日的数据开发预测模型，并在2020年10月15日至2020年12月24日的数据上进行测试。 结果：最佳的LSTM模型可以以90.7%的准确率预测明天的每日COVID案例计数，以98.1%的准确率预测7天滚动平均COVID案例计数。

    Objective: To develop machine learning models that can predict the number of COVID-19 cases per day given the last 14 days of environmental and mobility data.  Approach: COVID-19 data from four counties around Toronto, Ontario, were used. Data were prepared into daily records containing the number of new COVID case counts, patient demographic data, outdoor weather variables, indoor environment factors, and human movement based on cell mobility and public health restrictions. This data was analyzed to determine the most important variables and their interactions. Predictive models were developed using CNN and LSTM deep neural network approaches. A 5-fold chronological cross-validation approach used these methods to develop predictive models using data from Mar 1 to Oct 14 2020, and test them on data covering Oct 15 to Dec 24 2020. Results: The best LSTM models forecasted tomorrow's daily COVID case counts with 90.7% accuracy, and the 7-day rolling average COVID case counts with 98.1% ac
    
[^46]: 固定设计下正则化连续学习的分析

    Fixed Design Analysis of Regularization-Based Continual Learning. (arXiv:2303.10263v1 [cs.LG])

    [http://arxiv.org/abs/2303.10263](http://arxiv.org/abs/2303.10263)

    该论文分析了固定设计下的正则化连续学习问题，提出算法在遗忘和不妥协之间存在权衡，可能会存在灾难性遗忘问题。

    

    我们考虑一个连续学习问题，其中有两个线性回归任务，特征向量被假定为固定的，标签被假定为随机变量。我们考虑一个$\ell_2$-正则化的连续学习算法，它计算一个普通最小二乘参数来拟合第一个数据集，然后计算另一个参数，在$\ell_2$-正则化约束下，拟合第二个数据集并输出第二个参数。对于这个算法，我们提供了两个任务的平均风险的严格界限。我们的风险界限揭示了$\ell_2$-正则化连续学习算法中一个可证明的遗忘和不妥协的权衡关系：使用大的正则化参数，算法输出比较不会遗忘第一个任务的信息，但不愿从第二个任务中提取新信息；反之亦然。我们的结果表明，具有不相似任务的连续学习可能会发生灾难性的遗忘。

    We consider a continual learning (CL) problem with two linear regression tasks in the fixed design setting, where the feature vectors are assumed fixed and the labels are assumed to be random variables. We consider an $\ell_2$-regularized CL algorithm, which computes an Ordinary Least Squares parameter to fit the first dataset, then computes another parameter that fits the second dataset under an $\ell_2$-regularization penalizing its deviation from the first parameter, and outputs the second parameter. For this algorithm, we provide tight bounds on the average risk over the two tasks. Our risk bounds reveal a provable trade-off between forgetting and intransigence of the $\ell_2$-regularized CL algorithm: with a large regularization parameter, the algorithm output forgets less information about the first task but is intransigent to extract new information from the second task; and vice versa. Our results suggest that catastrophic forgetting could happen for CL with dissimilar tasks (u
    
[^47]: 用于随机控制和博弈的机器学习方法的最新发展

    Recent Developments in Machine Learning Methods for Stochastic Control and Games. (arXiv:2303.10257v1 [math.OC])

    [http://arxiv.org/abs/2303.10257](http://arxiv.org/abs/2303.10257)

    本文回顾了基于机器学习的随机控制问题和博弈的计算方法，特别是着重介绍了使用深度学习算法解决高维度和非常复杂结构情况下问题的新方法。

    

    随机最优控制和博弈已经在金融、经济学、社会科学、机器人和能源管理等领域中找到了广泛的应用。许多真实世界的应用都涉及到复杂的模型，这推动了先进的数值方法的发展。最近，基于机器学习的计算方法已经发展用于随机控制问题和博弈。我们回顾这些方法，重点关注已经解锁了高维度和非常复杂结构情况下解决此类问题的可能性的深度学习算法，这是传统数值方法无法完成的。在这里，我们主要考虑连续时间和连续空间设置。许多新方法基于最近用于高维偏微分方程或反向随机微分方程的神经网络方法，或者基于无模型强化学习的马尔科夫决策过程，这导致了突破性的发展。

    Stochastic optimal control and games have found a wide range of applications, from finance and economics to social sciences, robotics and energy management. Many real-world applications involve complex models which have driven the development of sophisticated numerical methods. Recently, computational methods based on machine learning have been developed for stochastic control problems and games. We review such methods, with a focus on deep learning algorithms that have unlocked the possibility to solve such problems even when the dimension is high or when the structure is very complex, beyond what is feasible with traditional numerical methods. Here, we consider mostly the continuous time and continuous space setting. Many of the new approaches build on recent neural-network based methods for high-dimensional partial differential equations or backward stochastic differential equations, or on model-free reinforcement learning for Markov decision processes that have led to breakthrough 
    
[^48]: 利用神经网络和空间分解在电力系统动力学中求解微分代数方程

    Solving Differential-Algebraic Equations in Power Systems Dynamics with Neural Networks and Spatial Decomposition. (arXiv:2303.10256v1 [eess.SY])

    [http://arxiv.org/abs/2303.10256](http://arxiv.org/abs/2303.10256)

    本文提出了一种使用神经网络和空间分解来近似电力系统动力学微分代数方程的方法，旨在加速仿真，提高数值稳定性和精度。

    

    电力系统的动力学由一组微分代数方程描述。时间域仿真用于理解系统动态的演变。由于系统的刚度需要使用精细离散化的时间步长，因此这些仿真可能具有计算代价较高的特点。通过增加允许的时间步长，我们旨在加快这样的仿真。本文使用观察结果，即尽管各个组件使用代数和微分方程来描述，但它们的耦合仅涉及代数方程的观察结果，利用神经网络（NN）来近似组件状态演变，从而产生快速、准确和数值稳定的近似器，使得可以使用更大的时间步长。为了解释网络对组件以及组件对网络的影响，NN将耦合代数变量的时间演化作为其预测的输入。我们最初使用空间分解方法来估计NN，其中系统被分成空间区域，每个区域有单独的NN估计器。我们将基于NN的仿真与传统的数值积分方案进行比较，以展示我们的方法的有效性。

    The dynamics of the power system are described by a system of differential-algebraic equations. Time-domain simulations are used to understand the evolution of the system dynamics. These simulations can be computationally expensive due to the stiffness of the system which requires the use of finely discretized time-steps. By increasing the allowable time-step size, we aim to accelerate such simulations. In this paper, we use the observation that even though the individual components are described using both algebraic and differential equations, their coupling only involves algebraic equations. Following this observation, we use Neural Networks (NNs) to approximate the components' state evolution, leading to fast, accurate, and numerically stable approximators, which enable larger time-steps. To account for effects of the network on the components and vice-versa, the NNs take the temporal evolution of the coupling algebraic variables as an input for their prediction. We initially estima
    
[^49]: 异构网络中联邦支持向量机的多任务模型个性化

    Multi-Task Model Personalization for Federated Supervised SVM in Heterogeneous Networks. (arXiv:2303.10254v1 [cs.LG])

    [http://arxiv.org/abs/2303.10254](http://arxiv.org/abs/2303.10254)

    本文提出了一种用于异构联邦网络中的高效分布式迭代学习方法，通过支持向量机实现对联邦分类和回归任务的处理，并支持个性化的学习模型。为了保护隐私，引入了一种随机掩码过程。研究结果表明，所提出的方法对于解决异构网络中联邦学习任务是有效的。

    

    本文设计了一种基于支持向量机（SVM）的高效分布式迭代学习方法，用于处理联邦分类和回归。该方法支持在异构节点网络中进行高效的计算和模型交换，并允许在存在非独立同分布数据的情况下学习模型的个性化。为进一步提高隐私保护，我们引入了一种随机掩码过程，有助于避免数据反演。最后，我们分析了所提出的隐私机制以及参与者硬件和数据的异构性对系统性能的影响。

    In this paper, we design an efficient distributed iterative learning method based on support vector machines (SVMs), which tackles federated classification and regression. The proposed method supports efficient computations and model exchange in a network of heterogeneous nodes and allows personalization of the learning model in the presence of non-i.i.d. data. To further enhance privacy, we introduce a random mask procedure that helps avoid data inversion. Finally, we analyze the impact of the proposed privacy mechanisms and the heterogeneity of participant hardware and data on the system performance.
    
[^50]: 内在对称性生成模型的三角面上的拟合

    Conformal Generative Modeling on Triangulated Surfaces. (arXiv:2303.10251v1 [cs.LG])

    [http://arxiv.org/abs/2303.10251](http://arxiv.org/abs/2303.10251)

    提出了一种在三角面上进行生成模型的内在对称性生成模型，使用离散共形几何的进展将面网格映射到球形，可以通过任何已经开发为简单流形的生成模型方法进行训练。演示了这种方法在多个流形和多个生成模型子例程上的有效性。

    

    我们提出了一种称为内在对称性生成模型的框架，用于在由离散三角形网格近似的二维面上进行生成建模。我们的方法利用离散共形几何的进展，开发了一种从源三角网格到简单流形（例如球体）的目标三角网格的映射。在考虑网格离散化误差后，我们可以将任何为简单流形开发的生成建模方法作为即插即用的子例程使用。我们在多个复杂的流形和多个生成建模子例程上演示了我们的框架，在那里我们展示了我们的方法可以从样本中学习网格上的分布的良好估计，并且也能同时从同一底层流形的多个不同网格中进行学习。

    We propose conformal generative modeling, a framework for generative modeling on 2D surfaces approximated by discrete triangle meshes. Our approach leverages advances in discrete conformal geometry to develop a map from a source triangle mesh to a target triangle mesh of a simple manifold such as a sphere. After accounting for errors due to the mesh discretization, we can use any generative modeling approach developed for simple manifolds as a plug-and-play subroutine. We demonstrate our framework on multiple complicated manifolds and multiple generative modeling subroutines, where we show that our approach can learn good estimates of distributions on meshes from samples, and can also learn simultaneously from multiple distinct meshes of the same underlying manifold.
    
[^51]: 基于线性Swin Transformer的无人机图像超分辨率LSwinSR

    LSwinSR: UAV Imagery Super-Resolution based on Linear Swin Transformer. (arXiv:2303.10232v1 [eess.IV])

    [http://arxiv.org/abs/2303.10232](http://arxiv.org/abs/2303.10232)

    本文提出一种新型LSwinSR超分辨率算法，基于最先进的Swin Transformer，应用于UAV图像超分辨率处理，同时引入更全面的评估指标F1得分，实验结果证明其比其他最先进算法更具优越性能。

    

    超分辨率技术旨在从低分辨率图像重建高分辨率图像，因其对计算机视觉和遥感社区具有重要意义，并且已经受到广泛关注。 对于无人机（UAV）拍摄的图像，超分辨率技术尤其有益，因为UAV捕获的图像数量和分辨率受物理约束（如飞行高度和载荷能力）的限制。 本文提出了一种基于最先进的Swin Transformer的新型网络，用于UAV图像的超分辨率，具有更高的效率和竞争性的准确性，同时引入了更全面的评估指标F1得分来更好地反映超分辨率算法在实际应用中的性能表现。

    Super-resolution, which aims to reconstruct high-resolution images from low-resolution images, has drawn considerable attention and has been intensively studied in computer vision and remote sensing communities. The super-resolution technology is especially beneficial for Unmanned Aerial Vehicles (UAV), as the amount and resolution of images captured by UAV are highly limited by physical constraints such as flight altitude and load capacity. In the wake of the successful application of deep learning methods in the super-resolution task, in recent years, a series of super-resolution algorithms have been developed. In this paper, for the super-resolution of UAV images, a novel network based on the state-of-the-art Swin Transformer is proposed with better efficiency and competitive accuracy. Meanwhile, as one of the essential applications of the UAV is land cover and land use monitoring, simple image quality assessments such as the Peak-Signal-to-Noise Ratio (PSNR) and the Structural Simi
    
[^52]: 增强神经网络对多样化$\ell_p$攻击的鲁棒性:鲁棒模态连接导向的对抗性防御

    Robust Mode Connectivity-Oriented Adversarial Defense: Enhancing Neural Network Robustness Against Diversified $\ell_p$ Attacks. (arXiv:2303.10225v1 [cs.AI])

    [http://arxiv.org/abs/2303.10225](http://arxiv.org/abs/2303.10225)

    本文提出一种新颖的鲁棒模态连接导向的对抗性防御，实现神经网络对多样化$\ell_p$攻击的鲁棒性，其中包括两个基于种群学习的学习阶段。

    

    对抗性鲁棒性是衡量神经网络在推理阶段抵御对抗性攻击能力的关键概念。最近的研究表明，尽管使用的强化鲁棒性训练技术能够提高对一种类型的攻击的鲁棒性，但模型仍然容易受到多样化的$\ell_p$攻击。为了实现多样化的$\ell_p$鲁棒性，我们提出了一种新颖的鲁棒模态连接 (RMC) 导向的对抗性防御，它包含两个基于种群学习的学习阶段。第一个阶段，RMC，能够搜索两个预先训练模型之间的模型参数空间，并找到包含高鲁棒性点的路径以抵御多样化的$\ell_p$攻击。基于RMC的有效性，我们开发了第二个阶段，基于RMC的优化，其中RMC作为神经网络多样化$\ell_p$鲁棒性进一步增强的基本单元。为了提高计算效率，我们将学习与仅选择子集的对抗性示例相结合，这导致了一组较小的代表性对抗性示例，可用于增强神经网络对多样化$\ell_p$攻击的鲁棒性。

    Adversarial robustness is a key concept in measuring the ability of neural networks to defend against adversarial attacks during the inference phase. Recent studies have shown that despite the success of improving adversarial robustness against a single type of attack using robust training techniques, models are still vulnerable to diversified $\ell_p$ attacks. To achieve diversified $\ell_p$ robustness, we propose a novel robust mode connectivity (RMC)-oriented adversarial defense that contains two population-based learning phases. The first phase, RMC, is able to search the model parameter space between two pre-trained models and find a path containing points with high robustness against diversified $\ell_p$ attacks. In light of the effectiveness of RMC, we develop a second phase, RMC-based optimization, with RMC serving as the basic unit for further enhancement of neural network diversified $\ell_p$ robustness. To increase computational efficiency, we incorporate learning with a sel
    
[^53]: 基于卷标检测的颈部癌症分类方法——使用交叉注意力和潜变换器

    CerviFormer: A Pap-smear based cervical cancer classification method using cross attention and latent transformer. (arXiv:2303.10222v1 [eess.IV])

    [http://arxiv.org/abs/2303.10222](http://arxiv.org/abs/2303.10222)

    本文提出了一种基于交叉注意力和Transfomer的方法，可靠地将PAP检查图像分类为颈癌。对两个公开可用的PAP检查数据集上的分类效果分别为93.70％和97.15％。

    

    目的：颈部癌症是女性死亡的主要原因之一。早期诊断和根据最佳医学建议进行治疗，就像其他疾病一样，以确保其影响尽可能小。 PAP检查图像是识别这种癌症最具建设性的方式之一。本研究提出了一种基于交叉注意力和Transfomer的方法，可靠地将PAP检查图像分类为颈癌。方法：在本研究中，我们提出了CerviFormer模型，该模型依赖于Transfomer，因此对于输入数据的大小仅需最小的架构假设。该模型使用交叉注意力技术，将输入数据重复汇集到一个紧凑的潜在Transformer模块中，从而使其能够管理非常大规模的输入。我们在两个公开可用的PAP检查数据集上评估了我们的模型。结果：对于Sipakmed数据的3状态分类，该模型的准确率为93.70％。对于Herlev数据集上的2状态分类，该模型的准确率为97.15％。结论：我们的研究证明了所提出的CerviFormer模型在将PAP检查图像分类为颈部癌症方面的有效性，这有助于早期检测和治疗这种致命疾病。

    Purpose: Cervical cancer is one of the primary causes of death in women. It should be diagnosed early and treated according to the best medical advice, as with other diseases, to ensure that its effects are as minimal as possible. Pap smear images are one of the most constructive ways for identifying this type of cancer. This study proposes a cross-attention-based Transfomer approach for the reliable classification of cervical cancer in Pap smear images. Methods: In this study, we propose the CerviFormer -- a model that depends on the Transformers and thereby requires minimal architectural assumptions about the size of the input data. The model uses a cross-attention technique to repeatedly consolidate the input data into a compact latent Transformer module, which enables it to manage very large-scale inputs. We evaluated our model on two publicly available Pap smear datasets. Results: For 3-state classification on the Sipakmed data, the model achieved an accuracy of 93.70%. For 2-stat
    
[^54]: 联邦情境赌博算法的实证评估。

    An Empirical Evaluation of Federated Contextual Bandit Algorithms. (arXiv:2303.10218v1 [cs.LG])

    [http://arxiv.org/abs/2303.10218](http://arxiv.org/abs/2303.10218)

    本文通过联邦情境赌博算法对隐式信号进行学习，避免了访问难以获得的显式标签，并发现了简单而常用的softmax启发式在平衡算法方面的惊人有效性。

    

    随着联邦学习在学习用户设备上的敏感数据方面的应用增加，自然而然地会问，是否可以使用隐式信号来进行学习，而不需要访问很难在许多任务中获得的显式标签。我们采用联邦情境赌博算法框架来解决这些问题，并针对联邦设置从中心化的情况下发展出着名的情境赌博算法的变体。我们在使用公开可用的数据集模拟的一系列场景中仔细评估了这些算法。我们的模拟模型通常在现实世界中遇到的设置，例如初始预训练模型和随后用户交互之间的各种不对齐，这是由于数据的非稳定性和/或客户端之间的异质性造成的。我们的实验揭示了简单和常用的softmax启发式在平衡情境赌博算法方面的惊人有效性。

    As the adoption of federated learning increases for learning from sensitive data local to user devices, it is natural to ask if the learning can be done using implicit signals generated as users interact with the applications of interest, rather than requiring access to explicit labels which can be difficult to acquire in many tasks. We approach such problems with the framework of federated contextual bandits, and develop variants of prominent contextual bandit algorithms from the centralized seting for the federated setting. We carefully evaluate these algorithms in a range of scenarios simulated using publicly available datasets. Our simulations model typical setups encountered in the real-world, such as various misalignments between an initial pre-trained model and the subsequent user interactions due to non-stationarity in the data and/or heterogeneity across clients. Our experiments reveal the surprising effectiveness of the simple and commonly used softmax heuristic in balancing 
    
[^55]: 利用蒙特卡罗采样的联盟结构群体解释器逼近

    Approximation of group explainers with coalition structure using Monte Carlo sampling on the product space of coalitions and features. (arXiv:2303.10216v1 [cs.LG])

    [http://arxiv.org/abs/2303.10216](http://arxiv.org/abs/2303.10216)

    本文研究了使用蒙特卡罗采样算法逼近联盟结构群体解释器的复杂问题，提出一种快速、易于实现且与特定模型无关的新型算法，并提供了严密的统计分析及误差界的证明。

    

    近年来，许多机器学习（ML）解释技术都采用了合作博弈理论的思想。这些游戏理论解释器由于复杂度高，在实际环境下无法精确计算。本文采用适当的样本空间估计期望值，设计了一种新型蒙特卡罗采样算法, 来以依赖于背景数据集大小的线性复杂度估计特定机器学习模型和预测向量的边际博弈的联盟值，这种方法快速、易于执行且与其他已知的许多更复杂和特定于模型的估计技术具有类似的统计准确性。我们提供了严密的证明。

    In recent years, many Machine Learning (ML) explanation techniques have been designed using ideas from cooperative game theory. These game-theoretic explainers suffer from high complexity, hindering their exact computation in practical settings. In our work, we focus on a wide class of linear game values, as well as coalitional values, for the marginal game based on a given ML model and predictor vector. By viewing these explainers as expectations over appropriate sample spaces, we design a novel Monte Carlo sampling algorithm that estimates them at a reduced complexity that depends linearly on the size of the background dataset. We set up a rigorous framework for the statistical analysis and obtain error bounds for our sampling methods. The advantage of this approach is that it is fast, easily implementable, and model-agnostic. Furthermore, it has similar statistical accuracy as other known estimation techniques that are more complex and model-specific. We provide rigorous proofs of s
    
[^56]: 深度学习基于脑部MRI到CT合成的对比度泛化探究

    Exploring contrast generalisation in deep learning-based brain MRI-to-CT synthesis. (arXiv:2303.10202v1 [physics.med-ph])

    [http://arxiv.org/abs/2303.10202](http://arxiv.org/abs/2303.10202)

    本文研究了领域随机化（DR）来提高DL模型在生成基于MRI的脑部sCT的泛化能力。

    

    背景：已经提出并在临床上越来越受到认可的合成计算机断层扫描（sCT）以便基于磁共振成像（MRI）的放疗。深度学习（DL）最近展示了从固定MRI采集中生成准确sCT的能力。然而，MRI协议可能随着时间的推移或在不同中心之间有所不同，导致模型泛化能力差而生成低质量的sCT。目的：研究领域随机化（DR）以增加用于生成脑部sCT 的DL模型的泛化能力。方法：收集95名接受放疗的患者的CT和相应的T1加权MRI /不加对比剂，T2加权和FLAIR MRI，其中考虑FLAIR作为研究泛化的未见序列。训练了一个基准生成对抗网络，其中包括或不包括FLAIR序列以测试模型在没有DR的情况下的表现。评估了图像相似性和基于sCT的剂量计划的准确性，以选择最佳表现的DR方法。

    Background: Synthetic computed tomography (sCT) has been proposed and increasingly clinically adopted to enable magnetic resonance imaging (MRI)-based radiotherapy. Deep learning (DL) has recently demonstrated the ability to generate accurate sCT from fixed MRI acquisitions. However, MRI protocols may change over time or differ between centres resulting in low-quality sCT due to poor model generalisation. Purpose: investigating domain randomisation (DR) to increase the generalisation of a DL model for brain sCT generation. Methods: CT and corresponding T1-weighted MRI with/without contrast, T2-weighted, and FLAIR MRI from 95 patients undergoing RT were collected, considering FLAIR the unseen sequence where to investigate generalisation. A ``Baseline'' generative adversarial network was trained with/without the FLAIR sequence to test how a model performs without DR. Image similarity and accuracy of sCT-based dose plans were assessed against CT to select the best-performing DR approach a
    
[^57]: 条件可逆神经网络实现无监督域转移

    Unsupervised Domain Transfer with Conditional Invertible Neural Networks. (arXiv:2303.10191v1 [eess.IV])

    [http://arxiv.org/abs/2303.10191](http://arxiv.org/abs/2303.10191)

    本研究提出了一种基于条件可逆神经网络的无监督域转移方法，通过可逆架构保证循环一致性，避免了模式崩溃问题，在高光谱成像和光声成像方面表现优于最先进的方法。

    

    合成医学图像生成已成为神经网络训练和验证的关键技术。然而，模拟数据和真实数据之间的领域差距仍是一个核心难题。尽管基于循环生成对抗网络和类似架构的深度学习领域转移在该领域取得了实质性进展，但在某些用例中，最先进的方法仍无法生成使相关下游任务产生令人信服结果的训练图像。在本文中，我们通过基于条件可逆神经网络（cINNs）的域转移方法来解决这个问题。作为一个特别的优势，我们的方法通过其可逆架构从本质上保证了循环一致性，而网络训练可以有效地进行最大似然训练。为了展示我们方法的通用适用性，我们将其应用于两个不同尺度的光谱成像模态——高光谱成像（像素级）和光声成像（图像级）。我们提出的方法在视觉真实性和定量性能方面优于最先进的方法，并且关键地避免了模式崩溃问题。

    Synthetic medical image generation has evolved as a key technique for neural network training and validation. A core challenge, however, remains in the domain gap between simulations and real data. While deep learning-based domain transfer using Cycle Generative Adversarial Networks and similar architectures has led to substantial progress in the field, there are use cases in which state-of-the-art approaches still fail to generate training images that produce convincing results on relevant downstream tasks. Here, we address this issue with a domain transfer approach based on conditional invertible neural networks (cINNs). As a particular advantage, our method inherently guarantees cycle consistency through its invertible architecture, and network training can efficiently be conducted with maximum likelihood training. To showcase our method's generic applicability, we apply it to two spectral imaging modalities at different scales, namely hyperspectral imaging (pixel-level) and photoac
    
[^58]: 一种利用机器学习和特征工程方法预测太空物体失控再入的论文研究

    A machine learning and feature engineering approach for the prediction of the uncontrolled re-entry of space objects. (arXiv:2303.10183v1 [cs.LG])

    [http://arxiv.org/abs/2303.10183](http://arxiv.org/abs/2303.10183)

    本论文利用深度学习模型对低轨道下未经控制的太空物体进行再入预测，避免了物理建模误差的影响，具有较高的预测精度。

    

    地球轨道上物体数量的不断增加，预计将伴随着越来越频繁的物体重返地球大气层。许多这样的再入都是无控的，因此预测非常具有挑战性，并且存在多个不确定性。传统上，再入预测是基于使用最先进的模型技术来传播物体动力学，并对作用于物体的力进行建模。然而，与大气阻力预测相关的建模误差可能导致预测精度不佳。在这种情况下，我们探讨了从基于物理的方法向数据驱动方法的范式转变的可能性。为此，我们提出了一种基于序列到序列结构的深度学习模型的开发，用于预测低轨道下（LEO）未经控制的物体的再入。该模型是基于平均高度曲线进行训练的。

    The continuously growing number of objects orbiting around the Earth is expected to be accompanied by an increasing frequency of objects re-entering the Earth's atmosphere. Many of these re-entries will be uncontrolled, making their prediction challenging and subject to several uncertainties. Traditionally, re-entry predictions are based on the propagation of the object's dynamics using state-of-the-art modelling techniques for the forces acting on the object. However, modelling errors, particularly related to the prediction of atmospheric drag may result in poor prediction accuracies. In this context, we explore the possibility to perform a paradigm shift, from a physics-based approach to a data-driven approach. To this aim, we present the development of a deep learning model for the re-entry prediction of uncontrolled objects in Low Earth Orbit (LEO). The model is based on a modified version of the Sequence-to-Sequence architecture and is trained on the average altitude profile as de
    
[^59]: 高维数据的简单、快速和高效特征选择算法SFE

    SFE: A Simple, Fast and Efficient Feature Selection Algorithm for High-Dimensional Data. (arXiv:2303.10182v1 [cs.LG])

    [http://arxiv.org/abs/2303.10182](http://arxiv.org/abs/2303.10182)

    提出一种针对高维数据集的特征选择算法SFE，由搜索代理和两个算子执行探索和开发阶段，对于高维度数据集表现良好，但在降维后性能不能提高

    

    本文提出了一种针对高维数据集的新型特征选择算法——SFE（简单、快速、高效）。SFE算法使用搜索代理和两个算子（非选择和选择）执行搜索过程，包括两个阶段：探索和开发。在探索阶段，非选择算子在整个问题搜索空间中搜索与分类结果不相关、冗余、微不足道和有噪声的特征，并将这些特征的状态从选择模式更改为非选择模式。在开发阶段，选择算子在问题搜索空间中搜索对分类结果影响较大的特征，并将这些特征的状态从非选择模式更改为选择模式。该SFE算法在高维度数据集上表现良好。但是，在降低数据维度之后，它的性能不能显著提高。在这种情况下，需要对降维后的数据集进行进一步的分析。

    In this paper, a new feature selection algorithm, called SFE (Simple, Fast, and Efficient), is proposed for high-dimensional datasets. The SFE algorithm performs its search process using a search agent and two operators: non-selection and selection. It comprises two phases: exploration and exploitation. In the exploration phase, the non-selection operator performs a global search in the entire problem search space for the irrelevant, redundant, trivial, and noisy features, and changes the status of the features from selected mode to non-selected mode. In the exploitation phase, the selection operator searches the problem search space for the features with a high impact on the classification results, and changes the status of the features from non-selected mode to selected mode. The proposed SFE is successful in feature selection from high-dimensional datasets. However, after reducing the dimensionality of a dataset, its performance cannot be increased significantly. In these situations
    
[^60]: 在资源受限的环境中运行关键的机器学习模型

    Operating critical machine learning models in resource constrained regimes. (arXiv:2303.10181v1 [cs.LG])

    [http://arxiv.org/abs/2303.10181](http://arxiv.org/abs/2303.10181)

    本文分享了在关键场景下使用机器学习模型时资源消耗和性能之间的权衡方法。考虑到模型在全球诊所中的部署，机器学习界正在为改进模型效率而努力。

    

    机器学习方法，特别是深度学习，在医学图像分析和计算机辅助干预方面的最新突破，使其得到快速发展。深度学习模型在训练数据，计算和能源成本方面的资源消耗是巨大的。这些巨大的资源成本可能会阻碍这些模型在全球诊所中的部署。为了解决这个问题，机器学习界正在努力引入资源效率的概念。例如，使用量化来减轻内存消耗。虽然大多数这些方法已被证明可以减少资源利用，但可能会以性能为代价。在这项工作中，我们探讨了资源消耗和性能之间的权衡，特别是在诊所等关键环境中使用的模型方面。

    The accelerated development of machine learning methods, primarily deep learning, are causal to the recent breakthroughs in medical image analysis and computer aided intervention. The resource consumption of deep learning models in terms of amount of training data, compute and energy costs are known to be massive. These large resource costs can be barriers in deploying these models in clinics, globally. To address this, there are cogent efforts within the machine learning community to introduce notions of resource efficiency. For instance, using quantisation to alleviate memory consumption. While most of these methods are shown to reduce the resource utilisation, they could come at a cost in performance. In this work, we probe into the trade-off between resource consumption and performance, specifically, when dealing with models that are used in critical settings such as in clinics.
    
[^61]: 使用深度离线强化学习实现安全的丙泊酚全麻剂量控制

    Towards Safe Propofol Dosing during General Anesthesia Using Deep Offline Reinforcement Learning. (arXiv:2303.10180v1 [cs.LG])

    [http://arxiv.org/abs/2303.10180](http://arxiv.org/abs/2303.10180)

    本文提出了一种基于真实临床数据集的数据驱动强化学习算法Policy Constraint Q-Learning(PCQL)来实现全麻药物剂量控制，添加了保守Q-Learning方法和策略约束项以确保智能体做出更安全的决策。

    

    自动化麻醉有望实现更精确和个性化的麻醉管理，使麻醉师免于重复性任务，专注于患者手术护理的最关键方面。当前的研究通常集中于创建模拟环境，以便智能体进行学习。这些方法已经展示出良好的实验结果，但离临床应用还有很大的差距。本文提出了一种基于真实临床数据集的数据驱动强化学习算法Policy Constraint Q-Learning(PCQL)来解决学习麻醉策略的问题。首先引入了保守Q-Learning方法以缓解脱线情况下Q函数过度估计的问题。在智能体的培训中添加一项策略约束项，以保持智能体和麻醉师的策略分布一致，以确保智能体在全麻情景下做出更安全的决策。通过实验证明了PCQL的有效性。

    Automated anesthesia promises to enable more precise and personalized anesthetic administration and free anesthesiologists from repetitive tasks, allowing them to focus on the most critical aspects of a patient's surgical care. Current research has typically focused on creating simulated environments from which agents can learn. These approaches have demonstrated good experimental results, but are still far from clinical application. In this paper, Policy Constraint Q-Learning (PCQL), a data-driven reinforcement learning algorithm for solving the problem of learning anesthesia strategies on real clinical datasets, is proposed. Conservative Q-Learning was first introduced to alleviate the problem of Q function overestimation in an offline context. A policy constraint term is added to agent training to keep the policy distribution of the agent and the anesthesiologist consistent to ensure safer decisions made by the agent in anesthesia scenarios. The effectiveness of PCQL was validated b
    
[^62]: 基于QUBO的分子指纹用于化学性质预测

    QUBO-inspired Molecular Fingerprint for Chemical Property Prediction. (arXiv:2303.10179v1 [cs.LG])

    [http://arxiv.org/abs/2303.10179](http://arxiv.org/abs/2303.10179)

    本研究提出了一种基于QUBO的分子指纹，使用二次无约束二进制优化寻找更有效的交互指纹，并在QM9数据集上取得了成功。

    

    分子指纹广泛用于预测化学性质，选择合适的指纹非常重要。我们基于使用更有效指纹进行预测性能更好的假设，生成了新的指纹。我们生成了有效的交互指纹，它们是多种基础指纹的乘积。由于计算的限制，评估所有交互指纹的组合是困难的。针对这个问题，我们将寻找更有效的交互指纹问题转化为二次无约束二进制优化问题。在本研究中，我们使用QM9数据集发现了有效的交互指纹。

    Molecular fingerprints are widely used for predicting chemical properties, and selecting appropriate fingerprints is important. We generate new fingerprints based on the assumption that a performance of prediction using a more effective fingerprint is better. We generate effective interaction fingerprints that are the product of multiple base fingerprints. It is difficult to evaluate all combinations of interaction fingerprints because of computational limitations. Against this problem, we transform a problem of searching more effective interaction fingerprints into a quadratic unconstrained binary optimization problem. In this study, we found effective interaction fingerprints using QM9 dataset.
    
[^63]: 通过图表自编码器进行内部数据结构的深度非参数估计：广义误差和鲁棒性。

    Deep Nonparametric Estimation of Intrinsic Data Structures by Chart Autoencoders: Generalization Error and Robustness. (arXiv:2303.09863v1 [stat.ML])

    [http://arxiv.org/abs/2303.09863](http://arxiv.org/abs/2303.09863)

    本文介绍了一种图表自编码器用于深度非参数估计内部数据结构，并证明了其广义误差保证和去噪能力。

    

    自编码器在学习高维数据的低维潜在特征方面已经在各种应用中展现出了显着的成功。假设数据在低维流形附近采样，我们采用图表自编码器，将数据编码为一组图表上的低维潜在特征，从而保留了数据流形的拓扑和几何。我们的论文为图表自编码器的广义误差建立了统计保证，并且通过考虑$d$维流形上$n$个带噪声训练样本及其无噪声对应物来展示它们的去噪能力。通过训练自编码器，我们展示了图表自编码器能够有效地去噪输入数据和正态分布噪声。我们证明，在适当的网络架构下，图表自编码器实现了一个大致为$\displaystyle n^{-\frac{2}{d+2}}\log^4 n$阶的平方广义误差，该误差取决于流形的内在维度，并且仅弱依赖于样本数量$n$。

    Autoencoders have demonstrated remarkable success in learning low-dimensional latent features of high-dimensional data across various applications. Assuming that data are sampled near a low-dimensional manifold, we employ chart autoencoders, which encode data into low-dimensional latent features on a collection of charts, preserving the topology and geometry of the data manifold. Our paper establishes statistical guarantees on the generalization error of chart autoencoders, and we demonstrate their denoising capabilities by considering $n$ noisy training samples, along with their noise-free counterparts, on a $d$-dimensional manifold. By training autoencoders, we show that chart autoencoders can effectively denoise the input data with normal noise. We prove that, under proper network architectures, chart autoencoders achieve a squared generalization error in the order of $\displaystyle n^{-\frac{2}{d+2}}\log^4 n$, which depends on the intrinsic dimension of the manifold and only weakly
    
[^64]: SemDeDup:通过语义去重实现网络规模数据的高效学习（arXiv:2303.09540v1 [cs.LG]）

    SemDeDup: Data-efficient learning at web-scale through semantic deduplication. (arXiv:2303.09540v1 [cs.LG])

    [http://arxiv.org/abs/2303.09540](http://arxiv.org/abs/2303.09540)

    SemDeDup是一种利用预训练模型的嵌入来识别和删除语义重复项的方法。通过对LAION的子集进行分析，SemDeDup可以最小化性能损失的同时删除50%的数据，实际上将训练时间减半。此外，SemDeDup在提供效率收益的同时改进了先前的方法。

    

    机器学习领域的进展很大程度上是由海量数据的增加推动的。然而，像LAION这样的大型网络规模数据集在除查找精确重复项外，大部分未经精心筛选，可能存在很多冗余。在这里，我们介绍SemDeDup，一种基于预训练模型的嵌入来识别和删除语义重复项的方法：即语义上相似但并非完全相同的数据对。去除语义重复项可以保持性能并加速学习。通过对LAION的子集进行分析，我们展示了SemDeDup可以最小化性能损失的同时删除50%的数据，实际上将训练时间减半。此外，性能在分布以外得到提高。同时，通过分析在部分筛选过的数据集C4上训练的语言模型，我们展示了SemDeDup在提供效率收益的同时改进了先前的方法。SemDeDup提供了一个利用质量嵌入简单方法来使模型更快地学习的示例。

    Progress in machine learning has been driven in large part by massive increases in data. However, large web-scale datasets such as LAION are largely uncurated beyond searches for exact duplicates, potentially leaving much redundancy. Here, we introduce SemDeDup, a method which leverages embeddings from pre-trained models to identify and remove semantic duplicates: data pairs which are semantically similar, but not exactly identical. Removing semantic duplicates preserves performance and speeds up learning. Analyzing a subset of LAION, we show that SemDeDup can remove 50% of the data with minimal performance loss, effectively halving training time. Moreover, performance increases out of distribution. Also, analyzing language models trained on C4, a partially curated dataset, we show that SemDeDup improves over prior approaches while providing efficiency gains. SemDeDup provides an example of how simple ways of leveraging quality embeddings can be used to make models learn faster with le
    
[^65]: 利用肺部分割增强CT扫描检测COVID-19的存在和严重程度

    Enhanced detection of the presence and severity of COVID-19 from CT scans using lung segmentation. (arXiv:2303.09440v1 [eess.IV])

    [http://arxiv.org/abs/2303.09440](http://arxiv.org/abs/2303.09440)

    本论文介绍了一个深度学习模型，该模型利用肺部分割进行预处理，其验证 F1 分数在预测 CT 扫描中 COVID-19 的存在和严重程度方面显著超过基线水平。

    

    改进医学成像的自动化分析将为临床医生提供更多治疗患者的选择。在2023年的AI-MIA-COV19D 竞赛中，通过 CT 扫描检测 COVID-19 的存在和严重程度的机器学习方法得到了测试和改进。本文介绍了2022年竞赛提交的深度学习模型 Cov3d 的第二个版本，通过对 CT 扫描进行肺部分割和裁剪输入到该区域的预处理步骤对模型进行了改进。在预测 CT 扫描中 COVID-19 存在方面，该模型得到了92.2%的验证宏 F1 分数，明显高于基线74%。它对任务二的验证集预测 COVID-19 的严重程度的宏 F1 分数为67%，高于基线38%。

    Improving automated analysis of medical imaging will provide clinicians more options in providing care for patients. The 2023 AI-enabled Medical Image Analysis Workshop and Covid-19 Diagnosis Competition (AI-MIA-COV19D) provides an opportunity to test and refine machine learning methods for detecting the presence and severity of COVID-19 in patients from CT scans. This paper presents version 2 of Cov3d, a deep learning model submitted in the 2022 competition. The model has been improved through a preprocessing step which segments the lungs in the CT scan and crops the input to this region. It results in a validation macro F1 score for predicting the presence of COVID-19 in the CT scans at 92.2% which is significantly above the baseline of 74%. It gives a macro F1 score for predicting the severity of COVID-19 on the validation set for task 2 as 67% which is above the baseline of 38%.
    
[^66]: 基于生成对抗网络的黑色素瘤个性化艺术治疗方法

    Generative Adversarial Network for Personalized Art Therapy in Melanoma Disease Management. (arXiv:2303.09232v1 [eess.IV])

    [http://arxiv.org/abs/2303.09232](http://arxiv.org/abs/2303.09232)

    该论文提出了一种基于生成对抗网络的黑色素瘤个性化艺术治疗方法，该方法快速生成出个性化的艺术治疗内容，有效减轻患者心理压力。

    

    黑色素瘤是一种最致命的皮肤癌，患者容易患有心理健康疾病，这可能会降低癌症治疗的效果和患者用药计划的遵循性。因此，保护患者在接受治疗时的心理健康至关重要。然而，目前的艺术治疗方法并不是个性化和独特的。我们旨在提供一个训练有素的图像风格转换模型，它可以快速从个人皮肤镜黑色素瘤图像中生成独特的艺术品，作为辅助黑色素瘤病管理中的艺术治疗工具。视觉艺术欣赏是疾病管理中常见的艺术治疗形式，它可以明显减轻心理压力。我们基于循环一致生成对抗网络（cycle-consistent generative adversarial network）开发了一种基于风格转换的网络来生成来自皮肤镜黑色素瘤图像的个性化和独特艺术品。我们开发的模型将黑色素瘤图像转换为独特的花卉主题艺术品

    Melanoma is the most lethal type of skin cancer. Patients are vulnerable to mental health illnesses which can reduce the effectiveness of the cancer treatment and the patients adherence to drug plans. It is crucial to preserve the mental health of patients while they are receiving treatment. However, current art therapy approaches are not personal and unique to the patient. We aim to provide a well-trained image style transfer model that can quickly generate unique art from personal dermoscopic melanoma images as an additional tool for art therapy in disease management of melanoma. Visual art appreciation as a common form of art therapy in disease management that measurably reduces the degree of psychological distress. We developed a network based on the cycle-consistent generative adversarial network for style transfer that generates personalized and unique artworks from dermoscopic melanoma images. We developed a model that converts melanoma images into unique flower-themed artworks 
    
[^67]: 观看或听取：具有视觉损坏建模和可靠性评分的强韧音视频语音识别

    Watch or Listen: Robust Audio-Visual Speech Recognition with Visual Corruption Modeling and Reliability Scoring. (arXiv:2303.08536v1 [cs.MM])

    [http://arxiv.org/abs/2303.08536](http://arxiv.org/abs/2303.08536)

    本论文研究了音视频语音识别在多模态输入损坏情况下的问题，并设计了音视频可靠性评分模块来提高模型的韧性。

    

    本文针对音视频语音识别（AVSR）在多模态输入损坏情况下进行研究，其中音频输入和视觉输入均受损，这在先前的研究方向中没有得到很好的解决。先前的研究集中于如何用清晰的视觉输入来补充受损的音频输入，假设可用清晰的视觉输入，但在现实生活中，清晰的视觉输入并不总是可用的，甚至可能被遮挡的唇部区域或噪音所损坏。

    This paper deals with Audio-Visual Speech Recognition (AVSR) under multimodal input corruption situations where audio inputs and visual inputs are both corrupted, which is not well addressed in previous research directions. Previous studies have focused on how to complement the corrupted audio inputs with the clean visual inputs with the assumption of the availability of clean visual inputs. However, in real life, clean visual inputs are not always accessible and can even be corrupted by occluded lip regions or noises. Thus, we firstly analyze that the previous AVSR models are not indeed robust to the corruption of multimodal input streams, the audio and the visual inputs, compared to uni-modal models. Then, we design multimodal input corruption modeling to develop robust AVSR models. Lastly, we propose a novel AVSR framework, namely Audio-Visual Reliability Scoring module (AV-RelScore), that is robust to the corrupted multimodal inputs. The AV-RelScore can determine which input modal 
    
[^68]: R^2: 基于区间正则化的模型压缩与量化

    R^2: Range Regularization for Model Compression and Quantization. (arXiv:2303.08253v1 [cs.LG])

    [http://arxiv.org/abs/2303.08253](http://arxiv.org/abs/2303.08253)

    R^2提出了一种基于区间正则化的新方法，利用有效的最小值和最大值调整权重分布，从而使模型压缩和量化技术能够更好地利用其数值表示能力。该方法可以提高模型优化的质量，尤其是在较低位上。

    

    参数正则化是一种广泛应用于提高泛化性能的技术，也可用于调整权重分布以达到各种目的。本文探讨了如何利用权重正则化来辅助模型量化和压缩技术，然后提出了区间正则化(R^2)以进一步提高模型优化的质量，重点放在防止异常值方面。通过有效地调整分布中的最小值和最大值，将整个分布塑造成紧凑的形状，从而使模型压缩和量化技术能够更好地利用它们有限的数值表示能力。我们引入了L-inf正则化，其扩展间隔正则化和新的soft-min-max正则化，作为全精度模型训练期间的正则化损失。结合最先进的量化和压缩技术，利用R^2训练的模型在平均水平上表现更好，尤其是在较低位上。

    Model parameter regularization is a widely used technique to improve generalization, but also can be used to shape the weight distributions for various purposes. In this work, we shed light on how weight regularization can assist model quantization and compression techniques, and then propose range regularization (R^2) to further boost the quality of model optimization by focusing on the outlier prevention. By effectively regulating the minimum and maximum weight values from a distribution, we mold the overall distribution into a tight shape so that model compression and quantization techniques can better utilize their limited numeric representation powers. We introduce L-inf regularization, its extension margin regularization and a new soft-min-max regularization to be used as a regularization loss during full-precision model training. Coupled with state-of-the-art quantization and compression techniques, models trained with R^2 perform better on an average, specifically at lower bit 
    
[^69]: 多维数组的多切片聚类中的DBSCAN算法

    DBSCAN of Multi-Slice Clustering for three-order Tensor. (arXiv:2303.07768v1 [cs.LG])

    [http://arxiv.org/abs/2303.07768](http://arxiv.org/abs/2303.07768)

    本文提出了 MSC-DBSCAN扩展算法，可以在三元聚类中从数据中提取不同子空间的不同切片聚类，并可以获得与 MSC 算法在处理秩一张量数据时相同的解决方案。

    

    对于三维数据的三元聚类，现有的几种方法需要指定每个维度的聚类大小或聚类数量。为了解决这个问题，三元聚类(MSC)算法可以在低维子空间中找到保留信号的切片以便基于相似度阈值找到聚类。本文提出了 MSC-DBSCAN扩展算法以从数据中提取位于不同子空间的不同切片聚类(如果数据集是r个秩一张量(r>1)的总和)。我们的算法使用和 MSC 算法相同的输入，可以在处理秩一张量数据时与 MSC 算法获得相同的解决方案。

    Several methods for triclustering three-dimensional data require the cluster size or the number of clusters in each dimension to be specified. To address this issue, the Multi-Slice Clustering (MSC) for 3-order tensor finds signal slices that lie in a low dimensional subspace for a rank-one tensor dataset in order to find a cluster based on the threshold similarity. We propose an extension algorithm called MSC-DBSCAN to extract the different clusters of slices that lie in the different subspaces from the data if the dataset is a sum of r rank-one tensor (r > 1). Our algorithm uses the same input as the MSC algorithm and can find the same solution for rank-one tensor data as MSC.
    
[^70]: 基于自监督学习的StyleGAN图像自动分割算法

    Self-Supervised One-Shot Learning for Automatic Segmentation of StyleGAN Images. (arXiv:2303.05639v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.05639](http://arxiv.org/abs/2303.05639)

    本文提出了一种基于Self-Supervised对比聚类算法的自动分割方法，该方法可以利用StyleGAN生成器中的多尺度隐藏特征对图像进行快速分割，优于半监督基准方法的平均wIoU值1.02%，并提高了推理速度达4.5倍，同时提出了一种通用的一次学习方案，适用于其他GAN生成的图像。

    

    我们提出了一个基于Self-Supervised的框架，用于自动分割生成的StyleGAN图像。在GAN生成器的多尺度隐藏特征中，含有有用的语义信息，可以利用这些特征实现对生成图像的自动分割。使用这些特征，我们的框架使用自监督对比聚类算法来学习分割合成图像，该算法可以将隐藏特征投影到紧凑空间进行像素分类。这种新型的对比学习器是基于使用像素交换预测损失进行图像分割的，从而可以更快地学习用于一次分割的特征向量。我们在多个标准基准测试中测试了我们的实现，得出的分割性能不仅比半监督基准方法平均wIoU高出1.02％，而且提高了推理速度4.5倍。最后，我们提出一种通用的一次学习方案，可以应用于其他GAN生成的图像，从而使我们的框架能够推广到广泛的其他StyleGAN应用中。

    We propose a framework for the automatic one-shot segmentation of synthetic images generated by a StyleGAN. Our framework is based on the observation that the multi-scale hidden features in the GAN generator hold useful semantic information that can be utilized for automatic on-the-fly segmentation of the generated images. Using these features, our framework learns to segment synthetic images using a self-supervised contrastive clustering algorithm that projects the hidden features into a compact space for per-pixel classification. This novel contrastive learner is based on using a pixel-wise swapped prediction loss for image segmentation that leads to faster learning of the feature vectors for one-shot segmentation. We have tested our implementation on a number of standard benchmarks to yield a segmentation performance that not only outperforms the semi-supervised baseline methods by an average wIoU margin of 1.02% but also improves the inference speeds by a factor of 4.5. Finally, we
    
[^71]: 计算连续的强化学习目标是可PAC学习的

    Computably Continuous Reinforcement-Learning Objectives are PAC-learnable. (arXiv:2303.05518v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.05518](http://arxiv.org/abs/2303.05518)

    本研究证明了计算连续的强化学习目标是可PAC学习的，这对于设计高效的强化学习算法打开了新的突破口。

    

    在强化学习中，最大化折扣和有限时间步长累积奖励的经典目标是可PAC学习的：存在算法使用有限数量的样本和计算，高概率地学习到近似最优策略。近年来，研究人员引入了超越经典累积奖励的新目标和相应的强化学习算法，如用线性时态逻辑公式指定的目标。然而，这些新目标的PAC可学习性仍然存在问题。本论文通过在两种分析设置中充分条件的PAC可学习性，证明了强化学习目标的可PAC学习性。特别地，在仅考虑样本复杂性的分析中，我们证明如果一个目标作为一个oracle是一致连续的，那么它是可PAC学习的。此外，在考虑计算复杂度的分析中，我们证明如果一个目标是计算连续的，并满足“统一连续条件”，那么它是可PAC学习的。我们的结果为超越累积奖励的目标的可学习性提供了见解，并为设计这些目标的高效强化学习算法铺平了道路。

    In reinforcement learning, the classic objectives of maximizing discounted and finite-horizon cumulative rewards are PAC-learnable: There are algorithms that learn a near-optimal policy with high probability using a finite amount of samples and computation. In recent years, researchers have introduced objectives and corresponding reinforcement-learning algorithms beyond the classic cumulative rewards, such as objectives specified as linear temporal logic formulas. However, questions about the PAC-learnability of these new objectives have remained open.  This work demonstrates the PAC-learnability of general reinforcement-learning objectives through sufficient conditions for PAC-learnability in two analysis settings. In particular, for the analysis that considers only sample complexity, we prove that if an objective given as an oracle is uniformly continuous, then it is PAC-learnable. Further, for the analysis that considers computational complexity, we prove that if an objective is com
    
[^72]: 多组学习实现群组条件有效性

    Group conditional validity via multi-group learning. (arXiv:2303.03995v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.03995](http://arxiv.org/abs/2303.03995)

    本文提出了一种实现群组条件有效性的方法，通过将问题规约为多组学习问题，从而解决了现有方法中受限于特定分组结构或分布假设等问题。同时，利用一种新的多组学习算法，进一步提高了样本复杂度保证和简化预测结构。

    

    本文研究无分布限制置信度预测问题，着重讨论群组条件有效性的准则。这一准则具有许多实际应用，例如隐藏分层和群组公平。现有的方法要么受限于特定的分组结构或分布假设，要么在异方差噪声下过于保守。我们提出一个简单的规约，将实现群体条件有效性的问题转化为解决一种多组学习问题。这使我们能够通过借助多组学习算法的理论保证来获得置信度预测的样本复杂度保证。我们还提供了一种新的针对具有分层结构的群体的多组学习算法。在我们的规约中使用此算法可带来改进的样本复杂度保证和更简单的预测结构。

    We consider the problem of distribution-free conformal prediction and the criterion of group conditional validity. This criterion is motivated by many practical scenarios including hidden stratification and group fairness. Existing methods achieve such guarantees under either restrictive grouping structure or distributional assumptions, or they are overly-conservative under heteroskedastic noise. We propose a simple reduction to the problem of achieving validity guarantees for individual populations by leveraging algorithms for a problem called multi-group learning. This allows us to port theoretical guarantees from multi-group learning to obtain obtain sample complexity guarantees for conformal prediction. We also provide a new algorithm for multi-group learning for groups with hierarchical structure. Using this algorithm in our reduction leads to improved sample complexity guarantees with a simpler predictor structure.
    
[^73]: 联邦强化学习中的本地环境毒化攻击

    Local Environment Poisoning Attacks on Federated Reinforcement Learning. (arXiv:2303.02725v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02725](http://arxiv.org/abs/2303.02725)

    本文提出了第一个广义框架，将FRL的毒化攻击视为限制预算的优化问题，并设计了一个可以应用于基于策略的FRL的毒化攻击协议，通过训练一对私人评价者和公共评价者将其扩展到使用演员-评论家作为本地RL算法的FRL。

    

    联邦学习已成为解决传统强化学习任务的热门工具。多代理结构解决了传统强化学习中数据需求大的主要问题，而联邦机制保护了各个代理个体的数据隐私。然而，联邦机制也会暴露系统面临恶意代理的毒化攻击。本工作提出了第一个广义框架，将FRL的毒化攻击视为限制预算的优化问题，并设计了一个可以应用于基于策略的FRL的毒化攻击协议，并通过训练一对私人评价者和公共评价者将其扩展到使用演员-评论家作为本地RL算法的FRL。我们也讨论了从FL继承的一种传统防御策略以减轻这种风险。我们通过进行实验来验证我们的毒化攻击的有效性。

    Federated learning (FL) has become a popular tool for solving traditional Reinforcement Learning (RL) tasks. The multi-agent structure addresses the major concern of data-hungry in traditional RL, while the federated mechanism protects the data privacy of individual agents. However, the federated mechanism also exposes the system to poisoning by malicious agents that can mislead the trained policy. Despite the advantage brought by FL, the vulnerability of Federated Reinforcement Learning (FRL) has not been well-studied before. In this work, we propose the first general framework to characterize FRL poisoning as an optimization problem constrained by a limited budget and design a poisoning protocol that can be applied to policy-based FRL and extended to FRL with actor-critic as a local RL algorithm by training a pair of private and public critics. We also discuss a conventional defense strategy inherited from FL to mitigate this risk. We verify our poisoning effectiveness by conducting 
    
[^74]: Seq-HyGAN: 基于超图注意力网络的序列分类

    Seq-HyGAN: Sequence Classification via Hypergraph Attention Network. (arXiv:2303.02393v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02393](http://arxiv.org/abs/2303.02393)

    本文提出了一种基于超图注意力网络的序列分类模型Seq-HyGAN，通过创建超图和引入注意力机制来处理序列数据中的复杂结构相似性，从而提高分类准确率。

    

    序列分类在不同领域有广泛的实际应用，例如在健康领域中的基因组分类和在商业领域的异常检测。然而，序列数据中缺乏显式的特征，这使得机器学习模型难以处理。虽然神经网络模型通过自动学习特征来解决这个问题，但它们仅限于捕获相邻结构连接并忽略序列之间的全局、高阶信息。为了解决序列分类问题中的这些挑战，我们提出了一种新的超图注意力网络模型——Seq-HyGAN。为了捕捉序列数据之间的复杂结构相似性，我们首先创建一个超图，其中序列被描绘为超边，从序列中提取的子序列被描绘为节点。此外，我们引入了基于注意力的超图神经网络模型，它利用了双层注意力机制。该模型生成一个序列表示

    Sequence classification has a wide range of real-world applications in different domains, such as genome classification in health and anomaly detection in business. However, the lack of explicit features in sequence data makes it difficult for machine learning models. While Neural Network (NN) models address this with learning features automatically, they are limited to capturing adjacent structural connections and ignore global, higher-order information between the sequences. To address these challenges in the sequence classification problems, we propose a novel Hypergraph Attention Network model, namely Seq-HyGAN. To capture the complex structural similarity between sequence data, we first create a hypergraph where the sequences are depicted as hyperedges and subsequences extracted from sequences are depicted as nodes. Additionally, we introduce an attention-based Hypergraph Neural Network model that utilizes a two-level attention mechanism. This model generates a sequence representa
    
[^75]: 利用早期退出进行深度神经网络的分层训练

    Hierarchical Training of Deep Neural Networks Using Early Exiting. (arXiv:2303.02384v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.02384](http://arxiv.org/abs/2303.02384)

    本文提出了一种使用早期退出的分层训练方法，将深度神经网络分为边缘和云工作者，以减少通信成本、训练运行时间和隐私问题。

    

    深度神经网络提供了视觉任务的最先进准确性，但需要大量资源进行训练。因此，它们在远离获取数据的边缘设备上的云服务器上进行训练。这增加了通信成本、运行时间和隐私问题。本研究提出了一种新的深度神经网络分层训练方法，它使用早期退出在边缘和云工作者之间分割架构，以减少通信成本、训练运行时间和隐私问题。

    Deep neural networks provide state-of-the-art accuracy for vision tasks but they require significant resources for training. Thus, they are trained on cloud servers far from the edge devices that acquire the data. This issue increases communication cost, runtime and privacy concerns. In this study, a novel hierarchical training method for deep neural networks is proposed that uses early exits in a divided architecture between edge and cloud workers to reduce the communication cost, training runtime and privacy concerns. The method proposes a brand-new use case for early exits to separate the backward pass of neural networks between the edge and the cloud during the training phase. We address the issues of most available methods that due to the sequential nature of the training phase, cannot train the levels of hierarchy simultaneously or they do it with the cost of compromising privacy. In contrast, our method can use both edge and cloud workers simultaneously, does not share the raw i
    
[^76]: 使用机器学习发现和识别数学公式概念

    Discovery and Recognition of Formula Concepts using Machine Learning. (arXiv:2303.01994v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2303.01994](http://arxiv.org/abs/2303.01994)

    本文提出了一种使用机器学习来自动识别和发现科学文献中数学概念的方法，包括公式概念发现和公式概念识别两个子任务，通过对arXiv子集中1.8M个公式出现进行实验，结果显示该方法优于强基线。

    

    基于引用的信息检索方法已经被证明可以在需要引用大量文献的学术学科中有效地进行作弊检测或文献推荐系统等信息检索应用。在科学、技术、工程和数学领域，研究人员通常通过公式符号来引用先前的知识。我们的长期目标是将基于引用的信息检索方法推广，并将其应用于经典参考文献和数学概念。本文提出了如何引用数学公式，并定义了一个“公式概念检索任务”，其中包括公式概念发现（Formula Concept Discovery，FCD）和公式概念识别（Formula Concept Recognition，FCR）两个子任务。FCD旨在定义和探索命名为绑定等效表示的“公式概念”，而FCR旨在将给定的公式匹配到先前分配的唯一数学概念标识符上。我们提出了基于机器学习的方法来解决这两个任务，并在一个arXiv子集的1.8M个公式出现中评估了我们的模型。我们的结果显示，我们提出的方法优于强基线，并为科学文献中的数学概念的自动识别和发现提供了有希望的步骤。

    Citation-based Information Retrieval (IR) methods for scientific documents have proven effective for IR applications, such as Plagiarism Detection or Literature Recommender Systems in academic disciplines that use many references. In science, technology, engineering, and mathematics, researchers often employ mathematical concepts through formula notation to refer to prior knowledge. Our long-term goal is to generalize citation-based IR methods and apply this generalized method to both classical references and mathematical concepts. In this paper, we suggest how mathematical formulas could be cited and define a Formula Concept Retrieval task with two subtasks: Formula Concept Discovery (FCD) and Formula Concept Recognition (FCR). While FCD aims at the definition and exploration of a 'Formula Concept' that names bundled equivalent representations of a formula, FCR is designed to match a given formula to a prior assigned unique mathematical concept identifier. We present machine learning-
    
[^77]: ABAW: 表情情感估计、表情识别、肌肉动作检测和情感反应强度估计挑战

    ABAW: Valence-Arousal Estimation, Expression Recognition, Action Unit Detection & Emotional Reaction Intensity Estimation Challenges. (arXiv:2303.01498v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.01498](http://arxiv.org/abs/2303.01498)

    本比赛提供两个数据集，包含视频和声音数据，针对表情情感估计、表情识别、肌肉动作检测和情感反应强度估计四个任务进行挑战。参赛者需要提供能在复杂、不受控制的环境下运行的有效模型。

    

    第五届情感行为分析(ABAW)比赛是该领域相应的ABAW研讨会的一部分，该研讨会将与IEEE计算机视觉和模式识别会议(CVPR)2023年联合举办。ABAW 5比赛是ECCV 2022、IEEE CVPR 2022、ICCV 2021、IEEE FG 2020和CVPR 2017会议上举办的比赛的延续，旨在自动分析情感。为今年的比赛，我们提供了两个数据集：i) Aff-Wild2数据库的扩展版本和ii) Hume-Reaction数据库。前者是一个包含约600个视频、约300万帧的视听数据集，针对以下方面进行了注释：a) 两个连续的情感维度——情感价值(一个人的积极/消极程度)和情感唤醒度(一个人的活跃/被动程度)——；b) 基本表情(例如快乐、悲伤、中性状态)；和c) 原子人脸肌肉动作(即，动作单元)。后者数据集捕捉了个体对自然环境中事件的反应。ABAW比赛提出了三个子任务：情感价值-唤醒度估计、表情识别和动作单元检测，以及一个新的子任务，情感反应强度估计。参赛团队需要开发和评估能在具有挑战性和较少控制的环境中运行的模型。

    The fifth Affective Behavior Analysis in-the-wild (ABAW) Competition is part of the respective ABAW Workshop which will be held in conjunction with IEEE Computer Vision and Pattern Recognition Conference (CVPR), 2023. The 5th ABAW Competition is a continuation of the Competitions held at ECCV 2022, IEEE CVPR 2022, ICCV 2021, IEEE FG 2020 and CVPR 2017 Conferences, and is dedicated at automatically analyzing affect. For this year's Competition, we feature two corpora: i) an extended version of the Aff-Wild2 database and ii) the Hume-Reaction dataset. The former database is an audiovisual one of around 600 videos of around 3M frames and is annotated with respect to:a) two continuous affect dimensions -valence (how positive/negative a person is) and arousal (how active/passive a person is)-; b) basic expressions (e.g. happiness, sadness, neutral state); and c) atomic facial muscle actions (i.e., action units). The latter dataset is an audiovisual one in which reactions of individuals to e
    
[^78]: AugGPT：利用ChatGPT进行文本数据增强

    AugGPT: Leveraging ChatGPT for Text Data Augmentation. (arXiv:2302.13007v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.13007](http://arxiv.org/abs/2302.13007)

    AugGPT提出了一种基于ChatGPT的文本数据增强方法，该方法能够更忠实地保留正确标记的生成数据并提供足够的多样性，从而有效地缓解了自然语言处理任务中的限制样本量的问题。

    

    文本数据增强是受限制的样本量在许多自然语言处理（NLP）任务中克服挑战的有效策略。对于少样本学习场景，其中目标域的数据通常更加稀缺且质量更低，这一挑战特别突出。为了更好地捕捉数据不变性并增加样本量，缓解这种挑战的一种自然且广泛使用的策略是执行数据增强。然而，当前的文本数据增强方法要么无法保证生成数据的正确标记（缺乏忠实度），要么无法保证生成的数据具有足够的多样性（缺乏紧凑性），或者两者都有。在大语言模型的最近成功，尤其是在开发ChatGPT方面展示了改进的语言理解能力的基础上，本文提出了一种基于ChatGPT（名为AugGPT）的文本数据增强方法。

    Text data augmentation is an effective strategy for overcoming the challenge of limited sample sizes in many natural language processing (NLP) tasks. This challenge is especially prominent in the few-shot learning scenario, where the data in the target domain is generally much scarcer and of lowered quality. A natural and widely-used strategy to mitigate such challenges is to perform data augmentation to better capture the data invariance and increase the sample size. However, current text data augmentation methods either can't ensure the correct labeling of the generated data (lacking faithfulness) or can't ensure sufficient diversity in the generated data (lacking compactness), or both. Inspired by the recent success of large language models, especially the development of ChatGPT, which demonstrated improved language comprehension abilities, in this work, we propose a text data augmentation approach based on ChatGPT (named AugGPT). AugGPT rephrases each sentence in the training sampl
    
[^79]: FrankenSplit:基于显著性指导的神经特征压缩与浅层变分瓶颈注入

    FrankenSplit: Saliency Guided Neural Feature Compression with Shallow Variational Bottleneck Injection. (arXiv:2302.10681v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2302.10681](http://arxiv.org/abs/2302.10681)

    本文提出了一种基于显著性指导的神经特征压缩与浅层变分瓶颈注入的新的资源意识压缩模型的框架，实现了比最先进的SC方法低60％的比特率，并且比现有的编解码标准的下放快16倍。

    

    移动AI加速器的崛起使得对延迟敏感的应用可以在客户端上执行轻量级深度神经网络（DNN）。然而，需要强大模型的关键应用程序需要将请求下放，而高维数据将争夺有限的带宽。本文提出了一种新的资源意识压缩模型的框架并在反映边缘设备和服务器之间不对称资源分配的环境中进行了广泛评估。我们的方法在不降低准确性的情况下实现了比最先进的SC方法低60％的比特率，并且比现有的编解码标准的下放快16倍。

    The rise of mobile AI accelerators allows latency-sensitive applications to execute lightweight Deep Neural Networks (DNNs) on the client side. However, critical applications require powerful models that edge devices cannot host and must therefore offload requests, where the high-dimensional data will compete for limited bandwidth. This work proposes shifting away from focusing on executing shallow layers of partitioned DNNs. Instead, it advocates concentrating the local resources on variational compression optimized for machine interpretability. We introduce a novel framework for resource-conscious compression models and extensively evaluate our method in an environment reflecting the asymmetric resource distribution between edge devices and servers. Our method achieves 60\% lower bitrate than a state-of-the-art SC method without decreasing accuracy and is up to 16x faster than offloading with existing codec standards.
    
[^80]: SurvLIMEpy: 一个实现了SurvLIME算法的Python包

    SurvLIMEpy: A Python package implementing SurvLIME. (arXiv:2302.10571v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.10571](http://arxiv.org/abs/2302.10571)

    本文介绍了一个名为SurvLIMEpy的Python包，它实现了一种算法，可以计算适用于建模生存分析数据的机器学习算法的局部特征重要性，并支持各种生存模型。

    

    本文介绍SurvLIMEpy，这是一个开源的Python包，实现了SurvLIME算法。该算法可以计算适用于建模生存分析数据的机器学习算法的局部特征重要性。我们的实现利用了并行化范例，因为所有计算都以矩阵方式执行，从而加快了执行时间。此外，SurvLIMEpy还提供了可视化工具来更好地理解算法的结果。该包支持各种生存模型，从Cox比例风险模型到DeepHit或DeepSurv等深度学习模型。本文介绍了两种类型的实验。首先，通过模拟数据，我们研究了算法捕获特征重要性的能力。其次，在使用三个开源生存数据集以及一组生存算法时，我们展示了SurvLIMEpy在应用于不同数据集时的表现。

    In this paper we present SurvLIMEpy, an open-source Python package that implements the SurvLIME algorithm. This method allows to compute local feature importance for machine learning algorithms designed for modelling Survival Analysis data. Our implementation takes advantage of the parallelisation paradigm as all computations are performed in a matrix-wise fashion which speeds up execution time. Additionally, SurvLIMEpy assists the user with visualization tools to better understand the result of the algorithm. The package supports a wide variety of survival models, from the Cox Proportional Hazards Model to deep learning models such as DeepHit or DeepSurv. Two types of experiments are presented in this paper. First, by means of simulated data, we study the ability of the algorithm to capture the importance of the features. Second, we use three open source survival datasets together with a set of survival algorithms in order to demonstrate how SurvLIMEpy behaves when applied to differen
    
[^81]: 随机生成流网络

    Stochastic Generative Flow Networks. (arXiv:2302.09465v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09465](http://arxiv.org/abs/2302.09465)

    Stochastic GFlowNets通过学习动态模型来扩展了GFlowNets的适用性，解决了原模型只适用于确定性环境的问题，并在具有随机动态的各种标准基准测试中表现良好，具有显著优势。

    

    生成流网络（或简称GFlowNets）是一类通过“推理即控制”学习采样复杂组合结构的概率代理。它们在从给定的能量景观中生成高质量、多样化的候选方面表现出巨大的潜力。然而，现有的GFlowNets仅适用于确定性环境，并在具有随机动态的更一般任务中失败，这可能限制了它们的适用性。为了克服这一挑战，本文介绍了一种新的算法，称为随机GFlowNets，它将GFlowNets扩展到随机环境。通过将状态转移分解为两个步骤，随机GFlowNets分离了环境随机性并学习了一个动态模型来捕捉它。广泛的实验结果表明，随机GFlowNets在具有随机动态的各种标准基准测试中，相较于标准GFlowNets以及MCMC和基于RL的方法，具有显著的优势。

    Generative Flow Networks (or GFlowNets for short) are a family of probabilistic agents that learn to sample complex combinatorial structures through the lens of "inference as control". They have shown great potential in generating high-quality and diverse candidates from a given energy landscape. However, existing GFlowNets can be applied only to deterministic environments, and fail in more general tasks with stochastic dynamics, which can limit their applicability. To overcome this challenge, this paper introduces Stochastic GFlowNets, a new algorithm that extends GFlowNets to stochastic environments. By decomposing state transitions into two steps, Stochastic GFlowNets isolate environmental stochasticity and learn a dynamics model to capture it. Extensive experimental results demonstrate that Stochastic GFlowNets offer significant advantages over standard GFlowNets as well as MCMC- and RL-based approaches, on a variety of standard benchmarks with stochastic dynamics.
    
[^82]: T2I-Adapter：学习适配器以挖掘文本到图像扩散模型更多可控制能力

    T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models. (arXiv:2302.08453v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.08453](http://arxiv.org/abs/2302.08453)

    该论文提出利用T2I模型隐含学习的知识来更加精确地控制生成结果。通过训练简单轻量级的T2I适配器来对齐内部知识与外部控制信号，实现在生成结果的颜色和结构方面丰富的控制和编辑效果。

    

    大规模文本到图像（T2I）模型的惊人生成能力已经展示了学习复杂结构和有意义语义的强大能力。然而，仅仅依靠文本提示无法充分利用模型所学的知识，特别是当需要灵活精确的控制（例如颜色和结构）时。本文旨在“挖掘”T2I模型隐含学习的能力，然后明确地使用它们来更加精确地控制生成结果。具体而言，我们提出学习简单轻量级的T2I适配器，将T2I模型内部知识与外部控制信号对齐，同时保持原始大型T2I模型不变。通过这种方式，我们可以根据不同的条件训练各种适配器，实现在生成结果的颜色和结构方面丰富的控制和编辑效果。此外，所提出的T2I适配器具有组合性和生成效率等实际价值的吸引力。

    The incredible generative ability of large-scale text-to-image (T2I) models has demonstrated strong power of learning complex structures and meaningful semantics. However, relying solely on text prompts cannot fully take advantage of the knowledge learned by the model, especially when flexible and accurate controlling (e.g., color and structure) is needed. In this paper, we aim to ``dig out" the capabilities that T2I models have implicitly learned, and then explicitly use them to control the generation more granularly. Specifically, we propose to learn simple and lightweight T2I-Adapters to align internal knowledge in T2I models with external control signals, while freezing the original large T2I models. In this way, we can train various adapters according to different conditions, achieving rich control and editing effects in the color and structure of the generation results. Further, the proposed T2I-Adapters have attractive properties of practical value, such as composability and gen
    
[^83]: 通过深度学生机器实现空间异质性学习

    Spatially heterogeneous learning by a deep student machine. (arXiv:2302.07419v3 [cond-mat.dis-nn] UPDATED)

    [http://arxiv.org/abs/2302.07419](http://arxiv.org/abs/2302.07419)

    本论文研究了一种深度学生机器的教师-学生设置，通过学生机器的集合来研究由具有大量可调参数的DNN的监督学习。研究表明DNN的学习在网络空间中相当异质。

    

    尽管深度神经网络（DNN）取得了非凡的成功，但由于具有大量可调参数，其仍然是黑匣子。为了研究DNN的隐藏层，本文通过一种统计力学方法称为教师-学生设置，研究了由宽度为N，深度为L，由具有c个输入的感知机组成的DNN的监督学习。我们考虑了一个学生机器的集合，该集合可以精确重现由教师机器提供的M组N维输入/输出关系。我们使用副本方法（H. Yoshino（2020））理论分析了集合，并进行了贪婪的Monte Carlo模拟。对于高维数据$N \gg 1$，理论在'密集极限' $N \gg c \gg 1$ 和 $M \gg 1$ 且固定$\alpha=M/c$时变得精确。理论和模拟都表明，DNN的学习在网络空间中相当异质：机器的配置在靠近输入/输出的层内更加相关。

    Despite the spectacular successes, deep neural networks (DNN) with a huge number of adjustable parameters remain largely black boxes. To shed light on the hidden layers of DNN, we study supervised learning by a DNN of width $N$ and depth $L$ consisting of perceptrons with $c$ inputs by a statistical mechanics approach called the teacher-student setting. We consider an ensemble of student machines that exactly reproduce $M$ sets of $N$ dimensional input/output relations provided by a teacher machine. We analyze the ensemble theoretically using a replica method (H. Yoshino (2020)) and numerically performing greedy Monte Carlo simulations. The replica theory which works on high dimensional data $N \gg 1$ becomes exact in 'dense limit' $N \gg c \gg 1$ and $M \gg 1$ with fixed $\alpha=M/c$. Both the theory and the simulation suggest learning by the DNN is quite heterogeneous in the network space: configurations of the machines are more correlated within the layers closer to the input/output
    
[^84]: 基于光学的XNOR位计数加速器用于二值神经网络的高效推断

    An Optical XNOR-Bitcount Based Accelerator for Efficient Inference of Binary Neural Networks. (arXiv:2302.06405v2 [cs.AR] UPDATED)

    [http://arxiv.org/abs/2302.06405](http://arxiv.org/abs/2302.06405)

    本论文提出了一种基于光学XNOR门和光电荷累加器的BNN加速器设计，相比现有的PICs加速器，该设计在面积、能效和吞吐量等方面获得了显着的改进。

    

    二值神经网络（BNN）越来越受欢迎，因为它们可将卷积神经网络（CNN）模型参数转换为1位精度，从而最小化推断时的存储和计算需求，并且几乎没有精度损失。并且它们使得基于XNOR和位计数的操作能够进行硬件加速。虽然已经提出了几种基于光电集成电路（PIC）的BNN加速器，它们提供的吞吐量和能效比其电子对应物更高，但这些加速器中使用的XNOR和位计数电路还需要进一步增强以提高它们的面积、能效和吞吐量。本文旨在满足这一需求，我们发明了一种基于单-MRR的光学XNOR门（OXG）。此外，我们提出了一种新的位计数电路设计，称为光电荷累加器（PCA）。我们使用多个OXGs来实现PCA。我们提出的加速器设计在面积、能效和吞吐量方面实现了显着的改进，比最先进的基于PIC的BNN加速器更加优越。

    Binary Neural Networks (BNNs) are increasingly preferred over full-precision Convolutional Neural Networks(CNNs) to reduce the memory and computational requirements of inference processing with minimal accuracy drop. BNNs convert CNN model parameters to 1-bit precision, allowing inference of BNNs to be processed with simple XNOR and bitcount operations. This makes BNNs amenable to hardware acceleration. Several photonic integrated circuits (PICs) based BNN accelerators have been proposed. Although these accelerators provide remarkably higher throughput and energy efficiency than their electronic counterparts, the utilized XNOR and bitcount circuits in these accelerators need to be further enhanced to improve their area, energy efficiency, and throughput. This paper aims to fulfill this need. For that, we invent a single-MRR-based optical XNOR gate (OXG). Moreover, we present a novel design of bitcount circuit which we refer to as Photo-Charge Accumulator (PCA). We employ multiple OXGs 
    
[^85]: 利用逻辑学习从嘈杂的众包标签中学习

    Learning from Noisy Crowd Labels with Logics. (arXiv:2302.06337v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06337](http://arxiv.org/abs/2302.06337)

    这篇论文介绍了一种利用逻辑引导的从嘈杂的众包标签中学习的框架，能够改进文本分类和命名实体识别等任务中，学习从嘈杂数据中提取有效信息的方法，这种框架能够融合逻辑知识，提高现有技术水平。

    

    本文探讨了将符号逻辑知识集成到深度神经网络中，从嘈杂的众包标签中学习的方法。我们引入了一种逻辑引导的从嘈杂的众包标签中学习的框架（Logic-LNCL），这是一种类似EM的迭代逻辑知识蒸馏框架，能够从嘈杂的标记数据和感兴趣的逻辑规则中进行学习。与传统的EM方法不同，我们的框架包含一种“伪E步骤”，从逻辑规则中蒸馏出一种新类型的学习目标，然后在“伪M步骤”中使用该目标来训练分类器。在文本情感分类和命名实体识别的两个真实世界数据集上进行的广泛评估表明，所提出的框架改进了最先进技术并为从嘈杂的众包标签中学习提供了一种新的解决方案。

    This paper explores the integration of symbolic logic knowledge into deep neural networks for learning from noisy crowd labels. We introduce Logic-guided Learning from Noisy Crowd Labels (Logic-LNCL), an EM-alike iterative logic knowledge distillation framework that learns from both noisy labeled data and logic rules of interest. Unlike traditional EM methods, our framework contains a ``pseudo-E-step'' that distills from the logic rules a new type of learning target, which is then used in the ``pseudo-M-step'' for training the classifier. Extensive evaluations on two real-world datasets for text sentiment classification and named entity recognition demonstrate that the proposed framework improves the state-of-the-art and provides a new solution to learning from noisy crowd labels.
    
[^86]: 浅层视觉Transformer的理论理解：学习、泛化和样本复杂性的分析

    A Theoretical Understanding of Shallow Vision Transformers: Learning, Generalization, and Sample Complexity. (arXiv:2302.06015v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06015](http://arxiv.org/abs/2302.06015)

    本文提供了第一份对于浅层ViT进行训练的理论分析，证明了使用SGD训练会产生稀疏的注意力图，目前的样本复杂度与标记相关令牌的分数倒数、标记级别的令牌噪声水平和初始模型错误呈正相关关系。

    

    近年来，具有自我注意机制的视觉Transformer（ViTs）在许多视觉任务中取得了巨大的实证成功。然而，由于层间的非凸交互，理论上的学习和泛化分析大多是难以理解的。本文提供了对于一项分类任务，使用一个自我注意层和两层感知机的浅层ViT进行训练的第一篇理论分析，建立了对于数据模型的描述，该模型可以同时表征标记相关和标记不相关的令牌。我们界定了达到零泛化误差的样本复杂性。我们的样本复杂性限制与标记相关令牌的部分倒数、标记级别的令牌噪声水平和初始模型误差呈正相关。我们还证明了使用随机梯度下降SGD（stochastic gradient descent）进行训练过程会导致稀疏的注意力图，这是对于注意力成功的一种形式证明。此外，本文指出，适当的令牌确定是确保实现最优性能的关键。

    Vision Transformers (ViTs) with self-attention modules have recently achieved great empirical success in many vision tasks. Due to non-convex interactions across layers, however, theoretical learning and generalization analysis is mostly elusive. Based on a data model characterizing both label-relevant and label-irrelevant tokens, this paper provides the first theoretical analysis of training a shallow ViT, i.e., one self-attention layer followed by a two-layer perceptron, for a classification task. We characterize the sample complexity to achieve a zero generalization error. Our sample complexity bound is positively correlated with the inverse of the fraction of label-relevant tokens, the token noise level, and the initial model error. We also prove that a training process using stochastic gradient descent (SGD) leads to a sparse attention map, which is a formal verification of the general intuition about the success of attention. Moreover, this paper indicates that a proper token spa
    
[^87]: 跨模态微调：先对齐再精细调整

    Cross-Modal Fine-Tuning: Align then Refine. (arXiv:2302.05738v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05738](http://arxiv.org/abs/2302.05738)

    ORCA是一个通用的跨模态微调框架，它将单个大规模预训练模型的适用性扩展到多种模态，通过先对齐再微调的方法使跨模态微调成为可能，实验结果优于广泛范围的方法。

    

    大规模预训练模型的微调已经在诸如视觉和NLP等经过研究的模态中取得了巨大进展。但是，在许多其他模态中由于缺乏相关的预训练模型，类似的进展并没有得到观察。在这项工作中，我们提出了ORCA，这是一个通用的跨模态微调框架，它将单个大规模预训练模型的适用性扩展到多种模态。通过对目标输入进行一种先对齐再精细调整的工作流来适应目标任务：给定目标输入，ORCA首先学习一个嵌入网络，该网络将嵌入特征分布与预训练模态进行对齐，然后在嵌入数据上微调预训练模型，以利用模态之间分享的知识。通过广泛的实验，我们展示了ORCA在包含来自12种模态、超过60个数据集的3个基准测试中获得了最先进的结果，优于广泛范围的手工设计、AutoML、通用型和任务特定型的方法。我们强调了跨模态微调的重要性。

    Fine-tuning large-scale pretrained models has led to tremendous progress in well-studied modalities such as vision and NLP. However, similar gains have not been observed in many other modalities due to a lack of relevant pretrained models. In this work, we propose ORCA, a general cross-modal fine-tuning framework that extends the applicability of a single large-scale pretrained model to diverse modalities. ORCA adapts to a target task via an align-then-refine workflow: given the target input, ORCA first learns an embedding network that aligns the embedded feature distribution with the pretraining modality. The pretrained model is then fine-tuned on the embedded data to exploit the knowledge shared across modalities. Through extensive experiments, we show that ORCA obtains state-of-the-art results on 3 benchmarks containing over 60 datasets from 12 modalities, outperforming a wide range of hand-designed, AutoML, general-purpose, and task-specific methods. We highlight the importance of 
    
[^88]: 基于经济深度学习模型的IoT僵尸网络检测

    IoT Botnet Detection Using an Economic Deep Learning Model. (arXiv:2302.02013v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2302.02013](http://arxiv.org/abs/2302.02013)

    本文提出了一种经济深度学习模型来检测物联网僵尸网络攻击以及不同类型的攻击。该模型能够在较小的预算下加速训练和检测过程，并且具有比最先进的检测模型更高的准确性。

    

    技术创新的快速进步增加了过去十年的使用和分发。全球物联网系统的快速增长增加了由恶意第三方创建的网络安全挑战。因此，考虑安全问题和物联网系统限制的可靠入侵检测和网络取证系统对于保护这些系统至关重要。物联网僵尸网络攻击是企业和个人面临的重大威胁之一。因此，本文提出了一种基于经济的深度学习模型来检测物联网僵尸网络攻击以及不同类型的攻击。所提出的模型在较小的实现预算下，加速了训练和检测过程，并获得了比最先进的检测模型更高的准确性。

    The rapid progress in technology innovation usage and distribution has increased in the last decade. The rapid growth of the Internet of Things (IoT) systems worldwide has increased network security challenges created by malicious third parties. Thus, reliable intrusion detection and network forensics systems that consider security concerns and IoT systems limitations are essential to protect such systems. IoT botnet attacks are one of the significant threats to enterprises and individuals. Thus, this paper proposed an economic deep learning-based model for detecting IoT botnet attacks along with different types of attacks. The proposed model achieved higher accuracy than the state-of-the-art detection models using a smaller implementation budget and accelerating the training and detecting processes.
    
[^89]: 对比与聚类：学习邻域对表示用于无源域自适应

    Contrast and Clustering: Learning Neighborhood Pair Representation for Source-free Domain Adaptation. (arXiv:2301.13428v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.13428](http://arxiv.org/abs/2301.13428)

    本文提出了一个无源域适应的实用且具有挑战性的方案，通过在原始特征空间聚类，构建真正困难的负对，结合噪声对比估计理论，学习一个域不变的特征来解决域上的差异问题，能够在三个常见的基准数据集上实现有效结果。

    

    无监督域适应利用来自不同分布的源数据解决从未标记目标域分类数据的问题。然而，传统方法需要访问源数据，这经常引起数据隐私方面的担忧。本文中，我们考虑了一个更加实际但充满挑战的设置，在这个设置中，源域数据不可用，目标域数据未标记。具体而言，我们从对比学习的角度解决了域间差异问题。我们的工作的关键思想是通过以下方式学习一个域不变的特征:1)在原始特征空间中直接执行具有最近邻居的聚类；2)通过扩展邻居构造真正困难的负对，而不引入额外的计算复杂度；3)结合噪声对比估计理论以获得计算优势。我们在三个常见的基准数据集VisDA、Office-Home和Office-31上进行了仔细的消融研究和广泛的实验。

    Unsupervised domain adaptation uses source data from different distributions to solve the problem of classifying data from unlabeled target domains. However, conventional methods require access to source data, which often raise concerns about data privacy. In this paper, we consider a more practical but challenging setting where the source domain data is unavailable and the target domain data is unlabeled. Specifically, we address the domain discrepancy problem from the perspective of contrastive learning. The key idea of our work is to learn a domain-invariant feature by 1) performing clustering directly in the original feature space with nearest neighbors; 2) constructing truly hard negative pairs by extended neighbors without introducing additional computational complexity; and 3) combining noise-contrastive estimation theory to gain computational advantage. We conduct careful ablation studies and extensive experiments on three common benchmarks: VisDA, Office-Home, and Office-31. T
    
[^90]: 使用来自成对或$K$元比较的人类反馈的规范强化学习

    Principled Reinforcement Learning with Human Feedback from Pairwise or $K$-wise Comparisons. (arXiv:2301.11270v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11270](http://arxiv.org/abs/2301.11270)

    该论文提供了带有人类反馈强化学习问题的理论框架，证明了最大似然估计在Bradley-Terry-Luce和Plackett-Luce模型下收敛。此外，提出了在一定的覆盖假设下，基于悲观估计的MLE提供了性能更好的策略。在证明了真实MLE和以成对比较形式替代的备选MLE都可以在PL模型下收敛的同时，也表明了真实MLE的高效性。这些结果为RLHF算法提供了新的见解，并统一了RLHF问题和IRL问题。

    

    我们为带有人类反馈的强化学习问题提供了一个理论框架。我们的分析表明，当真实奖励函数为线性函数时，最大似然估计（MLE）在Bradley-Terry-Luce（BTL）模型和Plackett-Luce（PL）模型下均收敛。然而，我们发现当基于学得的奖励模型训练策略时，MLE会失败，而基于悲观估计的MLE在一定的覆盖假设下提供性能更好的策略。此外，我们证明在PL模型下，真实MLE和将$k$元比较拆分为成对比较的备选MLE都收敛。而真实MLE是渐近更为高效的。我们的结果验证了现有RLHF算法（如InstructGPT）的实验成功，并为算法设计提供了新的见解。此外，我们的结果统一了RLHF问题和最大熵反向强化学习(IRL)问题，并为其提供了第一个样本复杂度界。

    We provide a theoretical framework for Reinforcement Learning with Human Feedback (RLHF). Our analysis shows that when the true reward function is linear, the widely used maximum likelihood estimator (MLE) converges under both the Bradley-Terry-Luce (BTL) model and the Plackett-Luce (PL) model. However, we show that when training a policy based on the learned reward model, MLE fails while a pessimistic MLE provides policies with improved performance under certain coverage assumptions. Additionally, we demonstrate that under the PL model, the true MLE and an alternative MLE that splits the $K$-wise comparison into pairwise comparisons both converge. Moreover, the true MLE is asymptotically more efficient. Our results validate the empirical success of existing RLHF algorithms in InstructGPT and provide new insights for algorithm design. Furthermore, our results unify the problem of RLHF and max-entropy Inverse Reinforcement Learning (IRL), and provide the first sample complexity bound fo
    
[^91]: 用单调梯度网络学习凸函数的梯度

    Learning Gradients of Convex Functions with Monotone Gradient Networks. (arXiv:2301.10862v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10862](http://arxiv.org/abs/2301.10862)

    本文提出了 C-MGN 和 M-MGN 两种单调梯度神经网络结构，用于直接学习凸函数的梯度，并显示出比现有的方法更好的效果。

    

    尽管已经付出了大量的努力来推导和分析信号处理问题的有效凸函数形式，但凸函数的梯度也具有重要的应用，从基于梯度的优化到最优运输等。最近的研究探索了数据驱动的方法来学习凸目标函数，但很少研究学习它们单调梯度的方法。在本文中，我们提出了两种单调梯度神经网络结构 C-MGN 和 M-MGN，用于直接学习凸函数的梯度。我们发现，与现有的方法相比，我们的网络容易训练，获得了更准确的单调梯度场，并使用了更少的参数。我们进一步展示了它们学习最优运输映射以增强驱动图像数据的能力。

    While much effort has been devoted to deriving and analyzing effective convex formulations of signal processing problems, the gradients of convex functions also have critical applications ranging from gradient-based optimization to optimal transport. Recent works have explored data-driven methods for learning convex objective functions, but learning their monotone gradients is seldom studied. In this work, we propose C-MGN and M-MGN, two monotone gradient neural network architectures for directly learning the gradients of convex functions. We show that, compared to state of the art methods, our networks are easier to train, learn monotone gradient fields more accurately, and use significantly fewer parameters. We further demonstrate their ability to learn optimal transport mappings to augment driving image data.
    
[^92]: 走向AI-enabled连接产业: AGV通信和传感器测量数据集

    Towards an AI-enabled Connected Industry: AGV Communication and Sensor Measurement Datasets. (arXiv:2301.03364v3 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2301.03364](http://arxiv.org/abs/2301.03364)

    本文介绍了两个无线测量活动所提供的数据集，并将其与机器学习结合起来用于指纹识别、视线检测、服务质量预测或链路选择等任务。

    

    本文介绍了两个工业测试平台上的无线测量活动: 工业车辆间通信(iV2V)和工业车辆到基础设施加传感器(iV2I+)。提供了关于这两个捕获数据集的详细信息。iV2V涵盖了自动引导车(AGVs)之间的侧向链路通信场景，而iV2I+则是在工业设置中进行的，其中自主清洁机器人连接到私有蜂窝网络。不同通信技术的组合，连同共同的测量方法，提供了机器学习(ML)可以利用的洞察力，用于指纹识别、视线检测、服务质量预测或链路选择等任务。此外，数据集已标记和预过滤，以便快速启动和应用。对于两个数据集，还详细介绍了相应的测试平台和测量情况。

    This paper presents two wireless measurement campaigns in industrial testbeds: industrial Vehicle-to-vehicle (iV2V) and industrial Vehicle-to-infrastructure plus Sensor (iV2I+). Detailed information about the two captured datasets is provided as well. iV2V covers sidelink communication scenarios between Automated Guided Vehicles (AGVs), while iV2I+ is conducted at an industrial setting where an autonomous cleaning robot is connected to a private cellular network. The combination of different communication technologies, together with a common measurement methodology, provides insights that can be exploited by Machine Learning (ML) for tasks such as fingerprinting, line-of-sight detection, prediction of quality of service or link selection. Moreover, the datasets are labelled and pre-filtered for fast on-boarding and applicability. The corresponding testbeds and measurements are also presented in detail for both datasets.
    
[^93]: DARTS搜索空间的伪反转瓶颈卷积

    Pseudo-Inverted Bottleneck Convolution for DARTS Search Space. (arXiv:2301.01286v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.01286](http://arxiv.org/abs/2301.01286)

    本文增加了ConvNeXt的微小设计变化来扩充DARTS搜索空间，提出了PIBConv块来减少计算占用，我们的架构在层数仅为2时优于一个具有类似规模的DARTS网络。

    

    可微架构搜索（DARTS）作为一种基于梯度的神经结构搜索方法引起了广泛关注。然而，自引入DARTS以来，很少有工作基于最先进的卷积神经网络结构设计原则逐步扩展DARTS搜索空间。本文旨在通过增加ConvNeXt的微小设计变化来逐步扩充DARTS搜索空间，并研究准确性、评估层数和计算成本之间的权衡。我们引入了伪反转瓶颈卷积（PIBConv）块，旨在减少ConvNeXt中提出的反转瓶颈块的计算占用。我们提出的架构对于评估层数的敏感度要小得多，并且在层数仅为2时显着优于一个具有类似规模的DARTS网络。此外，使用更少的层数，它不仅在较低的GMACs和参数数量下实现更高的准确性 ，计算占用也更小。

    Differentiable Architecture Search (DARTS) has attracted considerable attention as a gradient-based neural architecture search method. Since the introduction of DARTS, there has been little work done on adapting the action space based on state-of-art architecture design principles for CNNs. In this work, we aim to address this gap by incrementally augmenting the DARTS search space with micro-design changes inspired by ConvNeXt and studying the trade-off between accuracy, evaluation layer count, and computational cost. We introduce the Pseudo-Inverted Bottleneck Conv (PIBConv) block intending to reduce the computational footprint of the inverted bottleneck block proposed in ConvNeXt. Our proposed architecture is much less sensitive to evaluation layer count and outperforms a DARTS network with similar size significantly, at layer counts as small as 2. Furthermore, with less layers, not only does it achieve higher accuracy with lower computational footprint (measured in GMACs) and parame
    
[^94]: KoopmanLab: 用机器学习求解复杂物理方程

    KoopmanLab: machine learning for solving complex physics equations. (arXiv:2301.01104v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.01104](http://arxiv.org/abs/2301.01104)

    KoopmanLab 提出了一种机器学习模块，名为 Koopman neural operator (KNO), 用于在没有解析解或闭合形式的条件下学习 PDEs，解决了传统数值方法精度和效率平衡的问题，并且适用于未知潜在 PDEs 生成的经验数据。

    

    众多物理理论都基于偏微分方程（PDEs），然而越来越复杂的物理方程，特别是缺乏解析解或闭合形式的方程，阻碍了物理学的进一步发展。传统数值方法在计算求解PDEs时面临精度和效率之间的平衡问题，并且不适用于未知潜在PDEs生成的经验数据。为了克服这一挑战，我们提出了KoopmanLab，一种高效的Koopman神经算子系列模块，用于学习没有解析解或闭合形式的PDEs。我们的模块包括多个变种的Koopman神经算子（KNO），这是一种基于动态系统理论开发的基于神经网络的PDE求解器，无需网格。紧凑的KNO变体可以准确地求解小模型尺寸的PDEs，而大型的KNO变体在预测由未知，高维动态系统控制的高度复杂的系统方面更具竞争力。

    Numerous physics theories are rooted in partial differential equations (PDEs). However, the increasingly intricate physics equations, especially those that lack analytic solutions or closed forms, have impeded the further development of physics. Computationally solving PDEs by classic numerical approaches suffers from the trade-off between accuracy and efficiency and is not applicable to the empirical data generated by unknown latent PDEs. To overcome this challenge, we present KoopmanLab, an efficient module of the Koopman neural operator family, for learning PDEs without analytic solutions or closed forms. Our module consists of multiple variants of the Koopman neural operator (KNO), a kind of mesh-independent neural-network-based PDE solvers developed following dynamic system theory. The compact variants of KNO can accurately solve PDEs with small model sizes while the large variants of KNO are more competitive in predicting highly complicated dynamic systems govern by unknown, high
    
[^95]: 接近峰值地面真相

    Approaching Peak Ground Truth. (arXiv:2301.00243v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.00243](http://arxiv.org/abs/2301.00243)

    介绍了PGT理论概念和近似PGT的定量技术，为评估和提高机器学习模型在生物医学领域中的性能提供了策略。

    

    通常通过计算与参考标注的相似度来评估机器学习模型，并通过最大化与它们的相似度进行训练。尤其是在生物医学领域，标注是主观的，并且具有低的评标者间和评标者内可靠性。由于标注仅反映世界的一种解释，即使模型达到了高相似度分数，这也可能导致次优预测。本文引入了理论上的PGT概念。PGT标记了与参考注释的相似度增加不再转化为更好的RWMP的点。此外，还提出了一种通过计算评标者间和评标者内可靠性来近似PGT的定量技术。最后，回顾了四类PGT感知策略，以评估和提高模型性能。

    Machine learning models are typically evaluated by computing similarity with reference annotations and trained by maximizing similarity with such. Especially in the biomedical domain, annotations are subjective and suffer from low inter- and intra-rater reliability. Since annotations only reflect one interpretation of the real world, this can lead to sub-optimal predictions even though the model achieves high similarity scores. Here, the theoretical concept of PGT is introduced. PGT marks the point beyond which an increase in similarity with the \emph{reference annotation} stops translating to better RWMP. Additionally, a quantitative technique to approximate PGT by computing inter- and intra-rater reliability is proposed. Finally, four categories of PGT-aware strategies to evaluate and improve model performance are reviewed.
    
[^96]: MN-DS：新闻文章层次分类的多标签数据集

    MN-DS: A Multilabeled News Dataset for News Articles Hierarchical Classification. (arXiv:2212.12061v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.12061](http://arxiv.org/abs/2212.12061)

    本文介绍了一个包含10,917篇新闻文章的多标签数据集，可用于训练机器学习模型自动按主题对新闻文章进行分类，对新闻结构、分类和预测未来事件的研究人员非常有帮助。

    

    本文介绍了一个数据集，其中包含10,917篇新闻文章，涵盖了从2019年1月1日到2019年12月31日的层次新闻分类。我们根据17个一级类别和109个二级类别的层次分类手动标记了这些文章。该数据集可用于训练机器学习模型，以自动按主题分类新闻文章。该数据集对于从事新闻结构、分类和根据发布的新闻预测未来事件的研究人员非常有帮助。

    This article presents a dataset of 10,917 news articles with hierarchical news categories collected between January 1st 2019, and December 31st 2019. We manually labelled the articles based on a hierarchical taxonomy with 17 first-level and 109 second-level categories. This dataset can be used to train machine learning models for automatically classifying news articles by topic. This dataset can be helpful for researchers working on news structuring, classification, and predicting future events based on released news.
    
[^97]: 关于语义分割模型的校准：分析与算法

    On Calibrating Semantic Segmentation Models: Analyses and An Algorithm. (arXiv:2212.12053v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.12053](http://arxiv.org/abs/2212.12053)

    本文系统研究了语义分割模型的校准问题，提出了一种简单而有效的方法——选择性缩放，通过将正确/错误预测分开进行缩放，并更加关注错误预测的逻辑平滑，此方法在语义分割校准上取得了良好效果。

    

    我们研究了语义分割校准的问题。虽然已经提出了很多解决方案来处理图像分类置信度的模型误校准，但至今为止，对语义分割的置信度校准研究仍然很有限。我们对语义分割模型的校准进行了系统研究，并提出了一种简单而有效的方法。首先，我们发现模型容量、裁剪大小、多尺度测试和预测正确性对校准有影响。其中，预测正确性，特别是错误预测，对由于过度置信而导致的误校准更为重要。接下来，我们提出了一种简单、统一且有效的方法，即选择性缩放，通过将正确/错误预测分开进行缩放，并更加关注错误预测的逻辑平滑。然后，我们研究了流行的现有校准方法，并将它们与选择性缩放在语义分割校准上进行比较。我们进行了大量实验，使用了多种数据集和语义分割模型，以验证我们提出的方法的有效性。

    We study the problem of semantic segmentation calibration. Lots of solutions have been proposed to approach model miscalibration of confidence in image classification. However, to date, confidence calibration research on semantic segmentation is still limited. We provide a systematic study on the calibration of semantic segmentation models and propose a simple yet effective approach. First, we find that model capacity, crop size, multi-scale testing, and prediction correctness have impact on calibration. Among them, prediction correctness, especially misprediction, is more important to miscalibration due to over-confidence. Next, we propose a simple, unifying, and effective approach, namely selective scaling, by separating correct/incorrect prediction for scaling and more focusing on misprediction logit smoothing. Then, we study popular existing calibration methods and compare them with selective scaling on semantic segmentation calibration. We conduct extensive experiments with a vari
    
[^98]: 异构传感器情境下的少样本人体运动预测

    Few-shot human motion prediction for heterogeneous sensors. (arXiv:2212.11771v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.11771](http://arxiv.org/abs/2212.11771)

    本研究提出了一种扩展时间序列预测方法来考虑异构传感器对人体运动预测的影响，并成功将其应用于不同传感器情境下的少样本有限运动任务中。

    

    人体运动预测是一项复杂的任务，因为它涉及在连接的传感器图中预测随时间变化的变量。本文提出了一种利用图神经网络扩展具有异构属性的少样本时间序列预测方法，来明确纳入空间图，同时跨越具有异构传感器的两个运动任务中进行预测时表现出色。

    Human motion prediction is a complex task as it involves forecasting variables over time on a graph of connected sensors. This is especially true in the case of few-shot learning, where we strive to forecast motion sequences for previously unseen actions based on only a few examples. Despite this, almost all related approaches for few-shot motion prediction do not incorporate the underlying graph, while it is a common component in classical motion prediction. Furthermore, state-of-the-art methods for few-shot motion prediction are restricted to motion tasks with a fixed output space meaning these tasks are all limited to the same sensor graph. In this work, we propose to extend recent works on few-shot time-series forecasting with heterogeneous attributes with graph neural networks to introduce the first few-shot motion approach that explicitly incorporates the spatial graph while also generalizing across motion tasks with heterogeneous sensors. In our experiments on motion tasks with 
    
[^99]: 损失通信下车辆之间协同感知的学习

    Learning for Vehicle-to-Vehicle Cooperative Perception under Lossy Communication. (arXiv:2212.08273v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.08273](http://arxiv.org/abs/2212.08273)

    本文研究了复杂驾驶场景下可能存在的损失共享特征的情况对车辆协同感知的影响，并提出了一种新的损失通信感知特征融合方法，通过损失通信感知修复网络缓解了损失通信的副作用并增强了感知性能。

    

    深度学习被广泛应用于智能车辆驾驶中的感知（例如3D物体检测）中。由于有益的车辆间通信（V2V），基于深度学习的来自其他代理的特征可以共享给本车，以提高本车的感知能力。这被称为V2V研究中的协同感知，其算法最近得到了显著的发展。然而，所有现有的协同感知算法都假定理想的V2V通信，而没有考虑复杂的现实驾驶场景中可能出现的损失共享特征的情况。本文首先研究了V2V协同感知中损失通信的副作用（例如检测性能下降），然后提出了一种新的中间损失通信感知特征融合方法，通过损失通信感知修复网络（LCRN）缓解了损失通信的副作用并增强了感知性能。

    Deep learning has been widely used in the perception (e.g., 3D object detection) of intelligent vehicle driving. Due to the beneficial Vehicle-to-Vehicle (V2V) communication, the deep learning based features from other agents can be shared to the ego vehicle so as to improve the perception of the ego vehicle. It is named as Cooperative Perception in the V2V research, whose algorithms have been dramatically advanced recently. However, all the existing cooperative perception algorithms assume the ideal V2V communication without considering the possible lossy shared features because of the Lossy Communication (LC) which is common in the complex real-world driving scenarios. In this paper, we first study the side effect (e.g., detection performance drop) by the lossy communication in the V2V Cooperative Perception, and then we propose a novel intermediate LC-aware feature fusion method to relieve the side effect of lossy communication by a LC-aware Repair Network (LCRN) and enhance the int
    
[^100]: 图上非独立同分布的迁移学习

    Non-IID Transfer Learning on Graphs. (arXiv:2212.08174v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.08174](http://arxiv.org/abs/2212.08174)

    本文提出了一种用于图上跨网络迁移学习的新方法，在利用Weisfeiler-Lehman图同构测试的基础上，提出了图子树差异度量算法来衡量源目标之间的图分布差异，进而推导出跨网络迁移学习的一般化误差界限，同时还开发了GDSL算法用于对齐图子树，实验结果表明我们的方法优于现有的最先进方法。

    

    迁移学习是指从相关的源域到目标域转移知识或信息的过程。然而，大多数现有的迁移学习理论和算法都集中在假设源/目标样本相互独立同分布的IID任务上。很少有研究专门针对图上的非独立同分布任务，例如跨网络挖掘的知识迁移。为了解决这个问题，本文从Weisfeiler-Lehman图同构测试的角度，提出了严格的一般化界限和算法，用于从源图到目标图的跨网络迁移学习。本文提出了一种新的图子树差异度量方法来衡量源图和目标图之间的图分布差异，然后在此基础上推导了跨网络迁移学习的一般化误差界限，包括节点分类和链接预测。此外，本文还开发了一个实用的算法GDSL，通过对齐图子树可以有效地减少跨网络分布偏移。多个真实世界的数据集上的实验表明，我们提出的方法优于现有的几种最先进的方法。

    Transfer learning refers to the transfer of knowledge or information from a relevant source domain to a target domain. However, most existing transfer learning theories and algorithms focus on IID tasks, where the source/target samples are assumed to be independent and identically distributed. Very little effort is devoted to theoretically studying the knowledge transferability on non-IID tasks, e.g., cross-network mining. To bridge the gap, in this paper, we propose rigorous generalization bounds and algorithms for cross-network transfer learning from a source graph to a target graph. The crucial idea is to characterize the cross-network knowledge transferability from the perspective of the Weisfeiler-Lehman graph isomorphism test. To this end, we propose a novel Graph Subtree Discrepancy to measure the graph distribution shift between source and target graphs. Then the generalization error bounds on cross-network transfer learning, including both cross-network node classification and
    
[^101]: PDE的非等间隔傅里叶神经求解器

    Non-equispaced Fourier Neural Solvers for PDEs. (arXiv:2212.04689v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.04689](http://arxiv.org/abs/2212.04689)

    提出了一种名为\textsc {NFS}的非等间隔傅里叶PDE求解器，能够在处理实际系统的空间域中非等间隔的数据情况下保持较高精度，并且具有网格不变推断能力。

    

    解决偏微分方程是困难的。最近提出了神经分辨率不变模型，尽管它们有效和高效，但通常需要等间隔的数据点。然而，在实际系统的空间域中采样有时不可避免地是非等间隔的，从而限制了它们的适用性。在本文中，我们提出了一种带有自适应插值的非等间隔傅里叶PDE求解器（\textsc {NFS}）和傅里叶神经算子的变体作为其组成部分。对复杂PDE进行的实验结果表明，它在精度和效率方面具有优势。与空间等间隔基准方法相比，它在MAE上取得了$42.85\%$的改进，并且能够在略有精度损失的情况下处理非等间隔数据。此外，据我们所知，\textsc{NFS}是第一个具有网格不变推断能力的基于机器学习的方法，在非等间隔场景下成功模拟湍流流动。

    Solving partial differential equations is difficult. Recently proposed neural resolution-invariant models, despite their effectiveness and efficiency, usually require equispaced spatial points of data. However, sampling in spatial domain is sometimes inevitably non-equispaced in real-world systems, limiting their applicability. In this paper, we propose a Non-equispaced Fourier PDE Solver (\textsc{NFS}) with adaptive interpolation on resampled equispaced points and a variant of Fourier Neural Operators as its components. Experimental results on complex PDEs demonstrate its advantages in accuracy and efficiency. Compared with the spatially-equispaced benchmark methods, it achieves superior performance with $42.85\%$ improvements on MAE, and is able to handle non-equispaced data with a tiny loss of accuracy. Besides, to our best knowledge, \textsc{NFS} is the first ML-based method with mesh invariant inference ability to successfully model turbulent flows in non-equispaced scenarios, wit
    
[^102]: DRIP: 多面体域精炼迭代用于神经回路反向可达性分析

    DRIP: Domain Refinement Iteration with Polytopes for Backward Reachability Analysis of Neural Feedback Loops. (arXiv:2212.04646v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2212.04646](http://arxiv.org/abs/2212.04646)

    本文提出了 DRIP 算法，使用多面体域精炼迭代来提高神经回路反向可达性分析的精度，进一步收紧了反投影集合的边缘。同时，还引入了一个新的公式来表示多面体边缘，以加强边缘限制。

    

    数据驱动控制技术的安全认证仍然是一个主要的开放性问题。本文研究逆向可达性作为一个框架，为被神经网络 (NN) 策略控制的系统提供避碰保证。由于 NN 通常不可逆，现有方法保守地假定一个域来松弛 NN，这导致状态集的过度逼近，可能导致系统进入障碍物 (即反投影 (BP) 集)。为了解决这个问题，我们引入 DRIP，一个在松弛域上进行精化循环的算法，可以大大收紧 BP 集的边界。此外，我们引入一个公式，可以直接获得用于限制 BP 集的多面体闭合形式表示，比以前需要求解线性规划和使用超矩形的方法更为紧凑。此外，本研究将 NN 松弛算法扩展到处理多面体域，进一步收紧边界。

    Safety certification of data-driven control techniques remains a major open problem. This work investigates backward reachability as a framework for providing collision avoidance guarantees for systems controlled by neural network (NN) policies. Because NNs are typically not invertible, existing methods conservatively assume a domain over which to relax the NN, which causes loose over-approximations of the set of states that could lead the system into the obstacle (i.e., backprojection (BP) sets). To address this issue, we introduce DRIP, an algorithm with a refinement loop on the relaxation domain, which substantially tightens the BP set bounds. Furthermore, we introduce a formulation that enables directly obtaining closed-form representations of polytopes to bound the BP sets tighter than prior work, which required solving linear programs and using hyper-rectangles. Furthermore, this work extends the NN relaxation algorithm to handle polytope domains, which further tightens the bound
    
[^103]: LLM-Planner: 利用大型语言模型进行少样本实体代理规划

    LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models. (arXiv:2212.04088v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.04088](http://arxiv.org/abs/2212.04088)

    本研究提出了一种新颖的方法LLM-Planner，利用大型语言模型为实体代理进行少样本规划，以实体代理目前所在的环境为基础，增强LLMs生成和更新计划，实验表明其在多任务和快速学习新任务的通用代理的开发中具有很好的表现。

    

    本研究关注利用大型语言模型（LLMs）作为规划器，让实体代理可以按照自然语言指令完成在视觉感知环境中的复杂任务。现有方法的高数据成本和低样本效率阻碍了多任务和快速学习新任务的通用代理的开发。本文提出了一种新颖的方法LLM-Planner，利用大型语言模型为实体代理进行少样本规划。我们进一步提出了一种简单但有效的方法，以实体代理目前所在的环境为基础，增强LLMs生成和更新计划。在ALFRED数据集上的实验表明，我们的方法可以取得非常有竞争力的少样本性能：尽管使用的配对训练数据不到0.5％，LLM-Planner的表现与使用完整训练数据训练的最新基线相当。现有方法几乎无法完成任何任务。

    This study focuses on using large language models (LLMs) as a planner for embodied agents that can follow natural language instructions to complete complex tasks in a visually-perceived environment. The high data cost and poor sample efficiency of existing methods hinders the development of versatile agents that are capable of many tasks and can learn new tasks quickly. In this work, we propose a novel method, LLM-Planner, that harnesses the power of large language models to do few-shot planning for embodied agents. We further propose a simple but effective way to enhance LLMs with physical grounding to generate and update plans that are grounded in the current environment. Experiments on the ALFRED dataset show that our method can achieve very competitive few-shot performance: Despite using less than 0.5% of paired training data, LLM-Planner achieves competitive performance with recent baselines that are trained using the full training data. Existing methods can barely complete any ta
    
[^104]: 基于配对互补时间循环一致对抗网络的雷达降水预测方法

    PCT-CycleGAN: Paired Complementary Temporal Cycle-Consistent Adversarial Networks for Radar-Based Precipitation Nowcasting. (arXiv:2211.15046v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15046](http://arxiv.org/abs/2211.15046)

    本文提出了一种基于配对互补时间循环一致对抗网络的雷达降水预测方法，该方法包括两个生成器网络和循环一致性损失和对抗性损失。实验证明，该方法在准确性和推广能力方面优于现有的技术方法。

    

    降水预测方法已经在过去几个世纪里得到了很好的发展，因为雨水对人类生活有着至关重要的影响。现有的降水预测模型包括定量降水预测 (QPF) 模型、卷积长短期记忆 (ConvLSTM) 模型以及最新的 MetNet-2 等多种复杂的方法。本文提出了基于配对互补时间循环一致对抗网络 (PCT-CycleGAN) 的雷达降水预测方法，受对抗生成网络 (CycleGAN) 强大的图像转换性能启发。PCT-CycleGAN 使用两个具有向前和向后时间动态的生成器网络生成时序性，每个生成器网络学习一个庞大的一对一映射，以逼近表示每个方向上的时间动态的映射函数。为了创建配对互补循环之间的强健时间因果关系，我们应用了循环一致性损失和对抗性损失。广泛的实验证明，PCT-CycleGAN 在准确性和推广能力方面优于现有的技术方法。

    The precipitation nowcasting methods have been elaborated over the centuries because rain has a crucial impact on human life. Not only quantitative precipitation forecast (QPF) models and convolutional long short-term memory (ConvLSTM), but also various sophisticated methods such as the latest MetNet-2 are emerging. In this paper, we propose a paired complementary temporal cycle-consistent adversarial networks (PCT-CycleGAN) for radar-based precipitation nowcasting, inspired by cycle-consistent adversarial networks (CycleGAN), which shows strong performance in image-to-image translation. PCT-CycleGAN generates temporal causality using two generator networks with forward and backward temporal dynamics in paired complementary cycles. Each generator network learns a huge number of one-to-one mappings about time-dependent radar-based precipitation data to approximate a mapping function representing the temporal dynamics in each direction. To create robust temporal causality between paired 
    
[^105]: SpaText: 基于空间文本表示的可控图像生成

    SpaText: Spatio-Textual Representation for Controllable Image Generation. (arXiv:2211.14305v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.14305](http://arxiv.org/abs/2211.14305)

    SpaText提出了一种新方法，利用开放词汇场景以及基于CLIP的空间文本表示进行文本到图像生成，能够实现对不同区域/对象的形状或布局的细粒度控制。

    

    最近的文本到图像扩散模型能够生成前所未有的令人信服的高质量结果。然而，几乎不可能以细粒度的方式控制不同区域/对象的形状或其布局。以往试图提供此类控制的尝试受到其依赖于固定标签集的限制。为此，我们提出了SpaText-一种利用开放词汇场景控制的文本到图像生成新方法。除了描述整个场景的全局文本提示外，用户还提供一个分割地图，其中每个感兴趣区域都由自由形式的自然语言描述进行注释。由于缺乏具有详细文本描述的大规模数据集以描述图像中每个区域，我们选择利用当前的大规模文本到图像数据集，并基于一种新颖的基于CLIP的空间文本表示来实现，同时展示其在基于像素和隐变量的两个最先进的扩散模型上的有效性。

    Recent text-to-image diffusion models are able to generate convincing results of unprecedented quality. However, it is nearly impossible to control the shapes of different regions/objects or their layout in a fine-grained fashion. Previous attempts to provide such controls were hindered by their reliance on a fixed set of labels. To this end, we present SpaText - a new method for text-to-image generation using open-vocabulary scene control. In addition to a global text prompt that describes the entire scene, the user provides a segmentation map where each region of interest is annotated by a free-form natural language description. Due to lack of large-scale datasets that have a detailed textual description for each region in the image, we choose to leverage the current large-scale text-to-image datasets and base our approach on a novel CLIP-based spatio-textual representation, and show its effectiveness on two state-of-the-art diffusion models: pixel-based and latent-based. In addition
    
[^106]: 通过自回归光场反演从单张图像中重构形状、姿态和外观

    Shape, Pose, and Appearance from a Single Image via Bootstrapped Radiance Field Inversion. (arXiv:2211.11674v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.11674](http://arxiv.org/abs/2211.11674)

    本文提出了一个自回归光场反演框架，可以从单个图像中恢复 SDF 参数化的 3D 形状、姿态和外观，而无需准确的真实姿势，且速度快、计算有效。

    

    组合神经辐射场 (NeRF) 和生成对抗网络 (GAN) 在从单视角进行三维重建方面展现出了很好的潜力，因为它们能够有效地建模任意拓扑结构。然而，最近该领域的工作主要集中在合成数据集上，其中确切的地面真实姿势是已知的，并忽略了姿势估计，而姿势估计是某些下游应用，如增强现实 (AR) 和机器人技术中的关键问题。本文提出了一种自然图像的有原则的端到端重建框架，其中准确的真实姿势是不可用的。我们的方法从一个物体的单个图像中恢复了 SDF 参数化的 3D 形状、姿态和外观，而没有在训练过程中利用多个视角。具体来说，我们利用了一个无条件的 3D-aware 生成器，对其应用了混合反演方案，其中一个模型产生了解的第一个猜测，然后通过优化进行改进。我们的框架可以在一次前向传递中对图像进行去渲染，因此速度快且计算有效。我们证明了我们的方法在 DTU 和 Tanks & Temples 等基准测试中优于现有最先进技术，并证明了在真实世界图像上的有效性。

    Neural Radiance Fields (NeRF) coupled with GANs represent a promising direction in the area of 3D reconstruction from a single view, owing to their ability to efficiently model arbitrary topologies. Recent work in this area, however, has mostly focused on synthetic datasets where exact ground-truth poses are known, and has overlooked pose estimation, which is important for certain downstream applications such as augmented reality (AR) and robotics. We introduce a principled end-to-end reconstruction framework for natural images, where accurate ground-truth poses are not available. Our approach recovers an SDF-parameterized 3D shape, pose, and appearance from a single image of an object, without exploiting multiple views during training. More specifically, we leverage an unconditional 3D-aware generator, to which we apply a hybrid inversion scheme where a model produces a first guess of the solution which is then refined via optimization. Our framework can de-render an image in as few a
    
[^107]: 自适应合并下的纵向网络有效估计

    Efficient Estimation for Longitudinal Network via Adaptive Merging. (arXiv:2211.07866v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.07866](http://arxiv.org/abs/2211.07866)

    本文提出了一个有效的纵向网络估计框架，利用自适应合并、张量分解和点过程等方法来减少估计偏差和方差。

    

    纵向网络由多个节点之间的时间边序列组成，其中时间边在实时中被观察到。随着在线社交平台和电子商务的兴起，它已经变得普遍，但在文献中往往被忽略。本文提出了一个有效的纵向网络估计框架，利用自适应网络合并、张量分解和点过程的优势。它合并相邻的稀疏网络，以扩大观测边的数量并减少估计方差，同时通过利用本地时间结构进行自适应网络邻域控制引入的估计偏差。提出了一个投影梯度下降算法来促进估计，其中每次迭代的估计错误上界被建立。进行了彻底的分析，以量化所提出方法的渐近行为，结果表明它可以显着减少估计偏差。

    Longitudinal network consists of a sequence of temporal edges among multiple nodes, where the temporal edges are observed in real time. It has become ubiquitous with the rise of online social platform and e-commerce, but largely under-investigated in literature. In this paper, we propose an efficient estimation framework for longitudinal network, leveraging strengths of adaptive network merging, tensor decomposition and point process. It merges neighboring sparse networks so as to enlarge the number of observed edges and reduce estimation variance, whereas the estimation bias introduced by network merging is controlled by exploiting local temporal structures for adaptive network neighborhood. A projected gradient descent algorithm is proposed to facilitate estimation, where the upper bound of the estimation error in each iteration is established. A thorough analysis is conducted to quantify the asymptotic behavior of the proposed method, which shows that it can significantly reduce the
    
[^108]: 在线合同设计的样本复杂度

    The Sample Complexity of Online Contract Design. (arXiv:2211.05732v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2211.05732](http://arxiv.org/abs/2211.05732)

    本文解决了在线合同设计中一个悬而未决的问题，证明了指数级的$m$个样本就足以学习一个近乎最优的合同。

    

    本文研究在线情境下的隐藏-行动委托问题。在每轮中，委托人发布一份合同，根据每个结果规定代理人的支付。代理人然后做出一个最大化她自己效用的战略行动选择，但直接观察不到行动。委托人观察结果并从代理人的行动选择中获得效用。根据过去的观察，委托人动态地调整合同，目标是最大化其效用。我们引入了一种在线学习算法，并给出了其Stackelberg遗憾的上界。我们证明，在合同空间为$[0,1]^m$时，Stackelberg遗憾的上界为$\widetilde O(\sqrt{m} \cdot T^{1-1/(2m+1)})$，下界为$\Omega(T^{1-1/(m+2)})$，其中$\widetilde O$排除对数因子。 这个结果表明，指数级的$m$个样本就足以学习一个近乎最优的合同，解决了在线合同设计中的一个悬而未决的问题。

    We study the hidden-action principal-agent problem in an online setting. In each round, the principal posts a contract that specifies the payment to the agent based on each outcome. The agent then makes a strategic choice of action that maximizes her own utility, but the action is not directly observable by the principal. The principal observes the outcome and receives utility from the agent's choice of action. Based on past observations, the principal dynamically adjusts the contracts with the goal of maximizing her utility.  We introduce an online learning algorithm and provide an upper bound on its Stackelberg regret. We show that when the contract space is $[0,1]^m$, the Stackelberg regret is upper bounded by $\widetilde O(\sqrt{m} \cdot T^{1-1/(2m+1)})$, and lower bounded by $\Omega(T^{1-1/(m+2)})$, where $\widetilde O$ omits logarithmic factors. This result shows that exponential-in-$m$ samples are sufficient and necessary to learn a near-optimal contract, resolving an open probl
    
[^109]: NESTER：一种自适应的神经符号化方法进行治疗效果评估

    NESTER: An Adaptive Neurosymbolic Method for Treatment Effect Estimation. (arXiv:2211.04370v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.04370](http://arxiv.org/abs/2211.04370)

    NESTER是一种自适应的神经符号化方法进行治疗效果评估，将治疗效果估计的所有要求集成到一个框架中，该方法比现有最先进的方法在多个基准数据集上性能更好。

    

    从观测数据中进行治疗效果评估是因果推断中的一个核心问题。基于潜在结果框架的方法通过利用因果推断中的归纳偏置和启发式方法来解决这个问题。每种现有的技术都通过设计神经网络架构和正则化器来解决治疗效果评估的特定方面，例如控制倾向得分、强制随机化等。在本文中，我们提出了一种自适应方法，称为神经符号治疗效果估计器（NESTER），它是一种治疗效果评估的通用方法。NESTER将治疗效果估计的所有要求集成到一个框架中。为此，我们设计了一个基于文献中使用的归纳偏置的治疗效果估计的领域特定语言（DSL）。我们还在理论上研究了NESTER在治疗效果估计任务中的能力。我们全面的实证结果表明，与现有的最先进方法相比，NESTER在多个基准数据集上的效果更好。

    Treatment effect estimation from observational data is a central problem in causal inference. Methods based on potential outcomes framework solve this problem by exploiting inductive biases and heuristics from causal inference. Each existing technique addresses a specific aspect of treatment effect estimation, such as controlling propensity score, enforcing randomization, etc., by designing neural network architectures and regularizers. In this paper, we propose an adaptive method called Neurosymbolic Treatment Effect Estimator (NESTER), a generalized method for treatment effect estimation. NESTER brings together all the desiderata for treatment effect estimation into one framework. For this purpose, we design a Domain Specific Language (DSL) for the treatment effect estimation based on inductive biases used in literature. We also theoretically study NESTER's capability for the treatment effect estimation task. Our comprehensive empirical results show that NESTER performs better on ben
    
[^110]: 一种个性化癫痫检测和分类的元图神经网络方法

    A Meta-GNN approach to personalized seizure detection and classification. (arXiv:2211.02642v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2211.02642](http://arxiv.org/abs/2211.02642)

    本文提出了一种个性化癫痫检测和分类的元图神经网络方法，通过将图神经网络和元学习相结合，能够在有限的样本中快速适应特定患者，并在 TUSZ 数据集上取得了优于基线的结果。

    

    本文提出了一种个性化癫痫检测和分类框架，从有限的癫痫样本中快速适应于特定患者。我们通过将两种新兴的范例——图神经网络和元学习相结合，实现了这一目标。我们训练了一种基于元图神经网络的分类器，该分类器从一组训练患者中学习了一个全局模型，以便最终可以使用非常有限的样本对新的未见过的患者进行调整。我们在 TUSZ 数据集上应用了我们的方法，这是公开可用的癫痫基准数据集之一。我们证明了我们的方法在仅进行了 20 次迭代后，即可在新的未见过的患者上达到 82.7% 的准确率和 82.08% 的 F1 分数，优于基线。

    In this paper, we propose a personalized seizure detection and classification framework that quickly adapts to a specific patient from limited seizure samples. We achieve this by combining two novel paradigms that have recently seen much success in a wide variety of real-world applications: graph neural networks (GNN), and meta-learning. We train a Meta-GNN based classifier that learns a global model from a set of training patients such that this global model can eventually be adapted to a new unseen patient using very limited samples. We apply our approach on the TUSZ-dataset, one of the largest and publicly available benchmark datasets for epilepsy. We show that our method outperforms the baselines by reaching 82.7% on accuracy and 82.08% on F1 score after only 20 iterations on new unseen patients.
    
[^111]: 隐变量高斯过程模型的完全贝叶斯推断

    Fully Bayesian inference for latent variable Gaussian process models. (arXiv:2211.02218v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.02218](http://arxiv.org/abs/2211.02218)

    隐变量高斯过程模型通过将定性因素映射到底层隐变量的方式解决了标准高斯过程无法适应定性输入的问题。本文提出了一种考虑隐变量估计不确定性的完全贝叶斯方法，支持通过隐变量可视化定性输入的效果。

    

    实际工程和科学应用常常涉及一个或多个定性输入。然而，标准高斯过程（GP）不能直接适应定性输入。最近引入的隐变量高斯过程（LVGP）通过首先将每个定性因素映射到底层隐变量（LV），然后在这些LV上使用任何标准GP协方差函数来解决这个问题。通过最大似然估计，对LV进行估计，然后将其插入到预测表达式中。然而，这种插入方法不考虑LV估计的不确定性，而这种不确定性在训练数据有限的情况下可能会很大。在这项工作中，我们为LVGP模型开发了一个完全贝叶斯方法，并通过其LV可视化了定性输入的效果。我们还开发了适用于扩展LVGP和LVGP超参数的完全贝叶斯推断的近似方法。我们进行了数值研究。

    Real engineering and scientific applications often involve one or more qualitative inputs. Standard Gaussian processes (GPs), however, cannot directly accommodate qualitative inputs. The recently introduced latent variable Gaussian process (LVGP) overcomes this issue by first mapping each qualitative factor to underlying latent variables (LVs), and then uses any standard GP covariance function over these LVs. The LVs are estimated similarly to the other GP hyperparameters through maximum likelihood estimation, and then plugged into the prediction expressions. However, this plug-in approach will not account for uncertainty in estimation of the LVs, which can be significant especially with limited training data. In this work, we develop a fully Bayesian approach for the LVGP model and for visualizing the effects of the qualitative inputs via their LVs. We also develop approximations for scaling up LVGPs and fully Bayesian inference for the LVGP hyperparameters. We conduct numerical studi
    
[^112]: 对象轨迹表示模型的经验贝叶斯分析

    An Empirical Bayes Analysis of Object Trajectory Representation Models. (arXiv:2211.01696v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01696](http://arxiv.org/abs/2211.01696)

    本论文对对象轨迹表示模型的复杂度和拟合误差之间的权衡进行了经验分析，发现简单的线性模型就能够高度重现真实世界的轨迹，通过使用经验贝叶斯方法可以为轨迹跟踪问题中必要的运动模型提供信息，并可以帮助规范预测模型。

    

    我们对对象轨迹建模中的模型复杂度和拟合误差之间的权衡进行了深入的经验分析。通过分析多个大型公共数据集，我们发现简单的线性模型在相关时间范围内使用较少的模型复杂度就能够高度重现真实世界的轨迹。这一发现允许将轨迹跟踪和预测作为贝叶斯过滤问题进行公式化。我们采用经验贝叶斯方法，从数据中估计模型参数的先验分布，这些先验分布可以为轨迹跟踪问题中必要的运动模型提供信息，并可以帮助规范预测模型。我们主张在轨迹预测任务中使用线性轨迹表示模型，因为它们目前并不会限制预测性能。

    We present an in-depth empirical analysis of the trade-off between model complexity and fit error in modelling object trajectories. Analyzing several large public datasets, we show that simple linear models do represent real-world trajectories with high fidelity over relevant time scales at very moderate model complexity. This finding allows the formulation of trajectory tracking and prediction as a Bayesian filtering problem. Using an Empirical Bayes approach, we estimate prior distributions over model parameters from the data. These prior distributions inform the motion models necessary in the trajectory tracking problem and can help regularize prediction models. We argue for the use of linear trajectory representation models in trajectory prediction tasks as they do not limit prediction performance currently.
    
[^113]: 从30秒心电图解释性地评估心力衰竭住院风险

    Interpretable estimation of the risk of heart failure hospitalization from a 30-second electrocardiogram. (arXiv:2211.00819v2 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2211.00819](http://arxiv.org/abs/2211.00819)

    本论文研究展示了使用机器学习方法可以通过30秒单导联心电图信号估计心力衰竭住院，并提供了临床有意义的解释。通过训练极限梯度提升加速故障时间模型，该模型在测试集上取得了较高的预测能力，可以在定位和治疗中发挥作用。

    

    医疗保健中的生存建模依赖于可解释的统计模型，但其潜在假设往往过于简单和不切实际。机器学习模型可以估计更复杂的关系并导致更精确的预测，但是它们是不可解释的。本研究表明可以通过30秒单导联心电图信号估计心力衰竭住院。使用机器学习方法不仅具有更强的预测能力，还提供临床有意义的解释。我们训练了一个极限梯度提升加速故障时间模型，并利用Shapley Additive exPlanations值解释每个特征对预测的影响。我们的模型在6,573名患者的测试集上取得了一年的协调指数为0.828，一年和两年的曲线下面积分别为0.853和0.858。这些结果表明，基于心电图的快速测试可能对定位和治疗至关重要。

    Survival modeling in healthcare relies on explainable statistical models; yet, their underlying assumptions are often simplistic and, thus, unrealistic. Machine learning models can estimate more complex relationships and lead to more accurate predictions, but are non-interpretable. This study shows it is possible to estimate hospitalization for congestive heart failure by a 30 seconds single-lead electrocardiogram signal. Using a machine learning approach not only results in greater predictive power but also provides clinically meaningful interpretations. We train an eXtreme Gradient Boosting accelerated failure time model and exploit SHapley Additive exPlanations values to explain the effect of each feature on predictions. Our model achieved a concordance index of 0.828 and an area under the curve of 0.853 at one year and 0.858 at two years on a held-out test set of 6,573 patients. These results show that a rapid test based on an electrocardiogram could be crucial in targeting and tre
    
[^114]: 为改进神经算法推理任务的非分布式泛化能力而努力

    Towards Better Out-of-Distribution Generalization of Neural Algorithmic Reasoning Tasks. (arXiv:2211.00692v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.00692](http://arxiv.org/abs/2211.00692)

    本文研究神经算法推理任务的非分布式泛化能力，分析当前基准测试的挑战并提出了解决方案。

    

    本文研究了神经算法推理任务的非分布式泛化能力，其中目标是使用深度神经网络从输入输出对中学习算法（例如排序、广度优先搜索和深度优先搜索）。首先，我们认为在这种情况下，非分布式泛化与常见的非分布式设置显着不同。例如，通常在图像分类的非分布式泛化中观察到的一些现象，例如\emph{直线上的准确度}在这里没有观察到，而数据增强方法等技术不起作用，因为许多增强技术的假设通常被违反。其次，我们分析了当前领先的基准测试CLRS\citep{deepmind2021clrs}的主要挑战（例如，输入分布偏移、非代表性数据生成和无信息的验证指标），该测试包含30个算法推理任务。我们提出了几个解决方案，包括一个简单而有效的修复输入分布偏移和改进数据的方法。

    In this paper, we study the OOD generalization of neural algorithmic reasoning tasks, where the goal is to learn an algorithm (e.g., sorting, breadth-first search, and depth-first search) from input-output pairs using deep neural networks. First, we argue that OOD generalization in this setting is significantly different than common OOD settings. For example, some phenomena in OOD generalization of image classifications such as \emph{accuracy on the line} are not observed here, and techniques such as data augmentation methods do not help as assumptions underlying many augmentation techniques are often violated. Second, we analyze the main challenges (e.g., input distribution shift, non-representative data generation, and uninformative validation metrics) of the current leading benchmark, i.e., CLRS \citep{deepmind2021clrs}, which contains 30 algorithmic reasoning tasks. We propose several solutions, including a simple-yet-effective fix to the input distribution shift and improved data 
    
[^115]: 随机零阶梯度下降在L-Lojasiewicz函数上的收敛速率

    Convergence Rates of Stochastic Zeroth-order Gradient Descent for \L ojasiewicz Functions. (arXiv:2210.16997v5 [math.OC] UPDATED)

    [http://arxiv.org/abs/2210.16997](http://arxiv.org/abs/2210.16997)

    该论文证明了在Lojasiewicz函数上，随机零阶梯度下降算法具有收敛速率，且比 $\{ \|\mathbf{x}_t-\mathbf{x}_\infty\| \}_{t \in \mathbb{N}}$更快，无论$f$是平滑还是非平滑的。

    

    我们证明了随机零阶梯度下降（SZGD）算法在Lojasiewicz函数上的收敛速率。SZGD算法迭代如下：$ \mathbf{x}_{t+1} = \mathbf{x}_t - \eta_t \widehat{\nabla} f (\mathbf{x}_t) $，其中$f$是满足Lojasiewicz不等式的目标函数，具有Lojasiewicz指数$\theta$，$\eta_t$是步长（学习率），$ \widehat{\nabla} f (\mathbf{x}_t)$是使用零阶信息估计的近似梯度。我们的结果表明，$ \{ f (\mathbf{x}_t) - f (\mathbf{x}_\infty) \}_{t \in \mathbb{N} }$的收敛速度可以比 $ \{ \| \mathbf{x}_t \mathbf{x}_\infty \| \}_{t \in \mathbb{N} } $更快，无论目标$f$是平滑还是非平滑的。

    We prove convergence rates of Stochastic Zeroth-order Gradient Descent (SZGD) algorithms for Lojasiewicz functions. The SZGD algorithm iterates as \begin{align*}  \mathbf{x}_{t+1} = \mathbf{x}_t - \eta_t \widehat{\nabla} f (\mathbf{x}_t), \qquad t = 0,1,2,3,\cdots , \end{align*} where $f$ is the objective function that satisfies the \L ojasiewicz inequality with \L ojasiewicz exponent $\theta$, $\eta_t$ is the step size (learning rate), and $ \widehat{\nabla} f (\mathbf{x}_t) $ is the approximate gradient estimated using zeroth-order information only.  Our results show that $ \{ f (\mathbf{x}_t) - f (\mathbf{x}_\infty) \}_{t \in \mathbb{N} } $ can converge faster than $ \{ \| \mathbf{x}_t \mathbf{x}_\infty \| \}_{t \in \mathbb{N} }$, regardless of whether the objective $f$ is smooth or nonsmooth.
    
[^116]: 破碎的神经缩放定律

    Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14891](http://arxiv.org/abs/2210.14891)

    本文提出了一个平滑破碎的幂律函数形式，可以准确地模拟和外推深度神经网络的缩放行为，适用于各种架构和大量不同任务，包括视觉、语言、音频、视频、生成建模、对比学习、机器人、不确定性估计/校准、对抗鲁棒性、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。

    This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.

    我们提出了一个平滑破碎的幂律函数形式（我们称之为破碎的神经缩放定律（BNSL）），它准确地模拟和外推了深度神经网络的缩放行为（即感兴趣的评估指标随用于训练的计算量、模型参数数量、训练数据集大小或上游性能变化而变化）对于各种架构和大量不同任务中的每个任务，包括大规模视觉、语言、音频、视频、扩散、生成建模、多模态学习、对比学习、AI对齐、机器人、超出分布（OOD）泛化、持续学习、不确定性估计/校准、超出分布检测、对抗鲁棒性、蒸馏、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。

    We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
    
[^117]: 结合数据和深度学习模型不确定性：在固体燃料回归速率测量中的应用

    Combined Data and Deep Learning Model Uncertainties: An Application to the Measurement of Solid Fuel Regression Rate. (arXiv:2210.14287v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14287](http://arxiv.org/abs/2210.14287)

    本文研究了固体燃料回归速率测量中的不确定性问题，并提出了一种可用于处理多个不确定性来源的不确定性量化过程。

    

    在复杂的物理过程表征中，对于来自多个来源的不确定性的观测数据和模型的综合，对于感兴趣的量（QoI）仍然是一项挑战。本文提出了一种前向传播不确定性量化（UQ）过程，以产生观测回归速率$\dot{r}$的概率分布。我们从实验中表征了两个输入数据不确定性源（摄像机失真$U_c$和燃料放置的非零角度$U_\gamma$），来自深度神经网络的预测和模型形式不确定性（$U_m$），以及用于训练它的手动分割图像的变异性（$U_s$）。我们对这些不确定性源与模型形式不确定性的七种组合进行了研究。本文的主要贡献是对固体混合火箭燃料回归速率测量的不确定性进行全面调查，并展示了一种可用于处理多个不确定性源的前向传播UQ过程。

    In complex physical process characterization, such as the measurement of the regression rate for solid hybrid rocket fuels, where both the observation data and the model used have uncertainties originating from multiple sources, combining these in a systematic way for quantities of interest(QoI) remains a challenge. In this paper, we present a forward propagation uncertainty quantification (UQ) process to produce a probabilistic distribution for the observed regression rate $\dot{r}$. We characterized two input data uncertainty sources from the experiment (the distortion from the camera $U_c$ and the non-zero angle fuel placement $U_\gamma$), the prediction and model form uncertainty from the deep neural network ($U_m$), as well as the variability from the manually segmented images used for training it ($U_s$). We conducted seven case studies on combinations of these uncertainty sources with the model form uncertainty. The main contribution of this paper is the investigation and inclus
    
[^118]: 高效的多阶段聚类实时流式嵌入式演讲人分离

    Highly Efficient Real-Time Streaming and Fully On-Device Speaker Diarization with Multi-Stage Clustering. (arXiv:2210.13690v3 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2210.13690](http://arxiv.org/abs/2210.13690)

    本文提出了一种多阶段聚类策略，可以解决流式嵌入式演讲者分离系统的多方面挑战，从而提高效率。此策略使用不同的聚类算法处理不同长度的输入，并可适应不同资源约束的设备。

    

    在说话者分离方面，最近的研究进展主要集中于提高分离结果的质量，但同时也越来越关注提高分离系统的效率。本文证明了一种多阶段聚类策略——对于不同长度的输入使用不同的聚类算法，从而解决了嵌入式演讲人分离应用的多方面挑战。具体而言，采用后备聚类器来处理短形式输入；主聚类器用于处理中等长度的输入；在经过主聚类器之前，采用预处理聚类器来压缩长形式输入。主聚类器和预处理聚类器都可以配置为计算复杂度上界，以适应不同资源约束的设备。该多阶段聚类策略对于CPU、内存和电池预算紧张的流式嵌入式演讲者分离系统至关重要。

    While recent research advances in speaker diarization mostly focus on improving the quality of diarization results, there is also an increasing interest in improving the efficiency of diarization systems. In this paper, we demonstrate that a multi-stage clustering strategy that uses different clustering algorithms for input of different lengths can address multi-faceted challenges of on-device speaker diarization applications. Specifically, a fallback clusterer is used to handle short-form inputs; a main clusterer is used to handle medium-length inputs; and a pre-clusterer is used to compress long-form inputs before they are processed by the main clusterer. Both the main clusterer and the pre-clusterer can be configured with an upper bound of the computational complexity to adapt to devices with different resource constraints. This multi-stage clustering strategy is critical for streaming on-device speaker diarization systems, where the budgets of CPU, memory and battery are tight.
    
[^119]: 对称（乐观）自然策略梯度的参数收敛性多智能体学习

    Symmetric (Optimistic) Natural Policy Gradient for Multi-agent Learning with Parameter Convergence. (arXiv:2210.12812v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2210.12812](http://arxiv.org/abs/2210.12812)

    本文研究了多智能体学习中的机器人学习算法，指出了香草自然策略梯度算法可能因具有不收敛的参数而存在学习不稳定性问题，并提出了新的NPG算法变种以解决此问题。

    

    在强化学习中，多智能体互动越来越重要，并且策略梯度方法的理论基础引起了人们的浓厚兴趣。本文研究了多智能体学习中自然策略梯度（NPG）算法的全局收敛性。我们首先展示了香草NPG可能没有参数收敛，即参数化策略的向量的收敛，即使成本被规则化（在文献中使策略空间有强的收敛保证）。这些非收敛的参数导致学习中的稳定性问题，在函数逼近的情况下尤为重要，在这种情况下，我们只能处理低维参数，而不是高维策略。然后我们针对几种标准多智能体学习场景提出了NPG算法的变种：两个玩家的零和矩阵和马尔可夫博弈，以及多个玩家的单调博弈，其具有全局最后迭代收敛性。

    Multi-agent interactions are increasingly important in the context of reinforcement learning, and the theoretical foundations of policy gradient methods have attracted surging research interest. We investigate the global convergence of natural policy gradient (NPG) algorithms in multi-agent learning. We first show that vanilla NPG may not have parameter convergence, i.e., the convergence of the vector that parameterizes the policy, even when the costs are regularized (which enabled strong convergence guarantees in the policy space in the literature). This non-convergence of parameters leads to stability issues in learning, which becomes especially relevant in the function approximation setting, where we can only operate on low-dimensional parameters, instead of the high-dimensional policy. We then propose variants of the NPG algorithm, for several standard multi-agent learning scenarios: two-player zero-sum matrix and Markov games, and multi-player monotone games, with global last-iter
    
[^120]: MTEB: 大规模文本嵌入基准测试

    MTEB: Massive Text Embedding Benchmark. (arXiv:2210.07316v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.07316](http://arxiv.org/abs/2210.07316)

    本文提出了一个大规模文本嵌入基准测试(MTEB)，该基准测试涵盖了8个嵌入任务、58个数据集和112种语言，以解决文本嵌入在不同任务中表现差异的问题。通过33个模型的测试，作者发现该领域尚未收敛于一种通用的文本嵌入方法，

    

    文本嵌入通常在覆盖其他任务的可能应用时，仅在单个任务的少量数据集上进行评估。目前还不清楚在语义文本相似度（STS）上最先进的嵌入方法是否同样适用于其他任务，比如聚类或重新排序。这使得评估该领域的进展变得困难，因为各种模型不断被提出却没有得到适当的评估。为了解决这个问题，我们引入了大规模文本嵌入基准测试（MTEB）。MTEB涵盖8个嵌入任务，涵盖58个数据集和112个语言。通过在MTEB上对33个模型进行基准测试，我们建立了迄今为止最全面的文本嵌入基准。我们发现，没有任何一种特定的文本嵌入方法在所有任务中都占据优势。这表明该领域尚未收敛于一种通用的文本嵌入方法，并将其扩展足够大以在所有嵌入任务中提供最先进的结果。MTEB附带开源代码和数据，以使社区能够基准测试新的嵌入模型并跟踪该领域的进展。

    Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and
    
[^121]: 基于特征符合预测的预测推断

    Predictive Inference with Feature Conformal Prediction. (arXiv:2210.00173v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.00173](http://arxiv.org/abs/2210.00173)

    本文提出基于特征符合预测的预测推断方法，通过利用深度表示学习的归纳偏置，扩展了符合预测到语义特征空间。从理论和实验结果来看，该方法优于常规符合预测，并在大规模任务上展现了最先进性能。

    

    符合预测是一种无分布技术，用于建立有效的预测间隔。虽然传统上人们在输出空间中进行符合预测，但这并不是唯一的可能性。在本文中，我们提出了基于特征的符合预测，通过利用深度表示学习的归纳偏置，扩展了符合预测对语义特征空间的范围。从理论上讲，我们证明了基于特征的符合预测在温和假设下可以证明优于常规符合预测。我们的方法不仅可以与普通符合预测结合使用，而且可以与其他自适应符合预测方法结合使用。除了现有预测推断基准测试的实验外，我们还展示了该方法在大规模任务（如ImageNet分类和Cityscapes图像分割）上的最先进性能。

    Conformal prediction is a distribution-free technique for establishing valid prediction intervals. Although conventionally people conduct conformal prediction in the output space, this is not the only possibility. In this paper, we propose feature conformal prediction, which extends the scope of conformal prediction to semantic feature spaces by leveraging the inductive bias of deep representation learning. From a theoretical perspective, we demonstrate that feature conformal prediction provably outperforms regular conformal prediction under mild assumptions. Our approach could be combined with not only vanilla conformal prediction, but also other adaptive conformal prediction methods. Apart from experiments on existing predictive inference benchmarks, we also demonstrate the state-of-the-art performance of the proposed methods on large-scale tasks such as ImageNet classification and Cityscapes image segmentation.
    
[^122]: 存在扰动情况下强化学习的安全探索方法

    Safe Exploration Method for Reinforcement Learning under Existence of Disturbance. (arXiv:2209.15452v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.15452](http://arxiv.org/abs/2209.15452)

    本文提出了一种能够在存在扰动情况下安全进行强化学习探索的方法，该方法利用了所控制对象和扰动的部分先验知识，可以保证以预先指定的概率满足显式状态约束。

    

    近年来，强化学习算法的快速发展在许多领域为我们提供了新的可能性。然而，由于其探索特性，当我们将这些算法应用于安全关键问题，特别是在实际环境中时，我们必须考虑风险。本文提出了一种涉及存在扰动时强化学习中的安全探索问题的方法。我们将学习过程中的安全性定义为以状态明确定义的限制条件的满足，并提出了一种安全探索方法，该方法利用所控制对象和扰动的部分先验知识。即使所控制对象暴露于遵循正态分布的随机扰动，该方法也能保证以预先指定的概率满足显式状态约束。在理论上，我们引入了足够的条件来构建不包含传统探索方法中的探索因素的保守输入。在摆动任务的模拟实验中，我们证明了所提出的方法可以在满足显式约束条件的情况下安全地学习。

    Recent rapid developments in reinforcement learning algorithms have been giving us novel possibilities in many fields. However, due to their exploring property, we have to take the risk into consideration when we apply those algorithms to safety-critical problems especially in real environments. In this study, we deal with a safe exploration problem in reinforcement learning under the existence of disturbance. We define the safety during learning as satisfaction of the constraint conditions explicitly defined in terms of the state and propose a safe exploration method that uses partial prior knowledge of a controlled object and disturbance. The proposed method assures the satisfaction of the explicit state constraints with a pre-specified probability even if the controlled object is exposed to a stochastic disturbance following a normal distribution. As theoretical results, we introduce sufficient conditions to construct conservative inputs not containing an exploring aspect used in th
    
[^123]: 十亿级预训练多模态商业知识图谱的构建与应用

    Construction and Applications of Billion-Scale Pre-Trained Multimodal Business Knowledge Graph. (arXiv:2209.15214v6 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2209.15214](http://arxiv.org/abs/2209.15214)

    本文介绍了一个基于阿里巴巴集团的空前规模的 OpenBG 商业知识图谱，包含超过 88 百万实体和 26 亿三元组。它具有精细的分类和多模态事实，有助于推动商业智能化的发展。

    

    商业知识图谱是当前许多企业的重要组成部分，为许多产品提供事实知识和结构化数据，使它们变得更加智能化。尽管它们有着许多潜在的好处，但构建商业知识图谱需要解决结构不足和多模态的限制等问题。本文深入探讨了在非微不足道的实际应用系统中构建知识图谱所面临的挑战。我们介绍了一个基于阿里巴巴集团的 OpenBG 商业知识图谱的构建过程。具体来说，我们定义了一个核心本体，涵盖各种抽象产品和消费需求，并在部署的应用中提供精细的分类和多模态事实。OpenBG 是一个空前规模的商业知识图谱：包含超过 88 百万实体、覆盖超过 1 百万个核心类/概念和 2,681 种关系的 26 亿三元组。我们公开了所有的资源和基准数据集，以促进知识图谱的发展和研究。

    Business Knowledge Graphs (KGs) are important to many enterprises today, providing factual knowledge and structured data that steer many products and make them more intelligent. Despite their promising benefits, building business KG necessitates solving prohibitive issues of deficient structure and multiple modalities. In this paper, we advance the understanding of the practical challenges related to building KG in non-trivial real-world systems. We introduce the process of building an open business knowledge graph (OpenBG) derived from a well-known enterprise, Alibaba Group. Specifically, we define a core ontology to cover various abstract products and consumption demands, with fine-grained taxonomy and multimodal facts in deployed applications. OpenBG is an open business KG of unprecedented scale: 2.6 billion triples with more than 88 million entities covering over 1 million core classes/concepts and 2,681 types of relations. We release all the open resources (OpenBG benchmarks) deri
    
[^124]: 减少人工干预：增强单一演示的深度强化学习

    Minimizing Human Assistance: Augmenting a Single Demonstration for Deep Reinforcement Learning. (arXiv:2209.11275v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.11275](http://arxiv.org/abs/2209.11275)

    本文通过使用虚拟现实模拟收集的单个人类示例辅助RL训练，从而在最小化人类干预的同时保留了使用人类示范的性能优势。这种方法使用单个示例生成的多个演示可显著提高训练速度，并使智能体能够解决复杂任务。

    

    强化学习中使用人类示范已被证明可以显著提高智能体的表现。然而，任何需要人为地手动“教授”模型的要求都与加强学习的目标相冲突。本文尝试在保留性能优势的同时，最小化人类参与学习过程，使用通过简单易用的虚拟现实模拟收集的单个人类示例来协助RL培训。我们的方法增强了单个演示，生成了许多类似于人类的演示。当这些演示与深度确定性策略梯度和后见之明的经验重放（DDPG + HER）相结合时，可以显著提高简单任务的训练时间，并使智能体能够解决DDPG + HER无法解决的复杂任务（块叠放）。该模型使用单个人类示例实现了这一显著的训练优势，需要不到一分钟的人类输入。此外，尽管从人类示例中学习，但代理的运行表现与原始演示者的行为风格不同。

    The use of human demonstrations in reinforcement learning has proven to significantly improve agent performance. However, any requirement for a human to manually 'teach' the model is somewhat antithetical to the goals of reinforcement learning. This paper attempts to minimize human involvement in the learning process while retaining the performance advantages by using a single human example collected through a simple-to-use virtual reality simulation to assist with RL training. Our method augments a single demonstration to generate numerous human-like demonstrations that, when combined with Deep Deterministic Policy Gradients and Hindsight Experience Replay (DDPG + HER) significantly improve training time on simple tasks and allows the agent to solve a complex task (block stacking) that DDPG + HER alone cannot solve. The model achieves this significant training advantage using a single human example, requiring less than a minute of human input. Moreover, despite learning from a human e
    
[^125]: 一种图上的多臂赌博机学习方法

    Multi-armed Bandit Learning on a Graph. (arXiv:2209.09419v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.09419](http://arxiv.org/abs/2209.09419)

    本文提出了一种基于乐观原则和离线图形规划算法的学习算法G-UCB，能够平衡长期探索利用，用于解决一种名为图赌博机的MAB扩展，从而获得最大化的收益。

    

    多臂赌博机问题是一个简单而强大的框架，在不确定性决策方面已经得到广泛研究。在许多现实世界的应用中，例如机器人应用中，选择一个臂对应于限制下一个可用臂（动作）的选择。出于这个目的，我们研究了一种名为图赌博机的MAB扩展，在此过程中，智能体从不同节点中收集奖励以获得最大化的收益。图定义了智能体在每一步中选择下一个可用节点的自由度。我们假设图的结构是完全可用的，但奖励分布是未知的。基于离线图形规划算法和乐观原则，我们设计了一种学习算法G-UCB，平衡长期探索利用使用乐观原则。我们证明了我们提出的算法达到了$O(\sqrt{|S|T\log(T)}+D|S|\log T)$的学习遗憾。其中$|S|$是

    The multi-armed bandit(MAB) problem is a simple yet powerful framework that has been extensively studied in the context of decision-making under uncertainty. In many real-world applications, such as robotic applications, selecting an arm corresponds to a physical action that constrains the choices of the next available arms (actions). Motivated by this, we study an extension of MAB called the graph bandit, where an agent travels over a graph to maximize the reward collected from different nodes. The graph defines the agent's freedom in selecting the next available nodes at each step. We assume the graph structure is fully available, but the reward distributions are unknown. Built upon an offline graph-based planning algorithm and the principle of optimism, we design a learning algorithm, G-UCB, that balances long-term exploration-exploitation using the principle of optimism. We show that our proposed algorithm achieves $O(\sqrt{|S|T\log(T)}+D|S|\log T)$ learning regret, where $|S|$ is 
    
[^126]: EcoFormer：具有线性复杂度的节能注意力机制

    EcoFormer: Energy-Saving Attention with Linear Complexity. (arXiv:2209.09004v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.09004](http://arxiv.org/abs/2209.09004)

    EcoFormer是一种通过核哈希技术进行高维softmax注意力二值化的新方法，实现了具有线性时间复杂度的高效注意力模块，从而显著降低了计算和能量开销，同时在多个任务上取得了高性能。

    

    Transformer是一种用于建模序列数据的革命性框架，在广泛的任务中已经取得了显著的性能，但其高计算量和能源成本限制了其实际应用。为了提高Transformer的效率，压缩模型是一个受欢迎的选择，其中最常用的方法是通过二值化来将浮点值限制为二进制值，以便于进行位运算从而节省计算和能源开销。然而，现有的二值化方法只注重最大化统计上的输入分布信息而忽略了注意力机制中核心的相似度建模问题。为此，我们提出了一个新的二值化方法，通过核哈希技术对高维softmax注意力机制进行定制化处理，将原始查询和键嵌入到哈明空间的低维二进制编码中。核哈希函数是以自监督方式从注意力图中提取基本关系的相似度所学习的。在这些二进制编码的基础上，我们开发了一种具有线性时间复杂度的高效注意力模块，从而显著降低了计算和能量开销，相比于基本模型实现了较高的性能和可比性。我们进行了大量实验来验证其在图像分类、语言模型预训练和机器翻译等多个任务上的性能。

    Transformer is a transformative framework that models sequential data and has achieved remarkable performance on a wide range of tasks, but with high computational and energy cost. To improve its efficiency, a popular choice is to compress the models via binarization which constrains the floating-point values into binary ones to save resource consumption owing to cheap bitwise operations significantly. However, existing binarization methods only aim at minimizing the information loss for the input distribution statistically, while ignoring the pairwise similarity modeling at the core of the attention. To this end, we propose a new binarization paradigm customized to high-dimensional softmax attention via kernelized hashing, called EcoFormer, to map the original queries and keys into low-dimensional binary codes in Hamming space. The kernelized hash functions are learned to match the ground-truth similarity relations extracted from the attention map in a self-supervised way. Based on th
    
[^127]: 双谱神经网络

    Bispectral Neural Networks. (arXiv:2209.03416v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.03416](http://arxiv.org/abs/2209.03416)

    本文提出了一种称为双谱神经网络的模型，能够从数据的隐含对称性中学习群、不可约表示和对应的完全不变映射，具有强大的基于不变性的对抗鲁棒性。

    

    本文提出了一种神经网络架构，双谱神经网络(BNNs)，用于学习表示在紧致可交换群在定义信号的空间上的作用下具有不变性的表示。该模型结合了双谱的思想，即是一个解析定义的群不变量，它是完整的——也就是说，它保留了所有信号结构，同时只去除了由于群作用引起的变化。在这里，我们证明了BNNs能够通过数据中的隐含对称性同时学习群、它们的不可约表示和对应的等变和完全不变映射。此外，我们证明了完整性属性赋予了这些网络强大的基于不变性的对抗鲁棒性。这项工作将Bispectral Neural Networks确立为稳健不变表示学习的强大计算原语。

    We present a neural network architecture, Bispectral Neural Networks (BNNs) for learning representations that are invariant to the actions of compact commutative groups on the space over which a signal is defined. The model incorporates the ansatz of the bispectrum, an analytically defined group invariant that is complete -- that is, it preserves all signal structure while removing only the variation due to group actions. Here, we demonstrate that BNNs are able to simultaneously learn groups, their irreducible representations, and corresponding equivariant and complete-invariant maps purely from the symmetries implicit in data. Further, we demonstrate that the completeness property endows these networks with strong invariance-based adversarial robustness. This work establishes Bispectral Neural Networks as a powerful computational primitive for robust invariant representation learning
    
[^128]: 可扩展的多智能体实验室优化框架

    Scalable Multi-Agent Lab Framework for Lab Optimization. (arXiv:2208.09099v3 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2208.09099](http://arxiv.org/abs/2208.09099)

    本文介绍了一种可扩展的多智能体实验室优化框架，能够有效地控制设备使用，让具有不同学习能力和目标的机器学习智能体协同运行研究活动。

    

    自主材料研究系统让科学家能够更加聪明地失败、更快地学习，以及在研究中节省更少的资源。随着这些系统在数量、能力和复杂性上的增长，一个新的挑战出现了——如何让它们在大型设施中共同工作呢？我们探讨了一种解决这个问题的方法——多智能体实验室控制框架。我们以自主材料科学实验室为例来演示这个框架，其中来自不同研究活动的信息可以被合并来解决手头的科学问题。这个框架可以 1）考虑到现实资源限制，如设备使用；2）允许具有不同学习能力和目标的机器学习智能体来运行研究活动；以及 3）促进多智能体协作和团队。该框架被称为可扩展的多智能体自主设施框架，即MULTITASK。MULTITASK使得设施范围内的模拟成为可能，包括智能体-仪器和智能体-智能体之间的交互。

    Autonomous materials research systems allow scientists to fail smarter, learn faster, and spend less resources in their studies. As these systems grow in number, capability, and complexity, a new challenge arises - how will they work together across large facilities? We explore one solution to this question - a multi-agent laboratory control frame-work. We demonstrate this framework with an autonomous material science lab in mind - where information from diverse research campaigns can be combined to ad-dress the scientific question at hand. This framework can 1) account for realistic resource limits such as equipment use, 2) allow for machine learning agents with diverse learning capabilities and goals capable of running re-search campaigns, and 3) facilitate multi-agent collaborations and teams. The framework is dubbed the MULTI-agent auTonomous fAcilities - a Scalable frameworK aka MULTITASK. MULTITASK makes possible facility-wide simulations, including agent-instrument and agent-age
    
[^129]: 具有预训练变压器的S-Prompts学习：领域增量学习的奥卡姆剃刀

    S-Prompts Learning with Pre-trained Transformers: An Occam's Razor for Domain Incremental Learning. (arXiv:2207.12819v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2207.12819](http://arxiv.org/abs/2207.12819)

    该论文提出了一种名为"S-Prompting"的学习范例，使用预先训练的变压器独立地跨越不同领域学习提示，大幅度减少连续学习中的遗忘问题，并具有出色的可适应性。

    

    目前的深度神经网络仍然无法解决连续学习中的灾难性遗忘问题。在本文中，我们提出了一种简单的范例（称为S-Prompting）和两种具体方法，极大地减少了其中一种最典型的连续学习场景（即领域增量学习）中的遗忘程度。该范例的关键思想是使用预先训练的变压器独立地跨越不同领域学习提示，并避免常见于传统方法中的示例使用。这导致了一个双赢的局面，即提示可以为每个领域取得最佳效果。跨越领域的独立提示仅需要一次交叉熵损失进行训练，并需要一个简单的K-NN操作作为推理的领域标识符。该学习范例派生了一种图像提示学习方法和一种新颖的语言-图像提示学习方法。具有出色的可伸缩性（每个领域增加0.03％的参数），是现有最佳方法之一。

    State-of-the-art deep neural networks are still struggling to address the catastrophic forgetting problem in continual learning. In this paper, we propose one simple paradigm (named as S-Prompting) and two concrete approaches to highly reduce the forgetting degree in one of the most typical continual learning scenarios, i.e., domain increment learning (DIL). The key idea of the paradigm is to learn prompts independently across domains with pre-trained transformers, avoiding the use of exemplars that commonly appear in conventional methods. This results in a win-win game where the prompting can achieve the best for each domain. The independent prompting across domains only requests one single cross-entropy loss for training and one simple K-NN operation as a domain identifier for inference. The learning paradigm derives an image prompt learning approach and a novel language-image prompt learning approach. Owning an excellent scalability (0.03% parameter increase per domain), the best of
    
[^130]: 基于有向图的跨模态特征补充方法用于多模态对话情感识别

    GraphCFC: A Directed Graph based Cross-modal Feature Complementation Approach for Multimodal Conversational Emotion Recognition. (arXiv:2207.12261v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2207.12261](http://arxiv.org/abs/2207.12261)

    本文提出了一种基于有向图的跨模态特征补充方法，可以提取多模态上下文信息和交互信息，缓解了多模态融合中的异构性差距问题。

    

    对话情感识别在人机交互系统中起着重要作用，因为它可以提供有共情心理的服务。多模态对话情感识别可以缓解单模态方法的缺点。最近，由于关系建模方面的卓越性能，图神经网络已被广泛用于各种领域。在多模态对话情感识别中，图神经网络能够提取远距离的上下文信息和跨模态的交互信息。不幸的是，由于现有方法（如MMGCN）直接融合多个模态，可能会产生冗余信息，且可能丢失多样化的信息。在本文中，我们提出了一种基于有向图的跨模态特征补充（GraphCFC）模块，可以有效地模拟上下文和互动信息。GraphCFC通过利用多个子空间提取器和成对跨模态补充（PairCC）策略，缓解了多模态融合中的异构性差距问题。

    Emotion Recognition in Conversation (ERC) plays a significant part in Human-Computer Interaction (HCI) systems since it can provide empathetic services. Multimodal ERC can mitigate the drawbacks of uni-modal approaches. Recently, Graph Neural Networks (GNNs) have been widely used in a variety of fields due to their superior performance in relation modeling. In multimodal ERC, GNNs are capable of extracting both long-distance contextual information and inter-modal interactive information. Unfortunately, since existing methods such as MMGCN directly fuse multiple modalities, redundant information may be generated and diverse information may be lost. In this work, we present a directed Graph based Cross-modal Feature Complementation (GraphCFC) module that can efficiently model contextual and interactive information. GraphCFC alleviates the problem of heterogeneity gap in multimodal fusion by utilizing multiple subspace extractors and Pair-wise Cross-modal Complementary (PairCC) strategy. 
    
[^131]: 移动无线网络中学习自适应转发策略：资源使用与延迟的平衡

    Learning an Adaptive Forwarding Strategy for Mobile Wireless Networks: Resource Usage vs. Latency. (arXiv:2207.11386v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2207.11386](http://arxiv.org/abs/2207.11386)

    本文利用深度强化学习为移动无线网络设计了一种可扩展和普适的单副本路由策略，通过权衡竞争的网络目标，设计了一种具有奖励函数的路由策略，并提出了一组新颖的关系领域、路径和上下文特征来描述设备移动情况，同时采用灵活的训练方法来训练单个DeepRL代理。

    

    为移动无线网络设计有效的路由策略是一项具有挑战性的任务，因为需要无缝地适应时空变化的网络条件。本文利用深度强化学习（DeepRL）来学习适用于此类网络的可扩展且普适的单副本路由策略。本文的贡献如下：一、设计了一种奖励函数，使得DeepRL代理可以明确权衡竞争的网络目标，如在最小化延迟与每个包的传输次数之间进行权衡；二、提出了一组新颖的关系领域、路径和上下文特征来描述移动无线网络，并独立于特定的网络拓扑模型设备移动模型；三、采用灵活的训练方法，将所有数据从所有包和设备汇总成一个离线中心化的训练集，以训练单个DeepRL代理。为了评估其可扩展性和普适性，我们将其与其他机器学习模型进行比较，并在真实数据集上进行了实验。

    Designing effective routing strategies for mobile wireless networks is challenging due to the need to seamlessly adapt routing behavior to spatially diverse and temporally changing network conditions. In this work, we use deep reinforcement learning (DeepRL) to learn a scalable and generalizable single-copy routing strategy for such networks. We make the following contributions: i) we design a reward function that enables the DeepRL agent to explicitly trade-off competing network goals, such as minimizing delay vs. the number of transmissions per packet; ii) we propose a novel set of relational neighborhood, path, and context features to characterize mobile wireless networks and model device mobility independently of a specific network topology; and iii) we use a flexible training approach that allows us to combine data from all packets and devices into a single offline centralized training set to train a single DeepRL agent. To evaluate generalizeability and scalability, we train our 
    
[^132]: 逆强化学习的主动探索方法

    Active Exploration for Inverse Reinforcement Learning. (arXiv:2207.08645v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.08645](http://arxiv.org/abs/2207.08645)

    AceIRL提出了一种新的逆强化学习算法，通过主动探索来学习奖励函数和策略，在不需要环境生成模型的情况下，能够确定可行奖励函数的置信区间，并找到侧重于环境中最有信息的区域的探索策略。

    

    逆强化学习（IRL）是从专家演示中推断奖励函数的强大范式。许多IRL算法需要已知的转移模型，有时甚至需要已知的专家策略，或者至少需要访问生成模型。但是，这些假设对于许多实际应用来说太强了，因为只能通过顺序交互来访问环境。我们提出了一种新的IRL算法：主动探索逆强化学习（AceIRL），它主动探索未知环境和专家策略，快速学习专家的奖励函数并识别出一个好的策略。AceIRL使用先前的观察结果构建置信区间来捕捉可行的奖励函数，并找到侧重于环境中最有信息的区域的探索策略。AceIRL是第一种具有样本复杂度界限且不需要环境生成模型的主动IRL方法。

    Inverse Reinforcement Learning (IRL) is a powerful paradigm for inferring a reward function from expert demonstrations. Many IRL algorithms require a known transition model and sometimes even a known expert policy, or they at least require access to a generative model. However, these assumptions are too strong for many real-world applications, where the environment can be accessed only through sequential interaction. We propose a novel IRL algorithm: Active exploration for Inverse Reinforcement Learning (AceIRL), which actively explores an unknown environment and expert policy to quickly learn the expert's reward function and identify a good policy. AceIRL uses previous observations to construct confidence intervals that capture plausible reward functions and find exploration policies that focus on the most informative regions of the environment. AceIRL is the first approach to active IRL with sample-complexity bounds that does not require a generative model of the environment. AceIRL 
    
[^133]: 多轨音乐 Transformer

    Multitrack Music Transformer. (arXiv:2207.06983v3 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2207.06983](http://arxiv.org/abs/2207.06983)

    这篇论文提出了一种新的多轨音乐表示方法，可以支持各种不同的乐器，在短的序列长度下实现了性能上的显著提升，同时提出了一种新方法用于分析音乐自我关注，并验证了模型更关注与当前音符形成和谐跨度和位于同一八度的音符。

    

    目前使用 Transformer 模型生成多轨音乐的方法在乐器数量、音乐片段长度和推理速度方面有限制，部分原因在于已有表示方式需要长度较长的输入序列，从而需要更多的内存。在本文中，我们提出了一种新的多轨音乐表示方法，可以支持各种不同的乐器，同时使输入序列长度更短。我们提出的 Multitrack Music Transformer（MMT）与最先进的系统相比具有可比性，在主观听测试中排在两个最近提出的模型之间，同时在速度和内存占用上都实现了显著的提升，使得该方法在实时即兴创作或接近实时的创意应用中更为实用。此外，我们提出了一种分析音乐自我关注的新方法，并展示了训练模型更关注与当前音符形成和谐跨度和位于同一八度的音符。

    Existing approaches for generating multitrack music with transformer models have been limited in terms of the number of instruments, the length of the music segments and slow inference. This is partly due to the memory requirements of the lengthy input sequences necessitated by existing representations. In this work, we propose a new multitrack music representation that allows a diverse set of instruments while keeping a short sequence length. Our proposed Multitrack Music Transformer (MMT) achieves comparable performance with state-of-the-art systems, landing in between two recently proposed models in a subjective listening test, while achieving substantial speedups and memory reductions over both, making the method attractive for real time improvisation or near real time creative applications. Further, we propose a new measure for analyzing musical self-attention and show that the trained model attends more to notes that form a consonant interval with the current note and to notes th
    
[^134]: 从人类示范中学习使用机械手进行连续抓取

    Learning Continuous Grasping Function with a Dexterous Hand from Human Demonstrations. (arXiv:2207.05053v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2207.05053](http://arxiv.org/abs/2207.05053)

    该论文提出了一种利用隐式函数进行连续抓取规划的方法，名为连续抓取功能(CG)，通过人类示范学习，可在操纵多个物体时获得更高的成功率，并在真实机器人上得到验证，成功率显著提高。

    

    我们提出使用隐式函数学习生成机械手操纵的抓取运动。使用连续时间输入，该模型可以生成连续且平滑的抓取计划。我们将所提出的模型命名为连续抓取功能（CGF）。通过使用3D人类示范，利用条件变分自动编码器进行生成建模，并将大规模的人体-物体交互轨迹通过运动重新定向转换成机器人演示，然后使用这些演示来训练CGF。在推断过程中，我们使用CGF进行抽样，在模拟器中生成不同的抓取计划，并选择成功的计划转移到真实机器人上。通过在多样化的人类数据上进行训练，我们的CGF允许操纵多个对象上的泛化。与先前的规划算法相比，CGF更加有效，在转移到使用真实Allegro手进行抓取时，成功率显著提高。

    We propose to learn to generate grasping motion for manipulation with a dexterous hand using implicit functions. With continuous time inputs, the model can generate a continuous and smooth grasping plan. We name the proposed model Continuous Grasping Function (CGF). CGF is learned via generative modeling with a Conditional Variational Autoencoder using 3D human demonstrations. We will first convert the large-scale human-object interaction trajectories to robot demonstrations via motion retargeting, and then use these demonstrations to train CGF. During inference, we perform sampling with CGF to generate different grasping plans in the simulator and select the successful ones to transfer to the real robot. By training on diverse human data, our CGF allows generalization to manipulate multiple objects. Compared to previous planning algorithms, CGF is more efficient and achieves significant improvement on success rate when transferred to grasping with the real Allegro Hand. Our project pa
    
[^135]: 并行的一致置信区间超参数优化

    Parallel Conformal Hyperparameter Optimization. (arXiv:2207.03017v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.03017](http://arxiv.org/abs/2207.03017)

    本文提出了一种基于一致置信区间上限抽样的优化框架，其假设具有交换性，能够提供更多的搜索模型架构选择，并且在超参数调整时具有优异的性能。

    

    在过去的十年中，出现了几个新颖的超参数搜索框架，但大多数都依赖于严格的、通常是正态分布假设，限制了搜索模型的灵活性。本文提出了一种基于置信区间一致性上限抽样的优化框架，其交换性假设能够提供更多的搜索模型架构选择。对超参优化的多个架构进行了探索和基准测试，包括密集和卷积神经网络，在性能上优于随机搜索。

    Several novel frameworks for hyperparameter search have emerged in the last decade, but most rely on strict, often normal, distributional assumptions, limiting search model flexibility. This paper proposes a novel optimization framework based on upper confidence bound sampling of conformal confidence intervals, whose assumption of exchangeability enables greater choice of search model architectures. Several such architectures were explored and benchmarked on hyperparameter tuning of both dense and convolutional neural networks, displaying superior performance to random search.
    
[^136]: Ask-AC: 一种循环中的主动顾问演员-评论家框架

    Ask-AC: An Initiative Advisor-in-the-Loop Actor-Critic Framework. (arXiv:2207.01955v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.01955](http://arxiv.org/abs/2207.01955)

    本文提出了一种新颖的主动顾问演员-评论家框架，Ask-AC，它替换了传统的被动监督信号机制，实现了定制化和高效的信息交换，其中的两个互补组件允许代理主动寻求顾问干预和识别漏掉的不稳定状态。

    

    尽管交互式强化学习方案取得了很多有希望的结果，但目前的方案仍然依赖于来自顾问专家的被动监督信号，形式包括持续监控或预定义规则，这不可避免地导致了一种麻烦而昂贵的学习过程。在本文中，我们介绍了一种新的主动顾问演员-评论家框架，称为Ask-AC，它用一个双向的学习者主动机制替换了单向的顾问指导机制，从而实现了学习者和顾问之间的定制化和有效的信息交换。Ask-AC 的核心是两个互补的组件，分别是动作请求者和自适应状态选择器，可以方便地纳入各种离散的演员-评论家架构中。前者允许代理主动寻求不确定状态下的顾问干预，后者则可以识别漏掉的不稳定状态。

    Despite the promising results achieved, state-of-the-art interactive reinforcement learning schemes rely on passively receiving supervision signals from advisor experts, in the form of either continuous monitoring or pre-defined rules, which inevitably result in a cumbersome and expensive learning process. In this paper, we introduce a novel initiative advisor-in-the-loop actor-critic framework, termed as Ask-AC, that replaces the unilateral advisor-guidance mechanism with a bidirectional learner-initiative one, and thereby enables a customized and efficacious message exchange between learner and advisor. At the heart of Ask-AC are two complementary components, namely action requester and adaptive state selector, that can be readily incorporated into various discrete actor-critic architectures. The former component allows the agent to initiatively seek advisor intervention in the presence of uncertain states, while the latter identifies the unstable states potentially missed by the for
    
[^137]: 超越mAP：迈向更好的实例分割评估。

    Beyond mAP: Towards better evaluation of instance segmentation. (arXiv:2207.01614v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2207.01614](http://arxiv.org/abs/2207.01614)

    本文提出了两个新的度量标准以明确衡量空间和分类重复预测的数量，研究人员还提出了一种Semantic Sorting and NMS模块以消除这些重复预测，以超越传统的平均精度度量标准。

    

    正确性是实例分割中的一个重要问题，它包括计算正确定位所有预测的对象数和对每个定位预测进行分类。平均精度是衡量这些分割构成部分的事实标准。然而，这个指标在高召回率范围内不惩罚重复预测，并且不能区分定位正确但分类错误的实例。这个弱点无意中导致了网络设计在AP上取得了显著增益，但也引入了大量的误报。因此，我们不能依赖AP来选择提供假阳性和高召回之间最佳折衷的模型。为解决这一问题，我们回顾了文献中的替代度量标准，并提出了两个新的度量标准来明确测量空间重复预测和分类重复预测的数量。我们还提出了一种基于像素占用的语义排序和NMS模块来消除这些重复预测。

    Correctness of instance segmentation constitutes counting the number of objects, correctly localizing all predictions and classifying each localized prediction. Average Precision is the de-facto metric used to measure all these constituents of segmentation. However, this metric does not penalize duplicate predictions in the high-recall range, and cannot distinguish instances that are localized correctly but categorized incorrectly. This weakness has inadvertently led to network designs that achieve significant gains in AP but also introduce a large number of false positives. We therefore cannot rely on AP to choose a model that provides an optimal tradeoff between false positives and high recall. To resolve this dilemma, we review alternative metrics in the literature and propose two new measures to explicitly measure the amount of both spatial and categorical duplicate predictions. We also propose a Semantic Sorting and NMS module to remove these duplicates based on a pixel occupancy 
    
[^138]: 随机深度神经网络的浓度不等式和最优层数

    Concentration inequalities and optimal number of layers for stochastic deep neural networks. (arXiv:2206.11241v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.11241](http://arxiv.org/abs/2206.11241)

    该论文提出了随机深度神经网络的浓度不等式，通过期望分类器给出了分类误差的概率上界，并确定了最优层数。

    

    我们提出了随机深度神经网络（SDNN）隐藏层输出的浓度不等式，以及整个SDNN输出的浓度不等式。这些结果使我们能够引入期望分类器（EC），并给出EC分类误差的概率上界。我们还通过最优停止策略确定了SDNN的最佳层数。我们将分析应用于具有ReLU激活函数的前馈神经网络的随机版本。

    We state concentration inequalities for the output of the hidden layers of a stochastic deep neural network (SDNN), as well as for the output of the whole SDNN. These results allow us to introduce an expected classifier (EC), and to give probabilistic upper bound for the classification error of the EC. We also state the optimal number of layers for the SDNN via an optimal stopping procedure. We apply our analysis to a stochastic version of a feedforward neural network with ReLU activation function.
    
[^139]: Merak：高效的分布式DNN训练框架，具备自动化的三维并行技术，适用于庞大的基础模型

    Merak: An Efficient Distributed DNN Training Framework with Automated 3D Parallelism for Giant Foundation Models. (arXiv:2206.04959v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.04959](http://arxiv.org/abs/2206.04959)

    Merak是一个自动化的三维并行深度学习训练框架，它解决了当前其他框架中需要手动修改模型才可并行的问题，并具有较高的计算、GPU内存和网络带宽利用率。

    

    基础模型正在成为深度学习技术的主流。由于模型参数和训练数据集规模庞大，预训练基础模型始终需要耗费时间。除了计算密集型，训练过程还极其依赖内存和通信，这就需要应用三维并行技术，即集成数据并行、管道模型并行和张量模型并行，以实现高效训练。为此，研发了一些自定义软件框架，如Megatron-LM和DeepSpeed。然而，当前的三位并行技术框架仍存在两个问题：i）对于需要手动修改模型以并行训练的模型开发人员来说，框架并不透明。ii）它们的计算、GPU内存和网络带宽利用不足。我们提出了Merak，一个高资源利用率的自动化三维并行深度学习训练框架。

    Foundation models are becoming the dominant deep learning technologies. Pretraining a foundation model is always time-consumed due to the large scale of both the model parameter and training dataset. Besides being computing-intensive, the training process is extremely memory-intensive and communication-intensive. These features make it necessary to apply 3D parallelism, which integrates data parallelism, pipeline model parallelism and tensor model parallelism, to achieve high training efficiency.  To achieve this goal, some custom software frameworks such as Megatron-LM and DeepSpeed are developed. However, current 3D parallelism frameworks still meet two issues: i) they are not transparent to model developers, which need to manually modify the model to parallelize training. ii) their utilization of computation, GPU memory and network bandwidth are not sufficient. We propose Merak, an automated 3D parallelism deep learning training framework with high resource utilization. Merak automa
    
[^140]: 检测异常加密货币交易：基于机器学习的取证在AML/CFT中的应用。

    Detecting Anomalous Cryptocurrency Transactions: an AML/CFT Application of Machine Learning-based Forensics. (arXiv:2206.04803v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2206.04803](http://arxiv.org/abs/2206.04803)

    本研究探讨机器学习和交易图分析方法在反洗钱和打击恐怖主义领域的应用，特别关注这些技术在社会技术生态系统中部署所产生的影响。

    

    区块链和分布式分类账技术在金融领域的应用引发了监管担忧，其中用户匿名性可能保障隐私和数据保护，但不可识别性阻碍追责，挑战反洗钱和打击恐怖主义和扩散（AML/CFT）的努力。本文关注这些技术在社会技术生态系统中部署对该领域特征和发展的影响，特别是对机器学习和交易图分析方法的应用提供了情境化的见解。它通过各种技术分析了以有向图网络表示的比特币实时数据集。

    In shaping the Internet of Money, the application of blockchain and distributed ledger technologies (DLTs) to the financial sector triggered regulatory concerns. Notably, while the user anonymity enabled in this field may safeguard privacy and data protection, the lack of identifiability hinders accountability and challenges the fight against money laundering and the financing of terrorism and proliferation (AML/CFT). As law enforcement agencies and the private sector apply forensics to track crypto transfers across ecosystems that are socio-technical in nature, this paper focuses on the growing relevance of these techniques in a domain where their deployment impacts the traits and evolution of the sphere. In particular, this work offers contextualized insights into the application of methods of machine learning and transaction graph analysis. Namely, it analyzes a real-world dataset of Bitcoin transactions represented as a directed graph network through various techniques. The modelin
    
[^141]: 三维蛋白质主干的扩散概率建模在基于模体脚手架问题中的应用

    Diffusion probabilistic modeling of protein backbones in 3D for the motif-scaffolding problem. (arXiv:2206.04119v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2206.04119](http://arxiv.org/abs/2206.04119)

    本研究提出了一种新的方法来解决蛋白质脚手架问题，采用E(3)-等变图神经网络来学习多样和长的蛋白质主干结构的分布，并用SMCDiff算法从分布中有效地采样出符合给定模体条件的脚手架，这一算法在大计算极限中可以理论上保证条件样本的采样；研究结果表明，我们的方法可以生成长达80个残基的脚手架，并可以为固定模体实现结构多样的脚手架。

    

    构建支持所需模体的脚手架结构，即赋予蛋白质功能的脚手架设计对于疫苗和酶的设计具有潜在价值。然而，模体脚手架问题的通用解决方案仍然没有出现。目前用于脚手架设计的机器学习技术只适用于长度不超过20的不真实脚手架或难以生成多个不同样式的脚手架。我们提出了利用E(3)-等变图神经网络学习多种多样且长度更长的蛋白质主干结构的分布。我们开发了SMCDiff以有效地从给定的模体条件下的分布中采样出脚手架；我们的算法是第一个理论上保证从大计算极限中扩散模型中的条件样本的算法。我们通过与AlphaFold2预测结构的对齐程度评估我们设计的主干。我们表明我们的方法可以(1)采样长达80个残基的脚手架，并且(2)为固定模体实现结构多样的脚手架。

    Construction of a scaffold structure that supports a desired motif, conferring protein function, shows promise for the design of vaccines and enzymes. But a general solution to this motif-scaffolding problem remains open. Current machine-learning techniques for scaffold design are either limited to unrealistically small scaffolds (up to length 20) or struggle to produce multiple diverse scaffolds. We propose to learn a distribution over diverse and longer protein backbone structures via an E(3)-equivariant graph neural network. We develop SMCDiff to efficiently sample scaffolds from this distribution conditioned on a given motif; our algorithm is the first to theoretically guarantee conditional samples from a diffusion model in the large-compute limit. We evaluate our designed backbones by how well they align with AlphaFold2-predicted structures. We show that our method can (1) sample scaffolds up to 80 residues and (2) achieve structurally diverse scaffolds for a fixed motif.
    
[^142]: 基于可解释的深度强化学习的无人机引导和规划的鲁棒性对抗攻击检测

    Robust Adversarial Attacks Detection based on Explainable Deep Reinforcement Learning For UAV Guidance and Planning. (arXiv:2206.02670v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.02670](http://arxiv.org/abs/2206.02670)

    本文提出了一种基于可解释的深度学习方法的创新性检测方案，以保护采用这些方法的无人机免受对抗攻击。

    

    针对无人机在公共领域遭受对抗攻击的风险增加的问题，本文提出了一种基于可解释的深度学习方法的创新性检测方案，以保护采用这些方法的无人机免受攻击。该方案采用深度强化学习进行引导和规划，利用人工势场来提高训练效率和障碍物避免率。

    The dangers of adversarial attacks on Uncrewed Aerial Vehicle (UAV) agents operating in public are increasing. Adopting AI-based techniques and, more specifically, Deep Learning (DL) approaches to control and guide these UAVs can be beneficial in terms of performance but can add concerns regarding the safety of those techniques and their vulnerability against adversarial attacks. Confusion in the agent's decision-making process caused by these attacks can seriously affect the safety of the UAV. This paper proposes an innovative approach based on the explainability of DL methods to build an efficient detector that will protect these DL schemes and the UAVs adopting them from attacks. The agent adopts a Deep Reinforcement Learning (DRL) scheme for guidance and planning. The agent is trained with a Deep Deterministic Policy Gradient (DDPG) with Prioritised Experience Replay (PER) DRL scheme that utilises Artificial Potential Field (APF) to improve training times and obstacle avoidance per
    
[^143]: 基于主成分分析的高效缺失数据插补算法框架

    Principal Component Analysis based frameworks for efficient missing data imputation algorithms. (arXiv:2205.15150v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.15150](http://arxiv.org/abs/2205.15150)

    本文提出了基于主成分分析的缺失数据插补框架，可适用于高维数据，能够显著优于基线方法，同时在分类任务中实现可比较或更好的分类准确性。

    

    缺失数据是实践中普遍存在的问题。许多填补缺失值的方法已经被开发出来。然而，并不是所有的方法都能够适用于高维数据，特别是多重插补技术。同时，如今的数据趋向于高维。因此，在这项工作中，我们提出了基于主成分分析（PCA）的简单而通用的缺失值插补框架PCA Imputation（PCAI），以加快插补过程并减轻许多可用插补技术的内存问题，而不会牺牲均方误差方面的插补质量。此外，即使部分或全部缺失特征是分类的，或者缺失特征数量较大，该框架也可以使用。接下来，我们介绍PCA Imputation - Classification（PIC），这是对具有一些调整的分类问题PCAI的应用。我们通过各种情况的实验验证了我们的方法，表明PCA I和PIC可以显著优于基线方法，同时在分类任务中实现可比较或更好的分类准确性。

    Missing data is a commonly occurring problem in practice. Many imputation methods have been developed to fill in the missing entries. However, not all of them can scale to high-dimensional data, especially the multiple imputation techniques. Meanwhile, the data nowadays tends toward high-dimensional. Therefore, in this work, we propose Principal Component Analysis Imputation (PCAI), a simple but versatile framework based on Principal Component Analysis (PCA) to speed up the imputation process and alleviate memory issues of many available imputation techniques, without sacrificing the imputation quality in term of MSE. In addition, the frameworks can be used even when some or all of the missing features are categorical, or when the number of missing features is large. Next, we introduce PCA Imputation - Classification (PIC), an application of PCAI for classification problems with some adjustments. We validate our approach by experiments on various scenarios, which shows that PCAI and PI
    
[^144]: 深度学习在群体级脑解码中的应用

    Group-level Brain Decoding with Deep Learning. (arXiv:2205.14102v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.14102](http://arxiv.org/abs/2205.14102)

    本文提出了一种深度学习方法，利用主体嵌入技术解决个体差异带来的群体级脑解码问题，并在脑磁图像数据上进行了验证。

    

    脑成像数据解码在脑机接口和神经表示研究中应用越来越广泛。由于受到个体之间变异的影响，解码通常是针对个体的，并且在个体间无法很好地推广。克服这一问题的方法不仅可以提供更丰富的神经科学见解，还可以使群体模型优于个体模型。本文提出了一种方法，利用类似自然语言处理中单词嵌入的主体嵌入来学习和利用个体间变化的结构作为解码模型的一部分，我们改编了 WaveNet 架构用于分类。我们将其应用于脑磁图像数据，15 个受试者观看了 118 种不同的图像，每个图像有 30 个样本，使用整个图像呈现后的 1s 窗口进行图像分类。我们表明，深度学习和主体嵌入的结合对于分类具有至关重要的作用。

    Decoding brain imaging data is gaining popularity, with applications in brain-computer interfaces and the study of neural representations. Decoding is typically subject-specific and does not generalise well over subjects, due to high amounts of between subject variability. Techniques that overcome this will not only provide richer neuroscientific insights but also make it possible for group-level models to outperform subject-specific models. Here, we propose a method that uses subject embedding, analogous to word embedding in Natural Language Processing, to learn and exploit the structure in between-subject variability as part of a decoding model, our adaptation of the WaveNet architecture for classification. We apply this to magnetoencephalography data, where 15 subjects viewed 118 different images, with 30 examples per image; to classify images using the entire 1s window following image presentation. We show that the combination of deep learning and subject embedding is crucial to cl
    
[^145]: 具有最优统计率和隐私保证的拜占庭鲁棒联邦学习

    Byzantine-Robust Federated Learning with Optimal Statistical Rates and Privacy Guarantees. (arXiv:2205.11765v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.11765](http://arxiv.org/abs/2205.11765)

    本文提出了一种具有近乎最优统计率的拜占庭鲁棒联邦学习协议，并展示了其与竞争协议相比的经验优越性，协议通过分桶可以结合隐私保障程序以对半诚实服务器进行安全保障。

    

    我们提出了一种具有近乎最优统计率的拜占庭鲁棒联邦学习协议。与之前的工作相比，我们提出的协议提高了维度依赖性，并在强凸损失的所有参数方面实现了紧密的统计率。我们对竞争协议进行了基准测试，并展示了所提出协议的经验优越性。最后，我们指出，我们的分桶协议可以与隐私保障程序自然地结合起来，以引入对半诚实服务器的安全保障。评估代码位于https://github.com/wanglun1996/secure-robust-federated-learning中。

    We propose Byzantine-robust federated learning protocols with nearly optimal statistical rates. In contrast to prior work, our proposed protocols improve the dimension dependence and achieve a tight statistical rate in terms of all the parameters for strongly convex losses. We benchmark against competing protocols and show the empirical superiority of the proposed protocols. Finally, we remark that our protocols with bucketing can be naturally combined with privacy-guaranteeing procedures to introduce security against a semi-honest server. The code for evaluation is provided in https://github.com/wanglun1996/secure-robust-federated-learning.
    
[^146]: GAN作为渐进流的梯度下降

    GANs as Gradient Flows that Converge. (arXiv:2205.02910v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.02910](http://arxiv.org/abs/2205.02910)

    本文通过在概率密度函数空间中的梯度下降方式来解决无监督学习问题，证明了GAN的训练过程可以被视为一种分布依赖的ODE的模拟。GAN算法最小化了两组样本之间的均方误差，并且只有在判别器足够强大时才能真正收敛。

    

    本文通过在概率密度函数空间中的梯度下降方法来解决无监督学习问题。其中，一个主要结果表明，通过分布依赖的普通微分方程引导的梯度流，未知的数据分布将在长时间极限下出现。也就是说，通过模拟分布依赖ODE，可以揭示数据分布。有趣的是，ODE的模拟被证明等价于生成对抗网络（GAN）的训练。这种等价关系提供了GAN的新“合作”视角，并且更重要的是，为GAN的发散带来了新的启示。特别地，它揭示了GAN算法隐式地最小化了两组样本之间的均方误差（MSE），仅仅这个MSE拟合就足以导致GAN发散。为了构建分布依赖ODE的解，我们首先证明相关的非线性Fokker-Planck方程的弱解是唯一的，这是通过Crandall-Liggett定理得到的。然后，我们展示了使用一个已知的反向采样技术的简单修改，可以构建一个数值方案来近似ODE的解。最后，我们使用这个数值方案来展示，当判别器足够强大时，GAN算法确实收敛于数据分布，从而为GAN收敛的长期难题提供了新的视角。

    This paper approaches the unsupervised learning problem by gradient descent in the space of probability density functions. A main result shows that along the gradient flow induced by a distribution-dependent ordinary differential equation (ODE), the unknown data distribution emerges as the long-time limit. That is, one can uncover the data distribution by simulating the distribution-dependent ODE. Intriguingly, the simulation of the ODE is shown equivalent to the training of generative adversarial networks (GANs). This equivalence provides a new "cooperative" view of GANs and, more importantly, sheds new light on the divergence of GANs. In particular, it reveals that the GAN algorithm implicitly minimizes the mean squared error (MSE) between two sets of samples, and this MSE fitting alone can cause GANs to diverge. To construct a solution to the distribution-dependent ODE, we first show that the associated nonlinear Fokker-Planck equation has a unique weak solution, by the Crandall-Lig
    
[^147]: 强化学习的奖励报告

    Reward Reports for Reinforcement Learning. (arXiv:2204.10817v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.10817](http://arxiv.org/abs/2204.10817)

    本文提出一种名为奖励报告的框架，用于记录已部署和迭代更新的学习系统。通过追踪设计选择和假设，帮助透明地传达系统目标，并跟踪目标的演变。

    

    在面对复杂的社会影响时构建对社会有益的系统需要一种动态的方法。最近的机器学习（ML）文献记录方法展示了讨论这些复杂性的文本框架的前景。然而，这些发展基于静态的机器学习范例，忽略了反馈和部署后性能的作用。同时，强化学习的最近工作表明，反馈和优化目标对系统行为的影响可能是广泛且不可预测的。在本文中，我们提出了一个名为奖励报告的框架，用于记录已部署和迭代更新的学习系统。受到强化学习技术文献的各种贡献启示，我们将奖励报告概述为跟踪一个特定自动化系统正在优化的设计选择和假设的活动文件。它们旨在作为透明地传达系统目标的手段，并跟踪随着时间的推移不断演变，从而更好地了解社会影响技术的动态。

    Building systems that are good for society in the face of complex societal effects requires a dynamic approach. Recent approaches to machine learning (ML) documentation have demonstrated the promise of discursive frameworks for deliberation about these complexities. However, these developments have been grounded in a static ML paradigm, leaving the role of feedback and post-deployment performance unexamined. Meanwhile, recent work in reinforcement learning has shown that the effects of feedback and optimization objectives on system behavior can be wide-ranging and unpredictable. In this paper we sketch a framework for documenting deployed and iteratively updated learning systems, which we call Reward Reports. Taking inspiration from various contributions to the technical literature on reinforcement learning, we outline Reward Reports as living documents that track updates to design choices and assumptions behind what a particular automated system is optimizing for. They are intended to
    
[^148]: 基于偏好的奖励学习中的因果混淆和奖励误识别

    Causal Confusion and Reward Misidentification in Preference-Based Reward Learning. (arXiv:2204.06601v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.06601](http://arxiv.org/abs/2204.06601)

    基于偏好的奖励学习中存在因果混淆和奖励误识别，非因果分散特征、偏好中的噪声以及状态的局部可观察性可能加剧奖励误识别，必须注意保证学习到的奖励的可识别性和因果性。

    

    通过基于偏好的奖励学习来学习策略是定制智能体行为的一种越来越流行的方法，但据说会容易出现虚假相关性和奖励欺骗行为。本文研究了从偏好中学习时因果混淆和奖励误识别的系统性研究，而现有的大部分工作则关注于强化学习和行为克隆中的因果混淆。我们在几个基准领域上进行了一系列的敏感性和消融分析，结果表明，从偏好中学习到的奖励虽然可以在学习环境中获得最小的测试误差，但在分布不同的状态下无法进行泛化，从而导致策略在优化时表现较差。我们发现非因果分散特征的存在、陈述偏好中的噪声以及局部状态的可观察性都可能加剧奖励误识别。我们还确定了一组方法来解释被误识别的学习奖励。总的来说，我们观察到偏好奖励学习中优化的策略对奖励函数的选择非常敏感，必须注意确保学习到的奖励的可识别性和因果性。

    Learning policies via preference-based reward learning is an increasingly popular method for customizing agent behavior, but has been shown anecdotally to be prone to spurious correlations and reward hacking behaviors. While much prior work focuses on causal confusion in reinforcement learning and behavioral cloning, we focus on a systematic study of causal confusion and reward misidentification when learning from preferences. In particular, we perform a series of sensitivity and ablation analyses on several benchmark domains where rewards learned from preferences achieve minimal test error but fail to generalize to out-of-distribution states -- resulting in poor policy performance when optimized. We find that the presence of non-causal distractor features, noise in the stated preferences, and partial state observability can all exacerbate reward misidentification. We also identify a set of methods with which to interpret misidentified learned rewards. In general, we observe that optim
    
[^149]: 从情感信号预测加密货币回报：BERT分类器和弱监督分析

    Forecasting Cryptocurrency Returns from Sentiment Signals: An Analysis of BERT Classifiers and Weak Supervision. (arXiv:2204.05781v3 [q-fin.ST] UPDATED)

    [http://arxiv.org/abs/2204.05781](http://arxiv.org/abs/2204.05781)

    本文主要研究了使用情感信号预测加密货币回报的方法，提出了一种弱监督学习的NLP方法来解决文本数据无标签的问题，并证实它能提高预测价值和准确性。

    

    在预测中预测金融市场价格变动是一个受持续关注的话题。随着深度学习和自然语言处理（NLP）的进步以及以新闻文章、社交媒体帖子等形式提供的大量文本数据的可用性，越来越多的研究将基于文本的预测因素纳入预测模型中。我们通过引入弱监督学习，一种最近提出的NLP方法来解决文本数据无标签的问题，为这个领域做出了贡献。没有依赖变量，就不可能在自定义语料库上微调预训练的NLP模型。我们证实了使用弱标签微调的文本特征增强了预测价值，并提高了预测加密货币收益的准确性。更基本的是，我们所提出的建模范式——弱标记特定领域文本和微调预训练的NLP模型——在（金融）预测中普适适用。

    Anticipating price developments in financial markets is a topic of continued interest in forecasting. Funneled by advancements in deep learning and natural language processing (NLP) together with the availability of vast amounts of textual data in form of news articles, social media postings, etc., an increasing number of studies incorporate text-based predictors in forecasting models. We contribute to this literature by introducing weak learning, a recently proposed NLP approach to address the problem that text data is unlabeled. Without a dependent variable, it is not possible to finetune pretrained NLP models on a custom corpus. We confirm that finetuning using weak labels enhances the predictive value of text-based features and raises forecast accuracy in the context of predicting cryptocurrency returns. More fundamentally, the modeling paradigm we present, weak labeling domain-specific text and finetuning pretrained NLP models, is universally applicable in (financial) forecasting 
    
[^150]: 随机预剪枝神经网络的神经切向核分析

    On the Neural Tangent Kernel Analysis of Randomly Pruned Neural Networks. (arXiv:2203.14328v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.14328](http://arxiv.org/abs/2203.14328)

    本文研究了预剪枝在神经网络的神经切向核中的应用，并得出了两种情况下的结论。

    

    本文研究了预剪枝在神经网络的神经切向核（NTK）中的应用。通过证明，我们建立了预剪枝的权重和原始网络的NTK等价，得出了两种情况下的结论：当每层的宽度按顺序增长到无限大时，预剪枝的神经网络的NTK会渐近于原始网络的极限NTK，而宽度对稀疏参数的依赖是渐近线性的。

    Motivated by both theory and practice, we study how random pruning of the weights affects a neural network's neural tangent kernel (NTK). In particular, this work establishes an equivalence of the NTKs between a fully-connected neural network and its randomly pruned version. The equivalence is established under two cases. The first main result studies the infinite-width asymptotic. It is shown that given a pruning probability, for fully-connected neural networks with the weights randomly pruned at the initialization, as the width of each layer grows to infinity sequentially, the NTK of the pruned neural network converges to the limiting NTK of the original network with some extra scaling. If the network weights are rescaled appropriately after pruning, this extra scaling can be removed. The second main result considers the finite-width case. It is shown that to ensure the NTK's closeness to the limit, the dependence of width on the sparsity parameter is asymptotically linear, as the NT
    
[^151]: Sionna：下一代物理层研究的开源库

    Sionna: An Open-Source Library for Next-Generation Physical Layer Research. (arXiv:2203.11854v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2203.11854](http://arxiv.org/abs/2203.11854)

    Sionna是一个GPU加速的基于TensorFlow的开源库，实现了最先进算法，可以用于基准测试和端到端性能评估。它能够提供对神经网络的原生支持，使得研究人员能够更加专注于自己的研究。

    

    Sionna是一种基于TensorFlow的GPU加速开源库，用于链路层模拟。它实现了广泛而精心测试的最先进算法，可用于基准测试和端到端性能评估，并提供原生支持神经网络的集成。这使研究人员能够专注于他们的研究，使其更具影响力和可重复性，同时节省了实现他们所不熟悉的组件的时间。本文简要介绍了Sionna，解释了它的设计原则和功能，以及未来的扩展，如集成光线跟踪和自定义CUDA内核。我们认为Sionna是研究下一代通信系统（如6G）的有价值的工具，欢迎来自我们社区的贡献。

    Sionna is a GPU-accelerated open-source library for link-level simulations based on TensorFlow. It enables the rapid prototyping of complex communication system architectures and provides native support for the integration of neural networks. Sionna implements a wide breadth of carefully tested state-of-the-art algorithms that can be used for benchmarking and end-to-end performance evaluation. This allows researchers to focus on their research, making it more impactful and reproducible, while saving time implementing components outside their area of expertise. This white paper provides a brief introduction to Sionna, explains its design principles and features, as well as future extensions, such as integrated ray tracing and custom CUDA kernels. We believe that Sionna is a valuable tool for research on next-generation communication systems, such as 6G, and we welcome contributions from our community.
    
[^152]: 数据中的表征偏差：识别和解决技术的综述

    Representation Bias in Data: A Survey on Identification and Resolution Techniques. (arXiv:2203.11852v2 [cs.DB] UPDATED)

    [http://arxiv.org/abs/2203.11852](http://arxiv.org/abs/2203.11852)

    本文综述了识别和解决数据表征偏差的文献，提出基于多个因素的分类法，该偏差可能影响人工智能应用的公平性和社会影响。

    

    数据驱动算法表现的好坏取决于其所使用的数据，而数据集尤其是社交数据经常未能足够地代表少数群体。数据中的表征偏差可能出现在不同的原因中，包括历史性的歧视和数据获取和准备方法中的选择和抽样偏差。由于“垃圾进，垃圾出”，如果不解决表征偏差等问题，人工智能解决社会问题的结果就不能公平。虽然关于机器学习模型中的公平性已经有了大量的研究，包括几篇综述文章，但数据中的偏差研究相对较少。本文将作为一种数据特征，独立于后续处理的方式，综述识别和解决表征偏差的文献。本文的范围仅限于结构化（表格）和非结构化（例如图像、文本、图形）数据。文章提出了基于多个因素的分类法，对研究的技术进行分类。

    Data-driven algorithms are only as good as the data they work with, while data sets, especially social data, often fail to represent minorities adequately. Representation Bias in data can happen due to various reasons ranging from historical discrimination to selection and sampling biases in the data acquisition and preparation methods. Given that "bias in, bias out", one cannot expect AI-based solutions to have equitable outcomes for societal applications, without addressing issues such as representation bias. While there has been extensive study of fairness in machine learning models, including several review papers, bias in the data has been less studied. This paper reviews the literature on identifying and resolving representation bias as a feature of a data set, independent of how consumed later. The scope of this survey is bounded to structured (tabular) and unstructured (e.g., image, text, graph) data. It presents taxonomies to categorize the studied techniques based on multiple
    
[^153]: 衍生品定价模型的校准：多智能体强化学习观点

    Calibration of Derivative Pricing Models: a Multi-Agent Reinforcement Learning Perspective. (arXiv:2203.06865v3 [q-fin.CP] UPDATED)

    [http://arxiv.org/abs/2203.06865](http://arxiv.org/abs/2203.06865)

    本文利用多智能体强化学习提出校准衍生品定价模型问题的博弈论解决方案，并希望该方法可用于解决其他金融领域的问题。实验证明，该算法能够学习局部波动率以及最小化百慕大期权价格所需的路径依赖性。

    

    在量化金融中最基本的问题之一是存在适合给定一组期权市场价格的连续时间扩散模型。传统上，人们使用直觉、理论和经验分析的混合方法来寻找实现精确或近似匹配的模型。我们的贡献在于展示如何通过适当的博弈理论形式化问题，借助现代深度多智能体强化学习的现有进展来搜索随机过程空间，以解决这个问题。更重要的是，我们希望我们的技术可以被社区利用和扩展，以解决该领域的重要问题，如联合SPX-VIX校准问题。我们的实验表明，我们能够学习局部波动率以及在波动率过程中所需的路径依赖性，以最小化百慕大期权的价格。我们的算法可以看作是一种粒子方法，类似于Guyon et Henry-Labordere的方法。

    One of the most fundamental questions in quantitative finance is the existence of continuous-time diffusion models that fit market prices of a given set of options. Traditionally, one employs a mix of intuition, theoretical and empirical analysis to find models that achieve exact or approximate fits. Our contribution is to show how a suitable game theoretical formulation of this problem can help solve this question by leveraging existing developments in modern deep multi-agent reinforcement learning to search in the space of stochastic processes. More importantly, we hope that our techniques can be leveraged and extended by the community to solve important problems in that field, such as the joint SPX-VIX calibration problem. Our experiments show that we are able to learn local volatility, as well as path-dependence required in the volatility process to minimize the price of a Bermudan option. Our algorithm can be seen as a particle method \`{a} la Guyon et Henry-Labordere where partic
    
[^154]: NELA-GT-2022：一份用于研究新闻报道中误导信息的大型多标签新闻数据集

    NELA-GT-2022: A Large Multi-Labelled News Dataset for The Study of Misinformation in News Articles. (arXiv:2203.05659v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2203.05659](http://arxiv.org/abs/2203.05659)

    NELA-GT-2022是一份包含361个来源的1,778,361篇文章的多标签新闻数据集，可用于研究新闻报道中的误导信息。

    

    本文介绍了NELA-GT数据集的第五版，即NELA-GT-2022。该数据集包含361个来源的1,778,361篇文章，时间跨度为2022年1月1日至12月31日。与过去版本一样，NELA-GT-2022还包括来自Media Bias / Fact Check的出口级真实性标签以及嵌入收集的新闻文章中的推文。NELA-GT-2022数据集的获取链接为： https://doi.org/10.7910/DVN/AMCV2H

    In this paper, we present the fifth installment of the NELA-GT datasets, NELA-GT-2022. The dataset contains 1,778,361 articles from 361 outlets between January 1st, 2022 and December 31st, 2022. Just as in past releases of the dataset, NELA-GT-2022 includes outlet-level veracity labels from Media Bias/Fact Check and tweets embedded in collected news articles. The NELA-GT-2022 dataset can be found at: https://doi.org/10.7910/DVN/AMCV2H
    
[^155]: 无需投影的在线凸优化算法与自适应遗憾保证

    New Projection-free Algorithms for Online Convex Optimization with Adaptive Regret Guarantees. (arXiv:2202.04721v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.04721](http://arxiv.org/abs/2202.04721)

    本文提出了无需投影的新型在线凸优化算法，基于在线梯度下降算法和高效的“不可行投影”计算方法，实现了自适应遗憾保证，并达到了$O(T^{3/4})$的定常性收敛速度。

    

    本文提出了新的高效无需投影的在线凸优化算法，它们避免了计算推向可行集上的正交投影，并使用不同且可能更有效的预测。这些算法不是基于现有的跟随领导者（follow-the-leader）框架，而是基于具有新颖而有效的“不可行投影”的在线梯度下降算法。因此，我们得到了第一个能够自然地产生自适应遗憾保证（即与序列的任何子间隔相关的遗憾上限）的无需投影算法。在假设有一个线性优化预测（LOO）可供使用的情况下，对于长度为$T$的序列，我们的算法保证了$O(T^{3/4})$的自适应遗憾和$O(T^{3/4})$的定常性收敛速度。

    We present new efficient \textit{projection-free} algorithms for online convex optimization (OCO), where by projection-free we refer to algorithms that avoid computing orthogonal projections onto the feasible set, and instead relay on different and potentially much more efficient oracles. While most state-of-the-art projection-free algorithms are based on the \textit{follow-the-leader} framework, our algorithms are fundamentally different and are based on the \textit{online gradient descent} algorithm with a novel and efficient approach to computing so-called \textit{infeasible projections}. As a consequence, we obtain the first projection-free algorithms which naturally yield \textit{adaptive regret} guarantees, i.e., regret bounds that hold w.r.t. any sub-interval of the sequence. Concretely, when assuming the availability of a linear optimization oracle (LOO) for the feasible set, on a sequence of length $T$, our algorithms guarantee $O(T^{3/4})$ adaptive regret and $O(T^{3/4})$ ada
    
[^156]: 消息传递神经偏微分方程求解器

    Message Passing Neural PDE Solvers. (arXiv:2202.03376v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.03376](http://arxiv.org/abs/2202.03376)

    本文提出了一种基于神经消息传递的求解器，用反向传播优化的神经函数逼近器取代计算图中所有启发式设计的组件，并建立了一种带有完全状态更新的消息传递方法，可推广到任何具有经典解结构的偏微分方程中。

    

    偏微分方程的数值求解一直是一个困难的问题，迄今已经有一个世纪的研究历程。近年来，出现了建立神经数值混合求解器的趋势，这使得现代完全端对端学习系统的发展变得更为高效。迄今为止，大多数工作只能在一些特定的属性方面进行泛化，这些属性包括分辨率、拓扑、几何形状、边界条件、域离散正则性和维度等等。在这项工作中，我们建立了一个解决这些属性的求解器，其中所有组件都基于神经消息传递，用反向传播优化的神经函数逼近器取代了计算图中所有启发式设计的组件。我们展示了神经消息传递求解器在表示上包含一些经典的方法，如有限差分、有限体积和 WENO 方案。为了在训练自回归模型时鼓励稳定性，我们提出了一种带有完全状态更新的消息传递方法，可以推广到具有经典解结构的任何 PDE 中。

    The numerical solution of partial differential equations (PDEs) is difficult, having led to a century of research so far. Recently, there have been pushes to build neural--numerical hybrid solvers, which piggy-backs the modern trend towards fully end-to-end learned systems. Most works so far can only generalize over a subset of properties to which a generic solver would be faced, including: resolution, topology, geometry, boundary conditions, domain discretization regularity, dimensionality, etc. In this work, we build a solver, satisfying these properties, where all the components are based on neural message passing, replacing all heuristically designed components in the computation graph with backprop-optimized neural function approximators. We show that neural message passing solvers representationally contain some classical methods, such as finite differences, finite volumes, and WENO schemes. In order to encourage stability in training autoregressive models, we put forward a metho
    
[^157]: SimGRACE: 一种简单的图形对比学习框架不需要数据增强

    SimGRACE: A Simple Framework for Graph Contrastive Learning without Data Augmentation. (arXiv:2202.03104v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.03104](http://arxiv.org/abs/2202.03104)

    SimGRACE是一种简单的图形对比学习框架，它不需要数据增强就能在几个基准数据集上实现最新的结果，并采用了一种高效的对比损失函数、负采样策略、以及一些附加项来增强性能。

    

    图形对比学习已经成为图形表示学习的一种主流技术，它最大化了共享相同语义的成对图形增强之间的互信息。不幸的是，鉴于图形数据的多样性，很难在增强过程中很好地保留语义。目前，在广泛保留语义的图形对比学习中，数据增强分为三种不令人满意的方式。第一，可以通过尝试错误来手动选择每个数据集的增强。第二，可以通过繁琐的搜索选择增强。第三，可以通过引入昂贵的领域特定知识作为指导来获得增强。所有这些都限制了现有GCL方法的效率和更广泛的适用性。为了解决这些关键问题，我们提出了一个名为SimGRACE的简单图形对比学习框架，它不需要数据增强就能在几个基准数据集上实现最新的结果。SimGRACE采用基于最近提出的InfoNCE损失的简单对比损失函数，并利用高效的负采样策略。此外，我们引入了一种新颖的节点对比损失和一个简单的图形正则化项来进一步增强SimGRACE的性能。我们的实验结果证明了SimGRACE相对于其他最新的GCL方法的有效性和效率。

    Graph contrastive learning (GCL) has emerged as a dominant technique for graph representation learning which maximizes the mutual information between paired graph augmentations that share the same semantics. Unfortunately, it is difficult to preserve semantics well during augmentations in view of the diverse nature of graph data. Currently, data augmentations in GCL that are designed to preserve semantics broadly fall into three unsatisfactory ways. First, the augmentations can be manually picked per dataset by trial-and-errors. Second, the augmentations can be selected via cumbersome search. Third, the augmentations can be obtained by introducing expensive domain-specific knowledge as guidance. All of these limit the efficiency and more general applicability of existing GCL methods. To circumvent these crucial issues, we propose a \underline{Sim}ple framework for \underline{GRA}ph \underline{C}ontrastive l\underline{E}arning, \textbf{SimGRACE} for brevity, which does not require data 
    
[^158]: 自适应采样的L-SVRG和L-Katyusha优化方法

    L-SVRG and L-Katyusha with Adaptive Sampling. (arXiv:2201.13387v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.13387](http://arxiv.org/abs/2201.13387)

    本文提出了一种自适应采样策略来提高L-SVRG和L-Katyusha优化方法在训练机器学习模型中的性能表现，可以在少量计算开销内实现采样分布的学习，同时不需要先验知识，并证明了其收敛性保证。

    

    基于随机梯度的优化方法，如L-SVRG及其加速变种L-Katyusha在训练机器学习模型时得到广泛应用。本文提出了一种自适应采样策略，可以实现在少量计算开销内学习采样分布，同时可以随着迭代而改变，而且不需要先验知识。对于凸目标，我们证明了L-SVRG和L-Katyusha的收敛性保证。

    Stochastic gradient-based optimization methods, such as L-SVRG and its accelerated variant L-Katyusha (Kovalev et al., 2020), are widely used to train machine learning models.The theoretical and empirical performance of L-SVRG and L-Katyusha can be improved by sampling observations from a non-uniform distribution (Qian et al., 2021). However,designing a desired sampling distribution requires prior knowledge of smoothness constants, which can be computationally intractable to obtain in practice when the dimension of the model parameter is high. To address this issue, we propose an adaptive sampling strategy for L-SVRG and L-Katyusha that can learn the sampling distribution with little computational overhead, while allowing it to change with iterates, and at the same time does not require any prior knowledge of the problem parameters. We prove convergence guarantees for L-SVRG and L-Katyusha for convex objectives when the sampling distribution changes with iterates. Our results show that
    
[^159]: 微型、始终在线且易碎: 设计选择中的偏见传递与在线机器学习工作流程。

    Tiny, always-on and fragile: Bias propagation through design choices in on-device machine learning workflows. (arXiv:2201.07677v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.07677](http://arxiv.org/abs/2201.07677)

    本研究调查了边缘端机器学习工作流程中偏见传递的设计选择。结果表明，在模型训练过程中的技术设计选择，如模型架构和优化算法，会放大和传播可靠性偏见。

    

    数十亿个分布式、异构的 IOT 设备，在个人数据上部署的边缘端机器学习，用于私密、快速且离线推理。边缘端的机器学习高度依赖上下文，对用户、用法、硬件和环境属性非常敏感。这种敏感性和机器学习中偏见的倾向使得研究边缘端机器学习中的偏见非常重要。本研究是对这一新兴领域中偏见研究的首次探索，为构建更公平的边缘端机器学习奠定了重要基础。本文通过软件工程角度调查了边缘端机器学习工作流程中偏见传递的设计选择。我们首先将可靠性偏见确定为不公平性的来源，并提出了一种量化可靠性偏见的方法。然后我们进行了一项关键词检测任务的实验，展示了复杂的和相互作用的技术设计选择如何放大和传播可靠性偏见。我们的研究结果验证了模型训练过程中的设计选择，如模型架构和优化算法，会对可靠性偏见的传播产生巨大影响。

    Billions of distributed, heterogeneous and resource constrained IoT devices deploy on-device machine learning (ML) for private, fast and offline inference on personal data. On-device ML is highly context dependent, and sensitive to user, usage, hardware and environment attributes. This sensitivity and the propensity towards bias in ML makes it important to study bias in on-device settings. Our study is one of the first investigations of bias in this emerging domain, and lays important foundations for building fairer on-device ML. We apply a software engineering lens, investigating the propagation of bias through design choices in on-device ML workflows. We first identify reliability bias as a source of unfairness and propose a measure to quantify it. We then conduct empirical experiments for a keyword spotting task to show how complex and interacting technical design choices amplify and propagate reliability bias. Our results validate that design choices made during model training, lik
    
[^160]: 用统计和机器学习打击洗钱：综述与介绍

    Fighting Money Laundering with Statistics and Machine Learning: An Introduction and Review. (arXiv:2201.04207v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2201.04207](http://arxiv.org/abs/2201.04207)

    本文介绍了银行反洗钱的统计和机器学习方法，并提出客户风险评估和可疑行为标识两个核心要素。未来的研究方向包括生成合成数据、半监督和深度学习、可解释性以及结果的公平性。

    

    洗钱是一个严重的全球性问题，但是针对反洗钱的统计和机器学习方法的科学文献却很少。本文着重于银行反洗钱，并提供了文献综述和介绍。我们提出了一个统一的术语，其中包括客户风险评估和可疑行为标识两个核心要素。我们发现，客户风险评估是通过诊断来寻找和解释风险因素，而可疑行为标识则是通过未公开的特征和手工风险指数来实现的。最后，我们讨论了未来研究的方向，其中主要挑战之一是需要更多的公共数据集，这可能可以通过生成合成数据来解决，其他可能的研究方向包括半监督和深度学习、可解释性以及结果的公平性。

    Money laundering is a profound global problem. Nonetheless, there is little scientific literature on statistical and machine learning methods for anti-money laundering. In this paper, we focus on anti-money laundering in banks and provide an introduction and review of the literature. We propose a unifying terminology with two central elements: (i) client risk profiling and (ii) suspicious behavior flagging. We find that client risk profiling is characterized by diagnostics, i.e., efforts to find and explain risk factors. On the other hand, suspicious behavior flagging is characterized by non-disclosed features and hand-crafted risk indices. Finally, we discuss directions for future research. One major challenge is the need for more public data sets. This may potentially be addressed by synthetic data generation. Other possible research directions include semi-supervised and deep learning, interpretability, and fairness of the results.
    
[^161]: 用机器学习增强宇宙天体物理学的比例关系: 应用于减少 Sunyaev-Zeldovich 荧光质量散射

    Augmenting astrophysical scaling relations with machine learning: application to reducing the Sunyaev-Zeldovich flux-mass scatter. (arXiv:2201.01305v3 [astro-ph.CO] UPDATED)

    [http://arxiv.org/abs/2201.01305](http://arxiv.org/abs/2201.01305)

    本文使用符号回归的机器学习工具，在Sunyaev-Zeldovich 荧光$-$星团质量关系中找到了一个新的代理变量，结合了$Y_\mathrm{SZ}$和电离气体浓度($c_\mathrm{gas}$)，极大地减小了该关系中的散射，提高了宇宙学分析的灵敏度。

    

    复杂的天体系统通常展示出可观测特性（如亮度、速度分散、振荡周期）之间的低离散比例关系，这些比例关系揭示了其中的物理现象，并为质量和距离估算等提供了观测工具。机器学习可以在高维度参数空间中提供快速系统的搜索新比例关系的方法。本文使用了一种名为“符号回归”的机器学习工具，将数据中的模式建模成解析方程。本文聚焦于 Sunyaev-Zeldovich 荧光$-$星团质量关系($Y_\mathrm{SZ}-M$)，这个关系中的散射影响从星团丰度数据中推断出的宇宙学参数。通过在Illustris TNG水力模拟数据上使用符号回归，我们找到了一个新的星团质量代理变量，结合了$Y_\mathrm{SZ}$和电离气体浓度($c_\mathrm{gas}$)：$M \propto Y_\mathrm{SZ}(c_\mathrm{gas}/C)^{1.7}$，其中$C$是一个常数。这个关系显著减小了$Y_\mathrm{SZ}-M$中的散射，提高了基于星系团的当前和未来的宇宙学分析的灵敏度。

    Complex astrophysical systems often exhibit low-scatter relations between observable properties (e.g., luminosity, velocity dispersion, oscillation period). These scaling relations illuminate the underlying physics, and can provide observational tools for estimating masses and distances. Machine learning can provide a fast and systematic way to search for new scaling relations (or for simple extensions to existing relations) in abstract high-dimensional parameter spaces. We use a machine learning tool called symbolic regression (SR), which models patterns in a dataset in the form of analytic equations. We focus on the Sunyaev-Zeldovich flux$-$cluster mass relation ($Y_\mathrm{SZ}-M$), the scatter in which affects inference of cosmological parameters from cluster abundance data. Using SR on the data from the IllustrisTNG hydrodynamical simulation, we find a new proxy for cluster mass which combines $Y_\mathrm{SZ}$ and concentration of ionized gas ($c_\mathrm{gas}$): $M \propto Y_\mathrm
    
[^162]: 学习增强的在线Steiner树算法

    Learning-Augmented Algorithms for Online Steiner Tree. (arXiv:2112.05353v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.05353](http://arxiv.org/abs/2112.05353)

    本文提出了一种集成机器学习预测与在线算法设计的超越最坏情况算法分析模型，并基于该模型研究了有向图和无向图的在线Steiner树问题。通过预测在线终端节点，算法在有良好预测的情况下打破在线下界，并且竞争比例会优雅地降低。

    

    本篇论文考虑了最近流行的基于机器学习预测与在线算法设计相结合的超越最坏情况算法分析模型。我们在这个模型中研究了有向图和无向图的在线Steiner树问题。在线设置下，Steiner树已知具有强劲的下界，并且任何算法的最坏情况保证都距离理想很远。本文考虑了预测在线终端节点的算法。这些预测可能是错误的，算法的性能取决于预测错误的终端节点数量。这些保证确保了算法在有良好预测的情况下打破在线下界，并且当预测错误率增加时，竞争比例会优雅地降低。然后我们观察到理论是预测实验结果的准确性。我们展示了在终端节点从分布中抽取的图上，新的在线算法即使具有适度的正确预测值，也具有强大的性能。

    This paper considers the recently popular beyond-worst-case algorithm analysis model which integrates machine-learned predictions with online algorithm design. We consider the online Steiner tree problem in this model for both directed and undirected graphs. Steiner tree is known to have strong lower bounds in the online setting and any algorithm's worst-case guarantee is far from desirable. This paper considers algorithms that predict which terminal arrives online. The predictions may be incorrect and the algorithms' performance is parameterized by the number of incorrectly predicted terminals. These guarantees ensure that algorithms break through the online lower bounds with good predictions and the competitive ratio gracefully degrades as the prediction error grows. We then observe that the theory is predictive of what will occur empirically. We show on graphs where terminals are drawn from a distribution, the new online algorithms have strong performance even with modestly correct 
    
[^163]: 通过Langevin函数方差估计超参数模型的泛化缺口

    A generalization gap estimation for overparameterized models via the Langevin functional variance. (arXiv:2112.03660v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2112.03660](http://arxiv.org/abs/2112.03660)

    本文提出了一种函数方差的Langevin估计方法，用于高效计算超参数模型的泛化缺口，实现与基于梯度的优化算法一致。

    

    本文讨论了泛化缺口的估计，即泛化性能与训练性能之间的差异，针对包括神经网络在内的超参数模型。我们首先展示了函数方差——一个定义广泛的信息准则中的关键概念——在超参数模型中也能表征泛化缺口，即使传统理论无法应用于超参数模型。由于函数方差的计算成本对于超参数模型而言非常昂贵，因此我们提出了函数方差的高效近似方法——函数方差的Langevin估计（Langevin FV）。这种方法只利用了平方损失函数的一阶梯度，而没有使用二阶梯度，从而保证了计算的高效性，并且实现与基于梯度的优化算法是一致的。我们通过数值演示了Langevin FV，估计了超参数模型的泛化缺口。

    This paper discusses the estimation of the generalization gap, the difference between generalization performance and training performance, for overparameterized models including neural networks. We first show that a functional variance, a key concept in defining a widely-applicable information criterion, characterizes the generalization gap even in overparameterized settings where a conventional theory cannot be applied. As the computational cost of the functional variance is expensive for the overparameterized models, we propose an efficient approximation of the function variance, the Langevin approximation of the functional variance (Langevin FV). This method leverages only the $1$st-order gradient of the squared loss function, without referencing the $2$nd-order gradient; this ensures that the computation is efficient and the implementation is consistent with gradient-based optimization algorithms. We demonstrate the Langevin FV numerically by estimating the generalization gaps of o
    
[^164]: 基于黎曼Matern核的几何感知贝叶斯优化在机器人学中的应用

    Geometry-aware Bayesian Optimization in Robotics using Riemannian Mat\'ern Kernels. (arXiv:2111.01460v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2111.01460](http://arxiv.org/abs/2111.01460)

    本文探讨了基于Riemannian Mat\'ern kernels技术在机器人学中的实现，并在一组基准函数上展示了其性能。该技术可以有效地处理非欧几里得域上定义的函数。

    

    贝叶斯优化是一种数据有效的技术，可用于控制参数调整、参数策略适应和机器人结构设计。其中许多问题需要优化在非欧几里得域上定义的函数，如球、旋转群或正定矩阵空间。为此，必须在所关注的空间中放置高斯过程先验，或等价地定义一个核。有效的核通常反映了它们所定义空间的几何特性，但设计它们通常是非平凡的。基于随机偏微分方程和拉普拉斯 – 贝尔特拉米算子的谱理论的最近研究，提供了构建这种几何感知核的有前途的途径。本文研究了在机器人学中实现这些内核的技术，在一组人工基准功能上展示了它们的性能，并说明了几何感知优化在实际机器人控制问题中的应用。

    Bayesian optimization is a data-efficient technique which can be used for control parameter tuning, parametric policy adaptation, and structure design in robotics. Many of these problems require optimization of functions defined on non-Euclidean domains like spheres, rotation groups, or spaces of positive-definite matrices. To do so, one must place a Gaussian process prior, or equivalently define a kernel, on the space of interest. Effective kernels typically reflect the geometry of the spaces they are defined on, but designing them is generally non-trivial. Recent work on the Riemannian Mat\'ern kernels, based on stochastic partial differential equations and spectral theory of the Laplace-Beltrami operator, offers promising avenues towards constructing such geometry-aware kernels. In this paper, we study techniques for implementing these kernels on manifolds of interest in robotics, demonstrate their performance on a set of artificial benchmark functions, and illustrate geometry-aware
    
[^165]: 知识花束：适用于知识图谱嵌入的花扭理论框架

    Knowledge Sheaves: A Sheaf-Theoretic Framework for Knowledge Graph Embedding. (arXiv:2110.03789v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.03789](http://arxiv.org/abs/2110.03789)

    本研究提出了一种“知识花扭”方法来描述知识图谱嵌入模型，并可以表达广泛的嵌入先验约束，可轻松应对复合关系推理。

    

    知识图谱嵌入指学习实体（图的顶点）和关系（图的边）的表示，以使得生成的表示编码知识图谱中已知的事实信息，并可以用于推理新的关系。我们展示了知识图谱嵌入自然地表达为“细胞花扭”的拓扑和范畴语言：一个知识图谱嵌入可以描述为适当的“知识花扭”在图上的近似全局截面，其一致性约束是由知识图谱的架构引起的。这种方法提供了一个广义的框架来推理知识图谱嵌入模型，并允许表达广泛的嵌入先验约束。此外，生成的嵌入可以轻松地适应于复合关系的推理，无需特殊的训练。我们实现了这些想法，以展示这种方法的效果。

    Knowledge graph embedding involves learning representations of entities -the vertices of the graph -- and relations -- the edges of the graph -- such that the resulting representations encode the known factual information represented by the knowledge graph and can be used in the inference of new relations. We show that knowledge graph embedding is naturally expressed in the topological and categorical language of \textit{cellular sheaves}: a knowledge graph embedding can be described as an approximate global section of an appropriate \textit{knowledge sheaf} over the graph, with consistency constraints induced by the knowledge graph's schema. This approach provides a generalized framework for reasoning about knowledge graph embedding models and allows for the expression of a wide range of prior constraints on embeddings. Further, the resulting embeddings can be easily adapted for reasoning over composite relations without special training. We implement these ideas to highlight the be
    
[^166]: 近似牛顿策略梯度算法

    Approximate Newton policy gradient algorithms. (arXiv:2110.02398v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.02398](http://arxiv.org/abs/2110.02398)

    本文提出了一种用于政策梯度算法的近似牛顿方法，包括自然策略梯度算法和全新的策略梯度算法，具有快速收敛的优势。

    

    最近几年，政策梯度算法被广泛应用于马尔可夫决策过程和强化学习问题中。常常使用各种熵函数进行正则化，以鼓励探索和提高稳定性。本文提出了一种用于熵正则化策略梯度算法的近似牛顿方法。在Shannon熵的情况下，所得到的算法复制了自然策略梯度算法。对于其他熵函数，这种方法得到了全新的策略梯度算法。我们证明了所有这些算法具有牛顿类型的二次收敛性，并且相应的梯度流全局收敛于最优解。我们使用合成和工业规模的示例来证明，所提出的近似牛顿方法通常在单位数迭代中收敛，并且往往比其他最先进的算法快几个数量级。

    Policy gradient algorithms have been widely applied to Markov decision processes and reinforcement learning problems in recent years. Regularization with various entropy functions is often used to encourage exploration and improve stability. This paper proposes an approximate Newton method for the policy gradient algorithm with entropy regularization. In the case of Shannon entropy, the resulting algorithm reproduces the natural policy gradient algorithm. For other entropy functions, this method results in brand-new policy gradient algorithms. We prove that all these algorithms enjoy Newton-type quadratic convergence and that the corresponding gradient flow converges globally to the optimal solution. We use synthetic and industrial-scale examples to demonstrate that the proposed approximate Newton method typically converges in single-digit iterations, often orders of magnitude faster than other state-of-the-art algorithms.
    
[^167]: 具有互补约束的数学规划问题作为双层成像学习问题的改进和理论研究

    Bilevel Imaging Learning Problems as Mathematical Programs with Complementarity Constraints: Reformulation and Theory. (arXiv:2110.02273v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2110.02273](http://arxiv.org/abs/2110.02273)

    本论文提出了一种新的数学规划问题（MPCC）的方法用于解决双层成像学习问题。该方法在满足一定条件下，可以得出稳定条件、最优性条件和局部唯一性结果，可用于函数空间中的问题，并且可以加入对梯度的约束。

    

    我们研究了一类双层成像学习问题，其中较低层次的实例对应于涉及一阶和二阶非平滑稀疏正则化器的凸变分模型。通过使用较低级问题的原始对偶重构的几何性质和引入适当的辅助变量，我们能够将原始的双层问题重新表述为具有互补约束的数学规划问题（MPCC）。对于后者，我们证明了紧的约束条件合规性条件（MPCC-RCPLD和部分MPCC-LICQ）并导出Mordukhovich（M-）和Strong（S-）稳定条件。MPCC的静止系统也可以成为原始公式的静止条件。此外，导出了二阶充分最优性条件，还得出了静止点的局部唯一性结果。所提出的改进可以扩展到函数空间中的问题，导致对梯度的约束的具有互补约束的数学规划问题。

    We investigate a family of bilevel imaging learning problems where the lower-level instance corresponds to a convex variational model involving firstand second-order nonsmooth sparsity-based regularizers. By using geometric properties of the primal-dual reformulation of the lower-level problem and introducing suitable auxiliar variables, we are able to reformulate the original bilevel problems as Mathematical Programs with Complementarity Constraints (MPCC). For the latter, we prove tight constraint qualification conditions (MPCC-RCPLD and partial MPCC-LICQ) and derive Mordukhovich (M-) and Strong (S-) stationarity conditions. The stationarity systems for the MPCC turn also into stationarity conditions for the original formulation. Second-order sufficient optimality conditions are derived as well, together with a local uniqueness result for stationary points. The proposed reformulation may be extended to problems in function spaces, leading to MPCC's with constraints on the gradient 
    
[^168]: 面向动态时空预测的长距离Transformer

    Long-Range Transformers for Dynamic Spatiotemporal Forecasting. (arXiv:2109.12218v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.12218](http://arxiv.org/abs/2109.12218)

    本研究提出了一种名为Spacetimeformer的方法，将多元时间序列预测转化为“时空序列”形式进行建模，实现了对变量之间动态空间关系的学习，同时在多个基准测试上取得了最先进的结果。

    

    多元时间序列预测致力于基于历史情境预测未来值。现有序列到序列模型利用神经关注机制进行时间学习，但未考虑变量间的空间关系。相比而言，基于图神经网络的方法明确建模变量关系，但往往依赖于预定义图，不能随时间变化且在每个时间步骤中对各变量进行独立的空间和时间更新。我们的工作通过将多元预测转化为“时空序列”形式来解决这些问题，其中每个Transformer输入表示给定时间单个变量的值。长距离Transformer可以沿着这个扩展序列共同学习空间、时间和值信息之间的交互。我们的方法称为Spacetimeformer，在多个多元预测基准测试上取得了最先进的结果，并可以动态更新变量关系。

    Multivariate time series forecasting focuses on predicting future values based on historical context. State-of-the-art sequence-to-sequence models rely on neural attention between timesteps, which allows for temporal learning but fails to consider distinct spatial relationships between variables. In contrast, methods based on graph neural networks explicitly model variable relationships. However, these methods often rely on predefined graphs that cannot change over time and perform separate spatial and temporal updates without establishing direct connections between each variable at every timestep. Our work addresses these problems by translating multivariate forecasting into a "spatiotemporal sequence" formulation where each Transformer input token represents the value of a single variable at a given time. Long-Range Transformers can then learn interactions between space, time, and value information jointly along this extended sequence. Our method, which we call Spacetimeformer, achie
    
[^169]: 基于数据的本构关系揭示流体力学传输系数的比例定律

    Data-Driven Constitutive Relation Reveals Scaling Law for Hydrodynamic Transport Coefficients. (arXiv:2108.00413v4 [physics.flu-dyn] UPDATED)

    [http://arxiv.org/abs/2108.00413](http://arxiv.org/abs/2108.00413)

    本文研究了基于数据的模型中导数选择及其局限性，认为这些模型相当于非线性长度尺度比例定律，建模比例定律可避免实际困难。

    

    寻找从高密度气体到稀薄气体区域均适用的扩展流体力学方程仍是一个重大挑战，成功的关键在于获得精确的应力和热流的本构关系。基于数据的模型通过对高阶导数的回归，使得本构关系能够扩展牛顿粘度定律和傅里叶热传导定律。然而，这些模型中导数的选择是即兴决策的，缺乏明确的物理解释。我们对线性系统进行了理论探究，并认为这些模型相当于非线性长度尺度比例定律。比例定律的等效性证明了它的物理可行性，并揭示了基于数据的模型的局限性。我们的观点还指出，建模比例定律可以避免基于数据的模型中的实际困难，如：

    Finding extended hydrodynamics equations valid from the dense gas region to the rarefied gas region remains a great challenge. The key to success is to obtain accurate constitutive relations for stress and heat flux. Data-driven models offer a new phenomenological approach to learning constitutive relations from data. Such models enable complex constitutive relations that extend Newton's law of viscosity and Fourier's law of heat conduction by regression on higher derivatives. However, the choices of derivatives in these models are ad-hoc without a clear physical explanation. We investigated data-driven models theoretically on a linear system. We argue that these models are equivalent to non-linear length scale scaling laws of transport coefficients. The equivalence to scaling laws justified the physical plausibility and revealed the limitation of data-driven models. Our argument also points out that modeling the scaling law could avoid practical difficulties in data-driven models like
    
[^170]: 最强敌人是谁？探索深度强化学习中最优和高效的规避攻击方法。

    Who Is the Strongest Enemy? Towards Optimal and Efficient Evasion Attacks in Deep RL. (arXiv:2106.05087v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.05087](http://arxiv.org/abs/2106.05087)

    本文提出了一种新的攻击方法，通过一个名为“演员”的设计函数和名为“导演”的基于RL的学习器之间的协作，最新方法可以找到最优攻击，提高了关于RL代理鲁棒性的理解。

    

    在一些限制条件下，通过在状态观察中进行最优对抗扰动来对强化学习(RL)代理的最坏情况性能进行评估，对于理解RL代理的鲁棒性至关重要。然而，找到最优对手很具有挑战性，无论是我们能否找到最优攻击的好坏，还是如何高效地找到最优攻击。现有的对抗RL工作要么使用启发式方法来寻找最强对手，要么通过将代理视为环境的一部分直接训练基于RL的对手，可以找到最佳对手，但在大状态空间中可能变得棘手。本文介绍了一种新的攻击方法，通过一个名为“演员”的设计函数和名为“导演”的基于RL的学习器之间的协作来寻找最优攻击。演员为给定的策略扰动方向制作状态扰动，导演学习提出最佳策略扰动。

    Evaluating the worst-case performance of a reinforcement learning (RL) agent under the strongest/optimal adversarial perturbations on state observations (within some constraints) is crucial for understanding the robustness of RL agents. However, finding the optimal adversary is challenging, in terms of both whether we can find the optimal attack and how efficiently we can find it. Existing works on adversarial RL either use heuristics-based methods that may not find the strongest adversary, or directly train an RL-based adversary by treating the agent as a part of the environment, which can find the optimal adversary but may become intractable in a large state space. This paper introduces a novel attacking method to find the optimal attacks through collaboration between a designed function named "actor" and an RL-based learner named "director". The actor crafts state perturbations for a given policy perturbation direction, and the director learns to propose the best policy perturbation
    
[^171]: 追寻可教学自主智能体

    Towards Teachable Autotelic Agents. (arXiv:2105.11977v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2105.11977](http://arxiv.org/abs/2105.11977)

    本文提出了可教学自主智能体的概念，它们可以从内部和教学信号中同步学习，从而提高技能习得的效率，此举是构建具有人类级智能的代理的关键步骤。

    

    自主探索和直接指导是儿童学习的两个不同来源，但教育科学证明，辅助发现或引导游戏等混合方法可以提高技能习得。在人工智能领域，这些极端分别映射为自主代理从自己的信号中学习，以及完全被教师教授的交互式学习代理。在两者之间应该站立可教学的自主智能体（TAA）：它们从内部和教学信号中学习，以从辅助发现的更高效率中受益。设计这样的代理将使真实世界的非专业用户将代理的学习轨迹定向于他们的期望。更基本地，这也可能是构建具有人类级智能的代理的关键步骤。本文提出了通向可教学自主代理设计的路线图。基于发展心理学和教育科学，我们首先确定了关键特征。

    Autonomous discovery and direct instruction are two distinct sources of learning in children but education sciences demonstrate that mixed approaches such as assisted discovery or guided play result in improved skill acquisition. In the field of Artificial Intelligence, these extremes respectively map to autonomous agents learning from their own signals and interactive learning agents fully taught by their teachers. In between should stand teachable autotelic agents (TAA): agents that learn from both internal and teaching signals to benefit from the higher efficiency of assisted discovery. Designing such agents will enable real-world non-expert users to orient the learning trajectories of agents towards their expectations. More fundamentally, this may also be a key step to build agents with human-level intelligence. This paper presents a roadmap towards the design of teachable autonomous agents. Building on developmental psychology and education sciences, we start by identifying key fe
    
[^172]: 在生成潜在形状模型中解离几何变形空间

    Disentangling Geometric Deformation Spaces in Generative Latent Shape Models. (arXiv:2103.00142v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2103.00142](http://arxiv.org/abs/2103.00142)

    研究改进了先前的生成模型，通过将物体几何空间分解为刚性方向、非刚性姿态和固有形状，并使用谱几何和概率分离的组合进行训练，赋予了对对象变形空间的可解释性描述。同时改进包括更复杂的旋转不变处理和使用流形变形网络来连接潜在和谱空间，可以更好地控制生成形状。

    

    一个完整的 3D 对象描述需要用可解释的方式表征变形空间，从单个实例的关节构造到跨类别的形状变化。在这项工作中，我们改进了一种先前的 3D 形状几何分离生成模型，该模型将物体几何空间分解为刚性方向、非刚性姿态和固有形状。该模型可以通过经典谱几何和结构化潜在表示空间的概率分离的组合，从原始 3D 形状进行训练，无需对应关系、标签，甚至无需刚性对齐。我们的改进包括更复杂的旋转不变处理和使用流形变形网络来连接潜在和谱空间。潜在空间的几何结构赋予了对对象变形空间的可解释性描述。此外，它还可以使姿态转移和姿态感知注册等任务更加容易，同时可以更好地控制生成形状。

    A complete representation of 3D objects requires characterizing the space of deformations in an interpretable manner, from articulations of a single instance to changes in shape across categories. In this work, we improve on a prior generative model of geometric disentanglement for 3D shapes, wherein the space of object geometry is factorized into rigid orientation, non-rigid pose, and intrinsic shape. The resulting model can be trained from raw 3D shapes, without correspondences, labels, or even rigid alignment, using a combination of classical spectral geometry and probabilistic disentanglement of a structured latent representation space. Our improvements include more sophisticated handling of rotational invariance and the use of a diffeomorphic flow network to bridge latent and spectral space. The geometric structuring of the latent space imparts an interpretable characterization of the deformation space of an object. Furthermore, it enables tasks like pose transfer and pose-aware r
    
[^173]: Q学习是否是极小极大最优的？一项紧密的样本复杂性分析。

    Is Q-Learning Minimax Optimal? A Tight Sample Complexity Analysis. (arXiv:2102.06548v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2102.06548](http://arxiv.org/abs/2102.06548)

    本文通过紧密的样本复杂度分析回答了Q学习是否是极小极大最优的问题。

    

    Q学习是强化学习的核心，旨在以模型自由的方式学习马尔可夫决策过程的最优Q函数。针对同步设置（即在每次迭代中从生成模型中独立地抽取所有状态-动作对的样本），在理解Q学习的样本效率方面已经取得了重大进展。对于一个具有状态空间Σ和动作空间Α的γ折扣的无限时间阶段MDP，为了产生最优Q函数的元素级ε近似，针对Q学习的最新理论需要一个样本大小超过Σ∣∣×Α∣∣∕(1−γ)^5ε^{2}的量级，但这并不符合现有的极小极大下界。这引出了一个自然的问题：Q学习的样本复杂性是多少？Q学习是否可证明是次优的？本文针对同步设置回答了这些问题：(1)

    Q-learning, which seeks to learn the optimal Q-function of a Markov decision process (MDP) in a model-free fashion, lies at the heart of reinforcement learning. When it comes to the synchronous setting (such that independent samples for all state-action pairs are drawn from a generative model in each iteration), substantial progress has been made towards understanding the sample efficiency of Q-learning. Consider a $\gamma$-discounted infinite-horizon MDP with state space $\mathcal{S}$ and action space $\mathcal{A}$: to yield an entrywise $\varepsilon$-approximation of the optimal Q-function, state-of-the-art theory for Q-learning requires a sample size exceeding the order of $\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^5\varepsilon^{2}}$, which fails to match existing minimax lower bounds. This gives rise to natural questions: what is the sharp sample complexity of Q-learning? Is Q-learning provably sub-optimal? This paper addresses these questions for the synchronous setting: (1) wh
    
[^174]: 基于虚假测试的强化学习的鲁棒性对抗方法

    Falsification-Based Robust Adversarial Reinforcement Learning. (arXiv:2007.00691v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2007.00691](http://arxiv.org/abs/2007.00691)

    本论文提出了基于虚假测试的鲁棒性对抗学习框架，通过使用虚假测试方法提高了强化学习算法在测试场景下的准确性和鲁棒性，得到了显著的实验结果。

    

    强化学习已经在各种顺序决策问题中取得了巨大进展，如机器人控制任务。由于策略对培训环境进行了过度拟合，强化学习方法通常无法推广到安全关键的测试场景中。我们提出了基于虚假测试的鲁棒性对抗学习 (FRARL) 框架，通过虚假测试方法在对抗学习中整合时间逻辑，从而提高策略的鲁棒性。我们在不同形态和任务的三个机器人系统上展示了我们方法的有效性。

    Reinforcement learning (RL) has achieved enormous progress in solving various sequential decision-making problems, such as control tasks in robotics. Since policies are overfitted to training environments, RL methods have often failed to be generalized to safety-critical test scenarios. Robust adversarial RL (RARL) was previously proposed to train an adversarial network that applies disturbances to a system, which improves the robustness in test scenarios. However, an issue of neural network-based adversaries is that integrating system requirements without handcrafting sophisticated reward signals are difficult. Safety falsification methods allow one to find a set of initial conditions and an input sequence, such that the system violates a given property formulated in temporal logic. In this paper, we propose falsification-based RARL (FRARL): this is the first generic framework for integrating temporal logic falsification in adversarial learning to improve policy robustness. By applyin
    

