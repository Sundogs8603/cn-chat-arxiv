# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Dexterity from Touch: Self-Supervised Pre-Training of Tactile Representations with Robotic Play.](http://arxiv.org/abs/2303.12076) | T-Dex是一种基于触觉的灵巧性方法，可在自我监督式的触觉编码器和一些灵巧性任务演示的指导下，将触觉和视觉结合起来，相比于传统的纯视觉方法更有效，并在现实世界中具有更好的应用性。 |
| [^2] | [Grading Conversational Responses Of Chatbots.](http://arxiv.org/abs/2303.12038) | 本文研究了ChatGPT聊天机器人对问题的回答表现，并使用标准的机器翻译评测指标进行比较，结果表明它们的表现仍然落后于典型人类的反应能力。 |
| [^3] | [The Representational Status of Deep Learning Models.](http://arxiv.org/abs/2303.12032) | 该论文澄清了深度学习模型的表征状态。尽管通常称为“表征”，但实际上它们更适合理解为高度理想化的模型，这一结果对可解释的AI有着直接影响，也引起了哲学家对其在未来科学研究中的作用的关注。 |
| [^4] | [Semantic Latent Space Regression of Diffusion Autoencoders for Vertebral Fracture Grading.](http://arxiv.org/abs/2303.12031) | 提出了一种利用扩散自编码器的语义潜空间回归进行椎体骨折分级的新方法，并通过连续回归来反映骨折的平稳进展。通过无监督特征提取器和数据增强，该方法取得了显著的改进。 |
| [^5] | [Graph Kalman Filters.](http://arxiv.org/abs/2303.12021) | 本文首次将卡尔曼和扩展卡尔曼滤波器推广到图形上，使得它可以适用于输出是向量或标量的情况，并且可以学习未知的状态转移和读取函数。 |
| [^6] | [End-to-End Integration of Speech Separation and Voice Activity Detection for Low-Latency Diarization of Telephone Conversations.](http://arxiv.org/abs/2303.12002) | 本文重点研究了低延迟流式分离应用中的基于语音分离的发言者分离（SSGD）在会话电话语音（CTS）领域中的应用。通过分离说话人并在每个分离的流上应用语音活动检测（VAD）来执行发言者分离，提出了一种新型、因果和计算效率高的泄漏去除算法。在CALLHOME和Fisher语料库（第1和2部分）上的性能评估表明，SSGD算法能够有效地提高分离和发言者分离的性能。 |
| [^7] | [Deep trip generation with graph neural networks for bike sharing system expansion.](http://arxiv.org/abs/2303.11977) | 本文提出了一种使用图神经网络进行共享单车系统扩展的深度出行生成方法，用于预测基于多源城市建筑和地理数据的站点需求。 |
| [^8] | [The Power of Nudging: Exploring Three Interventions for Metacognitive Skills Instruction across Intelligent Tutoring Systems.](http://arxiv.org/abs/2303.11965) | 本研究探究了涉及推动、示例和提出三种干预方法教授默认学生何时如何使用哪种策略，研究结果表明Nudge表现最佳。 |
| [^9] | [Strategic Trading in Quantitative Markets through Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2303.11959) | 本文将CPPI和TIPP策略集成到多智能体深度确定性策略梯度中，提出了两种特定设计的MARL方法CPPI-MADDPG和TIPP-MADDPG，用于量化市场的战略交易，结果显示这些方法通常优于传统方法。 |
| [^10] | [Formulation of Weighted Average Smoothing as a Projection of the Origin onto a Convex Polytope.](http://arxiv.org/abs/2303.11958) | 本文研究了加权移动平均平滑器的最佳加权窗口，将问题转化为原点在凸多面体上的投影，并提供了一些解决方案。 |
| [^11] | [Bayesian Optimization for Function Compositions with Applications to Dynamic Pricing.](http://arxiv.org/abs/2303.11954) | 本文提出了一种基于贝叶斯优化的函数组合方法，可以解决黑盒函数难以评估的问题，并在动态定价中得到应用。 |
| [^12] | [Hierarchical Memory Pool Based Edge Semi-Supervised Continual Learning Method.](http://arxiv.org/abs/2303.11952) | 本文提出了名为EdgeHML的低成本半监督增量学习方法，通过分层存储池和多级存储结构，利用大量未标记样本和少量标记样本，有效应对灾难性遗忘和边缘智能领域的资源限制问题。 |
| [^13] | [A fuzzy adaptive evolutionary-based feature selection and machine learning framework for single and multi-objective body fat prediction.](http://arxiv.org/abs/2303.11949) | 本文提出了一种模糊自适应进化特征选择与机器学习框架用于单目标和多目标身体脂肪预测，该方法在管理参数化和计算成本的同时确定了适当的探索和开发水平，可以避免陷入局部最优问题。 |
| [^14] | [Unsupervised Cross-Domain Rumor Detection with Contrastive Learning and Cross-Attention.](http://arxiv.org/abs/2303.11945) | 本文提出一种无监督的、基于对比学习和交叉注意力机制的跨领域谣言检测模型。该模型不仅可以进行跨领域特征对齐，还可以强制目标样本与源域中的相应原型对齐，并使用聚类方法产生伪标签来学习域不变表示，能够显著提高跨领域谣言检测的性能。 |
| [^15] | [High Probability Bounds for Stochastic Continuous Submodular Maximization.](http://arxiv.org/abs/2303.11937) | 本论文提出了针对随机CSF最大化问题的高概率边界算法，取得了比现有方法更好的收敛速度，经过验证在最坏情况下也能收敛到较优解。 |
| [^16] | [Clustering US Counties to Find Patterns Related to the COVID-19 Pandemic.](http://arxiv.org/abs/2303.11936) | 该论文旨在利用聚类算法找到美国疫情相关的县份的模式，并从中了解疫情。 |
| [^17] | [Vision Transformer-based Model for Severity Quantification of Lung Pneumonia Using Chest X-ray Images.](http://arxiv.org/abs/2303.11935) | 本论文提出了一种基于视觉转换器的模型，可以使用少量可训练参数量化 COVID-19 和其他肺部疾病的严重程度，具有较好的基于数据集的泛化性能。 |
| [^18] | [Sparse Distributed Memory is a Continual Learner.](http://arxiv.org/abs/2303.11934) | 该论文提出了一个使用稀疏分布式内存的修改多层感知器（MLP）模型，它是一个强大的持续学习者，并且该方法不需要记忆重放或任务信息。这是一种训练稀疏网络的新方法，具有广泛的适用性。 |
| [^19] | [Do intermediate feature coalitions aid explainability of black-box models?.](http://arxiv.org/abs/2303.11920) | 本文引入了中间概念的级别结构，利用领域专家建立部分-整体关系，从而在不同抽象级别上帮助解释黑盒模型。 |
| [^20] | [Deephys: Deep Electrophysiology, Debugging Neural Networks under Distribution Shifts.](http://arxiv.org/abs/2303.11912) | 本文介绍了一种名为Deephys的可视化和理解DNN在超出分布范围的场景中失败的工具，使用神经电生理学的概念，通过比较内分布和外分布数据集中的神经活动，无缝分析单个神经元、单个图像和类别图像集，并能揭示假特征和新特征存在导致的失败。 |
| [^21] | [Time Series Contrastive Learning with Information-Aware Augmentations.](http://arxiv.org/abs/2303.11911) | 本研究通过信息感知数据增强，解决了在对比学习中对于时间序列数据的多样增强方式选择问题。 |
| [^22] | [Non-Asymptotic Pointwise and Worst-Case Bounds for Classical Spectrum Estimators.](http://arxiv.org/abs/2303.11908) | 本文给出了非渐进误差边界，为广泛类别的谱估计器提供了误差边界，包括在特定频率处的点值误差边界和所有频率下的最坏情况误差边界，并利用该方法导出了经典谱估计器（Blackman-Tukey、Bartlett 和 Welch 估计器）的误差边界。 |
| [^23] | [Better Understanding Differences in Attribution Methods via Systematic Evaluations.](http://arxiv.org/abs/2303.11884) | 本文提出了三种新的评估方案，通过这些方案，可以更可靠地测量归因方法的可信度。 |
| [^24] | [Protective Self-Adaptive Pruning to Better Compress DNNs.](http://arxiv.org/abs/2303.11881) | 本文提出了一种新的保护性自适应剪枝（PSAP）方法，该方法利用权重稀疏比自适应地调整层的剪枝比率，并通过监督梯度来避免重要过滤器被剪枝，从而避免不可恢复的信息丢失。 |
| [^25] | [A Tale of Two Circuits: Grokking as Competition of Sparse and Dense Subnetworks.](http://arxiv.org/abs/2303.11873) | 本文研究了网络内部结构，发现Grokking现象对应于稀疏子网络的出现，该子网络在优化过程中由于少数神经元的快速各向同性增长而出现，从而与支配泛化能力差的密集子网络竞争。 |
| [^26] | [Online Transformers with Spiking Neurons for Fast Prosthetic Hand Control.](http://arxiv.org/abs/2303.11860) | 该论文采用了滑动窗口注意机制来替代Transformer中的自注意机制，从而使得使用逐个元素地处理序列，更加适用于在线信号处理，并且在手指位置回归数据集上实现了最好的准确性纪录，每个推理步骤仅需要非常短的时间窗口(3.5毫秒)。 |
| [^27] | [Dens-PU: PU Learning with Density-Based Positive Labeled Augmentation.](http://arxiv.org/abs/2303.11848) | Dens-PU是一种基于密度的正样本增强的PU Learning方法，可以用于解决二元分类问题，且在基准图像数据集上表现出最先进的结果。 |
| [^28] | [Doubly Regularized Entropic Wasserstein Barycenters.](http://arxiv.org/abs/2303.11844) | 本文提出了一种双重正则化熵Wasserstein重心公式，具有好的正则化、逼近、稳定性和（无网格）优化特性; 其中，只有在$\tau=\lambda/2$的情况下是无偏差的。 |
| [^29] | [Lipschitz-bounded 1D convolutional neural networks using the Cayley transform and the controllability Gramian.](http://arxiv.org/abs/2303.11835) | 本文提出了一个逐层参数化方法，用于实现内置鲁棒性保证的1D卷积神经网络。该方法基于CNN特征的Lipschitz常数作为鲁棒性度量，并使用Cayley变换和可控性Gram矩来实现CNN的Lipschitz连续性和无约束训练，最后在心律失常数据分类任务中取得了改进的鲁棒性。 |
| [^30] | [Materials Discovery with Extreme Properties via AI-Driven Combinatorial Chemistry.](http://arxiv.org/abs/2303.11833) | 本文提出了一种基于人工智能驱动的组合化学方法，不依赖于数据，可以发现未知材料并具有更优越的性质。实验证明这种方法比概率分布学习的模型更适合于发现更好的材料。 |
| [^31] | [GLADE: Gradient Loss Augmented Degradation Enhancement for Unpaired Super-Resolution of Anisotropic MRI.](http://arxiv.org/abs/2303.11831) | 本文提出了一种可用于加速全腹MRI扫描的新方法GLADE，它通过使用梯度映射损失来合成高分辨率等向性3D腹部MR图像。 |
| [^32] | [Addressing Class Variable Imbalance in Federated Semi-supervised Learning.](http://arxiv.org/abs/2303.11809) | 本文介绍了一种称为联邦半监督学习与类变量不平衡（FSSL-CVI）的新方法，它使用动态类别加权方案来处理FSSL中的类别变量不平衡问题，并且在多个数据集上进行了实验验证。通过实验结果，本文表明 FSSL-CVI 方法在各方面性能上优于现有的联邦学习和FSSL 方法。 |
| [^33] | [Random Inverse Problems Over Graphs: Decentralized Online Learning.](http://arxiv.org/abs/2303.11789) | 本文提出了一种基于在线数据流的分布式在线学习算法，将希尔伯特空间中的分布参数估计和再生核希尔伯特空间中的最小均方问题统一起来，并发展了一种新的L2-渐近稳定性理论。该算法在网络图为连通且正向算子序列满足无限维度时空励磁条件的情况下，能够实现均方和几乎必然的强一致估计。 |
| [^34] | [Skeleton Regression: A Graph-Based Approach to Estimation with Manifold Structure.](http://arxiv.org/abs/2303.11786) | 这是一个处理低维流形数据的回归框架，首先通过构建图形骨架来捕捉潜在的流形几何结构，然后在其上运用非参数回归技术来估计回归函数，除了具有非参数优点之外，在处理多个流形数据，嘈杂观察时也表现出较好的鲁棒性。 |
| [^35] | [Lightweight Contrastive Protein Structure-Sequence Transformation.](http://arxiv.org/abs/2303.11783) | 该论文提出了一种新的无监督学习的蛋白质结构表示预训练方法，使用强大的蛋白质语言模型和自监督结构约束，避免了破坏真实的空间结构表示和标记数据的限制。 |
| [^36] | [Exact Non-Oblivious Performance of Rademacher Random Embeddings.](http://arxiv.org/abs/2303.11774) | 本文证实了Rademacher随机投影的非遗忘性能，并建立了Schur-凹性质。这项成果为随机投影的性能提供了新的几何视角，具有更好的量化界限，填补了理论和实践之间的差距。 |
| [^37] | [Reasonable Scale Machine Learning with Open-Source Metaflow.](http://arxiv.org/abs/2303.11761) | 本文介绍了一个针对机器学习项目设计的开源框架Metaflow，其可以通过将ML代码的执行与业务逻辑的定义分离，来提高数据从业者的生产力。 |
| [^38] | [Improving Deep Dynamics Models for Autonomous Vehicles with Multimodal Latent Mapping of Surfaces.](http://arxiv.org/abs/2303.11756) | 该论文提出了一种新的方法来改进自动驾驶汽车的深度动力学模型，通过从多个感知模态中获取信息并将其存储在地图中的方式来更新潜在地图，从而实现对表面信息的感知和识别，在不同表面上的精准操纵得到保障。 |
| [^39] | [Projections of Model Spaces for Latent Graph Inference.](http://arxiv.org/abs/2303.11754) | 本文将双曲和球形模型空间的立体投影以及Riemannian隐空间的乘积应用于潜在图推断，实现了与非投影空间相当的性能并提供理论保证。 |
| [^40] | [Recommendation Systems in Libraries: an Application with Heterogeneous Data Sources.](http://arxiv.org/abs/2303.11746) | 本文介绍了一种图书馆推荐系统，利用了来自异构数据源的数据，包括借阅记录和在线社交读者的反馈和信息。研究发现，协同过滤（CF）方法优于基于内容的（CB）方法，可以提高推荐系统的性能达到47%。 |
| [^41] | [Beam Management Driven by Radio Environment Maps in O-RAN Architecture.](http://arxiv.org/abs/2303.11742) | 本文提出了一种基于REM的BM算法，使用RL从不同目标函数的角度来优化该过程，从而实现最大化接收功率和最小化波束重新选择。 |
| [^42] | [Tensor networks for quantum machine learning.](http://arxiv.org/abs/2303.11735) | 张量网络作为机器学习的成功范例，被移植回量子领域, 可以评估传统计算机无法高效解决的问题，并可在量子计算机上轻松部署。MPS，PEPS，TTN和MERA等布局被用于量子机器学习，本文介绍了它们在量子计算机上的映射，并讨论了实现技术对其性能的影响。 |
| [^43] | [Unlocking Layer-wise Relevance Propagation for Autoencoders.](http://arxiv.org/abs/2303.11734) | 本文提出了一种解释自编码器的快速解决方案，通过将深度泰勒分解框架与逐层相关传播方法相结合。结果显示，该方法计算和质量方面具有比现有方法更好的优势。 |
| [^44] | [Task-based Generation of Optimized Projection Sets using Differentiable Ranking.](http://arxiv.org/abs/2303.11724) | 该论文提出了一种使用前向神经网络通过可微分排名函数实现任务化优化投影集生成的方法，该方法能够选择出具有高价值的投影，以增强CT扫描的图像重建和诊断质量。 |
| [^45] | [Lidar Line Selection with Spatially-Aware Shapley Value for Cost-Efficient Depth Completion.](http://arxiv.org/abs/2303.11720) | 本文提出了一种采用具有空间感知Shapley值的Lidar线路选择进行成本效益的深度补全方法，成功地实现减少线路数量和保持深度补全质量的目标，使用了一半的线路就能够达到与完整的Lidar输入相当的深度精度。 |
| [^46] | [A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?.](http://arxiv.org/abs/2303.11717) | 本文通过完整的调研介绍了生成型AI，包含了从技术到应用的各个方面。ChatGPT虽然是一个有用的工具，但并不足以覆盖所有的AIGC任务，对于实现多样化的内容创造还需要GPT-5等未来的发展。 |
| [^47] | [A Single-Step Multiclass SVM based on Quantum Annealing for Remote Sensing Data Classification.](http://arxiv.org/abs/2303.11705) | 本研究提出了一种新的基于量子退火的直接多类分类的量子SVM(QMSVM)，可以快速而准确地对遥感数据进行分类。 |
| [^48] | [Linking generative semi-supervised learning and generative open-set recognition.](http://arxiv.org/abs/2303.11702) | 本研究旨在探究生成半监督学习和生成开放集识别之间的关系。SSL-GANs和OSR-GANs方法的相似性在于都要求生成器在互补空间中产生样本，并通过正则化来推广开放空间。研究结果表明SSL优化边缘-GAN在结合SSL-OSR任务方面树立新的标准，但在某些OSR任务中OSR优化的ARP-GAN仍然略优于SSL-GAN。 |
| [^49] | [A High-Frequency Focused Network for Lightweight Single Image Super-Resolution.](http://arxiv.org/abs/2303.11701) | 本文提出一种名为高频聚焦网络的模型，通过高频聚焦块（HFFB）实现对高频信息的有选择性增强，从而提高单图像超分辨率的重建效果。 |
| [^50] | [Neural networks trained on synthetically generated crystals can extract structural information from ICSD powder X-ray diffractograms.](http://arxiv.org/abs/2303.11699) | 本研究提出了一种使用合成晶体数据集，通过神经网络从ICSD粉末X射线衍射图中提取结构信息的方法，并且在空间群分类任务上，取得了比直接在ICSD晶体上进行训练更高的准确度。 |
| [^51] | [Data Augmentation For Label Enhancement.](http://arxiv.org/abs/2303.11698) | 该论文提出了一种数据增强方法用于标签增强。该方法利用特征空间的拓扑信息生成更准确的标签置信度，并采用新的非线性LE模型来显著提高标签增强的准确性。 |
| [^52] | [Transcriptomics-based matching of drugs to diseases with deep learning.](http://arxiv.org/abs/2303.11695) | 本文通过使用深度学习方法，基于转录组学进行无假设的药物和疾病匹配研究。研究结果显示，相比于其他转录组学匹配方法，我们提出的方法具备更高的性能。此外，我们的模型还能够捕捉不同基因表达之间的相互作用，为新药开发提供了新思路。 |
| [^53] | [ALOFT: A Lightweight MLP-like Architecture with Dynamic Low-frequency Transform for Domain Generalization.](http://arxiv.org/abs/2303.11674) | 本文介绍了一种轻量级的类MLP架构ALOFT，它可使用动态低频变换用于域泛化。与CNN相比，该架构更好地捕捉全局表示，因此具有更好的泛化能力。 |
| [^54] | [A Survey on Class Imbalance in Federated Learning.](http://arxiv.org/abs/2303.11673) | 本文对联邦学习中的类别不平衡问题和解决方案进行了调查和总结，包括平衡感知优化、数据增强和客户端加权等。目前仍存在着一些问题需要解决。 |
| [^55] | [Universal Smoothed Score Functions for Generative Modeling.](http://arxiv.org/abs/2303.11669) | 本文提出了通用平滑得分函数用于生成模型，通过导出其参数化的通用形式，详细描述了学习平滑后的M-密度的时间复杂度。同时，使用M个独立高斯通道的因子核对未知的兴趣密度进行平滑，并评估了其在CIFAR-10数据集上的表现。 |
| [^56] | [Uniform Risk Bounds for Learning with Dependent Data Sequences.](http://arxiv.org/abs/2303.11650) | 本文针对相关数据序列的学习，不依赖混合参数或者连续带参照的复杂度度量，展示了统一的风险界限，并提出了可应用于场景优化的相关约束随机程序的样本复杂度计算方法。 |
| [^57] | [Are uGLAD? Time will tell!.](http://arxiv.org/abs/2303.11647) | 本文提出了一种使用条件独立图的多元时间序列分割方法$\texttt{tGLAD}$，可以有效地标识时间序列的有意义的段。 |
| [^58] | [Manipulating Transfer Learning for Property Inference.](http://arxiv.org/abs/2303.11643) | 本文研究了转移学习中的属性推断攻击，攻击者可以操纵上游模型，对受害者调整的下游模型进行高效且特定的推断攻击，需要注意和防范此类攻击。 |
| [^59] | [Deep Q-Network Based Decision Making for Autonomous Driving.](http://arxiv.org/abs/2303.11634) | 本文介绍了一种将深度 Q-Network 和控制理论见解结合起来，在高速公路场景中安全导航自主车辆的方法，该方法可以产生高效且安全的驾驶行为。 |
| [^60] | [An Embarrassingly Simple Approach for Wafer Feature Extraction and Defect Pattern Recognition.](http://arxiv.org/abs/2303.11632) | 本文提出了一种非常简单但有效的从晶圆图像中提取特征的技术，其速度快，直观且可解释，在缺陷模式识别方面的表现优于传统的深度学习模型。 |
| [^61] | [Assessor-Guided Learning for Continual Environments.](http://arxiv.org/abs/2303.11624) | 本文提出了一种评估者指导学习策略，用于持续学习，其中评估者通过控制学习方向和速度来指导学习过程，以有效地学习新环境并避免灾难性干扰问题的发生。 |
| [^62] | [Blow-up Algorithm for Sum-of-Products Polynomials and Real Log Canonical Thresholds.](http://arxiv.org/abs/2303.11619) | 本文研究了和积多项式的炸裂算法和其 RLCT。 |
| [^63] | [Convergence of stochastic gradient descent on parameterized sphere with applications to variational Monte Carlo simulation.](http://arxiv.org/abs/2303.11602) | 本论文在高维度球体上，通过神经网络参数化，使用SGD算法在监督学习和无监督学习中，提供了一种新算法，并且证明了其收敛性。 |
| [^64] | [Difficulty in learning chirality for Transformer fed with SMILES.](http://arxiv.org/abs/2303.11593) | 应用SMILES序列的Transformer模型在学习分子结构的整体性和手性方面存在困难，需要进行长时间的训练。生成的描述符用于分子性质预测时的准确率从开始到训练结束都是相似的。 |
| [^65] | [Adaptive Experimentation at Scale: Bayesian Algorithms for Flexible Batches.](http://arxiv.org/abs/2303.11582) | 本文提出了一个基于贝叶斯算法的自适应实验框架，可灵活处理任何批处理大小。通过正态近似指导可扩展自适应设计，采用残余时限优化选择采样分配，实现了最先进的性能。 |
| [^66] | [Efficient Multi-stage Inference on Tabular Data.](http://arxiv.org/abs/2303.11580) | 该论文通过将推断算法简化并嵌入产品代码中，以减少网络通信，在处理表格数据的实时平台上可将推断延迟降低1.3倍，CPU资源减少30％，并将应用程序前端和后端之间的网络通信减少60％。 |
| [^67] | [Feature-adjacent multi-fidelity physics-informed machine learning for partial differential equations.](http://arxiv.org/abs/2303.11577) | 提出了一种基于特征相邻的多保真体系结构，通过共享低保真度和高保真度解决方案的特征空间来减少或消除对高精度数据的依赖，这在解决复杂问题时具有重要意义。 |
| [^68] | [Dynamic Healthcare Embeddings for Improving Patient Care.](http://arxiv.org/abs/2303.11563) | DECENT是一种自动编码异构共同演化的动态神经网络，可以从各种数据流中学习患者、医生、房间和药物的异构动态嵌入，用于改善患者护理，如药物推荐、患者风险分层和医院容量管理。 |
| [^69] | [Dynamic-Aware Loss for Learning with Label Noise.](http://arxiv.org/abs/2303.11562) | 本文提出一种动态感知损失函数，采用增强拟合能力，逐渐增加鲁棒性的权重来处理标签噪声。在后期阶段，引入自举项，让DNN更加重视容易的样例，证明了这种方法的优越性。 |
| [^70] | [Dynamic Vertex Replacement Grammars.](http://arxiv.org/abs/2303.11553) | 本文提出动态顶点替换文法（DyVeRG），它们提供一种在时间域内更新学习图文法的形式框架，用于生成和预测真实世界的动态图，同时保持了人类可解释性。 |
| [^71] | [ModEFormer: Modality-Preserving Embedding for Audio-Video Synchronization using Transformers.](http://arxiv.org/abs/2303.11551) | ModEFormer使用变压器独立提取音频和视频嵌入，并保留输入流的模态，使用更大的批量和更多的负音频样本进行对比学习，这使得其在检测电视广播和视频会议中的音视频同步方面取得了最先进的性能。 |
| [^72] | [Fix the Noise: Disentangling Source Feature for Controllable Domain Translation.](http://arxiv.org/abs/2303.11545) | 本文提出了一种新的可控领域翻译方法，通过在目标特征空间的已分解子空间中保留源特征，使得只使用单个模型就能平滑地控制保留源特征的程度，产生更一致、更逼真的图像。 |
| [^73] | [MSTFormer: Motion Inspired Spatial-temporal Transformer with Dynamic-aware Attention for long-term Vessel Trajectory Prediction.](http://arxiv.org/abs/2303.11540) | 该论文提出了一种基于Transformer和动态感知注意力机制的运动启发式船舶轨迹预测方法，通过数据增强方法描述轨迹的空间和运动特征，能够更加准确地预测船舶轨迹。 |
| [^74] | [Indeterminate Probability Neural Network.](http://arxiv.org/abs/2303.11536) | 本文提出了一种新型通用模型——不定概率神经网络；它可以进行无监督聚类和使用很小的神经网络处理大规模分类，其理论优势体现在新的概率理论和神经网络框架中。 |
| [^75] | [Counterfactually Fair Regression with Double Machine Learning.](http://arxiv.org/abs/2303.11529) | 本文提出了一种双机器学习公平性方法，用于回归问题中的对抗事实公平，假设两组变量的影响是可加的并且相互独立的，结果将近似平等，个人级别的结果将是对抗事实公平的。 |
| [^76] | [SIFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency.](http://arxiv.org/abs/2303.11525) | 本研究提出了一种名为SIFT的方法，用于提高深度神经网络的训练效率、准确性和表示能力，通过稀疏等FLOP转换，缩短训练时间。 |
| [^77] | [Online Learning for Equilibrium Pricing in Markets under Incomplete Information.](http://arxiv.org/abs/2303.11522) | 该论文研究了在不完全信息的市场中使用在线学习进行平衡定价的问题，提出了解决选择性谎言问题的新方法。 |
| [^78] | [STDLens: Model Hijacking-resilient Federated Learning for Object Detection.](http://arxiv.org/abs/2303.11511) | STDLens 是一种可以防止FL受到模型挟持的攻击的安全方法。它基于三层的取证框架来识别和排除特殊的梯度，并恢复FL的性能。STDLens在物体检测方面实现了最先进的性能并且具有防止模型挟持的鲁棒性。 |
| [^79] | [AI-in-the-Loop -- The impact of HMI in AI-based Application.](http://arxiv.org/abs/2303.11508) | 人机交互(HMI)被加入到AI架构设计过程中可以有效减少AI开发所需的资源，避免训练和评估具有非生产层的AI架构，从而导致轻量级AI架构的产生。 |
| [^80] | [FlexVDW: A machine learning approach to account for protein flexibility in ligand docking.](http://arxiv.org/abs/2303.11494) | 提出了一种采用机器学习的能量项来隐式考虑受体柔性的方法，该方法可以提高小分子配体位姿预测的结果，尤其是在存在大量蛋白形变的情况下。 |
| [^81] | [Sandwiched Video Compression: Efficiently Extending the Reach of Standard Codecs with Neural Wrappers.](http://arxiv.org/abs/2303.11473) | 本文提出了夹心视频压缩方法，通过包装标准编解码器来使神经网络优化压缩性能，在高清视频传输和语音识别视频压缩等场景中表现显著。 |
| [^82] | [Did You Train on My Dataset? Towards Public Dataset Protection with Clean-Label Backdoor Watermarking.](http://arxiv.org/abs/2303.11470) | 提出了一种基于背门数字水印的方法，以确保公共数据的安全。通过在数据集中插入极少量的数字水印样本，隐式学习一个隐藏的函数作为数字水印，以跟踪非法使用此数据集的模型。使用“清洁标签背门”方法实现了数字水印，不会破坏原始数据集。实验证明，该方法有效地检测到非法利用数据集的行为。 |
| [^83] | [Seven open problems in applied combinatorics.](http://arxiv.org/abs/2303.11464) | 这篇论文介绍了七个应用组合数学中的未解之谜，这些问题涉及量子计算，算法分化，拓扑数据分析，超图切割算法和电力系统等多个研究领域。 |
| [^84] | [Fairness-Aware Graph Filter Design.](http://arxiv.org/abs/2303.11459) | 本文设计了一种公平图滤波器，用于在图形学习任务中缓解偏见。实验表明，该设计在减轻偏见方面具有显著的功效，同时相对于基准算法，实用度更高且更稳定。 |
| [^85] | [Large Language Models and Simple, Stupid Bugs.](http://arxiv.org/abs/2303.11455) | 本文研究表明，大型语言模型如Codex虽然有助于避免一些简单Bug，但经常会生成已知的，逐字的Bug，因此需要采取避免策略来减少这种情况，并增加有创意的Bug的生产。 |
| [^86] | [How (Implicit) Regularization of ReLU Neural Networks Characterizes the Learned Function -- Part II: the Multi-D Case of Two Layers with Random First Layer.](http://arxiv.org/abs/2303.11454) | 本文提出了具有ReLU激活的随机浅层神经网络的特征描述，对于回归问题，它们类似于无限广义加性模型（IGAM） |
| [^87] | [Greedy Pruning with Group Lasso Provably Generalizes for Matrix Sensing and Neural Networks with Quadratic Activations.](http://arxiv.org/abs/2303.11453) | 本文在矩阵感知问题中研究了基于Group Lasso正则化器的贪婪剪枝方法，证明了修剪低$\ell_2$范数列的解可以泛化到新样本上。 |
| [^88] | [Bias mitigation techniques in image classification: fair machine learning in human heritage collections.](http://arxiv.org/abs/2303.11449) | 该论文研究了在人类文化遗产中运用图像分类技术时面临的公平性问题， 并且提出了三种偏见缓解技术与迁移学习来解决这一问题。研究显示，这些方案有效地降低了性别分类中的偏见。 |
| [^89] | [Geometrical aspects of lattice gauge equivariant convolutional neural networks.](http://arxiv.org/abs/2303.11448) | 本文研究了栅格规范等变卷积神经网络的全局群等变性，并提供了一个几何公式，表明L-CNN中的卷积是SU($N$)主丛上规范等变神经网络的一个特例。 |
| [^90] | [Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration.](http://arxiv.org/abs/2303.11435) | InDI是一种新的监督式图像修复公式，通过逐步改进图像质量来生成比现有回归方法更真实和详细的图像，修复效果更具有感知质量。 |
| [^91] | [ResDTA: Predicting Drug-Target Binding Affinity Using Residual Skip Connections.](http://arxiv.org/abs/2303.11434) | 本论文提出了一种只利用靶标和药物的测序信息来预测药物靶标结合亲和力的深度学习模型，该模型使用1D表示，无需额外特征或描述。 |
| [^92] | [Machine learning-based detection of cardiovascular disease using ECG signals: performance vs. complexity.](http://arxiv.org/abs/2303.11429) | 本文提出了多种基于机器学习的方法来检测心血管疾病，其中基于深度学习的图像分类器在预测房颤方面表现较好，而XGBoost在长期数据中表现良好，一维卷积模型则特别适用于长期ECG记录的分类和推断。 |
| [^93] | [Lamarr: LHCb ultra-fast simulation based on machine learning models deployed within Gauss.](http://arxiv.org/abs/2303.11428) | LHCb实验中的90%计算资源用于生产模拟数据样本，而Lamarr是一个基于机器学习模型的系统，通过对LHCb实验的探测器响应和重建算法进行参数化，加快了模拟产出。 |
| [^94] | [Learning Model-Free Robust Precoding for Cooperative Multibeam Satellite Communications.](http://arxiv.org/abs/2303.11427) | 本研究利用深度强化学习算法学习基于模型无关且不需要先验知识的稳健预编码，以实现协作多波束卫星通信。 |
| [^95] | [Heart Murmur and Abnormal PCG Detection via Wavelet Scattering Transform & a 1D-CNN.](http://arxiv.org/abs/2303.11423) | 该研究提出了一种小波散射变换和1D-CNN深度学习技术结合的心杂音自动检测方法，可实现97.98%的高准确率。 |
| [^96] | [Improving EEG-based Emotion Recognition by Fusing Time-frequency And Spatial Representations.](http://arxiv.org/abs/2303.11421) | 论文提出了一种基于跨域特征融合方法的EEG信号分类网络，其中融合了时频域和空间域的多重表示方法，该方法在EEG情感识别中取得了最先进的结果。 |
| [^97] | [EPiC: Ensemble of Partial Point Clouds for Robust Classification.](http://arxiv.org/abs/2303.11419) | 本文提出了一种基于部分点云采样的通用集成框架，由多种采样方法联合使用提高鲁棒性，达到了最优性能。 |
| [^98] | [Vibration Signal Denoising Using Deep Learning.](http://arxiv.org/abs/2303.11413) | 本文研究了基于深度学习的去除脚步引起的振动信号的噪声的方法，该方法适用于高斯噪声和非平稳噪声。 |
| [^99] | [Targeted Analysis of High-Risk States Using an Oriented Variational Autoencoder.](http://arxiv.org/abs/2303.11410) | 本文提出了一种OVAE方法，可以约束潜变量空间代码与生成数据之间的链接，以生成与电力系统中特定的高风险状态对齐的具有定向特征的合成数据，提高了对关键事件的预测和缓解的效率和精度。 |
| [^100] | [eP-ALM: Efficient Perceptual Augmentation of Language Models.](http://arxiv.org/abs/2303.11403) | 本论文提出了一种用对比学习提高语言模型的感知能力的高效方法eP-ALM，可以实现视觉感知信息和文本信息的融合，同时还能在多模态基准测试上实现最先进的结果。 |
| [^101] | [Combining Deep Metric Learning Approaches for Aerial Scene Classification.](http://arxiv.org/abs/2303.11389) | 本论文提出了六种深度度量学习方法来实现航空场景分类任务，并通过进化计算算法将它们结合起来。实验结果表明，这些方法与传统图像分类方法有类似的精度。 |
| [^102] | [MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action.](http://arxiv.org/abs/2303.11381) | MM-REACT是一种融合视觉专家和ChatGPT的多模态推理和行动系统，通过文字提示设计，实现了ChatGPT和各种视觉专家的协同，具有广泛的高级视觉理解应用价值。 |
| [^103] | [Solving High-Dimensional Inverse Problems with Auxiliary Uncertainty via Operator Learning with Limited Data.](http://arxiv.org/abs/2303.11379) | 本文提出了一个基于深度神经网络代理模型进行流图校准的框架， 并结合先前辅助过程的知识来解决高维逆问题，成功识别了高维系统中的源。 |
| [^104] | [GNN-Ensemble: Towards Random Decision Graph Neural Networks.](http://arxiv.org/abs/2303.11376) | 本文提出了一种名为GNN-Ensemble的方法，它可以构建随机决策图神经网络的集合，以提高GNNs的性能，泛化能力和抗攻击性，并遵循随机建模的原则。 |
| [^105] | [Neural Constraint Satisfaction: Hierarchical Abstraction for Combinatorial Generalization in Object Rearrangement.](http://arxiv.org/abs/2303.11373) | 该研究提出了一种神经网络约束满足的方法，通过层级结构的抽象实现了在组合状态下的物体重新排列任务的泛化能力。 |
| [^106] | [Optimized preprocessing and Tiny ML for Attention State Classification.](http://arxiv.org/abs/2303.11371) | 本文提出了一种结合信号处理和机器学习算法的EEG信号处理方法，用于注意力状态分类，在性能和效率方面优于其他最先进的方法。 |
| [^107] | [Bridging Imitation and Online Reinforcement Learning: An Optimistic Tale.](http://arxiv.org/abs/2303.11369) | 本文提出了两种算法，iPSRL和iRLSVI，旨在解决给定离线演示数据集的问题，可以显著减少强化学习中的遗憾，桥接了在线 RL 和模仿学习。 |
| [^108] | [Reflexion: an autonomous agent with dynamic memory and self-reflection.](http://arxiv.org/abs/2303.11366) | 本文提出 Reflexion 方法，给智能体赋予了动态记忆和自我反思能力，以增强其任务特定的行动选择能力。 |
| [^109] | [HDformer: A Higher Dimensional Transformer for Diabetes Detection Utilizing Long Range Vascular Signals.](http://arxiv.org/abs/2303.11340) | 本研究提出了一种新的基于高维Transformer的架构HDformer，并利用长距离PPG信号进行糖尿病检测，其中提出了一种新的注意力模块TSA，成功将标记体积减少10倍以上，提高了模型的能力和效率。 |
| [^110] | [FedMAE: Federated Self-Supervised Learning with One-Block Masked Auto-Encoder.](http://arxiv.org/abs/2303.11339) | 本文提出了一个新的联邦自监督学习框架 FedMAE，可以利用轻量级设备上的大规模未标记图像进行联邦学习。FedMAE可以预训练一个单块遮蔽自编码器，并将多个预训练的单块MAE级联在服务器上构建用于下游任务的多块ViT骨干。实验结果表明，FedMAE相较于最先进的FSSL方法具有卓越的性能。 |
| [^111] | [Towards Domain Generalization for ECG and EEG Classification: Algorithms and Benchmarks.](http://arxiv.org/abs/2303.11338) | 本论文提出了一个开源生物信号领域泛化评估基准，并引入一种专门解决生物信号中领域泛化问题的神经网络架构DGNet-Bio。通过实验证明，DGNet-Bio在ECG和EEG分类领域泛化上优于现有方法。 |
| [^112] | [Recursive Euclidean Distance Based Robust Aggregation Technique For Federated Learning.](http://arxiv.org/abs/2303.11337) | 本文提出了一种递归欧几里得距离计算的鲁棒聚合方法来防御联邦学习中的恶意攻击，该方法分配权重以最小化数据污染效应，实验表明其精度优于现有算法并且时间复杂度降低。 |
| [^113] | [Studying Limits of Explainability by Integrated Gradients for Gene Expression Models.](http://arxiv.org/abs/2303.11336) | 本文研究了使用 Integrated Gradients 进行机器学习特征归属时的可解释性极限，证明仅通过特征影响值的排序无法可靠地识别生物标志物的重要性，还提出了新的评估方法以评估可靠性。 |
| [^114] | [Training Invertible Neural Networks as Autoencoders.](http://arxiv.org/abs/2303.11239) | 本文提出了使用INN自动编码器进行训练的方法，实验证明其在大瓶颈大小的情况下优于经典自动编码器。 |
| [^115] | [Rotating without Seeing: Towards In-hand Dexterity through Touch.](http://arxiv.org/abs/2303.10880) | 本研究提出了一种新系统Touch Dexterity，通过密集二进制力传感器实现了多指机器人手不看就能旋转物体，同时大大降低了成本和与实际应用的差距。 |
| [^116] | [A Comprehensive Review of Spiking Neural Networks: Interpretation, Optimization, Efficiency, and Best Practices.](http://arxiv.org/abs/2303.10780) | 本文综述了最新的脉冲神经网络的解释、优化、效率和精度的进展，介绍了最前沿的优化、节能和评价方法，方便新从业者理解。 |
| [^117] | [Generalized partitioned local depth.](http://arxiv.org/abs/2303.10167) | 本文提出了一个广义的凝聚概念，构建在分区局部深度的技术基础上，扩展了早期结果并应用于具有不确定性的数据的社区发现中。 |
| [^118] | [Efficient and Feasible Robotic Assembly Sequence Planning via Graph Representation Learning.](http://arxiv.org/abs/2303.10135) | 本文提出了一种基于图表示学习的装配序列规划方法，通过GRACE模型可以从装配图中提取信息并预测可行的装配序列。 |
| [^119] | [Topology optimization with physics-informed neural networks: application to noninvasive detection of hidden geometries.](http://arxiv.org/abs/2303.09280) | 该论文介绍了一种基于物理知识神经网络的拓扑优化方法，应用于无先验知识的几何结构检测，通过材料密度场表示任意解决方案拓扑，并通过Eikonal正则化实现。该方法可用于医疗和工业应用中的非侵入式成像技术。 |
| [^120] | [Maximum Margin Learning of t-SPNs for Cell Classification with Filtering.](http://arxiv.org/abs/2303.09065) | 本研究提出了一种基于t-SPN算法和滤波技术的细胞分类方法，通过最大化边缘和L2正则化，该方法在HEp-2和Feulgen基准数据集上取得了最高的准确率。 |
| [^121] | [Large-scale End-of-Life Prediction of Hard Disks in Distributed Datacenters.](http://arxiv.org/abs/2303.08955) | 本文通过定制特征工程和序列学习器，提出一种编码器-解码器LSTM模型，用于大规模预测硬盘剩余寿命，从而降低运营成本 |
| [^122] | [WDiscOOD: Out-of-Distribution Detection via Whitened Linear Discriminative Analysis.](http://arxiv.org/abs/2303.07543) | 本论文提出了一种名为WDiscOOD的新型OOD检测方法，其中使用白化线性判别分析将特征投影到判别子空间和残留子空间中，确定OOD分数。在大规模ImageNet-1k基准测试和六个OOD数据集中，WDiscOOD表现出了优越的性能。 |
| [^123] | [Fast exploration and learning of latent graphs with aliased observations.](http://arxiv.org/abs/2303.07397) | 本文介绍了一种在具有别名观测的潜在图上，能够显著提高最大化探索效率的政策算法 eFeX，相比于随机策略，该算法能够更快地恢复各种拓扑结构下的图表。 |
| [^124] | [Long-tailed Classification from a Bayesian-decision-theory Perspective.](http://arxiv.org/abs/2303.06075) | 本文从贝叶斯决策理论的角度提出了一个通用且有原则的框架，为长尾分类提供了理论上的支持，并采用综合风险和贝叶斯深度集成方法以提高所有类别的准确性，特别是“长尾”类别。 |
| [^125] | [ChatGPT Is on the Horizon: Could a Large Language Model Be All We Need for Intelligent Transportation?.](http://arxiv.org/abs/2303.05382) | 本文探讨了ChatGPT在解决交通问题方面的应用。通过利用具有跨模态编码器的LLM，可以处理来自不同模态的交通数据并执行交通运营。作者提供了一个基于智能手机的碰撞报告自动生成和分析框架作为用例展示了这种潜力。 |
| [^126] | [Policy Mirror Descent Inherently Explores Action Space.](http://arxiv.org/abs/2303.04386) | 本文研究了在线策略梯度方法如何固有地探索行动空间，并首次确定了不需要探索策略的情况下，这种方法的最优样本复杂度，进一步展示了明确的行动空间探索并不一定是必要条件。 |
| [^127] | [QAID: Question Answering Inspired Few-shot Intent Detection.](http://arxiv.org/abs/2303.01593) | 本文提出了一个启发式的Few-shot意图检测方法，通过将意图检测重新定义为一个问题-回答检索任务来解决语义相似的细粒度意图问题，结果在三个few-shot意图检测基准测试上取得了最优表现。 |
| [^128] | [Privacy-Preserving Tree-Based Inference with Fully Homomorphic Encryption.](http://arxiv.org/abs/2303.01254) | 本研究介绍了一种基于全同态加密的数据隐私保护方法，能够针对加密表格数据进行任意计算，并得到了最新的解决方案，适用于一系列树型模型，包括决策树，随机森林和梯度增强树。此方法已应用在Concrete-ML开源库中，能够在准确性方面接近未受保护的版本。 |
| [^129] | [Opto-UNet: Optimized UNet for Segmentation of Varicose Veins in Optical Coherence Tomography.](http://arxiv.org/abs/2302.14808) | 本文提出了一个新的分割模型Opto-UNet，用于分割静脉壁结构，并采用U-Net架构搭建，使用孔洞和可分离卷积提取空间广范围和可分离的特征映射。 |
| [^130] | [Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning.](http://arxiv.org/abs/2302.14115) | 本文介绍了 Vid2Seq，这是一个在大规模 narrated 视频数据集上预先训练的多模态密集事件字幕模型。通过将转录语音的句子边界转化为伪事件边界，并使用转录语音句子作为伪事件字幕，我们有效利用未标注 narrated 视频数据集进行了密集视频字幕的训练。该模型在多个基准测试中表现出色，是目前最优秀的模型之一。 |
| [^131] | [PaGE-Link: Path-based Graph Neural Network Explanation for Heterogeneous Link Prediction.](http://arxiv.org/abs/2302.12465) | 这篇论文提出了一种基于路径的GNN解释方法PaGE-Link，用于异构链接预测，具有连接可解释性，模型可扩展性和处理图形异构性能力。 |
| [^132] | [Statistical Analysis of Karcher Means for Random Restricted PSD Matrices.](http://arxiv.org/abs/2302.12426) | 本文通过研究限制半正定矩阵流形上的本质均值模型及其Karcher均值的统计分析，提出了一般外部信号加噪声模型下的确定性误差界，并表明LRC-dPCA算法与全样本PCA算法性能相同。 |
| [^133] | [Explainable AI does not provide the explanations end-users are asking for.](http://arxiv.org/abs/2302.11577) | 可解释性人工智能对提高人工智能系统信任度的效果有局限性，透明度和严格的验证更适合打造可靠的人工智能系统。 |
| [^134] | [Simplifying Momentum-based Riemannian Submanifold Optimization.](http://arxiv.org/abs/2302.09738) | 本文针对黎曼子流形优化算法进行了简化，提出了黎曼正常坐标的广义版本，可用于对称正定矩阵的子流形优化，并为深度学习开发了高效的二阶优化器，无需显式矩阵求逆。 |
| [^135] | [GPT4MIA: Utilizing Generative Pre-trained Transformer (GPT-3) as A Plug-and-Play Transductive Model for Medical Image Analysis.](http://arxiv.org/abs/2302.08722) | 本文提出 GPT4MIA 方法，利用 GPT-3 作为插入式检验工具进行医学图像分析；该方法在提示结构设计、样本选择及提示排序等方面优化，能有效提高预测准确性。 |
| [^136] | [A Case Study on Designing Evaluations of ML Explanations with Simulated User Studies.](http://arxiv.org/abs/2302.07444) | 本研究进行了一项真实用例的“SimEvals”评估，以评估在电子商务欺诈检测中解释是否可以更好地支持机器学习辅助决策，并证实了其与用户研究的结论一致。 |
| [^137] | [Learning Representations of Bi-level Knowledge Graphs for Reasoning beyond Link Prediction.](http://arxiv.org/abs/2302.02601) | 本文提出了一种基于双层知识图谱的方法来学习嵌入，将三元组之间的关系考虑进去，并使用数据增强策略来增加合理的三元组。 |
| [^138] | [Diversity Induced Environment Design via Self-Play.](http://arxiv.org/abs/2302.02119) | 本文提出了一种利用自我对战技术的任务不可知方法，来识别环境中的观察/隐藏状态，并将多样性引入非监督环境设计框架中，从而提高了环境设计的效率和有效性。 |
| [^139] | [Towards Models that Can See and Read.](http://arxiv.org/abs/2301.07389) | 本文提出了UniTNT模型，该模型能兼顾场景文本和图像的理解，通过与先前的模态融合提高了图像问题回答和图像字幕生成任务的性能。 |
| [^140] | [Short-length SSVEP data extension by a novel generative adversarial networks based framework.](http://arxiv.org/abs/2301.05599) | 本文提出了一种基于GAN的端到端信号转化网络TEGAN，可以将短SSVEP信号转换成长的人工SSVEP信号，并显著提高BCI系统的效率和准确性。 |
| [^141] | [Risk-Sensitive Reinforcement Learning with Exponential Criteria.](http://arxiv.org/abs/2212.09010) | 本文介绍了一种风险敏感的强化学习算法，使用指数判据来提高其系统抗干扰性和实用性。作者进行了在模拟和实际机器人上的实验验证，表明该算法能够有效地提高样本效率和执行效果。 |
| [^142] | [Multi-Resolution Online Deterministic Annealing: A Hierarchical and Progressive Learning Architecture.](http://arxiv.org/abs/2212.08189) | 本文提出了一种基于逐渐增加子集数量的分区序列的通用的分层学习结构，并使用无梯度随机逼近更新进行在线解决优化问题的方法，可以定义函数逼近问题并使用双时间尺度随机逼近算法的理论解决，模拟了一种退火过程。 |
| [^143] | [Sim-to-Real Transfer for Quadrupedal Locomotion via Terrain Transformer.](http://arxiv.org/abs/2212.07740) | 本文介绍了一种基于地形变换器（TERT）的高容量 Transformer 模型，用于各种地形中四足运动控制，同时提出了一种新的两阶段训练框架，可以使 Transformer 更好地应用于 sim-to-real 场景中。 |
| [^144] | [Policy Adaptation from Foundation Model Feedback.](http://arxiv.org/abs/2212.07398) | 本文提出了基于基础模型反馈的策略适应（PAFF）方法，通过让策略使用随机生成的指令进行演示，并利用预训练的基础模型提供反馈来重新标记演示，自动提供新的演示-指令数据对进行策略微调，以实现机器人操作的泛化。实验结果表明，PAFF优于现有最先进的方法。 |
| [^145] | [Scheduling and Aggregation Design for Asynchronous Federated Learning over Wireless Networks.](http://arxiv.org/abs/2212.07356) | 本文提出了一种异步联邦学习的调度策略和聚合加权设计，通过采用基于信道感知数据重要性的调度策略和“年龄感知”的聚合加权设计来解决FL系统中的“拖沓”问题，并通过仿真证实了其有效性。 |
| [^146] | [Whose Emotion Matters? Speaking Activity Localisation without Prior Knowledge.](http://arxiv.org/abs/2211.15377) | 本文介绍了一种新方法MELD-FAIR来解决情感识别中的挑战，通过使用主动说话者检测和自动语音识别模型，重新对齐了MELD视频，并成功捕获了讲话者的面部表情。 |
| [^147] | [Multiagent Reinforcement Learning for Autonomous Routing and Pickup Problem with Adaptation to Variable Demand.](http://arxiv.org/abs/2211.14983) | 本论文提出了一种多智能体强化学习的学习框架，用于在城市地图上服务于随机出现的请求，可以产生协调作用并考虑先前可能出现的未来请求，能够适应不同需求分布的变化。 |
| [^148] | [SAMSON: Sharpness-Aware Minimization Scaled by Outlier Normalization for Improving DNN Generalization and Robustness.](http://arxiv.org/abs/2211.11561) | 通过应用锐度感知训练并将其优化为损失值和损失锐度，SAMSON方法显著提高了对噪声硬件的推断鲁棒性，而无需关于目标硬件的任何假设。 |
| [^149] | [Environmental Sensor Placement with Convolutional Gaussian Neural Processes.](http://arxiv.org/abs/2211.10381) | 本论文提出了一种新的方式——卷积高斯神经过程（ConvGNP），用于提高环境传感器的放置效率。ConvGNP使用神经网络来参数化联合高斯分布，通过学习空间和季节性非平稳性，优于传统的非平稳高斯过程模型。 |
| [^150] | [DeepSense 6G: A Large-Scale Real-World Multi-Modal Sensing and Communication Dataset.](http://arxiv.org/abs/2211.09769) | DeepSense 6G是一个大型真实世界多模态感知和通信数据集，旨在推动在多模态感知、通信和定位交叉领域的广泛应用中的深度学习研究。 |
| [^151] | [Contrastive learning for regression in multi-site brain age prediction.](http://arxiv.org/abs/2211.08326) | 本篇论文研究针对多中心脑龄预测回归问题的对比学习方法，通过提高数据和标签的稳健性，实现了在MRI扫描下脑龄预测的最先进性能和最佳泛化能力。 |
| [^152] | [Contextual Bandits with Packing and Covering Constraints: A Modular Lagrangian Approach via Regression.](http://arxiv.org/abs/2211.07484) | 该论文研究了一种带有资源线性约束的上下文幸存者问题的变种，提出了一种新的算法，该算法简单、计算效率高，同时能够实现较低的后悔。此外，当某些约束被违反时，算法在统计上是最优的。 |
| [^153] | [MixMask: Revisiting Masking Strategy for Siamese ConvNets.](http://arxiv.org/abs/2210.11456) | 本文提出了一种新的填充式遮盖策略MixMask，在Siamese ConvNets中实现遮盖和对比学习目标的匹配，提高了Siamese ConvNets的性能并在多个基准测试中实现了最先进的结果。 |
| [^154] | [Mitigating Covertly Unsafe Text within Natural Language Systems.](http://arxiv.org/abs/2210.09306) | 本文讨论了智能技术中日益普遍的文本安全问题，并强调了一个被忽视的类别：隐蔽不安全文本。该文提出了缓解策略以解决这一问题，以提高智能系统内部的安全性。 |
| [^155] | [Verifiable and Provably Secure Machine Unlearning.](http://arxiv.org/abs/2210.09126) | 该论文提出了可证明安全的机器学习去除算法，可以让用户审计这个过程，以确保训练数据的隐私得到保护。 |
| [^156] | [Compute-Efficient Deep Learning: Algorithmic Trends and Opportunities.](http://arxiv.org/abs/2210.06640) | 本文概述了算法效率高深度学习的研究。首先，提出了算法加速问题并制定了分类法。分类法突显了看似不同方法的共同点，并揭示了当前研究的空白。其次，提出了评估最佳实践以实现全面、公正和可靠的速度提升技术比较。最后，讨论了训练流程中常见的瓶颈。 |
| [^157] | [Computational Choreography using Human Motion Synthesis.](http://arxiv.org/abs/2210.04366) | 本文介绍了一种利用深度学习模型分析舞蹈动作和生成新动作序列的方法，同时也结合了前人的努力来开发出一套系统。 |
| [^158] | [Neighborhood Gradient Clustering: An Efficient Decentralized Learning Method for Non-IID Data Distributions.](http://arxiv.org/abs/2209.14390) | 本文提出一种名为NGC的去中心化学习算法，用于对非独立同分布数据进行学习。该算法使用邻近代理的自梯度，模型变异交叉梯度和数据变异交叉梯度的加权平均值来修改每个代理的局部梯度。 |
| [^159] | [AirTrack: Onboard Deep Learning Framework for Long-Range Aircraft Detection and Tracking.](http://arxiv.org/abs/2209.12849) | AirTrack是一个用于小型无人机系统的实时视觉检测和跟踪框架，使用全分辨率图像和深度学习框架以提高检测和跟踪性能。实验结果显示，在Amazon AOT数据集上表现出优越性。同时，多次真实世界中进行的飞行测试均显示该方法的有效性。 |
| [^160] | [Cross-Domain Evaluation of a Deep Learning-Based Type Inference System.](http://arxiv.org/abs/2208.09189) | 本文研究了基于深度学习的类型推断系统Type4Py的跨领域泛化能力，并通过广泛的实验表明，Type4Py在应用于不同领域的类型推断时能够提供优异的性能。 |
| [^161] | [Valid Inference after Causal Discovery.](http://arxiv.org/abs/2208.05949) | 本研究开发了工具以实现因果发现后的有效推断，解决了使用相同数据运行因果发现算法后估计因果效应导致经典置信区间的覆盖保证无效问题。 |
| [^162] | [PhyGNNet: Solving spatiotemporal PDEs with Physics-informed Graph Neural Network.](http://arxiv.org/abs/2208.04319) | 本文提出了PhyGNNet，一种使用图神经网络解决偏微分方程问题的方法，通过将计算区域划分为规则的网格并构造偏微分损失来优化网络模型。实验证明该方法在时间和空间范围内具有更好的拟合能力和外推能力。 |
| [^163] | [TaDaa: real time Ticket Assignment Deep learning Auto Advisor for customer support, help desk, and issue ticketing systems.](http://arxiv.org/abs/2207.11187) | TaDaa是一个利用最新的Transformer模型和机器学习技术，用于客户支持、帮助台和问题登记系统的实时票务分配深度学习自动顾问，它可以分配问题给正确的组、分配问题给最佳的解决者，并向解决者提供最相关的先前解决的问题，其在实验中表现优异，能够极大地提高平均解决时间。 |
| [^164] | [D3G: Learning Multi-robot Coordination from Demonstrations.](http://arxiv.org/abs/2207.08892) | 本文提出了一个D3G框架，可以从演示中学习多机器人协调。通过最小化轨迹与演示之间的不匹配，每个机器人可以自动调整其个体动态和目标，提高了学习效率和效果。 |
| [^165] | [Ask-AC: An Initiative Advisor-in-the-Loop Actor-Critic Framework.](http://arxiv.org/abs/2207.01955) | 本文提出了一种新颖的主动顾问演员-评论家框架，Ask-AC，它替换了传统的被动监督信号机制，实现了定制化和高效的信息交换，其中的两个互补组件允许代理主动寻求顾问干预和识别漏掉的不稳定状态。 |
| [^166] | [Denoised MDPs: Learning World Models Better Than the World Itself.](http://arxiv.org/abs/2206.15477) | 本文提出一种新的去噪MDP学习方法，该方法可以将现实数据中的噪声干扰因素去除，学习一个更好的世界模型，实验结果表明该方法在任务上表现更加优秀。 |
| [^167] | [Merak: An Efficient Distributed DNN Training Framework with Automated 3D Parallelism for Giant Foundation Models.](http://arxiv.org/abs/2206.04959) | Merak是一个自动化的三维并行深度学习训练框架，它解决了当前其他框架中需要手动修改模型才可并行的问题，并具有较高的计算、GPU内存和网络带宽利用率。 |
| [^168] | [GAMR: A Guided Attention Model for (visual) Reasoning.](http://arxiv.org/abs/2206.04928) | 本文介绍了一个新的模块——GAMR，它是一种用于(视觉)推理的引导式注意力模型，以动态选择任务相关的视觉信息并将其路由到记忆中来解决复杂的视觉推理任务，并在各种任务和数据集上取得了成功的实验结果。 |
| [^169] | [Semi-supervised Semantics-guided Adversarial Training for Trajectory Prediction.](http://arxiv.org/abs/2205.14230) | 提出了一种半监督对抗训练方法，通过半监督对抗自编码器提取出领域知识并建模分离的语义特征，为轨迹预测任务增强了鲁棒性。 |
| [^170] | [Calibration Matters: Tackling Maximization Bias in Large-scale Advertising Recommendation Systems.](http://arxiv.org/abs/2205.09809) | 本文提出了一种有效、稳健且实用的方差调整去偏（VAD）算法，用于解决大规模广告推荐系统中的极大化偏差问题，该算法可以缓解标定经常受到的极大化偏差问题，并提高广告推荐系统的实用性能。 |
| [^171] | [DeepGraviLens: a Multi-Modal Architecture for Classifying Gravitational Lensing Data.](http://arxiv.org/abs/2205.00701) | DeepGraviLens是一种多模态神经网络，用于分类属于不同类型的引力透镜数据，具有高精度和优于现有方法的结果。 |
| [^172] | [Learning and controlling the source-filter representation of speech with a variational autoencoder.](http://arxiv.org/abs/2204.07075) | 本文提出了一种用变分自编码器学习和控制语音的源-滤波表示的方法。基于源-滤波模型假设，提出了一种方法来识别编码$f_0$和前三个共振峰频率的潜在子空间，并证明了这些子空间可以被用于对语音进行控制。 |
| [^173] | [Neural Message Passing for Objective-Based Uncertainty Quantification and Optimal Experimental Design.](http://arxiv.org/abs/2203.07120) | 本论文提出了一种降低计算成本实现客观目标不确定性量化和最优实验设计的新方案。 |
| [^174] | [Bandits Corrupted by Nature: Lower Bounds on Regret and Robust Optimistic Algorithm.](http://arxiv.org/abs/2203.03186) | 本文研究了一种被自然扭曲的赌徒问题，提出了HubUCB算法，利用Huber估计实现了接近最优的效果。 |
| [^175] | [Joint Differentiable Optimization and Verification for Certified Reinforcement Learning.](http://arxiv.org/abs/2201.12243) | 本文提出了一个新的优化问题框架，用于基于模型的强化学习中的正式验证，同时解决了在前期学习和后期验证之间难以获得证书的问题。该框架通过梯度来微分证书和价值函数，并且在实验中表现出大于其他方法的优势，可以找到具有保护函数和李雅普诺夫函数的可行控制器，确保了系统的安全性。 |
| [^176] | [Fighting Money Laundering with Statistics and Machine Learning: An Introduction and Review.](http://arxiv.org/abs/2201.04207) | 本文介绍了银行反洗钱的统计和机器学习方法，并提出客户风险评估和可疑行为标识两个核心要素。未来的研究方向包括生成合成数据、半监督和深度学习、可解释性以及结果的公平性。 |
| [^177] | [Stochastic regularized majorization-minimization with weakly convex and multi-convex surrogates.](http://arxiv.org/abs/2201.01652) | 本文提出了一个SMM的扩展，该扩展支持代理函数的弱凸性和块多凸性，并在解决非凸约束问题上具有显著优越性能。 |
| [^178] | [Sharpness-aware Quantization for Deep Neural Networks.](http://arxiv.org/abs/2111.12273) | 本文提出了一种新的方法，称为尖锐感知量化（SAQ），它利用尖锐感知最小化（SAM）来平滑尖锐的量化噪声和扰动，增强了模型的鲁棒性，同时保持了高压缩性。实验结果表明，SAQ 在 ImageNet 上的精度比现有的最先进的量化方法高出3.6％。 |
| [^179] | [DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing.](http://arxiv.org/abs/2111.09543) | 本论文介绍了一种新的预训练语言模型DeBERTaV3，使用更加样本有效的替换令牌检测（RTD）取代了掩码语言建模（MLM）并提出了一种新的梯度去耦合嵌入共享方法，避免了“拔河”动态，提高了预训练模型的训练效率和质量。在多个下游自然语言理解任务中，DeBERTaV3表现出优秀的性能。 |
| [^180] | [CoReS: Compatible Representations via Stationarity.](http://arxiv.org/abs/2111.07632) | CoReS是一种基于平稳性的方法，可以学习内部特征表示模型，并使其与之前学习的模型“兼容”。这使得在逐步升级表示模型时，无需为所有之前看到的图像提取新的特征，从而大大降低了成本。 |
| [^181] | [Holistic Deep Learning.](http://arxiv.org/abs/2110.15829) | 本文提出了一种全面深度学习框架，通过解决输入扰动的脆弱性、过度参数化和性能不稳定性等挑战，全面提高了准确性、鲁棒性、稀疏性和稳定性，适用于表格和图像数据集。提供了选择适当的训练损失函数的建议。 |
| [^182] | [The Threat of Adversarial Attacks on Machine Learning in Network Security -- A Survey.](http://arxiv.org/abs/1911.02621) | 机器学习在网络安全领域应用面临对抗攻击威胁，攻击者通过专门设计的输入来绕过系统引导错误预测，文章提供机器学习和网络安全应用的分类法，并介绍了各种对抗攻击和防御方法。 |
| [^183] | [Contextual Linear Bandits under Noisy Features: Towards Bayesian Oracles.](http://arxiv.org/abs/1703.01347) | 本论文研究了具有噪声特征的上下文线性Bandit问题。我们提出了一个算法，通过观察信息，实现了贝叶斯神谕并得到了$\tilde{O}(d\sqrt{T})$的遗憾界。 |

# 详细

[^1]: 触觉训练下的机器人灵巧性

    Dexterity from Touch: Self-Supervised Pre-Training of Tactile Representations with Robotic Play. (arXiv:2303.12076v1 [cs.RO])

    [http://arxiv.org/abs/2303.12076](http://arxiv.org/abs/2303.12076)

    T-Dex是一种基于触觉的灵巧性方法，可在自我监督式的触觉编码器和一些灵巧性任务演示的指导下，将触觉和视觉结合起来，相比于传统的纯视觉方法更有效，并在现实世界中具有更好的应用性。

    

    在机器人领域，让具有多指的机器人具有灵巧性一直是一个长期的挑战。此前，最重要的工作都集中在学习控制器或策略上，这些控制器或策略要么基于视觉观测，要么基于从视觉推断得到的状态估计。然而，这些方法在需要推理接触力或通过手本身遮挡的物体的细粒度操作任务上表现不佳。本研究提出了一种新的基于触觉的灵巧性方法T-Dex，该方法分为两个阶段：在第一阶段，收集2.5小时的游戏数据并使用这些数据训练自我监督型触觉编码器；在第二阶段，利用少量的灵巧性任务演示，学习将触觉观测和视觉观测相结合的非参数化策略。通过五个具有挑战性的灵巧性任务，我们展示了我们的基于触觉的灵巧性模型比纯视觉方法更有效，并且能够推广到现实世界中视觉方法失败的情况。

    Teaching dexterity to multi-fingered robots has been a longstanding challenge in robotics. Most prominent work in this area focuses on learning controllers or policies that either operate on visual observations or state estimates derived from vision. However, such methods perform poorly on fine-grained manipulation tasks that require reasoning about contact forces or about objects occluded by the hand itself. In this work, we present T-Dex, a new approach for tactile-based dexterity, that operates in two phases. In the first phase, we collect 2.5 hours of play data, which is used to train self-supervised tactile encoders. This is necessary to bring high-dimensional tactile readings to a lower-dimensional embedding. In the second phase, given a handful of demonstrations for a dexterous task, we learn non-parametric policies that combine the tactile observations with visual ones. Across five challenging dexterous tasks, we show that our tactile-based dexterity models outperform purely vi
    
[^2]: 评估聊天机器人的对话回复

    Grading Conversational Responses Of Chatbots. (arXiv:2303.12038v1 [cs.CL])

    [http://arxiv.org/abs/2303.12038](http://arxiv.org/abs/2303.12038)

    本文研究了ChatGPT聊天机器人对问题的回答表现，并使用标准的机器翻译评测指标进行比较，结果表明它们的表现仍然落后于典型人类的反应能力。

    

    聊天机器人能够回答基本问题，甚至回应奇怪的提示，但最近它们的改进显著提高。像OpenAI ChatGPT3这样的现代聊天机器人不仅能够回答基本问题，还能编写代码、电影剧本并模仿知名人物。在本文中，我们分析了ChatGPT对从流行Quora论坛的数据集中提出的各种问题的回答。我们向ChatGPT提交了60个问题，并根据三个行业标准评分机器翻译的度量标准(BLEU、METEOR和ROUGE)对答案进行评分。这些度量标准允许我们将机器的回复与同一问题的最多赞同的人类答案进行比较，以评估ChatGPT提交人性化回复的能力。结果显示，虽然ChatGPT的响应和翻译能力非常出色，但仍然达不到典型人类反应的水平。

    Chatbots have long been capable of answering basic questions and even responding to obscure prompts, but recently their improvements have been far more significant. Modern chatbots like Open AIs ChatGPT3 not only have the ability to answer basic questions but can write code and movie scripts and imitate well-known people. In this paper, we analyze ChatGPTs' responses to various questions from a dataset of queries from the popular Quora forum. We submitted sixty questions to ChatGPT and scored the answers based on three industry-standard metrics for grading machine translation: BLEU, METEOR, and ROUGE. These metrics allow us to compare the machine responses with the most upvoted human answer to the same question to assess ChatGPT's ability to submit a humanistic reply. The results showed that while the responses and translation abilities of ChatGPT are remarkable, they still fall short of what a typical human reaction would be.
    
[^3]: 深度学习模型的表征状态

    The Representational Status of Deep Learning Models. (arXiv:2303.12032v1 [cs.AI])

    [http://arxiv.org/abs/2303.12032](http://arxiv.org/abs/2303.12032)

    该论文澄清了深度学习模型的表征状态。尽管通常称为“表征”，但实际上它们更适合理解为高度理想化的模型，这一结果对可解释的AI有着直接影响，也引起了哲学家对其在未来科学研究中的作用的关注。

    

    本文旨在澄清深度学习模型（DLMs）的表征状态。由于功能和关系概念的混淆，尽管通常称为“表征”，但这意味着含糊不清。本文认为，虽然DLM以关系意义上的表征其目标，但最好理解为高度理想化的模型。这个结果对可解释的AI（XAI）有直接影响，并引导哲学关注DLM表征的理想化性质及其在未来科学研究中的作用。

    This paper aims to clarify the representational status of Deep Learning Models (DLMs). While commonly referred to as 'representations', what this entails is ambiguous due to a conflation of functional and relational conceptions of representation. This paper argues that while DLMs represent their targets in a relational sense, they are best understood as highly idealized models. This result has immediate implications for explainable AI (XAI) and directs philosophical attention toward examining the idealized nature of DLM representations and their role in future scientific investigation.
    
[^4]: 用扩散自编码器的语义潜空间回归进行椎体骨折分级

    Semantic Latent Space Regression of Diffusion Autoencoders for Vertebral Fracture Grading. (arXiv:2303.12031v1 [cs.CV])

    [http://arxiv.org/abs/2303.12031](http://arxiv.org/abs/2303.12031)

    提出了一种利用扩散自编码器的语义潜空间回归进行椎体骨折分级的新方法，并通过连续回归来反映骨折的平稳进展。通过无监督特征提取器和数据增强，该方法取得了显著的改进。

    

    椎体骨折是骨质疏松症的后果，对患者的健康有重要影响。然而，使用 CT 检查对其进行分级是困难和主观的，这促使着自动化分级方法的开发。然而，目前的方法受到数据不平衡和稀缺性以及缺乏可解释性的制约。为了解决这些挑战，本文提出了一种新方法，利用未标记数据训练生成式扩散自编码器 (DAE) 模型作为无监督特征提取器。我们将骨折分级建模为连续回归，这更能反映骨折的平稳进展。具体地，我们使用二元有监督的骨折分类器构建 DAE 的潜空间中的超平面。然后，我们将骨折的严重程度回归为距离超平面的函数，对结果进行 Genant 标度的校准。重要的是，我们方法的生成本质允许我们视觉化给定骨折的不同级别，并生成合成数据，我们使用该数据来增强训练集。我们在公开可用的腰椎 CT 扫描数据集上评估了我们的方法，证明相对于最先进的基线方法，我们有了显著的改进。

    Vertebral fractures are a consequence of osteoporosis, with significant health implications for affected patients. Unfortunately, grading their severity using CT exams is hard and subjective, motivating automated grading methods. However, current approaches are hindered by imbalance and scarcity of data and a lack of interpretability. To address these challenges, this paper proposes a novel approach that leverages unlabelled data to train a generative Diffusion Autoencoder (DAE) model as an unsupervised feature extractor. We model fracture grading as a continuous regression, which is more reflective of the smooth progression of fractures. Specifically, we use a binary, supervised fracture classifier to construct a hyperplane in the DAE's latent space. We then regress the severity of the fracture as a function of the distance to this hyperplane, calibrating the results to the Genant scale. Importantly, the generative nature of our method allows us to visualize different grades of a give
    
[^5]: 图卡尔曼滤波器

    Graph Kalman Filters. (arXiv:2303.12021v1 [cs.LG])

    [http://arxiv.org/abs/2303.12021](http://arxiv.org/abs/2303.12021)

    本文首次将卡尔曼和扩展卡尔曼滤波器推广到图形上，使得它可以适用于输出是向量或标量的情况，并且可以学习未知的状态转移和读取函数。

    

    众所周知，卡尔曼滤波器通过使用状态空间表示来模拟动态系统，下一个状态的更新以及与新观察到的系统输出相关的信息来控制其不确定性。本文首次将卡尔曼和扩展卡尔曼滤波器推广到离散时间的设置下，其中输入、状态和输出均表示为带属性的图形，其拓扑和属性可以随时间变化。此设置使得我们可以将框架适应于输出是向量或标量的情况（节点/图级任务）。在所提出的理论框架内，未知的状态转移和读取函数与下游预测任务一起端到端学习。

    The well-known Kalman filters model dynamical systems by relying on state-space representations with the next state updated, and its uncertainty controlled, by fresh information associated with newly observed system outputs. This paper generalizes, for the first time in the literature, Kalman and extended Kalman filters to discrete-time settings where inputs, states, and outputs are represented as attributed graphs whose topology and attributes can change with time. The setup allows us to adapt the framework to cases where the output is a vector or a scalar too (node/graph level tasks). Within the proposed theoretical framework, the unknown state-transition and the readout functions are learned end-to-end along with the downstream prediction task.
    
[^6]: 电话会话的低延迟发言分离和语音活动检测的端到端集成

    End-to-End Integration of Speech Separation and Voice Activity Detection for Low-Latency Diarization of Telephone Conversations. (arXiv:2303.12002v1 [eess.AS])

    [http://arxiv.org/abs/2303.12002](http://arxiv.org/abs/2303.12002)

    本文重点研究了低延迟流式分离应用中的基于语音分离的发言者分离（SSGD）在会话电话语音（CTS）领域中的应用。通过分离说话人并在每个分离的流上应用语音活动检测（VAD）来执行发言者分离，提出了一种新型、因果和计算效率高的泄漏去除算法。在CALLHOME和Fisher语料库（第1和2部分）上的性能评估表明，SSGD算法能够有效地提高分离和发言者分离的性能。

    

    最近的研究表明，基于语音分离的发言者分离（SSGD）是一个越来越有前途的方向，这主要得益于语音分离的最新进展。它通过首先分离说话人，然后在每个分离的流上应用语音活动检测（VAD）来执行发言者分离。在本研究中，我们对会话电话语音（CTS）领域中的SSGD进行了深入研究，重点是低延迟流式分离应用。我们考虑了三种最先进的语音分离（SSep）算法，并研究了它们在在线和离线场景下的性能，考虑了非因果和因果实现以及连续SSep（CSS）窗口推理。我们比较了不同的SSGD算法在两个广泛使用的CTS数据集CALLHOME和Fisher语料库（第1和2部分）上的性能，并评估了分离和发言者分离的性能。为了提高性能，我们提出了一种新型、因果和计算效率高的泄漏去除算法。

    Recent works show that speech separation guided diarization (SSGD) is an increasingly promising direction, mainly thanks to the recent progress in speech separation. It performs diarization by first separating the speakers and then applying voice activity detection (VAD) on each separated stream. In this work we conduct an in-depth study of SSGD in the conversational telephone speech (CTS) domain, focusing mainly on low-latency streaming diarization applications. We consider three state-of-the-art speech separation (SSep) algorithms and study their performance both in online and offline scenarios, considering non-causal and causal implementations as well as continuous SSep (CSS) windowed inference. We compare different SSGD algorithms on two widely used CTS datasets: CALLHOME and Fisher Corpus (Part 1 and 2) and evaluate both separation and diarization performance. To improve performance, a novel, causal and computationally efficient leakage removal algorithm is proposed, which signifi
    
[^7]: 使用图神经网络进行共享单车系统扩展的深度出行生成

    Deep trip generation with graph neural networks for bike sharing system expansion. (arXiv:2303.11977v1 [cs.LG])

    [http://arxiv.org/abs/2303.11977](http://arxiv.org/abs/2303.11977)

    本文提出了一种使用图神经网络进行共享单车系统扩展的深度出行生成方法，用于预测基于多源城市建筑和地理数据的站点需求。

    

    共享单车正在全球范围内作为一种活跃，方便和可持续的交通方式而兴起。为了计划成功的共享单车系统（BSS），许多城市从小规模试点开始，并逐步扩大系统覆盖更多区域。对于基于站点的BSS，这意味着随着时间的推移基于现有站点规划新站点，这需要预测整个系统中这些新站点产生的旅行次数。先前的研究通常依赖于相对简单的回归或机器学习模型，在捕捉复杂空间关系方面受到限制。尽管在旅行需求预测方面，深度学习方法的文献越来越多，但它们大多是基于时序数据的短期预测，假设系统没有结构性变化。在本研究中，我们重点研究了BSS扩展的出行生成问题，并提出了一种基于图神经网络（GNN）的方法来预测基于多源城市建筑和地理数据的站点需求。

    Bike sharing is emerging globally as an active, convenient, and sustainable mode of transportation. To plan successful bike-sharing systems (BSSs), many cities start from a small-scale pilot and gradually expand the system to cover more areas. For station-based BSSs, this means planning new stations based on existing ones over time, which requires prediction of the number of trips generated by these new stations across the whole system. Previous studies typically rely on relatively simple regression or machine learning models, which are limited in capturing complex spatial relationships. Despite the growing literature in deep learning methods for travel demand prediction, they are mostly developed for short-term prediction based on time series data, assuming no structural changes to the system. In this study, we focus on the trip generation problem for BSS expansion, and propose a graph neural network (GNN) approach to predicting the station-level demand based on multi-source urban bui
    
[^8]: “推动力”：智能辅导系统中元认知技能教学的三种干预方法的探究

    The Power of Nudging: Exploring Three Interventions for Metacognitive Skills Instruction across Intelligent Tutoring Systems. (arXiv:2303.11965v1 [cs.HC])

    [http://arxiv.org/abs/2303.11965](http://arxiv.org/abs/2303.11965)

    本研究探究了涉及推动、示例和提出三种干预方法教授默认学生何时如何使用哪种策略，研究结果表明Nudge表现最佳。

    

    认证域通常具有很多认知技能，其中没有一种单一的解决策略可适用于解决所有问题。研究表明，知道何时如何使用每种策略的学生（StrTime）表现优于那些谁不知道并坚持默认策略（Default）的学生。在这项工作中，学生在逻辑辅导员的训练中使用默认的前向链接和后向链接（BC）策略，然后在概率辅导员中只使用BC支持的策略。我们研究了三种在逻辑辅导员上教授默认学生何时如何使用哪种策略的干预措施：示例、推动和提出。同时，StrTime学生没有接受任何干预措施。总的来说，我们的结果表明，Nudge的表现优于其Default同伴，并在两个辅导员上追赶StrTime。

    Deductive domains are typical of many cognitive skills in that no single problem-solving strategy is always optimal for solving all problems. It was shown that students who know how and when to use each strategy (StrTime) outperformed those who know neither and stick to the default strategy (Default). In this work, students were trained on a logic tutor that supports a default forward-chaining and a backward-chaining (BC) strategy, then a probability tutor that only supports BC. We investigated three types of interventions on teaching the Default students how and when to use which strategy on the logic tutor: Example, Nudge and Presented. Meanwhile, StrTime students received no interventions. Overall, our results show that Nudge outperformed their Default peers and caught up with StrTime on both tutors.
    
[^9]: 多智能体强化学习在量化市场中的战略交易研究

    Strategic Trading in Quantitative Markets through Multi-Agent Reinforcement Learning. (arXiv:2303.11959v1 [q-fin.TR])

    [http://arxiv.org/abs/2303.11959](http://arxiv.org/abs/2303.11959)

    本文将CPPI和TIPP策略集成到多智能体深度确定性策略梯度中，提出了两种特定设计的MARL方法CPPI-MADDPG和TIPP-MADDPG，用于量化市场的战略交易，结果显示这些方法通常优于传统方法。

    

    在量化市场中，由于市场动态快速变化和大量的不确定性，如何采取适当的行动利润仍然是一个具有挑战性的问题。强化学习作为一种面向奖励的最优控制方法，在这种复杂的金融场景中已成为解决策略决策问题的有希望的方法。本文将两种先前的金融交易策略（恒定比例组合保险（CPPI）和时间不变组合保护（TIPP））集成到多智能体深度确定性策略梯度（MADDPG）中，并提出了两种特别设计的多智能体强化学习（MARL）方法：CPPI-MADDPG和TIPP-MADDPG研究量化市场中的战略交易。之后，我们选择了实际金融市场上的100种不同股票来测试这些特别提出的方法。实验结果表明，CPPI-MADDPG和TIPP-MADDPG方法通常优于传统方法。

    Due to the rapid dynamics and a mass of uncertainties in the quantitative markets, the issue of how to take appropriate actions to make profits in stock trading remains a challenging one. Reinforcement learning (RL), as a reward-oriented approach for optimal control, has emerged as a promising method to tackle this strategic decision-making problem in such a complex financial scenario. In this paper, we integrated two prior financial trading strategies named constant proportion portfolio insurance (CPPI) and time-invariant portfolio protection (TIPP) into multi-agent deep deterministic policy gradient (MADDPG) and proposed two specifically designed multi-agent RL (MARL) methods: CPPI-MADDPG and TIPP-MADDPG for investigating strategic trading in quantitative markets. Afterward, we selected 100 different shares in the real financial market to test these specifically proposed approaches. The experiment results show that CPPI-MADDPG and TIPP-MADDPG approaches generally outperform the conve
    
[^10]: 加权平滑在凸多面体上的投影式

    Formulation of Weighted Average Smoothing as a Projection of the Origin onto a Convex Polytope. (arXiv:2303.11958v1 [cs.LG])

    [http://arxiv.org/abs/2303.11958](http://arxiv.org/abs/2303.11958)

    本文研究了加权移动平均平滑器的最佳加权窗口，将问题转化为原点在凸多面体上的投影，并提供了一些解决方案。

    

    本文研究了在平方损失下加权移动平均平滑器的最佳加权窗口。我们证明存在一个最优的对称加权窗口。我们研究了逐步减弱的加权窗口，它们的权重随着远离中心而减少。我们将相应的最小二乘问题公式化为一个二次规划问题，最终将其转化为原点在凸多面体上的投影。此外，当输入数据满足某些条件时，我们还提供了一些最佳窗口的分析解决方案。

    Our study focuses on determining the best weight windows for a weighted moving average smoother under squared loss. We show that there exists an optimal weight window that is symmetrical around its center. We study the class of tapered weight windows, which decrease in weight as they move away from the center. We formulate the corresponding least squares problem as a quadratic program and finally as a projection of the origin onto a convex polytope. Additionally, we provide some analytical solutions to the best window when some conditions are met on the input data.
    
[^11]: 基于贝叶斯优化的函数组合方法及其在动态定价中的应用

    Bayesian Optimization for Function Compositions with Applications to Dynamic Pricing. (arXiv:2303.11954v1 [cs.LG])

    [http://arxiv.org/abs/2303.11954](http://arxiv.org/abs/2303.11954)

    本文提出了一种基于贝叶斯优化的函数组合方法，可以解决黑盒函数难以评估的问题，并在动态定价中得到应用。

    

    贝叶斯优化（BO）被用来找到黑盒函数的全局最优解。本文提出了一种针对函数组合的实用BO方法，其中组合的形式已知，但各组成函数难以评估。通过为每个黑盒函数建立独立的高斯过程（GP）模型，我们提出EI和UCB基于BO算法，并证明它们能够胜过传统BO和目前先进算法。我们展示了所提出的方法在收益管理中动态定价时的一种新颖应用，尤其适用于昂贵的需求函数。

    Bayesian Optimization (BO) is used to find the global optima of black box functions. In this work, we propose a practical BO method of function compositions where the form of the composition is known but the constituent functions are expensive to evaluate. By assuming an independent Gaussian process (GP) model for each of the constituent black-box function, we propose EI and UCB based BO algorithms and demonstrate their ability to outperform vanilla BO and the current state-of-art algorithms. We demonstrate a novel application of the proposed methods to dynamic pricing in revenue management when the underlying demand function is expensive to evaluate.
    
[^12]: 基于分层存储池的边缘半监督增量学习方法

    Hierarchical Memory Pool Based Edge Semi-Supervised Continual Learning Method. (arXiv:2303.11952v1 [cs.LG])

    [http://arxiv.org/abs/2303.11952](http://arxiv.org/abs/2303.11952)

    本文提出了名为EdgeHML的低成本半监督增量学习方法，通过分层存储池和多级存储结构，利用大量未标记样本和少量标记样本，有效应对灾难性遗忘和边缘智能领域的资源限制问题。

    

    当前世界的不断变化导致神经网络的表现下降。因此，增量学习领域逐渐引起更多研究人员的关注。对于边缘智能而言，增量学习模型不仅需要克服灾难性遗忘，还需要应对严重的资源限制：缺乏标记资源和强大的设备。然而，现有的经典增量学习方法通常依赖于大量标记样本来维护可塑性和稳定性，而半监督学习方法通常需要付出大量的计算和内存开销来提高精度。为应对这些问题，本文提出了一种名为边缘分层存储学习器（EdgeHML）的低成本半监督增量学习方法，它可以有效地利用大量未标记样本和少量标记样本，基于分层存储池，利用多级存储结构来存储数据。

    The continuous changes in the world have resulted in the performance regression of neural networks. Therefore, continual learning (CL) area gradually attracts the attention of more researchers. For edge intelligence, the CL model not only needs to overcome catastrophic for-getting, but also needs to face the huge challenge of severely limited resources: the lack of labeled resources and powerful devices. However, the existing classic CL methods usually rely on a large number of labeled samples to maintain the plasticity and stability, and the semi-supervised learning methods often need to pay a large computational and memory overhead for higher accuracy. In response to these prob-lems, a low-cost semi-supervised CL method named Edge Hierarchical Memory Learner (EdgeHML) will be proposed. EdgeHML can effec-tively utilize a large number of unlabeled samples and a small number of labeled samples. It is based on a hierarchical memory pool, lever-age multi-level storage structure to store a
    
[^13]: 一种模糊自适应进化特征选择与机器学习框架用于单目标和多目标身体脂肪预测

    A fuzzy adaptive evolutionary-based feature selection and machine learning framework for single and multi-objective body fat prediction. (arXiv:2303.11949v1 [cs.NE])

    [http://arxiv.org/abs/2303.11949](http://arxiv.org/abs/2303.11949)

    本文提出了一种模糊自适应进化特征选择与机器学习框架用于单目标和多目标身体脂肪预测，该方法在管理参数化和计算成本的同时确定了适当的探索和开发水平，可以避免陷入局部最优问题。

    

    预测身体脂肪可以为医学从业者和用户提供预防和诊断心脏疾病的重要信息。混合机器学习模型通过选择相关的身体测量值和捕捉模型中所选特征之间的复杂非线性关系，提供了比简单的回归分析方法更好的性能。然而，它们仍然存在一些缺点。当前的机器学习建模方法将身体脂肪预测问题建模为组合的单目标和多目标优化问题，往往会陷入局部最优。当多个特征子集产生类似或接近的预测时，避免局部最优变得更加复杂。进化特征选择已被用于解决几个基于机器学习的优化问题。一个模糊集理论确定适当的探索和开发水平，同时管理参数化和计算成本。采用加权和身体脂肪预测方法进行实验评估

    Predicting body fat can provide medical practitioners and users with essential information for preventing and diagnosing heart diseases. Hybrid machine learning models offer better performance than simple regression analysis methods by selecting relevant body measurements and capturing complex nonlinear relationships among selected features in modelling body fat prediction problems. There are, however, some disadvantages to them. Current machine learning. Modelling body fat prediction as a combinatorial single- and multi-objective optimisation problem often gets stuck in local optima. When multiple feature subsets produce similar or close predictions, avoiding local optima becomes more complex. Evolutionary feature selection has been used to solve several machine-learning-based optimisation problems. A fuzzy set theory determines appropriate levels of exploration and exploitation while managing parameterisation and computational costs. A weighted-sum body fat prediction approach was ex
    
[^14]: 无监督跨领域谣言检测：基于对比学习和交叉注意力的方法

    Unsupervised Cross-Domain Rumor Detection with Contrastive Learning and Cross-Attention. (arXiv:2303.11945v1 [cs.SI])

    [http://arxiv.org/abs/2303.11945](http://arxiv.org/abs/2303.11945)

    本文提出一种无监督的、基于对比学习和交叉注意力机制的跨领域谣言检测模型。该模型不仅可以进行跨领域特征对齐，还可以强制目标样本与源域中的相应原型对齐，并使用聚类方法产生伪标签来学习域不变表示，能够显著提高跨领域谣言检测的性能。

    

    大规模谣言通常伴随着突发新闻或热门话题而出现，严重阻碍真相的查证。现有的谣言检测方法大多专注于相同领域，因此在跨领域情况下表现不佳。本文提出了一种端到端的基于实例和原型的，带有交叉注意力机制的对比学习模型，用于跨领域谣言检测。该模型不仅可以进行跨领域特征对齐，还可以强制目标样本与给定源域的相应原型对齐。由于目标域中的目标标签不可用，因此我们使用一种聚类的方法，并通过一批源域样本的仔细初始化中心来产生伪标签。此外，我们使用交叉注意力机制处理具有相同标签的一对源数据和目标数据，以学习域不变表示。由于领域对中的样本倾向于表达相似的语义模式，因此这种方法能够提高模型的检测性能。

    Massive rumors usually appear along with breaking news or trending topics, seriously hindering the truth. Existing rumor detection methods are mostly focused on the same domain, and thus have poor performance in cross-domain scenarios due to domain shift. In this work, we propose an end-to-end instance-wise and prototype-wise contrastive learning model with a cross-attention mechanism for cross-domain rumor detection. The model not only performs cross-domain feature alignment but also enforces target samples to align with the corresponding prototypes of a given source domain. Since target labels in a target domain are unavailable, we use a clustering-based approach with carefully initialized centers by a batch of source domain samples to produce pseudo labels. Moreover, we use a cross-attention mechanism on a pair of source data and target data with the same labels to learn domain-invariant representations. Because the samples in a domain pair tend to express similar semantic patterns,
    
[^15]: 随机连续次模最大化问题高概率边界研究

    High Probability Bounds for Stochastic Continuous Submodular Maximization. (arXiv:2303.11937v1 [cs.DS])

    [http://arxiv.org/abs/2303.11937](http://arxiv.org/abs/2303.11937)

    本论文提出了针对随机CSF最大化问题的高概率边界算法，取得了比现有方法更好的收敛速度，经过验证在最坏情况下也能收敛到较优解。

    

    本论文研究了具有递减收益特性的随机单调连续次模函数（CSF）最大化问题。现有算法只能提供期望表现保证，并不能限制得到不好解的概率。这意味着对于算法的某个运行，得到的解可能要比期望保证的更糟糕。在本文中，我们首先通过实验证实了这一点。然后，我们提供了现有随机CSF最大化方法的第一分析，即PGA，boosted PGA，SCG和SCG ++。最后，我们在稍微强一点的假设下提供了SCG的改进高概率界，其收敛速度比预期解更快。通过对非凸二次规划（NQP）和最优预算分配的广泛实验，我们确认了我们的界限的有效性，并表明即使在最坏情况下，PGA也会收敛到$OPT / 2$，一个。

    We consider maximization of stochastic monotone continuous submodular functions (CSF) with a diminishing return property. Existing algorithms only guarantee the performance \textit{in expectation}, and do not bound the probability of getting a bad solution. This implies that for a particular run of the algorithms, the solution may be much worse than the provided guarantee in expectation. In this paper, we first empirically verify that this is indeed the case. Then, we provide the first \textit{high-probability} analysis of the existing methods for stochastic CSF maximization, namely PGA, boosted PGA, SCG, and SCG++. Finally, we provide an improved high-probability bound for SCG, under slightly stronger assumptions, with a better convergence rate than that of the expected solution. Through extensive experiments on non-concave quadratic programming (NQP) and optimal budget allocation, we confirm the validity of our bounds and show that even in the worst-case, PGA converges to $OPT/2$, an
    
[^16]: 利用聚类技术发现与COVID-19疫情有关的美国县的模式

    Clustering US Counties to Find Patterns Related to the COVID-19 Pandemic. (arXiv:2303.11936v1 [cs.LG])

    [http://arxiv.org/abs/2303.11936](http://arxiv.org/abs/2303.11936)

    该论文旨在利用聚类算法找到美国疫情相关的县份的模式，并从中了解疫情。

    

    当COVID-19开始传播和隔离措施实施时，明尼苏达州双子城的工业和应用数学协会（SIAM）学生分会开始与Ecolab合作，利用我们作为数据科学家和数学家的技能从与大流行相关的数据中提取有用的见解。这种合作由多个小组在不同的项目上工作组成。本文重点介绍了我们使用聚类技术帮助我们找到美国相似县组的方法，并利用这一方法来帮助我们了解大流行病。本项目的团队包括明尼苏达大学的学生Cora Brown，Sarah Milstein，Tianyi Sun和Cooper Zhao，Ecolab数据科学家Jimmy Broomfield以及明尼苏达大学的学生Skye Ke提供帮助。在下面的章节中，我们描述了本项目的所有工作。在第2节中，我们列出了我们收集的数据以及我们执行的特征工程。在第3节中，我们描述了我们如何执行聚类算法，并且如何使用它来识别模式。

    When COVID-19 first started spreading and quarantine was implemented, the Society for Industrial and Applied Mathematics (SIAM) Student Chapter at the University of Minnesota-Twin Cities began a collaboration with Ecolab to use our skills as data scientists and mathematicians to extract useful insights from relevant data relating to the pandemic. This collaboration consisted of multiple groups working on different projects. In this write-up we focus on using clustering techniques to help us find groups of similar counties in the US and use that to help us understand the pandemic. Our team for this project consisted of University of Minnesota students Cora Brown, Sarah Milstein, Tianyi Sun, and Cooper Zhao, with help from Ecolab Data Scientist Jimmy Broomfield and University of Minnesota student Skye Ke. In the sections below we describe all of the work done for this project. In Section 2, we list the data we gathered, as well as the feature engineering we performed. In Section 3, we de
    
[^17]: 基于视觉转换器的模型用于使用胸部 X 射线图像量化肺炎的严重程度

    Vision Transformer-based Model for Severity Quantification of Lung Pneumonia Using Chest X-ray Images. (arXiv:2303.11935v1 [eess.IV])

    [http://arxiv.org/abs/2303.11935](http://arxiv.org/abs/2303.11935)

    本论文提出了一种基于视觉转换器的模型，可以使用少量可训练参数量化 COVID-19 和其他肺部疾病的严重程度，具有较好的基于数据集的泛化性能。

    

    为了针对胸部 X 射线（CXR）开发诊断和评估 COVID-19 的严重程度的通用和可靠方法，需要大量维护良好的 COVID-19 数据集。现有的严重程度量化结构需要昂贵的训练计算才能取得最好的结果。需要计算机工具，以便医疗保健专业人员快速自动识别 COVID-19 患者并预测相关的严重程度指标。在本研究中，我们提出了一种基于视觉转换器（ViT）的神经网络模型，该模型依靠少量可训练参数来量化 COVID-19 和其他肺部疾病的严重程度。我们提出了一种可行的方法来量化 CXR 的严重程度，称为 Vision Transformer Regressor Infection Prediction（ViTReg-IP），它由 ViT 和一个回归头导出。我们使用来自不同开放源的各种其他测试胸部放射图数据集来研究我们模型的泛化潜力。

    To develop generic and reliable approaches for diagnosing and assessing the severity of COVID-19 from chest X-rays (CXR), a large number of well-maintained COVID-19 datasets are needed. Existing severity quantification architectures require expensive training calculations to achieve the best results. For healthcare professionals to quickly and automatically identify COVID-19 patients and predict associated severity indicators, computer utilities are needed. In this work, we propose a Vision Transformer (ViT)-based neural network model that relies on a small number of trainable parameters to quantify the severity of COVID-19 and other lung diseases. We present a feasible approach to quantify the severity of CXR, called Vision Transformer Regressor Infection Prediction (ViTReg-IP), derived from a ViT and a regression head. We investigate the generalization potential of our model using a variety of additional test chest radiograph datasets from different open sources. In this context, we 
    
[^18]: 稀疏分布式内存是一个持续学习者

    Sparse Distributed Memory is a Continual Learner. (arXiv:2303.11934v1 [cs.NE])

    [http://arxiv.org/abs/2303.11934](http://arxiv.org/abs/2303.11934)

    该论文提出了一个使用稀疏分布式内存的修改多层感知器（MLP）模型，它是一个强大的持续学习者，并且该方法不需要记忆重放或任务信息。这是一种训练稀疏网络的新方法，具有广泛的适用性。

    

    持续学习是人工神经网络面临的问题，而它们的生物学对应物擅长解决。在利用稀疏分布式内存（SDM）将核心神经电路与强大的Transformer模型相连接的研究基础上，我们创建了一个改进的多层感知器（MLP），它是一个强大的持续学习者。我们发现，从生物学上翻译过来的我们的MLP变体的每个组成部分都是持续学习所必需的。我们的解决方案也不需要任何记忆重放或任务信息，并引入了训练稀疏网络的新方法，这可能具有广泛的适用性。

    Continual learning is a problem for artificial neural networks that their biological counterparts are adept at solving. Building on work using Sparse Distributed Memory (SDM) to connect a core neural circuit with the powerful Transformer model, we create a modified Multi-Layered Perceptron (MLP) that is a strong continual learner. We find that every component of our MLP variant translated from biology is necessary for continual learning. Our solution is also free from any memory replay or task information, and introduces novel methods to train sparse networks that may be broadly applicable.
    
[^19]: 中间特征联盟能帮助解释黑盒模型吗？

    Do intermediate feature coalitions aid explainability of black-box models?. (arXiv:2303.11920v1 [cs.LG])

    [http://arxiv.org/abs/2303.11920](http://arxiv.org/abs/2303.11920)

    本文引入了中间概念的级别结构，利用领域专家建立部分-整体关系，从而在不同抽象级别上帮助解释黑盒模型。

    

    本文引入了基于级别结构的中间概念，以帮助黑盒模型的可解释性。级别结构是一种分层结构，每个级别对应数据集的特征（即玩家集分区）。从只包含单元素的平凡集合到只包含大联盟的集合，粗糙度的级别逐渐增加。此外，可以通过领域专家建立部分-整体关系来生成抽象级别的解释。我们在一个实际的汽车模型示例和泰坦尼克号的数据集中说明了这种方法的可用性，其中中间概念在不同抽象级别上帮助解释。

    This work introduces the notion of intermediate concepts based on levels structure to aid explainability for black-box models. The levels structure is a hierarchical structure in which each level corresponds to features of a dataset (i.e., a player-set partition). The level of coarseness increases from the trivial set, which only comprises singletons, to the set, which only contains the grand coalition. In addition, it is possible to establish meronomies, i.e., part-whole relationships, via a domain expert that can be utilised to generate explanations at an abstract level. We illustrate the usability of this approach in a real-world car model example and the Titanic dataset, where intermediate concepts aid in explainability at different levels of abstraction.
    
[^20]: Deephys：分布漂移下神经网络的调试与可视化工具

    Deephys: Deep Electrophysiology, Debugging Neural Networks under Distribution Shifts. (arXiv:2303.11912v1 [cs.LG])

    [http://arxiv.org/abs/2303.11912](http://arxiv.org/abs/2303.11912)

    本文介绍了一种名为Deephys的可视化和理解DNN在超出分布范围的场景中失败的工具，使用神经电生理学的概念，通过比较内分布和外分布数据集中的神经活动，无缝分析单个神经元、单个图像和类别图像集，并能揭示假特征和新特征存在导致的失败。

    

    深度神经网络（DNN）在超出分布范围的场景下经常会出现失败。本文提出了一个工具来可视化和理解这种失败。我们从神经电生理学的概念中汲取灵感，通过分析单个神经元的特征调谐和不变性，来检查神经网络的内部功能。Deep Electrophysiology，简称Deephys，通过比较可视化内分布和外分布数据集中的神经活动，提供了有关DNN在超出分布范围的场景中失败的见解。Deephys提供了对单个神经元，单个图像以及类别图像集的无缝分析，并且能够揭示由于假特征和新特征的存在而导致的失败。我们通过在几个数据集和分布漂移中使用卷积神经网络和变换器架构进行数量分析，证实了Deephys的定性可视化的有效性。

    Deep Neural Networks (DNNs) often fail in out-of-distribution scenarios. In this paper, we introduce a tool to visualize and understand such failures. We draw inspiration from concepts from neural electrophysiology, which are based on inspecting the internal functioning of a neural networks by analyzing the feature tuning and invariances of individual units. Deep Electrophysiology, in short Deephys, provides insights of the DNN's failures in out-of-distribution scenarios by comparative visualization of the neural activity in in-distribution and out-of-distribution datasets. Deephys provides seamless analyses of individual neurons, individual images, and a set of set of images from a category, and it is capable of revealing failures due to the presence of spurious features and novel features. We substantiate the validity of the qualitative visualizations of Deephys thorough quantitative analyses using convolutional and transformers architectures, in several datasets and distribution shi
    
[^21]: 带信息感知数据增强的时间序列对比学习

    Time Series Contrastive Learning with Information-Aware Augmentations. (arXiv:2303.11911v1 [cs.LG])

    [http://arxiv.org/abs/2303.11911](http://arxiv.org/abs/2303.11911)

    本研究通过信息感知数据增强，解决了在对比学习中对于时间序列数据的多样增强方式选择问题。

    

    近年来，许多对比学习方法已被提出并取得了显著的实证成果。虽然有效且普遍，但对于时间序列数据，对比学习的探索较少。对比学习的关键组成部分是选择适当的数据增强方式，引入一些先验信息来构造可行的正样本，从而训练编码器学习强大而具有区分度的表示。与图像和语言领域可以通过人类先验知识生成期望的增强样本不同，由于时间序列增强样本具有多样的、人类难以识别的时间结构，如何找到对于给定对比学习任务和数据集有意义的时间序列增强方式仍是一个开放的问题。本文通过鼓励基于信息感知的数据增强，同时保证高保真度和多样性来解决这个问题。

    Various contrastive learning approaches have been proposed in recent years and achieve significant empirical success. While effective and prevalent, contrastive learning has been less explored for time series data. A key component of contrastive learning is to select appropriate augmentations imposing some priors to construct feasible positive samples, such that an encoder can be trained to learn robust and discriminative representations. Unlike image and language domains where ``desired'' augmented samples can be generated with the rule of thumb guided by prefabricated human priors, the ad-hoc manual selection of time series augmentations is hindered by their diverse and human-unrecognizable temporal structures. How to find the desired augmentations of time series data that are meaningful for given contrastive learning tasks and datasets remains an open question. In this work, we address the problem by encouraging both high \textit{fidelity} and \textit{variety} based upon information
    
[^22]: 经典谱估计的非渐进式点值和最坏情况边界

    Non-Asymptotic Pointwise and Worst-Case Bounds for Classical Spectrum Estimators. (arXiv:2303.11908v1 [math.ST])

    [http://arxiv.org/abs/2303.11908](http://arxiv.org/abs/2303.11908)

    本文给出了非渐进误差边界，为广泛类别的谱估计器提供了误差边界，包括在特定频率处的点值误差边界和所有频率下的最坏情况误差边界，并利用该方法导出了经典谱估计器（Blackman-Tukey、Bartlett 和 Welch 估计器）的误差边界。

    

    谱估计是时间序列数据分析的基本方法，应用包括医学、语音分析和控制设计。虽然谱估计的渐进理论很好理解，但在样本数量固定且有限的情况下，该理论存在一定局限性。本文为广泛类别的谱估计器提供了非渐进误差边界，包括在特定频率处的点值误差边界和所有频率下的最坏情况误差边界。本文所提的一般方法也用于导出了经典的 Blackman-Tukey、Bartlett 和 Welch 估计器的误差边界。

    Spectrum estimation is a fundamental methodology in the analysis of time-series data, with applications including medicine, speech analysis, and control design. The asymptotic theory of spectrum estimation is well-understood, but the theory is limited when the number of samples is fixed and finite. This paper gives non-asymptotic error bounds for a broad class of spectral estimators, both pointwise (at specific frequencies) and in the worst case over all frequencies. The general method is used to derive error bounds for the classical Blackman-Tukey, Bartlett, and Welch estimators.
    
[^23]: 通过系统评估更好地理解归因方法的差异

    Better Understanding Differences in Attribution Methods via Systematic Evaluations. (arXiv:2303.11884v1 [cs.CV])

    [http://arxiv.org/abs/2303.11884](http://arxiv.org/abs/2303.11884)

    本文提出了三种新的评估方案，通过这些方案，可以更可靠地测量归因方法的可信度。

    

    深度神经网络在许多视觉任务上取得了巨大成功，但其黑盒性质使其难以解释。为了克服这一问题，提出了各种后续归因方法来确定对模型决策最有影响力的图像区域。由于不存在基准归因，因此评估这些方法是具有挑战性的。因此，我们提出了三种新的评估方案，以更可靠地测量这些方法的可信度，使它们之间的比较更公平，并使视觉检查更系统化。

    Deep neural networks are very successful on many vision tasks, but hard to interpret due to their black box nature. To overcome this, various post-hoc attribution methods have been proposed to identify image regions most influential to the models' decisions. Evaluating such methods is challenging since no ground truth attributions exist. We thus propose three novel evaluation schemes to more reliably measure the faithfulness of those methods, to make comparisons between them more fair, and to make visual inspection more systematic. To address faithfulness, we propose a novel evaluation setting (DiFull) in which we carefully control which parts of the input can influence the output in order to distinguish possible from impossible attributions. To address fairness, we note that different methods are applied at different layers, which skews any comparison, and so evaluate all methods on the same layers (ML-Att) and discuss how this impacts their performance on quantitative metrics. For mo
    
[^24]: 用保护性自适应剪枝来更好地压缩DNN

    Protective Self-Adaptive Pruning to Better Compress DNNs. (arXiv:2303.11881v1 [cs.CV])

    [http://arxiv.org/abs/2303.11881](http://arxiv.org/abs/2303.11881)

    本文提出了一种新的保护性自适应剪枝（PSAP）方法，该方法利用权重稀疏比自适应地调整层的剪枝比率，并通过监督梯度来避免重要过滤器被剪枝，从而避免不可恢复的信息丢失。

    

    自适应网络剪枝方法因其优秀的能力来识别层和过滤器的重要性和冗余性并定制合适的剪枝方案而受到了极大的关注。然而，由于当前的自适应剪枝方法大多依赖于额外的监视器来评分层和过滤器的重要性，因此仍然不尽人意，面临着复杂性和可解释性弱的问题。为了解决这些问题，我们深入研究了迭代修剪-训练过程中的重量重构过程，并提出了一种保护性自适应修剪（PSAP）方法。首先，PSAP可以利用自身信息，即重量稀疏比，以使每个修剪步骤之前自适应地调整层的修剪比率。此外，我们提出了一种保护性重建机制，通过监督梯度来防止重要过滤器被修剪并避免不可恢复的信息丢失。我们的PSAP非常便捷和明确，因为它仅依赖于权重。

    Adaptive network pruning approach has recently drawn significant attention due to its excellent capability to identify the importance and redundancy of layers and filters and customize a suitable pruning solution. However, it remains unsatisfactory since current adaptive pruning methods rely mostly on an additional monitor to score layer and filter importance, and thus faces high complexity and weak interpretability. To tackle these issues, we have deeply researched the weight reconstruction process in iterative prune-train process and propose a Protective Self-Adaptive Pruning (PSAP) method. First of all, PSAP can utilize its own information, weight sparsity ratio, to adaptively adjust pruning ratio of layers before each pruning step. Moreover, we propose a protective reconstruction mechanism to prevent important filters from being pruned through supervising gradients and to avoid unrecoverable information loss as well. Our PSAP is handy and explicit because it merely depends on weigh
    
[^25]: 两个电路的故事：稀疏和密集子网络的竞争解析

    A Tale of Two Circuits: Grokking as Competition of Sparse and Dense Subnetworks. (arXiv:2303.11873v1 [cs.LG])

    [http://arxiv.org/abs/2303.11873](http://arxiv.org/abs/2303.11873)

    本文研究了网络内部结构，发现Grokking现象对应于稀疏子网络的出现，该子网络在优化过程中由于少数神经元的快速各向同性增长而出现，从而与支配泛化能力差的密集子网络竞争。

    

    Grokking是指在算法任务上训练的模型首先出现过拟合，但是在大量额外的训练后，出现了完美的泛化的现象。我们在稀疏奇偶任务上经验地研究了正在经历Grokking的网络的内部结构，并发现Grokking的相变对应于支配模型预测的稀疏子网络的出现。在优化级别上，我们发现当少数神经元经历快速的各向同性增长时，这个子网络会出现，而网络中的其他神经元则缓慢地衰减。因此，我们建议Grokking的相变可以理解为两个大不相同的子网络之间的竞争：在转变之前支配的是密集子网络，但它泛化能力很差，在转变之后支配的是稀疏子网络。

    Grokking is a phenomenon where a model trained on an algorithmic task first overfits but, then, after a large amount of additional training, undergoes a phase transition to generalize perfectly. We empirically study the internal structure of networks undergoing grokking on the sparse parity task, and find that the grokking phase transition corresponds to the emergence of a sparse subnetwork that dominates model predictions. On an optimization level, we find that this subnetwork arises when a small subset of neurons undergoes rapid norm growth, whereas the other neurons in the network decay slowly in norm. Thus, we suggest that the grokking phase transition can be understood to emerge from competition of two largely distinct subnetworks: a dense one that dominates before the transition and generalizes poorly, and a sparse one that dominates afterwards.
    
[^26]: 使用脉冲神经元的在线Transformer用于快速假肢手控制

    Online Transformers with Spiking Neurons for Fast Prosthetic Hand Control. (arXiv:2303.11860v1 [cs.NE])

    [http://arxiv.org/abs/2303.11860](http://arxiv.org/abs/2303.11860)

    该论文采用了滑动窗口注意机制来替代Transformer中的自注意机制，从而使得使用逐个元素地处理序列，更加适用于在线信号处理，并且在手指位置回归数据集上实现了最好的准确性纪录，每个推理步骤仅需要非常短的时间窗口(3.5毫秒)。

    

    Transformer网络是大多数序列处理任务的最先进网络。然而，Transformer中经常使用的自注意机制需要大的时间窗口来进行每个计算步骤，因此与递归神经网络(RNN)相比，使得它们不太适用于在线信号处理。在本文中，我们使用滑动窗口注意机制来代替自注意机制。我们展示了这种机制对于在输入和目标之间存在有限范围依赖的连续信号更为高效，并且可以用它来逐个元素地处理序列，因此使其适用于在线处理。我们在一个指尖位置回归数据集(NinaproDB8)上测试了模型，该数据集使用在前臂皮肤上测量的Surface Electromyographic (sEMG)信号来估计肌肉活动。我们的方法在每个推理步骤中仅需要非常短的时间窗口(3.5毫秒)就能在这个数据集上取得最新的准确性纪录。

    Transformers are state-of-the-art networks for most sequence processing tasks. However, the self-attention mechanism often used in Transformers requires large time windows for each computation step and thus makes them less suitable for online signal processing compared to Recurrent Neural Networks (RNNs). In this paper, instead of the self-attention mechanism, we use a sliding window attention mechanism. We show that this mechanism is more efficient for continuous signals with finite-range dependencies between input and target, and that we can use it to process sequences element-by-element, this making it compatible with online processing. We test our model on a finger position regression dataset (NinaproDB8) with Surface Electromyographic (sEMG) signals measured on the forearm skin to estimate muscle activities. Our approach sets the new state-of-the-art in terms of accuracy on this dataset while requiring only very short time windows of 3.5 ms at each inference step. Moreover, we inc
    
[^27]: 基于密度的正样本增强的 PU Learning 方法：Dens-PU

    Dens-PU: PU Learning with Density-Based Positive Labeled Augmentation. (arXiv:2303.11848v1 [cs.LG])

    [http://arxiv.org/abs/2303.11848](http://arxiv.org/abs/2303.11848)

    Dens-PU是一种基于密度的正样本增强的PU Learning方法，可以用于解决二元分类问题，且在基准图像数据集上表现出最先进的结果。

    

    本研究提出了一种新的 PU Learning 方法，基于异常检测策略解决该问题。从正样本数据中提取的潜在编码线性组合以获得新样本。这些新样本被用作嵌入以增加正样本数据的密度，从而定义近似正类的边界。样本距离边界越远，则认为它是负样本的可能性越大。一旦获得一组负样本，PU Learning 问题就转化为二元分类。名为 Dens-PU 的方法，由于其依赖于正样本数据的密度，经过基准图像数据集的评估，取得了最先进的结果。

    This study proposes a novel approach for solving the PU learning problem based on an anomaly-detection strategy. Latent encodings extracted from positive-labeled data are linearly combined to acquire new samples. These new samples are used as embeddings to increase the density of positive-labeled data and, thus, define a boundary that approximates the positive class. The further a sample is from the boundary the more it is considered as a negative sample. Once a set of negative samples is obtained, the PU learning problem reduces to binary classification. The approach, named Dens-PU due to its reliance on the density of positive-labeled data, was evaluated using benchmark image datasets, and state-of-the-art results were attained.
    
[^28]: 双重正则化熵 Wasserstein 重心

    Doubly Regularized Entropic Wasserstein Barycenters. (arXiv:2303.11844v1 [math.OC])

    [http://arxiv.org/abs/2303.11844](http://arxiv.org/abs/2303.11844)

    本文提出了一种双重正则化熵Wasserstein重心公式，具有好的正则化、逼近、稳定性和（无网格）优化特性; 其中，只有在$\tau=\lambda/2$的情况下是无偏差的。

    

    我们研究了一种常规的正则化Wasserstein重心的公式，这个公式具有良好的正则化、逼近、稳定性和（无网格）优化特性。这个重心被定义为唯一一种最小化关于一族给定概率测度的熵最优输运（EOT）成本之和及熵项的概率测度。我们称之为$(\lambda,\tau)$-重心，其中，$\lambda$ 是内部正则化强度，$\tau$ 是外部正则化强度。这种公式恢复了已经提出的多种EOT重心，适合于不同的 $\lambda, \tau \geq 0$ 选择，并对它们进行了泛化。首先，尽管具有双重正则化，但在$\tau=\lambda/2$ 的情况下，我们证明了我们的公式是无偏的: 对于光滑密度，（未正则化的）Wasserstein 重心目标函数中的次优性是熵正则化强度$\lambda^2$的，而不是一般情况下的$\max \{\lambda, \tau\}$。

    We study a general formulation of regularized Wasserstein barycenters that enjoys favorable regularity, approximation, stability and (grid-free) optimization properties. This barycenter is defined as the unique probability measure that minimizes the sum of entropic optimal transport (EOT) costs with respect to a family of given probability measures, plus an entropy term. We denote it $(\lambda,\tau)$-barycenter, where $\lambda$ is the inner regularization strength and $\tau$ the outer one. This formulation recovers several previously proposed EOT barycenters for various choices of $\lambda,\tau \geq 0$ and generalizes them. First, in spite of -- and in fact owing to -- being \emph{doubly} regularized, we show that our formulation is debiased for $\tau=\lambda/2$: the suboptimality in the (unregularized) Wasserstein barycenter objective is, for smooth densities, of the order of the strength $\lambda^2$ of entropic regularization, instead of $\max\{\lambda,\tau\}$ in general. We discuss 
    
[^29]: 利用Cayley变换和可控性Gram矩的Lipschitz-bounded 1D卷积神经网络(arXiv:2303.11835v1 [cs.LG])

    Lipschitz-bounded 1D convolutional neural networks using the Cayley transform and the controllability Gramian. (arXiv:2303.11835v1 [cs.LG])

    [http://arxiv.org/abs/2303.11835](http://arxiv.org/abs/2303.11835)

    本文提出了一个逐层参数化方法，用于实现内置鲁棒性保证的1D卷积神经网络。该方法基于CNN特征的Lipschitz常数作为鲁棒性度量，并使用Cayley变换和可控性Gram矩来实现CNN的Lipschitz连续性和无约束训练，最后在心律失常数据分类任务中取得了改进的鲁棒性。

    

    我们建立了一种用于1D卷积神经网络（CNN）的逐层参数化，具有内置的端到端鲁棒性保证。我们使用CNN特征的Lipschitz常数作为鲁棒性度量。我们基于Cayley变换对正交矩阵进行参数化以及对卷积层的状态空间表征的可控性Gram矩进行参数化。所提出的参数化设计满足线性矩阵不等式，从而实现CNN的Lipschitz连续性，进一步实现Lipschitz-bounded 1D CNNs的无约束训练。最后，我们对心律失常数据进行Lipschitz-bounded 1D CNNs的分类训练，并展示了其改进的鲁棒性。

    We establish a layer-wise parameterization for 1D convolutional neural networks (CNNs) with built-in end-to-end robustness guarantees. Herein, we use the Lipschitz constant of the input-output mapping characterized by a CNN as a robustness measure. We base our parameterization on the Cayley transform that parameterizes orthogonal matrices and the controllability Gramian for the state space representation of the convolutional layers. The proposed parameterization by design fulfills linear matrix inequalities that are sufficient for Lipschitz continuity of the CNN, which further enables unconstrained training of Lipschitz-bounded 1D CNNs. Finally, we train Lipschitz-bounded 1D CNNs for the classification of heart arrythmia data and show their improved robustness.
    
[^30]: 基于人工智能驱动的组合化学在极端特性材料发现中的应用

    Materials Discovery with Extreme Properties via AI-Driven Combinatorial Chemistry. (arXiv:2303.11833v1 [q-bio.BM])

    [http://arxiv.org/abs/2303.11833](http://arxiv.org/abs/2303.11833)

    本文提出了一种基于人工智能驱动的组合化学方法，不依赖于数据，可以发现未知材料并具有更优越的性质。实验证明这种方法比概率分布学习的模型更适合于发现更好的材料。

    

    大多数材料的发现都旨在发现比目前已知材料更优越的材料。然而，这很接近于外推，对于大多数学习数据概率分布的机器学习模型来说这是一个弱点。本文提出了一种基于人工智能驱动的组合化学方法，它是一种基于规则的反向分子设计器，不依赖于数据。由于我们的模型有可能生成从分子片段组合中获得的所有可能的分子结构，因此可以发现具有更优越性质的未知材料。我们理论和实验证明了我们的模型比概率分布学习的模型更适合于发现更好的材料。在一个旨在发现七个目标特性分子的实验中，我们的模型在10万次试验中发现了1315个达到全部目标的分子和7629个达到五个目标的分子，而概率分布学习模型只有发现几个。

    The goal of most materials discovery is to discover materials that are superior to those currently known. Fundamentally, this is close to extrapolation, which is a weak point for most machine learning models that learn the probability distribution of data. Herein, we develop AI-driven combinatorial chemistry, which is a rule-based inverse molecular designer that does not rely on data. Since our model has the potential to generate all possible molecular structures that can be obtained from combinations of molecular fragments, unknown materials with superior properties can be discovered. We theoretically and empirically demonstrate that our model is more suitable for discovering better materials than probability distribution-learning models. In an experiment aimed at discovering molecules that hit seven target properties, our model discovered 1,315 of all target-hitting molecules and 7,629 of five target-hitting molecules out of 100,000 trials, whereas the probability distribution-learni
    
[^31]: GLADE：用于非配对超分辨率各向异性MRI的梯度损失增强退化增强

    GLADE: Gradient Loss Augmented Degradation Enhancement for Unpaired Super-Resolution of Anisotropic MRI. (arXiv:2303.11831v1 [cs.CV])

    [http://arxiv.org/abs/2303.11831](http://arxiv.org/abs/2303.11831)

    本文提出了一种可用于加速全腹MRI扫描的新方法GLADE，它通过使用梯度映射损失来合成高分辨率等向性3D腹部MR图像。

    

    我们提出了一种新方法，在非配对的情况下，从各向异性3D图像中合成高分辨率等向性3D腹部MR图像。通过使用修改后的CycleGAN架构，并使用梯度映射损失，我们利用来自各向异性体积高分辨率（面内）数据的不重叠的补丁，强制网络生成器增加低分辨率（面外）切片的分辨率。这将使在短时间内以高分辨率等向性图像进行全腹扫描成为可能。

    We present a novel approach to synthesise high-resolution isotropic 3D abdominal MR images, from anisotropic 3D images in an unpaired fashion. Using a modified CycleGAN architecture with a gradient mapping loss, we leverage disjoint patches from the high-resolution (in-plane) data of an anisotropic volume to enforce the network generator to increase the resolution of the low-resolution (through-plane) slices. This will enable accelerated whole-abdomen scanning with high-resolution isotropic images within short breath-hold times.
    
[^32]: 解决联邦半监督学习中的类变量不平衡问题

    Addressing Class Variable Imbalance in Federated Semi-supervised Learning. (arXiv:2303.11809v1 [cs.LG])

    [http://arxiv.org/abs/2303.11809](http://arxiv.org/abs/2303.11809)

    本文介绍了一种称为联邦半监督学习与类变量不平衡（FSSL-CVI）的新方法，它使用动态类别加权方案来处理FSSL中的类别变量不平衡问题，并且在多个数据集上进行了实验验证。通过实验结果，本文表明 FSSL-CVI 方法在各方面性能上优于现有的联邦学习和FSSL 方法。

    

    联邦半监督学习（FSSL）结合联邦学习和半监督学习的技术，通过使用少量标注数据和大量未标注数据在分布式环境中提高模型的准确性和性能。在不需要将所有数据集中于一处进行训练的情况下，它会在设备本地训练模型后收集模型训练更新，从而可以保护用户数据的隐私。然而，在联邦训练过程中，一些设备无法收集足够的本地训练数据，同时新设备将被添加到组训练中。这导致不平衡的全局数据分布，从而影响全局模型训练的性能。大多数当前的研究着重于固定类别数量的类别不平衡问题，而很少有注意力放在具有可变类别数量的数据不平衡问题上。因此，在本文中，我们提出了联邦半监督学习与类变量不平衡（FSSL-CVI），它使用动态类权重方案来解决FSSL中的类变量不平衡问题。实验结果表明，我们提出的方法在具有类变量不平衡的各种数据集上的分类准确率方面优于现有的联邦学习和FSSL方法。

    Federated Semi-supervised Learning (FSSL) combines techniques from both fields of federated and semi-supervised learning to improve the accuracy and performance of models in a distributed environment by using a small fraction of labeled data and a large amount of unlabeled data. Without the need to centralize all data in one place for training, it collect updates of model training after devices train models at local, and thus can protect the privacy of user data. However, during the federal training process, some of the devices fail to collect enough data for local training, while new devices will be included to the group training. This leads to an unbalanced global data distribution and thus affect the performance of the global model training. Most of the current research is focusing on class imbalance with a fixed number of classes, while little attention is paid to data imbalance with a variable number of classes. Therefore, in this paper, we propose Federated Semi-supervised Learni
    
[^33]: 图上随机逆问题：分布式在线学习

    Random Inverse Problems Over Graphs: Decentralized Online Learning. (arXiv:2303.11789v1 [cs.LG])

    [http://arxiv.org/abs/2303.11789](http://arxiv.org/abs/2303.11789)

    本文提出了一种基于在线数据流的分布式在线学习算法，将希尔伯特空间中的分布参数估计和再生核希尔伯特空间中的最小均方问题统一起来，并发展了一种新的L2-渐近稳定性理论。该算法在网络图为连通且正向算子序列满足无限维度时空励磁条件的情况下，能够实现均方和几乎必然的强一致估计。

    

    我们建立了一个随机逆问题的框架，该问题具有实时的图上观测，并提出了一种基于在线数据流的分布式在线学习算法，将希尔伯特空间中的分布参数估计和再生核希尔伯特空间中的最小均方问题统一起来。我们将算法收敛性转化为带有L2有界鞅差分项的希尔伯特空间中随机时变差分方程的渐近稳定性，并发展了L2-渐近稳定性理论。结果表明，如果网络图是连通的，并且正向算子序列满足无限维度时空励磁条件，则所有节点的估计均为均方和几乎必然的强一致的。通过将RKHS中的分布式学习问题等效地转化为图上随机逆问题，我们提出了一种基于无中心节点的RKHS分布式在线学习算法。

    We establish a framework of random inverse problems with real-time observations over graphs, and present a decentralized online learning algorithm based on online data streams, which unifies the distributed parameter estimation in Hilbert space and the least mean square problem in reproducing kernel Hilbert space (RKHS-LMS). We transform the algorithm convergence into the asymptotic stability of randomly time-varying difference equations in Hilbert space with L2-bounded martingale difference terms and develop the L2 -asymptotic stability theory. It is shown that if the network graph is connected and the sequence of forward operators satisfies the infinitedimensional spatio-temporal persistence of excitation condition, then the estimates of all nodes are mean square and almost surely strongly consistent. By equivalently transferring the distributed learning problem in RKHS to the random inverse problem over graphs, we propose a decentralized online learning algorithm in RKHS based on no
    
[^34]: Skeleton Regression：一种基于流形结构估计的基于图形的方法。

    Skeleton Regression: A Graph-Based Approach to Estimation with Manifold Structure. (arXiv:2303.11786v1 [cs.LG])

    [http://arxiv.org/abs/2303.11786](http://arxiv.org/abs/2303.11786)

    这是一个处理低维流形数据的回归框架，首先通过构建图形骨架来捕捉潜在的流形几何结构，然后在其上运用非参数回归技术来估计回归函数，除了具有非参数优点之外，在处理多个流形数据，嘈杂观察时也表现出较好的鲁棒性。

    

    我们引入了一个新的回归框架，旨在处理围绕低维流形的复杂数据。我们的方法首先构建一个图形表示，称为骨架，以捕获潜在的几何结构。然后，我们在骨架图上定义指标，应用非参数回归技术，以及基于图形的特征转换来估计回归函数。除了包括的非参数方法外，我们还讨论了一些非参数回归器在骨架图等一般度量空间方面的限制。所提出的回归框架使我们能够避开维度灾难，具有可以处理多个流形的并集并且鲁棒性能应对加性噪声和嘈杂观察的额外优势。我们为所提出的方法提供了统计保证，并通过模拟和实际数据示例证明了其有效性。

    We introduce a new regression framework designed to deal with large-scale, complex data that lies around a low-dimensional manifold. Our approach first constructs a graph representation, referred to as the skeleton, to capture the underlying geometric structure. We then define metrics on the skeleton graph and apply nonparametric regression techniques, along with feature transformations based on the graph, to estimate the regression function. In addition to the included nonparametric methods, we also discuss the limitations of some nonparametric regressors with respect to the general metric space such as the skeleton graph. The proposed regression framework allows us to bypass the curse of dimensionality and provides additional advantages that it can handle the union of multiple manifolds and is robust to additive noise and noisy observations. We provide statistical guarantees for the proposed method and demonstrate its effectiveness through simulations and real data examples.
    
[^35]: 轻量级对比蛋白质结构-序列变换

    Lightweight Contrastive Protein Structure-Sequence Transformation. (arXiv:2303.11783v1 [q-bio.BM])

    [http://arxiv.org/abs/2303.11783](http://arxiv.org/abs/2303.11783)

    该论文提出了一种新的无监督学习的蛋白质结构表示预训练方法，使用强大的蛋白质语言模型和自监督结构约束，避免了破坏真实的空间结构表示和标记数据的限制。

    

    在大多数蛋白质下游应用中，无标签的预训练蛋白质结构模型是关键基础。传统的结构预训练方法遵循成熟的自然语言预训练方法，例如去噪重构和掩码语言建模，但通常会破坏真实的空间结构表示。其他常见的预训练方法可能会预测一组固定的预定对象类别，其中受限的监督方式限制了它们的通用性和可用性，因为需要额外的标记数据来指定任何其他的蛋白质概念。在这项工作中，我们引入了一种新的无监督蛋白质结构表示预训练方法，其中使用强大的蛋白质语言模型。特别地，我们首先建议利用现有的预训练语言模型通过无监督的对比对齐来指导结构模型的学习。此外，我们提出了一种自监督结构约束，以进一步学习内在的蛋白质结构表示形式。

    Pretrained protein structure models without labels are crucial foundations for the majority of protein downstream applications. The conventional structure pretraining methods follow the mature natural language pretraining methods such as denoised reconstruction and masked language modeling but usually destroy the real representation of spatial structures. The other common pretraining methods might predict a fixed set of predetermined object categories, where a restricted supervised manner limits their generality and usability as additional labeled data is required to specify any other protein concepts. In this work, we introduce a novel unsupervised protein structure representation pretraining with a robust protein language model. In particular, we first propose to leverage an existing pretrained language model to guide structure model learning through an unsupervised contrastive alignment. In addition, a self-supervised structure constraint is proposed to further learn the intrinsic i
    
[^36]: Rademacher随机嵌入的精确非遗忘性能

    Exact Non-Oblivious Performance of Rademacher Random Embeddings. (arXiv:2303.11774v1 [cs.LG])

    [http://arxiv.org/abs/2303.11774](http://arxiv.org/abs/2303.11774)

    本文证实了Rademacher随机投影的非遗忘性能，并建立了Schur-凹性质。这项成果为随机投影的性能提供了新的几何视角，具有更好的量化界限，填补了理论和实践之间的差距。

    

    本文重新审视了Rademacher随机投影的性能，建立了新颖的统计保证，与输入数据相比具有数值上的精度和非遗忘性。具体而言，中心结果是Rademacher随机投影与输入的Schur-凹性质。这提供了随机投影性能的新颖几何视角，同时在比以前的研究更好地提高了量化界限。作为这一更广泛结果的推论，我们在稀疏数据或分布具有小范围的数据上获得了改进的表现。这种非遗忘性分析与以前的技术相比是一种新奇之处，并弥合了理论和实践之间经常观察到的差距。主要结果使用代数框架来证明Schur-凹性质，这是一个独立于兴趣的贡献，并且是导数基础标准的一种优雅的替代方法。

    This paper revisits the performance of Rademacher random projections, establishing novel statistical guarantees that are numerically sharp and non-oblivious with respect to the input data. More specifically, the central result is the Schur-concavity property of Rademacher random projections with respect to the inputs. This offers a novel geometric perspective on the performance of random projections, while improving quantitatively on bounds from previous works. As a corollary of this broader result, we obtained the improved performance on data which is sparse or is distributed with small spread. This non-oblivious analysis is a novelty compared to techniques from previous work, and bridges the frequently observed gap between theory and practise. The main result uses an algebraic framework for proving Schur-concavity properties, which is a contribution of independent interest and an elegant alternative to derivative-based criteria.
    
[^37]: 采用开源Metaflow进行合理规模的机器学习

    Reasonable Scale Machine Learning with Open-Source Metaflow. (arXiv:2303.11761v1 [cs.LG])

    [http://arxiv.org/abs/2303.11761](http://arxiv.org/abs/2303.11761)

    本文介绍了一个针对机器学习项目设计的开源框架Metaflow，其可以通过将ML代码的执行与业务逻辑的定义分离，来提高数据从业者的生产力。

    

    随着机器学习（ML）在各行各业和新的用例中的应用日益增多，从业者越来越意识到在有效开发和迭代ML系统方面所面临的挑战：重现性、调试、可伸缩性和文档编写对于技术第一类公司以外的真实世界管道来说是难以实现的目标。本文回顾了面向ML的工作负载的性质，并认为重新利用现有工具无法解决当前生产力问题，因为ML的特殊性需要专门的开发工具。然后，我们介绍了Metaflow，一个专门为ML项目设计的开源框架，通过将ML代码的执行与业务逻辑的定义分离来提高数据从业者的生产力。我们展示了我们的设计如何解决ML操作（MLOps）中的主要挑战，并通过示例、采访和用例记录了它对该领域的实际影响。

    As Machine Learning (ML) gains adoption across industries and new use cases, practitioners increasingly realize the challenges around effectively developing and iterating on ML systems: reproducibility, debugging, scalability, and documentation are elusive goals for real-world pipelines outside tech-first companies. In this paper, we review the nature of ML-oriented workloads and argue that re-purposing existing tools won't solve the current productivity issues, as ML peculiarities warrant specialized development tooling. We then introduce Metaflow, an open-source framework for ML projects explicitly designed to boost the productivity of data practitioners by abstracting away the execution of ML code from the definition of the business logic. We show how our design addresses the main challenges in ML operations (MLOps), and document through examples, interviews and use cases its practical impact on the field.
    
[^38]: 利用多模态潜在映射表面来改进自动驾驶车辆的深度动力学模型

    Improving Deep Dynamics Models for Autonomous Vehicles with Multimodal Latent Mapping of Surfaces. (arXiv:2303.11756v1 [cs.RO])

    [http://arxiv.org/abs/2303.11756](http://arxiv.org/abs/2303.11756)

    该论文提出了一种新的方法来改进自动驾驶汽车的深度动力学模型，通过从多个感知模态中获取信息并将其存储在地图中的方式来更新潜在地图，从而实现对表面信息的感知和识别，在不同表面上的精准操纵得到保障。

    

    自主驾驶汽车的安全部署依赖于它们有效反应环境变化的能力。这可能需要在不同表面上操纵，这仍然是一个难题，特别是对于滑动地形。为了解决这个问题，我们提出了一种新的方法，通过将当前位置的表面信息存储在潜在变量向量中来条件化学习表面感知的动力学模型。通过在推理中从对应位置的多个模态获取信息，并将信息存储到地图中，训练潜在映射器来更新这些潜在变量。通过将一切都与动力学模型的损失进行端对端的训练，我们强制潜在映射器学习潜在地图的更新规则，使其对随后的动力学模型有用。我们在真实的迷你电动汽车上实现和评估了我们的方法。结果表明，相比没有表面信息的模型，潜在地图的更新可以使动力学模型的预测更加准确。

    The safe deployment of autonomous vehicles relies on their ability to effectively react to environmental changes. This can require maneuvering on varying surfaces which is still a difficult problem, especially for slippery terrains. To address this issue we propose a new approach that learns a surface-aware dynamics model by conditioning it on a latent variable vector storing surface information about the current location. A latent mapper is trained to update these latent variables during inference from multiple modalities on every traversal of the corresponding locations and stores them in a map. By training everything end-to-end with the loss of the dynamics model, we enforce the latent mapper to learn an update rule for the latent map that is useful for the subsequent dynamics model. We implement and evaluate our approach on a real miniature electric car. The results show that the latent map is updated to allow more accurate predictions of the dynamics model compared to a model with
    
[^39]: 潜在图推断中的模型空间投影。

    Projections of Model Spaces for Latent Graph Inference. (arXiv:2303.11754v1 [cs.LG])

    [http://arxiv.org/abs/2303.11754](http://arxiv.org/abs/2303.11754)

    本文将双曲和球形模型空间的立体投影以及Riemannian隐空间的乘积应用于潜在图推断，实现了与非投影空间相当的性能并提供理论保证。

    

    图神经网络利用图的连接结构作为归纳偏差。潜在图推断关注于学习一个合适的图结构来扩散信息并提高模型的下游性能。本文利用双曲和球形模型空间的立体投影，以及Riemannian隐空间的乘积，用于潜在图推断。在避免曲率趋于零时空间发散的理论保证下，立体投影模型空间能实现与非投影对应模型空间相当的性能。我们在同构和异构图上进行实验。

    Graph Neural Networks leverage the connectivity structure of graphs as an inductive bias. Latent graph inference focuses on learning an adequate graph structure to diffuse information on and improve the downstream performance of the model. In this work we employ stereographic projections of the hyperbolic and spherical model spaces, as well as products of Riemannian manifolds, for the purpose of latent graph inference. Stereographically projected model spaces achieve comparable performance to their non-projected counterparts, while providing theoretical guarantees that avoid divergence of the spaces when the curvature tends to zero. We perform experiments on both homophilic and heterophilic graphs.
    
[^40]: 图书馆中的推荐系统：一种异构数据源应用

    Recommendation Systems in Libraries: an Application with Heterogeneous Data Sources. (arXiv:2303.11746v1 [cs.IR])

    [http://arxiv.org/abs/2303.11746](http://arxiv.org/abs/2303.11746)

    本文介绍了一种图书馆推荐系统，利用了来自异构数据源的数据，包括借阅记录和在线社交读者的反馈和信息。研究发现，协同过滤（CF）方法优于基于内容的（CB）方法，可以提高推荐系统的性能达到47%。

    

    Reading&Machine项目利用数字化支持，提高图书馆的吸引力和改善用户体验。该项目实现了一个应用程序，通过推荐系统生成用户可能感兴趣的书籍列表，并通过基于虚拟现实（VR）的图形用户界面（GUI）互动显示，帮助用户进行决策。本文重点研究了推荐系统的设计和测试，采用意大利都灵图书馆网络过去9年所有用户借阅的数据，以及读者在线社交平台Anobii收集的反馈和关于读过书的附加信息。凭借这种异构的数据，我们构建和评估了基于内容的（CB）和协同过滤（CF）方法。结果表明，CF优于CB方法，提高了最高47%。

    The Reading&Machine project exploits the support of digitalization to increase the attractiveness of libraries and improve the users' experience. The project implements an application that helps the users in their decision-making process, providing recommendation system (RecSys)-generated lists of books the users might be interested in, and showing them through an interactive Virtual Reality (VR)-based Graphical User Interface (GUI). In this paper, we focus on the design and testing of the recommendation system, employing data about all users' loans over the past 9 years from the network of libraries located in Turin, Italy. In addition, we use data collected by the Anobii online social community of readers, who share their feedback and additional information about books they read. Armed with this heterogeneous data, we build and evaluate Content Based (CB) and Collaborative Filtering (CF) approaches. Our results show that the CF outperforms the CB approach, improving by up to 47\% the
    
[^41]: 基于射频环境地图的O-RAN架构中的波束管理

    Beam Management Driven by Radio Environment Maps in O-RAN Architecture. (arXiv:2303.11742v1 [cs.NI])

    [http://arxiv.org/abs/2303.11742](http://arxiv.org/abs/2303.11742)

    本文提出了一种基于REM的BM算法，使用RL从不同目标函数的角度来优化该过程，从而实现最大化接收功率和最小化波束重新选择。

    

    大规模多输入多输出(M-MIMO)被认为是5G和未来6G网络的关键技术之一。本文旨在提出一种基于射频环境地图(Radio Environment Map,REM)的波束管理算法，它利用接收功率映射和用户移动模式，在不同目标函数下从强化学习(Reinforcement Learning,RL)的角度来优化BM过程，例如最大化接收功率或在避免无线电链路故障的同时最小化波束重新选择。

    The Massive Multiple-Input Multiple-Output (M-MIMO) is considered as one of the key technologies in 5G, and future 6G networks. From the perspective of, e.g., channel estimation, especially for high-speed users it is easier to implement an M-MIMO network exploiting a static set of beams, i.e., Grid of Beams (GoB). While considering GoB it is important to properly assign users to the beams, i.e., to perform Beam Management (BM). BM can be enhanced by taking into account historical knowledge about the radio environment, e.g., to avoid radio link failures. The aim of this paper is to propose such a BM algorithm, that utilizes location-dependent data stored in a Radio Environment Map (REM). It utilizes received power maps, and user mobility patterns to optimize the BM process in terms of Reinforcement Learning (RL) by using the Policy Iteration method under different goal functions, e.g., maximization of received power or minimization of beam reselections while avoiding radio link failures
    
[^42]: “张量网络在量子机器学习中的应用”

    Tensor networks for quantum machine learning. (arXiv:2303.11735v1 [quant-ph])

    [http://arxiv.org/abs/2303.11735](http://arxiv.org/abs/2303.11735)

    张量网络作为机器学习的成功范例，被移植回量子领域, 可以评估传统计算机无法高效解决的问题，并可在量子计算机上轻松部署。MPS，PEPS，TTN和MERA等布局被用于量子机器学习，本文介绍了它们在量子计算机上的映射，并讨论了实现技术对其性能的影响。

    

    张量网络原本是为量子理论而开发的，但却被证明是一种成功的机器学习范例。现在，张量网络已经被移植回量子领域，用于评估传统计算机无法高效解决的问题，它们在物理学和机器学习之间的界面性质使得它们可以轻松地在量子计算机上部署。本综述文章介绍了其中一种被认为是用于变分量子机器学习的主要架构，特别是讨论了MPS，PEPS，TTN和MERA等布局如何被映射到量子计算机，以及它们如何用于机器学习和数据编码，以及哪些实现技术可以提高它们的性能。

    Once developed for quantum theory, tensor networks have been established as a successful machine learning paradigm. Now, they have been ported back to the quantum realm in the emerging field of quantum machine learning to assess problems that classical computers are unable to solve efficiently. Their nature at the interface between physics and machine learning makes tensor networks easily deployable on quantum computers. In this review article, we shed light on one of the major architectures considered to be predestined for variational quantum machine learning. In particular, we discuss how layouts like MPS, PEPS, TTNs and MERA can be mapped to a quantum computer, how they can be used for machine learning and data encoding and which implementation techniques improve their performance.
    
[^43]: 解锁自编码器的逐层相关传播

    Unlocking Layer-wise Relevance Propagation for Autoencoders. (arXiv:2303.11734v1 [cs.LG])

    [http://arxiv.org/abs/2303.11734](http://arxiv.org/abs/2303.11734)

    本文提出了一种解释自编码器的快速解决方案，通过将深度泰勒分解框架与逐层相关传播方法相结合。结果显示，该方法计算和质量方面具有比现有方法更好的优势。

    

    自编码器是一种强大而多功能的工具，常用于各种问题，如异常检测、图像处理和机器翻译。然而，它们的重建并不总是易于解释。因此，我们提出了一种通过将深度泰勒分解框架与逐层相关传播方法相结合来快速解释的解决方案。此外，我们引入了一种新的验证技术，用于比较我们的可解释性方法和缺失地面真实数据的基准方法。我们的结果凸显了所提出的可解释性解决方案相对于现有方法的计算和质量优势。

    Autoencoders are a powerful and versatile tool often used for various problems such as anomaly detection, image processing and machine translation. However, their reconstructions are not always trivial to explain. Therefore, we propose a fast explainability solution by extending the Layer-wise Relevance Propagation method with the help of Deep Taylor Decomposition framework. Furthermore, we introduce a novel validation technique for comparing our explainability approach with baseline methods in the case of missing ground-truth data. Our results highlight computational as well as qualitative advantages of the proposed explainability solution with respect to existing methods.
    
[^44]: 使用可微分排名的任务化优化投影集生成方法

    Task-based Generation of Optimized Projection Sets using Differentiable Ranking. (arXiv:2303.11724v1 [cs.CV])

    [http://arxiv.org/abs/2303.11724](http://arxiv.org/abs/2303.11724)

    该论文提出了一种使用前向神经网络通过可微分排名函数实现任务化优化投影集生成的方法，该方法能够选择出具有高价值的投影，以增强CT扫描的图像重建和诊断质量。

    

    我们提出了一种方法来选择在计算机体层摄影（CT）扫描中具有价值的投影，以增强图像重建和诊断。该方法将投影检测能力和数据完整性两个重要因素集成到一个前向神经网络中。网络评估投影的价值，通过可微分排名函数对其进行处理，并使用直通估计器进行最终选择。通过训练提供的标签来确保数据完整性。该方法消除了启发式强制实现数据完整性的需求，因为这可能会排除有价值的投影。该方法在模拟数据的非破坏性测试场景中进行评估，其目的是在指定的感兴趣区域内最大化重建质量。我们实现了与以前方法可比较的结果，为使用基于重建的损失函数学习投影选择奠定了基础。

    We present a method for selecting valuable projections in computed tomography (CT) scans to enhance image reconstruction and diagnosis. The approach integrates two important factors, projection-based detectability and data completeness, into a single feed-forward neural network. The network evaluates the value of projections, processes them through a differentiable ranking function and makes the final selection using a straight-through estimator. Data completeness is ensured through the label provided during training. The approach eliminates the need for heuristically enforcing data completeness, which may exclude valuable projections. The method is evaluated on simulated data in a non-destructive testing scenario, where the aim is to maximize the reconstruction quality within a specified region of interest. We achieve comparable results to previous methods, laying the foundation for using reconstruction-based loss functions to learn the selection of projections.
    
[^45]: 采用具有空间感知Shapley值的Lidar线路选择进行成本效益的深度补全

    Lidar Line Selection with Spatially-Aware Shapley Value for Cost-Efficient Depth Completion. (arXiv:2303.11720v1 [cs.LG])

    [http://arxiv.org/abs/2303.11720](http://arxiv.org/abs/2303.11720)

    本文提出了一种采用具有空间感知Shapley值的Lidar线路选择进行成本效益的深度补全方法，成功地实现减少线路数量和保持深度补全质量的目标，使用了一半的线路就能够达到与完整的Lidar输入相当的深度精度。

    

    Lidar是估计场景深度的重要传感器。 典型的旋转式Lidar发射排列在几个水平线上的脉冲，传感器的货币成本随这些线的数量增加而增加。 在本文中，我们提出了优化Lidar线路位置以寻找深度补全任务的最有效配置的新问题。 我们提出了一种解决方案，以减少线路数量，同时保持深度补全质量，这种方法包括两个组件，（1）基于Shapley值计算的线路边际贡献的线路选择，和（2）将线路位置分布结合起来，以考虑到其需要到达全局深度。具有空间感知Shapley值（SaS）成功地选择了线路子集，这些线路子集使用了一半的线路，但具有与完整的Lidar输入相当的深度精度。

    Lidar is a vital sensor for estimating the depth of a scene. Typical spinning lidars emit pulses arranged in several horizontal lines and the monetary cost of the sensor increases with the number of these lines. In this work, we present the new problem of optimizing the positioning of lidar lines to find the most effective configuration for the depth completion task. We propose a solution to reduce the number of lines while retaining the up-to-the-mark quality of depth completion. Our method consists of two components, (1) line selection based on the marginal contribution of a line computed via the Shapley value and (2) incorporating line position spread to take into account its need to arrive at image-wide depth completion. Spatially-aware Shapley values (SaS) succeed in selecting line subsets that yield a depth accuracy comparable to the full lidar input while using just half of the lines.
    
[^46]: 生成型AI（AIGC）完整调研：ChatGPT从GPT-4到GPT-5是你需要的全部吗？

    A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?. (arXiv:2303.11717v1 [cs.AI])

    [http://arxiv.org/abs/2303.11717](http://arxiv.org/abs/2303.11717)

    本文通过完整的调研介绍了生成型AI，包含了从技术到应用的各个方面。ChatGPT虽然是一个有用的工具，但并不足以覆盖所有的AIGC任务，对于实现多样化的内容创造还需要GPT-5等未来的发展。

    

    随着ChatGPT的流行，生成型AI（AIGC，即AI生成的内容）因其分析和创造文本、图像等能力而在各个领域引起轰动。在如此广泛的媒体报导下，几乎不可能错过从特定角度窥探AIGC的机会。在AI从纯粹的分析转向创造的时代，值得注意的是，ChatGPT仅仅是众多AIGC任务中的一个工具，其最新的语言模型GPT-4。许多人对ChatGPT的能力印象深刻，同时也在思考它的局限性：GPT-5（或其他未来的GPT变种）是否能帮助ChatGPT统一各种AIGC任务，实现多样化的内容创造？为了回答这个问题，需要进行现有AIGC任务的全面审查。因此，我们的工作迅速填补了这一空白，首次介绍了AIGC，涵盖了从技术到应用的所有方面。现代的生成型AI依赖于各种技术基础，包括模型架构等。

    As ChatGPT goes viral, generative AI (AIGC, a.k.a AI-generated content) has made headlines everywhere because of its ability to analyze and create text, images, and beyond. With such overwhelming media coverage, it is almost impossible for us to miss the opportunity to glimpse AIGC from a certain angle. In the era of AI transitioning from pure analysis to creation, it is worth noting that ChatGPT, with its most recent language model GPT-4, is just a tool out of numerous AIGC tasks. Impressed by the capability of the ChatGPT, many people are wondering about its limits: can GPT-5 (or other future GPT variants) help ChatGPT unify all AIGC tasks for diversified content creation? Toward answering this question, a comprehensive review of existing AIGC tasks is needed. As such, our work comes to fill this gap promptly by offering a first look at AIGC, ranging from its techniques to applications. Modern generative AI relies on various technical foundations, ranging from model architecture and 
    
[^47]: 基于量子退火的多类SVM一步分类研究应用于遥感数据分类

    A Single-Step Multiclass SVM based on Quantum Annealing for Remote Sensing Data Classification. (arXiv:2303.11705v1 [cs.LG])

    [http://arxiv.org/abs/2303.11705](http://arxiv.org/abs/2303.11705)

    本研究提出了一种新的基于量子退火的直接多类分类的量子SVM(QMSVM)，可以快速而准确地对遥感数据进行分类。

    

    近年来，量子退火器的发展促进了实验演示并增加了对量子退火应用的研究兴趣，例如在量子机器学习中以及特别是流行的量子SVM中。已经提出了量子SVM的几个版本，并已证明量子退火在其中是有效的。也已经进行了扩展到多类问题，包括多个二进制分类器的集成。本文提出了一种新的基于量子退火的直接多类分类的量子SVM(QMSVM)。多类分类问题被公式化为单个二次无约束二进制优化(QUBO)问题，并用量子退火求解。本文的主要目标是评估这种方法的可行性、准确性和时间性能。在D-Wave Advantage量子退火器上进行了分类的实验。

    In recent years, the development of quantum annealers has enabled experimental demonstrations and has increased research interest in applications of quantum annealing, such as in quantum machine learning and in particular for the popular quantum SVM. Several versions of the quantum SVM have been proposed, and quantum annealing has been shown to be effective in them. Extensions to multiclass problems have also been made, which consist of an ensemble of multiple binary classifiers. This work proposes a novel quantum SVM formulation for direct multiclass classification based on quantum annealing, called Quantum Multiclass SVM (QMSVM). The multiclass classification problem is formulated as a single Quadratic Unconstrained Binary Optimization (QUBO) problem solved with quantum annealing. The main objective of this work is to evaluate the feasibility, accuracy, and time performance of this approach. Experiments have been performed on the D-Wave Advantage quantum annealer for a classification
    
[^48]: 连接生成半监督学习和生成开放集识别

    Linking generative semi-supervised learning and generative open-set recognition. (arXiv:2303.11702v1 [cs.CV])

    [http://arxiv.org/abs/2303.11702](http://arxiv.org/abs/2303.11702)

    本研究旨在探究生成半监督学习和生成开放集识别之间的关系。SSL-GANs和OSR-GANs方法的相似性在于都要求生成器在互补空间中产生样本，并通过正则化来推广开放空间。研究结果表明SSL优化边缘-GAN在结合SSL-OSR任务方面树立新的标准，但在某些OSR任务中OSR优化的ARP-GAN仍然略优于SSL-GAN。

    

    本研究在生成对抗网络（GANs）的背景下，探究了半监督学习（SSL）和开放集识别（OSR）之间的关系。尽管以前没有正式将SSL和OSR联系起来的研究，但它们各自的方法有惊人的相似之处。具体而言，SSL-GAN和OSR-GAN要求生成器在互补空间中产生样本。随后，通过对生成样本进行正则化，SSL和OSR分类器都可以完全识别开放空间。为了证明SSL和OSR之间的关联，我们在理论上和实验上比较了最先进的SSL-GAN方法和最先进的OSR-GAN方法。结果表明，文献基础更加牢固的SSL优化边缘-GAN在结合SSL-OSR任务方面树立新的标准，并在某些一般的OSR实验中取得了新的最先进的结果。然而，OSR优化的对抗性互惠点（ARP）-GAN在一些OSR任务中仍然略优于SSL-GAN。

    This study investigates the relationship between semi-supervised learning (SSL) and open-set recognition (OSR) in the context of generative adversarial networks (GANs). Although no previous study has formally linked SSL and OSR, their respective methods share striking similarities. Specifically, SSL-GANs and OSR-GANs require generator to produce samples in the complementary space. Subsequently, by regularising networks with generated samples, both SSL and OSR classifiers generalize the open space. To demonstrate the connection between SSL and OSR, we theoretically and experimentally compare state-of-the-art SSL-GAN methods with state-of-the-art OSR-GAN methods. Our results indicate that the SSL optimised margin-GANs, which have a stronger foundation in literature, set the new standard for the combined SSL-OSR task and achieves new state-of-other art results in certain general OSR experiments. However, the OSR optimised adversarial reciprocal point (ARP)-GANs still slightly out-performe
    
[^49]: 一种面向轻量级单图像超分辨率的高频聚焦网络

    A High-Frequency Focused Network for Lightweight Single Image Super-Resolution. (arXiv:2303.11701v1 [eess.IV])

    [http://arxiv.org/abs/2303.11701](http://arxiv.org/abs/2303.11701)

    本文提出一种名为高频聚焦网络的模型，通过高频聚焦块（HFFB）实现对高频信息的有选择性增强，从而提高单图像超分辨率的重建效果。

    

    近年来，轻量级神经网络在单图像超分辨率（SISR）任务方面取得了重大突破。与低频信息相比，高频细节的重建更加困难。大多数SISR模型为低频和高频信息分配相同的计算资源，这导致了对简单低频信息的冗余处理以及对更具挑战性的高频信息恢复不足。我们通过高频聚焦块（HFFB）提出了一种新颖的高频聚焦网络（HFFN），从而选择性地增强高频信息，同时最小化低频信息的冗余特征计算。HFFB有效地将更多的计算资源分配给更具挑战性的高频信息重建。此外，我们提出了一种本地特征融合块（LFFB），在本地区域内有效地融合了多个HFFB的特征，利用了互补信息。

    Lightweight neural networks for single-image super-resolution (SISR) tasks have made substantial breakthroughs in recent years. Compared to low-frequency information, high-frequency detail is much more difficult to reconstruct. Most SISR models allocate equal computational resources for low-frequency and high-frequency information, which leads to redundant processing of simple low-frequency information and inadequate recovery of more challenging high-frequency information. We propose a novel High-Frequency Focused Network (HFFN) through High-Frequency Focused Blocks (HFFBs) that selectively enhance high-frequency information while minimizing redundant feature computation of low-frequency information. The HFFB effectively allocates more computational resources to the more challenging reconstruction of high-frequency information. Moreover, we propose a Local Feature Fusion Block (LFFB) effectively fuses features from multiple HFFBs in a local region, utilizing complementary information a
    
[^50]: 在合成晶体数据集上训练的神经网络可以从ICSD粉末X射线衍射图中提取结构信息

    Neural networks trained on synthetically generated crystals can extract structural information from ICSD powder X-ray diffractograms. (arXiv:2303.11699v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2303.11699](http://arxiv.org/abs/2303.11699)

    本研究提出了一种使用合成晶体数据集，通过神经网络从ICSD粉末X射线衍射图中提取结构信息的方法，并且在空间群分类任务上，取得了比直接在ICSD晶体上进行训练更高的准确度。

    

    机器学习技术已成功地应用于从粉末X射线衍射图中提取结构信息，如晶体空间群。然而，直接在ICSD等数据库的模拟衍射图上进行训练存在挑战，原因是其规模有限、类别不均匀并且偏向某些结构类型。我们提出了一种替代方法，即通过利用每个空间群的对称操作，在随机坐标下生成合成晶体。基于这种方法，我们演示了使用深度ResNet类模型进行在线训练，每小时最多可生成少量百万个唯一的合成衍射图。针对我们选择的空间群分类任务，在大多数空间群的未见ICSD结构类型上，我们实现了79.9%的测试准确性。这超过了直接在ICSD晶体上进行训练的当前最先进方法的56.1%准确性。我们的结果证明合成晶体数据集可用于通过神经网络提取ICSD粉末X射线衍射图中的结构信息。

    Machine learning techniques have successfully been used to extract structural information such as the crystal space group from powder X-ray diffractograms. However, training directly on simulated diffractograms from databases such as the ICSD is challenging due to its limited size, class-inhomogeneity, and bias toward certain structure types. We propose an alternative approach of generating synthetic crystals with random coordinates by using the symmetry operations of each space group. Based on this approach, we demonstrate online training of deep ResNet-like models on up to a few million unique on-the-fly generated synthetic diffractograms per hour. For our chosen task of space group classification, we achieved a test accuracy of 79.9% on unseen ICSD structure types from most space groups. This surpasses the 56.1% accuracy of the current state-of-the-art approach of training on ICSD crystals directly. Our results demonstrate that synthetically generated crystals can be used to extract
    
[^51]: 标签增强的数据增强方法

    Data Augmentation For Label Enhancement. (arXiv:2303.11698v1 [cs.LG])

    [http://arxiv.org/abs/2303.11698](http://arxiv.org/abs/2303.11698)

    该论文提出了一种数据增强方法用于标签增强。该方法利用特征空间的拓扑信息生成更准确的标签置信度，并采用新的非线性LE模型来显著提高标签增强的准确性。

    

    标签分布（LD）使用描述程度来描述实例，在处理标签模糊的情况下提供了更加细粒度的监督信息。然而，在许多实际应用中，LD是不可用的。为了获取LD，标签增强（LE）被提出来从逻辑标签中恢复LD。现有的LE方法存在以下问题：（i）它们使用逻辑标签来训练到LD的映射，但是监督信息太宽松，可能导致模型预测不准确；（ii）它们忽略了特征的冗余性，并直接使用收集的特征。为了解决（i），我们使用特征空间的拓扑结构生成更准确的标签置信度。为了解决（ii），我们提出了一种新的监督式LE降维方法，它将原始数据投影到较低维的特征空间中。将以上两种方法结合起来，我们获得了用于LE的增强数据。此外，我们还提出了一种基于变换学习的新型非线性LE模型，显著提高了标签增强的准确性。在基准数据集上的实验结果表明，我们提出的方法在准确性和效率方面优于现有的最先进方法。

    Label distribution (LD) uses the description degree to describe instances, which provides more fine-grained supervision information when learning with label ambiguity. Nevertheless, LD is unavailable in many real-world applications. To obtain LD, label enhancement (LE) has emerged to recover LD from logical label. Existing LE approach have the following problems: (\textbf{i}) They use logical label to train mappings to LD, but the supervision information is too loose, which can lead to inaccurate model prediction; (\textbf{ii}) They ignore feature redundancy and use the collected features directly. To solve (\textbf{i}), we use the topology of the feature space to generate more accurate label-confidence. To solve (\textbf{ii}), we proposed a novel supervised LE dimensionality reduction approach, which projects the original data into a lower dimensional feature space. Combining the above two, we obtain the augmented data for LE. Further, we proposed a novel nonlinear LE model based on t
    
[^52]: 基于转录组学和深度学习的药物和疾病匹配研究

    Transcriptomics-based matching of drugs to diseases with deep learning. (arXiv:2303.11695v1 [q-bio.GN])

    [http://arxiv.org/abs/2303.11695](http://arxiv.org/abs/2303.11695)

    本文通过使用深度学习方法，基于转录组学进行无假设的药物和疾病匹配研究。研究结果显示，相比于其他转录组学匹配方法，我们提出的方法具备更高的性能。此外，我们的模型还能够捕捉不同基因表达之间的相互作用，为新药开发提供了新思路。

    

    本文提出了一种深度学习方法，用于进行无假设的基于转录组学的药物和疾病匹配研究。我们提出的神经网络架构是在批准的药物 - 疾病适应症方面进行训练的，以相关的疾病和药物差异基因表达谱作为输入，并学习识别新的适应症。我们组装了一个包含68种疾病的疾病 - 药物适应症评估数据集，并在计算机模拟中将我们的方法与最广泛使用的转录组学匹配基线CMap和Characteristic Direction进行了评估。我们的结果显示，在标准检索度量方面，我们的方法比这两个基线提高了200%以上。我们进一步展示了我们的模型捕获了不同基因表达之间的相互作用，包括药物和疾病。我们提供了我们训练的模型，数据和代码供学者在 https://github.com/healx/dgem-nn-public 上使用。

    In this work we present a deep learning approach to conduct hypothesis-free, transcriptomics-based matching of drugs for diseases. Our proposed neural network architecture is trained on approved drug-disease indications, taking as input the relevant disease and drug differential gene expression profiles, and learns to identify novel indications. We assemble an evaluation dataset of disease-drug indications spanning 68 diseases and evaluate in silico our approach against the most widely used transcriptomics-based matching baselines, CMap and the Characteristic Direction. Our results show a more than 200% improvement over both baselines in terms of standard retrieval metrics. We further showcase our model's ability to capture different genes' expressions interactions among drugs and diseases. We provide our trained models, data and code to predict with them at https://github.com/healx/dgem-nn-public.
    
[^53]: ALOFT：一种轻量化的类MLP架构，配合动态低频变换用于域泛化

    ALOFT: A Lightweight MLP-like Architecture with Dynamic Low-frequency Transform for Domain Generalization. (arXiv:2303.11674v1 [cs.CV])

    [http://arxiv.org/abs/2303.11674](http://arxiv.org/abs/2303.11674)

    本文介绍了一种轻量级的类MLP架构ALOFT，它可使用动态低频变换用于域泛化。与CNN相比，该架构更好地捕捉全局表示，因此具有更好的泛化能力。

    

    域泛化旨在学习一个模型，它在不重新训练的情况下利用多个源域来很好地推广到看不见的目标域。大多数现有的域泛化工作都基于卷积神经网络（CNN）。然而，卷积核的局部操作使得模型过于关注局部表示（例如纹理），这从本质上使得模型更容易过拟合源域并阻碍其泛化能力。最近，几种基于MLP的方法通过学习图像不同块之间的全局交互，在监督学习任务中取得了有希望的结果。本文在此受到启发，首先分析了CNN和MLP方法在DG中的差异，并发现MLP方法表现出更好的泛化能力，因为它们可以比CNN方法更好地捕捉全局表示（例如结构）。然后，基于最近的一种轻量级MLP方法，我们获得了一个强大的基准线，其性能优于大多数统计学方法。

    Domain generalization (DG) aims to learn a model that generalizes well to unseen target domains utilizing multiple source domains without re-training. Most existing DG works are based on convolutional neural networks (CNNs). However, the local operation of the convolution kernel makes the model focus too much on local representations (e.g., texture), which inherently causes the model more prone to overfit to the source domains and hampers its generalization ability. Recently, several MLP-based methods have achieved promising results in supervised learning tasks by learning global interactions among different patches of the image. Inspired by this, in this paper, we first analyze the difference between CNN and MLP methods in DG and find that MLP methods exhibit a better generalization ability because they can better capture the global representations (e.g., structure) than CNN methods. Then, based on a recent lightweight MLP method, we obtain a strong baseline that outperforms most stat
    
[^54]: 联邦学习中类别不平衡问题的调查

    A Survey on Class Imbalance in Federated Learning. (arXiv:2303.11673v1 [cs.LG])

    [http://arxiv.org/abs/2303.11673](http://arxiv.org/abs/2303.11673)

    本文对联邦学习中的类别不平衡问题和解决方案进行了调查和总结，包括平衡感知优化、数据增强和客户端加权等。目前仍存在着一些问题需要解决。

    

    联邦学习是一种新兴的分布式学习技术，允许网络中的多个客户端设备在不直接暴露客户数据的情况下共同训练一个机器学习模型，由于其隐私保护的性质而备受关注。然而，在联邦学习中训练的模型通常表现不如在标准集中式学习模式下训练的模型，特别是在训练数据不平衡的情况下。在联邦学习中，数据失衡可能在单个客户端设备上发生，也可能在许多设备上全局发生。不同类型数据不平衡的复杂性给联邦学习技术的发展带来了挑战，尤其是考虑到需要同时缓解数据不平衡问题和保护数据隐私。因此，许多尝试已经在文献中对联邦学习中的类别不平衡进行了处理。本文对这一问题的最新进展进行了详细回顾，包括平衡感知优化、数据增强和客户端加权。我们还讨论了解决联邦学习中类别不平衡问题的开放问题和潜在研究方向。

    Federated learning, which allows multiple client devices in a network to jointly train a machine learning model without direct exposure of clients' data, is an emerging distributed learning technique due to its nature of privacy preservation. However, it has been found that models trained with federated learning usually have worse performance than their counterparts trained in the standard centralized learning mode, especially when the training data is imbalanced. In the context of federated learning, data imbalance may occur either locally one one client device, or globally across many devices. The complexity of different types of data imbalance has posed challenges to the development of federated learning technique, especially considering the need of relieving data imbalance issue and preserving data privacy at the same time. Therefore, in the literature, many attempts have been made to handle class imbalance in federated learning. In this paper, we present a detailed review of recen
    
[^55]: 通用平滑得分函数用于生成模型

    Universal Smoothed Score Functions for Generative Modeling. (arXiv:2303.11669v1 [stat.ML])

    [http://arxiv.org/abs/2303.11669](http://arxiv.org/abs/2303.11669)

    本文提出了通用平滑得分函数用于生成模型，通过导出其参数化的通用形式，详细描述了学习平滑后的M-密度的时间复杂度。同时，使用M个独立高斯通道的因子核对未知的兴趣密度进行平滑，并评估了其在CIFAR-10数据集上的表现。

    

    本文探讨基于用具有等噪声级的M个独立高斯通道的因子核平滑未知$\mathbb{R}^d$兴趣密度的生成建模问题，首先通过导出其参数化的通用形式，完整描述了在$\mathbb{R}^{Md}$中学习平滑后的密度的时间复杂度（称为M-密度），并因为其构造方式而具有排列不变性；接着通过分析该类高斯分布的条件数，研究了M-密度抽样的时间复杂度，得到了随着$M$的增加而“形状”变化的几何观察结论；最后，在CIFAR-10数据集上针对这类生成模型的样本质量进行了呈现自由创造距离（14.15）结果的实验，尤其在长时间混合的MCMC链上仅使用单个噪声程序的情况下获得了值得注意的结果。

    We consider the problem of generative modeling based on smoothing an unknown density of interest in $\mathbb{R}^d$ using factorial kernels with $M$ independent Gaussian channels with equal noise levels introduced by Saremi and Srivastava (2022). First, we fully characterize the time complexity of learning the resulting smoothed density in $\mathbb{R}^{Md}$, called M-density, by deriving a universal form for its parametrization in which the score function is by construction permutation equivariant. Next, we study the time complexity of sampling an M-density by analyzing its condition number for Gaussian distributions. This spectral analysis gives a geometric insight on the "shape" of M-densities as one increases $M$. Finally, we present results on the sample quality in this class of generative models on the CIFAR-10 dataset where we report Fr\'echet inception distances (14.15), notably obtained with a single noise level on long-run fast-mixing MCMC chains.
    
[^56]: 带有相关数据序列的学习的统一风险界限

    Uniform Risk Bounds for Learning with Dependent Data Sequences. (arXiv:2303.11650v1 [cs.LG])

    [http://arxiv.org/abs/2303.11650](http://arxiv.org/abs/2303.11650)

    本文针对相关数据序列的学习，不依赖混合参数或者连续带参照的复杂度度量，展示了统一的风险界限，并提出了可应用于场景优化的相关约束随机程序的样本复杂度计算方法。

    

    本文将独立数据的学习理论推广到了相关数据序列的情况。与大多数文献不同的是，我们不依赖于混合参数或者连续带参照的复杂度度量，而是采用了经典的证明模式和容量度量来推导统一的风险界限。特别地，我们展示了标准分类风险最小化理论在相关数据情况下的VC维界限。此外，我们还提供了基于R算子的复杂度界限，其与独立同分布情况下标准的界限没有变化。最后，我们展示了如何将这些结果应用于场景优化的情形，从而计算出具有相关约束的随机程序的样本复杂度。

    This paper extends standard results from learning theory with independent data to sequences of dependent data. Contrary to most of the literature, we do not rely on mixing arguments or sequential measures of complexity and derive uniform risk bounds with classical proof patterns and capacity measures. In particular, we show that the standard classification risk bounds based on the VC-dimension hold in the exact same form for dependent data, and further provide Rademacher complexity-based bounds, that remain unchanged compared to the standard results for the identically and independently distributed case. Finally, we show how to apply these results in the context of scenario-based optimization in order to compute the sample complexity of random programs with dependent constraints.
    
[^57]: 你用$\texttt{tGLAD}$了吗？时间会告诉你！

    Are uGLAD? Time will tell!. (arXiv:2303.11647v1 [cs.LG])

    [http://arxiv.org/abs/2303.11647](http://arxiv.org/abs/2303.11647)

    本文提出了一种使用条件独立图的多元时间序列分割方法$\texttt{tGLAD}$，可以有效地标识时间序列的有意义的段。

    

    我们经常遇到周围存在多个相关的时间序列，例如用于检查脑活动变化的脑电图数据或用于监测身体运动的传感器。多元时间序列数据的分割是一种识别时间序列中含义的模式或变化的技术，这可以标志系统行为的变化。但是，大多数分割算法主要设计用于单变量时间序列，它们在多元数据上的表现仍然不令人满意，这是一个具有挑战性的问题。在这项工作中，我们介绍了一种使用条件独立（CI）图的多元时间序列分割的新方法。CI图是表示节点之间的偏相关关系的概率图模型。我们提出了一个领域不可知的多元分割框架“$\texttt{tGLAD}$”，它将CI图节点和时间序列的变量进行并行。在CI图上应用图形结构学习算法可以标识时间序列的有意义的段。我们在合成和实际多元时间序列数据集上展示了我们方法的有效性。

    We frequently encounter multiple series that are temporally correlated in our surroundings, such as EEG data to examine alterations in brain activity or sensors to monitor body movements. Segmentation of multivariate time series data is a technique for identifying meaningful patterns or changes in the time series that can signal a shift in the system's behavior. However, most segmentation algorithms have been designed primarily for univariate time series, and their performance on multivariate data remains largely unsatisfactory, making this a challenging problem. In this work, we introduce a novel approach for multivariate time series segmentation using conditional independence (CI) graphs. CI graphs are probabilistic graphical models that represents the partial correlations between the nodes. We propose a domain agnostic multivariate segmentation framework `$\texttt{tGLAD}$' which draws a parallel between the CI graph nodes and the variables of the time series. Consider applying a gra
    
[^58]: 利用转移学习来进行属性推断的研究

    Manipulating Transfer Learning for Property Inference. (arXiv:2303.11643v1 [cs.LG])

    [http://arxiv.org/abs/2303.11643](http://arxiv.org/abs/2303.11643)

    本文研究了转移学习中的属性推断攻击，攻击者可以操纵上游模型，对受害者调整的下游模型进行高效且特定的推断攻击，需要注意和防范此类攻击。

    

    转移学习是一种常用的方法，用于利用有限的数据和计算资源来调整预训练的（上游）模型，用于不同的下游任务。我们研究了一个拥有对用于转移学习中的上游模型进行控制的对手如何对受害者调整的下游模型进行属性推断攻击。我们展示了攻击的情况，即对手可以操纵上游模型进行高效且特定的属性推断攻击（AUC得分>0.9），而不会在主要任务中产生显著的性能损失。操纵的主要思想是使上游模型为具有目标属性的样本生成具有不同分布的激活（中间特征），从而使对手能够轻松区分训练有具有目标属性的样本和没有的样本。我们的代码和实验基于使用卷积神经网络作为上游模型的面部识别任务和ColorFeret数据集作为下游模型的训练集。我们的结果表明，从业者在使用转移学习时需要注意属性推断攻击的可能性，并采取措施来防止此类攻击。

    Transfer learning is a popular method for tuning pretrained (upstream) models for different downstream tasks using limited data and computational resources. We study how an adversary with control over an upstream model used in transfer learning can conduct property inference attacks on a victim's tuned downstream model. For example, to infer the presence of images of a specific individual in the downstream training set. We demonstrate attacks in which an adversary can manipulate the upstream model to conduct highly effective and specific property inference attacks (AUC score $> 0.9$), without incurring significant performance loss on the main task. The main idea of the manipulation is to make the upstream model generate activations (intermediate features) with different distributions for samples with and without a target property, thus enabling the adversary to distinguish easily between downstream models trained with and without training examples that have the target property. Our cod
    
[^59]: 自主驾驶的基于深度 Q-Network 的决策制定方法

    Deep Q-Network Based Decision Making for Autonomous Driving. (arXiv:2303.11634v1 [cs.RO])

    [http://arxiv.org/abs/2303.11634](http://arxiv.org/abs/2303.11634)

    本文介绍了一种将深度 Q-Network 和控制理论见解结合起来，在高速公路场景中安全导航自主车辆的方法，该方法可以产生高效且安全的驾驶行为。

    

    目前，自主驾驶中的决策制定是面临的最大挑战之一。本文介绍了一种将深度 Q-Network 和控制理论见解结合起来，在高速公路场景中安全导航自主车辆的方法。在模拟环境中训练深度 Q-Network 作为一个中央决策单元，提出轨迹规划器的目标。基于所生成的轨迹和纵向运动的控制器，实现了车道变更操作。为了证明这种方法的功能性，本文在两种不同的高速公路交通场景下进行了评估，并分析了不同状态表示对其性能和训练过程的影响。结果表明，该系统可以产生高效且安全的驾驶行为。

    Currently decision making is one of the biggest challenges in autonomous driving. This paper introduces a method for safely navigating an autonomous vehicle in highway scenarios by combining deep Q-Networks and insight from control theory. A Deep Q-Network is trained in simulation to serve as a central decision-making unit by proposing targets for a trajectory planner. The generated trajectories in combination with a controller for longitudinal movement are used to execute lane change maneuvers. In order to prove the functionality of this approach it is evaluated on two different highway traffic scenarios. Furthermore, the impact of different state representations on the performance and training process is analyzed. The results show that the proposed system can produce efficient and safe driving behavior.
    
[^60]: 一种极其简单的芯片特征提取和缺陷模式识别方法

    An Embarrassingly Simple Approach for Wafer Feature Extraction and Defect Pattern Recognition. (arXiv:2303.11632v1 [cs.CV])

    [http://arxiv.org/abs/2303.11632](http://arxiv.org/abs/2303.11632)

    本文提出了一种非常简单但有效的从晶圆图像中提取特征的技术，其速度快，直观且可解释，在缺陷模式识别方面的表现优于传统的深度学习模型。

    

    在制造过程中，识别芯片图中的缺陷模式对于找到潜在问题的根本原因并提高晶圆的产量至关重要。目前使用深度神经网络来识别这些缺陷模式。这些方法通常非常庞大，并具有显着的推理时间。它们还需要GPU支持才能有效运作。所有这些问题使这些模型不适合于制造晶圆时的在线预测。在本文中，我们提出了一种极其简单但有效的方法来从晶圆图像中提取特征。提出的方法极其快速，直观，非参数化且可解释。实验结果表明，我们提出的流程优于传统的深度学习模型。我们的特征提取不需要训练或微调，同时保留了数据点的相对形状和位置，如我们的可解释性分析所揭示的那样。

    Identifying defect patterns in a wafer map during manufacturing is crucial to find the root cause of the underlying issue and provides valuable insights on improving yield in the foundry. Currently used methods use deep neural networks to identify the defects. These methods are generally very huge and have significant inference time. They also require GPU support to efficiently operate. All these issues make these models not fit for on-line prediction in the manufacturing foundry. In this paper, we propose an extremely simple yet effective technique to extract features from wafer images. The proposed method is extremely fast, intuitive, and non-parametric while being explainable. The experiment results show that the proposed pipeline outperforms conventional deep learning models. Our feature extraction requires no training or fine-tuning while preserving the relative shape and location of data points as revealed by our interpretability analysis.
    
[^61]: 面向持续环境的评估者指导学习

    Assessor-Guided Learning for Continual Environments. (arXiv:2303.11624v1 [cs.LG])

    [http://arxiv.org/abs/2303.11624](http://arxiv.org/abs/2303.11624)

    本文提出了一种评估者指导学习策略，用于持续学习，其中评估者通过控制学习方向和速度来指导学习过程，以有效地学习新环境并避免灾难性干扰问题的发生。

    

    本文提出了一种评估者指导学习策略，用于持续学习，其中一个评估者通过控制学习过程的方向和速度来指导基础学习者的学习过程，从而有效地学习新环境并防止灾难性干扰问题的发生。评估者以元学习的方式进行训练，其元目标是增强基础学习者的学习过程。它执行每个样本的软加权机制，接受正样本并拒绝负样本。基础学习者的训练目标是最小化交叉熵损失函数、暗体验重放（DER）损失函数和知识蒸馏损失函数的元加权组合，这些交互以获得更好的性能。

    This paper proposes an assessor-guided learning strategy for continual learning where an assessor guides the learning process of a base learner by controlling the direction and pace of the learning process thus allowing an efficient learning of new environments while protecting against the catastrophic interference problem. The assessor is trained in a meta-learning manner with a meta-objective to boost the learning process of the base learner. It performs a soft-weighting mechanism of every sample accepting positive samples while rejecting negative samples. The training objective of a base learner is to minimize a meta-weighted combination of the cross entropy loss function, the dark experience replay (DER) loss function and the knowledge distillation loss function whose interactions are controlled in such a way to attain an improved performance. A compensated over-sampling (COS) strategy is developed to overcome the class imbalanced problem of the episodic memory due to limited memor
    
[^62]: 求解和积多项式和实对数规范阈值的炸裂算法

    Blow-up Algorithm for Sum-of-Products Polynomials and Real Log Canonical Thresholds. (arXiv:2303.11619v1 [math.ST])

    [http://arxiv.org/abs/2303.11619](http://arxiv.org/abs/2303.11619)

    本文研究了和积多项式的炸裂算法和其 RLCT。

    

    在考虑给出贝叶斯广义误差的实对数规范阈值时，论文用稍微简单的多项式替换平均误差函数，其 RLCT 对应于平均误差函数的 RLCT，并通过称为炸裂的代数操作解决其奇点来获得其 RLCT。虽然众所周知，任何多项式的奇点都可以通过有限次的炸裂迭代来解决，但并没有明确是否可以通过应用特定的炸裂算法来解决特定多项式的奇点。因此，本文考虑了称为和积多项式的多项式的炸裂算法及其 RLCT。

    When considering a real log canonical threshold (RLCT) that gives a Bayesian generalization error, in general, papers replace a mean error function with a relatively simple polynomial whose RLCT corresponds to that of the mean error function, and obtain its RLCT by resolving its singularities through an algebraic operation called blow-up. Though it is known that the singularities of any polynomial can be resolved by a finite number of blow-up iterations, it is not clarified whether or not it is possible to resolve singularities of a specific polynomial by applying a specific blow-up algorithm. Therefore this paper considers the blow-up algorithm for the polynomials called sum-of-products (sop) polynomials and its RLCT.
    
[^63]: 带应用于变分蒙特卡罗模拟的参数化球的随机梯度下降的收敛性分析。

    Convergence of stochastic gradient descent on parameterized sphere with applications to variational Monte Carlo simulation. (arXiv:2303.11602v1 [cs.LG])

    [http://arxiv.org/abs/2303.11602](http://arxiv.org/abs/2303.11602)

    本论文在高维度球体上，通过神经网络参数化，使用SGD算法在监督学习和无监督学习中，提供了一种新算法，并且证明了其收敛性。

    

    本文分析了在由神经网络参数化为常数倍的高维球上使用随机梯度下降（SGD）类型算法。我们为有监督学习提供了一种新算法，并在理论和数值上证明了其收敛性。我们还首次提供了无监督设置的收敛证明，该设置对应于量子物理学中广泛使用的变分蒙特卡罗（VMC）方法。

    We analyze stochastic gradient descent (SGD) type algorithms on a high-dimensional sphere which is parameterized by a neural network up to a normalization constant. We provide a new algorithm for the setting of supervised learning and show its convergence both theoretically and numerically. We also provide the first proof of convergence for the unsupervised setting, which corresponds to the widely used variational Monte Carlo (VMC) method in quantum physics.
    
[^64]: 应用SMILES序列的Transformer模型在学习手性时存在困难

    Difficulty in learning chirality for Transformer fed with SMILES. (arXiv:2303.11593v1 [cs.LG])

    [http://arxiv.org/abs/2303.11593](http://arxiv.org/abs/2303.11593)

    应用SMILES序列的Transformer模型在学习分子结构的整体性和手性方面存在困难，需要进行长时间的训练。生成的描述符用于分子性质预测时的准确率从开始到训练结束都是相似的。

    

    近年来，基于对极其多样的分子进行表示学习的描述符生成已经得到了发展，特别是那些将自然语言处理（NLP）模型应用于SMILES，即分子结构的文字表示的模型。然而，关于这些模型如何理解化学结构的研究很少。为了解决这个问题，我们调查了一种代表性的NLP模型——Transformer，在学习SMILES和化学结构之间的关系。结果表明，虽然Transformer快速学习分子的部分结构，但需要进行长时间的训练才能理解整体结构。与之一致的是，在不同的学习步骤中生成的描述符用于分子性质预测时的准确率从开始到训练结束都是相似的。此外，我们发现Transformer需要特别长的训练时间才能学习手性，并且有时会出现低翻译准确率的停滞现象。

    Recent years have seen development of descriptor generation based on representation learning of extremely diverse molecules, especially those that apply natural language processing (NLP) models to SMILES, a literal representation of molecular structure. However, little research has been done on how these models understand chemical structure. To address this, we investigated the relationship between the learning progress of SMILES and chemical structure using a representative NLP model, the Transformer. The results suggest that while the Transformer learns partial structures of molecules quickly, it requires extended training to understand overall structures. Consistently, the accuracy of molecular property predictions using descriptors generated from models at different learning steps was similar from the beginning to the end of training. Furthermore, we found that the Transformer requires particularly long training to learn chirality and sometimes stagnates with low translation accura
    
[^65]: 大规模适应性实验：灵活批处理的贝叶斯算法

    Adaptive Experimentation at Scale: Bayesian Algorithms for Flexible Batches. (arXiv:2303.11582v1 [cs.LG])

    [http://arxiv.org/abs/2303.11582](http://arxiv.org/abs/2303.11582)

    本文提出了一个基于贝叶斯算法的自适应实验框架，可灵活处理任何批处理大小。通过正态近似指导可扩展自适应设计，采用残余时限优化选择采样分配，实现了最先进的性能。

    

    标准的贝叶斯算法假定持续重新分配测量工作，这在实现过程中存在延迟反馈和基础设施/组织难题等挑战。本文针对仅有少数重新分配阶段的实际情况，其中测量结果是以批处理形式测量的，提出了一种新的适应性实验框架，可灵活处理任何批处理大小。我们的主要观察是，在统计推断中普遍使用的正态近似也可以指导可扩展自适应设计。通过推导渐进顺序实验，我们制定了一种动态规划，可以利用平均回报的先验信息。动态规划的状态转移相对于采样分配是可微的，允许使用基于梯度的方法进行规划和策略优化。我们提出了一种简单的迭代规划方法，即残余时限优化，通过优化平衡探索和利用的规划目标来选择采样分配。在合成和真实世界基准测试问题上的实验结果表明，我们的框架实现了最先进的性能，同时具有模块化和易用性。

    Standard bandit algorithms that assume continual reallocation of measurement effort are challenging to implement due to delayed feedback and infrastructural/organizational difficulties. Motivated by practical instances involving a handful of reallocation epochs in which outcomes are measured in batches, we develop a new adaptive experimentation framework that can flexibly handle any batch size. Our main observation is that normal approximations universal in statistical inference can also guide the design of scalable adaptive designs. By deriving an asymptotic sequential experiment, we formulate a dynamic program that can leverage prior information on average rewards. State transitions of the dynamic program are differentiable with respect to the sampling allocations, allowing the use of gradient-based methods for planning and policy optimization. We propose a simple iterative planning method, Residual Horizon Optimization, which selects sampling allocations by optimizing a planning obj
    
[^66]: 基于表格数据的高效多级推断

    Efficient Multi-stage Inference on Tabular Data. (arXiv:2303.11580v1 [cs.LG])

    [http://arxiv.org/abs/2303.11580](http://arxiv.org/abs/2303.11580)

    该论文通过将推断算法简化并嵌入产品代码中，以减少网络通信，在处理表格数据的实时平台上可将推断延迟降低1.3倍，CPU资源减少30％，并将应用程序前端和后端之间的网络通信减少60％。

    

    许多机器学习应用和产品通过中等数量的输入数据进行训练，但在实时推断时被瓶颈所困。传统智慧在实现机器学习系统时，倾向于将ML代码分割成服务，并通过远程过程调用（RPC）API被产品代码查询。这种方法澄清了整体软件架构，并通过抽象ML内部简化了产品代码。然而，这种分离增加了网络延迟，并带来了额外的CPU开销。因此，我们简化推断算法并将其嵌入产品代码中，以减少网络通信。针对公共数据集和处理表格数据的高性能实时平台，我们表明通常有超过一半的输入可以适应这种优化，而其余部分可以由原始模型处理。通过将AutoML应用于训练和推断，我们将推断延迟降低了1.3倍，CPU资源减少了30％，应用程序前端和后端之间的网络通信减少了60％。

    Many ML applications and products train on medium amounts of input data but get bottlenecked in real-time inference. When implementing ML systems, conventional wisdom favors segregating ML code into services queried by product code via Remote Procedure Call (RPC) APIs. This approach clarifies the overall software architecture and simplifies product code by abstracting away ML internals. However, the separation adds network latency and entails additional CPU overhead. Hence, we simplify inference algorithms and embed them into the product code to reduce network communication. For public datasets and a high-performance real-time platform that deals with tabular data, we show that over half of the inputs are often amenable to such optimization, while the remainder can be handled by the original model. By applying our optimization with AutoML to both training and inference, we reduce inference latency by 1.3x, CPU resources by 30%, and network communication between application front-end an
    
[^67]: 特征相邻多保真物理学习用于偏微分方程

    Feature-adjacent multi-fidelity physics-informed machine learning for partial differential equations. (arXiv:2303.11577v1 [cs.LG])

    [http://arxiv.org/abs/2303.11577](http://arxiv.org/abs/2303.11577)

    提出了一种基于特征相邻的多保真体系结构，通过共享低保真度和高保真度解决方案的特征空间来减少或消除对高精度数据的依赖，这在解决复杂问题时具有重要意义。

    

    物理学习神经网络已成为求解偏微分方程的备选方法。然而，对于复杂问题，这种网络的训练仍然需要高精度的数据，而这些数据的生成成本可能很高。为了减少甚至消除对高保真数据的依赖，我们提出了一种基于特征空间的多保真体系结构，该空间由低保真度和高保真度解决方案共享。在特征空间中，低精度和高精度解决方案的投影相邻，并通过约束它们的相对距离来实现。特征空间由编码器表示，其映射到原始解空间通过解码器实现。所提出的多保真方法在由偏微分方程描述的定态和非定态问题的正问题和逆问题上进行了验证。

    Physics-informed neural networks have emerged as an alternative method for solving partial differential equations. However, for complex problems, the training of such networks can still require high-fidelity data which can be expensive to generate. To reduce or even eliminate the dependency on high-fidelity data, we propose a novel multi-fidelity architecture which is based on a feature space shared by the low- and high-fidelity solutions. In the feature space, the projections of the low-fidelity and high-fidelity solutions are adjacent by constraining their relative distance. The feature space is represented with an encoder and its mapping to the original solution space is effected through a decoder. The proposed multi-fidelity approach is validated on forward and inverse problems for steady and unsteady problems described by partial differential equations.
    
[^68]: 动态健康嵌入改善患者护理

    Dynamic Healthcare Embeddings for Improving Patient Care. (arXiv:2303.11563v1 [cs.LG])

    [http://arxiv.org/abs/2303.11563](http://arxiv.org/abs/2303.11563)

    DECENT是一种自动编码异构共同演化的动态神经网络，可以从各种数据流中学习患者、医生、房间和药物的异构动态嵌入，用于改善患者护理，如药物推荐、患者风险分层和医院容量管理。

    

    随着医院向自动化和集成他们的计算系统，越来越多的精细化医院运营数据变得可用。这些数据包括医院建筑图纸，患者和医护人员之间的交互日志，处方数据，程序数据以及关于患者入院、出院和转移的数据。这为改善患者护理开辟了许多迷人的可能性。然而，为了利用现成的机器学习软件进行这些任务，我们需要从异构的、动态的数据流中学习涉及实体的结构表示。在这里，我们提出了DECTEN，一种自动编码异构共同演化的动态神经网络，用于从各种数据流中学习患者、医生、房间和药物的异构动态嵌入。这些嵌入基于静态属性和动态行为捕捉医生、房间、患者和药物之间的相似性，并可用于各种预测任务，如药物推荐、患者风险分层和医院容量管理。

    As hospitals move towards automating and integrating their computing systems, more fine-grained hospital operations data are becoming available. These data include hospital architectural drawings, logs of interactions between patients and healthcare professionals, prescription data, procedures data, and data on patient admission, discharge, and transfers. This has opened up many fascinating avenues for healthcare-related prediction tasks for improving patient care. However, in order to leverage off-the-shelf machine learning software for these tasks, one needs to learn structured representations of entities involved from heterogeneous, dynamic data streams. Here, we propose DECENT, an auto-encoding heterogeneous co-evolving dynamic neural network, for learning heterogeneous dynamic embeddings of patients, doctors, rooms, and medications from diverse data streams. These embeddings capture similarities among doctors, rooms, patients, and medications based on static attributes and dynamic
    
[^69]: 动态感知损失函数用于标签噪声学习

    Dynamic-Aware Loss for Learning with Label Noise. (arXiv:2303.11562v1 [cs.LG])

    [http://arxiv.org/abs/2303.11562](http://arxiv.org/abs/2303.11562)

    本文提出一种动态感知损失函数，采用增强拟合能力，逐渐增加鲁棒性的权重来处理标签噪声。在后期阶段，引入自举项，让DNN更加重视容易的样例，证明了这种方法的优越性。

    

    标签噪声对深度神经网络(DNN)构成严重威胁。使用既能拟合又具有鲁棒性的强健损失函数是处理此问题的一种简单而有效的策略。然而，这两个因素之间的静态权衡与DNN学习标签噪声的动态性相矛盾，导致性能不佳。因此，我们提出动态感知损失(DAL)来解决这个问题。考虑到DNN倾向于先学习一般化的模式，然后逐渐过拟合标签噪声，DAL最初增强了拟合能力，然后逐渐增加了鲁棒性的权重。此外，在后期阶段，我们让DNN更加重视容易的样例，这些样例更容易标记为正确的标签，并引入自举项来进一步减少标签噪声的负面影响。详细的理论分析和广泛的实验结果都证明了我们方法的优越性。

    Label noise poses a serious threat to deep neural networks (DNNs). Employing robust loss function which reconciles fitting ability with robustness is a simple but effective strategy to handle this problem. However, the widely-used static trade-off between these two factors contradicts the dynamic nature of DNNs learning with label noise, leading to inferior performance. Therefore, we propose a dynamics-aware loss (DAL) to solve this problem. Considering that DNNs tend to first learn generalized patterns, then gradually overfit label noise, DAL strengthens the fitting ability initially, then gradually increases the weight of robustness. Moreover, at the later stage, we let DNNs put more emphasis on easy examples which are more likely to be correctly labeled than hard ones and introduce a bootstrapping term to further reduce the negative impact of label noise. Both the detailed theoretical analyses and extensive experimental results demonstrate the superiority of our method.
    
[^70]: 动态顶点替换文法

    Dynamic Vertex Replacement Grammars. (arXiv:2303.11553v1 [cs.LG])

    [http://arxiv.org/abs/2303.11553](http://arxiv.org/abs/2303.11553)

    本文提出动态顶点替换文法（DyVeRG），它们提供一种在时间域内更新学习图文法的形式框架，用于生成和预测真实世界的动态图，同时保持了人类可解释性。

    

    上下文无关图文法已经显示出在现实世界中建模结构的惊人能力。然而，图文法缺乏捕捉时变现象的能力，因为产生规则的从左到右的转换不表示时间变化。在本文中，我们描述动态顶点替换文法（DyVeRG），它们在时间域中推广了顶点替换文法，通过为学习图文法更新提供形式框架，以符合其基础数据的修改。我们展示了DyVeRG文法可以从真实世界的动态图中进行学习，并被用于忠实地生成这些图，同时保持可解释性。我们还通过计算DyVeRG曲线生成的德沃尔贝格差异分数（dyvergence scores），展示了它们的预测能力，这是由该框架引出的一种新的图相似度度量。

    Context-free graph grammars have shown a remarkable ability to model structures in real-world relational data. However, graph grammars lack the ability to capture time-changing phenomena since the left-to-right transitions of a production rule do not represent temporal change. In the present work, we describe dynamic vertex-replacement grammars (DyVeRG), which generalize vertex replacement grammars in the time domain by providing a formal framework for updating a learned graph grammar in accordance with modifications to its underlying data. We show that DyVeRG grammars can be learned from, and used to generate, real-world dynamic graphs faithfully while remaining human-interpretable. We also demonstrate their ability to forecast by computing dyvergence scores, a novel graph similarity measurement exposed by this framework.
    
[^71]: ModEFormer：使用变压器进行音视频同步的模态保留嵌入

    ModEFormer: Modality-Preserving Embedding for Audio-Video Synchronization using Transformers. (arXiv:2303.11551v1 [cs.CV])

    [http://arxiv.org/abs/2303.11551](http://arxiv.org/abs/2303.11551)

    ModEFormer使用变压器独立提取音频和视频嵌入，并保留输入流的模态，使用更大的批量和更多的负音频样本进行对比学习，这使得其在检测电视广播和视频会议中的音视频同步方面取得了最先进的性能。

    

    电视广播和视频会议中音视频不同步是一个常见的问题，会导致观看体验不佳。一种广泛接受的范式是创建一个误差检测机制来识别音频超前或滞后的情况。我们提出了ModEFormer，通过模态特定的变压器独立提取音频和视频嵌入。与其他基于变压器的方法不同的是，ModEFormer保留输入流的模态，这使我们可以使用更大的批量和更多的负音频样本进行对比学习。此外，我们提出了批处理中负样本数量和唯一样本数量之间的权衡，以明显超过以前方法的性能。实验结果表明，ModEFormer实现了最先进的性能，对于LRS2和LRS3分别达到了94.5％和90.9％。最后，我们演示了如何使用ModEFormer对测试片段进行偏移检测。

    Lack of audio-video synchronization is a common problem during television broadcasts and video conferencing, leading to an unsatisfactory viewing experience. A widely accepted paradigm is to create an error detection mechanism that identifies the cases when audio is leading or lagging. We propose ModEFormer, which independently extracts audio and video embeddings using modality-specific transformers. Different from the other transformer-based approaches, ModEFormer preserves the modality of the input streams which allows us to use a larger batch size with more negative audio samples for contrastive learning. Further, we propose a trade-off between the number of negative samples and number of unique samples in a batch to significantly exceed the performance of previous methods. Experimental results show that ModEFormer achieves state-of-the-art performance, 94.5% for LRS2 and 90.9% for LRS3. Finally, we demonstrate how ModEFormer can be used for offset detection for test clips.
    
[^72]: 修正噪声：为可控领域翻译分解源特征

    Fix the Noise: Disentangling Source Feature for Controllable Domain Translation. (arXiv:2303.11545v1 [cs.CV])

    [http://arxiv.org/abs/2303.11545](http://arxiv.org/abs/2303.11545)

    本文提出了一种新的可控领域翻译方法，通过在目标特征空间的已分解子空间中保留源特征，使得只使用单个模型就能平滑地控制保留源特征的程度，产生更一致、更逼真的图像。

    

    最近的研究表明，在无条件生成器上使用转移学习技术，特别是在领域翻译方面，表现出了强大的生成能力。但是，使用单个模型控制不同领域特征之间的控制仍然具有挑战性。现有方法通常需要额外的模型，这在计算上是要求很高的，而且会导致不令人满意的视觉质量。此外，它们具有受限控制步骤，从而防止平滑过渡。在本文中，我们提出了一种新的高质量领域翻译方法，具有更好的可控性。其关键思想是在目标特征空间的已分解子空间中保留源特征。这使得我们的方法能够在只使用单个模型的情况下，平滑地控制保留源特征的程度，同时从完全新的领域生成图像。我们广泛的实验表明，所提出的方法可以产生比先前的工作更一致、更逼真的图像，并保持精确的可控性。

    Recent studies show strong generative performance in domain translation especially by using transfer learning techniques on the unconditional generator. However, the control between different domain features using a single model is still challenging. Existing methods often require additional models, which is computationally demanding and leads to unsatisfactory visual quality. In addition, they have restricted control steps, which prevents a smooth transition. In this paper, we propose a new approach for high-quality domain translation with better controllability. The key idea is to preserve source features within a disentangled subspace of a target feature space. This allows our method to smoothly control the degree to which it preserves source features while generating images from an entirely new domain using only a single model. Our extensive experiments show that the proposed method can produce more consistent and realistic images than previous works and maintain precise controllab
    
[^73]: MSTFormer：动态感知注意力的运动启发式时空Transformer用于长期船舶轨迹预测

    MSTFormer: Motion Inspired Spatial-temporal Transformer with Dynamic-aware Attention for long-term Vessel Trajectory Prediction. (arXiv:2303.11540v1 [cs.LG])

    [http://arxiv.org/abs/2303.11540](http://arxiv.org/abs/2303.11540)

    该论文提出了一种基于Transformer和动态感知注意力机制的运动启发式船舶轨迹预测方法，通过数据增强方法描述轨迹的空间和运动特征，能够更加准确地预测船舶轨迹。

    

    将动态知识纳入模型对于准确预测航迹至关重要，同时考虑船舶的空间和时间特征。不过，现有方法很少考虑潜在的动态知识，直接使用机器学习算法来预测航迹。直观地，船舶的运动遵循动力学定律，例如，转弯时速度会下降。然而，由于深度神经网络和动力学定律的内在异质性，将动态知识与神经网络相结合具有挑战性。在此背景下，我们提出了MSTFormer，一种基于Transformer的运动启发式船舶轨迹预测方法。本文的贡献有三个方面。首先，我们设计了一种数据增强方法来描述轨迹的空间特征和运动特征。其次，我们提出了一个多头动态感知自注意机制，以专注于运动变换频繁的轨迹点。

    Incorporating the dynamics knowledge into the model is critical for achieving accurate trajectory prediction while considering the spatial and temporal characteristics of the vessel. However, existing methods rarely consider the underlying dynamics knowledge and directly use machine learning algorithms to predict the trajectories. Intuitively, the vessel's motions are following the laws of dynamics, e.g., the speed of a vessel decreases when turning a corner. Yet, it is challenging to combine dynamic knowledge and neural networks due to their inherent heterogeneity. Against this background, we propose MSTFormer, a motion inspired vessel trajectory prediction method based on Transformer. The contribution of this work is threefold. First, we design a data augmentation method to describe the spatial features and motion features of the trajectory. Second, we propose a Multi-headed Dynamic-aware Self-attention mechanism to focus on trajectory points with frequent motion transformations. Fin
    
[^74]: 不定概率神经网络

    Indeterminate Probability Neural Network. (arXiv:2303.11536v1 [cs.LG])

    [http://arxiv.org/abs/2303.11536](http://arxiv.org/abs/2303.11536)

    本文提出了一种新型通用模型——不定概率神经网络；它可以进行无监督聚类和使用很小的神经网络处理大规模分类，其理论优势体现在新的概率理论和神经网络框架中。

    

    本文提出了一个称为IPNN的新型通用模型，它将神经网络和概率论结合在一起。在传统概率论中，概率的计算是基于事件的发生，而这在当前的神经网络中几乎不使用。因此，我们提出了一种新的概率理论，它是经典概率论的扩展，并使经典概率论成为我们理论的一种特殊情况。此外，对于我们提出的神经网络框架，神经网络的输出被定义为概率事件，并基于这些事件的统计分析，推导出分类任务的推理模型。IPNN展现了新的特性：它在进行分类的同时可以执行无监督聚类。此外，IPNN能够使用非常小的神经网络进行非常大的分类，例如100个输出节点的模型可以分类10亿类别。理论优势体现在新的概率理论和神经网络框架中，并且实验结果展示了IPNN在各种应用中的潜力。

    We propose a new general model called IPNN - Indeterminate Probability Neural Network, which combines neural network and probability theory together. In the classical probability theory, the calculation of probability is based on the occurrence of events, which is hardly used in current neural networks. In this paper, we propose a new general probability theory, which is an extension of classical probability theory, and makes classical probability theory a special case to our theory. Besides, for our proposed neural network framework, the output of neural network is defined as probability events, and based on the statistical analysis of these events, the inference model for classification task is deduced. IPNN shows new property: It can perform unsupervised clustering while doing classification. Besides, IPNN is capable of making very large classification with very small neural network, e.g. model with 100 output nodes can classify 10 billion categories. Theoretical advantages are refl
    
[^75]: 双机器学习在对抗事实公平回归中的应用

    Counterfactually Fair Regression with Double Machine Learning. (arXiv:2303.11529v1 [cs.LG])

    [http://arxiv.org/abs/2303.11529](http://arxiv.org/abs/2303.11529)

    本文提出了一种双机器学习公平性方法，用于回归问题中的对抗事实公平，假设两组变量的影响是可加的并且相互独立的，结果将近似平等，个人级别的结果将是对抗事实公平的。

    

    对抗事实公平是AI公平的一种方法，尝试基于某种敏感状态下的个人结果，排除这种状态对结果造成的影响。本文提出了双机器学习公平性，将此回归问题的对抗事实公平性类比到在潜在结果框架下估计因果推断中的对抗事实结果。它使用任意机器学习方法来分析敏感变量对非敏感变量和结果的影响。假设两组变量的影响是可加的并且相互独立的，结果将近似平等，个人级别的结果将是对抗事实公平的。本文通过模拟研究来演示了这种方法，研究涉及职场招聘中的歧视问题和对法学院学生GPA的实际数据应用。然后讨论了何时适用这种方法。

    Counterfactual fairness is an approach to AI fairness that tries to make decisions based on the outcomes that an individual with some kind of sensitive status would have had without this status. This paper proposes Double Machine Learning (DML) Fairness which analogises this problem of counterfactual fairness in regression problems to that of estimating counterfactual outcomes in causal inference under the Potential Outcomes framework. It uses arbitrary machine learning methods to partial out the effect of sensitive variables on nonsensitive variables and outcomes. Assuming that the effects of the two sets of variables are additively separable, outcomes will be approximately equalised and individual-level outcomes will be counterfactually fair. This paper demonstrates the approach in a simulation study pertaining to discrimination in workplace hiring and an application on real data estimating the GPAs of law school students. It then discusses when it is appropriate to apply such a meth
    
[^76]: SIFT: 稀疏等FLOP转换以最大限度提高训练效率

    SIFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency. (arXiv:2303.11525v1 [cs.LG])

    [http://arxiv.org/abs/2303.11525](http://arxiv.org/abs/2303.11525)

    本研究提出了一种名为SIFT的方法，用于提高深度神经网络的训练效率、准确性和表示能力，通过稀疏等FLOP转换，缩短训练时间。

    

    最近的研究探索了使用权重稀疏性来改善深度神经网络（DNN）的训练效率（与训练FLOPS相关的测试准确性）。 这些工作旨在减少训练FLOP，但使用稀疏权重进行训练通常会导致准确性损失或需要更长的训练周期，使得结果的训练效率不够清晰。 相比之下，我们专注于使用稀疏性提高准确性，同时使用与密集模型相同的FLOPS，并通过更高的准确性展示训练效率提高。 在本文中，我们介绍了SIFT，一组用作密集层的即插即用替代品来提高其表示能力和FLOP效率的稀疏等FLOP转换。 每个转换都由一个单一参数（稀疏级别）参数化，并提供更大的搜索空间以找到最佳的稀疏掩膜。

    Recent works have explored the use of weight sparsity to improve the training efficiency (test accuracy w.r.t training FLOPs) of deep neural networks (DNNs). These works aim to reduce training FLOPs but training with sparse weights often leads to accuracy loss or requires longer train schedules, making the resulting training efficiency less clear. In contrast, we focus on using sparsity to increase accuracy while using the same FLOPS as the dense model and show training efficiency gains through higher accuracy. In this work, we introduce SIFT, a family of Sparse Iso-FLOP Transformations which are used as drop-in replacements for dense layers to improve their representational capacity and FLOP efficiency. Each transformation is parameterized by a single parameter (sparsity level) and provides a larger search space to find optimal sparse masks. Without changing any training hyperparameters, replacing dense layers with SIFT leads to significant improvements across computer vision (CV) and
    
[^77]: 在不完全信息的市场中使用在线学习进行平衡定价

    Online Learning for Equilibrium Pricing in Markets under Incomplete Information. (arXiv:2303.11522v1 [cs.GT])

    [http://arxiv.org/abs/2303.11522](http://arxiv.org/abs/2303.11522)

    该论文研究了在不完全信息的市场中使用在线学习进行平衡定价的问题，提出了解决选择性谎言问题的新方法。

    

    市场平衡的研究是经济理论的核心，特别是在有效配置稀缺资源方面。然而，定价均衡的计算通常依赖于完整的个体属性信息，如供应商的成本函数等，这在实践中往往不可用。因此，我们考虑了在不完全信息的情况下解决定价均衡的问题。在这种情况下，市场经营者寻求通过从成本函数未知的竞争供应商购买所需数量来满足客户需求。在这种不完整信息的情况下，我们考虑了在线学习问题，即学习随时间变化的平衡价格，同时联合优化三个性能指标——未满足的需求、成本失误和付款失误——这是在定价均衡的情况下相关的。

    The study of market equilibria is central to economic theory, particularly in efficiently allocating scarce resources. However, the computation of equilibrium prices at which the supply of goods matches their demand typically relies on having access to complete information on private attributes of agents, e.g., suppliers' cost functions, which are often unavailable in practice. Motivated by this practical consideration, we consider the problem of setting equilibrium prices in the incomplete information setting wherein a market operator seeks to satisfy the customer demand for a commodity by purchasing the required amount from competing suppliers with privately known cost functions unknown to the market operator. In this incomplete information setting, we consider the online learning problem of learning equilibrium prices over time while jointly optimizing three performance metrics -- unmet demand, cost regret, and payment regret -- pertinent in the context of equilibrium pricing over a
    
[^78]: STDLens：基于模型挟持的物体检测联邦学习的安全防护方法

    STDLens: Model Hijacking-resilient Federated Learning for Object Detection. (arXiv:2303.11511v1 [cs.CR])

    [http://arxiv.org/abs/2303.11511](http://arxiv.org/abs/2303.11511)

    STDLens 是一种可以防止FL受到模型挟持的攻击的安全方法。它基于三层的取证框架来识别和排除特殊的梯度，并恢复FL的性能。STDLens在物体检测方面实现了最先进的性能并且具有防止模型挟持的鲁棒性。

    

    联邦学习（FL）作为协同学习框架在分布式客户端中训练基于深度学习的物体检测模型已经越来越受欢迎。尽管它具有诸多优点，FL容易受到模型挟持的攻击。攻击者可以仅仅利用一小部分可以被攻击的客户端控制物体检测系统的正确性，通过植入特殊梯度实现攻击。本文提出了一种名为STDLens的安全方法以保护FL免受此类攻击。我们首先调查现有的缓解机制并分析它们在空间聚类分析梯度时由于固有误差而产生的失败情况。基于这些洞见，我们提出了一个三层的取证框架来识别和排除这种特殊的梯度，并在FL过程中恢复性能。我们考虑了三种类型的自适应攻击，并展示了STDLens对高级对手具有的稳健性。大量实验表明，STDLens在物体检测方面实现了最先进的性能，并且具有防止模型挟持的鲁棒性。

    Federated Learning (FL) has been gaining popularity as a collaborative learning framework to train deep learning-based object detection models over a distributed population of clients. Despite its advantages, FL is vulnerable to model hijacking. The attacker can control how the object detection system should misbehave by implanting Trojaned gradients using only a small number of compromised clients in the collaborative learning process. This paper introduces STDLens, a principled approach to safeguarding FL against such attacks. We first investigate existing mitigation mechanisms and analyze their failures caused by the inherent errors in spatial clustering analysis on gradients. Based on the insights, we introduce a three-tier forensic framework to identify and expel Trojaned gradients and reclaim the performance over the course of FL. We consider three types of adaptive attacks and demonstrate the robustness of STDLens against advanced adversaries. Extensive experiments show that STD
    
[^79]: AI在人机交互中的影响-基于人机交互的应用

    AI-in-the-Loop -- The impact of HMI in AI-based Application. (arXiv:2303.11508v1 [cs.HC])

    [http://arxiv.org/abs/2303.11508](http://arxiv.org/abs/2303.11508)

    人机交互(HMI)被加入到AI架构设计过程中可以有效减少AI开发所需的资源，避免训练和评估具有非生产层的AI架构，从而导致轻量级AI架构的产生。

    

    人工智能(AI)和人机交互(HMI)是通常不适用于嵌入式应用的两个关键词。在将AI应用于解决特定任务之前需要的步骤中，HMI通常在AI架构设计和AI模型训练过程中缺失。人在环路概念在AI开发的所有其他步骤中都普遍存在，从数据分析到数据选择和清洗、性能评估。在AI架构设计过程中，HMI可以立即突出显示架构的非生产层，从而可以轻松创建用于嵌入式应用的轻量级网络架构。我们展示了通过使用这种HMI，用户可以立即区分哪种AI架构应该首先进行训练和评估，因为可以预期在任务中高精度。这种方法通过避免训练和评估具有非生产层的AI架构来减少AI开发所需的资源，从而导致轻量级AI架构。

    Artificial intelligence (AI) and human-machine interaction (HMI) are two keywords that usually do not fit embedded applications. Within the steps needed before applying AI to solve a specific task, HMI is usually missing during the AI architecture design and the training of an AI model. The human-in-the-loop concept is prevalent in all other steps of developing AI, from data analysis via data selection and cleaning to performance evaluation. During AI architecture design, HMI can immediately highlight unproductive layers of the architecture so that lightweight network architecture for embedded applications can be created easily. We show that by using this HMI, users can instantly distinguish which AI architecture should be trained and evaluated first since a high accuracy on the task could be expected. This approach reduces the resources needed for AI development by avoiding training and evaluating AI architectures with unproductive layers and leads to lightweight AI architectures. The
    
[^80]: FlexVDW:一种在配体对接中考虑蛋白柔性的机器学习方法。

    FlexVDW: A machine learning approach to account for protein flexibility in ligand docking. (arXiv:2303.11494v1 [q-bio.BM])

    [http://arxiv.org/abs/2303.11494](http://arxiv.org/abs/2303.11494)

    提出了一种采用机器学习的能量项来隐式考虑受体柔性的方法，该方法可以提高小分子配体位姿预测的结果，尤其是在存在大量蛋白形变的情况下。

    

    大多数被广泛使用的配体对接方法都假设蛋白结构是刚性的，当靶蛋白在配体结合时发生形变时，就会出现问题。特别是由于配体和蛋白质原子之间的明显冲突导致计算出的范德华能量项极高，因此配体的真实结合位姿通常会被评分得非常不利。传统上，这个问题是通过显式搜索受体构象以考虑受体在配体结合中的柔性来解决的。在这里，我们提出了一个深度学习模型，训练它在预测范德华能量时隐式考虑受体的柔性。我们展示了将这个机器学习能量项合并到最先进的基于物理的评分函数中可以提高小分子配体位姿预测结果的性能，特别是在存在大量蛋白形变的情况下，而不会恶化在存在最小蛋白形变的情况下的性能。

    Most widely used ligand docking methods assume a rigid protein structure. This leads to problems when the structure of the target protein deforms upon ligand binding. In particular, the ligand's true binding pose is often scored very unfavorably due to apparent clashes between ligand and protein atoms, which lead to extremely high values of the calculated van der Waals energy term. Traditionally, this problem has been addressed by explicitly searching for receptor conformations to account for the flexibility of the receptor in ligand binding. Here we present a deep learning model trained to take receptor flexibility into account implicitly when predicting van der Waals energy. We show that incorporating this machine-learned energy term into a state-of-the-art physics-based scoring function improves small molecule ligand pose prediction results in cases with substantial protein deformation, without degrading performance in cases with minimal protein deformation. This work demonstrates t
    
[^81]: 夹心视频压缩：通过神经网络封装来高效扩展标准编解码器的应用

    Sandwiched Video Compression: Efficiently Extending the Reach of Standard Codecs with Neural Wrappers. (arXiv:2303.11473v1 [eess.IV])

    [http://arxiv.org/abs/2303.11473](http://arxiv.org/abs/2303.11473)

    本文提出了夹心视频压缩方法，通过包装标准编解码器来使神经网络优化压缩性能，在高清视频传输和语音识别视频压缩等场景中表现显著。

    

    我们提出了夹心视频压缩--一种在标准视频编解码器周围包装神经网络的视频压缩系统。该夹心框架由神经前处理器、标准视频编解码器和神经后处理器组成。这些网络被联合训练以优化码率-失真损失函数，旨在在各种压缩场景中显着改善标准编解码器。在这个设置下的端到端训练需要一个可微的标准视频编解码器代理，它包括时间处理、运动补偿、内/间模式决策和循环滤波。我们提出了针对关键视频编解码器组件的可微逼近，并证明了夹心的神经编码相对于在两个重要场景中压缩输入视频的原始帧而言，具有显着更好的码率失真性能。在通过低分辨率HEVC传输高分辨率视频的情况下，夹心系统获得了6.5 dB的PSNR改善；在另一种场景中，压缩大词汇语音识别视频在0.02 bpp的情况下使用夹心系统，获得了30%的单词错误率降低。

    We propose sandwiched video compression -- a video compression system that wraps neural networks around a standard video codec. The sandwich framework consists of a neural pre- and post-processor with a standard video codec between them. The networks are trained jointly to optimize a rate-distortion loss function with the goal of significantly improving over the standard codec in various compression scenarios. End-to-end training in this setting requires a differentiable proxy for the standard video codec, which incorporates temporal processing with motion compensation, inter/intra mode decisions, and in-loop filtering. We propose differentiable approximations to key video codec components and demonstrate that the neural codes of the sandwich lead to significantly better rate-distortion performance compared to compressing the original frames of the input video in two important scenarios. When transporting high-resolution video via low-resolution HEVC, the sandwich system obtains 6.5 dB
    
[^82]: 你有在使用我的数据集进行训练吗？使用清洁标签背门数字水印实现公共数据集保护

    Did You Train on My Dataset? Towards Public Dataset Protection with Clean-Label Backdoor Watermarking. (arXiv:2303.11470v1 [cs.CR])

    [http://arxiv.org/abs/2303.11470](http://arxiv.org/abs/2303.11470)

    提出了一种基于背门数字水印的方法，以确保公共数据的安全。通过在数据集中插入极少量的数字水印样本，隐式学习一个隐藏的函数作为数字水印，以跟踪非法使用此数据集的模型。使用“清洁标签背门”方法实现了数字水印，不会破坏原始数据集。实验证明，该方法有效地检测到非法利用数据集的行为。

    

    互联网上源源不断的支持训练数据是深度学习模型成功的关键因素。然而，这种大量的公共数据也引起了对数据集被未经授权的用于商业目的的担忧，这是数据集许可证所禁止的。本文提出了一种基于背门数字水印的方法，作为保护公共数据的通用框架。通过向数据集中插入少量的数字水印样本，我们的方法使学习模型能够隐式学习由防御者设置的秘密函数。这个隐藏的函数可以作为数字水印，用于跟踪非法使用数据集的第三方模型。不幸的是，现有的背门插入方法往往涉及向训练集中添加任意的、错误标记的数据，导致性能显著下降，并容易被异常检测算法检测到。为了克服这个挑战，我们引入了一种清洁标记背门方法，实现了数字水印而不破坏原始数据集。我们的方法在几个图像分类任务上进行了评估，证明了它在检测非法数据集使用方面的有效性。

    The huge supporting training data on the Internet has been a key factor in the success of deep learning models. However, this abundance of public-available data also raises concerns about the unauthorized exploitation of datasets for commercial purposes, which is forbidden by dataset licenses. In this paper, we propose a backdoor-based watermarking approach that serves as a general framework for safeguarding public-available data. By inserting a small number of watermarking samples into the dataset, our approach enables the learning model to implicitly learn a secret function set by defenders. This hidden function can then be used as a watermark to track down third-party models that use the dataset illegally. Unfortunately, existing backdoor insertion methods often entail adding arbitrary and mislabeled data to the training set, leading to a significant drop in performance and easy detection by anomaly detection algorithms. To overcome this challenge, we introduce a clean-label backdoo
    
[^83]: 应用组合数学中的七个未解之谜

    Seven open problems in applied combinatorics. (arXiv:2303.11464v1 [math.CO])

    [http://arxiv.org/abs/2303.11464](http://arxiv.org/abs/2303.11464)

    这篇论文介绍了七个应用组合数学中的未解之谜，这些问题涉及量子计算，算法分化，拓扑数据分析，超图切割算法和电力系统等多个研究领域。

    

    我们提出并讨论了应用组合数学中的七个不同的未解之谜。这个研究领域包括量子计算，算法分化，拓扑数据分析，迭代方法，超图切割算法和电力系统。

    We present and discuss seven different open problems in applied combinatorics. The application areas relevant to this compilation include quantum computing, algorithmic differentiation, topological data analysis, iterative methods, hypergraph cut algorithms, and power systems.
    
[^84]: 公正感知的图滤波器设计

    Fairness-Aware Graph Filter Design. (arXiv:2303.11459v1 [cs.LG])

    [http://arxiv.org/abs/2303.11459](http://arxiv.org/abs/2303.11459)

    本文设计了一种公平图滤波器，用于在图形学习任务中缓解偏见。实验表明，该设计在减轻偏见方面具有显著的功效，同时相对于基准算法，实用度更高且更稳定。

    

    图是用来表示复杂的现实世界系统的数学工具，例如金融市场和社交网络。因此，最近图机器学习（ML）引起了重要的关注。然而，由于在偏见图结构上的信息聚合，已经证明了 ML 对于各种决策问题中已存在的某些欠代表群体的偏见的放大。面对这一挑战，本文设计了一种公平图滤波器，可在各种基于图的学习任务中灵活使用。所提出的滤波器设计基于偏见分析，并建立了其在缓解偏见方面的优越性与其无关公平性对应物相比的最优性。节点分类的实验表明，该提议的过滤器设计在减轻偏见方面具有显著的功效，同时相对于基准算法，实用度更高且更稳定。

    Graphs are mathematical tools that can be used to represent complex real-world systems, such as financial markets and social networks. Hence, machine learning (ML) over graphs has attracted significant attention recently. However, it has been demonstrated that ML over graphs amplifies the already existing bias towards certain under-represented groups in various decision-making problems due to the information aggregation over biased graph structures. Faced with this challenge, in this paper, we design a fair graph filter that can be employed in a versatile manner for graph-based learning tasks. The design of the proposed filter is based on a bias analysis and its optimality in mitigating bias compared to its fairness-agnostic counterpart is established. Experiments on real-world networks for node classification demonstrate the efficacy of the proposed filter design in mitigating bias, while attaining similar utility and better stability compared to baseline algorithms.
    
[^85]: 大型语言模型与简单愚蠢 Bug

    Large Language Models and Simple, Stupid Bugs. (arXiv:2303.11455v1 [cs.SE])

    [http://arxiv.org/abs/2303.11455](http://arxiv.org/abs/2303.11455)

    本文研究表明，大型语言模型如Codex虽然有助于避免一些简单Bug，但经常会生成已知的，逐字的Bug，因此需要采取避免策略来减少这种情况，并增加有创意的Bug的生产。

    

    随着强大的神经语言模型的出现，用于辅助开发者进行编码任务的基于AI的系统变得普遍可用；Copilot便是这样的系统。Copilot使用Codex这个大型语言模型（LLM）来完成一个相应的“提示”后的代码。然而，Codex是在公共GitHub存储库上训练的，即可能包含错误和漏洞的代码。先前的研究表明Codex会复制训练中出现的漏洞。在这项研究中，我们研究了Codex生成的一个有趣的 Bug 类型的易发性，即单语句 Bug，通常被称为简单愚蠢 Bug 或 SStuBs。我们发现Codex和类似的LLMs确实有助于避免一些SStuBs，但确实会生成已知的，逐字的SStuBs，其出现的可能性是已知正确代码的2倍。我们探讨了Codex生成的SStuBs的后果，并提出了避免策略，这些策略可能有助于减少已知的，逐字的SStubs的生产，并增加有创意的Bug的生产。

    With the advent of powerful neural language models, AI-based systems to assist developers in coding tasks are becoming widely available; Copilot is one such system. Copilot uses Codex, a large language model (LLM), to complete code conditioned on a preceding "prompt". Codex, however, is trained on public GitHub repositories, viz., on code that may include bugs and vulnerabilities. Previous studies [1], [2] show Codex reproduces vulnerabilities seen in training. In this study, we examine how prone Codex is to generate an interesting bug category, single statement bugs, commonly referred to as simple, stupid bugs or SStuBs in the MSR community. We find that Codex and similar LLMs do help avoid some SStuBs, but do produce known, verbatim SStuBs as much as 2x as likely than known, verbatim correct code. We explore the consequences of the Codex generated SStuBs and propose avoidance strategies that suggest the possibility of reducing the production of known, verbatim SStubs, and increase th
    
[^86]: 如何描述ReLU神经网络的隐式正则化特性——第二部分：具有随机第一层的两层多维情况

    How (Implicit) Regularization of ReLU Neural Networks Characterizes the Learned Function -- Part II: the Multi-D Case of Two Layers with Random First Layer. (arXiv:2303.11454v1 [cs.LG])

    [http://arxiv.org/abs/2303.11454](http://arxiv.org/abs/2303.11454)

    本文提出了具有ReLU激活的随机浅层神经网络的特征描述，对于回归问题，它们类似于无限广义加性模型（IGAM）

    

    随机化神经网络是一种强大的模型，其中仅优化了最终层的权重，可降低神经网络模型的计算时间。同时，这些模型在各种回归和分类任务中的泛化能力惊人。本文对于具有ReLU激活的随机、浅层神经网络提出了一个宏观精确的特征描述，即广义加性模型（GAM）类型的回归问题，其中考虑了无限多个方向：无限广义加性模型（IGAM）。 IGAM被形式化为函数空间中特定正则化泛函和相当一般的损失函数的优化问题的解。本文是在先前研究的基础上对多元神经网络进行的扩展，我们在先前研究中展示了在某些条件下具有ReLU激活的宽式RSNs的行为类似于样条回归。

    Randomized neural networks (randomized NNs), where only the terminal layer's weights are optimized constitute a powerful model class to reduce computational time in training the neural network model. At the same time, these models generalize surprisingly well in various regression and classification tasks. In this paper, we give an exact macroscopic characterization (i.e., a characterization in function space) of the generalization behavior of randomized, shallow NNs with ReLU activation (RSNs). We show that RSNs correspond to a generalized additive model (GAM)-typed regression in which infinitely many directions are considered: the infinite generalized additive model (IGAM). The IGAM is formalized as solution to an optimization problem in function space for a specific regularization functional and a fairly general loss. This work is an extension to multivariate NNs of prior work, where we showed how wide RSNs with ReLU activation behave like spline regression under certain conditions 
    
[^87]: 基于Group Lasso的贪婪剪枝在矩阵感知和二次激活神经网络上可证地泛化

    Greedy Pruning with Group Lasso Provably Generalizes for Matrix Sensing and Neural Networks with Quadratic Activations. (arXiv:2303.11453v1 [cs.LG])

    [http://arxiv.org/abs/2303.11453](http://arxiv.org/abs/2303.11453)

    本文在矩阵感知问题中研究了基于Group Lasso正则化器的贪婪剪枝方法，证明了修剪低$\ell_2$范数列的解可以泛化到新样本上。

    

    剪枝方案广泛用于降低具有大量参数的模型的复杂性。实践研究表明，修剪过度参数化模型并微调可很好地泛化到新样本上。虽然以上被称为剪枝+微调的流程在降低训练模型的复杂性方面非常成功，但其背后的理论仍然不甚了解。本文通过研究超参数化矩阵感知问题上的剪枝+微调框架来解决这个问题，其中真实结果表示为$U_\star \in \mathbb{R}^{d \times r}$，而超参数化模型表示为$U \in \mathbb{R}^{d \times k}$，其中$k \gg r$。我们研究加上Group Lasso正则化器的平滑版本$\sum_{i=1}^k \| U e_i \|_2$的平均误差的近似局部极小值，证明修剪低$\ell_2$范数列的解$U_{

    Pruning schemes have been widely used in practice to reduce the complexity of trained models with a massive number of parameters. Several practical studies have shown that pruning an overparameterized model and fine-tuning generalizes well to new samples. Although the above pipeline, which we refer to as pruning + fine-tuning, has been extremely successful in lowering the complexity of trained models, there is very little known about the theory behind this success. In this paper we address this issue by investigating the pruning + fine-tuning framework on the overparameterized matrix sensing problem, with the ground truth denoted $U_\star \in \mathbb{R}^{d \times r}$ and the overparameterized model $U \in \mathbb{R}^{d \times k}$ with $k \gg r$. We study the approximate local minima of the empirical mean square error, augmented with a smooth version of a group Lasso regularizer, $\sum_{i=1}^k \| U e_i \|_2$ and show that pruning the low $\ell_2$-norm columns results in a solution $U_{\
    
[^88]: 图像分类中的偏见缓解技术：人类文化遗产中公平的机器学习

    Bias mitigation techniques in image classification: fair machine learning in human heritage collections. (arXiv:2303.11449v1 [cs.CV])

    [http://arxiv.org/abs/2303.11449](http://arxiv.org/abs/2303.11449)

    该论文研究了在人类文化遗产中运用图像分类技术时面临的公平性问题， 并且提出了三种偏见缓解技术与迁移学习来解决这一问题。研究显示，这些方案有效地降低了性别分类中的偏见。

    

    使用自动化分类系统的主要问题是，如果它们没有正确地进行工程化和公平性考虑，它们可能对特定人群具有不利影响。此外，虽然工程师们已经开发了前沿的图像分类技术，但在人类文化遗产收藏中应用这些模型仍存在差距，因为这里的数据集通常包含质量低劣的人们照片，这些人们具有不同的族裔、性别和年龄。在本研究中，我们使用两个最先进的神经网络Xception和EfficientNet评估了三种偏见缓解技术，用于性别分类。此外，我们探索使用公平数据集的迁移学习，以克服训练数据的稀缺性。我们在19世纪和20世纪的文化遗产照片收藏上评估了偏见缓解管道的有效性，并使用FairFace数据集进行了迁移学习实验。经过评估，我们发现我们提出的偏见缓解技术成功地减少了性别分类中的偏见，尤其是对于弱势群体。我们的研究为如何解决人类文化遗产图像分类中的公平问题提供了洞见。

    A major problem with using automated classification systems is that if they are not engineered correctly and with fairness considerations, they could be detrimental to certain populations. Furthermore, while engineers have developed cutting-edge technologies for image classification, there is still a gap in the application of these models in human heritage collections, where data sets usually consist of low-quality pictures of people with diverse ethnicity, gender, and age. In this work, we evaluate three bias mitigation techniques using two state-of-the-art neural networks, Xception and EfficientNet, for gender classification. Moreover, we explore the use of transfer learning using a fair data set to overcome the training data scarcity. We evaluated the effectiveness of the bias mitigation pipeline on a cultural heritage collection of photographs from the 19th and 20th centuries, and we used the FairFace data set for the transfer learning experiments. After the evaluation, we found th
    
[^89]: 栅格规范等变卷积神经网络的几何方面

    Geometrical aspects of lattice gauge equivariant convolutional neural networks. (arXiv:2303.11448v1 [hep-lat])

    [http://arxiv.org/abs/2303.11448](http://arxiv.org/abs/2303.11448)

    本文研究了栅格规范等变卷积神经网络的全局群等变性，并提供了一个几何公式，表明L-CNN中的卷积是SU($N$)主丛上规范等变神经网络的一个特例。

    

    栅格规范等变卷积神经网络(L-CNNs)是卷积神经网络的框架，可应用于非阿贝尔栅格规范理论，而不违反规范对称性。我们演示了如何将L-CNNs配备全局群等变性。这使我们能够将公式扩展为既等变于平移，又等变于全局纹格对称性，例如旋转和反射。此外，我们提供了一个L-CNN的几何公式，并显示L-CNN中的卷积是SU($N$)主丛上规范等变神经网络的特例。

    Lattice gauge equivariant convolutional neural networks (L-CNNs) are a framework for convolutional neural networks that can be applied to non-Abelian lattice gauge theories without violating gauge symmetry. We demonstrate how L-CNNs can be equipped with global group equivariance. This allows us to extend the formulation to be equivariant not just under translations but under global lattice symmetries such as rotations and reflections. Additionally, we provide a geometric formulation of L-CNNs and show how convolutions in L-CNNs arise as a special case of gauge equivariant neural networks on SU($N$) principal bundles.
    
[^90]: 直接迭代反演：图像修复的替代方法

    Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration. (arXiv:2303.11435v1 [eess.IV])

    [http://arxiv.org/abs/2303.11435](http://arxiv.org/abs/2303.11435)

    InDI是一种新的监督式图像修复公式，通过逐步改进图像质量来生成比现有回归方法更真实和详细的图像，修复效果更具有感知质量。

    

    直接迭代反演（InDI）是一种新的监督式图像修复公式，它避免了所谓的“均值回归”效应，并生成比现有回归方法更真实和详细的图像。它通过逐步改进图像质量来实现，类似于生成式去噪扩散模型。图像修复是一个欠定问题，多个高质量图像都可能是给定低质量输入的可行重构。因此，单步回归模型的结果通常是所有可能解释的聚合结果，因此缺乏细节和真实感。

    Inversion by Direct Iteration (InDI) is a new formulation for supervised image restoration that avoids the so-called ``regression to the mean'' effect and produces more realistic and detailed images than existing regression-based methods. It does this by gradually improving image quality in small steps, similar to generative denoising diffusion models.  Image restoration is an ill-posed problem where multiple high-quality images are plausible reconstructions of a given low-quality input. Therefore, the outcome of a single step regression model is typically an aggregate of all possible explanations, therefore lacking details and realism. % The main advantage of InDI is that it does not try to predict the clean target image in a single step but instead gradually improves the image in small steps, resulting in better perceptual quality.  While generative denoising diffusion models also work in small steps, our formulation is distinct in that it does not require knowledge of any analytic f
    
[^91]: ResDTA: 使用残差跳跃连接预测药物-靶标结合亲和力

    ResDTA: Predicting Drug-Target Binding Affinity Using Residual Skip Connections. (arXiv:2303.11434v1 [cs.LG])

    [http://arxiv.org/abs/2303.11434](http://arxiv.org/abs/2303.11434)

    本论文提出了一种只利用靶标和药物的测序信息来预测药物靶标结合亲和力的深度学习模型，该模型使用1D表示，无需额外特征或描述。

    

    发现新药物靶点（DT）相互作用是新药开发过程中的重要步骤。大部分用于预测DT交互的计算机技术都集中于二元分类，目的在于确定DT对是否相互作用。然而，蛋白质配体相互作用则假设存在一系列不同的绑定强度值，也称为结合亲和力，预测这些值一直是一个难点。随着DT知识库中亲和力数据数量的增加，可以使用深度学习架构等先进的学习技术来预测结合亲和力。本文提出了一种基于深度学习的方法，仅使用来自靶标和药物的测序信息来预测DT结合亲和力。结果表明，这种使用1D表示来自靶标和药物的深度学习模型是一种有效的药物靶标结合亲和力预测方法，不需要额外的特征或描述。

    The discovery of novel drug target (DT) interactions is an important step in the drug development process. The majority of computer techniques for predicting DT interactions have focused on binary classification, with the goal of determining whether or not a DT pair interacts. Protein ligand interactions, on the other hand, assume a continuous range of binding strength values, also known as binding affinity, and forecasting this value remains a difficulty. As the amount of affinity data in DT knowledge-bases grows, advanced learning techniques such as deep learning architectures can be used to predict binding affinities. In this paper, we present a deep-learning-based methodology for predicting DT binding affinities using just sequencing information from both targets and drugs. The results show that the proposed deep learning-based model that uses the 1D representations of targets and drugs is an effective approach for drug target binding affinity prediction and it does not require add
    
[^92]: 基于机器学习的心血管疾病检测：心电图信号性能与复杂性的比较

    Machine learning-based detection of cardiovascular disease using ECG signals: performance vs. complexity. (arXiv:2303.11429v1 [eess.SP])

    [http://arxiv.org/abs/2303.11429](http://arxiv.org/abs/2303.11429)

    本文提出了多种基于机器学习的方法来检测心血管疾病，其中基于深度学习的图像分类器在预测房颤方面表现较好，而XGBoost在长期数据中表现良好，一维卷积模型则特别适用于长期ECG记录的分类和推断。

    

    心血管疾病在现代社会仍是一个重大问题。在非侵入性技术中，心电图（ECG）是检测心脏活动异常最可靠的方法之一。然而，ECG解读需要专业知识且耗时。开发一种新的方法可以及早检测疾病可能会防止死亡和并发症。本文提出了几种新的方法来从ECG记录中分类心脏疾病。第一种方法建议使用ECG信号的泊松分布表示和基于深度学习的图像分类器（ResNet50和DenseNet121学习了泊松分布），在预测房颤方面表现不错但不能预测其他类型的心律失常。XGBoost，一种梯度提升模型，在长期数据中表现良好但由于预处理阶段中高耗时计算，推断时间较长。最后，一维卷积模型特别适用于长期ECG记录分类和推断。

    Cardiovascular disease remains a significant problem in modern society. Among non-invasive techniques, the electrocardiogram (ECG) is one of the most reliable methods for detecting abnormalities in cardiac activities. However, ECG interpretation requires expert knowledge and it is time-consuming. Developing a novel method to detect the disease early could prevent death and complication. The paper presents novel various approaches for classifying cardiac diseases from ECG recordings. The first approach suggests the Poincare representation of ECG signal and deep-learning-based image classifiers (ResNet50 and DenseNet121 were learned over Poincare diagrams), which showed decent performance in predicting AF (atrial fibrillation) but not other types of arrhythmia. XGBoost, a gradient-boosting model, showed an acceptable performance in long-term data but had a long inference time due to highly-consuming calculation within the pre-processing phase. Finally, the 1D convolutional model, specifi
    
[^93]: 基于机器学习模型的LHCb超快速模拟系统Lamarr在Gauss中的应用

    Lamarr: LHCb ultra-fast simulation based on machine learning models deployed within Gauss. (arXiv:2303.11428v1 [hep-ex])

    [http://arxiv.org/abs/2303.11428](http://arxiv.org/abs/2303.11428)

    LHCb实验中的90%计算资源用于生产模拟数据样本，而Lamarr是一个基于机器学习模型的系统，通过对LHCb实验的探测器响应和重建算法进行参数化，加快了模拟产出。

    

    LHCb实验可用的计算资源的约90%用于生产Large Hadron Collider（LHC）运行2的模拟数据样本。升级后的LHCb探测器将能够收集更多的数据样本，需要更多的模拟事件来分析将在运行3中收集的数据。模拟是分析的关键需求，以解释信号与背景并测量效率。这种需要的模拟将远远超出已承诺的资源，需要技术和技巧的演变来生产这些模拟数据样本。在这项贡献中，我们讨论了Lamarr，这是一种基于Gaudi框架的系统，该系统通过对LHCb实验的探测器响应和重建算法进行参数化，加快了模拟产出。使用基于多种算法和策略的深度生成模型，有效地参数化了LHCb探测器单个组件的高级响应，在神经网络中编码。

    About 90% of the computing resources available to the LHCb experiment has been spent to produce simulated data samples for Run 2 of the Large Hadron Collider at CERN. The upgraded LHCb detector will be able to collect larger data samples, requiring many more simulated events to analyze the data to be collected in Run 3. Simulation is a key necessity of analysis to interpret signal vs background and measure efficiencies. The needed simulation will far exceed the pledged resources, requiring an evolution in technologies and techniques to produce these simulated data samples. In this contribution, we discuss Lamarr, a Gaudi-based framework to speed-up the simulation production parametrizing both the detector response and the reconstruction algorithms of the LHCb experiment. Deep Generative Models powered by several algorithms and strategies are employed to effectively parametrize the high-level response of the single components of the LHCb detector, encoding within neural networks the exp
    
[^94]: 学习基于模型无关的稳健预编码以实现协作多波束卫星通信

    Learning Model-Free Robust Precoding for Cooperative Multibeam Satellite Communications. (arXiv:2303.11427v1 [eess.SP])

    [http://arxiv.org/abs/2303.11427](http://arxiv.org/abs/2303.11427)

    本研究利用深度强化学习算法学习基于模型无关且不需要先验知识的稳健预编码，以实现协作多波束卫星通信。

    

    直接的低地球轨道卫星到手持设备的链接有望成为卫星通信的新时代。空间分集多址预编码是一种减少卫星波束干扰、通过卫星合作重新使用频率从而提高频谱效率的技术。在过去的几十年中，针对几种情况已经提出了具有完美信道状态信息的最优预编码解决方案，而仅具有不完美信道状态信息的稳健预编码主要针对简化模型进行研究。特别地，对于低地球轨道卫星应用来说，这种简化模型可能是不准确的。在本文中，我们使用软性Actor-Critic深度强化学习算法的函数逼近能力，学习不需要系统缺陷知识的稳健预编码。

    Direct Low Earth Orbit satellite-to-handheld links are expected to be part of a new era in satellite communications. Space-Division Multiple Access precoding is a technique that reduces interference among satellite beams, therefore increasing spectral efficiency by allowing cooperating satellites to reuse frequency. Over the past decades, optimal precoding solutions with perfect channel state information have been proposed for several scenarios, whereas robust precoding with only imperfect channel state information has been mostly studied for simplified models. In particular, for Low Earth Orbit satellite applications such simplified models might not be accurate. In this paper, we use the function approximation capabilities of the Soft Actor-Critic deep Reinforcement Learning algorithm to learn robust precoding with no knowledge of the system imperfections.
    
[^95]: 利用小波散射变换和1D-CNN进行听诊器心杂音和异常PCG检测

    Heart Murmur and Abnormal PCG Detection via Wavelet Scattering Transform & a 1D-CNN. (arXiv:2303.11423v1 [eess.SP])

    [http://arxiv.org/abs/2303.11423](http://arxiv.org/abs/2303.11423)

    该研究提出了一种小波散射变换和1D-CNN深度学习技术结合的心杂音自动检测方法，可实现97.98%的高准确率。

    

    本项研究利用深度学习技术对听诊器心杂音的自动和准确检测，并使用两个公共PCG数据集（CirCor Digiscope 2022和PCG 2016数据集）进行训练和测试三个自定义神经网络：一维卷积神经网络（CNN），长短期记忆（LSTM）递归神经网络（RNN）和卷积RNN（C-RNN）。

    This work leverages deep learning (DL) techniques in order to do automatic and accurate heart murmur detection from phonocardiogram (PCG) recordings. Two public PCG datasets (CirCor Digiscope 2022 dataset and PCG 2016 dataset) from Physionet online database are utilized to train and test three custom neural networks (NN): a 1D convolutional neural network (CNN), a long short-term memory (LSTM) recurrent neural network (RNN), and a convolutional RNN (C-RNN). Under our proposed method, we first do pre-processing on both datasets in order to prepare the data for the NNs. Key pre-processing steps include the following: denoising, segmentation, re-labeling of noise-only segments, data normalization, and time-frequency analysis of the PCG segments using wavelet scattering transform. To evaluate the performance of the three NNs we have implemented, we conduct four experiments, first three using PCG 2022 dataset, and fourth using PCG 2016 dataset. It turns out that our custom 1D-CNN outperform
    
[^96]: 融合时频和空间表示来改善基于EEG的情感识别

    Improving EEG-based Emotion Recognition by Fusing Time-frequency And Spatial Representations. (arXiv:2303.11421v1 [eess.SP])

    [http://arxiv.org/abs/2303.11421](http://arxiv.org/abs/2303.11421)

    论文提出了一种基于跨域特征融合方法的EEG信号分类网络，其中融合了时频域和空间域的多重表示方法，该方法在EEG情感识别中取得了最先进的结果。

    

    使用深度学习方法对EEG信号进行分类可以准确识别人们的情绪。然而，现有研究很少考虑将另一个领域的表示中的信息应用于时频域的特征选择。我们提出了一种基于跨域特征融合方法的EEG信号分类网络，通过使用多域注意机制，使网络更专注于与脑活动和思维变化最相关的特征。此外，我们提出了一个两步融合方法，并将这些方法应用于EEG情感识别网络中。实验结果表明，我们提出的融合时频域和空间域的多重表示方法，优于先前公开数据集上的方法，在当前取得了最先进的结果。

    Using deep learning methods to classify EEG signals can accurately identify people's emotions. However, existing studies have rarely considered the application of the information in another domain's representations to feature selection in the time-frequency domain. We propose a classification network of EEG signals based on the cross-domain feature fusion method, which makes the network more focused on the features most related to brain activities and thinking changes by using the multi-domain attention mechanism. In addition, we propose a two-step fusion method and apply these methods to the EEG emotion recognition network. Experimental results show that our proposed network, which combines multiple representations in the time-frequency domain and spatial domain, outperforms previous methods on public datasets and achieves state-of-the-art at present.
    
[^97]: EPiC: 基于部分点云集成的鲁棒分类方法

    EPiC: Ensemble of Partial Point Clouds for Robust Classification. (arXiv:2303.11419v1 [cs.CV])

    [http://arxiv.org/abs/2303.11419](http://arxiv.org/abs/2303.11419)

    本文提出了一种基于部分点云采样的通用集成框架，由多种采样方法联合使用提高鲁棒性，达到了最优性能。

    

    对于真实应用中的点云分类来说，由于消费型3D传感器通常采集的是部分和带有噪声的数据，且会受到各种因素的影响而变得降质。本文提出了一种基于部分点云采样的通用集成框架。每个集成成员只暴露于部分输入数据。我们同时使用两种基于补丁和曲线的局部采样策略以及一种全局的随机采样策略。我们证明了我们的方法对各种局部和全局污染具有很强的鲁棒性。我们展示了我们的框架极大地提高了顶级分类网络的鲁棒性。我们的实验设置使用了Ren等人[24]引入的最新ModelNet-C数据库，在不经过数据增强和经过数据增强的情况下都达到了最优（SOTA）。我们的未经数据增强的均值腐蚀误差（mCE）为0.64（当前SOTA为0.86），经过数据增强后为0.50（当前SOTA为0.57）。我们通过多样性和可视化分析了这些显著的结果。

    Robust point cloud classification is crucial for real-world applications, as consumer-type 3D sensors often yield partial and noisy data, degraded by various artifacts. In this work we propose a general ensemble framework, based on partial point cloud sampling. Each ensemble member is exposed to only partial input data. Three sampling strategies are used jointly, two local ones, based on patches and curves, and a global one of random sampling. We demonstrate the robustness of our method to various local and global degradations. We show that our framework significantly improves the robustness of top classification netowrks by a large margin. Our experimental setting uses the recently introduced ModelNet-C database by Ren et al.[24], where we reach SOTA both on unaugmented and on augmented data. Our unaugmented mean Corruption Error (mCE) is 0.64 (current SOTA is 0.86) and 0.50 for augmented data (current SOTA is 0.57). We analyze and explain these remarkable results through diversity an
    
[^98]: 基于深度学习的振动信号去噪方法

    Vibration Signal Denoising Using Deep Learning. (arXiv:2303.11413v1 [eess.SP])

    [http://arxiv.org/abs/2303.11413](http://arxiv.org/abs/2303.11413)

    本文研究了基于深度学习的去除脚步引起的振动信号的噪声的方法，该方法适用于高斯噪声和非平稳噪声。

    

    由脚步引起的结构振动信号被广泛用于人员识别、定位、人类活动推断、结构健康监测等任务。然而，由于环境噪声、电磁干扰等因素的影响，实际采集的信号通常会带有噪声。噪声的存在影响了信号处理过程，从而影响了最终任务的准确性和误差。本文主要探讨了基于深度学习的去除脚步引起的振动信号的噪声的方法。我们考虑了不同类型的噪声，包括高斯噪声和非平稳噪声等。

    Structure vibration signals induced by footsteps are widely used for tasks like occupant identification, localization, human activity inference, structure health monitoring and so on. The vibration signals are collected as time series with amplitude values. However, the collected signals are always noisy in practice due to the influence of environmental noise, electromagnetic interference and other factors. The presence of noise affects the process of signal analysis, thus affecting the accuracy and error of the final tasks. In this paper, we mainly explore the denoising methods for footstep-induced vibration signals. We have considered different kinds of noise including stationary noises such as gaussian noises and non-stationary noises such as item-dropping vibration noise and music noises.
    
[^99]: 使用定向变分自编码器进行高风险状态的定向分析

    Targeted Analysis of High-Risk States Using an Oriented Variational Autoencoder. (arXiv:2303.11410v1 [eess.SY])

    [http://arxiv.org/abs/2303.11410](http://arxiv.org/abs/2303.11410)

    本文提出了一种OVAE方法，可以约束潜变量空间代码与生成数据之间的链接，以生成与电力系统中特定的高风险状态对齐的具有定向特征的合成数据，提高了对关键事件的预测和缓解的效率和精度。

    

    变分自编码器（VAE）神经网络可以被训练以生成捕捉历史数据边际分布和多元依赖性的电力系统状态。VAE的潜变量空间的坐标已被证明与数据的概念特征相关，这可以利用来合成具有所需特征的定向数据。然而，VAE的潜变量空间代码对应特定属性的位置并不受限制。此外，生成具有特定特征的数据可能需要将具有对应难以获得标签的数据馈入生成模型进行训练。为了使数据生成更具可控性和效率，本文提出了一种定向变分自编码器（OVAE），以Spearman相关性的形式约束潜变量空间代码与生成数据之间的链接，这为数据合成过程提供了更精细的控制。在此基础上，提出了一种重要性采样过程，以针对电力系统中特定的高风险状态。 OVAE被证明能够生成与所需高风险状态对齐的具有定向特征的合成数据，从而实现对电力系统中关键事件的更有效预测和缓解。

    Variational autoencoder (VAE) neural networks can be trained to generate power system states that capture both marginal distribution and multivariate dependencies of historical data. The coordinates of the latent space codes of VAEs have been shown to correlate with conceptual features of the data, which can be leveraged to synthesize targeted data with desired features. However, the locations of the VAEs' latent space codes that correspond to specific properties are not constrained. Additionally, the generation of data with specific characteristics may require data with corresponding hard-to-get labels fed into the generative model for training. In this paper, to make data generation more controllable and efficient, an oriented variation autoencoder (OVAE) is proposed to constrain the link between latent space code and generated data in the form of a Spearman correlation, which provides increased control over the data synthesis process. On this basis, an importance sampling process is
    
[^100]: eP-ALM:语言模型的高效感知增强

    eP-ALM: Efficient Perceptual Augmentation of Language Models. (arXiv:2303.11403v1 [cs.CV])

    [http://arxiv.org/abs/2303.11403](http://arxiv.org/abs/2303.11403)

    本论文提出了一种用对比学习提高语言模型的感知能力的高效方法eP-ALM，可以实现视觉感知信息和文本信息的融合，同时还能在多模态基准测试上实现最先进的结果。

    

    大型语言模型(LLM)迄今为止给世界留下了深刻印象，具有大规模模型所具有的非同寻常的能力。在视觉方面，变压器模型（即ViT）也在追随同一趋势，取得了最具挑战性的基准测试的最佳表现。随着这种单模型的丰富多样，自然会引发一个问题：我们是否需要跟随这个趋势来处理多模态任务？在这项工作中，我们提出将努力集中于现有模型的高效适应，并提出用感知来增强语言模型。现有的适应预训练模型用于视觉语言任务的方法仍然依赖于几个关键组件，从而影响了它们的效率。特别地，他们仍然训练大量的参数，依赖大规模的多模态预训练，使用在巨大的图像-文本数据集上训练的编码器（例如CLIP），并添加了显著的推理开销。此外，这些方法中的大多数关注Zero-Shot和In Context Learning，观察到两种范式之间的巨大差异。在本文中，我们介绍了eP-ALM，一种将视觉感知信息与语言模型相结合的高效方法。我们提出了一种方法，利用对比学习来实现视觉感知和文本信息的融合，具有极小的计算成本。我们的方法不需要任何新的预训练，仍然在多模态基准测试上实现了最先进的结果。

    Large Language Models (LLMs) have so far impressed the world, with unprecedented capabilities that emerge in models at large scales. On the vision side, transformer models (i.e., ViT) are following the same trend, achieving the best performance on challenging benchmarks. With the abundance of such unimodal models, a natural question arises; do we need also to follow this trend to tackle multimodal tasks? In this work, we propose to rather direct effort to efficient adaptations of existing models, and propose to augment Language Models with perception. Existing approaches for adapting pretrained models for vision-language tasks still rely on several key components that hinder their efficiency. In particular, they still train a large number of parameters, rely on large multimodal pretraining, use encoders (e.g., CLIP) trained on huge image-text datasets, and add significant inference overhead. In addition, most of these approaches have focused on Zero-Shot and In Context Learning, with l
    
[^101]: 通过深度度量学习方法结合实现航空场景分类

    Combining Deep Metric Learning Approaches for Aerial Scene Classification. (arXiv:2303.11389v1 [cs.CV])

    [http://arxiv.org/abs/2303.11389](http://arxiv.org/abs/2303.11389)

    本论文提出了六种深度度量学习方法来实现航空场景分类任务，并通过进化计算算法将它们结合起来。实验结果表明，这些方法与传统图像分类方法有类似的精度。

    

    航空场景分类是一项具有挑战性的遥感任务，旨在为一组预定义的类别（例如农业、海滩和港口）对遥感图像进行语义标签。由于数据集图像中所包含的对象具有不同的尺度和方向，因此该任务具有高度的类内变化。在遥感领域中，CNN架构的使用是场景分类任务的替代解决方案。通常，这些CNN被用于执行传统的图像分类任务。然而，另一个不太常用的远程感知图像分类方法可能是使用深度度量学习（DML）方法。因此，本文提出了使用六种DML方法进行航空场景分类任务的方法，并分析它们与四种不同的预训练CNN的行为，通过进化计算算法（UMDA）将它们进行结合。通过实验可观察到，DML方法可以达到与传统图像分类方法相竞争的精度，特别是当它们通过进化计算算法结合时。

    Aerial scene classification, which aims to semantically label remote sensing images in a set of predefined classes (e.g., agricultural, beach, and harbor), is a very challenging task in remote sensing due to high intra-class variability and the different scales and orientations of the objects present in the dataset images. In remote sensing area, the use of CNN architectures as an alternative solution is also a reality for scene classification tasks. Generally, these CNNs are used to perform the traditional image classification task. However, another less used way to classify remote sensing image might be the one that uses deep metric learning (DML) approaches. In this sense, this work proposes to employ six DML approaches for aerial scene classification tasks, analysing their behave with four different pre-trained CNNs as well as combining them through the use of evolutionary computation algorithm (UMDA). In performed experiments, it is possible to observe than DML approaches can achi
    
[^102]: MM-REACT：促进ChatGPT进行多模态推理和行动的系统范式

    MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action. (arXiv:2303.11381v1 [cs.CV])

    [http://arxiv.org/abs/2303.11381](http://arxiv.org/abs/2303.11381)

    MM-REACT是一种融合视觉专家和ChatGPT的多模态推理和行动系统，通过文字提示设计，实现了ChatGPT和各种视觉专家的协同，具有广泛的高级视觉理解应用价值。

    

    本文提出了MM-REACT，一种将ChatGPT与视觉专家池集成以实现多模态推理和行动的系统范式。我们定义并探索了一系列先进的视觉任务，这些任务很有趣，但可能超出了现有视觉和视觉语言模型的能力范围。为了实现这种高级视觉智能，MM-REACT引入了一种文本提示设计，可以表示文本描述、文本化的空间坐标和对齐的文件名，用于处理图像和视频等密集的视觉信号。MM-REACT的提示设计允许语言模型接受、关联和处理多模态信息，从而促进了ChatGPT和各种视觉专家的协同组合。零样本实验证明了MM-REACT解决感兴趣的特定能力以及在需要高级视觉理解的不同场景中具有广泛应用的有效性。此外，我们还讨论和比较了M

    We propose MM-REACT, a system paradigm that integrates ChatGPT with a pool of vision experts to achieve multimodal reasoning and action. In this paper, we define and explore a comprehensive list of advanced vision tasks that are intriguing to solve, but may exceed the capabilities of existing vision and vision-language models. To achieve such advanced visual intelligence, MM-REACT introduces a textual prompt design that can represent text descriptions, textualized spatial coordinates, and aligned file names for dense visual signals such as images and videos. MM-REACT's prompt design allows language models to accept, associate, and process multimodal information, thereby facilitating the synergetic combination of ChatGPT and various vision experts. Zero-shot experiments demonstrate MM-REACT's effectiveness in addressing the specified capabilities of interests and its wide application in different scenarios that require advanced visual understanding. Furthermore, we discuss and compare M
    
[^103]: 利用辅助不确定性进行的运算器学习来解决高维逆问题

    Solving High-Dimensional Inverse Problems with Auxiliary Uncertainty via Operator Learning with Limited Data. (arXiv:2303.11379v1 [stat.ML])

    [http://arxiv.org/abs/2303.11379](http://arxiv.org/abs/2303.11379)

    本文提出了一个基于深度神经网络代理模型进行流图校准的框架， 并结合先前辅助过程的知识来解决高维逆问题，成功识别了高维系统中的源。

    

    在复杂的大规模系统（如气候）中，重要的影响是由多种未被完全观察到的混淆过程相互作用引起的。根据系统状态的观察结果来识别出这些影响因素对于归因和预测至关重要，这些结果又为决策制定提供了重要的信息。这些逆问题的难点在于无法隔离影响因素和计算模型模拟的成本。我们引入了一个基于深度神经网络代理模型进行流图校准，并结合先前辅助过程的知识来解决有限数据下的高维逆问题的框架。我们的方法成功地在观察数据稀疏噪声较大的高维系统中确定出源，表现优于现有有限数据源识别方法。

    In complex large-scale systems such as climate, important effects are caused by a combination of confounding processes that are not fully observable. The identification of sources from observations of system state is vital for attribution and prediction, which inform critical policy decisions. The difficulty of these types of inverse problems lies in the inability to isolate sources and the cost of simulating computational models. Surrogate models may enable the many-query algorithms required for source identification, but data challenges arise from high dimensionality of the state and source, limited ensembles of costly model simulations to train a surrogate model, and few and potentially noisy state observations for inversion due to measurement limitations. The influence of auxiliary processes adds an additional layer of uncertainty that further confounds source identification. We introduce a framework based on (1) calibrating deep neural network surrogates to the flow maps provided 
    
[^104]: GNN-Ensemble：面向随机决策图神经网络

    GNN-Ensemble: Towards Random Decision Graph Neural Networks. (arXiv:2303.11376v1 [cs.LG])

    [http://arxiv.org/abs/2303.11376](http://arxiv.org/abs/2303.11376)

    本文提出了一种名为GNN-Ensemble的方法，它可以构建随机决策图神经网络的集合，以提高GNNs的性能，泛化能力和抗攻击性，并遵循随机建模的原则。

    

    图神经网络（GNNs）在图结构数据方面广泛应用，但是现有的基于图的应用通常缺乏注释数据。GNNs需要从有限的训练数据中学习潜在的模式，以对大量的测试数据进行推断。GNNs的增加复杂性，以及单点模型参数初始化，通常会导致过度适应和次优性能。此外，众所周知GNNs易受到对抗性攻击。在本文中，我们提出了一种名为GNN-Ensemble的新方法，该方法遵循随机建模的原则，在拓扑上随机选择子结构中构建多个GNNs来构建随机决策图神经网络，其容量可以任意扩展，以提高性能和对抗鲁棒性。

    Graph Neural Networks (GNNs) have enjoyed wide spread applications in graph-structured data. However, existing graph based applications commonly lack annotated data. GNNs are required to learn latent patterns from a limited amount of training data to perform inferences on a vast amount of test data. The increased complexity of GNNs, as well as a single point of model parameter initialization, usually lead to overfitting and sub-optimal performance. In addition, it is known that GNNs are vulnerable to adversarial attacks. In this paper, we push one step forward on the ensemble learning of GNNs with improved accuracy, generalization, and adversarial robustness. Following the principles of stochastic modeling, we propose a new method called GNN-Ensemble to construct an ensemble of random decision graph neural networks whose capacity can be arbitrarily expanded for improvement in performance. The essence of the method is to build multiple GNNs in randomly selected substructures in the topo
    
[^105]: 神经约束满足：层级抽象实现组合式物体重新排列中的泛化能力

    Neural Constraint Satisfaction: Hierarchical Abstraction for Combinatorial Generalization in Object Rearrangement. (arXiv:2303.11373v1 [cs.LG])

    [http://arxiv.org/abs/2303.11373](http://arxiv.org/abs/2303.11373)

    该研究提出了一种神经网络约束满足的方法，通过层级结构的抽象实现了在组合状态下的物体重新排列任务的泛化能力。

    

    对于具有实体机器人智能的问题而言，物体重新排列任务是一个巨大挑战，因为解决这些任务需要泛化到极大组合的实体状态。更糟糕的是，这些实体的表示是未知的，必须从感官输入进行推断。我们提出了一种层次抽象的方法，以揭示这些底层实体，并从非结构化视觉输入中实现组合泛化。通过在像素聚类上构建一个因子化的转换图，我们展示了如何学习代理模型中实体状态和环境物体之间的对应关系。我们利用这种关系开发了一种控制方法，可泛化到不同数量和配置的物体，当在模拟重新排列任务上进行评估时，该方法胜过当前的离线深度强化学习方法。

    Object rearrangement is a challenge for embodied agents because solving these tasks requires generalizing across a combinatorially large set of configurations of entities and their locations. Worse, the representations of these entities are unknown and must be inferred from sensory percepts. We present a hierarchical abstraction approach to uncover these underlying entities and achieve combinatorial generalization from unstructured visual inputs. By constructing a factorized transition graph over clusters of entity representations inferred from pixels, we show how to learn a correspondence between intervening on states of entities in the agent's model and acting on objects in the environment. We use this correspondence to develop a method for control that generalizes to different numbers and configurations of objects, which outperforms current offline deep RL methods when evaluated on simulated rearrangement tasks.
    
[^106]: 针对注意力状态分类的优化预处理和小型机器学习

    Optimized preprocessing and Tiny ML for Attention State Classification. (arXiv:2303.11371v1 [cs.LG])

    [http://arxiv.org/abs/2303.11371](http://arxiv.org/abs/2303.11371)

    本文提出了一种结合信号处理和机器学习算法的EEG信号处理方法，用于注意力状态分类，在性能和效率方面优于其他最先进的方法。

    

    本文通过结合信号处理技术和机器学习算法，提出了一种新的EEG信号处理方法，用于注意力状态分类。我们在一个认知负荷任务期间收集的EEG记录数据集上测试了所提出方法的性能，并将其与其他最先进的方法进行了比较。结果表明，所提出的方法能够高准确度地分类注意力状态，并在分类精度和计算效率方面优于最先进的方法。

    In this paper, we present a new approach to mental state classification from EEG signals by combining signal processing techniques and machine learning (ML) algorithms. We evaluate the performance of the proposed method on a dataset of EEG recordings collected during a cognitive load task and compared it to other state-of-the-art methods. The results show that the proposed method achieves high accuracy in classifying mental states and outperforms state-of-the-art methods in terms of classification accuracy and computational efficiency.
    
[^107]: 《桥接模仿学习和在线强化学习：一个乐观的故事》

    Bridging Imitation and Online Reinforcement Learning: An Optimistic Tale. (arXiv:2303.11369v1 [cs.LG])

    [http://arxiv.org/abs/2303.11369](http://arxiv.org/abs/2303.11369)

    本文提出了两种算法，iPSRL和iRLSVI，旨在解决给定离线演示数据集的问题，可以显著减少强化学习中的遗憾，桥接了在线 RL 和模仿学习。

    

    本文研究以下问题：给定一个来自不完美专家的离线演示数据集，最好的方式是什么来利用它来引导 MDP 中的在线学习表现。我们首先提出了一种基于知情后验采样的 RL（iPSRL）算法，它使用离线数据集和专家的行为策略信息来生成离线数据集。如果专家足够能干，则其累积贝叶斯遗憾在离线数据集大小 N 下会指数快速下降到零。由于该算法计算时间复杂度过高，我们随后提出了 iRLSVI 算法，可看作是在线 RL 和模仿学习的 RLSVI 算法的组合。我们的实验结果表明，与两个基准（没有离线数据，或使用离线数据集但不利用生成策略信息）相比，所提出的 iRLSVI 算法能够显著减少遗憾。我们的算法桥接了在线 RL 和模仿学习。

    In this paper, we address the following problem: Given an offline demonstration dataset from an imperfect expert, what is the best way to leverage it to bootstrap online learning performance in MDPs. We first propose an Informed Posterior Sampling-based RL (iPSRL) algorithm that uses the offline dataset, and information about the expert's behavioral policy used to generate the offline dataset. Its cumulative Bayesian regret goes down to zero exponentially fast in N, the offline dataset size if the expert is competent enough. Since this algorithm is computationally impractical, we then propose the iRLSVI algorithm that can be seen as a combination of the RLSVI algorithm for online RL, and imitation learning. Our empirical results show that the proposed iRLSVI algorithm is able to achieve significant reduction in regret as compared to two baselines: no offline data, and offline dataset but used without information about the generative policy. Our algorithm bridges online RL and imitation
    
[^108]: Reflexion：具有动态记忆和自我反思的自主智能体

    Reflexion: an autonomous agent with dynamic memory and self-reflection. (arXiv:2303.11366v1 [cs.AI])

    [http://arxiv.org/abs/2303.11366](http://arxiv.org/abs/2303.11366)

    本文提出 Reflexion 方法，给智能体赋予了动态记忆和自我反思能力，以增强其任务特定的行动选择能力。

    

    最近决策大型语言模型（LLM）代理的发展在各种基准测试中展现出卓越的性能。然而，这些最先进的方法通常需要内部模型微调、外部模型微调或在定义的状态空间上进行策略优化。由于高质量训练数据的稀缺性或缺乏良好定义的状态空间，实现这些方法可能会具有挑战性。此外，这些代理没有人类决策过程固有的某些品质，特别是从错误中学习的能力。通过反思，人类可以通过试错过程高效地解决新的问题。在最近的研究基础上，我们提出 Reflexion，一种将动态记忆和自我反思能力赋予智能体的方法，以增强其现有的推理轨迹和任务特定的行动选择能力。为了实现完全自动化，我们介绍了一种简单而有效的方法。

    Recent advancements in decision-making large language model (LLM) agents have demonstrated impressive performance across various benchmarks. However, these state-of-the-art approaches typically necessitate internal model fine-tuning, external model fine-tuning, or policy optimization over a defined state space. Implementing these methods can prove challenging due to the scarcity of high-quality training data or the lack of well-defined state space. Moreover, these agents do not possess certain qualities inherent to human decision-making processes, specifically the ability to learn from mistakes. Self-reflection allows humans to efficiently solve novel problems through a process of trial and error. Building on recent research, we propose Reflexion, an approach that endows an agent with dynamic memory and self-reflection capabilities to enhance its existing reasoning trace and task-specific action choice abilities. To achieve full automation, we introduce a straightforward yet effective 
    
[^109]: HDformer: 一种利用长距离血管信号进行糖尿病检测的高维Transformer

    HDformer: A Higher Dimensional Transformer for Diabetes Detection Utilizing Long Range Vascular Signals. (arXiv:2303.11340v1 [cs.LG])

    [http://arxiv.org/abs/2303.11340](http://arxiv.org/abs/2303.11340)

    本研究提出了一种新的基于高维Transformer的架构HDformer，并利用长距离PPG信号进行糖尿病检测，其中提出了一种新的注意力模块TSA，成功将标记体积减少10倍以上，提高了模型的能力和效率。

    

    糖尿病是全球性问题，早期检测有助于预防严重并发症。已出现将心血管信号纳入深度学习模型的低成本、非侵入式检测方法，但限制其临床应用的是有限的准确性。本文提出了一种新的基于Transformer的架构，即Higher Dimensional Transformer（HDformer），它利用长距离光电容积图（PPG）信号来检测糖尿病。相较于现有研究常用的不足一分钟的PPG信号，长距离PPG包含更广泛、更深入的信号上下文信息。为了增加处理长距离数据的能力和效率，我们提出了一种新的注意力模块Time Square Attention（TSA），将标记体积减少10倍以上，同时保留本地/全局依赖关系。它将一维输入 转换为二维表示，并将相邻点组成一个单独的2D标记。

    Diabetes mellitus is a worldwide concern, and early detection can help to prevent serious complications. Low-cost, non-invasive detection methods, which take cardiovascular signals into deep learning models, have emerged. However, limited accuracy constrains their clinical usage. In this paper, we present a new Transformer-based architecture, Higher Dimensional Transformer (HDformer), which takes long-range photoplethysmography (PPG) signals to detect diabetes. The long-range PPG contains broader and deeper signal contextual information compared to the less-than-one-minute PPG signals commonly utilized in existing research. To increase the capability and efficiency of processing the long range data, we propose a new attention module Time Square Attention (TSA), reducing the volume of the tokens by more than 10x, while retaining the local/global dependencies. It converts the 1-dimensional inputs into 2-dimensional representations and groups adjacent points into a single 2D token, using 
    
[^110]: FedMAE：带有单块遮蔽自编码器的联邦自监督学习

    FedMAE: Federated Self-Supervised Learning with One-Block Masked Auto-Encoder. (arXiv:2303.11339v1 [cs.LG])

    [http://arxiv.org/abs/2303.11339](http://arxiv.org/abs/2303.11339)

    本文提出了一个新的联邦自监督学习框架 FedMAE，可以利用轻量级设备上的大规模未标记图像进行联邦学习。FedMAE可以预训练一个单块遮蔽自编码器，并将多个预训练的单块MAE级联在服务器上构建用于下游任务的多块ViT骨干。实验结果表明，FedMAE相较于最先进的FSSL方法具有卓越的性能。

    

    最新的联邦学习方法开始关注如何利用用户设备中未标记的数据进行训练，原因是用户关注隐私，成本高，或者缺乏专业知识。然而，当前的Federated Semi-Supervised/Self-Supervised Learning（FSSL）方法由于本地客户端的有限计算资源而无法学习大规模图像。本文提出了一个新的框架FedMAE，即Federated Masked AutoEncoder，以解决如何利用未标记的大尺度图像进行联邦学习的问题。具体来说，FedMAE可以使用轻量级客户端设备中的大型图像预训练单块遮蔽自编码器（MAE），然后在服务器中级联多个预训练的单块MAE以构建下游任务的多块ViT骨干。 图像重建和分类的理论分析和实验结果表明，与最先进的FSSL方法相比，我们的FedMAE获得了更优秀的性能。

    Latest federated learning (FL) methods started to focus on how to use unlabeled data in clients for training due to users' privacy concerns, high labeling costs, or lack of expertise. However, current Federated Semi-Supervised/Self-Supervised Learning (FSSL) approaches fail to learn large-scale images because of the limited computing resources of local clients. In this paper, we introduce a new framework FedMAE, which stands for Federated Masked AutoEncoder, to address the problem of how to utilize unlabeled large-scale images for FL. Specifically, FedMAE can pre-train one-block Masked AutoEncoder (MAE) using large images in lightweight client devices, and then cascades multiple pre-trained one-block MAEs in the server to build a multi-block ViT backbone for downstream tasks. Theoretical analysis and experimental results on image reconstruction and classification show that our FedMAE achieves superior performance compared to the state-of-the-art FSSL methods.
    
[^111]: 面向心电图和脑电图分类的领域泛化：算法和基准

    Towards Domain Generalization for ECG and EEG Classification: Algorithms and Benchmarks. (arXiv:2303.11338v1 [eess.SP])

    [http://arxiv.org/abs/2303.11338](http://arxiv.org/abs/2303.11338)

    本论文提出了一个开源生物信号领域泛化评估基准，并引入一种专门解决生物信号中领域泛化问题的神经网络架构DGNet-Bio。通过实验证明，DGNet-Bio在ECG和EEG分类领域泛化上优于现有方法。

    

    尽管机器和深度学习系统在许多领域取得了巨大的成功，但它们尚未能够在医疗保健的关键任务中牢固地确立自己。其中一个主要原因在于，当模型面对之前未见过的分布之外的样本时，它们的性能会显著下降。这就是领域泛化（DG）问题。我们的目标是提出一个评估DG算法的基准，并引入一种新颖的架构来解决生物信号分类中的DG问题。在本文中，我们描述了生物信号的领域泛化问题，重点关注心电图（ECG）和脑电图（EEG），并提出并实现了一个开源生物信号领域泛化评估基准。此外，我们将计算机视觉领域的最先进DG算法改进为1D生物信号分类问题，并评估它们的有效性。最后，我们还介绍了一种新颖的神经网络架构，称为DGNet-Bio，专门设计用于解决生物信号中的DG问题。我们的实验表明，DGNet-Bio在新提出的ECG和EEG分类领域泛化基准上优于现有方法。

    Despite their immense success in numerous fields, machine and deep learning systems have not have not yet been able to firmly establish themselves in mission-critical applications in healthcare. One of the main reasons lies in the fact that when models are presented with previously unseen, Out-of-Distribution samples, their performance deteriorates significantly. This is known as the Domain Generalization (DG) problem. Our objective in this work is to propose a benchmark for evaluating DG algorithms, in addition to introducing a novel architecture for tackling DG in biosignal classification. In this paper, we describe the Domain Generalization problem for biosignals, focusing on electrocardiograms (ECG) and electroencephalograms (EEG) and propose and implement an open-source biosignal DG evaluation benchmark. Furthermore, we adapt state-of-the-art DG algorithms from computer vision to the problem of 1D biosignal classification and evaluate their effectiveness. Finally, we also introduc
    
[^112]: 递归欧几里得距离基于鲁棒聚合技术的联邦学习方法

    Recursive Euclidean Distance Based Robust Aggregation Technique For Federated Learning. (arXiv:2303.11337v1 [cs.LG])

    [http://arxiv.org/abs/2303.11337](http://arxiv.org/abs/2303.11337)

    本文提出了一种递归欧几里得距离计算的鲁棒聚合方法来防御联邦学习中的恶意攻击，该方法分配权重以最小化数据污染效应，实验表明其精度优于现有算法并且时间复杂度降低。

    

    联邦学习作为解决机器学习中数据可用性和隐私挑战的解决方案已经变得非常受欢迎。然而，在联邦学习中，本地模型更新的聚合过程容易受到恶意攻击，如背包攻击、标签翻转和成员推断。恶意用户旨在通过恶意数据训练本地模型来破坏合作学习过程。本文提出了一种基于递归欧几里得距离计算的新型鲁棒聚合方法。我们的方法测量本地模型与之前全局模型的距离，并相应地分配权重。远离全局模型的本地模型被分配较小的权重，以最小化聚合过程中的数据污染效应。我们的实验表明，所提出的算法在精度方面的表现优于现有算法，且时间复杂度减少不超过$55\%$。我们的贡献有两个：1）我们提出了一种新的递归欧几里得距离聚合技术，以防范联邦学习中的恶意攻击。2）我们的实验证明了所提出算法的精度和时间复杂度的有效性。

    Federated learning has gained popularity as a solution to data availability and privacy challenges in machine learning. However, the aggregation process of local model updates to obtain a global model in federated learning is susceptible to malicious attacks, such as backdoor poisoning, label-flipping, and membership inference. Malicious users aim to sabotage the collaborative learning process by training the local model with malicious data. In this paper, we propose a novel robust aggregation approach based on recursive Euclidean distance calculation. Our approach measures the distance of the local models from the previous global model and assigns weights accordingly. Local models far away from the global model are assigned smaller weights to minimize the data poisoning effect during aggregation. Our experiments demonstrate that the proposed algorithm outperforms state-of-the-art algorithms by at least $5\%$ in accuracy while reducing time complexity by less than $55\%$. Our contribut
    
[^113]: 研究用于基因表达模型的 Integrated Gradients 的可解释性极限

    Studying Limits of Explainability by Integrated Gradients for Gene Expression Models. (arXiv:2303.11336v1 [q-bio.GN])

    [http://arxiv.org/abs/2303.11336](http://arxiv.org/abs/2303.11336)

    本文研究了使用 Integrated Gradients 进行机器学习特征归属时的可解释性极限，证明仅通过特征影响值的排序无法可靠地识别生物标志物的重要性，还提出了新的评估方法以评估可靠性。

    

    理解驱动细胞生命周期的分子过程是生物学研究中的一个基本问题。最近的工作采用了监督式机器学习方法以解密复杂的细胞相互作用，将科学问题公式化为标签数据或图形上的经典学习问题，例如来自基因表达数据的表型预测。在这些工作中，个体预测的输入特征经常被解释为表型成因的指示性标志，例如癌症识别。本文提出探讨 Integrated Gradients 在机器学习特征归属中所鉴别到的生物标志物的相关性。通过在癌症基因组图谱上的一个令人激动的案例，我们展示了仅通过特征影响值的排序并不能够可靠地识别生物标志物的重要性。在难以评估特征归属的正确性的情况下，我们提出了新的方法来评估可解释性的极限，即在给定方法的输出的情况下我们可以可靠地得出多少有关生物标志物相关性的结论。

    Understanding the molecular processes that drive cellular life is a fundamental question in biological research. Ambitious programs have gathered a number of molecular datasets on large populations. To decipher the complex cellular interactions, recent work has turned to supervised machine learning methods. The scientific questions are formulated as classical learning problems on tabular data or on graphs, e.g. phenotype prediction from gene expression data. In these works, the input features on which the individual predictions are predominantly based are often interpreted as indicative of the cause of the phenotype, such as cancer identification. Here, we propose to explore the relevance of the biomarkers identified by Integrated Gradients, an explainability method for feature attribution in machine learning. Through a motivating example on The Cancer Genome Atlas, we show that ranking features by importance is not enough to robustly identify biomarkers. As it is difficult to evaluate
    
[^114]: 将可逆神经网络作为自动编码器进行训练

    Training Invertible Neural Networks as Autoencoders. (arXiv:2303.11239v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.11239](http://arxiv.org/abs/2303.11239)

    本文提出了使用INN自动编码器进行训练的方法，实验证明其在大瓶颈大小的情况下优于经典自动编码器。

    

    自动编码器能够以无监督的方式学习有用的数据表示，并已被广泛应用于各种机器学习和计算机视觉任务中。在本文中，我们提出了一种方法，将可逆神经网络（INN）作为（变分）自动编码器进行训练，称为INN（变分）自动编码器。我们在MNIST，CIFAR和CelebA数据集上的实验表明，在瓶颈大小较小的情况下，我们的INN自动编码器实现了与经典自动编码器类似的结果。然而，在大瓶颈大小的情况下，我们的INN自动编码器优于其经典对应物。基于实证结果，我们假设INN自动编码器可能没有任何固有信息损失，因此不受最大层数（深度）的限制，达到该层数后只能实现次优结果。

    Autoencoders are able to learn useful data representations in an unsupervised matter and have been widely used in various machine learning and computer vision tasks. In this work, we present methods to train Invertible Neural Networks (INNs) as (variational) autoencoders which we call INN (variational) autoencoders. Our experiments on MNIST, CIFAR and CelebA show that for low bottleneck sizes our INN autoencoder achieves results similar to the classical autoencoder. However, for large bottleneck sizes our INN autoencoder outperforms its classical counterpart. Based on the empirical results, we hypothesize that INN autoencoders might not have any intrinsic information loss and thereby are not bounded to a maximal number of layers (depth) after which only suboptimal results can be achieved.
    
[^115]: 不看就能旋转: 通过触觉实现手部灵活性

    Rotating without Seeing: Towards In-hand Dexterity through Touch. (arXiv:2303.10880v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2303.10880](http://arxiv.org/abs/2303.10880)

    本研究提出了一种新系统Touch Dexterity，通过密集二进制力传感器实现了多指机器人手不看就能旋转物体，同时大大降低了成本和与实际应用的差距。

    

    触感信息在人类灵巧性中扮演着至关重要的角色，它可以提供有用的接触信息，直接从视觉中无法推断。这篇论文探讨了是否能够使多指机器人手具备与人类类似的不看就能旋转物体的能力。作者们提出了一个新的系统Touch Dexterity，通过使用覆盖整个机器人手的密集二进制力传感器（触摸或未触摸）代替仅仅在小区域内进行精准的触觉传感，使系统具有低成本、覆盖范围广等优点，并通过强化学习在多样的物体模拟中训练出了一种触感旋转策略，能够在真实的机器人手上直接实施不看就能旋转新型物体。

    Tactile information plays a critical role in human dexterity. It reveals useful contact information that may not be inferred directly from vision. In fact, humans can even perform in-hand dexterous manipulation without using vision. Can we enable the same ability for the multi-finger robot hand? In this paper, we present Touch Dexterity, a new system that can perform in-hand object rotation using only touching without seeing the object. Instead of relying on precise tactile sensing in a small region, we introduce a new system design using dense binary force sensors (touch or no touch) overlaying one side of the whole robot hand (palm, finger links, fingertips). Such a design is low-cost, giving a larger coverage of the object, and minimizing the Sim2Real gap at the same time. We train an in-hand rotation policy using Reinforcement Learning on diverse objects in simulation. Relying on touch-only sensing, we can directly deploy the policy in a real robot hand and rotate novel objects tha
    
[^116]: 脉冲神经网络综述：解释、优化、效率和最佳实践

    A Comprehensive Review of Spiking Neural Networks: Interpretation, Optimization, Efficiency, and Best Practices. (arXiv:2303.10780v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2303.10780](http://arxiv.org/abs/2303.10780)

    本文综述了最新的脉冲神经网络的解释、优化、效率和精度的进展，介绍了最前沿的优化、节能和评价方法，方便新从业者理解。

    

    生物神经网络持续激发着神经网络性能的突破。然而，一个被低估和未经调查的神经计算领域是符合生物学、高效能的脉冲神经网络。它们的潜力在尤其适合低功耗、移动或其他硬件受限的环境中。本文综述了脉冲神经网络的解释、优化、效率和精度的最新进展。主要贡献包括识别、讨论和比较最前沿的脉冲神经网络优化、节能和评价方法，从基本原理出发，方便新从业者理解。

    Biological neural networks continue to inspire breakthroughs in neural network performance. And yet, one key area of neural computation that has been under-appreciated and under-investigated is biologically plausible, energy-efficient spiking neural networks, whose potential is especially attractive for low-power, mobile, or otherwise hardware-constrained settings. We present a literature review of recent developments in the interpretation, optimization, efficiency, and accuracy of spiking neural networks. Key contributions include identification, discussion, and comparison of cutting-edge methods in spiking neural network optimization, energy-efficiency, and evaluation, starting from first principles so as to be accessible to new practitioners.
    
[^117]: 广义划分局部深度

    Generalized partitioned local depth. (arXiv:2303.10167v1 [stat.ML])

    [http://arxiv.org/abs/2303.10167](http://arxiv.org/abs/2303.10167)

    本文提出了一个广义的凝聚概念，构建在分区局部深度的技术基础上，扩展了早期结果并应用于具有不确定性的数据的社区发现中。

    

    本文提供了一个最近由Berenhaut、Moore和Melvin [Proccedings of the National Academy of Sciences, 119 (4) (2022)]提出的凝聚概念的概括。所提出的表述基于分区局部深度的技术并提炼了两个关键概率概念：局部相关性和支持分割。早期结果在新的背景下得到扩展，并包括在具有不确定性的数据中揭示社区的应用示例。

    In this paper we provide a generalization of the concept of cohesion as introduced recently by Berenhaut, Moore and Melvin [Proceedings of the National Academy of Sciences, 119 (4) (2022)]. The formulation presented builds on the technique of partitioned local depth by distilling two key probabilistic concepts: local relevance and support division. Earlier results are extended within the new context, and examples of applications to revealing communities in data with uncertainty are included.
    
[^118]: 基于图表示学习的高效可行的机器人装配序列规划

    Efficient and Feasible Robotic Assembly Sequence Planning via Graph Representation Learning. (arXiv:2303.10135v1 [cs.RO])

    [http://arxiv.org/abs/2303.10135](http://arxiv.org/abs/2303.10135)

    本文提出了一种基于图表示学习的装配序列规划方法，通过GRACE模型可以从装配图中提取信息并预测可行的装配序列。

    

    自动机器人装配序列规划（RASP）可以显著提高现代制造业的生产力和适应力，随着对更大量化生产需求的不断增长。实现这种自动化的主要挑战之一在于从不断增加的潜在序列中高效地找到解决方案，进行越来越复杂的装配还需要成本昂贵的可行性检查。为了解决这个问题，我们提出了一种包括产品装配图的图形方法和一个名为GRACE的策略架构，用于装配序列生成。其次，我们使用GRACE从图形输入中提取有意义的信息，并逐步预测装配序列。在实验中，我们展示了我们的方法可以根据在模拟中收集的数据，预测铝型材产品变体的可行装配序列。

    Automatic Robotic Assembly Sequence Planning (RASP) can significantly improve productivity and resilience in modern manufacturing along with the growing need for greater product customization. One of the main challenges in realizing such automation resides in efficiently finding solutions from a growing number of potential sequences for increasingly complex assemblies. Besides, costly feasibility checks are always required for the robotic system. To address this, we propose a holistic graphical approach including a graph representation called Assembly Graph for product assemblies and a policy architecture, Graph Assembly Processing Network, dubbed GRACE for assembly sequence generation. Secondly, we use GRACE to extract meaningful information from the graph input and predict assembly sequences in a step-by-step manner. In experiments, we show that our approach can predict feasible assembly sequences across product variants of aluminum profiles based on data collected in simulation of a
    
[^119]: 物理知识神经网络拓扑优化：应用于隐藏几何结构的非侵入式探测。

    Topology optimization with physics-informed neural networks: application to noninvasive detection of hidden geometries. (arXiv:2303.09280v1 [cs.LG])

    [http://arxiv.org/abs/2303.09280](http://arxiv.org/abs/2303.09280)

    该论文介绍了一种基于物理知识神经网络的拓扑优化方法，应用于无先验知识的几何结构检测，通过材料密度场表示任意解决方案拓扑，并通过Eikonal正则化实现。该方法可用于医疗和工业应用中的非侵入式成像技术。

    

    在医疗和工业应用中，通过电磁、声学或机械负载从表面测量中检测隐藏的几何结构是非侵入成像技术的目标。由于未知的拓扑和几何形状、数据的稀疏性以及物理规律的复杂性，解决逆问题是具有挑战性的。物理知识神经网络已经表现出许多优点，是一个简单而强大的问题反演工具，但它们尚未应用于具有先验未知拓扑的一般问题。在这里，我们介绍了一个基于PINNs的拓扑优化框架，它可以解决没有形状数量或类型先验知识的几何检测问题。我们允许任意的解决方案拓扑，通过使用材料密度场来表示几何形状，并通过新的Eikonal正则化接近二进制值。我们通过检测隐含虚空和包含物的数量、位置和形状来验证我们的框架。

    Detecting hidden geometrical structures from surface measurements under electromagnetic, acoustic, or mechanical loading is the goal of noninvasive imaging techniques in medical and industrial applications. Solving the inverse problem can be challenging due to the unknown topology and geometry, the sparsity of the data, and the complexity of the physical laws. Physics-informed neural networks (PINNs) have shown promise as a simple-yet-powerful tool for problem inversion, but they have yet to be applied to general problems with a priori unknown topology. Here, we introduce a topology optimization framework based on PINNs that solves geometry detection problems without prior knowledge of the number or types of shapes. We allow for arbitrary solution topology by representing the geometry using a material density field that approaches binary values thanks to a novel eikonal regularization. We validate our framework by detecting the number, locations, and shapes of hidden voids and inclusio
    
[^120]: 基于t-SPN和滤波的细胞分类的最大间隔学习

    Maximum Margin Learning of t-SPNs for Cell Classification with Filtering. (arXiv:2303.09065v1 [cs.LG])

    [http://arxiv.org/abs/2303.09065](http://arxiv.org/abs/2303.09065)

    本研究提出了一种基于t-SPN算法和滤波技术的细胞分类方法，通过最大化边缘和L2正则化，该方法在HEp-2和Feulgen基准数据集上取得了最高的准确率。

    

    本文探讨了一种基于深度概率体系结构的算法，称为树形求和产品网络(t-SPN)，用于细胞分类。构建t-SPN的目的是表示未归一化概率作为最相似的细胞类别的条件概率。通过最大化边缘来学习构建的t-SPN体系结构，该边缘是真实标签和最有竞争力的错误标签之间的条件概率差。为了增强体系结构的泛化能力，在学习过程中考虑了L2正则化（REG）和最大间隔（MM）标准。为了突出细胞特征，本文探讨了两种通用的高通滤波器的有效性：理想高通滤波和拉普拉斯滤波(Log)。在HEp-2和Feulgen基准数据集上，基于最大间隔准则与正则化学习的t-SPN体系结构产生了最高的准确率。

    An algorithm based on a deep probabilistic architecture referred to as a tree-structured sum-product network (t-SPN) is considered for cell classification. The t-SPN is constructed such that the unnormalized probability is represented as conditional probabilities of a subset of most similar cell classes. The constructed t-SPN architecture is learned by maximizing the margin, which is the difference in the conditional probability between the true and the most competitive false label. To enhance the generalization ability of the architecture, L2-regularization (REG) is considered along with the maximum margin (MM) criterion in the learning process. To highlight cell features, this paper investigates the effectiveness of two generic high-pass filters: ideal high-pass filtering and the Laplacian of Gaussian (LOG) filtering. On both HEp-2 and Feulgen benchmark datasets, the t-SPN architecture learned based on the max-margin criterion with regularization produced the highest accuracy rate co
    
[^121]: 分布式数据中心中硬盘寿命大规模预测

    Large-scale End-of-Life Prediction of Hard Disks in Distributed Datacenters. (arXiv:2303.08955v1 [cs.LG])

    [http://arxiv.org/abs/2303.08955](http://arxiv.org/abs/2303.08955)

    本文通过定制特征工程和序列学习器，提出一种编码器-解码器LSTM模型，用于大规模预测硬盘剩余寿命，从而降低运营成本

    

    数据中心每天处理海量数据，这些数据存储在价格便宜的硬盘中，用于金融、医疗和航天等重要领域。硬盘的过早损坏和数据的丢失可能会造成灾难性的后果。为了降低故障的风险，云存储提供商执行基于条件的监控并在故障之前更换硬盘。通过估计硬盘剩余寿命，可以预测特定设备的故障时间并在合适的时间内替换硬盘，确保最大利用率同时降低运营成本。本文使用定制的特征工程和一套序列学习器对极度偏斜的健康统计数据进行大规模预测分析。过去的工作表明，使用LSTM预测剩余寿命是一个很好的方法。因此，我们提出了一种编码器-解码器LSTM模型

    On a daily basis, data centers process huge volumes of data backed by the proliferation of inexpensive hard disks. Data stored in these disks serve a range of critical functional needs from financial, and healthcare to aerospace. As such, premature disk failure and consequent loss of data can be catastrophic. To mitigate the risk of failures, cloud storage providers perform condition-based monitoring and replace hard disks before they fail. By estimating the remaining useful life of hard disk drives, one can predict the time-to-failure of a particular device and replace it at the right time, ensuring maximum utilization whilst reducing operational costs. In this work, large-scale predictive analyses are performed using severely skewed health statistics data by incorporating customized feature engineering and a suite of sequence learners. Past work suggests using LSTMs as an excellent approach to predicting remaining useful life. To this end, we present an encoder-decoder LSTM model whe
    
[^122]: WDiscOOD：通过白化线性判别分析进行区分度优化的OOD检测

    WDiscOOD: Out-of-Distribution Detection via Whitened Linear Discriminative Analysis. (arXiv:2303.07543v1 [cs.CV])

    [http://arxiv.org/abs/2303.07543](http://arxiv.org/abs/2303.07543)

    本论文提出了一种名为WDiscOOD的新型OOD检测方法，其中使用白化线性判别分析将特征投影到判别子空间和残留子空间中，确定OOD分数。在大规模ImageNet-1k基准测试和六个OOD数据集中，WDiscOOD表现出了优越的性能。

    

    深度神经网络容易在遇到未知概念的情形下产生过度自信但错误的预测。这个挑战突显了在开放世界中检测OOD样本的重要性。本文提出了一种新颖的特征空间OOD检测分数，同时结合了类别特定和类别不可知的信息。具体地，我们的方法使用白化线性判别分析将特征投影到两个子空间中——判别子空间和残留子空间，其中ID类在判别子空间中被最大化地分离，并在残差子空间中被紧密地聚类。然后，在两个子空间中将来自输入数据与ID分布的偏差组合起来确定OOD分数。我们的方法名为WDiscOOD，在覆盖多种分布偏移的六个OOD数据集上验证了其高效性，包括大规模ImageNet-1k基准测试。WDiscOOD在深度分类器上表现出了优越的性能。

    Deep neural networks are susceptible to generating overconfident yet erroneous predictions when presented with data beyond known concepts. This challenge underscores the importance of detecting out-of-distribution (OOD) samples in the open world. In this work, we propose a novel feature-space OOD detection score that jointly reasons with both class-specific and class-agnostic information. Specifically, our approach utilizes Whitened Linear Discriminative Analysis to project features into two subspaces - the discriminative and residual subspaces - in which the ID classes are maximally separated and closely clustered, respectively. The OOD score is then determined by combining the deviation from the input data to the ID distribution in both subspaces. The efficacy of our method, named WDiscOOD, is verified on the large-scale ImageNet-1k benchmark, with six OOD datasets that covers a variety of distribution shifts. WDiscOOD demonstrates superior performance on deep classifiers with divers
    
[^123]: 具有别名观测的潜在图的快速探索与学习

    Fast exploration and learning of latent graphs with aliased observations. (arXiv:2303.07397v1 [cs.LG])

    [http://arxiv.org/abs/2303.07397](http://arxiv.org/abs/2303.07397)

    本文介绍了一种在具有别名观测的潜在图上，能够显著提高最大化探索效率的政策算法 eFeX，相比于随机策略，该算法能够更快地恢复各种拓扑结构下的图表。

    

    考虑这种场景：一个智能体通过执行操作从一个节点到另一个节点来导航潜在图。所选操作确定了下一个访问节点上的概率分布。在每个节点处，智能体收到一个观测，但该观测不是唯一的，因此它不能唯一地标识节点，这使得问题别名化。本文旨在提供一个政策，该政策约等于最大化探索效率（即在给定的探索预算下如何恢复图表）。在非别名化的情况下，我们展示了相对于现有最先进强化学习基线的改进性能。对于别名化的情况，我们不知道适用的基线，而是展示了在各种拓扑结构下相对于随机策略更快的恢复速度，并且对于具有挑战性的拓扑结构，恢复速度比随机策略快指数倍。我们将该算法称为 eFeX（来自于 efficient exploration 的缩写）。

    Consider this scenario: an agent navigates a latent graph by performing actions that take it from one node to another. The chosen action determines the probability distribution over the next visited node. At each node, the agent receives an observation, but this observation is not unique, so it does not identify the node, making the problem aliased. The purpose of this work is to provide a policy that approximately maximizes exploration efficiency (i.e., how well the graph is recovered for a given exploration budget). In the unaliased case, we show improved performance w.r.t. state-of-the-art reinforcement learning baselines. For the aliased case we are not aware of suitable baselines and instead show faster recovery w.r.t. a random policy for a wide variety of topologies, and exponentially faster recovery than a random policy for challenging topologies. We dub the algorithm eFeX (from eFficient eXploration).
    
[^124]: 从贝叶斯决策理论的角度看待长尾分类

    Long-tailed Classification from a Bayesian-decision-theory Perspective. (arXiv:2303.06075v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06075](http://arxiv.org/abs/2303.06075)

    本文从贝叶斯决策理论的角度提出了一个通用且有原则的框架，为长尾分类提供了理论上的支持，并采用综合风险和贝叶斯深度集成方法以提高所有类别的准确性，特别是“长尾”类别。

    

    长尾分类由于类别概率的严重不平衡和对称错误预测成本存在尾部敏感风险而面临挑战。最近的尝试采用重新平衡损失和集成方法，但它们很大程度上是启发式的，并且严重依赖经验结果，缺乏理论解释。此外，现有方法忽略了决策损失，它刻画了与尾部类别相关的不同成本。本文提出了一个从贝叶斯决策理论的角度看待长尾分类的通用且有原则的框架，它统一了包括重新平衡和集成方法在内的现有技术，并为它们的有效性提供了理论上的证明。从这个角度看，我们基于综合风险导出了一个新的目标和一个贝叶斯深度集成方法，以提高所有类别的准确性，特别是“长尾”类别的准确性。此外，我们的框架允许任务自适应决策损失，从而提供了在不同任务中可证明的最优决策。

    Long-tailed classification poses a challenge due to its heavy imbalance in class probabilities and tail-sensitivity risks with asymmetric misprediction costs. Recent attempts have used re-balancing loss and ensemble methods, but they are largely heuristic and depend heavily on empirical results, lacking theoretical explanation. Furthermore, existing methods overlook the decision loss, which characterizes different costs associated with tailed classes. This paper presents a general and principled framework from a Bayesian-decision-theory perspective, which unifies existing techniques including re-balancing and ensemble methods, and provides theoretical justifications for their effectiveness. From this perspective, we derive a novel objective based on the integrated risk and a Bayesian deep-ensemble approach to improve the accuracy of all classes, especially the "tail". Besides, our framework allows for task-adaptive decision loss which provides provably optimal decisions in varying task
    
[^125]: ChatGPT已在地平线上：大语言模型是否就是我们需要的智能交通解决方案？

    ChatGPT Is on the Horizon: Could a Large Language Model Be All We Need for Intelligent Transportation?. (arXiv:2303.05382v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.05382](http://arxiv.org/abs/2303.05382)

    本文探讨了ChatGPT在解决交通问题方面的应用。通过利用具有跨模态编码器的LLM，可以处理来自不同模态的交通数据并执行交通运营。作者提供了一个基于智能手机的碰撞报告自动生成和分析框架作为用例展示了这种潜力。

    

    ChatGPT是由OpenAI开发的具有60亿参数的重要大语言模型之一。ChatGPT展示了LLM的卓越的语言理解能力，特别是在生成对话响应方面。随着LLM在各种研究或工程领域越来越受到关注，现在是时候设想LLM如何革新我们处理智能交通系统的方式了。本文探讨了LLM在解决关键交通问题方面的未来应用。通过利用具有跨模态编码器的LLM，智能系统还可以处理来自不同模态的交通数据并通过LLM执行交通运营。我们提出并验证了LLM装备的这些潜在的交通应用。为了进一步证明这种潜力，我们还提供了一个具体的基于智能手机的碰撞报告自动生成和分析框架作为用例。尽管存在潜在的益处，但与数据隐私相关的挑战仍然存在。

    ChatGPT, developed by OpenAI, is one of the milestone large language models (LLMs) with 6 billion parameters. ChatGPT has demonstrated the impressive language understanding capability of LLM, particularly in generating conversational response. As LLMs start to gain more attention in various research or engineering domains, it is time to envision how LLM may revolutionize the way we approach intelligent transportation systems. This paper explores the future applications of LLM in addressing key transportation problems. By leveraging LLM with cross-modal encoder, an intelligent system can also process traffic data from different modalities and execute transportation operations through an LLM. We present and validate these potential transportation applications equipped by LLM. To further demonstrate this potential, we also provide a concrete smartphone-based crash report auto-generation and analysis framework as a use case. Despite the potential benefits, challenges related to data privac
    
[^126]: 策略镜像下降方法固有地探索行动空间

    Policy Mirror Descent Inherently Explores Action Space. (arXiv:2303.04386v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04386](http://arxiv.org/abs/2303.04386)

    本文研究了在线策略梯度方法如何固有地探索行动空间，并首次确定了不需要探索策略的情况下，这种方法的最优样本复杂度，进一步展示了明确的行动空间探索并不一定是必要条件。

    

    本文研究了如何在没有加入探索策略的情况下，采用在线策略梯度方法来解决有限状态和行动空间下的通用增强学习问题，原以为在行动空间内进行显式探索是不可或缺的，以避免样本复杂度的剧烈降低。本文首次确定了在线策略梯度方法的$\tilde{\mathcal{O}}(1/\epsilon^2)$的样本复杂度，并提出了两个新的策略评估算子和一种新的分析随机策略镜像下降方法（ SPMD）。结果表明，SPMD通过其在线评估算子固有地探索行动空间，明确的行动空间探索并不一定是策略梯度方法实现最佳样本复杂度的必要条件。其中一个评估算子称为基于价值函数的估计，另一个评估算子称为基于策略的估计。

    Explicit exploration in the action space was assumed to be indispensable for online policy gradient methods to avoid a drastic degradation in sample complexity, for solving general reinforcement learning problems over finite state and action spaces. In this paper, we establish for the first time an $\tilde{\mathcal{O}}(1/\epsilon^2)$ sample complexity for online policy gradient methods without incorporating any exploration strategies. The essential development consists of two new on-policy evaluation operators and a novel analysis of the stochastic policy mirror descent method (SPMD). SPMD with the first evaluation operator, called value-based estimation, tailors to the Kullback-Leibler divergence. Provided the Markov chains on the state space of generated policies are uniformly mixing with non-diminishing minimal visitation measure, an $\tilde{\mathcal{O}}(1/\epsilon^2)$ sample complexity is obtained with a linear dependence on the size of the action space. SPMD with the second evalua
    
[^127]: QAID：启发式的Few-shot意图检测方法

    QAID: Question Answering Inspired Few-shot Intent Detection. (arXiv:2303.01593v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.01593](http://arxiv.org/abs/2303.01593)

    本文提出了一个启发式的Few-shot意图检测方法，通过将意图检测重新定义为一个问题-回答检索任务来解决语义相似的细粒度意图问题，结果在三个few-shot意图检测基准测试上取得了最优表现。

    

    意图检测涉及到一些语义相似的细粒度意图，是一个具有挑战性的任务。为了解决这个问题，我们将意图检测重新定义为一个问题-回答检索任务，将话语和意图名作为问题和答案。为此，我们利用了一个问题-回答检索体系结构，并采用了一个两阶段培训模式，其中包括批量对比损失。在预训练阶段，我们通过自我监督培训来改善查询表示。然后，在微调阶段中，我们增加了查询和同一意图答案之间的上下文化令牌级相似度分数。我們在三个few-shot意图检测基准测试上的结果达到了最优表现。

    Intent detection with semantically similar fine-grained intents is a challenging task. To address it, we reformulate intent detection as a question-answering retrieval task by treating utterances and intent names as questions and answers. To that end, we utilize a question-answering retrieval architecture and adopt a two stages training schema with batch contrastive loss. In the pre-training stage, we improve query representations through self-supervised training. Then, in the fine-tuning stage, we increase contextualized token-level similarity scores between queries and answers from the same intent. Our results on three few-shot intent detection benchmarks achieve state-of-the-art performance.
    
[^128]: 基于全同态加密的隐私保护树型推理

    Privacy-Preserving Tree-Based Inference with Fully Homomorphic Encryption. (arXiv:2303.01254v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2303.01254](http://arxiv.org/abs/2303.01254)

    本研究介绍了一种基于全同态加密的数据隐私保护方法，能够针对加密表格数据进行任意计算，并得到了最新的解决方案，适用于一系列树型模型，包括决策树，随机森林和梯度增强树。此方法已应用在Concrete-ML开源库中，能够在准确性方面接近未受保护的版本。

    

    隐私增强技术(PETs)被提出作为一种保护数据隐私同时允许数据分析的方式。在本文中，我们关注一种强大的工具——全同态加密(FHE)，它允许对加密数据进行任意计算。我们展示了如何将FHE应用于基于树型模型的数据分析中，得到了针对加密表格数据的最新解决方案。我们证明了该方法适用于一系列树型模型，包括决策树，随机森林和梯度增强树，并已实现在Concrete-ML库中，该库在https://github.com/zama-ai/concrete-ml. 开源。通过选择一组应用案例，我们证明了我们的FHE版本在准确性方面非常接近未受保护的版本。

    Privacy enhancing technologies (PETs) have been proposed as a way to protect the privacy of data while still allowing for data analysis. In this work, we focus on Fully Homomorphic Encryption (FHE), a powerful tool that allows for arbitrary computations to be performed on encrypted data. FHE has received lots of attention in the past few years and has reached realistic execution times and correctness.  More precisely, we explain in this paper how we apply FHE to tree-based models and get state-of-the-art solutions over encrypted tabular data. We show that our method is applicable to a wide range of tree-based models, including decision trees, random forests, and gradient boosted trees, and has been implemented within the Concrete-ML library, which is open-source at https://github.com/zama-ai/concrete-ml. With a selected set of use-cases, we demonstrate that our FHE version is very close to the unprotected version in terms of accuracy.
    
[^129]: Opto-UNet：用于光学相干断层扫描中静脉曲张分割的优化UNet

    Opto-UNet: Optimized UNet for Segmentation of Varicose Veins in Optical Coherence Tomography. (arXiv:2302.14808v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2302.14808](http://arxiv.org/abs/2302.14808)

    本文提出了一个新的分割模型Opto-UNet，用于分割静脉壁结构，并采用U-Net架构搭建，使用孔洞和可分离卷积提取空间广范围和可分离的特征映射。

    

    人类静脉对于将血液从身体各部位输送至心脏具有重要作用。静脉疾病可能导致静脉功能不良。其中一种疾病是静脉曲张，血液可能会发生反流，导致静脉压力增加或由于静脉结构的变化而导致血流受限。为了检查静脉曲张的功能特征，研究静脉壁的物理和生物力学特性至关重要。本文提出了一个分割模型Opto-UNet，用于分割静脉壁结构。采用光学相干断层扫描系统获取静脉曲张图像。由于提取的静脉形状不均匀，因此需要适当的分割方法来分割静脉壁。Opto-UNet模型基于U-Net架构，其中整合了一个新的块，采用孔洞和可分离卷积来提取空间广范围和可分离的特征映射。

    Human veins are important for carrying the blood from the body-parts to the heart. The improper functioning of the human veins may arise from several venous diseases. Varicose vein is one such disease wherein back flow of blood can occur, often resulting in increased venous pressure or restricted blood flow due to changes in the structure of vein. To examine the functional characteristics of the varicose vein, it is crucial to study the physical and bio mechanical properties of the vein. This work proposes a segmentation model Opto-UNet, for segmenting the venous wall structure. Optical Coherence Tomography system is used to acquire images of varicose vein. As the extracted vein is not uniform in shape, hence adequate method of segmentation is required to segment the venous wall. Opto-UNet model is based on the U-Net architecture wherein a new block is integrated into the architecture, employing atrous and separable convolution to extract spatially wide-range and separable features map
    
[^130]: Vid2Seq: 用于密集视频字幕的视觉语言模型的大规模预训练

    Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning. (arXiv:2302.14115v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.14115](http://arxiv.org/abs/2302.14115)

    本文介绍了 Vid2Seq，这是一个在大规模 narrated 视频数据集上预先训练的多模态密集事件字幕模型。通过将转录语音的句子边界转化为伪事件边界，并使用转录语音句子作为伪事件字幕，我们有效利用未标注 narrated 视频数据集进行了密集视频字幕的训练。该模型在多个基准测试中表现出色，是目前最优秀的模型之一。

    

    本文介绍了 Vid2Seq，这是一个多模态的单级密集事件字幕模型，它是在大规模 narrated 视频数据集上预先训练的。 Vid2Seq 架构通过特殊的时间标记来增强语言模型，使其能够无缝地预测事件边界和文本描述。我们展示了如何利用未标注 narrated 视频数据集进行密集视频字幕的训练，通过将转录语音的句子边界转化为伪事件边界，并使用转录语音句子作为伪事件字幕。使用 YT-Temporal-1B 数据集预训练的 Vid2Seq 模型在各种密集视频字幕基准测试中均表现出色，包括 YouCook2、ViTT 和 ActivityNet Captions。 Vid2Seq 还可以很好地推广到视频段落字幕和视频片段字幕的任务中。

    In this work, we introduce Vid2Seq, a multi-modal single-stage dense event captioning model pretrained on narrated videos which are readily-available at scale. The Vid2Seq architecture augments a language model with special time tokens, allowing it to seamlessly predict event boundaries and textual descriptions in the same output sequence. Such a unified model requires large-scale training data, which is not available in current annotated datasets. We show that it is possible to leverage unlabeled narrated videos for dense video captioning, by reformulating sentence boundaries of transcribed speech as pseudo event boundaries, and using the transcribed speech sentences as pseudo event captions. The resulting Vid2Seq model pretrained on the YT-Temporal-1B dataset improves the state of the art on a variety of dense video captioning benchmarks including YouCook2, ViTT and ActivityNet Captions. Vid2Seq also generalizes well to the tasks of video paragraph captioning and video clip captionin
    
[^131]: 基于路径的图神经网络解释方法用于异构链接预测（PaGE-Link）

    PaGE-Link: Path-based Graph Neural Network Explanation for Heterogeneous Link Prediction. (arXiv:2302.12465v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12465](http://arxiv.org/abs/2302.12465)

    这篇论文提出了一种基于路径的GNN解释方法PaGE-Link，用于异构链接预测，具有连接可解释性，模型可扩展性和处理图形异构性能力。

    

    透明度和问责制已成为黑箱机器学习模型的主要问题。模型行为的适当解释增加了模型的透明度，并帮助研究人员开发更负责任的模型。图神经网络（GNN）最近在许多图形ML问题中表现出优越性能，解释它们已引起了越来越多的关注。然而，在链接预测（LP）方面，GNN的解释尚缺少文献支持。 LP是一项基本的GNN任务，对应于Web上的推荐和赞助搜索等应用程序。鉴于现有的GNN解释方法仅解决节点/图级任务，我们提出了一种基于路径的GNN解释方法，用于异构链接预测（PaGE-Link），该方法生成具有连接可解释性的解释，具有模型可扩展性和处理图形异构性能力。定性地，PaGE-Link可以生成将节点对连接起来的路径作为解释，自然地捕捉到连接关系。

    Transparency and accountability have become major concerns for black-box machine learning (ML) models. Proper explanations for the model behavior increase model transparency and help researchers develop more accountable models. Graph neural networks (GNN) have recently shown superior performance in many graph ML problems than traditional methods, and explaining them has attracted increased interest. However, GNN explanation for link prediction (LP) is lacking in the literature. LP is an essential GNN task and corresponds to web applications like recommendation and sponsored search on web. Given existing GNN explanation methods only address node/graph-level tasks, we propose Path-based GNN Explanation for heterogeneous Link prediction (PaGE-Link) that generates explanations with connection interpretability, enjoys model scalability, and handles graph heterogeneity. Qualitatively, PaGE-Link can generate explanations as paths connecting a node pair, which naturally captures connections be
    
[^132]: 限制半正定矩阵的Karcher均值的统计分析

    Statistical Analysis of Karcher Means for Random Restricted PSD Matrices. (arXiv:2302.12426v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.12426](http://arxiv.org/abs/2302.12426)

    本文通过研究限制半正定矩阵流形上的本质均值模型及其Karcher均值的统计分析，提出了一般外部信号加噪声模型下的确定性误差界，并表明LRC-dPCA算法与全样本PCA算法性能相同。

    

    现代几何感知机器学习算法通常由于可能复杂的非线性流形结构而缺乏非渐进统计分析。本文在限制半正定矩阵流形上研究本质均值模型，并提供Karcher均值的非渐进统计分析。我们还考虑了一般的外部信号加噪声模型，在此模型下提供了Karcher均值的确定性误差界。作为一个应用，我们展示分布式主成分分析算法LRC-dPCA的性能与全样本PCA算法相同。数值实验充分支持我们的理论。

    Non-asymptotic statistical analysis is often missing for modern geometry-aware machine learning algorithms due to the possibly intricate non-linear manifold structure. This paper studies an intrinsic mean model on the manifold of restricted positive semi-definite matrices and provides a non-asymptotic statistical analysis of the Karcher mean. We also consider a general extrinsic signal-plus-noise model, under which a deterministic error bound of the Karcher mean is provided. As an application, we show that the distributed principal component analysis algorithm, LRC-dPCA, achieves the same performance as the full sample PCA algorithm. Numerical experiments lend strong support to our theories.
    
[^133]: 可解释性人工智能无法提供最终用户所要求的解释

    Explainable AI does not provide the explanations end-users are asking for. (arXiv:2302.11577v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2302.11577](http://arxiv.org/abs/2302.11577)

    可解释性人工智能对提高人工智能系统信任度的效果有局限性，透明度和严格的验证更适合打造可靠的人工智能系统。

    

    可解释性人工智能（XAI）技术经常被许多人工智能系统的用户要求使用，旨在了解复杂模型及其相关预测，并建立信任。虽然在开发的某些特定任务中是适用的，但组织采用这些技术来增强对机器学习系统的信任时，会产生意想不到的后果。在本文中，我们讨论了XAI在部署中的局限性，并得出结论认为透明度和严格的验证更适合获得人工智能系统的信任。

    Explainable Artificial Intelligence (XAI) techniques are frequently required by users in many AI systems with the goal of understanding complex models, their associated predictions, and gaining trust. While suitable for some specific tasks during development, their adoption by organisations to enhance trust in machine learning systems has unintended consequences. In this paper we discuss XAI's limitations in deployment and conclude that transparency alongside with rigorous validation are better suited to gaining trust in AI systems.
    
[^134]: 简化基于动量的黎曼子流形优化

    Simplifying Momentum-based Riemannian Submanifold Optimization. (arXiv:2302.09738v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.09738](http://arxiv.org/abs/2302.09738)

    本文针对黎曼子流形优化算法进行了简化，提出了黎曼正常坐标的广义版本，可用于对称正定矩阵的子流形优化，并为深度学习开发了高效的二阶优化器，无需显式矩阵求逆。

    

    带有动量的黎曼子流形优化在计算上是具有挑战性的，因为确保迭代保持在子流形上通常需要解决困难的微分方程。本文针对具有仿射不变度量的对称正定矩阵的子流形优化算法进行了简化。我们提出了黎曼正常坐标的广义版本，可以将问题动态地简化为欧几里得无约束问题。我们使用我们的方法来解释和简化现有的结构化协方差方法，并为深度学习开发了高效的二阶优化器，而无需显式矩阵求逆。

    Riemannian submanifold optimization with momentum is computationally challenging because ensuring iterates remain on the submanifold often requires solving difficult differential equations. We simplify such optimization algorithms for the submanifold of symmetric positive-definite matrices with the affine invariant metric. We propose a generalized version of the Riemannian normal coordinates which dynamically trivializes the problem into a Euclidean unconstrained problem. We use our approach to explain and simplify existing approaches for structured covariances and develop efficient second-order optimizers for deep learning without explicit matrix inverses.
    
[^135]: GPT4MIA: 利用生成预训练变压器 (GPT-3) 作为插入式检验模型进行医学图像分析

    GPT4MIA: Utilizing Generative Pre-trained Transformer (GPT-3) as A Plug-and-Play Transductive Model for Medical Image Analysis. (arXiv:2302.08722v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.08722](http://arxiv.org/abs/2302.08722)

    本文提出 GPT4MIA 方法，利用 GPT-3 作为插入式检验工具进行医学图像分析；该方法在提示结构设计、样本选择及提示排序等方面优化，能有效提高预测准确性。

    

    本文提出了一种称为 GPT4MIA 的新方法，利用生成预训练变压器 (GPT) 作为插入式检验工具，用于医学图像分析 (MIA)。我们提供了理论分析，解释了为什么像 GPT-3 这样的大型预训练语言模型可以作为插入式检验模型用于 MIA。在方法学层面上，我们开发了几种技术处理方法，包括更好的提示结构设计、样本选择以及代表性样本/特征的提示排序，以提高 GPT4MIA 的效率和有效性。我们呈现了两种具体的 GPT4MIA 使用案例 (带有工作流程)：(1) 检测预测错误和 (2) 改进预测准确性，与已经建立的基于视觉的图像分类模型 (例如 ResNet) 协同工作。实验验证了我们提出的方法对于这两个任务的有效性。我们进一步讨论了利用基于变压器的大型语言模型进行 MIA 任务的机会和挑战。

    In this paper, we propose a novel approach (called GPT4MIA) that utilizes Generative Pre-trained Transformer (GPT) as a plug-and-play transductive inference tool for medical image analysis (MIA). We provide theoretical analysis on why a large pre-trained language model such as GPT-3 can be used as a plug-and-play transductive inference model for MIA. At the methodological level, we develop several technical treatments to improve the efficiency and effectiveness of GPT4MIA, including better prompt structure design, sample selection, and prompt ordering of representative samples/features. We present two concrete use cases (with workflow) of GPT4MIA: (1) detecting prediction errors and (2) improving prediction accuracy, working in conjecture with well-established vision-based models for image classification (e.g., ResNet). Experiments validate that our proposed method is effective for these two tasks. We further discuss the opportunities and challenges in utilizing Transformer-based large
    
[^136]: 基于模拟用户研究设计机器学习解释性评估的案例研究

    A Case Study on Designing Evaluations of ML Explanations with Simulated User Studies. (arXiv:2302.07444v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07444](http://arxiv.org/abs/2302.07444)

    本研究进行了一项真实用例的“SimEvals”评估，以评估在电子商务欺诈检测中解释是否可以更好地支持机器学习辅助决策，并证实了其与用户研究的结论一致。

    

    当进行用户研究以确定模型解释在帮助人类决策方面的有效性时，使用真实世界的用例、数据和用户非常重要。然而，这个过程可能需要耗费大量资源，使得只能评估少量的解释方法。已经提出了使用机器学习模型作为人类用户代理的模拟用户评估（SimEvals）作为选择有前景的解释方法的中间步骤。在这项工作中，我们对一个真实世界的用例进行了第一次 SimEvals，以评估解释是否可以更好地支持在电子商务欺诈检测中进行机器学习辅助决策。我们研究了 SimEvals 是否能够证实在这个欺诈检测背景下进行的用户研究的结果。特别地，我们发现 SimEvals 表明，所有考虑的解释器性能相当，并且没有一个能够超过没有解释的基准 -- 这与原始用户研究的结论相符。这些对应关系

    When conducting user studies to ascertain the usefulness of model explanations in aiding human decision-making, it is important to use real-world use cases, data, and users. However, this process can be resource-intensive, allowing only a limited number of explanation methods to be evaluated. Simulated user evaluations (SimEvals), which use machine learning models as a proxy for human users, have been proposed as an intermediate step to select promising explanation methods. In this work, we conduct the first SimEvals on a real-world use case to evaluate whether explanations can better support ML-assisted decision-making in e-commerce fraud detection. We study whether SimEvals can corroborate findings from a user study conducted in this fraud detection context. In particular, we find that SimEvals suggest that all considered explainers are equally performant, and none beat a baseline without explanations -- this matches the conclusions of the original user study. Such correspondences be
    
[^137]: 学习双层知识图谱的表示以进行超越链接预测的推理

    Learning Representations of Bi-level Knowledge Graphs for Reasoning beyond Link Prediction. (arXiv:2302.02601v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02601](http://arxiv.org/abs/2302.02601)

    本文提出了一种基于双层知识图谱的方法来学习嵌入，将三元组之间的关系考虑进去，并使用数据增强策略来增加合理的三元组。

    

    知识图谱使用三元组来表示已知事实。现有的知识图谱嵌入方法仅考虑实体之间的连接，而本文提出考虑三元组之间的关系。本文定义了一个更高级的三元组来表示三元组之间的关系，例如，$\langle T_1$, PrerequisiteFor, $T_2\rangle$，其中PrerequisiteFor是更高级别的关系。我们定义一个由基本级别和更高级别的三元组组成的双层知识图谱。我们还提出了一种基于双层知识图谱上的随机游走的数据增强策略来增加合理的三元组。我们的模型BiVE通过考虑基本级别和更高级别三元组的结构来学习嵌入。

    Knowledge graphs represent known facts using triplets. While existing knowledge graph embedding methods only consider the connections between entities, we propose considering the relationships between triplets. For example, let us consider two triplets $T_1$ and $T_2$ where $T_1$ is (Academy_Awards, Nominates, Avatar) and $T_2$ is (Avatar, Wins, Academy_Awards). Given these two base-level triplets, we see that $T_1$ is a prerequisite for $T_2$. In this paper, we define a higher-level triplet to represent a relationship between triplets, e.g., $\langle T_1$, PrerequisiteFor, $T_2\rangle$ where PrerequisiteFor is a higher-level relation. We define a bi-level knowledge graph that consists of the base-level and the higher-level triplets. We also propose a data augmentation strategy based on the random walks on the bi-level knowledge graph to augment plausible triplets. Our model called BiVE learns embeddings by taking into account the structures of the base-level and the higher-level tripl
    
[^138]: 通过自我对战实现多样化诱导的环境设计

    Diversity Induced Environment Design via Self-Play. (arXiv:2302.02119v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.02119](http://arxiv.org/abs/2302.02119)

    本文提出了一种利用自我对战技术的任务不可知方法，来识别环境中的观察/隐藏状态，并将多样性引入非监督环境设计框架中，从而提高了环境设计的效率和有效性。

    

    最近关于环境分布设计的研究已经展示出训练有效的通用能力代理的前景。它的成功部分在于一种自适应课程学习的形式，该形式通过生成代理能力的前沿环境实例（或级别）。然而，这种环境设计框架经常在具有挑战性的设计空间中发现有效级别方面存在困难，并需要与环境进行高成本交互。本文的目的是在非监督环境设计（UED）框架中引入多样性。具体来说，我们提出了一种任务不可知的方法来识别对给定级别具有代表性的观察/隐藏状态。然后利用这种方法的结果来表征两个级别之间的多样性，正如我们所展示的，这对于有效性能至关重要。此外，为了提高采样效率，我们加入了自我对战技术，使得环境生成器能够自动生成环境。

    Recent work on designing an appropriate distribution of environments has shown promise for training effective generally capable agents. Its success is partly because of a form of adaptive curriculum learning that generates environment instances (or levels) at the frontier of the agent's capabilities. However, such an environment design framework often struggles to find effective levels in challenging design spaces and requires costly interactions with the environment. In this paper, we aim to introduce diversity in the Unsupervised Environment Design (UED) framework. Specifically, we propose a task-agnostic method to identify observed/hidden states that are representative of a given level. The outcome of this method is then utilized to characterize the diversity between two levels, which as we show can be crucial to effective performance. In addition, to improve sampling efficiency, we incorporate the self-play technique that allows the environment generator to automatically generate e
    
[^139]: 走向既能看又能读的模型

    Towards Models that Can See and Read. (arXiv:2301.07389v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.07389](http://arxiv.org/abs/2301.07389)

    本文提出了UniTNT模型，该模型能兼顾场景文本和图像的理解，通过与先前的模态融合提高了图像问题回答和图像字幕生成任务的性能。

    

    视觉问答（VQA）和图像字幕生成（CAP）是最受欢迎的视觉语言任务之一，它们有着类似的场景文本版本，需要从图像中的文本进行推理。尽管它们明显相似，但两者独立处理，产生可以看或读但不能两者兼备的专门方法。在这项工作中，我们对这种现象进行了深入分析，并建议UniTNT，一种统一的文本-非文本方法，为现有的多模态架构提供场景文本理解能力。具体而言，我们将场景文本信息视为一种额外的模态，并通过指定的模块将其与任何预训练的编码器-解码器架构融合在一起。彻底的实验表明，UniTNT是第一个成功处理两种任务类型的单一模型。此外，我们证明场景文本的理解能力可以将视觉语言模型在一般VQA和CAP上的性能提高高达2.69％和0.6 CIDEr。

    Visual Question Answering (VQA) and Image Captioning (CAP), which are among the most popular vision-language tasks, have analogous scene-text versions that require reasoning from the text in the image. Despite their obvious resemblance, the two are treated independently and, as we show, yield task-specific methods that can either see or read, but not both. In this work, we conduct an in-depth analysis of this phenomenon and propose UniTNT, a Unified Text-Non-Text approach, which grants existing multimodal architectures scene-text understanding capabilities. Specifically, we treat scene-text information as an additional modality, fusing it with any pretrained encoder-decoder-based architecture via designated modules. Thorough experiments reveal that UniTNT leads to the first single model that successfully handles both task types. Moreover, we show that scene-text understanding capabilities can boost vision-language models' performance on general VQA and CAP by up to 2.69% and 0.6 CIDEr,
    
[^140]: 基于对抗生成网络的短SSVEP数据扩展框架

    Short-length SSVEP data extension by a novel generative adversarial networks based framework. (arXiv:2301.05599v3 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2301.05599](http://arxiv.org/abs/2301.05599)

    本文提出了一种基于GAN的端到端信号转化网络TEGAN，可以将短SSVEP信号转换成长的人工SSVEP信号，并显著提高BCI系统的效率和准确性。

    

    基于SSVEP的脑机接口因其高信息传输速率和目标数量可用性而受到广泛关注。然而，频率识别方法的性能在很大程度上取决于用户校准数据的数量和数据长度，这限制了它在实际应用中的部署。最近，基于生成对抗网络（GANs）的数据生成方法已被广泛采用来创建合成的脑电数据，有望解决这些问题。本文提出了一种基于GANs的端到端信号转化网络TEGAN，用于数据长度扩展。TEGAN可以将短SSVEP信号转换成长的人工SSVEP信号。通过将一个新颖的U型生成器架构和一个辅助分类器加入到网络结构中，TEGAN可以在合成数据中产生有条件的特征。此外，我们实现并比较了两种最先进的频率识别方法，以评估TEGAN生成数据的有效性。实验结果表明，所提出的TEGAN方法优于传统的线性插值方法和最先进的基于深度学习的方法。所提出的TEGAN方法可以显著提高BCI系统的效率，减少所需的校准时间并改善分类的准确性。

    Steady-state visual evoked potentials (SSVEPs) based brain-computer interface (BCI) has received considerable attention due to its high information transfer rate (ITR) and available quantity of targets. However, the performance of frequency identification methods heavily hinges on the amount of user calibration data and data length, which hinders the deployment in real-world applications. Recently, generative adversarial networks (GANs)-based data generation methods have been widely adopted to create synthetic electroencephalography (EEG) data, holds promise to address these issues. In this paper, we proposed a GAN-based end-to-end signal transformation network for data length extension, termed as TEGAN. TEGAN transforms short-length SSVEP signals into long-length artificial SSVEP signals. By incorporating a novel U-Net generator architecture and an auxiliary classifier into the network architecture, the TEGAN could produce conditioned features in the synthetic data. Additionally, we i
    
[^141]: 风险敏感的强化学习算法：指数标准的应用

    Risk-Sensitive Reinforcement Learning with Exponential Criteria. (arXiv:2212.09010v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2212.09010](http://arxiv.org/abs/2212.09010)

    本文介绍了一种风险敏感的强化学习算法，使用指数判据来提高其系统抗干扰性和实用性。作者进行了在模拟和实际机器人上的实验验证，表明该算法能够有效地提高样本效率和执行效果。

    

    尽管风险中性的强化学习已经在很多应用中得到了实验成功，但是这种方法容易受到噪声和系统参数扰动的影响而不够稳健。因此,对风险敏感的强化学习算法进行了研究，以提高其系统抗干扰性，样本效率和实用性。本文介绍了一种新型的无模型风险敏感学习算法，将广泛使用的策略梯度算法进行变体，其实现过程类似。具体来说，本文研究了指数标准对强化学习代理的策略风险敏感性的影响，并开发了蒙特卡罗策略梯度算法和在线(时间差分)演员-评论家算法的变体。分析结果表明，指数标准的使用能够推广常用的特定正则化方法。作者在摆动杆和摆摆杆任务上进行了测试，验证了所提出的算法的实现性能和稳健性。

    While risk-neutral reinforcement learning has shown experimental success in a number of applications, it is well-known to be non-robust with respect to noise and perturbations in the parameters of the system. For this reason, risk-sensitive reinforcement learning algorithms have been studied to introduce robustness and sample efficiency, and lead to better real-life performance. In this work, we introduce new model-free risk-sensitive reinforcement learning algorithms as variations of widely-used Policy Gradient algorithms with similar implementation properties. In particular, we study the effect of exponential criteria on the risk-sensitivity of the policy of a reinforcement learning agent, and develop variants of the Monte Carlo Policy Gradient algorithm and the online (temporal-difference) Actor-Critic algorithm. Analytical results showcase that the use of exponential criteria generalize commonly used ad-hoc regularization approaches. The implementation, performance, and robustness 
    
[^142]: 多分辨率在线确定性退火：一种分层和渐进学习架构

    Multi-Resolution Online Deterministic Annealing: A Hierarchical and Progressive Learning Architecture. (arXiv:2212.08189v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.08189](http://arxiv.org/abs/2212.08189)

    本文提出了一种基于逐渐增加子集数量的分区序列的通用的分层学习结构，并使用无梯度随机逼近更新进行在线解决优化问题的方法，可以定义函数逼近问题并使用双时间尺度随机逼近算法的理论解决，模拟了一种退火过程。

    

    随着时间和计算资源的限制，逐步逼近基于数据的优化问题的解决方案的分层学习算法对于决策系统至关重要。本研究提出了一种通用的分层学习结构，基于可能的多分辨率数据空间的渐进分区。最优分区通过解决一系列优化子问题逐步逼近，生成具有逐渐增加的子集数量的分区序列。我们展示对每个优化问题的解可以使用无梯度随机逼近更新进行在线估计。因此，可以在分区的每个子集中定义函数逼近问题，并使用双时间尺度随机逼近算法的理论解决。这模拟了一种退火过程，并定义了一种强大且可解释的启发式方法，逐步增加复杂性。

    Hierarchical learning algorithms that gradually approximate a solution to a data-driven optimization problem are essential to decision-making systems, especially under limitations on time and computational resources. In this study, we introduce a general-purpose hierarchical learning architecture that is based on the progressive partitioning of a possibly multi-resolution data space. The optimal partition is gradually approximated by solving a sequence of optimization sub-problems that yield a sequence of partitions with increasing number of subsets. We show that the solution of each optimization problem can be estimated online using gradient-free stochastic approximation updates. As a consequence, a function approximation problem can be defined within each subset of the partition and solved using the theory of two-timescale stochastic approximation algorithms. This simulates an annealing process and defines a robust and interpretable heuristic method to gradually increase the complexi
    
[^143]: 基于地形变换器的四足运动模拟到真实世界的迁移

    Sim-to-Real Transfer for Quadrupedal Locomotion via Terrain Transformer. (arXiv:2212.07740v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2212.07740](http://arxiv.org/abs/2212.07740)

    本文介绍了一种基于地形变换器（TERT）的高容量 Transformer 模型，用于各种地形中四足运动控制，同时提出了一种新的两阶段训练框架，可以使 Transformer 更好地应用于 sim-to-real 场景中。

    

    近年来，深度强化学习在多种地形的足式 locomotion 中通过在物理模拟中训练策略并将其迁移到真实世界（即 sim-to-real 迁移）上已成为一种有吸引力的替代方法。尽管取得了相当大的进展，但传统神经网络的容量和可扩展性仍然受到限制，这可能阻碍它们在更复杂的环境中的应用。相反，Transformer 架构在广泛的大规模序列建模任务中已经展示了它的优越性，包括自然语言处理和决策问题。在本文中，我们提出了 Terrain Transformer（TERT），一种用于各种地形中四足运动控制的高容量 Transformer 模型。此外，为了更好地利用 Transformer 在 sim-to-real 场景中，我们提出了一种新的两阶段训练框架，包括离线预训练阶段和在线校正阶段，可以自然地将 Transformer 与 privilege

    Deep reinforcement learning has recently emerged as an appealing alternative for legged locomotion over multiple terrains by training a policy in physical simulation and then transferring it to the real world (i.e., sim-to-real transfer). Despite considerable progress, the capacity and scalability of traditional neural networks are still limited, which may hinder their applications in more complex environments. In contrast, the Transformer architecture has shown its superiority in a wide range of large-scale sequence modeling tasks, including natural language processing and decision-making problems. In this paper, we propose Terrain Transformer (TERT), a high-capacity Transformer model for quadrupedal locomotion control on various terrains. Furthermore, to better leverage Transformer in sim-to-real scenarios, we present a novel two-stage training framework consisting of an offline pretraining stage and an online correction stage, which can naturally integrate Transformer with privilege
    
[^144]: 基于基础模型反馈的策略适应

    Policy Adaptation from Foundation Model Feedback. (arXiv:2212.07398v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.07398](http://arxiv.org/abs/2212.07398)

    本文提出了基于基础模型反馈的策略适应（PAFF）方法，通过让策略使用随机生成的指令进行演示，并利用预训练的基础模型提供反馈来重新标记演示，自动提供新的演示-指令数据对进行策略微调，以实现机器人操作的泛化。实验结果表明，PAFF优于现有最先进的方法。

    

    最近在视觉-语言基础模型方面的进展为构建通用机器人带来了显著进步。通过使用预训练模型将场景和指令编码为决策输入，指令条件化策略可以在不同的对象和任务之间进行泛化。尽管这是令人鼓舞的，但策略在遇到未见过的任务或环境时仍然失败。在本工作中，我们提出了一种基于基础模型反馈的策略适应（PAFF）。当将训练好的策略部署到新任务或新环境时，我们首先让策略使用随机生成的指令进行演示。虽然执行可能出现错误，但我们可以利用预训练的基础模型提供反馈来重新标记演示。这自动为策略微调提供了新的演示-指令数据对。我们在机器人操作设置中进行了各种实验的评估，重点是在未见过的对象、任务和未观察到的环境中的泛化。我们的实验结果表明，PAFF在最终任务成功率和训练效率方面优于现有最先进的方法。

    Recent progress on vision-language foundation models have brought significant advancement to building general-purpose robots. By using the pre-trained models to encode the scene and instructions as inputs for decision making, the instruction-conditioned policy can generalize across different objects and tasks. While this is encouraging, the policy still fails in most cases given an unseen task or environment. In this work, we propose Policy Adaptation from Foundation model Feedback (PAFF). When deploying the trained policy to a new task or a new environment, we first let the policy play with randomly generated instructions to record the demonstrations. While the execution could be wrong, we can use the pre-trained foundation models to provide feedback to relabel the demonstrations. This automatically provides new pairs of demonstration-instruction data for policy fine-tuning. We evaluate our method on a broad range of experiments with the focus on generalization on unseen objects, unse
    
[^145]: 异步联邦学习在无线网络中的调度和聚合设计

    Scheduling and Aggregation Design for Asynchronous Federated Learning over Wireless Networks. (arXiv:2212.07356v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.07356](http://arxiv.org/abs/2212.07356)

    本文提出了一种异步联邦学习的调度策略和聚合加权设计，通过采用基于信道感知数据重要性的调度策略和“年龄感知”的聚合加权设计来解决FL系统中的“拖沓”问题，并通过仿真证实了其有效性。

    

    联邦学习（FL）是一种协作的机器学习（ML）框架，它结合了设备上的训练和基于服务器的聚合来在分布式代理间训练通用的ML模型。本文中，我们提出了一种异步FL设计，采用周期性的聚合来解决FL系统中的“拖沓”问题。考虑到有限的无线通信资源，我们研究了不同调度策略和聚合设计对收敛性能的影响。基于降低聚合模型更新的偏差和方差的重要性，我们提出了一个调度策略，它同时考虑了用户设备的信道质量和训练数据表示。通过仿真验证了我们的基于信道感知数据重要性的调度策略相对于同步联邦学习提出的现有最新方法的有效性。此外，我们还展示了一种“年龄感知”的聚合加权设计可以显著提高学习性能。

    Federated Learning (FL) is a collaborative machine learning (ML) framework that combines on-device training and server-based aggregation to train a common ML model among distributed agents. In this work, we propose an asynchronous FL design with periodic aggregation to tackle the straggler issue in FL systems. Considering limited wireless communication resources, we investigate the effect of different scheduling policies and aggregation designs on the convergence performance. Driven by the importance of reducing the bias and variance of the aggregated model updates, we propose a scheduling policy that jointly considers the channel quality and training data representation of user devices. The effectiveness of our channel-aware data-importance-based scheduling policy, compared with state-of-the-art methods proposed for synchronous FL, is validated through simulations. Moreover, we show that an ``age-aware'' aggregation weighting design can significantly improve the learning performance i
    
[^146]: 谁的情绪更重要？没有先前知识的说话活动定位。

    Whose Emotion Matters? Speaking Activity Localisation without Prior Knowledge. (arXiv:2211.15377v3 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2211.15377](http://arxiv.org/abs/2211.15377)

    本文介绍了一种新方法MELD-FAIR来解决情感识别中的挑战，通过使用主动说话者检测和自动语音识别模型，重新对齐了MELD视频，并成功捕获了讲话者的面部表情。

    

    会话情感识别（ERC）的任务受益于多种模态的可用性，例如在基于视频的多模态情感线数据集（MELD）中提供的信息。然而，只有少数研究方法使用了MELD视频中的声学和视觉信息。这有两个原因：首先，MELD中的标签到视频的对齐是有噪声的，这使得那些视频成为了情感语音数据的不可靠来源。其次，会话可以涉及到同一场景中的几个人，这需要定位话语来源。在本文中，我们通过使用最近的主动说话者检测和自动语音识别模型，引入了带有固定音频视觉信息的MELD-FAIR，能够重新对齐MELD视频并捕获96.92％的MELD中提供的话语的讲话者面部表情。使用自我监督的声音识别模型进行的实验表明，重新对齐的MELD-FAIR视频更清晰地显示了讲话者的面部表情。

    The task of emotion recognition in conversations (ERC) benefits from the availability of multiple modalities, as provided, for example, in the video-based Multimodal EmotionLines Dataset (MELD). However, only a few research approaches use both acoustic and visual information from the MELD videos. There are two reasons for this: First, label-to-video alignments in MELD are noisy, making those videos an unreliable source of emotional speech data. Second, conversations can involve several people in the same scene, which requires the localisation of the utterance source. In this paper, we introduce MELD with Fixed Audiovisual Information via Realignment (MELD-FAIR) by using recent active speaker detection and automatic speech recognition models, we are able to realign the videos of MELD and capture the facial expressions from speakers in 96.92% of the utterances provided in MELD. Experiments with a self-supervised voice recognition model indicate that the realigned MELD-FAIR videos more cl
    
[^147]: 多智能体强化学习在自主路由和接送问题中的应用，可适应需求的变化

    Multiagent Reinforcement Learning for Autonomous Routing and Pickup Problem with Adaptation to Variable Demand. (arXiv:2211.14983v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2211.14983](http://arxiv.org/abs/2211.14983)

    本论文提出了一种多智能体强化学习的学习框架，用于在城市地图上服务于随机出现的请求，可以产生协调作用并考虑先前可能出现的未来请求，能够适应不同需求分布的变化。

    

    我们推导出一个学习框架，用于为一组自主车辆在城市地图上服务于随机出现的请求时生成路由/接送策略。我们着重研究的策略是：1）产生协调作用，从而减少为服务请求等待的时间；2）是非近视策略，并考虑先前可能出现的未来请求；3）可以适应基础需求分布的变化。具体来说，我们感兴趣的策略是适应城市环境中实际需求条件的波动，例如高峰时间和非高峰时间等。我们通过以下方式实现：(i)能够改进离线训练策略性能的在线玩算法，和(ii)一种离线逼近方案，允许适应基于需求模型的变化。特别地，我们通过计算Wasserstein距离的q-valid半径来量化有效区域，从而实现对我们已学习策略对不同需求分布的适应性。我们的实验结果表明，我们提出的框架可以比基线启发式策略改善平均等待时间，并能够适应不断变化的需求模型。

    We derive a learning framework to generate routing/pickup policies for a fleet of autonomous vehicles tasked with servicing stochastically appearing requests on a city map. We focus on policies that 1) give rise to coordination amongst the vehicles, thereby reducing wait times for servicing requests, 2) are non-myopic, and consider a-priori potential future requests, 3) can adapt to changes in the underlying demand distribution. Specifically, we are interested in policies that are adaptive to fluctuations of actual demand conditions in urban environments, such as on-peak vs. off-peak hours. We achieve this through a combination of (i) an online play algorithm that improves the performance of an offline-trained policy, and (ii) an offline approximation scheme that allows for adapting to changes in the underlying demand model. In particular, we achieve adaptivity of our learned policy to different demand distributions by quantifying a region of validity using the q-valid radius of a Wass
    
[^148]: SAMSON：针对深度神经网络泛化和鲁棒性问题的异常归一化尺度下锐度感知最小化方法

    SAMSON: Sharpness-Aware Minimization Scaled by Outlier Normalization for Improving DNN Generalization and Robustness. (arXiv:2211.11561v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.11561](http://arxiv.org/abs/2211.11561)

    通过应用锐度感知训练并将其优化为损失值和损失锐度，SAMSON方法显著提高了对噪声硬件的推断鲁棒性，而无需关于目标硬件的任何假设。

    

    能效较高的深度神经网络（DNN）加速器容易出现非理想情况，从而降低DNN的推断性能。为了缓解这种情况，现有的方法通常在训练期间向DNN权重添加扰动，以模拟噪声硬件上的推断过程。然而，这通常需要关于目标硬件的知识，并且会导致在DNN性能和鲁棒性之间进行权衡，降低前者以提高后者。在本文中，我们展示了应用锐度感知训练，在优化损失值和损失锐度的同时，可以显著提高对噪声硬件的推断鲁棒性，而无需依赖于有关目标硬件的任何假设。特别地，我们提出了一种新的自适应锐度感知方法，它不仅将给定权重的最坏情况扰动取决于其大小，而且还取决于权重分布的范围。这是通过执行在异常值归一化尺度下的锐度感知最小化来实现的。

    Energy-efficient deep neural network (DNN) accelerators are prone to non-idealities that degrade DNN performance at inference time. To mitigate such degradation, existing methods typically add perturbations to the DNN weights during training to simulate inference on noisy hardware. However, this often requires knowledge about the target hardware and leads to a trade-off between DNN performance and robustness, decreasing the former to increase the latter. In this work, we show that applying sharpness-aware training, by optimizing for both the loss value and loss sharpness, significantly improves robustness to noisy hardware at inference time without relying on any assumptions about the target hardware. In particular, we propose a new adaptive sharpness-aware method that conditions the worst-case perturbation of a given weight not only on its magnitude but also on the range of the weight distribution. This is achieved by performing sharpness-aware minimization scaled by outlier minimizat
    
[^149]: 带有卷积高斯神经过程的环境传感器放置

    Environmental Sensor Placement with Convolutional Gaussian Neural Processes. (arXiv:2211.10381v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.10381](http://arxiv.org/abs/2211.10381)

    本论文提出了一种新的方式——卷积高斯神经过程（ConvGNP），用于提高环境传感器的放置效率。ConvGNP使用神经网络来参数化联合高斯分布，通过学习空间和季节性非平稳性，优于传统的非平稳高斯过程模型。

    

    环境传感器对于监测天气和气候变化的影响至关重要。然而，在像南极这样的偏远地区，最大化测量信息和有效放置传感器是具有挑战性的。概率机器学习模型可以通过预测新传感器提供的不确定性减少来评估放置信息。高斯过程模型广泛用于此目的，但难以捕捉复杂的非平稳行为并缩放到大型数据集。本文提出使用卷积高斯神经过程（ConvGNP）来解决这些问题。ConvGNP使用神经网络来参数化任意目标位置的联合高斯分布，实现了灵活性和可扩展性。使用模拟的南极地区地面温度异常作为真实数据，ConvGNP学习了空间和季节性非平稳性，并优于非平稳GP基线。在模拟的s中，

    Environmental sensors are crucial for monitoring weather conditions and the impacts of climate change. However, it is challenging to maximise measurement informativeness and place sensors efficiently, particularly in remote regions like Antarctica. Probabilistic machine learning models can evaluate placement informativeness by predicting the uncertainty reduction provided by a new sensor. Gaussian process (GP) models are widely used for this purpose, but they struggle with capturing complex non-stationary behaviour and scaling to large datasets. This paper proposes using a convolutional Gaussian neural process (ConvGNP) to address these issues. A ConvGNP uses neural networks to parameterise a joint Gaussian distribution at arbitrary target locations, enabling flexibility and scalability. Using simulated surface air temperature anomaly over Antarctica as ground truth, the ConvGNP learns spatial and seasonal non-stationarities, outperforming a non-stationary GP baseline. In a simulated s
    
[^150]: DeepSense 6G：一个大规模真实世界多模态感知和通信数据集。

    DeepSense 6G: A Large-Scale Real-World Multi-Modal Sensing and Communication Dataset. (arXiv:2211.09769v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2211.09769](http://arxiv.org/abs/2211.09769)

    DeepSense 6G是一个大型真实世界多模态感知和通信数据集，旨在推动在多模态感知、通信和定位交叉领域的广泛应用中的深度学习研究。

    

    本文介绍了DeepSense 6G数据集，这是一个基于共存多模态感知和通信数据的真实世界测量数据的大规模数据集。DeepSense 6G数据集旨在推动在多模态感知、通信和定位交叉领域的广泛应用中的深度学习研究。本文详细介绍了DeepSense数据集的结构、采用的测试平台、数据收集和处理方法、部署场景以及示例应用，旨在促进多模态感知和通信数据集的采用和可重复性。

    This article presents the DeepSense 6G dataset, which is a large-scale dataset based on real-world measurements of co-existing multi-modal sensing and communication data. The DeepSense 6G dataset is built to advance deep learning research in a wide range of applications in the intersection of multi-modal sensing, communication, and positioning. This article provides a detailed overview of the DeepSense dataset structure, adopted testbeds, data collection and processing methodology, deployment scenarios, and example applications, with the objective of facilitating the adoption and reproducibility of multi-modal sensing and communication datasets.
    
[^151]: 对比学习用于多中心脑龄预测回归的研究

    Contrastive learning for regression in multi-site brain age prediction. (arXiv:2211.08326v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2211.08326](http://arxiv.org/abs/2211.08326)

    本篇论文研究针对多中心脑龄预测回归问题的对比学习方法，通过提高数据和标签的稳健性，实现了在MRI扫描下脑龄预测的最先进性能和最佳泛化能力。

    

    在神经影像学中，构建准确的深度学习模型用于脑龄预测是一个非常相关的主题，因为它可以帮助更好地理解神经退行性疾病并寻找新的生物标志物。为了估计准确且可泛化的模型，已经收集了大量的数据集，它们通常是多中心和多扫描器的。这种大规模异构性会对深度学习模型的泛化性能产生负面影响，因为它们容易过度拟合与特定场地相关的噪音。最近，对比学习方法表现出在数据或标签噪声方面更为稳健。因此，我们提出了一种新的对比学习回归损失，用于MRI扫描的脑龄预测。我们的方法在OpenBHB挑战赛上实现了最先进的性能，具有最佳的泛化能力和抗场地相关噪声的鲁棒性。

    Building accurate Deep Learning (DL) models for brain age prediction is a very relevant topic in neuroimaging, as it could help better understand neurodegenerative disorders and find new biomarkers. To estimate accurate and generalizable models, large datasets have been collected, which are often multi-site and multi-scanner. This large heterogeneity negatively affects the generalization performance of DL models since they are prone to overfit site-related noise. Recently, contrastive learning approaches have been shown to be more robust against noise in data or labels. For this reason, we propose a novel contrastive learning regression loss for robust brain age prediction using MRI scans. Our method achieves state-of-the-art performance on the OpenBHB challenge, yielding the best generalization capability and robustness to site-related noise.
    
[^152]: 带装载和覆盖约束的上下文幸存者问题：基于回归的模块化Lagrangian方法

    Contextual Bandits with Packing and Covering Constraints: A Modular Lagrangian Approach via Regression. (arXiv:2211.07484v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.07484](http://arxiv.org/abs/2211.07484)

    该论文研究了一种带有资源线性约束的上下文幸存者问题的变种，提出了一种新的算法，该算法简单、计算效率高，同时能够实现较低的后悔。此外，当某些约束被违反时，算法在统计上是最优的。

    

    我们考虑一种上下文幸存者问题的变种，其中算法在总消费的线性约束下使用多个资源。这个问题推广了带背包的上下文幸存者问题(CBwK)，允许装载和覆盖约束，以及正负资源消耗。我们提出了一种新算法，简单、计算效率高，能够实现退化的后悔。当某些约束被违反时，对于CBwK，它在统计上是最优的。我们的算法基于LagrangianBwK(Immorlica等人，FOCS 2019)，这是一种面向CBwK的Lagrangian技术，以及SquareCB(Foster和Rakhlin，ICML 2020)，这是一种面向上下文幸存者的回归技术。我们的分析利用了两种技术本质上的模块化。

    We consider a variant of contextual bandits in which the algorithm consumes multiple resources subject to linear constraints on total consumption. This problem generalizes contextual bandits with knapsacks (CBwK), allowing for packing and covering constraints, as well as positive and negative resource consumption. We present a new algorithm that is simple, computationally efficient, and admits vanishing regret. It is statistically optimal for CBwK when an algorithm must stop once some constraint is violated. Our algorithm builds on LagrangeBwK (Immorlica et al., FOCS 2019) , a Lagrangian-based technique for CBwK, and SquareCB (Foster and Rakhlin, ICML 2020), a regression-based technique for contextual bandits. Our analysis leverages the inherent modularity of both techniques.
    
[^153]: MixMask: 重新审视Siamese ConvNets的遮盖策略

    MixMask: Revisiting Masking Strategy for Siamese ConvNets. (arXiv:2210.11456v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.11456](http://arxiv.org/abs/2210.11456)

    本文提出了一种新的填充式遮盖策略MixMask，在Siamese ConvNets中实现遮盖和对比学习目标的匹配，提高了Siamese ConvNets的性能并在多个基准测试中实现了最先进的结果。

    

    最近自监督学习的进展将Masked Image Modeling（MIM）和Siamese网络整合成一个统一的框架，利用了两种技术的优点。然而，在Siamese ConvNets中应用传统的基于擦除的遮盖策略时，存在一些未解决的问题，包括（I）在连续处理数据时不能放弃不相关的遮盖区域，导致训练效率低于ViT模型;（II）基于擦除的遮盖与Siamese ConvNets中的对比学习目标不匹配，与MIM方法不同。本文提出了一种称为MixMask的填充式遮盖策略，以防止香草遮盖方法中图像中的随机遮盖区域导致信息不完整。此外，我们引入了一种灵活的损失函数设计，考虑两个不同混合视图之间的语义距离变化，以适应集成架构并防止遮盖和对比学习目标之间的不匹配。实验表明，MixMask显着提高了Siamese ConvNets的性能，并在几个基准测试中实现了最先进的结果。

    Recent advances in self-supervised learning have integrated Masked Image Modeling (MIM) and Siamese Networks into a unified framework that leverages the benefits of both techniques. However, several issues remain unaddressed when applying conventional erase-based masking with Siamese ConvNets. These include (I) the inability to drop uninformative masked regions in ConvNets as they process data continuously, resulting in low training efficiency compared to ViT models; and (II) the mismatch between erase-based masking and the contrastive-based objective in Siamese ConvNets, which differs from the MIM approach. In this paper, we propose a filling-based masking strategy called MixMask to prevent information incompleteness caused by the randomly erased regions in an image in the vanilla masking method. Furthermore, we introduce a flexible loss function design that considers the semantic distance change between two different mixed views to adapt the integrated architecture and prevent mismat
    
[^154]: 缓解自然语言系统中隐蔽不安全的文本

    Mitigating Covertly Unsafe Text within Natural Language Systems. (arXiv:2210.09306v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2210.09306](http://arxiv.org/abs/2210.09306)

    本文讨论了智能技术中日益普遍的文本安全问题，并强调了一个被忽视的类别：隐蔽不安全文本。该文提出了缓解策略以解决这一问题，以提高智能系统内部的安全性。

    

    智能技术中一个日益普遍的问题是文本安全性，因为不受控制的系统可能会向用户生成导致伤害或威胁生命的建议。然而，可能导致身体伤害的生成语句的明确程度不同。在本文中，我们区分了可能导致身体伤害的文本类型，并建立了一个尤其未被探索的类别：隐蔽不安全文本。然后，我们进一步分解了这个类别并分析了每个小类别中文本的生成方式。最终，我们的工作定义了导致物理伤害的隐蔽不安全语言问题，并指出这个微妙但危险的问题需要成为相关利益相关者和监管机构的优先考虑问题。我们提出了缓解策略，以启发未来研究人员解决这个具有挑战性的问题，并帮助提高智能系统内部的安全性。

    An increasingly prevalent problem for intelligent technologies is text safety, as uncontrolled systems may generate recommendations to their users that lead to injury or life-threatening consequences. However, the degree of explicitness of a generated statement that can cause physical harm varies. In this paper, we distinguish types of text that can lead to physical harm and establish one particularly underexplored category: covertly unsafe text. Then, we further break down this category with respect to the system's information and discuss solutions to mitigate the generation of text in each of these subcategories. Ultimately, our work defines the problem of covertly unsafe language that causes physical harm and argues that this subtle yet dangerous issue needs to be prioritized by stakeholders and regulators. We highlight mitigation strategies to inspire future researchers to tackle this challenging problem and help improve safety within smart systems.
    
[^155]: 可验证且具有证明安全性的机器学习去除算法

    Verifiable and Provably Secure Machine Unlearning. (arXiv:2210.09126v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.09126](http://arxiv.org/abs/2210.09126)

    该论文提出了可证明安全的机器学习去除算法，可以让用户审计这个过程，以确保训练数据的隐私得到保护。

    

    机器学习去除算法旨在在训练后从训练数据集中移除某些点；例如当用户请求删除数据时。虽然已经提出了许多机器学习去除算法，但是没有一种算法使得用户可以审计这个过程。此外，最近的研究表明，用户无法通过检查模型本身来验证其数据是否已被删除。为了解决这个问题，我们不是考虑模型参数，而是将可验证的算法视为一种安全问题。为此，我们提出了可验证去除算法的第一个加密定义，以正式捕捉机器学习去除算法系统的保证。在此框架下，服务器首先计算一个证明，证明该模型在数据集 $D$ 上进行了训练。给定一个要删除的用户数据点 $d$，服务器使用去除算法更新模型。然后它提供正确执行去除算法并且 $d \notin D'$ 的证明，其中 $D'$ 是新的训练数据集。

    Machine unlearning aims to remove points from the training dataset of a machine learning model after training; for example when a user requests their data to be deleted. While many machine unlearning methods have been proposed, none of them enable users to audit the procedure. Furthermore, recent work shows a user is unable to verify if their data was unlearnt from an inspection of the model alone. Rather than reasoning about model parameters, we propose to view verifiable unlearning as a security problem. To this end, we present the first cryptographic definition of verifiable unlearning to formally capture the guarantees of a machine unlearning system. In this framework, the server first computes a proof that the model was trained on a dataset $D$. Given a user data point $d$ requested to be deleted, the server updates the model using an unlearning algorithm. It then provides a proof of the correct execution of unlearning and that $d \notin D'$, where $D'$ is the new training dataset
    
[^156]: 计算效率高的深度学习：算法趋势和机遇

    Compute-Efficient Deep Learning: Algorithmic Trends and Opportunities. (arXiv:2210.06640v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.06640](http://arxiv.org/abs/2210.06640)

    本文概述了算法效率高深度学习的研究。首先，提出了算法加速问题并制定了分类法。分类法突显了看似不同方法的共同点，并揭示了当前研究的空白。其次，提出了评估最佳实践以实现全面、公正和可靠的速度提升技术比较。最后，讨论了训练流程中常见的瓶颈。

    

    虽然深度学习在近年来取得了巨大的进展，但训练神经网络所产生的经济和环境成本已经变得不可持续。为了解决这个问题，已经有大量的研究致力于通过对训练程序语义的变化来降低训练成本，即算法效率高的深度学习。本文对这一领域的研究进行了系统全面的概述。首先，我们对"算法加速"问题进行了正式化，然后利用算法效率高训练的基本构建块来制定了分类法。我们的分类法突显了看似不同方法的共同点，并揭示了当前研究的空白。接下来，我们提出评估最佳实践以实现全面、公正和可靠的速度提升技术比较。为了进一步帮助研究和应用，我们讨论了训练流程中常见的瓶颈。

    Although deep learning has made great progress in recent years, the exploding economic and environmental costs of training neural networks are becoming unsustainable. To address this problem, there has been a great deal of research on *algorithmically-efficient deep learning*, which seeks to reduce training costs not at the hardware or implementation level, but through changes in the semantics of the training program. In this paper, we present a structured and comprehensive overview of the research in this field. First, we formalize the *algorithmic speedup* problem, then we use fundamental building blocks of algorithmically efficient training to develop a taxonomy. Our taxonomy highlights commonalities of seemingly disparate methods and reveals current research gaps. Next, we present evaluation best practices to enable comprehensive, fair, and reliable comparisons of speedup techniques. To further aid research and applications, we discuss common bottlenecks in the training pipeline (i
    
[^157]: 利用人体动作合成进行计算编舞

    Computational Choreography using Human Motion Synthesis. (arXiv:2210.04366v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.04366](http://arxiv.org/abs/2210.04366)

    本文介绍了一种利用深度学习模型分析舞蹈动作和生成新动作序列的方法，同时也结合了前人的努力来开发出一套系统。

    

    深度学习模型是否应该被训练来分析人体表演艺术？为了回答这个问题，我们探索了深度神经网络在合成艺术人体动作方面的应用。人体运动合成中的问题任务包括预测野外环境中人体运动，以及生成基于这些预测的新动作序列。我们将讨论一个非传统的应用潜力，即将学习模型应用于预测舞蹈动作。最近有一些显著的努力，以计算的方式分析舞蹈动作，例如Everybody Dance Now（EDN）学习模型和Cal Poly硕士论文Take The Lead（TTL）。我们有效地将这两个作品与我们自己的深度神经网络结合起来，生成了一种新的舞蹈动作预测系统、图像到图像的转换和视频生成。

    Should deep learning models be trained to analyze human performance art? To help answer this question, we explore an application of deep neural networks to synthesize artistic human motion. Problem tasks in human motion synthesis can include predicting the motions of humans in-the-wild, as well as generating new sequences of motions based on said predictions. We will discuss the potential of a less traditional application, where learning models are applied to predicting dance movements. There have been notable, recent efforts to analyze dance movements in a computational light, such as the Everybody Dance Now (EDN) learning model and a Cal Poly master's thesis, Take The Lead (TTL). We have effectively combined these two works along with our own deep neural network to produce a new system for dance motion prediction, image-to-image translation, and video generation.
    
[^158]: 邻域梯度聚类：一种用于非独立同分布数据分布的高效去中心化学习方法

    Neighborhood Gradient Clustering: An Efficient Decentralized Learning Method for Non-IID Data Distributions. (arXiv:2209.14390v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.14390](http://arxiv.org/abs/2209.14390)

    本文提出一种名为NGC的去中心化学习算法，用于对非独立同分布数据进行学习。该算法使用邻近代理的自梯度，模型变异交叉梯度和数据变异交叉梯度的加权平均值来修改每个代理的局部梯度。

    

    分布式数据集上的去中心化学习可能在代理之间具有显着不同的数据分布。当前最先进的去中心化算法大多假设数据分布是独立同分布的。本文旨在改进在非独立同分布数据上的去中心化学习。我们提出了一种新的去中心化学习算法Neighborhood Gradient Clustering (NGC)，该算法修改每个代理的局部梯度，使用自身和交叉梯度信息。交叉梯度是指相邻代理的模型参数的导数与另一个代理数据集关于参数的导数。特别地，该方法将模型的局部梯度替换为自梯度，模型变异交叉梯度（相邻代理关于本地数据集参数的导数）和数据变异交叉梯度（本地模型关于邻居数据集的导数）的加权平均值。

    Decentralized learning over distributed datasets can have significantly different data distributions across the agents. The current state-of-the-art decentralized algorithms mostly assume the data distributions to be Independent and Identically Distributed. This paper focuses on improving decentralized learning over non-IID data. We propose \textit{Neighborhood Gradient Clustering (NGC)}, a novel decentralized learning algorithm that modifies the local gradients of each agent using self- and cross-gradient information. Cross-gradients for a pair of neighboring agents are the derivatives of the model parameters of an agent with respect to the dataset of the other agent. In particular, the proposed method replaces the local gradients of the model with the weighted mean of the self-gradients, model-variant cross-gradients (derivatives of the neighbors' parameters with respect to the local dataset), and data-variant cross-gradients (derivatives of the local model with respect to its neighb
    
[^159]: AirTrack：用于长程飞机检测和跟踪的机载深度学习框架

    AirTrack: Onboard Deep Learning Framework for Long-Range Aircraft Detection and Tracking. (arXiv:2209.12849v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.12849](http://arxiv.org/abs/2209.12849)

    AirTrack是一个用于小型无人机系统的实时视觉检测和跟踪框架，使用全分辨率图像和深度学习框架以提高检测和跟踪性能。实验结果显示，在Amazon AOT数据集上表现出优越性。同时，多次真实世界中进行的飞行测试均显示该方法的有效性。

    

    无人机系统安全运行需要具备检测和避免碰撞（DAA）能力。本文引入了AirTrack，这是一个实时的、仅使用视觉信息的检测和跟踪框架，考虑了小型无人机系统的大小、重量和功耗（SWaP）限制。鉴于远程飞机的低信噪比（SNR），我们提出使用全分辨率图像，使用深度学习框架对齐连续图像以消除自我运动。对齐后的图像在级联的主要和次要分类器中使用，以提高多个指标的检测和跟踪性能。我们展示了AirTrack在Amazon空中物体跟踪（AOT）数据集上优于最先进的基准线。多次在真实世界中进行的飞行测试，包括与通用航空交通互动的Cessna 182和Bell直升机向受控飞行无人机靠近的附近碰撞飞行测试显示，该方法满足新引入的A要求。

    Detect-and-Avoid (DAA) capabilities are critical for safe operations of unmanned aircraft systems (UAS). This paper introduces, AirTrack, a real-time vision-only detect and tracking framework that respects the size, weight, and power (SWaP) constraints of sUAS systems. Given the low Signal-to-Noise ratios (SNR) of far away aircraft, we propose using full resolution images in a deep learning framework that aligns successive images to remove ego-motion. The aligned images are then used downstream in cascaded primary and secondary classifiers to improve detection and tracking performance on multiple metrics. We show that AirTrack outperforms state-of-the art baselines on the Amazon Airborne Object Tracking (AOT) Dataset. Multiple real world flight tests with a Cessna 182 interacting with general aviation traffic and additional near-collision flight tests with a Bell helicopter flying towards a UAS in a controlled setting showcase that the proposed approach satisfies the newly introduced A
    
[^160]: 基于深度学习的类型推断系统的跨域评估

    Cross-Domain Evaluation of a Deep Learning-Based Type Inference System. (arXiv:2208.09189v3 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2208.09189](http://arxiv.org/abs/2208.09189)

    本文研究了基于深度学习的类型推断系统Type4Py的跨领域泛化能力，并通过广泛的实验表明，Type4Py在应用于不同领域的类型推断时能够提供优异的性能。

    

    可选类型注释允许在动态编程语言中增加静态类型特性，例如更好的集成开发环境（IDE）支持、更精确的程序分析以及类型相关的运行时错误的早期检测和预防。基于机器学习的类型推断为自动化此任务提供了有趣的结果。然而，这些系统的实际使用取决于它们跨不同领域的泛化能力，因为它们经常被应用于训练领域之外。本文通过进行广泛的跨领域实验，将Type4Py作为最先进的基于深度学习的类型推断系统的代表进行了研究。我们解决以下问题：类不平衡、词汇表外单词、数据集转移和未知类。为了进行这样的实验，我们使用了ManyTypes4Py和CrossDomainTypes4Py数据集。我们在本文中介绍了后者。我们的数据集可以在更现实的情况下评估类型推断系统，在该情况下程序领域与训练集中的领域不同。我们表明，Type4Py优于基准方法，并为不同领域的类型推断提供了可伸缩的解决方案。

    Optional type annotations allow for enriching dynamic programming languages with static typing features like better Integrated Development Environment (IDE) support, more precise program analysis, and early detection and prevention of type-related runtime errors. Machine learning-based type inference promises interesting results for automating this task. However, the practical usage of such systems depends on their ability to generalize across different domains, as they are often applied outside their training domain. In this work, we investigate Type4Py as a representative of state-of-the-art deep learning-based type inference systems, by conducting extensive cross-domain experiments. Thereby, we address the following problems: class imbalances, out-of-vocabulary words, dataset shifts, and unknown classes. To perform such experiments, we use the datasets ManyTypes4Py and CrossDomainTypes4Py. The latter we introduce in this paper. Our dataset enables the evaluation of type inference sy
    
[^161]: 因果发现后的有效推断

    Valid Inference after Causal Discovery. (arXiv:2208.05949v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2208.05949](http://arxiv.org/abs/2208.05949)

    本研究开发了工具以实现因果发现后的有效推断，解决了使用相同数据运行因果发现算法后估计因果效应导致经典置信区间的覆盖保证无效问题。

    

    因果发现和因果效应估计是因果推断中的两个基本任务。虽然已经针对每个任务单独开发了许多方法，但是同时应用这些方法时会出现统计上的挑战：在对相同数据运行因果发现算法后估计因果效应会导致"双重挑选"，从而使经典置信区间的覆盖保证无效。为此，我们开发了针对因果发现后有效的推断工具。通过实证研究，我们发现，天真组合因果发现算法和随后推断算法会导致高度膨胀的误覆盖率，而应用我们的方法则提供可靠的覆盖并实现比数据分割更准确的因果发现。

    Causal discovery and causal effect estimation are two fundamental tasks in causal inference. While many methods have been developed for each task individually, statistical challenges arise when applying these methods jointly: estimating causal effects after running causal discovery algorithms on the same data leads to "double dipping," invalidating the coverage guarantees of classical confidence intervals. To this end, we develop tools for valid post-causal-discovery inference. Across empirical studies, we show that a naive combination of causal discovery and subsequent inference algorithms leads to highly inflated miscoverage rates; on the other hand, applying our method provides reliable coverage while achieving more accurate causal discovery than data splitting.
    
[^162]: PhyGNNet：使用物理信息图神经网络解决时空偏微分方程问题

    PhyGNNet: Solving spatiotemporal PDEs with Physics-informed Graph Neural Network. (arXiv:2208.04319v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2208.04319](http://arxiv.org/abs/2208.04319)

    本文提出了PhyGNNet，一种使用图神经网络解决偏微分方程问题的方法，通过将计算区域划分为规则的网格并构造偏微分损失来优化网络模型。实验证明该方法在时间和空间范围内具有更好的拟合能力和外推能力。

    

    解决偏微分方程（PDE）是物理、生物和化学领域重要的研究手段，近年来，PINN作为数值方法的一种近似替代品，受到了广泛的关注并在许多领域中发挥了重要的作用。然而，PINN用全连接网络作为其模型，其拟合能力和时间空间外推能力有限。在本文中，我们基于图神经网络提出了PhyGNNet用于解决偏微分方程，该网络由编码器、处理器和解码器块组成。特别的，我们将计算区域划分为规则的网格，定义在网格上的偏微分算子，然后构造网络的偏微分损失以优化构建PhyGNNet模型。此外，我们在Burgers方程和热方程上进行比较实验以验证我们的方法，结果表明我们的方法在时间和空间范围内拥有更好的拟合能力和外推能力。

    Solving partial differential equations (PDEs) is an important research means in the fields of physics, biology, and chemistry. As an approximate alternative to numerical methods, PINN has received extensive attention and played an important role in many fields. However, PINN uses a fully connected network as its model, which has limited fitting ability and limited extrapolation ability in both time and space. In this paper, we propose PhyGNNet for solving partial differential equations on the basics of a graph neural network which consists of encoder, processer, and decoder blocks. In particular, we divide the computing area into regular grids, define partial differential operators on the grids, then construct pde loss for the network to optimize to build PhyGNNet model. What's more, we conduct comparative experiments on Burgers equation and heat equation to validate our approach, the results show that our method has better fit ability and extrapolation ability both in time and spatial
    
[^163]: TaDaa: 用于客户支持、帮助台和问题登记系统的实时票务分配深度学习自动顾问

    TaDaa: real time Ticket Assignment Deep learning Auto Advisor for customer support, help desk, and issue ticketing systems. (arXiv:2207.11187v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2207.11187](http://arxiv.org/abs/2207.11187)

    TaDaa是一个利用最新的Transformer模型和机器学习技术，用于客户支持、帮助台和问题登记系统的实时票务分配深度学习自动顾问，它可以分配问题给正确的组、分配问题给最佳的解决者，并向解决者提供最相关的先前解决的问题，其在实验中表现优异，能够极大地提高平均解决时间。

    

    本文提出了TaDaa：票务分配深度学习自动顾问，利用最新的Transformer模型和机器学习技术，快速分配组织内的问题，如客户支持、帮助台和其他问题登记系统。该项目提供以下功能：1）将问题分配给正确的组；2）将问题分配给最佳的解决者；3）向解决者提供最相关的先前解决的问题。我们利用一个样本数据集，其中包括3k+个组和10k+个解决者，实现了95.2%的前三建议准确率和79.0%的前五解决者建议准确率。我们希望这项研究将大大提高客户支持、帮助台和问题登记系统的平均解决时间。

    This paper proposes TaDaa: Ticket Assignment Deep learning Auto Advisor, which leverages the latest Transformers models and machine learning techniques quickly assign issues within an organization, like customer support, help desk and alike issue ticketing systems. The project provides functionality to 1) assign an issue to the correct group, 2) assign an issue to the best resolver, and 3) provide the most relevant previously solved tickets to resolvers. We leverage one ticketing system sample dataset, with over 3k+ groups and over 10k+ resolvers to obtain a 95.2% top 3 accuracy on group suggestions and a 79.0% top 5 accuracy on resolver suggestions. We hope this research will greatly improve average issue resolution time on customer support, help desk, and issue ticketing systems.
    
[^164]: D3G: 从演示中学习多机器人协调

    D3G: Learning Multi-robot Coordination from Demonstrations. (arXiv:2207.08892v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2207.08892](http://arxiv.org/abs/2207.08892)

    本文提出了一个D3G框架，可以从演示中学习多机器人协调。通过最小化轨迹与演示之间的不匹配，每个机器人可以自动调整其个体动态和目标，提高了学习效率和效果。

    

    本文开发了一个分布式可微动态游戏（D3G）框架，可以实现从演示中学习多机器人协调。我们将多机器人协调表示为一个动态游戏，其中一个机器人的行为受其自身动态和目标的控制，同时也取决于其他机器人的行为。因此，通过调整每个机器人的目标和动态，可以适应协调。所提出的D3G使每个机器人通过最小化其轨迹与演示之间的不匹配，在分布式方式下自动调整其个体动态和目标。该学习框架具有新的设计，包括一个前向传递，所有机器人合作寻找游戏的纳什均衡，以及一个反向传递，在通信图中传播梯度。我们在仿真中测试了D3G，并给出了不同任务配置的两种机器人。结果证明了D3G学习多机器人协调的能力。

    This paper develops a Distributed Differentiable Dynamic Game (D3G) framework, which enables learning multi-robot coordination from demonstrations. We represent multi-robot coordination as a dynamic game, where the behavior of a robot is dictated by its own dynamics and objective that also depends on others' behavior. The coordination thus can be adapted by tuning the objective and dynamics of each robot. The proposed D3G enables each robot to automatically tune its individual dynamics and objectives in a distributed manner by minimizing the mismatch between its trajectory and demonstrations. This learning framework features a new design, including a forward-pass, where all robots collaboratively seek Nash equilibrium of a game, and a backward-pass, where gradients are propagated via the communication graph. We test the D3G in simulation with two types of robots given different task configurations. The results validate the capability of D3G for learning multi-robot coordination from de
    
[^165]: Ask-AC: 一种循环中的主动顾问演员-评论家框架

    Ask-AC: An Initiative Advisor-in-the-Loop Actor-Critic Framework. (arXiv:2207.01955v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.01955](http://arxiv.org/abs/2207.01955)

    本文提出了一种新颖的主动顾问演员-评论家框架，Ask-AC，它替换了传统的被动监督信号机制，实现了定制化和高效的信息交换，其中的两个互补组件允许代理主动寻求顾问干预和识别漏掉的不稳定状态。

    

    尽管交互式强化学习方案取得了很多有希望的结果，但目前的方案仍然依赖于来自顾问专家的被动监督信号，形式包括持续监控或预定义规则，这不可避免地导致了一种麻烦而昂贵的学习过程。在本文中，我们介绍了一种新的主动顾问演员-评论家框架，称为Ask-AC，它用一个双向的学习者主动机制替换了单向的顾问指导机制，从而实现了学习者和顾问之间的定制化和有效的信息交换。Ask-AC 的核心是两个互补的组件，分别是动作请求者和自适应状态选择器，可以方便地纳入各种离散的演员-评论家架构中。前者允许代理主动寻求不确定状态下的顾问干预，后者则可以识别漏掉的不稳定状态。

    Despite the promising results achieved, state-of-the-art interactive reinforcement learning schemes rely on passively receiving supervision signals from advisor experts, in the form of either continuous monitoring or pre-defined rules, which inevitably result in a cumbersome and expensive learning process. In this paper, we introduce a novel initiative advisor-in-the-loop actor-critic framework, termed as Ask-AC, that replaces the unilateral advisor-guidance mechanism with a bidirectional learner-initiative one, and thereby enables a customized and efficacious message exchange between learner and advisor. At the heart of Ask-AC are two complementary components, namely action requester and adaptive state selector, that can be readily incorporated into various discrete actor-critic architectures. The former component allows the agent to initiatively seek advisor intervention in the presence of uncertain states, while the latter identifies the unstable states potentially missed by the for
    
[^166]: 消除噪声的MDPs：学习比现实世界本身更好的世界模型

    Denoised MDPs: Learning World Models Better Than the World Itself. (arXiv:2206.15477v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.15477](http://arxiv.org/abs/2206.15477)

    本文提出一种新的去噪MDP学习方法，该方法可以将现实数据中的噪声干扰因素去除，学习一个更好的世界模型，实验结果表明该方法在任务上表现更加优秀。

    

    分离信号与噪声，并能理性地把握有效信息对于智能至关重要。这使得人类可以高效地完成现实任务而不必考虑所有可能的烦琐因素。那么，人工智能代理如何才能做到这一点？代理可以放弃哪些信息以避免噪声的干扰？本文基于可控性和与奖励的关系将信息分为四类，并将有用信息定义为既可控制又与奖励相关的信息。该框架澄清了各种强化学习（RL）中的表示学习的先前工作删除的信息类型，并导致我们提出了学习消除某些噪声干扰的去噪MDP的方法。在DeepMind控制套件和RoboDesk的各种变体上进行的广泛实验表明，与仅使用原始观测数据以及先前的工作相比，我们的去噪世界模型表现出卓越的性能。

    The ability to separate signal from noise, and reason with clean abstractions, is critical to intelligence. With this ability, humans can efficiently perform real world tasks without considering all possible nuisance factors.How can artificial agents do the same? What kind of information can agents safely discard as noises?  In this work, we categorize information out in the wild into four types based on controllability and relation with reward, and formulate useful information as that which is both controllable and reward-relevant. This framework clarifies the kinds information removed by various prior work on representation learning in reinforcement learning (RL), and leads to our proposed approach of learning a Denoised MDP that explicitly factors out certain noise distractors. Extensive experiments on variants of DeepMind Control Suite and RoboDesk demonstrate superior performance of our denoised world model over using raw observations alone, and over prior works, across policy opt
    
[^167]: Merak：高效的分布式DNN训练框架，具备自动化的三维并行技术，适用于庞大的基础模型

    Merak: An Efficient Distributed DNN Training Framework with Automated 3D Parallelism for Giant Foundation Models. (arXiv:2206.04959v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.04959](http://arxiv.org/abs/2206.04959)

    Merak是一个自动化的三维并行深度学习训练框架，它解决了当前其他框架中需要手动修改模型才可并行的问题，并具有较高的计算、GPU内存和网络带宽利用率。

    

    基础模型正在成为深度学习技术的主流。由于模型参数和训练数据集规模庞大，预训练基础模型始终需要耗费时间。除了计算密集型，训练过程还极其依赖内存和通信，这就需要应用三维并行技术，即集成数据并行、管道模型并行和张量模型并行，以实现高效训练。为此，研发了一些自定义软件框架，如Megatron-LM和DeepSpeed。然而，当前的三位并行技术框架仍存在两个问题：i）对于需要手动修改模型以并行训练的模型开发人员来说，框架并不透明。ii）它们的计算、GPU内存和网络带宽利用不足。我们提出了Merak，一个高资源利用率的自动化三维并行深度学习训练框架。

    Foundation models are becoming the dominant deep learning technologies. Pretraining a foundation model is always time-consumed due to the large scale of both the model parameter and training dataset. Besides being computing-intensive, the training process is extremely memory-intensive and communication-intensive. These features make it necessary to apply 3D parallelism, which integrates data parallelism, pipeline model parallelism and tensor model parallelism, to achieve high training efficiency.  To achieve this goal, some custom software frameworks such as Megatron-LM and DeepSpeed are developed. However, current 3D parallelism frameworks still meet two issues: i) they are not transparent to model developers, which need to manually modify the model to parallelize training. ii) their utilization of computation, GPU memory and network bandwidth are not sufficient. We propose Merak, an automated 3D parallelism deep learning training framework with high resource utilization. Merak automa
    
[^168]: GAMR: 一种用于 (视觉) 推理的引导式注意力模型

    GAMR: A Guided Attention Model for (visual) Reasoning. (arXiv:2206.04928v5 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2206.04928](http://arxiv.org/abs/2206.04928)

    本文介绍了一个新的模块——GAMR，它是一种用于(视觉)推理的引导式注意力模型，以动态选择任务相关的视觉信息并将其路由到记忆中来解决复杂的视觉推理任务，并在各种任务和数据集上取得了成功的实验结果。

    

    人类在灵活分析和理解复杂的视觉场景方面仍然优于现代人工智能系统。本文提出了一种新的视觉推理模块——引导式注意力模型(GAMR)，它通过选择和路由与任务相关的视觉信息到记忆中的注意力转移序列来体现主动视觉理论。在各种视觉推理任务和数据集上的实验显示，GAMR能够以稳健且样本高效的方式学习视觉例程。此外，GAMR在全新的推理任务上表现出了零样本泛化的能力。总的来说，我们的研究提供了计算支持，支持认知理论假设需要注意力和记忆之间的关键相互作用，以动态地维护和操作任务相关的视觉信息来解决复杂的视觉推理任务。

    Humans continue to outperform modern AI systems in their ability to flexibly parse and understand complex visual scenes. Here, we present a novel module for visual reasoning, the Guided Attention Model for (visual) Reasoning (GAMR), which instantiates an active vision theory -- positing that the brain solves complex visual reasoning problems dynamically -- via sequences of attention shifts to select and route task-relevant visual information into memory. Experiments on an array of visual reasoning tasks and datasets demonstrate GAMR's ability to learn visual routines in a robust and sample-efficient manner. In addition, GAMR is shown to be capable of zero-shot generalization on completely novel reasoning tasks. Overall, our work provides computational support for cognitive theories that postulate the need for a critical interplay between attention and memory to dynamically maintain and manipulate task-relevant visual information to solve complex visual reasoning tasks.
    
[^169]: 半监督语义引导的对抗训练用于轨迹预测

    Semi-supervised Semantics-guided Adversarial Training for Trajectory Prediction. (arXiv:2205.14230v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.14230](http://arxiv.org/abs/2205.14230)

    提出了一种半监督对抗训练方法，通过半监督对抗自编码器提取出领域知识并建模分离的语义特征，为轨迹预测任务增强了鲁棒性。

    

    针对自动驾驶车辆等自主系统重要的任务之一——预测周围物体的轨迹，本文提出了一种新的对抗训练方法。相较于典型的图像任务的对抗训练，我们的工作面临更多的随机输入和缺少类标签的挑战。为了解决这些问题，我们提出了一种基于半监督对抗自编码器的方法，通过领域知识建模分离的语义特征，为对抗训练提供了额外的潜在标签。

    Predicting the trajectories of surrounding objects is a critical task for self-driving vehicles and many other autonomous systems. Recent works demonstrate that adversarial attacks on trajectory prediction, where small crafted perturbations are introduced to history trajectories, may significantly mislead the prediction of future trajectories and induce unsafe planning. However, few works have addressed enhancing the robustness of this important safety-critical task.In this paper, we present a novel adversarial training method for trajectory prediction. Compared with typical adversarial training on image tasks, our work is challenged by more random input with rich context and a lack of class labels. To address these challenges, we propose a method based on a semi-supervised adversarial autoencoder, which models disentangled semantic features with domain knowledge and provides additional latent labels for the adversarial training. Extensive experiments with different types of attacks de
    
[^170]: 标定至关重要：解决大规模广告推荐系统中的极大化偏差问题

    Calibration Matters: Tackling Maximization Bias in Large-scale Advertising Recommendation Systems. (arXiv:2205.09809v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.09809](http://arxiv.org/abs/2205.09809)

    本文提出了一种有效、稳健且实用的方差调整去偏（VAD）算法，用于解决大规模广告推荐系统中的极大化偏差问题，该算法可以缓解标定经常受到的极大化偏差问题，并提高广告推荐系统的实用性能。

    

    标定是指平均预测点击率与真实点击率之比。标定的优化对许多在线广告推荐系统至关重要，因为它直接影响广告竞价和向广告客户收取的费用。尽管其重要性，标定优化经常受到称为“极大化偏差”的问题的困扰。极大化偏差指的是预测值的最大值高估了真正的最大值的现象。该问题是由于标定是在预测模型本身选择的集合上计算的。即使可以在每个数据点上实现无偏的预测，该问题仍然存在，并且在训练和测试集之间存在协变量移位时会变得更糟。为了缓解这个问题，本文提出了一个理论上对极大化偏差进行量化并提出方差调整去偏（VAD）元算法的算法。该算法高效、稳健且实用，因为它具有线性时间和空间复杂度。

    Calibration is defined as the ratio of the average predicted click rate to the true click rate. The optimization of calibration is essential to many online advertising recommendation systems because it directly affects the downstream bids in ads auctions and the amount of money charged to advertisers. Despite its importance, calibration optimization often suffers from a problem called "maximization bias". Maximization bias refers to the phenomenon that the maximum of predicted values overestimates the true maximum. The problem is introduced because the calibration is computed on the set selected by the prediction model itself. It persists even if unbiased predictions can be achieved on every datapoint and worsens when covariate shifts exist between the training and test sets. To mitigate this problem, we theorize the quantification of maximization bias and propose a variance-adjusting debiasing (VAD) meta-algorithm in this paper. The algorithm is efficient, robust, and practical as it 
    
[^171]: DeepGraviLens：一种用于分类引力透镜数据的多模态网络架构

    DeepGraviLens: a Multi-Modal Architecture for Classifying Gravitational Lensing Data. (arXiv:2205.00701v3 [astro-ph.IM] UPDATED)

    [http://arxiv.org/abs/2205.00701](http://arxiv.org/abs/2205.00701)

    DeepGraviLens是一种多模态神经网络，用于分类属于不同类型的引力透镜数据，具有高精度和优于现有方法的结果。

    

    引力透镜是由大质量物体产生的相对论效应，会弯曲其周围的时空。这是天体物理学中一个深入研究的课题，允许验证理论相对论结果并研究一些否则不可见的微弱天体物体。近年来，机器学习方法已被应用于支持引力透镜现象的分析，通过检测与亮度变化时间序列相关的图像数据集中的透镜效应。然而，当前的方法要么仅考虑图像而忽略时间序列数据，要么在最困难的数据集上实现相对较低的准确性。本文介绍了 DeepGraviLens，这是一种新颖的多模态网络，用于分类属于一个非透镜系统类型和三个透镜系统类型的时空数据。它在准确性方面超过当前的 state-of-art 方法，提高了约 19% 到 43%，具体取决于所考虑的数据集。

    Gravitational lensing is the relativistic effect generated by massive bodies, which bend the space-time surrounding them. It is a deeply investigated topic in astrophysics and allows validating theoretical relativistic results and studying faint astrophysical objects that would not be visible otherwise. In recent years Machine Learning methods have been applied to support the analysis of the gravitational lensing phenomena by detecting lensing effects in data sets consisting of images associated with brightness variation time series. However, the state-of-art approaches either consider only images and neglect time-series data or achieve relatively low accuracy on the most difficult data sets. This paper introduces DeepGraviLens, a novel multi-modal network that classifies spatio-temporal data belonging to one non-lensed system type and three lensed system types. It surpasses the current state of the art accuracy results by $\approx$ 19% to $\approx$ 43%, depending on the considered dat
    
[^172]: 用变分自编码器学习和控制语音的源-滤波表示

    Learning and controlling the source-filter representation of speech with a variational autoencoder. (arXiv:2204.07075v3 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2204.07075](http://arxiv.org/abs/2204.07075)

    本文提出了一种用变分自编码器学习和控制语音的源-滤波表示的方法。基于源-滤波模型假设，提出了一种方法来识别编码$f_0$和前三个共振峰频率的潜在子空间，并证明了这些子空间可以被用于对语音进行控制。

    

    理解和控制深度生成模型中的潜在表示对于分析、转换和生成各种类型的数据是一个具有挑战性但重要的问题。在语音处理方面，受到声音生理学机制的启发，源-滤波模型认为语音信号是从几个独立且物理意义连续的潜在因素产生的，其中基频$f_0$和共振峰是最重要的。本文从一个在大量未标记的自然语音信号上无监督训练的变分自编码器（VAE）开始，展示了语音产生的源-滤波模型自然地显现为VAE潜在空间的正交子空间。仅使用人工语音合成器生成的少量标记语音信号，我们提出了一种方法来识别编码$f_0$和前三个共振峰频率的潜在子空间，并证明了这些子空间可以被用于对语音进行控制。

    Understanding and controlling latent representations in deep generative models is a challenging yet important problem for analyzing, transforming and generating various types of data. In speech processing, inspiring from the anatomical mechanisms of phonation, the source-filter model considers that speech signals are produced from a few independent and physically meaningful continuous latent factors, among which the fundamental frequency $f_0$ and the formants are of primary importance. In this work, we start from a variational autoencoder (VAE) trained in an unsupervised manner on a large dataset of unlabeled natural speech signals, and we show that the source-filter model of speech production naturally arises as orthogonal subspaces of the VAE latent space. Using only a few seconds of labeled speech signals generated with an artificial speech synthesizer, we propose a method to identify the latent subspaces encoding $f_0$ and the first three formant frequencies, we show that these su
    
[^173]: 神经消息传递用于客观目标的不确定性量化和最优实验设计

    Neural Message Passing for Objective-Based Uncertainty Quantification and Optimal Experimental Design. (arXiv:2203.07120v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.07120](http://arxiv.org/abs/2203.07120)

    本论文提出了一种降低计算成本实现客观目标不确定性量化和最优实验设计的新方案。

    

    许多实际科学应用涉及对具有许多未知参数的复杂不确定系统进行数学建模。在这些系统中，准确的参数估计通常是不切实际的，因为可用的训练数据可能是不充分的，获取更多数据的成本可能很高。在这种情况下，我们可以基于贝叶斯模型设计强大的操作符，并设计最优实验来有效地减少不确定性，从而最大程度地提高这些操作符的性能表现。

    Various real-world scientific applications involve the mathematical modeling of complex uncertain systems with numerous unknown parameters. Accurate parameter estimation is often practically infeasible in such systems, as the available training data may be insufficient and the cost of acquiring additional data may be high. In such cases, based on a Bayesian paradigm, we can design robust operators retaining the best overall performance across all possible models and design optimal experiments that can effectively reduce uncertainty to enhance the performance of such operators maximally. While objective-based uncertainty quantification (objective-UQ) based on MOCU (mean objective cost of uncertainty) provides an effective means for quantifying uncertainty in complex systems, the high computational cost of estimating MOCU has been a challenge in applying it to real-world scientific/engineering problems. In this work, we propose a novel scheme to reduce the computational cost for objectiv
    
[^174]: 被自然扭曲的赌徒问题: 对遗憾和鲁棒乐观算法的下限分析

    Bandits Corrupted by Nature: Lower Bounds on Regret and Robust Optimistic Algorithm. (arXiv:2203.03186v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.03186](http://arxiv.org/abs/2203.03186)

    本文研究了一种被自然扭曲的赌徒问题，提出了HubUCB算法，利用Huber估计实现了接近最优的效果。

    

    本文研究了赌徒问题中的一种特殊情况，即在$k$个未知奖励分布为重尾分布的臂中选择，在每轮操作中，以$1-\varepsilon \in(0.5,1]$的概率来自奖励分布，以$\varepsilon \in[0,0.5)$的概率来自未知的扭曲分布。首先，我们提供了任何扭曲赌徒算法“遗憾”的一个问题相关下限。较之亚高斯或重尾奖励的经典随机赌徒问题，上述结果表明扭曲赌徒问题更为困难。接下来，我们提出了针对扭曲赌徒的一种新型上置信界算法，名为 HubUCB，该算法基于 Huber 的鲁棒均值估计，利用 Huber 估计量的一种新型集中不等式，证明了 HubUCB 实现了接近最优的效果。

    We study the corrupted bandit problem, i.e. a stochastic multi-armed bandit problem with $k$ unknown reward distributions, which are heavy-tailed and corrupted by a history-independent adversary or Nature. To be specific, the reward obtained by playing an arm comes from corresponding heavy-tailed reward distribution with probability $1-\varepsilon \in (0.5,1]$ and an arbitrary corruption distribution of unbounded support with probability $\varepsilon \in [0,0.5)$.  First, we provide $\textit{a problem-dependent lower bound on the regret}$ of any corrupted bandit algorithm. The lower bounds indicate that the corrupted bandit problem is harder than the classical stochastic bandit problem with sub-Gaussian or heavy-tail rewards.  Following that, we propose a novel UCB-type algorithm for corrupted bandits, namely HubUCB, that builds on Huber's estimator for robust mean estimation. Leveraging a novel concentration inequality of Huber's estimator, we prove that HubUCB achieves a near-optimal
    
[^175]: 基于模型的强化学习的可证实性优化与验证

    Joint Differentiable Optimization and Verification for Certified Reinforcement Learning. (arXiv:2201.12243v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.12243](http://arxiv.org/abs/2201.12243)

    本文提出了一个新的优化问题框架，用于基于模型的强化学习中的正式验证，同时解决了在前期学习和后期验证之间难以获得证书的问题。该框架通过梯度来微分证书和价值函数，并且在实验中表现出大于其他方法的优势，可以找到具有保护函数和李雅普诺夫函数的可行控制器，确保了系统的安全性。

    

    在面对安全关键控制系统的模型化强化学习中，根据学习到的控制器正式认证系统属性（例如安全、稳定）是非常重要的。然而，现有方法通常在学习控制器之后才应用形式验证，在经历了多次迭代的学习和验证之后语言考取得到任何证书有时是非常困难的。为了解决这个问题，我们提出了一个框架，通过构建和解决一个新的双层优化问题来同时进行强化学习和正式验证，该问题可以由价值函数和证书的梯度进行微分。在各种示例的实验中，我们的框架在与基于模型的随机值梯度（SVG）方法和基于模型的无模型近端策略优化（PPO）方法相比，在寻找具有保护函数和李雅普诺夫函数的可行控制器方面具有显著的优势，从而保证了系统的安全性。

    In model-based reinforcement learning for safety-critical control systems, it is important to formally certify system properties (e.g., safety, stability) under the learned controller. However, as existing methods typically apply formal verification \emph{after} the controller has been learned, it is sometimes difficult to obtain any certificate, even after many iterations between learning and verification. To address this challenge, we propose a framework that jointly conducts reinforcement learning and formal verification by formulating and solving a novel bilevel optimization problem, which is differentiable by the gradients from the value function and certificates. Experiments on a variety of examples demonstrate the significant advantages of our framework over the model-based stochastic value gradient (SVG) method and the model-free proximal policy optimization (PPO) method in finding feasible controllers with barrier functions and Lyapunov functions that ensure system safety and 
    
[^176]: 用统计和机器学习打击洗钱：综述与介绍

    Fighting Money Laundering with Statistics and Machine Learning: An Introduction and Review. (arXiv:2201.04207v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2201.04207](http://arxiv.org/abs/2201.04207)

    本文介绍了银行反洗钱的统计和机器学习方法，并提出客户风险评估和可疑行为标识两个核心要素。未来的研究方向包括生成合成数据、半监督和深度学习、可解释性以及结果的公平性。

    

    洗钱是一个严重的全球性问题，但是针对反洗钱的统计和机器学习方法的科学文献却很少。本文着重于银行反洗钱，并提供了文献综述和介绍。我们提出了一个统一的术语，其中包括客户风险评估和可疑行为标识两个核心要素。我们发现，客户风险评估是通过诊断来寻找和解释风险因素，而可疑行为标识则是通过未公开的特征和手工风险指数来实现的。最后，我们讨论了未来研究的方向，其中主要挑战之一是需要更多的公共数据集，这可能可以通过生成合成数据来解决，其他可能的研究方向包括半监督和深度学习、可解释性以及结果的公平性。

    Money laundering is a profound global problem. Nonetheless, there is little scientific literature on statistical and machine learning methods for anti-money laundering. In this paper, we focus on anti-money laundering in banks and provide an introduction and review of the literature. We propose a unifying terminology with two central elements: (i) client risk profiling and (ii) suspicious behavior flagging. We find that client risk profiling is characterized by diagnostics, i.e., efforts to find and explain risk factors. On the other hand, suspicious behavior flagging is characterized by non-disclosed features and hand-crafted risk indices. Finally, we discuss directions for future research. One major challenge is the need for more public data sets. This may potentially be addressed by synthetic data generation. Other possible research directions include semi-supervised and deep learning, interpretability, and fairness of the results.
    
[^177]: 弱凸和多凸代理函数的随机正则化主导极小化算法。

    Stochastic regularized majorization-minimization with weakly convex and multi-convex surrogates. (arXiv:2201.01652v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2201.01652](http://arxiv.org/abs/2201.01652)

    本文提出了一个SMM的扩展，该扩展支持代理函数的弱凸性和块多凸性，并在解决非凸约束问题上具有显著优越性能。

    

    随机主导极小化（SMM）是一类采样新数据点并最小化目标函数的代理函数的递归平均数的优化算法。要求代理函数是强凸的，非凸情况下的收敛率分析并不可行。该论文提出了一个SMM的扩展，该扩展允许代理函数仅为弱凸或块多凸，并且平均代理函数在近端正则化或块最小化的减小半径中近似最小化。实验证明，该算法在凸约束下处理非i.i.d.数据样本的一阶最优性差距以速率$O((\log n)^{1+\epsilon}/n^{1/2})$下降，期望损失为$O((\log n)^{1+\epsilon}/n^{1/4})$，其中$n$表示处理的数据样本数量。在一些额外的假设下，还提供了目标函数的后期收敛速率。该方法在多个真实数据集上得到展示，并与最先进的方法进行了比较。

    Stochastic majorization-minimization (SMM) is a class of stochastic optimization algorithms that proceed by sampling new data points and minimizing a recursive average of surrogate functions of an objective function. The surrogates are required to be strongly convex and convergence rate analysis for the general non-convex setting was not available. In this paper, we propose an extension of SMM where surrogates are allowed to be only weakly convex or block multi-convex, and the averaged surrogates are approximately minimized with proximal regularization or block-minimized within diminishing radii, respectively. For the general nonconvex constrained setting with non-i.i.d. data samples, we show that the first-order optimality gap of the proposed algorithm decays at the rate $O((\log n)^{1+\epsilon}/n^{1/2})$ for the empirical loss and $O((\log n)^{1+\epsilon}/n^{1/4})$ for the expected loss, where $n$ denotes the number of data samples processed. Under some additional assumption, the lat
    
[^178]: 针对深度神经网络的尖锐感知量化方法

    Sharpness-aware Quantization for Deep Neural Networks. (arXiv:2111.12273v5 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2111.12273](http://arxiv.org/abs/2111.12273)

    本文提出了一种新的方法，称为尖锐感知量化（SAQ），它利用尖锐感知最小化（SAM）来平滑尖锐的量化噪声和扰动，增强了模型的鲁棒性，同时保持了高压缩性。实验结果表明，SAQ 在 ImageNet 上的精度比现有的最先进的量化方法高出3.6％。

    

    网络量化是模型压缩的主导方法。然而，量化训练中量化权重的突然变化通常会导致严重的损失波动，导致尖锐的损失地形，使梯度不稳定，从而降低性能。最近，提出了尖锐感知最小化（SAM）来平滑损失地形，并提高模型的泛化性能。然而，直接将SAM应用于量化模型可能会导致扰动不匹配或缩减问题，从而导致次优性能。在本文中，我们提出了一种新的方法，称为尖锐感知量化（SAQ），首次探索了SAM在模型压缩中的效果，特别是量化。具体而言，我们首先将量化和SAM视为分别引入模型权重的量化噪声和对抗扰动的方法，并提供了一个统一的视角。根据噪声和扰动术语是平滑还是尖锐，我们将其分为四类，并研究它们对模型性能的影响。基于这种理解，我们提出了SAQ，明确地利用SAM平滑尖锐的量化噪声和扰动，从而增强了模型的鲁棒性，同时保持了高压缩性。广泛的实验表明，SAQ始终优于现有的最先进的量化方法，比最接近的竞争对手在ImageNet上的精度高达3.6％。

    Network quantization is a dominant paradigm of model compression. However, the abrupt changes in quantized weights during training often lead to severe loss fluctuations and result in a sharp loss landscape, making the gradients unstable and thus degrading the performance. Recently, Sharpness-Aware Minimization (SAM) has been proposed to smooth the loss landscape and improve the generalization performance of the models. Nevertheless, directly applying SAM to the quantized models can lead to perturbation mismatch or diminishment issues, resulting in suboptimal performance. In this paper, we propose a novel method, dubbed Sharpness-Aware Quantization (SAQ), to explore the effect of SAM in model compression, particularly quantization for the first time. Specifically, we first provide a unified view of quantization and SAM by treating them as introducing quantization noises and adversarial perturbations to the model weights, respectively. According to whether the noise and perturbation ter
    
[^179]: DeBERTaV3：使用梯度去耦合嵌入共享的ELECTRA风格预训练来改进DeBERTa模型

    DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing. (arXiv:2111.09543v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2111.09543](http://arxiv.org/abs/2111.09543)

    本论文介绍了一种新的预训练语言模型DeBERTaV3，使用更加样本有效的替换令牌检测（RTD）取代了掩码语言建模（MLM）并提出了一种新的梯度去耦合嵌入共享方法，避免了“拔河”动态，提高了预训练模型的训练效率和质量。在多个下游自然语言理解任务中，DeBERTaV3表现出优秀的性能。

    

    本文介绍了一种新的预训练语言模型DeBERTaV3，它通过将掩码语言建模（MLM）替换为更加样本有效的替换令牌检测（RTD）来改进原始的DeBERTa模型。我们的分析表明，ELECTRA中的香草嵌入共享会影响训练效率和模型性能，因为判别器和生成器的训练损失将令牌嵌入拉向不同的方向，会造成“拔河”动态。因此，我们提出了一种新的梯度去耦合嵌入共享方法，避免了“拔河”动态，提高了预训练模型的训练效率和质量。我们使用与DeBERTa相同的设置预训练了DeBERTaV3，以展示其在各种下游自然语言理解（NLU）任务中的优秀性能。以八项任务为例的GLUE基准测试中，DeBERTaV3 Large模型平均得分为91.37％，比D高1.37％。

    This paper presents a new pre-trained language model, DeBERTaV3, which improves the original DeBERTa model by replacing mask language modeling (MLM) with replaced token detection (RTD), a more sample-efficient pre-training task. Our analysis shows that vanilla embedding sharing in ELECTRA hurts training efficiency and model performance. This is because the training losses of the discriminator and the generator pull token embeddings in different directions, creating the "tug-of-war" dynamics. We thus propose a new gradient-disentangled embedding sharing method that avoids the tug-of-war dynamics, improving both training efficiency and the quality of the pre-trained model. We have pre-trained DeBERTaV3 using the same settings as DeBERTa to demonstrate its exceptional performance on a wide range of downstream natural language understanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an example, the DeBERTaV3 Large model achieves a 91.37% average score, which is 1.37% over D
    
[^180]: CoReS: 基于平稳性的兼容表示方法

    CoReS: Compatible Representations via Stationarity. (arXiv:2111.07632v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2111.07632](http://arxiv.org/abs/2111.07632)

    CoReS是一种基于平稳性的方法，可以学习内部特征表示模型，并使其与之前学习的模型“兼容”。这使得在逐步升级表示模型时，无需为所有之前看到的图像提取新的特征，从而大大降低了成本。

    

    本文提出了一种新方法来学习内部特征表示模型，使其与先前学习的模型“兼容”。兼容特征使旧和新的特征可以直接比较，允许它们在时间上互换使用，从而消除了在逐步升级表示模型时，在图库中提取所有先前看到的图像的新特征的需要，这通常在庞大的图库集和/或实时系统中非常昂贵或不可行（即人脸识别系统、社交网络、终身学习系统、机器人和监控系统）。我们的方法名为基于平稳性的兼容表示（CoReS），通过鼓励表示模型的平稳性来实现兼容性，而不依赖于先前学习的模型。平稳性使特征的统计特性在时间偏移下不发生变化。

    In this paper, we propose a novel method to learn internal feature representation models that are \textit{compatible} with previously learned ones. Compatible features enable for direct comparison of old and new learned features, allowing them to be used interchangeably over time. This eliminates the need for visual search systems to extract new features for all previously seen images in the gallery-set when sequentially upgrading the representation model. Extracting new features is typically quite expensive or infeasible in the case of very large gallery-sets and/or real time systems (i.e., face-recognition systems, social networks, life-long learning systems, robotics and surveillance systems). Our approach, called Compatible Representations via Stationarity (CoReS), achieves compatibility by encouraging stationarity to the learned representation model without relying on previously learned models. Stationarity allows features' statistical properties not to change under time shift so 
    
[^181]: 全面深度学习

    Holistic Deep Learning. (arXiv:2110.15829v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.15829](http://arxiv.org/abs/2110.15829)

    本文提出了一种全面深度学习框架，通过解决输入扰动的脆弱性、过度参数化和性能不稳定性等挑战，全面提高了准确性、鲁棒性、稀疏性和稳定性，适用于表格和图像数据集。提供了选择适当的训练损失函数的建议。

    This paper proposes a holistic deep learning framework that addresses the challenges of vulnerability to input perturbations, overparametrization, and performance instability from different train-validation splits. The proposed framework improves accuracy, robustness, sparsity, and stability over standard deep learning models, as demonstrated by extensive experiments on both tabular and image data sets. A prescriptive approach is provided to support practitioners in selecting an appropriate training loss function based on their specific objectives.

    本文提出了一种新颖的全面深度学习框架，同时解决了对输入扰动的脆弱性、过度参数化和来自不同训练验证拆分的性能不稳定性等挑战。所提出的框架在标准深度学习模型上全面提高了准确性、鲁棒性、稀疏性和稳定性，这在对表格和图像数据集进行广泛实验中得到了证明。结果进一步通过消融实验和SHAP值分析进行验证，揭示了不同评估指标之间的交互作用和权衡。为了支持实践者应用我们的框架，我们提供了一种指导性方法，根据他们的具体目标，提供选择适当的训练损失函数的建议。所有用于重现结果的代码都可以在https://github.com/kimvc7/HDL找到。

    This paper presents a novel holistic deep learning framework that simultaneously addresses the challenges of vulnerability to input perturbations, overparametrization, and performance instability from different train-validation splits. The proposed framework holistically improves accuracy, robustness, sparsity, and stability over standard deep learning models, as demonstrated by extensive experiments on both tabular and image data sets. The results are further validated by ablation experiments and SHAP value analysis, which reveal the interactions and trade-offs between the different evaluation metrics. To support practitioners applying our framework, we provide a prescriptive approach that offers recommendations for selecting an appropriate training loss function based on their specific objectives. All the code to reproduce the results can be found at https://github.com/kimvc7/HDL.
    
[^182]: 机器学习在网络安全中的对抗攻击威胁--调查

    The Threat of Adversarial Attacks on Machine Learning in Network Security -- A Survey. (arXiv:1911.02621v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/1911.02621](http://arxiv.org/abs/1911.02621)

    机器学习在网络安全领域应用面临对抗攻击威胁，攻击者通过专门设计的输入来绕过系统引导错误预测，文章提供机器学习和网络安全应用的分类法，并介绍了各种对抗攻击和防御方法。

    

    机器学习模型使许多决策支持系统更快、更准确、更高效。然而，在网络安全中应用机器学习面临比其他领域更不成比例的对抗攻击威胁。这是因为网络安全中的机器学习应用，如恶意软件检测、入侵检测和垃圾邮件过滤本身就是对抗性的。在攻击者和防御者之间可能可以被视为一场军备竞赛的过程中，攻击者不断地用专门设计来绕过系统的输入来探测机器学习系统并诱导错误的预测。在这项调查中，我们首先提供了机器学习技术，任务和深度的分类法。然后，我们介绍了网络安全应用中机器学习的分类方法。接下来，我们研究了针对机器学习在网络安全中的各种对抗攻击，并介绍了两种对抗攻击的分类方法。最后，我们讨论了当前对对抗攻击的防御，并强调了这个领域中的开放问题。

    Machine learning models have made many decision support systems to be faster, more accurate, and more efficient. However, applications of machine learning in network security face a more disproportionate threat of active adversarial attacks compared to other domains. This is because machine learning applications in network security such as malware detection, intrusion detection, and spam filtering are by themselves adversarial in nature. In what could be considered an arm's race between attackers and defenders, adversaries constantly probe machine learning systems with inputs that are explicitly designed to bypass the system and induce a wrong prediction. In this survey, we first provide a taxonomy of machine learning techniques, tasks, and depth. We then introduce a classification of machine learning in network security applications. Next, we examine various adversarial attacks against machine learning in network security and introduce two classification approaches for adversarial att
    
[^183]: 带噪声特征的上下文线性Bandit：朝向贝叶斯神谕前进

    Contextual Linear Bandits under Noisy Features: Towards Bayesian Oracles. (arXiv:1703.01347v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/1703.01347](http://arxiv.org/abs/1703.01347)

    本论文研究了具有噪声特征的上下文线性Bandit问题。我们提出了一个算法，通过观察信息，实现了贝叶斯神谕并得到了$\tilde{O}(d\sqrt{T})$的遗憾界。

    

    我们研究了带有噪声和缺失项的上下文线性Bandit问题。为了解决噪声的挑战，我们分析了在观测噪声特征的情况下给出的贝叶斯神谕。我们的贝叶斯分析发现，最优假设可能会远离潜在的可实现函数，这取决于噪声特征，这是高度非直观的，并且在经典的无噪声设置下不会发生。这意味着经典方法不能保证非平凡的遗憾界（regret bound）。因此，我们提出了一个算法，旨在从这个模型下的观察信息中实现贝叶斯神谕，当有大量手臂时，可以实现$\tilde{O}(d\sqrt{T})$遗憾界。我们使用合成和实际数据集演示了所提出的算法。

    We study contextual linear bandit problems under feature uncertainty; they are noisy with missing entries. To address the challenges of the noise, we analyze Bayesian oracles given observed noisy features. Our Bayesian analysis finds that the optimal hypothesis can be far from the underlying realizability function, depending on the noise characteristics, which are highly non-intuitive and do not occur for classical noiseless setups. This implies that classical approaches cannot guarantee a non-trivial regret bound. Therefore, we propose an algorithm that aims at the Bayesian oracle from observed information under this model, achieving $\tilde{O}(d\sqrt{T})$ regret bound when there is a large number of arms. We demonstrate the proposed algorithm using synthetic and real-world datasets.
    

