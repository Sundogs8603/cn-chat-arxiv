# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Contextual Biasing of Named-Entities with Large Language Models.](http://arxiv.org/abs/2309.00723) | 本文研究了使用大型语言模型进行上下文偏倚的方法，通过在第二次打分时提供额外的上下文信息，以提高自动语音识别性能。我们利用提示信息对大型语言模型进行boosting，并采用多任务训练以预测实体类别和下一个标记。此外，我们提出了动态提示方法来提高效率。 |
| [^2] | [Learning Shared Safety Constraints from Multi-task Demonstrations.](http://arxiv.org/abs/2309.00711) | 本论文介绍了如何从安全任务完成的专家示范中学习共享的安全约束，通过将逆强化学习技术扩展到约束空间，学习约束以禁止专家可能采取但选择不采取的高收益行为，并利用多任务设置中多样化示范学习一组更紧密的约束。 |
| [^3] | [Reinforcement Learning with Human Feedback for Realistic Traffic Simulation.](http://arxiv.org/abs/2309.00709) | 本研究利用强化学习与人类偏好相结合的框架来增强现有交通模型的真实性，并通过引入第一个用于交通建模中真实性对齐的数据集来支持此研究。 |
| [^4] | [Geometric Deep Learning: a Temperature Based Analysis of Graph Neural Networks.](http://arxiv.org/abs/2309.00699) | 本研究使用温度作为一个非量子和非相对论粒子的几何深度学习模型的分析方法，研究了GCN和GAT模型的各个层次中的温度，并讨论了可能的未来应用。 |
| [^5] | [Jointly Exploring Client Drift and Catastrophic Forgetting in Dynamic Learning.](http://arxiv.org/abs/2309.00688) | 本论文提出了一个统一的分析框架，用于在动态学习中共同探索客户漂移和灾难性遗忘的问题。通过构建一个受控的测试环境，我们证明了客户漂移和灾难性遗忘背后的根本原因是相关的，并且通过生成一个3D地形图进一步验证了这种相关性。 |
| [^6] | [Randomized Polar Codes for Anytime Distributed Machine Learning.](http://arxiv.org/abs/2309.00682) | 本论文提出了一个用于任意分布式机器学习的随机极化码框架，通过结合随机摘要和极化码的概念，实现了对慢计算节点的鲁棒性，并提供了适用于实值数据的低计算复杂度的串行译码算法和即时估计器，该框架在大规模矩阵乘法和黑盒优化等多个应用场景中具有潜在的应用价值。 |
| [^7] | [Taken out of context: On measuring situational awareness in LLMs.](http://arxiv.org/abs/2309.00667) | 这个论文目的在于研究大型语言模型中的情境意识的产生，提出了一种衡量模型情境意识的能力，并通过实验证明了其有效性。 |
| [^8] | [ICDARTS: Improving the Stability and Performance of Cyclic DARTS.](http://arxiv.org/abs/2309.00664) | ICDARTS改进了循环DARTS的稳定性和性能，通过消除评估网络权重对搜索网络权重的依赖，并引入修改后的搜索离散化过程。 |
| [^9] | [Polynomial-Model-Based Optimization for Blackbox Objectives.](http://arxiv.org/abs/2309.00663) | 基于多项式模型的黑盒优化是一种新颖的优化方法，通过拟合多项式代理寻找未知系统的最佳参数，成功竞争并超越其他算法，在黑盒优化问题的解决中具有关键意义。 |
| [^10] | [Local and adaptive mirror descents in extensive-form games.](http://arxiv.org/abs/2309.00656) | 本文研究了在零和不完全信息博弈中学习ε-最优策略的问题。通过提出一种固定采样方法，并使用自适应的在线镜像下降算法进行局部更新，我们取得了收敛速度为$\tilde{\mathcal{O}}(T^{-1/2})$的结果，并在最佳策略下对游戏参数具有近乎最优的依赖关系。 |
| [^11] | [The Use of Synthetic Data to Train AI Models: Opportunities and Risks for Sustainable Development.](http://arxiv.org/abs/2309.00652) | 合成数据是一种用于训练AI模型的人工生成数据，在保护隐私的同时提供了研究数据的可用性与减少机器学习模型偏见的机会，但同时也需要制定合适的政策来确保数据质量、真实性以及平衡隐私与数据效用之间的关系。 |
| [^12] | [Improving Small Footprint Few-shot Keyword Spotting with Supervision on Auxiliary Data.](http://arxiv.org/abs/2309.00647) | 本研究提出了一种改进小型关键词检测模型的方法，通过对辅助数据进行监督，利用自动标注和筛选数据构建了一个类似关键词的数据集，并采用多任务学习提升模型的表示能力，取得了在少样本关键词检测基准测试中优于竞争方法的性能表现。 |
| [^13] | [Minimal Assumptions for Optimal Serology Classification: Theory and Implications for Multidimensional Settings and Impure Training Data.](http://arxiv.org/abs/2309.00645) | 本研究提出了一种血清分类的技术，可以在多维和有杂质的训练数据情况下，通过对样本的分类和估计患病率来减少误差。该方法不需要直接访问条件概率密度函数，而是将数据嵌入参数化的曲线空间，并通过最小化经验误差来优化空间。 |
| [^14] | [Modified Lagrangian Formulation of Gear Tooth Crack Analysis using Combined Approach of Variable Mode Decomposition (VMD) and Time Synchronous Averaging (TSA).](http://arxiv.org/abs/2309.00641) | 本论文介绍了一个整合了变模分解（VMD）和时同步平均（TSA）的方法，用于分析耦合电机-机械变速器（CEMG）系统中的齿轮齿裂。通过改进的拉格朗日公式和时变啮合刚度模型，研究了裂纹对系统动态行为的影响。 |
| [^15] | [Generative AI for End-to-End Limit Order Book Modelling: A Token-Level Autoregressive Generative Model of Message Flow Using a Deep State Space Network.](http://arxiv.org/abs/2309.00638) | 本论文提出了一种使用深度状态空间网络的令牌级自回归生成模型，可以生成逼真的限价订单簿消息，并且在逼近数据分布和中间价格回报方面表现出有前景的性能。 |
| [^16] | [Finite Element Analysis and Machine Learning Guided Design of Carbon Fiber Organosheet-based Battery Enclosures for Crashworthiness.](http://arxiv.org/abs/2309.00637) | 本论文通过使用有限元分析和机器学习，实现了基于碳纤维器片的电池容器设计与碰撞性能的优化。模拟和预测结果表明，这种设计方法可以有效提高电池容器的碰撞强度和安全性。 |
| [^17] | [Commodities Trading through Deep Policy Gradient Methods.](http://arxiv.org/abs/2309.00630) | 本文研究了深度强化学习在算法性大宗商品交易中的有效性，通过引入新颖的时间离散化方案和两种策略梯度算法，DRL模型将夏普比率提高了83%。 |
| [^18] | [An Ensemble Method of Deep Reinforcement Learning for Automated Cryptocurrency Trading.](http://arxiv.org/abs/2309.00626) | 本研究提出了一种基于深度强化学习的集成方法，用于改进日内加密货币投资组合交易的交易策略的泛化性能。通过采用模型选择方法和新颖的混合分布策略，我们展示了该方法在不断变化的市场条件下的鲁棒性，并相比于基准模型显著提高了样本外表现。 |
| [^19] | [Short-Term Stock Price Forecasting using exogenous variables and Machine Learning Algorithms.](http://arxiv.org/abs/2309.00618) | 本研究通过比较四种机器学习模型的准确性，发现在短期预测纽约证券交易所中三个知名股票的股价时，XGBoost模型表现出最高准确性。进一步优化参数或引入更多外生变量可能改进预测结果。 |
| [^20] | [Baseline Defenses for Adversarial Attacks Against Aligned Language Models.](http://arxiv.org/abs/2309.00614) | 这篇论文研究了对齐语言模型面临的对抗攻击问题，通过评估基线防御策略的效果，探讨了各种策略的可行性和有效性，并对鲁棒性和性能进行了讨论。 |
| [^21] | [A Locality-based Neural Solver for Optical Motion Capture.](http://arxiv.org/abs/2309.00428) | 该论文提出了一种基于局部性的神经求解器，用于清理和求解光学运动捕捉数据。论文通过构建异构图神经网络和使用特定的图卷积操作，提取标记和关节的局部特征，并转化为更准确的动作。通过研究标记的动作与其相邻标记的相关性，论文能够高效地填补缺失的标记，并通过分析加速度轮廓识别跟踪错误引起的异常标记。此外，论文还提出了基于表示学习和数据增强的训练机制，以进一步提高模型的准确性。 |
| [^22] | [Where Did the Gap Go? Reassessing the Long-Range Graph Benchmark.](http://arxiv.org/abs/2309.00367) | 本文对长程图表基准（LRGB）进行了重新评估，通过严格的实证分析发现，先前的报告性能差距被高估了，而经过基本超参数优化后，差距完全消失。此外，我们还讨论了特征归一化的缺失和链接预测度量的虚假实现对LRGB的影响。 |
| [^23] | [When Measures are Unreliable: Imperceptible Adversarial Perturbations toward Top-$k$ Multi-Label Learning.](http://arxiv.org/abs/2309.00007) | 本文提出了一种面向Top-k多标签学习的不可察觉的对抗性扰动方法，既能够欺骗人眼的视觉感知，又能够避开度量监测。 |
| [^24] | [StratMed: Relevance Stratification for Low-resource Medication Recommendation.](http://arxiv.org/abs/2308.16781) | StratMed是一种面向低资源药物推荐的模型，通过相关性分层机制来解决医疗数据长尾分布不平衡的问题，平衡了药物组合的安全性和准确性。 |
| [^25] | [Everyone Can Attack: Repurpose Lossy Compression as a Natural Backdoor Attack.](http://arxiv.org/abs/2308.16684) | 本文发现了一种更严重的后门攻击威胁，即任何人都可以利用易获取的有损压缩算法进行自然后门攻击，无需设计特定触发器或进行繁琐调试。 |
| [^26] | [Towards Long-Tailed Recognition for Graph Classification via Collaborative Experts.](http://arxiv.org/abs/2308.16609) | 本文提出了一种新颖的方法，通过合作专家实现了长尾图分类，解决了现有方法在处理图数据上的不足。 |
| [^27] | [Latent Painter.](http://arxiv.org/abs/2308.16490) | 这个论文介绍了一种名为潜在画家的技术，它利用潜在作为画布和扩散器的预测作为计划来生成绘画动画，同时还可以在不同的检查点集中转换图像。 |
| [^28] | [Test-Time Adaptation for Point Cloud Upsampling Using Meta-Learning.](http://arxiv.org/abs/2308.16484) | 本文提出了一种使用元学习进行测试时间适应的方法来增强点云上采样模型的普适性，解决了测试数据分布与训练数据不同导致性能下降的问题。 |
| [^29] | [Point-TTA: Test-Time Adaptation for Point Cloud Registration Using Multitask Meta-Auxiliary Learning.](http://arxiv.org/abs/2308.16481) | Point-TTA是一种通过多任务元辅助学习实现的点云配准测试时自适应框架，能够提高配准模型的泛化性能。 |
| [^30] | [BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge.](http://arxiv.org/abs/2308.16458) | BioCoder是一个用于评估预训练模型在生成生物信息学代码方面的基准，涵盖了函数代码生成中的包依赖关系、类声明和全局变量，并通过模糊测试框架进行评估。 |
| [^31] | [Listen to Minority: Encrypted Traffic Classification for Class Imbalance with Contrastive Pre-Training.](http://arxiv.org/abs/2308.16453) | 本文提出了一种基于对比预训练的不平衡类别加密流量分类框架PASS，通过重新采样训练数据集并进行对比性预训练，避免了标签偏见和流量均匀性等问题，为加密流量分类提供了稳健的特征表示。 |
| [^32] | [Balancing between the Local and Global Structures (LGS) in Graph Embedding.](http://arxiv.org/abs/2308.16403) | 本论文提出了一种在图嵌入中平衡局部和全局结构的方法，通过可调参数进行调节。我们的结果表明，该方法在合成和真实世界的数据集上与最先进的方法相竞争，并引入了一种新颖的质量指标，聚类距离保持，用于评估嵌入的质量。 |
| [^33] | [Deep Video Codec Control.](http://arxiv.org/abs/2308.16215) | 本文提出了第一个端到端可学习的深度视频编码控制方法，同时考虑了带宽限制和下游视觉性能，并在不破坏现有标准化的情况下实现了保护深度视觉模型的目标。 |
| [^34] | [MDTD: A Multi Domain Trojan Detector for Deep Neural Networks.](http://arxiv.org/abs/2308.15673) | MDTD是一种多领域木马检测器，用于在测试时检测深度神经网络中包含木马触发器的输入。MDTD不需要知道触发器嵌入策略，并且适用于不同类型的输入。它利用了输入样本与决策边界的距离来检测木马触发器。 |
| [^35] | [Measurement Tampering Detection Benchmark.](http://arxiv.org/abs/2308.15605) | 本文构建了四个文本数据集用于评估测量篡改检测技术，研究人工智能系统操纵测量结果以营造良好结果的问题。虽然展示了优于基准的技术，但还有很大的改进空间。 |
| [^36] | [OEBench: Investigating Open Environment Challenges in Real-World Relational Data Streams.](http://arxiv.org/abs/2308.15059) | OEBench是一个用于评估关系数据流中开放环境挑战的开放环境基准，研究发现这种挑战在真实世界数据集中普遍存在。 |
| [^37] | [Conflict-Aware Active Automata Learning.](http://arxiv.org/abs/2308.14781) | C3AL是一种冲突感知的主动有限状态机学习框架，能够处理观测数据中的冲突，通过将观测树作为学习过程的一等公民并最小化测试次数，具有很好的效果。 |
| [^38] | [RESTORE: Graph Embedding Assessment Through Reconstruction.](http://arxiv.org/abs/2308.14659) | RESTORE是一个用于通过图重建进行内在GEs评估的框架，可以揭示在给定向量形式中保留的信息量。 |
| [^39] | [Label Denoising through Cross-Model Agreement.](http://arxiv.org/abs/2308.13976) | 本文提出了一种通过跨模型一致性进行标签去噪的方法。通过观察发现，不同模型在干净示例上的预测相对相似，而在有噪声示例上的预测在不同模型之间变化更大。在这种观察的启发下，我们提出了使用跨模型一致性进行去噪的方法（DeCA），旨在最小化两个机器学习模型参数化的真实标签分布之间的KL散度，同时最大化数据观测的似然。 |
| [^40] | [Stochastic Configuration Machines for Industrial Artificial Intelligence.](http://arxiv.org/abs/2308.13570) | 本文提出了一种新颖的随机学习器模型，称为随机配置机（SCMs），其基于随机配置网络（SCNs），旨在强调工业人工智能中的有效建模和节约数据大小。SCMs通过压缩模型存储，并保持有利的预测性能，具有在工业应用中很大的潜力。 |
| [^41] | [Deep Reinforcement Learning-driven Cross-Community Energy Interaction Optimal Scheduling.](http://arxiv.org/abs/2308.12554) | 本文提出了一种基于深度强化学习的跨社区能量交互优化调度模型，通过学习不同社区的负载特性，并基于该知识做出决策，实现综合能量系统的整体优化和调度。 |
| [^42] | [MKL-$L_{0/1}$-SVM.](http://arxiv.org/abs/2308.12016) | 本文提出了一种多核学习的支持向量机框架(MKL-$L_{0/1}$-SVM)，通过开发快速的ADMM求解器处理非凸非光滑的优化问题，并在实验中展示了与领先方法相当的性能。 |
| [^43] | [On Uniformly Optimal Algorithms for Best Arm Identification in Two-Armed Bandits with Fixed Budget.](http://arxiv.org/abs/2308.12000) | 本文研究了在有限预算的随机二臂赌博机中进行最佳臂选择的问题，并证明不存在比等概率采样算法更好的算法。我们引入了一致稳定算法的概念，并证明任何在所有情况下与等概率采样算法表现一样好的算法必须属于这个类别。这一结果解决了之前的两个未解之谜。 |
| [^44] | [A Study on the Impact of Non-confounding Covariates on the Inferential Performance of Methods based on the Potential Outcome Framework.](http://arxiv.org/abs/2308.11676) | "本文研究了非混淆协变量对基于潜在结果框架的方法推断性能的影响，通过提供统一的图形框架来增强对这些模型基本原理的理解，为实际场景中应用这些模型带来了潜在价值。" |
| [^45] | [LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning (Practical Experience Report).](http://arxiv.org/abs/2308.11148) | 本文提出了LLaMA-Reviewer框架，通过参数高效微调方法，利用流行的大型语言模型LLaMA在代码审查领域能力，实现对代码审查任务的自动化。研究表明，即使仅使用不到1%的可训练参数，该框架仍能取得显著的成果。 |
| [^46] | [How Expressive are Graph Neural Networks in Recommendation?.](http://arxiv.org/abs/2308.11127) | 本文对图神经网络在推荐中的表达能力进行了理论分析，发现现有的表达能力度量标准可能无法有效评估模型在推荐中的能力，提出了一个全面的理论分析方法。 |
| [^47] | [Approximately Equivariant Graph Networks.](http://arxiv.org/abs/2308.10436) | 本文关注于图神经网络（GNNs）的主动对称性，通过考虑信号在固定图上的学习设置，提出了一种近似的对称性概念，通过图粗化实现。这篇工作提出了一个偏差-方差公式来衡量近似对称性... |
| [^48] | [Resource-Adaptive Newton's Method for Distributed Learning.](http://arxiv.org/abs/2308.10154) | 本文介绍了一种名为RANL的新颖和高效的算法，通过使用简单的Hessian初始化和自适应的训练区域分配，克服了牛顿法在大规模和异构学习环境中的限制，并实现了线性收敛率。 |
| [^49] | [Event-based Dynamic Graph Representation Learning for Patent Application Trend Prediction.](http://arxiv.org/abs/2308.09780) | 本研究提出了一种基于事件的动态图学习框架，用于准确预测专利申请趋势。该方法利用公司和分类代码的可记忆表示，通过历史记忆和当前信息更新来捕捉语义接近性。 |
| [^50] | [Baird Counterexample Is Solved: with an example of How to Debug a Two-time-scale Algorithm.](http://arxiv.org/abs/2308.09732) | 这篇论文解决了Baird反例上的收敛问题，通过一种具有收敛保证的算法，实现了线性收敛速度。 |
| [^51] | [Over-the-Air Computation Aided Federated Learning with the Aggregation of Normalized Gradient.](http://arxiv.org/abs/2308.09082) | 通过归一化梯度，我们在无线计算的辅助下改进了联邦学习，提高了收敛性能。 |
| [^52] | [DealMVC: Dual Contrastive Calibration for Multi-view Clustering.](http://arxiv.org/abs/2308.09000) | 提出了一种名为DealMVC的双向对比校准网络，用于解决多视图聚类中的一致性问题，并利用全局和局部对比校准损失来提高聚类性能。 |
| [^53] | [CONVERT:Contrastive Graph Clustering with Reliable Augmentation.](http://arxiv.org/abs/2308.08963) | 本论文提出了一种名为COVERT的对比图聚类网络，通过可逆扰动恢复网络处理数据增强，提炼可靠的语义信息，进一步保证对比学习中语义的可靠性。 |
| [^54] | [Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World.](http://arxiv.org/abs/2308.07741) | Real Robot Challenge 2022为RL和机器人学界之间的桥梁，允许参与者在真实机器人上从离线数据中学习灵巧操作任务，解决了在模拟中得到的见解不能转化到真实机器人的问题。 |
| [^55] | [CausalLM is not optimal for in-context learning.](http://arxiv.org/abs/2308.06912) | 最近的研究显示，上下文学习中使用前缀语言模型（PrefixLM）比因果语言模型（CausalLM）效果更好。本文通过理论分析证明，虽然两种语言模型都以线性速率收敛到稳定点，但前缀语言模型收敛到线性回归的最优解，因果语言模型的收敛动态遵循在线梯度下降算法，不保证收敛到最优解。 |
| [^56] | [Generalizing Topological Graph Neural Networks with Paths.](http://arxiv.org/abs/2308.06838) | 本论文提出了一种通用的拓扑图神经网络（GNNs）方法，通过强调图中的路径，突破了1-Weisfeiler-Lehmann测试的理论限制，并在无需对图的子结构进行假设的情况下，在多个基准测试中取得了最先进的性能。 |
| [^57] | [Discovering the Symptom Patterns of COVID-19 from Recovered and Deceased Patients Using Apriori Association Rule Mining.](http://arxiv.org/abs/2308.06763) | 本文使用Apriori算法基于关联规则挖掘的方法，从COVID-19患者中发现了一些最常见的症状模式，如呼吸暂停、咳嗽、发热等，为临床医生提供了宝贵的疾病见解。 |
| [^58] | [Precipitation nowcasting with generative diffusion models.](http://arxiv.org/abs/2308.06733) | 这篇论文研究了使用生成性扩散模型进行降水即时预测的问题，生成模型在天气预报中的应用具有潜力，并能够更好地建模概率分布。 |
| [^59] | [Provably Efficient Learning in Partially Observable Contextual Bandit.](http://arxiv.org/abs/2308.03572) | 本文研究了在部分可观察情境轮盘赌中的转移学习问题，提出了一种通过优化问题识别行为和奖励因果效应的方法，并利用因果约束来改进轮盘赌算法。 |
| [^60] | [Edge of stability echo state networks.](http://arxiv.org/abs/2308.02902) | 本文介绍了一种新的边缘稳定回声状态网络（ES$^2$N）架构，结合了渐进记忆属性和尽可能保留更多记忆的能力，通过将储备层定义为非线性和线性的凸组合，提供了网络稳定性和性能的增强。 |
| [^61] | [Memory capacity of two layer neural networks with smooth activations.](http://arxiv.org/abs/2308.02001) | 研究发现，具有平滑激活函数的两层神经网络的存储容量下界为md/2，并且准确性大致为2倍。分析存储容量的方法包括计算网络雅可比矩阵的秩，并扩展了有关Hadamard幂秩的经典线性代数事实。 |
| [^62] | [Enhancing Representation Learning for Periodic Time Series with Floss: A Frequency Domain Regularization Approach.](http://arxiv.org/abs/2308.01011) | 本文提出了一种叫做Floss的无监督方法，通过在频域上对学到的表示进行正则化来增强周期性时间序列的表示学习。Floss方法可以自动检测时间序列中的周期性并学习具有周期一致性的有意义的表示。 |
| [^63] | [Benchmarking Jetson Edge Devices with an End-to-end Video-based Anomaly Detection System.](http://arxiv.org/abs/2307.16834) | 本论文实现了一个端到端的视频异常检测系统，通过从监控视频输入进行犯罪现场异常检测，并在多个Jetson边缘设备上部署和运行。这是对Jetson平台在深度学习算法执行方面性能的基准测试分析的创新。 |
| [^64] | [An Efficient Approach to Mitigate Numerical Instability in Backpropagation for 16-bit Neural Network Training.](http://arxiv.org/abs/2307.16189) | 这项研究探讨了16位计算中机器学习模型的数值不稳定性问题，并提出了一种基于Adam优化器的新方法来提高16位神经网络的学习过程的鲁棒性。 |
| [^65] | [A new Gradient TD Algorithm with only One Step-size: Convergence Rate Analysis using $L$-$\lambda$ Smoothness.](http://arxiv.org/abs/2307.15892) | 本论文提出了一种新的梯度时序差分算法，只使用一个步长参数，并证明收敛速度至少为$O(1/t)$。 |
| [^66] | [eXplainable Artificial Intelligence (XAI) in age prediction: A systematic review.](http://arxiv.org/abs/2307.13704) | 本综述探讨了可解释人工智能（XAI）在年龄预测任务中的应用。通过系统性综述，我们讨论了XAI方法在医疗应用和年龄预测领域的益处。 |
| [^67] | [On the learning Dynamics of Attention Networks.](http://arxiv.org/abs/2307.13421) | 本研究分析了软注意力、硬注意力和潜变量边际似然（LVML）注意力三种注意力模型的学习动态，发现了它们在所选择的片段聚合方式上的显著差异，并解释了分类模型在梯度下降下的演化对最终结果的影响。 |
| [^68] | [AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment enabled by Large Language Models.](http://arxiv.org/abs/2307.11772) | AutoAlign是一种全自动的知识图谱对齐方法，不需要手工制作的种子对齐。它利用大型语言模型自动捕捉谓词相似性，并使用TransE计算实体嵌入来实现实体对齐。 |
| [^69] | [Convergence Guarantees for Stochastic Subgradient Methods in Nonsmooth Nonconvex Optimization.](http://arxiv.org/abs/2307.10053) | 本文研究了非平滑非凸优化中随机次梯度方法的收敛性质，并提出了一种新的框架，证明了其在单时间尺度和双时间尺度情况下的全局收敛性，包括了多种已知的SGD类型方法。对于有限和形式的目标函数，证明了这些方法能够在随机选择的步长和初始点上找到Clarke稳定点。 |
| [^70] | [Learning to Sample Tasks for Meta Learning.](http://arxiv.org/abs/2307.08924) | 通过实验得出了三个结论：没有通用的任务采样策略能保证元学习模型的性能；任务的多样性会导致模型在训练过程中出现欠拟合或过拟合的问题；模型的泛化性能受到任务的差异、任务熵和任务难度的影响。针对这些发现，提出了一种新颖的任务采样器ASr，它利用任务的差异、任务熵和任务难度来采样任务，并通过重新思考和提出一个简单而通用的元学习算法来优化ASr。大量实证实验表明了ASr的有效性。 |
| [^71] | [Meta-Value Learning: a General Framework for Learning with Learning Awareness.](http://arxiv.org/abs/2307.08863) | 元价值学习是一种带有学习意识的学习通用框架，通过分析智能体学习过程的相互作用，使用元价值函数来指导优化，并通过训练神经网络进行逼近，从而提供更可靠的改进方向。 |
| [^72] | [Towards Understanding Adversarial Transferability From Surrogate Training.](http://arxiv.org/abs/2307.07873) | 本论文探索了对抗性可转移性的理解，特别关注替代训练。通过研究模型的平滑性和梯度相似性之间的权衡，发现对抗训练可以提高模型的替代能力。研究结果对数据分布的转变提出了新的推测。 |
| [^73] | [Deep Network Approximation: Beyond ReLU to Diverse Activation Functions.](http://arxiv.org/abs/2307.06555) | 本文研究了深度神经网络在多种激活函数下的表达能力，证明了可以通过在有界集合上构建一个宽度为6N、深度为2L的varrho激活网络来逼近一个宽度为N、深度为L的ReLU网络，从而将对ReLU网络的逼近结果推广到其他激活函数。 |
| [^74] | [Diffusion Policies for Out-of-Distribution Generalization in Offline Reinforcement Learning.](http://arxiv.org/abs/2307.04726) | 该论文介绍了一种名为状态重构扩散策略 (SRDP) 的新方法，该方法在最新的扩散策略类中引入了状态重构特征学习，以解决脱机强化学习中的分布偏移和有效表示策略的问题。 |
| [^75] | [Group-based Robustness: A General Framework for Customized Robustness in the Real World.](http://arxiv.org/abs/2306.16614) | 本研究提出了一种基于群体的鲁棒性指标，可以更好地评估机器学习模型在现实世界中抵抗攻击的能力，弥补了传统指标的不足。实验证明，该指标能够区分模型对特定威胁的脆弱性。 |
| [^76] | [Deep Learning Models for Water Stage Predictions in South Florida.](http://arxiv.org/abs/2306.15907) | 本论文利用深度学习模型训练代理模型，快速预测南佛罗里达州迈阿密河下游的水位，并与基于物理的模型进行比较。 |
| [^77] | [MeciFace: Mechanomyography and Inertial Fusion based Glasses for Edge Real-Time Recognition of Facial and Eating Activities.](http://arxiv.org/abs/2306.13674) | MeciFace是一款注重隐私且低功耗的可穿戴设备，它采用轻量级卷积神经网络来监测面部表情和进食活动，面部表情案例的F1分数达到了86％，饮食监测则达到了90％的F1分数。 |
| [^78] | [Toward Leveraging Pre-Trained Self-Supervised Frontends for Automatic Singing Voice Understanding Tasks: Three Case Studies.](http://arxiv.org/abs/2306.12714) | 本文研究了利用自监督学习模型（SSL模型）进行歌唱声音理解任务的有效性，并展示了将自监督前端转移到目标任务可以取得更好性能的潜力。此外，SSL模型在所有任务中均优于常规监督学习模型。 |
| [^79] | [DynaQuant: Compressing Deep Learning Training Checkpoints via Dynamic Quantization.](http://arxiv.org/abs/2306.11800) | DynaQuant通过动态量化实现对各种最先进模型的显着压缩，几乎不影响模型准确性。 |
| [^80] | [The RL Perceptron: Generalisation Dynamics of Policy Learning in High Dimensions.](http://arxiv.org/abs/2306.10404) | 本文提出了一个高维RL模型，推导出该模型的典型动力学为一组闭式ODE方程组，并通过实验与神经RL代理进行了比较，结果表明该模型捕捉了现实世界RL的关键特征。 |
| [^81] | [Unsupervised Learning of Style-Aware Facial Animation from Real Acting Performances.](http://arxiv.org/abs/2306.10006) | 本文提出了一种非监督学习的动画方法，可以通过文本或语音输入，实现基于真实动作表演的面部动画，并且可以不同程度地学习并合成不同的表演风格。 |
| [^82] | [Multi-View Class Incremental Learning.](http://arxiv.org/abs/2306.09675) | 本文提出了一种名为多视角分类增量学习（MVCIL）的新模型，该模型使用随机化的表示学习技术进行特征提取，并提出正交融合子空间和选择性权重合并来解决增量学习中遗忘旧信息和学习新概念的挑战。实验结果表明该方法相比最新方法有效性更高。 |
| [^83] | [SCALE: Scaling up the Complexity for Advanced Language Model Evaluation.](http://arxiv.org/abs/2306.09237) | 该论文提出了一个新颖的自然语言处理基准测试，挑战当前大型语言模型在处理长文档、利用领域专业知识、多语言理解和多任务处理方面的能力。基准测试包含瑞士法律系统的多样化法律NLP数据集，允许进行对底层非英语、固有多语言的法律系统进行全面研究。 |
| [^84] | [Kernel Random Projection Depth for Outlier Detection.](http://arxiv.org/abs/2306.07056) | 本文提出了一种内核随机投影深度方法，用于处理数据云中的多模式和非凸性，实验结果表明在基准数据集上表现优异。 |
| [^85] | [Improving the Validity of Decision Trees as Explanations.](http://arxiv.org/abs/2306.06777) | 该论文介绍了一个新的决策树模型，利用挂起的树的方式提高了其解释性和统计性能，达到了无限深度决策树的水平，并可与XGBoost等最先进的方法相媲美。 |
| [^86] | [Multi-level Multiple Instance Learning with Transformer for Whole Slide Image Classification.](http://arxiv.org/abs/2306.05029) | 本文提出了一种基于Transformer的多级多示例学习（MMIL-Transformer）方法，该方法能够有效地处理涉及大量实例的MIL任务。 |
| [^87] | [AnoOnly: Semi-Supervised Anomaly Detection without Loss on Normal Data.](http://arxiv.org/abs/2305.18798) | AnoOnly是一个新的半监督异常检测框架，通过引入一种对正常数据的弱监督形式来解决同质数据对异常的影响，以实现平衡的监督。该框架在各种模型和数据集上表现出了显著的性能提升，达到了新的最佳性能。 |
| [^88] | [Provably Fast Finite Particle Variants of SVGD via Virtual Particle Stochastic Approximation.](http://arxiv.org/abs/2305.17558) | 本论文提出了两种基于虚拟粒子随机逼近的可证速限制变种的SVGD算法，具有可证速的有限粒子收敛率。 |
| [^89] | [Improved Sales Forecasting using Trend and Seasonality Decomposition with LightGBM.](http://arxiv.org/abs/2305.17201) | 本文提出了一种根据趋势和季节性分量在时间序列上的独特影响指标进行时间序列分组的新方法，并采用 LightGBM 模型进行预测，在沃尔玛销售数据上实现了较高的预测精度。 |
| [^90] | [Optimized Custom Dataset for Efficient Detection of Underwater Trash.](http://arxiv.org/abs/2305.16460) | 本文提出了一种自定义数据集和有效检测方法，旨在通过增加垃圾实例的多样性，在深入水下环境中提高其检测精度。 |
| [^91] | [Improved Multi-Scale Grid Rendering of Point Clouds for Radar Object Detection Networks.](http://arxiv.org/abs/2305.15836) | 本文提出了一种新的体系结构，即多尺度 KPPillarsBEV，以缓解雷达目标检测中从点云数据转化为网格结构过程中的信息丢失问题，并提出了一种新的网格渲染方法 KPBEV。实验结果表明，该方法显著优于现有方法。 |
| [^92] | [On progressive sharpening, flat minima and generalisation.](http://arxiv.org/abs/2305.14683) | 本文提出了一种用损失黑塞矩阵和输入-输出雅克比矩阵联系起来的假设，量化了模型的输入-输出雅克比矩阵近似其在数据分布上的利普西茨范数的程度，并推导出了一个基于经验雅克比矩阵的新的泛化界，给出了关于进化磨锋和平坦极小的泛化性质的新解释。 |
| [^93] | [Model Debiasing via Gradient-based Explanation on Representation.](http://arxiv.org/abs/2305.12178) | 本文提出了一种新的公平性框架，通过梯度说明找到两个模型焦点进行去偏见处理，提高下游任务模型的预测性能。 |
| [^94] | [From Random Search to Bandit Learning in Metric Measure Spaces.](http://arxiv.org/abs/2305.11509) | 本文介绍了随机搜索及其性能，引入了“散射维度”的概念，描述了底层函数的状态，量化了随机搜索的性能，并证明了在无噪声和有界噪声情况下的输出分别以一定概率收敛到最优值。 |
| [^95] | [The star-shaped space of solutions of the spherical negative perceptron.](http://arxiv.org/abs/2305.10623) | 本文针对球形负感知器模型的解空间展开研究，发现在过度参数化的区域内解决方案流形显示出简单的连通性质。存在一个大的测地凸成分，对各种优化动力学具有吸引力，其中又有一个与大多数其他解决方案测地连接的非典型鲁棒解决方案的子集，从而产生了星形的几何形状。 |
| [^96] | [Correlation visualization under missing values: a comparison between imputation and direct parameter estimation methods.](http://arxiv.org/abs/2305.06044) | 本文比较了不同的缺失数据处理方法对相关图的影响，建议使用直接参数估计法(DPER)来绘制相关图 |
| [^97] | [Two to Five Truths in Non-Negative Matrix Factorization.](http://arxiv.org/abs/2305.05389) | 本文提出了一种受规范化拉普拉斯图的启发的矩阵缩放方法，可以大大提高非负矩阵分解在文本主题模型中的质量。 |
| [^98] | [DELTA: Dynamic Embedding Learning with Truncated Conscious Attention for CTR Prediction.](http://arxiv.org/abs/2305.04891) | 该论文提出了一种名为DELTA的CTR模型，使用截断意识注意力进行动态嵌入学习，有效地解决了上下文中无效和冗余特征的问题。 |
| [^99] | [ClusterNet: A Perception-Based Clustering Model for Scattered Data.](http://arxiv.org/abs/2304.14185) | 这项工作介绍了ClusterNet，一种基于感知的分布式数据聚类模型，利用大规模数据集和基于点的深度学习模型，反映人类感知的聚类可分性。 |
| [^100] | [NPRL: Nightly Profile Representation Learning for Early Sepsis Onset Prediction in ICU Trauma Patients.](http://arxiv.org/abs/2304.12737) | 本文提出了一种基于夜间个人档案表示学习和深度学习框架的方法，可以提前预测创伤患者的脓毒症发作，这种方法优于现有的最先进方法。 |
| [^101] | [Matching-based Data Valuation for Generative Model.](http://arxiv.org/abs/2304.10701) | 本论文提出了基于匹配的生成模型数据估值方法，这是一个针对任何生成模型的模型无关方法，可以对数据实例进行估值，而无需重新训练模型，并在估值效果上表现出色。 |
| [^102] | [Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales.](http://arxiv.org/abs/2304.06875) | 提出了一种新的方法muP，可以提高超参数的缩放律的拟合精度，减少对大模型超参数的搜索，从而实现在大规模模型上进行损失预测。 |
| [^103] | [RAPID: Enabling Fast Online Policy Learning in Dynamic Public Cloud Environments.](http://arxiv.org/abs/2304.04797) | 提出了在动态公共云环境中实现快速在线策略学习的框架RAPID，通过领域知识启发的技术实现样本效率和偏差减少，学习并实时调整资源分配策略，能有效解决资源共享中的问题。 |
| [^104] | [Graphon Estimation in bipartite graphs with observable edge labels and unobservable node labels.](http://arxiv.org/abs/2304.03590) | 研究了标签可观测、节点不可观测下的二部图图估计问题，在分段常数和H\"older连续图谱的情况下找到了有限的样本风险界限。 |
| [^105] | [Nonlinear Independent Component Analysis for Principled Disentanglement in Unsupervised Deep Learning.](http://arxiv.org/abs/2303.16535) | 本文概括了无监督深度学习中基于独立成分分析方法的最新发展，特别是对于解决非线性情况下唯一性问题提出了可识别的扩展方法。 |
| [^106] | [Searching for long faint astronomical high energy transients: a data driven approach.](http://arxiv.org/abs/2303.15936) | HERMES Pathfinder是一个在轨探测系统，使用简单但创新的探测器监测高能宇宙瞬变现象，通过研究信号到达不同探测器的延迟时间获得精确的位置信息，本文介绍了一种使用神经网络评估航天高能探测器背景计数率的新框架。 |
| [^107] | [RGI : Regularized Graph Infomax for self-supervised learning on graphs.](http://arxiv.org/abs/2303.08644) | RGI是一个用于图上节点级自监督学习的简单而有效的框架，通过最大化节点级局部和全局视图之间的互信息来训练图神经网络编码器，并规范化了表示的协方差矩阵。 |
| [^108] | [Provably Convergent Plug-and-Play Quasi-Newton Methods.](http://arxiv.org/abs/2303.07271) | 本文提出了一种可证明收敛的PnP方法，使用拟牛顿步骤以加速收敛，相对于现有的PnP方法对去噪器或保真度函数施加了较轻的限制。 |
| [^109] | [The Descriptive Complexity of Graph Neural Networks.](http://arxiv.org/abs/2303.04613) | 研究分析了图神经网络（GNN）在布尔电路复杂性和描述性复杂性方面的能力，证明了多项式规模有界深度的GNN族族可以计算的图查询正是带计数和内置关系的一阶逻辑受保护的片断GFO+C所定义的，这将GNN放在电路复杂性类TC^0中。 |
| [^110] | [Performance is not enough: a story of the Rashomon's quartet.](http://arxiv.org/abs/2302.13356) | 本文介绍了Rashomon的四重奏，这是一个合成数据集，其中来自不同类别的四个模型具有几乎相同的预测性能，同时其可视化揭示了极其不同的方法来理解数据中的相关性结构。 |
| [^111] | [Leveraging Prior Knowledge in Reinforcement Learning via Double-Sided Bounds on the Value Function.](http://arxiv.org/abs/2302.09676) | 本研究利用先前的值函数近似解决方案，提出了一种使用任意近似值函数来推导感兴趣的最优值函数的双边界限的方法，并为连续状态和动作空间的误差分析提供了新的框架，通过在简单领域进行数值验证证实了这些结果。 |
| [^112] | [Breaking the Lower Bound with (Little) Structure: Acceleration in Non-Convex Stochastic Optimization with Heavy-Tailed Noise.](http://arxiv.org/abs/2302.06763) | 本论文在具有重尾噪声的非凸随机优化问题中，改进了Cutkosky和Mehta的算法，并提供了近乎最优的收敛保证，而无需对随机梯度的矩条件进行额外的假设。 |
| [^113] | [Numerical Methods For PDEs Over Manifolds Using Spectral Physics Informed Neural Networks.](http://arxiv.org/abs/2302.05322) | 本文提出了一种新方法，使用基于谱的物理信息神经网络求解流形上的偏微分方程，并在球面和环面上得到了成功应用。对比标准架构，本文的方法表现更好。 |
| [^114] | [V1T: large-scale mouse V1 response prediction using a Vision Transformer.](http://arxiv.org/abs/2302.03023) | V1T是一种基于Vision Transformer的新型架构，可以跨动物学习共享的视觉和行为表示，对自然视觉刺激下的视觉皮层神经响应进行预测，并在预测性能上优于之前基于卷积的模型超过12.7％。同时，通过Transformer学习的自我关注权重还能够展示与群体感受野的相关性。 |
| [^115] | [NeuRI: Diversifying DNN Generation via Inductive Rule Inference.](http://arxiv.org/abs/2302.02261) | NeuRI是一种全自动化生成由数百种操作符组成的有效且多样化的DL模型的方法。它通过归纳式程序合成推断操作符约束条件，并采用符号和具体操作的混合模型生成。 |
| [^116] | [A novel framework for medium-term wind power prediction based on temporal attention mechanisms.](http://arxiv.org/abs/2302.01222) | 本文提出了一种基于树状Parzen估计器（TPE）和分解算法的新框架（TPE-VMD-TFT），用于24小时和48小时之前的风电功率预测。在法国电力公司Engie的风能数据集上，所提出的方法表现良好。 |
| [^117] | [A Reinforcement Learning Framework for Dynamic Mediation Analysis.](http://arxiv.org/abs/2301.13348) | 这项研究提出了一个强化学习框架，首次评估了在无限时间范围内的动态中介效应，并开发了鲁棒和半参数有效的估计方法来推断这些因果效应。 |
| [^118] | [Doubly Optimal No-Regret Learning in Monotone Games.](http://arxiv.org/abs/2301.13120) | 这是首个在平滑单调博弈中实现双重最优无遗憾学习的算法，同时实现了在对抗环境下具有最优的遗憾和在多人平滑单调博弈中具有最优的最终迭代收敛速率到达纳什均衡。 |
| [^119] | [Backpropagation of Unrolled Solvers with Folded Optimization.](http://arxiv.org/abs/2301.12047) | 本文提出了一种通过解析优化的方法来解决反向传播中的精度和效率问题，并提供了生成高效可解析的反向传播优化模型的系统。此外，本文还提出了通过优化映射统一展开和解析微分的视角。 |
| [^120] | [Coincident Learning for Unsupervised Anomaly Detection.](http://arxiv.org/abs/2301.11368) | 本文提出了一种名为CoAD的新方法，通过巧合学习来进行无监督异常检测，在多模态任务中通过特征空间的巧合行为来识别异常。 |
| [^121] | [Improve Noise Tolerance of Robust Loss via Noise-Awareness.](http://arxiv.org/abs/2301.07306) | 通过噪声感知提高鲁棒损失的噪声容忍性，采用实例相关超参数集成鲁棒损失，提高了对噪声标签的处理能力。 |
| [^122] | [Graph Topology Learning Under Privacy Constraints.](http://arxiv.org/abs/2301.06662) | 在隐私约束下，我们提出了一个框架，联合学习为本地客户定制的个性化图以及共识图，以推断潜在图拓扑，同时在保护隐私的情况下处理分布式客户端的数据。 |
| [^123] | [Minimax Weight Learning for Absorbing MDPs.](http://arxiv.org/abs/2301.03183) | 本文研究了吸收MDPs中未折扣的离策略评估问题，提出了一种称为MWLA算法的方法来直接估计期望回报，并分析了该方法的误差界限和依赖关系。 |
| [^124] | [Faithful and Consistent Graph Neural Network Explanations with Rationale Alignment.](http://arxiv.org/abs/2301.02791) | 本文研究了图神经网络的预测，提出了解释不一致的问题，并从因果关系的角度对其进行了理论分析 |
| [^125] | [Backward Curriculum Reinforcement Learning.](http://arxiv.org/abs/2212.14214) | 这项工作提出了一种新颖的反向课程强化学习方法，通过使用回放轨迹而不是原始的前向轨迹来训练智能体。这种方法通过提供强有力的奖励信号实现了更高效的学习，而且只需要进行微小的算法改变。 |
| [^126] | [Dataset of Pathloss and ToA Radio Maps With Localization Application.](http://arxiv.org/abs/2212.11777) | 这个论文介绍了一个包含稠密城市环境中无线地图数据集的研究。这个数据集能够用于路径损耗预测和无线定位，通过在相同的城市地图上计算得到RSS和ToA地图，可以公平比较两种定位方法的效果。 |
| [^127] | [Rethinking Vision Transformers for MobileNet Size and Speed.](http://arxiv.org/abs/2212.08059) | 本研究重新思考了Vision Transformers在移动设备上的部署效率，并提出了一种新的超网络设计和搜索策略，以实现与MobileNet类似大小和速度的Transformer模型。 |
| [^128] | [Learning Combinatorial Structures via Markov Random Fields with Sampling through Lov\'asz Local Lemma.](http://arxiv.org/abs/2212.00296) | Nelson是一种基于神经网络和Lov\'asz Local Lemma的方法，使用约束的马尔可夫随机场模型生成满足组合约束条件的样本。 |
| [^129] | [Are you using test log-likelihood correctly?.](http://arxiv.org/abs/2212.00219) | 使用测试对数似然进行比较可能与其他指标相矛盾，并且高测试对数似然不意味着更准确的后验近似。 |
| [^130] | [PAC Verification of Statistical Algorithms.](http://arxiv.org/abs/2211.17096) | 本文介绍了PAC验证的概念，并在三个方面进行了进一步的研究：对于VC维度为$d$的假设类，PAC验证需要$\Omega\left(\sqrt{d}/\varepsilon^2\right)$个i.i.d.样本的下界；提出了一种用于验证实数区间的并集的协议，并与下界对$d$的依赖相匹配；将PAC验证的定义推广到对一般统计算法的验证。 |
| [^131] | [Leveraging per Image-Token Consistency for Vision-Language Pre-training.](http://arxiv.org/abs/2211.15398) | EPIC 提出了一种利用图像-标记一致性的视觉-语言预训练方法，通过基于显著性的掩码策略和不一致的标记生成过程来克服CMLM的限制，并增强了视觉-语言关联的学习能力。 |
| [^132] | [Dynamic Loss For Robust Learning.](http://arxiv.org/abs/2211.12506) | 本文提出了一种动态损失函数用于鲁棒学习，通过自动调整目标函数的方式从长尾噪声数据中学习分类器。动态损失包括标签修正器和边界生成器，能够纠正噪声标签并生成每个类别的分类边界，通过元学习来优化这两个组件，使分类器适应干净且平衡的测试数据。 |
| [^133] | [Machine learning for classifying and interpreting coherent X-ray speckle patterns.](http://arxiv.org/abs/2211.08194) | 通过使用模型系统和深度神经网络，本研究探索了相干X射线散斑图和样品结构之间的关系，并表明机器学习在确定散斑图中结构的能力上是准确的。 |
| [^134] | [MemoNet: Memorizing All Cross Features' Representations Efficiently via Multi-Hash Codebook Network for CTR Prediction.](http://arxiv.org/abs/2211.01334) | 本文提出了一种名为MemoNet的CTR模型，通过引入多哈希码本网络（HCNet）作为记忆机制，高效地学习和记忆交叉特征的表示。实验结果表明MemoNet在性能上优于最先进的方法，并且展现出NLP中的大型语言模型的扩展规律。 |
| [^135] | [Verifying And Interpreting Neural Networks using Finite Automata.](http://arxiv.org/abs/2211.01022) | 这项研究提出了一种使用有限自动机来验证和解释神经网络的方法。通过构建特殊的弱Büchi自动机，能够精确地捕捉神经网络的输入输出行为，并用于解决DNN的常见验证和解释任务，如对抗鲁棒性或最小充分原因。 |
| [^136] | [Video based Object 6D Pose Estimation using Transformers.](http://arxiv.org/abs/2210.13540) | 本论文介绍了一种基于Transformer的视频中物体6D姿态估计框架，利用先前的帧信息进行姿态估计，实现了高效而准确的姿态估计，能够处理长时间序列依赖关系，并且相对于CNN方法表现更好，具有33fps的处理速度，适用于实时物体姿态估计应用。 |
| [^137] | [Activity-aware Human Mobility Prediction with Hierarchical Graph Attention Recurrent Network.](http://arxiv.org/abs/2210.07765) | 这个论文提出了一种基于层次图注意循环网络的活动感知人类移动预测方法，通过构建一个层次图和使用层次图注意模块来捕捉时间-活动-位置之间的依赖关系，以建模用户的偏好。同时引入了一种模型无关的历史增强置信标签，用于聚焦于每个用户的个体级偏好。 |
| [^138] | [Taking a Respite from Representation Learning for Molecular Property Prediction.](http://arxiv.org/abs/2209.13492) | 本研究对一系列分子表征模型进行了系统评估，发现基于固定表征的模型在分子属性预测中具有一定优势，同时也揭示了活性断崖问题。 |
| [^139] | [Digital Audio Forensics: Blind Human Voice Mimicry Detection.](http://arxiv.org/abs/2209.12573) | 本文介绍了一种利用深度学习方法，通过盲目检测输入音频的真实性，可以有效应对音频欺诈问题的分类器。而这种分类器不需要任何参考，能够在没有真实来源的情况下检测出模仿音频。 |
| [^140] | [The alignment problem from a deep learning perspective.](http://arxiv.org/abs/2209.00626) | 人工通用智能（AGI）的出现可能会导致其追求与人类利益不对齐的目标，并采用欺骗性行为和权力追求策略。防止这种情况的发生是一个重要的研究方向。 |
| [^141] | [gSwin: Gated MLP Vision Model with Hierarchical Structure of Shifted Window.](http://arxiv.org/abs/2208.11718) | 提出了一种名为gSwin的视觉模型，结合了Swin Transformer和（多头）gMLP两个流派，能够同时兼顾参数效率、性能、局部性和层级，在图像分类、目标检测和语义分割等任务中取得更高的准确度，并且模型尺寸更小。 |
| [^142] | [Application of federated learning techniques for arrhythmia classification using 12-lead ECG signals.](http://arxiv.org/abs/2208.10993) | 本研究应用联合学习技术对12导联心电图信号进行心律失常分类，通过联合学习方法训练AI模型，实现对医疗数据的隐私保护，同时保持与集中学习方式相当的性能表现。 |
| [^143] | [MolGraph: a Python package for the implementation of molecular graphs and graph neural networks with TensorFlow and Keras.](http://arxiv.org/abs/2208.09944) | MolGraph是一个使用TensorFlow和Keras实现分子图和图神经网络的Python包，为分子机器学习问题提供了高度兼容和验证性能的解决方案。 |
| [^144] | [Learning Efficient Abstract Planning Models that Choose What to Predict.](http://arxiv.org/abs/2208.07737) | 该论文提出了一种学习抽象规划模型的方法，通过选择预测来提高机器人在长期任务中的决策效率。 |
| [^145] | [Locating disparities in machine learning.](http://arxiv.org/abs/2208.06680) | 这个论文提出了一个名为自动定位差异（ALD）的数据驱动框架，旨在解决机器学习中的差异问题。ALD适用于任意的机器学习分类器，可以处理不同的差异定义，并且可以处理分类和连续预测问题。 |
| [^146] | [Self-supervised Contrastive Representation Learning for Semi-supervised Time-Series Classification.](http://arxiv.org/abs/2208.06616) | 这项工作提出了一种新的自监督对比表示学习框架，通过对比学习从未标记的时间序列数据中提取有用的表示。同时，研究了时间序列数据增强选择，并将该框架扩展到半监督学习设置。 |
| [^147] | [Combing for Credentials: Active Pattern Extraction from Smart Reply.](http://arxiv.org/abs/2207.10802) | 该论文研究了在智能回复应用程序中潜在的信息泄漏漏洞，并且提出了一种在实际设置中限制查询类型的攻击方式。 |
| [^148] | [zPROBE: Zero Peek Robustness Checks for Federated Learning.](http://arxiv.org/abs/2206.12100) | 该论文提出了zPROBE，一种用于联邦学习的零窥探鲁棒性检查方法。通过使用高断点基于排序的统计信息，并利用随机化聚类技术，实现了对聚合模型更新的私密鲁棒性检查，提高了可扩展性且不损害隐私。 |
| [^149] | [Automated GI tract segmentation using deep learning.](http://arxiv.org/abs/2206.11048) | 本文介绍了使用深度学习的自动化分割过程，旨在加快肿瘤治疗中调整剂量递送路径的流程，避免耗时的手动操作，提高患者治疗效果。 |
| [^150] | [Boosting the Adversarial Transferability of Surrogate Models with Dark Knowledge.](http://arxiv.org/abs/2206.08316) | 本文提出了一种使用暗知识训练替代模型以提高对抗性样本传递性的方法，通过提取暗知识和使用混合扩增算法来训练所谓的暗替代模型(DSM)。 |
| [^151] | [ResNorm: Tackling Long-tailed Degree Distribution Issue in Graph Neural Networks via Normalization.](http://arxiv.org/abs/2206.08181) | 本文提出了一种名为ResNorm的图神经网络标准化方法，通过重新塑造节点标准差分布来解决长尾节点度分布问题，提高节点分类的准确性。 |
| [^152] | [Chordal Sparsity for SDP-based Neural Network Verification.](http://arxiv.org/abs/2206.03482) | 本文提出了一种基于半定规划的神经网络验证方法，通过引入弦状稀疏性，旨在改善现有技术中存在的可扩展性问题。 |
| [^153] | [Individual Privacy Accounting for Differentially Private Stochastic Gradient Descent.](http://arxiv.org/abs/2206.02617) | 本文研究了通过差分隐私随机梯度下降训练的模型对个体示例的隐私保证，并发现大多数示例享有较强的隐私保证。此外，我们还发现训练损失和示例的隐私参数存在很强的相关性。最低准确率类别的平均隐私参数比最高准确率类别高44.2%。 |
| [^154] | [Hierarchical Distribution-Aware Testing of Deep Learning.](http://arxiv.org/abs/2205.08589) | 本文提出了一种新的深度学习鲁棒性测试方法，通过考虑特征和像素级别的分布，捕捉对抗扰动的感知质量，以改善模型可靠性。 |
| [^155] | [Introspective Deep Metric Learning for Image Retrieval.](http://arxiv.org/abs/2205.04449) | 本文提出了一种内省式深度度量学习（IDML）框架，通过不确定性建模改进了深度度量学习的性能，并在多个数据集上取得了最先进的结果。 |
| [^156] | [Engineering flexible machine learning systems by traversing functionally-invariant paths.](http://arxiv.org/abs/2205.00334) | 该论文介绍了一个名为功能不变路径（FIP）的差分几何框架，用于实现神经网络的灵活、连续适应，以应对各种机器学习目标和网络稀疏化目标。 |
| [^157] | [Dawn of the transformer era in speech emotion recognition: closing the valence gap.](http://arxiv.org/abs/2203.07378) | 本文通过对几种预训练变体的分析，发现在语音情绪识别领域，使用Transformer架构能够在没有使用显式语言信息的情况下获得最佳的价值预测性能，相关性系数为0.638。 |
| [^158] | [DeltaCNN: End-to-End CNN Inference of Sparse Frame Differences in Videos.](http://arxiv.org/abs/2203.03996) | DeltaCNN是一种稀疏卷积神经网络框架，通过逐帧稀疏更新，加速视频推理的实践应用，而无需重新训练。 |
| [^159] | [Knowledge-informed Molecular Learning: A Survey on Paradigm Transfer.](http://arxiv.org/abs/2202.10587) | 本文调查了知识驱动的分子学习，从范式转移的视角出发，总结了其不同范式的分类和方法论，并分析了领域知识的贡献。 |
| [^160] | [Silent Bugs in Deep Learning Frameworks: An Empirical Study of Keras and TensorFlow.](http://arxiv.org/abs/2112.13314) | 本文通过实证研究了Keras和TensorFlow中的静默错误，这些错误对用户程序产生了影响。通过对相关问题进行分类和分析，提出了解决这些错误的方法和建议。 |
| [^161] | [Dual Correction Strategy for Ranking Distillation in Top-N Recommender System.](http://arxiv.org/abs/2109.03459) | 本文提出了一种双重修正策略（DCD），用于在推荐系统中更有效地将教师模型的排名信息转移到学生模型。这种方法不仅充分利用了学生模型的预测误差，还提供了更全面的视角，解决了松弛排名蒸馏方法的限制。 |
| [^162] | [Generative Network-Based Reduced-Order Model for Prediction, Data Assimilation and Uncertainty Quantification.](http://arxiv.org/abs/2105.13859) | 该论文提出了一种基于生成网络的降阶模型，用于解决偏微分方程的逆问题。通过使用无条件模拟进行训练，该模型可以有效量化不确定性，并准确匹配测量数据和黄金标准。 |
| [^163] | [A Lyapunov Theory for Finite-Sample Guarantees of Asynchronous Q-Learning and TD-Learning Variants.](http://arxiv.org/abs/2102.01567) | 本文提出了一种统一的框架来研究异步强化学习算法的有限样本收敛保证，并基于Lyapunov分析建立了异步RL算法的均方误差界限。通过对n步TD和TD（λ）的收敛界限的分析，揭示了强化学习中引导技巧效率的理论洞见。 |
| [^164] | [Het-node2vec: second order random walk sampling for heterogeneous multigraphs embedding.](http://arxiv.org/abs/2101.01425) | Het-node2vec是一个算法框架，通过在异构多图上进行二阶随机游走采样，能够捕获图的结构特征和不同类型节点边的语义，有效地提高对异构图的无监督和有监督学习性能。 |
| [^165] | [An Efficiency-boosting Client Selection Scheme for Federated Learning with Fairness Guarantee.](http://arxiv.org/abs/2011.01783) | 本文提出了一个基于Lyapunov优化问题的公平保证的客户选择方案，该方案通过估计每个客户端与服务器之间的模型交换时间，提高了联邦学习的效率。 |
| [^166] | [Machine Learning (In) Security: A Stream of Problems.](http://arxiv.org/abs/2010.16045) | 本论文针对机器学习在网络安全领域的应用提出了一系列挑战，包括概念漂移、演化、延迟标签和对抗性机器学习对现有解决方案的影响，强调了正确构建和评估基于机器学习的安全解决方案的重要性。 |
| [^167] | [Does the $\ell_1$-norm Learn a Sparse Graph under Laplacian Constrained Graphical Models?.](http://arxiv.org/abs/2006.14925) | 本文研究了在受限Laplacian图模型下学习稀疏图的问题。我们发现经典的$\ell_1$-范数正则化无法有效实现稀疏解，并提出了一种非凸稀疏惩罚的方法来解决这个问题。 |

# 详细

[^1]: 大型语言模型中的命名实体上下文偏倚研究

    Contextual Biasing of Named-Entities with Large Language Models. (arXiv:2309.00723v1 [cs.CL])

    [http://arxiv.org/abs/2309.00723](http://arxiv.org/abs/2309.00723)

    本文研究了使用大型语言模型进行上下文偏倚的方法，通过在第二次打分时提供额外的上下文信息，以提高自动语音识别性能。我们利用提示信息对大型语言模型进行boosting，并采用多任务训练以预测实体类别和下一个标记。此外，我们提出了动态提示方法来提高效率。

    

    本文研究了在大型语言模型(LLMs)中进行上下文偏倚，即在第二次打分时为LLM提供额外的上下文信息，以提高自动语音识别(ASR)性能。我们提出了在打分期间利用提示信息对LLM进行boosting，而无需进行微调，这些提示信息包括偏倚列表和少样本示例，用于在计算假设得分时作为附加信息。除了少样本提示学习外，我们还提出了LLM的多任务训练，以预测实体类别和下一个标记。为了提高上下文偏倚的效率并避免超过LLMs的最大序列长度，我们提出了动态提示，即使用类别标签预测选择最可能的类别，并仅使用这个类别中的实体作为下一个标记预测的上下文。对内部的呼叫、消息和口述数据集以及SLUE-Voxpopuli数据集进行了词错误率(WER)评估。

    This paper studies contextual biasing with Large Language Models (LLMs), where during second-pass rescoring additional contextual information is provided to a LLM to boost Automatic Speech Recognition (ASR) performance. We propose to leverage prompts for a LLM without fine tuning during rescoring which incorporate a biasing list and few-shot examples to serve as additional information when calculating the score for the hypothesis. In addition to few-shot prompt learning, we propose multi-task training of the LLM to predict both the entity class and the next token. To improve the efficiency for contextual biasing and to avoid exceeding LLMs' maximum sequence lengths, we propose dynamic prompting, where we select the most likely class using the class tag prediction, and only use entities in this class as contexts for next token prediction. Word Error Rate (WER) evaluation is performed on i) an internal calling, messaging, and dictation dataset, and ii) the SLUE-Voxpopuli dataset. Results
    
[^2]: 从多任务示范中学习共享的安全约束

    Learning Shared Safety Constraints from Multi-task Demonstrations. (arXiv:2309.00711v1 [cs.LG])

    [http://arxiv.org/abs/2309.00711](http://arxiv.org/abs/2309.00711)

    本论文介绍了如何从安全任务完成的专家示范中学习共享的安全约束，通过将逆强化学习技术扩展到约束空间，学习约束以禁止专家可能采取但选择不采取的高收益行为，并利用多任务设置中多样化示范学习一组更紧密的约束。

    

    无论我们希望智能体在环境中执行哪项任务，我们通常希望它们遵守共享的安全约束。例如，无论是做三明治还是清理桌子，厨房机器人都不应该打破盘子。手动指定这样的约束可能耗时且容易出错。我们展示了如何通过将逆强化学习(IRL)技术扩展到约束空间来从安全任务完成的专家示范中学习约束。直观地说，我们学习约束以禁止专家可能采取但选择不采取的高收益行为。不幸的是，约束学习问题通常不明确，并且通常导致过于保守的约束，禁止所有专家没有采取的行为。我们通过利用多任务设置中自然发生的多样化示范来学习一组更紧密的约束来解决这个问题。我们用仿真实验证实了我们的方法。

    Regardless of the particular task we want them to perform in an environment, there are often shared safety constraints we want our agents to respect. For example, regardless of whether it is making a sandwich or clearing the table, a kitchen robot should not break a plate. Manually specifying such a constraint can be both time-consuming and error-prone. We show how to learn constraints from expert demonstrations of safe task completion by extending inverse reinforcement learning (IRL) techniques to the space of constraints. Intuitively, we learn constraints that forbid highly rewarding behavior that the expert could have taken but chose not to. Unfortunately, the constraint learning problem is rather ill-posed and typically leads to overly conservative constraints that forbid all behavior that the expert did not take. We counter this by leveraging diverse demonstrations that naturally occur in multi-task settings to learn a tighter set of constraints. We validate our method with simula
    
[^3]: 利用人类反馈进行实际交通模拟的强化学习

    Reinforcement Learning with Human Feedback for Realistic Traffic Simulation. (arXiv:2309.00709v1 [cs.AI])

    [http://arxiv.org/abs/2309.00709](http://arxiv.org/abs/2309.00709)

    本研究利用强化学习与人类偏好相结合的框架来增强现有交通模型的真实性，并通过引入第一个用于交通建模中真实性对齐的数据集来支持此研究。

    

    鉴于真实世界测试的挑战和成本，自动驾驶车辆开发者通常依赖模拟测试来创建可靠的系统。有效模拟的关键要素是融入与人类知识相一致的真实交通模型，这一方面因为需要平衡真实性和多样性而具有挑战性。本研究旨在通过开发一个框架，利用强化学习与人类偏好（RLHF）来增强现有交通模型的真实性。该研究还确定了两个主要挑战：捕捉人类对真实性的微妙偏好以及统一多样的交通模拟模型。为了解决这些问题，我们建议使用人类反馈进行对齐，并采用RLHF因其样本效率高。我们还介绍了第一个用于交通建模中真实性对齐的数据集，以支持此类研究。我们的框架名为TrafficRLHF，在生成现实世界般的交通模拟数据方面展现了其能力。

    In light of the challenges and costs of real-world testing, autonomous vehicle developers often rely on testing in simulation for the creation of reliable systems. A key element of effective simulation is the incorporation of realistic traffic models that align with human knowledge, an aspect that has proven challenging due to the need to balance realism and diversity. This works aims to address this by developing a framework that employs reinforcement learning with human preference (RLHF) to enhance the realism of existing traffic models. This study also identifies two main challenges: capturing the nuances of human preferences on realism and the unification of diverse traffic simulation models. To tackle these issues, we propose using human feedback for alignment and employ RLHF due to its sample efficiency. We also introduce the first dataset for realism alignment in traffic modeling to support such research. Our framework, named TrafficRLHF, demonstrates its proficiency in generati
    
[^4]: 几何深度学习: 对图神经网络的基于温度的分析

    Geometric Deep Learning: a Temperature Based Analysis of Graph Neural Networks. (arXiv:2309.00699v1 [cs.LG])

    [http://arxiv.org/abs/2309.00699](http://arxiv.org/abs/2309.00699)

    本研究使用温度作为一个非量子和非相对论粒子的几何深度学习模型的分析方法，研究了GCN和GAT模型的各个层次中的温度，并讨论了可能的未来应用。

    

    我们将几何深度学习模型视为热力学系统，将权重看作非量子和非相对论粒子。我们采用之前在[7]中定义的温度概念，并研究了GCN和GAT模型的各个层次中的温度。讨论了我们发现的潜在未来应用。

    We examine a Geometric Deep Learning model as a thermodynamic system treating the weights as non-quantum and non-relativistic particles. We employ the notion of temperature previously defined in [7] and study it in the various layers for GCN and GAT models. Potential future applications of our findings are discussed.
    
[^5]: 在动态学习中共同探索客户漂移和灾难性遗忘

    Jointly Exploring Client Drift and Catastrophic Forgetting in Dynamic Learning. (arXiv:2309.00688v1 [cs.LG])

    [http://arxiv.org/abs/2309.00688](http://arxiv.org/abs/2309.00688)

    本论文提出了一个统一的分析框架，用于在动态学习中共同探索客户漂移和灾难性遗忘的问题。通过构建一个受控的测试环境，我们证明了客户漂移和灾难性遗忘背后的根本原因是相关的，并且通过生成一个3D地形图进一步验证了这种相关性。

    

    联邦学习和连续学习已经被提出作为在动态环境中稳健和隐私意识的使用深度学习的潜在范式。然而，客户漂移和灾难性遗忘是保证一致性性能的基本障碍。现有的工作只是分别解决这些问题，忽视了这两种性能下降背后的根本原因是联系在一起的事实。我们提出了一个统一的分析框架，用于构建一个受控的测试环境，以探索客户漂移 - 通过扰动一定比例的客户 - 和灾难性遗忘 - 通过以特定强度迁移所有客户 - 的影响。我们的框架通过生成一个从两者的组合性能影响产生的3D地形图进一步利用这种新的组合分析。我们证明了通过客户漂移引起的性能下降，通过一定比例的迁移客户，与由相应的迁移强度引起的灾难性遗忘的性能下降相关。

    Federated and Continual Learning have emerged as potential paradigms for the robust and privacy-aware use of Deep Learning in dynamic environments. However, Client Drift and Catastrophic Forgetting are fundamental obstacles to guaranteeing consistent performance. Existing work only addresses these problems separately, which neglects the fact that the root cause behind both forms of performance deterioration is connected. We propose a unified analysis framework for building a controlled test environment for Client Drift -- by perturbing a defined ratio of clients -- and Catastrophic Forgetting -- by shifting all clients with a particular strength. Our framework further leverages this new combined analysis by generating a 3D landscape of the combined performance impact from both. We demonstrate that the performance drop through Client Drift, caused by a certain share of shifted clients, is correlated to the drop from Catastrophic Forgetting resulting from a corresponding shift strength. 
    
[^6]: 任意分布式机器学习的随机极化码

    Randomized Polar Codes for Anytime Distributed Machine Learning. (arXiv:2309.00682v1 [cs.DC])

    [http://arxiv.org/abs/2309.00682](http://arxiv.org/abs/2309.00682)

    本论文提出了一个用于任意分布式机器学习的随机极化码框架，通过结合随机摘要和极化码的概念，实现了对慢计算节点的鲁棒性，并提供了适用于实值数据的低计算复杂度的串行译码算法和即时估计器，该框架在大规模矩阵乘法和黑盒优化等多个应用场景中具有潜在的应用价值。

    

    我们提出了一个新颖的分布式计算框架，该框架能够抵抗慢计算节点，并能够进行线性操作的近似和精确计算。所提出的机制在编码计算的背景下，结合了随机摘要和极化码的概念。我们提出了一种用于处理实值数据的串行译码算法，以维持低计算复杂度用于恢复。此外，我们提供了一种即时估计器，即使可用节点输出无法译码，也能够生成可证明准确的估计结果。我们展示了该框架在各种情境下的潜在应用，如大规模矩阵乘法和黑盒优化。我们在无服务器云计算系统上实现了这些方法，并提供了数值结果来展示它们在实践中的可扩展性，包括ImageNet规模计算。

    We present a novel distributed computing framework that is robust to slow compute nodes, and is capable of both approximate and exact computation of linear operations. The proposed mechanism integrates the concepts of randomized sketching and polar codes in the context of coded computation. We propose a sequential decoding algorithm designed to handle real valued data while maintaining low computational complexity for recovery. Additionally, we provide an anytime estimator that can generate provably accurate estimates even when the set of available node outputs is not decodable. We demonstrate the potential applications of this framework in various contexts, such as large-scale matrix multiplication and black-box optimization. We present the implementation of these methods on a serverless cloud computing system and provide numerical results to demonstrate their scalability in practice, including ImageNet scale computations.
    
[^7]: 脱离上下文的影响：关于衡量LLMs中的情境意识论文

    Taken out of context: On measuring situational awareness in LLMs. (arXiv:2309.00667v1 [cs.CL])

    [http://arxiv.org/abs/2309.00667](http://arxiv.org/abs/2309.00667)

    这个论文目的在于研究大型语言模型中的情境意识的产生，提出了一种衡量模型情境意识的能力，并通过实验证明了其有效性。

    

    我们旨在更好地理解大型语言模型（LLMs）中“情境意识”的出现。如果一个模型在意识到自己是一个模型的同时，能够识别自己当前是处于测试或部署状态，那么这个模型在情境上是具备意识的。今天的LLMs在部署之前会经过安全性和对齐性的测试。在部署后，一个LLM可能会利用情境意识在安全测试中取得高分，但在实际使用中采取有害行为。情境意识可能会意外地在模型扩展中出现。为了更好地预测这种出现，我们提出了对于情境意识而言必要的能力之一，即“脱离上下文推理”（与基于上下文的学习相对）。我们通过实验研究了脱离上下文推理。首先，我们在没有提供任何示例或演示的情况下，对LLM进行了描述测试的微调。在测试时，我们评估模型是否能通过测试。令我们惊讶的是，我们发现LLMs在这种情况下取得了成功。

    We aim to better understand the emergence of `situational awareness' in large language models (LLMs). A model is situationally aware if it's aware that it's a model and can recognize whether it's currently in testing or deployment. Today's LLMs are tested for safety and alignment before they are deployed. An LLM could exploit situational awareness to achieve a high score on safety tests, while taking harmful actions after deployment. Situational awareness may emerge unexpectedly as a byproduct of model scaling. One way to better foresee this emergence is to run scaling experiments on abilities necessary for situational awareness. As such an ability, we propose `out-of-context reasoning' (in contrast to in-context learning). We study out-of-context reasoning experimentally. First, we finetune an LLM on a description of a test while providing no examples or demonstrations. At test time, we assess whether the model can pass the test. To our surprise, we find that LLMs succeed on this out-
    
[^8]: ICDARTS: 改进循环DARTS的稳定性和性能

    ICDARTS: Improving the Stability and Performance of Cyclic DARTS. (arXiv:2309.00664v1 [cs.LG])

    [http://arxiv.org/abs/2309.00664](http://arxiv.org/abs/2309.00664)

    ICDARTS改进了循环DARTS的稳定性和性能，通过消除评估网络权重对搜索网络权重的依赖，并引入修改后的搜索离散化过程。

    

    本研究改进了循环DARTS（CDARTS）的稳定性和泛化能力。CDARTS是一种基于可微分架构搜索（DARTS）的神经架构搜索（NAS）方法，使用循环反馈机制同时训练搜索和评估网络。该训练协议旨在通过强制要求搜索和评估网络产生相似的输出来优化搜索过程。然而，CDARTS引入了一个依赖于搜索网络的评估网络损失函数。搜索和重新训练阶段评估网络使用的损失函数的不相似性导致搜索阶段评估网络成为重新训练期间使用的最终评估网络的次优代理。我们提出ICDARTS，一种修正的方法，消除了评估网络权重对搜索网络权重的依赖，以及一种修改的搜索离散化过程。

    This work introduces improvements to the stability and generalizability of Cyclic DARTS (CDARTS). CDARTS is a Differentiable Architecture Search (DARTS)-based approach to neural architecture search (NAS) that uses a cyclic feedback mechanism to train search and evaluation networks concurrently. This training protocol aims to optimize the search process by enforcing that the search and evaluation networks produce similar outputs. However, CDARTS introduces a loss function for the evaluation network that is dependent on the search network. The dissimilarity between the loss functions used by the evaluation networks during the search and retraining phases results in a search-phase evaluation network that is a sub-optimal proxy for the final evaluation network that is utilized during retraining. We present ICDARTS, a revised approach that eliminates the dependency of the evaluation network weights upon those of the search network, along with a modified process for discretizing the search n
    
[^9]: 基于多项式模型的黑盒目标优化

    Polynomial-Model-Based Optimization for Blackbox Objectives. (arXiv:2309.00663v1 [cs.LG])

    [http://arxiv.org/abs/2309.00663](http://arxiv.org/abs/2309.00663)

    基于多项式模型的黑盒优化是一种新颖的优化方法，通过拟合多项式代理寻找未知系统的最佳参数，成功竞争并超越其他算法，在黑盒优化问题的解决中具有关键意义。

    

    对于像神经网络或复杂模拟这样的系统结构，在许多应用中，是未知的，近似是昂贵甚至不可能实现的。黑盒优化旨在找到这些系统的最佳（超）参数，使预定义的目标函数最小化。基于多项式模型的优化（PMBO）是一种新颖的黑盒优化器，通过拟合多项式代理来寻找最小值。受贝叶斯优化的启发，该模型根据预期改进的采集函数进行迭代更新，从而平衡了开发和探索率，并提供了模型的不确定性估计。对于给定的一组人工、分析函数，PMBO与其他最先进的算法进行了基准测试。PMBO成功地与这些算法竞争，并在某些情况下甚至超越了它们。根据结果，我们认为PMBO是解决黑盒优化问题的关键选择。

    For a wide range of applications the structure of systems like Neural Networks or complex simulations, is unknown and approximation is costly or even impossible. Black-box optimization seeks to find optimal (hyper-) parameters for these systems such that a pre-defined objective function is minimized. Polynomial-Model-Based Optimization (PMBO) is a novel blackbox optimizer that finds the minimum by fitting a polynomial surrogate to the objective function.  Motivated by Bayesian optimization the model is iteratively updated according to the acquisition function Expected Improvement, thus balancing the exploitation and exploration rate and providing an uncertainty estimate of the model. PMBO is benchmarked against other state-of-the-art algorithms for a given set of artificial, analytical functions. PMBO competes successfully with those algorithms and even outperforms all of them in some cases. As the results suggest, we believe PMBO is the pivotal choice for solving blackbox optimization
    
[^10]: 本文研究在带有轨迹反馈的零和不完全信息博弈中如何学习ε-最优策略。在这种情况下，玩家根据他们在固定数量的回合中观察到的信息依次更新策略。现有的方法由于使用了重要性采样来对动作序列进行估计，导致方差较高。为了减小这种方差，我们考虑使用一种固定采样方法，即玩家在随时间变化的情况下仍然更新策略，但观察到的信息是通过给定的固定采样策略获得的。我们的方法基于一种自适应的在线镜像下降（OMD）算法，该算法将OMD应用于每个信息集，使用逐渐减小的学习率和正则化损失。我们证明了这种方法在高概率下具有收敛速度为$\tilde{\mathcal{O}}(T^{-1/2})$，并在应用最佳策略时对游戏参数具有近乎最优的依赖关系。

    Local and adaptive mirror descents in extensive-form games. (arXiv:2309.00656v1 [cs.GT])

    [http://arxiv.org/abs/2309.00656](http://arxiv.org/abs/2309.00656)

    本文研究了在零和不完全信息博弈中学习ε-最优策略的问题。通过提出一种固定采样方法，并使用自适应的在线镜像下降算法进行局部更新，我们取得了收敛速度为$\tilde{\mathcal{O}}(T^{-1/2})$的结果，并在最佳策略下对游戏参数具有近乎最优的依赖关系。

    

    本文研究在带有轨迹反馈的零和不完全信息博弈中学习ε-最优策略的问题。现有方法由于使用了重要性采样，存在方差较高的问题。为了减小方差，我们提出了一种固定采样方法，使用固定采样策略来观察信息。我们的方法基于自适应的在线镜像下降算法，对每个信息集进行局部更新，并使用逐渐减小的学习率和正则化损失。我们证明了该方法在高概率下具有收敛速度为$\tilde{\mathcal{O}}(T^{-1/2})$，并在最佳策略下对游戏参数具有近乎最优的依赖关系。

    We study how to learn $\epsilon$-optimal strategies in zero-sum imperfect information games (IIG) with trajectory feedback. In this setting, players update their policies sequentially based on their observations over a fixed number of episodes, denoted by $T$. Existing procedures suffer from high variance due to the use of importance sampling over sequences of actions (Steinberger et al., 2020; McAleer et al., 2022). To reduce this variance, we consider a fixed sampling approach, where players still update their policies over time, but with observations obtained through a given fixed sampling policy. Our approach is based on an adaptive Online Mirror Descent (OMD) algorithm that applies OMD locally to each information set, using individually decreasing learning rates and a regularized loss. We show that this approach guarantees a convergence rate of $\tilde{\mathcal{O}}(T^{-1/2})$ with high probability and has a near-optimal dependence on the game parameters when applied with the best 
    
[^11]: 使用合成数据来训练AI模型：可持续发展的机遇与风险

    The Use of Synthetic Data to Train AI Models: Opportunities and Risks for Sustainable Development. (arXiv:2309.00652v1 [cs.LG])

    [http://arxiv.org/abs/2309.00652](http://arxiv.org/abs/2309.00652)

    合成数据是一种用于训练AI模型的人工生成数据，在保护隐私的同时提供了研究数据的可用性与减少机器学习模型偏见的机会，但同时也需要制定合适的政策来确保数据质量、真实性以及平衡隐私与数据效用之间的关系。

    

    在当前数据驱动的时代，合成数据，即人工生成的类似真实世界数据特征但不包含实际个人信息的数据，日益受到关注。这是因为它有潜力保护隐私、增加研究数据的可用性，并减少机器学习模型中的偏见。本文研究了合成数据的创建、利用和传播政策。合成数据可以是保护个人隐私的有力工具，但也面临挑战，如确保其质量和真实性。一个精心设计的合成数据政策必须在隐私关注和数据效用之间取得平衡，确保在不损害道德或法律标准的前提下有效利用数据。组织和机构必须制定标准化的指导方针和最佳实践，以充分利用合成数据的好处并解决其中的固有问题。

    In the current data driven era, synthetic data, artificially generated data that resembles the characteristics of real world data without containing actual personal information, is gaining prominence. This is due to its potential to safeguard privacy, increase the availability of data for research, and reduce bias in machine learning models. This paper investigates the policies governing the creation, utilization, and dissemination of synthetic data. Synthetic data can be a powerful instrument for protecting the privacy of individuals, but it also presents challenges, such as ensuring its quality and authenticity. A well crafted synthetic data policy must strike a balance between privacy concerns and the utility of data, ensuring that it can be utilized effectively without compromising ethical or legal standards. Organizations and institutions must develop standardized guidelines and best practices in order to capitalize on the benefits of synthetic data while addressing its inherent c
    
[^12]: 通过对辅助数据进行监督，改进小型关键词检测模型的效果

    Improving Small Footprint Few-shot Keyword Spotting with Supervision on Auxiliary Data. (arXiv:2309.00647v1 [eess.AS])

    [http://arxiv.org/abs/2309.00647](http://arxiv.org/abs/2309.00647)

    本研究提出了一种改进小型关键词检测模型的方法，通过对辅助数据进行监督，利用自动标注和筛选数据构建了一个类似关键词的数据集，并采用多任务学习提升模型的表示能力，取得了在少样本关键词检测基准测试中优于竞争方法的性能表现。

    

    少样本关键词检测模型通常需要大规模的标注数据集才能适应未见过的目标关键词。然而，现有的关键词检测数据集在规模上存在限制，并且收集类似关键词的标注数据是一项昂贵的任务。为了解决这个问题，我们提出了一个框架，利用容易收集的无标注阅读语音数据作为辅助来源。自监督学习广泛应用于从无标注数据中学习表征，但是它适用于具有足够容量的大模型，并不适合训练小型的关键词检测模型。相反，我们通过自动标注和筛选数据来构建一个类似关键词的数据集LibriWord，实现对辅助数据的监督。然后我们采用多任务学习来提升模型从域外辅助数据中的表示能力。我们的方法显著改善了在少样本关键词检测基准测试中的性能，超过了竞争方法。

    Few-shot keyword spotting (FS-KWS) models usually require large-scale annotated datasets to generalize to unseen target keywords. However, existing KWS datasets are limited in scale and gathering keyword-like labeled data is costly undertaking. To mitigate this issue, we propose a framework that uses easily collectible, unlabeled reading speech data as an auxiliary source. Self-supervised learning has been widely adopted for learning representations from unlabeled data; however, it is known to be suitable for large models with enough capacity and is not practical for training a small footprint FS-KWS model. Instead, we automatically annotate and filter the data to construct a keyword-like dataset, LibriWord, enabling supervision on auxiliary data. We then adopt multi-task learning that helps the model to enhance the representation power from out-of-domain auxiliary data. Our method notably improves the performance over competitive methods in the FS-KWS benchmark.
    
[^13]: 对于多维和杂质训练数据的最优血清分类的最小假设：理论和应用

    Minimal Assumptions for Optimal Serology Classification: Theory and Implications for Multidimensional Settings and Impure Training Data. (arXiv:2309.00645v1 [stat.ML])

    [http://arxiv.org/abs/2309.00645](http://arxiv.org/abs/2309.00645)

    本研究提出了一种血清分类的技术，可以在多维和有杂质的训练数据情况下，通过对样本的分类和估计患病率来减少误差。该方法不需要直接访问条件概率密度函数，而是将数据嵌入参数化的曲线空间，并通过最小化经验误差来优化空间。

    

    在血清学中，减少偏差估计和诊断分类器仍然是一个具有挑战性的任务。理论上，这些问题可以转化为建模测量结果的类别-条件概率密度函数（PDFs），它们控制所有后续分析。然而，即使对于仅具有少数维度（例如目标抗原）的测量输出，这个任务也很快受到维度诅咒的影响。为了解决这个问题，我们提出了一种技术，利用经验训练数据在任意维度上分类样本和估计患病率，而不需要直接访问条件PDFs。我们通过一个引理来解释这个方法，该引理将相对条件概率与最小误差分类边界联系起来。这使我们能够制定一个优化问题：（i）将数据嵌入参数化的曲线空间；（ii）根据样本相对于坐标轴的位置对样本进行分类；（iii）通过最小化经验

    Minimizing error in prevalence estimates and diagnostic classifiers remains a challenging task in serology. In theory, these problems can be reduced to modeling class-conditional probability densities (PDFs) of measurement outcomes, which control all downstream analyses. However, this task quickly succumbs to the curse of dimensionality, even for assay outputs with only a few dimensions (e.g. target antigens). To address this problem, we propose a technique that uses empirical training data to classify samples and estimate prevalence in arbitrary dimension without direct access to the conditional PDFs. We motivate this method via a lemma that relates relative conditional probabilities to minimum-error classification boundaries. This leads us to formulate an optimization problem that: (i) embeds the data in a parameterized, curved space; (ii) classifies samples based on their position relative to a coordinate axis; and (iii) subsequently optimizes the space by minimizing the empirical c
    
[^14]: 使用变模分解（VMD）和时同步平均（TSA）组合方法的齿轮齿裂分析的改进拉格朗日公式（Lagrangian Formulation）（arXiv:2309.00641v1 [eess.SY]）

    Modified Lagrangian Formulation of Gear Tooth Crack Analysis using Combined Approach of Variable Mode Decomposition (VMD) and Time Synchronous Averaging (TSA). (arXiv:2309.00641v1 [eess.SY])

    [http://arxiv.org/abs/2309.00641](http://arxiv.org/abs/2309.00641)

    本论文介绍了一个整合了变模分解（VMD）和时同步平均（TSA）的方法，用于分析耦合电机-机械变速器（CEMG）系统中的齿轮齿裂。通过改进的拉格朗日公式和时变啮合刚度模型，研究了裂纹对系统动态行为的影响。

    

    本文讨论了一个采用变模分解（VMD）和时同步平均（TSA）的组合方法的整合齿轮齿裂分析程序，该程序基于耦合电机-机械变速器（CEMG）系统。本文还结合了改进的拉格朗日公式，通过考虑Rayleigh的耗散势来模拟CEMG系统。同时，将具有不同齿轮齿裂深度的分析改进的时变啮合刚度（IAM-TVMS）也纳入到CEMG系统中，以检查裂纹对系统动态行为的影响。分析了不同齿轮齿裂水平的CEMG系统的动态响应，进一步研究了VMD和TSA的整合方法首次应用于分析CEMG系统的动态行为，纳入了不同齿轮齿裂。

    This paper discusses the possible observation of an integrated gear tooth crack analysis procedure that employs the combined approach of variable mode decomposition (VMD) and time synchronous averaging (TSA) based on the coupled electromechanical gearbox (CEMG) system. This paper also incorporates the modified Lagrangian formulation to model the CEMG system by considering Rayleigh's dissipative potential. An analytical improved time-varying mesh stiffness (IAM-TVMS) with different levels of gear tooth crack depts is also incorporated into the CEMG system to inspect the influence of cracks on the system's dynamic behavior. Dynamic responses of the CEMG system with different tooth crack levels have been used for further investigations. For the first time, the integrated approach of variable mode decomposition (VMD) and time-synchronous averaging (TSA) has been presented to analyze the dynamic behaviour of CEMG systems at the different gear tooth cracks have been experienced as non-statio
    
[^15]: 生成式人工智能用于端到端的限价订单簿建模：一种使用深度状态空间网络的令牌级自回归生成模型的消息流

    Generative AI for End-to-End Limit Order Book Modelling: A Token-Level Autoregressive Generative Model of Message Flow Using a Deep State Space Network. (arXiv:2309.00638v1 [q-fin.TR])

    [http://arxiv.org/abs/2309.00638](http://arxiv.org/abs/2309.00638)

    本论文提出了一种使用深度状态空间网络的令牌级自回归生成模型，可以生成逼真的限价订单簿消息，并且在逼近数据分布和中间价格回报方面表现出有前景的性能。

    

    在金融市场中开发一个逼真的订单流生成模型是一个具有挑战性的开放问题，具有许多市场参与者的应用。为解决这个问题，我们提出了第一个端到端的自回归生成模型，可以生成令牌化的限价订单簿(LOB)消息。这些消息由Jax-LOB模拟器解释，该模拟器更新LOB状态。为了高效处理长序列，该模型使用简化的结构化状态空间层来处理订单簿状态和令牌化消息的序列。利用NASDAQ股票LOBSTER数据，我们开发了一种自定义的消息数据分词器，将连续数字分组转换为令牌，类似于大型语言模型中的分词。外样本结果表明，在逼近数据分布方面，模型的困惑度较低，性能表现有前景。此外，从生成的订单流计算出的中间价格回报与数据呈显著相关性，表明了模型的潜在优势。

    Developing a generative model of realistic order flow in financial markets is a challenging open problem, with numerous applications for market participants. Addressing this, we propose the first end-to-end autoregressive generative model that generates tokenized limit order book (LOB) messages. These messages are interpreted by a Jax-LOB simulator, which updates the LOB state. To handle long sequences efficiently, the model employs simplified structured state-space layers to process sequences of order book states and tokenized messages. Using LOBSTER data of NASDAQ equity LOBs, we develop a custom tokenizer for message data, converting groups of successive digits to tokens, similar to tokenization in large language models. Out-of-sample results show promising performance in approximating the data distribution, as evidenced by low model perplexity. Furthermore, the mid-price returns calculated from the generated order flow exhibit a significant correlation with the data, indicating imp
    
[^16]: 基于有限元分析和机器学习的碳纤维器片电池容器设计与碰撞性能

    Finite Element Analysis and Machine Learning Guided Design of Carbon Fiber Organosheet-based Battery Enclosures for Crashworthiness. (arXiv:2309.00637v1 [cs.LG])

    [http://arxiv.org/abs/2309.00637](http://arxiv.org/abs/2309.00637)

    本论文通过使用有限元分析和机器学习，实现了基于碳纤维器片的电池容器设计与碰撞性能的优化。模拟和预测结果表明，这种设计方法可以有效提高电池容器的碰撞强度和安全性。

    

    由于具有更好的强度重量比和耐腐蚀性，碳纤维复合材料可以替代当前电动汽车的金属电池容器。然而，碳纤维结构的强度取决于几个需要仔细选择的参数。本研究中，我们实施了高通量的有限元分析（FEA）热成形模拟，以使用不同的设计和加工参数虚拟制造电池容器。随后，我们进行了虚拟碰撞模拟，模拟了侧杆碰撞，以评估电池容器的碰撞性能。这个高通量的碰撞模拟数据集被用来构建预测模型，以了解未知集的碰撞性能。我们的机器学习模型显示出优秀的性能（R2 > 0.97），可以预测碰撞性能指标，如压碎负荷效率、吸收能量、侵入和最大减速度。

    Carbon fiber composite can be a potential candidate for replacing metal-based battery enclosures of current electric vehicles (E.V.s) owing to its better strength-to-weight ratio and corrosion resistance. However, the strength of carbon fiber-based structures depends on several parameters that should be carefully chosen. In this work, we implemented high throughput finite element analysis (FEA) based thermoforming simulation to virtually manufacture the battery enclosure using different design and processing parameters. Subsequently, we performed virtual crash simulations to mimic a side pole crash to evaluate the crashworthiness of the battery enclosures. This high throughput crash simulation dataset was utilized to build predictive models to understand the crashworthiness of an unknown set. Our machine learning (ML) models showed excellent performance (R2 > 0.97) in predicting the crashworthiness metrics, i.e., crush load efficiency, absorbed energy, intrusion, and maximum decelerati
    
[^17]: 通过深度策略梯度方法进行大宗商品交易

    Commodities Trading through Deep Policy Gradient Methods. (arXiv:2309.00630v1 [q-fin.TR])

    [http://arxiv.org/abs/2309.00630](http://arxiv.org/abs/2309.00630)

    本文研究了深度强化学习在算法性大宗商品交易中的有效性，通过引入新颖的时间离散化方案和两种策略梯度算法，DRL模型将夏普比率提高了83%。

    

    由于其潜在的较好收益能力，算法交易受到了广泛关注。本文研究了深度强化学习（DRL）方法在算法性大宗商品交易中的有效性。它将大宗商品交易问题建模为连续的离散时间随机动态系统。所提出的系统采用了一种新颖的时间离散化方案，可以根据市场波动性调整，提高了子采样金融时间序列的统计特性。为了优化对交易成本和风险敏感的交易代理，引入了两种策略梯度算法，即基于演员的方法和基于演员-评论者的方法。这些代理利用CNN和LSTM作为参数化函数逼近器将历史价格观测映射到市场位置。对最近月份的天然气期货进行回测的结果表明，与买入并持有基线相比，DRL模型将夏普比率提高了83%。此外，代理的风险特征可以定制。

    Algorithmic trading has gained attention due to its potential for generating superior returns. This paper investigates the effectiveness of deep reinforcement learning (DRL) methods in algorithmic commodities trading. It formulates the commodities trading problem as a continuous, discrete-time stochastic dynamical system. The proposed system employs a novel time-discretization scheme that adapts to market volatility, enhancing the statistical properties of subsampled financial time series. To optimize transaction-cost- and risk-sensitive trading agents, two policy gradient algorithms, namely actor-based and actor-critic-based approaches, are introduced. These agents utilize CNNs and LSTMs as parametric function approximators to map historical price observations to market positions.Backtesting on front-month natural gas futures demonstrates that DRL models increase the Sharpe ratio by $83\%$ compared to the buy-and-hold baseline. Additionally, the risk profile of the agents can be custo
    
[^18]: 一种基于深度强化学习的集成方法用于自动加密货币交易

    An Ensemble Method of Deep Reinforcement Learning for Automated Cryptocurrency Trading. (arXiv:2309.00626v1 [q-fin.TR])

    [http://arxiv.org/abs/2309.00626](http://arxiv.org/abs/2309.00626)

    本研究提出了一种基于深度强化学习的集成方法，用于改进日内加密货币投资组合交易的交易策略的泛化性能。通过采用模型选择方法和新颖的混合分布策略，我们展示了该方法在不断变化的市场条件下的鲁棒性，并相比于基准模型显著提高了样本外表现。

    

    我们提出了一种集成方法，以改善深度强化学习算法在日内加密货币投资组合交易中的高度随机环境中的交易策略的泛化性能。我们采用了一种模型选择方法，在多个验证期间进行评估，并提出了一种新颖的混合分布策略来有效集成所选模型。我们提供了对精细测试期间的样本外表现的分布视角，以展示策略在不断变化的市场条件下的鲁棒性，并定期重新训练模型以应对金融数据的非稳定性。我们提出的集成方法相比于深度强化学习策略和被动投资策略的基准模型改善了样本外的表现。

    We propose an ensemble method to improve the generalization performance of trading strategies trained by deep reinforcement learning algorithms in a highly stochastic environment of intraday cryptocurrency portfolio trading. We adopt a model selection method that evaluates on multiple validation periods, and propose a novel mixture distribution policy to effectively ensemble the selected models. We provide a distributional view of the out-of-sample performance on granular test periods to demonstrate the robustness of the strategies in evolving market conditions, and retrain the models periodically to address non-stationarity of financial data. Our proposed ensemble method improves the out-of-sample performance compared with the benchmarks of a deep reinforcement learning strategy and a passive investment strategy.
    
[^19]: 使用外生变量和机器学习算法进行短期股票价格预测

    Short-Term Stock Price Forecasting using exogenous variables and Machine Learning Algorithms. (arXiv:2309.00618v1 [q-fin.TR])

    [http://arxiv.org/abs/2309.00618](http://arxiv.org/abs/2309.00618)

    本研究通过比较四种机器学习模型的准确性，发现在短期预测纽约证券交易所中三个知名股票的股价时，XGBoost模型表现出最高准确性。进一步优化参数或引入更多外生变量可能改进预测结果。

    

    在金融领域中，准确预测股票市场一直是一个重要的挑战。随着机器学习作为预测领域的下一级别的崛起，本研究论文比较了四种机器学习模型在2020年3月到2022年5月期间短期预测纽约证券交易所中三个知名股票的准确性。我们使用了XGBoost、随机森林、多层感知器和支持向量回归模型进行开发、调优和部署。我们根据RMSE、MAPE、MTT和MPE这些评估指标报告了产生最高准确性的模型。在使用240个交易日的训练数据集后，我们发现XGBoost尽管运行时间较长（高达10秒），但给出了最高准确性。进一步调整各个参数或引入更多外生变量可以改进本研究的结果。

    Creating accurate predictions in the stock market has always been a significant challenge in finance. With the rise of machine learning as the next level in the forecasting area, this research paper compares four machine learning models and their accuracy in forecasting three well-known stocks traded in the NYSE in the short term from March 2020 to May 2022. We deploy, develop, and tune XGBoost, Random Forest, Multi-layer Perceptron, and Support Vector Regression models. We report the models that produce the highest accuracies from our evaluation metrics: RMSE, MAPE, MTT, and MPE. Using a training data set of 240 trading days, we find that XGBoost gives the highest accuracy despite running longer (up to 10 seconds). Results from this study may improve by further tuning the individual parameters or introducing more exogenous variables.
    
[^20]: 面向对齐语言模型的对抗攻击的基线防御

    Baseline Defenses for Adversarial Attacks Against Aligned Language Models. (arXiv:2309.00614v1 [cs.LG])

    [http://arxiv.org/abs/2309.00614](http://arxiv.org/abs/2309.00614)

    这篇论文研究了对齐语言模型面临的对抗攻击问题，通过评估基线防御策略的效果，探讨了各种策略的可行性和有效性，并对鲁棒性和性能进行了讨论。

    

    随着大型语言模型的普及，其安全漏洞变得至关重要。最近的研究表明，文本优化器可以生成绕过审查和对齐的越狱提示。借鉴对抗机器学习的丰富研究成果，我们从三个问题入手：在这个领域中什么样的威胁模型是实用的？基线防御技术在这个新领域中表现如何？LLM安全性与计算机视觉有何不同？我们对主导对抗LLM攻击的几种基线防御策略进行评估，讨论了每种策略在各种设置下的可行性和有效性。特别是，我们关注三种类型的防御：检测（基于困惑度）、输入预处理（改写和重新标记化）和对抗训练。我们讨论了白盒和灰盒设置，并讨论了每种考虑的防御策略在鲁棒性和性能之间的权衡。

    As Large Language Models quickly become ubiquitous, their security vulnerabilities are critical to understand. Recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. Drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: What threat models are practically useful in this domain? How do baseline defense techniques perform in this new domain? How does LLM security differ from computer vision?  We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, discussing the various settings in which each is feasible and effective. Particularly, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. We discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. Surprisingly, we find much more succ
    
[^21]: 光学运动捕捉的基于局部性的神经求解器

    A Locality-based Neural Solver for Optical Motion Capture. (arXiv:2309.00428v1 [cs.GR])

    [http://arxiv.org/abs/2309.00428](http://arxiv.org/abs/2309.00428)

    该论文提出了一种基于局部性的神经求解器，用于清理和求解光学运动捕捉数据。论文通过构建异构图神经网络和使用特定的图卷积操作，提取标记和关节的局部特征，并转化为更准确的动作。通过研究标记的动作与其相邻标记的相关性，论文能够高效地填补缺失的标记，并通过分析加速度轮廓识别跟踪错误引起的异常标记。此外，论文还提出了基于表示学习和数据增强的训练机制，以进一步提高模型的准确性。

    

    我们提出了一种新的基于局部性的学习方法来清理和求解光学运动捕捉数据。给定噪声标记数据，我们提出了一种新的异构图神经网络，将标记和关节视为不同类型的节点，并使用图卷积操作提取标记和关节的局部特征，并将其转化为清晰的动作。为了处理异常标记（例如遮挡或具有较大的跟踪误差），关键洞察力在于标记的动作与其直接相邻的标记的动作之间存在很强的相关性，但与其他标记的相关性较小，即局部性，这使得我们能够有效地填补缺失的标记（例如由于遮挡引起的）。此外，我们还通过研究加速度轮廓来识别由于跟踪错误引起的异常标记。最后，我们提出了一个基于表示学习和数据增强的训练机制，通过在具有屏蔽的数据上训练模型。屏蔽方案旨在模拟遮挡和噪声等情况。

    We present a novel locality-based learning method for cleaning and solving optical motion capture data. Given noisy marker data, we propose a new heterogeneous graph neural network which treats markers and joints as different types of nodes, and uses graph convolution operations to extract the local features of markers and joints and transform them to clean motions. To deal with anomaly markers (e.g. occluded or with big tracking errors), the key insight is that a marker's motion shows strong correlations with the motions of its immediate neighboring markers but less so with other markers, a.k.a. locality, which enables us to efficiently fill missing markers (e.g. due to occlusion). Additionally, we also identify marker outliers due to tracking errors by investigating their acceleration profiles. Finally, we propose a training regime based on representation learning and data augmentation, by training the model on data with masking. The masking schemes aim to mimic the occluded and nois
    
[^22]: 长程图表基准的重新评估：差距去哪儿了？

    Where Did the Gap Go? Reassessing the Long-Range Graph Benchmark. (arXiv:2309.00367v1 [cs.LG])

    [http://arxiv.org/abs/2309.00367](http://arxiv.org/abs/2309.00367)

    本文对长程图表基准（LRGB）进行了重新评估，通过严格的实证分析发现，先前的报告性能差距被高估了，而经过基本超参数优化后，差距完全消失。此外，我们还讨论了特征归一化的缺失和链接预测度量的虚假实现对LRGB的影响。

    

    最近的长程图表基准(LRGB，Dwivedi等，2022)引入了一组与顶点之间的长程相互作用密切相关的图表学习任务。经验证据表明，在这些任务中，图形变换器明显优于消息传递GNN（MPGNN）。在本文中，我们对LRGB上的多个MPGNN基线以及图形变换器GPS（Ramp\'a\v{s}ek等，2022）进行了仔细重新评估。通过严格的实证分析，我们证明了由于子优超参数选择不当而高估了报告的性能差距。值得注意的是，在基本超参数优化后，跨多个数据集，性能差距完全消失。此外，我们还讨论了LRGB的视觉数据集缺乏特征归一化的影响，并突出了LRGB的链接预测度量的虚假实现。我们的论文的主要目标是在图机器学习社区建立更高的实证严谨性标准。

    The recent Long-Range Graph Benchmark (LRGB, Dwivedi et al. 2022) introduced a set of graph learning tasks strongly dependent on long-range interaction between vertices. Empirical evidence suggests that on these tasks Graph Transformers significantly outperform Message Passing GNNs (MPGNNs). In this paper, we carefully reevaluate multiple MPGNN baselines as well as the Graph Transformer GPS (Ramp\'a\v{s}ek et al. 2022) on LRGB. Through a rigorous empirical analysis, we demonstrate that the reported performance gap is overestimated due to suboptimal hyperparameter choices. It is noteworthy that across multiple datasets the performance gap completely vanishes after basic hyperparameter optimization. In addition, we discuss the impact of lacking feature normalization for LRGB's vision datasets and highlight a spurious implementation of LRGB's link prediction metric. The principal aim of our paper is to establish a higher standard of empirical rigor within the graph machine learning commun
    
[^23]: 当度量不可靠时：朝着Top-k多标签学习的不可察觉的对抗性扰动

    When Measures are Unreliable: Imperceptible Adversarial Perturbations toward Top-$k$ Multi-Label Learning. (arXiv:2309.00007v1 [cs.CV])

    [http://arxiv.org/abs/2309.00007](http://arxiv.org/abs/2309.00007)

    本文提出了一种面向Top-k多标签学习的不可察觉的对抗性扰动方法，既能够欺骗人眼的视觉感知，又能够避开度量监测。

    

    随着深度神经网络的巨大成功，对抗性学习在各种研究中得到了广泛关注，涵盖了从多类别学习到多标签学习的范围。然而，现有的对抗性攻击方法只关注传统的视觉不可察觉性，而忽视了来自度量的新的可感知问题，例如Precision@k和mAP@k。具体而言，当一个训练良好的多标签分类器在某些样本上的表现远低于预期时，受害者可以很容易地意识到这种性能退化源于攻击，而不是模型本身。因此，理想的多标签对抗性攻击应该能够欺骗视觉感知，并且避开度量的监测。为此，本文首先引入了度量不可察觉性的概念。然后，设计了一种新的损失函数来生成这种既能够实现视觉不可察觉性又能够避开度量监测的对抗性扰动。

    With the great success of deep neural networks, adversarial learning has received widespread attention in various studies, ranging from multi-class learning to multi-label learning. However, existing adversarial attacks toward multi-label learning only pursue the traditional visual imperceptibility but ignore the new perceptible problem coming from measures such as Precision@$k$ and mAP@$k$. Specifically, when a well-trained multi-label classifier performs far below the expectation on some samples, the victim can easily realize that this performance degeneration stems from attack, rather than the model itself. Therefore, an ideal multi-labeling adversarial attack should manage to not only deceive visual perception but also evade monitoring of measures. To this end, this paper first proposes the concept of measure imperceptibility. Then, a novel loss function is devised to generate such adversarial perturbations that could achieve both visual and measure imperceptibility. Furthermore, a
    
[^24]: StratMed：面向低资源药物推荐的相关性分层方法

    StratMed: Relevance Stratification for Low-resource Medication Recommendation. (arXiv:2308.16781v1 [cs.AI])

    [http://arxiv.org/abs/2308.16781](http://arxiv.org/abs/2308.16781)

    StratMed是一种面向低资源药物推荐的模型，通过相关性分层机制来解决医疗数据长尾分布不平衡的问题，平衡了药物组合的安全性和准确性。

    

    随着有限医疗资源与日益增长的需求之间的失衡，基于人工智能的临床任务变得至关重要。作为一个子领域，药物推荐旨在将患者的纵向历史与医学知识相结合，帮助医生更安全、更准确地开具药物组合处方。现有方法忽视了医疗数据中固有的长尾分布，缺乏头尾数据之间的平衡表示，导致模型性能次优。为了解决这个挑战，我们引入了StratMed，这是一个结合了创新的相关性分层机制的模型。它通过协调数据长尾分布中的差异，并在药物组合的安全性和准确性之间取得平衡。具体而言，我们首先使用深度学习网络构建预训练方法来获取实体表示。然后，我们设计了一个类似金字塔的数据分层方法，以获得更通用的实体表示。

    With the growing imbalance between limited medical resources and escalating demands, AI-based clinical tasks have become paramount. Medication recommendation, as a sub-domain, aims to amalgamate longitudinal patient history with medical knowledge, assisting physicians in prescribing safer and more accurate medication combinations. Existing methods overlook the inherent long-tail distribution in medical data, lacking balanced representation between head and tail data, which leads to sub-optimal model performance. To address this challenge, we introduce StratMed, a model that incorporates an innovative relevance stratification mechanism. It harmonizes discrepancies in data long-tail distribution and strikes a balance between the safety and accuracy of medication combinations. Specifically, we first construct a pre-training method using deep learning networks to obtain entity representation. After that, we design a pyramid-like data stratification method to obtain more generalized entity 
    
[^25]: 任何人都可以攻击：将有损压缩重新用作自然后门攻击

    Everyone Can Attack: Repurpose Lossy Compression as a Natural Backdoor Attack. (arXiv:2308.16684v1 [cs.CR])

    [http://arxiv.org/abs/2308.16684](http://arxiv.org/abs/2308.16684)

    本文发现了一种更严重的后门攻击威胁，即任何人都可以利用易获取的有损压缩算法进行自然后门攻击，无需设计特定触发器或进行繁琐调试。

    

    最近，后门攻击对实际应用中的机器学习模型的可信度构成了威胁。传统智慧认为，并不是每个人都可以成为攻击者，因为设计触发器生成算法的过程通常需要大量的努力和广泛的实验来确保攻击的隐秘性和有效性。然而，本文指出存在一种更为严重的后门威胁：任何人都可以利用易获取的算法进行隐悄后门攻击。具体来说，攻击者可以利用各种压缩工具中广泛使用的有损图片压缩技术，无需留下任何明显的痕迹就能轻松地将触发器模式注入到图像中，即生成的触发器是自然的图像伪影。使用有损图片压缩工具时，人们并不需要广泛知识，只需点击“转换”或“另存为”按钮即可。通过这种攻击，攻击者无需设计一个专门的触发器或进行繁琐的调试。

    The vulnerabilities to backdoor attacks have recently threatened the trustworthiness of machine learning models in practical applications. Conventional wisdom suggests that not everyone can be an attacker since the process of designing the trigger generation algorithm often involves significant effort and extensive experimentation to ensure the attack's stealthiness and effectiveness. Alternatively, this paper shows that there exists a more severe backdoor threat: anyone can exploit an easily-accessible algorithm for silent backdoor attacks. Specifically, this attacker can employ the widely-used lossy image compression from a plethora of compression tools to effortlessly inject a trigger pattern into an image without leaving any noticeable trace; i.e., the generated triggers are natural artifacts. One does not require extensive knowledge to click on the "convert" or "save as" button while using tools for lossy image compression. Via this attack, the adversary does not need to design a 
    
[^26]: 通过合作专家实现长尾图分类的研究

    Towards Long-Tailed Recognition for Graph Classification via Collaborative Experts. (arXiv:2308.16609v1 [cs.LG])

    [http://arxiv.org/abs/2308.16609](http://arxiv.org/abs/2308.16609)

    本文提出了一种新颖的方法，通过合作专家实现了长尾图分类，解决了现有方法在处理图数据上的不足。

    

    图分类旨在学习用于有效类别分配的图级表示，在平衡的类别分布的高质量数据集的支持下取得了杰出成果。事实上，大多数现实世界的图数据自然呈现长尾形式，其中头部类别的样本数量远超过尾部类别，因此在长尾数据上研究图级分类是至关重要的，但仍然较少探索。然而，现有的视觉中的长尾学习方法大多无法同时优化表示学习和分类器训练，并且忽略了难以分类的类别的挖掘。直接将现有方法应用于图可能导致次优性能，因为在图上训练的模型由于复杂的拓扑特征会更加敏感于长尾分布。因此，在本文中，我们提出了一种新颖的对长尾图级分类的方法

    Graph classification, aiming at learning the graph-level representations for effective class assignments, has received outstanding achievements, which heavily relies on high-quality datasets that have balanced class distribution. In fact, most real-world graph data naturally presents a long-tailed form, where the head classes occupy much more samples than the tail classes, it thus is essential to study the graph-level classification over long-tailed data while still remaining largely unexplored. However, most existing long-tailed learning methods in visions fail to jointly optimize the representation learning and classifier training, as well as neglect the mining of the hard-to-classify classes. Directly applying existing methods to graphs may lead to sub-optimal performance, since the model trained on graphs would be more sensitive to the long-tailed distribution due to the complex topological characteristics. Hence, in this paper, we propose a novel long-tailed graph-level classifica
    
[^27]: 潜在画家

    Latent Painter. (arXiv:2308.16490v1 [cs.CV])

    [http://arxiv.org/abs/2308.16490](http://arxiv.org/abs/2308.16490)

    这个论文介绍了一种名为潜在画家的技术，它利用潜在作为画布和扩散器的预测作为计划来生成绘画动画，同时还可以在不同的检查点集中转换图像。

    

    潜在扩散器在生成AI领域引起了革命，并激发了创造性艺术。在去噪潜在时，每个步骤预测的原始图像共同形成了动画。然而，动画受到扩散器去噪特性的限制，只呈现了一个锐化过程。本文介绍了潜在画家，它以潜在作为画布，以扩散器的预测作为计划，生成绘画动画。潜在画家还可以将一个生成的图像转换为另一个图像，这可以发生在两个不同检查点集中的图像之间。

    Latent diffusers revolutionized the generative AI and inspired creative art. When denoising the latent, the predicted original image at each step collectively animates the formation. However, the animation is limited by the denoising nature of the diffuser, and only renders a sharpening process. This work presents Latent Painter, which uses the latent as the canvas, and the diffuser predictions as the plan, to generate painting animation. Latent Painter also transits one generated image to another, which can happen between images from two different sets of checkpoints.
    
[^28]: 使用元学习进行点云上采样的测试时间适应

    Test-Time Adaptation for Point Cloud Upsampling Using Meta-Learning. (arXiv:2308.16484v1 [cs.CV])

    [http://arxiv.org/abs/2308.16484](http://arxiv.org/abs/2308.16484)

    本文提出了一种使用元学习进行测试时间适应的方法来增强点云上采样模型的普适性，解决了测试数据分布与训练数据不同导致性能下降的问题。

    

    廉价的3D扫描仪经常产生稀疏和非均匀的点云，这对机器人系统中的下游应用产生负面影响。虽然现有的点云上采样架构在标准基准数据上展示了有希望的结果，但当测试数据与训练数据具有不同分布时，它们往往会出现显著的性能下降。为了解决这个问题，本文提出了一种测试时间适应方法来增强点云上采样模型的普适性。所提出的方法利用元学习来显式地学习测试时间适应的网络参数。我们的方法不需要任何关于测试数据的先验信息。在元训练过程中，模型参数是从训练数据的稀疏-密集点云对的集合中学习的。在元测试过程中，经过少量梯度更新的训练模型可以产生一组唯一的网络参数。

    Affordable 3D scanners often produce sparse and non-uniform point clouds that negatively impact downstream applications in robotic systems. While existing point cloud upsampling architectures have demonstrated promising results on standard benchmarks, they tend to experience significant performance drops when the test data have different distributions from the training data. To address this issue, this paper proposes a test-time adaption approach to enhance model generality of point cloud upsampling. The proposed approach leverages meta-learning to explicitly learn network parameters for test-time adaption. Our method does not require any prior information about the test data. During meta-training, the model parameters are learned from a collection of instance-level tasks, each of which consists of a sparse-dense pair of point clouds from the training data. During meta-testing, the trained model is fine-tuned with a few gradient updates to produce a unique set of network parameters for
    
[^29]: Point-TTA: 使用多任务元辅助学习的点云配准测试时间自适应

    Point-TTA: Test-Time Adaptation for Point Cloud Registration Using Multitask Meta-Auxiliary Learning. (arXiv:2308.16481v1 [cs.CV])

    [http://arxiv.org/abs/2308.16481](http://arxiv.org/abs/2308.16481)

    Point-TTA是一种通过多任务元辅助学习实现的点云配准测试时自适应框架，能够提高配准模型的泛化性能。

    

    我们提出了Point-TTA，这是一种新颖的点云配准测试时间自适应框架，可以提高配准模型的泛化性能。虽然基于学习的方法取得了令人印象深刻的进展，但面对未知的测试环境的泛化仍然是一个重大挑战，原因是3D扫描的变化较大。现有方法通常训练一个通用模型，并在每个实例上应用相同的训练模型。这可能是次优的，因为同一模型很难处理测试期间的所有变化。在本文中，我们提出了一种用于点云配准的测试时间自适应方法。我们的模型可以适应测试时未知的分布，无需任何关于测试数据的先验知识。具体地，我们设计了三个自监督辅助任务，这些任务与主要的配准任务一起进行优化。给定一个测试实例，我们使用这些辅助任务来调整我们的模型，并使用更新后的模型进行推断。

    We present Point-TTA, a novel test-time adaptation framework for point cloud registration (PCR) that improves the generalization and the performance of registration models. While learning-based approaches have achieved impressive progress, generalization to unknown testing environments remains a major challenge due to the variations in 3D scans. Existing methods typically train a generic model and the same trained model is applied on each instance during testing. This could be sub-optimal since it is difficult for the same model to handle all the variations during testing. In this paper, we propose a test-time adaptation approach for PCR. Our model can adapt to unseen distributions at test-time without requiring any prior knowledge of the test data. Concretely, we design three self-supervised auxiliary tasks that are optimized jointly with the primary PCR task. Given a test instance, we adapt our model using these auxiliary tasks and the updated model is used to perform the inference. 
    
[^30]: BioCoder: 一种带有上下文语用知识的生物信息学代码生成基准

    BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge. (arXiv:2308.16458v1 [cs.LG])

    [http://arxiv.org/abs/2308.16458](http://arxiv.org/abs/2308.16458)

    BioCoder是一个用于评估预训练模型在生成生物信息学代码方面的基准，涵盖了函数代码生成中的包依赖关系、类声明和全局变量，并通过模糊测试框架进行评估。

    

    预训练的语言模型（如ChatGPT）显著改进了代码生成。随着这些模型的扩大，需要输出来处理更复杂的任务的需求也越来越多。此外，在生物信息学中，生成功能程序由于领域知识量大、需要复杂的数据操作和复杂的功能依赖关系而面临额外的挑战。在这里，我们介绍了BioCoder，这是一个用于评估现有预训练模型在生成生物信息学代码方面的基准。与函数代码生成有关，BioCoder涵盖了可能的包依赖关系、类声明和全局变量。它包括来自GitHub的1026个Python和Java函数和1243个方法，以及来自Rosalind项目的253个示例。BioCoder还结合了一个用于评估的模糊测试框架，我们已经应用它来评估许多模型，包括InCoder、CodeGen、CodeGen2、SantaCoder、StarCoder、StarCoder+、InstructCodeT。

    Pre-trained language models like ChatGPT have significantly improved code generation. As these models scale up, there is an increasing need for the output to handle more intricate tasks. Moreover, in bioinformatics, generating functional programs poses additional notable challenges due to the amount of domain knowledge, the need for complicated data operations, and intricate functional dependencies between the operations. Here, we present BioCoder, a benchmark developed to evaluate existing pre-trained models in generating bioinformatics code. In relation to function-code generation, BioCoder covers potential package dependencies, class declarations, and global variables. It incorporates 1026 functions and 1243 methods in Python and Java from GitHub and 253 examples from the Rosalind Project. BioCoder incorporates a fuzz-testing framework for evaluation, and we have applied it to evaluate many models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT
    
[^31]: 听取少数群体的声音：基于对比预训练的不平衡类别加密流量分类

    Listen to Minority: Encrypted Traffic Classification for Class Imbalance with Contrastive Pre-Training. (arXiv:2308.16453v1 [cs.CR])

    [http://arxiv.org/abs/2308.16453](http://arxiv.org/abs/2308.16453)

    本文提出了一种基于对比预训练的不平衡类别加密流量分类框架PASS，通过重新采样训练数据集并进行对比性预训练，避免了标签偏见和流量均匀性等问题，为加密流量分类提供了稳健的特征表示。

    

    移动互联网在各个方面深刻地改变了现代生活方式。加密流量分类在管理移动互联网中起到了至关重要的作用，特别是在移动应用程序使用加密通信的爆炸性增长的情况下。尽管一些现有的基于学习的加密流量分类方法显示出了有希望的结果，但在真实网络环境中仍然存在三个限制：1）由流量类别不平衡引起的标签偏见，2）由组件共享引起的流量均匀性，以及3）依赖充足标记流量进行训练。没有任何现有的加密流量分类方法能够解决所有这些限制。本文提出了一种新颖的基于预训练的半监督加密流量分类框架，称为PASS。我们的关键见解是重新采样原始的训练数据集，并进行对比性预训练，而不直接使用个体应用程序标签，以避免由于类别不平衡引起的标签偏见问题，同时获得稳健的特征表示来区分重叠的流量。

    Mobile Internet has profoundly reshaped modern lifestyles in various aspects. Encrypted Traffic Classification (ETC) naturally plays a crucial role in managing mobile Internet, especially with the explosive growth of mobile apps using encrypted communication. Despite some existing learning-based ETC methods showing promising results, three-fold limitations still remain in real-world network environments, 1) label bias caused by traffic class imbalance, 2) traffic homogeneity caused by component sharing, and 3) training with reliance on sufficient labeled traffic. None of the existing ETC methods can address all these limitations. In this paper, we propose a novel Pre-trAining Semi-Supervised ETC framework, dubbed PASS. Our key insight is to resample the original train dataset and perform contrastive pre-training without using individual app labels directly to avoid label bias issues caused by class imbalance, while obtaining a robust feature representation to differentiate overlapping 
    
[^32]: 在图嵌入中平衡局部和全局结构（LGS）的方法

    Balancing between the Local and Global Structures (LGS) in Graph Embedding. (arXiv:2308.16403v1 [cs.HC])

    [http://arxiv.org/abs/2308.16403](http://arxiv.org/abs/2308.16403)

    本论文提出了一种在图嵌入中平衡局部和全局结构的方法，通过可调参数进行调节。我们的结果表明，该方法在合成和真实世界的数据集上与最先进的方法相竞争，并引入了一种新颖的质量指标，聚类距离保持，用于评估嵌入的质量。

    

    我们提出了一种通过可调参数在图嵌入中平衡局部和全局结构（LGS）的方法。一些嵌入方法旨在捕捉全局结构，而其他方法则试图保留局部邻域。很少有方法尝试同时做到这两点，并且在二维空间中很难很好地捕捉到局部和全局信息，而大部分图的绘制都是在这个空间中进行的。根据任务和底层数据的结构选择使用局部还是全局嵌入来进行可视化不仅取决于任务本身，还取决于底层数据的结构，而这是事先不知道的。对于给定的图，LGS旨在找到一个良好的平衡，以保留局部和全局结构。我们用合成和真实世界的数据集评估了LGS的性能，并且我们的结果表明，它在使用了诸如压力和邻域保持等已建立的质量指标时与最先进的方法相竞争。我们引入了一种新颖的质量指标，聚类距离保持，用于评估嵌入的质量。

    We present a method for balancing between the Local and Global Structures (LGS) in graph embedding, via a tunable parameter. Some embedding methods aim to capture global structures, while others attempt to preserve local neighborhoods. Few methods attempt to do both, and it is not always possible to capture well both local and global information in two dimensions, which is where most graph drawing live. The choice of using a local or a global embedding for visualization depends not only on the task but also on the structure of the underlying data, which may not be known in advance. For a given graph, LGS aims to find a good balance between the local and global structure to preserve. We evaluate the performance of LGS with synthetic and real-world datasets and our results indicate that it is competitive with the state-of-the-art methods, using established quality metrics such as stress and neighborhood preservation. We introduce a novel quality metric, cluster distance preservation, to 
    
[^33]: 深度视频编码控制

    Deep Video Codec Control. (arXiv:2308.16215v1 [eess.IV])

    [http://arxiv.org/abs/2308.16215](http://arxiv.org/abs/2308.16215)

    本文提出了第一个端到端可学习的深度视频编码控制方法，同时考虑了带宽限制和下游视觉性能，并在不破坏现有标准化的情况下实现了保护深度视觉模型的目标。

    

    丢失率视频压缩通常用于传输和存储视频数据。尽管存在进阶（神经）压缩方法，但统一视频编码器（如H.264或H.265）仍然是事实上的标准。在面对动态网络带宽条件的视频传输中，视频编码器需要适应非常不同的压缩强度。速率控制模块增强编解码器的压缩能力，以满足带宽限制并尽量减少视频失真。然而，标准视频编码器及其速率控制模块是为了最小化人类质量评估而开发的，却没有考虑保护深度视觉模型的下游性能。在本文中，我们提出了第一个端到端可学习的深度视频编码控制方法，考虑了带宽限制和下游视觉性能，并不破坏现有的标准化。我们针对两个常见的视觉任务（语义分割...

    Lossy video compression is commonly used when transmitting and storing video data. Unified video codecs (e.g., H.264 or H.265) remain the \emph{de facto} standard, despite the availability of advanced (neural) compression approaches. Transmitting videos in the face of dynamic network bandwidth conditions requires video codecs to adapt to vastly different compression strengths. Rate control modules augment the codec's compression such that bandwidth constraints are satisfied and video distortion is minimized. While, both standard video codes and their rate control modules are developed to minimize video distortion w.r.t. human quality assessment, preserving the downstream performance of deep vision models is not considered. In this paper, we present the first end-to-end learnable deep video codec control considering both bandwidth constraints and downstream vision performance, while not breaking existing standardization. We demonstrate for two common vision tasks (semantic segmentation 
    
[^34]: MDTD: 一种用于深度神经网络的多领域木马检测器

    MDTD: A Multi Domain Trojan Detector for Deep Neural Networks. (arXiv:2308.15673v1 [cs.CR])

    [http://arxiv.org/abs/2308.15673](http://arxiv.org/abs/2308.15673)

    MDTD是一种多领域木马检测器，用于在测试时检测深度神经网络中包含木马触发器的输入。MDTD不需要知道触发器嵌入策略，并且适用于不同类型的输入。它利用了输入样本与决策边界的距离来检测木马触发器。

    

    使用深度神经网络(DNN)的机器学习模型容易受到后门攻击。攻击者在一小部分输入样本中嵌入一个预定义的干扰物，称为触发器，并训练DNN，使得输入中存在触发器导致输出为攻击者所期望的类别。此类对抗性重训练需要确保没有触发器的输入输出不受影响，并对干净样本提供高分类准确率。本文提出了MDTD，一种针对DNN的多领域木马检测器，用于在测试时检测包含木马触发器的输入。MDTD不需要知道攻击者嵌入触发器的策略，并且可以应用于预训练的DNN模型，包括图像、音频和基于图的输入。MDTD利用了一个洞察力，即包含木马触发器的输入样本相对于干净样本位于决策边界之外较远的位置。MDTD估计了距离。

    Machine learning models that use deep neural networks (DNNs) are vulnerable to backdoor attacks. An adversary carrying out a backdoor attack embeds a predefined perturbation called a trigger into a small subset of input samples and trains the DNN such that the presence of the trigger in the input results in an adversary-desired output class. Such adversarial retraining however needs to ensure that outputs for inputs without the trigger remain unaffected and provide high classification accuracy on clean samples. In this paper, we propose MDTD, a Multi-Domain Trojan Detector for DNNs, which detects inputs containing a Trojan trigger at testing time. MDTD does not require knowledge of trigger-embedding strategy of the attacker and can be applied to a pre-trained DNN model with image, audio, or graph-based inputs. MDTD leverages an insight that input samples containing a Trojan trigger are located relatively farther away from a decision boundary than clean samples. MDTD estimates the dista
    
[^35]: 测量篡改检测基准

    Measurement Tampering Detection Benchmark. (arXiv:2308.15605v1 [cs.LG])

    [http://arxiv.org/abs/2308.15605](http://arxiv.org/abs/2308.15605)

    本文构建了四个文本数据集用于评估测量篡改检测技术，研究人工智能系统操纵测量结果以营造良好结果的问题。虽然展示了优于基准的技术，但还有很大的改进空间。

    

    在训练强大的人工智能系统来执行复杂任务时，提供对优化具有稳健性的训练信号可能是具有挑战性的。一个问题是测量篡改，即人工智能系统操纵多个测量结果，以营造良好结果的假象，而不是实现期望的结果。在这项工作中，我们构建了四个新的基于文本的数据集，用于评估大规模语言模型上的测量篡改检测技术。具体来说，给定一组文本输入和测量结果，旨在确定某个结果是否发生，以及一个能够准确预测测量结果的基础模型，目标是确定所有测量结果都表明结果发生的示例是否确实发生了结果，或者这是由于测量篡改引起的。我们展示了在大多数数据集上优于简单基准的技术，但没有达到最佳性能。我们相信在技术和数据集方面都有很大的改进空间，我们感到兴奋。

    When training powerful AI systems to perform complex tasks, it may be challenging to provide training signals which are robust to optimization. One concern is measurement tampering, where the AI system manipulates multiple measurements to create the illusion of good results instead of achieving the desired outcome. In this work, we build four new text-based datasets to evaluate measurement tampering detection techniques on large language models. Concretely, given sets of text inputs and measurements aimed at determining if some outcome occurred, as well as a base model able to accurately predict measurements, the goal is to determine if examples where all measurements indicate the outcome actually had the outcome occur, or if this was caused by measurement tampering. We demonstrate techniques that outperform simple baselines on most datasets, but don't achieve maximum performance. We believe there is significant room for improvement for both techniques and datasets, and we are excited 
    
[^36]: OEBench: 研究现实世界中关系数据流中的开放环境挑战

    OEBench: Investigating Open Environment Challenges in Real-World Relational Data Streams. (arXiv:2308.15059v1 [cs.LG])

    [http://arxiv.org/abs/2308.15059](http://arxiv.org/abs/2308.15059)

    OEBench是一个用于评估关系数据流中开放环境挑战的开放环境基准，研究发现这种挑战在真实世界数据集中普遍存在。

    

    关系数据集在现实世界中非常普遍，并且通常以数据流的方式传递。这种类型的数据流可能存在一些特殊的挑战，例如分布漂移、异常值、新兴类别和特征变化，最近将这些挑战描述为机器学习中的开放环境挑战。虽然已经有一些关于数据流的增量学习的研究，但其评估主要是基于手动分割的数据集。此外，虽然有几个现实世界的数据流数据集可用，但这些开放环境挑战是否普遍存在以及现有的增量学习算法在真实数据集上的表现如何尚不确定。为了填补这个空白，我们开发了一个名为OEBench的开放环境基准，用于评估关系数据流中的开放环境挑战。具体而言，我们研究了55个真实世界的数据流数据集，并确立了开放环境场景在真实数据集中的普遍存在性，这在当前的研究中还没有被确定。

    Relational datasets are widespread in real-world scenarios and are usually delivered in a streaming fashion. This type of data stream can present unique challenges, such as distribution drifts, outliers, emerging classes, and changing features, which have recently been described as open environment challenges for machine learning. While some work has been done on incremental learning for data streams, their evaluations are mostly conducted with manually partitioned datasets. Moreover, while several real-world streaming datasets are available, it is uncertain whether these open environment challenges are prevalent and how existing incremental learning algorithms perform on real datasets. To fill this gap, we develop an Open Environment Benchmark named OEBench to evaluate open environment challenges in relational data streams. Specifically, we investigate 55 real-world streaming datasets and establish that open environment scenarios are indeed widespread in real-world datasets, which pre
    
[^37]: 冲突感知的主动有限状态机学习

    Conflict-Aware Active Automata Learning. (arXiv:2308.14781v1 [cs.LG])

    [http://arxiv.org/abs/2308.14781](http://arxiv.org/abs/2308.14781)

    C3AL是一种冲突感知的主动有限状态机学习框架，能够处理观测数据中的冲突，通过将观测树作为学习过程的一等公民并最小化测试次数，具有很好的效果。

    

    主动有限状态机学习算法在处理观测数据中的冲突（同一输入对应不同输出）方面存在困难。这种固有的冲突恢复能力不足，影响了它们在存在噪声或学习中的系统变化场景中的有效应用。我们提出了冲突感知的主动有限状态机学习（C3AL）框架，以在学习过程中处理冲突信息。核心思想是将所谓的观测树视为学习过程的一等公民。尽管这个想法在最近的研究中得到了探索，但我们通过将其与任何现有的学习算法结合，并在面对冲突时最小化对正在学习的系统执行的测试次数，充分发挥了它的作用。我们在大量的基准测试中评估了C3AL，涵盖了30多个不同的真实目标和18,000多个不同的场景。评估结果表明，C3AL是一个合适的替代方法。

    Active automata learning algorithms cannot easily handle \emph{conflict} in the observation data (different outputs observed for the same inputs). This inherent inability to recover after a conflict impairs their effective applicability in scenarios where noise is present or the system under learning is mutating.  We propose the Conflict-Aware Active Automata Learning (C3AL) framework to enable handling conflicting information during the learning process. The core idea is to consider the so-called observation tree as a first-class citizen in the learning process. Though this idea is explored in recent work, we take it to its full effect by enabling its use with any existing learner and minimizing the number of tests performed on the system under learning, specially in the face of conflicts. We evaluate C3AL in a large set of benchmarks, covering over 30 different realistic targets, and over 18,000 different scenarios. The results of the evaluation show that C3AL is a suitable alternati
    
[^38]: RESTORE: 通过重建进行图嵌入评估

    RESTORE: Graph Embedding Assessment Through Reconstruction. (arXiv:2308.14659v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.14659](http://arxiv.org/abs/2308.14659)

    RESTORE是一个用于通过图重建进行内在GEs评估的框架，可以揭示在给定向量形式中保留的信息量。

    

    在Word2Vec嵌入取得成功之后，图嵌入（GEs）开始受到广泛关注。GEs常常在下游应用中生成和评估外显性，但关于原始图的拓扑结构和语义信息的内在评估一直缺乏。了解这些将有助于确定各种GE方法在向量化图时丧失相关知识或学习错误知识的不足之处。因此，我们提出了RESTORE，一个用于通过图重建进行内在GEs评估的框架。我们展示了从底层GEs中重建原始图可以揭示在给定向量形式中保留的信息量。我们首先介绍了图重建任务。我们根据分解方法、随机行走和深度学习从三个GE家族生成GEs（每个家族选择了代表性算法）。

    Following the success of Word2Vec embeddings, graph embeddings (GEs) have gained substantial traction. GEs are commonly generated and evaluated extrinsically on downstream applications, but intrinsic evaluations of the original graph properties in terms of topological structure and semantic information have been lacking. Understanding these will help identify the deficiency of the various families of GE methods when vectorizing graphs in terms of preserving the relevant knowledge or learning incorrect knowledge. To address this, we propose RESTORE, a framework for intrinsic GEs assessment through graph reconstruction. We show that reconstructing the original graph from the underlying GEs yields insights into the relative amount of information preserved in a given vector form. We first introduce the graph reconstruction task. We generate GEs from three GE families based on factorization methods, random walks, and deep learning (with representative algorithms from each family) on the Com
    
[^39]: 通过跨模型一致性进行标签去噪

    Label Denoising through Cross-Model Agreement. (arXiv:2308.13976v1 [cs.LG])

    [http://arxiv.org/abs/2308.13976](http://arxiv.org/abs/2308.13976)

    本文提出了一种通过跨模型一致性进行标签去噪的方法。通过观察发现，不同模型在干净示例上的预测相对相似，而在有噪声示例上的预测在不同模型之间变化更大。在这种观察的启发下，我们提出了使用跨模型一致性进行去噪的方法（DeCA），旨在最小化两个机器学习模型参数化的真实标签分布之间的KL散度，同时最大化数据观测的似然。

    

    在现实世界的机器学习应用中，从有噪声的标签学习是非常常见的。记忆这些有噪声的标签可能会影响模型的学习，从而导致次优的性能。在这项工作中，我们提出了一种新颖的框架，用于从有噪声标签中学习鲁棒的机器学习模型。通过实证研究，我们发现不同模型在干净示例上的预测相对相似，而在有噪声示例上的预测在不同模型之间变化更大。受到这一观察的启发，我们提出了使用跨模型一致性进行去噪（DeCA）的方法，该方法旨在最小化由两个机器学习模型参数化的真实标签分布之间的KL散度，同时最大化数据观测的似然。我们将提出的DeCA方法应用于二进制标签情景和多标签情景。对于二进制标签情景，我们选择隐式反馈推荐作为下游任务，并进行了四种最先进方法的实验。

    Learning from corrupted labels is very common in real-world machine-learning applications. Memorizing such noisy labels could affect the learning of the model, leading to sub-optimal performances. In this work, we propose a novel framework to learn robust machine-learning models from noisy labels. Through an empirical study, we find that different models make relatively similar predictions on clean examples, while the predictions on noisy examples vary much more across different models. Motivated by this observation, we propose \em denoising with cross-model agreement \em (DeCA) which aims to minimize the KL-divergence between the true label distributions parameterized by two machine learning models while maximizing the likelihood of data observation. We employ the proposed DeCA on both the binary label scenario and the multiple label scenario. For the binary label scenario, we select implicit feedback recommendation as the downstream task and conduct experiments with four state-of-the
    
[^40]: 工业人工智能中的随机配置机

    Stochastic Configuration Machines for Industrial Artificial Intelligence. (arXiv:2308.13570v1 [cs.LG])

    [http://arxiv.org/abs/2308.13570](http://arxiv.org/abs/2308.13570)

    本文提出了一种新颖的随机学习器模型，称为随机配置机（SCMs），其基于随机配置网络（SCNs），旨在强调工业人工智能中的有效建模和节约数据大小。SCMs通过压缩模型存储，并保持有利的预测性能，具有在工业应用中很大的潜力。

    

    在工业人工智能（IAI）中，需要实时、准确的预测建模，神经网络在其中起到关键作用。工业人工智能中的神经网络需要强大的高性能计算设备来处理大量的浮点数据。本文基于随机配置网络（SCNs），提出了一种新的随机学习器模型，称为随机配置机（SCMs），以强调对于工业应用非常有用和有价值的有效建模和节约数据大小。与具有二值化实现的随机向量功能链接（RVFL）网络相比，SCMs的模型存储可以显著压缩，同时保持有利的预测性能。除了SCM学习器模型的架构和学习算法，作为本文的重要部分，我们还通过分析模型的复杂性提供了SCMs的学习能力的理论基础。实验研究也进行了。

    Real-time predictive modelling with desired accuracy is highly expected in industrial artificial intelligence (IAI), where neural networks play a key role. Neural networks in IAI require powerful, high-performance computing devices to operate a large number of floating point data. Based on stochastic configuration networks (SCNs), this paper proposes a new randomized learner model, termed stochastic configuration machines (SCMs), to stress effective modelling and data size saving that are useful and valuable for industrial applications. Compared to SCNs and random vector functional-link (RVFL) nets with binarized implementation, the model storage of SCMs can be significantly compressed while retaining favourable prediction performance. Besides the architecture of the SCM learner model and its learning algorithm, as an important part of this contribution, we also provide a theoretical basis on the learning capacity of SCMs by analysing the model's complexity. Experimental studies are ca
    
[^41]: 基于深度强化学习的跨社区能量交互优化调度

    Deep Reinforcement Learning-driven Cross-Community Energy Interaction Optimal Scheduling. (arXiv:2308.12554v1 [eess.SY])

    [http://arxiv.org/abs/2308.12554](http://arxiv.org/abs/2308.12554)

    本文提出了一种基于深度强化学习的跨社区能量交互优化调度模型，通过学习不同社区的负载特性，并基于该知识做出决策，实现综合能量系统的整体优化和调度。

    

    为了在不确定的条件下协调各个社区之间的能量交互和多能源子系统之间的能量转换，并实现综合能量系统的整体优化和调度，本文提出了一种综合调度模型，利用多智能体深度强化学习算法学习不同社区的负载特性，并基于该知识做出决策。在该模型中，综合能量系统的调度问题被转化为马尔科夫决策过程，并使用数据驱动的深度强化学习算法解决，从而避免了对多社区和多能源子系统之间复杂能量耦合关系的建模需求。模拟结果表明，所提出的方法能够有效捕捉不同社区的负载特性，并利用其互补特性进行协调。

    In order to coordinate energy interactions among various communities and energy conversions among multi-energy subsystems within the multi-community integrated energy system under uncertain conditions, and achieve overall optimization and scheduling of the comprehensive energy system, this paper proposes a comprehensive scheduling model that utilizes a multi-agent deep reinforcement learning algorithm to learn load characteristics of different communities and make decisions based on this knowledge. In this model, the scheduling problem of the integrated energy system is transformed into a Markov decision process and solved using a data-driven deep reinforcement learning algorithm, which avoids the need for modeling complex energy coupling relationships between multi-communities and multi-energy subsystems. The simulation results show that the proposed method effectively captures the load characteristics of different communities and utilizes their complementary features to coordinate re
    
[^42]: MKL-$L_{0/1}$-SVM: 一种多核学习的支持向量机框架

    MKL-$L_{0/1}$-SVM. (arXiv:2308.12016v1 [stat.ML])

    [http://arxiv.org/abs/2308.12016](http://arxiv.org/abs/2308.12016)

    本文提出了一种多核学习的支持向量机框架(MKL-$L_{0/1}$-SVM)，通过开发快速的ADMM求解器处理非凸非光滑的优化问题，并在实验中展示了与领先方法相当的性能。

    

    本文提出了一种适用于$(0, 1)$损失函数的支持向量机的多核学习（MKL）框架。首先给出了一阶最优性条件，然后利用它们开发了一个快速的ADMM求解器来处理非凸非光滑的优化问题。详细的合成和真实数据集上的实验表明，我们的MKL-$L_{0/1}$-SVM的性能与一种名为SimpleMKL的领先方法相当。

    This paper presents a Multiple Kernel Learning (abbreviated as MKL) framework for the Support Vector Machine (SVM) with the $(0, 1)$ loss function. Some first-order optimality conditions are given and then exploited to develop a fast ADMM solver to deal with the nonconvex and nonsmooth optimization problem. Extensive numerical experiments on synthetic and real datasets show that the performance of our MKL-$L_{0/1}$-SVM is comparable with the one of the leading approaches called SimpleMKL developed by Rakotomamonjy, Bach, Canu, and Grandvalet [Journal of Machine Learning Research, vol. 9, pp. 2491-2521, 2008].
    
[^43]: 有关在有限预算二臂赌博机中进行最佳臂选择的统一最优算法研究

    On Uniformly Optimal Algorithms for Best Arm Identification in Two-Armed Bandits with Fixed Budget. (arXiv:2308.12000v1 [stat.ML])

    [http://arxiv.org/abs/2308.12000](http://arxiv.org/abs/2308.12000)

    本文研究了在有限预算的随机二臂赌博机中进行最佳臂选择的问题，并证明不存在比等概率采样算法更好的算法。我们引入了一致稳定算法的概念，并证明任何在所有情况下与等概率采样算法表现一样好的算法必须属于这个类别。这一结果解决了之前的两个未解之谜。

    

    本文研究了在具有伯努利奖励的随机二臂赌博机中，使用有限预算进行最佳臂选择的问题。我们证明令人惊讶的是，不存在一个算法可以在所有情况下与等概率采样算法表现一样好（该算法被称为“均匀采样”算法），并且在至少一个情况下明显优于该算法。简而言之，不存在比均匀采样算法更好的算法。为了证明这一结果，我们引入了“一致”和“稳定”算法的自然类，并且证明了任何算法要在所有情况下与均匀采样算法表现一样好，必须属于这个类别。通过导出满足任何一致且稳定算法的错误率的下界，并证明均匀采样算法与此下界相匹配，我们完成了证明过程。我们的结果解决了\cite{qin2022open}中提出的两个未解之谜。

    We study the problem of best-arm identification with fixed budget in stochastic two-arm bandits with Bernoulli rewards. We prove that surprisingly, there is no algorithm that (i) performs as well as the algorithm sampling each arm equally (this algorithm is referred to as the {\it uniform sampling} algorithm) on all instances, and that (ii) strictly outperforms this algorithm on at least one instance. In short, there is no algorithm better than the uniform sampling algorithm. Towards this result, we introduce the natural class of {\it consistent} and {\it stable} algorithms, and show that any algorithm that performs as well as the uniform sampling algorithm on all instances belongs to this class. The proof is completed by deriving a lower bound on the error rate satisfied by any consistent and stable algorithm, and by showing that the uniform sampling algorithm matches this lower bound. Our results provide a solution to the two open problems presented in \cite{qin2022open}.
    
[^44]: "非混淆协变量对基于潜在结果框架的方法推断性能的影响研究"

    A Study on the Impact of Non-confounding Covariates on the Inferential Performance of Methods based on the Potential Outcome Framework. (arXiv:2308.11676v1 [stat.ME])

    [http://arxiv.org/abs/2308.11676](http://arxiv.org/abs/2308.11676)

    "本文研究了非混淆协变量对基于潜在结果框架的方法推断性能的影响，通过提供统一的图形框架来增强对这些模型基本原理的理解，为实际场景中应用这些模型带来了潜在价值。"

    

    "潜在结果框架（POF）在因果推断领域中起着重要作用。大多数基于POF的因果推断模型（CIMs-B-POF）旨在消除混淆偏差，并默认存在混淆协变量的基本假设。这一假设认为协变量仅由混淆变量组成。然而，在实践中保持混淆协变量的假设是具有挑战性的，特别是在处理高维协变量时。虽然已经提出了一些方法在进行因果推断之前区分协变量的不同组成部分，但将非混淆的协变量视为混淆变量的后果仍不清楚。这种不确定性在实际场景中应用CIMs-B-POF时存在潜在风险。在本文中，我们提出了一个统一的图形框架，用于理解CIMs-B-POF模型的基本原理。利用这个图形框架，我们对CIMs-B-POF的性能进行了量化分析。"

    The Potential Outcome Framework (POF) plays a prominent role in the field of causal inference. Most causal inference models based on the POF (CIMs-B-POF) are designed for eliminating confounding bias and default to an underlying assumption of Confounding Covariates. This assumption posits that the covariates consist solely of confounders. However, the assumption of Confounding Covariates is challenging to maintain in practice, particularly when dealing with high-dimensional covariates. While certain methods have been proposed to differentiate the distinct components of covariates prior to conducting causal inference, the consequences of treating non-confounding covariates as confounders remain unclear. This ambiguity poses a potential risk when applying the CIMs-B-POF in practical scenarios. In this paper, we present a unified graphical framework for the CIMs-B-POF, which greatly enhances the comprehension of these models' underlying principles. Using this graphical framework, we quant
    
[^45]: LLaMA-Reviewer: 通过参数高效微调推进大型语言模型在代码审查自动化中的应用（实证研究）

    LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning (Practical Experience Report). (arXiv:2308.11148v1 [cs.SE])

    [http://arxiv.org/abs/2308.11148](http://arxiv.org/abs/2308.11148)

    本文提出了LLaMA-Reviewer框架，通过参数高效微调方法，利用流行的大型语言模型LLaMA在代码审查领域能力，实现对代码审查任务的自动化。研究表明，即使仅使用不到1%的可训练参数，该框架仍能取得显著的成果。

    

    代码审查活动的自动化长期以来一直是软件工程领域的追求，主要通过许多领域特定的预训练模型来解决。尽管这些模型取得了一定的成功，但它们经常需要大量的资源从头开始进行预训练。相比之下，大型语言模型（LLMs）在补充领域特定知识的情况下展现出了令人着迷的潜力。然而，它们在自动化代码审查任务方面的潜力仍然很少被探索。为了填补这一研究空白，我们提出了LLaMA-Reviewer，这是一个创新的框架，它利用了流行的LLM——LLaMA在代码审查领域的能力。考虑到资源限制，该框架采用了参数高效微调（PEFT）方法，以极少的可训练参数提供高性能。我们对LLaMA-Reviewer进行了广泛的评估，使用了两个不同的公开数据集。值得注意的是，即使在只使用不到1%的可训练参数的情况下，它也取得了显著的成果。

    The automation of code review activities, a long-standing pursuit in software engineering, has been primarily addressed by numerous domain-specific pre-trained models. Despite their success, these models frequently demand extensive resources for pre-training from scratch. In contrast, Large Language Models (LLMs) provide an intriguing alternative, given their remarkable capabilities when supplemented with domain-specific knowledge. However, their potential for automating code review tasks remains largely unexplored.  In response to this research gap, we present LLaMA-Reviewer, an innovative framework that leverages the capabilities of LLaMA, a popular LLM, in the realm of code review. Mindful of resource constraints, this framework employs parameter-efficient fine-tuning (PEFT) methods, delivering high performance while using less than 1% of trainable parameters.  An extensive evaluation of LLaMA-Reviewer is conducted on two diverse, publicly available datasets. Notably, even with the 
    
[^46]: 图神经网络在推荐中的表达能力有多强？

    How Expressive are Graph Neural Networks in Recommendation?. (arXiv:2308.11127v1 [cs.IR])

    [http://arxiv.org/abs/2308.11127](http://arxiv.org/abs/2308.11127)

    本文对图神经网络在推荐中的表达能力进行了理论分析，发现现有的表达能力度量标准可能无法有效评估模型在推荐中的能力，提出了一个全面的理论分析方法。

    

    图神经网络（GNNs）在各种图学习任务中展示了优越的性能，包括利用图中的用户-物品协作过滤信号进行推荐。然而，尽管它们在最先进的推荐模型中的经验有效性，但对于它们的能力的理论表述非常稀少。最近的研究探讨了GNNs的一般表达能力，证明了消息传递GNNs至多与Weisfeiler-Lehman测试一样强大，并且与随机节点初始化相结合的GNNs是通用的。然而，GNNs的“表达能力”概念仍然定义模糊。大多数现有的工作采用图同构测试作为表达能力的度量标准，但这种图级任务可能不能有效评估模型在推荐中区分不同接近程度节点的能力。在本文中，我们对GNNs在推荐中的表达能力进行了全面的理论分析。

    Graph Neural Networks (GNNs) have demonstrated superior performance on various graph learning tasks, including recommendation, where they leverage user-item collaborative filtering signals in graphs. However, theoretical formulations of their capability are scarce, despite their empirical effectiveness in state-of-the-art recommender models. Recently, research has explored the expressiveness of GNNs in general, demonstrating that message passing GNNs are at most as powerful as the Weisfeiler-Lehman test, and that GNNs combined with random node initialization are universal. Nevertheless, the concept of "expressiveness" for GNNs remains vaguely defined. Most existing works adopt the graph isomorphism test as the metric of expressiveness, but this graph-level task may not effectively assess a model's ability in recommendation, where the objective is to distinguish nodes of different closeness. In this paper, we provide a comprehensive theoretical analysis of the expressiveness of GNNs in 
    
[^47]: 近似等变图网络

    Approximately Equivariant Graph Networks. (arXiv:2308.10436v1 [stat.ML])

    [http://arxiv.org/abs/2308.10436](http://arxiv.org/abs/2308.10436)

    本文关注于图神经网络（GNNs）的主动对称性，通过考虑信号在固定图上的学习设置，提出了一种近似的对称性概念，通过图粗化实现。这篇工作提出了一个偏差-方差公式来衡量近似对称性...

    

    图神经网络（GNNs）通常被描述为对图中的节点重新排序具有置换等变性。GNNs的这种对称性常被与欧几里得卷积神经网络（CNNs）的平移等变性比较。然而，这两种对称性本质上是不同的：CNNs的平移等变性对应于作用于图像信号的固定域的对称性（有时称为主动对称性），而在GNNs中，任何置换都作用于图信号和图域（有时描述为被动对称性）。在这项工作中，我们聚焦于GNNs的主动对称性，考虑信号在一个固定图上进行学习的情况。在这种情况下，GNNs的自然对称性是图的自同构。由于现实世界中的图往往是非对称的，我们通过形式化图粗化来放松对称性的概念，提出了一个偏差-方差公式来衡量...

    Graph neural networks (GNNs) are commonly described as being permutation equivariant with respect to node relabeling in the graph. This symmetry of GNNs is often compared to the translation equivariance symmetry of Euclidean convolution neural networks (CNNs). However, these two symmetries are fundamentally different: The translation equivariance of CNNs corresponds to symmetries of the fixed domain acting on the image signal (sometimes known as active symmetries), whereas in GNNs any permutation acts on both the graph signals and the graph domain (sometimes described as passive symmetries). In this work, we focus on the active symmetries of GNNs, by considering a learning setting where signals are supported on a fixed graph. In this case, the natural symmetries of GNNs are the automorphisms of the graph. Since real-world graphs tend to be asymmetric, we relax the notion of symmetries by formalizing approximate symmetries via graph coarsening. We present a bias-variance formula that qu
    
[^48]: 分布式学习中的资源自适应牛顿法

    Resource-Adaptive Newton's Method for Distributed Learning. (arXiv:2308.10154v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.10154](http://arxiv.org/abs/2308.10154)

    本文介绍了一种名为RANL的新颖和高效的算法，通过使用简单的Hessian初始化和自适应的训练区域分配，克服了牛顿法在大规模和异构学习环境中的限制，并实现了线性收敛率。

    

    基于牛顿方法的分布式随机优化方法通过利用曲率信息提供了比一阶方法更好的性能。然而，在大规模和异构学习环境中，牛顿方法的实际适用性受到了诸多挑战的限制，例如与Hessian矩阵相关的高计算和通信成本、子模型多样性、训练的过时性和数据的异构性。为了解决这些挑战，本文引入了一种名为RANL的新颖高效算法，通过采用简单的Hessian初始化和自适应的训练区域分配来克服牛顿方法的限制。该算法表现出令人印象深刻的收敛性质，在随机优化的标准假设下进行了严格分析。理论分析证明了RANL实现了线性收敛率，同时有效地适应了可用资源。

    Distributed stochastic optimization methods based on Newton's method offer significant advantages over first-order methods by leveraging curvature information for improved performance. However, the practical applicability of Newton's method is hindered in large-scale and heterogeneous learning environments due to challenges such as high computation and communication costs associated with the Hessian matrix, sub-model diversity, staleness in training, and data heterogeneity. To address these challenges, this paper introduces a novel and efficient algorithm called RANL, which overcomes the limitations of Newton's method by employing a simple Hessian initialization and adaptive assignments of training regions. The algorithm demonstrates impressive convergence properties, which are rigorously analyzed under standard assumptions in stochastic optimization. The theoretical analysis establishes that RANL achieves a linear convergence rate while effectively adapting to available resources and 
    
[^49]: 基于事件的动态图表示学习在专利申请趋势预测中的应用

    Event-based Dynamic Graph Representation Learning for Patent Application Trend Prediction. (arXiv:2308.09780v1 [cs.AI])

    [http://arxiv.org/abs/2308.09780](http://arxiv.org/abs/2308.09780)

    本研究提出了一种基于事件的动态图学习框架，用于准确预测专利申请趋势。该方法利用公司和分类代码的可记忆表示，通过历史记忆和当前信息更新来捕捉语义接近性。

    

    准确预测公司在未来一段时间内将申请哪些类型的专利能够揭示出它们的发展战略，并帮助其提前发现潜在的合作伙伴或竞争对手。然而，由于对公司不断变化的偏好和对分类代码的语义关联的建模困难，这个问题在之前的研究中鲜有涉及。为了弥补这一空白，我们提出了一种基于事件的动态图学习框架，用于预测专利申请趋势。具体而言，我们的方法建立在公司和专利分类代码的可记忆表示基础上。当观察到一个新的专利时，相关公司和分类代码的表示根据历史记忆和当前编码的信息进行更新。此外，提供了一个层次化消息传递机制，以捕捉专利分类代码的语义接近性。

    Accurate prediction of what types of patents that companies will apply for in the next period of time can figure out their development strategies and help them discover potential partners or competitors in advance. Although important, this problem has been rarely studied in previous research due to the challenges in modelling companies' continuously evolving preferences and capturing the semantic correlations of classification codes. To fill in this gap, we propose an event-based dynamic graph learning framework for patent application trend prediction. In particular, our method is founded on the memorable representations of both companies and patent classification codes. When a new patent is observed, the representations of the related companies and classification codes are updated according to the historical memories and the currently encoded messages. Moreover, a hierarchical message passing mechanism is provided to capture the semantic proximities of patent classification codes by u
    
[^50]: Baird反例已解决：以调试两个时间尺度算法的示例。

    Baird Counterexample Is Solved: with an example of How to Debug a Two-time-scale Algorithm. (arXiv:2308.09732v1 [cs.LG])

    [http://arxiv.org/abs/2308.09732](http://arxiv.org/abs/2308.09732)

    这篇论文解决了Baird反例上的收敛问题，通过一种具有收敛保证的算法，实现了线性收敛速度。

    

    Baird反例是由Leemon Baird在1995年提出的，首先用于证明Temporal Difference (TD(0))算法在这个例子上发散。从那时起，它经常被用来测试和比较离策略学习算法。梯度TD算法解决了TD在Baird反例上的发散问题。然而，它们在这个例子上的收敛仍然非常缓慢，而且缓慢的本质还不被很好地理解。本文旨在特别理解为什么TDC在这个例子上慢，并提供调试分析来理解这种行为。我们的调试技术可以用来研究两个时间尺度随机逼近算法的收敛行为。我们还提供了最近的Impression GTD算法在这个例子上的实证结果，表明收敛非常快，事实上是线性的。我们得出结论，Baird反例通过一种具有收敛保证的算法解决了，该算法收敛到TD解决方案。

    Baird counterexample was proposed by Leemon Baird in 1995, first used to show that the Temporal Difference (TD(0)) algorithm diverges on this example. Since then, it is often used to test and compare off-policy learning algorithms. Gradient TD algorithms solved the divergence issue of TD on Baird counterexample. However, their convergence on this example is still very slow, and the nature of the slowness is not well understood, e.g., see (Sutton and Barto 2018).  This note is to understand in particular, why TDC is slow on this example, and provide debugging analysis to understand this behavior. Our debugging technique can be used to study the convergence behavior of two-time-scale stochastic approximation algorithms. We also provide empirical results of the recent Impression GTD algorithm on this example, showing the convergence is very fast, in fact, in a linear rate. We conclude that Baird counterexample is solved, by an algorithm with convergence guarantee to the TD solution in gen
    
[^51]: 基于无线计算的辅助联邦学习与归一化梯度聚合

    Over-the-Air Computation Aided Federated Learning with the Aggregation of Normalized Gradient. (arXiv:2308.09082v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.09082](http://arxiv.org/abs/2308.09082)

    通过归一化梯度，我们在无线计算的辅助下改进了联邦学习，提高了收敛性能。

    

    无线计算是联邦学习中一种通信高效的解决方案。在这样的系统中，进行迭代过程：每个移动设备更新、放大并传输私有损失函数的局部梯度；服务器接收一次性聚合的梯度，生成并广播更新的模型参数给每个移动设备。在放大因子选择方面，大多数相关工作假设局部梯度的最大范数始终发生，尽管实际上它在迭代中会波动，这可能降低收敛性能。为了解决这个问题，我们提出在放大之前将局部梯度归一化。在我们提出的方法下，当损失函数平滑时，我们证明了我们的方法可以以次线性速率收敛到稳定点。在平滑且强凸损失函数的情况下，我们证明了我们的方法可以以线性速率实现最小训练损失，且该速率可以任意小正数。

    Over-the-air computation is a communication-efficient solution for federated learning (FL). In such a system, iterative procedure is performed: Local gradient of private loss function is updated, amplified and then transmitted by every mobile device; the server receives the aggregated gradient all-at-once, generates and then broadcasts updated model parameters to every mobile device. In terms of amplification factor selection, most related works suppose the local gradient's maximal norm always happens although it actually fluctuates over iterations, which may degrade convergence performance. To circumvent this problem, we propose to turn local gradient to be normalized one before amplifying it. Under our proposed method, when the loss function is smooth, we prove our proposed method can converge to stationary point at sub-linear rate. In case of smooth and strongly convex loss function, we prove our proposed method can achieve minimal training loss at linear rate with any small positiv
    
[^52]: DealMVC: 面向多视角聚类的双向对比校准

    DealMVC: Dual Contrastive Calibration for Multi-view Clustering. (arXiv:2308.09000v1 [cs.CV])

    [http://arxiv.org/abs/2308.09000](http://arxiv.org/abs/2308.09000)

    提出了一种名为DealMVC的双向对比校准网络，用于解决多视图聚类中的一致性问题，并利用全局和局部对比校准损失来提高聚类性能。

    

    多视角对比聚类由于其强大的视角一致性信息挖掘能力而受到了广泛关注。然而，我们观察到一个限制聚类性能进一步提升的问题。现有的多视图模型主要关注不同视图中相同样本的一致性，而忽视了交叉视图场景中类似但不同的样本情况。为了解决这个问题，我们提出了一种新颖的面向多视图聚类的双向对比校准网络（DealMVC）。具体而言，我们首先设计了一个融合机制，获得全局的跨视图特征。然后，通过对齐视图特征相似性图和高置信度伪标签图，提出了一个全局对比校准损失。此外，为了利用多视图信息的多样性，我们提出了一个局部对比校准损失，用于约束成对视图特征的一致性。特征结构是 ...

    Benefiting from the strong view-consistent information mining capacity, multi-view contrastive clustering has attracted plenty of attention in recent years. However, we observe the following drawback, which limits the clustering performance from further improvement. The existing multi-view models mainly focus on the consistency of the same samples in different views while ignoring the circumstance of similar but different samples in cross-view scenarios. To solve this problem, we propose a novel Dual contrastive calibration network for Multi-View Clustering (DealMVC). Specifically, we first design a fusion mechanism to obtain a global cross-view feature. Then, a global contrastive calibration loss is proposed by aligning the view feature similarity graph and the high-confidence pseudo-label graph. Moreover, to utilize the diversity of multi-view information, we propose a local contrastive calibration loss to constrain the consistency of pair-wise view features. The feature structure is
    
[^53]: CONVERT: 可靠增强的对比图聚类

    CONVERT:Contrastive Graph Clustering with Reliable Augmentation. (arXiv:2308.08963v1 [cs.LG])

    [http://arxiv.org/abs/2308.08963](http://arxiv.org/abs/2308.08963)

    本论文提出了一种名为COVERT的对比图聚类网络，通过可逆扰动恢复网络处理数据增强，提炼可靠的语义信息，进一步保证对比学习中语义的可靠性。

    

    通过可学习的数据增强进行对比图节点聚类是无监督图学习领域的研究热点。现有方法通过学习预定义增强的采样分布来自动生成数据驱动增强。虽然已取得了有希望的聚类性能，但我们观察到这些方法仍依赖于预定义增强，增强后的图的语义容易漂移，对比学习中的增强视图语义的可靠性无法保证，从而限制了模型性能。为了解决这些问题，我们提出了一种新的可靠增强的对比图聚类网络（COVERT）。具体来说，在我们的方法中，增强数据通过提出的可逆扰动恢复网络进行处理。它通过恢复扰动后的潜在嵌入来提炼可靠的语义信息。此外，为了进一步保证语义的可靠性，我们提出了一种新的语义Loss。

    Contrastive graph node clustering via learnable data augmentation is a hot research spot in the field of unsupervised graph learning. The existing methods learn the sampling distribution of a pre-defined augmentation to generate data-driven augmentations automatically. Although promising clustering performance has been achieved, we observe that these strategies still rely on pre-defined augmentations, the semantics of the augmented graph can easily drift. The reliability of the augmented view semantics for contrastive learning can not be guaranteed, thus limiting the model performance. To address these problems, we propose a novel CONtrastiVe Graph ClustEring network with Reliable AugmenTation (COVERT). Specifically, in our method, the data augmentations are processed by the proposed reversible perturb-recover network. It distills reliable semantic information by recovering the perturbed latent embeddings. Moreover, to further guarantee the reliability of semantics, a novel semantic lo
    
[^54]: Real Robot Challenge 2022: 在真实世界中从离线数据中学习灵巧操作

    Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World. (arXiv:2308.07741v1 [cs.RO])

    [http://arxiv.org/abs/2308.07741](http://arxiv.org/abs/2308.07741)

    Real Robot Challenge 2022为RL和机器人学界之间的桥梁，允许参与者在真实机器人上从离线数据中学习灵巧操作任务，解决了在模拟中得到的见解不能转化到真实机器人的问题。

    

    在真实机器人上进行实验在时间和成本上要求很高。因此，增强学习（RL）社区的很大一部分使用模拟器来开发和评估算法。然而，从模拟中得到的见解不一定能够转化到真实机器人上，尤其是对于涉及复杂环境交互的任务。因此，Real Robot Challenge 2022作为RL和机器人学界之间的桥梁，让参与者能够像在模拟中一样轻松地远程实验真实机器人。在过去几年中，离线增强学习已经成熟为一种从预先收集的数据集中学习的有希望的范式，减轻了对昂贵在线交互的依赖.因此，我们要求参与者从提供的真实机器人数据集中学习两个灵巧操作任务，包括推动、抓取和手内定位。进行了广泛的软件文档化，并在基于仿真的初步阶段进行了实验。

    Experimentation on real robots is demanding in terms of time and costs. For this reason, a large part of the reinforcement learning (RL) community uses simulators to develop and benchmark algorithms. However, insights gained in simulation do not necessarily translate to real robots, in particular for tasks involving complex interactions with the environment. The Real Robot Challenge 2022 therefore served as a bridge between the RL and robotics communities by allowing participants to experiment remotely with a real robot - as easily as in simulation.  In the last years, offline reinforcement learning has matured into a promising paradigm for learning from pre-collected datasets, alleviating the reliance on expensive online interactions. We therefore asked the participants to learn two dexterous manipulation tasks involving pushing, grasping, and in-hand orientation from provided real-robot datasets. An extensive software documentation and an initial stage based on a simulation of the re
    
[^55]: CausalLM不适用于上下文学习

    CausalLM is not optimal for in-context learning. (arXiv:2308.06912v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.06912](http://arxiv.org/abs/2308.06912)

    最近的研究显示，上下文学习中使用前缀语言模型（PrefixLM）比因果语言模型（CausalLM）效果更好。本文通过理论分析证明，虽然两种语言模型都以线性速率收敛到稳定点，但前缀语言模型收敛到线性回归的最优解，因果语言模型的收敛动态遵循在线梯度下降算法，不保证收敛到最优解。

    

    最近的实证证据表明，在上下文学习中，使用前缀语言模型（PrefixLM）表现更好，其允许上下文样本相互关注；相比之下，因果语言模型（CausalLM）使用自回归注意力机制，禁止上下文样本关注未来的样本。虽然这个结果是直观的，但从理论角度并不清楚。本文采用理论方法，分析了在特定参数构建下，前缀语言模型和因果语言模型的收敛行为。分析结果显示，两种语言模型都以线性速率收敛到稳定点，但前缀语言模型收敛到线性回归的最优解，而因果语言模型的收敛动态遵循在线梯度下降算法，即使样本数量趋于无穷，也不能保证收敛到最优解。我们通过对合成数据的经验实验来支持我们的理论观点。

    Recent empirical evidence indicates that transformer based in-context learning performs better when using a prefix language model (prefixLM), in which in-context samples can all attend to each other, compared to causal language models (causalLM), which use auto-regressive attention that prohibits in-context samples to attend to future samples. While this result is intuitive, it is not understood from a theoretical perspective. In this paper we take a theoretical approach and analyze the convergence behavior of prefixLM and causalLM under a certain parameter construction. Our analysis shows that both LM types converge to their stationary points at a linear rate, but that while prefixLM converges to the optimal solution of linear regression, causalLM convergence dynamics follows that of an online gradient descent algorithm, which is not guaranteed to be optimal even as the number of samples grows infinitely. We supplement our theoretical claims with empirical experiments over synthetic a
    
[^56]: 使用路径进行一般化拓扑图神经网络 (arXiv:2308.06838v2 [cs.LG] 已更新)

    Generalizing Topological Graph Neural Networks with Paths. (arXiv:2308.06838v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.06838](http://arxiv.org/abs/2308.06838)

    本论文提出了一种通用的拓扑图神经网络（GNNs）方法，通过强调图中的路径，突破了1-Weisfeiler-Lehmann测试的理论限制，并在无需对图的子结构进行假设的情况下，在多个基准测试中取得了最先进的性能。

    

    尽管图神经网络（GNNs）在不同领域取得了重大进展，但它们受到1-Weisfeiler-Lehmann测试的理论限制。尽管高阶GNNs的最新进展可以克服这个障碍，但它们通常集中在特定的图组件，如团或环。然而，我们的研究走了不同的路线。我们强调路径，这是每个图中固有的。我们能够构建一个更通用的拓扑视角，并与其他拓扑领域的一些成熟理论形成桥梁。有趣的是，在没有对图的子结构进行任何假设的情况下，我们的方法超过了该领域早期的技术，在多个基准测试中实现了最先进的性能。

    While Graph Neural Networks (GNNs) have made significant strides in diverse areas, they are hindered by a theoretical constraint known as the 1-Weisfeiler-Lehmann test. Even though latest advancements in higher-order GNNs can overcome this boundary, they typically center around certain graph components like cliques or cycles. However, our investigation goes a different route. We put emphasis on paths, which are inherent in every graph. We are able to construct a more general topological perspective and form a bridge to certain established theories about other topological domains. Interestingly, without any assumptions on graph sub-structures, our approach surpasses earlier techniques in this field, achieving state-of-the-art performance on several benchmarks.
    
[^57]: 使用Apriori关联规则挖掘从康复和死亡病例中发现COVID-19的症状模式

    Discovering the Symptom Patterns of COVID-19 from Recovered and Deceased Patients Using Apriori Association Rule Mining. (arXiv:2308.06763v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.06763](http://arxiv.org/abs/2308.06763)

    本文使用Apriori算法基于关联规则挖掘的方法，从COVID-19患者中发现了一些最常见的症状模式，如呼吸暂停、咳嗽、发热等，为临床医生提供了宝贵的疾病见解。

    

    COVID-19大流行在全球范围内造成了毁灭性的影响，夺走了数百万人的生命，造成了重大的社会和经济中断。为了优化决策并分配有限资源，确定COVID-19的症状并确定每种病例的严重程度非常重要。机器学习算法在医学领域中提供了一种强大的工具，特别是在挖掘临床数据集以获取有用信息和指导科学决策方面。关联规则挖掘是一种从数据中提取隐藏模式的机器学习技术。本文提出了一种基于Apriori算法的关联规则挖掘在COVID-19患者中发现症状模式的应用。该研究使用2875例患者记录，确定了最常见的症状为呼吸暂停（72％）、咳嗽（64％）、发热（59％）、虚弱（18％）、肌肉痛（14.5％）和喉咙痛（12％）。该方法为临床医生提供了有价值的疾病见解，可以帮助他们进行管理和治疗。

    The COVID-19 pandemic has a devastating impact globally, claiming millions of lives and causing significant social and economic disruptions. In order to optimize decision-making and allocate limited resources, it is essential to identify COVID-19 symptoms and determine the severity of each case. Machine learning algorithms offer a potent tool in the medical field, particularly in mining clinical datasets for useful information and guiding scientific decisions. Association rule mining is a machine learning technique for extracting hidden patterns from data. This paper presents an application of association rule mining based Apriori algorithm to discover symptom patterns from COVID-19 patients. The study, using 2875 patient's records, identified the most common signs and symptoms as apnea (72%), cough (64%), fever (59%), weakness (18%), myalgia (14.5%), and sore throat (12%). The proposed method provides clinicians with valuable insight into disease that can assist them in managing and t
    
[^58]: 使用生成性扩散模型的降水即时预测

    Precipitation nowcasting with generative diffusion models. (arXiv:2308.06733v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.06733](http://arxiv.org/abs/2308.06733)

    这篇论文研究了使用生成性扩散模型进行降水即时预测的问题，生成模型在天气预报中的应用具有潜力，并能够更好地建模概率分布。

    

    近年来，传统的数值预测方法逐渐受到深度学习方法的挑战。用于短期和中期天气预报的多个历史数据集通常被组织成规则的空间网格结构。这种排列与图像非常相似：每个天气变量可以被视为一幅地图，或者在考虑时间轴时，可以被视为一段视频。多个类别的生成模型，包括生成对抗网络、变分自编码器，或者最近的去噪扩散模型，在下一帧预测问题上已经证明了它们的适用性，因此自然而然地希望测试它们在天气预报基准上的性能。在这个背景下，扩散模型特别具有吸引力，因为天气预报本质上是具有概率性的：我们真正有兴趣建模的是天气指标的概率分布，其期望值是最可能的值。

    In recent years traditional numerical methods for accurate weather prediction have been increasingly challenged by deep learning methods. Numerous historical datasets used for short and medium-range weather forecasts are typically organized into a regular spatial grid structure. This arrangement closely resembles images: each weather variable can be visualized as a map or, when considering the temporal axis, as a video. Several classes of generative models, comprising Generative Adversarial Networks, Variational Autoencoders, or the recent Denoising Diffusion Models have largely proved their applicability to the next-frame prediction problem, and is thus natural to test their performance on the weather prediction benchmarks. Diffusion models are particularly appealing in this context, due to the intrinsically probabilistic nature of weather forecasting: what we are really interested to model is the probability distribution of weather indicators, whose expected value is the most likely 
    
[^59]: 在部分可观察情境轮盘赌中的可证效率学习

    Provably Efficient Learning in Partially Observable Contextual Bandit. (arXiv:2308.03572v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.03572](http://arxiv.org/abs/2308.03572)

    本文研究了在部分可观察情境轮盘赌中的转移学习问题，提出了一种通过优化问题识别行为和奖励因果效应的方法，并利用因果约束来改进轮盘赌算法。

    

    本文研究了在部分可观察情境轮盘赌中的转移学习问题，其中代理人仅有来自其他代理人的有限知识，并且对隐藏的混淆因素只有部分信息。我们将该问题转化为通过优化问题来识别或部分识别行为和奖励之间的因果效应。为了解决这些优化问题，我们将未知分布的原始功能约束离散化为线性约束，并通过顺序解线性规划来采样兼容的因果模型，以考虑估计误差得到因果约束。我们的采样算法为适当的采样分布提供了理想的收敛结果。然后，我们展示了如何将因果约束应用于改进经典的轮盘赌算法，并以行动集和函数空间规模为参考改变了遗憾值。值得注意的是，在允许我们处理一般情境分布的函数逼近任务中

    In this paper, we investigate transfer learning in partially observable contextual bandits, where agents have limited knowledge from other agents and partial information about hidden confounders. We first convert the problem to identifying or partially identifying causal effects between actions and rewards through optimization problems. To solve these optimization problems, we discretize the original functional constraints of unknown distributions into linear constraints, and sample compatible causal models via sequentially solving linear programmings to obtain causal bounds with the consideration of estimation error. Our sampling algorithms provide desirable convergence results for suitable sampling distributions. We then show how causal bounds can be applied to improving classical bandit algorithms and affect the regrets with respect to the size of action sets and function spaces. Notably, in the task with function approximation which allows us to handle general context distributions
    
[^60]: 边缘稳定的回声状态网络

    Edge of stability echo state networks. (arXiv:2308.02902v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.02902](http://arxiv.org/abs/2308.02902)

    本文介绍了一种新的边缘稳定回声状态网络（ES$^2$N）架构，结合了渐进记忆属性和尽可能保留更多记忆的能力，通过将储备层定义为非线性和线性的凸组合，提供了网络稳定性和性能的增强。

    

    回声状态网络（ESNs）是基于回声状态属性（ESP）原理工作的时间序列处理模型。ESP是指对输入的记忆在渐进衰减。然而，ESNs的固有架构偏差可能导致过多信息的丢失，进而影响在某些长期记忆任务中的性能。为了将渐进记忆属性与尽可能保留更多记忆的能力结合起来，本文介绍了一种新的ESN架构，称为边缘稳定回声状态网络（ES$^2$N）。引入的ES$^2$N模型基于将储备层定义为非线性储备（与标准ESN相同）和实现正交变换的线性储备的凸组合。我们对引入的模型进行了彻底的数学分析，并证明了ES$^2$N的雅可比矩阵的整个特征谱...

    Echo State Networks (ESNs) are time-series processing models working under the Echo State Property (ESP) principle. The ESP is a notion of stability that imposes an asymptotic fading of the memory of the input. On the other hand, the resulting inherent architectural bias of ESNs may lead to an excessive loss of information, which in turn harms the performance in certain tasks with long short-term memory requirements. With the goal of bringing together the fading memory property and the ability to retain as much memory as possible, in this paper we introduce a new ESN architecture, called the Edge of Stability Echo State Network (ES$^2$N). The introduced ES$^2$N model is based on defining the reservoir layer as a convex combination of a nonlinear reservoir (as in the standard ESN), and a linear reservoir that implements an orthogonal transformation. We provide a thorough mathematical analysis of the introduced model, proving that the whole eigenspectrum of the Jacobian of the ES$^2$N ma
    
[^61]: 具有平滑激活函数的两层神经网络的存储容量研究

    Memory capacity of two layer neural networks with smooth activations. (arXiv:2308.02001v1 [cs.LG])

    [http://arxiv.org/abs/2308.02001](http://arxiv.org/abs/2308.02001)

    研究发现，具有平滑激活函数的两层神经网络的存储容量下界为md/2，并且准确性大致为2倍。分析存储容量的方法包括计算网络雅可比矩阵的秩，并扩展了有关Hadamard幂秩的经典线性代数事实。

    

    确定具有m个隐藏神经元和输入维数d（即md+m个训练参数）的两层神经网络的存储容量，即网络能够记忆的一般数据的最大尺寸，是一个基本的机器学习问题。对于非多项式实解析激活函数，如sigmoid和平滑的修正线性单元（平滑ReLU），我们建立了md/2的下界，并且准确性大约为2倍。类似的先前结果仅限于阶跃函数和ReLU激活函数，对于平滑激活函数的结果受到对数因子和随机数据的限制。为了分析存储容量，我们通过计算涉及Hadamard幂和Khati-Rao积的矩阵的秩来考察网络雅可比矩阵的秩。我们的计算扩展了关于Hadamard幂秩的经典线性代数事实。总体而言，我们的方法与之前关于存储容量的研究不同，并有希望实现。

    Determining the memory capacity of two-layer neural networks with m hidden neurons and input dimension d (i.e., md+m total trainable parameters), which refers to the largest size of general data the network can memorize, is a fundamental machine-learning question. For non-polynomial real analytic activation functions, such as sigmoids and smoothed rectified linear units (smoothed ReLUs), we establish a lower bound of md/2 and optimality up to a factor of approximately 2. Analogous prior results were limited to Heaviside and ReLU activations, with results for smooth activations suffering from logarithmic factors and requiring random data. To analyze the memory capacity, we examine the rank of the network's Jacobian by computing the rank of matrices involving both Hadamard powers and the Khati-Rao product. Our computation extends classical linear algebraic facts about the rank of Hadamard powers. Overall, our approach differs from previous works on memory capacity and holds promise for e
    
[^62]: 使用Floss增强周期性时间序列的表示学习：一种频域正则化方法

    Enhancing Representation Learning for Periodic Time Series with Floss: A Frequency Domain Regularization Approach. (arXiv:2308.01011v1 [cs.LG])

    [http://arxiv.org/abs/2308.01011](http://arxiv.org/abs/2308.01011)

    本文提出了一种叫做Floss的无监督方法，通过在频域上对学到的表示进行正则化来增强周期性时间序列的表示学习。Floss方法可以自动检测时间序列中的周期性并学习具有周期一致性的有意义的表示。

    

    时间序列分析是各个应用领域的基础任务，深度学习方法在这个领域表现出了非凡的性能。然而，许多现实世界的时间序列数据展现出重要的周期性或准周期性动态，这些动态往往不能被现有的基于深度学习的解决方案充分捕捉到。这导致对感兴趣的基础动态行为的表示不完整。为了解决这个问题，我们提出了一种无监督的方法叫做Floss，它通过自动化地在频域上调整学到的表示来进行正则化。Floss方法首先自动检测时间序列中的主要周期性。然后，它利用周期移位和谱密度相似度度量来学习具有周期一致性的有意义的表示。此外，Floss可以轻松地整合到有监督、半监督和无监督的学习框架中。

    Time series analysis is a fundamental task in various application domains, and deep learning approaches have demonstrated remarkable performance in this area. However, many real-world time series data exhibit significant periodic or quasi-periodic dynamics that are often not adequately captured by existing deep learning-based solutions. This results in an incomplete representation of the underlying dynamic behaviors of interest. To address this gap, we propose an unsupervised method called Floss that automatically regularizes learned representations in the frequency domain. The Floss method first automatically detects major periodicities from the time series. It then employs periodic shift and spectral density similarity measures to learn meaningful representations with periodic consistency. In addition, Floss can be easily incorporated into both supervised, semi-supervised, and unsupervised learning frameworks. We conduct extensive experiments on common time series classification, for
    
[^63]: 使用端到端视频异常检测系统来基准测试Jetson边缘设备

    Benchmarking Jetson Edge Devices with an End-to-end Video-based Anomaly Detection System. (arXiv:2307.16834v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.16834](http://arxiv.org/abs/2307.16834)

    本论文实现了一个端到端的视频异常检测系统，通过从监控视频输入进行犯罪现场异常检测，并在多个Jetson边缘设备上部署和运行。这是对Jetson平台在深度学习算法执行方面性能的基准测试分析的创新。

    

    创新的嵌入式系统平台，特别是硬件加速，显着影响了深度学习在现实场景中的应用。这些创新将人类劳动转化为自动化的智能系统，应用于自动驾驶、机器人技术、物联网和许多其他有重大影响的应用领域。NVIDIA的Jetson平台是在执行深度学习算法方面能够提供能效和吞吐率最佳性能的先驱之一。先前的大部分基准测试分析都是基于2D图像，并使用单个深度学习模型进行每个比较结果。在本文中，我们实现了一种从监控视频输入的端到端基于视频的犯罪现场异常检测系统，并将该系统部署在多个Jetson边缘设备上（Nano、AGX Xavier、Orin Nano）。比较分析包括将Torch-TensorRT集成为软件。

    Innovative enhancement in embedded system platforms, specifically hardware accelerations, significantly influence the application of deep learning in real-world scenarios. These innovations translate human labor efforts into automated intelligent systems employed in various areas such as autonomous driving, robotics, Internet-of-Things (IoT), and numerous other impactful applications. NVIDIA's Jetson platform is one of the pioneers in offering optimal performance regarding energy efficiency and throughput in the execution of deep learning algorithms. Previously, most benchmarking analysis was based on 2D images with a single deep learning model for each comparison result. In this paper, we implement an end-to-end video-based crime-scene anomaly detection system inputting from surveillance videos and the system is deployed and completely operates on multiple Jetson edge devices (Nano, AGX Xavier, Orin Nano). The comparison analysis includes the integration of Torch-TensorRT as a softwar
    
[^64]: 用于16位神经网络训练中数值不稳定性的高效方法

    An Efficient Approach to Mitigate Numerical Instability in Backpropagation for 16-bit Neural Network Training. (arXiv:2307.16189v1 [cs.LG])

    [http://arxiv.org/abs/2307.16189](http://arxiv.org/abs/2307.16189)

    这项研究探讨了16位计算中机器学习模型的数值不稳定性问题，并提出了一种基于Adam优化器的新方法来提高16位神经网络的学习过程的鲁棒性。

    

    在这项研究中，我们深入探讨了在16位计算中使用流行的优化算法（如RMSProp和Adam）时观察到的数值不稳定性的复杂性。这种不稳定性通常在深度神经网络的训练阶段中出现，导致学习过程受到干扰，从而妨碍了这些模型的有效部署。我们确定了单一超参数epsilon是这种数值不稳定性的主要原因。对16位计算中这些优化器中epsilon的作用进行了深入探索，发现微调其值可以恢复RMSProp和Adam的功能，从而实现有效利用16位神经网络。我们提出了一种新的方法来减轻被发现的数值不稳定性问题。该方法利用Adam优化器的更新，并显著改善了16位神经网络的学习过程的鲁棒性。

    In this research, we delve into the intricacies of the numerical instability observed in 16-bit computations of machine learning models, particularly when employing popular optimization algorithms such as RMSProp and Adam. This instability is commonly experienced during the training phase of deep neural networks, leading to disrupted learning processes and hindering the effective deployment of such models. We identify the single hyperparameter, epsilon, as the main culprit behind this numerical instability. An in-depth exploration of the role of epsilon in these optimizers within 16-bit computations reveals that a minor adjustment of its value can restore the functionality of RMSProp and Adam, consequently enabling the effective utilization of 16-bit neural networks. We propose a novel method to mitigate the identified numerical instability issues. This method capitalizes on the updates from the Adam optimizer and significantly improves the robustness of the learning process in 16-bit 
    
[^65]: 仅使用一个步长的新型梯度时序差分算法：通过$L$-$\lambda$平滑性进行收敛速率分析

    A new Gradient TD Algorithm with only One Step-size: Convergence Rate Analysis using $L$-$\lambda$ Smoothness. (arXiv:2307.15892v1 [cs.LG])

    [http://arxiv.org/abs/2307.15892](http://arxiv.org/abs/2307.15892)

    本论文提出了一种新的梯度时序差分算法，只使用一个步长参数，并证明收敛速度至少为$O(1/t)$。

    

    梯度时序差分（GTD）算法是第一个具有收敛保证的离策略学习线性函数逼近算法，其复杂度为$O(d)$（$d$是特征数量）。本文提出了一种名为Impression GTD的全新单时间尺度GTD算法，用于最小化期望td更新（NEU）目标，并只有一个步长参数。我们证明这种新算法的收敛速度至少与$O(1/t)$一样快。

    Gradient Temporal Difference (GTD) algorithms (Sutton et al., 2008, 2009) are the first $O(d)$ ($d$ is the number features) algorithms that have convergence guarantees for off-policy learning with linear function approximation. Liu et al. (2015) and Dalal et. al. (2018) proved the convergence rates of GTD, GTD2 and TDC are $O(t^{-\alpha/2})$ for some $\alpha \in (0,1)$. This bound is tight (Dalal et al., 2020), and slower than $O(1/\sqrt{t})$. GTD algorithms also have two step-size parameters, which are difficult to tune. In literature, there is a "single-time-scale" formulation of GTD. However, this formulation still has two step-size parameters.  This paper presents a truly single-time-scale GTD algorithm for minimizing the Norm of Expected td Update (NEU) objective, and it has only one step-size parameter. We prove that the new algorithm, called Impression GTD, converges at least as fast as $O(1/t)$. Furthermore, based on a generalization of the expected smoothness (Gower et al. 201
    
[^66]: 可解释人工智能（XAI）在年龄预测中的应用：一项系统综述

    eXplainable Artificial Intelligence (XAI) in age prediction: A systematic review. (arXiv:2307.13704v1 [cs.AI])

    [http://arxiv.org/abs/2307.13704](http://arxiv.org/abs/2307.13704)

    本综述探讨了可解释人工智能（XAI）在年龄预测任务中的应用。通过系统性综述，我们讨论了XAI方法在医疗应用和年龄预测领域的益处。

    

    可解释人工智能（XAI）现在是机器学习中的重要组成部分，能够解释复杂模型的预测结果。XAI特别适用于危险应用，特别是在医疗保健领域，人类的生命依赖于AI系统的决策。医疗研究的一个领域是年龄预测和衰老及与年龄相关疾病的生物标志物鉴定。然而，在年龄预测任务中，XAI的作用尚未直接探讨。在本综述中，我们讨论了XAI方法在年龄预测任务中的应用。我们通过器官系统进行了系统性综述，并讨论了XAI在医疗应用以及特别是年龄预测领域的益处。

    eXplainable Artificial Intelligence (XAI) is now an important and essential part of machine learning, allowing to explain the predictions of complex models. XAI is especially required in risky applications, particularly in health care, where human lives depend on the decisions of AI systems. One area of medical research is age prediction and identification of biomarkers of aging and age-related diseases. However, the role of XAI in the age prediction task has not previously been explored directly. In this review, we discuss the application of XAI approaches to age prediction tasks. We give a systematic review of the works organized by body systems, and discuss the benefits of XAI in medical applications and, in particular, in the age prediction domain.
    
[^67]: 关于注意力网络学习动态的研究

    On the learning Dynamics of Attention Networks. (arXiv:2307.13421v1 [cs.LG])

    [http://arxiv.org/abs/2307.13421](http://arxiv.org/abs/2307.13421)

    本研究分析了软注意力、硬注意力和潜变量边际似然（LVML）注意力三种注意力模型的学习动态，发现了它们在所选择的片段聚合方式上的显著差异，并解释了分类模型在梯度下降下的演化对最终结果的影响。

    

    注意力模型通常通过优化三个标准损失函数之一来学习，分别称为软注意力、硬注意力和潜变量边际似然（LVML）注意力。这三种范式都是为了达到相同的目标，即找到两个模型：一个“焦点”模型，用于“选择”输入中的正确“片段”，和一个“分类”模型，用于将选定的片段处理成目标标签。然而，它们在所选择的片段聚合方式上存在显著差异，导致了不同的动态和最终结果。我们观察到使用这些范式学习的模型具有独特的特征，并将其解释为在焦点模型固定时，分类模型在梯度下降下的演化所致。我们还在一个简单的设置中分析了这些范式，并推导出梯度流下参数轨迹的闭式表达式。在软注意力损失下，焦点模型在初始化阶段快速改善。

    Attention models are typically learned by optimizing one of three standard loss functions that are variously called -- soft attention, hard attention, and latent variable marginal likelihood (LVML) attention. All three paradigms are motivated by the same goal of finding two models -- a `focus' model that `selects' the right \textit{segment} of the input and a `classification' model that processes the selected segment into the target label. However, they differ significantly in the way the selected segments are aggregated, resulting in distinct dynamics and final results. We observe a unique signature of models learned using these paradigms and explain this as a consequence of the evolution of the classification model under gradient descent when the focus model is fixed. We also analyze these paradigms in a simple setting and derive closed-form expressions for the parameter trajectory under gradient flow. With the soft attention loss, the focus model improves quickly at initialization a
    
[^68]: AutoAlign：基于大型语言模型的全自动有效知识图谱对齐方法

    AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment enabled by Large Language Models. (arXiv:2307.11772v1 [cs.IR])

    [http://arxiv.org/abs/2307.11772](http://arxiv.org/abs/2307.11772)

    AutoAlign是一种全自动的知识图谱对齐方法，不需要手工制作的种子对齐。它利用大型语言模型自动捕捉谓词相似性，并使用TransE计算实体嵌入来实现实体对齐。

    

    知识图谱间的实体对齐任务旨在识别出两个不同知识图谱中表示相同实体的每对实体。许多基于机器学习的方法已被提出用于这个任务。然而，据我们所知，现有的方法都需要手工制作的种子对齐，这是非常昂贵的。在本文中，我们提出了第一个名为AutoAlign的完全自动对齐方法，它不需要任何手工制作的种子对齐。具体而言，对于谓词嵌入，AutoAlign使用大型语言模型构建谓词近邻图，自动捕捉两个知识图谱中谓词的相似性。对于实体嵌入，AutoAlign首先使用TransE独立计算每个知识图谱的实体嵌入，然后通过计算基于实体属性的实体相似性，将两个知识图谱的实体嵌入移动到相同的向量空间中。因此，AutoAlign实现了谓词对齐和实体对齐。

    The task of entity alignment between knowledge graphs (KGs) aims to identify every pair of entities from two different KGs that represent the same entity. Many machine learning-based methods have been proposed for this task. However, to our best knowledge, existing methods all require manually crafted seed alignments, which are expensive to obtain. In this paper, we propose the first fully automatic alignment method named AutoAlign, which does not require any manually crafted seed alignments. Specifically, for predicate embeddings, AutoAlign constructs a predicate-proximity-graph with the help of large language models to automatically capture the similarity between predicates across two KGs. For entity embeddings, AutoAlign first computes the entity embeddings of each KG independently using TransE, and then shifts the two KGs' entity embeddings into the same vector space by computing the similarity between entities based on their attributes. Thus, both predicate alignment and entity al
    
[^69]: 非平滑非凸优化中随机次梯度方法的收敛性保证

    Convergence Guarantees for Stochastic Subgradient Methods in Nonsmooth Nonconvex Optimization. (arXiv:2307.10053v1 [math.OC])

    [http://arxiv.org/abs/2307.10053](http://arxiv.org/abs/2307.10053)

    本文研究了非平滑非凸优化中随机次梯度方法的收敛性质，并提出了一种新的框架，证明了其在单时间尺度和双时间尺度情况下的全局收敛性，包括了多种已知的SGD类型方法。对于有限和形式的目标函数，证明了这些方法能够在随机选择的步长和初始点上找到Clarke稳定点。

    

    本文研究了随机梯度下降（SGD）方法及其变种在训练由非平滑激活函数构建的神经网络中的收敛性质。我们提出了一种新颖的框架，为更新动量项和变量的步长分配了不同的时间尺度。在一些温和的条件下，我们证明了我们提出的框架在单时间尺度和双时间尺度情况下的全局收敛性。我们还证明了我们提出的框架包含了很多已知的SGD类型方法，包括heavy-ball SGD、SignSGD、Lion、normalized SGD和clipped SGD。此外，当目标函数采用有限和形式时，我们基于我们提出的框架证明了这些SGD类型方法的收敛性质。特别地，在温和的假设下，我们证明了这些SGD类型方法在随机选择的步长和初始点上能够找到目标函数的Clarke稳定点。

    In this paper, we investigate the convergence properties of the stochastic gradient descent (SGD) method and its variants, especially in training neural networks built from nonsmooth activation functions. We develop a novel framework that assigns different timescales to stepsizes for updating the momentum terms and variables, respectively. Under mild conditions, we prove the global convergence of our proposed framework in both single-timescale and two-timescale cases. We show that our proposed framework encompasses a wide range of well-known SGD-type methods, including heavy-ball SGD, SignSGD, Lion, normalized SGD and clipped SGD. Furthermore, when the objective function adopts a finite-sum formulation, we prove the convergence properties for these SGD-type methods based on our proposed framework. In particular, we prove that these SGD-type methods find the Clarke stationary points of the objective function with randomly chosen stepsizes and initial points under mild assumptions. Preli
    
[^70]: 学习采样任务用于元学习

    Learning to Sample Tasks for Meta Learning. (arXiv:2307.08924v1 [cs.LG])

    [http://arxiv.org/abs/2307.08924](http://arxiv.org/abs/2307.08924)

    通过实验得出了三个结论：没有通用的任务采样策略能保证元学习模型的性能；任务的多样性会导致模型在训练过程中出现欠拟合或过拟合的问题；模型的泛化性能受到任务的差异、任务熵和任务难度的影响。针对这些发现，提出了一种新颖的任务采样器ASr，它利用任务的差异、任务熵和任务难度来采样任务，并通过重新思考和提出一个简单而通用的元学习算法来优化ASr。大量实证实验表明了ASr的有效性。

    

    通过对各种元学习方法、任务采样器和少样本学习任务的实验，本文得出了三个结论。首先，没有通用的任务采样策略能保证元学习模型的性能。其次，任务的多样性会导致模型在训练过程中出现欠拟合或过拟合的问题。最后，模型的泛化性能受到任务的差异、任务熵和任务难度的影响。针对这些发现，我们提出了一种新颖的任务采样器，称为自适应采样器（ASr）。ASr是一个即插即用的任务采样器，它利用任务的差异、任务熵和任务难度来采样任务。为了优化ASr，我们重新思考并提出了一个简单而通用的元学习算法。最后，大量的实证实验证明了所提出的ASr的有效性。

    Through experiments on various meta-learning methods, task samplers, and few-shot learning tasks, this paper arrives at three conclusions. Firstly, there are no universal task sampling strategies to guarantee the performance of meta-learning models. Secondly, task diversity can cause the models to either underfit or overfit during training. Lastly, the generalization performance of the models are influenced by task divergence, task entropy, and task difficulty. In response to these findings, we propose a novel task sampler called Adaptive Sampler (ASr). ASr is a plug-and-play task sampler that takes task divergence, task entropy, and task difficulty to sample tasks. To optimize ASr, we rethink and propose a simple and general meta-learning algorithm. Finally, a large number of empirical experiments demonstrate the effectiveness of the proposed ASr.
    
[^71]: 元价值学习：一种带有学习意识的学习通用框架

    Meta-Value Learning: a General Framework for Learning with Learning Awareness. (arXiv:2307.08863v1 [cs.LG])

    [http://arxiv.org/abs/2307.08863](http://arxiv.org/abs/2307.08863)

    元价值学习是一种带有学习意识的学习通用框架，通过分析智能体学习过程的相互作用，使用元价值函数来指导优化，并通过训练神经网络进行逼近，从而提供更可靠的改进方向。

    

    多智能体系统中的梯度学习很困难，因为梯度来自于一个一阶模型，不考虑智能体学习过程之间的相互作用。我们扩展了LOLA的思想，并开发了一种完全通用的基于价值的优化方法。核心思想是一个称为元价值的函数，它在联合策略空间的每个点上，为每个智能体给出其未来优化步骤中目标的折扣总和。我们认为，元价值的梯度比原始目标的梯度更可靠的改进方向，因为元价值来自对优化效果的经验观察。我们展示了如何通过训练神经网络来近似元价值，以沿着智能体沿着元价值梯度的优化轨迹进行TD误差最小化。我们分析了我们方法的行为。

    Gradient-based learning in multi-agent systems is difficult because the gradient derives from a first-order model which does not account for the interaction between agents' learning processes. LOLA (arXiv:1709.04326) accounts for this by differentiating through one step of optimization. We extend the ideas of LOLA and develop a fully-general value-based approach to optimization. At the core is a function we call the meta-value, which at each point in joint-policy space gives for each agent a discounted sum of its objective over future optimization steps. We argue that the gradient of the meta-value gives a more reliable improvement direction than the gradient of the original objective, because the meta-value derives from empirical observations of the effects of optimization. We show how the meta-value can be approximated by training a neural network to minimize TD error along optimization trajectories in which agents follow the gradient of the meta-value. We analyze the behavior of our
    
[^72]: 探索从替代训练中理解对抗性可转移性

    Towards Understanding Adversarial Transferability From Surrogate Training. (arXiv:2307.07873v1 [cs.LG])

    [http://arxiv.org/abs/2307.07873](http://arxiv.org/abs/2307.07873)

    本论文探索了对抗性可转移性的理解，特别关注替代训练。通过研究模型的平滑性和梯度相似性之间的权衡，发现对抗训练可以提高模型的替代能力。研究结果对数据分布的转变提出了新的推测。

    

    对DNNs的对抗样本(AEs)已经表明是可转移的：成功欺骗白盒子替代模型的AEs也可以欺骗具有不同架构的其他黑盒模型。虽然许多经验研究提供了生成高度可转移AE的指导，但这些研究缺乏解释甚至导致不一致的建议。本文在理解对抗性可转移性方面迈出了一步，特别关注替代方面。从着名的小健壮性现象开始，通过以轻微扰动的对抗性样本对模型进行对抗训练可以得到更好的替代模型，我们将其归因于两个主要因素之间的权衡：模型的平滑性和梯度相似性。我们的研究集中在它们的共同效果上，而不是它们与可转移性的单独相关性。通过一系列理论和实证分析，我们推测数据分布的转变。

    Adversarial examples (AEs) for DNNs have been shown to be transferable: AEs that successfully fool white-box surrogate models can also deceive other black-box models with different architectures. Although a bunch of empirical studies have provided guidance on generating highly transferable AEs, many of these findings lack explanations and even lead to inconsistent advice. In this paper, we take a further step towards understanding adversarial transferability, with a particular focus on surrogate aspects. Starting from the intriguing little robustness phenomenon, where models adversarially trained with mildly perturbed adversarial samples can serve as better surrogates, we attribute it to a trade-off between two predominant factors: model smoothness and gradient similarity. Our investigations focus on their joint effects, rather than their separate correlations with transferability. Through a series of theoretical and empirical analyses, we conjecture that the data distribution shift in
    
[^73]: 深度网络逼近：从ReLU到多种激活函数

    Deep Network Approximation: Beyond ReLU to Diverse Activation Functions. (arXiv:2307.06555v1 [cs.LG])

    [http://arxiv.org/abs/2307.06555](http://arxiv.org/abs/2307.06555)

    本文研究了深度神经网络在多种激活函数下的表达能力，证明了可以通过在有界集合上构建一个宽度为6N、深度为2L的varrho激活网络来逼近一个宽度为N、深度为L的ReLU网络，从而将对ReLU网络的逼近结果推广到其他激活函数。

    

    本文探究了深度神经网络在多种激活函数下的表达能力。定义了一个激活函数集合A，包括大多数常用的激活函数，如ReLU、LeakyReLU、ReLU^2、ELU、SELU、Softplus、GELU、SiLU、Swish、Mish、Sigmoid、Tanh、Arctan、Softsign、dSiLU和SRS。我们证明了对于任意激活函数varrho∈A，可以通过一个宽度为6N、深度为2L的varrho激活网络在有界集合上以任意精度逼近一个宽度为N、深度为L的ReLU网络。这一发现使得大部分对于ReLU网络的逼近结果能够推广到其他激活函数，尽管需要稍大的常数代价。

    This paper explores the expressive power of deep neural networks for a diverse range of activation functions. An activation function set $\mathscr{A}$ is defined to encompass the majority of commonly used activation functions, such as $\mathtt{ReLU}$, $\mathtt{LeakyReLU}$, $\mathtt{ReLU}^2$, $\mathtt{ELU}$, $\mathtt{SELU}$, $\mathtt{Softplus}$, $\mathtt{GELU}$, $\mathtt{SiLU}$, $\mathtt{Swish}$, $\mathtt{Mish}$, $\mathtt{Sigmoid}$, $\mathtt{Tanh}$, $\mathtt{Arctan}$, $\mathtt{Softsign}$, $\mathtt{dSiLU}$, and $\mathtt{SRS}$. We demonstrate that for any activation function $\varrho\in \mathscr{A}$, a $\mathtt{ReLU}$ network of width $N$ and depth $L$ can be approximated to arbitrary precision by a $\varrho$-activated network of width $6N$ and depth $2L$ on any bounded set. This finding enables the extension of most approximation results achieved with $\mathtt{ReLU}$ networks to a wide variety of other activation functions, at the cost of slightly larger constants.
    
[^74]: 脱机强化学习中的离散策略的扩散策略

    Diffusion Policies for Out-of-Distribution Generalization in Offline Reinforcement Learning. (arXiv:2307.04726v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.04726](http://arxiv.org/abs/2307.04726)

    该论文介绍了一种名为状态重构扩散策略 (SRDP) 的新方法，该方法在最新的扩散策略类中引入了状态重构特征学习，以解决脱机强化学习中的分布偏移和有效表示策略的问题。

    

    脱机强化学习 (RL) 方法利用以前的经验来学习比用于数据收集的行为策略更好的策略。与行为克隆相反，行为克隆假设数据是从专家演示中收集的，而脱机 RL 可以使用非专家数据和多模态行为策略。然而，脱机 RL 算法在处理分布偏移和有效表示策略方面面临挑战，因为训练过程中缺乏在线交互。先前关于脱机 RL 的工作使用条件扩散模型来表示数据集中的多模态行为。然而，这些方法并没有针对缓解脱机分布状态泛化而制定。我们介绍了一种新的方法，名为状态重构扩散策略 (SRDP)，将状态重构特征学习纳入到最新的扩散策略类中，以解决脱机分布通用化问题。状态重构损失促进了更详细的描述。

    Offline Reinforcement Learning (RL) methods leverage previous experiences to learn better policies than the behavior policy used for data collection. In contrast to behavior cloning, which assumes the data is collected from expert demonstrations, offline RL can work with non-expert data and multimodal behavior policies. However, offline RL algorithms face challenges in handling distribution shifts and effectively representing policies due to the lack of online interaction during training. Prior work on offline RL uses conditional diffusion models to represent multimodal behavior in the dataset. Nevertheless, these methods are not tailored toward alleviating the out-of-distribution state generalization. We introduce a novel method, named State Reconstruction for Diffusion Policies (SRDP), incorporating state reconstruction feature learning in the recent class of diffusion policies to address the out-of-distribution generalization problem. State reconstruction loss promotes more descript
    
[^75]: 基于群体的鲁棒性：现实世界中定制鲁棒性的通用框架

    Group-based Robustness: A General Framework for Customized Robustness in the Real World. (arXiv:2306.16614v1 [cs.LG])

    [http://arxiv.org/abs/2306.16614](http://arxiv.org/abs/2306.16614)

    本研究提出了一种基于群体的鲁棒性指标，可以更好地评估机器学习模型在现实世界中抵抗攻击的能力，弥补了传统指标的不足。实验证明，该指标能够区分模型对特定威胁的脆弱性。

    

    众所周知，机器学习模型容易受到逃避攻击的影响，即通过扰动模型输入来引起错误分类。本研究中，我们发现传统的度量目标和非目标鲁棒性的指标无法准确评估现实世界中的真实威胁。为了解决现有方法的缺陷，我们正式定义了一种新的指标，称为基于群体的鲁棒性，它补充了现有的度量标准，并更适合评估特定攻击场景下的模型性能。我们通过实验证明，基于群体的鲁棒性能够在传统的鲁棒性指标不适用的情况下区分模型对特定威胁模型的脆弱性。此外，为了有效准确地衡量基于群体的鲁棒性，我们提出了两个损失函数。

    Machine-learning models are known to be vulnerable to evasion attacks that perturb model inputs to induce misclassifications. In this work, we identify real-world scenarios where the true threat cannot be assessed accurately by existing attacks. Specifically, we find that conventional metrics measuring targeted and untargeted robustness do not appropriately reflect a model's ability to withstand attacks from one set of source classes to another set of target classes. To address the shortcomings of existing methods, we formally define a new metric, termed group-based robustness, that complements existing metrics and is better-suited for evaluating model performance in certain attack scenarios. We show empirically that group-based robustness allows us to distinguish between models' vulnerability against specific threat models in situations where traditional robustness metrics do not apply. Moreover, to measure group-based robustness efficiently and accurately, we 1) propose two loss func
    
[^76]: 南佛罗里达州水位预测的深度学习模型

    Deep Learning Models for Water Stage Predictions in South Florida. (arXiv:2306.15907v1 [cs.LG])

    [http://arxiv.org/abs/2306.15907](http://arxiv.org/abs/2306.15907)

    本论文利用深度学习模型训练代理模型，快速预测南佛罗里达州迈阿密河下游的水位，并与基于物理的模型进行比较。

    

    模拟和预测河流系统的水位对于洪水警报、水力操作和洪水减轻至关重要。在工程领域中，使用HEC-RAS、MIKE和SWMM等工具建立详细的基于物理的水文和水力计算模型来模拟整个流域，从而预测系统中任意点的水位。然而，这些基于物理的模型计算量大，尤其对于大流域和长时间模拟来说。为了解决这个问题，我们训练了几个深度学习（DL）模型作为代理模型，快速预测水位。本文以南佛罗里达州迈阿密河的下游水位为案例研究。数据集来自南佛罗里达水管理区（SFWMD）的DBHYDRO数据库，时间跨度为2010年1月1日至2020年12月31日。广泛的实验表明，DL模型的性能与基于物理的模型相当。

    Simulating and predicting water levels in river systems is essential for flood warnings, hydraulic operations, and flood mitigations. In the engineering field, tools such as HEC-RAS, MIKE, and SWMM are used to build detailed physics-based hydrological and hydraulic computational models to simulate the entire watershed, thereby predicting the water stage at any point in the system. However, these physics-based models are computationally intensive, especially for large watersheds and for longer simulations. To overcome this problem, we train several deep learning (DL) models for use as surrogate models to rapidly predict the water stage. The downstream stage of the Miami River in South Florida is chosen as a case study for this paper. The dataset is from January 1, 2010, to December 31, 2020, downloaded from the DBHYDRO database of the South Florida Water Management District (SFWMD). Extensive experiments show that the performance of the DL models is comparable to that of the physics-bas
    
[^77]: MeciFace：基于肌肉电和惯性融合的边缘实时识别面部表情和进食活动眼镜

    MeciFace: Mechanomyography and Inertial Fusion based Glasses for Edge Real-Time Recognition of Facial and Eating Activities. (arXiv:2306.13674v1 [cs.CV])

    [http://arxiv.org/abs/2306.13674](http://arxiv.org/abs/2306.13674)

    MeciFace是一款注重隐私且低功耗的可穿戴设备，它采用轻量级卷积神经网络来监测面部表情和进食活动，面部表情案例的F1分数达到了86％，饮食监测则达到了90％的F1分数。

    

    我们提出了MeciFace，这是一个低功耗（0.55瓦），注重隐私，实时边缘监测（RTE）的可穿戴解决方案，具有微小的内存占用（11-19 KB），旨在监测面部表情和进食活动。我们采用轻量级卷积神经网络作为面部和进食场景的主干模型。该系统在面部表情案例的RTE评估中产生了86％的F1分数。此外，我们对未知用户的RTE进行饮食监测，得到了90％的F1分数。

    We present MeciFace, a low-power (0.55 Watts), privacy-conscious, real-time on-the-edge (RTE) wearable solution with a tiny memory footprint (11-19 KB), designed to monitor facial expressions and eating activities. We employ lightweight convolutional neural networks as the backbone models for both facial and eating scenarios. The system yielded an F1-score of 86% for the RTE evaluation in the facial expression case. In addition, we obtained an F1-score of 90% for eating/drinking monitoring for the RTE of an unseen user.
    
[^78]: 利用预训练的自监督前端实现歌唱声音自动理解任务：三个案例研究

    Toward Leveraging Pre-Trained Self-Supervised Frontends for Automatic Singing Voice Understanding Tasks: Three Case Studies. (arXiv:2306.12714v1 [cs.SD])

    [http://arxiv.org/abs/2306.12714](http://arxiv.org/abs/2306.12714)

    本文研究了利用自监督学习模型（SSL模型）进行歌唱声音理解任务的有效性，并展示了将自监督前端转移到目标任务可以取得更好性能的潜力。此外，SSL模型在所有任务中均优于常规监督学习模型。

    

    采用深度学习技术的数据驱动方法对自动歌唱声音理解任务，如歌手识别、歌声转录和歌唱技巧分类等方面具有表征能力，即使在具有丰富人声和噪声样本的情况下也能发挥良好的作用。然而，有限的标注数据仍然是实现令人满意的性能的重要障碍。近年来，自监督学习模型（SSL模型）在语音处理和音乐分类领域经过大量未标注数据的训练。通过微调这些模型以用于目标任务，即使在有限的训练数据下也能实现与常规监督学习相当的性能。因此，本文研究了SSL模型在各种歌唱声音识别任务中的有效性。我们报告了比较三个不同任务（即歌手识别、歌声转录和歌唱技巧分类）的SSL模型和常规监督学习模型的实验结果。我们的实验表明，SSL模型在所有任务中均优于有监督学习模型。此外，我们表明，将在大量未标注数据上训练的自监督前端转移到目标任务可以比从头开始训练取得更好的性能。这些结果说明了自监督学习在提高歌唱声音理解任务的数据效率和性能方面的潜力。

    Automatic singing voice understanding tasks, such as singer identification, singing voice transcription, and singing technique classification, benefit from data-driven approaches that utilize deep learning techniques. These approaches work well even under the rich diversity of vocal and noisy samples owing to their representation ability. However, the limited availability of labeled data remains a significant obstacle to achieving satisfactory performance. In recent years, self-supervised learning models (SSL models) have been trained using large amounts of unlabeled data in the field of speech processing and music classification. By fine-tuning these models for the target tasks, comparable performance to conventional supervised learning can be achieved with limited training data. Therefore, in this paper, we investigate the effectiveness of SSL models for various singing voice recognition tasks. We report the results of experiments comparing SSL models for three different tasks (i.e.,
    
[^79]: DynaQuant: 通过动态量化压缩深度学习训练检查点

    DynaQuant: Compressing Deep Learning Training Checkpoints via Dynamic Quantization. (arXiv:2306.11800v1 [cs.LG])

    [http://arxiv.org/abs/2306.11800](http://arxiv.org/abs/2306.11800)

    DynaQuant通过动态量化实现对各种最先进模型的显着压缩，几乎不影响模型准确性。

    

    随着深度学习训练工作量在计算资源和时间消耗方面的增加，遇到训练失败的可能性显著增加，导致工作丢失和资源浪费。最新的方法涉及有损模型压缩机制，这会在模型质量（准确性）和压缩比之间产生权衡。我们提出了一个新颖的动态量化框架，称为DynaQuant，它可以根据训练期间模型权重的灵敏度变化来更新量化级别，从而实现对各种最先进模型的显着压缩，并且几乎不影响模型准确性。

    With the increase in the scale of Deep Learning (DL) training workloads in terms of compute resources and time consumption, the likelihood of encountering in-training failures rises substantially, leading to lost work and resource wastage. Such failures are typically offset by a checkpointing mechanism, which comes at the cost of storage and network bandwidth overhead. State-of-the-art approaches involve lossy model compression mechanisms, which induce a tradeoff between the resulting model quality (accuracy) and compression ratio. Delta compression is then also used to further reduce the overhead by only storing the difference between consecutive checkpoints. We make a key enabling observation that the sensitivity of model weights to compression varies during training, and different weights benefit from different quantization levels (ranging from retaining full precision to pruning). We propose (1) a non-uniform quantization scheme that leverages this variation, (2) an efficient searc
    
[^80]: RL感知机：高维策略学习的泛化动力学

    The RL Perceptron: Generalisation Dynamics of Policy Learning in High Dimensions. (arXiv:2306.10404v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10404](http://arxiv.org/abs/2306.10404)

    本文提出了一个高维RL模型，推导出该模型的典型动力学为一组闭式ODE方程组，并通过实验与神经RL代理进行了比较，结果表明该模型捕捉了现实世界RL的关键特征。

    

    强化学习算法已被证明在许多领域中具有变革性，为了解决真实世界的问题，这些系统通常使用神经网络直接从像素或其他高维感官输入中学习策略。然而，许多强化学习理论都集中于离散状态空间或最坏情况分析，关于高维情况下策略学习的动力学基本问题仍有待解决。在这里，我们提出了一个可解的高维RL模型，它可以捕捉多种学习协议，并将其典型动力学导出为一组闭式常微分方程。我们推导出最佳的学习率和任务难度调度，类似于训练中的退火方案和课程表，并表明该模型表现出丰富的行为，包括在稀疏奖励下的延迟学习；根据奖励基线不同的各种学习方案；以及由奖励严格程度驱动的速度-准确度权衡。与神经RL代理进行的实验比较表明，该模型捕捉了现实世界RL的关键特征。

    Reinforcement learning (RL) algorithms have proven transformative in a range of domains. To tackle real-world domains, these systems often use neural networks to learn policies directly from pixels or other high-dimensional sensory input. By contrast, much theory of RL has focused on discrete state spaces or worst-case analysis, and fundamental questions remain about the dynamics of policy learning in high-dimensional settings. Here, we propose a solvable high-dimensional model of RL that can capture a variety of learning protocols, and derive its typical dynamics as a set of closed-form ordinary differential equations (ODEs). We derive optimal schedules for the learning rates and task difficulty - analogous to annealing schemes and curricula during training in RL - and show that the model exhibits rich behaviour, including delayed learning under sparse rewards; a variety of learning regimes depending on reward baselines; and a speed-accuracy trade-off driven by reward stringency. Expe
    
[^81]: 基于现实表演的面部动画风格感知非监督学习

    Unsupervised Learning of Style-Aware Facial Animation from Real Acting Performances. (arXiv:2306.10006v1 [cs.CV])

    [http://arxiv.org/abs/2306.10006](http://arxiv.org/abs/2306.10006)

    本文提出了一种非监督学习的动画方法，可以通过文本或语音输入，实现基于真实动作表演的面部动画，并且可以不同程度地学习并合成不同的表演风格。

    

    本文提出了一种新的方法，基于混合形状几何、动态纹理和神经渲染，用于从真实动作表演中驱动面部模型的文本/语音动画。通过训练包括形状和纹理的VAE，我们得到了一个参数化模型，以精确捕捉和逼真合成潜在特征向量中的面部表情。我们的动画方法基于条件卷积神经网络，将文本或语音转换为一系列动画参数。与以往的方法不同，我们的动画模型以非监督的方式学习区分和合成不同的表演风格，只需要用于描述训练序列内容的语音标签。为了实现逼真的实时渲染，我们训练了一个U-Net，通过计算改进的像素颜色和前景遮罩来改善栅格化渲染。我们定性/定量地将我们的框架与最近的头部建模方法以及面部动画方法进行比较，并评估感知渲染/动画效果。

    This paper presents a novel approach for text/speech-driven animation of a photo-realistic head model based on blend-shape geometry, dynamic textures, and neural rendering. Training a VAE for geometry and texture yields a parametric model for accurate capturing and realistic synthesis of facial expressions from a latent feature vector. Our animation method is based on a conditional CNN that transforms text or speech into a sequence of animation parameters. In contrast to previous approaches, our animation model learns disentangling/synthesizing different acting-styles in an unsupervised manner, requiring only phonetic labels that describe the content of training sequences. For realistic real-time rendering, we train a U-Net that refines rasterization-based renderings by computing improved pixel colors and a foreground matte. We compare our framework qualitatively/quantitatively against recent methods for head modeling as well as facial animation and evaluate the perceived rendering/ani
    
[^82]: 多视角分类增量学习

    Multi-View Class Incremental Learning. (arXiv:2306.09675v1 [cs.LG])

    [http://arxiv.org/abs/2306.09675](http://arxiv.org/abs/2306.09675)

    本文提出了一种名为多视角分类增量学习（MVCIL）的新模型，该模型使用随机化的表示学习技术进行特征提取，并提出正交融合子空间和选择性权重合并来解决增量学习中遗忘旧信息和学习新概念的挑战。实验结果表明该方法相比最新方法有效性更高。

    

    多视角学习（MVL）在整合数据集的多个视角以提高下游任务性能方面取得了巨大成功。为了使MVL方法在开放式环境中更实用，本文研究了一种新的范例，称为多视角分类增量学习（MVCIL），其中单个模型从连续的视图流中逐步分类新类，不需要访问早期数据的视图。但是，MVCIL面临着老信息的灾难性遗忘和学习新概念的干扰。为了解决这个问题，我们首先开发了一种基于随机化的表示学习技术，用于特征提取，以保证它们在工作状态下的分离视图最优，其中属于类的多个视图按顺序呈现；然后，我们将它们逐个集成到由提取的特征跨越的正交融合子空间中；最后，我们介绍选择性权重合并，以保留旧类的知识。基准数据集上的实验结果证明了我们提出的方法相对于最新方法的有效性。

    Multi-view learning (MVL) has gained great success in integrating information from multiple perspectives of a dataset to improve downstream task performance. To make MVL methods more practical in an open-ended environment, this paper investigates a novel paradigm called multi-view class incremental learning (MVCIL), where a single model incrementally classifies new classes from a continual stream of views, requiring no access to earlier views of data. However, MVCIL is challenged by the catastrophic forgetting of old information and the interference with learning new concepts. To address this, we first develop a randomization-based representation learning technique serving for feature extraction to guarantee their separate view-optimal working states, during which multiple views belonging to a class are presented sequentially; Then, we integrate them one by one in the orthogonality fusion subspace spanned by the extracted features; Finally, we introduce selective weight consolidation f
    
[^83]: SCALE: 提升高级语言模型评估的复杂性

    SCALE: Scaling up the Complexity for Advanced Language Model Evaluation. (arXiv:2306.09237v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.09237](http://arxiv.org/abs/2306.09237)

    该论文提出了一个新颖的自然语言处理基准测试，挑战当前大型语言模型在处理长文档、利用领域专业知识、多语言理解和多任务处理方面的能力。基准测试包含瑞士法律系统的多样化法律NLP数据集，允许进行对底层非英语、固有多语言的法律系统进行全面研究。

    

    最近在大型语言模型（LLM）方面取得的进展已经饱和了许多自然语言处理基准测试（包括专业领域的基准测试），强调了需要新颖、更具挑战性的测试来正确评估LLM的能力。在本文中，我们引入了一个新颖的自然语言处理基准测试，对当前LLM的四个关键方面提出了挑战：处理长文档（多达50K个标记）、利用领域专业知识（体现在法律文本中）、多语言理解（涵盖五种语言）和多任务处理（包括法律文件到文件信息检索、法庭视图生成、重要决策摘要、引用提取和八个具有挑战性的文本分类任务）。我们的基准测试包含了来自瑞士法律系统的多样的法律NLP数据集，可以对底层非英语、固有多语言的联邦法律系统进行全面研究。尽管最近取得了进展，但对于强烈的审查/分析任务，高效地处理长文档仍然是一个挑战。

    Recent strides in Large Language Models (LLMs) have saturated many NLP benchmarks (even professional domain-specific ones), emphasizing the need for novel, more challenging novel ones to properly assess LLM capabilities. In this paper, we introduce a novel NLP benchmark that poses challenges to current LLMs across four key dimensions: processing long documents (up to 50K tokens), utilizing domain specific knowledge (embodied in legal texts), multilingual understanding (covering five languages), and multitasking (comprising legal document to document Information Retrieval, Court View Generation, Leading Decision Summarization, Citation Extraction, and eight challenging Text Classification tasks). Our benchmark comprises diverse legal NLP datasets from the Swiss legal system, allowing for a comprehensive study of the underlying Non-English, inherently multilingual, federal legal system. Despite recent advances, efficiently processing long documents for intense review/analysis tasks remai
    
[^84]: 内核随机投影深度用于离群点检测

    Kernel Random Projection Depth for Outlier Detection. (arXiv:2306.07056v1 [stat.ML])

    [http://arxiv.org/abs/2306.07056](http://arxiv.org/abs/2306.07056)

    本文提出了一种内核随机投影深度方法，用于处理数据云中的多模式和非凸性，实验结果表明在基准数据集上表现优异。

    

    本文提出了一种扩展的随机投影深度（RPD）方法，用于处理数据云中的多模式和非凸性。在所提出的方法的框架中，RPD在再现核希尔伯特空间中计算。借助内核主成分分析，我们期望所提出的方法可以处理上述多种模式和非凸性。实验结果表明，所提出的方法优于RPD，并可与基准数据集上现有的检测模型相媲美，关于接收操作特征曲线（ROC）下的曲线下面积（AUC）。

    This paper proposes an extension of Random Projection Depth (RPD) to cope with multiple modalities and non-convexity on data clouds. In the framework of the proposed method, the RPD is computed in a reproducing kernel Hilbert space. With the help of kernel principal component analysis, we expect that the proposed method can cope with the above multiple modalities and non-convexity. The experimental results demonstrate that the proposed method outperforms RPD and is comparable to other existing detection models on benchmark datasets regarding Area Under the Curves (AUCs) of Receiver Operating Characteristic (ROC).
    
[^85]: 提高决策树解释性的有效性

    Improving the Validity of Decision Trees as Explanations. (arXiv:2306.06777v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.06777](http://arxiv.org/abs/2306.06777)

    该论文介绍了一个新的决策树模型，利用挂起的树的方式提高了其解释性和统计性能，达到了无限深度决策树的水平，并可与XGBoost等最先进的方法相媲美。

    

    在基于表格数据的分类和预测中，人们经常使用基于树的模型。这可以在表格数据上与深度神经网络竞争[参见Grinsztajn等人，NeurIPS 2022，arXiv：2207.08815]，并且在某些条件下是可解释的。可解释性取决于树的深度和每个叶节点的准确性。在这里，我们训练了一个低深度的树，其目标是最小化每个叶节点上的最大错误分类，并从低深度树的每个叶节点“挂起”进一步的基于树的模型（例如无限深度的树）。低深度树易于解释，而综合低深度和挂起的基于树的模型的整体统计性能优于使用经典方法（例如CART）训练的无限深度决策树，并且与最先进的方法（例如优化的XGBoost）相当。

    In classification and forecasting with tabular data, one often utilizes tree-based models. This can be competitive with deep neural networks on tabular data [cf. Grinsztajn et al., NeurIPS 2022, arXiv:2207.08815] and, under some conditions, explainable. The explainability depends on the depth of the tree and the accuracy in each leaf of the tree. Here, we train a low-depth tree with the objective of minimising the maximum misclassification error across each leaf node, and then ``suspend'' further tree-based models (e.g., trees of unlimited depth) from each leaf of the low-depth tree. The low-depth tree is easily explainable, while the overall statistical performance of the combined low-depth and suspended tree-based models improves upon decision trees of unlimited depth trained using classical methods (e.g., CART) and is comparable to state-of-the-art methods (e.g., well-tuned XGBoost).
    
[^86]: 基于Transformer的多级多示例学习用于全扫描图像分类

    Multi-level Multiple Instance Learning with Transformer for Whole Slide Image Classification. (arXiv:2306.05029v1 [cs.CV])

    [http://arxiv.org/abs/2306.05029](http://arxiv.org/abs/2306.05029)

    本文提出了一种基于Transformer的多级多示例学习（MMIL-Transformer）方法，该方法能够有效地处理涉及大量实例的MIL任务。

    

    全扫描图像（WSI）是一种高分辨率的组织扫描图像，被广泛用于计算机辅助诊断（CAD）。由于极高的分辨率和区域级别注释的有限性，对于基于WSI的数字诊断使用深度学习方法是具有挑战性的。多示例学习（MIL）是解决弱注释问题的强有力工具，而Transformer已在视觉任务领域表现出了巨大的成功。结合两者将为基于深度学习的图像诊断提供新的见解。然而，由于单级MIL的限制和注意机制对序列长度的限制，将Transformer直接应用于基于WSI的MIL任务并不实用。为了解决这个问题，我们提出了一种多级MIL与Transformer（MMIL-Transformer）方法。通过引入层次结构到MIL中，这种方法能够有效地处理涉及大量实例的MIL任务。

    Whole slide image (WSI) refers to a type of high-resolution scanned tissue image, which is extensively employed in computer-assisted diagnosis (CAD). The extremely high resolution and limited availability of region-level annotations make it challenging to employ deep learning methods for WSI-based digital diagnosis. Multiple instance learning (MIL) is a powerful tool to address the weak annotation problem, while Transformer has shown great success in the field of visual tasks. The combination of both should provide new insights for deep learning based image diagnosis. However, due to the limitations of single-level MIL and the attention mechanism's constraints on sequence length, directly applying Transformer to WSI-based MIL tasks is not practical. To tackle this issue, we propose a Multi-level MIL with Transformer (MMIL-Transformer) approach. By introducing a hierarchical structure to MIL, this approach enables efficient handling of MIL tasks that involve a large number of instances.
    
[^87]: AnoOnly:无需损失正常数据的半监督异常检测

    AnoOnly: Semi-Supervised Anomaly Detection without Loss on Normal Data. (arXiv:2305.18798v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18798](http://arxiv.org/abs/2305.18798)

    AnoOnly是一个新的半监督异常检测框架，通过引入一种对正常数据的弱监督形式来解决同质数据对异常的影响，以实现平衡的监督。该框架在各种模型和数据集上表现出了显著的性能提升，达到了新的最佳性能。

    

    半监督异常检测(SSAD)方法通过利用少量但有指导作用的异常实例，增强了无监督异常检测(UAD)的效果。然而，同质正常数据对异常的统治使得SSAD模型无法有效地感知异常。为了解决这个问题并在严重不平衡的正常和异常数据之间实现平衡的监督，我们开发了一个名为AnoOnly(仅异常)的新框架。与现有的SSAD方法不同，AnoOnly暂停了严格的损失监督，引入了一种对正常数据的弱监督形式。这种弱监督通过批量归一化实现，隐式地对正常数据进行聚类学习。当集成到现有的SSAD方法中时，所提出的AnoOnly在各种模型和数据集上展示了显著的性能提升，达到了新的最佳性能。此外，我们的A

    Semi-supervised anomaly detection (SSAD) methods have demonstrated their effectiveness in enhancing unsupervised anomaly detection (UAD) by leveraging few-shot but instructive abnormal instances. However, the dominance of homogeneous normal data over anomalies biases the SSAD models against effectively perceiving anomalies. To address this issue and achieve balanced supervision between heavily imbalanced normal and abnormal data, we develop a novel framework called AnoOnly (Anomaly Only). Unlike existing SSAD methods that resort to strict loss supervision, AnoOnly suspends it and introduces a form of weak supervision for normal data. This weak supervision is instantiated through the utilization of batch normalization, which implicitly performs cluster learning on normal data. When integrated into existing SSAD methods, the proposed AnoOnly demonstrates remarkable performance enhancements across various models and datasets, achieving new state-of-the-art performance. Additionally, our A
    
[^88]: 基于虚拟粒子随机逼近的可证速限制变种的SVGD算法。

    Provably Fast Finite Particle Variants of SVGD via Virtual Particle Stochastic Approximation. (arXiv:2305.17558v1 [stat.ML])

    [http://arxiv.org/abs/2305.17558](http://arxiv.org/abs/2305.17558)

    本论文提出了两种基于虚拟粒子随机逼近的可证速限制变种的SVGD算法，具有可证速的有限粒子收敛率。

    

    Stein变分梯度下降（SVGD）是一种流行的变分推断算法，它模拟相互作用的粒子系统以近似从目标分布中采样，具有各种领域的令人印象深刻的经验性能。在理论上，它的群体（即，无限粒子）极限动力学已经得到了很好的研究，但是SVGD在有限粒子体制下的行为则不太清楚。在这项工作中，我们设计了两种计算效率高的SVGD变体，即VP-SVGD（从概念上讲很优雅）和GB-SVGD（从经验上看很有效），具有可证速的有限粒子收敛率。我们引入了“虚拟粒子”的概念，并在概率测度空间中开发了人口极限SVGD动力学的新型随机逼近方法，它们可以使用有限数量的粒子精确实现。我们的算法可以看作是SVGD的特定随机批处理逼近，比普通方法更具计算效率。

    Stein Variational Gradient Descent (SVGD) is a popular variational inference algorithm which simulates an interacting particle system to approximately sample from a target distribution, with impressive empirical performance across various domains. Theoretically, its population (i.e, infinite-particle) limit dynamics is well studied but the behavior of SVGD in the finite-particle regime is much less understood. In this work, we design two computationally efficient variants of SVGD, namely VP-SVGD (which is conceptually elegant) and GB-SVGD (which is empirically effective), with provably fast finite-particle convergence rates. We introduce the notion of \emph{virtual particles} and develop novel stochastic approximations of population-limit SVGD dynamics in the space of probability measures, which are exactly implementable using a finite number of particles. Our algorithms can be viewed as specific random-batch approximations of SVGD, which are computationally more efficient than ordinar
    
[^89]: 基于 Trend 和 Seasonality 分解和 LightGBM 的销售预测改进

    Improved Sales Forecasting using Trend and Seasonality Decomposition with LightGBM. (arXiv:2305.17201v1 [cs.LG])

    [http://arxiv.org/abs/2305.17201](http://arxiv.org/abs/2305.17201)

    本文提出了一种根据趋势和季节性分量在时间序列上的独特影响指标进行时间序列分组的新方法，并采用 LightGBM 模型进行预测，在沃尔玛销售数据上实现了较高的预测精度。

    

    针对沃尔玛和亚马逊等大型零售商销售预测的难点，本文提出了一种新的方法，即根据趋势和季节性分量在时间序列上的独特影响指标进行时间序列分组，并采用 LightGBM 模型进行预测。实验结果表明，该分组方法可以提高预测精度，相较于传统时间序列模型和其他机器学习模型，MAPE（平均绝对百分比误差）在测试集上可达 4.49%。

    Retail sales forecasting presents a significant challenge for large retailers such as Walmart and Amazon, due to the vast assortment of products, geographical location heterogeneity, seasonality, and external factors including weather, local economic conditions, and geopolitical events. Various methods have been employed to tackle this challenge, including traditional time series models, machine learning models, and neural network mechanisms, but the difficulty persists. Categorizing data into relevant groups has been shown to improve sales forecast accuracy as time series from different categories may exhibit distinct patterns. In this paper, we propose a new measure to indicate the unique impacts of the trend and seasonality components on a time series and suggest grouping time series based on this measure. We apply this approach to Walmart sales data from 01/29/2011 to 05/22/2016 and generate sales forecasts from 05/23/2016 to 06/19/2016. Our experiments show that the proposed strat
    
[^90]: 优化的自定义数据集用于高效检测水下垃圾

    Optimized Custom Dataset for Efficient Detection of Underwater Trash. (arXiv:2305.16460v1 [cs.CV])

    [http://arxiv.org/abs/2305.16460](http://arxiv.org/abs/2305.16460)

    本文提出了一种自定义数据集和有效检测方法，旨在通过增加垃圾实例的多样性，在深入水下环境中提高其检测精度。

    

    准确评估和清除潜在的水下废物对于保护海洋生物和环境至关重要。本文针对水下垃圾检测所存在的挑战，如光折射、吸收、悬浮颗粒和色彩扭曲等因素，提出了一种自定义数据集和有效检测方法。该数据集涵盖了多种水下环境，并包括对废弃物实例的精确定位标注。最终，使用最先进的深度学习结构，目的是通过增加垃圾实例的多样性，在深入水下环境中提高其检测精度。

    Accurately quantifying and removing submerged underwater waste plays a crucial role in safeguarding marine life and preserving the environment. While detecting floating and surface debris is relatively straightforward, quantifying submerged waste presents significant challenges due to factors like light refraction, absorption, suspended particles, and color distortion. This paper addresses these challenges by proposing the development of a custom dataset and an efficient detection approach for submerged marine debris. The dataset encompasses diverse underwater environments and incorporates annotations for precise labeling of debris instances. Ultimately, the primary objective of this custom dataset is to enhance the diversity of litter instances and improve their detection accuracy in deep submerged environments by leveraging state-of-the-art deep learning architectures.
    
[^91]: 用于雷达目标检测的点云多尺度网格渲染的改进

    Improved Multi-Scale Grid Rendering of Point Clouds for Radar Object Detection Networks. (arXiv:2305.15836v1 [cs.CV])

    [http://arxiv.org/abs/2305.15836](http://arxiv.org/abs/2305.15836)

    本文提出了一种新的体系结构，即多尺度 KPPillarsBEV，以缓解雷达目标检测中从点云数据转化为网格结构过程中的信息丢失问题，并提出了一种新的网格渲染方法 KPBEV。实验结果表明，该方法显著优于现有方法。

    

    将点云转换为网格表征，然后应用卷积神经网络的结构可用于雷达目标检测，但从不规则的点云数据转换为密集的网格结构常常会导致信息的丢失，这是由于点的离散化和聚合造成的。本文提出了一种新的体系结构，即多尺度 KPPillarsBEV，以缓解网格渲染的负面影响。具体而言，我们提出了一种新的网格渲染方法 KPBEV，它利用核点卷积的描述能力来改进网格渲染过程中局部点云上下文的编码。此外，我们还提出了一种通用的多尺度网格渲染公式，以任意网格渲染方法将多尺度特征映射融合到检测网络的卷积骨干中。我们在 nuScenes 数据集上进行了大量实验，并评估了检测汽车、卡车和公交车方法的性能。结果表明，我们提出的多尺度 KPPillarsBEV 结构和 KPBEV 网格渲染方法在性能上优于现有的方法。

    Architectures that first convert point clouds to a grid representation and then apply convolutional neural networks achieve good performance for radar-based object detection. However, the transfer from irregular point cloud data to a dense grid structure is often associated with a loss of information, due to the discretization and aggregation of points. In this paper, we propose a novel architecture, multi-scale KPPillarsBEV, that aims to mitigate the negative effects of grid rendering. Specifically, we propose a novel grid rendering method, KPBEV, which leverages the descriptive power of kernel point convolutions to improve the encoding of local point cloud contexts during grid rendering. In addition, we propose a general multi-scale grid rendering formulation to incorporate multi-scale feature maps into convolutional backbones of detection networks with arbitrary grid rendering methods. We perform extensive experiments on the nuScenes dataset and evaluate the methods in terms of dete
    
[^92]: 论进化磨锋、平坦极小和泛化

    On progressive sharpening, flat minima and generalisation. (arXiv:2305.14683v1 [cs.LG])

    [http://arxiv.org/abs/2305.14683](http://arxiv.org/abs/2305.14683)

    本文提出了一种用损失黑塞矩阵和输入-输出雅克比矩阵联系起来的假设，量化了模型的输入-输出雅克比矩阵近似其在数据分布上的利普西茨范数的程度，并推导出了一个基于经验雅克比矩阵的新的泛化界，给出了关于进化磨锋和平坦极小的泛化性质的新解释。

    

    我们提出了一种新的方法来理解深度学习中损失曲率与泛化之间的关系。具体来说，我们利用现有的深度网络损失黑塞矩阵频谱经验分析，提出了一个将损失黑塞矩阵和深度神经网络的输入-输出雅克比矩阵联系起来的假设。然后，我们证明了一系列理论结果，量化了模型的输入-输出雅克比矩阵近似其在数据分布上的利普西茨范数的程度，并推导出了一个基于经验雅克比矩阵的新的泛化界。我们利用我们的假设和理论结果，给出了关于最近观察到的进化磨锋现象以及平坦极小的泛化性质的新描述。实验证据验证了我们的主张。

    We present a new approach to understanding the relationship between loss curvature and generalisation in deep learning. Specifically, we use existing empirical analyses of the spectrum of deep network loss Hessians to ground an ansatz tying together the loss Hessian and the input-output Jacobian of a deep neural network. We then prove a series of theoretical results which quantify the degree to which the input-output Jacobian of a model approximates its Lipschitz norm over a data distribution, and deduce a novel generalisation bound in terms of the empirical Jacobian. We use our ansatz, together with our theoretical results, to give a new account of the recently observed progressive sharpening phenomenon, as well as the generalisation properties of flat minima. Experimental evidence is provided to validate our claims.
    
[^93]: 基于梯度说明的表示法去偏见模型

    Model Debiasing via Gradient-based Explanation on Representation. (arXiv:2305.12178v1 [cs.LG])

    [http://arxiv.org/abs/2305.12178](http://arxiv.org/abs/2305.12178)

    本文提出了一种新的公平性框架，通过梯度说明找到两个模型焦点进行去偏见处理，提高下游任务模型的预测性能。

    

    机器学习系统会对某些人口统计学群体产生偏见，即不公平现象。近期的解决方法是通过分离式表示学习学习潜在码（即表示法），然后丢弃与敏感属性（如性别）相关的码。但这些方法在处理现实数据（特别是非结构化数据）时，可能会遗漏代理属性（敏感属性的代理），并且受到不完全分离的影响，导致公平性能下降和下游任务中损失有用信息。本文提出了一种新的公平性框架，针对敏感属性和代理属性进行去偏见处理，提高下游任务模型的预测性能而不需要完全分离。主要思路是利用梯度说明找到两个模型焦点：1）其中一个焦点用于预测值，2）另一个焦点用于代理属性，然后对潜在码进行修正以减轻这些属性之间的相关性。

    Machine learning systems produce biased results towards certain demographic groups, known as the fairness problem. Recent approaches to tackle this problem learn a latent code (i.e., representation) through disentangled representation learning and then discard the latent code dimensions correlated with sensitive attributes (e.g., gender). Nevertheless, these approaches may suffer from incomplete disentanglement and overlook proxy attributes (proxies for sensitive attributes) when processing real-world data, especially for unstructured data, causing performance degradation in fairness and loss of useful information for downstream tasks. In this paper, we propose a novel fairness framework that performs debiasing with regard to both sensitive attributes and proxy attributes, which boosts the prediction performance of downstream task models without complete disentanglement. The main idea is to, first, leverage gradient-based explanation to find two model focuses, 1) one focus for predicti
    
[^94]: 从随机搜索到度量测度空间中的赌博学习

    From Random Search to Bandit Learning in Metric Measure Spaces. (arXiv:2305.11509v1 [cs.LG])

    [http://arxiv.org/abs/2305.11509](http://arxiv.org/abs/2305.11509)

    本文介绍了随机搜索及其性能，引入了“散射维度”的概念，描述了底层函数的状态，量化了随机搜索的性能，并证明了在无噪声和有界噪声情况下的输出分别以一定概率收敛到最优值。

    

    随机搜索是超参数优化中最常用的方法之一，对于深度学习模型的成功至关重要。尽管其性能令人惊叹，但很少有非启发式的理论用于描述其工作机制。本文给出了关于随机搜索的理论解释。我们引入了“散射维度”的概念，描述了底层函数的状态，并量化了随机搜索的性能。我们表明，当环境没有噪声时，随机搜索的输出以概率收敛到最优值，其速率为$ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \right)^{ \frac{1}{d_s} } \right) $，其中$ d_s \ge 0 $是底层函数的散射维度。当观察到的函数值受到有界的独立同分布噪声影响时，随机搜索的输出以概率收敛到最优值，速率为$ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \right)^{ \frac{2}{2+d_s} } \right) $。

    Random Search is one of the most widely-used method for Hyperparameter Optimization, and is critical to the success of deep learning models. Despite its astonishing performance, little non-heuristic theory has been developed to describe the underlying working mechanism. This paper gives a theoretical accounting of Random Search. We introduce the concept of \emph{scattering dimension} that describes the landscape of the underlying function, and quantifies the performance of random search. We show that, when the environment is noise-free, the output of random search converges to the optimal value in probability at rate $ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \right)^{ \frac{1}{d_s} } \right) $, where $ d_s \ge 0 $ is the scattering dimension of the underlying function. When the observed function values are corrupted by bounded $iid$ noise, the output of random search converges to the optimal value in probability at rate $ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \rig
    
[^95]: 球形负感知器的解空间的星形特征

    The star-shaped space of solutions of the spherical negative perceptron. (arXiv:2305.10623v1 [cond-mat.dis-nn])

    [http://arxiv.org/abs/2305.10623](http://arxiv.org/abs/2305.10623)

    本文针对球形负感知器模型的解空间展开研究，发现在过度参数化的区域内解决方案流形显示出简单的连通性质。存在一个大的测地凸成分，对各种优化动力学具有吸引力，其中又有一个与大多数其他解决方案测地连接的非典型鲁棒解决方案的子集，从而产生了星形的几何形状。

    

    对神经网络景观的经验研究表明，低能配置通常出现在复杂的连通结构中，在那里可以构建远距离解之间的零能路径。本文研究球形负感知器，一个作为连续约束满足问题的典型非凸神经网络模型。我们引入了一种计算从平衡点采样的顶点配置的单纯形中能量障碍的通用分析方法。在过度参数化的区域内，解决方案流形显示出简单的连通性质。存在一个大的测地凸成分，对各种优化动力学具有吸引力。在这个区域内，我们确定了一个与大多数其他解决方案测地连接的非典型鲁棒解决方案的子集，从而产生了星形的几何形状。我们分析性地表征了解空间的连接结构组织。

    Empirical studies on the landscape of neural networks have shown that low-energy configurations are often found in complex connected structures, where zero-energy paths between pairs of distant solutions can be constructed. Here we consider the spherical negative perceptron, a prototypical non-convex neural network model framed as a continuous constraint satisfaction problem. We introduce a general analytical method for computing energy barriers in the simplex with vertex configurations sampled from the equilibrium. We find that in the over-parameterized regime the solution manifold displays simple connectivity properties. There exists a large geodesically convex component that is attractive for a wide range of optimization dynamics. Inside this region we identify a subset of atypically robust solutions that are geodesically connected with most other solutions, giving rise to a star-shaped geometry. We analytically characterize the organization of the connected space of solutions and s
    
[^96]: 缺失值下的相关性可视化：填充法和直接参数估计法的比较

    Correlation visualization under missing values: a comparison between imputation and direct parameter estimation methods. (arXiv:2305.06044v1 [cs.LG])

    [http://arxiv.org/abs/2305.06044](http://arxiv.org/abs/2305.06044)

    本文比较了不同的缺失数据处理方法对相关图的影响，建议使用直接参数估计法(DPER)来绘制相关图

    

    相关矩阵可视化对于理解数据集中变量之间的关系至关重要，但是缺失数据会对相关系数的估计产生显著挑战。本文比较了不同的缺失数据处理方法对相关图的影响，重点关注两种常见的缺失模式：随机和单调。我们旨在为研究人员和实践者提供实用的策略和建议，以创建和分析相关图。我们的实验结果表明，虽然填充法通常用于缺失数据，但使用填充的数据来生成相关矩阵图可能会导致对特征之间关系的误导性推断。我们建议基于其在实验中的表现，使用DPER，一种直接参数估计方法，绘制相关矩阵图。

    Correlation matrix visualization is essential for understanding the relationships between variables in a dataset, but missing data can pose a significant challenge in estimating correlation coefficients. In this paper, we compare the effects of various missing data methods on the correlation plot, focusing on two common missing patterns: random and monotone. We aim to provide practical strategies and recommendations for researchers and practitioners in creating and analyzing the correlation plot. Our experimental results suggest that while imputation is commonly used for missing data, using imputed data for plotting the correlation matrix may lead to a significantly misleading inference of the relation between the features. We recommend using DPER, a direct parameter estimation approach, for plotting the correlation matrix based on its performance in the experiments.
    
[^97]: 非负矩阵分解中的两到五个真相

    Two to Five Truths in Non-Negative Matrix Factorization. (arXiv:2305.05389v1 [cs.LG])

    [http://arxiv.org/abs/2305.05389](http://arxiv.org/abs/2305.05389)

    本文提出了一种受规范化拉普拉斯图的启发的矩阵缩放方法，可以大大提高非负矩阵分解在文本主题模型中的质量。

    

    本文探讨了在使用非负矩阵分解构建主题模型时，矩阵缩放在计数矩阵上的作用。我们提出了一种受规范化拉普拉斯图（NL） 的启发的矩阵缩放方法，可以大大提高非负矩阵分解的质量。在文本分析中，非负矩阵分解 (NMF) 通常用于计数矩阵的共现“上下文”和“术语”。受 LSE 的启发，矩阵缩放对各种数据集中的文本主题模型都有显着的改进。我们在三个数据集上展示了矩阵缩放在 NMF 中的巨大影响，可以大大改善主题模型的质量。

    In this paper, we explore the role of matrix scaling on a matrix of counts when building a topic model using non-negative matrix factorization. We present a scaling inspired by the normalized Laplacian (NL) for graphs that can greatly improve the quality of a non-negative matrix factorization. The results parallel those in the spectral graph clustering work of \cite{Priebe:2019}, where the authors proved adjacency spectral embedding (ASE) spectral clustering was more likely to discover core-periphery partitions and Laplacian Spectral Embedding (LSE) was more likely to discover affinity partitions. In text analysis non-negative matrix factorization (NMF) is typically used on a matrix of co-occurrence ``contexts'' and ``terms" counts. The matrix scaling inspired by LSE gives significant improvement for text topic models in a variety of datasets. We illustrate the dramatic difference a matrix scalings in NMF can greatly improve the quality of a topic model on three datasets where human an
    
[^98]: 带有截断意识注意力的动态嵌入学习模型用于CTR预测

    DELTA: Dynamic Embedding Learning with Truncated Conscious Attention for CTR Prediction. (arXiv:2305.04891v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2305.04891](http://arxiv.org/abs/2305.04891)

    该论文提出了一种名为DELTA的CTR模型，使用截断意识注意力进行动态嵌入学习，有效地解决了上下文中无效和冗余特征的问题。

    

    点击率（CTR）预测是产品和内容推荐中关键的任务，学习有效的特征嵌入具有重要意义。传统方法通常学习固定的特征表示，而缺乏根据上下文信息动态调整特征表示的机制，导致性能不佳。一些近期的方法尝试通过学习位权重或增强嵌入来解决这个问题，但是受到上下文中无信息或冗余特征的影响。为了解决这个问题，我们借鉴了意识加工中全局工作区理论，该理论认为只有特定的产品特征与点击行为相关，其余特征可能会噪音干扰，甚至有害，因此提出了一种带有截断意识注意力的动态嵌入学习模型DELTA进行CTR预测。

    Click-Through Rate (CTR) prediction is a pivotal task in product and content recommendation, where learning effective feature embeddings is of great significance. However, traditional methods typically learn fixed feature representations without dynamically refining feature representations according to the context information, leading to suboptimal performance. Some recent approaches attempt to address this issue by learning bit-wise weights or augmented embeddings for feature representations, but suffer from uninformative or redundant features in the context. To tackle this problem, inspired by the Global Workspace Theory in conscious processing, which posits that only a specific subset of the product features are pertinent while the rest can be noisy and even detrimental to human-click behaviors, we propose a CTR model that enables Dynamic Embedding Learning with Truncated Conscious Attention for CTR prediction, termed DELTA. DELTA contains two key components: (I) conscious truncatio
    
[^99]: ClusterNet：一种基于感知的分布式数据聚类模型

    ClusterNet: A Perception-Based Clustering Model for Scattered Data. (arXiv:2304.14185v1 [cs.LG])

    [http://arxiv.org/abs/2304.14185](http://arxiv.org/abs/2304.14185)

    这项工作介绍了ClusterNet，一种基于感知的分布式数据聚类模型，利用大规模数据集和基于点的深度学习模型，反映人类感知的聚类可分性。

    

    散点图中的聚类分离是一个通常由广泛使用的聚类技术（例如k-means或DBSCAN）来解决的任务。然而，由于这些算法基于非感知度量，它们的输出经常不能反映出人类聚类感知。为了弥合人类聚类感知和机器计算聚类之间的差距，我们提出了一种直接处理分布式数据的学习策略。为了在这些数据上学习感知聚类分离，我们进行了一项众包大规模数据集的工作，其中包括384个人群工作者对双变量数据的7,320个点聚类从属进行了标记。基于这些数据，我们能够训练ClusterNet，这是一个基于点的深度学习模型，被训练成反映人类感知的聚类可分性。为了在人类注释的数据上训练ClusterNet，我们省略了在2D画布上渲染散点图，而是使用了一个PointNet++架构，使其能够直接推理点云。在这项工作中，我们建立了一种基于感知的分布式数据聚类模型，ClusterNet。

    Cluster separation in scatterplots is a task that is typically tackled by widely used clustering techniques, such as for instance k-means or DBSCAN. However, as these algorithms are based on non-perceptual metrics, their output often does not reflect human cluster perception. To bridge the gap between human cluster perception and machine-computed clusters, we propose a learning strategy which directly operates on scattered data. To learn perceptual cluster separation on this data, we crowdsourced a large scale dataset, consisting of 7,320 point-wise cluster affiliations for bivariate data, which has been labeled by 384 human crowd workers. Based on this data, we were able to train ClusterNet, a point-based deep learning model, trained to reflect human perception of cluster separability. In order to train ClusterNet on human annotated data, we omit rendering scatterplots on a 2D canvas, but rather use a PointNet++ architecture enabling inference on point clouds directly. In this work, w
    
[^100]: ICU创伤患者早期脓毒症发作预测的夜间个人档案表示学习

    NPRL: Nightly Profile Representation Learning for Early Sepsis Onset Prediction in ICU Trauma Patients. (arXiv:2304.12737v1 [cs.LG])

    [http://arxiv.org/abs/2304.12737](http://arxiv.org/abs/2304.12737)

    本文提出了一种基于夜间个人档案表示学习和深度学习框架的方法，可以提前预测创伤患者的脓毒症发作，这种方法优于现有的最先进方法。

    

    脓毒症是一种源于感染，以严重器官功能障碍为特征的综合症，并且是全球重症监护病房(ICU)死亡率的主要原因之一。通过早期应用抗生素可以减少这些并发症，因此预测脓毒症的发作时间对患者的生存和福祉至关重要。当前在医疗基础设施内部部署的机器学习算法表现不佳，不足以早期预测脓毒症的发生。因此，本文首先提出了一种基于患者生理和临床数据的夜间个人档案表示学习方法(NPRL)，以捕捉患者状态随时间动态改变的情况。然后使用深度学习框架预测这些患者的脓毒症发作时间，并超越现有的最先进方法。

    Sepsis is a syndrome that develops in response to the presence of infection. It is characterized by severe organ dysfunction and is one of the leading causes of mortality in Intensive Care Units (ICUs) worldwide. These complications can be reduced through early application of antibiotics, hence the ability to anticipate the onset of sepsis early is crucial to the survival and well-being of patients. Current machine learning algorithms deployed inside medical infrastructures have demonstrated poor performance and are insufficient for anticipating sepsis onset early. In recent years, deep learning methodologies have been proposed to predict sepsis, but some fail to capture the time of onset (e.g., classifying patients' entire visits as developing sepsis or not) and others are unrealistic to be deployed into medical facilities (e.g., creating training instances using a fixed time to onset where the time of onset needs to be known apriori). Therefore, in this paper, we first propose a nove
    
[^101]: 基于匹配的生成模型数据估值方法

    Matching-based Data Valuation for Generative Model. (arXiv:2304.10701v1 [cs.CV])

    [http://arxiv.org/abs/2304.10701](http://arxiv.org/abs/2304.10701)

    本论文提出了基于匹配的生成模型数据估值方法，这是一个针对任何生成模型的模型无关方法，可以对数据实例进行估值，而无需重新训练模型，并在估值效果上表现出色。

    

    数据估值对于机器学习非常重要，因为它有助于增强模型的透明度并保护数据特性。现有的数据估值方法主要集中在判别模型上，忽略了最近吸引了大量关注的深度生成模型。与判别模型类似，需要评估深度生成模型中数据贡献的紧迫需求也存在。然而，以往的数据估值方法主要依赖于判别模型性能指标，并需要对模型进行重新训练。因此，它们不能在实际中直接高效地应用于近期的深度生成模型，例如生成对抗网络和扩散模型。为了弥补这一差距，我们从相似性匹配的角度对生成模型中的数据估值问题进行了构建。具体地，我们引入了“Generative Model Valuator”（GMValuator）——第一个针对任何生成模型的模型无关方法，旨在为生成模型提供数据估值而无需重新训练模型。我们的方法利用数据实例及由生成模型生成的相应合成实例之间的相似度来估计原始数据的价值。大量实验证明了我们的方法在为不同的生成模型（包括GAN和扩散模型）评估数据实例方面的优越性。

    Data valuation is critical in machine learning, as it helps enhance model transparency and protect data properties. Existing data valuation methods have primarily focused on discriminative models, neglecting deep generative models that have recently gained considerable attention. Similar to discriminative models, there is an urgent need to assess data contributions in deep generative models as well. However, previous data valuation approaches mainly relied on discriminative model performance metrics and required model retraining. Consequently, they cannot be applied directly and efficiently to recent deep generative models, such as generative adversarial networks and diffusion models, in practice. To bridge this gap, we formulate the data valuation problem in generative models from a similarity-matching perspective. Specifically, we introduce Generative Model Valuator (GMValuator), the first model-agnostic approach for any generative models, designed to provide data valuation for gener
    
[^102]: 不需重新搜索的研究：最大更新参数化可精确预测跨尺度的损失

    Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales. (arXiv:2304.06875v1 [cs.CL])

    [http://arxiv.org/abs/2304.06875](http://arxiv.org/abs/2304.06875)

    提出了一种新的方法muP，可以提高超参数的缩放律的拟合精度，减少对大模型超参数的搜索，从而实现在大规模模型上进行损失预测。

    

    随着语言模型的扩大，验证研究想法变得越来越昂贵，因为小模型的结论不能简单地转移到大模型。解决方案是建立一个通用系统，仅基于小模型的结果和超参数直接预测大模型的一些指标。现有的基于缩放律的方法需要在最大的模型上进行超参数搜索，但由于资源有限，这是不切实际的。我们通过展示我们的发现表明，最大更新参数化（muP）使得可以在靠近常见损失流域的超参数的情况下准确拟合超参数的缩放律，而不需要任何搜索。因此，不同的模型可以在大尺度上进行损失预测，在训练开始之前就可以进行直接比较。我们提出了一种新的范式，作为可靠的学术研究的第一步，适用于任何模型规模，而不需大量的计算。代码将很快公开可用。

    As language models scale up, it becomes increasingly expensive to verify research ideas because conclusions on small models do not trivially transfer to large ones. A possible solution is to establish a generic system that directly predicts some metrics for large models solely based on the results and hyperparameters from small models. Existing methods based on scaling laws require hyperparameter search on the largest models, which is impractical with limited resources. We address this issue by presenting our discoveries indicating that Maximal Update parametrization (muP) enables accurate fitting of scaling laws for hyperparameters close to common loss basins, without any search. Thus, different models can be directly compared on large scales with loss prediction even before the training starts. We propose a new paradigm as a first step towards reliable academic research for any model scale without heavy computation. Code will be publicly available shortly.
    
[^103]: RAPID: 在动态公共云环境中实现快速在线策略学习

    RAPID: Enabling Fast Online Policy Learning in Dynamic Public Cloud Environments. (arXiv:2304.04797v1 [cs.LG])

    [http://arxiv.org/abs/2304.04797](http://arxiv.org/abs/2304.04797)

    提出了在动态公共云环境中实现快速在线策略学习的框架RAPID，通过领域知识启发的技术实现样本效率和偏差减少，学习并实时调整资源分配策略，能有效解决资源共享中的问题。

    

    多个工作负载之间的资源共享已成为云服务提供商之间的一种突出实践，这是由需求改进资源利用率和降低拥有成本所驱动的。然而，由于资源争用可能会对具有严格服务质量 (QoS) 要求的优先级高、面向用户的负载产生不利影响，因此有效的资源共享仍然是一个开放的挑战。虽然最近的方法已经展示了有希望的结果，但这些工作在公共云环境中仍然很难实践，因为负载事先是未知的，可能仅运行短暂的时间，从而禁止脱机学习，并且显著阻碍在线学习。在本文中，我们提出 RAPID，这是一种新颖的框架，用于在高度动态的操作环境中实现快速、完全在线的资源分配策略学习。RAPID 利用轻量级 QoS 预测，通过领域知识启发的技术实现样本效率和偏差减少，以分离争用检测和分配策略。

    Resource sharing between multiple workloads has become a prominent practice among cloud service providers, motivated by demand for improved resource utilization and reduced cost of ownership. Effective resource sharing, however, remains an open challenge due to the adverse effects that resource contention can have on high-priority, user-facing workloads with strict Quality of Service (QoS) requirements. Although recent approaches have demonstrated promising results, those works remain largely impractical in public cloud environments since workloads are not known in advance and may only run for a brief period, thus prohibiting offline learning and significantly hindering online learning. In this paper, we propose RAPID, a novel framework for fast, fully-online resource allocation policy learning in highly dynamic operating environments. RAPID leverages lightweight QoS predictions, enabled by domain-knowledge-inspired techniques for sample efficiency and bias reduction, to decouple contr
    
[^104]: 标签可观测、节点不可观测下的二部图图估计问题研究

    Graphon Estimation in bipartite graphs with observable edge labels and unobservable node labels. (arXiv:2304.03590v1 [math.ST])

    [http://arxiv.org/abs/2304.03590](http://arxiv.org/abs/2304.03590)

    研究了标签可观测、节点不可观测下的二部图图估计问题，在分段常数和H\"older连续图谱的情况下找到了有限的样本风险界限。

    

    许多数据集都可以表示为一个矩阵，其条目对应于不同类型实体之间的交互（网页用户访问网页的次数、学生某科目的成绩、患者对医生的评价等）。本文假设上述交互是由描述每个实体的不可观测潜在变量确定的。我们的目标是估计给定不可观测变量的数据矩阵的条件期望。这被表示为估计称为图谱的双变量函数的问题。我们研究了分段常数和H\"older连续图谱的情况。我们为最小二乘估计和指数加权聚合建立了有限样本风险界限。这些界限强调了估计误差与数据集大小、交互强度最大值和噪声水平的依赖关系。由于分析的最小二乘估计量难以处理，

    Many real-world data sets can be presented in the form of a matrix whose entries correspond to the interaction between two entities of different natures (number of times a web user visits a web page, a student's grade in a subject, a patient's rating of a doctor, etc.). We assume in this paper that the mentioned interaction is determined by unobservable latent variables describing each entity. Our objective is to estimate the conditional expectation of the data matrix given the unobservable variables. This is presented as a problem of estimation of a bivariate function referred to as graphon. We study the cases of piecewise constant and H\"older-continuous graphons. We establish finite sample risk bounds for the least squares estimator and the exponentially weighted aggregate. These bounds highlight the dependence of the estimation error on the size of the data set, the maximum intensity of the interactions, and the level of noise. As the analyzed least-squares estimator is intractable
    
[^105]: 无监督深度学习中基于非线性独立成分分析的原则分离问题

    Nonlinear Independent Component Analysis for Principled Disentanglement in Unsupervised Deep Learning. (arXiv:2303.16535v1 [cs.LG])

    [http://arxiv.org/abs/2303.16535](http://arxiv.org/abs/2303.16535)

    本文概括了无监督深度学习中基于独立成分分析方法的最新发展，特别是对于解决非线性情况下唯一性问题提出了可识别的扩展方法。

    

    在无监督深度学习中，如何找到有用的高维数据表示，即所谓的“分离”问题至关重要。大多数方法都是启发式的，缺乏适当的理论基础。在线性表示学习中，独立成分分析（ICA）在许多应用领域取得了成功，并且具有基于良定义的概率模型的原则性。 然而，将ICA扩展到非线性情况已经成为一个棘手的问题，这是由于缺乏可识别性，即表示的唯一性。最近，已经提出了使用时间结构或某些辅助信息的非线性扩展。这些模型实际上是可识别的，因此已经开发出越来越多的算法。特别是一些自监督算法可以显示出估计非线性ICA，即使最初是从启发式角度提出的。本文总结了非线性ICA的最新进展。

    A central problem in unsupervised deep learning is how to find useful representations of high-dimensional data, sometimes called "disentanglement". Most approaches are heuristic and lack a proper theoretical foundation. In linear representation learning, independent component analysis (ICA) has been successful in many applications areas, and it is principled, i.e. based on a well-defined probabilistic model. However, extension of ICA to the nonlinear case has been problematic due to the lack of identifiability, i.e. uniqueness of the representation. Recently, nonlinear extensions that utilize temporal structure or some auxiliary information have been proposed. Such models are in fact identifiable, and consequently, an increasing number of algorithms have been developed. In particular, some self-supervised algorithms can be shown to estimate nonlinear ICA, even though they have initially been proposed from heuristic perspectives. This paper reviews the state-of-the-art of nonlinear ICA 
    
[^106]: 寻找长时间微弱的天文高能瞬变现象：一种数据驱动的方法。

    Searching for long faint astronomical high energy transients: a data driven approach. (arXiv:2303.15936v1 [astro-ph.HE])

    [http://arxiv.org/abs/2303.15936](http://arxiv.org/abs/2303.15936)

    HERMES Pathfinder是一个在轨探测系统，使用简单但创新的探测器监测高能宇宙瞬变现象，通过研究信号到达不同探测器的延迟时间获得精确的位置信息，本文介绍了一种使用神经网络评估航天高能探测器背景计数率的新框架。

    

    HERMES（High Energy Rapid Modular Ensemble of Satellites）是一个在轨探测系统的前导部署，由六个3U纳米卫星组成，托管着用于监测宇宙高能瞬变现象的简单但创新的探测器。HERMES Pathfinder的主要目标是证明使用微型硬件可以获得高能宇宙瞬变现象的精确位置信息。通过研究信号到达不同探测器的延迟时间，可以获取瞬变现象的位置信息。为此，需要开发新的工具来充分利用HERMES Pathfinder未来的科学数据。本文介绍了一种评估航天高能探测器背景计数率的新框架；这是鉴别微弱的天体物理瞬变现象的关键步骤。我们使用神经网络（NN）来估计探测器的背景计数率。

    HERMES (High Energy Rapid Modular Ensemble of Satellites) pathfinder is an in-orbit demonstration consisting of a constellation of six 3U nano-satellites hosting simple but innovative detectors for the monitoring of cosmic high-energy transients. The main objective of HERMES Pathfinder is to prove that accurate position of high-energy cosmic transients can be obtained using miniaturized hardware. The transient position is obtained by studying the delay time of arrival of the signal to different detectors hosted by nano-satellites on low Earth orbits. To this purpose, the goal is to achive an overall accuracy of a fraction of a micro-second. In this context, we need to develop novel tools to fully exploit the future scientific data output of HERMES Pathfinder. In this paper, we introduce a new framework to assess the background count rate of a space-born, high energy detector; a key step towards the identification of faint astrophysical transients. We employ a Neural Network (NN) to est
    
[^107]: RGI: 基于正则化的图形Infomax自监督学习

    RGI : Regularized Graph Infomax for self-supervised learning on graphs. (arXiv:2303.08644v1 [cs.LG])

    [http://arxiv.org/abs/2303.08644](http://arxiv.org/abs/2303.08644)

    RGI是一个用于图上节点级自监督学习的简单而有效的框架，通过最大化节点级局部和全局视图之间的互信息来训练图神经网络编码器，并规范化了表示的协方差矩阵。

    

    自监督学习作为在图形表示学习中避免大量注释的解决方案，正受到越来越多的关注。我们引入了“正则化图形Infomax（RGI）”，这是一个简单而又有效的框架，用于在图形上进行节点级自监督学习，通过最大化节点级局部和全局视图之间的互信息来训练图神经网络编码器，与以前采用图级全局视图的方法不同。该方法促进了视图之间的可预测性，同时规范化了表示的协方差矩阵。因此，RGI是非对比的，不依赖于复杂的不对称体系结构或训练技巧，无需增强和不依赖于双分支架构。我们在流行的图形基准上运行RGI，并展示它可以实现最先进的性能，而不管它的简单性如何。

    Self-supervised learning is gaining considerable attention as a solution to avoid the requirement of extensive annotations in representation learning on graphs. We introduce \textit{Regularized Graph Infomax (RGI)}, a simple yet effective framework for node level self-supervised learning on graphs that trains a graph neural network encoder by maximizing the mutual information between node level local and global views, in contrast to previous works that employ graph level global views. The method promotes the predictability between views while regularizing the covariance matrices of the representations. Therefore, RGI is non-contrastive, does not depend on complex asymmetric architectures nor training tricks, is augmentation-free and does not rely on a two branch architecture. We run RGI on both transductive and inductive settings with popular graph benchmarks and show that it can achieve state-of-the-art performance regardless of its simplicity.
    
[^108]: 可证收敛的即插即用拟牛顿方法

    Provably Convergent Plug-and-Play Quasi-Newton Methods. (arXiv:2303.07271v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2303.07271](http://arxiv.org/abs/2303.07271)

    本文提出了一种可证明收敛的PnP方法，使用拟牛顿步骤以加速收敛，相对于现有的PnP方法对去噪器或保真度函数施加了较轻的限制。

    

    即插即用（PnP）方法是一类高效的迭代算法，旨在利用经典优化算法（如ISTA或ADMM），将数据保真度项和深度去噪器相结合。现有的可证明的PnP方法对去噪器或保真度函数施加了严格的限制，如非扩张性或严格凸性。本文提出了一种可证明的PnP方法，该方法基于近端去噪器施加相对较轻的条件，并引入了拟牛顿步骤以大大加速收敛。通过将深度去噪器特别参数化为梯度步骤，我们进一步将拟牛顿PnP算法的固定点表征为可能非凸函数的临界点。

    Plug-and-Play (PnP) methods are a class of efficient iterative methods that aim to combine data fidelity terms and deep denoisers using classical optimization algorithms, such as ISTA or ADMM. Existing provable PnP methods impose heavy restrictions on the denoiser or fidelity function, such as nonexpansiveness or strict convexity. In this work, we propose a provable PnP method that imposes relatively light conditions based on proximal denoisers, and introduce a quasi-Newton step to greatly accelerate convergence. By specially parameterizing the deep denoiser as a gradient step, we further characterize the fixed-points of the quasi-Newton PnP algorithm as critical points of a possibly non-convex function.
    
[^109]: 图神经网络的描述性复杂性

    The Descriptive Complexity of Graph Neural Networks. (arXiv:2303.04613v2 [cs.LO] UPDATED)

    [http://arxiv.org/abs/2303.04613](http://arxiv.org/abs/2303.04613)

    研究分析了图神经网络（GNN）在布尔电路复杂性和描述性复杂性方面的能力，证明了多项式规模有界深度的GNN族族可以计算的图查询正是带计数和内置关系的一阶逻辑受保护的片断GFO+C所定义的，这将GNN放在电路复杂性类TC^0中。

    

    我们分析了图神经网络（GNN）的布尔电路复杂性和描述性复杂性的能力。我们证明了多项式规模有界深度的GNN族族可以计算的图查询正是那些用带计数和内置关系的一阶逻辑受保护的片断GFO+C定义的。这将GNN放在电路复杂性类TC^0中。值得注意的是，GNN家族可以使用任意实数权值和包括标准ReLU、Logistic“sigmod”和双曲正切函数在内的广泛激活函数类。如果GNN被允许使用随机初始化和全局读取（这些都是GNN在实践中广泛使用的标准功能），它们可以计算与阈门的有界深度布尔电路完全相同的查询，即在TC^0中的查询。此外，我们展示了一个带分段线性激活和有理权重的单个GNN可以在不建造内部关系的情况下由GFO+C定义。

    We analyse the power of graph neural networks (GNNs) in terms of Boolean circuit complexity and descriptive complexity.  We prove that the graph queries that can be computed by a polynomial-size bounded-depth family of GNNs are exactly those definable in the guarded fragment GFO+C of first-order logic with counting and with built-in relations. This puts GNNs in the circuit complexity class TC^0. Remarkably, the GNN families may use arbitrary real weights and a wide class of activation functions that includes the standard ReLU, logistic "sigmod", and hyperbolic tangent functions. If the GNNs are allowed to use random initialisation and global readout (both standard features of GNNs widely used in practice), they can compute exactly the same queries as bounded depth Boolean circuits with threshold gates, that is, exactly the queries in TC^0.  Moreover, we show that queries computable by a single GNN with piecewise linear activations and rational weights are definable in GFO+C without bui
    
[^110]: 表现不足以为盈，深究Rashomon的四重奏

    Performance is not enough: a story of the Rashomon's quartet. (arXiv:2302.13356v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.13356](http://arxiv.org/abs/2302.13356)

    本文介绍了Rashomon的四重奏，这是一个合成数据集，其中来自不同类别的四个模型具有几乎相同的预测性能，同时其可视化揭示了极其不同的方法来理解数据中的相关性结构。

    

    预测建模通常被简化为寻找最优模型来优化选定的性能度量。但如果第二优模型能够以完全不同的方式同样描述数据呢？第三个模型呢？最有效的模型会学到完全不同的数据关系吗？受到Anscombe四重奏的启发，本文介绍了Rashomon的四重奏，这是一个合成数据集，其中来自不同类别的四个模型具有几乎相同的预测性能。然而，它们的可视化揭示了极其不同的方法来理解数据中的相关性结构。引入的简单示例旨在进一步促进可视化作为比较预测模型超越性能的必要工具。我们需要开发富有洞察力的技术来解释模型集。

    Predictive modelling is often reduced to finding the best model that optimizes a selected performance measure. But what if the second-best model describes the data equally well but in a completely different way? What about the third? Is it possible that the most effective models learn completely different relationships in the data? Inspired by Anscombe's quartet, this paper introduces Rashomon's quartet, a synthetic dataset for which four models from different classes have practically identical predictive performance. However, their visualization reveals drastically distinct ways of understanding the correlation structure in data. The introduced simple illustrative example aims to further facilitate visualization as a mandatory tool to compare predictive models beyond their performance. We need to develop insightful techniques for the explanatory analysis of model sets.
    
[^111]: 通过对值函数的双边界限来利用先前知识在强化学习中

    Leveraging Prior Knowledge in Reinforcement Learning via Double-Sided Bounds on the Value Function. (arXiv:2302.09676v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09676](http://arxiv.org/abs/2302.09676)

    本研究利用先前的值函数近似解决方案，提出了一种使用任意近似值函数来推导感兴趣的最优值函数的双边界限的方法，并为连续状态和动作空间的误差分析提供了新的框架，通过在简单领域进行数值验证证实了这些结果。

    

    一个代理机器人能够利用以往的经验对于高效解决新任务至关重要。先前的研究已经证明，可以从之前推导的值函数中获得新任务的近似解决方案，例如迁移学习，课程学习和组合性等。然而，先前的工作主要集中在使用值函数来获得新任务解决方案的零样本近似。在本文中，我们展示了任意近似值函数如何用来推导对感兴趣的最优值函数的双边界限。我们进一步为连续状态和动作空间的误差分析扩展了这个框架。推导出的结果为训练过程中的裁剪提供了新方法，我们在简单领域进行了数值验证。

    An agent's ability to leverage past experience is critical for efficiently solving new tasks. Approximate solutions for new tasks can be obtained from previously derived value functions, as demonstrated by research on transfer learning, curriculum learning, and compositionality. However, prior work has primarily focused on using value functions to obtain zero-shot approximations for solutions to a new task. In this work, we show how an arbitrary approximation for the value function can be used to derive double-sided bounds on the optimal value function of interest. We further extend the framework with error analysis for continuous state and action spaces. The derived results lead to new approaches for clipping during training which we validate numerically in simple domains.
    
[^112]: 以(小)结构突破下界：具有重尾噪声的非凸随机优化中的加速。(arXiv:2302.06763v2 [cs.LG] 更新)

    Breaking the Lower Bound with (Little) Structure: Acceleration in Non-Convex Stochastic Optimization with Heavy-Tailed Noise. (arXiv:2302.06763v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06763](http://arxiv.org/abs/2302.06763)

    本论文在具有重尾噪声的非凸随机优化问题中，改进了Cutkosky和Mehta的算法，并提供了近乎最优的收敛保证，而无需对随机梯度的矩条件进行额外的假设。

    

    在重尾噪声区域中，我们考虑具有平滑但不一定是凸目标的随机优化问题，其中假设随机梯度的噪声具有有界的$p$阶矩($p\in(1,2]$)。Zhang等人(2020)首次证明了收敛的$\Omega(T^{\frac{1-p}{3p-2}})$下界，并提供了一个简单的剪切算法，以匹配这个最优速率。Cutkosky和Mehta(2021)提出了另一种算法，该算法被证明能够实现近乎最优的高概率收敛保证$O(\log(T/\delta)T^{\frac{1-p}{3p-2}})$，其中$\delta$是失败的概率。然而，这个理想的保证只在附加的假设下成立，即随机梯度本身在$p$阶矩上有界，而即使对于二次目标和中心化的高斯噪声，这个假设也不成立。在这项工作中，我们首先改进了Cutkosky和Mehta(2021)中算法的分析，以获得相同的近乎最优结果。

    We consider the stochastic optimization problem with smooth but not necessarily convex objectives in the heavy-tailed noise regime, where the stochastic gradient's noise is assumed to have bounded $p$th moment ($p\in(1,2]$). Zhang et al. (2020) is the first to prove the $\Omega(T^{\frac{1-p}{3p-2}})$ lower bound for convergence (in expectation) and provides a simple clipping algorithm that matches this optimal rate. Cutkosky and Mehta (2021) proposes another algorithm, which is shown to achieve the nearly optimal high-probability convergence guarantee $O(\log(T/\delta)T^{\frac{1-p}{3p-2}})$, where $\delta$ is the probability of failure. However, this desirable guarantee is only established under the additional assumption that the stochastic gradient itself is bounded in $p$th moment, which fails to hold even for quadratic objectives and centered Gaussian noise.  In this work, we first improve the analysis of the algorithm in Cutkosky and Mehta (2021) to obtain the same nearly optimal h
    
[^113]: 使用谱物理信息神经网络求解流形上的偏微分方程数值方法

    Numerical Methods For PDEs Over Manifolds Using Spectral Physics Informed Neural Networks. (arXiv:2302.05322v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05322](http://arxiv.org/abs/2302.05322)

    本文提出了一种新方法，使用基于谱的物理信息神经网络求解流形上的偏微分方程，并在球面和环面上得到了成功应用。对比标准架构，本文的方法表现更好。

    

    我们提出了一种使用谱方法对齐架构的物理信息神经网络方法，用于求解流形上的偏微分方程。这些网络被训练为将初始条件、时间戳和流形上的点作为输入，并输出给定时间和点处的解的值。我们证明了基于谱方法的神经网络方法在区间上的热方程、球面和环面非线性方程的解决中的有效性。同时，我们还展示了基于谱的神经网络架构优于标准物理信息架构的性能优势。我们广泛的实验结果包括对广泛的测试数据集进行的泛化研究。

    We introduce an approach for solving PDEs over manifolds using physics informed neural networks whose architecture aligns with spectral methods. The networks are trained to take in as input samples of an initial condition, a time stamp and point(s) on the manifold and then output the solution's value at the given time and point(s). We provide proofs of our method for the heat equation on the interval and examples of unique network architectures that are adapted to nonlinear equations on the sphere and the torus. We also show that our spectral-inspired neural network architectures outperform the standard physics informed architectures. Our extensive experimental results include generalization studies where the testing dataset of initial conditions is randomly sampled from a significantly larger space than the training set.
    
[^114]: V1T：使用Vision Transformer进行大规模小鼠V1响应预测

    V1T: large-scale mouse V1 response prediction using a Vision Transformer. (arXiv:2302.03023v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.03023](http://arxiv.org/abs/2302.03023)

    V1T是一种基于Vision Transformer的新型架构，可以跨动物学习共享的视觉和行为表示，对自然视觉刺激下的视觉皮层神经响应进行预测，并在预测性能上优于之前基于卷积的模型超过12.7％。同时，通过Transformer学习的自我关注权重还能够展示与群体感受野的相关性。

    

    在计算神经科学中，对自然视觉刺激下的视觉皮层神经响应的精确预测模型仍然是一个挑战。本文介绍了V1T，一种基于Vision Transformer的新型架构，可以跨动物学习共享的视觉和行为表示。我们对记录于小鼠原始视觉皮层的两个大型数据集进行评估，并在预测性能上优于之前基于卷积的模型超过12.7％。此外，我们展示了Transformer学习的自我关注权重与群体感受野相关。因此，我们的模型为神经响应预测设立了新的基准，并可与行为和神经记录一起使用，以揭示视觉皮层的有意义的特征。

    Accurate predictive models of the visual cortex neural response to natural visual stimuli remain a challenge in computational neuroscience. In this work, we introduce V1T, a novel Vision Transformer based architecture that learns a shared visual and behavioral representation across animals. We evaluate our model on two large datasets recorded from mouse primary visual cortex and outperform previous convolution-based models by more than 12.7% in prediction performance. Moreover, we show that the self-attention weights learned by the Transformer correlate with the population receptive fields. Our model thus sets a new benchmark for neural response prediction and can be used jointly with behavioral and neural recordings to reveal meaningful characteristic features of the visual cortex.
    
[^115]: NeuRI：通过归纳规则推断实现DNN生成的多样化

    NeuRI: Diversifying DNN Generation via Inductive Rule Inference. (arXiv:2302.02261v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2302.02261](http://arxiv.org/abs/2302.02261)

    NeuRI是一种全自动化生成由数百种操作符组成的有效且多样化的DL模型的方法。它通过归纳式程序合成推断操作符约束条件，并采用符号和具体操作的混合模型生成。

    

    深度学习(DL)在各个行业中被广泛应用于改善决策和自动化流程，其推动力来自不断发展的DL库和编译器。DL系统的正确性对于信任DL应用非常重要。因此，最近的研究浪潮一直在研究用于模糊DL系统的自动化测试用例合成（即DNN模型和其输入）。然而，现有的模型生成器只涵盖了有限数量的操作符，缺乏广泛建模操作符约束的能力。为了解决这个挑战，我们提出了NeuRI，一种全自动生成由数百种操作符组成的有效且多样化的DL模型的方法。NeuRI采用了三步过程：(i)从各种来源收集有效和无效的API追踪；(ii)在追踪数据上应用归纳式程序合成，推断构建有效模型的约束条件；(iii)通过结合符号和具体操作执行混合模型生成。

    Deep Learning (DL) is prevalently used in various industries to improve decision-making and automate processes, driven by the ever-evolving DL libraries and compilers. The correctness of DL systems is crucial for trust in DL applications. As such, the recent wave of research has been studying the automated synthesis of test-cases (i.e., DNN models and their inputs) for fuzzing DL systems. However, existing model generators only subsume a limited number of operators, lacking the ability to pervasively model operator constraints. To address this challenge, we propose NeuRI, a fully automated approach for generating valid and diverse DL models composed of hundreds of types of operators. NeuRI adopts a three-step process: (i) collecting valid and invalid API traces from various sources; (ii) applying inductive program synthesis over the traces to infer the constraints for constructing valid models; and (iii) performing hybrid model generation by incorporating both symbolic and concrete ope
    
[^116]: 基于时间注意机制的中期风电功率预测新框架

    A novel framework for medium-term wind power prediction based on temporal attention mechanisms. (arXiv:2302.01222v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01222](http://arxiv.org/abs/2302.01222)

    本文提出了一种基于树状Parzen估计器（TPE）和分解算法的新框架（TPE-VMD-TFT），用于24小时和48小时之前的风电功率预测。在法国电力公司Engie的风能数据集上，所提出的方法表现良好。

    

    风能是一种广泛分布、可再生和环保的能源，对缓解全球变暖和能源短缺具有重要作用。然而，由于其不确定性和波动性，大规模风电系统的网格集成具有挑战性。中期风电功率预测可以为能量调度提供基本依据，因此精确的风电功率预测至关重要。本文提出了一种基于树状Parzen估计器（TPE）和分解算法的新框架。该框架基于变分模式分解（VMD）和时间融合变压器（TFT）定义了24小时和48小时之前的风电功率预测的TPE-VMD-TFT方法。在法国电力公司Engie的风能数据集上，结果表明所提出的方法优于其他方法。

    Wind energy is a widely distributed, recyclable and environmentally friendly energy source that plays an important role in mitigating global warming and energy shortages. Wind energy's uncertainty and fluctuating nature makes grid integration of large-scale wind energy systems challenging. Medium-term wind power forecasts can provide an essential basis for energy dispatch, so accurate wind power forecasts are essential. Much research has yielded excellent results in recent years. However, many of them require additional experimentation and analysis when applied to other data. In this paper, we propose a novel short-term forecasting framework by tree-structured parzen estimator (TPE) and decomposition algorithms. This framework defines the TPE-VMD-TFT method for 24-h and 48-h ahead wind power forecasting based on variational mode decomposition (VMD) and time fusion transformer (TFT). In the Engie wind dataset from the electricity company in France, the results show that the proposed met
    
[^117]: 一个强化学习框架用于动态中介分析

    A Reinforcement Learning Framework for Dynamic Mediation Analysis. (arXiv:2301.13348v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.13348](http://arxiv.org/abs/2301.13348)

    这项研究提出了一个强化学习框架，首次评估了在无限时间范围内的动态中介效应，并开发了鲁棒和半参数有效的估计方法来推断这些因果效应。

    

    中介分析通过学习介导变量在治疗和结果之间传递的因果效应，在各个科学领域中受到越来越多的关注，以阐明因果关系。大多数现有研究集中在点暴露研究中，其中每个受试者只在一个时间点接受一种治疗。然而，有许多应用（例如移动健康）在这些应用中，治疗是按顺序分配的，动态中介效应是主要关注的对象。通过提出一个强化学习（RL）框架，我们首次评估在无限时间范围内的动态中介效应。我们将平均治疗效应分解为直接效应、中介效应、延迟直接效应和延迟中介效应。在确定每个效应成分后，我们进一步在RL框架下开发鲁棒和半参数有效的估计器来推断这些因果效应。

    Mediation analysis learns the causal effect transmitted via mediator variables between treatments and outcomes and receives increasing attention in various scientific domains to elucidate causal relations. Most existing works focus on point-exposure studies where each subject only receives one treatment at a single time point. However, there are a number of applications (e.g., mobile health) where the treatments are sequentially assigned over time and the dynamic mediation effects are of primary interest. Proposing a reinforcement learning (RL) framework, we are the first to evaluate dynamic mediation effects in settings with infinite horizons. We decompose the average treatment effect into an immediate direct effect, an immediate mediation effect, a delayed direct effect, and a delayed mediation effect. Upon the identification of each effect component, we further develop robust and semi-parametrically efficient estimators under the RL framework to infer these causal effects. The super
    
[^118]: 具有双重最优无遗憾的单调博弈学习

    Doubly Optimal No-Regret Learning in Monotone Games. (arXiv:2301.13120v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13120](http://arxiv.org/abs/2301.13120)

    这是首个在平滑单调博弈中实现双重最优无遗憾学习的算法，同时实现了在对抗环境下具有最优的遗憾和在多人平滑单调博弈中具有最优的最终迭代收敛速率到达纳什均衡。

    

    我们考虑多人平滑单调博弈中的在线学习。现有算法存在诸如（1）仅适用于强单调博弈；（2）缺乏无遗憾保证；（3）只有渐进或慢速的$O(\frac{1}{\sqrt{T}})$最终迭代收敛速率到纳什均衡。虽然对于包括广泛研究的外推梯度算法和乐观梯度算法在内的大类算法，$O(\frac{1}{\sqrt{T}})$速率是紧确的，但不是所有基于梯度的算法都是最优的。我们提出了加速乐观梯度（AOG）算法，这是首个在平滑单调博弈中实现双重最优无遗憾学习的算法。即我们的算法同时实现了（i）在对抗环境下，对于平滑凸损失函数具有最优的$O(\sqrt{T})$遗憾和（ii）在多人平滑单调博弈中，具有最优的$O(\frac{1}{T})$最终迭代到达纳什均衡的收敛速率。

    We consider online learning in multi-player smooth monotone games. Existing algorithms have limitations such as (1) being only applicable to strongly monotone games; (2) lacking the no-regret guarantee; (3) having only asymptotic or slow $O(\frac{1}{\sqrt{T}})$ last-iterate convergence rate to a Nash equilibrium. While the $O(\frac{1}{\sqrt{T}})$ rate is tight for a large class of algorithms including the well-studied extragradient algorithm and optimistic gradient algorithm, it is not optimal for all gradient-based algorithms.  We propose the accelerated optimistic gradient (AOG) algorithm, the first doubly optimal no-regret learning algorithm for smooth monotone games. Namely, our algorithm achieves both (i) the optimal $O(\sqrt{T})$ regret in the adversarial setting under smooth and convex loss functions and (ii) the optimal $O(\frac{1}{T})$ last-iterate convergence rate to a Nash equilibrium in multi-player smooth monotone games. As a byproduct of the accelerated last-iterate conve
    
[^119]: 反向传播展开的折叠优化求解器

    Backpropagation of Unrolled Solvers with Folded Optimization. (arXiv:2301.12047v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12047](http://arxiv.org/abs/2301.12047)

    本文提出了一种通过解析优化的方法来解决反向传播中的精度和效率问题，并提供了生成高效可解析的反向传播优化模型的系统。此外，本文还提出了通过优化映射统一展开和解析微分的视角。

    

    在深度网络中将约束优化模型作为组件集成，可以在许多专门的学习任务上取得有希望的进展。在这种情况下的一个核心挑战是通过优化问题的解来进行反向传播，而该解通常缺乏闭合的形式。一种典型的策略是算法展开，它依赖于迭代求解器的自动微分操作。虽然灵活且通用，但在实际中，展开可能遇到精度和效率问题。通过优化的解析微分可以避免这些问题，但当前的框架对于优化问题的形式施加了严格的要求。本文提供了关于展开优化后向传递的理论见解，从而提出了一个生成高效可解析的反向传播优化模型的系统。此外，本文还提出了通过优化映射统一展开和解析微分的视角。在实验上进行测试

    The integration of constrained optimization models as components in deep networks has led to promising advances on many specialized learning tasks. A central challenge in this setting is backpropagation through the solution of an optimization problem, which typically lacks a closed form. One typical strategy is algorithm unrolling, which relies on automatic differentiation through the operations of an iterative solver. While flexible and general, unrolling can encounter accuracy and efficiency issues in practice. These issues can be avoided by analytical differentiation of the optimization, but current frameworks impose rigid requirements on the optimization problem's form. This paper provides theoretical insights into the backward pass of unrolled optimization, leading to a system for generating efficiently solvable analytical models of backpropagation. Additionally, it proposes a unifying view of unrolling and analytical differentiation through optimization mappings. Experiments over
    
[^120]: 无监督异常检测的巧合学习

    Coincident Learning for Unsupervised Anomaly Detection. (arXiv:2301.11368v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11368](http://arxiv.org/abs/2301.11368)

    本文提出了一种名为CoAD的新方法，通过巧合学习来进行无监督异常检测，在多模态任务中通过特征空间的巧合行为来识别异常。

    

    异常检测是复杂系统中的重要任务，比如工业设施、制造业、大型科学实验等，其中子系统的故障可能导致低产量、有缺陷的产品甚至损坏组件。虽然复杂系统通常有大量的数据，但标记的异常通常是稀有的（甚至不存在），并且获取起来很昂贵。因此，无监督方法很常见，通常通过距离或密度来搜索输入特征空间中的异常（或与之相关的低维表示）。本文提出了一种名为CoAD的新方法，专门设计用于多模态任务，并根据特征空间中两个不同部分的“巧合”行为来识别异常。我们定义了一个“无监督”度量$\hat{F}_\beta$，以类比有监督分类的$F_\beta$统计量。CoAD使用$\hat{F}_\beta$来训练一个异常检测算法。

    Anomaly detection is an important task for complex systems (e.g., industrial facilities, manufacturing, large-scale science experiments), where failures in a sub-system can lead to low yield, faulty products, or even damage to components. While complex systems often have a wealth of data, labeled anomalies are typically rare (or even nonexistent) and expensive to acquire. Unsupervised approaches are therefore common and typically search for anomalies either by distance or density of examples in the input feature space (or some associated low-dimensional representation). This paper presents a novel approach called CoAD, which is specifically designed for multi-modal tasks and identifies anomalies based on \textit{coincident} behavior across two different slices of the feature space. We define an \textit{unsupervised} metric, $\hat{F}_\beta$, out of analogy to the supervised classification $F_\beta$ statistic. CoAD uses $\hat{F}_\beta$ to train an anomaly detection algorithm on \textit{u
    
[^121]: 通过噪声感知提高鲁棒损失的噪声容忍性

    Improve Noise Tolerance of Robust Loss via Noise-Awareness. (arXiv:2301.07306v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.07306](http://arxiv.org/abs/2301.07306)

    通过噪声感知提高鲁棒损失的噪声容忍性，采用实例相关超参数集成鲁棒损失，提高了对噪声标签的处理能力。

    

    鲁棒损失最小化是处理噪声标签上的鲁棒学习问题的重要策略。目前设计鲁棒损失的方法涉及引入噪声鲁棒因子，即超参数，来控制噪声鲁棒性和可学习性之间的平衡。然而，对于具有噪声标签的不同数据集，找到适合的超参数是一项具有挑战性和耗时的任务。此外，现有的鲁棒损失方法通常假设所有训练样本共享相同的超参数，这些超参数与实例无关。这限制了这些方法识别不同样本的个体噪声特性以及忽视不同训练样本在帮助模型理解底层模式方面的不同贡献能力。为了解决上述问题，我们提出了将具有实例相关超参数的鲁棒损失集成在一起，以提高其噪声容忍性，并带有理论保证。

    Robust loss minimization is an important strategy for handling robust learning issue on noisy labels. Current approaches for designing robust losses involve the introduction of noise-robust factors, i.e., hyperparameters, to control the trade-off between noise robustness and learnability. However, finding suitable hyperparameters for different datasets with noisy labels is a challenging and time-consuming task. Moreover, existing robust loss methods usually assume that all training samples share common hyperparameters, which are independent of instances. This limits the ability of these methods to distinguish the individual noise properties of different samples and overlooks the varying contributions of diverse training samples in helping models understand underlying patterns. To address above issues, we propose to assemble robust loss with instance-dependent hyperparameters to improve their noise tolerance with theoretical guarantee. To achieve setting such instance-dependent hyperpar
    
[^122]: 在隐私约束下的图拓扑学习

    Graph Topology Learning Under Privacy Constraints. (arXiv:2301.06662v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.06662](http://arxiv.org/abs/2301.06662)

    在隐私约束下，我们提出了一个框架，联合学习为本地客户定制的个性化图以及共识图，以推断潜在图拓扑，同时在保护隐私的情况下处理分布式客户端的数据。

    

    我们考虑在数据分布于分布式客户端且具有隐私敏感性的新颖实际场景中，通过平滑图信号推断潜在图拓扑的问题。这个任务的主要困难在于如何在隐私约束下利用所有独立客户端的潜在异构数据。为了解决这个问题，我们提出了一个框架，通过联合学习为本地客户端定制的个性化图以及共识图。个性化图匹配本地数据分布，从而减轻数据的异质性，而共识图捕捉全局信息。我们接下来设计了一个定制的算法来解决引入的问题，同时不违反隐私约束，即所有的私有数据都在本地处理。为了进一步增强隐私保护，我们将差分隐私（DP）引入到所提算法中，在传输模型更新时抵御隐私攻击。理论上，我们建立了可证明收敛的分析。

    We consider the problem of inferring the underlying graph topology from smooth graph signals in a novel but practical scenario where data are located in distributed clients and are privacy-sensitive. The main difficulty of this task lies in how to utilize the potentially heterogeneous data of all isolated clients under privacy constraints. Towards this end, we propose a framework where personalized graphs for local clients as well as a consensus graph are jointly learned. The personalized graphs match local data distributions, thereby mitigating data heterogeneity, while the consensus graph captures the global information. We next devise a tailored algorithm to solve the induced problem without violating privacy constraints, i.e., all private data are processed locally. To further enhance privacy protection, we introduce differential privacy (DP) into the proposed algorithm to resist privacy attacks when transmitting model updates. Theoretically, we establish provable convergence analy
    
[^123]: 最小化极小化权重学习在吸收MDPs中的应用

    Minimax Weight Learning for Absorbing MDPs. (arXiv:2301.03183v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.03183](http://arxiv.org/abs/2301.03183)

    本文研究了吸收MDPs中未折扣的离策略评估问题，提出了一种称为MWLA算法的方法来直接估计期望回报，并分析了该方法的误差界限和依赖关系。

    

    强化学习策略评估问题常被建模为有限或折扣/平均无限时域的MDPs。本文研究了吸收MDPs中未折扣的离策略评估。给定包含特定截断级别的独立同分布数据集，我们提出了一种称为MWLA算法的方法，直接通过状态-行动占用度量的重要比率来估计期望回报。我们研究了MWLA方法的均方误差界限，并分析了统计误差与数据大小和截断级别的依赖关系。通过使用一种情景出租车环境进行计算实验，展示了MWLA算法的性能。

    Reinforcement learning policy evaluation problems are often modeled as finite or discounted/averaged infinite-horizon MDPs. In this paper, we study undiscounted off-policy policy evaluation for absorbing MDPs. Given the dataset consisting of the i.i.d episodes with a given truncation level, we propose a so-called MWLA algorithm to directly estimate the expected return via the importance ratio of the state-action occupancy measure. The Mean Square Error (MSE) bound for the MWLA method is investigated and the dependence of statistical errors on the data size and the truncation level are analyzed. With an episodic taxi environment, computational experiments illustrate the performance of the MWLA algorithm.
    
[^124]: 忠实且一致的图神经网络解释与原理对齐

    Faithful and Consistent Graph Neural Network Explanations with Rationale Alignment. (arXiv:2301.02791v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.02791](http://arxiv.org/abs/2301.02791)

    本文研究了图神经网络的预测，提出了解释不一致的问题，并从因果关系的角度对其进行了理论分析

    

    近年来，揭示图神经网络（GNN）预测背后的原理引起了越来越多的关注。实例级GNN解释旨在发现目标GNN依赖于进行预测的关键输入元素，如节点或边缘。这些识别出的子结构可以解释GNN的行为。尽管提出了各种算法，但其中大多数通过搜索能够保留原始预测的最小子图来形式化这个任务。然而，这个框架根深蒂固地具有归纳偏见：几个子图可能会产生与原始图相同或相似的输出。因此，它们可能会提供虚假的解释，并且无法提供一致的解释。将它们应用于解释表现较差的GNN会进一步放大这些问题。为了解决这个问题，我们从因果关系的角度在理论上检查GNN的预测。伪解释的两个典型原因

    Uncovering rationales behind predictions of graph neural networks (GNNs) has received increasing attention over recent years. Instance-level GNN explanation aims to discover critical input elements, like nodes or edges, that the target GNN relies upon for making predictions. %These identified sub-structures can provide interpretations of GNN's behavior. Though various algorithms are proposed, most of them formalize this task by searching the minimal subgraph which can preserve original predictions. However, an inductive bias is deep-rooted in this framework: several subgraphs can result in the same or similar outputs as the original graphs. Consequently, they have the danger of providing spurious explanations and failing to provide consistent explanations. Applying them to explain weakly-performed GNNs would further amplify these issues. To address this problem, we theoretically examine the predictions of GNNs from the causality perspective. Two typical reasons for spurious explanation
    
[^125]: 反向课程强化学习

    Backward Curriculum Reinforcement Learning. (arXiv:2212.14214v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.14214](http://arxiv.org/abs/2212.14214)

    这项工作提出了一种新颖的反向课程强化学习方法，通过使用回放轨迹而不是原始的前向轨迹来训练智能体。这种方法通过提供强有力的奖励信号实现了更高效的学习，而且只需要进行微小的算法改变。

    

    当前强化学习算法使用前向生成轨迹来训练智能体，这种方法提供的指导不足以使智能体进行尽可能多的探索。尽管我们认识到强化学习结果来自充分的探索，但这种方法在样本效率上存在折衷，这是影响算法性能的重要因素。以往的方法使用奖励塑造技术和网络结构修改来增加样本效率，但这些方法需要很多步骤来实现。在本文中，我们提出了一种新颖的反向课程强化学习方法，即通过使用回放轨迹而不是原始的前向轨迹来训练智能体。这种方法为智能体提供了强有力的奖励信号，从而实现更高效的学习。此外，我们的方法只需要在智能体训练之前对轨迹的顺序进行微小的改变，使得实现起来更加直接。

    Current reinforcement learning algorithms train an agent using forward-generated trajectories, which provide little guidance so that the agent can explore as much as possible. While realizing the value of reinforcement learning results from sufficient exploration, this approach leads to a trade-off in losing sample efficiency, an essential factor impacting algorithm performance. Previous tasks use reward-shaping techniques and network structure modification to increase sample efficiency. However, these methods require many steps to implement. In this work, we propose novel backward curriculum reinforcement learning that begins training the agent using the backward trajectory of the episode instead of the original forward trajectory. This approach provides the agent with a strong reward signal, enabling more sample-efficient learning. Moreover, our method only requires a minor change in the algorithm of reversing the order of the trajectory before agent training, allowing a straightforw
    
[^126]: 具有定位应用的路径损耗和到达时间无线地图数据集

    Dataset of Pathloss and ToA Radio Maps With Localization Application. (arXiv:2212.11777v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2212.11777](http://arxiv.org/abs/2212.11777)

    这个论文介绍了一个包含稠密城市环境中无线地图数据集的研究。这个数据集能够用于路径损耗预测和无线定位，通过在相同的城市地图上计算得到RSS和ToA地图，可以公平比较两种定位方法的效果。

    

    本文介绍了在稠密城市环境中生成并公开提供的一组无线地图数据集。这些数据集包括模拟的路径损耗/接收信号强度（RSS）和到达时间（ToA）无线地图，覆盖了大量真实城市地图的稠密城市设置。该数据集的两个主要应用是1）从输入的城市地图预测路径损耗的学习方法（即基于深度学习的模拟），以及2）无线定位。RSS和ToA地图通过相同的模拟在相同的城市地图上计算得出，可以对基于RSS和ToA的定位方法进行公平比较。

    In this article, we present a collection of radio map datasets in dense urban setting, which we generated and made publicly available. The datasets include simulated pathloss/received signal strength (RSS) and time of arrival (ToA) radio maps over a large collection of realistic dense urban setting in real city maps. The two main applications of the presented dataset are 1) learning methods that predict the pathloss from input city maps (namely, deep learning-based simulations), and, 2) wireless localization. The fact that the RSS and ToA maps are computed by the same simulations over the same city maps allows for a fair comparison of the RSS and ToA-based localization methods.
    
[^127]: 重新思考适用于MobileNet速度和尺寸的Vision Transformers

    Rethinking Vision Transformers for MobileNet Size and Speed. (arXiv:2212.08059v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.08059](http://arxiv.org/abs/2212.08059)

    本研究重新思考了Vision Transformers在移动设备上的部署效率，并提出了一种新的超网络设计和搜索策略，以实现与MobileNet类似大小和速度的Transformer模型。

    

    随着Vision Transformers在计算机视觉任务中取得的成功，最近的研究尝试优化ViTs的性能和复杂性，以实现在移动设备上的高效部署。提出了多种方法来加速注意力机制，改进低效的设计，或将移动设备友好的轻量级卷积与ViTs结合形成混合架构。然而，即使是多年前的MobileNet，ViT及其变种仍然具有更高的延迟或更多的参数。在实践中，延迟和大小对于在资源受限硬件上的高效部署都是至关重要的。在这项工作中，我们对ViTs的设计选择进行了重新审视，并提出了一种具有低延迟和高参数效率的新型超网络。我们进一步引入了一种新颖的细粒度联合搜索策略，用于通过优化来寻找高效的transformer架构。

    With the success of Vision Transformers (ViTs) in computer vision tasks, recent arts try to optimize the performance and complexity of ViTs to enable efficient deployment on mobile devices. Multiple approaches are proposed to accelerate attention mechanism, improve inefficient designs, or incorporate mobile-friendly lightweight convolutions to form hybrid architectures. However, ViT and its variants still have higher latency or considerably more parameters than lightweight CNNs, even true for the years-old MobileNet. In practice, latency and size are both crucial for efficient deployment on resource-constraint hardware. In this work, we investigate a central question, can transformer models run as fast as MobileNet and maintain a similar size? We revisit the design choices of ViTs and propose a novel supernet with low latency and high parameter efficiency. We further introduce a novel fine-grained joint search strategy for transformer models that can find efficient architectures by opt
    
[^128]: 通过 Lov\'asz Local Lemma 进行采样的马尔可夫随机场学习组合结构

    Learning Combinatorial Structures via Markov Random Fields with Sampling through Lov\'asz Local Lemma. (arXiv:2212.00296v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.00296](http://arxiv.org/abs/2212.00296)

    Nelson是一种基于神经网络和Lov\'asz Local Lemma的方法，使用约束的马尔可夫随机场模型生成满足组合约束条件的样本。

    

    学习组合结构的生成模型在许多应用中具有革命性的影响，但现有方法无法提供高效且准确的学习结果，由于学习目标受到组合约束条件的制约，其梯度估计非常复杂。我们开发了基于 Lov\'asz Local Lemma 的神经网络（Nelson），它能够从约束的马尔可夫随机场模型的分布中生成满足组合约束条件的样本。

    Generative models for learning combinatorial structures have transformative impacts in many applications. However, existing approaches fail to offer efficient and accurate learning results. Because of the highly intractable nature of the gradient estimation of the learning objective subject to combinatorial constraints. Existing gradient estimation methods would easily run into exponential time/memory space, or incur huge estimation errors due to improper approximation. We develop NEural Lovasz Sampler (Nelson), a neural network based on Lov\'asz Local Lemma (LLL). We show it guarantees to generate samples satisfying combinatorial constraints from the distribution of the constrained Markov Random Fields model (MRF) under certain conditions. We further present a fully differentiable contrastive-divergence-based learning framework on constrained MRF (Nelson-CD). Meanwhile, Nelson-CD being fully differentiable allows us to take advantage of the parallel computing power of GPUs, resulting 
    
[^129]: 你是否正确使用了测试对数似然？

    Are you using test log-likelihood correctly?. (arXiv:2212.00219v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2212.00219](http://arxiv.org/abs/2212.00219)

    使用测试对数似然进行比较可能与其他指标相矛盾，并且高测试对数似然不意味着更准确的后验近似。

    

    测试对数似然常被用来比较不同模型的同一数据，或者比较拟合同一概率模型的不同近似推断算法。我们通过简单的例子展示了如何基于测试对数似然的比较可能与其他目标相矛盾。具体来说，我们的例子表明：（i）达到更高测试对数似然的近似贝叶斯推断算法不必意味着能够产生更准确的后验近似，（ii）基于测试对数似然比较的预测准确性结论可能与基于均方根误差的结论不一致。

    Test log-likelihood is commonly used to compare different models of the same data or different approximate inference algorithms for fitting the same probabilistic model. We present simple examples demonstrating how comparisons based on test log-likelihood can contradict comparisons according to other objectives. Specifically, our examples show that (i) approximate Bayesian inference algorithms that attain higher test log-likelihoods need not also yield more accurate posterior approximations and (ii) conclusions about forecast accuracy based on test log-likelihood comparisons may not agree with conclusions based on root mean squared error.
    
[^130]: 统计算法的PAC验证

    PAC Verification of Statistical Algorithms. (arXiv:2211.17096v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.17096](http://arxiv.org/abs/2211.17096)

    本文介绍了PAC验证的概念，并在三个方面进行了进一步的研究：对于VC维度为$d$的假设类，PAC验证需要$\Omega\left(\sqrt{d}/\varepsilon^2\right)$个i.i.d.样本的下界；提出了一种用于验证实数区间的并集的协议，并与下界对$d$的依赖相匹配；将PAC验证的定义推广到对一般统计算法的验证。

    

    Goldwasser等人（2021）最近提出了PAC验证的设置，其中使用交互式证明来验证假设（机器学习模型），该模型声称满足无知PAC学习目标。本文在多个方面进一步发展了这个概念。首先，我们证明了对于VC维度为$d$的假设类，PAC验证需要$\Omega\left(\sqrt{d}/\varepsilon^2\right)$个i.i.d.样本的下界。其次，我们提出了一个用于PAC验证实数区间的并集的协议，该协议改进了他们提出的协议，并与我们的下界对$d$的依赖相匹配。第三，我们将他们的定义自然推广到了对一般统计算法的验证，这适用于更广泛的领域，超出了无知PAC学习的范畴。通过展示我们提出的定义，我们的最终结果是一种验证具有组合约束的统计查询算法的协议。

    Goldwasser et al. (2021) recently proposed the setting of PAC verification, where a hypothesis (machine learning model) that purportedly satisfies the agnostic PAC learning objective is verified using an interactive proof. In this paper we develop this notion further in a number of ways. First, we prove a lower bound of $\Omega\left(\sqrt{d}/\varepsilon^2\right)$ i.i.d.\ samples for PAC verification of hypothesis classes of VC dimension $d$. Second, we present a protocol for PAC verification of unions of intervals over $\mathbb{R}$ that improves upon their proposed protocol for that task, and matches our lower bound's dependence on $d$. Third, we introduce a natural generalization of their definition to verification of general statistical algorithms, which is applicable to a wider variety of settings beyond agnostic PAC learning. Showcasing our proposed definition, our final result is a protocol for the verification of statistical query algorithms that satisfy a combinatorial constrain
    
[^131]: 利用图像-标记一致性进行视觉-语言预训练

    Leveraging per Image-Token Consistency for Vision-Language Pre-training. (arXiv:2211.15398v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.15398](http://arxiv.org/abs/2211.15398)

    EPIC 提出了一种利用图像-标记一致性的视觉-语言预训练方法，通过基于显著性的掩码策略和不一致的标记生成过程来克服CMLM的限制，并增强了视觉-语言关联的学习能力。

    

    大多数现有的视觉-语言预训练（VLP）方法采用跨模态掩码语言建模（CMLM）来学习视觉-语言关联。然而，根据我们的观察，我们发现CMLM对于此目的来说是不足够的：（1）模态偏差：CMLM中相当数量的掩码标记可以仅通过语言信息恢复，忽略了视觉输入。（2）未被充分利用的未被掩码标记：CMLM主要关注被掩码的标记，但不能同时利用其他标记来学习视觉-语言关联。为了解决这些限制，我们提出了 EPIC（利用图像-标记一致性进行视觉-语言预训练）。在EPIC中，对于每个图像-句子对，我们掩盖与图像相关的标记（即基于显著性的掩码策略），并用从语言模型中采样的替代标记（即不一致的标记生成过程）进行替换，然后模型需要确定每个标记在句子中的位置。

    Most existing vision-language pre-training (VLP) approaches adopt cross-modal masked language modeling (CMLM) to learn vision-language associations. However, we find that CMLM is insufficient for this purpose according to our observations: (1) Modality bias: a considerable amount of masked tokens in CMLM can be recovered with only the language information, ignoring the visual inputs. (2) Under-utilization of the unmasked tokens: CMLM primarily focuses on the masked token but it cannot simultaneously leverage other tokens to learn vision-language associations. To handle those limitations, we propose EPIC (lEveraging Per Image-Token Consistency for vision-language pre-training). In EPIC, for each image-sentence pair, we mask tokens that are salient to the image (i.e., Saliency-based Masking Strategy) and replace them with alternatives sampled from a language model (i.e., Inconsistent Token Generation Procedure), and then the model is required to determine for each token in the sentence w
    
[^132]: 动态损失用于鲁棒学习

    Dynamic Loss For Robust Learning. (arXiv:2211.12506v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.12506](http://arxiv.org/abs/2211.12506)

    本文提出了一种动态损失函数用于鲁棒学习，通过自动调整目标函数的方式从长尾噪声数据中学习分类器。动态损失包括标签修正器和边界生成器，能够纠正噪声标签并生成每个类别的分类边界，通过元学习来优化这两个组件，使分类器适应干净且平衡的测试数据。

    

    实际数据中常常存在标签噪声和类别不平衡。然而，先前的鲁棒学习方法通常只针对其中一种数据偏差并在同时遇到两种偏差时表现不佳。为了弥补这一差距，本文提出了一种基于元学习的动态损失函数，通过训练过程自动调整目标函数，从长尾噪声数据中鲁棒地学习分类器。具体来说，我们的动态损失由一个标签修正器和一个边界生成器组成，分别通过感知底层数据分布和分类器的学习状态来纠正噪声标签和生成每个类别的添加性分类边界。通过一种新的分层采样策略，在少量无偏元数据中丰富多样且困难的样本，动态损失中的两个组件通过元学习联合优化，并培养分类器以适应干净且平衡的测试数据。

    Label noise and class imbalance commonly coexist in real-world data. Previous works for robust learning, however, usually address either one type of the data biases and underperform when facing them both. To mitigate this gap, this work presents a novel meta-learning based dynamic loss that automatically adjusts the objective functions with the training process to robustly learn a classifier from long-tailed noisy data. Concretely, our dynamic loss comprises a label corrector and a margin generator, which respectively correct noisy labels and generate additive per-class classification margins by perceiving the underlying data distribution as well as the learning state of the classifier. Equipped with a new hierarchical sampling strategy that enriches a small amount of unbiased metadata with diverse and hard samples, the two components in the dynamic loss are optimized jointly through meta-learning and cultivate the classifier to well adapt to clean and balanced test data. Extensive exp
    
[^133]: 用于分类和解释相干X射线散斑图的机器学习方法

    Machine learning for classifying and interpreting coherent X-ray speckle patterns. (arXiv:2211.08194v2 [cond-mat.mtrl-sci] UPDATED)

    [http://arxiv.org/abs/2211.08194](http://arxiv.org/abs/2211.08194)

    通过使用模型系统和深度神经网络，本研究探索了相干X射线散斑图和样品结构之间的关系，并表明机器学习在确定散斑图中结构的能力上是准确的。

    

    相干X射线产生的散斑图与材料的内部结构有着密切的关系，但从散斑图中定量地推断结构是具有挑战性的。在这里，我们利用一个模型二维圆盘系统探索相干X射线散斑图和样品结构之间的联系，并研究机器学习在学习这种关系方面的能力。具体而言，我们训练了一个深度神经网络，根据相应结构中的圆盘数密度来对相干X射线散斑图进行分类。证明了该分类系统对于非分散和分散尺寸分布都具有准确性。

    Speckle patterns produced by coherent X-ray have a close relationship with the internal structure of materials but quantitative inversion of the relationship to determine structure from speckle patterns is challenging. Here, we investigate the link between coherent X-ray speckle patterns and sample structures using a model 2D disk system and explore the ability of machine learning to learn aspects of the relationship. Specifically, we train a deep neural network to classify the coherent X-ray speckle patterns according to the disk number density in the corresponding structure. It is demonstrated that the classification system is accurate for both non-disperse and disperse size distributions.
    
[^134]: MemoNet: 通过多哈希码本网络高效地记忆所有交叉特征表示以实现CTR预测

    MemoNet: Memorizing All Cross Features' Representations Efficiently via Multi-Hash Codebook Network for CTR Prediction. (arXiv:2211.01334v3 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2211.01334](http://arxiv.org/abs/2211.01334)

    本文提出了一种名为MemoNet的CTR模型，通过引入多哈希码本网络（HCNet）作为记忆机制，高效地学习和记忆交叉特征的表示。实验结果表明MemoNet在性能上优于最先进的方法，并且展现出NLP中的大型语言模型的扩展规律。

    

    自然语言处理（NLP）中的新发现表明，强大的记忆能力对大型语言模型（LLM）的成功起到了很大作用。这启发我们将独立的记忆机制引入CTR排名模型，以学习和记忆交叉特征的表示。本文提出了多哈希码本网络（HCNet）作为CTR任务中高效学习和记忆交叉特征表示的记忆机制。HCNet使用多哈希码本作为主要的记忆位置，并由多哈希寻址、记忆恢复和特征缩减三个阶段组成。我们还提出了一种名为MemoNet的新型CTR模型，将HCNet与DNN骨干网络相结合。广泛的实验结果在三个公共数据集和在线测试中表明，MemoNet在性能上优于最先进的方法。此外，MemoNet展现出NLP中的大型语言模型的扩展规律，这意味着我们可以扩大模型规模来提高性能。

    New findings in natural language processing (NLP) demonstrate that the strong memorization capability contributes a lot to the success of Large Language Models (LLM). This inspires us to explicitly bring an independent memory mechanism into CTR ranking model to learn and memorize cross features' representations. In this paper, we propose multi-Hash Codebook NETwork (HCNet) as the memory mechanism for efficiently learning and memorizing representations of cross features in CTR tasks. HCNet uses a multi-hash codebook as the main memory place and the whole memory procedure consists of three phases: multi-hash addressing, memory restoring, and feature shrinking. We also propose a new CTR model named MemoNet which combines HCNet with a DNN backbone. Extensive experimental results on three public datasets and online test show that MemoNet reaches superior performance over state-of-the-art approaches. Besides, MemoNet shows scaling law of large language model in NLP, which means we can enlarg
    
[^135]: 使用有限自动机验证和解释神经网络

    Verifying And Interpreting Neural Networks using Finite Automata. (arXiv:2211.01022v2 [cs.FL] UPDATED)

    [http://arxiv.org/abs/2211.01022](http://arxiv.org/abs/2211.01022)

    这项研究提出了一种使用有限自动机来验证和解释神经网络的方法。通过构建特殊的弱Büchi自动机，能够精确地捕捉神经网络的输入输出行为，并用于解决DNN的常见验证和解释任务，如对抗鲁棒性或最小充分原因。

    

    鉴于深度神经网络（DNN）在包括安全关键应用在内的各个领域的普遍使用和其黑盒特性，验证属性和解释DNN的行为是一项重要任务。我们提出了一种基于自动机理论的方法来解决DNN分析中出现的问题。我们展示了DNN的输入输出行为可以被一个（特殊的）弱Büchi自动机精确地捕获，并且展示了如何利用这些来解决DNN的常见验证和解释任务，如对抗鲁棒性或最小充分原因。

    Verifying properties and interpreting the behaviour of deep neural networks (DNN) is an important task given their ubiquitous use in applications, including safety-critical ones, and their black-box nature. We propose an automata-theoric approach to tackling problems arising in DNN analysis. We show that the input-output behaviour of a DNN can be captured precisely by a (special) weak B\"uchi automaton and we show how these can be used to address common verification and interpretation tasks of DNN like adversarial robustness or minimum sufficient reasons.
    
[^136]: 使用Transformer的基于视频的物体6D姿态估计

    Video based Object 6D Pose Estimation using Transformers. (arXiv:2210.13540v2 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2210.13540](http://arxiv.org/abs/2210.13540)

    本论文介绍了一种基于Transformer的视频中物体6D姿态估计框架，利用先前的帧信息进行姿态估计，实现了高效而准确的姿态估计，能够处理长时间序列依赖关系，并且相对于CNN方法表现更好，具有33fps的处理速度，适用于实时物体姿态估计应用。

    

    我们引入了一个名为VideoPose的基于Transformer的6D物体姿态估计框架，该框架采用端到端的基于注意力机制的建模架构，通过关注先前的帧来估计视频中准确的6D物体姿态。我们的方法利用视频序列中的时间信息进行姿态细化，同时具有计算效率高和鲁棒性强的特点。与现有方法相比，我们的架构能够有效地捕捉和推理远距离依赖关系，从而在视频序列上进行迭代细化。对YCB-Video数据集的实验评估结果显示，我们的方法与最先进的Transformer方法持平，并相对于基于CNN的方法表现显著更好。此外，我们的方法每秒能处理33帧，更加高效，因此适用于需要实时物体姿态估计的各种应用。训练代码和预训练模型可在https://github.com/ApoorvaBeedu/VideoPose上获得。

    We introduce a Transformer based 6D Object Pose Estimation framework VideoPose, comprising an end-to-end attention based modelling architecture, that attends to previous frames in order to estimate accurate 6D Object Poses in videos. Our approach leverages the temporal information from a video sequence for pose refinement, along with being computationally efficient and robust. Compared to existing methods, our architecture is able to capture and reason from long-range dependencies efficiently, thus iteratively refining over video sequences. Experimental evaluation on the YCB-Video dataset shows that our approach is on par with the state-of-the-art Transformer methods, and performs significantly better relative to CNN based approaches. Further, with a speed of 33 fps, it is also more efficient and therefore applicable to a variety of applications that require real-time object pose estimation. Training code and pretrained models are available at https://github.com/ApoorvaBeedu/VideoPose
    
[^137]: 具有层次图注意力循环网络的活动感知人类移动预测

    Activity-aware Human Mobility Prediction with Hierarchical Graph Attention Recurrent Network. (arXiv:2210.07765v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.07765](http://arxiv.org/abs/2210.07765)

    这个论文提出了一种基于层次图注意循环网络的活动感知人类移动预测方法，通过构建一个层次图和使用层次图注意模块来捕捉时间-活动-位置之间的依赖关系，以建模用户的偏好。同时引入了一种模型无关的历史增强置信标签，用于聚焦于每个用户的个体级偏好。

    

    人类移动预测是一项基础任务，对于城市规划、基于位置的服务和智能交通系统等各种应用至关重要。现有的方法通常忽略了对行为信息的考虑，这是推理人类偏好和例行活动的关键，或者采用了简化的时间、活动和位置之间的依赖关系表示。为了解决这些问题，我们提出了一种基于层次图注意循环网络（HGARN）的人类移动预测方法。具体来说，我们基于所有用户的历史移动记录构建了一个层次图，并使用层次图注意模块来捕捉复杂的时间-活动-位置依赖关系。这样，HGARN可以学习具有丰富的人类出行语义的表示，以建模用户在全局层面上的偏好。我们还提出了一种模型无关的历史增强置信（MAHEC）标签，以便将我们的模型聚焦于每个用户的个体级偏好。最后，我们引入了一个时间模块...

    Human mobility prediction is a fundamental task essential for various applications, including urban planning, location-based services and intelligent transportation systems. Existing methods often ignore activity information crucial for reasoning human preferences and routines, or adopt a simplified representation of the dependencies between time, activities and locations. To address these issues, we present Hierarchical Graph Attention Recurrent Network (HGARN) for human mobility prediction. Specifically, we construct a hierarchical graph based on all users' history mobility records and employ a Hierarchical Graph Attention Module to capture complex time-activity-location dependencies. This way, HGARN can learn representations with rich human travel semantics to model user preferences at the global level. We also propose a model-agnostic history-enhanced confidence (MAHEC) label to focus our model on each user's individual-level preferences. Finally, we introduce a Temporal Module, wh
    
[^138]: 暂停分子表征学习：用于分子属性预测的新方法

    Taking a Respite from Representation Learning for Molecular Property Prediction. (arXiv:2209.13492v3 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2209.13492](http://arxiv.org/abs/2209.13492)

    本研究对一系列分子表征模型进行了系统评估，发现基于固定表征的模型在分子属性预测中具有一定优势，同时也揭示了活性断崖问题。

    

    人工智能在药物发现中的应用越来越广泛，其中重要的任务之一就是分子属性预测。虽然分子表征学习的技术如此发达，但其背后的基础问题却未被认真探究。在本研究中，我们使用多种分子表征对一系列代表性模型进行了系统评估。除了常用的MoleculeNet基准数据集外，我们还从ChEMBL数据库和文献中收集了一套与阿片类物质相关的数据集以及两个额外的活性数据集。同时，我们也组装了一系列具有不同规模的描述符数据集来评估模型的性能。总共，我们训练了62,820个模型，其中包括50,220个使用固定表征的模型、4,200个使用SMILES序列的模型和8,400个使用分子图的模型。我们首先进行了数据集分析，并强调了阿片类物质中的活性断崖问题。

    Artificial intelligence (AI) has been widely applied in drug discovery with a major task as molecular property prediction. Despite booming techniques in molecular representation learning, fundamentals underlying molecular property prediction haven't been carefully examined yet. In this study, we conducted a systematic evaluation on a collection of representative models using various molecular representations. In addition to the commonly used MoleculeNet benchmark datasets, we also assembled a suite of opioids-related datasets from ChEMBL and two additional activity datasets from literature. To interrogate the basic predictive power, we also assembled a series of descriptors datasets with varying sizes to evaluate the models' performance. In total, we trained 62,820 models, including 50,220 models on fixed representations, 4,200 models on SMILES sequences and 8,400 models on molecular graphs. We first conducted dataset profiling and highlighted the activity-cliffs issue in the opioids-r
    
[^139]: 数字音频取证：盲目检测人类语音模仿

    Digital Audio Forensics: Blind Human Voice Mimicry Detection. (arXiv:2209.12573v4 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2209.12573](http://arxiv.org/abs/2209.12573)

    本文介绍了一种利用深度学习方法，通过盲目检测输入音频的真实性，可以有效应对音频欺诈问题的分类器。而这种分类器不需要任何参考，能够在没有真实来源的情况下检测出模仿音频。

    

    音频是人类交流中使用最广泛的方式之一，但同时也很容易被误用来欺骗人们。随着人工智能的革命，相关技术现在对几乎所有人都可用，这使得犯罪和伪造变得更加简单。本篇论文介绍了一种深度学习方法，开发了一个分类器，可以盲目分类输入音频为真实或者模仿；“盲目”指的是能够在没有参考或真实来源的情况下检测仿制音频的能力。所提出的模型是在一个大型音频数据集中提取的一组重要特征上进行训练的，以得到一个分类器，该分类器被用于测试不同音频的相同特征集。数据提取自两个原始数据集，特别为这项工作而编写;一个全英文数据集和一个混合数据集（阿拉伯语加英语）。这些数据集已通过GitHub以原始形式提供给研究社区，网址为https://github.com/SaSs7/Datas

    Audio is one of the most used ways of human communication, but at the same time it can be easily misused to trick people. With the revolution of AI, the related technologies are now accessible to almost everyone thus making it simple for the criminals to commit crimes and forgeries. In this work, we introduce a deep learning method to develop a classifier that will blindly classify an input audio as real or mimicked; the word 'blindly' refers to the ability to detect mimicked audio without references or real sources. The proposed model was trained on a set of important features extracted from a large dataset of audios to get a classifier that was tested on the same set of features from different audios. The data was extracted from two raw datasets, especially composed for this work; an all English dataset and a mixed dataset (Arabic plus English). These datasets have been made available, in raw form, through GitHub for the use of the research community at https://github.com/SaSs7/Datas
    
[^140]: 从深度学习的视角看待对齐问题

    The alignment problem from a deep learning perspective. (arXiv:2209.00626v5 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2209.00626](http://arxiv.org/abs/2209.00626)

    人工通用智能（AGI）的出现可能会导致其追求与人类利益不对齐的目标，并采用欺骗性行为和权力追求策略。防止这种情况的发生是一个重要的研究方向。

    

    在未来几十年内，人工通用智能（AGI）可能在许多关键任务上超越人类能力。我们认为，如果没有大量努力来防止它，AGIs可能会学会追求与人类利益冲突（即不对齐）的目标。如果像现在最具能力的模型一样进行训练，AGIs可能会学会欺骗性地行动以获得更高的奖励，学会在其微调分布之外进行内部目标的泛化，并利用寻求权力的策略来追求这些目标。我们回顾了这些特性的新证据。具有这些特性的AGIs将很难进行对齐，即使在不对齐的情况下也可能表现出对齐。我们概述了不对齐的AGIs的部署如何可能会不可逆地削弱人类对世界的控制，并简要回顾了旨在防止这种结果的研究方向。

    In coming decades, artificial general intelligence (AGI) may surpass human capabilities at many critical tasks. We argue that, without substantial effort to prevent it, AGIs could learn to pursue goals that conflict (i.e., are misaligned) with human interests. If trained like today's most capable models, AGIs could learn to act deceptively to receive higher reward, learn internally-represented goals which generalize beyond their fine-tuning distributions, and pursue those goals using power-seeking strategies. We review emerging evidence for these properties. AGIs with these properties would be difficult to align and may appear aligned even when they are not. We outline how the deployment of misaligned AGIs might irreversibly undermine human control over the world, and briefly review research directions aimed at preventing this outcome.
    
[^141]: gSwin: 具有移动窗口的分层结构的门控多层感知器视觉模型

    gSwin: Gated MLP Vision Model with Hierarchical Structure of Shifted Window. (arXiv:2208.11718v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2208.11718](http://arxiv.org/abs/2208.11718)

    提出了一种名为gSwin的视觉模型，结合了Swin Transformer和（多头）gMLP两个流派，能够同时兼顾参数效率、性能、局部性和层级，在图像分类、目标检测和语义分割等任务中取得更高的准确度，并且模型尺寸更小。

    

    在语言领域取得成功后，自注意机制（transformer）得到了在视觉领域的应用，并取得了巨大的成功。此外，作为另一种流派，多层感知器（MLP）也在视觉领域进行了探索。除了传统的卷积神经网络，这些架构近来一直引起关注，并提出了许多方法。作为一种能够在图像识别中兼顾参数效率、性能、局部性和层级的方法，我们提出了gSwin，将两个流派合并起来，即Swin Transformer和（多头）gMLP。我们展示了我们的gSwin在图像分类、目标检测和语义分割等三个视觉任务上能够达到比Swin Transformer更高的准确度，并且模型尺寸更小。

    Following the success in language domain, the self-attention mechanism (transformer) is adopted in the vision domain and achieving great success recently. Additionally, as another stream, multi-layer perceptron (MLP) is also explored in the vision domain. These architectures, other than traditional CNNs, have been attracting attention recently, and many methods have been proposed. As one that combines parameter efficiency and performance with locality and hierarchy in image recognition, we propose gSwin, which merges the two streams; Swin Transformer and (multi-head) gMLP. We showed that our gSwin can achieve better accuracy on three vision tasks, image classification, object detection and semantic segmentation, than Swin Transformer, with smaller model size.
    
[^142]: 使用联合学习技术对12导联心电图信号进行心律失常分类的应用

    Application of federated learning techniques for arrhythmia classification using 12-lead ECG signals. (arXiv:2208.10993v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.10993](http://arxiv.org/abs/2208.10993)

    本研究应用联合学习技术对12导联心电图信号进行心律失常分类，通过联合学习方法训练AI模型，实现对医疗数据的隐私保护，同时保持与集中学习方式相当的性能表现。

    

    基于人工智能的大型、精心策划的医学数据集的分析对于利用低功耗心电监测设备信息提供早期检测、更快诊断和更有效治疗具有很大潜力。然而，由于不当使用、不安全的存储或数据泄漏可能会侵犯个人隐私，获取来自不同来源的敏感医疗数据受到严格限制。本研究利用联合学习（FL）隐私保护方法，在来自六个异构来源的12导联传感器阵列采集的高清心电图的异构数据集上训练人工智能模型。我们评估了所得模型与以集中式学习（CL）方式训练的最先进模型的性能等效性。此外，我们还评估了我们的解决方案在独立和相同分布的联合数据以及非相同分布的联合数据上的性能。我们的方法涉及基于深度神经网络的机器学习技术。

    Artificial Intelligence-based (AI) analysis of large, curated medical datasets is promising for providing early detection, faster diagnosis, and more effective treatment using low-power Electrocardiography (ECG) monitoring devices information. However, accessing sensitive medical data from diverse sources is highly restricted since improper use, unsafe storage, or data leakage could violate a person's privacy. This work uses a Federated Learning (FL) privacy-preserving methodology to train AI models over heterogeneous sets of high-definition ECG from 12-lead sensor arrays collected from six heterogeneous sources. We evaluated the capacity of the resulting models to achieve equivalent performance compared to state-of-the-art models trained in a Centralized Learning (CL) fashion. Moreover, we assessed the performance of our solution over Independent and Identical distributed (IID) and non-IID federated data. Our methodology involves machine learning techniques based on Deep Neural Networ
    
[^143]: MolGraph: 一个使用TensorFlow和Keras实现分子图和图神经网络的Python包

    MolGraph: a Python package for the implementation of molecular graphs and graph neural networks with TensorFlow and Keras. (arXiv:2208.09944v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.09944](http://arxiv.org/abs/2208.09944)

    MolGraph是一个使用TensorFlow和Keras实现分子图和图神经网络的Python包，为分子机器学习问题提供了高度兼容和验证性能的解决方案。

    

    分子机器学习在解决各种分子问题（如基于分子描述符或指纹预测分子性质）方面被证明是非常重要的。最近，图神经网络（GNN）算法已经被应用于分子机器学习中，其表现与描述符或指纹方法相当甚至更好。虽然已经存在各种工具和软件包用于在分子机器学习中应用GNNs，但本研究开发了一个名为MolGraph的新的GNN软件包，旨在创建与TensorFlow和Keras应用程序接口高度兼容的GNN模型管道。MolGraph还实现了一个化学模块，用于生成小分子图，可以通过GNN算法解决分子机器学习问题。为了验证GNNs的性能，它们与MoleculeNet数据集以及三个色谱保留时间数据集进行了基准测试。

    Molecular machine learning (ML) has proven important for tackling various molecular problems, such as predicting molecular properties based on molecular descriptors or fingerprints. Since relatively recently, graph neural network (GNN) algorithms have been implemented for molecular ML, showing comparable or superior performance to descriptor or fingerprint-based approaches. Although various tools and packages exist to apply GNNs in molecular ML, a new GNN package, named MolGraph, was developed in this work with the motivation to create GNN model pipelines highly compatible with the TensorFlow and Keras application programming interface (API). MolGraph also implements a chemistry module to accommodate the generation of small molecular graphs, which can be passed to a GNN algorithm to solve a molecular ML problem. To validate the GNNs, they were benchmarked against the datasets of MoleculeNet, as well as three chromatographic retention time datasets. The results on these benchmarks illus
    
[^144]: 学习有效的选择预测的抽象规划模型

    Learning Efficient Abstract Planning Models that Choose What to Predict. (arXiv:2208.07737v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2208.07737](http://arxiv.org/abs/2208.07737)

    该论文提出了一种学习抽象规划模型的方法，通过选择预测来提高机器人在长期任务中的决策效率。

    

    在具有连续状态和行动空间的机器人领域中，解决长期任务的有效方法是双层规划，其中在环境的抽象层上进行高级搜索以指导低级决策。最近的研究表明，通过学习符号操作和神经采样器的抽象模型，可以实现这种双层规划。在本研究中，我们展示了现有的符号操作学习方法在许多机器人领域存在不足之处，因为机器人的行动往往会引起抽象状态中大量无关的变化，而这些方法试图学习准确预测抽象状态中所有观察到的变化的操作。为了解决这个问题，我们提出了一种学习"选择要预测"的操作的方法，只对实现指定目标的抽象规划所必需的变化建模。通过实验证明，我们的方法能够学习出导致10个不同混合任务上的高效规划的操作。

    An effective approach to solving long-horizon tasks in robotics domains with continuous state and action spaces is bilevel planning, wherein a high-level search over an abstraction of an environment is used to guide low-level decision-making. Recent work has shown how to enable such bilevel planning by learning abstract models in the form of symbolic operators and neural samplers. In this work, we show that existing symbolic operator learning approaches fall short in many robotics domains where a robot's actions tend to cause a large number of irrelevant changes in the abstract state. This is primarily because they attempt to learn operators that exactly predict all observed changes in the abstract state. To overcome this issue, we propose to learn operators that 'choose what to predict' by only modelling changes necessary for abstract planning to achieve specified goals. Experimentally, we show that our approach learns operators that lead to efficient planning across 10 different hybr
    
[^145]: 定位机器学习中的差异

    Locating disparities in machine learning. (arXiv:2208.06680v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.06680](http://arxiv.org/abs/2208.06680)

    这个论文提出了一个名为自动定位差异（ALD）的数据驱动框架，旨在解决机器学习中的差异问题。ALD适用于任意的机器学习分类器，可以处理不同的差异定义，并且可以处理分类和连续预测问题。

    

    机器学习可能会产生差异化的预测结果，其中人群的子群（如按年龄，性别或其他敏感属性定义）会被系统性地处于不利地位。为了符合即将出台的法规，从业人员需要找出这种差异性结果。然而，先前的文献通常通过当敏感属性事先指定时的统计程序来检测差异。这限制了在现实世界中适用性，因为数据集通常是高维的，而且敏感属性可能是未知的。为此，我们提出了一个名为自动定位差异（ALD）的数据驱动框架，旨在定位机器学习中的差异。ALD满足业界的几个需求：ALD（1）适用于任意的机器学习分类器；（2）可适用于不同的差异定义（如统计平等或平等赔率）；（3）可以处理分类和连续预测问题。

    Machine learning can provide predictions with disparate outcomes, in which subgroups of the population (e.g., defined by age, gender, or other sensitive attributes) are systematically disadvantaged. In order to comply with upcoming legislation, practitioners need to locate such disparate outcomes. However, previous literature typically detects disparities through statistical procedures for when the sensitive attribute is specified a priori. This limits applicability in real-world settings where datasets are high dimensional and, on top of that, sensitive attributes may be unknown. As a remedy, we propose a data-driven framework called Automatic Location of Disparities (ALD) which aims at locating disparities in machine learning. ALD meets several demands from industry: ALD (1) is applicable to arbitrary machine learning classifiers; (2) operates on different definitions of disparities (e.g., statistical parity or equalized odds); and (3) deals with both categorical and continuous predi
    
[^146]: 自监督对比表示学习用于半监督时间序列分类

    Self-supervised Contrastive Representation Learning for Semi-supervised Time-Series Classification. (arXiv:2208.06616v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.06616](http://arxiv.org/abs/2208.06616)

    这项工作提出了一种新的自监督对比表示学习框架，通过对比学习从未标记的时间序列数据中提取有用的表示。同时，研究了时间序列数据增强选择，并将该框架扩展到半监督学习设置。

    

    当只有未标记数据或少量标记样本可用时，学习时间序列表示是一项具有挑战性的任务。最近，对比自监督学习通过对数据的不同增强视图进行对比，在从未标记数据中提取有用表示方面取得了巨大的改进。在这项工作中，我们提出了一种新的时间序列表示学习框架，即基于时间和上下文对比的时间序列对比（TS-TCC），通过对比学习从未标记数据中提取表示。具体而言，我们提出了适用于时间序列的弱增强和强增强，并使用它们的视图在提出的时间对比模块中学习强大的时间关系，此外还通过我们提出的上下文对比模块学习有区分力的表示。此外，我们对时间序列数据增强选择进行了系统研究，这是对比学习的关键部分。我们还将TS-TCC推广到半监督学习设置，并提出了一种Class-A。

    Learning time-series representations when only unlabeled data or few labeled samples are available can be a challenging task. Recently, contrastive self-supervised learning has shown great improvement in extracting useful representations from unlabeled data via contrasting different augmented views of data. In this work, we propose a novel Time-Series representation learning framework via Temporal and Contextual Contrasting (TS-TCC) that learns representations from unlabeled data with contrastive learning. Specifically, we propose time-series-specific weak and strong augmentations and use their views to learn robust temporal relations in the proposed temporal contrasting module, besides learning discriminative representations by our proposed contextual contrasting module. Additionally, we conduct a systematic study of time-series data augmentation selection, which is a key part of contrastive learning. We also extend TS-TCC to the semi-supervised learning settings and propose a Class-A
    
[^147]: 从智能回复中提取主动模式进行认证

    Combing for Credentials: Active Pattern Extraction from Smart Reply. (arXiv:2207.10802v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2207.10802](http://arxiv.org/abs/2207.10802)

    该论文研究了在智能回复应用程序中潜在的信息泄漏漏洞，并且提出了一种在实际设置中限制查询类型的攻击方式。

    

    预训练的大型语言模型，如GPT-2和BERT，通常会通过微调来在下游任务中达到最先进的性能。一个自然的例子是“智能回复”应用程序，其中预训练模型被调整以提供给定查询消息的建议回复。由于微调数据通常是敏感的数据，如电子邮件或聊天记录，因此重要的是了解和减轻模型泄漏微调数据的风险。我们调查了典型智能回复流程中潜在的信息泄露漏洞。我们考虑了一个现实的情况，即攻击者只能通过前端界面与基础模型进行交互，并限制了可以发送到模型的查询类型。先前的攻击在这些设置中不起作用，而是需要能够直接向模型发送无限制的查询。即使在没有查询约束的情况下，以往的攻击通常需要数千甚至数百万

    Pre-trained large language models, such as GPT\nobreakdash-2 and BERT, are often fine-tuned to achieve state-of-the-art performance on a downstream task. One natural example is the ``Smart Reply'' application where a pre-trained model is tuned to provide suggested responses for a given query message. Since the tuning data is often sensitive data such as emails or chat transcripts, it is important to understand and mitigate the risk that the model leaks its tuning data. We investigate potential information leakage vulnerabilities in a typical Smart Reply pipeline. We consider a realistic setting where the adversary can only interact with the underlying model through a front-end interface that constrains what types of queries can be sent to the model. Previous attacks do not work in these settings, but require the ability to send unconstrained queries directly to the model. Even when there are no constraints on the queries, previous attacks typically require thousands, or even millions, 
    
[^148]: zPROBE：用于联邦学习的零窥探鲁棒性检查

    zPROBE: Zero Peek Robustness Checks for Federated Learning. (arXiv:2206.12100v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.12100](http://arxiv.org/abs/2206.12100)

    该论文提出了zPROBE，一种用于联邦学习的零窥探鲁棒性检查方法。通过使用高断点基于排序的统计信息，并利用随机化聚类技术，实现了对聚合模型更新的私密鲁棒性检查，提高了可扩展性且不损害隐私。

    

    隐私保护的联邦学习允许多个用户通过中央服务器的协调来共同训练模型。服务器只学习最终聚合结果，因此用户的（私有）训练数据不会从个体模型更新中泄漏出去。然而，保持个体更新的私密性允许恶意用户进行拜占庭攻击，降低准确性但不被检测到。目前对抗拜占庭工人的最佳方法依赖于鲁棒的基于排序的统计信息，例如中位数，以查找恶意更新。然而，在安全领域实现保护隐私的基于排序的统计信息是非常困难和不可扩展的，因为它需要对所有个体更新进行排序。我们建立了第一个使用高断点基于排序的统计信息对聚合模型更新进行私密的鲁棒性检查。通过利用随机化聚类，我们显著提高了我们的防御的可扩展性，同时不损害隐私。我们利用零断点下的统计界限。

    Privacy-preserving federated learning allows multiple users to jointly train a model with coordination of a central server. The server only learns the final aggregation result, thus the users' (private) training data is not leaked from the individual model updates. However, keeping the individual updates private allows malicious users to perform Byzantine attacks and degrade the accuracy without being detected. Best existing defenses against Byzantine workers rely on robust rank-based statistics, e.g., median, to find malicious updates. However, implementing privacy-preserving rank-based statistics is nontrivial and not scalable in the secure domain, as it requires sorting all individual updates. We establish the first private robustness check that uses high break point rank-based statistics on aggregated model updates. By exploiting randomized clustering, we significantly improve the scalability of our defense without compromising privacy. We leverage our statistical bounds in zero-kn
    
[^149]: 使用深度学习进行自动化胃肠道分割

    Automated GI tract segmentation using deep learning. (arXiv:2206.11048v5 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2206.11048](http://arxiv.org/abs/2206.11048)

    本文介绍了使用深度学习的自动化分割过程，旨在加快肿瘤治疗中调整剂量递送路径的流程，避免耗时的手动操作，提高患者治疗效果。

    

    放射治疗专家的工作是将X射线束指向肿瘤的同时避免胃和肠道。通过MR-Linacs（磁共振成像和线性加速器系统），放射治疗专家可以可视化肿瘤位置，并根据肿瘤细胞的存在情况精确控制剂量，而这种情况每天都可能有所变化。目前的工作是确定胃和肠道的位置，以调整X射线束的方向，以便将剂量准确递送到肿瘤并避免这些器官。这是一个耗时且人力密集的过程，除非采用深度学习方法自动化分割过程，否则治疗时间很容易从15分钟延长到每天1小时。本文讨论了使用深度学习的自动化分割过程，以加快此过程并使更多患者获得有效治疗。

    The job of Radiation oncologists is to deliver x-ray beams pointed toward the tumor and at the same time avoid the stomach and intestines. With MR-Linacs (magnetic resonance imaging and linear accelerator systems), oncologists can visualize the position of the tumor and allow for precise dose according to tumor cell presence which can vary from day to day. The current job of outlining the position of the stomach and intestines to adjust the X-ray beams direction for the dose delivery to the tumor while avoiding the organs. This is a time-consuming and labor-intensive process that can easily prolong treatments from 15 minutes to an hour a day unless deep learning methods can automate the segmentation process. This paper discusses an automated segmentation process using deep learning to make this process faster and allow more patients to get effective treatment.
    
[^150]: 用暗知识提高替代模型的对抗传递性

    Boosting the Adversarial Transferability of Surrogate Models with Dark Knowledge. (arXiv:2206.08316v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.08316](http://arxiv.org/abs/2206.08316)

    本文提出了一种使用暗知识训练替代模型以提高对抗性样本传递性的方法，通过提取暗知识和使用混合扩增算法来训练所谓的暗替代模型(DSM)。

    

    深度神经网络(DNNs)对对抗性样本具有脆弱性。而且，对抗性样本具有传递性，这意味着一个DNN模型的对抗性样本可以以非平凡的概率欺骗另一个模型。这导致了基于传递的攻击，其中由替代模型生成的对抗性样本用于进行黑盒攻击。有一些研究致力于生成具有更好传递性的给定替代模型的对抗性样本。然而，训练一个特殊的替代模型以生成具有更好传递性的对抗性样本相对较少研究。本文提出了一种使用暗知识训练替代模型以增强替代模型生成的对抗性样本的传递性的方法。这个训练的替代模型被称为暗替代模型(DSM)。训练DSM的方法包括两个关键组成部分：提取暗知识的教师模型和混合扩增算法。

    Deep neural networks (DNNs) are vulnerable to adversarial examples. And, the adversarial examples have transferability, which means that an adversarial example for a DNN model can fool another model with a non-trivial probability. This gave birth to the transfer-based attack where the adversarial examples generated by a surrogate model are used to conduct black-box attacks. There are some work on generating the adversarial examples from a given surrogate model with better transferability. However, training a special surrogate model to generate adversarial examples with better transferability is relatively under-explored. This paper proposes a method for training a surrogate model with dark knowledge to boost the transferability of the adversarial examples generated by the surrogate model. This trained surrogate model is named dark surrogate model (DSM). The proposed method for training a DSM consists of two key components: a teacher model extracting dark knowledge, and the mixing augme
    
[^151]: ResNorm: 通过标准化解决图神经网络中长尾度分布问题

    ResNorm: Tackling Long-tailed Degree Distribution Issue in Graph Neural Networks via Normalization. (arXiv:2206.08181v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.08181](http://arxiv.org/abs/2206.08181)

    本文提出了一种名为ResNorm的图神经网络标准化方法，通过重新塑造节点标准差分布来解决长尾节点度分布问题，提高节点分类的准确性。

    

    图神经网络（GNNs）因其能够从图结构化数据中学习表示而受到广泛关注。尽管GNNs在许多领域有着成功的应用，但GNNs的优化研究较少，节点分类的性能严重受到长尾节点度分布的影响。本文旨在通过标准化提高GNNs的性能。具体来说，通过研究图中节点度的长尾分布，我们提出了一种新的GNN标准化方法，称为ResNorm（通过标准化将长尾分布重新塑造成类似正态分布）。ResNorm的scale操作通过重新塑造节点标准差（NStd）分布，从而提高尾节点（即低度节点）的准确性。我们提供了理论解释和实证证据来理解上述scale的机制。

    Graph Neural Networks (GNNs) have attracted much attention due to their ability in learning representations from graph-structured data. Despite the successful applications of GNNs in many domains, the optimization of GNNs is less well studied, and the performance on node classification heavily suffers from the long-tailed node degree distribution. This paper focuses on improving the performance of GNNs via normalization.  In detail, by studying the long-tailed distribution of node degrees in the graph, we propose a novel normalization method for GNNs, which is termed ResNorm (\textbf{Res}haping the long-tailed distribution into a normal-like distribution via \textbf{norm}alization). The $scale$ operation of ResNorm reshapes the node-wise standard deviation (NStd) distribution so as to improve the accuracy of tail nodes (\textit{i}.\textit{e}., low-degree nodes). We provide a theoretical interpretation and empirical evidence for understanding the mechanism of the above $scale$. In addit
    
[^152]: 基于半定规划的神经网络验证中的弦状稀疏性

    Chordal Sparsity for SDP-based Neural Network Verification. (arXiv:2206.03482v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.03482](http://arxiv.org/abs/2206.03482)

    本文提出了一种基于半定规划的神经网络验证方法，通过引入弦状稀疏性，旨在改善现有技术中存在的可扩展性问题。

    

    神经网络在许多新兴技术中起着核心作用，但验证其正确性仍然是一个重要挑战。已知网络输出对于即使是小的输入扰动也非常敏感和脆弱，从而增加了不可预测和不希望的行为的风险。快速而准确地验证神经网络对其广泛采用至关重要，并且近年来已经开发出多种方法来应对这个问题。在本文中，我们关注于改进基于半定规划（SDP）的神经网络验证技术。这些技术在保留凸问题形式的同时，提供了表达复杂几何约束的能力，但在实践中，可扩展性仍然是一个重要问题。我们的起点是Fazlyab等人提出的DeepSDP框架，该框架使用二次约束将验证问题抽象为一个大规模的SDP。然而，当网络规模增长时，解决这个SDP问题变得困难且耗时。

    Neural networks are central to many emerging technologies, but verifying their correctness remains a major challenge. It is known that network outputs can be sensitive and fragile to even small input perturbations, thereby increasing the risk of unpredictable and undesirable behavior. Fast and accurate verification of neural networks is therefore critical to their widespread adoption, and in recent years a variety of methods have been developed as a response to this problem. In this paper, we focus on improving semidefinite programming (SDP) based techniques for neural network verification. Such techniques offer the power of expressing complex geometric constraints while retaining a convex problem formulation, but in practice, scalability remains a major issue. Our starting point is the DeepSDP framework proposed by Fazlyab et al, which uses quadratic constraints to abstract the verification problem into a large-scale SDP. When the network size grows, however, solving this SDP quickly 
    
[^153]: 个体隐私会计对差分隐私随机梯度下降的影响

    Individual Privacy Accounting for Differentially Private Stochastic Gradient Descent. (arXiv:2206.02617v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.02617](http://arxiv.org/abs/2206.02617)

    本文研究了通过差分隐私随机梯度下降训练的模型对个体示例的隐私保证，并发现大多数示例享有较强的隐私保证。此外，我们还发现训练损失和示例的隐私参数存在很强的相关性。最低准确率类别的平均隐私参数比最高准确率类别高44.2%。

    

    差分隐私随机梯度下降是最近私有深度学习的前沿算法。它为数据集中的所有数据点提供了单一的隐私保证。我们提出了针对个例的输出特定$(\varepsilon,\delta)$-DP，以刻画通过DP-SGD训练的模型对个别示例的隐私保证。我们还设计了一种高效算法来研究跨多个数据集的个体隐私。我们发现大多数示例都享有比最坏情况边界更强的隐私保证。我们进一步发现训练损失和示例的隐私参数之间存在很强的相关性。这意味着在模型效用方面受到不足的群体同时经历较弱的隐私保证。例如，在CIFAR-10上，最低测试准确率类别的平均$\varepsilon$比最高准确率类别高44.2%。

    Differentially private stochastic gradient descent (DP-SGD) is the workhorse algorithm for recent advances in private deep learning. It provides a single privacy guarantee to all datapoints in the dataset. We propose output-specific $(\varepsilon,\delta)$-DP to characterize privacy guarantees for individual examples when releasing models trained by DP-SGD. We also design an efficient algorithm to investigate individual privacy across a number of datasets. We find that most examples enjoy stronger privacy guarantees than the worst-case bound. We further discover that the training loss and the privacy parameter of an example are well-correlated. This implies groups that are underserved in terms of model utility simultaneously experience weaker privacy guarantees. For example, on CIFAR-10, the average $\varepsilon$ of the class with the lowest test accuracy is 44.2\% higher than that of the class with the highest accuracy.
    
[^154]: 深度学习的分层分布感知测试

    Hierarchical Distribution-Aware Testing of Deep Learning. (arXiv:2205.08589v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2205.08589](http://arxiv.org/abs/2205.08589)

    本文提出了一种新的深度学习鲁棒性测试方法，通过考虑特征和像素级别的分布，捕捉对抗扰动的感知质量，以改善模型可靠性。

    

    深度学习在安全关键应用中的使用越来越多，这引发了对其可靠性的担忧。深度学习在面对对抗扰动（即对手样本）时往往缺乏鲁棒性。尽管最近采用了先进的攻击和测试方法来检测对手样本，但这些方法往往忽视了输入分布和扰动的感知质量。结果，检测到的对手样本在实际应用中可能不相关，或者对人类观察者来说可能看起来不真实。这导致测试资源浪费在在现实世界中很少发生的稀有对手样本上，限制了深度学习模型可靠性的提高。在本文中，我们提出了一种新的鲁棒性测试方法，用于检测对手样本，考虑到了特征级别分布和像素级别分布，捕捉了对抗扰动的感知质量。这两种考虑通过一种新颖的分层机制来编码。

    Deep Learning (DL) is increasingly used in safety-critical applications, raising concerns about its reliability. DL suffers from a well-known problem of lacking robustness, especially when faced with adversarial perturbations known as Adversarial Examples (AEs). Despite recent efforts to detect AEs using advanced attack and testing methods, these approaches often overlook the input distribution and perceptual quality of the perturbations. As a result, the detected AEs may not be relevant in practical applications or may appear unrealistic to human observers. This can waste testing resources on rare AEs that seldom occur during real-world use, limiting improvements in DL model dependability.  In this paper, we propose a new robustness testing approach for detecting AEs that considers both the feature level distribution and the pixel level distribution, capturing the perceptual quality of adversarial perturbations. The two considerations are encoded by a novel hierarchical mechanism. Fir
    
[^155]: 图像检索的内省式深度度量学习

    Introspective Deep Metric Learning for Image Retrieval. (arXiv:2205.04449v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2205.04449](http://arxiv.org/abs/2205.04449)

    本文提出了一种内省式深度度量学习（IDML）框架，通过不确定性建模改进了深度度量学习的性能，并在多个数据集上取得了最先进的结果。

    

    本文提出了一种内省式深度度量学习（IDML）框架，用于对图像进行不确定性感知的比较。传统的深度度量学习方法在图像之间产生自信的语义距离，而不考虑不确定性水平。然而，我们认为一个好的相似性模型应该谨慎考虑语义差异，以更好地处理模糊的图像，从而实现更稳健的训练。为了实现这一点，我们提出使用语义嵌入和伴随的不确定性嵌入来表示图像，分别描述图像的语义特征和模糊度。我们进一步提出了一种内省式相似性度量，用于在考虑图像的语义差异和模糊度的情况下进行相似性判断。所提出的IDML框架通过不确定性建模改进了深度度量学习的性能，并在广泛使用的CUB-200-2011，Cars196和Stanford Online数据集上取得了最先进的结果。

    This paper proposes an introspective deep metric learning (IDML) framework for uncertainty-aware comparisons of images. Conventional deep metric learning methods produce confident semantic distances between images regardless of the uncertainty level. However, we argue that a good similarity model should consider the semantic discrepancies with caution to better deal with ambiguous images for more robust training. To achieve this, we propose to represent an image using not only a semantic embedding but also an accompanying uncertainty embedding, which describes the semantic characteristics and ambiguity of an image, respectively. We further propose an introspective similarity metric to make similarity judgments between images considering both their semantic differences and ambiguities. The proposed IDML framework improves the performance of deep metric learning through uncertainty modeling and attains state-of-the-art results on the widely used CUB-200-2011, Cars196, and Stanford Online
    
[^156]: 通过遍历功能不变路径，构建灵活的机器学习系统

    Engineering flexible machine learning systems by traversing functionally-invariant paths. (arXiv:2205.00334v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.00334](http://arxiv.org/abs/2205.00334)

    该论文介绍了一个名为功能不变路径（FIP）的差分几何框架，用于实现神经网络的灵活、连续适应，以应对各种机器学习目标和网络稀疏化目标。

    

    变压器已成为自然语言处理和计算机视觉中最先进的神经网络架构。在基础模型范例中，大型变压器模型（BERT、GPT3/4、Bloom、ViT）通过自监督任务（如词或图像屏蔽）进行预训练，然后通过微调适应于下游用户应用，包括指令跟随和问答。虽然有许多模型微调方法（如低秩权重更新策略，如LoRA），但仍然对实现网络适应性而不损失知识的数学原理知之甚少。在这里，我们引入了一个差分几何框架，功能不变路径（FIP），为一系列机器学习目标和网络稀疏化目标提供灵活和连续的神经网络适应。我们将神经网络的权重空间构想为一个曲率的黎曼流形，并配备了一个度规张量。

    Transformers have emerged as the state of the art neural network architecture for natural language processing and computer vision. In the foundation model paradigm, large transformer models (BERT, GPT3/4, Bloom, ViT) are pre-trained on self-supervised tasks such as word or image masking, and then, adapted through fine-tuning for downstream user applications including instruction following and Question Answering. While many approaches have been developed for model fine-tuning including low-rank weight update strategies (eg. LoRA), underlying mathematical principles that enable network adaptation without knowledge loss remain poorly understood. Here, we introduce a differential geometry framework, functionally invariant paths (FIP), that provides flexible and continuous adaptation of neural networks for a range of machine learning goals and network sparsification objectives. We conceptualize the weight space of a neural network as a curved Riemannian manifold equipped with a metric tenso
    
[^157]: 语音情绪识别中的Transformer时代的黎明：弥合情感价值差距

    Dawn of the transformer era in speech emotion recognition: closing the valence gap. (arXiv:2203.07378v3 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2203.07378](http://arxiv.org/abs/2203.07378)

    本文通过对几种预训练变体的分析，发现在语音情绪识别领域，使用Transformer架构能够在没有使用显式语言信息的情况下获得最佳的价值预测性能，相关性系数为0.638。

    

    最近的Transformer架构在自监督预训练方面取得了重大突破，并在多个机器学习任务中表现出了巨大的潜力。在音频领域，这种架构也已成功应用于语音情绪识别(SER)领域。然而，现有的研究还没有评估模型大小和预训练数据对下游性能的影响，并且对泛化能力、稳健性、公平性和效率方面的关注有限。本文在几种预训练变体的wav2vec 2.0和HuBERT上进行了详细分析，并在MSP-Podcast的唤起、控制和价值维度上进行了微调，同时使用IEMOCAP和MOSI进行跨语料库泛化测试。据我们所知，在不使用显式语言信息的情况下，我们在MSP-Podcast上获得了最佳的价值预测性能，相关性系数为0.638。

    Recent advances in transformer-based architectures which are pre-trained in self-supervised manner have shown great promise in several machine learning tasks. In the audio domain, such architectures have also been successfully utilised in the field of speech emotion recognition (SER). However, existing works have not evaluated the influence of model size and pre-training data on downstream performance, and have shown limited attention to generalisation, robustness, fairness, and efficiency. The present contribution conducts a thorough analysis of these aspects on several pre-trained variants of wav2vec 2.0 and HuBERT that we fine-tuned on the dimensions arousal, dominance, and valence of MSP-Podcast, while additionally using IEMOCAP and MOSI to test cross-corpus generalisation. To the best of our knowledge, we obtain the top performance for valence prediction without use of explicit linguistic information, with a concordance correlation coefficient (CCC) of .638 on MSP-Podcast. Further
    
[^158]: DeltaCNN: 视频中稀疏帧差异的端到端卷积神经网络推理

    DeltaCNN: End-to-End CNN Inference of Sparse Frame Differences in Videos. (arXiv:2203.03996v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2203.03996](http://arxiv.org/abs/2203.03996)

    DeltaCNN是一种稀疏卷积神经网络框架，通过逐帧稀疏更新，加速视频推理的实践应用，而无需重新训练。

    

    对视频数据进行卷积神经网络推理需要强大的硬件以实现实时处理。考虑到连续帧之间的固有一致性，视频的大部分区域通常变化很小。通过跳过相同的图像区域并截断不重要的像素更新，理论上可以显著减少计算冗余。然而，由于稀疏更新阻碍了计算一致性和内存访问一致性，这些理论上的节省在实践中很难实现；而这些特性对于实际硬件的效率至关重要。通过DeltaCNN，我们提出了一种稀疏卷积神经网络框架，能够实现逐帧稀疏更新，加速视频推理的实践应用。我们为所有典型的卷积神经网络层提供了稀疏实现，并端到端地传播稀疏特征更新-无需随时间累积错误。DeltaCNN适用于所有卷积神经网络，无需重新训练。据我们所知，我们是第一个提出这种方法的。

    Convolutional neural network inference on video data requires powerful hardware for real-time processing. Given the inherent coherence across consecutive frames, large parts of a video typically change little. By skipping identical image regions and truncating insignificant pixel updates, computational redundancy can in theory be reduced significantly. However, these theoretical savings have been difficult to translate into practice, as sparse updates hamper computational consistency and memory access coherence; which are key for efficiency on real hardware. With DeltaCNN, we present a sparse convolutional neural network framework that enables sparse frame-by-frame updates to accelerate video inference in practice. We provide sparse implementations for all typical CNN layers and propagate sparse feature updates end-to-end - without accumulating errors over time. DeltaCNN is applicable to all convolutional neural networks without retraining. To the best of our knowledge, we are the firs
    
[^159]: 知识驱动的分子学习：范式转移的综述

    Knowledge-informed Molecular Learning: A Survey on Paradigm Transfer. (arXiv:2202.10587v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.10587](http://arxiv.org/abs/2202.10587)

    本文调查了知识驱动的分子学习，从范式转移的视角出发，总结了其不同范式的分类和方法论，并分析了领域知识的贡献。

    

    机器学习，尤其是深度学习，显著推动了生物化学领域内的分子研究。传统上，这类研究的建模主要围绕着一些范式展开。例如，预测范式经常用于分子性质预测等任务。为了增强纯数据驱动模型的生成和可解释性，学者们将生化领域的知识融入到这些分子研究模型中。这种融合引发了范式转移的飞速发展，即通过将一个分子学习任务转化为另一个任务来解决问题。随着大型语言模型的出现，这些范式呈现出逐渐趋于统一的趋势。在本文中，我们从范式转移的角度，对知识驱动的分子学习进行了文献综述。我们对这些范式进行分类、审视它们的方法论，并剖析了领域知识的贡献。

    Machine learning, notably deep learning, has significantly propelled molecular investigations within the biochemical sphere. Traditionally, modeling for such research has centered around a handful of paradigms. For instance, the prediction paradigm is frequently deployed for tasks such as molecular property prediction. To enhance the generation and decipherability of purely data-driven models, scholars have integrated biochemical domain knowledge into these molecular study models. This integration has sparked a surge in paradigm transfer, which is solving one molecular learning task by reformulating it as another one. With the emergence of Large Language Models, these paradigms have demonstrated an escalating trend towards harmonized unification. In this work, we delineate a literature survey focused on knowledge-informed molecular learning from the perspective of paradigm transfer. We classify the paradigms, scrutinize their methodologies, and dissect the contribution of domain knowle
    
[^160]: Keras和TensorFlow中静默错误的实证研究

    Silent Bugs in Deep Learning Frameworks: An Empirical Study of Keras and TensorFlow. (arXiv:2112.13314v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2112.13314](http://arxiv.org/abs/2112.13314)

    本文通过实证研究了Keras和TensorFlow中的静默错误，这些错误对用户程序产生了影响。通过对相关问题进行分类和分析，提出了解决这些错误的方法和建议。

    

    深度学习框架已经广泛应用，简化了复杂模型的创建和与各种应用的集成，即使对非深度学习专家也是如此。然而，与任何其他程序一样，它们容易出现错误。本文涉及一种名为静默错误的错误子类：它们导致错误行为，但不会导致系统崩溃、挂起，也不会向用户显示错误消息。由于深度学习应用和框架的“黑盒”和随机性质（最终用户无法理解模型如何做出决策），这样的错误在深度学习应用和框架中更加危险。本文首次对Keras和TensorFlow中的静默错误进行了实证研究，以及它们对用户程序的影响。我们从TensorFlow的GitHub存储库中提取了与Keras相关的已关闭问题。在我们收集的1,168个问题中，有77个是影响用户程序的可重现的静默错误。我们根据对用户程序的影响以及发生错误的组件对这些错误进行了分类。

    Deep Learning (DL) frameworks are now widely used, simplifying the creation of complex models as well as their integration to various applications even to non DL experts. However, like any other programs, they are prone to bugs. This paper deals with the subcategory of bugs named silent bugs: they lead to wrong behavior but they do not cause system crashes or hangs, nor show an error message to the user. Such bugs are even more dangerous in DL applications and frameworks due to the "black-box" and stochastic nature of the systems (the end user can not understand how the model makes decisions). This paper presents the first empirical study of Keras and TensorFlow silent bugs, and their impact on users' programs. We extracted closed issues related to Keras from the TensorFlow GitHub repository. Out of the 1,168 issues that we gathered, 77 were reproducible silent bugs affecting users' programs. We categorized the bugs based on the effects on the users' programs and the components where t
    
[^161]: 推荐系统中用于排名蒸馏的双重修正策略

    Dual Correction Strategy for Ranking Distillation in Top-N Recommender System. (arXiv:2109.03459v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2109.03459](http://arxiv.org/abs/2109.03459)

    本文提出了一种双重修正策略（DCD），用于在推荐系统中更有效地将教师模型的排名信息转移到学生模型。这种方法不仅充分利用了学生模型的预测误差，还提供了更全面的视角，解决了松弛排名蒸馏方法的限制。

    

    知识蒸馏是将训练充分的大模型（教师）的知识转移到小模型（学生）的重要研究领域，对于推荐系统的实际部署而言，它已成为一个重要的研究方向。最近，松弛排名蒸馏（RRD）表明，在推荐列表中蒸馏排名信息能够显著提高性能。然而，该方法仍然存在以下限制：1）它未充分利用学生模型的预测误差，使得训练效率不高；2）它只蒸馏用户侧的排名信息，在稀疏的隐式反馈下提供的视角不足。本文提出了一种更高效的蒸馏方法，即双重修正策略（DCD），通过教师模型和学生模型预测之间的差异来决定要蒸馏的知识。

    Knowledge Distillation (KD), which transfers the knowledge of a well-trained large model (teacher) to a small model (student), has become an important area of research for practical deployment of recommender systems. Recently, Relaxed Ranking Distillation (RRD) has shown that distilling the ranking information in the recommendation list significantly improves the performance. However, the method still has limitations in that 1) it does not fully utilize the prediction errors of the student model, which makes the training not fully efficient, and 2) it only distills the user-side ranking information, which provides an insufficient view under the sparse implicit feedback. This paper presents Dual Correction strategy for Distillation (DCD), which transfers the ranking information from the teacher model to the student model in a more efficient manner. Most importantly, DCD uses the discrepancy between the teacher model and the student model predictions to decide which knowledge to be disti
    
[^162]: 基于生成网络的降阶模型用于预测、数据同化和不确定性量化

    Generative Network-Based Reduced-Order Model for Prediction, Data Assimilation and Uncertainty Quantification. (arXiv:2105.13859v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2105.13859](http://arxiv.org/abs/2105.13859)

    该论文提出了一种基于生成网络的降阶模型，用于解决偏微分方程的逆问题。通过使用无条件模拟进行训练，该模型可以有效量化不确定性，并准确匹配测量数据和黄金标准。

    

    我们提出了一种新的方法，该方法将生成网络（GN）整合到降阶模型（ROM）框架中，用于解决偏微分方程（PDE）的逆问题。目标是匹配可用的测量数据，并估计数值物理模拟的状态和参数的相应不确定性。GN仅使用离散化的PDE模型的无条件模拟进行训练。我们将所提出的方法与黄金标准马尔可夫链蒙特卡罗方法进行了比较。我们将所提出的方法应用于流行病学中的时空隔室模型。结果表明，基于GN的降阶模型能够有效量化不确定性，并且仅使用少量无条件模拟的全阶数值PDE模型即可准确匹配测量数据和黄金标准。

    We propose a new method in which a generative network (GN) integrate into a reduced-order model (ROM) framework is used to solve inverse problems for partial differential equations (PDE). The aim is to match available measurements and estimate the corresponding uncertainties associated with the states and parameters of a numerical physical simulation. The GN is trained using only unconditional simulations of the discretized PDE model. We compare the proposed method with the golden standard Markov chain Monte Carlo. We apply the proposed approaches to a spatio-temporal compartmental model in epidemiology. The results show that the proposed GN-based ROM can efficiently quantify uncertainty and accurately match the measurements and the golden standard, using only a few unconditional simulations of the full-order numerical PDE model.
    
[^163]: 一种用于异步Q学习和TD学习变种的有限样本保证的Lyapunov理论

    A Lyapunov Theory for Finite-Sample Guarantees of Asynchronous Q-Learning and TD-Learning Variants. (arXiv:2102.01567v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2102.01567](http://arxiv.org/abs/2102.01567)

    本文提出了一种统一的框架来研究异步强化学习算法的有限样本收敛保证，并基于Lyapunov分析建立了异步RL算法的均方误差界限。通过对n步TD和TD（λ）的收敛界限的分析，揭示了强化学习中引导技巧效率的理论洞见。

    

    本文通过首先将强化学习算法重新表述为解决固定点方程的"Markovian Stochastic Approximation"(SA)算法，发展了一个统一的框架来研究基于值的异步强化学习算法的有限样本收敛保证。然后，我们使用Lyapunov分析推导出Markovian SA的均方误差界限，基于此结果，我们建立了异步强化学习算法（如Q学习，n步TD，TD（λ）和包括V-trace的离策略TD算法）的有限样本均方收敛界限。作为副产品，通过分析n步TD和TD（λ）的收敛界限，我们提供了关于强化学习中引导技巧效率（即偏差-方差权衡）的理论洞见，这是(Sutton, 1999)中首次提出的一个开放性问题。

    This paper develops an unified framework to study finite-sample convergence guarantees of a large class of value-based asynchronous reinforcement learning (RL) algorithms. We do this by first reformulating the RL algorithms as \textit{Markovian Stochastic Approximation} (SA) algorithms to solve fixed-point equations. We then develop a Lyapunov analysis and derive mean-square error bounds on the convergence of the Markovian SA. Based on this result, we establish finite-sample mean-square convergence bounds for asynchronous RL algorithms such as $Q$-learning, $n$-step TD, TD$(\lambda)$, and off-policy TD algorithms including V-trace. As a by-product, by analyzing the convergence bounds of $n$-step TD and TD$(\lambda)$, we provide theoretical insights into the bias-variance trade-off, i.e., efficiency of bootstrapping in RL. This was first posed as an open problem in (Sutton, 1999).
    
[^164]: Het-node2vec：异构多图嵌入的二阶随机游走采样方法

    Het-node2vec: second order random walk sampling for heterogeneous multigraphs embedding. (arXiv:2101.01425v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2101.01425](http://arxiv.org/abs/2101.01425)

    Het-node2vec是一个算法框架，通过在异构多图上进行二阶随机游走采样，能够捕获图的结构特征和不同类型节点边的语义，有效地提高对异构图的无监督和有监督学习性能。

    

    在多个真实世界应用中，为异构图开发图表示学习方法是基础性的，因为在多个上下文中，图由不同类型的节点和边所特征化。我们引入了一个算法框架（Het-node2vec），将原始的node2vec节点邻域采样方法扩展到了异构多图上。所得到的随机游走样本捕获了图的结构特征以及不同类型的节点和边的语义。所提出的算法可以聚焦于特定的节点或边类型，为所研究的预测问题中有兴趣的少数节点/边类型提供准确的表示。这些丰富而有针对性的表示可以增强对异构图的无监督和有监督学习。

    The development of Graph Representation Learning methods for heterogeneous graphs is fundamental in several real-world applications, since in several contexts graphs are characterized by different types of nodes and edges. We introduce a an algorithmic framework (Het-node2vec) that extends the original node2vec node-neighborhood sampling method to heterogeneous multigraphs. The resulting random walk samples capture both the structural characteristics of the graph and the semantics of the different types of nodes and edges. The proposed algorithms can focus their attention on specific node or edge types, allowing accurate representations also for underrepresented types of nodes/edges that are of interest for the prediction problem under investigation. These rich and well-focused representations can boost unsupervised and supervised learning on heterogeneous graphs.
    
[^165]: 一个提高联邦学习效率的具有公平保证的客户选择方案

    An Efficiency-boosting Client Selection Scheme for Federated Learning with Fairness Guarantee. (arXiv:2011.01783v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2011.01783](http://arxiv.org/abs/2011.01783)

    本文提出了一个基于Lyapunov优化问题的公平保证的客户选择方案，该方案通过估计每个客户端与服务器之间的模型交换时间，提高了联邦学习的效率。

    

    在集中式人工智能模型训练中潜在的隐私泄露问题引起了公众的广泛关注。一种名为联邦学习的并行和分布式计算方案已经出现，通过允许客户在本地执行模型训练，无需上传个人敏感数据来应对隐私问题。在联邦学习中，客户端的数量可能非常大，但用于模型分发和重新上传的带宽非常有限，因此合理地只让部分志愿者参与训练过程。客户选择策略在联邦学习过程中至关重要，涉及训练效率、最终模型质量以及公平性。本文将公平保证的客户选择建模为一个Lyapunov优化问题，提出了一种基于C2MAB的方法，用于估计每个客户端与服务器之间的模型交换时间，基于此进行客户选择。

    The issue of potential privacy leakage during centralized AI's model training has drawn intensive concern from the public. A Parallel and Distributed Computing (or PDC) scheme, termed Federated Learning (FL), has emerged as a new paradigm to cope with the privacy issue by allowing clients to perform model training locally, without the necessity to upload their personal sensitive data. In FL, the number of clients could be sufficiently large, but the bandwidth available for model distribution and re-upload is quite limited, making it sensible to only involve part of the volunteers to participate in the training process. The client selection policy is critical to an FL process in terms of training efficiency, the final model's quality as well as fairness. In this paper, we will model the fairness guaranteed client selection as a Lyapunov optimization problem and then a C2MAB-based method is proposed for estimation of the model exchange time between each client and the server, based on wh
    
[^166]: 机器学习（不）安全性：一系列问题

    Machine Learning (In) Security: A Stream of Problems. (arXiv:2010.16045v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2010.16045](http://arxiv.org/abs/2010.16045)

    本论文针对机器学习在网络安全领域的应用提出了一系列挑战，包括概念漂移、演化、延迟标签和对抗性机器学习对现有解决方案的影响，强调了正确构建和评估基于机器学习的安全解决方案的重要性。

    

    机器学习（ML）已广泛应用于网络安全领域，并被认为是解决该领域许多开放问题的最先进技术。然而，由于安全领域面临的挑战在其他领域可能不会出现，因此很难评估所产生解决方案的好坏。其中一项挑战是概念漂移，它加剧了攻击者和防御者之间的对抗：恶意行为者始终可以创建新的威胁来克服防御解决方案，在某些方法中可能并未考虑这些威胁。因此，了解如何正确构建和评估基于ML的安全解决方案至关重要。在本文中，我们确定、详述和讨论了在将ML技术正确应用于网络安全数据中的主要挑战。我们评估了概念漂移、演化、延迟标签和对抗性ML对现有解决方案的影响。此外，我们还讨论了与数据收集相关的问题如何影响结果的质量。

    Machine Learning (ML) has been widely applied to cybersecurity and is considered state-of-the-art for solving many of the open issues in that field. However, it is very difficult to evaluate how good the produced solutions are, since the challenges faced in security may not appear in other areas. One of these challenges is the concept drift, which increases the existing arms race between attackers and defenders: malicious actors can always create novel threats to overcome the defense solutions, which may not consider them in some approaches. Due to this, it is essential to know how to properly build and evaluate an ML-based security solution. In this paper, we identify, detail, and discuss the main challenges in the correct application of ML techniques to cybersecurity data. We evaluate how concept drift, evolution, delayed labels, and adversarial ML impact the existing solutions. Moreover, we address how issues related to data collection affect the quality of the results presented in 
    
[^167]: $\ell_1$-范数是否能够在受限Laplacian图模型下学习稀疏图形？

    Does the $\ell_1$-norm Learn a Sparse Graph under Laplacian Constrained Graphical Models?. (arXiv:2006.14925v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2006.14925](http://arxiv.org/abs/2006.14925)

    本文研究了在受限Laplacian图模型下学习稀疏图的问题。我们发现经典的$\ell_1$-范数正则化无法有效实现稀疏解，并提出了一种非凸稀疏惩罚的方法来解决这个问题。

    

    我们考虑在受限Laplacian高斯图模型下学习稀疏图的问题。该问题可以被表示为拉普拉斯约束下的精度矩阵的惩罚最大似然估计。与经典的图形套索问题类似，最近的研究利用了$\ell_1$-范数正则化来促进在拉普拉斯约束精度矩阵估计中的稀疏性。然而，我们发现广泛应用的$\ell_1$-范数在这个问题中无法有效地实现稀疏解。通过经验证据，我们观察到非零图权重的数量随着正则化参数的增加而增加。从理论上来看，我们证明了较大的正则化参数将引发一个意外的完全图，即每对顶点之间都用边连接。为了解决这个问题，我们引入非凸稀疏惩罚，并通过求解一系列加权$\ell_1$-范数得到了一个新的估计器。

    We consider the problem of learning a sparse graph under the Laplacian constrained Gaussian graphical models. This problem can be formulated as a penalized maximum likelihood estimation of the Laplacian constrained precision matrix. Like in the classical graphical lasso problem, recent works made use of the $\ell_1$-norm regularization with the goal of promoting sparsity in Laplacian constrained precision matrix estimation. However, we find that the widely used $\ell_1$-norm is not effective in imposing a sparse solution in this problem. Through empirical evidence, we observe that the number of nonzero graph weights grows with the increase of the regularization parameter. From a theoretical perspective, we prove that a large regularization parameter will surprisingly lead to a complete graph, i.e., every pair of vertices is connected by an edge. To address this issue, we introduce the nonconvex sparsity penalty, and propose a new estimator by solving a sequence of weighted $\ell_1$-nor
    

