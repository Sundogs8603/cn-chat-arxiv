# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Graph Rewriting for Graph Neural Networks.](http://arxiv.org/abs/2305.18632) | 该论文提出使用图重写作为建模和分析复杂图形转换的工具，同时将GNN表示为图重写系统可以帮助设计和分析GNN的架构和算法。 |
| [^2] | [Identification of stormwater control strategies and their associated uncertainties using Bayesian Optimization.](http://arxiv.org/abs/2305.18630) | 本文提出了一种基于贝叶斯优化的方法来识别暴雨控制策略并估计相关的不确定性，实现了通过动态配置暴雨网络中的控制资产，调整暴雨网络行为，减少城市洪涝风险的目标。 |
| [^3] | [Global-QSGD: Practical Floatless Quantization for Distributed Learning with Theoretical Guarantees.](http://arxiv.org/abs/2305.18627) | Global-QSGD是一种新颖的全局缩放量化机制，可以提高分布式学习的效率，并且不需要昂贵的误差反馈，并提供了高达$O(\ sqrt{n})$的额外压缩比。 |
| [^4] | [W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition.](http://arxiv.org/abs/2305.18624) | W-procer是一种基于加权原型对比学习的医学少样本命名实体识别方法，在构建基于原型的对比损失和加权网络方面具有创新性，优于现有的最先进方法。 |
| [^5] | [Alfred: A System for Prompted Weak Supervision.](http://arxiv.org/abs/2305.18623) | Alfred是一个系统，通过提示创建机器学习训练数据，而不是由专家编写的程序式弱监督(PWS)系统，用户可以通过自然语言提示为语言和视觉-语言模型编写主题专业知识。Alfred为这种新兴范式的关键步骤提供简单的Python接口，具有大规模数据标注的高吞吐量后端。 |
| [^6] | [Instant Representation Learning for Recommendation over Large Dynamic Graphs.](http://arxiv.org/abs/2305.18622) | 提出了一种名为SUPA的新型图神经网络模型，用于在大型动态图中即时学习表示，以提高推荐系统的效率和效果。 |
| [^7] | [Likelihood-Based Diffusion Language Models.](http://arxiv.org/abs/2305.18619) | 本论文介绍了基于似然的扩散语言模型，并通过算法改进、缩放定律和增加计算，成功构建和发布了一个超过小但广为人知的自回归模型的扩散模型，优于GPT-2 124M。 |
| [^8] | [Networked Time Series Imputation via Position-aware Graph Enhanced Variational Autoencoders.](http://arxiv.org/abs/2305.18612) | 本文提出了一种新颖的基于位置感知图增强变分自编码器方法来处理网络时间序列插补问题。该方法能够处理NTS数据中的图动态和缺失边，从而在合成和真实数据集上实现了优异的表现。 |
| [^9] | [Improving Generalization for Multimodal Fake News Detection.](http://arxiv.org/abs/2305.18599) | 本研究提出了三种模型，采用和微调最先进的多模态Transformer进行多模态假新闻检测，并提出了训练数据增强来提高模型泛化能力。 |
| [^10] | [An Analytic End-to-End Deep Learning Algorithm based on Collaborative Learning.](http://arxiv.org/abs/2305.18594) | 本文提出了一种平滑激活函数的全连接神经网络端到端深度学习收敛分析方法，避免了潜在的抖动问题，并且可以使用协作学习进一步结合多个网络的优势。 |
| [^11] | [On Diffusion Modeling for Anomaly Detection.](http://arxiv.org/abs/2305.18593) | 本文研究了扩散建模在无监督和半监督异常检测中的应用，发现去噪扩散概率模型表现很好但计算成本高，因此提出了一种替代方法——扩散时间概率模型，该模型能够通过较大的时间步长上的高后验密度识别异常，并通过深度神经网络提高效率。 |
| [^12] | [Deep Neural Networks Generalization and Fine-Tuning for 12-lead ECG Classification.](http://arxiv.org/abs/2305.18592) | 该论文提出了一种改善心脏疾病预测质量的方法，即通过在多个数据集上的训练和微调，在不同数据集和临床设置下都可以获得高质量的深度神经网络分类器。 |
| [^13] | [Coeditor: Leveraging Contextual Changes for Multi-round Code Auto-editing.](http://arxiv.org/abs/2305.18584) | Coeditor开发了一个多轮代码自动编辑模型，利用同一代码库中的最近变化来预测对代码区域的编辑，表现出更高的准确率。 |
| [^14] | [Quick Adaptive Ternary Segmentation: An Efficient Decoding Procedure For Hidden Markov Models.](http://arxiv.org/abs/2305.18578) | 提出了一种名为QATS的新方法，用于高效解码隐藏马尔可夫模型序列。它的计算复杂性为多对数和立方，特别适用于具有相对较少状态的大型HMM。 |
| [^15] | [Towards Constituting Mathematical Structures for Learning to Optimize.](http://arxiv.org/abs/2305.18577) | 本文提出了一种结构受到数学启发的L2O模型，其具有广泛的适用性和良好的推广性能，并基于成功的更新规则通常满足的基本数学条件进行了推导。 |
| [^16] | [Fairness of ChatGPT.](http://arxiv.org/abs/2305.18569) | 本文提供了一个使用ChatGPT作为研究案例的LLM有效性和公平性的系统评估，旨在评估ChatGPT在高风险领域的表现，以提供更深入的了解LLM的公平表现，并为偏见缓解和负责任的人工智能系统的发展做出贡献。 |
| [^17] | [PaLI-X: On Scaling up a Multilingual Vision and Language Model.](http://arxiv.org/abs/2305.18565) | 论文介绍了一个多语言视觉与语言模型PaLI-X，通过扩展模型组件和训练任务范围，实现了在复杂任务上的新性能水平，包括图像字幕问答、对象检测、视频问答和视频字幕等，同时在视觉语言基准测试中取得了最新的研究成果。 |
| [^18] | [SHARP: Sparsity and Hidden Activation RePlay for Neuro-Inspired Continual Learning.](http://arxiv.org/abs/2305.18563) | SHARP是一种神经启发式的连续学习方法，利用稀疏的动态连接和激活回放来回放处理过的神经模式，优先回放最近学到的信息，并可以持续更新所有层。 |
| [^19] | [DelBugV: Delta-Debugging Neural Network Verifiers.](http://arxiv.org/abs/2305.18558) | DelBugV是一种使用自动增量调试技术进行DNN验证器调试的工具，它可以产生更简单的验证实例帮助用户快速发现验证器的错误。 |
| [^20] | [Controllable Path of Destruction.](http://arxiv.org/abs/2305.18553) | 本文介绍了可控毁灭路径方法，该方法是一种自我监督的迭代生成器学习方法，通过向修复轨迹的状态-动作对添加条件输入来实现可控性。 |
| [^21] | [Learning Linear Groups in Neural Networks.](http://arxiv.org/abs/2305.18552) | 本研究提出了一种神经网络架构——线性群网络(LGNs)，它可以学习作用于神经网络权重空间上的线性群，而无需预先指定所需的对称性。该结构具有良好的可解释性，并且可以适用于不同数据集及下游任务。 |
| [^22] | [Meta-Regression Analysis of Errors in Short-Term Electricity Load Forecasting.](http://arxiv.org/abs/2305.18550) | 本文通过元回归分析研究了短期电力负荷预测的影响因素，发现网络级别、预测粒度和所使用的算法是影响预测精度的重要因素。 |
| [^23] | [Robust Lipschitz Bandits to Adversarial Corruptions.](http://arxiv.org/abs/2305.18543) | 本文提出的强健Lipschitz赌徒算法，能够在对抗性攻击的情况下实现次线性遗憾，并在强敌手情况下最优。 |
| [^24] | [A Rainbow in Deep Network Black Boxes.](http://arxiv.org/abs/2305.18512) | 彩虹网络是训练深度神经网络的概率模型，通过层内神经元权重互相独立的对齐和随机特征映射来进行线性降维和非线性高维嵌入，在ImageNet和CIFAR-10数据集上进行验证。 |
| [^25] | [Contextual Bandits with Budgeted Information Reveal.](http://arxiv.org/abs/2305.18511) | 本文介绍了一种针对医疗领域“亲治疗”操作的限制，且考虑到了操作预算的具有信息预算的情境赌博机算法，这种算法将在线原始-对偶算法和情境赌博机学习算法有机地结合在一起，取得了很好的效果。 |
| [^26] | [RLAD: Reinforcement Learning from Pixels for Autonomous Driving in Urban Environments.](http://arxiv.org/abs/2305.18510) | RLAD是首个在城市自动驾驶领域应用基于像素的强化学习方法，通过优化图像编码器和路径点编码器等技术，可以提高自动驾驶的性能和效率。 |
| [^27] | [On the Variance, Admissibility, and Stability of Empirical Risk Minimization.](http://arxiv.org/abs/2305.18508) | 本文指出，对于使用平方损失函数的经验风险最小化(ERM)，其次优性必须归因于大的偏差而非方差，并且在ERM的平方误差的偏差-方差分解中，方差项必然具有极小的失误率。作者还提供了Chatterjee的不可允许性定理的简单证明，并表示他们的估计表明ERM的稳定性。 |
| [^28] | [Generalization Ability of Wide Residual Networks.](http://arxiv.org/abs/2305.18506) | 本文研究了在$\mathbb{S}^{d-1}$上使用ReLU激活函数的宽残差网络的泛化能力，表明当宽度$m\rightarrow\infty$时，残差网络核(RNK)统一收敛到残差神经切向核(RNTK)，并且早停策略的宽残差网络可以达到极小化速率，但在训练过度拟合数据时无法很好地推广。 |
| [^29] | [How to Query Human Feedback Efficiently in RL?.](http://arxiv.org/abs/2305.18505) | 该论文提出了一种针对强化学习中人类反馈查询的有效采样方法，以在最少的人类反馈下学习最佳策略，并可应用于具有线性参数化和未知过渡的偏好模型，并引入了基于行动比较反馈的RLHF。 |
| [^30] | [Generalized Disparate Impact for Configurable Fairness Solutions in ML.](http://arxiv.org/abs/2305.18504) | 这篇论文在AI公平领域的连续保护属性中，提出了一族指标，相比于现有的HGR指标，在语义上更为补充，具有充分的可解释性和透明性，在有限样本下鲁棒，可配置以适应特定应用程序，并允许定义细粒度约束条件。 |
| [^31] | [From Adversarial Arms Race to Model-centric Evaluation: Motivating a Unified Automatic Robustness Evaluation Framework.](http://arxiv.org/abs/2305.18503) | 本文旨在建立一个统一的自动鲁棒性评估框架，向以模型为中心的评估转型，利用对抗攻击的优势。研究者们根据模型能力确定鲁棒性评估维度，并针对每个维度指定合理的算法来生成对抗样本。 |
| [^32] | [Escaping mediocrity: how two-layer networks learn hard single-index models with SGD.](http://arxiv.org/abs/2305.18502) | 研究探讨在 SGD 下双层神经网络学习单指数目标函数的样本复杂度问题，发现过参数化只会增加一定因子的收敛性，不同维度和宽度的前置因子精确结果揭示。 |
| [^33] | [DoMo-AC: Doubly Multi-step Off-policy Actor-Critic Algorithm.](http://arxiv.org/abs/2305.18501) | 双重多步骤离策略Actor-Critic算法能够通过结合多步骤策略改进和策略评估来促进最优控制，提高策略梯度估计，并在一般的离策略学习设置中有效。 |
| [^34] | [VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset.](http://arxiv.org/abs/2305.18500) | 本文提出了一种全模态视频文本基础模型VAST及其数据集VAST-27M。该模型可以感知和处理视频中的视觉、音频和字幕模态，通过自动集成多模态字幕和资源，提供更好的支持文本关联的多种任务。 |
| [^35] | [Pre-training Contextualized World Models with In-the-wild Videos for Reinforcement Learning.](http://arxiv.org/abs/2305.18499) | 本文研究了基于野外视频预训练的上下文化世界模型（ContextWM）用于强化学习。ContextWM采用上下文化扩展的潜在动态模型建模，从而可以更好地泛化不同场景之间的知识转移，提高下游视觉控制任务的样本效率和控制性能。 |
| [^36] | [ANPL: Compiling Natural Programs with Interactive Decomposition.](http://arxiv.org/abs/2305.18498) | ANPL是一个编程系统，可以让用户直接操作草图，使用自然语言描述注释模块或孔，并生成一个有机的Python程序，它优于基线。 |
| [^37] | [Generalized equivalences between subsampling and ridge regularization.](http://arxiv.org/abs/2305.18496) | 此篇论文研究了举集岭估计器中子采样和岭回归之间的等价性，发现二者在一定路径中是渐近等价的，并提出了数据相关的方法确定等价路径，间接解决了岭回归调优中预测风险单调性的影响因素问题。 |
| [^38] | [Hardware-aware Training Techniques for Improving Robustness of Ex-Situ Neural Network Transfer onto Passive TiO2 ReRAM Crossbars.](http://arxiv.org/abs/2305.18495) | 本文提出了一种通过应用dropout、重新参数化技巧和正则化等训练方法，提高被动TiO2 ReRAM交叉棒硬件传输神经网络权重的精度的方案。 |
| [^39] | [Adapting Learned Sparse Retrieval for Long Documents.](http://arxiv.org/abs/2305.18494) | 本文提出了两种方法，即 ExactSDM 和 SoftSDM，将顺序依赖模型 (SDM) 适应到学习稀疏检索 (LSR) 中，以解决长篇文档检索的问题。在 MSMARCO 文档和 TREC Robust04 数据集上的实验证明，ExactSDM 和 SoftSDM 都优于现有的 LSR 聚合方法。 |
| [^40] | [Insights from the Design Space Exploration of Flow-Guided Nanoscale Localization.](http://arxiv.org/abs/2305.18493) | 研究了基于流导向纳米定位的设计空间，考虑了能源和信号衰减等因素，为这一新兴领域提供了有希望的解决方案。 |
| [^41] | [DMS: Differentiable Mean Shift for Dataset Agnostic Task Specific Clustering Using Side Information.](http://arxiv.org/abs/2305.18492) | 本研究提出了一种基于少量成对样例的侧信息直接学习数据聚类的方法DMS，与以往方法不同，我们无需知道类别数、类中心或者任何相似的距离度量，该方法可以根据侧信息的任务需求将相同的数据点分成不同的聚类，且在固有的和非固有的数据集任务上表现优异。 |
| [^42] | [Towards a Better Understanding of Representation Dynamics under TD-learning.](http://arxiv.org/abs/2305.18491) | 本文研究了TD学习中对表示动态的影响，并发现端到端TD学习在环境可逆的情况下可以严格降低值逼近误差，在环境进一步假设的情况下，我们可以将表示动态连接到转移矩阵的谱分解。从随机生成的奖励中适合多个值函数作为表示学习的有用辅助任务。 |
| [^43] | [SANE: The phases of gradient descent through Sharpness Adjusted Number of Effective parameters.](http://arxiv.org/abs/2305.18490) | 本文提出了一种基于锐度调整的的有效参数数量梯度下降SANE算法，用于解质量的有效维数度量，并且对大学习率也有较好的鲁棒性。 |
| [^44] | [A Transfer Learning and Explainable Solution to Detect mpox from Smartphones images.](http://arxiv.org/abs/2305.18489) | 该研究提出了一种使用迁移学习和可解释的方法，以从智能手机图像中检测猴痘病毒，可以帮助低收入国家和不具备相应疫苗和检测设施的区域进行初步筛查。 |
| [^45] | [A Bayesian sparse factor model with adaptive posterior concentration.](http://arxiv.org/abs/2305.18488) | 本文提出了一种自适应后验集中的贝叶斯稀疏因子模型，可以推断因子维数和加载矩阵的稀疏结构，同时保持计算可行性，并获得了优越的性能表现。 |
| [^46] | [Solar Irradiance Anticipative Transformer.](http://arxiv.org/abs/2305.18487) | 本文提出了一种预测太阳辐照度的模型，通过学习天空图像的相关特征以及天空图像之间的长距离依赖，对未来的辐照度进行了有效的预测，预测准确率比智能持续模型高出21.45％。 |
| [^47] | [A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets.](http://arxiv.org/abs/2305.18486) | 本文对基准数据集上 ChatGPT 的性能进行了全面的评估，包括问答、文本摘要、代码生成、常识推理、数学问题求解、机器翻译、偏见检测和伦理考虑等任务。研究旨在验证 ChatGPT 的优势和弱点，并为使用语言模型的未来研究提供见解。 |
| [^48] | [Autoencoding Conditional Neural Processes for Representation Learning.](http://arxiv.org/abs/2305.18485) | 本文提出了部分像素空间变分自编码器，结合了自编码器与条件神经过程，可以学习到一系列基本物理和文化概念的表示，并且可以提高上下文预测的准确性。 |
| [^49] | [Neural Fourier Transform: A General Approach to Equivariant Representation Learning.](http://arxiv.org/abs/2305.18484) | 神经傅里叶变换是一种通用的等变表示学习方法，它可以在不需要显式知识的情况下学习组的潜在线性作用，实现对数据隐藏结构的提取。 |
| [^50] | [Bringing regularized optimal transport to lightspeed: a splitting method adapted for GPUs.](http://arxiv.org/abs/2305.18483) | 该论文提出了一种高效的算法，使用分裂技术解决正则化最优输运问题，具有全局收敛性保证和GPU并行化优势，适用于多种应用领域。 |
| [^51] | [Fashion Object Detection for Tops & Bottoms.](http://arxiv.org/abs/2305.18482) | 本文提出了一种面向上下装的时尚物体检测的流程，可以自动区分图像中的上衣和下衣。该方法采用了能够自动识别人体部位的模型，通过选择最佳性能的基于锚点和无锚点的物体检测算法，在速度和准确性方面优于最先进的方法。 |
| [^52] | [A Hybrid Framework of Reinforcement Learning and Convex Optimization for UAV-Based Autonomous Metaverse Data Collection.](http://arxiv.org/abs/2305.18481) | 本论文提出了一种使用无人机辅助的元宇宙网络模型，将资源分配和轨迹控制集成到系统模型中，设计了一种强化学习和凸优化的混合框架，以“合作”方式解决时间顺序优化问题，使得数据收集效率得到提高。 |
| [^53] | [Human Body Shape Classification Based on a Single Image.](http://arxiv.org/abs/2305.18480) | 本文提出一种基于单张图片的分类人体形状的方法，结合实例分割和关键点估计模型，无需基于三维人体重建且可以有效地在噪声环境下进行。我们使用开源基准数据集进行模型训练，并将此方法在新颖图像数据集上进行了定量和定性的评估。 |
| [^54] | [FMM-X3D: FPGA-based modeling and mapping of X3D for Human Action Recognition.](http://arxiv.org/abs/2305.18479) | 本论文解决了将Human Action Recognition中最先进的模型之一——X3D映射到任何FPGA设备上的问题，并首次实现了针对这种复杂模型架构进行Human Action Recognition任务的目标。 |
| [^55] | [Forward and Inverse Approximation Theory for Linear Temporal Convolutional Networks.](http://arxiv.org/abs/2305.18478) | 本论文对卷积神经网络在序列建模时的逼近性质进行了理论分析并提出了一种全面的特征描述, 可以有效捕捉时间卷积体系结构所能捕捉的顺序关系类型。 |
| [^56] | [Beyond the Meta: Leveraging Game Design Parameters for Patch-Agnostic Esport Analytics.](http://arxiv.org/abs/2305.18477) | 本论文提出了一种新的跨版本的电子竞技分析方法，通过利用游戏设计参数并利用聚类技术创建角色表征形式来解决传统方法短寿命的问题。以Dota 2为例验证了这种方法，取得了显著的性能提升。 |
| [^57] | [Approximation theory of transformer networks for sequence modeling.](http://arxiv.org/abs/2305.18475) | 本文证明了变压器假设空间的普遍逼近定理，并提出了一种新的规律概念用于精确逼近速率估计，揭示了变压器适用于逼近哪些类型的序列关系，并讨论了其与传统序列建模方法之间的结构偏差。 |
| [^58] | [Alg{\i}lanan Stres Testinin Makine \"O\u{g}renmesi ile Analiz Edilmesi.](http://arxiv.org/abs/2305.18473) | 本研究利用机器学习重新分析了感知压力测试，揭示了测试问题的重要性不相等，展示了在心理上观察到的不同模式。 |
| [^59] | [Deep Predictive Coding with Bi-directional Propagation for Classification and Reconstruction.](http://arxiv.org/abs/2305.18472) | 本文提出了一种新的学习算法，双向预测编码（DBPC）将现有的预测编码方法扩展为支持正向和反馈信息传播的网络。该网络可以同时执行分类和重建任务。 |
| [^60] | [Convergence of AdaGrad for Non-convex Objectives: Simple Proofs and Relaxed Assumptions.](http://arxiv.org/abs/2305.18471) | 本文提出了仅有互易噪声方差和有界平滑性假设的非凸目标的AdaGrad算法的简单收敛性证明，证明中基于辅助函数$\xi$，比现有结果更紧密，在超参数化的情况下，能够确保梯度范数小于$\varepsilon$的迭代次数为$\mathcal{O}(\frac{1}{\varepsilon^2})$，并考虑了一种实际平滑假设$(L_0,L_1)$-平滑条件。 |
| [^61] | [Reducing Communication for Split Learning by Randomized Top-k Sparsification.](http://arxiv.org/abs/2305.18469) | 提出了一种通过随机Top-k稀疏化减少通信效率的Split Learning方法，实验证明该方法在相同压缩水平下比其他方法有更好的模型性能。 |
| [^62] | [Geometric Graph Filters and Neural Networks: Limit Properties and Discriminability Trade-offs.](http://arxiv.org/abs/2305.18467) | 本文研究了从流形采样点构造的图与流形神经网络的关系，并证明了这些图形上的卷积滤波器和神经网络收敛于连续流形上的卷积滤波器和神经网络。研究发现，图滤波器的可分性和近似流形滤波器所需行为的能力之间存在重要权衡，在神经网络中由于非线性的频率混合属性而得到改善。 |
| [^63] | [Test-Time Training on Nearest Neighbors for Large Language Models.](http://arxiv.org/abs/2305.18466) | 该论文提出了一种基于最近邻的测试时间训练方法，通过检索和微调少量邻居的文本数据，该方法在大语言模型上显著提高了性能。 |
| [^64] | [Federated Learning of Gboard Language Models with Differential Privacy.](http://arxiv.org/abs/2305.18465) | 本文讨论了在Gboard中使用联合学习和差分隐私(DP)训练语言模型(LMs)的方法，提出了新的客户参与标准，在实现有意义的形式DP保证的同时提供了有利的隐私-效用交换。在对公共数据进行预训练的基础上，我们训练并部署了超过20个LMs以实现高效用和$\rho-$zCDP隐私保证。 |
| [^65] | [Privileged Knowledge Distillation for Sim-to-Real Policy Generalization.](http://arxiv.org/abs/2305.18464) | 论文提出了一种新的单阶段特权知识蒸馏方法（HIB），通过捕捉历史轨迹的特权知识表示来学习，缩小模拟和真实之间的差距。 |
| [^66] | [Membership Inference Attacks against Language Models via Neighbourhood Comparison.](http://arxiv.org/abs/2305.18462) | 本文提出两种新的基于邻域比较的攻击策略，利用语言数据的内在结构来提高成员推断攻击的性能，并在几个公开数据集上证明这些攻击的有效性。 |
| [^67] | [Bandwidth Optimal Pipeline Schedule for Collective Communication.](http://arxiv.org/abs/2305.18461) | 本文提出了一种强多项式时间算法，用于在任何拓扑结构上实现带宽最优的全聚合/归约散开，为解决方案提供了通用性，可以轻松扩展到其他形式的集合通信方法。 |
| [^68] | [Minimum Width of Leaky-ReLU Neural Networks for Uniform Universal Approximation.](http://arxiv.org/abs/2305.18460) | 研究表明具有临界宽度的Leaky-ReLU神经网络可以在紧致域K上实现$L^p(K,\mathbb{R}^{d_y})$的UAP，而本文给出的最小宽度$w_{\min}=\max(d_x+1,d_y)+1_{d_y=d_x+1}$则适用于函数类$C(K,\mathbb{R}^{d_y})$，考虑到输出维度的影响。 |
| [^69] | [Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning.](http://arxiv.org/abs/2305.18459) | 本文研究了单一扩散模型在建模多个任务的策略训练的大规模离线数据时的有效性。该方法名为Multi-Task Diffusion Model (MTDiff)，结合了Transformer骨干和提示学习，能够在如此复杂的多任务环境下取得相当不错的性能。 |
| [^70] | [Conditional Support Alignment for Domain Adaptation with Label Shift.](http://arxiv.org/abs/2305.18458) | 本文提出了一种新颖的条件对抗支持对齐方法，以最小化源域和目标域特征表示分布之间的条件对称支持分歧，针对标签分布偏移的域适应问题，相对现有的方法有显著提高。 |
| [^71] | [Learning Strong Graph Neural Networks with Weak Information.](http://arxiv.org/abs/2305.18457) | 本文提出了D$^2$PT，一个双通道的GNN框架，以处理具有多种数据缺失且相互影响的情况，其关键点包括在GNN中实现长程传播和允许信息传播到偏离节点。 |
| [^72] | [Baselines for Identifying Watermarked Large Language Models.](http://arxiv.org/abs/2305.18456) | 该论文介绍了一套基线算法，用于识别带有水印的大型语言模型，这些算法分析了带有和不带有水印的LLM所产生的输出分布和逻辑分布之间的差异。 |
| [^73] | [Diff-Instruct: A Universal Approach for Transferring Knowledge From Pre-trained Diffusion Models.](http://arxiv.org/abs/2305.18455) | 本文提出了一种通用框架 Diff-Instruct，能够以无需数据方式将预训练扩散模型中的知识传递给其他生成模型，仅需预训练 DM 和一个数据集。该框架是建立在严谨的数学基础上的，指导过程直接对应于最小化一种新型散度——Integral Kullback-Leibler (IKL) 散度。我们的方法在半监督学习、图像合成和视频预测中展示了其优越性。 |
| [^74] | [PubChemQC B3LYP/6-31G*//PM6 dataset: the Electronic Structures of 86 Million Molecules using B3LYP/6-31G* calculations.](http://arxiv.org/abs/2305.18454) | 这个数据集包含了8600万个分子的电子性质，覆盖了从基本化合物到生物分子的各种分子，使用了B3LYP/6-31G*和PM6方法计算得出，并提供了多种格式以及五个子数据集。 |
| [^75] | [Conditional Diffusion Models for Semantic 3D Medical Image Synthesis.](http://arxiv.org/abs/2305.18453) | 这篇论文提出了Med-DDPM，一种使用扩散模型进行语义化三维医学图像合成的创新解决方案，它通过控制像素级掩码标签的生成过程，能够生成高质量逼真的医学图像，并且在精度、稳定性和多样性等指标上优于GAN技术，也优于传统的增强技术和GAN合成图像。 |
| [^76] | [Generating Driving Scenes with Diffusion.](http://arxiv.org/abs/2305.18452) | 本文提出一种利用扩散和对象检测相结合的“场景扩散”系统，直接生成具有真实感和物理合理性的场景，能够适应美国不同地区生成特定的场景。 |
| [^77] | [Shift-Robust Molecular Relational Learning with Causal Substructure.](http://arxiv.org/abs/2305.18451) | 本文提出了一种鲁棒性强的分子关系学习模型CMRL，通过检测与化学反应因果相关的核心亚结构来应对分子关系学习中的数据分布变化。 |
| [^78] | [GBG++: A Fast and Stable Granular Ball Generation Method for Classification.](http://arxiv.org/abs/2305.18450) | 本文提出了一种基于关注机制的快速稳定的颗粒球生成方法，可以在分类任务中应用。 |
| [^79] | [Taming AI Bots: Controllability of Neural States in Large Language Models.](http://arxiv.org/abs/2305.18449) | 本文提出了一个问题，是否可以通过适当选择提示，控制AI bot到达任何状态，而研究表明训练良好的Bot能几乎确定地到达任何意义子集，具有可控性。 |
| [^80] | [Neural Network Reduction with Guided Regularizers.](http://arxiv.org/abs/2305.18448) | 本文提出了一种称为 “导向性正则化” 的新方法，通过在训练中优先考虑某些神经网络单元的权重，从而使得某些单元不那么重要，从而可以削减神经元，实现神经网络的压缩。 |
| [^81] | [Unleashing the Power of Randomization in Auditing Differentially Private ML.](http://arxiv.org/abs/2305.18447) | 本论文为差分隐私机器学习算法的审计提出了一种严格的方法，通过设计随机化的canaries来增强模型的容错能力和可解释性，同时在样本复杂度方面得到显著改进。 |
| [^82] | [Trompt: Towards a Better Deep Neural Network for Tabular Data.](http://arxiv.org/abs/2305.18446) | Trompt是一种新颖的深度神经网络结构，其中分离了表格数据学习策略，它通过提示学习的方式来调整大型预先训练的模型，以便更好地能够处理表格数据，并取得了很好的效果。 |
| [^83] | [Intelligent gradient amplification for deep neural networks.](http://arxiv.org/abs/2305.18445) | 本文提出了一种智能确定梯度放大层次的方法，用于解决深度学习模型中的梯度消失问题和加速训练，实验结果表明可以提高深度神经网络的收敛速度和性能。 |
| [^84] | [Continual Task Allocation in Meta-Policy Network via Sparse Prompting.](http://arxiv.org/abs/2305.18444) | 本文提出的CoTASP可以通过学习过完备字典来生成稀疏掩码作为提示，从而从元策略网络中提取与每个任务相关的子网络，实现了快速适应新任务，同时保留了之前任务的共同知识。 |
| [^85] | [Improved Projection-free Online Continuous Submodular Maximization.](http://arxiv.org/abs/2305.18442) | 本文提出了一种名为POBGA的改进的无投影算法，该算法可用于在线连续子模最大化问题，具有更低的后悔度上限，同时保持与先前算法相同的计算复杂性。这是通过将投影算法、阻塞技术和在线boosting梯度上升技术相结合实现的。该算法具有分散式实现的变体，适用于低局部性的问题。 |
| [^86] | [DeCoR: Defy Knowledge Forgetting by Predicting Earlier Audio Codes.](http://arxiv.org/abs/2305.18441) | DeCoR是一种持续音频表示学习方法，通过预测早期音频编码中的量化索引，间接从早期模型向最新模型提取知识，避免在学习新数据时遗忘以前学习的任务。这种方法提高了声学场景分类准确性，与持续自监督表示学习相结合，存储和计算开销小，是一种高效的持续学习解决方案。 |
| [^87] | [Black-Box Anomaly Attribution.](http://arxiv.org/abs/2305.18440) | 本论文提出了一种名为“可能性补偿 ”的新颖归因框架，该框架可以在没有训练数据的情况下解决黑盒模型的异常归因问题。 |
| [^88] | [Alteration-free and Model-agnostic Origin Attribution of Generated Images.](http://arxiv.org/abs/2305.18439) | 该论文提出了一种无需改动且适用于多种生成模型的源头归属方法，通过反向工程分析生成图片的来源，解决了现有方法对特定型号的依赖限制问题，并取得了准确性和灵活性的提高。 |
| [^89] | [Explainable Machine Learning for Categorical and Mixed Data with Lossless Visualization.](http://arxiv.org/abs/2305.18437) | 本文提出了一些数值编码和可视化方法，以支持机器学习算法处理混合数据，并提出了可解释的多分类模型和SRG算法来生成解释性分类模型。 |
| [^90] | [Statistically Optimal K-means Clustering via Nonnegative Low-rank Semidefinite Programming.](http://arxiv.org/abs/2305.18436) | 本文提出了一种与NMF算法一样简单且可扩展的K均值聚类算法，该算法通过解决非负低秩半定规划问题获得了强大的统计最优性保证，实验证明该算法在合成和实际数据集上表现优异。 |
| [^91] | [Cross-Entropy Estimators for Sequential Experiment Design with Reinforcement Learning.](http://arxiv.org/abs/2305.18435) | 这篇论文提出了一个基于交叉熵估计器的备选下界估计方法，这个方法不需要对比样本，可以更精确地估计高信息增益，允许学习更优秀的设计策略，并且与隐式概率模型兼容。 |
| [^92] | [Parallel Coordinates for Discovery of Interpretable Machine Learning Models.](http://arxiv.org/abs/2305.18434) | 本研究使用平行坐标进行可视化知识发现，提出了一种新的数据分类器算法Hyper，并能够发现终端用户易于理解的、用于提高可解释性的混合和纯粹的超块。 |
| [^93] | [Cognitively Inspired Cross-Modal Data Generation Using Diffusion Models.](http://arxiv.org/abs/2305.18433) | 本文提出一种基于多模态扩散模型训练和采样的数据生成方法，使用通道图像调节来学习跨模态相关性，实现条件生成。 |
| [^94] | [Interactive Decision Tree Creation and Enhancement with Complete Visualization for Explainable Modeling.](http://arxiv.org/abs/2305.18432) | 本文提出了两个新方法用于创建和增强决策树模型的可解释性和可视化能力，分别为弯曲坐标和移位配对坐标。 |
| [^95] | [Optimizing Airbnb Search Journey with Multi-task Learning.](http://arxiv.org/abs/2305.18431) | 本文提出了一种新的多任务深度学习模型架构Journey Ranker，来解决Airbnb搜索过程中的唯一挑战，即客户和主机的偏好，该模型可应用于多个用例。 |
| [^96] | [Scalable and Weakly Supervised Bank Transaction Classification.](http://arxiv.org/abs/2305.18430) | 本文提出了一种可扩展的银行交易分类方法，利用弱监督、自然语言处理和深度神经网络技术，最小化对手动注释的依赖，能够快速扩展到新的和组合用例，可用于财务健康报告和信用风险评估等金融应用。 |
| [^97] | [Visual Knowledge Discovery with General Line Coordinates.](http://arxiv.org/abs/2305.18429) | 本文提出了基于General Line Coordinates的可视化知识发现方法，用于生成、解释和可视化非线性分类器及其解释规则，并且可以提供更好的性能。 |
| [^98] | [GRD: A Generative Approach for Interpretable Reward Redistribution in Reinforcement Learning.](http://arxiv.org/abs/2305.18427) | 本文提出了一种可解释奖励再分配的方法，通过因果透视建模状态和行动贡献，产生可解释的返回分解。生成返回分解（GRD）框架用于延迟奖励场景中的策略优化。 |
| [^99] | [Employing Explainable Artificial Intelligence (XAI) Methodologies to Analyze the Correlation between Input Variables and Tensile Strength in Additively Manufactured Samples.](http://arxiv.org/abs/2305.18426) | 本研究使用可解释的人工智能方法，探究了增材制造中输入变量和拉伸强度的相关性，发现Infill百分比和挤出温度对于拉伸强度具有最高的正相关和负相关影响。 |
| [^100] | [Efficient Storage of Fine-Tuned Models via Low-Rank Approximation of Weight Residuals.](http://arxiv.org/abs/2305.18425) | 本论文提出了一种利用权重残差低秩特性实现精细调整模型高效存储的新方法ERE，并通过额外量化和分层秩分配来提高存储效率，实验结果表明该方法显著减少内存占用，同时保持性能。 |
| [^101] | [On the Role of Noise in the Sample Complexity of Learning Recurrent Neural Networks: Exponential Gaps for Long Sequences.](http://arxiv.org/abs/2305.18423) | 本文研究了添加噪声的多层Sigmoid循环神经网络在学习序列分类问题上的样本复杂度问题，发现带噪声情况下样本复杂度可以用$\log(T/\sigma)$来界定，不存在噪声时下界为$wT$，两者存在指数级别的差距。 |
| [^102] | [HyperTime: Hyperparameter Optimization for Combating Temporal Distribution Shifts.](http://arxiv.org/abs/2305.18421) | 本文提出了一种名为HyperTime的超参数优化方法，用于寻找时间上鲁棒的预测性能超参数，该方法在历史验证数据集上对平均验证损失和最坏情况验证损失设置了词典优先级顺序，并在多个带时间分布偏移的机器学习任务上表现强劲。 |
| [^103] | [Sample Complexity of Variance-reduced Distributionally Robust Q-learning.](http://arxiv.org/abs/2305.18420) | 本文提出了两种新颖的无模型算法，为动态决策面对分布变化问题提供了鲁棒的解决方案，并通过将Q-learning与方差减少技术相结合，实现了样本复杂度的有效控制。 |
| [^104] | [Semantic Segmentation with Bidirectional Language Models Improves Long-form ASR.](http://arxiv.org/abs/2305.18419) | 本研究提出一种采用双向语言模型进行语义分割的方法，可以有效提高长篇音频识别的准确性和速度。 |
| [^105] | [Just a Glimpse: Rethinking Temporal Information for Video Continual Learning.](http://arxiv.org/abs/2305.18418) | 本文提出了一种基于单个帧的新型重播机制SMILE，用于有效的视频连续学习。实验表明，在内存受到极端限制时，视频的多样性比时间信息更重要。 |
| [^106] | [Determinantal Point Process Attention Over Grid Codes Supports Out of Distribution Generalization.](http://arxiv.org/abs/2305.18417) | 本文描述了一个基于点过程注意力和网格编码的算法，在分布外测试集上实现了泛化能力，为理解大脑强泛化能力提供了见解。 |
| [^107] | [Examining the Role and Limits of Batchnorm Optimization to Mitigate Diverse Hardware-noise in In-memory Computing.](http://arxiv.org/abs/2305.18416) | 本文研究了模拟交叉栏的非理想性以及它们对点积操作的影响，并通过实时交叉栏感知精细调整批量归一化参数的方法来减轻影响，从而降低内存计算的重新训练的硬件成本。 |
| [^108] | [Geometric Algebra Transformers.](http://arxiv.org/abs/2305.18415) | 本文介绍了一种通用架构几何代数变换器（GATr），用于解决几何数据问题。GATr使用投影几何代数表示输入输出和状态，具有可缩放性、表达性、多功能性。在n体建模和机器人规划的实验中，GATr相对于非几何基线表现出强大的改进。 |
| [^109] | [Learning to Learn from APIs: Black-Box Data-Free Meta-Learning.](http://arxiv.org/abs/2305.18413) | 该论文提出了一个BiDf-MKD框架，可以从一组API库中无需访问训练数据，直接进行元学习；能够在更广泛的黑盒API上进行元学习，提高了元模型的泛化性能和应用范围。 |
| [^110] | [Short-term Temporal Dependency Detection under Heterogeneous Event Dynamic with Hawkes Processes.](http://arxiv.org/abs/2305.18412) | 本论文讨论了Hawkes过程下异质事件动态的短时序依赖检测问题，提出了一种结合MHP和自激神经网络的鲁棒框架，实验结果显示其卓越性能。 |
| [^111] | [Feature-Learning Networks Are Consistent Across Widths At Realistic Scales.](http://arxiv.org/abs/2305.18411) | 本研究发现宽度对于神经网络的动态没有影响，网络在早期训练中表现出一致性，对于简单任务来说这一一致性贯穿整个训练过程，且大宽度下的结构特性是一致的。这表明特征学习极限可以捕捉到现实模型中出现的现象。 |
| [^112] | [Understanding Breast Cancer Survival: Using Causality and Language Models on Multi-omics Data.](http://arxiv.org/abs/2305.18410) | 本文旨在探究多组学数据在因果推断、基因组学和乳腺癌中的应用，利用大规模语言模型辅助解决因果推断方法的评估问题，突出了如何利用因果关系分析基因组扰动对乳腺癌患者生存的影响。 |
| [^113] | [Direction-oriented Multi-objective Learning: Simple and Provable Stochastic Algorithms.](http://arxiv.org/abs/2305.18409) | 本文提出了一种新的面向方向的多目标问题，并给出了两种随机算法以解决这个问题，理论上收敛到帕累托稳定点。 |
| [^114] | [A Group Symmetric Stochastic Differential Equation Model for Molecule Multi-modal Pretraining.](http://arxiv.org/abs/2305.18407) | MoleculeSDE是用于分子多模态预训练的群对称随机微分方程模型，通过在输入空间中直接生成3D几何与2D拓扑之间的转换，它能够更有效地保存分子结构信息。 |
| [^115] | [A machine learning approach to the prediction of heat-transfer coefficients in micro-channels.](http://arxiv.org/abs/2305.18406) | 本文使用一种新的方法——多输出高斯过程回归模型，来预测微通道中的传热系数，该模型的表现优于传统的经验相关式。 |
| [^116] | [Dink-Net: Neural Clustering on Large Graphs.](http://arxiv.org/abs/2305.18405) | Dink-Net是一个可扩展的大规模图形神经聚类方法，该方法利用了膨胀和收缩的思想来处理百万节点的大图，并在各种基准数据集上优于现有的最先进方法。 |
| [^117] | [Conformal Prediction with Large Language Models for Multi-Choice Question Answering.](http://arxiv.org/abs/2305.18404) | 本研究探讨了如何利用符合性预测技术，在多项选择题回答任务中为语言模型提供不确定性量化。我们发现符合性预测的不确定性估计与预测准确性密切相关。 |
| [^118] | [Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning.](http://arxiv.org/abs/2305.18403) | 本论文提出了一种名为LoRAPrune的框架，可以高效微调和部署大型预训练模型，通过利用低秩自适应的值和梯度来设计PEFT感知的剪枝准则，并提出了一个迭代剪枝过程来去除冗余参数，实验结果表明与最先进的方法相比，可以显著降低模型大小和推理时间，同时保持竞争性的准确性。 |
| [^119] | [Neural Sculpting: Uncovering hierarchically modular task structure through pruning and network analysis.](http://arxiv.org/abs/2305.18402) | 本文提出了一种名为“神经雕塑”的方法，该方法通过神经网络的修剪和分析生成的图形结构来揭示任务的子函数的层次结构。该方法在布尔任务上得到了有效验证。 |
| [^120] | [A Meta-learning Framework for Tuning Parameters of Protection Mechanisms in Trustworthy Federated Learning.](http://arxiv.org/abs/2305.18400) | 提出了一个元学习框架，用于调整可信联邦学习保护机制的参数，以在隐私泄露、效用损失和效率降低之间进行权衡。 |
| [^121] | [On the impact of activation and normalization in obtaining isometric embeddings at initialization.](http://arxiv.org/abs/2305.18399) | 本论文研究了深度神经网络中的 Gram 矩阵结构，证明了激活函数和层规范化结合使用可以在初始化时偏向指数级深度等距，从而弥补了现有理论的空白。 |
| [^122] | [Mitigating Inappropriateness in Image Generation: Can there be Value in Reflecting the World's Ugliness?.](http://arxiv.org/abs/2305.18398) | 该论文研究了文本驱动的图像生成模型复制不适当人类行为的问题，并提出了抑制生成不适当内容的策略，该策略利用模型对世界丑陋的表现与人类偏好对齐。 |
| [^123] | [Prediction of the 2023 Turkish Presidential Election Results Using Social Media Data.](http://arxiv.org/abs/2305.18397) | 本文利用社交媒体数据和传统民调数据预测2023年土耳其总统选举结果，结果表明基于社交媒体交互数量的ARIMAX模型最优。 |
| [^124] | [LLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly Transformers.](http://arxiv.org/abs/2305.18396) | 本文中，研究人员通过使用隐私计算友好的近似方法替换transformer架构中计算和通信密集的运算符，实现了大幅降低私有推断成本的效果，并在保持准确性的前提下实现了计算加速和通信开销降低。 |
| [^125] | [Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks.](http://arxiv.org/abs/2305.18395) | 本论文提出了一种KARD方法，可以通过向小型语言模型中加入从外部知识库检索到的增强知识来解决知识密集型推理任务中小型语言模型记忆能力有限的问题。 |
| [^126] | [On Optimal Regularization Parameters via Bilevel Learning.](http://arxiv.org/abs/2305.18394) | 本文提出了一个利用双层学习确定正则化参数的方法，并提出了一个新的表征正确参数的条件。 |
| [^127] | [Speech Intelligibility Assessment of Dysarthric Speech by using Goodness of Pronunciation with Uncertainty Quantification.](http://arxiv.org/abs/2305.18392) | 本文提出了一种利用不确定性量化的发音准确度评估方法，能够有效评估运动障碍患者的语音清晰度，其中，利用先验规范化的最大对数几率发音准确度（maxlogit GoP）取得了最佳表现。 |
| [^128] | [MemeGraphs: Linking Memes to Knowledge Graphs.](http://arxiv.org/abs/2305.18391) | 该论文提出了一种使用场景图和知识图谱结构化表达网络文化表情包的方法，并将其用于分类。结果显示该方法相比使用学习表达式的模型有所改善。 |
| [^129] | [Emergent Modularity in Pre-trained Transformers.](http://arxiv.org/abs/2305.18390) | 本论文研究了预训练Transformers中的自发模块化现象，发现神经元可以进行功能专业化，并通过聚类建立起模块化结构，此结构可被有效扰动。 |
| [^130] | [AnoRand: A Semi Supervised Deep Learning Anomaly Detection Method by Random Labeling.](http://arxiv.org/abs/2305.18389) | 本文提出了一种名为AnoRand的半监督深度学习异常检测方法，通过深度架构和随机合成标签生成相结合，能够尽可能地学习一个类别。 |
| [^131] | [The Statistical Benefits of Quantile Temporal-Difference Learning for Value Estimation.](http://arxiv.org/abs/2305.18388) | 本文研究了强化学习中的时间差分策略评估问题，分析了量化时间差分学习算法在任务中的应用。结果表明，即使从实践者没有超过平均回报之外的回报分布的兴趣之处，在表格设置中，QTD也可以提供比传统TD学习等方法更好的性能。 |
| [^132] | [Augmenting Character Designers Creativity Using Generative Adversarial Networks.](http://arxiv.org/abs/2305.18387) | 本文探讨了如何使用生成对抗网络来增强角色设计师的创造力，在多媒体项目中构思新的角色，研究结果表明GAN可以提供新的、多样化的设计选项，同时减少设计时间和工作量。 |
| [^133] | [A Synergistic Framework Leveraging Autoencoders and Generative Adversarial Networks for the Synthesis of Computational Fluid Dynamics Results in Aerofoil Aerodynamics.](http://arxiv.org/abs/2305.18386) | 本研究提出了一种创新方法，协同利用自编码器和生成对抗网络来生成CFD结果，以减少气动预测中的时间和成本方面具有深远的潜力。 |
| [^134] | [Self-attention Dual Embedding for Graphs with Heterophily.](http://arxiv.org/abs/2305.18385) | 本研究提出了一种新颖的图神经网络，采用自注意力机制，适用于异质性图和同质性图，并在许多标准数据集上展示出最先进的性能。 |
| [^135] | [Backdoor Attacks Against Incremental Learners: An Empirical Evaluation Study.](http://arxiv.org/abs/2305.18384) | 本文提出了增量学习器的后门攻击可能存在的安全风险，并实证揭示了11个典型的增量学习器对基于中毒的后门攻击的高度脆弱性。 |
| [^136] | [A Three-regime Model of Network Pruning.](http://arxiv.org/abs/2305.18383) | 该论文提出了一种基于学习统计力学的现象学模型，使用类似温度和负载的模型参数来建模神经网络训练超参数对修剪性能的影响，通过分类修剪后神经网络损失景观的全局结构构建了一个三重模型，揭示了修剪的优化过程以及对神经网络损失景观的变化规律。 |
| [^137] | [Adaptive Sparsity Level during Training for Efficient Time Series Forecasting with Transformers.](http://arxiv.org/abs/2305.18382) | 本文提出了“具有自适应稀疏度级别的修剪”(PALS), 通过稀疏训练和训练期间方法中的“扩张”机制，在Transformer模型中实现高效的时间序列预测。 |
| [^138] | [Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection.](http://arxiv.org/abs/2305.18381) | 研究提出了一种基于选择最有价值的样本的方法，以扩展现有的蒸馏算法，从而更好地利用训练样本，显著降低训练成本，拓展对更大更多元化数据集的数据集蒸馏，并持续提高性能。 |
| [^139] | [Potential-based Credit Assignment for Cooperative RL-based Testing of Autonomous Vehicles.](http://arxiv.org/abs/2305.18380) | 本文提出了一种协作强化学习方法，利用基于潜力的奖励塑形来解决自动驾驶车辆中多个代理的信用分配问题。 |
| [^140] | [Constrained Optimization via Exact Augmented Lagrangian and Randomized Iterative Sketching.](http://arxiv.org/abs/2305.18379) | 本文提出了一种自适应的不精确牛顿法来求解等式约束的非线性、非凸优化问题，通过随机迭代草图求解增广拉格朗日牛顿系统，并通过在精确增广拉格朗日优势函数上执行线搜索来选择合适的步长。该方法具有高效、鲁棒性好的特点。 |
| [^141] | [Disentanglement via Latent Quantization.](http://arxiv.org/abs/2305.18378) | 本文通过潜在量化的方式实现了解缠表示学习，并通过严格的交流瓶颈和强大的模型规范化成功将数据进行了组合编码和解码，最终在多个基准数据集上实现了最先进的解缠性能，并提高了标准VAE模型学习表征的可解释性。 |
| [^142] | [BadLabel: A Robust Perspective on Evaluating and Enhancing Label-noise Learning.](http://arxiv.org/abs/2305.18377) | 本文介绍了一种新的标签噪声类型BadLabel，它通过标签翻转攻击显著降低现有标签噪声学习（LNL）算法性能，因此提出了一种鲁棒的LNL方法，表现出在六个数据集上最先进的性能。 |
| [^143] | [Fast and Accurate Dual-Way Streaming PARAFAC2 for Irregular Tensors -- Algorithm and Application.](http://arxiv.org/abs/2305.18376) | 本文提出了一种名为Dash的有效而准确的PARAFAC2分解方法，其采用两阶段ALS算法，在双向流处理非规则张量时高效地处理新行，同时提供了在该应用场景下的异常检测度量方法，实验结果显示Dash方法优于现有的方法。 |
| [^144] | [Learning to Jump: Thinning and Thickening Latent Counts for Generative Modeling.](http://arxiv.org/abs/2305.18375) | 本文探讨了如何使用学习跳跃方法来生成建模各种类型的数据，特别是对于计数和非负连续数据等高稀疏度、倾斜度、重尾度或过度分散度的数据，使用学习跳跃相比于学习去噪有更好的效果。 |
| [^145] | [Pure Spectral Graph Embeddings: Reinterpreting Graph Convolution for Top-N Recommendation.](http://arxiv.org/abs/2305.18374) | 本文提出了一种新的纯谱图嵌入方法，在Top-N推荐任务中比现有基于图卷积的模型具有更优的性能。 |
| [^146] | [Assumption Generation for the Verification of Learning-Enabled Autonomous Systems.](http://arxiv.org/abs/2305.18372) | 本文提出了一种为安全保证提供假设的做法，以用于验证具有复杂环境和学习增强组件的自主系统，通过自动生成适当的DNN行为假设，来满足要求的安全属性。 |
| [^147] | [Explainable Brain Age Prediction using coVariance Neural Networks.](http://arxiv.org/abs/2305.18370) | 本文提出了使用协方差神经网络进行可解释的脑龄预测的框架，可以通过皮质厚度特征捕捉加速老化，并反映出增加的神经疾病或认知障碍的风险。 |
| [^148] | [Using VGG16 Algorithms for classification of lung cancer in CT scans Image.](http://arxiv.org/abs/2305.18367) | 研究开发了基于VGG16算法的肺癌结节检测系统，该系统可以在CT扫描图像中将结节分类为恶性、良性和健康患者，具有较高的准确性和敏感性。 |
| [^149] | [DeepSI: Interactive Deep Learning for Semantic Interaction.](http://arxiv.org/abs/2305.18357) | 本文提出了一种基于语义交互的交互式深度学习方法，可以高效地学习用户和任务特定的数据表示，改善视觉分析应用中的语义交互，并通过比较验证了该方法的优势。 |
| [^150] | [Emergent representations in networks trained with the Forward-Forward algorithm.](http://arxiv.org/abs/2305.18353) | 研究表明使用Forward-Forward算法训练的网络内部表征具有高稀疏度，类别特定的集合，这与生物学观察到的皮层表征相似。 |
| [^151] | [Multi-Objective Genetic Algorithm for Multi-View Feature Selection.](http://arxiv.org/abs/2305.18352) | 多视角数据提高了预测模型的准确性，但也使得高维数据增加，影响模型泛化能力。研究者提出了一种多视角多目标特征选择遗传算法（MMFS-GA），用于从多视角数据中选择最优的特征子集以提高模型精度和可解释性。 |
| [^152] | [Towards Open-World Product Attribute Mining: A Lightly-Supervised Approach.](http://arxiv.org/abs/2305.18350) | 该研究提出了一种用于电子商务产品属性挖掘的新任务设置，可以利用高质量的种子属性集合轻度监督并自动发现新的属性类型。通过自我监督启发式和无监督潜在属性，该方法能够以额外的隐含语义信号作为辅助监督，将现有类型的属性扩展最多12倍，并成功发掘了39％的新属性值。 |
| [^153] | [Neural Task Synthesis for Visual Programming.](http://arxiv.org/abs/2305.18342) | 该论文提出了一种基于神经符号技术的可视化编程任务合成方法NeurTaskSyn。该方法能够针对规范中给出的解决方案代码所需要的编程概念和对可视化任务的限制，自动生成编程任务。 |
| [^154] | [Tuning Models of Code with Compiler-Generated Reinforcement Learning Feedback.](http://arxiv.org/abs/2305.18341) | 本文提出了一种叫做RLCF的方法，使用代码编译器反馈进一步训练预训练的大型语言模型，以生成符合目标分布的代码，并通过所有静态正确性检查，显著提高了性能。 |
| [^155] | [Are We There Yet? Product Quantization and its Hardware Acceleration.](http://arxiv.org/abs/2305.18334) | 本文研究了产品量化（PQ）在深度神经网络中替代传统乘加（MAC）运算的效果。作者发现FLOP和参数数量等指标可能具有误导性，并设计了第一个PQ定制硬件加速器评估其性能和效率。 |
| [^156] | [Ranking with Popularity Bias: User Welfare under Self-Amplification Dynamics.](http://arxiv.org/abs/2305.18333) | 研究了物品流行度、质量和位置偏差对用户福利的影响，提出了通过探索减轻流行度偏见负面影响的算法。 |
| [^157] | [Reconfigurable Distributed FPGA Cluster Design for Deep Learning Accelerators.](http://arxiv.org/abs/2305.18332) | 该论文提出了一种基于分布式FPGA集群的深度学习加速器，能够在多个配置中评估和管理神经网络工作负载，实现最佳的延迟和功耗效率。 |
| [^158] | [Training an Ising Machine with Equilibrium Propagation.](http://arxiv.org/abs/2305.18321) | 本研究利用平衡传播算法以监督的方式成功训练了伊辛机器。同时，我们在 MNIST 数据集上取得了与软件实现相当的结果。此外，伊辛机器的连接还支持卷积操作，实现了一个紧凑的卷积网络，每个神经元最少使用自旋数。 |
| [^159] | [Automated Feedback Generation for a Chemistry Database and Abstracting Exercise.](http://arxiv.org/abs/2305.18319) | 本研究利用BERT模型，对化学数据库中摘要练习的答案结构进行反馈，该模型在学生提交的句子中将其归为三类，即背景、技术和观察，提供了一种方法对学生作业进行自动化反馈。 |
| [^160] | [CDJUR-BR -- A Golden Collection of Legal Document from Brazilian Justice with Fine-Grained Named Entities.](http://arxiv.org/abs/2305.18315) | CDJUR-BR是一份稳健的黄金收藏，包含巴西司法文件中的精细命名实体，该收藏涵盖各种法律程序文件，并有助于解决目前命名实体识别（NER）无法轻而易举地识别法律实践文本中实体的问题。 |
| [^161] | [Balancing Test Accuracy and Security in Computerized Adaptive Testing.](http://arxiv.org/abs/2305.18312) | 本文介绍了一种基于双层优化的计算机自适应测试(CAT)框架的约束版本C-BOBCAT，通过权衡测试准确性和问题暴露率及测试重叠率，解决了BOBCAT存在的高问题暴露率和测试重叠率的问题。 |
| [^162] | [Multi-View Interactive Collaborative Filtering.](http://arxiv.org/abs/2305.18306) | 提出了基于多视角交互主题回归算法（MV-ICTR）的推荐系统，在不同视角下同时纳入评分和上下文信息来建模物品特定功能的相关性和用户的个人偏好，采用多臂老虎机策略进行持续的在线个性化，显著提高了数据集上性能。 |
| [^163] | [High Accuracy and Low Regret for User-Cold-Start Using Latent Bandits.](http://arxiv.org/abs/2305.18305) | 使用潜在Bandits算法解决用户冷启动问题，同时实现更高的准确率和更低的遗憾。 |
| [^164] | [Structured model selection via $\ell_1-\ell_2$ optimization.](http://arxiv.org/abs/2305.17467) | 通过稀疏最小二乘拟合一大组候选函数，使用 $\ell_1-\ell_2$ 稀疏优化方法进行结构模型选择，实现从不充分且嘈杂的时空数据中识别结构化动态系统；该方法在合成数据集上得到了验证，并证明具有理论保证和高效性。 |
| [^165] | [No-Regret Online Reinforcement Learning with Adversarial Losses and Transitions.](http://arxiv.org/abs/2305.17380) | 本文提出了一种算法，可以处理对抗性损失和对抗性转换，且后悔逐渐增加与对手的恶意程度成比例。 |
| [^166] | [Kernel-SSL: Kernel KL Divergence for Self-supervised Learning.](http://arxiv.org/abs/2305.17326) | 本文提出了一种名为Kernel-SSL的自监督学习方法，将多种现有非对比学习方法建立在了再生核希尔伯特空间（RKHS）理解之上并优化了其中的均值嵌入和协方差算子，实验结果显示，在ImageNet数据集下表现显著超越最先进的方法，提高了4.6%。 |
| [^167] | [mldr.resampling: Efficient Reference Implementations of Multilabel Resampling Algorithms.](http://arxiv.org/abs/2305.17152) | mldr.resampling是一个软件包，提供11种多标签重采样方法的参考实现，旨在应对多标签学习中的不平衡情况，并具有高效性。 |
| [^168] | [Policy Synthesis and Reinforcement Learning for Discounted LTL.](http://arxiv.org/abs/2305.17115) | 该论文研究使用折扣线性时间逻辑实现强化学习中的政策合成，并探讨如何减少其对状态转移的微小扰动的敏感性。 |
| [^169] | [PlaNeRF: SVD Unsupervised 3D Plane Regularization for NeRF Large-Scale Scene Reconstruction.](http://arxiv.org/abs/2305.16914) | 本文提出了一种利用SVD无监督三维平面正则化的方法，仅使用RGB图像和语义地图即可改善NeRF的三维结构，有效解决了训练视图的过拟合导致低纹理区域的几何重建不佳的问题。 |
| [^170] | [Graph Neural Convection-Diffusion with Heterophily.](http://arxiv.org/abs/2305.16780) | 本论文提出了一种考虑了异质性原则的新型图神经网络，该网络使用对流扩散方程对节点上的信息流进行建模，可以同时考虑基于同质性和异质性的信息传递，在处理异质性图的节点分类任务中具有竞争性的表现。 |
| [^171] | [Leveraging Domain Knowledge for Inclusive and Bias-aware Humanitarian Response Entry Classification.](http://arxiv.org/abs/2305.16756) | 本研究提出了一种以人道主义本体为基础的新型语言模型HumBert，并提供了一种系统的方法来衡量和减少偏见，以实现对人道主义数据分析的有效和道德意识的支持。 |
| [^172] | [Most Neural Networks Are Almost Learnable.](http://arxiv.org/abs/2305.16508) | 本研究提出了一种学习随机常数深度网络的PTAS方法，对于任何固定误差和深度，几乎所有的神经网络都是可学习的。 |
| [^173] | [Differentiable Clustering with Perturbed Spanning Forests.](http://arxiv.org/abs/2305.16358) | 介绍了一种基于扰动生成树的可微聚类方法，依赖于线性规划解的随机扰动，具有良好的性能。 |
| [^174] | [Scaling Data-Constrained Language Models.](http://arxiv.org/abs/2305.16264) | 研究人员研究了在数据受限制的情况下缩放语言模型，并提出了一个计算最优性的缩放定律，考虑到重复令牌和过量参数的价值递减。 |
| [^175] | [The Benefits of Being Distributional: Small-Loss Bounds for Reinforcement Learning.](http://arxiv.org/abs/2305.15703) | 通过小损失边界的视角，我们提供了分布式RL好处的一个解释，该边界与实例相关的最优成本成比例。如果最优成本很小，分布式方法优于非分布式方法。 |
| [^176] | [SPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning.](http://arxiv.org/abs/2305.15486) | SPRING是一个新的方法，能够在开放世界游戏中表现出色，它通过阅读游戏的原始学术论文并使用所学知识进行推理并玩游戏。。 |
| [^177] | [Knowledge-Design: Pushing the Limit of Protein Deign via Knowledge Refinement.](http://arxiv.org/abs/2305.15151) | 本文提出了一种知识感知模块来提炼低质量残基，引入记忆检索机制实现了超过50%的训练时间节省，并取得了较好的性能表现，是蛋白质设计领域的一次创新。 |
| [^178] | [Fourier Neural Operators for Arbitrary Resolution Climate Data Downscaling.](http://arxiv.org/abs/2305.14452) | 提出了一种基于傅里叶神经算子的任意分辨率气候数据降尺度方法，通过小采样训练，并能将其输入零样本降尺度到任意未见高分辨率，显著优于现有的降尺度模型。 |
| [^179] | [Video Prediction Models as Rewards for Reinforcement Learning.](http://arxiv.org/abs/2305.14343) | 本文提出了VIPER算法，利用预训练的视频预测模型作为强化学习的奖励信号来学习复杂行为，从而实现在广泛任务范围内的专家级控制，同时具有泛化性。 |
| [^180] | [Towards Understanding the Dynamics of Gaussian--Stein Variational Gradient Descent.](http://arxiv.org/abs/2305.14076) | 本文探究了高斯-斯坦变分梯度下降动态性。对于从高斯目标中采样，只要初始值是高斯的，具有双线性核的SVGD动态将保持高斯状态。当目标函数呈现出强对数凹性时，证明了均场高斯-SVGD动态会线性收敛于KL散度下最接近目标高斯分布。在有限粒子设置中，存在对均场极限的时间微步一致收敛以及线性收敛至目标高斯分布。 |
| [^181] | [Debiased Automatic Speech Recognition for Dysarthric Speech via Sample Reweighting with Sample Affinity Test.](http://arxiv.org/abs/2305.13108) | 本文提出了一种样本重新加权与样本关联测试（Re-SAT）的新方法，用于缓解失语症患者的偏差问题，在不影响健康患者语音的ASR性能的情况下，有效提高了ASR的性能表现。 |
| [^182] | [Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models.](http://arxiv.org/abs/2305.12827) | 本文研究了在切线空间中进行任务算术的方法，发现权重分离是其有效的关键因素。我们提出了一种简单而有效的基于切线空间中的任务算术技术Tan，其优于现有的最先进方法。 |
| [^183] | [Diversifying Deep Ensembles: A Saliency Map Approach for Enhanced OOD Detection, Calibration, and Accuracy.](http://arxiv.org/abs/2305.11616) | 这项研究提出了一种使用显著性图来促进深度集成多样性的方法，用于改善OOD检测、校准和准确性，能够优于传统的集成技术，并在OpenOOD基准测试上证明了其有效性。 |
| [^184] | [Few-Shot Continual Learning for Conditional Generative Adversarial Networks.](http://arxiv.org/abs/2305.11400) | 本文提出了一种新的连续学习方法，适用于条件生成对抗网络，根据cGAN的判别器数据识别出最接近目标的现有模式，并通过扩展连续学习模型，使用回放生成的数据来训练目标模式的cGAN模型，以避免灾难性遗忘，提高了生成性能。 |
| [^185] | [Catch-Up Distillation: You Only Need to Train Once for Accelerating Sampling.](http://arxiv.org/abs/2305.10769) | 本文提出了一种名为“追赶蒸馏”的方法，通过调整传统采样算法，让速度估计模型的当前时刻输出与其先前时刻输出和地面真实标签对齐，从而实现只需一次训练便能加速采样的效果。 |
| [^186] | [RelationMatch: Matching In-batch Relationships for Semi-supervised Learning.](http://arxiv.org/abs/2305.10397) | RelationMatch是一种利用矩阵交叉熵（MCE）损失函数的方法，可以匹配批内关系，有效提高半监督学习和监督学习的性能。 |
| [^187] | ["I'm fully who I am": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation.](http://arxiv.org/abs/2305.09941) | 本论文研究了如何以TGNB人群的声音为中心，评估开放式语言生成中的偏见。通过理解TGNB个体的经历，提出了以TGNB人群为中心的OLG系统评估框架，并且包括一个为TGNB人群设计的调查工具和分析方法。 |
| [^188] | [Analysis of Visual Question Answering Algorithms with attention model.](http://arxiv.org/abs/2305.09782) | 本文批评性地检查和审查了使用共同注意力方法的VQA算法的方法，重点关注文本语义生成、对象识别和答案分类技术。 |
| [^189] | [Document Understanding Dataset and Evaluation (DUDE).](http://arxiv.org/abs/2305.08455) | DUDE推出了一个新的数据集和评估方法，旨在创造一个更实际的基准测试并推动当前方法的边界，以更准确地模拟真实世界的情况 |
| [^190] | [Consistent Text Categorization using Data Augmentation in e-Commerce.](http://arxiv.org/abs/2305.05402) | 本文提出了一种在电子商务中使用数据增强实现一致的文本分类的新框架，该框架旨在改进产品分类模型的一致性，同时保持其生产水平的性能。 |
| [^191] | [GNNs,You can be Stronger,Deeper and Faster.](http://arxiv.org/abs/2305.05368) | 本文介绍了一种新的理解GNN表现能力的视角，并提出了一个新的采样节点级残差模块SDF，具有更好的表现能力。 |
| [^192] | [New metrics and search algorithms for weighted causal DAGs.](http://arxiv.org/abs/2305.04445) | 本研究提供了针对加权因果 DAGs的新度量和搜索算法，发现了用于自适应干预的因果图，提供了一个新的基准来捕捉搜索算法的最坏干预成本，并提供自适应搜索算法实现对数逼近。 |
| [^193] | [Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling.](http://arxiv.org/abs/2305.04111) | 本文提出了EDGE，一种新的离散扩散模型用于生成大型图，并通过删除边来鼓励图的稀疏性。EDGE在每个去噪步骤中仅关注图中一部分节点，并且可以明确地对图的节点度数进行建模。实验表明，EDGE比竞争方法更有效，并且可以生成具有数千个节点的大型图。 |
| [^194] | [Semi-Asynchronous Federated Edge Learning Mechanism via Over-the-air Computation.](http://arxiv.org/abs/2305.04066) | 本文提出了一种半异步聚合FEEL机制PAOTA，以改善数据和设备存在显著异质性的情况下FEEL系统的训练效率，通过调整边缘设备的上行传输功率来最小化FEEL全局模型的收敛上界。实验结果表明，所提出的机制在达到相同的目标精度下，训练速度显著快于具有空中计算方案的传统同步FEEL机制。 |
| [^195] | [Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation.](http://arxiv.org/abs/2305.00909) | 提出一种基于语法引导的粗-细代码生成模型，支持从粗到细的多次迭代，实现了更加符合人脑思维方式的代码编写方式。 |
| [^196] | [AI-based Radio and Computing Resource Allocation and Path Planning in NOMA NTNs: AoI Minimization under CSI Uncertainty.](http://arxiv.org/abs/2305.00780) | 本论文提出一种基于高空平台和无人机构成的分层式空中计算框架，旨在通过调整航迹和资源分配来最小化用户的信息年龄，并受到多重资源约束和信道状态不确定性的限制。结果表明，所提出的方法可以显著减少信息年龄。 |
| [^197] | [How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model.](http://arxiv.org/abs/2305.00586) | 本研究运用机械式可解释性技术探究了GPT-2 Small的数学能力，并确定了它的计算图中的一个小电路用于计算大于符号，该电路的多层感知器提高了结束年份大于开始年份的概率，并且该电路具有广泛的适用性。 |
| [^198] | [Object-Centric Voxelization of Dynamic Scenes via Inverse Neural Rendering.](http://arxiv.org/abs/2305.00393) | 本文提出了一个逆向神经渲染框架DynaVol，可以在多实体动态场景中学习时间变化的体积表示，通过维护一个时间依赖的3D格点和联合学习格点级局部动态、物体级全局动态和组合神经辐射场来增强物体中心场景体素化的时空一致性。 |
| [^199] | [Analogy-Forming Transformers for Few-Shot 3D Parsing.](http://arxiv.org/abs/2304.14382) | "模拟网络"模型在3D物体场景分割中采用类比推理，通过在内存中检索相关场景并预测类似结构进行分割，能够在一发、少发或多发学习中得出相似的解析，与最新的3D分割变压器模型相竞争。 |
| [^200] | [Contrastive Energy Prediction for Exact Energy-Guided Diffusion Sampling in Offline Reinforcement Learning.](http://arxiv.org/abs/2304.12824) | 本文提出了一种基于对比能量预测的确切能量引导扩散采样方法，用于离线强化学习中，可以解决中间引导估计的难题，并在D4RL基准测试上取得了优异的效果。 |
| [^201] | [CF-VAE: Causal Disentangled Representation Learning with VAE and Causal Flows.](http://arxiv.org/abs/2304.09010) | 本文提出了一种新的因果流以进行因果分离表示学习，设计了一个新模型CF-VAE，利用因果流增强了VAE编码器的分离能力，并展示了在合成和真实数据集上实现因果分离并进行干预实验的结果。 |
| [^202] | [Wasserstein PAC-Bayes Learning: A Bridge Between Generalisation and Optimisation.](http://arxiv.org/abs/2304.07048) | 本文介绍了扩展的 Wasserstein PAC-Bayes 框架，利用损失函数上的几何假设提供了新的泛化界限。通过该框架，证明了 \cite{lambert2022variational} 中算法的输出具有强大的泛化能力。同时，建立了 PAC-Bayes 和优化算法之间的桥梁。 |
| [^203] | [Efficient Sequence Transduction by Jointly Predicting Tokens and Durations.](http://arxiv.org/abs/2304.06795) | 本文提出了一种新型的序列转导架构TDT，它可以联合预测标记和持续时间，从而实现比传统Transducers更高的准确性和显着更快的推理速度。 |
| [^204] | [Did we personalize? Assessing personalization by an online reinforcement learning algorithm using resampling.](http://arxiv.org/abs/2304.05365) | 本论文提出了一种使用重复采样的政策评估方法，以评估在线 RL 算法实现的个性化程度。该方法可用于优化数字健康的个性化干预。 |
| [^205] | [On the Optimal Recovery of Graph Signals.](http://arxiv.org/abs/2304.00474) | 本文提出了一种计算图信号最优或接近最优正则化参数的方法，为图信号处理问题提供了新的解释和超参数选择的新见解。 |
| [^206] | [Contrastive Learning Is Spectral Clustering On Similarity Graph.](http://arxiv.org/abs/2303.15103) | 本文通过证明标准InfoNCE损失下的对比学习等同于相似性图上的谱聚类，揭示了这种方法的内在等价性，并进一步将这种分析扩展到CLIP模型，提出新的核混合损失函数。 |
| [^207] | [Geometry-Aware Latent Representation Learning for Modeling Disease Progression of Barrett's Esophagus.](http://arxiv.org/abs/2303.12711) | 本文提出了一种基于几何思想的潜在表示学习方法，用于建模Barrett食管疾病进程，与传统方法相比，具有更好的重建损失。 |
| [^208] | [Universal Approximation Property of Hamiltonian Deep Neural Networks.](http://arxiv.org/abs/2303.12147) | 本文研究了离散化的哈密顿神经常微分方程引起的Hamiltonian深度神经网络的通用逼近能力，证明了其中的一部分流可以逐渐逼近紧致域上的任何连续函数，为实际使用提供了理论基础。 |
| [^209] | [LNO: Laplace Neural Operator for Solving Differential Equations.](http://arxiv.org/abs/2303.10528) | LNO是一种用于解微分方程的算法，相比其他算法（如FNO）具有更好的逼近精度并适用于非周期性信号和瞬态响应，能够更好地解释模型并改进泛化能力。 |
| [^210] | [CB2: Collaborative Natural Language Interaction Research Platform.](http://arxiv.org/abs/2303.08127) | CB2是一个用于研究基于任务的合作自然语言交互的平台，在3D游戏环境中提供了后端服务器和各种工具和流程。它在可扩展的研究中展示了学习的指令跟随模型。 |
| [^211] | [Transformer-based Planning for Symbolic Regression.](http://arxiv.org/abs/2303.06833) | 该论文提出了一种基于Transformer和蒙特卡罗树搜索的符号回归规划策略TPSR，可以将非可微的反馈作为知识的外部来源融入到方程生成过程中，提高方程生成的准确性和复杂性。 |
| [^212] | [One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale.](http://arxiv.org/abs/2303.06555) | 本论文提出了UniDiffuser框架，采用一个Transformer模型来统一处理多模态数据的分布拟合问题，覆盖了边缘、条件和联合分布，并且能够在图文配对数据上实现多种生成任务。 |
| [^213] | [Reducing the Prior Mismatch of Stochastic Differential Equations for Diffusion-based Speech Enhancement.](http://arxiv.org/abs/2302.14748) | 研究提出了一种基于布朗桥的前向过程，以减少扩散型语音增强中由重建过程和先验之间的不匹配问题引起的误差，并提出的方法只需一半的迭代次数和一个可调的超参数即可改进处理过程。 |
| [^214] | [Data-Centric AI: Deep Generative Differentiable Feature Selection via Discrete Subsetting as Continuous Embedding Space Optimization.](http://arxiv.org/abs/2302.13221) | 该论文提出一种将离散特征子集作为连续嵌入空间优化的深度生成可微分特征选择方法，解决了在高维小样本数据集中通用、准确和维度无关的特征选择问题。 |
| [^215] | [Why Target Networks Stabilise Temporal Difference Methods.](http://arxiv.org/abs/2302.12537) | 本文解释了深度强化学习中一种流行的时序差分方法中关键的稳定性问题：为什么目标网络能够有效降低不满足条件时的影响。 |
| [^216] | [Some Fundamental Aspects about Lipschitz Continuity of Neural Network Functions.](http://arxiv.org/abs/2302.10886) | 本文深入研究和描述神经网络实现的函数的Lipschitz行为，在多种设置下进行实证研究，并揭示了神经网络函数Lipschitz连续性的基本和有趣的特性，其中最引人注目的是在Lipschitz常数的上限和下限中识别出了明显的双下降趋势。 |
| [^217] | [Multi-View Clustering from the Perspective of Mutual Information.](http://arxiv.org/abs/2302.08743) | 本文提出了一种信息性多视角聚类（IMVC）方法，通过约束共同表示和视图特定表示之间的互信息最小，提高多视角数据聚类效果。 |
| [^218] | [Zero-Shot Batch-Level Anomaly Detection.](http://arxiv.org/abs/2302.07849) | 本文提出了一种名为“自适应中心表示”的方法，用于零样本批次级异常检测。该方法利用批量归一化来训练现成的深度异常检测器，可以自动零样本泛化为未见过的AD任务。在实验中，该方法显示出了在多种数据集上的优秀表现，对表格数据进行了零样本AD。 |
| [^219] | [Multi-task Representation Learning for Pure Exploration in Linear Bandits.](http://arxiv.org/abs/2302.04441) | 本文研究了纯探索设置下的多任务表示学习，通过学习所有任务之间的公共低维线性表示，设计了计算和样本高效的算法来加速最佳臂或策略识别过程，在合成和现实数据集上实验结果证明了算法的有效性。 |
| [^220] | [Feature Likelihood Score: Evaluating Generalization of Generative Models Using Samples.](http://arxiv.org/abs/2302.04440) | 本文提出了一种新的特征似然分数（FLS）评估深度生成模型泛化能力的度量指标，用于评估生成样本的新颖性、保真度和多样性，通过实验证明其优于现有的指标，并能够检测出过拟合问题。 |
| [^221] | [Information Theoretical Importance Sampling Clustering.](http://arxiv.org/abs/2302.04421) | 本文提出了一种信息理论重要性采样聚类方法，该方法最小化在分布偏差约束下期望失真的最坏情况。 |
| [^222] | [IB-UQ: Information bottleneck based uncertainty quantification for neural function regression and neural operator learning.](http://arxiv.org/abs/2302.03271) | 我们提出了基于信息瓶颈的不确定性量化框架IB-UQ，用于深度学习任务，在回归和操作器学习中提供了一种更有效和可扩展的量化预测不确定性的方法。 |
| [^223] | [V1T: large-scale mouse V1 response prediction using a Vision Transformer.](http://arxiv.org/abs/2302.03023) | V1T是一种基于Vision Transformer的新型架构，可以跨动物学习共享的视觉和行为表示，对自然视觉刺激下的视觉皮层神经响应进行预测，并在预测性能上优于之前基于卷积的模型超过12.7％。同时，通过Transformer学习的自我关注权重还能够展示与群体感受野的相关性。 |
| [^224] | [Multi-Source Diffusion Models for Simultaneous Music Generation and Separation.](http://arxiv.org/abs/2302.02257) | 本研究提出了一种基于扩散模型的生成模型，能够同时进行音乐合成和源分离，并于部分生成和分离任务上实现有竞争力的定量结果。 |
| [^225] | [Sharp Spectral Rates for Koopman Operator Learning.](http://arxiv.org/abs/2302.02004) | 本文提出了Koopman算子的非渐进学习界限，重点研究了时间可逆随机动力系统，提出了扩展动态模分解（EDMD）和降低秩回归（RRR）两种流行的估计器，并比较了它们的方差。 |
| [^226] | [Are Diffusion Models Vulnerable to Membership Inference Attacks?.](http://arxiv.org/abs/2302.01316) | 本文研究了扩散模型是否易受成员隐私攻击，提出了一种基于查询的成员隐私攻击方法。结果表明现有的成员隐私攻击对扩散模型基本无效。 |
| [^227] | [Convolutional Neural Operators for robust and accurate learning of PDEs.](http://arxiv.org/abs/2302.01178) | 该论文提出了卷积神经算子（CNO），它能够在学习偏微分方程的上下文中处理函数输入和输出，并且证明了 CNOs 可以近似偏微分方程中出现的算子到所需的精度。在测试中，CNOs 显着优于基线，这为鲁棒准确操作学习的另一种框架铺平了道路。 |
| [^228] | [Speed-Oblivious Online Scheduling: Knowing (Precise) Speeds is not Necessary.](http://arxiv.org/abs/2302.00985) | 本论文提出了一种速度遗忘算法，用于在异构机器上进行在线调度，算法不需要准确的作业处理速度。作者提供了理论保证和实证评估。 |
| [^229] | [CD-GraB: Coordinating Distributed Example Orders for Provably Accelerated Training.](http://arxiv.org/abs/2302.00845) | 该论文提出了一种名为CD-GraB的算法，可以协调分布式示例顺序以加速机器学习训练。CD-GraB展现出线性加速收敛速率并且在基准任务上优于其他基线方法。 |
| [^230] | [End-to-End Full-Atom Antibody Design.](http://arxiv.org/abs/2302.00203) | dyMEAN是一个端到端全原子模型，可以根据表位和不完整的抗体序列进行抗体设计，在处理全原子时能够处理可变大小的蛋白残基。旨在解决现有学习方法的两个主要问题：只处理抗体设计过程中的某个子任务和无法捕捉全原子几何形状。 |
| [^231] | [Dissecting the Effects of SGD Noise in Distinct Regimes of Deep Learning.](http://arxiv.org/abs/2301.13703) | 本文通过改变训练集大小P和初始化规模a的方法来控制SGD噪声大小T对于MNIST和CIFAR10图像分类的影响，得到SGD噪声可能有害或有益的相图，同时发现特征温度Tc存在相似的尺度关系，在训练集为1000～10000之间时达到最佳水平。 |
| [^232] | [PAC-Bayesian Soft Actor-Critic Learning.](http://arxiv.org/abs/2301.12776) | 本文提出了一种使用PAC-Bayesian bound作为Soft Actor-Critic (SAC)算法评论家训练目标的方法，以解决训练不稳定的问题，并通过评论家引导的随机搜索探索多个未来来提高在线学习性能。在多个经典控制和运动任务中，该算法具有样本效率和遗憾最小化方面的明显优势。 |
| [^233] | [Deep Operator Learning Lessens the Curse of Dimensionality for PDEs.](http://arxiv.org/abs/2301.12227) | 本文指出，利用DNN对Banach空间上的Lipschitz算子进行深度算子学习可以减轻PDE中的维数灾难，包括椭圆方程、抛物线方程和Burgers方程，并可用于提供算子学习中的离散化不变性的见解。 |
| [^234] | [ZegOT: Zero-shot Segmentation Through Optimal Transport of Text Prompts.](http://arxiv.org/abs/2301.12171) | 这篇论文提出了一种通过最优传输的方法，利用多个文本提示来实现零样本分割，达到了最先进的性能水平。 |
| [^235] | [Coin Sampling: Gradient-Based Bayesian Inference without Learning Rates.](http://arxiv.org/abs/2301.11294) | 本文提出了一种基于硬币投注的贝叶斯推断方法，完全不需要学习速率，可以在高维模型和数据集上表现出与其他方法相当的性能。 |
| [^236] | [On the Global Convergence of Risk-Averse Policy Gradient Methods with Expected Conditional Risk Measures.](http://arxiv.org/abs/2301.10932) | 本论文研究了具有期望条件风险度量的风险厌恶策略梯度方法，提出了策略梯度更新，证明了其在约束和无约束情况下的全局收敛性和迭代复杂度，并测试了REINFORCE和actor-critic算法的风险厌恶变体来展示方法的实用价值和风险控制的重要性。 |
| [^237] | [Tighter Bounds on the Expressivity of Transformer Encoders.](http://arxiv.org/abs/2301.10743) | 本文旨在更紧密地界定Transformer编码器的表达能力，提供了一个同时是下限和上限的一阶逻辑变体的策略，使我们更加接近准确刻画Transformer编码器可识别语言的目标。 |
| [^238] | [Leveraging generative adversarial networks to create realistic scanning transmission electron microscopy images.](http://arxiv.org/abs/2301.07743) | 本文提出一种循环生成对抗网络 (CycleGAN) 的方法，利用真实的空间频率信息来扩充模拟数据，生成几乎无法与真实数据区分的电子显微镜图像，并为机器学习应用提供标签，这展示了生成对抗网络在革命性转变材料研究方面具有潜力。 |
| [^239] | [Learning Deformation Trajectories of Boltzmann Densities.](http://arxiv.org/abs/2301.07388) | 本文介绍了一种学习Boltzmann密度变形轨迹的方法，其中通过插值能量函数等实现Boltzmann密度的变形，然后找到一个时间依赖向量场，将样本从一个分布转移到另一个分布，其表现在高斯混合和量子力学粒子的Boltzmann密度上比KL-反散度更具优势。 |
| [^240] | [Mind the Gap: Modelling Difference Between Censored and Uncensored Electric Vehicle Charging Demand.](http://arxiv.org/abs/2301.06418) | 电动汽车充电需求模型的天然偏见导致了观测到的需求与实际需求有差异，使用有关审查的模型来模拟充电需求可以更好地估计真实需求。 |
| [^241] | [Hair and Scalp Disease Detection using Machine Learning and Image Processing.](http://arxiv.org/abs/2301.00122) | 本研究使用深度学习方法成功预测了三种主要的脱发和头皮相关疾病：斑秃、银屑病和毛囊炎。 |
| [^242] | [Continual Contrastive Finetuning Improves Low-Resource Relation Extraction.](http://arxiv.org/abs/2212.10823) | 本文提出了一种使用连续对比微调的方法来改进低资源关系提取，通过使用一致的对比学习目标预训练和微调RE模型，以及多中心对比损失来允许一个关系形成多个聚类。实验结果表明该方法可以显着提高低资源情况和领域中的关系提取性能。 |
| [^243] | [When Not to Trust Language Models: Investigating Effectiveness and Limitations of Parametric and Non-Parametric Memories.](http://arxiv.org/abs/2212.10511) | 本文通过对10个模型和4种增强方法的实验，发现语言模型在记忆不太流行的实际知识方面存在困难，而检索增强的语言模型表现较好，提出了一种检索增强语言模型的简单有效方法。 |
| [^244] | [Sequential Predictive Conformal Inference for Time Series.](http://arxiv.org/abs/2212.03463) | 提出了一种新的适用于时序数据的自适应重新估计条件分位数的置信预测算法SPCI，相较于其他现有方法，SPCI在所需经验覆盖下的区间宽度显著减小。 |
| [^245] | [Dynamical Linear Bandits.](http://arxiv.org/abs/2211.08997) | 本文提出了动态线性臂机（DLB）这一概念，这是线性臂机的扩展，具有隐藏状态。这一方法可用于解决在实际决策中行动不会立即反映在反馈上，并在较长的时间范围内扩散其影响所引起的问题。 |
| [^246] | [FedGen: Generalizable Federated Learning for Sequential Data.](http://arxiv.org/abs/2211.01914) | FedGen为联邦学习带来可推广性，允许分布式设备共同识别和区分伪特征和不变特征，而不需要训练分布的先前知识。 |
| [^247] | [Learning Control by Iterative Inversion.](http://arxiv.org/abs/2211.01724) | 迭代反演是一种算法，能够学习无输入输出对的反函数。该算法通过样本学习期望输出分布，可以解决所期望输出与初始随机猜测输出分布之间的偏移问题。此方法被成功应用于学习控制问题，并获得了良好的表现。 |
| [^248] | [Data Level Lottery Ticket Hypothesis for Vision Transformers.](http://arxiv.org/abs/2211.01484) | 本文将传统的彩票假说（LTH）扩展到由图像补丁组成的输入数据中，证明存在一个子集的输入图像补丁使得可以从头开始训练视觉Transformer（ViT），并且达到与使用所有图像补丁训练的ViTs相似的精度，这种方法在各种视觉任务中都是可行的和有效的。 |
| [^249] | [Backtracking Counterfactuals.](http://arxiv.org/abs/2211.00472) | 本文介绍了一种基于回溯式反事实推理的形式化方法，该方法基于图形模型，提供了一种更加自然和直观的推理过去时反事实情景的方法。 |
| [^250] | [Consistent and Truthful Interpretation with Fourier Analysis.](http://arxiv.org/abs/2210.17426) | 该论文提出了一个称为真实解释的新概念，通过傅立叶分析获得严格保证，并在实验中证明了其在支持假设情景和降低解释误差方面的优势。 |
| [^251] | [Training Neural Networks for Sequential Change-point Detection.](http://arxiv.org/abs/2210.17312) | 本文介绍了一种使用神经网络进行在线变点检测的方法，通过训练神经网络逐步计算检测统计量的累积和来检测变点，并在合成和真实数据上证明了该方法的优越性和潜力。 |
| [^252] | [Men Also Do Laundry: Multi-Attribute Bias Amplification.](http://arxiv.org/abs/2210.11924) | 本文研究了计算机视觉中的多属性偏见放大现象，发现现有的度量可能会忽略多个属性之间的相关性，并可能错误地给出度量结果，使得最小或没有偏见放大的情况被误判。 |
| [^253] | [Call Graph Evolution Analytics over a Version Series of an Evolving Software System.](http://arxiv.org/abs/2210.08316) | 该论文提出了一种Call Graph Evolution Analytics的方法，用于从软件系统版本序列的演化调用图中提取信息，通过分析演化调用图的依赖关系模式的演化，帮助软件工程师进行版本演化管理。 |
| [^254] | [Dual control variate for faster black-box variational inference.](http://arxiv.org/abs/2210.07290) | 本论文提出了双控制变量方法，能够同时减少数据子抽样和蒙特卡罗抽样带来的梯度估计方差，提高黑盒变分推断的准确性和效率。 |
| [^255] | [Neural Importance Sampling for Rapid and Reliable Gravitational-Wave Inference.](http://arxiv.org/abs/2210.05686) | 本文提出了一种基于神经网络和重要性采样的方法，用于快速且准确地进行引力波推断。该方法不仅可以得到不受网络不准确性影响的校正后验，还可以评估建议和识别失败情况的性能诊断以及得到贝叶斯证据的无偏估计。 |
| [^256] | [ParaDime: A Framework for Parametric Dimensionality Reduction.](http://arxiv.org/abs/2210.04582) | ParaDime 是一个参数化降维框架，通过提供统一的接口，允许用户自定义数据之间的关系和转换，从而将多个现代 DR 技术统一起来。借助 ParaDime，用户可以方便地实验各种 DR 技术，如混合分类 / 嵌入模型和监督 DR，为高维数据的可视化打开新的可能性。 |
| [^257] | [InfoOT: Information Maximizing Optimal Transport.](http://arxiv.org/abs/2210.03164) | 提出了InfoOT，它是一种信息论扩展的最优输运方法，能够解决最优输运忽略了数据中相干结构的问题，同时能够处理离群值和集成新数据点，可以提高域自适应、跨域检索和单细胞对齐等任务的对齐质量。 |
| [^258] | [Learning Robust Kernel Ensembles with Kernel Average Pooling.](http://arxiv.org/abs/2210.00062) | 本文提出了核均值池（KAP），一种神经网络构建模块，它可以在卷积神经网络中自然地产生具有相似功能的核集合，提高模型对输入扰动的鲁棒性，并在多个数据集上的实验证明了其有效性。 |
| [^259] | [Training Normalizing Flows from Dependent Data.](http://arxiv.org/abs/2209.14933) | 该论文提出了一种考虑数据点依赖关系的归一化流似然目标和学习算法，在合成和真实数据上实现更好的经验结果和更高的统计功效。 |
| [^260] | [Proximal Point Imitation Learning.](http://arxiv.org/abs/2209.10968) | 本文提出了一种适用于无限时域模仿学习的新算法，采用了优化中的经典工具近端点法和对偶平滑，从而得到了比之前更好的效率保证。通过优化一个单个的凸平滑目标来同时更新成本和Q函数，避免了嵌套策略评估和成本更新，具有更好的应用前景。 |
| [^261] | [Does CLIP Know My Face?.](http://arxiv.org/abs/2209.07341) | 本文提出了一种新方法IDIA来评估视觉语言模型的隐私，大规模实验表明使用于训练的个人可以被非常高的准确率识别出来，表明需要更好地解决视觉语言模型中的隐私问题。 |
| [^262] | [PD-MORL: Preference-Driven Multi-Objective Reinforcement Learning Algorithm.](http://arxiv.org/abs/2208.07914) | PD-MORL是一种基于偏好的多目标强化学习算法，其采用偏好作为指导，适应于各种偏好空间的目标，可扩展到连续的机器人任务，并在多目标机器人控制任务上优于现有的MORL方法。 |
| [^263] | [Privacy-Preserving Decentralized Inference with Graph Neural Networks in Wireless Networks.](http://arxiv.org/abs/2208.06963) | 本文研究了在无线网络中采用图神经网络进行分散式推理所可能导致的隐私泄露问题，并提出使用本地差分隐私度量和新设计的隐私保护信号进行保护的方法。同时，为了增强透明度，通过模拟展示了隐私保护通信和推理的过程。 |
| [^264] | [RLang: A Declarative Language for Describing Partial World Knowledge to Reinforcement Learning Agents.](http://arxiv.org/abs/2208.06448) | RLang是一种声明性语言，可以为强化学习智能体提供部分世界模型的信息，包括无模型和有模型表格算法、策略梯度和基于价值的方法、分层方法和深度方法。 |
| [^265] | [MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures.](http://arxiv.org/abs/2208.00277) | 这篇论文提出了一种在移动设备上高效呈现神经现场的方法，通过使用纹理多边形而不是基于射线行进的体积渲染算法，并且利用传统渲染管线中的 z-缓冲器，使得 NeRF 可以通过像素级并行性来实现在移动设备上交互式帧速率。 |
| [^266] | [A Data-driven Latent Semantic Analysis for Automatic Text Summarization using LDA Topic Modelling.](http://arxiv.org/abs/2207.14687) | 本研究使用LDA主题建模方法进行文本摘要，针对与基因和疾病相关的医学科学期刊文章进行研究，提供了一个能够保留关键信息并保持原始意义的压缩版本，并使用PyLDAvis进行交互式可视化。 |
| [^267] | [Remote Medication Status Prediction for Individuals with Parkinson's Disease using Time-series Data from Smartphones.](http://arxiv.org/abs/2207.13700) | 本研究提出了一种使用智能手机时间序列数据预测帕金森病患者药物治疗状态的方法，通过考察病人个体的历史记录、使用 Transformer 模型学习注意权重，实现了较好的客观预测结果，为个性化远程健康传感提供了一种创新方法。 |
| [^268] | [When Fairness Meets Privacy: Fair Classification with Semi-Private Sensitive Attributes.](http://arxiv.org/abs/2207.08336) | 该论文研究在半隐私场景下如何通过本地差分隐私（LDP）实现公平分类，解决了在收集大规模用户敏感属性时的难题。 |
| [^269] | [Auditing Visualizations: Transparency Methods Struggle to Detect Anomalous Behavior.](http://arxiv.org/abs/2206.13498) | 本研究评估了可视化方法检测模型异常行为的能力，发现现有方法难以识别微妙的异常行为，并且无法识别导致异常行为的输入。因此，需要开发更可靠的模型透明度方法。 |
| [^270] | [Superiority of GNN over NN in generalizing bandlimited functions.](http://arxiv.org/abs/2206.05904) | 本文研究了GNN在节点分类中插值带限函数的表达能力，结果表明，使用GNN结构以相同的精度插值带限函数所需的权重比使用完全连接的神经网络（NN）少得多。 |
| [^271] | [When Does Adaptivity Help for Quantum State Learning?.](http://arxiv.org/abs/2206.05265) | 本文研究了量子态学习中的适应性问题，显示出任何使用非相干测量的协议，即使是自适应选择的，都需要Ω(d3/ε2)个复制品的量。 |
| [^272] | [Computational Doob's h-transforms for Online Filtering of Discretely Observed Diffusions.](http://arxiv.org/abs/2206.03369) | 本文提出了一种计算Doob h变换的计算框架，用于离散观测非线性扩散过程的在线滤波。实验证明，该方法在高度信息化、观测值在模型下极端或状态维数较大时比最先进的粒子滤波器高几个数量级的效率。 |
| [^273] | [Learning Instance-Specific Augmentations by Capturing Local Invariances.](http://arxiv.org/abs/2206.00051) | InstaAug是一种能够自动学习输入特定增强的方法，通过引入可学习的不变性模块从输入中映射到量身定制的变换参数，捕捉局部不变性，从而提高了监督和自我监督任务的性能。 |
| [^274] | [BrainIB: Interpretable Brain Network-based Psychiatric Diagnosis with Graph Information Bottleneck.](http://arxiv.org/abs/2205.03612) | BrainIB是一种基于图信息瓶颈原理开发的图神经网络框架，在分析fMRI图像中的功能连接时能够识别最具信息量的边缘，具有良好的泛化能力和可解释性，适用于未见样本的场景。 |
| [^275] | [Multi-armed bandits for resource efficient, online optimization of language model pre-training: the use case of dynamic masking.](http://arxiv.org/abs/2203.13151) | 本文提出了一种多臂老虎机框架，用于顺序选择TLM预训练超参数，旨在以资源高效的方式优化语言模型性能。并设计了基于高斯过程的Thompson抽样（GP-TS）算法，加速Pre-training过程并降低MLM损失。 |
| [^276] | [Bilevel Optimization with a Lower-level Contraction: Optimal Sample Complexity without Warm-Start.](http://arxiv.org/abs/2202.03397) | 本文针对一类双层问题，提出了无需warm-start也可实现最优样本复杂度的方法。 |
| [^277] | [Efficient Direct-Connect Topologies for Collective Communications.](http://arxiv.org/abs/2202.03356) | 本论文提出了一种算法框架，可以构建高效的直连网络拓扑结构，以优化节点延迟和带宽权衡，适用于集体通信负载。 |
| [^278] | [Mixed Membership Distribution-Free Model.](http://arxiv.org/abs/2112.04389) | 本文提出了一种混合成员无分布模型，用于重叠加权网络的社群检测，支持节点所属多个社群和有限实数权值。提出的模型可以推广到之前的模型，包括混合成员随机块模型，并支持具有潜在社群结构的重叠符号网络的生成。我们使用高效谱算法估计模型下的社群成员资格，并提出了模糊加权模块度来评估重叠加权网络的社群检测质量并确定加权网络社群数量。 |
| [^279] | [Group equivariant neural posterior estimation.](http://arxiv.org/abs/2111.13139) | 本文提出了一种群等变神经后验估计（GNPE）算法，能够在参数和数据联合变换下整合等变性，用于从引力波观测中对双黑洞系统进行摊销推断。 |
| [^280] | [Which Design Decisions in AI-enabled Mobile Applications Contribute to Greener AI?.](http://arxiv.org/abs/2109.15284) | 在移动设备上部署复杂 AI 模型需要平衡准确性和复杂性，设计决策对实现高准确度和低资源消耗有影响。 |
| [^281] | [The Fragility of Optimized Bandit Algorithms.](http://arxiv.org/abs/2109.13595) | 本文表明，使用优化设计的赌博算法遗憾分布具有非常重的尾部，对于$p>1$，遗憾分布的$p$'th矩增长要比多对数级别快得多，当问题略微错误时，优化UCB赌博设计的遗憾可以比传统理论建议的增长得更快。 |
| [^282] | [Community detection for weighted bipartite networks.](http://arxiv.org/abs/2109.10319) | 本文提出了一种名为Bipartite Distribution-Free的模型，可用于建模和探测加权二分网络的社区结构，该模型考虑了节点度数的变化以及期望的块结构。同时，我们提出了谱算法用于识别社区。 |
| [^283] | [Which Invariance Should We Transfer? A Causal Minimax Learning Approach.](http://arxiv.org/abs/2107.01876) | 该论文从因果的角度提出了一种全面的极小化分析，旨在回答机器学习模型在转移稳定信息时应该转移哪个子集从而达到最佳的泛化能力这一问题 |
| [^284] | [Real-time gravitational-wave science with neural posterior estimation.](http://arxiv.org/abs/2106.12594) | 使用神经网络作为贝叶斯后验分布的替代品，提出的算法 "DINGO" 能够实现对检测到的引力波事件物理参数的快速准确推断，可在不损失精度的情况下实现实时数据分析。 |
| [^285] | [Private Federated Learning Without a Trusted Server: Optimal Algorithms for Convex Losses.](http://arxiv.org/abs/2106.09779) | 本文研究了无需信任服务器或其他数据源的跨 silo 联邦学习，考虑了跨 silo 记录级差分隐私 ISRL-DP。该算法可以确保来自每个人的数据都不会被泄漏。 |
| [^286] | [On the Tightness of the Moment Accountant for DP-SGD.](http://arxiv.org/abs/2102.09030) | 通过改进Moment Accountant方法，DP-SGD具有可关闭形式的$(\epsilon，\delta)$-DP保证，并且其保证接近是紧密的，具有最小的计算成本。 |
| [^287] | [How Powerful are Shallow Neural Networks with Bandlimited Random Weights?.](http://arxiv.org/abs/2008.08427) | 本文研究了深度为2的带限制随机神经网络的表达能力，通过数学证明确定了当隐藏参数分布于有界域时，网络可能无法达到零逼近误差。 |
| [^288] | [Nearest Neighbor Sampling of Point Sets using Rays.](http://arxiv.org/abs/1911.10737) | 本论文提出了一种新的框架用于最近邻的点集抽样，所涉及的RaySense草图可以捕捉点的基本几何形态以及提取与之相关的统计信息，且可高效地进行点集上的线积分计算。 |

# 详细

[^1]: 图重写用于图神经网络

    Graph Rewriting for Graph Neural Networks. (arXiv:2305.18632v1 [cs.LG])

    [http://arxiv.org/abs/2305.18632](http://arxiv.org/abs/2305.18632)

    该论文提出使用图重写作为建模和分析复杂图形转换的工具，同时将GNN表示为图重写系统可以帮助设计和分析GNN的架构和算法。

    

    图神经网络（GNN）支持对节点、边缘、属性或图属性进行推理。图重写研究基于规则的图形操作，以建模复杂的图形转换。我们认为，（i）图重写包含GNN并可以作为研究和比较它们的正式模型，（ii）将GNN表示为图重写系统可以帮助设计和分析GNN、它们的架构和算法。因此，我们提出Graph Rewriting Neural Networks (GReNN)作为GNN的新语义基础和工程学科。我们开展了一个案例研究，类似于以Groove图形重写模型实现的消息传递神经网络，并探索了它对动态更新的增量操作。

    Given graphs as input, Graph Neural Networks (GNNs) support the inference of nodes, edges, attributes, or graph properties. Graph Rewriting investigates the rule-based manipulation of graphs to model complex graph transformations. We propose that, therefore, (i) graph rewriting subsumes GNNs and could serve as formal model to study and compare them, and (ii) the representation of GNNs as graph rewrite systems can help to design and analyse GNNs, their architectures and algorithms. Hence we propose Graph Rewriting Neural Networks (GReNN) as both novel semantic foundation and engineering discipline for GNNs. We develop a case study reminiscent of a Message Passing Neural Network realised as a Groove graph rewriting model and explore its incremental operation in response to dynamic updates.
    
[^2]: 基于贝叶斯优化的暴雨控制策略及其不确定性识别

    Identification of stormwater control strategies and their associated uncertainties using Bayesian Optimization. (arXiv:2305.18630v1 [cs.LG])

    [http://arxiv.org/abs/2305.18630](http://arxiv.org/abs/2305.18630)

    本文提出了一种基于贝叶斯优化的方法来识别暴雨控制策略并估计相关的不确定性，实现了通过动态配置暴雨网络中的控制资产，调整暴雨网络行为，减少城市洪涝风险的目标。

    

    动态控制越来越成为应对快速演变的天气模式对暴雨系统操作的有效方法。通过降雨预测和实时传感器测量，可以动态配置暴雨网络中的控制资产，调整暴雨网络的行为，减少城市洪涝的风险，将流量均衡到用水再生设施，并保护接收水体。然而，开发这样的控制策略需要大量的人力和计算资源，并且还不存在一种方法来量化实施这些控制策略所带来的风险。为了解决这些挑战，在本文中，我们介绍了一种基于贝叶斯优化的方法来识别暴雨控制策略并估计相关的不确定性。我们在仿真环境中评估了这种方法在识别实际中可行的控制策略方面的功效。

    Dynamic control is emerging as an effective methodology for operating stormwater systems under stress from rapidly evolving weather patterns. Informed by rainfall predictions and real-time sensor measurements, control assets in the stormwater network can be dynamically configured to tune the behavior of the stormwater network to reduce the risk of urban flooding, equalize flows to the water reclamation facilities, and protect the receiving water bodies. However, developing such control strategies requires significant human and computational resources, and a methodology does not yet exist for quantifying the risks associated with implementing these control strategies. To address these challenges, in this paper, we introduce a Bayesian Optimization-based approach for identifying stormwater control strategies and estimating the associated uncertainties. We evaluate the efficacy of this approach in identifying viable control strategies in a simulated environment on real-world inspired comb
    
[^3]: 全局缩放量化：具有理论保证的分布式学习实用的无浮点量化

    Global-QSGD: Practical Floatless Quantization for Distributed Learning with Theoretical Guarantees. (arXiv:2305.18627v1 [cs.LG])

    [http://arxiv.org/abs/2305.18627](http://arxiv.org/abs/2305.18627)

    Global-QSGD是一种新颖的全局缩放量化机制，可以提高分布式学习的效率，并且不需要昂贵的误差反馈，并提供了高达$O(\ sqrt{n})$的额外压缩比。

    

    高效的分布式训练是推动深度学习近期进展的主要驱动力。然而，通信常常是系统的主要瓶颈并具有高昂的代价。因此，需要设计高效的通信机制，既能在经验上提高吞吐量，又能提供理论保证。在这项工作中，我们介绍了全局-QSGD，一种新颖的量化运算符，通过全局缩放设计来加速基于分布式学习。我们证明Global-QSGD是第一个理论上严格的Allreduce兼容压缩机制，通过在压缩误差和通信节省之间取得平衡来实现可证明的加速。重要的是，由于其固有的无偏性，Global-QSGD不依赖昂贵的误差反馈，并且相对于流行的QSGD量化能提供高达$O(\sqrt{n})$ 的额外压缩比（其中$n$表示工作者的数量）。为了获得理论保证，我们采用了信息论和凸分析技术。

    Efficient distributed training is a principal driver of recent advances in deep learning. However, communication often proves costly and becomes the primary bottleneck in these systems. As a result, there is a demand for the design of efficient communication mechanisms that can empirically boost throughput while providing theoretical guarantees. In this work, we introduce Global-QSGD, a novel family of quantization operators, engineered to accelerate distributed training based on global scaling. We demonstrate that Global-QSGD is the first theoretically rigorous Allreduce-compatible compression mechanism that achieves a provable speed-up by striking a balance between compression error and communication savings. Importantly, Global-QSGD does not rely on costly error feedback due to its inherent unbiasedness and offers up to $O(\sqrt{n})$ additional compression ratio compared to the popular QSGD quantization ($n$ represents the number of workers). To obtain theoretical guarantees, we gen
    
[^4]: W-procer: 基于加权原型对比学习的医学少样本命名实体识别

    W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition. (arXiv:2305.18624v1 [cs.CL])

    [http://arxiv.org/abs/2305.18624](http://arxiv.org/abs/2305.18624)

    W-procer是一种基于加权原型对比学习的医学少样本命名实体识别方法，在构建基于原型的对比损失和加权网络方面具有创新性，优于现有的最先进方法。

    

    对比学习已成为少样本命名实体识别（NER）的一种受欢迎的解决方案。传统配置力求减少具有相同标签的标记之间的距离，并增加具有不同标签的标记之间的距离。然而，在医学领域中存在大量被注释为“O”（即“OUTSIDE”）的实体，并且它们不希望被推离到当前对比学习方法标记为“O”以外的其他实体，这种设定效果不佳，可能会得出含有噪声原型标签的语义表示，尽管存在许多“O”标签实体与有标签实体相关。为解决这个挑战，我们提出了一种名为医学少样本命名实体识别中基于加权原型的对比学习方法（W-PROCER）。我们的方法主要围绕构建基于原型的对比损失和加权网络展开。这些组件在协助在医学领域中的迁移学习方面发挥了至关重要的作用。在实验中，我们将W-PROCER应用于一个公共的医学数据集，并展示了其相对于现有的最先进方法的优异表现。

    Contrastive learning has become a popular solution for few-shot Name Entity Recognization (NER). The conventional configuration strives to reduce the distance between tokens with the same labels and increase the distance between tokens with different labels. The effect of this setup may, however, in the medical domain, there are a lot of entities annotated as OUTSIDE (O), and they are undesirably pushed apart to other entities that are not labeled as OUTSIDE (O) by the current contrastive learning method end up with a noisy prototype for the semantic representation of the label, though there are many OUTSIDE (O) labeled entities are relevant to the labeled entities. To address this challenge, we propose a novel method named Weighted Prototypical Contrastive Learning for Medical Few Shot Named Entity Recognization (W-PROCER). Our approach primarily revolves around constructing the prototype-based contractive loss and weighting network. These components play a crucial role in assisting t
    
[^5]: Alfred: 一种通过提示进行弱监督的系统

    Alfred: A System for Prompted Weak Supervision. (arXiv:2305.18623v1 [cs.LG])

    [http://arxiv.org/abs/2305.18623](http://arxiv.org/abs/2305.18623)

    Alfred是一个系统，通过提示创建机器学习训练数据，而不是由专家编写的程序式弱监督(PWS)系统，用户可以通过自然语言提示为语言和视觉-语言模型编写主题专业知识。Alfred为这种新兴范式的关键步骤提供简单的Python接口，具有大规模数据标注的高吞吐量后端。

    

    Alfred是第一个使用提示创建机器学习训练数据的程序式弱监督(PWS)系统。与典型的PWS系统不同，其中弱监督源由专家编写的程序，Alfred允许用户通过自然语言提示为语言和视觉-语言模型编写主题专业知识。Alfred为这种新兴范式的关键步骤提供简单的Python接口，具有大规模数据标注的高吞吐量后端。用户可以快速创建、评估和完善基于提示的弱监督来源；将结果映射到弱标签；并使用标签模型解决不同意见。Alfred支持被自管理的计算集群提供的模型服务，实现无缝的本地开发体验。它使用优化的批处理机制自动优化提示的执行。我们发现，与简单方法相比，这种优化可以将查询吞吐量提高2.9倍。我们提供了两个示例。

    Alfred is the first system for programmatic weak supervision (PWS) that creates training data for machine learning by prompting. In contrast to typical PWS systems where weak supervision sources are programs coded by experts, Alfred enables users to encode their subject matter expertise via natural language prompts for language and vision-language models. Alfred provides a simple Python interface for the key steps of this emerging paradigm, with a high-throughput backend for large-scale data labeling. Users can quickly create, evaluate, and refine their prompt-based weak supervision sources; map the results to weak labels; and resolve their disagreements with a label model. Alfred enables a seamless local development experience backed by models served from self-managed computing clusters. It automatically optimizes the execution of prompts with optimized batching mechanisms. We find that this optimization improves query throughput by 2.9x versus a naive approach. We present two example
    
[^6]: 应用于大型动态图的即时表示学习推荐模型

    Instant Representation Learning for Recommendation over Large Dynamic Graphs. (arXiv:2305.18622v1 [cs.IR])

    [http://arxiv.org/abs/2305.18622](http://arxiv.org/abs/2305.18622)

    提出了一种名为SUPA的新型图神经网络模型，用于在大型动态图中即时学习表示，以提高推荐系统的效率和效果。

    

    推荐系统通过用户和物品的表示学习来了解用户偏好，而现代推荐模型开始利用用户表现出的各种行为类型的信息，以提高表示学习效果。在真实世界的情况下，用户行为图不仅是多重的，而且是动态的，即图随时间快速演变，添加或删除各种类型的节点和边缘，这导致邻域扰动。然而，大多数现有方法忽略了这种流动动力学，因此一旦图形发生显着演变，它们需要重新训练，使它们不适用于在线学习环境。此外，动态图中存在的邻域扰动会恶化基于邻居聚合的图模型的性能。为此，我们提出了SUPA，这是一个新颖的动态多重异构图的图神经网络。与邻居聚合体系结构相比，SUPA开发了一个时间感知的消息传递方案，以自适应地聚合来自各种邻居的节点信息。此外，我们还整合了一个动态图编码器来捕捉图级动态。因此，我们可以以在线方式高效地生成与其历史行为相关的用户和物品表示。三个公共基准测试的实验结果验证了SUPA与最先进方法相比的有效性和效率。

    Recommender systems are able to learn user preferences based on user and item representations via their historical behaviors. To improve representation learning, recent recommendation models start leveraging information from various behavior types exhibited by users. In real-world scenarios, the user behavioral graph is not only multiplex but also dynamic, i.e., the graph evolves rapidly over time, with various types of nodes and edges added or deleted, which causes the Neighborhood Disturbance. Nevertheless, most existing methods neglect such streaming dynamics and thus need to be retrained once the graph has significantly evolved, making them unsuitable in the online learning environment. Furthermore, the Neighborhood Disturbance existing in dynamic graphs deteriorates the performance of neighbor-aggregation based graph models. To this end, we propose SUPA, a novel graph neural network for dynamic multiplex heterogeneous graphs. Compared to neighbor-aggregation architecture, SUPA dev
    
[^7]: 基于似然的扩散语言模型

    Likelihood-Based Diffusion Language Models. (arXiv:2305.18619v1 [cs.CL])

    [http://arxiv.org/abs/2305.18619](http://arxiv.org/abs/2305.18619)

    本论文介绍了基于似然的扩散语言模型，并通过算法改进、缩放定律和增加计算，成功构建和发布了一个超过小但广为人知的自回归模型的扩散模型，优于GPT-2 124M。

    

    尽管人们对基于扩散的语言模型越来越感兴趣，但现有的工作尚未表明这些模型可以在标准语言建模基准上获得非微不足道的似然度。在这项工作中，我们首先采取了措施来缩小自回归和扩散语言模型之间的似然差异，目标是构建和发布一个超过小但广为人知的自回归模型的扩散模型。我们通过算法改进、缩放定律和增加计算来实现这个目标。在算法前沿，我们引入了几种最大似然训练扩散语言模型的方法论改进。然后，我们研究了我们的扩散模型缩放定律，并发现计算优化的训练方案与自回归模型差别很大。使用我们的方法和缩放分析，我们训练并发布了Plaid 1B，一个大型扩散语言模型，它在基准数据集上的似然度和生成文本质量上优于GPT-2 124M。

    Despite a growing interest in diffusion-based language models, existing work has not shown that these models can attain nontrivial likelihoods on standard language modeling benchmarks. In this work, we take the first steps towards closing the likelihood gap between autoregressive and diffusion-based language models, with the goal of building and releasing a diffusion model which outperforms a small but widely-known autoregressive model. We pursue this goal through algorithmic improvements, scaling laws, and increased compute. On the algorithmic front, we introduce several methodological improvements for the maximum-likelihood training of diffusion language models. We then study scaling laws for our diffusion models and find compute-optimal training regimes which differ substantially from autoregressive models. Using our methods and scaling analysis, we train and release Plaid 1B, a large diffusion language model which outperforms GPT-2 124M in likelihood on benchmark datasets and gener
    
[^8]: 基于位置感知图增强变分自编码器的网络时间序列插补

    Networked Time Series Imputation via Position-aware Graph Enhanced Variational Autoencoders. (arXiv:2305.18612v1 [cs.LG])

    [http://arxiv.org/abs/2305.18612](http://arxiv.org/abs/2305.18612)

    本文提出了一种新颖的基于位置感知图增强变分自编码器方法来处理网络时间序列插补问题。该方法能够处理NTS数据中的图动态和缺失边，从而在合成和真实数据集上实现了优异的表现。

    

    多元时间序列插补是近年来广泛研究的问题。现有方法可以分为两大类，包括（1）主要关注时间序列特征的深度递归或生成模型，以及（2）基于图神经网络（GNN）的模型，利用MTS固有图结构的拓扑信息作为插补的关系归纳偏差。然而，这些方法要么忽略了拓扑信息，要么假定图结构固定且准确已知。因此，在更具挑战性的网络时间序列（NTS）数据中，它们无法充分利用图动态进行精确的插补，其中底层图不断变化并可能存在缺失边。本文提出了一种新方法来克服这些限制。首先，我们定义了包含节点时间序列特征和图结构中缺失值的NTS插补问题。然后，我们设计了一种名为PGE-VAE的新模型，它利用定位编码技术将时间序列信息合并到图神经网络中。具体而言，我们建议使用自我注意机制来捕捉图中不同时间步骤和不同节点之间的依赖关系。此外，我们引入了一个动态图生成网络来学习图结构的演化，可以处理缺失的边并适应图动态。在合成和真实数据集上的广泛实验表明，我们提出的方法优于现有最先进的方法。

    Multivariate time series (MTS) imputation is a widely studied problem in recent years. Existing methods can be divided into two main groups, including (1) deep recurrent or generative models that primarily focus on time series features, and (2) graph neural networks (GNNs) based models that utilize the topological information from the inherent graph structure of MTS as relational inductive bias for imputation. Nevertheless, these methods either neglect topological information or assume the graph structure is fixed and accurately known. Thus, they fail to fully utilize the graph dynamics for precise imputation in more challenging MTS data such as networked time series (NTS), where the underlying graph is constantly changing and might have missing edges. In this paper, we propose a novel approach to overcome these limitations. First, we define the problem of imputation over NTS which contains missing values in both node time series features and graph structures. Then, we design a new mod
    
[^9]: 提高多模态假新闻检测的泛化能力

    Improving Generalization for Multimodal Fake News Detection. (arXiv:2305.18599v1 [cs.CL])

    [http://arxiv.org/abs/2305.18599](http://arxiv.org/abs/2305.18599)

    本研究提出了三种模型，采用和微调最先进的多模态Transformer进行多模态假新闻检测，并提出了训练数据增强来提高模型泛化能力。

    

    现在，虚假信息的不断传播及其惊人的影响已经促使行业和学术界开发出假新闻检测方法。然而，最先进的方法通常使用规模较小或特定主题的有限数据集进行训练。因此，这些模型缺乏泛化能力，不能应用于现实世界的数据。本文提出了三种采用并微调最先进的多模态Transformer进行多模态假新闻检测的模型。我们通过操作输入数据进行深入分析，旨在探索这些模型在社交媒体上的实际使用情况下的性能。我们跨多个模型进行的研究表明，这些系统在受到操作数据的情况下会出现显着的性能下降。为了减少偏差并提高模型的泛化能力，我们建议进行训练数据增强，以在社交媒体上进行更有意义的假新闻检测实验。

    The increasing proliferation of misinformation and its alarming impact have motivated both industry and academia to develop approaches for fake news detection. However, state-of-the-art approaches are usually trained on datasets of smaller size or with a limited set of specific topics. As a consequence, these models lack generalization capabilities and are not applicable to real-world data. In this paper, we propose three models that adopt and fine-tune state-of-the-art multimodal transformers for multimodal fake news detection. We conduct an in-depth analysis by manipulating the input data aimed to explore models performance in realistic use cases on social media. Our study across multiple models demonstrates that these systems suffer significant performance drops against manipulated data. To reduce the bias and improve model generalization, we suggest training data augmentation to conduct more meaningful experiments for fake news detection on social media. The proposed data augmentat
    
[^10]: 基于协作学习的分析端到端深度学习算法

    An Analytic End-to-End Deep Learning Algorithm based on Collaborative Learning. (arXiv:2305.18594v1 [cs.LG])

    [http://arxiv.org/abs/2305.18594](http://arxiv.org/abs/2305.18594)

    本文提出了一种平滑激活函数的全连接神经网络端到端深度学习收敛分析方法，避免了潜在的抖动问题，并且可以使用协作学习进一步结合多个网络的优势。

    

    在大多数控制应用中，系统的理论分析对确保稳定性或收敛至关重要，以确保安全可靠的运行并获得更好的系统理解以进行进一步的发展。然而，大多数当前的深度学习方法是黑盒方法，更多地关注经验性研究。本文提出了一种基于平滑激活函数的全连接神经网络（FNN）端到端深度学习收敛分析方法，避免了潜在的抖动问题，同时也不易导致梯度消失问题。该算法可以同时训练多个两层全连接网络，并使用协作学习进一步结合它们的优势。

    In most control applications, theoretical analysis of the systems is crucial in ensuring stability or convergence, so as to ensure safe and reliable operations and also to gain a better understanding of the systems for further developments. However, most current deep learning methods are black-box approaches that are more focused on empirical studies. Recently, some results have been obtained for convergence analysis of end-to end deep learning based on non-smooth ReLU activation functions, which may result in chattering for control tasks. This paper presents a convergence analysis for end-to-end deep learning of fully connected neural networks (FNN) with smooth activation functions. The proposed method therefore avoids any potential chattering problem, and it also does not easily lead to gradient vanishing problems. The proposed End-to-End algorithm trains multiple two-layer fully connected networks concurrently and collaborative learning can be used to further combine their strengths
    
[^11]: 关于扩散建模在异常检测中的应用

    On Diffusion Modeling for Anomaly Detection. (arXiv:2305.18593v1 [cs.LG])

    [http://arxiv.org/abs/2305.18593](http://arxiv.org/abs/2305.18593)

    本文研究了扩散建模在无监督和半监督异常检测中的应用，发现去噪扩散概率模型表现很好但计算成本高，因此提出了一种替代方法——扩散时间概率模型，该模型能够通过较大的时间步长上的高后验密度识别异常，并通过深度神经网络提高效率。

    

    扩散模型以其在生成建模中的优异性能而闻名，成为基于密度的异常检测的有吸引力的候选算法。本文研究了各种扩散建模方法在无监督和半监督异常检测中的应用。尤其是发现去噪扩散概率模型（DDPM）在异常检测方面具备很好的表现，但计算成本较高。通过简化DDPM在异常检测中的应用，我们自然地引出另一种称为扩散时间概率模型（DTPM）的替代方法。DTPM估计给定输入的扩散时间的后验分布，能够通过较大的时间步长上的高后验密度识别异常。我们导出了此后验分布的解析形式，并利用深度神经网络提高推理效率。通过在ADBenh基准测试中的实证评估，我们证明了所有基于扩散的异常检测方法的实用性。

    Known for their impressive performance in generative modeling, diffusion models are attractive candidates for density-based anomaly detection. This paper investigates different variations of diffusion modeling for unsupervised and semi-supervised anomaly detection. In particular, we find that Denoising Diffusion Probability Models (DDPM) are performant on anomaly detection benchmarks yet computationally expensive. By simplifying DDPM in application to anomaly detection, we are naturally led to an alternative approach called Diffusion Time Probabilistic Model (DTPM). DTPM estimates the posterior distribution over diffusion time for a given input, enabling the identification of anomalies due to their higher posterior density at larger timesteps. We derive an analytical form for this posterior density and leverage a deep neural network to improve inference efficiency. Through empirical evaluations on the ADBench benchmark, we demonstrate that all diffusion-based anomaly detection methods 
    
[^12]: 12导联心电图分类的深度神经网络泛化和微调

    Deep Neural Networks Generalization and Fine-Tuning for 12-lead ECG Classification. (arXiv:2305.18592v1 [eess.SP])

    [http://arxiv.org/abs/2305.18592](http://arxiv.org/abs/2305.18592)

    该论文提出了一种改善心脏疾病预测质量的方法，即通过在多个数据集上的训练和微调，在不同数据集和临床设置下都可以获得高质量的深度神经网络分类器。

    

    许多研究都致力于使用深度学习方法基于12导联心电图（ECG）记录来诊断心脏疾病。这些研究通常使用特定的数据集，这些数据集在大小和参数方面都有所不同，如患者元数据、标注ECG的医生数量、用于ECG记录的设备类型、数据预处理技术等。众所周知，训练在一个ECG数据集上的高质量深度神经网络不一定在另一个数据集或临床设置中表现良好。在本文中，我们提出了一种方法，通过在各种数据集上训练神经网络并进一步对特定数据集进行微调，可以改善心脏疾病预测的质量。为了展示其适用性，我们在一个大型私人数据集TIS和一个相对较小的公共数据集PTB-XL上训练了不同的神经网络。我们证明，相较于仅在特定数据集上进行训练，通过在大型数据集上训练网络并进行特定数据集的微调，可以显著提高分类准确性。此外，我们还表明仅微调预训练网络的最后一层就足以实现高质量的结果。

    Numerous studies are aimed at diagnosing heart diseases based on 12-lead electrocardiographic (ECG) records using deep learning methods. These studies usually use specific datasets that differ in size and parameters, such as patient metadata, number of doctors annotating ECGs, types of devices for ECG recording, data preprocessing techniques, etc. It is well-known that high-quality deep neural networks trained on one ECG dataset do not necessarily perform well on another dataset or clinical settings. In this paper, we propose a methodology to improve the quality of heart disease prediction regardless of the dataset by training neural networks on a variety of datasets with further fine-tuning for the specific dataset. To show its applicability, we train different neural networks on a large private dataset TIS containing various ECG records from multiple hospitals and on a relatively small public dataset PTB-XL. We demonstrate that training the networks on a large dataset and fine-tuning
    
[^13]: Coeditor：利用上下文变化进行多轮代码自动编辑

    Coeditor: Leveraging Contextual Changes for Multi-round Code Auto-editing. (arXiv:2305.18584v1 [cs.SE])

    [http://arxiv.org/abs/2305.18584](http://arxiv.org/abs/2305.18584)

    Coeditor开发了一个多轮代码自动编辑模型，利用同一代码库中的最近变化来预测对代码区域的编辑，表现出更高的准确率。

    

    开发人员经常花费大量时间来维护和重构现有代码。然而，大多数关于生成模型的先前工作都仅关注于创建新代码，忽略了对编辑现有代码的独特要求。在这项工作中，我们探索了一个多轮代码自动编辑的设置，旨在基于同一代码库中的最近变化来预测对代码区域的编辑。我们的模型Coeditor是一个经过细化的CodeT5模型，具有专门设计用于代码编辑任务的增强功能。我们使用行差异格式对代码更改进行编码，并采用静态分析来形成大型定制模型上下文，以确保适当的预测信息。我们从1650个开源Python项目的提交历史中收集了一个代码编辑数据集用于训练和评估。在简化的单轮单编辑任务中，Coeditor的准确性显著优于最佳的代码完成方法，准确率近乎翻倍，即使使用的模型更小。

    Developers often dedicate significant time to maintaining and refactoring existing code. However, most prior work on generative models for code focuses solely on creating new code, neglecting the unique requirements of editing existing code. In this work, we explore a multi-round code auto-editing setting, aiming to predict edits to a code region based on recent changes within the same codebase. Our model, Coeditor, is a fine-tuned CodeT5 model with enhancements specifically designed for code editing tasks. We encode code changes using a line diff format and employ static analysis to form large customized model contexts, ensuring appropriate information for prediction. We collect a code editing dataset from the commit histories of 1650 open-source Python projects for training and evaluation. In a simplified single-round, single-edit task, Coeditor significantly outperforms the best code completion approach -- nearly doubling its exact-match accuracy, despite using a much smaller model 
    
[^14]: 快速自适应三元分割：隐马尔可夫模型的有效解码程序。

    Quick Adaptive Ternary Segmentation: An Efficient Decoding Procedure For Hidden Markov Models. (arXiv:2305.18578v1 [stat.ME])

    [http://arxiv.org/abs/2305.18578](http://arxiv.org/abs/2305.18578)

    提出了一种名为QATS的新方法，用于高效解码隐藏马尔可夫模型序列。它的计算复杂性为多对数和立方，特别适用于具有相对较少状态的大型HMM。

    

    隐马尔可夫模型（HMM）以不可观察的（隐藏的）马尔可夫链和可观测的过程为特征，后者是隐藏链的噪声版本。从嘈杂的观测中解码原始信号（即隐藏链）是几乎所有基于HMM的数据分析的主要目标。现有的解码算法，如维特比算法，在观测序列长度最多线性的情况下具有计算复杂度，并且在马尔可夫链状态空间的大小中具有次二次计算复杂度。我们提出了快速自适应三元分割（QATS），这是一种分而治之的过程，可在序列长度的多对数计算复杂度和马尔可夫链状态空间的三次计算复杂度下解码隐藏的序列，因此特别适用于具有相对较少状态的大规模HMM。该程序还建议一种有效的数据存储方式，即特定的累积总和。实质上，估计的状态序列按顺序最大化局部似然。

    Hidden Markov models (HMMs) are characterized by an unobservable (hidden) Markov chain and an observable process, which is a noisy version of the hidden chain. Decoding the original signal (i.e., hidden chain) from the noisy observations is one of the main goals in nearly all HMM based data analyses. Existing decoding algorithms such as the Viterbi algorithm have computational complexity at best linear in the length of the observed sequence, and sub-quadratic in the size of the state space of the Markov chain. We present Quick Adaptive Ternary Segmentation (QATS), a divide-and-conquer procedure which decodes the hidden sequence in polylogarithmic computational complexity in the length of the sequence, and cubic in the size of the state space, hence particularly suited for large scale HMMs with relatively few states. The procedure also suggests an effective way of data storage as specific cumulative sums. In essence, the estimated sequence of states sequentially maximizes local likeliho
    
[^15]: 为学习优化构建数学结构

    Towards Constituting Mathematical Structures for Learning to Optimize. (arXiv:2305.18577v1 [cs.LG])

    [http://arxiv.org/abs/2305.18577](http://arxiv.org/abs/2305.18577)

    本文提出了一种结构受到数学启发的L2O模型，其具有广泛的适用性和良好的推广性能，并基于成功的更新规则通常满足的基本数学条件进行了推导。

    

    近年来，利用机器学习从数据中自动学习优化算法的学习优化(L2O)技术引起了人们的广泛关注。一种通用的L2O方法参数化了迭代更新规则，并将更新方向作为黑盒网络进行学习。虽然通用方法具有广泛适用性，但学习的模型可能过拟合，无法很好地推广到分布不同的测试集中。本文推导了成功的更新规则通常满足的基本数学条件。因此，我们提出了一种新的L2O模型，其结构受到数学启发，并且具有广泛的适用性和良好的推广性能。数值模拟验证了我们的理论发现，并展示了所提出的L2O模型的良好实验性能。

    Learning to Optimize (L2O), a technique that utilizes machine learning to learn an optimization algorithm automatically from data, has gained arising attention in recent years. A generic L2O approach parameterizes the iterative update rule and learns the update direction as a black-box network. While the generic approach is widely applicable, the learned model can overfit and may not generalize well to out-of-distribution test sets. In this paper, we derive the basic mathematical conditions that successful update rules commonly satisfy. Consequently, we propose a novel L2O model with a mathematics-inspired structure that is broadly applicable and generalized well to out-of-distribution problems. Numerical simulations validate our theoretical findings and demonstrate the superior empirical performance of the proposed L2O model.
    
[^16]: ChatGPT的公平性评估

    Fairness of ChatGPT. (arXiv:2305.18569v1 [cs.LG])

    [http://arxiv.org/abs/2305.18569](http://arxiv.org/abs/2305.18569)

    本文提供了一个使用ChatGPT作为研究案例的LLM有效性和公平性的系统评估，旨在评估ChatGPT在高风险领域的表现，以提供更深入的了解LLM的公平表现，并为偏见缓解和负责任的人工智能系统的发展做出贡献。

    

    理解和解决LLM中不公平的问题对于负责任的AI部署至关重要。然而，在将LLM应用于高风险领域时，尤其是关于公平评估方面，数量分析和深入研究的可用性有限。本文旨在提供一个使用ChatGPT作为研究案例的LLM有效性和公平性的系统评估，我们专注于评估ChatGPT在包括教育、犯罪学、金融和医疗保健等高风险领域的表现。为了进行全面的评估，我们考虑了群体公平性和个人公平性，并观察了在一系列有偏或无偏提示下ChatGPT输出的差异。该研究对于更深入的了解LLM的公平表现，便于偏见缓解，促进负责任的人工智能系统的发展具有意义。

    Understanding and addressing unfairness in LLMs are crucial for responsible AI deployment. However, there is a limited availability of quantitative analyses and in-depth studies regarding fairness evaluations in LLMs, especially when applying LLMs to high-stakes fields. This work aims to fill this gap by providing a systematic evaluation of the effectiveness and fairness of LLMs using ChatGPT as a study case. We focus on assessing ChatGPT's performance in high-takes fields including education, criminology, finance and healthcare. To make thorough evaluation, we consider both group fairness and individual fairness and we also observe the disparities in ChatGPT's outputs under a set of biased or unbiased prompts. This work contributes to a deeper understanding of LLMs' fairness performance, facilitates bias mitigation and fosters the development of responsible artificial intelligence systems.
    
[^17]: PaLI-X：关于扩展一种多语言视觉与语言模型的研究

    PaLI-X: On Scaling up a Multilingual Vision and Language Model. (arXiv:2305.18565v1 [cs.CV])

    [http://arxiv.org/abs/2305.18565](http://arxiv.org/abs/2305.18565)

    论文介绍了一个多语言视觉与语言模型PaLI-X，通过扩展模型组件和训练任务范围，实现了在复杂任务上的新性能水平，包括图像字幕问答、对象检测、视频问答和视频字幕等，同时在视觉语言基准测试中取得了最新的研究成果。

    

    我们介绍了扩展PaLI-X，一种多语言视觉与语言模型的训练配方和结果，包括组件的大小和训练任务范围的广度。我们的模型在多个基于图像的字幕和问答任务、基于图像的文档理解和少量（上下文内）学习以及对象检测、视频问答和视频字幕等各种复杂任务上实现了新的性能水平。PaLI-X 在大多数视觉语言基准测试中取得了最新的研究成果（考虑了25个以上的测试）。最后，我们观察到了一些新兴的能力，如复杂计数和多语言对象检测，这些任务并没有明确列在训练范围之内。

    We present the training recipe and results of scaling up PaLI-X, a multilingual vision and language model, both in terms of size of the components and the breadth of its training task mixture. Our model achieves new levels of performance on a wide-range of varied and complex tasks, including multiple image-based captioning and question-answering tasks, image-based document understanding and few-shot (in-context) learning, as well as object detection, video question answering, and video captioning. PaLI-X advances the state-of-the-art on most vision-and-language benchmarks considered (25+ of them). Finally, we observe emerging capabilities, such as complex counting and multilingual object detection, tasks that are not explicitly in the training mix.
    
[^18]: SHARP: 稀疏性和隐藏激活回放用于神经启发式的连续学习

    SHARP: Sparsity and Hidden Activation RePlay for Neuro-Inspired Continual Learning. (arXiv:2305.18563v1 [cs.LG])

    [http://arxiv.org/abs/2305.18563](http://arxiv.org/abs/2305.18563)

    SHARP是一种神经启发式的连续学习方法，利用稀疏的动态连接和激活回放来回放处理过的神经模式，优先回放最近学到的信息，并可以持续更新所有层。

    

    深度神经网络(DNNs)在动态环境下学习困难，因为它们依赖于固定的数据集或恒定的环境。连续学习(CL)的目标是解决这个限制，使DNN能够逐步积累知识，类似于人类学习。受到大脑如何巩固记忆的启发，回放是CL的强有力策略，它涉及对DNN进行新和所有已见类别的混合训练。然而，现有的回放方法忽略了生物回放的两个关键方面: 1) 大脑回放处理过的神经模式而不是原始输入，2)它优先回放最近学到的信息，而不是重温所有过去的经验。为了解决这些差异，我们提出了SHARP，这是一种高效的神经启发式CL方法，利用稀疏的动态连接和激活回放。与其他激活回放方法不同，它假定没有受到回放的层已经经过预训练并固定，SHARP可以持续更新所有层。

    Deep neural networks (DNNs) struggle to learn in dynamic environments since they rely on fixed datasets or stationary environments. Continual learning (CL) aims to address this limitation and enable DNNs to accumulate knowledge incrementally, similar to human learning. Inspired by how our brain consolidates memories, a powerful strategy in CL is replay, which involves training the DNN on a mixture of new and all seen classes. However, existing replay methods overlook two crucial aspects of biological replay: 1) the brain replays processed neural patterns instead of raw input, and 2) it prioritizes the replay of recently learned information rather than revisiting all past experiences. To address these differences, we propose SHARP, an efficient neuro-inspired CL method that leverages sparse dynamic connectivity and activation replay. Unlike other activation replay methods, which assume layers not subjected to replay have been pretrained and fixed, SHARP can continually update all layers
    
[^19]: DelBugV: Delta-Debugging神经网络验证器

    DelBugV: Delta-Debugging Neural Network Verifiers. (arXiv:2305.18558v1 [cs.LO])

    [http://arxiv.org/abs/2305.18558](http://arxiv.org/abs/2305.18558)

    DelBugV是一种使用自动增量调试技术进行DNN验证器调试的工具，它可以产生更简单的验证实例帮助用户快速发现验证器的错误。

    

    深度神经网络已经成为各种系统的关键组成部分。然而，尽管它们非常成功，但它们经常出现严重的错误；这引起了对其进行形式验证的重大兴趣。不幸的是，DNN验证器是复杂的工具，它们本身容易受到正确性错误的影响。由于DNN验证器的复杂性以及正在验证的DNN的大小，因此调试此类错误是一项艰巨的任务。在这里，我们介绍了一种名为DelBugV的新型工具，它使用自动化的增量调试技术在DNN验证器上进行调试。给定一个有故障的DNN验证器和一个正确的验证器作为参考点（或在某些情况下仅一个有故障的验证器），DelBugV可以产生更简单的DNN验证实例，仍然会触发不需要的行为--这极大地促进了调试故障验证器的任务。我们的工具是模块化和可扩展的，并且可以轻松地通过附加网络简化方法和策略进行增强。

    Deep neural networks (DNNs) are becoming a key component in diverse systems across the board. However, despite their success, they often err miserably; and this has triggered significant interest in formally verifying them. Unfortunately, DNN verifiers are intricate tools, and are themselves susceptible to soundness bugs. Due to the complexity of DNN verifiers, as well as the sizes of the DNNs being verified, debugging such errors is a daunting task. Here, we present a novel tool, named DelBugV, that uses automated delta debugging techniques on DNN verifiers. Given a malfunctioning DNN verifier and a correct verifier as a point of reference (or, in some cases, just a single, malfunctioning verifier), DelBugV can produce much simpler DNN verification instances that still trigger undesired behavior -- greatly facilitating the task of debugging the faulty verifier. Our tool is modular and extensible, and can easily be enhanced with additional network simplification methods and strategies.
    
[^20]: 可控毁灭路径

    Controllable Path of Destruction. (arXiv:2305.18553v1 [cs.AI])

    [http://arxiv.org/abs/2305.18553](http://arxiv.org/abs/2305.18553)

    本文介绍了可控毁灭路径方法，该方法是一种自我监督的迭代生成器学习方法，通过向修复轨迹的状态-动作对添加条件输入来实现可控性。

    

    毁灭路径（PoD）是一种自我监督的迭代生成器学习方法。其核心思想是通过破坏一组物品来产生一个训练集，为每个破坏步骤创建一个与相应修复动作相关的训练实例。在此数据集上训练的生成器可以通过从任意状态“修复”来生成新的物品。PoD方法在原始训练示例方面非常节省，并且非常适合由分类数据组成的功能部件，例如游戏关卡和离散的3D结构。在本文中，我们将毁灭路径方法扩展到允许设计师控制生成的物品的各个方面。通过向构成修复轨迹的状态-动作对添加条件输入来引入可控性。我们在2D地牢设置以及小型3D乐高汽车领域测试了可控PoD方法。

    Path of Destruction (PoD) is a self-supervised method for learning iterative generators. The core idea is to produce a training set by destroying a set of artifacts, and for each destructive step create a training instance based on the corresponding repair action. A generator trained on this dataset can then generate new artifacts by ``repairing'' from arbitrary states. The PoD method is very data-efficient in terms of original training examples and well-suited to functional artifacts composed of categorical data, such as game levels and discrete 3D structures. In this paper, we extend the Path of Destruction method to allow designer control over aspects of the generated artifacts. Controllability is introduced by adding conditional inputs to the state-action pairs that make up the repair trajectories. We test the controllable PoD method in a 2D dungeon setting, as well as in the domain of small 3D Lego cars.
    
[^21]: 学习神经网络中的线性群

    Learning Linear Groups in Neural Networks. (arXiv:2305.18552v1 [cs.LG])

    [http://arxiv.org/abs/2305.18552](http://arxiv.org/abs/2305.18552)

    本研究提出了一种神经网络架构——线性群网络(LGNs)，它可以学习作用于神经网络权重空间上的线性群，而无需预先指定所需的对称性。该结构具有良好的可解释性，并且可以适用于不同数据集及下游任务。

    

    在神经网络中利用等变性在架构中编码领域知识，可导致更大的参数效率和改进的泛化性能。然而，大部分现有方法需要事先指定所需的对称性。本文提出了一种名为线性群网络（LGNs）的神经网络架构，用于学习作用于神经网络权重空间上的线性群。线性群具有固有的可解释性，因为它们可以表示为有限矩阵。LGNs学习群体，而不需要任何监督或隐藏对称性的知识，并且这些群体可以映射到机器学习中的众所周知的操作。我们使用LGNs来在多个数据集上学习群体，同时考虑不同的下游任务; 我们证明，线性群结构取决于数据分布和考虑的任务。

    Employing equivariance in neural networks leads to greater parameter efficiency and improved generalization performance through the encoding of domain knowledge in the architecture; however, the majority of existing approaches require an a priori specification of the desired symmetries. We present a neural network architecture, Linear Group Networks (LGNs), for learning linear groups acting on the weight space of neural networks. Linear groups are desirable due to their inherent interpretability, as they can be represented as finite matrices. LGNs learn groups without any supervision or knowledge of the hidden symmetries in the data and the groups can be mapped to well known operations in machine learning. We use LGNs to learn groups on multiple datasets while considering different downstream tasks; we demonstrate that the linear group structure depends on both the data distribution and the considered task.
    
[^22]: 短期电力负荷预测误差的元回归分析

    Meta-Regression Analysis of Errors in Short-Term Electricity Load Forecasting. (arXiv:2305.18550v1 [cs.LG])

    [http://arxiv.org/abs/2305.18550](http://arxiv.org/abs/2305.18550)

    本文通过元回归分析研究了短期电力负荷预测的影响因素，发现网络级别、预测粒度和所使用的算法是影响预测精度的重要因素。

    

    预测电力需求对于确保电力供应的可靠和成本效益至关重要。随着分布式可再生能源的全球转型以及供暖和交通的电气化，准确的负荷预测变得更加重要。本文利用来自59篇研究中421个预测模型的数据，进行了影响短期电力负荷预测精度的因素的元回归分析（MRA）。结果表明，网络级别（特别是个体、聚合和系统）、预测粒度和所使用的算法似乎对MAPE、文献计量数据、数据集大小和预测误差有重要影响。

    Forecasting electricity demand plays a critical role in ensuring reliable and cost-efficient operation of the electricity supply. With the global transition to distributed renewable energy sources and the electrification of heating and transportation, accurate load forecasts become even more important. While numerous empirical studies and a handful of review articles exist, there is surprisingly little quantitative analysis of the literature, most notably none that identifies the impact of factors on forecasting performance across the entirety of empirical studies. In this article, we therefore present a Meta-Regression Analysis (MRA) that examines factors that influence the accuracy of short-term electricity load forecasts. We use data from 421 forecast models published in 59 studies. While the grid level (esp. individual vs. aggregated vs. system), the forecast granularity, and the algorithms used seem to have a significant impact on the MAPE, bibliometric data, dataset sizes, and pr
    
[^23]: 针对对抗性攻击的强健Lipschitz赌徒算法

    Robust Lipschitz Bandits to Adversarial Corruptions. (arXiv:2305.18543v1 [cs.LG])

    [http://arxiv.org/abs/2305.18543](http://arxiv.org/abs/2305.18543)

    本文提出的强健Lipschitz赌徒算法，能够在对抗性攻击的情况下实现次线性遗憾，并在强敌手情况下最优。

    

    Lipschitz赌徒算法是一种处理定义在度量空间上的连续臂集的随机赌徒算法的变体，其中奖励函数受到Lipschitz约束。本文介绍了一种新的Lipschitz赌徒问题，即在对抗性破坏存在的情况下，自适应敌手将随机奖励损坏到总预算 $C$。 预算通过时间跨度 $T$ 中的破坏水平之和来衡量。 我们考虑弱和强敌手，其中弱敌手在攻击之前不知道当前的行动，而强敌手可以观察行动。我们的工作提出了第一行强健Lipschitz赌徒算法，在两种类型的敌手下，甚至在损坏总预算 $C$ 未向代理披露的情况下，均能实现次线性遗憾。我们在每种类型的敌手下提供下限，并证明了我们的算法在强类型下是最优的。最后，我们进行实验以说明该算法的有效性。

    Lipschitz bandit is a variant of stochastic bandits that deals with a continuous arm set defined on a metric space, where the reward function is subject to a Lipschitz constraint. In this paper, we introduce a new problem of Lipschitz bandits in the presence of adversarial corruptions where an adaptive adversary corrupts the stochastic rewards up to a total budget $C$. The budget is measured by the sum of corruption levels across the time horizon $T$. We consider both weak and strong adversaries, where the weak adversary is unaware of the current action before the attack, while the strong one can observe it. Our work presents the first line of robust Lipschitz bandit algorithms that can achieve sub-linear regret under both types of adversary, even when the total budget of corruption $C$ is unrevealed to the agent. We provide a lower bound under each type of adversary, and show that our algorithm is optimal under the strong case. Finally, we conduct experiments to illustrate the effecti
    
[^24]: 深度网络黑盒中的彩虹

    A Rainbow in Deep Network Black Boxes. (arXiv:2305.18512v1 [cs.LG])

    [http://arxiv.org/abs/2305.18512](http://arxiv.org/abs/2305.18512)

    彩虹网络是训练深度神经网络的概率模型，通过层内神经元权重互相独立的对齐和随机特征映射来进行线性降维和非线性高维嵌入，在ImageNet和CIFAR-10数据集上进行验证。

    

    我们引入了彩虹网络作为训练好的深度神经网络的概率模型。该模型级联随机特征映射，其权重分布是可以学习的。它假设不同层之间的权重依赖性被减少到将输入激活对准的旋转。层内的神经元权重在这种对齐后是相互独立的。它们的激活定义了在无穷宽度极限下变得确定的内核。这在ImageNet数据集上训练的ResNets中通过数字验证。我们还发现，学习的权重分布具有低秩协方差。因此，彩虹网络在线性降维和非线性高维嵌入与白色随机特征之间交替。我们提供了具有高斯权重分布的高斯彩虹网络定义。这些模型在使用小波散射网络进行CIFAR-10图像分类方面进行了数字验证。我们还证明了，在训练期间，SGD更新权重的协方差。

    We introduce rainbow networks as a probabilistic model of trained deep neural networks. The model cascades random feature maps whose weight distributions are learned. It assumes that dependencies between weights at different layers are reduced to rotations which align the input activations. Neuron weights within a layer are independent after this alignment. Their activations define kernels which become deterministic in the infinite-width limit. This is verified numerically for ResNets trained on the ImageNet dataset. We also show that the learned weight distributions have low-rank covariances. Rainbow networks thus alternate between linear dimension reductions and non-linear high-dimensional embeddings with white random features. Gaussian rainbow networks are defined with Gaussian weight distributions. These models are validated numerically on image classification on the CIFAR-10 dataset, with wavelet scattering networks. We further show that during training, SGD updates the weight cov
    
[^25]: 具有信息预算的情境赌博机算法

    Contextual Bandits with Budgeted Information Reveal. (arXiv:2305.18511v1 [cs.LG])

    [http://arxiv.org/abs/2305.18511](http://arxiv.org/abs/2305.18511)

    本文介绍了一种针对医疗领域“亲治疗”操作的限制，且考虑到了操作预算的具有信息预算的情境赌博机算法，这种算法将在线原始-对偶算法和情境赌博机学习算法有机地结合在一起，取得了很好的效果。

    

    情境赌博机算法常用于推荐个性化的医疗处理方式，但在实际操作中，为保证治疗效果，医生通常需要要求患者采取没有直接好处的“亲治疗”操作，而且临床医生的操作预算有限。本文提出了一种新的优化学习算法，有效结合了两种算法方法之长：1）一个决定最佳时机与患者联系的在线原始-对偶（primal-dual）算法，2）用于向患者提供个性化治疗的情境赌博机学习算法。我们证明了该算法具有亚线性的回归界限。我们在合成和实际数据上展示了该算法的实用价值。

    Contextual bandit algorithms are commonly used in digital health to recommend personalized treatments. However, to ensure the effectiveness of the treatments, patients are often requested to take actions that have no immediate benefit to them, which we refer to as pro-treatment actions. In practice, clinicians have a limited budget to encourage patients to take these actions and collect additional information. We introduce a novel optimization and learning algorithm to address this problem. This algorithm effectively combines the strengths of two algorithmic approaches in a seamless manner, including 1) an online primal-dual algorithm for deciding the optimal timing to reach out to patients, and 2) a contextual bandit learning algorithm to deliver personalized treatment to the patient. We prove that this algorithm admits a sub-linear regret bound. We illustrate the usefulness of this algorithm on both synthetic and real-world data.
    
[^26]: RLAD：基于像素的强化学习在城市环境下自动驾驶中的应用

    RLAD: Reinforcement Learning from Pixels for Autonomous Driving in Urban Environments. (arXiv:2305.18510v1 [cs.CV])

    [http://arxiv.org/abs/2305.18510](http://arxiv.org/abs/2305.18510)

    RLAD是首个在城市自动驾驶领域应用基于像素的强化学习方法，通过优化图像编码器和路径点编码器等技术，可以提高自动驾驶的性能和效率。

    

    当前应用于城市自动驾驶的强化学习（RL）方法主要集中在将感知训练与驾驶策略训练分离。主要原因是为了避免在策略网络旁边训练卷积编码器，因为这种方法在样本效率、特征表示退化和自我过度拟合等方面存在问题。然而，这种方法可能会导致环境表示与下游任务不一致，从而导致次优性能。本文提出了RLAD，这是首个在城市自动驾驶领域应用基于像素的强化学习（RLfP）方法。我们提出了几种技术来提高在此领域中RLfP算法的性能，包括：i）一种图像编码器，利用图像增强和自适应局部信号混合（A-LIX）层的优势；ii）WayConv1D，一种利用2D几何信息的路径点编码器。

    Current approaches of Reinforcement Learning (RL) applied in urban Autonomous Driving (AD) focus on decoupling the perception training from the driving policy training. The main reason is to avoid training a convolution encoder alongside a policy network, which is known to have issues related to sample efficiency, degenerated feature representations, and catastrophic self-overfitting. However, this paradigm can lead to representations of the environment that are not aligned with the downstream task, which may result in suboptimal performances. To address this limitation, this paper proposes RLAD, the first Reinforcement Learning from Pixels (RLfP) method applied in the urban AD domain. We propose several techniques to enhance the performance of an RLfP algorithm in this domain, including: i) an image encoder that leverages both image augmentations and Adaptive Local Signal Mixing (A-LIX) layers; ii) WayConv1D, which is a waypoint encoder that harnesses the 2D geometrical information of
    
[^27]: 论经验风险最小化的方差、可允许性和稳定性

    On the Variance, Admissibility, and Stability of Empirical Risk Minimization. (arXiv:2305.18508v1 [math.ST])

    [http://arxiv.org/abs/2305.18508](http://arxiv.org/abs/2305.18508)

    本文指出，对于使用平方损失函数的经验风险最小化(ERM)，其次优性必须归因于大的偏差而非方差，并且在ERM的平方误差的偏差-方差分解中，方差项必然具有极小的失误率。作者还提供了Chatterjee的不可允许性定理的简单证明，并表示他们的估计表明ERM的稳定性。

    

    众所周知，使用平方损失的经验风险最小化可能会达到极小的最大失误率。本文的关键信息是，在温和的假设下，ERM的次优性必须归因于大的偏差而非方差。在ERM的平方误差的偏差-方差分解中，方差项必然具有极小的失误率。我们为固定设计提供了一个简单的、使用概率方法证明这一事实的证明。然后，我们在随机设计设置下为各种模型证明了这一结果。此外，我们提供了 Chatterjee 不可允许性定理 (Chatterjee, 2014, Theorem 1.4) 的简单证明，该定理指出，在固定设计设置中，ERM不能被排除为一种最优方法，并将该结果扩展到随机设计设置。我们还表明，我们的估计表明ERM的稳定性，为Caponnetto和Rakhlin(2006)的非Donsker类的主要结果提供了补充。

    It is well known that Empirical Risk Minimization (ERM) with squared loss may attain minimax suboptimal error rates (Birg\'e and Massart, 1993). The key message of this paper is that, under mild assumptions, the suboptimality of ERM must be due to large bias rather than variance. More precisely, in the bias-variance decomposition of the squared error of the ERM, the variance term necessarily enjoys the minimax rate. In the case of fixed design, we provide an elementary proof of this fact using the probabilistic method. Then, we prove this result for various models in the random design setting. In addition, we provide a simple proof of Chatterjee's admissibility theorem (Chatterjee, 2014, Theorem 1.4), which states that ERM cannot be ruled out as an optimal method, in the fixed design setting, and extend this result to the random design setting. We also show that our estimates imply stability of ERM, complementing the main result of Caponnetto and Rakhlin (2006) for non-Donsker classes.
    
[^28]: 宽残差网络的泛化能力

    Generalization Ability of Wide Residual Networks. (arXiv:2305.18506v1 [stat.ML])

    [http://arxiv.org/abs/2305.18506](http://arxiv.org/abs/2305.18506)

    本文研究了在$\mathbb{S}^{d-1}$上使用ReLU激活函数的宽残差网络的泛化能力，表明当宽度$m\rightarrow\infty$时，残差网络核(RNK)统一收敛到残差神经切向核(RNTK)，并且早停策略的宽残差网络可以达到极小化速率，但在训练过度拟合数据时无法很好地推广。

    

    本文研究了在$\mathbb{S}^{d-1}$上使用ReLU激活函数的宽残差网络的泛化能力。我们首先表明，当宽度$m\rightarrow\infty$时，残差网络核(RNK)统一收敛到残差神经切向核(RNTK)。这种统一收敛进一步保证了残差网络的泛化误差收敛于相对于RNTK的核回归的误差。作为直接推论，我们指出：$i$)如果目标回归函数落在与RNTK相关联的再生核希尔伯特空间(RKHS)中，采用早停策略的宽残差网络可以达到极小化速率；$ii$)如果训练到过度拟合数据，则无法很好地推广宽残差网络。最后，我们介绍一些实验来调和我们的理论结果与广泛观察到的“良性过拟合现象”之间的矛盾。

    In this paper, we study the generalization ability of the wide residual network on $\mathbb{S}^{d-1}$ with the ReLU activation function. We first show that as the width $m\rightarrow\infty$, the residual network kernel (RNK) uniformly converges to the residual neural tangent kernel (RNTK). This uniform convergence further guarantees that the generalization error of the residual network converges to that of the kernel regression with respect to the RNTK. As direct corollaries, we then show $i)$ the wide residual network with the early stopping strategy can achieve the minimax rate provided that the target regression function falls in the reproducing kernel Hilbert space (RKHS) associated with the RNTK; $ii)$ the wide residual network can not generalize well if it is trained till overfitting the data. We finally illustrate some experiments to reconcile the contradiction between our theoretical result and the widely observed ``benign overfitting phenomenon''
    
[^29]: 如何有效地在强化学习中进行人类反馈查询？

    How to Query Human Feedback Efficiently in RL?. (arXiv:2305.18505v1 [cs.LG])

    [http://arxiv.org/abs/2305.18505](http://arxiv.org/abs/2305.18505)

    该论文提出了一种针对强化学习中人类反馈查询的有效采样方法，以在最少的人类反馈下学习最佳策略，并可应用于具有线性参数化和未知过渡的偏好模型，并引入了基于行动比较反馈的RLHF。

    

    人类反馈强化学习（RLHF）是一种范例，在此范例下，RL代理学习使用对轨迹的成对优先级反馈来最优化任务，而不是使用明确的奖励信号。尽管RLHF在微调语言模型方面已经取得了实用成功，但现有的实证研究并未解决如何高效采样轨迹对以查询人类反馈的挑战。在本研究中，我们提出了一种有效的采样方法，用于获取探索性轨迹，在收集任何人类反馈之前，使学习隐藏的奖励函数更加准确。理论分析表明，与现有文献相比，我们的算法在线性参数化和未知过渡的基于偏好模型下学习最优策略所需的人类反馈更少。具体而言，我们的框架可以纳入线性和低秩MDPs。此外，我们研究了使用基于行动比较的反馈的RLHF，并介绍了一种高效的采样方法，以在优化具有有限反馈的任务时获得探索性轨迹。

    Reinforcement Learning with Human Feedback (RLHF) is a paradigm in which an RL agent learns to optimize a task using pair-wise preference-based feedback over trajectories, rather than explicit reward signals. While RLHF has demonstrated practical success in fine-tuning language models, existing empirical work does not address the challenge of how to efficiently sample trajectory pairs for querying human feedback. In this study, we propose an efficient sampling approach to acquiring exploratory trajectories that enable accurate learning of hidden reward functions before collecting any human feedback. Theoretical analysis demonstrates that our algorithm requires less human feedback for learning the optimal policy under preference-based models with linear parameterization and unknown transitions, compared to the existing literature. Specifically, our framework can incorporate linear and low-rank MDPs. Additionally, we investigate RLHF with action-based comparison feedback and introduce an
    
[^30]: 可配置公平性解决方案中的广义不同影响

    Generalized Disparate Impact for Configurable Fairness Solutions in ML. (arXiv:2305.18504v1 [cs.LG])

    [http://arxiv.org/abs/2305.18504](http://arxiv.org/abs/2305.18504)

    这篇论文在AI公平领域的连续保护属性中，提出了一族指标，相比于现有的HGR指标，在语义上更为补充，具有充分的可解释性和透明性，在有限样本下鲁棒，可配置以适应特定应用程序，并允许定义细粒度约束条件。

    

    我们在连续保护属性的AI公平领域做了两项贡献。首先，我们展示了Hirschfeld-Gebelein-Renyi（HGR）指标（目前仅适用于这种情况）是有价值的，但受到一些关键限制，包括语义、可解释性和鲁棒性。其次，我们介绍了一族指标，在语义上是HGR的补充；具有充分的可解释性和透明性；在有限样本下鲁棒；可配置以适应特定应用程序。我们的方法还允许我们定义细粒度约束条件，以允许某些类型的依赖性并有选择地禁止其他类型的依赖。通过扩展连续保护属性的可用选项，我们的方法对公平人工智能领域表示出了重要的贡献。

    We make two contributions in the field of AI fairness over continuous protected attributes. First, we show that the Hirschfeld-Gebelein-Renyi (HGR) indicator (the only one currently available for such a case) is valuable but subject to a few crucial limitations regarding semantics, interpretability, and robustness. Second, we introduce a family of indicators that are: 1) complementary to HGR in terms of semantics; 2) fully interpretable and transparent; 3) robust over finite samples; 4) configurable to suit specific applications. Our approach also allows us to define fine-grained constraints to permit certain types of dependence and forbid others selectively. By expanding the available options for continuous protected attributes, our approach represents a significant contribution to the area of fair artificial intelligence.
    
[^31]: 从对抗性竞争到以模型为中心的评价：推动一个统一的自动鲁棒性评估框架

    From Adversarial Arms Race to Model-centric Evaluation: Motivating a Unified Automatic Robustness Evaluation Framework. (arXiv:2305.18503v1 [cs.CL])

    [http://arxiv.org/abs/2305.18503](http://arxiv.org/abs/2305.18503)

    本文旨在建立一个统一的自动鲁棒性评估框架，向以模型为中心的评估转型，利用对抗攻击的优势。研究者们根据模型能力确定鲁棒性评估维度，并针对每个维度指定合理的算法来生成对抗样本。

    

    文本对抗攻击可以通过向输入添加语义保留但具有误导性的扰动来发现模型的弱点。自然语言处理（NLP）中长期的对抗性攻击和防御竞争是算法中心的，为自动鲁棒性评估提供了有价值的技术。但是，现有的鲁棒性评估实践可能存在评估不全面、评估协议不实用以及对抗样本失效等问题。本文旨在建立一个统一的自动鲁棒性评估框架，向以模型为中心的评估转型，进一步利用对抗攻击的优势。为了解决上述挑战，我们首先基于模型能力确定鲁棒性评估维度，并针对每个维度指定合理的算法来生成对抗样本。然后，我们建立了评估协议，包括评估设置和指标，以满足实际需求。最后，我们利用该框架在多个数据集上进行实验，并探讨了其在不同场景下的适用性和局限性。

    Textual adversarial attacks can discover models' weaknesses by adding semantic-preserved but misleading perturbations to the inputs. The long-lasting adversarial attack-and-defense arms race in Natural Language Processing (NLP) is algorithm-centric, providing valuable techniques for automatic robustness evaluation. However, the existing practice of robustness evaluation may exhibit issues of incomprehensive evaluation, impractical evaluation protocol, and invalid adversarial samples. In this paper, we aim to set up a unified automatic robustness evaluation framework, shifting towards model-centric evaluation to further exploit the advantages of adversarial attacks. To address the above challenges, we first determine robustness evaluation dimensions based on model capabilities and specify the reasonable algorithm to generate adversarial samples for each dimension. Then we establish the evaluation protocol, including evaluation settings and metrics, under realistic demands. Finally, we u
    
[^32]: 逃离平庸：双层神经网络如何在 SGD 下学习困难的单指标模型

    Escaping mediocrity: how two-layer networks learn hard single-index models with SGD. (arXiv:2305.18502v1 [stat.ML])

    [http://arxiv.org/abs/2305.18502](http://arxiv.org/abs/2305.18502)

    研究探讨在 SGD 下双层神经网络学习单指数目标函数的样本复杂度问题，发现过参数化只会增加一定因子的收敛性，不同维度和宽度的前置因子精确结果揭示。

    

    本研究探讨了在随机梯度下降（SGD）下双层神经网络学习单指数目标函数的样本复杂度问题，重点关注在初始化时存在许多平坦方向的挑战性情况。已经有研究表明，这种情况下通常需要 $n=O(d\log{d})$ 个样本。但是，我们提供了在高维度和不同宽度情况下的前置因子的精确结果。值得注意的是，我们的发现表明，在这个问题类中，过参数化只会增加一定因子的收敛性。这些见解基于 SGD 动态的低维度随机过程模型，其中逃离平庸等同于计算出站出时间。然而，我们证明这个过程的确定性近似足以代表逃逸时间，这意味着在这种情况下随机性的作用可能很小。

    This study explores the sample complexity for two-layer neural networks to learn a single-index target function under Stochastic Gradient Descent (SGD), focusing on the challenging regime where many flat directions are present at initialization. It is well-established that in this scenario $n=O(d\log{d})$ samples are typically needed. However, we provide precise results concerning the pre-factors in high-dimensional contexts and for varying widths. Notably, our findings suggest that overparameterization can only enhance convergence by a constant factor within this problem class. These insights are grounded in the reduction of SGD dynamics to a stochastic process in lower dimensions, where escaping mediocrity equates to calculating an exit time. Yet, we demonstrate that a deterministic approximation of this process adequately represents the escape time, implying that the role of stochasticity may be minimal in this scenario.
    
[^33]: DoMo-AC: 双重多步骤离策略Actor-Critic算法

    DoMo-AC: Doubly Multi-step Off-policy Actor-Critic Algorithm. (arXiv:2305.18501v1 [cs.LG])

    [http://arxiv.org/abs/2305.18501](http://arxiv.org/abs/2305.18501)

    双重多步骤离策略Actor-Critic算法能够通过结合多步骤策略改进和策略评估来促进最优控制，提高策略梯度估计，并在一般的离策略学习设置中有效。

    

    多步骤学习在策略评估中具有前瞻性，但在最优控制情况下，多步骤学习的影响相对有限，原因是多步骤策略改进需要的操作无法用随机样本来近似，阻碍了这种方法在实践中的广泛应用。为了解决这些限制，我们引入了双重多步骤离策略VI(DoMo-VI)，这是一种新颖的Oracle算法，它结合了多步骤策略改进和策略评估。DoMo-VI保证收敛速度加速到最优策略，并适用于一般的离策略学习设置。然后我们提出了双重多步离策略actor-critic(DoMo-AC)，这是一个实际的DoMo-VI算法实例。DoMo-AC引入了偏差-方差权衡，以确保改进的策略梯度估计。当与...

    Multi-step learning applies lookahead over multiple time steps and has proved valuable in policy evaluation settings. However, in the optimal control case, the impact of multi-step learning has been relatively limited despite a number of prior efforts. Fundamentally, this might be because multi-step policy improvements require operations that cannot be approximated by stochastic samples, hence hindering the widespread adoption of such methods in practice. To address such limitations, we introduce doubly multi-step off-policy VI (DoMo-VI), a novel oracle algorithm that combines multi-step policy improvements and policy evaluations. DoMo-VI enjoys guaranteed convergence speed-up to the optimal policy and is applicable in general off-policy learning settings. We then propose doubly multi-step off-policy actor-critic (DoMo-AC), a practical instantiation of the DoMo-VI algorithm. DoMo-AC introduces a bias-variance trade-off that ensures improved policy gradient estimates. When combined with
    
[^34]: VAST：一种视听字幕文本全模态基础模型与数据集

    VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset. (arXiv:2305.18500v1 [cs.CV])

    [http://arxiv.org/abs/2305.18500](http://arxiv.org/abs/2305.18500)

    本文提出了一种全模态视频文本基础模型VAST及其数据集VAST-27M。该模型可以感知和处理视频中的视觉、音频和字幕模态，通过自动集成多模态字幕和资源，提供更好的支持文本关联的多种任务。

    

    当代视频文本基础模型已经完全探索了视觉和文本，而其他模态，如视频中的音频和字幕，却没有得到足够的关注。本文旨在通过探索自动生成的大规模全模态视频字幕数据集VAST-27M，建立多模态视频轨迹之间的连接，包括视觉、音频和字幕，并与文本进行关联。具体而言，我们首先收集了2700万个开放领域视频片段，并分别训练视觉和音频字幕生成器以生成视觉和音频字幕。然后，我们使用一个现有的大语言模型（LLM）将生成的字幕、字幕和指导提示集成到全模态字幕中。基于提出的VAST-27M数据集，我们训练了一种全模态视频文本基础模型VAST，它可以感知和处理视频中的视觉、音频和字幕模态，并更好地支持各种任务，包括视觉和文本之间的关联。

    Vision and text have been fully explored in contemporary video-text foundational models, while other modalities such as audio and subtitles in videos have not received sufficient attention. In this paper, we resort to establish connections between multi-modality video tracks, including Vision, Audio, and Subtitle, and Text by exploring an automatically generated large-scale omni-modality video caption dataset called VAST-27M. Specifically, we first collect 27 million open-domain video clips and separately train a vision and an audio captioner to generate vision and audio captions. Then, we employ an off-the-shelf Large Language Model (LLM) to integrate the generated captions, together with subtitles and instructional prompts into omni-modality captions. Based on the proposed VAST-27M dataset, we train an omni-modality video-text foundational model named VAST, which can perceive and process vision, audio, and subtitle modalities from video, and better support various tasks including vis
    
[^35]: 基于野外视频预训练的上下文化世界模型用于强化学习

    Pre-training Contextualized World Models with In-the-wild Videos for Reinforcement Learning. (arXiv:2305.18499v1 [cs.CV])

    [http://arxiv.org/abs/2305.18499](http://arxiv.org/abs/2305.18499)

    本文研究了基于野外视频预训练的上下文化世界模型（ContextWM）用于强化学习。ContextWM采用上下文化扩展的潜在动态模型建模，从而可以更好地泛化不同场景之间的知识转移，提高下游视觉控制任务的样本效率和控制性能。

    

    利用大规模多样化的数据集进行的无监督预训练已在各种领域取得了巨大成功。最近的工作调查了这种无监督预训练方法在基于模型的强化学习（MBRL）中的应用，但仅限于特定领域或模拟数据。本文研究了使用大量野外视频进行预训练世界模型，以高效学习下游视觉控制任务的问题。然而，野外视频存在各种上下文因素，如错综复杂的背景和纹理外观，这使得世界模型无法提取共享的世界知识以更好地泛化。为了解决这个问题，我们引入了上下文化世界模型（ContextWM），显式地对上下文和动态进行建模，以克服野外视频的复杂性和多样性，并促进不同场景之间的知识转移。具体而言，使用上下文化扩展的潜在动态模型来捕捉高级状态和低级观察之间的上下文依赖关系。通过使用野外视频对ContextWM进行预训练，我们展示了与从头开始训练相比，下游视觉控制任务的样本效率和控制性能均得到了显着提高。

    Unsupervised pre-training methods utilizing large and diverse datasets have achieved tremendous success across a range of domains. Recent work has investigated such unsupervised pre-training methods for model-based reinforcement learning (MBRL) but is limited to domain-specific or simulated data. In this paper, we study the problem of pre-training world models with abundant in-the-wild videos for efficient learning of downstream visual control tasks. However, in-the-wild videos are complicated with various contextual factors, such as intricate backgrounds and textured appearance, which precludes a world model from extracting shared world knowledge to generalize better. To tackle this issue, we introduce Contextualized World Models (ContextWM) that explicitly model both the context and dynamics to overcome the complexity and diversity of in-the-wild videos and facilitate knowledge transfer between distinct scenes. Specifically, a contextualized extension of the latent dynamics model is 
    
[^36]: ANPL：使用交互式分解编译自然程序

    ANPL: Compiling Natural Programs with Interactive Decomposition. (arXiv:2305.18498v1 [cs.PL])

    [http://arxiv.org/abs/2305.18498](http://arxiv.org/abs/2305.18498)

    ANPL是一个编程系统，可以让用户直接操作草图，使用自然语言描述注释模块或孔，并生成一个有机的Python程序，它优于基线。

    

    大型语言模型的出现在通过自然交互增强编程方面显示出了潜力。然而，虽然大型语言模型擅长将常见的使用模式编译为编程语言，例如Python，但如何编辑和调试由大型语言模型生成的程序仍然是一个挑战。我们介绍了ANPL，一种编程系统，允许用户分解特定于用户的任务。在ANPL程序中，用户可以直接操作草图，该草图指定生成的程序的数据流。用户使用自然语言描述注释模块或孔，将生成功能的昂贵任务卸载到大型语言模型中。给定一个ANPL程序，ANPL编译器会生成一个有机的Python程序，实现孔中的功能，并遵守草图中指定的数据流。我们将ANPL部署在抽象和推理语料库（ARC）上，它是一组对于最先进的AI系统而言具有挑战性的独特任务，结果表明它优于基线。

    The advents of Large Language Models (LLMs) have shown promise in augmenting programming using natural interactions. However, while LLMs are proficient in compiling common usage patterns into a programming language, e.g., Python, it remains a challenge how to edit and debug an LLM-generated program. We introduce ANPL, a programming system that allows users to decompose user-specific tasks. In an ANPL program, a user can directly manipulate sketch, which specifies the data flow of the generated program. The user annotates the modules, or hole with natural language descriptions offloading the expensive task of generating functionalities to the LLM. Given an ANPL program, the ANPL compiler generates a cohesive Python program that implements the functionalities in hole, while respecting the dataflows specified in sketch. We deploy ANPL on the Abstraction and Reasoning Corpus (ARC), a set of unique tasks that are challenging for state-of-the-art AI systems, showing it outperforms baseline p
    
[^37]: 子采样与岭回归的广义等价性研究

    Generalized equivalences between subsampling and ridge regularization. (arXiv:2305.18496v1 [math.ST])

    [http://arxiv.org/abs/2305.18496](http://arxiv.org/abs/2305.18496)

    此篇论文研究了举集岭估计器中子采样和岭回归之间的等价性，发现二者在一定路径中是渐近等价的，并提出了数据相关的方法确定等价路径，间接解决了岭回归调优中预测风险单调性的影响因素问题。

    

    我们针对举集岭估计器，建立了子采样和岭回归之间的精确结构和风险等价性。具体而言，我们证明了，当用不同的岭正则化水平$\lambda$和子采样比例$\psi$拟合子样岭估计器的线性和二次泛函，在$(\lambda,\psi)$-平面上沿着特定路径渐近等价（其中$\psi$是特征维度与子采样大小的比率）。我们的结果仅要求特征和响应分布具有有界矩，并允许任意联合分布。此外，我们提供了一种数据相关的方法来确定$(\lambda,\psi)$的等价路径。我们结果的间接含义是，在数据方面比例中，调优的岭回归呈现出单调预测风险。这解决了Nakkiran等人提出的一个近期未解决的开放性问题，在一般数据分布和温和的正则条件下。

    We establish precise structural and risk equivalences between subsampling and ridge regularization for ensemble ridge estimators. Specifically, we prove that linear and quadratic functionals of subsample ridge estimators, when fitted with different ridge regularization levels $\lambda$ and subsample aspect ratios $\psi$, are asymptotically equivalent along specific paths in the $(\lambda, \psi )$-plane (where $\psi$ is the ratio of the feature dimension to the subsample size). Our results only require bounded moment assumptions on feature and response distributions and allow for arbitrary joint distributions. Furthermore, we provide a datadependent method to determine the equivalent paths of $(\lambda, \psi )$. An indirect implication of our equivalences is that optimally-tuned ridge regression exhibits a monotonic prediction risk in the data aspect ratio. This resolves a recent open problem raised by Nakkiran et al. under general data distributions and a mild regularity condition that
    
[^38]: 提高Ex-Situ神经网络传输到被动TiO2 ReRAM十字架上的鲁棒性的硬件感知训练技术

    Hardware-aware Training Techniques for Improving Robustness of Ex-Situ Neural Network Transfer onto Passive TiO2 ReRAM Crossbars. (arXiv:2305.18495v1 [cs.AR])

    [http://arxiv.org/abs/2305.18495](http://arxiv.org/abs/2305.18495)

    本文提出了一种通过应用dropout、重新参数化技巧和正则化等训练方法，提高被动TiO2 ReRAM交叉棒硬件传输神经网络权重的精度的方案。

    

    被动电阻式随机存取存储器（ReRAM）十字架阵列是一种有前途的新兴技术，用于模拟矩阵向量乘法，与主动电阻式晶体管与电阻器的组合（1T1R）相比具有更高的集成密度。然而，由于硬件变异性，如偷跑路径电流、偏置方案效应和电导调谐不准确等，当前将神经网络权重传输到十字架结构中存储器设备的电导态时精度有显著损失。本文提出了将dropout、重新参数化技巧和正则化等方法用于适应TiO2交叉棒的变异性的训练方法，从而生成更适合其硬件传输的模型。通过使用斯坦福大学的硬件模拟器，在数千次权重传输中比较所提出的硬件感知网络和常规全连接网络的输出和精度来演示这种方法的可行性。

    Passive resistive random access memory (ReRAM) crossbar arrays, a promising emerging technology used for analog matrix-vector multiplications, are far superior to their active (1T1R) counterparts in terms of the integration density. However, current transfers of neural network weights into the conductance state of the memory devices in the crossbar architecture are accompanied by significant losses in precision due to hardware variabilities such as sneak path currents, biasing scheme effects and conductance tuning imprecision. In this work, training approaches that adapt techniques such as dropout, the reparametrization trick and regularization to TiO2 crossbar variabilities are proposed in order to generate models that are better adapted to their hardware transfers. The viability of this approach is demonstrated by comparing the outputs and precision of the proposed hardware-aware network with those of a regular fully connected network over a few thousand weight transfers using the ha
    
[^39]: 学习稀疏检索在长文档中的应用

    Adapting Learned Sparse Retrieval for Long Documents. (arXiv:2305.18494v1 [cs.IR])

    [http://arxiv.org/abs/2305.18494](http://arxiv.org/abs/2305.18494)

    本文提出了两种方法，即 ExactSDM 和 SoftSDM，将顺序依赖模型 (SDM) 适应到学习稀疏检索 (LSR) 中，以解决长篇文档检索的问题。在 MSMARCO 文档和 TREC Robust04 数据集上的实验证明，ExactSDM 和 SoftSDM 都优于现有的 LSR 聚合方法。

    

    学习稀疏检索 (LSR) 是一种将查询和文档转换成与词汇表对齐的稀疏权重向量的神经检索方法。虽然像 Splade 这样的 LSR 方法在短文段上表现良好，但它们如何处理更长的文档还不清楚。本文研究了用于适应 LSR 到长文档的现有聚合方法，并发现接近打分对于 LSR 处理长文档是至关重要的。为了利用这个特性，我们提出了两种将顺序依赖模型 (SDM) 适应到 LSR 的方法：ExactSDM 和 SoftSDM。ExactSDM 假定只有精确的查询项依赖性，而 SoftSDM 使用潜在函数对查询项及其扩展项 (即使用转换器的掩码语言建模头识别的项) 的依赖关系进行建模。在 MSMARCO 文档和 TREC Robust04 数据集上的实验证明，ExactSDM 和 SoftSDM 都优于现有的 LSR 聚合方法，针对不同的文档长度约束表现出最佳的性能。

    Learned sparse retrieval (LSR) is a family of neural retrieval methods that transform queries and documents into sparse weight vectors aligned with a vocabulary. While LSR approaches like Splade work well for short passages, it is unclear how well they handle longer documents. We investigate existing aggregation approaches for adapting LSR to longer documents and find that proximal scoring is crucial for LSR to handle long documents. To leverage this property, we proposed two adaptations of the Sequential Dependence Model (SDM) to LSR: ExactSDM and SoftSDM. ExactSDM assumes only exact query term dependence, while SoftSDM uses potential functions that model the dependence of query terms and their expansion terms (i.e., terms identified using a transformer's masked language modeling head).  Experiments on the MSMARCO Document and TREC Robust04 datasets demonstrate that both ExactSDM and SoftSDM outperform existing LSR aggregation approaches for different document length constraints. Surp
    
[^40]: 基于流导向纳米定位的设计空间探索的见解

    Insights from the Design Space Exploration of Flow-Guided Nanoscale Localization. (arXiv:2305.18493v1 [cs.NI])

    [http://arxiv.org/abs/2305.18493](http://arxiv.org/abs/2305.18493)

    研究了基于流导向纳米定位的设计空间，考虑了能源和信号衰减等因素，为这一新兴领域提供了有希望的解决方案。

    

    具有太赫兹无线通信能力的纳米设备为在人类血液中进行流导向定位提供了基础。此类定位使得将所感受到的事件的位置与事件本身进行匹配成为可能，从而实现了精准医疗方面的早期和精准诊断、降低成本和侵入性。流导向定位仍处于原始阶段，只有少数论文涉及此问题。尽管如此，所提出解决方案的性能评估仍然以非标准化的方式进行，通常只考虑单一的性能指标，并忽略了在这种规模（例如，纳米器件的能量受限）和对于这种具有挑战性的环境（例如，体内太赫兹传播的严重衰减）下相关的各个方面。因此，这些评估具有低水平的真实性，并且无法以客观的方式进行比较。为了解决这个问题，我们考虑了传输能量消耗和信号衰减，对流导向纳米定位的设计空间进行了探索。我们的分析考虑了各种性能指标（例如能量消耗和定位精度）和挑战（例如身体运动和血压），导致我们可以为这个新兴领域提供有希望的解决方案。

    Nanodevices with Terahertz (THz)-based wireless communication capabilities are providing a primer for flow-guided localization within the human bloodstreams. Such localization is allowing for assigning the locations of sensed events with the events themselves, providing benefits in precision medicine along the lines of early and precise diagnostics, and reduced costs and invasiveness. Flow-guided localization is still in a rudimentary phase, with only a handful of works targeting the problem. Nonetheless, the performance assessments of the proposed solutions are already carried out in a non-standardized way, usually along a single performance metric, and ignoring various aspects that are relevant at such a scale (e.g., nanodevices' limited energy) and for such a challenging environment (e.g., extreme attenuation of in-body THz propagation). As such, these assessments feature low levels of realism and cannot be compared in an objective way. Toward addressing this issue, we account for t
    
[^41]: DMS：基于侧信息的无需知道类别数和距离度量的聚类算法

    DMS: Differentiable Mean Shift for Dataset Agnostic Task Specific Clustering Using Side Information. (arXiv:2305.18492v1 [cs.LG])

    [http://arxiv.org/abs/2305.18492](http://arxiv.org/abs/2305.18492)

    本研究提出了一种基于少量成对样例的侧信息直接学习数据聚类的方法DMS，与以往方法不同，我们无需知道类别数、类中心或者任何相似的距离度量，该方法可以根据侧信息的任务需求将相同的数据点分成不同的聚类，且在固有的和非固有的数据集任务上表现优异。

    

    本研究提出了一种新的聚类方法，通过少量成对样例的侧信息直接学习数据聚类。与以往方法不同，我们无需知道类别数、类中心或者任何相似的距离度量。该方法可以根据侧信息的任务需求将相同的数据点分成不同的聚类。受均值漂移算法启发，我们使用一种自定义的迭代神经网络来实现我们的聚类方法——Differentiable Mean Shift (DMS)，一种最先进的数据集无关聚类方法。我们发现可以训练一个强大的聚类定义，而不必强制要求每个簇在训练期间呈现。DMS在固有的和非固有的数据集任务上表现优异。

    We present a novel approach, in which we learn to cluster data directly from side information, in the form of a small set of pairwise examples. Unlike previous methods, with or without side information, we do not need to know the number of clusters, their centers or any kind of distance metric for similarity. Our method is able to divide the same data points in various ways dependant on the needs of a specific task, defined by the side information. Contrastingly, other work generally finds only the intrinsic, most obvious, clusters. Inspired by the mean shift algorithm, we implement our new clustering approach using a custom iterative neural network to create Differentiable Mean Shift (DMS), a state of the art, dataset agnostic, clustering method. We found that it was possible to train a strong cluster definition without enforcing a constraint that each cluster must be presented during training. DMS outperforms current methods in both the intrinsic and non-intrinsic dataset tasks.
    
[^42]: TD学习中对表示动态的更好理解

    Towards a Better Understanding of Representation Dynamics under TD-learning. (arXiv:2305.18491v1 [cs.LG])

    [http://arxiv.org/abs/2305.18491](http://arxiv.org/abs/2305.18491)

    本文研究了TD学习中对表示动态的影响，并发现端到端TD学习在环境可逆的情况下可以严格降低值逼近误差，在环境进一步假设的情况下，我们可以将表示动态连接到转移矩阵的谱分解。从随机生成的奖励中适合多个值函数作为表示学习的有用辅助任务。

    

    TD学习是值预测的基础强化学习算法。值预测的准确性与状态表示的质量密切相关。本文考虑一个问题：端到端TD学习如何随时间影响表示？本文提供了一系列分析，进一步阐明了TD学习下的表示动态。在环境可逆的情况下，我们首先证明端到端TD学习可严格降低时间上的值逼近误差。在环境进一步的假设下，我们可以将表示动态连接到转移矩阵的谱分解。该发现证实了从随机生成的奖励中适合多个值函数作为表示学习的有用辅助任务，我们在表格和Atari游戏套件上进行了实证验证。

    TD-learning is a foundation reinforcement learning (RL) algorithm for value prediction. Critical to the accuracy of value predictions is the quality of state representations. In this work, we consider the question: how does end-to-end TD-learning impact the representation over time? Complementary to prior work, we provide a set of analysis that sheds further light on the representation dynamics under TD-learning. We first show that when the environments are reversible, end-to-end TD-learning strictly decreases the value approximation error over time. Under further assumptions on the environments, we can connect the representation dynamics with spectral decomposition over the transition matrix. This latter finding establishes fitting multiple value functions from randomly generated rewards as a useful auxiliary task for representation learning, as we empirically validate on both tabular and Atari game suites.
    
[^43]: 基于锐度调整的有效参数数量梯度下降SANE算法

    SANE: The phases of gradient descent through Sharpness Adjusted Number of Effective parameters. (arXiv:2305.18490v1 [cs.LG])

    [http://arxiv.org/abs/2305.18490](http://arxiv.org/abs/2305.18490)

    本文提出了一种基于锐度调整的的有效参数数量梯度下降SANE算法，用于解质量的有效维数度量，并且对大学习率也有较好的鲁棒性。

    

    现代神经网络非常成功。许多研究已经调查了损失面曲率如何影响解的质量。本文考虑神经网络训练期间的Hessian矩阵。我们重申了“确定良好”或“有效”参数的数量与神经网络泛化性能之间的联系，并将其演示为模型比较工具。通过考虑局部曲率，我们提出了Sharpness Adjusted Number of Effective parameters (SANE)算法，这是一种针对解质量的有效维数度量。我们表明，SANE对大学习率具有鲁棒性，这代表了有吸引力但声名狼藉的不稳定学习区域。我们提供证据并表征了大学习率下“损失盆地”的Hessian矩阵变化。最后，扩展我们的分析到更深的神经网络中，我们提供了对全网络Hessian矩阵的近似，利用神经元自然排序。

    Modern neural networks are undeniably successful. Numerous studies have investigated how the curvature of loss landscapes can affect the quality of solutions. In this work we consider the Hessian matrix during network training. We reiterate the connection between the number of "well-determined" or "effective" parameters and the generalisation performance of neural nets, and we demonstrate its use as a tool for model comparison. By considering the local curvature, we propose Sharpness Adjusted Number of Effective parameters (SANE), a measure of effective dimensionality for the quality of solutions. We show that SANE is robust to large learning rates, which represent learning regimes that are attractive but (in)famously unstable. We provide evidence and characterise the Hessian shifts across "loss basins" at large learning rates. Finally, extending our analysis to deeper neural networks, we provide an approximation to the full-network Hessian, exploiting the natural ordering of neural we
    
[^44]: 一个可解释的迁移学习方案用于检测智能手机图片中的猴痘病毒

    A Transfer Learning and Explainable Solution to Detect mpox from Smartphones images. (arXiv:2305.18489v1 [eess.IV])

    [http://arxiv.org/abs/2305.18489](http://arxiv.org/abs/2305.18489)

    该研究提出了一种使用迁移学习和可解释的方法，以从智能手机图像中检测猴痘病毒，可以帮助低收入国家和不具备相应疫苗和检测设施的区域进行初步筛查。

    

    最近几个月，原本仅流行于有限世界范围内的猴痘病毒（mpox）开始在多个国家传播，最终被世界卫生组织宣布为“国际关注的公共卫生紧急事件”。由于多个国家存在持续的病例和可能的新爆发，该警报于2023年2月得以更新。特别是那些缺乏疫苗和检测设施的低收入国家，面临着极大的风险。猴痘病毒感染的症状之一是皮疹和爆发的出现，这可能会促使人们寻求医疗建议。一种可能帮助根据皮损外观进行初步筛查的技术是使用机器学习进行图像分类，但要使这种技术适用于大规模使用，它应该可以直接在人们的移动设备上使用，并且能通知远程医疗专家。在这项工作中，我们提出了一种使用迁移学习和可解释的方法，以从智能手机图像中检测猴痘病毒。

    In recent months, the monkeypox (mpox) virus -- previously endemic in a limited area of the world -- has started spreading in multiple countries until being declared a ``public health emergency of international concern'' by the World Health Organization. The alert was renewed in February 2023 due to a persisting sustained incidence of the virus in several countries and worries about possible new outbreaks. Low-income countries with inadequate infrastructures for vaccine and testing administration are particularly at risk.  A symptom of mpox infection is the appearance of skin rashes and eruptions, which can drive people to seek medical advice. A technology that might help perform a preliminary screening based on the aspect of skin lesions is the use of Machine Learning for image classification. However, to make this technology suitable on a large scale, it should be usable directly on mobile devices of people, with a possible notification to a remote medical expert.  In this work, we i
    
[^45]: 一种自适应后验集中的贝叶斯稀疏因子模型

    A Bayesian sparse factor model with adaptive posterior concentration. (arXiv:2305.18488v1 [stat.ML])

    [http://arxiv.org/abs/2305.18488](http://arxiv.org/abs/2305.18488)

    本文提出了一种自适应后验集中的贝叶斯稀疏因子模型，可以推断因子维数和加载矩阵的稀疏结构，同时保持计算可行性，并获得了优越的性能表现。

    

    本文提出了一种新的贝叶斯推断方法，用于高维稀疏因子模型的推断，既可以推断因子维数，又可以推断加载矩阵的稀疏结构。其创新之处在于引入了一定的依赖关系，使得稀疏水平和因子维数进行自适应后验集中，同时保持计算可行性。我们证明后验分布在渐近意义下集中于真实因子维度，更重要的是，这种后验一致性会随着真实加载矩阵的稀疏水平和噪声方差而自适应。此外，我们还证明了这种方法在更一般的情况下达到了因子维数的最优检测率。同时，我们还获得了近乎最优的协方差矩阵后验集中速率。我们进行了数值实验，并展示了与现有方法的比较结果。

    In this paper, we propose a new Bayesian inference method for a high-dimensional sparse factor model that allows both the factor dimensionality and the sparse structure of the loading matrix to be inferred. The novelty is to introduce a certain dependence between the sparsity level and the factor dimensionality, which leads to adaptive posterior concentration while keeping computational tractability. We show that the posterior distribution asymptotically concentrates on the true factor dimensionality, and more importantly, this posterior consistency is adaptive to the sparsity level of the true loading matrix and the noise variance. We also prove that the proposed Bayesian model attains the optimal detection rate of the factor dimensionality in a more general situation than those found in the literature. Moreover, we obtain a near-optimal posterior concentration rate of the covariance matrix. Numerical studies are conducted and show the superiority of the proposed method compared with 
    
[^46]: 太阳辐照度预测变压器

    Solar Irradiance Anticipative Transformer. (arXiv:2305.18487v1 [cs.CV])

    [http://arxiv.org/abs/2305.18487](http://arxiv.org/abs/2305.18487)

    本文提出了一种预测太阳辐照度的模型，通过学习天空图像的相关特征以及天空图像之间的长距离依赖，对未来的辐照度进行了有效的预测，预测准确率比智能持续模型高出21.45％。

    

    本文提出了一种基于变压器的模型，用于短期太阳辐照度预测。给定一系列天空图像，我们的模型可以编码连续图像的特征，并馈入变压器解码器中，以预测未来未见过的天空图像相关的辐照度值。我们表明，我们的模型可以有效地只关注与辐照度预测相关的特征图像。此外，所提出的预测变压器可以捕捉天空图像之间的长距离依赖，比智能持续模型在一个新引入的全天空图像数据集的15分钟预测中达到了21.45％的预测准确率。

    This paper proposes an anticipative transformer-based model for short-term solar irradiance forecasting. Given a sequence of sky images, our proposed vision transformer encodes features of consecutive images, feeding into a transformer decoder to predict irradiance values associated with future unseen sky images. We show that our model effectively learns to attend only to relevant features in images in order to forecast irradiance. Moreover, the proposed anticipative transformer captures long-range dependencies between sky images to achieve a forecasting skill of 21.45 % on a 15 minute ahead prediction for a newly introduced dataset of all-sky images when compared to a smart persistence model.
    
[^47]: 基准数据集上 ChatGPT 的系统研究和全面评估

    A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. (arXiv:2305.18486v1 [cs.CL])

    [http://arxiv.org/abs/2305.18486](http://arxiv.org/abs/2305.18486)

    本文对基准数据集上 ChatGPT 的性能进行了全面的评估，包括问答、文本摘要、代码生成、常识推理、数学问题求解、机器翻译、偏见检测和伦理考虑等任务。研究旨在验证 ChatGPT 的优势和弱点，并为使用语言模型的未来研究提供见解。

    

    最近，如 ChatGPT 这样的大型语言模型（LLM）的开发引起了很多关注。然而，由于难以将该模型生成的产出与基本事实进行比较，因此其在基准学术数据集上的评估仍未充分探索。本文旨在对 ChatGPT 在包括问答、文本摘要、代码生成、常识推理、数学问题求解、机器翻译、偏见检测和伦理考虑等任务中的表现进行彻底评估。具体而言，我们在 140 个任务中评估了 ChatGPT，并分析了其在这些数据集中生成的 255K 次响应，这使我们的工作成为了在 NLP 基准测试中对 ChatGPT 进行的最大评估。简而言之，我们的研究旨在验证 ChatGPT 在各种任务中的优势和弱点，并为使用 LLM 的未来研究提供见解。我们还报告了一种新的迸发能力，即遵循多个查询指令。

    The development of large language models (LLMs) such as ChatGPT has brought a lot of attention recently. However, their evaluation in the benchmark academic datasets remains under-explored due to the difficulty of evaluating the generative outputs produced by this model against the ground truth. In this paper, we aim to present a thorough evaluation of ChatGPT's performance on diverse academic datasets, covering tasks like question-answering, text summarization, code generation, commonsense reasoning, mathematical problem-solving, machine translation, bias detection, and ethical considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze 255K responses it generates in these datasets. This makes our work the largest evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate the strengths and weaknesses of ChatGPT in various tasks and provide insights for future research using LLMs. We also report a new emergent ability to follow multi-query instruct
    
[^48]: 基于自编码器的条件神经过程用于表示学习

    Autoencoding Conditional Neural Processes for Representation Learning. (arXiv:2305.18485v1 [cs.LG])

    [http://arxiv.org/abs/2305.18485](http://arxiv.org/abs/2305.18485)

    本文提出了部分像素空间变分自编码器，结合了自编码器与条件神经过程，可以学习到一系列基本物理和文化概念的表示，并且可以提高上下文预测的准确性。

    

    条件神经过程(CNPs)是一种灵活高效的模型族群，可以从观测值中学习出一个随机过程。在视觉领域中，CNPs 在上下文图像补全中得到了特别的应用，即通过观察某些位置的像素值来预测其他未观察位置上的值的分布。然而，学习这样一个 CNP 的像素选择通常是随机的或者是通过一个简单的统计量(例如像素方差)导出的。本文将问题转变一下：一个 CNP 想要观察哪些像素？也就是说，哪些像素允许拟合 CNP，这样的像素能告诉我们一些关于潜在图像的信息吗？将提供给 CNP 的上下文视为固定大小的潜在表示，我们构建了一个一次性变分框架，部分像素空间变分自编码器(Partical Pixel Space VAE, PPS-VAE)，同时预测这个上下文，并学习一个 CNP。我们在一组视觉数据集上评估了 PPS-VAE，发现通过相对大小或变化预测像素的选择可以安排学习，且更准确地进行了上下文预测，并且可以对基本物理和文化概念进行有意义的表示。

    Conditional neural processes (CNPs) are a flexible and efficient family of models that learn to learn a stochastic process from observations. In the visual domain, they have seen particular application in contextual image completion - observing pixel values at some locations to predict a distribution over values at other unobserved locations. However, the choice of pixels in learning such a CNP is typically either random or derived from a simple statistical measure (e.g. pixel variance). Here, we turn the problem on its head and ask: which pixels would a CNP like to observe? That is, which pixels allow fitting CNP, and do such pixels tell us something about the underlying image? Viewing the context provided to the CNP as fixed-size latent representations, we construct an amortised variational framework, Partial Pixel Space Variational Autoencoder (PPS-VAE), for predicting this context simultaneously with learning a CNP. We evaluate PPS-VAE on a set of vision datasets, and find that not
    
[^49]: 神经傅里叶变换：等变表示学习的通用方法

    Neural Fourier Transform: A General Approach to Equivariant Representation Learning. (arXiv:2305.18484v1 [stat.ML])

    [http://arxiv.org/abs/2305.18484](http://arxiv.org/abs/2305.18484)

    神经傅里叶变换是一种通用的等变表示学习方法，它可以在不需要显式知识的情况下学习组的潜在线性作用，实现对数据隐藏结构的提取。

    

    对称学习已被证明是提取数据隐藏结构的有效方法，其中等变关系概念起着中心作用。然而，大多数当前研究都建立在建筑理论和对数据形式的相应假设之上。我们提出了神经傅里叶变换（NFT），这是一种学习组的潜在线性作用的通用框架，而无需假设关于组如何作用于数据的显式知识。我们展示了NFT的理论基础，并表明等变特征的存在，即在等变性学习中普遍假定的，等价于数据空间中存在一组不变核。我们还提供实验结果，演示了在具有不同程度的关于操作组的知识的典型场景中应用NFT的应用。

    Symmetry learning has proven to be an effective approach for extracting the hidden structure of data, with the concept of equivariance relation playing the central role. However, most of the current studies are built on architectural theory and corresponding assumptions on the form of data. We propose Neural Fourier Transform (NFT), a general framework of learning the latent linear action of the group without assuming explicit knowledge of how the group acts on data. We present the theoretical foundations of NFT and show that the existence of a linear equivariant feature, which has been assumed ubiquitously in equivariance learning, is equivalent to the existence of a group invariant kernel on the dataspace. We also provide experimental results to demonstrate the application of NFT in typical scenarios with varying levels of knowledge about the acting group.
    
[^50]: 将正则化最优输运带入高速：适用于GPU的分裂方法

    Bringing regularized optimal transport to lightspeed: a splitting method adapted for GPUs. (arXiv:2305.18483v1 [cs.LG])

    [http://arxiv.org/abs/2305.18483](http://arxiv.org/abs/2305.18483)

    该论文提出了一种高效的算法，使用分裂技术解决正则化最优输运问题，具有全局收敛性保证和GPU并行化优势，适用于多种应用领域。

    

    我们提出了一种高效的正则化最优输运算法。与以前的方法不同，我们使用Douglas-Rachford分裂技术开发了一个高效的求解器，可以处理广泛的正则化器。该算法具有强大的全局收敛性保证，低迭代成本，并且可以利用GPU并行化，使其在许多问题上比现有技术快得多。我们在几个应用中阐述了其竞争力，包括领域自适应和生成模型的学习。

    We present an efficient algorithm for regularized optimal transport. In contrast to previous methods, we use the Douglas-Rachford splitting technique to develop an efficient solver that can handle a broad class of regularizers. The algorithm has strong global convergence guarantees, low per-iteration cost, and can exploit GPU parallelization, making it considerably faster than the state-of-the-art for many problems. We illustrate its competitiveness in several applications, including domain adaptation and learning of generative models.
    
[^51]: 面向上下装的时尚物体检测

    Fashion Object Detection for Tops & Bottoms. (arXiv:2305.18482v1 [cs.CV])

    [http://arxiv.org/abs/2305.18482](http://arxiv.org/abs/2305.18482)

    本文提出了一种面向上下装的时尚物体检测的流程，可以自动区分图像中的上衣和下衣。该方法采用了能够自动识别人体部位的模型，通过选择最佳性能的基于锚点和无锚点的物体检测算法，在速度和准确性方面优于最先进的方法。

    

    时尚是世界上最大的行业之一，计算机视觉技术近年来变得越来越流行，特别是对于物体检测和服装分割等任务。尽管计算机视觉解决方案在时尚行业中快速增长，但许多问题还远未得到解决。因此，使用开箱即用的预训练计算机视觉模型不一定能提供期望的解决方案。本文提出了一种流程，它可以接受含噪音的带有人物的图像，并特别检测属于上装或下装的服装区域。我们的解决方案实现了可以在图像中找到人体部位的模型，例如全身对半身，或者根本没有找到人体。然后，其他模型通过知道有人和其组成（例如，我们并不总是有一个完整的人体）来找到图像的边界框/区域，这些框很可能对应于下装或上装。对于边界框/区域的创建，我们评估了基于锚点和无锚点的物体检测算法，并选择了最佳性能的算法。我们在Polyvore数据集上进行了实验，并将结果与最先进的方法进行了比较，结果表明，所提出的方法在准确性和速度方面优于它们。

    Fashion is one of the largest world's industries and computer vision techniques have been becoming more popular in recent years, in particular, for tasks such as object detection and apparel segmentation. Even with the rapid growth in computer vision solutions, specifically for the fashion industry, many problems are far for being resolved. Therefore, not at all times, adjusting out-of-the-box pre-trained computer vision models will provide the desired solution. In the present paper is proposed a pipeline that takes a noisy image with a person and specifically detects the regions with garments that are bottoms or tops. Our solution implements models that are capable of finding human parts in an image e.g. full-body vs half-body, or no human is found. Then, other models knowing that there's a human and its composition (e.g. not always we have a full-body) finds the bounding boxes/regions of the image that very likely correspond to a bottom or a top. For the creation of bounding boxes/re
    
[^52]: 一种基于强化学习和凸优化的混合框架，用于基于UAV的自主元宇宙数据收集

    A Hybrid Framework of Reinforcement Learning and Convex Optimization for UAV-Based Autonomous Metaverse Data Collection. (arXiv:2305.18481v1 [cs.LG])

    [http://arxiv.org/abs/2305.18481](http://arxiv.org/abs/2305.18481)

    本论文提出了一种使用无人机辅助的元宇宙网络模型，将资源分配和轨迹控制集成到系统模型中，设计了一种强化学习和凸优化的混合框架，以“合作”方式解决时间顺序优化问题，使得数据收集效率得到提高。

    

    无人机（UAV）由于成本和灵活性的优势，在提供通信服务方面具有很大的潜力，尤其是在新兴元宇宙和物联网（IoT）的背景下。本文考虑了一种UAV辅助的元宇宙网络，其中UAV扩展了基站（BS）的覆盖范围，收集路边单元（RSU）产生的元宇宙数据。为了提高数据收集效率，系统模型中集成了资源分配和轨迹控制。优化问题的时间依赖性使传统凸优化方法难以解决。基于所提出的UAV辅助的元宇宙网络系统模型，我们设计了一种强化学习和凸优化的混合框架，以“合作”方式解决时间顺序优化问题。仿真结果表明，所提出的框架能够在保证收集数据质量的同时，减少任务完成时间。

    Unmanned aerial vehicles (UAVs) are promising for providing communication services due to their advantages in cost and mobility, especially in the context of the emerging Metaverse and Internet of Things (IoT). This paper considers a UAV-assisted Metaverse network, in which UAVs extend the coverage of the base station (BS) to collect the Metaverse data generated at roadside units (RSUs). Specifically, to improve the data collection efficiency, resource allocation and trajectory control are integrated into the system model. The time-dependent nature of the optimization problem makes it non-trivial to be solved by traditional convex optimization methods. Based on the proposed UAV-assisted Metaverse network system model, we design a hybrid framework with reinforcement learning and convex optimization to {cooperatively} solve the time-sequential optimization problem. Simulation results show that the proposed framework is able to reduce the mission completion time with a given transmission 
    
[^53]: 基于单张图片的人体形状分类

    Human Body Shape Classification Based on a Single Image. (arXiv:2305.18480v1 [cs.CV])

    [http://arxiv.org/abs/2305.18480](http://arxiv.org/abs/2305.18480)

    本文提出一种基于单张图片的分类人体形状的方法，结合实例分割和关键点估计模型，无需基于三维人体重建且可以有效地在噪声环境下进行。我们使用开源基准数据集进行模型训练，并将此方法在新颖图像数据集上进行了定量和定性的评估。

    

    现在在线时装推荐系统接纳了消费者的身体形状需求。本文提出了一种基于单张图片的分类人体形状的方法。通过使用Open-Source基准数据集训练实例分割和关键点估计模型，针对噪声环境具有鲁棒的背景差分能力。本文所提方法不需要基于三维人体重建，而是基于关键点估计分类，操作时无需历史信息计算所需所有测量值。我们将我们的方法定性评估与现有的体形分类器以及定量评估与我们提供给社区使用的新颖图像数据集进行对比。所产生的人体形状分类可以用于多种下游任务，如输入大小和适配推荐等。

    There is high demand for online fashion recommender systems that incorporate the needs of the consumer's body shape. As such, we present a methodology to classify human body shape from a single image. This is achieved through the use of instance segmentation and keypoint estimation models, trained only on open-source benchmarking datasets. The system is capable of performing in noisy environments owing to to robust background subtraction. The proposed methodology does not require 3D body recreation as a result of classification based on estimated keypoints, nor requires historical information about a user to operate - calculating all required measurements at the point of use. We evaluate our methodology both qualitatively against existing body shape classifiers and quantitatively against a novel dataset of images, which we provide for use to the community. The resultant body shape classification can be utilised in a variety of downstream tasks, such as input to size and fit recommendat
    
[^54]: FMM-X3D：基于FPGA的X3D建模和映射用于人体动作识别

    FMM-X3D: FPGA-based modeling and mapping of X3D for Human Action Recognition. (arXiv:2305.18479v1 [cs.CV])

    [http://arxiv.org/abs/2305.18479](http://arxiv.org/abs/2305.18479)

    本论文解决了将Human Action Recognition中最先进的模型之一——X3D映射到任何FPGA设备上的问题，并首次实现了针对这种复杂模型架构进行Human Action Recognition任务的目标。

    

    三维卷积神经网络越来越受到研究人员和实践者的关注，在监控系统、自动驾驶车辆、人体监测系统和视频检索等许多领域都有应用。然而，它们的广泛应用受到了高计算和内存要求的限制，特别是针对资源受限的系统。本文解决了将Human Action Recognition中最先进的模型之一——X3D映射到任何FPGA设备上的问题。提出的工具流生成了一个经过优化的基于流的硬件系统，考虑了FPGA设备的可用资源和外部存储器特性。所生成的设计推进了当前性能和准确性帕累托前沿，并首次实现了针对这种复杂模型架构进行Human Action Recognition任务的目标。

    3D Convolutional Neural Networks are gaining increasing attention from researchers and practitioners and have found applications in many domains, such as surveillance systems, autonomous vehicles, human monitoring systems, and video retrieval. However, their widespread adoption is hindered by their high computational and memory requirements, especially when resource-constrained systems are targeted. This paper addresses the problem of mapping X3D, a state-of-the-art model in Human Action Recognition that achieves accuracy of 95.5\% in the UCF101 benchmark, onto any FPGA device. The proposed toolflow generates an optimised stream-based hardware system, taking into account the available resources and off-chip memory characteristics of the FPGA device. The generated designs push further the current performance-accuracy pareto front, and enable for the first time the targeting of such complex model architectures for the Human Action Recognition task.
    
[^55]: 前向和反向逼近理论在线性时间卷积网络中的应用

    Forward and Inverse Approximation Theory for Linear Temporal Convolutional Networks. (arXiv:2305.18478v1 [cs.LG])

    [http://arxiv.org/abs/2305.18478](http://arxiv.org/abs/2305.18478)

    本论文对卷积神经网络在序列建模时的逼近性质进行了理论分析并提出了一种全面的特征描述, 可以有效捕捉时间卷积体系结构所能捕捉的顺序关系类型。

    

    我们对卷积神经网络在建模时序序列时的逼近性质进行了理论分析。具体来说，我们证明了逼近速率估计（类似于Jackson的结果）和反向逼近定理（类似于Bernstein的结果），二者共同提供了一种全面的特征描述，可以有效捕捉时间卷积体系结构所能捕捉的顺序关系类型。该速率估计通过引入一种精细的复杂度度量改进了先前的结果，而反向逼近定理是新的。

    We present a theoretical analysis of the approximation properties of convolutional architectures when applied to the modeling of temporal sequences. Specifically, we prove an approximation rate estimate (Jackson-type result) and an inverse approximation theorem (Bernstein-type result), which together provide a comprehensive characterization of the types of sequential relationships that can be efficiently captured by a temporal convolutional architecture. The rate estimate improves upon a previous result via the introduction of a refined complexity measure, whereas the inverse approximation theorem is new.
    
[^56]: 超越元数据：利用游戏设计参数进行跨版本电子竞技分析

    Beyond the Meta: Leveraging Game Design Parameters for Patch-Agnostic Esport Analytics. (arXiv:2305.18477v1 [cs.LG])

    [http://arxiv.org/abs/2305.18477](http://arxiv.org/abs/2305.18477)

    本论文提出了一种新的跨版本的电子竞技分析方法，通过利用游戏设计参数并利用聚类技术创建角色表征形式来解决传统方法短寿命的问题。以Dota 2为例验证了这种方法，取得了显著的性能提升。

    

    电子竞技游戏是全球游戏市场的重要组成部分，并且是增长最快的游戏细分领域。这导致了电子竞技分析的领域产生，其使用游戏提取的遥测数据来为玩家、教练、播音员和其他利益相关者提供信息。与传统的体育比赛相比，电子竞技游戏的机制和规则经常发生快速变化。由于游戏参数的频繁更改，电子竞技分析模型的使用寿命可能很短，这在文献中很大程度上被忽略了。本文提取游戏设计信息（即补丁说明），利用聚类技术提出了一种新的角色表征形式。以Dota 2游戏中击杀次数的预测为案例，利用这种创新的角色表征技术训练了一个神经网络模型。然后将此模型的性能与包括常规技术在内的两个不同基线进行了评估。这个模型不仅达到了显著的表现水平，还克服了电子竞技游戏中版本更迭的困境。

    Esport games comprise a sizeable fraction of the global games market, and is the fastest growing segment in games. This has given rise to the domain of esports analytics, which uses telemetry data from games to inform players, coaches, broadcasters and other stakeholders. Compared to traditional sports, esport titles change rapidly, in terms of mechanics as well as rules. Due to these frequent changes to the parameters of the game, esport analytics models can have a short life-spam, a problem which is largely ignored within the literature. This paper extracts information from game design (i.e. patch notes) and utilises clustering techniques to propose a new form of character representation. As a case study, a neural network model is trained to predict the number of kills in a Dota 2 match utilising this novel character representation technique. The performance of this model is then evaluated against two distinct baselines, including conventional techniques. Not only did the model signi
    
[^57]: 序列建模的变压器网络的逼近理论

    Approximation theory of transformer networks for sequence modeling. (arXiv:2305.18475v1 [cs.LG])

    [http://arxiv.org/abs/2305.18475](http://arxiv.org/abs/2305.18475)

    本文证明了变压器假设空间的普遍逼近定理，并提出了一种新的规律概念用于精确逼近速率估计，揭示了变压器适用于逼近哪些类型的序列关系，并讨论了其与传统序列建模方法之间的结构偏差。

    

    变压器是序列建模应用中广泛应用的架构，但其工作原理的理论理解有限。在本文中，我们研究了变压器逼近序列关系的能力。我们首先证明了变压器假设空间的普遍逼近定理。通过推导，我们确定了一种新的规律概念，在此概念下，我们可以证明一个明确的逼近速率估计。这个估计揭示了变压器的关键结构特性，并暗示了变压器适用于逼近哪些类型的序列关系。特别地，它使我们能够具体地讨论变压器与传统序列建模方法（如循环神经网络）之间的结构偏差。我们的研究结果得到了数字实验的支持。

    The transformer is a widely applied architecture in sequence modeling applications, but the theoretical understanding of its working principles is limited. In this work, we investigate the ability of transformers to approximate sequential relationships. We first prove a universal approximation theorem for the transformer hypothesis space. From its derivation, we identify a novel notion of regularity under which we can prove an explicit approximation rate estimate. This estimate reveals key structural properties of the transformer and suggests the types of sequence relationships that the transformer is adapted to approximating. In particular, it allows us to concretely discuss the structural bias between the transformer and classical sequence modeling methods, such as recurrent neural networks. Our findings are supported by numerical experiments.
    
[^58]: 用机器学习分析感知压力测试

    Alg{\i}lanan Stres Testinin Makine \"O\u{g}renmesi ile Analiz Edilmesi. (arXiv:2305.18473v1 [cs.LG])

    [http://arxiv.org/abs/2305.18473](http://arxiv.org/abs/2305.18473)

    本研究利用机器学习重新分析了感知压力测试，揭示了测试问题的重要性不相等，展示了在心理上观察到的不同模式。

    

    本研究旨在使用机器学习重新分析感知压力测试，以确定150个个体的感知压力水平并测量测试问题的影响。该测试包括14个问题，每个问题的得分范围为0到4，总得分范围为0-56。其中，7个问题以负面方式表述并相应评分，而其余7个问题以正面方式表述并按相反方式评分。该测试还设计为识别两个子因素：感知自我效能和压力/不适感知。本研究的主要目标是展示利用人工智能技术测试问题可能并不具有相等的重要性，揭示使用机器学习在社会中出现变化的问题，并最终证明在心理上观察到不同的模式存在。这项研究通过重复现有的心理学文献，提供了不同的视角。

    The aim of this study is to reanalyze the perceived stress test using machine learning to determine the perceived stress levels of 150 individuals and measure the impact of the test questions. The test consists of 14 questions, each scored on a scale of 0 to 4, resulting in a total score range of 0-56. Out of these questions, 7 are formulated in a negative context and scored accordingly, while the remaining 7 are formulated in a positive context and scored in reverse. The test is also designed to identify two sub-factors: perceived self-efficacy and stress/discomfort perception. The main objectives of this research are to demonstrate that test questions may not have equal importance using artificial intelligence techniques, reveal which questions exhibit variations in the society using machine learning, and ultimately demonstrate the existence of distinct patterns observed psychologically. This study provides a different perspective from the existing psychology literature by repeating 
    
[^59]: 双向传播的深度预测编码网络用于分类和重建

    Deep Predictive Coding with Bi-directional Propagation for Classification and Reconstruction. (arXiv:2305.18472v1 [cs.LG])

    [http://arxiv.org/abs/2305.18472](http://arxiv.org/abs/2305.18472)

    本文提出了一种新的学习算法，双向预测编码（DBPC）将现有的预测编码方法扩展为支持正向和反馈信息传播的网络。该网络可以同时执行分类和重建任务。

    

    本文提出了一种新的学习算法，称为双向预测编码（DBPC），允许开发网络使用相同的权重同时执行分类和重建任务。预测编码（PC）已成为支配大脑信息处理的突出理论。在本文中，我们通过开发支持正向和反馈信息传播的网络来扩展现有的PC方法。使用DBPC训练的网络中的每个层都学习预测前一层和后一层神经元的活动，这使得网络能够同时执行分类和重建任务。

    This paper presents a new learning algorithm, termed Deep Bi-directional Predictive Coding (DBPC) that allows developing networks to simultaneously perform classification and reconstruction tasks using the same weights. Predictive Coding (PC) has emerged as a prominent theory underlying information processing in the brain. The general concept for learning in PC is that each layer learns to predict the activities of neurons in the previous layer which enables local computation of error and in-parallel learning across layers. In this paper, we extend existing PC approaches by developing a network which supports both feedforward and feedback propagation of information. Each layer in the networks trained using DBPC learn to predict the activities of neurons in the previous and next layer which allows the network to simultaneously perform classification and reconstruction tasks using feedforward and feedback propagation, respectively. DBPC also relies on locally available information for le
    
[^60]: AdaGrad算法在非凸目标优化中的收敛性: 简明证明和宽松假设

    Convergence of AdaGrad for Non-convex Objectives: Simple Proofs and Relaxed Assumptions. (arXiv:2305.18471v1 [cs.LG])

    [http://arxiv.org/abs/2305.18471](http://arxiv.org/abs/2305.18471)

    本文提出了仅有互易噪声方差和有界平滑性假设的非凸目标的AdaGrad算法的简单收敛性证明，证明中基于辅助函数$\xi$，比现有结果更紧密，在超参数化的情况下，能够确保梯度范数小于$\varepsilon$的迭代次数为$\mathcal{O}(\frac{1}{\varepsilon^2})$，并考虑了一种实际平滑假设$(L_0,L_1)$-平滑条件。

    

    我们提供了针对仅有互易噪声方差和有界平滑性假设的非凸目标的AdaGrad算法的简单收敛性证明。该证明基本上基于一个新颖的辅助函数$\xi$，有助于消除处理AdaGrad更新的分子和分母之间相关性的复杂性。通过简单的证明，我们能够获得比现有结果\citep{faw2022power}更紧密的结果，并将分析扩展到几种新的重要情况。具体而言，在超参数化的情况下，我们表明AdaGrad只需要$\mathcal{O}(\frac{1}{\varepsilon^2})$次迭代，就可以确保梯度范数小于$\varepsilon$，这与SGD的速率相匹配，并且比AdaGrad的现有速率$\mathcal{O}(\frac{1}{\varepsilon^4})$明显更紧密。然后，我们放弃有界平滑假设，并考虑一种称为$(L_0,L_1)$-平滑条件的实际平滑假设，该假设允许本地平滑性增长

    We provide a simple convergence proof for AdaGrad optimizing non-convex objectives under only affine noise variance and bounded smoothness assumptions. The proof is essentially based on a novel auxiliary function $\xi$ that helps eliminate the complexity of handling the correlation between the numerator and denominator of AdaGrad's update. Leveraging simple proofs, we are able to obtain tighter results than existing results \citep{faw2022power} and extend the analysis to several new and important cases. Specifically, for the over-parameterized regime, we show that AdaGrad needs only $\mathcal{O}(\frac{1}{\varepsilon^2})$ iterations to ensure the gradient norm smaller than $\varepsilon$, which matches the rate of SGD and significantly tighter than existing rates $\mathcal{O}(\frac{1}{\varepsilon^4})$ for AdaGrad. We then discard the bounded smoothness assumption and consider a realistic assumption on smoothness called $(L_0,L_1)$-smooth condition, which allows local smoothness to grow w
    
[^61]: 通过随机Top-k稀疏化减少Split Learning的通信成本

    Reducing Communication for Split Learning by Randomized Top-k Sparsification. (arXiv:2305.18469v1 [cs.LG])

    [http://arxiv.org/abs/2305.18469](http://arxiv.org/abs/2305.18469)

    提出了一种通过随机Top-k稀疏化减少通信效率的Split Learning方法，实验证明该方法在相同压缩水平下比其他方法有更好的模型性能。

    

    Split Learning是Vertical Federated Learning (VFL)的简单解决方案之一，由于其简单性和高效性，在研究和应用中受到了广泛关注。然而，通信效率仍然是Split Learning的一个重要问题。本文研究了多种Split Learning的通信量减少方法，包括减少切割层大小、Top-k稀疏化、量化和L1正则化。通过对减少切割层大小和Top-k稀疏化的分析，我们进一步提出了随机Top-k稀疏化，以使模型更好地推广和收敛。这是通过在选择Top-k元素的同时，有小概率地选择非Top-k元素来实现的。实验结果表明，与其他通信量减少方法相比，我们提出的随机Top-k稀疏化在相同压缩水平下实现了更好的模型性能。

    Split learning is a simple solution for Vertical Federated Learning (VFL), which has drawn substantial attention in both research and application due to its simplicity and efficiency. However, communication efficiency is still a crucial issue for split learning. In this paper, we investigate multiple communication reduction methods for split learning, including cut layer size reduction, top-k sparsification, quantization, and L1 regularization. Through analysis of the cut layer size reduction and top-k sparsification, we further propose randomized top-k sparsification, to make the model generalize and converge better. This is done by selecting top-k elements with a large probability while also having a small probability to select non-top-k elements. Empirical results show that compared with other communication-reduction methods, our proposed randomized top-k sparsification achieves a better model performance under the same compression level.
    
[^62]: 几何图滤波器和神经网络：极限性质和判别度的权衡

    Geometric Graph Filters and Neural Networks: Limit Properties and Discriminability Trade-offs. (arXiv:2305.18467v1 [cs.LG])

    [http://arxiv.org/abs/2305.18467](http://arxiv.org/abs/2305.18467)

    本文研究了从流形采样点构造的图与流形神经网络的关系，并证明了这些图形上的卷积滤波器和神经网络收敛于连续流形上的卷积滤波器和神经网络。研究发现，图滤波器的可分性和近似流形滤波器所需行为的能力之间存在重要权衡，在神经网络中由于非线性的频率混合属性而得到改善。

    

    本文研究了图神经网络（GNN）和流形神经网络（MNN）之间的关系，当图是从流形采样点构造而成时，从而编码几何信息。我们考虑了卷积MNN和GNN，其中流形和图卷积分别以拉普拉斯-贝尔特拉米算子和图Laplacian为定义。使用适当的核，我们分析了密集和中等稀疏图。我们证明了非渐近误差界，表明这些图形上的卷积滤波器和神经网络收敛于连续流形上的卷积滤波器和神经网络。通过这个分析的副产品，我们观察到了图滤波器的可分性和近似流形滤波器所需行为的能力之间的重要权衡。然后，我们讨论了这种权衡如何在神经网络中由于非线性的频率混合属性而改善。

    This paper studies the relationship between a graph neural network (GNN) and a manifold neural network (MNN) when the graph is constructed from a set of points sampled from the manifold, thus encoding geometric information. We consider convolutional MNNs and GNNs where the manifold and the graph convolutions are respectively defined in terms of the Laplace-Beltrami operator and the graph Laplacian. Using the appropriate kernels, we analyze both dense and moderately sparse graphs. We prove non-asymptotic error bounds showing that convolutional filters and neural networks on these graphs converge to convolutional filters and neural networks on the continuous manifold. As a byproduct of this analysis, we observe an important trade-off between the discriminability of graph filters and their ability to approximate the desired behavior of manifold filters. We then discuss how this trade-off is ameliorated in neural networks due to the frequency mixing property of nonlinearities. We further d
    
[^63]: 基于最近邻的大语言模型的测试时间训练

    Test-Time Training on Nearest Neighbors for Large Language Models. (arXiv:2305.18466v1 [cs.CL])

    [http://arxiv.org/abs/2305.18466](http://arxiv.org/abs/2305.18466)

    该论文提出了一种基于最近邻的测试时间训练方法，通过检索和微调少量邻居的文本数据，该方法在大语言模型上显著提高了性能。

    

    最近的许多工作都旨在在测试时从数据库中检索相关信息以增强语言模型。我们通过直接在测试时使用其标准训练设置对检索到的数据对模型进行微调，避免了提示工程的需要。为此，我们建立了一个基于“Pile”数据集的文本嵌入的大规模分布式最近邻索引。给定一个语言模型的查询，我们的系统检索查询的邻居，并在对应于这些邻居的文本数据上微调模型。令人惊讶的是，检索和训练仅20个邻居，每个邻居仅进行一次梯度迭代，就显著提高了在“Pile”基准测试中超过二十个语言建模任务的性能。例如，测试时间训练显著缩小了小型GPT2模型和GPTNeo模型之间的性能差距，后者是专门对“Pile”进行收敛训练的，体积却是前者的十倍以上。然而，其方法的成功还取决于充分的索引质量和大小。

    Many recent efforts aim to augment language models with relevant information retrieved from a database at test time. We avoid the need for prompt engineering by directly fine-tuning the model on data retrieved at test time using its standard training setup. For this purpose, we build a large-scale distributed nearest neighbor index based on text embeddings of the Pile dataset. Given a query to a language model, our system retrieves the neighbors of the query and fine-tunes the model on the text data corresponding to those neighbors. Surprisingly, retrieving and training on as few as 20 neighbors, each for only one gradient iteration, drastically improves performance across more than twenty language modeling tasks in the Pile benchmark. For example, test-time training significantly narrows the performance gap between a small GPT2 model and a GPTNeo model, more than ten times larger, that was specifically trained to convergence on the Pile. Sufficient index quality and size, however, are
    
[^64]: 《带差分隐私的Gboard语言模型联合学习》

    Federated Learning of Gboard Language Models with Differential Privacy. (arXiv:2305.18465v1 [cs.LG])

    [http://arxiv.org/abs/2305.18465](http://arxiv.org/abs/2305.18465)

    本文讨论了在Gboard中使用联合学习和差分隐私(DP)训练语言模型(LMs)的方法，提出了新的客户参与标准，在实现有意义的形式DP保证的同时提供了有利的隐私-效用交换。在对公共数据进行预训练的基础上，我们训练并部署了超过20个LMs以实现高效用和$\rho-$zCDP隐私保证。

    

    本文在谷歌键盘(Gboard)中使用联合学习和差分隐私(DP)训练语言模型(LMs)。我们应用DP-FTRL算法，在不要求对客户设备进行均匀采样的情况下实现了有意义的形式 DP 保证。为了提供有利的隐私-效用交换，我们引入了新的客户参与标准，并讨论了其在大规模系统中的配置影响。我们展示了如何将基于分位数的剪切估计与DP-FTRL相结合，以在训练过程中自适应选择剪切范数或减少超参数调整以准备训练。借助于对公共数据的预训练，我们训练并部署了超过20个Gboard LM，这些模型在$\rho \in (0.2, 2)$下实现了高效用和$\rho-$zCDP隐私保证，其中两个模型还使用了安全合并。

    We train language models (LMs) with federated learning (FL) and differential privacy (DP) in the Google Keyboard (Gboard). We apply the DP-Follow-the-Regularized-Leader (DP-FTRL)~\citep{kairouz21b} algorithm to achieve meaningfully formal DP guarantees without requiring uniform sampling of client devices. To provide favorable privacy-utility trade-offs, we introduce a new client participation criterion and discuss the implication of its configuration in large scale systems. We show how quantile-based clip estimation~\citep{andrew2019differentially} can be combined with DP-FTRL to adaptively choose the clip norm during training or reduce the hyperparameter tuning in preparation for training. With the help of pretraining on public data, we train and deploy more than twenty Gboard LMs that achieve high utility and $\rho-$zCDP privacy guarantees with $\rho \in (0.2, 2)$, with two models additionally trained with secure aggregation~\citep{bonawitz2017practical}. We are happy to announce tha
    
[^65]: 特权知识蒸馏用于 Sim-to-Real 策略泛化

    Privileged Knowledge Distillation for Sim-to-Real Policy Generalization. (arXiv:2305.18464v1 [cs.LG])

    [http://arxiv.org/abs/2305.18464](http://arxiv.org/abs/2305.18464)

    论文提出了一种新的单阶段特权知识蒸馏方法（HIB），通过捕捉历史轨迹的特权知识表示来学习，缩小模拟和真实之间的差距。

    

    强化学习最近在机器人控制方面取得了显著的成功。但是，多数强化学习方法在模拟环境中运行，那里的特权知识（例如动力学，环境，地形）是轻松获取的。相反，在真实场景中，机器人代理通常仅依赖于本地状态（例如机器人关节的本体感反馈）来选择动作，导致显著的模拟到真实的差距。现有方法通过逐渐减少对特权知识的依赖或执行两阶段策略模仿来解决这个差距。但我们认为，这些方法在充分利用特权知识的能力方面存在局限性，导致性能次优。本文提出了一种称为历史信息瓶颈（HIB）的新型单阶段特权知识蒸馏方法，以缩小模拟到真实的差距。具体而言，HIB通过捕捉历史轨迹的特权知识表示来学习。

    Reinforcement Learning (RL) has recently achieved remarkable success in robotic control. However, most RL methods operate in simulated environments where privileged knowledge (e.g., dynamics, surroundings, terrains) is readily available. Conversely, in real-world scenarios, robot agents usually rely solely on local states (e.g., proprioceptive feedback of robot joints) to select actions, leading to a significant sim-to-real gap. Existing methods address this gap by either gradually reducing the reliance on privileged knowledge or performing a two-stage policy imitation. However, we argue that these methods are limited in their ability to fully leverage the privileged knowledge, resulting in suboptimal performance. In this paper, we propose a novel single-stage privileged knowledge distillation method called the Historical Information Bottleneck (HIB) to narrow the sim-to-real gap. In particular, HIB learns a privileged knowledge representation from historical trajectories by capturing 
    
[^66]: 基于邻域比较的语言模型成员推断攻击

    Membership Inference Attacks against Language Models via Neighbourhood Comparison. (arXiv:2305.18462v1 [cs.CL])

    [http://arxiv.org/abs/2305.18462](http://arxiv.org/abs/2305.18462)

    本文提出两种新的基于邻域比较的攻击策略，利用语言数据的内在结构来提高成员推断攻击的性能，并在几个公开数据集上证明这些攻击的有效性。

    

    成员推断攻击(MIAs)旨在预测一个数据样本是否存在于机器学习模型的训练数据中，广泛用于评估语言模型的隐私风险。现有的大多数攻击依赖于这样一个观察结果，即模型倾向于将更高的概率分配给训练样本而非非训练点。然而，对模型分数的简单阈值设定往往导致高误报率，因为它没有考虑样本的内在复杂性。最近的研究表明，基于参考模型的攻击可以将模型分数与在类似数据上训练的参考模型获得的分数进行比较，可以显著提高MIAs的性能。然而，为了训练参考模型，这种攻击的做法是假定敌方知道与原始训练数据密切相似的样本，这是一个强假设。因此，我们在更现实的情况下，假定攻击者只能访问有限的邻域样本，研究了这些攻击的性能。我们提出了两种新的攻击策略，利用语言数据的内在结构，可以用于评估在更现实的成员推断场景下的语言模型的隐私风险。我们的实验表明，我们的攻击在几个公开可用的数据集上是有效的，其中包括文本分类、自然语言推理和对话生成，并突显了语言模型在实际应用中的潜在隐私风险。

    Membership Inference attacks (MIAs) aim to predict whether a data sample was present in the training data of a machine learning model or not, and are widely used for assessing the privacy risks of language models. Most existing attacks rely on the observation that models tend to assign higher probabilities to their training samples than non-training points. However, simple thresholding of the model score in isolation tends to lead to high false-positive rates as it does not account for the intrinsic complexity of a sample. Recent work has demonstrated that reference-based attacks which compare model scores to those obtained from a reference model trained on similar data can substantially improve the performance of MIAs. However, in order to train reference models, attacks of this kind make the strong and arguably unrealistic assumption that an adversary has access to samples closely resembling the original training data. Therefore, we investigate their performance in more realistic sce
    
[^67]: 集合通信带宽最优管道调度算法

    Bandwidth Optimal Pipeline Schedule for Collective Communication. (arXiv:2305.18461v1 [cs.NI])

    [http://arxiv.org/abs/2305.18461](http://arxiv.org/abs/2305.18461)

    本文提出了一种强多项式时间算法，用于在任何拓扑结构上实现带宽最优的全聚合/归约散开，为解决方案提供了通用性，可以轻松扩展到其他形式的集合通信方法。

    

    我们提出了一种强多项式时间算法，用于在任何网络拓扑上（无论是否有交换机）生成带宽最优的全聚合/归约散开。我们的算法构建了一种管道调度，可在给定拓扑中实现保证最佳可行带宽性能。为了提供一个通用解决方案，我们将网络拓扑建模为具有异构链路容量和交换机的有向图，并将交换机直接建模为图表示中的顶点。该算法相对于拓扑大小具有强多项式时间。此工作严重依赖于先前的图论研究中的边不相交生成树和边分裂。虽然我们的重点是全聚合，但本文中的方法可以轻松扩展为归约、广播、归约散开和全归约的调度生成。

    We present a strongly polynomial-time algorithm to generate bandwidth optimal allgather/reduce-scatter on any network topology, with or without switches. Our algorithm constructs pipeline schedules achieving provably the best possible bandwidth performance on a given topology. To provide a universal solution, we model the network topology as a directed graph with heterogeneous link capacities and switches directly as vertices in the graph representation. The algorithm is strongly polynomial-time with respect to the topology size. This work heavily relies on previous graph theory work on edge-disjoint spanning trees and edge splitting. While we focus on allgather, the methods in this paper can be easily extended to generate schedules for reduce, broadcast, reduce-scatter, and allreduce.
    
[^68]: Leaky-ReLU神经网络在均匀通用逼近中的最小宽度研究

    Minimum Width of Leaky-ReLU Neural Networks for Uniform Universal Approximation. (arXiv:2305.18460v1 [cs.LG])

    [http://arxiv.org/abs/2305.18460](http://arxiv.org/abs/2305.18460)

    研究表明具有临界宽度的Leaky-ReLU神经网络可以在紧致域K上实现$L^p(K,\mathbb{R}^{d_y})$的UAP，而本文给出的最小宽度$w_{\min}=\max(d_x+1,d_y)+1_{d_y=d_x+1}$则适用于函数类$C(K,\mathbb{R}^{d_y})$，考虑到输出维度的影响。

    

    对神经网络的通用逼近性质（UAP）的研究历史悠久。当网络宽度不受限制时，只需要一个隐藏层即可进行UAP。相反，当深度不受限制时，UAP的宽度需要不小于临界宽度$w^*_{\min}=\max(d_x,d_y)$, 其中$d_x$和$d_y$分别是输入和输出的维度。最近，\cite{cai2022achieve}表明，具有这种临界宽度的Leaky-ReLU神经网络可以在紧致域$K$上实现$L^p$函数的UAP，即$L^p(K,\mathbb{R}^{d_y})$的UAP。本文研究了函数类$C(K,\mathbb{R}^{d_y})$的均匀UAP，并给出了Leaky-ReLU NN的确切最小宽度，为$w_{\min}=\max(d_x+1,d_y)+1_{d_y=d_x+1}$，其中涉及输出维度的影响。为了得到这个结果，我们提出了一种新的lift-flow-discretization方法，证明了均匀UAP与拓扑理论有着深刻的联系。

    The study of universal approximation properties (UAP) for neural networks (NN) has a long history. When the network width is unlimited, only a single hidden layer is sufficient for UAP. In contrast, when the depth is unlimited, the width for UAP needs to be not less than the critical width $w^*_{\min}=\max(d_x,d_y)$, where $d_x$ and $d_y$ are the dimensions of the input and output, respectively. Recently, \cite{cai2022achieve} shows that a leaky-ReLU NN with this critical width can achieve UAP for $L^p$ functions on a compact domain $K$, \emph{i.e.,} the UAP for $L^p(K,\mathbb{R}^{d_y})$. This paper examines a uniform UAP for the function class $C(K,\mathbb{R}^{d_y})$ and gives the exact minimum width of the leaky-ReLU NN as $w_{\min}=\max(d_x+1,d_y)+1_{d_y=d_x+1}$, which involves the effects of the output dimensions. To obtain this result, we propose a novel lift-flow-discretization approach that shows that the uniform UAP has a deep connection with topological theory.
    
[^69]: 扩散模型是多任务强化学习的有效规划器和数据合成器

    Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning. (arXiv:2305.18459v1 [cs.LG])

    [http://arxiv.org/abs/2305.18459](http://arxiv.org/abs/2305.18459)

    本文研究了单一扩散模型在建模多个任务的策略训练的大规模离线数据时的有效性。该方法名为Multi-Task Diffusion Model (MTDiff)，结合了Transformer骨干和提示学习，能够在如此复杂的多任务环境下取得相当不错的性能。

    

    扩散模型在视觉和NLP领域中展现出了高度表现力的生成能力。最近的研究表明，在强化学习领域中，扩散模型也能够有效地建模离线数据集中的复杂策略或轨迹。然而，这些研究仅限于单任务设置，没有考虑多任务的情况。本文旨在研究单一扩散模型在建模用于多个任务策略训练的大规模离线数据时的有效性，具有多样化和多模态的数据分布。具体而言，我们提出了Multi-Task Diffusion Model（MTDiff），这是一种基于扩散的方法，结合Transformer骨干和提示学习，用于多任务离线设置中的生成规划和数据合成。MTDiff利用大量可用于多任务数据中的知识，并在任务之间执行隐式知识共享以进行虚拟规划。

    Diffusion models have demonstrated highly-expressive generative capabilities in vision and NLP. Recent studies in reinforcement learning (RL) have shown that diffusion models are also powerful in modeling complex policies or trajectories in offline datasets. However, these works have been limited to single-task settings where a generalist agent capable of addressing multi-task predicaments is absent. In this paper, we aim to investigate the effectiveness of a single diffusion model in modeling large-scale multi-task offline data, which can be challenging due to diverse and multimodal data distribution. Specifically, we propose Multi-Task Diffusion Model (\textsc{MTDiff}), a diffusion-based method that incorporates Transformer backbones and prompt learning for generative planning and data synthesis in multi-task offline settings. \textsc{MTDiff} leverages vast amounts of knowledge available in multi-task data and performs implicit knowledge sharing among tasks. For generative planning, 
    
[^70]: 带标签偏移的域适应中的条件支持对齐

    Conditional Support Alignment for Domain Adaptation with Label Shift. (arXiv:2305.18458v1 [cs.LG])

    [http://arxiv.org/abs/2305.18458](http://arxiv.org/abs/2305.18458)

    本文提出了一种新颖的条件对抗支持对齐方法，以最小化源域和目标域特征表示分布之间的条件对称支持分歧，针对标签分布偏移的域适应问题，相对现有的方法有显著提高。

    

    无监督域适应 (UDA) 是指一种域适应框架，在该框架中，学习模型基于源域上的标记样本和目标域中的未标记样本进行训练。领域内现有的主流方法依赖于经典的协变量偏移假设来学习域不变特征表示，在源域和目标域之间的标签分布偏移下，性能亚优。在本文中，我们提出了一种新颖的条件对抗支持对齐 (CASA)，其旨在最小化源域和目标域特征表示分布之间的条件对称支持分歧，以便更好地表示分类任务。我们还引入了一种新的理论目标风险界限，证明了调整条件特征分布支持与现有的UDA设置中的边缘支持对齐方法相比的优点。然后，我们提供了一种新的高效优化算法，可应用于大规模数据集。我们的实验结果表明，相对于多个基准数据集中的现有方法，我们的方法得到了显著的改进。

    Unsupervised domain adaptation (UDA) refers to a domain adaptation framework in which a learning model is trained based on the labeled samples on the source domain and unlabelled ones in the target domain. The dominant existing methods in the field that rely on the classical covariate shift assumption to learn domain-invariant feature representation have yielded suboptimal performance under the label distribution shift between source and target domains. In this paper, we propose a novel conditional adversarial support alignment (CASA) whose aim is to minimize the conditional symmetric support divergence between the source's and target domain's feature representation distributions, aiming at a more helpful representation for the classification task. We also introduce a novel theoretical target risk bound, which justifies the merits of aligning the supports of conditional feature distributions compared to the existing marginal support alignment approach in the UDA settings. We then provi
    
[^71]: 学习强大的图神经网络在信息不足情况下

    Learning Strong Graph Neural Networks with Weak Information. (arXiv:2305.18457v1 [cs.LG])

    [http://arxiv.org/abs/2305.18457](http://arxiv.org/abs/2305.18457)

    本文提出了D$^2$PT，一个双通道的GNN框架，以处理具有多种数据缺失且相互影响的情况，其关键点包括在GNN中实现长程传播和允许信息传播到偏离节点。

    

    图神经网络在许多图学习任务中表现出了令人印象深刻的性能。然而，当输入的图数据存在信息不足时（即不完整的结构、不完整的特征和不充分的标签），GNN的性能可能会下降。大多数此前的研究都试图从具有特定类型的信息不足的图数据中学习，但这远不足以有效应对各种数据缺失存在且相互影响的情况。为了填补这一空白，本文旨在开发一种有效和原则性的方法来解决具有弱信息的图学习问题(GLWI)。基于我们经验分析的发现，我们推导出了两个设计关键点来解决GLWI的问题，即在GNN中实现长程传播，并允许信息传播到那些与最大连接组件隔离的偏离节点。因此，我们提出了D$^2$PT，这是一个双通道的GNN框架

    Graph Neural Networks (GNNs) have exhibited impressive performance in many graph learning tasks. Nevertheless, the performance of GNNs can deteriorate when the input graph data suffer from weak information, i.e., incomplete structure, incomplete features, and insufficient labels. Most prior studies, which attempt to learn from the graph data with a specific type of weak information, are far from effective in dealing with the scenario where diverse data deficiencies exist and mutually affect each other. To fill the gap, in this paper, we aim to develop an effective and principled approach to the problem of graph learning with weak information (GLWI). Based on the findings from our empirical analysis, we derive two design focal points for solving the problem of GLWI, i.e., enabling long-range propagation in GNNs and allowing information propagation to those stray nodes isolated from the largest connected component. Accordingly, we propose D$^2$PT, a dual-channel GNN framework that perfor
    
[^72]: 识别水印大语言模型的基线

    Baselines for Identifying Watermarked Large Language Models. (arXiv:2305.18456v1 [cs.LG])

    [http://arxiv.org/abs/2305.18456](http://arxiv.org/abs/2305.18456)

    该论文介绍了一套基线算法，用于识别带有水印的大型语言模型，这些算法分析了带有和不带有水印的LLM所产生的输出分布和逻辑分布之间的差异。

    

    我们考虑了一种新兴问题：如何识别广泛使用的，公开托管的，闭源的大型语言模型（LLM）中的水印方案。我们引入了一套基线算法，用于识别LLM水印，这些算法依赖于分析由带有和不带有水印的LLM生成的输出令牌和逻辑分布。值得注意的是，带有水印的LLMs tend to产生分布与标准模型 qualitatively and identifiably不同。此外，我们研究了在不同强度下识别水印的可行性，并考虑了每个识别机制在水印场景中的权衡。在此过程中，我们正式化了识别LLMs中的水印的具体问题，以及LLM水印和水印检测的一般性问题，为研究它们提供了框架和基础。

    We consider the emerging problem of identifying the presence and use of watermarking schemes in widely used, publicly hosted, closed source large language models (LLMs). We introduce a suite of baseline algorithms for identifying watermarks in LLMs that rely on analyzing distributions of output tokens and logits generated by watermarked and unmarked LLMs. Notably, watermarked LLMs tend to produce distributions that diverge qualitatively and identifiably from standard models. Furthermore, we investigate the identifiability of watermarks at varying strengths and consider the tradeoffs of each of our identification mechanisms with respect to watermarking scenario. Along the way, we formalize the specific problem of identifying watermarks in LLMs, as well as LLM watermarks and watermark detection in general, providing a framework and foundations for studying them.
    
[^73]: Diff-Instruct: 一种从预训练扩散模型中传递知识的通用方法

    Diff-Instruct: A Universal Approach for Transferring Knowledge From Pre-trained Diffusion Models. (arXiv:2305.18455v1 [cs.LG])

    [http://arxiv.org/abs/2305.18455](http://arxiv.org/abs/2305.18455)

    本文提出了一种通用框架 Diff-Instruct，能够以无需数据方式将预训练扩散模型中的知识传递给其他生成模型，仅需预训练 DM 和一个数据集。该框架是建立在严谨的数学基础上的，指导过程直接对应于最小化一种新型散度——Integral Kullback-Leibler (IKL) 散度。我们的方法在半监督学习、图像合成和视频预测中展示了其优越性。

    

    由于训练容易、可扩展性和样本质量高，扩散模型 (DMs) 已成为生成建模的首选方法，并有大量已预训练的模型适用于各种数据集。预训练 DMs 包含有关数据分布的复杂信息，对下游应用非常有价值。本文考虑从预训练 DMs 中学习并以无需数据方式将其知识传递给其他生成模型。具体而言，我们提出了一个通用框架 Diff-Instruct，该框架能指导任何生成模型的训练，只要生成的样本在模型参数方面是可微的。我们的 Diff-Instruct 建立在一个严谨的数学基础上，其中指导过程直接对应于最小化称为积分Kullback-Leibler (IKL) 散度的新型散度。IKL 是针对 DMs 定制的，通过计算沿扩散轨迹的 KL 散度的积分来捕获扩散过程信息，因此只需要一个预训练 DM 和一个数据集。我们通过在三个不同的应用程序：半监督学习、图像合成和视频预测中展示其优越性来证明了我们方法的有效性。

    Due to the ease of training, ability to scale, and high sample quality, diffusion models (DMs) have become the preferred option for generative modeling, with numerous pre-trained models available for a wide variety of datasets. Containing intricate information about data distributions, pre-trained DMs are valuable assets for downstream applications. In this work, we consider learning from pre-trained DMs and transferring their knowledge to other generative models in a data-free fashion. Specifically, we propose a general framework called Diff-Instruct to instruct the training of arbitrary generative models as long as the generated samples are differentiable with respect to the model parameters. Our proposed Diff-Instruct is built on a rigorous mathematical foundation where the instruction process directly corresponds to minimizing a novel divergence we call Integral Kullback-Leibler (IKL) divergence. IKL is tailored for DMs by calculating the integral of the KL divergence along a diffu
    
[^74]: "PubChemQC B3LYP/6-31G*//PM6数据集: 使用B3LYP/6-31G*计算的8600万分子的电子结构"

    PubChemQC B3LYP/6-31G*//PM6 dataset: the Electronic Structures of 86 Million Molecules using B3LYP/6-31G* calculations. (arXiv:2305.18454v1 [physics.chem-ph])

    [http://arxiv.org/abs/2305.18454](http://arxiv.org/abs/2305.18454)

    这个数据集包含了8600万个分子的电子性质，覆盖了从基本化合物到生物分子的各种分子，使用了B3LYP/6-31G*和PM6方法计算得出，并提供了多种格式以及五个子数据集。

    

    本文介绍了“PubChemQC B3LYP/6-31G*//PM6”数据集，其中包含了85938443个分子的电子性质。它包括轨道，轨道能量，总能量，偶极矩等相关性质。数据集囊括了从基本化合物到生物分子的各种分子，分子量高达1000，覆盖了94.0%的原始PubChem Compound目录(截至2016年8月29日)。该电子性质是用B3LYP/6-31G*和PM6方法计算得出。数据集提供三种格式：(i) GAMESS量子化学程序文件，(ii) 选定的JSON输出文件，(iii) 一个PostgreSQL数据库，使研究人员能够查询分子性质。五个子数据集提供了更具体的数据。前两个子集包括分子含有C、H、O和N，在300和500分子量以下。第三和第四个子数据集包含了分子含有C、H、N、O、P、S、F和Cl，在300和500分子量以下。第五个子集

    This article presents the "PubChemQC B3LYP/6-31G*//PM6" dataset, containing electronic properties of 85,938,443 molecules. It includes orbitals, orbital energies, total energies, dipole moments, and other relevant properties. The dataset encompasses a wide range of molecules, from essential compounds to biomolecules up to 1000 molecular weight, covering 94.0% of the original PubChem Compound catalog (as of August 29, 2016). The electronic properties were calculated using the B3LYP/6-31G* and PM6 methods. The dataset is available in three formats: (i) GAMESS quantum chemistry program files, (ii) selected JSON output files, and (iii) a PostgreSQL database, enabling researchers to query molecular properties. Five sub-datasets offer more specific data. The first two subsets include molecules with C, H, O, and N, under 300 and 500 molecular weight respectively. The third and fourth subsets contain C, H, N, O, P, S, F, and Cl, under 300 and 500 molecular weight respectively. The fifth subset
    
[^75]: 基于条件扩散模型的语义化三维医学图像合成

    Conditional Diffusion Models for Semantic 3D Medical Image Synthesis. (arXiv:2305.18453v1 [eess.IV])

    [http://arxiv.org/abs/2305.18453](http://arxiv.org/abs/2305.18453)

    这篇论文提出了Med-DDPM，一种使用扩散模型进行语义化三维医学图像合成的创新解决方案，它通过控制像素级掩码标签的生成过程，能够生成高质量逼真的医学图像，并且在精度、稳定性和多样性等指标上优于GAN技术，也优于传统的增强技术和GAN合成图像。

    

    本文提出了Med-DDPM，它是一种创新的解决方案，使用扩散模型进行语义化的三维医学图像合成，解决了医学成像中数据稀缺、采集方法不一致和隐私问题等普遍存在的问题。实验结果表明，扩散模型在稳定性和性能方面都超过了生成对抗网络（GAN），能够生成高质量、逼真的三维医学图像。Med-DDPM的独特特点在于使用语义条件进行三维图像合成的扩散模型。通过控制像素级掩码标签的生成过程，它便于创建逼真的医学图像。经验证明，Med-DDPM在精度、稳定性和多样性等指标上优于GAN技术。此外，Med-DDPM在增强分割模型的准确性方面也优于传统的增强技术和GAN合成图像。它解决了医学图像合成中的难点。

    This paper introduces Med-DDPM, an innovative solution using diffusion models for semantic 3D medical image synthesis, addressing the prevalent issues in medical imaging such as data scarcity, inconsistent acquisition methods, and privacy concerns. Experimental evidence illustrates that diffusion models surpass Generative Adversarial Networks (GANs) in stability and performance, generating high-quality, realistic 3D medical images. The distinct feature of Med-DDPM is its use of semantic conditioning for the diffusion model in 3D image synthesis. By controlling the generation process through pixel-level mask labels, it facilitates the creation of realistic medical images. Empirical evaluations underscore the superior performance of Med-DDPM over GAN techniques in metrics such as accuracy, stability, and versatility. Furthermore, Med-DDPM outperforms traditional augmentation techniques and synthetic GAN images in enhancing the accuracy of segmentation models. It addresses challenges such
    
[^76]: 利用扩散生成行车场景

    Generating Driving Scenes with Diffusion. (arXiv:2305.18452v1 [cs.CV])

    [http://arxiv.org/abs/2305.18452](http://arxiv.org/abs/2305.18452)

    本文提出一种利用扩散和对象检测相结合的“场景扩散”系统，直接生成具有真实感和物理合理性的场景，能够适应美国不同地区生成特定的场景。

    

    本文介绍了一种学习方法，用于模拟自驾车的感知系统输出的交通场景生成。在我们的“场景扩散”系统中，受潜在扩散的启发，我们使用扩散和对象检测的新型组合，直接创建具有真实感和物理合理性的离散边界框代理的现实场景。我们展示了我们的场景生成模型能够适应美国不同地区，产生捕捉每个地区细节的场景。

    In this paper we describe a learned method of traffic scene generation designed to simulate the output of the perception system of a self-driving car. In our "Scene Diffusion" system, inspired by latent diffusion, we use a novel combination of diffusion and object detection to directly create realistic and physically plausible arrangements of discrete bounding boxes for agents. We show that our scene generation model is able to adapt to different regions in the US, producing scenarios that capture the intricacies of each region.
    
[^77]: 具有因果亚结构的分子关系学习模型在数据分布变化时的鲁棒性

    Shift-Robust Molecular Relational Learning with Causal Substructure. (arXiv:2305.18451v1 [cs.LG])

    [http://arxiv.org/abs/2305.18451](http://arxiv.org/abs/2305.18451)

    本文提出了一种鲁棒性强的分子关系学习模型CMRL，通过检测与化学反应因果相关的核心亚结构来应对分子关系学习中的数据分布变化。

    

    最近，分子关系学习引起了分子科学领域的广泛关注，其目标是预测分子对之间的相互作用行为。本文提出了一种鲁棒性强的分子关系学习模型CMRL，它通过检测与化学反应因果相关的核心亚结构来应对分子关系学习中的数据分布变化。为此，我们首先假定基于分子科学领域知识的因果关系，并构建结构因果模型（SCM）来揭示变量之间的关系。基于SCM，我们引入了一个新的条件干预框架，其干预是基于成对分子条件的。使用条件干预框架，我们的模型成功地从因果亚结构中学习，并减轻了与化学反应虚假相关的快捷亚结构的混淆效应。本文在各种任务和真实和合成数据集上进行了广泛实验。

    Recently, molecular relational learning, whose goal is to predict the interaction behavior between molecular pairs, got a surge of interest in molecular sciences due to its wide range of applications. In this work, we propose CMRL that is robust to the distributional shift in molecular relational learning by detecting the core substructure that is causally related to chemical reactions. To do so, we first assume a causal relationship based on the domain knowledge of molecular sciences and construct a structural causal model (SCM) that reveals the relationship between variables. Based on the SCM, we introduce a novel conditional intervention framework whose intervention is conditioned on the paired molecule. With the conditional intervention framework, our model successfully learns from the causal substructure and alleviates the confounding effect of shortcut substructures that are spuriously correlated to chemical reactions. Extensive experiments on various tasks with real-world and sy
    
[^78]: GBG++：分类的快速和稳定的颗粒球生成方法

    GBG++: A Fast and Stable Granular Ball Generation Method for Classification. (arXiv:2305.18450v1 [cs.LG])

    [http://arxiv.org/abs/2305.18450](http://arxiv.org/abs/2305.18450)

    本文提出了一种基于关注机制的快速稳定的颗粒球生成方法，可以在分类任务中应用。

    

    颗粒球计算作为一种高效、稳健、可扩展的学习方法，已成为颗粒计算的研究热点。颗粒球计算包括两个阶段：颗粒球生成（GBG）和基于颗粒球（GB）的多粒度学习。然而，现有的GBG方法的稳定性和效率需要进一步提高，因为它们在很大程度上依赖于k均值或k分割。此外，基于GB的分类器仅单向考虑GB的几何特征构建分类规则，而忽视了GB的质量。因此，本文提出了一种快速稳定的基于关注机制的GBG（GBG++）方法。具体而言，所提出的GBG++方法仅需要在分割每个GB时计算从数据驱动的中心到未分割样本的距离，而不是随机选择中心并计算它到所有样本的距离。此外，引入了一种异常值检测方法。

    Granular ball computing (GBC), as an efficient, robust, and scalable learning method, has become a popular research topic of granular computing. GBC includes two stages: granular ball generation (GBG) and multi-granularity learning based on the granular ball (GB). However, the stability and efficiency of existing GBG methods need to be further improved due to their strong dependence on $k$-means or $k$-division. In addition, GB-based classifiers only unilaterally consider the GB's geometric characteristics to construct classification rules, but the GB's quality is ignored. Therefore, in this paper, based on the attention mechanism, a fast and stable GBG (GBG++) method is proposed first. Specifically, the proposed GBG++ method only needs to calculate the distances from the data-driven center to the undivided samples when splitting each GB, instead of randomly selecting the center and calculating the distances between it to all samples. Moreover, an outlier detection method is introduced
    
[^79]: 驯服AI Bot：大型语言模型中神经状态的可控性

    Taming AI Bots: Controllability of Neural States in Large Language Models. (arXiv:2305.18449v1 [cs.AI])

    [http://arxiv.org/abs/2305.18449](http://arxiv.org/abs/2305.18449)

    本文提出了一个问题，是否可以通过适当选择提示，控制AI bot到达任何状态，而研究表明训练良好的Bot能几乎确定地到达任何意义子集，具有可控性。

    

    我们解决了一个问题，即是否可以通过适当选择提示，将AI bot 控制到任何状态。为此，我们首先介绍了一个正式的“意义”定义，便于分析。然后，我们通过一些条件，表征了大型语言模型（LLM）所显然训练的“有意义的数据”和“训练良好的LLM”。虽然训练良好的LLM构建了一个欧几里得意义嵌入空间，但是意义本身并不形成一个向量（线性）子空间，而是一个商空间。我们然后表征了某个输入提示的状态所能到达的意义子集，并展示了训练良好的Bot能够到达任何意义，尽管概率很小。我们接着引入了更强的可控性概念，即“几乎确定可达性”，并展示了限制到意义空间时，AI bot是可控的。我们在引入功能特征的情况下这样做了。

    We tackle the question of whether an agent can, by suitable choice of prompts, control an AI bot to any state. To that end, we first introduce a formal definition of ``meaning'' that is amenable to analysis. Then, we characterize ``meaningful data'' on which large language models (LLMs) are ostensibly trained, and ``well-trained LLMs'' through conditions that are largely met by today's LLMs. While a well-trained LLM constructs an embedding space of meanings that is Euclidean, meanings themselves do not form a vector (linear) subspace, but rather a quotient space within. We then characterize the subset of meanings that can be reached by the state of the LLMs for some input prompt, and show that a well-trained bot can reach any meaning albeit with small probability. We then introduce a stronger notion of controllability as {\em almost certain reachability}, and show that, when restricted to the space of meanings, an AI bot is controllable. We do so after introducing a functional characte
    
[^80]: 有导向性正则化的神经网络压缩

    Neural Network Reduction with Guided Regularizers. (arXiv:2305.18448v1 [cs.LG])

    [http://arxiv.org/abs/2305.18448](http://arxiv.org/abs/2305.18448)

    本文提出了一种称为 “导向性正则化” 的新方法，通过在训练中优先考虑某些神经网络单元的权重，从而使得某些单元不那么重要，从而可以削减神经元，实现神经网络的压缩。

    

    正则化技术例如 $\mathcal{L}_1$ 和 $\mathcal{L}_2$ 正则化在删减神经网络中非常有效。然而，为了移除神经网络中的某个神经元或通道，所有与该神经元或通道相关的权重元素需要是可删减的，这不能由传统的正则化方法保证。本文提出了一种简单的新方法，称为 “导向性正则化”，在训练过程中优先考虑某些神经网络单元的权重，使一些单元不那么重要，从而可以削减神经元。这与 $\mathcal{L}_1$ 和 $\mathcal{L}_2$ 正则化的分散稀疏化不同，其中被置为零的权重矩阵的分量可以位于任何位置。所提出的方法以自然的方式减少神经网络，即在培训模型的同时中和不必要的单位。我们通过实验证明，我们提出的方法在保持性能的同时有效地修剪了神经网络。

    Regularization techniques such as $\mathcal{L}_1$ and $\mathcal{L}_2$ regularizers are effective in sparsifying neural networks (NNs). However, to remove a certain neuron or channel in NNs, all weight elements related to that neuron or channel need to be prunable, which is not guaranteed by traditional regularization. This paper proposes a simple new approach named "Guided Regularization" that prioritizes the weights of certain NN units more than others during training, which renders some of the units less important and thus, prunable. This is different from the scattered sparsification of $\mathcal{L}_1$ and $\mathcal{L}_2$ regularizers where the the components of a weight matrix that are zeroed out can be located anywhere. The proposed approach offers a natural reduction of NN in the sense that a model is being trained while also neutralizing unnecessary units. We empirically demonstrate that our proposed method is effective in pruning NNs while maintaining performance.
    
[^81]: 发挥随机化在差分隐私机器学习审计中的作用

    Unleashing the Power of Randomization in Auditing Differentially Private ML. (arXiv:2305.18447v1 [cs.LG])

    [http://arxiv.org/abs/2305.18447](http://arxiv.org/abs/2305.18447)

    本论文为差分隐私机器学习算法的审计提出了一种严格的方法，通过设计随机化的canaries来增强模型的容错能力和可解释性，同时在样本复杂度方面得到显著改进。

    

    我们提出了一种严格的方法，通过添加多个精心设计的示例(称为canaries)，来审计差分隐私机器学习算法。我们采用基于三个关键组件的第一原则方法。首先，我们介绍了扩展差分隐私定义来处理随机数据集的Lifted Differential Privacy (LiDP)。这赋予了我们设计随机化的canaries的自由。其次，我们通过尝试区分训练有$K$个canaries和训练没有一个canary时的模型来审计LiDP，即留出一个canary。利用canaries的i.i.d，LiDP可以利用设计中的对称性并复用每个私有训练模型来运行多个统计测试，一个针对每个canary。第三，我们引入了新的置信区间，通过适应经验高阶相关性来利用多个测试统计量。总之，这个新配方展示了在样本复杂度方面的显著改进，同时保证了严格的差分隐私。

    We present a rigorous methodology for auditing differentially private machine learning algorithms by adding multiple carefully designed examples called canaries. We take a first principles approach based on three key components. First, we introduce Lifted Differential Privacy (LiDP) that expands the definition of differential privacy to handle randomized datasets. This gives us the freedom to design randomized canaries. Second, we audit LiDP by trying to distinguish between the model trained with $K$ canaries versus $K - 1$ canaries in the dataset, leaving one canary out. By drawing the canaries i.i.d., LiDP can leverage the symmetry in the design and reuse each privately trained model to run multiple statistical tests, one for each canary. Third, we introduce novel confidence intervals that take advantage of the multiple test statistics by adapting to the empirical higher-order correlations. Together, this new recipe demonstrates significant improvements in sample complexity, both the
    
[^82]: Trompt：面向表格数据的更好的深度神经网络

    Trompt: Towards a Better Deep Neural Network for Tabular Data. (arXiv:2305.18446v1 [cs.LG])

    [http://arxiv.org/abs/2305.18446](http://arxiv.org/abs/2305.18446)

    Trompt是一种新颖的深度神经网络结构，其中分离了表格数据学习策略，它通过提示学习的方式来调整大型预先训练的模型，以便更好地能够处理表格数据，并取得了很好的效果。

    

    表格数据可谓是各种实际领域中最常用的数据结构之一，包括金融、医疗和电子商务。内在的异质性允许表格数据存储丰富的信息。然而，根据最近发布的表格基准，我们可以看到深度神经网络在表格数据集上仍然落后于树状模型。本文提出了Trompt，它是一个新颖的架构，灵感来自于语言模型的提示学习。提示学习的本质是通过一组模型外的提示来调整一个大型预训练模型，而不是直接修改模型。基于这个思想，Trompt将表格数据的学习策略分为两部分。第一部分类似于预训练模型，注重学习表格的内在信息。第二部分类似于提示，注重学习样本之间的差异。 Trompt在上述基准中进行了评估。

    Tabular data is arguably one of the most commonly used data structures in various practical domains, including finance, healthcare and e-commerce. The inherent heterogeneity allows tabular data to store rich information. However, based on a recently published tabular benchmark, we can see deep neural networks still fall behind tree-based models on tabular datasets. In this paper, we propose Trompt--which stands for Tabular Prompt--a novel architecture inspired by prompt learning of language models. The essence of prompt learning is to adjust a large pre-trained model through a set of prompts outside the model without directly modifying the model. Based on this idea, Trompt separates the learning strategy of tabular data into two parts. The first part, analogous to pre-trained models, focus on learning the intrinsic information of a table. The second part, analogous to prompts, focus on learning the variations among samples. Trompt is evaluated with the benchmark mentioned above. The ex
    
[^83]: 智能梯度放大用于深度神经网络

    Intelligent gradient amplification for deep neural networks. (arXiv:2305.18445v1 [cs.LG])

    [http://arxiv.org/abs/2305.18445](http://arxiv.org/abs/2305.18445)

    本文提出了一种智能确定梯度放大层次的方法，用于解决深度学习模型中的梯度消失问题和加速训练，实验结果表明可以提高深度神经网络的收敛速度和性能。

    

    深度学习模型在各种任务和领域中表现出卓越的性能，但也带来了一些挑战，特别是模型深度增加会导致训练时间增加，且会出现梯度消失的问题。虽然已有很多解决方案来单独解决这些问题，但是很少有综合性的解决方案来同时解决梯度消失和加速训练的问题。本文提出了一种智能地确定何时对深度学习模型中的哪些层应用梯度放大的方法，使用一种分析训练期间层梯度波动的方法进行计算。分别使用两种不同的智能度量和两个不同的阈值来确定放大的层，并提出了一种迁移学习的方法。结果表明，使用智能梯度放大可以提高深度神经网络的收敛速度，并在性能上优于传统的训练方法。

    Deep learning models offer superior performance compared to other machine learning techniques for a variety of tasks and domains, but pose their own challenges. In particular, deep learning models require larger training times as the depth of a model increases, and suffer from vanishing gradients. Several solutions address these problems independently, but there have been minimal efforts to identify an integrated solution that improves the performance of a model by addressing vanishing gradients, as well as accelerates the training process to achieve higher performance at larger learning rates. In this work, we intelligently determine which layers of a deep learning model to apply gradient amplification to, using a formulated approach that analyzes gradient fluctuations of layers during training. Detailed experiments are performed for simpler and deeper neural networks using two different intelligent measures and two different thresholds that determine the amplification layers, and a t
    
[^84]: 基于稀疏提示的元策略网络中的持续任务分配

    Continual Task Allocation in Meta-Policy Network via Sparse Prompting. (arXiv:2305.18444v1 [cs.LG])

    [http://arxiv.org/abs/2305.18444](http://arxiv.org/abs/2305.18444)

    本文提出的CoTASP可以通过学习过完备字典来生成稀疏掩码作为提示，从而从元策略网络中提取与每个任务相关的子网络，实现了快速适应新任务，同时保留了之前任务的共同知识。

    

    如何通过不断学习一系列任务来训练一个具有一般化能力的元策略，是当前强化学习面临的挑战。本文提出了一种名为“连续任务分配的稀疏提示（CoTASP）”的解决方案，通过学习过完备字典来生成稀疏掩码作为提示，从元策略网络中提取与每个任务相关的子网络。通过交替优化子网络和提示，CoTASP更新了元策略，通过训练特定于任务的策略来实现。然后更新字典，以使优化后的提示与任务嵌入相匹配，从而捕捉其语义相关性。因此，相关任务通过相似的提示在元策略网络中共享更多的神经元，而跨任务干扰导致遗忘被有效地约束。给定经过训练的元策略和更新后的字典，我们可以通过推导相应的提示来迅速适应新任务，从而从元策略中提取相关的子网络。我们在一组导航任务上评估了CoTASP，并展示了它在任务完成度、样本效率和泛化能力方面优于现有的基线方法。

    How to train a generalizable meta-policy by continually learning a sequence of tasks? It is a natural human skill yet challenging to achieve by current reinforcement learning: the agent is expected to quickly adapt to new tasks (plasticity) meanwhile retaining the common knowledge from previous tasks (stability). We address it by "Continual Task Allocation via Sparse Prompting (CoTASP)", which learns over-complete dictionaries to produce sparse masks as prompts extracting a sub-network for each task from a meta-policy network. By optimizing the sub-network and prompts alternatively, CoTASP updates the meta-policy via training a task-specific policy. The dictionary is then updated to align the optimized prompts with tasks' embedding, thereby capturing their semantic correlations. Hence, relevant tasks share more neurons in the meta-policy network via similar prompts while cross-task interference causing forgetting is effectively restrained. Given a trained meta-policy with updated dicti
    
[^85]: 改进的无投影在线连续次模最大化算法

    Improved Projection-free Online Continuous Submodular Maximization. (arXiv:2305.18442v1 [cs.LG])

    [http://arxiv.org/abs/2305.18442](http://arxiv.org/abs/2305.18442)

    本文提出了一种名为POBGA的改进的无投影算法，该算法可用于在线连续子模最大化问题，具有更低的后悔度上限，同时保持与先前算法相同的计算复杂性。这是通过将投影算法、阻塞技术和在线boosting梯度上升技术相结合实现的。该算法具有分散式实现的变体，适用于低局部性的问题。

    

    本文研究了具有单调和连续DR-子模奖励函数的在线学习问题，该问题近年来受到了广泛关注。为了有效地处理这个问题，尤其是在具有复杂决策集的情况下，先前的研究使用了一种名为Mono-Frank-Wolfe（Mono-FW）的高效无投影算法，总共需要 $O(T)$ 梯度评估和线性优化步骤。然而，它只能实现 $O(T^{4/5})$ 的$(1-1/e)$-后悔度上限。本文提出一种改进的无投影算法，即POBGA，该算法可以将后悔度上限降低到 $O(T^{3/4})$，同时保持与Mono-FW相同的计算复杂性。我们的关键想法不是修改Mono-FW，而是将一种称为在线boosting梯度上升的投影算法、一种不可行的投影技术和一种阻塞技术巧妙结合。此外，我们考虑去中心化设置并开发了POBGA的一个变体，它不仅减少了当前代码的计算复杂性，而且在具有低局部性的设置中表现出色。

    We investigate the problem of online learning with monotone and continuous DR-submodular reward functions, which has received great attention recently. To efficiently handle this problem, especially in the case with complicated decision sets, previous studies have proposed an efficient projection-free algorithm called Mono-Frank-Wolfe (Mono-FW) using $O(T)$ gradient evaluations and linear optimization steps in total. However, it only attains a $(1-1/e)$-regret bound of $O(T^{4/5})$. In this paper, we propose an improved projection-free algorithm, namely POBGA, which reduces the regret bound to $O(T^{3/4})$ while keeping the same computational complexity as Mono-FW. Instead of modifying Mono-FW, our key idea is to make a novel combination of a projection-based algorithm called online boosting gradient ascent, an infeasible projection technique, and a blocking technique. Furthermore, we consider the decentralized setting and develop a variant of POBGA, which not only reduces the current 
    
[^86]: DeCoR: 通过预测早期音频编码来避免知识遗忘

    DeCoR: Defy Knowledge Forgetting by Predicting Earlier Audio Codes. (arXiv:2305.18441v1 [eess.AS])

    [http://arxiv.org/abs/2305.18441](http://arxiv.org/abs/2305.18441)

    DeCoR是一种持续音频表示学习方法，通过预测早期音频编码中的量化索引，间接从早期模型向最新模型提取知识，避免在学习新数据时遗忘以前学习的任务。这种方法提高了声学场景分类准确性，与持续自监督表示学习相结合，存储和计算开销小，是一种高效的持续学习解决方案。

    

    终身音频特征提取需要逐步学习新的声音类别，以适应随时间变化的新数据分布。然而，仅在新数据上优化模型可能会导致先前学习的任务灾难性遗忘，这会破坏模型在长期内的良好表现能力。本文介绍了一种名为DeCoR的新的持续音频表示学习方法。与存储先前数据、特征或模型的其他方法不同，DeCoR通过从延迟码本预测量化索引间接从早期模型向最新模型提炼知识。我们证明了DeCoR提高了声学场景分类准确性，并与持续的自监督表示学习相结合效果良好。我们的方法提供了极小的存储和计算开销，是持续学习的一种轻量级和高效的解决方案。

    Lifelong audio feature extraction involves learning new sound classes incrementally, which is essential for adapting to new data distributions over time. However, optimizing the model only on new data can lead to catastrophic forgetting of previously learned tasks, which undermines the model's ability to perform well over the long term. This paper introduces a new approach to continual audio representation learning called DeCoR. Unlike other methods that store previous data, features, or models, DeCoR indirectly distills knowledge from an earlier model to the latest by predicting quantization indices from a delayed codebook. We demonstrate that DeCoR improves acoustic scene classification accuracy and integrates well with continual self-supervised representation learning. Our approach introduces minimal storage and computation overhead, making it a lightweight and efficient solution for continual learning.
    
[^87]: 黑盒子异常归因

    Black-Box Anomaly Attribution. (arXiv:2305.18440v1 [cs.LG])

    [http://arxiv.org/abs/2305.18440](http://arxiv.org/abs/2305.18440)

    本论文提出了一种名为“可能性补偿 ”的新颖归因框架，该框架可以在没有训练数据的情况下解决黑盒模型的异常归因问题。

    

    当黑盒子机器学习模型的预测偏离真实观察结果时，如何判断偏差背后的原因是一个基本而普遍的问题。这是业务或工业AI应用的最终用户经常问的问题。偏差可能是由于次优黑盒模型，或者仅仅因为样本是异常值。在任何情况下，理想情况下应该获得某种形式的归因分数，即指示输入变量对异常的影响程度的值。

    When the prediction of a black-box machine learning model deviates from the true observation, what can be said about the reason behind that deviation? This is a fundamental and ubiquitous question that the end user in a business or industrial AI application often asks. The deviation may be due to a sub-optimal black-box model, or it may be simply because the sample in question is an outlier. In either case, one would ideally wish to obtain some form of attribution score -- a value indicative of the extent to which an input variable is responsible for the anomaly.  In the present paper we address this task of ``anomaly attribution,'' particularly in the setting in which the model is black-box and the training data are not available. Specifically, we propose a novel likelihood-based attribution framework we call the ``likelihood compensation (LC),'' in which the responsibility score is equated with the correction on each input variable needed to attain the highest possible likelihood. We
    
[^88]: 无改动且模型无关的生成图像源头归属方法

    Alteration-free and Model-agnostic Origin Attribution of Generated Images. (arXiv:2305.18439v1 [cs.CV])

    [http://arxiv.org/abs/2305.18439](http://arxiv.org/abs/2305.18439)

    该论文提出了一种无需改动且适用于多种生成模型的源头归属方法，通过反向工程分析生成图片的来源，解决了现有方法对特定型号的依赖限制问题，并取得了准确性和灵活性的提高。

    

    近年来，生成图像模型受到了越来越多的关注。然而，这些模型存在潜在的滥用和知识产权侵权问题，因此有必要分析图像的来源，推断某个特定的模型是否生成了一张特定的图像，即原始归属。现有的方法在适用于特定类型的生成模型时存在限制，并且需要在训练或生成过程中进行额外的处理步骤。这限制了它们与缺少这些特定操作的预训练模型的使用，并可能影响图像生成的质量。为了解决这个问题，我们首先通过对生成图像模型的输入反向工程来开发一种无改动且模型无关的源头归属方法，即对于特定图像反转某特定模型的输入。给定一个特定的模型，我们首先分析反向工程任务在生成的图像和真实图像之间的难度差异，然后提出了一种度量方法来衡量这些任务之间的差异。基于这个度量标准，我们设计了一个模型无关的源头归属方法，不依赖于改动或额外的训练要求。我们的结果表明，我们的方法在准确性和灵活性方面优于现有技术。

    Recently, there has been a growing attention in image generation models. However, concerns have emerged regarding potential misuse and intellectual property (IP) infringement associated with these models. Therefore, it is necessary to analyze the origin of images by inferring if a specific image was generated by a particular model, i.e., origin attribution. Existing methods are limited in their applicability to specific types of generative models and require additional steps during training or generation. This restricts their use with pre-trained models that lack these specific operations and may compromise the quality of image generation. To overcome this problem, we first develop an alteration-free and model-agnostic origin attribution method via input reverse-engineering on image generation models, i.e., inverting the input of a particular model for a specific image. Given a particular model, we first analyze the differences in the hardness of reverse-engineering tasks for the gener
    
[^89]: 可解释机器学习在类别和混合数据上的应用：无损可视化

    Explainable Machine Learning for Categorical and Mixed Data with Lossless Visualization. (arXiv:2305.18437v1 [cs.LG])

    [http://arxiv.org/abs/2305.18437](http://arxiv.org/abs/2305.18437)

    本文提出了一些数值编码和可视化方法，以支持机器学习算法处理混合数据，并提出了可解释的多分类模型和SRG算法来生成解释性分类模型。

    

    为混合数据构建准确可解释的机器学习模型一直是算法面对非数值数据的挑战。本文提出了数值编码方案和无损可视化方法，为机器学习算法支持准确可解释的模型，提出了可解释的多分类模型和演示其重要作用。论文提出了一种分类混合数据类型的方法，并提出了一种工具包，以对混合数据的所有内部操作实现可解释性。论文还提出了一种新的“顺序规则生成（SRG）”算法，用于生成可解释的分类模型，并在多个计算实验中成功评估该算法。

    Building accurate and interpretable Machine Learning (ML) models for heterogeneous/mixed data is a long-standing challenge for algorithms designed for numeric data. This work focuses on developing numeric coding schemes for non-numeric attributes for ML algorithms to support accurate and explainable ML models, methods for lossless visualization of n-D non-numeric categorical data with visual rule discovery in these visualizations, and accurate and explainable ML models for categorical data. This study proposes a classification of mixed data types and analyzes their important role in Machine Learning. It presents a toolkit for enforcing interpretability of all internal operations of ML algorithms on mixed data with a visual data exploration on mixed data. A new Sequential Rule Generation (SRG) algorithm for explainable rule generation with categorical data is proposed and successfully evaluated in multiple computational experiments. This work is one of the steps to the full scope ML alg
    
[^90]: 通过非负低秩半定规划实现最优K均值聚类

    Statistically Optimal K-means Clustering via Nonnegative Low-rank Semidefinite Programming. (arXiv:2305.18436v1 [stat.ML])

    [http://arxiv.org/abs/2305.18436](http://arxiv.org/abs/2305.18436)

    本文提出了一种与NMF算法一样简单且可扩展的K均值聚类算法，该算法通过解决非负低秩半定规划问题获得了强大的统计最优性保证，实验证明该算法在合成和实际数据集上表现优异。

    

    K均值聚类是一种广泛应用于大数据集中发现模式的机器学习方法。半定规划（SDP）松弛最近被提出用于解决K均值优化问题，具有很强的统计最优性保证。但实现SDP求解器的巨大成本使得这些保证无法应用于实际数据集。相比之下，非负矩阵分解（NMF）是一种简单的聚类算法，被机器学习从业者广泛使用，但缺乏坚实的统计基础或严格的保证。在本文中，我们描述了一种类似于NMF的算法，通过使用非凸Burer-Monteiro分解方法解决半定规划松弛的K均值公式的非负低秩限制。所得到的算法与最先进的NMF算法一样简单和可扩展，同时也享有与SDP相同的强大的统计最优性保证。在我们的实验中，我们证明了我们的算法优于现有的NMF算法，并在合成和实际数据集上表现与最先进的SDP求解器相当。

    $K$-means clustering is a widely used machine learning method for identifying patterns in large datasets. Semidefinite programming (SDP) relaxations have recently been proposed for solving the $K$-means optimization problem that enjoy strong statistical optimality guarantees, but the prohibitive cost of implementing an SDP solver renders these guarantees inaccessible to practical datasets. By contrast, nonnegative matrix factorization (NMF) is a simple clustering algorithm that is widely used by machine learning practitioners, but without a solid statistical underpinning nor rigorous guarantees. In this paper, we describe an NMF-like algorithm that works by solving a nonnegative low-rank restriction of the SDP relaxed $K$-means formulation using a nonconvex Burer--Monteiro factorization approach. The resulting algorithm is just as simple and scalable as state-of-the-art NMF algorithms, while also enjoying the same strong statistical optimality guarantees as the SDP. In our experiments,
    
[^91]: 用于强化学习顺序实验设计的交叉熵估计器

    Cross-Entropy Estimators for Sequential Experiment Design with Reinforcement Learning. (arXiv:2305.18435v1 [cs.LG])

    [http://arxiv.org/abs/2305.18435](http://arxiv.org/abs/2305.18435)

    这篇论文提出了一个基于交叉熵估计器的备选下界估计方法，这个方法不需要对比样本，可以更精确地估计高信息增益，允许学习更优秀的设计策略，并且与隐式概率模型兼容。

    

    强化学习可有效地学习设计实验序列的摊销设计策略。然而，当前的方法依赖于期望信息增益的对比估计器，需要指数级的对比样本来达到无偏估计。我们提出了一种基于联合模型分布和灵活的提议分布的备选下界估计器。提议分布逼近模型参数在实验历史和设计策略条件下给定的真实后验分布。我们的估计器不需要对比样本，可以实现更准确的高信息增益估计，允许学习更优秀的设计策略，并且与隐式概率模型兼容。我们评估了我们算法在各种任务中的性能，包括连续和离散设计以及显式和隐式可能性。

    Reinforcement learning can effectively learn amortised design policies for designing sequences of experiments. However, current methods rely on contrastive estimators of expected information gain, which require an exponential number of contrastive samples to achieve an unbiased estimation. We propose an alternative lower bound estimator, based on the cross-entropy of the joint model distribution and a flexible proposal distribution. This proposal distribution approximates the true posterior of the model parameters given the experimental history and the design policy. Our estimator requires no contrastive samples, can achieve more accurate estimates of high information gains, allows learning of superior design policies, and is compatible with implicit probabilistic models. We assess our algorithm's performance in various tasks, including continuous and discrete designs and explicit and implicit likelihoods.
    
[^92]: 基于平行坐标的解释性机器学习模型发现

    Parallel Coordinates for Discovery of Interpretable Machine Learning Models. (arXiv:2305.18434v1 [cs.LG])

    [http://arxiv.org/abs/2305.18434](http://arxiv.org/abs/2305.18434)

    本研究使用平行坐标进行可视化知识发现，提出了一种新的数据分类器算法Hyper，并能够发现终端用户易于理解的、用于提高可解释性的混合和纯粹的超块。

    

    本文使用平行坐标进行可视化知识发现，提升了可解释性机器学习方法。平行坐标中的图形数据表示使得终端用户能够轻松理解超立方体和超块的概念。文章建议在所提出的数据分类器算法Hyper中同时使用混合和纯粹的超块。实验证明，Hyper模型具有决策树的泛化能力。算法被用在了多种设置和选项中，以交互方式或自动方式发现重叠或非重叠的超块。此外，文章还演示了使用超块和视觉模式的语言描述相结合的方法。UCI ML数据集被用于评估Hyper算法。通过10折交叉验证，Hyper算法能够发现混合和纯粹的超块。超块、降维和可视化之间的联系已经建立起来。最终用户能够找到和观察这些超块的能力也得到了提升。

    This work uses visual knowledge discovery in parallel coordinates to advance methods of interpretable machine learning. The graphic data representation in parallel coordinates made the concepts of hypercubes and hyperblocks (HBs) simple to understand for end users. It is suggested to use mixed and pure hyperblocks in the proposed data classifier algorithm Hyper. It is shown that Hyper models generalize decision trees. The algorithm is presented in several settings and options to discover interactively or automatically overlapping or non-overlapping hyperblocks. Additionally, the use of hyperblocks in conjunction with language descriptions of visual patterns is demonstrated. The benchmark data from the UCI ML repository were used to evaluate the Hyper algorithm. It enabled the discovery of mixed and pure HBs evaluated using 10-fold cross validation. Connections among hyperblocks, dimension reduction and visualization have been established. The capability of end users to find and observe
    
[^93]: 基于扩散模型的认知跨模态数据生成

    Cognitively Inspired Cross-Modal Data Generation Using Diffusion Models. (arXiv:2305.18433v1 [cs.LG])

    [http://arxiv.org/abs/2305.18433](http://arxiv.org/abs/2305.18433)

    本文提出一种基于多模态扩散模型训练和采样的数据生成方法，使用通道图像调节来学习跨模态相关性，实现条件生成。

    

    多数基于扩散模型的跨模态生成方法使用指导方式在潜在空间上提供控制，以实现不同模态的条件生成。这些方法通过分别训练每个模态的模型来提供指导，因此受到跨模态信息丢失的影响，且仅能实现单向条件生成。本文灵感来自于人类同步获取多模态信息并学习模态间相关性的方式，我们探讨了使用通道图像调节的多模态扩散模型训练和采样方案，以在训练阶段学习跨模态相关性，更好地模仿大脑中的学习过程。我们的实验结果表明，我们的方法可以实现基于所有相关模态的数据生成。

    Most existing cross-modal generative methods based on diffusion models use guidance to provide control over the latent space to enable conditional generation across different modalities. Such methods focus on providing guidance through separately-trained models, each for one modality. As a result, these methods suffer from cross-modal information loss and are limited to unidirectional conditional generation. Inspired by how humans synchronously acquire multi-modal information and learn the correlation between modalities, we explore a multi-modal diffusion model training and sampling scheme that uses channel-wise image conditioning to learn cross-modality correlation during the training phase to better mimic the learning process in the brain. Our empirical results demonstrate that our approach can achieve data generation conditioned on all correlated modalities.
    
[^94]: 可解释建模的完全可视化交互式决策树创建和增强

    Interactive Decision Tree Creation and Enhancement with Complete Visualization for Explainable Modeling. (arXiv:2305.18432v1 [cs.LG])

    [http://arxiv.org/abs/2305.18432](http://arxiv.org/abs/2305.18432)

    本文提出了两个新方法用于创建和增强决策树模型的可解释性和可视化能力，分别为弯曲坐标和移位配对坐标。

    

    为了增加机器学习（ML）模型的可解释性和预测精度，ML模型可视化是ML过程的关键部分。决策树（DTs）在机器学习（ML）中是必不可少的，因为它们用于理解许多黑匣子ML模型，包括深度学习模型。本研究提出了两种用于创建和增强DT的完全可视化的可理解模型的新方法。这些方法使用两个版本的通用线坐标（GLC）：弯曲坐标（BC）和移位配对坐标（SPC）。

    To increase the interpretability and prediction accuracy of the Machine Learning (ML) models, visualization of ML models is a key part of the ML process. Decision Trees (DTs) are essential in machine learning (ML) because they are used to understand many black box ML models including Deep Learning models. In this research, two new methods for creation and enhancement with complete visualizing Decision Trees as understandable models are suggested. These methods use two versions of General Line Coordinates (GLC): Bended Coordinates (BC) and Shifted Paired Coordinates (SPC). The Bended Coordinates are a set of line coordinates, where each coordinate is bended in a threshold point of the respective DT node. In SPC, each n-D point is visualized in a set of shifted pairs of 2-D Cartesian coordinates as a directed graph. These new methods expand and complement the capabilities of existing methods to visualize DT models more completely. These capabilities allow us to observe and analyze: (1) r
    
[^95]: 多任务学习优化Airbnb搜索之旅

    Optimizing Airbnb Search Journey with Multi-task Learning. (arXiv:2305.18431v1 [cs.IR])

    [http://arxiv.org/abs/2305.18431](http://arxiv.org/abs/2305.18431)

    本文提出了一种新的多任务深度学习模型架构Journey Ranker，来解决Airbnb搜索过程中的唯一挑战，即客户和主机的偏好，该模型可应用于多个用例。

    

    Airbnb是一个在线住宿和体验市场，客人通常需要花费数周来探索和比较多个物品，并在做出最后的预订请求之前平衡客人和主机的偏好。搜索过程的长期性质以及需要平衡客人和主机的偏好，这些都为Airbnb的搜索排名提供了独特的挑战。本文提出了一种新的多任务深度学习模型架构Journey Ranker，以解决这些挑战。Journey Ranker利用中间的客户操作作为里程碑（无论是积极的还是消极的）来更好地将客户推向成功的预订。它还使用上下文信息（如客户状态和搜索查询）来平衡客人和主机的偏好。其模块化和可扩展的设计包括四个模块，分离明确，可以方便地应用于Airbnb搜索排名以外的用例。

    At Airbnb, an online marketplace for stays and experiences, guests often spend weeks exploring and comparing multiple items before making a final reservation request. Each reservation request may then potentially be rejected or cancelled by the host prior to check-in. The long and exploratory nature of the search journey, as well as the need to balance both guest and host preferences, present unique challenges for Airbnb search ranking. In this paper, we present Journey Ranker, a new multi-task deep learning model architecture that addresses these challenges. Journey Ranker leverages intermediate guest actions as milestones, both positive and negative, to better progress the guest towards a successful booking. It also uses contextual information such as guest state and search query to balance guest and host preferences. Its modular and extensible design, consisting of four modules with clear separation of concerns, allows for easy application to use cases beyond the Airbnb search ranki
    
[^96]: 可扩展的弱监督银行交易分类方法

    Scalable and Weakly Supervised Bank Transaction Classification. (arXiv:2305.18430v1 [cs.LG])

    [http://arxiv.org/abs/2305.18430](http://arxiv.org/abs/2305.18430)

    本文提出了一种可扩展的银行交易分类方法，利用弱监督、自然语言处理和深度神经网络技术，最小化对手动注释的依赖，能够快速扩展到新的和组合用例，可用于财务健康报告和信用风险评估等金融应用。

    

    本文旨在利用弱监督、自然语言处理和深度神经网络技术对银行交易进行分类。我们的方法通过利用启发式和领域知识来训练准确的交易分类器，从而最小化对昂贵和难以获取的手动注释的依赖。我们提出了一个有效且可扩展的端到端数据管道，包括数据预处理、交易文本嵌入、锚定、标签生成、判别式神经网络训练以及系统架构概述。我们通过展示它优于现有的市场领先解决方案、实现准确分类并且可以快速扩展到新的和组合用例来证明我们方法的有效性。这反过来可以开启许多金融应用，例如财务健康报告和信用风险评估。

    This paper aims to categorize bank transactions using weak supervision, natural language processing, and deep neural network techniques. Our approach minimizes the reliance on expensive and difficult-to-obtain manual annotations by leveraging heuristics and domain knowledge to train accurate transaction classifiers. We present an effective and scalable end-to-end data pipeline, including data preprocessing, transaction text embedding, anchoring, label generation, discriminative neural network training, and an overview of the system architecture. We demonstrate the effectiveness of our method by showing it outperforms existing market-leading solutions, achieves accurate categorization, and can be quickly extended to novel and composite use cases. This can in turn unlock many financial applications such as financial health reporting and credit risk assessment.
    
[^97]: 基于General Line Coordinates的可视化知识发现

    Visual Knowledge Discovery with General Line Coordinates. (arXiv:2305.18429v1 [cs.LG])

    [http://arxiv.org/abs/2305.18429](http://arxiv.org/abs/2305.18429)

    本文提出了基于General Line Coordinates的可视化知识发现方法，用于生成、解释和可视化非线性分类器及其解释规则，并且可以提供更好的性能。

    

    多维数据上的黑盒子机器学习方法的理解是机器学习中的主要挑战。虽然已经存在许多强大的机器学习方法，但这些方法通常难以解释，或者在复杂数据上表现不佳。本文提出了基于多种无损General Line Coordinates的可视化知识发现方法。这些方法是先前引入的General Line Coordinates Linear和Dynamic Scaffolding Coordinates的扩展，用于生成、解释和可视化非线性分类器及其解释规则。为确保这些非线性模型和规则的准确性，General Line Coordinates Linear还开发了新的交互式可视化知识发现算法用于查找最坏情况验证分裂。这些扩展包括General Line Coordinates非线性，交互式规则线性，超块规则线性和最坏情况线性。多个基准数据集上的实验表明，这种可视化知识发现方法可以在解释和预测方面提供更好的性能。

    Understanding black-box Machine Learning methods on multidimensional data is a key challenge in Machine Learning. While many powerful Machine Learning methods already exist, these methods are often unexplainable or perform poorly on complex data. This paper proposes visual knowledge discovery approaches based on several forms of lossless General Line Coordinates. These are an expansion of the previously introduced General Line Coordinates Linear and Dynamic Scaffolding Coordinates to produce, explain, and visualize non-linear classifiers with explanation rules. To ensure these non-linear models and rules are accurate, General Line Coordinates Linear also developed new interactive visual knowledge discovery algorithms for finding worst-case validation splits. These expansions are General Line Coordinates non-linear, interactive rules linear, hyperblock rules linear, and worst-case linear. Experiments across multiple benchmark datasets show that this visual knowledge discovery method can
    
[^98]: GRD: 一种在强化学习中用于可解释奖励再分配的生成方式

    GRD: A Generative Approach for Interpretable Reward Redistribution in Reinforcement Learning. (arXiv:2305.18427v1 [cs.LG])

    [http://arxiv.org/abs/2305.18427](http://arxiv.org/abs/2305.18427)

    本文提出了一种可解释奖励再分配的方法，通过因果透视建模状态和行动贡献，产生可解释的返回分解。生成返回分解（GRD）框架用于延迟奖励场景中的策略优化。

    

    在强化学习中，一个重要的挑战是确定哪些状态-行动对应该对未来的分步奖励负责。Return Decomposition提供了一种解决方案，通过重新分配观测序列中的奖励来保持策略不变。虽然大多数现有方法都以不可解释的方式构建奖励再分配，但我们建议采用因果透视来明确建模状态和行动的贡献，从而产生可解释的返回分解。本文首先研究了因果生成模型在返回分解中的作用，通过描述马尔可夫奖励和基于轨迹的长期回报的生成过程，并进一步提出了一种名为生成返回分解（GRD）的框架，用于延迟奖励场景中的策略优化。具体而言，GRD首先确定生成过程中不可观测的马尔可夫奖励和因果关系，然后利用确定的因果模型计算可观测奖励的期望，进而提高策略性能。

    A major challenge in reinforcement learning is to determine which state-action pairs are responsible for future rewards that are delayed. Return Decomposition offers a solution by redistributing rewards from observed sequences while preserving policy invariance. While the majority of current approaches construct the reward redistribution in an uninterpretable manner, we propose to explicitly model the contributions of state and action from a causal perspective, resulting in an interpretable return decomposition. In this paper, we start by studying the role of causal generative models in return decomposition by characterizing the generation of Markovian rewards and trajectory-wise long-term return and further propose a framework, called Generative Return Decomposition (GRD), for policy optimization in delayed reward scenarios. Specifically, GRD first identifies the unobservable Markovian rewards and causal relations in the generative process. Then, GRD makes use of the identified causal
    
[^99]: 应用可解释的人工智能方法探究增材制造样品中输入变量和拉伸强度的相关性

    Employing Explainable Artificial Intelligence (XAI) Methodologies to Analyze the Correlation between Input Variables and Tensile Strength in Additively Manufactured Samples. (arXiv:2305.18426v1 [cs.LG])

    [http://arxiv.org/abs/2305.18426](http://arxiv.org/abs/2305.18426)

    本研究使用可解释的人工智能方法，探究了增材制造中输入变量和拉伸强度的相关性，发现Infill百分比和挤出温度对于拉伸强度具有最高的正相关和负相关影响。

    

    本研究探讨了包括Infill百分比、层高、挤出温度和打印速度在内的各种输入参数对于产生的增材制造物体的拉伸强度的影响。本研究的主要目标是提高我们对输入参数和拉伸强度之间相关性的理解，以及确定影响增材制造过程性能的关键因素。为了实现这一目标，我们首次引入了可解释的人工智能（XAI）技术，通过SHAP（Shapley Additive Explanations）方法解释机器学习模型的预测行为，以分析数据并获得有价值的洞察。我们的发现表明，Infill百分比和挤出温度与拉伸强度具有最高的相关性，分别为正相关和负相关。使用XAI技术可以更好地理解输入参数和增材制造中拉伸强度之间的相关性。

    This research paper explores the impact of various input parameters, including Infill percentage, Layer Height, Extrusion Temperature, and Print Speed, on the resulting Tensile Strength in objects produced through additive manufacturing. The main objective of this study is to enhance our understanding of the correlation between the input parameters and Tensile Strength, as well as to identify the key factors influencing the performance of the additive manufacturing process. To achieve this objective, we introduced the utilization of Explainable Artificial Intelligence (XAI) techniques for the first time, which allowed us to analyze the data and gain valuable insights into the system's behavior. Specifically, we employed SHAP (SHapley Additive exPlanations), a widely adopted framework for interpreting machine learning model predictions, to provide explanations for the behavior of a machine learning model trained on the data. Our findings reveal that the Infill percentage and Extrusion T
    
[^100]: 通过基于权重残差的低秩逼近实现精细调整模型的高效存储

    Efficient Storage of Fine-Tuned Models via Low-Rank Approximation of Weight Residuals. (arXiv:2305.18425v1 [cs.LG])

    [http://arxiv.org/abs/2305.18425](http://arxiv.org/abs/2305.18425)

    本论文提出了一种利用权重残差低秩特性实现精细调整模型高效存储的新方法ERE，并通过额外量化和分层秩分配来提高存储效率，实验结果表明该方法显著减少内存占用，同时保持性能。

    

    本文提出了一种利用权重残差的低秩特性来高效存储精细调整模型的方法。我们的关键观察是，大型超参数模型中的权重残差表现出更强的低秩特性。基于此，我们提出了一种名为高效残差编码（ERE）的新方法，通过近似低秩权重残差来实现对精细调整模型权重的高效存储。此外，我们分析了权重残差的稳健性，并通过使用额外的量化和分层秩分配来推动存储效率的极限。我们的实验结果表明，我们的方法可以在各种任务和模态下显著减少内存占用，同时保持性能。我们公开了代码。

    In this paper, we present an efficient method for storing fine-tuned models by leveraging the low-rank properties of weight residuals. Our key observation is that weight residuals in large overparameterized models exhibit even stronger low-rank characteristics. Based on this insight, we propose Efficient Residual Encoding (ERE), a novel approach that achieves efficient storage of fine-tuned model weights by approximating the low-rank weight residuals. Furthermore, we analyze the robustness of weight residuals and push the limit of storage efficiency by utilizing additional quantization and layer-wise rank allocation. Our experimental results demonstrate that our method significantly reduces memory footprint while preserving performance in various tasks and modalities. We release our code.
    
[^101]: 噪音在学习循环神经网络的样本复杂度中的作用：长序列的指数差距

    On the Role of Noise in the Sample Complexity of Learning Recurrent Neural Networks: Exponential Gaps for Long Sequences. (arXiv:2305.18423v1 [stat.ML])

    [http://arxiv.org/abs/2305.18423](http://arxiv.org/abs/2305.18423)

    本文研究了添加噪声的多层Sigmoid循环神经网络在学习序列分类问题上的样本复杂度问题，发现带噪声情况下样本复杂度可以用$\log(T/\sigma)$来界定，不存在噪声时下界为$wT$，两者存在指数级别的差距。

    

    我们考虑添加独立噪音的多层Sigmoid循环神经网络来分类长度为T的序列。我们的主要结果表明，这个类的PAC学习的样本复杂度可以被界定为$O (w\log(T/\sigma))$。对于相同类的非噪声版本（即$\sigma=0$），我们证明样本复杂度的下界为$\Omega (wT)$。我们的结果显示出在噪声和非噪声网络的样本复杂度对T的依赖性中存在指数差距。此外，考虑到上限对$1/\sigma$的对数依赖度很小，即使针对数值上可以忽略的$\sigma$，这个差距仍然存在。

    We consider the class of noisy multi-layered sigmoid recurrent neural networks with $w$ (unbounded) weights for classification of sequences of length $T$, where independent noise distributed according to $\mathcal{N}(0,\sigma^2)$ is added to the output of each neuron in the network. Our main result shows that the sample complexity of PAC learning this class can be bounded by $O (w\log(T/\sigma))$. For the non-noisy version of the same class (i.e., $\sigma=0$), we prove a lower bound of $\Omega (wT)$ for the sample complexity. Our results indicate an exponential gap in the dependence of sample complexity on $T$ for noisy versus non-noisy networks. Moreover, given the mild logarithmic dependence of the upper bound on $1/\sigma$, this gap still holds even for numerically negligible values of $\sigma$.
    
[^102]: HyperTime: 应对时间分布偏移的超参数优化方法

    HyperTime: Hyperparameter Optimization for Combating Temporal Distribution Shifts. (arXiv:2305.18421v1 [cs.LG])

    [http://arxiv.org/abs/2305.18421](http://arxiv.org/abs/2305.18421)

    本文提出了一种名为HyperTime的超参数优化方法，用于寻找时间上鲁棒的预测性能超参数，该方法在历史验证数据集上对平均验证损失和最坏情况验证损失设置了词典优先级顺序，并在多个带时间分布偏移的机器学习任务上表现强劲。

    

    本文提出了一种名为“HyperTime”的超参数优化方法，旨在寻找对未知测试数据的潜在时间分布偏移具有鲁棒性的超参数。我们的工作得到了一个重要观察的启发，即通过超参数优化，在许多情况下可以实现时间上的鲁棒预测性能。基于这一观察，我们借鉴了鲁棒优化文献中的最坏情况导向哲学，帮助找到这样的鲁棒性超参数配置。HyperTime在历史验证数据集上对平均验证损失和最坏情况验证损失设置了词典优先级顺序。我们对预期测试损失的上限进行了理论分析，揭示了我们的方法的独特优势。我们还在多个具有时间分布偏移的机器学习任务上展示了所提出方法的强大实证表现。

    In this work, we propose a hyperparameter optimization method named \emph{HyperTime} to find hyperparameters robust to potential temporal distribution shifts in the unseen test data. Our work is motivated by an important observation that it is, in many cases, possible to achieve temporally robust predictive performance via hyperparameter optimization. Based on this observation, we leverage the `worst-case-oriented' philosophy from the robust optimization literature to help find such robust hyperparameter configurations. HyperTime imposes a lexicographic priority order on average validation loss and worst-case validation loss over chronological validation sets. We perform a theoretical analysis on the upper bound of the expected test loss, which reveals the unique advantages of our approach. We also demonstrate the strong empirical performance of the proposed method on multiple machine learning tasks with temporal distribution shifts.
    
[^103]: 方差减少的分布式鲁棒Q-learning的样本复杂度

    Sample Complexity of Variance-reduced Distributionally Robust Q-learning. (arXiv:2305.18420v1 [cs.LG])

    [http://arxiv.org/abs/2305.18420](http://arxiv.org/abs/2305.18420)

    本文提出了两种新颖的无模型算法，为动态决策面对分布变化问题提供了鲁棒的解决方案，并通过将Q-learning与方差减少技术相结合，实现了样本复杂度的有效控制。

    

    在强化学习的理论和应用中，面对分布转移的动态决策是基本问题，因为数据收集所基于的环境分布可能会不同于模型部署所基于的分布。本文提出了两种新颖的无模型算法，即分布式鲁棒Q-learning和它的方差减少对应算法，能够高效地学习鲁棒策略，尽管会面对分布变化。这些算法旨在将带有Kullback-Leibler不确定性集的无限时域$\gamma$-折扣鲁棒马尔科夫决策过程的$q$-函数以元素$\epsilon$-精度有效逼近。进一步地，方差减少的分布式鲁棒Q-learning将同步Q-learning与方差减少技术相结合，以增强其性能，并且我们建立了它达到$ \tilde O(|S||A|(1-\gamma)^{-4}\epsilon^{-4}$的最小最大样本复杂度上界。

    Dynamic decision making under distributional shifts is of fundamental interest in theory and applications of reinforcement learning: The distribution of the environment on which the data is collected can differ from that of the environment on which the model is deployed. This paper presents two novel model-free algorithms, namely the distributionally robust Q-learning and its variance-reduced counterpart, that can effectively learn a robust policy despite distributional shifts. These algorithms are designed to efficiently approximate the $q$-function of an infinite-horizon $\gamma$-discounted robust Markov decision process with Kullback-Leibler uncertainty set to an entry-wise $\epsilon$-degree of precision. Further, the variance-reduced distributionally robust Q-learning combines the synchronous Q-learning with variance-reduction techniques to enhance its performance. Consequently, we establish that it attains a minmax sample complexity upper bound of $\tilde O(|S||A|(1-\gamma)^{-4}\e
    
[^104]: 采用双向语言模型进行语义分割，提高长篇音频识别的准确性

    Semantic Segmentation with Bidirectional Language Models Improves Long-form ASR. (arXiv:2305.18419v1 [cs.CL])

    [http://arxiv.org/abs/2305.18419](http://arxiv.org/abs/2305.18419)

    本研究提出一种采用双向语言模型进行语义分割的方法，可以有效提高长篇音频识别的准确性和速度。

    

    我们提出了一种通过分离发音中的语义完整句子实现长篇音频分割的方法，这可以防止ASR解码器处理远距离上下文的无效信息，同时避免它错过当前句子中的相关信息。在书面文本中，语义完整的句子通常由标点符号分隔；但不幸的是，实际口语中的发音很少包含标点符号。因此，我们通过提炼从书面标点文本中训练的双向语言模型（LM）中的标点符号知识来解决这个问题。我们将从LM教师提炼的分割器与以其他方法使用基于声学停顿的教师提炼的分割器进行了比较，并在流媒体ASR管道上进行了测试。我们的分割器在YouTube字幕任务中实现了3.2%的相对WER增益以及60ms中位段末端延迟的减少。

    We propose a method of segmenting long-form speech by separating semantically complete sentences within the utterance. This prevents the ASR decoder from needlessly processing faraway context while also preventing it from missing relevant context within the current sentence. Semantically complete sentence boundaries are typically demarcated by punctuation in written text; but unfortunately, spoken real-world utterances rarely contain punctuation. We address this limitation by distilling punctuation knowledge from a bidirectional teacher language model (LM) trained on written, punctuated text. We compare our segmenter, which is distilled from the LM teacher, against a segmenter distilled from a acoustic-pause-based teacher used in other works, on a streaming ASR pipeline. The pipeline with our segmenter achieves a 3.2% relative WER gain along with a 60 ms median end-of-segment latency reduction on a YouTube captioning task.
    
[^105]: 一瞥：重新思考视频不断学习中的时间信息

    Just a Glimpse: Rethinking Temporal Information for Video Continual Learning. (arXiv:2305.18418v1 [cs.CV])

    [http://arxiv.org/abs/2305.18418](http://arxiv.org/abs/2305.18418)

    本文提出了一种基于单个帧的新型重播机制SMILE，用于有效的视频连续学习。实验表明，在内存受到极端限制时，视频的多样性比时间信息更重要。

    

    增量学习是连续学习研究中最重要的设置之一，因为它与现实世界的应用场景密切相关。随着类别/任务数量的增加，由于受到内存大小的限制，灾难性遗忘会出现。在视频领域研究持续学习面临更大的挑战，因为视频数据包含大量帧，这会使回放记忆负担更重。目前的常见做法是从视频流中对帧进行子采样，并将其存储在回放记忆中。在本文中，我们提出了一种基于单个帧的新型重播机制SMILE，用于有效的视频连续学习。通过大量实验，我们表明在极端内存限制下，视频的多样性比时间信息更重要。因此，我们的方法侧重于从代表大量独特视频的少量帧中学习。我们在三个代表性视频数据集Kin上进行了实验。

    Class-incremental learning is one of the most important settings for the study of Continual Learning, as it closely resembles real-world application scenarios. With constrained memory sizes, catastrophic forgetting arises as the number of classes/tasks increases. Studying continual learning in the video domain poses even more challenges, as video data contains a large number of frames, which places a higher burden on the replay memory. The current common practice is to sub-sample frames from the video stream and store them in the replay memory. In this paper, we propose SMILE a novel replay mechanism for effective video continual learning based on individual/single frames. Through extensive experimentation, we show that under extreme memory constraints, video diversity plays a more significant role than temporal information. Therefore, our method focuses on learning from a small number of frames that represent a large number of unique videos. On three representative video datasets, Kin
    
[^106]: 点过程注意力支持网格编码以实现分布外泛化

    Determinantal Point Process Attention Over Grid Codes Supports Out of Distribution Generalization. (arXiv:2305.18417v1 [cs.LG])

    [http://arxiv.org/abs/2305.18417](http://arxiv.org/abs/2305.18417)

    本文描述了一个基于点过程注意力和网格编码的算法，在分布外测试集上实现了泛化能力，为理解大脑强泛化能力提供了见解。

    

    深度神经网络在模仿类人智能方面取得了巨大进展，并且越来越多地被用来理解大脑如何解决复杂的计算问题。然而，它们仍然不能提供关于大脑如何支持人类能够实现的强形式泛化的见解。其中一个例子是分布外（OOD）泛化——在训练集分布之外的测试样例上成功执行。在这里，我们识别出大脑处理的特征，这些特征可能有助于实现这种能力。我们描述了一个两部分算法，利用神经计算的特定特征实现OOD泛化，并通过评估两个具有挑战性的认知任务的表现来提供概念验证。首先，我们利用哺乳动物大脑使用类似网格的表示（例如在内嗅皮层中）来表示度量空间的事实。

    Deep neural networks have made tremendous gains in emulating human-like intelligence, and have been used increasingly as ways of understanding how the brain may solve the complex computational problems on which this relies. However, these still fall short of, and therefore fail to provide insight into how the brain supports strong forms of generalization of which humans are capable. One such case is out-of-distribution (OOD) generalization -successful performance on test examples that lie outside the distribution of the training set. Here, we identify properties of processing in the brain that may contribute to this ability. We describe a two-part algorithm that draws on specific features of neural computation to achieve OOD generalization, and provide a proof of concept by evaluating performance on two challenging cognitive tasks. First we draw on the fact that the mammalian brain represents metric spaces using grid-like representations (e.g., in entorhinal cortex): abstract represe
    
[^107]: 检查批量归一化优化在内存计算中减轻不同硬件噪声的作用和限制

    Examining the Role and Limits of Batchnorm Optimization to Mitigate Diverse Hardware-noise in In-memory Computing. (arXiv:2305.18416v1 [cs.LG])

    [http://arxiv.org/abs/2305.18416](http://arxiv.org/abs/2305.18416)

    本文研究了模拟交叉栏的非理想性以及它们对点积操作的影响，并通过实时交叉栏感知精细调整批量归一化参数的方法来减轻影响，从而降低内存计算的重新训练的硬件成本。

    

    内存计算（IMC）平台如模拟交叉栏正在受到关注，因为它们促进了低精度深度神经网络（DNN）的加速，并具有高面积和计算效率。然而，交叉栏中的固有非理想性使得部署的DNN的性能降低。除了量化误差之外，推理过程中经常遇到的非理想性包括交叉栏电路级寄生电阻和设备级非理想性，如随机读噪声和时间漂移。在这项工作中，我们的目标是密切检查这些非理想性对模拟交叉栏中的点积操作造成的扭曲，并探索实时通过感知交叉栏精细调整批量归一化参数来减轻非理想性影响的几乎无需训练的解决方案的可行性。这有助于减少IMC噪声感知重新训练的硬件成本，包括内存和训练能量。

    In-Memory Computing (IMC) platforms such as analog crossbars are gaining focus as they facilitate the acceleration of low-precision Deep Neural Networks (DNNs) with high area- & compute-efficiencies. However, the intrinsic non-idealities in crossbars, which are often non-deterministic and non-linear, degrade the performance of the deployed DNNs. In addition to quantization errors, most frequently encountered non-idealities during inference include crossbar circuit-level parasitic resistances and device-level non-idealities such as stochastic read noise and temporal drift. In this work, our goal is to closely examine the distortions caused by these non-idealities on the dot-product operations in analog crossbars and explore the feasibility of a nearly training-less solution via crossbar-aware fine-tuning of batchnorm parameters in real-time to mitigate the impact of the non-idealities. This enables reduction in hardware costs in terms of memory and training energy for IMC noise-aware re
    
[^108]: 几何代数变换器

    Geometric Algebra Transformers. (arXiv:2305.18415v1 [cs.LG])

    [http://arxiv.org/abs/2305.18415](http://arxiv.org/abs/2305.18415)

    本文介绍了一种通用架构几何代数变换器（GATr），用于解决几何数据问题。GATr使用投影几何代数表示输入输出和状态，具有可缩放性、表达性、多功能性。在n体建模和机器人规划的实验中，GATr相对于非几何基线表现出强大的改进。

    

    几何数据问题涉及计算机视觉、机器人、化学和物理领域。这些数据可以采用许多形式，例如点、方向向量、平面或变换，但迄今为止还没有一种单一的架构，可以应用于如此多种几何类型, 同时尊重它们的对称性。在本文中，我们介绍了几何代数变换器（GATr），一种用于几何数据的通用架构。GATr使用投影几何代数来表示输入、输出和隐藏状态，其提供常见几何对象的高效16维向量空间表示以及作用于它们的运算符。GATr是相对于E(3)（3D欧几里得空间的对称群）等变的。作为变换器，GATr可扩展、表达丰富且多功能。在n体建模和机器人规划的实验中，GATr相对于非几何基线均表现出强大的改进。

    Problems involving geometric data arise in a variety of fields, including computer vision, robotics, chemistry, and physics. Such data can take numerous forms, such as points, direction vectors, planes, or transformations, but to date there is no single architecture that can be applied to such a wide variety of geometric types while respecting their symmetries. In this paper we introduce the Geometric Algebra Transformer (GATr), a general-purpose architecture for geometric data. GATr represents inputs, outputs, and hidden states in the projective geometric algebra, which offers an efficient 16-dimensional vector space representation of common geometric objects as well as operators acting on them. GATr is equivariant with respect to E(3), the symmetry group of 3D Euclidean space. As a transformer, GATr is scalable, expressive, and versatile. In experiments with n-body modeling and robotic planning, GATr shows strong improvements over non-geometric baselines.
    
[^109]: 从API学习学习：黑盒数据无关元学习

    Learning to Learn from APIs: Black-Box Data-Free Meta-Learning. (arXiv:2305.18413v1 [cs.LG])

    [http://arxiv.org/abs/2305.18413](http://arxiv.org/abs/2305.18413)

    该论文提出了一个BiDf-MKD框架，可以从一组API库中无需访问训练数据，直接进行元学习；能够在更广泛的黑盒API上进行元学习，提高了元模型的泛化性能和应用范围。

    

    无数据元学习（DFML）旨在通过从一组预训练模型进行元学习而无需访问训练数据，从而实现高效学习新任务。现有的DFML工作仅能从（i）白盒和（ii）小规模预训练模型（iii）相同的架构中元学习，忽略了更实际的设置，即用户仅能通过任意模型架构和规模的API进行推断。为解决这个问题，我们提出了一个双层数据无关元知识蒸馏（BiDf-MKD）框架，将更通用的元知识从一组黑盒API转移到一个单一的元模型中。

    Data-free meta-learning (DFML) aims to enable efficient learning of new tasks by meta-learning from a collection of pre-trained models without access to the training data. Existing DFML work can only meta-learn from (i) white-box and (ii) small-scale pre-trained models (iii) with the same architecture, neglecting the more practical setting where the users only have inference access to the APIs with arbitrary model architectures and model scale inside. To solve this issue, we propose a Bi-level Data-free Meta Knowledge Distillation (BiDf-MKD) framework to transfer more general meta knowledge from a collection of black-box APIs to one single meta model. Specifically, by just querying APIs, we inverse each API to recover its training data via a zero-order gradient estimator and then perform meta-learning via a novel bi-level meta knowledge distillation structure, in which we design a boundary query set recovery technique to recover a more informative query set near the decision boundary. 
    
[^110]: Hawkes过程下异质事件动态的短时序依赖检测

    Short-term Temporal Dependency Detection under Heterogeneous Event Dynamic with Hawkes Processes. (arXiv:2305.18412v1 [stat.AP])

    [http://arxiv.org/abs/2305.18412](http://arxiv.org/abs/2305.18412)

    本论文讨论了Hawkes过程下异质事件动态的短时序依赖检测问题，提出了一种结合MHP和自激神经网络的鲁棒框架，实验结果显示其卓越性能。

    

    许多事件序列数据表现出相互激励或抑制的模式。可靠地检测这种时间依赖性对科学研究至关重要。现有模型中最常用的是多元Hawkes过程(MHP)，其影响函数自然地编码了Granger因果结构。然而，绝大多数现有方法使用标准MHP强度的直接或非线性变换，与真实数据不一致。在不规则和未知的异质强度下，捕捉时序依赖性很困难，因为人们难以区分相互作用的影响和强度波动的影响。本文针对短时序依赖性检测问题进行了讨论。我们展示了来自MHP的交叉影响的最大似然估计(MLE)存在无法消除但可以通过数量级减少的误差，使用的是互动HP的异质强度而不是目标HP的异质强度。然后，我们提出了一种结合MHP和自激神经网络用于短时序依赖检测的鲁棒框架。基于合成和真实世界数据的实证实验表明了所提出方法的卓越性能。

    Many event sequence data exhibit mutually exciting or inhibiting patterns. Reliable detection of such temporal dependency is crucial for scientific investigation. The de facto model is the Multivariate Hawkes Process (MHP), whose impact function naturally encodes a causal structure in Granger causality. However, the vast majority of existing methods use direct or nonlinear transform of standard MHP intensity with constant baseline, inconsistent with real-world data. Under irregular and unknown heterogeneous intensity, capturing temporal dependency is hard as one struggles to distinguish the effect of mutual interaction from that of intensity fluctuation. In this paper, we address the short-term temporal dependency detection issue. We show the maximum likelihood estimation (MLE) for cross-impact from MHP has an error that can not be eliminated but may be reduced by order of magnitude, using heterogeneous intensity not of the target HP but of the interacting HP. Then we proposed a robust
    
[^111]: 特征学习网络在实际规模下具有一致性

    Feature-Learning Networks Are Consistent Across Widths At Realistic Scales. (arXiv:2305.18411v1 [cs.LG])

    [http://arxiv.org/abs/2305.18411](http://arxiv.org/abs/2305.18411)

    本研究发现宽度对于神经网络的动态没有影响，网络在早期训练中表现出一致性，对于简单任务来说这一一致性贯穿整个训练过程，且大宽度下的结构特性是一致的。这表明特征学习极限可以捕捉到现实模型中出现的现象。

    

    我们研究了一系列网络结构和数据集上的特征学习神经网络宽度对网络动态的影响。在训练早期，基于在线数据训练的宽网络不仅具有相同的损失曲线，而且在整个训练过程中的测试预测也是一致的。对于像CIFAR-5m这样的简单任务，这适用于具有实际宽度的网络的整个训练过程。我们还展示了模型的结构特性，包括内部表示、预激活分布、稳定性边缘现象和大学习率效应，在大宽度下是一致的。这启发了一个假设：在特征学习极限下，可以捕捉到现实模型中出现的现象。对于更难的任务（如ImageNet和语言建模）和更晚的训练时间，有限宽度的偏差会系统地增长。这些偏差是由两个不同的效应引起的。首先，网络输出具有初始化相关的方差

    We study the effect of width on the dynamics of feature-learning neural networks across a variety of architectures and datasets. Early in training, wide neural networks trained on online data have not only identical loss curves but also agree in their point-wise test predictions throughout training. For simple tasks such as CIFAR-5m this holds throughout training for networks of realistic widths. We also show that structural properties of the models, including internal representations, preactivation distributions, edge of stability phenomena, and large learning rate effects are consistent across large widths. This motivates the hypothesis that phenomena seen in realistic models can be captured by infinite-width, feature-learning limits. For harder tasks (such as ImageNet and language modeling), and later training times, finite-width deviations grow systematically. Two distinct effects cause these deviations across widths. First, the network output has initialization-dependent variance 
    
[^112]: 理解乳腺癌生存：在多组学数据上利用因果关系和语言模型

    Understanding Breast Cancer Survival: Using Causality and Language Models on Multi-omics Data. (arXiv:2305.18410v1 [cs.LG])

    [http://arxiv.org/abs/2305.18410](http://arxiv.org/abs/2305.18410)

    本文旨在探究多组学数据在因果推断、基因组学和乳腺癌中的应用，利用大规模语言模型辅助解决因果推断方法的评估问题，突出了如何利用因果关系分析基因组扰动对乳腺癌患者生存的影响。

    

    随着医疗保健领域对更易用和可解释的机器学习模型的需求增加，发展和利用因果推断算法以通过分析观测数据发现因果关系变得越来越重要。可解释的方法有助于临床医生和生物学家预测疾病预后并建议适当的治疗方法。然而，在因果推断、基因组学和乳腺癌交叉领域上极少进行研究，我们旨在填补这一空白。此外，真实数据上对因果推断方法的评估普遍极为困难，因为地面真实的因果关系通常是未知的。因此，在本文中，我们还提议使用大规模语言模型来解决评估问题。具体来说，我们利用合适的因果推断算法来研究基因组中各种扰动如何影响乳腺癌患者的生存。我们使用了三种主要的因果推断方法。

    The need for more usable and explainable machine learning models in healthcare increases the importance of developing and utilizing causal discovery algorithms, which aim to discover causal relations by analyzing observational data. Explainable approaches aid clinicians and biologists in predicting the prognosis of diseases and suggesting proper treatments. However, very little research has been conducted at the crossroads between causal discovery, genomics, and breast cancer, and we aim to bridge this gap. Moreover, evaluation of causal discovery methods on real data is in general notoriously difficult because ground-truth causal relations are usually unknown, and accordingly, in this paper, we also propose to address the evaluation problem with large language models. In particular, we exploit suitable causal discovery algorithms to investigate how various perturbations in the genome can affect the survival of patients diagnosed with breast cancer. We used three main causal discovery 
    
[^113]: 面向方向的多目标学习：简单且可证明的随机算法

    Direction-oriented Multi-objective Learning: Simple and Provable Stochastic Algorithms. (arXiv:2305.18409v1 [cs.LG])

    [http://arxiv.org/abs/2305.18409](http://arxiv.org/abs/2305.18409)

    本文提出了一种新的面向方向的多目标问题，并给出了两种随机算法以解决这个问题，理论上收敛到帕累托稳定点。

    

    多目标优化（MOO）已成为许多与多个目标相关的机器学习问题（如多标准学习和多任务学习（MTL））中一个有影响力的框架。本文提出了一种新的面向方向的多目标问题，通过在一个方向的邻域内限制公共下降方向来规范线性组合目标的最优方向，例如MTL中的平均损失。 这个公式包括GD和MGDA作为特殊情况，享受像CAGrad中的面向方向的好处，以及有利于随机算法的设计。为了解决这个问题，我们提出了随机方向导向多目标梯度下降（SDMGrad），它使用简单的SGD类型的更新算法，以及在目标数量较多的情况下，使用高效的目标采样的SDMGrad-OS算法。 对于恒定的正则化参数λ，我们证明SDMGrad和SDMGrad-OS确实收敛到帕累托稳定点。

    Multi-objective optimization (MOO) has become an influential framework in many machine learning problems with multiple objectives such as learning with multiple criteria and multi-task learning (MTL). In this paper, we propose a new direction-oriented multi-objective problem by regularizing the common descent direction within a neighborhood of a direction that optimizes a linear combination of objectives such as the average loss in MTL. This formulation includes GD and MGDA as special cases, enjoys the direction-oriented benefit as in CAGrad, and facilitates the design of stochastic algorithms. To solve this problem, we propose Stochastic Direction-oriented Multi-objective Gradient descent (SDMGrad) with simple SGD type of updates, and its variant SDMGrad-OS with an efficient objective sampling in the setting where the number of objectives is large. For a constant-level regularization parameter $\lambda$, we show that SDMGrad and SDMGrad-OS provably converge to a Pareto stationary poin
    
[^114]: 一种用于分子多模态预训练的群对称随机微分方程模型。

    A Group Symmetric Stochastic Differential Equation Model for Molecule Multi-modal Pretraining. (arXiv:2305.18407v1 [cs.LG])

    [http://arxiv.org/abs/2305.18407](http://arxiv.org/abs/2305.18407)

    MoleculeSDE是用于分子多模态预训练的群对称随机微分方程模型，通过在输入空间中直接生成3D几何与2D拓扑之间的转换，它能够更有效地保存分子结构信息。

    

    分子预训练已经成为提高基于 AI 的药物发现性能的主流方法。然而，大部分现有的方法只关注单一的模态。最近的研究表明，最大化两种模态之间的互信息（MI）可以增强分子表示能力。而现有的分子多模态预训练方法基于从拓扑和几何编码的表示空间来估计 MI，因此丢失了分子的关键结构信息。为解决这一问题，我们提出了 MoleculeSDE。MoleculeSDE利用群对称（如 SE（3）-等变和反射-反对称）随机微分方程模型在输入空间中直接生成 3D 几何形状与 2D 拓扑之间的转换。它不仅获得更紧的MI界限，而且还能够有效地保存分子结构信息。

    Molecule pretraining has quickly become the go-to schema to boost the performance of AI-based drug discovery. Naturally, molecules can be represented as 2D topological graphs or 3D geometric point clouds. Although most existing pertaining methods focus on merely the single modality, recent research has shown that maximizing the mutual information (MI) between such two modalities enhances the molecule representation ability. Meanwhile, existing molecule multi-modal pretraining approaches approximate MI based on the representation space encoded from the topology and geometry, thus resulting in the loss of critical structural information of molecules. To address this issue, we propose MoleculeSDE. MoleculeSDE leverages group symmetric (e.g., SE(3)-equivariant and reflection-antisymmetric) stochastic differential equation models to generate the 3D geometries from 2D topologies, and vice versa, directly in the input space. It not only obtains tighter MI bound but also enables prosperous dow
    
[^115]: 一种机器学习方法预测微通道中的传热系数

    A machine learning approach to the prediction of heat-transfer coefficients in micro-channels. (arXiv:2305.18406v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2305.18406](http://arxiv.org/abs/2305.18406)

    本文使用一种新的方法——多输出高斯过程回归模型，来预测微通道中的传热系数，该模型的表现优于传统的经验相关式。

    

    准确预测双相传热系数对压缩式换热器的优化设计和运行至关重要，而人工智能研究的进步使得机器学习算法的应用有所提升。本文使用多输出高斯过程回归模型来估计微通道中的传热系数。该模型通过质量流量、热流量、系统压力和通道直径和长度的函数来训练和预测。该模型的表现优于传统的经验相关式，但存在过拟合、不确定性估计和结果解释等困难。

    The accurate prediction of the two-phase heat transfer coefficient (HTC) as a function of working fluids, channel geometries and process conditions is key to the optimal design and operation of compact heat exchangers. Advances in artificial intelligence research have recently boosted the application of machine learning (ML) algorithms to obtain data-driven surrogate models for the HTC. For most supervised learning algorithms, the task is that of a nonlinear regression problem. Despite the fact that these models have been proven capable of outperforming traditional empirical correlations, they have key limitations such as overfitting the data, the lack of uncertainty estimation, and interpretability of the results. To address these limitations, in this paper, we use a multi-output Gaussian process regression (GPR) to estimate the HTC in microchannels as a function of the mass flow rate, heat flux, system pressure and channel diameter and length. The model is trained using the Brunel Tw
    
[^116]: Dink-Net: 大规模图形神经聚类方法

    Dink-Net: Neural Clustering on Large Graphs. (arXiv:2305.18405v1 [cs.LG])

    [http://arxiv.org/abs/2305.18405](http://arxiv.org/abs/2305.18405)

    Dink-Net是一个可扩展的大规模图形神经聚类方法，该方法利用了膨胀和收缩的思想来处理百万节点的大图，并在各种基准数据集上优于现有的最先进方法。

    

    近年来，深度图聚类通过深度神经网络将图形的节点进行分组取得了很大的进展，但现有方法无法处理百万节点的大图。为了解决这个问题，我们提出了一种可扩展的Dink-Net深度图聚类方法，利用了膨胀和收缩的思想。首先，通过区分带增强的跟不带增强的节点，自我监督方式学习表示形式。同时，将聚类中心初始化为可学习的神经网络参数。随后，通过对抗性方式最小化提出的集群膨胀损失和集群收缩损失，优化聚类分布。通过这些设置，我们将表示学习和聚类优化两个步骤统一为一个端到端框架，引导网络学习聚类友好的特征。此外，Dink-Net能很好地扩展到大规模的图形上，因为设计的膨胀收缩操作可以有效地减少计算和内存消耗。实验结果表明，Dink-Net在处理百万节点图形的各种基准数据集上优于现有的最先进方法，证明了该方法在大图聚类中的可扩展性和有效性。

    Deep graph clustering, which aims to group the nodes of a graph into disjoint clusters with deep neural networks, has achieved promising progress in recent years. However, the existing methods fail to scale to the large graph with million nodes. To solve this problem, a scalable deep graph clustering method (Dink-Net) is proposed with the idea of dilation and shrink. Firstly, by discriminating nodes, whether being corrupted by augmentations, representations are learned in a self-supervised manner. Meanwhile, the cluster centres are initialized as learnable neural parameters. Subsequently, the clustering distribution is optimized by minimizing the proposed cluster dilation loss and cluster shrink loss in an adversarial manner. By these settings, we unify the two-step clustering, i.e., representation learning and clustering optimization, into an end-to-end framework, guiding the network to learn clustering-friendly features. Besides, Dink-Net scales well to large graphs since the designe
    
[^117]: 基于大语言模型的多项选择题答案确认预测研究

    Conformal Prediction with Large Language Models for Multi-Choice Question Answering. (arXiv:2305.18404v1 [cs.CL])

    [http://arxiv.org/abs/2305.18404](http://arxiv.org/abs/2305.18404)

    本研究探讨了如何利用符合性预测技术，在多项选择题回答任务中为语言模型提供不确定性量化。我们发现符合性预测的不确定性估计与预测准确性密切相关。

    

    随着大型语言模型的广泛开发，对它们进行健壮的不确定性量化技术将成为它们在高风险场景下安全部署的关键。本研究探讨了如何利用符合性预测技术，在多项选择题回答任务中为语言模型提供不确定性量化。我们发现符合性预测的不确定性估计与预测准确性密切相关。这种观察对于下游应用，如选择性分类和过滤低质量预测，可能会有用。我们还研究了符合性预测对于超出主题的问题的交换性假设，这可能是许多实际应用的更为现实的场景。本研究为在需要可靠保证错误率的安全关键情况下更加值得信赖和可靠地使用大型语言模型做出了贡献。

    As large language models continue to be widely developed, robust uncertainty quantification techniques will become crucial for their safe deployment in high-stakes scenarios. In this work, we explore how conformal prediction can be used to provide uncertainty quantification in language models for the specific task of multiple-choice question-answering. We find that the uncertainty estimates from conformal prediction are tightly correlated with prediction accuracy. This observation can be useful for downstream applications such as selective classification and filtering out low-quality predictions. We also investigate the exchangeability assumption required by conformal prediction to out-of-subject questions, which may be a more realistic scenario for many practical applications. Our work contributes towards more trustworthy and reliable usage of large language models in safety-critical situations, where robust guarantees of error rate are required.
    
[^118]: 剪枝与低秩参数高效微调相遇

    Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning. (arXiv:2305.18403v1 [cs.LG])

    [http://arxiv.org/abs/2305.18403](http://arxiv.org/abs/2305.18403)

    本论文提出了一种名为LoRAPrune的框架，可以高效微调和部署大型预训练模型，通过利用低秩自适应的值和梯度来设计PEFT感知的剪枝准则，并提出了一个迭代剪枝过程来去除冗余参数，实验结果表明与最先进的方法相比，可以显著降低模型大小和推理时间，同时保持竞争性的准确性。

    

    大型预训练模型（LPM）在各种任务中表现出卓越的性能。虽然出现了参数高效微调（PEFT）来便宜地微调这些大型模型用于下游任务，但它们的部署仍然受到巨大的模型规模和计算成本的制约。神经网络剪枝通过删除冗余参数来提供模型压缩的解决方案，但大多数现有方法依赖于计算参数梯度。然而，对于LPM而言，获得梯度是计算上禁止的，这需要探索替代方法。为此，我们提出了一种用于LPM高效微调和部署的统一框架，称为LoRAPrune。我们首先设计了一个PEFT感知的剪枝准则，该准则利用了低秩自适应（LoRA）的值和梯度，而不是预训练参数的梯度进行重要性评估。然后，我们提出了一个迭代剪枝过程来基于剪枝准则去除冗余参数。各种基准数据集上的实验结果表明，与最先进的方法相比，我们的框架可以显著降低模型大小和推理时间，同时保持竞争性的准确性。

    Large pre-trained models (LPMs), such as LLaMA and ViT-G, have shown exceptional performance across various tasks. Although parameter-efficient fine-tuning (PEFT) has emerged to cheaply fine-tune these large models on downstream tasks, their deployment is still hindered by the vast model scale and computational costs. Neural network pruning offers a solution for model compression by removing redundant parameters, but most existing methods rely on computing parameter gradients. However, obtaining the gradients is computationally prohibitive for LPMs, which necessitates the exploration of alternative approaches. To this end, we propose a unified framework for efficient fine-tuning and deployment of LPMs, termed LoRAPrune. We first design a PEFT-aware pruning criterion, which utilizes the values and gradients of Low-Rank Adaption (LoRA), rather than the gradients of pre-trained parameters for importance estimation. We then propose an iterative pruning procedure to remove redundant paramet
    
[^119]: 神经雕塑：通过修剪和网络分析揭示分层模块化任务结构

    Neural Sculpting: Uncovering hierarchically modular task structure through pruning and network analysis. (arXiv:2305.18402v1 [cs.LG])

    [http://arxiv.org/abs/2305.18402](http://arxiv.org/abs/2305.18402)

    本文提出了一种名为“神经雕塑”的方法，该方法通过神经网络的修剪和分析生成的图形结构来揭示任务的子函数的层次结构。该方法在布尔任务上得到了有效验证。

    

    自然目标函数和任务通常表现为分层模块化，可以将其分解为更简单的子函数以分层组织。这些子函数具有两个重要特征：它们有一组不同的输入（输入可分离性），并且在更高层次中作为输入被重用（可重复使用性）。以往的研究已经确立了分层模块化神经网络的优点，包括学习效率、泛化、多任务学习和可转移性。但是，对于给定的任务，如何识别潜在的子函数及其分层结构仍然具有挑战性。本文提出了一种名为“神经雕塑”的方法，该方法涉及神经网络的修剪和分析生成的图形结构，以揭示子函数的层次结构。我们在几个基准布尔任务上证明了神经雕塑的有效性，并表明它可以准确地识别任务的潜在模块化结构。此外，我们证明，修剪后的网络具有更好的泛化能力，更容易被人类解释。我们的方法也可以扩展到现实任务中，为复杂问题的潜在模块化结构提供洞察。

    Natural target functions and tasks typically exhibit hierarchical modularity - they can be broken down into simpler sub-functions that are organized in a hierarchy. Such sub-functions have two important features: they have a distinct set of inputs (input-separability) and they are reused as inputs higher in the hierarchy (reusability). Previous studies have established that hierarchically modular neural networks, which are inherently sparse, offer benefits such as learning efficiency, generalization, multi-task learning, and transferability. However, identifying the underlying sub-functions and their hierarchical structure for a given task can be challenging. The high-level question in this work is: if we learn a task using a sufficiently deep neural network, how can we uncover the underlying hierarchy of sub-functions in that task? As a starting point, we examine the domain of Boolean functions, where it is easier to determine whether a task is hierarchically modular. We propose an ap
    
[^120]: 一种元学习框架用于调整可信联邦学习保护机制的参数

    A Meta-learning Framework for Tuning Parameters of Protection Mechanisms in Trustworthy Federated Learning. (arXiv:2305.18400v1 [cs.LG])

    [http://arxiv.org/abs/2305.18400](http://arxiv.org/abs/2305.18400)

    提出了一个元学习框架，用于调整可信联邦学习保护机制的参数，以在隐私泄露、效用损失和效率降低之间进行权衡。

    

    可信联邦学习（TFL）通常利用保护机制来保证隐私安全。然而，保护机制不可避免地会引入效用损失或效率降低，同时保护数据隐私。因此，保护机制及其参数应该仔细选择，以在保护隐私泄露、效用损失和效率降低之间取得最佳平衡。为此，联邦学习从业者需要工具来衡量这三个因素，并优化它们之间的权衡，选择最适合手头应用的保护机制。基于这个要求，我们提出了一个框架，它(1)将TFL定义为找到保护机制来优化隐私泄露、效用损失和效率降低三者之间的权衡的问题；(2)正式定义了这三个因素的有界测量。然后，我们提出了一个元学习算法来近似解决此优化问题。

    Trustworthy Federated Learning (TFL) typically leverages protection mechanisms to guarantee privacy. However, protection mechanisms inevitably introduce utility loss or efficiency reduction while protecting data privacy. Therefore, protection mechanisms and their parameters should be carefully chosen to strike an optimal tradeoff between \textit{privacy leakage}, \textit{utility loss}, and \textit{efficiency reduction}. To this end, federated learning practitioners need tools to measure the three factors and optimize the tradeoff between them to choose the protection mechanism that is most appropriate to the application at hand. Motivated by this requirement, we propose a framework that (1) formulates TFL as a problem of finding a protection mechanism to optimize the tradeoff between privacy leakage, utility loss, and efficiency reduction and (2) formally defines bounded measurements of the three factors. We then propose a meta-learning algorithm to approximate this optimization proble
    
[^121]: 关于激活函数和规范化对初始化等距嵌入的影响

    On the impact of activation and normalization in obtaining isometric embeddings at initialization. (arXiv:2305.18399v1 [cs.LG])

    [http://arxiv.org/abs/2305.18399](http://arxiv.org/abs/2305.18399)

    本论文研究了深度神经网络中的 Gram 矩阵结构，证明了激活函数和层规范化结合使用可以在初始化时偏向指数级深度等距，从而弥补了现有理论的空白。

    

    本文探讨了深度神经网络中倒数第二个 Gram 矩阵的结构，该矩阵包含与一批输入对应的输出之间的成对内积。在几种架构中，观察到在初始化时该 Gram 矩阵会随着深度变得退化，从而严重减缓训练速度。规范化层如批处理规范化或层规范化，在防止秩崩溃问题方面起着关键作用。然而现有的理论结果无法全面覆盖广泛用于 transformer 中的层规范化和有限深度下规范化的量化偏差。为了解决这个问题，我们证明了在初始化时，结合激活函数层使用的层规范化可以使多层感知机的 Gram 矩阵偏向指数级深度等距，并使用激活函数的 Hermite 展开来量化这个速度，从而填补了现有理论的空白。

    In this paper, we explore the structure of the penultimate Gram matrix in deep neural networks, which contains the pairwise inner products of outputs corresponding to a batch of inputs. In several architectures it has been observed that this Gram matrix becomes degenerate with depth at initialization, which dramatically slows training. Normalization layers, such as batch or layer normalization, play a pivotal role in preventing the rank collapse issue. Despite promising advances, the existing theoretical results (i) do not extend to layer normalization, which is widely used in transformers, (ii) can not characterize the bias of normalization quantitatively at finite depth.  To bridge this gap, we provide a proof that layer normalization, in conjunction with activation layers, biases the Gram matrix of a multilayer perceptron towards isometry at an exponential rate with depth at initialization. We quantify this rate using the Hermite expansion of the activation function, highlighting th
    
[^122]: 图像生成中的不当行为缓解：反映世界丑陋是否有价值？

    Mitigating Inappropriateness in Image Generation: Can there be Value in Reflecting the World's Ugliness?. (arXiv:2305.18398v1 [cs.CV])

    [http://arxiv.org/abs/2305.18398](http://arxiv.org/abs/2305.18398)

    该论文研究了文本驱动的图像生成模型复制不适当人类行为的问题，并提出了抑制生成不适当内容的策略，该策略利用模型对世界丑陋的表现与人类偏好对齐。

    

    近期，文本驱动的图像生成模型在图像质量和文本对齐方面取得了惊人的成果，并因此在越来越多的应用程序中得到应用。由于它们高度依赖于从网络上随机抓取的数十亿大小的数据集，因此它们还会复制不适当的人类行为。具体而言，我们证明了各种文本到图像生成模型的大规模不当退化，因此需要在部署时对其进行监视和调节。为此，我们评估了推理时的缓解策略，以抑制不合适内容的生成。我们的研究结果表明，我们可以使用模型对世界丑陋的表现来将其与人类偏好进行对齐。

    Text-conditioned image generation models have recently achieved astonishing results in image quality and text alignment and are consequently employed in a fast-growing number of applications. Since they are highly data-driven, relying on billion-sized datasets randomly scraped from the web, they also reproduce inappropriate human behavior. Specifically, we demonstrate inappropriate degeneration on a large-scale for various generative text-to-image models, thus motivating the need for monitoring and moderating them at deployment. To this end, we evaluate mitigation strategies at inference to suppress the generation of inappropriate content. Our findings show that we can use models' representations of the world's ugliness to align them with human preferences.
    
[^123]: 利用社交媒体数据预测2023年土耳其总统选举结果

    Prediction of the 2023 Turkish Presidential Election Results Using Social Media Data. (arXiv:2305.18397v1 [cs.SI])

    [http://arxiv.org/abs/2305.18397](http://arxiv.org/abs/2305.18397)

    本文利用社交媒体数据和传统民调数据预测2023年土耳其总统选举结果，结果表明基于社交媒体交互数量的ARIMAX模型最优。

    

    社交媒体平台影响了政治竞选的方式，因此它们已成为政治家直接与公民互动的越来越重要的工具。许多国家的前几次选举表明，社交媒体数据可能会对选举结果产生重大影响。本研究旨在通过将来自不同平台的社交媒体数据与传统民调数据相结合，预测参加2023年土耳其选举的政党的得票率。我们采用的是基于体积的方法，而不是基于内容的方法来考虑社交媒体交互的数量。我们比较了几个不同时间窗口的预测模型。结果表明，在所有时间窗口中，ARIMAX模型优于其他算法。

    Social media platforms influence the way political campaigns are run and therefore they have become an increasingly important tool for politicians to directly interact with citizens. Previous elections in various countries have shown that social media data may significantly impact election results. In this study, we aim to predict the vote shares of parties participating in the 2023 elections in Turkey by combining social media data from various platforms together with traditional polling data. Our approach is a volume-based approach that considers the number of social media interactions rather than content. We compare several prediction models across varying time windows. Our results show that for all time windows, the ARIMAX model outperforms the other algorithms.
    
[^124]: LLM可以理解加密提示：面向隐私计算友好的Transformers

    LLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly Transformers. (arXiv:2305.18396v1 [cs.LG])

    [http://arxiv.org/abs/2305.18396](http://arxiv.org/abs/2305.18396)

    本文中，研究人员通过使用隐私计算友好的近似方法替换transformer架构中计算和通信密集的运算符，实现了大幅降低私有推断成本的效果，并在保持准确性的前提下实现了计算加速和通信开销降低。

    

    先前的研究尝试在服务器客户端环境中为基于transformer的大型语言模型 (LLMs) 构建私有推断框架，其中服务器持有模型参数，客户端输入私有数据进行推断。然而，当私有输入通过原始LLMs进行前向传播时，这些框架会产生显着的开销。在本文中，我们展示了通过用隐私计算友好的近似替换transformer架构中计算和通信密集的运算符可以大大降低私有推断成本，对模型性能的影响微乎其微。与最新的Iron（NeurIPS 2022）相比，我们的隐私计算友好的模型推断管道在计算上实现了$5 \times$的加速，在通信开销上实现了80\%的降低，同时几乎保持了相同的准确性。

    Prior works have attempted to build private inference frameworks for transformer-based large language models (LLMs) in a server-client setting, where the server holds the model parameters and the client inputs the private data for inference. However, these frameworks impose significant overhead when the private inputs are forward propagated through the original LLMs. In this paper, we show that substituting the computation- and communication-heavy operators in the transformer architecture with privacy-computing friendly approximations can greatly reduce the private inference costs with minor impact on model performance. Compared to the state-of-the-art Iron (NeurIPS 2022), our privacy-computing friendly model inference pipeline achieves a $5\times$ acceleration in computation and an 80\% reduction in communication overhead, while retaining nearly identical accuracy.
    
[^125]: 知识增强的推理蒸馏：面向知识密集型任务的小型语言模型

    Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks. (arXiv:2305.18395v1 [cs.CL])

    [http://arxiv.org/abs/2305.18395](http://arxiv.org/abs/2305.18395)

    本论文提出了一种KARD方法，可以通过向小型语言模型中加入从外部知识库检索到的增强知识来解决知识密集型推理任务中小型语言模型记忆能力有限的问题。

    

    大型语言模型在需要复合知识理解的知识密集型推理任务中表现出了良好的性能。但是，由于计算要求高且涉及数据隐私，将此类模型部署到现实世界的应用中可能会具有挑战性。以往的研究专注于通过微调具有标记数据或蒸馏大型语言模型来构建任务特定的小型语言模型，但是由于小型语言模型在记忆所需知识方面的能力有限，这些方法不适用于知识密集型推理任务。在理论分析的基础上，我们提出了一种名为知识增强的推理蒸馏 (KARD) 的新方法，该方法微调小型语言模型以生成从外部知识库检索到的增强知识的依据。此外，我们还提出了一个神经重排器，用于获得与依据生成相关的文档。我们实证表明，KARD在三项知识密集型任务上显着优于以前的方法，并且在模型尺寸相同的情况下可以达到与LLMs可比较的结果。

    Large Language Models (LLMs) have shown promising performance in knowledge-intensive reasoning tasks that require a compound understanding of knowledge. However, deployment of the LLMs in real-world applications can be challenging due to their high computational requirements and concerns on data privacy. Previous studies have focused on building task-specific small language models (LMs) by fine-tuning them with labeled data or distilling LLMs. However, these approaches are ill-suited for knowledge-intensive reasoning tasks due to the limited capacity of small LMs in memorizing the knowledge required. Motivated by our theoretical analysis on memorization, we propose Knowledge-Augmented Reasoning Distillation (KARD), a novel method that fine-tunes small LMs to generate rationales with augmented knowledge retrieved from an external knowledge base. Moreover, we further propose a neural reranker to obtain documents relevant to rationale generation. We empirically show that KARD significantl
    
[^126]: 基于双层学习的最优正则化参数研究

    On Optimal Regularization Parameters via Bilevel Learning. (arXiv:2305.18394v1 [math.OC])

    [http://arxiv.org/abs/2305.18394](http://arxiv.org/abs/2305.18394)

    本文提出了一个利用双层学习确定正则化参数的方法，并提出了一个新的表征正确参数的条件。

    

    变分正则化常用于解线性反问题，它通过添加正则化项来提高先验信息质量，并通过正则化参数加以权衡，而合适的正则化参数的选择至关重要。现有的策略例如差异原则和L-曲线可以用于确定合适的参数值，但是近年来，一种叫做双层学习的监督机器学习方法被用于确定最优参数。虽然以前的策略有各种理论结果，但在这种情况下，双层学习的良好性质仍然是一个发展中的领域。本文提出了一个更好的条件来表征确定正则化参数的正值性。

    Variational regularization is commonly used to solve linear inverse problems, and involves augmenting a data fidelity by a regularizer. The regularizer is used to promote a priori information, and is weighted by a regularization parameter. Selection of an appropriate regularization parameter is critical, with various choices leading to very different reconstructions. Existing strategies such as the discrepancy principle and L-curve can be used to determine a suitable parameter value, but in recent years a supervised machine learning approach called bilevel learning has been employed. Bilevel learning is a powerful framework to determine optimal parameters, and involves solving a nested optimisation problem. While previous strategies enjoy various theoretical results, the well-posedness of bilevel learning in this setting is still a developing field. One necessary property is positivity of the determined regularization parameter. In this work, we provide a new condition that better char
    
[^127]: 利用不确定性量化的发音准确度评估方法对运动障碍患者的语音清晰度进行评估

    Speech Intelligibility Assessment of Dysarthric Speech by using Goodness of Pronunciation with Uncertainty Quantification. (arXiv:2305.18392v1 [cs.SD])

    [http://arxiv.org/abs/2305.18392](http://arxiv.org/abs/2305.18392)

    本文提出了一种利用不确定性量化的发音准确度评估方法，能够有效评估运动障碍患者的语音清晰度，其中，利用先验规范化的最大对数几率发音准确度（maxlogit GoP）取得了最佳表现。

    

    本文提出了一种改进的发音准确度评估方法——利用不确定性量化（UQ）来自动评估运动障碍患者的语音清晰度。当前的发音准确度评估方法依赖于神经网络驱动的自负预测，这对于评估运动障碍患者的语音来说不太适用，因为其与健康人的语音存在显著的声学差异。

    This paper proposes an improved Goodness of Pronunciation (GoP) that utilizes Uncertainty Quantification (UQ) for automatic speech intelligibility assessment for dysarthric speech. Current GoP methods rely heavily on neural network-driven overconfident predictions, which is unsuitable for assessing dysarthric speech due to its significant acoustic differences from healthy speech. To alleviate the problem, UQ techniques were used on GoP by 1) normalizing the phoneme prediction (entropy, margin, maxlogit, logit-margin) and 2) modifying the scoring function (scaling, prior normalization). As a result, prior-normalized maxlogit GoP achieves the best performance, with a relative increase of 5.66%, 3.91%, and 23.65% compared to the baseline GoP for English, Korean, and Tamil, respectively. Furthermore, phoneme analysis is conducted to identify which phoneme scores significantly correlate with intelligibility scores in each language.
    
[^128]: MemeGraphs: 将网络文化表情包与知识图谱相连

    MemeGraphs: Linking Memes to Knowledge Graphs. (arXiv:2305.18391v1 [cs.LG])

    [http://arxiv.org/abs/2305.18391](http://arxiv.org/abs/2305.18391)

    该论文提出了一种使用场景图和知识图谱结构化表达网络文化表情包的方法，并将其用于分类。结果显示该方法相比使用学习表达式的模型有所改善。

    

    网络文化表情包是一种在社交媒体和互联网上流行的传播趋势和观点的形式，结合了图像和文本的模式。它们可以表达幽默和讽刺，但也可能含有冒犯性的内容。自动分析和分类网络文化表情包具有挑战性，因为其解释依赖于对视觉元素、语言和背景知识的理解。因此，重要的是有意义地表示这些来源以及它们之间的交互，以便将表情包作为整体分类。在这项工作中，我们提出使用场景图作为表示图像中物体及其视觉关系的结构化表达方式，并将知识图谱作为网络文化表情包分类的结构化表示，使用基于Transformer的架构。我们将我们的方法与ImgBERT进行比较，后者使用仅学习（而不是结构化）的表达式进行多模式建模，我们观察到始终有所改善。我们还提供了一个具有人工图注释的数据集，供比较。

    Memes are a popular form of communicating trends and ideas in social media and on the internet in general, combining the modalities of images and text. They can express humor and sarcasm but can also have offensive content. Analyzing and classifying memes automatically is challenging since their interpretation relies on the understanding of visual elements, language, and background knowledge. Thus, it is important to meaningfully represent these sources and the interaction between them in order to classify a meme as a whole. In this work, we propose to use scene graphs, that express images in terms of objects and their visual relations, and knowledge graphs as structured representations for meme classification with a Transformer-based architecture. We compare our approach with ImgBERT, a multimodal model that uses only learned (instead of structured) representations of the meme, and observe consistent improvements. We further provide a dataset with human graph annotations that we compa
    
[^129]: 预训练Transformers中的自发模块化

    Emergent Modularity in Pre-trained Transformers. (arXiv:2305.18390v1 [cs.CL])

    [http://arxiv.org/abs/2305.18390](http://arxiv.org/abs/2305.18390)

    本论文研究了预训练Transformers中的自发模块化现象，发现神经元可以进行功能专业化，并通过聚类建立起模块化结构，此结构可被有效扰动。

    

    本论文研究了预训练Transformers中的模块化特征，这是人脑中常见的特点，被认为对于普遍智能至关重要。本文主要考虑了模块化的两个主要特征：（1）神经元的功能专业化：我们评估了每个神经元是否主要专业化于某一功能，结果表明是的。（2）基于功能聚类的神经元分组：我们探究了将神经元按功能分组的结构寻找方法，每个模块均为其相应功能工作。鉴于可能存在的大量结构，我们将重点放在了分层专家模型身上，并将神经元划分为专家，通常为不同的输入激活不同的专家。实验结果表明存在功能专家，聚集了某一功能的神经元。此外，扰动功能专家的激活显著影响了相应的f键

    This work examines the presence of modularity in pre-trained Transformers, a feature commonly found in human brains and thought to be vital for general intelligence. In analogy to human brains, we consider two main characteristics of modularity: (1) functional specialization of neurons: we evaluate whether each neuron is mainly specialized in a certain function, and find that the answer is yes. (2) function-based neuron grouping: we explore finding a structure that groups neurons into modules by function, and each module works for its corresponding function. Given the enormous amount of possible structures, we focus on Mixture-of-Experts as a promising candidate, which partitions neurons into experts and usually activates different experts for different inputs. Experimental results show that there are functional experts, where clustered are the neurons specialized in a certain function. Moreover, perturbing the activations of functional experts significantly affects the corresponding f
    
[^130]: AnoRand: 一种半监督的深度学习异常检测方法，利用随机标注

    AnoRand: A Semi Supervised Deep Learning Anomaly Detection Method by Random Labeling. (arXiv:2305.18389v1 [cs.LG])

    [http://arxiv.org/abs/2305.18389](http://arxiv.org/abs/2305.18389)

    本文提出了一种名为AnoRand的半监督深度学习异常检测方法，通过深度架构和随机合成标签生成相结合，能够尽可能地学习一个类别。

    

    异常检测或者更一般的离群值检测是理论和应用机器学习中最受欢迎和具有挑战性的主题之一。其主要挑战在于通常访问非常少的有标签数据或根本没有标签。在本文中，我们通过将深度学习架构与随机合成标签生成相结合的方式，提出了一种新的半监督异常检测方法 \textbf{AnoRand}。所提出的架构具有两个构建块：(1) 由前馈神经网络(feed forward ferceptron)组成的噪声检测(ND)块和(2) 自编码器(AE)块。这种新架构的主要思想是利用自编码器在潜空间中表示数据的能力和前馈神经网络在数据高度不平衡时学习一个类别的能力来尽可能地学习一个类别 (例如，在异常检测的情况下是大多数类别)。首先，我们通过随机扰动 (添加噪声) 少量样本 (例如。

    Anomaly detection or more generally outliers detection is one of the most popular and challenging subject in theoretical and applied machine learning. The main challenge is that in general we have access to very few labeled data or no labels at all. In this paper, we present a new semi-supervised anomaly detection method called \textbf{AnoRand} by combining a deep learning architecture with random synthetic label generation. The proposed architecture has two building blocks: (1) a noise detection (ND) block composed of feed forward ferceptron and (2) an autoencoder (AE) block. The main idea of this new architecture is to learn one class (e.g. the majority class in case of anomaly detection) as well as possible by taking advantage of the ability of auto encoders to represent data in a latent space and the ability of Feed Forward Perceptron (FFP) to learn one class when the data is highly imbalanced. First, we create synthetic anomalies by randomly disturbing (add noise) few samples (e.g
    
[^131]: 量化时间差分学习在价值估计中的统计优势

    The Statistical Benefits of Quantile Temporal-Difference Learning for Value Estimation. (arXiv:2305.18388v1 [cs.LG])

    [http://arxiv.org/abs/2305.18388](http://arxiv.org/abs/2305.18388)

    本文研究了强化学习中的时间差分策略评估问题，分析了量化时间差分学习算法在任务中的应用。结果表明，即使从实践者没有超过平均回报之外的回报分布的兴趣之处，在表格设置中，QTD也可以提供比传统TD学习等方法更好的性能。

    

    本文研究强化学习中基于时间差分的策略评估问题，特别是分析了一种分布式强化学习算法——量化时间差分学习（QTD）在这个任务中的应用。我们得出了一个惊人的结论：即使从实践者没有超过平均回报之外的回报分布的兴趣之处，在表格设置中，QTD（学习关于全部回报分布的预测）也可以提供比诸如传统TD学习（仅预测平均回报）等方法更好的性能。

    We study the problem of temporal-difference-based policy evaluation in reinforcement learning. In particular, we analyse the use of a distributional reinforcement learning algorithm, quantile temporal-difference learning (QTD), for this task. We reach the surprising conclusion that even if a practitioner has no interest in the return distribution beyond the mean, QTD (which learns predictions about the full distribution of returns) may offer performance superior to approaches such as classical TD learning, which predict only the mean return, even in the tabular setting.
    
[^132]: 使用生成对抗网络增强角色设计师的创造力

    Augmenting Character Designers Creativity Using Generative Adversarial Networks. (arXiv:2305.18387v1 [cs.HC])

    [http://arxiv.org/abs/2305.18387](http://arxiv.org/abs/2305.18387)

    本文探讨了如何使用生成对抗网络来增强角色设计师的创造力，在多媒体项目中构思新的角色，研究结果表明GAN可以提供新的、多样化的设计选项，同时减少设计时间和工作量。

    

    随着生成对抗网络（GANs）的最近进展，由于其关键特征的广泛应用，吸引了不同领域的研究者的关注。最近的GAN专注于现实主义，然而，对于某些领域，如本项工作，生成超现实的输出并不是首要任务。在此，生成的结果被用作认知组成部分，以增强角色设计师的创造力，为不同的多媒体项目构思新的角色。为了选择最适合这种创意环境的GAN，我们首先比较了不同的GAN架构以及它们在使用单个图形处理单元对新的视觉角色数据集进行从头训练时的性能。我们还探索了替代技术，如迁移学习和数据增强，以克服计算资源限制，这是许多研究人员面临的挑战。此外，还提出了混合方法，将手工草图与GAN生成的输出相结合，以增强创意探索，充分利用人类设计师和自主系统的优势。初步结果表明，GAN确实可以增强角色设计师的创造力，提供新的、多样化的设计选项，同时减少设计时间和工作量。

    Recent advances in Generative Adversarial Networks (GANs) continue to attract the attention of researchers in different fields due to the wide range of applications devised to take advantage of their key features. Most recent GANs are focused on realism, however, generating hyper-realistic output is not a priority for some domains, as in the case of this work. The generated outcomes are used here as cognitive components to augment character designers creativity while conceptualizing new characters for different multimedia projects. To select the best-suited GANs for such a creative context, we first present a comparison between different GAN architectures and their performance when trained from scratch on a new visual characters dataset using a single Graphics Processing Unit. We also explore alternative techniques, such as transfer learning and data augmentation, to overcome computational resource limitations, a challenge faced by many researchers in the domain. Additionally, mixed me
    
[^133]: 利用自编码器和生成对抗网络的协同框架合成机翼空气动力学计算流体力学结果

    A Synergistic Framework Leveraging Autoencoders and Generative Adversarial Networks for the Synthesis of Computational Fluid Dynamics Results in Aerofoil Aerodynamics. (arXiv:2305.18386v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2305.18386](http://arxiv.org/abs/2305.18386)

    本研究提出了一种创新方法，协同利用自编码器和生成对抗网络来生成CFD结果，以减少气动预测中的时间和成本方面具有深远的潜力。

    

    在计算流体力学领域，准确预测气动行为在机翼设计和优化中起着关键作用。本研究提出了一种创新方法，协同地结合自编码器和生成对抗网络来生成CFD结果。我们的创新框架利用自编码器的内在能力将机翼几何形状编码成一个具有压缩性和信息价值的20长度向量表示。随后，一个有条件的GAN网络将其翻译成精确的压力分布图，考虑到固定的风速、攻角和湍流水平规范。训练过程利用从JavaFoil软件获得的细心策划的数据集，涵盖广泛的机翼几何形状。所提出的方法在减少气动预测中的时间和成本方面具有深远的潜力，使得评估效率更高。

    In the realm of computational fluid dynamics (CFD), accurate prediction of aerodynamic behaviour plays a pivotal role in aerofoil design and optimization. This study proposes a novel approach that synergistically combines autoencoders and Generative Adversarial Networks (GANs) for the purpose of generating CFD results. Our innovative framework harnesses the intrinsic capabilities of autoencoders to encode aerofoil geometries into a compressed and informative 20-length vector representation. Subsequently, a conditional GAN network adeptly translates this vector into precise pressure-distribution plots, accounting for fixed wind velocity, angle of attack, and turbulence level specifications. The training process utilizes a meticulously curated dataset acquired from JavaFoil software, encompassing a comprehensive range of aerofoil geometries. The proposed approach exhibits profound potential in reducing the time and costs associated with aerodynamic prediction, enabling efficient evaluati
    
[^134]: 自注意力双重嵌入：适用于异质性图的图神经网络

    Self-attention Dual Embedding for Graphs with Heterophily. (arXiv:2305.18385v1 [cs.LG])

    [http://arxiv.org/abs/2305.18385](http://arxiv.org/abs/2305.18385)

    本研究提出了一种新颖的图神经网络，采用自注意力机制，适用于异质性图和同质性图，并在许多标准数据集上展示出最先进的性能。

    

    图神经网络（GNNs）在节点分类任务中取得了重大成功。GNNs通常假设图是同质的，即相邻节点很可能属于相同的类别。然而，许多真实世界的图都是异质的，这导致使用标准的GNNs时分类精度要低得多。在本文中，我们设计了一种新颖的GNN，它对异质性和同质性图都有效。我们的工作基于三个主要观察结果。首先，我们展示了在不同的图中，节点特征和图拓扑提供不同数量的信息，因此应该独立编码并以自适应方式优先级化。其次，我们展示了当传播图拓扑信息时允许负的注意权重可以提高精度。最后，我们展示了节点之间不对称的注意权重是有帮助的。我们设计了一种GNN，利用这些观察结果通过新颖的自注意力机制。我们评估了我们的算法在一些标准的节点分类数据集上，并展示了在同质性和异质性图上的最新性能。

    Graph Neural Networks (GNNs) have been highly successful for the node classification task. GNNs typically assume graphs are homophilic, i.e. neighboring nodes are likely to belong to the same class. However, a number of real-world graphs are heterophilic, and this leads to much lower classification accuracy using standard GNNs. In this work, we design a novel GNN which is effective for both heterophilic and homophilic graphs. Our work is based on three main observations. First, we show that node features and graph topology provide different amounts of informativeness in different graphs, and therefore they should be encoded independently and prioritized in an adaptive manner. Second, we show that allowing negative attention weights when propagating graph topology information improves accuracy. Finally, we show that asymmetric attention weights between nodes are helpful. We design a GNN which makes use of these observations through a novel self-attention mechanism. We evaluate our algor
    
[^135]: 增量学习器的后门攻击：一项实证评估研究

    Backdoor Attacks Against Incremental Learners: An Empirical Evaluation Study. (arXiv:2305.18384v1 [cs.CR])

    [http://arxiv.org/abs/2305.18384](http://arxiv.org/abs/2305.18384)

    本文提出了增量学习器的后门攻击可能存在的安全风险，并实证揭示了11个典型的增量学习器对基于中毒的后门攻击的高度脆弱性。

    

    已经提出了大量的增量学习算法，以缓解在时间序列上处理顺序数据时出现的灾难性遗忘问题。然而，增量学习器的对抗鲁棒性尚未得到广泛验证，存在潜在的安全风险。具体而言，对于基于中毒的后门攻击，我们认为增量学习中流式数据的本质为对手提供了很大的便利，通过数据污染，对手可以在任何时间或时间序列上创建分布式和跨任务攻击，从而影响任何未知的先前或后续任务，并且仅需要注入极小数量的后门样本 (例如，根据我们的观察，仅需要注入0.1%）。为了吸引研究社区的关注，在本文中，我们从三个学习场景出发，实证揭示了11个典型的增量学习器对基于中毒的后门攻击的高度脆弱性，特别是跨任务泛化效应。

    Large amounts of incremental learning algorithms have been proposed to alleviate the catastrophic forgetting issue arises while dealing with sequential data on a time series. However, the adversarial robustness of incremental learners has not been widely verified, leaving potential security risks. Specifically, for poisoning-based backdoor attacks, we argue that the nature of streaming data in IL provides great convenience to the adversary by creating the possibility of distributed and cross-task attacks -- an adversary can affect \textbf{any unknown} previous or subsequent task by data poisoning \textbf{at any time or time series} with extremely small amount of backdoor samples injected (e.g., $0.1\%$ based on our observations). To attract the attention of the research community, in this paper, we empirically reveal the high vulnerability of 11 typical incremental learners against poisoning-based backdoor attack on 3 learning scenarios, especially the cross-task generalization effect 
    
[^136]: 神经网络修剪的三重模型

    A Three-regime Model of Network Pruning. (arXiv:2305.18383v1 [stat.ML])

    [http://arxiv.org/abs/2305.18383](http://arxiv.org/abs/2305.18383)

    该论文提出了一种基于学习统计力学的现象学模型，使用类似温度和负载的模型参数来建模神经网络训练超参数对修剪性能的影响，通过分类修剪后神经网络损失景观的全局结构构建了一个三重模型，揭示了修剪的优化过程以及对神经网络损失景观的变化规律。

    

    最近的研究强调了训练超参数（例如训练轮数）对机器学习模型修剪的影响，然而如何精确预测调整某一特定超参数对修剪的影响仍具有挑战性。为了解决这个问题，本文提出了一种基于学习统计力学的现象学模型，使用类似温度和负载的模型参数来建模神经网络训练超参数对修剪性能的影响。我们发现了一个关键的实证结果：根据修剪后的模型中的一种负载类参数的值，当增加修剪前模型中一种类似温度的参数的值时，修剪性能可能会得到优化或损害。基于这种转变，我们通过分类修剪后神经网络损失景观的全局结构构建了一个三重模型。该模型揭示了修剪的优化过程以及与修剪相关的神经网络损失景观的变化规律。

    Recent work has highlighted the complex influence training hyperparameters, e.g., the number of training epochs, can have on the prunability of machine learning models. Perhaps surprisingly, a systematic approach to predict precisely how adjusting a specific hyperparameter will affect prunability remains elusive. To address this gap, we introduce a phenomenological model grounded in the statistical mechanics of learning. Our approach uses temperature-like and load-like parameters to model the impact of neural network (NN) training hyperparameters on pruning performance. A key empirical result we identify is a sharp transition phenomenon: depending on the value of a load-like parameter in the pruned model, increasing the value of a temperature-like parameter in the pre-pruned model may either enhance or impair subsequent pruning performance. Based on this transition, we build a three-regime model by taxonomizing the global structure of the pruned NN loss landscape. Our model reveals tha
    
[^137]: 适应性稀疏度训练过程中的变化，用于利用Transformer进行高效时间序列预测

    Adaptive Sparsity Level during Training for Efficient Time Series Forecasting with Transformers. (arXiv:2305.18382v1 [cs.LG])

    [http://arxiv.org/abs/2305.18382](http://arxiv.org/abs/2305.18382)

    本文提出了“具有自适应稀疏度级别的修剪”(PALS), 通过稀疏训练和训练期间方法中的“扩张”机制，在Transformer模型中实现高效的时间序列预测。

    

    高效的时间序列预测对于深度神经网络应用变得至关重要。通过稀疏连接和减小模型尺寸，可以实现DNN的高效性。然而，在训练过程中自动确定稀疏度仍然是一个具有挑战性的任务，因为不同数据集中的损失稀疏度权衡是异构的。本文提出了“具有自适应稀疏度级别的修剪”(PALS)，来自动寻求损失和稀疏性之间的最佳平衡，无需预定义稀疏水平。PALS从稀疏训练和训练期间方法中吸取灵感。它在训练稀疏神经网络中引入了新颖的“扩张”机制，允许模型动态收缩、扩张或保持稳定，以找到适当的稀疏度。本文专注于在以Transformer著称的模型中实现效率, 该模型以其出色的时间序列预测表现而闻名。

    Efficient time series forecasting has become critical for real-world applications, particularly with deep neural networks (DNNs). Efficiency in DNNs can be achieved through sparse connectivity and reducing the model size. However, finding the sparsity level automatically during training remains a challenging task due to the heterogeneity in the loss-sparsity tradeoffs across the datasets. In this paper, we propose \enquote{\textbf{P}runing with \textbf{A}daptive \textbf{S}parsity \textbf{L}evel} (\textbf{PALS}), to automatically seek an optimal balance between loss and sparsity, all without the need for a predefined sparsity level. PALS draws inspiration from both sparse training and during-training methods. It introduces the novel "expand" mechanism in training sparse neural networks, allowing the model to dynamically shrink, expand, or remain stable to find a proper sparsity level. In this paper, we focus on achieving efficiency in transformers known for their excellent time series f
    
[^138]: 从大型矿石中提炼黄金: 基于关键样本选择的高效数据集蒸馏

    Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection. (arXiv:2305.18381v1 [cs.LG])

    [http://arxiv.org/abs/2305.18381](http://arxiv.org/abs/2305.18381)

    研究提出了一种基于选择最有价值的样本的方法，以扩展现有的蒸馏算法，从而更好地利用训练样本，显著降低训练成本，拓展对更大更多元化数据集的数据集蒸馏，并持续提高性能。

    

    数据效率学习近年来备受关注，特别是对于拥有大量多模型的现在，数据集蒸馏可以成为有效的解决方案。然而，数据集蒸馏过程本身仍然非常低效。在本文中，我们首先引用信息理论来建模蒸馏问题，观察到数据集蒸馏中存在严重的数据冗余，我们提出了一种方法来扩展现有的蒸馏算法，以便通过选择最有价值的样本来更好地利用这些训练样本。我们进一步对样本选择进行全面分析，并验证了其优化过程。这种新策略能够显著减少训练成本，扩大现有算法范围以对更庞大和多元化的数据集进行数据集蒸馏，例如，在某些情况下，只需要0.04％的训练数据就足以保持可比的蒸馏效果。此外，我们的策略能够持续提高性能，其贡献可能为蒸馏过程的动力学开辟新的分析方法。

    Data-efficient learning has drawn significant attention, especially given the current trend of large multi-modal models, where dataset distillation can be an effective solution. However, the dataset distillation process itself is still very inefficient. In this work, we model the distillation problem with reference to information theory. Observing that severe data redundancy exists in dataset distillation, we argue to put more emphasis on the utility of the training samples. We propose a family of methods to exploit the most valuable samples, which is validated by our comprehensive analysis of the optimal data selection. The new strategy significantly reduces the training cost and extends a variety of existing distillation algorithms to larger and more diversified datasets, e.g. in some cases only 0.04% training data is sufficient for comparable distillation performance. Moreover, our strategy consistently enhances the performance, which may open up new analyses on the dynamics of dist
    
[^139]: 基于潜力的信用分配方法用于自动驾驶车辆的协作强化学习测试

    Potential-based Credit Assignment for Cooperative RL-based Testing of Autonomous Vehicles. (arXiv:2305.18380v1 [cs.LG])

    [http://arxiv.org/abs/2305.18380](http://arxiv.org/abs/2305.18380)

    本文提出了一种协作强化学习方法，利用基于潜力的奖励塑形来解决自动驾驶车辆中多个代理的信用分配问题。

    

    尽管自动驾驶车辆(AVs)在普通实际情况下表现出色，但在一些意外情况下，它们的不理性行为引发严重安全关切。本文引入协作强化学习(RL)的概念，为AV规划和决策模块生成具有挑战性的测试用例。协作RL面临的关键挑战之一是信用分配问题，在交通场景中为多个代理分配奖励，考虑所有参数和时序，这是一个非平凡的问题。为了解决这个问题，我们提出了一种新颖的基于潜力的奖励塑形方法，借鉴了反事实分析，用于解决信用分配问题。在模拟环境中的评估证明了我们提出的方法优于其他使用局部和全局奖励的方法。

    While autonomous vehicles (AVs) may perform remarkably well in generic real-life cases, their irrational action in some unforeseen cases leads to critical safety concerns. This paper introduces the concept of collaborative reinforcement learning (RL) to generate challenging test cases for AV planning and decision-making module. One of the critical challenges for collaborative RL is the credit assignment problem, where a proper assignment of rewards to multiple agents interacting in the traffic scenario, considering all parameters and timing, turns out to be non-trivial. In order to address this challenge, we propose a novel potential-based reward-shaping approach inspired by counterfactual analysis for solving the credit-assignment problem. The evaluation in a simulated environment demonstrates the superiority of our proposed approach against other methods using local and global rewards.
    
[^140]: 精确增广拉格朗日和随机迭代草图算法求解约束优化问题

    Constrained Optimization via Exact Augmented Lagrangian and Randomized Iterative Sketching. (arXiv:2305.18379v1 [math.OC])

    [http://arxiv.org/abs/2305.18379](http://arxiv.org/abs/2305.18379)

    本文提出了一种自适应的不精确牛顿法来求解等式约束的非线性、非凸优化问题，通过随机迭代草图求解增广拉格朗日牛顿系统，并通过在精确增广拉格朗日优势函数上执行线搜索来选择合适的步长。该方法具有高效、鲁棒性好的特点。

    

    本文考虑解决等式约束的非线性、非凸优化问题。这类问题在机器学习和工程领域的各种应用中广泛出现，包括受约束的深度神经网络、最优控制和PDE约束优化。我们针对这类问题开发了一种自适应的不精确牛顿法。在每次迭代中，我们通过随机迭代草图求解增广拉格朗日牛顿系统，并通过在精确增广拉格朗日优势函数上执行线搜索来选择合适的步长。当配备适当的草图矩阵时，随机求解器相对于确定性线性系统求解器具有明显优势，可以显著减少每次迭代的浮点运算复杂度和存储成本。我们的方法自适应地控制随机求解器的精度和增广拉格朗日的惩罚参数，以确保不精确的牛顿方向是精确增广拉格朗日函数的下降方向。理论分析和数值实验均证明了所提出方法的效率和鲁棒性。

    We consider solving equality-constrained nonlinear, nonconvex optimization problems. This class of problems appears widely in a variety of applications in machine learning and engineering, ranging from constrained deep neural networks, to optimal control, to PDE-constrained optimization. We develop an adaptive inexact Newton method for this problem class. In each iteration, we solve the Lagrangian Newton system inexactly via a randomized iterative sketching solver, and select a suitable stepsize by performing line search on an exact augmented Lagrangian merit function. The randomized solvers have advantages over deterministic linear system solvers by significantly reducing per-iteration flops complexity and storage cost, when equipped with suitable sketching matrices. Our method adaptively controls the accuracy of the randomized solver and the penalty parameters of the exact augmented Lagrangian, to ensure that the inexact Newton direction is a descent direction of the exact augmented 
    
[^141]: 通过潜在量化进行解缠

    Disentanglement via Latent Quantization. (arXiv:2305.18378v1 [cs.LG])

    [http://arxiv.org/abs/2305.18378](http://arxiv.org/abs/2305.18378)

    本文通过潜在量化的方式实现了解缠表示学习，并通过严格的交流瓶颈和强大的模型规范化成功将数据进行了组合编码和解码，最终在多个基准数据集上实现了最先进的解缠性能，并提高了标准VAE模型学习表征的可解释性。

    

    在解缠表示学习中，模型需要将数据集的基础变化因素分开并独立地表示出来，而模型并没有提供有关这些因素的真实信息，归纳偏见在实现解缠方面发挥着重要作用。在本文中，我们通过施加严格的交流瓶颈和强大的模型规范化，构建了一种朝着组合编码和解码数据的归纳偏见。具体来说，我们对潜在维度进行可学习的离散编码，并为每个维度应用一个单独的标量码书。潜在量化迫使编码器在许多数据点上使用少量潜在值，从而使解码器能够为每个值分配一致的含义。规范化有助于将模型引向这种简明策略。我们在多个基准数据集上展示了该方法的广泛应用性，并且展示了我们的方法显著提高了一系列标准VAE模型学习的表征的可解释性。

    In disentangled representation learning, a model is asked to tease apart a dataset's underlying sources of variation and represent them independently of one another. Since the model is provided with no ground truth information about these sources, inductive biases take a paramount role in enabling disentanglement. In this work, we construct an inductive bias towards compositionally encoding and decoding data by enforcing a harsh communication bottleneck. Concretely, we do this by (i) quantizing the latent space into learnable discrete codes with a separate scalar codebook per dimension and (ii) applying strong model regularization via an unusually high weight decay. Intuitively, the quantization forces the encoder to use a small number of latent values across many datapoints, which in turn enables the decoder to assign a consistent meaning to each value. Regularization then serves to drive the model towards this parsimonious strategy. We demonstrate the broad applicability of this appr
    
[^142]: BadLabel: 评估与增强标签噪声学习的鲁棒性视角

    BadLabel: A Robust Perspective on Evaluating and Enhancing Label-noise Learning. (arXiv:2305.18377v1 [cs.LG])

    [http://arxiv.org/abs/2305.18377](http://arxiv.org/abs/2305.18377)

    本文介绍了一种新的标签噪声类型BadLabel，它通过标签翻转攻击显著降低现有标签噪声学习（LNL）算法性能，因此提出了一种鲁棒的LNL方法，表现出在六个数据集上最先进的性能。

    

    标签噪声学习旨在提高模型对带有噪声标签的训练数据的泛化能力。为了推进实用的标签噪声学习算法，研究人员提出了不同的标签噪声类型，从条件类噪声到实例依赖噪声不等。本文介绍了一种新的标签噪声类型BadLabel，可以通过标签翻转攻击显著降低现有LNL算法的性能。BadLabel基于标准分类的标签翻转攻击进行制作，选择特定样本并将其标签翻转为其他标签，使得干净标签和噪声标签的损失值变得无法区分。为了解决BadLabel带来的挑战，我们进一步提出了一种鲁棒的LNL方法，每个epoch对标签进行敌对扰动，使干净标签和噪声标签的损失值再次可区分。一旦选择了一小部分（大多数）干净标记数据，我们可以将半监督翻译技术应用到LNL中。我们在合成和真实数据集上评估了BadLabel类型和提出的鲁棒LNL方法。实验表明，在六个数据集上，BadLabel可以将七个现有LNL算法的性能降低高达60％。此外，我们提出的方法在合成和真实数据集上均表现出最先进的性能，比六个数据集上的七个现有LNL算法性能提高多达17％。

    Label-noise learning (LNL) aims to increase the model's generalization given training data with noisy labels. To facilitate practical LNL algorithms, researchers have proposed different label noise types, ranging from class-conditional to instance-dependent noises. In this paper, we introduce a novel label noise type called BadLabel, which can significantly degrade the performance of existing LNL algorithms by a large margin. BadLabel is crafted based on the label-flipping attack against standard classification, where specific samples are selected and their labels are flipped to other labels so that the loss values of clean and noisy labels become indistinguishable. To address the challenge posed by BadLabel, we further propose a robust LNL method that perturbs the labels in an adversarial manner at each epoch to make the loss values of clean and noisy labels again distinguishable. Once we select a small set of (mostly) clean labeled data, we can apply the techniques of semi-supervised
    
[^143]: 快速而准确的PARAFAC2双向流算法及其在非规则张量中的应用

    Fast and Accurate Dual-Way Streaming PARAFAC2 for Irregular Tensors -- Algorithm and Application. (arXiv:2305.18376v1 [cs.LG])

    [http://arxiv.org/abs/2305.18376](http://arxiv.org/abs/2305.18376)

    本文提出了一种名为Dash的有效而准确的PARAFAC2分解方法，其采用两阶段ALS算法，在双向流处理非规则张量时高效地处理新行，同时提供了在该应用场景下的异常检测度量方法，实验结果显示Dash方法优于现有的方法。

    

    本文针对一种非规则张量的双向流处理，其中张量的两个维度在时间上增加，提出了一种有效而准确的PARAFAC2分解方法Dash。该方法采用两阶段ALS算法，在处理新行时效率高。同时，本文还提出了在双向流上检测异常情况的度量方法。实验结果表明，Dash方法在性能和检测效果上均优于现有的静态和流式PARAFAC2分解方法以及异常检测方法。

    How can we efficiently and accurately analyze an irregular tensor in a dual-way streaming setting where the sizes of two dimensions of the tensor increase over time? What types of anomalies are there in the dual-way streaming setting? An irregular tensor is a collection of matrices whose column lengths are the same while their row lengths are different. In a dual-way streaming setting, both new rows of existing matrices and new matrices arrive over time. PARAFAC2 decomposition is a crucial tool for analyzing irregular tensors. Although real-time analysis is necessary in the dual-way streaming, static PARAFAC2 decomposition methods fail to efficiently work in this setting since they perform PARAFAC2 decomposition for accumulated tensors whenever new data arrive. Existing streaming PARAFAC2 decomposition methods work in a limited setting and fail to handle new rows of matrices efficiently. In this paper, we propose Dash, an efficient and accurate PARAFAC2 decomposition method working in 
    
[^144]: 学习跳跃: 薄化和加厚潜在计数用于生成建模

    Learning to Jump: Thinning and Thickening Latent Counts for Generative Modeling. (arXiv:2305.18375v1 [cs.LG])

    [http://arxiv.org/abs/2305.18375](http://arxiv.org/abs/2305.18375)

    本文探讨了如何使用学习跳跃方法来生成建模各种类型的数据，特别是对于计数和非负连续数据等高稀疏度、倾斜度、重尾度或过度分散度的数据，使用学习跳跃相比于学习去噪有更好的效果。

    

    学习去噪已成为设计最先进的深度生成模型（如扩散模型）的重要范式，用于建模连续的实值数据和分类数据已经有很好的研究。然而，本文发现学习去噪在建模某些其他类型的数据（如计数和非负连续数据）时能力有限，这些数据经常是高度稀疏、倾斜、重尾或过度分散的。为此，我们提出了学习跳跃作为各种类型数据的生成建模的通用方法。使用正向计数稀化方法构建学习目标，训练深度神经网络，通过逆向计数加厚过程迭代地改进其生成结果。我们演示了什么情况下学习跳跃与学习去噪表现相当，并且什么情况下学习跳跃表现更好。例如，建议在建模计数和非负连续数据时使用学习跳跃，这些数据往往具有稀疏性、倾斜性、重尾性或过度分散性。

    Learning to denoise has emerged as a prominent paradigm to design state-of-the-art deep generative models for natural images. How to use it to model the distributions of both continuous real-valued data and categorical data has been well studied in recently proposed diffusion models. However, it is found in this paper to have limited ability in modeling some other types of data, such as count and non-negative continuous data, that are often highly sparse, skewed, heavy-tailed, and/or overdispersed. To this end, we propose learning to jump as a general recipe for generative modeling of various types of data. Using a forward count thinning process to construct learning objectives to train a deep neural network, it employs a reverse count thickening process to iteratively refine its generation through that network. We demonstrate when learning to jump is expected to perform comparably to learning to denoise, and when it is expected to perform better. For example, learning to jump is recom
    
[^145]: 纯谱图嵌入：将图卷积重新解释为Top-N推荐中的方法

    Pure Spectral Graph Embeddings: Reinterpreting Graph Convolution for Top-N Recommendation. (arXiv:2305.18374v1 [cs.IR])

    [http://arxiv.org/abs/2305.18374](http://arxiv.org/abs/2305.18374)

    本文提出了一种新的纯谱图嵌入方法，在Top-N推荐任务中比现有基于图卷积的模型具有更优的性能。

    

    最近在协同过滤任务（CF）的推荐系统算法的开发中，使用图卷积已经取得了最先进的结果。虽然已经证明，图卷积操作与图谱域上的过滤操作有关，但为什么这会导致协同过滤问题的更高性能的理论基础仍不为人知。本文提出了两个贡献。首先，我们研究了在用户和项目表示学习过程中使用图卷积的效果，并展示了学习到的潜在特征如何从过滤操作推进到由归一化邻接矩阵的最高特征值对应的特征向量张成的子空间中，并且该子空间上的向量是与训练数据上的预测函数的求和相关的目标函数的最优解。然后，我们提出了一种方法，将图卷积操作重新解释为纯谱嵌入，将其与谱方法文献对齐，并突出其与Laplacian Eigenmaps和Common Neighbour Ranking Mechanism的联系。通过利用图Laplacian的谱特性，该方法能够在Top-N推荐任务中取得优于现有基于图卷积的模型的性能。

    The use of graph convolution in the development of recommender system algorithms has recently achieved state-of-the-art results in the collaborative filtering task (CF). While it has been demonstrated that the graph convolution operation is connected to a filtering operation on the graph spectral domain, the theoretical rationale for why this leads to higher performance on the collaborative filtering problem remains unknown. The presented work makes two contributions. First, we investigate the effect of using graph convolution throughout the user and item representation learning processes, demonstrating how the latent features learned are pushed from the filtering operation into the subspace spanned by the eigenvectors associated with the highest eigenvalues of the normalised adjacency matrix, and how vectors lying on this subspace are the optimal solutions for an objective function related to the sum of the prediction function over the training data. Then, we present an approach that 
    
[^146]: 学习增强自主系统验证中的假设生成

    Assumption Generation for the Verification of Learning-Enabled Autonomous Systems. (arXiv:2305.18372v1 [cs.AI])

    [http://arxiv.org/abs/2305.18372](http://arxiv.org/abs/2305.18372)

    本文提出了一种为安全保证提供假设的做法，以用于验证具有复杂环境和学习增强组件的自主系统，通过自动生成适当的DNN行为假设，来满足要求的安全属性。

    

    自主系统的安全保证具有挑战性，因为这些系统在需要使用深度神经网络（DNN）等学习增强组件的复杂环境中运行，用于视觉感知。 DNN由于规模大（可能具有成千上万个参数）、缺乏正式规范（DNN通常是从有标签的数据中学习，缺乏任何正式要求）以及对环境中微小变化的敏感性而难以分析。 我们提出了一种假设保证式组合方法，用于验证这种自主系统的系统级安全属性。 我们的洞察力在于，我们可以通过自动合成有保证的DNN行为的假设，在没有DNN感知组件的情况下分析系统，以保证满足所需的安全属性。 合成的假设是最弱的，因为它们表征了所有可能的DNN的输出序列。

    Providing safety guarantees for autonomous systems is difficult as these systems operate in complex environments that require the use of learning-enabled components, such as deep neural networks (DNNs) for visual perception. DNNs are hard to analyze due to their size (they can have thousands or millions of parameters), lack of formal specifications (DNNs are typically learnt from labeled data, in the absence of any formal requirements), and sensitivity to small changes in the environment. We present an assume-guarantee style compositional approach for the formal verification of system-level safety properties of such autonomous systems. Our insight is that we can analyze the system in the absence of the DNN perception components by automatically synthesizing assumptions on the DNN behaviour that guarantee the satisfaction of the required safety properties. The synthesized assumptions are the weakest in the sense that they characterize the output sequences of all the possible DNNs that, 
    
[^147]: 使用协方差神经网络进行可解释的脑龄预测

    Explainable Brain Age Prediction using coVariance Neural Networks. (arXiv:2305.18370v1 [q-bio.QM])

    [http://arxiv.org/abs/2305.18370](http://arxiv.org/abs/2305.18370)

    本文提出了使用协方差神经网络进行可解释的脑龄预测的框架，可以通过皮质厚度特征捕捉加速老化，并反映出增加的神经疾病或认知障碍的风险。

    

    在计算神经科学中，越来越多的机器学习算法被用于利用脑成像数据为个体提供“脑龄”估计。由于脑龄与实际年龄存在差异（称为“脑龄差”），因此可以捕捉由于不良健康状况导致的加速老化，并因此反映出增加的神经疾病或认知障碍的风险。然而，大多数现有的脑龄预测算法缺乏透明度和方法论依据，限制了其在临床决策支持方面的广泛应用。在本文中，我们利用协方差神经网络 (VNN)来提出一种解剖可解释的框架，利用皮质厚度特征进行脑龄预测。具体而言，我们的脑龄预测框架不仅扩展到阿尔茨海默病 (AD) 中脑龄差的粗略指标，而且我们还提出了两个重要观察。

    In computational neuroscience, there has been an increased interest in developing machine learning algorithms that leverage brain imaging data to provide estimates of "brain age" for an individual. Importantly, the discordance between brain age and chronological age (referred to as "brain age gap") can capture accelerated aging due to adverse health conditions and therefore, can reflect increased vulnerability towards neurological disease or cognitive impairments. However, widespread adoption of brain age for clinical decision support has been hindered due to lack of transparency and methodological justifications in most existing brain age prediction algorithms. In this paper, we leverage coVariance neural networks (VNN) to propose an anatomically interpretable framework for brain age prediction using cortical thickness features. Specifically, our brain age prediction framework extends beyond the coarse metric of brain age gap in Alzheimer's disease (AD) and we make two important obser
    
[^148]: 使用VGG16算法对CT扫描图像中的肺癌进行分类

    Using VGG16 Algorithms for classification of lung cancer in CT scans Image. (arXiv:2305.18367v1 [eess.IV])

    [http://arxiv.org/abs/2305.18367](http://arxiv.org/abs/2305.18367)

    研究开发了基于VGG16算法的肺癌结节检测系统，该系统可以在CT扫描图像中将结节分类为恶性、良性和健康患者，具有较高的准确性和敏感性。

    

    肺癌是全球癌症相关死亡率的主要原因。早期检测肺结节对于提高癌症患者的生存率至关重要。传统上，医生必须手动识别可能有肺癌的区域。在开发这些检测系统时，肺结节的形状、大小和质地的任意性是一个挑战。许多研究表明，计算机视觉算法在肺结节的准确诊断和分类方面有应用。本文开发了一种深度学习算法VGG16，以帮助医学专业人员诊断和分类肺癌结节。VGG16可将肺癌的医学图像分类为恶性、良性和健康患者。本文表明，使用此单个神经网络的结节检测具有92.08％的敏感性，91％的准确性和93％的AUC。

    Lung cancer is the leading reason behind cancer-related deaths within the world. Early detection of lung nodules is vital for increasing the survival rate of cancer patients. Traditionally, physicians should manually identify the world suspected of getting carcinoma. When developing these detection systems, the arbitrariness of lung nodules' shape, size, and texture could be a challenge. Many studies showed the applied of computer vision algorithms to accurate diagnosis and classification of lung nodules. A deep learning algorithm called the VGG16 was developed during this paper to help medical professionals diagnose and classify carcinoma nodules. VGG16 can classify medical images of carcinoma in malignant, benign, and healthy patients. This paper showed that nodule detection using this single neural network had 92.08% sensitivity, 91% accuracy, and an AUC of 93%.
    
[^149]: 基于语义交互的交互式深度学习方法

    DeepSI: Interactive Deep Learning for Semantic Interaction. (arXiv:2305.18357v1 [cs.LG])

    [http://arxiv.org/abs/2305.18357](http://arxiv.org/abs/2305.18357)

    本文提出了一种基于语义交互的交互式深度学习方法，可以高效地学习用户和任务特定的数据表示，改善视觉分析应用中的语义交互，并通过比较验证了该方法的优势。

    

    本文提出了一种新颖的交互式深度学习方法，旨在改善视觉分析应用中的语义交互。语义交互推断分析人员在感知过程中的精确意图取决于底层数据表示的质量。我们提出了$\text{DeepSI}_{\text{finetune}}$框架，将深度学习集成到人在交互式感知管道中，具有两个重要属性。首先，深度学习从原始数据中提取有意义的表示，提高了语义交互推断的质量。其次，利用语义交互来微调深度学习表示，进一步提高了语义交互推断的质量。人机交互和深度学习之间的反馈循环使得可以高效地学习用户和任务特定的表示。为了评估将深度学习嵌入到语义交互循环中的优势，我们比较了$\text{DeepSI}_{\te

    In this paper, we design novel interactive deep learning methods to improve semantic interactions in visual analytics applications. The ability of semantic interaction to infer analysts' precise intents during sensemaking is dependent on the quality of the underlying data representation. We propose the $\text{DeepSI}_{\text{finetune}}$ framework that integrates deep learning into the human-in-the-loop interactive sensemaking pipeline, with two important properties. First, deep learning extracts meaningful representations from raw data, which improves semantic interaction inference. Second, semantic interactions are exploited to fine-tune the deep learning representations, which then further improves semantic interaction inference. This feedback loop between human interaction and deep learning enables efficient learning of userand task-specific representations. To evaluate the advantage of embedding the deep learning within the semantic interaction loop, we compare $\text{DeepSI}_{\te
    
[^150]: Forward-Forward算法训练的网络中的突现表征

    Emergent representations in networks trained with the Forward-Forward algorithm. (arXiv:2305.18353v1 [cs.NE])

    [http://arxiv.org/abs/2305.18353](http://arxiv.org/abs/2305.18353)

    研究表明使用Forward-Forward算法训练的网络内部表征具有高稀疏度，类别特定的集合，这与生物学观察到的皮层表征相似。

    

    Backpropagation算法被广泛用于训练神经网络，但其缺乏生物学上的现实性。为了寻找一种更具生物学可行性的替代方案，并避免反向传播梯度，而是使用本地学习规则，最近介绍的Forward-Forward算法将Backpropagation的传递替换为两个前向传递。本研究表明，使用Forward-Forward算法获得的内部表征组织为稳健的，类别特定的集合，由极少量的有效单元(高稀疏度)组成。这与感觉处理过程中观察到的皮层表征非常相似。虽然在使用标准Backpropagation进行训练的模型中没有发现，但是在使用与Forward-Forward相同的训练目标进行优化的网络中也出现了稀疏性。这些结果表明，Forward-Forward提议的学习过程可能更接近生物学学习的现实情况。

    The Backpropagation algorithm, widely used to train neural networks, has often been criticised for its lack of biological realism. In an attempt to find a more biologically plausible alternative, and avoid to back-propagate gradients in favour of using local learning rules, the recently introduced Forward-Forward algorithm replaces the traditional forward and backward passes of Backpropagation with two forward passes. In this work, we show that internal representations obtained with the Forward-Forward algorithm organize into robust, category-specific ensembles, composed by an extremely low number of active units (high sparsity). This is remarkably similar to what is observed in cortical representations during sensory processing. While not found in models trained with standard Backpropagation, sparsity emerges also in networks optimized by Backpropagation, on the same training objective of Forward-Forward. These results suggest that the learning procedure proposed by Forward-Forward ma
    
[^151]: 多目标遗传算法用于多视角特征选择

    Multi-Objective Genetic Algorithm for Multi-View Feature Selection. (arXiv:2305.18352v1 [cs.NE])

    [http://arxiv.org/abs/2305.18352](http://arxiv.org/abs/2305.18352)

    多视角数据提高了预测模型的准确性，但也使得高维数据增加，影响模型泛化能力。研究者提出了一种多视角多目标特征选择遗传算法（MMFS-GA），用于从多视角数据中选择最优的特征子集以提高模型精度和可解释性。

    

    多视角数据集提供了不同形式的数据，可以通过提供补充信息来增强预测模型。但是，使用多视角数据会导致高维数据的增加，这对可以导致泛化能力差的预测模型带来显著挑战。因此，从多视角数据集中选择相关特征不仅可以解决不良的泛化能力，还可以增强模型的可解释性。尽管传统特征选择方法取得了成功，但它们在利用跨模态的内在信息、缺乏泛化性和适用于特定分类任务方面存在局限性。我们提出了一种新的遗传算法策略，以克服传统特征选择方法在多视角数据上的这些局限性。我们提出的方法称为多视角多目标特征选择遗传算法（MMFS-GA）。这种方法同时选择最优的特征子集。

    Multi-view datasets offer diverse forms of data that can enhance prediction models by providing complementary information. However, the use of multi-view data leads to an increase in high-dimensional data, which poses significant challenges for the prediction models that can lead to poor generalization. Therefore, relevant feature selection from multi-view datasets is important as it not only addresses the poor generalization but also enhances the interpretability of the models. Despite the success of traditional feature selection methods, they have limitations in leveraging intrinsic information across modalities, lacking generalizability, and being tailored to specific classification tasks. We propose a novel genetic algorithm strategy to overcome these limitations of traditional feature selection methods for multi-view data. Our proposed approach, called the multi-view multi-objective feature selection genetic algorithm (MMFS-GA), simultaneously selects the optimal subset of feature
    
[^152]: 实现开放世界产品属性挖掘：基于轻度监督方法的研究

    Towards Open-World Product Attribute Mining: A Lightly-Supervised Approach. (arXiv:2305.18350v1 [cs.LG])

    [http://arxiv.org/abs/2305.18350](http://arxiv.org/abs/2305.18350)

    该研究提出了一种用于电子商务产品属性挖掘的新任务设置，可以利用高质量的种子属性集合轻度监督并自动发现新的属性类型。通过自我监督启发式和无监督潜在属性，该方法能够以额外的隐含语义信号作为辅助监督，将现有类型的属性扩展最多12倍，并成功发掘了39％的新属性值。

    

    我们提出了一种新的电子商务产品属性挖掘任务设置，用于提取开放世界属性，同时减少人工干预。我们的监督来自于现有资源中引导的高质量种子属性集合，旨在扩展现有种子类型的属性词汇，并通过自动方式发现任何新的属性类型。我们创建了一个新数据集以支持我们的设置，并提出了特定于受限监督的Amacer方法。尤其是，由于那些未见过的新属性没有直接监督，我们的新颖公式利用了自我监督启发式和无监督潜在属性，利用产品上下文获得额外的隐含语义信号作为辅助监督。实验表明，我们的方法在F1值上超过了各种基线方法12个百分点，使现有类型的属性大大扩展了最多12倍，并且发现新属性值的能力达到了39％。

    We present a new task setting for attribute mining on e-commerce products, serving as a practical solution to extract open-world attributes without extensive human intervention. Our supervision comes from a high-quality seed attribute set bootstrapped from existing resources, and we aim to expand the attribute vocabulary of existing seed types, and also to discover any new attribute types automatically. A new dataset is created to support our setting, and our approach Amacer is proposed specifically to tackle the limited supervision. Especially, given that no direct supervision is available for those unseen new attributes, our novel formulation exploits self-supervised heuristic and unsupervised latent attributes, which attains implicit semantic signals as additional supervision by leveraging product context. Experiments suggest that our approach surpasses various baselines by 12 F1, expanding attributes of existing types significantly by up to 12 times, and discovering values from 39%
    
[^153]: 可视化编程中神经任务合成

    Neural Task Synthesis for Visual Programming. (arXiv:2305.18342v1 [cs.LG])

    [http://arxiv.org/abs/2305.18342](http://arxiv.org/abs/2305.18342)

    该论文提出了一种基于神经符号技术的可视化编程任务合成方法NeurTaskSyn。该方法能够针对规范中给出的解决方案代码所需要的编程概念和对可视化任务的限制，自动生成编程任务。

    

    通过合成新的内容，生成式神经模型在增强编程教育方面具有巨大的潜力。我们旨在设计神经模型，能够根据可视化编程环境下给定的规范自动生成编程任务。尽管近年来像 GPT-4 这样的大型生成模型获得了成功，但我们的初步结果显示，这些模型在合成可视化编程任务方面效果不佳，并且在逻辑和空间推理方面存在困难。我们提出了一种新颖的神经符号技术 NeurTaskSyn，该技术能够针对规范中给出的解决方案代码所需要的编程概念和对可视化任务的限制，合成编程任务。NeurTaskSyn 由两个部分构成：第一个部分通过模仿学习程序进行训练，生成可能的解决方案代码，第二个部分通过强化学习程序进行训练，指导底层符号执行引擎生成可视化任务。

    Generative neural models hold great promise in enhancing programming education by synthesizing new content for students. We seek to design neural models that can automatically generate programming tasks for a given specification in the context of visual programming domains. Despite the recent successes of large generative models like GPT-4, our initial results show that these models are ineffective in synthesizing visual programming tasks and struggle with logical and spatial reasoning. We propose a novel neuro-symbolic technique, NeurTaskSyn, that can synthesize programming tasks for a specification given in the form of desired programming concepts exercised by its solution code and constraints on the visual task. NeurTaskSyn has two components: the first component is trained via imitation learning procedure to generate possible solution codes, and the second component is trained via reinforcement learning procedure to guide an underlying symbolic execution engine that generates visua
    
[^154]: 使用编译器生成的强化学习反馈调整代码模型

    Tuning Models of Code with Compiler-Generated Reinforcement Learning Feedback. (arXiv:2305.18341v1 [cs.PL])

    [http://arxiv.org/abs/2305.18341](http://arxiv.org/abs/2305.18341)

    本文提出了一种叫做RLCF的方法，使用代码编译器反馈进一步训练预训练的大型语言模型，以生成符合目标分布的代码，并通过所有静态正确性检查，显著提高了性能。

    

    最近，对代码进行预训练的大型语言模型成为程序合成的主要方法。然而，这些模型生成的代码可能违反基本的语言级别不变性，从而降低下游任务的性能。本文提出了一种称为RLCF的方法，它使用代码编译器的反馈进一步训练预训练的大型语言模型。 RLCF将LLM视为通过RL代理逐步生成代码，并接收以下反馈：（i）编译器派生的反馈与所生成的代码是否通过一组正确性检查有关; （ii）不同LLM的反馈，与训练语料库中一组参考程序相似。这些反馈机制帮助所生成的代码在通过所有静态正确性检查的同时保持在目标分布中。RLCF是模型和语言无关的。我们在Java的MBJP和MathQA任务上进行了实证评估，实验结果表明，RLCF显著提高了性能。

    Large Language Models (LLMs) pre-trained on code have recently emerged as the dominant approach to program synthesis. However, the code that these models produce can violate basic language-level invariants, leading to lower performance in downstream tasks. We address this issue through an approach, called RLCF, that further trains a pre-trained LLM using feedback from a code compiler. RLCF views the LLM as an RL agent that generates code step by step and receives: (i) compiler-derived feedback on whether the code it generates passes a set of correctness checks; and (ii) feedback from a different LLM on whether the generated code is similar to a set of reference programs in the training corpus. Together, these feedback mechanisms help the generated code remain within the target distribution while passing all static correctness checks. RLCF is model- and language-agnostic. We empirically evaluate it on the MBJP and MathQA tasks for Java. Our experiments show that RLCF significantly raise
    
[^155]: 这里是翻译过的论文标题：我们到了吗？产品量化及其硬件加速。

    Are We There Yet? Product Quantization and its Hardware Acceleration. (arXiv:2305.18334v1 [cs.AR])

    [http://arxiv.org/abs/2305.18334](http://arxiv.org/abs/2305.18334)

    本文研究了产品量化（PQ）在深度神经网络中替代传统乘加（MAC）运算的效果。作者发现FLOP和参数数量等指标可能具有误导性，并设计了第一个PQ定制硬件加速器评估其性能和效率。

    

    传统的乘加（MAC）运算长期以来一直主导深度神经网络（DNN）的计算时间。最近，产品量化（PQ）已成功地应用于这些工作负载，用预先计算的点积的内存查找替换了MAC。虽然这个属性使PQ成为模型加速的一个有吸引力的解决方案，但人们很少了解与计算和存储器占用相关的权衡，以及对准确性的影响。我们的实证研究调查了不同PQ设置和训练方法对逐层重建误差和端到端模型准确性的影响。在研究部署PQ DNN的效率时，我们发现FLOP、参数数量甚至CPU/GPU性能等指标可能具有误导性。为了解决这个问题，更公平地评估PQ的硬件效率，我们设计了第一个定制的硬件加速器，用于评估运行PQ模型的速度和效率。我们确定了PQ配置的硬件性能和存储要求之间的权衡。

    Conventional multiply-accumulate (MAC) operations have long dominated computation time for deep neural networks (DNNs). Recently, product quantization (PQ) has been successfully applied to these workloads, replacing MACs with memory lookups to pre-computed dot products. While this property makes PQ an attractive solution for model acceleration, little is understood about the associated trade-offs in terms of compute and memory footprint, and the impact on accuracy. Our empirical study investigates the impact of different PQ settings and training methods on layerwise reconstruction error and end-to-end model accuracy. When studying the efficiency of deploying PQ DNNs, we find that metrics such as FLOPs, number of parameters, and even CPU/GPU performance, can be misleading. To address this issue, and to more fairly assess PQ in terms of hardware efficiency, we design the first custom hardware accelerator to evaluate the speed and efficiency of running PQ models. We identify PQ configurat
    
[^156]: 具有流行度偏见的排名：自增强动态下的用户福利

    Ranking with Popularity Bias: User Welfare under Self-Amplification Dynamics. (arXiv:2305.18333v1 [cs.IR])

    [http://arxiv.org/abs/2305.18333](http://arxiv.org/abs/2305.18333)

    研究了物品流行度、质量和位置偏差对用户福利的影响，提出了通过探索减轻流行度偏见负面影响的算法。

    

    虽然已经确认流行度偏见在推荐（和其他基于排名的）系统中发挥作用，但其对用户福利的影响的详细分析仍然缺乏。我们提出了一种通用机制，通过它，物品的流行度、质量和位置偏差可以影响用户选择，并且可以负面影响各种推荐策略的集体用户效用。我们将问题表述为非平稳上下文脱靶机，强调不是为了消除流行度偏见而是为了减轻其负面影响而进行探索的重要性。首先，普通的有流行度偏差的推荐系统会通过混淆物品质量和流行度而引发线性遗憾。更一般地，我们展示了即使在线性设置下，由于流行度偏见的混淆效应，物品质量的可识别性也可能无法实现。然而，在足够变异的假设下，我们开发了一种高效的类UCB算法，并证明了有效的遗憾保证。我们通过实验验证了我们提出的算法的有效性，并证实了流行度偏见的负面影响。

    While popularity bias is recognized to play a role in recommmender (and other ranking-based) systems, detailed analyses of its impact on user welfare have largely been lacking. We propose a general mechanism by which item popularity, item quality, and position bias can impact user choice, and how it can negatively impact the collective user utility of various recommender policies. Formulating the problem as a non-stationary contextual bandit, we highlight the importance of exploration, not to eliminate popularity bias, but to mitigate its negative effects. First, naive popularity-biased recommenders are shown to induce linear regret by conflating item quality and popularity. More generally, we show that, even in linear settings, identifiability of item quality may not be possible due to the confounding effects of popularity bias. However, under sufficient variability assumptions, we develop an efficient UCB-style algorithm and prove efficient regret guarantees. We complement our analys
    
[^157]: 可重构分布式FPGA集群设计用于深度学习加速器

    Reconfigurable Distributed FPGA Cluster Design for Deep Learning Accelerators. (arXiv:2305.18332v1 [cs.DC])

    [http://arxiv.org/abs/2305.18332](http://arxiv.org/abs/2305.18332)

    该论文提出了一种基于分布式FPGA集群的深度学习加速器，能够在多个配置中评估和管理神经网络工作负载，实现最佳的延迟和功耗效率。

    

    我们提出了一种基于低功耗嵌入式FPGA的分布式系统，旨在为边缘计算应用探索分布式调度优化的深度学习工作负载，以获得最佳的延迟和功耗效率。我们的集群在实验过程中是模块化的，我们已经实现了高达12个基于Zynq-7020芯片的板和5个UltraScale+ MPSoC FPGA板通过以太网交换机连接，并且集群将评估可配置的深度学习加速器（DLA）Versatile Tensor Accelerator（VTA）。这种可适应的分布式架构的优点在于其能够在许多配置中评估和管理神经网络工作负载，使用户能够根据其特定的应用程序需求进行多个实验。所提出的系统可以同时执行不同的神经网络模型，将计算图排列成管道结构，并手动分配计算资源。

    We propose a distributed system based on lowpower embedded FPGAs designed for edge computing applications focused on exploring distributing scheduling optimizations for Deep Learning (DL) workloads to obtain the best performance regarding latency and power efficiency. Our cluster was modular throughout the experiment, and we have implementations that consist of up to 12 Zynq-7020 chip-based boards as well as 5 UltraScale+ MPSoC FPGA boards connected through an ethernet switch, and the cluster will evaluate configurable Deep Learning Accelerator (DLA) Versatile Tensor Accelerator (VTA). This adaptable distributed architecture is distinguished by its capacity to evaluate and manage neural network workloads in numerous configurations which enables users to conduct multiple experiments tailored to their specific application needs. The proposed system can simultaneously execute diverse Neural Network (NN) models, arrange the computation graph in a pipeline structure, and manually allocate g
    
[^158]: 通过平衡传播算法训练伊辛机器

    Training an Ising Machine with Equilibrium Propagation. (arXiv:2305.18321v1 [cs.NE])

    [http://arxiv.org/abs/2305.18321](http://arxiv.org/abs/2305.18321)

    本研究利用平衡传播算法以监督的方式成功训练了伊辛机器。同时，我们在 MNIST 数据集上取得了与软件实现相当的结果。此外，伊辛机器的连接还支持卷积操作，实现了一个紧凑的卷积网络，每个神经元最少使用自旋数。

    

    伊辛机器是伊辛模型耦合自旋的硬件实现，在人工智能（AI）的起源中对于非监督学习算法的发展有着重要影响。然而，由于监督训练方法与伊辛机器物理特性之间的复杂匹配，这些方法对于获得高精度非常关键，但其应用却受到了限制。在本研究中，我们展示了一种新颖的通过平衡传播算法以监督的方式训练伊辛机器的方法，并在 MNIST 数据集上实现了与软件实现相当的结果。我们采用 D-Wave 伊辛机器的量子退火过程来训练一个全连接神经网络。此外，我们还展示了机器的连接支持卷积操作，从而实现了一个紧凑的卷积网络，每个神经元最少使用自旋数。我们的发现证明了伊辛机器作为训练神经网络的有希望的方法。

    Ising machines, which are hardware implementations of the Ising model of coupled spins, have been influential in the development of unsupervised learning algorithms at the origins of Artificial Intelligence (AI). However, their application to AI has been limited due to the complexities in matching supervised training methods with Ising machine physics, even though these methods are essential for achieving high accuracy. In this study, we demonstrate a novel approach to train Ising machines in a supervised way through the Equilibrium Propagation algorithm, achieving comparable results to software-based implementations. We employ the quantum annealing procedure of the D-Wave Ising machine to train a fully-connected neural network on the MNIST dataset. Furthermore, we demonstrate that the machine's connectivity supports convolution operations, enabling the training of a compact convolutional network with minimal spins per neuron. Our findings establish Ising machines as a promising traina
    
[^159]: 化学数据库和摘要练习的自动生成反馈

    Automated Feedback Generation for a Chemistry Database and Abstracting Exercise. (arXiv:2305.18319v1 [cs.CL])

    [http://arxiv.org/abs/2305.18319](http://arxiv.org/abs/2305.18319)

    本研究利用BERT模型，对化学数据库中摘要练习的答案结构进行反馈，该模型在学生提交的句子中将其归为三类，即背景、技术和观察，提供了一种方法对学生作业进行自动化反馈。

    

    及时的反馈对于教学和学习来说非常重要。本文描述了如何利用现成的神经网络变换器（机器学习）模型（BERT）来对摘要练习的答案结构进行反馈。在这项任务中，要求学生们从出版数据库中找到一篇文章并对其内容进行总结。数据集包括207个提交品，总共摘要了21篇来自主要文献的文章。该模型使用一个现有的数据集（约15,000个样本）进行了预训练，然后在80%的已提交数据集上进行了微调，这一步骤被认为是重要的。学生提交的句子被归为三类——背景、技术和观察——这使得可以将每个提交品的结构进行比较。通过比较学生的摘要结构以及来自PubMed数据库的大量摘要，可以发现...

    Timely feedback is an important part of teaching and learning. Here we describe how a readily available neural network transformer (machine-learning) model (BERT) can be used to give feedback on the structure of the response to an abstracting exercise where students are asked to summarise the contents of a published article after finding it from a publication database. The dataset contained 207 submissions from two consecutive years of the course, summarising a total of 21 different papers from the primary literature. The model was pre-trained using an available dataset (approx. 15,000 samples) and then fine-tuned on 80% of the submitted dataset. This fine tuning was seen to be important. The sentences in the student submissions are characterised into three classes - background, technique and observation - which allows a comparison of how each submission is structured. Comparing the structure of the students' abstract a large collection of those from the PubMed database shows that stud
    
[^160]: CDJUR-BR -- 带有精细命名实体的巴西司法文件黄金收藏

    CDJUR-BR -- A Golden Collection of Legal Document from Brazilian Justice with Fine-Grained Named Entities. (arXiv:2305.18315v1 [cs.CL])

    [http://arxiv.org/abs/2305.18315](http://arxiv.org/abs/2305.18315)

    CDJUR-BR是一份稳健的黄金收藏，包含巴西司法文件中的精细命名实体，该收藏涵盖各种法律程序文件，并有助于解决目前命名实体识别（NER）无法轻而易举地识别法律实践文本中实体的问题。

    

    对于大多数法律人工智能（Legal AI）应用程序而言，命名实体识别（NER）是一项基本任务。然而，法律实践中产生的文本涉及到的实体并非当前可用的NER轻而易举地识别。缺乏法规、判例、证据、惩罚、法律程序中人们的角色（法官、律师、受害者、被告、证人）、位置类型（犯罪地点、被告地址）等的分类。因此，仍需要一个用法律领域的精细实体进行注释的稳健的黄金收藏，涵盖法律程序的各种文件，例如请愿书、调查、投诉、决定和判决。在本文中，我们描述了巴西司法黄金收藏（CDJUR-BR）的开发，该收藏包含一组由法律文献专家注释的精细命名实体。创建CDJUR-BR遵循了自己的

    A basic task for most Legal Artificial Intelligence (Legal AI) applications is Named Entity Recognition (NER). However, texts produced in the context of legal practice make references to entities that are not trivially recognized by the currently available NERs. There is a lack of categorization of legislation, jurisprudence, evidence, penalties, the roles of people in a legal process (judge, lawyer, victim, defendant, witness), types of locations (crime location, defendant's address), etc. In this sense, there is still a need for a robust golden collection, annotated with fine-grained entities of the legal domain, and which covers various documents of a legal process, such as petitions, inquiries, complaints, decisions and sentences. In this article, we describe the development of the Golden Collection of the Brazilian Judiciary (CDJUR-BR) contemplating a set of fine-grained named entities that have been annotated by experts in legal documents. The creation of CDJUR-BR followed its ow
    
[^161]: 计算机自适应测试中平衡测试准确性与安全性

    Balancing Test Accuracy and Security in Computerized Adaptive Testing. (arXiv:2305.18312v1 [cs.CY])

    [http://arxiv.org/abs/2305.18312](http://arxiv.org/abs/2305.18312)

    本文介绍了一种基于双层优化的计算机自适应测试(CAT)框架的约束版本C-BOBCAT，通过权衡测试准确性和问题暴露率及测试重叠率，解决了BOBCAT存在的高问题暴露率和测试重叠率的问题。

    

    计算机自适应测试(CAT)是一种可以准确测量学生知识水平且缩短测试时间的个性化测试形式。基于双层优化的CAT(BOBCAT)是一个最近的框架，它学习了一种数据驱动的问题选择算法，有效地缩短了测试时间并提高了测试准确性。然而，它存在高问题暴露率和测试重叠率的问题，这可能影响测试安全性。本文介绍了BOBCAT的一种约束版本，通过更改其优化设置使我们能够权衡测试准确性和问题暴露率及测试重叠率。我们通过在两个真实的成人测试数据集上进行大量实验，证明了这种方法的有效性。

    Computerized adaptive testing (CAT) is a form of personalized testing that accurately measures students' knowledge levels while reducing test length. Bilevel optimization-based CAT (BOBCAT) is a recent framework that learns a data-driven question selection algorithm to effectively reduce test length and improve test accuracy. However, it suffers from high question exposure and test overlap rates, which potentially affects test security. This paper introduces a constrained version of BOBCAT to address these problems by changing its optimization setup and enabling us to trade off test accuracy for question exposure and test overlap rates. We show that C-BOBCAT is effective through extensive experiments on two real-world adult testing datasets.
    
[^162]: 多视角交互式协同过滤

    Multi-View Interactive Collaborative Filtering. (arXiv:2305.18306v1 [cs.IR])

    [http://arxiv.org/abs/2305.18306](http://arxiv.org/abs/2305.18306)

    提出了基于多视角交互主题回归算法（MV-ICTR）的推荐系统，在不同视角下同时纳入评分和上下文信息来建模物品特定功能的相关性和用户的个人偏好，采用多臂老虎机策略进行持续的在线个性化，显著提高了数据集上性能。

    

    在许多场景下，推荐系统中的用户交互数据（如点击或评分）往往很少，物品的换手率（例如新文章、招聘信息）很高。因此，除了用户-物品评分外，集成上下文“边”信息是非常可取的。虽然存在可以同时处理评分和上下文数据的算法，但这些算法通常仅能进行样本内推荐，受到维度诅咒的限制，并不采用多臂老虎机（MAB）策略进行长期累积收益优化。我们提出了多视角交互主题回归（MV-ICTR）算法，这是一种新颖的部分在线潜在因子推荐算法，同时纳入评分和上下文信息来建模物品特定功能的相关性和用户的个人偏好，采用多臂老虎机策略进行持续在线个性化。该算法在数据集上的性能显著提高。

    In many scenarios, recommender system user interaction data such as clicks or ratings is sparse, and item turnover rates (e.g., new articles, job postings) high. Given this, the integration of contextual "side" information in addition to user-item ratings is highly desirable. Whilst there are algorithms that can handle both rating and contextual data simultaneously, these algorithms are typically limited to making only in-sample recommendations, suffer from the curse of dimensionality, and do not incorporate multi-armed bandit (MAB) policies for long-term cumulative reward optimization. We propose multi-view interactive topic regression (MV-ICTR) a novel partially online latent factor recommender algorithm that incorporates both rating and contextual information to model item-specific feature dependencies and users' personal preferences simultaneously, with multi-armed bandit policies for continued online personalization. The result is significantly increased performance on datasets wi
    
[^163]: 使用潜在Bandits的高准确度和低遗憾用户冷启动方法

    High Accuracy and Low Regret for User-Cold-Start Using Latent Bandits. (arXiv:2305.18305v1 [cs.IR])

    [http://arxiv.org/abs/2305.18305](http://arxiv.org/abs/2305.18305)

    使用潜在Bandits算法解决用户冷启动问题，同时实现更高的准确率和更低的遗憾。

    

    我们开发了一种新型的潜在Bandits算法，用于解决新用户加入推荐系统时面临的冷启动问题。这种新算法在同时实现更高的准确率和更低的遗憾方面明显优于现有技术。

    We develop a novel latent-bandit algorithm for tackling the cold-start problem for new users joining a recommender system. This new algorithm significantly outperforms the state of the art, simultaneously achieving both higher accuracy and lower regret.
    
[^164]: 通过 $\ell_1-\ell_2$ 优化进行结构模型选择

    Structured model selection via $\ell_1-\ell_2$ optimization. (arXiv:2305.17467v1 [stat.ML])

    [http://arxiv.org/abs/2305.17467](http://arxiv.org/abs/2305.17467)

    通过稀疏最小二乘拟合一大组候选函数，使用 $\ell_1-\ell_2$ 稀疏优化方法进行结构模型选择，实现从不充分且嘈杂的时空数据中识别结构化动态系统；该方法在合成数据集上得到了验证，并证明具有理论保证和高效性。

    

    自动化模型选择在科学和工程中具有重要应用。本文提出了一种学习方法，通过稀疏最小二乘拟合一大组候选函数，用一种非凸 $\ell_1-\ell_2$ 稀疏优化方法求解，通过交替方向乘法的方法进行。我们证明，如果候选函数集合形成边界正交系统的结构随机采样矩阵，就可以通过伯恩斯坦样式的不等式和一致性条件稳定恢复，并且误差有界。该学习方法在由粘性Burgers'方程和两个反应扩散方程产生的合成数据上进行了验证。计算结果证明了成功的理论保证和相对于环境维数和候选函数数量的效率。

    Automated model selection is an important application in science and engineering. In this work, we develop a learning approach for identifying structured dynamical systems from undersampled and noisy spatiotemporal data. The learning is performed by a sparse least-squares fitting over a large set of candidate functions via a nonconvex $\ell_1-\ell_2$ sparse optimization solved by the alternating direction method of multipliers. Using a Bernstein-like inequality with a coherence condition, we show that if the set of candidate functions forms a structured random sampling matrix of a bounded orthogonal system, the recovery is stable and the error is bounded. The learning approach is validated on synthetic data generated by the viscous Burgers' equation and two reaction-diffusion equations. The computational results demonstrate the theoretical guarantees of success and the efficiency with respect to the ambient dimension and the number of candidate functions.
    
[^165]: 具有对抗性损失和转换的无遗憾在线强化学习

    No-Regret Online Reinforcement Learning with Adversarial Losses and Transitions. (arXiv:2305.17380v1 [cs.LG])

    [http://arxiv.org/abs/2305.17380](http://arxiv.org/abs/2305.17380)

    本文提出了一种算法，可以处理对抗性损失和对抗性转换，且后悔逐渐增加与对手的恶意程度成比例。

    

    现有的对抗性马尔可夫决策过程的在线学习算法可以在与对手的$ T $轮交互之后实现${ O}(\sqrt{T})$的后悔，即使损失函数是由对手任意选择的，但前提是转移函数必须固定。这是因为已经有研究表明，对抗性转移函数使无悔学习变得不可能。尽管存在这种不可能性结果，我们开发了可以处理对抗性损失和对抗性转换的算法，后悔逐渐增加与对手的恶意程度成比例。更具体地说，我们首先提出了一种算法，它的后悔为$\widetilde{{O}}(\sqrt{T} + C^{\textsf{P}})$，其中$C^{\textsf{P}}$表示转换函数的对抗性，最多可以为${O}(T)$。虽然此算法本身需要$C^{\textsf{P}}$的知识，但我们还开发了一种黑盒缩减方法来消除此要求。此外，我们还展示了一种进一步的方法，使得算法能够处理任意长度的锚定期。

    Existing online learning algorithms for adversarial Markov Decision Processes achieve ${O}(\sqrt{T})$ regret after $T$ rounds of interactions even if the loss functions are chosen arbitrarily by an adversary, with the caveat that the transition function has to be fixed. This is because it has been shown that adversarial transition functions make no-regret learning impossible. Despite such impossibility results, in this work, we develop algorithms that can handle both adversarial losses and adversarial transitions, with regret increasing smoothly in the degree of maliciousness of the adversary. More concretely, we first propose an algorithm that enjoys $\widetilde{{O}}(\sqrt{T} + C^{\textsf{P}})$ regret where $C^{\textsf{P}}$ measures how adversarial the transition functions are and can be at most ${O}(T)$. While this algorithm itself requires knowledge of $C^{\textsf{P}}$, we further develop a black-box reduction approach that removes this requirement. Moreover, we also show that furth
    
[^166]: 基于内核KL散度的自监督学习方法Kernel-SSL

    Kernel-SSL: Kernel KL Divergence for Self-supervised Learning. (arXiv:2305.17326v1 [cs.LG])

    [http://arxiv.org/abs/2305.17326](http://arxiv.org/abs/2305.17326)

    本文提出了一种名为Kernel-SSL的自监督学习方法，将多种现有非对比学习方法建立在了再生核希尔伯特空间（RKHS）理解之上并优化了其中的均值嵌入和协方差算子，实验结果显示，在ImageNet数据集下表现显著超越最先进的方法，提高了4.6%。

    

    对比学习通常将一个正锚点样本与许多负样本进行比较，来完成自监督学习（SSL）。相反，非对比学习，例如BYOL、SimSiam和Barlow Twins等方法，在没有显式使用负样本的情况下完成SSL。受对比学习现有分析的启发，我们提供了多种现有非对比学习方法的再生核希尔伯特空间（RKHS）理解。随后，我们提出了一种新的损失函数Kernel-SSL，直接优化RKHS中的均值嵌入和协方差算子。实验中，我们的方法Kernel-SSL在线性评估设置下在ImageNet数据集上大幅优于最先进的方法。具体来说，在进行100个epoch的预训练时，我们的方法比SimCLR表现提高了4.6%。

    Contrastive learning usually compares one positive anchor sample with lots of negative samples to perform Self-Supervised Learning (SSL). Alternatively, non-contrastive learning, as exemplified by methods like BYOL, SimSiam, and Barlow Twins, accomplishes SSL without the explicit use of negative samples. Inspired by the existing analysis for contrastive learning, we provide a reproducing kernel Hilbert space (RKHS) understanding of many existing non-contrastive learning methods. Subsequently, we propose a novel loss function, Kernel-SSL, which directly optimizes the mean embedding and the covariance operator within the RKHS. In experiments, our method Kernel-SSL outperforms state-of-the-art methods by a large margin on ImageNet datasets under the linear evaluation settings. Specifically, when performing 100 epochs pre-training, our method outperforms SimCLR by 4.6%.
    
[^167]: mldr.resampling: 多标签重采样算法有效的参考实现

    mldr.resampling: Efficient Reference Implementations of Multilabel Resampling Algorithms. (arXiv:2305.17152v1 [cs.LG])

    [http://arxiv.org/abs/2305.17152](http://arxiv.org/abs/2305.17152)

    mldr.resampling是一个软件包，提供11种多标签重采样方法的参考实现，旨在应对多标签学习中的不平衡情况，并具有高效性。

    

    重采样算法是应对多标签学习中不平衡情况的有用方法。这些方法必须处理多标签数据中的奇异性，例如同一实例中频繁和不频繁标签的出现。这篇原创软件发表介绍了 mldr.resampling，这是一个软件包，提供了11种多标签重采样方法的参考实现，强调效率，因为这些算法通常耗时。

    Resampling algorithms are a useful approach to deal with imbalanced learning in multilabel scenarios. These methods have to deal with singularities in the multilabel data, such as the occurrence of frequent and infrequent labels in the same instance. Implementations of these methods are sometimes limited to the pseudocode provided by their authors in a paper. This Original Software Publication presents mldr.resampling, a software package that provides reference implementations for eleven multilabel resampling methods, with an emphasis on efficiency since these algorithms are usually time-consuming.
    
[^168]: 基于折扣线性时间逻辑的政策合成和强化学习

    Policy Synthesis and Reinforcement Learning for Discounted LTL. (arXiv:2305.17115v2 [cs.LO] UPDATED)

    [http://arxiv.org/abs/2305.17115](http://arxiv.org/abs/2305.17115)

    该论文研究使用折扣线性时间逻辑实现强化学习中的政策合成，并探讨如何减少其对状态转移的微小扰动的敏感性。

    

    手动指定奖励函数的困难性使得使用线性时间逻辑（LTL）来表达强化学习（RL）目标成为研究热点。然而，LTL 的缺点是它对状态转移的微小扰动非常敏感，这阻碍了基于概率近似正确（PAC）的学习，除非采用额外的假设。时间折扣为消除这种敏感性提供了一种方式，同时保留了该逻辑的高表达力。我们研究了在具有未知状态转换概率的马尔可夫决策过程中使用折扣 LTL 进行政策合成的方法，并展示了如何通过奖励机器将折扣 LTL 降解为折扣和奖励之和，当所有折扣因子相同时。

    The difficulty of manually specifying reward functions has led to an interest in using linear temporal logic (LTL) to express objectives for reinforcement learning (RL). However, LTL has the downside that it is sensitive to small perturbations in the transition probabilities, which prevents probably approximately correct (PAC) learning without additional assumptions. Time discounting provides a way of removing this sensitivity, while retaining the high expressivity of the logic. We study the use of discounted LTL for policy synthesis in Markov decision processes with unknown transition probabilities, and show how to reduce discounted LTL to discounted-sum reward via a reward machine when all discount factors are identical.
    
[^169]: PlaNeRF：SVD无监督三维平面正则化用于NeRF大规模场景重建。

    PlaNeRF: SVD Unsupervised 3D Plane Regularization for NeRF Large-Scale Scene Reconstruction. (arXiv:2305.16914v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.16914](http://arxiv.org/abs/2305.16914)

    本文提出了一种利用SVD无监督三维平面正则化的方法，仅使用RGB图像和语义地图即可改善NeRF的三维结构，有效解决了训练视图的过拟合导致低纹理区域的几何重建不佳的问题。

    

    神经辐射场（NeRF）利用2D图像和相机姿态进行3D场景重建以进行新颖视图合成。尽管NeRF能产生逼真的结果，但它经常遭受过拟合于训练视图的困扰，导致几何重建不佳，尤其是在低纹理区域。这种限制限制了许多需要准确几何形态的重要应用，例如外推NVS，高清映射和场景编辑。为了解决这个问题，我们提出了一种新的方法，仅使用RGB图像和语义地图即可改善NeRF的三维结构。我们的方法引入了基于奇异值分解（SVD）的新颖平面正则化，不依赖于任何几何先验知识。此外，我们在我们的损失设计中利用结构相似性指数测量（SSIM）来正确初始化NeRF的体积表示。定量和定性的结果表明，我们的方法优于流行的正则化方法以实现准确的几何重建。

    Neural Radiance Fields (NeRF) enable 3D scene reconstruction from 2D images and camera poses for Novel View Synthesis (NVS). Although NeRF can produce photorealistic results, it often suffers from overfitting to training views, leading to poor geometry reconstruction, especially in low-texture areas. This limitation restricts many important applications which require accurate geometry, such as extrapolated NVS, HD mapping and scene editing. To address this limitation, we propose a new method to improve NeRF's 3D structure using only RGB images and semantic maps. Our approach introduces a novel plane regularization based on Singular Value Decomposition (SVD), that does not rely on any geometric prior. In addition, we leverage the Structural Similarity Index Measure (SSIM) in our loss design to properly initialize the volumetric representation of NeRF. Quantitative and qualitative results show that our method outperforms popular regularization approaches in accurate geometry reconstructi
    
[^170]: 具有异质性的图神经对流扩散

    Graph Neural Convection-Diffusion with Heterophily. (arXiv:2305.16780v1 [cs.LG])

    [http://arxiv.org/abs/2305.16780](http://arxiv.org/abs/2305.16780)

    本论文提出了一种考虑了异质性原则的新型图神经网络，该网络使用对流扩散方程对节点上的信息流进行建模，可以同时考虑基于同质性和异质性的信息传递，在处理异质性图的节点分类任务中具有竞争性的表现。

    

    图神经网络（GNN）已经在各种图学习任务中展现出了很高的性能，但是它们通常假设同质性，这可能会导致对异质性图的性能表现较差。通过使用对流扩散方程（CDE）对节点上的信息流建模，提出了一种新的GNN，该模型融合了异质性原则。这使得CDE能够考虑到基于同质性的信息扩散和基于异质性的信息“对流”。我们进行了广泛的实验验证，表明相比于现有技术，我们的框架在针对异质性图的节点分类任务中能够实现竞争性能。代码可以在 \url{https://github.com/zknus/Graph-Diffusion-CDE} 上获得。

    Graph neural networks (GNNs) have shown promising results across various graph learning tasks, but they often assume homophily, which can result in poor performance on heterophilic graphs. The connected nodes are likely to be from different classes or have dissimilar features on heterophilic graphs. In this paper, we propose a novel GNN that incorporates the principle of heterophily by modeling the flow of information on nodes using the convection-diffusion equation (CDE). This allows the CDE to take into account both the diffusion of information due to homophily and the ``convection'' of information due to heterophily. We conduct extensive experiments, which suggest that our framework can achieve competitive performance on node classification tasks for heterophilic graphs, compared to the state-of-the-art methods. The code is available at \url{https://github.com/zknus/Graph-Diffusion-CDE}.
    
[^171]: 利用领域知识实现包容和偏见感知的人道主义响应入口分类

    Leveraging Domain Knowledge for Inclusive and Bias-aware Humanitarian Response Entry Classification. (arXiv:2305.16756v1 [cs.CL])

    [http://arxiv.org/abs/2305.16756](http://arxiv.org/abs/2305.16756)

    本研究提出了一种以人道主义本体为基础的新型语言模型HumBert，并提供了一种系统的方法来衡量和减少偏见，以实现对人道主义数据分析的有效和道德意识的支持。

    

    在人道主义危机期间，准确和快速的情况分析对于高效地提供人道主义援助至关重要，并且是人道主义原则和不留任何人落后原则的基础。语言处理系统可以极大地受益于这种数据分析，例如，按照人道主义本体对文本数据进行分类。然而，仅仅通过微调通用的大型语言模型 (LLM) 来实现，涉及一些实践和道德问题，特别是在数据稀疏和复杂子领域上的效果不佳以及社会偏见和不良关联的编码。在这项工作中，我们旨在为人道主义数据分析提供一种有效和道德意识的系统。我们通过 (1) 引入一个适合人道主义分析框架的新架构，(2) 创建和发布一个新的人道主义特定 LLM，称为 HumBert，并且 (3) 提出了一种系统的方式来衡量和减少偏见。

    Accurate and rapid situation analysis during humanitarian crises is critical to delivering humanitarian aid efficiently and is fundamental to humanitarian imperatives and the Leave No One Behind (LNOB) principle. This data analysis can highly benefit from language processing systems, e.g., by classifying the text data according to a humanitarian ontology. However, approaching this by simply fine-tuning a generic large language model (LLM) involves considerable practical and ethical issues, particularly the lack of effectiveness on data-sparse and complex subdomains, and the encoding of societal biases and unwanted associations. In this work, we aim to provide an effective and ethically-aware system for humanitarian data analysis. We approach this by (1) introducing a novel architecture adjusted to the humanitarian analysis framework, (2) creating and releasing a novel humanitarian-specific LLM called HumBert, and (3) proposing a systematic way to measure and mitigate biases. Our experi
    
[^172]: 大部分神经网络几乎是可学习的

    Most Neural Networks Are Almost Learnable. (arXiv:2305.16508v1 [cs.LG])

    [http://arxiv.org/abs/2305.16508](http://arxiv.org/abs/2305.16508)

    本研究提出了一种学习随机常数深度网络的PTAS方法，对于任何固定误差和深度，几乎所有的神经网络都是可学习的。

    

    我们提出了一个PTAS来学习随机常数深度网络。我们证明了对于任何固定的$\epsilon>0$和深度$i$，存在一个多项式时间算法，对于$\sqrt{d} \cdot \mathbb{S}^{d-1}$上的任何分布，学习随机Xavier网络的深度$i$，误差为$\epsilon$。该算法的时间和样本复杂度为$(\bar{d})^{\mathrm{poly}(\epsilon^{-1})}$，其中$\bar d$是网络的大小。对于某些类似于Sigmoid和ReLU的激活函数，可以将误差界限改进为$(\bar{d})^{\mathrm{polylog}(\epsilon^{-1})}$，从而得到一种几乎多项式时间算法来学习常数深度随机网络。

    We present a PTAS for learning random constant-depth networks. We show that for any fixed $\epsilon>0$ and depth $i$, there is a poly-time algorithm that for any distribution on $\sqrt{d} \cdot \mathbb{S}^{d-1}$ learns random Xavier networks of depth $i$, up to an additive error of $\epsilon$. The algorithm runs in time and sample complexity of $(\bar{d})^{\mathrm{poly}(\epsilon^{-1})}$, where $\bar d$ is the size of the network. For some cases of sigmoid and ReLU-like activations the bound can be improved to $(\bar{d})^{\mathrm{polylog}(\epsilon^{-1})}$, resulting in a quasi-poly-time algorithm for learning constant depth random networks.
    
[^173]: 带扰动生成树的可微聚类方法

    Differentiable Clustering with Perturbed Spanning Forests. (arXiv:2305.16358v1 [cs.LG])

    [http://arxiv.org/abs/2305.16358](http://arxiv.org/abs/2305.16358)

    介绍了一种基于扰动生成树的可微聚类方法，依赖于线性规划解的随机扰动，具有良好的性能。

    

    我们介绍了一种基于最小权重生成树的可微聚类方法，它是生成树的一种变体，具有多个连通分量。我们的方法依赖于线性规划解的随机扰动，以实现平滑和高效的梯度计算。这使我们能够在端到端可训练的流水线中包含聚类。我们证明了我们的方法即使在嘈杂的数据集和具有挑战性的几何环境下也能良好地工作。我们还利用这种方法制定了一个特别的损失，以有效地从部分聚类数据学习。我们在几个现实世界的数据集上展示了它在监督和半监督任务中的表现。

    We introduce a differentiable clustering method based on minimum-weight spanning forests, a variant of spanning trees with several connected components. Our method relies on stochastic perturbations of solutions of linear programs, for smoothing and efficient gradient computations. This allows us to include clustering in end-to-end trainable pipelines. We show that our method performs well even in difficult settings, such as datasets with high noise and challenging geometries. We also formulate an ad hoc loss to efficiently learn from partial clustering data using this operation. We demonstrate its performance on several real world datasets for supervised and semi-supervised tasks.
    
[^174]: 缩放数据受限的语言模型

    Scaling Data-Constrained Language Models. (arXiv:2305.16264v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.16264](http://arxiv.org/abs/2305.16264)

    研究人员研究了在数据受限制的情况下缩放语言模型，并提出了一个计算最优性的缩放定律，考虑到重复令牌和过量参数的价值递减。

    

    现在扩展语言模型的趋势涉及增加参数计数和训练数据集大小。推断这个趋势表明，训练数据集大小可能很快就会受到互联网上可用文本数据的限制。出于此限制的动机，我们研究在数据受限制的情况下缩放语言模型。具体而言，我们运行了大量的实验，变化数据重复程度和计算预算，范围达到了9000亿个训练令牌和9亿参数模型。我们发现，在有限的数据的情况下，使用高达4次重复数据的训练与使用唯一数据相比对损失的贡献微不足道。然而，使用更多的重复数据，添加计算的价值最终会衰减为零。我们提出并经验证了一个计算最优性的缩放定律，考虑到重复令牌和过量参数的价值递减。最后，我们尝试了缓解数据稀缺的方法。

    The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity,
    
[^175]: 分布式强化学习的好处：小损失边界

    The Benefits of Being Distributional: Small-Loss Bounds for Reinforcement Learning. (arXiv:2305.15703v1 [cs.LG])

    [http://arxiv.org/abs/2305.15703](http://arxiv.org/abs/2305.15703)

    通过小损失边界的视角，我们提供了分布式RL好处的一个解释，该边界与实例相关的最优成本成比例。如果最优成本很小，分布式方法优于非分布式方法。

    

    虽然分布式强化学习已经取得了实证成果，但其何时何地有益的问题尚未得到回答。在这项工作中，通过小损失边界的视角，我们提供了分布式RL好处的一个解释，该边界与实例相关的最优成本成比例。如果最优成本很小，我们的边界会比非分布式方法更强。作为热身，我们展示了学习成本分布会在情境展开（CB）中导致小损失后悔边界，我们发现分布式CB在三个具有挑战性的任务上比最先进的技术在实证上表现更好。对于在线RL，我们提出了一个分布式版本空间算法，该算法使用最大似然估计构建置信区间，并证明了它在表格MDP中实现了小损失后悔，同时在潜变量模型中享有小损失PAC边界。以类似的见解为基础，我们提出了一个分布式离线RL算法

    While distributional reinforcement learning (RL) has demonstrated empirical success, the question of when and why it is beneficial has remained unanswered. In this work, we provide one explanation for the benefits of distributional RL through the lens of small-loss bounds, which scale with the instance-dependent optimal cost. If the optimal cost is small, our bounds are stronger than those from non-distributional approaches. As warmup, we show that learning the cost distribution leads to small-loss regret bounds in contextual bandits (CB), and we find that distributional CB empirically outperforms the state-of-the-art on three challenging tasks. For online RL, we propose a distributional version-space algorithm that constructs confidence sets using maximum likelihood estimation, and we prove that it achieves small-loss regret in the tabular MDPs and enjoys small-loss PAC bounds in latent variable models. Building on similar insights, we propose a distributional offline RL algorithm bas
    
[^176]: SPRING: GPT-4通过学习论文和推理在游戏中表现超过RL算法

    SPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning. (arXiv:2305.15486v1 [cs.AI])

    [http://arxiv.org/abs/2305.15486](http://arxiv.org/abs/2305.15486)

    SPRING是一个新的方法，能够在开放世界游戏中表现出色，它通过阅读游戏的原始学术论文并使用所学知识进行推理并玩游戏。。

    

    开放世界游戏由于其多任务、深度探索和目标优先级要求，对AI算法提出了重大挑战。尽管强化学习（RL）在解决游戏方面很受欢迎，但其高样本复杂性限制了它在像Crafter或Minecraft这样复杂的开放世界游戏中的有效性。我们提出了一种新颖的方法SPRING，通过阅读游戏的原始学术论文并使用所学知识进行推理并玩游戏，来解决这个问题。在给定LaTeX源作为游戏语境和代理当前观察的描述的情况下，我们的SPRING框架利用具有游戏相关问题的定向无环图（DAG）作为节点和依赖关系作为边。通过按拓扑顺序遍历DAG并计算每个节点的LLM响应来确定在环境中采取的最优行动，LLM对最终节点的答案直接转化为环境行动。在我们的实验中，我们研究了

    Open-world survival games pose significant challenges for AI algorithms due to their multi-tasking, deep exploration, and goal prioritization requirements. Despite reinforcement learning (RL) being popular for solving games, its high sample complexity limits its effectiveness in complex open-world games like Crafter or Minecraft. We propose a novel approach, SPRING, to read the game's original academic paper and use the knowledge learned to reason and play the game through a large language model (LLM). Prompted with the LaTeX source as game context and a description of the agent's current observation, our SPRING framework employs a directed acyclic graph (DAG) with game-related questions as nodes and dependencies as edges. We identify the optimal action to take in the environment by traversing the DAG and calculating LLM responses for each node in topological order, with the LLM's answer to final node directly translating to environment actions. In our experiments, we study the quality
    
[^177]: 知识设计：通过知识提炼推动蛋白质设计的极限

    Knowledge-Design: Pushing the Limit of Protein Deign via Knowledge Refinement. (arXiv:2305.15151v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2305.15151](http://arxiv.org/abs/2305.15151)

    本文提出了一种知识感知模块来提炼低质量残基，引入记忆检索机制实现了超过50%的训练时间节省，并取得了较好的性能表现，是蛋白质设计领域的一次创新。

    

    最近的研究表明，在蛋白质设计中，寻找折叠为所期望结构的氨基酸序列已经取得了竞争优势。然而，大多数研究忽略了预测置信度的重要性，未能覆盖广泛的蛋白质空间，并且没有融入常见的蛋白质知识。本文提出了一种知识感知模块来提炼低质量残基，并引入了一种记忆检索机制来节省超过50%的训练时间。我们在CATH、TS50和TS500数据集上对所提出的方法进行了广泛评估，结果显示我们的知识设计方法在CATH数据集上的性能超过了先前的PiFold方法约9％。具体来说，知识设计是第一个实现了...

    Recent studies have shown competitive performance in protein design that aims to find the amino acid sequence folding into the desired structure. However, most of them disregard the importance of predictive confidence, fail to cover the vast protein space, and do not incorporate common protein knowledge. After witnessing the great success of pretrained models on diverse protein-related tasks and the fact that recovery is highly correlated with confidence, we wonder whether this knowledge can push the limits of protein design further. As a solution, we propose a knowledge-aware module that refines low-quality residues. We also introduce a memory-retrieval mechanism to save more than 50\% of the training time. We extensively evaluate our proposed method on the CATH, TS50, and TS500 datasets and our results show that our Knowledge-Design method outperforms the previous PiFold method by approximately 9\% on the CATH dataset. Specifically, Knowledge-Design is the first method that achieves 
    
[^178]: 任意分辨率气候数据降尺度的傅里叶神经算子

    Fourier Neural Operators for Arbitrary Resolution Climate Data Downscaling. (arXiv:2305.14452v1 [cs.LG])

    [http://arxiv.org/abs/2305.14452](http://arxiv.org/abs/2305.14452)

    提出了一种基于傅里叶神经算子的任意分辨率气候数据降尺度方法，通过小采样训练，并能将其输入零样本降尺度到任意未见高分辨率，显著优于现有的降尺度模型。

    

    气候模拟在引导我们了解气候变化和应对其影响方面具有重要作用。然而，以高空间分辨率来解析复杂的气候过程计算成本高昂。为了加速气候模拟，神经网络已用于从快速运行的低分辨率模拟中降尺度气候变量，但高分辨率训练数据通常难以获得或缺乏，大大限制了准确性。在这项工作中，我们提出了一种基于傅里叶神经算子的降尺度方法。它使用小的上采样因子的数据进行训练，然后可以将其输入零样本降尺度到任意未见高分辨率。我们的降尺度模型在ERA5气候模型数据和Navier-Stokes方程解法数据上进行评估，在标准单分辨率降尺度和零样本推广到更高上采样因子方面，均显著优于现有的卷积和生成对抗式降尺度模型。

    Climate simulations are essential in guiding our understanding of climate change and responding to its effects. However, it is computationally expensive to resolve complex climate processes at high spatial resolution. As one way to speed up climate simulations, neural networks have been used to downscale climate variables from fast-running low-resolution simulations, but high-resolution training data are often unobtainable or scarce, greatly limiting accuracy. In this work, we propose a downscaling method based on the Fourier neural operator. It trains with data of a small upsampling factor and then can zero-shot downscale its input to arbitrary unseen high resolution. Evaluated both on ERA5 climate model data and on the Navier-Stokes equation solution data, our downscaling model significantly outperforms state-of-the-art convolutional and generative adversarial downscaling models, both in standard single-resolution downscaling and in zero-shot generalization to higher upsampling facto
    
[^179]: 作为奖励的视频预测模型用于强化学习

    Video Prediction Models as Rewards for Reinforcement Learning. (arXiv:2305.14343v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.14343](http://arxiv.org/abs/2305.14343)

    本文提出了VIPER算法，利用预训练的视频预测模型作为强化学习的奖励信号来学习复杂行为，从而实现在广泛任务范围内的专家级控制，同时具有泛化性。

    

    在强化学习中，制定让代理学习复杂行为的奖励信号一直是一个长期的挑战。一种有前途的方法是从广泛可用于互联网上的无标注视频中提取行为偏好。我们提出了Video Prediction Rewards (VIPER)，这种算法利用预训练的视频预测模型作为不需要行为干预的强化学习奖励信号。具体而言，我们首先在专家视频上训练一个自回归Transformer，然后将视频预测可能性用作强化学习代理的奖励信号。VIPER使得在DMC、Atari和RLBench任务等广泛的任务范围内，在没有编程任务奖励的情况下实现专家级的控制。此外，视频预测模型的泛化使得我们能够为没有专家数据可用的分布外环境导出奖励信号，从而实现桌面操纵的跨体现能力。我们认为我们的工作是具有伸缩性的奖励制定的起点。

    Specifying reward signals that allow agents to learn complex behaviors is a long-standing challenge in reinforcement learning. A promising approach is to extract preferences for behaviors from unlabeled videos, which are widely available on the internet. We present Video Prediction Rewards (VIPER), an algorithm that leverages pretrained video prediction models as action-free reward signals for reinforcement learning. Specifically, we first train an autoregressive transformer on expert videos and then use the video prediction likelihoods as reward signals for a reinforcement learning agent. VIPER enables expert-level control without programmatic task rewards across a wide range of DMC, Atari, and RLBench tasks. Moreover, generalization of the video prediction model allows us to derive rewards for an out-of-distribution environment where no expert data is available, enabling cross-embodiment generalization for tabletop manipulation. We see our work as starting point for scalable reward s
    
[^180]: 关于高斯-斯坦变分梯度下降动态性的探究

    Towards Understanding the Dynamics of Gaussian--Stein Variational Gradient Descent. (arXiv:2305.14076v1 [math.ST])

    [http://arxiv.org/abs/2305.14076](http://arxiv.org/abs/2305.14076)

    本文探究了高斯-斯坦变分梯度下降动态性。对于从高斯目标中采样，只要初始值是高斯的，具有双线性核的SVGD动态将保持高斯状态。当目标函数呈现出强对数凹性时，证明了均场高斯-SVGD动态会线性收敛于KL散度下最接近目标高斯分布。在有限粒子设置中，存在对均场极限的时间微步一致收敛以及线性收敛至目标高斯分布。

    

    Stein Variational Gradient Descent (SVGD)是一种非参数基于粒子的确定性采样算法。尽管其被广泛使用，但理解SVGD的理论属性一直是一个具有挑战性的问题。对于从高斯目标中采样，只要初始值是高斯的，具有双线性核的SVGD动态将保持高斯状态。受此事实的启发，我们通过双线性核将SVGD投影到高斯分布族中，即高斯变分推断 (GVI) 与 SVGD。我们通过考虑均场 PDE 和离散粒子系统，提供了一个完整的图像。当目标函数呈现出强对数凹性时，证明了均场高斯-SVGD动态会线性收敛于KL散度下最接近目标高斯分布。在有限粒子设置中，存在对均场极限的时间微步一致收敛以及线性收敛至目标高斯分布。我们的分析基于一个新的代数恒等式，该等式将目标高斯分布的费希尔信息矩阵与粒子均匀分布的费希尔信息矩阵相关联。这个等式为我们提供了透视 GVI with SVGD 在均场和粒子设置中的动态性的统一视角。

    Stein Variational Gradient Descent (SVGD) is a nonparametric particle-based deterministic sampling algorithm. Despite its wide usage, understanding the theoretical properties of SVGD has remained a challenging problem. For sampling from a Gaussian target, the SVGD dynamics with a bilinear kernel will remain Gaussian as long as the initializer is Gaussian. Inspired by this fact, we undertake a detailed theoretical study of the Gaussian-SVGD, i.e., SVGD projected to the family of Gaussian distributions via the bilinear kernel, or equivalently Gaussian variational inference (GVI) with SVGD. We present a complete picture by considering both the mean-field PDE and discrete particle systems. When the target is strongly log-concave, the mean-field Gaussian-SVGD dynamics is proven to converge linearly to the Gaussian distribution closest to the target in KL divergence. In the finite-particle setting, there is both uniform in time convergence to the mean-field limit and linear convergence in ti
    
[^181]: 通过样本重新加权与样本关联测试进行失语症语音的无偏自动语音识别

    Debiased Automatic Speech Recognition for Dysarthric Speech via Sample Reweighting with Sample Affinity Test. (arXiv:2305.13108v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2305.13108](http://arxiv.org/abs/2305.13108)

    本文提出了一种样本重新加权与样本关联测试（Re-SAT）的新方法，用于缓解失语症患者的偏差问题，在不影响健康患者语音的ASR性能的情况下，有效提高了ASR的性能表现。

    

    基于深度学习的自动语音识别系统主要是通过经验风险最小化（ERM）进行训练的。由于ERM利用数据样本的平均表现而不考虑一个群体，例如健康或失语症患者，因此ASR系统无法识别跨群体的性能差异，导致ASR系统存在偏差且其群体性能差异严重。本研究旨在提高语音识别系统的群体稳健性，针对失语症患者进行改进。为了实现我们的目标，我们提出了一种新方法，即样本重新加权与样本关联测试（Re-SAT）。 Re-SAT系统地衡量所给数据样本的去偏帮助性，并通过去偏帮助性加权来缓解偏差。实验结果表明， Re-SAT有助于改善失语症语音的ASR性能，而不会影响健康语音的ASR性能。

    Automatic speech recognition systems based on deep learning are mainly trained under empirical risk minimization (ERM). Since ERM utilizes the averaged performance on the data samples regardless of a group such as healthy or dysarthric speakers, ASR systems are unaware of the performance disparities across the groups. This results in biased ASR systems whose performance differences among groups are severe. In this study, we aim to improve the ASR system in terms of group robustness for dysarthric speakers. To achieve our goal, we present a novel approach, sample reweighting with sample affinity test (Re-SAT). Re-SAT systematically measures the debiasing helpfulness of the given data sample and then mitigates the bias by debiasing helpfulness-based sample reweighting. Experimental results demonstrate that Re-SAT contributes to improved ASR performance on dysarthric speech without performance degradation on healthy speech.
    
[^182]: 切线空间中的任务算术：预训练模型改进的方法

    Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models. (arXiv:2305.12827v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.12827](http://arxiv.org/abs/2305.12827)

    本文研究了在切线空间中进行任务算术的方法，发现权重分离是其有效的关键因素。我们提出了一种简单而有效的基于切线空间中的任务算术技术Tan，其优于现有的最先进方法。

    

    最近，任务算术已经成为一种经济高效且可扩展的方法，可以直接在权重空间中编辑预训练模型：通过添加不同任务的微调权重，可以提高模型在这些任务上的性能，而抵消它们则会导致任务遗忘。然而，我们对任务算法的有效性和其基本原理的理解仍然有限。本文在视觉语言模型中对任务算法进行了全面研究，并表明权重分离是使其有效的关键因素。这种属性在预训练期间出现，并在权重空间中的不同方向上产生，在与任务相关的函数空间中治理独立的局部区域时体现。值得注意的是，我们发现通过将模型线性化以在切线空间中微调模型可以放大权重分离。这导致在多个任务算法基准和不同模型上实现了实质性的性能改进。基于这些发现，我们提出了一种简单而有效的基于切线空间中的任务算术技术Tan，其优于现有的最先进方法。我们的方法依赖于一种将任务权重增量投影到切线空间上的新投影，确保编辑的权重保持接近预训练流形。我们的研究为任务算术的工作原理提供了新的见解，并指出权重分离是使其成为可能的基本机制。

    Task arithmetic has recently emerged as a cost-effective and scalable approach to edit pre-trained models directly in weight space: By adding the fine-tuned weights of different tasks, the model's performance can be improved on these tasks, while negating them leads to task forgetting. Yet, our understanding of the effectiveness of task arithmetic and its underlying principles remains limited. We present a comprehensive study of task arithmetic in vision-language models and show that weight disentanglement is the crucial factor that makes it effective. This property arises during pre-training and manifests when distinct directions in weight space govern separate, localized regions in function space associated with the tasks. Notably, we show that fine-tuning models in their tangent space by linearizing them amplifies weight disentanglement. This leads to substantial performance improvements across multiple task arithmetic benchmarks and diverse models. Building on these findings, we pr
    
[^183]: 多样化深度集成：一种使用显著性图的方法以增强OOD检测、校准和准确性

    Diversifying Deep Ensembles: A Saliency Map Approach for Enhanced OOD Detection, Calibration, and Accuracy. (arXiv:2305.11616v1 [cs.CV])

    [http://arxiv.org/abs/2305.11616](http://arxiv.org/abs/2305.11616)

    这项研究提出了一种使用显著性图来促进深度集成多样性的方法，用于改善OOD检测、校准和准确性，能够优于传统的集成技术，并在OpenOOD基准测试上证明了其有效性。

    

    深度集成在分类和 OOD 检测方面取得了最先进的成果；然而，由于集成中学习的模式的同质性，它们的效果仍然有限。为了克服这一挑战，本研究引入了一种促进集成成员之间多样性的新方法，该方法利用显著性图。通过整合显著性图多样化，我们的方法在多个分类和OOD检测任务中优于传统的集成技术，同时也提高了校准性。在已建立的OpenOOD基准测试上的实验凸显了我们的方法在实际应用中的潜力。

    Deep ensembles achieved state-of-the-art results in classification and out-of-distribution (OOD) detection; however, their effectiveness remains limited due to the homogeneity of learned patterns within the ensemble. To overcome this challenge, our study introduces a novel approach that promotes diversity among ensemble members by leveraging saliency maps. By incorporating saliency map diversification, our method outperforms conventional ensemble techniques in multiple classification and OOD detection tasks, while also improving calibration. Experiments on well-established OpenOOD benchmarks highlight the potential of our method in practical applications.
    
[^184]: 面向有条件生成对抗网络的少样本连续学习

    Few-Shot Continual Learning for Conditional Generative Adversarial Networks. (arXiv:2305.11400v1 [cs.LG])

    [http://arxiv.org/abs/2305.11400](http://arxiv.org/abs/2305.11400)

    本文提出了一种新的连续学习方法，适用于条件生成对抗网络，根据cGAN的判别器数据识别出最接近目标的现有模式，并通过扩展连续学习模型，使用回放生成的数据来训练目标模式的cGAN模型，以避免灾难性遗忘，提高了生成性能。

    

    在生成模型的少样本连续学习中，必须学习目标模式，并在不影响先前学习到的模式的情况下仅使用有限的样本。本文针对条件生成对抗网络提出了一种新的连续学习方法，基于一种新的用于生成建模的模式亲和力量度。我们的度量完全基于cGAN的判别器，可以识别最接近目标的现有模式。随后，我们通过包含基于最接近模式的加权标签来扩展连续学习模型。为了预防灾难性遗忘，我们首先使用cGAN的生成器生成带标签的数据样本，然后通过回放生成的数据来训练目标模式的cGAN模型。我们的实验结果证明了我们的方法在提高生成性能方面的有效性，超越了各种标准和最先进的方法。

    In few-shot continual learning for generative models, a target mode must be learned with limited samples without adversely affecting the previously learned modes. In this paper, we propose a new continual learning approach for conditional generative adversarial networks (cGAN) based on a new mode-affinity measure for generative modeling. Our measure is entirely based on the cGAN's discriminator and can identify the existing modes that are most similar to the target. Subsequently, we expand the continual learning model by including the target mode using a weighted label derived from those of the closest modes. To prevent catastrophic forgetting, we first generate labeled data samples using the cGAN's generator, and then train the cGAN model for the target mode while memory replaying with the generated data. Our experimental results demonstrate the efficacy of our approach in improving the generation performance over the baselines and the state-of-the-art approaches for various standard 
    
[^185]: 追赶蒸馏：加速采样只需一次训练

    Catch-Up Distillation: You Only Need to Train Once for Accelerating Sampling. (arXiv:2305.10769v1 [cs.LG])

    [http://arxiv.org/abs/2305.10769](http://arxiv.org/abs/2305.10769)

    本文提出了一种名为“追赶蒸馏”的方法，通过调整传统采样算法，让速度估计模型的当前时刻输出与其先前时刻输出和地面真实标签对齐，从而实现只需一次训练便能加速采样的效果。

    

    扩散概率模型在各种机器学习领域取得了令人瞩目的进展。然而，为了实现高质量的合成样本，通常需要执行大量的采样步骤，这阻碍了实时样本合成的可能性。传统的通过知识蒸馏加速采样的算法依赖于预训练的模型权重和离散时间步骤场景，需要额外的培训课程才能实现他们的目标。为了解决这些问题，我们提出了追赶蒸馏（CUD），它鼓励速度估计模型的当前时刻输出“追赶”其先前时刻输出。具体而言，CUD调整了原始的常微分方程（ODE）训练目标，以使当前时刻输出与地面真实标签和先前时刻输出对齐，利用基于龙格-库塔的多步对齐蒸馏进行精确的ODE估计，同时防止异步更新。

    Diffusion Probability Models (DPMs) have made impressive advancements in various machine learning domains. However, achieving high-quality synthetic samples typically involves performing a large number of sampling steps, which impedes the possibility of real-time sample synthesis. Traditional accelerated sampling algorithms via knowledge distillation rely on pre-trained model weights and discrete time step scenarios, necessitating additional training sessions to achieve their goals. To address these issues, we propose the Catch-Up Distillation (CUD), which encourages the current moment output of the velocity estimation model ``catch up'' with its previous moment output. Specifically, CUD adjusts the original Ordinary Differential Equation (ODE) training objective to align the current moment output with both the ground truth label and the previous moment output, utilizing Runge-Kutta-based multi-step alignment distillation for precise ODE estimation while preventing asynchronous updates
    
[^186]: RelationMatch：用于半监督学习的批内关系匹配技术

    RelationMatch: Matching In-batch Relationships for Semi-supervised Learning. (arXiv:2305.10397v1 [cs.LG])

    [http://arxiv.org/abs/2305.10397](http://arxiv.org/abs/2305.10397)

    RelationMatch是一种利用矩阵交叉熵（MCE）损失函数的方法，可以匹配批内关系，有效提高半监督学习和监督学习的性能。

    

    半监督学习通过利用少量标记数据和未标记数据中的信息，已经在许多领域取得了显着的成功。然而，现有算法通常集中在来自相同来源的成对数据点的预测对准上，并忽略了每个批次内的点间关系。本文介绍了一种新方法RelationMatch，它利用一种矩阵交叉熵（MCE）损失函数来发掘批内关系。通过应用MCE，我们的方法在各种视觉数据集中始终优于现有最先进的方法，如FixMatch和FlexMatch。值得注意的是，在仅使用40个标签的STL-10数据集上，我们观察到相对于FlexMatch有15.21％的显著提高。此外，我们将MCE应用于监督学习场景，并观察到了一致的改进。

    Semi-supervised learning has achieved notable success by leveraging very few labeled data and exploiting the wealth of information derived from unlabeled data. However, existing algorithms usually focus on aligning predictions on paired data points augmented from an identical source, and overlook the inter-point relationships within each batch. This paper introduces a novel method, RelationMatch, which exploits in-batch relationships with a matrix cross-entropy (MCE) loss function. Through the application of MCE, our proposed method consistently surpasses the performance of established state-of-the-art methods, such as FixMatch and FlexMatch, across a variety of vision datasets. Notably, we observed a substantial enhancement of 15.21% in accuracy over FlexMatch on the STL-10 dataset using only 40 labels. Moreover, we apply MCE to supervised learning scenarios, and observe consistent improvements as well.
    
[^187]: “我全然成为我自己”：以TGNB人群为中心，评估开放式语言生成中的偏见

    "I'm fully who I am": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation. (arXiv:2305.09941v1 [cs.CL])

    [http://arxiv.org/abs/2305.09941](http://arxiv.org/abs/2305.09941)

    本论文研究了如何以TGNB人群的声音为中心，评估开放式语言生成中的偏见。通过理解TGNB个体的经历，提出了以TGNB人群为中心的OLG系统评估框架，并且包括一个为TGNB人群设计的调查工具和分析方法。

    

    跨性别和非二元（TGNB）人群在日常生活中经历了不成比例的歧视和排斥。随着语言生成技术的日益普及和应用，进一步边缘化这一人群的可能性也在增加。虽然大量的NLP公平文献着重于阐明和解决性别偏见，但评估TGNB身份所带来的性别伤害需要理解这些身份如何独特地与社会性别规范互动以及与性别二元中心的视角相区分。这样的测量框架本质上需要以TGNB声音为中心，帮助指导包容性别的自然语言处理应该为谁服务。为实现这一目标，我们以TGNB社区和现有的跨学科文献为基础，评估了TGNB个体经历边缘化所形成的社会现实是如何影响和存在于开放式语言生成（OLG）中。首先理解TGNB个体的经历，我们提出了一个评估OLG系统的框架，旨在以TGNB人群为中心，度量与该人群相关的偏见。我们的框架包括特别为TGNB人群设计的调查工具，以及交叉分析结果的交叉方法。我们相信，这项工作将有助于实现更公平、更包容的自然语言处理社区，并潜在地解决NLP研究中广泛的交叉身份问题。

    Transgender and non-binary (TGNB) individuals disproportionately experience discrimination and exclusion from daily life. Given the recent popularity and adoption of language generation technologies, the potential to further marginalize this population only grows. Although a multitude of NLP fairness literature focuses on illuminating and addressing gender biases, assessing gender harms for TGNB identities requires understanding how such identities uniquely interact with societal gender norms and how they differ from gender binary-centric perspectives. Such measurement frameworks inherently require centering TGNB voices to help guide the alignment between gender-inclusive NLP and whom they are intended to serve. Towards this goal, we ground our work in the TGNB community and existing interdisciplinary literature to assess how the social reality surrounding experienced marginalization by TGNB persons contributes to and persists within Open Language Generation (OLG). By first understandi
    
[^188]: 带有注意力模型的视觉问答算法分析

    Analysis of Visual Question Answering Algorithms with attention model. (arXiv:2305.09782v1 [cs.CV])

    [http://arxiv.org/abs/2305.09782](http://arxiv.org/abs/2305.09782)

    本文批评性地检查和审查了使用共同注意力方法的VQA算法的方法，重点关注文本语义生成、对象识别和答案分类技术。

    

    视觉问答（VQA）使用图像处理算法处理图像，使用自然语言处理方法理解并回答问题。 VQA 对视觉受损者有帮助，可用于安全监控系统和从网络中学习的在线聊天机器人。 它使用自然语言处理方法学习问题的语义并提取文本特征。 计算机视觉技术用于以一种能够识别所问问题涉及的物体的方式生成图像表示。 注意力模型试图模仿人类根据语境关注图像不同区域的行为。 本文批评性地检查和审查了使用共同注意力方法的 VQA 算法的方法，例如生成文本语义，识别对象和答案分类技术。

    Visual question answering (VQA) usesimage processing algorithms to process the image and natural language processing methods to understand and answer the question. VQA is helpful to a visually impaired person, can be used for the security surveillance system and online chatbots that learn from the web. It uses NLP methods to learn the semantic of the question and to derive the textual features. Computer vision techniques are used for generating image representation in such a way that they can identify the objects about which question is asked. The Attention model tries to mimic the human behavior of giving attention to a different region of an image according to our understanding of its context. This paper critically examines and reviews methods of VQA algorithm such as generation of semantics of text, identification of objects and answer classification techniques that use the co-attention approach.
    
[^189]: 文档理解数据集和评估（DUDE）

    Document Understanding Dataset and Evaluation (DUDE). (arXiv:2305.08455v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.08455](http://arxiv.org/abs/2305.08455)

    DUDE推出了一个新的数据集和评估方法，旨在创造一个更实际的基准测试并推动当前方法的边界，以更准确地模拟真实世界的情况

    

    我们呼吁文档AI社区重新评估当前的方法论，拥抱创建更实际取向的基准测试的挑战。文档理解数据集和评估（DUDE）旨在纠正在理解视觉丰富文档（VRD）方面的研究进展停滞不前的情况。我们提供了一个新的数据集，其中包括与多行业、多领域和多页VRD相关的问题类型、答案和文档布局的创新，具有各种来源和日期。此外，我们通过创建多任务和多领域的评估设置来推动当前方法的边界，这些设置更准确地模拟了真实世界的情况，在这些情况下需要在低资源环境下进行强大的泛化和适应。DUDE旨在成为一个更实际、更长期的基准测试标准，并希望它会引领未来的扩展和贡献，以应对真实世界的挑战。最后，我们的工作说明了以下重要性。

    We call on the Document AI (DocAI) community to reevaluate current methodologies and embrace the challenge of creating more practically-oriented benchmarks. Document Understanding Dataset and Evaluation (DUDE) seeks to remediate the halted research progress in understanding visually-rich documents (VRDs). We present a new dataset with novelties related to types of questions, answers, and document layouts based on multi-industry, multi-domain, and multi-page VRDs of various origins, and dates. Moreover, we are pushing the boundaries of current methods by creating multi-task and multi-domain evaluation setups that more accurately simulate real-world situations where powerful generalization and adaptation under low-resource settings are desired. DUDE aims to set a new standard as a more practical, long-standing benchmark for the community, and we hope that it will lead to future extensions and contributions that address real-world challenges. Finally, our work illustrates the importance o
    
[^190]: 在电子商务中使用数据增强实现一致的文本分类

    Consistent Text Categorization using Data Augmentation in e-Commerce. (arXiv:2305.05402v1 [cs.LG])

    [http://arxiv.org/abs/2305.05402](http://arxiv.org/abs/2305.05402)

    本文提出了一种在电子商务中使用数据增强实现一致的文本分类的新框架，该框架旨在改进产品分类模型的一致性，同时保持其生产水平的性能。

    

    大规模电子商务数据分类是一项关键的、广泛应用于工业领域的任务。本文旨在改进一家主要网络公司已经在使用的产品分类模型，该模型用于多种应用。在该模型核心中，产品分类模型是一个文本分类模型，接受产品标题作为输入，并从数千个可用候选项中输出最合适的类别。经过进一步观察，我们发现了类似物品标签上的不一致性。例如，标题中关于颜色或尺寸的小变化，会对模型产生较大影响。这种现象可能会对下游的推荐或搜索应用造成负面影响，导致用户体验下降。为了解决这个问题，我们提出了一个新的框架，实现一致的文本分类。我们的目标是提高模型的一致性，并保持其生产水平的性能。

    The categorization of massive e-Commerce data is a crucial, well-studied task, which is prevalent in industrial settings. In this work, we aim to improve an existing product categorization model that is already in use by a major web company, serving multiple applications. At its core, the product categorization model is a text classification model that takes a product title as an input and outputs the most suitable category out of thousands of available candidates. Upon a closer inspection, we found inconsistencies in the labeling of similar items. For example, minor modifications of the product title pertaining to colors or measurements majorly impacted the model's output. This phenomenon can negatively affect downstream recommendation or search applications, leading to a sub-optimal user experience.  To address this issue, we propose a new framework for consistent text categorization. Our goal is to improve the model's consistency while maintaining its production-level performance. W
    
[^191]: GNNs: 可以更强、更新、更快

    GNNs,You can be Stronger,Deeper and Faster. (arXiv:2305.05368v1 [cs.LG])

    [http://arxiv.org/abs/2305.05368](http://arxiv.org/abs/2305.05368)

    本文介绍了一种新的理解GNN表现能力的视角，并提出了一个新的采样节点级残差模块SDF，具有更好的表现能力。

    

    图神经网络（GNNs）是一类可以从图结构数据中学习并通过集成邻居节点的表示学习来表现出色的神经网络。然而，GNN的性能会随着层数增加而逐渐降低。本文引入了一个新的概念——k跳子图聚合，提出了一种新的理解GNN表现能力的视角，揭示了传统深层GNN表现逐渐退化的潜在原因，包括聚合子图的重叠以及基于残差的GNN实际上利用了1到k跳子图聚合结果来提高有效性。此外，我们提出了一种新的采样节点级残差模块SDF，通过理论推导证明其比之前的残差方法具有更优的表现能力，可以利用1到k跳跃子图的信息。

    Graph neural networks (GNNs), a type of neural network that can learn from graph-structured data and learn the representation of nodes by aggregating their neighbors, have shown excellent performance in downstream tasks.However, it is known that the performance of graph neural networks (GNNs) degrades gradually as the number of layers increases. Based on k-hop subgraph aggregation, which is a new concept, we propose a new perspective to understand the expressive power of GNN.From this perspective, we reveal the potential causes of the performance degradation of the deep traditional GNN - aggregated subgraph overlap, and the fact that the residual-based graph neural networks in fact exploit the aggregation results of 1 to k hop subgraphs to improve the effectiveness.Further, we propose a new sampling-based node-level residual module named SDF, which is shown by theoretical derivation to obtain a superior expressive power compared to previous residual methods by using information from 1 
    
[^192]: 用于加权因果 DAG 的新度量和搜索算法

    New metrics and search algorithms for weighted causal DAGs. (arXiv:2305.04445v1 [cs.LG])

    [http://arxiv.org/abs/2305.04445](http://arxiv.org/abs/2305.04445)

    本研究提供了针对加权因果 DAGs的新度量和搜索算法，发现了用于自适应干预的因果图，提供了一个新的基准来捕捉搜索算法的最坏干预成本，并提供自适应搜索算法实现对数逼近。

    

    从数据中恢复因果关系是一个重要的问题。在使用观测数据时，只能恢复到一个马尔科夫等价类的因果图，并且需要额外的假设或干预数据来完成恢复。本文在一些标准假设下，通过节点相关干预成本的自适应干预，研究因果图发现。对于这种情况，我们证明没有算法能够比验证次数的顺序更好地实现渐近保证，验证次数是自适应搜索算法的一个成熟基准。在这个负面结果的基础上，我们定义了一个捕捉任何搜索算法最坏干预成本的新基准。此外，针对这个新基准，我们提供了自适应搜索算法，在各种设置下都能实现对数逼近：原子、有界大小的干预和广义成本。

    Recovering causal relationships from data is an important problem. Using observational data, one can typically only recover causal graphs up to a Markov equivalence class and additional assumptions or interventional data are needed for complete recovery. In this work, under some standard assumptions, we study causal graph discovery via adaptive interventions with node-dependent interventional costs. For this setting, we show that no algorithm can achieve an approximation guarantee that is asymptotically better than linear in the number of vertices with respect to the verification number; a well-established benchmark for adaptive search algorithms. Motivated by this negative result, we define a new benchmark that captures the worst-case interventional cost for any search algorithm. Furthermore, with respect to this new benchmark, we provide adaptive search algorithms that achieve logarithmic approximations under various settings: atomic, bounded size interventions and generalized cost o
    
[^193]: 离散扩散建模下的高效和度数引导图生成

    Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling. (arXiv:2305.04111v1 [cs.LG])

    [http://arxiv.org/abs/2305.04111](http://arxiv.org/abs/2305.04111)

    本文提出了EDGE，一种新的离散扩散模型用于生成大型图，并通过删除边来鼓励图的稀疏性。EDGE在每个去噪步骤中仅关注图中一部分节点，并且可以明确地对图的节点度数进行建模。实验表明，EDGE比竞争方法更有效，并且可以生成具有数千个节点的大型图。

    

    基于扩散的生成图模型已被证明在生成高质量小图方面非常有效。然而，它们需要更可扩展性，以生成包含数千个节点的大图并满足图统计。本文提出了EDGE，一种新的基于扩散的生成图模型，用于生成大型图的生成任务。为了提高计算效率，我们通过在每个时间步长随机删除边来鼓励图的稀疏性，并最终获得一张空白图。EDGE仅在每个去噪步骤中关注图中一部分节点。它比以前的基于扩散的模型更少地进行边预测。此外，EDGE明确地允许对图的节点度数进行建模，进一步提高了模型的性能。实证研究表明，EDGE比竞争方法更有效，并且可以生成具有数千个节点的大型图。它还在生成质量方面优于基准模型。

    Diffusion-based generative graph models have been proven effective in generating high-quality small graphs. However, they need to be more scalable for generating large graphs containing thousands of nodes desiring graph statistics. In this work, we propose EDGE, a new diffusion-based generative graph model that addresses generative tasks with large graphs. To improve computation efficiency, we encourage graph sparsity by using a discrete diffusion process that randomly removes edges at each time step and finally obtains an empty graph. EDGE only focuses on a portion of nodes in the graph at each denoising step. It makes much fewer edge predictions than previous diffusion-based models. Moreover, EDGE admits explicitly modeling the node degrees of the graphs, further improving the model performance. The empirical study shows that EDGE is much more efficient than competing methods and can generate large graphs with thousands of nodes. It also outperforms baseline models in generation qual
    
[^194]: 基于空中计算的半异步联邦边缘学习机制

    Semi-Asynchronous Federated Edge Learning Mechanism via Over-the-air Computation. (arXiv:2305.04066v1 [cs.LG])

    [http://arxiv.org/abs/2305.04066](http://arxiv.org/abs/2305.04066)

    本文提出了一种半异步聚合FEEL机制PAOTA，以改善数据和设备存在显著异质性的情况下FEEL系统的训练效率，通过调整边缘设备的上行传输功率来最小化FEEL全局模型的收敛上界。实验结果表明，所提出的机制在达到相同的目标精度下，训练速度显著快于具有空中计算方案的传统同步FEEL机制。

    

    空中计算是提高联邦边缘学习（FEEL）效率的有效传输方案。然而，现有的具有空中计算方案的FEEL系统通常在每个全局轮次中采用传统的同步聚合机制，而这些机制容易受到滞后者的影响。本文提出了一种基于空中计算方案的半异步聚合FEEL机制（PAOTA），以改善数据和设备存在显著异质性的情况下FEEL系统的训练效率。考虑到来自边缘设备模型更新的陈旧性和发散性，我们通过在每个聚合期调整边缘设备的上行传输功率来最小化FEEL全局模型的收敛上界。模拟结果表明，我们提出的算法实现了接近理想的局部SGD的收敛性能。此外，在相同的目标准确度下，所提出的机制的训练速度显着快于具有空中计算方案的传统同步FEEL机制。

    Over-the-air Computation (AirComp) has been demonstrated as an effective transmission scheme to boost the efficiency of federated edge learning (FEEL). However, existing FEEL systems with AirComp scheme often employ traditional synchronous aggregation mechanisms for local model aggregation in each global round, which suffer from the stragglers issues. In this paper, we propose a semi-asynchronous aggregation FEEL mechanism with AirComp scheme (PAOTA) to improve the training efficiency of the FEEL system in the case of significant heterogeneity in data and devices. Taking the staleness and divergence of model updates from edge devices into consideration, we minimize the convergence upper bound of the FEEL global model by adjusting the uplink transmit power of edge devices at each aggregation period. The simulation results demonstrate that our proposed algorithm achieves convergence performance close to that of the ideal Local SGD. Furthermore, with the same target accuracy, the training
    
[^195]: 大纲先行，细节后至：基于语法引导的粗-细代码生成

    Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation. (arXiv:2305.00909v2 [cs.PL] UPDATED)

    [http://arxiv.org/abs/2305.00909](http://arxiv.org/abs/2305.00909)

    提出一种基于语法引导的粗-细代码生成模型，支持从粗到细的多次迭代，实现了更加符合人脑思维方式的代码编写方式。

    

    对于一个复杂算法的实现，人类程序员的做法通常是先概述一下控制流程，然后迭代进行丰富，最终生成一些精心加工的语法结构和层次变量。然而，现有的大型语言模型一次性生成代码，没有中间环节，以反映"大纲先行，细节后至"的结构化思维过程。受到思维链提示的最新成功启发，我们提出了ChainCoder，这是一种程序综合语言模型，它逐步生成Python代码，即从粗到细进行多次迭代。我们首先通过抽象语法树解析将源代码分解为布局框架组件和附件组件，以构建层次表示。然后我们将预测目标重新启动，形成多次通过目标，每次生成一个子序列，这些子序列在层次结构中串联起来。最后，我们利用量身定制的Transformer体系结构来实现模型的优化。

    For a complicated algorithm, its implementation by a human programmer usually starts with outlining a rough control flow followed by iterative enrichments, eventually yielding carefully generated syntactic structures and variables in a hierarchy. However, state-of-the-art large language models generate codes in a single pass, without intermediate warm-ups to reflect the structured thought process of "outline-then-detail". Inspired by the recent success of chain-of-thought prompting, we propose ChainCoder, a program synthesis language model that generates Python code progressively, i.e. from coarse to fine in multiple passes. We first decompose source code into layout frame components and accessory components via abstract syntax tree parsing to construct a hierarchical representation. We then reform our prediction target into a multi-pass objective, each pass generates a subsequence, which is concatenated in the hierarchy. Finally, a tailored transformer architecture is leveraged to joi
    
[^196]: 基于AI的NOMA NTNs中的无线电和计算资源分配和路径规划：在CSI不确定性下最小化AoI

    AI-based Radio and Computing Resource Allocation and Path Planning in NOMA NTNs: AoI Minimization under CSI Uncertainty. (arXiv:2305.00780v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2305.00780](http://arxiv.org/abs/2305.00780)

    本论文提出一种基于高空平台和无人机构成的分层式空中计算框架，旨在通过调整航迹和资源分配来最小化用户的信息年龄，并受到多重资源约束和信道状态不确定性的限制。结果表明，所提出的方法可以显著减少信息年龄。

    

    本文提出了一个分层的空中计算框架，由高空平台（HAP）和无人机（UAV）组成，用于计算通过上行非正交多址（UL-NOMA）连接的陆地移动用户的完全卸载任务。为了更好地评估计算密集型应用程序中信息的新鲜度，考虑了信息年龄（AoI）的标准。特别地，通过调整UAV和HAP上的资源分配和UAV的轨迹，以最小化具有弹性任务的用户的平均AoI，并受到CSI不确定性和UAV和HAP的多重资源约束的限制。为了解决这个非凸优化问题，提出了两种方法：多智能体深度确定性策略梯度（MADDPG）和联合强化学习（FRL）来设计UAV的轨迹，并获得通道、功率和CPU分配。结果表明，与传统方法相比，所提出的方法可以显著减少AoI。

    In this paper, we develop a hierarchical aerial computing framework composed of high altitude platform (HAP) and unmanned aerial vehicles (UAVs) to compute the fully offloaded tasks of terrestrial mobile users which are connected through an uplink non-orthogonal multiple access (UL-NOMA). To better assess the freshness of information in computation-intensive applications the criterion of age of information (AoI) is considered. In particular, the problem is formulated to minimize the average AoI of users with elastic tasks, by adjusting UAVs trajectory and resource allocation on both UAVs and HAP, which is restricted by the channel state information (CSI) uncertainty and multiple resource constraints of UAVs and HAP. In order to solve this non-convex optimization problem, two methods of multi-agent deep deterministic policy gradient (MADDPG) and federated reinforcement learning (FRL) are proposed to design the UAVs trajectory, and obtain channel, power, and CPU allocations. It is shown 
    
[^197]: GPT-2是如何计算大于符号的？解释预训练语言模型中的数学能力

    How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. (arXiv:2305.00586v1 [cs.CL])

    [http://arxiv.org/abs/2305.00586](http://arxiv.org/abs/2305.00586)

    本研究运用机械式可解释性技术探究了GPT-2 Small的数学能力，并确定了它的计算图中的一个小电路用于计算大于符号，该电路的多层感知器提高了结束年份大于开始年份的概率，并且该电路具有广泛的适用性。

    

    预训练语言模型在未被明确训练的任务上表现出惊人的能力，但它们如何实现这些功能却不为人所知。本文通过机械式可解释性技术探究预训练语言模型通常具有的基本数学能力。具体来说，我们以GPT-2 Small为例，研究其能否通过输入"战争持续时间是从1732年到17年"，预测出有效的两位数字的截止年份 (大于32年)。我们首先确定了一个电路，即GPT-2 Small计算图的一个小子集，用于计算这个任务的输出，然后我们解释了每个电路组件的作用，显示出GPT-2 Small的最终多层感知器提高了结束年份大于开始年份的概率。最后，我们证明了我们的电路适用于其他任务，在其他大于场景中发挥作用。

    Pre-trained language models can be surprisingly adept at tasks they were not explicitly trained on, but how they implement these capabilities is poorly understood. In this paper, we investigate the basic mathematical abilities often acquired by pre-trained language models. Concretely, we use mechanistic interpretability techniques to explain the (limited) mathematical abilities of GPT-2 small. As a case study, we examine its ability to take in sentences such as "The war lasted from the year 1732 to the year 17", and predict valid two-digit end years (years > 32). We first identify a circuit, a small subset of GPT-2 small's computational graph that computes this task's output. Then, we explain the role of each circuit component, showing that GPT-2 small's final multi-layer perceptrons boost the probability of end years greater than the start year. Finally, we show that our circuit generalizes to other tasks, playing a role in other greater-than scenarios.
    
[^198]: 通过逆向神经渲染对动态场景进行物体中心体素化

    Object-Centric Voxelization of Dynamic Scenes via Inverse Neural Rendering. (arXiv:2305.00393v1 [cs.CV])

    [http://arxiv.org/abs/2305.00393](http://arxiv.org/abs/2305.00393)

    本文提出了一个逆向神经渲染框架DynaVol，可以在多实体动态场景中学习时间变化的体积表示，通过维护一个时间依赖的3D格点和联合学习格点级局部动态、物体级全局动态和组合神经辐射场来增强物体中心场景体素化的时空一致性。

    

    在无监督的3D场景中理解世界的组成动态非常具有挑战性。现有的方法要么未能有效利用时间线索，要么忽略了场景分解的多视角一致性。本文提出了DynaVol，一种逆向神经渲染框架，为多实体（如物体）的动态场景学习时间变化的体积表示提供了一个学习方法。它的主要贡献有两个。首先，它维护一个时间依赖的3D格点，动态而灵活地将空间位置绑定到不同的实体，从而在代表性水平上鼓励信息的分离。其次，我们的方法在端到端架构中联合学习格点级局部动态、物体级全局动态和组合神经辐射场，从而增强了物体中心场景体素化的时空一致性。我们提出了一个两阶段的DynaVol训练方案，并在合成和真实世界数据集上验证了它的有效性。

    Understanding the compositional dynamics of the world in unsupervised 3D scenarios is challenging. Existing approaches either fail to make effective use of time cues or ignore the multi-view consistency of scene decomposition. In this paper, we propose DynaVol, an inverse neural rendering framework that provides a pilot study for learning time-varying volumetric representations for dynamic scenes with multiple entities (like objects). It has two main contributions. First, it maintains a time-dependent 3D grid, which dynamically and flexibly binds the spatial locations to different entities, thus encouraging the separation of information at a representational level. Second, our approach jointly learns grid-level local dynamics, object-level global dynamics, and the compositional neural radiance fields in an end-to-end architecture, thereby enhancing the spatiotemporal consistency of object-centric scene voxelization. We present a two-stage training scheme for DynaVol and validate its ef
    
[^199]: 模拟形式转换器用于少样本3D解析

    Analogy-Forming Transformers for Few-Shot 3D Parsing. (arXiv:2304.14382v1 [cs.CV])

    [http://arxiv.org/abs/2304.14382](http://arxiv.org/abs/2304.14382)

    "模拟网络"模型在3D物体场景分割中采用类比推理，通过在内存中检索相关场景并预测类似结构进行分割，能够在一发、少发或多发学习中得出相似的解析，与最新的3D分割变压器模型相竞争。

    

    我们提出了一种称为“模拟网络”的模型，它在一组有标记的结构化3D场景中显式地编码领域知识（作为模型参数的一部分），并通过类比推理对3D物体场景进行分割：我们的模型首先从内存中检索相关场景及其相应的部分结构，然后通过端到端可学习的调制机制为输入场景预测类似的部分结构，而不是直接将场景映射到部分分割。通过对多个检索的记忆进行条件控制，预测混合匹配检索记忆的结构合成。在“模拟网络”中，一发、少发或多发学习被一致地处理，通过对适当的记忆集进行条件谓词，无论是从单个、少数还是许多存储实例中继承相似的解析。我们展示了“模拟网络”在许多样本情况下与最新的3D分割变压器模型相竞争。

    We present Analogical Networks, a model that encodes domain knowledge explicitly, in a collection of structured labelled 3D scenes, in addition to implicitly, as model parameters, and segments 3D object scenes with analogical reasoning: instead of mapping a scene to part segments directly, our model first retrieves related scenes from memory and their corresponding part structures, and then predicts analogous part structures for the input scene, via an end-to-end learnable modulation mechanism. By conditioning on more than one retrieved memories, compositions of structures are predicted, that mix and match parts across the retrieved memories. One-shot, few-shot or many-shot learning are treated uniformly in Analogical Networks, by conditioning on the appropriate set of memories, whether taken from a single, few or many memory exemplars, and inferring analogous parses. We show Analogical Networks are competitive with state-of-the-art 3D segmentation transformers in many-shot settings, a
    
[^200]: 基于对比能量预测的确切能量引导扩散采样在离线强化学习中的应用

    Contrastive Energy Prediction for Exact Energy-Guided Diffusion Sampling in Offline Reinforcement Learning. (arXiv:2304.12824v1 [cs.LG])

    [http://arxiv.org/abs/2304.12824](http://arxiv.org/abs/2304.12824)

    本文提出了一种基于对比能量预测的确切能量引导扩散采样方法，用于离线强化学习中，可以解决中间引导估计的难题，并在D4RL基准测试上取得了优异的效果。

    

    引导采样是将扩散模型应用于现实任务的重要方法，其在采样过程中嵌入人类定义的指导。本文考虑了一般设置，其中指导是由一个（非标准化）能量函数定义的。这种设置的主要挑战是，在扩散采样过程中的中间指导是由采样分布和能量函数共同定义的，很难估计。为了解决这个挑战，我们提出了一个精确的中间指导配方以及一种名为对比能量预测（CEP）的新型训练目标来学习精确的中间指导。我们的方法在无限制的模型能力和数据样本下保证收敛到精确指导，而之前的方法无法做到。我们将其应用于离线强化学习中，并在D4RL基准测试上进行了广泛的实验，证明了我们的方法优于之前的最先进方法。

    Guided sampling is a vital approach for applying diffusion models in real-world tasks that embeds human-defined guidance during the sampling procedure. This paper considers a general setting where the guidance is defined by an (unnormalized) energy function. The main challenge for this setting is that the intermediate guidance during the diffusion sampling procedure, which is jointly defined by the sampling distribution and the energy function, is unknown and is hard to estimate. To address this challenge, we propose an exact formulation of the intermediate guidance as well as a novel training objective named contrastive energy prediction (CEP) to learn the exact guidance. Our method is guaranteed to converge to the exact guidance under unlimited model capacity and data samples, while previous methods can not. We demonstrate the effectiveness of our method by applying it to offline reinforcement learning (RL). Extensive experiments on D4RL benchmarks demonstrate that our method outperf
    
[^201]: CF-VAE：基于VAE和因果流的因果分离表示学习

    CF-VAE: Causal Disentangled Representation Learning with VAE and Causal Flows. (arXiv:2304.09010v1 [cs.LG])

    [http://arxiv.org/abs/2304.09010](http://arxiv.org/abs/2304.09010)

    本文提出了一种新的因果流以进行因果分离表示学习，设计了一个新模型CF-VAE，利用因果流增强了VAE编码器的分离能力，并展示了在合成和真实数据集上实现因果分离并进行干预实验的结果。

    

    学习分离表示在表示学习中至关重要，旨在学习数据的低维表示，其中每个维度对应一个潜在的生成因素。由于生成因素之间可能存在因果关系，因果分离表示学习已经受到广泛关注。本文首先提出了一种新的可以将因果结构信息引入模型中的流，称为因果流。基于广泛用于分离表示学习的变分自编码器（VAE），我们设计了一个新模型CF-VAE，利用因果流增强了VAE编码器的分离能力。通过进一步引入基准因素的监督，我们展示了我们模型的分离可识别性。在合成和真实数据集上的实验结果表明，CF-VAE可以实现因果分离并进行干预实验。

    Learning disentangled representations is important in representation learning, aiming to learn a low dimensional representation of data where each dimension corresponds to one underlying generative factor. Due to the possibility of causal relationships between generative factors, causal disentangled representation learning has received widespread attention. In this paper, we first propose a new flows that can incorporate causal structure information into the model, called causal flows. Based on the variational autoencoders(VAE) commonly used in disentangled representation learning, we design a new model, CF-VAE, which enhances the disentanglement ability of the VAE encoder by utilizing the causal flows. By further introducing the supervision of ground-truth factors, we demonstrate the disentanglement identifiability of our model. Experimental results on both synthetic and real datasets show that CF-VAE can achieve causal disentanglement and perform intervention experiments. Moreover, C
    
[^202]: Wasserstein PAC-Bayes 学习：泛化与优化之间的桥梁。

    Wasserstein PAC-Bayes Learning: A Bridge Between Generalisation and Optimisation. (arXiv:2304.07048v1 [stat.ML])

    [http://arxiv.org/abs/2304.07048](http://arxiv.org/abs/2304.07048)

    本文介绍了扩展的 Wasserstein PAC-Bayes 框架，利用损失函数上的几何假设提供了新的泛化界限。通过该框架，证明了 \cite{lambert2022variational} 中算法的输出具有强大的泛化能力。同时，建立了 PAC-Bayes 和优化算法之间的桥梁。

    

    PAC-Bayes 学习是一种已建立的框架，用于在训练阶段评估学习算法的泛化能力。然而，在训练之前，弄清楚为什么知名算法的输出具有良好的泛化特性而 PAC-Bayes 是否有用仍然具有挑战性。我们通过扩展简要介绍在文献 \cite{amit2022ipm} 中提出的 \emph{Wasserstein PAC-Bayes} 框架来积极回答这个问题。我们提供了新的泛化界限，利用损失函数上的几何假设。使用我们的框架，我们在任何训练之前就证明了 \cite{lambert2022variational} 中算法的输出具有强大的渐近泛化能力。更具体地说，我们展示了如何在泛化框架中将优化结果结合起来，构建了 PAC-Bayes 和优化算法之间的桥梁。

    PAC-Bayes learning is an established framework to assess the generalisation ability of learning algorithm during the training phase. However, it remains challenging to know whether PAC-Bayes is useful to understand, before training, why the output of well-known algorithms generalise well. We positively answer this question by expanding the \emph{Wasserstein PAC-Bayes} framework, briefly introduced in \cite{amit2022ipm}. We provide new generalisation bounds exploiting geometric assumptions on the loss function. Using our framework, we prove, before any training, that the output of an algorithm from \citet{lambert2022variational} has a strong asymptotic generalisation ability. More precisely, we show that it is possible to incorporate optimisation results within a generalisation framework, building a bridge between PAC-Bayes and optimisation algorithms.
    
[^203]: Token-and-Duration Transducer架构：联合预测标记与时长的高效序列转导

    Efficient Sequence Transduction by Jointly Predicting Tokens and Durations. (arXiv:2304.06795v1 [eess.AS])

    [http://arxiv.org/abs/2304.06795](http://arxiv.org/abs/2304.06795)

    本文提出了一种新型的序列转导架构TDT，它可以联合预测标记和持续时间，从而实现比传统Transducers更高的准确性和显着更快的推理速度。

    

    本文提出了一种用于序列到序列任务的新型Token-and-Duration Transducer(TDT)架构。TDT通过联合预测标记和持续时间，即发射的标记覆盖的输入帧的数量，来扩展传统的RNN-Transducer架构。它使用具有两个独立标准化输出的联合网络来生成标记和持续时间的分布。在推理期间，TDT模型可以通过预测的持续时间输出跳过输入帧，使其比逐帧处理编码器输出的传统Transducers显着更快。在不同的序列转导任务上，TDT模型均实现了更高的准确性和显着更快的推理速度。语音识别的TDT模型比RNN-Transducers获得更好的准确性，并且推理速度高达2.82倍。语音翻译的TDT模型与MUST-C测试相比提高了1个BLEU分数。

    This paper introduces a novel Token-and-Duration Transducer (TDT) architecture for sequence-to-sequence tasks. TDT extends conventional RNN-Transducer architectures by jointly predicting both a token and its duration, i.e. the number of input frames covered by the emitted token. This is achieved by using a joint network with two outputs which are independently normalized to generate distributions over tokens and durations. During inference, TDT models can skip input frames guided by the predicted duration output, which makes them significantly faster than conventional Transducers which process the encoder output frame by frame. TDT models achieve both better accuracy and significantly faster inference than conventional Transducers on different sequence transduction tasks. TDT models for Speech Recognition achieve better accuracy and up to 2.82X faster inference than RNN-Transducers. TDT models for Speech Translation achieve an absolute gain of over 1 BLEU on the MUST-C test compared wi
    
[^204]: 我们实现了个性化治疗吗？使用重复采样的在线强化学习算法进行个性化评估

    Did we personalize? Assessing personalization by an online reinforcement learning algorithm using resampling. (arXiv:2304.05365v1 [cs.LG])

    [http://arxiv.org/abs/2304.05365](http://arxiv.org/abs/2304.05365)

    本论文提出了一种使用重复采样的政策评估方法，以评估在线 RL 算法实现的个性化程度。该方法可用于优化数字健康的个性化干预。

    

    在数字健康中，使用强化学习（RL）个性化治疗序列以支持用户采取更健康的行为越来越受到关注。这种连续决策问题涉及到基于用户的上下文（例如，先前的活动水平、位置等）在何时治疗以及如何治疗的决定。在线RL算法是这个问题的一个有前途的数据驱动方法，因为它基于每个用户的历史反馈进行学习，并利用这些知识个性化这些决策。然而，要决定是否应在实际部署的“优化”干预中包含RL算法，我们必须评估数据证据，表明RL算法实际上正在将治疗个性化适应其用户。由于RL算法中的随机性，人们可能会对其在某些状态下的学习并使用此学习来提供特定治疗的能力产生误解。我们使用工作定义的个性化，并介绍了一种重复采样政策评估方法来评估在线RL算法实现的个性化水平。我们使用模拟评估了我们提出的方法，并展示了我们的方法可以准确地识别个性化的策略。我们提出的方法在优化数字健康的个性化干预方面具有潜在应用。

    There is a growing interest in using reinforcement learning (RL) to personalize sequences of treatments in digital health to support users in adopting healthier behaviors. Such sequential decision-making problems involve decisions about when to treat and how to treat based on the user's context (e.g., prior activity level, location, etc.). Online RL is a promising data-driven approach for this problem as it learns based on each user's historical responses and uses that knowledge to personalize these decisions. However, to decide whether the RL algorithm should be included in an ``optimized'' intervention for real-world deployment, we must assess the data evidence indicating that the RL algorithm is actually personalizing the treatments to its users. Due to the stochasticity in the RL algorithm, one may get a false impression that it is learning in certain states and using this learning to provide specific treatments. We use a working definition of personalization and introduce a resamp
    
[^205]: 关于图信号最优恢复的研究

    On the Optimal Recovery of Graph Signals. (arXiv:2304.00474v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.00474](http://arxiv.org/abs/2304.00474)

    本文提出了一种计算图信号最优或接近最优正则化参数的方法，为图信号处理问题提供了新的解释和超参数选择的新见解。

    

    在基于图的机器学习领域，从部分观测数据中学习出平滑的图信号是一个研究已久的任务。本文从最优恢复的角度考虑这个任务，这是从实际观测数据中学习函数的一种数学方法，它采用了与要学习的函数模型有关的最坏情形观点。先前的最优恢复研究表明，最小化正则化目标在一般问题类别上产生最优解，但未完全确定正则化参数。我们的主要贡献在于提供了一种方法来计算正则化参数，这些参数对于图信号处理问题是最优的或接近最优的（取决于设置）。我们的结果为基于图的学习中的经典优化技术提供了新的解释，并用于超参数选择的新见解。我们在几个半合成数值实验中展示了我们方法的潜力。

    Learning a smooth graph signal from partially observed data is a well-studied task in graph-based machine learning. We consider this task from the perspective of optimal recovery, a mathematical framework for learning a function from observational data that adopts a worst-case perspective tied to model assumptions on the function to be learned. Earlier work in the optimal recovery literature has shown that minimizing a regularized objective produces optimal solutions for a general class of problems, but did not fully identify the regularization parameter. Our main contribution provides a way to compute regularization parameters that are optimal or near-optimal (depending on the setting), specifically for graph signal processing problems. Our results offer a new interpretation for classical optimization techniques in graph-based learning and also come with new insights for hyperparameter selection. We illustrate the potential of our methods in numerical experiments on several semi-synth
    
[^206]: 对比学习是相似性图谱上的谱聚类

    Contrastive Learning Is Spectral Clustering On Similarity Graph. (arXiv:2303.15103v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.15103](http://arxiv.org/abs/2303.15103)

    本文通过证明标准InfoNCE损失下的对比学习等同于相似性图上的谱聚类，揭示了这种方法的内在等价性，并进一步将这种分析扩展到CLIP模型，提出新的核混合损失函数。

    

    对比学习是一种强大的自监督学习方法，但我们对其运作原理和原因的理论理解有限。本文通过证明标准InfoNCE损失下的对比学习等同于相似性图上的谱聚类，揭示了这种方法的内在等价性。利用这种等价性作为基石，我们将分析扩展到CLIP模型，并严格描述多模态对象如何被嵌入到一起。在理论洞见的推动下，我们引入了核混合损失，结合新颖的核函数，在多个视觉数据集上优于标准高斯核。

    Contrastive learning is a powerful self-supervised learning method, but we have a limited theoretical understanding of how it works and why it works. In this paper, we prove that contrastive learning with the standard InfoNCE loss is equivalent to spectral clustering on the similarity graph. Using this equivalence as the building block, we extend our analysis to the CLIP model and rigorously characterize how similar multi-modal objects are embedded together. Motivated by our theoretical insights, we introduce the kernel mixture loss, incorporating novel kernel functions that outperform the standard Gaussian kernel on several vision datasets.
    
[^207]: 基于几何感知的潜在表示学习用于建模Barrett食管疾病进程

    Geometry-Aware Latent Representation Learning for Modeling Disease Progression of Barrett's Esophagus. (arXiv:2303.12711v1 [eess.IV])

    [http://arxiv.org/abs/2303.12711](http://arxiv.org/abs/2303.12711)

    本文提出了一种基于几何思想的潜在表示学习方法，用于建模Barrett食管疾病进程，与传统方法相比，具有更好的重建损失。

    

    Barrett食管是食管腺癌的唯一先驱，这是一种在诊断时预后不良的食管癌症。因此，诊断Barrett食管对于预防和治疗食管癌至关重要。监督机器学习支持Barrett食管诊断，但组织病理学训练数据的高观察者变异限制了这些方法。用变分自动编码器(VAEs)进行无监督表示学习显示出潜在优势，因为它们将输入数据映射到具有仅有用特征的低维流形，为改进下游任务和见解将Barrett食管病程表征。然而，VAE的欧几里得潜在空间扭曲了点之间的关系，从而阻碍了疾病进展建模。几何VAEs为潜在空间提供附加几何结构，RHVAE假设为黎曼流形，$\mathcal{S}$-VAE假设为超球面流形。我们的研究表明，$\mathcal{S}$-VAE优于常规VAE，具有更好的重建损失。

    Barrett's Esophagus (BE) is the only precursor known to Esophageal Adenocarcinoma (EAC), a type of esophageal cancer with poor prognosis upon diagnosis. Therefore, diagnosing BE is crucial in preventing and treating esophageal cancer. While supervised machine learning supports BE diagnosis, high interobserver variability in histopathological training data limits these methods. Unsupervised representation learning via Variational Autoencoders (VAEs) shows promise, as they map input data to a lower-dimensional manifold with only useful features, characterizing BE progression for improved downstream tasks and insights. However, the VAE's Euclidean latent space distorts point relationships, hindering disease progression modeling. Geometric VAEs provide additional geometric structure to the latent space, with RHVAE assuming a Riemannian manifold and $\mathcal{S}$-VAE a hyperspherical manifold. Our study shows that $\mathcal{S}$-VAE outperforms vanilla VAE with better reconstruction losses, 
    
[^208]: Hamiltonian深度神经网络的万能逼近性质研究

    Universal Approximation Property of Hamiltonian Deep Neural Networks. (arXiv:2303.12147v1 [cs.LG])

    [http://arxiv.org/abs/2303.12147](http://arxiv.org/abs/2303.12147)

    本文研究了离散化的哈密顿神经常微分方程引起的Hamiltonian深度神经网络的通用逼近能力，证明了其中的一部分流可以逐渐逼近紧致域上的任何连续函数，为实际使用提供了理论基础。

    

    本文研究了由离散化的哈密顿神经常微分方程引起的Hamiltonian深度神经网络（HDNN）的通用逼近能力。最近的研究表明，HDNN因设计而具有非消失梯度，在训练过程中提供数值稳定性。然而，尽管在几个应用中HDNN已经展示了最先进的性能，但缺少量化其表现力的全面研究。因此，我们提供了一个HDNN的通用逼近定理，并证明了HDNN的一部分流可以逐渐逼近紧致域上的任何连续函数。此结果为实际使用HDNN提供了牢固的理论基础。

    This paper investigates the universal approximation capabilities of Hamiltonian Deep Neural Networks (HDNNs) that arise from the discretization of Hamiltonian Neural Ordinary Differential Equations. Recently, it has been shown that HDNNs enjoy, by design, non-vanishing gradients, which provide numerical stability during training. However, although HDNNs have demonstrated state-of-the-art performance in several applications, a comprehensive study to quantify their expressivity is missing. In this regard, we provide a universal approximation theorem for HDNNs and prove that a portion of the flow of HDNNs can approximate arbitrary well any continuous function over a compact domain. This result provides a solid theoretical foundation for the practical use of HDNNs.
    
[^209]: LNO: 用于解微分方程的拉普拉斯神经算子

    LNO: Laplace Neural Operator for Solving Differential Equations. (arXiv:2303.10528v1 [cs.LG])

    [http://arxiv.org/abs/2303.10528](http://arxiv.org/abs/2303.10528)

    LNO是一种用于解微分方程的算法，相比其他算法（如FNO）具有更好的逼近精度并适用于非周期性信号和瞬态响应，能够更好地解释模型并改进泛化能力。

    

    我们引入了拉普拉斯神经算子（LNO），利用拉普拉斯变换对输入空间进行分解。与傅里叶神经算子（FNO）不同，LNO可以处理非周期性信号，考虑瞬态响应，并呈指数收敛。LNO结合了输入和输出空间之间的极点-残差关系，具有更强的可解释性和改进的泛化能力。我们展示了单个拉普拉斯层在逼近三个ODE（Duffing振子、驱动引力摆和Lorenz系统）和三个PDE（Euler-Bernoulli梁、扩散方程和反应扩散系统）的解时，比FNO中的四个傅里叶模块具有更好的逼近精度。值得注意的是，在未阻尼情况下，LNO在捕捉瞬态响应方面优于FNO。对于线性欧拉-伯努利梁和扩散方程， LNO对极点-残差公式的精确表示比FNO产生了显着更好的结果。

    We introduce the Laplace neural operator (LNO), which leverages the Laplace transform to decompose the input space. Unlike the Fourier Neural Operator (FNO), LNO can handle non-periodic signals, account for transient responses, and exhibit exponential convergence. LNO incorporates the pole-residue relationship between the input and the output space, enabling greater interpretability and improved generalization ability. Herein, we demonstrate the superior approximation accuracy of a single Laplace layer in LNO over four Fourier modules in FNO in approximating the solutions of three ODEs (Duffing oscillator, driven gravity pendulum, and Lorenz system) and three PDEs (Euler-Bernoulli beam, diffusion equation, and reaction-diffusion system). Notably, LNO outperforms FNO in capturing transient responses in undamped scenarios. For the linear Euler-Bernoulli beam and diffusion equation, LNO's exact representation of the pole-residue formulation yields significantly better results than FNO. Fo
    
[^210]: CB2：合作自然语言交互研究平台

    CB2: Collaborative Natural Language Interaction Research Platform. (arXiv:2303.08127v1 [cs.LG])

    [http://arxiv.org/abs/2303.08127](http://arxiv.org/abs/2303.08127)

    CB2是一个用于研究基于任务的合作自然语言交互的平台，在3D游戏环境中提供了后端服务器和各种工具和流程。它在可扩展的研究中展示了学习的指令跟随模型。

    

    CB2 是一个多智能体平台，用于研究基于任务的情境下的合作自然语言交互。它包括一个 3D 游戏环境、一个后端服务器，可为人类智能体提供训练模型，以及各种工具和流程，以实现可扩展性的研究。我们在 https://cb2.ai 上展示了一个具有学习指令跟随模型的系统演示。

    CB2 is a multi-agent platform to study collaborative natural language interaction in a grounded task-oriented scenario. It includes a 3D game environment, a backend server designed to serve trained models to human agents, and various tools and processes to enable scalable studies. We deploy CB2 at https://cb2.ai as a system demonstration with a learned instruction following model.
    
[^211]: 基于Transformer的符号回归规划

    Transformer-based Planning for Symbolic Regression. (arXiv:2303.06833v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06833](http://arxiv.org/abs/2303.06833)

    该论文提出了一种基于Transformer和蒙特卡罗树搜索的符号回归规划策略TPSR，可以将非可微的反馈作为知识的外部来源融入到方程生成过程中，提高方程生成的准确性和复杂性。

    

    符号回归是机器学习中一项具有挑战性的任务，它涉及基于函数值查找其数学表达式。最近，符号回归的一些进展表明，预训练的基于Transformer的模型对于生成方程序列是有效的，这些模型从合成数据集的大规模预训练中获益，并在推理时间方面比基于GP的方法具有显著优势。然而，这些模型关注的是借鉴文本生成的监督预训练目标，而忽略了方程的特定目标，如准确性和复杂性。为了解决这个问题，我们提出了TPSR，一种基于Transformer的符号回归规划策略，将蒙特卡罗树搜索融入到Transformer解码过程中。与传统的解码策略不同，TPSR允许将非可微的反馈（如拟合准确性和复杂性）作为知识的外部来源融入到方程生成过程中。

    Symbolic regression (SR) is a challenging task in machine learning that involves finding a mathematical expression for a function based on its values. Recent advancements in SR have demonstrated the efficacy of pretrained transformer-based models for generating equations as sequences, which benefit from large-scale pretraining on synthetic datasets and offer considerable advantages over GP-based methods in terms of inference time. However, these models focus on supervised pretraining goals borrowed from text generation and ignore equation-specific objectives like accuracy and complexity. To address this, we propose TPSR, a Transformer-based Planning strategy for Symbolic Regression that incorporates Monte Carlo Tree Search into the transformer decoding process. TPSR, as opposed to conventional decoding strategies, allows for the integration of non-differentiable feedback, such as fitting accuracy and complexity, as external sources of knowledge into the equation generation process. Ext
    
[^212]: 多模态扩散中的一个Transformer适应所有分布

    One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale. (arXiv:2303.06555v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06555](http://arxiv.org/abs/2303.06555)

    本论文提出了UniDiffuser框架，采用一个Transformer模型来统一处理多模态数据的分布拟合问题，覆盖了边缘、条件和联合分布，并且能够在图文配对数据上实现多种生成任务。

    

    本文提出了一个统一的扩散框架(UniDiffuser)，用于在一个模型中拟合与一组多模态数据相关的所有分布。我们的关键见解是：针对边缘、条件和联合分布学习扩散模型可以被统一为预测扰动数据中的噪声，其中对不同模态的扰动级别（即时间步长）可能是不同的。在统一观点的启发下，UniDiffuser使用了最小的修改来同时学习所有分布，即对所有模态扰动数据，输入不同模态中的单独时间步长，并预测所有模态的噪声而不是单一模态的噪声。UniDiffuser是使用Transformer参数化的扩散模型，以处理不同模态的输入类型。在大规模的图文配对数据上实现，UniDiffuser能够执行图像、文本、文本到图像、图像到文本、图像文本对的生成。

    This paper proposes a unified diffusion framework (dubbed UniDiffuser) to fit all distributions relevant to a set of multi-modal data in one model. Our key insight is -- learning diffusion models for marginal, conditional, and joint distributions can be unified as predicting the noise in the perturbed data, where the perturbation levels (i.e. timesteps) can be different for different modalities. Inspired by the unified view, UniDiffuser learns all distributions simultaneously with a minimal modification to the original diffusion model -perturbs data in all modalities instead of a single modality, inputs individual timesteps in different modalities, and predicts the noise of all modalities instead of a single modality. UniDiffuser is parameterized by a transformer for diffusion models to handle input types of different modalities. Implemented on large-scale paired image-text data, UniDiffuser is able to perform image, text, text-to-image, image-to-text, and image-text pair generation 
    
[^213]: 减少扩散型语音增强中随机微分方程先验不匹配问题

    Reducing the Prior Mismatch of Stochastic Differential Equations for Diffusion-based Speech Enhancement. (arXiv:2302.14748v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2302.14748](http://arxiv.org/abs/2302.14748)

    研究提出了一种基于布朗桥的前向过程，以减少扩散型语音增强中由重建过程和先验之间的不匹配问题引起的误差，并提出的方法只需一半的迭代次数和一个可调的超参数即可改进处理过程。

    

    最近，基于评分的生成模型被成功地用于语音增强任务。一种随机微分方程用于建模迭代向前过程，在每个步骤中，环境噪声和白噪声被添加到干净的语音信号中。虽然在极限情况下，向前过程的均值会结束于嘈杂混合信号，但在实践中，它会提前结束并以嘈杂混合信号的近似值结束。这导致向前过程的终止分布与用于推理反向过程的先验之间存在差异。本文解决了这个问题，并提出了一种基于布朗桥的前向过程。我们证明了这样的过程与以前的扩散过程相比，可以减少不匹配。更重要的是，我们表明，与基准处理过程相比，我们的方法在客观指标上提供了改进，仅使用一半的迭代步骤并具有一个可调的超参数。

    Recently, score-based generative models have been successfully employed for the task of speech enhancement. A stochastic differential equation is used to model the iterative forward process, where at each step environmental noise and white Gaussian noise are added to the clean speech signal. While in limit the mean of the forward process ends at the noisy mixture, in practice it stops earlier and thus only at an approximation of the noisy mixture. This results in a discrepancy between the terminating distribution of the forward process and the prior used for solving the reverse process at inference. In this paper, we address this discrepancy and propose a forward process based on a Brownian bridge. We show that such a process leads to a reduction of the mismatch compared to previous diffusion processes. More importantly, we show that our approach improves in objective metrics over the baseline process with only half of the iteration steps and having one hyperparameter less to tune.
    
[^214]: 数据中心人工智能：通过离散子集作为连续嵌入空间优化实现深度生成可微分特征选择

    Data-Centric AI: Deep Generative Differentiable Feature Selection via Discrete Subsetting as Continuous Embedding Space Optimization. (arXiv:2302.13221v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13221](http://arxiv.org/abs/2302.13221)

    该论文提出一种将离散特征子集作为连续嵌入空间优化的深度生成可微分特征选择方法，解决了在高维小样本数据集中通用、准确和维度无关的特征选择问题。

    

    特征选择（FS）旨在为给定的下游任务找到最佳特征子集，例如过滤器、包装器和嵌入式方法。但在许多实际应用中，FS的标准在不同领域中变化，并且当数据是高维和小样本时，FS容易出现问题。选择的特征子集是否可以更通用、准确和维度无关？我们将这个问题泛化为一个深度可微分特征选择任务，并提出了一个新的视角：将离散特征子集作为连续嵌入空间优化。我们开发了一个通用和原则性的框架，包括深度特征子集编码器、准确性评估器、解码器和梯度上升优化器。这个框架实现了四个步骤：1) 特征-准确性训练数据准备；2) 深度特征子集嵌入；3) 梯度优化搜索；4) 特征子集重建。我们提出了新的技术洞见：将强化作为训练数据生成器、多样化的集成模型视为搜索加速器、多尺度的特征选择和逐渐增强的探索

    Feature Selection (FS), such as filter, wrapper, and embedded methods, aims to find the optimal feature subset for a given downstream task. However, in many real-world practices, 1) the criteria of FS vary across domains; 2) FS is brittle when data is a high-dimensional and small sample size. Can selected feature subsets be more generalized, accurate, and input dimensionality agnostic? We generalize this problem into a deep differentiable feature selection task and propose a new perspective: discrete feature subsetting as continuous embedding space optimization. We develop a generic and principled framework including a deep feature subset encoder, accuracy evaluator, decoder, and gradient ascent optimizer. This framework implements four steps: 1) features-accuracy training data preparation; 2) deep feature subset embedding; 3) gradient-optimized search; 4) feature subset reconstruction. We develop new technical insights: reinforcement as a training data generator, ensembles of diverse 
    
[^215]: 目标网络如何稳定时间差分方法

    Why Target Networks Stabilise Temporal Difference Methods. (arXiv:2302.12537v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12537](http://arxiv.org/abs/2302.12537)

    本文解释了深度强化学习中一种流行的时序差分方法中关键的稳定性问题：为什么目标网络能够有效降低不满足条件时的影响。

    

    深度强化学习中近期成功的关键在于一类使用不频繁更新目标值进行策略评估的时序差分方法。然而，有关目标网络有效性的完整理论解释仍然难以捉摸。本文针对这种流行算法进行了分析，最终回答了“为什么目标网络可以稳定时间差分学习”的问题。我们规范化了部分拟合的策略评估方法的概念，其中包括目标网络的使用，并且填补了拟合方法和半梯度时序差分算法之间的差距。利用这个框架，我们能够独特地描述所谓的致命三元组，即使用时序差分更新，结合（非线性）函数逼近和处于离线状态的数据，这经常会导致不收敛的算法。这一认识使我们得出结论：目标网络的使用可以减轻条件差时的影响。

    Integral to recent successes in deep reinforcement learning has been a class of temporal difference methods that use infrequently updated target values for policy evaluation in a Markov Decision Process. Yet a complete theoretical explanation for the effectiveness of target networks remains elusive. In this work, we provide an analysis of this popular class of algorithms, to finally answer the question: `why do target networks stabilise TD learning'? To do so, we formalise the notion of a partially fitted policy evaluation method, which describes the use of target networks and bridges the gap between fitted methods and semigradient temporal difference algorithms. Using this framework we are able to uniquely characterise the so-called deadly triad - the use of TD updates with (nonlinear) function approximation and off-policy data - which often leads to nonconvergent algorithms. This insight leads us to conclude that the use of target networks can mitigate the effects of poor conditionin
    
[^216]: 神经网络函数的Lipschitz连续性的一些基本方面

    Some Fundamental Aspects about Lipschitz Continuity of Neural Network Functions. (arXiv:2302.10886v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10886](http://arxiv.org/abs/2302.10886)

    本文深入研究和描述神经网络实现的函数的Lipschitz行为，在多种设置下进行实证研究，并揭示了神经网络函数Lipschitz连续性的基本和有趣的特性，其中最引人注目的是在Lipschitz常数的上限和下限中识别出了明显的双下降趋势。

    

    Lipschitz连续性是任何预测模型的一个简单但关键的功能性质，它处于模型的稳健性、泛化性和对抗性脆弱性的核心。本文旨在深入研究和描述神经网络实现的函数的Lipschitz行为。因此，我们通过耗尽最简单和最一般的下限和上限的极限，在各种不同设置下进行实证研究（即，体系结构、损失、优化器、标签噪音等），虽然这一选择主要是受计算难度结果的驱动，但它也非常丰富，并揭示了神经网络函数Lipschitz连续性的几个基本和有趣的特性，我们还补充了适当的理论论证。

    Lipschitz continuity is a simple yet crucial functional property of any predictive model for it lies at the core of the model's robustness, generalisation, as well as adversarial vulnerability. Our aim is to thoroughly investigate and characterise the Lipschitz behaviour of the functions realised by neural networks. Thus, we carry out an empirical investigation in a range of different settings (namely, architectures, losses, optimisers, label noise, and more) by exhausting the limits of the simplest and the most general lower and upper bounds. Although motivated primarily by computational hardness results, this choice nevertheless turns out to be rather resourceful and sheds light on several fundamental and intriguing traits of the Lipschitz continuity of neural network functions, which we also supplement with suitable theoretical arguments. As a highlight of this investigation, we identify a striking double descent trend in both upper and lower bounds to the Lipschitz constant with in
    
[^217]: 多视角互信息聚类方法

    Multi-View Clustering from the Perspective of Mutual Information. (arXiv:2302.08743v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08743](http://arxiv.org/abs/2302.08743)

    本文提出了一种信息性多视角聚类（IMVC）方法，通过约束共同表示和视图特定表示之间的互信息最小，提高多视角数据聚类效果。

    

    探究多视角数据之间的互补信息以提高聚类效果是多视角聚类中的关键问题。本文提出了一种基于信息理论的新模型——信息性多视角聚类（IMVC），它提取多视角数据中隐藏的共同信息和特定视角信息，并构建面向聚类的综合表示。具体来说，我们将多个特征连接成一个统一特征表示，然后通过编码器将其传递以检索跨视图的共同表示。同时，每个视图的特征被发送到编码器以产生紧凑的视图特定表示。因此，我们约束共同表示和视图特定表示之间的互信息最小，以获得多层次信息。进一步地，我们将共同表示和视图特定表示拼接起来，建模每个视图的精细表示。

    Exploring the complementary information of multi-view data to improve clustering effects is a crucial issue in multi-view clustering. In this paper, we propose a novel model based on information theory termed Informative Multi-View Clustering (IMVC), which extracts the common and view-specific information hidden in multi-view data and constructs a clustering-oriented comprehensive representation. More specifically, we concatenate multiple features into a unified feature representation, then pass it through a encoder to retrieve the common representation across views. Simultaneously, the features of each view are sent to a encoder to produce a compact view-specific representation, respectively. Thus, we constrain the mutual information between the common representation and view-specific representations to be minimal for obtaining multi-level information. Further, the common representation and view-specific representation are spliced to model the refined representation of each view, whic
    
[^218]: 零样本批次级异常检测

    Zero-Shot Batch-Level Anomaly Detection. (arXiv:2302.07849v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07849](http://arxiv.org/abs/2302.07849)

    本文提出了一种名为“自适应中心表示”的方法，用于零样本批次级异常检测。该方法利用批量归一化来训练现成的深度异常检测器，可以自动零样本泛化为未见过的AD任务。在实验中，该方法显示出了在多种数据集上的优秀表现，对表格数据进行了零样本AD。

    

    异常检测（AD）在许多安全关键的应用领域中发挥着关键作用。适应正常数据分布漂移的异常检测器调整，特别是当没有针对“新正常”进行训练的数据时，这一挑战导致产生了零样本AD技术。在本文中，我们提出了一种名为自适应中心表示（ACR）的简单而有效的方法，用于零样本批次级AD。我们的方法使用批量归一化来训练现成的深度异常检测器（例如深度SVDD）来适应一组相互关联的训练数据分布，使其能够自动零样本泛化为未见过的AD任务。这个简单的方法，批量归一化加元训练，是一种非常有效和多功能的工具。我们的结果展示了对表格数据的第一个零样本AD结果，并在来自专业领域的图像数据的零样本异常检测和分段方面优于现有方法。

    Anomaly detection (AD) plays a crucial role in many safety-critical application domains. The challenge of adapting an anomaly detector to drift in the normal data distribution, especially when no training data is available for the "new normal," has led to the development of zero-shot AD techniques. In this paper, we propose a simple yet effective method called Adaptive Centered Representations (ACR) for zero-shot batch-level AD. Our approach trains off-the-shelf deep anomaly detectors (such as deep SVDD) to adapt to a set of inter-related training data distributions in combination with batch normalization, enabling automatic zero-shot generalization for unseen AD tasks. This simple recipe, batch normalization plus meta-training, is a highly effective and versatile tool. Our results demonstrate the first zero-shot AD results for tabular data and outperform existing methods in zero-shot anomaly detection and segmentation on image data from specialized domains.
    
[^219]: 线性赌臂算法中的多任务表示学习

    Multi-task Representation Learning for Pure Exploration in Linear Bandits. (arXiv:2302.04441v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04441](http://arxiv.org/abs/2302.04441)

    本文研究了纯探索设置下的多任务表示学习，通过学习所有任务之间的公共低维线性表示，设计了计算和样本高效的算法来加速最佳臂或策略识别过程，在合成和现实数据集上实验结果证明了算法的有效性。

    

    尽管序列决策中表示学习的最近成功, 但在纯探索场景 (即, 找到最佳选项和最小化样本复杂度) 的研究仍然有限. 在本文中, 我们研究了纯探索设置中的多任务表示学习, 包括线性赌臂中的最佳臂识别 (RepBAI-LB) 和上下文线性赌臂中的最佳策略识别 (RepBPI-CLB). 在这两个问题中, 所有任务共享相同的低维线性表示, 我们的目标是利用这个特性加速所有任务的最佳臂 (策略) 识别过程. 我们设计了计算和样本高效的算法 DouExpDes and C-DouExpDes 来解决这些问题, 它们执行双重实验设计来规划学习全局表示的最佳样本分配。我们表明，通过学习各个任务之间的公共表示，我们的算法可以实现与现有最先进算法相比样本复杂度的显著改进。在合成和现实数据集上的实验结果证明了我们提出方法的有效性。

    Despite the recent success of representation learning in sequential decision making, the study of the pure exploration scenario (i.e., identify the best option and minimize the sample complexity) is still limited. In this paper, we study multi-task representation learning for best arm identification in linear bandits (RepBAI-LB) and best policy identification in contextual linear bandits (RepBPI-CLB), two popular pure exploration settings with wide applications, e.g., clinical trials and web content optimization. In these two problems, all tasks share a common low-dimensional linear representation, and our goal is to leverage this feature to accelerate the best arm (policy) identification process for all tasks. For these problems, we design computationally and sample efficient algorithms DouExpDes and C-DouExpDes, which perform double experimental designs to plan optimal sample allocations for learning the global representation. We show that by learning the common representation among 
    
[^220]: 使用样本评估生成模型的泛化能力

    Feature Likelihood Score: Evaluating Generalization of Generative Models Using Samples. (arXiv:2302.04440v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04440](http://arxiv.org/abs/2302.04440)

    本文提出了一种新的特征似然分数（FLS）评估深度生成模型泛化能力的度量指标，用于评估生成样本的新颖性、保真度和多样性，通过实验证明其优于现有的指标，并能够检测出过拟合问题。

    

    过去几年，深度生成模型的发展取得了令人印象深刻的进展，能够生成高维、复杂和照片般逼真的数据。然而，目前评估这些模型的方法仍然不完全：标准的基于似然的指标并不总是适用于这些模型，也很少与感知保真度相关，而基于样本的指标（如FID）对过拟合不敏感。为了解决这些局限性，我们提出了一种新的度量指标，称为特征似然分数（FLS），它是一个参数化的基于样本的分数，使用密度估计来提供全面的三相评估，考虑生成样本的新颖性（即与训练样本不同）、保真度和多样性。我们通过实验证明了FLS在检测过拟合问题上的能力，先前提出的度量指标无法解决这些问题。我们还对各种图像数据集和模型类别进行了广泛的FLS评估。

    The past few years have seen impressive progress in the development of deep generative models capable of producing high-dimensional, complex, and photo-realistic data. However, current methods for evaluating such models remain incomplete: standard likelihood-based metrics do not always apply and rarely correlate with perceptual fidelity, while sample-based metrics, such as FID, are insensitive to overfitting, i.e., inability to generalize beyond the training set. To address these limitations, we propose a new metric called the Feature Likelihood Score (FLS), a parametric sample-based score that uses density estimation to provide a comprehensive trichotomic evaluation accounting for novelty (i.e., different from the training samples), fidelity, and diversity of generated samples. We empirically demonstrate the ability of FLS to identify specific overfitting problem cases, where previously proposed metrics fail. We also extensively evaluate FLS on various image datasets and model classes
    
[^221]: 信息理论重要性采样聚类

    Information Theoretical Importance Sampling Clustering. (arXiv:2302.04421v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.04421](http://arxiv.org/abs/2302.04421)

    本文提出了一种信息理论重要性采样聚类方法，该方法最小化在分布偏差约束下期望失真的最坏情况。

    

    大多数聚类方法假设训练数据和未来数据来自相同的分布，但在大多数真实场景下，这种假设可能不成立。本文提出了一种基于信息理论重要性采样的聚类方法（ITISC），其在分布偏差的约束下最小化期望失真的最坏情况。分布偏差约束可以转换为以基于重要性采样的均匀分布为中心的一组权重分布的约束。所提出方法的目标是在最大降级下最小化损失，因此得到的问题是一个带有约束的最小最大化优化问题，可以使用拉格朗日方法将其重新表达为无约束问题。该优化问题可以通过交替优化算法或使用商业可用软件的通用优化例程来解决。

    A current assumption of most clustering methods is that the training data and future data are taken from the same distribution. However, this assumption may not hold in most real-world scenarios. In this paper, we propose an information theoretical importance sampling based approach for clustering problems (ITISC) which minimizes the worst case of expected distortions under the constraint of distribution deviation. The distribution deviation constraint can be converted to the constraint over a set of weight distributions centered on the uniform distribution derived from importance sampling. The objective of the proposed approach is to minimize the loss under maximum degradation hence the resulting problem is a constrained minimax optimization problem which can be reformulated to an unconstrained problem using the Lagrange method. The optimization problem can be solved by both an alternative optimization algorithm or a general optimization routine by commercially available software. Exp
    
[^222]: 基于信息瓶颈的神经函数回归和神经操作器学习的不确定性量化

    IB-UQ: Information bottleneck based uncertainty quantification for neural function regression and neural operator learning. (arXiv:2302.03271v2 [math.NA] UPDATED)

    [http://arxiv.org/abs/2302.03271](http://arxiv.org/abs/2302.03271)

    我们提出了基于信息瓶颈的不确定性量化框架IB-UQ，用于深度学习任务，在回归和操作器学习中提供了一种更有效和可扩展的量化预测不确定性的方法。

    

    我们提出了一种新的不确定性量化框架IB-UQ，用于科学机器学习任务，包括深度神经网络（DNN）回归和神经操作器学习（DeepONet）。特别地，我们通过自信感知的编码器加入了瓶颈，该编码器根据输入数据属于训练数据所在区域的可信度将输入编码为潜在表示，并利用高斯解码器条件地预测输出的均值和方差。此外，我们提出了一种基于数据增强的信息瓶颈目标，可以增强外推不确定性的量化质量，并且编码器和解码器都可以通过最小化该目标的可计算变分下界来进行训练。与依赖于哈密尔顿蒙特卡罗后验估计的贝叶斯神经网络的不确定性量化（UQ）方法相比，我们提出的IB-UQ框架提供了一种更有效和可扩展的方法，用于量化具有可比较准确度的预测不确定性。

    We propose a novel framework for uncertainty quantification via information bottleneck (IB-UQ) for scientific machine learning tasks, including deep neural network (DNN) regression and neural operator learning (DeepONet). Specifically, we incorporate the bottleneck by a confidence-aware encoder, which encodes inputs into latent representations according to the confidence of the input data belonging to the region where training data is located, and utilize a Gaussian decoder to predict means and variances of outputs conditional on representation variables. Furthermore, we propose a data augmentation based information bottleneck objective which can enhance the quantification quality of the extrapolation uncertainty, and the encoder and decoder can be both trained by minimizing a tractable variational bound of the objective. In comparison to uncertainty quantification (UQ) methods for scientific learning tasks that rely on Bayesian neural networks with Hamiltonian Monte Carlo posterior es
    
[^223]: V1T：使用Vision Transformer进行大规模小鼠V1响应预测

    V1T: large-scale mouse V1 response prediction using a Vision Transformer. (arXiv:2302.03023v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.03023](http://arxiv.org/abs/2302.03023)

    V1T是一种基于Vision Transformer的新型架构，可以跨动物学习共享的视觉和行为表示，对自然视觉刺激下的视觉皮层神经响应进行预测，并在预测性能上优于之前基于卷积的模型超过12.7％。同时，通过Transformer学习的自我关注权重还能够展示与群体感受野的相关性。

    

    在计算神经科学中，对自然视觉刺激下的视觉皮层神经响应的精确预测模型仍然是一个挑战。本文介绍了V1T，一种基于Vision Transformer的新型架构，可以跨动物学习共享的视觉和行为表示。我们对记录于小鼠原始视觉皮层的两个大型数据集进行评估，并在预测性能上优于之前基于卷积的模型超过12.7％。此外，我们展示了Transformer学习的自我关注权重与群体感受野相关。因此，我们的模型为神经响应预测设立了新的基准，并可与行为和神经记录一起使用，以揭示视觉皮层的有意义的特征。

    Accurate predictive models of the visual cortex neural response to natural visual stimuli remain a challenge in computational neuroscience. In this work, we introduce V1T, a novel Vision Transformer based architecture that learns a shared visual and behavioral representation across animals. We evaluate our model on two large datasets recorded from mouse primary visual cortex and outperform previous convolution-based models by more than 12.7% in prediction performance. Moreover, we show that the self-attention weights learned by the Transformer correlate with the population receptive fields. Our model thus sets a new benchmark for neural response prediction and can be used jointly with behavioral and neural recordings to reveal meaningful characteristic features of the visual cortex.
    
[^224]: 多源扩散模型用于同时进行音乐生成和分离。

    Multi-Source Diffusion Models for Simultaneous Music Generation and Separation. (arXiv:2302.02257v3 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2302.02257](http://arxiv.org/abs/2302.02257)

    本研究提出了一种基于扩散模型的生成模型，能够同时进行音乐合成和源分离，并于部分生成和分离任务上实现有竞争力的定量结果。

    

    在本研究中，我们定义了一个扩散基于的生成模型，可以通过学习共享上下文源的联合概率密度的得分来进行音乐合成和源分离。除了经典的总推理任务（即生成混合物体，分离源），我们还引入并在源填充的部分生成任务上进行实验，在这个任务中，我们生成一些源给其他人（例如，演奏一条与鼓相配的钢琴曲）。此外，我们还提出了一种基于Dirac似然函数的分离任务的新推理方法。我们在Slakh2100这个标准的音乐源分离数据集上训练我们的模型，在生成设置中提供定性结果，并展示在源分离设置中的有竞争力的定量结果。我们的方法是第一个能够处理生成和分离任务的单一模型的例子，因此代表了通用音频模型的一步。

    In this work, we define a diffusion-based generative model capable of both music synthesis and source separation by learning the score of the joint probability density of sources sharing a context. Alongside the classic total inference tasks (i.e., generating a mixture, separating the sources), we also introduce and experiment on the partial generation task of source imputation, where we generate a subset of the sources given the others (e.g., play a piano track that goes well with the drums). Additionally, we introduce a novel inference method for the separation task based on Dirac likelihood functions. We train our model on Slakh2100, a standard dataset for musical source separation, provide qualitative results in the generation settings, and showcase competitive quantitative results in the source separation setting. Our method is the first example of a single model that can handle both generation and separation tasks, thus representing a step toward general audio models.
    
[^225]: Koopman算子学习的尖锐谱率

    Sharp Spectral Rates for Koopman Operator Learning. (arXiv:2302.02004v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02004](http://arxiv.org/abs/2302.02004)

    本文提出了Koopman算子的非渐进学习界限，重点研究了时间可逆随机动力系统，提出了扩展动态模分解（EDMD）和降低秩回归（RRR）两种流行的估计器，并比较了它们的方差。

    

    非线性动力系统可以方便地描述为相关的Koopman算子，其作用使系统的每个可观测量随时间向前演化。通过许多算法可以从数据中学习Koopman算子及其谱分解。在本文中，我们首次提出了Koopman特征值和特征函数的非渐进学习界限。我们重点研究了时间可逆随机动力系统，包括重要的Langevin动力学示例。我们分析了两种流行的估计器：扩展动态模分解（EDMD）和降低秩回归（RRR）。我们的结果关键依赖于最小二乘估计误差的新奇极小化界限，这可能是独立工作的重要内容。我们的谱学习边界受到算子范数误差和估计特征函数的新型度量失真函数的同时控制驱动。边界表明，EDMD和RRR的方差相似，但EDMD优于...

    Non-linear dynamical systems can be handily described by the associated Koopman operator, whose action evolves every observable of the system forward in time. Learning the Koopman operator and its spectral decomposition from data is enabled by a number of algorithms. In this work we present for the first time non-asymptotic learning bounds for the Koopman eigenvalues and eigenfunctions. We focus on time-reversal-invariant stochastic dynamical systems, including the important example of Langevin dynamics. We analyze two popular estimators: Extended Dynamic Mode Decomposition (EDMD) and Reduced Rank Regression (RRR). Our results critically hinge on novel minimax estimation bounds for the operator norm error, that may be of independent interest. Our spectral learning bounds are driven by the simultaneous control of the operator norm error and a novel metric distortion functional of the estimated eigenfunctions. The bounds indicates that both EDMD and RRR have similar variance, but EDMD su
    
[^226]: 扩散模型易受成员隐私攻击吗？

    Are Diffusion Models Vulnerable to Membership Inference Attacks?. (arXiv:2302.01316v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.01316](http://arxiv.org/abs/2302.01316)

    本文研究了扩散模型是否易受成员隐私攻击，提出了一种基于查询的成员隐私攻击方法。结果表明现有的成员隐私攻击对扩散模型基本无效。

    

    基于扩散的生成模型在图像合成方面表现出了巨大的潜力，但是它们可能带来安全和隐私风险的研究却很少。本文研究了扩散模型对成员隐私攻击的易受性，这是一种常见的隐私问题。我们的研究结果表明，现有的面向GANs或VAE的成员隐私攻击对于扩散模型来说基本无效，要么是因为适用场景不同（例如需要GANs的判别器），要么是因为假设不当（例如，合成样本和成员样本之间的距离更近）。为了填补这一空白，我们提出了一个基于查询的成员隐私攻击方法Step-wise Error Comparing Membership Inference（SecMI），它通过评估在每个时间步长中前向过程后验估计的匹配情况来推断成员身份。SecMI遵循MIA中的常见过拟合假设，即成员样本通常具有较小的估计误差，而比较保持样本。我们同时考虑了标准的扩散模型和切比雪夫扩展模型，并在多个数据集上进行了评估。

    Diffusion-based generative models have shown great potential for image synthesis, but there is a lack of research on the security and privacy risks they may pose. In this paper, we investigate the vulnerability of diffusion models to Membership Inference Attacks (MIAs), a common privacy concern. Our results indicate that existing MIAs designed for GANs or VAE are largely ineffective on diffusion models, either due to inapplicable scenarios (e.g., requiring the discriminator of GANs) or inappropriate assumptions (e.g., closer distances between synthetic samples and member samples). To address this gap, we propose Step-wise Error Comparing Membership Inference (SecMI), a query-based MIA that infers memberships by assessing the matching of forward process posterior estimation at each timestep. SecMI follows the common overfitting assumption in MIA where member samples normally have smaller estimation errors, compared with hold-out samples. We consider both the standard diffusion models, e
    
[^227]: 卷积神经算子用于偏微分方程的鲁棒准确学习

    Convolutional Neural Operators for robust and accurate learning of PDEs. (arXiv:2302.01178v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01178](http://arxiv.org/abs/2302.01178)

    该论文提出了卷积神经算子（CNO），它能够在学习偏微分方程的上下文中处理函数输入和输出，并且证明了 CNOs 可以近似偏微分方程中出现的算子到所需的精度。在测试中，CNOs 显着优于基线，这为鲁棒准确操作学习的另一种框架铺平了道路。

    

    尽管在传统机器学习中非常成功，基于卷积的神经网络架构在函数空间中存在不一致性，因此在学习偏微分方程的解算子的上下文中被广泛忽视。在这里，我们提出了卷积神经网络的新适应方法，以证明它们确实能够处理输入和输出的函数。结果产生的架构称为卷积神经算子（CNO），专门设计为在计算机上以离散形式实现时保持其基本的连续性质。我们证明了一个普适定理，以展示CNOs可以近似偏微分方程中出现的算子到所需的精度。 CNOs在一个包括各种具有可能具有多尺度解的偏微分方程的新套件上进行测试，并被观察到显着优于基线，为鲁棒准确操作学习的另一种框架铺平了道路。

    Although very successfully used in conventional machine learning, convolution based neural network architectures -- believed to be inconsistent in function space -- have been largely ignored in the context of learning solution operators of PDEs. Here, we present novel adaptations for convolutional neural networks to demonstrate that they are indeed able to process functions as inputs and outputs. The resulting architecture, termed as convolutional neural operators (CNOs), is designed specifically to preserve its underlying continuous nature, even when implemented in a discretized form on a computer. We prove a universality theorem to show that CNOs can approximate operators arising in PDEs to desired accuracy. CNOs are tested on a novel suite of benchmarks, encompassing a diverse set of PDEs with possibly multi-scale solutions and are observed to significantly outperform baselines, paving the way for an alternative framework for robust and accurate operator learning.
    
[^228]: 无需精确了解速度的快速在线调度：Speed-Oblivious Online Scheduling

    Speed-Oblivious Online Scheduling: Knowing (Precise) Speeds is not Necessary. (arXiv:2302.00985v2 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2302.00985](http://arxiv.org/abs/2302.00985)

    本论文提出了一种速度遗忘算法，用于在异构机器上进行在线调度，算法不需要准确的作业处理速度。作者提供了理论保证和实证评估。

    

    我们考虑在异构机器上进行在线调度，并采用速度遗忘的算法，其中算法不知道准确的作业处理速度。我们对预言算法和非预言算法进行了强大的不可能结果，并在受实际环境启发的模型中克服了这些问题：(i)我们提供了具有竞争学习性的算法，假设给出了（可能是错误的）速度预测；(ii)我们提供了适用于速度排序模型的有竞争力的算法，其中根据其未知的作业处理速度，已知一种单一全局排序算法。我们证明了强有力的理论保证，并对代表性的异构多核处理器进行了评估。这似乎是第一次在非合成硬件环境中评估具有预测的调度算法的实证结果。

    We consider online scheduling on unrelated (heterogeneous) machines in a speed-oblivious setting, where an algorithm is unaware of the exact job-dependent processing speeds. We show strong impossibility results for clairvoyant and non-clairvoyant algorithms and overcome them in models inspired by practical settings: (i) we provide competitive learning-augmented algorithms, assuming that (possibly erroneous) predictions on the speeds are given, and (ii) we provide competitive algorithms for the speed-ordered model, where a single global order of machines according to their unknown job-dependent speeds is known. We prove strong theoretical guarantees and evaluate our findings on a representative heterogeneous multi-core processor. These seem to be the first empirical results for scheduling algorithms with predictions that are evaluated in a non-synthetic hardware environment.
    
[^229]: CD-GraB：协调分布式示例顺序以证明加速训练

    CD-GraB: Coordinating Distributed Example Orders for Provably Accelerated Training. (arXiv:2302.00845v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00845](http://arxiv.org/abs/2302.00845)

    该论文提出了一种名为CD-GraB的算法，可以协调分布式示例顺序以加速机器学习训练。CD-GraB展现出线性加速收敛速率并且在基准任务上优于其他基线方法。

    

    最近有关在线梯度平衡（GraB）的研究表明，存在基于置换的示例排序可以保证优于随机重排（RR）。而RR会任意排列训练示例，GraB利用先前时期的陈旧梯度对示例进行排序--实现比RR更快的收敛速率。但是，GraB在设计上存在限制：虽然它展示了在集中数据上扩展训练的出色能力，但并不自然地扩展到现代分布式ML工作负载。因此，我们提出了协调分布式GraB（CD-GraB），它利用先前关于内核稀疏化工作的洞察力，将置换排序的可证明更快的优势转化为分布式设置。CD-GraB具有可忽略的开销，在中央集权GraB上具有线性加速收敛速率的性能，并在各种基准任务上经验性地优于分布式RR等基线方法。

    Recent research on online Gradient Balancing (GraB) has revealed that there exist permutation-based example orderings that are guaranteed to outperform random reshuffling (RR). Whereas RR arbitrarily permutes training examples, GraB leverages stale gradients from prior epochs to order examples -- achieving a provably faster convergence rate than RR. However, GraB is limited by design: While it demonstrates an impressive ability to scale-up training on centralized data, it does not naturally extend to modern distributed ML workloads. We therefore propose Coordinated Distributed GraB (CD-GraB), which uses insights from prior work on kernel thinning to translate the benefits of provably faster permutation-based example ordering to distributed settings. With negligible overhead, CD-GraB exhibits a linear speedup in convergence rate over centralized GraB and outperforms baselines empirically, including distributed RR, on a variety of benchmark tasks.
    
[^230]: 端到端全原子抗体设计

    End-to-End Full-Atom Antibody Design. (arXiv:2302.00203v3 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2302.00203](http://arxiv.org/abs/2302.00203)

    dyMEAN是一个端到端全原子模型，可以根据表位和不完整的抗体序列进行抗体设计，在处理全原子时能够处理可变大小的蛋白残基。旨在解决现有学习方法的两个主要问题：只处理抗体设计过程中的某个子任务和无法捕捉全原子几何形状。

    

    抗体设计是药物治疗和生物学等领域中一个重要且具有挑战性的任务。当前的基于学习的方法有两个主要缺陷：1）只处理整个抗体设计流程中的某个子任务，使其次优或资源密集。2）忽略框架区域或侧链中的任一部分，因此无法捕捉全原子几何形状。为解决这些问题，我们提出了动态多通道等变图神经网络（dyMEAN），这是一个端到端全原子模型，用于根据表位和不完整的抗体序列进行 E（3）等变抗体设计。具体来说，我们首先探究结构初始化作为抗体结构的有知识猜测，然后提出影响表位-抗体相互作用的“shadow paratope”。通过自适应多通道等变编码器来更新1D序列和3D结构，该编码器可在考虑全原子时处理可变大小的蛋白残基。最后，我们证明了dyMEAN在多个基准测试中的有效性，这为完全自动化的抗体设计提供了新的可能性。

    Antibody design is an essential yet challenging task in various domains like therapeutics and biology. There are two major defects in current learning-based methods: 1) tackling only a certain subtask of the whole antibody design pipeline, making them suboptimal or resource-intensive. 2) omitting either the framework regions or side chains, thus incapable of capturing the full-atom geometry. To address these pitfalls, we propose dynamic Multi-channel Equivariant grAph Network (dyMEAN), an end-to-end full-atom model for E(3)-equivariant antibody design given the epitope and the incomplete sequence of the antibody. Specifically, we first explore structural initialization as a knowledgeable guess of the antibody structure and then propose shadow paratope to bridge the epitope-antibody connections. Both 1D sequences and 3D structures are updated via an adaptive multi-channel equivariant encoder that is able to process protein residues of variable sizes when considering full atoms. Finally,
    
[^231]: 《揭示随机梯度下降噪声在深度学习不同模式下的作用》

    Dissecting the Effects of SGD Noise in Distinct Regimes of Deep Learning. (arXiv:2301.13703v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13703](http://arxiv.org/abs/2301.13703)

    本文通过改变训练集大小P和初始化规模a的方法来控制SGD噪声大小T对于MNIST和CIFAR10图像分类的影响，得到SGD噪声可能有害或有益的相图，同时发现特征温度Tc存在相似的尺度关系，在训练集为1000～10000之间时达到最佳水平。

    

    深度神经网络的随机梯度下降（SGD）噪声何时会影响到其泛化效果一直是一个挑战，因为网络可以在不同的训练模式下运行。本文研究了如何通过改变训练集大小P和初始化规模a的方法来控制SGD噪声大小T对性能的影响。对于MNIST和CIFAR10图像分类，我们的主要贡献有：（i）在（a，T）平面上得到性能的相图。它们表明，根据训练模式，SGD噪声可能有害或有益。此外，尽管增加T或减小α都能使网络跳出惰性模式，但这些变化对性能的影响可能是相反的。 （ii）最重要的是，我们发现特征温度Tc与训练集大小存在相似的尺度关系，在训练集为1000～10000之间（这是许多应用的实际训练集大小）时，Tc在深度网络中的性能达到最佳水平。

    Understanding when the noise in stochastic gradient descent (SGD) affects generalization of deep neural networks remains a challenge, complicated by the fact that networks can operate in distinct training regimes. Here we study how the magnitude of this noise $T$ affects performance as the size of the training set $P$ and the scale of initialization $\alpha$ are varied. For gradient descent, $\alpha$ is a key parameter that controls if the network is `lazy'($\alpha\gg1$) or instead learns features ($\alpha\ll1$). For classification of MNIST and CIFAR10 images, our central results are: (i) obtaining phase diagrams for performance in the $(\alpha,T)$ plane. They show that SGD noise can be detrimental or instead useful depending on the training regime. Moreover, although increasing $T$ or decreasing $\alpha$ both allow the net to escape the lazy regime, these changes can have opposite effects on performance. (ii) Most importantly, we find that the characteristic temperature $T_c$ where th
    
[^232]: PAC-Bayesian软演员-评论家学习

    PAC-Bayesian Soft Actor-Critic Learning. (arXiv:2301.12776v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12776](http://arxiv.org/abs/2301.12776)

    本文提出了一种使用PAC-Bayesian bound作为Soft Actor-Critic (SAC)算法评论家训练目标的方法，以解决训练不稳定的问题，并通过评论家引导的随机搜索探索多个未来来提高在线学习性能。在多个经典控制和运动任务中，该算法具有样本效率和遗憾最小化方面的明显优势。

    

    演员-评论家算法通过两个分别作策略评估和改进的功能逼近器来解决增强学习(RL)的双重目标。此方法的实用性是以训练不稳定为代价的，主要原因是评论家逼近误差对演员的破坏性影响。我们通过首次采用一个现有的可能近似正确(PAC)Bayesian界限作为Soft Actor-Critic (SAC)算法的评论家训练目标来解决这个瓶颈。此外，我们进一步证明了当随机演员通过评论家引导的随机搜索探索多个未来时，在线学习性能显著提高。我们观察到我们得到的算法在多个经典控制和运动任务中，在样本效率和遗憾最小化方面与现有技术相比具有明显优势。

    Actor-critic algorithms address the dual goals of reinforcement learning (RL), policy evaluation and improvement, via two separate function approximators. The practicality of this approach comes at the expense of training instability, caused mainly by the destructive effect of the approximation errors of the critic on the actor. We tackle this bottleneck by employing an existing Probably Approximately Correct (PAC) Bayesian bound for the first time as the critic training objective of the Soft Actor-Critic (SAC) algorithm. We further demonstrate that online learning performance improves significantly when a stochastic actor explores multiple futures by critic-guided random search. We observe our resulting algorithm to compare favorably to the state of the art on multiple classical control and locomotion tasks in terms of both sample efficiency and regret minimization.
    
[^233]: 深度算子学习减轻了PDE中维数灾难

    Deep Operator Learning Lessens the Curse of Dimensionality for PDEs. (arXiv:2301.12227v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12227](http://arxiv.org/abs/2301.12227)

    本文指出，利用DNN对Banach空间上的Lipschitz算子进行深度算子学习可以减轻PDE中的维数灾难，包括椭圆方程、抛物线方程和Burgers方程，并可用于提供算子学习中的离散化不变性的见解。

    

    深度神经网络在许多领域均已获得显著成功，并且它们在PDE相关问题中的应用正在迅速发展。本文利用DNNs估计了在Banach空间上学习Lipschitz算子的泛化误差，并应用于各种PDE解算子。我们的目标是指定DNN的宽度、深度和需要训练样本的数量，以保证一定的测试误差。在对数据分布或算子结构进行温和的假设下，我们的分析表明，深度算子学习可以放松对PDE离散化分辨率的依赖，从而减轻许多与PDE相关的问题中的维数灾难，包括椭圆方程、抛物线方程和Burgers方程。我们的结果也应用于提供算子学习中的离散化不变性的见解。

    Deep neural networks (DNNs) have achieved remarkable success in numerous domains, and their application to PDE-related problems has been rapidly advancing. This paper provides an estimate for the generalization error of learning Lipschitz operators over Banach spaces using DNNs with applications to various PDE solution operators. The goal is to specify DNN width, depth, and the number of training samples needed to guarantee a certain testing error. Under mild assumptions on data distributions or operator structures, our analysis shows that deep operator learning can have a relaxed dependence on the discretization resolution of PDEs and, hence, lessen the curse of dimensionality in many PDE-related problems including elliptic equations, parabolic equations, and Burgers equations. Our results are also applied to give insights about discretization-invariant in operator learning.
    
[^234]: ZegOT:使用文本提示的最优传输实现零样本分割。

    ZegOT: Zero-shot Segmentation Through Optimal Transport of Text Prompts. (arXiv:2301.12171v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.12171](http://arxiv.org/abs/2301.12171)

    这篇论文提出了一种通过最优传输的方法，利用多个文本提示来实现零样本分割，达到了最先进的性能水平。

    

    通过将图像和文本对齐的方法，利用大规模对比性语言-图像预训练（CLIP）的成功为零样本语义分割带来了很大的希望，然而现有的方法通常需要额外的图像编码器或对CLIP模块进行重新训练或微调。本论文提出了一种新的ZegOT方法，通过最优传输将多个文本提示与冻结的图像嵌入匹配，从而实现零样本分割。特别是，通过引入一种新的多提示最优传输求解器（MPOT），该方法为每个文本提示与冻结的图像编码器隐藏层的视觉特征映射之间学习了一种最优映射。这种独特的映射方法有效地使每个文本提示关注不同的视觉语义属性。通过在基准数据集上进行广泛的实验，我们展示了我们的方法优于现有方法，达到了最先进的性能水平。

    Recent success of large-scale Contrastive Language-Image Pre-training (CLIP) has led to great promise in zero-shot semantic segmentation by transferring image-text aligned knowledge to pixel-level classification. However, existing methods usually require an additional image encoder or retraining/tuning the CLIP module. Here, we propose a novel Zero-shot segmentation with Optimal Transport (ZegOT) method that matches multiple text prompts with frozen image embeddings through optimal transport. In particular, we introduce a novel Multiple Prompt Optimal Transport Solver (MPOT), which is designed to learn an optimal mapping between multiple text prompts and visual feature maps of the frozen image encoder hidden layers. This unique mapping method facilitates each of the multiple text prompts to effectively focus on distinct visual semantic attributes. Through extensive experiments on benchmark datasets, we show that our method achieves the state-of-the-art (SOTA) performance over existing 
    
[^235]: 基于硬币采样的无需学习速率的基于梯度的贝叶斯推断方法

    Coin Sampling: Gradient-Based Bayesian Inference without Learning Rates. (arXiv:2301.11294v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.11294](http://arxiv.org/abs/2301.11294)

    本文提出了一种基于硬币投注的贝叶斯推断方法，完全不需要学习速率，可以在高维模型和数据集上表现出与其他方法相当的性能。

    

    近年来，基于粒子的变分推断（ParVI）方法如Stein变分梯度下降（SVGD）因可扩展性在贝叶斯推理中越来越受欢迎。然而，这些方法的性质不可避免地取决于超参数（如学习速率），必须由从业者仔细调整，以确保以合适的速率收敛到目标测度。在本文中，我们引入了一组新的基于硬币投注的可扩展贝叶斯推断方法，这些方法完全不需要学习速率。我们在一系列数值例子中演示了我们方法的性能，包括几个高维模型和数据集，证明了与其他ParVI算法相当的性能，而无需调整学习速率。

    In recent years, particle-based variational inference (ParVI) methods such as Stein variational gradient descent (SVGD) have grown in popularity as scalable methods for Bayesian inference. Unfortunately, the properties of such methods invariably depend on hyperparameters such as the learning rate, which must be carefully tuned by the practitioner in order to ensure convergence to the target measure at a suitable rate. In this paper, we introduce a suite of new particle-based methods for scalable Bayesian inference based on coin betting, which are entirely learning-rate free. We illustrate the performance of our approach on a range of numerical examples, including several high-dimensional models and datasets, demonstrating comparable performance to other ParVI algorithms with no need to tune a learning rate.
    
[^236]: 关于具有期望条件风险度量的风险厌恶策略梯度方法的全局收敛性

    On the Global Convergence of Risk-Averse Policy Gradient Methods with Expected Conditional Risk Measures. (arXiv:2301.10932v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10932](http://arxiv.org/abs/2301.10932)

    本论文研究了具有期望条件风险度量的风险厌恶策略梯度方法，提出了策略梯度更新，证明了其在约束和无约束情况下的全局收敛性和迭代复杂度，并测试了REINFORCE和actor-critic算法的风险厌恶变体来展示方法的实用价值和风险控制的重要性。

    

    风险敏感的强化学习已经成为控制不确定结果和确保各种顺序决策问题的可靠性能的流行工具。虽然针对风险敏感的强化学习已经开发出了策略梯度方法，但这些方法是否具有与风险中性情况下相同的全局收敛保证还不清楚。本文考虑了一类动态时间一致风险度量，称为期望条件风险度量（ECRM），并为基于ECRM的目标函数推导出策略梯度更新。在约束直接参数化和无约束softmax参数化下，我们提供了相应的风险厌恶策略梯度算法的全局收敛性和迭代复杂度。我们进一步测试了REINFORCE和actor-critic算法的风险厌恶变体，以展示我们的方法的有效性和风险控制的重要性。

    Risk-sensitive reinforcement learning (RL) has become a popular tool to control the risk of uncertain outcomes and ensure reliable performance in various sequential decision-making problems. While policy gradient methods have been developed for risk-sensitive RL, it remains unclear if these methods enjoy the same global convergence guarantees as in the risk-neutral case. In this paper, we consider a class of dynamic time-consistent risk measures, called Expected Conditional Risk Measures (ECRMs), and derive policy gradient updates for ECRM-based objective functions. Under both constrained direct parameterization and unconstrained softmax parameterization, we provide global convergence and iteration complexities of the corresponding risk-averse policy gradient algorithms. We further test risk-averse variants of REINFORCE and actor-critic algorithms to demonstrate the efficacy of our method and the importance of risk control.
    
[^237]: 对Transformer编码器表达能力的更紧密界定

    Tighter Bounds on the Expressivity of Transformer Encoders. (arXiv:2301.10743v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10743](http://arxiv.org/abs/2301.10743)

    本文旨在更紧密地界定Transformer编码器的表达能力，提供了一个同时是下限和上限的一阶逻辑变体的策略，使我们更加接近准确刻画Transformer编码器可识别语言的目标。

    

    用更易理解的形式系统表征神经网络有潜力揭示这些网络的能力和局限性，然而对于Transformer来说这仍是一个活跃的研究领域。Bhattamishra等人已经表明，Transformer编码器至少与一种特定的计数机同等表达能力，而Merrill和Sabharwal则表明固定精度的Transformer编码器只能识别统一的$TC^0$语言。我们通过确认具有计数量词的一阶逻辑的变体，既是固定精度Transformer编码器的上界，也是Transformer编码器的下界，从而将这些结果联系起来并加以加强。这使我们比以前更接近准确刻画Transformer编码器可识别的语言的目标。

    Characterizing neural networks in terms of better-understood formal systems has the potential to yield new insights into the power and limitations of these networks. Doing so for transformers remains an active area of research. Bhattamishra and others have shown that transformer encoders are at least as expressive as a certain kind of counter machine, while Merrill and Sabharwal have shown that fixed-precision transformer encoders recognize only languages in uniform $TC^0$. We connect and strengthen these results by identifying a variant of first-order logic with counting quantifiers that is simultaneously an upper bound for fixed-precision transformer encoders and a lower bound for transformer encoders. This brings us much closer than before to an exact characterization of the languages that transformer encoders recognize.
    
[^238]: 利用生成对抗网络创建逼真的扫描透射电子显微镜图像

    Leveraging generative adversarial networks to create realistic scanning transmission electron microscopy images. (arXiv:2301.07743v2 [cond-mat.mtrl-sci] UPDATED)

    [http://arxiv.org/abs/2301.07743](http://arxiv.org/abs/2301.07743)

    本文提出一种循环生成对抗网络 (CycleGAN) 的方法，利用真实的空间频率信息来扩充模拟数据，生成几乎无法与真实数据区分的电子显微镜图像，并为机器学习应用提供标签，这展示了生成对抗网络在革命性转变材料研究方面具有潜力。

    

    自动化和机器学习在电子显微学中的应用具有通过自动数据收集和处理来革命性转变材料研究的潜力。如何开发出快速推广到不同实验条件下的大型数据集的机器学习模型是一个重大挑战。本文采用循环生成对抗网络 (CycleGAN) 和一个互补空间判别器，通过增加真实的空间频率信息来扩充模拟数据。这使得CycleGAN能够生成几乎无法与真实数据区分的图像，并为机器学习应用提供标签。我们通过训练完全卷积网络 (FCN) 在一个由450万个原子组成的数据集中识别单个原子缺陷来展示我们的方法。我们的方法能够产生适应性强的FCN，并能够在最小的干预下适应动态变化的实验变量，展示了生成对抗网络在利用高通量电子显微学革命材料研究方面的潜力。

    The rise of automation and machine learning (ML) in electron microscopy has the potential to revolutionize materials research through autonomous data collection and processing. A significant challenge lies in developing ML models that rapidly generalize to large data sets under varying experimental conditions. We address this by employing a cycle generative adversarial network (CycleGAN) with a reciprocal space discriminator, which augments simulated data with realistic spatial frequency information. This allows the CycleGAN to generate images nearly indistinguishable from real data and provide labels for ML applications. We showcase our approach by training a fully convolutional network (FCN) to identify single atom defects in a 4.5 million atom data set, collected using automated acquisition in an aberration-corrected scanning transmission electron microscope (STEM). Our method produces adaptable FCNs that can adjust to dynamically changing experimental variables with minimal interve
    
[^239]: 学习Boltzmann密度的变形轨迹

    Learning Deformation Trajectories of Boltzmann Densities. (arXiv:2301.07388v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.07388](http://arxiv.org/abs/2301.07388)

    本文介绍了一种学习Boltzmann密度变形轨迹的方法，其中通过插值能量函数等实现Boltzmann密度的变形，然后找到一个时间依赖向量场，将样本从一个分布转移到另一个分布，其表现在高斯混合和量子力学粒子的Boltzmann密度上比KL-反散度更具优势。

    

    我们提出了一种连续标准化流的训练方法，可以在没有样本但存在能量函数的情况下使用。我们的方法依赖于能量函数$f_1$和广义高斯函数$f_0$之间的预定或学习插值$f_t$。能量函数的插值引起Boltzmann密度$p_t\propto e^{-f_t}$的插值，我们旨在找到一个沿着族$p_t$的时间依赖向量场$V_t$，将样本从一个分布转移到另一个分布。将样本沿着族$p_t$从一个分布转移到另一个分布的条件可以转化为$V_t$和$f_t$之间的PDE，我们优化$V_t$和$f_t$以满足此PDE。我们在高斯混合和双井势的量子力学粒子的Boltzmann密度上实验比较了所提出的训练目标与KL-反散度的差异。

    We introduce a training objective for continuous normalizing flows that can be used in the absence of samples but in the presence of an energy function. Our method relies on either a prescribed or a learnt interpolation $f_t$ of energy functions between the target energy $f_1$ and the energy function of a generalized Gaussian $f_0(x) = ||x/\sigma||_p^p$. The interpolation of energy functions induces an interpolation of Boltzmann densities $p_t \propto e^{-f_t}$ and we aim to find a time-dependent vector field $V_t$ that transports samples along the family $p_t$ of densities. The condition of transporting samples along the family $p_t$ can be translated to a PDE between $V_t$ and $f_t$ and we optimize $V_t$ and $f_t$ to satisfy this PDE. We experimentally compare the proposed training objective to the reverse KL-divergence on Gaussian mixtures and on the Boltzmann density of a quantum mechanical particle in a double-well potential.
    
[^240]: 注意差距：建模被审查和未被审查的电动汽车充电需求的差异。

    Mind the Gap: Modelling Difference Between Censored and Uncensored Electric Vehicle Charging Demand. (arXiv:2301.06418v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.06418](http://arxiv.org/abs/2301.06418)

    电动汽车充电需求模型的天然偏见导致了观测到的需求与实际需求有差异，使用有关审查的模型来模拟充电需求可以更好地估计真实需求。

    

    基于充电记录的电动汽车充电需求模型会天然地对可用充电器的供应产生偏见。这些模型通常无法考虑到被占用充电站和竞争对充电需求的损失。这些损失表明实际需求很可能比充电记录反映的需求更高，即真实需求是潜在的（未观察到的），而观察结果则是被审查的。因此，依赖这些观察结果进行预测充电需求的机器学习模型可能在未来基础设施扩展和供应管理方面受到限制，因为它们无法估计充电的真实需求。我们提出使用有关审查的模型来模拟充电需求以解决此限制。这些模型将审查纳入损失函数中，并从观察到的充电记录中学习真实的潜在需求分布。我们研究了被占用充电站和竞争服务如何使用GPS轨迹审查需求。

    Electric vehicle charging demand models, with charging records as input, will inherently be biased toward the supply of available chargers. These models often fail to account for demand lost from occupied charging stations and competitors. The lost demand suggests that the actual demand is likely higher than the charging records reflect, i.e., the true demand is latent (unobserved), and the observations are censored. As a result, machine learning models that rely on these observed records for forecasting charging demand may be limited in their application in future infrastructure expansion and supply management, as they do not estimate the true demand for charging. We propose using censorship-aware models to model charging demand to address this limitation. These models incorporate censorship in their loss functions and learn the true latent demand distribution from observed charging records. We study how occupied charging stations and competing services censor demand using GPS traject
    
[^241]: 使用机器学习和图像处理技术进行头发和头皮病检测

    Hair and Scalp Disease Detection using Machine Learning and Image Processing. (arXiv:2301.00122v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.00122](http://arxiv.org/abs/2301.00122)

    本研究使用深度学习方法成功预测了三种主要的脱发和头皮相关疾病：斑秃、银屑病和毛囊炎。

    

    约有8000万美国人因衰老、压力、药物或基因缘由而患上脱发症。头发和头皮相关的疾病往往在早期时被忽视。有时，患者无法区分脱发和正常的掉发。诊断与头发有关的疾病很耗时，因为它需要专业的皮肤科医生进行视觉和医学测试。因此，总体诊断时间的延迟加剧了疾病的严重程度。由于具有图像处理能力，神经网络应用于各个领域，尤其是医疗保健和健康信息学，可以预测像癌症和肿瘤等致命疾病。这些应用程序协助临床医生和患者，提供早期症状的初步见解。在本研究中，我们使用深度学习方法成功预测了三种主要的脱发和头皮相关疾病：斑秃、银屑病和毛囊炎。然而，这方面的研究还很有限。

    Almost 80 million Americans suffer from hair loss due to aging, stress, medication, or genetic makeup. Hair and scalp-related diseases often go unnoticed in the beginning. Sometimes, a patient cannot differentiate between hair loss and regular hair fall. Diagnosing hair-related diseases is time-consuming as it requires professional dermatologists to perform visual and medical tests. Because of that, the overall diagnosis gets delayed, which worsens the severity of the illness. Due to the image-processing ability, neural network-based applications are used in various sectors, especially healthcare and health informatics, to predict deadly diseases like cancers and tumors. These applications assist clinicians and patients and provide an initial insight into early-stage symptoms. In this study, we used a deep learning approach that successfully predicts three main types of hair loss and scalp-related diseases: alopecia, psoriasis, and folliculitis. However, limited study in this area, una
    
[^242]: 连续对比微调改进低资源关系提取

    Continual Contrastive Finetuning Improves Low-Resource Relation Extraction. (arXiv:2212.10823v1 [cs.CL] CROSS LISTED)

    [http://arxiv.org/abs/2212.10823](http://arxiv.org/abs/2212.10823)

    本文提出了一种使用连续对比微调的方法来改进低资源关系提取，通过使用一致的对比学习目标预训练和微调RE模型，以及多中心对比损失来允许一个关系形成多个聚类。实验结果表明该方法可以显着提高低资源情况和领域中的关系提取性能。

    

    关系提取（RE）依赖结构化注释语料库进行模型训练，尤其在低资源情况和领域中，该任务具有挑战性。近期研究通过自监督学习来解决低资源的RE，其中解决方案包括通过RE目标预训练关系嵌入，并通过分类为基础的目标对有标签数据进行微调。然而，这种方法的一个关键挑战是目标之间的差距，它阻止RE模型充分利用预训练表示中的知识。本文旨在弥合差距，并提出使用一致的对比学习目标预训练和微调RE模型。由于在这种表示学习范式中，一个关系可能在表示空间中轻松形成多个聚类，因此我们进一步提出了多中心对比损失，允许一个关系形成多个聚类以更好地对齐预训练。在两个文档中的实验表明，所提出的方法可以在低资源情况和领域中显着提高关系提取性能。

    Relation extraction (RE), which has relied on structurally annotated corpora for model training, has been particularly challenging in low-resource scenarios and domains. Recent literature has tackled low-resource RE by self-supervised learning, where the solution involves pretraining the relation embedding by RE-based objective and finetuning on labeled data by classification-based objective. However, a critical challenge to this approach is the gap in objectives, which prevents the RE model from fully utilizing the knowledge in pretrained representations. In this paper, we aim at bridging the gap and propose to pretrain and finetune the RE model using consistent objectives of contrastive learning. Since in this kind of representation learning paradigm, one relation may easily form multiple clusters in the representation space, we further propose a multi-center contrastive loss that allows one relation to form multiple clusters to better align with pretraining. Experiments on two docum
    
[^243]: 何时不信任语言模型：探索参数和非参数记忆的有效性和限制。

    When Not to Trust Language Models: Investigating Effectiveness and Limitations of Parametric and Non-Parametric Memories. (arXiv:2212.10511v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10511](http://arxiv.org/abs/2212.10511)

    本文通过对10个模型和4种增强方法的实验，发现语言模型在记忆不太流行的实际知识方面存在困难，而检索增强的语言模型表现较好，提出了一种检索增强语言模型的简单有效方法。

    

    尽管大型语言模型在各种任务上表现出色，但仍然难以处理需要丰富世界知识的任务，这暗示了仅依靠其参数来编码丰富的世界知识的局限性。本文旨在通过对10个模型和4种增强方法在PopQA上进行大规模知识探测实验，以了解语言模型在记忆事实知识方面的优点和局限性。我们发现，语言模型难以记忆不太流行的实际知识，并且在长尾中，扩展规模无法明显改善记忆实际知识。然后，我们展示了检索增强的语言模型在很大程度上胜过级别大得多的语言模型，而未经协助的语言模型在涉及高流行实体的问题上仍然具有竞争力。基于这些发现，我们设计了一种简单而有效的强大和高效的检索增强语言模型方法，该方法仅在需要时检索非参数记忆。

    Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the limitations of relying solely on their parameters to encode a wealth of world knowledge. This paper aims to understand LMs' strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments of 10 models and 4 augmentation methods on PopQA, our new open-domain QA dataset with 14k questions. We find that LMs struggle with less popular factual knowledge, and that scaling fails to appreciably improve memorization of factual knowledge in the long tail. We then show that retrieval-augmented LMs largely outperform orders of magnitude larger LMs, while unassisted LMs remain competitive in questions about high-popularity entities. Based on those findings, we devise a simple, yet effective, method for powerful and efficient retrieval-augmented LMs, which retrieves non-parametric memories only whe
    
[^244]: 时序数据的序列预测置信推断算法

    Sequential Predictive Conformal Inference for Time Series. (arXiv:2212.03463v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2212.03463](http://arxiv.org/abs/2212.03463)

    提出了一种新的适用于时序数据的自适应重新估计条件分位数的置信预测算法SPCI，相较于其他现有方法，SPCI在所需经验覆盖下的区间宽度显著减小。

    

    我们提出了一种新的分布自由的序列数据（例如时间序列）置信预测算法，称为“序列预测置信推断”（SPCI）。我们特别考虑到时间序列数据是不可交换的性质，因此许多现有的置信预测算法不适用。主要思想是在利用它们之间的时间依赖性时，自适应重新估计非一致性分数（例如预测残差）的条件分位数。更具体地，我们将置信预测区间的问题视为预测未来残差的分位数，给定用户指定的点预测算法。从理论上讲，在扩展分位数回归的一致性分析的基础上，我们建立了渐近有效的条件覆盖。通过模拟和真实数据实验，我们证明了SPCI相对于其他现有方法在所需经验覆盖下的区间宽度显著减小。

    We present a new distribution-free conformal prediction algorithm for sequential data (e.g., time series), called the \textit{sequential predictive conformal inference} (\texttt{SPCI}). We specifically account for the nature that time series data are non-exchangeable, and thus many existing conformal prediction algorithms are not applicable. The main idea is to adaptively re-estimate the conditional quantile of non-conformity scores (e.g., prediction residuals), upon exploiting the temporal dependence among them. More precisely, we cast the problem of conformal prediction interval as predicting the quantile of a future residual, given a user-specified point prediction algorithm. Theoretically, we establish asymptotic valid conditional coverage upon extending consistency analyses in quantile regression. Using simulation and real-data experiments, we demonstrate a significant reduction in interval width of \texttt{SPCI} compared to other existing methods under the desired empirical cover
    
[^245]: 动态线性臂机（Dynamical Linear Bandits）

    Dynamical Linear Bandits. (arXiv:2211.08997v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.08997](http://arxiv.org/abs/2211.08997)

    本文提出了动态线性臂机（DLB）这一概念，这是线性臂机的扩展，具有隐藏状态。这一方法可用于解决在实际决策中行动不会立即反映在反馈上，并在较长的时间范围内扩散其影响所引起的问题。

    

    在许多实际的顺序决策问题中，一个行动不会立即反映在反馈上，并在较长的时间范围内扩散其影响。例如，在在线广告中，投资于某个平台会产生即时的意识增长，但实际回报，即转化，可能发生在未来较远的时间。此外，转化是否发生取决于意识的增长速度、其消失效应以及与其他广告平台的协同或干扰。先前的研究调查了具有延迟和聚合反馈可能性的多臂赌博机框架，没有关于一个行动在未来如何传播的特定结构，忽略了可能的动态效应。在本文中，我们引入了一种新的设置，即动态线性臂机（DLB），这是线性臂机的一种扩展，其特征是具有隐藏状态。当执行一个行动时，学习者观察到一个噪声回报，其均值是一个线性函数

    In many real-world sequential decision-making problems, an action does not immediately reflect on the feedback and spreads its effects over a long time frame. For instance, in online advertising, investing in a platform produces an instantaneous increase of awareness, but the actual reward, i.e., a conversion, might occur far in the future. Furthermore, whether a conversion takes place depends on: how fast the awareness grows, its vanishing effects, and the synergy or interference with other advertising platforms. Previous work has investigated the Multi-Armed Bandit framework with the possibility of delayed and aggregated feedback, without a particular structure on how an action propagates in the future, disregarding possible dynamical effects. In this paper, we introduce a novel setting, the Dynamical Linear Bandits (DLB), an extension of the linear bandits characterized by a hidden state. When an action is performed, the learner observes a noisy reward whose mean is a linear functio
    
[^246]: FedGen: 适用于序列数据的可推广联邦学习

    FedGen: Generalizable Federated Learning for Sequential Data. (arXiv:2211.01914v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01914](http://arxiv.org/abs/2211.01914)

    FedGen为联邦学习带来可推广性，允许分布式设备共同识别和区分伪特征和不变特征，而不需要训练分布的先前知识。

    

    现有的遵循标准机器学习风险最小化范式的联邦学习模型在训练数据存在伪相关性的情况下往往难以推广。在许多现实世界的分布式场景中，由于分布式设备或客户端的偏差和数据采样问题，会产生伪相关性，从而错误地影响模型。当前的推广方法是为集中式训练设计的，试图识别具有与目标不变因果关系的特征，从而减少伪特征的影响。然而，这种不变风险最小化方法依赖于训练数据分布的先验知识，在许多应用中难以获得。在这项工作中，我们提出了一个名为FedGen的可推广联邦学习框架，它允许客户端以协作的方式识别和区分伪特征和不变特征，而不需要训练分布的先前知识。

    Existing federated learning models that follow the standard risk minimization paradigm of machine learning often fail to generalize in the presence of spurious correlations in the training data. In many real-world distributed settings, spurious correlations exist due to biases and data sampling issues on distributed devices or clients that can erroneously influence models. Current generalization approaches are designed for centralized training and attempt to identify features that have an invariant causal relationship with the target, thereby reducing the effect of spurious features. However, such invariant risk minimization approaches rely on apriori knowledge of training data distributions which is hard to obtain in many applications. In this work, we present a generalizable federated learning framework called FedGen, which allows clients to identify and distinguish between spurious and invariant features in a collaborative manner without prior knowledge of training distributions. We
    
[^247]: 学习通过迭代反演控制

    Learning Control by Iterative Inversion. (arXiv:2211.01724v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01724](http://arxiv.org/abs/2211.01724)

    迭代反演是一种算法，能够学习无输入输出对的反函数。该算法通过样本学习期望输出分布，可以解决所期望输出与初始随机猜测输出分布之间的偏移问题。此方法被成功应用于学习控制问题，并获得了良好的表现。

    

    我们提出了一种名为“迭代反演”的算法，用于学习无输入输出对的反函数，只使用所需输出分布的样本和对前向函数的访问。关键挑战是所期望输出与初始随机猜测的输出之间的分布偏移。我们证明了在函数的相当严格的条件下，迭代反演可以正确地引导学习。我们将迭代反演应用于学习控制。我们的输入是一组所需行为的演示，以轨迹的视频嵌入（没有动作）的形式给出，我们的方法迭代地学习模仿由当前策略生成的轨迹，受到随机探索噪声的扰动。我们的方法不需要奖励，仅使用监督学习，可以轻松扩大使用最先进的轨迹嵌入技术和策略表示。事实上，使用VQ-VAE嵌入和基于变压器的策略表示，我们在几个控制任务上获得了良好的表现。

    We propose $\textit{iterative inversion}$ -- an algorithm for learning an inverse function without input-output pairs, but only with samples from the desired output distribution and access to the forward function. The key challenge is a $\textit{distribution shift}$ between the desired outputs and the outputs of an initial random guess, and we prove that iterative inversion can steer the learning correctly, under rather strict conditions on the function. We apply iterative inversion to learn control. Our input is a set of demonstrations of desired behavior, given as video embeddings of trajectories (without actions), and our method iteratively learns to imitate trajectories generated by the current policy, perturbed by random exploration noise. Our approach does not require rewards, and only employs supervised learning, which can be easily scaled to use state-of-the-art trajectory embedding techniques and policy representations. Indeed, with a VQ-VAE embedding, and a transformer-based 
    
[^248]: 面向视觉Transformer的数据级彩票假设

    Data Level Lottery Ticket Hypothesis for Vision Transformers. (arXiv:2211.01484v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.01484](http://arxiv.org/abs/2211.01484)

    本文将传统的彩票假说（LTH）扩展到由图像补丁组成的输入数据中，证明存在一个子集的输入图像补丁使得可以从头开始训练视觉Transformer（ViT），并且达到与使用所有图像补丁训练的ViTs相似的精度，这种方法在各种视觉任务中都是可行的和有效的。

    

    传统彩票假说（LTH）声称在密集神经网络中存在着一个稀疏的子网络和一个称为“获奖彩票”的适当随机初始化方法，以便可以从头开始训练它，使其几乎像密集网络一样好。与此同时，对于视觉Transformer（ViTs）中LTH的研究却很少被评估。本文首先表明了在现有方法下，在ViTs的权重级别上寻找传统的获奖彩票是困难的。然后，我们将ViTs的LTH推广到由图像补丁组成的输入数据中，受ViTs输入依赖启发。也就是说，存在一个输入图像补丁的子集，使得通过仅使用该子集，可以从头开始训练ViT，并达到与使用所有图像补丁训练的ViTs相似的精度。我们将这个输入补丁子集称为em获奖彩票，它代表了输入数据中的大量信息。我们使用票选择器生成带有em获奖彩票的权重子集，并使用EMD训练这些“获奖”子集，可以证明这种方法在各种视觉任务中都是可行的和有效的。

    The conventional lottery ticket hypothesis (LTH) claims that there exists a sparse subnetwork within a dense neural network and a proper random initialization method called the winning ticket, such that it can be trained from scratch to almost as good as the dense counterpart. Meanwhile, the research of LTH in vision transformers (ViTs) is scarcely evaluated. In this paper, we first show that the conventional winning ticket is hard to find at the weight level of ViTs by existing methods. Then, we generalize the LTH for ViTs to input data consisting of image patches inspired by the input dependence of ViTs. That is, there exists a subset of input image patches such that a ViT can be trained from scratch by using only this subset of patches and achieve similar accuracy to the ViTs trained by using all image patches. We call this subset of input patches the em winning tickets, which represent a significant amount of information in the input data. We use a ticket selector to generate the w
    
[^249]: 回溯式反事实推理

    Backtracking Counterfactuals. (arXiv:2211.00472v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.00472](http://arxiv.org/abs/2211.00472)

    本文介绍了一种基于回溯式反事实推理的形式化方法，该方法基于图形模型，提供了一种更加自然和直观的推理过去时反事实情景的方法。

    

    反事实推理是人类思维中广泛存在的一种推理方式——设想一些假设场景或可能存在的情况，这些情况与实际情况不同。传统上，反事实情景被视为局部违反自然规律的“小奇迹”，但它们具有相同的初始条件。而在Pearl的结构因果模型(SCM)框架中，这通过修改因果定律的干预而使得外生变量的值共享得到了数学上的严格化。但近年来，哲学家和心理学家对这种单纯的干预主义反事实观点提出了越来越多的质疑。相反，他们提出了一种回溯式反事实观点，即在反事实世界中因果定律保持不变，将与实际情况的差异“回溯”到改变的初始条件(外生变量)。在本文中，我们提出了一种基于简单但灵活的图形模型的回溯式反事实推理的形式化方法。我们认为，在某些情况下，特别是在推理过去时，我们的模型提供了一种更自然、更直观的反事实推理方法。

    Counterfactual reasoning -- envisioning hypothetical scenarios, or possible worlds, where some circumstances are different from what (f)actually occurred (counter-to-fact) -- is ubiquitous in human cognition. Conventionally, counterfactually-altered circumstances have been treated as "small miracles" that locally violate the laws of nature while sharing the same initial conditions. In Pearl's structural causal model (SCM) framework this is made mathematically rigorous via interventions that modify the causal laws while the values of exogenous variables are shared. In recent years, however, this purely interventionist account of counterfactuals has increasingly come under scrutiny from both philosophers and psychologists. Instead, they suggest a backtracking account of counterfactuals, according to which the causal laws remain unchanged in the counterfactual world; differences to the factual world are instead "backtracked" to altered initial conditions (exogenous variables). In the pres
    
[^250]: 傅立叶分析实现一致且真实的模型解释

    Consistent and Truthful Interpretation with Fourier Analysis. (arXiv:2210.17426v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2210.17426](http://arxiv.org/abs/2210.17426)

    该论文提出了一个称为真实解释的新概念，通过傅立叶分析获得严格保证，并在实验中证明了其在支持假设情景和降低解释误差方面的优势。

    

    对于许多跨学科领域，机器学习的解释需要与当前案例相关的假设情景一致，即如果一个因素改变，模型会如何反应？尽管归因方法由优雅的公理系统支持，但它们主要关注单个输入，并且通常不一致。为支持假设情景，我们引入了一个称为真实解释的新概念，并应用布尔函数的傅立叶分析来获得严格的保证。实验结果表明，对于各种半径的邻域，我们的方法与其他方法相比，可以实现2倍至50倍更低的解释误差。

    For many interdisciplinary fields, ML interpretations need to be consistent with what-if scenarios related to the current case, i.e., if one factor changes, how does the model react? Although the attribution methods are supported by the elegant axiomatic systems, they mainly focus on individual inputs, and are generally inconsistent. To support what-if scenarios, we introduce a new notion called truthful interpretation, and apply Fourier analysis of Boolean functions to get rigorous guarantees. Experimental results show that for neighborhoods with various radii, our method achieves 2x - 50x lower interpretation error compared with the other methods.
    
[^251]: 训练神经网络用于时序变点检测

    Training Neural Networks for Sequential Change-point Detection. (arXiv:2210.17312v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.17312](http://arxiv.org/abs/2210.17312)

    本文介绍了一种使用神经网络进行在线变点检测的方法，通过训练神经网络逐步计算检测统计量的累积和来检测变点，并在合成和真实数据上证明了该方法的优越性和潜力。

    

    检测数据流中的突变分布转换，即所谓的变点检测，是统计学和机器学习中的一个基本问题。我们引入了一种新颖的方法，使用神经网络进行在线变点检测。具体而言，我们的方法是训练神经网络来逐步计算检测统计量的累积和，当发生变点时，该量会显著变化。我们使用合成和真实世界数据证明了所提出的方法在检测变点方面的优越性和潜力。

    Detecting an abrupt distributional shift of a data stream, known as change-point detection, is a fundamental problem in statistics and machine learning. We introduce a novel approach for online change-point detection using neural networks. To be specific, our approach is training neural networks to compute the cumulative sum of a detection statistic sequentially, which exhibits a significant change when a change-point occurs. We demonstrated the superiority and potential of the proposed method in detecting change-point using both synthetic and real-world data.
    
[^252]: 男性也洗衣服：多属性偏见放大

    Men Also Do Laundry: Multi-Attribute Bias Amplification. (arXiv:2210.11924v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.11924](http://arxiv.org/abs/2210.11924)

    本文研究了计算机视觉中的多属性偏见放大现象，发现现有的度量可能会忽略多个属性之间的相关性，并可能错误地给出度量结果，使得最小或没有偏见放大的情况被误判。

    

    随着计算机视觉系统越来越广泛地部署，研究界和公众对这些系统不仅会复制，而且会放大有害社会偏见的担忧日益增加。本文所关注的偏见放大现象是指模型在测试时放大固有的训练集偏见。现有的指标针对单个注释属性（例如，“电脑”）测量偏见放大。然而，几个视觉数据集由具有多个属性注释的图像组成。我们展示了模型可以学习利用多个属性（例如{“电脑”，“键盘”}）的相关性，而这些相关性不被当前的度量所考虑。此外，我们还展示了当前的度量可能会错误地给出最小或没有偏见放大的印象，因为它们涉及对正值和负值进行聚合。此外，这些指标缺乏明确的期望值，使它们变得困难。

    As computer vision systems become more widely deployed, there is increasing concern from both the research community and the public that these systems are not only reproducing but amplifying harmful social biases. The phenomenon of bias amplification, which is the focus of this work, refers to models amplifying inherent training set biases at test time. Existing metrics measure bias amplification with respect to single annotated attributes (e.g., $\texttt{computer}$). However, several visual datasets consist of images with multiple attribute annotations. We show models can learn to exploit correlations with respect to multiple attributes (e.g., {$\texttt{computer}$, $\texttt{keyboard}$}), which are not accounted for by current metrics. In addition, we show current metrics can give the erroneous impression that minimal or no bias amplification has occurred as they involve aggregating over positive and negative values. Further, these metrics lack a clear desired value, making them diffic
    
[^253]: 一种软件系统版本序列上的调用图演化分析方法

    Call Graph Evolution Analytics over a Version Series of an Evolving Software System. (arXiv:2210.08316v1 [cs.SE] CROSS LISTED)

    [http://arxiv.org/abs/2210.08316](http://arxiv.org/abs/2210.08316)

    该论文提出了一种Call Graph Evolution Analytics的方法，用于从软件系统版本序列的演化调用图中提取信息，通过分析演化调用图的依赖关系模式的演化，帮助软件工程师进行版本演化管理。

    

    调用图演化分析可以帮助软件工程师在维护或演进软件系统时进行更好的决策。该论文提出了一种Call Graph Evolution Analytics的方法，用于从软件系统版本序列 VS = V_1, V_2, … V_N 的演化调用图 ECG = CG_1, CG_2, … CG_N 中提取信息。这是通过使用Call Graph Evolution Rules（CGERs）和Call Graph Evolution Subgraphs（CGESs）完成的。类似于关联规则挖掘，CGERs用于捕获系统中依赖关系的共现。与调用图中的子图模式类似，CGESs用于捕获演化调用图中的依赖关系模式的演化。对这些模式的演化进行调用图分析可以识别出需要关注的潜在受影响的依赖关系（或过程调用）。

    Call Graph evolution analytics can aid a software engineer when maintaining or evolving a software system. This paper proposes Call Graph Evolution Analytics to extract information from an evolving call graph ECG = CG_1, CG_2,... CG_N for their version series VS = V_1, V_2, ... V_N of an evolving software system. This is done using Call Graph Evolution Rules (CGERs) and Call Graph Evolution Subgraphs (CGESs). Similar to association rule mining, the CGERs are used to capture co-occurrences of dependencies in the system. Like subgraph patterns in a call graph, the CGESs are used to capture evolution of dependency patterns in evolving call graphs. Call graph analytics on the evolution in these patterns can identify potentially affected dependencies (or procedure calls) that need attention. The experiments are done on the evolving call graphs of 10 large evolving systems to support dependency evolution management. We also consider results from a detailed study for evolving call graphs of M
    
[^254]: 双控制变量加速黑盒变分推断

    Dual control variate for faster black-box variational inference. (arXiv:2210.07290v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.07290](http://arxiv.org/abs/2210.07290)

    本论文提出了双控制变量方法，能够同时减少数据子抽样和蒙特卡罗抽样带来的梯度估计方差，提高黑盒变分推断的准确性和效率。

    

    黑盒变分推断是一种广泛使用的贝叶斯后验推断框架，但在某些情况下，梯度估计中的高方差会损害准确性和效率。这种方差来自两个随机源：数据子抽样和蒙特卡罗抽样。现有的控制变量仅解决蒙特卡罗噪声，而增量梯度方法通常仅解决数据子抽样，我们提出了一种新的“双”控制变量，能够同时减少两种噪声源的方差。我们确认这导致了减少方差和在多个现实世界应用中提高优化效果。

    Black-box variational inference is a widely-used framework for Bayesian posterior inference, but in some cases suffers from high variance in gradient estimates, harming accuracy and efficiency. This variance comes from two sources of randomness: Data subsampling and Monte Carlo sampling. Whereas existing control variates only address Monte Carlo noise and incremental gradient methods typically only address data subsampling, we propose a new "dual" control variate capable of jointly reducing variance from both sources of noise. We confirm that this leads to reduced variance and improved optimization in several real-world applications.
    
[^255]: 基于神经重要性采样的快速可靠引力波推断

    Neural Importance Sampling for Rapid and Reliable Gravitational-Wave Inference. (arXiv:2210.05686v2 [gr-qc] UPDATED)

    [http://arxiv.org/abs/2210.05686](http://arxiv.org/abs/2210.05686)

    本文提出了一种基于神经网络和重要性采样的方法，用于快速且准确地进行引力波推断。该方法不仅可以得到不受网络不准确性影响的校正后验，还可以评估建议和识别失败情况的性能诊断以及得到贝叶斯证据的无偏估计。

    

    我们将摊余神经后验估计与重要性采样相结合，以实现快速准确的引力波推断。首先使用神经网络生成贝叶斯后验的快速建议，然后根据基本似然和先验计算重要性权重。这提供了（1）一个不受网络不准确性影响的校正后验、（2）评估建议和识别失败情况的性能诊断（样本效率）以及（3）贝叶斯证据的无偏估计。通过建立这种独立的验证和校正机制，我们解决了一些针对用深度学习进行科学推断的最频繁批评。我们进行了一项大型研究，使用SEOBv4PHM和IMRPhenomXPHM波形模型分析了LIGO和Virgo观察到的42个双黑洞合并案例。研究结果显示，中位样本效率约为10％（比标准采样器高两个数量级），同时状态空间体积减小了十倍以上。

    We combine amortized neural posterior estimation with importance sampling for fast and accurate gravitational-wave inference. We first generate a rapid proposal for the Bayesian posterior using neural networks, and then attach importance weights based on the underlying likelihood and prior. This provides (1) a corrected posterior free from network inaccuracies, (2) a performance diagnostic (the sample efficiency) for assessing the proposal and identifying failure cases, and (3) an unbiased estimate of the Bayesian evidence. By establishing this independent verification and correction mechanism we address some of the most frequent criticisms against deep learning for scientific inference. We carry out a large study analyzing 42 binary black hole mergers observed by LIGO and Virgo with the SEOBNRv4PHM and IMRPhenomXPHM waveform models. This shows a median sample efficiency of $\approx 10\%$ (two orders-of-magnitude better than standard samplers) as well as a ten-fold reduction in the sta
    
[^256]: ParaDime：参数化降维框架

    ParaDime: A Framework for Parametric Dimensionality Reduction. (arXiv:2210.04582v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.04582](http://arxiv.org/abs/2210.04582)

    ParaDime 是一个参数化降维框架，通过提供统一的接口，允许用户自定义数据之间的关系和转换，从而将多个现代 DR 技术统一起来。借助 ParaDime，用户可以方便地实验各种 DR 技术，如混合分类 / 嵌入模型和监督 DR，为高维数据的可视化打开新的可能性。

    

    ParaDime 是一个参数化降维（DR）框架。在参数化 DR 中，神经网络被训练成将高维数据点嵌入到低维空间中，同时最小化一个目标函数。ParaDime 建立在一些现代 DR 技术的目标函数是基于转换过的数据之间的关系的思想上，为这些关系和转换提供了统一的接口，定义了它们如何被用于规范训练过程中的损失。通过这个接口，ParaDime 统一了度量 MDS、t-SNE 和 UMAP 等 DR 技术的参数化版本。它允许用户完全自定义 DR 过程的所有方面。我们展示了这种易于定制的特点使 ParaDime 适用于实验性技术，例如混合分类 / 嵌入模型和监督 DR。通过 ParaDime，打开了可视化高维数据的新可能性。

    ParaDime is a framework for parametric dimensionality reduction (DR). In parametric DR, neural networks are trained to embed high-dimensional data items in a low-dimensional space while minimizing an objective function. ParaDime builds on the idea that the objective functions of several modern DR techniques result from transformed inter-item relationships. It provides a common interface for specifying these relations and transformations and for defining how they are used within the losses that govern the training process. Through this interface, ParaDime unifies parametric versions of DR techniques such as metric MDS, t-SNE, and UMAP. It allows users to fully customize all aspects of the DR process. We show how this ease of customization makes ParaDime suitable for experimenting with interesting techniques such as hybrid classification/embedding models and supervised DR. This way, ParaDime opens up new possibilities for visualizing high-dimensional data.
    
[^257]: InfoOT: 信息最大化的最优输运

    InfoOT: Information Maximizing Optimal Transport. (arXiv:2210.03164v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.03164](http://arxiv.org/abs/2210.03164)

    提出了InfoOT，它是一种信息论扩展的最优输运方法，能够解决最优输运忽略了数据中相干结构的问题，同时能够处理离群值和集成新数据点，可以提高域自适应、跨域检索和单细胞对齐等任务的对齐质量。

    

    最优输运通过最小化它们之间的运输成本（例如几何距离）对分布中的样本进行对齐。然而，它忽略了数据中的相干结构，例如簇，不能很好地处理离群值，也不能集成新数据点。为解决这些问题，我们提出了InfoOT，它是最优输运的信息论扩展，可以在最小化几何距离的同时最大化域之间的互信息。最终的目标仍然可以被公式化为（广义的）最优输运问题，并可以通过投影梯度下降有效地求解。这种公式化产生了一种新的投影方法，它对离群值具有鲁棒性，并且可以推广到未见过的样本。实证结果表明，在域自适应、跨域检索和单细胞对齐等基准中，InfoOT可以提高对齐的质量。

    Optimal transport aligns samples across distributions by minimizing the transportation cost between them, e.g., the geometric distances. Yet, it ignores coherence structure in the data such as clusters, does not handle outliers well, and cannot integrate new data points. To address these drawbacks, we propose InfoOT, an information-theoretic extension of optimal transport that maximizes the mutual information between domains while minimizing geometric distances. The resulting objective can still be formulated as a (generalized) optimal transport problem, and can be efficiently solved by projected gradient descent. This formulation yields a new projection method that is robust to outliers and generalizes to unseen samples. Empirically, InfoOT improves the quality of alignments across benchmarks in domain adaptation, cross-domain retrieval, and single-cell alignment.
    
[^258]: 使用核均值池化学习鲁棒的核集成

    Learning Robust Kernel Ensembles with Kernel Average Pooling. (arXiv:2210.00062v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.00062](http://arxiv.org/abs/2210.00062)

    本文提出了核均值池（KAP），一种神经网络构建模块，它可以在卷积神经网络中自然地产生具有相似功能的核集合，提高模型对输入扰动的鲁棒性，并在多个数据集上的实验证明了其有效性。

    

    模型集成一直被用于机器学习中，以减少个别模型预测的方差，使其对输入扰动更加鲁棒。假集成方法（如dropout）也常用于深度学习模型中以提高泛化能力。然而，利用这些技术提高神经网络对输入扰动的鲁棒性仍未得到充分探索。我们引入了核均值池（KAP），它是一个神经网络构建模块，可沿着层激活张量的核维度应用均值滤波器。我们展示了在使用KAP以及通过反向传播训练的卷积神经网络中，具有相似功能的核集合自然地产生。此外，我们展示了在使用加性高斯噪声扰动的输入上进行训练时，KAP模型对各种形式的对抗性攻击具有显著的鲁棒性。在CIFAR10、CIFAR100、TinyImagenet和Imagenet数据集上的实证评估显示了实质性的结果。

    Model ensembles have long been used in machine learning to reduce the variance in individual model predictions, making them more robust to input perturbations. Pseudo-ensemble methods like dropout have also been commonly used in deep learning models to improve generalization. However, the application of these techniques to improve neural networks' robustness against input perturbations remains underexplored. We introduce Kernel Average Pooling (KAP), a neural network building block that applies the mean filter along the kernel dimension of the layer activation tensor. We show that ensembles of kernels with similar functionality naturally emerge in convolutional neural networks equipped with KAP and trained with backpropagation. Moreover, we show that when trained on inputs perturbed with additive Gaussian noise, KAP models are remarkably robust against various forms of adversarial attacks. Empirical evaluations on CIFAR10, CIFAR100, TinyImagenet, and Imagenet datasets show substantial 
    
[^259]: 从相关数据中训练归一化流

    Training Normalizing Flows from Dependent Data. (arXiv:2209.14933v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.14933](http://arxiv.org/abs/2209.14933)

    该论文提出了一种考虑数据点依赖关系的归一化流似然目标和学习算法，在合成和真实数据上实现更好的经验结果和更高的统计功效。

    

    归一化流是一种功能强大的非参数统计模型，它是密度估计器和生成模型之间的混合体。目前的归一化流学习算法假定数据点是独立采样的，这一假设在实践中经常被违反，可能导致密度估计和数据生成的错误。我们提出了一种考虑数据点之间依赖关系的归一化流似然目标，并为此推导了一种适用于不同依赖结构的灵活高效的学习算法。我们表明，尊重观察值之间的依赖关系可以改善合成和实际数据的经验结果，并在对全基因组关联研究的下游应用中导致更高的统计功效。

    Normalizing flows are powerful non-parametric statistical models that function as a hybrid between density estimators and generative models. Current learning algorithms for normalizing flows assume that data points are sampled independently, an assumption that is frequently violated in practice, which may lead to erroneous density estimation and data generation. We propose a likelihood objective of normalizing flows incorporating dependencies between the data points, for which we derive a flexible and efficient learning algorithm suitable for different dependency structures. We show that respecting dependencies between observations can improve empirical results on both synthetic and real-world data, and leads to higher statistical power in a downstream application to genome-wide association studies.
    
[^260]: 近端点模仿学习

    Proximal Point Imitation Learning. (arXiv:2209.10968v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.10968](http://arxiv.org/abs/2209.10968)

    本文提出了一种适用于无限时域模仿学习的新算法，采用了优化中的经典工具近端点法和对偶平滑，从而得到了比之前更好的效率保证。通过优化一个单个的凸平滑目标来同时更新成本和Q函数，避免了嵌套策略评估和成本更新，具有更好的应用前景。

    

    本文提出了一种用于具有线性函数逼近但没有限制一致性假设的无限时域模仿学习（IL）的新算法，并具有严格的效率保证。我们从问题的极小化-极大化形式开始，并概述如何利用优化中的经典工具-近端点法（PPM）和对偶平滑来进行在线和离线IL。由于PPM，我们避免了先前文献中出现的在线IL中出现的嵌套策略评估和成本更新。特别地，我们通过优化一个单个的凸平滑目标来同时更新成本和Q函数，摆脱了常规的交替更新。当不精确地求解时，我们将优化误差与恢复策略的次优性相关联。作为额外的奖励，通过将PPM重新解释为以专家策略为中心点的对偶平滑，我们还获得了离线IL算法，享有关于所需专家所具有的理论保证。

    This work develops new algorithms with rigorous efficiency guarantees for infinite horizon imitation learning (IL) with linear function approximation without restrictive coherence assumptions. We begin with the minimax formulation of the problem and then outline how to leverage classical tools from optimization, in particular, the proximal-point method (PPM) and dual smoothing, for online and offline IL, respectively. Thanks to PPM, we avoid nested policy evaluation and cost updates for online IL appearing in the prior literature. In particular, we do away with the conventional alternating updates by the optimization of a single convex and smooth objective over both cost and Q-functions. When solved inexactly, we relate the optimization errors to the suboptimality of the recovered policy. As an added bonus, by re-interpreting PPM as dual smoothing with the expert policy as a center point, we also obtain an offline IL algorithm enjoying theoretical guarantees in terms of required expert
    
[^261]: CLIP是否知道我的脸？

    Does CLIP Know My Face?. (arXiv:2209.07341v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.07341](http://arxiv.org/abs/2209.07341)

    本文提出了一种新方法IDIA来评估视觉语言模型的隐私，大规模实验表明使用于训练的个人可以被非常高的准确率识别出来，表明需要更好地解决视觉语言模型中的隐私问题。

    

    随着深度学习在各个应用中的普及，保护训练数据的隐私问题已经成为一个关键的研究领域。以前的研究主要关注单模型的隐私风险，我们提出了一种新的方法来评估多模型的隐私，特别是像CLIP这样的视觉语言模型。所提出的身份推断攻击(IDIA)通过用同一人的图片向模型查询，从而揭示该个人是否被包含在训练数据中。让模型从各种可能的文本标签中选择，模型会透露是否识别该人物，从而表明其被用于训练。我们在CLIP上进行的大规模实验表明，使用于训练的个人可以被非常高的准确率识别出来。我们确认该模型已经学会将名称与描绘的个人相关联，这意味着敏感信息存在于其中，可以被对手提取。我们的结果凸显了需要在视觉语言模型中更好地解决隐私问题。

    With the rise of deep learning in various applications, privacy concerns around the protection of training data has become a critical area of research. Whereas prior studies have focused on privacy risks in single-modal models, we introduce a novel method to assess privacy for multi-modal models, specifically vision-language models like CLIP. The proposed Identity Inference Attack (IDIA) reveals whether an individual was included in the training data by querying the model with images of the same person. Letting the model choose from a wide variety of possible text labels, the model reveals whether it recognizes the person and, therefore, was used for training. Our large-scale experiments on CLIP demonstrate that individuals used for training can be identified with very high accuracy. We confirm that the model has learned to associate names with depicted individuals, implying the existence of sensitive information that can be extracted by adversaries. Our results highlight the need for 
    
[^262]: 基于偏好的多目标强化学习算法：PD-MORL

    PD-MORL: Preference-Driven Multi-Objective Reinforcement Learning Algorithm. (arXiv:2208.07914v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.07914](http://arxiv.org/abs/2208.07914)

    PD-MORL是一种基于偏好的多目标强化学习算法，其采用偏好作为指导，适应于各种偏好空间的目标，可扩展到连续的机器人任务，并在多目标机器人控制任务上优于现有的MORL方法。

    

    多目标强化学习（MORL）方法通过最大化由偏好向量加权的联合目标函数，针对多个冲突目标的实际问题进行了处理。然而，在现实情况下，设计约束和目标通常会动态变化。此外，存储每个潜在偏好的策略是不可扩展的。因此，在给定域中使用单次训练获取整个偏好空间的帕累托前沿解集是至关重要的。为此，我们提出了一种新的MORL算法，训练一个单一的通用网络以覆盖整个偏好空间，并可扩展到连续的机器人任务。所提出的方法“基于偏好的MORL（PD-MORL）”利用偏好以更新网络参数，还采用了一种新颖的并行化方法以提高样本效率并适应当前情况下的各种偏好空间的目标。我们在多目标机器人控制任务上评估PD-MORL，并表明它在帕累托前沿覆盖和样本效率方面优于现有的MORL方法。

    Multi-objective reinforcement learning (MORL) approaches have emerged to tackle many real-world problems with multiple conflicting objectives by maximizing a joint objective function weighted by a preference vector. These approaches find fixed customized policies corresponding to preference vectors specified during training. However, the design constraints and objectives typically change dynamically in real-life scenarios. Furthermore, storing a policy for each potential preference is not scalable. Hence, obtaining a set of Pareto front solutions for the entire preference space in a given domain with a single training is critical. To this end, we propose a novel MORL algorithm that trains a single universal network to cover the entire preference space scalable to continuous robotic tasks. The proposed approach, Preference-Driven MORL (PD-MORL), utilizes the preferences as guidance to update the network parameters. It also employs a novel parallelization approach to increase sample effi
    
[^263]: 无线网络中基于图神经网络的隐私保护分布式推理

    Privacy-Preserving Decentralized Inference with Graph Neural Networks in Wireless Networks. (arXiv:2208.06963v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2208.06963](http://arxiv.org/abs/2208.06963)

    本文研究了在无线网络中采用图神经网络进行分散式推理所可能导致的隐私泄露问题，并提出使用本地差分隐私度量和新设计的隐私保护信号进行保护的方法。同时，为了增强透明度，通过模拟展示了隐私保护通信和推理的过程。

    

    作为一种有效的用于图数据的神经网络模型，图神经网络最近在各种无线优化问题中得到成功应用。鉴于GNN的推理阶段可以自然地实现分散式，因此GNN是次世代无线通信的分散式控制/管理的潜在推手。然而，在GNN的分散推理中，由于邻居之间的信息交换，可能导致隐私泄露。为了解决这个问题，本文分析并增强了无线网络中基于GNN的分散推理的隐私。具体而言，我们采用本地差分隐私作为度量标准，并设计了新的保护隐私的信号以及保证隐私的训练算法来实现保护隐私的推理。我们还定义了SNR-隐私权衡函数，以分析在无线网络中采用GNN的分散推理的性能上限。为了进一步增强透明度，我们通过广泛的模拟展示了基于GNN的隐私保护通信和推理过程。

    As an efficient neural network model for graph data, graph neural networks (GNNs) recently find successful applications for various wireless optimization problems. Given that the inference stage of GNNs can be naturally implemented in a decentralized manner, GNN is a potential enabler for decentralized control/management in the next-generation wireless communications. Privacy leakage, however, may occur due to the information exchanges among neighbors during decentralized inference with GNNs. To deal with this issue, in this paper, we analyze and enhance the privacy of decentralized inference with GNNs in wireless networks. Specifically, we adopt local differential privacy as the metric, and design novel privacy-preserving signals as well as privacy-guaranteed training algorithms to achieve privacy-preserving inference. We also define the SNR-privacy trade-off function to analyze the performance upper bound of decentralized inference with GNNs in wireless networks. To further enhance t
    
[^264]: RLang：一种描述部分领域知识给强化学习智能体的声明性语言

    RLang: A Declarative Language for Describing Partial World Knowledge to Reinforcement Learning Agents. (arXiv:2208.06448v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2208.06448](http://arxiv.org/abs/2208.06448)

    RLang是一种声明性语言，可以为强化学习智能体提供部分世界模型的信息，包括无模型和有模型表格算法、策略梯度和基于价值的方法、分层方法和深度方法。

    

    我们介绍了一种名为 RLang 的领域特定语言(DSL)，用于与强化学习智能体通信。与现有的只与决策制定形式中的一个元素(如奖励函数或策略)相关的 DSL 不同，RLang 可以指定关于Markov决策过程的每个元素的信息。我们为RLang定义了精确的语法和基础语义，并提供了一个解析器，将RLang程序基于算法时不变的部分世界模型和策略。我们提供了一系列演示不同RL方法如何利用所得知识的RLang程序，包括无模型和有模型表格算法、策略梯度和基于价值的方法、分层方法和深度方法。

    We introduce RLang, a domain-specific language (DSL) for communicating domain knowledge to an RL agent. Unlike existing RL DSLs that ground to \textit{single} elements of a decision-making formalism (e.g., the reward function or policy), RLang can specify information about every element of a Markov decision process. We define precise syntax and grounding semantics for RLang, and provide a parser that grounds RLang programs to an algorithm-agnostic \textit{partial} world model and policy that can be exploited by an RL agent. We provide a series of example RLang programs demonstrating how different RL methods can exploit the resulting knowledge, encompassing model-free and model-based tabular algorithms, policy gradient and value-based methods, hierarchical approaches, and deep methods.
    
[^265]: MobileNeRF：利用多边形光栅化管线在移动架构上高效呈现神经现场。

    MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures. (arXiv:2208.00277v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2208.00277](http://arxiv.org/abs/2208.00277)

    这篇论文提出了一种在移动设备上高效呈现神经现场的方法，通过使用纹理多边形而不是基于射线行进的体积渲染算法，并且利用传统渲染管线中的 z-缓冲器，使得 NeRF 可以通过像素级并行性来实现在移动设备上交互式帧速率。

    

    神经辐射场（NeRF）展示了从新颖的视角合成3D场景图像的惊人能力。然而，它们依赖于基于射线行进的专业体积渲染算法，这些算法与广泛部署的图形硬件的能力不匹配。本文介绍了一种基于纹理多边形的新型NeRF表示法，它可以使用标准渲染管线高效地合成新的图像。将NeRF表示为一组多边形，纹理表示二元不透明度和特征向量。使用z-缓冲器传统呈现多边形，可以在每个像素处获得特征的图像，这些特征由小型、视图相关的MLP在片段着色器中解释以生成最终的像素颜色。这种方法使得NeRF可以使用传统的多边形光栅化管线进行呈现，这提供了大规模的像素级并行性，在包括移动电话在内的各种计算平台上实现交互式帧速率。

    Neural Radiance Fields (NeRFs) have demonstrated amazing ability to synthesize images of 3D scenes from novel views. However, they rely upon specialized volumetric rendering algorithms based on ray marching that are mismatched to the capabilities of widely deployed graphics hardware. This paper introduces a new NeRF representation based on textured polygons that can synthesize novel images efficiently with standard rendering pipelines. The NeRF is represented as a set of polygons with textures representing binary opacities and feature vectors. Traditional rendering of the polygons with a z-buffer yields an image with features at every pixel, which are interpreted by a small, view-dependent MLP running in a fragment shader to produce a final pixel color. This approach enables NeRFs to be rendered with the traditional polygon rasterization pipeline, which provides massive pixel-level parallelism, achieving interactive frame rates on a wide range of compute platforms, including mobile pho
    
[^266]: 基于LDA主题建模的自动文本摘要的数据驱动潜在语意分析

    A Data-driven Latent Semantic Analysis for Automatic Text Summarization using LDA Topic Modelling. (arXiv:2207.14687v7 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2207.14687](http://arxiv.org/abs/2207.14687)

    本研究使用LDA主题建模方法进行文本摘要，针对与基因和疾病相关的医学科学期刊文章进行研究，提供了一个能够保留关键信息并保持原始意义的压缩版本，并使用PyLDAvis进行交互式可视化。

    

    随着现代时代大数据挖掘和大文本分析的到来和普及，自动文本摘要成为从文档中提取和检索重要信息的重要手段。本研究从单个和多个文档的角度探讨了自动文本摘要的各个方面。摘要是将庞大的文本文章压缩为短小汇总版本的任务。目的是缩小文本的大小，但要保留关键重要信息并保持原始文档的意义。“潜在狄利克雷分配”（LDA）方法被用于从与基因和疾病相关的医学科学期刊文章中进行主题建模。在本研究中，使用PyLDAvis的基于网页的交互式可视化工具来可视化所选主题。这种可视化提供了主要主题的总体视图，同时允许深入理解个体的普遍性。

    With the advent and popularity of big data mining and huge text analysis in modern times, automated text summarization became prominent for extracting and retrieving important information from documents. This research investigates aspects of automatic text summarization from the perspectives of single and multiple documents. Summarization is a task of condensing huge text articles into short, summarized versions. The text is reduced in size for summarization purpose but preserving key vital information and retaining the meaning of the original document. This study presents the Latent Dirichlet Allocation (LDA) approach used to perform topic modelling from summarised medical science journal articles with topics related to genes and diseases. In this study, PyLDAvis web-based interactive visualization tool was used to visualise the selected topics. The visualisation provides an overarching view of the main topics while allowing and attributing deep meaning to the prevalence individual to
    
[^267]: 使用智能手机时间序列数据预测帕金森病患者远程用药状态

    Remote Medication Status Prediction for Individuals with Parkinson's Disease using Time-series Data from Smartphones. (arXiv:2207.13700v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.13700](http://arxiv.org/abs/2207.13700)

    本研究提出了一种使用智能手机时间序列数据预测帕金森病患者药物治疗状态的方法，通过考察病人个体的历史记录、使用 Transformer 模型学习注意权重，实现了较好的客观预测结果，为个性化远程健康传感提供了一种创新方法。

    

    神经系统疾病的药物治疗通常是远程进行的，远离医院的环境对及时准确地收集健康状态数据构成了挑战。穿戴式传感器收集的行为信号的个体差异也导致采用当前的通用机器学习分析流程存在困难。为了应对这些挑战，本研究提出一种使用公共 mPower 数据集的方法，该数据集包含来自 487 名患者的智能手机收集的62,182个远程多模式测试记录，用于预测帕金森病患者的用药状态。提出的方法通过考察病人个体的历史记录，并通过 Transformer 模型学习注意权重，有望在客观上预测三种不同的用药状态：用药前（AUC = 0.95）、用药后（AUC = 0.958）和其他时刻（AUC = 0.976）。本方法为个性化远程健康传感提供了一种创新方法。

    Medication for neurological diseases such as the Parkinson's disease usually happens remotely away from hospitals. Such out-of-lab environments pose challenges in collecting timely and accurate health status data. Individual differences in behavioral signals collected from wearable sensors also lead to difficulties in adopting current general machine learning analysis pipelines. To address these challenges, we present a method for predicting the medication status of Parkinson's disease patients using the public mPower dataset, which contains 62,182 remote multi-modal test records collected on smartphones from 487 patients. The proposed method shows promising results in predicting three medication statuses objectively: Before Medication (AUC=0.95), After Medication (AUC=0.958), and Another Time (AUC=0.976) by examining patient-wise historical records with the attention weights learned through a Transformer model. Our method provides an innovative way for personalized remote health sensi
    
[^268]: 当公平遇到隐私：半隐私敏感属性下的公平分类

    When Fairness Meets Privacy: Fair Classification with Semi-Private Sensitive Attributes. (arXiv:2207.08336v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.08336](http://arxiv.org/abs/2207.08336)

    该论文研究在半隐私场景下如何通过本地差分隐私（LDP）实现公平分类，解决了在收集大规模用户敏感属性时的难题。

    

    机器学习模型在许多领域已经展示出良好的性能。然而，对特定人口群体的偏见可能会阻碍它们在高风险应用中的采用。因此，在机器学习模型中确保公平性是至关重要的。大多数先前的努力需要直接访问敏感属性以减轻偏见。然而，考虑到用户对数据收集过程中隐私的担忧，通常难以获取大规模用户的敏感属性。由于法律合规性和人们对隐私的日益关注，隐私机制例如本地差分隐私（LDP）被广泛强制执行于敏感信息的数据收集阶段。因此，一个关键的问题是如何在隐私保护下进行公平预测。我们研究了一个新颖且实用的半隐私场景下的公平分类问题，其中大部分敏感属性是私有的，只有一小部分是可用的敏感属性是干净的。

    Machine learning models have demonstrated promising performance in many areas. However, the concerns that they can be biased against specific demographic groups hinder their adoption in high-stake applications. Thus, it is essential to ensure fairness in machine learning models. Most previous efforts require direct access to sensitive attributes for mitigating bias. Nonetheless, it is often infeasible to obtain large-scale users' sensitive attributes considering users' concerns about privacy in the data collection process. Privacy mechanisms such as local differential privacy (LDP) are widely enforced on sensitive information in the data collection stage due to legal compliance and people's increasing awareness of privacy. Therefore, a critical problem is how to make fair predictions under privacy. We study a novel and practical problem of fair classification in a semi-private setting, where most of the sensitive attributes are private and only a small amount of clean ones are availabl
    
[^269]: 可视化审计：透明方法难以检测异常行为。

    Auditing Visualizations: Transparency Methods Struggle to Detect Anomalous Behavior. (arXiv:2206.13498v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.13498](http://arxiv.org/abs/2206.13498)

    本研究评估了可视化方法检测模型异常行为的能力，发现现有方法难以识别微妙的异常行为，并且无法识别导致异常行为的输入。因此，需要开发更可靠的模型透明度方法。

    

    模型可视化提供了仅有输出可能会忽略的信息。但我们能相信模型可视化反映了模型行为吗？例如，它们能否诊断出种植的后门或过度正则化等异常行为？为了评估可视化方法，我们测试了它们是否将不正常训练的模型和正常模型分配给不同的可视化。我们发现，虽然现有的方法可以检测到明显异常行为的模型，但它们很难识别更微妙的异常。此外，它们经常无法识别导致异常行为的输入，例如包含虚假提示的图像。这些结果揭示了一些流行模型可视化的盲点和局限性。通过引入一种新的可视化评估框架，我们的工作为未来开发更可靠的模型透明度方法铺平了道路。

    Model visualizations provide information that outputs alone might miss. But can we trust that model visualizations reflect model behavior? For instance, can they diagnose abnormal behavior such as planted backdoors or overregularization? To evaluate visualization methods, we test whether they assign different visualizations to anomalously trained models and normal models. We find that while existing methods can detect models with starkly anomalous behavior, they struggle to identify more subtle anomalies. Moreover, they often fail to recognize the inputs that induce anomalous behavior, e.g. images containing a spurious cue. These results reveal blind spots and limitations of some popular model visualizations. By introducing a novel evaluation framework for visualizations, our work paves the way for developing more reliable model transparency methods in the future.
    
[^270]: GNN在推广带限函数方面的优越性比NN更加明显

    Superiority of GNN over NN in generalizing bandlimited functions. (arXiv:2206.05904v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.05904](http://arxiv.org/abs/2206.05904)

    本文研究了GNN在节点分类中插值带限函数的表达能力，结果表明，使用GNN结构以相同的精度插值带限函数所需的权重比使用完全连接的神经网络（NN）少得多。

    

    图神经网络（GNN）以其整合图形信息的能力被广泛用于数据分析。然而，GNN的表达能力仅针对图级任务进行了研究，而不是针对节点级任务，例如节点分类，其中试图从观察到的节点标签中插值出缺失的标签信息。本文研究了GNN在所述分类任务中的表达能力，它实质上是一个函数插值问题。具体而言，我们导出了GNN插值$\mathbb{R}^d$中带限函数所需的权重和层数。我们的结果显示，使用GNN架构以$\epsilon$-近似离散带限信号仅需要$O((\log \epsilon^{-1})^{d})$个权重，这比使用完全连接的神经网络（NN）得到的最佳结果的所需权重少得多 - 特别地，使用使用$O((\log \epsilon^{-1})^{d})$个样本来训练GNN以$\epsilon$-逼近带限函数。

    Graph Neural Network (GNN) with its ability to integrate graph information has been widely used for data analyses. However, the expressive power of GNN has only been studied for graph-level tasks but not for node-level tasks, such as node classification, where one tries to interpolate missing nodal labels from the observed ones. In this paper, we study the expressive power of GNN for the said classification task, which is in essence a function interpolation problem. Explicitly, we derive the number of weights and layers needed for a GNN to interpolate a band-limited function in $\mathbb{R}^d$. Our result shows that, the number of weights needed to $\epsilon$-approximate a bandlimited function using the GNN architecture is much fewer than the best known one using a fully connected neural network (NN) - in particular, one only needs $O((\log \epsilon^{-1})^{d})$ weights using a GNN trained by $O((\log \epsilon^{-1})^{d})$ samples to $\epsilon$-approximate a discretized bandlimited signal
    
[^271]: 何时适应性有助于量子状态学习?

    When Does Adaptivity Help for Quantum State Learning?. (arXiv:2206.05265v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2206.05265](http://arxiv.org/abs/2206.05265)

    本文研究了量子态学习中的适应性问题，显示出任何使用非相干测量的协议，即使是自适应选择的，都需要Ω(d3/ε2)个复制品的量。

    

    我们考虑关于“态重构”的经典问题：在知道一个未知量子态$\rho\in\mathbb{C^{d x d}}$的情况下，给出一个近似的$\widehat{\rho}$，使得在某种意义下，例如迹距离或保真度，它接近于$\rho$。当允许进行纠缠的相干测量时，需要$\Theta(d^2/\epsilon^2)$个复制体才能获得迹距离$\epsilon$的近似。不幸的是，实现这种速率的协议需要大量的量子内存开销，因此不能在近期设备上实现。另一方面，使用非相干（单个复制）测量的协议所需的复制品数量为$O(d^3/\epsilon^2)$，多篇论文提出了如何理解这个速率是上限还是下限的问题。在这项工作中，通过展示任何使用非相干测量的协议，即使是自适应选择的，都需要$\Omega(d^3/\epsilon^2)$个复制品的量，从而匹配已知的最好的上限，我们完全解决了这个问题。通过一种新的P...

    We consider the classic question of state tomography: given copies of an unknown quantum state $\rho\in\mathbb{C}^{d\times d}$, output $\widehat{\rho}$ which is close to $\rho$ in some sense, e.g. trace distance or fidelity. When one is allowed to make coherent measurements entangled across all copies, $\Theta(d^2/\epsilon^2)$ copies are necessary and sufficient to get trace distance $\epsilon$. Unfortunately, the protocols achieving this rate incur large quantum memory overheads that preclude implementation on near-term devices. On the other hand, the best known protocol using incoherent (single-copy) measurements uses $O(d^3/\epsilon^2)$ copies, and multiple papers have posed it as an open question to understand whether or not this rate is tight. In this work, we fully resolve this question, by showing that any protocol using incoherent measurements, even if they are chosen adaptively, requires $\Omega(d^3/\epsilon^2)$ copies, matching the best known upper bound.  We do so by a new p
    
[^272]: 离散观测扩散过程的计算Doob h变换在线滤波

    Computational Doob's h-transforms for Online Filtering of Discretely Observed Diffusions. (arXiv:2206.03369v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.03369](http://arxiv.org/abs/2206.03369)

    本文提出了一种计算Doob h变换的计算框架，用于离散观测非线性扩散过程的在线滤波。实验证明，该方法在高度信息化、观测值在模型下极端或状态维数较大时比最先进的粒子滤波器高几个数量级的效率。

    

    本文关注的是离散观测非线性扩散过程的在线滤波。我们的方法基于完全适应的辅助粒子滤波器，其中包括通常难以处理的Doob h变换。我们提出了一个计算框架，通过使用非线性Feynman-Kac公式和神经网络求解潜在的反向Kolmogorov方程来近似这些h变换。该方法允许在数据同化过程之前训练局部最优的粒子滤波器。数值实验表明，当观测值高度信息化，观测值在模型下极端，或状态维数较大时，所提出的方法比最先进的粒子滤波器高几个数量级的效率。

    This paper is concerned with online filtering of discretely observed nonlinear diffusion processes. Our approach is based on the fully adapted auxiliary particle filter, which involves Doob's $h$-transforms that are typically intractable. We propose a computational framework to approximate these $h$-transforms by solving the underlying backward Kolmogorov equations using nonlinear Feynman-Kac formulas and neural networks. The methodology allows one to train a locally optimal particle filter prior to the data-assimilation procedure. Numerical experiments illustrate that the proposed approach can be orders of magnitude more efficient than state-of-the-art particle filters in the regime of highly informative observations, when the observations are extreme under the model, or if the state dimension is large.
    
[^273]: 捕捉局部不变性的学习实例特定增强方法

    Learning Instance-Specific Augmentations by Capturing Local Invariances. (arXiv:2206.00051v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.00051](http://arxiv.org/abs/2206.00051)

    InstaAug是一种能够自动学习输入特定增强的方法，通过引入可学习的不变性模块从输入中映射到量身定制的变换参数，捕捉局部不变性，从而提高了监督和自我监督任务的性能。

    

    我们介绍了InstaAug，一种从数据中自动学习特定于输入的增强方法。以往的学习增强方法通常假设原始输入和应用于该输入的变换之间存在独立性。这可能会非常限制，因为我们希望我们的增强捕捉的不变性本身通常高度依赖于输入。InstaAug引入了可学习的不变性模块，从输入中映射到量身定制的变换参数，以捕捉局部不变性。这可以同时与下游模型完全端到端地训练，或者为预训练模型单独学习。我们通过实验证明，InstaAug学习了广泛的变换类别的有意义的输入相关增强，这进而提高了监督和自我监督任务的性能。

    We introduce InstaAug, a method for automatically learning input-specific augmentations from data. Previous methods for learning augmentations have typically assumed independence between the original input and the transformation applied to that input. This can be highly restrictive, as the invariances we hope our augmentation will capture are themselves often highly input dependent. InstaAug instead introduces a learnable invariance module that maps from inputs to tailored transformation parameters, allowing local invariances to be captured. This can be simultaneously trained alongside the downstream model in a fully end-to-end manner, or separately learned for a pre-trained model. We empirically demonstrate that InstaAug learns meaningful input-dependent augmentations for a wide range of transformation classes, which in turn provides better performance on both supervised and self-supervised tasks.
    
[^274]: BrainIB：基于图信息瓶颈的可解释性脑网络精神疾病诊断

    BrainIB: Interpretable Brain Network-based Psychiatric Diagnosis with Graph Information Bottleneck. (arXiv:2205.03612v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2205.03612](http://arxiv.org/abs/2205.03612)

    BrainIB是一种基于图信息瓶颈原理开发的图神经网络框架，在分析fMRI图像中的功能连接时能够识别最具信息量的边缘，具有良好的泛化能力和可解释性，适用于未见样本的场景。

    

    发展一种基于潜在生物机制而非主观症状对精神障碍进行诊断的新模型已成为一种新的共识。最近，使用功能连接（FC）进行精神障碍和健康对照的机器学习分类器被开发出来，用于确定大脑标记物。然而，现有的基于机器学习的诊断模型容易出现过拟合的情况（由于训练样本不足），在新的测试环境中表现差。此外，难以获得可解释的、可靠的大脑生物标记物来解释潜在的诊断决策。这些问题阻碍了其可能的临床应用。在本研究中，我们提出了BrainIB，一种新的图神经网络（GNN）框架，通过利用著名的信息瓶颈（IB）原理来分析功能磁共振图像（fMRI）。BrainIB能够识别大脑中最具信息量的边缘（即子图）并具有良好的泛化能力，适用于未见样本的场景。

    Developing a new diagnostic models based on the underlying biological mechanisms rather than subjective symptoms for psychiatric disorders is an emerging consensus. Recently, machine learning-based classifiers using functional connectivity (FC) for psychiatric disorders and healthy controls are developed to identify brain markers. However, existing machine learningbased diagnostic models are prone to over-fitting (due to insufficient training samples) and perform poorly in new test environment. Furthermore, it is difficult to obtain explainable and reliable brain biomarkers elucidating the underlying diagnostic decisions. These issues hinder their possible clinical applications. In this work, we propose BrainIB, a new graph neural network (GNN) framework to analyze functional magnetic resonance images (fMRI), by leveraging the famed Information Bottleneck (IB) principle. BrainIB is able to identify the most informative edges in the brain (i.e., subgraph) and generalizes well to unseen 
    
[^275]: 多臂老虎机用于语言模型预训练的资源高效、在线优化：动态遮盖的使用案例

    Multi-armed bandits for resource efficient, online optimization of language model pre-training: the use case of dynamic masking. (arXiv:2203.13151v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2203.13151](http://arxiv.org/abs/2203.13151)

    本文提出了一种多臂老虎机框架，用于顺序选择TLM预训练超参数，旨在以资源高效的方式优化语言模型性能。并设计了基于高斯过程的Thompson抽样（GP-TS）算法，加速Pre-training过程并降低MLM损失。

    

    我们设计并评估了一种贝叶斯优化框架，以资源高效的方式预训练基于Transformer的语言模型（TLM）。 TLM预训练需要高计算资源，并引入许多未解决的设计选择，例如选择其预训练超参数。 我们提出了一个多臂老虎机框架，用于顺序选择TLM预训练超参数，旨在以资源高效的方式优化语言模型性能。 我们设计了一个Thompson抽样算法，用于其顺序最小化的带有掩码语言模型（MLM）预训练目标的代理高斯过程奖励模型。 提出的基于高斯过程的Thompson抽样（GP-TS）不是使用固定掩码概率进行MLM预训练，而是通过顺序选择改善性能的掩码超参数来加速预训练。 我们通过实验证明了GP-TS如何高效进行语言模型的预训练，即在少量迭代中实现更低的MLM损失。

    We design and evaluate a Bayesian optimization framework for resource efficient pre-training of Transformer-based language models (TLMs). TLM pre-training requires high computational resources and introduces many unresolved design choices, such as selecting its pre-training hyperparameters. We propose a multi-armed bandit framework for the sequential selection of TLM pre-training hyperparameters, aimed at optimizing language model performance, in a resource efficient manner. We design a Thompson sampling algorithm, with a surrogate Gaussian process reward model of the Masked Language Model (MLM) pre-training objective, for its sequential minimization. Instead of MLM pre-training with fixed masking probabilities, the proposed Gaussian process-based Thompson sampling (GP-TS) accelerates pre-training by sequentially selecting masking hyperparameters that improve performance. We empirically demonstrate how GP-TS pre-trains language models efficiently, i.e., it achieves lower MLM loss in fe
    
[^276]: 有下层压缩的双层优化: 无warm-start情况下最优样本复杂度分析

    Bilevel Optimization with a Lower-level Contraction: Optimal Sample Complexity without Warm-Start. (arXiv:2202.03397v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2202.03397](http://arxiv.org/abs/2202.03397)

    本文针对一类双层问题，提出了无需warm-start也可实现最优样本复杂度的方法。

    

    本文分析了一类一般的双层问题，其中上层问题是将一光滑目标函数最小化，下层问题是寻找一光滑收缩映射的不动点。这类问题包括元学习、均衡模型、超参数优化和数据污染对抗攻击的实例。我们展示了，即使没有warm-start，在某些情况下，如元学习和均衡模型，仍然可以实现顺序最优的样本复杂度。

    We analyse a general class of bilevel problems, in which the upper-level problem consists in the minimization of a smooth objective function and the lower-level problem is to find the fixed point of a smooth contraction map. This type of problems include instances of meta-learning, equilibrium models, hyperparameter optimization and data poisoning adversarial attacks. Several recent works have proposed algorithms which warm-start the lower-level problem, i.e. they use the previous lower-level approximate solution as a staring point for the lower-level solver. This warm-start procedure allows one to improve the sample complexity in both the stochastic and deterministic settings, achieving in some cases the order-wise optimal sample complexity. However, there are situations, e.g., meta learning and equilibrium models, in which the warm-start procedure is not well-suited or ineffective. In this work we show that without warm-start, it is still possible to achieve order-wise (near) optimal
    
[^277]: 高效直连拓扑结构用于集体通信

    Efficient Direct-Connect Topologies for Collective Communications. (arXiv:2202.03356v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2202.03356](http://arxiv.org/abs/2202.03356)

    本论文提出了一种算法框架，可以构建高效的直连网络拓扑结构，以优化节点延迟和带宽权衡，适用于集体通信负载。

    

    本文研究了如何构建适用于集体通信的高效网络拓扑结构。我们提出了一种算法框架，用于构建针对节点延迟与带宽权衡优化的直连拓扑结构。这个算法框架从小的基础拓扑结构和相关的通信进度开始，并使用一组可以迭代应用的技术来派生更大的拓扑结构。这些衍生的拓扑结构的时间表可以与扩展一起合成，也可以使用优化公式计算。我们的方法允许我们为给定的集群大小和度数合成许多不同的拓扑结构和时间表，然后为给定的工作负载确定适当的拓扑和时间表。我们在使用补丁面板配置所需拓扑结构的12节点光学实验平台上评估了我们的方法，并增加了基于分析模型的评估，用于更大的部署。

    We consider the problem of distilling efficient network topologies for collective communications. We provide an algorithmic framework for constructing direct-connect topologies optimized for the node latency vs bandwidth trade-off given a collective communication workload. Our algorithmic framework allows us to start from small base topologies and associated communication schedules and use a set of techniques that can be iteratively applied to derive much larger topologies. The schedules for these derived topologies are either synthesized along with the expansions or computed using an optimization formulation. Our approach allows us to synthesize many different topologies and schedules for a given cluster size and degree, and then identify the appropriate topology and schedule for a given workload. We evaluate our approach on a 12-node optical testbed that uses patch panels for configuring the desired topology and augment it with an analytical-model-based evaluation for larger deployme
    
[^278]: 混合成员无分布模型

    Mixed Membership Distribution-Free Model. (arXiv:2112.04389v4 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2112.04389](http://arxiv.org/abs/2112.04389)

    本文提出了一种混合成员无分布模型，用于重叠加权网络的社群检测，支持节点所属多个社群和有限实数权值。提出的模型可以推广到之前的模型，包括混合成员随机块模型，并支持具有潜在社群结构的重叠符号网络的生成。我们使用高效谱算法估计模型下的社群成员资格，并提出了模糊加权模块度来评估重叠加权网络的社群检测质量并确定加权网络社群数量。

    

    本文考虑在具有重叠加权网络中进行社群检测的问题，其中节点可以属于多个社群，边权可以是有限实数。为了对这样的复杂网络进行建模，我们提出了一个通用框架——混合成员无分布（MMDF）模型。MMDF没有对边权的分布约束，可以被视为一些先前模型的推广，包括著名的混合成员随机块模型。特别地，具有潜在社群结构的重叠符号网络也可以从我们的模型中生成。我们使用具有收敛率理论保证的高效谱算法来估计模型下的社群成员资格。我们还提出了模糊加权模块度来评估具有正负边权的重叠加权网络的社群检测质量。然后，我们提供了一种利用我们的fuzzy weighted modularity来确定加权网络社群数量的方法。

    We consider the problem of community detection in overlapping weighted networks, where nodes can belong to multiple communities and edge weights can be finite real numbers. To model such complex networks, we propose a general framework - the mixed membership distribution-free (MMDF) model. MMDF has no distribution constraints of edge weights and can be viewed as generalizations of some previous models, including the well-known mixed membership stochastic blockmodels. Especially, overlapping signed networks with latent community structures can also be generated from our model. We use an efficient spectral algorithm with a theoretical guarantee of convergence rate to estimate community memberships under the model. We also propose fuzzy weighted modularity to evaluate the quality of community detection for overlapping weighted networks with positive and negative edge weights. We then provide a method to determine the number of communities for weighted networks by taking advantage of our f
    
[^279]: 群等变神经后验估计

    Group equivariant neural posterior estimation. (arXiv:2111.13139v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.13139](http://arxiv.org/abs/2111.13139)

    本文提出了一种群等变神经后验估计（GNPE）算法，能够在参数和数据联合变换下整合等变性，用于从引力波观测中对双黑洞系统进行摊销推断。

    

    基于条件神经密度估计的仿真推理是解决科学领域反问题的强大方法。然而，这些方法通常将基础正向模型视为一个黑盒子，不会利用等变性等几何性质。等变性在科学模型中很常见，但将其直接整合到表达式推理网络（如归一化流）中并不简单。本文介绍了一种替代方法，可以在参数和数据的联合变换下整合等变性。我们的方法——称为群等变神经后验估计（GNPE）——基于自洽地标准化数据的“姿态”，同时估计参数后验。它是独立于体系结构的，并适用于精确和近似等变性。作为现实世界的应用，我们利用GNPE从引力波观测中对双黑洞系统进行了摊销推断。

    Simulation-based inference with conditional neural density estimators is a powerful approach to solving inverse problems in science. However, these methods typically treat the underlying forward model as a black box, with no way to exploit geometric properties such as equivariances. Equivariances are common in scientific models, however integrating them directly into expressive inference networks (such as normalizing flows) is not straightforward. We here describe an alternative method to incorporate equivariances under joint transformations of parameters and data. Our method -- called group equivariant neural posterior estimation (GNPE) -- is based on self-consistently standardizing the "pose" of the data while estimating the posterior over parameters. It is architecture-independent, and applies both to exact and approximate equivariances. As a real-world application, we use GNPE for amortized inference of astrophysical binary black hole systems from gravitational-wave observations. W
    
[^280]: AI-Enabled 移动应用中的哪些设计决策有助于更环保的 AI？

    Which Design Decisions in AI-enabled Mobile Applications Contribute to Greener AI?. (arXiv:2109.15284v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.15284](http://arxiv.org/abs/2109.15284)

    在移动设备上部署复杂 AI 模型需要平衡准确性和复杂性，设计决策对实现高准确度和低资源消耗有影响。

    

    背景：构建、发展和使用复杂的人工智能（AI）模型需要昂贵的计算资源。虽然当前可用的高性能计算环境支持这种复杂性，但将 AI 模型部署到移动设备上（这是一种不断增长的趋势）是具有挑战性的。移动应用程序包括计算资源缺乏的环境，因此意味着在 AI 启用软件工程生命周期中进行设计决策的时候需要平衡移动应用程序的准确性和复杂性的权衡。目标：我们的目标是系统地评估在移动设备上部署复杂的 AI 模型（如神经网络）时准确性和复杂性之间的权衡，它们具有隐式的资源限制。我们旨在涵盖（i）设计决策对实现高准确度和低资源消耗的影响；以及（ii）验证剖析的实用性和可行性，针对在低资源环境下的 AI。

    Background: The construction, evolution and usage of complex artificial intelligence (AI) models demand expensive computational resources. While currently available high-performance computing environments support well this complexity, the deployment of AI models in mobile devices, which is an increasing trend, is challenging. Mobile applications consist of environments with low computational resources and hence imply limitations in the design decisions during the AI-enabled software engineering lifecycle that balance the trade-off between the accuracy and the complexity of the mobile applications.  Objective: Our objective is to systematically assess the trade-off between accuracy and complexity when deploying complex AI models (e.g. neural networks) to mobile devices, which have an implicit resource limitation. We aim to cover (i) the impact of the design decisions on the achievement of high-accuracy and low resource-consumption implementations; and (ii) the validation of profiling to
    
[^281]: 优化赌博算法的脆弱性

    The Fragility of Optimized Bandit Algorithms. (arXiv:2109.13595v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.13595](http://arxiv.org/abs/2109.13595)

    本文表明，使用优化设计的赌博算法遗憾分布具有非常重的尾部，对于$p>1$，遗憾分布的$p$'th矩增长要比多对数级别快得多，当问题略微错误时，优化UCB赌博设计的遗憾可以比传统理论建议的增长得更快。

    

    大部分关于赌博算法最优设计的文献都是基于期望遗憾的最小化。已知对于某些指数族，最优设计在拉依-罗宾斯下界指导下，可实现期望遗憾以对数级别增长。本文表明，当使用这种最优设计时，关联算法的遗憾分布必然具有一个非常重的尾部，具体来说是截断柯西分布。此外，对于$p>1$，遗憾分布的$p$'th矩增长要比多对数级别快得多，特别是作为衣袖数的幂函数。我们表明，优化UCB赌博设计在另一个方面也很脆弱，即当问题略微错误时，遗憾可以比传统理论建议的增长得更快。我们的论点基于标准的措施改变思想，并表明最优设计可能导致一些不太可能发生的情况，因此应该以谨慎的方式对待。

    Much of the literature on optimal design of bandit algorithms is based on minimization of expected regret. It is well known that designs that are optimal over certain exponential families can achieve expected regret that grows logarithmically in the number of arm plays, at a rate governed by the Lai-Robbins lower bound. In this paper, we show that when one uses such optimized designs, the regret distribution of the associated algorithms necessarily has a very heavy tail, specifically, that of a truncated Cauchy distribution. Furthermore, for $p>1$, the $p$'th moment of the regret distribution grows much faster than poly-logarithmically, in particular as a power of the total number of arm plays. We show that optimized UCB bandit designs are also fragile in an additional sense, namely when the problem is even slightly mis-specified, the regret can grow much faster than the conventional theory suggests. Our arguments are based on standard change-of-measure ideas, and indicate that the mos
    
[^282]: 加权二分网络的社区发现

    Community detection for weighted bipartite networks. (arXiv:2109.10319v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2109.10319](http://arxiv.org/abs/2109.10319)

    本文提出了一种名为Bipartite Distribution-Free的模型，可用于建模和探测加权二分网络的社区结构，该模型考虑了节点度数的变化以及期望的块结构。同时，我们提出了谱算法用于识别社区。

    

    二分网络出现在多个领域，例如生物学、社会学、生理学和计算机科学中。过去的研究提出了随机共同块模型（ScBM）来检测二分图数据的社区结构，但是ScBM完全忽略边权并且无法解释加权二分网络的块结构。为了解决这个问题，我们通过放宽ScBM的分布约束，提出了一个名为Bipartite Distribution-Free的模型来建模加权二分网络，并考虑节点度数的变化来构建模型扩展。我们的模型不需要在邻接矩阵的生成元上指定特定的分布，而只需要在期望邻接矩阵上指定块结构。本文中，我们提出了具有理论保证的谱算法，来识别社区。通过模拟和实证例子来展示我们所提出的模型和算法。

    The bipartite network appears in various areas, such as biology, sociology, physiology, and computer science. \cite{rohe2016co} proposed Stochastic co-Blockmodel (ScBM) as a tool for detecting community structure of binary bipartite graph data in network studies. However, ScBM completely ignores edge weight and is unable to explain the block structure of a weighted bipartite network. Here, to model a weighted bipartite network, we introduce a Bipartite Distribution-Free model by releasing ScBM's distribution restriction. We also build an extension of the proposed model by considering the variation of node degree. Our models do not require a specific distribution on generating elements of the adjacency matrix but only a block structure on the expected adjacency matrix. Spectral algorithms with theoretical guarantees on the consistent estimation of node labels are presented to identify communities. Our proposed methods are illustrated by simulated and empirical examples.
    
[^283]: 我们应该转移哪种不变性？一种因果极小化学习方法

    Which Invariance Should We Transfer? A Causal Minimax Learning Approach. (arXiv:2107.01876v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2107.01876](http://arxiv.org/abs/2107.01876)

    该论文从因果的角度提出了一种全面的极小化分析，旨在回答机器学习模型在转移稳定信息时应该转移哪个子集从而达到最佳的泛化能力这一问题

    

    当前机器学习模型无法可靠应对数据集变化，因此大多数现有研究试图将稳定信息转移到看不见的环境中。特别地，基于独立因果机制的方法通过do-operator消除可变的因果机制。与之前的方法相比，所得到的稳定预测因为能够更有效地识别稳定信息而更加有效。然而，一个关键问题仍然存在：为了达到最佳的泛化能力，应该转移这整个稳定信息中的哪个子集？为了回答这个问题，我们从因果的角度提出了一种全面的极小化分析。具体来说，我们首先提供了一个用于判断整个稳定集是否最优的图形条件。当这个条件失败时，我们惊讶地发现，通过一个例子，这个整个稳定集虽然能够充分利用稳定信息，但并不是最优的转移集。为了确定最优集，我们提出了因果最小含义的方法，并给出了仿真和实际数据中的实验结果。

    A major barrier to deploying current machine learning models lies in their non-reliability to dataset shifts. To resolve this problem, most existing studies attempted to transfer stable information to unseen environments. Particularly, independent causal mechanisms-based methods proposed to remove mutable causal mechanisms via the do-operator. Compared to previous methods, the obtained stable predictors are more effective in identifying stable information. However, a key question remains: which subset of this whole stable information should the model transfer, in order to achieve optimal generalization ability? To answer this question, we present a comprehensive minimax analysis from a causal perspective. Specifically, we first provide a graphical condition for the whole stable set to be optimal. When this condition fails, we surprisingly find with an example that this whole stable set, although can fully exploit stable information, is not the optimal one to transfer. To identify the o
    
[^284]: 用神经后验估计进行实时引力波科学

    Real-time gravitational-wave science with neural posterior estimation. (arXiv:2106.12594v2 [gr-qc] UPDATED)

    [http://arxiv.org/abs/2106.12594](http://arxiv.org/abs/2106.12594)

    使用神经网络作为贝叶斯后验分布的替代品，提出的算法 "DINGO" 能够实现对检测到的引力波事件物理参数的快速准确推断，可在不损失精度的情况下实现实时数据分析。

    

    我们展示了使用深度学习进行引力波参数评估的前所未有的准确性。使用神经网络作为贝叶斯后验分布的替代品，我们分析了第一个LIGO-Virgo引力波瞬变目录中的8个引力波事件，并发现与标准推断代码非常接近，但推断时间从 O(day) 降低到每个事件的一分钟。我们使用模拟数据训练神经网络，包括事件附近的探测器噪声特征估计。这样可以将信号和噪声模型编码在数百万个神经网络参数中，并能够针对与训练分布一致的任何观测数据进行推断，考虑从事件到事件的噪声非稳态性。我们的算法 -- 名为 "DINGO" -- 设定了检测到的引力波事件物理参数快速准确推断的新标准，这应该使实时数据分析成为可能，而不损失精度。

    We demonstrate unprecedented accuracy for rapid gravitational-wave parameter estimation with deep learning. Using neural networks as surrogates for Bayesian posterior distributions, we analyze eight gravitational-wave events from the first LIGO-Virgo Gravitational-Wave Transient Catalog and find very close quantitative agreement with standard inference codes, but with inference times reduced from O(day) to a minute per event. Our networks are trained using simulated data, including an estimate of the detector-noise characteristics near the event. This encodes the signal and noise models within millions of neural-network parameters, and enables inference for any observed data consistent with the training distribution, accounting for noise nonstationarity from event to event. Our algorithm -- called "DINGO" -- sets a new standard in fast-and-accurate inference of physical parameters of detected gravitational-wave events, which should enable real-time data analysis without sacrificing acc
    
[^285]: 无需信任的私有联邦学习：凸损失函数的最优算法

    Private Federated Learning Without a Trusted Server: Optimal Algorithms for Convex Losses. (arXiv:2106.09779v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.09779](http://arxiv.org/abs/2106.09779)

    本文研究了无需信任服务器或其他数据源的跨 silo 联邦学习，考虑了跨 silo 记录级差分隐私 ISRL-DP。该算法可以确保来自每个人的数据都不会被泄漏。

    

    本文探讨了联邦学习（FL）的研究，特别是跨数据源（跨 silo）FL，这些数据源的数据主人都不信任服务器或其他 silos。在这种情况下，每个数据源（例如医院）都有来自不同人（例如患者）的数据，并且必须维护每个人（例如医疗记录）数据的隐私，即使服务器或其他数据源是恶意监听者。这种要求促进了对跨 silo 记录级差分隐私（ISRL-DP）的研究，它要求 silo i 的通信满足记录 / 项目级差分隐私 (DP)。ISRL-DP 确保 silo i 中每个人（例如患者）的数据都不会泄漏。ISRL-DP 不同于各种已有的隐私概念。中心和用户级差分隐私假定人们信任服务器/其他数据源。在极端情况下，本地DP 假定人们根本不信任任何人（甚至是他们自己的数据源）。ISRL-DP 处于中心和本地DP 之间，使得在跨 silo 的真实情况下具有现实意义。

    This paper studies federated learning (FL)--especially cross-silo FL--with data from people who do not trust the server or other silos. In this setting, each silo (e.g. hospital) has data from different people (e.g. patients) and must maintain the privacy of each person's data (e.g. medical record), even if the server or other silos act as adversarial eavesdroppers. This requirement motivates the study of Inter-Silo Record-Level Differential Privacy (ISRL-DP), which requires silo i's communications to satisfy record/item-level differential privacy (DP). ISRL-DP ensures that the data of each person (e.g. patient) in silo i (e.g. hospital i) cannot be leaked. ISRL-DP is different from well-studied privacy notions. Central and user-level DP assume that people trust the server/other silos. On the other end of the spectrum, local DP assumes that people do not trust anyone at all (even their own silo). Sitting between central and local DP, ISRL-DP makes the realistic assumption (in cross-sil
    
[^286]: 论DP-SGD的Moment Accountant方法的紧密性

    On the Tightness of the Moment Accountant for DP-SGD. (arXiv:2102.09030v8 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2102.09030](http://arxiv.org/abs/2102.09030)

    通过改进Moment Accountant方法，DP-SGD具有可关闭形式的$(\epsilon，\delta)$-DP保证，并且其保证接近是紧密的，具有最小的计算成本。

    

    为了提供差分隐私，在差分隐私SGD（DP-SGD）中，在执行剪切操作后，向本地SGD更新添加标准差为$ \sigma $的高斯噪声。通过非平凡地改进Moment Accountant方法，我们证明了一个封闭形式的$(\epsilon，\delta)$-DP保证：如果$ \sigma=\sqrt{ 2(\epsilon+\ln(1/\delta))/\epsilon} $，则DP-SGD是$ (\epsilon \leq 1/2，\delta = 1 / N) $-DP，其中$T$至少为$ \approx 2k^2/\epsilon$， $(2/e)^2k^2-1/2\geq \ln(N)$，其中$T$是回合的总数，$ K = kN $是梯度计算的总数，其中$ k $用数据集的大小$N$的时代数量来衡量。我们证明我们的表达式接近紧，在$T$小于约为$ 8 $倍于下界$ \approx 2k^2/\epsilon$的常数因子时，$(\epsilon，\delta)$-DP保证将被违反。选择最小可能值的$T \approx 2k^2/\epsilon$不仅会导致接近密集的DP保证，而且还会最小化计算成本。

    In order to provide differential privacy, Gaussian noise with standard deviation $\sigma$ is added to local SGD updates after performing a clipping operation in Differential Private SGD (DP-SGD). By non-trivially improving the moment account method we prove a closed form $(\epsilon,\delta)$-DP guarantee: DP-SGD is $(\epsilon\leq 1/2,\delta=1/N)$-DP if $\sigma=\sqrt{2(\epsilon +\ln(1/\delta))/\epsilon}$ with $T$ at least $\approx 2k^2/\epsilon$ and $(2/e)^2k^2-1/2\geq \ln(N)$, where $T$ is the total number of rounds, and $K=kN$ is the total number of gradient computations where $k$ measures $K$ in number of epochs of size $N$ of the local data set. We prove that our expression is close to tight in that if $T$ is more than a constant factor $\approx 8$ smaller than the lower bound $\approx 2k^2/\epsilon$, then the $(\epsilon,\delta)$-DP guarantee is violated. Choosing the smallest possible value $T\approx 2k^2/\epsilon$ not only leads to a close to tight DP guarantee, but also minimizes 
    
[^287]: 浅层神经网络带限制的随机权重有多大的能力？

    How Powerful are Shallow Neural Networks with Bandlimited Random Weights?. (arXiv:2008.08427v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2008.08427](http://arxiv.org/abs/2008.08427)

    本文研究了深度为2的带限制随机神经网络的表达能力，通过数学证明确定了当隐藏参数分布于有界域时，网络可能无法达到零逼近误差。

    

    本文探讨了深度为2的带限制随机神经网络的表达能力。随机网络是指隐藏层参数被冻结并赋予随机分配的神经网络，只有输出层参数通过损失最小化进行训练。使用随机权重的隐藏层是避免标准梯度下降学习中的非凸优化的有效方法，并已被近期深度学习理论所采用。尽管神经网络是普适逼近器的众所周知的事实，在这项研究中，我们数学上证明了当隐藏参数分布于有界域时，网络可能无法达到零逼近误差。我们特别导出了一个新的非平凡逼近误差下界。证明利用了Ridgelet分析技术，这是一种为神经网络设计的谐波分析方法。这种方法受到了经典信号处理中的基本原理的启发，特别是信号在某种限制下的采样。

    We investigate the expressive power of depth-2 bandlimited random neural networks. A random net is a neural network where the hidden layer parameters are frozen with random assignment, and only the output layer parameters are trained by loss minimization. Using random weights for a hidden layer is an effective method to avoid non-convex optimization in standard gradient descent learning. It has also been adopted in recent deep learning theories. Despite the well-known fact that a neural network is a universal approximator, in this study, we mathematically show that when hidden parameters are distributed in a bounded domain, the network may not achieve zero approximation error. In particular, we derive a new nontrivial approximation error lower bound. The proof utilizes the technique of ridgelet analysis, a harmonic analysis method designed for neural networks. This method is inspired by fundamental principles in classical signal processing, specifically the idea that signals with limit
    
[^288]: 用射线进行最近邻点集抽样

    Nearest Neighbor Sampling of Point Sets using Rays. (arXiv:1911.10737v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/1911.10737](http://arxiv.org/abs/1911.10737)

    本论文提出了一种新的框架用于最近邻的点集抽样，所涉及的RaySense草图可以捕捉点的基本几何形态以及提取与之相关的统计信息，且可高效地进行点集上的线积分计算。

    

    我们提出了一个新的框架，用于抽样、压缩和分析欧几里得空间中嵌入的点集和其他几何对象的分布。我们的方法涉及构建一种称为RaySense草图的张量，该张量捕捉沿着一组射线的点的基本几何形态的最近邻居。我们探讨了可以在RaySense草图上执行的各种操作，从而导致不同的属性和潜在的应用。可以在不考虑射线集的情况下从草图中提取有关数据集的统计信息。使用草图可以高效地计算点集上的线积分。我们还提供了几个示例，说明了所提出策略在实际情况下的应用。

    We propose a new framework for the sampling, compression, and analysis of distributions of point sets and other geometric objects embedded in Euclidean spaces. Our approach involves the construction of a tensor called the RaySense sketch, which captures the nearest neighbors from the underlying geometry of points along a set of rays. We explore various operations that can be performed on the RaySense sketch, leading to different properties and potential applications. Statistical information about the data set can be extracted from the sketch, independent of the ray set. Line integrals on point sets can be efficiently computed using the sketch. We also present several examples illustrating applications of the proposed strategy in practical scenarios.
    

