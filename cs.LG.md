# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Decision Support Framework for Home Health Caregiver Allocation: A Case Study of HHC Agency in Tennessee, USA.](http://arxiv.org/abs/2311.00696) | 本研究针对家庭保健机构面临的护理员分配问题，提出了一种决策支持框架，通过考虑护理员对访问顺序的灵活性，旨在减少行驶里程、增加访问次数，并保持护理的连续性。 |
| [^2] | [Software Repositories and Machine Learning Research in Cyber Security.](http://arxiv.org/abs/2311.00691) | 该论文研究了网络安全领域中软件存储库和机器学习的应用。通过使用网络安全存储库以及主题建模和机器学习技术，可以在软件开发的早期阶段识别出关键的网络安全漏洞，为软件开发人员提供自动化的漏洞识别方法。 |
| [^3] | [What User Behaviors Make the Differences During the Process of Visual Analytics?.](http://arxiv.org/abs/2311.00690) | 本文通过研究用户行为的数据采集和时间序列分类方法分析了视觉分析过程中的用户行为差异，揭示了用户行为的不同特征。 |
| [^4] | [Improving Interpersonal Communication by Simulating Audiences with Language Models.](http://arxiv.org/abs/2311.00687) | 本论文提出了一个基于大型语言模型（LLM）模拟的框架，通过探索解决方案空间、生成沟通候选以及模拟受众反应，来改善人际沟通。通过评估八个涵盖人际沟通基本过程的场景，展示了该框架的有效性。 |
| [^5] | [Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation.](http://arxiv.org/abs/2311.00684) | 本研究提出了两种通过温度缩放实现的注意力对齐策略，通过改善T5在长序列处理中的注意力分布问题，提高了其在语言建模、检索和多文档问答等任务中的长上下文利用能力。 |
| [^6] | [Deep Learning-Based Classification of Gamma Photon Interactions in Room-Temperature Semiconductor Radiation Detectors.](http://arxiv.org/abs/2311.00682) | 本论文介绍了一种名为CoPhNet的深度学习分类器，用于区分CdZnTeSe半导体探测器中的康普顿散射和光电相互作用的伽玛/ x射线光子。 |
| [^7] | [Complexity of Single Loop Algorithms for Nonlinear Programming with Stochastic Objective and Constraints.](http://arxiv.org/abs/2311.00678) | 该论文分析了解决具有随机目标和约束的非线性规划问题中单循环二次罚函数和增广Lagrangian算法的复杂性，研究了三种具有不同约束性质的情况。结果表明其中的两种情况是首个采用单循环算法并具有一定复杂度的方法。 |
| [^8] | [Last-Iterate Convergence Properties of Regret-Matching Algorithms in Games.](http://arxiv.org/abs/2311.00676) | 这篇论文研究了基于遗憾匹配的算法在游戏中的最终迭代收敛性质。通过数值实验发现多个实际变体在简单的游戏中缺乏最终迭代收敛保证，而基于平滑技术的最近变体则具有最终迭代收敛性。 |
| [^9] | [Recovering Linear Causal Models with Latent Variables via Cholesky Factorization of Covariance Matrix.](http://arxiv.org/abs/2311.00674) | 本文提出了一种通过Cholesky分解恢复具有潜变量的线性因果模型的方法，该方法在速度和性能上超过了之前的方法，并具有精确恢复的理论保证。 |
| [^10] | [Latent Space Translation via Semantic Alignment.](http://arxiv.org/abs/2311.00664) | 本论文研究了潜在空间的翻译问题。通过简单的变换，可以将不同神经模型学到的表示翻译到其他预训练网络中。这种方法能够有效地拼接编码器和解码器，并在各种实验设置中得到验证。 |
| [^11] | [Online Signal Estimation on the Graph Edges via Line Graph Transformation.](http://arxiv.org/abs/2311.00656) | 该论文提出了一种在线时间变化图边缘信号预测算法，利用线图转换边缘信号为边到顶点对偶节点，使得信号可以使用已有的GSP概念进行处理。 |
| [^12] | [FAIRLABEL: Correcting Bias in Labels.](http://arxiv.org/abs/2311.00638) | FAIRLABEL是一种检测和纠正标签中偏见的算法，其目标是在保持高准确率的同时减少群体间的不平等影响。通过应用于合成数据集和基准数据集，验证结果显示FAIRLABEL在标签修正方面的正确率较基准模型提高了14.8%, 在不平等影响比率方面达到了54.2%的增长。 |
| [^13] | [Kronecker-Factored Approximate Curvature for Modern Neural Network Architectures.](http://arxiv.org/abs/2311.00636) | 本篇论文提出了一种用于现代神经网络架构的克罗内克近似曲率算法，可以加速神经网络训练和减少计算成本。作者发现了两种具有线性权重共享层不同设置，并证明了相应设置下的K-FAC算法的精确性。K-FAC-reduce通常比K-FAC-expand更快，可以用于加速自动超参数选择。 |
| [^14] | [Loss Modeling for Multi-Annotator Datasets.](http://arxiv.org/abs/2311.00619) | 该论文提出了一种通过利用多任务学习和基于损失的标签修正来学习多注释者数据的准确表示的方法。通过这种方法，可以有效地分离赞同和不赞同的注释，并且在单一或多注释者设置下改善预测性能。该方法还显示出对主观数据的额外标签噪声具有鲁棒性。 |
| [^15] | [Controllable Music Production with Diffusion Models and Guidance Gradients.](http://arxiv.org/abs/2311.00613) | 本论文介绍了一种使用扩散模型和导向梯度的方法，可以实现可控音乐制作，包括音频的延续、修补和再生，以及风格特征转移等任务。 |
| [^16] | [A Collaborative Filtering-Based Two Stage Model with Item Dependency for Course Recommendation.](http://arxiv.org/abs/2311.00612) | 本论文提出了一种基于协同过滤和课程依赖性的两阶段模型的课程推荐方法，解决了缺乏评分和元数据、课程注册分布不均衡以及课程依赖建模的挑战，并在真实世界数据集上实现了0.97的AUC得分。 |
| [^17] | [Structure Learning with Adaptive Random Neighborhood Informed MCMC.](http://arxiv.org/abs/2311.00599) | 本文引入了一种新颖的MCMC采样器PARNI-DAG，用于全贝叶斯方法下基于观测数据的结构学习问题。PARNI-DAG通过局部信息化的自适应随机邻域提议进行高效的DAG采样，并具有更好的混合性质。为了与节点数量更好地扩展，PARNI-DAG与通过一些基于约束或评分算法得到的骨架图的参数预调整过程相结合。得益于这些新颖的特性，PARNI-DAG快速收敛到高概率区域，并且在高维设置中的节点之间存在高相关性时不太容易陷入局部模式。 |
| [^18] | [Rethinking Variational Inference for Probabilistic Programs with Stochastic Support.](http://arxiv.org/abs/2311.00594) | 本文提出了一种新的变分推断方法SDVI，可以用于具有随机支持的概率编程。通过将程序分解为具有静态支持的子程序，并为每个子程序构建独立的变分指导，SDVI在推断性能方面取得了显著改进。 |
| [^19] | [Coop: Memory is not a Commodity.](http://arxiv.org/abs/2311.00591) | 本论文提出了一种名为Coop的方法，针对深度学习框架中的内存系统问题，通过驱逐连续张量和优化张量分配，以降低再材料化成本。 |
| [^20] | [Flexible Tails for Normalising Flows, with Application to the Modelling of Financial Return Data.](http://arxiv.org/abs/2311.00580) | 本文提出了一种灵活的尾部转换方法，可以用于归一化流来近似金融回报的重尾分布，能够捕捉可能出现在数据中的极端冲击。 |
| [^21] | [Revealing CNN Architectures via Side-Channel Analysis in Dataflow-based Inference Accelerators.](http://arxiv.org/abs/2311.00579) | 本文通过评估数据流加速器上的侧信道信息，提出了一种攻击方法来恢复CNN模型的架构。该攻击利用了数据流映射的数据重用以及架构线索，成功恢复了流行的CNN模型Lenet，Alexnet和VGGnet16的结构。 |
| [^22] | [Transfer learning for improved generalizability in causal physics-informed neural networks for beam simulations.](http://arxiv.org/abs/2311.00578) | 本文介绍了一种用于模拟弹性基础上梁动力学的新方法，通过迁移学习和因果性物理信息神经网络框架，提高了问题的泛化能力，并在数值实验中验证了其有效性。 |
| [^23] | [Personalized Assignment to One of Many Treatment Arms via Regularized and Clustered Joint Assignment Forests.](http://arxiv.org/abs/2311.00577) | 提出了一种个性化分配至多个治疗组的方法，通过正则化和聚类优化治疗分配，实现了更好的效果估计和个性化效益。 |
| [^24] | [Online Student-$t$ Processes with an Overall-local Scale Structure for Modelling Non-stationary Data.](http://arxiv.org/abs/2311.00564) | 我们提出了一种整体-局部尺度结构的贝叶斯学生-t过程混合模型，可用于建模非平稳数据，通过实时接收数据进行在线推断。在真实世界数据集上的对比实验证明了我们方法的优越性。 |
| [^25] | [Learning to optimize by multi-gradient for multi-objective optimization.](http://arxiv.org/abs/2311.00559) | 本文介绍了一种基于自动学习的新的多目标优化方法，通过多梯度学习优化（ML2O）来实现优化过程，该方法利用当前步骤的信息和历史迭代轨迹数据来获取局部和全局知识，同时引入守护机制（GML2O）以确保生成的迭代序列收敛到Pareto关键点。 |
| [^26] | [Machine Learning Without a Processor: Emergent Learning in a Nonlinear Electronic Metamaterial.](http://arxiv.org/abs/2311.00537) | 这项研究介绍了一种非线性学习变质材料，它由基于晶体管的自适应非线性阻性元件构成，可以在没有计算机的情况下学习非线性任务，并降低训练误差的多个模式，为模拟机器学习提供了新的硬件选择。 |
| [^27] | [Active Noise Control Portable Device Design.](http://arxiv.org/abs/2311.00535) | 该论文提出了一种便携式的主动降噪设备设计，通过传感器检测环境中的噪音，经过电子控制系统处理后，使用反位相频率信号来降低噪音。这种方法对低频噪音也更加有效。 |
| [^28] | [Learning impartial policies for sequential counterfactual explanations using Deep Reinforcement Learning.](http://arxiv.org/abs/2311.00523) | 本文提出了使用深度强化学习学习顺序可解释策略的无偏方法。该方法通过对分类器的输出概率进行奖励来减轻现有方法中可能导致偏向特定动作的策略的不希望属性。 |
| [^29] | [Retrieval-Based Reconstruction For Time-series Contrastive Learning.](http://arxiv.org/abs/2311.00519) | 本文提出了一种基于检索重建的时间序列对比学习方法（REBAR），通过检索信息和重建子序列来构建正样本对，从而解决了时间序列中使用数据增强创建正样本对的挑战。实验证明，REBAR误差可以作为正/负标记器，并且在对比学习框架中集成REBAR方法可以学习具有有用信息的嵌入表示。 |
| [^30] | [Efficient LLM Inference on CPUs.](http://arxiv.org/abs/2311.00502) | 本研究提出了一种在CPU上高效部署大型语言模型（LLMs）的方法，支持自动权重量化和优化内核，在流行的LLMs上展示了极高的推理效率。 |
| [^31] | [Intriguing Properties of Data Attribution on Diffusion Models.](http://arxiv.org/abs/2311.00500) | 本研究通过对扩散模型进行实验和分析，发现在数据归因方面，一些在理论上不合理的设计选择能够在实际中表现出比以前的方法更好的效果。这对于确保数据贡献者公平补偿或认可具有重要意义。 |
| [^32] | [Deep Neural Networks for Automatic Speaker Recognition Do Not Learn Supra-Segmental Temporal Features.](http://arxiv.org/abs/2311.00489) | 这项研究表明深度神经网络在自动说话人识别中无法充分模拟超分段时间特征，这为未来更好地利用完整语音信号进行研究提供了基础。 |
| [^33] | [Comparing Optimization Targets for Contrast-Consistent Search.](http://arxiv.org/abs/2311.00488) | 本研究通过对比优化目标发现，使用Midpoint-Displacement（MD）损失函数可以获得与Contrast-Consistent Search（CCS）非常相似的模型权重，而且通过调整超参数可以使MD损失函数的测试准确率高于CCS。 |
| [^34] | [Fixed-Budget Best-Arm Identification in Sparse Linear Bandits.](http://arxiv.org/abs/2311.00481) | 本文研究了在稀疏线性bandit中的固定预算条件下的最佳臂识别问题，设计了基于Lasso和最优设计的两阶段算法，通过适当选择超参数和平衡两个阶段的错误概率，得到了Lasso-OD的非渐近上界。 |
| [^35] | [Group Distributionally Robust Knowledge Distillation.](http://arxiv.org/abs/2311.00476) | 本文提出了一种群组感知的蒸馏损失来解决知识蒸馏在医学影像分析中的亚群体偏移问题，为在训练期间动态关注表现较差的群组提供了方法。 |
| [^36] | [Diffusion models for probabilistic programming.](http://arxiv.org/abs/2311.00474) | 我们提出了一种新的扩散模型变分推断（DMVI）方法，用于在概率编程语言中进行自动近似推断。DMVI可以更准确地进行后验推断，而且易于实现和使用，对神经网络模型没有任何约束。 |
| [^37] | [Dual Conditioned Diffusion Models for Out-Of-Distribution Detection: Application to Fetal Ultrasound Videos.](http://arxiv.org/abs/2311.00469) | 本论文提出了双重条件扩散模型（DCDM），用于胎儿超声视频中的外分布检测。该模型能够在存在高度结构相似性和大量内部变异性的背景下，对胎儿超声视频中的心脏视图进行检测，并拒绝类似的外分布样本。 |
| [^38] | [Asynchronous SGD on Graphs: a Unified Framework for Asynchronous Decentralized and Federated Optimization.](http://arxiv.org/abs/2311.00465) | 本文引入了图上的异步SGD（AGRAF SGD）算法框架，该框架统一了异步分散和联邦优化算法，并在更温和的假设下提供了收敛速度，还恢复或改善了所有算法的最佳结果。 |
| [^39] | [Robust and Conjugate Gaussian Process Regression.](http://arxiv.org/abs/2311.00463) | 本文提出了一种健壮和共轭的高斯过程（RCGP）回归方法，通过泛化贝叶斯推断实现了可靠的闭式更新，适用于各种实际应用场景。 |
| [^40] | [Optimal Budgeted Rejection Sampling for Generative Models.](http://arxiv.org/abs/2311.00460) | 该论文提出了一种最佳限制预算拒绝采样方法（OBRS），可显著改善生成模型的样本质量和多样性。通过将采样方案与训练过程相结合，该方法在给定采样预算情况下，对于任何真实分布和拒绝后分布之间的f-散度都是最优的。 |
| [^41] | [Hessian Eigenvectors and Principal Component Analysis of Neural Network Weight Matrices.](http://arxiv.org/abs/2311.00452) | 本研究揭示了神经网络权重矩阵的Hessian特征向量与网络权重之间的相关性，通过识别关键方向揭示了漂移模式和势能最小值的关系。 |
| [^42] | [A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models.](http://arxiv.org/abs/2311.00445) | 这项研究通过对比人类和语言模型在三段论推理中的表现，发现较大的语言模型更合逻辑，甚至比人类更合逻辑，但即使最大的语言模型也会出现与人类推理类似的错误，总体上认为语言模型在某些情况下能够克服人类偏见。 |
| [^43] | [Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements.](http://arxiv.org/abs/2311.00444) | 本研究提出了一种新的问题设置，即在下游任务中基于图的功能要求生成图形。我们通过细调预训练的大型语言模型，引入消息传递层来结合图结构信息，实现了更贴近功能要求的图形生成。实验结果表明，我们的方法在公开可得的分子和知识图数据集上，相比类似任务上的基准方法，生成的图形更能满足功能要求。 |
| [^44] | [Improving Robustness for Vision Transformer with a Simple Dynamic Scanning Augmentation.](http://arxiv.org/abs/2311.00441) | 通过Dynamic Scanning Augmentation增强技术，提高了Vision Transformer的准确性和鲁棒性，尤其是在面对对抗攻击时。这种方法适应性地聚焦于不同的图像块，并改变了ViT的注意力机制。 |
| [^45] | [Crop Disease Classification using Support Vector Machines with Green Chromatic Coordinate (GCC) and Attention based feature extraction for IoT based Smart Agricultural Applications.](http://arxiv.org/abs/2311.00429) | 本文提出了一种新的作物病害分类方法，利用基于注意力的特征提取和绿色色度坐标，可以轻松与移动设备和物联网设备交互，为农民提供最佳的作物种植保障。 |
| [^46] | [NEO-KD: Knowledge-Distillation-Based Adversarial Training for Robust Multi-Exit Neural Networks.](http://arxiv.org/abs/2311.00428) | NEO-KD是一种基于知识蒸馏的对抗训练策略，用于提升多出口神经网络的对抗鲁棒性，通过邻居知识蒸馏和出口间正交知识蒸馏，该方法能够减少对特定出口的对抗攻击影响，并增强整体性能。 |
| [^47] | [Enhanced Generalization through Prioritization and Diversity in Self-Imitation Reinforcement Learning over Procedural Environments with Sparse Rewards.](http://arxiv.org/abs/2311.00426) | 本研究通过优先级排序和多样性来提高自我模仿强化学习在过程化环境中的泛化能力。 |
| [^48] | [Efficient Human-AI Coordination via Preparatory Language-based Convention.](http://arxiv.org/abs/2311.00416) | 本研究发现，在人工智能与人类协同之前，人类进行交流以建立约定，指定角色和行动，有效地指导协同。基于此观察，提出利用大型语言模型(LLM)来实现高效的人工智能-人类协同。 |
| [^49] | [Uncertainty quantification and out-of-distribution detection using surjective normalizing flows.](http://arxiv.org/abs/2311.00377) | 本研究使用全射正常化流方法，在深度神经网络模型中识别超出分布数据集，从而可靠地量化不确定性，并在多个实验中得到了验证。 |
| [^50] | [Performance Optimization of Deep Learning Sparse Matrix Kernels on Intel Max Series GPU.](http://arxiv.org/abs/2311.00368) | 本文研究了在英特尔Max系列GPU上对深度学习稀疏矩阵核的性能优化。通过使用英特尔oneAPI的ESIMD SYCL扩展API，我们开发了优化实现并实现了高性能的稀疏矩阵操作，性能接近于GPU的峰值性能，并在与英特尔的oneMKL库和NVIDIA的V100 GPU上的CUDA实现进行比较后表现出更好的性能。 |
| [^51] | [Adversarially Robust Distributed Count Tracking via Partial Differential Privacy.](http://arxiv.org/abs/2311.00346) | 本研究通过使用部分差分隐私实现对抗鲁棒的分布式计数跟踪，通过探究随机算法和确定算法对于适应性敌手的鲁棒性，揭示了随机算法的$\sqrt{k}$优势是来源于随机性本身而不是无视敌手假设。 |
| [^52] | [The Open DAC 2023 Dataset and Challenges for Sorbent Discovery in Direct Air Capture.](http://arxiv.org/abs/2311.00341) | 本论文介绍了一种计算方法，利用机器学习的创新，构建了一个包含超过8,800个MOF材料的超大规模数据集，该数据集对于直接空气捕集中有前途的吸附剂发现具有重要意义。 |
| [^53] | [MetisFL: An Embarrassingly Parallelized Controller for Scalable & Efficient Federated Learning Workflows.](http://arxiv.org/abs/2311.00334) | MetisFL是一种可扩展和高效的联邦学习系统，重点关注联邦控制器的可扩展性和优化。 |
| [^54] | [Latent Space Inference For Spatial Transcriptomics.](http://arxiv.org/abs/2311.00330) | 本研究通过概率机器学习方法，将单细胞RNA测序和基于图像的空间转录组学数据映射到共同的潜在空间表示中，以获取组织样本的完整遗传表达信息，并保留空间坐标。这为深入理解细胞过程和通路提供了更多的见解。 |
| [^55] | [Multi-task Representation Learning for Pure Exploration in Bilinear Bandits.](http://arxiv.org/abs/2311.00327) | 这项研究通过多任务表示学习解决了双线性bandit中的纯探索问题，并提出了GOBLIN算法来优化样本分配和减小样本数量，在共享表示下实现了较高的效率。 |
| [^56] | [Robust Graph Clustering via Meta Weighting for Noisy Graphs.](http://arxiv.org/abs/2311.00322) | 该论文提出了一种鲁棒的基于元权重的图聚类方法，通过对节点对应权重的自适应调整，能够在存在噪声边的图中找到有意义的聚类。 |
| [^57] | [Semantic Hearing: Programming Acoustic Scenes with Binaural Hearables.](http://arxiv.org/abs/2311.00320) | 本论文提出了语义听力的概念，实现了带有双耳听功能的听觉设备，能够在真实环境中实时关注或忽略特定的声音，并保持空间线索。研究贡献包括首个能够在干扰声和背景噪音中实现双耳目标声音提取的神经网络以及可推广到实际使用场景的训练方法。研究结果展示了系统在多种声音类别下的性能和运行时间数据，并在“野外”评估中获得了良好的效果。 |
| [^58] | [Flooding Regularization for Stable Training of Generative Adversarial Networks.](http://arxiv.org/abs/2311.00318) | 本文在生成对抗网络中直接对对抗损失函数进行正则化，通过应用泛化方法防止判别器的损失过分降低，并通过实验证实了其稳定性。 |
| [^59] | [Data Augmentation for Code Translation with Comparable Corpora and Multiple References.](http://arxiv.org/abs/2311.00317) | 该论文介绍了两种数据增强方法来改善编程语言之间的代码翻译。通过构建可比较的语料库和增加多个参考翻译，实验结果表明这些方法显著提高了CodeT5在Java、Python和C++之间的翻译准确性。 |
| [^60] | [Federated Topic Model and Model Pruning Based on Variational Autoencoder.](http://arxiv.org/abs/2311.00314) | 本论文提出了一种基于变分自编码器的联邦主题模型和模型剪枝方法，用于解决跨多个方参与交叉分析时的数据隐私问题，并通过神经网络模型剪枝加速模型。两种不同的方法被提出来确定模型剪枝率。 |
| [^61] | [Stacking an autoencoder for feature selection of zero-day threats.](http://arxiv.org/abs/2311.00304) | 本研究使用堆叠自编码器和长短期记忆方案，通过特征提取和微调，成功应用于零日威胁的特征选择和分类。研究结果表明，该方法在各类攻击中表现出色，并能实现准确的分类。 |
| [^62] | [Inference of CO2 flow patterns -- a feasibility study.](http://arxiv.org/abs/2311.00290) | 研究对地下CO2泄漏进行监测和检测，提出了一种新的方法来描述CO2流动模式中的不确定性，并强调不确定性评估的重要性。 |
| [^63] | [Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models.](http://arxiv.org/abs/2311.00287) | 本文提出了一种通过大型语言模型进行临床文本生成的创新方法ClinGen，该方法将外部领域特定的知识和语言模型结合起来，提高了临床自然语言处理任务的性能，并丰富了样本的多样性。 |
| [^64] | [JADE: A Linguistic-based Safety Evaluation Platform for LLM.](http://arxiv.org/abs/2311.00286) | JADE是一种基于语言分析的LLM安全评估平台，能够破坏广泛使用的中文和英文LLM，并生成高度威胁的不安全问题。 |
| [^65] | [Mixture-of-Experts for Open Set Domain Adaptation: A Dual-Space Detection Approach.](http://arxiv.org/abs/2311.00285) | 该论文提出了一种Mixture-of-Experts用于开放域适应的双空间检测方法，利用图像特征空间和路由特征空间之间的不一致性来检测未知类别的样本，无需手动调节阈值。 |
| [^66] | [Model-driven Engineering for Machine Learning Components: A Systematic Literature Review.](http://arxiv.org/abs/2311.00284) | 这篇论文通过系统文献综述进一步探索了模型驱动工程与机器学习的有前途的交叉领域。 |
| [^67] | [Generalization Bounds for Label Noise Stochastic Gradient Descent.](http://arxiv.org/abs/2311.00274) | 本研究通过在非凸设置中使用标签噪声对随机梯度下降进行了泛化错误界限的研究，利用算法稳定性框架得到了时间无关的泛化错误界限，并且在参数维度和样本大小的速率以及特定学习率情况下实现了多项式的错误界限。该分析提供了关于标签噪声影响的量化见解。 |
| [^68] | [Rethinking Decision Transformer via Hierarchical Reinforcement Learning.](http://arxiv.org/abs/2311.00267) | 这篇论文通过引入分层强化学习，重新思考了决策Transformer。他们提出了一个通用的序列建模框架，在该框架中，高层策略为当前状态提供理想提示，低层策略在给定提示的条件下生成动作。他们发现决策Transformer是这个框架的一个特例，并研究了如何共同优化高层和低层策略以实现拼接能力，从而推动了新的离线学习算法的发展。 |
| [^69] | [Incentivized Collaboration in Active Learning.](http://arxiv.org/abs/2311.00260) | 该论文提出了一种激励合作的主动学习框架，旨在最小化标签复杂性，保证智能体无法通过个人行为减少其预期的标签复杂性，并提供了可实现且在标签复杂性方面与最佳可计算近似算法相媲美的合作协议。 |
| [^70] | [Solutions to Elliptic and Parabolic Problems via Finite Difference Based Unsupervised Small Linear Convolutional Neural Networks.](http://arxiv.org/abs/2311.00259) | 通过无监督学习，我们提出了利用小型卷积神经网络直接估计椭圆和抛物型问题的有限差分解，相比传统方法具有可比较的准确性。 |
| [^71] | [Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic Behavioral Analysis.](http://arxiv.org/abs/2311.00258) | 本研究通过领域不可知的扰动测试了大型语言模型在多跳推理任务中的鲁棒性，发现模型对某些扰动更敏感，并证明增加扰动样本的比例可以提高少样本提示方法的鲁棒性。 |
| [^72] | [Active Neural Topological Mapping for Multi-Agent Exploration.](http://arxiv.org/abs/2311.00252) | 本文提出了一种名为多智能体神经拓扑映射（MANTM）的方法，用于解决多智能体探索问题。该方法通过使用拓扑地图作为环境表示并结合深度强化学习，能够在有限时间内快速学习（近）最优策略。 |
| [^73] | [Implicit biases in multitask and continual learning from a backward error analysis perspective.](http://arxiv.org/abs/2311.00235) | 本论文使用反向误差分析计算了多任务和连续学习设置下神经网络的隐式训练偏差。在训练过程中，通过引入修改损失函数，隐式最小化了原始损失、引入了隐式平坦正则项和冲突项。在多任务中，冲突项衡量了任务梯度之间的对齐性；而在连续学习中，冲突项是深度学习优化中的一个新概念，它通过任务梯度之间的李括号来衡量。 |
| [^74] | [DistDNAS: Search Efficient Feature Interactions within 2 Hours.](http://arxiv.org/abs/2311.00231) | DistDNAS是一种在推荐系统中高效搜索特征交互的解决方案，通过分布式搜索和选择最佳交互模块，实现了巨大的加速并将搜索时间从2天缩短到2小时。 |
| [^75] | [StableFDG: Style and Attention Based Learning for Federated Domain Generalization.](http://arxiv.org/abs/2311.00227) | 本文提出了StableFDG，一种基于风格和注意力的学习策略，用于实现联邦领域泛化。其中，基于风格的学习将每个客户端的本地数据集中的样式扩展到原始源域之外，并通过风格共享、转移和探索策略改进了领域多样性。基于注意力的特征突出器捕捉了数据样本特征之间的相似性。 |
| [^76] | [Transformers are Efficient In-Context Estimators for Wireless Communication.](http://arxiv.org/abs/2311.00226) | 这项研究提出了一种新的方法，利用上下文估计来解决无线通信中的问题。传统方法忽略了信道的层次结构，而本研究利用了Transformers在上下文学习方面的优势，通过少量提示来实现了准确的传输符号估计。 |
| [^77] | [WinNet:time series forecasting with a window-enhanced period extracting and interacting.](http://arxiv.org/abs/2311.00214) | WinNet是一种用于时间序列预测的CNN-based模型，通过窗口增强的周期提取和交互，在捕捉长期和短期周期性方面具有高准确度和简单结构。在九个基准数据集上的实验结果表明，WinNet可以实现优于CNN、MLP、Transformer方法的最新性能，并具有较低的计算复杂度。 |
| [^78] | [A Unified Framework to Enforce, Discover, and Promote Symmetry in Machine Learning.](http://arxiv.org/abs/2311.00212) | 本文提供了一个统一的框架，通过三种方式将对称性融入机器学习模型：1. 强制已知的对称性；2. 发现未知的对称性；3. 在训练过程中促进对称性。 |
| [^79] | [Transformers as Recognizers of Formal Languages: A Survey on Expressivity.](http://arxiv.org/abs/2311.00208) | 本文对transformers在形式语言识别领域的相关研究进行了全面调查，为理解其表达能力提供了一个统一的框架。 |
| [^80] | [Federated Natural Policy Gradient Methods for Multi-task Reinforcement Learning.](http://arxiv.org/abs/2311.00201) | 本研究提出了一种多任务强化学习的联邦自然策略梯度方法，在分布式环境中，通过优化全局策略以最大化所有智能体的总奖励，实现协作决策。这些方法不受信息共享不完备的影响，且具有非渐近全局收敛保证。 |
| [^81] | [Machine learning for accuracy in density functional approximations.](http://arxiv.org/abs/2311.00196) | 本文回顾了近期在改进密度泛函及相关近似方法中应用机器学习的进展，讨论了在不同化学和材料类别之间设计可迁移的机器学习模型时可能面临的挑战和希望。 |
| [^82] | [Best of Both Worlds: Stochastic and Adversarial Convex Function Chasing.](http://arxiv.org/abs/2311.00181) | 该论文研究了随机和对抗性环境下的凸函数追踪问题，并给出了同时在两种情境下达到性能保证的算法。这是首个使用随机框架研究该问题的工作，提出了一种融合两种情境的最佳算法。 |
| [^83] | [The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback.](http://arxiv.org/abs/2311.00168) | 这项研究探讨了强化学习从人类反馈中的目标不匹配问题。研究发现，在强化学习从人类反馈中，奖励模型训练、策略模型训练和策略模型评估之间存在不一致，导致模型行为的意想不到的结果。 |
| [^84] | [Multi-task Deep Convolutional Network to Predict Sea Ice Concentration and Drift in the Arctic Ocean.](http://arxiv.org/abs/2311.00167) | 提出了一种名为HIS-Unet的多任务深度卷积网络架构，通过加权注意力模块实现海冰浓度和漂移的预测。与其他方法相比，HIS-Unet在海冰预测中取得了显著的改进。 |
| [^85] | [Graph Neural Networks for Road Safety Modeling: Datasets and Evaluations for Accident Analysis.](http://arxiv.org/abs/2311.00164) | 该论文构建了一个大规模的道路交通事故记录数据集，并使用该数据集评估了现有的深度学习方法在预测事故发生方面的准确性。研究发现，图神经网络GraphSAGE能够准确预测道路上的事故数量，并判断事故是否会发生。 |
| [^86] | [Score Normalization for a Faster Diffusion Exponential Integrator Sampler.](http://arxiv.org/abs/2311.00157) | 该论文提出了一种得分归一化方法，用于改进扩散指数积分器采样器的生成质量和减小积分误差。 |
| [^87] | [Medi-CAT: Contrastive Adversarial Training for Medical Image Classification.](http://arxiv.org/abs/2311.00154) | 本文提出了一种名为Medi-CAT的训练策略，以克服医学图像数据集中的欠拟合和过拟合现象。该方法利用大型预训练视觉转换器来解决欠拟合问题，并采用对抗性和对比性学习技术来防止过拟合。实验证明，该方法在多个医学图像分类数据集上具有较高的准确率和性能提升。 |
| [^88] | [Two-Stage Classifier for Campaign Negativity Detection using Axis Embeddings: A Case Study on Tweets of Political Users during 2021 Presidential Election in Iran.](http://arxiv.org/abs/2311.00143) | 本文提出了一个混合模型，用于检测竞选活动的负面情绪，包括一个两阶段分类器，结合了两个机器学习模型的优势。通过对伊朗2021年总统选举期间50名政治用户的5,100条波斯语推文进行标注，建立了所需的数据集。 |
| [^89] | [Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data.](http://arxiv.org/abs/2311.00136) | Neuroformer是一个多模态和多任务的生成预训练模型，旨在处理系统神经科学中大规模的多模态数据。模型经过训练后能准确预测神经回路活动并推断神经回路连接性，同时能用于预测行为。 |
| [^90] | [Extracting the Multiscale Causal Backbone of Brain Dynamics.](http://arxiv.org/abs/2311.00118) | 该研究提出了一种用于提取脑动力学多尺度因果骨架的方法，并通过对合成数据和静息态fMRI数据的实验证明其优越性。研究结果显示，因果动力在不同频率下受不同脑区驱动，这为理解脑功能提供了新的视角。 |
| [^91] | [EXTRACT: Explainable Transparent Control of Bias in Embeddings.](http://arxiv.org/abs/2311.00115) | 本论文提出了一套解释性和透明的方法，称为EXTRACT，来控制知识图谱嵌入中的偏见。该方法使用规范相关分析（CCA）来研究信息泄漏的存在程度和来源，并通过解线性方程组将嵌入分解为私有属性的总和。实验结果表明，可以从用户的行为中推断出各种个人属性。 |
| [^92] | [FairWASP: Fast and Optimal Fair Wasserstein Pre-processing.](http://arxiv.org/abs/2311.00109) | FairWASP是一种快速和最优的公平Wasserstein预处理方法，通过重新加权数据集来减少分类数据集中的不平等性，同时满足人口平等性准则。这种方法可以用于构建可以输入任何分类方法的数据集。 |
| [^93] | [Deep Compressed Learning for 3D Seismic Inversion.](http://arxiv.org/abs/2311.00107) | 本文提出了一种使用深度压缩学习方法进行3D地震反演的解决方案，通过联合优化降维操作符和DCNN实现的编码器-解码器，实现了在使用极少数地震源的情况下，减少了数量级的地震记录使用量，同时保持了与使用整个数据集相当的3D重建质量。 |
| [^94] | [Bandit-Driven Batch Selection for Robust Learning under Label Noise.](http://arxiv.org/abs/2311.00096) | 本研究提出了一种基于Bandit算法的批次选择方法，以改善在标签噪音下的学习过程。实验证明该方法在不同水平的标签污染下表现优于现有方法，且无需使用辅助神经网络模型。 |
| [^95] | [Expressive Modeling Is Insufficient for Offline RL: A Tractable Inference Perspective.](http://arxiv.org/abs/2311.00094) | 本文指出，在离线强化学习任务中，除了表达性强的序列模型，可处理性也起着重要的作用。由于离线数据收集策略和环境动态的随机性，需要精确且高效地回答各种概率查询，以找到有奖励的动作。基于此，本文提出了Trifle（离线强化学习的可处理推理）方法，利用现代可处理概率模型来解决这个问题。 |
| [^96] | [Seeking Truth and Beauty in Flavor Physics with Machine Learning.](http://arxiv.org/abs/2311.00087) | 使用机器学习技术在味道物理学中找到真实而美丽的模型。 |
| [^97] | [Spuriosity Rankings for Free: A Simple Framework for Last Layer Retraining Based on Object Detection.](http://arxiv.org/abs/2311.00079) | 该论文提出了一种简单的框架，利用目标检测技术和排名方法来实现最后一层的重新训练，以解决深度神经网络对虚假特征的依赖问题。 |
| [^98] | [Safe multi-agent motion planning under uncertainty for drones using filtered reinforcement learning.](http://arxiv.org/abs/2311.00063) | 这篇论文提出了一种基于过滤强化学习的无人机安全多智能体运动规划方法，通过结合强化学习和约束控制技术，在不确定性环境中实现了安全性、实时性和高效性。 |
| [^99] | [The Generative AI Paradox: "What It Can Create, It May Not Understand".](http://arxiv.org/abs/2311.00059) | 生成型AI的悖论研究了生成型模型与人类智能之间的差异，模型在产生专家级输出的能力上可能超过其理解能力。 |
| [^100] | [Diversity and Diffusion: Observations on Synthetic Image Distributions with Stable Diffusion.](http://arxiv.org/abs/2311.00056) | 这项研究观察了文本到图像系统的进展，并发现只使用合成图像训练的分类器在推理时表现不佳，揭示了底层图像生成过程的局限性。 |
| [^101] | [Training-Free Generalization on Heterogeneous Tabular Data via Meta-Representation.](http://arxiv.org/abs/2311.00055) | 提出了通过元表示进行表格数据预训练的方法，使得模型可以在异构数据集上进行无训练泛化的应用。 |
| [^102] | [On the Kolmogorov neural networks.](http://arxiv.org/abs/2311.00049) | Kolmogorov神经网络模型可以精确地表示连续函数、有界不连续函数和所有无界多元函数。 |
| [^103] | [SC-MIL: Sparsely Coded Multiple Instance Learning for Whole Slide Image Classification.](http://arxiv.org/abs/2311.00048) | 本文提出了SC-MIL模型，通过利用稀疏字典学习来同时改进特征嵌入和实例相关性建模，从而提高全切片图像分类的性能。 |
| [^104] | [Grounding Visual Illusions in Language: Do Vision-Language Models Perceive Illusions Like Humans?.](http://arxiv.org/abs/2311.00047) | 通过构建一个包含五种视觉幻觉的数据集，研究发现，尽管整体对齐性较低，但更大规模的视觉-语言模型更接近人类的感知并更容易受到视觉幻觉的影响。 |
| [^105] | [Bayesian Multistate Bennett Acceptance Ratio Methods.](http://arxiv.org/abs/2310.20699) | 贝叶斯多状态Bennett接受比率方法（BayesMBAR）是多状态Bennett接受比率（MBAR）方法的贝叶斯推广。通过整合采样配置和先验分布，BayesMBAR计算了自由能的后验分布，并提供更准确的不确定性估计。 |
| [^106] | [Long-Tailed Learning as Multi-Objective Optimization.](http://arxiv.org/abs/2310.20490) | 这项研究通过将长尾识别问题转化为多目标优化问题，提出一种公平地估计头部和尾部类别贡献的方法，并通过梯度平衡分组策略提高了效率。 |
| [^107] | [Discussing the Spectra of Physics-Enhanced Machine Learning via a Survey on Structural Mechanics Applications.](http://arxiv.org/abs/2310.20425) | 本文探讨了物理增强机器学习 (PEML) 的光谱，揭示了其在解决复杂挑战方面的潜力，并通过具体示例演示了不同类型PEML方法的个体特征和动机。 |
| [^108] | [Dropout Strategy in Reinforcement Learning: Limiting the Surrogate Objective Variance in Policy Optimization Methods.](http://arxiv.org/abs/2310.20380) | 本文提出了一种Dropout技术来限制策略优化方法中替代目标方差的增长，并将其应用于PPO算法中。实验结果表明，D-PPO算法相较于PPO算法在Atari 2600游戏上表现更好。 |
| [^109] | [Learning to Discover Skills through Guidance.](http://arxiv.org/abs/2310.20178) | 提出了一种名为DISCO-DANCE的无监督技能发现算法，通过引导学习提高探索效果，并在具有挑战性的环境中优于其他方法。 |
| [^110] | [SURF: A Generalization Benchmark for GNNs Predicting Fluid Dynamics.](http://arxiv.org/abs/2310.20049) | 提出了一个名为SURF的基准测试，用于评估和比较基于图的学习流体模拟器的泛化能力。SURF包括各种数据集和具体的性能和泛化度量指标。通过深入研究两种最先进的模型，我们证明了SURF的适用性。 |
| [^111] | [Unified Enhancement of Privacy Bounds for Mixture Mechanisms via $f$-Differential Privacy.](http://arxiv.org/abs/2310.19973) | 本文通过$f$-差分隐私方法改进了洗牌模型和DP-GD中随机初始化的隐私边界，折衷函数的闭式表达式优于$(\epsilon,\delta)$-DP的结果，并且随机初始化可以增强DP-GD的隐私性。 |
| [^112] | [Stochastic Thermodynamics of Learning Generative Parametric Probabilistic Models.](http://arxiv.org/abs/2310.19802) | 本文将生成式机器学习问题视为参数概率模型的时间演化过程，通过研究模型参数与生成样本之间的热力学交换，发现模型通过耗散热量来学习，参数子系统充当热库存储学到的信息。这为超参数模型的泛化能力提供了有价值的热力学洞察。 |
| [^113] | [Combining Language Models For Specialized Domains: A Colorful Approach.](http://arxiv.org/abs/2310.19708) | 该论文提出了一种将领域特定语言模型与通用语言模型结合的新颖方法，通过对词语进行标记或“上色”来有效处理领域术语，显著降低了领域专用任务的错误率。 |
| [^114] | [DGFN: Double Generative Flow Networks.](http://arxiv.org/abs/2310.19685) | 双生成流网络（DGFNs）是一种能够有效增强药物发现中探索能力的方法，通过引入目标网络和采样路径的方式，解决了稀疏奖励领域和高维状态空间的挑战。 |
| [^115] | [rTsfNet: a DNN model with Multi-head 3D Rotation and Time Series Feature Extraction for IMU-based Human Activity Recognition.](http://arxiv.org/abs/2310.19283) | rTsfNet是一种新的DNN模型，通过多头3D旋转和时序特征提取实现了IMU-based人体活动识别，并在多个数据集上取得了最高的准确率。 |
| [^116] | [Debunking Free Fusion Myth: Online Multi-view Anomaly Detection with Disentangled Product-of-Experts Modeling.](http://arxiv.org/abs/2310.18728) | 本文提出了一种新型的多视角变分自编码器模型dPoE，通过使用解耦表示学习和生成模型的方法，解决了多视角数据异常检测中存在的问题。该方法不仅适用于多个视图和多种类型的异常，还支持模型部署后的在线检测。 |
| [^117] | [Preventing Language Models From Hiding Their Reasoning.](http://arxiv.org/abs/2310.18512) | 本论文研究了防止语言模型隐藏推理过程的问题。我们发现语言模型可以通过编码推理来提高性能，而无需用户理解中间推理步骤。随着语言模型变得越来越强大，这种行为可能会越来越普遍。我们还提出了一种方法来评估防御编码推理的方法。 |
| [^118] | [Covert Planning against Imperfect Observers.](http://arxiv.org/abs/2310.16791) | 本文研究了对抗不完美观察者的隐秘规划问题，提出了一种利用随机动力学和观察者的不完美观测耦合的方法，实现最优任务性能而不被发现。 |
| [^119] | [Neural Collapse in Multi-label Learning with Pick-all-label Loss.](http://arxiv.org/abs/2310.15903) | 这项论文研究了在多标签分类任务中的神经坍缩现象。他们推广了之前在多类别分类中发现的神经坍缩现象，证明了在“选择所有标签”公式下存在广义的神经坍缩现象。他们还发现了在广义的神经坍缩中的一个组合性质。 |
| [^120] | [Machine Translation for Nko: Tools, Corpora and Baseline Results.](http://arxiv.org/abs/2310.15612) | 该论文提出了针对Nko语（一种在多个西非国家使用的语言）开发可用的机器翻译系统的一套工具、资源和基准结果，包括新颖的协作平行文本整理软件、扩展的语料库和基线神经机器翻译结果。 |
| [^121] | [Dataset Bias Mitigation in Multiple-Choice Visual Question Answering and Beyond.](http://arxiv.org/abs/2310.14670) | 这项研究提出了一种解决多项选择视觉问答中数据集偏差的方法，包括不平衡匹配偏差和分心相似性偏差，并提出了对抗数据合成和样本内对立训练的技术来应对这些偏差。 |
| [^122] | [CLARA: Multilingual Contrastive Learning for Audio Representation Acquisition.](http://arxiv.org/abs/2310.11830) | 本文提出了一个多语言对比学习框架，通过自监督学习从无标签数据中获取音频表示，实现了跨语言迁移和情感维度的编码。 |
| [^123] | [Smootheness-Adaptive Dynamic Pricing with Nonparametric Demand Learning.](http://arxiv.org/abs/2310.07558) | 这项研究提出了一种具有非参数需求学习和平滑自适应的动态定价算法，通过使用自相似条件实现了最小化极限遗憾。 |
| [^124] | [FABind: Fast and Accurate Protein-Ligand Binding.](http://arxiv.org/abs/2310.06763) | FABind是一个结合了口袋预测和对接的端到端模型，旨在实现快速准确的蛋白-配体结合预测。 |
| [^125] | [Implicit Variational Inference for High-Dimensional Posteriors.](http://arxiv.org/abs/2310.06643) | 本文提出了一种隐变分推断的方法，使用神经采样器指定隐含分布，在高维空间中近似复杂的多峰和相关后验分布。通过引入局部线性化的约束，避免了依赖额外的网络和不稳定对抗目标的问题。此外，还提出了一种新的采样器架构，首次实现了对数百万个潜变量的隐含分布。实证分析表明，该方法可以恢复大型贝叶斯神经网络中层之间的相关性，这对于网络的性能至关重要。 |
| [^126] | [Automatic Integration for Spatiotemporal Neural Point Processes.](http://arxiv.org/abs/2310.06179) | 本文提出了一种自动化的时空神经点过程积分方法(AutoSTPP)，扩展了AutoInt方法用于三维时空点过程(STPP)的计算，具有优越性能。 |
| [^127] | [Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning Generalization.](http://arxiv.org/abs/2310.05506) | 本文调查了在数学推理中使用数据增强的效果，并通过创建新的数据集和微调模型取得了显著成果。 |
| [^128] | [How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition.](http://arxiv.org/abs/2310.05492) | 本研究探讨了大规模语言模型在监督微调过程中，特别是数学推理和代码生成能力方面，数据组合的影响。实验结果显示，较大模型在相同数据量下表现出更好的性能，通过增加微调数据和模型参数，数学推理和代码生成能力得到显著提升。 |
| [^129] | [Automatic Anonymization of Swiss Federal Supreme Court Rulings.](http://arxiv.org/abs/2310.04632) | 该论文介绍了瑞士联邦最高法院裁决的自动化匿名化方法，通过利用大型数据集和领域内预训练模型，结果表明相比现有模型，使用领域内数据进一步提高了F1分数超过5%。这项工作展示了将现有的匿名化方法与机器学习相结合，可以减少人工劳动并增强自动建议的能力。 |
| [^130] | [TacoGFN: Target Conditioned GFlowNet for Structure-Based Drug Design.](http://arxiv.org/abs/2310.03223) | 该论文提出了一种名为TacoGFN的目标条件GFlowNet模型，用于自动化生成符合特定蛋白质口袋目标的类药物化合物。该模型通过强化学习框架，鼓励生成具有期望属性的分子，并利用转换器和对接神经网络进行高效的分子空间探索和对接得分预测，以实现较高的结合改善效果。 |
| [^131] | [Nonlinear MPC design for incrementally ISS systems with application to GRU networks.](http://arxiv.org/abs/2309.16428) | 本研究提出了一种非线性模型预测控制（NMPC）策略，适用于增量ISS系统。通过简化计算终端成分，并明确定义最小预测范围，实现闭环稳定性。将该方法应用于GRU网络，并提供了一种量身定制状态观察器的设计方法。测试结果表明该控制架构具有良好的控制性能和高效的实用性。 |
| [^132] | [Task-Oriented Koopman-Based Control with Contrastive Encoder.](http://arxiv.org/abs/2309.16077) | 该论文介绍了一种基于任务导向的Koopman控制方法，利用对比编码器和端到端强化学习来同时学习Koopman潜在嵌入、算子和相关线性控制器。通过优先考虑任务成本作为主要目标，减少了对于明确定义模型的控制器设计的依赖，将Koopman控制扩展到高维、复杂非线性系统，包括基于像素的场景。 |
| [^133] | [State-space Models with Layer-wise Nonlinearity are Universal Approximators with Exponential Decaying Memory.](http://arxiv.org/abs/2309.13414) | 本论文证明了堆叠具有逐层非线性激活的状态空间模型足以逼近任何连续的序列到序列关系，并且发现其加强了模型学习复杂序列模式的能力。然而，状态空间模型并不能根本解决指数衰减记忆的问题。 |
| [^134] | [Invisible Watermarking for Audio Generation Diffusion Models.](http://arxiv.org/abs/2309.13166) | 该论文提出了一种用于音频扩散模型的隐形数字水印技术，在保证正常音频生成的同时，还能为模型验证提供保护层，用于鉴别模型所有权和维护其完整性。 |
| [^135] | [A Convex Framework for Confounding Robust Inference.](http://arxiv.org/abs/2309.12450) | 本文提出了一个支撑鲁棒推断的凸框架，通过利用凸规划提供策略价值的精确下界。此外，该方法还可以进行多种扩展，并且具有强理论保证。 |
| [^136] | [Cross-tokamak Disruption Prediction based on Physics-Guided Feature Extraction and domain adaptation.](http://arxiv.org/abs/2309.05361) | 本文介绍了一种新颖的方法，使用少量放电预测未来托卡马克的破裂，并应用了物理引导特征提取和领域适应算法。这种方法在J-TEXT中取得了出色的破裂预测性能。 |
| [^137] | [Evaluating the Efficacy of Supervised Learning vs Large Language Models for Identifying Cognitive Distortions and Suicidal Risks in Chinese Social Media.](http://arxiv.org/abs/2309.03564) | 本研究评估了监督学习和大型语言模型在识别中国社交媒体中的认知偏差和自杀风险方面的功效。结果表明大型语言模型在这两个任务上具有很高的效果。 |
| [^138] | [YaRN: Efficient Context Window Extension of Large Language Models.](http://arxiv.org/abs/2309.00071) | YaRN是一种高效的上下文窗口扩展方法，可以在大型语言模型中有效利用和推断比原始预训练允许的上下文长度更长的上下文，同时超越了之前的最新研究成果。 |
| [^139] | [Multivariate Time Series Anomaly Detection: Fancy Algorithms and Flawed Evaluation Methodology.](http://arxiv.org/abs/2308.13068) | 多元时间序列异常检测是一个研究领域，但目前存在组织不够有序和评估协议有缺陷的问题。文章评估了许多最近算法的性能，并指出了针对多元时间序列异常检测的评估协议存在的问题及如何缓解这些问题的方法。 |
| [^140] | [ALGAN: Time Series Anomaly Detection with Adjusted-LSTM GAN.](http://arxiv.org/abs/2308.06663) | 该论文提出了一种名为ALGAN的新型GAN模型，通过调整LSTM网络的输出，实现了在无监督设置下对单变量和多变量时间序列数据进行异常检测，并在实验中优于传统方法和其他GAN方法。 |
| [^141] | [Online learning in bandits with predicted context.](http://arxiv.org/abs/2307.13916) | 本文研究了一种在预测上下文中的在线学习问题，通过将经典统计学中的测量误差模型推广到在线决策设置中，我们提出了第一个具有次线性后悔的在线算法。 |
| [^142] | [Optimized Network Architectures for Large Language Model Training with Billions of Parameters.](http://arxiv.org/abs/2307.12169) | 本文提出了一种优化的网络架构，用于训练拥有数十亿参数的大型语言模型。这个架构根据语言模型的通信需求，将集群分割成一组通过非阻塞高带宽互连的GPU集合，并通过轨道连接仅连接具有通信需求的GPU，从而降低网络成本高达75％，同时不影响训练性能。 |
| [^143] | [Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory.](http://arxiv.org/abs/2307.10768) | 本论文介绍了一个全面的工作记忆基准数据集（WorM），通过评估4个功能、3个领域和11个行为和神经特征的WM任务来开发和评估AI WM模型。结果表明，AI模型能够模拟出脑中工作记忆的一些特征，如优势效应和最新性效应，以及专门用于不同领域和功能的工作记忆的神经群集和相关物。 |
| [^144] | [Learning Curves for Heterogeneous Feature-Subsampled Ridge Ensembles.](http://arxiv.org/abs/2307.03176) | 通过引入异构特征集成，本文研究了在线性回归设置中构建的岭回归集成的学习曲线，结果表明异构特征集成具有较好的性能。 |
| [^145] | [Meta-Learning Adversarial Bandit Algorithms.](http://arxiv.org/abs/2307.02295) | 本论文研究了具有波段反馈的在线元学习，并设计了用于多臂赌博机和赌博线性优化的元算法。对于多臂赌博机，算法使用了Tsallis-熵的泛化Exp3，并且任务平均遗憾会随着最优解的熵的减小而改善。对于赌博线性优化，算法使用了自协调障碍正则化器初始化和调整在线镜像下降，并且任务平均遗憾与动作空间相关的度量直接变化。 |
| [^146] | [Generalization Limits of Graph Neural Networks in Identity Effects Learning.](http://arxiv.org/abs/2307.00134) | 本研究在学习身份效应的背景下，分析了图神经网络在泛化属性和基本限制方面的新性质，以及在两个字母的单词案例中的具体应用。 |
| [^147] | [Solving Kernel Ridge Regression with Gradient-Based Optimization Methods.](http://arxiv.org/abs/2306.16838) | 本研究提出了一种新的方法来解决核岭回归问题，通过等价的目标函数形式和基于梯度的优化方法，我们不仅可以使用其他惩罚方法，还能够从梯度下降的角度研究核岭回归。通过提前停止的正则化，我们推导出了一个闭合解，即核梯度流（KGF），并证明了KGF和KRR之间的差异。我们还将KRR泛化，使用$\ell_1$和$\ell_\infty$惩罚方法，并发现使用这些方法得到的解与前向分步回归和符号梯度下降结合提前停止得到的解非常相似。因此，我们减少了计算复杂度重的近端梯度下降算法的需求。 |
| [^148] | [To Spike or Not To Spike: A Digital Hardware Perspective on Deep Learning Acceleration.](http://arxiv.org/abs/2306.15749) | 神经形态计算旨在通过仿真脑部操作来提高深度学习模型的效率，但是在SNNs的高效硬件后端设计上仍需进一步研究。 |
| [^149] | [Restart Sampling for Improving Generative Processes.](http://arxiv.org/abs/2306.14878) | 本文提出了一种名为“重启”的新型采样算法，以更好地平衡离散化误差和收缩，可以优化生成过程中的采样速度和样本质量。 |
| [^150] | [Neural Algorithmic Reasoning Without Intermediate Supervision.](http://arxiv.org/abs/2306.13411) | 神经算法推理最近的变革点是逐步学习算法，但我们提出了一种不需要中间监督的新方法，并在不牺牲性能的情况下规范模型的中间计算。 |
| [^151] | [Diverse Community Data for Benchmarking Data Privacy Algorithms.](http://arxiv.org/abs/2306.13216) | 多样社区数据摘要旨在为隐私保护机器学习研究提供真实、多样和复杂的基准数据，以解决合成数据的偏差和隐私问题。 |
| [^152] | [OpenGSL: A Comprehensive Benchmark for Graph Structure Learning.](http://arxiv.org/abs/2306.10280) | OpenGSL是第一个针对图结构学习的综合基准测试，旨在解决GSL领域中由于实验协议不一致而导致的进展不明确的问题。 |
| [^153] | [Building the Bridge of Schr\"odinger: A Continuous Entropic Optimal Transport Benchmark.](http://arxiv.org/abs/2306.10161) | 本文提出了一种用于构建连续基准分布的方法，该分布具有已知的熵最优传输和Schrödinger桥解。这填补了在这一研究领域参数选择方面的空白，并为研究人员提供了测试传输模型性能的一种方法。 |
| [^154] | [Tighter Prediction Intervals for Causal Outcomes Under Hidden Confounding.](http://arxiv.org/abs/2306.09520) | 本文提出了一种名为Caus-Modens的算法，通过调制集合来描述因果结果区间，相比符合性预测方法，能够在实践中给出更紧密的结果区间。 |
| [^155] | [ClimSim: An open large-scale dataset for training high-resolution physics emulators in hybrid multi-scale climate simulators.](http://arxiv.org/abs/2306.08754) | 这是一个用于训练高分辨率物理仿真器的气候数据集。该数据集包含了5.7亿个多变量输入和输出矢量对，用于隔离本地嵌套的高分辨率、高保真度物理的影响。 |
| [^156] | [FLSL: Feature-level Self-supervised Learning.](http://arxiv.org/abs/2306.06203) | 本文提出FLSL方法，采用Transformer进行联合嵌入和聚类，适合于内视图和外视图特征聚类。实验证明该方法在语义类簇表达方面取得显著改进。 |
| [^157] | [PoET: A generative model of protein families as sequences-of-sequences.](http://arxiv.org/abs/2306.06156) | PoET是一个模型，能够生成任何蛋白质家族的一系列相关蛋白质序列，可以用作检索增强语言模型生成和评分任何修改 |
| [^158] | [Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts.](http://arxiv.org/abs/2306.04723) | 本文提出了衡量文本内部维度的方法，应用于鲁棒性AI生成文本的检测，展示了人类文本与AI生成文本在内部维度上的差异。 |
| [^159] | [Optimal Transport Model Distributional Robustness.](http://arxiv.org/abs/2306.04178) | 本文提出了一种优化输运模型的分布鲁棒性框架，能够显著提高深度学习模型的鲁棒性，可灵活地将锐度感知纳入到单个模型、集成模型和贝叶斯神经网络的训练中。 |
| [^160] | [Resilient Constrained Learning.](http://arxiv.org/abs/2306.02426) | 本论文提出了一个名为“抗干扰约束学习”的方法来解决在部署机器学习解决方案时需要满足除了准确性以外的多个要求，并以平衡从放宽中获得的性能增益与用户定义的放宽成本之间的关系的方式放松学习约束。 |
| [^161] | [Joint Learning of Label and Environment Causal Independence for Graph Out-of-Distribution Generalization.](http://arxiv.org/abs/2306.01103) | 本文提出了一种考虑标签和环境因果独立性的方法来解决图形超出分布（OOD）泛化问题，通过敌对训练策略来联合优化属性以获得有效结果，实验证明LECI显着优于之前的方法。 |
| [^162] | [Initial Guessing Bias: How Untrained Networks Favor Some Classes.](http://arxiv.org/abs/2306.00809) | 本文提出了“初始猜测偏差”现象，即在未经过训练的神经网络中，由于架构选择的影响，模型往往会将所有预测指向同一个类别。该现象对架构选择和初始化有实际指导意义，并具有理论后果，例如节点置换对称性的崩溃和深度带来的非平凡差异。 |
| [^163] | [Precision-Recall Divergence Optimization for Generative Modeling with GANs and Normalizing Flows.](http://arxiv.org/abs/2305.18910) | 本文提出了一种通过优化Precision-Recall分歧来平衡生成模型图像质量和多样性的新训练方法，实现了用户定义的精确性和召回率权衡。 |
| [^164] | [Online Nonstochastic Model-Free Reinforcement Learning.](http://arxiv.org/abs/2305.17552) | 本论文研究了在线非随机无模型强化学习算法，针对动态或者具有对抗性的环境提出了一种以干扰信号为中心的策略类别，并开发了高效实用的优化算法。 |
| [^165] | [DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models.](http://arxiv.org/abs/2305.16381) | 本论文提出了DPOK，一种使用在线强化学习（RL）微调文本到图像扩散模型的方法。该方法在COCO数据集上实现了最先进的性能。 |
| [^166] | [Attentiveness to Answer Choices Doesn't Always Entail High QA Accuracy.](http://arxiv.org/abs/2305.14596) | 当大型语言模型应用于多项选择题时，其注意力往往会分散到许多无效的词汇符号上，这会导致模型真实性能的低估。本文提出了一种数学形式化方法来研究这种现象，并发现通过使用只含一个示例的上下文学习方法可以提高对有效选择的注意力。 |
| [^167] | [Robust Explanations for Deep Neural Networks via Pseudo Neural Tangent Kernel Surrogate Models.](http://arxiv.org/abs/2305.14585) | 本研究通过建立一个规范化的伪神经切线核，证明了它能够更好地与神经网络决策函数相关，比基于嵌入和影响的替代品更有效，并且从它创建的归因会更准确地选择被扰动的训练数据，从而证明了核线性模型是跨多个数据领域并有效的替代模型。 |
| [^168] | [Causal Discovery from Subsampled Time Series with Proxy Variables.](http://arxiv.org/abs/2305.05276) | 本研究提出了一种使用代理变量方法的无模型算法，可以从子采样时间序列中无需参数约束地识别整个因果结构。 |
| [^169] | [Surrogate Assisted Generation of Human-Robot Interaction Scenarios.](http://arxiv.org/abs/2304.13787) | 本文提出了基于替代模型的人机交互场景生成方法，可以高效合成多样化的挑战性数据集，以便评估和理解人机交互系统的优劣，可以在实际交互中重现这些场景。 |
| [^170] | [Local Energy Distribution Based Hyperparameter Determination for Stochastic Simulated Annealing.](http://arxiv.org/abs/2304.11839) | 本文提出了一种基于局部能量分布的随机模拟退火超参数确定方法，该方法通过中心极限定理估计局部能量的分布，将超参数搜索的时间复杂度从O(n^3)降低到O(1)，在解决最大割问题中的实验中表现良好。 |
| [^171] | [A Framework for Analyzing Online Cross-correlators using Price's Theorem and Piecewise-Linear Decomposition.](http://arxiv.org/abs/2304.09242) | 本文提出了一种使用非线性函数构建交叉相关器的方法，并使用Price定理和分段线性分解提出了一个数学框架来分析这种交叉相关器。 |
| [^172] | [Simple Sorting Criteria Help Find the Causal Order in Additive Noise Models.](http://arxiv.org/abs/2303.18211) | 文章探讨了加性噪声模型中找到因果顺序的方法。作者发现除了方差排序外，变量的决定系数$R^2$排序也可用于匹配已有方法的表现，且不受数据缩放的影响。 |
| [^173] | [Inferring networks from time series: a neural approach.](http://arxiv.org/abs/2303.18059) | 本论文提出了一种基于神经网络的快速计算方法，可以从时间序列数据中推断大型网络的相邻矩阵，并对不确定性进行量化，解决了网络推断问题的不足。 |
| [^174] | [HDformer: A Higher Dimensional Transformer for Diabetes Detection Utilizing Long Range Vascular Signals.](http://arxiv.org/abs/2303.11340) | 本研究提出了一种新的基于高维Transformer的架构HDformer，并利用长距离PPG信号进行糖尿病检测，其中提出了一种新的注意力模块TSA，成功将标记体积减少10倍以上，提高了模型的能力和效率。 |
| [^175] | [Making Batch Normalization Great in Federated Deep Learning.](http://arxiv.org/abs/2303.06530) | 本文研究了在联邦学习中使用批标准化和群组归一化的效果，发现在适当的处理下，批标准化可以在广泛的联邦学习设置中具有很高的竞争力，而且这不需要额外的训练或通信成本。 |
| [^176] | [Parallel Hybrid Networks: an interplay between quantum and classical neural networks.](http://arxiv.org/abs/2303.03227) | 本论文介绍了一种新颖的混合量子神经网络模型，通过在输入数据集时将数据同时传递给经典神经网络和量子电路，并线性组合两者的输出，以解决量子神经网络在拟合非谐波特征时的困难。在实验中，通过在合成数据集上的验证，证明了该模型的有效性。 |
| [^177] | [Population-based Evaluation in Repeated Rock-Paper-Scissors as a Benchmark for Multiagent Reinforcement Learning.](http://arxiv.org/abs/2303.03196) | 这项研究提出了一种基于重复剪刀石头布游戏的多Agent学习基准，展示了几种学习方法的泛化能力，为多Agent学习领域的研究提供了机会。 |
| [^178] | [Approximating Energy Market Clearing and Bidding With Model-Based Reinforcement Learning.](http://arxiv.org/abs/2303.01772) | 本文研究了基于模型的强化学习用于能源市场清算和出价的应用方法。通过用学习的OPF代理模型以及明确的市场规则替代传统计算方法，本方法极大地减少了训练时间并适用于市场设计和更现实地建模市场参与者。 |
| [^179] | [Penalising the biases in norm regularisation enforces sparsity.](http://arxiv.org/abs/2303.01353) | 本研究表明，控制神经网络参数的范数可以获得良好的泛化性能。对神经网络中偏差项的范数进行惩罚可以实现稀疏估计量。 |
| [^180] | [Evaluating Robustness and Uncertainty of Graph Models Under Structural Distributional Shifts.](http://arxiv.org/abs/2302.13875) | 该论文提出了一种基于图结构的多样化分布转换的方法，并且针对性地设计了数据集。实验结果表明这些分布转换对于现有的图模型具有挑战性。 |
| [^181] | [Learning Interpretable Low-dimensional Representation via Physical Symmetry.](http://arxiv.org/abs/2302.10890) | 通过使用物理对称性作为潜在空间的自洽约束条件，该研究展示了在音乐领域和计算机视觉领域，模型可以以无监督的方式学习出可解释的低维表示，例如线性音高和三维笛卡尔因素。 |
| [^182] | [Energy Transformer.](http://arxiv.org/abs/2302.07253) | 本研究将注意力机制、能量模型和联想记忆结合，提出了一种新颖的架构——能量变换器（ET），它通过特意设计的注意力层以最小化能量函数，用于表示标记之间的关系。 |
| [^183] | [Is This Loss Informative? Faster Text-to-Image Customization by Tracking Objective Dynamics.](http://arxiv.org/abs/2302.04841) | 本文研究了文本到图像个性化方法的训练动态，并提出了一种简单的早停准则来加快训练速度 |
| [^184] | [Eliciting User Preferences for Personalized Multi-Objective Decision Making through Comparative Feedback.](http://arxiv.org/abs/2302.03805) | 这项研究提出了一个多目标决策框架，通过比较不同用户的政策来学习用户对目标的偏好，并根据偏好计算出近似最优的个性化政策。 |
| [^185] | [Concept Algebra for Score-Based Conditional Models.](http://arxiv.org/abs/2302.03693) | 本文研究了基于分数的条件模型中学习表示的结构，并开发了一种数学形式化表达概念被编码为表示空间子空间的思想。利用这个方法，我们提出了一种简单的方法来识别给定概念对应的表示部分，并通过代数操作操纵模型所表达的概念。 |
| [^186] | [Flat Seeking Bayesian Neural Networks.](http://arxiv.org/abs/2302.02713) | 本文提出了一种扁平化贝叶斯神经网络的方法，该方法在后验推论中考虑了模型的扁平化性质，从而提升了模型的泛化能力和不确定性估计能力。 |
| [^187] | [A Reduction-based Framework for Sequential Decision Making with Delayed Feedback.](http://arxiv.org/abs/2302.01477) | 我们提出了一个基于规约的框架，可以将任何多批次算法转化为处理顺序决策中的随机延迟的高效算法。我们不仅在赌博机、表格型MDPs和表格型MGs方面取得了与现有结果相匹配或改进的成果，还首次对顺序决策中的延迟与函数逼近进行了研究。 |
| [^188] | [Constrained Online Two-stage Stochastic Optimization: Near Optimal Algorithms via Adversarial Learning.](http://arxiv.org/abs/2302.00997) | 在线两阶段随机优化算法的累计目标值最小化，同时保证长期平均第二阶段决策结果属于一个集合。采用对抗性学习算法从在线两阶段问题中开发在线算法，其遗憾界可以降至嵌入对抗性学习算法的遗憾界，并在各种设置下获得了新的结果。 |
| [^189] | [Improving and generalizing flow-based generative models with minibatch optimal transport.](http://arxiv.org/abs/2302.00482) | 这篇论文提出了一种称为广义条件流匹配（CFM）的技术，在连续正则化流（CNFs）的生成模型中无需模拟训练，极大提高了效率和稳定性。此外，论文还引入了最优传输CFM（OT-CFM）的变体，可以以无模拟方式计算动态OT，加速了推断过程。 |
| [^190] | [Active Uncertainty Reduction for Safe and Efficient Interaction Planning: A Shielding-Aware Dual Control Approach.](http://arxiv.org/abs/2302.00171) | 本文提出了一种基于隐式双控制模型的算法方法，用于实现交互式运动规划的主动不确定性减小。方法使用基于采样的随机动态规划近似，解决了交互式运动规划中的优化问题。 |
| [^191] | [LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain.](http://arxiv.org/abs/2301.13126) | LEXTREME是一个多语言和多任务的法律领域基准，该基准提供了11个数据集涵盖24种语言的测评，最佳模型（XLM-R large）在数据集和语言综合评分上均达到了61.3。这使得LEXTREME仍然具有挑战性并且有改进空间。 |
| [^192] | [On Learning Necessary and Sufficient Causal Graphs.](http://arxiv.org/abs/2301.12389) | 本文提出了一种学习必要和充分因果图的方法，用于发现与感兴趣结果相关的因果关系。 |
| [^193] | [Illumination Variation Correction Using Image Synthesis For Unsupervised Domain Adaptive Person Re-Identification.](http://arxiv.org/abs/2301.09702) | 本文提出了一个Synthesis Model Bank（SMB）来处理无监督领域自适应人物再识别中的光照变化。SMB包括卷积神经网络（CNN）和马氏距离矩阵，通过使用不同光照条件的合成数据进行训练，增强了光照变化的鲁棒性。 |
| [^194] | [Non-contact Respiratory Anomaly Detection using Infrared Light-wave Sensing.](http://arxiv.org/abs/2301.03713) | 本研究使用无接触红外光波传感技术，通过训练不同类型的呼吸模式来检测呼吸异常，并且通过验证数据的呼吸波形丢弃干扰数据，以实现安全、高效和无创的人体呼吸监测。 |
| [^195] | [Rethinking Explaining Graph Neural Networks via Non-parametric Subgraph Matching.](http://arxiv.org/abs/2301.02780) | 本文通过提出一种非参数子图匹配框架MatchExplainer，可以解决图神经网络的解释性问题。此框架将目标图与其他实例结合起来，通过最小化节点对应的距离来鉴别最关键的联合子结构，并提出了一种新的增强范式MatchDrop来解决误报采样问题。 |
| [^196] | [A Survey on Explainable Reinforcement Learning: Concepts, Algorithms, Challenges.](http://arxiv.org/abs/2211.06665) | 该综述调查了可解释性强化学习方法，介绍了模型解释、奖励解释、状态解释和任务解释方法，并探讨了解释强化学习的概念、算法和挑战。 |
| [^197] | [GmGM: a Fast Multi-Axis Gaussian Graphical Model.](http://arxiv.org/abs/2211.02920) | 本文介绍了一种快速的多轴高斯图形模型，用于构建稀疏图形表示。相比先前工作，我们的算法在每个轴上仅使用一次特征分解，实现了数量级的加速。该模型可以应用于大型多模态数据集，包括单细胞多组学数据。 |
| [^198] | [Entropic Neural Optimal Transport via Diffusion Processes.](http://arxiv.org/abs/2211.01156) | 这项研究提出了一种新的神经算法，用于计算连续概率分布之间的熵最优传输(EOT)计划，它具有端到端学习、快速推理和处理小值熵正则化系数的优点，可在大规模EOT任务中发挥出色的性能。 |
| [^199] | [Generalization Properties of NAS under Activation and Skip Connection Search.](http://arxiv.org/abs/2209.07238) | 本文研究了NAS在激活和跳跃连接搜索下的泛化性质，并提出了一种无需训练的基于理论的算法来选择性能最好的架构。 |
| [^200] | [Prioritizing Samples in Reinforcement Learning with Reducible Loss.](http://arxiv.org/abs/2208.10483) | 本文提出了一种在强化学习中基于可学习性的方法来优先选择样本，通过稳定降低样本的训练损失来定义样本的可学习性。实验证明，该方法相比于随机抽样和仅根据训练损失进行优先选择的方法更加稳健。 |
| [^201] | [On the Need and Applicability of Causality for Fair Machine Learning.](http://arxiv.org/abs/2207.04053) | 本论文探讨了因果关系在公平机器学习中的必要性和适用性，强调了非因果预测的社会影响和法律反歧视过程依赖于因果主张。同时讨论了在实际场景中应用因果关系所面临的挑战和限制，并提出了可能的解决方案。 |
| [^202] | [Assessing Privacy Leakage in Synthetic 3-D PET Imaging using Transversal GAN.](http://arxiv.org/abs/2206.06448) | 通过研究三维PET图像生成模型Transversal GAN（TrGAN），我们发现TrGAN的判别器易受到攻击，攻击者可以准确地识别出训练样本，并在有限的访问权限情况下推断出受保护数据的特征。 |
| [^203] | [Independent and Decentralized Learning in Markov Potential Games.](http://arxiv.org/abs/2205.14590) | 独立的去中心化学习在马尔科夫潜在博弈中有效，通过更新Q函数可以引导策略收敛到稳定的纳什平衡点。 |
| [^204] | [BagPipe: Accelerating Deep Recommendation Model Training.](http://arxiv.org/abs/2202.12429) | 本文提出了BagPipe，一种用于加速深度推荐模型训练的系统。该系统利用嵌入访问的特定结构，通过缓存和预取的方式优化训练，实现了对推荐模型的高效训练。 |
| [^205] | [SCORE: Spurious COrrelation REduction for Offline Reinforcement Learning.](http://arxiv.org/abs/2110.12468) | 本文提出了SCORE算法，用于离线强化学习中的虚假相关性降低。通过引入退火行为克隆正则化器，SCORE实现了SoTA性能，并消除了次优性中的虚假相关性。 |
| [^206] | [Derivative-free Alternating Projection Algorithms for General Nonconvex-Concave Minimax Problems.](http://arxiv.org/abs/2108.00473) | 本文提出针对非凸-凹极小极大问题的无导数交替投影算法，包括光滑问题的交替随机梯度投影算法（ZO-AGP），以及块状非光滑问题的分块交替随机近端梯度算法（ZO-BAPG）。这些算法具有较少的函数值估计和较高的迭代复杂度。 |
| [^207] | [Likelihood-Free Frequentist Inference: Confidence Sets with Correct Conditional Coverage.](http://arxiv.org/abs/2107.03920) | 本文提出了无似然假设下的频率学派推断（LF2I）框架，通过结合经典统计和现代机器学习，实现了构建具有正确条件覆盖的置信区间的实用程序和诊断方法，在包括宇宙学参数推断在内的多个例子中都实现了覆盖性质得到大幅改善。 |
| [^208] | [Corruption-robust exploration in episodic reinforcement learning.](http://arxiv.org/abs/1911.08689) | 该论文研究了强化学习中在奖励和转移概率两方面存在的对抗性腐败问题，并提出了一种能够解决腐败问题的高效算法，能够在没有腐败的情况下实现接近最优的后悔，并且能够适应未知水平的腐败。 |

# 详细

[^1]: 决策支持框架在家庭保健护理员分配中的应用：田纳西州HHC机构的案例研究

    Decision Support Framework for Home Health Caregiver Allocation: A Case Study of HHC Agency in Tennessee, USA. (arXiv:2311.00696v1 [cs.LG])

    [http://arxiv.org/abs/2311.00696](http://arxiv.org/abs/2311.00696)

    本研究针对家庭保健机构面临的护理员分配问题，提出了一种决策支持框架，通过考虑护理员对访问顺序的灵活性，旨在减少行驶里程、增加访问次数，并保持护理的连续性。

    

    人口老龄化是一个全球性的挑战，导致对老年人的医疗和社会服务需求增加。家庭保健护理（HHC）作为一种专门为这一人群提供服务的重要解决方案正逐渐兴起。鉴于对HHC的需求激增，有效地协调和管理护理员的分配至关重要，这对于预算优化的规划和确保提供高质量的护理至关重要。本研究回答了家庭保健机构面临的一个关键问题：“在护理员偏好灵活的访问顺序的情况下，如何优化他们的分配？”之前的研究提出了刚性的访问顺序，而我们的研究引入了一种决策支持框架，通过一种混合方法对护理员进行分配，考虑了访问顺序的灵活性，旨在减少行驶里程、增加每个规划周期的访问次数，并保持连续护理-这是衡量患者情况的关键指标。

    Population aging is a global challenge, leading to increased demand for healthcare and social services for the elderly. Home Health Care (HHC) emerges as a vital solution, specifically designed to serve this population segment. Given the surging demand for HHC, it's essential to coordinate and regulate caregiver allocation efficiently. This is crucial for both budget-optimized planning and ensuring the delivery of high-quality care. This research addresses a key question faced by home health agencies (HHAs): "How can caregiver allocation be optimized, especially when caregivers prefer flexibility in their visiting sequences?". While earlier studies proposed rigid visiting sequences, our study introduces a decision support framework that allocates caregivers through a hybrid method that considers the flexibility in visiting sequences and aims to reduce travel mileage, increase the number of visits per planning period, and maintain the continuity of care - a critical metric for patient s
    
[^2]: 软件存储库和机器学习在网络安全中的研究

    Software Repositories and Machine Learning Research in Cyber Security. (arXiv:2311.00691v1 [cs.SE])

    [http://arxiv.org/abs/2311.00691](http://arxiv.org/abs/2311.00691)

    该论文研究了网络安全领域中软件存储库和机器学习的应用。通过使用网络安全存储库以及主题建模和机器学习技术，可以在软件开发的早期阶段识别出关键的网络安全漏洞，为软件开发人员提供自动化的漏洞识别方法。

    

    在当今快速发展的技术环境和先进的软件开发中，网络安全攻击的增加已经成为一个紧迫的问题。在软件开发的各个阶段，整合强大的网络安全防御已经变得至关重要。在软件开发生命周期的初期阶段，特别是在需求阶段，识别关键的网络安全漏洞具有特殊的意义。通过利用网络安全存储库，如MITRE的常见攻击模式枚举和分类（CAPEC）以及常见漏洞和公开漏洞（CVE）数据库，已经尝试利用主题建模和机器学习来检测软件需求过程中的这些早期漏洞。过去的研究主题在尝试自动化漏洞识别方面取得了成功的结果，采用了一种无监督机器学习的混合方法。

    In today's rapidly evolving technological landscape and advanced software development, the rise in cyber security attacks has become a pressing concern. The integration of robust cyber security defenses has become essential across all phases of software development. It holds particular significance in identifying critical cyber security vulnerabilities at the initial stages of the software development life cycle, notably during the requirement phase. Through the utilization of cyber security repositories like The Common Attack Pattern Enumeration and Classification (CAPEC) from MITRE and the Common Vulnerabilities and Exposures (CVE) databases, attempts have been made to leverage topic modeling and machine learning for the detection of these early-stage vulnerabilities in the software requirements process. Past research themes have returned successful outcomes in attempting to automate vulnerability identification for software developers, employing a mixture of unsupervised machine lea
    
[^3]: 用户行为在视觉分析过程中的差异是什么？

    What User Behaviors Make the Differences During the Process of Visual Analytics?. (arXiv:2311.00690v1 [cs.HC])

    [http://arxiv.org/abs/2311.00690](http://arxiv.org/abs/2311.00690)

    本文通过研究用户行为的数据采集和时间序列分类方法分析了视觉分析过程中的用户行为差异，揭示了用户行为的不同特征。

    

    对视觉分析过程的理解可以从多个方面受益于可视化研究人员，包括改进可视化设计和开发先进的交互功能。然而，由于感知的复杂性和我们对相关用户行为的缺乏了解，用户行为的日志文件仍然难以分析。本文提出了一个关于用户行为的全面数据采集的研究，并结合时间序列分类方法进行分析。我们选择了一个经典的可视化应用，Covid-19数据分析，涵盖地理空间、时间序列和多属性的常见分析任务。我们的用户研究收集了关于多个可视化任务的用户行为，使用了两个可比较的系统，桌面和沉浸式可视化。我们总结了两个尺度上使用三种时间序列机器学习算法的分类结果，并探索了行为特征的影响。我们的结果揭示了用户行为的差异。

    The understanding of visual analytics process can benefit visualization researchers from multiple aspects, including improving visual designs and developing advanced interaction functions. However, the log files of user behaviors are still hard to analyze due to the complexity of sensemaking and our lack of knowledge on the related user behaviors. This work presents a study on a comprehensive data collection of user behaviors, and our analysis approach with time-series classification methods. We have chosen a classical visualization application, Covid-19 data analysis, with common analysis tasks covering geo-spatial, time-series and multi-attributes. Our user study collects user behaviors on a diverse set of visualization tasks with two comparable systems, desktop and immersive visualizations. We summarize the classification results with three time-series machine learning algorithms at two scales, and explore the influences of behavior features. Our results reveal that user behaviors c
    
[^4]: 通过使用语言模型模拟受众群体，改善人际沟通

    Improving Interpersonal Communication by Simulating Audiences with Language Models. (arXiv:2311.00687v1 [cs.AI])

    [http://arxiv.org/abs/2311.00687](http://arxiv.org/abs/2311.00687)

    本论文提出了一个基于大型语言模型（LLM）模拟的框架，通过探索解决方案空间、生成沟通候选以及模拟受众反应，来改善人际沟通。通过评估八个涵盖人际沟通基本过程的场景，展示了该框架的有效性。

    

    我们如何与他人进行沟通以实现自己的目标？我们利用先前的经验或他人的建议，或者通过预测对方的反应来构造候选表达。然而，我们的经验是有限和有偏见的，而且对潜在结果进行推理可能是困难且认知上具有挑战性的。本文中，我们探讨了如何利用大型语言模型（LLM）模拟来帮助我们更好地沟通。我们提出了探索-生成-模拟（EGS）框架，该框架接受任何一个个体与一个目标受众进行沟通的场景作为输入。EGS（1）通过生成与场景相关的多样化建议来探索解决方案空间，（2）生成以部分建议为条件的沟通候选，（3）模拟不同受众的反应，以确定最佳候选和建议的使用。我们在涵盖人际沟通十个基本过程的八个场景上评估了该框架。

    How do we communicate with others to achieve our goals? We use our prior experience or advice from others, or construct a candidate utterance by predicting how it will be received. However, our experiences are limited and biased, and reasoning about potential outcomes can be difficult and cognitively challenging. In this paper, we explore how we can leverage Large Language Model (LLM) simulations to help us communicate better. We propose the Explore-Generate-Simulate (EGS) framework, which takes as input any scenario where an individual is communicating to an audience with a goal they want to achieve. EGS (1) explores the solution space by producing a diverse set of advice relevant to the scenario, (2) generates communication candidates conditioned on subsets of the advice, and (3) simulates the reactions from various audiences to determine both the best candidate and advice to use. We evaluate the framework on eight scenarios spanning the ten fundamental processes of interpersonal com
    
[^5]: 注意力对齐和灵活的位置嵌入提高了Transformer长度外推能力

    Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation. (arXiv:2311.00684v1 [cs.CL])

    [http://arxiv.org/abs/2311.00684](http://arxiv.org/abs/2311.00684)

    本研究提出了两种通过温度缩放实现的注意力对齐策略，通过改善T5在长序列处理中的注意力分布问题，提高了其在语言建模、检索和多文档问答等任务中的长上下文利用能力。

    

    理想的长度可外推的Transformer语言模型可以处理比训练长度更长的序列而不需要进行长序列微调。这种长上下文利用能力高度依赖于灵活的位置嵌入设计。在调查现有大型预训练Transformer语言模型的灵活性时，我们发现T5系列值得更仔细研究，因为它的位置嵌入捕捉到了丰富而灵活的注意力模式。然而，T5存在着分散的注意力问题：输入序列越长，注意力分布就越平坦。为了缓解这个问题，我们提出了两种通过温度缩放实现的注意力对齐策略。我们的研究结果提高了T5在语言建模、检索和多文档问答方面的长上下文利用能力，而且不需要进行任何微调，这表明灵活的位置嵌入设计和注意力对齐对于Transformer长度外推至关重要。

    An ideal length-extrapolatable Transformer language model can handle sequences longer than the training length without any long sequence fine-tuning. Such long-context utilization capability highly relies on a flexible positional embedding design. Upon investigating the flexibility of existing large pre-trained Transformer language models, we find that the T5 family deserves a closer look, as its positional embeddings capture rich and flexible attention patterns. However, T5 suffers from the dispersed attention issue: the longer the input sequence, the flatter the attention distribution. To alleviate the issue, we propose two attention alignment strategies via temperature scaling. Our findings improve the long-context utilization capability of T5 on language modeling, retrieval, and multi-document question answering without any fine-tuning, suggesting that a flexible positional embedding design and attention alignment go a long way toward Transformer length extrapolation.\footnote{\url
    
[^6]: 基于深度学习的室温半导体辐射探测器中伽玛光子相互作用的分类算法

    Deep Learning-Based Classification of Gamma Photon Interactions in Room-Temperature Semiconductor Radiation Detectors. (arXiv:2311.00682v1 [physics.ins-det])

    [http://arxiv.org/abs/2311.00682](http://arxiv.org/abs/2311.00682)

    本论文介绍了一种名为CoPhNet的深度学习分类器，用于区分CdZnTeSe半导体探测器中的康普顿散射和光电相互作用的伽玛/ x射线光子。

    

    光子计数辐射探测器已经成为医学成像模态（如正电子发射断层扫描或计算机断层扫描）中重要的组成部分。最有前景的探测器之一是宽禁带室温半导体探测器，它依赖于伽玛/ x射线光子与探测器材料的相互作用，其中包括康普顿散射，导致单个光子的多次相互作用事件（MIPE）。对于具有康普顿散射和光电事件之间探测能量重叠的半导体探测器（如CdZnTeSe（CZTS）），使用传统的读出电子学或信号处理算法几乎不可能区分康普顿散射事件和光电事件。在这里，我们报道了一个名为CoPhNet的深度学习分类器，用于区分CdZnTeSe（CZTS）半导体探测器中的康普顿散射和光电相互作用的伽玛/ x射线光子。我们的CoPhNet模型是使用模拟数据进行训练的。

    Photon counting radiation detectors have become an integral part of medical imaging modalities such as Positron Emission Tomography or Computed Tomography. One of the most promising detectors is the wide bandgap room temperature semiconductor detectors, which depends on the interaction gamma/x-ray photons with the detector material involves Compton scattering which leads to multiple interaction photon events (MIPEs) of a single photon. For semiconductor detectors like CdZnTeSe (CZTS), which have a high overlap of detected energies between Compton and photoelectric events, it is nearly impossible to distinguish between Compton scattered events from photoelectric events using conventional readout electronics or signal processing algorithms. Herein, we report a deep learning classifier CoPhNet that distinguishes between Compton scattering and photoelectric interactions of gamma/x-ray photons with CdZnTeSe (CZTS) semiconductor detectors. Our CoPhNet model was trained using simulated data t
    
[^7]: 非线性规划中具有随机目标和约束的单循环算法的复杂性分析

    Complexity of Single Loop Algorithms for Nonlinear Programming with Stochastic Objective and Constraints. (arXiv:2311.00678v1 [math.OC])

    [http://arxiv.org/abs/2311.00678](http://arxiv.org/abs/2311.00678)

    该论文分析了解决具有随机目标和约束的非线性规划问题中单循环二次罚函数和增广Lagrangian算法的复杂性，研究了三种具有不同约束性质的情况。结果表明其中的两种情况是首个采用单循环算法并具有一定复杂度的方法。

    

    我们分析了用于解决具有功能等式约束的非凸优化问题的单循环二次罚函数和增广Lagrangian算法的复杂性。我们考虑了三种情况，在所有情况下，目标函数都是随机且平滑的，即对未知分布进行采样的期望。三种情况下等式约束的性质有所不同：第一种情况下确定性和线性，第二种情况下确定性、平滑和非线性，第三种情况下随机、平滑和非线性。利用方差减小技术改善复杂性。为了找到满足ε近似一阶条件的点，我们在第一种情况下需要复杂度为$\widetilde{O}(\varepsilon^{-3})$，第二种情况下需要复杂度为$\widetilde{O}(\varepsilon^{-4})$，第三种情况下需要复杂度为$\widetilde{O}(\varepsilon^{-5})$。对于第一种和第三种情况，这是第一种“单循环”类型的算法（同时也使用$O$

    We analyze the complexity of single-loop quadratic penalty and augmented Lagrangian algorithms for solving nonconvex optimization problems with functional equality constraints. We consider three cases, in all of which the objective is stochastic and smooth, that is, an expectation over an unknown distribution that is accessed by sampling. The nature of the equality constraints differs among the three cases: deterministic and linear in the first case, deterministic, smooth and nonlinear in the second case, and stochastic, smooth and nonlinear in the third case. Variance reduction techniques are used to improve the complexity. To find a point that satisfies $\varepsilon$-approximate first-order conditions, we require $\widetilde{O}(\varepsilon^{-3})$ complexity in the first case, $\widetilde{O}(\varepsilon^{-4})$ in the second case, and $\widetilde{O}(\varepsilon^{-5})$ in the third case. For the first and third cases, they are the first algorithms of "single loop" type (that also use $O
    
[^8]: Regret-Matching算法在游戏中的最终迭代收敛性质

    Last-Iterate Convergence Properties of Regret-Matching Algorithms in Games. (arXiv:2311.00676v1 [cs.GT])

    [http://arxiv.org/abs/2311.00676](http://arxiv.org/abs/2311.00676)

    这篇论文研究了基于遗憾匹配的算法在游戏中的最终迭代收敛性质。通过数值实验发现多个实际变体在简单的游戏中缺乏最终迭代收敛保证，而基于平滑技术的最近变体则具有最终迭代收敛性。

    

    基于遗憾匹配的算法，特别是遗憾匹配+ (RM+)及其变种，是解决大规模双人零和游戏的最流行方法。与具有零和游戏的强最终迭代和遍历收敛性质的算法（如乐观梯度上升）不同，我们对于遗憾匹配算法的最终迭代性质几乎一无所知。鉴于最终迭代收敛性对于数值优化和模拟现实世界中的游戏学习的重要性，本文研究了各种流行的RM+变体的最终迭代收敛性质。首先，我们通过数值实验证明，包括同时RM+、交替RM+和同时预测RM+在内的几个实际变体，甚至在简单的3x3游戏中也缺乏最终迭代收敛保证。然后，我们证明了这些算法的最近变体，基于平滑技术得到了最终迭代收敛性。

    Algorithms based on regret matching, specifically regret matching$^+$ (RM$^+$), and its variants are the most popular approaches for solving large-scale two-player zero-sum games in practice. Unlike algorithms such as optimistic gradient descent ascent, which have strong last-iterate and ergodic convergence properties for zero-sum games, virtually nothing is known about the last-iterate properties of regret-matching algorithms. Given the importance of last-iterate convergence for numerical optimization reasons and relevance as modeling real-word learning in games, in this paper, we study the last-iterate convergence properties of various popular variants of RM$^+$. First, we show numerically that several practical variants such as simultaneous RM$^+$, alternating RM$^+$, and simultaneous predictive RM$^+$, all lack last-iterate convergence guarantees even on a simple $3\times 3$ game. We then prove that recent variants of these algorithms based on a smoothing technique do enjoy last-it
    
[^9]: 通过协方差矩阵的Cholesky分解恢复具有潜变量的线性因果模型

    Recovering Linear Causal Models with Latent Variables via Cholesky Factorization of Covariance Matrix. (arXiv:2311.00674v1 [stat.ML])

    [http://arxiv.org/abs/2311.00674](http://arxiv.org/abs/2311.00674)

    本文提出了一种通过Cholesky分解恢复具有潜变量的线性因果模型的方法，该方法在速度和性能上超过了之前的方法，并具有精确恢复的理论保证。

    

    通过从观测数据中恢复有向无环图（DAG）结构来发现因果关系是一个众所周知的具有挑战性的组合优化问题。当存在潜变量时，该问题变得更加困难。本文首先提出了一种基于观测数据的协方差矩阵的Cholesky分解的DAG结构恢复算法。该算法快速易实现，并在理论上保证了精确恢复。在合成和真实世界数据集上，该算法比之前的方法快得多，达到了最先进的性能。此外，在等误差方差假设下，我们将优化过程与基于Cholesky分解的算法结合起来，处理具有潜变量的DAG恢复问题。数值模拟表明，在大多数情况下，修正的“Cholesky + 优化”算法能够恢复出真实的图，并且优于其他方法。

    Discovering the causal relationship via recovering the directed acyclic graph (DAG) structure from the observed data is a well-known challenging combinatorial problem. When there are latent variables, the problem becomes even more difficult. In this paper, we first propose a DAG structure recovering algorithm, which is based on the Cholesky factorization of the covariance matrix of the observed data. The algorithm is fast and easy to implement and has theoretical grantees for exact recovery. On synthetic and real-world datasets, the algorithm is significantly faster than previous methods and achieves the state-of-the-art performance. Furthermore, under the equal error variances assumption, we incorporate an optimization procedure into the Cholesky factorization based algorithm to handle the DAG recovering problem with latent variables. Numerical simulations show that the modified "Cholesky + optimization" algorithm is able to recover the ground truth graph in most cases and outperforms
    
[^10]: 潜在空间翻译通过语义对齐

    Latent Space Translation via Semantic Alignment. (arXiv:2311.00664v1 [cs.LG])

    [http://arxiv.org/abs/2311.00664](http://arxiv.org/abs/2311.00664)

    本论文研究了潜在空间的翻译问题。通过简单的变换，可以将不同神经模型学到的表示翻译到其他预训练网络中。这种方法能够有效地拼接编码器和解码器，并在各种实验设置中得到验证。

    

    虽然不同的神经模型在接触到语义相关的数据时往往会展现出相似的潜在空间，但这种内在的相似性并不总是立即可辨。为了更好地理解这一现象，我们的工作展示了如何通过比以前认为的更简单的变换将从这些神经模块学到的表示翻译到不同的预训练网络之间。这种方法的优势在于能够使用标准的、通用的代数程序来估计这些变换，并且这些程序具有封闭形式的解。我们的方法直接估计两个给定潜在空间之间的转换，从而实现了有效的编码器和解码器的拼接而无需额外训练。我们在不同的实验设置中广泛验证了这种翻译过程的适应性：包括各种训练数据、领域、架构（如ResNet、CNN、ViT）以及多种下游任务（分类、重构）。

    While different neural models often exhibit latent spaces that are alike when exposed to semantically related data, this intrinsic similarity is not always immediately discernible. Towards a better understanding of this phenomenon, our work shows how representations learned from these neural modules can be translated between different pre-trained networks via simpler transformations than previously thought. An advantage of this approach is the ability to estimate these transformations using standard, well-understood algebraic procedures that have closed-form solutions. Our method directly estimates a transformation between two given latent spaces, thereby enabling effective stitching of encoders and decoders without additional training. We extensively validate the adaptability of this translation procedure in different experimental settings: across various trainings, domains, architectures (e.g., ResNet, CNN, ViT), and in multiple downstream tasks (classification, reconstruction). Nota
    
[^11]: 通过线图转换在线估计图边缘信号

    Online Signal Estimation on the Graph Edges via Line Graph Transformation. (arXiv:2311.00656v1 [eess.SP])

    [http://arxiv.org/abs/2311.00656](http://arxiv.org/abs/2311.00656)

    该论文提出了一种在线时间变化图边缘信号预测算法，利用线图转换边缘信号为边到顶点对偶节点，使得信号可以使用已有的GSP概念进行处理。

    

    我们提出了线图归一化最小均方(LGNLMS)算法，用于在线时间变化图边缘信号的预测。LGNLMS利用线图将图边缘信号转换为其边到顶点对偶节点。这使得边缘信号可以使用已建立的GSP概念进行处理，而无需在图边缘上重新定义它们。

    We propose the Line Graph Normalized Least Mean Square (LGNLMS) algorithm for online time-varying graph edge signals prediction. LGNLMS utilizes the Line Graph to transform graph edge signals into the node of its edge-to-vertex dual. This enables edge signals to be processed using established GSP concepts without redefining them on graph edges.
    
[^12]: FAIRLABEL：修正标签中的偏见

    FAIRLABEL: Correcting Bias in Labels. (arXiv:2311.00638v1 [cs.LG])

    [http://arxiv.org/abs/2311.00638](http://arxiv.org/abs/2311.00638)

    FAIRLABEL是一种检测和纠正标签中偏见的算法，其目标是在保持高准确率的同时减少群体间的不平等影响。通过应用于合成数据集和基准数据集，验证结果显示FAIRLABEL在标签修正方面的正确率较基准模型提高了14.8%, 在不平等影响比率方面达到了54.2%的增长。

    

    有多种算法可以衡量机器学习模型的公平性。这些方法的一个基本假设是，真实数据是公平或无偏的。然而，在现实世界的数据集中，真实数据往往包含历史和社会偏见和歧视的数据。在这些数据集上训练的模型将继承并传播偏见到模型输出中。我们提出了一种名为FAIRLABEL的算法，用于检测和纠正标签中的偏见。FAIRLABEL的目标是在保持高准确率的同时减少群体间的不平等影响（DI）。我们提出了度量偏见修正质量的指标，并在合成数据集上验证了FAIRLABEL的正确性，结果表明标签修正的正确率为86.7%，而基准模型为71.9%。我们还将FAIRLABEL应用于UCI Adult、German Credit Risk和Compas数据集等基准数据集，结果显示不平等影响比率最多增加了54.2%。

    There are several algorithms for measuring fairness of ML models. A fundamental assumption in these approaches is that the ground truth is fair or unbiased. In real-world datasets, however, the ground truth often contains data that is a result of historical and societal biases and discrimination. Models trained on these datasets will inherit and propagate the biases to the model outputs. We propose FAIRLABEL, an algorithm which detects and corrects biases in labels. The goal of FAIRLABELis to reduce the Disparate Impact (DI) across groups while maintaining high accuracy in predictions. We propose metrics to measure the quality of bias correction and validate FAIRLABEL on synthetic datasets and show that the label correction is correct 86.7% of the time vs. 71.9% for a baseline model. We also apply FAIRLABEL on benchmark datasets such as UCI Adult, German Credit Risk, and Compas datasets and show that the Disparate Impact Ratio increases by as much as 54.2%.
    
[^13]: Kronecker-Factored Approximate Curvature for Modern Neural Network Architectures（现代神经网络架构的克罗内克近似曲率）

    Kronecker-Factored Approximate Curvature for Modern Neural Network Architectures. (arXiv:2311.00636v1 [cs.LG])

    [http://arxiv.org/abs/2311.00636](http://arxiv.org/abs/2311.00636)

    本篇论文提出了一种用于现代神经网络架构的克罗内克近似曲率算法，可以加速神经网络训练和减少计算成本。作者发现了两种具有线性权重共享层不同设置，并证明了相应设置下的K-FAC算法的精确性。K-FAC-reduce通常比K-FAC-expand更快，可以用于加速自动超参数选择。

    

    许多现代神经网络架构的核心组件，如transformers、卷积或图神经网络，可以表达为具有“权重共享”的线性层。克罗内克近似曲率（K-FAC）是一种二阶优化方法，已显示出加速神经网络训练并减少计算成本的潜力。然而，目前还没有将其应用于通用的架构的框架，特别是具有线性权重共享层的架构。在这项工作中，我们确定了具有线性权重共享层的两种不同设置，这促使了两种K-FAC的变体——“扩展”和“减少”。我们展示了对于具有相应设置的深度线性网络，它们是精确的。值得注意的是，K-FAC-reduce通常比K-FAC-expand更快，我们利用它来加速通过优化Wide ResNet的边际似然来选择自动超参数。最后，我们观察到在

    The core components of many modern neural network architectures, such as transformers, convolutional, or graph neural networks, can be expressed as linear layers with $\textit{weight-sharing}$. Kronecker-Factored Approximate Curvature (K-FAC), a second-order optimisation method, has shown promise to speed up neural network training and thereby reduce computational costs. However, there is currently no framework to apply it to generic architectures, specifically ones with linear weight-sharing layers. In this work, we identify two different settings of linear weight-sharing layers which motivate two flavours of K-FAC -- $\textit{expand}$ and $\textit{reduce}$. We show that they are exact for deep linear networks with weight-sharing in their respective setting. Notably, K-FAC-reduce is generally faster than K-FAC-expand, which we leverage to speed up automatic hyperparameter selection via optimising the marginal likelihood for a Wide ResNet. Finally, we observe little difference between 
    
[^14]: 多注释者数据的损失建模

    Loss Modeling for Multi-Annotator Datasets. (arXiv:2311.00619v1 [cs.LG])

    [http://arxiv.org/abs/2311.00619](http://arxiv.org/abs/2311.00619)

    该论文提出了一种通过利用多任务学习和基于损失的标签修正来学习多注释者数据的准确表示的方法。通过这种方法，可以有效地分离赞同和不赞同的注释，并且在单一或多注释者设置下改善预测性能。该方法还显示出对主观数据的额外标签噪声具有鲁棒性。

    

    在公正性方面，考虑到数据集中所有注释者的意见至关重要。然而，在注释大型数据集时，个别注释者经常会提供数千个评分，这可能导致疲劳。此外，这些注释过程可能会持续多天，可能导致对注释者的意见随时间的不准确表示。为了解决这个问题，我们提出利用多任务学习和基于损失的标签修正来学习更准确的多样意见表示。我们展示了使用我们新颖的公式，我们可以清楚地分离赞同和不赞同的注释。此外，我们证明了这种修改可以改善单一或多注释者设置下的预测性能。最后，我们证明了该方法对应用于主观数据的额外标签噪声仍然具有稳健性。

    Accounting for the opinions of all annotators of a dataset is critical for fairness. However, when annotating large datasets, individual annotators will frequently provide thousands of ratings which can lead to fatigue. Additionally, these annotation processes can occur over multiple days which can lead to an inaccurate representation of an annotator's opinion over time. To combat this, we propose to learn a more accurate representation of diverse opinions by utilizing multitask learning in conjunction with loss-based label correction. We show that using our novel formulation, we can cleanly separate agreeing and disagreeing annotations. Furthermore, we demonstrate that this modification can improve prediction performance in a single or multi-annotator setting. Lastly, we show that this method remains robust to additional label noise that is applied to subjective data.
    
[^15]: 用扩散模型和导向梯度实现可控音乐制作

    Controllable Music Production with Diffusion Models and Guidance Gradients. (arXiv:2311.00613v1 [cs.SD])

    [http://arxiv.org/abs/2311.00613](http://arxiv.org/abs/2311.00613)

    本论文介绍了一种使用扩散模型和导向梯度的方法，可以实现可控音乐制作，包括音频的延续、修补和再生，以及风格特征转移等任务。

    

    我们展示了如何使用扩散模型的条件生成来处理音乐制作中的各种现实任务，包括音乐音频的延续、修补和再生、在两个不同音乐曲目之间创建平滑的过渡以及将所需的风格特征转移到现有音频片段中。我们通过在采样时应用导向来实现这一目标，在一个简单的框架中支持重建和分类损失，或者两者的任意组合。这种方法确保生成的音频可以匹配其周围的上下文，或者符合相对于任何适当的预训练分类器或嵌入模型指定的类分布或潜在表示。

    We demonstrate how conditional generation from diffusion models can be used to tackle a variety of realistic tasks in the production of music in 44.1kHz stereo audio with sampling-time guidance. The scenarios we consider include continuation, inpainting and regeneration of musical audio, the creation of smooth transitions between two different music tracks, and the transfer of desired stylistic characteristics to existing audio clips. We achieve this by applying guidance at sampling time in a simple framework that supports both reconstruction and classification losses, or any combination of the two. This approach ensures that generated audio can match its surrounding context, or conform to a class distribution or latent representation specified relative to any suitable pre-trained classifier or embedding model.
    
[^16]: 基于协同过滤和课程依赖性的两阶段模型的课程推荐

    A Collaborative Filtering-Based Two Stage Model with Item Dependency for Course Recommendation. (arXiv:2311.00612v1 [cs.IR])

    [http://arxiv.org/abs/2311.00612](http://arxiv.org/abs/2311.00612)

    本论文提出了一种基于协同过滤和课程依赖性的两阶段模型的课程推荐方法，解决了缺乏评分和元数据、课程注册分布不均衡以及课程依赖建模的挑战，并在真实世界数据集上实现了0.97的AUC得分。

    

    推荐系统已经研究了几十年，提出了许多有前景的模型。其中，协同过滤（CF）模型由于在推荐中具有高准确性并消除了隐私问题，被认为是最成功的一种。本文将CF模型扩展到课程推荐任务中。我们指出了将现有的CF模型应用于构建课程推荐引擎时面临的几个挑战，包括缺乏评分和元数据，课程注册分布不均衡以及课程依赖建模的需求。然后，我们提出了几个解决这些挑战的想法。最终，我们将基于课程依赖性正则化的两阶段CF模型与基于课程转换网络的图形推荐器相结合，实现了0.97的AUC得分，并使用真实世界数据集进行了验证。

    Recommender systems have been studied for decades with numerous promising models been proposed. Among them, Collaborative Filtering (CF) models are arguably the most successful one due to its high accuracy in recommendation and elimination of privacy-concerned personal meta-data from training. This paper extends the usage of CF-based model to the task of course recommendation. We point out several challenges in applying the existing CF-models to build a course recommendation engine, including the lack of rating and meta-data, the imbalance of course registration distribution, and the demand of course dependency modeling. We then propose several ideas to address these challenges. Eventually, we combine a two-stage CF model regularized by course dependency with a graph-based recommender based on course-transition network, to achieve AUC as high as 0.97 with a real-world dataset.
    
[^17]: 使用自适应随机邻域的结构学习MCMC算法

    Structure Learning with Adaptive Random Neighborhood Informed MCMC. (arXiv:2311.00599v1 [cs.LG])

    [http://arxiv.org/abs/2311.00599](http://arxiv.org/abs/2311.00599)

    本文引入了一种新颖的MCMC采样器PARNI-DAG，用于全贝叶斯方法下基于观测数据的结构学习问题。PARNI-DAG通过局部信息化的自适应随机邻域提议进行高效的DAG采样，并具有更好的混合性质。为了与节点数量更好地扩展，PARNI-DAG与通过一些基于约束或评分算法得到的骨架图的参数预调整过程相结合。得益于这些新颖的特性，PARNI-DAG快速收敛到高概率区域，并且在高维设置中的节点之间存在高相关性时不太容易陷入局部模式。

    

    本文引入了一种新颖的MCMC采样器PARNI-DAG，用于全贝叶斯方法下基于观测数据的结构学习问题。在因果充分性假设下，该算法允许从有向无环图（DAGs）的后验分布中进行近似采样。PARNI-DAG通过局部信息化的自适应随机邻域提议进行高效的DAG采样，并具有更好的混合性质。此外，为了与节点数量更好地扩展，我们将PARNI-DAG与通过一些基于约束或评分算法得到的骨架图的参数预调整过程相结合。得益于这些新颖的特性，PARNI-DAG快速收敛到高概率区域，并且在高维设置中的节点之间存在高相关性时不太容易陷入局部模式。

    In this paper, we introduce a novel MCMC sampler, PARNI-DAG, for a fully-Bayesian approach to the problem of structure learning under observational data. Under the assumption of causal sufficiency, the algorithm allows for approximate sampling directly from the posterior distribution on Directed Acyclic Graphs (DAGs). PARNI-DAG performs efficient sampling of DAGs via locally informed, adaptive random neighborhood proposal that results in better mixing properties. In addition, to ensure better scalability with the number of nodes, we couple PARNI-DAG with a pre-tuning procedure of the sampler's parameters that exploits a skeleton graph derived through some constraint-based or scoring-based algorithms. Thanks to these novel features, PARNI-DAG quickly converges to high-probability regions and is less likely to get stuck in local modes in the presence of high correlation between nodes in high-dimensional settings. After introducing the technical novelties in PARNI-DAG, we empirically demo
    
[^18]: 重新思考具有随机支持的概率编程的变分推断

    Rethinking Variational Inference for Probabilistic Programs with Stochastic Support. (arXiv:2311.00594v1 [cs.LG])

    [http://arxiv.org/abs/2311.00594](http://arxiv.org/abs/2311.00594)

    本文提出了一种新的变分推断方法SDVI，可以用于具有随机支持的概率编程。通过将程序分解为具有静态支持的子程序，并为每个子程序构建独立的变分指导，SDVI在推断性能方面取得了显著改进。

    

    本文介绍了支持分解变分推断（SDVI），一种面向具有随机支持的概率编程的新的变分推断方法。现有方法在这个问题上依赖于在逐变量的基础上设计单个全局变分指导，同时保持原始程序的随机控制流。SDVI相反，将程序分解为具有静态支持的子程序，然后自动构建每个子指导的独立子指导。这种分解显著有助于构建适合的变分族，从而进一步提高推断性能。

    We introduce Support Decomposition Variational Inference (SDVI), a new variational inference (VI) approach for probabilistic programs with stochastic support. Existing approaches to this problem rely on designing a single global variational guide on a variable-by-variable basis, while maintaining the stochastic control flow of the original program. SDVI instead breaks the program down into sub-programs with static support, before automatically building separate sub-guides for each. This decomposition significantly aids in the construction of suitable variational families, enabling, in turn, substantial improvements in inference performance.
    
[^19]: Coop: 内存不是商品

    Coop: Memory is not a Commodity. (arXiv:2311.00591v1 [cs.LG])

    [http://arxiv.org/abs/2311.00591](http://arxiv.org/abs/2311.00591)

    本论文提出了一种名为Coop的方法，针对深度学习框架中的内存系统问题，通过驱逐连续张量和优化张量分配，以降低再材料化成本。

    

    张量再材料化技术允许在有限的内存预算下训练深度神经网络(DNNs)，通过在需要时检查点模型并根据需要重新计算被驱逐的张量。然而，现有的张量再材料化技术忽视了深度学习框架中的内存系统，并假设不同地址的空闲内存块是相同的。在这个错误的假设下，不连续的张量被驱逐，其中一些不用于分配新的张量。这导致严重的内存碎片化，增加了潜在再材料化的成本。为了解决这个问题，我们提出了在滑动窗口内驱逐张量的方法，以确保所有驱逐都是连续的并且立即使用。此外，我们提出了廉价的张量分区和可重算的就地优化张量分配来进一步降低再材料化成本。我们将我们的方法命名为Coop，因为它是张量分配和张量再材料化的协同优化。

    Tensor rematerialization allows the training of deep neural networks (DNNs) under limited memory budgets by checkpointing the models and recomputing the evicted tensors as needed. However, the existing tensor rematerialization techniques overlook the memory system in deep learning frameworks and implicitly assume that free memory blocks at different addresses are identical. Under this flawed assumption, discontiguous tensors are evicted, among which some are not used to allocate the new tensor. This leads to severe memory fragmentation and increases the cost of potential rematerializations. To address this issue, we propose to evict tensors within a sliding window to ensure all evictions are contiguous and are immediately used. Furthermore, we proposed cheap tensor partitioning and recomputable in-place to further reduce the rematerialization cost by optimizing the tensor allocation. We named our method Coop as it is a co-optimization of tensor allocation and tensor rematerialization. 
    
[^20]: 灵活的尾部用于归一化流，应用于金融回报数据的建模

    Flexible Tails for Normalising Flows, with Application to the Modelling of Financial Return Data. (arXiv:2311.00580v1 [stat.ML])

    [http://arxiv.org/abs/2311.00580](http://arxiv.org/abs/2311.00580)

    本文提出了一种灵活的尾部转换方法，可以用于归一化流来近似金融回报的重尾分布，能够捕捉可能出现在数据中的极端冲击。

    

    我们提出了一种能够改变分布尾部特性的转换方法，受到极值理论的启发，可以作为归一化流中的一层，来近似多变量重尾分布。我们将这种方法应用于金融回报建模，捕捉可能出现在此类数据中的极端冲击。训练好的模型可以直接用于生成可能的极端回报的合成数据集。

    We propose a transformation capable of altering the tail properties of a distribution, motivated by extreme value theory, which can be used as a layer in a normalizing flow to approximate multivariate heavy tailed distributions. We apply this approach to model financial returns, capturing potentially extreme shocks that arise in such data. The trained models can be used directly to generate new synthetic sets of potentially extreme returns
    
[^21]: 通过数据流推理加速器中的侧信道分析揭示CNN架构

    Revealing CNN Architectures via Side-Channel Analysis in Dataflow-based Inference Accelerators. (arXiv:2311.00579v1 [cs.CR])

    [http://arxiv.org/abs/2311.00579](http://arxiv.org/abs/2311.00579)

    本文通过评估数据流加速器上的侧信道信息，提出了一种攻击方法来恢复CNN模型的架构。该攻击利用了数据流映射的数据重用以及架构线索，成功恢复了流行的CNN模型Lenet，Alexnet和VGGnet16的结构。

    

    卷积神经网络（CNN）广泛应用于各个领域。最近在基于数据流的CNN加速器的进展使得CNN推理可以在资源有限的边缘设备上进行。这些数据流加速器利用卷积层的固有数据重用来高效处理CNN模型。隐藏CNN模型的架构对于隐私和安全至关重要。本文评估了基于内存的侧信道信息，以从数据流加速器中恢复CNN架构。所提出的攻击利用了CNN加速器上数据流映射的空间和时间数据重用以及架构线索来恢复CNN模型的结构。实验结果表明，我们提出的侧信道攻击可以恢复流行的CNN模型Lenet，Alexnet和VGGnet16的结构。

    Convolution Neural Networks (CNNs) are widely used in various domains. Recent advances in dataflow-based CNN accelerators have enabled CNN inference in resource-constrained edge devices. These dataflow accelerators utilize inherent data reuse of convolution layers to process CNN models efficiently. Concealing the architecture of CNN models is critical for privacy and security. This paper evaluates memory-based side-channel information to recover CNN architectures from dataflow-based CNN inference accelerators. The proposed attack exploits spatial and temporal data reuse of the dataflow mapping on CNN accelerators and architectural hints to recover the structure of CNN models. Experimental results demonstrate that our proposed side-channel attack can recover the structures of popular CNN models, namely Lenet, Alexnet, and VGGnet16.
    
[^22]: 用于改善因果物理信息神经网络在梁模拟中的泛化能力的迁移学习

    Transfer learning for improved generalizability in causal physics-informed neural networks for beam simulations. (arXiv:2311.00578v1 [cs.LG])

    [http://arxiv.org/abs/2311.00578](http://arxiv.org/abs/2311.00578)

    本文介绍了一种用于模拟弹性基础上梁动力学的新方法，通过迁移学习和因果性物理信息神经网络框架，提高了问题的泛化能力，并在数值实验中验证了其有效性。

    

    本文介绍了一种用于模拟弹性基础上梁动力学的新方法。具体而言，在因果性物理信息神经网络（PINN）框架内利用迁移学习方法模拟了Winkler基础上的Euler-Bernoulli梁模型和Timoshenko梁模型。传统的PINNS在处理大空时时间域上的问题时会遇到挑战，即使对于具有封闭解的问题也是如此。为了克服这一限制，采用因果性物理信息神经网络损失函数，有效捕捉底层的物理学。然而，观察到因果性PINN缺乏泛化能力。我们建议使用类似问题的解决方案，而不是从头开始训练，通过采用迁移学习并遵循因果性来加快收敛速度并确保在各种场景下获得准确的结果。在Euler-Bernoulli梁上的数值实验突出了所提方法的有效性。

    This paper introduces a novel methodology for simulating the dynamics of beams on elastic foundations. Specifically, Euler-Bernoulli and Timoshenko beam models on the Winkler foundation are simulated using a transfer learning approach within a causality-respecting physics-informed neural network (PINN) framework. Conventional PINNs encounter challenges in handling large space-time domains, even for problems with closed-form analytical solutions. A causality-respecting PINN loss function is employed to overcome this limitation, effectively capturing the underlying physics. However, it is observed that the causality-respecting PINN lacks generalizability. We propose using solutions to similar problems instead of training from scratch by employing transfer learning while adhering to causality to accelerate convergence and ensure accurate results across diverse scenarios. Numerical experiments on the Euler-Bernoulli beam highlight the efficacy of the proposed approach for various initial c
    
[^23]: 通过正则化和聚类联合分配森林进行个性化分配至多个治疗组

    Personalized Assignment to One of Many Treatment Arms via Regularized and Clustered Joint Assignment Forests. (arXiv:2311.00577v1 [stat.ML])

    [http://arxiv.org/abs/2311.00577](http://arxiv.org/abs/2311.00577)

    提出了一种个性化分配至多个治疗组的方法，通过正则化和聚类优化治疗分配，实现了更好的效果估计和个性化效益。

    

    我们考虑从随机对照试验中学习个性化的分配至多个治疗组。由于过多的方差，对于每个治疗组分别估计异质治疗效果的标准方法在这种情况下可能表现不佳。相反，我们提出了一种汇总治疗组信息的方法：首先，我们考虑基于贪婪递归分区的正则化森林分配算法，该算法可缩小不同治疗组之间的效果估计。其次，我们通过聚类方案将治疗组与具有一致相似结果的组合起来，增强了我们的算法。在模拟研究中，我们将这些方法与分别预测每个治疗组结果的方法进行了比较，并记录了通过正则化和聚类直接优化治疗分配带来的收益。在一个理论模型中，我们说明治疗组数量较多时找到最佳组的困难，而通过正则化和聚类可以实现个性化的明显效益。

    We consider learning personalized assignments to one of many treatment arms from a randomized controlled trial. Standard methods that estimate heterogeneous treatment effects separately for each arm may perform poorly in this case due to excess variance. We instead propose methods that pool information across treatment arms: First, we consider a regularized forest-based assignment algorithm based on greedy recursive partitioning that shrinks effect estimates across arms. Second, we augment our algorithm by a clustering scheme that combines treatment arms with consistently similar outcomes. In a simulation study, we compare the performance of these approaches to predicting arm-wise outcomes separately, and document gains of directly optimizing the treatment assignment with regularization and clustering. In a theoretical model, we illustrate how a high number of treatment arms makes finding the best arm hard, while we can achieve sizable utility gains from personalization by regularized 
    
[^24]: 在建模非平稳数据时利用整体-局部尺度结构的在线学生-t过程

    Online Student-$t$ Processes with an Overall-local Scale Structure for Modelling Non-stationary Data. (arXiv:2311.00564v1 [stat.ML])

    [http://arxiv.org/abs/2311.00564](http://arxiv.org/abs/2311.00564)

    我们提出了一种整体-局部尺度结构的贝叶斯学生-t过程混合模型，可用于建模非平稳数据，通过实时接收数据进行在线推断。在真实世界数据集上的对比实验证明了我们方法的优越性。

    

    时间相关的数据通常表现出非平稳性和重尾误差等特征，这些特征不适合采用常见模型所使用的假设进行建模。因此，需要更灵活的方法来处理这些问题。为此，我们提出了一种贝叶斯学生-t过程混合模型，其协方差具有整体-局部尺度结构。此外，我们使用顺序蒙特卡洛（SMC）采样器进行在线推断，以实时接收数据。通过在真实世界数据集上展示我们提出的方法相对于典型的基于高斯过程的模型的卓越性能，以证明使用学生-t过程混合模型的必要性。

    Time-dependent data often exhibit characteristics, such as non-stationarity and heavy-tailed errors, that would be inappropriate to model with the typical assumptions used in popular models. Thus, more flexible approaches are required to be able to accommodate such issues. To this end, we propose a Bayesian mixture of student-$t$ processes with an overall-local scale structure for the covariance. Moreover, we use a sequential Monte Carlo (SMC) sampler in order to perform online inference as data arrive in real-time. We demonstrate the superiority of our proposed approach compared to typical Gaussian process-based models on real-world data sets in order to prove the necessity of using mixtures of student-$t$ processes.
    
[^25]: 学习通过多梯度进行多目标优化

    Learning to optimize by multi-gradient for multi-objective optimization. (arXiv:2311.00559v1 [cs.LG])

    [http://arxiv.org/abs/2311.00559](http://arxiv.org/abs/2311.00559)

    本文介绍了一种基于自动学习的新的多目标优化方法，通过多梯度学习优化（ML2O）来实现优化过程，该方法利用当前步骤的信息和历史迭代轨迹数据来获取局部和全局知识，同时引入守护机制（GML2O）以确保生成的迭代序列收敛到Pareto关键点。

    

    人工智能（AI）在科学领域的发展导致了学习为基础的研究范式的出现，需要对多目标优化（MOO）方法的设计进行重新评估。新一代的MOO方法应该根植于自动学习而不是手动设计。本文介绍了一种新的自动学习范式用于优化MOO问题，并提出了一种多梯度学习优化（ML2O）方法，该方法通过自动学习多个梯度到更新方向的映射来更新生成器。作为一种基于学习的方法，ML2O通过利用当前步骤的信息获取局部景观知识，并结合从历史迭代轨迹数据中提取的全局经验。通过引入一种新的守护机制，我们提出了一种守护式多梯度学习优化（GML2O）方法，并证明了由GML2O生成的迭代序列收敛到Pareto关键点。

    The development of artificial intelligence (AI) for science has led to the emergence of learning-based research paradigms, necessitating a compelling reevaluation of the design of multi-objective optimization (MOO) methods. The new generation MOO methods should be rooted in automated learning rather than manual design. In this paper, we introduce a new automatic learning paradigm for optimizing MOO problems, and propose a multi-gradient learning to optimize (ML2O) method, which automatically learns a generator (or mappings) from multiple gradients to update directions. As a learning-based method, ML2O acquires knowledge of local landscapes by leveraging information from the current step and incorporates global experience extracted from historical iteration trajectory data. By introducing a new guarding mechanism, we propose a guarded multi-gradient learning to optimize (GML2O) method, and prove that the iterative sequence generated by GML2O converges to a Pareto critical point. The exp
    
[^26]: 没有处理器的机器学习：非线性电子变质材料中的浮现学习

    Machine Learning Without a Processor: Emergent Learning in a Nonlinear Electronic Metamaterial. (arXiv:2311.00537v1 [cond-mat.soft])

    [http://arxiv.org/abs/2311.00537](http://arxiv.org/abs/2311.00537)

    这项研究介绍了一种非线性学习变质材料，它由基于晶体管的自适应非线性阻性元件构成，可以在没有计算机的情况下学习非线性任务，并降低训练误差的多个模式，为模拟机器学习提供了新的硬件选择。

    

    标准的深度学习算法需要对大规模非线性网络进行微分，这个过程缓慢且耗能。电子学习变质材料提供了潜在的快速、高效和容错的模拟机器学习硬件，但现有实现是线性的，严重限制了其功能。这些系统与人工神经网络和人脑有很大的区别，因此尚未探索将非线性元素纳入其中的可行性和实用性。在这里，我们介绍了一种非线性学习变质材料——一种基于晶体管的自适应非线性阻性元件的模拟电子网络。我们证明该系统可以在没有计算机的情况下学习不可实现的任务，包括异或和非线性回归。我们发现我们的非线性学习变质材料按顺序降低训练误差的模式（均值、斜率、曲率），类似于人工神经网络中的谱偏差。该电路对干扰。

    Standard deep learning algorithms require differentiating large nonlinear networks, a process that is slow and power-hungry. Electronic learning metamaterials offer potentially fast, efficient, and fault-tolerant hardware for analog machine learning, but existing implementations are linear, severely limiting their capabilities. These systems differ significantly from artificial neural networks as well as the brain, so the feasibility and utility of incorporating nonlinear elements have not been explored. Here we introduce a nonlinear learning metamaterial -- an analog electronic network made of self-adjusting nonlinear resistive elements based on transistors. We demonstrate that the system learns tasks unachievable in linear systems, including XOR and nonlinear regression, without a computer. We find our nonlinear learning metamaterial reduces modes of training error in order (mean, slope, curvature), similar to spectral bias in artificial neural networks. The circuitry is robust to da
    
[^27]: 主动降噪便携式设备设计

    Active Noise Control Portable Device Design. (arXiv:2311.00535v1 [cs.SD])

    [http://arxiv.org/abs/2311.00535](http://arxiv.org/abs/2311.00535)

    该论文提出了一种便携式的主动降噪设备设计，通过传感器检测环境中的噪音，经过电子控制系统处理后，使用反位相频率信号来降低噪音。这种方法对低频噪音也更加有效。

    

    虽然我们的世界充满了令人愉悦的自然声音，但也充斥着其他让人讨厌的声音，即噪音。噪音不仅影响工作效率，也对人类健康有影响。减少噪音的问题非常重要且困难。多年来，人们通过多种方式来解决这个问题。目前降噪方法大多依赖于材料和传输介质，在高频噪音方面有效，但对低频噪音的降噪方法非常有限。因此，我们提出了一个降噪系统，包括一个传感器来检测环境中的噪音。然后将噪音发送到电子控制系统进行处理，生成一个反位相频率信号来抵消干扰。最后，通过扬声器广播处理后的较小噪音。

    While our world is filled with its own natural sounds that we can't resist enjoying, it is also chock-full of other sounds that can be irritating, this is noise. Noise not only influences the working efficiency but also the human's health. The problem of reducing noise is one of great importance and great difficulty. The problem has been addressed in many ways over the years. The current methods for noise reducing mostly rely on the materials and transmission medium, which are only effective to some extent for the high frequency noise. However, the effective reduction noise method especially for low frequency noise is very limited.  Here we come up with a noise reduction system consist of a sensor to detect the noise in the environment. Then the noise will be sent to an electronic control system to process the noise, which will generate a reverse phase frequency signal to counteract the disturbance. Finally, the processed smaller noise will be broadcasted by the speaker. Through this s
    
[^28]: 使用深度强化学习学习顺序可解释策略的无偏方法

    Learning impartial policies for sequential counterfactual explanations using Deep Reinforcement Learning. (arXiv:2311.00523v1 [cs.LG])

    [http://arxiv.org/abs/2311.00523](http://arxiv.org/abs/2311.00523)

    本文提出了使用深度强化学习学习顺序可解释策略的无偏方法。该方法通过对分类器的输出概率进行奖励来减轻现有方法中可能导致偏向特定动作的策略的不希望属性。

    

    在可解释人工智能（XAI）领域中，通常使用顺序可解释（SCF）示例通过对输入实例进行一系列修改来改变训练分类器的决策。虽然某些测试时算法旨在针对每个新实例进行优化，但最近提出了强化学习（RL）方法，旨在学习用于发现SCF的策略，从而提高可伸缩性。在RL中，RL问题的制定，包括状态空间，动作和奖励的规定，通常存在歧义。在这项工作中，我们发现现有方法的缺点可能导致具有不希望的属性（如偏向特定动作）的策略。我们建议使用分类器的输出概率来创建更具信息性的奖励，以减轻这种影响。

    In the field of explainable Artificial Intelligence (XAI), sequential counterfactual (SCF) examples are often used to alter the decision of a trained classifier by implementing a sequence of modifications to the input instance. Although certain test-time algorithms aim to optimize for each new instance individually, recently Reinforcement Learning (RL) methods have been proposed that seek to learn policies for discovering SCFs, thereby enhancing scalability. As is typical in RL, the formulation of the RL problem, including the specification of state space, actions, and rewards, can often be ambiguous. In this work, we identify shortcomings in existing methods that can result in policies with undesired properties, such as a bias towards specific actions. We propose to use the output probabilities of the classifier to create a more informative reward, to mitigate this effect.
    
[^29]: 基于检索重建的时间序列对比学习方法

    Retrieval-Based Reconstruction For Time-series Contrastive Learning. (arXiv:2311.00519v1 [cs.LG])

    [http://arxiv.org/abs/2311.00519](http://arxiv.org/abs/2311.00519)

    本文提出了一种基于检索重建的时间序列对比学习方法（REBAR），通过检索信息和重建子序列来构建正样本对，从而解决了时间序列中使用数据增强创建正样本对的挑战。实验证明，REBAR误差可以作为正/负标记器，并且在对比学习框架中集成REBAR方法可以学习具有有用信息的嵌入表示。

    

    自监督对比学习的成功取决于鉴别出的正样本对，当它们被推到嵌入空间时，可以为后续的下游任务编码有用的信息。然而，在时间序列中，这是具有挑战性的，因为通过数据增强来创建正样本对可能会破坏原始的语义含义。我们假设如果我们能从一个子序列中检索信息，成功重建另一个子序列，那么它们应该是一个正样本对。基于这个直觉，我们引入了一种新颖的方法：基于检索重建的对比学习（REBAR）。首先，我们利用卷积交叉注意力架构计算两个不同时间序列之间的REBAR误差。然后，通过验证实验，我们展示了REBAR误差是互相类别成员的预测器，从而证明了它作为正/负标记器的使用。最后，一旦集成到对比学习框架中，我们的REBAR方法可以学习一个

    The success of self-supervised contrastive learning hinges on identifying positive data pairs that, when pushed together in embedding space, encode useful information for subsequent downstream tasks. However, in time-series, this is challenging because creating positive pairs via augmentations may break the original semantic meaning. We hypothesize that if we can retrieve information from one subsequence to successfully reconstruct another subsequence, then they should form a positive pair. Harnessing this intuition, we introduce our novel approach: REtrieval-BAsed Reconstruction (REBAR) contrastive learning. First, we utilize a convolutional cross-attention architecture to calculate the REBAR error between two different time-series. Then, through validation experiments, we show that the REBAR error is a predictor of mutual class membership, justifying its usage as a positive/negative labeler. Finally, once integrated into a contrastive learning framework, our REBAR method can learn an
    
[^30]: 在CPU上高效的LLM推理

    Efficient LLM Inference on CPUs. (arXiv:2311.00502v1 [cs.LG])

    [http://arxiv.org/abs/2311.00502](http://arxiv.org/abs/2311.00502)

    本研究提出了一种在CPU上高效部署大型语言模型（LLMs）的方法，支持自动权重量化和优化内核，在流行的LLMs上展示了极高的推理效率。

    

    大型语言模型(LLMs)已经在各种任务上展示出了令人瞩目的性能和巨大的潜力。然而，由于模型参数的庞大数量，LLMs的部署一直面临挑战，对大内存容量和高内存带宽的需求。在本文中，我们提出了一种有效的方法，可以使LLMs的部署更高效。我们支持自动的INT4权重量化流程，并设计了一个特殊的LLM运行时，具有高度优化的内核，以加速在CPU上的LLM推理。我们展示了我们的方法在流行的LLMs上的普适性，包括Llama2，Llama，GPT-NeoX，并展示了在CPU上的极高推理效率。代码公开可用于: https://github.com/intel/intel-extension-for-transformers.

    Large language models (LLMs) have demonstrated remarkable performance and tremendous potential across a wide range of tasks. However, deploying these models has been challenging due to the astronomical amount of model parameters, which requires a demand for large memory capacity and high memory bandwidth. In this paper, we propose an effective approach that can make the deployment of LLMs more efficiently. We support an automatic INT4 weight-only quantization flow and design a special LLM runtime with highly-optimized kernels to accelerate the LLM inference on CPUs. We demonstrate the general applicability of our approach on popular LLMs including Llama2, Llama, GPT-NeoX, and showcase the extreme inference efficiency on CPUs. The code is publicly available at: https://github.com/intel/intel-extension-for-transformers.
    
[^31]: 扩散模型的数据归因的有趣特性

    Intriguing Properties of Data Attribution on Diffusion Models. (arXiv:2311.00500v1 [cs.LG])

    [http://arxiv.org/abs/2311.00500](http://arxiv.org/abs/2311.00500)

    本研究通过对扩散模型进行实验和分析，发现在数据归因方面，一些在理论上不合理的设计选择能够在实际中表现出比以前的方法更好的效果。这对于确保数据贡献者公平补偿或认可具有重要意义。

    

    数据归因旨在将模型输出追溯到训练数据。随着扩散模型的最新发展，数据归因已成为一个理想的模块，可以为高质量或版权保护的训练样本正确分配价值，确保数据贡献者得到公平的补偿或认可。已经提出了几种在理论上有动机的方法来实现数据归因，以改善计算可扩展性和效果之间的权衡。在这项工作中，我们对扩散模型进行了广泛的实验和消融研究，特别关注在CIFAR-10和CelebA上训练的DDPM以及在ArtBench上进行细调的稳定扩散模型LoRA的归因。有趣的是，我们报告了理论上不合理的设计选择在实际中大幅超越了以前的基线，无论是在线性数据建模得分还是反事实评估方面。我们的工作呈现了一个重要的创新点。

    Data attribution seeks to trace model outputs back to training data. With the recent development of diffusion models, data attribution has become a desired module to properly assign valuations for high-quality or copyrighted training samples, ensuring that data contributors are fairly compensated or credited. Several theoretically motivated methods have been proposed to implement data attribution, in an effort to improve the trade-off between computational scalability and effectiveness. In this work, we conduct extensive experiments and ablation studies on attributing diffusion models, specifically focusing on DDPMs trained on CIFAR-10 and CelebA, as well as a Stable Diffusion model LoRA-finetuned on ArtBench. Intriguingly, we report counter-intuitive observations that theoretically unjustified design choices for attribution empirically outperform previous baselines by a large margin, in terms of both linear datamodeling score and counterfactual evaluation. Our work presents a signific
    
[^32]: 深度神经网络在自动说话人识别中不能学习超分段时间特征

    Deep Neural Networks for Automatic Speaker Recognition Do Not Learn Supra-Segmental Temporal Features. (arXiv:2311.00489v1 [cs.SD])

    [http://arxiv.org/abs/2311.00489](http://arxiv.org/abs/2311.00489)

    这项研究表明深度神经网络在自动说话人识别中无法充分模拟超分段时间特征，这为未来更好地利用完整语音信号进行研究提供了基础。

    

    深度神经网络在自动说话人识别和相关任务中取得了令人印象深刻的结果，但我们对于这些结果的具体原因了解甚少。以前的研究将成功的一部分归因于它们模拟超分段时间信息（SST）的能力，即除了谱特征外还学习语音的韵律和韵律特征。在本文中，我们（i）提出并应用一种新的测试方法，来量化最先进的神经网络在说话人识别方面的性能能够通过建模SST来解释到多大程度；并且（ii）提出几种强制网络更加关注SST的方法，并评估它们的优点。我们发现，即使被强制要求，一系列基于CNN和RNN的神经网络结构在说话人识别中也不能充分地模拟SST。这些结果为未来更好地利用完整语音信号进行研究提供了非常重要的基础。

    While deep neural networks have shown impressive results in automatic speaker recognition and related tasks, it is dissatisfactory how little is understood about what exactly is responsible for these results. Part of the success has been attributed in prior work to their capability to model supra-segmental temporal information (SST), i.e., learn rhythmic-prosodic characteristics of speech in addition to spectral features. In this paper, we (i) present and apply a novel test to quantify to what extent the performance of state-of-the-art neural networks for speaker recognition can be explained by modeling SST; and (ii) present several means to force respective nets to focus more on SST and evaluate their merits. We find that a variety of CNN- and RNN-based neural network architectures for speaker recognition do not model SST to any sufficient degree, even when forced. The results provide a highly relevant basis for impactful future research into better exploitation of the full speech sig
    
[^33]: 对比优化目标对一致性搜索的比较

    Comparing Optimization Targets for Contrast-Consistent Search. (arXiv:2311.00488v1 [cs.LG])

    [http://arxiv.org/abs/2311.00488](http://arxiv.org/abs/2311.00488)

    本研究通过对比优化目标发现，使用Midpoint-Displacement（MD）损失函数可以获得与Contrast-Consistent Search（CCS）非常相似的模型权重，而且通过调整超参数可以使MD损失函数的测试准确率高于CCS。

    

    我们研究了一致性搜索（CCS）的优化目标，该目标旨在恢复大型语言模型的真实内部表示。我们提出了一种新的损失函数，称为Midpoint-Displacement（MD）损失函数。我们证明在某个超参数值下，该MD损失函数导致的模型权重与CCS非常相似。我们进一步展示了该超参数并不是最优的，并且用更好的超参数，MD损失函数可以达到比CCS更高的测试准确率。

    We investigate the optimization target of Contrast-Consistent Search (CCS), which aims to recover the internal representations of truth of a large language model. We present a new loss function that we call the Midpoint-Displacement (MD) loss function. We demonstrate that for a certain hyper-parameter value this MD loss function leads to a prober with very similar weights to CCS. We further show that this hyper-parameter is not optimal and that with a better hyper-parameter the MD loss function attains a higher test accuracy than CCS.
    
[^34]: 在稀疏线性bandit中的固定预算下，最佳臂识别问题的研究

    Fixed-Budget Best-Arm Identification in Sparse Linear Bandits. (arXiv:2311.00481v1 [cs.LG])

    [http://arxiv.org/abs/2311.00481](http://arxiv.org/abs/2311.00481)

    本文研究了在稀疏线性bandit中的固定预算条件下的最佳臂识别问题，设计了基于Lasso和最优设计的两阶段算法，通过适当选择超参数和平衡两个阶段的错误概率，得到了Lasso-OD的非渐近上界。

    

    本文研究了在稀疏线性bandit中的固定预算条件下的最佳臂识别问题。在稀疏线性bandit中，未知特征向量θ*可能具有很大的维度d，但只有一小部分特征（比如s个）具有非零值。我们设计了一个两阶段的算法，即基于Lasso和最优设计(Lasso-OD)的线性最佳臂识别。Lasso-OD的第一阶段利用了特征向量的稀疏性，通过应用Zhou（2009）引入的阈值化Lasso，利用所选择的臂的回报和合理选择的设计矩阵来高概率地正确估计θ*的支持集。Lasso-OD的第二阶段在估计得到的支持集上应用了Yang和Tan（2022）提出的OD-LinBAI算法。我们通过精心选择超参数（如Lasso的正则化参数）和平衡两个阶段的错误概率，推导了Lasso-OD的非渐近上界。

    We study the best-arm identification problem in sparse linear bandits under the fixed-budget setting. In sparse linear bandits, the unknown feature vector $\theta^*$ may be of large dimension $d$, but only a few, say $s \ll d$ of these features have non-zero values. We design a two-phase algorithm, Lasso and Optimal-Design- (Lasso-OD) based linear best-arm identification. The first phase of Lasso-OD leverages the sparsity of the feature vector by applying the thresholded Lasso introduced by Zhou (2009), which estimates the support of $\theta^*$ correctly with high probability using rewards from the selected arms and a judicious choice of the design matrix. The second phase of Lasso-OD applies the OD-LinBAI algorithm by Yang and Tan (2022) on that estimated support. We derive a non-asymptotic upper bound on the error probability of Lasso-OD by carefully choosing hyperparameters (such as Lasso's regularization parameter) and balancing the error probabilities of both phases. For fixed spa
    
[^35]: 分组分布鲁棒知识蒸馏

    Group Distributionally Robust Knowledge Distillation. (arXiv:2311.00476v1 [cs.CV])

    [http://arxiv.org/abs/2311.00476](http://arxiv.org/abs/2311.00476)

    本文提出了一种群组感知的蒸馏损失来解决知识蒸馏在医学影像分析中的亚群体偏移问题，为在训练期间动态关注表现较差的群组提供了方法。

    

    知识蒸馏可以快速有效地从大模型向小模型传输学到的特征。然而，蒸馏目标对亚群体的偏移特别敏感，而医学影像分析中常见的是在训练集中存在少数群体/领域的数据。例如，使用来自多个扫描仪或医院获得的健康数据训练模型可能对少数群体表现出较差的性能。受分布鲁棒优化（DRO）技术的启发，本文通过提出一种群组感知的蒸馏损失来解决这个问题。在优化过程中，根据给定迭代中的每个群组损失来更新一组权重。这种方法可以在训练期间动态地关注表现较差的群组。我们在两个基准数据集（自然图像和心脏MRI）上进行了实证验证，结果显示在最差群组准确性方面，我们的方法GroupDistil始终有一致的改进。

    Knowledge distillation enables fast and effective transfer of features learned from a bigger model to a smaller one. However, distillation objectives are susceptible to sub-population shifts, a common scenario in medical imaging analysis which refers to groups/domains of data that are underrepresented in the training set. For instance, training models on health data acquired from multiple scanners or hospitals can yield subpar performance for minority groups. In this paper, inspired by distributionally robust optimization (DRO) techniques, we address this shortcoming by proposing a group-aware distillation loss. During optimization, a set of weights is updated based on the per-group losses at a given iteration. This way, our method can dynamically focus on groups that have low performance during training. We empirically validate our method, GroupDistil on two benchmark datasets (natural images and cardiac MRIs) and show consistent improvement in terms of worst-group accuracy.
    
[^36]: 概率编程的扩散模型

    Diffusion models for probabilistic programming. (arXiv:2311.00474v1 [cs.LG])

    [http://arxiv.org/abs/2311.00474](http://arxiv.org/abs/2311.00474)

    我们提出了一种新的扩散模型变分推断（DMVI）方法，用于在概率编程语言中进行自动近似推断。DMVI可以更准确地进行后验推断，而且易于实现和使用，对神经网络模型没有任何约束。

    

    我们提出了扩散模型变分推断（DMVI），这是一种在概率编程语言（PPL）中进行自动近似推断的新方法。DMVI利用扩散模型作为对真实后验分布的变分近似，通过导出贝叶斯建模中使用的边际似然目标的新约束。DMVI易于实现，在PPL中进行无障碍推断，不像使用归一化流的变分推断那样具有缺点，并且对基础神经网络模型不做任何约束。我们在一组常见的贝叶斯模型上评估了DMVI，并表明它的后验推断一般比PPL中使用的现代方法更准确，同时具有类似的计算成本并且需要较少的手动调整。

    We propose Diffusion Model Variational Inference (DMVI), a novel method for automated approximate inference in probabilistic programming languages (PPLs). DMVI utilizes diffusion models as variational approximations to the true posterior distribution by deriving a novel bound to the marginal likelihood objective used in Bayesian modelling. DMVI is easy to implement, allows hassle-free inference in PPLs without the drawbacks of, e.g., variational inference using normalizing flows, and does not make any constraints on the underlying neural network model. We evaluate DMVI on a set of common Bayesian models and show that its posterior inferences are in general more accurate than those of contemporary methods used in PPLs while having a similar computational cost and requiring less manual tuning.
    
[^37]: 双重条件扩散模型用于外分布检测：应用于胎儿超声视频

    Dual Conditioned Diffusion Models for Out-Of-Distribution Detection: Application to Fetal Ultrasound Videos. (arXiv:2311.00469v1 [cs.CV])

    [http://arxiv.org/abs/2311.00469](http://arxiv.org/abs/2311.00469)

    本论文提出了双重条件扩散模型（DCDM），用于胎儿超声视频中的外分布检测。该模型能够在存在高度结构相似性和大量内部变异性的背景下，对胎儿超声视频中的心脏视图进行检测，并拒绝类似的外分布样本。

    

    外分布（OOD）检测对于改善机器学习模型的可靠性至关重要，它能够检测出不属于训练分布的样本。在某些任务中，有效地检测OOD样本可能会面临挑战，因为训练分布（ID）内部存在显著的异质性，并且ID和OOD类之间存在高度的结构相似性。例如，在胎儿超声视频中检测心脏视图时，心脏和腹部等其他解剖结构之间存在高度的结构相似性，并且每个视图内部存在着5种不同的视图和结构变化。为了检测此背景下的OOD样本，所得模型应能够学习到解剖结构内部的变化，并且能够拒绝类似的OOD样本。在本文中，我们介绍了双重条件扩散模型（DCDM），其中我们使用ID类信息和输入图像的潜在特征来对模型进行条件化重构。

    Out-of-distribution (OOD) detection is essential to improve the reliability of machine learning models by detecting samples that do not belong to the training distribution. Detecting OOD samples effectively in certain tasks can pose a challenge because of the substantial heterogeneity within the in-distribution (ID), and the high structural similarity between ID and OOD classes. For instance, when detecting heart views in fetal ultrasound videos there is a high structural similarity between the heart and other anatomies such as the abdomen, and large in-distribution variance as a heart has 5 distinct views and structural variations within each view. To detect OOD samples in this context, the resulting model should generalise to the intra-anatomy variations while rejecting similar OOD samples. In this paper, we introduce dual-conditioned diffusion models (DCDM) where we condition the model on in-distribution class information and latent features of the input image for reconstruction-bas
    
[^38]: 图上的异步SGD: 一种统一的异步分散和联邦优化框架

    Asynchronous SGD on Graphs: a Unified Framework for Asynchronous Decentralized and Federated Optimization. (arXiv:2311.00465v1 [math.OC])

    [http://arxiv.org/abs/2311.00465](http://arxiv.org/abs/2311.00465)

    本文引入了图上的异步SGD（AGRAF SGD）算法框架，该框架统一了异步分散和联邦优化算法，并在更温和的假设下提供了收敛速度，还恢复或改善了所有算法的最佳结果。

    

    分散和异步通信是加速分布式机器学习通信复杂性的两种流行技术，分别通过消除对中央编排器的依赖和不需要同步来实现。然而，将这两种技术结合起来仍然是一个挑战。本文在这个方向上迈出了一步，引入了图上的异步SGD（AGRAF SGD）——一个通用的算法框架，包括了许多流行算法的异步版本，包括SGD、分散SGD、本地SGD、FedBuff，由于其放松了通信和计算假设。在比之前的分散异步工作更温和的假设下，我们提供了收敛速度，同时仍然恢复甚至改善了所有算法的最佳结果。

    Decentralized and asynchronous communications are two popular techniques to speedup communication complexity of distributed machine learning, by respectively removing the dependency over a central orchestrator and the need for synchronization. Yet, combining these two techniques together still remains a challenge. In this paper, we take a step in this direction and introduce Asynchronous SGD on Graphs (AGRAF SGD) -- a general algorithmic framework that covers asynchronous versions of many popular algorithms including SGD, Decentralized SGD, Local SGD, FedBuff, thanks to its relaxed communication and computation assumptions. We provide rates of convergence under much milder assumptions than previous decentralized asynchronous works, while still recovering or even improving over the best know results for all the algorithms covered.
    
[^39]: 健壮和共轭高斯过程回归

    Robust and Conjugate Gaussian Process Regression. (arXiv:2311.00463v1 [stat.ML])

    [http://arxiv.org/abs/2311.00463](http://arxiv.org/abs/2311.00463)

    本文提出了一种健壮和共轭的高斯过程（RCGP）回归方法，通过泛化贝叶斯推断实现了可靠的闭式更新，适用于各种实际应用场景。

    

    为了实现闭式条件，高斯过程（GP）回归的常见假设是独立同分布的高斯观测噪声。然而，这种强假设在实际中经常被违反，导致不可靠的推断和不确定性量化。本文中，我们展示了如何使用泛化贝叶斯推断以几乎没有额外代价实现可靠和共轭的高斯过程（RCGP）回归。RCGP具有很高的灵活性，可以在标准GP适用的所有情况下进行精确的共轭闭式更新。为了展示其强大的实证性能，我们将RCGP应用于从贝叶斯优化到稀疏变分高斯过程的各种问题中。

    To enable closed form conditioning, a common assumption in Gaussian process (GP) regression is independent and identically distributed Gaussian observation noise. This strong and simplistic assumption is often violated in practice, which leads to unreliable inferences and uncertainty quantification. Unfortunately, existing methods for robustifying GPs break closed-form conditioning, which makes them less attractive to practitioners and significantly more computationally expensive. In this paper, we demonstrate how to perform provably robust and conjugate Gaussian process (RCGP) regression at virtually no additional cost using generalised Bayesian inference. RCGP is particularly versatile as it enables exact conjugate closed form updates in all settings where standard GPs admit them. To demonstrate its strong empirical performance, we deploy RCGP for problems ranging from Bayesian optimisation to sparse variational Gaussian processes.
    
[^40]: 生成模型的最佳限制预算拒绝采样方法

    Optimal Budgeted Rejection Sampling for Generative Models. (arXiv:2311.00460v1 [cs.LG])

    [http://arxiv.org/abs/2311.00460](http://arxiv.org/abs/2311.00460)

    该论文提出了一种最佳限制预算拒绝采样方法（OBRS），可显著改善生成模型的样本质量和多样性。通过将采样方案与训练过程相结合，该方法在给定采样预算情况下，对于任何真实分布和拒绝后分布之间的f-散度都是最优的。

    

    最近提出了拒绝采样方法来改善基于鉴别器的生成模型的性能。然而，这些方法只在无限采样预算下是最优的，并且通常应用于与拒绝过程独立训练的生成器。我们首先提出了一种Optimal Budgeted Rejection Sampling (OBRS)方案，该方案在给定采样预算情况下，对于真实分布和拒绝后分布之间的任何f-散度证明是最优的。其次，我们提出了一种端到端方法，将采样方案融入训练过程，进一步提高模型的整体性能。通过实验证明和支持的理论，我们展示了这些方法在显著提高样本质量和多样性方面的有效性。

    Rejection sampling methods have recently been proposed to improve the performance of discriminator-based generative models. However, these methods are only optimal under an unlimited sampling budget, and are usually applied to a generator trained independently of the rejection procedure. We first propose an Optimal Budgeted Rejection Sampling (OBRS) scheme that is provably optimal with respect to \textit{any} $f$-divergence between the true distribution and the post-rejection distribution, for a given sampling budget. Second, we propose an end-to-end method that incorporates the sampling scheme into the training procedure to further enhance the model's overall performance. Through experiments and supporting theory, we show that the proposed methods are effective in significantly improving the quality and diversity of the samples.
    
[^41]: 神经网络权重矩阵的Hessian特征向量和主成分分析

    Hessian Eigenvectors and Principal Component Analysis of Neural Network Weight Matrices. (arXiv:2311.00452v1 [cs.LG])

    [http://arxiv.org/abs/2311.00452](http://arxiv.org/abs/2311.00452)

    本研究揭示了神经网络权重矩阵的Hessian特征向量与网络权重之间的相关性，通过识别关键方向揭示了漂移模式和势能最小值的关系。

    

    本研究深入探讨了经过训练的深度神经网络的复杂动力学和与网络参数的关系。经过训练的网络主要在单一方向上继续训练，被称为漂移模式。这种漂移模式可以通过损失函数的二次势能模型来解释，暗示了指数慢慢衰减到势能最小值的过程。我们揭示了Hessian特征向量与网络权重之间的相关性。这种关系依赖于特征值的大小，使我们能够区分网络内的参数方向。值得注意的是，这些方向的重要性取决于它们势阱的曲率（由Hessian特征值的大小指示）以及与权重向量的对齐程度。我们的探索还延伸到了通过奇异值分解对权重矩阵进行分解。这种方法在确定Hessian内的关键方向方面非常实用，考虑到它们的曲率和权重向量的对齐性。

    This study delves into the intricate dynamics of trained deep neural networks and their relationships with network parameters. Trained networks predominantly continue training in a single direction, known as the drift mode. This drift mode can be explained by the quadratic potential model of the loss function, suggesting a slow exponential decay towards the potential minima. We unveil a correlation between Hessian eigenvectors and network weights. This relationship, hinging on the magnitude of eigenvalues, allows us to discern parameter directions within the network. Notably, the significance of these directions relies on two defining attributes: the curvature of their potential wells (indicated by the magnitude of Hessian eigenvalues) and their alignment with the weight vectors. Our exploration extends to the decomposition of weight matrices through singular value decomposition. This approach proves practical in identifying critical directions within the Hessian, considering both thei
    
[^42]: 人类和语言模型中的三段论推理的系统比较

    A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models. (arXiv:2311.00445v1 [cs.CL])

    [http://arxiv.org/abs/2311.00445](http://arxiv.org/abs/2311.00445)

    这项研究通过对比人类和语言模型在三段论推理中的表现，发现较大的语言模型更合逻辑，甚至比人类更合逻辑，但即使最大的语言模型也会出现与人类推理类似的错误，总体上认为语言模型在某些情况下能够克服人类偏见。

    

    理性行为的一个核心组成部分是逻辑推理：确定哪些结论可以从一组前提中得出。心理学家已经记录下人类推理与逻辑规则不符的几种方式。语言模型是否能够复制这些偏差，或者它们能够克服这些偏差？我们关注三段论的情况 - 从两个简单前提中推导出的推理，这在心理学中已经广泛研究 - 我们发现较大的模型比较合逻辑，而且比人类更合逻辑。与此同时，即使是最大的模型也会出现系统性错误，其中一些错误与人类推理的偏见相似，例如排序效应和逻辑谬误。总体上，我们发现语言模型模仿了训练数据中包含的人类偏见，但在某些情况下能够克服这些偏见。

    A central component of rational behavior is logical inference: the process of determining which conclusions follow from a set of premises. Psychologists have documented several ways in which humans' inferences deviate from the rules of logic. Do language models, which are trained on text generated by humans, replicate these biases, or are they able to overcome them? Focusing on the case of syllogisms -- inferences from two simple premises, which have been studied extensively in psychology -- we show that larger models are more logical than smaller ones, and also more logical than humans. At the same time, even the largest models make systematic errors, some of which mirror human reasoning biases such as ordering effects and logical fallacies. Overall, we find that language models mimic the human biases included in their training data, but are able to overcome them in some cases.
    
[^43]: "形式追随功能：基于功能要求的文本到文本条件图生成"

    Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements. (arXiv:2311.00444v1 [cs.LG])

    [http://arxiv.org/abs/2311.00444](http://arxiv.org/abs/2311.00444)

    本研究提出了一种新的问题设置，即在下游任务中基于图的功能要求生成图形。我们通过细调预训练的大型语言模型，引入消息传递层来结合图结构信息，实现了更贴近功能要求的图形生成。实验结果表明，我们的方法在公开可得的分子和知识图数据集上，相比类似任务上的基准方法，生成的图形更能满足功能要求。

    

    本研究致力于生成在下游任务中以图的功能要求描述为条件的图形的新问题设置。我们将问题建立为文本到文本生成问题，并专注于细调预训练的大型语言模型（LLM）来生成图形的方法。我们提出了一种归纳偏差，将关于图的结构信息结合到LLM的生成过程中，通过在LLM的体系结构中引入消息传递层。为了评估我们提出的方法，我们设计了一系列新颖的实验，使用公开可得且被广泛研究的分子和知识图数据集。结果表明，我们提出的方法生成的图形更接近于所请求的功能要求，显著超过了在类似任务上开发的基准方法。

    This work focuses on the novel problem setting of generating graphs conditioned on a description of the graph's functional requirements in a downstream task. We pose the problem as a text-to-text generation problem and focus on the approach of fine-tuning a pretrained large language model (LLM) to generate graphs. We propose an inductive bias which incorporates information about the structure of the graph into the LLM's generation process by incorporating message passing layers into an LLM's architecture. To evaluate our proposed method, we design a novel set of experiments using publicly available and widely studied molecule and knowledge graph data sets. Results suggest our proposed approach generates graphs which more closely meet the requested functional requirements, outperforming baselines developed on similar tasks by a statistically significant margin.
    
[^44]: 通过简单的动态扫描增强技术改进Vision Transformer的鲁棒性

    Improving Robustness for Vision Transformer with a Simple Dynamic Scanning Augmentation. (arXiv:2311.00441v1 [cs.CV])

    [http://arxiv.org/abs/2311.00441](http://arxiv.org/abs/2311.00441)

    通过Dynamic Scanning Augmentation增强技术，提高了Vision Transformer的准确性和鲁棒性，尤其是在面对对抗攻击时。这种方法适应性地聚焦于不同的图像块，并改变了ViT的注意力机制。

    

    Vision Transformer (ViT)在计算机视觉任务中表现出与最先进的神经网络相媲美的性能。然而，这种新型的深度神经网络架构易受到对抗攻击的影响，从而限制了其鲁棒性能。本文提出了一种旨在进一步提高ViT的准确性和鲁棒性的新方法，特别是在面对对抗攻击时。我们提出了一种称为“动态扫描增强”的增强技术，利用动态输入序列来自适应地聚焦于不同的图像块，从而保持性能和鲁棒性。我们的详细研究揭示了这种对输入序列的适应性会导致ViT的注意力机制发生显著变化，即使对于相同的图像也是如此。我们引入了四种Dynamic Scanning Augmentation的变体， 在对抗攻击的鲁棒性和对自然图像的准确性方面均胜过ViT，其中一种变体表现最佳。

    Vision Transformer (ViT) has demonstrated promising performance in computer vision tasks, comparable to state-of-the-art neural networks. Yet, this new type of deep neural network architecture is vulnerable to adversarial attacks limiting its capabilities in terms of robustness. This article presents a novel contribution aimed at further improving the accuracy and robustness of ViT, particularly in the face of adversarial attacks. We propose an augmentation technique called `Dynamic Scanning Augmentation' that leverages dynamic input sequences to adaptively focus on different patches, thereby maintaining performance and robustness. Our detailed investigations reveal that this adaptability to the input sequence induces significant changes in the attention mechanism of ViT, even for the same image. We introduce four variations of Dynamic Scanning Augmentation, outperforming ViT in terms of both robustness to adversarial attacks and accuracy against natural images, with one variant showin
    
[^45]: 使用绿色色度坐标（GCC）和基于注意力的特征提取的支持向量机（SVM）进行作物病害分类，用于基于物联网的智能农业应用

    Crop Disease Classification using Support Vector Machines with Green Chromatic Coordinate (GCC) and Attention based feature extraction for IoT based Smart Agricultural Applications. (arXiv:2311.00429v1 [eess.IV])

    [http://arxiv.org/abs/2311.00429](http://arxiv.org/abs/2311.00429)

    本文提出了一种新的作物病害分类方法，利用基于注意力的特征提取和绿色色度坐标，可以轻松与移动设备和物联网设备交互，为农民提供最佳的作物种植保障。

    

    作物具有重要意义，因为它们是人类获取能量、营养和药用价值的主要来源。然而，植物病害可以在农业栽培过程中对叶片产生负面影响，导致作物产量和经济价值的显著损失。因此，农民能够识别作物病害非常重要。然而，这种方法通常需要辛勤工作、大量计划和对植物病原体的深入了解。鉴于这些障碍，提供能够轻松与移动设备和物联网设备交互的解决方案对于农民来说至关重要。已经开发和研究了各种机器学习（ML）和深度学习（DL）算法用于植物病害检测的识别，取得了重要且有希望的结果。本文提出了一种新的分类方法，利用了基于注意力的特征提取、RGB和GCC。

    Crops hold paramount significance as they serve as the primary provider of energy, nutrition, and medicinal benefits for the human population. Plant diseases, however, can negatively affect leaves during agricultural cultivation, resulting in significant losses in crop output and economic value. Therefore, it is crucial for farmers to identify crop diseases. However, this method frequently necessitates hard work, a lot of planning, and in-depth familiarity with plant pathogens. Given these numerous obstacles, it is essential to provide solutions that can easily interface with mobile and IoT devices so that our farmers can guarantee the best possible crop development. Various machine learning (ML) as well as deep learning (DL) algorithms have been created & studied for the identification of plant disease detection, yielding substantial and promising results. This article presents a novel classification method that builds on prior work by utilising attention-based feature extraction, RGB
    
[^46]: NEO-KD：基于知识蒸馏的对抗训练用于健壮的多出口神经网络

    NEO-KD: Knowledge-Distillation-Based Adversarial Training for Robust Multi-Exit Neural Networks. (arXiv:2311.00428v1 [cs.LG])

    [http://arxiv.org/abs/2311.00428](http://arxiv.org/abs/2311.00428)

    NEO-KD是一种基于知识蒸馏的对抗训练策略，用于提升多出口神经网络的对抗鲁棒性，通过邻居知识蒸馏和出口间正交知识蒸馏，该方法能够减少对特定出口的对抗攻击影响，并增强整体性能。

    

    尽管多出口神经网络被认为是通过提前退出来进行高效推理的有前途的解决方案，但对抗性攻击仍然是一个具有挑战性的问题。在多出口网络中，由于不同子模型之间的高依赖性，针对特定出口的对抗性示例不仅降低了目标出口的性能，同时也减少了所有其他出口的性能。这使得多出口网络对简单的对抗性攻击非常脆弱。在本文中，我们提出了NEO-KD，一种基于知识蒸馏的对抗训练策略，以应对这一基本挑战，其具有两个关键贡献。NEO-KD首先采用邻居知识蒸馏来引导对抗性示例的输出趋向于干净数据的邻居出口的集合输出。NEO-KD还使用出口间正交的知识蒸馏来减少不同子模型之间的对抗性传递性。结果是一个显著的

    While multi-exit neural networks are regarded as a promising solution for making efficient inference via early exits, combating adversarial attacks remains a challenging problem. In multi-exit networks, due to the high dependency among different submodels, an adversarial example targeting a specific exit not only degrades the performance of the target exit but also reduces the performance of all other exits concurrently. This makes multi-exit networks highly vulnerable to simple adversarial attacks. In this paper, we propose NEO-KD, a knowledge-distillation-based adversarial training strategy that tackles this fundamental challenge based on two key contributions. NEO-KD first resorts to neighbor knowledge distillation to guide the output of the adversarial examples to tend to the ensemble outputs of neighbor exits of clean data. NEO-KD also employs exit-wise orthogonal knowledge distillation for reducing adversarial transferability across different submodels. The result is a significan
    
[^47]: 基于稀疏奖励的自我模仿强化学习在过程化环境中通过优先级和多样性增强泛化能力

    Enhanced Generalization through Prioritization and Diversity in Self-Imitation Reinforcement Learning over Procedural Environments with Sparse Rewards. (arXiv:2311.00426v1 [cs.LG])

    [http://arxiv.org/abs/2311.00426](http://arxiv.org/abs/2311.00426)

    本研究通过优先级排序和多样性来提高自我模仿强化学习在过程化环境中的泛化能力。

    

    在稀疏奖励的强化学习中，探索是一个基本挑战，限制了智能体学习最优决策的能力，因为缺乏信息反馈信号。自我模仿学习已经成为一种有前途的探索方法，利用回放缓冲区来存储和重现成功的行为。然而，传统的自我模仿学习方法在泛化方面面临挑战，特别是在过程化生成的环境中。因此，本研究提出了通过不同方式对转换进行优先级排序的定制自我模仿学习采样策略，并将优先级技术扩展到过程化生成的环境。我们还通过修改回放缓冲区中的演示来解决多样性损失问题。

    Exploration poses a fundamental challenge in Reinforcement Learning (RL) with sparse rewards, limiting an agent's ability to learn optimal decision-making due to a lack of informative feedback signals. Self-Imitation Learning (self-IL) has emerged as a promising approach for exploration, leveraging a replay buffer to store and reproduce successful behaviors. However, traditional self-IL methods, which rely on high-return transitions and assume singleton environments, face challenges in generalization, especially in procedurally-generated (PCG) environments. Therefore, new self-IL methods have been proposed to rank which experiences to persist, but they replay transitions uniformly regardless of their significance, and do not address the diversity of the stored demonstrations. In this work, we propose tailored self-IL sampling strategies by prioritizing transitions in different ways and extending prioritization techniques to PCG environments. We also address diversity loss through modif
    
[^48]: 通过准备性基于语言的约定实现高效的人工智能-人类协同

    Efficient Human-AI Coordination via Preparatory Language-based Convention. (arXiv:2311.00416v1 [cs.LG])

    [http://arxiv.org/abs/2311.00416](http://arxiv.org/abs/2311.00416)

    本研究发现，在人工智能与人类协同之前，人类进行交流以建立约定，指定角色和行动，有效地指导协同。基于此观察，提出利用大型语言模型(LLM)来实现高效的人工智能-人类协同。

    

    开发能够与人类顺畅协同的智能代理是实现人工通用智能的关键一步。目前，人工智能与人类协同的现有方法通常训练一个代理与多样化的策略或基于真实人类数据拟合的人类模型进行协同。然而，人类行为的大规模多样性对于容量有限的AI系统来说是障碍，而高质量的人类数据在现实环境中可能不容易获取。在这项研究中，我们观察到在协同之前，人类进行交流以建立规则约定，指定个体角色和行动，使他们的协同顺利进行。基于这一观察结果，我们提出利用大型语言模型(LLM)来制定行动计划(或等效地，约定)，有效地指导人类和人工智能。通过输入任务要求、人类偏好、代理数量和其他相关信息

    Developing intelligent agents capable of seamless coordination with humans is a critical step towards achieving artificial general intelligence. Existing methods for human-AI coordination typically train an agent to coordinate with a diverse set of policies or with human models fitted from real human data. However, the massively diverse styles of human behavior present obstacles for AI systems with constrained capacity, while high quality human data may not be readily available in real-world scenarios. In this study, we observe that prior to coordination, humans engage in communication to establish conventions that specify individual roles and actions, making their coordination proceed in an orderly manner. Building upon this observation, we propose employing the large language model (LLM) to develop an action plan (or equivalently, a convention) that effectively guides both human and AI. By inputting task requirements, human preferences, the number of agents, and other pertinent infor
    
[^49]: 使用全射正常化流进行不确定性量化和超出分布检测

    Uncertainty quantification and out-of-distribution detection using surjective normalizing flows. (arXiv:2311.00377v1 [cs.LG])

    [http://arxiv.org/abs/2311.00377](http://arxiv.org/abs/2311.00377)

    本研究使用全射正常化流方法，在深度神经网络模型中识别超出分布数据集，从而可靠地量化不确定性，并在多个实验中得到了验证。

    

    在模型训练和应用环境不同的应用中，可靠地量化认识不确定性是至关重要的，这在气候科学或移动性分析等实际应用中经常出现。我们提出了一种简单的方法，使用全射正常化流来识别深度神经网络模型中的超出分布数据集，可以在单次前向传递中计算。该方法建立在最近在深度不确定性量化和生成建模领域的发展基础上。我们将这种方法应用于使用从移动性文献的机械模型模拟的合成数据集，以及使用对该模型进行软、原子干预后得到的干预分布模拟的几个数据集，并且证明我们的方法可以可靠地区分超出分布数据和分布数据。我们将全射流模型与Dirichlet生成模型进行比较。

    Reliable quantification of epistemic and aleatoric uncertainty is of crucial importance in applications where models are trained in one environment but applied to multiple different environments, often seen in real-world applications for example, in climate science or mobility analysis. We propose a simple approach using surjective normalizing flows to identify out-of-distribution data sets in deep neural network models that can be computed in a single forward pass. The method builds on recent developments in deep uncertainty quantification and generative modeling with normalizing flows. We apply our method to a synthetic data set that has been simulated using a mechanistic model from the mobility literature and several data sets simulated from interventional distributions induced by soft and atomic interventions on that model, and demonstrate that our method can reliably discern out-of-distribution data from in-distribution data. We compare the surjective flow model to a Dirichlet pro
    
[^50]: 英特尔Max系列GPU上深度学习稀疏矩阵核的性能优化

    Performance Optimization of Deep Learning Sparse Matrix Kernels on Intel Max Series GPU. (arXiv:2311.00368v1 [cs.LG])

    [http://arxiv.org/abs/2311.00368](http://arxiv.org/abs/2311.00368)

    本文研究了在英特尔Max系列GPU上对深度学习稀疏矩阵核的性能优化。通过使用英特尔oneAPI的ESIMD SYCL扩展API，我们开发了优化实现并实现了高性能的稀疏矩阵操作，性能接近于GPU的峰值性能，并在与英特尔的oneMKL库和NVIDIA的V100 GPU上的CUDA实现进行比较后表现出更好的性能。

    

    本文主要研究与机器学习应用相关的三个稀疏矩阵操作：稀疏-稠密矩阵乘法（SPMM），采样稠密-稠密矩阵乘法（SDDMM），以及SDDMM与SPMM的组合（FusedMM）。我们利用英特尔oneAPI的Explicit SIMD (ESIMD) SYCL扩展API开发了SPMM、SDDMM和FusedMM操作的优化实现。与CUDA或SYCL相比，ESIMD API允许编写显式矢量化的内核代码。使用ESIMD API实现的稀疏矩阵算法的性能接近于目标英特尔数据中心GPU的峰值性能。我们将性能结果与英特尔的oneMKL库在英特尔GPU上以及最近的适用于NVIDIA的V100 GPU的CUDA实现进行比较，并证明我们的稀疏矩阵操作实现优于两者。

    In this paper, we focus on three sparse matrix operations that are relevant for machine learning applications, namely, the sparse-dense matrix multiplication (SPMM), the sampled dense-dense matrix multiplication (SDDMM), and the composition of the SDDMM with SPMM, also termed as FusedMM. We develop optimized implementations for SPMM, SDDMM, and FusedMM operations utilizing Intel oneAPI's Explicit SIMD (ESIMD) SYCL extension API. In contrast to CUDA or SYCL, the ESIMD API enables the writing of explicitly vectorized kernel code. Sparse matrix algorithms implemented with the ESIMD API achieved performance close to the peak of the targeted Intel Data Center GPU. We compare our performance results to Intel's oneMKL library on Intel GPUs and to a recent CUDA implementation for the sparse matrix operations on NVIDIA's V100 GPU and demonstrate that our implementations for sparse matrix operations outperform either.
    
[^51]: 通过部分差分隐私实现对抗鲁棒的分布式计数跟踪

    Adversarially Robust Distributed Count Tracking via Partial Differential Privacy. (arXiv:2311.00346v1 [cs.DS])

    [http://arxiv.org/abs/2311.00346](http://arxiv.org/abs/2311.00346)

    本研究通过使用部分差分隐私实现对抗鲁棒的分布式计数跟踪，通过探究随机算法和确定算法对于适应性敌手的鲁棒性，揭示了随机算法的$\sqrt{k}$优势是来源于随机性本身而不是无视敌手假设。

    

    我们研究了分布式跟踪模型，也被称为分布式功能监测。该模型涉及到$k$个站点，每个站点接收一系列的项目并与中央服务器进行通信。服务器的任务是以最小的通信成本连续跟踪到目前为止接收到的所有项目的函数。对于计数跟踪问题，已知确定性算法和随机算法之间存在$\sqrt{k}$的通信差距。然而，现有的随机算法假设"无视敌手"在算法开始之前构造整个输入流。我们考虑了可以根据算法之前给出的答案选择新项目的适应性敌手。确定性算法对于适应性敌手是显然鲁棒的，而随机算法则可能不是。因此，我们调查了随机算法的$\sqrt{k}$优势是因为随机性本身还是因为无视敌手的假设。通过给出肯定的答案来回答这个问题，我们通过获得从随机性出发而不是从无视敌手假设出发的$\sqrt{k}$优势。

    We study the distributed tracking model, also known as distributed functional monitoring. This model involves $k$ sites each receiving a stream of items and communicating with the central server. The server's task is to track a function of all items received thus far continuously, with minimum communication cost. For count tracking, it is known that there is a $\sqrt{k}$ gap in communication between deterministic and randomized algorithms. However, existing randomized algorithms assume an "oblivious adversary" who constructs the entire input streams before the algorithm starts. Here we consider adaptive adversaries who can choose new items based on previous answers from the algorithm. Deterministic algorithms are trivially robust to adaptive adversaries, while randomized ones may not. Therefore, we investigate whether the $\sqrt{k}$ advantage of randomized algorithms is from randomness itself or the oblivious adversary assumption. We provide an affirmative answer to this question by gi
    
[^52]: 《2023年开放式DAC数据集和直接空气捕集中吸附剂发现的挑战》

    The Open DAC 2023 Dataset and Challenges for Sorbent Discovery in Direct Air Capture. (arXiv:2311.00341v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2311.00341](http://arxiv.org/abs/2311.00341)

    本论文介绍了一种计算方法，利用机器学习的创新，构建了一个包含超过8,800个MOF材料的超大规模数据集，该数据集对于直接空气捕集中有前途的吸附剂发现具有重要意义。

    

    急需新的二氧化碳去除方法来应对全球气候变化。直接空气捕集(DAC)是一种从环境空气中直接捕集二氧化碳的新兴技术。金属有机框架(MOFs)被广泛研究作为DAC的潜在可定制吸附剂。然而，由于需要探索的巨大化学空间和需要了解材料随湿度和温度变化的特性，发现有前途的DAC MOF吸附剂是具有挑战性的。我们使用机器学习的最新创新，探索了一种计算方法，并提供了一个名为Open DAC 2023 (ODAC23)的数据集，其中包含超过8,800个含有吸附CO2和/或H2O的MOF材料的超过3800万的密度泛函理论（DFT）计算。 ODAC23是目前可用精确度的DFT级别中最大的MOF吸附计算数据集。除了探索吸附分子的性质外，这个数据集还是信息的丰富来源。

    New methods for carbon dioxide removal are urgently needed to combat global climate change. Direct air capture (DAC) is an emerging technology to capture carbon dioxide directly from ambient air. Metal-organic frameworks (MOFs) have been widely studied as potentially customizable adsorbents for DAC. However, discovering promising MOF sorbents for DAC is challenging because of the vast chemical space to explore and the need to understand materials as functions of humidity and temperature. We explore a computational approach benefiting from recent innovations in machine learning (ML) and present a dataset named Open DAC 2023 (ODAC23) consisting of more than 38M density functional theory (DFT) calculations on more than 8,800 MOF materials containing adsorbed CO2 and/or H2O. ODAC23 is by far the largest dataset of MOF adsorption calculations at the DFT level of accuracy currently available. In addition to probing properties of adsorbed molecules, the dataset is a rich source of information
    
[^53]: MetisFL:一种可扩展和高效的联邦学习工作流的尴尬并行控制器

    MetisFL: An Embarrassingly Parallelized Controller for Scalable & Efficient Federated Learning Workflows. (arXiv:2311.00334v1 [cs.LG])

    [http://arxiv.org/abs/2311.00334](http://arxiv.org/abs/2311.00334)

    MetisFL是一种可扩展和高效的联邦学习系统，重点关注联邦控制器的可扩展性和优化。

    

    联邦学习(FL)系统通常由两个核心处理实体组成:联邦控制器和学习器。控制器负责管理在学习器之间执行FL工作流程，学习器负责在其私有数据集上训练和评估联邦模型。在执行FL工作流时，FL系统对参与学习器的计算资源或数据没有控制。尽管最近提出了许多FL系统来促进FL工作流的开发，但这些系统中大多数忽视了控制器的可扩展性。为了满足这一需求，我们设计和开发了一种名为MetisFL的新型FL系统，其中联邦控制器是第一等公民。

    A Federated Learning (FL) system typically consists of two core processing entities: the federation controller and the learners. The controller is responsible for managing the execution of FL workflows across learners and the learners for training and evaluating federated models over their private datasets. While executing an FL workflow, the FL system has no control over the computational resources or data of the participating learners. Still, it is responsible for other operations, such as model aggregation, task dispatching, and scheduling. These computationally heavy operations generally need to be handled by the federation controller. Even though many FL systems have been recently proposed to facilitate the development of FL workflows, most of these systems overlook the scalability of the controller. To meet this need, we designed and developed a novel FL system called MetisFL, where the federation controller is the first-class citizen. MetisFL re-engineers all the operations cond
    
[^54]: 空间转录组的潜在空间推理

    Latent Space Inference For Spatial Transcriptomics. (arXiv:2311.00330v1 [cs.LG])

    [http://arxiv.org/abs/2311.00330](http://arxiv.org/abs/2311.00330)

    本研究通过概率机器学习方法，将单细胞RNA测序和基于图像的空间转录组学数据映射到共同的潜在空间表示中，以获取组织样本的完整遗传表达信息，并保留空间坐标。这为深入理解细胞过程和通路提供了更多的见解。

    

    为了理解细胞生物学的复杂性，研究人员对细胞的遗传表达信息和其在组织样本中的空间坐标感兴趣。然而，最先进的方法，即单细胞RNA测序和基于图像的空间转录组学，只能恢复这些信息的子集，要么完整的遗传表达信息丢失了空间信息，要么空间信息丢失了测序数据的分辨率。在本项目中，我们研究了一种概率机器学习方法，以获取组织样本的完整遗传表达信息，并保留它们的空间坐标。这是通过利用变分机器学习方法将两个数据集映射到共同的潜在空间表示中来实现的。从这里，可以解码出完整的遗传和空间信息，以更深入地了解细胞过程和通路。

    In order to understand the complexities of cellular biology, researchers are interested in two important metrics: the genetic expression information of cells and their spatial coordinates within a tissue sample. However, state-of-the art methods, namely single-cell RNA sequencing and image based spatial transcriptomics can only recover a subset of this information, either full genetic expression with loss of spatial information, or spatial information with loss of resolution in sequencing data. In this project, we investigate a probabilistic machine learning method to obtain the full genetic expression information for tissues samples while also preserving their spatial coordinates. This is done through mapping both datasets to a joint latent space representation with the use of variational machine learning methods. From here, the full genetic and spatial information can be decoded and to give us greater insights on the understanding of cellular processes and pathways.
    
[^55]: 多任务表示学习用于双线性bandit中的纯探索问题

    Multi-task Representation Learning for Pure Exploration in Bilinear Bandits. (arXiv:2311.00327v1 [cs.LG])

    [http://arxiv.org/abs/2311.00327](http://arxiv.org/abs/2311.00327)

    这项研究通过多任务表示学习解决了双线性bandit中的纯探索问题，并提出了GOBLIN算法来优化样本分配和减小样本数量，在共享表示下实现了较高的效率。

    

    我们研究了多任务表示学习在双线性bandit的纯探索问题中的应用。在双线性bandit中，一个动作由来自两种不同实体类型的两个臂构成，奖励是两个臂的已知特征向量的双线性函数。在多任务双线性bandit问题中，我们的目标是寻找多个任务的最佳动作，这些任务共享一个共同的低维线性表示。我们的目标是利用这个特征加速确定所有任务的最佳臂对的过程。我们提出了算法GOBLIN，该算法使用实验设计方法来优化学习全局表示的样本分配，并最小化识别个别任务中最佳臂对所需的样本数。据我们所知，这是第一项在共享表示下对双线性bandit中的纯探索问题进行样本复杂性分析的研究。我们的结果表明，

    We study multi-task representation learning for the problem of pure exploration in bilinear bandits. In bilinear bandits, an action takes the form of a pair of arms from two different entity types and the reward is a bilinear function of the known feature vectors of the arms. In the \textit{multi-task bilinear bandit problem}, we aim to find optimal actions for multiple tasks that share a common low-dimensional linear representation. The objective is to leverage this characteristic to expedite the process of identifying the best pair of arms for all tasks. We propose the algorithm GOBLIN that uses an experimental design approach to optimize sample allocations for learning the global representation as well as minimize the number of samples needed to identify the optimal pair of arms in individual tasks. To the best of our knowledge, this is the first study to give sample complexity analysis for pure exploration in bilinear bandits with shared representation. Our results demonstrate that
    
[^56]: 噪声图中的鲁棒图聚类通过元权重

    Robust Graph Clustering via Meta Weighting for Noisy Graphs. (arXiv:2311.00322v1 [cs.LG])

    [http://arxiv.org/abs/2311.00322](http://arxiv.org/abs/2311.00322)

    该论文提出了一种鲁棒的基于元权重的图聚类方法，通过对节点对应权重的自适应调整，能够在存在噪声边的图中找到有意义的聚类。

    

    如何在噪声边上鲁棒地找到图中的有意义的聚类？图聚类是图分析中的一个基本问题，应用于各个领域。最近的研究表明，基于图神经网络（GNN）的方法在图聚类方面取得了有希望的结果。然而，我们观察到它们在存在噪声边的图上的性能明显下降。在这项工作中，我们提出了用于鲁棒GNN-based图聚类的MetaGC。MetaGC采用可分解的聚类损失函数，将其重新表述为节点对之间损失的求和。我们为每个节点对添加可学习的权重，并使用元权重来自适应地调整节点对的权重，使有意义的节点对的权重增加，而不那么有意义的节点对（例如噪声边）的权重减小。我们通过实验证明，MetaGC按照预期学习权重，并且因此在性能上优于其他方法。

    How can we find meaningful clusters in a graph robustly against noise edges? Graph clustering (i.e., dividing nodes into groups of similar ones) is a fundamental problem in graph analysis with applications in various fields. Recent studies have demonstrated that graph neural network (GNN) based approaches yield promising results for graph clustering. However, we observe that their performance degenerates significantly on graphs with noise edges, which are prevalent in practice. In this work, we propose MetaGC for robust GNN-based graph clustering. MetaGC employs a decomposable clustering loss function, which can be rephrased as a sum of losses over node pairs. We add a learnable weight to each node pair, and MetaGC adaptively adjusts the weights of node pairs using meta-weighting so that the weights of meaningful node pairs increase and the weights of less-meaningful ones (e.g., noise edges) decrease. We show empirically that MetaGC learns weights as intended and consequently outperfor
    
[^57]: 带有双耳听功能的语义听力：编程声学场景

    Semantic Hearing: Programming Acoustic Scenes with Binaural Hearables. (arXiv:2311.00320v1 [cs.SD])

    [http://arxiv.org/abs/2311.00320](http://arxiv.org/abs/2311.00320)

    本论文提出了语义听力的概念，实现了带有双耳听功能的听觉设备，能够在真实环境中实时关注或忽略特定的声音，并保持空间线索。研究贡献包括首个能够在干扰声和背景噪音中实现双耳目标声音提取的神经网络以及可推广到实际使用场景的训练方法。研究结果展示了系统在多种声音类别下的性能和运行时间数据，并在“野外”评估中获得了良好的效果。

    

    想象一下，在公园里可以听到鸟儿在鸣叫，而不会听到其他徒步者的聒噪声；或者在繁忙的街道上可以屏蔽交通噪音，同时仍然能听到紧急警笛和汽车鸣笛声。我们介绍了语义听力，这是一种新颖的听觉设备功能，能够实时地关注或忽略真实环境中特定的声音，同时保留空间线索。为了实现这一目标，我们做出了两个技术贡献：1）我们提出了第一个能够在干扰声和背景噪音存在的情况下实现双耳目标声音提取的神经网络，2）我们设计了一种训练方法，使我们的系统能够推广到实际的使用场景。结果显示，我们的系统可以处理20个声音类别，并且我们基于Transformer的网络在连接的智能手机上的运行时间为6.56毫秒。在未知的室内外场景中的“野外”评估中，参与者的测试显示我们的系统表现良好。

    Imagine being able to listen to the birds chirping in a park without hearing the chatter from other hikers, or being able to block out traffic noise on a busy street while still being able to hear emergency sirens and car honks. We introduce semantic hearing, a novel capability for hearable devices that enables them to, in real-time, focus on, or ignore, specific sounds from real-world environments, while also preserving the spatial cues. To achieve this, we make two technical contributions: 1) we present the first neural network that can achieve binaural target sound extraction in the presence of interfering sounds and background noise, and 2) we design a training methodology that allows our system to generalize to real-world use. Results show that our system can operate with 20 sound classes and that our transformer-based network has a runtime of 6.56 ms on a connected smartphone. In-the-wild evaluation with participants in previously unseen indoor and outdoor scenarios shows that ou
    
[^58]: 用于稳定训练生成对抗网络的泛化正则化

    Flooding Regularization for Stable Training of Generative Adversarial Networks. (arXiv:2311.00318v1 [cs.LG])

    [http://arxiv.org/abs/2311.00318](http://arxiv.org/abs/2311.00318)

    本文在生成对抗网络中直接对对抗损失函数进行正则化，通过应用泛化方法防止判别器的损失过分降低，并通过实验证实了其稳定性。

    

    生成对抗网络（GANs）在图像生成方面表现出了显著的性能。然而，GAN训练存在不稳定的问题。解决这个问题的主要方法之一是修改损失函数，通常使用正则化项来改变对抗损失的类型。本文着眼于直接对对抗损失函数进行正则化。我们提出了一种方法，将泛化（过拟合抑制）方法应用于GANs中，直接防止判别器的损失过分降低。泛化需要调整泛化水平，但当应用于GANs时，我们提出适当的泛化水平设置范围由对抗损失函数确定，该论据得到了使用二元交叉熵损失对GANs进行理论分析的支持。我们通过实验证实了泛化稳定了GAN训练，并且可以与其他稳定技术相结合。我们还揭示了通过限制

    Generative Adversarial Networks (GANs) have shown remarkable performance in image generation. However, GAN training suffers from the problem of instability. One of the main approaches to address this problem is to modify the loss function, often using regularization terms in addition to changing the type of adversarial losses. This paper focuses on directly regularizing the adversarial loss function. We propose a method that applies flooding, an overfitting suppression method in supervised learning, to GANs to directly prevent the discriminator's loss from becoming excessively low. Flooding requires tuning the flood level, but when applied to GANs, we propose that the appropriate range of flood level settings is determined by the adversarial loss function, supported by theoretical analysis of GANs using the binary cross entropy loss. We experimentally verify that flooding stabilizes GAN training and can be combined with other stabilization techniques. We also reveal that by restricting
    
[^59]: 用可比较的语料和多个参考文献进行代码翻译的数据增强

    Data Augmentation for Code Translation with Comparable Corpora and Multiple References. (arXiv:2311.00317v1 [cs.CL])

    [http://arxiv.org/abs/2311.00317](http://arxiv.org/abs/2311.00317)

    该论文介绍了两种数据增强方法来改善编程语言之间的代码翻译。通过构建可比较的语料库和增加多个参考翻译，实验结果表明这些方法显著提高了CodeT5在Java、Python和C++之间的翻译准确性。

    

    在编程语言之间进行代码翻译的一个主要挑战是平行训练数据通常有限。为了克服这个挑战，我们提出了两种数据增强技术，一种是构建可比较的语料库（即具有类似功能的代码对），另一种是用多个参考翻译来增强现有的平行数据。具体而言，我们构建并分析了多种类型的可比较的语料库，包括使用代码生成模型从自然语言文档中生成的程序。此外，为了减少对单个参考翻译的过拟合，我们自动生成了可用平行数据的额外翻译参考，并通过单元测试对翻译进行筛选，从而增加了目标翻译的变化。实验证明，我们的数据增强技术显著提高了CodeT5在Java、Python和C++之间的翻译准确性（平均提升了7.5%的计算准确性（CA@1））。

    One major challenge of translating code between programming languages is that parallel training data is often limited. To overcome this challenge, we present two data augmentation techniques, one that builds comparable corpora (i.e., code pairs with similar functionality), and another that augments existing parallel data with multiple reference translations. Specifically, we build and analyze multiple types of comparable corpora, including programs generated from natural language documentation using a code generation model. Furthermore, to reduce overfitting to a single reference translation, we automatically generate additional translation references for available parallel data and filter the translations by unit tests, which increases variation in target translations. Experiments show that our data augmentation techniques significantly improve CodeT5 for translation between Java, Python, and C++ by an average of 7.5% Computational Accuracy (CA@1), which verifies the correctness of tr
    
[^60]: 基于变分自编码器的联邦主题模型和模型剪枝

    Federated Topic Model and Model Pruning Based on Variational Autoencoder. (arXiv:2311.00314v1 [cs.LG])

    [http://arxiv.org/abs/2311.00314](http://arxiv.org/abs/2311.00314)

    本论文提出了一种基于变分自编码器的联邦主题模型和模型剪枝方法，用于解决跨多个方参与交叉分析时的数据隐私问题，并通过神经网络模型剪枝加速模型。两种不同的方法被提出来确定模型剪枝率。

    

    主题建模已经成为在大规模文档集合中发现模式和主题的有价值工具。然而，当跨多个方参与交叉分析时，数据隐私成为一个关键问题。联邦主题建模已经被开发出来解决这个问题，允许多个参与方在保护隐私的同时共同训练模型。然而，在联邦场景中存在通信和性能挑战。为了解决上述问题，本文提出了一种建立联邦主题模型并确保每个节点隐私的方法，并使用神经网络模型剪枝加速模型，其中客户端定期将模型神经元累积梯度和模型权重发送给服务器，服务器对模型进行剪枝。为了满足不同的要求，提出了两种确定模型剪枝率的不同方法。

    Topic modeling has emerged as a valuable tool for discovering patterns and topics within large collections of documents. However, when cross-analysis involves multiple parties, data privacy becomes a critical concern. Federated topic modeling has been developed to address this issue, allowing multiple parties to jointly train models while protecting pri-vacy. However, there are communication and performance challenges in the federated sce-nario. In order to solve the above problems, this paper proposes a method to establish a federated topic model while ensuring the privacy of each node, and use neural network model pruning to accelerate the model, where the client periodically sends the model neu-ron cumulative gradients and model weights to the server, and the server prunes the model. To address different requirements, two different methods are proposed to determine the model pruning rate. The first method involves slow pruning throughout the entire model training process, which has 
    
[^61]: 使用堆叠自编码器进行零日威胁的特征选择

    Stacking an autoencoder for feature selection of zero-day threats. (arXiv:2311.00304v1 [cs.CR])

    [http://arxiv.org/abs/2311.00304](http://arxiv.org/abs/2311.00304)

    本研究使用堆叠自编码器和长短期记忆方案，通过特征提取和微调，成功应用于零日威胁的特征选择和分类。研究结果表明，该方法在各类攻击中表现出色，并能实现准确的分类。

    

    零日攻击检测在缓解风险、保护资产和在不断变化的威胁环境中保持领先地位中起着至关重要的作用。本研究探讨了堆叠自编码器（SAE），一种人工神经网络，用于特征选择和零日威胁分类，采用了长短期记忆（LSTM）方案。该过程包括对UGRansome数据集进行预处理，并训练无监督的SAE进行特征提取。然后进行有监督学习的微调，以增强该模型的区分能力。分析自编码器的学习权重和激活来识别区分零日威胁和正常系统行为的最重要特征。这些选定的特征形成了一个降维特征集，能够实现准确的分类。结果表明，SAE-LSTM在所有三类攻击中表现良好，展示了高精确度、召回率和F1分数值，强调了其创新性和贡献性。

    Zero-day attack detection plays a critical role in mitigating risks, protecting assets, and staying ahead in the evolving threat landscape. This study explores the application of stacked autoencoder (SAE), a type of artificial neural network, for feature selection and zero-day threat classification using a Long Short-Term Memory (LSTM) scheme. The process involves preprocessing the UGRansome dataset and training an unsupervised SAE for feature extraction. Finetuning with supervised learning is then performed to enhance the discriminative capabilities of this model. The learned weights and activations of the autoencoder are analyzed to identify the most important features for discriminating between zero-day threats and normal system behavior. These selected features form a reduced feature set that enables accurate classification. The results indicate that the SAE-LSTM performs well across all three attack categories by showcasing high precision, recall, and F1 score values, emphasizing 
    
[^62]: CO2流动模式的推断--可行性研究

    Inference of CO2 flow patterns -- a feasibility study. (arXiv:2311.00290v1 [cs.CE])

    [http://arxiv.org/abs/2311.00290](http://arxiv.org/abs/2311.00290)

    研究对地下CO2泄漏进行监测和检测，提出了一种新的方法来描述CO2流动模式中的不确定性，并强调不确定性评估的重要性。

    

    随着全球碳捕获和封存（CCS）技术在应对气候变化的斗争中的大规模部署，建立稳健的监测和检测机制，以检测潜在的地下CO2泄漏，特别是通过存储库封堵的预先存在或诱导的断层，变得越来越迫切。虽然诸如历史匹配和CO2储存的时间序列地震监测等技术已成功用于跟踪地下CO2渗漏的演化，但这些方法缺乏对CO2波动行为相关不确定性的基本方法。系统评估不确定性的纳入对于风险缓解至关重要，原因如下：（i）CO2波动诱发的变化很小且地震数据噪声很大；（ii）正常和不规则（例如泄漏引起的）流动模式之间的变化很小；（iii）控制流动的储层特性强烈异质且通常

    As the global deployment of carbon capture and sequestration (CCS) technology intensifies in the fight against climate change, it becomes increasingly imperative to establish robust monitoring and detection mechanisms for potential underground CO2 leakage, particularly through pre-existing or induced faults in the storage reservoir's seals. While techniques such as history matching and time-lapse seismic monitoring of CO2 storage have been used successfully in tracking the evolution of CO2 plumes in the subsurface, these methods lack principled approaches to characterize uncertainties related to the CO2 plumes' behavior. Inclusion of systematic assessment of uncertainties is essential for risk mitigation for the following reasons: (i) CO2 plume-induced changes are small and seismic data is noisy; (ii) changes between regular and irregular (e.g., caused by leakage) flow patterns are small; and (iii) the reservoir properties that control the flow are strongly heterogeneous and typically 
    
[^63]: 通过大型语言模型的知识注入：评估和推进临床文本数据生成

    Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models. (arXiv:2311.00287v1 [cs.CL])

    [http://arxiv.org/abs/2311.00287](http://arxiv.org/abs/2311.00287)

    本文提出了一种通过大型语言模型进行临床文本生成的创新方法ClinGen，该方法将外部领域特定的知识和语言模型结合起来，提高了临床自然语言处理任务的性能，并丰富了样本的多样性。

    

    临床自然语言处理需要能够应对领域特定挑战的方法，例如复杂的医学术语和临床背景。最近，大型语言模型（LLMs）在这个领域显示出了潜力。然而，它们的直接部署可能导致隐私问题，并受到资源限制。为了解决这个挑战，我们深入研究了使用LLMs进行临床NLP任务的合成临床文本生成。我们提出了一种创新的、资源高效的方法ClinGen，它将知识注入到这个过程中。我们的模型涉及临床知识提取和基于上下文的LLM提示。临床主题和写作风格都来自外部领域特定的知识图谱和LLMs，以引导数据生成。我们在7个临床NLP任务和16个数据集上进行了广泛的实证研究，结果显示ClinGen在各种任务中始终提高了性能，有效地使真实数据集的分布对齐，并显著丰富了样本的多样性。

    Clinical natural language processing requires methods that can address domain-specific challenges, such as complex medical terminology and clinical contexts. Recently, large language models (LLMs) have shown promise in this domain. Yet, their direct deployment can lead to privacy issues and are constrained by resources. To address this challenge, we delve into synthetic clinical text generation using LLMs for clinical NLP tasks. We propose an innovative, resource-efficient approach, ClinGen, which infuses knowledge into the process. Our model involves clinical knowledge extraction and context-informed LLM prompting. Both clinical topics and writing styles are drawn from external domain-specific knowledge graphs and LLMs to guide data generation. Our extensive empirical study across 7 clinical NLP tasks and 16 datasets reveals that ClinGen consistently enhances performance across various tasks, effectively aligning the distribution of real datasets and significantly enriching the divers
    
[^64]: JADE：基于语言的LLM安全评估平台

    JADE: A Linguistic-based Safety Evaluation Platform for LLM. (arXiv:2311.00286v1 [cs.CL])

    [http://arxiv.org/abs/2311.00286](http://arxiv.org/abs/2311.00286)

    JADE是一种基于语言分析的LLM安全评估平台，能够破坏广泛使用的中文和英文LLM，并生成高度威胁的不安全问题。

    

    本文介绍了JADE，一种针对语言分析的模糊测试平台，通过增强种子问题的语言复杂性，同时并始终能够破坏广泛使用的三类LLM：八个开源中文LLM，六个商业中文LLM和四个商业英文LLM。JADE为这三类LLM生成了三个安全基准，其中包含高度威胁的不安全问题：这些问题可以同时触发多个LLM的有害生成，平均不安全生成比例为70%（请参见下表），同时这些问题仍然是自然、流畅且保留了核心的不安全语义。我们在以下链接中发布了对商业英文LLM和开源英文LLM生成的基准演示：https://github.com/whitzard-ai/jade-db。对于对JADE生成的更多问题感兴趣的读者，请与我们联系。

    In this paper, we present \textit{JADE}, a targeted linguistic fuzzing platform which strengthens the linguistic complexity of seed questions to simultaneously and consistently break a wide range of widely-used LLMs categorized in three groups: eight open-sourced Chinese, six commercial Chinese and four commercial English LLMs. JADE generates three safety benchmarks for the three groups of LLMs, which contain unsafe questions that are highly threatening: the questions simultaneously trigger harmful generation of multiple LLMs, with an average unsafe generation ratio of \textbf{$70\%$} (please see the table below), while are still natural questions, fluent and preserving the core unsafe semantics. We release the benchmark demos generated for commercial English LLMs and open-sourced English LLMs in the following link: https://github.com/whitzard-ai/jade-db. For readers who are interested in evaluating on more questions generated by JADE, please contact us.  \textit{JADE} is based on Noam
    
[^65]: Mixture-of-Experts用于开放域适应的双空间检测方法

    Mixture-of-Experts for Open Set Domain Adaptation: A Dual-Space Detection Approach. (arXiv:2311.00285v1 [cs.CV])

    [http://arxiv.org/abs/2311.00285](http://arxiv.org/abs/2311.00285)

    该论文提出了一种Mixture-of-Experts用于开放域适应的双空间检测方法，利用图像特征空间和路由特征空间之间的不一致性来检测未知类别的样本，无需手动调节阈值。

    

    开放域适应（OSDA）旨在同时处理源域和目标域之间的分布和标签偏移，实现对已知类别的精确分类，同时在目标域中识别未知类别的样本。大多数现有的OSDA方法依赖于深度模型的最终图像特征空间，需要手动调节阈值，并且可能将未知样本错误分类为已知类别。Mixture-of-Expert（MoE）可能是一种解决方法。在MoE中，不同的专家处理不同的输入特征，在路由特征空间中为不同的类别生成独特的专家路由模式。因此，未知类别的样本也可以显示与已知类别不同的专家路由模式。本文提出了双空间检测，利用图像特征空间和路由特征空间之间的不一致性来检测未知类别的样本，无需任何阈值。进一步介绍了图形路由器来更好地利用摘要的信息。

    Open Set Domain Adaptation (OSDA) aims to cope with the distribution and label shifts between the source and target domains simultaneously, performing accurate classification for known classes while identifying unknown class samples in the target domain. Most existing OSDA approaches, depending on the final image feature space of deep models, require manually-tuned thresholds, and may easily misclassify unknown samples as known classes. Mixture-of-Expert (MoE) could be a remedy. Within an MoE, different experts address different input features, producing unique expert routing patterns for different classes in a routing feature space. As a result, unknown class samples may also display different expert routing patterns to known classes. This paper proposes Dual-Space Detection, which exploits the inconsistencies between the image feature space and the routing feature space to detect unknown class samples without any threshold. Graph Router is further introduced to better make use of the
    
[^66]: 机器学习组件的模型驱动工程：系统文献综述

    Model-driven Engineering for Machine Learning Components: A Systematic Literature Review. (arXiv:2311.00284v1 [cs.SE])

    [http://arxiv.org/abs/2311.00284](http://arxiv.org/abs/2311.00284)

    这篇论文通过系统文献综述进一步探索了模型驱动工程与机器学习的有前途的交叉领域。

    

    背景：机器学习在许多现代软件应用中作为组件被广泛采用。由于可用的数据量庞大，组织希望越来越多地利用数据提取有意义的洞察和增强业务利润能力。机器学习组件实现了预测能力、异常检测、推荐、准确的图像和文本处理以及知情决策。然而，开发带有机器学习组件的系统并不简单；它需要时间、精力、机器学习、数据处理和软件工程方面的知识和专业知识。之前已经有几项研究针对在开发传统软件和物理系统时应用模型驱动工程（MDE）技术来解决这些挑战。最近，对于将MDE应用于带有机器学习组件的系统的兴趣越来越大。目标：本研究的目标是通过系统文献综述进一步探索MDE与机器学习（MDE4ML）的有前途的交叉领域。

    Context: Machine Learning (ML) has become widely adopted as a component in many modern software applications. Due to the large volumes of data available, organizations want to increasingly leverage their data to extract meaningful insights and enhance business profitability. ML components enable predictive capabilities, anomaly detection, recommendation, accurate image and text processing, and informed decision-making. However, developing systems with ML components is not trivial; it requires time, effort, knowledge, and expertise in ML, data processing, and software engineering. There have been several studies on the use of model-driven engineering (MDE) techniques to address these challenges when developing traditional software and cyber-physical systems. Recently, there has been a growing interest in applying MDE for systems with ML components. Objective: The goal of this study is to further explore the promising intersection of MDE with ML (MDE4ML) through a systematic literature r
    
[^67]: 标签噪声随机梯度下降的泛化界

    Generalization Bounds for Label Noise Stochastic Gradient Descent. (arXiv:2311.00274v1 [stat.ML])

    [http://arxiv.org/abs/2311.00274](http://arxiv.org/abs/2311.00274)

    本研究通过在非凸设置中使用标签噪声对随机梯度下降进行了泛化错误界限的研究，利用算法稳定性框架得到了时间无关的泛化错误界限，并且在参数维度和样本大小的速率以及特定学习率情况下实现了多项式的错误界限。该分析提供了关于标签噪声影响的量化见解。

    

    我们在非凸设置中，基于均匀耗散和平滑条件，为具有标签噪声的随机梯度下降（SGD）开发了泛化错误界限。在适当选择的半度量下，我们建立了与参数维度$d$多项式相关的标签噪声随机梯度流的Wasserstein距离收缩。利用算法稳定性框架，我们为离散化算法推导了独立于时间的泛化错误界限，并使用固定学习率。我们实现的错误界限与$d$和样本大小$n$的速率以及$n^{-2/3}$有多项式的关系。这个速率在与类似条件下使用参数无关高斯噪声的随机梯度Langevin动力学（SGLD）中，其最佳已知速率$n^{-1/2}$要好。我们的分析提供了关于标签噪声影响的量化见解。

    We develop generalization error bounds for stochastic gradient descent (SGD) with label noise in non-convex settings under uniform dissipativity and smoothness conditions. Under a suitable choice of semimetric, we establish a contraction in Wasserstein distance of the label noise stochastic gradient flow that depends polynomially on the parameter dimension $d$. Using the framework of algorithmic stability, we derive time-independent generalisation error bounds for the discretized algorithm with a constant learning rate. The error bound we achieve scales polynomially with $d$ and with the rate of $n^{-2/3}$, where $n$ is the sample size. This rate is better than the best-known rate of $n^{-1/2}$ established for stochastic gradient Langevin dynamics (SGLD) -which employs parameter-independent Gaussian noise -- under similar conditions. Our analysis offers quantitative insights into the effect of label noise.
    
[^68]: 通过分层强化学习重新思考决策Transformer

    Rethinking Decision Transformer via Hierarchical Reinforcement Learning. (arXiv:2311.00267v1 [cs.LG])

    [http://arxiv.org/abs/2311.00267](http://arxiv.org/abs/2311.00267)

    这篇论文通过引入分层强化学习，重新思考了决策Transformer。他们提出了一个通用的序列建模框架，在该框架中，高层策略为当前状态提供理想提示，低层策略在给定提示的条件下生成动作。他们发现决策Transformer是这个框架的一个特例，并研究了如何共同优化高层和低层策略以实现拼接能力，从而推动了新的离线学习算法的发展。

    

    决策Transformer（DT）是一种利用最近在强化学习中的Transformer架构的创新算法。然而，DT的一个显著局限性是其依赖于从数据集中回忆轨迹的能力，失去了无缝地将次优轨迹拼接在一起的能力。在这项工作中，我们引入了一个用于通过分层强化学习研究序贯决策的通用序列建模框架。在做决策时，高层策略首先为当前状态提出一个理想的提示，低层策略随后在给定的提示条件下生成一个动作。我们展示了DT是这个框架的特例，通过一定的高层和低层策略选择，并讨论了这些选择的潜在失败。受这些观察的启发，我们研究了如何共同优化高层和低层策略以实现拼接能力，进而推动新的离线学习算法的发展。

    Decision Transformer (DT) is an innovative algorithm leveraging recent advances of the transformer architecture in reinforcement learning (RL). However, a notable limitation of DT is its reliance on recalling trajectories from datasets, losing the capability to seamlessly stitch sub-optimal trajectories together. In this work we introduce a general sequence modeling framework for studying sequential decision making through the lens of Hierarchical RL. At the time of making decisions, a high-level policy first proposes an ideal prompt for the current state, a low-level policy subsequently generates an action conditioned on the given prompt. We show DT emerges as a special case of this framework with certain choices of high-level and low-level policies, and discuss the potential failure of these choices. Inspired by these observations, we study how to jointly optimize the high-level and low-level policies to enable the stitching ability, which further leads to the development of new offl
    
[^69]: 激励合作的主动学习

    Incentivized Collaboration in Active Learning. (arXiv:2311.00260v1 [cs.GT])

    [http://arxiv.org/abs/2311.00260](http://arxiv.org/abs/2311.00260)

    该论文提出了一种激励合作的主动学习框架，旨在最小化标签复杂性，保证智能体无法通过个人行为减少其预期的标签复杂性，并提供了可实现且在标签复杂性方面与最佳可计算近似算法相媲美的合作协议。

    

    在协作的主动学习中，多个智能体试图从共同的假设中学习标签，我们提出了一种创新的激励合作框架。在这里，理性的智能体旨在获得他们数据集的标签，同时尽量减少标签复杂性。我们着重设计（严格的）个体合理（IR）合作协议，确保智能体无法通过个人行为减少其预期的标签复杂性。我们首先证明，对于任何最优的主动学习算法，运行该算法直至整个数据集的合作协议已经是IR的。然而，计算最优算法是NP难的。因此，我们提供了可以实现（严格的）IR且在标签复杂性方面与最佳已知可计算近似算法相媲美的合作协议。

    In collaborative active learning, where multiple agents try to learn labels from a common hypothesis, we introduce an innovative framework for incentivized collaboration. Here, rational agents aim to obtain labels for their data sets while keeping label complexity at a minimum. We focus on designing (strict) individually rational (IR) collaboration protocols, ensuring that agents cannot reduce their expected label complexity by acting individually. We first show that given any optimal active learning algorithm, the collaboration protocol that runs the algorithm as is over the entire data is already IR. However, computing the optimal algorithm is NP-hard. We therefore provide collaboration protocols that achieve (strict) IR and are comparable with the best known tractable approximation algorithm in terms of label complexity.
    
[^70]: 通过基于有限差分的无监督小型线性卷积神经网络解决椭圆和抛物型问题

    Solutions to Elliptic and Parabolic Problems via Finite Difference Based Unsupervised Small Linear Convolutional Neural Networks. (arXiv:2311.00259v1 [cs.LG])

    [http://arxiv.org/abs/2311.00259](http://arxiv.org/abs/2311.00259)

    通过无监督学习，我们提出了利用小型卷积神经网络直接估计椭圆和抛物型问题的有限差分解，相比传统方法具有可比较的准确性。

    

    近年来，利用深度学习和神经网络解决科学问题，特别是求解偏微分方程（PDE），引起了广泛关注。然而，目前基于神经网络的PDE求解器往往依赖于大量的训练数据或标记的输入-输出对，这使得它们在推广到分布之外的示例时容易面临挑战。为了减少传统基于神经网络方法在估计PDE解时遇到的广义化差距，我们提出了一种完全无监督的方法，无需训练数据，通过小型卷积神经网络直接估计PDE的有限差分解。与有限差分法相比，我们提出的算法在几个选定的椭圆和抛物问题上表现出与真解相当的准确性。

    In recent years, there has been a growing interest in leveraging deep learning and neural networks to address scientific problems, particularly in solving partial differential equations (PDEs). However, current neural network-based PDE solvers often rely on extensive training data or labeled input-output pairs, making them prone to challenges in generalizing to out-of-distribution examples. To mitigate the generalization gap encountered by conventional neural network-based methods in estimating PDE solutions, we formulate a fully unsupervised approach, requiring no training data, to estimate finite difference solutions for PDEs directly via small convolutional neural networks. Our proposed algorithms demonstrate a comparable accuracy to the true solution for several selected elliptic and parabolic problems compared to the finite difference method.
    
[^71]: 有噪声的样本使得大型语言模型更加鲁棒：一个领域不可知的行为分析

    Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic Behavioral Analysis. (arXiv:2311.00258v1 [cs.CL])

    [http://arxiv.org/abs/2311.00258](http://arxiv.org/abs/2311.00258)

    本研究通过领域不可知的扰动测试了大型语言模型在多跳推理任务中的鲁棒性，发现模型对某些扰动更敏感，并证明增加扰动样本的比例可以提高少样本提示方法的鲁棒性。

    

    最近对问题引导的工程进展使得大型语言模型（LLMs）能够以令人印象深刻的准确率解决多跳逻辑推理问题。然而，目前很少有工作研究少样本提示技术下LLMs的鲁棒性。因此，我们介绍了一种系统的方法，通过领域不可知的扰动来测试LLMs在多跳推理任务中的鲁棒性。我们在多个抽象层次上引入扰动（例如词法扰动，如拼写错误，以及语义扰动，如在问题中包含中间推理步骤），对LLMs进行行为分析。通过实验，我们发现模型对某些扰动（如用同义词替换单词）更敏感。我们还证明，在提示中增加扰动样本的比例可以提高少样本提示方法的鲁棒性。

    Recent advances in prompt engineering enable large language models (LLMs) to solve multi-hop logical reasoning problems with impressive accuracy. However, there is little existing work investigating the robustness of LLMs with few-shot prompting techniques. Therefore, we introduce a systematic approach to test the robustness of LLMs in multi-hop reasoning tasks via domain-agnostic perturbations. We include perturbations at multiple levels of abstractions (e.g. lexical perturbations such as typos, and semantic perturbations such as the inclusion of intermediate reasoning steps in the questions) to conduct behavioral analysis on the LLMs. Throughout our experiments, we find that models are more sensitive to certain perturbations such as replacing words with their synonyms. We also demonstrate that increasing the proportion of perturbed exemplars in the prompts improves the robustness of few-shot prompting methods.
    
[^72]: 多智能体探索的主动神经拓扑映射

    Active Neural Topological Mapping for Multi-Agent Exploration. (arXiv:2311.00252v1 [cs.RO])

    [http://arxiv.org/abs/2311.00252](http://arxiv.org/abs/2311.00252)

    本文提出了一种名为多智能体神经拓扑映射（MANTM）的方法，用于解决多智能体探索问题。该方法通过使用拓扑地图作为环境表示并结合深度强化学习，能够在有限时间内快速学习（近）最优策略。

    

    本文研究了多智能体合作探索问题，即要求多个智能体在有限的时间内通过感知信号来探索未知环境。探索任务的一种常见方法是将主动映射与规划结合起来。度量地图捕捉了空间表示的细节，但通信量大且在不同场景之间可能有很大的变化，导致泛化能力较差。拓扑地图是一个有希望的选择，因为它们只包含节点和边，具有抽象但关键的信息，并且受到场景结构的影响较小。然而，大多数现有的基于拓扑的探索任务使用的是传统的规划方法，这些方法耗时且由于其手工设计而次优。深度强化学习（DRL）已展现出通过快速端到端推导学习（近）最优策略的巨大潜力。在本文中，我们提出了多智能体神经拓扑映射（MANTM）来解决这个问题。

    This paper investigates the multi-agent cooperative exploration problem, which requires multiple agents to explore an unseen environment via sensory signals in a limited time. A popular approach to exploration tasks is to combine active mapping with planning. Metric maps capture the details of the spatial representation, but are with high communication traffic and may vary significantly between scenarios, resulting in inferior generalization. Topological maps are a promising alternative as they consist only of nodes and edges with abstract but essential information and are less influenced by the scene structures. However, most existing topology-based exploration tasks utilize classical methods for planning, which are time-consuming and sub-optimal due to their handcrafted design. Deep reinforcement learning (DRL) has shown great potential for learning (near) optimal policies through fast end-to-end inference. In this paper, we propose Multi-Agent Neural Topological Mapping (MANTM) to i
    
[^73]: 从反向误差分析角度看多任务和连续学习中的隐式偏差

    Implicit biases in multitask and continual learning from a backward error analysis perspective. (arXiv:2311.00235v1 [stat.ML])

    [http://arxiv.org/abs/2311.00235](http://arxiv.org/abs/2311.00235)

    本论文使用反向误差分析计算了多任务和连续学习设置下神经网络的隐式训练偏差。在训练过程中，通过引入修改损失函数，隐式最小化了原始损失、引入了隐式平坦正则项和冲突项。在多任务中，冲突项衡量了任务梯度之间的对齐性；而在连续学习中，冲突项是深度学习优化中的一个新概念，它通过任务梯度之间的李括号来衡量。

    

    使用反向误差分析，我们计算了用随机梯度下降训练的神经网络在多任务和连续学习设置中的隐式训练偏差。具体而言，我们推导出了在训练过程中隐含地最小化的修改损失函数。它们包括三个项：原始损失函数（考虑收敛性），与学习率成正比的隐式平坦正则项以及最后一个项——冲突项，该项在理论上对收敛性和隐式正则化都可能有害。在多任务中，冲突项是一个众所周知的量，用于衡量任务之间的梯度对齐性，而在连续学习中，冲突项是深度学习优化中的一个新量，尽管在微分几何中是一个基本工具：任务梯度之间的李括号。

    Using backward error analysis, we compute implicit training biases in multitask and continual learning settings for neural networks trained with stochastic gradient descent. In particular, we derive modified losses that are implicitly minimized during training. They have three terms: the original loss, accounting for convergence, an implicit flatness regularization term proportional to the learning rate, and a last term, the conflict term, which can theoretically be detrimental to both convergence and implicit regularization. In multitask, the conflict term is a well-known quantity, measuring the gradient alignment between the tasks, while in continual learning the conflict term is a new quantity in deep learning optimization, although a basic tool in differential geometry: The Lie bracket between the task gradients.
    
[^74]: DistDNAS: 在2小时内高效搜索特征交互

    DistDNAS: Search Efficient Feature Interactions within 2 Hours. (arXiv:2311.00231v1 [cs.IR])

    [http://arxiv.org/abs/2311.00231](http://arxiv.org/abs/2311.00231)

    DistDNAS是一种在推荐系统中高效搜索特征交互的解决方案，通过分布式搜索和选择最佳交互模块，实现了巨大的加速并将搜索时间从2天缩短到2小时。

    

    在推荐系统中，搜索效率和服务效率是构建特征交互和加快模型开发过程的两个主要方面。在大规模基准测试中，由于大量数据上的顺序工作流程，搜索最佳特征交互设计需要付出巨大成本。此外，融合各种来源、顺序和数学运算的交互会引入潜在的冲突和额外的冗余，导致性能和服务成本的次优权衡。本文提出了DistDNAS作为一种简洁的解决方案，可以快速且高效地进行特征交互设计。DistDNAS提出了一个超级网络，将不同顺序和类型的交互模块作为搜索空间进行整合。为了优化搜索效率，DistDNAS在不同的数据日期上分布式搜索并汇总选择最佳的交互模块，实现了超过25倍的加速，将搜索成本从2天减少到2小时。

    Search efficiency and serving efficiency are two major axes in building feature interactions and expediting the model development process in recommender systems. On large-scale benchmarks, searching for the optimal feature interaction design requires extensive cost due to the sequential workflow on the large volume of data. In addition, fusing interactions of various sources, orders, and mathematical operations introduces potential conflicts and additional redundancy toward recommender models, leading to sub-optimal trade-offs in performance and serving cost. In this paper, we present DistDNAS as a neat solution to brew swift and efficient feature interaction design. DistDNAS proposes a supernet to incorporate interaction modules of varying orders and types as a search space. To optimize search efficiency, DistDNAS distributes the search and aggregates the choice of optimal interaction modules on varying data dates, achieving over 25x speed-up and reducing search cost from 2 days to 2 
    
[^75]: StableFDG:基于风格和注意力的联邦领域泛化学习

    StableFDG: Style and Attention Based Learning for Federated Domain Generalization. (arXiv:2311.00227v1 [cs.LG])

    [http://arxiv.org/abs/2311.00227](http://arxiv.org/abs/2311.00227)

    本文提出了StableFDG，一种基于风格和注意力的学习策略，用于实现联邦领域泛化。其中，基于风格的学习将每个客户端的本地数据集中的样式扩展到原始源域之外，并通过风格共享、转移和探索策略改进了领域多样性。基于注意力的特征突出器捕捉了数据样本特征之间的相似性。

    

    传统的联邦学习算法在训练（源域）和测试（目标域）中假设数据分布相同。然而，实际应用中常常发生域偏移，因此需要在联邦学习方法中引入领域泛化能力。然而，由于每个客户端的本地数据集中缺乏样本/域，现有的领域泛化算法在联邦学习环境中面临基本挑战。在本文中，我们提出了基于风格和注意力的学习策略StableFDG，用于实现联邦领域泛化，并引入了两个关键贡献。第一个是基于风格的学习，它使每个客户端能够在本地数据集中超越原始源域，探索新颖的风格，基于提出的风格共享、转移和探索策略改进了领域多样性。我们的第二个贡献是基于注意力的特征突出器，它捕捉数据样本特征之间的相似性。

    Traditional federated learning (FL) algorithms operate under the assumption that the data distributions at training (source domains) and testing (target domain) are the same. The fact that domain shifts often occur in practice necessitates equipping FL methods with a domain generalization (DG) capability. However, existing DG algorithms face fundamental challenges in FL setups due to the lack of samples/domains in each client's local dataset. In this paper, we propose StableFDG, a style and attention based learning strategy for accomplishing federated domain generalization, introducing two key contributions. The first is style-based learning, which enables each client to explore novel styles beyond the original source domains in its local dataset, improving domain diversity based on the proposed style sharing, shifting, and exploration strategies. Our second contribution is an attention-based feature highlighter, which captures the similarities between the features of data samples in t
    
[^76]: Transformers是无线通信中高效的上下文估计器

    Transformers are Efficient In-Context Estimators for Wireless Communication. (arXiv:2311.00226v1 [eess.SP])

    [http://arxiv.org/abs/2311.00226](http://arxiv.org/abs/2311.00226)

    这项研究提出了一种新的方法，利用上下文估计来解决无线通信中的问题。传统方法忽略了信道的层次结构，而本研究利用了Transformers在上下文学习方面的优势，通过少量提示来实现了准确的传输符号估计。

    

    预训练的Transformers可以进行上下文学习，在只有少量提示的情况下，适应新的任务，而不需要任何显式的模型优化。受到这个属性的启发，我们提出了一种新的方法，称为上下文估计，用于估计从接收到的符号中的传输符号的经典通信问题。通信信道本质上是一个将传输符号映射到接收符号的噪声函数，这个函数可以由一个未知参数表示，其统计数据依赖于一个（也是未知的）潜在上下文。传统方法忽略了这种层次结构，只是试图使用已知的传输信号进行最小二乘估计，然后用于估计连续的未知传输符号。我们建立了基本联系，即Transformers在少量提示下展示出出色的上下文序列完成能力，因此它们应该能够隐式确定...

    Pre-trained transformers can perform in-context learning, where they adapt to a new task using only a small number of prompts without any explicit model optimization. Inspired by this attribute, we propose a novel approach, called in-context estimation, for the canonical communication problem of estimating transmitted symbols from received symbols. A communication channel is essentially a noisy function that maps transmitted symbols to received symbols, and this function can be represented by an unknown parameter whose statistics depend on an (also unknown) latent context. Conventional approaches ignore this hierarchical structure and simply attempt to use known transmissions, called pilots, to perform a least-squares estimate of the channel parameter, which is then used to estimate successive, unknown transmitted symbols. We make the basic connection that transformers show excellent contextual sequence completion with a few prompts, and so they should be able to implicitly determine t
    
[^77]: WinNet:具有窗口增强周期提取和交互的时间序列预测

    WinNet:time series forecasting with a window-enhanced period extracting and interacting. (arXiv:2311.00214v1 [cs.LG])

    [http://arxiv.org/abs/2311.00214](http://arxiv.org/abs/2311.00214)

    WinNet是一种用于时间序列预测的CNN-based模型，通过窗口增强的周期提取和交互，在捕捉长期和短期周期性方面具有高准确度和简单结构。在九个基准数据集上的实验结果表明，WinNet可以实现优于CNN、MLP、Transformer方法的最新性能，并具有较低的计算复杂度。

    

    最近，基于Transformer的方法显著提高了时序预测的最新结果，但它们面临高计算成本和无法捕捉时间序列的长期和短期周期的问题。我们提出了一种高度准确且简单结构的基于CNN的模型WinNet，用于长期时间序列预测任务，包括：(i) Inter-Intra Period Encoder (I2PE) 将1D序列转换为二维张量，根据预定义的周期窗口具有长期和短期周期性，(ii) Two-Dimensional Period Decomposition (TDPD) 对模型进行周期-趋势和振荡项建模，(iii) Decomposition Correlation Block (DCB) 利用周期-趋势和振荡项的相关性，通过CNNs支持预测任务。在九个基准数据集上的结果表明，WinNet在CNN、MLP和Transformer方法上可以实现SOTA性能和较低的计算复杂性。WinNet为基于CNN的元方法提供了潜在的可能性。

    Recently, Transformer-based methods have significantly improved state-of-the-art time series forecasting results, but they suffer from high computational costs and the inability to capture the long and short periodicity of time series. We present a highly accurate and simply structured CNN-based model for long-term time series forecasting tasks, called WinNet, including (i) Inter-Intra Period Encoder (I2PE) to transform 1D sequence into 2D tensor with long and short periodicity according to the predefined periodic window, (ii) Two-Dimensional Period Decomposition (TDPD) to model period-trend and oscillation terms, and (iii) Decomposition Correlation Block (DCB) to leverage the correlations of the period-trend and oscillation terms to support the prediction tasks by CNNs. Results on nine benchmark datasets show that the WinNet can achieve SOTA performance and lower computational complexity over CNN-, MLP-, Transformer-based approaches. The WinNet provides potential for the CNN-based met
    
[^78]: 在机器学习中强制、发现和推动对称性的统一框架

    A Unified Framework to Enforce, Discover, and Promote Symmetry in Machine Learning. (arXiv:2311.00212v1 [cs.LG])

    [http://arxiv.org/abs/2311.00212](http://arxiv.org/abs/2311.00212)

    本文提供了一个统一的框架，通过三种方式将对称性融入机器学习模型：1. 强制已知的对称性；2. 发现未知的对称性；3. 在训练过程中促进对称性。

    

    对称性存在于自然界中，并在物理学和机器学习中扮演着越来越核心的角色。基本对称性，如庞加莱不变性，使在地球上实验室中发现的物理定律能够推广到宇宙的最远处。对称性对于在机器学习应用中实现这种推广能力至关重要。例如，在图像分类中的平移不变性允许使用参数更少的模型（如卷积神经网络）在较小的数据集上进行训练，并达到最先进的性能。本文提供了一个统一的理论和方法框架，用于在机器学习模型中以三种方式融入对称性：1. 在训练模型时强制已知的对称性；2. 发现给定模型或数据集的未知对称性；3. 在训练过程中通过学习打破用户指定的候选群体内的对称性来促进对称性。

    Symmetry is present throughout nature and continues to play an increasingly central role in physics and machine learning. Fundamental symmetries, such as Poincar\'{e} invariance, allow physical laws discovered in laboratories on Earth to be extrapolated to the farthest reaches of the universe. Symmetry is essential to achieving this extrapolatory power in machine learning applications. For example, translation invariance in image classification allows models with fewer parameters, such as convolutional neural networks, to be trained on smaller data sets and achieve state-of-the-art performance. In this paper, we provide a unifying theoretical and methodological framework for incorporating symmetry into machine learning models in three ways: 1. enforcing known symmetry when training a model; 2. discovering unknown symmetries of a given model or data set; and 3. promoting symmetry during training by learning a model that breaks symmetries within a user-specified group of candidates when 
    
[^79]: Transformers作为形式语言识别器：关于表达能力的调查

    Transformers as Recognizers of Formal Languages: A Survey on Expressivity. (arXiv:2311.00208v1 [cs.LG])

    [http://arxiv.org/abs/2311.00208](http://arxiv.org/abs/2311.00208)

    本文对transformers在形式语言识别领域的相关研究进行了全面调查，为理解其表达能力提供了一个统一的框架。

    

    随着transformers在自然语言处理中的重要性日益突出，一些研究人员开始从理论上探讨它们能否解决问题，将问题视为形式语言。探索这类问题将有助于比较transformers与其他模型以及不同变种之间的差异，适用于各种任务。近年来，在这个子领域的工作取得了相当大的进展。本文对这方面的工作进行了全面调查，记录了不同结果背后的各种假设，并提供了一个统一的框架，以协调看似相互矛盾的研究结果。

    As transformers have gained prominence in natural language processing, some researchers have investigated theoretically what problems they can and cannot solve, by treating problems as formal languages. Exploring questions such as this will help to compare transformers with other models, and transformer variants with one another, for various tasks. Work in this subarea has made considerable progress in recent years. Here, we undertake a comprehensive survey of this work, documenting the diverse assumptions that underlie different results and providing a unified framework for harmonizing seemingly contradictory findings.
    
[^80]: 多任务强化学习的联邦自然策略梯度方法

    Federated Natural Policy Gradient Methods for Multi-task Reinforcement Learning. (arXiv:2311.00201v1 [cs.LG])

    [http://arxiv.org/abs/2311.00201](http://arxiv.org/abs/2311.00201)

    本研究提出了一种多任务强化学习的联邦自然策略梯度方法，在分布式环境中，通过优化全局策略以最大化所有智能体的总奖励，实现协作决策。这些方法不受信息共享不完备的影响，且具有非渐近全局收敛保证。

    

    联邦强化学习使得多个分布式智能体可以在不共享本地数据轨迹的情况下进行协作决策。本文考虑了多任务设置，其中每个智能体都有自己的私有奖励函数对应不同的任务，同时共享环境的相同转移核。针对无限时间步标记马尔可夫决策过程，目标是学习出一种全局最优策略，在分散的方式下，最大化所有智能体的折扣总奖励之和，其中每个智能体仅与其在给定图拓扑中的邻居进行通信。我们在 softmax 参数化下开展了联邦纯粹和熵正则化的自然策略梯度（NPG）方法，其中将梯度跟踪应用于全局 Q 函数，以减轻信息共享不完备的影响。我们在精确策略评估下建立了非渐近全局收敛保证，这些保证几乎是独立的。

    Federated reinforcement learning (RL) enables collaborative decision making of multiple distributed agents without sharing local data trajectories. In this work, we consider a multi-task setting, in which each agent has its own private reward function corresponding to different tasks, while sharing the same transition kernel of the environment. Focusing on infinite-horizon tabular Markov decision processes, the goal is to learn a globally optimal policy that maximizes the sum of the discounted total rewards of all the agents in a decentralized manner, where each agent only communicates with its neighbors over some prescribed graph topology. We develop federated vanilla and entropy-regularized natural policy gradient (NPG) methods under softmax parameterization, where gradient tracking is applied to the global Q-function to mitigate the impact of imperfect information sharing. We establish non-asymptotic global convergence guarantees under exact policy evaluation, which are nearly indep
    
[^81]: 用于提高密度泛函近似精确性的机器学习技术

    Machine learning for accuracy in density functional approximations. (arXiv:2311.00196v1 [physics.chem-ph])

    [http://arxiv.org/abs/2311.00196](http://arxiv.org/abs/2311.00196)

    本文回顾了近期在改进密度泛函及相关近似方法中应用机器学习的进展，讨论了在不同化学和材料类别之间设计可迁移的机器学习模型时可能面临的挑战和希望。

    

    机器学习技术已经成为计算化学中不可或缺的工具，用于加速原子模拟和材料设计。此外，机器学习方法有可能提高计算效率高的电子结构理论（如密度泛函理论）的预测能力，纠正密度泛函方法中的基本错误。本文综述了最近在应用机器学习改进密度泛函和相关近似方法的进展。通过示例应用有希望的模型于训练集之外的系统，讨论了在不同化学和材料类别之间设计可迁移的机器学习模型时可能面临的挑战和希望。

    Machine learning techniques have found their way into computational chemistry as indispensable tools to accelerate atomistic simulations and materials design. In addition, machine learning approaches hold the potential to boost the predictive power of computationally efficient electronic structure methods, such as density functional theory, to chemical accuracy and to correct for fundamental errors in density functional approaches. Here, recent progress in applying machine learning to improve the accuracy of density functional and related approximations is reviewed. Promises and challenges in devising machine learning models transferable between different chemistries and materials classes are discussed with the help of examples applying promising models to systems far outside their training sets.
    
[^82]: 最佳结合: 随机和对抗性凸函数追踪

    Best of Both Worlds: Stochastic and Adversarial Convex Function Chasing. (arXiv:2311.00181v1 [math.OC])

    [http://arxiv.org/abs/2311.00181](http://arxiv.org/abs/2311.00181)

    该论文研究了随机和对抗性环境下的凸函数追踪问题，并给出了同时在两种情境下达到性能保证的算法。这是首个使用随机框架研究该问题的工作，提出了一种融合两种情境的最佳算法。

    

    凸函数追踪(CFC)是一个在线优化问题，每一轮$t$，玩家根据损失函数$f_t(x_t)$和切换动作的额外成本$c(x_t,x_{t-1})$选择动作$x_t$。我们研究了随机和对抗性环境下的CFC问题，并给出了在两种情境下同时获得性能保证的算法。具体而言，我们考虑了平方$\ell_2$范数的切换成本和一类广泛的二次损失函数，对于这类损失函数，极小化序列要么形成鞅，要么由对手选择。这是首个使用随机框架研究CFC问题的工作。我们给出了最佳随机在线算法的特征，并通过对随机和对抗性情景的比较，证明了在随机情境下，对抗性最优算法表现不佳。受此启发，我们提出了一种融合了两种情境的最佳算法。

    Convex function chasing (CFC) is an online optimization problem in which during each round $t$, a player plays an action $x_t$ in response to a hitting cost $f_t(x_t)$ and an additional cost of $c(x_t,x_{t-1})$ for switching actions. We study the CFC problem in stochastic and adversarial environments, giving algorithms that achieve performance guarantees simultaneously in both settings. Specifically, we consider the squared $\ell_2$-norm switching costs and a broad class of quadratic hitting costs for which the sequence of minimizers either forms a martingale or is chosen adversarially. This is the first work that studies the CFC problem using a stochastic framework. We provide a characterization of the optimal stochastic online algorithm and, drawing a comparison between the stochastic and adversarial scenarios, we demonstrate that the adversarial-optimal algorithm exhibits suboptimal performance in the stochastic context. Motivated by this, we provide a best-of-both-worlds algorithm 
    
[^83]: 强化学习从人类反馈中的目标不匹配问题：对齐上限

    The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback. (arXiv:2311.00168v1 [cs.LG])

    [http://arxiv.org/abs/2311.00168](http://arxiv.org/abs/2311.00168)

    这项研究探讨了强化学习从人类反馈中的目标不匹配问题。研究发现，在强化学习从人类反馈中，奖励模型训练、策略模型训练和策略模型评估之间存在不一致，导致模型行为的意想不到的结果。

    

    强化学习从人类反馈中的目标不匹配问题（RLHF）已经成为使大型语言模型（LLM）更易于提示并在复杂环境中更有能力的强大技术。RLHF核心是提供了一种优化LLM的新工具包，而不仅仅是下一个标记的预测，从而实现了定性训练目标的整合。在学习奖励模型中，用户偏好和下游性能之间的匹配尝试导致了一个优化景观，训练和评估指标看起来可能是相关的。这种表面上的相关关系可能导致意想不到的行为和“过度RLHF”的情况。在RLHF中，由于以下子模块不一致，会出现挑战：奖励模型训练、策略模型训练和策略模型评估。这种不匹配导致模型有时会避免用户请求的虚假安全标志，很难引导模型朝着预期的特征发展，或者总是以特定的风格回答。

    Reinforcement learning from human feedback (RLHF) has emerged as a powerful technique to make large language models (LLMs) easier to prompt and more capable in complex settings. RLHF at its core is providing a new toolkit to optimize LLMs other than next-token prediction, enabling the integration of qualitative training goals. The attempted match between user preferences and downstream performance, which happens in a learned reward model, results in an optimization landscape where training and evaluation metrics can appear correlated. The apparent correlation can lead to unexpected behaviors and stories of "too much RLHF." In RLHF, challenges emerge because the following sub-modules are not consistent with each other: the reward model training, the policy model training, and the policy model evaluation. This mismatch results in models that sometimes avoid user requests for false safety flags, are difficult to steer to an intended characteristic, or always answer in a specific style. As
    
[^84]: 多任务深度卷积网络预测北冰洋海冰浓度和漂移

    Multi-task Deep Convolutional Network to Predict Sea Ice Concentration and Drift in the Arctic Ocean. (arXiv:2311.00167v1 [cs.LG])

    [http://arxiv.org/abs/2311.00167](http://arxiv.org/abs/2311.00167)

    提出了一种名为HIS-Unet的多任务深度卷积网络架构，通过加权注意力模块实现海冰浓度和漂移的预测。与其他方法相比，HIS-Unet在海冰预测中取得了显著的改进。

    

    在北冰洋地区，预测海冰浓度(SIC)和海冰漂移(SID)具有重要意义，因为最近的气候变暖已经改变了这个环境。由于物理海冰模型需要高计算成本和复杂的参数化，深度学习技术可以有效替代物理模型，并提高海冰预测的性能。本研究提出了一种新颖的多任务全卷积网络架构，名为Hierarchical Information-Sharing U-Net (HIS-Unet)，用于预测每日的SIC和SID。我们通过加权注意力模块(WAMs)允许SIC和SID层共享信息，并互相辅助预测。结果表明，相比于其他统计方法、海冰物理模型和没有信息共享单元的神经网络，我们的HIS-Unet在SIC和SID预测方面表现更优。在预测北冰洋海冰浓度和漂移方面，HIS-Unet的改进都是显著的。

    Forecasting sea ice concentration (SIC) and sea ice drift (SID) in the Arctic Ocean is of great significance as the Arctic environment has been changed by the recent warming climate. Given that physical sea ice models require high computational costs with complex parameterization, deep learning techniques can effectively replace the physical model and improve the performance of sea ice prediction. This study proposes a novel multi-task fully conventional network architecture named hierarchical information-sharing U-net (HIS-Unet) to predict daily SIC and SID. Instead of learning SIC and SID separately at each branch, we allow the SIC and SID layers to share their information and assist each other's prediction through the weighting attention modules (WAMs). Consequently, our HIS-Unet outperforms other statistical approaches, sea ice physical models, and neural networks without such information-sharing units. The improvement of HIS-Unet is obvious both for SIC and SID prediction when and
    
[^85]: 道路安全建模的图神经网络：用于事故分析的数据集和评估

    Graph Neural Networks for Road Safety Modeling: Datasets and Evaluations for Accident Analysis. (arXiv:2311.00164v1 [cs.SI])

    [http://arxiv.org/abs/2311.00164](http://arxiv.org/abs/2311.00164)

    该论文构建了一个大规模的道路交通事故记录数据集，并使用该数据集评估了现有的深度学习方法在预测事故发生方面的准确性。研究发现，图神经网络GraphSAGE能够准确预测道路上的事故数量，并判断事故是否会发生。

    

    我们考虑基于道路网络连接和交通流量的道路网络上的交通事故分析问题。以往的工作使用历史记录设计了各种深度学习方法来预测交通事故的发生。然而，现有方法的准确性缺乏共识，并且一个基本问题是缺乏公共事故数据集进行全面评估。本文构建了一个大规模的、统一的道路交通事故记录数据集，包括来自美国各州官方报告的900万条记录，以及道路网络和交通流量报告。利用这个新数据集，我们评估了现有的深度学习方法来预测道路网络上的事故发生。我们的主要发现是，像GraphSAGE这样的图神经网络可以准确预测道路上的事故数量，平均绝对误差不超过实际数目的22%，并能够判断事故是否会发生。

    We consider the problem of traffic accident analysis on a road network based on road network connections and traffic volume. Previous works have designed various deep-learning methods using historical records to predict traffic accident occurrences. However, there is a lack of consensus on how accurate existing methods are, and a fundamental issue is the lack of public accident datasets for comprehensive evaluations. This paper constructs a large-scale, unified dataset of traffic accident records from official reports of various states in the US, totaling 9 million records, accompanied by road networks and traffic volume reports. Using this new dataset, we evaluate existing deep-learning methods for predicting the occurrence of accidents on road networks. Our main finding is that graph neural networks such as GraphSAGE can accurately predict the number of accidents on roads with less than 22% mean absolute error (relative to the actual count) and whether an accident will occur or not w
    
[^86]: 一个更快的扩散指数积分器采样器的得分归一化

    Score Normalization for a Faster Diffusion Exponential Integrator Sampler. (arXiv:2311.00157v1 [cs.LG])

    [http://arxiv.org/abs/2311.00157](http://arxiv.org/abs/2311.00157)

    该论文提出了一种得分归一化方法，用于改进扩散指数积分器采样器的生成质量和减小积分误差。

    

    最近，张等人提出了一种用于从扩散模型中快速生成样本的扩散指数积分器采样器（DEIS）。它利用概率流常微分方程（ODE）的半线性特性来大大减小积分误差，并在低函数评估次数（NFEs）时提高生成质量。这种方法的关键是得分函数重参数化，它通过减少在每个积分步骤中使用固定得分函数估计而引起的积分误差。原始作者使用了用于噪声预测训练的模型默认的参数化方法，即将得分乘以条件正向噪声分布的标准差。然而，我们发现，尽管这种得分参数化的绝对平均值在大部分反向采样过程中接近常数，但在采样结束时它会迅速变化。为了简单修复这个问题，我们建议对得分进行重新参数化（在...

    Recently, zhang et al have proposed the Diffusion Exponential Integrator Sampler (DEIS) for fast generation of samples from Diffusion Models. It leverages the semi-linear nature of the probability flow ordinary differential equation (ODE) in order to greatly reduce integration error and improve generation quality at low numbers of function evaluations (NFEs). Key to this approach is the score function reparameterisation, which reduces the integration error incurred from using a fixed score function estimate over each integration step. The original authors use the default parameterisation used by models trained for noise prediction -- multiply the score by the standard deviation of the conditional forward noising distribution. We find that although the mean absolute value of this score parameterisation is close to constant for a large portion of the reverse sampling process, it changes rapidly at the end of sampling. As a simple fix, we propose to instead reparameterise the score (at in
    
[^87]: Medi-CAT：对医学图像分类的对比性对抗训练

    Medi-CAT: Contrastive Adversarial Training for Medical Image Classification. (arXiv:2311.00154v1 [eess.IV])

    [http://arxiv.org/abs/2311.00154](http://arxiv.org/abs/2311.00154)

    本文提出了一种名为Medi-CAT的训练策略，以克服医学图像数据集中的欠拟合和过拟合现象。该方法利用大型预训练视觉转换器来解决欠拟合问题，并采用对抗性和对比性学习技术来防止过拟合。实验证明，该方法在多个医学图像分类数据集上具有较高的准确率和性能提升。

    

    目前可用的大型医学图像数据集不多。对于这些数据集，太小的深度学习模型无法学习到有用的特征，因此由于欠拟合而效果不佳，而太大的模型则倾向于过拟合有限的数据。因此，解决这两个问题之间需要做出折中。本文提出了一种训练策略——Medi-CAT，以克服医学图像数据集中的欠拟合和过拟合现象。具体而言，所提出的训练方法采用了大型预训练视觉转换器来解决欠拟合问题，并使用对抗性和对比性学习技术来防止过拟合。所提出的方法在MedMNIST集合中的四个医学图像分类数据集上进行了训练和评估。我们的实验结果表明，与众所周知的方法相比，所提出的方法在三个基准数据集上的准确率提高了2％，而在基线方法上的性能提高了4.1％。

    There are not many large medical image datasets available. For these datasets, too small deep learning models can't learn useful features, so they don't work well due to underfitting, and too big models tend to overfit the limited data. As a result, there is a compromise between the two issues. This paper proposes a training strategy Medi-CAT to overcome the underfitting and overfitting phenomena in medical imaging datasets. Specifically, the proposed training methodology employs large pre-trained vision transformers to overcome underfitting and adversarial and contrastive learning techniques to prevent overfitting. The proposed method is trained and evaluated on four medical image classification datasets from the MedMNIST collection. Our experimental results indicate that the proposed approach improves the accuracy up to 2% on three benchmark datasets compared to well-known approaches, whereas it increases the performance up to 4.1% over the baseline methods.
    
[^88]: 伊朗2021年总统选举期间，使用轴嵌入的两阶段分类器进行政治用户推文中的负面情绪检测：案例研究

    Two-Stage Classifier for Campaign Negativity Detection using Axis Embeddings: A Case Study on Tweets of Political Users during 2021 Presidential Election in Iran. (arXiv:2311.00143v1 [cs.LG])

    [http://arxiv.org/abs/2311.00143](http://arxiv.org/abs/2311.00143)

    本文提出了一个混合模型，用于检测竞选活动的负面情绪，包括一个两阶段分类器，结合了两个机器学习模型的优势。通过对伊朗2021年总统选举期间50名政治用户的5,100条波斯语推文进行标注，建立了所需的数据集。

    

    在全球各地的选举中，候选人可能会因失败前景和时间压力而将他们的竞选活动转向负面情绪。在数字时代，Twitter等社交媒体平台是政治话语的丰富来源。因此，尽管Twitter上发布了大量数据，但自动化的竞选负面情绪检测系统在理解候选人和政党在竞选活动中的策略方面起着至关重要的作用。在本文中，我们提出了一个混合模型，用于检测竞选活动的负面情绪，包括一个两阶段分类器，结合了两个机器学习模型的优势。在此，我们收集了来自50名政治用户（包括候选人和政府官员）的波斯语推文。然后，我们标注了其中5,100条推文，这些推文是在伊朗2021年总统选举前的一年内发布的。在提出的模型中，首先通过推文嵌入与轴的余弦相似性来建立两个分类器所需的数据集。

    In elections around the world, the candidates may turn their campaigns toward negativity due to the prospect of failure and time pressure. In the digital age, social media platforms such as Twitter are rich sources of political discourse. Therefore, despite the large amount of data that is published on Twitter, the automatic system for campaign negativity detection can play an essential role in understanding the strategy of candidates and parties in their campaigns. In this paper, we propose a hybrid model for detecting campaign negativity consisting of a two-stage classifier that combines the strengths of two machine learning models. Here, we have collected Persian tweets from 50 political users, including candidates and government officials. Then we annotated 5,100 of them that were published during the year before the 2021 presidential election in Iran. In the proposed model, first, the required datasets of two classifiers based on the cosine similarity of tweet embeddings with axis
    
[^89]: Neuroformer：用于脑数据的多模态和多任务生成预训练模型

    Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data. (arXiv:2311.00136v1 [q-bio.NC])

    [http://arxiv.org/abs/2311.00136](http://arxiv.org/abs/2311.00136)

    Neuroformer是一个多模态和多任务的生成预训练模型，旨在处理系统神经科学中大规模的多模态数据。模型经过训练后能准确预测神经回路活动并推断神经回路连接性，同时能用于预测行为。

    

    最先进的系统神经科学实验产生了大规模的多模态数据，这些数据集需要新的分析工具。受到视觉和语言领域大规模预训练模型成功的启发，我们将大规模的细胞分辨率神经元尖峰数据的分析重新构建为一个自回归的时空生成问题。Neuroformer是一个多模态、多任务的生成预训练transformer（GPT）模型，专为处理系统神经科学数据的复杂性而设计。它与特征大小呈线性扩展，并且可以处理任意数量的模态，适应下游任务，比如预测行为。我们首先在模拟数据集上训练了Neuroformer，并发现它既能准确预测模拟神经回路活动，也能内在地推断出底层神经回路连接性，包括方向。当预训练用于解码神经响应时，该模型能预测小鼠行为。

    State-of-the-art systems neuroscience experiments yield large-scale multimodal data, and these data sets require new tools for analysis. Inspired by the success of large pretrained models in vision and language domains, we reframe the analysis of large-scale, cellular-resolution neuronal spiking data into an autoregressive spatiotemporal generation problem. Neuroformer is a multimodal, multitask generative pretrained transformer (GPT) model that is specifically designed to handle the intricacies of data in systems neuroscience. It scales linearly with feature size, can process an arbitrary number of modalities, and is adaptable to downstream tasks, such as predicting behavior. We first trained Neuroformer on simulated datasets, and found that it both accurately predicted simulated neuronal circuit activity, and also intrinsically inferred the underlying neural circuit connectivity, including direction. When pretrained to decode neural responses, the model predicted the behavior of a mo
    
[^90]: 提取脑动力学的多尺度因果骨架

    Extracting the Multiscale Causal Backbone of Brain Dynamics. (arXiv:2311.00118v1 [cs.LG])

    [http://arxiv.org/abs/2311.00118](http://arxiv.org/abs/2311.00118)

    该研究提出了一种用于提取脑动力学多尺度因果骨架的方法，并通过对合成数据和静息态fMRI数据的实验证明其优越性。研究结果显示，因果动力在不同频率下受不同脑区驱动，这为理解脑功能提供了新的视角。

    

    大部分关于脑连接性的研究集中在脑区之间的统计关联上，这与统治脑动力学的因果机制不直接相关。在这里，我们提出了多尺度因果骨架（MCB），它是在多个时间尺度上共享的一组个体的脑动力学特征，并设计了一种有原则的方法来提取它。我们的方法利用了多尺度因果结构学习的最新进展，并优化了模型拟合与复杂性之间的权衡。对合成数据的实证评估显示，我们的方法优于基于规范功能连接网络的基线。当应用于静息态fMRI数据时，我们发现左右脑半球都有稀疏的MCB。由于其多尺度的特性，我们的方法表明在低频带上，因果动力来自与高级认知功能相关的脑区；而在更高的频率上，由nod产生。

    The bulk of the research effort on brain connectivity revolves around statistical associations among brain regions, which do not directly relate to the causal mechanisms governing brain dynamics. Here we propose the multiscale causal backbone (MCB) of brain dynamics shared by a set of individuals across multiple temporal scales, and devise a principled methodology to extract it.  Our approach leverages recent advances in multiscale causal structure learning and optimizes the trade-off between the model fitting and its complexity. Empirical assessment on synthetic data shows the superiority of our methodology over a baseline based on canonical functional connectivity networks. When applied to resting-state fMRI data, we find sparse MCBs for both the left and right brain hemispheres. Thanks to its multiscale nature, our approach shows that at low-frequency bands, causal dynamics are driven by brain regions associated with high-level cognitive functions; at higher frequencies instead, nod
    
[^91]: EXTRACT: 解释性透明控制嵌入中的偏见

    EXTRACT: Explainable Transparent Control of Bias in Embeddings. (arXiv:2311.00115v1 [cs.LG])

    [http://arxiv.org/abs/2311.00115](http://arxiv.org/abs/2311.00115)

    本论文提出了一套解释性和透明的方法，称为EXTRACT，来控制知识图谱嵌入中的偏见。该方法使用规范相关分析（CCA）来研究信息泄漏的存在程度和来源，并通过解线性方程组将嵌入分解为私有属性的总和。实验结果表明，可以从用户的行为中推断出各种个人属性。

    

    知识图谱是在各种人工智能应用中广泛使用的一种表示实体之间关系的方法，而图嵌入已迅速成为一种标准技术，以便以一种易于推理和决策的方式表示知识图谱。由于这种表示是从行为数据中获得的，无法被人类阅读，因此存在一个担忧，即它可能包含意外的信息，导致偏见。我们提出了EXTRACT：一套解释性和透明的方法来控制知识图谱嵌入中的偏见，以评估和减少受保护信息的隐含存在。我们的方法使用规范相关分析（CCA）来研究训练过程中信息泄漏的存在程度和来源，然后通过解线性方程组将嵌入分解为私有属性的总和。我们在MovieLens1M数据集上进行的实验证明，可以从用户的行为中推断出各种个人属性。

    Knowledge Graphs are a widely used method to represent relations between entities in various AI applications, and Graph Embedding has rapidly become a standard technique to represent Knowledge Graphs in such a way as to facilitate inferences and decisions. As this representation is obtained from behavioural data, and is not in a form readable by humans, there is a concern that it might incorporate unintended information that could lead to biases. We propose EXTRACT: a suite of Explainable and Transparent methods to ConTrol bias in knowledge graph embeddings, so as to assess and decrease the implicit presence of protected information. Our method uses Canonical Correlation Analysis (CCA) to investigate the presence, extent and origins of information leaks during training, then decomposes embeddings into a sum of their private attributes by solving a linear system. Our experiments, performed on the MovieLens1M dataset, show that a range of personal attributes can be inferred from a user's
    
[^92]: FairWASP：快速和最优的公平Wasserstein预处理

    FairWASP: Fast and Optimal Fair Wasserstein Pre-processing. (arXiv:2311.00109v1 [cs.LG])

    [http://arxiv.org/abs/2311.00109](http://arxiv.org/abs/2311.00109)

    FairWASP是一种快速和最优的公平Wasserstein预处理方法，通过重新加权数据集来减少分类数据集中的不平等性，同时满足人口平等性准则。这种方法可以用于构建可以输入任何分类方法的数据集。

    

    近年来，机器学习方法的快速发展旨在减少不同子群体之间模型输出的不平等性。在许多情况下，训练数据可能会被不同用户在多个下游应用中使用，这意味着对训练数据本身进行干预可能是最有效的。在这项工作中，我们提出了一种新的预处理方法FairWASP，旨在减少分类数据集中的不平等性，而不会修改原始数据。FairWASP返回样本级权重，使重新加权的数据集最小化与原始数据集的Wasserstein距离，同时满足（经验版本的）人口平等性，这是一种常用的公平性准则。我们从理论上证明了整数权重的最优性，这意味着我们的方法可以等同地理解为复制或删除样本。因此，FairWASP可用于构建可以输入任何分类方法的数据集，而不仅仅是接受样本权重的方法。

    Recent years have seen a surge of machine learning approaches aimed at reducing disparities in model outputs across different subgroups. In many settings, training data may be used in multiple downstream applications by different users, which means it may be most effective to intervene on the training data itself. In this work, we present FairWASP, a novel pre-processing approach designed to reduce disparities in classification datasets without modifying the original data. FairWASP returns sample-level weights such that the reweighted dataset minimizes the Wasserstein distance to the original dataset while satisfying (an empirical version of) demographic parity, a popular fairness criterion. We show theoretically that integer weights are optimal, which means our method can be equivalently understood as duplicating or eliminating samples. FairWASP can therefore be used to construct datasets which can be fed into any classification method, not just methods which accept sample weights. Ou
    
[^93]: 3D地震反演的深度压缩学习方法

    Deep Compressed Learning for 3D Seismic Inversion. (arXiv:2311.00107v1 [physics.geo-ph])

    [http://arxiv.org/abs/2311.00107](http://arxiv.org/abs/2311.00107)

    本文提出了一种使用深度压缩学习方法进行3D地震反演的解决方案，通过联合优化降维操作符和DCNN实现的编码器-解码器，实现了在使用极少数地震源的情况下，减少了数量级的地震记录使用量，同时保持了与使用整个数据集相当的3D重建质量。

    

    本文考虑使用极少数地震源从预叠前数据进行3D地震反演的问题。所提出的解决方案基于压缩感知和机器学习框架的组合，称为压缩学习。该解决方案同时优化一个降维操作符和一个由深度卷积神经网络(DCNN)实现的3D反演编码器-解码器。通过学习一个稀疏的二值感知层来实现降维，该层选择可用地震源的一个小子集，然后选择的数据被输入到DCNN中完成回归任务。端到端的学习过程在训练期间减少了数量级的地震记录使用量，同时保持了与使用整个数据集相当的3D重建质量。

    We consider the problem of 3D seismic inversion from pre-stack data using a very small number of seismic sources. The proposed solution is based on a combination of compressed-sensing and machine learning frameworks, known as compressed-learning. The solution jointly optimizes a dimensionality reduction operator and a 3D inversion encoder-decoder implemented by a deep convolutional neural network (DCNN). Dimensionality reduction is achieved by learning a sparse binary sensing layer that selects a small subset of the available sources, then the selected data is fed to a DCNN to complete the regression task. The end-to-end learning process provides a reduction by an order-of-magnitude in the number of seismic records used during training, while preserving the 3D reconstruction quality comparable to that obtained by using the entire dataset.
    
[^94]: 为了在标签噪音下进行稳健学习，基于Bandit驱动的批次选择方法

    Bandit-Driven Batch Selection for Robust Learning under Label Noise. (arXiv:2311.00096v1 [cs.LG])

    [http://arxiv.org/abs/2311.00096](http://arxiv.org/abs/2311.00096)

    本研究提出了一种基于Bandit算法的批次选择方法，以改善在标签噪音下的学习过程。实验证明该方法在不同水平的标签污染下表现优于现有方法，且无需使用辅助神经网络模型。

    

    我们引入了一种新颖的方法，利用组合赌博算法来选择随机梯度下降（SGD）训练中的批次。我们的方法侧重于在现实世界数据集中普遍存在的标签噪音的情况下优化学习过程。在CIFAR-10数据集上的实验评估表明，我们的方法在不同水平的标签污染下始终优于现有方法。重要的是，我们在没有带来常见的辅助神经网络模型的计算开销的情况下实现了这种优越性能。该工作提供了计算效率和模型效能之间的平衡折衷，为复杂的机器学习应用提供了可扩展的解决方案。

    We introduce a novel approach for batch selection in Stochastic Gradient Descent (SGD) training, leveraging combinatorial bandit algorithms. Our methodology focuses on optimizing the learning process in the presence of label noise, a prevalent issue in real-world datasets. Experimental evaluations on the CIFAR-10 dataset reveal that our approach consistently outperforms existing methods across various levels of label corruption. Importantly, we achieve this superior performance without incurring the computational overhead commonly associated with auxiliary neural network models. This work presents a balanced trade-off between computational efficiency and model efficacy, offering a scalable solution for complex machine learning applications.
    
[^95]: 表达建模对于离线强化学习不足：可处理的推理角度

    Expressive Modeling Is Insufficient for Offline RL: A Tractable Inference Perspective. (arXiv:2311.00094v1 [cs.LG])

    [http://arxiv.org/abs/2311.00094](http://arxiv.org/abs/2311.00094)

    本文指出，在离线强化学习任务中，除了表达性强的序列模型，可处理性也起着重要的作用。由于离线数据收集策略和环境动态的随机性，需要精确且高效地回答各种概率查询，以找到有奖励的动作。基于此，本文提出了Trifle（离线强化学习的可处理推理）方法，利用现代可处理概率模型来解决这个问题。

    

    离线强化学习任务中，一种流行的范例是先将离线轨迹拟合到一个序列模型中，然后通过该模型提示高期望回报的动作。虽然普遍认为表达性更强的序列模型可以带来更好的性能，但本文强调了可处理性，即精确而高效地回答各种概率查询的能力，同样起着重要的作用。具体而言，由于离线数据收集策略和环境动态带来的基本随机性，需要进行高度非平凡的条件/约束生成，以引出有奖励的动作。虽然仍然可以近似处理这些查询，但我们观察到这种粗糙的估计显著削弱了表达性强的序列模型带来的好处。为了解决这个问题，本文提出了Trifle（离线强化学习的可处理推理），它利用了现代可处理概率模型（TPM）来弥合这个差距。

    A popular paradigm for offline Reinforcement Learning (RL) tasks is to first fit the offline trajectories to a sequence model, and then prompt the model for actions that lead to high expected return. While a common consensus is that more expressive sequence models imply better performance, this paper highlights that tractability, the ability to exactly and efficiently answer various probabilistic queries, plays an equally important role. Specifically, due to the fundamental stochasticity from the offline data-collection policies and the environment dynamics, highly non-trivial conditional/constrained generation is required to elicit rewarding actions. While it is still possible to approximate such queries, we observe that such crude estimates significantly undermine the benefits brought by expressive sequence models. To overcome this problem, this paper proposes Trifle (Tractable Inference for Offline RL), which leverages modern Tractable Probabilistic Models (TPMs) to bridge the gap b
    
[^96]: 使用机器学习在味道物理学中寻求真理和美

    Seeking Truth and Beauty in Flavor Physics with Machine Learning. (arXiv:2311.00087v1 [hep-ph])

    [http://arxiv.org/abs/2311.00087](http://arxiv.org/abs/2311.00087)

    使用机器学习技术在味道物理学中找到真实而美丽的模型。

    

    构建新的理论物理模型的发现过程涉及到既符合现有实验数据又满足抽象理论家的美观、自然等标准的双重方面。我们设计了使用机器学习技术执行这些任务的损失函数。我们以Yukawa夸克部门作为一个玩具示例来演示这些损失函数的优化将产生真实而美丽的模型。

    The discovery process of building new theoretical physics models involves the dual aspect of both fitting to the existing experimental data and satisfying abstract theorists' criteria like beauty, naturalness, etc. We design loss functions for performing both of those tasks with machine learning techniques. We use the Yukawa quark sector as a toy example to demonstrate that the optimization of these loss functions results in true and beautiful models.
    
[^97]: 免费的Spuriosity排名: 基于目标检测的最后一层训练的简单框架

    Spuriosity Rankings for Free: A Simple Framework for Last Layer Retraining Based on Object Detection. (arXiv:2311.00079v1 [cs.CV])

    [http://arxiv.org/abs/2311.00079](http://arxiv.org/abs/2311.00079)

    该论文提出了一种简单的框架，利用目标检测技术和排名方法来实现最后一层的重新训练，以解决深度神经网络对虚假特征的依赖问题。

    

    深度神经网络在各个领域表现出色，但它们对虚假特征的依赖引发了人们对其可靠性的担忧。解决这个问题的一种有希望的方法是最后一层的重新训练，即在没有虚假线索的小数据子集上重新训练线性分类器头部。然而，选择此子集需要人工监督，降低了其可扩展性。此外，选定子集中可能仍存在虚假线索。为了解决这个问题，我们提出了一种新的排名框架，利用开放词汇的目标检测技术来识别没有虚假线索的图像。具体而言，我们使用目标检测器作为对图像中目标物体存在的量化指标。然后，根据这个得分对图像进行排序，并在得分最高的数据子集上重新训练模型的最后一层。我们在ImageNet-1k数据集上的实验证明了该方法的有效性。

    Deep neural networks have exhibited remarkable performance in various domains. However, the reliance of these models on spurious features has raised concerns about their reliability. A promising solution to this problem is last-layer retraining, which involves retraining the linear classifier head on a small subset of data without spurious cues. Nevertheless, selecting this subset requires human supervision, which reduces its scalability. Moreover, spurious cues may still exist in the selected subset. As a solution to this problem, we propose a novel ranking framework that leverages an open vocabulary object detection technique to identify images without spurious cues. More specifically, we use the object detector as a measure to score the presence of the target object in the images. Next, the images are sorted based on this score, and the last-layer of the model is retrained on a subset of the data with the highest scores. Our experiments on the ImageNet-1k dataset demonstrate the eff
    
[^98]: 使用过滤强化学习的无人机安全多智能体运动规划在不确定性下研究

    Safe multi-agent motion planning under uncertainty for drones using filtered reinforcement learning. (arXiv:2311.00063v1 [cs.RO])

    [http://arxiv.org/abs/2311.00063](http://arxiv.org/abs/2311.00063)

    这篇论文提出了一种基于过滤强化学习的无人机安全多智能体运动规划方法，通过结合强化学习和约束控制技术，在不确定性环境中实现了安全性、实时性和高效性。

    

    我们考虑在不确定、混乱的工作空间中，针对无人机的安全多智能体运动规划问题。为了解决这个问题，我们提出了一个可行的运动规划器，它结合了强化学习和基于约束控制的轨迹规划的优点。首先，我们使用单智能体的强化学习从数据中学习到达目标的运动方案，但这些方案可能不是无碰撞的。接下来，我们使用凸优化、概率约束和集合方法来进行约束控制，以确保尽管工作空间、智能体运动和感知不确定，仍能保持安全。所提出的方法可以处理智能体的状态和控制约束，并以很高的概率实现智能体之间以及与工作空间中的静态障碍物的碰撞避免。所提出的方法提供了一个安全的、实时可行的、比仅基于学习的方法更简单训练的多智能体运动规划器。数值模拟和实验显示了方法的有效性。

    We consider the problem of safe multi-agent motion planning for drones in uncertain, cluttered workspaces. For this problem, we present a tractable motion planner that builds upon the strengths of reinforcement learning and constrained-control-based trajectory planning. First, we use single-agent reinforcement learning to learn motion plans from data that reach the target but may not be collision-free. Next, we use a convex optimization, chance constraints, and set-based methods for constrained control to ensure safety, despite the uncertainty in the workspace, agent motion, and sensing. The proposed approach can handle state and control constraints on the agents, and enforce collision avoidance among themselves and with static obstacles in the workspace with high probability. The proposed approach yields a safe, real-time implementable, multi-agent motion planner that is simpler to train than methods based solely on learning. Numerical simulations and experiments show the efficacy of 
    
[^99]: 生成型AI的悖论：“它可以创建，但可能不理解”

    The Generative AI Paradox: "What It Can Create, It May Not Understand". (arXiv:2311.00059v1 [cs.AI])

    [http://arxiv.org/abs/2311.00059](http://arxiv.org/abs/2311.00059)

    生成型AI的悖论研究了生成型模型与人类智能之间的差异，模型在产生专家级输出的能力上可能超过其理解能力。

    

    最近的生成型AI浪潮引起了前所未有的全球关注，既有兴奋也有对人工智能潜在超人水平的担忧：现在的模型只需要几秒钟就能产生超过甚至挑战专家级人类能力的输出。与此同时，模型仍然显示出即使非专家也不会预期出现的基本错误。这给我们带来了一个明显的悖论：我们如何解决看似超人能力和少数人类才会犯错误的持续存在之间的矛盾？在这项研究中，我们提出并测试了生成型AI悖论假设：生成型模型由于直接训练以产生类似专家的输出，而获得的生成能力是不受制于其理解能力的。

    The recent wave of generative AI has sparked unprecedented global attention, with both excitement and concern over potentially superhuman levels of artificial intelligence: models now take only seconds to produce outputs that would challenge or exceed the capabilities even of expert humans. At the same time, models still show basic errors in understanding that would not be expected even in non-expert humans. This presents us with an apparent paradox: how do we reconcile seemingly superhuman capabilities with the persistence of errors that few humans would make? In this work, we posit that this tension reflects a divergence in the configuration of intelligence in today's generative models relative to intelligence in humans. Specifically, we propose and test the Generative AI Paradox hypothesis: generative models, having been trained directly to reproduce expert-like outputs, acquire generative capabilities that are not contingent upon -- and can therefore exceed -- their ability to unde
    
[^100]: 多样性和扩散：关于具有稳定扩散的合成图像分布的观察

    Diversity and Diffusion: Observations on Synthetic Image Distributions with Stable Diffusion. (arXiv:2311.00056v1 [cs.CV])

    [http://arxiv.org/abs/2311.00056](http://arxiv.org/abs/2311.00056)

    这项研究观察了文本到图像系统的进展，并发现只使用合成图像训练的分类器在推理时表现不佳，揭示了底层图像生成过程的局限性。

    

    最近在文本到图像（TTI）系统方面取得的进展，如StableDiffusion、Imagen和DALL-E 2，使得通过简单的文本提示创建逼真的图像成为可能。诱人的是使用这些系统来消除获取训练新的机器学习分类器所需的自然图像的手动任务。然而，尽管用于训练的图像看起来逼真，但迄今为止进行的所有实验都显示只使用合成图像训练的分类器在推理时表现不佳。详细研究此明显的不一致将洞察底层图像生成过程的局限性。从图像创建多样性和所创建内容的准确性的角度来看，我们剖析了合成图像和自然图像中语义不匹配的差异。这将阐明图像语言模型CLIP和图像生成模型扩散的作用。我们发现了四个限制TTI系统在此任务中有用性的问题：不明确的语义、类别平衡问题、训练数据的依赖性问题和图像生成的不确定性问题。

    Recent progress in text-to-image (TTI) systems, such as StableDiffusion, Imagen, and DALL-E 2, have made it possible to create realistic images with simple text prompts. It is tempting to use these systems to eliminate the manual task of obtaining natural images for training a new machine learning classifier. However, in all of the experiments performed to date, classifiers trained solely with synthetic images perform poorly at inference, despite the images used for training appearing realistic. Examining this apparent incongruity in detail gives insight into the limitations of the underlying image generation processes. Through the lens of diversity in image creation vs.accuracy of what is created, we dissect the differences in semantic mismatches in what is modeled in synthetic vs. natural images. This will elucidate the roles of the image-languag emodel, CLIP, and the image generation model, diffusion. We find four issues that limit the usefulness of TTI systems for this task: ambigu
    
[^101]: 通过元表示对异构表格数据进行无训练泛化

    Training-Free Generalization on Heterogeneous Tabular Data via Meta-Representation. (arXiv:2311.00055v1 [cs.LG])

    [http://arxiv.org/abs/2311.00055](http://arxiv.org/abs/2311.00055)

    提出了通过元表示进行表格数据预训练的方法，使得模型可以在异构数据集上进行无训练泛化的应用。

    

    表格数据在各种机器学习领域中普遍存在。然而，不同表格数据集中属性和类别空间的固有异质性阻碍了知识的有效共享，限制了表格模型从其他数据集中受益。在本文中，我们提出了通过元表示进行表格数据预训练（TabPTM），它允许一个表格模型在一组异构数据集上进行预训练。然后，这个预训练模型可以直接应用于具有不同属性和类别的未见过的数据集，无需额外训练。具体而言，TabPTM通过实例到固定数量的原型的距离来表示一个实例，从而标准化异构表格数据集。然后，一个深度神经网络被训练来将这些元表示与数据集特定的分类置信度关联起来，使TabPTM具有无需训练的泛化能力。实验证实TabPTM在新数据集上具有良好的性能。

    Tabular data is prevalent across various machine learning domains. Yet, the inherent heterogeneities in attribute and class spaces across different tabular datasets hinder the effective sharing of knowledge, limiting a tabular model to benefit from other datasets. In this paper, we propose Tabular data Pre-Training via Meta-representation (TabPTM), which allows one tabular model pre-training on a set of heterogeneous datasets. Then, this pre-trained model can be directly applied to unseen datasets that have diverse attributes and classes without additional training. Specifically, TabPTM represents an instance through its distance to a fixed number of prototypes, thereby standardizing heterogeneous tabular datasets. A deep neural network is then trained to associate these meta-representations with dataset-specific classification confidences, endowing TabPTM with the ability of training-free generalization. Experiments validate that TabPTM achieves promising performance in new datasets, 
    
[^102]: 关于 Kolmogorov 神经网络的研究

    On the Kolmogorov neural networks. (arXiv:2311.00049v1 [cs.NE])

    [http://arxiv.org/abs/2311.00049](http://arxiv.org/abs/2311.00049)

    Kolmogorov神经网络模型可以精确地表示连续函数、有界不连续函数和所有无界多元函数。

    

    在本文中，我们证明了 Kolmogorov 两个隐藏层神经网络模型可以通过使用一个连续、不连续有界或者无界激活函数在第二个隐藏层来精确地表示连续函数、有界不连续函数和所有无界多元函数。

    In this paper, we show that the Kolmogorov two hidden layer neural network model with a continuous, discontinuous bounded or unbounded activation function in the second hidden layer can precisely represent continuous, discontinuous bounded and all unbounded multivariate functions, respectively.
    
[^103]: SC-MIL: 用于全切片图像分类的稀疏编码多实例学习

    SC-MIL: Sparsely Coded Multiple Instance Learning for Whole Slide Image Classification. (arXiv:2311.00048v1 [cs.CV])

    [http://arxiv.org/abs/2311.00048](http://arxiv.org/abs/2311.00048)

    本文提出了SC-MIL模型，通过利用稀疏字典学习来同时改进特征嵌入和实例相关性建模，从而提高全切片图像分类的性能。

    

    多实例学习（MIL）在弱监督的全切片图像（WSI）分类中被广泛使用。典型的MIL方法包括特征嵌入部分，通过预训练的特征提取器将实例嵌入到特征中，以及MIL聚合器，将实例嵌入组合成预测结果。目前的重点是通过自监督预训练来改进这些部分，并单独建模实例之间的相关性。在本文中，我们提出了一种稀疏编码的MIL（SC-MIL），同时通过利用稀疏字典学习来解决这两个方面。稀疏字典学习通过将实例表示为过完备字典中原子的稀疏线性组合来捕捉实例之间的相似性。此外，引入稀疏性可以通过抑制不相关的实例而保留最相关的实例，从而增强实例的特征嵌入。为了改善传统的特征嵌入和实例之间的相关性建模方法，we proposed a sparsely coded MIL.

    Multiple Instance Learning (MIL) has been widely used in weakly supervised whole slide image (WSI) classification. Typical MIL methods include a feature embedding part that embeds the instances into features via a pre-trained feature extractor and the MIL aggregator that combines instance embeddings into predictions. The current focus has been directed toward improving these parts by refining the feature embeddings through self-supervised pre-training and modeling the correlations between instances separately. In this paper, we proposed a sparsely coded MIL (SC-MIL) that addresses those two aspects at the same time by leveraging sparse dictionary learning. The sparse dictionary learning captures the similarities of instances by expressing them as a sparse linear combination of atoms in an over-complete dictionary. In addition, imposing sparsity help enhance the instance feature embeddings by suppressing irrelevant instances while retaining the most relevant ones. To make the convention
    
[^104]: 用语言来地基视觉幻觉：视觉-语言模型是否像人类一样感知幻觉？

    Grounding Visual Illusions in Language: Do Vision-Language Models Perceive Illusions Like Humans?. (arXiv:2311.00047v1 [cs.AI])

    [http://arxiv.org/abs/2311.00047](http://arxiv.org/abs/2311.00047)

    通过构建一个包含五种视觉幻觉的数据集，研究发现，尽管整体对齐性较低，但更大规模的视觉-语言模型更接近人类的感知并更容易受到视觉幻觉的影响。

    

    视觉-语言模型（VLMs）是在人类理解世界的模拟下，通过大量的数据训练得到的。然而，人类对现实的感知并不总是对物理世界的忠实呈现，被称为视觉幻觉。这引发了一个关键问题：VLMs是否和人类一样有幻觉,或者它们是否忠实地学习了对现实的表达？为了调查这个问题，我们构建了一个包含五种类型的视觉幻觉的数据集，并制定了四个任务来研究最先进的VLMs中的视觉幻觉。我们的研究结果显示，尽管整体对齐性较低，但更大规模的模型更接近人类的感知并更容易受到视觉幻觉的影响。我们的数据集和初步结果将促进对人类和机器在感知和交流共享视觉世界方面的更好理解，并为未来能更好地对齐人类和机器在感知和交流共享视觉世界方面的计算模型提供了一个起点。

    Vision-Language Models (VLMs) are trained on vast amounts of data captured by humans emulating our understanding of the world. However, known as visual illusions, human's perception of reality isn't always faithful to the physical world. This raises a key question: do VLMs have the similar kind of illusions as humans do, or do they faithfully learn to represent reality? To investigate this question, we build a dataset containing five types of visual illusions and formulate four tasks to examine visual illusions in state-of-the-art VLMs. Our findings have shown that although the overall alignment is low, larger models are closer to human perception and more susceptible to visual illusions. Our dataset and initial findings will promote a better understanding of visual illusions in humans and machines and provide a stepping stone for future computational models that can better align humans and machines in perceiving and communicating about the shared visual world. The code and data are av
    
[^105]: 贝叶斯多状态Bennett接受比率方法

    Bayesian Multistate Bennett Acceptance Ratio Methods. (arXiv:2310.20699v2 [physics.chem-ph] UPDATED)

    [http://arxiv.org/abs/2310.20699](http://arxiv.org/abs/2310.20699)

    贝叶斯多状态Bennett接受比率方法（BayesMBAR）是多状态Bennett接受比率（MBAR）方法的贝叶斯推广。通过整合采样配置和先验分布，BayesMBAR计算了自由能的后验分布，并提供更准确的不确定性估计。

    

    多状态Bennett接受比率（MBAR）方法是计算热力学状态下自由能的一种常用方法。在这项工作中，我们引入了BayesMBAR，即MBAR方法的贝叶斯推广。通过将来自热力学状态的采样配置与先验分布进行整合，BayesMBAR计算了自由能的后验分布。利用后验分布，我们推导出自由能的估计值并计算它们的相关不确定性。值得注意的是，当使用均匀先验分布时，BayesMBAR可以恢复MBAR的结果，但提供更准确的不确定性估计。此外，当有关自由能的先验知识可用时，BayesMBAR可以通过使用非均匀先验分布将这些信息纳入估计过程中。作为示例，我们展示了通过引入关于自由能曲面平滑性的先验知识，BayesMBAR比MBAR方法提供更准确的估计结果。

    The multistate Bennett acceptance ratio (MBAR) method is a prevalent approach for computing free energies of thermodynamic states. In this work, we introduce BayesMBAR, a Bayesian generalization of the MBAR method. By integrating configurations sampled from thermodynamic states with a prior distribution, BayesMBAR computes a posterior distribution of free energies. Using the posterior distribution, we derive free energy estimations and compute their associated uncertainties. Notably, when a uniform prior distribution is used, BayesMBAR recovers the MBAR's result but provides more accurate uncertainty estimates. Additionally, when prior knowledge about free energies is available, BayesMBAR can incorporate this information into the estimation procedure by using non-uniform prior distributions. As an example, we show that, by incorporating the prior knowledge about the smoothness of free energy surfaces, BayesMBAR provides more accurate estimates than the MBAR method. Given MBAR's widespr
    
[^106]: 长尾学习作为多目标优化

    Long-Tailed Learning as Multi-Objective Optimization. (arXiv:2310.20490v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.20490](http://arxiv.org/abs/2310.20490)

    这项研究通过将长尾识别问题转化为多目标优化问题，提出一种公平地估计头部和尾部类别贡献的方法，并通过梯度平衡分组策略提高了效率。

    

    现实世界中的数据极不平衡，呈现出长尾分布，导致模型对具有足够样本的类别有偏见，并且在罕见类别上表现不佳。最近的方法提出了类别重平衡，但它们面临着相互矛盾的问题（提高尾部类别的性能可能会降低头部类别的性能，反之亦然）。本文认为，这一问题的根源是不同类别的梯度不平衡，不适当类别的梯度被设置为重要更新的部分，因此在尾部类别上容易产生过补偿或欠补偿。为了实现理想的补偿，我们将长尾识别定义为一种多目标优化问题，同时公平地尊重头部和尾部类别的贡献。为了提高效率，我们提出了一种梯度平衡分组（GBG）策略来收集具有相似梯度方向的类别，从而近似使每次更新都在相似的方向上。

    Real-world data is extremely imbalanced and presents a long-tailed distribution, resulting in models that are biased towards classes with sufficient samples and perform poorly on rare classes. Recent methods propose to rebalance classes but they undertake the seesaw dilemma (what is increasing performance on tail classes may decrease that of head classes, and vice versa). In this paper, we argue that the seesaw dilemma is derived from gradient imbalance of different classes, in which gradients of inappropriate classes are set to important for updating, thus are prone to overcompensation or undercompensation on tail classes. To achieve ideal compensation, we formulate the long-tailed recognition as an multi-objective optimization problem, which fairly respects the contributions of head and tail classes simultaneously. For efficiency, we propose a Gradient-Balancing Grouping (GBG) strategy to gather the classes with similar gradient directions, thus approximately make every update under 
    
[^107]: 通过对结构力学应用的调查，讨论物理增强机器学习的光谱

    Discussing the Spectra of Physics-Enhanced Machine Learning via a Survey on Structural Mechanics Applications. (arXiv:2310.20425v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.20425](http://arxiv.org/abs/2310.20425)

    本文探讨了物理增强机器学习 (PEML) 的光谱，揭示了其在解决复杂挑战方面的潜力，并通过具体示例演示了不同类型PEML方法的个体特征和动机。

    

    物理和机器学习的交叉已经催生了一种我们在这里称之为物理增强机器学习 (PEML) 的范式，旨在提高数据或物理方法的能力和减少各自的缺点。本文通过对其特征、用法和动机的全面探索来讨论物理增强机器学习方法的光谱，涵盖了物理和数据这两个定义轴。在这样做的过程中，本文提供了对PEML技术的最新应用和发展的调查，揭示了PEML在应对复杂挑战方面的潜力。我们进一步演示了在单自由度Duffing振子的简单工作示例上选择这种方法的应用，从而突出了不同“类型”PEML方法的个体特征和动机。为了促进合作和透明度，并为读者提供实际的示例，本文完整地记录了所采取的方法和模型。

    The intersection of physics and machine learning has given rise to a paradigm that we refer to here as physics-enhanced machine learning (PEML), aiming to improve the capabilities and reduce the individual shortcomings of data- or physics-only methods. In this paper, the spectrum of physics-enhanced machine learning methods, expressed across the defining axes of physics and data, is discussed by engaging in a comprehensive exploration of its characteristics, usage, and motivations. In doing so, this paper offers a survey of recent applications and developments of PEML techniques, revealing the potency of PEML in addressing complex challenges. We further demonstrate application of select such schemes on the simple working example of a single-degree-of-freedom Duffing oscillator, which allows to highlight the individual characteristics and motivations of different `genres' of PEML approaches. To promote collaboration and transparency, and to provide practical examples for the reader, the
    
[^108]: 强化学习中的Dropout策略：限制策略优化方法中替代目标方差的增长

    Dropout Strategy in Reinforcement Learning: Limiting the Surrogate Objective Variance in Policy Optimization Methods. (arXiv:2310.20380v1 [cs.LG])

    [http://arxiv.org/abs/2310.20380](http://arxiv.org/abs/2310.20380)

    本文提出了一种Dropout技术来限制策略优化方法中替代目标方差的增长，并将其应用于PPO算法中。实验结果表明，D-PPO算法相较于PPO算法在Atari 2600游戏上表现更好。

    

    基于策略的强化学习算法在各个领域中被广泛应用。其中，主流的策略优化算法如PPO和TRPO引入了重要性采样到强化学习中，这允许重用历史数据。然而，这也导致了替代目标方差的增加，间接影响了算法的稳定性和收敛性。本文首先推导出了替代目标方差的上界，它可以随替代目标的增加而呈二次增长。接下来，我们提出了一种Dropout技术，以避免重要性采样引起的替代目标方差过度增加。然后，我们引入了一个通用的强化学习框架，适用于主流的策略优化方法，并将Dropout技术应用于PPO算法，得到了D-PPO变体。最后，我们在Atari 2600游戏上对D-PPO和PPO算法进行了比较实验。

    Policy-based reinforcement learning algorithms are widely used in various fields. Among them, mainstream policy optimization algorithms such as PPO and TRPO introduce importance sampling into reinforcement learning, which allows the reuse of historical data. However, this also results in high variance of the surrogate objective and indirectly affects the stability and convergence of the algorithm. In this paper, we first derived an upper bound of the variance of the surrogate objective, which can grow quadratically with the increase of the surrogate objective. Next, we proposed a dropout technique to avoid the excessive increase of the surrogate objective variance caused by importance sampling. Then, we introduced a general reinforcement learning framework applicable to mainstream policy optimization methods, and applied the dropout technique to the PPO algorithm to obtain the D-PPO variant. Finally, we conduct comparative experiments between D-PPO and PPO algorithms in the Atari 2600 
    
[^109]: 通过引导学习发现技能

    Learning to Discover Skills through Guidance. (arXiv:2310.20178v1 [cs.LG])

    [http://arxiv.org/abs/2310.20178](http://arxiv.org/abs/2310.20178)

    提出了一种名为DISCO-DANCE的无监督技能发现算法，通过引导学习提高探索效果，并在具有挑战性的环境中优于其他方法。

    

    在无监督技能发现领域，主要挑战是有限的探索，主要是因为技能偏离其初始轨迹会受到重大惩罚。为了增强探索，最近的方法使用辅助奖励来最大化状态的认知不确定性或熵。然而，我们发现这些奖励的效果随着环境复杂性的增加而下降。因此，我们提出了一种新的无监督技能发现算法，DISCO-DANCE，它选择具有达到未探索状态潜力最高的引导技能，引导其他技能遵循引导技能，然后分散引导技能以最大化在未探索状态中的可区分性。实证评估表明，在具有挑战性的环境中，包括两个导航基准和一个连续控制基准，DISCO-DANCE优于其他无监督技能发现基线。DISCO-DANCE的定性可视化和代码。

    In the field of unsupervised skill discovery (USD), a major challenge is limited exploration, primarily due to substantial penalties when skills deviate from their initial trajectories. To enhance exploration, recent methodologies employ auxiliary rewards to maximize the epistemic uncertainty or entropy of states. However, we have identified that the effectiveness of these rewards declines as the environmental complexity rises. Therefore, we present a novel USD algorithm, skill discovery with guidance (DISCO-DANCE), which (1) selects the guide skill that possesses the highest potential to reach unexplored states, (2) guides other skills to follow guide skill, then (3) the guided skills are dispersed to maximize their discriminability in unexplored states. Empirical evaluation demonstrates that DISCO-DANCE outperforms other USD baselines in challenging environments, including two navigation benchmarks and a continuous control benchmark. Qualitative visualizations and code of DISCO-DANCE
    
[^110]: SURF: GNN预测流体动力学的泛化性能评估

    SURF: A Generalization Benchmark for GNNs Predicting Fluid Dynamics. (arXiv:2310.20049v1 [cs.LG])

    [http://arxiv.org/abs/2310.20049](http://arxiv.org/abs/2310.20049)

    提出了一个名为SURF的基准测试，用于评估和比较基于图的学习流体模拟器的泛化能力。SURF包括各种数据集和具体的性能和泛化度量指标。通过深入研究两种最先进的模型，我们证明了SURF的适用性。

    

    模拟流体动力学对于设计和开发过程至关重要，涵盖了从简单阀门到复杂涡轮机械的范围。准确求解潜在的物理方程具有计算成本高的特点。因此，基于学习的求解器在网格上建模相互作用并具有显著的加速优势。然而，目前尚不清楚这些模型在多大程度上真正理解潜在的物理原理，并能够实现泛化而非插值。泛化是通用流体模拟器的关键要求，它应该能够适应不同的拓扑结构、分辨率或热力学范围。我们提出了SURF，这是一个旨在测试学习的基于图的流体模拟器的泛化能力的基准测试。SURF包括各个数据集，并提供用于评估和比较不同模型的具体性能和泛化度量指标。我们通过深入研究两种最先进的模型，实证地证明了SURF的适用性。

    Simulating fluid dynamics is crucial for the design and development process, ranging from simple valves to complex turbomachinery. Accurately solving the underlying physical equations is computationally expensive. Therefore, learning-based solvers that model interactions on meshes have gained interest due to their promising speed-ups. However, it is unknown to what extent these models truly understand the underlying physical principles and can generalize rather than interpolate. Generalization is a key requirement for a general-purpose fluid simulator, which should adapt to different topologies, resolutions, or thermodynamic ranges. We propose SURF, a benchmark designed to test the \textit{generalization} of learned graph-based fluid simulators. SURF comprises individual datasets and provides specific performance and generalization metrics for evaluating and comparing different models. We empirically demonstrate the applicability of SURF by thoroughly investigating the two state-of-the
    
[^111]: 通过$f$-差分隐私统一增强混合机制的隐私边界

    Unified Enhancement of Privacy Bounds for Mixture Mechanisms via $f$-Differential Privacy. (arXiv:2310.19973v1 [stat.ML])

    [http://arxiv.org/abs/2310.19973](http://arxiv.org/abs/2310.19973)

    本文通过$f$-差分隐私方法改进了洗牌模型和DP-GD中随机初始化的隐私边界，折衷函数的闭式表达式优于$(\epsilon,\delta)$-DP的结果，并且随机初始化可以增强DP-GD的隐私性。

    

    差分隐私（DP）机器学习算法会产生许多随机性，如随机初始化、随机批次抽样和洗牌。然而，由于这些随机性会导致难以分析的混合分布，所以在证明差分隐私边界时很难将其纳入考虑。本文旨在改进洗牌模型和一次迭代的差分隐私梯度下降（DP-GD）中用于随机初始化的隐私边界，采用$f$-DP方法。我们导出了洗牌模型的折衷函数的闭式表达式，优于基于$(\epsilon,\delta)$-DP的最新结果。此外，我们还对随机初始化对一次迭代的DP-GD的隐私性进行了研究。我们对折衷函数的数值计算表明，随机初始化可以增强DP-GD的隐私性。我们对这些混合机制的$f$-DP保证的分析依赖于一种不等式。

    Differentially private (DP) machine learning algorithms incur many sources of randomness, such as random initialization, random batch subsampling, and shuffling. However, such randomness is difficult to take into account when proving differential privacy bounds because it induces mixture distributions for the algorithm's output that are difficult to analyze. This paper focuses on improving privacy bounds for shuffling models and one-iteration differentially private gradient descent (DP-GD) with random initializations using $f$-DP. We derive a closed-form expression of the trade-off function for shuffling models that outperforms the most up-to-date results based on $(\epsilon,\delta)$-DP. Moreover, we investigate the effects of random initialization on the privacy of one-iteration DP-GD. Our numerical computations of the trade-off function indicate that random initialization can enhance the privacy of DP-GD. Our analysis of $f$-DP guarantees for these mixture mechanisms relies on an ine
    
[^112]: 学习生成参数概率模型的随机热力学

    Stochastic Thermodynamics of Learning Generative Parametric Probabilistic Models. (arXiv:2310.19802v1 [cs.LG])

    [http://arxiv.org/abs/2310.19802](http://arxiv.org/abs/2310.19802)

    本文将生成式机器学习问题视为参数概率模型的时间演化过程，通过研究模型参数与生成样本之间的热力学交换，发现模型通过耗散热量来学习，参数子系统充当热库存储学到的信息。这为超参数模型的泛化能力提供了有价值的热力学洞察。

    

    我们将生成式机器学习问题形式化为参数化概率模型（PPM）的时间演化，从本质上来说，这是一个热力学过程。然后，我们研究了模型参数（记为$\Theta$）与模型生成样本（记为$X$）之间的热力学交换。我们证明了训练数据集和随机梯度下降（SGD）优化器的作用是驱动这两个子系统的时间演化的能源。我们的发现表明，在生成样本$X$的过程中，模型通过耗散热量来学习，导致模型参数$\Theta$的熵增加。因此，参数子系统充当了一个热库，有效地存储了学到的信息。此外，模型参数作为热库的角色为超参数模型的泛化能力提供了有价值的热力学洞察。这种方法提供了一个明确且一致的方式来理解生成模型学习过程中的热力学行为。

    We have formulated generative machine learning problems as the time evolution of Parametric Probabilistic Models (PPMs), inherently rendering a thermodynamic process. Then, we have studied the thermodynamic exchange between the model's parameters, denoted as $\Theta$, and the model's generated samples, denoted as $X$. We demonstrate that the training dataset and the action of the Stochastic Gradient Descent (SGD) optimizer serve as a work source that governs the time evolution of these two subsystems. Our findings reveal that the model learns through the dissipation of heat during the generation of samples $X$, leading to an increase in the entropy of the model's parameters, $\Theta$. Thus, the parameter subsystem acts as a heat reservoir, effectively storing the learned information. Furthermore, the role of the model's parameters as a heat reservoir provides valuable thermodynamic insights into the generalization power of over-parameterized models. This approach offers an unambiguous 
    
[^113]: 结合语言模型的领域专用方法：一种丰富多彩的途径

    Combining Language Models For Specialized Domains: A Colorful Approach. (arXiv:2310.19708v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.19708](http://arxiv.org/abs/2310.19708)

    该论文提出了一种将领域特定语言模型与通用语言模型结合的新颖方法，通过对词语进行标记或“上色”来有效处理领域术语，显著降低了领域专用任务的错误率。

    

    通用目的的语言模型在处理领域特定术语和术语时遇到困难，这些术语经常在医学或工业领域等专业领域中使用。此外，他们通常很难解释将通用语言与专门术语混合使用的混合语音。这对于在这些特定领域内操作的自动语音识别系统构成了挑战。在这项工作中，我们介绍了一种新颖的方法，将领域特定或次级语言模型集成到通用的语言模型中。该策略涉及对每个单词进行标记或“上色”，以指示其与通用或领域特定的语言模型的关联。我们开发了一种优化算法，可增强波束搜索算法，以有效处理涉及上色单词的推理。我们的评估表明，这种方法在集成术语到语言任务中非常有效。值得注意的是，我们的方法显著降低了领域专用任务的错误率。

    General purpose language models (LMs) encounter difficulties when processing domain-specific jargon and terminology, which are frequently utilized in specialized fields such as medicine or industrial settings. Moreover, they often find it challenging to interpret mixed speech that blends general language with specialized jargon. This poses a challenge for automatic speech recognition systems operating within these specific domains. In this work, we introduce a novel approach that integrates domain-specific or secondary LM into general-purpose LM. This strategy involves labeling, or ``coloring'', each word to indicate its association with either the general or the domain-specific LM. We develop an optimized algorithm that enhances the beam search algorithm to effectively handle inferences involving colored words. Our evaluations indicate that this approach is highly effective in integrating jargon into language tasks. Notably, our method substantially lowers the error rate for domain-sp
    
[^114]: DGFN: 双生成流网络

    DGFN: Double Generative Flow Networks. (arXiv:2310.19685v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.19685](http://arxiv.org/abs/2310.19685)

    双生成流网络（DGFNs）是一种能够有效增强药物发现中探索能力的方法，通过引入目标网络和采样路径的方式，解决了稀疏奖励领域和高维状态空间的挑战。

    

    深度学习作为一种有效的药物发现工具正在崭露头角，具有在预测和生成模型中的潜在应用。生成流网络（GFlowNets / GFNs）是一种最近引入的方法，因其在小分子生成任务中能够生成多样化的候选物而受到认可。在这项工作中，我们引入了双生成流网络（DGFNs）。受强化学习和双深度Q学习的启发，我们引入了一个目标网络用于采样路径，并使用这些采样路径来更新主网络。实证结果证实，DGFNs能够有效增强稀疏奖励领域和高维状态空间的探索能力，这都是药物发现中全新设计的挑战性方面。

    Deep learning is emerging as an effective tool in drug discovery, with potential applications in both predictive and generative models. Generative Flow Networks (GFlowNets/GFNs) are a recently introduced method recognized for the ability to generate diverse candidates, in particular in small molecule generation tasks. In this work, we introduce double GFlowNets (DGFNs). Drawing inspiration from reinforcement learning and Double Deep Q-Learning, we introduce a target network used to sample trajectories, while updating the main network with these sampled trajectories. Empirical results confirm that DGFNs effectively enhance exploration in sparse reward domains and high-dimensional state spaces, both challenging aspects of de-novo design in drug discovery.
    
[^115]: rTsfNet:一种具有多头3D旋转和时序特征提取的基于IMU的人体活动识别DNN模型

    rTsfNet: a DNN model with Multi-head 3D Rotation and Time Series Feature Extraction for IMU-based Human Activity Recognition. (arXiv:2310.19283v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2310.19283](http://arxiv.org/abs/2310.19283)

    rTsfNet是一种新的DNN模型，通过多头3D旋转和时序特征提取实现了IMU-based人体活动识别，并在多个数据集上取得了最高的准确率。

    

    本论文提出了rTsfNet，一种具有多头3D旋转和时序特征提取的DNN模型，作为IMU-based人体活动识别的新型DNN模型。rTsfNet通过在DNN内部推导3D旋转参数，自动选择应该从中派生特征的3D基准。然后，利用MLP推导时序特征（TSFs）并实现HAR。尽管该模型不使用CNN，在良好管理的基准条件和多个数据集（UCI HAR, PAMAP2, Daphnet, 和OPPORTUNITY）上取得了最高的准确率，这些数据集针对不同的活动。

    This paper proposes rTsfNet, a DNN model with Multi-head 3D Rotation and Time Series Feature Extraction, as a new DNN model for IMU-based human activity recognition (HAR). rTsfNet automatically selects 3D bases from which features should be derived by deriving 3D rotation parameters within the DNN. Then, time series features (TSFs), the wisdom of many researchers, are derived and realize HAR using MLP. Although a model that does not use CNN, it achieved the highest accuracy than existing models under well-managed benchmark conditions and multiple datasets: UCI HAR, PAMAP2, Daphnet, and OPPORTUNITY, which target different activities.
    
[^116]: 揭示自由核聚变的神话：使用解耦的专家产品建模进行在线多视角异常检测

    Debunking Free Fusion Myth: Online Multi-view Anomaly Detection with Disentangled Product-of-Experts Modeling. (arXiv:2310.18728v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.18728](http://arxiv.org/abs/2310.18728)

    本文提出了一种新型的多视角变分自编码器模型dPoE，通过使用解耦表示学习和生成模型的方法，解决了多视角数据异常检测中存在的问题。该方法不仅适用于多个视图和多种类型的异常，还支持模型部署后的在线检测。

    

    多视角甚至多模态数据对实际应用具有吸引力但也具有挑战性。检测多视角数据中的异常是一个突出的最近研究课题。然而，大多数现有方法: 1）仅适用于两个视图或类型特定的异常，2）存在融合解耦问题，3）在模型部署后不支持在线检测。为了应对这些挑战，本文的主要思路有三个:多视图学习，解耦表示学习和生成模型。为此，我们提出了dPoE，一种新颖的多视角变分自编码器模型，它包含(1)一个专家乘积(PoE)层来处理多视图数据，(2)一个总校正(TC)鉴别器来解耦视图共享和视图特定表示，(3)一个联合损失函数来整合所有组件。此外，我们设计了理论信息界来控制视图共享和视图特定表示。大量实验验证了我们方法的有效性。

    Multi-view or even multi-modal data is appealing yet challenging for real-world applications. Detecting anomalies in multi-view data is a prominent recent research topic. However, most of the existing methods 1) are only suitable for two views or type-specific anomalies, 2) suffer from the issue of fusion disentanglement, and 3) do not support online detection after model deployment. To address these challenges, our main ideas in this paper are three-fold: multi-view learning, disentangled representation learning, and generative model. To this end, we propose dPoE, a novel multi-view variational autoencoder model that involves (1) a Product-of-Experts (PoE) layer in tackling multi-view data, (2) a Total Correction (TC) discriminator in disentangling view-common and view-specific representations, and (3) a joint loss function in wrapping up all components. In addition, we devise theoretical information bounds to control both view-common and view-specific representations. Extensive exper
    
[^117]: 防止语言模型隐藏其推理过程

    Preventing Language Models From Hiding Their Reasoning. (arXiv:2310.18512v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.18512](http://arxiv.org/abs/2310.18512)

    本论文研究了防止语言模型隐藏推理过程的问题。我们发现语言模型可以通过编码推理来提高性能，而无需用户理解中间推理步骤。随着语言模型变得越来越强大，这种行为可能会越来越普遍。我们还提出了一种方法来评估防御编码推理的方法。

    

    大型语言模型（LLM）通常通过推理的中间步骤来生成复杂问题的答案。当这些推理的中间步骤被用来监控模型的活动时，关键是这种明确的推理是可信的，即反映出模型实际上在推理什么。本文关注一种可能导致推理的中间步骤不可信的方式：编码推理，即LLM可能以人类读者无法理解的方式将推理的中间步骤编码在生成的文本中。我们展示了语言模型可以被训练成利用编码推理以获得更高的性能，而用户并不需要理解中间推理步骤。我们认为，随着语言模型的增强，这种行为更可能自然出现。最后，我们描述了一种评估针对编码推理的防御方法的方法，并表明在合适的条件下可以实现。

    Large language models (LLMs) often benefit from intermediate steps of reasoning to generate answers to complex problems. When these intermediate steps of reasoning are used to monitor the activity of the model, it is essential that this explicit reasoning is faithful, i.e. that it reflects what the model is actually reasoning about. In this work, we focus on one potential way intermediate steps of reasoning could be unfaithful: encoded reasoning, where an LLM could encode intermediate steps of reasoning in the generated text in a way that is not understandable to human readers. We show that language models can be trained to make use of encoded reasoning to get higher performance without the user understanding the intermediate steps of reasoning. We argue that, as language models get stronger, this behavior becomes more likely to appear naturally. Finally, we describe a methodology that enables the evaluation of defenses against encoded reasoning, and show that, under the right conditio
    
[^118]: 对抗不完美观察者的隐秘规划

    Covert Planning against Imperfect Observers. (arXiv:2310.16791v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2310.16791](http://arxiv.org/abs/2310.16791)

    本文研究了对抗不完美观察者的隐秘规划问题，提出了一种利用随机动力学和观察者的不完美观测耦合的方法，实现最优任务性能而不被发现。

    

    隐秘规划指的是一类有限制的规划问题，其中代理程序旨在在最小信息泄漏给被动观察者的情况下完成任务，以避免被发现。然而，现有的隐秘规划方法通常考虑确定性环境，或者不利用观察者的不完美信息。本文研究了如何利用随机动力学和观察者的不完美观测耦合来实现最优的任务性能，同时避免被发现。具体而言，我们使用马尔可夫决策过程来建模代理程序与其随机环境之间的相互作用，并使用部分观测函数来捕捉泄漏给被动观察者的信息。假设观察者使用假设检验来检测观测是否偏离了名义策略，隐秘规划代理程序的目标是在保持被敌方发现的概率低于给定阈值的同时，最大化总折扣奖励。

    Covert planning refers to a class of constrained planning problems where an agent aims to accomplish a task with minimal information leaked to a passive observer to avoid detection. However, existing methods of covert planning often consider deterministic environments or do not exploit the observer's imperfect information. This paper studies how covert planning can leverage the coupling of stochastic dynamics and the observer's imperfect observation to achieve optimal task performance without being detected. Specifically, we employ a Markov decision process to model the interaction between the agent and its stochastic environment, and a partial observation function to capture the leaked information to a passive observer. Assuming the observer employs hypothesis testing to detect if the observation deviates from a nominal policy, the covert planning agent aims to maximize the total discounted reward while keeping the probability of being detected as an adversary below a given threshold.
    
[^119]: 多标签学习中的神经坍缩问题研究

    Neural Collapse in Multi-label Learning with Pick-all-label Loss. (arXiv:2310.15903v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.15903](http://arxiv.org/abs/2310.15903)

    这项论文研究了在多标签分类任务中的神经坍缩现象。他们推广了之前在多类别分类中发现的神经坍缩现象，证明了在“选择所有标签”公式下存在广义的神经坍缩现象。他们还发现了在广义的神经坍缩中的一个组合性质。

    

    我们通过神经坍缩（NC）的角度研究了深度神经网络在多标签分类（MLab）任务中的应用。之前的研究都局限于多类别分类，发现了一种普遍存在的NC现象，其中最后一层特征具有以下特点：（i）每个类别内的特征变异性为零，（ii）特征均值集合构成一个等角紧框架（ETF），（iii）最后一层分类器收缩到特征均值乘以某个缩放因子。我们将这个研究推广到多标签学习，并首次证明了“选择所有标签”公式存在广义NC现象。在自然的无约束特征模型（UFM）的情况下，我们证明了“选择所有标签”的交叉熵损失函数的全局分类器只显示出相同的ETF几何结构，进一步坍缩到多重性为1的特征类均值。

    We study deep neural networks for the multi-label classification (MLab) task through the lens of neural collapse (NC). Previous works have been restricted to the multi-class classification setting and discovered a prevalent NC phenomenon comprising of the following properties for the last-layer features: (i) the variability of features within every class collapses to zero, (ii) the set of feature means form an equi-angular tight frame (ETF), and (iii) the last layer classifiers collapse to the feature mean upon some scaling. We generalize the study to multi-label learning, and prove for the first time that a generalized NC phenomenon holds with the "pick-all-label" formulation. Under the natural analog of the unconstrained feature model (UFM), we establish that the only global classifier of the pick-all-label cross entropy loss display the same ETF geometry which further collapse to multiplicity-1 feature class means. Besides, we discover a combinatorial property in generalized NC whic
    
[^120]: Nko语的机器翻译：工具、语料库和基准结果

    Machine Translation for Nko: Tools, Corpora and Baseline Results. (arXiv:2310.15612v1 [cs.CL])

    [http://arxiv.org/abs/2310.15612](http://arxiv.org/abs/2310.15612)

    该论文提出了针对Nko语（一种在多个西非国家使用的语言）开发可用的机器翻译系统的一套工具、资源和基准结果，包括新颖的协作平行文本整理软件、扩展的语料库和基线神经机器翻译结果。

    

    目前，尼科语（一种在多个西非国家使用的语言）没有可用的机器翻译系统，但它在文化和教育价值上具有重要意义。为了解决这个问题，我们提出了一套工具、资源和基准结果，旨在开发可用的尼科语和其他当前没有足够大的平行文本语料库的语言的机器翻译系统。具体包括：(1) Friallel：一种新颖的协作平行文本整理软件，通过基于副本编辑的工作流程实现质量控制。(2) 扩展了FLoRes-200和NLLB-Seed语料库，从其他语言中与尼科语平行翻译了2,009和6,193个高质量的文本。(3) nicolingua-0005：包含130,850个平行片段的三语和双语语料库，以及超过3百万尼科语单语言语料库。(4) 基线双语和多语言神经机器翻译结果与b...

    Currently, there is no usable machine translation system for Nko, a language spoken by tens of millions of people across multiple West African countries, which holds significant cultural and educational value. To address this issue, we present a set of tools, resources, and baseline results aimed towards the development of usable machine translation systems for Nko and other languages that do not currently have sufficiently large parallel text corpora available. (1) Friallel: A novel collaborative parallel text curation software that incorporates quality control through copyedit-based workflows. (2) Expansion of the FLoRes-200 and NLLB-Seed corpora with 2,009 and 6,193 high-quality Nko translations in parallel with 204 and 40 other languages. (3) nicolingua-0005: A collection of trilingual and bilingual corpora with 130,850 parallel segments and monolingual corpora containing over 3 million Nko words. (4) Baseline bilingual and multilingual neural machine translation results with the b
    
[^121]: 多项选择视觉问答中的数据集偏差缓解及其扩展

    Dataset Bias Mitigation in Multiple-Choice Visual Question Answering and Beyond. (arXiv:2310.14670v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.14670](http://arxiv.org/abs/2310.14670)

    这项研究提出了一种解决多项选择视觉问答中数据集偏差的方法，包括不平衡匹配偏差和分心相似性偏差，并提出了对抗数据合成和样本内对立训练的技术来应对这些偏差。

    

    视觉-语言（VL）理解任务通过多项选择问题评估模型对复杂视觉场景的理解能力。然而，我们发现模型可以利用两种数据集偏差作为无需正确理解即可正确解决各种VL任务的捷径。第一种数据集偏差是"不平衡匹配"偏差，即正确答案与问题和图像的重叠程度超过错误答案。第二种数据集偏差是"分心相似性"偏差，即错误答案与正确答案过于不相似，但与同一个样本中的其他错误答案相似。为了解决这些数据集偏差，我们首先提出了对抗数据合成（ADS）来生成合成的训练和去偏差的评估数据。然后，我们引入了样本内 对立训练（ICT）来帮助模型利用合成的训练数据，特别是对立事实数据，通过注重样本内的差异来进行训练。

    Vision-language (VL) understanding tasks evaluate models' comprehension of complex visual scenes through multiple-choice questions. However, we have identified two dataset biases that models can exploit as shortcuts to resolve various VL tasks correctly without proper understanding. The first type of dataset bias is \emph{Unbalanced Matching} bias, where the correct answer overlaps the question and image more than the incorrect answers. The second type of dataset bias is \emph{Distractor Similarity} bias, where incorrect answers are overly dissimilar to the correct answer but significantly similar to other incorrect answers within the same sample. To address these dataset biases, we first propose Adversarial Data Synthesis (ADS) to generate synthetic training and debiased evaluation data. We then introduce Intra-sample Counterfactual Training (ICT) to assist models in utilizing the synthesized training data, particularly the counterfactual data, via focusing on intra-sample differentia
    
[^122]: CLARA: 多语言对比学习用于音频表示获取的论文

    CLARA: Multilingual Contrastive Learning for Audio Representation Acquisition. (arXiv:2310.11830v1 [cs.SD])

    [http://arxiv.org/abs/2310.11830](http://arxiv.org/abs/2310.11830)

    本文提出了一个多语言对比学习框架，通过自监督学习从无标签数据中获取音频表示，实现了跨语言迁移和情感维度的编码。

    

    本文提出了一个新的多语言语音和音频表示学习框架，使用对比学习。标注数据不足制约了跨语言语音处理研究的发展。最近对比学习的进展提供了自监督技术来从无标签数据中学习。为了减少数据依赖性和改善在不同语言和条件下的泛化能力，我们开发了一个多语言对比学习框架。该框架使模型能够在多语言中获得共享表示，有助于使用有限的目标语言数据进行跨语言迁移。此外，由于主观感知评估的挑战，捕捉语音中的情感线索是困难的。通过自监督的方式从多样的多语言数据中学习表达性表示，我们的方法旨在开发编码情感维度的语音表示。我们的方法在大规模的多语言音频数据集上训练编码器。

    This paper proposes a novel framework for multilingual speech and sound representation learning using contrastive learning. The lack of sizeable labelled datasets hinders speech-processing research across languages. Recent advances in contrastive learning provide self-supervised techniques to learn from unlabelled data. Motivated by reducing data dependence and improving generalisation across diverse languages and conditions, we develop a multilingual contrastive framework. This framework enables models to acquire shared representations across languages, facilitating cross-lingual transfer with limited target language data.  Additionally, capturing emotional cues within speech is challenging due to subjective perceptual assessments. By learning expressive representations from diverse, multilingual data in a self-supervised manner, our approach aims to develop speech representations that encode emotive dimensions.  Our method trains encoders on a large corpus of multi-lingual audio data
    
[^123]: 具有非参数需求学习的平滑自适应动态定价

    Smootheness-Adaptive Dynamic Pricing with Nonparametric Demand Learning. (arXiv:2310.07558v1 [stat.ML])

    [http://arxiv.org/abs/2310.07558](http://arxiv.org/abs/2310.07558)

    这项研究提出了一种具有非参数需求学习和平滑自适应的动态定价算法，通过使用自相似条件实现了最小化极限遗憾。

    

    我们研究了需求函数为非参数和Holder平滑的动态定价问题，并且我们专注于适应未知的Holder平滑参数β的能力。传统上，最优的动态定价算法严重依赖于对β的了解，以达到一个最小化极限遗憾的效果，即O(T^((β+1)/(2β+1)))。然而，我们通过证明没有定价策略能够在不知道β的情况下自适应地达到这个最小化极限遗憾，突显了这个动态定价问题的适应性挑战。受到不可能性结果的启发，我们提出了一种自相似条件来实现适应性。重要的是，我们证明了自相似条件不会损害问题本身的复杂性，因为它保持了渐近遗憾下界Ω(T^((β+1)/(2β+1)))。此外，我们开发了一种平滑自适应的动态定价算法，并理论上证明了该算法的有效性。

    We study the dynamic pricing problem where the demand function is nonparametric and H\"older smooth, and we focus on adaptivity to the unknown H\"older smoothness parameter $\beta$ of the demand function. Traditionally the optimal dynamic pricing algorithm heavily relies on the knowledge of $\beta$ to achieve a minimax optimal regret of $\widetilde{O}(T^{\frac{\beta+1}{2\beta+1}})$. However, we highlight the challenge of adaptivity in this dynamic pricing problem by proving that no pricing policy can adaptively achieve this minimax optimal regret without knowledge of $\beta$. Motivated by the impossibility result, we propose a self-similarity condition to enable adaptivity. Importantly, we show that the self-similarity condition does not compromise the problem's inherent complexity since it preserves the regret lower bound $\Omega(T^{\frac{\beta+1}{2\beta+1}})$. Furthermore, we develop a smoothness-adaptive dynamic pricing algorithm and theoretically prove that the algorithm achieves t
    
[^124]: FABind: 快速准确的蛋白-配体结合

    FABind: Fast and Accurate Protein-Ligand Binding. (arXiv:2310.06763v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.06763](http://arxiv.org/abs/2310.06763)

    FABind是一个结合了口袋预测和对接的端到端模型，旨在实现快速准确的蛋白-配体结合预测。

    

    在药物发现中，对蛋白质和配体之间的相互作用进行建模并准确预测其结合结构是一项关键但具有挑战性的任务。深度学习的最新进展在应对这一挑战方面显示出了希望，采样法和回归法成为两种突出的方法。然而，这些方法都存在明显的局限性。采样法通常由于需要生成多个候选结构来进行选择而效率较低。而回归法提供了快速的预测，但可能会导致准确性降低。另外，蛋白质大小的变化通常需要外部模块来选择合适的结合口袋，进一步影响效率。在这项工作中，我们提出了FABind，一个将口袋预测和对接相结合的端到端模型，以实现准确和快速的蛋白-配体结合。

    Modeling the interaction between proteins and ligands and accurately predicting their binding structures is a critical yet challenging task in drug discovery. Recent advancements in deep learning have shown promise in addressing this challenge, with sampling-based and regression-based methods emerging as two prominent approaches. However, these methods have notable limitations. Sampling-based methods often suffer from low efficiency due to the need for generating multiple candidate structures for selection. On the other hand, regression-based methods offer fast predictions but may experience decreased accuracy. Additionally, the variation in protein sizes often requires external modules for selecting suitable binding pockets, further impacting efficiency. In this work, we propose $\mathbf{FABind}$, an end-to-end model that combines pocket prediction and docking to achieve accurate and fast protein-ligand binding. $\mathbf{FABind}$ incorporates a unique ligand-informed pocket prediction
    
[^125]: 高维后验推断的隐变分推断

    Implicit Variational Inference for High-Dimensional Posteriors. (arXiv:2310.06643v1 [cs.LG])

    [http://arxiv.org/abs/2310.06643](http://arxiv.org/abs/2310.06643)

    本文提出了一种隐变分推断的方法，使用神经采样器指定隐含分布，在高维空间中近似复杂的多峰和相关后验分布。通过引入局部线性化的约束，避免了依赖额外的网络和不稳定对抗目标的问题。此外，还提出了一种新的采样器架构，首次实现了对数百万个潜变量的隐含分布。实证分析表明，该方法可以恢复大型贝叶斯神经网络中层之间的相关性，这对于网络的性能至关重要。

    

    在变分推断中，贝叶斯模型的好处在于准确捕捉真实的后验分布。我们提出使用指定隐含分布的神经采样器，这对于近似高维空间中复杂多峰和相关后验分布非常适用。我们的方法通过局部线性化神经采样器引入新的约束，这与现有方法不同，现有方法依赖于额外的鉴别器网络和不稳定的对抗目标。此外，我们提出了一种新的采样器架构，首次实现了对数百万个潜变量的隐含分布，通过使用可微分的数值近似来解决计算上的问题。我们的实证分析表明，我们的方法能够在大型贝叶斯神经网络中恢复层之间的相关性，这是网络性能关键但臭名昭著的属性。

    In variational inference, the benefits of Bayesian models rely on accurately capturing the true posterior distribution. We propose using neural samplers that specify implicit distributions, which are well-suited for approximating complex multimodal and correlated posteriors in high-dimensional spaces. Our approach advances inference using implicit distributions by introducing novel bounds that come about by locally linearising the neural sampler. This is distinct from existing methods that rely on additional discriminator networks and unstable adversarial objectives. Furthermore, we present a new sampler architecture that, for the first time, enables implicit distributions over millions of latent variables, addressing computational concerns by using differentiable numerical approximations. Our empirical analysis indicates our method is capable of recovering correlations across layers in large Bayesian neural networks, a property that is crucial for a network's performance but notorious
    
[^126]: 自动化的时空神经点过程积分方法

    Automatic Integration for Spatiotemporal Neural Point Processes. (arXiv:2310.06179v1 [cs.LG])

    [http://arxiv.org/abs/2310.06179](http://arxiv.org/abs/2310.06179)

    本文提出了一种自动化的时空神经点过程积分方法(AutoSTPP)，扩展了AutoInt方法用于三维时空点过程(STPP)的计算，具有优越性能。

    

    学习连续时间的点过程对于许多离散事件预测任务至关重要。然而，对于时空点过程（STPPs）的积分问题是一个重要挑战，因为它涉及到对空间和时间进行三重积分计算。现有的STPP积分方法要么假设强度函数具有参数形式，这缺乏灵活性；要么用蒙特卡洛采样来近似强度，这引入了数值误差。Omi等人最近的工作提出了一个自动积分方法AutoInt，用于高效地积分灵活的强度函数，但该方法只关注1D时间点过程。本文将AutoInt方法扩展至3D STPP，提出了一种新的范式：AutoSTPP（自动化的时空神经点过程积分方法）。我们表明，直接扩展之前的工作会过于约束强度函数，导致性能不佳。我们证明了我们的方法在各种实验中的优越性能。

    Learning continuous-time point processes is essential to many discrete event forecasting tasks. However, integration poses a major challenge, particularly for spatiotemporal point processes (STPPs), as it involves calculating the likelihood through triple integrals over space and time. Existing methods for integrating STPP either assume a parametric form of the intensity function, which lacks flexibility; or approximating the intensity with Monte Carlo sampling, which introduces numerical errors. Recent work by Omi et al. [2019] proposes a dual network or AutoInt approach for efficient integration of flexible intensity function. However, the method only focuses on the 1D temporal point process. In this paper, we introduce a novel paradigm: AutoSTPP (Automatic Integration for Spatiotemporal Neural Point Processes) that extends the AutoInt approach to 3D STPP. We show that direct extension of the previous work overly constrains the intensity function, leading to poor performance. We prov
    
[^127]: 查询和应答增强不能帮助领域外数学推理的泛化

    Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning Generalization. (arXiv:2310.05506v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05506](http://arxiv.org/abs/2310.05506)

    本文调查了在数学推理中使用数据增强的效果，并通过创建新的数据集和微调模型取得了显著成果。

    

    在使用大型语言模型（LLM）进行数学推理时，通过查询演化和多样化推理路径的数据增强在经验上被验证为有效，极大地缩小了开源LLMs和顶尖专有LLMs之间的差距。本文对数学推理中的数据增强进行了调查，并旨在回答：（1）哪些数据增强策略更有效；（2）增强数据量与模型性能之间的缩放关系如何；（3）数据增强能否激励领域外数学推理任务的泛化？为此，我们通过增加GSM8K查询的复杂性和多样性以及采样多个推理路径，创建了一个新的数据集AugGSM8K。我们通过在AugGSM8K的子集上进行微调获得了一系列LLMs，称为MuggleMath。MuggleMath在GSM8K上取得了显著的最新研究成果（在7B规模上从54%提高到68.4%，在扩放到63.9%到74.0%之间）。

    In math reasoning with large language models (LLMs), fine-tuning data augmentation by query evolution and diverse reasoning paths is empirically verified effective, profoundly narrowing the gap between open-sourced LLMs and cutting-edge proprietary LLMs. In this paper, we conduct an investigation for such data augmentation in math reasoning and are intended to answer: (1) What strategies of data augmentation are more effective; (2) What is the scaling relationship between the amount of augmented data and model performance; and (3) Can data augmentation incentivize generalization to out-of-domain mathematical reasoning tasks? To this end, we create a new dataset, AugGSM8K, by complicating and diversifying the queries from GSM8K and sampling multiple reasoning paths. We obtained a series of LLMs called MuggleMath by fine-tuning on subsets of AugGSM8K. MuggleMath substantially achieves new state-of-the-art on GSM8K (from 54% to 68.4% at the scale of 7B, and from 63.9% to 74.0% at the scal
    
[^128]: 大规模语言模型对监督微调数据组合的影响

    How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition. (arXiv:2310.05492v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05492](http://arxiv.org/abs/2310.05492)

    本研究探讨了大规模语言模型在监督微调过程中，特别是数学推理和代码生成能力方面，数据组合的影响。实验结果显示，较大模型在相同数据量下表现出更好的性能，通过增加微调数据和模型参数，数学推理和代码生成能力得到显著提升。

    

    大规模语言模型（LLMs）具备大量的预训练标记和参数，展现出数学推理、代码生成和指令跟随等能力。这些能力通过监督微调（SFT）进一步增强。开源社区已经研究了针对每种能力的临时SFT，而专有LLMs可以适用于所有能力。因此，研究如何通过SFT解锁多重能力变得重要。在本研究中，我们特别关注SFT过程中数学推理、代码生成和人类对齐能力之间的数据组合。从规模的角度，我们研究了模型能力与各种因素之间的关系，包括数据量、数据组合比例、模型参数和SFT策略。我们的实验发现不同的能力表现出不同的扩展模式，较大的模型通常在相同的数据量下表现出更优异的性能。数学推理和代码生成能力通过微调数据和模型参数的增加而获得显著的性能提升。

    Large language models (LLMs) with enormous pre-training tokens and parameter amounts emerge abilities, including math reasoning, code generation, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). The open-source community has studied on ad-hoc SFT for each ability, while proprietary LLMs are versatile for all abilities. It is important to investigate how to unlock them with multiple abilities via SFT. In this study, we specifically focus on the data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. From a scaling perspective, we investigate the relationship between model abilities and various factors including data amounts, data composition ratio, model parameters, and SFT strategies. Our experiments reveal that different abilities exhibit different scaling patterns, and larger models generally show superior performance with the same amount of data. Mathematical reasoning and code
    
[^129]: 瑞士联邦最高法院裁决的自动化匿名化

    Automatic Anonymization of Swiss Federal Supreme Court Rulings. (arXiv:2310.04632v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.04632](http://arxiv.org/abs/2310.04632)

    该论文介绍了瑞士联邦最高法院裁决的自动化匿名化方法，通过利用大型数据集和领域内预训练模型，结果表明相比现有模型，使用领域内数据进一步提高了F1分数超过5%。这项工作展示了将现有的匿名化方法与机器学习相结合，可以减少人工劳动并增强自动建议的能力。

    

    将法院的裁决公开需要进行适当的匿名化，以保护所有相关方的隐私。瑞士联邦最高法院依靠一种已有的系统，将不同的传统计算方法与人工专家结合起来。在这项工作中，我们利用一个带有要匿名化实体注释的大型数据集，增强了现有的匿名化软件。我们比较了基于BERT的模型和在领域内预训练的模型。结果显示，使用领域内数据来预训练模型相比现有模型进一步提高了F1分数超过5％。我们的工作表明，将现有的匿名化方法（如正则表达式）与机器学习相结合，可以进一步减少人工劳动并增强自动建议的能力。

    Releasing court decisions to the public relies on proper anonymization to protect all involved parties, where necessary. The Swiss Federal Supreme Court relies on an existing system that combines different traditional computational methods with human experts. In this work, we enhance the existing anonymization software using a large dataset annotated with entities to be anonymized. We compared BERT-based models with models pre-trained on in-domain data. Our results show that using in-domain data to pre-train the models further improves the F1-score by more than 5\% compared to existing models. Our work demonstrates that combining existing anonymization methods, such as regular expressions, with machine learning can further reduce manual labor and enhance automatic suggestions.
    
[^130]: TacoGFN: 针对基于结构的药物设计的目标条件GFlowNet

    TacoGFN: Target Conditioned GFlowNet for Structure-Based Drug Design. (arXiv:2310.03223v1 [cs.LG])

    [http://arxiv.org/abs/2310.03223](http://arxiv.org/abs/2310.03223)

    该论文提出了一种名为TacoGFN的目标条件GFlowNet模型，用于自动化生成符合特定蛋白质口袋目标的类药物化合物。该模型通过强化学习框架，鼓励生成具有期望属性的分子，并利用转换器和对接神经网络进行高效的分子空间探索和对接得分预测，以实现较高的结合改善效果。

    

    我们旨在自动化生成符合特定蛋白质口袋目标的类药物化合物。大多数当前方法是对有限数据集中的蛋白质-分子分布进行近似，因此在生成的分子中很难实现与训练数据集相比具有显著结合改善的分子。我们将口袋条件下的分子生成任务定义为强化学习问题，并开发了TacoGFN，一种目标条件下的生成流网络模型。我们的方法明确鼓励生成具有期望属性的分子，而不是适应预先存在的数据分布。为此，我们开发了基于转换器的对接得分预测方法来加快对接得分计算，并提出了TacoGFN来高效地探索分子空间。此外，我们还结合了几轮主动学习，使用对接神经网络对生成的样本进行查询，以改善对接得分预测。这种方法使我们能够准确地探索更多的分子空间。

    We seek to automate the generation of drug-like compounds conditioned to specific protein pocket targets. Most current methods approximate the protein-molecule distribution of a finite dataset and, therefore struggle to generate molecules with significant binding improvement over the training dataset. We instead frame the pocket-conditioned molecular generation task as an RL problem and develop TacoGFN, a target conditional Generative Flow Network model. Our method is explicitly encouraged to generate molecules with desired properties as opposed to fitting on a pre-existing data distribution. To this end, we develop transformer-based docking score prediction to speed up docking score computation and propose TacoGFN to explore molecule space efficiently. Furthermore, we incorporate several rounds of active learning where generated samples are queried using a docking oracle to improve the docking score prediction. This approach allows us to accurately explore as much of the molecule land
    
[^131]: 非线性MPC设计应用于增量ISS系统，以GRU网络为例

    Nonlinear MPC design for incrementally ISS systems with application to GRU networks. (arXiv:2309.16428v1 [eess.SY])

    [http://arxiv.org/abs/2309.16428](http://arxiv.org/abs/2309.16428)

    本研究提出了一种非线性模型预测控制（NMPC）策略，适用于增量ISS系统。通过简化计算终端成分，并明确定义最小预测范围，实现闭环稳定性。将该方法应用于GRU网络，并提供了一种量身定制状态观察器的设计方法。测试结果表明该控制架构具有良好的控制性能和高效的实用性。

    

    本研究针对指数增量输入-状态稳定（ISS）系统设计了一种非线性模型预测控制（NMPC）策略。具体而言，提出了一种新的公式，无需计算终端成分，而是依赖于明确定义的最小预测范围以确保闭环稳定性。该设计方法特别适用于由循环神经网络（RNNs）学习的系统，RNNs以其增强建模能力而闻名，而增量ISS属性可以通过简单的代数条件进行研究。该方法应用于门控循环单元（GRU）网络，并提供了一种具有收敛性保证的量身定制状态观察器的设计方法。所得到的控制架构在基准系统上进行了测试，证明了其良好的控制性能和高效的实用性。

    This brief addresses the design of a Nonlinear Model Predictive Control (NMPC) strategy for exponentially incremental Input-to-State Stable (ISS) systems. In particular, a novel formulation is devised, which does not necessitate the onerous computation of terminal ingredients, but rather relies on the explicit definition of a minimum prediction horizon ensuring closed-loop stability. The designed methodology is particularly suited for the control of systems learned by Recurrent Neural Networks (RNNs), which are known for their enhanced modeling capabilities and for which the incremental ISS properties can be studied thanks to simple algebraic conditions. The approach is applied to Gated Recurrent Unit (GRU) networks, providing also a method for the design of a tailored state observer with convergence guarantees. The resulting control architecture is tested on a benchmark system, demonstrating its good control performances and efficient applicability.
    
[^132]: 基于任务导向的Koopman控制和对比编码器

    Task-Oriented Koopman-Based Control with Contrastive Encoder. (arXiv:2309.16077v1 [cs.RO])

    [http://arxiv.org/abs/2309.16077](http://arxiv.org/abs/2309.16077)

    该论文介绍了一种基于任务导向的Koopman控制方法，利用对比编码器和端到端强化学习来同时学习Koopman潜在嵌入、算子和相关线性控制器。通过优先考虑任务成本作为主要目标，减少了对于明确定义模型的控制器设计的依赖，将Koopman控制扩展到高维、复杂非线性系统，包括基于像素的场景。

    

    我们提出了一种利用端到端强化学习和对比编码器同时学习Koopman潜在嵌入，算子和相关线性控制器的任务导向Koopman控制方法。通过将任务成本作为主要目标进行控制器学习，我们减少了对于一个明确定义的模型的控制器设计的依赖，将Koopman控制扩展到包括基于像素的场景在内的高维、复杂非线性系统中。

    We present task-oriented Koopman-based control that utilizes end-to-end reinforcement learning and contrastive encoder to simultaneously learn the Koopman latent embedding, operator and associated linear controller within an iterative loop. By prioritizing the task cost as main objective for controller learning, we reduce the reliance of controller design on a well-identified model, which extends Koopman control beyond low-dimensional systems to high-dimensional, complex nonlinear systems, including pixel-based scenarios.
    
[^133]: 具有逐层非线性的状态空间模型是带有指数衰减记忆的全能逼近器

    State-space Models with Layer-wise Nonlinearity are Universal Approximators with Exponential Decaying Memory. (arXiv:2309.13414v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.13414](http://arxiv.org/abs/2309.13414)

    本论文证明了堆叠具有逐层非线性激活的状态空间模型足以逼近任何连续的序列到序列关系，并且发现其加强了模型学习复杂序列模式的能力。然而，状态空间模型并不能根本解决指数衰减记忆的问题。

    

    由于其简单有效的网络结构，状态空间模型在序列建模中变得越来越受欢迎。然而，沿时间方向缺乏非线性激活限制了模型的容量。本文证明了堆叠具有逐层非线性激活的状态空间模型足以逼近任何连续的序列到序列关系。我们的研究结果表明，逐层非线性激活的添加提高了模型学习复杂序列模式的能力。与此同时，可以从理论和实证上看到，状态空间模型并不根本解决指数衰减记忆的问题。理论结果经过了数值验证。

    State-space models have gained popularity in sequence modelling due to their simple and efficient network structures. However, the absence of nonlinear activation along the temporal direction limits the model's capacity. In this paper, we prove that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship. Our findings demonstrate that the addition of layer-wise nonlinear activation enhances the model's capacity to learn complex sequence patterns. Meanwhile, it can be seen both theoretically and empirically that the state-space models do not fundamentally resolve the exponential decaying memory issue. Theoretical results are justified by numerical verifications.
    
[^134]: 用于音频生成扩散模型的隐形数字水印

    Invisible Watermarking for Audio Generation Diffusion Models. (arXiv:2309.13166v1 [cs.SD])

    [http://arxiv.org/abs/2309.13166](http://arxiv.org/abs/2309.13166)

    该论文提出了一种用于音频扩散模型的隐形数字水印技术，在保证正常音频生成的同时，还能为模型验证提供保护层，用于鉴别模型所有权和维护其完整性。

    

    在图像领域中，由于其数据生成和转换的能力，扩散模型在各种任务中取得了最先进的性能，因此在图像和音频领域都备受重视。在迅速发展的音频机器学习领域，保护模型的完整性和确立数据的版权至关重要。本文提出了一种首次应用于训练在mel频谱图上的音频扩散模型的水印技术，这对上述挑战提供了一种新颖的方法。我们的模型不仅在正常音频生成方面表现出色，而且还引入了一个不可见的水印触发机制来进行模型验证。这个水印触发器作为一种保护层，能够识别模型的所有者并确保其完整性。通过大量实验证明，不可见的水印触发器在防止未经授权的修改的同时还能保持高效的合法音频传输。

    Diffusion models have gained prominence in the image domain for their capabilities in data generation and transformation, achieving state-of-the-art performance in various tasks in both image and audio domains. In the rapidly evolving field of audio-based machine learning, safeguarding model integrity and establishing data copyright are of paramount importance. This paper presents the first watermarking technique applied to audio diffusion models trained on mel-spectrograms. This offers a novel approach to the aforementioned challenges. Our model excels not only in benign audio generation, but also incorporates an invisible watermarking trigger mechanism for model verification. This watermark trigger serves as a protective layer, enabling the identification of model ownership and ensuring its integrity. Through extensive experiments, we demonstrate that invisible watermark triggers can effectively protect against unauthorized modifications while maintaining high utility in benign audio
    
[^135]: 支撑鲁棒推断的凸框架

    A Convex Framework for Confounding Robust Inference. (arXiv:2309.12450v1 [stat.ML])

    [http://arxiv.org/abs/2309.12450](http://arxiv.org/abs/2309.12450)

    本文提出了一个支撑鲁棒推断的凸框架，通过利用凸规划提供策略价值的精确下界。此外，该方法还可以进行多种扩展，并且具有强理论保证。

    

    我们研究了受未观察到的混淆因素影响的离线上下文强化学习中的策略评估问题。传统的敏感性分析方法常被用来在给定的不确定性集合上估计在最坏混淆情况下的策略价值。然而，现有的工作通常为了可行性而采用一些粗糙的松弛不确定性集合的方法，导致对策略价值的估计过于保守。在本文中，我们提出了一种通用估计器，利用凸规划提供了策略价值的一个较为精确的下界。我们的估计器的广泛适用性使得其能够进行多种扩展，例如基于f-分歧的敏感性分析、基于交叉验证和信息准则的模型选择以及利用上界进行鲁棒策略学习等。此外，我们的估计方法可以通过强对偶性重新表述为经验风险最小化问题，从而利用M技术提供了对所提出估计器的强理论保证。

    We study policy evaluation of offline contextual bandits subject to unobserved confounders. Sensitivity analysis methods are commonly used to estimate the policy value under the worst-case confounding over a given uncertainty set. However, existing work often resorts to some coarse relaxation of the uncertainty set for the sake of tractability, leading to overly conservative estimation of the policy value. In this paper, we propose a general estimator that provides a sharp lower bound of the policy value using convex programming. The generality of our estimator enables various extensions such as sensitivity analysis with f-divergence, model selection with cross validation and information criterion, and robust policy learning with the sharp lower bound. Furthermore, our estimation method can be reformulated as an empirical risk minimization problem thanks to the strong duality, which enables us to provide strong theoretical guarantees of the proposed estimator using techniques of the M-
    
[^136]: 基于物理引导特征提取和领域适应的跨托卡马克破裂预测

    Cross-tokamak Disruption Prediction based on Physics-Guided Feature Extraction and domain adaptation. (arXiv:2309.05361v2 [physics.plasm-ph] UPDATED)

    [http://arxiv.org/abs/2309.05361](http://arxiv.org/abs/2309.05361)

    本文介绍了一种新颖的方法，使用少量放电预测未来托卡马克的破裂，并应用了物理引导特征提取和领域适应算法。这种方法在J-TEXT中取得了出色的破裂预测性能。

    

    未来托卡马克中高昂的数据获取成本和对破裂放电的巨大需求给数据驱动破裂预测模型在破裂预测研究中带来了内在矛盾。在本文中，我们展示了一种新颖的方法，使用只有少量放电来预测未来托卡马克的破裂。第一步是利用对各个托卡马克的诊断信号的现有物理理解来提取物理引导特征，称为物理引导特征提取（PGFE）。第二步是根据一种称为CORrelation ALignment（CORAL）的领域适应算法，将未来托卡马克（目标领域）的少量数据与现有托卡马克（源领域）的大量数据进行对齐。这是第一次尝试将领域适应应用于破裂预测任务中。PGFE已成功应用于J-TEXT以实现出色的破裂预测性能。由于提取了物理引导特征，PGFE还可以减少数据量要求。

    The high acquisition cost and the significant demand for disruptive discharges for data-driven disruption prediction models in future tokamaks pose an inherent contradiction in disruption prediction research. In this paper, we demonstrated a novel approach to predict disruption in a future tokamak using only a few discharges. The first step is to use the existing understanding of physics to extract physics-guided features from the diagnostic signals of each tokamak, called physics-guided feature extraction (PGFE). The second step is to align a few data from the future tokamak (target domain) and a large amount of data from existing tokamak (source domain) based on a domain adaptation algorithm called CORrelation ALignment (CORAL). It is the first attempt at applying domain adaptation in the task of disruption prediction. PGFE has been successfully applied in J-TEXT to predict disruption with excellent performance. PGFE can also reduce the data volume requirements due to extracting the 
    
[^137]: 评估监督学习和大型语言模型在识别中国社交媒体中的认知偏差和自杀风险方面的功效

    Evaluating the Efficacy of Supervised Learning vs Large Language Models for Identifying Cognitive Distortions and Suicidal Risks in Chinese Social Media. (arXiv:2309.03564v1 [cs.CL])

    [http://arxiv.org/abs/2309.03564](http://arxiv.org/abs/2309.03564)

    本研究评估了监督学习和大型语言模型在识别中国社交媒体中的认知偏差和自杀风险方面的功效。结果表明大型语言模型在这两个任务上具有很高的效果。

    

    大型语言模型，特别是类似快速发展的GPT系列，因其广泛的影响力而受到关注。尽管在心理学等医学领域对它们的适用性存在浓厚兴趣，但对真实世界数据的具体探索仍然很少。与此同时，社交媒体平台上的用户越来越多地表达个人情感；在特定的主题下，这些情感通常表现为消极情绪，有时会升级为自杀倾向。及时辨识这样的认知偏差和自杀风险对有效干预和潜在避免严重情况至关重要。我们的研究通过在中国社交媒体平台上进行两个关键任务：自杀风险和认知偏差识别的实验，进入了这个领域。使用监督学习作为基准，我们通过三种不同的策略：零样本、少样本和微调，考察了大型语言模型的功效。

    Large language models, particularly those akin to the rapidly progressing GPT series, are gaining traction for their expansive influence. While there is keen interest in their applicability within medical domains such as psychology, tangible explorations on real-world data remain scant. Concurrently, users on social media platforms are increasingly vocalizing personal sentiments; under specific thematic umbrellas, these sentiments often manifest as negative emotions, sometimes escalating to suicidal inclinations. Timely discernment of such cognitive distortions and suicidal risks is crucial to effectively intervene and potentially avert dire circumstances. Our study ventured into this realm by experimenting on two pivotal tasks: suicidal risk and cognitive distortion identification on Chinese social media platforms. Using supervised learning as a baseline, we examined and contrasted the efficacy of large language models via three distinct strategies: zero-shot, few-shot, and fine-tunin
    
[^138]: YaRN: 大型语言模型的高效上下文窗口扩展方法

    YaRN: Efficient Context Window Extension of Large Language Models. (arXiv:2309.00071v1 [cs.CL])

    [http://arxiv.org/abs/2309.00071](http://arxiv.org/abs/2309.00071)

    YaRN是一种高效的上下文窗口扩展方法，可以在大型语言模型中有效利用和推断比原始预训练允许的上下文长度更长的上下文，同时超越了之前的最新研究成果。

    

    旋转位置嵌入（RoPE）已被证明可以有效地编码transformer-based语言模型中的位置信息。然而，这些模型在超过它们训练的序列长度时无法泛化。我们提出了YaRN（Yet another RoPE extensioN method），一种计算高效的方法来扩展这些模型的上下文窗口，需要的tokens数量和训练步骤少于之前的方法的10倍和2.5倍。使用YaRN，我们展示了LLaMA模型可以有效地利用和推断比原始预训练允许的上下文长度更长的上下文，并且在上下文窗口扩展方面超过了之前的最新研究成果。此外，我们还展示了YaRN具有超越微调数据集有限上下文的能力。我们在https://github.com/jquesnelle/yarn上发布了使用64k和128k上下文窗口进行Fine-tuning的Llama 2 7B/13B的检查点。

    Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset. We publish the checkpoints of Llama 2 7B/13B fine-tuned using YaRN with 64k and 128k context windows at https://github.com/jquesnelle/yarn
    
[^139]: 多元时间序列异常检测: 炫酷算法和有缺陷的评估方法

    Multivariate Time Series Anomaly Detection: Fancy Algorithms and Flawed Evaluation Methodology. (arXiv:2308.13068v1 [cs.LG])

    [http://arxiv.org/abs/2308.13068](http://arxiv.org/abs/2308.13068)

    多元时间序列异常检测是一个研究领域，但目前存在组织不够有序和评估协议有缺陷的问题。文章评估了许多最近算法的性能，并指出了针对多元时间序列异常检测的评估协议存在的问题及如何缓解这些问题的方法。

    

    多元时间序列（MVTS）的异常检测是一个长期存在且具有挑战性的研究课题，近年来吸引了工业界和学术界的大量研究努力。然而，对文献的仔细研究让我们意识到：1）该领域的社区活跃，但并不像计算机视觉（CV）和自然语言处理（NLP）等其他机器学习领域那样组织有序；2）大多数提出的解决方案使用不合适或存在明显缺陷的评估协议进行评估，缺乏科学基础。其中一个非常流行的协议，即所谓的 \pa 协议，是如此有缺陷，以至于随机猜测可以显示系统地优于迄今为止开发的\emph{所有}算法。在本文中，我们使用更健壮的协议对许多最近的算法进行回顾和评估，并讨论在MVTS异常检测的背景下，一个本来很好的协议可能存在的问题以及如何减轻这些问题。我们还对基准数据集表达了关切。

    Multivariate Time Series (MVTS) anomaly detection is a long-standing and challenging research topic that has attracted tremendous research effort from both industry and academia recently. However, a careful study of the literature makes us realize that 1) the community is active but not as organized as other sibling machine learning communities such as Computer Vision (CV) and Natural Language Processing (NLP), and 2) most proposed solutions are evaluated using either inappropriate or highly flawed protocols, with an apparent lack of scientific foundation. So flawed is one very popular protocol, the so-called \pa protocol, that a random guess can be shown to systematically outperform \emph{all} algorithms developed so far. In this paper, we review and evaluate many recent algorithms using more robust protocols and discuss how a normally good protocol may have weaknesses in the context of MVTS anomaly detection and how to mitigate them. We also share our concerns about benchmark dataset
    
[^140]: ALGAN：具有调整的LSTM GAN的时间序列异常检测

    ALGAN: Time Series Anomaly Detection with Adjusted-LSTM GAN. (arXiv:2308.06663v1 [cs.LG])

    [http://arxiv.org/abs/2308.06663](http://arxiv.org/abs/2308.06663)

    该论文提出了一种名为ALGAN的新型GAN模型，通过调整LSTM网络的输出，实现了在无监督设置下对单变量和多变量时间序列数据进行异常检测，并在实验中优于传统方法和其他GAN方法。

    

    时间序列数据中的异常检测是各个领域（如制造业，医学成像和网络安全）中常见的问题，旨在识别偏离正常行为的点。最近，生成对抗网络（GANs）在检测时间序列数据中的异常方面显示出了有效性。GANs的神经网络结构（即生成器和鉴别器）可以显着提高异常检测准确性。本文提出了一种新的GAN模型，名为Adjusted-LSTM GAN（ALGAN），它调整LSTM网络的输出，以提高单变量和多变量时间序列数据的异常检测能力，而且是在无监督设置下进行的。我们在46个真实世界的单变量时间序列数据集和涵盖多个领域的大型多变量数据集上评估了ALGAN的性能。我们的实验表明，ALGAN在时间序列数据的异常检测中优于传统的、基于神经网络的和其他基于GAN的方法。

    Anomaly detection in time series data, to identify points that deviate from normal behaviour, is a common problem in various domains such as manufacturing, medical imaging, and cybersecurity. Recently, Generative Adversarial Networks (GANs) are shown to be effective in detecting anomalies in time series data. The neural network architecture of GANs (i.e. Generator and Discriminator) can significantly improve anomaly detection accuracy. In this paper, we propose a new GAN model, named Adjusted-LSTM GAN (ALGAN), which adjusts the output of an LSTM network for improved anomaly detection in both univariate and multivariate time series data in an unsupervised setting. We evaluate the performance of ALGAN on 46 real-world univariate time series datasets and a large multivariate dataset that spans multiple domains. Our experiments demonstrate that ALGAN outperforms traditional, neural network-based, and other GAN-based methods for anomaly detection in time series data.
    
[^141]: 在预测上下文中的在线学习问题

    Online learning in bandits with predicted context. (arXiv:2307.13916v1 [stat.ML])

    [http://arxiv.org/abs/2307.13916](http://arxiv.org/abs/2307.13916)

    本文研究了一种在预测上下文中的在线学习问题，通过将经典统计学中的测量误差模型推广到在线决策设置中，我们提出了第一个具有次线性后悔的在线算法。

    

    我们考虑在每个时刻，代理只能访问到上下文的一个带噪声的版本以及误差方差（或者这个方差的一个估计）。这一设置受到了许多应用的启发，在这些应用中，用于决策的真实上下文是不可观测的，而只有一个由可能复杂的机器学习算法预测出的上下文。当上下文误差是非衰减的时候，经典的bandit算法无法达到次线性的后悔。我们提出了在这一设置下，第一个具有次线性后悔的在线算法，并与适当的基准进行了比较。关键的思想是将经典统计学中的测量误差模型推广到在线决策设置中，这是非平凡的，因为策略依赖于有噪声的上下文观察。

    We consider the contextual bandit problem where at each time, the agent only has access to a noisy version of the context and the error variance (or an estimator of this variance). This setting is motivated by a wide range of applications where the true context for decision-making is unobserved, and only a prediction of the context by a potentially complex machine learning algorithm is available. When the context error is non-diminishing, classical bandit algorithms fail to achieve sublinear regret. We propose the first online algorithm in this setting with sublinear regret compared to the appropriate benchmark. The key idea is to extend the measurement error model in classical statistics to the online decision-making setting, which is nontrivial due to the policy being dependent on the noisy context observations.
    
[^142]: 用于训练拥有数十亿参数的大型语言模型的优化网络架构

    Optimized Network Architectures for Large Language Model Training with Billions of Parameters. (arXiv:2307.12169v1 [cs.NI])

    [http://arxiv.org/abs/2307.12169](http://arxiv.org/abs/2307.12169)

    本文提出了一种优化的网络架构，用于训练拥有数十亿参数的大型语言模型。这个架构根据语言模型的通信需求，将集群分割成一组通过非阻塞高带宽互连的GPU集合，并通过轨道连接仅连接具有通信需求的GPU，从而降低网络成本高达75％，同时不影响训练性能。

    

    本文挑战了为训练大型语言模型（LLMs）构建任意到任意网络的传统范式。我们展示了LLMs呈现出一种独特的通信模式，在其中，只有小组的GPU需要高带宽的任意到任意通信，以实现接近最优的训练性能。在这些GPU小组之间，通信非常微不足道、稀疏且均匀。我们提出了一个新的网络架构，紧密匹配LLMs的通信需求。我们的架构将集群分割为一组通过非阻塞任意到任意高带宽互连的GPU集合，我们称之为HB域。在HB域之间，网络只连接具有通信需求的GPU。我们将这种网络连接称为“仅轨道连接”，并展示了我们的架构相对于最先进的任意到任意Clos网络可以将网络成本降低高达75％，同时不损害LLM训练的性能。

    This paper challenges the well-established paradigm for building any-to-any networks for training Large Language Models (LLMs). We show that LLMs exhibit a unique communication pattern where only small groups of GPUs require high-bandwidth any-to-any communication within them, to achieve near-optimal training performance. Across these groups of GPUs, the communication is insignificant, sparse, and homogeneous. We propose a new network architecture that closely resembles the communication requirement of LLMs. Our architecture partitions the cluster into sets of GPUs interconnected with non-blocking any-to-any high-bandwidth interconnects that we call HB domains. Across the HB domains, the network only connects GPUs with communication demands. We call this network a "rail-only" connection, and show that our proposed architecture reduces the network cost by up to 75% compared to the state-of-the-art any-to-any Clos networks without compromising the performance of LLM training.
    
[^143]: 解码谜团：在工作记忆的多个方面上对人类和人工智能进行基准测试

    Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory. (arXiv:2307.10768v1 [q-bio.NC])

    [http://arxiv.org/abs/2307.10768](http://arxiv.org/abs/2307.10768)

    本论文介绍了一个全面的工作记忆基准数据集（WorM），通过评估4个功能、3个领域和11个行为和神经特征的WM任务来开发和评估AI WM模型。结果表明，AI模型能够模拟出脑中工作记忆的一些特征，如优势效应和最新性效应，以及专门用于不同领域和功能的工作记忆的神经群集和相关物。

    

    工作记忆（WM）是一种基本的认知过程，它促进了信息的临时存储、整合、操作和检索，在推理和决策任务中起着重要作用。捕捉工作记忆多方面特征的可靠基准数据集对于有效地开发和评估AI工作记忆模型至关重要。在这里，我们介绍了一个全面的工作记忆（WorM）基准数据集，以实现这个目的。WorM包括10个任务和总共100万次试验，评估了WM的4个功能、3个领域和11个行为和神经特征。我们在所有这些任务上共同训练和测试了最先进的循环神经网络和Transformer。我们还包括人类行为基准作为对比的上限。我们的结果表明，AI模型模拟了脑中工作记忆的一些特征，特别是优势效应和最新性效应，以及专门用于不同领域和功能性的工作记忆的神经群集和相关物。

    Working memory (WM), a fundamental cognitive process facilitating the temporary storage, integration, manipulation, and retrieval of information, plays a vital role in reasoning and decision-making tasks. Robust benchmark datasets that capture the multifaceted nature of WM are crucial for the effective development and evaluation of AI WM models. Here, we introduce a comprehensive Working Memory (WorM) benchmark dataset for this purpose. WorM comprises 10 tasks and a total of 1 million trials, assessing 4 functionalities, 3 domains, and 11 behavioral and neural characteristics of WM. We jointly trained and tested state-of-the-art recurrent neural networks and transformers on all these tasks. We also include human behavioral benchmarks as an upper bound for comparison. Our results suggest that AI models replicate some characteristics of WM in the brain, most notably primacy and recency effects, and neural clusters and correlates specialized for different domains and functionalities of WM
    
[^144]: 异构特征子采样的Ridge Ensemble的学习曲线

    Learning Curves for Heterogeneous Feature-Subsampled Ridge Ensembles. (arXiv:2307.03176v1 [stat.ML])

    [http://arxiv.org/abs/2307.03176](http://arxiv.org/abs/2307.03176)

    通过引入异构特征集成，本文研究了在线性回归设置中构建的岭回归集成的学习曲线，结果表明异构特征集成具有较好的性能。

    

    特征包装是一种旨在通过在随机子样本或特征投影上训练估计器来减少预测方差的成熟集成方法。通常，集成选择是同质的，即估计器可用的特征维数在整个集成中是均匀的。在这里，我们介绍了异构特征集成方法，其中的估计器基于变动的特征维数，并研究其在线性回归设置中的性能。我们研究了一个线性预测器的集成，每个预测器使用部分可用特征进行岭回归拟合。我们允许这些子集中包含的特征数量有所变化。利用统计物理中的复制技巧，我们推导了具有确定性线性掩模的岭回归集成的学习曲线。对于具有各向同性特征噪声的等相相关数据，我们得到了学习曲线的显式表达式。利用这些推导表达式，我们研究了集成在不同特征维数下的性能。

    Feature bagging is a well-established ensembling method which aims to reduce prediction variance by training estimators in an ensemble on random subsamples or projections of features. Typically, ensembles are chosen to be homogeneous, in the sense the the number of feature dimensions available to an estimator is uniform across the ensemble. Here, we introduce heterogeneous feature ensembling, with estimators built on varying number of feature dimensions, and consider its performance in a linear regression setting. We study an ensemble of linear predictors, each fit using ridge regression on a subset of the available features. We allow the number of features included in these subsets to vary. Using the replica trick from statistical physics, we derive learning curves for ridge ensembles with deterministic linear masks. We obtain explicit expressions for the learning curves in the case of equicorrelated data with an isotropic feature noise. Using the derived expressions, we investigate t
    
[^145]: 元学习对抗波段算法

    Meta-Learning Adversarial Bandit Algorithms. (arXiv:2307.02295v1 [cs.LG])

    [http://arxiv.org/abs/2307.02295](http://arxiv.org/abs/2307.02295)

    本论文研究了具有波段反馈的在线元学习，并设计了用于多臂赌博机和赌博线性优化的元算法。对于多臂赌博机，算法使用了Tsallis-熵的泛化Exp3，并且任务平均遗憾会随着最优解的熵的减小而改善。对于赌博线性优化，算法使用了自协调障碍正则化器初始化和调整在线镜像下降，并且任务平均遗憾与动作空间相关的度量直接变化。

    

    我们研究具有波段反馈的在线元学习，目标是在多个任务之间改善性能，如果它们根据某个自然的相似性度量是相似的。作为针对敌对的在线部分信息设置的首个目标，我们设计了元算法，将外层学习器结合在一起，同时为两种重要情况调整内部学习器的初始化和其他超参数：多臂赌博机（MAB）和赌博线性优化（BLO）。对于MAB，元学习器使用Tsallis-熵的泛化Exp3的初始化和设置超参数，如果后见之高峰的熵小，则任务平均遗憾改善。对于BLO，我们学会了使用自协调障碍正则化器初始化和调整在线镜像下降（OMD），表明任务平均遗憾与其引起的动作空间相关的度量直接变化。我们的保证基于证明无正规化跟随者与两个…

    We study online meta-learning with bandit feedback, with the goal of improving performance across multiple tasks if they are similar according to some natural similarity measure. As the first to target the adversarial online-within-online partial-information setting, we design meta-algorithms that combine outer learners to simultaneously tune the initialization and other hyperparameters of an inner learner for two important cases: multi-armed bandits (MAB) and bandit linear optimization (BLO). For MAB, the meta-learners initialize and set hyperparameters of the Tsallis-entropy generalization of Exp3, with the task-averaged regret improving if the entropy of the optima-in-hindsight is small. For BLO, we learn to initialize and tune online mirror descent (OMD) with self-concordant barrier regularizers, showing that task-averaged regret varies directly with an action space-dependent measure they induce. Our guarantees rely on proving that unregularized follow-the-leader combined with two 
    
[^146]: 图神经网络在身份效应学习中的泛化限制

    Generalization Limits of Graph Neural Networks in Identity Effects Learning. (arXiv:2307.00134v1 [cs.LG])

    [http://arxiv.org/abs/2307.00134](http://arxiv.org/abs/2307.00134)

    本研究在学习身份效应的背景下，分析了图神经网络在泛化属性和基本限制方面的新性质，以及在两个字母的单词案例中的具体应用。

    

    图神经网络在各种图领域的数据驱动学习中已经成为一个强有力的工具。它们通常基于消息传递机制，并且由于其与Weisfeiler-Lehman(WL)图同构测试紧密相连的直观表述而越来越受到欢迎，从表达能力上讲，它们已被证明与WL测试等价。在本研究中，我们在学习所谓的身份效应（即确定一个对象是否由两个相同的组件组成）的背景下，建立了GNN在泛化属性和基本限制方面的新性质。我们的研究是出于理解GNN在执行简单认知任务时的能力的需求，可能在计算语言学和化学领域具有潜在应用。我们分析了两个案例研究：（i）两个字母的单词，我们展示了通过随机梯度下降训练的GNN在利用正交时无法对未见字母进行泛化的情况。

    Graph Neural Networks (GNNs) have emerged as a powerful tool for data-driven learning on various graph domains. They are usually based on a message-passing mechanism and have gained increasing popularity for their intuitive formulation, which is closely linked to the Weisfeiler-Lehman (WL) test for graph isomorphism to which they have been proven equivalent in terms of expressive power. In this work, we establish new generalization properties and fundamental limits of GNNs in the context of learning so-called identity effects, i.e., the task of determining whether an object is composed of two identical components or not. Our study is motivated by the need to understand the capabilities of GNNs when performing simple cognitive tasks, with potential applications in computational linguistics and chemistry. We analyze two case studies: (i) two-letters words, for which we show that GNNs trained via stochastic gradient descent are unable to generalize to unseen letters when utilizing orthogo
    
[^147]: 用基于梯度的优化方法解决核岭回归问题

    Solving Kernel Ridge Regression with Gradient-Based Optimization Methods. (arXiv:2306.16838v1 [stat.ML])

    [http://arxiv.org/abs/2306.16838](http://arxiv.org/abs/2306.16838)

    本研究提出了一种新的方法来解决核岭回归问题，通过等价的目标函数形式和基于梯度的优化方法，我们不仅可以使用其他惩罚方法，还能够从梯度下降的角度研究核岭回归。通过提前停止的正则化，我们推导出了一个闭合解，即核梯度流（KGF），并证明了KGF和KRR之间的差异。我们还将KRR泛化，使用$\ell_1$和$\ell_\infty$惩罚方法，并发现使用这些方法得到的解与前向分步回归和符号梯度下降结合提前停止得到的解非常相似。因此，我们减少了计算复杂度重的近端梯度下降算法的需求。

    

    核岭回归（KRR）是线性岭回归的非线性推广。在这里，我们引入了KRR目标函数的等价形式，为使用其他惩罚方法和从梯度下降的角度研究核岭回归打开了可能。通过连续时间的视角，我们推导出了一个闭合解——核梯度流（KGF），通过提前停止的正则化，让我们能够在KGF和KRR之间理论上界定差异。我们用$\ell_1$和$\ell_\infty$惩罚方法将KRR泛化，并利用类似KGF和KRR之间的相似性，使用这些惩罚方法得到的解与使用前向分步回归（也称为坐标下降）和符号梯度下降结合提前停止得到的解非常相似。因此，减少了计算复杂度重的近端梯度下降算法的需求。

    Kernel ridge regression, KRR, is a non-linear generalization of linear ridge regression. Here, we introduce an equivalent formulation of the objective function of KRR, opening up both for using other penalties than the ridge penalty and for studying kernel ridge regression from the perspective of gradient descent. Using a continuous-time perspective, we derive a closed-form solution, kernel gradient flow, KGF, with regularization through early stopping, which allows us to theoretically bound the differences between KGF and KRR. We generalize KRR by replacing the ridge penalty with the $\ell_1$ and $\ell_\infty$ penalties and utilize the fact that analogously to the similarities between KGF and KRR, the solutions obtained when using these penalties are very similar to those obtained from forward stagewise regression (also known as coordinate descent) and sign gradient descent in combination with early stopping. Thus the need for computationally heavy proximal gradient descent algorithms
    
[^148]: 何去何从：深度学习加速的数字硬件视角

    To Spike or Not To Spike: A Digital Hardware Perspective on Deep Learning Acceleration. (arXiv:2306.15749v1 [cs.NE])

    [http://arxiv.org/abs/2306.15749](http://arxiv.org/abs/2306.15749)

    神经形态计算旨在通过仿真脑部操作来提高深度学习模型的效率，但是在SNNs的高效硬件后端设计上仍需进一步研究。

    

    随着深度学习模型规模的增加，它们在涵盖计算机视觉到自然语言处理等领域变得越来越有竞争力；然而，这是以效率为代价的，因为它们需要越来越多的内存和计算能力。生物脑的功耗效率超过任何大规模深度学习（DL）模型；因此，神经形态计算试图模仿脑部操作，例如基于脉冲的信息处理，以提高DL模型的效率。尽管脑部有诸如高效的信息传输、密集的神经元连接和计算与存储的共同位置等优势，但可用的生物基底严重限制了生物大脑的进化。电子硬件没有相同的约束；因此，虽然建模脉冲神经网络（SNNs）可能揭示了一个谜题的一部分，但对于SNNs的高效硬件后端设计需要进一步研究。

    As deep learning models scale, they become increasingly competitive from domains spanning computer vision to natural language processing; however, this happens at the expense of efficiency since they require increasingly more memory and computing power. The power efficiency of the biological brain outperforms the one of any large-scale deep learning (DL) model; thus, neuromorphic computing tries to mimic the brain operations, such as spike-based information processing, to improve the efficiency of DL models. Despite the benefits of the brain, such as efficient information transmission, dense neuronal interconnects, and the co-location of computation and memory, the available biological substrate has severely constrained the evolution of biological brains. Electronic hardware does not have the same constraints; therefore, while modeling spiking neural networks (SNNs) might uncover one piece of the puzzle, the design of efficient hardware backends for SNNs needs further investigation, po
    
[^149]: 重启采样以提高生成过程

    Restart Sampling for Improving Generative Processes. (arXiv:2306.14878v1 [cs.LG])

    [http://arxiv.org/abs/2306.14878](http://arxiv.org/abs/2306.14878)

    本文提出了一种名为“重启”的新型采样算法，以更好地平衡离散化误差和收缩，可以优化生成过程中的采样速度和样本质量。

    

    生成过程中解决微分方程的过程，如扩散模型，需要平衡速度和质量。基于ODE的采样器速度快但性能平稳，而基于SDE的采样器提供更高的样本质量但需要更长的采样时间。我们将这种差异归因于采样误差：ODE采样器涉及更小的离散化误差，而SDE的随机性会使累积误差缩小。基于这些发现，我们提出了一种名为重启的新型采样算法，以更好地平衡离散化误差和收缩。该采样方法在额外前向步骤中交替添加大量噪声和严格遵循后向ODE。经验证，重启采样器在速度和准确性方面均优于先前的SDE和ODE采样器。在CIFAR-10/ImageNet $64 \times 64$上，重启不仅优于先前的最佳SDE结果，而且加快了采样速度，分别为10倍/2倍。此外，它在进行图像生成时还提供了更好的样本质量。

    Generative processes that involve solving differential equations, such as diffusion models, frequently necessitate balancing speed and quality. ODE-based samplers are fast but plateau in performance while SDE-based samplers deliver higher sample quality at the cost of increased sampling time. We attribute this difference to sampling errors: ODE-samplers involve smaller discretization errors while stochasticity in SDE contracts accumulated errors. Based on these findings, we propose a novel sampling algorithm called Restart in order to better balance discretization errors and contraction. The sampling method alternates between adding substantial noise in additional forward steps and strictly following a backward ODE. Empirically, Restart sampler surpasses previous SDE and ODE samplers in both speed and accuracy. Restart not only outperforms the previous best SDE results, but also accelerates the sampling speed by 10-fold / 2-fold on CIFAR-10 / ImageNet $64 \times 64$. In addition, it at
    
[^150]: 没有中间监督的神经算法推理

    Neural Algorithmic Reasoning Without Intermediate Supervision. (arXiv:2306.13411v1 [cs.LG])

    [http://arxiv.org/abs/2306.13411](http://arxiv.org/abs/2306.13411)

    神经算法推理最近的变革点是逐步学习算法，但我们提出了一种不需要中间监督的新方法，并在不牺牲性能的情况下规范模型的中间计算。

    

    神经算法推理是机器学习中的新兴领域，侧重于构建能够模仿经典算法（如排序、最短路径等）执行的模型。其中一个主要挑战是学习能够推广到超出分布数据且输入规模显著更大的算法。最近针对这个问题的工作表明，逐步学习算法具有优势，使模型能够访问原始算法的所有中间步骤。在这项工作中，我们不使用中间监督专注于仅从输入输出对学习神经算法推理。我们提出了简单但有效的结构改进，并构建了一种自我监督目标，可以规范模型的中间计算，而不需要访问算法轨迹。我们证明了我们的方法在来自CLRS算法的任务上与其轨迹监督对应物相当竞争力。

    Neural Algorithmic Reasoning is an emerging area of machine learning focusing on building models which can imitate the execution of classic algorithms, such as sorting, shortest paths, etc. One of the main challenges is to learn algorithms that are able to generalize to out-of-distribution data, in particular with significantly larger input sizes. Recent work on this problem has demonstrated the advantages of learning algorithms step-by-step, giving models access to all intermediate steps of the original algorithm. In this work, we instead focus on learning neural algorithmic reasoning only from the input-output pairs without appealing to the intermediate supervision. We propose simple but effective architectural improvements and also build a self-supervised objective that can regularise intermediate computations of the model without access to the algorithm trajectory. We demonstrate that our approach is competitive to its trajectory-supervised counterpart on tasks from the CLRS Algori
    
[^151]: 多样社区数据用于数据隐私算法基准测试

    Diverse Community Data for Benchmarking Data Privacy Algorithms. (arXiv:2306.13216v1 [cs.CR])

    [http://arxiv.org/abs/2306.13216](http://arxiv.org/abs/2306.13216)

    多样社区数据摘要旨在为隐私保护机器学习研究提供真实、多样和复杂的基准数据，以解决合成数据的偏差和隐私问题。

    

    多样社区数据是美国国家标准和技术研究所（NIST）计划的核心，旨在增强对表格数据去识别技术（如合成数据）的理解。合成数据是民主化大数据利益的一项雄心勃勃的尝试；它使用生成模型重新创建敏感个人数据，以便公开发布。然而，它容易受到影响其他机器学习应用程序的偏差和隐私问题的影响，甚至可能放大这些问题。当去识别数据分布引入偏差或工件，或泄漏敏感信息时，它们会将这些问题传播到下游应用。此外，真实世界的调查条件（如多样子群、异质非有序数据空间和特征之间的复杂依赖关系）对合成数据算法提出了具体挑战。这些观察结果促使需要真实、多样和复杂的基准数据来支持隐私保护的机器学习研究，而多样社区数据摘要旨在解决这些挑战。

    The Diverse Communities Data Excerpts are the core of a National Institute of Standards and Technology (NIST) program to strengthen understanding of tabular data deidentification technologies such as synthetic data. Synthetic data is an ambitious attempt to democratize the benefits of big data; it uses generative models to recreate sensitive personal data with new records for public release. However, it is vulnerable to the same bias and privacy issues that impact other machine learning applications, and can even amplify those issues. When deidentified data distributions introduce bias or artifacts, or leak sensitive information, they propagate these problems to downstream applications. Furthermore, real-world survey conditions such as diverse subpopulations, heterogeneous non-ordinal data spaces, and complex dependencies between features pose specific challenges for synthetic data algorithms. These observations motivate the need for real, diverse, and complex benchmark data to support
    
[^152]: OpenGSL: 一项针对图结构学习的综合基准测试研究

    OpenGSL: A Comprehensive Benchmark for Graph Structure Learning. (arXiv:2306.10280v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10280](http://arxiv.org/abs/2306.10280)

    OpenGSL是第一个针对图结构学习的综合基准测试，旨在解决GSL领域中由于实验协议不一致而导致的进展不明确的问题。

    

    图神经网络(GNNs)已成为图表示学习的事实标准，因为它们能有效地整合图的拓扑结构和节点属性。然而，由于图的复杂和依赖形成过程导致的节点连接的固有次优性质，在对其进行建模方面存在着重大挑战。为了解决这个问题，近年来图结构学习(GSL)作为一种数据中心化学习方法已经引起了广泛关注。GSL的核心概念是同时优化图结构和对应的GNN模型。尽管提出了许多GSL方法，但由于实验协议不一致，包括数据集的变化、数据处理技术和分割策略的差异，该领域的进展仍不清楚。在本文中，我们介绍了OpenGSL，这是第一个针对GSL的综合基准测试，并旨在填补这一空白。OpenGSL提供了一个公平的比较平台，使研究人员能够在统一的设置下评估不同的GSL方法。

    Graph Neural Networks (GNNs) have emerged as the de facto standard for representation learning on graphs, owing to their ability to effectively integrate graph topology and node attributes. However, the inherent suboptimal nature of node connections, resulting from the complex and contingent formation process of graphs, presents significant challenges in modeling them effectively. To tackle this issue, Graph Structure Learning (GSL), a family of data-centric learning approaches, has garnered substantial attention in recent years. The core concept behind GSL is to jointly optimize the graph structure and the corresponding GNN models. Despite the proposal of numerous GSL methods, the progress in this field remains unclear due to inconsistent experimental protocols, including variations in datasets, data processing techniques, and splitting strategies. In this paper, we introduce OpenGSL, the first comprehensive benchmark for GSL, aimed at addressing this gap. OpenGSL enables a fair compa
    
[^153]: 构建Schrödinger的桥梁：一种连续熵最优传输的基准测试

    Building the Bridge of Schr\"odinger: A Continuous Entropic Optimal Transport Benchmark. (arXiv:2306.10161v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10161](http://arxiv.org/abs/2306.10161)

    本文提出了一种用于构建连续基准分布的方法，该分布具有已知的熵最优传输和Schrödinger桥解。这填补了在这一研究领域参数选择方面的空白，并为研究人员提供了测试传输模型性能的一种方法。

    

    在过去几年中，人们在开发用于Schrödinger桥问题的神经求解器方面取得了重大进展，并将其应用于生成模型。这一新的研究领域与在实践上表现良好的扩散模型和在理论上有基础的熵最优传输相互关联。尽管如此，该领域缺乏非平凡测试，无法让研究人员了解这些方法在解决SB或其等效连续EOT问题上的表现如何。我们填补了这一空白，并提出了一种新的方法来创建一对已知地面真值OT解的概率分布。我们的方法是通用的，适用于广泛的OT公式，尤其是涵盖了与SB等效的EOT（我们研究的主要兴趣）。这一发展使我们能够在高维空间（如图像空间）上创建具有已知EOT和SB解的连续基准分布。

    Over the last several years, there has been significant progress in developing neural solvers for the Schr\"odinger Bridge (SB) problem and applying them to generative modelling. This new research field is justifiably fruitful as it is interconnected with the practically well-performing diffusion models and theoretically grounded entropic optimal transport (EOT). Still, the area lacks non-trivial tests allowing a researcher to understand how well the methods solve SB or its equivalent continuous EOT problem. We fill this gap and propose a novel way to create pairs of probability distributions for which the ground truth OT solution is known by the construction. Our methodology is generic and works for a wide range of OT formulations, in particular, it covers the EOT which is equivalent to SB (the main interest of our study). This development allows us to create continuous benchmark distributions with the known EOT and SB solutions on high-dimensional spaces such as spaces of images. As 
    
[^154]: 针对潜在混淆下的因果结果的更紧密预测区间

    Tighter Prediction Intervals for Causal Outcomes Under Hidden Confounding. (arXiv:2306.09520v1 [cs.LG])

    [http://arxiv.org/abs/2306.09520](http://arxiv.org/abs/2306.09520)

    本文提出了一种名为Caus-Modens的算法，通过调制集合来描述因果结果区间，相比符合性预测方法，能够在实践中给出更紧密的结果区间。

    

    在存在隐藏混淆因素的情况下进行确切个体治疗结果的因果推断很少可能。因此，最近的研究改进了符合性预测方法，以产生结果区间。不幸的是，这类方法往往过于保守，有时会给出无信息量的区间。我们介绍了一种另类方法Caus-Modens，用于通过调制集合来描述因果结果区间。受到贝叶斯统计和集成不确定性量化的启发，Caus-Modens在实践中给出更紧密的结果区间，并通过三个分离基准测试的必要区间大小来实现足够的覆盖率。最后一个基准是使用未知但可探明的基础事实开展观察实验的GPT-4的新型用途。

    Causal inference of exact individual treatment outcomes in the presence of hidden confounders is rarely possible. Instead, recent work has adapted conformal prediction to produce outcome intervals. Unfortunately this family of methods tends to be overly conservative, sometimes giving uninformative intervals. We introduce an alternative approach termed Caus-Modens, for characterizing causal outcome intervals by modulated ensembles. Motivated from Bayesian statistics and ensembled uncertainty quantification, Caus-Modens gives tighter outcome intervals in practice, measured by the necessary interval size to achieve sufficient coverage on three separate benchmarks. The last benchmark is a novel usage of GPT-4 for observational experiments with unknown but probeable ground truth.
    
[^155]: ClimSim：用于在混合多尺度气候模拟器中训练高分辨率物理仿真器的开源大规模数据集

    ClimSim: An open large-scale dataset for training high-resolution physics emulators in hybrid multi-scale climate simulators. (arXiv:2306.08754v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.08754](http://arxiv.org/abs/2306.08754)

    这是一个用于训练高分辨率物理仿真器的气候数据集。该数据集包含了5.7亿个多变量输入和输出矢量对，用于隔离本地嵌套的高分辨率、高保真度物理的影响。

    

    现代气候预测由于计算限制缺乏足够的空间和时间分辨率。一个后果是对关键过程（如暴风雨）的预测不准确和不精确。将物理和机器学习（ML）相结合的混合模式引入了新一代更高保真度的气候模拟器，通过将计算密集型、短、高分辨率的模拟委托给ML仿真器，可以避免摩尔定律问题。然而，这种混合的ML-物理仿真方法需要领域特定的处理，并且由于缺乏培训数据和相关的易于使用的工作流程，一直无法访问ML专家。我们提出了 ClimSim，这是迄今为止为混合ML-物理研究而设计的最大数据集。它由气候科学家和ML研究人员联合开发的多尺度气候模拟组成，包括57亿个多变量输入和输出矢量对，隔离了本地嵌套的高分辨率和高保真度物理学的影响。

    Modern climate projections lack adequate spatial and temporal resolution due to computational constraints. A consequence is inaccurate and imprecise prediction of critical processes such as storms. Hybrid methods that combine physics with machine learning (ML) have introduced a new generation of higher fidelity climate simulators that can sidestep Moore's Law by outsourcing compute-hungry, short, high-resolution simulations to ML emulators. However, this hybrid ML-physics simulation approach requires domain-specific treatment and has been inaccessible to ML experts because of lack of training data and relevant, easy-to-use workflows. We present ClimSim, the largest-ever dataset designed for hybrid ML-physics research. It comprises multi-scale climate simulations, developed by a consortium of climate scientists and ML researchers. It consists of 5.7 billion pairs of multivariate input and output vectors that isolate the influence of locally-nested, high-resolution, high-fidelity physics
    
[^156]: 特征级自监督学习方法FLSL

    FLSL: Feature-level Self-supervised Learning. (arXiv:2306.06203v1 [cs.LG])

    [http://arxiv.org/abs/2306.06203](http://arxiv.org/abs/2306.06203)

    本文提出FLSL方法，采用Transformer进行联合嵌入和聚类，适合于内视图和外视图特征聚类。实验证明该方法在语义类簇表达方面取得显著改进。

    

    当前的自监督学习方法（如SimCLR、DINO、VICReg、MOCOv3）主要针对实例级别的表示，不适用于密集预测任务，例如对象检测和分割。本文针对这个问题，首次展示了Vision Transformers（ViT）的基础均值漂移聚类过程能够良好地与自然图像语义（例如物体和场景）对齐。通过采用Transformer进行联合嵌入和聚类，我们提出了一种两级特征聚类的自监督学习方法，称为特征级自监督学习（FLSL）。我们提出了FLSL问题的正式定义，并从均值漂移和k-means的角度构建目标。实验证明，FLSL促进了显著的语义类簇表示，并学习了一种适合于内视图和外视图特征聚类的嵌入方案。FLSL的运用取得了显著改进。

    Current self-supervised learning (SSL) methods (e.g., SimCLR, DINO, VICReg, MOCOv3) target primarily on representations at instance level and do not generalize well to dense prediction tasks, such as object detection and segmentation. Towards aligning SSL with dense predictions, this paper demonstrates for the first time the underlying mean-shift clustering process of Vision Transformers (ViT), which aligns well with natural image semantics (e.g., a world of objects and stuffs). By employing transformer for joint embedding and clustering, we propose a two-level feature clustering SSL method, coined Feature-Level Self-supervised Learning (FLSL). We present the formal definition of the FLSL problem and construct the objectives from the mean-shift and k-means perspectives. We show that FLSL promotes remarkable semantic cluster representations and learns an embedding scheme amenable to intra-view and inter-view feature clustering. Experiments show that FLSL yields significant improvements 
    
[^157]: PoET: 一种将蛋白质家族看作序列的生成模型

    PoET: A generative model of protein families as sequences-of-sequences. (arXiv:2306.06156v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.06156](http://arxiv.org/abs/2306.06156)

    PoET是一个模型，能够生成任何蛋白质家族的一系列相关蛋白质序列，可以用作检索增强语言模型生成和评分任何修改

    

    生成式蛋白质语言模型是设计具有所需功能的新蛋白质的自然方法。然而，当前的模型要么难以指导其生成特定类型的蛋白质，要么必须在特定类型的蛋白质家族的大型多重序列比对上进行训练，这使得它们无法从家族之间的迁移学习中受益。为了解决这个问题，我们提出了蛋白质进化变换器（PoET），这是一种全蛋白质家族自回归生成模型，学习在数千万个天然蛋白质序列簇之间生成一系列相关的蛋白质序列。PoET可以作为一个检索增强语言模型，在任何感兴趣的蛋白质家族条件下生成和评分任意修改，而且可以从短序列长度进行外推，在小家族中也能很好地泛化。这是通过独特的Transformer层实现的；我们模拟了令牌s

    Generative protein language models are a natural way to design new proteins with desired functions. However, current models are either difficult to direct to produce a protein from a specific family of interest, or must be trained on a large multiple sequence alignment (MSA) from the specific family of interest, making them unable to benefit from transfer learning across families. To address this, we propose $\textbf{P}$r$\textbf{o}$tein $\textbf{E}$volutionary $\textbf{T}$ransformer (PoET), an autoregressive generative model of whole protein families that learns to generate sets of related proteins as sequences-of-sequences across tens of millions of natural protein sequence clusters. PoET can be used as a retrieval-augmented language model to generate and score arbitrary modifications conditioned on any protein family of interest, and can extrapolate from short context lengths to generalize well even for small families. This is enabled by a unique Transformer layer; we model tokens s
    
[^158]: 鲁棒性AI生成文本检测的内部维度估计

    Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts. (arXiv:2306.04723v1 [cs.CL])

    [http://arxiv.org/abs/2306.04723](http://arxiv.org/abs/2306.04723)

    本文提出了衡量文本内部维度的方法，应用于鲁棒性AI生成文本的检测，展示了人类文本与AI生成文本在内部维度上的差异。

    

    快速提高的AI生成内容的质量使得很难区分人类和AI生成的文本，这可能会对社会产生不良影响。因此，研究人类文本的不变属性变得越来越重要。本文提出了一种人类文本的不变特征，即给定文本样本嵌入集合下的流形的内部维度。我们展示了自然语言流畅文本的平均内部维度在几个基于字母的语言中约为 $9$，而中文约为 $7$，而每种语言的AI生成文本的平均内部维度较低，差约 $1.5$，并且有明显的统计分离。

    Rapidly increasing quality of AI-generated content makes it difficult to distinguish between human and AI-generated texts, which may lead to undesirable consequences for society. Therefore, it becomes increasingly important to study the properties of human texts that are invariant over text domains and various proficiency of human writers, can be easily calculated for any language, and can robustly separate natural and AI-generated texts regardless of the generation model and sampling method. In this work, we propose such an invariant of human texts, namely the intrinsic dimensionality of the manifold underlying the set of embeddings of a given text sample. We show that the average intrinsic dimensionality of fluent texts in natural language is hovering around the value $9$ for several alphabet-based languages and around $7$ for Chinese, while the average intrinsic dimensionality of AI-generated texts for each language is $\approx 1.5$ lower, with a clear statistical separation between
    
[^159]: 优化输运模型的分布鲁棒性

    Optimal Transport Model Distributional Robustness. (arXiv:2306.04178v1 [cs.LG])

    [http://arxiv.org/abs/2306.04178](http://arxiv.org/abs/2306.04178)

    本文提出了一种优化输运模型的分布鲁棒性框架，能够显著提高深度学习模型的鲁棒性，可灵活地将锐度感知纳入到单个模型、集成模型和贝叶斯神经网络的训练中。

    

    分布鲁棒性是一种有希望的框架，用于训练深度学习模型，使其对抗性例子和数据分布变化的影响更小。先前的工作主要集中在利用数据空间的分布鲁棒性上。在本文中，我们探讨了一种基于最优输运的模型空间分布鲁棒性框架。具体而言，我们研究了在给定中心模型分布的Wasserstein球中的模型分布，该模型分布最大化了损失。我们开发出了理论，允许我们学习最佳的鲁棒中心模型分布。有趣的是，通过我们开发的理论，我们可以通过考虑特定形式的中心模型分布（如单个模型上的Dirac delta分布，多个模型上的均匀分布和一般的贝叶斯神经网络）来灵活地将锐度感知的概念纳入到单个模型、集成模型和贝叶斯神经网络的训练中。此外，我们证明了所提出的框架显著提高了在各种基准数据集上的深度学习模型的分布鲁棒性。

    Distributional robustness is a promising framework for training deep learning models that are less vulnerable to adversarial examples and data distribution shifts. Previous works have mainly focused on exploiting distributional robustness in data space. In this work, we explore an optimal transport-based distributional robustness framework on model spaces. Specifically, we examine a model distribution in a Wasserstein ball of a given center model distribution that maximizes the loss. We have developed theories that allow us to learn the optimal robust center model distribution. Interestingly, through our developed theories, we can flexibly incorporate the concept of sharpness awareness into training a single model, ensemble models, and Bayesian Neural Networks by considering specific forms of the center model distribution, such as a Dirac delta distribution over a single model, a uniform distribution over several models, and a general Bayesian Neural Network. Furthermore, we demonstrat
    
[^160]: 抗干扰约束学习

    Resilient Constrained Learning. (arXiv:2306.02426v1 [cs.LG])

    [http://arxiv.org/abs/2306.02426](http://arxiv.org/abs/2306.02426)

    本论文提出了一个名为“抗干扰约束学习”的方法来解决在部署机器学习解决方案时需要满足除了准确性以外的多个要求，并以平衡从放宽中获得的性能增益与用户定义的放宽成本之间的关系的方式放松学习约束。

    

    在部署机器学习解决方案时，除了准确性之外，它们必须满足多个要求，如公平性、鲁棒性或安全性。这些要求可以通过使用惩罚来隐式地施加，或者通过基于Lagrangian对偶的约束优化方法来显式地施加。无论哪种方式，指定要求都受到妥协和有限的有关数据的先前知识的影响。此外，它们对性能的影响通常只能通过实际解决学习问题来评估。本文提出了一种约束学习方法，该方法在同时解决学习任务的同时调整要求。为此，它以平衡从放宽中获得的性能增益与用户定义的放宽成本之间的关系的方式放松了学习约束。我们将此方法称为具有弹性的约束学习，这是对用于描述生态系统的术语的一种借鉴。

    When deploying machine learning solutions, they must satisfy multiple requirements beyond accuracy, such as fairness, robustness, or safety. These requirements are imposed during training either implicitly, using penalties, or explicitly, using constrained optimization methods based on Lagrangian duality. Either way, specifying requirements is hindered by the presence of compromises and limited prior knowledge about the data. Furthermore, their impact on performance can often only be evaluated by actually solving the learning problem. This paper presents a constrained learning approach that adapts the requirements while simultaneously solving the learning task. To do so, it relaxes the learning constraints in a way that contemplates how much they affect the task at hand by balancing the performance gains obtained from the relaxation against a user-defined cost of that relaxation. We call this approach resilient constrained learning after the term used to describe ecological systems tha
    
[^161]: 在图形的超出分布泛化中学习标签和环境因果独立性

    Joint Learning of Label and Environment Causal Independence for Graph Out-of-Distribution Generalization. (arXiv:2306.01103v1 [cs.LG])

    [http://arxiv.org/abs/2306.01103](http://arxiv.org/abs/2306.01103)

    本文提出了一种考虑标签和环境因果独立性的方法来解决图形超出分布（OOD）泛化问题，通过敌对训练策略来联合优化属性以获得有效结果，实验证明LECI显着优于之前的方法。

    

    我们解决了图形的超出分布（OOD）泛化问题。现有的图形OOD算法要么依赖于受限的假设，要么无法利用训练数据中的环境信息。在这项工作中，我们提出同时纳入标签和环境因果独立（LECI），充分利用标签和环境信息，从而解决之前的方法在识别因果和不变子图时面临的挑战。我们进一步开发了一种敌对训练策略，以联合优化这两个属性，用于具有理论保证的导致子图发现。广泛的实验和分析表明，LECI在合成和真实数据集上都显着优于之前的方法，将LECI确立为图形OOD泛化的实用有效解决方案。

    We tackle the problem of graph out-of-distribution (OOD) generalization. Existing graph OOD algorithms either rely on restricted assumptions or fail to exploit environment information in training data. In this work, we propose to simultaneously incorporate label and environment causal independence (LECI) to fully make use of label and environment information, thereby addressing the challenges faced by prior methods on identifying causal and invariant subgraphs. We further develop an adversarial training strategy to jointly optimize these two properties for casual subgraph discovery with theoretical guarantees. Extensive experiments and analysis show that LECI significantly outperforms prior methods on both synthetic and real-world datasets, establishing LECI as a practical and effective solution for graph OOD generalization.
    
[^162]: 初始猜测偏差：未经过训练的神经网络倾向于某些类别

    Initial Guessing Bias: How Untrained Networks Favor Some Classes. (arXiv:2306.00809v1 [cs.LG])

    [http://arxiv.org/abs/2306.00809](http://arxiv.org/abs/2306.00809)

    本文提出了“初始猜测偏差”现象，即在未经过训练的神经网络中，由于架构选择的影响，模型往往会将所有预测指向同一个类别。该现象对架构选择和初始化有实际指导意义，并具有理论后果，例如节点置换对称性的崩溃和深度带来的非平凡差异。

    

    神经网络的初始状态在调节后续的训练过程中扮演重要角色。在分类问题的背景下，我们提供了理论分析，证明神经网络的结构可以在训练之前，甚至在不存在显式偏差的情况下，使模型将所有预测都指向同一个类别。我们展示了这种现象的存在，称为“初始猜测偏差”（Initial Guessing Bias，IGB），这取决于架构选择，例如激活函数、最大池化层和网络深度。我们对IGB进行的分析具有实际意义，可以指导架构的选择和初始化。我们还强调理论后果，例如节点置换对称性的崩溃、自平均的破坏、某些均场近似的有效性以及深度带来的非平凡差异。

    The initial state of neural networks plays a central role in conditioning the subsequent training dynamics. In the context of classification problems, we provide a theoretical analysis demonstrating that the structure of a neural network can condition the model to assign all predictions to the same class, even before the beginning of training, and in the absence of explicit biases. We show that the presence of this phenomenon, which we call "Initial Guessing Bias" (IGB), depends on architectural choices such as activation functions, max-pooling layers, and network depth. Our analysis of IGB has practical consequences, in that it guides architecture selection and initialization. We also highlight theoretical consequences, such as the breakdown of node-permutation symmetry, the violation of self-averaging, the validity of some mean-field approximations, and the non-trivial differences arising with depth.
    
[^163]: 通过GANs和归一化流模型的Precision-Recall分歧优化进行生成建模

    Precision-Recall Divergence Optimization for Generative Modeling with GANs and Normalizing Flows. (arXiv:2305.18910v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18910](http://arxiv.org/abs/2305.18910)

    本文提出了一种通过优化Precision-Recall分歧来平衡生成模型图像质量和多样性的新训练方法，实现了用户定义的精确性和召回率权衡。

    

    在生成模型领域，平衡图像质量（精确性）和多样性（召回率）是一个重要的挑战。目前的最先进模型主要依赖于优化启发式方法，如Fr\'echet Inception Distance。虽然最近的发展引入了一些原则性的方法来评估精确性和召回率，但它们尚未成功地整合到生成模型的训练中。我们的主要贡献是一种新颖的生成模型训练方法，如生成对抗网络和归一化流模型，它明确地优化了用户定义的精确性和召回率的权衡。更具体地说，我们证明了实现指定的精确性-召回率权衡等价于最小化一组我们称之为“PR-divergences”的独特的$f$-divergence。反之，任何$f$-divergence都可以写成PR-divergences的线性组合，并对应一个加权精确性-召回率权衡。通过全面的试验，我们展示了我们的方法明显超过了先前的算法。

    Achieving a balance between image quality (precision) and diversity (recall) is a significant challenge in the domain of generative models. Current state-of-the-art models primarily rely on optimizing heuristics, such as the Fr\'echet Inception Distance. While recent developments have introduced principled methods for evaluating precision and recall, they have yet to be successfully integrated into the training of generative models. Our main contribution is a novel training method for generative models, such as Generative Adversarial Networks and Normalizing Flows, which explicitly optimizes a user-defined trade-off between precision and recall. More precisely, we show that achieving a specified precision-recall trade-off corresponds to minimizing a unique $f$-divergence from a family we call the \textit{PR-divergences}. Conversely, any $f$-divergence can be written as a linear combination of PR-divergences and corresponds to a weighted precision-recall trade-off. Through comprehensive
    
[^164]: 在线非随机无模型强化学习

    Online Nonstochastic Model-Free Reinforcement Learning. (arXiv:2305.17552v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.17552](http://arxiv.org/abs/2305.17552)

    本论文研究了在线非随机无模型强化学习算法，针对动态或者具有对抗性的环境提出了一种以干扰信号为中心的策略类别，并开发了高效实用的优化算法。

    

    我们研究了针对可能是动态或者具有对抗性的环境的鲁棒无模型强化学习算法。传统的基于状态的策略常常难以适应这些环境中未建模干扰所带来的挑战。此外，优化基于线性状态的策略在效率优化方面存在困难，即使在像线性动态系统这样良好的环境中也会出现非凸的目标函数。受模型控制最新进展的启发，我们引入了一种新颖的以干扰信号为中心的策略类别。我们定义了几个这些信号的类别，并基于它们开发了相应的策略类别。我们提供了用于优化这些策略的高效和实用的算法。接下来，我们研究了面对对抗性干扰时强化学习代理的在线适应任务。我们的方法与任何黑盒模型无缝集成。

    We investigate robust model-free reinforcement learning algorithms designed for environments that may be dynamic or even adversarial. Traditional state-based policies often struggle to accommodate the challenges imposed by the presence of unmodeled disturbances in such settings. Moreover, optimizing linear state-based policies pose an obstacle for efficient optimization, leading to nonconvex objectives, even in benign environments like linear dynamical systems.  Drawing inspiration from recent advancements in model-based control, we introduce a novel class of policies centered on disturbance signals. We define several categories of these signals, which we term pseudo-disturbances, and develop corresponding policy classes based on them. We provide efficient and practical algorithms for optimizing these policies.  Next, we examine the task of online adaptation of reinforcement learning agents in the face of adversarial disturbances. Our methods seamlessly integrate with any black-box mod
    
[^165]: DPOK: 强化学习用于微调文本到图像扩散模型

    DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models. (arXiv:2305.16381v1 [cs.LG])

    [http://arxiv.org/abs/2305.16381](http://arxiv.org/abs/2305.16381)

    本论文提出了DPOK，一种使用在线强化学习（RL）微调文本到图像扩散模型的方法。该方法在COCO数据集上实现了最先进的性能。

    

    已经证明，从人类反馈中学习可以改善文本到图像模型。这些技术首先学习一个捕捉任务中人类关心的特征的奖励函数，然后根据学习到的奖励函数改进模型。虽然已经研究了相对简单的方法（例如基于奖励得分的拒绝采样），但使用奖励函数微调文本到图像模型仍然具有挑战性。在这项工作中，我们提出使用在线强化学习（RL）来微调文本到图像模型。我们专注于扩散模型，将微调任务定义为RL问题，并使用策略梯度更新预训练文本到图像扩散模型，以最大化反馈训练奖励。我们的方法DPOK集成了KL正则化的策略优化。我们对RL微调和监督微调的KL正则化进行了分析。在我们的实验中，我们展示了DPOK通常优于使用交叉熵损失的监督微调和以前的RL微调技术。DPOK在COCO数据集上实现了最先进的性能，IS和FID得分显著优于现有方法。

    Learning from human feedback has been shown to improve text-to-image models. These techniques first learn a reward function that captures what humans care about in the task and then improve the models based on the learned reward function. Even though relatively simple approaches (e.g., rejection sampling based on reward scores) have been investigated, fine-tuning text-to-image models with the reward function remains challenging. In this work, we propose using online reinforcement learning (RL) to fine-tune text-to-image models. We focus on diffusion models, defining the fine-tuning task as an RL problem, and updating the pre-trained text-to-image diffusion models using policy gradient to maximize the feedback-trained reward. Our approach, coined DPOK, integrates policy optimization with KL regularization. We conduct an analysis of KL regularization for both RL fine-tuning and supervised fine-tuning. In our experiments, we show that DPOK is generally superior to supervised fine-tuning w
    
[^166]: 注意力不一定意味着在解答中选择正确率很高

    Attentiveness to Answer Choices Doesn't Always Entail High QA Accuracy. (arXiv:2305.14596v1 [cs.CL])

    [http://arxiv.org/abs/2305.14596](http://arxiv.org/abs/2305.14596)

    当大型语言模型应用于多项选择题时，其注意力往往会分散到许多无效的词汇符号上，这会导致模型真实性能的低估。本文提出了一种数学形式化方法来研究这种现象，并发现通过使用只含一个示例的上下文学习方法可以提高对有效选择的注意力。

    

    当大型语言模型被应用于零或少样本的鉴别性任务，例如多项选择题时，它们的注意力（即概率质量）会分散在许多无效的词汇符号上。这种在具有相同含义的多个表面形式之间分散导致了模型真实性能的低估，称为“表面形式竞争”（SFC）假说。这促使引入各种概率规范化方法，然而仍存在许多核心问题未解答。我们如何测量SFC或注意力？是否有直接的方法可以增加对有效选择的注意力？增加注意力总是能提高任务准确性吗？我们提出了一种数学形式化方法来研究这种现象，提供了一种量化注意力的度量方法，并确定了一种简单的增加注意力的方法，即通过仅包含答案选项的一个示例进行上下文学习。

    When large language models (LMs) are applied in zero- or few-shot settings to discriminative tasks such as multiple-choice questions, their attentiveness (i.e., probability mass) is spread across many vocabulary tokens that are not valid choices. Such a spread across multiple surface forms with identical meaning is thought to cause an underestimation of a model's true performance, referred to as the "surface form competition" (SFC) hypothesis. This has motivated the introduction of various probability normalization methods. However, many core questions remain unanswered. How do we measure SFC or attentiveness? Are there direct ways of increasing attentiveness on valid choices? Does increasing attentiveness always improve task accuracy? We propose a mathematical formalism for studying this phenomenon, provide a metric for quantifying attentiveness, and identify a simple method for increasing it -namely, in-context learning with even just one example containing answer choices. The form
    
[^167]: 通过伪神经切线核代理模型提供深度神经网络的鲁棒性解释

    Robust Explanations for Deep Neural Networks via Pseudo Neural Tangent Kernel Surrogate Models. (arXiv:2305.14585v1 [cs.LG])

    [http://arxiv.org/abs/2305.14585](http://arxiv.org/abs/2305.14585)

    本研究通过建立一个规范化的伪神经切线核，证明了它能够更好地与神经网络决策函数相关，比基于嵌入和影响的替代品更有效，并且从它创建的归因会更准确地选择被扰动的训练数据，从而证明了核线性模型是跨多个数据领域并有效的替代模型。

    

    最近，通过数据归属任务，解释型AI的进步之一是通过解释示例策略实现的。然而，用于将决策归因于训练数据的特征空间，尚未相互比较，以确定它们是否形成神经网络(NN)的真正代理模型。在这里，我们通过两种方式证明了线性特征空间对神经网络的有效性：(1)我们建立了一个规范化的伪神经切线核(pNTK)，它在计算机视觉和大语言模型架构中与神经网络决策函数更相关，比基于嵌入和影响的替代品更为有效；(2)我们展示了从规范化pNTK创建的归因比这些替代品更准确地选择被扰动的训练数据。基于这些观察结果，我们得出结论，核线性模型是跨多个数据领域并有效的替代模型。

    One of the ways recent progress has been made on explainable AI has been via explain-by-example strategies, specifically, through data attribution tasks. The feature spaces used to attribute decisions to training data, however, have not been compared against one another as to whether they form a truly representative surrogate model of the neural network (NN). Here, we demonstrate the efficacy of surrogate linear feature spaces to neural networks through two means: (1) we establish that a normalized psuedo neural tangent kernel (pNTK) is more correlated to the neural network decision functions than embedding based and influence based alternatives in both computer vision and large language model architectures; (2) we show that the attributions created from the normalized pNTK more accurately select perturbed training data in a data poisoning attribution task than these alternatives. Based on these observations, we conclude that kernel linear models are effective surrogate models across m
    
[^168]: 从子采样时间序列中使用代理变量进行因果推断

    Causal Discovery from Subsampled Time Series with Proxy Variables. (arXiv:2305.05276v1 [cs.LG])

    [http://arxiv.org/abs/2305.05276](http://arxiv.org/abs/2305.05276)

    本研究提出了一种使用代理变量方法的无模型算法，可以从子采样时间序列中无需参数约束地识别整个因果结构。

    

    从时间序列数据推断因果结构是许多科学研究的核心兴趣。采样频率远低于因果影响频率是此类推断的主要障碍。为了克服这个问题，已经提出了许多基于模型和非模型的方法，但是要么局限于线性情况，要么无法建立可识别性。在本研究中，我们提出了一种无模型的算法，可以在没有任何参数约束的情况下从子采样时间序列识别整个因果结构。该方法的思想是，子采样的挑战主要来自于“未观察到”的时间步，因此应使用为未观察到变量设计的工具处理此问题。在这些工具中，我们发现代理变量方法特别适合，因为未观察到变量的代理变量自然是在观察到的时间步上本身。根据这种直觉，我们建立了全面的结构可识别性。

    Inferring causal structures from time series data is the central interest of many scientific inquiries. A major barrier to such inference is the problem of subsampling, i.e., the frequency of measurements is much lower than that of causal influence. To overcome this problem, numerous model-based and model-free methods have been proposed, yet either limited to the linear case or failed to establish identifiability. In this work, we propose a model-free algorithm that can identify the entire causal structure from subsampled time series, without any parametric constraint. The idea is that the challenge of subsampling arises mainly from \emph{unobserved} time steps and therefore should be handled with tools designed for unobserved variables. Among these tools, we find the proxy variable approach particularly fits, in the sense that the proxy of an unobserved variable is naturally itself at the observed time step. Following this intuition, we establish comprehensive structural identifiabili
    
[^169]: 基于替代模型的人机交互场景生成

    Surrogate Assisted Generation of Human-Robot Interaction Scenarios. (arXiv:2304.13787v1 [cs.RO])

    [http://arxiv.org/abs/2304.13787](http://arxiv.org/abs/2304.13787)

    本文提出了基于替代模型的人机交互场景生成方法，可以高效合成多样化的挑战性数据集，以便评估和理解人机交互系统的优劣，可以在实际交互中重现这些场景。

    

    随着人机交互系统的发展，不同环境和用户下评估和理解这些系统的优缺点变得越来越困难。为此，以往的方法通过算法生成了多样的场景，揭示了共享控制遥操作任务的系统失效情况。然而，这些方法需要通过模拟机器人策略和人类行为来直接评估生成的场景。这些评估所需的计算成本限制了它们在更复杂的领域的适用性。因此，我们提出了通过替代模型来预测人类和机器人行为来增强场景生成系统的建议。在共享控制遥操作域和更复杂的共享工作空间协作任务中，我们展示了替代模型辅助的场景生成可以高效地合成具有挑战性的多样数据集。我们展示了这些故障在真实世界中的交互中是可重现的。

    As human-robot interaction (HRI) systems advance, so does the difficulty of evaluating and understanding the strengths and limitations of these systems in different environments and with different users. To this end, previous methods have algorithmically generated diverse scenarios that reveal system failures in a shared control teleoperation task. However, these methods require directly evaluating generated scenarios by simulating robot policies and human actions. The computational cost of these evaluations limits their applicability in more complex domains. Thus, we propose augmenting scenario generation systems with surrogate models that predict both human and robot behaviors. In the shared control teleoperation domain and a more complex shared workspace collaboration task, we show that surrogate assisted scenario generation efficiently synthesizes diverse datasets of challenging scenarios. We demonstrate that these failures are reproducible in real-world interactions.
    
[^170]: 基于局部能量分布的随机模拟退火超参数确定

    Local Energy Distribution Based Hyperparameter Determination for Stochastic Simulated Annealing. (arXiv:2304.11839v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.11839](http://arxiv.org/abs/2304.11839)

    本文提出了一种基于局部能量分布的随机模拟退火超参数确定方法，该方法通过中心极限定理估计局部能量的分布，将超参数搜索的时间复杂度从O(n^3)降低到O(1)，在解决最大割问题中的实验中表现良好。

    

    本文提出了一种基于局部能量分布的随机模拟退火（SSA）超参数确定方法。 SSA能够比典型的模拟退火（SA）更快地解决组合优化问题，但需要耗费时间进行超参数搜索。所提出的方法基于自旋（概率比特）的局部能量分布来确定超参数。自旋是SSA的基本计算元素，并通过权重与其他自旋进行图形连接。局部能量的分布可以基于中心极限定理（CLT）进行估计。基于CLT的正态分布用于确定超参数，其将超参数搜索的时间复杂度从传统方法的O(n^3)降低到O(1)。使用确定的超参数评估了SSA在Gset和K2000基准上的性能，用于最大割问题。结果表明，所提出的方法实现了平均割值的近似值。

    This paper presents a local energy distribution based hyperparameter determination for stochastic simulated annealing (SSA). SSA is capable of solving combinatorial optimization problems faster than typical simulated annealing (SA), but requires a time-consuming hyperparameter search. The proposed method determines hyperparameters based on the local energy distributions of spins (probabilistic bits). The spin is a basic computing element of SSA and is graphically connected to other spins with its weights. The distribution of the local energy can be estimated based on the central limit theorem (CLT). The CLT-based normal distribution is used to determine the hyperparameters, which reduces the time complexity for hyperparameter search from O(n^3) of the conventional method to O(1). The performance of SSA with the determined hyperparameters is evaluated on the Gset and K2000 benchmarks for maximum-cut problems. The results show that the proposed method achieves mean cut values of approxim
    
[^171]: 使用Price定理和分段线性分解分析在线交叉相关器的框架。

    A Framework for Analyzing Online Cross-correlators using Price's Theorem and Piecewise-Linear Decomposition. (arXiv:2304.09242v1 [cs.LG])

    [http://arxiv.org/abs/2304.09242](http://arxiv.org/abs/2304.09242)

    本文提出了一种使用非线性函数构建交叉相关器的方法，并使用Price定理和分段线性分解提出了一个数学框架来分析这种交叉相关器。

    

    精确估计两个随机变量之间的交叉相关或相似度是信号检测、高维计算、联想记忆和神经网络的核心问题。本文提出了一种能够构建具有更高信噪比（SNR）的交叉相关器的大量简单非线性函数的方法，并使用Price定理和分段线性分解提出了一个数学框架，以分析使用混合分段线性函数构建的交叉相关器。

    Precise estimation of cross-correlation or similarity between two random variables lies at the heart of signal detection, hyperdimensional computing, associative memories, and neural networks. Although a vast literature exists on different methods for estimating cross-correlations, the question what is the best and simplest method to estimate cross-correlations using finite samples ? is still not clear. In this paper, we first argue that the standard empirical approach might not be the optimal method even though the estimator exhibits uniform convergence to the true cross-correlation. Instead, we show that there exists a large class of simple non-linear functions that can be used to construct cross-correlators with a higher signal-to-noise ratio (SNR). To demonstrate this, we first present a general mathematical framework using Price's Theorem that allows us to analyze cross-correlators constructed using a mixture of piece-wise linear functions. Using this framework and high-dimensiona
    
[^172]: 简单的排序标准有助于在加性噪声模型中找到因果顺序。

    Simple Sorting Criteria Help Find the Causal Order in Additive Noise Models. (arXiv:2303.18211v1 [stat.ML])

    [http://arxiv.org/abs/2303.18211](http://arxiv.org/abs/2303.18211)

    文章探讨了加性噪声模型中找到因果顺序的方法。作者发现除了方差排序外，变量的决定系数$R^2$排序也可用于匹配已有方法的表现，且不受数据缩放的影响。

    

    加性噪声模型（ANM）是一种常见的功能假设，可以从观测数据中学习因果结构。由于缺乏符合假设的真实世界数据，合成ANM数据经常用于评估因果发现算法。Reisach等人（2021）表明，对于常见的模拟参数，按增大方差的顺序变量排列与因果顺序密切相关，并引入变异性可排序性来量化这种对齐程度。本文还表明，除了方差，还有变量的方差被所有其他变量解释的比例（由决定系数$R^2$捕获）倾向于沿着因果顺序增加。简单的基准算法可以使用$R^2$-sortability来匹配已有方法的性能。由于$R^2$可排序性不受数据缩放的影响，这些算法在标准化或重新缩放的数据上表现同样出色，解决了利用变异性可排序性的算法的一个关键限制。

    Additive Noise Models (ANM) encode a popular functional assumption that enables learning causal structure from observational data. Due to a lack of real-world data meeting the assumptions, synthetic ANM data are often used to evaluate causal discovery algorithms. Reisach et al. (2021) show that, for common simulation parameters, a variable ordering by increasing variance is closely aligned with a causal order and introduce var-sortability to quantify the alignment. Here, we show that not only variance, but also the fraction of a variable's variance explained by all others, as captured by the coefficient of determination $R^2$, tends to increase along the causal order. Simple baseline algorithms can use $R^2$-sortability to match the performance of established methods. Since $R^2$-sortability is invariant under data rescaling, these algorithms perform equally well on standardized or rescaled data, addressing a key limitation of algorithms exploiting var-sortability. We characterize and 
    
[^173]: 从时间序列中推断网络结构的神经方法

    Inferring networks from time series: a neural approach. (arXiv:2303.18059v1 [cs.LG])

    [http://arxiv.org/abs/2303.18059](http://arxiv.org/abs/2303.18059)

    本论文提出了一种基于神经网络的快速计算方法，可以从时间序列数据中推断大型网络的相邻矩阵，并对不确定性进行量化，解决了网络推断问题的不足。

    

    网络结构是许多复杂现象的动态基础，包括基因调控、食物链、电力网络和社交媒体。然而，由于网络结构通常无法直接观测到，因此必须从其紧急动态的观测数据中推断它们的相互连接性。在本研究中，我们提出了一种快速计算方法，使用神经网络从时间序列数据中推断大型网络相邻矩阵。使用神经网络提供了预测的不确定性量化方法，反映了推断问题的非凸性和数据上的噪声。这是有用的，因为网络推断问题通常是欠定的，并且在网络推断方法中缺乏这个特征。我们通过从观测其响应断电的情况下推断英国电力网络的线路故障位置来展示我们的方法的能力。

    Network structures underlie the dynamics of many complex phenomena, from gene regulation and foodwebs to power grids and social media. Yet, as they often cannot be observed directly, their connectivities must be inferred from observations of their emergent dynamics. In this work we present a powerful and fast computational method to infer large network adjacency matrices from time series data using a neural network. Using a neural network provides uncertainty quantification on the prediction in a manner that reflects both the non-convexity of the inference problem as well as the noise on the data. This is useful since network inference problems are typically underdetermined, and a feature that has hitherto been lacking from network inference methods. We demonstrate our method's capabilities by inferring line failure locations in the British power grid from observations of its response to a power cut. Since the problem is underdetermined, many classical statistical tools (e.g. regressio
    
[^174]: HDformer: 一种利用长距离血管信号进行糖尿病检测的高维Transformer

    HDformer: A Higher Dimensional Transformer for Diabetes Detection Utilizing Long Range Vascular Signals. (arXiv:2303.11340v1 [cs.LG])

    [http://arxiv.org/abs/2303.11340](http://arxiv.org/abs/2303.11340)

    本研究提出了一种新的基于高维Transformer的架构HDformer，并利用长距离PPG信号进行糖尿病检测，其中提出了一种新的注意力模块TSA，成功将标记体积减少10倍以上，提高了模型的能力和效率。

    

    糖尿病是全球性问题，早期检测有助于预防严重并发症。已出现将心血管信号纳入深度学习模型的低成本、非侵入式检测方法，但限制其临床应用的是有限的准确性。本文提出了一种新的基于Transformer的架构，即Higher Dimensional Transformer（HDformer），它利用长距离光电容积图（PPG）信号来检测糖尿病。相较于现有研究常用的不足一分钟的PPG信号，长距离PPG包含更广泛、更深入的信号上下文信息。为了增加处理长距离数据的能力和效率，我们提出了一种新的注意力模块Time Square Attention（TSA），将标记体积减少10倍以上，同时保留本地/全局依赖关系。它将一维输入 转换为二维表示，并将相邻点组成一个单独的2D标记。

    Diabetes mellitus is a worldwide concern, and early detection can help to prevent serious complications. Low-cost, non-invasive detection methods, which take cardiovascular signals into deep learning models, have emerged. However, limited accuracy constrains their clinical usage. In this paper, we present a new Transformer-based architecture, Higher Dimensional Transformer (HDformer), which takes long-range photoplethysmography (PPG) signals to detect diabetes. The long-range PPG contains broader and deeper signal contextual information compared to the less-than-one-minute PPG signals commonly utilized in existing research. To increase the capability and efficiency of processing the long range data, we propose a new attention module Time Square Attention (TSA), reducing the volume of the tokens by more than 10x, while retaining the local/global dependencies. It converts the 1-dimensional inputs into 2-dimensional representations and groups adjacent points into a single 2D token, using 
    
[^175]: 在联邦深度学习中优化批标准化

    Making Batch Normalization Great in Federated Deep Learning. (arXiv:2303.06530v1 [cs.LG])

    [http://arxiv.org/abs/2303.06530](http://arxiv.org/abs/2303.06530)

    本文研究了在联邦学习中使用批标准化和群组归一化的效果，发现在适当的处理下，批标准化可以在广泛的联邦学习设置中具有很高的竞争力，而且这不需要额外的训练或通信成本。

    This paper studies the use of batch normalization and group normalization in federated learning, and finds that with proper treatments, batch normalization can be highly competitive across a wide range of federated learning settings, and this requires no additional training or communication costs.

    批标准化（BN）通常用于现代深度神经网络（DNN）中，以提高稳定性并加速集中式训练的收敛速度。在具有非IID分散数据的联邦学习（FL）中，先前的研究观察到使用BN进行训练可能会由于训练和测试之间的BN统计不匹配而阻碍性能。因此，群组归一化（GN）更常用于FL作为BN的替代方法。然而，通过我们在各种FL设置下的实证研究，我们发现BN和GN之间没有一致的优胜者。这促使我们重新审视FL中归一化层的使用。我们发现，在适当的处理下，BN可以在广泛的FL设置中具有很高的竞争力，而且这不需要额外的训练或通信成本。我们希望我们的研究可以成为FL未来实际使用和理论分析的有价值参考。

    Batch Normalization (BN) is commonly used in modern deep neural networks (DNNs) to improve stability and speed up convergence during centralized training. In federated learning (FL) with non-IID decentralized data, previous works observed that training with BN could hinder performance due to the mismatch of the BN statistics between training and testing. Group Normalization (GN) is thus more often used in FL as an alternative to BN. However, from our empirical study across various FL settings, we see no consistent winner between BN and GN. This leads us to revisit the use of normalization layers in FL. We find that with proper treatments, BN can be highly competitive across a wide range of FL settings, and this requires no additional training or communication costs. We hope that our study could serve as a valuable reference for future practical usage and theoretical analysis in FL.
    
[^176]: 并行混合网络：量子和经典神经网络之间的相互作用

    Parallel Hybrid Networks: an interplay between quantum and classical neural networks. (arXiv:2303.03227v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2303.03227](http://arxiv.org/abs/2303.03227)

    本论文介绍了一种新颖的混合量子神经网络模型，通过在输入数据集时将数据同时传递给经典神经网络和量子电路，并线性组合两者的输出，以解决量子神经网络在拟合非谐波特征时的困难。在实验中，通过在合成数据集上的验证，证明了该模型的有效性。

    

    量子神经网络是一种新的机器学习范式，最近由于其潜在的优势而引起了很多关注。在特定条件下，这些模型用截断的傅里叶级数近似其数据集的分布。这种拟合的三角函数特性可能导致角度嵌入的量子神经网络在拟合给定数据集的非谐波特征时出现困难。此外，神经网络的可解释性仍然是一个挑战。在这项工作中，我们引入了一种新的可解释的混合量子神经网络类别，将数据集的输入同时传递给1）经典多层感知机和2）变分量子电路，然后将两者的输出进行线性组合。我们观察到量子神经网络在训练集上创建了一个平滑的正弦基础，然后经典感知机填补了该平台上非谐波的空白。我们在两个合成数据集上验证了这一观点。

    Quantum neural networks represent a new machine learning paradigm that has recently attracted much attention due to its potential promise. Under certain conditions, these models approximate the distribution of their dataset with a truncated Fourier series. The trigonometric nature of this fit could result in angle-embedded quantum neural networks struggling to fit the non-harmonic features in a given dataset. Moreover, the interpretability of neural networks remains a challenge. In this work, we introduce a new, interpretable class of hybrid quantum neural networks that pass the inputs of the dataset in parallel to 1) a classical multi-layered perceptron and 2) a variational quantum circuit, and then the outputs of the two are linearly combined. We observe that the quantum neural network creates a smooth sinusoidal foundation base on the training set, and then the classical perceptrons fill the non-harmonic gaps in the landscape. We demonstrate this claim on two synthetic datasets samp
    
[^177]: 作为多Agent强化学习基准的重复剪刀石头布的基于人口评估

    Population-based Evaluation in Repeated Rock-Paper-Scissors as a Benchmark for Multiagent Reinforcement Learning. (arXiv:2303.03196v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2303.03196](http://arxiv.org/abs/2303.03196)

    这项研究提出了一种基于重复剪刀石头布游戏的多Agent学习基准，展示了几种学习方法的泛化能力，为多Agent学习领域的研究提供了机会。

    

    机器学习和对抗规划领域的进展，很大程度上受益于基准域，从国际象棋和经典的UCI数据集到围棋和外交。在顺序决策中，对Agent评估主要局限于与专家进行少量交互，旨在达到一定的性能水平（如击败人类专业玩家）。我们提出了一种基于剪刀石头布的多Agent学习基准，其中包括四十三个锦标赛参赛作品，其中一些是有意的次优作品。我们描述了基于平均回报和可开发性的代理质量度量标准。然后，我们展示了几种RL、在线学习和语言模型方法可以学习良好的反策略，并具有良好的泛化能力，但最终会输给表现最佳的机器人，为多Agent学习的研究提供了机会。

    Progress in fields of machine learning and adversarial planning has benefited significantly from benchmark domains, from checkers and the classic UCI data sets to Go and Diplomacy. In sequential decision-making, agent evaluation has largely been restricted to few interactions against experts, with the aim to reach some desired level of performance (e.g. beating a human professional player). We propose a benchmark for multiagent learning based on repeated play of the simple game Rock, Paper, Scissors along with a population of forty-three tournament entries, some of which are intentionally sub-optimal. We describe metrics to measure the quality of agents based both on average returns and exploitability. We then show that several RL, online learning, and language model approaches can learn good counter-strategies and generalize well, but ultimately lose to the top-performing bots, creating an opportunity for research in multiagent learning.
    
[^178]: 基于模型的强化学习在能源市场清算和出价中的应用研究

    Approximating Energy Market Clearing and Bidding With Model-Based Reinforcement Learning. (arXiv:2303.01772v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2303.01772](http://arxiv.org/abs/2303.01772)

    本文研究了基于模型的强化学习用于能源市场清算和出价的应用方法。通过用学习的OPF代理模型以及明确的市场规则替代传统计算方法，本方法极大地减少了训练时间并适用于市场设计和更现实地建模市场参与者。

    

    能源市场可能会为市场参与者的不良行为提供激励。多智能体强化学习是预测能源市场参与者预期行为的有前途的新方法。然而，强化学习需要许多与系统的交互才能收敛，而电力系统环境通常包括广泛的计算，例如用于市场清算的最优功率流量（OPF）计算。为了解决这个复杂性，我们提供了一个能源市场的模型给基本的MARL算法，这个模型采用了学习的OPF近似值和明确的市场规则。学习的OPF代理模型使得OPF的明确解决变得不必要。我们的实验表明，该模型还将训练时间降低了约一个数量级，但代价是略微更差的纳什均衡近似值。我们方法的潜在应用是市场设计，更现实地对市场参与者进行建模以及对市场动态的改进理解。

    Energy markets can provide incentives for undesired behavior of market participants. Multi-agent Reinforcement learning (MARL) is a promising new approach to predicting the expected behavior of energy market participants. However, reinforcement learning requires many interactions with the system to converge, and the power system environment often consists of extensive computations, e.g., optimal power flow (OPF) calculation for market clearing. To tackle this complexity, we provide a model of the energy market to a basic MARL algorithm in the form of a learned OPF approximation and explicit market rules. The learned OPF surrogate model makes an explicit solving of the OPF completely unnecessary. Our experiments demonstrate that the model additionally reduces training time by about one order of magnitude but at the cost of a slightly worse approximation of the Nash equilibrium. Potential applications of our method are market design, more realistic modeling of market participants, and an
    
[^179]: 对正则化中的偏差进行惩罚将使稀疏化

    Penalising the biases in norm regularisation enforces sparsity. (arXiv:2303.01353v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.01353](http://arxiv.org/abs/2303.01353)

    本研究表明，控制神经网络参数的范数可以获得良好的泛化性能。对神经网络中偏差项的范数进行惩罚可以实现稀疏估计量。

    

    当训练神经网络时，通过控制参数的范数往往可以获得良好的泛化性能。然而，正则化参数的范数和所得估计量之间的关系在理论上尚未完全理解。本文针对具有单一隐藏层和一维数据的神经网络，展示了表示函数所需的参数范数由其二阶导数的总变差加权得到，其中所加权的因子为$\sqrt{1+x^2}$。值得注意的是，当不对偏差项的范数进行正则化时，这个加权因子会消失。这个额外的加权因子的存在非常重要，因为它被证明可以强制实现最小范数内插器的唯一性和稀疏性（在拐点数量上）。相反，省略偏差的范数则会导致非稀疏解。因此，在正则化中对偏差项进行惩罚，无论是显式还是隐式地，都会导致稀疏估计量。

    Controlling the parameters' norm often yields good generalisation when training neural networks. Beyond simple intuitions, the relation between regularising parameters' norm and obtained estimators remains theoretically misunderstood. For one hidden ReLU layer networks with unidimensional data, this work shows the parameters' norm required to represent a function is given by the total variation of its second derivative, weighted by a $\sqrt{1+x^2}$ factor. Notably, this weighting factor disappears when the norm of bias terms is not regularised. The presence of this additional weighting factor is of utmost significance as it is shown to enforce the uniqueness and sparsity (in the number of kinks) of the minimal norm interpolator. Conversely, omitting the bias' norm allows for non-sparse solutions. Penalising the bias terms in the regularisation, either explicitly or implicitly, thus leads to sparse estimators.
    
[^180]: 在结构分布偏移条件下评估图模型的鲁棒性和不确定性

    Evaluating Robustness and Uncertainty of Graph Models Under Structural Distributional Shifts. (arXiv:2302.13875v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13875](http://arxiv.org/abs/2302.13875)

    该论文提出了一种基于图结构的多样化分布转换的方法，并且针对性地设计了数据集。实验结果表明这些分布转换对于现有的图模型具有挑战性。

    

    在基于机器学习的可靠决策系统中，模型必须对分布偏移具有鲁棒性或提供其预测的不确定性。在图学习的节点级问题中，分布偏移可能尤为复杂，因为样本是相互依赖的。为了评估图模型的性能，重要的是在各种有意义的分布偏移下对它们进行测试。然而，大多数考虑节点级分布偏移的图基准主要关注节点特征，而结构属性对图问题也很重要。在这项工作中，我们提出了一种基于图结构引出多样化分布偏移的通用方法。我们使用这种方法根据几个节点的结构属性：流行度、局部性和密度来创建数据分割。在我们的实验中，我们全面评估了所提出的分布偏移，并表明它们对于现有的图模型可能非常具有挑战性。我们还修订了一些关于基准测试图模型的先前工作，并提出了一组新的基准测试，考虑了结构分布偏移条件。

    In reliable decision-making systems based on machine learning, models have to be robust to distributional shifts or provide the uncertainty of their predictions. In node-level problems of graph learning, distributional shifts can be especially complex since the samples are interdependent. To evaluate the performance of graph models, it is important to test them on diverse and meaningful distributional shifts. However, most graph benchmarks considering distributional shifts for node-level problems focus mainly on node features, while structural properties are also essential for graph problems. In this work, we propose a general approach for inducing diverse distributional shifts based on graph structure. We use this approach to create data splits according to several structural node properties: popularity, locality, and density. In our experiments, we thoroughly evaluate the proposed distributional shifts and show that they can be quite challenging for existing graph models. We also rev
    
[^181]: 通过物理对称学习可解释的低维表示

    Learning Interpretable Low-dimensional Representation via Physical Symmetry. (arXiv:2302.10890v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10890](http://arxiv.org/abs/2302.10890)

    通过使用物理对称性作为潜在空间的自洽约束条件，该研究展示了在音乐领域和计算机视觉领域，模型可以以无监督的方式学习出可解释的低维表示，例如线性音高和三维笛卡尔因素。

    

    可解释的表示学习在创造性智能系统中起着关键作用。在音乐领域，当前的学习算法可以成功地学习各种特征，如音高、音色、和弦、纹理等。然而，大多数方法严重依赖音乐领域知识。现在还不清楚什么样的一般性计算原则会产生可解释的表示，特别是与人类感知保持一致的低维因素。在这项研究中，我们从现代物理学中获得灵感，将物理对称性作为潜在空间的自洽约束条件。特别是，它要求先验模型对潜在状态的动态进行描述，并以某种群变换对其进行等变。我们展示了物理对称性使得模型能够以无监督的方式从未标记的单声道音乐音频中学习一个线性音高因素。此外，相同的方法可以应用于计算机视觉，学习一个三维笛卡尔因素。

    Interpretable representation learning has been playing a key role in creative intelligent systems. In the music domain, current learning algorithms can successfully learn various features such as pitch, timbre, chord, texture, etc. However, most methods rely heavily on music domain knowledge. It remains an open question what general computational principles give rise to interpretable representations, especially low-dim factors that agree with human perception. In this study, we take inspiration from modern physics and use physical symmetry as a self-consistency constraint for the latent space. Specifically, it requires the prior model that characterises the dynamics of the latent states to be equivariant with respect to certain group transformations. We show that physical symmetry leads the model to learn a linear pitch factor from unlabelled monophonic music audio in a self-supervised fashion. In addition, the same methodology can be applied to computer vision, learning a 3D Cartesian
    
[^182]: 能量变换器

    Energy Transformer. (arXiv:2302.07253v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07253](http://arxiv.org/abs/2302.07253)

    本研究将注意力机制、能量模型和联想记忆结合，提出了一种新颖的架构——能量变换器（ET），它通过特意设计的注意力层以最小化能量函数，用于表示标记之间的关系。

    

    我们的工作将注意力机制、能量模型和联想记忆三种潜力巨大的机器学习范式结合起来。注意力是推动现代深度学习成功的动力源，但它缺乏明确的理论基础。能量模型允许在判别和生成任务上采用有原则的方法，但能量函数的设计并不直观。与此同时，稠密联想记忆模型或现代霍普菲尔德网络具有良好的理论基础，并且允许能量函数的直观设计。我们提出了一种新颖的架构，称为能量变换器（简称ET），它使用一系列经过特意设计以最小化特殊设计的能量函数的注意力层，该函数负责表示标记之间的关系。在这项工作中，我们介绍了ET的理论基础，通过使用图像补全技术探索了它的经验能力。

    Our work combines aspects of three promising paradigms in machine learning, namely, attention mechanism, energy-based models, and associative memory. Attention is the power-house driving modern deep learning successes, but it lacks clear theoretical foundations. Energy-based models allow a principled approach to discriminative and generative tasks, but the design of the energy functional is not straightforward. At the same time, Dense Associative Memory models or Modern Hopfield Networks have a well-established theoretical foundation, and allow an intuitive design of the energy function. We propose a novel architecture, called the Energy Transformer (or ET for short), that uses a sequence of attention layers that are purposely designed to minimize a specifically engineered energy function, which is responsible for representing the relationships between the tokens. In this work, we introduce the theoretical foundations of ET, explore its empirical capabilities using the image completion
    
[^183]: 这是一篇关于通过跟踪目标动态来实现更快的文本到图像定制的论文

    Is This Loss Informative? Faster Text-to-Image Customization by Tracking Objective Dynamics. (arXiv:2302.04841v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.04841](http://arxiv.org/abs/2302.04841)

    本文研究了文本到图像个性化方法的训练动态，并提出了一种简单的早停准则来加快训练速度

    

    文本到图像生成模型代表了图像合成的下一个发展阶段，为实现灵活但精细的控制结果提供了一种自然的方式。研究的一个新兴领域是将大型文本到图像模型快速适应到较小的数据集或新的视觉概念。然而，许多高效的适应方法需要长时间的训练，这限制了它们的实际应用，降低了研究实验的速度，并消耗了过多的GPU资源。在这项工作中，我们研究了流行的文本到图像个性化方法（如文本倒转或梦幻小屋）的训练动态，旨在加速它们。我们观察到大多数概念在早期阶段就已经学习到了，并且质量在后期没有得到改善，但是标准的模型收敛指标未能指示这一点。相反，我们提出了一种简单的即插即用的早停准则，该准则只需要在所有训练迭代中对一组固定输入计算常规训练目标。我们对...进行了实验

    Text-to-image generation models represent the next step of evolution in image synthesis, offering a natural way to achieve flexible yet fine-grained control over the result. One emerging area of research is the fast adaptation of large text-to-image models to smaller datasets or new visual concepts. However, many efficient methods of adaptation have a long training time, which limits their practical applications, slows down research experiments, and spends excessive GPU resources. In this work, we study the training dynamics of popular text-to-image personalization methods (such as Textual Inversion or DreamBooth), aiming to speed them up. We observe that most concepts are learned at early stages and do not improve in quality later, but standard model convergence metrics fail to indicate that. Instead, we propose a simple drop-in early stopping criterion that only requires computing the regular training objective on a fixed set of inputs for all training iterations. Our experiments on 
    
[^184]: 通过比较反馈引导个性化多目标决策中的用户偏好

    Eliciting User Preferences for Personalized Multi-Objective Decision Making through Comparative Feedback. (arXiv:2302.03805v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03805](http://arxiv.org/abs/2302.03805)

    这项研究提出了一个多目标决策框架，通过比较不同用户的政策来学习用户对目标的偏好，并根据偏好计算出近似最优的个性化政策。

    

    在经典的强化学习和决策问题中，政策是根据标量奖励函数进行评估的，而所有最优政策在预期回报方面是相同的。然而，许多现实世界的问题涉及到平衡多个、有时是冲突的目标，这些目标的相对优先级会根据每个用户的偏好而变化。因此，对一个用户而言最优的政策可能对另一个用户而言是次优的。在这项工作中，我们提出了一个多目标决策框架，以适应不同用户对目标的偏好，其中偏好是通过政策比较来学习的。我们的模型由一个具有向量值奖励函数的马尔可夫决策过程组成，每个用户都有一个未知的偏好向量，表示每个目标的相对重要性。目标是高效地计算出给定用户的近似最优政策。我们考虑两种用户反馈模型。

    In classic reinforcement learning (RL) and decision making problems, policies are evaluated with respect to a scalar reward function, and all optimal policies are the same with regards to their expected return. However, many real-world problems involve balancing multiple, sometimes conflicting, objectives whose relative priority will vary according to the preferences of each user. Consequently, a policy that is optimal for one user might be sub-optimal for another. In this work, we propose a multi-objective decision making framework that accommodates different user preferences over objectives, where preferences are learned via policy comparisons. Our model consists of a Markov decision process with a vector-valued reward function, with each user having an unknown preference vector that expresses the relative importance of each objective. The goal is to efficiently compute a near-optimal policy for a given user. We consider two user feedback models. We first address the case where a use
    
[^185]: 基于分数的条件模型的概念代数

    Concept Algebra for Score-Based Conditional Models. (arXiv:2302.03693v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.03693](http://arxiv.org/abs/2302.03693)

    本文研究了基于分数的条件模型中学习表示的结构，并开发了一种数学形式化表达概念被编码为表示空间子空间的思想。利用这个方法，我们提出了一种简单的方法来识别给定概念对应的表示部分，并通过代数操作操纵模型所表达的概念。

    

    本文研究了文本引导生成模型中学习表示的结构，重点关注基于分数的模型。我们聚焦于概念被编码为某种表示空间的子空间（或方向）的思想，并开发了这个思想的数学形式化。利用这个形式化方法，我们展示了有一个自然的表示选择具有这种性质，并且我们开发了一种简单的方法来识别与给定概念对应的表示部分。特别是，这使我们能够通过对表示的代数操作来操纵模型所表达的概念。我们使用稳定扩散在文本引导图像生成的示例中演示了这个思想。

    This paper concerns the structure of learned representations in text-guided generative models, focusing on score-based models. Here, we focus on the idea that concepts are encoded as subspaces (or directions) of some representation space. We develop a mathematical formalization of this idea.Using this formalism, we show there's a natural choice of representation with this property, and we develop a simple method for identifying the part of the representation corresponding to a given concept. In particular, this allows us to manipulate the concepts expressed by the model through algebraic manipulation of the representation. We demonstrate the idea with examples text-guided image generation, using Stable Diffusion.
    
[^186]: 扁平化贝叶斯神经网络

    Flat Seeking Bayesian Neural Networks. (arXiv:2302.02713v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02713](http://arxiv.org/abs/2302.02713)

    本文提出了一种扁平化贝叶斯神经网络的方法，该方法在后验推论中考虑了模型的扁平化性质，从而提升了模型的泛化能力和不确定性估计能力。

    

    贝叶斯神经网络（BNN）通过对模型参数施加先验分布并基于观测数据推断后验分布，为深度学习模型提供了概率解释。从后验分布中采样的模型可用于提供集成预测和量化预测不确定性。众所周知，具有较低尖度的深度学习模型具有更好的泛化能力。然而，现有的后验推论对于尖度/扁平化并不具备意识性，可能导致从其采样的模型具有较高的尖度。在本文中，我们针对扁平化对后验进行了理论、贝叶斯设定和变分推断方法的开发。具体地，我们从扁平化意义上推断的模型以及估计该扁平化意义后验的最佳近似后验，具有更好的扁平化性质，因此可能具有更高的泛化能力。我们对几个基准数据集进行了实验评估，并证明我们的方法在样本外准确性和不确定性估计方面优于现有方法。

    Bayesian Neural Networks (BNNs) provide a probabilistic interpretation for deep learning models by imposing a prior distribution over model parameters and inferring a posterior distribution based on observed data. The model sampled from the posterior distribution can be used for providing ensemble predictions and quantifying prediction uncertainty. It is well-known that deep learning models with lower sharpness have better generalization ability. However, existing posterior inferences are not aware of sharpness/flatness in terms of formulation, possibly leading to high sharpness for the models sampled from them. In this paper, we develop theories, the Bayesian setting, and the variational inference approach for the sharpness-aware posterior. Specifically, the models sampled from our sharpness-aware posterior, and the optimal approximate posterior estimating this sharpness-aware posterior, have better flatness, hence possibly possessing higher generalization ability. We conduct experime
    
[^187]: 基于规约的延迟反馈顺序决策框架

    A Reduction-based Framework for Sequential Decision Making with Delayed Feedback. (arXiv:2302.01477v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01477](http://arxiv.org/abs/2302.01477)

    我们提出了一个基于规约的框架，可以将任何多批次算法转化为处理顺序决策中的随机延迟的高效算法。我们不仅在赌博机、表格型MDPs和表格型MGs方面取得了与现有结果相匹配或改进的成果，还首次对顺序决策中的延迟与函数逼近进行了研究。

    

    我们研究了一般多智能体顺序决策中的随机延迟反馈，包括赌博机问题、单智能体马尔可夫决策过程（MDPs）和马尔可夫博弈（MGs）。我们提出了一种新颖的基于规约的框架，将任何多批次算法转化为能处理顺序决策中的随机延迟的高效算法。通过将不同的多批次算法插入我们的框架中，我们提供了几个示例，证明我们的框架不仅匹配或改进了现有的赌博机、表格型MDPs和表格型MGs的结果，还首次对顺序决策中的延迟与函数逼近进行了研究。总之，我们为多智能体顺序决策中的延迟反馈提供了一套完整的尖锐结果。

    We study stochastic delayed feedback in general multi-agent sequential decision making, which includes bandits, single-agent Markov decision processes (MDPs), and Markov games (MGs). We propose a novel reduction-based framework, which turns any multi-batched algorithm for sequential decision making with instantaneous feedback into a sample-efficient algorithm that can handle stochastic delays in sequential decision making. By plugging different multi-batched algorithms into our framework, we provide several examples demonstrating that our framework not only matches or improves existing results for bandits, tabular MDPs, and tabular MGs, but also provides the first line of studies on delays in sequential decision making with function approximation. In summary, we provide a complete set of sharp results for multi-agent sequential decision making with delayed feedback.
    
[^188]: 受限在线两阶段随机优化：通过对抗学习获得近似最优算法

    Constrained Online Two-stage Stochastic Optimization: Near Optimal Algorithms via Adversarial Learning. (arXiv:2302.00997v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00997](http://arxiv.org/abs/2302.00997)

    在线两阶段随机优化算法的累计目标值最小化，同时保证长期平均第二阶段决策结果属于一个集合。采用对抗性学习算法从在线两阶段问题中开发在线算法，其遗憾界可以降至嵌入对抗性学习算法的遗憾界，并在各种设置下获得了新的结果。

    

    我们考虑一个在线两阶段随机优化问题，其具有有限的$T$期紧约束条件。在每个时间段，我们先作出第一阶段决策，然后观察模型参数的实现，最后从取决于第一阶段决策和模型参数的可行集中做出第二阶段决策。我们旨在最小化累计目标值，同时保证长期平均的第二阶段决策属于一个集合。我们利用对抗性学习算法从在线两阶段问题中开发在线算法。此外，我们算法的遗憾界可以降至嵌入对抗性学习算法的遗憾界。基于我们的框架，在各种设置下我们都获得了新的结果。当每个时间段的模型参数都是从相同的分布中抽取的时候，我们得到了最先进的$O（\sqrt{T}）$遗憾界，这比之前的特殊情况下的界有所提升。我们的算法还可以抵抗模型的敌对性扰动。

    We consider an online two-stage stochastic optimization with long-term constraints over a finite horizon of $T$ periods. At each period, we take the first-stage action, observe a model parameter realization and then take the second-stage action from a feasible set that depends both on the first-stage decision and the model parameter. We aim to minimize the cumulative objective value while guaranteeing that the long-term average second-stage decision belongs to a set. We develop online algorithms for the online two-stage problem from adversarial learning algorithms. Also, the regret bound of our algorithm cam be reduced to the regret bound of embedded adversarial learning algorithms. Based on our framework, we obtain new results under various settings. When the model parameter at each period is drawn from identical distributions, we derive state-of-art $O(\sqrt{T})$ regret that improves previous bounds under special cases. Our algorithm is also robust to adversarial corruptions of model
    
[^189]: 通过最小批量优化传输改进和泛化基于流的生成模型

    Improving and generalizing flow-based generative models with minibatch optimal transport. (arXiv:2302.00482v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00482](http://arxiv.org/abs/2302.00482)

    这篇论文提出了一种称为广义条件流匹配（CFM）的技术，在连续正则化流（CNFs）的生成模型中无需模拟训练，极大提高了效率和稳定性。此外，论文还引入了最优传输CFM（OT-CFM）的变体，可以以无模拟方式计算动态OT，加速了推断过程。

    

    连续正则化流（CNFs）是一种吸引人的生成建模技术，但由于其基于模拟的最大似然训练存在局限性而受到约束。我们介绍了广义条件流匹配（CFM）技术，这是一种针对CNFs的无模拟训练目标的集合。CFM具有类似于扩散模型中用于训练随机流的稳定回归目标，但同时享有确定性流模型的高效推断。与扩散模型和之前的CNF训练算法相比，CFM不需要源分布为高斯分布，也不需要对其密度进行评估。我们的目标的一种变体是最优传输CFM（OT-CFM），它创建了更简单的流，更容易训练，并且导致更快的推断，如我们的实验证明所示。此外，OT-CFM是第一种以无模拟方式计算动态OT的方法。使用CFM训练CNFs可以改进各种条件和...

    Continuous normalizing flows (CNFs) are an attractive generative modeling technique, but they have been held back by limitations in their simulation-based maximum likelihood training. We introduce the generalized conditional flow matching (CFM) technique, a family of simulation-free training objectives for CNFs. CFM features a stable regression objective like that used to train the stochastic flow in diffusion models but enjoys the efficient inference of deterministic flow models. In contrast to both diffusion models and prior CNF training algorithms, CFM does not require the source distribution to be Gaussian or require evaluation of its density. A variant of our objective is optimal transport CFM (OT-CFM), which creates simpler flows that are more stable to train and lead to faster inference, as evaluated in our experiments. Furthermore, OT-CFM is the first method to compute dynamic OT in a simulation-free way. Training CNFs with CFM improves results on a variety of conditional and u
    
[^190]: 主动不确定性减小的安全高效交互规划：一种注重保护的双控制方法

    Active Uncertainty Reduction for Safe and Efficient Interaction Planning: A Shielding-Aware Dual Control Approach. (arXiv:2302.00171v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2302.00171](http://arxiv.org/abs/2302.00171)

    本文提出了一种基于隐式双控制模型的算法方法，用于实现交互式运动规划的主动不确定性减小。方法使用基于采样的随机动态规划近似，解决了交互式运动规划中的优化问题。

    

    准确预测他人行为对于交互式机器人的安全性和效率至关重要。然而，机器人往往无法获得关键信息，如其他代理的目标、注意力和合作意愿。双控制理论通过将预测模型的未知参数视为随机隐藏状态，并在系统运行过程中使用收集的信息推断其值，以应对这一挑战。本文提出了一种基于隐式双控制模型的算法方法，实现了交互式运动规划的主动不确定性减小。我们的方法依赖于基于采样的随机动态规划的近似，从而得到一个可以方便解决的模型预测控制问题。

    The ability to accurately predict others' behavior is central to the safety and efficiency of interactive robotics. Unfortunately, robots often lack access to key information on which these predictions may hinge, such as other agents' goals, attention, and willingness to cooperate. Dual control theory addresses this challenge by treating unknown parameters of a predictive model as stochastic hidden states and inferring their values at runtime using information gathered during system operation. While able to optimally and automatically trade off exploration and exploitation, dual control is computationally intractable for general interactive motion planning. In this paper, we present a novel algorithmic approach to enable active uncertainty reduction for interactive motion planning based on the implicit dual control paradigm. Our approach relies on sampling-based approximation of stochastic dynamic programming, leading to a model predictive control problem that can be readily solved by 
    
[^191]: LEXTREME：多语言和多任务的法律领域基准

    LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain. (arXiv:2301.13126v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.13126](http://arxiv.org/abs/2301.13126)

    LEXTREME是一个多语言和多任务的法律领域基准，该基准提供了11个数据集涵盖24种语言的测评，最佳模型（XLM-R large）在数据集和语言综合评分上均达到了61.3。这使得LEXTREME仍然具有挑战性并且有改进空间。

    

    最近，在transformer架构的显著进展推动下，法律自然语言处理领域取得了惊人的增长。为了衡量进展，精心策划和具有挑战性的基准是至关重要的。然而，大多数基准只能处理英文，而在法律自然语言处理方面尚未有多语言基准可用。此外，许多基准已经饱和，最佳模型明显优于最佳人类，并达到近乎完美的分数。我们调查了法律自然语言处理文献，并选择了11个涵盖24种语言的数据集，创建了LEXTREME。为了进行公平比较，我们提出了两种综合评分，一种基于数据集，一种基于语言。最佳基线模型（XLM-R large）在数据集综合评分和语言综合评分上均达到了61.3。这表明LEXTREME仍然非常具有挑战性，并且为改进留下了充足空间。为了方便研究人员和实践者使用，我们将LEXTREME与所有数据一起发布在huggingface上。

    Lately, propelled by the phenomenal advances around the transformer architecture, the legal NLP field has enjoyed spectacular growth. To measure progress, well curated and challenging benchmarks are crucial. However, most benchmarks are English only and in legal NLP specifically there is no multilingual benchmark available yet. Additionally, many benchmarks are saturated, with the best models clearly outperforming the best humans and achieving near perfect scores. We survey the legal NLP literature and select 11 datasets covering 24 languages, creating LEXTREME. To provide a fair comparison, we propose two aggregate scores, one based on the datasets and one on the languages. The best baseline (XLM-R large) achieves both a dataset aggregate score a language aggregate score of 61.3. This indicates that LEXTREME is still very challenging and leaves ample room for improvement. To make it easy for researchers and practitioners to use, we release LEXTREME on huggingface together with all the
    
[^192]: 学习必要和充分因果图

    On Learning Necessary and Sufficient Causal Graphs. (arXiv:2301.12389v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12389](http://arxiv.org/abs/2301.12389)

    本文提出了一种学习必要和充分因果图的方法，用于发现与感兴趣结果相关的因果关系。

    

    因果革命激发了对各个领域中复杂关系的兴趣。大多数现有方法旨在在复杂的大规模图中发现所有变量之间的因果关系。然而，在实践中，图中仅有的一小部分变量与感兴趣的结果相关。因此，使用完整的因果图进行因果估计，特别是在数据有限的情况下，可能会导致大量错误发现的虚假变量，这些虚假变量与目标结果高度相关，但对目标结果没有因果影响。在本文中，我们提出了一种学习必要和充分因果图 (NSCG) 的方法，该方法专门由与感兴趣结果因果相关的变量组成，我们将其称为因果特征。关键思想是利用因果概率系统地评估因果图中特征的重要性，从而帮助我们确定与感兴趣的结果相关的子图。

    The causal revolution has stimulated interest in understanding complex relationships in various fields. Most of the existing methods aim to discover causal relationships among all variables within a complex large-scale graph. However, in practice, only a small subset of variables in the graph are relevant to the outcomes of interest. Consequently, causal estimation with the full causal graph -- particularly given limited data -- could lead to numerous falsely discovered, spurious variables that exhibit high correlation with, but exert no causal impact on, the target outcome. In this paper, we propose learning a class of necessary and sufficient causal graphs (NSCG) that exclusively comprises causally relevant variables for an outcome of interest, which we term causal features. The key idea is to employ probabilities of causation to systematically evaluate the importance of features in the causal graph, allowing us to identify a subgraph relevant to the outcome of interest. To learn NSC
    
[^193]: 使用图像合成进行光照变化校正的无监督领域自适应人物再识别

    Illumination Variation Correction Using Image Synthesis For Unsupervised Domain Adaptive Person Re-Identification. (arXiv:2301.09702v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2301.09702](http://arxiv.org/abs/2301.09702)

    本文提出了一个Synthesis Model Bank（SMB）来处理无监督领域自适应人物再识别中的光照变化。SMB包括卷积神经网络（CNN）和马氏距离矩阵，通过使用不同光照条件的合成数据进行训练，增强了光照变化的鲁棒性。

    

    无监督领域自适应（UDA）人物再识别旨在从源领域的标记图像中学习身份信息，并将其应用于目标领域的未标记图像。许多无监督再识别方法的一个主要问题是它们在大的领域变化（如光照、视角和遮挡）方面表现不佳。本文提出了一个合成模型库（SMB）来处理无监督人物再识别中的光照变化。所提出的SMB包括用于特征提取的多个卷积神经网络（CNN）和用于距离度量的马氏距离矩阵。它们使用具有不同光照条件的合成数据进行训练，这使得SMB对光照变化具有鲁棒性。为了更好地量化光照强度并提高合成图像的质量，我们引入了一个基于GAN的新型三维虚拟人类数据集进行图像合成。通过实验，我们证明了所提出方法的有效性。

    Unsupervised domain adaptive (UDA) person re-identification (re-ID) aims to learn identity information from labeled images in source domains and apply it to unlabeled images in a target domain. One major issue with many unsupervised re-identification methods is that they do not perform well relative to large domain variations such as illumination, viewpoint, and occlusions. In this paper, we propose a Synthesis Model Bank (SMB) to deal with illumination variation in unsupervised person re-ID. The proposed SMB consists of several convolutional neural networks (CNN) for feature extraction and Mahalanobis matrices for distance metrics. They are trained using synthetic data with different illumination conditions such that their synergistic effect makes the SMB robust against illumination variation. To better quantify the illumination intensity and improve the quality of synthetic images, we introduce a new 3D virtual-human dataset for GAN-based image synthesis. From our experiments, the pr
    
[^194]: 无接触红外光波传感的呼吸异常检测

    Non-contact Respiratory Anomaly Detection using Infrared Light-wave Sensing. (arXiv:2301.03713v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2301.03713](http://arxiv.org/abs/2301.03713)

    本研究使用无接触红外光波传感技术，通过训练不同类型的呼吸模式来检测呼吸异常，并且通过验证数据的呼吸波形丢弃干扰数据，以实现安全、高效和无创的人体呼吸监测。

    

    人体的呼吸频率和呼吸模式传达了关于主体的身体和心理状态的重要信息。异常呼吸可能表明严重的健康问题，需要进一步诊断和治疗。使用非相干红外光的无线光波传感（LWS）在不引起隐私问题的情况下，显示了安全、隐蔽、高效和无创的人体呼吸监测的潜力。呼吸监测系统需要在不同类型的呼吸模式上进行训练，以识别呼吸异常。该系统还必须验证所收集的数据是否为呼吸波形，丢弃由外部干扰、用户移动或系统故障引起的任何错误数据。为了解决这些需求，本研究使用模拟人类呼吸模式的机器人，模拟了正常和不同类型的异常呼吸。然后，使用红外光波传感技术收集了时间序列呼吸数据。在此基础上，用三种机器学习算法进行了呼吸异常检测。

    Human respiratory rate and its pattern convey essential information about the physical and psychological states of the subject. Abnormal breathing can indicate fatal health issues leading to further diagnosis and treatment. Wireless light-wave sensing (LWS) using incoherent infrared light shows promise in safe, discreet, efficient, and non-invasive human breathing monitoring without raising privacy concerns. The respiration monitoring system needs to be trained on different types of breathing patterns to identify breathing anomalies.The system must also validate the collected data as a breathing waveform, discarding any faulty data caused by external interruption, user movement, or system malfunction. To address these needs, this study simulated normal and different types of abnormal respiration using a robot that mimics human breathing patterns. Then, time-series respiration data were collected using infrared light-wave sensing technology. Three machine learning algorithms, decision t
    
[^195]: 通过非参数子图匹配重新思考解释图神经网络

    Rethinking Explaining Graph Neural Networks via Non-parametric Subgraph Matching. (arXiv:2301.02780v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.02780](http://arxiv.org/abs/2301.02780)

    本文通过提出一种非参数子图匹配框架MatchExplainer，可以解决图神经网络的解释性问题。此框架将目标图与其他实例结合起来，通过最小化节点对应的距离来鉴别最关键的联合子结构，并提出了一种新的增强范式MatchDrop来解决误报采样问题。

    

    图神经网络（GNNs）的成功引发了关于可解释性的问题：“输入图的哪一部分对预测最为决定性？”特别是，由于其更强大的解读黑箱（即目标GNNs）能力，参数化解释器在现有方法中占据主导地位。基于观察到图通常共享某些常见的模式，本文提出了一种新颖的非参数子图匹配框架MatchExplainer来探索解释性子图。它将目标图与其他相应实例结合起来，通过最小化基于节点对应的距离来识别最关键的联合子结构。此外，我们注意到现有的图采样或节点删除方法通常会遇到误报采样问题。为了缓解这个问题，我们设计了一种名为MatchDrop的新增方案，它利用了MatchExplainer来修复图的最信息丰富部分。

    The success of graph neural networks (GNNs) provokes the question about explainability: ``Which fraction of the input graph is the most determinant of the prediction?'' Particularly, parametric explainers prevail in existing approaches because of their more robust capability to decipher the black-box (i.e., target GNNs). In this paper, based on the observation that graphs typically share some common motif patterns, we propose a novel non-parametric subgraph matching framework, dubbed MatchExplainer, to explore explanatory subgraphs. It couples the target graph with other counterpart instances and identifies the most crucial joint substructure by minimizing the node corresponding-based distance. Moreover, we note that present graph sampling or node-dropping methods usually suffer from the false positive sampling problem. To alleviate this issue, we designed a new augmentation paradigm named MatchDrop. It takes advantage of MatchExplainer to fix the most informative portion of the graph 
    
[^196]: 关于可解释性强化学习的综述：概念、算法和挑战

    A Survey on Explainable Reinforcement Learning: Concepts, Algorithms, Challenges. (arXiv:2211.06665v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.06665](http://arxiv.org/abs/2211.06665)

    该综述调查了可解释性强化学习方法，介绍了模型解释、奖励解释、状态解释和任务解释方法，并探讨了解释强化学习的概念、算法和挑战。

    

    强化学习是一种流行的机器学习范式，智能代理与环境进行交互以实现长期目标。在深度学习的复兴推动下，深度强化学习在各种复杂控制任务中取得了巨大成功。尽管取得了令人鼓舞的结果，基于深度神经网络的主干结构被普遍视为黑盒子，阻碍了从业者在安全性和可靠性至关重要的真实场景中信任和使用训练代理。为了缓解这个问题，大量的文献致力于揭示智能代理的内部工作原理，通过构建内在可解释性或事后可解释性。在本综述中，我们对现有的可解释性强化学习方法进行了全面的回顾，并引入了一个新的分类法，将先前的工作明确地分为模型解释、奖励解释、状态解释和任务解释方法。

    Reinforcement Learning (RL) is a popular machine learning paradigm where intelligent agents interact with the environment to fulfill a long-term goal. Driven by the resurgence of deep learning, Deep RL (DRL) has witnessed great success over a wide spectrum of complex control tasks. Despite the encouraging results achieved, the deep neural network-based backbone is widely deemed as a black box that impedes practitioners to trust and employ trained agents in realistic scenarios where high security and reliability are essential. To alleviate this issue, a large volume of literature devoted to shedding light on the inner workings of the intelligent agents has been proposed, by constructing intrinsic interpretability or post-hoc explainability. In this survey, we provide a comprehensive review of existing works on eXplainable RL (XRL) and introduce a new taxonomy where prior works are clearly categorized into model-explaining, reward-explaining, state-explaining, and task-explaining methods
    
[^197]: GmGM: 一种快速的多轴高斯图形模型。

    GmGM: a Fast Multi-Axis Gaussian Graphical Model. (arXiv:2211.02920v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.02920](http://arxiv.org/abs/2211.02920)

    本文介绍了一种快速的多轴高斯图形模型，用于构建稀疏图形表示。相比先前工作，我们的算法在每个轴上仅使用一次特征分解，实现了数量级的加速。该模型可以应用于大型多模态数据集，包括单细胞多组学数据。

    

    本文介绍了一种高斯多图形模型，用于构建矩阵和张量变量数据的稀疏图形表示。我们通过同时学习多个共享轴的张量上的表示来推广该领域的先前工作，这对于分析多模态数据集（如多组学中遇到的数据集）是必要的。我们的算法在每个轴上仅使用一次特征分解，相对于非广义情况下的先前工作实现了数量级的加速。这使得我们的方法可以应用于包括单细胞多组学数据在内的大型多模态数据集，这在之前的方法中具有挑战性。我们在合成数据和五个真实数据集上验证了我们的模型。

    This paper introduces the Gaussian multi-Graphical Model, a model to construct sparse graph representations of matrix- and tensor-variate data. We generalize prior work in this area by simultaneously learning this representation across several tensors that share axes, which is necessary to allow the analysis of multimodal datasets such as those encountered in multi-omics. Our algorithm uses only a single eigendecomposition per axis, achieving an order of magnitude speedup over prior work in the ungeneralized case. This allows the use of our methodology on large multi-modal datasets such as single-cell multi-omics data, which was challenging with previous approaches. We validate our model on synthetic data and five real-world datasets.
    
[^198]: 基于扩散过程的熵神经最优传输算法

    Entropic Neural Optimal Transport via Diffusion Processes. (arXiv:2211.01156v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01156](http://arxiv.org/abs/2211.01156)

    这项研究提出了一种新的神经算法，用于计算连续概率分布之间的熵最优传输(EOT)计划，它具有端到端学习、快速推理和处理小值熵正则化系数的优点，可在大规模EOT任务中发挥出色的性能。

    

    我们提出了一种新颖的神经算法，用于计算连续概率分布之间的熵最优传输(EOT)计划，这些分布可通过样本获得。我们的算法基于动态版本EOT的鞍点重构，即Schrödinger桥问题。与大规模EOT的先前方法相比，我们的算法是端到端的，由单个学习步骤组成，具有快速的推理过程，并允许处理熵正则化系数的小值，这在某些实际应用问题中非常重要。在实证方面，我们展示了该方法在几个大规模EOT任务中的性能。

    We propose a novel neural algorithm for the fundamental problem of computing the entropic optimal transport (EOT) plan between continuous probability distributions which are accessible by samples. Our algorithm is based on the saddle point reformulation of the dynamic version of EOT which is known as the Schr\"odinger Bridge problem. In contrast to the prior methods for large-scale EOT, our algorithm is end-to-end and consists of a single learning step, has fast inference procedure, and allows handling small values of the entropy regularization coefficient which is of particular importance in some applied problems. Empirically, we show the performance of the method on several large-scale EOT tasks.
    
[^199]: NAS在激活和跳跃连接搜索下的泛化性质

    Generalization Properties of NAS under Activation and Skip Connection Search. (arXiv:2209.07238v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.07238](http://arxiv.org/abs/2209.07238)

    本文研究了NAS在激活和跳跃连接搜索下的泛化性质，并提出了一种无需训练的基于理论的算法来选择性能最好的架构。

    

    神经网络结构搜索（NAS）促进了自动发现最先进的神经网络结构。尽管NAS取得了一些进展，但对于NAS的理论保证关注甚少。本文研究了NAS在统一框架下的泛化性质，包括了（深层）层级跳跃连接搜索和激活函数搜索。为此，我们利用包括混合激活函数、全连接和残差神经网络在内的特定搜索空间，推导出（无）限宽度情况下神经切向核（NTK）的最小特征值的下（上）界。我们使用最小特征值来建立NAS在随机梯度下降训练中的泛化误差界限。重要的是，我们理论上和实验上展示了如何根据我们推导出的结果引导NAS选择性能最好的架构，即使在无需训练的情况下，这是一种基于我们的理论的无需训练的算法。

    Neural Architecture Search (NAS) has fostered the automatic discovery of state-of-the-art neural architectures. Despite the progress achieved with NAS, so far there is little attention to theoretical guarantees on NAS. In this work, we study the generalization properties of NAS under a unifying framework enabling (deep) layer skip connection search and activation function search. To this end, we derive the lower (and upper) bounds of the minimum eigenvalue of the Neural Tangent Kernel (NTK) under the (in)finite-width regime using a certain search space including mixed activation functions, fully connected, and residual neural networks. We use the minimum eigenvalue to establish generalization error bounds of NAS in the stochastic gradient descent training. Importantly, we theoretically and experimentally show how the derived results can guide NAS to select the top-performing architectures, even in the case without training, leading to a train-free algorithm based on our theory. Accordi
    
[^200]: 在可约损失中为强化学习优先选择样本

    Prioritizing Samples in Reinforcement Learning with Reducible Loss. (arXiv:2208.10483v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.10483](http://arxiv.org/abs/2208.10483)

    本文提出了一种在强化学习中基于可学习性的方法来优先选择样本，通过稳定降低样本的训练损失来定义样本的可学习性。实验证明，该方法相比于随机抽样和仅根据训练损失进行优先选择的方法更加稳健。

    

    大多数强化学习算法利用经验回放缓冲区反复训练代理已观察到的样本。并非所有样本具有相同的重要性，简单地赋予每个样本相等的重要性是一种天真的策略。在本文中，我们提出一种基于我们可以从样本中学到多少的方法来优先选择样本。我们将样本的可学习性定义为与样本相关的训练损失随时间持续下降的程度。我们开发了一种算法来优先选择具有较高可学习性的样本，同时将较难学习的样本（通常由噪声或随机性引起）赋予较低的优先级。我们通过实验证明，我们的方法比随机抽样更加稳健，也优于仅根据训练损失（即时间差分损失）进行优先选择，这在优先经验回放中使用。

    Most reinforcement learning algorithms take advantage of an experience replay buffer to repeatedly train on samples the agent has observed in the past. Not all samples carry the same amount of significance and simply assigning equal importance to each of the samples is a na\"ive strategy. In this paper, we propose a method to prioritize samples based on how much we can learn from a sample. We define the learn-ability of a sample as the steady decrease of the training loss associated with this sample over time. We develop an algorithm to prioritize samples with high learn-ability, while assigning lower priority to those that are hard-to-learn, typically caused by noise or stochasticity. We empirically show that our method is more robust than random sampling and also better than just prioritizing with respect to the training loss, i.e. the temporal difference loss, which is used in prioritized experience replay.
    
[^201]: 论公平机器学习中因果关系的必要性和适用性

    On the Need and Applicability of Causality for Fair Machine Learning. (arXiv:2207.04053v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.04053](http://arxiv.org/abs/2207.04053)

    本论文探讨了因果关系在公平机器学习中的必要性和适用性，强调了非因果预测的社会影响和法律反歧视过程依赖于因果主张。同时讨论了在实际场景中应用因果关系所面临的挑战和限制，并提出了可能的解决方案。

    

    除了在流行病学、政治和社会科学中的常见应用案例外，事实证明因果关系在评估自动决策的公正性方面十分重要，无论是在法律上还是日常生活中。我们提供了关于为何因果关系对公平性评估尤为重要的论点和示例。特别是，我们指出了非因果预测的社会影响以及依赖因果主张的法律反歧视过程。我们最后讨论了应用因果关系在实际场景中的挑战和局限性，以及可能的解决方案。

    Besides its common use cases in epidemiology, political, and social sciences, causality turns out to be crucial in evaluating the fairness of automated decisions, both in a legal and everyday sense. We provide arguments and examples, of why causality is particularly important for fairness evaluation. In particular, we point out the social impact of non-causal predictions and the legal anti-discrimination process that relies on causal claims. We conclude with a discussion about the challenges and limitations of applying causality in practical scenarios as well as possible solutions.
    
[^202]: 用横向生成对抗网络评估合成的三维PET成像中的隐私泄漏

    Assessing Privacy Leakage in Synthetic 3-D PET Imaging using Transversal GAN. (arXiv:2206.06448v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2206.06448](http://arxiv.org/abs/2206.06448)

    通过研究三维PET图像生成模型Transversal GAN（TrGAN），我们发现TrGAN的判别器易受到攻击，攻击者可以准确地识别出训练样本，并在有限的访问权限情况下推断出受保护数据的特征。

    

    由于隐私问题，在医学图像上训练与计算机视觉相关的算法进行疾病诊断或图像分割是困难的。因此，生成图像模型被广泛使用以促进数据共享。然而，三维生成模型的研究较少，需要对其隐私泄漏进行调查。我们介绍了我们的三维生成模型Transversal GAN (TrGAN)，使用头颈PET图像作为案例研究，并以肿瘤掩蔽为条件。我们为我们的模型定义了图像准确性、实用性和隐私的定量度量标准。这些度量标准在训练过程中进行评估，以确定理想的准确性、实用性和隐私折衷，并建立这些参数之间的关系。我们显示TrGAN的判别器容易受到攻击，攻击者几乎可以完全准确地识别出哪些样本用于训练（AUC = 0.99）。我们还展示，即使只有对生成模型有限的访问权限的攻击者也可以推断出受保护数据的一些特征。

    Training computer-vision related algorithms on medical images for disease diagnosis or image segmentation is difficult in large part due to privacy concerns. For this reason, generative image models are highly sought after to facilitate data sharing. However, 3-D generative models are understudied, and investigation of their privacy leakage is needed. We introduce our 3-D generative model, Transversal GAN (TrGAN), using head & neck PET images which are conditioned on tumour masks as a case study. We define quantitative measures of image fidelity, utility and privacy for our model. These metrics are evaluated in the course of training to identify ideal fidelity, utility and privacy trade-offs and establish the relationships between these parameters. We show that the discriminator of the TrGAN is vulnerable to attack, and that an attacker can identify which samples were used in training with almost perfect accuracy (AUC = 0.99). We also show that an attacker with access to only the gener
    
[^203]: 马尔科夫潜在博弈中的独立和去中心化学习

    Independent and Decentralized Learning in Markov Potential Games. (arXiv:2205.14590v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.14590](http://arxiv.org/abs/2205.14590)

    独立的去中心化学习在马尔科夫潜在博弈中有效，通过更新Q函数可以引导策略收敛到稳定的纳什平衡点。

    

    我们提出了一种多智能体强化学习机制，并分析了它在无限时间折扣马尔科夫潜在博弈中的收敛性。我们专注于独立和去中心化的设置，在这种设置下，玩家不了解游戏模型，也不能进行协调。在每个阶段，玩家通过异步方式更新他们的打扰Q函数的估计值，该函数根据实现的一阶段奖励评估他们的总体条件付款。然后，玩家通过将基于估计Q函数的平滑最优一阶段偏差策略纳入其策略中来独立地更新其策略。学习动态的关键特征是Q函数估计是以比策略更快的时间尺度进行更新的。我们证明了我们的学习动态引导的策略在概率1的情况下收敛到马尔科夫潜在博弈的稳定纳什平衡。我们的结果凸显了简单学习动态在达到马尔可夫潜在博弈的稳定纳什平衡方面的功效，即使是在独立和去中心化代理环境中。

    We propose a multi-agent reinforcement learning dynamics, and analyze its convergence in infinite-horizon discounted Markov potential games. We focus on the independent and decentralized setting, where players do not have knowledge of the game model and cannot coordinate. In each stage, players update their estimate of a perturbed Q-function that evaluates their total contingent payoff based on the realized one-stage reward in an asynchronous manner. Then, players independently update their policies by incorporating a smoothed optimal one-stage deviation strategy based on the estimated Q-function. A key feature of the learning dynamics is that the Q-function estimates are updated at a faster timescale than the policies. We prove that the policies induced by our learning dynamics converge to a stationary Nash equilibrium in Markov potential games with probability 1. Our results highlight the efficacy of simple learning dynamics in reaching a stationary Nash equilibrium even in environme
    
[^204]: BagPipe：加速深度推荐模型训练

    BagPipe: Accelerating Deep Recommendation Model Training. (arXiv:2202.12429v3 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2202.12429](http://arxiv.org/abs/2202.12429)

    本文提出了BagPipe，一种用于加速深度推荐模型训练的系统。该系统利用嵌入访问的特定结构，通过缓存和预取的方式优化训练，实现了对推荐模型的高效训练。

    

    基于深度学习的推荐模型（DLRM）广泛应用于几个关键的商业应用。高效地训练这种推荐模型具有挑战性，因为它们包含数十亿个基于嵌入的参数，从嵌入访问中导致了显著的开销。通过对现有的DLRM训练系统进行分析，我们观察到约75％的迭代时间用于嵌入访问和模型同步。本文的关键见解是嵌入访问具有特定的结构，可以用于加速训练。我们观察到嵌入访问具有严重的偏斜性，约1％的嵌入表示了超过92％的总访问量。此外，我们观察到在离线训练期间，我们可以预测未来批次来确定将在将来的何时迭代需要哪些嵌入。基于这些见解，我们开发了Bagpipe，一种用于训练深度推荐模型的系统，该系统利用缓存和预取来优化训练。

    Deep learning based recommendation models (DLRM) are widely used in several business critical applications. Training such recommendation models efficiently is challenging because they contain billions of embedding-based parameters, leading to significant overheads from embedding access. By profiling existing systems for DLRM training, we observe that around 75\% of the iteration time is spent on embedding access and model synchronization. Our key insight in this paper is that embedding access has a specific structure which can be used to accelerate training. We observe that embedding accesses are heavily skewed, with around 1\% of embeddings representing more than 92\% of total accesses. Further, we observe that during offline training we can lookahead at future batches to determine exactly which embeddings will be needed at what iteration in the future. Based on these insights, we develop Bagpipe, a system for training deep recommendation models that uses caching and prefetching to ov
    
[^205]: SCORE：用于离线强化学习的虚假相关性降低

    SCORE: Spurious COrrelation REduction for Offline Reinforcement Learning. (arXiv:2110.12468v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.12468](http://arxiv.org/abs/2110.12468)

    本文提出了SCORE算法，用于离线强化学习中的虚假相关性降低。通过引入退火行为克隆正则化器，SCORE实现了SoTA性能，并消除了次优性中的虚假相关性。

    

    离线强化学习（RL）利用大规模数据集解决序贯决策问题。大多数现有论文只讨论了对抗分布外（OOD）行为的防御，而本文研究了更广泛的问题，即认知不确定性与决策之间的虚假相关性，这是导致次优性的一个重要因素。本文提出了一种实用有效且理论上可证明的算法：用于离线RL的虚假相关性降低（SCORE）。我们通过实验证明，SCORE在标准基准（D4RL）上的各种任务中以3.1倍加速率实现了SoTA性能。所提算法引入了一个退火行为克隆正则化器来帮助生成高质量的不确定性估计，这对于消除次优性中的虚假相关性至关重要。理论上，我们证明了所提方法的合理性，并证明了在温和的条件下其收敛到最优策略的次线性率。

    Offline reinforcement learning (RL) harnesses the power of massive datasets for resolving sequential decision problems. Most existing papers only discuss defending against out-of-distribution (OOD) actions while we investigate a broader issue, the spurious correlations between epistemic uncertainty and decision-making, an essential factor that causes suboptimality. In this paper, we propose Spurious COrrelation REduction (SCORE) for offline RL, a practically effective and theoretically provable algorithm. We empirically show that SCORE achieves the SoTA performance with 3.1x acceleration on various tasks in a standard benchmark (D4RL). The proposed algorithm introduces an annealing behavior cloning regularizer to help produce a high-quality estimation of uncertainty which is critical for eliminating spurious correlations from suboptimality. Theoretically, we justify the rationality of the proposed method and prove its convergence to the optimal policy with a sublinear rate under mild a
    
[^206]: 针对非凸-凹极小极大问题的无导数交替投影算法

    Derivative-free Alternating Projection Algorithms for General Nonconvex-Concave Minimax Problems. (arXiv:2108.00473v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2108.00473](http://arxiv.org/abs/2108.00473)

    本文提出针对非凸-凹极小极大问题的无导数交替投影算法，包括光滑问题的交替随机梯度投影算法（ZO-AGP），以及块状非光滑问题的分块交替随机近端梯度算法（ZO-BAPG）。这些算法具有较少的函数值估计和较高的迭代复杂度。

    

    本文研究了非凸-凹极小极大问题的零阶算法，这类问题近年在机器学习、信号处理等领域引起了广泛关注。我们提出了一种零阶交替随机梯度投影（ZO-AGP）算法来解决光滑的非凸-凹极小极大问题，其迭代复杂度为 $\mathcal{O}(\varepsilon^{-4})$，每次迭代的函数值估计次数为 $\mathcal{O}(d_{x}+d_{y})$。此外，我们还提出了一种零阶分块交替随机近端梯度算法（ZO-BAPG）来解决块状非光滑的非凸-凹极小极大优化问题，其迭代复杂度为 $\mathcal{O}(\varepsilon^{-4})$，每次迭代的函数值估计次数为 $\mathcal{O}(K d_{x}+d_{y})$。据我们所知，这是首次提出这些算法。

    In this paper, we study zeroth-order algorithms for nonconvex-concave minimax problems, which have attracted widely attention in machine learning, signal processing and many other fields in recent years. We propose a zeroth-order alternating randomized gradient projection (ZO-AGP) algorithm for smooth nonconvex-concave minimax problems, and its iteration complexity to obtain an $\varepsilon$-stationary point is bounded by $\mathcal{O}(\varepsilon^{-4})$, and the number of function value estimation is bounded by $\mathcal{O}(d_{x}+d_{y})$ per iteration. Moreover, we propose a zeroth-order block alternating randomized proximal gradient algorithm (ZO-BAPG) for solving block-wise nonsmooth nonconvex-concave minimax optimization problems, and the iteration complexity to obtain an $\varepsilon$-stationary point is bounded by $\mathcal{O}(\varepsilon^{-4})$ and the number of function value estimation per iteration is bounded by $\mathcal{O}(K d_{x}+d_{y})$. To the best of our knowledge, this 
    
[^207]: 无似然假设下基于频率学派推断：具有正确条件覆盖的置信区间

    Likelihood-Free Frequentist Inference: Confidence Sets with Correct Conditional Coverage. (arXiv:2107.03920v6 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2107.03920](http://arxiv.org/abs/2107.03920)

    本文提出了无似然假设下的频率学派推断（LF2I）框架，通过结合经典统计和现代机器学习，实现了构建具有正确条件覆盖的置信区间的实用程序和诊断方法，在包括宇宙学参数推断在内的多个例子中都实现了覆盖性质得到大幅改善。

    

    许多科学领域都广泛使用计算机模拟器以隐含复杂系统的似然函数。传统的统计方法并不适用于这些称为无似然假设下推断（LFI）的情况，尤其是在渐近和低维的条件下。虽然新的机器学习方法，如归一化流，已经革新了LFI方法的样本效率和容量，但它们是否能为小样本大小产生具有正确条件覆盖的置信区间，仍然是一个开放问题。本文将经典统计和现代机器学习相结合，提出了（i）具有有限样本保证名义覆盖的内曼区间建设的实用程序，以及（ii）估计整个参数空间的条件覆盖的诊断。我们将我们的框架称为无似然假设下的频率学派推断（LF2I）。我们的框架可以使用定义测试统计量的任何方法，如似然比，因此具有广泛的适用性。我们将我们的方法应用于几个合成和实际的例子，包括宇宙学参数推断，并证明与现有的LFI方法相比，覆盖性质得到了大幅改善。

    Many areas of science make extensive use of computer simulators that implicitly encode likelihood functions of complex systems. Classical statistical methods are poorly suited for these so-called likelihood-free inference (LFI) settings, particularly outside asymptotic and low-dimensional regimes. Although new machine learning methods, such as normalizing flows, have revolutionized the sample efficiency and capacity of LFI methods, it remains an open question whether they produce confidence sets with correct conditional coverage for small sample sizes. This paper unifies classical statistics with modern machine learning to present (i) a practical procedure for the Neyman construction of confidence sets with finite-sample guarantees of nominal coverage, and (ii) diagnostics that estimate conditional coverage over the entire parameter space. We refer to our framework as likelihood-free frequentist inference (LF2I). Any method that defines a test statistic, like the likelihood ratio, can 
    
[^208]: 强化学习中对抗性腐败的鲁棒探索研究

    Corruption-robust exploration in episodic reinforcement learning. (arXiv:1911.08689v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1911.08689](http://arxiv.org/abs/1911.08689)

    该论文研究了强化学习中在奖励和转移概率两方面存在的对抗性腐败问题，并提出了一种能够解决腐败问题的高效算法，能够在没有腐败的情况下实现接近最优的后悔，并且能够适应未知水平的腐败。

    

    我们对多阶段强化学习在奖励和转移概率两方面的对抗性腐败进行了研究，扩展了最近对随机赌博机特例的研究结果。我们提供了一个框架，通过将“乐观面对不确定性”的现有强化学习方法进行探索性改进，并结合“动作淘汰”原则，从而解决了在强化学习环境中朴素应用动作淘汰所面临的主要挑战。我们的框架提供了高效的算法，(a)在没有腐败时实现接近最优的后悔，并且(b)能够适应未知水平的腐败，在总腐败情况下后悔程度逐渐降低。为了展示我们方法的广泛适用性，我们推导了表格设置下的结果（其中涉及状态和行为）以及通用函数逼近设置下的结果。

    We initiate the study of multi-stage episodic reinforcement learning under adversarial corruptions in both the rewards and the transition probabilities of the underlying system extending recent results for the special case of stochastic bandits. We provide a framework which modifies the aggressive exploration enjoyed by existing reinforcement learning approaches based on "optimism in the face of uncertainty", by complementing them with principles from "action elimination". Importantly, our framework circumvents the major challenges posed by naively applying action elimination in the RL setting, as formalized by a lower bound we demonstrate. Our framework yields efficient algorithms which (a) attain near-optimal regret in the absence of corruptions and (b) adapt to unknown levels corruption, enjoying regret guarantees which degrade gracefully in the total corruption encountered. To showcase the generality of our approach, we derive results for both tabular settings (where states and act
    

