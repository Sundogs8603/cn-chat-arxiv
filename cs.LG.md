# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [The Disharmony Between BN and ReLU Causes Gradient Explosion, but is Offset by the Correlation Between Activations.](http://arxiv.org/abs/2304.11692) | 本研究阐述了BN和ReLU之间的不和谐是导致梯度爆炸的主要原因，同时发现输入之间的相关性可以缓解这个问题。提出一种基于二阶优化算法的自适应学习率算法，在大批量训练中表现优异，并可替代WarmUp，在小批量训练中也表现不错。 |
| [^2] | [A Lightweight Recurrent Learning Network for Sustainable Compressed Sensing.](http://arxiv.org/abs/2304.11674) | 本文提出了一种轻量级但高效的深度神经网络，基于循环学习来实现可持续的压缩感知系统；它需要更少的参数，但可以获得高质量的重构结果。 |
| [^3] | [Device management and network connectivity as missing elements in TinyML landscape.](http://arxiv.org/abs/2304.11669) | TinyML面临硬件异构性、MCU架构和资源可用性限制等多重挑战。本文强调解决TinyML解决方案的网络连接问题需要更加注重标准协议、解决方案的互操作性及安全性，并介绍了LwM2M协议如何解决这些挑战。 |
| [^4] | [Accelerated Doubly Stochastic Gradient Algorithm for Large-scale Empirical Risk Minimization.](http://arxiv.org/abs/2304.11665) | 本文提出了一种针对大规模经验风险最小化问题的加速双随机梯度算法，采用新型加速多动量技术，每次迭代仅访问一小批样本和更新少量变量坐标，具有快速收敛和小内存占用特点。 |
| [^5] | [Efficient Training of Deep Equilibrium Models.](http://arxiv.org/abs/2304.11663) | 本文介绍一种简单而有效的策略来避免深度平衡模型层中反向传播的计算负担。该方法可以显著加速训练，同时不会影响性能。 |
| [^6] | [Capturing Fine-grained Semantics in Contrastive Graph Representation Learning.](http://arxiv.org/abs/2304.11658) | 本文提出了一种新的增强细粒度语义对比学习方法（FSGCL），首先使用基于图案的图构造方法从输入数据中提取多种语义，在此基础上探索语义级对比任务，从而提高图对比学习的性能。 |
| [^7] | [Stochastic Cell Transmission Models of Traffic Networks.](http://arxiv.org/abs/2304.11654) | 本文介绍了一个适用于交通网络的随机单元传输模型，通过偏好函数和可接受设计来评估交通系统的性能。数值实现结合了模拟、高斯过程回归和随机探索过程。 |
| [^8] | [An Asynchronous Decentralized Algorithm for Wasserstein Barycenter Problem.](http://arxiv.org/abs/2304.11653) | 本文提出了一种用于 WBP 的异步分散式算法 A$^2$DWB，该算法通过更新本地变量以减轻等待开销，从而极大地提高了时间效率，实验证明其优于最新的同步算法。 |
| [^9] | [SATIN: A Multi-Task Metadataset for Classifying Satellite Imagery using Vision-Language Models.](http://arxiv.org/abs/2304.11619) | 本研究介绍了一个遥感图像元数据集SATIN，它由27个现有的遥感数据集组成，并使用一系列视觉-语言（VL）模型全面评估了它的零-shot转移分类能力。该研究发现SATIN是一个具有挑战性的基准测试，强大方法的分类精度为52.0％，并提供了一个公共排行榜以跟踪模型的进展。 |
| [^10] | [Transductive Few-shot Learning with Prototype-based Label Propagation by Iterative Graph Refinement.](http://arxiv.org/abs/2304.11598) | 本文提出一种基于原型的标签传播方法，通过迭代图形细化解决了传统方法中原型估计不准确和核函数下亚优图形构建的问题，在少样迁移学习和半监督FSL方面表现优异。 |
| [^11] | [Learning Partial Correlation based Deep Visual Representation for Image Classification.](http://arxiv.org/abs/2304.11597) | 本文提出了一种基于偏相关的深度视觉表示学习方法，解决了使用协方差矩阵表征相关性在存在混淆效应时的误导问题。 |
| [^12] | [Segment Anything in Non-Euclidean Domains: Challenges and Opportunities.](http://arxiv.org/abs/2304.11595) | 本文探讨了一种新的Segment Non-Euclidean Anything（SNA）方法，旨在扩大语义分割的范围，导致更多的基础模型用于非欧几里得域中的分割任务。 |
| [^13] | [System III: Learning with Domain Knowledge for Safety Constraints.](http://arxiv.org/abs/2304.11593) | 本文提出了一种称为System III的新颖框架，该框架将安全性的领域专家知识表示为一阶逻辑形式，通过p范数评估约束的满足程度，用于在安全关键领域中整合领域知识以帮助引导安全探索，并提高样本效率。实验证明该方法在一个模拟自主驾驶任务上有效。 |
| [^14] | [Diffusion Model for GPS Trajectory Generation.](http://arxiv.org/abs/2304.11582) | 该论文提出了基于扩散模型的GPS轨迹生成框架，通过将真实轨迹逐渐转换为噪声，再从噪声重构伪造的轨迹，以达到生成隐私信息保护的高质量轨迹的目的。 |
| [^15] | [Identifying Stochasticity in Time-Series with Autoencoder-Based Content-aware 2D Representation: Application to Black Hole Data.](http://arxiv.org/abs/2304.11560) | 通过自编码器和内容感知2D表示，该论文提出了一个识别随机性的方法并对黑洞数据进行了应用。 |
| [^16] | [Bi-Level Attention Graph Neural Networks.](http://arxiv.org/abs/2304.11533) | 提出了一种双层注意力图神经网络，用于处理多关系和多实体的大规模异构图。通过基于两个重要性级别的层次图关注机制，以个性化的方式模拟节点-节点和关系-关系的相互作用。 |
| [^17] | [Triple Structural Information Modelling for Accurate, Explainable and Interactive Recommendation.](http://arxiv.org/abs/2304.11528) | 该论文提出了TriSIM4Rec算法，它基于动态交互图，同时利用用户-物品共现、用户交互时序信息和物品对的转移概率三种结构信息，进而实现了更准确、可解释和交互式的推荐。 |
| [^18] | [Hierarchical Weight Averaging for Deep Neural Networks.](http://arxiv.org/abs/2304.11519) | HWA是一种将在线WA和离线WA相结合的通用训练框架，在提高SGD性能方面表现出色。 |
| [^19] | [LayerNAS: Neural Architecture Search in Polynomial Complexity.](http://arxiv.org/abs/2304.11517) | LayerNAS提出了一种多项式复杂度的神经架构搜索方法，将搜索分为多个目标，并将搜索成本和奖励元素分开，能够快速有效地发现优越模型。 |
| [^20] | [QuMoS: A Framework for Preserving Security of Quantum Machine Learning Model.](http://arxiv.org/abs/2304.11511) | QuMoS是一个保护 QML 模型安全的框架，通过经典加密、量子混淆和诱饵样本等多种技术来保护模型免受窃取攻击，并具有较高的分类准确性。 |
| [^21] | [Machine learning framework for end-to-end implementation of Incident duration prediction.](http://arxiv.org/abs/2304.11507) | 该研究提出了一个端到端机器学习框架，通过分析交通事故报告中可以获取的信息，预测并优化应急响应团队的资源部署，从而减少事故持续时间，同时提高公众安全。 |
| [^22] | [Improved Churn Causal Analysis Through Restrained High-Dimensional Feature Space Effects in Financial Institutions.](http://arxiv.org/abs/2304.11503) | 本研究提出了一个概念性框架，通过特征限制来改善金融机构中的客户流失因果分析，以发现与独立变量相关且与影响流失的因变量因果相关的混淆特征。 |
| [^23] | [Physics-guided generative adversarial network to learn physical models.](http://arxiv.org/abs/2304.11488) | 该论文提出了一种物理引导生成对抗网络（PG-GAN）的模型，以判断神经网络输出是否符合物理规律，解决了DNNs输出不满足物理方程的问题。 |
| [^24] | [Understanding Lexical Biases when Identifying Gang-related Social Media Communications.](http://arxiv.org/abs/2304.11485) | 本研究使用自然语言处理工具有效识别了帮派相关社交媒体上可能需要社区资源帮助的人群，拓展了社区成员照顾的范畴。 |
| [^25] | [(Vector) Space is Not the Final Frontier: Product Search as Program Synthesis.](http://arxiv.org/abs/2304.11473) | 本文主张将产品搜索看作程序合成，相比向量空间模型有着重大优势。 |
| [^26] | [A Comparative Study of Pre-trained Speech and Audio Embeddings for Speech Emotion Recognition.](http://arxiv.org/abs/2304.11472) | 本文对来自八个语音和音频PTMs提取的嵌入进行了比较分析，旨在提高情感识别模型的发展速度和效率，并使其能够在实际环境中得到应用。 |
| [^27] | [Increasing the Scope as You Learn: Adaptive Bayesian Optimization in Nested Subspaces.](http://arxiv.org/abs/2304.11468) | 该论文提出了一种自适应贝叶斯优化方法BAxUS，通过利用嵌套子空间来避免高维贝叶斯优化中的风险并确保高性能，相对于现有最先进方法在广泛应用中取得更好结果。 |
| [^28] | [Recurrent Neural Networks and Long Short-Term Memory Networks: Tutorial and Survey.](http://arxiv.org/abs/2304.11461) | 本文是一篇关于循环神经网络（RNN）、长短期记忆网络（LSTM）及其变体的教程和调研，介绍了解决长期依赖问题的方法，以及双向RNN、双向LSTM和ELMo网络等进一步的应用。 |
| [^29] | [Reinforcement Learning with an Abrupt Model Change.](http://arxiv.org/abs/2304.11460) | 本文提出了一种模型突变下的强化学习算法，能够让智能体获得最优的长期折扣回报。该算法利用了回报-检测平衡，并使用最快的变化检测算法来检测模型变化。 |
| [^30] | [Constructing a meta-learner for unsupervised anomaly detection.](http://arxiv.org/abs/2304.11438) | 构建了一种新的元学习方法来选择适当的无监督AD算法。元学习器的表现优于当前的最优解决方案。 |
| [^31] | [Breaching FedMD: Image Recovery via Paired-Logits Inversion Attack.](http://arxiv.org/abs/2304.11436) | 本文揭示了即便使用FedMD的安全机制，仍存在被精心设计的恶意攻击利用的风险，如Paired-Logits反演攻击，会导致隐私数据曝光。 |
| [^32] | [Hyper-Laplacian Regularized Concept Factorization in Low-rank Tensor Space for Multi-view Clustering.](http://arxiv.org/abs/2304.11435) | 本文提出了一种采用超拉普拉斯正则化概念因式分解的多视角聚类方法，在低秩张量空间中对每个视角进行聚类并提取非线性局部结构。 |
| [^33] | [L3Cube-IndicSBERT: A simple approach for learning cross-lingual sentence representations using multilingual BERT.](http://arxiv.org/abs/2304.11434) | 该论文提出了一种简单但有效的方法，使用合成语料库将BERT模型转换成SBERT模型。该方法在10种主要的印欧语言中具有很好的效果，并展示了其在非印欧语言上的应用性。 |
| [^34] | [Conditional Denoising Diffusion for Sequential Recommendation.](http://arxiv.org/abs/2304.11433) | 提出了一种条件去噪扩散模型，通过条件自回归的方式将优化和生成过程分解为更容易和可处理的步骤，并引入了一种新的优化模式，结合交叉熵损失和对抗性损失稳定训练过程。在多个数据集上的实验表明，该模型在顺序推荐方面具有较优的性能。 |
| [^35] | [Pipeline MoE: A Flexible MoE Implementation with Pipeline Parallelism.](http://arxiv.org/abs/2304.11414) | Pipeline MoE是一种利用管道并行性架构解决了MoE模型通信开销和限制性扩展问题的灵活实现， 在标准基准测试中比现有最新模型具有更好的可扩展性和高达17%的训练效率提高。 |
| [^36] | [Towards Carbon-Neutral Edge Computing: Greening Edge AI by Harnessing Spot and Future Carbon Markets.](http://arxiv.org/abs/2304.11374) | 本研究利用两时间尺度Lyapunov最优控制方法设计一个在线算法，成功实现了碳中和边缘计算，降低碳排放，而不会影响服务质量。 |
| [^37] | [Learning Symbolic Representations Through Joint GEnerative and DIscriminative Training.](http://arxiv.org/abs/2304.11357) | GEDI是一种将自监督学习和基于似然生成模型结合的贝叶斯框架。它与现有的神经符号框架联合训练，无需额外监督或预训练步骤，能够产生更好的符号表示。通过实验，证明GEDI可以在聚类性能上显著超越现有的自监督学习策略，在小数据范围内的性能也得到提高。 |
| [^38] | [Input Augmentation with SAM: Boosting Medical Image Segmentation with Segmentation Foundation Model.](http://arxiv.org/abs/2304.11332) | 本文介绍如何使用大型的通用分割模型SAM来提升医学图像分割，展示了如何通过使用SAM生成的掩模、特征和稳定性分数来构建和训练更好的医学图像分割模型，并在两个数据集上进行了验证。 |
| [^39] | [On Accelerating Diffusion-Based Sampling Process via Improved Integration Approximation.](http://arxiv.org/abs/2304.11328) | 本文提出了一种通过优化系数来加速流行的基于反向ODE解算的扩散采样过程的方法，优化方法为改进的积分逼近（IIA），在每个反向时间步长，我们建议针对某些选择的系数最小化MSE函数，给定预训练的扩散模型，只需要在一批样本上进行特定数量的神经功能评估（NFEs）一次IIA过程即可获得最佳解。 |
| [^40] | [Towards Understanding Feature Learning in Out-of-Distribution Generalization.](http://arxiv.org/abs/2304.11327) | 研究发现，ERM本质上同时学习了具有误导性的特征和不变特征，在ERM预训练期间学习到的特征质量影响了最终的OOD性能，未能捕获所有潜在的有用特征将限制最终的OOD性能。 |
| [^41] | [Unmatched uncertainty mitigation through neural network supported model predictive control.](http://arxiv.org/abs/2304.11315) | 本文利用基于深度学习的模型预测控制算法，采用双时间尺度适应机制，结合深度神经网络作为预言机，实时估计不匹配的不确定性，成功地实现了对具有未知结构的不匹配和有界状态-动作相关不确定性的系统的控制。 |
| [^42] | [Lookahead Diffusion Probabilistic Models for Refining Mean Estimation.](http://arxiv.org/abs/2304.11312) | 本文提出基于前瞻扩散概率模型的技术来优化条件高斯分布的均值估计，通过对两个估计进行外推来计算更准确的估计值。该方法不需要对DNN模型进行微调，并在基准数据集上获得了比最新方法更好的实验结果。 |
| [^43] | [On the Identification of the Energy related Issues from the App Reviews.](http://arxiv.org/abs/2304.11292) | 本文研究了自动提取与能源相关的应用程序评论的不同技术。结果表明，神经网络优于其他机器学习模型。 |
| [^44] | [Identifying Appropriate Intellectual Property Protection Mechanisms for Machine Learning Models: A Systematization of Watermarking, Fingerprinting, Model Access, and Attacks.](http://arxiv.org/abs/2304.11285) | 本文系统化分析了机器学习模型在知识产权保护方面的挑战，提出了针对水印、指纹、模型访问和攻击的保护技术，并构建了综合的威胁模型。 |
| [^45] | [PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel.](http://arxiv.org/abs/2304.11277) | 本论文介绍了基于PyTorch的Fully Sharded Data Parallel（FSDP）解决方案，该方案可扩展大型模型训练，并优化各种硬件配置的资源利用率。 |
| [^46] | [Speed Is All You Need: On-Device Acceleration of Large Diffusion Models via GPU-Aware Optimizations.](http://arxiv.org/abs/2304.11267) | 该研究提出一系列通过GPU-aware优化大型扩散模型的方法，实现在配备GPU的移动设备上极快的推论延迟，扩大了生成性人工智能的适用范围并改善了用户体验。 |
| [^47] | [Time Series Classification for Detecting Parkinson's Disease from Wrist Motions.](http://arxiv.org/abs/2304.11265) | 该研究使用InceptionTime和ROCKET方法进行时间序列分类，以监测帕金森病患者的手腕运动。研究发现，所有方法都适用于估计震颤严重程度和肌肉强直的存在，但在检测运动障碍方面存在困难。具有岭分类器的InceptionTime方法展示了最先进的分类性能，显示时间序列分类在基于可穿戴设备的PD症状监测中具有潜力。 |
| [^48] | [Benchmarking Low-Shot Robustness to Natural Distribution Shifts.](http://arxiv.org/abs/2304.11263) | 本文通过对不同少样本数据集、架构、预训练初始化和稳健性干预的自然分布漂移的稳健性进行了首次深入研究，发现没有单一的选择模型比其他模型更稳健，现有的干预措施也可能无法提高某些数据集的稳健性。 |
| [^49] | [Machine Learning and the Future of Bayesian Computation.](http://arxiv.org/abs/2304.11251) | 本文讨论了利用机器学习的思想来改进贝叶斯计算的潜力，并探讨了几个具体的未来方向。 |
| [^50] | [eWaSR -- an embedded-compute-ready maritime obstacle detection network.](http://arxiv.org/abs/2304.11249) | 本论文提出了一种嵌入式计算准备好的海事障碍物检测网络——eWaSR，能够在保证检测质量的前提下运行速度更快，且在F1得分方面也优于其他目前最先进的嵌入式就绪架构。 |
| [^51] | [Quantum physics-informed neural networks for simulating computational fluid dynamics in complex shapes.](http://arxiv.org/abs/2304.11247) | 本文提出了一种基于量子物理的神经网络方法，用于模拟复杂几何形状中的流体流动。该方法不需要重新模拟，能够适用于不同的形状，并且相比于普通神经网络提高了21%的精度。 |
| [^52] | [AutoNeRF: Training Implicit Scene Representations with Autonomous Agents.](http://arxiv.org/abs/2304.11241) | 本文提出了AutoNeRF方法，使用自主体代理收集训练NeRF所需数据，训练NeRF成功。 |
| [^53] | [Adapting model-based deep learning to multiple acquisition conditions: Ada-MoDL.](http://arxiv.org/abs/2304.11238) | 本研究提出了一种适应多种采集条件的基于模型的深度学习方法，可从欠采样平行MRI数据中提供高质量重建，通过使用适当的权重对CNN特征和正则化参数进行缩放，使模型适应每个设置。 |
| [^54] | [Effective Neural Network $L_0$ Regularization With BinMask.](http://arxiv.org/abs/2304.11237) | 本文提出了一种简单而高效的神经网络$L_0$正则化方法——BinMask，该方法采用确定性二进制掩码乘以权重并使用标识直通估计器进行反向传播。实验证明，与其他方法相比，Binmask不需要针对不同任务进行专门调整即可在特征选择、网络稀疏化和模型正则化等任务中实现有竞争力的表现。解耦权重与掩码优化是实现有效$L_0$正则化的关键组成部分。 |
| [^55] | [Probabilistic selection and design of concrete using machine learning.](http://arxiv.org/abs/2304.11226) | 本文介绍了一种利用机器学习进行混凝土配比设计和性能预测的方法，该方法可以提高混凝土建筑的可持续性，同时满足强度、密度和成本等目标。 |
| [^56] | [DP-Adam: Correcting DP Bias in Adam's Second Moment Estimation.](http://arxiv.org/abs/2304.11208) | 该论文提出了DP-Adam，解决了在Adam优化器中传统DP的使用引入的偏差导致的问题。 |
| [^57] | [Fast GraspNeXt: A Fast Self-Attention Neural Network Architecture for Multi-task Learning in Computer Vision Tasks for Robotic Grasping on the Edge.](http://arxiv.org/abs/2304.11196) | 本文提出了一种针对边缘计算机器人抓取的计算机视觉任务定制的多任务深度自注意神经网络架构，名为快速GraspNeXt。实验表明，快速GraspNeXt在精度和速度方面表现优异。 |
| [^58] | [Automatically identifying dynamical systems from data.](http://arxiv.org/abs/2304.11182) | 该论文提出了一种从经验数据中自动识别动态规律的方法，这种方法能够较为准确地识别三维系统，具有在各种领域中理解复杂系统的潜力。 |
| [^59] | [Task-Adaptive Pseudo Labeling for Transductive Meta-Learning.](http://arxiv.org/abs/2304.11173) | 本文提出了一种名为“任务自适应伪标签”的跨感知元学习方法，利用伪标签生成未标记的查询集，使用监督设置并利用未标记的查询集，可以处理更多实例，从而带来更好的分类性能。 |
| [^60] | [Granular ball computing: an efficient, robust, and interpretable adaptive multi-granularity representation and computation method.](http://arxiv.org/abs/2304.11171) | 本文提出了一种基于颗粒球计算的自适应多粒度表示和计算方法，能够提高机器学习的效率、鲁棒性和可解释性。 |
| [^61] | [Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT.](http://arxiv.org/abs/2304.11116) | 本文旨在通过Graph-ToolFormer框架赋予LLMs图形推理能力，并解决现有LLMs在执行图形学习任务中存在的固有弱点。 |
| [^62] | [Prediction, Learning, Uniform Convergence, and Scale-sensitive Dimensions.](http://arxiv.org/abs/2304.11059) | 本文介绍了一种新的通用算法，利用尺度敏感的Vapnik维度来学习$[0,1]$值函数类，并获得了关于期望绝对误差的一般上限。文中证明该上限不能在一般情况下进一步改善一个常数因子。这篇论文对无偏学习样本复杂度的提高具有重要的意义。 |
| [^63] | [Exogenous Data in Forecasting: FARM -- An Approach for Relevance Evaluation.](http://arxiv.org/abs/2304.11028) | 该论文介绍了一种名为FARM的方法，用于有效处理实时数据流并提供平衡的相关性度量，进而确定外部数据在预测中的重要性。 |
| [^64] | [Reconciling High Accuracy, Cost-Efficiency, and Low Latency of Inference Serving Systems.](http://arxiv.org/abs/2304.10892) | InfAdapter提出了一个解决高准确性、低延迟和成本效益之间权衡问题的方法，通过主动选择一组带有资源分配的 ML 模型变体来满足延迟 SLO，并最大化由准确性和成本组成的目标函数，相较于其他方法降低了 SLO 违规和成本。 |
| [^65] | [Using Z3 for Formal Modeling and Verification of FNN Global Robustness.](http://arxiv.org/abs/2304.10558) | 本文介绍了使用Z3求解器对全局鲁棒性可验证框架DeepGlobal进行更明确的定义和优化的工作，来建立FNN的形式化模型，以实现更有效的验证。 |
| [^66] | [An Introduction to Transformers.](http://arxiv.org/abs/2304.10557) | Transformer是一种神经网络组件，可以学习序列或数据集表示，在自然语言处理、计算机视觉和时空建模方面取得了重大进展。本论文提供了一个数学精确、直观、简洁的Transformer架构描述。 |
| [^67] | [The impact of the AI revolution on asset management.](http://arxiv.org/abs/2304.10212) | 本论文分享了作者关于人工智能可能对资产管理产生影响的观点，同时提供了一个衡量特定基金是否真正开发了AI的简单标准。 |
| [^68] | [Two-Memory Reinforcement Learning.](http://arxiv.org/abs/2304.10098) | 本文提出了双记忆强化学习代理 (2M)，它结合了情节记忆和强化学习的优点来提高学习速度和准确性。 |
| [^69] | [A Latent Space Theory for Emergent Abilities in Large Language Models.](http://arxiv.org/abs/2304.09960) | 本文探讨了大规模语言模型中的贝叶斯推断和稀疏联合分布，证明了LLMs能够完成语言理解、上下文学习、思路启发以及有效指令微调的新兴能力。 |
| [^70] | [SemEval 2023 Task 6: LegalEval -- Understanding Legal Texts.](http://arxiv.org/abs/2304.09548) | SemEval 2023举办了LegalEval共享任务，即理解法律文本，包括 自动结构化和语义连贯化的法律文件（Task-A），法律命名实体识别（Task-B）以及自动预测法律案件结果和提供预测解释（Task-C）。26个团队提交了系统论文并在所有子任务中优于基准线，但仍有改进空间。 |
| [^71] | [FastMRI Prostate: A Publicly Available, Biparametric MRI Dataset to Advance Machine Learning for Prostate Cancer Imaging.](http://arxiv.org/abs/2304.09254) | FastMRI前列腺推出了一个公开可用的双参数MRI数据集，用于研究MRI图像重建和评估，以提高MRI在前列腺癌检测和评估上的实用性。 |
| [^72] | [Did we personalize? Assessing personalization by an online reinforcement learning algorithm using resampling.](http://arxiv.org/abs/2304.05365) | 本论文提出了一种使用重复采样的政策评估方法，以评估在线 RL 算法实现的个性化程度。该方法可用于优化数字健康的个性化干预。 |
| [^73] | [Evaluation of Differentially Constrained Motion Models for Graph-Based Trajectory Prediction.](http://arxiv.org/abs/2304.05116) | 使用深度学习模型进行运动预测在自动驾驶中表现出色，但缺乏解释性和可能违反物理约束。因此，结合差分约束运动模型能提供物理上可行的轨迹，研究表明低阶积分器模型表现更好，并且数值求解器对模型性能产生影响。 |
| [^74] | [Wav2code: Restore Clean Speech Representations via Codebook Lookup for Noise-Robust ASR.](http://arxiv.org/abs/2304.04974) | Wav2code是一种基于自监督学习的ASR模型，可以实现用于噪声鲁棒的无失真增强，从而提供更好的语音表征。 |
| [^75] | [First-order methods for Stochastic Variational Inequality problems with Function Constraints.](http://arxiv.org/abs/2304.04778) | 本文提出了一种新的一阶方法，适用于具有随机算子和/或随机约束的带函数约束的变分不等式问题，当FCVI问题是确定性非光滑的或随机的时，这些方法可以实现最优算子或样本复杂性。 |
| [^76] | [Unsupervised Story Discovery from Continuous News Streams via Scalable Thematic Embedding.](http://arxiv.org/abs/2304.04099) | 本研究提出了一种新颖的主题嵌入方法和一个可扩展的无监督在线故事发现框架USTORY，可以动态表示文章和故事，并考虑它们共享的时间主题和新颖性，以帮助人们消化大量的新闻流。 |
| [^77] | [SSS at SemEval-2023 Task 10: Explainable Detection of Online Sexism using Majority Voted Fine-Tuned Transformers.](http://arxiv.org/abs/2304.03518) | 本文描述了使用细调BERT模型和多数投票集成模型来检测和解释在线性别歧视的方法。翻转显着降低了女性在社交媒体平台上经历不成比例的性别歧视的风险。 |
| [^78] | [TransPimLib: A Library for Efficient Transcendental Functions on Processing-in-Memory Systems.](http://arxiv.org/abs/2304.01951) | TransPimLib提供了处理器内存系统上高效的超越函数计算方法，有助于提高PIM系统的计算能力和支持更广泛的工作负载，特别是机器学习应用中的激活函数。 |
| [^79] | [CoReFusion: Contrastive Regularized Fusion for Guided Thermal Super-Resolution.](http://arxiv.org/abs/2304.01243) | 本文提出了一种新颖的数据融合框架和正则化技术，用于热成像的引导超分辨率，具有计算效率，轻量级和鲁棒性，实现了在缺失数据的情况下性能不受影响，并且在基准数据集上表现出较高的性能。 |
| [^80] | [Operator learning with PCA-Net: upper and lower complexity bounds.](http://arxiv.org/abs/2303.16317) | 本文发展了PCA-Net的近似理论，得出了通用逼近结果，并识别出了使用PCA-Net进行高效操作学习的潜在障碍：输出分布的复杂性和算子空间的内在复杂性。 |
| [^81] | [Strategy Synthesis in Markov Decision Processes Under Limited Sampling Access.](http://arxiv.org/abs/2303.12718) | 本文提出了一种针对灰箱 MDP 的策略综合算法，使用区间 MDP 作为内部模型，并通过强化学习，结合下置信区间探索和行动划分的方法，解决有限采样下的问题，用于合成最大化回报的实用策略。 |
| [^82] | [EPiC: Ensemble of Partial Point Clouds for Robust Classification.](http://arxiv.org/abs/2303.11419) | 本文提出了一种基于部分点云采样的通用集成框架，由多种采样方法联合使用提高鲁棒性，达到了最优性能。 |
| [^83] | [Lon-ea at SemEval-2023 Task 11: A Comparison of Activation Functions for Soft and Hard Label Prediction.](http://arxiv.org/abs/2303.02468) | 本研究研究了在软硬标签预测中，不同激活函数对于深度神经网络模型输出的影响，并引入了一种新的正弦激活函数。 |
| [^84] | [Hierarchical Training of Deep Neural Networks Using Early Exiting.](http://arxiv.org/abs/2303.02384) | 本文提出了一种使用早期退出的分层训练方法，将深度神经网络分为边缘和云工作者，以减少通信成本、训练运行时间和隐私问题。 |
| [^85] | [Guarded Policy Optimization with Imperfect Online Demonstrations.](http://arxiv.org/abs/2303.01728) | 本文放弃了教师表现良好的假设，提出了一种新的算法，能够融合任意教师策略，并通过基于轨迹的价值估计实现高效的探索和安全保障。 |
| [^86] | [FuNVol: A Multi-Asset Implied Volatility Market Simulator using Functional Principal Components and Neural SDEs.](http://arxiv.org/abs/2303.00859) | FuNVol是一个多资产隐含波动率市场模拟器，使用函数主成分和神经SDE生成真实历史价格的IV表面序列，并在无静态套利的表面次流形内产生一致的市场情景。同时，使用模拟表面进行对冲可以生成与实现P＆L一致的损益分布。 |
| [^87] | [Bounding Training Data Reconstruction in DP-SGD.](http://arxiv.org/abs/2302.07225) | 本文在DP-SGD的上下文中研究了如何设置隐私参数以保护免受训练数据重构攻击，并提供了相关攻击的上限和匹配的攻击方式。 |
| [^88] | [A Policy Gradient Framework for Stochastic Optimal Control Problems with Global Convergence Guarantee.](http://arxiv.org/abs/2302.05816) | 该论文提出了一种带有全局收敛保证的随机最优控制问题的策略梯度框架，并引入局部最优控制函数的概念来表征迭代的局部最优性。 |
| [^89] | [Gentlest ascent dynamics on manifolds defined by adaptively sampled point-clouds.](http://arxiv.org/abs/2302.04426) | 本文介绍了一种将最温和上升动力学扩展到点云定义的流形上的方法，可以在迭代过程中自适应采样，驱动系统从初始构型到达鞍点。 |
| [^90] | [Geometric Deep Learning for Autonomous Driving: Unlocking the Power of Graph Neural Networks With CommonRoad-Geometric.](http://arxiv.org/abs/2302.01259) | 该论文提出了一种Python框架，能够从交通场景中提取标准化的图形数据集，以利用图神经网络（GNNs）进行各种机器学习应用，例如轨迹预测，为自主驾驶研究提供了基于GNN的平台。 |
| [^91] | [Analyzing Leakage of Personally Identifiable Information in Language Models.](http://arxiv.org/abs/2302.00539) | 本研究针对语言模型中泄漏个人身份信息的风险进行了严格的定义，并通过黑盒提取、推断和重建攻击进行了实证评估。 |
| [^92] | [Bagging Provides Assumption-free Stability.](http://arxiv.org/abs/2301.12600) | 本文证明了Bagging技术可提供无偏差稳定性，适用于各种数据分布和算法，具有良好的实证效果。 |
| [^93] | [Hamiltonian Neural Networks with Automatic Symmetry Detection.](http://arxiv.org/abs/2301.07928) | 本研究提出了一种哈密顿神经网络模型，采用李代数框架检测和嵌入对称性，以保留辛系统结构。考虑摆在小车上和天体动力学中的二体问题作为案例，结果表明这种方法是有效的。 |
| [^94] | [A Finite Element-Inspired Hypergraph Neural Network: Application to Fluid Dynamics Simulations.](http://arxiv.org/abs/2212.14545) | 本文提出了一种有限元灵感启发的超图神经网络（FEIH($\phi$)-GNN），能够模拟局部刚度矩阵的计算过程，具备旋转等变性，并能准确预测流体动力学模拟中的时间滚动。 |
| [^95] | [Learning Multimodal Data Augmentation in Feature Space.](http://arxiv.org/abs/2212.14453) | 本文介绍了一种名为LeMDA的易于使用的方法，它在特征空间中自动学习联合增强多模态数据，提高了多模态学习算法的性能。 |
| [^96] | [CC-FedAvg: Computationally Customized Federated Averaging.](http://arxiv.org/abs/2212.13679) | 本论文提出了一个称为CC-FedAvg的计算定制的联邦平均算法，可让参与者根据其计算预算决定在每轮中是否执行传统的本地训练或模型估算。实验结果表明，CC-FedAvg能够显著提高模型性能并降低通信成本。 |
| [^97] | [MN-DS: A Multilabeled News Dataset for News Articles Hierarchical Classification.](http://arxiv.org/abs/2212.12061) | 本文介绍了一个包含10,917篇新闻文章的多标签数据集，可用于训练机器学习模型自动按主题对新闻文章进行分类，对新闻结构、分类和预测未来事件的研究人员非常有帮助。 |
| [^98] | [PyGFI: Analyzing and Enhancing Robustness of Graph Neural Networks Against Hardware Errors.](http://arxiv.org/abs/2212.03475) | 本文针对图神经网络对硬件错误的鲁棒性进行了研究，提出了PyGFI框架，利用故障注入生成合成故障来训练更加鲁棒的GNN模型，可以容忍各种类型的硬件故障，有效提高GNN模型的准确性和鲁棒性，并可以轻松集成到现有的GNN训练流程中。 |
| [^99] | [Edge Impulse: An MLOps Platform for Tiny Machine Learning.](http://arxiv.org/abs/2212.03332) | Edge Impulse是一个面向微型机器学习的MLOps平台，旨在为开发人员提供嵌入式和边缘ML系统的软硬件优化支持，解决TinyML的可移植性和优化问题。 |
| [^100] | [Guaranteed Conformance of Neurosymbolic Models to Natural Constraints.](http://arxiv.org/abs/2212.01346) | 该论文提出了一种能够担保数据驱动模型符合自然科学已有知识并最优逼近系统模型的方法。 |
| [^101] | [Dynamic Sparse Training via Balancing the Exploration-Exploitation Trade-off.](http://arxiv.org/abs/2211.16667) | 本文提出了一种动态稀疏训练方法，采用开发和探索收购函数来平衡探索和开发之间的权衡，从而摆脱了局部最优和鞍点问题，实验结果表明，该方法在精度和收敛速度方面超过了现有的稀疏训练方法。 |
| [^102] | [Latent Graph Inference using Product Manifolds.](http://arxiv.org/abs/2211.16199) | 本文提出了一种利用积流形进行推断的潜在图学习方法，以动态学习问题内在的图结构。通过使用Riemannian几何学和生成更复杂的嵌入空间，可以提高系统性能并产生更丰富的表示。 |
| [^103] | [Establishment of Neural Networks Robust to Label Noise.](http://arxiv.org/abs/2211.15279) | 本文介绍了建立鲁棒性强的神经网络以克服标签噪声的研究，通过创建转换矩阵估计器、研究两个卷积神经网络分类器的标签噪声鲁棒性等，揭示了两个FashionMINIST数据集的鲁棒性，旨在解决深度学习模型训练中标签噪声带来的难题。 |
| [^104] | [Continuous Episodic Control.](http://arxiv.org/abs/2211.15183) | CEC是一种新颖的非参数情景记忆算法，用于连续性行动空间问题中的序列决策制定，其在几个稀疏奖励连续控制环境中比最先进的RL和记忆增强RL算法学习更快，是学习连续控制任务的快速方法。 |
| [^105] | [A Mathematical Programming Approach to Optimal Classification Forests.](http://arxiv.org/abs/2211.10502) | 提出了一种新的分类器——最优分类森林，通过数学优化方法构建最优的决策树集成实现高预测准确性，并且比随机森林使用数量级更少的树。同时提供了三个真实世界的案例研究。 |
| [^106] | [Let's Enhance: A Deep Learning Approach to Extreme Deblurring of Text Images.](http://arxiv.org/abs/2211.10103) | 本文提出了一种用于极端文本图像去模糊的深度学习方法，利用合成数据进行增强和预训练。该方法通过数据驱动的前向模型估计和U-Net实现去模糊，该方法在2021年赫尔辛基去模糊挑战赛中获胜。 |
| [^107] | [Decentralized Federated Learning: Fundamentals, State-of-the-art, Frameworks, Trends, and Challenges.](http://arxiv.org/abs/2211.08413) | 本文提出去中心化联邦学习（DFL）来解决传统中心化FL（CFL）模型中的问题，主要研究DFL与CFL的差异、DFL的基础理论、DFL框架的设计与评估以及DFL在实际应用中的场景。 |
| [^108] | [MMD-B-Fair: Learning Fair Representations with Statistical Testing.](http://arxiv.org/abs/2211.07907) | 提出了一种基于统计检验的 MMD-B-Fair 方法，用于学习公平的数据表示，并在各种数据集上得到了验证。 |
| [^109] | [An Empirical Study on Data Leakage and Generalizability of Link Prediction Models for Issues and Commits.](http://arxiv.org/abs/2211.00381) | 本文提出LinkFormer，一种基于Transformer 架构的预测模型，用于分析软件问题和提交之间的依赖关系。LinkFormer 可以减少数据泄露和加强模型的可迁移性，提高预测准确性，应用于自动链接恢复技术领域。 |
| [^110] | [GammaE: Gamma Embeddings for Logical Queries on Knowledge Graphs.](http://arxiv.org/abs/2210.15578) | GammaE是一种新颖的概率嵌入模型，利用了Gamma分布的线性特性和强边界支持来捕捉实体和查询的更多特征，解决了知识图谱中否定和联合算符的建模问题，并在知识图谱上回答不同类型的FOL查询。 |
| [^111] | [Resource Constrained Vehicular Edge Federated Learning with Highly Mobile Connected Vehicles.](http://arxiv.org/abs/2210.15496) | 本文提出了一种在高度移动的连接车辆下，边缘服务器利用局部数据集和处理单元进行训练的边缘联邦学习解决方案，可以通过权重组合和子集选择来聚合模型参数并最大化成功接收本地训练模型的概率。 |
| [^112] | [Broken Neural Scaling Laws.](http://arxiv.org/abs/2210.14891) | 本文提出了一个平滑破碎的幂律函数形式，可以准确地模拟和外推深度神经网络的缩放行为，适用于各种架构和大量不同任务，包括视觉、语言、音频、视频、生成建模、对比学习、机器人、不确定性估计/校准、对抗鲁棒性、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。 |
| [^113] | [A study of uncertainty quantification in overparametrized high-dimensional models.](http://arxiv.org/abs/2210.12760) | 本论文研究了过度参数化高维模型中的不确定性问题，探讨了几种方法，比较了校准和分类准确性之间的权衡。结果发现最佳正则化估计量的校准曲线具有双重下降行为，与经验贝叶斯方法形成对比。 |
| [^114] | [Universal hidden monotonic trend estimation with contrastive learning.](http://arxiv.org/abs/2210.09817) | 本文提出了一种称为对比趋势估计 (CTE) 的方法，它是一种通用隐含单调趋势估计方法，可适用于任何类型的时间数据，并避免了用于单调趋势识别的标准假设。 |
| [^115] | [Just Round: Quantized Observation Spaces Enable Memory Efficient Learning of Dynamic Locomotion.](http://arxiv.org/abs/2210.08065) | 本文通过观测空间量化，实现了对复杂机器人行为的高效学习，可以降低内存成本。 |
| [^116] | [Language Models are Realistic Tabular Data Generators.](http://arxiv.org/abs/2210.06280) | 本文提出了GReaT模型，利用自回归生成的大型语言模型来生成高度真实的表格数据，可以通过调节任意子集特征建模表格数据分布。 |
| [^117] | [Alternating Differentiation for Optimization Layers.](http://arxiv.org/abs/2210.01802) | Alt-Diff是一种新的框架，可以在不需要对整个雅可比矩阵进行昂贵计算的情况下，以快速和递归的方式微分优化问题，从而大大提高隐式微分的计算速度。 |
| [^118] | [PAC-Bayes Generalisation Bounds for Heavy-Tailed Losses through Supermartingales.](http://arxiv.org/abs/2210.00928) | 本文为重尾损失情况下的PAC-Bayes提供了泛化界，扩展了先前的研究，并通过马尔科夫不等式的扩展为不同的PAC-Bayesian框架提供了界限。 |
| [^119] | [Upper bounds on the Natarajan dimensions of some function classes.](http://arxiv.org/abs/2209.07015) | 本研究建立了一些函数类的Natarajan维度上界，这些结果可以用于描述某些多类学习算法的性能。 |
| [^120] | [An Analysis of Collocation on GPUs for Deep Learning Training.](http://arxiv.org/abs/2209.06018) | 本文通过研究GPU上深度学习训练中的协同分析，证明了协同多个模型训练可以产生显着的效益。MIG技术通过将GPU分区，更好地适应不需要完整GPU内存和计算资源的工作负载，具有优势，并且具有优化的放置策略的MIG一般优于所有其他方法。 |
| [^121] | [Incorporating Task-specific Concept Knowledge into Script Learning.](http://arxiv.org/abs/2209.00068) | 本文提出了一个新任务Tetris，并提出了概念提示和面向脚本的对比学习来解决输入包括用户上下文时的问题，两种方法均能提高任务完成性能。 |
| [^122] | [Deep Symbolic Learning: Discovering Symbols and Rules from Perceptions.](http://arxiv.org/abs/2208.11561) | DSL是一种能够在学习中自动发现有意义符号规则的NeSy系统，并在各种任务中取得最先进的结果。 |
| [^123] | [Retrieval-based Controllable Molecule Generation.](http://arxiv.org/abs/2208.11126) | 本文提出了一种基于检索的方法，通过使用少量的例子分子来引导生成模型生成满足给定设计条件的分子。并且使用自我监督的目标函数训练检索机制，来提高生成的分子的泛化性。 |
| [^124] | [Accelerating discrete dislocation dynamics simulations with graph neural networks.](http://arxiv.org/abs/2208.03296) | 研究团队提出了一种利用基于DDD轨迹训练的图神经网络模型替代耗时的位错运动时间积分的DDD-GNN框架，有效加速介观尺度下晶体材料塑性行为的计算模拟。 |
| [^125] | [Conformal Risk Control.](http://arxiv.org/abs/2208.02814) | 该论文提出了一种符合保序的风险控制方法，可以控制任何单调损失函数的期望值，示例证明其在计算机视觉和自然语言处理领域具有控制误报率、图形距离和令牌级F1得分的能力。 |
| [^126] | [GraphCFC: A Directed Graph based Cross-modal Feature Complementation Approach for Multimodal Conversational Emotion Recognition.](http://arxiv.org/abs/2207.12261) | 本文提出了一种基于有向图的跨模态特征补充方法，可以提取多模态上下文信息和交互信息，缓解了多模态融合中的异构性差距问题。 |
| [^127] | [An Experimental Evaluation of Machine Learning Training on a Real Processing-in-Memory System.](http://arxiv.org/abs/2207.07886) | 该研究评估了在处理内存系统上训练机器学习算法的潜能，并证明基于PIM的ML训练实现了显着的加速和能量效率。 |
| [^128] | [Know Your Space: Inlier and Outlier Construction for Calibrating Medical OOD Detectors.](http://arxiv.org/abs/2207.05286) | 本文研究了医学图像OOD探测器的校准问题，发现内线和外线的合成空间以及增强类型对OOD探测器的校准起到关键作用。研究表明，最佳协议是合成潜空间内线和多样化的像素空间外线，该方法相比现有技术可以提高OOD检测的AUROC $15％-35％$。 |
| [^129] | [SALSA: Attacking Lattice Cryptography with Transformers.](http://arxiv.org/abs/2207.04785) | SALSA是一种使用Transformer和统计密码分析技术的机器学习攻击，可以攻击小到中等大小、具有稀疏二进制秘密的LWE实例，可能会扩展到实际的LWE密码系统。 |
| [^130] | [Code Translation with Compiler Representations.](http://arxiv.org/abs/2207.03578) | 本文提出了一种将编译器中间表示与神经机器翻译相结合的方法，能够更好地捕捉代码的语义，从而提高了代码翻译的质量和实用性。 |
| [^131] | [q-Learning in Continuous Time.](http://arxiv.org/abs/2207.00713) | 本文研究了连续时间下的q-Learning，通过引入小q函数作为一阶近似，研究了q-learning理论，应用于设计不同的演员-评论家算法。 |
| [^132] | [Beyond neural scaling laws: beating power law scaling via data pruning.](http://arxiv.org/abs/2206.14486) | 本研究通过数据修剪算法突破神经网络训练集大小与模型误差幂律的尺度界限，并在多个数据集实验中验证了有效性，同时进行了首次大规模数据修剪算法基准测试研究。 |
| [^133] | [A Conditional Gradient-based Method for Simple Bilevel Optimization with Convex Lower-level Problem.](http://arxiv.org/abs/2206.08868) | 本文提出了一种新的双层优化方法，该方法通过局部逼近下层问题的解集，然后运行条件梯度更新来减少上层目标函数，并且收敛性保证较好。 |
| [^134] | [Deep Isolation Forest for Anomaly Detection.](http://arxiv.org/abs/2206.06602) | 本文提出了深度隔离森林，利用神经网络将原始数据映射到随机表示集合中，通过随机轴并行切割来执行数据分区，以解决孤立森林算法不能成功检测高维/非线性可分数据空间中的难以隔离的困难异常的问题。 |
| [^135] | [ACMP: Allen-Cahn Message Passing for Graph Neural Networks with Particle Phase Transition.](http://arxiv.org/abs/2206.05437) | 本文提出了一种基于ACMP的图神经网络模型，它可以通过具有吸引力和排斥力的相互作用粒子系统进行消息传递传播，克服了GNN过度平滑问题，将网络深度推到100层，并在基准数据集上实现了最先进的节点分类和图匹配性能。 |
| [^136] | [Formal Interpretability with Merlin-Arthur Classifiers.](http://arxiv.org/abs/2206.00759) | 该论文提出了一种新型的多智能体交互分类器，利用“Merlin-Arthur”协议的启发，在不假设最优智能体或特征独立分布的情况下，通过相对强度和“非对称特征相关性”概念捕捉特征之间精确的相关性，提供可证明的可解释性保证。 |
| [^137] | [Frustratingly Easy Regularization on Representation Can Boost Deep Reinforcement Learning.](http://arxiv.org/abs/2205.14557) | 本文证明了DRL的学习表示应该满足一个有利的可区分表示属性，提出了一种正则化器PEER，旨在通过对内部表示进行显式正则化来维持可区分表示属性。 |
| [^138] | [TFLEX: Temporal Feature-Logic Embedding Framework for Complex Reasoning over Temporal Knowledge Graph.](http://arxiv.org/abs/2205.14307) | 本文提出了第一个用于时间知识图谱的复杂查询嵌入方法TFLEX，能够自然地建模所有一阶逻辑（FOL）运算，同时扩展了向量逻辑以处理三个额外的时间运算符。 |
| [^139] | [Equivariant quantum circuits for learning on weighted graphs.](http://arxiv.org/abs/2205.06109) | 本文提出了一种考虑到带权图对称性的等变量子电路（ansatz），并在神经组合优化问题中证实了其性能优异，充分说明在量子机器学习领域，保持对称性的ansatz是成功关键。 |
| [^140] | [Automated Algorithm Selection for Radar Network Configuration.](http://arxiv.org/abs/2205.03670) | 研究了13种黑盒优化算法在153个雷达网络配置问题实例上的表现，这些算法比人类专家要更好。但它们的排名取决于预算和位置高程。 |
| [^141] | [Interpretable Battery Cycle Life Range Prediction Using Early Degradation Data at Cell Level.](http://arxiv.org/abs/2204.12420) | 该论文提出了一种利用电池早期退化数据预测电池循环寿命范围的方法，其QRF模型能够实现高精度的点预测，并通过预测区间宽度来量化不确定性，具有可解释性。 |
| [^142] | [Hierarchical Embedded Bayesian Additive Regression Trees.](http://arxiv.org/abs/2204.07207) | 本文提出了分层嵌入BART模型，通过在回归树的末端节点级别上包含随机效应，避免了用户需要指定随机效应结构的需要。模型在混合效应模型数据集上表现优异，能够提供随机效应方差的一致估计。 |
| [^143] | [Learning to Compose Soft Prompts for Compositional Zero-Shot Learning.](http://arxiv.org/abs/2204.03574) | 通过学习组合式软提示实现了预测看不见的属性-对象组合，超过了基准数据集上的特定体系结构，并在曲线下面积上平均高10.9％。 |
| [^144] | [Random vector functional link network: recent developments, applications, and future directions.](http://arxiv.org/abs/2203.11316) | 本文介绍随机向量功能链接（RVFL）网络的演变、改进和应用，该网络具有快速训练速度、直观的连接、简单的结构和通用近似能力，是一种有前途的随机化神经网络。 |
| [^145] | [Robust PAC$^m$: Training Ensemble Models Under Model Misspecification and Outliers.](http://arxiv.org/abs/2203.01859) | 对于存在模型规格不准确和异常值情况下的集成学习，本文提出了一个新的鲁棒自由能量准则，通过将广义对数得分函数与PAC$^m$结合，实现了更好的模型性能。 |
| [^146] | [StratDef: Strategic Defense Against Adversarial Attacks in ML-based Malware Detection.](http://arxiv.org/abs/2202.07568) | 本文提出了一个名为StratDef的移动目标防御方法的战略防御系统，针对机器学习恶意软件检测的防御措施进行了全面评估。StratDef动态地和策略地选择最佳模型，以增加攻击者的不确定性，同时最小化对攻击的影响，使其具有很高的对抗鲁棒性。 |
| [^147] | [Tutorial on amortized optimization.](http://arxiv.org/abs/2202.00665) | 该教程介绍了分摊优化的基础，并总结了其在变分推断、稀疏编码、元学习、控制、强化学习、凸优化、最优传输和深度平衡网络中的应用。 |
| [^148] | [Deep Reinforcement Learning, a textbook.](http://arxiv.org/abs/2201.02135) | 深度强化学习是一种引人注目的技术，计算机程序通过尝试、得到反馈和再次尝试来自我解决困难问题，甚至在某些领域比最好的人类表现更好。 |
| [^149] | [GREED: A Neural Framework for Learning Graph Distance Functions.](http://arxiv.org/abs/2112.13143) | GREED是一种可以学习和预测图编辑距离（GED和SED）的神经网络框架，通过多任务学习来确保距离保持度量属性并使SED的高效计算成为可能，比现有最先进的方法表现更好。 |
| [^150] | [Probabilistic Approach for Road-Users Detection.](http://arxiv.org/abs/2112.01360) | 本文介绍了一种用于缓解深度目标检测模型中过于自信预测问题的方法，通过引入一种新颖的概率层来避免传统的预测层，实验证明该方法能够减少假阳性中的过度自信。 |
| [^151] | [Non-Asymptotic Analysis of Stochastic Approximation Algorithms for Streaming Data.](http://arxiv.org/abs/2109.07117) | 该论文介绍了流数据随机逼近算法的非渐近收敛速度，包括随机梯度下降、小批量SG和时间变化的小批量SG算法以及它们的迭代平均值，同时展示了加速收敛的方法和同时提供方差减少和加速收敛的优势。 |
| [^152] | [A theory of representation learning in deep neural networks gives a deep generalisation of kernel methods.](http://arxiv.org/abs/2108.13097) | 本文提出了一种新的无限宽度限制——贝叶斯表示学习限制，旨在解决标准无限宽度限制消除表示学习的问题。该方法可以实现类似于有限宽度模型中的表示学习效果，并保留标准无限宽度限制的简单性。 |
| [^153] | [MRCpy: A Library for Minimax Risk Classifiers.](http://arxiv.org/abs/2108.01952) | MRCpy是一种用于实现最小化风险分类器的Python库，它基于鲁棒风险最小化技术，可以利用0-1损失并提供了多种分类方法，其中一些提供了紧密的期望损失界限。 |
| [^154] | [A Modulation Layer to Increase Neural Network Robustness Against Data Quality Issues.](http://arxiv.org/abs/2107.08574) | 提出了一种新颖的神经网络修正方法，用于缓解低质量和缺失数据的影响，具备神经调制特征，通过一个额外的输入的函数替换了全连接层的固定权重，使得在测试中具有调制层的模型对于数据质量的降解更加鲁棒，同时也能够节省训练时间并且不会受到插补错误的影响。 |
| [^155] | [Auto-NBA: Efficient and Effective Search Over the Joint Space of Networks, Bitwidths, and Accelerators.](http://arxiv.org/abs/2106.06575) | Auto-NBA是一种能够高效搜索深度神经网络加速器的算法，它可以同时考虑优化网络、比特宽度和加速器三个耦合的方面。 |
| [^156] | [Robust Model Selection and Nearly-Proper Learning for GMMs.](http://arxiv.org/abs/2106.02774) | 本文研究了一元高斯混合模型（GMMs）鲁棒模型选择的问题，提出了一个鲁棒算法，可以在对抗性扰动下近似正确地学习GMMs，实现了最佳样本复杂度，能够近似确定拟合分布所需的最少组件数。 |
| [^157] | [Two-step reinforcement learning for model-free redesign of nonlinear optimal regulator.](http://arxiv.org/abs/2103.03808) | 强化学习可以无模型地重新设计针对非线性系统的最优控制器。为了提高学习性能并减少实验次数，提出了一个无模型的两步设计方法，先设计线性控制律达到初步控制，再使用强化学习进一步优化。 |
| [^158] | [Weak Signal Asymptotics for Sequentially Randomized Experiments.](http://arxiv.org/abs/2101.09855) | 本文使用弱信号渐近行为的方法研究了一类顺序随机实验，认为这类顺序实验的样本路径会弱收敛到扩散极限，并能获得关于几种顺序实验的后悔和信念演变的多个见解。 |
| [^159] | [Recursion, evolution and conscious self.](http://arxiv.org/abs/2001.11825) | 本文介绍并研究了一种基于自我参照的学习理论，结论与生物学、神经科学的科学发现一致，并提供了大量解释，包括演化和人脑的功能和学习能力。 |
| [^160] | [The Simulator: Understanding Adaptive Sampling in the Moderate-Confidence Regime.](http://arxiv.org/abs/1702.05186) | 提出一种名为模拟器的新技术用于分析自适应采样。将重点放在了区分好的采样策略和坏采样策略的难度上。在纯探索场景的结构化多臂赌博问题中应用了该技术，展示了有中等置信度的样本复杂度和文献中在 $\delta \to 0$ 时得到的渐近复杂度之间存在着实质性差异，并且还证明了作为顶部-k问题的第一个基于实例的下界。 |

# 详细

[^1]: BN与ReLU之间的不和谐引起梯度爆炸，但被激活之间的相关性所抵消。

    The Disharmony Between BN and ReLU Causes Gradient Explosion, but is Offset by the Correlation Between Activations. (arXiv:2304.11692v1 [cs.LG])

    [http://arxiv.org/abs/2304.11692](http://arxiv.org/abs/2304.11692)

    本研究阐述了BN和ReLU之间的不和谐是导致梯度爆炸的主要原因，同时发现输入之间的相关性可以缓解这个问题。提出一种基于二阶优化算法的自适应学习率算法，在大批量训练中表现优异，并可替代WarmUp，在小批量训练中也表现不错。

    

    基于批标准化和ReLU等激活函数的深度神经网络可能会在训练初期由于时间梯度爆炸而出现不稳定。我们解释了ReLU如何比预期更多地减少方差，以及批标准化如何在恢复期间放大梯度，导致前向传播保持稳定而梯度爆炸。此外，我们还讨论了深度神经网络在训练过程中的动力学变化以及输入之间的相关性如何缓解这个问题。最后，我们提出了一种灵感来自二阶优化算法的更好的自适应学习率算法，在大批量训练中优于现有的学习率缩放方法，并可替换小批量训练中的WarmUp。

    Deep neural networks based on batch normalization and ReLU-like activation functions can experience instability during the early stages of training due to the high gradient induced by temporal gradient explosion. We explain how ReLU reduces variance more than expected, and how batch normalization amplifies the gradient during recovery, which causes gradient explosion while forward propagation remains stable. Additionally, we discuss how the dynamics of a deep neural network change during training and how the correlation between inputs can alleviate this problem. Lastly, we propose a better adaptive learning rate algorithm inspired by second-order optimization algorithms, which outperforms existing learning rate scaling methods in large batch training and can also replace WarmUp in small batch training.
    
[^2]: 一种轻量级循环学习网络用于可持续压缩感知

    A Lightweight Recurrent Learning Network for Sustainable Compressed Sensing. (arXiv:2304.11674v1 [cs.CV])

    [http://arxiv.org/abs/2304.11674](http://arxiv.org/abs/2304.11674)

    本文提出了一种轻量级但高效的深度神经网络，基于循环学习来实现可持续的压缩感知系统；它需要更少的参数，但可以获得高质量的重构结果。

    

    最近，基于深度学习的压缩感知（CS）在减少传感系统的采样和计算成本以及提高重构质量方面取得了巨大成功。但是这些方法在很大程度上忽视了计算成本这一问题，它们依赖于复杂的结构和任务特定运算符设计，导致 CS 成像系统具有广泛的存储要求和高能耗。在本文中，我们提出了一种轻量级但高效的深度神经网络，基于循环学习来实现可持续的 CS 系统；它需要更少的参数，但可以获得高质量的重构结果。具体而言，我们提出的网络由初始重构子网络和残差重构子网络组成。初始重构子网络具有分层结构，逐步恢复图像，减少参数数量；而残差重构子网络则有助于从中间估计中提取循环残差特征，使网络对底层结构的捕捉更加有效。实验结果表明，所提出的网络在重构质量方面取得了最先进的性能，同时与现有方法相比，需要更少的参数和计算量。

    Recently, deep learning-based compressed sensing (CS) has achieved great success in reducing the sampling and computational cost of sensing systems and improving the reconstruction quality. These approaches, however, largely overlook the issue of the computational cost; they rely on complex structures and task-specific operator designs, resulting in extensive storage and high energy consumption in CS imaging systems. In this paper, we propose a lightweight but effective deep neural network based on recurrent learning to achieve a sustainable CS system; it requires a smaller number of parameters but obtains high-quality reconstructions. Specifically, our proposed network consists of an initial reconstruction sub-network and a residual reconstruction sub-network. While the initial reconstruction sub-network has a hierarchical structure to progressively recover the image, reducing the number of parameters, the residual reconstruction sub-network facilitates recurrent residual feature extr
    
[^3]: TinyML景观中缺失的设备管理和网络连接要素

    Device management and network connectivity as missing elements in TinyML landscape. (arXiv:2304.11669v1 [cs.NI])

    [http://arxiv.org/abs/2304.11669](http://arxiv.org/abs/2304.11669)

    TinyML面临硬件异构性、MCU架构和资源可用性限制等多重挑战。本文强调解决TinyML解决方案的网络连接问题需要更加注重标准协议、解决方案的互操作性及安全性，并介绍了LwM2M协议如何解决这些挑战。

    

    部署基于TinyML的解决方案需要解决多个挑战，包括硬件异构性、微控制器（MCU）架构和资源可用性限制。另一个挑战是MCU的操作系统多样性，有限的内存管理实现和设备之间的有限软件互操作性。本文讨论的挑战是这些解决方案的网络连接问题，指出应更强调标准协议、解决方案的互操作性和安全性。最后，本文讨论了LwM2M协议如何解决与网络连接和互操作性相关的挑战。

    Deployment of solutions based on TinyML requires meeting several challenges. These include hardware heterogeneity, microprocessor (MCU) architectures, and resource availability constraints. Another challenge is the variety of operating systems for MCU, limited memory management implementations and limited software interoperability between devices. A number of these challenges are solved by dedicated programming libraries and the ability to compile code for specific devices. Nevertheless, the challenge discussed in the paper is the issue of network connectivity for such solutions. We point out that more emphasis should be placed on standard protocols, interoperability of solutions and security. Finally, the paper discusses how the LwM2M protocol can solve the identified challenges related to network connectivity and interoperability.
    
[^4]: 大规模经验风险最小化加速双随机梯度算法

    Accelerated Doubly Stochastic Gradient Algorithm for Large-scale Empirical Risk Minimization. (arXiv:2304.11665v1 [cs.LG])

    [http://arxiv.org/abs/2304.11665](http://arxiv.org/abs/2304.11665)

    本文提出了一种针对大规模经验风险最小化问题的加速双随机梯度算法，采用新型加速多动量技术，每次迭代仅访问一小批样本和更新少量变量坐标，具有快速收敛和小内存占用特点。

    

    如今，对于人工智能应用来说，具有快速收敛、小内存占用和低每次迭代复杂度的算法特别有利。在本文中，我们提出了一种带有新型加速多动量技术的双随机算法，用于解决大规模经验风险最小化问题。虽然享有可证明的更优收敛速度，但每次迭代中，该算法仅访问一小批样本，同时更新少量变量坐标，大大减少了在涉及大量样本大小和超高维度时的内存引用量。我们进行了针对巨大规模数据集的实证研究，以展示我们的方法在实践中的效率。

    Nowadays, algorithms with fast convergence, small memory footprints, and low per-iteration complexity are particularly favorable for artificial intelligence applications. In this paper, we propose a doubly stochastic algorithm with a novel accelerating multi-momentum technique to solve large scale empirical risk minimization problem for learning tasks. While enjoying a provably superior convergence rate, in each iteration, such algorithm only accesses a mini batch of samples and meanwhile updates a small block of variable coordinates, which substantially reduces the amount of memory reference when both the massive sample size and ultra-high dimensionality are involved. Empirical studies on huge scale datasets are conducted to illustrate the efficiency of our method in practice.
    
[^5]: 深度平衡模型的高效训练

    Efficient Training of Deep Equilibrium Models. (arXiv:2304.11663v1 [cs.LG])

    [http://arxiv.org/abs/2304.11663](http://arxiv.org/abs/2304.11663)

    本文介绍一种简单而有效的策略来避免深度平衡模型层中反向传播的计算负担。该方法可以显著加速训练，同时不会影响性能。

    

    深度平衡模型（DEQ）在学习数据表示方面已经被证明非常强大。其思想是用隐式的固定点方程替换传统（显式的）前馈神经网络，从而允许解耦前向和后向传递。特别地，通过隐式函数定理，DEQ层的训练变得非常高效。但是，通过DEQ层的反向传播仍需要解决一个昂贵的基于Jacobian的方程。在本文中，我们介绍了一种简单而有效的策略来避免这种计算负担。我们的方法依赖于Broyden方法的Jacobian近似，在前向传递之后计算反向传递中的梯度。实验证明，简单地重复使用这个近似可以显著加速训练，同时不会造成性能降低。

    Deep equilibrium models (DEQs) have proven to be very powerful for learning data representations. The idea is to replace traditional (explicit) feedforward neural networks with an implicit fixed-point equation, which allows to decouple the forward and backward passes. In particular, training DEQ layers becomes very memory-efficient via the implicit function theorem. However, backpropagation through DEQ layers still requires solving an expensive Jacobian-based equation. In this paper, we introduce a simple but effective strategy to avoid this computational burden. Our method relies on the Jacobian approximation of Broyden's method after the forward pass to compute the gradients during the backward pass. Experiments show that simply re-using this approximation can significantly speed up the training while not causing any performance degradation.
    
[^6]: 捕捉对比图表示学习中的细粒度语义

    Capturing Fine-grained Semantics in Contrastive Graph Representation Learning. (arXiv:2304.11658v1 [cs.LG])

    [http://arxiv.org/abs/2304.11658](http://arxiv.org/abs/2304.11658)

    本文提出了一种新的增强细粒度语义对比学习方法（FSGCL），首先使用基于图案的图构造方法从输入数据中提取多种语义，在此基础上探索语义级对比任务，从而提高图对比学习的性能。

    

    图对比学习定义了一个对比的任务，以将相似的实例拉近，将不相似的实例推远，学习区分性节点嵌入而无需监督标签，这在过去几年中引起了越来越多的关注。然而，现有的图对比学习方法忽略了图中存在的不同语义之间的差异，学习了粗粒度的节点嵌入，导致下游任务表现亚优。为了弥补这一差距，本文提出了一种新颖的增强细粒度语义的对比学习方法（FSGCL）。具体而言，FSGCL首先引入了基于图案的图构造方法，从输入数据的角度利用图案提取了存在于图中的多种语义。然后，从模型训练的角度探索了语义级对比任务，进一步增强了对细粒度语义的利用。在五个真实数据集上的实验证明了FSGCL相比于最先进的图对比学习方法的优越性。

    Graph contrastive learning defines a contrastive task to pull similar instances close and push dissimilar instances away. It learns discriminative node embeddings without supervised labels, which has aroused increasing attention in the past few years. Nevertheless, existing methods of graph contrastive learning ignore the differences between diverse semantics existed in graphs, which learn coarse-grained node embeddings and lead to sub-optimal performances on downstream tasks. To bridge this gap, we propose a novel Fine-grained Semantics enhanced Graph Contrastive Learning (FSGCL) in this paper. Concretely, FSGCL first introduces a motif-based graph construction, which employs graph motifs to extract diverse semantics existed in graphs from the perspective of input data. Then, the semantic-level contrastive task is explored to further enhance the utilization of fine-grained semantics from the perspective of model training. Experiments on five real-world datasets demonstrate the superio
    
[^7]: 交通网络的随机单元传输模型

    Stochastic Cell Transmission Models of Traffic Networks. (arXiv:2304.11654v1 [cs.LG])

    [http://arxiv.org/abs/2304.11654](http://arxiv.org/abs/2304.11654)

    本文介绍了一个适用于交通网络的随机单元传输模型，通过偏好函数和可接受设计来评估交通系统的性能。数值实现结合了模拟、高斯过程回归和随机探索过程。

    

    我们为一般交通网络引入了随机单元传输模型的严格框架。通过偏好函数和可接受设计评估交通系统的性能。数值实现结合了模拟、高斯过程回归和随机探索过程。该方法在两个案例研究中得到了说明。

    We introduce a rigorous framework for stochastic cell transmission models for general traffic networks. The performance of traffic systems is evaluated based on preference functionals and acceptable designs. The numerical implementation combines simulation, Gaussian process regression, and a stochastic exploration procedure. The approach is illustrated in two case studies.
    
[^8]: 一种异步分散式算法解决 Wasserstein Barycenter 问题。

    An Asynchronous Decentralized Algorithm for Wasserstein Barycenter Problem. (arXiv:2304.11653v1 [cs.LG])

    [http://arxiv.org/abs/2304.11653](http://arxiv.org/abs/2304.11653)

    本文提出了一种用于 WBP 的异步分散式算法 A$^2$DWB，该算法通过更新本地变量以减轻等待开销，从而极大地提高了时间效率，实验证明其优于最新的同步算法。

    

    近年来，Wasserstein Barycenter Problem (WBP) 在人工智能领域受到了广泛关注。本文专注于 WBP 的分散式设置，并提出了一种异步分散式算法 (A$^2$DWB)。A$^2$DWB 基于一种新颖的随机块协调下降方法来优化熵正则化 WBP 的对偶问题。据我们所知，A$^2$DWB 是 WBP 的第一个异步分散式算法。与其同步版本不同的是，A$^2$DWB 以一种仅依赖于过时的邻居信息的方式更新本地变量，从而有效地减轻了等待开销，从而实质性地提高了时间效率。实验证明，与最新的同步算法相比，A$^2$DWB 的性能优异。

    Wasserstein Barycenter Problem (WBP) has recently received much attention in the field of artificial intelligence. In this paper, we focus on the decentralized setting for WBP and propose an asynchronous decentralized algorithm (A$^2$DWB). A$^2$DWB is induced by a novel stochastic block coordinate descent method to optimize the dual of entropy regularized WBP. To our knowledge, A$^2$DWB is the first asynchronous decentralized algorithm for WBP. Unlike its synchronous counterpart, it updates local variables in a manner that only relies on the stale neighbor information, which effectively alleviate the waiting overhead, and thus substantially improve the time efficiency. Empirical results validate its superior performance compared to the latest synchronous algorithm.
    
[^9]: SATIN：一个使用视觉-语言模型对卫星图像进行分类的多任务元数据集。

    SATIN: A Multi-Task Metadataset for Classifying Satellite Imagery using Vision-Language Models. (arXiv:2304.11619v1 [cs.CV])

    [http://arxiv.org/abs/2304.11619](http://arxiv.org/abs/2304.11619)

    本研究介绍了一个遥感图像元数据集SATIN，它由27个现有的遥感数据集组成，并使用一系列视觉-语言（VL）模型全面评估了它的零-shot转移分类能力。该研究发现SATIN是一个具有挑战性的基准测试，强大方法的分类精度为52.0％，并提供了一个公共排行榜以跟踪模型的进展。

    

    解释遥感图像可以实现许多下游应用，从土地利用规划到森林砍伐监测都有可能。由于地球地理多样性的存在，对这些数据进行稳健分类是具有挑战性的。虽然存在许多不同的卫星和航空图像分类数据集，但尚未有一个适合涵盖这种多样性的基准。在这项工作中，我们介绍了来自27个现有遥感数据集的元数据集SATellite ImageNet（SATIN），并全面评估了一系列视觉-语言（VL）模型在SATIN上的零-shot转移分类能力。我们发现SATIN是一个具有挑战性的基准测试-我们评估的最强方法的分类精度为52.0％。我们提供了一个公共排行榜，以指导和跟踪VL模型在这一重要领域的进展。

    Interpreting remote sensing imagery enables numerous downstream applications ranging from land-use planning to deforestation monitoring. Robustly classifying this data is challenging due to the Earth's geographic diversity. While many distinct satellite and aerial image classification datasets exist, there is yet to be a benchmark curated that suitably covers this diversity. In this work, we introduce SATellite ImageNet (SATIN), a metadataset curated from 27 existing remotely sensed datasets, and comprehensively evaluate the zero-shot transfer classification capabilities of a broad range of vision-language (VL) models on SATIN. We find SATIN to be a challenging benchmark-the strongest method we evaluate achieves a classification accuracy of 52.0%. We provide a $\href{https://satinbenchmark.github.io}{\text{public leaderboard}}$ to guide and track the progress of VL models in this important domain.
    
[^10]: 迭代图形细化的基于原型的标签传播的迁移少样学习。

    Transductive Few-shot Learning with Prototype-based Label Propagation by Iterative Graph Refinement. (arXiv:2304.11598v1 [cs.CV])

    [http://arxiv.org/abs/2304.11598](http://arxiv.org/abs/2304.11598)

    本文提出一种基于原型的标签传播方法，通过迭代图形细化解决了传统方法中原型估计不准确和核函数下亚优图形构建的问题，在少样迁移学习和半监督FSL方面表现优异。

    

    少样学习(FSL)因其适应新领域的能力而受欢迎。与归纳式少样学习相比，传导式模型通常表现更好，因为它们利用查询集的所有样本。现有方法的两个类别，基于原型和基于图形的方法，分别具有原型估计不准确和核函数下亚优图形构建的缺点。本文提出了一种新的基于原型的标签传播方法来解决这些问题。具体而言，我们的图形构建基于原型和样本之间的关系，而不是样本之间。随着原型的更新，图形也会随之变化。我们还估计每个原型的标签，而不是将原型视为类中心。在mini-ImageNet，tiered-ImageNet，CIFAR-FS和CUB数据集上，我们展示了所提出的方法在少样迁移学习和半监督FSL方面优于其他现有最先进的方法。

    Few-shot learning (FSL) is popular due to its ability to adapt to novel classes. Compared with inductive few-shot learning, transductive models typically perform better as they leverage all samples of the query set. The two existing classes of methods, prototype-based and graph-based, have the disadvantages of inaccurate prototype estimation and sub-optimal graph construction with kernel functions, respectively. In this paper, we propose a novel prototype-based label propagation to solve these issues. Specifically, our graph construction is based on the relation between prototypes and samples rather than between samples. As prototypes are being updated, the graph changes. We also estimate the label of each prototype instead of considering a prototype be the class centre. On mini-ImageNet, tiered-ImageNet, CIFAR-FS and CUB datasets, we show the proposed method outperforms other state-of-the-art methods in transductive FSL and semi-supervised FSL when some unlabeled data accompanies the 
    
[^11]: 基于偏相关的深度视觉表示学习用于图像分类

    Learning Partial Correlation based Deep Visual Representation for Image Classification. (arXiv:2304.11597v1 [cs.CV])

    [http://arxiv.org/abs/2304.11597](http://arxiv.org/abs/2304.11597)

    本文提出了一种基于偏相关的深度视觉表示学习方法，解决了使用协方差矩阵表征相关性在存在混淆效应时的误导问题。

    

    基于协方差矩阵的视觉表示已经证明了其在图像分类中的有效性，通过对卷积特征映射中不同通道之间的成对相关性进行建模。然而，如果存在另一个通道与感兴趣的两个通道相关，则成对相关性将变得误导人，导致“混淆”效应。针对这种情况，应该估计“偏相关”，以消除混淆效应。然而，可靠地估计偏相关需要解决一个对称正定矩阵优化问题，即稀疏逆协方差矩阵估计（SICE）。如何将此过程融入CNN中仍然是一个开放问题。在这项工作中，我们将SICE制定为CNN的一个新结构层。为确保端到端的可训练性，我们开发了一种迭代方法，在前向和后向传播步骤中解决上述矩阵优化问题。我们的工作获得了基于偏相关的深度视觉表示。

    Visual representation based on covariance matrix has demonstrates its efficacy for image classification by characterising the pairwise correlation of different channels in convolutional feature maps. However, pairwise correlation will become misleading once there is another channel correlating with both channels of interest, resulting in the ``confounding'' effect. For this case, ``partial correlation'' which removes the confounding effect shall be estimated instead. Nevertheless, reliably estimating partial correlation requires to solve a symmetric positive definite matrix optimisation, known as sparse inverse covariance estimation (SICE). How to incorporate this process into CNN remains an open issue. In this work, we formulate SICE as a novel structured layer of CNN. To ensure end-to-end trainability, we develop an iterative method to solve the above matrix optimisation during forward and backward propagation steps. Our work obtains a partial correlation based deep visual representa
    
[^12]: 非欧几里得域中的分割问题：挑战与机遇

    Segment Anything in Non-Euclidean Domains: Challenges and Opportunities. (arXiv:2304.11595v1 [cs.CV])

    [http://arxiv.org/abs/2304.11595](http://arxiv.org/abs/2304.11595)

    本文探讨了一种新的Segment Non-Euclidean Anything（SNA）方法，旨在扩大语义分割的范围，导致更多的基础模型用于非欧几里得域中的分割任务。

    

    最近的一项工作称为Segment Anything（SA）在将语义分割的边界推向基础模型时取得了重大进展。SA的影响引发了极为活跃的讨论，并引领了一波新的发展浪潮，用于欧几里得域中的各种任务，如物体检测和图像修复。尽管SA所带来的进展很有前途，但该概念尚未扩展到非欧几里得图形领域。在本文中，我们探索了一种称为Segment Non-Euclidean Anything（SNA）的新范式，该范式旨在开发能够处理非欧几里得域中各种图形数据的基础模型，试图扩大SA的范围并为未来的研究奠定基础。为了实现这一目标，我们首先讨论了与SA相关的基础模型的最新成就。然后，我们着眼于将SA概念应用于图形数据时出现的独特挑战。

    The recent work known as Segment Anything (SA) has made significant strides in pushing the boundaries of semantic segmentation into the era of foundation models. The impact of SA has sparked extremely active discussions and ushered in an encouraging new wave of developing foundation models for the diverse tasks in the Euclidean domain, such as object detection and image inpainting. Despite the promising advances led by SA, the concept has yet to be extended to the non-Euclidean graph domain. In this paper, we explore a novel Segment Non-Euclidean Anything (SNA) paradigm that strives to develop foundation models that can handle the diverse range of graph data within the non-Euclidean domain, seeking to expand the scope of SA and lay the groundwork for future research in this direction. To achieve this goal, we begin by discussing the recent achievements in foundation models associated with SA. We then shed light on the unique challenges that arise when applying the SA concept to graph a
    
[^13]: 系统III：在安全约束下学习领域知识

    System III: Learning with Domain Knowledge for Safety Constraints. (arXiv:2304.11593v1 [cs.LG])

    [http://arxiv.org/abs/2304.11593](http://arxiv.org/abs/2304.11593)

    本文提出了一种称为System III的新颖框架，该框架将安全性的领域专家知识表示为一阶逻辑形式，通过p范数评估约束的满足程度，用于在安全关键领域中整合领域知识以帮助引导安全探索，并提高样本效率。实验证明该方法在一个模拟自主驾驶任务上有效。

    

    强化学习代理通过大量探索自然地学习。探索是昂贵的，并且在安全关键领域中可能不安全。本文提出了一种新颖的框架，用于整合领域知识以帮助引导安全探索并提高样本效率。我们的方法被称为System III，受心理学家关于大脑System I和System II的概念启发，我们将安全性的领域专家知识表示为一阶逻辑形式。我们通过状态向量空间中的p范数评估这些约束的满足程度。在我们的公式中，约束类似于在探索过程中必须避免的状态的危险，对象和区域。我们在模拟自主驾驶任务上评估了我们的方法的有效性，其中代理需要在避免与障碍物碰撞的同时安全导航轨迹。实验结果表明，我们的方法成功地整合了专家领域知识，以安全地引导探索并提高样本效率。

    Reinforcement learning agents naturally learn from extensive exploration. Exploration is costly and can be unsafe in $\textit{safety-critical}$ domains. This paper proposes a novel framework for incorporating domain knowledge to help guide safe exploration and boost sample efficiency. Previous approaches impose constraints, such as regularisation parameters in neural networks, that rely on large sample sets and often are not suitable for safety-critical domains where agents should almost always avoid unsafe actions. In our approach, called $\textit{System III}$, which is inspired by psychologists' notions of the brain's $\textit{System I}$ and $\textit{System II}$, we represent domain expert knowledge of safety in form of first-order logic. We evaluate the satisfaction of these constraints via p-norms in state vector space. In our formulation, constraints are analogous to hazards, objects, and regions of state that have to be avoided during exploration. We evaluated the effectiveness o
    
[^14]: GPS轨迹生成的扩散模型

    Diffusion Model for GPS Trajectory Generation. (arXiv:2304.11582v1 [cs.LG])

    [http://arxiv.org/abs/2304.11582](http://arxiv.org/abs/2304.11582)

    该论文提出了基于扩散模型的GPS轨迹生成框架，通过将真实轨迹逐渐转换为噪声，再从噪声重构伪造的轨迹，以达到生成隐私信息保护的高质量轨迹的目的。

    

    随着GPS设备和数据采集技术的部署，大量的GPS轨迹数据为推进时空数据挖掘研究提供了核心支持。但是，GPS轨迹包括个人地理位置信息，因此不可避免地涉及到隐私问题。解决此问题的一种有前途的方法是使用轨迹生成，用生成的无隐私信息替换原始数据。然而，由于人类活动的复杂和随机行为，生成高质量的轨迹仍处于初级阶段。为了实现这一目标，我们提出了一个基于扩散的轨迹生成（Diff-Traj）框架，有效地将扩散模型的生成能力与轨迹的时空特征学习相结合。具体地，我们通过前向轨迹噪声处理逐渐将真实轨迹转换为噪声。然后，Diff-Traj从噪声重构伪造的轨迹。

    With the deployment of GPS-enabled devices and data acquisition technology, the massively generated GPS trajectory data provide a core support for advancing spatial-temporal data mining research. Nonetheless, GPS trajectories comprise personal geo-location information, rendering inevitable privacy concerns on plain data. One promising solution to this problem is trajectory generation, replacing the original data with the generated privacy-free ones. However, owing to the complex and stochastic behavior of human activities, generating high-quality trajectories is still in its infancy. To achieve the objective, we propose a diffusion-based trajectory generation (Diff-Traj) framework, effectively integrating the generation capability of the diffusion model and learning from the spatial-temporal features of trajectories. Specifically, we gradually convert real trajectories to noise through a forward trajectory noising process. Then, Diff-Traj reconstructs forged trajectories from the noise
    
[^15]: 利用基于自编码器的内容感知二维表示识别时间序列中的随机性：黑洞数据应用。

    Identifying Stochasticity in Time-Series with Autoencoder-Based Content-aware 2D Representation: Application to Black Hole Data. (arXiv:2304.11560v1 [cs.LG])

    [http://arxiv.org/abs/2304.11560](http://arxiv.org/abs/2304.11560)

    通过自编码器和内容感知2D表示，该论文提出了一个识别随机性的方法并对黑洞数据进行了应用。

    

    本文报告了一种基于自编码器的2D表示方法，用于将时间序列分类为随机或非随机，以了解其潜在的物理过程。提出了一种内容感知的1D时间序列到2D表示的转换，同时利用时间和频域特征。使用损失函数训练自编码器学习潜在空间（同时使用时间和频域）表示，旨在是时间不变的。时间序列的每个元素都表示为一个元组，每个元组从时间和频率域的潜在空间表示中各取一个元素，形成一个二进制图像。在这个二进制图像中，代表时间序列点的那些元组组合在一起形成了输入时间序列的“潜空间签名”（LSS）。获得的二进制LSS图像被送入分类网络。使用421个合成时间序列训练EfficientNetv2-S分类器，其中包括随机和非随机序列，以进行评估和比较。

    In this work, we report an autoencoder-based 2D representation to classify a time-series as stochastic or non-stochastic, to understand the underlying physical process. Content-aware conversion of 1D time-series to 2D representation, that simultaneously utilizes time- and frequency-domain characteristics, is proposed. An autoencoder is trained with a loss function to learn latent space (using both time- and frequency domains) representation, that is designed to be, time-invariant. Every element of the time-series is represented as a tuple with two components, one each, from latent space representation in time- and frequency-domains, forming a binary image. In this binary image, those tuples that represent the points in the time-series, together form the ``Latent Space Signature" (LSS) of the input time-series. The obtained binary LSS images are fed to a classification network. The EfficientNetv2-S classifier is trained using 421 synthetic time-series, with fair representation from both
    
[^16]: 双层注意力图神经网络

    Bi-Level Attention Graph Neural Networks. (arXiv:2304.11533v1 [cs.LG])

    [http://arxiv.org/abs/2304.11533](http://arxiv.org/abs/2304.11533)

    提出了一种双层注意力图神经网络，用于处理多关系和多实体的大规模异构图。通过基于两个重要性级别的层次图关注机制，以个性化的方式模拟节点-节点和关系-关系的相互作用。

    

    最近，具有注意力机制的图神经网络(GNNs)在历史上一直局限于小规模同质图(HoGs)。然而，处理异构图(HeGs)的GNNs，在处理注意力方面存在缺陷。大多数处理HeGs的GNNs只学习节点级别或关系级别的注意力，而不是两者兼备，限制了它们在预测HeGs中的重要实体和关系方面的能力。即使是现有学习两种级别注意力的最佳方法，也存在假定图关系是独立的，并且其学习的注意力忽略了这种依赖关联的限制。为了有效地模拟多关系和多实体的大规模HeGs，我们提出了双层注意力图神经网络(BA-GNN)，这是一种可扩展的神经网络(NNs)，通过基于两个重要性级别的层次图关注机制，以个性化的方式模拟了节点-节点和关系-关系的相互作用，并学会了考虑图关系之间的依赖关联。

    Recent graph neural networks (GNNs) with the attention mechanism have historically been limited to small-scale homogeneous graphs (HoGs). However, GNNs handling heterogeneous graphs (HeGs), which contain several entity and relation types, all have shortcomings in handling attention. Most GNNs that learn graph attention for HeGs learn either node-level or relation-level attention, but not both, limiting their ability to predict both important entities and relations in the HeG. Even the best existing method that learns both levels of attention has the limitation of assuming graph relations are independent and that its learned attention disregards this dependency association. To effectively model both multi-relational and multi-entity large-scale HeGs, we present Bi-Level Attention Graph Neural Networks (BA-GNN), scalable neural networks (NNs) that use a novel bi-level graph attention mechanism. BA-GNN models both node-node and relation-relation interactions in a personalized way, by hier
    
[^17]: 三元结构信息建模用于准确、可解释和交互式推荐

    Triple Structural Information Modelling for Accurate, Explainable and Interactive Recommendation. (arXiv:2304.11528v1 [cs.IR])

    [http://arxiv.org/abs/2304.11528](http://arxiv.org/abs/2304.11528)

    该论文提出了TriSIM4Rec算法，它基于动态交互图，同时利用用户-物品共现、用户交互时序信息和物品对的转移概率三种结构信息，进而实现了更准确、可解释和交互式的推荐。

    

    在动态交互图中，用户与物品的交互通常遵循异构模式，表示为不同的结构信息，如用户-物品共现、用户交互的时序信息和物品对的转移概率。然而，现有方法不能同时利用这三种结构信息，导致表现不佳。为此，我们提出了TriSIM4Rec，一种基于三元结构信息建模的动态交互图准确、可解释和交互式推荐方法。具体地，TriSIM4Rec包括1)一个动态理想低通图滤波器，通过增量奇异值分解（SVD）动态地挖掘用户-物品交互中的共现信息；2)一个无需参数的注意力模块，以有效、高效地捕获用户交互的时序信息；和3)一个物品转移矩阵以存储物品对的转移概率。

    In dynamic interaction graphs, user-item interactions usually follow heterogeneous patterns, represented by different structural information, such as user-item co-occurrence, sequential information of user interactions and the transition probabilities of item pairs. However, the existing methods cannot simultaneously leverage all three structural information, resulting in suboptimal performance. To this end, we propose TriSIM4Rec, a triple structural information modeling method for accurate, explainable and interactive recommendation on dynamic interaction graphs. Specifically, TriSIM4Rec consists of 1) a dynamic ideal low-pass graph filter to dynamically mine co-occurrence information in user-item interactions, which is implemented by incremental singular value decomposition (SVD); 2) a parameter-free attention module to capture sequential information of user interactions effectively and efficiently; and 3) an item transition matrix to store the transition probabilities of item pairs.
    
[^18]: 深度神经网络的分层加权平均方法

    Hierarchical Weight Averaging for Deep Neural Networks. (arXiv:2304.11519v1 [cs.LG])

    [http://arxiv.org/abs/2304.11519](http://arxiv.org/abs/2304.11519)

    HWA是一种将在线WA和离线WA相结合的通用训练框架，在提高SGD性能方面表现出色。

    

    尽管随机梯度下降（SGD）算法非常简单，但对于深度神经网络（DNNs）的训练非常成功。在各种试图改进SGD的尝试中，加权平均（WA）近来在文献中受到了广泛的关注，它平均了多个模型的权重。WA分为两类：在线WA和离线WA。我们在本工作中将这两种方法结合起来，提出了一种名为层次加权平均（HWA）的通用训练框架。我们展示了HWA能够更好地利用在线和离线WA的优势，并且可以用于改进SGD。我们也从理论上和实际上研究了HWA的收敛性质，并在基准数据集上进行了大量实验。我们的实验表明，HWA可以持续改善SGD的性能，并且相对于最先进的方法，可以取得竞争性的结果。

    Despite the simplicity, stochastic gradient descent (SGD)-like algorithms are successful in training deep neural networks (DNNs). Among various attempts to improve SGD, weight averaging (WA), which averages the weights of multiple models, has recently received much attention in the literature. Broadly, WA falls into two categories: 1) online WA, which averages the weights of multiple models trained in parallel, is designed for reducing the gradient communication overhead of parallel mini-batch SGD, and 2) offline WA, which averages the weights of one model at different checkpoints, is typically used to improve the generalization ability of DNNs. Though online and offline WA are similar in form, they are seldom associated with each other. Besides, these methods typically perform either offline parameter averaging or online parameter averaging, but not both. In this work, we firstly attempt to incorporate online and offline WA into a general training framework termed Hierarchical Weight 
    
[^19]: LayerNAS：多目标神经架构搜索的多项式复杂度方法

    LayerNAS: Neural Architecture Search in Polynomial Complexity. (arXiv:2304.11517v1 [cs.LG])

    [http://arxiv.org/abs/2304.11517](http://arxiv.org/abs/2304.11517)

    LayerNAS提出了一种多项式复杂度的神经架构搜索方法，将搜索分为多个目标，并将搜索成本和奖励元素分开，能够快速有效地发现优越模型。

    

    神经架构搜索（NAS）已成为发现有效模型架构的流行方法，尤其是对于目标硬件而言。因此，在约束条件下找到最佳架构的NAS方法至关重要。在本文中，我们提出了LayerNAS，将多目标NAS的挑战转化为组合优化问题，有效地将搜索复杂度限制为多项式。对于具有$L$层的模型架构，我们为每一层执行逐层搜索，从一个搜索选项集$\mathbb{S}$中进行选择。LayerNAS根据一个目标，例如模型大小或延迟，对模型候选进行分组，并基于另一个目标搜索最佳模型，从而分割了搜索的成本和奖励元素。这种方法将搜索复杂度限制在$O(H \cdot |\mathbb{S}| \cdot L)$，其中$H$是在LayerNAS中设定的常数。我们的实验表明，LayerNAS能够持续发现优越的模型。

    Neural Architecture Search (NAS) has become a popular method for discovering effective model architectures, especially for target hardware. As such, NAS methods that find optimal architectures under constraints are essential. In our paper, we propose LayerNAS to address the challenge of multi-objective NAS by transforming it into a combinatorial optimization problem, which effectively constrains the search complexity to be polynomial.  For a model architecture with $L$ layers, we perform layerwise-search for each layer, selecting from a set of search options $\mathbb{S}$. LayerNAS groups model candidates based on one objective, such as model size or latency, and searches for the optimal model based on another objective, thereby splitting the cost and reward elements of the search. This approach limits the search complexity to $ O(H \cdot |\mathbb{S}| \cdot L) $, where $H$ is a constant set in LayerNAS.  Our experiments show that LayerNAS is able to consistently discover superior models
    
[^20]: QuMoS: 保护量子机器学习模型安全的框架

    QuMoS: A Framework for Preserving Security of Quantum Machine Learning Model. (arXiv:2304.11511v1 [quant-ph])

    [http://arxiv.org/abs/2304.11511](http://arxiv.org/abs/2304.11511)

    QuMoS是一个保护 QML 模型安全的框架，通过经典加密、量子混淆和诱饵样本等多种技术来保护模型免受窃取攻击，并具有较高的分类准确性。

    

    安全性一直是机器学习应用中的重要问题。由于模型训练的高成本，如收集相关样本、标记数据和消耗计算资源等，模型窃取攻击是最基本但至关重要的问题之一。而在量子计算中，这样的量子机器学习（QML）模型窃取攻击也存在，甚至更加严重，因为传统的加密方法很难直接应用于量子计算。另一方面，由于有限的量子计算资源，近期培训 QML 模型的货币成本甚至可能比经典模型更高。因此，一家公司开发的经过良好调整的 QML 模型可以被委派给量子云提供商作为服务，供普通用户使用。在这种情况下，如果云提供商受到攻击，QML 模型将泄漏。为了解决这个问题，我们提出了一个新的框架，即 QuMoS，用于保护 QML 模型的安全性。QuMoS 包括一系列技术，包括经典加密、量子混淆和诱饵样本，以防止模型窃取攻击。我们还提供了使用 PennyLane 软件库和 Google Cirq 包的具体 QuMoS 实现。模拟结果表明，我们的框架可以有效地防止 QML 模型被盗，同时保持高分类准确性。

    Security has always been a critical issue in machine learning (ML) applications. Due to the high cost of model training -- such as collecting relevant samples, labeling data, and consuming computing power -model-stealing attack is one of the most fundamental but vitally important issues. When it comes to quantum computing, such a quantum machine learning (QML) model-stealing attack also exists and it is even more severe because the traditional encryption method can hardly be directly applied to quantum computation. On the other hand, due to the limited quantum computing resources, the monetary cost of training QML model can be even higher than classical ones in the near term. Therefore, a well-tuned QML model developed by a company can be delegated to a quantum cloud provider as a service to be used by ordinary users. In this case, the QML model will be leaked if the cloud provider is under attack. To address such a problem, we propose a novel framework, namely QuMoS, to preserve mod
    
[^21]: 用于一体化事件持续时间预测的机器学习框架

    Machine learning framework for end-to-end implementation of Incident duration prediction. (arXiv:2304.11507v1 [cs.LG])

    [http://arxiv.org/abs/2304.11507](http://arxiv.org/abs/2304.11507)

    该研究提出了一个端到端机器学习框架，通过分析交通事故报告中可以获取的信息，预测并优化应急响应团队的资源部署，从而减少事故持续时间，同时提高公众安全。

    

    非经常性事件，如车辆碰撞和障碍物所引起的交通拥堵是交通管理中心(TMCs)的一个关键问题。及时清除事件对于改善安全、减少延误和排放对公众出行的影响至关重要。然而，TMCs和其他应急响应团队往往难以准确地预测事件的持续时间(等到路面得到清理)，使得决策如何部署资源存在挑战。为了解决这个问题，该研究开发了一种分析框架和端到端的机器学习解决方案，根据事件报告后立即可用的信息预测事件持续时间。对事件持续时间的质量预测可以帮助TMCs和其他应急响应团队采取主动措施，部署救援服务，如拖车、维护团队或激活备选路线。预测使用分类和回归机器学习模块的组合。该模型在来自佛罗里达交通部的实际数据中进行了评估，结果很有希望。

    Traffic congestion caused by non-recurring incidents such as vehicle crashes and debris is a key issue for Traffic Management Centers (TMCs). Clearing incidents in a timely manner is essential for improving safety and reducing delays and emissions for the traveling public. However, TMCs and other responders face a challenge in predicting the duration of incidents (until the roadway is clear), making decisions of what resources to deploy difficult. To address this problem, this research developed an analytical framework and end-to-end machine-learning solution for predicting incident duration based on information available as soon as an incident report is received. Quality predictions of incident duration can help TMCs and other responders take a proactive approach in deploying responder services such as tow trucks, maintenance crews or activating alternative routes. The predictions use a combination of classification and regression machine learning modules. The performance of the devel
    
[^22]: 金融机构中的客户流失因果分析的高维特征空间效果约束改进

    Improved Churn Causal Analysis Through Restrained High-Dimensional Feature Space Effects in Financial Institutions. (arXiv:2304.11503v1 [cs.LG])

    [http://arxiv.org/abs/2304.11503](http://arxiv.org/abs/2304.11503)

    本研究提出了一个概念性框架，通过特征限制来改善金融机构中的客户流失因果分析，以发现与独立变量相关且与影响流失的因变量因果相关的混淆特征。

    

    客户流失指终止与企业的关系或在特定期间减少客户参与。由于获取新客户的成本可能是保留现有客户成本的五到六倍，因此在有流失风险的客户上进行投资是明智的。客户流失模型的因果分析可以预测客户在可预见的未来是否会流失，并确定导致流失的效果和可能的原因。本研究提出了一个概念框架，以发现与独立变量相关且与影响流失的因变量因果相关的混淆特征。我们结合了不同的算法，包括SMOTE、集成ANN和贝叶斯网络，以解决在金融机构中生成的大规模和高维金融数据的客户流失预测问题，这些数据通常由客户关系管理系统中使用的基于区间的特征产生。还评估了维度诅咒和维度的福音效应。

    Customer churn describes terminating a relationship with a business or reducing customer engagement over a specific period. Customer acquisition cost can be five to six times that of customer retention, hence investing in customers with churn risk is wise. Causal analysis of the churn model can predict whether a customer will churn in the foreseeable future and identify effects and possible causes for churn. In general, this study presents a conceptual framework to discover the confounding features that correlate with independent variables and are causally related to those dependent variables that impact churn. We combine different algorithms including the SMOTE, ensemble ANN, and Bayesian networks to address churn prediction problems on a massive and high-dimensional finance data that is usually generated in financial institutions due to employing interval-based features used in Customer Relationship Management systems. The effects of the curse and blessing of dimensionality assessed 
    
[^23]: 物理引导生成对抗网络学习物理模型

    Physics-guided generative adversarial network to learn physical models. (arXiv:2304.11488v1 [cs.LG])

    [http://arxiv.org/abs/2304.11488](http://arxiv.org/abs/2304.11488)

    该论文提出了一种物理引导生成对抗网络（PG-GAN）的模型，以判断神经网络输出是否符合物理规律，解决了DNNs输出不满足物理方程的问题。

    

    本文介绍了一种引导深度神经网络（DNNs）训练的方法，以学习合理的物理解决方案。DNNs广泛用于预测物理和机械现象。其中一个问题是DNNs的输出并不总是满足物理方程。考虑物理方程的一个方法是将方程残差添加到损失函数中，这被称为物理信息神经网络（PINN）。PINN的一个特点是物理方程及其残差必须作为神经网络模型的一部分来实现。此外，残差并不总是收敛到很小的值。所提出的模型是一种物理引导生成对抗网络（PG-GAN），它使用GAN架构，在其中物理方程被用来判断神经网络的输出是否符合物理学。该方法应用于一个简单问题，以评估其潜在的可用性。

    This short note describes the concept of guided training of deep neural networks (DNNs) to learn physically reasonable solutions. DNNs are being widely used to predict phenomena in physics and mechanics. One of the issues of DNNs is that their output does not always satisfy physical equations. One approach to consider physical equations is adding a residual of equations into the loss function; this is called physics-informed neural network (PINN). One feature of PINNs is that the physical equations and corresponding residual must be implemented as part of a neural network model. In addition, the residual does not always converge to a small value. The proposed model is a physics-guided generative adversarial network (PG-GAN) that uses a GAN architecture in which physical equations are used to judge whether the neural network's output is consistent with physics. The proposed method was applied to a simple problem to assess its potential usability.
    
[^24]: 识别与帮助社区成员的帮派相关社交媒体交流中的词汇偏差研究

    Understanding Lexical Biases when Identifying Gang-related Social Media Communications. (arXiv:2304.11485v1 [cs.CL])

    [http://arxiv.org/abs/2304.11485](http://arxiv.org/abs/2304.11485)

    本研究使用自然语言处理工具有效识别了帮派相关社交媒体上可能需要社区资源帮助的人群，拓展了社区成员照顾的范畴。

    

    参与帮派活动的人使用包括Facebook和Twitter在内的主流社交媒体来表达嘲讽和威胁以及哀悼和纪念。然而，识别帮派相关活动的影响以通过社交媒体为社区成员提供帮助是具有独特挑战的，这包括道德上识别受帮派活动影响的个体的训练数据的困难和需要考虑这些个体在推文中常用的非标准语言风格。我们的研究提供了证据表明，自然语言处理工具可以有效地识别可能需要社区照顾资源，如顾问、冲突调解者或学术/专业培训计划的个体。我们证明了我们的二元逻辑分类器在识别受帮派相关暴力影响的个体时，在使用与2015年巴尔的摩暴动相关的帮派相关推文样本时，优于基线标准。

    Individuals involved in gang-related activity use mainstream social media including Facebook and Twitter to express taunts and threats as well as grief and memorializing. However, identifying the impact of gang-related activity in order to serve community member needs through social media sources has a unique set of challenges. This includes the difficulty of ethically identifying training data of individuals impacted by gang activity and the need to account for a non-standard language style commonly used in the tweets from these individuals. Our study provides evidence of methods where natural language processing tools can be helpful in efficiently identifying individuals who may be in need of community care resources such as counselors, conflict mediators, or academic/professional training programs. We demonstrate that our binary logistic classifier outperforms baseline standards in identifying individuals impacted by gang-related violence using a sample of gang-related tweets associ
    
[^25]: (向量)空间不是最后的疆域：将产品搜索看作程序合成

    (Vector) Space is Not the Final Frontier: Product Search as Program Synthesis. (arXiv:2304.11473v1 [cs.IR])

    [http://arxiv.org/abs/2304.11473](http://arxiv.org/abs/2304.11473)

    本文主张将产品搜索看作程序合成，相比向量空间模型有着重大优势。

    

    随着电子商务的不断增长，巨额投资用于信息检索的机器学习和自然语言处理也随之而来。虽然向量空间模型主宰了产品搜索中的检索模型，但随着深度学习的出现，向量化本身也发生了巨大变化。我们的立场论文以相反的方式主张，即程序合成对许多查询和市场中的大量参与者提供了重大优势。我们详细说明了所提出方法的行业重要性，概述了具体实现细节，并基于我们在Tooso构建类似系统的经验，回答了一些常见的反对意见。

    As ecommerce continues growing, huge investments in ML and NLP for Information Retrieval are following. While the vector space model dominated retrieval modelling in product search - even as vectorization itself greatly changed with the advent of deep learning -, our position paper argues in a contrarian fashion that program synthesis provides significant advantages for many queries and a significant number of players in the market. We detail the industry significance of the proposed approach, sketch implementation details, and address common objections drawing from our experience building a similar system at Tooso.
    
[^26]: 预训练语音和音频嵌入与情感识别的比较研究

    A Comparative Study of Pre-trained Speech and Audio Embeddings for Speech Emotion Recognition. (arXiv:2304.11472v1 [eess.AS])

    [http://arxiv.org/abs/2304.11472](http://arxiv.org/abs/2304.11472)

    本文对来自八个语音和音频PTMs提取的嵌入进行了比较分析，旨在提高情感识别模型的发展速度和效率，并使其能够在实际环境中得到应用。

    

    预训练模型（PTMs）在语音和音频领域中表现出巨大的应用潜力。从这些模型中提取出的嵌入可以作为输入，用于学习算法，可以应用于各种下游任务。其中一个关键任务是情感识别，它具有广泛的应用，包括对顾客呼叫的动态分析、心理健康评估和个性化语言学习等。PTM嵌入有助于推动情感识别的发展，但缺乏一个考虑多个方面的综合比较，例如嵌入模型架构、用于预训练的数据以及预训练过程等。PTM嵌入的彻底比较将有助于更快，更高效地开发模型，并使它们能够在实际场景中得到应用。本文利用这一研究空白，对来自八个语音和音频PTMs提取的嵌入进行了比较分析（包括wav2vec 2.0，data2vec，wavLM，UniSpeec）

    Pre-trained models (PTMs) have shown great promise in the speech and audio domain. Embeddings leveraged from these models serve as inputs for learning algorithms with applications in various downstream tasks. One such crucial task is Speech Emotion Recognition (SER) which has a wide range of applications, including dynamic analysis of customer calls, mental health assessment, and personalized language learning. PTM embeddings have helped advance SER, however, a comprehensive comparison of these PTM embeddings that consider multiple facets such as embedding model architecture, data used for pre-training, and the pre-training procedure being followed is missing. A thorough comparison of PTM embeddings will aid in the faster and more efficient development of models and enable their deployment in real-world scenarios. In this work, we exploit this research gap and perform a comparative analysis of embeddings extracted from eight speech and audio PTMs (wav2vec 2.0, data2vec, wavLM, UniSpeec
    
[^27]: 学习时扩大范围：嵌套子空间中的自适应贝叶斯优化

    Increasing the Scope as You Learn: Adaptive Bayesian Optimization in Nested Subspaces. (arXiv:2304.11468v1 [cs.LG])

    [http://arxiv.org/abs/2304.11468](http://arxiv.org/abs/2304.11468)

    该论文提出了一种自适应贝叶斯优化方法BAxUS，通过利用嵌套子空间来避免高维贝叶斯优化中的风险并确保高性能，相对于现有最先进方法在广泛应用中取得更好结果。

    

    最近的进展将贝叶斯优化（BO）的范围扩展到了具有几十个维度的昂贵黑盒函数，并渴望在生命科学、神经架构搜索和机器人等领域实现重大应用。然而，对高维贝叶斯优化（HDBO）的现有方法的更深入研究表明，随着维度数量的增加，性能会降低，甚至有失败风险，如果不满足某些无法验证的假设。该论文提出了BAxUS，它利用一族新颖的嵌套随机子空间来使其优化的空间适应问题。这确保了高性能，同时通过理论保证消除了失败的风险。全面评估表明，对于广泛的应用，BAxUS比现有的最先进方法取得了更好的结果。

    Recent advances have extended the scope of Bayesian optimization (BO) to expensive-to-evaluate black-box functions with dozens of dimensions, aspiring to unlock impactful applications, for example, in the life sciences, neural architecture search, and robotics. However, a closer examination reveals that the state-of-the-art methods for high-dimensional Bayesian optimization (HDBO) suffer from degrading performance as the number of dimensions increases or even risk failure if certain unverifiable assumptions are not met. This paper proposes BAxUS that leverages a novel family of nested random subspaces to adapt the space it optimizes over to the problem. This ensures high performance while removing the risk of failure, which we assert via theoretical guarantees. A comprehensive evaluation demonstrates that BAxUS achieves better results than the state-of-the-art methods for a broad set of applications.
    
[^28]: 循环神经网络和长短期记忆网络：教程和调研

    Recurrent Neural Networks and Long Short-Term Memory Networks: Tutorial and Survey. (arXiv:2304.11461v1 [cs.LG])

    [http://arxiv.org/abs/2304.11461](http://arxiv.org/abs/2304.11461)

    本文是一篇关于循环神经网络（RNN）、长短期记忆网络（LSTM）及其变体的教程和调研，介绍了解决长期依赖问题的方法，以及双向RNN、双向LSTM和ELMo网络等进一步的应用。

    

    本文是一篇关于循环神经网络（RNN）、长短期记忆网络（LSTM）及其变体的教程。我们首先从动态系统和RNN的时间反向传播开始讲述，然后讨论长期依赖问题中的梯度消失和梯度爆炸。接着，我们介绍了解决此类问题的方法，包括接近单位权重矩阵、长延迟、泄漏单元和回音状态网络。接着，我们介绍了LSTM门和单元、LSTM的历史和变体以及门控循环单元（GRU）。最后，我们介绍双向RNN、双向LSTM和来自语言模型（ELMo）网络，以在两个方向上处理序列。

    This is a tutorial paper on Recurrent Neural Network (RNN), Long Short-Term Memory Network (LSTM), and their variants. We start with a dynamical system and backpropagation through time for RNN. Then, we discuss the problems of gradient vanishing and explosion in long-term dependencies. We explain close-to-identity weight matrix, long delays, leaky units, and echo state networks for solving this problem. Then, we introduce LSTM gates and cells, history and variants of LSTM, and Gated Recurrent Units (GRU). Finally, we introduce bidirectional RNN, bidirectional LSTM, and the Embeddings from Language Model (ELMo) network, for processing a sequence in both directions.
    
[^29]: 模型突变下的强化学习

    Reinforcement Learning with an Abrupt Model Change. (arXiv:2304.11460v1 [eess.SY])

    [http://arxiv.org/abs/2304.11460](http://arxiv.org/abs/2304.11460)

    本文提出了一种模型突变下的强化学习算法，能够让智能体获得最优的长期折扣回报。该算法利用了回报-检测平衡，并使用最快的变化检测算法来检测模型变化。

    

    本文探讨了环境或者模型发生变化时的强化学习问题。本文提出了一种算法，能够让智能体应对这类问题，获得最优的长期折扣回报。该算法是无模型的，通过与环境交互来学习最优策略。并且已经证明了该算法具有强优化性能，在仿真实验中也展示了该算法的有效性。该算法利用了问题中一个基本的回报-检测平衡，并使用了最快的变化检测算法来检测模型变化。本文还提供了更快的检测模型变化和智能初始化策略的建议。

    The problem of reinforcement learning is considered where the environment or the model undergoes a change. An algorithm is proposed that an agent can apply in such a problem to achieve the optimal long-time discounted reward. The algorithm is model-free and learns the optimal policy by interacting with the environment. It is shown that the proposed algorithm has strong optimality properties. The effectiveness of the algorithm is also demonstrated using simulation results. The proposed algorithm exploits a fundamental reward-detection trade-off present in these problems and uses a quickest change detection algorithm to detect the model change. Recommendations are provided for faster detection of model changes and for smart initialization strategies.
    
[^30]: 构建元学习器进行无监督异常检测

    Constructing a meta-learner for unsupervised anomaly detection. (arXiv:2304.11438v1 [cs.LG])

    [http://arxiv.org/abs/2304.11438](http://arxiv.org/abs/2304.11438)

    构建了一种新的元学习方法来选择适当的无监督AD算法。元学习器的表现优于当前的最优解决方案。

    

    无监督异常检测（AD）对于从网络安全到医疗工具等一系列实际应用至关重要。由于问题的多样性，没有单一算法被发现在所有AD任务中都优越。在有监督分类问题中，通过使用元学习和AutoML，选择算法（也称为算法选择问题（ASP））得到了广泛的研究，但对无监督AD任务却少有关注。本研究提出了一种新的元学习方法，通过给定从未标记的输入数据集生成的元特征，识别出适当的无监督AD算法。所提出的元学习器的表现优于当前的最优解决方案。此外，还进行了混合模型统计分析，以检验元学习器的组成部分：元模型、元特征和基础AD算法集合对整体性能的影响。

    Unsupervised anomaly detection (AD) is critical for a wide range of practical applications, from network security to health and medical tools. Due to the diversity of problems, no single algorithm has been found to be superior for all AD tasks. Choosing an algorithm, otherwise known as the Algorithm Selection Problem (ASP), has been extensively examined in supervised classification problems, through the use of meta-learning and AutoML, however, it has received little attention in unsupervised AD tasks. This research proposes a new meta-learning approach that identifies an appropriate unsupervised AD algorithm given a set of meta-features generated from the unlabelled input dataset. The performance of the proposed meta-learner is superior to the current state of the art solution. In addition, a mixed model statistical analysis has been conducted to examine the impact of the meta-learner components: the meta-model, meta-features, and the base set of AD algorithms, on the overall performa
    
[^31]: 通过Paired-Logits反演攻击恢复图像的FedMD

    Breaching FedMD: Image Recovery via Paired-Logits Inversion Attack. (arXiv:2304.11436v1 [cs.CR])

    [http://arxiv.org/abs/2304.11436](http://arxiv.org/abs/2304.11436)

    本文揭示了即便使用FedMD的安全机制，仍存在被精心设计的恶意攻击利用的风险，如Paired-Logits反演攻击，会导致隐私数据曝光。

    

    联邦学习与模型蒸馏（FedMD）是一种新兴的协作学习范式，其中仅传输公共数据集的输出logits作为蒸馏知识，而不是传递易受梯度反演攻击的私有模型参数，这是联邦学习中已知的隐私风险。本文发现，即使共享公共数据集的输出 logit比直接共享梯度更安全，仍存在因精心设计的恶意攻击导致的数据曝光风险。我们的研究表明，恶意服务器可以训练一个反演神经网络来利用服务器和客户端模型之间的置信度差，针对FedMD及其变种进行PLI（配对logits反演）攻击。在多个人脸识别数据集上进行的实验证明，在类似于FedMD的方案中，仅使用公共数据集的配对服务器-客户端logits，恶意服务器能够重构私有图像。

    Federated Learning with Model Distillation (FedMD) is a nascent collaborative learning paradigm, where only output logits of public datasets are transmitted as distilled knowledge, instead of passing on private model parameters that are susceptible to gradient inversion attacks, a known privacy risk in federated learning. In this paper, we found that even though sharing output logits of public datasets is safer than directly sharing gradients, there still exists a substantial risk of data exposure caused by carefully designed malicious attacks. Our study shows that a malicious server can inject a PLI (Paired-Logits Inversion) attack against FedMD and its variants by training an inversion neural network that exploits the confidence gap between the server and client models. Experiments on multiple facial recognition datasets validate that under FedMD-like schemes, by using paired server-client logits of public datasets only, the malicious server is able to reconstruct private images on a
    
[^32]: 低秩张量空间中的超拉普拉斯正则化概念因式分解多视角聚类方法

    Hyper-Laplacian Regularized Concept Factorization in Low-rank Tensor Space for Multi-view Clustering. (arXiv:2304.11435v1 [cs.LG])

    [http://arxiv.org/abs/2304.11435](http://arxiv.org/abs/2304.11435)

    本文提出了一种采用超拉普拉斯正则化概念因式分解的多视角聚类方法，在低秩张量空间中对每个视角进行聚类并提取非线性局部结构。

    

    基于张量的多视角子空间聚类方法已经取得了重要进展，能够评估高阶相关性并提高多视角数据的聚类分析。然而，现有研究大多数存在两个缺陷。首先，基于自表示的张量子空间学习通常引起高时间和空间复杂度，并且局限于感知嵌入空间中的非线性局部结构。其次，张量奇异值分解（t-SVD）模型将每个奇异值等分地重新分布，而无法考虑它们之间的不同重要性。为了解决这些问题，我们提出了一种低秩张量空间中的超拉普拉斯正则化概念因式分解（HLRCF）多视角聚类方法。具体地，我们将概念因式分解应用于探索每个视角的潜在聚类表示。进一步地，超图拉普拉斯正则化赋予了该模型提取非线性局部结构的能力。

    Tensor-oriented multi-view subspace clustering has achieved significant strides in assessing high-order correlations and improving clustering analysis of multi-view data. Nevertheless, most of existing investigations are typically hampered by the two flaws. First, self-representation based tensor subspace learning usually induces high time and space complexity, and is limited in perceiving nonlinear local structure in the embedding space. Second, the tensor singular value decomposition (t-SVD) model redistributes each singular value equally without considering the diverse importance among them. To well cope with the issues, we propose a hyper-Laplacian regularized concept factorization (HLRCF) in low-rank tensor space for multi-view clustering. Specifically, we adopt the concept factorization to explore the latent cluster-wise representation of each view. Further, the hypergraph Laplacian regularization endows the model with the capability of extracting the nonlinear local structures i
    
[^33]: L3Cube-IndicSBERT: 使用多语言BERT学习跨语言句子表示的简单方法

    L3Cube-IndicSBERT: A simple approach for learning cross-lingual sentence representations using multilingual BERT. (arXiv:2304.11434v1 [cs.CL])

    [http://arxiv.org/abs/2304.11434](http://arxiv.org/abs/2304.11434)

    该论文提出了一种简单但有效的方法，使用合成语料库将BERT模型转换成SBERT模型。该方法在10种主要的印欧语言中具有很好的效果，并展示了其在非印欧语言上的应用性。

    

    多语言句子BERT (SBERT) 模型将不同语言映射到共同的表示空间，对于跨语言相似性和挖掘任务非常有用。我们提出了一种简单而有效的方法，使用合成语料库将普通的多语言BERT模型转换成多语言句子BERT模型。我们简单地聚合低资源目标语言的翻译 NLI 或 STS 数据集，并对普通的多语言BERT模型进行类似SBERT的微调。我们表明，多语言BERT模型具有内在的跨语言学习能力，这种简单的微调方法没有显式的跨语言训练，却产生了非常出色的跨语言表示效果。我们展示了我们的方法在10种主要的印欧语言中的有效性，并展示了我们的方法适用于非印欧语言德语和法语。利用这种方法，我们进一步提出了L3Cube-IndicSBERT，这是第一个专门针对印度语言印地语和马来语的多语言句子表示模型。

    The multilingual Sentence-BERT (SBERT) models map different languages to common representation space and are useful for cross-language similarity and mining tasks. We propose a simple yet effective approach to convert vanilla multilingual BERT models into multilingual sentence BERT models using synthetic corpus. We simply aggregate translated NLI or STS datasets of the low-resource target languages together and perform SBERT-like fine-tuning of the vanilla multilingual BERT model. We show that multilingual BERT models are inherent cross-lingual learners and this simple baseline fine-tuning approach without explicit cross-lingual training yields exceptional cross-lingual properties. We show the efficacy of our approach on 10 major Indic languages and also show the applicability of our approach to non-Indic languages German and French. Using this approach, we further present L3Cube-IndicSBERT, the first multilingual sentence representation model specifically for Indian languages Hindi, M
    
[^34]: 条件去噪扩散用于顺序推荐

    Conditional Denoising Diffusion for Sequential Recommendation. (arXiv:2304.11433v1 [cs.LG])

    [http://arxiv.org/abs/2304.11433](http://arxiv.org/abs/2304.11433)

    提出了一种条件去噪扩散模型，通过条件自回归的方式将优化和生成过程分解为更容易和可处理的步骤，并引入了一种新的优化模式，结合交叉熵损失和对抗性损失稳定训练过程。在多个数据集上的实验表明，该模型在顺序推荐方面具有较优的性能。

    

    由于能够学习内在的数据分布并处理不确定性，生成模型受到了广泛的关注。然而，两种主要的生成模型——生成对抗网络（GANs）和变分自编码器（VAEs）在顺序推荐任务中的表现存在挑战，GANs存在不稳定的优化，而VAEs则容易发生后验崩塌和过度平滑的生成。顺序推荐的稀疏和嘈杂的特性进一步加剧了这些问题。为了解决这些限制，我们提出了一个条件去噪扩散模型，包括序列编码器，交叉注意去噪解码器和逐步扩散器。这种方法以条件自回归的方式将优化和生成过程分解为更容易和可处理的步骤。此外，我们引入了一种新的优化模式，结合交叉熵损失和对抗性损失稳定训练过程。在多个数据集上的大量实验表明，我们的模型在顺序推荐方面优于几种最先进的方法，无论是在定量指标上还是在定性指标上。

    Generative models have attracted significant interest due to their ability to handle uncertainty by learning the inherent data distributions. However, two prominent generative models, namely Generative Adversarial Networks (GANs) and Variational AutoEncoders (VAEs), exhibit challenges that impede achieving optimal performance in sequential recommendation tasks. Specifically, GANs suffer from unstable optimization, while VAEs are prone to posterior collapse and over-smoothed generations. The sparse and noisy nature of sequential recommendation further exacerbates these issues. In response to these limitations, we present a conditional denoising diffusion model, which includes a sequence encoder, a cross-attentive denoising decoder, and a step-wise diffuser. This approach streamlines the optimization and generation process by dividing it into easier and tractable steps in a conditional autoregressive manner. Furthermore, we introduce a novel optimization schema that incorporates both cro
    
[^35]: Pipeline MoE:一种具有管道并行性的灵活MoE 实现

    Pipeline MoE: A Flexible MoE Implementation with Pipeline Parallelism. (arXiv:2304.11414v1 [cs.DC])

    [http://arxiv.org/abs/2304.11414](http://arxiv.org/abs/2304.11414)

    Pipeline MoE是一种利用管道并行性架构解决了MoE模型通信开销和限制性扩展问题的灵活实现， 在标准基准测试中比现有最新模型具有更好的可扩展性和高达17%的训练效率提高。

    

    随着深度学习模型的规模化，混合专家模型（MoE）因其训练和推断的亚线性计算复杂度而成为大型语言模型的重要选择。然而，现有的MoE模型存在两个关键缺点：1）全部调度和收集引入了巨大的内部节点和节点间通信开销，2）数据并行和专家并行维度受限，无法在专家维度上进行扩展。本文从并行框架的角度系统分析了这些缺点，并提出了一种称为Pipeline MoE（PPMoE）的新型MoE架构来解决它们。PPMoE以张量并行为基础构建专家并行，并用简单的张量索引切片和内部节点全局汇聚代替了通信密集的全部调度与收集。此外，PPMoE还可以方便地集成管道并行以进一步扩展骨干。在标准基准测试上的实验结果显示，PPMoE可以实现高达17％的训练效率提高，并在专家数量和批量大小方面显示出比现有最新模型更好的可扩展性。

    The Mixture of Experts (MoE) model becomes an important choice of large language models nowadays because of its scalability with sublinear computational complexity for training and inference. However, existing MoE models suffer from two critical drawbacks, 1) tremendous inner-node and inter-node communication overhead introduced by all-to-all dispatching and gathering, and 2) limited scalability for the backbone because of the bound data parallel and expert parallel to scale in the expert dimension. In this paper, we systematically analyze these drawbacks in terms of training efficiency in the parallel framework view and propose a novel MoE architecture called Pipeline MoE (PPMoE) to tackle them. PPMoE builds expert parallel incorporating with tensor parallel and replaces communication-intensive all-to-all dispatching and gathering with a simple tensor index slicing and inner-node all-reduce. Besides, it is convenient for PPMoE to integrate pipeline parallel to further scale the backbo
    
[^36]: 朝向碳中和边缘计算：利用现货和未来碳市场绿色边缘AI

    Towards Carbon-Neutral Edge Computing: Greening Edge AI by Harnessing Spot and Future Carbon Markets. (arXiv:2304.11374v1 [cs.LG])

    [http://arxiv.org/abs/2304.11374](http://arxiv.org/abs/2304.11374)

    本研究利用两时间尺度Lyapunov最优控制方法设计一个在线算法，成功实现了碳中和边缘计算，降低碳排放，而不会影响服务质量。

    

    为了实现碳意识机器学习任务卸载和限定碳排放量从而实现绿色边缘AI，我们建立了一个联合机器学习任务卸载和碳排放权购买问题。然而，由于资源价格、碳排放权价格、各个站点的碳排放强度和机器学习任务到达时间的不确定性，很难在线决定长期优化政策。为了克服这一困难，我们利用两时间尺度Lyapunov最优控制方法设计一个在线算法，该算法根据动态系统的实时反馈，在反应性方式下调整机器学习任务卸载和碳排放权购买的决策。

    Provisioning dynamic machine learning (ML) inference as a service for artificial intelligence (AI) applications of edge devices faces many challenges, including the trade-off among accuracy loss, carbon emission, and unknown future costs. Besides, many governments are launching carbon emission rights (CER) for operators to reduce carbon emissions further to reverse climate change. Facing these challenges, to achieve carbon-aware ML task offloading under limited carbon emission rights thus to achieve green edge AI, we establish a joint ML task offloading and CER purchasing problem, intending to minimize the accuracy loss under the long-term time-averaged cost budget of purchasing the required CER. However, considering the uncertainty of the resource prices, the CER purchasing prices, the carbon intensity of sites, and ML tasks' arrivals, it is hard to decide the optimal policy online over a long-running period time. To overcome this difficulty, we leverage the two-timescale Lyapunov opt
    
[^37]: 通过联合生成式和判别式训练学习符号表示

    Learning Symbolic Representations Through Joint GEnerative and DIscriminative Training. (arXiv:2304.11357v1 [cs.LG])

    [http://arxiv.org/abs/2304.11357](http://arxiv.org/abs/2304.11357)

    GEDI是一种将自监督学习和基于似然生成模型结合的贝叶斯框架。它与现有的神经符号框架联合训练，无需额外监督或预训练步骤，能够产生更好的符号表示。通过实验，证明GEDI可以在聚类性能上显著超越现有的自监督学习策略，在小数据范围内的性能也得到提高。

    

    我们介绍了GEDI，它是一种贝叶斯框架，将现有的自监督学习目标与基于似然的生成模型相结合。该框架利用生成式和判别式方法的优势，比独立解决方案产生了更好的符号表示。此外，GEDI可以轻松集成并与现有的神经符号框架联合训练，无需额外的监督或昂贵的预训练步骤。我们通过对包括SVHN、CIFAR10和CIFAR100在内的实际数据进行实验，证明了GEDI在聚类性能方面大大优于现有的自监督学习策略。符号组件进一步允许它利用逻辑约束形式的知识，提高小数据范围内的性能。

    We introduce GEDI, a Bayesian framework that combines existing self-supervised learning objectives with likelihood-based generative models. This framework leverages the benefits of both GEnerative and DIscriminative approaches, resulting in improved symbolic representations over standalone solutions. Additionally, GEDI can be easily integrated and trained jointly with existing neuro-symbolic frameworks without the need for additional supervision or costly pre-training steps. We demonstrate through experiments on real-world data, including SVHN, CIFAR10, and CIFAR100, that GEDI outperforms existing self-supervised learning strategies in terms of clustering performance by a significant margin. The symbolic component further allows it to leverage knowledge in the form of logical constraints to improve performance in the small data regime.
    
[^38]: 利用SAM的输入增强技术: 以分割基础模型为基础提升医学图像分割

    Input Augmentation with SAM: Boosting Medical Image Segmentation with Segmentation Foundation Model. (arXiv:2304.11332v1 [cs.CV])

    [http://arxiv.org/abs/2304.11332](http://arxiv.org/abs/2304.11332)

    本文介绍如何使用大型的通用分割模型SAM来提升医学图像分割，展示了如何通过使用SAM生成的掩模、特征和稳定性分数来构建和训练更好的医学图像分割模型，并在两个数据集上进行了验证。

    

    Segment Anything Model (SAM)是一个最近发展的通用分割模型，用于计算机视觉任务. SAM使用了超过1亿个掩模的1100万图像进行训练，可以为自然场景图像中的广泛对象生成分割结果。本研究展示了如何利用这样一个大型基础模型来进行医学图像分割，尽管SAM并没有立即为医学图像提供高质量的分割，但其生成的掩模、特征和稳定性分数对于构建和训练更好的医学图像分割模型非常有用。特别地，我们演示了如何使用SAM来增强经典的医学图像分割模型（如U-Net）的图像输入。对两个数据集的实验表明了我们所提出的方法的有效性。

    The Segment Anything Model (SAM) is a recently developed large model for general-purpose segmentation for computer vision tasks. SAM was trained using 11 million images with over 1 billion masks and can produce segmentation results for a wide range of objects in natural scene images. SAM can be viewed as a general perception model for segmentation (partitioning images into semantically meaningful regions). Thus, how to utilize such a large foundation model for medical image segmentation is an emerging research target. This paper shows that although SAM does not immediately give high-quality segmentation for medical images, its generated masks, features, and stability scores are useful for building and training better medical image segmentation models. In particular, we demonstrate how to use SAM to augment image inputs for a commonly-used medical image segmentation model (e.g., U-Net). Experiments on two datasets show the effectiveness of our proposed method.
    
[^39]: 通过改进的积分逼近加速扩散采样过程

    On Accelerating Diffusion-Based Sampling Process via Improved Integration Approximation. (arXiv:2304.11328v1 [cs.LG])

    [http://arxiv.org/abs/2304.11328](http://arxiv.org/abs/2304.11328)

    本文提出了一种通过优化系数来加速流行的基于反向ODE解算的扩散采样过程的方法，优化方法为改进的积分逼近（IIA），在每个反向时间步长，我们建议针对某些选择的系数最小化MSE函数，给定预训练的扩散模型，只需要在一批样本上进行特定数量的神经功能评估（NFEs）一次IIA过程即可获得最佳解。

    

    一种流行的基于扩散的采样策略尝试有效地解决反向常微分方程（ODEs）。所得ODE求解器的系数由ODE公式，反向离散的时间步长和使用的ODE方法预先确定。本文考虑通过优化某些系数来加速几种流行的基于ODE的采样过程，优化方法为改进的积分逼近（IIA）。在每个反向时间步长，我们建议针对某些选择的系数最小化均方误差（MSE）函数。通过应用一组细粒度时间步长的原始ODE求解器构造MSE，从原理上提供了更精确的积分逼近，以预测下一个扩散隐藏状态，给定预训练的扩散模型，只需要在一批样本上进行特定数量的神经功能评估（NFEs）一次IIA过程即可获得最佳解

    One popular diffusion-based sampling strategy attempts to solve the reverse ordinary differential equations (ODEs) effectively. The coefficients of the obtained ODE solvers are pre-determined by the ODE formulation, the reverse discrete timesteps, and the employed ODE methods. In this paper, we consider accelerating several popular ODE-based sampling processes by optimizing certain coefficients via improved integration approximation (IIA). At each reverse timestep, we propose to minimize a mean squared error (MSE) function with respect to certain selected coefficients. The MSE is constructed by applying the original ODE solver for a set of fine-grained timesteps which in principle provides a more accurate integration approximation in predicting the next diffusion hidden state. Given a pre-trained diffusion model, the procedure for IIA for a particular number of neural functional evaluations (NFEs) only needs to be conducted once over a batch of samples. The obtained optimal solutions f
    
[^40]: 探索外部分布广义化中的特征学习

    Towards Understanding Feature Learning in Out-of-Distribution Generalization. (arXiv:2304.11327v1 [cs.LG])

    [http://arxiv.org/abs/2304.11327](http://arxiv.org/abs/2304.11327)

    研究发现，ERM本质上同时学习了具有误导性的特征和不变特征，在ERM预训练期间学习到的特征质量影响了最终的OOD性能，未能捕获所有潜在的有用特征将限制最终的OOD性能。

    

    对于外部分布（OOD）广义化的失败，常见的解释是使用经验风险最小化（ERM）模型学习到具有误导性的特征而不是期望的不变特征。然而，最近的几项研究挑战了这种解释，发现深度网络可能已经学到了足够好的特征进行OOD广义化。这场辩论扩展到了许多OOD广义化任务的训练或微调神经网络的内部组织和OOD性能相关性中。为了理解这些似乎相互矛盾的现象，我们进行了理论研究，发现ERM本质上同时学习了具有误导性的特征和不变特征。另一方面，在ERM预训练期间学习到的特征质量显著影响了最终的OOD性能，因为OOD对象很少学习到新功能。未能在预训练期间捕获所有潜在的有用特征将进一步限制最终的OOD性能。

    A common explanation for the failure of out-of-distribution (OOD) generalization is that the model trained with empirical risk minimization (ERM) learns spurious features instead of the desired invariant features. However, several recent studies challenged this explanation and found that deep networks may have already learned sufficiently good features for OOD generalization. The debate extends to the in-distribution and OOD performance correlations along with training or fine-tuning neural nets across a variety of OOD generalization tasks. To understand these seemingly contradicting phenomena, we conduct a theoretical investigation and find that ERM essentially learns both spurious features and invariant features. On the other hand, the quality of learned features during ERM pre-training significantly affects the final OOD performance, as OOD objectives rarely learn new features. Failing to capture all the underlying useful features during pre-training will further limit the final OOD
    
[^41]: 神经网络支持的模型预测控制在不匹配不确定性缓解方面的应用

    Unmatched uncertainty mitigation through neural network supported model predictive control. (arXiv:2304.11315v1 [cs.LG])

    [http://arxiv.org/abs/2304.11315](http://arxiv.org/abs/2304.11315)

    本文利用基于深度学习的模型预测控制算法，采用双时间尺度适应机制，结合深度神经网络作为预言机，实时估计不匹配的不确定性，成功地实现了对具有未知结构的不匹配和有界状态-动作相关不确定性的系统的控制。

    

    本文提出了一种基于深度学习的模型预测控制（MPC）算法，用于具有未知结构的不匹配和有界状态-动作相关不确定性的系统。我们利用深度神经网络（DNN）作为学习基础MPC（LBMPC）中的预言机，以估计不匹配的不确定性。由于实时估计其系数的技术困难，通常认为LBMPC难以使用非参数预言机，例如DNN。我们采用双时间尺度适应机制，在内部层以缓慢的时间尺度使用在线收集并有选择地存储在缓冲区中的训练数据进行训练，同时实时更新神经网络的最后一层的权重。我们通过对喷气发动机压缩系统模型的数值实验进行验证。这些结果表明，所提出的方法是可以实时实现的，并具有理论意义。

    This paper presents a deep learning based model predictive control (MPC) algorithm for systems with unmatched and bounded state-action dependent uncertainties of unknown structure. We utilize a deep neural network (DNN) as an oracle in the underlying optimization problem of learning based MPC (LBMPC) to estimate unmatched uncertainties. Generally, non-parametric oracles such as DNN are considered difficult to employ with LBMPC due to the technical difficulties associated with estimation of their coefficients in real time. We employ a dual-timescale adaptation mechanism, where the weights of the last layer of the neural network are updated in real time while the inner layers are trained on a slower timescale using the training data collected online and selectively stored in a buffer. Our results are validated through a numerical experiment on the compression system model of jet engine. These results indicate that the proposed approach is implementable in real time and carries the theore
    
[^42]: 基于前瞻扩散概率模型的均值估计方法优化

    Lookahead Diffusion Probabilistic Models for Refining Mean Estimation. (arXiv:2304.11312v1 [cs.AI])

    [http://arxiv.org/abs/2304.11312](http://arxiv.org/abs/2304.11312)

    本文提出基于前瞻扩散概率模型的技术来优化条件高斯分布的均值估计，通过对两个估计进行外推来计算更准确的估计值。该方法不需要对DNN模型进行微调，并在基准数据集上获得了比最新方法更好的实验结果。

    

    本论文提出了基于前瞻扩散概率模型（LA-DPMs）的技术，该技术利用了深度神经网络（DNNs）在扩散概率模型（DPMs）中，在连续时间步骤之后输出之间的相关性，以优化条件高斯分布的均值估计。我们提出了通过对两个$\boldsymbol{x}$估计进行外推来计算更准确的$\boldsymbol{x}$估计的方法。这可以通过在现有DPMs的后向过程中引入额外的连接来轻松将其集成到后向过程中，并且不需要对DNN模型进行微调。在几个基准数据集上的实验结果表明，与现有的最新方法相比，所提出的LA-DPMs方法是有效的。

    We propose lookahead diffusion probabilistic models (LA-DPMs) to exploit the correlation in the outputs of the deep neural networks (DNNs) over subsequent timesteps in diffusion probabilistic models (DPMs) to refine the mean estimation of the conditional Gaussian distributions in the backward process. A typical DPM first obtains an estimate of the original data sample $\boldsymbol{x}$ by feeding the most recent state $\boldsymbol{z}_i$ and index $i$ into the DNN model and then computes the mean vector of the conditional Gaussian distribution for $\boldsymbol{z}_{i-1}$. We propose to calculate a more accurate estimate for $\boldsymbol{x}$ by performing extrapolation on the two estimates of $\boldsymbol{x}$ that are obtained by feeding $(\boldsymbol{z}_{i+1},i+1)$ and $(\boldsymbol{z}_{i},i)$ into the DNN model. The extrapolation can be easily integrated into the backward process of existing DPMs by introducing an additional connection over two consecutive timesteps, and fine-tuning is n
    
[^43]: 关于应用评论中能源相关问题的识别

    On the Identification of the Energy related Issues from the App Reviews. (arXiv:2304.11292v1 [cs.AI])

    [http://arxiv.org/abs/2304.11292](http://arxiv.org/abs/2304.11292)

    本文研究了自动提取与能源相关的应用程序评论的不同技术。结果表明，神经网络优于其他机器学习模型。

    

    应用程序的能源效率问题可能会对应用程序用户造成重大问题，并在应用商店广泛讨论。之前的研究表明，研究与能源相关的应用程序评论以确定能源相关用户反馈的主要原因或类别的重要性。然而，还没有研究有效地自动提取与能源相关的应用程序评论。在本文中，我们经验性地研究了不同的技术，以自动提取与能源相关的用户反馈。我们比较了许多机器学习模型的准确性、F1分数和运行时间，并与相关特征组合和相对较新的基于神经网络的模型进行比较。总共比较了60个机器学习模型，以及使用六种神经网络架构和三种单词嵌入模型构建的30个模型。我们开发了一个可视化工具，通过该工具，开发人员可以遍历这个大规模的结果集。结果表明，神经网络优于其他机器学习模型。

    The energy inefficiency of the apps can be a major issue for the app users which is discussed on App Stores extensively. Previous research has shown the importance of investigating the energy related app reviews to identify the major causes or categories of energy related user feedback. However, there is no study that efficiently extracts the energy related app reviews automatically. In this paper, we empirically study different techniques for automatic extraction of the energy related user feedback. We compare the accuracy, F1-score and run time of numerous machine-learning models with relevant feature combinations and relatively modern Neural Network-based models. In total, 60 machine learning models are compared to 30 models that we build using six neural network architectures and three word embedding models. We develop a visualization tool for this study through which a developer can traverse through this large-scale result set. The results show that neural networks outperform the 
    
[^44]: 机器学习模型适当的知识产权保护机制的辨别：针对水印、指纹、模型访问和攻击的系统化分析

    Identifying Appropriate Intellectual Property Protection Mechanisms for Machine Learning Models: A Systematization of Watermarking, Fingerprinting, Model Access, and Attacks. (arXiv:2304.11285v1 [cs.LG])

    [http://arxiv.org/abs/2304.11285](http://arxiv.org/abs/2304.11285)

    本文系统化分析了机器学习模型在知识产权保护方面的挑战，提出了针对水印、指纹、模型访问和攻击的保护技术，并构建了综合的威胁模型。

    

    机器学习（ML）的商业应用越来越普及；同时，ML模型变得越来越复杂，而且训练成本越来越高，这使得训练模型的知识产权保护（IPP）成为一个紧迫的问题。与其他领域可以建立在对其IP进行保护的威胁、攻击和防御措施有着深刻理解的基础上不同，机器学习领域的相关研究仍然非常零散。这也是由于缺少统一的视角以及这些方面的共同分类学。在本文中，我们系统化了我们在ML中关于IPP的发现，同时专注于编写时已确认的威胁、攻击和防御措施。我们为ML中的IP建立了综合的威胁模型，将攻击和防御措施分类到统一和整合的分类法中，从而搭起了来自ML和安全社区的研究之间的桥梁。

    The commercial use of Machine Learning (ML) is spreading; at the same time, ML models are becoming more complex and more expensive to train, which makes Intellectual Property Protection (IPP) of trained models a pressing issue. Unlike other domains that can build on a solid understanding of the threats, attacks and defenses available to protect their IP, the ML-related research in this regard is still very fragmented. This is also due to a missing unified view as well as a common taxonomy of these aspects.  In this paper, we systematize our findings on IPP in ML, while focusing on threats and attacks identified and defenses proposed at the time of writing. We develop a comprehensive threat model for IP in ML, categorizing attacks and defenses within a unified and consolidated taxonomy, thus bridging research from both the ML and security communities.
    
[^45]: PyTorch FSDP：全面分片数据并行规模化的经验

    PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel. (arXiv:2304.11277v1 [cs.DC])

    [http://arxiv.org/abs/2304.11277](http://arxiv.org/abs/2304.11277)

    本论文介绍了基于PyTorch的Fully Sharded Data Parallel（FSDP）解决方案，该方案可扩展大型模型训练，并优化各种硬件配置的资源利用率。

    

    众所周知，大型模型在广泛领域内具有优异的性能潜力。尽管机器学习系统研究领域取得了显著进展，使得开发和探索大型模型成为可能，但这些能力仍受限于少数高级用户和行业领袖，导致技术上的隐含壁垒阻碍广泛社区访问和利用这些技术。本文介绍了PyTorch Fully Sharded Data Parallel（FSDP）作为大型模型训练的产业级解决方案。FSDP已与几个关键PyTorch核心组件（包括张量实现、分发器系统和CUDA内存缓存分配器）密切协作，以提供非侵入式用户体验和高训练效率。此外，FSDP本地集成了一系列技术和设置，优化了各种硬件配置的资源利用率。

    It is widely acknowledged that large models have the potential to deliver superior performance across a broad range of domains. Despite the remarkable progress made in the field of machine learning systems research, which has enabled the development and exploration of large models, such abilities remain confined to a small group of advanced users and industry leaders, resulting in an implicit technical barrier for the wider community to access and leverage these technologies. In this paper, we introduce PyTorch Fully Sharded Data Parallel (FSDP) as an industry-grade solution for large model training. FSDP has been closely co-designed with several key PyTorch core components including Tensor implementation, dispatcher system, and CUDA memory caching allocator, to provide non-intrusive user experiences and high training efficiency. Additionally, FSDP natively incorporates a range of techniques and settings to optimize resource utilization across a variety of hardware configurations. The 
    
[^46]: 速度即一切：通过GPU-aware优化在设备上加速大型扩散模型

    Speed Is All You Need: On-Device Acceleration of Large Diffusion Models via GPU-Aware Optimizations. (arXiv:2304.11267v1 [cs.CV])

    [http://arxiv.org/abs/2304.11267](http://arxiv.org/abs/2304.11267)

    该研究提出一系列通过GPU-aware优化大型扩散模型的方法，实现在配备GPU的移动设备上极快的推论延迟，扩大了生成性人工智能的适用范围并改善了用户体验。

    

    基础模型的快速发展和应用已经彻底改变了人工智能领域。大型扩散模型因其生成逼真图像和支持各种任务的能力而受到了重视。这些模型在设备上的部署带来了许多好处，比如降低服务器成本、离线功能和改善用户隐私。然而，在设备上共同的大型扩散模型具有超过10亿的参数，由于设备的受限计算和内存资源存在挑战。我们提出了一系列大型扩散模型实现优化，以在配备GPU的移动设备上实现迄今为止最快的推论延迟（对于一个512x512的图像，在三星S23 Ultra上的"稳定扩散1.4"下，进行20次迭代的情况下，无需int8量化，推论的延迟小于12秒）。这些优化扩大了生成性人工智能的适用范围并改善了各种设备上的整体用户体验。

    The rapid development and application of foundation models have revolutionized the field of artificial intelligence. Large diffusion models have gained significant attention for their ability to generate photorealistic images and support various tasks. On-device deployment of these models provides benefits such as lower server costs, offline functionality, and improved user privacy. However, common large diffusion models have over 1 billion parameters and pose challenges due to restricted computational and memory resources on devices. We present a series of implementation optimizations for large diffusion models that achieve the fastest reported inference latency to-date (under 12 seconds for Stable Diffusion 1.4 without int8 quantization on Samsung S23 Ultra for a 512x512 image with 20 iterations) on GPU-equipped mobile devices. These enhancements broaden the applicability of generative AI and improve the overall user experience across a wide range of devices.
    
[^47]: 手腕动作时间序列分类用于帕金森病检测

    Time Series Classification for Detecting Parkinson's Disease from Wrist Motions. (arXiv:2304.11265v1 [cs.LG])

    [http://arxiv.org/abs/2304.11265](http://arxiv.org/abs/2304.11265)

    该研究使用InceptionTime和ROCKET方法进行时间序列分类，以监测帕金森病患者的手腕运动。研究发现，所有方法都适用于估计震颤严重程度和肌肉强直的存在，但在检测运动障碍方面存在困难。具有岭分类器的InceptionTime方法展示了最先进的分类性能，显示时间序列分类在基于可穿戴设备的PD症状监测中具有潜力。

    

    帕金森病是一种神经退行性疾病，具有频繁变化的运动症状，持续的症状监测可以实现更有针对性的治疗。传统的时间序列分类和深度学习技术在使用可穿戴加速度计数据进行PD症状监测时性能有限，因为PD运动模式具有复杂性，但数据集很小。我们研究了InceptionTime和RandOm卷积核变换（ROCKET），因为它们是TSC的最新技术，并且对于PD症状监测非常有前景：InceptionTime的高学习能力适用于建模复杂运动模式，而ROCKET适用于小数据集。我们使用随机搜索找到了最高得分的InceptionTime结构，并将其与具有岭分类器和多层感知器（MLP）的ROCKET进行了比较，用于PD患者的手腕运动。我们发现，所有方法都适用于估计震颤严重程度和肌肉强直的存在，但在检测运动障碍方面存在困难。具有岭分类器的InceptionTime优于其他方法，并实现了最先进的分类性能，展示了TSC在基于可穿戴设备的PD症状监测中的潜力。

    Parkinson's disease (PD) is a neurodegenerative disease with frequently changing motor symptoms where continuous symptom monitoring enables more targeted treatment. Classical time series classification (TSC) and deep learning techniques have limited performance for PD symptom monitoring using wearable accelerometer data because PD movement patterns are complex, but datasets are small. We investigate InceptionTime and RandOm Convolutional KErnel Transform (ROCKET) because they are state-of-the-art for TSC and promising for PD symptom monitoring: InceptionTime's high learning capacity is suited to modeling complex movement patterns while ROCKET is suited to small datasets. We used a random search to find the highest-scoring InceptionTime architecture and compared it to ROCKET with a ridge classifier and a multi-layer perceptron (MLP) on wrist motions of PD patients. We find that all approaches are suitable for estimating tremor severity and bradykinesia presence but struggle with detecti
    
[^48]: 自然分布漂移下低样本稳健性的基准测试

    Benchmarking Low-Shot Robustness to Natural Distribution Shifts. (arXiv:2304.11263v1 [cs.CV])

    [http://arxiv.org/abs/2304.11263](http://arxiv.org/abs/2304.11263)

    本文通过对不同少样本数据集、架构、预训练初始化和稳健性干预的自然分布漂移的稳健性进行了首次深入研究，发现没有单一的选择模型比其他模型更稳健，现有的干预措施也可能无法提高某些数据集的稳健性。

    

    近年来，结合更好的微调方法的预训练策略已经取得了针对自然分布漂移的鲁棒性的显著进展。然而，这样的微调假设可以访问大量标记数据，而当训练数据量不高时观察到的情况尚不清楚。我们通过对不同少样本数据集、架构、预训练初始化和最先进的稳健性干预的自然分布漂移的稳健性进行了首次深入研究，填补了这一空白。最重要的是，我们发现没有单一的选择模型比其他模型更稳健，即使在完整样本下，现有的干预措施也可能无法提高某些数据集的稳健性。我们希望我们的工作能够激励社区关注这个实际重要性的问题。

    Robustness to natural distribution shifts has seen remarkable progress thanks to recent pre-training strategies combined with better fine-tuning methods. However, such fine-tuning assumes access to large amounts of labelled data, and the extent to which the observations hold when the amount of training data is not as high remains unknown. We address this gap by performing the first in-depth study of robustness to various natural distribution shifts in different low-shot regimes: spanning datasets, architectures, pre-trained initializations, and state-of-the-art robustness interventions. Most importantly, we find that there is no single model of choice that is often more robust than others, and existing interventions can fail to improve robustness on some datasets even if they do so in the full-shot regime. We hope that our work will motivate the community to focus on this problem of practical importance.
    
[^49]: 机器学习与贝叶斯计算的未来

    Machine Learning and the Future of Bayesian Computation. (arXiv:2304.11251v1 [stat.ML])

    [http://arxiv.org/abs/2304.11251](http://arxiv.org/abs/2304.11251)

    本文讨论了利用机器学习的思想来改进贝叶斯计算的潜力，并探讨了几个具体的未来方向。

    

    贝叶斯模型是研究复杂数据的强大工具，允许分析人员编码丰富的层次依赖关系并利用先验信息。最重要的是，它们通过后验分布促进了对不确定性的完整表征。实用的后验计算通常通过MCMC进行，但对于具有许多观测值的高维模型而言，这可能计算上不可行。在本文中，我们讨论利用机器学习思想来改进后验计算的潜力。具体的未来方向在正态流、贝叶斯核心集、分布式贝叶斯推断和变分推断的vignettes中得到探讨。

    Bayesian models are a powerful tool for studying complex data, allowing the analyst to encode rich hierarchical dependencies and leverage prior information. Most importantly, they facilitate a complete characterization of uncertainty through the posterior distribution. Practical posterior computation is commonly performed via MCMC, which can be computationally infeasible for high dimensional models with many observations. In this article we discuss the potential to improve posterior computation using ideas from machine learning. Concrete future directions are explored in vignettes on normalizing flows, Bayesian coresets, distributed Bayesian inference, and variational inference.
    
[^50]: eWaSR——一种嵌入式计算准备好的海事障碍物检测网络

    eWaSR -- an embedded-compute-ready maritime obstacle detection network. (arXiv:2304.11249v1 [cs.CV])

    [http://arxiv.org/abs/2304.11249](http://arxiv.org/abs/2304.11249)

    本论文提出了一种嵌入式计算准备好的海事障碍物检测网络——eWaSR，能够在保证检测质量的前提下运行速度更快，且在F1得分方面也优于其他目前最先进的嵌入式就绪架构。

    

    海事障碍物检测对于自主水面船舶（ASV）的安全导航至关重要。虽然基于图像的检测方法的准确性已经大大提高，但它们的计算和内存要求阻止了在嵌入式设备上的部署。在本文中，我们对当前表现最佳的海事障碍物检测网络WaSR进行了分析。在分析基础上，我们提出替换最耗计算资源的阶段并提出其嵌入式计算准备好的变体eWaSR。特别地，新设计遵循了基于transformer的轻量级网络的最新进展。eWaSR在仅有0.52％的F1得分性能下降的情况下实现了与最先进的WaSR相当的检测结果，并在F1得分方面优于其他最先进的嵌入式就绪架构高达9.74％。在标准GPU上，eWaSR的运行速度比原始WaSR快10倍（115 FPS vs 11 FPS）。在OAK-D实际嵌入式设备上的测试表明，尽管WaSR由于内存限制而无法运行，但eWaSR仍然能够表现出色。

    Maritime obstacle detection is critical for safe navigation of autonomous surface vehicles (ASVs). While the accuracy of image-based detection methods has advanced substantially, their computational and memory requirements prohibit deployment on embedded devices. In this paper we analyze the currently best-performing maritime obstacle detection network WaSR. Based on the analysis we then propose replacements for the most computationally intensive stages and propose its embedded-compute-ready variant eWaSR. In particular, the new design follows the most recent advancements of transformer-based lightweight networks. eWaSR achieves comparable detection results to state-of-the-art WaSR with only 0.52% F1 score performance drop and outperforms other state-of-the-art embedded-ready architectures by over 9.74% in F1 score. On a standard GPU, eWaSR runs 10x faster than the original WaSR (115 FPS vs 11 FPS). Tests on a real embedded device OAK-D show that, while WaSR cannot run due to memory re
    
[^51]: 基于量子物理的神经网络用于在复杂形状中模拟计算流体力学

    Quantum physics-informed neural networks for simulating computational fluid dynamics in complex shapes. (arXiv:2304.11247v1 [cs.LG])

    [http://arxiv.org/abs/2304.11247](http://arxiv.org/abs/2304.11247)

    本文提出了一种基于量子物理的神经网络方法，用于模拟复杂几何形状中的流体流动。该方法不需要重新模拟，能够适用于不同的形状，并且相比于普通神经网络提高了21%的精度。

    

    解决流体的速度和压力分布（通过解决纳维尔-斯托克斯方程）是化学、能源、制药工业以及机械工程和管道系统设计中的一个主要任务。现有的求解器（如OpenFOAM和Ansys）在复杂几何形状中的流体动力学模拟是计算密集型的，需要重新模拟每当几何参数或初始和边界条件被改变。物理学信赖的神经网络（PINNs）是模拟复杂几何形状中流体流动的有前途的工具，因为它们可以适应几何形状和网格定义的变化，允许跨不同形状进行概括。我们提供了一种混合量子物理的神经网络，该网络模拟三维 Y 型混合器中的层流流体流动。我们的方法将量子模型的表达能力与 PINN 的灵活性相结合，精度比普通 PINN 提高了 21％。

    Finding the distribution of the velocities and pressures of a fluid (by solving the Navier-Stokes equations) is a principal task in the chemical, energy, and pharmaceutical industries, as well as in mechanical engineering and the design of pipeline systems. With existing solvers, such as OpenFOAM and Ansys, simulations of fluid dynamics in intricate geometries are computationally expensive and require re-simulation whenever the geometric parameters or the initial and boundary conditions are altered. Physics-informed neural networks (PINNs) are a promising tool for simulating fluid flows in complex geometries, as they can adapt to changes in the geometry and mesh definitions, allowing for generalization across different shapes. We present a hybrid quantum physics-informed neural network that simulates laminar fluid flows in 3D Y-shaped mixers. Our approach combines the expressive power of a quantum model with the flexibility of a PINN, resulting in a 21% higher accuracy compared to a pu
    
[^52]: AutoNeRF: 自主代理训练隐式场景表示

    AutoNeRF: Training Implicit Scene Representations with Autonomous Agents. (arXiv:2304.11241v1 [cs.CV])

    [http://arxiv.org/abs/2304.11241](http://arxiv.org/abs/2304.11241)

    本文提出了AutoNeRF方法，使用自主体代理收集训练NeRF所需数据，训练NeRF成功。

    

    隐式表示，如神经辐射场（NeRF），已被证明在新视角综合方面非常有效。然而，这些模型通常需要人工收集数据并进行细致的处理。在本文中，我们提出了AutoNeRF，这是一种使用自主体代理收集训练NeRF所需数据的方法。我们的方法允许代理有效地探索未知环境并使用经验自主地构建相应的隐式地图表示。我们比较了不同的探索策略，包括手工设计的基于前沿的探索和由经过训练的高级规划器和经典的低级路径追踪器组成的模块化方法的影响。我们使用针对这个问题量身定制的不同奖励函数来训练这些模型，并在四个不同的下游任务上评估学习表示的质量：经典视角渲染、地图重建、规划和姿态微调。实证结果表明，使用自主代理AutoNeRF可以成功地训练NeRF。

    Implicit representations such as Neural Radiance Fields (NeRF) have been shown to be very effective at novel view synthesis. However, these models typically require manual and careful human data collection for training. In this paper, we present AutoNeRF, a method to collect data required to train NeRFs using autonomous embodied agents. Our method allows an agent to explore an unseen environment efficiently and use the experience to build an implicit map representation autonomously. We compare the impact of different exploration strategies including handcrafted frontier-based exploration and modular approaches composed of trained high-level planners and classical low-level path followers. We train these models with different reward functions tailored to this problem and evaluate the quality of the learned representations on four different downstream tasks: classical viewpoint rendering, map reconstruction, planning, and pose refinement. Empirical results show that NeRFs can be trained 
    
[^53]: 适应多种采集条件的基于模型的深度学习：Ada-MoDL

    Adapting model-based deep learning to multiple acquisition conditions: Ada-MoDL. (arXiv:2304.11238v1 [eess.IV])

    [http://arxiv.org/abs/2304.11238](http://arxiv.org/abs/2304.11238)

    本研究提出了一种适应多种采集条件的基于模型的深度学习方法，可从欠采样平行MRI数据中提供高质量重建，通过使用适当的权重对CNN特征和正则化参数进行缩放，使模型适应每个设置。

    

    目的：本文旨在引入一种单一的基于模型的深度网络，可从使用多种序列、采集设置和磁场强度获取的欠采样平行MRI数据中提供高质量重建。方法：提出了一个单一的展开架构，可为多种采集设置提供良好的重建结果。该方案通过使用适当的权重对CNN特征和正则化参数进行缩放，从而使模型适应每个设置。通过使用条件向量从多层感知机模型派生缩放权重和正则化参数，该条件向量代表特定的采集设置。同时使用来自多种采集设置的数据进行多层感知机参数和CNN权重的联合训练。使用不同采集设置采集的数据集验证了条件网络。结果：适应能力好的多模态黄金标准评估显示，提出的方法相对于现有技术具有更好的重建质量和更高的稳健性。

    Purpose: The aim of this work is to introduce a single model-based deep network that can provide high-quality reconstructions from undersampled parallel MRI data acquired with multiple sequences, acquisition settings and field strengths.  Methods: A single unrolled architecture, which offers good reconstructions for multiple acquisition settings, is introduced. The proposed scheme adapts the model to each setting by scaling the CNN features and the regularization parameter with appropriate weights. The scaling weights and regularization parameter are derived using a multi-layer perceptron model from conditional vectors, which represents the specific acquisition setting. The perceptron parameters and the CNN weights are jointly trained using data from multiple acquisition settings, including differences in field strengths, acceleration, and contrasts. The conditional network is validated using datasets acquired with different acquisition settings.  Results: The comparison of the adaptiv
    
[^54]: 采用BinMask的高效神经网络$L_0$正则化

    Effective Neural Network $L_0$ Regularization With BinMask. (arXiv:2304.11237v1 [cs.LG])

    [http://arxiv.org/abs/2304.11237](http://arxiv.org/abs/2304.11237)

    本文提出了一种简单而高效的神经网络$L_0$正则化方法——BinMask，该方法采用确定性二进制掩码乘以权重并使用标识直通估计器进行反向传播。实验证明，与其他方法相比，Binmask不需要针对不同任务进行专门调整即可在特征选择、网络稀疏化和模型正则化等任务中实现有竞争力的表现。解耦权重与掩码优化是实现有效$L_0$正则化的关键组成部分。

    

    $L_0$正则化是神经网络的一个基本问题。除了为更好的泛化性而正则化模型外，$L_0$正则化还适用于选择输入特征和训练稀疏神经网络。相关领域中有大量的研究，其中一些方法相当复杂。本文显示，一种简单的形式化方法，即使用确定性二进制掩码将权重乘以并使用标识直通估计器进行反向传播的BinMask，是一种有效的$L_0$正则化器。我们在三个任务上评估了BinMask: 特征选择，网络稀疏化和模型正则化。尽管它很简单，但相比为每个任务设计的方法，BinMask在所有基准测试中都能够实现有竞争力的表现而无需特定的调整。我们的结果表明，解耦权重与掩码优化（这在以前的工作中被广泛采用）是实现有效$L_0$正则化的关键组成部分。

    $L_0$ regularization of neural networks is a fundamental problem. In addition to regularizing models for better generalizability, $L_0$ regularization also applies to selecting input features and training sparse neural networks. There is a large body of research on related topics, some with quite complicated methods. In this paper, we show that a straightforward formulation, BinMask, which multiplies weights with deterministic binary masks and uses the identity straight-through estimator for backpropagation, is an effective $L_0$ regularizer. We evaluate BinMask on three tasks: feature selection, network sparsification, and model regularization. Despite its simplicity, BinMask achieves competitive performance on all the benchmarks without task-specific tuning compared to methods designed for each task. Our results suggest that decoupling weights from mask optimization, which has been widely adopted by previous work, is a key component for effective $L_0$ regularization.
    
[^55]: 基于机器学习的混凝土概率选择及设计

    Probabilistic selection and design of concrete using machine learning. (arXiv:2304.11226v1 [cs.LG])

    [http://arxiv.org/abs/2304.11226](http://arxiv.org/abs/2304.11226)

    本文介绍了一种利用机器学习进行混凝土配比设计和性能预测的方法，该方法可以提高混凝土建筑的可持续性，同时满足强度、密度和成本等目标。

    

    在混凝土材料的固有差异和可能性组分的多样性下，开发具有更低环境影响的健壮混凝土配方是具有挑战性的。利用机器学习进行可靠的性能预测可以促进基于性能的混凝土规范的制定，减少材料的浪费并提高混凝土建筑的可持续性。本文开发了一种机器学习算法，可以利用中间目标变量及其相关的噪声来预测最终目标变量。我们将该方法应用于指定具有高抗碳化性的混凝土配合比，以及具有低环境影响的混凝土配合比。这两种混凝土配比也满足强度、密度和成本方面的目标。指定的混凝土配比已经得到实验验证。我们的通用方法使噪声在机器学习中具有广泛的应用，可用于结构和材料设计中。

    Development of robust concrete mixes with a lower environmental impact is challenging due to natural variability in constituent materials and a multitude of possible combinations of mix proportions. Making reliable property predictions with machine learning can facilitate performance-based specification of concrete, reducing material inefficiencies and improving the sustainability of concrete construction. In this work, we develop a machine learning algorithm that can utilize intermediate target variables and their associated noise to predict the final target variable. We apply the methodology to specify a concrete mix that has high resistance to carbonation, and another concrete mix that has low environmental impact. Both mixes also fulfill targets on the strength, density, and cost. The specified mixes are experimentally validated against their predictions. Our generic methodology enables the exploitation of noise in machine learning, which has a broad range of applications in struct
    
[^56]: DP-Adam: 改正Adam二阶动量估计中的DP偏差

    DP-Adam: Correcting DP Bias in Adam's Second Moment Estimation. (arXiv:2304.11208v1 [cs.LG])

    [http://arxiv.org/abs/2304.11208](http://arxiv.org/abs/2304.11208)

    该论文提出了DP-Adam，解决了在Adam优化器中传统DP的使用引入的偏差导致的问题。

    

    我们观察到在Adam优化器中传统DP的使用引入了偏差到二阶动量估计中，因为梯度计算中加入了独立噪声。这个偏差导致了对于低方差参数更新的不同缩放，这与非私有Adam和Adam的符号下降解释的行为不一致。实验结果表明，纠正DP噪声引入的偏差显着提高了DP-Adam的优化性能。

    We observe that the traditional use of DP with the Adam optimizer introduces a bias in the second moment estimation, due to the addition of independent noise in the gradient computation. This bias leads to a different scaling for low variance parameter updates, that is inconsistent with the behavior of non-private Adam, and Adam's sign descent interpretation. Empirically, correcting the bias introduced by DP noise significantly improves the optimization performance of DP-Adam.
    
[^57]: 快速GraspNeXt：用于边缘计算机器人抓取的多任务视觉学习的快速自注意神经网络架构

    Fast GraspNeXt: A Fast Self-Attention Neural Network Architecture for Multi-task Learning in Computer Vision Tasks for Robotic Grasping on the Edge. (arXiv:2304.11196v1 [cs.CV])

    [http://arxiv.org/abs/2304.11196](http://arxiv.org/abs/2304.11196)

    本文提出了一种针对边缘计算机器人抓取的计算机视觉任务定制的多任务深度自注意神经网络架构，名为快速GraspNeXt。实验表明，快速GraspNeXt在精度和速度方面表现优异。

    

    多任务学习已经被证明可以显著提高深度学习驱动的视觉系统在机器人抓取方面的性能。然而，高架构和计算复杂度可能导致在实际制造和仓库环境中通常用于机器人手臂的嵌入式设备上部署效果较差。因此，设计高效的、针对边缘计算机器人抓取的计算机视觉任务定制的多任务深度神经网络架构对于在制造环境中的广泛应用非常必要。为此，我们提出了快速GraspNeXt，一种针对嵌入式多任务学习的快速自注意神经网络架构，用于计算机视觉任务的机器人抓取。通过采用一个基于生成网络架构搜索策略和一组用于实现多任务学习性能和计算效率高度平衡的架构约束，我们构建了快速GraspNeXt。我们的实验结果表明，当在边缘设备上进行评估时，快速GraspNeXt在精度和速度方面均优于目前的最先进方法。

    Multi-task learning has shown considerable promise for improving the performance of deep learning-driven vision systems for the purpose of robotic grasping. However, high architectural and computational complexity can result in poor suitability for deployment on embedded devices that are typically leveraged in robotic arms for real-world manufacturing and warehouse environments. As such, the design of highly efficient multi-task deep neural network architectures tailored for computer vision tasks for robotic grasping on the edge is highly desired for widespread adoption in manufacturing environments. Motivated by this, we propose Fast GraspNeXt, a fast self-attention neural network architecture tailored for embedded multi-task learning in computer vision tasks for robotic grasping. To build Fast GraspNeXt, we leverage a generative network architecture search strategy with a set of architectural constraints customized to achieve a strong balance between multi-task learning performance a
    
[^58]: 从数据中自动识别动力系统

    Automatically identifying dynamical systems from data. (arXiv:2304.11182v1 [cs.LG])

    [http://arxiv.org/abs/2304.11182](http://arxiv.org/abs/2304.11182)

    该论文提出了一种从经验数据中自动识别动态规律的方法，这种方法能够较为准确地识别三维系统，具有在各种领域中理解复杂系统的潜力。

    

    从经验数据中发现描述系统动态的非线性微分方程是当代科学中的一个基本挑战。在这里，我们提出了一种方法来自动识别动态规律，该方法集成了去噪技术、稀疏回归和自助置信区间。我们使用一组随机初始条件的普通微分方程组，时序呈指数增长和各种信噪比进行评估。我们的算法一致识别三维系统，在时间序列适度的和高信号质量相对于背景噪声的情况下。通过准确识别动力系统，我们的方法具有潜力影响各种领域，如物理学和生物学以及工程学，在这些领域中理解复杂系统至关重要。

    Discovering nonlinear differential equations that describe system dynamics from empirical data is a fundamental challenge in contemporary science. Here, we propose a methodology to automatically identify dynamical laws by integrating denoising techniques, sparse regression, and bootstrap confidence intervals. We evaluate our method on well-known ordinary differential equations with an ensemble of random initial conditions, time series of increasing length, and varying signal-to-noise ratios. Our algorithm consistently identifies three-dimensional systems, given moderately-sized time series and high signal quality levels relative to background noise. By accurately identifying dynamical systems, our methodology has the potential to impact diverse fields, such as the physical and biological sciences, as well as engineering, where understanding complex systems is crucial.
    
[^59]: 基于任务自适应伪标签的跨感知元学习

    Task-Adaptive Pseudo Labeling for Transductive Meta-Learning. (arXiv:2304.11173v1 [cs.LG])

    [http://arxiv.org/abs/2304.11173](http://arxiv.org/abs/2304.11173)

    本文提出了一种名为“任务自适应伪标签”的跨感知元学习方法，利用伪标签生成未标记的查询集，使用监督设置并利用未标记的查询集，可以处理更多实例，从而带来更好的分类性能。

    

    元学习通过有限数量的支持集来进行适应性学习，这可能会导致样本偏差问题。为了解决这个问题，跨感知元学习越来越受到关注，超越了传统的感知性学习视角。本文提出了一种称为“任务自适应伪标签”的跨感知元学习方法。具体而言，利用标记的支持集生成未标记的查询集的伪标签，借此使得在适应过程中可以采用监督设置并利用未标记的查询集。结果，该方法比感知性方法能够处理更多的示例，从而可以产生更好的分类性能。需要注意的是，该提出的方法是首个将任务自适应应用于伪标签的方法。实验证明，该方法在5路1-shot few-shot中胜过了现有技术。

    Meta-learning performs adaptation through a limited amount of support set, which may cause a sample bias problem. To solve this problem, transductive meta-learning is getting more and more attention, going beyond the conventional inductive learning perspective. This paper proposes so-called task-adaptive pseudo labeling for transductive meta-learning. Specifically, pseudo labels for unlabeled query sets are generated from labeled support sets through label propagation. Pseudo labels enable to adopt the supervised setting as it is and also use the unlabeled query set in the adaptation process. As a result, the proposed method is able to deal with more examples in the adaptation process than inductive ones, which can result in better classification performance of the model. Note that the proposed method is the first approach of applying task adaptation to pseudo labeling. Experiments show that the proposed method outperforms the state-of-the-art (SOTA) technique in 5-way 1-shot few-shot 
    
[^60]: 颗粒球计算：一种高效、鲁棒和可解释的自适应多粒度表示和计算方法

    Granular ball computing: an efficient, robust, and interpretable adaptive multi-granularity representation and computation method. (arXiv:2304.11171v1 [cs.LG])

    [http://arxiv.org/abs/2304.11171](http://arxiv.org/abs/2304.11171)

    本文提出了一种基于颗粒球计算的自适应多粒度表示和计算方法，能够提高机器学习的效率、鲁棒性和可解释性。

    

    人类认知具有“先大后小”的认知机制，因此具有自适应的多粒度描述能力。这导致了有效性、鲁棒性和可解释性等计算特性。本文提出了一种新的基于颗粒球计算的自适应多粒度表示和计算方法。他们将这种方法应用于几个机器学习任务，并证明其相对于其他最先进的方法的有效性。

    Human cognition has a ``large-scale first'' cognitive mechanism, therefore possesses adaptive multi-granularity description capabilities. This results in computational characteristics such as efficiency, robustness, and interpretability. Although most existing artificial intelligence learning methods have certain multi-granularity features, they do not fully align with the ``large-scale first'' cognitive mechanism. Multi-granularity granular-ball computing is an important model method developed in recent years. This method can use granular-balls of different sizes to adaptively represent and cover the sample space, and perform learning based on granular-balls. Since the number of coarse-grained "granular-ball" is smaller than the number of sample points, granular-ball computing is more efficient; the coarse-grained characteristics of granular-balls are less likely to be affected by fine-grained sample points, making them more robust; the multi-granularity structure of granular-balls ca
    
[^61]: Graph-ToolFormer: 通过ChatGPT增强的提示，赋予LLMs图形推理能力

    Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT. (arXiv:2304.11116v1 [cs.AI])

    [http://arxiv.org/abs/2304.11116](http://arxiv.org/abs/2304.11116)

    本文旨在通过Graph-ToolFormer框架赋予LLMs图形推理能力，并解决现有LLMs在执行图形学习任务中存在的固有弱点。

    

    本文旨在开发一个能够对复杂图形数据进行推理的大语言模型（LLM）。当前，LLMs在各种自然语言学习任务上取得了非常出色的表现，这些扩展也已被应用于研究具有多模态数据的视觉任务。然而，在图形学习任务中，现有的LLMs由于在执行多步逻辑推理、精确的数学计算以及对空间和时间因素的感知方面存在一些固有弱点，因此呈现出非常严重的缺陷。为了解决这些挑战，本文将调查探索赋予现有LLMs图形推理能力的原理、方法和算法，这将对LLMs和图形学习的当前研究产生巨大影响。受最新的ChatGPT和Toolformer模型的启发，我们提出了Graph-ToolFormer（面向图形推理的Toolformer）框架，通过ChatGPT增强的提示来教导LLMs自身，旨在培养他们的图形推理能力。

    In this paper, we aim to develop a large language model (LLM) with the reasoning ability on complex graph data. Currently, LLMs have achieved very impressive performance on various natural language learning tasks, extensions of which have also been applied to study the vision tasks with multi-modal data. However, when it comes to the graph learning tasks, existing LLMs present very serious flaws due to their several inherited weaknesses in performing {multi-step logic reasoning}, {precise mathematical calculation} and {perception about the spatial and temporal factors}.  To address such challenges, in this paper, we will investigate the principles, methodologies and algorithms to empower existing LLMs with graph reasoning ability, which will have tremendous impacts on the current research of both LLMs and graph learning. Inspired by the latest ChatGPT and Toolformer models, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer) framework to teach LLMs themselves with pro
    
[^62]: 预测、学习、一致收敛和尺度敏感维度

    Prediction, Learning, Uniform Convergence, and Scale-sensitive Dimensions. (arXiv:2304.11059v1 [cs.LG])

    [http://arxiv.org/abs/2304.11059](http://arxiv.org/abs/2304.11059)

    本文介绍了一种新的通用算法，利用尺度敏感的Vapnik维度来学习$[0,1]$值函数类，并获得了关于期望绝对误差的一般上限。文中证明该上限不能在一般情况下进一步改善一个常数因子。这篇论文对无偏学习样本复杂度的提高具有重要的意义。

    

    我们提出了一种新的通用算法，用于在预测模型的推广中学习$[0,1]$值函数类，并证明了一般性的上限，该上限反映了由Alon、Ben-David、Cesa-Bianchi和Haussler提出的尺度敏感的Vapnik维度的推广。我们给出了下限，这表明我们的上限不能在一般情况下进一步改善一个常数因子。我们应用此结果和Haussler以及Benedek和Itai的技术，以利用这种尺度敏感的维度概念获得新的填充数上限。我们利用不同的技术，利用Kearns和Schapire的fat-shattering函数得到了新的填充数上限。我们展示了如何应用这两种填充上限来获得对无偏学习样本复杂度的改进一般性上限。对于每个$\epsilon > 0$，我们建立了一个类的足够条件和必要条件。

    We present a new general-purpose algorithm for learning classes of $[0,1]$-valued functions in a generalization of the prediction model, and prove a general upper bound on the expected absolute error of this algorithm in terms of a scale-sensitive generalization of the Vapnik dimension proposed by Alon, Ben-David, Cesa-Bianchi and Haussler. We give lower bounds implying that our upper bounds cannot be improved by more than a constant factor in general. We apply this result, together with techniques due to Haussler and to Benedek and Itai, to obtain new upper bounds on packing numbers in terms of this scale-sensitive notion of dimension. Using a different technique, we obtain new bounds on packing numbers in terms of Kearns and Schapire's fat-shattering function. We show how to apply both packing bounds to obtain improved general bounds on the sample complexity of agnostic learning. For each $\epsilon > 0$, we establish weaker sufficient and stronger necessary conditions for a class of 
    
[^63]: 预测中的外在数据：一种用于关联评估的FARM方法

    Exogenous Data in Forecasting: FARM -- An Approach for Relevance Evaluation. (arXiv:2304.11028v1 [eess.SP])

    [http://arxiv.org/abs/2304.11028](http://arxiv.org/abs/2304.11028)

    该论文介绍了一种名为FARM的方法，用于有效处理实时数据流并提供平衡的相关性度量，进而确定外部数据在预测中的重要性。

    

    外在数据被认为在提高预测准确性方面起着关键作用。针对恰当的选择，全面的相关性分析是一个基本的第一步，从外在数据与参考时间序列的相似性开始。受现有时间序列相似性指标的启发，我们介绍了一种名为FARM（前向角相关度量）的新方法，能够有效地处理实时数据流。我们的前向方法依赖于一种角度特征，该特征利用后续数据点的变化比较来对齐经过时间变形的序列。所提出的算法结合了本地和全局指标，提供了一个平衡的相关性度量。这导致将部分、中间匹配也视为外在数据序列重要指标的考虑因素。作为第一步验证，我们介绍了我们的FARM方法对合成但具有代表性的信号和真实世界时间序列记录的应用。同时展示了FARM方法提高了预测准确度的结果。

    Exogenous data is believed to play a key role for increasing forecasting accuracy. For an appropriate selection, a throughout relevance analysis is a fundamental first step, starting from the exogenous data similarity with the reference time series. Inspired by existing metrics for time series similarity, we introduce a new approach named FARM - Forward Angular Relevance Measure, able to effectively deal with real-time data streams. Our forward method relies on an angular feature that compares changes in subsequent data points to align time-warped series in an efficient way. The proposed algorithm combines local and global measures to provide a balanced relevance measure. This results in considering also partial, intermediate matches as relevant indicators for exogenous data series significance. As a first validation step, we present the application of our FARM approach to both synthetic but representative signals and real-world time series recordings. While demonstrating the improved 
    
[^64]: 协调推理服务系统的高准确性、成本效益和低延迟

    Reconciling High Accuracy, Cost-Efficiency, and Low Latency of Inference Serving Systems. (arXiv:2304.10892v1 [cs.LG])

    [http://arxiv.org/abs/2304.10892](http://arxiv.org/abs/2304.10892)

    InfAdapter提出了一个解决高准确性、低延迟和成本效益之间权衡问题的方法，通过主动选择一组带有资源分配的 ML 模型变体来满足延迟 SLO，并最大化由准确性和成本组成的目标函数，相较于其他方法降低了 SLO 违规和成本。

    

    机器学习（ML）推理服务的使用正在急剧增加。ML推理服务与用户直接交互，需要快速准确的响应。此外，这些服务面临不断变化的请求工作负载，需要调整其计算资源。计算资源不合理会导致延迟服务级别目标 (SLOs) 违规或浪费计算资源。考虑准确性、延迟和资源成本等方面的所有因素来适应动态工作负载具有挑战性。为了应对这些挑战，我们提出了 InfAdapter，它会主动选择一组带有资源分配的 ML 模型变体，以满足延迟 SLO，并最大化由准确性和成本组成的目标函数。相较于流行的行业自动缩放器 (Kubernetes Vertical Pod Autoscaler)，InfAdapter 分别降低了 SLO 违规和成本达 65% 和 33%。

    The use of machine learning (ML) inference for various applications is growing drastically. ML inference services engage with users directly, requiring fast and accurate responses. Moreover, these services face dynamic workloads of requests, imposing changes in their computing resources. Failing to right-size computing resources results in either latency service level objectives (SLOs) violations or wasted computing resources. Adapting to dynamic workloads considering all the pillars of accuracy, latency, and resource cost is challenging. In response to these challenges, we propose InfAdapter, which proactively selects a set of ML model variants with their resource allocations to meet latency SLO while maximizing an objective function composed of accuracy and cost. InfAdapter decreases SLO violation and costs up to 65% and 33%, respectively, compared to a popular industry autoscaler (Kubernetes Vertical Pod Autoscaler).
    
[^65]: 使用Z3进行FNN全局鲁棒性的形式化建模和验证

    Using Z3 for Formal Modeling and Verification of FNN Global Robustness. (arXiv:2304.10558v1 [cs.LG])

    [http://arxiv.org/abs/2304.10558](http://arxiv.org/abs/2304.10558)

    本文介绍了使用Z3求解器对全局鲁棒性可验证框架DeepGlobal进行更明确的定义和优化的工作，来建立FNN的形式化模型，以实现更有效的验证。

    

    虽然前馈神经网络（FNN）在各种任务中取得了显著的成功，但它们对对抗样本很容易受到攻击。已经开发了几种技术来验证FNN的对抗鲁棒性，但大多数技术都集中在针对单个数据点的局部扰动邻域的鲁棒性验证上。全局鲁棒性分析仍存在较大的研究空白。DeepGlobal是一种全局鲁棒性可验证框架，旨在确定FNN的所有可能的对抗危险区域（ADR），不限于测试集中的数据样本。本文提出了DeepGlobal的完整规范和实现，利用SMT求解器Z3进行更明确的定义，并提出了几项改进以进行更高效的验证。为了评估我们的实现和改进的有效性，我们对一组基准数据集进行了广泛的实验。我们的实验结果进行了可视化。

    While Feedforward Neural Networks (FNNs) have achieved remarkable success in various tasks, they are vulnerable to adversarial examples. Several techniques have been developed to verify the adversarial robustness of FNNs, but most of them focus on robustness verification against the local perturbation neighborhood of a single data point. There is still a large research gap in global robustness analysis. The global-robustness verifiable framework DeepGlobal has been proposed to identify \textit{all} possible Adversarial Dangerous Regions (ADRs) of FNNs, not limited to data samples in a test set. In this paper, we propose a complete specification and implementation of DeepGlobal utilizing the SMT solver Z3 for more explicit definition, and propose several improvements to DeepGlobal for more efficient verification. To evaluate the effectiveness of our implementation and improvements, we conduct extensive experiments on a set of benchmark datasets. Visualization of our experiment results s
    
[^66]: Transformer介绍

    An Introduction to Transformers. (arXiv:2304.10557v1 [cs.LG])

    [http://arxiv.org/abs/2304.10557](http://arxiv.org/abs/2304.10557)

    Transformer是一种神经网络组件，可以学习序列或数据集表示，在自然语言处理、计算机视觉和时空建模方面取得了重大进展。本论文提供了一个数学精确、直观、简洁的Transformer架构描述。

    

    Transformer是一种可以学习序列或数据集表示的神经网络组件。Transformer在自然语言处理、计算机视觉和时空建模方面取得了重大进展。虽然有很多Transformer的介绍，但大多数都缺少对其架构的精确数学描述，其设计选择的直觉也常常缺失。此外，随着研究路径的曲折，Transformer部件的解释可能是异质的。在这篇论文中，我们旨在提供一个数学精确、直观、简洁的Transformer架构描述。

    The transformer is a neural network component that can be used to learn useful representations of sequences or sets of datapoints. The transformer has driven recent advances in natural language processing, computer vision, and spatio-temporal modelling. There are many introductions to transformers, but most do not contain precise mathematical descriptions of the architecture and the intuitions behind the design choices are often also missing. Moreover, as research takes a winding path, the explanations for the components of the transformer can be idiosyncratic. In this note we aim for a mathematically precise, intuitive, and clean description of the transformer architecture.
    
[^67]: 人工智能革命对资产管理的影响

    The impact of the AI revolution on asset management. (arXiv:2304.10212v1 [q-fin.GN])

    [http://arxiv.org/abs/2304.10212](http://arxiv.org/abs/2304.10212)

    本论文分享了作者关于人工智能可能对资产管理产生影响的观点，同时提供了一个衡量特定基金是否真正开发了AI的简单标准。

    

    深度学习的最新进展使得机器具备了杰出的能力：它们可以阅读和理解自由流动的文本，与人类进行推理和交涉，翻译不同语言的文本，学习如何做出最优决策等等。如今，机器已经在癌症检测、蛋白质结构预测、药物设计、核聚变反应堆控制等方面实现了革命性突破。虽然这些能力仍处于萌芽阶段，但它们在持续完善和应用中的技术影响几乎将在人类活动的几乎所有社会和经济领域中发挥作用，这是我们以前从未见过的。本文将分享我的观点，即人工智能如何影响资产管理，我将提供一个思维框架，使读者可以用一个简单的标准来评估一个特定基金是否真正开发

    Recent progress in deep learning, a special form of machine learning, has led to remarkable capabilities machines can now be endowed with: they can read and understand free flowing text, reason and bargain with human counterparts, translate texts between languages, learn how to take decisions to maximize certain outcomes, etc. Today, machines have revolutionized the detection of cancer, the prediction of protein structures, the design of drugs, the control of nuclear fusion reactors etc. Although these capabilities are still in their infancy, it seems clear that their continued refinement and application will result in a technological impact on nearly all social and economic areas of human activity, the likes of which we have not seen before. In this article, I will share my view as to how AI will likely impact asset management in general and I will provide a mental framework that will equip readers with a simple criterion to assess whether and to what degree a given fund really exploi
    
[^68]: 双记忆强化学习

    Two-Memory Reinforcement Learning. (arXiv:2304.10098v1 [cs.LG])

    [http://arxiv.org/abs/2304.10098](http://arxiv.org/abs/2304.10098)

    本文提出了双记忆强化学习代理 (2M)，它结合了情节记忆和强化学习的优点来提高学习速度和准确性。

    

    虽然深度强化学习取得了重要的经验性成功，但由于奖励信息传播和参数神经网络更新的速度较慢，它倾向于学习得比较慢。另一方面，非参数化的情节记忆提供了相对较快的学习替代方案，它不需要表示学习，并使用最大情节回报作为状态-动作值进行行动选择。情节记忆和强化学习都有各自的优点和缺点。值得注意的是，人类可以同时利用多个记忆系统进行学习，并从中获益。在这项工作中，我们提出了一种称为双记忆强化学习代理（2M）的方法，它结合了情节记忆和强化学习的优点。 2M 代理利用情节记忆部分的速度和强化学习部分的最优性和广泛适用性相互补充。我们的实验表明，

    While deep reinforcement learning has shown important empirical success, it tends to learn relatively slow due to slow propagation of rewards information and slow update of parametric neural networks. Non-parametric episodic memory, on the other hand, provides a faster learning alternative that does not require representation learning and uses maximum episodic return as state-action values for action selection. Episodic memory and reinforcement learning both have their own strengths and weaknesses. Notably, humans can leverage multiple memory systems concurrently during learning and benefit from all of them. In this work, we propose a method called Two-Memory reinforcement learning agent (2M) that combines episodic memory and reinforcement learning to distill both of their strengths. The 2M agent exploits the speed of the episodic memory part and the optimality and the generalization capacity of the reinforcement learning part to complement each other. Our experiments demonstrate that 
    
[^69]: 大规模语言模型中潜在空间理论对应新兴能力

    A Latent Space Theory for Emergent Abilities in Large Language Models. (arXiv:2304.09960v1 [cs.CL])

    [http://arxiv.org/abs/2304.09960](http://arxiv.org/abs/2304.09960)

    本文探讨了大规模语言模型中的贝叶斯推断和稀疏联合分布，证明了LLMs能够完成语言理解、上下文学习、思路启发以及有效指令微调的新兴能力。

    

    语言并不是随机生成，而是为了传递信息。语言与其底层含义之间存在强烈的关联，在其相关性方面有着严重偏差的稀疏联合分布。此外，由于稀疏性，这些高峰值恰好与语言的边缘分布匹配。随着大数据和大模型上训练的LLMs的出现，我们现在可以精确评估语言的边缘分布，这提供了一种方便的探索联合分布稀疏结构实现有效推理的方式。在本文中，我们将语言分类为明确与{\epsilon}-模糊，并提出定量结果，以表明LLMs的新兴能力（例如语言理解、上下文学习、思路启发以及有效指令微调）都可以归因于对稀疏联合分布进行贝叶斯推断。

    Languages are not created randomly but rather to communicate information. There is a strong association between languages and their underlying meanings, resulting in a sparse joint distribution that is heavily peaked according to their correlations. Moreover, these peak values happen to match with the marginal distribution of languages due to the sparsity. With the advent of LLMs trained on big data and large models, we can now precisely assess the marginal distribution of languages, providing a convenient means of exploring the sparse structures in the joint distribution for effective inferences. In this paper, we categorize languages as either unambiguous or {\epsilon}-ambiguous and present quantitative results to demonstrate that the emergent abilities of LLMs, such as language understanding, in-context learning, chain-of-thought prompting, and effective instruction fine-tuning, can all be attributed to Bayesian inference on the sparse joint distribution of languages.
    
[^70]: SemEval 2023 任务6: LegalEval -- 理解法律文本

    SemEval 2023 Task 6: LegalEval -- Understanding Legal Texts. (arXiv:2304.09548v1 [cs.CL])

    [http://arxiv.org/abs/2304.09548](http://arxiv.org/abs/2304.09548)

    SemEval 2023举办了LegalEval共享任务，即理解法律文本，包括 自动结构化和语义连贯化的法律文件（Task-A），法律命名实体识别（Task-B）以及自动预测法律案件结果和提供预测解释（Task-C）。26个团队提交了系统论文并在所有子任务中优于基准线，但仍有改进空间。

    

    在人口众多的国家，待处理的法律案件呈指数增长。有必要开发基于自然语言处理的技术，对法律文件进行处理和自动理解。为了促进在法律自然语言处理领域的研究，我们在 SemEval 2023 上组织了共享任务 LegalEval - 理解法律文本。LegalEval 任务有三个子任务：Task-A（修辞角色标记）是自动将法律文件结构化为语义连贯的单元，Task-B（法律命名实体识别）处理在法律文件中识别相关实体，而 Task-C（法院判决预测与解释）探索了自动预测法律案件结果以及提供预测解释的可能性。共有26个团队（分布在全球的约100名参与者）提交了系统论文。在每个子任务中，所提出的系统都优于基准线；但是，仍然有很大的改进空间。本文介绍了 LegalEval 任务的组织和细节，并概述了参与系统及其性能。

    In populous countries, pending legal cases have been growing exponentially. There is a need for developing NLP-based techniques for processing and automatically understanding legal documents. To promote research in the area of Legal NLP we organized the shared task LegalEval - Understanding Legal Texts at SemEval 2023. LegalEval task has three sub-tasks: Task-A (Rhetorical Roles Labeling) is about automatically structuring legal documents into semantically coherent units, Task-B (Legal Named Entity Recognition) deals with identifying relevant entities in a legal document and Task-C (Court Judgement Prediction with Explanation) explores the possibility of automatically predicting the outcome of a legal case along with providing an explanation for the prediction. In total 26 teams (approx. 100 participants spread across the world) submitted systems paper. In each of the sub-tasks, the proposed systems outperformed the baselines; however, there is a lot of scope for improvement. This pape
    
[^71]: FastMRI前列腺：一个公开可用的双参数MRI数据集，以推进前列腺癌成像的机器学习技术

    FastMRI Prostate: A Publicly Available, Biparametric MRI Dataset to Advance Machine Learning for Prostate Cancer Imaging. (arXiv:2304.09254v1 [physics.med-ph] CROSS LISTED)

    [http://arxiv.org/abs/2304.09254](http://arxiv.org/abs/2304.09254)

    FastMRI前列腺推出了一个公开可用的双参数MRI数据集，用于研究MRI图像重建和评估，以提高MRI在前列腺癌检测和评估上的实用性。

    

    fastMRI大脑和膝盖数据集通过新颖、临床相关的重建方法，使得磁共振成像（MRI）的速度和图像质量得到了显著提高。本研究描述了fastMRI数据集的2023年4月扩展，包括对临床人群进行的双参数前列腺MRI数据获取。该数据集包含T2加权和扩散加权序列的原始k-空间和重建图像，以及指示前列腺癌存在和分级的切片级标签。与fastMRI一样，提高对原始前列腺MRI数据的可访问性将进一步促进MR图像重建和评估的研究，以实现改进MRI在前列腺癌检测和评估中的实用性的更大目标。该数据集可在https://fastmri.med.nyu.edu 上获取。

    The fastMRI brain and knee dataset has enabled significant advances in exploring reconstruction methods for improving speed and image quality for Magnetic Resonance Imaging (MRI) via novel, clinically relevant reconstruction approaches. In this study, we describe the April 2023 expansion of the fastMRI dataset to include biparametric prostate MRI data acquired on a clinical population. The dataset consists of raw k-space and reconstructed images for T2-weighted and diffusion-weighted sequences along with slice-level labels that indicate the presence and grade of prostate cancer. As has been the case with fastMRI, increasing accessibility to raw prostate MRI data will further facilitate research in MR image reconstruction and evaluation with the larger goal of improving the utility of MRI for prostate cancer detection and evaluation. The dataset is available at https://fastmri.med.nyu.edu.
    
[^72]: 我们实现了个性化治疗吗？使用重复采样的在线强化学习算法进行个性化评估

    Did we personalize? Assessing personalization by an online reinforcement learning algorithm using resampling. (arXiv:2304.05365v1 [cs.LG])

    [http://arxiv.org/abs/2304.05365](http://arxiv.org/abs/2304.05365)

    本论文提出了一种使用重复采样的政策评估方法，以评估在线 RL 算法实现的个性化程度。该方法可用于优化数字健康的个性化干预。

    

    在数字健康中，使用强化学习（RL）个性化治疗序列以支持用户采取更健康的行为越来越受到关注。这种连续决策问题涉及到基于用户的上下文（例如，先前的活动水平、位置等）在何时治疗以及如何治疗的决定。在线RL算法是这个问题的一个有前途的数据驱动方法，因为它基于每个用户的历史反馈进行学习，并利用这些知识个性化这些决策。然而，要决定是否应在实际部署的“优化”干预中包含RL算法，我们必须评估数据证据，表明RL算法实际上正在将治疗个性化适应其用户。由于RL算法中的随机性，人们可能会对其在某些状态下的学习并使用此学习来提供特定治疗的能力产生误解。我们使用工作定义的个性化，并介绍了一种重复采样政策评估方法来评估在线RL算法实现的个性化水平。我们使用模拟评估了我们提出的方法，并展示了我们的方法可以准确地识别个性化的策略。我们提出的方法在优化数字健康的个性化干预方面具有潜在应用。

    There is a growing interest in using reinforcement learning (RL) to personalize sequences of treatments in digital health to support users in adopting healthier behaviors. Such sequential decision-making problems involve decisions about when to treat and how to treat based on the user's context (e.g., prior activity level, location, etc.). Online RL is a promising data-driven approach for this problem as it learns based on each user's historical responses and uses that knowledge to personalize these decisions. However, to decide whether the RL algorithm should be included in an ``optimized'' intervention for real-world deployment, we must assess the data evidence indicating that the RL algorithm is actually personalizing the treatments to its users. Due to the stochasticity in the RL algorithm, one may get a false impression that it is learning in certain states and using this learning to provide specific treatments. We use a working definition of personalization and introduce a resamp
    
[^73]: 不同约束运动模型在基于图的轨迹预测中的评估

    Evaluation of Differentially Constrained Motion Models for Graph-Based Trajectory Prediction. (arXiv:2304.05116v1 [cs.RO])

    [http://arxiv.org/abs/2304.05116](http://arxiv.org/abs/2304.05116)

    使用深度学习模型进行运动预测在自动驾驶中表现出色，但缺乏解释性和可能违反物理约束。因此，结合差分约束运动模型能提供物理上可行的轨迹，研究表明低阶积分器模型表现更好，并且数值求解器对模型性能产生影响。

    

    随着深度学习模型在自动驾驶中表现出色且可调性高，成为运动预测的标准。然而，高度灵活性伴随的是解释性缺失和可能违反的物理约束。使用差分约束运动模型来提供物理上可行的轨迹，可以作为与这些数据驱动方法相配合的一个有前途的方向。本研究基于先前提出的基于图神经网络的模型 MTP-GO，研究了各种运动模型结合数值求解器进行预测任务的表现。研究表明，为了获得精确的预测结果，简单的模型，如低阶积分器模型，优于更复杂的运动学模型。此外，数值求解器可以对运动预测模型的性能产生重大影响。

    Given their adaptability and encouraging performance, deep-learning models are becoming standard for motion prediction in autonomous driving. However, with great flexibility comes a lack of interpretability and possible violations of physical constraints. Accompanying these data-driven methods with differentially-constrained motion models to provide physically feasible trajectories is a promising future direction. The foundation for this work is a previously introduced graph-neural-network-based model, MTP-GO. The neural network learns to compute the inputs to an underlying motion model to provide physically feasible trajectories. This research investigates the performance of various motion models in combination with numerical solvers for the prediction task. The study shows that simpler models, such as low-order integrator models, are preferred over more complex ones, e.g., kinematic models, to achieve accurate predictions. Further, the numerical solver can have a substantial impact o
    
[^74]: Wav2code：通过码本查找恢复干净的语音表征，用于噪声鲁棒的ASR

    Wav2code: Restore Clean Speech Representations via Codebook Lookup for Noise-Robust ASR. (arXiv:2304.04974v1 [eess.AS])

    [http://arxiv.org/abs/2304.04974](http://arxiv.org/abs/2304.04974)

    Wav2code是一种基于自监督学习的ASR模型，可以实现用于噪声鲁棒的无失真增强，从而提供更好的语音表征。

    

    自动语音识别(ASR)由于深度学习的最新进展已经取得了显著的成功，但在实际嘈杂环境下，其性能通常会显著降低。最近的研究将语音增强(SE)引入作为前端来提高语音质量，证明了其有效性，但由于语音失真问题，可能对下游ASR不是最优的。基于这一点，最新的工作将SE和当前流行的自监督学习(SSL)结合起来来缓解失真问题并提高噪声鲁棒性。尽管有效性，但传统SE引起的语音失真仍无法完全消除。本文提出了一种名为Wav2code的自监督框架，用于实现用于噪声鲁棒ASR的无失真广义SE。首先，在预训练阶段，从SSL模型获得干净的语音表征，通过最近邻特征匹配查找离散码本，然后利用得到的代码序列来重构原始音频信号以获得干净的语音表征；接着，该代码序列被用于无失真地增强带噪语音以便于提高语音识别的鲁棒性。

    Automatic speech recognition (ASR) has gained a remarkable success thanks to recent advances of deep learning, but it usually degrades significantly under real-world noisy conditions. Recent works introduce speech enhancement (SE) as front-end to improve speech quality, which is proved effective but may not be optimal for downstream ASR due to speech distortion problem. Based on that, latest works combine SE and currently popular self-supervised learning (SSL) to alleviate distortion and improve noise robustness. Despite the effectiveness, the speech distortion caused by conventional SE still cannot be completely eliminated. In this paper, we propose a self-supervised framework named Wav2code to implement a generalized SE without distortions for noise-robust ASR. First, in pre-training stage the clean speech representations from SSL model are sent to lookup a discrete codebook via nearest-neighbor feature matching, the resulted code sequence are then exploited to reconstruct the origin
    
[^75]: 带函数约束的随机变分不等式问题的一阶方法

    First-order methods for Stochastic Variational Inequality problems with Function Constraints. (arXiv:2304.04778v1 [math.OC])

    [http://arxiv.org/abs/2304.04778](http://arxiv.org/abs/2304.04778)

    本文提出了一种新的一阶方法，适用于具有随机算子和/或随机约束的带函数约束的变分不等式问题，当FCVI问题是确定性非光滑的或随机的时，这些方法可以实现最优算子或样本复杂性。

    

    在机器学习中，单调变分不等式是一个重要的问题。许多情况下，变分不等式问题伴随着可能是数据驱动的函数约束，这使得投影算子的计算变得具有挑战性。本文针对各种情况下的带函数约束的变分不等式问题，包括具有随机算子和/或随机约束的光滑或非光滑问题，提出了新的一阶方法。首先，我们介绍了{\texttt{OpConEx}}方法及其随机变体，它们采用算子和限制评估的外推来更新变量和Lagrangian乘数。当FCVI问题是确定性非光滑的或随机的（包括光滑或非光滑的随机约束）时，这些方法可以实现最优算子或样本复杂性。值得注意的是，我们的算法是简单的单循环程序，不需要知道拉格朗日乘数就可以达到最优性。

    The monotone Variational Inequality (VI) is an important problem in machine learning. In numerous instances, the VI problems are accompanied by function constraints which can possibly be data-driven, making the projection operator challenging to compute. In this paper, we present novel first-order methods for function constrained VI (FCVI) problem under various settings, including smooth or nonsmooth problems with a stochastic operator and/or stochastic constraints. First, we introduce the~{\texttt{OpConEx}} method and its stochastic variants, which employ extrapolation of the operator and constraint evaluations to update the variables and the Lagrangian multipliers. These methods achieve optimal operator or sample complexities when the FCVI problem is either (i) deterministic nonsmooth, or (ii) stochastic, including smooth or nonsmooth stochastic constraints. Notably, our algorithms are simple single-loop procedures and do not require the knowledge of Lagrange multipliers to attain th
    
[^76]: 通过可扩展的主题嵌入从连续新闻流中无监督地发现故事

    Unsupervised Story Discovery from Continuous News Streams via Scalable Thematic Embedding. (arXiv:2304.04099v1 [cs.IR])

    [http://arxiv.org/abs/2304.04099](http://arxiv.org/abs/2304.04099)

    本研究提出了一种新颖的主题嵌入方法和一个可扩展的无监督在线故事发现框架USTORY，可以动态表示文章和故事，并考虑它们共享的时间主题和新颖性，以帮助人们消化大量的新闻流。

    

    无监督地发现实时相关新闻文章故事，有助于人们在不需要昂贵人工注释的情况下消化大量的新闻流。现有的无监督在线故事发现研究的普遍方法是用符号或基于图的嵌入来表示新闻文章，并将它们逐步聚类成故事。最近的大型语言模型有望进一步改善嵌入，但是通过无差别地编码文章中的所有信息来直接采用这些模型无法有效处理富含文本且不断发展的新闻流。在这项工作中，我们提出了一种新颖的主题嵌入方法，使用现成的预训练句子编码器来动态表示文章和故事，并考虑它们共享的时间主题。为了实现无监督在线故事发现的想法，引入了一个可扩展框架USTORY，包括两个主要技术，即主题和时间感知的动态嵌入和新颖性感知的自适应聚类。

    Unsupervised discovery of stories with correlated news articles in real-time helps people digest massive news streams without expensive human annotations. A common approach of the existing studies for unsupervised online story discovery is to represent news articles with symbolic- or graph-based embedding and incrementally cluster them into stories. Recent large language models are expected to improve the embedding further, but a straightforward adoption of the models by indiscriminately encoding all information in articles is ineffective to deal with text-rich and evolving news streams. In this work, we propose a novel thematic embedding with an off-the-shelf pretrained sentence encoder to dynamically represent articles and stories by considering their shared temporal themes. To realize the idea for unsupervised online story discovery, a scalable framework USTORY is introduced with two main techniques, theme- and time-aware dynamic embedding and novelty-aware adaptive clustering, fuel
    
[^77]: SSS在SemEval-2023任务10中的论文：使用投票细调变压器可解释的检测在线性别歧视。 (arXiv：2304.03518v1 [cs.CL])

    SSS at SemEval-2023 Task 10: Explainable Detection of Online Sexism using Majority Voted Fine-Tuned Transformers. (arXiv:2304.03518v1 [cs.CL])

    [http://arxiv.org/abs/2304.03518](http://arxiv.org/abs/2304.03518)

    本文描述了使用细调BERT模型和多数投票集成模型来检测和解释在线性别歧视的方法。翻转显着降低了女性在社交媒体平台上经历不成比例的性别歧视的风险。

    

    本文描述了我们在SemEval 2023任务10中提交的作品-可解释的在线性别歧视检测（EDOS），分为三个子任务。社交媒体平台的不断增长导致女性在社交媒体平台上面临不成比例的性别歧视。这使得检测和解释在线性别歧视内容变得比以往更加重要，以使社交媒体对女性更加安全和可访问。我们的方法包括实验和微调基于BERT的模型，并使用多数投票集合模型，该模型优于单个基线模型得分。我们的系统在任务A中实现了宏F1分数0.8392，在任务B中为0.6092，在任务C中为0.4319。

    This paper describes our submission to Task 10 at SemEval 2023-Explainable Detection of Online Sexism (EDOS), divided into three subtasks. The recent rise in social media platforms has seen an increase in disproportionate levels of sexism experienced by women on social media platforms. This has made detecting and explaining online sexist content more important than ever to make social media safer and more accessible for women. Our approach consists of experimenting and finetuning BERT-based models and using a Majority Voting ensemble model that outperforms individual baseline model scores. Our system achieves a macro F1 score of 0.8392 for Task A, 0.6092 for Task B, and 0.4319 for Task C.
    
[^78]: TransPimLib：用于处理器内存系统上高效的超越函数的库

    TransPimLib: A Library for Efficient Transcendental Functions on Processing-in-Memory Systems. (arXiv:2304.01951v1 [cs.MS])

    [http://arxiv.org/abs/2304.01951](http://arxiv.org/abs/2304.01951)

    TransPimLib提供了处理器内存系统上高效的超越函数计算方法，有助于提高PIM系统的计算能力和支持更广泛的工作负载，特别是机器学习应用中的激活函数。

    

    处理器内存系统（PIM）承诺减轻现代计算系统中的数据移动瓶颈。然而，现有的真实PIM系统有一个内在的劣势，即它们的硬件比传统的处理器（CPU、GPU）更加受限，因为在内存附近或内部构建处理元件的难度和成本很高。因此，通用的PIM架构支持相当有限的指令集，并且难以执行复杂的操作，例如超越函数和其他难以计算的操作（例如平方根）。这些操作对于一些现代工作负载尤其重要，例如机器学习应用中的激活函数。为了在通用的PIM系统中提供对超越（和其他难以计算）函数的支持，我们介绍了TransPimLib，这是一个库，提供基于CORDIC和LUT的三角函数、双曲函数、指数、对数、平方根等难以计算的函数的方法。

    Processing-in-memory (PIM) promises to alleviate the data movement bottleneck in modern computing systems. However, current real-world PIM systems have the inherent disadvantage that their hardware is more constrained than in conventional processors (CPU, GPU), due to the difficulty and cost of building processing elements near or inside the memory. As a result, general-purpose PIM architectures support fairly limited instruction sets and struggle to execute complex operations such as transcendental functions and other hard-to-calculate operations (e.g., square root). These operations are particularly important for some modern workloads, e.g., activation functions in machine learning applications.  In order to provide support for transcendental (and other hard-to-calculate) functions in general-purpose PIM systems, we present \emph{TransPimLib}, a library that provides CORDIC-based and LUT-based methods for trigonometric functions, hyperbolic functions, exponentiation, logarithm, squar
    
[^79]: CoReFusion: 基于对比正则化融合实现热红外引导超分辨率

    CoReFusion: Contrastive Regularized Fusion for Guided Thermal Super-Resolution. (arXiv:2304.01243v1 [eess.IV])

    [http://arxiv.org/abs/2304.01243](http://arxiv.org/abs/2304.01243)

    本文提出了一种新颖的数据融合框架和正则化技术，用于热成像的引导超分辨率，具有计算效率，轻量级和鲁棒性，实现了在缺失数据的情况下性能不受影响，并且在基准数据集上表现出较高的性能。

    

    热成像由于在低光情况下表现出色而比一般可见光成像具有更多优势。 超分辨率技术可以通过使用低成本、低分辨率的热成像传感器测量来复制准确的高分辨率热成像图片，而且在关键地区，无法捕捉到可见光再生能力的应用程序可能会失败。然而，由于成像光谱范围不一致，使用可见光图像进行热成像引导超分辨率很困难。我们提出了一种新颖的数据融合框架和正则化技术，用于热成像的引导超分辨率。所提出的体系结构在丢失一个模态（高分辨率RGB图像或更低分辨率的热成像）时仍具备计算效率和轻量级的特点，并且在数据缺失的情况下具备鲁棒性。该方法在基准数据集上提供了最先进的性能，并在视觉质量和客观指标方面优于现有方法。

    Thermal imaging has numerous advantages over regular visible-range imaging since it performs well in low-light circumstances. Super-Resolution approaches can broaden their usefulness by replicating accurate high-resolution thermal pictures using measurements from low-cost, low-resolution thermal sensors. Because of the spectral range mismatch between the images, Guided Super-Resolution of thermal images utilizing visible range images is difficult. However, In case of failure to capture Visible Range Images can prevent the operations of applications in critical areas. We present a novel data fusion framework and regularization technique for Guided Super Resolution of Thermal images. The proposed architecture is computationally in-expensive and lightweight with the ability to maintain performance despite missing one of the modalities, i.e., high-resolution RGB image or the lower-resolution thermal image, and is designed to be robust in the presence of missing data. The proposed method pr
    
[^80]: PCA-Net：操作学习的复杂性上下界

    Operator learning with PCA-Net: upper and lower complexity bounds. (arXiv:2303.16317v1 [cs.LG])

    [http://arxiv.org/abs/2303.16317](http://arxiv.org/abs/2303.16317)

    本文发展了PCA-Net的近似理论，得出了通用逼近结果，并识别出了使用PCA-Net进行高效操作学习的潜在障碍：输出分布的复杂性和算子空间的内在复杂性。

    

    神经算子在计算科学和工程中备受关注。PCA-Net是一种最近提出的神经算子架构，它将主成分分析(PCA)与神经网络相结合，以逼近潜在的算子。本文对这种方法进行了近似理论的发展，改进并显着扩展了此方向的以前的工作。在定性界限方面，本文得出了新颖的通用逼近结果，在对潜在算子和数据生成分布的最小假设的前提下。在定量限制方面，本文识别了使用PCA-Net进行高效操作学习的两个潜在障碍，通过导出下界进行了严格证明，第一个障碍与输出分布的复杂性有关，由PCA特征值的缓慢衰减来衡量；另一个障碍涉及无限维输入和输出空间之间的算子空间的内在复杂性。

    Neural operators are gaining attention in computational science and engineering. PCA-Net is a recently proposed neural operator architecture which combines principal component analysis (PCA) with neural networks to approximate an underlying operator. The present work develops approximation theory for this approach, improving and significantly extending previous work in this direction. In terms of qualitative bounds, this paper derives a novel universal approximation result, under minimal assumptions on the underlying operator and the data-generating distribution. In terms of quantitative bounds, two potential obstacles to efficient operator learning with PCA-Net are identified, and made rigorous through the derivation of lower complexity bounds; the first relates to the complexity of the output distribution, measured by a slow decay of the PCA eigenvalues. The other obstacle relates the inherent complexity of the space of operators between infinite-dimensional input and output spaces, 
    
[^81]: 有限采样下的马尔可夫决策过程策略综合

    Strategy Synthesis in Markov Decision Processes Under Limited Sampling Access. (arXiv:2303.12718v1 [cs.LG])

    [http://arxiv.org/abs/2303.12718](http://arxiv.org/abs/2303.12718)

    本文提出了一种针对灰箱 MDP 的策略综合算法，使用区间 MDP 作为内部模型，并通过强化学习，结合下置信区间探索和行动划分的方法，解决有限采样下的问题，用于合成最大化回报的实用策略。

    

    在控制理论、人工智能和形式方法中，一个核心的任务是为在部分未知环境下操作的代理合成最大化回报的策略。在灰箱马尔可夫决策过程 (MDPs) 模型中，代理的行为影响以后的状态而不是涉及到的概率。在本文中，我们通过强化学习为灰箱 MDP 合成策略设计了一个内部模型为区间 MDP 的策略综合算法。为了应对强化学习中的有限采样问题，我们将两个新的概念引入到算法中，专注于快速成功的学习而不是随机保证和最优性：下置信区间探索加强已经学习的可行策略的变体，行动划分将学习行动空间缩小到有前途的行动。我们通过具有代表性的实例说明了算法的优点。

    A central task in control theory, artificial intelligence, and formal methods is to synthesize reward-maximizing strategies for agents that operate in partially unknown environments. In environments modeled by gray-box Markov decision processes (MDPs), the impact of the agents' actions are known in terms of successor states but not the stochastics involved. In this paper, we devise a strategy synthesis algorithm for gray-box MDPs via reinforcement learning that utilizes interval MDPs as internal model. To compete with limited sampling access in reinforcement learning, we incorporate two novel concepts into our algorithm, focusing on rapid and successful learning rather than on stochastic guarantees and optimality: lower confidence bound exploration reinforces variants of already learned practical strategies and action scoping reduces the learning action space to promising actions. We illustrate benefits of our algorithms by means of a prototypical implementation applied on examples fro
    
[^82]: EPiC: 基于部分点云集成的鲁棒分类方法

    EPiC: Ensemble of Partial Point Clouds for Robust Classification. (arXiv:2303.11419v1 [cs.CV])

    [http://arxiv.org/abs/2303.11419](http://arxiv.org/abs/2303.11419)

    本文提出了一种基于部分点云采样的通用集成框架，由多种采样方法联合使用提高鲁棒性，达到了最优性能。

    

    对于真实应用中的点云分类来说，由于消费型3D传感器通常采集的是部分和带有噪声的数据，且会受到各种因素的影响而变得降质。本文提出了一种基于部分点云采样的通用集成框架。每个集成成员只暴露于部分输入数据。我们同时使用两种基于补丁和曲线的局部采样策略以及一种全局的随机采样策略。我们证明了我们的方法对各种局部和全局污染具有很强的鲁棒性。我们展示了我们的框架极大地提高了顶级分类网络的鲁棒性。我们的实验设置使用了Ren等人[24]引入的最新ModelNet-C数据库，在不经过数据增强和经过数据增强的情况下都达到了最优（SOTA）。我们的未经数据增强的均值腐蚀误差（mCE）为0.64（当前SOTA为0.86），经过数据增强后为0.50（当前SOTA为0.57）。我们通过多样性和可视化分析了这些显著的结果。

    Robust point cloud classification is crucial for real-world applications, as consumer-type 3D sensors often yield partial and noisy data, degraded by various artifacts. In this work we propose a general ensemble framework, based on partial point cloud sampling. Each ensemble member is exposed to only partial input data. Three sampling strategies are used jointly, two local ones, based on patches and curves, and a global one of random sampling. We demonstrate the robustness of our method to various local and global degradations. We show that our framework significantly improves the robustness of top classification netowrks by a large margin. Our experimental setting uses the recently introduced ModelNet-C database by Ren et al.[24], where we reach SOTA both on unaugmented and on augmented data. Our unaugmented mean Corruption Error (mCE) is 0.64 (current SOTA is 0.86) and 0.50 for augmented data (current SOTA is 0.57). We analyze and explain these remarkable results through diversity an
    
[^83]: SemEval-2023任务11的Lon-ea：软硬标签预测中激活函数的比较。

    Lon-ea at SemEval-2023 Task 11: A Comparison of Activation Functions for Soft and Hard Label Prediction. (arXiv:2303.02468v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.02468](http://arxiv.org/abs/2303.02468)

    本研究研究了在软硬标签预测中，不同激活函数对于深度神经网络模型输出的影响，并引入了一种新的正弦激活函数。

    

    我们研究在学习不同意任务的软硬标签预测中，深度神经网络模型输出层中不同激活函数的影响。在该任务中，目标是通过预测软标签来量化不同意量。为了预测软标签，我们使用基于BERT的预处理器和编码器，并改变输出层中使用的激活函数，同时保持其他参数不变。然后将软标签用于硬标签预测。考虑的激活函数包括sigmoid函数以及添加到模型中的阶跃函数和本文中首次介绍的正弦激活函数。

    We study the influence of different activation functions in the output layer of deep neural network models for soft and hard label prediction in the learning with disagreement task. In this task, the goal is to quantify the amount of disagreement via predicting soft labels. To predict the soft labels, we use BERT-based preprocessors and encoders and vary the activation function used in the output layer, while keeping other parameters constant. The soft labels are then used for the hard label prediction. The activation functions considered are sigmoid as well as a step-function that is added to the model post-training and a sinusoidal activation function, which is introduced for the first time in this paper.
    
[^84]: 利用早期退出进行深度神经网络的分层训练

    Hierarchical Training of Deep Neural Networks Using Early Exiting. (arXiv:2303.02384v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.02384](http://arxiv.org/abs/2303.02384)

    本文提出了一种使用早期退出的分层训练方法，将深度神经网络分为边缘和云工作者，以减少通信成本、训练运行时间和隐私问题。

    

    深度神经网络提供了视觉任务的最先进准确性，但需要大量资源进行训练。因此，它们在远离获取数据的边缘设备上的云服务器上进行训练。这增加了通信成本、运行时间和隐私问题。本研究提出了一种新的深度神经网络分层训练方法，它使用早期退出在边缘和云工作者之间分割架构，以减少通信成本、训练运行时间和隐私问题。

    Deep neural networks provide state-of-the-art accuracy for vision tasks but they require significant resources for training. Thus, they are trained on cloud servers far from the edge devices that acquire the data. This issue increases communication cost, runtime and privacy concerns. In this study, a novel hierarchical training method for deep neural networks is proposed that uses early exits in a divided architecture between edge and cloud workers to reduce the communication cost, training runtime and privacy concerns. The method proposes a brand-new use case for early exits to separate the backward pass of neural networks between the edge and the cloud during the training phase. We address the issues of most available methods that due to the sequential nature of the training phase, cannot train the levels of hierarchy simultaneously or they do it with the cost of compromising privacy. In contrast, our method can use both edge and cloud workers simultaneously, does not share the raw i
    
[^85]: 有缺陷在线演示的保护策略优化

    Guarded Policy Optimization with Imperfect Online Demonstrations. (arXiv:2303.01728v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01728](http://arxiv.org/abs/2303.01728)

    本文放弃了教师表现良好的假设，提出了一种新的算法，能够融合任意教师策略，并通过基于轨迹的价值估计实现高效的探索和安全保障。

    

    “教师-学生框架”（TSF）是一种强化学习设置，其中教师代理通过干预和提供在线演示来监督学生代理的训练。本文放宽了教师表现良好的假设，并开发了一种能够融合任意教师策略的新方法。我们实例化了一个离策略强化学习算法,即教师-学生共享控制（TS2C），它基于基于轨迹的价值估计来融入教师干预。理论分析证明了所提出的TS2C算法实现了高效的探索和可靠的安全保障，而不会过分依赖教师策略的质量。

    The Teacher-Student Framework (TSF) is a reinforcement learning setting where a teacher agent guards the training of a student agent by intervening and providing online demonstrations. Assuming optimal, the teacher policy has the perfect timing and capability to intervene in the learning process of the student agent, providing safety guarantee and exploration guidance. Nevertheless, in many real-world settings it is expensive or even impossible to obtain a well-performing teacher policy. In this work, we relax the assumption of a well-performing teacher and develop a new method that can incorporate arbitrary teacher policies with modest or inferior performance. We instantiate an Off-Policy Reinforcement Learning algorithm, termed Teacher-Student Shared Control (TS2C), which incorporates teacher intervention based on trajectory-based value estimation. Theoretical analysis validates that the proposed TS2C algorithm attains efficient exploration and substantial safety guarantee without be
    
[^86]: FuNVol：使用函数主成分和神经SDE的多资产隐含波动率市场模拟器

    FuNVol: A Multi-Asset Implied Volatility Market Simulator using Functional Principal Components and Neural SDEs. (arXiv:2303.00859v2 [q-fin.CP] UPDATED)

    [http://arxiv.org/abs/2303.00859](http://arxiv.org/abs/2303.00859)

    FuNVol是一个多资产隐含波动率市场模拟器，使用函数主成分和神经SDE生成真实历史价格的IV表面序列，并在无静态套利的表面次流形内产生一致的市场情景。同时，使用模拟表面进行对冲可以生成与实现P＆L一致的损益分布。

    

    我们介绍了一种新的方法，使用函数数据分析和神经随机微分方程，结合概率积分变换惩罚来生成多个资产的隐含波动率表面序列，该方法忠实于历史价格。我们证明了学习IV表面和价格的联合动态产生的市场情景与历史特征一致，并且在没有静态套利的表面次流形内。最后，我们证明使用模拟表面进行对冲会生成与实现P＆L一致的损益分布。

    Here, we introduce a new approach for generating sequences of implied volatility (IV) surfaces across multiple assets that is faithful to historical prices. We do so using a combination of functional data analysis and neural stochastic differential equations (SDEs) combined with a probability integral transform penalty to reduce model misspecification. We demonstrate that learning the joint dynamics of IV surfaces and prices produces market scenarios that are consistent with historical features and lie within the sub-manifold of surfaces that are essentially free of static arbitrage. Finally, we demonstrate that delta hedging using the simulated surfaces generates profit and loss (P&L) distributions that are consistent with realised P&Ls.
    
[^87]: DP-SGD中的训练数据重构界限研究

    Bounding Training Data Reconstruction in DP-SGD. (arXiv:2302.07225v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2302.07225](http://arxiv.org/abs/2302.07225)

    本文在DP-SGD的上下文中研究了如何设置隐私参数以保护免受训练数据重构攻击，并提供了相关攻击的上限和匹配的攻击方式。

    

    差分隐私训练通常被解释为对抗成员推断攻击的保护。最近的研究发现，如果只需要保护免受训练数据重构攻击的威胁，那么私有模型的效用可以改善，因为保护免受这些更有野心的攻击需要更少的噪声。本文在DP-SGD的上下文中进一步研究了这一问题，并提供了针对DP-SGD的任何重建攻击成功的上限以及与我们边界预测相匹配的攻击。这两个结果为设置DP-SGD的隐私参数以保护免受重建攻击开辟了细致的研究方向。

    Differentially private training offers a protection which is usually interpreted as a guarantee against membership inference attacks. By proxy, this guarantee extends to other threats like reconstruction attacks attempting to extract complete training examples. Recent works provide evidence that if one does not need to protect against membership attacks but instead only wants to protect against training data reconstruction, then utility of private models can be improved because less noise is required to protect against these more ambitious attacks. We investigate this further in the context of DP-SGD, a standard algorithm for private deep learning, and provide an upper bound on the success of any reconstruction attack against DP-SGD together with an attack that empirically matches the predictions of our bound. Together, these two results open the door to fine-grained investigations on how to set the privacy parameters of DP-SGD in practice to protect against reconstruction attacks. Fin
    
[^88]: 带有全局收敛保证的随机最优控制问题的策略梯度框架

    A Policy Gradient Framework for Stochastic Optimal Control Problems with Global Convergence Guarantee. (arXiv:2302.05816v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2302.05816](http://arxiv.org/abs/2302.05816)

    该论文提出了一种带有全局收敛保证的随机最优控制问题的策略梯度框架，并引入局部最优控制函数的概念来表征迭代的局部最优性。

    

    我们考虑连续时间中随机最优控制问题的策略梯度方法。具体而言，我们分析控制的梯度流，将其视为策略梯度方法的连续时间极限。我们证明了梯度流的全局收敛性，并在一些正则性假设下建立了收敛速度。分析中的主要创新是引入了局部最优控制函数的概念，用于表征迭代的局部最优性。

    We consider policy gradient methods for stochastic optimal control problem in continuous time. In particular, we analyze the gradient flow for the control, viewed as a continuous time limit of the policy gradient method. We prove the global convergence of the gradient flow and establish a convergence rate under some regularity assumptions. The main novelty in the analysis is the notion of local optimal control function, which is introduced to characterize the local optimality of the iterate.
    
[^89]: 基于自适应采样点云定义的流形上的最温和上升动力学

    Gentlest ascent dynamics on manifolds defined by adaptively sampled point-clouds. (arXiv:2302.04426v2 [math.DS] UPDATED)

    [http://arxiv.org/abs/2302.04426](http://arxiv.org/abs/2302.04426)

    本文介绍了一种将最温和上升动力学扩展到点云定义的流形上的方法，可以在迭代过程中自适应采样，驱动系统从初始构型到达鞍点。

    

    在实际应用中，如分子系统的稀有事件研究中，寻找动力学系统的鞍点是一个重要的问题。最温和上升动力学（GAD）是一种试图在动力学系统中寻找鞍点的算法之一。它通过导出一个新的动力学系统，使原系统的鞍点变为稳定平衡点。GAD最近已经推广到描述等式约束的流形（微分代数方程）上的动力学系统，并以外部形式给出。在本文中，我们提出了一种将GAD扩展到由点云定义的流形上的方法，并采用内在观点进行公式化。这些点云在迭代过程中自适应采样，驱动系统从初始构型（通常在稳定平衡点附近）到达鞍点。我们的方法需要反应物（初始构型），不需要显式的线性化。

    Finding saddle points of dynamical systems is an important problem in practical applications such as the study of rare events of molecular systems. Gentlest ascent dynamics (GAD) is one of a number of algorithms in existence that attempt to find saddle points in dynamical systems. It works by deriving a new dynamical system in which saddle points of the original system become stable equilibria. GAD has been recently generalized to the study of dynamical systems on manifolds (differential algebraic equations) described by equality constraints and given in an extrinsic formulation. In this paper, we present an extension of GAD to manifolds defined by point-clouds, formulated using the intrinsic viewpoint. These point-clouds are adaptively sampled during an iterative process that drives the system from the initial conformation (typically in the neighborhood of a stable equilibrium) to a saddle point. Our method requires the reactant (initial conformation), does not require the explicit co
    
[^90]: 自主驾驶的几何深度学习：通过CommonRoad-Geometric揭示图神经网络的力量

    Geometric Deep Learning for Autonomous Driving: Unlocking the Power of Graph Neural Networks With CommonRoad-Geometric. (arXiv:2302.01259v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01259](http://arxiv.org/abs/2302.01259)

    该论文提出了一种Python框架，能够从交通场景中提取标准化的图形数据集，以利用图神经网络（GNNs）进行各种机器学习应用，例如轨迹预测，为自主驾驶研究提供了基于GNN的平台。

    

    异构图可以为交通提供强大的数据表示，因为它们能够模拟不同数量的交通参与者和基础道路基础设施之间的复杂互动效应。随着图神经网络（GNNs）作为深度学习框架的最新发展，图结构可以有效地用于各种机器学习应用，如轨迹预测。作为首个Python框架，我们提出了一个易于使用且完全可定制的数据处理流程，从交通场景中提取标准化图形数据集。提供了一个基于GNN的自主驾驶研究平台，它提高了不同方法之间的比较性，并允许研究人员专注于模型实现而不是数据集的策划。

    Heterogeneous graphs offer powerful data representations for traffic, given their ability to model the complex interaction effects among a varying number of traffic participants and the underlying road infrastructure. With the recent advent of graph neural networks (GNNs) as the accompanying deep learning framework, the graph structure can be efficiently leveraged for various machine learning applications such as trajectory prediction. As a first of its kind, our proposed Python framework offers an easy-to-use and fully customizable data processing pipeline to extract standardized graph datasets from traffic scenarios. Providing a platform for GNN-based autonomous driving research, it improves comparability between approaches and allows researchers to focus on model implementation instead of dataset curation.
    
[^91]: 分析语言模型中个人识别信息泄露的情况

    Analyzing Leakage of Personally Identifiable Information in Language Models. (arXiv:2302.00539v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00539](http://arxiv.org/abs/2302.00539)

    本研究针对语言模型中泄漏个人身份信息的风险进行了严格的定义，并通过黑盒提取、推断和重建攻击进行了实证评估。

    

    语言模型已经被证明会通过句子级成员推断和重构攻击泄漏训练数据的信息。然而，我们对于语言模型泄露个人身份信息的风险了解不足。目前已经假设数据集整理技术（如数据清洗）足以防止个人身份信息泄露，但这一假设是错误的。实际上，数据清洗技术可以减少Pll泄露的风险，但并不能完全绝对地防止泄露。本文中，我们引入了三种类型的个人身份信息泄漏的严格基于博弈的定义，通过API访问语言模型进行黑盒提取、推断和重建攻击，并对其进行实证评估。

    Language Models (LMs) have been shown to leak information about training data through sentence-level membership inference and reconstruction attacks. Understanding the risk of LMs leaking Personally Identifiable Information (PII) has received less attention, which can be attributed to the false assumption that dataset curation techniques such as scrubbing are sufficient to prevent PII leakage. Scrubbing techniques reduce but do not prevent the risk of PII leakage: in practice scrubbing is imperfect and must balance the trade-off between minimizing disclosure and preserving the utility of the dataset. On the other hand, it is unclear to which extent algorithmic defenses such as differential privacy, designed to guarantee sentence- or user-level privacy, prevent PII disclosure. In this work, we introduce rigorous game-based definitions for three types of PII leakage via black-box extraction, inference, and reconstruction attacks with only API access to an LM. We empirically evaluate the 
    
[^92]: Bagging提供无偏差稳定性。

    Bagging Provides Assumption-free Stability. (arXiv:2301.12600v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.12600](http://arxiv.org/abs/2301.12600)

    本文证明了Bagging技术可提供无偏差稳定性，适用于各种数据分布和算法，具有良好的实证效果。

    

    Bagging是稳定机器学习模型的一个重要技术。在本文中，我们针对任何模型的稳定性推导了一个有限样本保证。我们的结果不对数据分布、基本算法的属性或协变量的维数进行任何假设。我们的保证适用于多种变体的Bagging，并且是最优的常数。实证结果验证了我们的发现，表明Bagging成功稳定了即使是高度不稳定的基本算法。

    Bagging is an important technique for stabilizing machine learning models. In this paper, we derive a finite-sample guarantee on the stability of bagging for any model. Our result places no assumptions on the distribution of the data, on the properties of the base algorithm, or on the dimensionality of the covariates. Our guarantee applies to many variants of bagging and is optimal up to a constant. Empirical results validate our findings, showing that bagging successfully stabilizes even highly unstable base algorithms.
    
[^93]: 具有自动对称性检测的哈密顿神经网络

    Hamiltonian Neural Networks with Automatic Symmetry Detection. (arXiv:2301.07928v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.07928](http://arxiv.org/abs/2301.07928)

    本研究提出了一种哈密顿神经网络模型，采用李代数框架检测和嵌入对称性，以保留辛系统结构。考虑摆在小车上和天体动力学中的二体问题作为案例，结果表明这种方法是有效的。

    

    最近，引入了哈密顿神经网络（HNN），以在学习哈密顿系统的动力学方程时纳入先前的物理知识。通过这种数据驱动的建模方法，保留了辛系统结构。然而，保留对称性需要额外关注。在这项研究中，我们用李代数框架增强了HNN，以检测和嵌入神经网络中的对称性。该方法允许同时学习对称性群的作用和系统的总能量。以摆在小车上和天体动力学中的二体问题为例说明了这种方法的有效性。

    Recently, Hamiltonian neural networks (HNN) have been introduced to incorporate prior physical knowledge when learning the dynamical equations of Hamiltonian systems. Hereby, the symplectic system structure is preserved despite the data-driven modeling approach. However, preserving symmetries requires additional attention. In this research, we enhance HNN with a Lie algebra framework to detect and embed symmetries in the neural network. This approach allows to simultaneously learn the symmetry group action and the total energy of the system. As illustrating examples, a pendulum on a cart and a two-body problem from astrodynamics are considered.
    
[^94]: 有限元灵感启发的超图神经网络：在流体动力学模拟中的应用。

    A Finite Element-Inspired Hypergraph Neural Network: Application to Fluid Dynamics Simulations. (arXiv:2212.14545v2 [physics.flu-dyn] UPDATED)

    [http://arxiv.org/abs/2212.14545](http://arxiv.org/abs/2212.14545)

    本文提出了一种有限元灵感启发的超图神经网络（FEIH($\phi$)-GNN），能够模拟局部刚度矩阵的计算过程，具备旋转等变性，并能准确预测流体动力学模拟中的时间滚动。

    

    深度学习研究中越来越多的关注点是图神经网络在基于网格的连续介质力学模拟中的应用。大多数这些学习框架操作的是每条边连接两个节点的图。受到有限元方法中数据联通性的启发，我们提出一种通过元素而不是边缘连接节点来构建超图的方法。在这样的节点-元素超图上定义了一种超图消息传递网络，模拟局部刚度矩阵的计算过程。我们将这种方法称为有限元灵感启发的超图神经网络，简称FEIH($\phi$)-GNN。我们还为该网络构建了旋转等变性，并探索其对于建模非定常流体流动系统的能力。该网络架构在两个常见的基准问题上，即圆柱周围的流体流动和翼型结构中展示了稳定和准确的时间滚动预测。

    An emerging trend in deep learning research focuses on the applications of graph neural networks (GNNs) for mesh-based continuum mechanics simulations. Most of these learning frameworks operate on graphs wherein each edge connects two nodes. Inspired by the data connectivity in the finite element method, we present a method to construct a hypergraph by connecting the nodes by elements rather than edges. A hypergraph message-passing network is defined on such a node-element hypergraph that mimics the calculation process of local stiffness matrices. We term this method a finite element-inspired hypergraph neural network, in short FEIH($\phi$)-GNN. We further equip the proposed network with rotation equivariance, and explore its capability for modeling unsteady fluid flow systems. The effectiveness of the network is demonstrated on two common benchmark problems, namely the fluid flow around a circular cylinder and airfoil configurations. Stabilized and accurate temporal roll-out predictio
    
[^95]: 在特征空间中学习多模态数据增强

    Learning Multimodal Data Augmentation in Feature Space. (arXiv:2212.14453v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.14453](http://arxiv.org/abs/2212.14453)

    本文介绍了一种名为LeMDA的易于使用的方法，它在特征空间中自动学习联合增强多模态数据，提高了多模态学习算法的性能。

    

    能够联合学习多个模态（如文本、音频和视觉数据）是智能系统的一个核心特征。尽管设计神经网络来利用多模态数据取得了令人鼓舞的进展，但数据增强的巨大成功仍然局限于单模态任务，如图像分类。确实，在保留数据的整体语义结构的同时增强每个模态是特别困难的；例如，在应用了标准增强方法（例如翻译）之后，标题可能不再是图像的良好描述。此外，仍然很难指定不针对特定模态的合理变换。本文介绍了一种易于使用的方法，名为LeMDA（Learning Multimodal Data Augmentation），它在特征空间中自动学习联合增强多模态数据，而不限制模态的身份或模态之间的关系。我们在标准基准测试上展示了我们方法的有效性，证明LeMDA在多个领域（包括图像字幕和语音识别）中提高了多模态学习算法的性能。

    The ability to jointly learn from multiple modalities, such as text, audio, and visual data, is a defining feature of intelligent systems. While there have been promising advances in designing neural networks to harness multimodal data, the enormous success of data augmentation currently remains limited to single-modality tasks like image classification. Indeed, it is particularly difficult to augment each modality while preserving the overall semantic structure of the data; for example, a caption may no longer be a good description of an image after standard augmentations have been applied, such as translation. Moreover, it is challenging to specify reasonable transformations that are not tailored to a particular modality. In this paper, we introduce LeMDA, Learning Multimodal Data Augmentation, an easy-to-use method that automatically learns to jointly augment multimodal data in feature space, with no constraints on the identities of the modalities or the relationship between modalit
    
[^96]: CC-FedAvg：计算定制的联邦平均算法

    CC-FedAvg: Computationally Customized Federated Averaging. (arXiv:2212.13679v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.13679](http://arxiv.org/abs/2212.13679)

    本论文提出了一个称为CC-FedAvg的计算定制的联邦平均算法，可让参与者根据其计算预算决定在每轮中是否执行传统的本地训练或模型估算。实验结果表明，CC-FedAvg能够显著提高模型性能并降低通信成本。

    

    联邦学习是一种新兴的模型训练方式，通过分布在众多物联网设备上的数据进行模型训练。它在本质上假设参与者的计算能力相同，但实际上，由于不同的能源预算或并行执行的任务不同，参与者计算资源存在着差异。缺乏计算预算的参与者必须适当规划其受限计算资源的使用，否则他们将无法完成整个训练过程，导致模型性能下降。为了解决这个问题，我们提出了一种估算本地模型而无需计算密集迭代的策略。基于此，我们提出了计算定制的联邦平均算法(CC-FedAvg)，允许参与者根据其当前的计算预算，在每个轮次中决定是执行传统的本地训练还是模型估算。理论分析和实验结果均表明，与传统的联邦平均算法相比，CC-FedAvg能显著提高模型性能并降低通信成本。

    Federated learning (FL) is an emerging paradigm to train model with distributed data from numerous Internet of Things (IoT) devices. It inherently assumes a uniform capacity among participants. However, due to different conditions such as differing energy budgets or executing parallel unrelated tasks, participants have diverse computational resources in practice. Participants with insufficient computation budgets must plan for the use of restricted computational resources appropriately, otherwise they would be unable to complete the entire training procedure, resulting in model performance decline. To address the this issue, we propose a strategy for estimating local models without computationally intensive iterations. Based on it, we propose Computationally Customized Federated Averaging (CC-FedAvg), which allows participants to determine whether to perform traditional local training or model estimation in each round based on their current computational budgets. Both theoretical analy
    
[^97]: MN-DS：新闻文章层次分类的多标签数据集

    MN-DS: A Multilabeled News Dataset for News Articles Hierarchical Classification. (arXiv:2212.12061v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.12061](http://arxiv.org/abs/2212.12061)

    本文介绍了一个包含10,917篇新闻文章的多标签数据集，可用于训练机器学习模型自动按主题对新闻文章进行分类，对新闻结构、分类和预测未来事件的研究人员非常有帮助。

    

    本文介绍了一个数据集，其中包含10,917篇新闻文章，涵盖了从2019年1月1日到2019年12月31日的层次新闻分类。我们根据17个一级类别和109个二级类别的层次分类手动标记了这些文章。该数据集可用于训练机器学习模型，以自动按主题分类新闻文章。该数据集对于从事新闻结构、分类和根据发布的新闻预测未来事件的研究人员非常有帮助。

    This article presents a dataset of 10,917 news articles with hierarchical news categories collected between January 1st 2019, and December 31st 2019. We manually labelled the articles based on a hierarchical taxonomy with 17 first-level and 109 second-level categories. This dataset can be used to train machine learning models for automatically classifying news articles by topic. This dataset can be helpful for researchers working on news structuring, classification, and predicting future events based on released news.
    
[^98]: PyGFI：分析和增强图神经网络对硬件错误的稳健性

    PyGFI: Analyzing and Enhancing Robustness of Graph Neural Networks Against Hardware Errors. (arXiv:2212.03475v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.03475](http://arxiv.org/abs/2212.03475)

    本文针对图神经网络对硬件错误的鲁棒性进行了研究，提出了PyGFI框架，利用故障注入生成合成故障来训练更加鲁棒的GNN模型，可以容忍各种类型的硬件故障，有效提高GNN模型的准确性和鲁棒性，并可以轻松集成到现有的GNN训练流程中。

    

    图神经网络（GNN）最近成为一种有前途的学习范式，用于学习图结构数据，并已在推荐系统、社交网络和电子设计自动化（EDA）等各个领域广泛成功。与其他深度学习（DL）方法一样，GNN被部署在先进的现代硬件系统以及专用加速器上。然而，尽管GNN的普及和将其引入硬件的最近努力，GNN的容错性和韧性通常被忽视。本文受DL方法内在的算法韧性启发，首次对GNN韧性进行了大规模和实证研究，旨在理解硬件故障与GNN准确性之间的关系。通过在PyTorch之上开发定制的故障注入工具，我们对各种GNN模型和应用数据集进行了广泛的故障注入实验。我们观察到，与传统基于图像的DL模型相比，GNN的容错能力显著较低，并且常见的硬件故障会显著降低GNN的准确性。基于我们的发现，我们提出了PyGFI，这是一种用于增强GNN对硬件错误鲁棒性的新框架。PyGFI利用故障注入在训练数据中生成合成故障，从而训练出更加鲁棒的GNN模型，可以容忍各种类型的硬件故障。在实际硬件上的实验结果表明，PyGFI可以有效提高GNN模型的准确性和鲁棒性，并可以轻松集成到现有的GNN训练流程中。

    Graph neural networks (GNNs) have recently emerged as a promising learning paradigm in learning graph-structured data and have demonstrated wide success across various domains such as recommendation systems, social networks, and electronic design automation (EDA). Like other deep learning (DL) methods, GNNs are being deployed in sophisticated modern hardware systems, as well as dedicated accelerators. However, despite the popularity of GNNs and the recent efforts of bringing GNNs to hardware, the fault tolerance and resilience of GNNs have generally been overlooked. Inspired by the inherent algorithmic resilience of DL methods, this paper conducts, for the first time, a large-scale and empirical study of GNN resilience, aiming to understand the relationship between hardware faults and GNN accuracy. By developing a customized fault injection tool on top of PyTorch, we perform extensive fault injection experiments on various GNN models and application datasets. We observe that the error 
    
[^99]: Edge Impulse: 面向微型机器学习的MLOps平台。

    Edge Impulse: An MLOps Platform for Tiny Machine Learning. (arXiv:2212.03332v2 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2212.03332](http://arxiv.org/abs/2212.03332)

    Edge Impulse是一个面向微型机器学习的MLOps平台，旨在为开发人员提供嵌入式和边缘ML系统的软硬件优化支持，解决TinyML的可移植性和优化问题。

    

    Edge Impulse是一个基于云的机器学习运营(MLOps)平台，用于开发可以部署到各种硬件目标的嵌入式和边缘ML(微型ML)系统。当前的TinyML工作流程存在着软件堆栈碎片化和异构部署硬件等问题，使得ML模型优化困难且不具备可移植性。我们提出了Edge Impulse，这是一个实用的MLOps平台，可实现大规模开发TinyML系统。Edge Impulse通过支持各种软件和硬件优化来解决这些挑战，并创建可扩展且可移植的软件堆栈，以适应各种嵌入式系统。 截至2022年10月，Edge Impulse托管了来自50,953名开发人员的118,185个项目。

    Edge Impulse is a cloud-based machine learning operations (MLOps) platform for developing embedded and edge ML (TinyML) systems that can be deployed to a wide range of hardware targets. Current TinyML workflows are plagued by fragmented software stacks and heterogeneous deployment hardware, making ML model optimizations difficult and unportable. We present Edge Impulse, a practical MLOps platform for developing TinyML systems at scale. Edge Impulse addresses these challenges and streamlines the TinyML design cycle by supporting various software and hardware optimizations to create an extensible and portable software stack for a multitude of embedded systems. As of Oct. 2022, Edge Impulse hosts 118,185 projects from 50,953 developers.
    
[^100]: 神经符号模型的自然约束担保一致性

    Guaranteed Conformance of Neurosymbolic Models to Natural Constraints. (arXiv:2212.01346v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.01346](http://arxiv.org/abs/2212.01346)

    该论文提出了一种能够担保数据驱动模型符合自然科学已有知识并最优逼近系统模型的方法。

    

    深度神经网络已经成为大部分机器人和控制应用的主要模型，特别是作为动态系统模型。这类数据驱动模型又可以用于设计和验证自主系统。在医疗系统建模方面尤其有用，因为数据能够被用于个体化治疗。在安全关键的应用中，数据驱动模型对来自自然科学的已有知识的符合性显得尤为重要。这些知识通常是可用的，或者可以被浓缩成（可能是黑盒的）模型。例如，F1赛车应符合牛顿定律（这被编码在一个单轮模型中）。鉴于这一点，我们考虑以下问题——给定一个模型M和一个状态转移数据集，我们希望在距离M的范围内最好地近似系统模型。我们提出一种方法来担保这种一致性。我们的第一步是将数据集浓缩成几个代表性的样例。

    Deep neural networks have emerged as the workhorse for a large section of robotics and control applications, especially as models for dynamical systems. Such data-driven models are in turn used for designing and verifying autonomous systems. They are particularly useful in modeling medical systems where data can be leveraged to individualize treatment. In safety-critical applications, it is important that the data-driven model is conformant to established knowledge from the natural sciences. Such knowledge is often available or can often be distilled into a (possibly black-box) model. For instance, an F1 racing car should conform to Newton's laws (which are encoded within a unicycle model). In this light, we consider the following problem - given a model $M$ and a state transition dataset, we wish to best approximate the system model while being a bounded distance away from $M$. We propose a method to guarantee this conformance. Our first step is to distill the dataset into a few repre
    
[^101]: 平衡探索和开发权衡的动态稀疏训练

    Dynamic Sparse Training via Balancing the Exploration-Exploitation Trade-off. (arXiv:2211.16667v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.16667](http://arxiv.org/abs/2211.16667)

    本文提出了一种动态稀疏训练方法，采用开发和探索收购函数来平衡探索和开发之间的权衡，从而摆脱了局部最优和鞍点问题，实验结果表明，该方法在精度和收敛速度方面超过了现有的稀疏训练方法。

    

    深度神经网络的超参数化已经在许多应用中表现出高预测准确性。虽然有效，但大量参数阻碍了它在资源受限设备上的普及，并对环境产生不良影响。使用固定数量的非零权重来进行稀疏训练可以显着减轻训练成本，减小模型大小，但现有的稀疏训练方法主要使用基于随机或贪婪的减少和增长策略，导致局部最小值和低精度问题。在本文中，我们将动态稀疏训练视为稀疏连通性搜索问题，并设计一个开发和探索收购函数来摆脱局部最优和鞍点。我们进一步设计了一种收购功能，并提供了所提方法的理论保证并阐明其收敛性质。实验结果表明，我们的程序获得了多种深度神经网络上与密集模型可比的精度（高达98％的稀疏度），并且在精度和收敛速度方面超过了现有的稀疏训练方法。

    Over-parameterization of deep neural networks (DNNs) has shown high prediction accuracy for many applications. Although effective, the large number of parameters hinders its popularity on resource-limited devices and has an outsize environmental impact. Sparse training (using a fixed number of nonzero weights in each iteration) could significantly mitigate the training costs by reducing the model size. However, existing sparse training methods mainly use either random-based or greedy-based drop-and-grow strategies, resulting in local minimal and low accuracy. In this work, we consider the dynamic sparse training as a sparse connectivity search problem and design an exploitation and exploration acquisition function to escape from local optima and saddle points. We further design an acquisition function and provide the theoretical guarantees for the proposed method and clarify its convergence property. Experimental results show that sparse models (up to 98\% sparsity) obtained by our pro
    
[^102]: 利用积流形进行潜在图推断

    Latent Graph Inference using Product Manifolds. (arXiv:2211.16199v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.16199](http://arxiv.org/abs/2211.16199)

    本文提出了一种利用积流形进行推断的潜在图学习方法，以动态学习问题内在的图结构。通过使用Riemannian几何学和生成更复杂的嵌入空间，可以提高系统性能并产生更丰富的表示。

    

    图神经网络通常依赖于图拓扑结构对网络的可用性以及对下游任务的最优性。而潜在图推断允许模型动态学习问题内在的图结构，这些数据的连通性模式可能无法直接访问。本文将离散可微图模块（dDGM）推广到潜在图学习。原始的dDGM架构使用欧几里得平面来编码潜在特征，基于此生成潜在图。通过将黎曼几何引入模型并生成更复杂的嵌入空间，我们可以提高潜在图推断系统的性能。特别是，我们提出了一种计算上可行的方法来产生常曲率模型空间的积流形，可以编码不同结构的潜在特征。映射到推断出的积流形上的潜在表示用于计算更丰富的表示。

    Graph Neural Networks usually rely on the assumption that the graph topology is available to the network as well as optimal for the downstream task. Latent graph inference allows models to dynamically learn the intrinsic graph structure of problems where the connectivity patterns of data may not be directly accessible. In this work, we generalize the discrete Differentiable Graph Module (dDGM) for latent graph learning. The original dDGM architecture used the Euclidean plane to encode latent features based on which the latent graphs were generated. By incorporating Riemannian geometry into the model and generating more complex embedding spaces, we can improve the performance of the latent graph inference system. In particular, we propose a computationally tractable approach to produce product manifolds of constant curvature model spaces that can encode latent features of varying structure. The latent representations mapped onto the inferred product manifold are used to compute richer s
    
[^103]: 建立鲁棒性强的神经网络以克服标签噪声

    Establishment of Neural Networks Robust to Label Noise. (arXiv:2211.15279v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15279](http://arxiv.org/abs/2211.15279)

    本文介绍了建立鲁棒性强的神经网络以克服标签噪声的研究，通过创建转换矩阵估计器、研究两个卷积神经网络分类器的标签噪声鲁棒性等，揭示了两个FashionMINIST数据集的鲁棒性，旨在解决深度学习模型训练中标签噪声带来的难题。

    

    标签噪声是深度学习模型训练中的重要障碍，尤其对于深度神经网络来说影响尤为明显，因为它们很容易记忆嘈杂的标签。在本文中，我们对相关标签噪声方法的基本概念进行了研究。我们创建了一个转换矩阵估计器，并展示了它对于实际转换矩阵的有效性。此外，我们还研究了两个卷积神经网络分类器（LeNet和AlexNet设计）的标签噪声鲁棒性。两个FashionMINIST数据集都展现了这两个模型的鲁棒性。但是，由于时间和计算资源限制，我们不能有效地展示转换矩阵噪声校正对鲁棒性增强的影响。因此，需要建立能够克服标签噪声挑战的神经网络。

    Label noise is a significant obstacle in deep learning model training. It can have a considerable impact on the performance of image classification models, particularly deep neural networks, which are especially susceptible because they have a strong propensity to memorise noisy labels. In this paper, we have examined the fundamental concept underlying related label noise approaches. A transition matrix estimator has been created, and its effectiveness against the actual transition matrix has been demonstrated. In addition, we examined the label noise robustness of two convolutional neural network classifiers with LeNet and AlexNet designs. The two FashionMINIST datasets have revealed the robustness of both models. We are not efficiently able to demonstrate the influence of the transition matrix noise correction on robustness enhancements due to our inability to correctly tune the complex convolutional neural network model due to time and computing resource constraints. There is a need
    
[^104]: 连续情景控制

    Continuous Episodic Control. (arXiv:2211.15183v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15183](http://arxiv.org/abs/2211.15183)

    CEC是一种新颖的非参数情景记忆算法，用于连续性行动空间问题中的序列决策制定，其在几个稀疏奖励连续控制环境中比最先进的RL和记忆增强RL算法学习更快，是学习连续控制任务的快速方法。

    

    非参数情景记忆可以用于快速锁定强化学习任务中高奖励的经验。与参数深度强化学习方法相比，在参数需要缓慢地反向传递奖励信号的方法中，这些方法只需要发现一次解决方案，然后就可以反复解决任务。然而，情景控制解决方案存储在离散表中，这种方法迄今只应用于离散行动空间问题。因此，本文介绍了连续情景控制（CEC），这是一种新颖的非参数情景记忆算法，可用于连续性行动空间问题中的序列决策制定。在几个稀疏奖励连续控制环境中的结果表明，我们提出的方法比最先进的无模型RL和记忆增强RL算法学习更快，同时保持良好的长期性能。简而言之，CEC可以是学习连续控制任务的快速方法。

    Non-parametric episodic memory can be used to quickly latch onto high-rewarded experience in reinforcement learning tasks. In contrast to parametric deep reinforcement learning approaches in which reward signals need to be back-propagated slowly, these methods only need to discover the solution once, and may then repeatedly solve the task. However, episodic control solutions are stored in discrete tables, and this approach has so far only been applied to discrete action space problems. Therefore, this paper introduces Continuous Episodic Control (CEC), a novel non-parametric episodic memory algorithm for sequential decision making in problems with a continuous action space. Results on several sparse-reward continuous control environments show that our proposed method learns faster than state-of-the-art model-free RL and memory-augmented RL algorithms, while maintaining good long-run performance as well. In short, CEC can be a fast approach for learning in continuous control tasks.
    
[^105]: 最优分类森林的数学规划方法

    A Mathematical Programming Approach to Optimal Classification Forests. (arXiv:2211.10502v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2211.10502](http://arxiv.org/abs/2211.10502)

    提出了一种新的分类器——最优分类森林，通过数学优化方法构建最优的决策树集成实现高预测准确性，并且比随机森林使用数量级更少的树。同时提供了三个真实世界的案例研究。

    

    本文介绍了一种新的分类器族群——最优分类森林，利用最优的决策树集成来得出准确且可解释的分类器。我们提出了一种新的基于数学优化的方法，同时构建给定数量的树，每个树为特征空间中的观测值提供一个预测类别。通过在森林中的每棵树中选出被最频繁预测的类别来求得分类规则。我们提供了一个混合整数线性规划公式来解决这个问题。通过我们的计算实验结果，我们认为我们提出的方法与最先进的基于树的分类方法相比，具有同等或更高的性能，并且可以实现高预测准确性，例如，比随机森林使用数量级更少的树。我们还展示了三个真实世界的案例研究。

    In this paper, we introduce Optimal Classification Forests, a new family of classifiers that takes advantage of an optimal ensemble of decision trees to derive accurate and interpretable classifiers. We propose a novel mathematical optimization-based methodology in which a given number of trees are simultaneously constructed, each of them providing a predicted class for the observations in the feature space. The classification rule is derived by assigning to each observation its most frequently predicted class among the trees in the forest. We provide a mixed integer linear programming formulation for the problem. We report the results of our computational experiments, from which we conclude that our proposed method has equal or superior performance compared with state-of-the-art tree-based classification methods. More importantly, it achieves high prediction accuracy with, for example, orders of magnitude fewer trees than random forests. We also present three real-world case studies s
    
[^106]: Let's Enhance：一种用于极端文本图像去模糊的深度学习方法

    Let's Enhance: A Deep Learning Approach to Extreme Deblurring of Text Images. (arXiv:2211.10103v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.10103](http://arxiv.org/abs/2211.10103)

    本文提出了一种用于极端文本图像去模糊的深度学习方法，利用合成数据进行增强和预训练。该方法通过数据驱动的前向模型估计和U-Net实现去模糊，该方法在2021年赫尔辛基去模糊挑战赛中获胜。

    

    本文提出了一种新颖的基于深度学习的图像去模糊逆问题处理流程，利用合成数据进行增强和预训练。作者的成果基于他们在2021年赫尔辛基去模糊挑战赛中的获胜结果，该挑战赛旨在探索最先进的去模糊算法在真实世界数据环境下的极限。挑战任务是去模糊随机文本的失焦图像，并通过光学字符识别评分函数的最大化来优化下游任务。解决方案的关键步骤是数据驱动的前向模型估计，用于描述模糊过程。这使得产生一系列合成数据，动态生成地面实况图像和模糊图像对，用于增强挑战数据。实际去模糊的管道包括径向透镜失真的近似反演（由估计的前向模型确定）和U-Net。

    This work presents a novel deep-learning-based pipeline for the inverse problem of image deblurring, leveraging augmentation and pre-training with synthetic data. Our results build on our winning submission to the recent Helsinki Deblur Challenge 2021, whose goal was to explore the limits of state-of-the-art deblurring algorithms in a real-world data setting. The task of the challenge was to deblur out-of-focus images of random text, thereby in a downstream task, maximizing an optical-character-recognition-based score function. A key step of our solution is the data-driven estimation of the physical forward model describing the blur process. This enables a stream of synthetic data, generating pairs of ground-truth and blurry images on-the-fly, which is used for an extensive augmentation of the small amount of challenge data provided. The actual deblurring pipeline consists of an approximate inversion of the radial lens distortion (determined by the estimated forward model) and a U-Net 
    
[^107]: 基于去中心化的联邦学习: 基础、现状、框架、趋势和挑战 (arXiv:2211.08413v2 [cs.LG] UPDATED)

    Decentralized Federated Learning: Fundamentals, State-of-the-art, Frameworks, Trends, and Challenges. (arXiv:2211.08413v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.08413](http://arxiv.org/abs/2211.08413)

    本文提出去中心化联邦学习（DFL）来解决传统中心化FL（CFL）模型中的问题，主要研究DFL与CFL的差异、DFL的基础理论、DFL框架的设计与评估以及DFL在实际应用中的场景。

    

    在过去的十年中，联邦学习（FL）已经成为在不共享敏感数据的情况下训练协作模型的一种重要方法。自问世以来，中心化FL（CFL）一直是文献中最常见的方法，其中一个中心化实体创建全局模型。然而，中心化方法会导致瓶颈增加、系统故障风险增高，影响负责创建全局模型的实体的可信度。去中心化联邦学习（DFL）应运而生，通过推广去中心化模型聚合并最小化对中心化架构的依赖，解决了这些问题。但是，尽管在DFL方面有所努力，文献还没有研究(i)DFL和CFL之间的主要差异;(ii)分析DFL框架以创建和评估新解决方案;(iii)回顾使用DFL的应用场景。因此，本文在联邦架构、安全性、通信等方面识别并分析了DFL的主要基础。

    In the last decade, Federated Learning (FL) has gained relevance in training collaborative models without sharing sensitive data. Since its birth, Centralized FL (CFL) has been the most common approach in the literature, where a central entity creates a global model. However, a centralized approach leads to increased latency due to bottlenecks, heightened vulnerability to system failures, and trustworthiness concerns affecting the entity responsible for the global model creation. Decentralized Federated Learning (DFL) emerged to address these concerns by promoting decentralized model aggregation and minimizing reliance on centralized architectures. However, despite the work done in DFL, the literature has not (i) studied the main aspects differentiating DFL and CFL; (ii) analyzed DFL frameworks to create and evaluate new solutions; and (iii) reviewed application scenarios using DFL. Thus, this article identifies and analyzes the main fundamentals of DFL in terms of federation architect
    
[^108]: 基于统计检验的MMD-B-Fair：学习公平的表示

    MMD-B-Fair: Learning Fair Representations with Statistical Testing. (arXiv:2211.07907v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.07907](http://arxiv.org/abs/2211.07907)

    提出了一种基于统计检验的 MMD-B-Fair 方法，用于学习公平的数据表示，并在各种数据集上得到了验证。

    

    我们提出了一种通过核双样本测试学习数据公平表示的方法MMD-B-Fair。我们找到了数据的神经特征，其中最大平均偏差（MMD）测试无法区分不同敏感组的表示，同时保留有关目标属性的信息。我们的方法利用块测试方案的简单渐近性能够有效地找到公平表示，而不需要使用现有公平表示学习方法中广泛使用的复杂对抗性优化或生成建模方案。我们在各种数据集上评估了我们的方法，显示其能够“隐藏”有关敏感属性的信息，并在下游传输任务中的有效性。

    We introduce a method, MMD-B-Fair, to learn fair representations of data via kernel two-sample testing. We find neural features of our data where a maximum mean discrepancy (MMD) test cannot distinguish between representations of different sensitive groups, while preserving information about the target attributes. Minimizing the power of an MMD test is more difficult than maximizing it (as done in previous work), because the test threshold's complex behavior cannot be simply ignored. Our method exploits the simple asymptotics of block testing schemes to efficiently find fair representations without requiring complex adversarial optimization or generative modelling schemes widely used by existing work on fair representation learning. We evaluate our approach on various datasets, showing its ability to ``hide'' information about sensitive attributes, and its effectiveness in downstream transfer tasks.
    
[^109]: 数据泄露和可迁移性对问题和提交链接预测模型的实证研究

    An Empirical Study on Data Leakage and Generalizability of Link Prediction Models for Issues and Commits. (arXiv:2211.00381v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2211.00381](http://arxiv.org/abs/2211.00381)

    本文提出LinkFormer，一种基于Transformer 架构的预测模型，用于分析软件问题和提交之间的依赖关系。LinkFormer 可以减少数据泄露和加强模型的可迁移性，提高预测准确性，应用于自动链接恢复技术领域。

    

    为加强文档和维护实践，开发者传统上手动建立相关软件工件之间的链接。 实证研究发现，开发者常常忽视这种做法，导致重要信息丢失。 为解决这个问题，提出了自动链接恢复技术，然而，这些方法主要集中在提高在随机拆分数据集上的预测准确度，而对数据泄露和预测模型的可迁移性关注有限。我们的方法旨在解决这些限制。我们不仅保留和提高现有预测的准确性，而且增强了它们与现实环境的匹配度和可迁移性。

    To enhance documentation and maintenance practices, developers conventionally establish links between related software artifacts manually. Empirical research has revealed that developers frequently overlook this practice, resulting in significant information loss. To address this issue, automatic link recovery techniques have been proposed. However, these approaches primarily focused on improving prediction accuracy on randomly-split datasets, with limited attention given to the impact of data leakage and the generalizability of the predictive models. LinkFormer seeks to address these limitations. Our approach not only preserves and improves the accuracy of existing predictions but also enhances their alignment with real-world settings and their generalizability. First, to better utilize contextual information for prediction, we employ the Transformer architecture and fine-tune multiple pre-trained models on both textual and metadata information of issues and commits. Next, to gauge th
    
[^110]: GammaE: 基于Gamma分布的知识图谱逻辑查询嵌入

    GammaE: Gamma Embeddings for Logical Queries on Knowledge Graphs. (arXiv:2210.15578v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.15578](http://arxiv.org/abs/2210.15578)

    GammaE是一种新颖的概率嵌入模型，利用了Gamma分布的线性特性和强边界支持来捕捉实体和查询的更多特征，解决了知识图谱中否定和联合算符的建模问题，并在知识图谱上回答不同类型的FOL查询。

    

    将知识图谱（KG）进行嵌入以进行多跳逻辑推理是一个具有挑战性的问题，因为许多KG具有庞大而复杂的结构。最近，许多有前途的工作将实体和查询投影到几何空间中以有效地找到答案。但是，建模否定和联合运算符仍然具有挑战性。否定运算符没有严格的边界，这会生成重叠的嵌入并导致获得模糊的答案。另一个限制是并集运算符是非闭合的，这削弱了模型处理一系列并集运算符的能力。为了解决这些问题，我们提出了一个新颖的概率嵌入模型，即Gamma Embeddings（GammaE），用于编码实体和查询以回答KG上不同类型的FOL查询。我们利用了Gamma分布的线性特性和强边界支持来捕捉实体和查询的更多特征，从而极大地减少了模型的不确定性。此外，GammaE实现了统一的框架来处理否定和联合运算符。

    Embedding knowledge graphs (KGs) for multi-hop logical reasoning is a challenging problem due to massive and complicated structures in many KGs. Recently, many promising works projected entities and queries into a geometric space to efficiently find answers. However, it remains challenging to model the negation and union operator. The negation operator has no strict boundaries, which generates overlapped embeddings and leads to obtaining ambiguous answers. An additional limitation is that the union operator is non-closure, which undermines the model to handle a series of union operators. To address these problems, we propose a novel probabilistic embedding model, namely Gamma Embeddings (GammaE), for encoding entities and queries to answer different types of FOL queries on KGs. We utilize the linear property and strong boundary support of the Gamma distribution to capture more features of entities and queries, which dramatically reduces model uncertainty. Furthermore, GammaE implements
    
[^111]: 带资源约束的高度移动连接车辆下的车联网边缘联邦学习

    Resource Constrained Vehicular Edge Federated Learning with Highly Mobile Connected Vehicles. (arXiv:2210.15496v3 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2210.15496](http://arxiv.org/abs/2210.15496)

    本文提出了一种在高度移动的连接车辆下，边缘服务器利用局部数据集和处理单元进行训练的边缘联邦学习解决方案，可以通过权重组合和子集选择来聚合模型参数并最大化成功接收本地训练模型的概率。

    

    本文提出了一种车联网边缘联邦学习（VEFL）解决方案，其中边缘服务器利用高度移动的连接车辆（CV）的本地数据集和中央处理单元（CPU）来训练全局模型。收敛分析表明，VEFL训练损失取决于成功接收CV通过间歇性车辆到基础设施（V2I）无线链路传输的训练模型。由于高度移动性，在全设备参与情况（FDPC）下，边缘服务器根据CV数据集大小和逗留时间的加权组合聚合客户端模型参数，而在部分设备参与情况（PDPC）下选择CV的子集。然后，我们设计了在延迟、能量和成本约束条件下的联合VEFL和无线接入技术（RAT）参数优化问题，以最大化成功接收本地训练模型的概率。考虑到优化问题是NP-hard问题，我们将其分解为待解决的子问题。

    This paper proposes a vehicular edge federated learning (VEFL) solution, where an edge server leverages highly mobile connected vehicles' (CVs') onboard central processing units (CPUs) and local datasets to train a global model. Convergence analysis reveals that the VEFL training loss depends on the successful receptions of the CVs' trained models over the intermittent vehicle-to-infrastructure (V2I) wireless links. Owing to high mobility, in the full device participation case (FDPC), the edge server aggregates client model parameters based on a weighted combination according to the CVs' dataset sizes and sojourn periods, while it selects a subset of CVs in the partial device participation case (PDPC). We then devise joint VEFL and radio access technology (RAT) parameters optimization problems under delay, energy and cost constraints to maximize the probability of successful reception of the locally trained models. Considering that the optimization problem is NP-hard, we decompose it i
    
[^112]: 破碎的神经缩放定律

    Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14891](http://arxiv.org/abs/2210.14891)

    本文提出了一个平滑破碎的幂律函数形式，可以准确地模拟和外推深度神经网络的缩放行为，适用于各种架构和大量不同任务，包括视觉、语言、音频、视频、生成建模、对比学习、机器人、不确定性估计/校准、对抗鲁棒性、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。

    This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.

    我们提出了一个平滑破碎的幂律函数形式（我们称之为破碎的神经缩放定律（BNSL）），它准确地模拟和外推了深度神经网络的缩放行为（即感兴趣的评估指标随用于训练的计算量、模型参数数量、训练数据集大小或上游性能变化而变化）对于各种架构和大量不同任务中的每个任务，包括大规模视觉、语言、音频、视频、扩散、生成建模、多模态学习、对比学习、AI对齐、机器人、超出分布（OOD）泛化、持续学习、不确定性估计/校准、超出分布检测、对抗鲁棒性、蒸馏、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。

    We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
    
[^113]: 过度参数化高维模型的不确定性量化研究

    A study of uncertainty quantification in overparametrized high-dimensional models. (arXiv:2210.12760v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.12760](http://arxiv.org/abs/2210.12760)

    本论文研究了过度参数化高维模型中的不确定性问题，探讨了几种方法，比较了校准和分类准确性之间的权衡。结果发现最佳正则化估计量的校准曲线具有双重下降行为，与经验贝叶斯方法形成对比。

    

    不确定性量化是可靠和可信机器学习的中心挑战。在过度参数化的神经网络背景下，朴素的度量方法(如最后一层分数)已经被广为人知地产生过度自信的估计。提出了几种方法，从温度缩放到神经网络的不同贝叶斯处理，以缓解过度自信，通常通过数值观察支持它们产生更好的校准不确定性度量。在这项工作中，我们在一个数学可处理的过度参数化神经网络模型中，对于二元分类，提供了常见不确定度量之间的尖锐比较：随机特征模型。我们讨论了分类准确性和校准之间的折衷，披露最佳正则化估计量的校准曲线与过参数化的函数的双重下降行为。这与经验贝叶斯方法形成对比，我们展示它的校准是良好的。

    Uncertainty quantification is a central challenge in reliable and trustworthy machine learning. Naive measures such as last-layer scores are well-known to yield overconfident estimates in the context of overparametrized neural networks. Several methods, ranging from temperature scaling to different Bayesian treatments of neural networks, have been proposed to mitigate overconfidence, most often supported by the numerical observation that they yield better calibrated uncertainty measures. In this work, we provide a sharp comparison between popular uncertainty measures for binary classification in a mathematically tractable model for overparametrized neural networks: the random features model. We discuss a trade-off between classification accuracy and calibration, unveiling a double descent like behavior in the calibration curve of optimally regularized estimators as a function of overparametrization. This is in contrast with the empirical Bayes method, which we show to be well calibrate
    
[^114]: 对比学习的通用隐含单调趋势估计

    Universal hidden monotonic trend estimation with contrastive learning. (arXiv:2210.09817v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.09817](http://arxiv.org/abs/2210.09817)

    本文提出了一种称为对比趋势估计 (CTE) 的方法，它是一种通用隐含单调趋势估计方法，可适用于任何类型的时间数据，并避免了用于单调趋势识别的标准假设。

    

    本文介绍了一种从时间序列数据中提取潜在单调趋势因素的通用方法。我们提出了一种与 Mann-Kendall 测试相关的方法，称为对比趋势估计 (CTE)。我们展示了 CTE 方法能够识别任何隐藏在时间数据中的趋势，同时避免了用于单调趋势识别的标准假设。特别地，CTE 可以将任何类型的时间数据 (向量、图像、图、时间序列等) 作为输入。最后，我们通过对不同类型的数据和问题进行多个实验，展示了我们提出的 CTE 方法的实用性。

    In this paper, we describe a universal method for extracting the underlying monotonic trend factor from time series data. We propose an approach related to the Mann-Kendall test, a standard monotonic trend detection method and call it contrastive trend estimation (CTE). We show that the CTE method identifies any hidden trend underlying temporal data while avoiding the standard assumptions used for monotonic trend identification. In particular, CTE can take any type of temporal data (vector, images, graphs, time series, etc.) as input. We finally illustrate the interest of our CTE method through several experiments on different types of data and problems.
    
[^115]: Just Round：量化观测空间实现动态运动的高效学习

    Just Round: Quantized Observation Spaces Enable Memory Efficient Learning of Dynamic Locomotion. (arXiv:2210.08065v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.08065](http://arxiv.org/abs/2210.08065)

    本文通过观测空间量化，实现了对复杂机器人行为的高效学习，可以降低内存成本。

    

    深度强化学习（DRL）是合成复杂机器人行为的最强大工具之一。但训练DRL模型需要大量计算和内存，需要大型训练数据集和回放缓冲区才能取得良好的结果。这对于下一代需要在边缘上学习以适应其环境的现场机器人提出了挑战。本文通过观测空间量化来开始解决这个问题。我们使用四个模拟机器人运动任务和两种最先进的DRL算法，在不影响学习性能的情况下，发现观测空间量化可以将整体内存成本降低多达4.2倍。

    Deep reinforcement learning (DRL) is one of the most powerful tools for synthesizing complex robotic behaviors. But training DRL models is incredibly compute and memory intensive, requiring large training datasets and replay buffers to achieve performant results. This poses a challenge for the next generation of field robots that will need to learn on the edge to adapt to their environment. In this paper, we begin to address this issue through observation space quantization. We evaluate our approach using four simulated robot locomotion tasks and two state-of-the-art DRL algorithms, the on-policy Proximal Policy Optimization (PPO) and off-policy Soft Actor-Critic (SAC) and find that observation space quantization reduces overall memory costs by as much as 4.2x without impacting learning performance.
    
[^116]: 语言模型是真实的表格数据生成器。

    Language Models are Realistic Tabular Data Generators. (arXiv:2210.06280v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.06280](http://arxiv.org/abs/2210.06280)

    本文提出了GReaT模型，利用自回归生成的大型语言模型来生成高度真实的表格数据，可以通过调节任意子集特征建模表格数据分布。

    

    表格数据是最古老和最普遍的数据之一。然而，合成具有原始数据特征的样本仍然是表格数据中的一项重大挑战。虽然计算机视觉领域的许多生成模型，如变分自动编码器或生成对抗网络，已被应用于表格数据生成，但较少的研究方向针对近期的基于转换器的大型语言模型（LLM），它们也具有生成性质。为此，我们提出了GReaT（真实表格数据的生成），它利用自回归生成LLM来采样合成具有高度真实感的表格数据。此外，GReaT可以通过对任意子集特征进行调节来建模表格数据分布；其余特征则无需额外开销进行采样。我们在一系列实验中展示了所提出方法的有效性，可量化所产生数据的有效性和质量。

    Tabular data is among the oldest and most ubiquitous forms of data. However, the generation of synthetic samples with the original data's characteristics remains a significant challenge for tabular data. While many generative models from the computer vision domain, such as variational autoencoders or generative adversarial networks, have been adapted for tabular data generation, less research has been directed towards recent transformer-based large language models (LLMs), which are also generative in nature. To this end, we propose GReaT (Generation of Realistic Tabular data), which exploits an auto-regressive generative LLM to sample synthetic and yet highly realistic tabular data. Furthermore, GReaT can model tabular data distributions by conditioning on any subset of features; the remaining features are sampled without additional overhead. We demonstrate the effectiveness of the proposed approach in a series of experiments that quantify the validity and quality of the produced data 
    
[^117]: 优化层的交替微分

    Alternating Differentiation for Optimization Layers. (arXiv:2210.01802v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01802](http://arxiv.org/abs/2210.01802)

    Alt-Diff是一种新的框架，可以在不需要对整个雅可比矩阵进行昂贵计算的情况下，以快速和递归的方式微分优化问题，从而大大提高隐式微分的计算速度。

    

    将优化问题嵌入深度神经网络作为优化层以编码约束和归纳先验的想法在近年来已经深入人心。现有的大多数方法都集中在以一种需要在雅可比矩阵上进行昂贵计算的方式隐式微分Karush-Kuhn-Tucker（KKT）条件上，这可能是慢和内存密集的。在本文中，我们开发了一种名为交替微分（Alt-Diff）的新框架，以一种快速且递归的方式微分优化问题（这里特别指带有多面体约束的凸优化问题）。Alt-Diff将微分过程分解为主问题更新和对偶问题更新的交替方式。因此，Alt-Diff尤其能够减小雅可比矩阵的维度，特别是针对具有大规模约束的优化问题，从而提高了隐式微分的计算速度。我们展示了通过Alt-Diff获得的梯度

    The idea of embedding optimization problems into deep neural networks as optimization layers to encode constraints and inductive priors has taken hold in recent years. Most existing methods focus on implicitly differentiating Karush-Kuhn-Tucker (KKT) conditions in a way that requires expensive computations on the Jacobian matrix, which can be slow and memory-intensive. In this paper, we developed a new framework, named Alternating Differentiation (Alt-Diff), that differentiates optimization problems (here, specifically in the form of convex optimization problems with polyhedral constraints) in a fast and recursive way. Alt-Diff decouples the differentiation procedure into a primal update and a dual update in an alternating way. Accordingly, Alt-Diff substantially decreases the dimensions of the Jacobian matrix especially for optimization with large-scale constraints and thus increases the computational speed of implicit differentiation. We show that the gradients obtained by Alt-Diff a
    
[^118]: 通过超马氏过程推导重尾损失的PAC-Bayes泛化界

    PAC-Bayes Generalisation Bounds for Heavy-Tailed Losses through Supermartingales. (arXiv:2210.00928v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.00928](http://arxiv.org/abs/2210.00928)

    本文为重尾损失情况下的PAC-Bayes提供了泛化界，扩展了先前的研究，并通过马尔科夫不等式的扩展为不同的PAC-Bayesian框架提供了界限。

    

    尽管PAC-Bayes已经成为一种用于轻尾损失（例如亚高斯或亚指数）的学习框架，但其在重尾损失情况下的推广仍然未得到广泛研究，近年来受到越来越多的关注。本文在假定损失函数有界方差的情况下，为重尾损失提供了PAC-Bayes泛化界。在该假设下，我们扩展了\citet{kuzborskij2019efron}的先前结果。我们的关键技术贡献在于利用超马氏过程的马尔科夫不等式的扩展。我们的证明技术通过为无界鞅提供界限，以及为重尾损失的批处理和在线学习提供界限，统一和扩展了不同的PAC-Bayesian框架。

    While PAC-Bayes is now an established learning framework for light-tailed losses (\emph{e.g.}, subgaussian or subexponential), its extension to the case of heavy-tailed losses remains largely uncharted and has attracted a growing interest in recent years. We contribute PAC-Bayes generalisation bounds for heavy-tailed losses under the sole assumption of bounded variance of the loss function. Under that assumption, we extend previous results from \citet{kuzborskij2019efron}. Our key technical contribution is exploiting an extention of Markov's inequality for supermartingales. Our proof technique unifies and extends different PAC-Bayesian frameworks by providing bounds for unbounded martingales as well as bounds for batch and online learning with heavy-tailed losses.
    
[^119]: 某些函数类的Natarajan维数的上界

    Upper bounds on the Natarajan dimensions of some function classes. (arXiv:2209.07015v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2209.07015](http://arxiv.org/abs/2209.07015)

    本研究建立了一些函数类的Natarajan维度上界，这些结果可以用于描述某些多类学习算法的性能。

    

    Natarajan维度是表征多类PAC可学习性的基本工具，将Vapnik-Chervonenkis（VC）维从二进制分类问题推广到多类分类问题。本研究建立了一些函数类的Natarajan维度上界，包括（i）多类决策树和随机森林，以及（ii）二进制、线性和ReLU激活的多类神经网络。这些结果可能对描述某些多类学习算法的性能有相关性。

    The Natarajan dimension is a fundamental tool for characterizing multi-class PAC learnability, generalizing the Vapnik-Chervonenkis (VC) dimension from binary to multi-class classification problems. This work establishes upper bounds on Natarajan dimensions for certain function classes, including (i) multi-class decision tree and random forests, and (ii) multi-class neural networks with binary, linear and ReLU activations. These results may be relevant for describing the performance of certain multi-class learning algorithms.
    
[^120]: GPU上深度学习训练的协同分析

    An Analysis of Collocation on GPUs for Deep Learning Training. (arXiv:2209.06018v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.06018](http://arxiv.org/abs/2209.06018)

    本文通过研究GPU上深度学习训练中的协同分析，证明了协同多个模型训练可以产生显着的效益。MIG技术通过将GPU分区，更好地适应不需要完整GPU内存和计算资源的工作负载，具有优势，并且具有优化的放置策略的MIG一般优于所有其他方法。

    

    深度学习训练是一项昂贵的过程，极大地使用GPU，但并不是所有模型训练都能充分利用现代强大的GPU。多实例GPU（MIG）是NVIDIA引入的一项新技术，可以分区GPU以更好地适应不需要完整GPU内存和计算资源的工作负载。本文研究了在包含各种大小和模型组合的深度学习工作负载下，MIG启用的A100 GPU的性能。我们将MIG的优势与旧的GPU工作负载协同方法进行对比：在同一GPU上直接提交多个进程和使用多进程服务（MPS）。我们的结果表明，协同多个模型训练可以产生显着的效益。在某些情况下，即使epoch时间增加，训练吞吐量也可能增加了四倍。另一方面，同时训练的模型的聚合内存占用和计算需求必须符合GPU的可用内存和计算资源。我们还研究了MIG放置策略在分配模型训练资源时对性能的影响。我们的实验表明，具有优化放置策略的MIG一般优于所有其他方法。

    Deep learning training is an expensive process that extensively uses GPUs, but not all model training saturates modern powerful GPUs. Multi-Instance GPU (MIG) is a new technology introduced by NVIDIA that can partition a GPU to better-fit workloads that do not require all the memory and compute resources of a full GPU. In this paper, we examine the performance of a MIG-enabled A100 GPU under deep learning workloads containing various sizes and combinations of models. We contrast the benefits of MIG to older workload collocation methods on GPUs: na\"ively submitting multiple processes on the same GPU and utilizing Multi-Process Service (MPS). Our results demonstrate that collocating multiple model training runs may yield significant benefits. In certain cases, it can lead up to four times training throughput despite increased epoch time. On the other hand, the aggregate memory footprint and compute needs of the models trained in parallel must fit the available memory and compute resourc
    
[^121]: 将任务特定的概念知识纳入脚本学习中

    Incorporating Task-specific Concept Knowledge into Script Learning. (arXiv:2209.00068v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.00068](http://arxiv.org/abs/2209.00068)

    本文提出了一个新任务Tetris，并提出了概念提示和面向脚本的对比学习来解决输入包括用户上下文时的问题，两种方法均能提高任务完成性能。

    

    本文提出了一个名为Tetris的新任务，名为目标导向脚本完成。与之前的工作不同的是，它考虑了一个更为现实和普遍的场景，输入不仅包括目标，还包括用户的额外上下文，包括喜好和历史。为了解决这个问题，我们提出了一个新颖的方法，它使用两种技术来提高性能：概念提示和面向脚本的对比学习，以解决步骤重复和幻觉问题。在基于WikiHow的数据集上，我们发现两种方法都可以提高性能。该数据集、仓库和模型将公开提供，以促进对这个新任务的进一步研究。

    In this paper, we present Tetris, a new task of Goal-Oriented Script Completion. Unlike previous work, it considers a more realistic and general setting, where the input includes not only the goal but also additional user context, including preferences and history. To address this problem, we propose a novel approach, which uses two techniques to improve performance: (1) concept prompting, and (2) script-oriented contrastive learning that addresses step repetition and hallucination problems. On our WikiHow-based dataset, we find that both methods improve performance. The dataset, repository, and models will be publicly available to facilitate further research on this new task.
    
[^122]: 深度符号学习：从感知中发现符号和规则

    Deep Symbolic Learning: Discovering Symbols and Rules from Perceptions. (arXiv:2208.11561v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.11561](http://arxiv.org/abs/2208.11561)

    DSL是一种能够在学习中自动发现有意义符号规则的NeSy系统，并在各种任务中取得最先进的结果。

    

    神经符号（NeSy）集成将符号推理与神经网络（NN）结合起来，用于需要感知和推理的任务。大多数NeSy系统依赖于逻辑知识的连续放松，并且模型管道内不做出离散决策。此外，这些方法假定给定符号规则。本文提出了深度符号学习（DSL），一种NeSy系统，它可以学习NeSy函数，即将连续数据映射到离散符号的（一组）感知函数的组合和符号函数。DSL同时学习感知和符号函数，仅在它们的组合（NeSy函数）上进行培训。DSL的关键创新在于，它可以在可微的NN学习管道内创建内部（可解释的）符号表示，并将它们映射到感知输入。创建的符号是自动选择的，以生成最好解释数据的符号函数。我们提供了一系列关于合成和真实数据集的实验，展示了DSL在各种任务中学习符号规则和发现有意义的符号的能力，并取得了最先进的结果。

    Neuro-Symbolic (NeSy) integration combines symbolic reasoning with Neural Networks (NNs) for tasks requiring perception and reasoning. Most NeSy systems rely on continuous relaxation of logical knowledge, and no discrete decisions are made within the model pipeline. Furthermore, these methods assume that the symbolic rules are given. In this paper, we propose Deep Symbolic Learning (DSL), a NeSy system that learns NeSy-functions, i.e., the composition of a (set of) perception functions which map continuous data to discrete symbols, and a symbolic function over the set of symbols. DSL learns simultaneously the perception and symbolic functions while being trained only on their composition (NeSy-function). The key novelty of DSL is that it can create internal (interpretable) symbolic representations and map them to perception inputs within a differentiable NN learning pipeline. The created symbols are automatically selected to generate symbolic functions that best explain the data. We pr
    
[^123]: 基于检索的可控化合物生成方法

    Retrieval-based Controllable Molecule Generation. (arXiv:2208.11126v3 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2208.11126](http://arxiv.org/abs/2208.11126)

    本文提出了一种基于检索的方法，通过使用少量的例子分子来引导生成模型生成满足给定设计条件的分子。并且使用自我监督的目标函数训练检索机制，来提高生成的分子的泛化性。

    

    通过生成模型生成具有特定化学和生物性质的新分子已成为药物发现的一个有希望的方向。然而，现有方法需要使用大量数据进行训练/微调，而这在实际的生成任务中通常不可用。在本文中，我们提出了一种新的基于检索的框架，用于可控化合物的生成。我们使用一小组例子分子来引导预先训练的生成模型合成满足给定设计条件的分子。我们设计了一个检索机制，通过预测输入分子的最近邻来训练并融合例子分子和输入分子。我们还提出了一个迭代的细化过程，以动态更新生成的分子和检索数据库，实现更好的泛化。我们的方法不依赖于生成模型的选择。

    Generating new molecules with specified chemical and biological properties via generative models has emerged as a promising direction for drug discovery. However, existing methods require extensive training/fine-tuning with a large dataset, often unavailable in real-world generation tasks. In this work, we propose a new retrieval-based framework for controllable molecule generation. We use a small set of exemplar molecules, i.e., those that (partially) satisfy the design criteria, to steer the pre-trained generative model towards synthesizing molecules that satisfy the given design criteria. We design a retrieval mechanism that retrieves and fuses the exemplar molecules with the input molecule, which is trained by a new self-supervised objective that predicts the nearest neighbor of the input molecule. We also propose an iterative refinement process to dynamically update the generated molecules and retrieval database for better generalization. Our approach is agnostic to the choice of 
    
[^124]: 利用图神经网络加速离散位错动力学仿真

    Accelerating discrete dislocation dynamics simulations with graph neural networks. (arXiv:2208.03296v2 [cond-mat.mtrl-sci] UPDATED)

    [http://arxiv.org/abs/2208.03296](http://arxiv.org/abs/2208.03296)

    研究团队提出了一种利用基于DDD轨迹训练的图神经网络模型替代耗时的位错运动时间积分的DDD-GNN框架，有效加速介观尺度下晶体材料塑性行为的计算模拟。

    

    离散位错动力学（DDD）是一种广泛应用于研究介观尺度下晶体材料塑性行为的计算方法，将位错线的运动与宏观响应相连。然而，DDD模拟的计算成本仍然是限制其适用范围的瓶颈。在这里，我们介绍了一种新的DDD-GNN框架，其中耗时的位错运动时间积分完全由基于DDD轨迹训练的图神经网络（GNN）模型替代。作为第一个应用，我们展示了我们的方法对于一种简单但相关的位错线在障碍物阵列上滑动的模型的可行性和潜力。我们展示了DDD-GNN模型在一定的应变速率和障碍物密度范围内稳定，并且很好地重现了未见过的真实DDD模拟响应，而无需在时间积分期间明确计算节点力或位错迁移率。我们的方法为将机器学习技术纳入介观尺度下力学行为的建模和仿真打开了新的有前途的途径。

    Discrete dislocation dynamics (DDD) is a widely employed computational method to study plasticity at the mesoscale that connects the motion of dislocation lines to the macroscopic response of crystalline materials. However, the computational cost of DDD simulations remains a bottleneck that limits its range of applicability. Here, we introduce a new DDD-GNN framework in which the expensive time-integration of dislocation motion is entirely substituted by a graph neural network (GNN) model trained on DDD trajectories. As a first application, we demonstrate the feasibility and potential of our method on a simple yet relevant model of a dislocation line gliding through an array of obstacles. We show that the DDD-GNN model is stable and reproduces very well unseen ground-truth DDD simulation responses for a range of straining rates and obstacle densities, without the need to explicitly compute nodal forces or dislocation mobilities during time-integration. Our approach opens new promising 
    
[^125]: 一种符合保序的风险控制方法

    Conformal Risk Control. (arXiv:2208.02814v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2208.02814](http://arxiv.org/abs/2208.02814)

    该论文提出了一种符合保序的风险控制方法，可以控制任何单调损失函数的期望值，示例证明其在计算机视觉和自然语言处理领域具有控制误报率、图形距离和令牌级F1得分的能力。

    

    我们将符合性预测推广至控制任何单调损失函数的期望值。该算法将分裂符合性预测及其覆盖保证进行了泛化。类似于符合性预测，符合保序的风险控制方法在$\mathcal{O}(1/n)$因子内保持紧密性。计算机视觉和自然语言处理领域的示例证明了我们算法在控制误报率、图形距离和令牌级F1得分方面的应用。

    We extend conformal prediction to control the expected value of any monotone loss function. The algorithm generalizes split conformal prediction together with its coverage guarantee. Like conformal prediction, the conformal risk control procedure is tight up to an $\mathcal{O}(1/n)$ factor. Worked examples from computer vision and natural language processing demonstrate the usage of our algorithm to bound the false negative rate, graph distance, and token-level F1-score.
    
[^126]: 基于有向图的跨模态特征补充方法用于多模态对话情感识别

    GraphCFC: A Directed Graph based Cross-modal Feature Complementation Approach for Multimodal Conversational Emotion Recognition. (arXiv:2207.12261v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2207.12261](http://arxiv.org/abs/2207.12261)

    本文提出了一种基于有向图的跨模态特征补充方法，可以提取多模态上下文信息和交互信息，缓解了多模态融合中的异构性差距问题。

    

    对话情感识别在人机交互系统中起着重要作用，因为它可以提供有共情心理的服务。多模态对话情感识别可以缓解单模态方法的缺点。最近，由于关系建模方面的卓越性能，图神经网络已被广泛用于各种领域。在多模态对话情感识别中，图神经网络能够提取远距离的上下文信息和跨模态的交互信息。不幸的是，由于现有方法（如MMGCN）直接融合多个模态，可能会产生冗余信息，且可能丢失多样化的信息。在本文中，我们提出了一种基于有向图的跨模态特征补充（GraphCFC）模块，可以有效地模拟上下文和互动信息。GraphCFC通过利用多个子空间提取器和成对跨模态补充（PairCC）策略，缓解了多模态融合中的异构性差距问题。

    Emotion Recognition in Conversation (ERC) plays a significant part in Human-Computer Interaction (HCI) systems since it can provide empathetic services. Multimodal ERC can mitigate the drawbacks of uni-modal approaches. Recently, Graph Neural Networks (GNNs) have been widely used in a variety of fields due to their superior performance in relation modeling. In multimodal ERC, GNNs are capable of extracting both long-distance contextual information and inter-modal interactive information. Unfortunately, since existing methods such as MMGCN directly fuse multiple modalities, redundant information may be generated and diverse information may be lost. In this work, we present a directed Graph based Cross-modal Feature Complementation (GraphCFC) module that can efficiently model contextual and interactive information. GraphCFC alleviates the problem of heterogeneity gap in multimodal fusion by utilizing multiple subspace extractors and Pair-wise Cross-modal Complementary (PairCC) strategy. 
    
[^127]: 基于处理内存系统的机器学习训练的实验评估

    An Experimental Evaluation of Machine Learning Training on a Real Processing-in-Memory System. (arXiv:2207.07886v2 [cs.AR] UPDATED)

    [http://arxiv.org/abs/2207.07886](http://arxiv.org/abs/2207.07886)

    该研究评估了在处理内存系统上训练机器学习算法的潜能，并证明基于PIM的ML训练实现了显着的加速和能量效率。

    

    训练机器学习算法是一种计算密集型的过程，由于不断访问大型训练数据集，这种过程通常会受到内存限制。因此，以处理器为中心的系统（例如CPU，GPU）在内存单元和处理单元之间的数据传输方面存在昂贵的瓶颈，这会消耗大量的能量和执行周期。具有处理内存（PIM）功能的内存中心计算系统可以缓解这种数据移动瓶颈。我们的目标是了解现代通用PIM架构加速ML训练的潜力。为此，我们（1）在实际通用PIM架构上实现了几种代表性的传统ML算法（即线性回归，逻辑回归，决策树，K-Means聚类），（2）严格评估和表征这些算法的准确性，性能和扩展性，并且（3）与它们在CPU和GPU上的相应实现进行比较。我们在实际内存中心计算平台上的评估表明，与相应的CPU和GPU方法相比，基于PIM的ML训练实现了显着的加速和能量效率。

    Training machine learning (ML) algorithms is a computationally intensive process, which is frequently memory-bound due to repeatedly accessing large training datasets. As a result, processor-centric systems (e.g., CPU, GPU) suffer from costly data movement between memory units and processing units, which consumes large amounts of energy and execution cycles. Memory-centric computing systems, i.e., with processing-in-memory (PIM) capabilities, can alleviate this data movement bottleneck.  Our goal is to understand the potential of modern general-purpose PIM architectures to accelerate ML training. To do so, we (1) implement several representative classic ML algorithms (namely, linear regression, logistic regression, decision tree, K-Means clustering) on a real-world general-purpose PIM architecture, (2) rigorously evaluate and characterize them in terms of accuracy, performance and scaling, and (3) compare to their counterpart implementations on CPU and GPU. Our evaluation on a real mem
    
[^128]: 了解您的空间：内线和外线构建用于校准医学OOD探测器

    Know Your Space: Inlier and Outlier Construction for Calibrating Medical OOD Detectors. (arXiv:2207.05286v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2207.05286](http://arxiv.org/abs/2207.05286)

    本文研究了医学图像OOD探测器的校准问题，发现内线和外线的合成空间以及增强类型对OOD探测器的校准起到关键作用。研究表明，最佳协议是合成潜空间内线和多样化的像素空间外线，该方法相比现有技术可以提高OOD检测的AUROC $15％-35％$。

    

    本文关注如何产生良好校准的医学图像OOD探测器，以便安全部署。由于筛选出合适的校准数据集较为困难，因此合成增强技术在内线/外线特定方面已广泛使用。本文发现，在内线和外线的合成空间以及增强类型方面还存在着一些需要注意的问题，这对于OOD探测器的校准至关重要。在流行的基于能量的OOD探测框架中，我们发现最佳协议是合成潜空间内线和多样化的像素空间外线。通过对多个医学成像基准进行实证研究，我们证明了我们的方法在各种开放式识别设置中始终优于现有技术，OOD检测的AUROC可相对提高$15％-35％$

    We focus on the problem of producing well-calibrated out-of-distribution (OOD) detectors, in order to enable safe deployment of medical image classifiers. Motivated by the difficulty of curating suitable calibration datasets, synthetic augmentations have become highly prevalent for inlier/outlier specification. While there have been rapid advances in data augmentation techniques, this paper makes a striking finding that the space in which the inliers and outliers are synthesized, in addition to the type of augmentation, plays a critical role in calibrating OOD detectors. Using the popular energy-based OOD detection framework, we find that the optimal protocol is to synthesize latent-space inliers along with diverse pixel-space outliers. Based on empirical studies with multiple medical imaging benchmarks, we demonstrate that our approach consistently leads to superior OOD detection ($15\% - 35\%$ in AUROC) over the state-of-the-art in a variety of open-set recognition settings.
    
[^129]: SALSA：用Transformers攻击格密码

    SALSA: Attacking Lattice Cryptography with Transformers. (arXiv:2207.04785v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2207.04785](http://arxiv.org/abs/2207.04785)

    SALSA是一种使用Transformer和统计密码分析技术的机器学习攻击，可以攻击小到中等大小、具有稀疏二进制秘密的LWE实例，可能会扩展到实际的LWE密码系统。

    

    当前的公钥密码系统将会面临全规模量子计算机的攻击。因此，“量子抗性”密码系统需求极高，基于Learn With Errors（LWE）问题的格密码系统已成为标准化的强有力竞争者。在本研究中，我们训练Transformer执行模数算术，并结合统计密码分析技术将半训练模型组合在一起，提出SALSA：一种用于攻击基于LWE的密码方案的机器学习攻击。SALSA可以完全恢复小到中等大小的具有稀疏二进制秘密的LWE实例的密钥，并可能扩展到攻击实际的基于LWE的密码系统。

    Currently deployed public-key cryptosystems will be vulnerable to attacks by full-scale quantum computers. Consequently, "quantum resistant" cryptosystems are in high demand, and lattice-based cryptosystems, based on a hard problem known as Learning With Errors (LWE), have emerged as strong contenders for standardization. In this work, we train transformers to perform modular arithmetic and combine half-trained models with statistical cryptanalysis techniques to propose SALSA: a machine learning attack on LWE-based cryptographic schemes. SALSA can fully recover secrets for small-to-mid size LWE instances with sparse binary secrets, and may scale to attack real-world LWE-based cryptosystems.
    
[^130]: 利用编译器中间表示进行代码翻译

    Code Translation with Compiler Representations. (arXiv:2207.03578v4 [cs.PL] UPDATED)

    [http://arxiv.org/abs/2207.03578](http://arxiv.org/abs/2207.03578)

    本文提出了一种将编译器中间表示与神经机器翻译相结合的方法，能够更好地捕捉代码的语义，从而提高了代码翻译的质量和实用性。

    

    本文提出了一种利用低级别的编译器中间表示（IR）来改进代码翻译的方法。我们的方法结合了神经机器翻译（NMT）和IR，能够更好地捕捉代码的语义，避免以往方法存在的常见错误，从而提高了翻译的质量和实用性。

    In this paper, we leverage low-level compiler intermediate representations (IR) to improve code translation. Traditional transpilers rely on syntactic information and handcrafted rules, which limits their applicability and produces unnatural-looking code. Applying neural machine translation (NMT) approaches to code has successfully broadened the set of programs on which one can get a natural-looking translation. However, they treat the code as sequences of text tokens, and still do not differentiate well enough between similar pieces of code which have different semantics in different languages. The consequence is low quality translation, reducing the practicality of NMT, and stressing the need for an approach significantly increasing its accuracy. Here we propose to augment code translation with IRs, specifically LLVM IR, with results on the C++, Java, Rust, and Go languages. Our method improves upon the state of the art for unsupervised code translation, increasing the number of corr
    
[^131]: 连续时间下的q-Learning

    q-Learning in Continuous Time. (arXiv:2207.00713v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.00713](http://arxiv.org/abs/2207.00713)

    本文研究了连续时间下的q-Learning，通过引入小q函数作为一阶近似，研究了q-learning理论，应用于设计不同的演员-评论家算法。

    

    我们研究了基于熵正则化的探索性扩散过程的Q-learning在连续时间下的应用。我们引入了“小q函数”作为大Q函数的一阶近似，研究了q函数的q-learning理论，并应用于设计不同的演员-评论家算法。

    We study the continuous-time counterpart of Q-learning for reinforcement learning (RL) under the entropy-regularized, exploratory diffusion process formulation introduced by Wang et al. (2020). As the conventional (big) Q-function collapses in continuous time, we consider its first-order approximation and coin the term ``(little) q-function". This function is related to the instantaneous advantage rate function as well as the Hamiltonian. We develop a ``q-learning" theory around the q-function that is independent of time discretization. Given a stochastic policy, we jointly characterize the associated q-function and value function by martingale conditions of certain stochastic processes, in both on-policy and off-policy settings. We then apply the theory to devise different actor-critic algorithms for solving underlying RL problems, depending on whether or not the density function of the Gibbs measure generated from the q-function can be computed explicitly. One of our algorithms inter
    
[^132]: 超越神经尺度定律：通过数据修剪打败幂律尺度

    Beyond neural scaling laws: beating power law scaling via data pruning. (arXiv:2206.14486v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.14486](http://arxiv.org/abs/2206.14486)

    本研究通过数据修剪算法突破神经网络训练集大小与模型误差幂律的尺度界限，并在多个数据集实验中验证了有效性，同时进行了首次大规模数据修剪算法基准测试研究。

    

    普遍存在的神经尺度定律以训练集大小、模型规模或两者的幂为模型误差下降的驱动力，为深度学习带来了显著的性能提升。但是，仅通过尺度来实现这些改进需要巨大的计算和能源成本。本文着重研究数据集大小与误差比例的尺度，并展示理论上我们如何突破幂律尺度，并在pruning算法条件下潜在地甚至能将其降至指数尺度。我们接着在CIFAR-10、SVHN和ImageNet的ResNet上进行了实验验证，并观察到实践中优于幂律尺度的表现。此外，鉴于寻找优质pruning算法的重要性，我们对ImageNet上的十种不同的数据修剪算法进行了首次大规模基准测试研究。

    Widely observed neural scaling laws, in which error falls off as a power of the training set size, model size, or both, have driven substantial performance improvements in deep learning. However, these improvements through scaling alone require considerable costs in compute and energy. Here we focus on the scaling of error with dataset size and show how in theory we can break beyond power law scaling and potentially even reduce it to exponential scaling instead if we have access to a high-quality data pruning metric that ranks the order in which training examples should be discarded to achieve any pruned dataset size. We then test this improved scaling prediction with pruned dataset size empirically, and indeed observe better than power law scaling in practice on ResNets trained on CIFAR-10, SVHN, and ImageNet. Next, given the importance of finding high-quality pruning metrics, we perform the first large-scale benchmarking study of ten different data pruning metrics on ImageNet. We fin
    
[^133]: 带约束下凸下层问题的简单双层优化条件梯度方法

    A Conditional Gradient-based Method for Simple Bilevel Optimization with Convex Lower-level Problem. (arXiv:2206.08868v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2206.08868](http://arxiv.org/abs/2206.08868)

    本文提出了一种新的双层优化方法，该方法通过局部逼近下层问题的解集，然后运行条件梯度更新来减少上层目标函数，并且收敛性保证较好。

    

    本文研究一类双层优化问题——简单双层优化，其中我们在另一个凸约束优化问题的最优解集上最小化平滑的目标函数。已经发展出了几种迭代方法来处理这类问题，但它们的收敛性保证要么是上层目标的渐近性，要么是收敛速率缓慢且亚优。为了解决这个问题，本文提出了一种新的双层优化方法，该方法通过切割平面局部逼近下层问题的解集，然后运行条件梯度更新来减少上层目标函数。当上层目标函数为凸函数时，我们证明了我们的方法需要${\mathcal{O}}(\max\{1/\epsilon_f,1/\epsilon_g\})$次迭代才能找到一个对于上层 和下层目标函数同时$\epsilon_f$和$\epsilon_g$最优的解。

    In this paper, we study a class of bilevel optimization problems, also known as simple bilevel optimization, where we minimize a smooth objective function over the optimal solution set of another convex constrained optimization problem. Several iterative methods have been developed for tackling this class of problems. Alas, their convergence guarantees are either asymptotic for the upper-level objective, or the convergence rates are slow and sub-optimal. To address this issue, in this paper, we introduce a novel bilevel optimization method that locally approximates the solution set of the lower-level problem via a cutting plane, and then runs a conditional gradient update to decrease the upper-level objective. When the upper-level objective is convex, we show that our method requires ${\mathcal{O}}(\max\{1/\epsilon_f,1/\epsilon_g\})$ iterations to find a solution that is $\epsilon_f$-optimal for the upper-level objective and $\epsilon_g$-optimal for the lower-level objective. Moreover,
    
[^134]: 深度隔离森林用于异常检测

    Deep Isolation Forest for Anomaly Detection. (arXiv:2206.06602v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.06602](http://arxiv.org/abs/2206.06602)

    本文提出了深度隔离森林，利用神经网络将原始数据映射到随机表示集合中，通过随机轴并行切割来执行数据分区，以解决孤立森林算法不能成功检测高维/非线性可分数据空间中的难以隔离的困难异常的问题。

    

    孤立森林（iForest）由于在不同基准测试中的普适有效性和强大的可扩展性而逐渐成为可能是最受欢迎的异常检测器。然而，它的线性轴并行隔离方法经常导致（i）检测难以在高维/非线性可分数据空间中隔离的困难异常的失败，以及（ii）臭名昭着的算法偏差，将预期较低的异常得分分配给工件区域。这些问题导致高误报率。引入了几个iForest扩展，但它们本质上仍然使用浅层的线性数据分区，限制了它们隔离真正异常的能力。因此，本文提出了深度隔离森林。我们引入了一种新的表示方案，利用随意初始化的神经网络将原始数据映射到随机表示集合中，随后应用随机轴并行切割来执行数据分区。

    Isolation forest (iForest) has been emerging as arguably the most popular anomaly detector in recent years due to its general effectiveness across different benchmarks and strong scalability. Nevertheless, its linear axis-parallel isolation method often leads to (i) failure in detecting hard anomalies that are difficult to isolate in high-dimensional/non-linear-separable data space, and (ii) notorious algorithmic bias that assigns unexpectedly lower anomaly scores to artefact regions. These issues contribute to high false negative errors. Several iForest extensions are introduced, but they essentially still employ shallow, linear data partition, restricting their power in isolating true anomalies. Therefore, this paper proposes deep isolation forest. We introduce a new representation scheme that utilises casually initialised neural networks to map original data into random representation ensembles, where random axis-parallel cuts are subsequently applied to perform the data partition. 
    
[^135]: ACMP: Allen-Cahn信息传递用于带有物质相变的图神经网络

    ACMP: Allen-Cahn Message Passing for Graph Neural Networks with Particle Phase Transition. (arXiv:2206.05437v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.05437](http://arxiv.org/abs/2206.05437)

    本文提出了一种基于ACMP的图神经网络模型，它可以通过具有吸引力和排斥力的相互作用粒子系统进行消息传递传播，克服了GNN过度平滑问题，将网络深度推到100层，并在基准数据集上实现了最先进的节点分类和图匹配性能。

    

    神经消息传递是基于图结构数据的特征提取单元，考虑从一层到下一层的网络传播中的相邻节点特征。我们通过具有吸引力和排斥力的相互作用粒子系统来建模这种过程，并在相变建模中引入Allen-Cahn力。系统的动力学是一种反应扩散过程，可以将粒子分离而不会扩散。这引出了一种Allen-Cahn信息传递(ACMP)用于图神经网络，其中粒子系统解的数值迭代构成了消息传递传播。ACMP具有简单的实现和神经ODE求解器，可以将网络深度推到100层，并具有理论上证明的Dirichlet能量严格正下界。因此，它提供了一种深度模型的GNN，避免了常见的GNN过度平滑问题。使用ACMP的GNN在基准数据集上实现了实际节点分类和图匹配任务的最先进性能。

    Neural message passing is a basic feature extraction unit for graph-structured data considering neighboring node features in network propagation from one layer to the next. We model such process by an interacting particle system with attractive and repulsive forces and the Allen-Cahn force arising in the modeling of phase transition. The dynamics of the system is a reaction-diffusion process which can separate particles without blowing up. This induces an Allen-Cahn message passing (ACMP) for graph neural networks where the numerical iteration for the particle system solution constitutes the message passing propagation. ACMP which has a simple implementation with a neural ODE solver can propel the network depth up to one hundred of layers with theoretically proven strictly positive lower bound of the Dirichlet energy. It thus provides a deep model of GNNs circumventing the common GNN problem of oversmoothing. GNNs with ACMP achieve state of the art performance for real-world node class
    
[^136]: Merlin-Arthur分类器的形式可解释性

    Formal Interpretability with Merlin-Arthur Classifiers. (arXiv:2206.00759v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.00759](http://arxiv.org/abs/2206.00759)

    该论文提出了一种新型的多智能体交互分类器，利用“Merlin-Arthur”协议的启发，在不假设最优智能体或特征独立分布的情况下，通过相对强度和“非对称特征相关性”概念捕捉特征之间精确的相关性，提供可证明的可解释性保证。

    

    我们提出了一种新类型的多智能体交互分类器，即使是像神经网络这样的复杂智能体也能提供可证明的可解释性保证。这些保证包括对此分类器选择的特征之间互信息的上下界约束。我们的结果受交互式证明系统中 Merlin-Arthur 协议的启发，并以可测量的指标（如声音和完整性）表达了这些约束。与现有的交互式设置相比，我们不依赖于最优智能体或特征独立分布的假设。相反，我们利用智能体的相对强度以及新的“非对称特征相关性”概念来捕捉使可解释性保证困难的精确相关性类型。 我们通过两个小规模数据集的数值实验来测试我们的结果，这些实验可验证高互信息性。

    We propose a new type of multi-agent interactive classifier that provides provable interpretability guarantees even for complex agents such as neural networks. These guarantees consist of bounds on the mutual information of the features selected by this classifier. Our results are inspired by the Merlin-Arthur protocol from Interactive Proof Systems and express these bounds in terms of measurable metrics such as soundness and completeness. Compared to existing interactive setups we do not rely on optimal agents or on the assumption that features are distributed independently. Instead, we use the relative strength of the agents as well as the new concept of Asymmetric Feature Correlation which captures the precise kind of correlations that make interpretability guarantees difficult. %relates the information carried by sets of features to one of the individual features. We test our results through numerical experiments on two small-scale datasets where high mutual information can be veri
    
[^137]: 令人沮丧的简单正则化可以提升深度强化学习的效果

    Frustratingly Easy Regularization on Representation Can Boost Deep Reinforcement Learning. (arXiv:2205.14557v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.14557](http://arxiv.org/abs/2205.14557)

    本文证明了DRL的学习表示应该满足一个有利的可区分表示属性，提出了一种正则化器PEER，旨在通过对内部表示进行显式正则化来维持可区分表示属性。

    

    深度强化学习(DRL)承诺代理能够从高维信息中学习到良好的策略，而表示学习则能够消除不相关和冗余的信息并保留相关的信息。本文证明了Q网络及其目标Q网络的学习表示在理论上应该满足一个有利的可区分表示属性。具体来说，在典型的DRL设置中两个相邻时间步长的价值函数的表示相似度存在一个上界。但是，通过说明性实验，我们发现学习到的DRL代理可能违反这个属性，并导致次优策略。因此，我们提出了一种名为"表示简单正则化的策略评估"(PEER)的简单而有效的正则化器，旨在通过对内部表示进行显式正则化来维持可区分表示属性。同时，我们提供了收敛速度分析。

    Deep reinforcement learning (DRL) gives the promise that an agent learns good policy from high-dimensional information, whereas representation learning removes irrelevant and redundant information and retains pertinent information. In this work, we demonstrate that the learned representation of the $Q$-network and its target $Q$-network should, in theory, satisfy a favorable distinguishable representation property. Specifically, there exists an upper bound on the representation similarity of the value functions of two adjacent time steps in a typical DRL setting. However, through illustrative experiments, we show that the learned DRL agent may violate this property and lead to a sub-optimal policy. Therefore, we propose a simple yet effective regularizer called Policy Evaluation with Easy Regularization on Representation (PEER), which aims to maintain the distinguishable representation property via explicit regularization on internal representations. And we provide the convergence rate
    
[^138]: TFLEX: 时间特征逻辑嵌入框架用于时间知识图谱上的复杂推理

    TFLEX: Temporal Feature-Logic Embedding Framework for Complex Reasoning over Temporal Knowledge Graph. (arXiv:2205.14307v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.14307](http://arxiv.org/abs/2205.14307)

    本文提出了第一个用于时间知识图谱的复杂查询嵌入方法TFLEX，能够自然地建模所有一阶逻辑（FOL）运算，同时扩展了向量逻辑以处理三个额外的时间运算符。

    

    在许多人工智能任务中，知识图谱上的多跳逻辑推理发挥着基本作用。最近的复杂查询嵌入（CQE）方法侧重于静态知识图谱，而时间知识图谱（TKG）尚未得到充分探索。TKG上的推理面临两个挑战：1.查询应该回答实体或时间戳；2.运算符应该同时考虑实体集上的集合逻辑和时间戳集上的时间逻辑。为了解决这个问题，我们定义了TKG上的多跳逻辑推理问题。通过三个生成的数据集，我们提出了第一个名为TFLEX的时间CQE，用于回答时间复杂查询。我们利用向量逻辑计算Temporal Feature-Logic嵌入的逻辑部分，从而自然地建模实体集上的所有一阶逻辑（FOL）运算。此外，我们的框架扩展时间戳集上的向量逻辑，以处理三个额外的时间运算符（After，Before和Between）。

    Multi-hop logical reasoning over knowledge graph (KG) plays a fundamental role in many artificial intelligence tasks. Recent complex query embedding (CQE) methods for reasoning focus on static KGs, while temporal knowledge graphs (TKGs) have not been fully explored. Reasoning over TKGs has two challenges: 1. The query should answer entities or timestamps; 2. The operators should consider both set logic on entity set and temporal logic on timestamp set. To bridge this gap, we define the multi-hop logical reasoning problem on TKGs. With generated three datasets, we propose the first temporal CQE named Temporal Feature-Logic Embedding framework (TFLEX) to answer the temporal complex queries. We utilize vector logic to compute the logic part of Temporal Feature-Logic embeddings, thus naturally modeling all First-Order Logic (FOL) operations on entity set. In addition, our framework extends vector logic on timestamp set to cope with three extra temporal operators (After, Before and Between)
    
[^139]: 带权图学习的等变量子电路

    Equivariant quantum circuits for learning on weighted graphs. (arXiv:2205.06109v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2205.06109](http://arxiv.org/abs/2205.06109)

    本文提出了一种考虑到带权图对称性的等变量子电路（ansatz），并在神经组合优化问题中证实了其性能优异，充分说明在量子机器学习领域，保持对称性的ansatz是成功关键。

    

    变分量子算法是在近期量子硬件上实现优势的领先候选。在这种情况下，当训练参数化的量子电路来解决特定问题时，选择合适的ansatz 是决定算法的可训练性和性能的最重要因素之一。然而，在量子机器学习（QML）中，基于训练数据结构的ansatz的文献相对较少。在本文中，我们引入了一种学习带权图任务的ansatz ，该ansatz尊重重要的图形对称性，即节点置换下的等变性。我们评估了这种ansatz的性能，并在一个复杂的学习任务，即神经组合优化中研究了其性能。我们对我们的模型进行了分析和数值研究，结果证实了保持对称性的 ansatzs是QML成功的关键。

    Variational quantum algorithms are the leading candidate for advantage on near-term quantum hardware. When training a parametrized quantum circuit in this setting to solve a specific problem, the choice of ansatz is one of the most important factors that determines the trainability and performance of the algorithm. In quantum machine learning (QML), however, the literature on ansatzes that are motivated by the training data structure is scarce. In this work, we introduce an ansatz for learning tasks on weighted graphs that respects an important graph symmetry, namely equivariance under node permutations. We evaluate the performance of this ansatz on a complex learning task, namely neural combinatorial optimization, where a machine learning model is used to learn a heuristic for a combinatorial optimization problem. We analytically and numerically study the performance of our model, and our results strengthen the notion that symmetry-preserving ansatzes are a key to success in QML.
    
[^140]: 自动化雷达网络配置算法选择

    Automated Algorithm Selection for Radar Network Configuration. (arXiv:2205.03670v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2205.03670](http://arxiv.org/abs/2205.03670)

    研究了13种黑盒优化算法在153个雷达网络配置问题实例上的表现，这些算法比人类专家要更好。但它们的排名取决于预算和位置高程。

    

    雷达网络的配置是一个复杂的问题，通常需要专家使用模拟器手动完成。不同数量和类型的雷达以及雷达应涵盖的不同位置给雷达配置问题带来了不同的实例。准确建模这些实例是复杂的，因为配置质量取决于大量参数、内部雷达处理以及雷达需要放置的地形。经典的优化算法因此不能应用于这个问题，我们依靠“试错”黑盒方法。本文研究了13种黑盒优化算法在153个雷达网络配置问题实例上的表现。这些算法的表现比人类专家要好得多。然而，它们的排名取决于可评估配置的预算和所处位置的高程文件。

    The configuration of radar networks is a complex problem that is often performed manually by experts with the help of a simulator. Different numbers and types of radars as well as different locations that the radars shall cover give rise to different instances of the radar configuration problem. The exact modeling of these instances is complex, as the quality of the configurations depends on a large number of parameters, on internal radar processing, and on the terrains on which the radars need to be placed. Classic optimization algorithms can therefore not be applied to this problem, and we rely on "trial-and-error" black-box approaches.  In this paper, we study the performances of 13 black-box optimization algorithms on 153 radar network configuration problem instances. The algorithms perform considerably better than human experts. Their ranking, however, depends on the budget of configurations that can be evaluated and on the elevation profile of the location. We therefore also inve
    
[^141]: 利用电池早期退化数据进行可解释的电池循环寿命范围预测

    Interpretable Battery Cycle Life Range Prediction Using Early Degradation Data at Cell Level. (arXiv:2204.12420v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2204.12420](http://arxiv.org/abs/2204.12420)

    该论文提出了一种利用电池早期退化数据预测电池循环寿命范围的方法，其QRF模型能够实现高精度的点预测，并通过预测区间宽度来量化不确定性，具有可解释性。

    

    利用早期退化数据预测电池循环寿命在电池产品生命周期中有很多潜在应用。因此，为了管理数量急剧增加的电池末处理，并降低经济和技术风险，需要预测具有量化不确定性的循环寿命范围，但目前还缺乏该方面的研究。这里引入了一种Quantile Regression Forest (QRF)模型，该模型不假设任何特定的循环寿命分布，在高精度的点预测的基础上，通过预测区间宽度来量化不确定性，实现了循环寿命范围的预测，并探究了高预测精度的原因。

    Battery cycle life prediction using early degradation data has many potential applications throughout the battery product life cycle. For that reason, various data-driven methods have been proposed for point prediction of battery cycle life with minimum knowledge of the battery degradation mechanisms. However, managing the rapidly increasing amounts of batteries at end-of-life with lower economic and technical risk requires prediction of cycle life with quantified uncertainty, which is still lacking. The interpretability (i.e., the reason for high prediction accuracy) of these advanced data-driven methods is also worthy of investigation. Here, a Quantile Regression Forest (QRF) model, having the advantage of not assuming any specific distribution of cycle life, is introduced to make cycle life range prediction with uncertainty quantified as the width of the prediction interval, in addition to point predictions with high accuracy. The hyperparameters of the QRF model are optimized with 
    
[^142]: 分层嵌入贝叶斯加性回归树

    Hierarchical Embedded Bayesian Additive Regression Trees. (arXiv:2204.07207v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2204.07207](http://arxiv.org/abs/2204.07207)

    本文提出了分层嵌入BART模型，通过在回归树的末端节点级别上包含随机效应，避免了用户需要指定随机效应结构的需要。模型在混合效应模型数据集上表现优异，能够提供随机效应方差的一致估计。

    

    我们提出了一种简单而强大的贝叶斯加性回归树（BART）扩展，称为分层嵌入BART (HE-BART)。该模型允许在一组回归树的末端节点级别上包含随机效应，使HE-BART成为混合效应模型的非参数替代方案，避免了用户在模型中指定随机效应结构的需要，并保持标准BART的预测和不确定性校准性质。使用模拟和真实世界的例子，我们证明了这种新的扩展在许多标准混合效应模型示例数据集上产生了更优越的预测结果，同时仍然提供了随机效应方差的一致估计。在本文的未来版本中，我们将概述它在更大、更高级的数据集和结构中的应用。

    We propose a simple yet powerful extension of Bayesian Additive Regression Trees which we name Hierarchical Embedded BART (HE-BART). The model allows for random effects to be included at the terminal node level of a set of regression trees, making HE-BART a non-parametric alternative to mixed effects models which avoids the need for the user to specify the structure of the random effects in the model, whilst maintaining the prediction and uncertainty calibration properties of standard BART. Using simulated and real-world examples, we demonstrate that this new extension yields superior predictions for many of the standard mixed effects models' example data sets, and yet still provides consistent estimates of the random effect variances. In a future version of this paper, we outline its use in larger, more advanced data sets and structures.
    
[^143]: 学习组合软提示以用于组合式零样本学习

    Learning to Compose Soft Prompts for Compositional Zero-Shot Learning. (arXiv:2204.03574v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.03574](http://arxiv.org/abs/2204.03574)

    通过学习组合式软提示实现了预测看不见的属性-对象组合，超过了基准数据集上的特定体系结构，并在曲线下面积上平均高10.9％。

    

    我们提出了组合软提示（CSP），这是一种参数高效的学习技术，用于改善大规模预训练的视觉语言模型（VLMs）如CLIP的零样本组合性。我们将CSP开发用于组合式零样本学习，也就是预测看不见的属性-对象组合（例如老猫和小老虎）。VLM具有灵活的文本编码器，可以用自然语言提示表示任意类别，但它们通常在组合零样本基准数据集上表现不如特定任务的体系结构。CSP将定义类别的属性和对象视为可学习的词汇标记。在训练期间，词汇表被调整为识别以多种方式组成令牌的类别（例如老猫和白猫）。在测试时，我们将所学的属性-对象词汇表以新的组合方式重新组合，以识别新的类别。我们展示了CSP在基准数据集上比CLIP平均高10.9个百分点的AUC（曲线下面积指标）。

    We introduce compositional soft prompting (CSP), a parameter-efficient learning technique to improve the zero-shot compositionality of large-scale pretrained vision-language models (VLMs) like CLIP. We develop CSP for compositional zero-shot learning, the task of predicting unseen attribute-object compositions (e.g., old cat and young tiger). VLMs have a flexible text encoder that can represent arbitrary classes as natural language prompts but they often underperform task-specific architectures on the compositional zero-shot benchmark datasets. CSP treats the attributes and objects that define classes as learnable tokens of vocabulary. During training, the vocabulary is tuned to recognize classes that compose tokens in multiple ways (e.g., old cat and white cat). At test time, we recompose the learned attribute-object vocabulary in new combinations to recognize novel classes. We show that CSP outperforms the CLIP on benchmark datasets by an average of 10.9 percentage points on AUC. CSP
    
[^144]: 随机向量功能链接网络：最新进展、应用和未来方向

    Random vector functional link network: recent developments, applications, and future directions. (arXiv:2203.11316v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2203.11316](http://arxiv.org/abs/2203.11316)

    本文介绍随机向量功能链接（RVFL）网络的演变、改进和应用，该网络具有快速训练速度、直观的连接、简单的结构和通用近似能力，是一种有前途的随机化神经网络。

    

    神经网络已成功应用于分类、回归、聚类等各个领域。通常情况下，基于BP的迭代训练方法被用于训练神经网络，但是这会导致出现局部最小值的问题、对学习率敏感以及收敛缓慢等问题。为了解决这些问题，提出了基于随机化的神经网络，例如随机向量功能链接（RVFL）网络。RVFL模型具有许多特点，如快速训练速度、直观的连接、简单的结构和通用近似能力，使其成为一种可行的随机化神经网络。本文介绍了RVFL模型演变的第一个全面综述文献，可作为新手和实践者的广泛总结。我们讨论了浅层RVFL、集成RVFL、深层RVFL和集成深层RVFL模型。RVFL模型的变化、改进和应用得到了详细讨论。

    Neural networks have been successfully employed in various domains such as classification, regression and clustering, etc. Generally, the back propagation (BP) based iterative approaches are used to train the neural networks, however, it results in the issues of local minima, sensitivity to learning rate and slow convergence. To overcome these issues, randomization based neural networks such as random vector functional link (RVFL) network have been proposed. RVFL model has several characteristics such as fast training speed, direct links, simple architecture, and universal approximation capability, that make it a viable randomized neural network. This article presents the first comprehensive review of the evolution of RVFL model, which can serve as the extensive summary for the beginners as well as practitioners. We discuss the shallow RVFLs, ensemble RVFLs, deep RVFLs and ensemble deep RVFL models. The variations, improvements and applications of RVFL models are discussed in detail. M
    
[^145]: 鲁棒PAC$^m$: 在模型规格不准确和存在异常值情况下训练集成模型

    Robust PAC$^m$: Training Ensemble Models Under Model Misspecification and Outliers. (arXiv:2203.01859v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.01859](http://arxiv.org/abs/2203.01859)

    对于存在模型规格不准确和异常值情况下的集成学习，本文提出了一个新的鲁棒自由能量准则，通过将广义对数得分函数与PAC$^m$结合，实现了更好的模型性能。

    

    传统的贝叶斯学习在模型规格不准确和存在异常值的情况下已知存在泛化能力的不足。PAC-Bayes理论证明了贝叶斯学习所最小化的自由能量准则是在假设未被异常值污染的采样分布下，对Gibbs预测器（即从后验随机抽取的单个模型）的泛化误差的一个上界。该观点提供了贝叶斯学习在模型规格不准确且需要集成，以及数据受到异常值影响时的局限性的证明。最近的工作中，推导出了PAC-Bayes上界 - 称为PAC$^m$ - 引入了自由能量度量，可考虑集合预测器的性能，从而获得在模型不准确的情况下提高模型性能。本文提出了一种新的鲁棒自由能量准则，将广义对数得分函数与PAC$^m$集成上界相结合。建议的自由能量训练...（摘要未完，详情请查看原文）

    Standard Bayesian learning is known to have suboptimal generalization capabilities under model misspecification and in the presence of outliers. PAC-Bayes theory demonstrates that the free energy criterion minimized by Bayesian learning is a bound on the generalization error for Gibbs predictors (i.e., for single models drawn at random from the posterior) under the assumption of sampling distributions uncontaminated by outliers. This viewpoint provides a justification for the limitations of Bayesian learning when the model is misspecified, requiring ensembling, and when data is affected by outliers. In recent work, PAC-Bayes bounds - referred to as PAC$^m$ - were derived to introduce free energy metrics that account for the performance of ensemble predictors, obtaining enhanced performance under misspecification. This work presents a novel robust free energy criterion that combines the generalized logarithm score function with PAC$^m$ ensemble bounds. The proposed free energy training 
    
[^146]: StratDef: 机器学习恶意软件检测对抗攻击的战略防御

    StratDef: Strategic Defense Against Adversarial Attacks in ML-based Malware Detection. (arXiv:2202.07568v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.07568](http://arxiv.org/abs/2202.07568)

    本文提出了一个名为StratDef的移动目标防御方法的战略防御系统，针对机器学习恶意软件检测的防御措施进行了全面评估。StratDef动态地和策略地选择最佳模型，以增加攻击者的不确定性，同时最小化对攻击的影响，使其具有很高的对抗鲁棒性。

    

    多年来针对机器学习模型的对抗攻击防御大多集中在图像识别领域。尽管恶意软件检测领域的重要性很高，但它却受到了较少的关注。而且，大多数关于这些防御措施的研究都集中在一些方法上，而没有具体的应用策略。本文提出了一个名为StratDef的战略防御系统，它基于移动目标防御方法。我们解决了涉及模型系统化构建、选择和战略使用的挑战，以最大化对抗鲁棒性。在对抗机器学习领域的关键方面，如攻击可转移性，StratDef动态地和策略地选择最佳模型，以增加攻击者的不确定性，同时最小化对攻击的影响。我们为机器学习恶意软件检测的防御措施首次进行了全面评估，其中我们的威胁模型探索了不同级别的威胁、攻击者的知识水平等。

    Over the years, most research towards defenses against adversarial attacks on machine learning models has been in the image recognition domain. The malware detection domain has received less attention despite its importance. Moreover, most work exploring these defenses has focused on several methods but with no strategy when applying them. In this paper, we introduce StratDef, which is a strategic defense system based on a moving target defense approach. We overcome challenges related to the systematic construction, selection, and strategic use of models to maximize adversarial robustness. StratDef dynamically and strategically chooses the best models to increase the uncertainty for the attacker while minimizing critical aspects in the adversarial ML domain, like attack transferability. We provide the first comprehensive evaluation of defenses against adversarial attacks on machine learning for malware detection, where our threat model explores different levels of threat, attacker know
    
[^147]: 关于分摊优化的教程

    Tutorial on amortized optimization. (arXiv:2202.00665v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.00665](http://arxiv.org/abs/2202.00665)

    该教程介绍了分摊优化的基础，并总结了其在变分推断、稀疏编码、元学习、控制、强化学习、凸优化、最优传输和深度平衡网络中的应用。

    

    优化是一种普遍的建模工具，经常在反复解决相同问题的情况下使用。分摊优化方法使用学习来预测这些设置中问题的解决方案，利用相似问题实例之间的共享结构。这些方法在变分推断和强化学习中至关重要，能够比不使用分摊的传统优化方法快几个数量级地解决优化问题。本次教程介绍了这些进步背后的分摊优化基础，并概述了它们在变分推断、稀疏编码、基于梯度的元学习、控制、强化学习、凸优化、最优传输和深度平衡网络中的应用。本教程的源代码可在https://github.com/facebookresearch/amortized-optimization-tutorial上获得。

    Optimization is a ubiquitous modeling tool and is often deployed in settings which repeatedly solve similar instances of the same problem. Amortized optimization methods use learning to predict the solutions to problems in these settings, exploiting the shared structure between similar problem instances. These methods have been crucial in variational inference and reinforcement learning and are capable of solving optimization problems many orders of magnitudes times faster than traditional optimization methods that do not use amortization. This tutorial presents an introduction to the amortized optimization foundations behind these advancements and overviews their applications in variational inference, sparse coding, gradient-based meta-learning, control, reinforcement learning, convex optimization, optimal transport, and deep equilibrium networks. The source code for this tutorial is available at https://github.com/facebookresearch/amortized-optimization-tutorial.
    
[^148]: 深度强化学习，一本教材

    Deep Reinforcement Learning, a textbook. (arXiv:2201.02135v5 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2201.02135](http://arxiv.org/abs/2201.02135)

    深度强化学习是一种引人注目的技术，计算机程序通过尝试、得到反馈和再次尝试来自我解决困难问题，甚至在某些领域比最好的人类表现更好。

    

    最近，深度强化学习引起了很多关注。在自动驾驶、游戏玩法、分子重组和机器人等各个领域都取得了惊人的成果。在所有这些领域中，计算机程序已经学会了自我解决困难问题。它们已经学会了飞行模型直升机和进行像环和翻滚这样的特技动作。在某些应用中，它们甚至比最好的人类表现得更好，例如 Atari、围棋、扑克和星际争霸。深度强化学习探索复杂环境的方式让我们想起了孩子们的学习方式，通过尝试、得到反馈和再次尝试来充满乐趣地学习。计算机似乎真正具备了人类学习的方面，这触动了人工智能梦想的核心。研究成功引起了教育工作者的关注，学校开始开设相关课程。本书的目的就是提供深度强化学习的全面概述。

    Deep reinforcement learning has gathered much attention recently. Impressive results were achieved in activities as diverse as autonomous driving, game playing, molecular recombination, and robotics. In all these fields, computer programs have taught themselves to solve difficult problems. They have learned to fly model helicopters and perform aerobatic manoeuvers such as loops and rolls. In some applications they have even become better than the best humans, such as in Atari, Go, poker and StarCraft. The way in which deep reinforcement learning explores complex environments reminds us of how children learn, by playfully trying out things, getting feedback, and trying again. The computer seems to truly possess aspects of human learning; this goes to the heart of the dream of artificial intelligence. The successes in research have not gone unnoticed by educators, and universities have started to offer courses on the subject. The aim of this book is to provide a comprehensive overview of
    
[^149]: GREED: 一种学习图距离函数的神经框架

    GREED: A Neural Framework for Learning Graph Distance Functions. (arXiv:2112.13143v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.13143](http://arxiv.org/abs/2112.13143)

    GREED是一种可以学习和预测图编辑距离（GED和SED）的神经网络框架，通过多任务学习来确保距离保持度量属性并使SED的高效计算成为可能，比现有最先进的方法表现更好。

    

    在各种图的距离函数中，图和子图编辑距离（GED和SED）是最受欢迎和最具有表现力的度量之一。不幸的是，对于它们的精确计算都是NP难问题。为了克服这个计算难点，在多项式时间内学习和预测编辑距离的神经方法受到了广泛的关注。在取得相当进展的同时，存在需要解决的局限性。首先，近似距离函数的功效不仅在于其逼近精度，还在于其属性的保持。虽然GED是一个度量，但其神经逼近并不提供这样的保证。这一点禁止了它们在依赖于度量距离函数的高阶任务（如聚类或索引）中的使用。其次，由于SED是不对称的，因此一些现有的GED框架无法扩展到SED。在这项工作中，我们设计了一种名为GREED的新型连锁图神经网络，通过多任务学习预测GED和SED距离。所提出的框架通过确保学习的距离保持度量属性，并使SED的高效计算成为可能，克服了以前方法的局限性。在基准数据集上进行的广泛实验表明，GREED优于现有的最先进方法。

    Among various distance functions for graphs, graph and subgraph edit distances (GED and SED respectively) are two of the most popular and expressive measures. Unfortunately, exact computations for both are NP-hard. To overcome this computational bottleneck, neural approaches to learn and predict edit distance in polynomial time have received much interest. While considerable progress has been made, there exist limitations that need to be addressed. First, the efficacy of an approximate distance function lies not only in its approximation accuracy, but also in the preservation of its properties. To elaborate, although GED is a metric, its neural approximations do not provide such a guarantee. This prohibits their usage in higher order tasks that rely on metric distance functions, such as clustering or indexing. Second, several existing frameworks for GED do not extend to SED due to SED being asymmetric. In this work, we design a novel siamese graph neural network called GREED, which thr
    
[^150]: 路用户检测的概率方法

    Probabilistic Approach for Road-Users Detection. (arXiv:2112.01360v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2112.01360](http://arxiv.org/abs/2112.01360)

    本文介绍了一种用于缓解深度目标检测模型中过于自信预测问题的方法，通过引入一种新颖的概率层来避免传统的预测层，实验证明该方法能够减少假阳性中的过度自信。

    

    自动驾驶应用中的目标检测意味着对语义对象的检测和跟踪通常是城市驾驶环境的特色，如行人和车辆。目前最先进的基于深度学习的目标检测中存在假阳性问题，这些问题通常带有过于自信的得分。在自动驾驶和其他关键的机器人感知领域，这是非常不希望看到的，因为涉及安全问题。本文提出了一种方法来缓解过于自信的预测问题，通过在测试中引入一种新颖的概率层，向深度目标检测网络中添加这种概率层。建议的方法避免了传统的Sigmoid或Softmax预测层，这些层通常会产生过于自信的预测。实验证明，所提出的技术能够减少假阳性中的过度自信，而不会降低真阳性的性能。该方法在2D-KITTI目标检测中进行了验证，使用了YOLOV4和S。

    Object detection in autonomous driving applications implies that the detection and tracking of semantic objects are commonly native to urban driving environments, as pedestrians and vehicles. One of the major challenges in state-of-the-art deep-learning based object detection are false positives which occur with overconfident scores. This is highly undesirable in autonomous driving and other critical robotic-perception domains because of safety concerns. This paper proposes an approach to alleviate the problem of overconfident predictions by introducing a novel probabilistic layer to deep object detection networks in testing. The suggested approach avoids the traditional Sigmoid or Softmax prediction layer which often produces overconfident predictions. It is demonstrated that the proposed technique reduces overconfidence in the false positives without degrading the performance on the true positives. The approach is validated on the 2D-KITTI objection detection through the YOLOV4 and S
    
[^151]: 流数据随机逼近算法的非渐进分析

    Non-Asymptotic Analysis of Stochastic Approximation Algorithms for Streaming Data. (arXiv:2109.07117v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.07117](http://arxiv.org/abs/2109.07117)

    该论文介绍了流数据随机逼近算法的非渐近收敛速度，包括随机梯度下降、小批量SG和时间变化的小批量SG算法以及它们的迭代平均值，同时展示了加速收敛的方法和同时提供方差减少和加速收敛的优势。

    

    我们引入了一个流式框架来分析随机逼近/优化问题。这个流式框架类似于使用逐步到达的时间变化的小批次来解决优化问题。我们提供了各种基于梯度的算法的非渐近收敛速度；这包括著名的随机梯度下降（SG）算法（也称为Robbins-Monro算法），小批量SG和时间变化的小批量SG算法，以及它们的迭代平均值（也称为Polyak-Ruppert平均）。我们展示了：i）如何通过根据时间变化的小批次来选择学习速率来加速收敛；ii）Polyak-Ruppert平均值在达到Cramer-Rao下界方面实现了最优收敛；iii）时间变化的小批次与Polyak-Ruppert平均值结合使用可以同时提供方差减少和加速收敛，这对于许多学习问题（如在线，顺序和大规模）都是有利的。

    We introduce a streaming framework for analyzing stochastic approximation/optimization problems. This streaming framework is analogous to solving optimization problems using time-varying mini-batches that arrive sequentially. We provide non-asymptotic convergence rates of various gradient-based algorithms; this includes the famous Stochastic Gradient (SG) descent (a.k.a. Robbins-Monro algorithm), mini-batch SG and time-varying mini-batch SG algorithms, as well as their iterated averages (a.k.a. Polyak-Ruppert averaging). We show i) how to accelerate convergence by choosing the learning rate according to the time-varying mini-batches, ii) that Polyak-Ruppert averaging achieves optimal convergence in terms of attaining the Cramer-Rao lower bound, and iii) how time-varying mini-batches together with Polyak-Ruppert averaging can provide variance reduction and accelerate convergence simultaneously, which is advantageous for many learning problems, such as online, sequential, and large-scale
    
[^152]: 一种深度神经网络中表示学习的理论给出了核方法的深度泛化。

    A theory of representation learning in deep neural networks gives a deep generalisation of kernel methods. (arXiv:2108.13097v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2108.13097](http://arxiv.org/abs/2108.13097)

    本文提出了一种新的无限宽度限制——贝叶斯表示学习限制，旨在解决标准无限宽度限制消除表示学习的问题。该方法可以实现类似于有限宽度模型中的表示学习效果，并保留标准无限宽度限制的简单性。

    

    现代深度机器学习方法的成功基于它们跨多个层次对输入进行变换以建立良好的高级表示能力。因此，理解这种表示学习过程至关重要。然而，常规的理论方法（正式为NNGPs）涉及无限宽限制消除了表示学习。因此，我们开发了一种新的无限宽限制——贝叶斯表示学习限制，它展现了在有限宽度模型中镜像表示学习的效果，同时保留了一些标准无限宽度限制的简单性。特别地，我们表明在贝叶斯表示学习极限下的深层高斯过程（DGPs）具有确切的多元高斯后验分布，后验协方差可以通过优化一种可解释目标得到，该目标结合了增强性能的对数似然和一系列的KL-散度，使得后验分布接近先验分布。

    The successes of modern deep machine learning methods are founded on their ability to transform inputs across multiple layers to build good high-level representations. It is therefore critical to understand this process of representation learning. However, standard theoretical approaches (formally NNGPs) involving infinite width limits eliminate representation learning. We therefore develop a new infinite width limit, the Bayesian representation learning limit, that exhibits representation learning mirroring that in finite-width models, yet at the same time, retains some of the simplicity of standard infinite-width limits. In particular, we show that Deep Gaussian processes (DGPs) in the Bayesian representation learning limit have exactly multivariate Gaussian posteriors, and the posterior covariances can be obtained by optimizing an interpretable objective combining a log-likelihood to improve performance with a series of KL-divergences which keep the posteriors close to the prior. We
    
[^153]: MRCpy：一种用于最小化风险分类器的库

    MRCpy: A Library for Minimax Risk Classifiers. (arXiv:2108.01952v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2108.01952](http://arxiv.org/abs/2108.01952)

    MRCpy是一种用于实现最小化风险分类器的Python库，它基于鲁棒风险最小化技术，可以利用0-1损失并提供了多种分类方法，其中一些提供了紧密的期望损失界限。

    

    目前现有的监督分类库都是基于经验风险最小化和使用代理损失技术的。本文介绍MRCpy库，该库实现了基于鲁棒风险最小化的最小化风险分类器（MRC），并可利用0-1损失。这种技术产生了许多分类方法，可以提供紧密的期望损失界限。MRCpy为不同变量的MRC提供了统一的接口，并遵循流行Python库的标准。此外，MRCpy还提供了实现一些流行技术的功能，这些技术可以看作是MRC，例如L1正则化逻辑回归，0-1对抗性和最大熵机。此外，MRCpy还实现了最近的特征映射，如傅里叶，ReLU和阈值特征。该库采用面向对象的方法设计，方便协作者和用户。

    Existing libraries for supervised classification implement techniques that are based on empirical risk minimization and utilize surrogate losses. We present MRCpy library that implements minimax risk classifiers (MRCs) that are based on robust risk minimization and can utilize 0-1-loss. Such techniques give rise to a manifold of classification methods that can provide tight bounds on the expected loss. MRCpy provides a unified interface for different variants of MRCs and follows the standards of popular Python libraries. The presented library also provides implementation for popular techniques that can be seen as MRCs such as L1-regularized logistic regression, zero-one adversarial, and maximum entropy machines. In addition, MRCpy implements recent feature mappings such as Fourier, ReLU, and threshold features. The library is designed with an object-oriented approach that facilitates collaborators and users.
    
[^154]: 一种增加神经网络对数据质量问题的鲁棒性的调制层

    A Modulation Layer to Increase Neural Network Robustness Against Data Quality Issues. (arXiv:2107.08574v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.08574](http://arxiv.org/abs/2107.08574)

    提出了一种新颖的神经网络修正方法，用于缓解低质量和缺失数据的影响，具备神经调制特征，通过一个额外的输入的函数替换了全连接层的固定权重，使得在测试中具有调制层的模型对于数据质量的降解更加鲁棒，同时也能够节省训练时间并且不会受到插补错误的影响。

    

    数据缺失和质量是机器学习中常见的问题，特别是在高风险应用领域，如医疗保健。开发者通常只使用高质量数据精心筛选出的数据集来训练机器学习模型；然而，这会降低这些模型在生产环境中的效用。本文提出了一种新颖的神经网络修正方法，用于缓解低质量和缺失数据的影响，其中利用一个额外的输入的函数替换了全连接层的固定权重。这受启发于生物神经网络中的神经调制，皮质可以根据输入的可靠性和其他数据的存在程度上下调节输入。在测试中，使用可靠性得分作为调制信号，发现具有调制层的模型对于数据质量的降解（包括额外的缺失数据）更加鲁棒。这些模型优于插补方法，因为它们通过完全跳过插补过程节省了训练时间，并且不会受到插补错误的影响。

    Data missingness and quality are common problems in machine learning, especially for high-stakes applications such as healthcare. Developers often train machine learning models on carefully curated datasets using only high quality data; however, this reduces the utility of such models in production environments. We propose a novel neural network modification to mitigate the impacts of low quality and missing data which involves replacing the fixed weights of a fully-connected layer with a function of an additional input. This is inspired from neuromodulation in biological neural networks where the cortex can up- and down-regulate inputs based on their reliability and the presence of other data. In testing, with reliability scores as a modulating signal, models with modulating layers were found to be more robust against degradation of data quality, including additional missingness. These models are superior to imputation as they save on training time by completely skipping the imputatio
    
[^155]: Auto-NBA: 针对网络、比特宽度和加速器三个联合空间进行高效搜索的算法

    Auto-NBA: Efficient and Effective Search Over the Joint Space of Networks, Bitwidths, and Accelerators. (arXiv:2106.06575v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.06575](http://arxiv.org/abs/2106.06575)

    Auto-NBA是一种能够高效搜索深度神经网络加速器的算法，它可以同时考虑优化网络、比特宽度和加速器三个耦合的方面。

    

    要同时优化和加速深度神经网络，需要联合考虑三个不同却高度耦合的方面：网络、比特宽度和加速器。然而，联合搜索面临的挑战尚未被完全理解和解决。这些关键挑战包括（1）是否扩大由于巨大联合空间而导致的内存消耗，还是采用次优设计，（2）加速器设计空间的离散性与网络和比特宽度设计空间相耦合且不同，以及（3）网络-加速器联合搜索中的“鸡生蛋蛋生鸡”问题：即联合搜索需要计算操作的硬件成本，然而在搜索期间，尚不知道整个网络的最佳加速器，因此无法计算这些成本。为了解决这些问题，我们提出了一个名为Auto-NBA的框架，以实现针对网络、比特宽度和加速器三个联合空间的高效搜索。

    While maximizing deep neural networks' (DNNs') acceleration efficiency requires a joint search/design of three different yet highly coupled aspects, including the networks, bitwidths, and accelerators, the challenges associated with such a joint search have not yet been fully understood and addressed. The key challenges include (1) the dilemma of whether to explode the memory consumption due to the huge joint space or achieve sub-optimal designs, (2) the discrete nature of the accelerator design space that is coupled yet different from that of the networks and bitwidths, and (3) the chicken and egg problem associated with network-accelerator co-search, i.e., co-search requires operation-wise hardware cost, which is lacking during search as the optimal accelerator depending on the whole network is still unknown during search. To tackle these daunting challenges towards optimal and fast development of DNN accelerators, we propose a framework dubbed Auto-NBA to enable jointly searching fo
    
[^156]: GMM的鲁棒模型选择和近似正确学习

    Robust Model Selection and Nearly-Proper Learning for GMMs. (arXiv:2106.02774v2 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2106.02774](http://arxiv.org/abs/2106.02774)

    本文研究了一元高斯混合模型（GMMs）鲁棒模型选择的问题，提出了一个鲁棒算法，可以在对抗性扰动下近似正确地学习GMMs，实现了最佳样本复杂度，能够近似确定拟合分布所需的最少组件数。

    

    在学习理论中，通常假定数据是从有限混合模型生成的。但是如果事先不知道组分数会发生什么呢？估计组分数的问题，在本身上是很重要的，但实际上就算是没有有效算法，更不用说能容忍对抗性扰动了。本文研究了一元高斯混合模型（GMMs）鲁棒模型选择的问题。我们会从一个与$k$个组分的GMM $\epsilon$ -close的分布中产生$\textsf{poly}(k / \epsilon)$个样本，用$\textsf{poly}(k / \epsilon)$时间构建一个有$\widetilde{O}(k)$个组件的GMM，可在$\widetilde {O} (\epsilon)$内近似表示分布。因此，我们能够近似确定拟合分布所需的最少组件数。在本研究之前，唯一已知的有效算法需要至少 $O(k \log \log n)$ 个组件才能完成此任务，这已近乎达到了极限。此外，我们还证明了我们的算法几乎是正确的，即，其具有最优的样本复杂度，仅具有对数因子。最后，我们证明了我们的结果使得我们能够在对抗扰动下鲁棒地学习GMMs。

    In learning theory, a standard assumption is that the data is generated from a finite mixture model. But what happens when the number of components is not known in advance? The problem of estimating the number of components, also called model selection, is important in its own right but there are essentially no known efficient algorithms with provable guarantees let alone ones that can tolerate adversarial corruptions. In this work, we study the problem of robust model selection for univariate Gaussian mixture models (GMMs). Given $\textsf{poly}(k/\epsilon)$ samples from a distribution that is $\epsilon$-close in TV distance to a GMM with $k$ components, we can construct a GMM with $\widetilde{O}(k)$ components that approximates the distribution to within $\widetilde{O}(\epsilon)$ in $\textsf{poly}(k/\epsilon)$ time. Thus we are able to approximately determine the minimum number of components needed to fit the distribution within a logarithmic factor. Prior to our work, the only known 
    
[^157]: 两步强化学习用于非线性最优调节器无模型重新设计

    Two-step reinforcement learning for model-free redesign of nonlinear optimal regulator. (arXiv:2103.03808v3 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2103.03808](http://arxiv.org/abs/2103.03808)

    强化学习可以无模型地重新设计针对非线性系统的最优控制器。为了提高学习性能并减少实验次数，提出了一个无模型的两步设计方法，先设计线性控制律达到初步控制，再使用强化学习进一步优化。

    

    在许多实际控制应用中，闭环系统的性能随着工厂特性的改变而随时间降低。因此，需要重新设计控制器，而不需要进行系统建模过程，这对于闭环系统来说通常是困难的。强化学习（RL）是一种有前途的方法，它使非线性动态系统的无模型最优控制器可以仅基于闭环系统测量数据重新设计。然而，RL的学习过程通常需要使用控制不良的系统进行相当数量的试错实验，这可能会加速工厂的磨损。为了克服这个限制，我们提出了一种无模型的两步设计方法，它提高了RL在未知非线性系统的最优调节器重新设计问题中的暂态学习性能。具体来说，我们首先设计了一个线性控制律，以实现一定程度的控制性能

    In many practical control applications, the performance level of a closed-loop system degrades over time due to the change of plant characteristics. Thus, there is a strong need for redesigning a controller without going through the system modeling process, which is often difficult for closed-loop systems. Reinforcement learning (RL) is one of the promising approaches that enable model-free redesign of optimal controllers for nonlinear dynamical systems based only on the measurement of the closed-loop system. However, the learning process of RL usually requires a considerable number of trial-and-error experiments using the poorly controlled system that may accumulate wear on the plant. To overcome this limitation, we propose a model-free two-step design approach that improves the transient learning performance of RL in an optimal regulator redesign problem for unknown nonlinear systems. Specifically, we first design a linear control law that attains some degree of control performance i
    
[^158]: 顺序随机实验的弱信号渐近行为

    Weak Signal Asymptotics for Sequentially Randomized Experiments. (arXiv:2101.09855v5 [math.ST] UPDATED)

    [http://arxiv.org/abs/2101.09855](http://arxiv.org/abs/2101.09855)

    本文使用弱信号渐近行为的方法研究了一类顺序随机实验，认为这类顺序实验的样本路径会弱收敛到扩散极限，并能获得关于几种顺序实验的后悔和信念演变的多个见解。

    

    我们使用弱信号渐近行为的方法研究了一类顺序随机实验，包括解决多臂赌博机问题的实验。在一个$n$个时间步骤的实验中，我们让不同动作的平均奖励间隙按照$1/\sqrt{n}$的比例缩放，以保持学习任务的难度随着$n$的增长而保持不变。在这种情况下，我们发现一类顺序随机实验的样本路径会弱收敛到扩散极限，其中，臂选择的概率会随着状态的变化而持续变化，并在满足连续性假设的情况下进行调整。扩散极限使我们能够推导出精细的、特定于实例的随机动力学特征，并获得关于几种顺序实验的后悔和信念演变的多个见解，包括汤普森采样（但不包括不满足我们连续性假设的UCB）。我们展示了所有顺序实验的表现都能在长时间内稳定。

    We use the lens of weak signal asymptotics to study a class of sequentially randomized experiments, including those that arise in solving multi-armed bandit problems. In an experiment with $n$ time steps, we let the mean reward gaps between actions scale to the order $1/\sqrt{n}$ so as to preserve the difficulty of the learning task as $n$ grows. In this regime, we show that the sample paths of a class of sequentially randomized experiments -- adapted to this scaling regime and with arm selection probabilities that vary continuously with state -- converge weakly to a diffusion limit, given as the solution to a stochastic differential equation. The diffusion limit enables us to derive refined, instance-specific characterization of stochastic dynamics, and to obtain several insights on the regret and belief evolution of a number of sequential experiments including Thompson sampling (but not UCB, which does not satisfy our continuity assumption). We show that all sequential experiments wh
    
[^159]: 递归、演化和自我意识

    Recursion, evolution and conscious self. (arXiv:2001.11825v4 [cs.LO] UPDATED)

    [http://arxiv.org/abs/2001.11825](http://arxiv.org/abs/2001.11825)

    本文介绍并研究了一种基于自我参照的学习理论，结论与生物学、神经科学的科学发现一致，并提供了大量解释，包括演化和人脑的功能和学习能力。

    

    我们引入并研究了一种学习理论，它大致是自动的，即仅需要最少量的初始编程，并且基于自我参照的潜在计算现象（即算法具有其程序作为输入的潜在能力）。结论与生物学和神经科学的科学发现一致，并提供了大量解释，包括演化（与达尔文主义相结合）以及人脑的功能和学习能力（最重要的是），正如我们在自己身上感知到的。

    We introduce and study a learning theory which is roughly automatic, that is, it does not require but a minimum of initial programming, and is based on the potential computational phenomenon of self-reference, (i.e. the potential ability of an algorithm to have its program as an input).  The conclusions agree with scientific findings in both biology and neuroscience and provide a plethora of explanations both (in conjunction with Darwinism) about evolution, as well as for the functionality and learning capabilities of human brain, (most importantly), as we perceive them in ourselves.
    
[^160]: 模拟器：理解在中等置信度条件下的自适应采样

    The Simulator: Understanding Adaptive Sampling in the Moderate-Confidence Regime. (arXiv:1702.05186v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1702.05186](http://arxiv.org/abs/1702.05186)

    提出一种名为模拟器的新技术用于分析自适应采样。将重点放在了区分好的采样策略和坏采样策略的难度上。在纯探索场景的结构化多臂赌博问题中应用了该技术，展示了有中等置信度的样本复杂度和文献中在 $\delta \to 0$ 时得到的渐近复杂度之间存在着实质性差异，并且还证明了作为顶部-k问题的第一个基于实例的下界。

    

    我们提出了一种新的技术，称为“模拟器”，用于分析自适应采样。我们的方法与现有方法不同，它不考虑任何固定采样策略可以收集多少信息，而是考虑在给定的有限数据收集时间内，区分好的采样策略和坏的采样策略有多难。这种视角的改变使我们能够匹配Fano和变量测量技术的优点，而不会陷入任何一种方法的局限性中。为了具体说明，我们将我们的技术应用到了一个固定置信水平的纯探索场景中的结构化多臂赌博问题，我们展示了在均值限制下，有中等置信度的样本复杂度和文献中在 $\delta \to 0$ 时得到的渐近复杂度之间存在着实质性差异。我们还证明了作为顶部-k问题的第一个基于实例的下界，其包括适当的对数因子。

    We propose a novel technique for analyzing adaptive sampling called the {\em Simulator}. Our approach differs from the existing methods by considering not how much information could be gathered by any fixed sampling strategy, but how difficult it is to distinguish a good sampling strategy from a bad one given the limited amount of data collected up to any given time. This change of perspective allows us to match the strength of both Fano and change-of-measure techniques, without succumbing to the limitations of either method. For concreteness, we apply our techniques to a structured multi-arm bandit problem in the fixed-confidence pure exploration setting, where we show that the constraints on the means imply a substantial gap between the moderate-confidence sample complexity, and the asymptotic sample complexity as $\delta \to 0$ found in the literature. We also prove the first instance-based lower bounds for the top-k problem which incorporate the appropriate log-factors. Moreover, o
    

