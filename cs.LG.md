# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty.](http://arxiv.org/abs/2401.15077) | EAGLE是一个无损加速语言模型推理的框架，通过在次顶层特征层面上自回归推理，并解决采样不确定性问题，实现了比传统方法更快3倍的速度。 |
| [^2] | [Expert with Clustering: Hierarchical Online Preference Learning Framework.](http://arxiv.org/abs/2401.15062) | 本研究提出了一种名为Expert with Clustering (EWC)的分层在线偏好学习框架，它利用聚类技术和专家建议的预测来加速用户偏好学习，并通过一种新颖的损失引导距离度量生成更具代表性的聚类中心。 |
| [^3] | [Fully Independent Communication in Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2401.15059) | 该论文研究了多智能体强化学习中的完全独立通信，并提出了一种新的学习方案，证明独立智能体仍可以学习通信策略。此外，论文还探讨了通信在不同网络容量下的影响。 |
| [^4] | [Health Text Simplification: An Annotated Corpus for Digestive Cancer Education and Novel Strategies for Reinforcement Learning.](http://arxiv.org/abs/2401.15043) | 该论文介绍了一个用于健康文本简化研究的消化癌症教育材料的注释语料库，并探索了基于大型语言模型的简化方法，包括微调、增强学习、增强学习与人类反馈、领域自适应和基于提示的应用。 |
| [^5] | [On the generalization capacity of neural networks during generic multimodal reasoning.](http://arxiv.org/abs/2401.15030) | 本研究评估了不同神经网络架构在多模态泛化方面的能力，并发现具有多个注意力层或利用交叉注意机制的模型表现更好。 |
| [^6] | [SliceGPT: Compress Large Language Models by Deleting Rows and Columns.](http://arxiv.org/abs/2401.15024) | SliceGPT是一种新的事后训练稀疏化方案，通过将每个权重矩阵替换为较小的矩阵以减小网络的维度，可以在保持高任务性能的同时减少模型参数。 |
| [^7] | [Machine learning-based analysis of glioma tissue sections: a review.](http://arxiv.org/abs/2401.15022) | 机器学习技术在胶质瘤组织切片分析中具有诊断和预测的潜力，当前研究聚焦于成人型弥漫性胶质瘤的苏木精和伊红染色组织切片，以及对该疾病的分类、分级、分子标记预测和生存预测等临床任务。 |
| [^8] | [Enhancement of a Text-Independent Speaker Verification System by using Feature Combination and Parallel-Structure Classifiers.](http://arxiv.org/abs/2401.15018) | 本文通过使用特征组合和并行结构分类器来增强无文本约束的说话人验证系统的性能，在特征提取和分类阶段进行了探索，提出了不同声学特征的组合比较，并针对传统支持向量机分类器的弱点进行了改进。 |
| [^9] | [Graph-based Active Learning for Entity Cluster Repair.](http://arxiv.org/abs/2401.14992) | 本文介绍了一种基于图的主动学习方法，用于修复实体聚类中的错误，该方法利用了基础相似性图推导出的图度量，并结合了主动学习来解决数据训练不足的挑战。 |
| [^10] | [Mapping-to-Parameter Nonlinear Functional Regression with Novel B-spline Free Knot Placement Algorithm.](http://arxiv.org/abs/2401.14989) | 提出了一种新的非线性函数回归方法，通过将函数数据映射到有限维参数空间，并使用新的自由节点放置算法来同时近似多个函数。该算法根据输入或输出函数的局部复杂性来决定节点位置，性能稳健。 |
| [^11] | [Embedding-based search in JetBrains IDEs.](http://arxiv.org/abs/2401.14975) | 该论文介绍了在JetBrains IDE中实施的基于嵌入式的搜索功能，通过机器学习方法提高了搜索项的可发现性。 |
| [^12] | [Discovering group dynamics in synchronous time series via hierarchical recurrent switching-state models.](http://arxiv.org/abs/2401.14973) | 通过分层的循环切换状态模型，我们可以无监督地同时解释系统级和个体级的动态，从而更好地建模同步时间序列中的群体动态。 |
| [^13] | [End-To-End Set-Based Training for Neural Network Verification.](http://arxiv.org/abs/2401.14961) | 本论文提出了一种端到端基于集合的训练方法，用于训练鲁棒性神经网络进行形式化验证，并证明该方法能够简化验证过程并有效训练出易于验证的神经网络。 |
| [^14] | [Learning Universal Predictors.](http://arxiv.org/abs/2401.14953) | 本论文探索了将Solomonoff Induction（SI）引入神经网络中的潜力，并使用万能图灵机（UTMs）生成的数据来进行元学习，研究结果表明UTM数据是元学习的有价值资源，可以用来训练能够学习通用预测的神经网络。 |
| [^15] | [Conserve-Update-Revise to Cure Generalization and Robustness Trade-off in Adversarial Training.](http://arxiv.org/abs/2401.14948) | 对抗训练在提高神经网络鲁棒性方面取得了进展，但是牺牲了标准和鲁棒泛化之间的权衡。研究发现，选择性地更新特定层可以显著提高网络学习能力。因此，提出了CURE框架，通过选择性保留、更新和修订权重来解决这一问题。这一方法可以有效解决记忆化和过度拟合问题，并提高鲁棒性和泛化性之间的权衡。 |
| [^16] | [Reinforcement Learning Interventions on Boundedly Rational Human Agents in Frictionful Tasks.](http://arxiv.org/abs/2401.14923) | 本论文介绍了行为模型强化学习（BMRL）框架，在摩擦性任务中，AI代理对有限理性人类代理的参数进行干预，通过对人类策略进行解释，帮助理解行为干预。 |
| [^17] | [Learning Local Control Barrier Functions for Safety Control of Hybrid Systems.](http://arxiv.org/abs/2401.14907) | 该论文提出了一种学习启用的方法，能够构建本地控制屏障函数，以保证广泛类别的非线性混合动力系统的安全性。该方法是高效的，对参考控制器的干预最小，适用于大规模系统，并通过实证评估和比较案例展示了其功效和灵活性。 |
| [^18] | [A structured regression approach for evaluating model performance across intersectional subgroups.](http://arxiv.org/abs/2401.14893) | 这项工作介绍了一种结构回归方法，用于评估模型在不同交叉子群体间的性能。它可以提供可靠的系统性能估计，即使对于很小的子群体。 |
| [^19] | [P3LS: Partial Least Squares under Privacy Preservation.](http://arxiv.org/abs/2401.14884) | P3LS是一种隐私保护的偏最小二乘回归技术，通过使用可移动的随机掩码保护每个数据持有者的隐私，实现了跨组织数据集成和过程建模。 |
| [^20] | [Cross-Space Adaptive Filter: Integrating Graph Topology and Node Attributes for Alleviating the Over-smoothing Problem.](http://arxiv.org/abs/2401.14876) | 本论文提出了一种跨空间自适应滤波器（CSF），可以从图拓扑和节点属性空间中提取自适应频率信息，以减轻图卷积网络（GCN）的过度平滑问题。 |
| [^21] | [Extracting Process-Aware Decision Models from Object-Centric Process Data.](http://arxiv.org/abs/2401.14847) | 本文提出了一种从以对象为中心的过程数据中提取过程感知的决策模型的算法，解决了决策挖掘中的多个对象和顺序约束的问题。 |
| [^22] | [Understanding Domain Generalization: A Noise Robustness Perspective.](http://arxiv.org/abs/2401.14846) | 本文通过研究域泛化算法和经验风险最小化算法在标签噪声下的表现，发现域泛化算法在有限样本训练中显示出隐性的标签噪声鲁棒性，有助于减轻假冗余相关和提高泛化能力，但在真实世界基准数据集上的实验证明，标签噪声鲁棒性并不一定意味着相比ERM具有更好的性能。 |
| [^23] | [Adaptive Point Transformer.](http://arxiv.org/abs/2401.14845) | AdaPT是一个自适应的点云Transformer模型，通过动态减少token的数量来高效处理大型点云，并且可以灵活调整计算成本。Experimental evaluation demonstrates that AdaPT significantly reduces computational cost. |
| [^24] | [GuardML: Efficient Privacy-Preserving Machine Learning Services Through Hybrid Homomorphic Encryption.](http://arxiv.org/abs/2401.14840) | GuardML引入了混合同态加密（HHE）到机器学习中，设计了一种面向终端设备的隐私保护机器学习方案，可以安全地对加密数据进行分类学习，具有高效扩展性。 |
| [^25] | [Endowing Protein Language Models with Structural Knowledge.](http://arxiv.org/abs/2401.14819) | 本研究提出了一种新的框架，通过整合蛋白质结构数据来增强蛋白质语言模型，以解决在实际应用场景中由于依赖大量序列数据和参数集而限制其灵活性和实用性的问题。 |
| [^26] | [On the Limitations of Markovian Rewards to Express Multi-Objective, Risk-Sensitive, and Modal Tasks.](http://arxiv.org/abs/2401.14811) | 本文研究了强化学习中标量、马尔可夫奖励函数的表达能力，并确定了它们的一些局限性。我们发现这些奖励函数无法表达多目标、风险敏感和模态任务中的大部分实例。 |
| [^27] | [Cyclic Group Projection for Enumerating Quasi-Cyclic Codes Trapping Sets.](http://arxiv.org/abs/2401.14810) | 介绍了一种循环群投影的方法，用于列举和评估准循环码中的陷阱集，利用表格技术简化重要采样步骤并利用数学框架阐明投影和抬升变换对拟码字的行为。 |
| [^28] | [Deep Variational Privacy Funnel: General Modeling with Applications in Face Recognition.](http://arxiv.org/abs/2401.14792) | 本文利用信息论隐私漏斗模型开发一种隐私保护的表示学习方法，并将其应用于人脸识别系统，具有适应性和竞争力。 |
| [^29] | [Off-Policy Primal-Dual Safe Reinforcement Learning.](http://arxiv.org/abs/2401.14758) | 该论文提出了离策略原双安全强化学习方法，通过引入保守策略优化和局部策略凸化来解决累积成本估计误差导致的安全约束不满足问题。 |
| [^30] | [Topology-Aware Exploration of Energy-Based Models Equilibrium: Toric QC-LDPC Codes and Hyperbolic MET QC-LDPC Codes.](http://arxiv.org/abs/2401.14749) | 本文介绍了一种在不均匀分布的电荷和不规则网格上实现能量模型均衡的方法，通过使用QC-LDPC码和玻尔兹曼机，将系统的维度扩展，将电荷替换为循环物质，并通过循环移位表示距离。通过这种方法，可以将不规则网格转化为均匀配置，适用于不同的拓扑结构。该方法还解决了代码在图形概率模型中的评估问题，并提供了在不同拓扑下玻尔兹曼机达到均衡状态的严格证明。 |
| [^31] | [Residual Quantization with Implicit Neural Codebooks.](http://arxiv.org/abs/2401.14732) | 本文提出了QINCo，一种神经网络残余量化变体，通过预测每个矢量的专门码书，提高了准确性，并在多个数据集和码书大小上优于现有方法。 |
| [^32] | [A Nonparametric Bayes Approach to Online Activity Prediction.](http://arxiv.org/abs/2401.14722) | 本研究提出了一种非参数贝叶斯方法，以准确预测在线活动的用户数量和达到所需用户参与门槛所需的时间轨迹。该方法通过捕捉用户参与的潜在异质性，提供了实验者在在线实验中重要的决策支持。 |
| [^33] | [Turn-taking and Backchannel Prediction with Acoustic and Large Language Model Fusion.](http://arxiv.org/abs/2401.14717) | 这个论文提出了一种利用声学和语言模型进行交替和回应预测的方法，通过融合这两种模型，可以在口语对话中实现更加自然和对话式的互动。 |
| [^34] | [Mitigating Feature Gap for Adversarial Robustness by Feature Disentanglement.](http://arxiv.org/abs/2401.14707) | 这项研究提出了一种通过特征解缠来缓解对抗鲁棒性中特征差距的方法，该方法明确建模和消除导致特征差距的潜在特征，有效提升了鲁棒性。 |
| [^35] | [FairSample: Training Fair and Accurate Graph Convolutional Neural Networks Efficiently.](http://arxiv.org/abs/2401.14702) | 本文提出了一种名为FairSample的框架，旨在高效训练公平准确的图卷积神经网络。该框架通过对图结构进行纠正，并使用可学习的邻居采样策略来同时减轻图卷积神经网络中的图结构偏见、节点属性偏见和模型参数偏见。 |
| [^36] | [Asymptotic Midpoint Mixup for Margin Balancing and Moderate Broadening.](http://arxiv.org/abs/2401.14696) | 提出了渐进中点混合方法来解决粗细转移学习中的类内崩塌问题。该方法通过逐渐将增强特征移动至类间特征对的中点，实现了边距平衡以及适度扩展边距的效果。 |
| [^37] | [Continuously Evolving Graph Neural Controlled Differential Equations for Traffic Forecasting.](http://arxiv.org/abs/2401.14695) | 本论文提出了一个名为持续演化图神经控制微分方程（CEGNCDE）的方法，用于同时捕捉连续的时空依赖关系和随时间演化的空间依赖关系。实验结果表明，该方法在交通预测任务上有很好的性能。 |
| [^38] | [TA-RNN: an Attention-based Time-aware Recurrent Neural Network Architecture for Electronic Health Records.](http://arxiv.org/abs/2401.14694) | TA-RNN和TA-RNN-AE是两种基于RNN的可解释深度学习架构，用于分析电子健康记录并预测患者的临床结果。这些架构考虑了EHR数据的不规则性和时间间隔，并采用时间嵌入的方法解决了这些问题。 |
| [^39] | [From Blurry to Brilliant Detection: YOLOv5-Based Aerial Object Detection with Super Resolution.](http://arxiv.org/abs/2401.14661) | 基于超分辨率和经过调整的轻量级YOLOv5架构，我们提出了一种创新的方法来解决航空影像中小而密集物体检测的挑战。我们的超分辨率YOLOv5模型采用Transformer编码器块，能够捕捉全局背景和上下文信息，从而在高密度、遮挡条件下提高检测结果。这种轻量级模型不仅准确性更高，而且资源利用效率高，非常适合实时应用。 |
| [^40] | [Validating Climate Models with Spherical Convolutional Wasserstein Distance.](http://arxiv.org/abs/2401.14657) | 引入了球面卷积Wasserstein距离来验证气候模型，相比传统方法更全面地衡量气候模型和再分析数据之间的差异，并应用于评估CMIP成员的模型输出。此外，研究发现CMIP第6阶段模型相较于第5阶段有适度改进。 |
| [^41] | [A Korean Legal Judgment Prediction Dataset for Insurance Disputes.](http://arxiv.org/abs/2401.14654) | 这篇论文介绍了一种面向保险争议的韩国法律判决预测数据集，研究发现在数据有限的情况下，使用句子转换器微调方法可以实现与大型数据集相似的性能。 |
| [^42] | [Omnipredictors for Regression and the Approximate Rank of Convex Functions.](http://arxiv.org/abs/2401.14645) | 全能预测器是一种预测器，其预测的损失小于任意损失下的最佳假设。我们提出了关于损失函数的充分统计量的概念，可以用来最小化任何损失的期望损失。 |
| [^43] | [Inferring Data Preconditions from Deep Learning Models for Trustworthy Prediction in Deployment.](http://arxiv.org/abs/2401.14628) | 本文提出了一种从深度学习模型中推断数据前提的新技术，用于确定其预测的可靠性。该方法使用了从神经网络计算中推导出的规则，并引入了一种新颖的抽象表示，以进行最弱前提推理。 |
| [^44] | [Resilient Practical Test-Time Adaptation: Soft Batch Normalization Alignment and Entropy-driven Memory Bank.](http://arxiv.org/abs/2401.14619) | 本文提出了一种弹性实用的测试时间适应方法，通过软批量归一化对齐和熵驱动的内存库来减轻目标领域转移和非独立同分布问题，并提高数据质量。 |
| [^45] | [Physically Informed Synchronic-adaptive Learning for Industrial Systems Modeling in Heterogeneous Media with Unavailable Time-varying Interface.](http://arxiv.org/abs/2401.14609) | 物理信息同步自适应学习方法（PISAL）用于解决工业系统在异质介质中建模的PDEs问题，通过构建Net1、Net2和NetI来同步逼近解和自适应学习不可用的时间变化界面。 |
| [^46] | [Ricci flow-guided autoencoders in learning time-dependent dynamics.](http://arxiv.org/abs/2401.14591) | 利用Ricci流引导的自编码器方法能够学习非线性动力学，尤其是偏微分方程。该方法通过在训练中学习流形，并使用Ricci流使流形潜空间逐步适应动力学的变化，从而获得更好的表示能力。在实验中，我们展示了该方法在具有周期性和随机性的PDE上的应用，并评估了在分布内和外推场景中的误差。 |
| [^47] | [Diffusion Stochastic Optimization for Min-Max Problems.](http://arxiv.org/abs/2401.14585) | 这里是中文总结出的一句话要点 |
| [^48] | [Design Your Own Universe: A Physics-Informed Agnostic Method for Enhancing Graph Neural Networks.](http://arxiv.org/abs/2401.14580) | 本文提出了一种物理信息引导的无偏方法来增强图神经网络，通过引入附加节点和使用正负权重重连连接来丰富图结构，以解决过度平滑和过度压缩的问题。 |
| [^49] | [GOAt: Explaining Graph Neural Networks via Graph Output Attribution.](http://arxiv.org/abs/2401.14578) | 本论文引入了一种名为Graph Output Attribution（GOAt）的新方法，通过将GNN扩展为涉及节点特征、边特征和激活模式的标量积之和，计算每个节点或边特征对每个标量积的贡献，并将贡献聚合起来，从而实现将图输出归因于输入图特征的GNN解释。 |
| [^50] | [PrivStream: An Algorithm for Streaming Differentially Private Data.](http://arxiv.org/abs/2401.14577) | PrivStream是一种用于流式差分隐私数据的算法，可以解决离线应用中的隐私保护和数据效用问题。算法可以针对空间数据集进行合成流数据生成，并提供了通用的在线选择性计数框架，验证了算法的实用性。 |
| [^51] | [Extension of Recurrent Kernels to different Reservoir Computing topologies.](http://arxiv.org/abs/2401.14557) | 该研究通过提供特定RC体系结构与相应循环内核形式等价性的经验分析，填补了Leaky RC、Sparse RC和Deep RC等已建立的RC范例尚未进行分析的空白。此外，研究还揭示了稀疏连接在RC体系结构中的作用，并提出了一种依赖储备大小的最佳稀疏性水平。最后，该研究的系统分析表明，在Deep RC模型中，通过减小尺寸的连续储备可以更好地实现收敛。 |
| [^52] | [Revisiting Active Learning in the Era of Vision Foundation Models.](http://arxiv.org/abs/2401.14555) | 本文评估了基础视觉模型对有效主动学习的三个关键组成部分的影响，并提出了一个新的简单优雅的主动学习策略，该策略通过平衡不确定性估计和样本多样性来实现。 |
| [^53] | [Bayesian Optimization through Gaussian Cox Process Models for Spatio-temporal Data.](http://arxiv.org/abs/2401.14544) | 本文提出了一种最大后验推断高斯Cox过程的方法，实现了对时空数据进行贝叶斯优化，扩展了现有工作，通过使用该结果，提出了一个基于高斯Cox过程模型的贝叶斯优化框架。 |
| [^54] | [Understanding Disparities in Post Hoc Machine Learning Explanation.](http://arxiv.org/abs/2401.14539) | 该研究通过模拟和实验评估了事后机器学习解释中的差异，并发现协变量偏移、概念转变和省略协变量会增加解释差异，对神经网络影响更大。 |
| [^55] | [CaRiNG: Learning Temporal Causal Representation under Non-Invertible Generation Process.](http://arxiv.org/abs/2401.14535) | CaRiNG提出了一种基于可辨识性理论的方法，用于学习具有非可逆生成过程的时间因果表示。这种方法能够恢复独立的潜在组分，即使它们来自于非线性且非可逆的混合过程。 |
| [^56] | [Meta-Learning Linear Quadratic Regulators: A Policy Gradient MAML Approach for the Model-free LQR.](http://arxiv.org/abs/2401.14534) | 本论文研究了在多任务、异构和无模型环境下学习线性二次调节器（LQR）的问题，提出了一种基于策略梯度元学习（MAML）方法的解决方案。该方法能够产生与每个任务特定的最优控制器接近的控制器，并在模型基础设置下以线性收敛速率实现。 |
| [^57] | [Relative Value Biases in Large Language Models.](http://arxiv.org/abs/2401.14530) | 该研究发现大型语言模型在做选择时表现出了与人类和动物相似的相对价值偏差，这对于理解人类选择中的背景依赖性机制具有重要意义。 |
| [^58] | [Towards Interpretable Physical-Conceptual Catchment-Scale Hydrological Modeling using the Mass-Conserving-Perceptron.](http://arxiv.org/abs/2401.14521) | 本研究通过利用质量守恒感知器构建基于有向图结构的水文模型，实现了对集水区尺度水文过程的解释能力，在保持简洁性的同时能够准确地模拟各种流量动力学行为，并通过引入输入旁路机制进一步优化了模型的表现。 |
| [^59] | [Who Are We Missing? A Principled Approach to Characterizing the Underrepresented Population.](http://arxiv.org/abs/2401.14512) | 本文提出了一种基于优化的方法，Rashomon Set of Optimal Trees (ROOT)，用于识别和描述随机对照试验中的少数人群。该方法通过最小化目标平均处理效应估计的方差来优化目标子群体分布，从而提供更精确和可解释的处理效应估计。与其他方法相比，该方法具有更高的精度和可解释性，通过合成数据实验进行了验证。 |
| [^60] | [Learning When to See for Long-term Traffic Data Collection on Power-constrained Devices.](http://arxiv.org/abs/2401.14504) | 本研究介绍了一种学习基于框架，通过策略性地决定观测时间并从稀疏采样的观测中重建数据流，以实现在电力受限的设备上进行长期交通数据收集的最小性能损失和显著延长系统寿命。 |
| [^61] | [MResT: Multi-Resolution Sensing for Real-Time Control with Vision-Language Models.](http://arxiv.org/abs/2401.14502) | MResT是一个基于多分辨率感知和视觉语言模型的实时控制框架，能够同时实现粗粒度和精确动作，并通过合理地利用不同的感知模式来提高机器人操作任务的性能。 |
| [^62] | [Predictive Analysis for Optimizing Port Operations.](http://arxiv.org/abs/2401.14498) | 本研究开发了一种具有竞争预测和分类能力的港口运营解决方案，用于准确估计船舶在港口的总时间和延迟时间，填补了港口分析模型在这方面的空白，并为海事物流领域提供了有价值的贡献。 |
| [^63] | [Investigating the Quality of DermaMNIST and Fitzpatrick17k Dermatological Image Datasets.](http://arxiv.org/abs/2401.14497) | 本文研究了DermaMNIST和Fitzpatrick17k皮肤科图像数据集的质量问题，对数据重复、数据泄漏、错误标记和缺乏测试分区等方面进行了详细分析，并提出纠正措施。 |
| [^64] | [K-QA: A Real-World Medical Q&A Benchmark.](http://arxiv.org/abs/2401.14493) | 本研究构建了K-QA数据集，包含1212个真实世界医疗对话中的患者问题，并聘请内部医生回答和分解。研究还制定了两个基于NLI的评估指标，用于评估模型的召回率和精确度。研究结果对于提升大型语言模型在临床环境下的准确性具有重要意义。 |
| [^65] | [Scilab-RL: A software framework for efficient reinforcement learning and cognitive modeling research.](http://arxiv.org/abs/2401.14488) | Scilab-RL是一种用于机器人代理的认知建模和增强学习研究的软件框架，通过提供稳定的基线3和OpenAI gym接口，以及实验可视化和超参数优化的功能，最大程度地提高了研究产出。 |
| [^66] | [CloudTracks: A Dataset for Localizing Ship Tracks in Satellite Images of Clouds.](http://arxiv.org/abs/2401.14486) | 这个论文介绍了一个名为CloudTracks的数据集，该数据集包含3560张带有超过12000个船舶路径实例注释的卫星图像。作者使用该数据集训练模型，并发现他们的最佳模型明显优于之前的模型。 |
| [^67] | [Four Facets of Forecast Felicity: Calibration, Predictiveness, Randomness and Regret.](http://arxiv.org/abs/2401.14483) | 本文展示了校准和遗憾在评估预测中的概念等价性，将评估问题构建为一个预测者、一个赌徒和自然之间的博弈，并将预测的评估与结果的随机性联系起来。 |
| [^68] | [Unveiling the Unseen: Identifiable Clusters in Trained Depthwise Convolutional Kernels.](http://arxiv.org/abs/2401.14469) | 本研究揭示了深度可分离卷积神经网络中训练的卷积核中出现的可辨别和可解释的模式，这些模式类似于高斯差分函数和它们的一阶和二阶导数。研究通过对数百万个训练滤波器进行无监督聚类，成功将最先进的模型中的大部分滤波器进行分类。 |
| [^69] | [Marabou 2.0: A Versatile Formal Analyzer of Neural Networks.](http://arxiv.org/abs/2401.14461) | Marabou 2.0是一个多功能的神经网络形式分析器，具有创新的架构设计和引入的主要功能和组件。 |
| [^70] | [Wordflow: Social Prompt Engineering for Large Language Models.](http://arxiv.org/abs/2401.14447) | 本文提出了一种名为Wordflow的工具，通过社交提示工程的方式让非专家用户更好地使用大型语言模型（LLMs），并可以轻松创建、运行、共享和发现LLM提示。通过利用现代网络技术，Wordflow允许用户在浏览器中本地和私下运行LLM。 |
| [^71] | [Improving Antibody Humanness Prediction using Patent Data.](http://arxiv.org/abs/2401.14442) | 本研究利用专利数据提高了抗体人性预测的能力，通过多阶段、多损失的训练过程以及弱监督对比学习的方法，成功地预测了抗体序列的人性评分。 |
| [^72] | [Semantic Sensitivities and Inconsistent Predictions: Measuring the Fragility of NLI Models.](http://arxiv.org/abs/2401.14440) | 这份论文研究发现，最先进的NLI模型对微小的语义保持表面形式变化非常敏感，导致推断结果不一致。其行为与对组合语义的有效理解不同，这对当前NLI模型的可靠性提出了挑战。 |
| [^73] | [Incremental Affinity Propagation based on Cluster Consolidation and Stratification.](http://arxiv.org/abs/2401.14439) | 本文提出了基于簇合并和分层的增量亲和传播算法(APP)，实现了对动态数据集的增量聚类，同时保持聚类结果的忠实性和遗忘过时簇的功能。 |
| [^74] | [Transforming gradient-based techniques into interpretable methods.](http://arxiv.org/abs/2401.14434) | 本文提出了一种基于梯度的技术支持框架，通过建立区别来强调重要区域, 并减少图像噪音。实证调查表明这些区域在促进类别区分方面起关键作用。 |
| [^75] | [A2C: A Modular Multi-stage Collaborative Decision Framework for Human-AI Teams.](http://arxiv.org/abs/2401.14432) | A2C是一个多阶段模块化的人工智能团队合作决策框架，通过综合利用人类和人工智能的优势，显著提高了动态和不断变化环境中复杂决策制定的效率和效果。 |
| [^76] | [[Re] The Discriminative Kalman Filter for Bayesian Filtering with Nonlinear and Non-Gaussian Observation Models.](http://arxiv.org/abs/2401.14429) | 该论文提供了一种针对非线性和非高斯观测模型的贝叶斯滤波的判别式卡尔曼滤波器，并在神经科学背景下证明了其有效性。 |
| [^77] | [Beimingwu: A Learnware Dock System.](http://arxiv.org/abs/2401.14427) | Beimingwu is a learnware paradigm that enables users to reuse well-trained models, submitted by developers worldwide to a dock system, for solving new user tasks. The dock system assigns a specification to accommodate the model, allowing future users to identify and assemble the model for reuse, even without prior knowledge. This paradigm offers excellent capabilities for both planned and specialized tasks. |
| [^78] | [M$^3$TN: Multi-gate Mixture-of-Experts based Multi-valued Treatment Network for Uplift Modeling.](http://arxiv.org/abs/2401.14426) | M$^3$TN是一种用于提升建模的新颖网络，通过多门专家混合和明确建模提升的方法，解决了现有方法中存在的一致性和效率问题。 |
| [^79] | [Discovering Mathematical Formulas from Data via GPT-guided Monte Carlo Tree Search.](http://arxiv.org/abs/2401.14424) | 通过结合MCTS和生成式预训练模型，我们提出了一种新的符号回归算法SR-GPT，在发现数据中的数学公式方面取得了显著的改进。 |
| [^80] | [Prompt Design and Engineering: Introduction and Advanced Methods.](http://arxiv.org/abs/2401.14423) | 本文介绍了提示设计与工程的主要概念，并回顾了基本和更高级的方法。 |
| [^81] | [Location Agnostic Source-Free Domain Adaptive Learning to Predict Solar Power Generation.](http://arxiv.org/abs/2401.14422) | 提出了一种基于深度学习的域自适应框架，利用气候特征预测太阳能发电量，在不同地区间具有一定的通用性和适应性。 |
| [^82] | [Multi-Agent Based Transfer Learning for Data-Driven Air Traffic Applications.](http://arxiv.org/abs/2401.14421) | 本文提出了一种基于多智能体的迁移学习方法，利用MA-BERT模型和预训练微调框架来解决空中交通管理中的长训练时间和大数据集需求的问题。该方法可以在具有少量数据或无历史数据的情况下实现高性能。 |
| [^83] | [Fuzzy Logic Function as a Post-hoc Explanator of the Nonlinear Classifier.](http://arxiv.org/abs/2401.14417) | 本文研究了使用模糊逻辑函数作为分类器的事后解释器，通过与黑盒分类器进行并行设计，实现了与黑盒分类器相同的决策结果。 |
| [^84] | [Acoustic characterization of speech rhythm: going beyond metrics with recurrent neural networks.](http://arxiv.org/abs/2401.14416) | 本研究通过训练递归神经网络进行语言识别任务，探索了深度学习在声学基础上进一步研究语音节奏的能力。 |
| [^85] | [Aprendizado de m\'aquina aplicado na eletroqu\'imica.](http://arxiv.org/abs/2401.14413) | 本文系统综述了机器学习技术在电化学应用中的使用情况，并介绍了其在医学诊断、化学品分类和环境监测等方面的重要作用。 |
| [^86] | [Harnessing Neuron Stability to Improve DNN Verification.](http://arxiv.org/abs/2401.14412) | 本论文提出了VeriStable方法，在DNN验证中利用稳定的神经元减少组合复杂性，同时保持抽象的准确性。这种方法与工业化SAT基准共享重要特征，并在有效性和可扩展性方面取得了显著的进展。 |
| [^87] | [Precision Mars Entry Navigation with Atmospheric Density Adaptation via Neural Networks.](http://arxiv.org/abs/2401.14411) | 这项工作介绍了一种利用神经网络进行火星进入导航的新方法，使用神经网络估计大气密度，并根据估计的不确定性进行实时参数调整，以提高导航滤波器的性能。 |
| [^88] | [DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence.](http://arxiv.org/abs/2401.14196) | DeepSeek-Coder是一系列开源代码模型，通过在高质量项目级代码语料库上进行预训练和采用填空任务和16K窗口来增强代码生成和填充，不仅在多个基准测试中取得了与开源代码模型同样的最新表现，而且超过了现有的闭源模型。 |
| [^89] | [Uncertainty-Guided Alignment for Unsupervised Domain Adaptation in Regression.](http://arxiv.org/abs/2401.13721) | 该论文提出了一种利用不确定性引导的无监督领域自适应回归方法，通过将不确定性作为置信度估计和嵌入空间的正则项来实现对齐。 |
| [^90] | [Pixel-Wise Recognition for Holistic Surgical Scene Understanding.](http://arxiv.org/abs/2401.11174) | 本文提出了一个整体和多粒度外科场景理解数据集，以及一个基于变形器的模型，该模型有效地结合了全局视频特征提取和局部器械分割，可用于多层次理解外科活动。 |
| [^91] | [Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities.](http://arxiv.org/abs/2401.11143) | 该论文提出了一个名为GAAM的多头高斯自适应注意力机制，用于增强跨多个模态的信息聚合。通过将可学习的均值和方差纳入注意力机制中，GAAM能够动态地重新调整特征的重要性，从而在处理非平稳数据时取得了显著的性能提升，超过了目前现有的注意力技术。该方法的适应性强且参数数量较少，具有改进现有注意力框架的潜力。 |
| [^92] | [Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction.](http://arxiv.org/abs/2401.10189) | 这篇论文提出了一种名为Chem-FINESE的方法来处理化学领域中细粒度少样本实体提取的问题。该方法通过使用序列到序列的实体提取器和自我验证模块来从输入句子中提取命名实体并重构原始输入句子。实验证明了该方法的有效性和可行性。 |
| [^93] | [DiConStruct: Causal Concept-based Explanations through Black-Box Distillation.](http://arxiv.org/abs/2401.08534) | DiConStruct是一种基于黑盒模型的因果概念解释方法，通过创建结构性因果模型和概念归因方式提供更具可解释性的局部解释。 |
| [^94] | [CCNETS: A Novel Brain-Inspired Approach for Enhanced Pattern Recognition in Imbalanced Datasets.](http://arxiv.org/abs/2401.04139) | CCNETS是一种新颖的脑启发方法，通过模拟大脑的信息处理，通过生成高质量的数据集来增强不平衡数据集中的模式识别，特别关注处理机器学习中的不平衡数据集的挑战。 |
| [^95] | [Agent AI: Surveying the Horizons of Multimodal Interaction.](http://arxiv.org/abs/2401.03568) | Agent AI是一种多模态交互系统，可以感知视觉刺激、语言输入和其他环境相关数据，通过将代理体嵌入物理或虚拟环境中来实现更复杂和上下文感知的人工智能系统。 |
| [^96] | [Where and How to Attack? A Causality-Inspired Recipe for Generating Counterfactual Adversarial Examples.](http://arxiv.org/abs/2312.13628) | 该论文通过考虑因果关系的视角，确定了深度神经网络（DNNs）脆弱性的源头，并提出了一种生成更真实的对抗样本的方法。 |
| [^97] | [Generative Network Layer for Communication Systems with Artificial Intelligence.](http://arxiv.org/abs/2312.05398) | 这个论文提出了一种使用生成式人工智能在中间或边缘网络节点上的生成网络层，通过将潜在表示压缩后的提示生成图像，可以显著改善网络中所需的数据速率。 |
| [^98] | [Semi-Supervised Active Learning for Semantic Segmentation in Unknown Environments Using Informative Path Planning.](http://arxiv.org/abs/2312.04402) | 提出了一种基于信息路径规划的半监督主动学习方法，用于解决在未知环境中进行语义分割的问题，通过减少人工标注量，提高了机器人的感知能力。 |
| [^99] | [Piecewise polynomial regression of tame functions via integer programming.](http://arxiv.org/abs/2311.13544) | 本论文提出了使用整数规划对温顺函数进行分段多项式回归的方法，这可以用于估计包含在许多应用中的温顺函数，并且展示了令人期待的计算结果。 |
| [^100] | [Communication-Constrained Bayesian Active Knowledge Distillation.](http://arxiv.org/abs/2311.08053) | 本研究提出了一种名为通信受限的贝叶斯主动知识蒸馏（CC-BAKD）的新协议，通过使用线性混合机制将贝叶斯主动学习与压缩相结合，解决了在学习者与教师之间进行通信时关于批次选择和批次编码的重要问题。 |
| [^101] | [ViR: Towards Efficient Vision Retention Backbones.](http://arxiv.org/abs/2310.19731) | ViR提出了一种新的计算机视觉模型，采用双重并行和递归公式，从而在快速推理和并行训练之间取得了最佳平衡，具有竞争力的性能。 |
| [^102] | [Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation.](http://arxiv.org/abs/2310.18628) | 通过个性化蒸馏，将闭源LLMs的能力传递给开源LLMs，并在代码生成任务中表现出比标准蒸馏更好的性能，只使用三分之一的数据。 |
| [^103] | [A General Framework for Robust G-Invariance in G-Equivariant Networks.](http://arxiv.org/abs/2310.18564) | 这项研究介绍了一种通用方法，通过引入G-三重相关层，在G-等变卷积神经网络中实现强鲁棒性。该方法利用完备的三重相关理论，这使得G-TC层能够在面对不变性攻击时具有强大的鲁棒性，并且能够在分类准确率上相比标准的Max G-Pooling有明显的改善。 |
| [^104] | [Machine Learning Estimation of Maximum Vertical Velocity from Radar.](http://arxiv.org/abs/2310.09392) | 本研究利用机器学习模型U-Nets，通过3D雷达反射率，成功地估计了最大垂直速度及其面积范围，并采用Sinh-arcsinh-normal（SHASH）分布参数回归技术进行了确定性和概率预测。 |
| [^105] | [Gesture Recognition for FMCW Radar on the Edge.](http://arxiv.org/abs/2310.08876) | 本文介绍了一种基于60 GHz FMCW雷达的轻量级手势识别系统，通过使用一组五个特征和精简的处理算法，可以在嵌入式平台上高效识别手势，同时具有较低的内存、计算和功耗要求。 |
| [^106] | [Safe Deep Policy Adaptation.](http://arxiv.org/abs/2310.08602) | 该论文提出了SafeDPA，一种新颖的强化学习和控制框架，用于同时解决策略适应和安全强化学习的问题。SafeDPA在仿真环境中联合学习自适应策略和动力学模型，并使用少量真实数据进行微调。在真实世界部署过程中，通过引入基于控制屏障函数的安全过滤器，确保了SafeDPA的安全性。 |
| [^107] | [ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models.](http://arxiv.org/abs/2310.02998) | ECoFLaP提出了一种高效的粗到细的逐层剪枝方法，解决了大型视觉语言模型在压缩和部署时的计算和能耗问题。 |
| [^108] | [Non-Exchangeable Conformal Risk Control.](http://arxiv.org/abs/2310.01262) | 本文提出了一种非交换式共形风险控制的框架，可以在数据不可交换的情况下控制任何单调损失函数的期望值。 |
| [^109] | [TraCE: Trajectory Counterfactual Explanation Scores.](http://arxiv.org/abs/2309.15965) | TraCE是一个模型无关的模块化框架，用于评估顺序决策任务中的进展。它能够将高度复杂场景中的进展凝练为一个单一值，并在医疗保健和气候变化领域展示了其实用性。 |
| [^110] | [Geometry of Linear Neural Networks: Equivariance and Invariance under Permutation Groups.](http://arxiv.org/abs/2309.13736) | 本研究探讨了线性神经网络在置换群作用下的等变性和不变性，并通过行列式变量的直积描述了等变或不变子变量的特性和奇异性。通过稀疏性和权值共享属性，我们提出了关于等变和不变线性网络参数化和设计的结论。 |
| [^111] | [Causal Reasoning: Charting a Revolutionary Course for Next-Generation AI-Native Wireless Networks.](http://arxiv.org/abs/2309.13223) | 本文提出了一种基于因果推理的框架，用于构建AI本地化的无线网络，以应对现有“AI for wireless”范式的短板。该框架通过解决AI模型的黑匣子特性、曲线拟合特性、对大量训练数据的依赖以及大型神经网络的能量效率低下等问题，克服了数据驱动型、训练密集型AI的局限性。 |
| [^112] | [Causal Entropy and Information Gain for Measuring Causal Control.](http://arxiv.org/abs/2309.07703) | 本文提出了一种考虑因果结构的信息论量，用于评估某个特定结果变量的因果重要性，解决了因果可解释性的挑战。 |
| [^113] | [Latent Representation and Simulation of Markov Processes via Time-Lagged Information Bottleneck.](http://arxiv.org/abs/2309.07200) | 本文介绍了一种通过时间滞后信息瓶颈的方法，将复杂系统映射到简化表示空间并模拟时间上的大跳跃。实验证明该方法能够准确模拟原始过程的统计特性和动力学，优于现有的时间滞后降维方法。 |
| [^114] | [Intelligent upper-limb exoskeleton using deep learning to predict human intention for sensory-feedback augmentation.](http://arxiv.org/abs/2309.04655) | 创新点在于引入了云端深度学习和嵌入式柔性传感器，实现了智能上肢外骨骼系统来预测人类上肢运动的意图并提供感觉反馈，准确率达到96.2%，能以人类意图为基础进行操作。 |
| [^115] | [A multiobjective continuation method to compute the regularization path of deep neural networks.](http://arxiv.org/abs/2308.12044) | 本文提出了一种多目标延续方法，用于计算深度神经网络的正则化路径，以解决DNNs中稀疏性和数值效率之间的冲突。 |
| [^116] | [Exploring Link Prediction over Hyper-Relational Temporal Knowledge Graphs Enhanced with Time-Invariant Relational Knowledge.](http://arxiv.org/abs/2307.10219) | 这项研究填补了时间KG和超关系KG推理之间的差距，并开发了两个新的基准超关系TKG数据集。 |
| [^117] | [Using the IBM Analog In-Memory Hardware Acceleration Kit for Neural Network Training and Inference.](http://arxiv.org/abs/2307.09357) | 本教程介绍了使用IBM Analog Hardware Acceleration Kit (AIHWKit)进行神经网络训练和推断的方法，该工具包模拟了模拟内存计算（AIMC）的推断和训练，并提供了最佳实践和云环境中使用的优势。 |
| [^118] | [Extreme heatwave sampling and prediction with analog Markov chain and comparisons with deep learning.](http://arxiv.org/abs/2307.09060) | 本研究提出了一种基于类比马尔可夫链和深度学习的数据驱动模拟器，适用于预测法国和斯堪的纳维亚的长时间热浪。与卷积神经网络相比，该模拟器在概率预测任务上表现更好，并且经过适当评估和性能评估。 |
| [^119] | [Learning IMM Filter Parameters from Measurements using Gradient Descent.](http://arxiv.org/abs/2307.06618) | 本文提出了一种使用梯度下降从测量中学习IMM滤波器参数的方法，通过仅使用测量数据即可优化滤波器的参数。在模拟数据上的实验结果表明，该方法能够达到使用真值参数化的滤波器的性能水平。 |
| [^120] | [A Synthetic Electrocardiogram (ECG) Image Generation Toolbox to Facilitate Deep Learning-Based Scanned ECG Digitization.](http://arxiv.org/abs/2307.01946) | 本文介绍了一种用于生成合成ECG图像的工具箱，旨在促进扫描ECG数字化。通过引入真实伪影，如手写文本伪影、皱纹、折痕和视角变换，该方法可以在标准纸质ECG背景上生成具有真实性的ECG图像。这种方法有助于解决合成ECG图像中缺乏参考时间序列的问题。 |
| [^121] | [Progressive Fourier Neural Representation for Sequential Video Compilation.](http://arxiv.org/abs/2306.11305) | 本研究提出了一种渐进傅里叶神经表示方法，通过在每个训练会话中找到自适应且紧凑的傅里叶空间子模块来编码顺序视频数据，克服了现有神经隐式表示方法在多个复杂数据上的泛化能力差的问题。 |
| [^122] | [Splitting and Parallelizing of Quantum Convolutional Neural Networks for Learning Translationally Symmetric Data.](http://arxiv.org/abs/2306.07331) | 提出一种基于平移对称性的分裂并行化QCNN架构，可以高效地学习平移对称量子数据，相比传统的QCNN极大地提高了测量效率和速度。 |
| [^123] | [Is Normalization Indispensable for Multi-domain Federated Learning?.](http://arxiv.org/abs/2306.05879) | 本研究旨在解决联邦学习中的多领域问题。我们提出一种新的方法，FedWon，通过消除规范化步骤来有效地处理来自不同领域的数据。 |
| [^124] | [Offline Prioritized Experience Replay.](http://arxiv.org/abs/2306.05412) | 本文提出了离线优先经验重放（OPER）方法来解决离线强化学习中的分布偏移问题。通过设计一类优先级函数来对高回报的转换进行优先处理，从而改善行为策略，并在此改进的策略约束下优化离线强化学习算法的解决方案。对于离线强化学习，OPER方法是一种有效的解决方案。 |
| [^125] | [Networked Communication for Decentralised Agents in Mean-Field Games.](http://arxiv.org/abs/2306.02766) | 本研究在均场博弈中引入网络通信，提出了一种提高分布式智能体学习效率的方案，并进行了实际实验验证。 |
| [^126] | [Multiple output samples per input in a single-output Gaussian process.](http://arxiv.org/abs/2306.02719) | 本文提出了在单输出高斯过程中允许单个输入具有多个输出样本的方法，以利用可用的输出不确定性信息。通过在speechocean762数据集上的评估，展示了该方法能够计算出更接近多个人工评级器参考输出集的测试集输出分布。 |
| [^127] | [MicroSegNet: A Deep Learning Approach for Prostate Segmentation on Micro-Ultrasound Images.](http://arxiv.org/abs/2305.19956) | 本文提出了一种基于深度学习的微型超声图像前列腺分割方法，利用多尺度注释引导的Transformer UNet模型和注释引导的二分类交叉熵损失解决低分辨率和界限不清的挑战，该方法更加关注难以分割的区域。 |
| [^128] | [MT-SLVR: Multi-Task Self-Supervised Learning for Transformation In(Variant) Representations.](http://arxiv.org/abs/2305.17191) | 该论文提出了一种名为 MT-SLVR 的框架，用于解决自监督学习中的不变性问题，以改善不同的下游任务的分类性能。 |
| [^129] | [Collaborative World Models: An Online-Offline Transfer RL Approach.](http://arxiv.org/abs/2305.15260) | 本论文提出了一种名为协作世界模型（CoWorld）的转移学习方法，以改善离线条件下视觉RL的性能。其核心想法是使用易于交互的模拟器来训练辅助RL模型作为离线策略的在线“测试床”，并执行域协作表示学习和域协作行为学习，缓解离线数据分布之外的价值函数过度估计问题。 |
| [^130] | [Optimal Low-Rank Matrix Completion: Semidefinite Relaxations and Eigenvector Disjunctions.](http://arxiv.org/abs/2305.12292) | 该论文通过重新表述低秩矩阵填补问题为投影矩阵的非凸问题，实现了能够确定最优解的分离分支定界方案，并且通过新颖和紧密的凸松弛方法，使得最优性差距相对于现有方法减少了两个数量级。 |
| [^131] | [Expectation Maximization Pseudo Labelling for Segmentation with Limited Annotations.](http://arxiv.org/abs/2305.01747) | 本文提出了一种伪标签的泛化方法，称为贝叶斯伪标签，在半监督医学图像分割任务中应用效果良好。 |
| [^132] | [Object-Centric Voxelization of Dynamic Scenes via Inverse Neural Rendering.](http://arxiv.org/abs/2305.00393) | 本文提出了一个逆向神经渲染框架DynaVol，可以在多实体动态场景中学习时间变化的体积表示，通过维护一个时间依赖的3D格点和联合学习格点级局部动态、物体级全局动态和组合神经辐射场来增强物体中心场景体素化的时空一致性。 |
| [^133] | [Can Perturbations Help Reduce Investment Risks? Risk-Aware Stock Recommendation via Split Variational Adversarial Training.](http://arxiv.org/abs/2304.11043) | 本文提出了一种基于分离变分对抗训练的风险感知型股票推荐方法，通过对抗性扰动提高模型对于风险的感知能力，通过变分扰动生成器模拟不同的风险因素并生成代表性的风险指标对抗样本。在真实股票数据上进行的实验表明该方法有效降低了投资风险同时保持高预期收益。 |
| [^134] | [A Polynomial Time, Pure Differentially Private Estimator for Binary Product Distributions.](http://arxiv.org/abs/2304.06787) | 本论文提出了第一个多项式时间、纯差分隐私估计器，可以在$\{0,1\}^d$上准确估计二元积分布的均值，达到了最优的样本复杂度。 |
| [^135] | [Function Space and Critical Points of Linear Convolutional Networks.](http://arxiv.org/abs/2304.05752) | 研究了具有一维卷积层的线性网络的函数空间，分析了网络架构对函数空间的影响并证明了对于步幅大于一且数据一般的架构，该优化问题的非零临界点是函数空间的平滑内部点。 |
| [^136] | [Generative Modeling with Flow-Guided Density Ratio Learning.](http://arxiv.org/abs/2303.03714) | FDRL是一种基于流引导的密度比学习的简单且可扩展的生成建模方法，通过训练密度比估计器从逐渐改进的样本中学习，缓解了密度鸿沟问题，并在生成高尺寸图像上表现优于现有基线方法。 |
| [^137] | [Representation Disentaglement via Regularization by Identification.](http://arxiv.org/abs/2303.00128) | 本文研究了从观测数据中学习解耦表示的问题，提出通过鉴别性正则化来实现表征解耦，解决了现代深度表征学习模型中出现的纠缠偏差行为问题。 |
| [^138] | [Dual RL: Unification and New Methods for Reinforcement and Imitation Learning.](http://arxiv.org/abs/2302.08560) | 这篇论文介绍了双重强化学习的概念，并在一个统一的框架下解释了几种最新深度强化学习算法及模仿学习方法。作者提出了双重模仿学习方法（DIL）直接最小化策略之间的距离，并提出了一种新的离线演员-评论家方法。 |
| [^139] | [Linear-Time Algorithms for Front-Door Adjustment in Causal Graphs.](http://arxiv.org/abs/2211.16468) | 在因果图中，提出了解决前门调整的线性时间算法，通过观察到的中介变量，即使存在未观测到的混淆，也可以识别因果效应。 |
| [^140] | [An Approach for Noisy, Crowdsourced Datasets Utilizing Ensemble Modeling, Normalized Distributions of Annotations, and Entropic Measures of Uncertainty.](http://arxiv.org/abs/2210.16380) | 在众包图像数据集上的分类是具有挑战性的。本文提出了一种利用集合建模、注释的归一化分布和熵测量的方法，以帮助识别标签不确定的图像，并量化这些样本的可信度。 |
| [^141] | [Finite-time analysis of single-timescale actor-critic.](http://arxiv.org/abs/2210.09921) | 这项研究提出了一种在线单时间尺度演员-评论家方法，通过线性函数逼近和马尔可夫样本更新，在连续状态空间中找到了一个$\epsilon$-近似的稳定点，并且在样本复杂度为$\widetilde{\mathcal{O}}(\epsilon^{-2})$的情况下证明了其收敛性。 |
| [^142] | [Signature Methods in Machine Learning.](http://arxiv.org/abs/2206.14674) | 本综述介绍了机器学习中应用的签名方法，通过数学洞察力理解复杂的流式数据之间的交互，并提供了用于分析非规则、非平稳的流式数据的数值方法。 |
| [^143] | [Modification-Fair Cluster Editing.](http://arxiv.org/abs/2112.03183) | 这个论文提出了一种修改公平的集群编辑问题，解决了在顶点着色图中标准算法可能产生的偏向子组的解决方案。具体而言，在双色图中，问题是 NP-hard 的，即使只允许在子组内部插入边缘。 |
| [^144] | [On minimizers and convolutional filters: theoretical connections and applications to genome analysis.](http://arxiv.org/abs/2111.08452) | 该论文通过对哈希函数属性进行数学分析，发现在分类字母表上的序列分析中，使用随机高斯初始化的卷积滤波器和最大池化等价于选择一种最小化器排序，能够有效提取与其他最小化器距离较近但与序列中的k-mer相距较远的重要特征。 |
| [^145] | [High-dimensional Functional Graphical Model Structure Learning via Neighborhood Selection Approach.](http://arxiv.org/abs/2105.02487) | 该论文提出了一种基于邻域选择方法的高维函数图模型结构学习方法，通过函数对函数回归估计节点邻域，然后结合这些估计的邻域恢复整个图结构，从而直接估计条件独立结构。 |
| [^146] | [On the Stability of Nonlinear Receding Horizon Control: A Geometric Perspective.](http://arxiv.org/abs/2103.15010) | 本文研究了非线性滑动视角控制的稳定性，特别关注反馈线性化系统，并证明了在一定条件下，RHC的一阶解可以使可线性化系统指数级稳定。 |

# 详细

[^1]: EAGLE: 推测采样需要重新思考特征不确定性

    EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty. (arXiv:2401.15077v1 [cs.LG])

    [http://arxiv.org/abs/2401.15077](http://arxiv.org/abs/2401.15077)

    EAGLE是一个无损加速语言模型推理的框架，通过在次顶层特征层面上自回归推理，并解决采样不确定性问题，实现了比传统方法更快3倍的速度。

    

    自回归解码使得大型语言模型（LLMs）的推理变得耗时。我们提出了一个简单的框架，EAGLE（用于提高语言模型效率的外推算法），实现了无损加速。与传统的推测采样方法不同，EAGLE在更规律的（次顶层）特征层面上自回归进行编写，并通过整合提前一个时间步的标记来解决下一个特征预测问题中的采样不确定性。EAGLE所提供的加速是无损的：它不需要微调目标LLM，并且生成的文本与原始的自回归解码的分布相同。截至本文提交时，EAGLE是已知推测采样家族中速度最快的框架。在MT-bench上，EAGLE比原始解码快3倍，比Lookahead快2倍，比Medusa快1.6倍。使用gpt-fast，EAGLE平均每秒达到160个标记与LLaMA2-Chat搭配。

    Auto-regressive decoding makes the inference of Large Language Models (LLMs) time-consuming. We propose a simple framework, EAGLE (Extrapolation Algorithm for Greater Language-model Efficiency), for lossless acceleration. Unlike traditional speculative sampling methods, EAGLE operates the drafting process auto-regressively at the more regular (second-top-layer) feature level and addresses the sampling uncertainty issues in the next-feature prediction problems by integrating tokens from one time step ahead. The acceleration provided by EAGLE is lossless: it involves no fine-tuning of the target LLM, and the generated text maintains the same distribution as that of vanilla auto-regressive decoding. As of the submission of this paper, EAGLE is the fastest known framework within the speculative sampling family. On MT-bench, EAGLE is 3x faster than vanilla decoding, 2x faster than Lookahead, and 1.6x faster than Medusa. Using gpt-fast, EAGLE attains on average 160 tokens/s with LLaMA2-Chat 
    
[^2]: 专家与聚类：分层在线偏好学习框架

    Expert with Clustering: Hierarchical Online Preference Learning Framework. (arXiv:2401.15062v1 [cs.LG])

    [http://arxiv.org/abs/2401.15062](http://arxiv.org/abs/2401.15062)

    本研究提出了一种名为Expert with Clustering (EWC)的分层在线偏好学习框架，它利用聚类技术和专家建议的预测来加速用户偏好学习，并通过一种新颖的损失引导距离度量生成更具代表性的聚类中心。

    

    新兴的移动系统越来越能够向移动用户推荐选项，以引导他们朝向个性化但可持续的系统结果。与典型的推荐系统相比，最小化后悔是至关重要的，因为1）移动选项直接影响用户的生活，2）系统的可持续性依赖于足够的用户参与。在本研究中，我们考虑通过利用捕捉用户移动偏好的低维潜在空间来加速用户偏好学习。我们引入了一种名为Expert with Clustering (EWC)的分层上下文Bandit框架，它集成了聚类技术和专家建议的预测。EWC有效地利用分层用户信息，并结合了一种新颖的损失引导距离度量。该度量在生成更具代表性的聚类中心方面起着关键作用。在每个用户$T$轮，$N$用户和$K$选项的推荐场景中，我们的算法根据用户的实时反馈来在线学习和改进推荐。

    Emerging mobility systems are increasingly capable of recommending options to mobility users, to guide them towards personalized yet sustainable system outcomes. Even more so than the typical recommendation system, it is crucial to minimize regret, because 1) the mobility options directly affect the lives of the users, and 2) the system sustainability relies on sufficient user participation. In this study, we consider accelerating user preference learning by exploiting a low-dimensional latent space that captures the mobility preferences of users. We introduce a hierarchical contextual bandit framework named Expert with Clustering (EWC), which integrates clustering techniques and prediction with expert advice. EWC efficiently utilizes hierarchical user information and incorporates a novel Loss-guided Distance metric. This metric is instrumental in generating more representative cluster centroids. In a recommendation scenario with $N$ users, $T$ rounds per user, and $K$ options, our alg
    
[^3]: 多智能体强化学习中的完全独立通信

    Fully Independent Communication in Multi-Agent Reinforcement Learning. (arXiv:2401.15059v1 [cs.LG])

    [http://arxiv.org/abs/2401.15059](http://arxiv.org/abs/2401.15059)

    该论文研究了多智能体强化学习中的完全独立通信，并提出了一种新的学习方案，证明独立智能体仍可以学习通信策略。此外，论文还探讨了通信在不同网络容量下的影响。

    

    多智能体强化学习（MARL）是多智能体系统领域的一个广泛研究领域。最近的一些工作专注于研究MARL中的通信方法。虽然已经提出了多种通信方法，但这些方法可能仍然过于复杂，不容易迁移到更实际的情境中。其中一个原因是使用了著名的参数共享技巧。在本文中，我们研究了在MARL中不共享参数的独立学习者如何进行通信。我们证明了这种设置可能会带来一些问题，为此我们提出了一种新的学习方案作为解决方案。我们的结果表明，尽管面临挑战，独立的智能体仍然可以通过我们的方法学习通信策略。此外，我们使用这种方法来研究MARL中的通信如何受到不同网络容量的影响，无论是共享参数还是不共享参数。我们观察到，通信的能力在这两种情况下都产生了影响。

    Multi-Agent Reinforcement Learning (MARL) comprises a broad area of research within the field of multi-agent systems. Several recent works have focused specifically on the study of communication approaches in MARL. While multiple communication methods have been proposed, these might still be too complex and not easily transferable to more practical contexts. One of the reasons for that is due to the use of the famous parameter sharing trick. In this paper, we investigate how independent learners in MARL that do not share parameters can communicate. We demonstrate that this setting might incur into some problems, to which we propose a new learning scheme as a solution. Our results show that, despite the challenges, independent agents can still learn communication strategies following our method. Additionally, we use this method to investigate how communication in MARL is affected by different network capacities, both for sharing and not sharing parameters. We observe that communication 
    
[^4]: 健康文本简化：消化癌症教育的注释语料库和增强学习的新策略

    Health Text Simplification: An Annotated Corpus for Digestive Cancer Education and Novel Strategies for Reinforcement Learning. (arXiv:2401.15043v1 [cs.CL])

    [http://arxiv.org/abs/2401.15043](http://arxiv.org/abs/2401.15043)

    该论文介绍了一个用于健康文本简化研究的消化癌症教育材料的注释语料库，并探索了基于大型语言模型的简化方法，包括微调、增强学习、增强学习与人类反馈、领域自适应和基于提示的应用。

    

    目标：健康教育材料的阅读水平显著影响信息的可理解性和可接触性，特别是对于少数族裔人群。许多患者教育资源超过了广泛接受的标准的阅读水平和复杂性。在健康信息中，急需高性能的文本简化模型以增强传播和识字能力。这种需要在癌症教育中尤为迫切，有效的预防和筛查教育可以大大减少发病率和死亡率。方法：我们引入了简化的消化癌症（SimpleDC）并行语料库，用于健康文本简化研究。利用SimpleDC和现有的Med-EASi语料库，我们探索了基于大型语言模型（LLM）的简化方法，包括微调、增强学习（RL）、增强学习与人类反馈（RLHF）、领域自适应和基于提示的应用。

    Objective: The reading level of health educational materials significantly influences information understandability and accessibility, particularly for minoritized populations. Many patient educational resources surpass the reading level and complexity of widely accepted standards. There is a critical need for high-performing text simplification models in health information to enhance dissemination and literacy. This need is particularly acute in cancer education, where effective prevention and screening education can substantially reduce morbidity and mortality.  Methods: We introduce Simplified Digestive Cancer (SimpleDC), a parallel corpus of cancer education materials tailored for health text simplification research. Utilizing SimpleDC alongside the existing Med-EASi corpus, we explore Large Language Model (LLM)-based simplification methods, including fine-tuning, reinforcement learning (RL), reinforcement learning with human feedback (RLHF), domain adaptation, and prompt-based app
    
[^5]: 关于神经网络在通用多模态推理中的泛化能力的研究

    On the generalization capacity of neural networks during generic multimodal reasoning. (arXiv:2401.15030v1 [cs.LG])

    [http://arxiv.org/abs/2401.15030](http://arxiv.org/abs/2401.15030)

    本研究评估了不同神经网络架构在多模态泛化方面的能力，并发现具有多个注意力层或利用交叉注意机制的模型表现更好。

    

    Transformer的出现导致了大型语言模型（LLM）的发展, 这些模型似乎展示了类似人类的能力。为了评估这类模型和其他基本的神经网络架构在多模态领域的一般性，我们评估和比较了它们在多模态泛化方面的能力。我们引入了一个多模态问答基准来评估三种特定类型的超出分布（OOD）泛化性能：分心泛化（在分心存在的情况下泛化），系统的组合泛化（对新的任务排列的泛化）和有益的组合泛化（对更复杂的任务结构进行泛化）。我们发现，在不同的模型架构上（如RNN，Transformer，Perceivers等），具有多个注意力层或者利用输入领域之间的交叉注意机制的模型更好。我们的积极结果表明，对于多模态泛化，模型架构是重要因素。

    The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks structures). We found that across model architectures (e.g., RNNs, Transformers, Perceivers, etc.), models with multiple attention layers, or models that leveraged cross-attention mechanisms between input domains, fared better. Our positive results demonstrate that for multi
    
[^6]: SliceGPT: 通过删除行和列来压缩大型语言模型

    SliceGPT: Compress Large Language Models by Deleting Rows and Columns. (arXiv:2401.15024v1 [cs.LG])

    [http://arxiv.org/abs/2401.15024](http://arxiv.org/abs/2401.15024)

    SliceGPT是一种新的事后训练稀疏化方案，通过将每个权重矩阵替换为较小的矩阵以减小网络的维度，可以在保持高任务性能的同时减少模型参数。

    

    大型语言模型已成为自然语言处理的基石，但使用它们需要大量计算和内存资源。稀疏化方法可以缓解这些资源限制，并且最近的研究表明训练好的模型可以进行事后的稀疏化处理。现有的稀疏化技术面临着挑战，因为它们需要额外的数据结构，并且在当前硬件上速度受限。在本文中，我们提出了一种新的事后训练稀疏化方案SliceGPT，该方案用较小的（稠密的）矩阵替换每个权重矩阵，从而减小网络的嵌入维度。通过大量的实验，我们展示了SliceGPT在保持相应稠密模型的99%、99%和90%的零-shot任务性能的同时，可以移除LLAMA2-70B、OPT 66B和Phi-2模型中多达25%的模型参数（包括嵌入）。我们的切片模型在较少的GPU上运行并且更快，无需额外的代码优化。

    Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post-hoc. Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware. In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. Through extensive experimentation, we show that SliceGPT can remove up to 25% of the model parameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models while maintaining 99%, 99% and 90% zero-shot task performance of the dense model respectively. Our sliced models run on fewer GPUs and run faster without any additional code optimi
    
[^7]: 基于机器学习的胶质瘤组织切片分析：一项综述

    Machine learning-based analysis of glioma tissue sections: a review. (arXiv:2401.15022v1 [eess.IV])

    [http://arxiv.org/abs/2401.15022](http://arxiv.org/abs/2401.15022)

    机器学习技术在胶质瘤组织切片分析中具有诊断和预测的潜力，当前研究聚焦于成人型弥漫性胶质瘤的苏木精和伊红染色组织切片，以及对该疾病的分类、分级、分子标记预测和生存预测等临床任务。

    

    近年来，胶质瘤的诊断变得越来越复杂。使用现代机器学习技术对胶质瘤组织进行组织学评估，为诊断和预测结果提供了新的机会。为了对当前研究的现状进行概述，本综述对70个公开可得的研究论文进行了研究，这些论文关于使用机器学习分析染色的胶质瘤组织切片，涵盖了分类（16/70），分级（23/70），分子标记预测（13/70）和生存预测（27/70）等诊断任务。所有的研究都在方法学方面及其临床适用性方面进行了评估。发现当前研究的重点是对成人型弥漫性胶质瘤的苏木精和伊红染色组织切片进行评估。多数研究（49/70）基于公开的胶质母细胞瘤和低级别胶质瘤数据集，仅有少数研究使用其他数据集。

    In recent years, the diagnosis of gliomas has become increasingly complex. Histological assessment of glioma tissue using modern machine learning techniques offers new opportunities to support diagnosis and outcome prediction. To give an overview of the current state of research, this review examines 70 publicly available research studies on machine learning-based analysis of stained human glioma tissue sections, covering the diagnostic tasks of subtyping (16/70), grading (23/70), molecular marker prediction (13/70), and survival prediction (27/70). All studies were reviewed with regard to methodological aspects as well as clinical applicability. It was found that the focus of current research is the assessment of hematoxylin and eosin-stained tissue sections of adult-type diffuse gliomas. The majority of studies (49/70) are based on the publicly available glioblastoma and low-grade glioma datasets from The Cancer Genome Atlas (TCGA) and only a few studies employed other datasets in is
    
[^8]: 通过使用特征组合和并行结构分类器增强无文本约束的说话人验证系统

    Enhancement of a Text-Independent Speaker Verification System by using Feature Combination and Parallel-Structure Classifiers. (arXiv:2401.15018v1 [eess.AS])

    [http://arxiv.org/abs/2401.15018](http://arxiv.org/abs/2401.15018)

    本文通过使用特征组合和并行结构分类器来增强无文本约束的说话人验证系统的性能，在特征提取和分类阶段进行了探索，提出了不同声学特征的组合比较，并针对传统支持向量机分类器的弱点进行了改进。

    

    说话人验证系统主要包含两个独立的阶段：特征提取和分类。本文旨在探索这两个模块，提高在嘈杂环境下说话人验证系统的性能。首先，选择最适合的声学特征是进行鲁棒性说话人验证的关键因素。本文提出系统中使用的声学参数包括：梅尔频率倒谱系数（MFCC），它们的一阶和二阶导数（Deltas和Delta-Deltas），巴克频率倒谱系数（BFCC），感知线性预测（PLP）和相对谱变换感知线性预测（RASTA-PLP）。本文对不同特征组合进行了全面比较。其次，传统支持向量机（SVM）分类器的主要弱点是使用通用传统的核函数计算数据点之间的距离。

    Speaker Verification (SV) systems involve mainly two individual stages: feature extraction and classification. In this paper, we explore these two modules with the aim of improving the performance of a speaker verification system under noisy conditions. On the one hand, the choice of the most appropriate acoustic features is a crucial factor for performing robust speaker verification. The acoustic parameters used in the proposed system are: Mel Frequency Cepstral Coefficients (MFCC), their first and second derivatives (Deltas and Delta- Deltas), Bark Frequency Cepstral Coefficients (BFCC), Perceptual Linear Predictive (PLP), and Relative Spectral Transform Perceptual Linear Predictive (RASTA-PLP). In this paper, a complete comparison of different combinations of the previous features is discussed. On the other hand, the major weakness of a conventional Support Vector Machine (SVM) classifier is the use of generic traditional kernel functions to compute the distances among data points
    
[^9]: 基于图的实体聚类修复的主动学习方法

    Graph-based Active Learning for Entity Cluster Repair. (arXiv:2401.14992v1 [cs.LG])

    [http://arxiv.org/abs/2401.14992](http://arxiv.org/abs/2401.14992)

    本文介绍了一种基于图的主动学习方法，用于修复实体聚类中的错误，该方法利用了基础相似性图推导出的图度量，并结合了主动学习来解决数据训练不足的挑战。

    

    聚类修复方法旨在确定聚类中的错误并修改它们，以使每个聚类包含代表同一实体的记录。当前的聚类修复方法主要假设数据源中没有重复记录，即每个来自一种数据源的记录对应于另一种数据源的唯一记录。然而，现实世界的数据往往不符合这一假设，由于质量问题。最近的方法将聚类方法与链接分类方法结合起来，以便可以应用于具有重复记录的数据源。然而，结果并没有展现出清晰的图片，因为质量在很大程度上取决于配置和数据集。在本研究中，我们介绍了一种利用基础相似性图推导的图度量的新方法进行聚类修复。这些度量在构建分类模型时起着关键作用，用于区分正确和错误的边。为了解决数据训练不足的挑战，我们还整合了一种主动学习方法。

    Cluster repair methods aim to determine errors in clusters and modify them so that each cluster consists of records representing the same entity. Current cluster repair methodologies primarily assume duplicate-free data sources, where each record from one source corresponds to a unique record from another. However, real-world data often deviates from this assumption due to quality issues. Recent approaches apply clustering methods in combination with link categorization methods so they can be applied to data sources with duplicates. Nevertheless, the results do not show a clear picture since the quality highly varies depending on the configuration and dataset. In this study, we introduce a novel approach for cluster repair that utilizes graph metrics derived from the underlying similarity graphs. These metrics are pivotal in constructing a classification model to distinguish between correct and incorrect edges. To address the challenge of limited training data, we integrate an active l
    
[^10]: 使用新型B样条自由节点放置算法的映射到参数非线性函数回归

    Mapping-to-Parameter Nonlinear Functional Regression with Novel B-spline Free Knot Placement Algorithm. (arXiv:2401.14989v1 [cs.LG])

    [http://arxiv.org/abs/2401.14989](http://arxiv.org/abs/2401.14989)

    提出了一种新的非线性函数回归方法，通过将函数数据映射到有限维参数空间，并使用新的自由节点放置算法来同时近似多个函数。该算法根据输入或输出函数的局部复杂性来决定节点位置，性能稳健。

    

    我们提出了一种新颖的非线性函数回归方法，称为映射到参数函数模型，通过采用任何监督学习技术处理参数空间中的复杂和非线性函数回归问题。该模型的核心是将函数数据从无限维函数空间映射到有限维参数空间。这是通过使用一组公用的B样条基函数同时近似多个函数来实现的，其节点分布由迭代局部放置算法确定，这是一种新提出的自由节点放置算法。与传统的等距节点放置策略相比，后者根据输入或输出函数的局部复杂性来确定节点位置，而不是基于预定义的节点数均匀分布节点位置。我们的节点放置算法的性能在两个方面都表现出了稳健性。

    We propose a novel approach to nonlinear functional regression, called the Mapping-to-Parameter function model, which addresses complex and nonlinear functional regression problems in parameter space by employing any supervised learning technique. Central to this model is the mapping of function data from an infinite-dimensional function space to a finite-dimensional parameter space. This is accomplished by concurrently approximating multiple functions with a common set of B-spline basis functions by any chosen order, with their knot distribution determined by the Iterative Local Placement Algorithm, a newly proposed free knot placement algorithm. In contrast to the conventional equidistant knot placement strategy that uniformly distributes knot locations based on a predefined number of knots, our proposed algorithms determine knot location according to the local complexity of the input or output functions. The performance of our knot placement algorithms is shown to be robust in both 
    
[^11]: 基于嵌入式的JetBrains IDE的搜索功能

    Embedding-based search in JetBrains IDEs. (arXiv:2401.14975v1 [cs.SE])

    [http://arxiv.org/abs/2401.14975](http://arxiv.org/abs/2401.14975)

    该论文介绍了在JetBrains IDE中实施的基于嵌入式的搜索功能，通过机器学习方法提高了搜索项的可发现性。

    

    大多数现代集成开发环境（IDE）和代码编辑器都具有通过可用功能和项目中的项进行搜索的功能。在JetBrains IDE中，这个功能称为"搜索全部"：它允许用户从一个入口点搜索文件、操作、类、符号、设置以及版本控制系统(VCS)历史记录中的任何内容。然而，它仅通过不考虑语义的算法获取候选项，比如同义词、复杂的单词排列、词性修改和拼写错误等。在这项工作中，我们描述了我们实施的机器学习方法，以提高搜索项的可发现性。我们还分享了在这个过程中遇到的障碍以及如何克服它们。

    Most modern Integrated Development Environments (IDEs) and code editors have a feature to search across available functionality and items in an open project. In JetBrains IDEs, this feature is called Search Everywhere: it allows users to search for files, actions, classes, symbols, settings, and anything from VCS history from a single entry point. However, it works with the candidates obtained by algorithms that don't account for semantics, e.g., synonyms, complex word permutations, part of the speech modifications, and typos. In this work, we describe the machine learning approach we implemented to improve the discoverability of search items. We also share the obstacles encountered during this process and how we overcame them.
    
[^12]: 通过分层循环切换状态模型发现同步时间序列中的群体动态

    Discovering group dynamics in synchronous time series via hierarchical recurrent switching-state models. (arXiv:2401.14973v1 [stat.ML])

    [http://arxiv.org/abs/2401.14973](http://arxiv.org/abs/2401.14973)

    通过分层的循环切换状态模型，我们可以无监督地同时解释系统级和个体级的动态，从而更好地建模同步时间序列中的群体动态。

    

    我们致力于对同一时间段内多个实体相互作用而产生的时间序列集合进行建模。最近的研究集中在建模个体时间序列方面对我们的预期应用是不足够的，其中集体系统级行为影响着个体实体的轨迹。为了解决这类问题，我们提出了一种新的分层切换状态模型，可以以无监督的方式训练，同时解释系统级和个体级的动态。我们采用了一个隐含的系统级离散状态马尔可夫链，驱动着隐含的实体级链，进而控制每个观测时间序列的动态。观测结果在实体和系统级的链之间进行反馈，通过依赖于上下文的状态转换来提高灵活性。我们的分层切换循环动力学模型可以通过封闭形式的变分坐标上升更新来学习，其在个体数量上呈线性扩展。

    We seek to model a collection of time series arising from multiple entities interacting over the same time period. Recent work focused on modeling individual time series is inadequate for our intended applications, where collective system-level behavior influences the trajectories of individual entities. To address such problems, we present a new hierarchical switching-state model that can be trained in an unsupervised fashion to simultaneously explain both system-level and individual-level dynamics. We employ a latent system-level discrete state Markov chain that drives latent entity-level chains which in turn govern the dynamics of each observed time series. Feedback from the observations to the chains at both the entity and system levels improves flexibility via context-dependent state transitions. Our hierarchical switching recurrent dynamical models can be learned via closed-form variational coordinate ascent updates to all latent chains that scale linearly in the number of indivi
    
[^13]: 神经网络验证的端到端基于集合的训练方法

    End-To-End Set-Based Training for Neural Network Verification. (arXiv:2401.14961v1 [cs.LG])

    [http://arxiv.org/abs/2401.14961](http://arxiv.org/abs/2401.14961)

    本论文提出了一种端到端基于集合的训练方法，用于训练鲁棒性神经网络进行形式化验证，并证明该方法能够简化验证过程并有效训练出易于验证的神经网络。

    

    神经网络容易受到对抗性攻击，即微小的输入扰动可能导致神经网络输出产生重大变化。安全关键环境需要对输入扰动具有鲁棒性的神经网络。然而，训练和形式化验证鲁棒性神经网络是具有挑战性的。我们首次采用端到端基于集合的训练方法来解决这个挑战，该训练方法能够训练出可进行形式化验证的鲁棒性神经网络。我们的训练方法能够大大简化已训练神经网络的后续形式化鲁棒性验证过程。相比于以往的研究主要关注增强神经网络训练的对抗性攻击，我们的方法利用基于集合的计算来训练整个扰动输入集合上的神经网络。此外，我们证明我们的基于集合的训练方法可以有效训练出易于验证的鲁棒性神经网络。

    Neural networks are vulnerable to adversarial attacks, i.e., small input perturbations can result in substantially different outputs of a neural network. Safety-critical environments require neural networks that are robust against input perturbations. However, training and formally verifying robust neural networks is challenging. We address this challenge by employing, for the first time, a end-to-end set-based training procedure that trains robust neural networks for formal verification. Our training procedure drastically simplifies the subsequent formal robustness verification of the trained neural network. While previous research has predominantly focused on augmenting neural network training with adversarial attacks, our approach leverages set-based computing to train neural networks with entire sets of perturbed inputs. Moreover, we demonstrate that our set-based training procedure effectively trains robust neural networks, which are easier to verify. In many cases, set-based trai
    
[^14]: 学习通用预测器

    Learning Universal Predictors. (arXiv:2401.14953v1 [cs.LG])

    [http://arxiv.org/abs/2401.14953](http://arxiv.org/abs/2401.14953)

    本论文探索了将Solomonoff Induction（SI）引入神经网络中的潜力，并使用万能图灵机（UTMs）生成的数据来进行元学习，研究结果表明UTM数据是元学习的有价值资源，可以用来训练能够学习通用预测的神经网络。

    

    元学习已经成为一个强大的方法，训练神经网络快速从有限的数据中学习新任务。对不同任务的广泛暴露导致了多功能表示，从而实现了通用问题解决能力。但是，元学习的限制是什么？在这项工作中，我们通过将最强大的通用预测器Solomonoff Induction（SI）通过元学习的极限进行分担，探索其潜力。我们使用万能图灵机（UTMs）生成训练数据，用于将网络暴露于广泛的模式。我们提供了关于UTM数据生成过程和元训练协议的理论分析。我们使用不同复杂性和普适性的算法数据生成器对神经网络架构（如LSTMs、Transformers）进行了全面的实验。我们的结果表明，UTM数据是元学习的宝贵资源，可以用来训练能够学习通用预测的神经网络。

    Meta-learning has emerged as a powerful approach to train neural networks to learn new tasks quickly from limited data. Broad exposure to different tasks leads to versatile representations enabling general problem solving. But, what are the limits of meta-learning? In this work, we explore the potential of amortizing the most powerful universal predictor, namely Solomonoff Induction (SI), into neural networks via leveraging meta-learning to its limits. We use Universal Turing Machines (UTMs) to generate training data used to expose networks to a broad range of patterns. We provide theoretical analysis of the UTM data generation processes and meta-training protocols. We conduct comprehensive experiments with neural architectures (e.g. LSTMs, Transformers) and algorithmic data generators of varying complexity and universality. Our results suggest that UTM data is a valuable resource for meta-learning, and that it can be used to train neural networks capable of learning universal predicti
    
[^15]: 保留-更新-修订以解决对抗训练中的泛化性和鲁棒性的权衡

    Conserve-Update-Revise to Cure Generalization and Robustness Trade-off in Adversarial Training. (arXiv:2401.14948v1 [cs.LG])

    [http://arxiv.org/abs/2401.14948](http://arxiv.org/abs/2401.14948)

    对抗训练在提高神经网络鲁棒性方面取得了进展，但是牺牲了标准和鲁棒泛化之间的权衡。研究发现，选择性地更新特定层可以显著提高网络学习能力。因此，提出了CURE框架，通过选择性保留、更新和修订权重来解决这一问题。这一方法可以有效解决记忆化和过度拟合问题，并提高鲁棒性和泛化性之间的权衡。

    

    对抗训练可以提高神经网络对抗攻击的鲁棒性，但会导致标准泛化和鲁棒泛化之间的权衡。为了揭示驱动这一现象的潜在因素，我们研究了神经网络在从标准设置到对抗设置过渡时的逐层学习能力。经验结果表明，选择性地更新特定层而保留其他层可以大幅增强网络的学习能力。因此，我们提出了一种新的训练框架CURE，利用梯度显著性准则对权重进行选择性保留、更新和修订。重要的是，CURE的设计是数据集和架构不可知的，确保其适用于各种情况。它有效解决了记忆化和过度拟合问题，从而增强了鲁棒性和泛化性之间的权衡，并且这种训练方法还可以

    Adversarial training improves the robustness of neural networks against adversarial attacks, albeit at the expense of the trade-off between standard and robust generalization. To unveil the underlying factors driving this phenomenon, we examine the layer-wise learning capabilities of neural networks during the transition from a standard to an adversarial setting. Our empirical findings demonstrate that selectively updating specific layers while preserving others can substantially enhance the network's learning capacity. We therefore propose CURE, a novel training framework that leverages a gradient prominence criterion to perform selective conservation, updating, and revision of weights. Importantly, CURE is designed to be dataset- and architecture-agnostic, ensuring its applicability across various scenarios. It effectively tackles both memorization and overfitting issues, thus enhancing the trade-off between robustness and generalization and additionally, this training approach also 
    
[^16]: 在有限理性人类代理在摩擦任务中进行强化学习干预

    Reinforcement Learning Interventions on Boundedly Rational Human Agents in Frictionful Tasks. (arXiv:2401.14923v1 [cs.AI])

    [http://arxiv.org/abs/2401.14923](http://arxiv.org/abs/2401.14923)

    本论文介绍了行为模型强化学习（BMRL）框架，在摩擦性任务中，AI代理对有限理性人类代理的参数进行干预，通过对人类策略进行解释，帮助理解行为干预。

    

    许多重要的行为变化是具有摩擦的；它们要求个体长期付出努力，但没有即时的满足。在这种情况下，人工智能（AI）代理可以提供个性化的干预，帮助个体坚持自己的目标。在这些设置中，AI代理必须快速个性化（在个体失去兴趣之前）并具有解释性，以帮助我们理解行为干预。在本文中，我们引入了行为模型强化学习（BMRL）框架，其中AI代理对属于有限理性人类代理的马尔可夫决策过程（MDP）的参数进行干预。我们将人类决策者的形式化为一个规划代理，这样我们就能够将不理想的人类策略（不会达到目标的策略）归因于其不适应的MDP参数，比如极低的折扣因子。此外，我们提出了一类可解释的人类模型，捕捉了摩擦任务中的基本行为。

    Many important behavior changes are frictionful; they require individuals to expend effort over a long period with little immediate gratification. Here, an artificial intelligence (AI) agent can provide personalized interventions to help individuals stick to their goals. In these settings, the AI agent must personalize rapidly (before the individual disengages) and interpretably, to help us understand the behavioral interventions. In this paper, we introduce Behavior Model Reinforcement Learning (BMRL), a framework in which an AI agent intervenes on the parameters of a Markov Decision Process (MDP) belonging to a boundedly rational human agent. Our formulation of the human decision-maker as a planning agent allows us to attribute undesirable human policies (ones that do not lead to the goal) to their maladapted MDP parameters, such as an extremely low discount factor. Furthermore, we propose a class of tractable human models that captures fundamental behaviors in frictionful tasks. Int
    
[^17]: 学习本地控制屏障函数以实现混合系统的安全控制

    Learning Local Control Barrier Functions for Safety Control of Hybrid Systems. (arXiv:2401.14907v1 [cs.RO])

    [http://arxiv.org/abs/2401.14907](http://arxiv.org/abs/2401.14907)

    该论文提出了一种学习启用的方法，能够构建本地控制屏障函数，以保证广泛类别的非线性混合动力系统的安全性。该方法是高效的，对参考控制器的干预最小，适用于大规模系统，并通过实证评估和比较案例展示了其功效和灵活性。

    

    混合动力系统在实际的机器人应用中普遍存在，常涉及连续状态和离散状态切换。安全性是混合机器人系统的首要关注点。现有的混合系统的安全关键控制方法要么计算效率低下，对系统性能有损，要么仅适用于小规模系统。为了解决这些问题，在本文中，我们提出了一种学习启用的方法，用于构建本地控制屏障函数（CBFs），以保证广泛类别的非线性混合动力系统的安全性。最终，我们得到了一个安全的基于神经网络的CBF切换控制器。我们的方法在计算上高效，对任何参考控制器的干预最小，并适用于大规模系统。通过两个机器人示例（包括高维自主赛车案例），我们对我们的框架进行了实证评估，并与其他基于CBF的方法和模型预测控制进行了比较，展示了其功效和灵活性。

    Hybrid dynamical systems are ubiquitous as practical robotic applications often involve both continuous states and discrete switchings. Safety is a primary concern for hybrid robotic systems. Existing safety-critical control approaches for hybrid systems are either computationally inefficient, detrimental to system performance, or limited to small-scale systems. To amend these drawbacks, in this paper, we propose a learningenabled approach to construct local Control Barrier Functions (CBFs) to guarantee the safety of a wide class of nonlinear hybrid dynamical systems. The end result is a safe neural CBFbased switching controller. Our approach is computationally efficient, minimally invasive to any reference controller, and applicable to large-scale systems. We empirically evaluate our framework and demonstrate its efficacy and flexibility through two robotic examples including a high-dimensional autonomous racing case, against other CBF-based approaches and model predictive control.
    
[^18]: 评估模型在交叉子群体间性能的结构回归方法

    A structured regression approach for evaluating model performance across intersectional subgroups. (arXiv:2401.14893v1 [cs.LG])

    [http://arxiv.org/abs/2401.14893](http://arxiv.org/abs/2401.14893)

    这项工作介绍了一种结构回归方法，用于评估模型在不同交叉子群体间的性能。它可以提供可靠的系统性能估计，即使对于很小的子群体。

    

    在人工智能公平性评估中，分解式评估是一项核心任务，目标是衡量人工智能系统在由人口统计学或其他敏感属性组合定义的不同子群体中的性能。标准方法是将评估数据分层到子群体中，并分别计算每个组的性能指标。然而，即使对于中等规模的评估数据集来说，在考虑到交叉子群体时样本数量也会迅速变小，这大大限制了许多分解评估中对交叉群体的考虑程度。在本研究中，我们引入了一种结构回归方法来进行分解评估，我们证明即使对于非常小的子群体，该方法也能产生可靠的系统性能估计。我们还提供了相应的推断策略来构建置信区间，并探索了拟合优度测试如何揭示交叉子群体所经历的与公平相关的伤害的结构。

    Disaggregated evaluation is a central task in AI fairness assessment, with the goal to measure an AI system's performance across different subgroups defined by combinations of demographic or other sensitive attributes. The standard approach is to stratify the evaluation data across subgroups and compute performance metrics separately for each group. However, even for moderately-sized evaluation datasets, sample sizes quickly get small once considering intersectional subgroups, which greatly limits the extent to which intersectional groups are considered in many disaggregated evaluations. In this work, we introduce a structured regression approach to disaggregated evaluation that we demonstrate can yield reliable system performance estimates even for very small subgroups. We also provide corresponding inference strategies for constructing confidence intervals and explore how goodness-of-fit testing can yield insight into the structure of fairness-related harms experienced by intersectio
    
[^19]: P3LS: 隐私保护下的偏最小二乘法

    P3LS: Partial Least Squares under Privacy Preservation. (arXiv:2401.14884v1 [stat.ML])

    [http://arxiv.org/abs/2401.14884](http://arxiv.org/abs/2401.14884)

    P3LS是一种隐私保护的偏最小二乘回归技术，通过使用可移动的随机掩码保护每个数据持有者的隐私，实现了跨组织数据集成和过程建模。

    

    现代制造业价值链需要跨公司边界智能协调流程，以最大化利润同时促进社会和环境可持续性。然而，基于数据的价值链决策的集成式系统级方法的实施，目前受到与跨组织数据交换和集成相关的隐私关注的阻碍。我们在这里提出了隐私保护的偏最小二乘（P3LS）回归，一种能够在有隐私保证的情况下实现跨组织数据集成和过程建模的新型联邦学习技术。P3LS涉及一种基于奇异值分解（SVD）的PLS算法，并采用由可信机构生成的可移动的随机掩码来保护每个数据持有者贡献的数据的隐私。我们展示了P3LS在由三个参与方组成的假想价值链上垂直整合过程数据并提高的能力。

    Modern manufacturing value chains require intelligent orchestration of processes across company borders in order to maximize profits while fostering social and environmental sustainability. However, the implementation of integrated, systems-level approaches for data-informed decision-making along value chains is currently hampered by privacy concerns associated with cross-organizational data exchange and integration. We here propose Privacy-Preserving Partial Least Squares (P3LS) regression, a novel federated learning technique that enables cross-organizational data integration and process modeling with privacy guarantees. P3LS involves a singular value decomposition (SVD) based PLS algorithm and employs removable, random masks generated by a trusted authority in order to protect the privacy of the data contributed by each data holder. We demonstrate the capability of P3LS to vertically integrate process data along a hypothetical value chain consisting of three parties and to improve t
    
[^20]: 跨空间自适应滤波器：集成图拓扑和节点属性以减轻过度平滑问题

    Cross-Space Adaptive Filter: Integrating Graph Topology and Node Attributes for Alleviating the Over-smoothing Problem. (arXiv:2401.14876v1 [cs.LG])

    [http://arxiv.org/abs/2401.14876](http://arxiv.org/abs/2401.14876)

    本论文提出了一种跨空间自适应滤波器（CSF），可以从图拓扑和节点属性空间中提取自适应频率信息，以减轻图卷积网络（GCN）的过度平滑问题。

    

    传统的图卷积网络（GCN）使用低通滤波器从图拓扑中提取低频信号，但当GCN深度增加时可能导致过度平滑问题。为解决这个问题，已经提出了各种方法通过引入从图拓扑中提取的额外滤波器（如高通滤波器）来创建自适应滤波器。然而，这些方法严重依赖拓扑信息，并忽视了节点属性空间，这严重牺牲了深层GCN的表达能力，特别是在处理非同配图时。本文提出了一种跨空间自适应滤波器，称为CSF，能够从拓扑和属性空间中提取自适应频率信息。具体而言，我们首先推导出了一个定制的基于属性的高通滤波器，可以从理论上解释为半监督核岭回归的最小化器。然后，我们将基于拓扑的低通滤波器视为Mercer's核函数。

    The vanilla Graph Convolutional Network (GCN) uses a low-pass filter to extract low-frequency signals from graph topology, which may lead to the over-smoothing problem when GCN goes deep. To this end, various methods have been proposed to create an adaptive filter by incorporating an extra filter (e.g., a high-pass filter) extracted from the graph topology. However, these methods heavily rely on topological information and ignore the node attribute space, which severely sacrifices the expressive power of the deep GCNs, especially when dealing with disassortative graphs. In this paper, we propose a cross-space adaptive filter, called CSF, to produce the adaptive-frequency information extracted from both the topology and attribute spaces. Specifically, we first derive a tailored attribute-based high-pass filter that can be interpreted theoretically as a minimizer for semi-supervised kernel ridge regression. Then, we cast the topology-based low-pass filter as a Mercer's kernel within the 
    
[^21]: 从以对象为中心的过程数据中提取过程感知的决策模型

    Extracting Process-Aware Decision Models from Object-Centric Process Data. (arXiv:2401.14847v1 [cs.LG])

    [http://arxiv.org/abs/2401.14847](http://arxiv.org/abs/2401.14847)

    本文提出了一种从以对象为中心的过程数据中提取过程感知的决策模型的算法，解决了决策挖掘中的多个对象和顺序约束的问题。

    

    组织在日常业务过程中执行决策时必须考虑多个利益相关方，这些利益相关方可能需要对同一流程有多个不同的观点。此外，运行这些业务流程的信息系统的复杂性通常很高，因为它们与存储所有相关数据和流程方面的数据库相连。鉴于信息系统中存在支持流程实施的多个对象，决策自然受到这两个视角的影响，记录在以对象为中心的过程日志中。然而，从以对象为中心的流程日志中发现这样的决策并不简单，因为它需要正确地链接所涉及的对象，同时考虑业务流程所施加的顺序约束，并正确地发现决策的实际作用。本文提出了第一个对象为中心的决策挖掘算法，称为综合对象中心算法

    Organizations execute decisions within business processes on a daily basis whilst having to take into account multiple stakeholders who might require multiple point of views of the same process. Moreover, the complexity of the information systems running these business processes is generally high as they are linked to databases storing all the relevant data and aspects of the processes. Given the presence of multiple objects within an information system which support the processes in their enactment, decisions are naturally influenced by both these perspectives, logged in object-centric process logs. However, the discovery of such decisions from object-centric process logs is not straightforward as it requires to correctly link the involved objects whilst considering the sequential constraints that business processes impose as well as correctly discovering what a decision actually does. This paper proposes the first object-centric decision-mining algorithm called Integrated Object-cent
    
[^22]: 理解域泛化：从噪声鲁棒性的角度

    Understanding Domain Generalization: A Noise Robustness Perspective. (arXiv:2401.14846v1 [cs.LG])

    [http://arxiv.org/abs/2401.14846](http://arxiv.org/abs/2401.14846)

    本文通过研究域泛化算法和经验风险最小化算法在标签噪声下的表现，发现域泛化算法在有限样本训练中显示出隐性的标签噪声鲁棒性，有助于减轻假冗余相关和提高泛化能力，但在真实世界基准数据集上的实验证明，标签噪声鲁棒性并不一定意味着相比ERM具有更好的性能。

    

    尽管快速发展了用于域泛化的机器学习算法，但没有明确的经验证据表明现有的域泛化算法在标准基准测试中优于经典的经验风险最小化（ERM）算法。为了更好地理解这一现象，我们通过标签噪声的视角研究了域泛化算法相对于ERM的优势。具体而言，我们的有限样本分析揭示了标签噪声加剧了ERM中假冗余相关性的影响，削弱了泛化能力。相反，我们证明了在有限样本训练中，域泛化算法在存在假冗余相关时具有隐性的标签噪声鲁棒性。这种有利性有助于减轻假冗余相关性并改善合成实验中的泛化能力。然而，对真实世界基准数据集进行的额外综合实验表明，标签噪声鲁棒性并不一定意味着相比ERM具有更好的性能。

    Despite the rapid development of machine learning algorithms for domain generalization (DG), there is no clear empirical evidence that the existing DG algorithms outperform the classic empirical risk minimization (ERM) across standard benchmarks. To better understand this phenomenon, we investigate whether there are benefits of DG algorithms over ERM through the lens of label noise. Specifically, our finite-sample analysis reveals that label noise exacerbates the effect of spurious correlations for ERM, undermining generalization. Conversely, we illustrate that DG algorithms exhibit implicit label-noise robustness during finite-sample training even when spurious correlation is present. Such desirable property helps mitigate spurious correlations and improve generalization in synthetic experiments. However, additional comprehensive experiments on real-world benchmark datasets indicate that label-noise robustness does not necessarily translate to better performance compared to ERM. We co
    
[^23]: 自适应点云 Transformer

    Adaptive Point Transformer. (arXiv:2401.14845v1 [cs.CV])

    [http://arxiv.org/abs/2401.14845](http://arxiv.org/abs/2401.14845)

    AdaPT是一个自适应的点云Transformer模型，通过动态减少token的数量来高效处理大型点云，并且可以灵活调整计算成本。Experimental evaluation demonstrates that AdaPT significantly reduces computational cost.

    

    最近3D数据获取的激增推动了几何深度学习模型在点云处理中的发展，这一发展受到了在自然语言处理中 Transformer 的显著成功的推动。虽然最近的点云 Transformer 已经取得了令人印象深刻的结果，但是它们与点云大小呈二次比例关系，对实际应用来说存在可伸缩性挑战。为了解决这个问题，我们提出了自适应点云 Transformer (AdaPT)，这是一个标准的Transformer模型，通过自适应的token选择机制加以扩展。AdaPT在推断过程中动态减少了token的数量，实现了对大型点云的高效处理。此外，我们引入了一个预算机制，在推断时灵活调整模型的计算成本，而无需重新训练或微调分离的模型。我们在点云分类任务上进行了广泛的实验评估，证明了AdaPT显著降低了计算成本。

    The recent surge in 3D data acquisition has spurred the development of geometric deep learning models for point cloud processing, boosted by the remarkable success of transformers in natural language processing. While point cloud transformers (PTs) have achieved impressive results recently, their quadratic scaling with respect to the point cloud size poses a significant scalability challenge for real-world applications. To address this issue, we propose the Adaptive Point Cloud Transformer (AdaPT), a standard PT model augmented by an adaptive token selection mechanism. AdaPT dynamically reduces the number of tokens during inference, enabling efficient processing of large point clouds. Furthermore, we introduce a budget mechanism to flexibly adjust the computational cost of the model at inference time without the need for retraining or fine-tuning separate models. Our extensive experimental evaluation on point cloud classification tasks demonstrates that AdaPT significantly reduces comp
    
[^24]: GuardML: 通过混合同态加密实现高效的隐私保护机器学习服务

    GuardML: Efficient Privacy-Preserving Machine Learning Services Through Hybrid Homomorphic Encryption. (arXiv:2401.14840v1 [cs.LG])

    [http://arxiv.org/abs/2401.14840](http://arxiv.org/abs/2401.14840)

    GuardML引入了混合同态加密（HHE）到机器学习中，设计了一种面向终端设备的隐私保护机器学习方案，可以安全地对加密数据进行分类学习，具有高效扩展性。

    

    机器学习（ML）已成为数据科学中最具变革性和影响力的领域之一。然而，ML的广泛应用引入了隐私相关的担忧，因为恶意攻击针对ML模型的数量不断增加。为了解决这些问题，引入了隐私保护机器学习（PPML）方法来保护ML模型的隐私和安全。其中一种方法是使用同态加密（HE）。然而，传统HE的显著缺点和低效性使其在高度可扩展的场景中不实用。幸运的是，最近出现了一种现代密码方案，混合同态加密（HHE），它结合了对称密码和HE的优点，克服了这些挑战。我们的工作旨在通过设计一种针对终端设备的PPML方案来引入HHE到ML中。我们利用HHE作为基本构建块，实现对加密数据上的分类结果的安全学习，同时保证高效扩展性。

    Machine Learning (ML) has emerged as one of data science's most transformative and influential domains. However, the widespread adoption of ML introduces privacy-related concerns owing to the increasing number of malicious attacks targeting ML models. To address these concerns, Privacy-Preserving Machine Learning (PPML) methods have been introduced to safeguard the privacy and security of ML models. One such approach is the use of Homomorphic Encryption (HE). However, the significant drawbacks and inefficiencies of traditional HE render it impractical for highly scalable scenarios. Fortunately, a modern cryptographic scheme, Hybrid Homomorphic Encryption (HHE), has recently emerged, combining the strengths of symmetric cryptography and HE to surmount these challenges. Our work seeks to introduce HHE to ML by designing a PPML scheme tailored for end devices. We leverage HHE as the fundamental building block to enable secure learning of classification outcomes over encrypted data, all wh
    
[^25]: 赋予蛋白质语言模型结构知识

    Endowing Protein Language Models with Structural Knowledge. (arXiv:2401.14819v1 [q-bio.QM])

    [http://arxiv.org/abs/2401.14819](http://arxiv.org/abs/2401.14819)

    本研究提出了一种新的框架，通过整合蛋白质结构数据来增强蛋白质语言模型，以解决在实际应用场景中由于依赖大量序列数据和参数集而限制其灵活性和实用性的问题。

    

    理解蛋白质序列、结构和功能之间的关系是一个长期存在的生物学挑战，对于药物设计和我们对进化的理解具有多方面的影响。最近，蛋白质语言模型已经成为这一挑战的首选方法，因为它们能够利用大规模的序列数据库。然而，它们对于广泛应用于实际场景的灵活性和实用性的依赖于大规模的序列数据和参数集的限制。与此同时，近年来计算预测蛋白质结构的增长为蛋白质表示学习提供了新的机会。虽然有很大的潜力，但是这样复杂数据所带来的计算负担仍然阻碍了广泛应用于实际应用。为了解决这些限制，我们提出了一个新的框架，通过整合蛋白质结构数据来增强蛋白质语言模型。从图变换的最新进展中汲取灵感，我们的方法改进了预训练模型的自注意机制。

    Understanding the relationships between protein sequence, structure and function is a long-standing biological challenge with manifold implications from drug design to our understanding of evolution. Recently, protein language models have emerged as the preferred method for this challenge, thanks to their ability to harness large sequence databases. Yet, their reliance on expansive sequence data and parameter sets limits their flexibility and practicality in real-world scenarios. Concurrently, the recent surge in computationally predicted protein structures unlocks new opportunities in protein representation learning. While promising, the computational burden carried by such complex data still hinders widely-adopted practical applications. To address these limitations, we introduce a novel framework that enhances protein language models by integrating protein structural data. Drawing from recent advances in graph transformers, our approach refines the self-attention mechanisms of pretr
    
[^26]: 关于马尔可夫奖励在多目标、风险敏感和模态任务中表达的局限性

    On the Limitations of Markovian Rewards to Express Multi-Objective, Risk-Sensitive, and Modal Tasks. (arXiv:2401.14811v1 [cs.AI])

    [http://arxiv.org/abs/2401.14811](http://arxiv.org/abs/2401.14811)

    本文研究了强化学习中标量、马尔可夫奖励函数的表达能力，并确定了它们的一些局限性。我们发现这些奖励函数无法表达多目标、风险敏感和模态任务中的大部分实例。

    

    本文研究了强化学习中标量、马尔可夫奖励函数的表达能力，并确定了它们的一些局限性。具体来说，我们研究了三类强化学习任务：多目标强化学习、风险敏感强化学习和模态强化学习。对于每一类任务，我们导出了描述什么样的问题可以使用标量、马尔可夫奖励函数来表达的必要和充分条件。此外，我们发现在这三个类别中，标量、马尔可夫奖励函数无法表达大部分实例。通过这项研究，我们为了解标准奖励函数可以和不能表达的内容做出了更完整的贡献。除此之外，我们还将模态问题作为一个新的问题类别引入，因为在强化学习文献中，目前尚未对其进行系统性研究。我们还简要概述了通过定制的强化学习算法解决所讨论问题的一些方法。

    In this paper, we study the expressivity of scalar, Markovian reward functions in Reinforcement Learning (RL), and identify several limitations to what they can express. Specifically, we look at three classes of RL tasks; multi-objective RL, risk-sensitive RL, and modal RL. For each class, we derive necessary and sufficient conditions that describe when a problem in this class can be expressed using a scalar, Markovian reward. Moreover, we find that scalar, Markovian rewards are unable to express most of the instances in each of these three classes. We thereby contribute to a more complete understanding of what standard reward functions can and cannot express. In addition to this, we also call attention to modal problems as a new class of problems, since they have so far not been given any systematic treatment in the RL literature. We also briefly outline some approaches for solving some of the problems we discuss, by means of bespoke RL algorithms.
    
[^27]: 循环群投影用于列举陷阱集中的准循环码

    Cyclic Group Projection for Enumerating Quasi-Cyclic Codes Trapping Sets. (arXiv:2401.14810v1 [cs.IT])

    [http://arxiv.org/abs/2401.14810](http://arxiv.org/abs/2401.14810)

    介绍了一种循环群投影的方法，用于列举和评估准循环码中的陷阱集，利用表格技术简化重要采样步骤并利用数学框架阐明投影和抬升变换对拟码字的行为。

    

    本文介绍了一种新的方法，用于列举和评估准循环码中的陷阱集，这些码具有非素数的循环大小。利用准循环码的特性，该方法使用表格技术来简化重要采样步骤，用于估计陷阱集的拟码字权重。所提出的方法利用了所提供的定理中建立的数学框架，阐明了投影和抬升变换对拟码字的行为。

    This paper introduces a novel approach to enumerate and assess Trapping sets in quasi-cyclic codes, those with circulant sizes that are non-prime numbers. Leveraging the quasi-cyclic properties, the method employs a tabular technique to streamline the importance sampling step for estimating the pseudo-codeword weight of Trapping sets. The presented methodology draws on the mathematical framework established in the provided theorem, which elucidates the behavior of projection and lifting transformations on pseudo-codewords
    
[^28]: 深度变分隐私漏斗：通用建模及在人脸识别中的应用

    Deep Variational Privacy Funnel: General Modeling with Applications in Face Recognition. (arXiv:2401.14792v1 [cs.CV])

    [http://arxiv.org/abs/2401.14792](http://arxiv.org/abs/2401.14792)

    本文利用信息论隐私漏斗模型开发一种隐私保护的表示学习方法，并将其应用于人脸识别系统，具有适应性和竞争力。

    

    本研究利用信息论隐私漏斗模型开发了一种隐私保护的表示学习方法，采用端到端训练框架。我们严格考虑了模糊与效用之间的平衡，通过对数损失量化两者，这也是一个被视为自信息损失的度量。这种探索深化了信息论隐私与表示学习之间的相互作用，为判别模型和生成模型的数据保护机制提供了实质性的见解。重要的是，我们将模型应用于最先进的人脸识别系统。该模型在各种输入上表现出了适应性，包括原始人脸图像以及派生或经过优化的嵌入，能够胜任分类、重构和生成等任务。

    In this study, we harness the information-theoretic Privacy Funnel (PF) model to develop a method for privacy-preserving representation learning using an end-to-end training framework. We rigorously address the trade-off between obfuscation and utility. Both are quantified through the logarithmic loss, a measure also recognized as self-information loss. This exploration deepens the interplay between information-theoretic privacy and representation learning, offering substantive insights into data protection mechanisms for both discriminative and generative models. Importantly, we apply our model to state-of-the-art face recognition systems. The model demonstrates adaptability across diverse inputs, from raw facial images to both derived or refined embeddings, and is competent in tasks such as classification, reconstruction, and generation.
    
[^29]: Off-Policy Primal-Dual Safe Reinforcement Learning. (arXiv:2401.14758v1 [cs.LG]) 论文的题目是：离策略原双安全强化学习

    Off-Policy Primal-Dual Safe Reinforcement Learning. (arXiv:2401.14758v1 [cs.LG])

    [http://arxiv.org/abs/2401.14758](http://arxiv.org/abs/2401.14758)

    该论文提出了离策略原双安全强化学习方法，通过引入保守策略优化和局部策略凸化来解决累积成本估计误差导致的安全约束不满足问题。

    

    原双安全强化学习方法通常在策略的原始更新和拉格朗日乘子的对偶更新之间进行迭代。由于累积成本估计作为连接原始和对偶更新过程的关键联系，这种训练范式极易受到累积成本估计误差的影响。我们表明，这个问题导致离策略方法使用时成本被严重低估，无法满足安全约束。为了解决这个问题，我们提出了一种“保守策略优化”的方法，通过考虑成本估计的不确定性，在约束满足的区域学习策略。这提高了约束的满足性，但也可能阻碍了奖励最大化。然后，我们引入了“局部策略凸化”来助于消除这种次优性，逐渐减小估计的不确定性。我们对这两个成分的联合作用进行了理论解释。

    Primal-dual safe RL methods commonly perform iterations between the primal update of the policy and the dual update of the Lagrange Multiplier. Such a training paradigm is highly susceptible to the error in cumulative cost estimation since this estimation serves as the key bond connecting the primal and dual update processes. We show that this problem causes significant underestimation of cost when using off-policy methods, leading to the failure to satisfy the safety constraint. To address this issue, we propose \textit{conservative policy optimization}, which learns a policy in a constraint-satisfying area by considering the uncertainty in cost estimation. This improves constraint satisfaction but also potentially hinders reward maximization. We then introduce \textit{local policy convexification} to help eliminate such suboptimality by gradually reducing the estimation uncertainty. We provide theoretical interpretations of the joint coupling effect of these two ingredients and furth
    
[^30]: 基于拓扑感知的能量模型均衡探索：Toric QC-LDPC码和Hyperbolic MET QC-LDPC码

    Topology-Aware Exploration of Energy-Based Models Equilibrium: Toric QC-LDPC Codes and Hyperbolic MET QC-LDPC Codes. (arXiv:2401.14749v1 [cs.IT])

    [http://arxiv.org/abs/2401.14749](http://arxiv.org/abs/2401.14749)

    本文介绍了一种在不均匀分布的电荷和不规则网格上实现能量模型均衡的方法，通过使用QC-LDPC码和玻尔兹曼机，将系统的维度扩展，将电荷替换为循环物质，并通过循环移位表示距离。通过这种方法，可以将不规则网格转化为均匀配置，适用于不同的拓扑结构。该方法还解决了代码在图形概率模型中的评估问题，并提供了在不同拓扑下玻尔兹曼机达到均衡状态的严格证明。

    

    本文提出了一种在不均匀分布的电荷和不规则网格上实现ISING哈密顿量均衡的方法。采用（多边缘）QC-LDPC码和玻尔兹曼机，我们的方法涉及对系统的维度扩展，用循环物质替代电荷，并通过循环移位表示距离。这导致电荷系统在空间上的系统映射，将不规则网格转化为均匀配置，适用于Torical和Circular Hyperboloid拓扑。本文涵盖了与QC-LDPC码、多边缘QC-LDPC码和玻尔兹曼机相关的基本定义和符号。它探讨了用于评估分区函数的代码在图形概率模型中的边际化问题，包括精确和近似估计技术。本文提供了严格的证明，证明了在Torical和Circular Hyper拓扑下，玻尔兹曼机可以达到均衡状态。

    This paper presents a method for achieving equilibrium in the ISING Hamiltonian when confronted with unevenly distributed charges on an irregular grid. Employing (Multi-Edge) QC-LDPC codes and the Boltzmann machine, our approach involves dimensionally expanding the system, substituting charges with circulants, and representing distances through circulant shifts. This results in a systematic mapping of the charge system onto a space, transforming the irregular grid into a uniform configuration, applicable to Torical and Circular Hyperboloid Topologies. The paper covers fundamental definitions and notations related to QC-LDPC Codes, Multi-Edge QC-LDPC codes, and the Boltzmann machine. It explores the marginalization problem in code on the graph probabilistic models for evaluating the partition function, encompassing exact and approximate estimation techniques. Rigorous proof is provided for the attainability of equilibrium states for the Boltzmann machine under Torical and Circular Hyper
    
[^31]: 隐式神经码书的残余量化方法

    Residual Quantization with Implicit Neural Codebooks. (arXiv:2401.14732v1 [cs.LG])

    [http://arxiv.org/abs/2401.14732](http://arxiv.org/abs/2401.14732)

    本文提出了QINCo，一种神经网络残余量化变体，通过预测每个矢量的专门码书，提高了准确性，并在多个数据集和码书大小上优于现有方法。

    

    矢量量化是数据压缩和矢量搜索的基本操作。为了获得高准确性，多码书方法通过使用多个码书中的码字来表示每个矢量来增加速率。残余量化（RQ）是一种方法，通过迭代量化上一步的误差来提高准确性。然而，误差分布依赖于先前选择的码字，在传统RQ中未对此进行考虑，因为它在每个量化步骤中使用通用码书。在本文中，我们提出了QINCo，一种神经网络残余量化变体，它使用神经网络来预测每个矢量的专门码书，条件是先前步骤的向量近似。实验证明，在多个数据集和码书大小上，QINCo的性能优于现有方法很多。例如，QINCo使用12字节的码字在BigANN上比使用16字节的其他方法实现更好的最近邻搜索准确性。

    Vector quantization is a fundamental operation for data compression and vector search. To obtain high accuracy, multi-codebook methods increase the rate by representing each vector using codewords across multiple codebooks. Residual quantization (RQ) is one such method, which increases accuracy by iteratively quantizing the error of the previous step. The error distribution is dependent on previously selected codewords. This dependency is, however, not accounted for in conventional RQ as it uses a generic codebook per quantization step. In this paper, we propose QINCo, a neural RQ variant which predicts specialized codebooks per vector using a neural network that is conditioned on the approximation of the vector from previous steps. Experiments show that QINCo outperforms state-of-the-art methods by a large margin on several datasets and code sizes. For example, QINCo achieves better nearest-neighbor search accuracy using 12 bytes codes than other methods using 16 bytes on the BigANN a
    
[^32]: 一种非参数贝叶斯方法用于在线活动预测

    A Nonparametric Bayes Approach to Online Activity Prediction. (arXiv:2401.14722v1 [stat.ME])

    [http://arxiv.org/abs/2401.14722](http://arxiv.org/abs/2401.14722)

    本研究提出了一种非参数贝叶斯方法，以准确预测在线活动的用户数量和达到所需用户参与门槛所需的时间轨迹。该方法通过捕捉用户参与的潜在异质性，提供了实验者在在线实验中重要的决策支持。

    

    在准确预测特定活动的发生时间内具有重要应用背景。对于运行在线实验（A/B测试）的实验者来说，准确预测未来将接受干预的用户数量是一项重要信息。在这项工作中，我们提出了一种新颖的方法来预测给定时间段内活动用户数量以及达到所需用户参与门槛所需的时间轨迹。我们使用贝叶斯非参数方法来建模用户活动，以捕捉用户参与的潜在异质性。我们推导了在给定时间段内期望的新用户数量的闭式表达式，并提出了一个简单的蒙特卡罗算法来估计达到所需用户数量所需的天数的后验分布；后者对于实验规划非常重要。我们展示了该方法在预测用户活动上的性能。

    Accurately predicting the onset of specific activities within defined timeframes holds significant importance in several applied contexts. In particular, accurate prediction of the number of future users that will be exposed to an intervention is an important piece of information for experimenters running online experiments (A/B tests). In this work, we propose a novel approach to predict the number of users that will be active in a given time period, as well as the temporal trajectory needed to attain a desired user participation threshold. We model user activity using a Bayesian nonparametric approach which allows us to capture the underlying heterogeneity in user engagement. We derive closed-form expressions for the number of new users expected in a given period, and a simple Monte Carlo algorithm targeting the posterior distribution of the number of days needed to attain a desired number of users; the latter is important for experimental planning. We illustrate the performance of o
    
[^33]: 用声学和大型语言模型融合进行交替和回应预测

    Turn-taking and Backchannel Prediction with Acoustic and Large Language Model Fusion. (arXiv:2401.14717v1 [cs.CL])

    [http://arxiv.org/abs/2401.14717](http://arxiv.org/abs/2401.14717)

    这个论文提出了一种利用声学和语言模型进行交替和回应预测的方法，通过融合这两种模型，可以在口语对话中实现更加自然和对话式的互动。

    

    我们提出了一种连续预测口语对话中交替和回应位置的方法，通过融合神经声学模型和大型语言模型（LLM）。在Switchboard人际对话数据集上的实验表明，我们的方法始终优于单模态的基线模型。我们还开发了一种新颖的多任务指令微调策略，以进一步从LLM编码的知识中受益，从而提高了性能。我们的方法展示了使用组合的LLM和声学模型在人类和语音AI代理之间实现更自然和对话式互动的潜力。

    We propose an approach for continuous prediction of turn-taking and backchanneling locations in spoken dialogue by fusing a neural acoustic model with a large language model (LLM). Experiments on the Switchboard human-human conversation dataset demonstrate that our approach consistently outperforms the baseline models with single modality. We also develop a novel multi-task instruction fine-tuning strategy to further benefit from LLM-encoded knowledge for understanding the tasks and conversational contexts, leading to additional improvements. Our approach demonstrates the potential of combined LLMs and acoustic models for a more natural and conversational interaction between humans and speech-enabled AI agents.
    
[^34]: 通过特征解缠来缓解对抗鲁棒性中的特征差距

    Mitigating Feature Gap for Adversarial Robustness by Feature Disentanglement. (arXiv:2401.14707v1 [cs.CV])

    [http://arxiv.org/abs/2401.14707](http://arxiv.org/abs/2401.14707)

    这项研究提出了一种通过特征解缠来缓解对抗鲁棒性中特征差距的方法，该方法明确建模和消除导致特征差距的潜在特征，有效提升了鲁棒性。

    

    深度神经网络对对抗样本很容易受到攻击。对抗微调方法旨在通过对已经在自然情况下进行预训练的模型进行对抗式微调来提升对抗鲁棒性。然而，我们发现对抗样本中的一些潜在特征被对抗扰动所混淆，并导致自然样本和对抗样本在最后一层隐藏层的特征之间出现意外增加的差距。为了解决这个问题，我们提出了一种基于解缠的方法来明确建模和进一步消除导致特征差距的潜在特征。具体而言，我们引入了特征解缠器，将对抗样本的潜在特征与对抗样本的特征分离开来，从而通过消除潜在特征来提升鲁棒性。此外，我们通过将预训练模型中的特征与对抗样本在微调模型中的特征对齐，进一步从自然样本的特征中获益，避免混淆。

    Deep neural networks are vulnerable to adversarial samples. Adversarial fine-tuning methods aim to enhance adversarial robustness through fine-tuning the naturally pre-trained model in an adversarial training manner. However, we identify that some latent features of adversarial samples are confused by adversarial perturbation and lead to an unexpectedly increasing gap between features in the last hidden layer of natural and adversarial samples. To address this issue, we propose a disentanglement-based approach to explicitly model and further remove the latent features that cause the feature gap. Specifically, we introduce a feature disentangler to separate out the latent features from the features of the adversarial samples, thereby boosting robustness by eliminating the latent features. Besides, we align features in the pre-trained model with features of adversarial samples in the fine-tuned model, to further benefit from the features from natural samples without confusion. Empirical 
    
[^35]: FairSample: 高效训练公平准确的图卷积神经网络

    FairSample: Training Fair and Accurate Graph Convolutional Neural Networks Efficiently. (arXiv:2401.14702v1 [cs.LG])

    [http://arxiv.org/abs/2401.14702](http://arxiv.org/abs/2401.14702)

    本文提出了一种名为FairSample的框架，旨在高效训练公平准确的图卷积神经网络。该框架通过对图结构进行纠正，并使用可学习的邻居采样策略来同时减轻图卷积神经网络中的图结构偏见、节点属性偏见和模型参数偏见。

    

    随着图卷积神经网络在许多关键应用中的应用，图卷积神经网络中的公平性越来越成为一个重要问题。许多真实世界的图中存在对敏感群体的社会偏见。在这篇论文中，我们采用了公平性的经典概念“人口统计平均值”，并解决了高效训练公平准确的图卷积神经网络的挑战。我们对图结构偏见、节点属性偏见和模型参数对图卷积神经网络的人口统计平均性能的影响进行了深入分析。我们的洞察力导致了FairSample，一个可以同时减轻这三种偏见的框架。我们采用了两种直观的策略来纠正图结构。首先，我们在不同敏感群体但具有相似节点特征的节点之间插入边。其次，为了增强模型的公平性并保持模型的质量，我们使用强化学习方法开发了一种可学习的邻居采样策略。

    Fairness in Graph Convolutional Neural Networks (GCNs) becomes a more and more important concern as GCNs are adopted in many crucial applications. Societal biases against sensitive groups may exist in many real world graphs. GCNs trained on those graphs may be vulnerable to being affected by such biases. In this paper, we adopt the well-known fairness notion of demographic parity and tackle the challenge of training fair and accurate GCNs efficiently. We present an in-depth analysis on how graph structure bias, node attribute bias, and model parameters may affect the demographic parity of GCNs. Our insights lead to FairSample, a framework that jointly mitigates the three types of biases. We employ two intuitive strategies to rectify graph structures. First, we inject edges across nodes that are in different sensitive groups but similar in node features. Second, to enhance model fairness and retain model quality, we develop a learnable neighbor sampling policy using reinforcement learni
    
[^36]: 渐进中点混合用于边距平衡和适度扩展的方法

    Asymptotic Midpoint Mixup for Margin Balancing and Moderate Broadening. (arXiv:2401.14696v1 [cs.LG])

    [http://arxiv.org/abs/2401.14696](http://arxiv.org/abs/2401.14696)

    提出了渐进中点混合方法来解决粗细转移学习中的类内崩塌问题。该方法通过逐渐将增强特征移动至类间特征对的中点，实现了边距平衡以及适度扩展边距的效果。

    

    在特征空间中，特征之间的崩塌导致了表示学习中的一些关键问题，使得特征无法区分。基于插值的数据增强方法，如mixup，在减轻不同类别之间的崩塌问题上显示出了有效性，这被称为类间崩塌。然而，粗细转移学习中引起的类内崩塌并未在增强方法中讨论。为了解决这些问题，我们提出了一种更好的特征增强方法，即渐进中点混合。该方法通过插值生成增强特征，但将它们逐渐移动到类间特征对的中点。结果是，该方法产生了两个效果：1）平衡所有类别的边距，2）仅适度扩展边距，直到达到最大置信度。我们通过测量可视化表示的对齐性和均匀性来经验分析了崩塌效应。然后，我们验证了增强方法所带来的类内崩塌问题。

    In the feature space, the collapse between features invokes critical problems in representation learning by remaining the features undistinguished. Interpolation-based augmentation methods such as mixup have shown their effectiveness in relieving the collapse problem between different classes, called inter-class collapse. However, intra-class collapse raised in coarse-to-fine transfer learning has not been discussed in the augmentation approach. To address them, we propose a better feature augmentation method, asymptotic midpoint mixup. The method generates augmented features by interpolation but gradually moves them toward the midpoint of inter-class feature pairs. As a result, the method induces two effects: 1) balancing the margin for all classes and 2) only moderately broadening the margin until it holds maximal confidence. We empirically analyze the collapse effects by measuring alignment and uniformity with visualizing representations. Then, we validate the intra-class collapse e
    
[^37]: 持续演化的图神经控制微分方程用于交通预测

    Continuously Evolving Graph Neural Controlled Differential Equations for Traffic Forecasting. (arXiv:2401.14695v1 [cs.LG])

    [http://arxiv.org/abs/2401.14695](http://arxiv.org/abs/2401.14695)

    本论文提出了一个名为持续演化图神经控制微分方程（CEGNCDE）的方法，用于同时捕捉连续的时空依赖关系和随时间演化的空间依赖关系。实验结果表明，该方法在交通预测任务上有很好的性能。

    

    作为发展智能城市的关键技术，交通预测已成为学术界和工业界的研究焦点。由于交通网络中存在复杂且动态的时空依赖关系，这一任务具有很高的挑战性。现有研究忽视了随时间演化的连续时空依赖关系。在本文中，我们提出了一种名为持续演化图神经控制微分方程（CEGNCDE）的方法，可以同时捕捉连续的时空依赖关系和随时间演化的空间依赖关系。具体来说，我们引入了一个基于NCDE的持续演化图生成器（CEGG），用于从离散的历史观测中生成随时间演化的空间依赖图。然后，我们引入了一个名为图神经控制微分方程（GNCDE）的框架，可以同时捕捉连续的时空依赖关系和随时间演化的空间依赖关系。大量的实验结果表明，CEGNCDE方法在交通预测任务上取得了很好的性能。

    As a crucial technique for developing a smart city, traffic forecasting has become a popular research focus in academic and industrial communities for decades. This task is highly challenging due to complex and dynamic spatial-temporal dependencies in traffic networks. Existing works ignore continuous temporal dependencies and spatial dependencies evolving over time. In this paper, we propose Continuously Evolving Graph Neural Controlled Differential Equations (CEGNCDE) to capture continuous temporal dependencies and spatial dependencies over time simultaneously. Specifically, a continuously evolving graph generator (CEGG) based on NCDE is introduced to generate the spatial dependencies graph that continuously evolves over time from discrete historical observations. Then, a graph neural controlled differential equations (GNCDE) framework is introduced to capture continuous temporal dependencies and spatial dependencies over time simultaneously. Extensive experiments demonstrate that CE
    
[^38]: TA-RNN：一种基于注意力机制的面向电子健康记录的时间感知递归神经网络架构

    TA-RNN: an Attention-based Time-aware Recurrent Neural Network Architecture for Electronic Health Records. (arXiv:2401.14694v1 [cs.LG])

    [http://arxiv.org/abs/2401.14694](http://arxiv.org/abs/2401.14694)

    TA-RNN和TA-RNN-AE是两种基于RNN的可解释深度学习架构，用于分析电子健康记录并预测患者的临床结果。这些架构考虑了EHR数据的不规则性和时间间隔，并采用时间嵌入的方法解决了这些问题。

    

    动机：电子健康记录（EHR）是患者医疗历史的全面资源。EHR对于利用深度学习（DL）等先进技术至关重要，使医疗提供者能够分析大量数据，提取有价值的见解，并做出精确、数据驱动的临床决策。DL方法如递归神经网络（RNN）已被用于分析EHR以建模疾病进展并预测诊断。然而，这些方法并没有解决EHR数据中一些固有的不规则性，如临床访问之间的不规则时间间隔。此外，大多数DL模型都不可解释。在这项研究中，我们提出了两种基于RNN的可解释DL架构，分别是时间感知RNN（TA-RNN）和TA-RNN-Autoencoder（TA-RNN-AE），用于预测下一次访问和多次未来访问中患者的临床结果。为了减轻不规则时间间隔的影响，我们提出了时间嵌入的方法将时间信息纳入模型中。

    Motivation: Electronic Health Records (EHR) represent a comprehensive resource of a patient's medical history. EHR are essential for utilizing advanced technologies such as deep learning (DL), enabling healthcare providers to analyze extensive data, extract valuable insights, and make precise and data-driven clinical decisions. DL methods such as Recurrent Neural Networks (RNN) have been utilized to analyze EHR to model disease progression and predict diagnosis. However, these methods do not address some inherent irregularities in EHR data such as irregular time intervals between clinical visits. Furthermore, most DL models are not interpretable. In this study, we propose two interpretable DL architectures based on RNN, namely Time-Aware RNN (TA-RNN) and TA-RNN-Autoencoder (TA-RNN-AE) to predict patient's clinical outcome in EHR at next visit and multiple visits ahead, respectively. To mitigate the impact of irregular time intervals, we propose incorporating time embedding of the elaps
    
[^39]: 从模糊到明亮的检测：基于YOLOv5的超分辨率航空物体检测

    From Blurry to Brilliant Detection: YOLOv5-Based Aerial Object Detection with Super Resolution. (arXiv:2401.14661v1 [cs.CV])

    [http://arxiv.org/abs/2401.14661](http://arxiv.org/abs/2401.14661)

    基于超分辨率和经过调整的轻量级YOLOv5架构，我们提出了一种创新的方法来解决航空影像中小而密集物体检测的挑战。我们的超分辨率YOLOv5模型采用Transformer编码器块，能够捕捉全局背景和上下文信息，从而在高密度、遮挡条件下提高检测结果。这种轻量级模型不仅准确性更高，而且资源利用效率高，非常适合实时应用。

    

    随着无人机和卫星技术的广泛应用，对航空影像中准确物体检测的需求大大增加。传统的物体检测模型在偏向大物体的数据集上训练，对于航空场景中普遍存在的小而密集的物体难以发挥最佳性能。为了解决这个挑战，我们提出了一种创新的方法，结合了超分辨率和经过调整的轻量级YOLOv5架构。我们使用多种数据集进行评估，包括VisDrone-2023、SeaDroneSee、VEDAI和NWPU VHR-10，以验证我们模型的性能。我们的超分辨率YOLOv5架构采用Transformer编码器块，使模型能够捕捉到全局背景和上下文信息，从而提高检测结果，特别是在高密度、遮挡条件下。这种轻量级模型不仅提供了更高的准确性，还确保了资源的有效利用，非常适合实时应用。我们的实验表明，我们的模型在航空物体检测任务中表现出色，特别是在复杂场景中。

    The demand for accurate object detection in aerial imagery has surged with the widespread use of drones and satellite technology. Traditional object detection models, trained on datasets biased towards large objects, struggle to perform optimally in aerial scenarios where small, densely clustered objects are prevalent. To address this challenge, we present an innovative approach that combines super-resolution and an adapted lightweight YOLOv5 architecture. We employ a range of datasets, including VisDrone-2023, SeaDroneSee, VEDAI, and NWPU VHR-10, to evaluate our model's performance. Our Super Resolved YOLOv5 architecture features Transformer encoder blocks, allowing the model to capture global context and context information, leading to improved detection results, especially in high-density, occluded conditions. This lightweight model not only delivers improved accuracy but also ensures efficient resource utilization, making it well-suited for real-time applications. Our experimental 
    
[^40]: 用球面卷积Wasserstein距离验证气候模型

    Validating Climate Models with Spherical Convolutional Wasserstein Distance. (arXiv:2401.14657v1 [physics.ao-ph])

    [http://arxiv.org/abs/2401.14657](http://arxiv.org/abs/2401.14657)

    引入了球面卷积Wasserstein距离来验证气候模型，相比传统方法更全面地衡量气候模型和再分析数据之间的差异，并应用于评估CMIP成员的模型输出。此外，研究发现CMIP第6阶段模型相较于第5阶段有适度改进。

    

    验证全球气候模型对于确保模型输出的准确性和有效性至关重要。我们引入球面卷积Wasserstein距离来更全面地衡量气候模型和再分析数据之间的差异。这个新的相似度测量方法利用卷积投影考虑了空间变异性，并量化了气候变量分布的局部差异。我们将该方法应用于评估耦合模式比较项目（CMIP）成员的历史模型输出，将其与观测数据和再分析数据产品进行比较。此外，我们研究了从CMIP第5阶段到第6阶段的进展，并发现第6阶段模型在生成真实气候学能力方面有适度改进。

    The validation of global climate models is crucial to ensure the accuracy and efficacy of model output. We introduce the spherical convolutional Wasserstein distance to more comprehensively measure differences between climate models and reanalysis data. This new similarity measure accounts for spatial variability using convolutional projections and quantifies local differences in the distribution of climate variables. We apply this method to evaluate the historical model outputs of the Coupled Model Intercomparison Project (CMIP) members by comparing them to observational and reanalysis data products. Additionally, we investigate the progression from CMIP phase 5 to phase 6 and find modest improvements in the phase 6 models regarding their ability to produce realistic climatologies.
    
[^41]: 面向保险争议的韩国法律判决预测数据集

    A Korean Legal Judgment Prediction Dataset for Insurance Disputes. (arXiv:2401.14654v1 [cs.CL])

    [http://arxiv.org/abs/2401.14654](http://arxiv.org/abs/2401.14654)

    这篇论文介绍了一种面向保险争议的韩国法律判决预测数据集，研究发现在数据有限的情况下，使用句子转换器微调方法可以实现与大型数据集相似的性能。

    

    本文介绍了一种面向保险争议的韩国法律判决预测（LJP）数据集。成功预测保险争议的LJP模型可以使保险公司及其客户受益。它可以通过预测如果进行争议调解过程，结果将如何出现来节省双方的时间和金钱。正如低资源语言经常面临的情况一样，该特定任务的可用数据量有限。为了缓解这个问题，我们研究了如何在数据有限的情况下实现良好的性能。在我们的实验中，我们证明了句子转换器微调（SetFit）（Tunstall等，2022）是在数据有限的情况下的标准微调的良好替代方法。使用SetFit方法在我们的数据上进行微调的模型与韩国LJP基准模型（Hwang等，2022）在性能上显示出相似的表现，尽管数据规模要小得多。

    This paper introduces a Korean legal judgment prediction (LJP) dataset for insurance disputes. Successful LJP models on insurance disputes can benefit insurance companies and their customers. It can save both sides' time and money by allowing them to predict how the result would come out if they proceed to the dispute mediation process. As is often the case with low-resource languages, there is a limitation on the amount of data available for this specific task. To mitigate this issue, we investigate how one can achieve a good performance despite the limitation in data. In our experiment, we demonstrate that Sentence Transformer Fine-tuning (SetFit, Tunstall et al., 2022) is a good alternative to standard fine-tuning when training data are limited. The models fine-tuned with the SetFit approach on our data show similar performance to the Korean LJP benchmark models (Hwang et al., 2022) despite the much smaller data size.
    
[^42]: 回归问题的全能预测器与凸函数的近似秩

    Omnipredictors for Regression and the Approximate Rank of Convex Functions. (arXiv:2401.14645v1 [cs.LG])

    [http://arxiv.org/abs/2401.14645](http://arxiv.org/abs/2401.14645)

    全能预测器是一种预测器，其预测的损失小于任意损失下的最佳假设。我们提出了关于损失函数的充分统计量的概念，可以用来最小化任何损失的期望损失。

    

    考虑在监督学习环境中，目标是学习通过给定分布中的点𝐱来预测标签𝐲。对于损失函数类𝒀和假设类𝒞，全能预测器是一种预测器，其预测的损失小于𝒀中任意损失下的最佳假设。自从引入这个概念的[GKR+21]的工作以来，在二分类标签的设置下，即𝐲∈{0,1}，已经有大量的工作，但是对于连续标签𝐲∈[0,1]的回归设置却知之甚少。我们的主要概念贡献是关于一类损失函数的损失最小化的充分统计量的概念：这是一组关于分布的统计量，通过了解它们可以采取最小化任何损失的期望损失的行动。充分统计量的概念直接相关到凸函数近似的概念上。

    Consider the supervised learning setting where the goal is to learn to predict labels $\mathbf y$ given points $\mathbf x$ from a distribution. An \textit{omnipredictor} for a class $\mathcal L$ of loss functions and a class $\mathcal C$ of hypotheses is a predictor whose predictions incur less expected loss than the best hypothesis in $\mathcal C$ for every loss in $\mathcal L$. Since the work of [GKR+21] that introduced the notion, there has been a large body of work in the setting of binary labels where $\mathbf y \in \{0, 1\}$, but much less is known about the regression setting where $\mathbf y \in [0,1]$ can be continuous. Our main conceptual contribution is the notion of \textit{sufficient statistics} for loss minimization over a family of loss functions: these are a set of statistics about a distribution such that knowing them allows one to take actions that minimize the expected loss for any loss in the family. The notion of sufficient statistics relates directly to the approx
    
[^43]: 从深度学习模型中推断数据前提，以实现可靠的部署预测

    Inferring Data Preconditions from Deep Learning Models for Trustworthy Prediction in Deployment. (arXiv:2401.14628v1 [cs.SE])

    [http://arxiv.org/abs/2401.14628](http://arxiv.org/abs/2401.14628)

    本文提出了一种从深度学习模型中推断数据前提的新技术，用于确定其预测的可靠性。该方法使用了从神经网络计算中推导出的规则，并引入了一种新颖的抽象表示，以进行最弱前提推理。

    

    在开发阶段，深度学习模型是根据对数据的某些假设进行训练的，然后在部署阶段用于预测。在部署期间，推理模型对于未见数据的预测结果的可靠性非常重要。现有的用于指定和验证传统软件的方法对于处理DNN模型架构和预期结果的复杂性不足。在这项工作中，我们提出了一种新颖的技术，利用从神经网络计算中推导出的规则来推断一个DNN模型的数据前提，以确定其预测的可靠性。我们的方法DeepInfer涉及引入一种新颖的训练DNN模型抽象表示，以使用Dijkstra的谓词变换语义进行最弱前提推理。通过导出在归纳类型的神经网络抽象表示上的规则，我们可以克服由于反向传播算法引起的矩阵维度问题。

    Deep learning models are trained with certain assumptions about the data during the development stage and then used for prediction in the deployment stage. It is important to reason about the trustworthiness of the model's predictions with unseen data during deployment. Existing methods for specifying and verifying traditional software are insufficient for this task, as they cannot handle the complexity of DNN model architecture and expected outcomes. In this work, we propose a novel technique that uses rules derived from neural network computations to infer data preconditions for a DNN model to determine the trustworthiness of its predictions. Our approach, DeepInfer involves introducing a novel abstraction for a trained DNN model that enables weakest precondition reasoning using Dijkstra's Predicate Transformer Semantics. By deriving rules over the inductive type of neural network abstract representation, we can overcome the matrix dimensionality issues that arise from the backward n
    
[^44]: 弹性实用的测试时间适应：软批量归一化对齐和熵驱动的内存库

    Resilient Practical Test-Time Adaptation: Soft Batch Normalization Alignment and Entropy-driven Memory Bank. (arXiv:2401.14619v1 [cs.LG])

    [http://arxiv.org/abs/2401.14619](http://arxiv.org/abs/2401.14619)

    本文提出了一种弹性实用的测试时间适应方法，通过软批量归一化对齐和熵驱动的内存库来减轻目标领域转移和非独立同分布问题，并提高数据质量。

    

    测试时间域适应有效地调整源域模型以适应目标域中的未见领域转移，在推理过程中。然而，连续的目标领域分布变化以及在实际场景中经常遇到的非独立同分布（non-i.i.d.）测试样本，可能会显著影响模型性能。虽然现有的内存库方法使用内存存储样本并减轻non-i.i.d.效应，但它们并不能从根本上防止潜在的模型退化。为了解决这个问题，我们提出了一种弹性实用的测试时间适应（ResiTTA）方法，重点关注参数弹性和数据质量。具体而言，我们开发了一种弹性批量归一化方法，通过估计归一化统计量和软对齐来减轻过拟合和模型退化。我们使用了一个熵驱动的内存库，考虑了时效性、过分自信样本的持久性以及样本的不确定性，以获得高质量的结果。

    Test-time domain adaptation effectively adjusts the source domain model to accommodate unseen domain shifts in a target domain during inference. However, the model performance can be significantly impaired by continuous distribution changes in the target domain and non-independent and identically distributed (non-i.i.d.) test samples often encountered in practical scenarios. While existing memory bank methodologies use memory to store samples and mitigate non-i.i.d. effects, they do not inherently prevent potential model degradation. To address this issue, we propose a resilient practical test-time adaptation (ResiTTA) method focused on parameter resilience and data quality. Specifically, we develop a resilient batch normalization with estimation on normalization statistics and soft alignments to mitigate overfitting and model degradation. We use an entropy-driven memory bank that accounts for timeliness, the persistence of over-confident samples, and sample uncertainty for high-qualit
    
[^45]: 异质介质中具有不可用时间变化界面的工业系统建模的物理信息同步自适应学习

    Physically Informed Synchronic-adaptive Learning for Industrial Systems Modeling in Heterogeneous Media with Unavailable Time-varying Interface. (arXiv:2401.14609v1 [cs.LG])

    [http://arxiv.org/abs/2401.14609](http://arxiv.org/abs/2401.14609)

    物理信息同步自适应学习方法（PISAL）用于解决工业系统在异质介质中建模的PDEs问题，通过构建Net1、Net2和NetI来同步逼近解和自适应学习不可用的时间变化界面。

    

    偏微分方程（PDEs）通常用于建模具有多变量依赖性的复杂工业系统。现有的物理信息神经网络（PINNs）在均质介质中解决PDEs方面表现出色。然而，当PDE参数由于缺乏物理属性和无法获得时间变化界面而不知道时，它们的可行性会降低，这是由于异质介质引起的。为此，我们提出了一种数据-物理混合方法，物理信息同步自适应学习（PISAL），以解决用于异质介质中的工业系统建模的PDEs问题。首先，构建Net1、Net2和NetI来逼近满足PDEs和界面的解。Net1和Net2被用于同步学习满足具有不同参数的每个PDEs的解，而NetI则用于自适应学习不可用的时间变化界面。然后，引入一个与NetI相结合的标准来自适应地区分属性。

    Partial differential equations (PDEs) are commonly employed to model complex industrial systems characterized by multivariable dependence. Existing physics-informed neural networks (PINNs) excel in solving PDEs in a homogeneous medium. However, their feasibility is diminished when PDE parameters are unknown due to a lack of physical attributions and time-varying interface is unavailable arising from heterogeneous media. To this end, we propose a data-physics-hybrid method, physically informed synchronic-adaptive learning (PISAL), to solve PDEs for industrial systems modeling in heterogeneous media. First, Net1, Net2, and NetI, are constructed to approximate the solutions satisfying PDEs and the interface. Net1 and Net2 are utilized to synchronously learn each solution satisfying PDEs with diverse parameters, while NetI is employed to adaptively learn the unavailable time-varying interface. Then, a criterion combined with NetI is introduced to adaptively distinguish the attributions of 
    
[^46]: 利用Ricci流引导的自编码器学习时变动力学

    Ricci flow-guided autoencoders in learning time-dependent dynamics. (arXiv:2401.14591v1 [cs.LG])

    [http://arxiv.org/abs/2401.14591](http://arxiv.org/abs/2401.14591)

    利用Ricci流引导的自编码器方法能够学习非线性动力学，尤其是偏微分方程。该方法通过在训练中学习流形，并使用Ricci流使流形潜空间逐步适应动力学的变化，从而获得更好的表示能力。在实验中，我们展示了该方法在具有周期性和随机性的PDE上的应用，并评估了在分布内和外推场景中的误差。

    

    我们提出了一种基于流形的自编码器方法，用于学习时间上的非线性动力学，尤其是偏微分方程（PDE），其中流形潜空间根据Ricci流发展。这可以通过在物理信息设置中模拟Ricci流来实现，并且可以匹配流形量，以便实现Ricci流。使用我们的方法，流形是作为训练过程的一部分学习的，因此可以识别出理想的几何形状，同时演变也能在静态方法上引起更宽容的潜在表示。我们在一系列数值实验中展示了我们的方法，包括具有周期性和随机性等理想特征的PDE，并在分布内和外推场景中进行误差评估。

    We present a manifold-based autoencoder method for learning nonlinear dynamics in time, notably partial differential equations (PDEs), in which the manifold latent space evolves according to Ricci flow. This can be accomplished by simulating Ricci flow in a physics-informed setting, and manifold quantities can be matched so that Ricci flow is empirically achieved. With our methodology, the manifold is learned as part of the training procedure, so ideal geometries may be discerned, while the evolution simultaneously induces a more accommodating latent representation over static methods. We present our method on a range of numerical experiments consisting of PDEs that encompass desirable characteristics such as periodicity and randomness, remarking error on in-distribution and extrapolation scenarios.
    
[^47]: 这里是翻译过的论文标题

    Diffusion Stochastic Optimization for Min-Max Problems. (arXiv:2401.14585v1 [cs.LG])

    [http://arxiv.org/abs/2401.14585](http://arxiv.org/abs/2401.14585)

    这里是中文总结出的一句话要点

    

    这里是翻译过的论文摘要

    The optimistic gradient method is useful in addressing minimax optimization problems. Motivated by the observation that the conventional stochastic version suffers from the need for a large batch size on the order of $\mathcal{O}(\varepsilon^{-2})$ to achieve an $\varepsilon$-stationary solution, we introduce and analyze a new formulation termed Diffusion Stochastic Same-Sample Optimistic Gradient (DSS-OG). We prove its convergence and resolve the large batch issue by establishing a tighter upper bound, under the more general setting of nonconvex Polyak-Lojasiewicz (PL) risk functions. We also extend the applicability of the proposed method to the distributed scenario, where agents communicate with their neighbors via a left-stochastic protocol. To implement DSS-OG, we can query the stochastic gradient oracles in parallel with some extra memory overhead, resulting in a complexity comparable to its conventional counterpart. To demonstrate the efficacy of the proposed algorithm, we condu
    
[^48]: 设计你自己的宇宙：一种物理信息引导的无偏方法来增强图神经网络

    Design Your Own Universe: A Physics-Informed Agnostic Method for Enhancing Graph Neural Networks. (arXiv:2401.14580v1 [cs.LG])

    [http://arxiv.org/abs/2401.14580](http://arxiv.org/abs/2401.14580)

    本文提出了一种物理信息引导的无偏方法来增强图神经网络，通过引入附加节点和使用正负权重重连连接来丰富图结构，以解决过度平滑和过度压缩的问题。

    

    物理信息引导的图神经网络通过缓解常见的GNN挑战（如过度平滑化、过度压缩和异质适应）在学习图结构数据方面取得了显著的性能。尽管取得了这些进展，仍然在开发一种简单而有效的范式来适当地整合处理所有这些挑战的先前方法。在本文中，我们将GNN的传播与物理粒子系统进行类比，提出了一种模型无关的增强框架。该框架通过引入附加节点和使用正负权重重连连接来丰富图结构，受节点标记信息的指导。我们理论上验证了通过我们的方法增强的GNN可以有效地避免过度平滑问题，并对过度压缩具有鲁棒性。此外，我们对重连图进行了谱分析，证明了相应的GNN可以...

    Physics-informed Graph Neural Networks have achieved remarkable performance in learning through graph-structured data by mitigating common GNN challenges such as over-smoothing, over-squashing, and heterophily adaption. Despite these advancements, the development of a simple yet effective paradigm that appropriately integrates previous methods for handling all these challenges is still underway. In this paper, we draw an analogy between the propagation of GNNs and particle systems in physics, proposing a model-agnostic enhancement framework. This framework enriches the graph structure by introducing additional nodes and rewiring connections with both positive and negative weights, guided by node labeling information. We theoretically verify that GNNs enhanced through our approach can effectively circumvent the over-smoothing issue and exhibit robustness against over-squashing. Moreover, we conduct a spectral analysis on the rewired graph to demonstrate that the corresponding GNNs can f
    
[^49]: GOAt: 通过图输出属性解释图神经网络

    GOAt: Explaining Graph Neural Networks via Graph Output Attribution. (arXiv:2401.14578v1 [cs.LG])

    [http://arxiv.org/abs/2401.14578](http://arxiv.org/abs/2401.14578)

    本论文引入了一种名为Graph Output Attribution（GOAt）的新方法，通过将GNN扩展为涉及节点特征、边特征和激活模式的标量积之和，计算每个节点或边特征对每个标量积的贡献，并将贡献聚合起来，从而实现将图输出归因于输入图特征的GNN解释。

    

    理解图神经网络（GNNs）的决策过程对于其可解释性至关重要。现有的大多数解释GNNs的方法通常依赖于训练辅助模型，导致解释结果仍然是黑盒的。本文介绍了一种名为Graph Output Attribution（GOAt）的新方法，用于将图输出归因于输入图特征，从而创建既忠实、有区别，又在相似样本上稳定的GNN解释。通过将GNN扩展为涉及节点特征、边特征和激活模式的标量积之和，我们提出了一种高效的分析方法，用于计算每个节点或边特征对每个标量积的贡献，并将扩展形式中所有标量积的贡献聚合起来，以推导出每个节点和边的重要性。通过对合成数据和现实世界数据的广泛实验证明，我们的方法不仅在评估GNN解释器的性能方面优于各种最新方法，

    Understanding the decision-making process of Graph Neural Networks (GNNs) is crucial to their interpretability. Most existing methods for explaining GNNs typically rely on training auxiliary models, resulting in the explanations remain black-boxed. This paper introduces Graph Output Attribution (GOAt), a novel method to attribute graph outputs to input graph features, creating GNN explanations that are faithful, discriminative, as well as stable across similar samples. By expanding the GNN as a sum of scalar products involving node features, edge features and activation patterns, we propose an efficient analytical method to compute contribution of each node or edge feature to each scalar product and aggregate the contributions from all scalar products in the expansion form to derive the importance of each node and edge. Through extensive experiments on synthetic and real-world data, we show that our method not only outperforms various state-ofthe-art GNN explainers in terms of the comm
    
[^50]: PrivStream：一种用于流式差分隐私数据的算法

    PrivStream: An Algorithm for Streaming Differentially Private Data. (arXiv:2401.14577v1 [cs.DB])

    [http://arxiv.org/abs/2401.14577](http://arxiv.org/abs/2401.14577)

    PrivStream是一种用于流式差分隐私数据的算法，可以解决离线应用中的隐私保护和数据效用问题。算法可以针对空间数据集进行合成流数据生成，并提供了通用的在线选择性计数框架，验证了算法的实用性。

    

    许多差分隐私研究都着重于假设所有数据一次性可用的离线应用。但当这些算法应用于实际中的数据流，要么违反了隐私保证，要么导致了糟糕的效用。我们提出了一种针对空间数据集的差分隐私合成流数据生成算法，并提供了一种在线选择性计数的通用框架，该框架可用于查询应答和合成数据生成等多个任务。我们的算法在真实数据集和模拟数据集上进行了实验验证。

    Much of the research in differential privacy has focused on offline applications with the assumption that all data is available at once. When these algorithms are applied in practice to streams where data is collected over time, this either violates the privacy guarantees or results in poor utility. We derive an algorithm for differentially private synthetic streaming data generation, especially curated towards spatial datasets. Furthermore, we provide a general framework for online selective counting among a collection of queries which forms a basis for many tasks such as query answering and synthetic data generation. The utility of our algorithm is verified on both real-world and simulated datasets.
    
[^51]: 不同的循环内核拓展到不同的储备计算拓扑的研究

    Extension of Recurrent Kernels to different Reservoir Computing topologies. (arXiv:2401.14557v1 [cs.LG])

    [http://arxiv.org/abs/2401.14557](http://arxiv.org/abs/2401.14557)

    该研究通过提供特定RC体系结构与相应循环内核形式等价性的经验分析，填补了Leaky RC、Sparse RC和Deep RC等已建立的RC范例尚未进行分析的空白。此外，研究还揭示了稀疏连接在RC体系结构中的作用，并提出了一种依赖储备大小的最佳稀疏性水平。最后，该研究的系统分析表明，在Deep RC模型中，通过减小尺寸的连续储备可以更好地实现收敛。

    

    近年来，由于其快速高效的计算能力，储备计算（RC）变得越来越受欢迎。标准的RC在渐近极限下已被证明与循环内核等效，这有助于分析其表达能力。然而，许多已建立的RC范例，如Leaky RC、Sparse RC和Deep RC，尚未以这种方式进行分析。本研究旨在通过提供特定RC体系结构与相应循环内核形式等价性的经验分析来填补这一空白。我们通过改变每个体系结构中实施的激活函数进行收敛研究。我们的研究还揭示了稀疏连接在RC体系结构中的作用，并提出了一种依赖储备大小的最佳稀疏性水平。此外，我们的系统分析表明，在Deep RC模型中，通过减小尺寸的连续储备可以更好地实现收敛。

    Reservoir Computing (RC) has become popular in recent years due to its fast and efficient computational capabilities. Standard RC has been shown to be equivalent in the asymptotic limit to Recurrent Kernels, which helps in analyzing its expressive power. However, many well-established RC paradigms, such as Leaky RC, Sparse RC, and Deep RC, are yet to be analyzed in such a way. This study aims to fill this gap by providing an empirical analysis of the equivalence of specific RC architectures with their corresponding Recurrent Kernel formulation. We conduct a convergence study by varying the activation function implemented in each architecture. Our study also sheds light on the role of sparse connections in RC architectures and propose an optimal sparsity level that depends on the reservoir size. Furthermore, our systematic analysis shows that in Deep RC models, convergence is better achieved with successive reservoirs of decreasing sizes.
    
[^52]: 在视觉基础模型时代重新审视主动学习

    Revisiting Active Learning in the Era of Vision Foundation Models. (arXiv:2401.14555v1 [cs.CV])

    [http://arxiv.org/abs/2401.14555](http://arxiv.org/abs/2401.14555)

    本文评估了基础视觉模型对有效主动学习的三个关键组成部分的影响，并提出了一个新的简单优雅的主动学习策略，该策略通过平衡不确定性估计和样本多样性来实现。

    

    基础视觉或视觉-语言模型是在大规模无标签或噪声数据上训练的，并学习到可以在各种任务上实现令人印象深刻的零标注或少标注性能的鲁棒表示。鉴于这些特性，它们是主动学习（AL）的自然选择，旨在实现标记效率的最大化，但在低预算条件下，基础模型的全部潜力在AL环境中尚未得到探索。在这项工作中，我们评估了基础模型对有效AL的三个关键组成部分的影响，即1）初始标记样本池的选择，2）确保多样性抽样，以及3）代表性和不确定性抽样之间的权衡。我们系统地研究了基础模型（DINOv2、OpenCLIP）的鲁棒表示如何挑战已有的主动学习结果。我们的观察结果为一个新的简单优雅的AL策略的有原则构建提供了指导，该策略通过使用dropout估计不确定性和样本多样性之间的平衡。

    Foundation vision or vision-language models are trained on large unlabeled or noisy data and learn robust representations that can achieve impressive zeroor few-shot performance on diverse tasks. Given these properties, they are a natural fit for active learning (AL), which aims to maximize labeling efficiency, but the full potential of foundation models has not been explored in the context of AL, specifically in the low-budget regime. In this work, we evaluate how foundation models influence three critical components of effective AL, namely, 1) initial labeled pool selection, 2) ensuring diverse sampling, and 3) the trade-off between representative and uncertainty sampling. We systematically study how the robust representations of foundation models (DINOv2, OpenCLIP) challenge existing findings in active learning. Our observations inform the principled construction of a new simple and elegant AL strategy that balances uncertainty estimated via dropout with sample diversity. We exten
    
[^53]: 基于高斯Cox过程模型的时空数据的贝叶斯优化

    Bayesian Optimization through Gaussian Cox Process Models for Spatio-temporal Data. (arXiv:2401.14544v1 [cs.LG])

    [http://arxiv.org/abs/2401.14544](http://arxiv.org/abs/2401.14544)

    本文提出了一种最大后验推断高斯Cox过程的方法，实现了对时空数据进行贝叶斯优化，扩展了现有工作，通过使用该结果，提出了一个基于高斯Cox过程模型的贝叶斯优化框架。

    

    贝叶斯优化已成为一种有效优化昂贵评估函数的主要策略。现有的贝叶斯优化方法主要依赖于高斯过程替代模型，并不适用于（双随机）高斯Cox过程，其中观测过程由作为高斯过程模型的潜在强度函数调制。在本文中，我们提出了一种新的最大后验推断高斯Cox过程的方法。它利用拉普拉斯近似和核函数变换技术，将问题转化为一个新的再生核希尔伯特空间，在这个空间中计算更容易处理。这使我们能够获得潜在强度函数的函数后验和后验的协方差，从而扩展了现有工作通常关注特定的连接函数或估计后验均值的问题。利用这个结果，我们提出了一个基于高斯Cox过程模型的贝叶斯优化框架，并进一步发展了Nystrom近似。

    Bayesian optimization (BO) has established itself as a leading strategy for efficiently optimizing expensive-to-evaluate functions. Existing BO methods mostly rely on Gaussian process (GP) surrogate models and are not applicable to (doubly-stochastic) Gaussian Cox processes, where the observation process is modulated by a latent intensity function modeled as a GP. In this paper, we propose a novel maximum a posteriori inference of Gaussian Cox processes. It leverages the Laplace approximation and change of kernel technique to transform the problem into a new reproducing kernel Hilbert space, where it becomes more tractable computationally. It enables us to obtain both a functional posterior of the latent intensity function and the covariance of the posterior, thus extending existing works that often focus on specific link functions or estimating the posterior mean. Using the result, we propose a BO framework based on the Gaussian Cox process model and further develop a Nystr\"om approx
    
[^54]: 理解事后机器学习解释中的差异

    Understanding Disparities in Post Hoc Machine Learning Explanation. (arXiv:2401.14539v1 [cs.LG])

    [http://arxiv.org/abs/2401.14539](http://arxiv.org/abs/2401.14539)

    该研究通过模拟和实验评估了事后机器学习解释中的差异，并发现协变量偏移、概念转变和省略协变量会增加解释差异，对神经网络影响更大。

    

    先前的研究已经指出，现有的事后解释方法在解释准确性上存在差异（涉及“种族”和“性别”等敏感属性），虽然已有大量研究致力于在解释度量水平上减少这些问题，但数据生成过程和黑盒模型与解释差异之间的关系仍然未被广泛探讨。因此，通过模拟和在真实数据集上的实验，我们特别评估了解释差异面临的挑战：数据性质引起的局限样本量、协变量偏移、概念转变、被省略的变量偏差，以及模型性质引起的挑战：敏感属性的包含和适当的函数形式。通过受控模拟分析，我们的研究证明增加协变量偏移、概念转变和省略协变量会增加解释差异，对于神经网络而言，这种效应更加显著。

    Previous work has highlighted that existing post-hoc explanation methods exhibit disparities in explanation fidelity (across 'race' and 'gender' as sensitive attributes), and while a large body of work focuses on mitigating these issues at the explanation metric level, the role of the data generating process and black box model in relation to explanation disparities remains largely unexplored. Accordingly, through both simulations as well as experiments on a real-world dataset, we specifically assess challenges to explanation disparities that originate from properties of the data: limited sample size, covariate shift, concept shift, omitted variable bias, and challenges based on model properties: inclusion of the sensitive attribute and appropriate functional form. Through controlled simulation analyses, our study demonstrates that increased covariate shift, concept shift, and omission of covariates increase explanation disparities, with the effect pronounced higher for neural network 
    
[^55]: CaRiNG: 在非可逆生成过程下学习时间因果表示

    CaRiNG: Learning Temporal Causal Representation under Non-Invertible Generation Process. (arXiv:2401.14535v1 [cs.LG])

    [http://arxiv.org/abs/2401.14535](http://arxiv.org/abs/2401.14535)

    CaRiNG提出了一种基于可辨识性理论的方法，用于学习具有非可逆生成过程的时间因果表示。这种方法能够恢复独立的潜在组分，即使它们来自于非线性且非可逆的混合过程。

    

    鉴别顺序数据中潜在的延迟时间因果过程对于把握时间动力学和进行下游推理至关重要。尽管最近的一些方法可以稳健地识别这些潜在的因果变量，但它们依赖于从潜在变量到观测数据的可逆生成过程的严格假设。然而，这些假设通常在包含信息损失的现实应用中难以满足。例如，视觉感知过程将3D空间转化为2D图像，或者视觉坚持现象在当前感知中融入历史数据。为了解决这个挑战，我们建立了一个可辨识性理论，允许在非线性和非可逆混合情况下恢复独立的潜在组分。在此理论基础上，我们提出了一种基于原则的方法，CaRiNG，用于学习具有可辨识性的非可逆生成时间数据的因果表示。

    Identifying the underlying time-delayed latent causal processes in sequential data is vital for grasping temporal dynamics and making downstream reasoning. While some recent methods can robustly identify these latent causal variables, they rely on strict assumptions about the invertible generation process from latent variables to observed data. However, these assumptions are often hard to satisfy in real-world applications containing information loss. For instance, the visual perception process translates a 3D space into 2D images, or the phenomenon of persistence of vision incorporates historical data into current perceptions. To address this challenge, we establish an identifiability theory that allows for the recovery of independent latent components even when they come from a nonlinear and non-invertible mix. Using this theory as a foundation, we propose a principled approach, CaRiNG, to learn the CAusal RepresentatIon of Non-invertible Generative temporal data with identifiability
    
[^56]: 元学习线性二次调节器: 一种针对无模型LQR的策略梯度MAML方法

    Meta-Learning Linear Quadratic Regulators: A Policy Gradient MAML Approach for the Model-free LQR. (arXiv:2401.14534v1 [math.OC])

    [http://arxiv.org/abs/2401.14534](http://arxiv.org/abs/2401.14534)

    本论文研究了在多任务、异构和无模型环境下学习线性二次调节器（LQR）的问题，提出了一种基于策略梯度元学习（MAML）方法的解决方案。该方法能够产生与每个任务特定的最优控制器接近的控制器，并在模型基础设置下以线性收敛速率实现。

    

    我们研究在多任务、异构和无模型环境中学习线性二次调节器（LQR）的问题。我们对一种基于策略梯度的模型不可知元学习（MAML）方法（Finn等人，2017）在不同任务异质性设置下的LQR问题的稳定性和个性化保证进行了刻画。我们证明在模型基础和无模型设置下，MAML-LQR方法产生的控制器与每个任务特定的最优控制器接近，除了任务异质性偏差。此外，我们还展示了在模型基础设置下，这种控制器以线性收敛速率实现，这改进了现有MAML-LQR工作中的次线性速率。与现有的MAML-LQR结果相比，我们的理论保证表明学习到的控制器可以高效地适应未知的LQR任务。

    We investigate the problem of learning Linear Quadratic Regulators (LQR) in a multi-task, heterogeneous, and model-free setting. We characterize the stability and personalization guarantees of a Policy Gradient-based (PG) Model-Agnostic Meta-Learning (MAML) (Finn et al., 2017) approach for the LQR problem under different task-heterogeneity settings. We show that the MAML-LQR approach produces a stabilizing controller close to each task-specific optimal controller up to a task-heterogeneity bias for both model-based and model-free settings. Moreover, in the model-based setting, we show that this controller is achieved with a linear convergence rate, which improves upon sub-linear rates presented in existing MAML-LQR work. In contrast to existing MAML-LQR results, our theoretical guarantees demonstrate that the learned controller can efficiently adapt to unseen LQR tasks.
    
[^57]: 大型语言模型中的相对价值偏差

    Relative Value Biases in Large Language Models. (arXiv:2401.14530v1 [cs.CL])

    [http://arxiv.org/abs/2401.14530](http://arxiv.org/abs/2401.14530)

    该研究发现大型语言模型在做选择时表现出了与人类和动物相似的相对价值偏差，这对于理解人类选择中的背景依赖性机制具有重要意义。

    

    人类和动物在强化学习方面的研究表明，即使那些选项与较低的绝对奖励相关，他们更倾向于选择过去相对更好结果的选项。本研究测试了大型语言模型是否会表现出类似的偏差。我们让gpt-4-1106-preview(GPT-4 Turbo)和Llama-2-70B在最大化回报的目标下反复在选项对之间进行选择。每个提示中都包含了先前结果的完整记录。两个模型表现出了与人类和动物观察到的相对价值决策偏差类似的行为。更明确地进行结果之间的相对比较会放大这种偏差，而促使模型估计预期结果会使偏差消失。这些结果对于了解人类选择中贡献到背景依赖性的潜在机制具有重要意义。

    Studies of reinforcement learning in humans and animals have demonstrated a preference for options that yielded relatively better outcomes in the past, even when those options are associated with lower absolute reward. The present study tested whether large language models would exhibit a similar bias. We had gpt-4-1106-preview (GPT-4 Turbo) and Llama-2-70B make repeated choices between pairs of options with the goal of maximizing payoffs. A complete record of previous outcomes was included in each prompt. Both models exhibited relative value decision biases similar to those observed in humans and animals. Making relative comparisons among outcomes more explicit magnified the bias, whereas prompting the models to estimate expected outcomes caused the bias to disappear. These results have implications for the potential mechanisms that contribute to context-dependent choice in human agents.
    
[^58]: 以质量守恒感知器为基础，实现可解释的物理-概念集水区尺度水文建模

    Towards Interpretable Physical-Conceptual Catchment-Scale Hydrological Modeling using the Mass-Conserving-Perceptron. (arXiv:2401.14521v1 [cs.LG])

    [http://arxiv.org/abs/2401.14521](http://arxiv.org/abs/2401.14521)

    本研究通过利用质量守恒感知器构建基于有向图结构的水文模型，实现了对集水区尺度水文过程的解释能力，在保持简洁性的同时能够准确地模拟各种流量动力学行为，并通过引入输入旁路机制进一步优化了模型的表现。

    

    本研究探讨了利用机器学习技术开发简洁可解释的集水区尺度水文模型的可行性，采用基于质量守恒感知器（MCP）的有向图结构作为基本计算单元。我们关注的是单个位置的结构复杂性（深度），而不是对大样本集水区具有普适性的广度。目标是发现一个最小的表示（单元状态数和流量路径数），用于表示能够解释给定集水区输入状态和输出行为的主要过程，特别强调模拟全范围（高、中、低）的流量动力学。我们发现，在我们的研究区域，采用类似HyMod的架构，具有3个单元状态和2个主要流动路径，能够实现这样的表示，但引入输入旁路机制可以显著改善水文图的时间和形状。

    We investigate the applicability of machine learning technologies to the development of parsimonious, interpretable, catchment-scale hydrologic models using directed-graph architectures based on the mass-conserving perceptron (MCP) as the fundamental computational unit. Here, we focus on architectural complexity (depth) at a single location, rather than universal applicability (breadth) across large samples of catchments. The goal is to discover a minimal representation (numbers of cell-states and flow paths) that represents the dominant processes that can explain the input-state-output behaviors of a given catchment, with particular emphasis given to simulating the full range (high, medium, and low) of flow dynamics. We find that a HyMod-like architecture with three cell-states and two major flow pathways achieves such a representation at our study location, but that the additional incorporation of an input-bypass mechanism significantly improves the timing and shape of the hydrograph
    
[^59]: 我们错过了谁？一种基于原则的揭示少数人群特征的方法

    Who Are We Missing? A Principled Approach to Characterizing the Underrepresented Population. (arXiv:2401.14512v1 [stat.ME])

    [http://arxiv.org/abs/2401.14512](http://arxiv.org/abs/2401.14512)

    本文提出了一种基于优化的方法，Rashomon Set of Optimal Trees (ROOT)，用于识别和描述随机对照试验中的少数人群。该方法通过最小化目标平均处理效应估计的方差来优化目标子群体分布，从而提供更精确和可解释的处理效应估计。与其他方法相比，该方法具有更高的精度和可解释性，通过合成数据实验进行了验证。

    

    随机对照试验在理解因果效应方面起到了关键作用，然而将推论扩展到目标人群时面临效应异质性和代表性不足的挑战。我们的论文解决了在随机对照试验中识别和描述少数人群的关键问题，提出了一种改进目标人群以提升普适性的创新框架。我们引入了一种基于优化的方法——Rashomon Set of Optimal Trees (ROOT)，来描述少数人群。ROOT通过最小化目标平均处理效应估计的方差来优化目标子群体分布，从而确保更精确的处理效应估计。值得注意的是，ROOT生成可解释的少数人群特征，有助于研究人员有效沟通。我们的方法在精度和可解释性方面相对于其他方法展现了改进，通过合成数据实验进行了验证。

    Randomized controlled trials (RCTs) serve as the cornerstone for understanding causal effects, yet extending inferences to target populations presents challenges due to effect heterogeneity and underrepresentation. Our paper addresses the critical issue of identifying and characterizing underrepresented subgroups in RCTs, proposing a novel framework for refining target populations to improve generalizability. We introduce an optimization-based approach, Rashomon Set of Optimal Trees (ROOT), to characterize underrepresented groups. ROOT optimizes the target subpopulation distribution by minimizing the variance of the target average treatment effect estimate, ensuring more precise treatment effect estimations. Notably, ROOT generates interpretable characteristics of the underrepresented population, aiding researchers in effective communication. Our approach demonstrates improved precision and interpretability compared to alternatives, as illustrated with synthetic data experiments. We ap
    
[^60]: 学习何时在电力受限设备上进行长期交通数据收集

    Learning When to See for Long-term Traffic Data Collection on Power-constrained Devices. (arXiv:2401.14504v1 [eess.SY])

    [http://arxiv.org/abs/2401.14504](http://arxiv.org/abs/2401.14504)

    本研究介绍了一种学习基于框架，通过策略性地决定观测时间并从稀疏采样的观测中重建数据流，以实现在电力受限的设备上进行长期交通数据收集的最小性能损失和显著延长系统寿命。

    

    收集交通数据对于交通系统和城市规划至关重要，并且通常更希望通过易于部署但电力受限的设备进行，这是由于电力和网络基础设施的不可用性或高成本所致。有限的电力意味着数据收集持续时间和准确性/分辨率之间不可避免的权衡。我们介绍了一种新颖的基于学习的框架，通过策略性地决定电池供电设备的观测时间，并从稀疏采样的观测中重建完整的数据流，从而实现最小的性能损失和显著延长系统寿命。我们的框架由预测器、控制器和估计器组成。预测器利用历史数据来预测固定时间范围内的未来趋势。控制器使用这些预测来确定下一个最优的数据收集时间。最后，估计器从采样观测中重建完整的数据配置文件。我们评估了该系统的性能。

    Collecting traffic data is crucial for transportation systems and urban planning, and is often more desirable through easy-to-deploy but power-constrained devices, due to the unavailability or high cost of power and network infrastructure. The limited power means an inevitable trade-off between data collection duration and accuracy/resolution. We introduce a novel learning-based framework that strategically decides observation timings for battery-powered devices and reconstructs the full data stream from sparsely sampled observations, resulting in minimal performance loss and a significantly prolonged system lifetime. Our framework comprises a predictor, a controller, and an estimator. The predictor utilizes historical data to forecast future trends within a fixed time horizon. The controller uses the forecasts to determine the next optimal timing for data collection. Finally, the estimator reconstructs the complete data profile from the sampled observations. We evaluate the performanc
    
[^61]: MResT: 多分辨率感知与视觉语言模型的实时控制

    MResT: Multi-Resolution Sensing for Real-Time Control with Vision-Language Models. (arXiv:2401.14502v1 [cs.RO])

    [http://arxiv.org/abs/2401.14502](http://arxiv.org/abs/2401.14502)

    MResT是一个基于多分辨率感知和视觉语言模型的实时控制框架，能够同时实现粗粒度和精确动作，并通过合理地利用不同的感知模式来提高机器人操作任务的性能。

    

    利用不同的空间和时间分辨率感知模式可以提高机器人操作任务的性能。多空间分辨率感知提供了在不同空间尺度上捕获的层次信息，并实现粗粒度和精确动作。同时，多时间分辨率感知使得机器人能够表现出高反应性和实时控制。在这项工作中，我们提出了一个名为MResT（多分辨率Transformer）的框架，用于学习通用的语言条件的多任务策略，利用不同空间和时间分辨率的感知，使用不同容量的网络来有效地执行精确和反应性任务的实时控制。我们利用现成的预训练视觉语言模型来处理低频全局特征，同时使用小型的非预训练模型来适应高频本地反馈。通过在粗粒度、精确和动态操作任务的三个领域进行广泛实验，我们证明了MResT的有效性。

    Leveraging sensing modalities across diverse spatial and temporal resolutions can improve performance of robotic manipulation tasks. Multi-spatial resolution sensing provides hierarchical information captured at different spatial scales and enables both coarse and precise motions. Simultaneously multi-temporal resolution sensing enables the agent to exhibit high reactivity and real-time control. In this work, we propose a framework, MResT (Multi-Resolution Transformer), for learning generalizable language-conditioned multi-task policies that utilize sensing at different spatial and temporal resolutions using networks of varying capacities to effectively perform real time control of precise and reactive tasks. We leverage off-the-shelf pretrained vision-language models to operate on low-frequency global features along with small non-pretrained models to adapt to high frequency local feedback. Through extensive experiments in 3 domains (coarse, precise and dynamic manipulation tasks), we
    
[^62]: 优化港口运营的预测分析

    Predictive Analysis for Optimizing Port Operations. (arXiv:2401.14498v1 [cs.LG])

    [http://arxiv.org/abs/2401.14498](http://arxiv.org/abs/2401.14498)

    本研究开发了一种具有竞争预测和分类能力的港口运营解决方案，用于准确估计船舶在港口的总时间和延迟时间，填补了港口分析模型在这方面的空白，并为海事物流领域提供了有价值的贡献。

    

    海运是远距离和大宗货物运输的重要物流方式。然而，这种运输模式中复杂的规划经常受到不确定性的影响，包括天气条件、货物多样性和港口动态，导致成本增加。因此，准确估计船舶在港口停留的总时间和潜在延迟变得至关重要，以便在港口运营中进行有效的规划和安排。本研究旨在开发具有竞争预测和分类能力的港口运营解决方案，用于估计船舶的总时间和延迟时间。该研究填补了港口分析模型在船舶停留和延迟时间方面的重要空白，为海事物流领域提供了有价值的贡献。所提出的解决方案旨在协助港口环境下的决策制定，并预测服务延迟。通过对巴西港口的案例研究进行验证，同时使用特征分析来理解...

    Maritime transport is a pivotal logistics mode for the long-distance and bulk transportation of goods. However, the intricate planning involved in this mode is often hindered by uncertainties, including weather conditions, cargo diversity, and port dynamics, leading to increased costs. Consequently, accurately estimating vessel total (stay) time at port and potential delays becomes imperative for effective planning and scheduling in port operations. This study aims to develop a port operation solution with competitive prediction and classification capabilities for estimating vessel Total and Delay times. This research addresses a significant gap in port analysis models for vessel Stay and Delay times, offering a valuable contribution to the field of maritime logistics. The proposed solution is designed to assist decision-making in port environments and predict service delays. This is demonstrated through a case study on Brazil ports. Additionally, feature analysis is used to understand
    
[^63]: 研究DermaMNIST和Fitzpatrick17k皮肤科图像数据集的质量

    Investigating the Quality of DermaMNIST and Fitzpatrick17k Dermatological Image Datasets. (arXiv:2401.14497v1 [cs.CV])

    [http://arxiv.org/abs/2401.14497](http://arxiv.org/abs/2401.14497)

    本文研究了DermaMNIST和Fitzpatrick17k皮肤科图像数据集的质量问题，对数据重复、数据泄漏、错误标记和缺乏测试分区等方面进行了详细分析，并提出纠正措施。

    

    深度学习在皮肤科任务中取得的显著进展使我们更接近于达到与人类专家相当的诊断准确性。然而，尽管大型数据集在可靠的深度神经网络模型的开发中起着关键作用，但数据集中的数据质量和其正确使用至关重要。多种因素可以影响数据质量，如重复数据的存在，训练-测试分区的数据泄漏，错误标记的图像以及缺乏明确定义的测试分区。在本文中，我们对两个流行的皮肤科图像数据集DermaMNIST和Fitzpatrick17k进行了详细分析，揭示了这些数据质量问题，测量了这些问题对基准结果的影响，并对数据集提出了纠正措施。通过公开我们的分析流程和配套代码，确保我们分析的可重复性，我们旨在鼓励类似的探索并促进这方面的研究发展。

    The remarkable progress of deep learning in dermatological tasks has brought us closer to achieving diagnostic accuracies comparable to those of human experts. However, while large datasets play a crucial role in the development of reliable deep neural network models, the quality of data therein and their correct usage are of paramount importance. Several factors can impact data quality, such as the presence of duplicates, data leakage across train-test partitions, mislabeled images, and the absence of a well-defined test partition. In this paper, we conduct meticulous analyses of two popular dermatological image datasets: DermaMNIST and Fitzpatrick17k, uncovering these data quality issues, measure the effects of these problems on the benchmark results, and propose corrections to the datasets. Besides ensuring the reproducibility of our analysis, by making our analysis pipeline and the accompanying code publicly available, we aim to encourage similar explorations and to facilitate the 
    
[^64]: K-QA：一个真实世界的医疗问答基准

    K-QA: A Real-World Medical Q&A Benchmark. (arXiv:2401.14493v1 [cs.CL])

    [http://arxiv.org/abs/2401.14493](http://arxiv.org/abs/2401.14493)

    本研究构建了K-QA数据集，包含1212个真实世界医疗对话中的患者问题，并聘请内部医生回答和分解。研究还制定了两个基于NLI的评估指标，用于评估模型的召回率和精确度。研究结果对于提升大型语言模型在临床环境下的准确性具有重要意义。

    

    确保大型语言模型（LLMs）提供的回答准确性是至关重要的，特别是在临床环境中，错误的信息可能直接影响患者健康。为了解决这个挑战，我们构建了K-QA数据集，其中包含1212个由K Health（一家AI驱动的临床平台）上的真实对话中的患者问题。我们聘请一组内部医生来回答并手动分解K-QA的子集为自包含的陈述。此外，我们制定了两个基于NLI的评估指标，近似于召回率和精确度：（1）全面性，衡量生成回答中所含的基本临床信息的百分比，（2）虚构率，衡量LLM回答所矛盾的医生策划回复中的陈述数量。最后，我们使用K-QA和这些指标来评估几种最先进的模型，以及上下文学习和医学导向增强检索的影响。

    Ensuring the accuracy of responses provided by large language models (LLMs) is crucial, particularly in clinical settings where incorrect information may directly impact patient health. To address this challenge, we construct K-QA, a dataset containing 1,212 patient questions originating from real-world conversations held on K Health (an AI-driven clinical platform). We employ a panel of in-house physicians to answer and manually decompose a subset of K-QA into self-contained statements. Additionally, we formulate two NLI-based evaluation metrics approximating recall and precision: (1) comprehensiveness, measuring the percentage of essential clinical information in the generated answer and (2) hallucination rate, measuring the number of statements from the physician-curated response contradicted by the LLM answer. Finally, we use K-QA along with these metrics to evaluate several state-of-the-art models, as well as the effect of in-context learning and medically-oriented augmented retri
    
[^65]: Scilab-RL：用于高效增强学习和认知建模研究的软件框架

    Scilab-RL: A software framework for efficient reinforcement learning and cognitive modeling research. (arXiv:2401.14488v1 [cs.LG])

    [http://arxiv.org/abs/2401.14488](http://arxiv.org/abs/2401.14488)

    Scilab-RL是一种用于机器人代理的认知建模和增强学习研究的软件框架，通过提供稳定的基线3和OpenAI gym接口，以及实验可视化和超参数优化的功能，最大程度地提高了研究产出。

    

    研究认知建模和增强学习的一个问题是，研究人员花费太多时间来设置适当的计算框架进行实验。存在许多当前增强学习算法的开源实现，但缺乏一个模块化的工具套件，结合不同的机器人模拟器和平台、数据可视化、超参数优化和基准实验。为解决这个问题，我们提出了Scilab-RL，这是一个用于机器人代理的认知建模和增强学习研究的软件框架。该框架专注于使用稳定的基线3和OpenAI gym接口进行目标条件增强学习。它提供了原生的实验可视化和超参数优化的可能性。我们描述了这些功能如何使研究人员只需最少的时间和精力就能进行实验，从而最大程度地提高研究产出。

    One problem with researching cognitive modeling and reinforcement learning (RL) is that researchers spend too much time on setting up an appropriate computational framework for their experiments. Many open source implementations of current RL algorithms exist, but there is a lack of a modular suite of tools combining different robotic simulators and platforms, data visualization, hyperparameter optimization, and baseline experiments. To address this problem, we present Scilab-RL, a software framework for efficient research in cognitive modeling and reinforcement learning for robotic agents. The framework focuses on goal-conditioned reinforcement learning using Stable Baselines 3 and the OpenAI gym interface. It enables native possibilities for experiment visualizations and hyperparameter optimization. We describe how these features enable researchers to conduct experiments with minimal time effort, thus maximizing research output.
    
[^66]: CloudTracks: 用于定位云层中船舶路径的卫星图像数据集

    CloudTracks: A Dataset for Localizing Ship Tracks in Satellite Images of Clouds. (arXiv:2401.14486v1 [cs.CV])

    [http://arxiv.org/abs/2401.14486](http://arxiv.org/abs/2401.14486)

    这个论文介绍了一个名为CloudTracks的数据集，该数据集包含3560张带有超过12000个船舶路径实例注释的卫星图像。作者使用该数据集训练模型，并发现他们的最佳模型明显优于之前的模型。

    

    通过对行星反照率的影响，云层在全球温度调节中起到重要作用。人为气溶胶排放可以改变云层的反照率，但这种影响的程度以及对温度变化的影响仍不确定。由船舶气溶胶排放引起的人为云层，通常称为船舶路径，提供了与相邻云层区域有所不同的可见效果，因此可用作研究人为云层的有用样本。然而，缺乏大规模船舶路径数据使得很难推测其对云层形成的总体影响。为了开发自动定位船舶路径的大规模方法，我们提出了CloudTracks，一个包含3560个卫星图像并标记了12000多个船舶路径实例注释的数据集。我们在我们的数据集上训练了语义分割和实例分割模型基线，并发现我们的最佳模型明显优于先前的统计模型。

    Clouds play a significant role in global temperature regulation through their effect on planetary albedo. Anthropogenic emissions of aerosols can alter the albedo of clouds, but the extent of this effect, and its consequent impact on temperature change, remains uncertain. Human-induced clouds caused by ship aerosol emissions, commonly referred to as ship tracks, provide visible manifestations of this effect distinct from adjacent cloud regions and therefore serve as a useful sandbox to study human-induced clouds. However, the lack of large-scale ship track data makes it difficult to deduce their general effects on cloud formation. Towards developing automated approaches to localize ship tracks at scale, we present CloudTracks, a dataset containing 3,560 satellite images labeled with more than 12,000 ship track instance annotations. We train semantic segmentation and instance segmentation model baselines on our dataset and find that our best model substantially outperforms previous stat
    
[^67]: 预测的四个方面：校准、预测性、随机性和遗憾

    Four Facets of Forecast Felicity: Calibration, Predictiveness, Randomness and Regret. (arXiv:2401.14483v1 [cs.LG])

    [http://arxiv.org/abs/2401.14483](http://arxiv.org/abs/2401.14483)

    本文展示了校准和遗憾在评估预测中的概念等价性，将评估问题构建为一个预测者、一个赌徒和自然之间的博弈，并将预测的评估与结果的随机性联系起来。

    

    机器学习是关于预测的。然而，预测只有经过评估后才具有其有用性。机器学习传统上关注损失类型及其相应的遗憾。目前，机器学习社区重新对校准产生了兴趣。在这项工作中，我们展示了校准和遗憾在评估预测中的概念等价性。我们将评估问题构建为一个预测者、一个赌徒和自然之间的博弈。通过对赌徒和预测者施加直观的限制，校准和遗憾自然地成为了这个框架的一部分。此外，这个博弈将预测的评估与结果的随机性联系起来。相对于预测而言，结果的随机性等同于关于结果的好的预测。我们称这两个方面为校准和遗憾、预测性和随机性，即预测的四个方面。

    Machine learning is about forecasting. Forecasts, however, obtain their usefulness only through their evaluation. Machine learning has traditionally focused on types of losses and their corresponding regret. Currently, the machine learning community regained interest in calibration. In this work, we show the conceptual equivalence of calibration and regret in evaluating forecasts. We frame the evaluation problem as a game between a forecaster, a gambler and nature. Putting intuitive restrictions on gambler and forecaster, calibration and regret naturally fall out of the framework. In addition, this game links evaluation of forecasts to randomness of outcomes. Random outcomes with respect to forecasts are equivalent to good forecasts with respect to outcomes. We call those dual aspects, calibration and regret, predictiveness and randomness, the four facets of forecast felicity.
    
[^68]: 揭示看不见的：训练的深度可分离卷积核中的可识别聚类

    Unveiling the Unseen: Identifiable Clusters in Trained Depthwise Convolutional Kernels. (arXiv:2401.14469v1 [cs.LG])

    [http://arxiv.org/abs/2401.14469](http://arxiv.org/abs/2401.14469)

    本研究揭示了深度可分离卷积神经网络中训练的卷积核中出现的可辨别和可解释的模式，这些模式类似于高斯差分函数和它们的一阶和二阶导数。研究通过对数百万个训练滤波器进行无监督聚类，成功将最先进的模型中的大部分滤波器进行分类。

    

    最近深度可分离卷积神经网络(DS-CNNs)的进展已经导致了新颖的架构，通过显着的可扩展性和准确性差距超越了经典CNNs的性能。本文揭示了DS-CNN架构的另一个引人注目的特性：在其所有层的训练深度卷积核中出现了可辨别和可解释的模式。通过对数百万个不同大小和来自各种模型的训练滤波器的广泛分析，我们使用自编码器的无监督聚类将这些滤波器分类。令人惊讶的是，这些模式收敛成几个主要的聚类，

    Recent advances in depthwise-separable convolutional neural networks (DS-CNNs) have led to novel architectures, that surpass the performance of classical CNNs, by a considerable scalability and accuracy margin. This paper reveals another striking property of DS-CNN architectures: discernible and explainable patterns emerge in their trained depthwise convolutional kernels in all layers. Through an extensive analysis of millions of trained filters, with different sizes and from various models, we employed unsupervised clustering with autoencoders, to categorize these filters. Astonishingly, the patterns converged into a few main clusters, each resembling the difference of Gaussian (DoG) functions, and their first and second-order derivatives. Notably, we were able to classify over 95\% and 90\% of the filters from state-of-the-art ConvNextV2 and ConvNeXt models, respectively. This finding is not merely a technological curiosity; it echoes the foundational models neuroscientists have long
    
[^69]: Marabou 2.0: 一个多功能的神经网络形式分析器

    Marabou 2.0: A Versatile Formal Analyzer of Neural Networks. (arXiv:2401.14461v1 [cs.AI])

    [http://arxiv.org/abs/2401.14461](http://arxiv.org/abs/2401.14461)

    Marabou 2.0是一个多功能的神经网络形式分析器，具有创新的架构设计和引入的主要功能和组件。

    

    本文是关于Marabou框架2.0版本的综合系统描述，用于神经网络的形式分析。我们讨论了工具的架构设计，并介绍了自初始发布以来引入的主要功能和组件。

    This paper serves as a comprehensive system description of version 2.0 of the Marabou framework for formal analysis of neural networks. We discuss the tool's architectural design and highlight the major features and components introduced since its initial release.
    
[^70]: Wordflow: 大型语言模型的社交提示工程

    Wordflow: Social Prompt Engineering for Large Language Models. (arXiv:2401.14447v1 [cs.HC])

    [http://arxiv.org/abs/2401.14447](http://arxiv.org/abs/2401.14447)

    本文提出了一种名为Wordflow的工具，通过社交提示工程的方式让非专家用户更好地使用大型语言模型（LLMs），并可以轻松创建、运行、共享和发现LLM提示。通过利用现代网络技术，Wordflow允许用户在浏览器中本地和私下运行LLM。

    

    大型语言模型（LLMs）需要精心设计的提示才能有效使用。对于非专家来说，这是一个具有挑战性的过程，因为他们对人工智能技术不那么熟悉。虽然研究人员提出了一些技术和工具来帮助LLM用户设计提示，但这些作品主要针对的是AI应用开发者而不是非专家。为了填补这一研究空白，我们提出了社交提示工程，这是一种利用社交计算技术促进协作提示设计的新范式。为了研究社交提示工程，我们介绍了Wordflow，一个开源的社交文本编辑器，使普通用户可以轻松创建、运行、共享和发现LLM提示。此外，通过利用现代网络技术，Wordflow允许用户在其浏览器中本地和私下运行LLM。两个使用场景突出了社交提示工程和我们的工具如何增强普通人与LLM的交互。

    Large language models (LLMs) require well-crafted prompts for effective use. Prompt engineering, the process of designing prompts, is challenging, particularly for non-experts who are less familiar with AI technologies. While researchers have proposed techniques and tools to assist LLM users in prompt design, these works primarily target AI application developers rather than non-experts. To address this research gap, we propose social prompt engineering, a novel paradigm that leverages social computing techniques to facilitate collaborative prompt design. To investigate social prompt engineering, we introduce Wordflow, an open-source and social text editor that enables everyday users to easily create, run, share, and discover LLM prompts. Additionally, by leveraging modern web technologies, Wordflow allows users to run LLMs locally and privately in their browsers. Two usage scenarios highlight how social prompt engineering and our tool can enhance laypeople's interaction with LLMs. Wor
    
[^71]: 利用专利数据提高抗体人性预测能力

    Improving Antibody Humanness Prediction using Patent Data. (arXiv:2401.14442v1 [q-bio.QM])

    [http://arxiv.org/abs/2401.14442](http://arxiv.org/abs/2401.14442)

    本研究利用专利数据提高了抗体人性预测的能力，通过多阶段、多损失的训练过程以及弱监督对比学习的方法，成功地预测了抗体序列的人性评分。

    

    我们研究了利用专利数据来提高抗体人性预测的潜力，采用了多阶段、多损失的训练过程。抗体人性作为对抗体治疗的免疫反应的代理，是药物发现中的主要原因之一，在临床环境中使用抗体治疗面临着具有挑战性的障碍。我们将初始学习阶段视为一个弱监督对比学习问题，每个抗体序列与可能有多个功能标识符相关联，目标是学习一个编码器，根据其专利属性将它们分组。然后，我们冻结对比编码器的一部分，并继续使用交叉熵损失在专利数据上训练，以预测给定抗体序列的人性评分。我们通过对三个不同的免疫原性数据集进行推理，展示了专利数据和我们的方法的效用。我们的实证结果表明，l

    We investigate the potential of patent data for improving the antibody humanness prediction using a multi-stage, multi-loss training process. Humanness serves as a proxy for the immunogenic response to antibody therapeutics, one of the major causes of attrition in drug discovery and a challenging obstacle for their use in clinical settings. We pose the initial learning stage as a weakly-supervised contrastive-learning problem, where each antibody sequence is associated with possibly multiple identifiers of function and the objective is to learn an encoder that groups them according to their patented properties. We then freeze a part of the contrastive encoder and continue training it on the patent data using the cross-entropy loss to predict the humanness score of a given antibody sequence. We illustrate the utility of the patent data and our approach by performing inference on three different immunogenicity datasets, unseen during training. Our empirical results demonstrate that the l
    
[^72]: 语义敏感性和不一致的预测：衡量NLI模型的脆弱性

    Semantic Sensitivities and Inconsistent Predictions: Measuring the Fragility of NLI Models. (arXiv:2401.14440v1 [cs.CL])

    [http://arxiv.org/abs/2401.14440](http://arxiv.org/abs/2401.14440)

    这份论文研究发现，最先进的NLI模型对微小的语义保持表面形式变化非常敏感，导致推断结果不一致。其行为与对组合语义的有效理解不同，这对当前NLI模型的可靠性提出了挑战。

    

    最近对基于transformer的自然语言理解（NLU）模型的新能力进行的研究表明，它们具备对词汇和组合语义的理解。然而，我们提供了证据表明这些说法应该持保留态度：我们发现目前最先进的自然语言推理（NLI）模型对微小的保留语义的表面形式变化敏感，这导致推断过程中出现大量不一致的模型决策。值得注意的是，这种行为与对组合语义的有效和深入理解不同，而在标准基准测试中评估模型准确度或探究句法、单调性和逻辑鲁棒性推理时均不会出现。我们提出了一个新颖的框架来衡量语义敏感性的程度。为此，我们使用含有微小保留语义的表面形式输入噪声的对抗生成样例来评估NLI模型。

    Recent studies of the emergent capabilities of transformer-based Natural Language Understanding (NLU) models have indicated that they have an understanding of lexical and compositional semantics. We provide evidence that suggests these claims should be taken with a grain of salt: we find that state-of-the-art Natural Language Inference (NLI) models are sensitive towards minor semantics preserving surface-form variations, which lead to sizable inconsistent model decisions during inference. Notably, this behaviour differs from valid and in-depth comprehension of compositional semantics, however does neither emerge when evaluating model accuracy on standard benchmarks nor when probing for syntactic, monotonic, and logically robust reasoning. We propose a novel framework to measure the extent of semantic sensitivity. To this end, we evaluate NLI models on adversarially generated examples containing minor semantics-preserving surface-form input noise. This is achieved using conditional text
    
[^73]: 基于簇合并和分层的增量亲和传播算法

    Incremental Affinity Propagation based on Cluster Consolidation and Stratification. (arXiv:2401.14439v1 [cs.LG])

    [http://arxiv.org/abs/2401.14439](http://arxiv.org/abs/2401.14439)

    本文提出了基于簇合并和分层的增量亲和传播算法(APP)，实现了对动态数据集的增量聚类，同时保持聚类结果的忠实性和遗忘过时簇的功能。

    

    现代数据挖掘应用程序需要通过跟踪结果簇的时间变化来对动态数据集进行增量聚类。本文提出了一种增量亲和传播算法（APP），它是亲和传播（AP）的增量扩展，基于簇合并和簇分层来实现忠实性和遗忘。APP实现了增量聚类，其中新到达的对象会动态合并到先前的簇中，而无需重新对整个对象数据集执行聚类，并且会在时间上生成和维护一系列忠实的聚类结果，同时允许遗忘过时的簇，带有递减学习功能。使用四个常用的带标签数据集来测试APP与传统AP和基于最近邻分配的增量亲和传播算法（IAPNA）的聚类性能。

    Modern data mining applications require to perform incremental clustering over dynamic datasets by tracing temporal changes over the resulting clusters. In this paper, we propose A-Posteriori affinity Propagation (APP), an incremental extension of Affinity Propagation (AP) based on cluster consolidation and cluster stratification to achieve faithfulness and forgetfulness. APP enforces incremental clustering where i) new arriving objects are dynamically consolidated into previous clusters without the need to re-execute clustering over the entire dataset of objects, and ii) a faithful sequence of clustering results is produced and maintained over time, while allowing to forget obsolete clusters with decremental learning functionalities. Four popular labeled datasets are used to test the performance of APP with respect to benchmark clustering performances obtained by conventional AP and Incremental Affinity Propagation based on Nearest neighbor Assignment (IAPNA) algorithms. Experimental 
    
[^74]: 将基于梯度的技术转化为可解释的方法

    Transforming gradient-based techniques into interpretable methods. (arXiv:2401.14434v1 [cs.CV])

    [http://arxiv.org/abs/2401.14434](http://arxiv.org/abs/2401.14434)

    本文提出了一种基于梯度的技术支持框架，通过建立区别来强调重要区域, 并减少图像噪音。实证调查表明这些区域在促进类别区分方面起关键作用。

    

    通过xAI技术解释卷积神经网络（CNN）通常在解释上面临挑战。图像提取的像素等输入特征的内在复杂性引发了复杂的相关性。集成梯度（IG）等基于梯度的方法有效地展示了这些特征的重要性。然而，将这些解释转化为图像时常常产生大量噪音。本文引入了梯度人工分离（GAD）作为梯度基于技术的支持框架。其主要目标是通过建立类别之间的区别来强调有影响力的区域。GAD的核心是在可视化过程中限制分析范围，从而减少图像噪音。通过对被遮挡图像的实证调查，我们证明了通过这种方法确定的区域确实在促进类别区分方面发挥了关键作用。

    The explication of Convolutional Neural Networks (CNN) through xAI techniques often poses challenges in interpretation. The inherent complexity of input features, notably pixels extracted from images, engenders complex correlations. Gradient-based methodologies, exemplified by Integrated Gradients (IG), effectively demonstrate the significance of these features. Nevertheless, the conversion of these explanations into images frequently yields considerable noise. Presently, we introduce GAD (Gradient Artificial Distancing) as a supportive framework for gradient-based techniques. Its primary objective is to accentuate influential regions by establishing distinctions between classes. The essence of GAD is to limit the scope of analysis during visualization and, consequently reduce image noise. Empirical investigations involving occluded images have demonstrated that the identified regions through this methodology indeed play a pivotal role in facilitating class differentiation.
    
[^75]: A2C：一个模块化的多阶段人工智能团队合作决策框架

    A2C: A Modular Multi-stage Collaborative Decision Framework for Human-AI Teams. (arXiv:2401.14432v1 [cs.HC])

    [http://arxiv.org/abs/2401.14432](http://arxiv.org/abs/2401.14432)

    A2C是一个多阶段模块化的人工智能团队合作决策框架，通过综合利用人类和人工智能的优势，显著提高了动态和不断变化环境中复杂决策制定的效率和效果。

    

    本文介绍了A2C，一个多阶段人工智能团队合作决策框架，旨在实现人工智能团队内强大的决策制定能力。A2C从拒绝学习和学习推迟等概念中获得灵感，将训练有素的人工智能系统纳入其决策过程中，使其能够识别自己决策中的不确定性并在需要时推迟到人类专家。此外，A2C适用于即使是人类专家也会遇到限制的场景，例如在网络安全运营中心（SOC）中的事件检测和响应。在这种情况下，A2C促进了协作探索，实现了对复杂挑战的集体解决。A2C支持人工智能团队中三种不同的决策模式：自动化、增强型和协作型，为人工智能与人类的有效协作策略开发提供了灵活的平台。通过充分发挥人类和人工智能的优势，A2C显著提高了动态和不断变化环境中复杂决策制定的效率和效果。

    This paper introduces A2C, a multi-stage collaborative decision framework designed to enable robust decision-making within human-AI teams. Drawing inspiration from concepts such as rejection learning and learning to defer, A2C incorporates AI systems trained to recognise uncertainty in their decisions and defer to human experts when needed. Moreover, A2C caters to scenarios where even human experts encounter limitations, such as in incident detection and response in cyber Security Operations Centres (SOC). In such scenarios, A2C facilitates collaborative explorations, enabling collective resolution of complex challenges. With support for three distinct decision-making modes in human-AI teams: Automated, Augmented, and Collaborative, A2C offers a flexible platform for developing effective strategies for human-AI collaboration. By harnessing the strengths of both humans and AI, it significantly improves the efficiency and effectiveness of complex decision-making in dynamic and evolving e
    
[^76]: [再论] 非线性和非高斯观测模型的贝叶斯滤波的判别式卡尔曼滤波器

    [Re] The Discriminative Kalman Filter for Bayesian Filtering with Nonlinear and Non-Gaussian Observation Models. (arXiv:2401.14429v1 [cs.LG])

    [http://arxiv.org/abs/2401.14429](http://arxiv.org/abs/2401.14429)

    该论文提供了一种针对非线性和非高斯观测模型的贝叶斯滤波的判别式卡尔曼滤波器，并在神经科学背景下证明了其有效性。

    

    卡尔曼滤波器为估计隐藏或潜在变量提供了一种直观且易于理解的方法，并在控制、机器人、信号处理和机器学习等领域应用广泛。其中一种应用是神经脑机接口的神经解码。2020年，Burkhart等人对他们的新版本卡尔曼滤波器进行了深入评估，利用贝叶斯定理改善了对高度非线性或非高斯观测模型的滤波性能。本研究提供了作者MATLAB算法的Python开源替代方案。具体而言，我们重新复现了他们在神经科学背景下最显著的结果，并使用多个随机种子和作者数据集中未使用的试验进一步检验了滤波器的效果。所有实验在一台计算机上离线进行。

    Kalman filters provide a straightforward and interpretable means to estimate hidden or latent variables, and have found numerous applications in control, robotics, signal processing, and machine learning. One such application is neural decoding for neuroprostheses. In 2020, Burkhart et al. thoroughly evaluated their new version of the Kalman filter that leverages Bayes' theorem to improve filter performance for highly non-linear or non-Gaussian observation models. This work provides an open-source Python alternative to the authors' MATLAB algorithm. Specifically, we reproduce their most salient results for neuroscientific contexts and further examine the efficacy of their filter using multiple random seeds and previously unused trials from the authors' dataset. All experiments were performed offline on a single computer.
    
[^77]: Beimingwu: 一个学习软件集线器系统

    Beimingwu: A Learnware Dock System. (arXiv:2401.14427v1 [cs.SE])

    [http://arxiv.org/abs/2401.14427](http://arxiv.org/abs/2401.14427)

    Beimingwu is a learnware paradigm that enables users to reuse well-trained models, submitted by developers worldwide to a dock system, for solving new user tasks. The dock system assigns a specification to accommodate the model, allowing future users to identify and assemble the model for reuse, even without prior knowledge. This paradigm offers excellent capabilities for both planned and specialized tasks.

    

    Beimingwu是一个学习软件范式，旨在使用户能够重复使用许多现有的经过良好训练的模型，而不是从头开始构建机器学习模型，期望能解决超出模型原始目的的新用户任务。在这个范式中，全球开发人员可以随时将他们高性能的模型匿名提交到学习软件集线器系统（之前称为学习软件市场），而不用透露他们的训练数据。一旦集线器系统接受了该模型，它会分配一个规范，并容纳该模型。这个规范允许根据未来用户的需求充分识别和组装模型的重复使用，即使他们没有对该模型的先验知识。这个范式与当前的大模型方向有很大区别，并且有望学习软件集线器系统可以容纳数百万甚至更多的高性能模型，为计划任务和非计划、专业任务提供出色的能力。

    The learnware paradigm proposed by Zhou [2016] aims to enable users to reuse numerous existing well-trained models instead of building machine learning models from scratch, with the hope of solving new user tasks even beyond models' original purposes. In this paradigm, developers worldwide can submit their high-performing models spontaneously to the learnware dock system (formerly known as learnware market) without revealing their training data. Once the dock system accepts the model, it assigns a specification and accommodates the model. This specification allows the model to be adequately identified and assembled to reuse according to future users' needs, even if they have no prior knowledge of the model. This paradigm greatly differs from the current big model direction and it is expected that a learnware dock system housing millions or more high-performing models could offer excellent capabilities for both planned tasks where big models are applicable; and unplanned, specialized, d
    
[^78]: M$^3$TN：基于多门专家混合的多值处理网络的提升建模

    M$^3$TN: Multi-gate Mixture-of-Experts based Multi-valued Treatment Network for Uplift Modeling. (arXiv:2401.14426v1 [cs.LG])

    [http://arxiv.org/abs/2401.14426](http://arxiv.org/abs/2401.14426)

    M$^3$TN是一种用于提升建模的新颖网络，通过多门专家混合和明确建模提升的方法，解决了现有方法中存在的一致性和效率问题。

    

    提升建模是一种用于预测处理（如折扣）对个体反应的技术。虽然已经提出了几种用于多值处理的方法，但它们都是从二值处理方法扩展而来的，存在一些局限性。首先，现有方法基于预测的响应计算提升，这可能不能保证处理组和对照组之间的一致提升分布。此外，这可能会对多值处理产生累积误差。其次，随着许多预测头，模型参数变得非常复杂，导致效率降低。为了解决这些问题，我们提出了一种新颖的基于多门专家混合的多值处理网络（M$^3$TN）。M$^3$TN由两个组件组成：1) 基于多门专家混合的特征表示模块，以提高效率；2) 通过明确建模提升的重新参数化模块。

    Uplift modeling is a technique used to predict the effect of a treatment (e.g., discounts) on an individual's response. Although several methods have been proposed for multi-valued treatment, they are extended from binary treatment methods. There are still some limitations. Firstly, existing methods calculate uplift based on predicted responses, which may not guarantee a consistent uplift distribution between treatment and control groups. Moreover, this may cause cumulative errors for multi-valued treatment. Secondly, the model parameters become numerous with many prediction heads, leading to reduced efficiency. To address these issues, we propose a novel \underline{M}ulti-gate \underline{M}ixture-of-Experts based \underline{M}ulti-valued \underline{T}reatment \underline{N}etwork (M$^3$TN). M$^3$TN consists of two components: 1) a feature representation module with Multi-gate Mixture-of-Experts to improve the efficiency; 2) a reparameterization module by modeling uplift explicitly to i
    
[^79]: 通过GPT引导的蒙特卡洛树搜索从数据中发现数学公式

    Discovering Mathematical Formulas from Data via GPT-guided Monte Carlo Tree Search. (arXiv:2401.14424v1 [cs.LG])

    [http://arxiv.org/abs/2401.14424](http://arxiv.org/abs/2401.14424)

    通过结合MCTS和生成式预训练模型，我们提出了一种新的符号回归算法SR-GPT，在发现数据中的数学公式方面取得了显著的改进。

    

    在科学研究和人工智能中，找到一个简洁且可解释的数学公式来准确描述数据中每个变量与预测值之间的关系是一个关键任务，也是一个重大挑战。这个问题被称为符号回归，是一个NP困难问题。去年，提出了一种基于蒙特卡洛树搜索（MCTS）的符号回归方法，并在多个数据集上获得了sota。虽然与以前的方法相比，该算法在恢复目标表达式方面显示出了相当大的改进，但是在MCTS过程中缺乏引导严重阻碍了其搜索效率。最近，一些算法在MCTS的搜索中添加了一个预训练的策略网络，但是这个预训练的策略网络的泛化能力很差。为了平衡效率和通用性，我们提出了SR-GPT，结合了AlphaZero的思想。SR-GPT是一种新的符号回归算法，将MCTS与一个通用性较好的生成式预训练模型相结合。

    Finding a concise and interpretable mathematical formula that accurately describes the relationship between each variable and the predicted value in the data is a crucial task in scientific research, as well as a significant challenge in artificial intelligence. This problem is referred to as symbolic regression, which is an NP-hard problem. Last year, a symbolic regression method based on Monte Carlo Tree Search (MCTS) was proposed and sota was obtained on multiple datasets. While this algorithm has shown considerable improvement in recovering target expressions compared to previous methods, the lack of guidance during the MCTS process severely hampers its search efficiency. Recently, some algorithms have added a pre-trained policy network to guide the search of MCTS, but the pre-trained policy network generalizes poorly. To balance efficiency and generality, we propose SR-GPT combining ideas from AlphaZero. SR-GPT is a new symbolic regression algorithm that combines MCTS with a Gener
    
[^80]: 提示设计与工程：介绍与高级方法

    Prompt Design and Engineering: Introduction and Advanced Methods. (arXiv:2401.14423v1 [cs.SE])

    [http://arxiv.org/abs/2401.14423](http://arxiv.org/abs/2401.14423)

    本文介绍了提示设计与工程的主要概念，并回顾了基本和更高级的方法。

    

    提示设计与工程在过去几个月中成为了一个重要的学科。在本文中，我们介绍了主要概念，并回顾了提示设计与工程的基本和更高级的方法。

    Prompt design and engineering has become an important discipline in just the past few months. In this paper, we provide an introduction to the main concepts as well as review basic and more advanced approaches to prompt design and engineering.
    
[^81]: 定位无关源免域自适应学习预测太阳能发电

    Location Agnostic Source-Free Domain Adaptive Learning to Predict Solar Power Generation. (arXiv:2401.14422v1 [cs.LG])

    [http://arxiv.org/abs/2401.14422](http://arxiv.org/abs/2401.14422)

    提出了一种基于深度学习的域自适应框架，利用气候特征预测太阳能发电量，在不同地区间具有一定的通用性和适应性。

    

    太阳能发电的预测是一项具有挑战性的任务，因为它依赖于呈现空间和时间变异性的气候特征。预测模型的性能可能因数据分布的变化而在不同地区产生变化，导致一个在某个地区工作良好但在其他地区不起作用的模型。此外，由于全球变暖，每年天气模式的变化在加速。这种现象可能导致现有模型在同一地理区域内随时间推移而效果减弱。本文提出了一种基于深度学习的域自适应框架，利用可以解决上述挑战的气候特征来估计太阳能发电量。采用前馈深度卷积网络模型来在已知位置数据集上进行有监督训练，并用于后续预测未知位置的太阳能发电量。

    The prediction of solar power generation is a challenging task due to its dependence on climatic characteristics that exhibit spatial and temporal variability. The performance of a prediction model may vary across different places due to changes in data distribution, resulting in a model that works well in one region but not in others. Furthermore, as a consequence of global warming, there is a notable acceleration in the alteration of weather patterns on an annual basis. This phenomenon introduces the potential for diminished efficacy of existing models, even within the same geographical region, as time progresses. In this paper, a domain adaptive deep learning-based framework is proposed to estimate solar power generation using weather features that can solve the aforementioned challenges. A feed-forward deep convolutional network model is trained for a known location dataset in a supervised manner and utilized to predict the solar power of an unknown location later. This adaptive da
    
[^82]: 多智能体基于迁移学习的数据驱动空中交通应用

    Multi-Agent Based Transfer Learning for Data-Driven Air Traffic Applications. (arXiv:2401.14421v1 [cs.LG])

    [http://arxiv.org/abs/2401.14421](http://arxiv.org/abs/2401.14421)

    本文提出了一种基于多智能体的迁移学习方法，利用MA-BERT模型和预训练微调框架来解决空中交通管理中的长训练时间和大数据集需求的问题。该方法可以在具有少量数据或无历史数据的情况下实现高性能。

    

    近年来，开发空中交通管理(ATM)的数据驱动模型的研究引起了巨大的兴趣。然而，众所周知，数据驱动模型具有较长的训练时间，并且需要大量的数据集才能达到良好的性能。为了解决这两个问题，本文提出了一种全面考虑ATM系统多智能体特性的Multi-Agent Bidirectional Encoder Representations from Transformers (MA-BERT)模型，并采用预训练和微调迁移学习框架。通过将MA-BERT在一个主要机场的大规模数据集上进行预训练，并在其他机场和特定空中交通应用上进行微调，可以节省大量的总训练时间。此外，对于新采用的程序和建立的机场，没有历史数据可用，本文展示了预训练的MA-BERT可以通过少量数据的定期更新实现高性能。

    Research in developing data-driven models for Air Traffic Management (ATM) has gained a tremendous interest in recent years. However, data-driven models are known to have long training time and require large datasets to achieve good performance. To address the two issues, this paper proposes a Multi-Agent Bidirectional Encoder Representations from Transformers (MA-BERT) model that fully considers the multi-agent characteristic of the ATM system and learns air traffic controllers' decisions, and a pre-training and fine-tuning transfer learning framework. By pre-training the MA-BERT on a large dataset from a major airport and then fine-tuning it to other airports and specific air traffic applications, a large amount of the total training time can be saved. In addition, for newly adopted procedures and constructed airports where no historical data is available, this paper shows that the pre-trained MA-BERT can achieve high performance by updating regularly with little data. The proposed t
    
[^83]: 模糊逻辑函数作为非线性分类器的事后解释器

    Fuzzy Logic Function as a Post-hoc Explanator of the Nonlinear Classifier. (arXiv:2401.14417v1 [cs.LG])

    [http://arxiv.org/abs/2401.14417](http://arxiv.org/abs/2401.14417)

    本文研究了使用模糊逻辑函数作为分类器的事后解释器，通过与黑盒分类器进行并行设计，实现了与黑盒分类器相同的决策结果。

    

    使用深度神经网络实现的模式识别系统比线性模型获得更好的结果。然而，它们的缺点是黑盒属性。这意味着没有经验使用非线性系统的人可能需要帮助理解决策结果。这种解决方案对于负责最终决策的用户来说是不可接受的。他不仅必须相信决策，还必须理解决策。因此，识别器必须具有允许解释者解释结果的架构。事后可解释的分类器的想法是设计一个可解释的分类器，与黑盒分类器并行，给出与黑盒分类器相同的决策。本文表明，如果Zadeh的模糊逻辑函数形成分类器，DeconvNet的重要性给出了真值，可解释的分类器在MNIST和FashionMNIST数据库上与黑盒分类器产生相同的分类决策。

    Pattern recognition systems implemented using deep neural networks achieve better results than linear models. However, their drawback is the black box property. This property means that one with no experience utilising nonlinear systems may need help understanding the outcome of the decision. Such a solution is unacceptable to the user responsible for the final decision. He must not only believe in the decision but also understand it. Therefore, recognisers must have an architecture that allows interpreters to interpret the findings. The idea of post-hoc explainable classifiers is to design an interpretable classifier parallel to the black box classifier, giving the same decisions as the black box classifier. This paper shows that the explainable classifier completes matching classification decisions with the black box classifier on the MNIST and FashionMNIST databases if Zadeh`s fuzzy logic function forms the classifier and DeconvNet importance gives the truth values. Since the other 
    
[^84]: 语音节奏的声学特征化：超越指标的递归神经网络研究

    Acoustic characterization of speech rhythm: going beyond metrics with recurrent neural networks. (arXiv:2401.14416v1 [eess.AS])

    [http://arxiv.org/abs/2401.14416](http://arxiv.org/abs/2401.14416)

    本研究通过训练递归神经网络进行语言识别任务，探索了深度学习在声学基础上进一步研究语音节奏的能力。

    

    长期以来，语言一直根据其感知的节奏属性进行描述。相关的分类对于心理语言学很有意义，因为它们在一定程度上预测了新生儿辨别语言的能力，并且揭示了成年听者如何处理非母语。尽管节奏度量在支持语言节奏类别存在方面取得了相对成功，但定量研究仍然没有捕捉到与语音节奏相关的时间规律的全部复杂性。我们认为，深度学习提供了一种强大的模式识别方法，可以推动对语音节奏声学基础的更深入的研究。为了探索这一假设，我们在21种语言的大型语音数据库上对中等大小的递归神经网络进行了语言识别任务的训练。该网络能够访问幅度包络和一个标识有声段的变量，假设这个信号无法很好地传达语音信息。

    Languages have long been described according to their perceived rhythmic attributes. The associated typologies are of interest in psycholinguistics as they partly predict newborns' abilities to discriminate between languages and provide insights into how adult listeners process non-native languages. Despite the relative success of rhythm metrics in supporting the existence of linguistic rhythmic classes, quantitative studies have yet to capture the full complexity of temporal regularities associated with speech rhythm. We argue that deep learning offers a powerful pattern-recognition approach to advance the characterization of the acoustic bases of speech rhythm. To explore this hypothesis, we trained a medium-sized recurrent neural network on a language identification task over a large database of speech recordings in 21 languages. The network had access to the amplitude envelopes and a variable identifying the voiced segments, assuming that this signal would poorly convey phonetic in
    
[^85]: 应用于电化学的机器学习技术的分析

    Aprendizado de m\'aquina aplicado na eletroqu\'imica. (arXiv:2401.14413v1 [cs.LG])

    [http://arxiv.org/abs/2401.14413](http://arxiv.org/abs/2401.14413)

    本文系统综述了机器学习技术在电化学应用中的使用情况，并介绍了其在医学诊断、化学品分类和环境监测等方面的重要作用。

    

    本系统性综述旨在分析机器学习技术在识别和量化各种电化学应用中的应用情况，并介绍了文献中可用的应用。机器学习是一种能够促进分析并增强涉及各种分析物的过程理解的工具。在电化学生物传感器中，它提高了医学诊断的精确性，提高了识别具有高可靠性的生物标志物和病原体的能力。它还可以有效用于复杂化学品的分类；在环境监测中，使用低成本传感器；在便携设备和可穿戴系统中等等。目前，某些分析物的分析仍然需要专家的专业知识并手动执行，从而阻碍了结果的普遍化。在当前人工智能的进展下，本研究拟进行一项系统综述。

    This systematic review focuses on analyzing the use of machine learning techniques for identifying and quantifying analytes in various electrochemical applications, presenting the available applications in the literature. Machine learning is a tool that can facilitate the analysis and enhance the understanding of processes involving various analytes. In electrochemical biosensors, it increases the precision of medical diagnostics, improving the identification of biomarkers and pathogens with high reliability. It can be effectively used for the classification of complex chemical products; in environmental monitoring, using low-cost sensors; in portable devices and wearable systems; among others. Currently, the analysis of some analytes is still performed manually, requiring the expertise of a specialist in the field and thus hindering the generalization of results. In light of the advancements in artificial intelligence today, this work proposes to carry out a systematic review of the l
    
[^86]: 利用神经元的稳定性改进DNN验证

    Harnessing Neuron Stability to Improve DNN Verification. (arXiv:2401.14412v1 [cs.LG])

    [http://arxiv.org/abs/2401.14412](http://arxiv.org/abs/2401.14412)

    本论文提出了VeriStable方法，在DNN验证中利用稳定的神经元减少组合复杂性，同时保持抽象的准确性。这种方法与工业化SAT基准共享重要特征，并在有效性和可扩展性方面取得了显著的进展。

    

    深度神经网络（DNN）已经成为解决现实世界问题的有效方法。然而，像人类编写的软件一样，DNN也容易受到错误和攻击的影响。这引发了对有效和可扩展的DNN验证技术和工具的重大关注。本文介绍了VeriStable，这是一种新颖的基于DPLL约束DNN验证方法的扩展。VeriStable利用了这样一个洞见：尽管神经元在整个DNN输入空间中的行为可能是非线性的，在验证过程中计算得到的中间状态中，许多神经元可能被约束为具有线性行为-这些神经元是稳定的。高效地检测稳定的神经元可以减少组合复杂性，而不会损害抽象的准确性。此外，DNN验证问题中产生的子句结构与工业化SAT基准具有重要特征。我们调整并融合了多线程和重启优化策略。

    Deep Neural Networks (DNN) have emerged as an effective approach to tackling real-world problems. However, like human-written software, DNNs are susceptible to bugs and attacks. This has generated significant interests in developing effective and scalable DNN verification techniques and tools. In this paper, we present VeriStable, a novel extension of recently proposed DPLL-based constraint DNN verification approach. VeriStable leverages the insight that while neuron behavior may be non-linear across the entire DNN input space, at intermediate states computed during verification many neurons may be constrained to have linear behavior - these neurons are stable. Efficiently detecting stable neurons reduces combinatorial complexity without compromising the precision of abstractions. Moreover, the structure of clauses arising in DNN verification problems shares important characteristics with industrial SAT benchmarks. We adapt and incorporate multi-threading and restart optimizations targ
    
[^87]: 利用神经网络进行大气密度自适应的火星精准进入导航

    Precision Mars Entry Navigation with Atmospheric Density Adaptation via Neural Networks. (arXiv:2401.14411v1 [cs.LG])

    [http://arxiv.org/abs/2401.14411](http://arxiv.org/abs/2401.14411)

    这项工作介绍了一种利用神经网络进行火星进入导航的新方法，使用神经网络估计大气密度，并根据估计的不确定性进行实时参数调整，以提高导航滤波器的性能。

    

    真实的火星大气密度与机载密度模型之间的差异会严重影响航天器进入导航滤波器的性能。本文介绍了一种新的火星进入在线滤波方法，使用神经网络估计大气密度，并利用一种考虑分析来考虑估计的不确定性。网络以指数大气密度模型进行训练，并实时动态调整它的参数，以适应真实密度与估计密度之间的任何不匹配。网络的调整被形式化为最大似然问题，利用滤波器的测量创新来识别最佳网络参数。神经网络的应用使得可以在最大似然方法的背景下使用在机器学习领域高效的随机优化器。与之前的方法进行了性能比较。

    Discrepancies between the true Martian atmospheric density and the onboard density model can significantly impair the performance of spacecraft entry navigation filters. This work introduces a new approach to online filtering for Martian entry by using a neural network to estimate atmospheric density and employing a consider analysis to account for the uncertainty in the estimate. The network is trained on an exponential atmospheric density model, and its parameters are dynamically adapted in real time to account for any mismatches between the true and estimated densities. The adaptation of the network is formulated as a maximum likelihood problem, leveraging the measurement innovations of the filter to identify optimal network parameters. The incorporation of a neural network enables the use of stochastic optimizers known for their efficiency in the machine learning domain within the context of the maximum likelihood approach. Performance comparisons against previous approaches are co
    
[^88]: DeepSeek-Coder: 在大型语言模型与编程相遇的时候--代码智能的崛起

    DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence. (arXiv:2401.14196v1 [cs.SE])

    [http://arxiv.org/abs/2401.14196](http://arxiv.org/abs/2401.14196)

    DeepSeek-Coder是一系列开源代码模型，通过在高质量项目级代码语料库上进行预训练和采用填空任务和16K窗口来增强代码生成和填充，不仅在多个基准测试中取得了与开源代码模型同样的最新表现，而且超过了现有的闭源模型。

    

    大型语言模型的快速发展为软件开发中的代码智能带来了革命。然而，闭源模型的主导地位限制了广泛的研究和开发。为了解决这个问题，我们介绍了DeepSeek-Coder系列，这是一系列开源代码模型，大小从1.3B到33B，从头开始在2万亿个标记上进行训练。这些模型在高质量项目级代码语料库上进行了预训练，并采用填空任务和16K窗口来增强代码生成和填充。我们广泛的评估表明，DeepSeek-Coder不仅在多个基准测试中取得了与开源代码模型同样的最新表现，而且超过了现有的Codex和GPT-3.5等闭源模型。此外，DeepSeek-Coder模型采用了宽松的许可证，既允许研究，也允许无限制的商业使用。

    The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.
    
[^89]: 无监督领域自适应回归中的不确定性引导对齐

    Uncertainty-Guided Alignment for Unsupervised Domain Adaptation in Regression. (arXiv:2401.13721v1 [cs.CV])

    [http://arxiv.org/abs/2401.13721](http://arxiv.org/abs/2401.13721)

    该论文提出了一种利用不确定性引导的无监督领域自适应回归方法，通过将不确定性作为置信度估计和嵌入空间的正则项来实现对齐。

    

    无监督领域自适应回归（UDAR）旨在将来自有标签源领域的模型调整到无标签目标领域，以完成回归任务。最近在UDAR领域取得的成功主要集中在子空间对齐上，涉及整个特征空间中所选择子空间的对齐。这与用于分类的特征对齐方法形成对比，后者旨在对齐整个特征空间，在分类任务中已被证明是有效的，但在回归任务中效果较差。具体而言，分类任务旨在在整个嵌入空间的维度上识别独立的簇，而回归任务对数据表示的结构性要求较低，需要额外的指导以实现有效的对齐。在本文中，我们提出了一种通过不确定性引导的有效UDAR方法。我们的方法具有双重作用：提供了对预测结果的置信度衡量，并作为嵌入空间的正则化。具体而言，我们利用深度证据模型来提供对预测的置信度估计，并将其作为嵌入空间的正则项进行优化。

    Unsupervised Domain Adaptation for Regression (UDAR) aims to adapt a model from a labeled source domain to an unlabeled target domain for regression tasks. Recent successful works in UDAR mostly focus on subspace alignment, involving the alignment of a selected subspace within the entire feature space. This contrasts with the feature alignment methods used for classification, which aim at aligning the entire feature space and have proven effective but are less so in regression settings. Specifically, while classification aims to identify separate clusters across the entire embedding dimension, regression induces less structure in the data representation, necessitating additional guidance for efficient alignment. In this paper, we propose an effective method for UDAR by incorporating guidance from uncertainty. Our approach serves a dual purpose: providing a measure of confidence in predictions and acting as a regularization of the embedding space. Specifically, we leverage the Deep Evid
    
[^90]: 像素级别识别用于整体外科场景理解

    Pixel-Wise Recognition for Holistic Surgical Scene Understanding. (arXiv:2401.11174v1 [cs.CV])

    [http://arxiv.org/abs/2401.11174](http://arxiv.org/abs/2401.11174)

    本文提出了一个整体和多粒度外科场景理解数据集，以及一个基于变形器的模型，该模型有效地结合了全局视频特征提取和局部器械分割，可用于多层次理解外科活动。

    

    本文提出了Prostatectomies的整体和多粒度外科场景理解（GraSP）数据集，该数据集对外科场景理解进行了层次化建模，包括不同粒度的互补任务。我们的方法实现了对外科活动的多层次理解，包括外科阶段和步骤的识别以及包括外科器械分割和原子可视动作检测在内的短期任务。为了利用我们提出的数据集，我们引入了基于变形器（Transformers）的行动、阶段、步骤和器械分割（TAPIS）模型，该模型将全局视频特征提取器与来自器械分割模型的局部区域建议相结合，以应对我们数据集的多粒度问题。通过广泛的实验，我们展示了在短期识别任务中包括分割注释的影响，并突显了不同的粒度要求。

    This paper presents the Holistic and Multi-Granular Surgical Scene Understanding of Prostatectomies (GraSP) dataset, a curated benchmark that models surgical scene understanding as a hierarchy of complementary tasks with varying levels of granularity. Our approach enables a multi-level comprehension of surgical activities, encompassing long-term tasks such as surgical phases and steps recognition and short-term tasks including surgical instrument segmentation and atomic visual actions detection. To exploit our proposed benchmark, we introduce the Transformers for Actions, Phases, Steps, and Instrument Segmentation (TAPIS) model, a general architecture that combines a global video feature extractor with localized region proposals from an instrument segmentation model to tackle the multi-granularity of our benchmark. Through extensive experimentation, we demonstrate the impact of including segmentation annotations in short-term recognition tasks, highlight the varying granularity require
    
[^91]: 高斯自适应注意力是唯一所需的：跨多个模态的健壮上下文表示

    Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities. (arXiv:2401.11143v1 [cs.LG])

    [http://arxiv.org/abs/2401.11143](http://arxiv.org/abs/2401.11143)

    该论文提出了一个名为GAAM的多头高斯自适应注意力机制，用于增强跨多个模态的信息聚合。通过将可学习的均值和方差纳入注意力机制中，GAAM能够动态地重新调整特征的重要性，从而在处理非平稳数据时取得了显著的性能提升，超过了目前现有的注意力技术。该方法的适应性强且参数数量较少，具有改进现有注意力框架的潜力。

    

    我们提出了多头高斯自适应注意力机制（GAAM），一种新颖的概率注意力框架，并设计了高斯自适应变压器（GAT），旨在增强跨多个模态（包括语音、文本和视觉）的信息聚合。GAAM将可学习的均值和方差融入其注意力机制中，采用多头框架实现，使其能够集体建模任何概率分布，以动态重新调整特征重要性。该方法在处理高度非平稳数据时表现出显著改进，通过识别特征空间中的关键元素，超越了现有的注意力技术在模型性能上的状态（精度增加约20%）。GAAM与基于点积的注意力模型兼容，并具有相对较低的参数数量，展示了其适应性和提升现有注意力框架的潜力。在实证方面，GAAM表现出卓越的适应性和功效。

    We propose the Multi-Head Gaussian Adaptive Attention Mechanism (GAAM), a novel probabilistic attention framework, and the Gaussian Adaptive Transformer (GAT), designed to enhance information aggregation across multiple modalities, including Speech, Text and Vision. GAAM integrates learnable mean and variance into its attention mechanism, implemented in a Multi-Headed framework enabling it to collectively model any Probability Distribution for dynamic recalibration of feature significance. This method demonstrates significant improvements, especially with highly non-stationary data, surpassing the state-of-the-art attention techniques in model performance (up to approximately +20% in accuracy) by identifying key elements within the feature space. GAAM's compatibility with dot-product-based attention models and relatively low number of parameters showcases its adaptability and potential to boost existing attention frameworks. Empirically, GAAM exhibits superior adaptability and efficacy
    
[^92]: Chem-FINESE: 通过文本重构验证细粒度少样本实体提取

    Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction. (arXiv:2401.10189v1 [cs.CL])

    [http://arxiv.org/abs/2401.10189](http://arxiv.org/abs/2401.10189)

    这篇论文提出了一种名为Chem-FINESE的方法来处理化学领域中细粒度少样本实体提取的问题。该方法通过使用序列到序列的实体提取器和自我验证模块来从输入句子中提取命名实体并重构原始输入句子。实验证明了该方法的有效性和可行性。

    

    在化学领域中，细粒度少样本实体提取面临两个独特的挑战。首先，与一般领域的实体提取任务相比，化学论文中的句子通常包含更多的实体。此外，实体提取模型通常难以提取长尾类型的实体。在本文中，我们提出了一种新颖的基于序列到序列的少样本实体提取方法Chem-FINESE来解决这两个挑战。我们的Chem-FINESE包含两个组件：一个序列到序列的实体提取器用于从输入句子中提取命名实体，以及一个序列到序列的自我验证模块用于从提取的实体中重构原始输入句子。受到一个好的实体提取系统需要忠实提取实体的事实启发，我们的新自我验证模块利用实体提取结果来重构原始输入句子。此外，我们设计了一种新的对比损失来减少在提取过程中的过度复制。

    Fine-grained few-shot entity extraction in the chemical domain faces two unique challenges. First, compared with entity extraction tasks in the general domain, sentences from chemical papers usually contain more entities. Moreover, entity extraction models usually have difficulty extracting entities of long-tailed types. In this paper, we propose Chem-FINESE, a novel sequence-to-sequence (seq2seq) based few-shot entity extraction approach, to address these two challenges. Our Chem-FINESE has two components: a seq2seq entity extractor to extract named entities from the input sentence and a seq2seq self-validation module to reconstruct the original input sentence from extracted entities. Inspired by the fact that a good entity extraction system needs to extract entities faithfully, our new self-validation module leverages entity extraction results to reconstruct the original input sentence. Besides, we design a new contrastive loss to reduce excessive copying during the extraction proces
    
[^93]: DiConStruct: 基于黑盒精华的因果概念解释

    DiConStruct: Causal Concept-based Explanations through Black-Box Distillation. (arXiv:2401.08534v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.08534](http://arxiv.org/abs/2401.08534)

    DiConStruct是一种基于黑盒模型的因果概念解释方法，通过创建结构性因果模型和概念归因方式提供更具可解释性的局部解释。

    

    模型可解释性在人工智能决策系统中起着核心作用。理想情况下，解释应该使用人可解释的语义概念来表达。此外，解释器应该捕捉这些概念之间的因果关系，以便对解释进行推理。最后，解释方法应该高效，并不损害预测任务的性能。尽管近年来AI解释性取得了快速进展，但据我们所知，至今没有一种方法满足这三个条件。事实上，主流的局部概念可解释性方法不产生因果解释，并在解释性和预测性能之间存在权衡。我们提出了DiConStruct，一种既基于概念又具有因果性的解释方法，旨在通过结构性因果模型和概念归因方式创建更具可解释性的局部解释。我们的解释器作为一个精华模型适用于任何黑盒机器学习模型。

    Model interpretability plays a central role in human-AI decision-making systems. Ideally, explanations should be expressed using human-interpretable semantic concepts. Moreover, the causal relations between these concepts should be captured by the explainer to allow for reasoning about the explanations. Lastly, explanation methods should be efficient and not compromise the performance of the predictive task. Despite the rapid advances in AI explainability in recent years, as far as we know to date, no method fulfills these three properties. Indeed, mainstream methods for local concept explainability do not produce causal explanations and incur a trade-off between explainability and prediction performance. We present DiConStruct, an explanation method that is both concept-based and causal, with the goal of creating more interpretable local explanations in the form of structural causal models and concept attributions. Our explainer works as a distillation model to any black-box machine l
    
[^94]: CCNETS:一种新颖的脑启发方法用于增强不平衡数据集中的模式识别

    CCNETS: A Novel Brain-Inspired Approach for Enhanced Pattern Recognition in Imbalanced Datasets. (arXiv:2401.04139v1 [cs.LG])

    [http://arxiv.org/abs/2401.04139](http://arxiv.org/abs/2401.04139)

    CCNETS是一种新颖的脑启发方法，通过模拟大脑的信息处理，通过生成高质量的数据集来增强不平衡数据集中的模式识别，特别关注处理机器学习中的不平衡数据集的挑战。

    

    本研究介绍了CCNETS（具有因果合作网络的因果学习），这是一种新颖的基于生成模型的分类器，旨在解决模式识别中不平衡数据集生成的挑战。CCNETS独特地设计成模拟类似于大脑的信息处理，并包括三个主要组件：解释器、生成器和推理器。每个组件都被设计成模仿特定的大脑功能，有助于生成高质量的数据集并增强分类性能。该模型特别关注在机器学习中处理不平衡数据集的常见和重要挑战。通过将CCNETS应用于一个“欺诈数据集”，其中正常交易明显多于欺诈交易（99.83％ vs. 0.17％），证明了CCNETS的有效性。传统方法往往在处理这种不平衡时遇到困难，导致性能指标不均衡。然而，CCNETS展现出优越的分类能力，通过其性能指标的改善来体现。

    This study introduces CCNETS (Causal Learning with Causal Cooperative Nets), a novel generative model-based classifier designed to tackle the challenge of generating data for imbalanced datasets in pattern recognition. CCNETS is uniquely crafted to emulate brain-like information processing and comprises three main components: Explainer, Producer, and Reasoner. Each component is designed to mimic specific brain functions, which aids in generating high-quality datasets and enhancing classification performance.  The model is particularly focused on addressing the common and significant challenge of handling imbalanced datasets in machine learning. CCNETS's effectiveness is demonstrated through its application to a "fraud dataset," where normal transactions significantly outnumber fraudulent ones (99.83% vs. 0.17%). Traditional methods often struggle with such imbalances, leading to skewed performance metrics. However, CCNETS exhibits superior classification ability, as evidenced by its pe
    
[^95]: Agent AI: 对多模态交互的横向调查

    Agent AI: Surveying the Horizons of Multimodal Interaction. (arXiv:2401.03568v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2401.03568](http://arxiv.org/abs/2401.03568)

    Agent AI是一种多模态交互系统，可以感知视觉刺激、语言输入和其他环境相关数据，通过将代理体嵌入物理或虚拟环境中来实现更复杂和上下文感知的人工智能系统。

    

    多模态人工智能系统很可能会成为我们日常生活中无处不在的存在。使其更互动的一种有前途的方法是将它们作为代理体现在物理和虚拟环境中。目前，系统利用现有的基础模型作为创建代理体的基本构建模块。将代理体嵌入这样的环境中有助于模型处理和解释视觉和环境数据的能力，这对于创建更复杂和上下文感知的人工智能系统至关重要。例如，一个系统可以感知用户动作、人类行为、环境物体、音频表达和场景的集体情感，从而在给定环境中为代理体提供信息和指导。为了加速代理体多模态智能的研究，我们将“Agent AI”定义为一类可以感知视觉刺激、语言输入和其他环境相关数据的交互系统。

    Multi-modal AI systems will likely become a ubiquitous presence in our everyday lives. A promising approach to making these systems more interactive is to embody them as agents within physical and virtual environments. At present, systems leverage existing foundation models as the basic building blocks for the creation of embodied agents. Embedding agents within such environments facilitates the ability of models to process and interpret visual and contextual data, which is critical for the creation of more sophisticated and context-aware AI systems. For example, a system that can perceive user actions, human behavior, environmental objects, audio expressions, and the collective sentiment of a scene can be used to inform and direct agent responses within the given environment. To accelerate research on agent-based multimodal intelligence, we define "Agent AI" as a class of interactive systems that can perceive visual stimuli, language inputs, and other environmentally-grounded data, an
    
[^96]: 如何发起攻击？一种灵感来源于因果关系的生成反事实对抗样本的方法

    Where and How to Attack? A Causality-Inspired Recipe for Generating Counterfactual Adversarial Examples. (arXiv:2312.13628v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.13628](http://arxiv.org/abs/2312.13628)

    该论文通过考虑因果关系的视角，确定了深度神经网络（DNNs）脆弱性的源头，并提出了一种生成更真实的对抗样本的方法。

    

    深度神经网络（DNNs）已经被证明对精心设计的"对抗样本"易受攻击，这些攻击是通过受限或非受限的$\mathcal{L}_p$范数生成的。然而，大多数方法假设对手可以任意修改任何特征，并忽视了数据的因果生成过程，这是不合理和不切实际的。通过考虑被低估的因果生成过程，我们首先通过因果关系的视角确定了DNNs的脆弱性的源头，然后给出了回答"如何发起攻击"的理论结果。其次，考虑到攻击干预对当前样本状态的影响，以生成更真实的对抗样本，我们提出了一个名为CADE的框架。

    Deep neural networks (DNNs) have been demonstrated to be vulnerable to well-crafted \emph{adversarial examples}, which are generated through either well-conceived $\mathcal{L}_p$-norm restricted or unrestricted attacks. Nevertheless, the majority of those approaches assume that adversaries can modify any features as they wish, and neglect the causal generating process of the data, which is unreasonable and unpractical. For instance, a modification in income would inevitably impact features like the debt-to-income ratio within a banking system. By considering the underappreciated causal generating process, first, we pinpoint the source of the vulnerability of DNNs via the lens of causality, then give theoretical results to answer \emph{where to attack}. Second, considering the consequences of the attack interventions on the current state of the examples to generate more realistic adversarial examples, we propose CADE, a framework that can generate \textbf{C}ounterfactual \textbf{AD}vers
    
[^97]: 具有人工智能的通信系统中的生成网络层

    Generative Network Layer for Communication Systems with Artificial Intelligence. (arXiv:2312.05398v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2312.05398](http://arxiv.org/abs/2312.05398)

    这个论文提出了一种使用生成式人工智能在中间或边缘网络节点上的生成网络层，通过将潜在表示压缩后的提示生成图像，可以显著改善网络中所需的数据速率。

    

    传统的网络层的作用是通过中间网络节点将数据包从源传输到目的地。我们提出了一种在中间或边缘网络节点使用生成式人工智能（GenAI）的生成网络层，并分析其对网络中所需数据速率的影响。我们进行了一个案例研究，其中使用GenAI辅助节点从包含大幅压缩的潜在表示的提示中生成图像。在图像质量约束下进行的网络流分析结果显示，生成网络层可以实现超过100%的数据速率要求改进。

    The traditional role of the network layer is the transfer of packet replicas from source to destination through intermediate network nodes. We present a generative network layer that uses Generative AI (GenAI) at intermediate or edge network nodes and analyze its impact on the required data rates in the network. We conduct a case study where the GenAI-aided nodes generate images from prompts that consist of substantially compressed latent representations. The results from network flow analyses under image quality constraints show that the generative network layer can achieve an improvement of more than 100% in terms of the required data rate.
    
[^98]: 在未知环境中使用信息路径规划的半监督主动学习进行语义分割

    Semi-Supervised Active Learning for Semantic Segmentation in Unknown Environments Using Informative Path Planning. (arXiv:2312.04402v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2312.04402](http://arxiv.org/abs/2312.04402)

    提出了一种基于信息路径规划的半监督主动学习方法，用于解决在未知环境中进行语义分割的问题，通过减少人工标注量，提高了机器人的感知能力。

    

    语义分割使得机器人能够对其环境进行感知和推理，超出了几何学的范畴。大多数这类系统建立在深度学习方法的基础上。由于自主机器人通常部署在初始未知环境中，对静态数据集进行预训练不能总是捕捉到多样的领域，限制了机器人在任务中的感知性能。最近出现了一些自监督和完全监督的主动学习方法来改进机器人的视觉。这些方法依赖于大规模的领域内预训练数据集，或者需要大量人工标注的工作。我们提出了一种基于地图的自适应路径规划方法，用于半监督的语义分割主动学习，相比完全监督方法，极大地减少了人工标注的需求。我们利用自适应路径规划器来引导机器人探索未知空间的边界，并收集具有高模型不确定性的训练数据进行人工标注。我们方法的一个关键方面是结合了稀疏的

    Semantic segmentation enables robots to perceive and reason about their environments beyond geometry. Most of such systems build upon deep learning approaches. As autonomous robots are commonly deployed in initially unknown environments, pre-training on static datasets cannot always capture the variety of domains and limits the robot's perception performance during missions. Recently, self-supervised and fully supervised active learning methods emerged to improve a robot's vision. These approaches rely on large in-domain pre-training datasets or require substantial human labelling effort. We propose a planning method for semi-supervised active learning of semantic segmentation that substantially reduces human labelling requirements compared to fully supervised approaches. We leverage an adaptive map-based planner guided towards the frontiers of unexplored space with high model uncertainty collecting training data for human labelling. A key aspect of our approach is to combine the spars
    
[^99]: 通过整数规划对温顺函数进行分段多项式回归

    Piecewise polynomial regression of tame functions via integer programming. (arXiv:2311.13544v1 [math.OC] CROSS LISTED)

    [http://arxiv.org/abs/2311.13544](http://arxiv.org/abs/2311.13544)

    本论文提出了使用整数规划对温顺函数进行分段多项式回归的方法，这可以用于估计包含在许多应用中的温顺函数，并且展示了令人期待的计算结果。

    

    我们考虑估计属于一类特定的非光滑函数的函数的任务，即所谓的温顺函数。这些函数出现在各种应用中：深度学习的训练、混合整数规划的价值函数或小分子的波函数。我们展示了温顺函数在任何完全维度的立方体上可用分段多项式来逼近。然后我们提出了第一个分段多项式回归的混合整数规划形式。这些方法可用于估计温顺函数。我们展示了令人期待的计算结果。

    We consider the task of estimating functions belonging to a specific class of nonsmooth functions, namely so-called tame functions. These functions appear in a wide range of applications: training deep learning, value functions of mixed-integer programs, or wave functions of small molecules. We show that tame functions are approximable by piecewise polynomials on any full-dimensional cube. We then present the first ever mixed-integer programming formulation of piecewise polynomial regression. Together, these can be used to estimate tame functions. We demonstrate promising computational results.
    
[^100]: 通信受限的贝叶斯主动知识蒸馏

    Communication-Constrained Bayesian Active Knowledge Distillation. (arXiv:2311.08053v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.08053](http://arxiv.org/abs/2311.08053)

    本研究提出了一种名为通信受限的贝叶斯主动知识蒸馏（CC-BAKD）的新协议，通过使用线性混合机制将贝叶斯主动学习与压缩相结合，解决了在学习者与教师之间进行通信时关于批次选择和批次编码的重要问题。

    

    传统的重传（ARQ）协议旨在确保接收方正确接收到发射方的所有分组。当发射方是一个学习者与一个教师进行通信时，这个目标与学习者的实际目标相冲突，学习者的目标是从教师那里获取最相关的标签信息。从主动学习的角度出发，本文解决以下关键协议设计问题：(i)主动批次选择：应该发送哪个批次的输入给教师以获取最有用的信息，从而减少通信轮次的数量？(ii)批次编码：是否可以组合数据点的批次以减少每个通信轮次所需的通信资源？具体而言，本研究引入了通信受限的贝叶斯主动知识蒸馏（CC-BAKD），这是一种通过线性混合机制将贝叶斯主动学习与压缩相结合的新型协议。

    Conventional retransmission (ARQ) protocols are designed with the goal of ensuring the correct reception of all the individual transmitter's packets at the receiver. When the transmitter is a learner communicating with a teacher, this goal is at odds with the actual aim of the learner, which is that of eliciting the most relevant label information from the teacher. Taking an active learning perspective, this paper addresses the following key protocol design questions: (i) Active batch selection: Which batch of inputs should be sent to the teacher to acquire the most useful information and thus reduce the number of required communication rounds? (ii) Batch encoding: Can batches of data points be combined to reduce the communication resources required at each communication round? Specifically, this work introduces Communication-Constrained Bayesian Active Knowledge Distillation (CC-BAKD), a novel protocol that integrates Bayesian active learning with compression via a linear mix-up mecha
    
[^101]: ViR: 迈向高效视觉保留骨干的研究

    ViR: Towards Efficient Vision Retention Backbones. (arXiv:2310.19731v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.19731](http://arxiv.org/abs/2310.19731)

    ViR提出了一种新的计算机视觉模型，采用双重并行和递归公式，从而在快速推理和并行训练之间取得了最佳平衡，具有竞争力的性能。

    

    近年来，视觉变换器（ViTs）因其在建模长程空间依赖性和大规模训练可扩展性方面的卓越能力而受到广泛关注。尽管自注意机制的训练并行性在保持出色性能方面起着重要作用，但其二次复杂度阻碍了ViTs在许多需要快速推理的场景中的应用。这种影响在需要自回归建模输入特征的应用中尤为明显。在自然语言处理（NLP）中，一种新的努力方向提出了具有可并行化模型和递归公式的模型，可以在生成应用中实现高效推理。受到这一趋势的启发，我们提出了一种新的计算机视觉模型，名为Vision Retention Networks（ViR），具有双重并行和递归公式，可以在快速推理和并行训练方面取得最佳平衡，并具有竞争力的性能。

    Vision Transformers (ViTs) have attracted a lot of popularity in recent years, due to their exceptional capabilities in modeling long-range spatial dependencies and scalability for large scale training. Although the training parallelism of self-attention mechanism plays an important role in retaining great performance, its quadratic complexity baffles the application of ViTs in many scenarios which demand fast inference. This effect is even more pronounced in applications in which autoregressive modeling of input features is required. In Natural Language Processing (NLP), a new stream of efforts has proposed parallelizable models with recurrent formulation that allows for efficient inference in generative applications. Inspired by this trend, we propose a new class of computer vision models, dubbed Vision Retention Networks (ViR), with dual parallel and recurrent formulations, which strike an optimal balance between fast inference and parallel training with competitive performance. In 
    
[^102]: 个性化蒸馏：为代码生成赋能自适应学习的开源LLMs

    Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation. (arXiv:2310.18628v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.18628](http://arxiv.org/abs/2310.18628)

    通过个性化蒸馏，将闭源LLMs的能力传递给开源LLMs，并在代码生成任务中表现出比标准蒸馏更好的性能，只使用三分之一的数据。

    

    随着强大的闭源LLMs（ChatGPT，GPT-4）的崛起，越来越多的人对将闭源LLMs的功能蒸馏到较小的开源LLMs中表示兴趣。以往的蒸馏方法通常是引导ChatGPT生成一组指令和答案，以供学生模型学习。然而，这种标准蒸馏方法忽视了学生模型的优点和条件。受现代教学原则的启发，我们设计了一种个性化蒸馏过程，其中学生首先尝试解决一个任务，然后老师提供自适应的改进方法来帮助学生提高。个性化蒸馏不同于提供给学生老师的先验知识，它使学生模型能够进行个性化学习，只在自己犯错误的示例上进行学习，并改进自己的解决方案。在代码生成方面，个性化蒸馏始终优于只使用三分之一数据的标准蒸馏方法。

    With the rise of powerful closed-sourced LLMs (ChatGPT, GPT-4), there are increasing interests in distilling the capabilies of close-sourced LLMs to smaller open-sourced LLMs. Previous distillation methods usually prompt ChatGPT to generate a set of instructions and answers, for the student model to learn. However, such standard distillation approach neglects the merits and conditions of the student model. Inspired by modern teaching principles, we design a personalised distillation process, in which the student attempts to solve a task first, then the teacher provides an adaptive refinement for the student to improve. Instead of feeding the student with teacher's prior, personalised distillation enables personalised learning for the student model, as it only learns on examples it makes mistakes upon and learns to improve its own solution. On code generation, personalised distillation consistently outperforms standard distillation with only one third of the data. With only 2.5-3K perso
    
[^103]: 一个通用框架实现G-等变网络中的强鲁棒性

    A General Framework for Robust G-Invariance in G-Equivariant Networks. (arXiv:2310.18564v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.18564](http://arxiv.org/abs/2310.18564)

    这项研究介绍了一种通用方法，通过引入G-三重相关层，在G-等变卷积神经网络中实现强鲁棒性。该方法利用完备的三重相关理论，这使得G-TC层能够在面对不变性攻击时具有强大的鲁棒性，并且能够在分类准确率上相比标准的Max G-Pooling有明显的改善。

    

    我们引入了一种通用方法，用于实现G-等变卷积神经网络（G-CNNs）中的强组不变性，我们将其称为G-三重相关（G-TC）层。该方法利用了群上的三重相关理论，该理论是唯一的、最低次数的多项式不变映射，同时也是完备的。许多常用的不变映射，例如max，是不完备的：它们会同时去除群和信号结构。相比之下，完备的不变映射只移除由于群作用引起的变化，同时保留有关信号结构的所有信息。三重相关的完备性赋予了G-TC层强大的鲁棒性，这可以在其对不变性攻击的抵抗中观察到。此外，我们观察到它相比于G-CNN架构中的标准Max G-Pooling在分类准确率上有明显的改善。我们提供了该方法的通用且高效的实现。

    We introduce a general method for achieving robust group-invariance in group-equivariant convolutional neural networks ($G$-CNNs), which we call the $G$-triple-correlation ($G$-TC) layer. The approach leverages the theory of the triple-correlation on groups, which is the unique, lowest-degree polynomial invariant map that is also complete. Many commonly used invariant maps--such as the max--are incomplete: they remove both group and signal structure. A complete invariant, by contrast, removes only the variation due to the actions of the group, while preserving all information about the structure of the signal. The completeness of the triple correlation endows the $G$-TC layer with strong robustness, which can be observed in its resistance to invariance-based adversarial attacks. In addition, we observe that it yields measurable improvements in classification accuracy over standard Max $G$-Pooling in $G$-CNN architectures. We provide a general and efficient implementation of the method 
    
[^104]: 从雷达中利用机器学习估计最大垂直速度

    Machine Learning Estimation of Maximum Vertical Velocity from Radar. (arXiv:2310.09392v1 [cs.LG])

    [http://arxiv.org/abs/2310.09392](http://arxiv.org/abs/2310.09392)

    本研究利用机器学习模型U-Nets，通过3D雷达反射率，成功地估计了最大垂直速度及其面积范围，并采用Sinh-arcsinh-normal（SHASH）分布参数回归技术进行了确定性和概率预测。

    

    尽管是严重天气灾害的源头，但对快速上升气流（即上升气流）的量化仍无法用于操作预测。像卫星图像中的透顶区域这样的上升气流代理物已被与严重天气灾害联系起来，但只与总体风暴上升气流的一小部分有关。本研究调查了一个机器学习模型，即U-Nets，是否能够仅利用三维（3D）格网雷达反射率，精确地提取最大垂直速度及其面积范围。该机器学习模型使用模拟雷达反射率和垂直速度训练于国家严重风暴实验室的预测预警系统（WoFS）。采用Sinh-arcsinh-normal（SHASH）分布的参数回归技术来适应UNets，允许对最大垂直速度进行确定性和概率预测。经过超参数搜索后，选出了最佳模型。

    Despite being the source region of severe weather hazards, the quantification of the fast current of upward moving air (i.e., updraft) remains unavailable for operational forecasting. Updraft proxies, like overshooting top area from satellite images, have been linked to severe weather hazards but only relate to a limited portion of the total storm updraft. This study investigates if a machine learning model, namely U-Nets, can skillfully retrieve maximum vertical velocity and its areal extent from 3-dimensional (3D) gridded radar reflectivity alone. The machine learning model is trained using simulated radar reflectivity and vertical velocity from the National Severe Storm Laboratory's convection permitting Warn on Forecast System (WoFS). A parametric regression technique using the Sinh-arcsinh-normal (SHASH) distribution is adapted to run with UNets, allowing for both deterministic and probabilistic predictions of maximum vertical velocity. The best models after hyperparameter search 
    
[^105]: 边缘FMCW雷达的手势识别

    Gesture Recognition for FMCW Radar on the Edge. (arXiv:2310.08876v1 [cs.LG])

    [http://arxiv.org/abs/2310.08876](http://arxiv.org/abs/2310.08876)

    本文介绍了一种基于60 GHz FMCW雷达的轻量级手势识别系统，通过使用一组五个特征和精简的处理算法，可以在嵌入式平台上高效识别手势，同时具有较低的内存、计算和功耗要求。

    

    本文介绍了一种基于60 GHz调频连续波(FMCW)雷达的轻量级手势识别系统。我们展示了手势可以通过一组五个特征有效地进行特征化，并提出了一种精简的雷达处理算法来提取这些特征。与之前的方法相比，我们避免了繁重的二维处理，即距离-多普勒成像，并改为进行早期目标检测-这使得我们能够将系统移植到具有内存、计算和功耗严格限制的完全嵌入式平台上。基于循环神经网络(RNN)的架构利用这些特征共同检测和分类五种不同的手势。所提出的系统在我们的保留测试数据集上以98.4%的F1分数识别手势，它在一个Arm Cortex-M4微控制器上运行，需要不到280 kB的闪存存储器，120 kB的RAM，并消耗75 mW的功耗。

    This paper introduces a lightweight gesture recognition system based on 60 GHz frequency modulated continuous wave (FMCW) radar. We show that gestures can be characterized efficiently by a set of five features, and propose a slim radar processing algorithm to extract these features. In contrast to previous approaches, we avoid heavy 2D processing, i.e. range-Doppler imaging, and perform instead an early target detection - this allows us to port the system to fully embedded platforms with tight constraints on memory, compute and power consumption. A recurrent neural network (RNN) based architecture exploits these features to jointly detect and classify five different gestures. The proposed system recognizes gestures with an F1 score of 98.4% on our hold-out test dataset, it runs on an Arm Cortex-M4 microcontroller requiring less than 280 kB of flash memory, 120 kB of RAM, and consuming 75 mW of power.
    
[^106]: 安全深度策略适应

    Safe Deep Policy Adaptation. (arXiv:2310.08602v1 [cs.RO])

    [http://arxiv.org/abs/2310.08602](http://arxiv.org/abs/2310.08602)

    该论文提出了SafeDPA，一种新颖的强化学习和控制框架，用于同时解决策略适应和安全强化学习的问题。SafeDPA在仿真环境中联合学习自适应策略和动力学模型，并使用少量真实数据进行微调。在真实世界部署过程中，通过引入基于控制屏障函数的安全过滤器，确保了SafeDPA的安全性。

    

    自主和人工智能的一个重要目标是使自主机器人能够在动态和不确定的环境中快速适应。经典的自适应控制和安全控制提供了稳定性和安全性保证，但仅限于特定的系统类别。相比之下，基于强化学习（RL）的策略适应提供了通用性和泛化性，但同时也带来了安全性和稳健性的挑战。我们提出了SafeDPA，一种新颖的RL和控制框架，同时解决了策略适应和安全强化学习的问题。SafeDPA在仿真环境中联合学习自适应策略和动力学模型，预测环境配置，并使用少量真实数据对动力学模型进行微调。在RL策略之上引入基于控制屏障函数（CBF）的安全过滤器，以确保在真实世界部署过程中的安全性。我们提供了SafeDPA的理论安全性保证，并展示了SafeDPA对学习误差的稳健性。

    A critical goal of autonomy and artificial intelligence is enabling autonomous robots to rapidly adapt in dynamic and uncertain environments. Classic adaptive control and safe control provide stability and safety guarantees but are limited to specific system classes. In contrast, policy adaptation based on reinforcement learning (RL) offers versatility and generalizability but presents safety and robustness challenges. We propose SafeDPA, a novel RL and control framework that simultaneously tackles the problems of policy adaptation and safe reinforcement learning. SafeDPA jointly learns adaptive policy and dynamics models in simulation, predicts environment configurations, and fine-tunes dynamics models with few-shot real-world data. A safety filter based on the Control Barrier Function (CBF) on top of the RL policy is introduced to ensure safety during real-world deployment. We provide theoretical safety guarantees of SafeDPA and show the robustness of SafeDPA against learning errors 
    
[^107]: ECoFLaP: 高效的粗到细的逐层剪枝方法用于视觉语言模型

    ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models. (arXiv:2310.02998v1 [cs.CV])

    [http://arxiv.org/abs/2310.02998](http://arxiv.org/abs/2310.02998)

    ECoFLaP提出了一种高效的粗到细的逐层剪枝方法，解决了大型视觉语言模型在压缩和部署时的计算和能耗问题。

    

    大型视觉语言模型（LVLMs）通过整合不同模态的丰富信息，全面理解世界，并在各种多模态下游任务上取得显著的性能提升。然而，由于其巨大的计算/能耗和碳排放，部署LVLMs往往存在问题。这些问题使得采用传统的迭代全局剪枝变得不可行，因为其需要计算整个大型模型的Hessian矩阵以进行稀疏化。相反，最近的研究提出了逐层剪枝方法，避免了全局剪枝的昂贵计算，并根据层内权重的重要性有效压缩模型。然而，这些方法常常由于缺乏全局视角而导致模型压缩不够优化。为了解决大型模型最近高效剪枝方法的这一局限性，我们提出了高效的粗到细的逐层剪枝方法（ECoFLaP）。

    Large Vision-Language Models (LVLMs) can understand the world comprehensively by integrating rich information from different modalities, achieving remarkable performance improvements on various multimodal downstream tasks. However, deploying LVLMs is often problematic due to their massive computational/energy costs and carbon consumption. Such issues make it infeasible to adopt conventional iterative global pruning, which is costly due to computing the Hessian matrix of the entire large model for sparsification. Alternatively, several studies have recently proposed layer-wise pruning approaches to avoid the expensive computation of global pruning and efficiently compress model weights according to their importance within a layer. However, these methods often suffer from suboptimal model compression due to their lack of a global perspective. To address this limitation in recent efficient pruning methods for large models, we propose Efficient Coarse-to-Fine Layer-Wise Pruning (ECoFLaP), 
    
[^108]: 非交换式共形风险控制

    Non-Exchangeable Conformal Risk Control. (arXiv:2310.01262v1 [cs.LG])

    [http://arxiv.org/abs/2310.01262](http://arxiv.org/abs/2310.01262)

    本文提出了一种非交换式共形风险控制的框架，可以在数据不可交换的情况下控制任何单调损失函数的期望值。

    

    最近，由于其能够为黑匣子神经模型的预测提供形式上保证的不确定性集合或区间，确保包含实际真实值的预定义概率，拆分共形预测引发了极大的兴趣。虽然最初的公式假设数据可交换，但一些扩展处理不可交换的数据，在许多现实世界的场景中经常发生。同时，一些进展已经在共形方法中取得，这些方法对更广泛的目标提供统计保证，例如限制最佳F1分数或以期望最小化误报率。在本文中，我们利用和扩展这两个工作线路，提出了非交换式共形风险控制，可以在数据不可交换的情况下控制任何单调损失函数的期望值。我们的框架灵活，假设很少，并允许根据数据的统计相似性进行加权处理。

    Split conformal prediction has recently sparked great interest due to its ability to provide formally guaranteed uncertainty sets or intervals for predictions made by black-box neural models, ensuring a predefined probability of containing the actual ground truth. While the original formulation assumes data exchangeability, some extensions handle non-exchangeable data, which is often the case in many real-world scenarios. In parallel, some progress has been made in conformal methods that provide statistical guarantees for a broader range of objectives, such as bounding the best F1-score or minimizing the false negative rate in expectation. In this paper, we leverage and extend these two lines of work by proposing non-exchangeable conformal risk control, which allows controlling the expected value of any monotone loss function when the data is not exchangeable. Our framework is flexible, makes very few assumptions, and allows weighting the data based on its statistical similarity with t
    
[^109]: TraCE: 轨迹反事实解释分数

    TraCE: Trajectory Counterfactual Explanation Scores. (arXiv:2309.15965v1 [cs.LG])

    [http://arxiv.org/abs/2309.15965](http://arxiv.org/abs/2309.15965)

    TraCE是一个模型无关的模块化框架，用于评估顺序决策任务中的进展。它能够将高度复杂场景中的进展凝练为一个单一值，并在医疗保健和气候变化领域展示了其实用性。

    

    反事实解释和相关算法补救通常被用于理解、解释和可能改变来自黑盒分类器的预测。在本文中，我们提出将反事实扩展应用于评估顺序决策任务中的进展。为此，我们引入了一个模型无关的模块化框架TraCE（轨迹反事实解释）分数，能够将高度复杂场景中的进展凝练为一个单一值。我们通过展示在涵盖医疗保健和气候变化两个案例研究中TraCE的实用性来证明其主要特点。

    Counterfactual explanations, and their associated algorithmic recourse, are typically leveraged to understand, explain, and potentially alter a prediction coming from a black-box classifier. In this paper, we propose to extend the use of counterfactuals to evaluate progress in sequential decision making tasks. To this end, we introduce a model-agnostic modular framework, TraCE (Trajectory Counterfactual Explanation) scores, which is able to distill and condense progress in highly complex scenarios into a single value. We demonstrate TraCE's utility across domains by showcasing its main properties in two case studies spanning healthcare and climate change.
    
[^110]: 线性神经网络的几何性质：置换群下的等变性和不变性

    Geometry of Linear Neural Networks: Equivariance and Invariance under Permutation Groups. (arXiv:2309.13736v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.13736](http://arxiv.org/abs/2309.13736)

    本研究探讨了线性神经网络在置换群作用下的等变性和不变性，并通过行列式变量的直积描述了等变或不变子变量的特性和奇异性。通过稀疏性和权值共享属性，我们提出了关于等变和不变线性网络参数化和设计的结论。

    

    由线性全连接神经网络参数化的函数集合是一个行列式变量。我们研究了在置换群作用下等变或不变的函数子变量。这样的群作用示例包括对图像的平移或90度旋转。我们将这样的等变或不变子变量描述为行列式变量的直积，从中推导出其维度、次数、欧几里得距离、以及奇异性。我们完全刻画了任意置换群的不变性，以及循环群的等变性。我们通过稀疏性和权值共享属性，对等变和不变线性网络的参数化和设计得出结论。我们证明了所有不变的线性函数都可以由一个具有权值共享属性的线性自编码器来参数化，该属性是由所考虑置换的循环分解所强加的。等变函数的秩受限空间

    The set of functions parameterized by a linear fully-connected neural network is a determinantal variety. We investigate the subvariety of functions that are equivariant or invariant under the action of a permutation group. Examples of such group actions are translations or $90^\circ$ rotations on images. We describe such equivariant or invariant subvarieties as direct products of determinantal varieties, from which we deduce their dimension, degree, Euclidean distance degree, and their singularities. We fully characterize invariance for arbitrary permutation groups, and equivariance for cyclic groups. We draw conclusions for the parameterization and the design of equivariant and invariant linear networks in terms of sparsity and weight-sharing properties. We prove that all invariant linear functions can be parameterized by a single linear autoencoder with a weight-sharing property imposed by the cycle decomposition of the considered permutation. The space of rank-bounded equivariant f
    
[^111]: 因果推理：为下一代AI本地化无线网络开辟革命性道路

    Causal Reasoning: Charting a Revolutionary Course for Next-Generation AI-Native Wireless Networks. (arXiv:2309.13223v1 [cs.IT])

    [http://arxiv.org/abs/2309.13223](http://arxiv.org/abs/2309.13223)

    本文提出了一种基于因果推理的框架，用于构建AI本地化的无线网络，以应对现有“AI for wireless”范式的短板。该框架通过解决AI模型的黑匣子特性、曲线拟合特性、对大量训练数据的依赖以及大型神经网络的能量效率低下等问题，克服了数据驱动型、训练密集型AI的局限性。

    

    尽管基本前提是下一代无线网络（例如6G）将是人工智能（AI）本地化的，但到目前为止，大多数现有的工作仍然要么是定性的，要么是对现有“AI用于无线”范式的增量扩展。实际上，创建AI本地化的无线网络面临着重要的技术挑战，因为数据驱动型、训练密集型的AI的局限性。这些限制包括AI模型的黑匣子特性、它们的曲线拟合特性（这可能限制它们的推理和适应能力）、它们对大量训练数据的依赖以及大型神经网络的能量效率低下等。作为对这些限制的回应，本文提出了一个全面的、具有前瞻性的愿景，通过引入一个基于因果推理的新框架来解决这些缺点。该框架基于因果发现、因果表示学习和因果推断。

    Despite the basic premise that next-generation wireless networks (e.g., 6G) will be artificial intelligence (AI)-native, to date, most existing efforts remain either qualitative or incremental extensions to existing ``AI for wireless'' paradigms. Indeed, creating AI-native wireless networks faces significant technical challenges due to the limitations of data-driven, training-intensive AI. These limitations include the black-box nature of the AI models, their curve-fitting nature, which can limit their ability to reason and adapt, their reliance on large amounts of training data, and the energy inefficiency of large neural networks. In response to these limitations, this article presents a comprehensive, forward-looking vision that addresses these shortcomings by introducing a novel framework for building AI-native wireless networks; grounded in the emerging field of causal reasoning. Causal reasoning, founded on causal discovery, causal representation learning, and causal inference, c
    
[^112]: 测量因果控制的因果熵和信息增益

    Causal Entropy and Information Gain for Measuring Causal Control. (arXiv:2309.07703v1 [cs.LG])

    [http://arxiv.org/abs/2309.07703](http://arxiv.org/abs/2309.07703)

    本文提出了一种考虑因果结构的信息论量，用于评估某个特定结果变量的因果重要性，解决了因果可解释性的挑战。

    

    人工智能模型和方法通常缺乏因果可解释性。尽管解释性机器学习（IML）方法取得了进展，但它们经常将重要性赋予那些对结果变量没有因果影响的特征。在模型训练之前或之后，选择因果相关的特征将提供一种解决方案。利用信息论量进行特征选择的方法在识别统计相关特征方面非常成功。然而，它们所基于的信息论量不包含因果关系，因此在这种情况下不适用。为了解决这个挑战，本文提出了能够考虑系统因果结构的信息论量，可以用于评估某个给定结果变量的因果重要性。具体来说，我们引入了因果熵和因果互信息的因果版本。

    Artificial intelligence models and methods commonly lack causal interpretability. Despite the advancements in interpretable machine learning (IML) methods, they frequently assign importance to features which lack causal influence on the outcome variable. Selecting causally relevant features among those identified as relevant by these methods, or even before model training, would offer a solution. Feature selection methods utilizing information theoretical quantities have been successful in identifying statistically relevant features. However, the information theoretical quantities they are based on do not incorporate causality, rendering them unsuitable for such scenarios. To address this challenge, this article proposes information theoretical quantities that incorporate the causal structure of the system, which can be used to evaluate causal importance of features for some given outcome variable. Specifically, we introduce causal versions of entropy and mutual information, termed cau
    
[^113]: 通过时间滞后信息瓶颈的潜在表示和马尔可夫过程模拟

    Latent Representation and Simulation of Markov Processes via Time-Lagged Information Bottleneck. (arXiv:2309.07200v1 [cs.LG])

    [http://arxiv.org/abs/2309.07200](http://arxiv.org/abs/2309.07200)

    本文介绍了一种通过时间滞后信息瓶颈的方法，将复杂系统映射到简化表示空间并模拟时间上的大跳跃。实验证明该方法能够准确模拟原始过程的统计特性和动力学，优于现有的时间滞后降维方法。

    

    马尔可夫过程是描述各个领域中动态系统的广泛使用的数学模型。然而，由于需要准确积分的短时间步长，精确模拟长时间尺度上的大规模系统计算量很大。在本文中，我们引入了一种将复杂系统映射到简化表示空间并模拟时间上的大跳跃的推理过程。为了实现这一点，我们提出了基于信息理论的原则目标-时间滞后信息瓶颈（T-IB），它旨在捕捉相关的时间特征，同时丢弃高频信息以简化模拟任务并最小化推理误差。我们的实验证明，T-IB学习了信息最优的表示，能够准确地模拟原始过程在选择的时间滞后下的统计特性和动力学，并且优于现有的时间滞后降维方法。

    Markov processes are widely used mathematical models for describing dynamic systems in various fields. However, accurately simulating large-scale systems at long time scales is computationally expensive due to the short time steps required for accurate integration. In this paper, we introduce an inference process that maps complex systems into a simplified representational space and models large jumps in time. To achieve this, we propose Time-lagged Information Bottleneck (T-IB), a principled objective rooted in information theory, which aims to capture relevant temporal features while discarding high-frequency information to simplify the simulation task and minimize the inference error. Our experiments demonstrate that T-IB learns information-optimal representations for accurately modeling the statistical properties and dynamics of the original process at a selected time lag, outperforming existing time-lagged dimensionality reduction methods.
    
[^114]: 使用深度学习预测人类意图的智能上肢外骨骼系统以增强感觉反馈

    Intelligent upper-limb exoskeleton using deep learning to predict human intention for sensory-feedback augmentation. (arXiv:2309.04655v1 [cs.RO])

    [http://arxiv.org/abs/2309.04655](http://arxiv.org/abs/2309.04655)

    创新点在于引入了云端深度学习和嵌入式柔性传感器，实现了智能上肢外骨骼系统来预测人类上肢运动的意图并提供感觉反馈，准确率达到96.2%，能以人类意图为基础进行操作。

    

    随着年龄增长和中风等因素，人体肌肉骨骼力量下降，影响了使用上肢进行日常任务的能力。虽然现有一些外骨骼装置，但由于缺乏传感器反馈和对运动意图的预测，需要手动操作。本研究引入了一种智能上肢外骨骼系统，利用云端深度学习来预测人类的意图以增强力量。嵌入式柔性传感器通过收集实时肌肉信号提供感觉反馈，并同时计算以确定用户的意图运动。云端深度学习预测四种上肢关节运动，平均准确率达到96.2%，响应速度为200-250毫秒，表明外骨骼系统完全依靠人类意图进行操作。此外，一组柔性气动装置通过提供897牛顿力和78.7毫米的位移来辅助意图运动。

    The age and stroke-associated decline in musculoskeletal strength degrades the ability to perform daily human tasks using the upper extremities. Although there are a few examples of exoskeletons, they need manual operations due to the absence of sensor feedback and no intention prediction of movements. Here, we introduce an intelligent upper-limb exoskeleton system that uses cloud-based deep learning to predict human intention for strength augmentation. The embedded soft wearable sensors provide sensory feedback by collecting real-time muscle signals, which are simultaneously computed to determine the user's intended movement. The cloud-based deep-learning predicts four upper-limb joint motions with an average accuracy of 96.2% at a 200-250 millisecond response rate, suggesting that the exoskeleton operates just by human intention. In addition, an array of soft pneumatics assists the intended movements by providing 897 newton of force and 78.7 millimeter of displacement at maximum. Col
    
[^115]: 用于计算深度神经网络正则化路径的多目标延续方法

    A multiobjective continuation method to compute the regularization path of deep neural networks. (arXiv:2308.12044v1 [cs.LG])

    [http://arxiv.org/abs/2308.12044](http://arxiv.org/abs/2308.12044)

    本文提出了一种多目标延续方法，用于计算深度神经网络的正则化路径，以解决DNNs中稀疏性和数值效率之间的冲突。

    

    稀疏性是深度神经网络(DNNs)中非常理想的特征，因为它确保了数值效率，提高了模型的可解释性(由于相关特征的数量较少)和鲁棒性。在基于线性模型的机器学习方法中，众所周知在$\ell^1$范数(即零权重)的最稀疏解和非正则化解之间存在一条连接路径，这条路径被称为正则化路径。最近，通过将经验损失和稀疏性($\ell^1$范数)作为两个冲突的标准，并解决由此产生的多目标优化问题，首次尝试将正则化路径的概念扩展到DNNs。然而，由于$\ell^1$范数的不光滑性和参数数量的高度，从计算的角度来看，这种方法并不是很有效。为了克服这个限制，我们提出了一种算法，可以近似计算整个帕累托曲线

    Sparsity is a highly desired feature in deep neural networks (DNNs) since it ensures numerical efficiency, improves the interpretability of models (due to the smaller number of relevant features), and robustness. In machine learning approaches based on linear models, it is well known that there exists a connecting path between the sparsest solution in terms of the $\ell^1$ norm (i.e., zero weights) and the non-regularized solution, which is called the regularization path. Very recently, there was a first attempt to extend the concept of regularization paths to DNNs by means of treating the empirical loss and sparsity ($\ell^1$ norm) as two conflicting criteria and solving the resulting multiobjective optimization problem. However, due to the non-smoothness of the $\ell^1$ norm and the high number of parameters, this approach is not very efficient from a computational perspective. To overcome this limitation, we present an algorithm that allows for the approximation of the entire Pareto
    
[^116]: 在增强的不变关系知识上探索超关系时间知识图的链接预测

    Exploring Link Prediction over Hyper-Relational Temporal Knowledge Graphs Enhanced with Time-Invariant Relational Knowledge. (arXiv:2307.10219v1 [cs.AI])

    [http://arxiv.org/abs/2307.10219](http://arxiv.org/abs/2307.10219)

    这项研究填补了时间KG和超关系KG推理之间的差距，并开发了两个新的基准超关系TKG数据集。

    

    超关系知识图(HKGs)是传统知识图(KGs)的延伸，为每个KG事实提供额外的键值对(即限定词)，以更好地限制事实的有效性。近年来，研究在HKGs上进行图推理越来越受关注。与此同时，由于世界知识的不断演变，大量平行工作集中在对时间KGs(TKGs)进行推理，其中每个TKG事实可以被视为带有时间戳(或时间段)的KG事实，指定其时间有效性。现有的HKG推理方法不考虑时间信息，因为在之前的基准数据集中没有显式地指定。此外，所有以前的TKG推理方法只重视时间推理，并没有办法从限定词中学习。因此，我们的目标是填补TKG推理和HKG推理之间的差距。我们开发了两个新的基准超关系TKG(HTKG)数据集，即Wiki-hy和...

    Stemming from traditional knowledge graphs (KGs), hyper-relational KGs (HKGs) provide additional key-value pairs (i.e., qualifiers) for each KG fact that help to better restrict the fact validity. In recent years, there has been an increasing interest in studying graph reasoning over HKGs. In the meantime, due to the ever-evolving nature of world knowledge, extensive parallel works have been focusing on reasoning over temporal KGs (TKGs), where each TKG fact can be viewed as a KG fact coupled with a timestamp (or time period) specifying its time validity. The existing HKG reasoning approaches do not consider temporal information because it is not explicitly specified in previous benchmark datasets. Besides, all the previous TKG reasoning methods only lay emphasis on temporal reasoning and have no way to learn from qualifiers. To this end, we aim to fill the gap between TKG reasoning and HKG reasoning. We develop two new benchmark hyper-relational TKG (HTKG) datasets, i.e., Wiki-hy and 
    
[^117]: 使用IBM模拟内存硬件加速套件进行神经网络训练和推断

    Using the IBM Analog In-Memory Hardware Acceleration Kit for Neural Network Training and Inference. (arXiv:2307.09357v1 [cs.ET])

    [http://arxiv.org/abs/2307.09357](http://arxiv.org/abs/2307.09357)

    本教程介绍了使用IBM Analog Hardware Acceleration Kit (AIHWKit)进行神经网络训练和推断的方法，该工具包模拟了模拟内存计算（AIMC）的推断和训练，并提供了最佳实践和云环境中使用的优势。

    

    模拟内存计算（AIMC）是减少深度神经网络（DNN）推断和训练的延迟和能源消耗的一种有前景的方法。然而，AIMC芯片中的噪声和非线性器件特性以及非理想的外围电路要求调整DNN以在此类硬件上实现与数字计算等效的精度。在这个教程中，我们详细介绍了如何使用最近发布的IBM模拟硬件加速套件（AIHWKit）进行这样的调整和评估，该套件可在https://github.com/IBM/aihwkit上免费获得。AIHWKit是一个Python库，可以使用AIMC模拟推断和训练DNN。我们详细描述了AIHWKit设计、功能和最佳实践，以正确进行推断和训练。我们还介绍了模拟AI云组合器的概述，该组合提供了在完全托管的云环境中使用AIHWKit模拟平台的优势。

    Analog In-Memory Computing (AIMC) is a promising approach to reduce the latency and energy consumption of Deep Neural Network (DNN) inference and training. However, the noisy and non-linear device characteristics, and the non-ideal peripheral circuitry in AIMC chips, require adapting DNNs to be deployed on such hardware to achieve equivalent accuracy to digital computing. In this tutorial, we provide a deep dive into how such adaptations can be achieved and evaluated using the recently released IBM Analog Hardware Acceleration Kit (AIHWKit), freely available at https://github.com/IBM/aihwkit. The AIHWKit is a Python library that simulates inference and training of DNNs using AIMC. We present an in-depth description of the AIHWKit design, functionality, and best practices to properly perform inference and training. We also present an overview of the Analog AI Cloud Composer, that provides the benefits of using the AIHWKit simulation platform in a fully managed cloud setting. Finally, we
    
[^118]: 用类比马尔可夫链和深度学习进行极端热浪的采样和预测

    Extreme heatwave sampling and prediction with analog Markov chain and comparisons with deep learning. (arXiv:2307.09060v1 [physics.ao-ph])

    [http://arxiv.org/abs/2307.09060](http://arxiv.org/abs/2307.09060)

    本研究提出了一种基于类比马尔可夫链和深度学习的数据驱动模拟器，适用于预测法国和斯堪的纳维亚的长时间热浪。与卷积神经网络相比，该模拟器在概率预测任务上表现更好，并且经过适当评估和性能评估。

    

    我们提出了一种数据驱动的模拟器，随机天气生成器（SWG），适用于估计法国和斯堪的纳维亚长时间热浪的概率。这个模拟器基于环流的类比方法，我们加入温度和土壤湿度作为预测字段。我们将模拟器训练在一个中等复杂度气候模型的运行上，并展示它能够预测样本外热浪的条件概率（预测）。我们特别注意，使用适用于罕见事件的合适评分来评估这个预测。为了加速类比的计算，降维技术被应用，并且性能得到评估。通过SWG实现的概率预测与使用卷积神经网络（CNN）实现的概率预测进行了比较。随着数百年的训练数据的可用性，CNN在概率预测任务上表现更好。此外，我们还展示了经过8个训练实例训练的SWG模拟器的效果。

    We present a data-driven emulator, stochastic weather generator (SWG), suitable for estimating probabilities of prolonged heatwaves in France and Scandinavia. This emulator is based on the method of analogs of circulation to which we add temperature and soil moisture as predictor fields. We train the emulator on an intermediate complexity climate model run and show that it is capable of predicting conditional probabilities (forecasting) of heatwaves out of sample. Special attention is payed that this prediction is evaluated using proper score appropriate for rare events. To accelerate the computation of analogs dimensionality reduction techniques are applied and the performance is evaluated. The probabilistic prediction achieved with SWG is compared with the one achieved with  Convolutional Neural Network (CNN). With the availability of hundreds of years of training data CNNs perform better at the task of probabilistic prediction. In addition, we show that the SWG emulator trained on 8
    
[^119]: 使用梯度下降从测量中学习IMM滤波器参数

    Learning IMM Filter Parameters from Measurements using Gradient Descent. (arXiv:2307.06618v1 [cs.LG])

    [http://arxiv.org/abs/2307.06618](http://arxiv.org/abs/2307.06618)

    本文提出了一种使用梯度下降从测量中学习IMM滤波器参数的方法，通过仅使用测量数据即可优化滤波器的参数。在模拟数据上的实验结果表明，该方法能够达到使用真值参数化的滤波器的性能水平。

    

    数据融合和跟踪算法的性能通常依赖于既描述传感器系统又可以是任务特定的参数。尽管调整这些变量对于传感器系统来说是耗时的，并且通常需要专家知识，但在跟踪目标的内在参数在系统部署之前甚至可能完全不可观测。随着最先进的传感器系统越来越复杂，参数的数量自然增加，需要自动优化模型变量。本文通过仅使用测量来优化交互多模型（IMM）滤波器的参数，因此无需任何基础数据。通过对模拟数据进行消融研究评估了结果方法，结果方法成功匹配了使用基础真值参数化的滤波器的性能。

    The performance of data fusion and tracking algorithms often depends on parameters that not only describe the sensor system, but can also be task-specific. While for the sensor system tuning these variables is time-consuming and mostly requires expert knowledge, intrinsic parameters of targets under track can even be completely unobservable until the system is deployed. With state-of-the-art sensor systems growing more and more complex, the number of parameters naturally increases, necessitating the automatic optimization of the model variables. In this paper, the parameters of an interacting multiple model (IMM) filter are optimized solely using measurements, thus without necessity for any ground-truth data. The resulting method is evaluated through an ablation study on simulated data, where the trained model manages to match the performance of a filter parametrized with ground-truth values.
    
[^120]: 一种用于深度学习的合成心电图（ECG）图像生成工具箱，以促进扫描ECG数字化

    A Synthetic Electrocardiogram (ECG) Image Generation Toolbox to Facilitate Deep Learning-Based Scanned ECG Digitization. (arXiv:2307.01946v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.01946](http://arxiv.org/abs/2307.01946)

    本文介绍了一种用于生成合成ECG图像的工具箱，旨在促进扫描ECG数字化。通过引入真实伪影，如手写文本伪影、皱纹、折痕和视角变换，该方法可以在标准纸质ECG背景上生成具有真实性的ECG图像。这种方法有助于解决合成ECG图像中缺乏参考时间序列的问题。

    

    心电图（ECG）是一种准确且广泛应用于诊断心血管疾病的工具。几十年来，ECG以印刷格式记录，并且将它们的数字化在算法性心电图诊断的机器学习模型训练中具有巨大潜力。物理性ECG存档面临退化风险，仅扫描印刷ECG是不够的，因为机器学习模型需要ECG时间序列数据。因此，将纸质ECG存档数字化和转换为时间序列数据至关重要。深度学习模型在图像处理方面显示出潜力。然而，具有参考时间序列的ECG存档稀缺是一个挑战。利用“数字孪生”的数据增强技术可能是一个潜在的解决方案。我们介绍了一种新的方法，以生成具有真实伪影的标准纸质ECG背景下的合成ECG图像。包括手写文本伪影、皱纹、折痕和视角转换等畸变。

    The electrocardiogram (ECG) is an accurate and widely available tool for diagnosing cardiovascular diseases. ECGs have been recorded in printed formats for decades and their digitization holds great potential for training machine learning (ML) models in algorithmic ECG diagnosis. Physical ECG archives are at risk of deterioration and scanning printed ECGs alone is insufficient, as ML models require ECG time-series data. Therefore, the digitization and conversion of paper ECG archives into time-series data is of utmost importance. Deep learning models for image processing show promise in this regard. However, the scarcity of ECG archives with reference time-series is a challenge. Data augmentation techniques utilizing \textit{digital twins} present a potential solution.  We introduce a novel method for generating synthetic ECG images on standard paper-like ECG backgrounds with realistic artifacts. Distortions including handwritten text artifacts, wrinkles, creases and perspective transf
    
[^121]: 渐进傅里叶神经表示用于顺序视频编译

    Progressive Fourier Neural Representation for Sequential Video Compilation. (arXiv:2306.11305v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.11305](http://arxiv.org/abs/2306.11305)

    本研究提出了一种渐进傅里叶神经表示方法，通过在每个训练会话中找到自适应且紧凑的傅里叶空间子模块来编码顺序视频数据，克服了现有神经隐式表示方法在多个复杂数据上的泛化能力差的问题。

    

    最近神经隐式表示(NIR)因其将复杂和高维数据编码为表示空间并通过可训练的映射函数轻松重构数据的非凡能力而引起了极大关注。然而，NIR方法假定目标数据和表示模型之间存在一对一的映射，而不考虑数据的相关性或相似性。这导致在多组复杂数据上泛化能力较差，并限制了其效率和可伸缩性。受持续学习的启发，本研究探讨了如何在顺序编码会话中累积和传递多个复杂视频数据的神经隐式表示。为了克服NIR的局限性，我们提出了一种新的方法，即渐进傅里叶神经表示(PFNR)，旨在找到一个自适应和紧凑的傅里叶空间子模块，以编码每个训练会话中的视频。这种稀疏的神经编码使神经网络能够持有自由权重，实现了一种可迭代地编码和解码多个顺序视频数据的方式。

    Neural Implicit Representation (NIR) has recently gained significant attention due to its remarkable ability to encode complex and high-dimensional data into representation space and easily reconstruct it through a trainable mapping function. However, NIR methods assume a one-to-one mapping between the target data and representation models regardless of data relevancy or similarity. This results in poor generalization over multiple complex data and limits their efficiency and scalability. Motivated by continual learning, this work investigates how to accumulate and transfer neural implicit representations for multiple complex video data over sequential encoding sessions. To overcome the limitation of NIR, we propose a novel method, Progressive Fourier Neural Representation (PFNR), that aims to find an adaptive and compact sub-module in Fourier space to encode videos in each training session. This sparsified neural encoding allows the neural network to hold free weights, enabling an imp
    
[^122]: 量子卷积神经网络的分裂和并行化用于学习平移对称数据

    Splitting and Parallelizing of Quantum Convolutional Neural Networks for Learning Translationally Symmetric Data. (arXiv:2306.07331v1 [quant-ph])

    [http://arxiv.org/abs/2306.07331](http://arxiv.org/abs/2306.07331)

    提出一种基于平移对称性的分裂并行化QCNN架构，可以高效地学习平移对称量子数据，相比传统的QCNN极大地提高了测量效率和速度。

    

    量子卷积神经网络(QCNN)是一种有望在经典难题上实现量子优势的量子机器学习(QML)模型。然而，QCNN需要大量的测量用于数据学习，从而限制了它在大规模问题上的实际应用。为了缓解这种需求，我们提出了一种新的架构，称为分裂并行化QCNN(sp-QCNN)，它利用量子数据的先验知识设计高效电路。这种架构从几何量子机器学习中获得灵感，针对凝聚态物理中常见的平移对称量子数据。通过基于平移对称性分裂量子电路，sp-QCNN极大地并行化了传统的QCNN，而不增加量子比特数，并进一步提高了测量效率，达到了量子相识别任务的加速效果。

    A quantum convolutional neural network (QCNN) is a promising quantum machine learning (QML) model to achieve quantum advantages in classically intractable problems. However, QCNN requires a large number of measurements for data learning, limiting its practical applications for large-scale problems. To relieve this requirement, we propose a novel architecture called split-parallelizing QCNN (sp-QCNN), which exploits the prior knowledge of quantum data for designing efficient circuits. This architecture draws inspiration from geometric quantum machine learning and targets translationally symmetric quantum data commonly encountered in condensed matter physics. By splitting the quantum circuit based on translational symmetry, sp-QCNN substantially parallelizes conventional QCNN without increasing the number of qubits and further improves the measurement efficiency by an order of the number of qubits. To demonstrate its effectiveness, we apply sp-QCNN to a quantum phase recognition task and
    
[^123]: 多领域联邦学习是否离不开标准化?

    Is Normalization Indispensable for Multi-domain Federated Learning?. (arXiv:2306.05879v1 [cs.LG])

    [http://arxiv.org/abs/2306.05879](http://arxiv.org/abs/2306.05879)

    本研究旨在解决联邦学习中的多领域问题。我们提出一种新的方法，FedWon，通过消除规范化步骤来有效地处理来自不同领域的数据。

    

    联邦学习通过分散在客户端上的协作式内部训练增强了数据隐私。然而，联邦学习面临诸多挑战，其中包括非独立同分布数据（non-i.i.d）导致的潜在性能下降和收敛受阻问题。我们的研究解决了一个关键但常常被忽视的问题——多领域联邦学习。在这种情况下，客户端数据来源于具有不同特征分布的各种领域，而不是标签分布。为了解决联邦学习中的多领域问题，我们提出了一种新方法称为不使用规范化的联邦学习（FedWon）。FedWon从一个观察出发，即批量归一化（BN）在有效地建模多个领域的统计信息方面面临挑战，而替代规范化技术具有自身的局限性。FedWon通过消除规范化步骤来解决这些问题。

    Federated learning (FL) enhances data privacy with collaborative in-situ training on decentralized clients. Nevertheless, FL encounters challenges due to non-independent and identically distributed (non-i.i.d) data, leading to potential performance degradation and hindered convergence. While prior studies predominantly addressed the issue of skewed label distribution, our research addresses a crucial yet frequently overlooked problem known as multi-domain FL. In this scenario, clients' data originate from diverse domains with distinct feature distributions, as opposed to label distributions. To address the multi-domain problem in FL, we propose a novel method called Federated learning Without normalizations (FedWon). FedWon draws inspiration from the observation that batch normalization (BN) faces challenges in effectively modeling the statistics of multiple domains, while alternative normalization techniques possess their own limitations. In order to address these issues, FedWon elimi
    
[^124]: 离线优先经验重放

    Offline Prioritized Experience Replay. (arXiv:2306.05412v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.05412](http://arxiv.org/abs/2306.05412)

    本文提出了离线优先经验重放（OPER）方法来解决离线强化学习中的分布偏移问题。通过设计一类优先级函数来对高回报的转换进行优先处理，从而改善行为策略，并在此改进的策略约束下优化离线强化学习算法的解决方案。对于离线强化学习，OPER方法是一种有效的解决方案。

    

    离线强化学习面临着分布偏移问题。为了解决这个问题，现有的工作主要集中在设计学习策略和行为策略之间的复杂约束。然而，这些约束通过均匀采样等方式被应用到表现良好和表现差的行动上，这可能会对学习策略产生负面影响。为了缓解这个问题，我们提出了离线优先经验重放（OPER），其中包括一类优先级函数，用于将高回报的转换置于更频繁的访问中。通过理论分析，我们证明了这类优先级函数能够引起行为策略的改善，当策略约束到这个改进的策略上时，离线强化学习算法很可能得到更好的解决方案。我们开发了两种实用策略来获得基于拟合值网络的优先权重（OPER-A）或者u

    Offline reinforcement learning (RL) is challenged by the distributional shift problem. To address this problem, existing works mainly focus on designing sophisticated policy constraints between the learned policy and the behavior policy. However, these constraints are applied equally to well-performing and inferior actions through uniform sampling, which might negatively affect the learned policy. To alleviate this issue, we propose Offline Prioritized Experience Replay (OPER), featuring a class of priority functions designed to prioritize highly-rewarding transitions, making them more frequently visited during training. Through theoretical analysis, we show that this class of priority functions induce an improved behavior policy, and when constrained to this improved policy, a policy-constrained offline RL algorithm is likely to yield a better solution. We develop two practical strategies to obtain priority weights by estimating advantages based on a fitted value network (OPER-A) or u
    
[^125]: 分布式智能体在均场博弈中的网络通信

    Networked Communication for Decentralised Agents in Mean-Field Games. (arXiv:2306.02766v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2306.02766](http://arxiv.org/abs/2306.02766)

    本研究在均场博弈中引入网络通信，提出了一种提高分布式智能体学习效率的方案，并进行了实际实验验证。

    

    我们将网络通信引入均场博弈框架，特别是在无oracle的情况下，N个分布式智能体沿着经过的经验系统的单一非周期演化路径学习。我们证明，我们的架构在只有一些关于网络结构的合理假设的情况下，具有样本保证，在集中学习和独立学习情况之间有界。我们讨论了三个理论算法的样本保证实际上并不会导致实际收敛。因此，我们展示了在实际设置中，当理论参数未被观察到（导致Q函数的估计不准确）时，我们的通信方案显著加速了收敛速度，而无需依赖于一个不可取的集中式控制器的假设。我们对三个理论算法进行了几种实际的改进，使我们能够展示它们的第一个实证表现。

    We introduce networked communication to the mean-field game framework, in particular to oracle-free settings where $N$ decentralised agents learn along a single, non-episodic evolution path of the empirical system. We prove that our architecture, with only a few reasonable assumptions about network structure, has sample guarantees bounded between those of the centralised- and independent-learning cases. We discuss how the sample guarantees of the three theoretical algorithms do not actually result in practical convergence. Accordingly, we show that in practical settings where the theoretical parameters are not observed (leading to poor estimation of the Q-function), our communication scheme significantly accelerates convergence over the independent case, without relying on the undesirable assumption of a centralised controller. We contribute several further practical enhancements to all three theoretical algorithms, allowing us to showcase their first empirical demonstrations. Our expe
    
[^126]: 单输出高斯过程中的单个输入多个输出样本

    Multiple output samples per input in a single-output Gaussian process. (arXiv:2306.02719v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.02719](http://arxiv.org/abs/2306.02719)

    本文提出了在单输出高斯过程中允许单个输入具有多个输出样本的方法，以利用可用的输出不确定性信息。通过在speechocean762数据集上的评估，展示了该方法能够计算出更接近多个人工评级器参考输出集的测试集输出分布。

    

    标准的高斯过程（GP）只考虑训练集中每个输入的单个输出样本。针对主观任务的数据集，例如口语评估，可以用多个人工评级器的输出标签对输入进行注释。本文提出将GP推广为允许在训练集中有这些多个输出样本，并且利用可用的输出不确定性信息。这与多输出GP不同，因为这里所有的输出样本都来自同一任务。输出密度函数被形式化为观察到所有输出样本的联合似然度量，为了减少计算成本，潜在变量不会重复。测试集预测类似于标准GP的推理方法，唯一不同的是优化的超参数。通过在speechocean762上进行评估，结果表明，该方法使得GP能够计算出与多个人工评级器的参考输出集更相似的测试集输出分布。

    The standard Gaussian Process (GP) only considers a single output sample per input in the training set. Datasets for subjective tasks, such as spoken language assessment, may be annotated with output labels from multiple human raters per input. This paper proposes to generalise the GP to allow for these multiple output samples in the training set, and thus make use of available output uncertainty information. This differs from a multi-output GP, as all output samples are from the same task here. The output density function is formulated to be the joint likelihood of observing all output samples, and latent variables are not repeated to reduce computation cost. The test set predictions are inferred similarly to a standard GP, with a difference being in the optimised hyper-parameters. This is evaluated on speechocean762, showing that it allows the GP to compute a test set output distribution that is more similar to the collection of reference outputs from the multiple human raters.
    
[^127]: MicroSegNet：一种基于深度学习的微型超声图像前列腺分割方法

    MicroSegNet: A Deep Learning Approach for Prostate Segmentation on Micro-Ultrasound Images. (arXiv:2305.19956v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.19956](http://arxiv.org/abs/2305.19956)

    本文提出了一种基于深度学习的微型超声图像前列腺分割方法，利用多尺度注释引导的Transformer UNet模型和注释引导的二分类交叉熵损失解决低分辨率和界限不清的挑战，该方法更加关注难以分割的区域。

    

    微型超声是一种新型的29MHz超声技术，提供比传统超声高3-4倍的分辨率，在诊断前列腺癌的准确性方面与MRI相当，但成本更低。然而，由于低分辨率和前列腺、膀胱和尿道中线之间的界限不清，基于微型超声的前列腺分割具有挑战性。本文提出了MicroSegNet，这是一个特别设计用于解决这些挑战的多尺度注释引导的Transformer UNet模型。在训练过程中，MicroSegNet更加关注难以分割（难区域）的区域，这些区域具有专家和非专家注释之间的差异。为此，我们提出了注释引导的二分类交叉熵（AG-BCE）损失，它在难区域中给预测误差分配更大的权重和较低的权重。

    Micro-ultrasound (micro-US) is a novel 29-MHz ultrasound technique that provides 3-4 times higher resolution than traditional ultrasound, delivering comparable accuracy for diagnosing prostate cancer to MRI but at a lower cost. Accurate prostate segmentation is crucial for prostate volume measurement, cancer diagnosis, prostate biopsy, and treatment planning. However, prostate segmentation on microUS is challenging due to artifacts and indistinct borders between the prostate, bladder, and urethra in the midline. This paper presents MicroSegNet, a multi-scale annotation-guided transformer UNet model designed specifically to tackle these challenges. During the training process, MicroSegNet focuses more on regions that are hard to segment (hard regions), characterized by discrepancies between expert and non-expert annotations. We achieve this by proposing an annotation-guided binary cross entropy (AG-BCE) loss that assigns a larger weight to prediction errors in hard regions and a lower w
    
[^128]: MT-SLVR: 多任务自监督学习用于变换表示中的特征学习

    MT-SLVR: Multi-Task Self-Supervised Learning for Transformation In(Variant) Representations. (arXiv:2305.17191v1 [cs.LG])

    [http://arxiv.org/abs/2305.17191](http://arxiv.org/abs/2305.17191)

    该论文提出了一种名为 MT-SLVR 的框架，用于解决自监督学习中的不变性问题，以改善不同的下游任务的分类性能。

    

    对比自监督学习因其能从大型未标记数据集中创建高质量表示而受到关注。这些强大的特征为下游任务的数据高效学习提供了扩充不变性，这通常是一种有用的归纳偏差。然而，从先验上不知道所需的不变性数量和类型，并且在不同的下游任务中变化。因此，我们提出了一种多任务自监督框架(MT-SLVR)，以一种参数高效的方式学习变体和不变特征。我们的多任务表示提供了强大和灵活的特征，可使多样的下游任务受益。我们在来自各种音频领域的少样本分类任务上评估了我们的方法，并证明了在所有任务上均有改善的分类性能。

    Contrastive self-supervised learning has gained attention for its ability to create high-quality representations from large unlabelled data sets. A key reason that these powerful features enable data-efficient learning of downstream tasks is that they provide augmentation invariance, which is often a useful inductive bias. However, the amount and type of invariances preferred is not known apriori, and varies across different downstream tasks. We therefore propose a multi-task self-supervised framework (MT-SLVR) that learns both variant and invariant features in a parameter-efficient manner. Our multi-task representation provides a strong and flexible feature that benefits diverse downstream tasks. We evaluate our approach on few-shot classification tasks drawn from a variety of audio domains and demonstrate improved classification performance on all of them
    
[^129]: 协作世界模型: 一种在线离线转移RL方法。

    Collaborative World Models: An Online-Offline Transfer RL Approach. (arXiv:2305.15260v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.15260](http://arxiv.org/abs/2305.15260)

    本论文提出了一种名为协作世界模型（CoWorld）的转移学习方法，以改善离线条件下视觉RL的性能。其核心想法是使用易于交互的模拟器来训练辅助RL模型作为离线策略的在线“测试床”，并执行域协作表示学习和域协作行为学习，缓解离线数据分布之外的价值函数过度估计问题。

    

    在离线数据集中训练视觉强化学习（RL）模型由于表征学习中的过度拟合问题和价值函数中的过度估计问题而具有挑战性。在本文中，我们提出了一种称为协作世界模型（CoWorld）的转移学习方法，以改善离线条件下视觉RL的性能。其核心想法是使用易于交互、现成的模拟器来训练辅助RL模型作为离线策略在目标域中学习的在线“测试床”，这为价值函数提供了灵活的约束——直观地说，我们想在不妨碍具有潜在优势的动作探索的情况下缓解离线数据分布之外的价值函数过度估计问题。具体而言，CoWorld执行域协作表示学习以弥合在线和离线隐藏状态分布之间的差距。此外，它执行域协作行为学习，使在离线数据集外的智能体能够学习在线行为策略。

    Training visual reinforcement learning (RL) models in offline datasets is challenging due to overfitting issues in representation learning and overestimation problems in value function. In this paper, we propose a transfer learning method called Collaborative World Models (CoWorld) to improve the performance of visual RL under offline conditions. The core idea is to use an easy-to-interact, off-the-shelf simulator to train an auxiliary RL model as the online "test bed" for the offline policy learned in the target domain, which provides a flexible constraint for the value function -- Intuitively, we want to mitigate the overestimation problem of value functions outside the offline data distribution without impeding the exploration of actions with potential advantages. Specifically, CoWorld performs domain-collaborative representation learning to bridge the gap between online and offline hidden state distributions. Furthermore, it performs domain-collaborative behavior learning that enab
    
[^130]: 最优低秩矩阵填补：半定松弛和特征向量分离

    Optimal Low-Rank Matrix Completion: Semidefinite Relaxations and Eigenvector Disjunctions. (arXiv:2305.12292v1 [cs.LG])

    [http://arxiv.org/abs/2305.12292](http://arxiv.org/abs/2305.12292)

    该论文通过重新表述低秩矩阵填补问题为投影矩阵的非凸问题，实现了能够确定最优解的分离分支定界方案，并且通过新颖和紧密的凸松弛方法，使得最优性差距相对于现有方法减少了两个数量级。

    

    低秩矩阵填补的目的是计算一个复杂度最小的矩阵，以尽可能准确地恢复给定的一组观测数据，并且具有众多应用，如产品推荐。不幸的是，现有的解决低秩矩阵填补的方法是启发式的，虽然高度可扩展并且通常能够确定高质量的解决方案，但不具备任何最优性保证。我们通过将低秩问题重新表述为投影矩阵的非凸问题，并实现一种分离分支定界方案来重新审视矩阵填补问题，以实现最优性导向。此外，我们通过将低秩矩阵分解为一组秩一矩阵的和，并通过 Shor 松弛来激励每个秩一矩阵中的每个 2*2 小矩阵的行列式为零，从而推导出一种新颖且通常很紧的凸松弛类。在数值实验中，相对于最先进的启发式方法，我们的新凸松弛方法将最优性差距减少了两个数量级。

    Low-rank matrix completion consists of computing a matrix of minimal complexity that recovers a given set of observations as accurately as possible, and has numerous applications such as product recommendation. Unfortunately, existing methods for solving low-rank matrix completion are heuristics that, while highly scalable and often identifying high-quality solutions, do not possess any optimality guarantees. We reexamine matrix completion with an optimality-oriented eye, by reformulating low-rank problems as convex problems over the non-convex set of projection matrices and implementing a disjunctive branch-and-bound scheme that solves them to certifiable optimality. Further, we derive a novel and often tight class of convex relaxations by decomposing a low-rank matrix as a sum of rank-one matrices and incentivizing, via a Shor relaxation, that each two-by-two minor in each rank-one matrix has determinant zero. In numerical experiments, our new convex relaxations decrease the optimali
    
[^131]: 带有有限注释的分割任务中的期望最大化伪标签方法研究

    Expectation Maximization Pseudo Labelling for Segmentation with Limited Annotations. (arXiv:2305.01747v1 [cs.CV])

    [http://arxiv.org/abs/2305.01747](http://arxiv.org/abs/2305.01747)

    本文提出了一种伪标签的泛化方法，称为贝叶斯伪标签，在半监督医学图像分割任务中应用效果良好。

    

    本文研究了半监督医学图像分割中的伪标签及其推广，伪标签通过利用未标记数据的原始推断作为自训练的伪标签，在半监督学习中取得了巨大的实证成功。我们建立了伪标签和期望最大化算法之间的联系，部分解释了其实证成功。在此基础上，我们展示了贝叶斯原理下伪标签的完全泛化，称为贝叶斯伪标签。然后，我们提供了一种变分方法来学习逼近贝叶斯伪标签，通过学习选择高质量伪标签的阈值。接下来，我们在医学图像分割的半监督学习中展示了伪标签和其推广贝叶斯伪标签的应用。

    We study pseudo labelling and its generalisation for semi-supervised segmentation of medical images. Pseudo labelling has achieved great empirical successes in semi-supervised learning, by utilising raw inferences on unlabelled data as pseudo labels for self-training. In our paper, we build a connection between pseudo labelling and the Expectation Maximization algorithm which partially explains its empirical successes. We thereby realise that the original pseudo labelling is an empirical estimation of its underlying full formulation. Following this insight, we demonstrate the full generalisation of pseudo labels under Bayes' principle, called Bayesian Pseudo Labels. We then provide a variational approach to learn to approximate Bayesian Pseudo Labels, by learning a threshold to select good quality pseudo labels. In the rest of the paper, we demonstrate the applications of Pseudo Labelling and its generalisation Bayesian Psuedo Labelling in semi-supervised segmentation of medical images
    
[^132]: 通过逆向神经渲染对动态场景进行物体中心体素化

    Object-Centric Voxelization of Dynamic Scenes via Inverse Neural Rendering. (arXiv:2305.00393v1 [cs.CV])

    [http://arxiv.org/abs/2305.00393](http://arxiv.org/abs/2305.00393)

    本文提出了一个逆向神经渲染框架DynaVol，可以在多实体动态场景中学习时间变化的体积表示，通过维护一个时间依赖的3D格点和联合学习格点级局部动态、物体级全局动态和组合神经辐射场来增强物体中心场景体素化的时空一致性。

    

    在无监督的3D场景中理解世界的组成动态非常具有挑战性。现有的方法要么未能有效利用时间线索，要么忽略了场景分解的多视角一致性。本文提出了DynaVol，一种逆向神经渲染框架，为多实体（如物体）的动态场景学习时间变化的体积表示提供了一个学习方法。它的主要贡献有两个。首先，它维护一个时间依赖的3D格点，动态而灵活地将空间位置绑定到不同的实体，从而在代表性水平上鼓励信息的分离。其次，我们的方法在端到端架构中联合学习格点级局部动态、物体级全局动态和组合神经辐射场，从而增强了物体中心场景体素化的时空一致性。我们提出了一个两阶段的DynaVol训练方案，并在合成和真实世界数据集上验证了它的有效性。

    Understanding the compositional dynamics of the world in unsupervised 3D scenarios is challenging. Existing approaches either fail to make effective use of time cues or ignore the multi-view consistency of scene decomposition. In this paper, we propose DynaVol, an inverse neural rendering framework that provides a pilot study for learning time-varying volumetric representations for dynamic scenes with multiple entities (like objects). It has two main contributions. First, it maintains a time-dependent 3D grid, which dynamically and flexibly binds the spatial locations to different entities, thus encouraging the separation of information at a representational level. Second, our approach jointly learns grid-level local dynamics, object-level global dynamics, and the compositional neural radiance fields in an end-to-end architecture, thereby enhancing the spatiotemporal consistency of object-centric scene voxelization. We present a two-stage training scheme for DynaVol and validate its ef
    
[^133]: 扰动有助于降低投资风险吗？ 基于分离变分对抗训练的风险感知型股票推荐方法

    Can Perturbations Help Reduce Investment Risks? Risk-Aware Stock Recommendation via Split Variational Adversarial Training. (arXiv:2304.11043v1 [q-fin.RM])

    [http://arxiv.org/abs/2304.11043](http://arxiv.org/abs/2304.11043)

    本文提出了一种基于分离变分对抗训练的风险感知型股票推荐方法，通过对抗性扰动提高模型对于风险的感知能力，通过变分扰动生成器模拟不同的风险因素并生成代表性的风险指标对抗样本。在真实股票数据上进行的实验表明该方法有效降低了投资风险同时保持高预期收益。

    

    在股票市场，成功的投资需要在利润和风险之间取得良好的平衡。最近，在量化投资中广泛研究了股票推荐，以为投资者选择具有更高收益率的股票。尽管在获利方面取得了成功，但大多数现有的推荐方法仍然在风险控制方面较弱，这可能导致实际股票投资中难以承受的亏损。为了有效降低风险，我们从对抗性扰动中获得启示，并提出了一种新的基于分离变分对抗训练（SVAT）框架的风险感知型股票推荐方法。本质上，SVAT鼓励模型对风险股票样本的对抗性扰动敏感，并通过学习扰动来增强模型的风险意识。为了生成代表性的风险指标对抗样本，我们设计了一个变分扰动生成器来模拟不同的风险因素。特别地，变分结构使我们的方法能够捕捉难以明确量化和建模的各种风险因素。在真实股票数据上的综合实验表明，SVAT在降低投资风险的同时保持高预期收益上非常有效。

    In the stock market, a successful investment requires a good balance between profits and risks. Recently, stock recommendation has been widely studied in quantitative investment to select stocks with higher return ratios for investors. Despite the success in making profits, most existing recommendation approaches are still weak in risk control, which may lead to intolerable paper losses in practical stock investing. To effectively reduce risks, we draw inspiration from adversarial perturbations and propose a novel Split Variational Adversarial Training (SVAT) framework for risk-aware stock recommendation. Essentially, SVAT encourages the model to be sensitive to adversarial perturbations of risky stock examples and enhances the model's risk awareness by learning from perturbations. To generate representative adversarial examples as risk indicators, we devise a variational perturbation generator to model diverse risk factors. Particularly, the variational architecture enables our method
    
[^134]: 二元积分布的多项式时间和纯差分隐私估计器

    A Polynomial Time, Pure Differentially Private Estimator for Binary Product Distributions. (arXiv:2304.06787v1 [cs.DS])

    [http://arxiv.org/abs/2304.06787](http://arxiv.org/abs/2304.06787)

    本论文提出了第一个多项式时间、纯差分隐私估计器，可以在$\{0,1\}^d$上准确估计二元积分布的均值，达到了最优的样本复杂度。

    

    我们提出了第一个ε-差分隐私、计算有效的算法，可以在总变化距离下准确地估计$\{0,1\}^d$上的乘积分布的均值，同时在多项式对数因子内获得了最优的样本复杂度。之前的工作要么在更弱的隐私概念下有效地解决了这个问题，要么在指数级运行时间内最优地解决了这个问题。

    We present the first $\varepsilon$-differentially private, computationally efficient algorithm that estimates the means of product distributions over $\{0,1\}^d$ accurately in total-variation distance, whilst attaining the optimal sample complexity to within polylogarithmic factors. The prior work had either solved this problem efficiently and optimally under weaker notions of privacy, or had solved it optimally while having exponential running times.
    
[^135]: 线性卷积网络的函数空间和临界点

    Function Space and Critical Points of Linear Convolutional Networks. (arXiv:2304.05752v1 [cs.LG])

    [http://arxiv.org/abs/2304.05752](http://arxiv.org/abs/2304.05752)

    研究了具有一维卷积层的线性网络的函数空间，分析了网络架构对函数空间的影响并证明了对于步幅大于一且数据一般的架构，该优化问题的非零临界点是函数空间的平滑内部点。

    

    我们研究了具有一维卷积层的线性网络的几何结构。这些网络的函数空间可以被认为是具有稀疏因子分解的半代数多项式族。我们分析了网络架构对函数空间的维度、边界和奇异点的影响。我们还描述了网络参数化映射的临界点。此外，我们研究了使用平方误差损失训练网络的优化问题。我们证明了对于所有步幅大于一且数据一般的架构，该优化问题的非零临界点是函数空间的平滑内部点。对于稠密的线性网络和步幅为一的线性卷积网络，这种特性被认为是错误的。

    We study the geometry of linear networks with one-dimensional convolutional layers. The function spaces of these networks can be identified with semi-algebraic families of polynomials admitting sparse factorizations. We analyze the impact of the network's architecture on the function space's dimension, boundary, and singular points. We also describe the critical points of the network's parameterization map. Furthermore, we study the optimization problem of training a network with the squared error loss. We prove that for architectures where all strides are larger than one and generic data, the non-zero critical points of that optimization problem are smooth interior points of the function space. This property is known to be false for dense linear networks and linear convolutional networks with stride one.
    
[^136]: 使用流引导的密度比学习进行生成建模

    Generative Modeling with Flow-Guided Density Ratio Learning. (arXiv:2303.03714v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.03714](http://arxiv.org/abs/2303.03714)

    FDRL是一种基于流引导的密度比学习的简单且可扩展的生成建模方法，通过训练密度比估计器从逐渐改进的样本中学习，缓解了密度鸿沟问题，并在生成高尺寸图像上表现优于现有基线方法。

    

    我们提出了一种简单且可扩展的生成建模方法，称为流引导的密度比学习（FDRL）。该方法基于DGflow中引入的基于熵正则化f-散度的梯度流的过时（时间无关）近似，并且通过GAN鉴别器给出的过时估计器近似了不可计算的时间相关密度比。在样本细化的情况下，这种近似足够，因为流的源分布和目标分布是相近的。然而，在生成的情况下，这个假设是无效的，而且过时估计器的朴素应用由于两个分布之间的大鸿沟而失败。FDRL提出了训练密度比估计器的方法，使其在训练过程中从逐渐改进的样本中学习。我们展示了这种简单的方法缓解了密度鸿沟问题，使得FDRL能够生成高达$128\times128$尺寸的图像，并且在质量上超过了现有的梯度流基线方法。

    We present Flow-Guided Density Ratio Learning (FDRL), a simple and scalable approach to generative modeling which builds on the stale (time-independent) approximation of the gradient flow of entropy-regularized f-divergences introduced in DGflow. In DGflow, the intractable time-dependent density ratio is approximated by a stale estimator given by a GAN discriminator. This is sufficient in the case of sample refinement, where the source and target distributions of the flow are close to each other. However, this assumption is invalid for generation and a naive application of the stale estimator fails due to the large chasm between the two distributions. FDRL proposes to train a density ratio estimator such that it learns from progressively improving samples during the training process. We show that this simple method alleviates the density chasm problem, allowing FDRL to generate images of dimensions as high as $128\times128$, as well as outperform existing gradient flow baselines on qua
    
[^137]: 通过鉴别性正则化来实现表征解耦

    Representation Disentaglement via Regularization by Identification. (arXiv:2303.00128v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00128](http://arxiv.org/abs/2303.00128)

    本文研究了从观测数据中学习解耦表示的问题，提出通过鉴别性正则化来实现表征解耦，解决了现代深度表征学习模型中出现的纠缠偏差行为问题。

    

    本文研究了从观测数据中学习解耦表示的问题。给定从$p(\mathbf{x}|\mathbf{y})$中生成的具有各自生成变量$\mathbf{y}_c$分解的分布$p(\mathbf{y}) = \prod_{c} p(\mathbf{y}_c )$的观测值${\mathbf{x}^{(i)}}$，我们尝试学习与每个$c$的后验分布$p(\mathbf{z}| \mathbf{x}, \hat{\mathbf{y}}_c)$匹配的解耦表示是否可行。我们认为现代深度表征学习模型无法解决与生成变量之间出现的纠缠偏差行为问题，这种行为上产生偏见。在因果推理的框架下，我们证明了可以在可识别性的条件下对这个问题进行解释和调和，这一点可以在监督或弱监督的条件下实现。因此，我们提出了一种通过鉴别性正则化（ReI）的模块化重新调整方法。

    This work focuses on the problem of learning disentangled representations from observational data. Given observations ${\mathbf{x}^{(i)}}$ for $i=1,...,N $ drawn from $p(\mathbf{x}|\mathbf{y})$ with generative variables $\mathbf{y}$ admitting the distribution factorization $p(\mathbf{y}) = \prod_{c} p(\mathbf{y}_c )$, we ask whether learning disentangled representations matching the space of observations with identification guarantees on the posterior $p(\mathbf{z}| \mathbf{x}, \hat{\mathbf{y}}_c)$ for each $c$, is plausible. We argue modern deep representation learning models of data matching the distributed factorization property are ill-posed with collider bias behaviour; a source of bias producing entanglement between generating variables. Under the rubric of causality, we show this issue can be explained and reconciled under the condition of identifiability; attainable under supervision or a weak-form of it. For this, we propose regularization by identification (ReI), a modular re
    
[^138]: 双重强化学习：强化学习和模仿学习的统一和新方法

    Dual RL: Unification and New Methods for Reinforcement and Imitation Learning. (arXiv:2302.08560v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08560](http://arxiv.org/abs/2302.08560)

    这篇论文介绍了双重强化学习的概念，并在一个统一的框架下解释了几种最新深度强化学习算法及模仿学习方法。作者提出了双重模仿学习方法（DIL）直接最小化策略之间的距离，并提出了一种新的离线演员-评论家方法。

    

    强化学习的目标是最大化期望累积回报。研究表明，这个目标可以通过在线性约束下优化状态-动作访问分布的优化问题来表示。这个表述的对偶问题，我们称之为双重强化学习，是无约束的并且更容易优化。我们展示了几个最先进的离线和在线强化学习算法以及模仿学习可以在一个统一的框架下被视为双重强化学习方法。这种统一提供了一个共同的基础，可以研究和识别这些方法成功的构成部分，并揭示这些方法的共同缺点和改进的新见解。我们的分析表明，以前的离线模仿学习方法基于一个不现实的覆盖率假设，并最小化了学习代理和专家访问分布之间的特定f-分布。我们提出的双重模仿学习方法（DIL）直接最小化策略之间的距离。在同样的双重框架下，我们还提出了一种新的离线演员-评论家方法，对几个基准任务有效。

    The goal of reinforcement learning (RL) is to maximize the expected cumulative return. It has been shown that this objective can be represented by an optimization problem of the state-action visitation distribution under linear constraints. The dual problem of this formulation, which we refer to as dual RL, is unconstrained and easier to optimize. We show that several state-of-the-art off-policy deep reinforcement learning (RL) algorithms, under both online and offline, RL and imitation learning (IL) settings, can be viewed as dual RL approaches in a unified framework. This unification provides a common ground to study and identify the components that contribute to the success of these methods and also reveals the common shortcomings across methods with new insights for improvement. Our analysis shows that prior off-policy imitation learning methods are based on an unrealistic coverage assumption and are minimizing a particular f-divergence between the visitation distributions of the l
    
[^139]: 因果图中前门调整的线性时间算法

    Linear-Time Algorithms for Front-Door Adjustment in Causal Graphs. (arXiv:2211.16468v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.16468](http://arxiv.org/abs/2211.16468)

    在因果图中，提出了解决前门调整的线性时间算法，通过观察到的中介变量，即使存在未观测到的混淆，也可以识别因果效应。

    

    从观测数据中估计因果效应是实证科学中的基本任务。当系统中涉及未观察到的混淆因素时，这变得尤为具有挑战性。本文侧重于前门调整——一种经典技术，它使用观察到的中介变量，即使存在未观测到的混淆，也可以识别因果效应。虽然前门估计的统计特性已经很好地理解了，但它的算法方面长期以来一直未得到探究。最近，Jeong，Tian和Barenboim [NeurIPS 2022]提出了一种第一个多项式时间算法，用于在给定的有向无环图（DAG）中找到满足前门准则的集合，其运行时间为$O（n^3（n+m））$，其中$n$表示变量的数量，$m$表示因果图的边的数量。在我们的工作中，我们提供了第一个具有线性时间复杂度的算法，即$O（n+m）$，用于这项任务，从而达到了渐近最优的时间复杂性。

    Causal effect estimation from observational data is a fundamental task in empirical sciences. It becomes particularly challenging when unobserved confounders are involved in a system. This paper focuses on front-door adjustment -- a classic technique which, using observed mediators allows to identify causal effects even in the presence of unobserved confounding. While the statistical properties of the front-door estimation are quite well understood, its algorithmic aspects remained unexplored for a long time. Recently, Jeong, Tian, and Barenboim [NeurIPS 2022] have presented the first polynomial-time algorithm for finding sets satisfying the front-door criterion in a given directed acyclic graph (DAG), with an $O(n^3(n+m))$ run time, where $n$ denotes the number of variables and $m$ the number of edges of the causal graph. In our work, we give the first linear-time, i.e., $O(n+m)$, algorithm for this task, which thus reaches the asymptotically optimal time complexity. This result impli
    
[^140]: 一种利用集合建模、注释的归一化分布和熵测量的不精确、众包数据集的方法

    An Approach for Noisy, Crowdsourced Datasets Utilizing Ensemble Modeling, Normalized Distributions of Annotations, and Entropic Measures of Uncertainty. (arXiv:2210.16380v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.16380](http://arxiv.org/abs/2210.16380)

    在众包图像数据集上的分类是具有挑战性的。本文提出了一种利用集合建模、注释的归一化分布和熵测量的方法，以帮助识别标签不确定的图像，并量化这些样本的可信度。

    

    在不精确的、众包的图像数据集上进行分类对于最好的神经网络来说都是具有挑战性的。两个问题使得这类数据集上的问题更加复杂，即类别不平衡和标签的不确定性。AL-ALL和AL-PUB数据集--包含来自古希腊纸草的图像的紧密裁剪的单个字符--受到这两个问题的严重影响。将集合建模应用于这样的数据集可以帮助识别标签不确定的图像，并量化这些样本的可信度。因此，我们应用由几乎相同的ResNets组成的堆叠泛化，它们具有不同的损失函数：一个利用稀疏交叉熵（CXE），另一个利用Kullback-Liebler散度（KLD）。两个网络都使用从众包一致性中得出的标签。对于第二个网络，KLD是相对于所提出的注释的归一化分布（NDA）计算的。对于我们的集合模型，我们应用k-近邻算法。

    Performing classification on noisy, crowdsourced image datasets can prove challenging even for the best neural networks. Two issues which complicate the problem on such datasets are class imbalance and ground-truth uncertainty in labeling. The AL-ALL and AL-PUB datasets -- consisting of tightly cropped, individual characters from images of ancient Greek papyri -- are strongly affected by both issues. The application of ensemble modeling to such datasets can help identify images where the ground-truth is questionable and quantify the trustworthiness of those samples. As such, we apply stacked generalization consisting of nearly identical ResNets with different loss functions: one utilizing sparse cross-entropy (CXE) and the other Kullback-Liebler Divergence (KLD). Both networks use labels drawn from the crowdsourced consensus. For the second network, the KLD is calculated with respect to the proposed Normalized Distribution of Annotations (NDA). For our ensemble model, we apply a k-near
    
[^141]: 单时间尺度演员-评论家法的有限时间分析

    Finite-time analysis of single-timescale actor-critic. (arXiv:2210.09921v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.09921](http://arxiv.org/abs/2210.09921)

    这项研究提出了一种在线单时间尺度演员-评论家方法，通过线性函数逼近和马尔可夫样本更新，在连续状态空间中找到了一个$\epsilon$-近似的稳定点，并且在样本复杂度为$\widetilde{\mathcal{O}}(\epsilon^{-2})$的情况下证明了其收敛性。

    

    在许多具有挑战性的应用中，演员-评论家方法取得了显着的成功。然而，在最实际的单时间尺度形式下，其有限时间收敛性仍然不够理解。现有的单时间尺度演员-评论家分析工作仅限于简化的i.i.d.采样或表格设置。我们研究了更实际的在线单时间尺度演员-评论家算法，该算法在连续状态空间中，评论家采用线性函数逼近，并在每个演员步骤中使用单个马尔可夫样本进行更新。先前的分析无法在这种具有挑战性的场景中实现收敛。我们证明，在标准假设下，在线单时间尺度演员-评论家方法能够在样本复杂度为$\widetilde{\mathcal{O}}(\epsilon^{-2})$的情况下找到一个$\epsilon$-近似的稳定点，而在i.i.d.采样下，这个复杂度可以进一步改进为$\mathcal{O}(\epsilon^{-2})$。我们的新框架系统地评估了一个

    Actor-critic methods have achieved significant success in many challenging applications. However, its finite-time convergence is still poorly understood in the most practical single-timescale form. Existing works on analyzing single-timescale actor-critic have been limited to i.i.d. sampling or tabular setting for simplicity. We investigate the more practical online single-timescale actor-critic algorithm on continuous state space, where the critic assumes linear function approximation and updates with a single Markovian sample per actor step. Previous analysis has been unable to establish the convergence for such a challenging scenario. We demonstrate that the online single-timescale actor-critic method provably finds an $\epsilon$-approximate stationary point with $\widetilde{\mathcal{O}}(\epsilon^{-2})$ sample complexity under standard assumptions, which can be further improved to $\mathcal{O}(\epsilon^{-2})$ under the i.i.d. sampling. Our novel framework systematically evaluates an
    
[^142]: 机器学习中的签名方法

    Signature Methods in Machine Learning. (arXiv:2206.14674v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.14674](http://arxiv.org/abs/2206.14674)

    本综述介绍了机器学习中应用的签名方法，通过数学洞察力理解复杂的流式数据之间的交互，并提供了用于分析非规则、非平稳的流式数据的数值方法。

    

    基于签名的技术为理解复杂的流式数据之间的相互作用提供了数学洞察力。这些洞察力可以很自然地转化为理解流式数据的数值方法，也许是因为它们具有数学的精确性，它们在分析非规则、非平稳的流式数据以及数据维度和样本大小都适中的情况下表现出了很有用的性质。对于理解流式多模态数据是指数级的问题：长度为$n$的字母串，来自大小为$d$的字母表，可以是$d^n$种不同的消息。签名消除了由于采样不规则性而产生的指数级的噪声，但仍然存在指数级的信息量。本综述旨在保持在可以直接管理这种指数级缩放的领域内。可扩展性问题是许多问题中的一个重要挑战，但需要另一篇综述文章和进一步的思路。本综述描述了一系列上下文。

    Signature-based techniques give mathematical insight into the interactions between complex streams of evolving data. These insights can be quite naturally translated into numerical approaches to understanding streamed data, and perhaps because of their mathematical precision, have proved useful in analysing streamed data in situations where the data is irregular, and not stationary, and the dimension of the data and the sample sizes are both moderate. Understanding streamed multi-modal data is exponential: a word in $n$ letters from an alphabet of size $d$ can be any one of $d^n$ messages. Signatures remove the exponential amount of noise that arises from sampling irregularity, but an exponential amount of information still remain. This survey aims to stay in the domain where that exponential scaling can be managed directly. Scalability issues are an important challenge in many problems but would require another survey article and further ideas. This survey describes a range of context
    
[^143]: 修改公平的集群编辑

    Modification-Fair Cluster Editing. (arXiv:2112.03183v2 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2112.03183](http://arxiv.org/abs/2112.03183)

    这个论文提出了一种修改公平的集群编辑问题，解决了在顶点着色图中标准算法可能产生的偏向子组的解决方案。具体而言，在双色图中，问题是 NP-hard 的，即使只允许在子组内部插入边缘。

    

    经典的集群编辑问题（也称为相关聚类）要求通过少量的边缘修改将给定的图形转换为不相交的团（集群）。当应用于顶点着色图时（颜色表示子组），对于 NP-hard 的集群编辑问题的标准算法可能会产生偏向数据子组（例如，人口统计学组）的解决方案，这种偏向是通过与子组成员相关的修改数量来衡量的。我们提出了修改公平约束，确保每个子组的编辑数量与其大小成比例。首先，我们研究了具有两种顶点颜色的图形的修改公平集群编辑问题。我们证明了即使只允许在子组内部插入边缘，该问题仍然是 NP-hard 的；需要注意的是，在经典的“非公平”设置中，此情况可以轻松在多项式时间内解决。然而，在更一般的编辑形式中，修改公平的变体仍然是固定的。

    The classic Cluster Editing problem (also known as Correlation Clustering) asks to transform a given graph into a disjoint union of cliques (clusters) by a small number of edge modifications. When applied to vertex-colored graphs (the colors representing subgroups), standard algorithms for the NP-hard Cluster Editing problem may yield solutions that are biased towards subgroups of data (e.g., demographic groups), measured in the number of modifications incident to the members of the subgroups. We propose a modification fairness constraint which ensures that the number of edits incident to each subgroup is proportional to its size. To start with, we study Modification-Fair Cluster Editing for graphs with two vertex colors. We show that the problem is NP-hard even if one may only insert edges within a subgroup; note that in the classic "non-fair" setting, this case is trivially polynomial-time solvable. However, in the more general editing form, the modification-fair variant remains fixe
    
[^144]: 关于最小化器和卷积滤波器的理论连接及其在基因组分析中的应用

    On minimizers and convolutional filters: theoretical connections and applications to genome analysis. (arXiv:2111.08452v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.08452](http://arxiv.org/abs/2111.08452)

    该论文通过对哈希函数属性进行数学分析，发现在分类字母表上的序列分析中，使用随机高斯初始化的卷积滤波器和最大池化等价于选择一种最小化器排序，能够有效提取与其他最小化器距离较近但与序列中的k-mer相距较远的重要特征。

    

    最小化器和卷积神经网络(CNN)是两种完全不同的流行技术，均被用于分析生物序列。从表面上看，这些方法似乎完全不同。最小化器使用滚动窗口的最小哈希方法提取每个窗口中的一个重要k-mer特征。CNN则以随机初始化的卷积滤波器和池化操作为基础，通过多个神经层来学习滤波器本身及其用于分类序列的方法。本文主要结果是对哈希函数属性进行了仔细的数学分析，显示对于分类字母表上的序列，使用随机高斯初始化的卷积滤波器和最大池化等价于选择一个最小化器排序，使得选择的k-mer与序列中的k-mer（按汉明距离）相距较远，但与其他最小化器相距较近。在实证实验中，我们发现这种方法能够有效降低计算复杂度并与传统方法具有相当的性能。

    Minimizers and convolutional neural networks (CNNs) are two quite distinct popular techniques that have both been employed to analyze categorical biological sequences. At face value, the methods seem entirely dissimilar. Minimizers use min-wise hashing on a rolling window to extract a single important k-mer feature per window. CNNs start with a wide array of randomly initialized convolutional filters, paired with a pooling operation, and then multiple additional neural layers to learn both the filters themselves and how they can be used to classify the sequence.  Here, our main result is a careful mathematical analysis of hash function properties showing that for sequences over a categorical alphabet, random Gaussian initialization of convolutional filters with max-pooling is equivalent to choosing a minimizer ordering such that selected k-mers are (in Hamming distance) far from the k-mers within the sequence but close to other minimizers. In empirical experiments, we find that this pr
    
[^145]: 基于邻域选择方法的高维函数图模型结构学习

    High-dimensional Functional Graphical Model Structure Learning via Neighborhood Selection Approach. (arXiv:2105.02487v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2105.02487](http://arxiv.org/abs/2105.02487)

    该论文提出了一种基于邻域选择方法的高维函数图模型结构学习方法，通过函数对函数回归估计节点邻域，然后结合这些估计的邻域恢复整个图结构，从而直接估计条件独立结构。

    

    无向图模型广泛用于建模向量值数据的条件独立结构。然而，在许多现代应用中，例如涉及EEG和fMRI数据的应用中，观测更适合被建模为多变量随机函数而不是向量。已经提出了函数图模型来建模这种函数数据的条件独立结构。我们提出了一种邻域选择方法来估计高斯函数图模型的结构，首先通过函数对函数回归估计每个节点的邻域，然后通过组合估计的邻域恢复整个图结构。我们的方法仅需要对随机函数的条件分布进行假设，并直接估计条件独立结构。因此，我们避免了对可能不存在的精度算子进行明确定义的需要，尤其是当函数具有无限维时。

    Undirected graphical models are widely used to model the conditional independence structure of vector-valued data. However, in many modern applications, for example those involving EEG and fMRI data, observations are more appropriately modeled as multivariate random functions rather than vectors. Functional graphical models have been proposed to model the conditional independence structure of such functional data. We propose a neighborhood selection approach to estimate the structure of Gaussian functional graphical models, where we first estimate the neighborhood of each node via a function-on-function regression and subsequently recover the entire graph structure by combining the estimated neighborhoods. Our approach only requires assumptions on the conditional distributions of random functions, and we estimate the conditional independence structure directly. We thus circumvent the need for a well-defined precision operator that may not exist when the functions are infinite dimension
    
[^146]: 关于非线性滑动视角控制稳定性的几何视角研究

    On the Stability of Nonlinear Receding Horizon Control: A Geometric Perspective. (arXiv:2103.15010v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2103.15010](http://arxiv.org/abs/2103.15010)

    本文研究了非线性滑动视角控制的稳定性，特别关注反馈线性化系统，并证明了在一定条件下，RHC的一阶解可以使可线性化系统指数级稳定。

    

    非线性滑动视角控制（RHC）策略的广泛应用已经促使人们在这些方法的稳定性保证方面进行了30多年的研究。然而，目前的理论保证要求每个（通常非凸）规划问题都可以被求解为（近似）全局最优解，这对于实际实施RHC的基于导数的局部优化方法来说是不现实的要求。本文在内部规划问题被解决为一阶稳定点而不一定是全局最优解时，首次探索了非线性RHC的稳定性保证。特别关注反馈线性化系统，并提供了正面和负面结果的混合。我们证明，在某些强条件下，RHC的一阶解可以指数级地稳定可线性化系统。令人惊讶的是，这些条件...

    %!TEX root = LCSS_main_max.tex  The widespread adoption of nonlinear Receding Horizon Control (RHC) strategies by industry has led to more than 30 years of intense research efforts to provide stability guarantees for these methods. However, current theoretical guarantees require that each (generally nonconvex) planning problem can be solved to (approximate) global optimality, which is an unrealistic requirement for the derivative-based local optimization methods generally used in practical implementations of RHC. This paper takes the first step towards understanding stability guarantees for nonlinear RHC when the inner planning problem is solved to first-order stationary points, but not necessarily global optima. Special attention is given to feedback linearizable systems, and a mixture of positive and negative results are provided. We establish that, under certain strong conditions, first-order solutions to RHC exponentially stabilize linearizable systems. Surprisingly, these conditio
    

