# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [SoftZoo: A Soft Robot Co-design Benchmark For Locomotion In Diverse Environments.](http://arxiv.org/abs/2303.09555) | SoftZoo是一个面向多种环境的软体机器人协同设计平台，支持广泛、自然启发的材料组合和多种任务，并提供了形态和控制的可微分设计表示。通过一致的评估指标，SoftZoo允许直接比较不同方法，并揭示了软体机器人设计中的权衡。 |
| [^2] | [Dataflow graphs as complete causal graphs.](http://arxiv.org/abs/2303.09552) | 本篇论文介绍了一种替代的软件设计方法——基于流的编程（FBP），并强调了FBP生成的数据流图和结构性因果模型之间的联系，通过这种联系可以改进软件项目中的日常任务，包括故障定位、业务分析和实验室实验。 |
| [^3] | [WebSHAP: Towards Explaining Any Machine Learning Models Anywhere.](http://arxiv.org/abs/2303.09545) | WebSHAP是第一个将先进的模型无关解释技术SHAP适应Web环境的浏览器内工具，可用于解释基于ML的贷款批准决策。 |
| [^4] | [SemDeDup: Data-efficient learning at web-scale through semantic deduplication.](http://arxiv.org/abs/2303.09540) | SemDeDup是一种利用预训练模型的嵌入来识别和删除语义重复项的方法。通过对LAION的子集进行分析，SemDeDup可以最小化性能损失的同时删除50%的数据，实际上将训练时间减半。此外，SemDeDup在提供效率收益的同时改进了先前的方法。 |
| [^5] | [Deep Metric Learning for Unsupervised Remote Sensing Change Detection.](http://arxiv.org/abs/2303.09536) | 提出一个基于深度度量学习的无监督CD方法，利用两个相互连接的深度网络（D-CPG和D-FE）来更好地预测变化和提取用于变化检测的特征，并在各种具有不同设置的数据集中优于几种最先进的无监督CD方法以及有监督的方法。 |
| [^6] | [Variational Principles for Mirror Descent and Mirror Langevin Dynamics.](http://arxiv.org/abs/2303.09532) | 本文提出了镜像下降和镜像 Langevin 动力学的变分形式，表明镜像下降作为某个最优控制问题的闭环解决方案出现，并给出了 Bellman 值函数的表达式。 |
| [^7] | [Tackling Clutter in Radar Data -- Label Generation and Detection Using PointNet++.](http://arxiv.org/abs/2303.09530) | 本文提出了两种神经网络来识别雷达数据中的杂波，并设计了一种方法来自动生成杂波标签。对现有数据进行评估，显示出比现有方法更好的性能。同时发布第一个表示实际驾驶场景的自由可用雷达杂波数据集。 |
| [^8] | [Fairness-aware Differentially Private Collaborative Filtering.](http://arxiv.org/abs/2303.09527) | 本文提出了DP-Fair，一个两阶段的协同过滤算法框架，它结合了差分隐私机制和公平约束，旨在保护用户隐私、确保公平推荐。 |
| [^9] | [PyVBMC: Efficient Bayesian inference in Python.](http://arxiv.org/abs/2303.09519) | PyVBMC是一种高效的Python工具，用于黑盒计算模型的贝叶斯推断和模型选择，可以处理连续参数不超过约10-15个的计算或统计模型。 |
| [^10] | [Gate Recurrent Unit Network based on Hilbert-Schmidt Independence Criterion for State-of-Health Estimation.](http://arxiv.org/abs/2303.09497) | 该论文提出一种基于Hilbert-Schmidt独立准则的GRU模型，能够很好地解决电池健康状态评估中存在的数据长度等问题，并通过提取最相关和独立的特征能够极大提高准确性和鲁棒性。 |
| [^11] | [Challenges and Opportunities in Quantum Machine Learning.](http://arxiv.org/abs/2303.09491) | 量子机器学习（QML）是机器学习和量子计算交叉领域中的前沿研究方向，具有应用于量子材料、生物化学和高能物理等领域的潜力，但其模型的可训练性仍有挑战，需要进一步解决。本文回顾了当前QML方法与应用，并探讨了QML的量子优势机会。 |
| [^12] | [Effectively Modeling Time Series with Simple Discrete State Spaces.](http://arxiv.org/abs/2303.09489) | 介绍了一种名为SpaceTime的状态空间时间序列架构，提出了基于伴随矩阵的新的SSM参数化方法，能够学习自回归过程，有效预测远期。 |
| [^13] | [A Novel Autoencoders-LSTM Model for Stroke Outcome Prediction using Multimodal MRI Data.](http://arxiv.org/abs/2303.09484) | 本研究提出了一种新的机器学习模型用于使用多模态MRI进行卒中预测，该模型融合了多个MRI模态和压缩的多模态特征，表现出比现有的最先进方法更好的效果。 |
| [^14] | [Achieving a Better Stability-Plasticity Trade-off via Auxiliary Networks in Continual Learning.](http://arxiv.org/abs/2303.09483) | 本文提出了一种辅助网络持续学习方法（ANCL），通过对流信息的控制，自然插值可塑性和稳定性之间的差异，有助于在神经网络中实现更好的稳定性-可塑性平衡。 |
| [^15] | [Arbitrary Order Meta-Learning with Simple Population-Based Evolution.](http://arxiv.org/abs/2303.09478) | 本文提出了一种简单的基于群体进化的元学习方法，可以隐式地优化任意高阶的元参数，从而加速学习。 |
| [^16] | [Gradient flow on extensive-rank positive semi-definite matrix denoising.](http://arxiv.org/abs/2303.09474) | 本文提出了一种新的方法来分析广义秩和高维情况下正半定矩阵去噪问题的梯度流，揭示了其中的连续相变。 |
| [^17] | [Combining Distance to Class Centroids and Outlier Discounting for Improved Learning with Noisy Labels.](http://arxiv.org/abs/2303.09470) | 本文提出了结合类中心距离和异常值折扣的方法，用于解决在存在噪声标签的情况下训练机器学习模型的问题，并通过实验证明了其有效性 。 |
| [^18] | [On the Existence of a Complexity in Fixed Budget Bandit Identification.](http://arxiv.org/abs/2303.09468) | 该论文探讨了固定预算赌博机标识中复杂度存在的问题，特别是在解决Bernoulli分布最佳臂识别等任务时无法实现统一最佳可达率。 |
| [^19] | [Learning Cross-lingual Visual Speech Representations.](http://arxiv.org/abs/2303.09455) | 本研究使用跨语言自监督学习方法，利用未标记的多语种数据进行音频-视觉预训练，然后在标记的转录上对视觉模型进行微调，实验证明多语种模型性能优越，使用更相似的语言可以得到更好的结果，同时在看不见的语言上进行微调竞争力相当。 |
| [^20] | [Knowledge Discovery from Atomic Structures using Feature Importances.](http://arxiv.org/abs/2303.09453) | 本文提出了一种利用机器学习方法进行原子结构分析的可解释过程。该过程通过构建预测DFT代理并分析特征重要性来解决原子相互作用问题。 |
| [^21] | [Steering Prototype with Prompt-tuning for Rehearsal-free Continual Learning.](http://arxiv.org/abs/2303.09447) | 本研究提出了一个新的连续学习模型——对比原型提示，使用任务特异性提示调整来提高原型性能，同时避免了语义漂移和原型干扰问题。基于此模型的CPP方法在四个具有挑战性的类增量学习基准测试中表现出色，相对于其他最先进的方法有4%至6%的绝对提升。该方法不需要重复训练，性能接近离线联合学习，展示了一种有前途的设计方案。 |
| [^22] | [Controlling High-Dimensional Data With Sparse Input.](http://arxiv.org/abs/2303.09446) | 本论文提出了一种新的控制机制，即将稀疏、易于人理解的控制空间映射到生成模型的潜在空间。通过实验，此方法表现出了效率、鲁棒性和保真性，即使只有少量的输入数值。 |
| [^23] | [Enhanced detection of the presence and severity of COVID-19 from CT scans using lung segmentation.](http://arxiv.org/abs/2303.09440) | 本论文介绍了一个深度学习模型，该模型利用肺部分割进行预处理，其验证 F1 分数在预测 CT 扫描中 COVID-19 的存在和严重程度方面显著超过基线水平。 |
| [^24] | [Improving CNN-base Stock Trading By Considering Data Heterogeneity and Burst.](http://arxiv.org/abs/2303.09407) | 本文提出了一种基于CNN和考虑了数据异质性和爆发性的新型规范化过程的智能股票交易模型。 |
| [^25] | [Stock Price Prediction Using Temporal Graph Model with Value Chain Data.](http://arxiv.org/abs/2303.09406) | 本论文提出了一种神经网络模型，LSTM-GCN，它能够结合价值链数据中的复杂结构和时间依赖性以预测股票价格。实验表明，该模型可以捕获价值链数据中未反映在价格数据中的信息，有助于交易者优化其交易策略和最大化利润。 |
| [^26] | [Speech Modeling with a Hierarchical Transformer Dynamical VAE.](http://arxiv.org/abs/2303.09404) | HiT-DVAE是一种层次Transformer动态变分自编码器，能够优于其他DVAEs在语音频谱建模方面。它具有两个潜变量水平和简单的训练过程，并且具有很高的潜力在低级别语音处理方面。 |
| [^27] | [Learning Feasibility Constraints for Control Barrier Functions.](http://arxiv.org/abs/2303.09403) | 本文通过机器学习技术学习控制屏障函数（CBFs）的新可行性约束，并提出了一种基于采样的学习方法来强制实施该约束，具有在约束优化控制问题中的实践应用价值。 |
| [^28] | [Cryptocurrency Price Prediction using Twitter Sentiment Analysis.](http://arxiv.org/abs/2303.09397) | 本研究使用历史价格和Twitter情感分析预测比特币价格，并取得了不错的效果。情感预测的平均绝对百分比误差为9.45％，价格预测的平均绝对百分比误差为3.6％。 |
| [^29] | [Text-to-ECG: 12-Lead Electrocardiogram Synthesis conditioned on Clinical Text Reports.](http://arxiv.org/abs/2303.09395) | 本文提出了一种基于临床文本报告的自回归生成模型Auto-TTE，用于生成逼真的12导联心电图，相对于传统心电图生成模型并不只考虑单个心电信号的限制，实现了更具复杂性、更加真实的生成，在文本到心电图合成领域具有更好的效果。 |
| [^30] | [On the Interplay Between Misspecification and Sub-optimality Gap in Linear Contextual Bandits.](http://arxiv.org/abs/2303.09390) | 本文研究了线性情境赌博机在错误规定的情境下的算法问题，提出一种新算法，将在一定水平内误差和最小次优间隙相互制约，以常数误差上限实现间隙相关的度量。 |
| [^31] | [LLMSecEval: A Dataset of Natural Language Prompts for Security Evaluations.](http://arxiv.org/abs/2303.09384) | 本文提出了一个 LLMSecEval 数据集，其中包含 150 个自然语言提示，可用于评估大型语言模型在生成容易出现安全漏洞的代码时的安全性能。 |
| [^32] | [Multi-modal Differentiable Unsupervised Feature Selection.](http://arxiv.org/abs/2303.09381) | 该论文提出了一个多模态无监督特征选择框架用于识别具有生物学相关性的信息变量。通过两个基于拉普拉斯的评分算子和可区分的掩码来增强图拉普拉斯所捕捉的结构的准确性，该方法在真实的多模态数据集上表现优于现有的无监督和监督方法。 |
| [^33] | [3D Masked Autoencoding and Pseudo-labeling for Domain Adaptive Segmentation of Heterogeneous Infant Brain MRI.](http://arxiv.org/abs/2303.09373) | 本文提出了一种名为 MAPSeg 的新框架，采用 3D 蒙版自编码和伪标签的方式，实现了跨年龄、跨模态和跨场景下对婴儿脑 MRI 中亚皮质区域的分割，充分考虑不同 MRI 扫描仪、供应商或采集序列以及不同的神经发育阶段所造成的内在异质性，提高了分割结果的鲁棒性。 |
| [^34] | [Goal-conditioned Offline Reinforcement Learning through State Space Partitioning.](http://arxiv.org/abs/2303.09367) | 本文提出了一种通过状态空间划分实现目标导向的离线强化学习方法，并采用补充的优势加权方案，以解决分布转移和多模式问题。 |
| [^35] | [The Scope of In-Context Learning for the Extraction of Medical Temporal Constraints.](http://arxiv.org/abs/2303.09366) | 本研究定义了一种MTC分类，开发了一种基于CFG模型的抽取方法，并通过ICL自动提取和标准化DUGs中的MTC，有望通过定义安全的患者活动模式来推进以患者为中心的医疗应用。 |
| [^36] | [The NCI Imaging Data Commons as a platform for reproducible research in computational pathology.](http://arxiv.org/abs/2303.09354) | 国家癌症研究所影像数据共享平台 (IDC) 旨在促进计算病理学领域的研究可重复性，实现了 FAIR 原则，提供公共库和云端技术支持，方便使用机器学习方法进行癌症组织分类研究。 |
| [^37] | [Unsupervised domain adaptation by learning using privileged information.](http://arxiv.org/abs/2303.09350) | 本文提出利用特权信息进行领域适应（DALUPI）算法，以在学习中放宽假设条件并提高样本效率，通过减少错误来促进医学图像分析等应用的发展。 |
| [^38] | [Improving Automated Hemorrhage Detection in Sparse-view Computed Tomography via Deep Convolutional Neural Network based Artifact Reduction.](http://arxiv.org/abs/2303.09340) | 本文提出了一种基于深度卷积神经网络的伪影降噪方法，用于改善稀疏视图下自动出血检测的图像质量，并证明其能够与完全采样的图像进行同等精确度的分类和检测。 |
| [^39] | [ExoplANNET: A deep learning algorithm to detect and identify planetary signals in radial velocity data.](http://arxiv.org/abs/2303.09335) | 本文介绍了一种神经网络算法ExoplANNET，旨在解决径向速度法检测系外行星的挑战，在存在与星体相关的噪声的情况下进行行星信号的检测和分类，经过合成数据和真实数据的测试，取得了有前途的结果。 |
| [^40] | [Model Based Explanations of Concept Drift.](http://arxiv.org/abs/2303.09331) | 本文提出了一种新的技术，通过各种解释技术，以空间特征的特征变化来表征概念漂移，可以适用于任何模型，并在多个数据集上评估和优化方法。 |
| [^41] | [Stock Trend Prediction: A Semantic Segmentation Approach.](http://arxiv.org/abs/2303.09323) | 本研究提出了一种采用语义分割和完全二维卷积编码器解码器预测股票价格未来趋势的方法 |
| [^42] | [Interpretability from a new lens: Integrating Stratification and Domain knowledge for Biomedical Applications.](http://arxiv.org/abs/2303.09322) | 本文提出了一种新的计算策略，将生物医学问题数据集分层为k折交叉验证(CVs)，并将领域知识解释技术嵌入到当前最先进的IML框架中。这种方法可以提高模型的稳定性、建立信任，并为IML模型生成的结果提供解释。 |
| [^43] | [Explaining Groups of Instances Counterfactually for XAI: A Use Case, Algorithm and User Study for Group-Counterfactuals.](http://arxiv.org/abs/2303.09297) | 该论文探索了一种新的用例，使用“群体反事实”集体解释类似实例的组，提出了一种新颖的群体反事实算法来生成高覆盖率的解释，适合人类对连贯、广泛解释的喜好。 |
| [^44] | [Image Classifiers Leak Sensitive Attributes About Their Classes.](http://arxiv.org/abs/2303.09289) | 该论文探讨了图像分类器存在的隐私泄露问题，提出的Class Attribute Inference Attack（Caia）能够从黑盒设置中准确地推断出敏感属性，包括个人的发色、性别和种族，这表明在鲁棒性和隐私之间存在权衡。 |
| [^45] | [Exploring Resiliency to Natural Image Corruptions in Deep Learning using Design Diversity.](http://arxiv.org/abs/2303.09283) | 本文研究了深度学习图像分类器集合的多样性指标、准确性和对自然图像污染的韧性之间的关系，并发现实现设计选择的多样性可以降低故障模式的数量。 |
| [^46] | [Topology optimization with physics-informed neural networks: application to noninvasive detection of hidden geometries.](http://arxiv.org/abs/2303.09280) | 该论文介绍了一种基于物理知识神经网络的拓扑优化方法，应用于无先验知识的几何结构检测，通过材料密度场表示任意解决方案拓扑，并通过Eikonal正则化实现。该方法可用于医疗和工业应用中的非侵入式成像技术。 |
| [^47] | [Adaptive Modeling of Uncertainties for Traffic Forecasting.](http://arxiv.org/abs/2303.09273) | 提出了QuanTraffic框架，可以自适应建模交通预测中的不确定性，生成预测区间并定义交通预测的真实值可能出现的范围。 |
| [^48] | [Copyright Protection and Accountability of Generative AI:Attack, Watermarking and Attribution.](http://arxiv.org/abs/2303.09272) | 本文提出了一个评估框架来评估用于生成式人工智能版权保护的方法，结果显示针对输入图像，模型水印和归属网络的当前知识产权保护方法在广泛的GAN范围内基本上令人满意，但为保护训练集必须寻找更有效的方法。 |
| [^49] | [Finding Minimum-Cost Explanations for Predictions made by Tree Ensembles.](http://arxiv.org/abs/2303.09271) | 本研究提出了一种高效的oracle系统，能够寻找树集成模型预测的最小代价解释，该算法比目前最先进的替代方案的运行表现更好。m-MARCO算法可以计算每个预测的单个最小解释，并证明相对于枚举所有最小解释的MARCO算法，我们的方法具有两倍的总体加速比。 |
| [^50] | [SmartBERT: A Promotion of Dynamic Early Exiting Mechanism for Accelerating BERT Inference.](http://arxiv.org/abs/2303.09266) | SmartBERT是一种改进的动态早期退出与层跳过机制，可以自适应地跳过一些层并自适应地选择是否退出，以加速BERT模型的推理速度。 |
| [^51] | [Generative Adversarial Network for Personalized Art Therapy in Melanoma Disease Management.](http://arxiv.org/abs/2303.09232) | 该论文提出了一种基于生成对抗网络的黑色素瘤个性化艺术治疗方法，该方法快速生成出个性化的艺术治疗内容，有效减轻患者心理压力。 |
| [^52] | [Controlled Descent Training.](http://arxiv.org/abs/2303.09216) | 本文提出了一种新颖的神经网络（ANN）训练方法，使用最优控制理论支持。通过增强训练标签，以可靠地保证训练损失收敛并提高训练收敛速率，实现动态标签增强。这种新颖的算法保证了局部收敛。 |
| [^53] | [Block-wise Bit-Compression of Transformer-based Models.](http://arxiv.org/abs/2303.09184) | 本论文提出了一种用于Transformer的块位压缩方法，称为BBCT。该方法可以更细粒度地压缩整个Transformer，包括嵌入、矩阵乘法、GELU、softmax、层归一化和所有中间结果。在GLUE数据集上测试结果表明，BBCT可以在大多数任务中实现少于1％的准确率下降。 |
| [^54] | [Learning Logic Specifications for Soft Policy Guidance in POMCP.](http://arxiv.org/abs/2303.09172) | 论文提出了一种从POMCP执行中学习逻辑规范的方法，以实现软政策指导，代替手动定义的策略相关规则，并用于解决环境状态空间大的问题。 |
| [^55] | [Bayesian Generalization Error in Linear Neural Networks with Concept Bottleneck Structure and Multitask Formulation.](http://arxiv.org/abs/2303.09154) | 本文数学上澄清了带有概念瓶颈结构和多任务组成的线性神经网络的贝叶斯泛化误差和自由能。 |
| [^56] | [Predicting nonlinear reshaping of periodic signals in optical fibre with a neural network.](http://arxiv.org/abs/2303.09133) | 本研究利用神经网络预测光纤中周期信号的非线性重塑，探究了正常和异常二阶色散区域，可生成定制脉冲列或出现显着的时间或频谱聚焦。 |
| [^57] | [Exploring Distributional Shifts in Large Language Models for Code Analysis.](http://arxiv.org/abs/2303.09128) | 研究了两种大型语言模型在代码分析中处理领域外数据的能力，提出了组织、项目和模块的自然边界分割方法，发现每个新领域的样本都会产生分布偏移的挑战，实现了多任务学习与少量微调相结合的解决方案。 |
| [^58] | [Evaluation of distance-based approaches for forensic comparison: Application to hand odor evidence.](http://arxiv.org/abs/2303.09126) | 本文提出了一种基于距离的法医学取证比较方法，同时评估并优化了直接方法和间接方法。间接方法更稳健，适用于机器学习，自动特征选择和降维。 |
| [^59] | [SigVIC: Spatial Importance Guided Variable-Rate Image Compression.](http://arxiv.org/abs/2303.09112) | SigVIC是一种空间重要性指导的可变比图像压缩方法，通过自适应学习空间重要性掩码指导特征缩放和比特分配，选择Top-K浅层特征来精细调整解码特征，实验结果表明其在速率失真性能和视觉质量方面均实现了最先进的性能。 |
| [^60] | [Machine learning based biomedical image processing for echocardiographic images.](http://arxiv.org/abs/2303.09103) | 本文介绍了一种通过KNN算法进行医学图像分割和分类的方法，使用灰度共生矩阵特征，可以提高超声心动图图像分析的准确性和效率。 |
| [^61] | [Patch-Token Aligned Bayesian Prompt Learning for Vision-Language Models.](http://arxiv.org/abs/2303.09100) | 本文提出了一种基于贝叶斯概率的视觉语言模型提示学习方法，通过将提示标记推向忠实捕捉标签特定的视觉概念，而不是过度拟合训练类别，解决了现有提示工程的问题。在各种视觉语言任务上的广泛实验表明，该方法优于现有的最先进模型。 |
| [^62] | [Preoperative Prognosis Assessment of Lumbar Spinal Surgery for Low Back Pain and Sciatica Patients based on Multimodalities and Multimodal Learning.](http://arxiv.org/abs/2303.09085) | 本研究结合了中医和机器学习，开发了一种术前评估工具，组合了标准手术评估、中医体质评估和计划手术方法，可用于预测腰椎手术的预后。 |
| [^63] | [SSL-Cleanse: Trojan Detection and Mitigation in Self-Supervised Learning.](http://arxiv.org/abs/2303.09079) | 本篇论文讨论了自监督学习中的木马攻击检测和缓解问题。由于这种攻击危险隐匿，且在下游分类器中很难检测出来。目前在超监督学习中的木马检测方法可以潜在地保护SSL下游分类器，但在其广泛传播之前识别和处理SSL编码器中的触发器是一项艰巨的任务。 |
| [^64] | [VFP: Converting Tabular Data for IIoT into Images Considering Correlations of Attributes for Convolutional Neural Networks.](http://arxiv.org/abs/2303.09068) | 这篇论文提出了一种新的将工业物联网表格数据转换为图像的方法，考虑了属性之间的相关性，可以更好地利用卷积神经网络进行数据处理。 |
| [^65] | [High-Dimensional Penalized Bernstein Support Vector Machines.](http://arxiv.org/abs/2303.09066) | 提出一种适用于高维度情况下的平滑支持向量机铰链损失函数，即Bernstein支持向量机（BernSVM），并提出两种有效算法求解该方法，实验结果表明该方法在现有竞争对手中具有优越性。 |
| [^66] | [Maximum Margin Learning of t-SPNs for Cell Classification with Filtering.](http://arxiv.org/abs/2303.09065) | 本研究提出了一种基于t-SPN算法和滤波技术的细胞分类方法，通过最大化边缘和L2正则化，该方法在HEp-2和Feulgen基准数据集上取得了最高的准确率。 |
| [^67] | [Plant Disease Detection using Region-Based Convolutional Neural Network.](http://arxiv.org/abs/2303.09063) | 本文提出了一种新的基于区域卷积神经网络（R-CNN）的轻量级深度学习模型，可用于检测番茄植物的叶病害。与传统方法相比，该模型利用整个番茄植物图像的特征进行检测，具有更高的准确性和效率。 |
| [^68] | [Knowledge Transfer for Pseudo-code Generation from Low Resource Programming Language.](http://arxiv.org/abs/2303.09062) | 本文研究了将高资源编程语言中训练的编码器-解码器神经模型通过迭代回译的方法，将其知识转移到低资源编程语言中用于伪代码生成，从而解决了缺少低资源编程语言-伪代码平行数据的问题。 |
| [^69] | [Robust Evaluation of Diffusion-Based Adversarial Purification.](http://arxiv.org/abs/2303.09051) | 本文分析了对基于扩散式净化方法的评估方式，并提出了一个新的指导方针，以衡量净化方法对抗性攻击的鲁棒性。同时，我们提出了一种新的净化策略，展示了与最先进的对抗性训练方法相竞争的结果。 |
| [^70] | [Improving Perceptual Quality, Intelligibility, and Acoustics on VoIP Platforms.](http://arxiv.org/abs/2303.09048) | 本文提出一种基于VoIP通信平台进行DNS模型微调的方法，提高其在语音增强方面的性能。这种多任务学习框架能够将噪声抑制和VoIP特有的声学特征相结合，优于行业性能和最先进的方法。 |
| [^71] | [Web and Mobile Platforms for Managing Elections based on IoT And Machine Learning Algorithms.](http://arxiv.org/abs/2303.09045) | 本研究使用IoT和ML技术降低选举成本并提高效率，解决E-voting系统安全性、准确性和可靠性问题。 |
| [^72] | [Embedding Theory of Reservoir Computing and Reducing Reservoir Network Using Time Delays.](http://arxiv.org/abs/2303.09042) | 本文结合延迟嵌入理论和广义嵌入理论，严谨证明了RC本质上是原始输入非线性动态系统的高维嵌入。我们进一步发现了时间延迟和网络神经元数量之间的权衡关系，并显着减小了洪泛计算网络的大小，实现了比全尺寸RC更好的性能。 |
| [^73] | [A Multimodal Data-driven Framework for Anxiety Screening.](http://arxiv.org/abs/2303.09041) | 提出了一种基于多模态数据驱动的焦虑筛查框架（MMD-AS），可以提高模型准确性。 |
| [^74] | [Only Pay for What Is Uncertain: Variance-Adaptive Thompson Sampling.](http://arxiv.org/abs/2303.09033) | 本文提出了一种针对多臂赌博机的方差自适应汤普森采样算法，通过考虑奖励方差的信息减少了遗憾，同时提高了鲁棒性 |
| [^75] | [Conditionally Optimistic Exploration for Cooperative Deep Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2303.09032) | 本文提出了一种名为条件乐观探索(COE)的基于UCT的探索方法，通过结构依赖关系鼓励智能体进行基于乐观主义的协同探索。 |
| [^76] | [A Picture is Worth a Thousand Words: Language Models Plan from Pixels.](http://arxiv.org/abs/2303.09031) | 本文探讨从像素中使用预训练的语言模型进行规划，相对于之前的方法在ALFWorld和VirtualHome基准测试中取得更好的表现。 |
| [^77] | [Learning Rewards to Optimize Global Performance Metrics in Deep Reinforcement Learning.](http://arxiv.org/abs/2303.09027) | 该论文提出了一种名为LR4GPM的（深度）强化学习方法，它可以在问题描述中提供的全局性能度量的基础上优化。LR4GPM在学习奖励函数和训练策略时使用了几种训练技巧，能够避免进行奖励工程，其效率高于DAI'2020组织最近举办的一次自动驾驶竞赛的获胜者。 |
| [^78] | [Self-Inspection Method of Unmanned Aerial Vehicles in Power Plants Using Deep Q-Network Reinforcement Learning.](http://arxiv.org/abs/2303.09013) | 该论文提出了一种电厂无人机自检的方法，通过使用基于强化学习技术的自主机器人，包括感知、规划和行动，达到优化问题的解决，使用DQN框架进行训练，并考虑内外部因素，提高了检查效率和准确性。 |
| [^79] | [Machine Learning for Flow Cytometry Data Analysis.](http://arxiv.org/abs/2303.09007) | 本篇论文介绍了使用机器学习，特别是随机森林算法，来分析流式细胞计数数据的方法。通过此方法，可以提高识别感兴趣的细胞组群的精度和效率，并深入理解数据复杂关系。 |
| [^80] | [Learning Spatio-Temporal Aggregations for Large-Scale Capacity Expansion Problems.](http://arxiv.org/abs/2303.08996) | 本文提出了一种新颖的方法，用于有效解决容量扩展问题，该方法通过时空聚合解决了由于网络规模大、节点特征异构等原因而导致的问题，并优于传统方法和现有基准。 |
| [^81] | [Physics-Informed Neural Networks for Time-Domain Simulations: Accuracy, Computational Cost, and Flexibility.](http://arxiv.org/abs/2303.08994) | 物理信息神经网络被应用于电力系统动态模拟，可以比传统方法快10至1000倍，同时准确性和数值稳定性也足够满足需求。同时，他们也提出了一个基于梯度的方法来规范化NN训练。 |
| [^82] | [Deep Learning Weight Pruning with RMT-SVD: Increasing Accuracy and Reducing Overfitting.](http://arxiv.org/abs/2303.08986) | 本文提出了一种使用随机矩阵理论技术进行深度学习权重剪枝的方法，可以通过奇异值分解技术去除一些特定奇异值，从而减少过拟合和提高准确性。 |
| [^83] | [Forecasting Particle Accelerator Interruptions Using Logistic LASSO Regression.](http://arxiv.org/abs/2303.08984) | 该论文提出了一种利用Logistic LASSO回归预测粒子加速器中断的二元分类模型。通过连续的评估指标测量保存的束流时间。 |
| [^84] | [Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement.](http://arxiv.org/abs/2303.08983) | 提出了一种名为数据集增强的策略，一次性改进数据集，从而提高任何经过增强的数据集训练的模型的准确性、鲁棒性和校准性。例如，使用ImageDataNet+训练的ResNet-50在ImageNet验证集上的准确率提高了1.7％，在ImageNetV2上提高了3.5％，在ImageNet-R上提高了10.0％。 |
| [^85] | [Active Semi-Supervised Learning by Exploring Per-Sample Uncertainty and Consistency.](http://arxiv.org/abs/2303.08978) | 该论文提出了一种称为主动半监督学习（ASSL）的方法，它结合了传统的主动学习和半监督学习，通过采用指数移动平均和上置信界等技术来解决在未标记数据中确定真实不确定性的问题。 |
| [^86] | [Gated Compression Layers for Efficient Always-On Models.](http://arxiv.org/abs/2303.08970) | 本文提出了一种Gated Compression层，可将现有的神经网络架构转化为Gated Neural Networks，可大幅度降低功耗、提高准确性并利用异构计算核心。实验结果在五个公共数据集上证明所提出的方法有效地压缩了正样本，同时保持或提高了模型准确性。 |
| [^87] | [CS-TGN: Community Search via Temporal Graph Neural Networks.](http://arxiv.org/abs/2303.08964) | 本论文提出一种基于查询驱动的时空图卷积网络（CS-TGN）方法，可以同时捕捉时间网络中灵活的社群模式和时态动态。 |
| [^88] | [NESS: Learning Node Embeddings from Static SubGraphs.](http://arxiv.org/abs/2303.08958) | NESS提出了一种在转导设置下使用图自编码器（GAE）从静态子图（NESS）中学习节点嵌入的新方法，并在多个基准数据集上达到了最新链接预测结果。 |
| [^89] | [Large-scale End-of-Life Prediction of Hard Disks in Distributed Datacenters.](http://arxiv.org/abs/2303.08955) | 本文通过定制特征工程和序列学习器，提出一种编码器-解码器LSTM模型，用于大规模预测硬盘剩余寿命，从而降低运营成本 |
| [^90] | [The Tiny Time-series Transformer: Low-latency High-throughput Classification of Astronomical Transients using Deep Model Compression.](http://arxiv.org/abs/2303.08951) | 本论文提出了一种能够以实时、鲁棒和准确的方式处理天文事件的快速高效分类算法，且通过深度压缩方法显著减小模型大小，适应日益增长的实时及时域天文学数据。 |
| [^91] | [Certifiable (Multi)Robustness Against Patch Attacks Using ERM.](http://arxiv.org/abs/2303.08944) | 该文研究了针对贴片攻击的防御方法，并提出了一种使用ERM算法学习可证明具有多种蒙版下都具有鲁棒性的预测模型算法，实现了严格的证明多鲁棒性对贴片攻击。 |
| [^92] | [Enhancing Data Space Semantic Interoperability through Machine Learning: a Visionary Perspective.](http://arxiv.org/abs/2303.08932) | 本文提出通过机器学习改善数据空间中的语义互操作性的计划，通过自动生成和更新元数据，产生更灵活的词汇，解决了传统数据交换的局限性，使数据对社区所有成员更具可访问性和价值。 |
| [^93] | [Applying unsupervised keyphrase methods on concepts extracted from discharge sheets.](http://arxiv.org/abs/2303.08928) | 本研究利用临床自然语言处理技术从出院单中提取概念，并应用无监督关键词方法识别重要概念，成功地解决了临床文本中的挑战。 |
| [^94] | [LRDB: LSTM Raw data DNA Base-caller based on long-short term models in an active learning environment.](http://arxiv.org/abs/2303.08915) | 本文提出了LRDB模型，一个轻量级的开源模型，可在关键应用中快速适应新的DNA样本，具有更好的读取标识，并可根据用户约束条件进行修改。 |
| [^95] | [Latent-Conditioned Policy Gradient for Multi-Objective Deep Reinforcement Learning.](http://arxiv.org/abs/2303.08909) | 该论文提出了一种新的多目标深度强化学习算法，通过策略梯度训练单个神经网络，以在单次训练运行中近似获取整个帕累托集，而不依赖于目标的线性标量化。 |
| [^96] | [Learning ground states of gapped quantum Hamiltonians with Kernel Methods.](http://arxiv.org/abs/2303.08902) | 本文提出了一种利用 kernel 方法学习具有能隙的量子哈密顿量基态的统计学习方法，理论上需要多项式资源实现，通过数值模拟证明了该方法的有效性，并展示了方法的灵活性。 |
| [^97] | [A Multifidelity deep operator network approach to closure for multiscale systems.](http://arxiv.org/abs/2303.08893) | 本文提出了一个基于多保真度深度算子网络框架和“内循环”训练方法解决多尺度系统的封闭问题的方法，并且在实验中得到了显著的改进。 |
| [^98] | [Discrete-Time Nonlinear Feedback Linearization via Physics-Informed Machine Learning.](http://arxiv.org/abs/2303.08884) | 本文提出了一种物理准确的机器学习方法（PIML），用于非线性离散时间动态系统的反馈线性化，通过极点放置来确保稳定性，在非线性转换规律中存在陡峭的梯度时，提出了一种贪心训练方法，其优于传统的数值实现。 |
| [^99] | [Bayesian Quadrature for Neural Ensemble Search.](http://arxiv.org/abs/2303.08874) | 本论文介绍了一种使用贝叶斯积分的新方法，可以在架构似然表面有分散、狭窄峰时构建加权集成神经网络，相比当前同类方法，在测试似然性、准确性和期望校准误差方面更为优秀。 |
| [^100] | [Machine Learning-Driven Adaptive OpenMP For Portable Performance on Heterogeneous Systems.](http://arxiv.org/abs/2303.08873) | 本文提出了基于机器学习的自适应OpenMP扩展，使用生产者-消费者模式动态地选择最快的代码变体，大大降低了用户在异构体系结构上编程自适应应用的工作量。 |
| [^101] | [EvalAttAI: A Holistic Approach to Evaluating Attribution Maps in Robust and Non-Robust Models.](http://arxiv.org/abs/2303.08866) | 该论文提出了一个综合方法EvalAttAI，旨在同时考虑归因映射在各种条件下的稳健性和保真度，以更好地评估归因映射的性能并选择适合特定应用的方法。 |
| [^102] | [Class-Guided Image-to-Image Diffusion: Cell Painting from Brightfield Images with Class Labels.](http://arxiv.org/abs/2303.08863) | 该论文介绍了一种新的算法，将图像至图像和类别引导去噪扩散概率模型组合，用于有元数据标签的图像重建问题。实验结果表明，类别引导的图像至图像扩散可以提升图像重建的内容，优于未引导的模型。 |
| [^103] | [On the Benefits of Leveraging Structural Information in Planning Over the Learned Model.](http://arxiv.org/abs/2303.08856) | 研究了利用系统结构信息减少基于模型的强化学习中的样本复杂性的好处。 |
| [^104] | [Wireless Sensor Networks anomaly detection using Machine Learning: A Survey.](http://arxiv.org/abs/2303.08823) | 本篇综述论文介绍了机器学习技术在无线传感器网络中进行数据异常检测的最新应用。讲述了这种技术能够解决传感数据中异常模式检测的问题，并比较了不同方法的优缺点。 |
| [^105] | [Understanding Post-hoc Explainers: The Case of Anchors.](http://arxiv.org/abs/2303.08806) | 本文对Anchors进行了理论分析，这是一种基于规则的可解释性方法，用于解释文本分类器的决策。 |
| [^106] | [Prompting Large Language Models With the Socratic Method.](http://arxiv.org/abs/2303.08769) | 本文提出了使用苏格拉底式方法开发提示模板的系统方法，以有效地与大型语言模型交互，并举出实例展示了这些方法在归纳、演绎和诱导推理方面的有效性。 |
| [^107] | [On the uncertainty analysis of the data-enabled physics-informed neural network for solving neutron diffusion eigenvalue problem.](http://arxiv.org/abs/2303.08455) | 本文针对数据不可避免带有噪声的情况，研究了DEPINN在求解中子扩散本征值问题方面的可行性，并提出了创新的区间损失函数用于减少噪声影响和提高先验数据利用率，此方法在两个基准问题上得到了验证。 |
| [^108] | [A Comprehensive Study on Post-Training Quantization for Large Language Models.](http://arxiv.org/abs/2303.08302) | 本文基于数万个零-shot实验对基于后训练量化的大型语言模型的不同量化组件进行了综合研究，结果发现细粒度量化和后训练量化方法很重要，用粗粒度量化的更高位数比用非常细粒度的更低位数更强大。我们给出了如何为不同大小的\llms利用量化的建议。 |
| [^109] | [Graph Neural Network Surrogates of Fair Graph Filtering.](http://arxiv.org/abs/2303.08157) | 通过引入过滤器感知的通用近似框架，该方法定义了合适的图神经网络在运行时训练以满足统计平等约束，同时最小程度扰动原始后验情况下实现此目标。 |
| [^110] | [Eliciting Latent Predictions from Transformers with the Tuned Lens.](http://arxiv.org/abs/2303.08112) | 本文提出了一种改进版的“逻辑透镜”技术——“调谐透镜”，通过训练一个仿射探针，可以将每个隐藏状态解码成词汇分布。这个方法被应用于各种自回归语言模型上，比逻辑透镜更具有预测性、可靠性和无偏性，并且通过因果实验验证使用的特征与模型本身类似。同时，本文发现潜在预测的轨迹可以用于高精度地检测恶意输入。 |
| [^111] | [Enhancing COVID-19 Severity Analysis through Ensemble Methods.](http://arxiv.org/abs/2303.07130) | 本文提出了一种基于领域知识的流程，使用图像处理算法和预训练的UNET模型结合从COVID-19患者中提取感染区域，并使用三种机器学习模型的集成将感染的严重程度分类为不同的类别，从而提高COVID-19重症分析。 |
| [^112] | [Transformer-based Planning for Symbolic Regression.](http://arxiv.org/abs/2303.06833) | 该论文提出了一种基于Transformer和蒙特卡罗树搜索的符号回归规划策略TPSR，可以将非可微的反馈作为知识的外部来源融入到方程生成过程中，提高方程生成的准确性和复杂性。 |
| [^113] | [ODIN: On-demand Data Formulation to Mitigate Dataset Lock-in.](http://arxiv.org/abs/2303.06832) | ODIN采用生成AI模型，通过根据用户需求生成按需数据集，解决了传统零样本学习方法在数据集约束方面的限制，使得人工智能能够学习超出训练数据集的未见知识。 |
| [^114] | [On Neural Architectures for Deep Learning-based Source Separation of Co-Channel OFDM Signals.](http://arxiv.org/abs/2303.06438) | 本文研究了涉及OFDM信号的单通道源分离问题，通过原型问题评估了使用面向音频的神经网络架构在分离共信道OFDM波形方面的有效性，并提出了关键的领域知识修改网络参数化的解决方案。 |
| [^115] | [Hardware Acceleration of Neural Graphics.](http://arxiv.org/abs/2303.05735) | 本文研究了神经图形是否需要硬件支持，发现当前GPU性能无法满足对4K分辨率60FPS渲染的需求，且在增强现实/虚拟现实应用中性能缺口更大。作者确定输入编码和MLP内核是性能瓶颈。 |
| [^116] | [Baldur: Whole-Proof Generation and Repair with Large Language Models.](http://arxiv.org/abs/2303.04910) | 本文介绍了一种新的自动化形式验证方法，使用经过微调的大型语言模型一次性生成整个定理的证明，而不是一步步进行，从而提高证明效率。 |
| [^117] | [Guiding Pseudo-labels with Uncertainty Estimation for Source-free Unsupervised Domain Adaptation.](http://arxiv.org/abs/2303.03770) | 本研究提出一种基于损失重新加权策略的无源自适应域自适应（SF-UDA）方法，用于适应目标域，其关键是通过估计伪标签的不确定性来指导其进一步精化，并利用自监督对比框架作为目标空间的正则化器以提高预测精度。 |
| [^118] | [Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering.](http://arxiv.org/abs/2303.01903) | 本研究提出了一个名为Prophet的框架，使用答案启发式方式促使GPT-3解决基于知识的视觉问答问题。在特定的知识型VQA数据集上训练一个纯VQA模型，并从中提取出答案启发式，可提高模型的性能。 |
| [^119] | [Self-Supervised Few-Shot Learning for Ischemic Stroke Lesion Segmentation.](http://arxiv.org/abs/2303.01332) | 本文提出了一种利用少量标注数据进行缺血性卒中病灶分割的自监督少样本学习方法，其利用了计算机断层扫描的参数图生成颜色编码的参数图进行训练，可以达到与需要大量标注数据的最先进方法相当的分割效果。 |
| [^120] | [AR3n: A Reinforcement Learning-based Assist-As-Needed Controller for Robotic Rehabilitation.](http://arxiv.org/abs/2303.00085) | 本文提出了一种基于强化学习的机器人康复辅助控制器AR3n，通过使用虚拟患者模型实现控制器的泛化，实时调节机器人辅助力度并最小化机器人辅助的量，该控制器在实验验证中表现出良好的效果。 |
| [^121] | [Towards Surgical Context Inference and Translation to Gestures.](http://arxiv.org/abs/2302.14237) | 本研究提出了一种自动推理手势并可解释生成手势转录的方法，利用图像分割的数据丰富性。通过检查工具和物体之间的距离和交点，使用分割蒙版检测手术背景。本方法可以高度一致地自动检测重要的手术状态，对机器人辅助手术具有实际应用价值。 |
| [^122] | [KS-DETR: Knowledge Sharing in Attention Learning for Detection Transformer.](http://arxiv.org/abs/2302.11208) | 本文提出了一个基于缩放点积注意力的知识共享DETR方法，通过使用真实的前景-背景蒙版进行权重/值学习，进而提高了DETR的准确性。 |
| [^123] | [Improving Interpretability of Deep Sequential Knowledge Tracing Models with Question-centric Cognitive Representations.](http://arxiv.org/abs/2302.06885) | 本文介绍了一种名为 QIKT 的基于问题的可解释知识追踪模型，通过采用以问题为中心的认知表示方法，明确地模拟学生的知识状态变化，解决了现有模型中同质化问题的假设对实际情况不准确以及解释预测结果具有挑战性的问题。 |
| [^124] | [OpenHLS: High-Level Synthesis for Low-Latency Deep Neural Networks for Experimental Science.](http://arxiv.org/abs/2302.06751) | OpenHLS是一个基于高级综合技术的开源编译器框架，将深度神经网络的高级表示转换为适用于近传感器设备的低级表示，解决了实验科学领域数据采集系统中低延迟处理问题。 |
| [^125] | [Offline Learning of Closed-Loop Deep Brain Stimulation Controllers for Parkinson Disease Treatment.](http://arxiv.org/abs/2302.02477) | 本研究针对DBS治疗帕金森氏症的问题，提出了一种基于离线强化学习框架的闭环深度脑电刺激控制器，以动态调整治疗幅度，减少能量使用，并检测其安全性和性能。 |
| [^126] | [FedPH: Privacy-enhanced Heterogeneous Federated Learning.](http://arxiv.org/abs/2301.11705) | 该论文提出了一种利用预训练模型作为本地模型的骨干，共享嵌入类向量来增强本地模型性能的异构联邦学习方法，并采用隐私保护的混合方法来保护隐私。 |
| [^127] | [Learning Vortex Dynamics for Fluid Inference and Prediction.](http://arxiv.org/abs/2301.11494) | 该论文提出了一种基于可微漩涡粒子的方法，从单个视频中推断和预测流体动力学，通过学习一个低维的物理约束流特征表示，相对于现有方法具有更好的性能和稳健性。 |
| [^128] | [Model Parameter Identification via a Hyperparameter Optimization Scheme for Autonomous Racing Systems.](http://arxiv.org/abs/2301.01470) | 本文提出了基于超参优化方案的模型参数识别方法（MIHO），并实现了AV-21全尺寸自主赛车的模型参数识别。MIHO收敛速度比传统方法快13倍以上，参数模型具有良好适应性和泛化能力，车辆在场地测试中达到了217公里/小时的高速行驶和稳定避障性能。 |
| [^129] | [The Challenges of HTR Model Training: Feedback from the Project Donner le gout de l'archive a l'ere numerique.](http://arxiv.org/abs/2212.11146) | 本文介绍了使用Transkribus平台改进手写文本识别（HTR）模型性能的实践经验，包括创建转录协议、完整使用语言模型以及确定最佳使用基础模型的方法，这些方法可以将单个模型的性能提高20%以上（达到字符错误率低于5%），并讨论了HTR平台的合作性质和数据分享等挑战。 |
| [^130] | [Human-Guided Fair Classification for Natural Language Processing.](http://arxiv.org/abs/2212.10154) | 本文提出了一种新方法，通过自动生成表达丰富的候选句子对并结合群众外包的成对人类判断，训练一个映射语义相似性到统计代理的个体公平性的模型，用于弥合人类直觉和公平分类规范之间的差距。该方法在表现能力、与人类直觉的一致性和两个基准数据集的分类准确性方面优于先前的方法。 |
| [^131] | [Multi-Resolution Online Deterministic Annealing: A Hierarchical and Progressive Learning Architecture.](http://arxiv.org/abs/2212.08189) | 本文提出了一种基于逐渐增加子集数量的分区序列的通用的分层学习结构，并使用无梯度随机逼近更新进行在线解决优化问题的方法，可以定义函数逼近问题并使用双时间尺度随机逼近算法的理论解决，模拟了一种退火过程。 |
| [^132] | [Toroidal Coordinates: Decorrelating Circular Coordinates With Lattice Reduction.](http://arxiv.org/abs/2212.07201) | 本文研究了环面坐标算法的问题，提出了圆值图形的几何相关性概念，并描述了一个系统性的过程，用于构建最小能量的环面值图。 |
| [^133] | [VideoCoCa: Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners.](http://arxiv.org/abs/2212.04979) | 本文提出了一种名为VideoCoCa的基于对比式字幕生成技术的零样本视频文本建模方法，能够在最小的额外训练下，取得最先进的零样本视频分类和零样本文本到视频检索结果，并在轻微微调情况下也能取得强大的结果。 |
| [^134] | [Vision and Structured-Language Pretraining for Cross-Modal Food Retrieval.](http://arxiv.org/abs/2212.04267) | 本研究提出了一种新的策略VLPCook，利用视觉-语言预训练技术和结构化数据，实现基于结构化文本的计算料理任务，并在跨模态食物检索任务上取得了超越当前SoTA的优异表现。 |
| [^135] | [Deep Incubation: Training Large Models by Divide-and-Conquering.](http://arxiv.org/abs/2212.04129) | 本文提出了一种称为深度孵化的训练大型模型的新方法，通过将大型模型分成较小的子模块进行训练并无缝地组装起来，从而实现了大型模型的高效、有效训练。 |
| [^136] | [Beyond Object Recognition: A New Benchmark towards Object Concept Learning.](http://arxiv.org/abs/2212.02710) | 本文提出了一个挑战性的 Object Concept Learning (OCL) 任务，涉及对象属性、作用及其因果关系。作者构建了密集注释的知识库以支持 OCL，提出了 Object Concept Reasoning Network (OCRN) 作为基线，提升了对象认知的发展。 |
| [^137] | [Hierarchical Policy Blending As Optimal Transport.](http://arxiv.org/abs/2212.01938) | 提出了一种分层策略混合方法，通过不平衡的最优传输实现策略混合，巩固底层黎曼运动策略的比例，有效调整黎曼矩阵，决定专家和代理人之间的优先级，保证安全和任务成功，并在一系列机器人控制的应用场景中超过现有基线。 |
| [^138] | [Direct-Effect Risk Minimization for Domain Generalization.](http://arxiv.org/abs/2211.14594) | 本文提出了一种针对域泛化的直接影响风险最小化算法，通过引入因果推断中的直接和间接影响概念解决了相关性转移问题。该算法与现有域泛化算法具有可比性。 |
| [^139] | [{\mu}Split: efficient image decomposition for microscopy data.](http://arxiv.org/abs/2211.12872) | uSplit是一种适用于荧光显微镜图像的高效图像分解方法，集成了横向上下文化，帮助训练更深的分层模型，并有效地减少平铺伪影问题。 |
| [^140] | [From Node Interaction to Hop Interaction: New Effective and Scalable Graph Learning Paradigm.](http://arxiv.org/abs/2211.11761) | 本文提出了一种新的跳跃交互范式，用于解决现有图神经网络（GNN）中存在的可扩展性限制和过度平滑问题。其核心思想是将节点之间的交互目标转换为节点内部经过预处理的多跳特征，并通过HopGNN框架实现跳跃交互。 |
| [^141] | [Parametric Classification for Generalized Category Discovery: A Baseline Study.](http://arxiv.org/abs/2211.11727) | 该研究提出了一种简单而有效的参数化分类方法，该方法可以受益于熵正则化，在多个广义类别发现基准测试中实现最先进的性能，并对未知类别数量具有强大的稳健性。 |
| [^142] | [Rickrolling the Artist: Injecting Backdoors into Text Encoders for Text-to-Image Synthesis.](http://arxiv.org/abs/2211.02408) | 本文证明了文本生成图像模型中使用的文本编码器存在重大的篡改风险，并提出了一种基于反向门攻击的方法，可以插入一个单一字符触发器进提示中，从而触发模型生成具有预定义属性的图像或遵循隐藏的、潜在的恶意描述的图像。 |
| [^143] | [Analysing Diffusion-based Generative Approaches versus Discriminative Approaches for Speech Restoration.](http://arxiv.org/abs/2211.02397) | 本文比较了生成扩散模型和判别方法在不同的语音恢复任务中的表现，结果发现生成扩散模型在语音降噪和带宽扩展等任务中表现较好。 |
| [^144] | [MPCFormer: fast, performant and private Transformer inference with MPC.](http://arxiv.org/abs/2211.01452) | MPCFormer使用MPC和KD技术实现私密的Transformer模型推理，并在速度和性能方面显著提升。 |
| [^145] | [Agglomeration of Polygonal Grids using Graph Neural Networks with applications to Multigrid solvers.](http://arxiv.org/abs/2210.17457) | 本文提出了使用图神经网络和 k-means 聚类算法来自动聚合多边形网格，实验结果表明这些策略比标准算法 METIS 更有效率并能显著提高求解器性能。 |
| [^146] | [Perceptual Grouping in Contrastive Vision-Language Models.](http://arxiv.org/abs/2210.09996) | 本文研究视觉语言模型是否能够理解物体在图像中的位置，并将视觉相关部分组合在一起。我们提出了一些修改，使模型独特地学习了语义和空间信息，并通过多个指标衡量了性能。 |
| [^147] | [Language Model Decoding as Likelihood-Utility Alignment.](http://arxiv.org/abs/2210.07228) | 解码算法的选择需要考虑模型似然度和任务效用的匹配度问题，通过分类不匹配缓解策略（MMS）的视角，可以提高解码算法的通用性 |
| [^148] | [DISCOVER: Deep identification of symbolically concise open-form PDEs via enhanced reinforcement-learning.](http://arxiv.org/abs/2210.02181) | 本文提出一种增强的深度强化学习框架，通过结构感知的递归神经网络代理和稀疏回归方法，在少量先前知识的情况下发现符号简洁的开放式PDE，并通过优秀的奖励函数和基于模型的强化学习进行更新和优化。 |
| [^149] | [Learning Minimally-Violating Continuous Control for Infeasible Linear Temporal Logic Specifications.](http://arxiv.org/abs/2210.01162) | 本文提出了一个模型自由框架，使用深度强化学习来实现复杂高级任务的目标驱动导航。通过将先前的多目标DRL问题转化为一个单一目标问题，并使用基于采样的路径规划算法来指导DRL智能体，该方法可以满足不可行的线性时态逻辑任务并尽可能减少违规。 |
| [^150] | [Neural Networks Efficiently Learn Low-Dimensional Representations with SGD.](http://arxiv.org/abs/2209.14863) | 本文研究使用随机梯度下降训练任意宽度的两层神经网络的问题，当输入为高斯分布，目标为多指数模型时，NN的第一层权重会收敛到真实模型中$k$维主子空间, 可以通过在子空间上使用均匀收敛建立广义误差边界为$O(\sqrt{{kd}/{T}})$, SGD训练的ReLU NN可以学习形如$y = f(\langle\boldsymbol{u},\boldsymbol{x}\rangle)$的单指数目标。 |
| [^151] | [Natural Language Processing Methods to Identify Oncology Patients at High Risk for Acute Care with Clinical Notes.](http://arxiv.org/abs/2209.13860) | 本文研究了利用自然语言处理方法识别化疗后癌症患者急救护理风险的问题，与以往使用结构化卫生数据进行预测的模型相比较，结果表明两者差异不大，从而说明了在临床应用中采用语言模型的可行性以及不同患者群体风险偏差的存在。 |
| [^152] | [Mine yOur owN Anatomy: Revisiting Medical Image Segmentation with Extremely Limited Labels.](http://arxiv.org/abs/2209.13476) | 提出了一种新的医学图像分割方法MOAN，该方法可以在极度有限的标签情况下实现高性能。MOAN由两个互补的流水线组成：协同邻域挖掘和自监督对比学习。实验结果表明，MOAN在Dice评分和其他评估指标方面优于现有最先进方法。 |
| [^153] | [LSAP: Rethinking Inversion Fidelity, Perception and Editability in GAN Latent Space.](http://arxiv.org/abs/2209.12746) | LSAP通过对潜空间实现对齐解决了反演和编辑结果中保真度、感知和可编辑性的问题，使得在保留重建保真度的前提下具有更好的感知和可编辑性。 |
| [^154] | [Feature Alignment by Uncertainty and Self-Training for Source-Free Unsupervised Domain Adaptation.](http://arxiv.org/abs/2208.14888) | 提出了一种新的源自由无监督域适应(UDA)方法，该方法仅使用预训练的源模型和无标签目标图像。方法对模型的特征生成器进行训练，通过捕获因素不确定性、一致性约束和两个不同的自训练阶段，增强了模型的适应能力，并在多个基准数据集上实现了优越性能。 |
| [^155] | [Enhancing Diffusion-Based Image Synthesis with Robust Classifier Guidance.](http://arxiv.org/abs/2208.08664) | 该论文提出利用来自时间相关的对手稳健分类器的梯度来指导生成扩散模型，以促进生成结果的改善。 |
| [^156] | [Using Model-Based Trees with Boosting to Fit Low-Order Functional ANOVA Models.](http://arxiv.org/abs/2207.06950) | 本文提出了一种新算法GAMI-Tree，使用基于模型的树以及新的交互过滤方法，可以更好地拟合底层交互，具有更好的预测性能和更高的效率。 |
| [^157] | [Comparing Feature Importance and Rule Extraction for Interpretability on Text Data.](http://arxiv.org/abs/2207.01420) | 本文比较了文本数据上两类方法：计算每个特征的重要性分数和提取简单逻辑规则，发现在相同模型下产生的解释也不同。我们提出了一种比较解释差异的方法。 |
| [^158] | [FibeRed: Fiberwise Dimensionality Reduction of Topologically Complex Data with Vector Bundles.](http://arxiv.org/abs/2206.06513) | FibeRed提出了一种通过向量丛描述拓扑复杂数据的纤维降维方法，利用该方法可以降低数据维度，同时保留其大规模拓扑特征。算法包含从局部线性降维得到的局部表示与初始全局表示相结合的过程，并在动力学系统和化学领域的数据集上表现出良好的效果。 |
| [^159] | [WaveMix: A Resource-efficient Neural Network for Image Analysis.](http://arxiv.org/abs/2205.14375) | WaveMix是一种资源高效的神经网络结构，可以在多项任务上达到可比或更好的准确率，并且需要更少的参数和GPU RAM，实现时间、成本和能量的节省。 |
| [^160] | [A Sea of Words: An In-Depth Analysis of Anchors for Text Data.](http://arxiv.org/abs/2205.13789) | Anchors 是一种后处理的规则性可解释性方法，它可以通过凸显出一个小组词语（锚点）来强调模型的决策，我们首次对 Anchors 进行了理论分析，考虑到寻找最佳锚点是详尽的，并通过 TF-IDF 向量化步骤以及模型层次的显式结果，探究其在不同类别模型中的行为特征。我们还发现，在神经网络中，最高偏导数所对应的词汇可以重新加权用作 Anchors 词汇。 |
| [^161] | [Constrained Monotonic Neural Networks.](http://arxiv.org/abs/2205.11775) | 本文针对实际应用场景需要的单调性，提出了一种通过在层中的一部分神经元中采用原始激活函数，同时在另一部分采用其点对称反射来解决构建单调深度神经网络的方法。实验证明，该方法的精度符合要求。 |
| [^162] | [Noisy Low-rank Matrix Optimization: Geometry of Local Minima and Convergence Rate.](http://arxiv.org/abs/2203.03899) | 本文提出了一个新的数学框架来处理噪声低秩矩阵优化问题，对受限等距常数的限制要少得多，并且只要无噪声目标的受限等距条件小于1/3，任何错误的局部优化解必须接近于真实解，可以在多项式时间内找到近似解。 |
| [^163] | [Corrupted Image Modeling for Self-Supervised Visual Pre-Training.](http://arxiv.org/abs/2202.03382) | 本文提出了一种损坏图像建模的自监督视觉预训练方法，通过协同训练生成器和增强网络来学习丰富的视觉表示，适用于多种网络架构，并在多个数据集上获得了优异的性能表现。 |
| [^164] | [Laplacian2Mesh: Laplacian-Based Mesh Understanding.](http://arxiv.org/abs/2202.00307) | Laplacian2Mesh是一种克服不规则三角形网格结构困难的方法，使用卷积神经网络来直接处理形状分析任务，并具有可扩展的感受野和个别特征学习的机制。 |
| [^165] | [Fast Distributed k-Means with a Small Number of Rounds.](http://arxiv.org/abs/2201.13217) | 该文提出了一种适用于分布式环境下聚类的快速算法，保证通信轮数较少且成本更低。 |
| [^166] | [Measuring Non-Probabilistic Uncertainty: A cognitive, logical and computational assessment of known and unknown unknowns.](http://arxiv.org/abs/2201.05818) | 该论文提出了一种测量非概率不确定性的方法，该方法可以通过分析文本来检测不确定性对决策因果关系的影响。 |
| [^167] | [An adaptation of InfoMap to absorbing random walks using absorption-scaled graphs.](http://arxiv.org/abs/2112.10953) | 我们使用吸收比例缩放图和马尔可夫时间扫描改进了InfoMap算法，检测网络上密集连接的节点社区，此方法适应节点具有不同移除率的情况，社区结构与不考虑节点吸收率的方法可能有显著不同，并对易感-感染-恢复（SI）模型产生重要影响。 |
| [^168] | [Approximation of functions with one-bit neural networks.](http://arxiv.org/abs/2112.09181) | 研究了一比特神经网络的逼近能力与参数范围，证明了任何满足条件的函数均可被逼近，并给出了实现方法。 |
| [^169] | [Collaborative Pure Exploration in Kernel Bandit.](http://arxiv.org/abs/2110.15771) | 本论文提出了CoPE-KB模型，为多智能体多任务决策提供了一个新的模型，提出了两种最优算法CoopKernelFC和CoopKernelFB，成功地量化了任务相似性对学习加速度的影响并应用于核赌博问题的求解。 |
| [^170] | [Deep Learning-Based Estimation and Goodness-of-Fit for Large-Scale Confirmatory Item Factor Analysis.](http://arxiv.org/abs/2109.09500) | 本文研究了针对大规模验证项目因素分析的参数估计与拟合度检验方法，提出了基于深度学习的算法和扩展测试与指标，具有高效准确和有效性的特点。 |
| [^171] | [Empirical Study of Named Entity Recognition Performance Using Distribution-aware Word Embedding.](http://arxiv.org/abs/2109.01636) | 研究开发了一种分布感知词嵌入，并实施了三种不同的方法来利用NER框架中的分布信息，实验表明将词的特异性融入NER方法可提高NER的性能。 |
| [^172] | [Optimal learning of quantum Hamiltonians from high-temperature Gibbs states.](http://arxiv.org/abs/2108.04842) | 本文研究了从已知高温吉布斯态中学习量子哈密顿量的问题，提出了可在多项式时间内实现的算法，并证明了算法的最优性。 |
| [^173] | [Open-Set Representation Learning through Combinatorial Embedding.](http://arxiv.org/abs/2106.15278) | 该论文提出了一种基于组合学习的方法来识别数据集中的新概念，扩展已知和新类别的识别性能。它使用多个监督元分类器给出的组成知识自然地对未见类别中的示例进行聚类，并通过无监督的成对关系学习让组合嵌入所提供的表示更加强健。 |
| [^174] | [Transient Stability Analysis with Physics-Informed Neural Networks.](http://arxiv.org/abs/2106.13638) | 本文介绍了物理知识嵌入神经网络的暂态稳定性分析，该方法直接将电力系统微分代数方程嵌入神经网络的训练中，并大大减少了对训练数据的需求。 |
| [^175] | [Towards Lower Bounds on the Depth of ReLU Neural Networks.](http://arxiv.org/abs/2105.14835) | 该研究运用数学和优化理论方法，就 ReLU 神经网络的深度下界做了探究，有助于更好地理解这种网络所能表示的函数类的性质。此外，该研究还肯定了一项旧的分段线性函数猜想。 |
| [^176] | [Geometric Analysis of Noisy Low-rank Matrix Recovery in the Exact Parameterized and the Overparameterized Regimes.](http://arxiv.org/abs/2105.08232) | 本文研究了噪声低秩矩阵恢复问题，提出了鲁棒错误定位几何分析算法和连续子空间优化算法，分别用于精确参数化和过度参数化的情况。通过约束等异性性质，我们提供了对全局最优解与局部解之间的最大距离的保证。 |
| [^177] | [Necessary and Sufficient Conditions for Inverse Reinforcement Learning of Bayesian Stopping Time Problems.](http://arxiv.org/abs/2007.03481) | 本文提出了一个Bayesian停时问题的逆强化学习框架，结合微观经济学中的Bayesian揭示偏好思路，通过观察Bayesian决策者的行动，确定其的最优性。并且通过两个停时问题示例得到了验证，并且已在一个真实的例子中得到了高精度地预测用户参与度。 |
| [^178] | [Spectral CUSUM for Online Network Structure Change Detection.](http://arxiv.org/abs/1910.09083) | 本文提出了一种在线变化检测算法Spectral-CUSUM，可用于从嘈杂的观测数据中检测网络社区结构的突变，并证明了其渐近最优性。 |
| [^179] | [Sequential Gaussian Processes for Online Learning of Nonstationary Functions.](http://arxiv.org/abs/1905.10003) | 本文提出了一种基于顺序蒙特卡罗算法的连续高斯过程模型，以解决高斯过程模型的计算复杂度高，难以在线顺序更新的问题，同时允许拟合具有非平稳性质的函数。方法优于现有最先进方法的性能。 |

# 详细

[^1]: SoftZoo：面向多样环境的软体机器人协同设计基准平台

    SoftZoo: A Soft Robot Co-design Benchmark For Locomotion In Diverse Environments. (arXiv:2303.09555v1 [cs.RO])

    [http://arxiv.org/abs/2303.09555](http://arxiv.org/abs/2303.09555)

    SoftZoo是一个面向多种环境的软体机器人协同设计平台，支持广泛、自然启发的材料组合和多种任务，并提供了形态和控制的可微分设计表示。通过一致的评估指标，SoftZoo允许直接比较不同方法，并揭示了软体机器人设计中的权衡。

    

    虽然在控制方面已经取得了显著的研究进展，但在同时协同优化形态时出现了独特的挑战。现有的工作通常是为特定环境或表示量量身定制的。为了更充分地理解固有的设计和性能权衡，并加速开发新型软体机器人，需要一个包含已建立好的任务、环境和评估指标的全面虚拟平台。在这项工作中，我们介绍了SoftZoo，这是一个面向多样环境的软体机器人协同设计平台。SoftZoo支持广泛、自然启发的材料组合，包括能够模拟平地、沙漠、湿地、黏土、冰、雪、浅水和海洋等多种环境。此外，它提供了多个与软体机器人相关的任务，包括快速移动、灵活转向和路径跟随，以及形态和控制的可微分设计表示。结合一致的评估指标，SoftZoo允许在方法之间进行直接比较，并揭示软体机器人设计中的权衡。

    While significant research progress has been made in robot learning for control, unique challenges arise when simultaneously co-optimizing morphology. Existing work has typically been tailored for particular environments or representations. In order to more fully understand inherent design and performance tradeoffs and accelerate the development of new breeds of soft robots, a comprehensive virtual platform with well-established tasks, environments, and evaluation metrics is needed. In this work, we introduce SoftZoo, a soft robot co-design platform for locomotion in diverse environments. SoftZoo supports an extensive, naturally-inspired material set, including the ability to simulate environments such as flat ground, desert, wetland, clay, ice, snow, shallow water, and ocean. Further, it provides a variety of tasks relevant for soft robotics, including fast locomotion, agile turning, and path following, as well as differentiable design representations for morphology and control. Combi
    
[^2]: 数据流图作为完整因果图

    Dataflow graphs as complete causal graphs. (arXiv:2303.09552v1 [cs.SE])

    [http://arxiv.org/abs/2303.09552](http://arxiv.org/abs/2303.09552)

    本篇论文介绍了一种替代的软件设计方法——基于流的编程（FBP），并强调了FBP生成的数据流图和结构性因果模型之间的联系，通过这种联系可以改进软件项目中的日常任务，包括故障定位、业务分析和实验室实验。

    

    组件化开发是现代软件工程实践的核心原则之一。理解软件系统组件之间的因果关系可以为开发人员带来显著的益处。然而，现代软件设计方法使得在系统规模下跟踪和发现这种关系变得困难，这导致了不断增长的智力负担。在本文中，我们考虑了一种替代的软件设计方法——基于流的编程（FBP），并引起了社区对FBP生成的数据流图和结构性因果模型之间关系的注意。我们通过说明性示例展示了如何利用这种关系来改进软件项目中的日常任务，包括故障定位、业务分析和实验室实验。

    Component-based development is one of the core principles behind modern software engineering practices. Understanding of causal relationships between components of a software system can yield significant benefits to developers. Yet modern software design approaches make it difficult to track and discover such relationships at system scale, which leads to growing intellectual debt. In this paper we consider an alternative approach to software design, flow-based programming (FBP), and draw the attention of the community to the connection between dataflow graphs produced by FBP and structural causal models. With expository examples we show how this connection can be leveraged to improve day-to-day tasks in software projects, including fault localisation, business analysis and experimentation.
    
[^3]: WebSHAP: 面向任意机器学习模型的解释工具

    WebSHAP: Towards Explaining Any Machine Learning Models Anywhere. (arXiv:2303.09545v1 [cs.LG])

    [http://arxiv.org/abs/2303.09545](http://arxiv.org/abs/2303.09545)

    WebSHAP是第一个将先进的模型无关解释技术SHAP适应Web环境的浏览器内工具，可用于解释基于ML的贷款批准决策。

    

    随着机器学习被越来越广泛地应用于我们日常的网络体验中，人们越来越需要透明和可解释的基于Web的机器学习。 然而，现有的解释技术往往需要专用的后端服务器，这限制了它们在向较低延迟和更高隐私性的基于浏览器的Web ML迁移时的实用性。为了解决客户端解释方案的迫切需求，我们提出了WebSHAP，这是第一个将先进的模型无关解释技术SHAP适应Web环境的浏览器内工具。我们的开源工具是使用现代Web技术（如WebGL）开发的，利用客户端硬件能力，易于集成到现有的Web ML应用程序中。我们演示了WebSHAP在解释基于ML的贷款批准决策的使用场景中的应用。 回顾我们的工作，我们讨论了透明Web ML的未来研究机会和挑战。WebSHAP现已提供。

    As machine learning (ML) is increasingly integrated into our everyday Web experience, there is a call for transparent and explainable web-based ML. However, existing explainability techniques often require dedicated backend servers, which limit their usefulness as the Web community moves toward in-browser ML for lower latency and greater privacy. To address the pressing need for a client-side explainability solution, we present WebSHAP, the first in-browser tool that adapts the state-of-the-art model-agnostic explainability technique SHAP to the Web environment. Our open-source tool is developed with modern Web technologies such as WebGL that leverage client-side hardware capabilities and make it easy to integrate into existing Web ML applications. We demonstrate WebSHAP in a usage scenario of explaining ML-based loan approval decisions to loan applicants. Reflecting on our work, we discuss the opportunities and challenges for future research on transparent Web ML. WebSHAP is available
    
[^4]: SemDeDup:通过语义去重实现网络规模数据的高效学习（arXiv:2303.09540v1 [cs.LG]）

    SemDeDup: Data-efficient learning at web-scale through semantic deduplication. (arXiv:2303.09540v1 [cs.LG])

    [http://arxiv.org/abs/2303.09540](http://arxiv.org/abs/2303.09540)

    SemDeDup是一种利用预训练模型的嵌入来识别和删除语义重复项的方法。通过对LAION的子集进行分析，SemDeDup可以最小化性能损失的同时删除50%的数据，实际上将训练时间减半。此外，SemDeDup在提供效率收益的同时改进了先前的方法。

    

    机器学习领域的进展很大程度上是由海量数据的增加推动的。然而，像LAION这样的大型网络规模数据集在除查找精确重复项外，大部分未经精心筛选，可能存在很多冗余。在这里，我们介绍SemDeDup，一种基于预训练模型的嵌入来识别和删除语义重复项的方法：即语义上相似但并非完全相同的数据对。去除语义重复项可以保持性能并加速学习。通过对LAION的子集进行分析，我们展示了SemDeDup可以最小化性能损失的同时删除50%的数据，实际上将训练时间减半。此外，性能在分布以外得到提高。同时，通过分析在部分筛选过的数据集C4上训练的语言模型，我们展示了SemDeDup在提供效率收益的同时改进了先前的方法。SemDeDup提供了一个利用质量嵌入简单方法来使模型更快地学习的示例。

    Progress in machine learning has been driven in large part by massive increases in data. However, large web-scale datasets such as LAION are largely uncurated beyond searches for exact duplicates, potentially leaving much redundancy. Here, we introduce SemDeDup, a method which leverages embeddings from pre-trained models to identify and remove semantic duplicates: data pairs which are semantically similar, but not exactly identical. Removing semantic duplicates preserves performance and speeds up learning. Analyzing a subset of LAION, we show that SemDeDup can remove 50% of the data with minimal performance loss, effectively halving training time. Moreover, performance increases out of distribution. Also, analyzing language models trained on C4, a partially curated dataset, we show that SemDeDup improves over prior approaches while providing efficiency gains. SemDeDup provides an example of how simple ways of leveraging quality embeddings can be used to make models learn faster with le
    
[^5]: 无监督遥感变化检测中的深度度量学习

    Deep Metric Learning for Unsupervised Remote Sensing Change Detection. (arXiv:2303.09536v1 [cs.CV])

    [http://arxiv.org/abs/2303.09536](http://arxiv.org/abs/2303.09536)

    提出一个基于深度度量学习的无监督CD方法，利用两个相互连接的深度网络（D-CPG和D-FE）来更好地预测变化和提取用于变化检测的特征，并在各种具有不同设置的数据集中优于几种最先进的无监督CD方法以及有监督的方法。

    

    遥感变化检测是从多时相遥感图像中检测相关变化，用于各种遥感应用，例如土地覆盖、土地利用、人类发展分析和灾难响应。现有的遥感变化检测方法的性能归因于大型注释数据集的训练。此外，大多数模型在培训和测试数据集之间存在领域差距时，训练的模型往往表现非常差。本文提出了一种基于深度度量学习的无监督CD方法，可以处理这两个问题。给定MT-RSI，所提出的方法通过迭代地优化无监督CD损失来生成对应的变化概率图，而不需要在大的数据集上进行训练。我们的无监督CD方法包含两个相互连接的深度网络，即Deep-Change Probability Generator（D-CPG）和Deep-Feature Extractor（D-FE）。D-CPG旨在通过学习判别特征来预测变化，而D-FE则用于提取用于变化检测的特征。实验结果表明，我们提出的方法在各种具有不同设置的数据集中优于几种最先进的无监督CD方法以及有监督的方法。

    Remote Sensing Change Detection (RS-CD) aims to detect relevant changes from Multi-Temporal Remote Sensing Images (MT-RSIs), which aids in various RS applications such as land cover, land use, human development analysis, and disaster response. The performance of existing RS-CD methods is attributed to training on large annotated datasets. Furthermore, most of these models are less transferable in the sense that the trained model often performs very poorly when there is a domain gap between training and test datasets. This paper proposes an unsupervised CD method based on deep metric learning that can deal with both of these issues. Given an MT-RSI, the proposed method generates corresponding change probability map by iteratively optimizing an unsupervised CD loss without training it on a large dataset. Our unsupervised CD method consists of two interconnected deep networks, namely Deep-Change Probability Generator (D-CPG) and Deep-Feature Extractor (D-FE). The D-CPG is designed to pred
    
[^6]: 镜像下降和镜像 Langevin 动力学的变分原理

    Variational Principles for Mirror Descent and Mirror Langevin Dynamics. (arXiv:2303.09532v1 [math.OC])

    [http://arxiv.org/abs/2303.09532](http://arxiv.org/abs/2303.09532)

    本文提出了镜像下降和镜像 Langevin 动力学的变分形式，表明镜像下降作为某个最优控制问题的闭环解决方案出现，并给出了 Bellman 值函数的表达式。

    

    镜像下降是由 Nemirovski 和 Yudin 在上世纪70年代引入的原始-对偶凸优化方法，可以通过选择强凸势函数来适应优化问题的几何形状。它是各种应用的基本原语，包括大规模优化、机器学习和控制。本文提出了镜像下降及其随机变体镜像 Langevin 动力学的变分形式。主要思想是受 Brezis 和 Ekeland 关于梯度流变分原理的经典工作启发，表明镜像下降作为某个最优控制问题的闭环解决方案出现，而 Bellman 值函数是初始条件和目标函数全局极小值之间的 Bregman 散度。

    Mirror descent, introduced by Nemirovski and Yudin in the 1970s, is a primal-dual convex optimization method that can be tailored to the geometry of the optimization problem at hand through the choice of a strongly convex potential function. It arises as a basic primitive in a variety of applications, including large-scale optimization, machine learning, and control. This paper proposes a variational formulation of mirror descent and of its stochastic variant, mirror Langevin dynamics. The main idea, inspired by the classic work of Brezis and Ekeland on variational principles for gradient flows, is to show that mirror descent emerges as a closed-loop solution for a certain optimal control problem, and the Bellman value function is given by the Bregman divergence between the initial condition and the global minimizer of the objective function.
    
[^7]: 利用PointNet++解决雷达数据中的杂波 ——标签生成和检测

    Tackling Clutter in Radar Data -- Label Generation and Detection Using PointNet++. (arXiv:2303.09530v1 [cs.CV])

    [http://arxiv.org/abs/2303.09530](http://arxiv.org/abs/2303.09530)

    本文提出了两种神经网络来识别雷达数据中的杂波，并设计了一种方法来自动生成杂波标签。对现有数据进行评估，显示出比现有方法更好的性能。同时发布第一个表示实际驾驶场景的自由可用雷达杂波数据集。

    

    在自动驾驶等环境感知领域中，雷达传感器输出了大量不需要的杂波。这些点没有对应的真实对象，是后续处理步骤（如物体检测或跟踪）中错误的主要来源。因此，我们提出了两种新颖的神经网络设置来识别杂波。输入数据、网络架构和训练配置都是针对这个任务进行调整的。特别注意的是，对于由多个传感器扫描组成的点云的下采样。在广泛的评估中，新的设置显示出比现有方法更好的性能。由于没有适合的公共数据集来注释杂波，我们设计了一种方法来自动生成相应的标签。通过将其应用于具有对象注释的现有数据并发布其代码，我们有效地创建了第一个代表实际驾驶场景的自由可用雷达杂波数据集。

    Radar sensors employed for environment perception, e.g. in autonomous vehicles, output a lot of unwanted clutter. These points, for which no corresponding real objects exist, are a major source of errors in following processing steps like object detection or tracking. We therefore present two novel neural network setups for identifying clutter. The input data, network architectures and training configuration are adjusted specifically for this task. Special attention is paid to the downsampling of point clouds composed of multiple sensor scans. In an extensive evaluation, the new setups display substantially better performance than existing approaches. Because there is no suitable public data set in which clutter is annotated, we design a method to automatically generate the respective labels. By applying it to existing data with object annotations and releasing its code, we effectively create the first freely available radar clutter data set representing real-world driving scenarios. C
    
[^8]: 公平感知的差分隐私协同过滤

    Fairness-aware Differentially Private Collaborative Filtering. (arXiv:2303.09527v1 [cs.IR])

    [http://arxiv.org/abs/2303.09527](http://arxiv.org/abs/2303.09527)

    本文提出了DP-Fair，一个两阶段的协同过滤算法框架，它结合了差分隐私机制和公平约束，旨在保护用户隐私、确保公平推荐。

    

    最近，越来越多的隐私保护机器学习任务采用差分隐私引导算法，然而，这样的算法使用在算法公平性方面有折衷，这一点被广泛认可。本文针对差分隐私随机梯度下降（DP-SGD）训练的经典协同过滤方法导致用户群体与不同用户参与水平之间存在不公平影响的问题，提出了一个两阶段框架DP-Fair，它将差分隐私机制与公平限制相结合，从而在保护用户隐私的同时确保公平推荐。

    Recently, there has been an increasing adoption of differential privacy guided algorithms for privacy-preserving machine learning tasks. However, the use of such algorithms comes with trade-offs in terms of algorithmic fairness, which has been widely acknowledged. Specifically, we have empirically observed that the classical collaborative filtering method, trained by differentially private stochastic gradient descent (DP-SGD), results in a disparate impact on user groups with respect to different user engagement levels. This, in turn, causes the original unfair model to become even more biased against inactive users. To address the above issues, we propose \textbf{DP-Fair}, a two-stage framework for collaborative filtering based algorithms. Specifically, it combines differential privacy mechanisms with fairness constraints to protect user privacy while ensuring fair recommendations. The experimental results, based on Amazon datasets, and user history logs collected from Etsy, one of th
    
[^9]: PyVBMC：Python中高效的贝叶斯推断

    PyVBMC: Efficient Bayesian inference in Python. (arXiv:2303.09519v1 [stat.ML])

    [http://arxiv.org/abs/2303.09519](http://arxiv.org/abs/2303.09519)

    PyVBMC是一种高效的Python工具，用于黑盒计算模型的贝叶斯推断和模型选择，可以处理连续参数不超过约10-15个的计算或统计模型。

    

    PyVBMC是Variational Bayesian Monte Carlo（VBMC）算法的Python实现，用于黑盒计算模型的后验和模型推断。VBMC是一种用于高效参数估计和模型评估的近似推断方法，当模型评估是有点到非常昂贵（例如第二次或更多次）和/或嘈杂时。具体而言，VBMC计算：

    PyVBMC is a Python implementation of the Variational Bayesian Monte Carlo (VBMC) algorithm for posterior and model inference for black-box computational models (Acerbi, 2018, 2020). VBMC is an approximate inference method designed for efficient parameter estimation and model assessment when model evaluations are mildly-to-very expensive (e.g., a second or more) and/or noisy. Specifically, VBMC computes:  - a flexible (non-Gaussian) approximate posterior distribution of the model parameters, from which statistics and posterior samples can be easily extracted;  - an approximation of the model evidence or marginal likelihood, a metric used for Bayesian model selection.  PyVBMC can be applied to any computational or statistical model with up to roughly 10-15 continuous parameters, with the only requirement that the user can provide a Python function that computes the target log likelihood of the model, or an approximation thereof (e.g., an estimate of the likelihood obtained via simulation
    
[^10]: 基于Hilbert-Schmidt独立准则的门控循环单元网络用于电池健康状态评估

    Gate Recurrent Unit Network based on Hilbert-Schmidt Independence Criterion for State-of-Health Estimation. (arXiv:2303.09497v1 [cs.LG])

    [http://arxiv.org/abs/2303.09497](http://arxiv.org/abs/2303.09497)

    该论文提出一种基于Hilbert-Schmidt独立准则的GRU模型，能够很好地解决电池健康状态评估中存在的数据长度等问题，并通过提取最相关和独立的特征能够极大提高准确性和鲁棒性。

    

    健康状态评估是确保电池安全可靠运行的关键步骤。由于不同周期中出现数据分布和序列长度变化等问题，大多数现有方法需要进行健康特征提取技术，而这可能耗时且费力。门控循环单元（GRU）由于其简单的结构和卓越的性能而得到了广泛关注，并能很好地解决这个问题。然而，网络中仍存在冗余信息，影响SOH评估的准确性。为了解决这个问题，提出了一种基于Hilbert-Schmidt独立准则（GRU-HSIC）的新型GRU网络。首先，使用零掩码网络将每个周期测量的电池数据转换为相同长度的序列，同时仍保留每个周期原始数据大小的信息。其次，使用从信息瓶颈（IB）理论演变而来的Hilbert-Schmidt独立准则（HSIC）瓶颈从序列中提取最相关和独立的特征。最后，使用提取的特征训练GRU-HSIC模型以准确估计电池的SOH。实验结果表明，该方法在准确性和鲁棒性方面优于现有方法。

    State-of-health (SOH) estimation is a key step in ensuring the safe and reliable operation of batteries. Due to issues such as varying data distribution and sequence length in different cycles, most existing methods require health feature extraction technique, which can be time-consuming and labor-intensive. GRU can well solve this problem due to the simple structure and superior performance, receiving widespread attentions. However, redundant information still exists within the network and impacts the accuracy of SOH estimation. To address this issue, a new GRU network based on Hilbert-Schmidt Independence Criterion (GRU-HSIC) is proposed. First, a zero masking network is used to transform all battery data measured with varying lengths every cycle into sequences of the same length, while still retaining information about the original data size in each cycle. Second, the Hilbert-Schmidt Independence Criterion (HSIC) bottleneck, which evolved from Information Bottleneck (IB) theory, is 
    
[^11]: 量子机器学习中的挑战与机遇

    Challenges and Opportunities in Quantum Machine Learning. (arXiv:2303.09491v1 [quant-ph])

    [http://arxiv.org/abs/2303.09491](http://arxiv.org/abs/2303.09491)

    量子机器学习（QML）是机器学习和量子计算交叉领域中的前沿研究方向，具有应用于量子材料、生物化学和高能物理等领域的潜力，但其模型的可训练性仍有挑战，需要进一步解决。本文回顾了当前QML方法与应用，并探讨了QML的量子优势机会。

    

    量子机器学习（QML）位于机器学习和量子计算的交叉领域，具有加速数据分析的潜力，特别是在量子数据应用于量子材料、生物化学和高能物理方面。然而，QML模型的可训练性仍然存在挑战。本文回顾了当前的QML方法与应用，并突出了量子机器学习与经典机器学习之间的差异，重点关注量子神经网络和量子深度学习。最后，我们讨论了QML的量子优势机会。

    At the intersection of machine learning and quantum computing, Quantum Machine Learning (QML) has the potential of accelerating data analysis, especially for quantum data, with applications for quantum materials, biochemistry, and high-energy physics. Nevertheless, challenges remain regarding the trainability of QML models. Here we review current methods and applications for QML. We highlight differences between quantum and classical machine learning, with a focus on quantum neural networks and quantum deep learning. Finally, we discuss opportunities for quantum advantage with QML.
    
[^12]: 用简单离散状态空间有效地建模时间序列

    Effectively Modeling Time Series with Simple Discrete State Spaces. (arXiv:2303.09489v1 [cs.LG])

    [http://arxiv.org/abs/2303.09489](http://arxiv.org/abs/2303.09489)

    介绍了一种名为SpaceTime的状态空间时间序列架构，提出了基于伴随矩阵的新的SSM参数化方法，能够学习自回归过程，有效预测远期。

    

    时间序列建模是一个已被广泛研究的问题，通常需要方法具备以下三个特性：表达复杂依赖关系，预测远期，以及有效训练长序列。状态空间模型（SSMs）是时间序列的经典模型，之前的工作将SSMs与深度学习层结合，以实现高效的序列建模。然而，我们发现这些先前方法存在根本上的限制，证明了它们的SSM表示不能表达自回归的时间序列过程。因此，我们介绍了SpaceTime，一种新的状态空间时间序列架构，改进了所有三个特性。为了提高表现力，我们提出了一种基于伴随矩阵的新的SSM参数化方法——离散时间过程的规范表示——它使得SpaceTime的SSM层能够学习称心的自回归过程。为了预测远期，我们介绍了一个“闭环”变体的伴随SSM，使得SpaceTime能够预测许多将来的时间点。

    Time series modeling is a well-established problem, which often requires that methods (1) expressively represent complicated dependencies, (2) forecast long horizons, and (3) efficiently train over long sequences. State-space models (SSMs) are classical models for time series, and prior works combine SSMs with deep learning layers for efficient sequence modeling. However, we find fundamental limitations with these prior approaches, proving their SSM representations cannot express autoregressive time series processes. We thus introduce SpaceTime, a new state-space time series architecture that improves all three criteria. For expressivity, we propose a new SSM parameterization based on the companion matrix -- a canonical representation for discrete-time processes -- which enables SpaceTime's SSM layers to learn desirable autoregressive processes. For long horizon forecasting, we introduce a "closed-loop" variation of the companion SSM, which enables SpaceTime to predict many future time
    
[^13]: 一种新颖的自编码-循环神经网络模型用于使用多模态MRI数据进行卒中预测

    A Novel Autoencoders-LSTM Model for Stroke Outcome Prediction using Multimodal MRI Data. (arXiv:2303.09484v1 [cs.CV])

    [http://arxiv.org/abs/2303.09484](http://arxiv.org/abs/2303.09484)

    本研究提出了一种新的机器学习模型用于使用多模态MRI进行卒中预测，该模型融合了多个MRI模态和压缩的多模态特征，表现出比现有的最先进方法更好的效果。

    

    病人预后预测对缺血性卒中的管理至关重要。本文提出了一种新的机器学习模型，用于使用多模态磁共振成像(MRI)进行卒中预后预测。所提出的模型由两个串行级别的自动编码器(AEs)组成，在第一级别的不同AE中使用不同的MRI模态学习单模态特征，并使用第二级别的AE将单模态特征组合成压缩的多模态特征。给定病人的多模态特征序列然后由LSTM网络用于预测结果分数。实验结果表明，所提出的AE2-LSTM模型是一种有效的方法，可以更好地解决MRI数据的多模态和体积性质。实验结果表明，所提出的AE2-LSTM模型通过达到最高AUC=0.71和最低MAE=0.34优于现有最先进的模型。

    Patient outcome prediction is critical in management of ischemic stroke. In this paper, a novel machine learning model is proposed for stroke outcome prediction using multimodal Magnetic Resonance Imaging (MRI). The proposed model consists of two serial levels of Autoencoders (AEs), where different AEs at level 1 are used for learning unimodal features from different MRI modalities and a AE at level 2 is used to combine the unimodal features into compressed multimodal features. The sequences of multimodal features of a given patient are then used by an LSTM network for predicting outcome score. The proposed AE2-LSTM model is proved to be an effective approach for better addressing the multimodality and volumetric nature of MRI data. Experimental results show that the proposed AE2-LSTM outperforms the existing state-of-the art models by achieving highest AUC=0.71 and lowest MAE=0.34.
    
[^14]: 辅助网络在持续学习中实现更好的稳定性-可塑性平衡

    Achieving a Better Stability-Plasticity Trade-off via Auxiliary Networks in Continual Learning. (arXiv:2303.09483v1 [cs.LG])

    [http://arxiv.org/abs/2303.09483](http://arxiv.org/abs/2303.09483)

    本文提出了一种辅助网络持续学习方法（ANCL），通过对流信息的控制，自然插值可塑性和稳定性之间的差异，有助于在神经网络中实现更好的稳定性-可塑性平衡。

    

    与人类顺序学习新任务的自然能力相比，神经网络被认为容易出现灾难性遗忘，即模型在被优化为新任务后，在旧任务上的表现急剧下降。为此，持续学习（CL）社区提出了几种解决方案，旨在使神经网络具有学习当前任务（可塑性）的能力，同时在以前的任务上实现高精度（稳定性）。尽管取得了显着的进展，但稳定性-可塑性平衡还远未得到解决，其基本机制尚不清楚。在这项工作中，我们提出了一种新方法——辅助网络持续学习（ANCL），它将一个额外的辅助网络应用于主要关注稳定性的持续学习模型中，从而促进模型的可塑性。更具体地说，所提出的框架通过控制主要网络和辅助网络之间信息的流动来自然地插值可塑性和稳定性之间的差异。多个数据集的实验结果表明，ANCL在可塑性和稳定性方面优于现有持续学习方法，实现了更好的平衡。

    In contrast to the natural capabilities of humans to learn new tasks in a sequential fashion, neural networks are known to suffer from catastrophic forgetting, where the model's performances on old tasks drop dramatically after being optimized for a new task. Since then, the continual learning (CL) community has proposed several solutions aiming to equip the neural network with the ability to learn the current task (plasticity) while still achieving high accuracy on the previous tasks (stability). Despite remarkable improvements, the plasticity-stability trade-off is still far from being solved and its underlying mechanism is poorly understood. In this work, we propose Auxiliary Network Continual Learning (ANCL), a novel method that applies an additional auxiliary network which promotes plasticity to the continually learned model which mainly focuses on stability. More concretely, the proposed framework materializes in a regularizer that naturally interpolates between plasticity and st
    
[^15]: 简单基于群体进化的任意阶元学习

    Arbitrary Order Meta-Learning with Simple Population-Based Evolution. (arXiv:2303.09478v1 [cs.LG])

    [http://arxiv.org/abs/2303.09478](http://arxiv.org/abs/2303.09478)

    本文提出了一种简单的基于群体进化的元学习方法，可以隐式地优化任意高阶的元参数，从而加速学习。

    

    元学习是学习如何学习的概念，能够使学习系统迅速、灵活地解决新任务。通常需要定义一组外循环元参数，然后用它们来更新一组内部循环参数。大多数元学习方法使用复杂的、计算开销很大的双层优化方案来更新这些元参数，但标准的元学习技术往往不适用于更高阶的元参数，因为元优化过程变得太复杂或不稳定。受到真实世界进化中高阶元学习的启发，我们显示出使用简单基于群体进化可以隐式地优化任意高阶的元参数。首先，我们从理论上证明并经验性地证明了基于群体进化隐式地优化了任意阶元参数。

    Meta-learning, the notion of learning to learn, enables learning systems to quickly and flexibly solve new tasks. This usually involves defining a set of outer-loop meta-parameters that are then used to update a set of inner-loop parameters. Most meta-learning approaches use complicated and computationally expensive bi-level optimisation schemes to update these meta-parameters. Ideally, systems should perform multiple orders of meta-learning, i.e. to learn to learn to learn and so on, to accelerate their own learning. Unfortunately, standard meta-learning techniques are often inappropriate for these higher-order meta-parameters because the meta-optimisation procedure becomes too complicated or unstable. Inspired by the higher-order meta-learning we observe in real-world evolution, we show that using simple population-based evolution implicitly optimises for arbitrarily-high order meta-parameters. First, we theoretically prove and empirically show that population-based evolution implici
    
[^16]: 广义秩正半定矩阵去噪的梯度流分析

    Gradient flow on extensive-rank positive semi-definite matrix denoising. (arXiv:2303.09474v1 [stat.ML])

    [http://arxiv.org/abs/2303.09474](http://arxiv.org/abs/2303.09474)

    本文提出了一种新的方法来分析广义秩和高维情况下正半定矩阵去噪问题的梯度流，揭示了其中的连续相变。

    

    本文提出了一种新的方法来分析广义秩和高维情况下正半定矩阵去噪问题的梯度流。我们使用最近的线性矩阵理论技术推导出固定点方程，跟踪问题的矩阵均方差的完整时间演化。所得到的固定点方程的预测结果通过数值实验得到验证。我们通过举例简要说明了我们的形式主义的几个预测，特别是我们揭示了广义秩和高维情况下的连续相变，这些相变在适当的极限下与低秩问题的经典相变有关。该形式主义有比本文更广泛的应用。

    In this work, we present a new approach to analyze the gradient flow for a positive semi-definite matrix denoising problem in an extensive-rank and high-dimensional regime. We use recent linear pencil techniques of random matrix theory to derive fixed point equations which track the complete time evolution of the matrix-mean-square-error of the problem. The predictions of the resulting fixed point equations are validated by numerical experiments. In this short note we briefly illustrate a few predictions of our formalism by way of examples, and in particular we uncover continuous phase transitions in the extensive-rank and high-dimensional regime, which connect to the classical phase transitions of the low-rank problem in the appropriate limit. The formalism has much wider applicability than shown in this communication.
    
[^17]: 结合类中心距离和异常值折扣的方法，提高在存在噪声标签的情况下训练机器学习模型的效果

    Combining Distance to Class Centroids and Outlier Discounting for Improved Learning with Noisy Labels. (arXiv:2303.09470v1 [cs.LG])

    [http://arxiv.org/abs/2303.09470](http://arxiv.org/abs/2303.09470)

    本文提出了结合类中心距离和异常值折扣的方法，用于解决在存在噪声标签的情况下训练机器学习模型的问题，并通过实验证明了其有效性 。

    

    本文提出了一种新的方法，用于解决在存在噪声标签的情况下训练机器学习模型的挑战。通过在物品的潜在空间中巧妙地使用距离类中心的方法，再结合折扣策略以减少距离所有类中心（即异常值）远的样本的重要性，我们的方法有效解决了噪声标签的问题。我们的方法是基于这样的想法：在训练的早期阶段，距离各自类中心更远的样本更可能是噪声。通过在几个流行的基准数据集上进行广泛实验，我们证明了我们的方法的有效性。结果表明，我们的方法在存在噪声标签的情况下，可以明显提高分类准确性，表现优于当前领域的最优方法。

    In this paper, we propose a new approach for addressing the challenge of training machine learning models in the presence of noisy labels. By combining a clever usage of distance to class centroids in the items' latent space with a discounting strategy to reduce the importance of samples far away from all the class centroids (i.e., outliers), our method effectively addresses the issue of noisy labels. Our approach is based on the idea that samples farther away from their respective class centroid in the early stages of training are more likely to be noisy. We demonstrate the effectiveness of our method through extensive experiments on several popular benchmark datasets. Our results show that our approach outperforms the state-of-the-art in this area, achieving significant improvements in classification accuracy when the dataset contains noisy labels.
    
[^18]: 固定预算赌博机标识中的复杂度存在问题

    On the Existence of a Complexity in Fixed Budget Bandit Identification. (arXiv:2303.09468v1 [stat.ML])

    [http://arxiv.org/abs/2303.09468](http://arxiv.org/abs/2303.09468)

    该论文探讨了固定预算赌博机标识中复杂度存在的问题，特别是在解决Bernoulli分布最佳臂识别等任务时无法实现统一最佳可达率。

    

    在固定预算赌博机标识中，算法按顺序观察来自多个分布的样本，直到给定最终时间。然后，它回答关于分布集的查询。一个好的算法将有小的错误概率。虽然这个概率随着最终时间的增加呈指数级下降，但对于大多数标识任务，最佳可达率并非精确已知。我们展示了如果固定预算任务接受复杂度（定义为单个算法在所有赌博问题中实现的错误概率的下限），则该复杂度由该问题的最佳非自适应抽样过程确定。我们证明了对于几个固定预算识别任务，包括具有两个臂的伯努利最佳臂识别，不存在这样的复杂度：没有单个算法能够随处实现最佳可能速率。

    In fixed budget bandit identification, an algorithm sequentially observes samples from several distributions up to a given final time. It then answers a query about the set of distributions. A good algorithm will have a small probability of error. While that probability decreases exponentially with the final time, the best attainable rate is not known precisely for most identification tasks. We show that if a fixed budget task admits a complexity, defined as a lower bound on the probability of error which is attained by a single algorithm on all bandit problems, then that complexity is determined by the best non-adaptive sampling procedure for that problem. We show that there is no such complexity for several fixed budget identification tasks including Bernoulli best arm identification with two arms: there is no single algorithm that attains everywhere the best possible rate.
    
[^19]: 跨语言视觉语音表示的学习

    Learning Cross-lingual Visual Speech Representations. (arXiv:2303.09455v1 [cs.CL])

    [http://arxiv.org/abs/2303.09455](http://arxiv.org/abs/2303.09455)

    本研究使用跨语言自监督学习方法，利用未标记的多语种数据进行音频-视觉预训练，然后在标记的转录上对视觉模型进行微调，实验证明多语种模型性能优越，使用更相似的语言可以得到更好的结果，同时在看不见的语言上进行微调竞争力相当。

    

    跨语言自监督学习是近年来逐渐流行的研究课题。当前相关工作仅限于利用音频信号进行表示学习。在本工作中，我们研究跨语言自监督的视觉表示学习。我们利用最近提出的RAVEn框架，对未标记的多语种数据进行音频-视觉预训练，然后在标记的转录上对视觉模型进行微调。我们的实验证明：（1）具有更多数据的多语种模型优于单语种模型，但当数据量固定时，单语种模型往往达到更好的性能；（2）多语种优于仅英语预训练；（3）使用更相似的语言可以得到更好的结果；（4）在看不见的语言上进行微调与在预训练集中使用目标语言相当竞争力。我们希望我们的研究能启发未来关于非英语语音表示学习的研究。

    Cross-lingual self-supervised learning has been a growing research topic in the last few years. However, current works only explored the use of audio signals to create representations. In this work, we study cross-lingual self-supervised visual representation learning. We use the recently-proposed Raw Audio-Visual Speech Encoders (RAVEn) framework to pre-train an audio-visual model with unlabelled multilingual data, and then fine-tune the visual model on labelled transcriptions. Our experiments show that: (1) multi-lingual models with more data outperform monolingual ones, but, when keeping the amount of data fixed, monolingual models tend to reach better performance; (2) multi-lingual outperforms English-only pre-training; (3) using languages which are more similar yields better results; and (4) fine-tuning on unseen languages is competitive to using the target language in the pre-training set. We hope our study inspires future research on non-English-only speech representation learni
    
[^20]: 利用特征重要性从原子结构中发现知识

    Knowledge Discovery from Atomic Structures using Feature Importances. (arXiv:2303.09453v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2303.09453](http://arxiv.org/abs/2303.09453)

    本文提出了一种利用机器学习方法进行原子结构分析的可解释过程。该过程通过构建预测DFT代理并分析特征重要性来解决原子相互作用问题。

    

    对于设计用于不同应用的新型材料，深入理解原子结构组成部分之间的相互作用至关重要。这种需求超出了了解原子数量、类型、化学组成和化学相互作用性质等基础知识。更大的图景发生在量子层面上，可以通过密度泛函理论（DFT）来解决。然而，使用DFT是一个计算成本高昂的过程，其结果并不能提供便于解释的原子相互作用见解，这些见解则对材料设计来说非常重要。解决原子相互作用的另一种方式是使用可解释的机器学习方法，构建并分析预测性DFT代理。本文的目的是使用最近发表的可解释距离回归方法的修改版本提出这样的过程。我们的测试使用典型的金属硼烷团簇进行。

    Molecular-level understanding of the interactions between the constituents of an atomic structure is essential for designing novel materials in various applications. This need goes beyond the basic knowledge of the number and types of atoms, their chemical composition, and the character of the chemical interactions. The bigger picture takes place on the quantum level which can be addressed by using the Density-functional theory (DFT). Use of DFT, however, is a computationally taxing process, and its results do not readily provide easily interpretable insight into the atomic interactions which would be useful information in material design. An alternative way to address atomic interactions is to use an interpretable machine learning approach, where a predictive DFT surrogate is constructed and analyzed. The purpose of this paper is to propose such a procedure using a modification of the recently published interpretable distance-based regression method. Our tests with a representative be
    
[^21]: 使用Prompt-Tuning的原型转向针对无需重复训练的连续学习

    Steering Prototype with Prompt-tuning for Rehearsal-free Continual Learning. (arXiv:2303.09447v1 [cs.LG])

    [http://arxiv.org/abs/2303.09447](http://arxiv.org/abs/2303.09447)

    本研究提出了一个新的连续学习模型——对比原型提示，使用任务特异性提示调整来提高原型性能，同时避免了语义漂移和原型干扰问题。基于此模型的CPP方法在四个具有挑战性的类增量学习基准测试中表现出色，相对于其他最先进的方法有4%至6%的绝对提升。该方法不需要重复训练，性能接近离线联合学习，展示了一种有前途的设计方案。

    

    原型作为类别嵌入的一种表示，已被探索用于减少连续学习情境下的内存占用或减轻遗忘。然而，基于原型的方法仍然存在语义漂移和原型干扰导致的性能急剧恶化的问题。在本研究中，我们提出了对比原型提示（CPP）方法，并展示了任务特定提示调整，当在对比学习目标上进行优化时，可以有效地解决这两个障碍并显着提高原型的性能。我们的实验表明，CPP在四个具有挑战性的类增量学习基准测试中表现出色，相对于现有最先进方法有4%至6%的绝对提升。此外，CPP不需要重复训练，它极大地缩小了连续学习和离线联合学习之间的性能差距，展示了一种有前途的Transformer体系结构下连续学习系统的设计方案。

    Prototype, as a representation of class embeddings, has been explored to reduce memory footprint or mitigate forgetting for continual learning scenarios. However, prototype-based methods still suffer from abrupt performance deterioration due to semantic drift and prototype interference. In this study, we propose Contrastive Prototypical Prompt (CPP) and show that task-specific prompt-tuning, when optimized over a contrastive learning objective, can effectively address both obstacles and significantly improve the potency of prototypes. Our experiments demonstrate that CPP excels in four challenging class-incremental learning benchmarks, resulting in 4% to 6% absolute improvements over state-of-the-art methods. Moreover, CPP does not require a rehearsal buffer and it largely bridges the performance gap between continual learning and offline joint-learning, showcasing a promising design scheme for continual learning systems under a Transformer architecture.
    
[^22]: 用稀疏输入控制高维数据

    Controlling High-Dimensional Data With Sparse Input. (arXiv:2303.09446v1 [eess.AS])

    [http://arxiv.org/abs/2303.09446](http://arxiv.org/abs/2303.09446)

    本论文提出了一种新的控制机制，即将稀疏、易于人理解的控制空间映射到生成模型的潜在空间。通过实验，此方法表现出了效率、鲁棒性和保真性，即使只有少量的输入数值。

    

    本论文解决了人在环路控制生成高度结构化数据的问题。由于现有的生成模型缺乏有效的接口，使得用户可以修改输出，这个任务变得具有挑战性。用户或手动探索不可解释的潜在空间，或者费力地注释数据标签。为了解决这个问题，我们引入了一个新的框架，其中编码器将稀疏、易于人理解的控制空间映射到生成模型的潜在空间。我们将这个框架应用于控制文本转语音合成中的韵律的任务。我们提出了一个模型，称为多实例条件变分自编码器(MICVAE)，它专门设计用于编码稀疏的韵律特征并输出完整的波形。我们通过实验证明，MICVAE表现出了稀疏的人在环路控制机制所需的良好品质：效率、鲁棒性和保真性。即使只有非常少量的输入数值(~4)，MICVAE也能让用户实现控制。

    We address the problem of human-in-the-loop control for generating highly-structured data. This task is challenging because existing generative models lack an efficient interface through which users can modify the output. Users have the option to either manually explore a non-interpretable latent space, or to laboriously annotate the data with conditioning labels. To solve this, we introduce a novel framework whereby an encoder maps a sparse, human interpretable control space onto the latent space of a generative model. We apply this framework to the task of controlling prosody in text-to-speech synthesis. We propose a model, called Multiple-Instance CVAE (MICVAE), that is specifically designed to encode sparse prosodic features and output complete waveforms. We show empirically that MICVAE displays desirable qualities of a sparse human-in-the-loop control mechanism: efficiency, robustness, and faithfulness. With even a very small number of input values (~4), MICVAE enables users to im
    
[^23]: 利用肺部分割增强CT扫描检测COVID-19的存在和严重程度

    Enhanced detection of the presence and severity of COVID-19 from CT scans using lung segmentation. (arXiv:2303.09440v1 [eess.IV])

    [http://arxiv.org/abs/2303.09440](http://arxiv.org/abs/2303.09440)

    本论文介绍了一个深度学习模型，该模型利用肺部分割进行预处理，其验证 F1 分数在预测 CT 扫描中 COVID-19 的存在和严重程度方面显著超过基线水平。

    

    改进医学成像的自动化分析将为临床医生提供更多治疗患者的选择。在2023年的AI-MIA-COV19D 竞赛中，通过 CT 扫描检测 COVID-19 的存在和严重程度的机器学习方法得到了测试和改进。本文介绍了2022年竞赛提交的深度学习模型 Cov3d 的第二个版本，通过对 CT 扫描进行肺部分割和裁剪输入到该区域的预处理步骤对模型进行了改进。在预测 CT 扫描中 COVID-19 存在方面，该模型得到了92.2%的验证宏 F1 分数，明显高于基线74%。它对任务二的验证集预测 COVID-19 的严重程度的宏 F1 分数为67%，高于基线38%。

    Improving automated analysis of medical imaging will provide clinicians more options in providing care for patients. The 2023 AI-enabled Medical Image Analysis Workshop and Covid-19 Diagnosis Competition (AI-MIA-COV19D) provides an opportunity to test and refine machine learning methods for detecting the presence and severity of COVID-19 in patients from CT scans. This paper presents version 2 of Cov3d, a deep learning model submitted in the 2022 competition. The model has been improved through a preprocessing step which segments the lungs in the CT scan and crops the input to this region. It results in a validation macro F1 score for predicting the presence of COVID-19 in the CT scans at 92.2% which is significantly above the baseline of 74%. It gives a macro F1 score for predicting the severity of COVID-19 on the validation set for task 2 as 67% which is above the baseline of 38%.
    
[^24]: 基于考虑数据异质性和爆发性的CNN股票交易模型改进

    Improving CNN-base Stock Trading By Considering Data Heterogeneity and Burst. (arXiv:2303.09407v1 [q-fin.ST])

    [http://arxiv.org/abs/2303.09407](http://arxiv.org/abs/2303.09407)

    本文提出了一种基于CNN和考虑了数据异质性和爆发性的新型规范化过程的智能股票交易模型。

    

    近年来，将智能技术应用于金融交易已有多次尝试，构建基于历史股价的自动、智能交易框架。由于金融市场的不可预测性、不确定性和波动性，研究人员也采用深度学习构建智能交易框架。本文提出使用CNN作为这种框架的核心功能，因为它能学习输入数据的空间依赖性（即行和列之间的依赖关系）。我们提出了新的规范化过程来准备股票数据，这与现有的基于深度学习的交易框架不同。具体而言，我们首先经验性地观察到股票数据本质上是异质性和爆发性的，然后从统计的角度验证了股票数据的异质性和爆发性。接下来，我们设计了数据规范化方法，以使数据的异质性得到解决。

    In recent years, there have been quite a few attempts to apply intelligent techniques to financial trading, i.e., constructing automatic and intelligent trading framework based on historical stock price. Due to the unpredictable, uncertainty and volatile nature of financial market, researchers have also resorted to deep learning to construct the intelligent trading framework. In this paper, we propose to use CNN as the core functionality of such framework, because it is able to learn the spatial dependency (i.e., between rows and columns) of the input data. However, different with existing deep learning-based trading frameworks, we develop novel normalization process to prepare the stock data. In particular, we first empirically observe that the stock data is intrinsically heterogeneous and bursty, and then validate the heterogeneity and burst nature of stock data from a statistical perspective. Next, we design the data normalization method in a way such that the data heterogeneity is 
    
[^25]: 基于价值链数据的时间图模型进行股票价格预测

    Stock Price Prediction Using Temporal Graph Model with Value Chain Data. (arXiv:2303.09406v1 [q-fin.ST])

    [http://arxiv.org/abs/2303.09406](http://arxiv.org/abs/2303.09406)

    本论文提出了一种神经网络模型，LSTM-GCN，它能够结合价值链数据中的复杂结构和时间依赖性以预测股票价格。实验表明，该模型可以捕获价值链数据中未反映在价格数据中的信息，有助于交易者优化其交易策略和最大化利润。

    

    股票价格预测是金融交易中至关重要的元素，它可以帮助交易者进行买卖和持有股票的决策。本文引入了一种基于神经网络的股票回报率预测方法，长短期记忆图卷积神经网络（LSTM-GCN）模型，它结合了图卷积网络（GCN）和长短期记忆细胞（LSTM）。GCN用于捕捉价值链数据的复杂拓扑结构和空间依赖性，而LSTM则捕捉股票回报数据的时间依赖性和动态变化。我们在由Eurostoxx 600和S＆P 500组成的两个数据集上评估了LSTM-GCN模型，实验表明该模型可以捕获价值链数据中未完全反映在价格数据中的附加信息。

    Stock price prediction is a crucial element in financial trading as it allows traders to make informed decisions about buying, selling, and holding stocks. Accurate predictions of future stock prices can help traders optimize their trading strategies and maximize their profits. In this paper, we introduce a neural network-based stock return prediction method, the Long Short-Term Memory Graph Convolutional Neural Network (LSTM-GCN) model, which combines the Graph Convolutional Network (GCN) and Long Short-Term Memory (LSTM) Cells. Specifically, the GCN is used to capture complex topological structures and spatial dependence from value chain data, while the LSTM captures temporal dependence and dynamic changes in stock returns data. We evaluated the LSTM-GCN model on two datasets consisting of constituents of Eurostoxx 600 and S&P 500. Our experiments demonstrate that the LSTM-GCN model can capture additional information from value chain data that are not fully reflected in price data, a
    
[^26]: 一种层次Transformer动态变分自编码器的语音建模

    Speech Modeling with a Hierarchical Transformer Dynamical VAE. (arXiv:2303.09404v1 [eess.AS])

    [http://arxiv.org/abs/2303.09404](http://arxiv.org/abs/2303.09404)

    HiT-DVAE是一种层次Transformer动态变分自编码器，能够优于其他DVAEs在语音频谱建模方面。它具有两个潜变量水平和简单的训练过程，并且具有很高的潜力在低级别语音处理方面。

    

    动态变分自编码器（DVAEs）是一类潜变量深度生成模型，扩展了VAE以对观察到的数据序列和相应的潜向量序列进行建模。在文献中的几乎所有DVAEs中，每个序列内部和两个序列之间的时间依赖性都由循环神经网络来建模。在本文中，我们提出使用Hierarchical Transformer DVAE（HiT-DVAE）模拟语音信号，它是一种具有两个潜变量水平（序列级和帧级）的DVAE，并且其中时间依赖性是使用Transformer架构实现的。我们展示了HiT-DVAE在语音频谱建模方面优于其他若干DVAEs，同时还能够实现更简单的训练过程，显示出其在下游低级别语音处理任务（如语音增强）中具有很高的潜力。

    The dynamical variational autoencoders (DVAEs) are a family of latent-variable deep generative models that extends the VAE to model a sequence of observed data and a corresponding sequence of latent vectors. In almost all the DVAEs of the literature, the temporal dependencies within each sequence and across the two sequences are modeled with recurrent neural networks. In this paper, we propose to model speech signals with the Hierarchical Transformer DVAE (HiT-DVAE), which is a DVAE with two levels of latent variable (sequence-wise and frame-wise) and in which the temporal dependencies are implemented with the Transformer architecture. We show that HiT-DVAE outperforms several other DVAEs for speech spectrogram modeling, while enabling a simpler training procedure, revealing its high potential for downstream low-level speech processing tasks such as speech enhancement.
    
[^27]: 学习控制屏障函数的可行性约束

    Learning Feasibility Constraints for Control Barrier Functions. (arXiv:2303.09403v1 [math.OC])

    [http://arxiv.org/abs/2303.09403](http://arxiv.org/abs/2303.09403)

    本文通过机器学习技术学习控制屏障函数（CBFs）的新可行性约束，并提出了一种基于采样的学习方法来强制实施该约束，具有在约束优化控制问题中的实践应用价值。

    

    通过使用控制屏障函数（CBFs）和控制Lyapunov函数（CLFs），已经证明在稳定具有所需状态和状态/控制约束的仿射控制系统时，优化二次成本可以通过将其减少到一系列二次规划问题（QPs）。在本文中，我们采用机器学习技术来确保这些QPs的可行性，这是一个具有挑战性的问题，特别是对于需要高阶CBFs的高相对次约束。为此，我们提出了一种基于采样的学习方法，用于学习CBFs的新可行性约束；然后，通过添加到QPs中的另一个高阶CBF来强制执行此约束。经过递归训练算法的不断改进，学习的可行性约束的准确性得到提高。我们重点针对机器人控制问题和在未知环境中进行自主驾驶的约束优化控制问题，展示所提出的学习方法的优势。

    It has been shown that optimizing quadratic costs while stabilizing affine control systems to desired (sets of) states subject to state and control constraints can be reduced to a sequence of Quadratic Programs (QPs) by using Control Barrier Functions (CBFs) and Control Lyapunov Functions (CLFs). In this paper, we employ machine learning techniques to ensure the feasibility of these QPs, which is a challenging problem, especially for high relative degree constraints where High Order CBFs (HOCBFs) are required. To this end, we propose a sampling-based learning approach to learn a new feasibility constraint for CBFs; this constraint is then enforced by another HOCBF added to the QPs. The accuracy of the learned feasibility constraint is recursively improved by a recurrent training algorithm. We demonstrate the advantages of the proposed learning approach to constrained optimal control problems with specific focus on a robot control problem and on autonomous driving in an unknown environm
    
[^28]: 利用Twitter情感分析预测加密货币价格

    Cryptocurrency Price Prediction using Twitter Sentiment Analysis. (arXiv:2303.09397v1 [q-fin.ST])

    [http://arxiv.org/abs/2303.09397](http://arxiv.org/abs/2303.09397)

    本研究使用历史价格和Twitter情感分析预测比特币价格，并取得了不错的效果。情感预测的平均绝对百分比误差为9.45％，价格预测的平均绝对百分比误差为3.6％。

    

    随着加密货币的波动性及多样化的意见，在许多社交媒体平台上都成为了讨论的中心话题。Twitter 迅速成为新闻来源和比特币讨论的媒介。我们的算法旨在利用历史价格和推文情感来预测比特币的价格。在本研究中，我们开发了一种端到端模型，可使用双向编码器转换的神经网络模型预测推文集的情感，并使用预测的情感以及历史加密货币价格数据、推文数量、用户的追随者数量以及用户是否通过验证来预测比特币的价格。情感预测的平均绝对百分比误差为9.45％，反应了实时数据和测试数据的平均误差。而价格预测的平均绝对百分比误差则为3.6％。

    The cryptocurrency ecosystem has been the centre of discussion on many social media platforms, following its noted volatility and varied opinions. Twitter is rapidly being utilised as a news source and a medium for bitcoin discussion. Our algorithm seeks to use historical prices and sentiment of tweets to forecast the price of Bitcoin. In this study, we develop an end-to-end model that can forecast the sentiment of a set of tweets (using a Bidirectional Encoder Representations from Transformers - based Neural Network Model) and forecast the price of Bitcoin (using Gated Recurrent Unit) using the predicted sentiment and other metrics like historical cryptocurrency price data, tweet volume, a user's following, and whether or not a user is verified. The sentiment prediction gave a Mean Absolute Percentage Error of 9.45%, an average of real-time data, and test data. The mean absolute percent error for the price prediction was 3.6%.
    
[^29]: 基于临床文本报告的文本到心电图合成

    Text-to-ECG: 12-Lead Electrocardiogram Synthesis conditioned on Clinical Text Reports. (arXiv:2303.09395v1 [cs.CL])

    [http://arxiv.org/abs/2303.09395](http://arxiv.org/abs/2303.09395)

    本文提出了一种基于临床文本报告的自回归生成模型Auto-TTE，用于生成逼真的12导联心电图，相对于传统心电图生成模型并不只考虑单个心电信号的限制，实现了更具复杂性、更加真实的生成，在文本到心电图合成领域具有更好的效果。

    

    心电图合成是一种研究领域，旨在生成逼真的合成心电信号以供医疗使用，无需担心注释成本或临床数据隐私限制。传统的心电图生成模型只考虑单个心电信号，并使用基于GAN的生成模型。这些模型只能生成单导联样本，并要求每个诊断分类进行单独的培训。心电图的诊断分类无法捕捉心电图之间的复杂差异，这些差异取决于各种特征（例如患者人口统计学细节，共存的诊断分类等）。为了解决这些挑战，我们提出了一个文本到心电图任务，其中使用文本输入生成心电图输出。然后，我们首次提出了一个自回归生成模型Auto-TTE，它是一个基于临床文本报告条件的生成模型，用于合成12导联心电图。我们将我们的模型与文本到语音和文本到心电图领域的其他代表性模型进行了比较。结果表明，我们提出的方法在文本到心电图合成方面的表现优于最先进的模型。

    Electrocardiogram (ECG) synthesis is the area of research focused on generating realistic synthetic ECG signals for medical use without concerns over annotation costs or clinical data privacy restrictions. Traditional ECG generation models consider a single ECG lead and utilize GAN-based generative models. These models can only generate single lead samples and require separate training for each diagnosis class. The diagnosis classes of ECGs are insufficient to capture the intricate differences between ECGs depending on various features (e.g. patient demographic details, co-existing diagnosis classes, etc.). To alleviate these challenges, we present a text-to-ECG task, in which textual inputs are used to produce ECG outputs. Then we propose Auto-TTE, an autoregressive generative model conditioned on clinical text reports to synthesize 12-lead ECGs, for the first time to our knowledge. We compare the performance of our model with other representative models in text-to-speech and text-to-
    
[^30]: 关于线性情境赌博机中错误规定与次优间隙的相互作用研究

    On the Interplay Between Misspecification and Sub-optimality Gap in Linear Contextual Bandits. (arXiv:2303.09390v1 [cs.LG])

    [http://arxiv.org/abs/2303.09390](http://arxiv.org/abs/2303.09390)

    本文研究了线性情境赌博机在错误规定的情境下的算法问题，提出一种新算法，将在一定水平内误差和最小次优间隙相互制约，以常数误差上限实现间隙相关的度量。

    

    本文研究了线性情境赌博机在错误规定的情境下，期望奖励函数可以以线性函数类来逼近的情况。我们提出了一种基于新的数据选择方案的算法，该算法仅选择具有大不确定性的情境向量进行在线回归。当误差规定水平被$\zeta>0$控制时，我们的算法的误差上限与好的指定情况下的结果相同。我们证明了一个现有的算法也可以在不知道亚优间隙$\Delta$的情况下实现间隙相关的常数误差上限。在Lattimore et al.（2020）的作品基础上，我们提供了一个下界，表明了错误规定和次优间隙之间的相互作用。

    We study linear contextual bandits in the misspecified setting, where the expected reward function can be approximated by a linear function class up to a bounded misspecification level $\zeta>0$. We propose an algorithm based on a novel data selection scheme, which only selects the contextual vectors with large uncertainty for online regression. We show that, when the misspecification level $\zeta$ is dominated by $\tilde O (\Delta / \sqrt{d})$ with $\Delta$ being the minimal sub-optimality gap and $d$ being the dimension of the contextual vectors, our algorithm enjoys the same gap-dependent regret bound $\tilde O (d^2/\Delta)$ as in the well-specified setting up to logarithmic factors. In addition, we show that an existing algorithm SupLinUCB (Chu et al., 2011) can also achieve a gap-dependent constant regret bound without the knowledge of sub-optimality gap $\Delta$. Together with a lower bound adapted from Lattimore et al. (2020), our result suggests an interplay between misspecific
    
[^31]: LLMSecEval: 一个用于安全评估的自然语言提示数据集

    LLMSecEval: A Dataset of Natural Language Prompts for Security Evaluations. (arXiv:2303.09384v1 [cs.SE])

    [http://arxiv.org/abs/2303.09384](http://arxiv.org/abs/2303.09384)

    本文提出了一个 LLMSecEval 数据集，其中包含 150 个自然语言提示，可用于评估大型语言模型在生成容易出现安全漏洞的代码时的安全性能。

    

    大型语言模型（LLM）如 Codex 在代码自动补全和生成任务方面具有强大的能力，因为它们通过公开可用的代码从数十亿行代码中进行训练。此外，这些模型能够通过从公共 GitHub 仓库学习语言和编程实践来生成来自自然语言描述的代码片段。尽管 LLM 承诺实现软件应用的 NL 驱动部署，但是它们生成的代码的安全性尚未得到广泛调查和记录。在这项工作中，我们提出了 LLMSecEval，这是一个包含 150 个 NL 提示的数据集，可用于评估此类模型的安全性能。这些提示是基于MITRE的前25个常见弱点列表中容易出现各种安全漏洞的代码片段的自然语言描述。我们数据集中的每个提示都配有一个安全实现示例，以便与由LLM生成的代码进行比较评估。

    Large Language Models (LLMs) like Codex are powerful tools for performing code completion and code generation tasks as they are trained on billions of lines of code from publicly available sources. Moreover, these models are capable of generating code snippets from Natural Language (NL) descriptions by learning languages and programming practices from public GitHub repositories. Although LLMs promise an effortless NL-driven deployment of software applications, the security of the code they generate has not been extensively investigated nor documented. In this work, we present LLMSecEval, a dataset containing 150 NL prompts that can be leveraged for assessing the security performance of such models. Such prompts are NL descriptions of code snippets prone to various security vulnerabilities listed in MITRE's Top 25 Common Weakness Enumeration (CWE) ranking. Each prompt in our dataset comes with a secure implementation example to facilitate comparative evaluations against code produced by
    
[^32]: 多模态可微无监督特征选择

    Multi-modal Differentiable Unsupervised Feature Selection. (arXiv:2303.09381v1 [cs.LG])

    [http://arxiv.org/abs/2303.09381](http://arxiv.org/abs/2303.09381)

    该论文提出了一个多模态无监督特征选择框架用于识别具有生物学相关性的信息变量。通过两个基于拉普拉斯的评分算子和可区分的掩码来增强图拉普拉斯所捕捉的结构的准确性，该方法在真实的多模态数据集上表现优于现有的无监督和监督方法。

    

    多模态高通量生物数据既是一个巨大的科学机遇，又是一个重大的计算挑战。在多模态测量中，每个样本同时被两个或更多组传感器观察。在这种情况下，两个模态中的许多观察变量通常都是不相关的，并且不 carry information about the phenomenon of interest。我们在此提出了一个多模态无监督特征选择框架，它基于耦合的高维测量识别信息变量。为此，我们提出了两个基于拉普拉斯的评分算子。我们将得分与可区分的掩码结合起来，以遮蔽无用特征并增强图拉普拉斯所捕捉的结构的准确性。该方法在合成和真实的多模态数据集上进行了评估，包括fMRI和基因表达数据。我们的特征选择方法优于现有的无监督和监督方法，并实现了生物相关的发现。

    Multi-modal high throughput biological data presents a great scientific opportunity and a significant computational challenge. In multi-modal measurements, every sample is observed simultaneously by two or more sets of sensors. In such settings, many observed variables in both modalities are often nuisance and do not carry information about the phenomenon of interest. Here, we propose a multi-modal unsupervised feature selection framework: identifying informative variables based on coupled high-dimensional measurements. Our method is designed to identify features associated with two types of latent low-dimensional structures: (i) shared structures that govern the observations in both modalities and (ii) differential structures that appear in only one modality. To that end, we propose two Laplacian-based scoring operators. We incorporate the scores with differentiable gates that mask nuisance features and enhance the accuracy of the structure captured by the graph Laplacian. The perform
    
[^33]: 3D蒙版自编码和伪标签用于异构婴儿脑 MRI 领域间适应性标记

    3D Masked Autoencoding and Pseudo-labeling for Domain Adaptive Segmentation of Heterogeneous Infant Brain MRI. (arXiv:2303.09373v1 [cs.CV])

    [http://arxiv.org/abs/2303.09373](http://arxiv.org/abs/2303.09373)

    本文提出了一种名为 MAPSeg 的新框架，采用 3D 蒙版自编码和伪标签的方式，实现了跨年龄、跨模态和跨场景下对婴儿脑 MRI 中亚皮质区域的分割，充分考虑不同 MRI 扫描仪、供应商或采集序列以及不同的神经发育阶段所造成的内在异质性，提高了分割结果的鲁棒性。

    

    婴儿脑 MRI 在跨年龄、跨模态、跨场景下实现鲁棒的分割仍然是具有挑战性的。本文介绍了一种名为 MAPSeg 的新框架，它使用 3D 蒙版自编码和蒙版伪标签的方式来对婴儿脑MRI的不同亚皮质区域进行分割，并联合学习标记源域数据和未标记目标域数据，以提高分割结果的鲁棒性。

    Robust segmentation of infant brain MRI across multiple ages, modalities, and sites remains challenging due to the intrinsic heterogeneity caused by different MRI scanners, vendors, or acquisition sequences, as well as varying stages of neurodevelopment. To address this challenge, previous studies have explored domain adaptation (DA) algorithms from various perspectives, including feature alignment, entropy minimization, contrast synthesis (style transfer), and pseudo-labeling. This paper introduces a novel framework called MAPSeg (Masked Autoencoding and Pseudo-labelling Segmentation) to address the challenges of cross-age, cross-modality, and cross-site segmentation of subcortical regions in infant brain MRI. Utilizing 3D masked autoencoding as well as masked pseudo-labeling, the model is able to jointly learn from labeled source domain data and unlabeled target domain data. We evaluated our framework on expert-annotated datasets acquired from different ages and sites. MAPSeg consist
    
[^34]: 通过状态空间划分实现目标导向的离线强化学习

    Goal-conditioned Offline Reinforcement Learning through State Space Partitioning. (arXiv:2303.09367v1 [cs.LG])

    [http://arxiv.org/abs/2303.09367](http://arxiv.org/abs/2303.09367)

    本文提出了一种通过状态空间划分实现目标导向的离线强化学习方法，并采用补充的优势加权方案，以解决分布转移和多模式问题。

    

    离线强化学习旨在仅使用离线数据集来推断出顺序决策策略。本文提出了一种想要学习实现多个不同目标或结果的目标导向决策策略的方法。为了解决分布转移和多模式问题，我们提出了一种补充的优势加权方案。

    Offline reinforcement learning (RL) aims to infer sequential decision policies using only offline datasets. This is a particularly difficult setup, especially when learning to achieve multiple different goals or outcomes under a given scenario with only sparse rewards. For offline learning of goal-conditioned policies via supervised learning, previous work has shown that an advantage weighted log-likelihood loss guarantees monotonic policy improvement. In this work we argue that, despite its benefits, this approach is still insufficient to fully address the distribution shift and multi-modality problems. The latter is particularly severe in long-horizon tasks where finding a unique and optimal policy that goes from a state to the desired goal is challenging as there may be multiple and potentially conflicting solutions. To tackle these challenges, we propose a complementary advantage-based weighting scheme that introduces an additional source of inductive bias: given a value-based part
    
[^35]: 基于上下文学习的医学时间约束抽取范围研究

    The Scope of In-Context Learning for the Extraction of Medical Temporal Constraints. (arXiv:2303.09366v1 [cs.CL])

    [http://arxiv.org/abs/2303.09366](http://arxiv.org/abs/2303.09366)

    本研究定义了一种MTC分类，开发了一种基于CFG模型的抽取方法，并通过ICL自动提取和标准化DUGs中的MTC，有望通过定义安全的患者活动模式来推进以患者为中心的医疗应用。

    

    药物治疗通常对患者的日常活动施加时间约束。违反医学时间约束（MTC）会导致缺乏治疗依从性，以及不良的健康结果和增加的医疗费用。这些MTC在患者教育材料和临床文本中的药物使用指南（DUGs）中被发现。通过在计算上表示DUGs中的MTC，将有助于通过帮助定义安全的患者活动模式来推进以患者为中心的医疗应用。我们定义了一种新颖的在DUGs中发现的MTC分类法，并开发了一种基于上下文无关文法（CFG）的模型来计算地表示MTC。此外，我们发布了三个新的数据集，共计N = 836个带标准化的MTC标记的DUGs。我们开发了一种上下文学习（ICL）解决方案，用于自动提取和标准化DUGs中发现的MTC，跨所有数据集实现了平均F1得分0.62。最后，我们对ICL模型进行了严格的研究。

    Medications often impose temporal constraints on everyday patient activity. Violations of such medical temporal constraints (MTCs) lead to a lack of treatment adherence, in addition to poor health outcomes and increased healthcare expenses. These MTCs are found in drug usage guidelines (DUGs) in both patient education materials and clinical texts. Computationally representing MTCs in DUGs will advance patient-centric healthcare applications by helping to define safe patient activity patterns. We define a novel taxonomy of MTCs found in DUGs and develop a novel context-free grammar (CFG) based model to computationally represent MTCs from unstructured DUGs. Additionally, we release three new datasets with a combined total of N = 836 DUGs labeled with normalized MTCs. We develop an in-context learning (ICL) solution for automatically extracting and normalizing MTCs found in DUGs, achieving an average F1 score of 0.62 across all datasets. Finally, we rigorously investigate ICL model perfor
    
[^36]: 国家癌症研究所影像数据共享平台：计算病理学可重复研究的基础

    The NCI Imaging Data Commons as a platform for reproducible research in computational pathology. (arXiv:2303.09354v1 [cs.CV])

    [http://arxiv.org/abs/2303.09354](http://arxiv.org/abs/2303.09354)

    国家癌症研究所影像数据共享平台 (IDC) 旨在促进计算病理学领域的研究可重复性，实现了 FAIR 原则，提供公共库和云端技术支持，方便使用机器学习方法进行癌症组织分类研究。

    

    目的：可重复性对于将计算病理学（CompPath）中基于机器学习（ML）的解决方案转化为实践至关重要。然而，越来越多的研究报告难以重复 ML 结果的困难。国家癌症研究所影像数据共享平台（IDC）是一个公共库，包含 >120 个癌症图像收集，包括 >38,000 张全切片图像（WSIs），旨在与云端 ML 服务一起使用。本文探讨了 IDC 促进 CompPath 研究可重复性的潜力。 材料和方法：IDC 实现了 FAIR 原则：所有图像都根据 DICOM 标准进行编码，具有持久化标识符、可通过丰富的元数据进行发现，并可通过开放式工具访问。借此优势，我们在 IDC 的不同数据集上实现了两个实验，针对肺癌组织分类的一种代表性基于 ML 的方法进行了训练和/或评估。为评估可重复性，实验被多次运行。

    Objective: Reproducibility is critical for translating machine learning-based (ML) solutions in computational pathology (CompPath) into practice. However, an increasing number of studies report difficulties in reproducing ML results. The NCI Imaging Data Commons (IDC) is a public repository of >120 cancer image collections, including >38,000 whole-slide images (WSIs), that is designed to be used with cloud-based ML services. Here, we explore the potential of the IDC to facilitate reproducibility of CompPath research.  Materials and Methods: The IDC realizes the FAIR principles: All images are encoded according to the DICOM standard, persistently identified, discoverable via rich metadata, and accessible via open tools. Taking advantage of this, we implemented two experiments in which a representative ML-based method for classifying lung tumor tissue was trained and/or evaluated on different datasets from the IDC. To assess reproducibility, the experiments were run multiple times with i
    
[^37]: 利用特权信息进行无监督领域自适应

    Unsupervised domain adaptation by learning using privileged information. (arXiv:2303.09350v1 [cs.LG])

    [http://arxiv.org/abs/2303.09350](http://arxiv.org/abs/2303.09350)

    本文提出利用特权信息进行领域适应（DALUPI）算法，以在学习中放宽假设条件并提高样本效率，通过减少错误来促进医学图像分析等应用的发展。

    

    成功的无监督领域自适应（UDA）只在强假设条件下得以实现，如协变量移位和输入领域之间的重叠。后者在高维应用中经常被违反，比如图像分类，在面对这种挑战时，图像分类仍然是算法开发的灵感和基准。本文表明，获取源域和目标域样本的有关信息能够帮助放宽这些假设，并在学习中提高样本效率，代价是收集更丰富的变量集。我们称之为利用特权信息进行领域适应（DALUPI）。为此，我们提出了一个简单的两阶段学习算法，并提出了一个针对多标签图像分类的实用端到端算法，受到我们分析的启发。通过一系列实验，包括医学图像分析的应用，我们证明了在学习过程中加入特权信息可以减少错误。

    Successful unsupervised domain adaptation (UDA) is guaranteed only under strong assumptions such as covariate shift and overlap between input domains. The latter is often violated in high-dimensional applications such as image classification which, despite this challenge, continues to serve as inspiration and benchmark for algorithm development. In this work, we show that access to side information about examples from the source and target domains can help relax these assumptions and increase sample efficiency in learning, at the cost of collecting a richer variable set. We call this domain adaptation by learning using privileged information (DALUPI). Tailored for this task, we propose a simple two-stage learning algorithm inspired by our analysis and a practical end-to-end algorithm for multi-label image classification. In a suite of experiments, including an application to medical image analysis, we demonstrate that incorporating privileged information in learning can reduce errors i
    
[^38]: 基于深度卷积神经网络伪影降噪的稀疏视图CT图像自动出血检测的改进

    Improving Automated Hemorrhage Detection in Sparse-view Computed Tomography via Deep Convolutional Neural Network based Artifact Reduction. (arXiv:2303.09340v1 [eess.IV])

    [http://arxiv.org/abs/2303.09340](http://arxiv.org/abs/2303.09340)

    本文提出了一种基于深度卷积神经网络的伪影降噪方法，用于改善稀疏视图下自动出血检测的图像质量，并证明其能够与完全采样的图像进行同等精确度的分类和检测。

    

    颅内出血是一种严重的健康问题，需要快速且常常非常密集的医疗治疗。为了诊断，通常要进行颅部计算机断层扫描（CCT）扫描。然而，由于辐射引起的增加的健康风险是一个问题。降低这种潜在风险的最重要策略是尽可能保持辐射剂量低，并与诊断任务一致。 稀疏视图CT可以通过减少所采集的视图总数，从而降低剂量，是一种有效的策略，但代价是降低图像质量。在这项工作中，我们使用U-Net架构来减少稀疏视图CCT的伪影，从稀疏视图中预测完全采样的重建图像。我们使用一个卷积神经网络对出血的检测和分类进行评估，并在完全采样的CCT上进行训练。我们的结果表明，伪影降噪后的CCT图像进行自动分类和检测的准确性与完全采样的CCT图像没有明显差异。

    Intracranial hemorrhage poses a serious health problem requiring rapid and often intensive medical treatment. For diagnosis, a Cranial Computed Tomography (CCT) scan is usually performed. However, the increased health risk caused by radiation is a concern. The most important strategy to reduce this potential risk is to keep the radiation dose as low as possible and consistent with the diagnostic task. Sparse-view CT can be an effective strategy to reduce dose by reducing the total number of views acquired, albeit at the expense of image quality. In this work, we use a U-Net architecture to reduce artifacts from sparse-view CCTs, predicting fully sampled reconstructions from sparse-view ones. We evaluate the hemorrhage detectability in the predicted CCTs with a hemorrhage classification convolutional neural network, trained on fully sampled CCTs to detect and classify different sub-types of hemorrhages. Our results suggest that the automated classification and detection accuracy of hemo
    
[^39]: ExoplANNET: 一种用于检测和确认径向速度数据中行星信号的深度学习算法

    ExoplANNET: A deep learning algorithm to detect and identify planetary signals in radial velocity data. (arXiv:2303.09335v1 [astro-ph.EP])

    [http://arxiv.org/abs/2303.09335](http://arxiv.org/abs/2303.09335)

    本文介绍了一种神经网络算法ExoplANNET，旨在解决径向速度法检测系外行星的挑战，在存在与星体相关的噪声的情况下进行行星信号的检测和分类，经过合成数据和真实数据的测试，取得了有前途的结果。

    

    利用径向速度法检测系外行星的方法在于探测由未知下部恒星伴星引起的恒星速度变化。仪器误差、不规则时间采样以及源自于恒星内在变异的不同噪声源可能会阻碍数据的解释，甚至导致虚假探测结果。近期，在系外行星领域出现了使用机器学习算法的研究，其中一些结果甚至超过了传统技术的结果。本研究旨在探索神经网络在径向速度法中的应用范围，特别是在存在与星体相关的噪声的情况下进行系外行星探测。提出了一种神经网络算法，以代替径向速度法检测到的信号的重要性计算并将其分类为行星还是非行星来源。该算法使用已知有行星系统的合成数据进行训练，然后在未知有行星系统的合成和真实数据上进行测试。ExoplANNET算法展现出有前途的结果，并表明深度学习算法可以在系外行星的鉴定和表征中发挥重要作用。

    The detection of exoplanets with the radial velocity method consists in detecting variations of the stellar velocity caused by an unseen sub-stellar companion. Instrumental errors, irregular time sampling, and different noise sources originating in the intrinsic variability of the star can hinder the interpretation of the data, and even lead to spurious detections. In recent times, work began to emerge in the field of extrasolar planets that use Machine Learning algorithms, some with results that exceed those obtained with the traditional techniques in the field. We seek to explore the scope of the neural networks in the radial velocity method, in particular for exoplanet detection in the presence of correlated noise of stellar origin. In this work, a neural network is proposed to replace the computation of the significance of the signal detected with the radial velocity method and to classify it as of planetary origin or not. The algorithm is trained using synthetic data of systems wi
    
[^40]: 基于模型的概念漂移解释

    Model Based Explanations of Concept Drift. (arXiv:2303.09331v1 [cs.LG])

    [http://arxiv.org/abs/2303.09331](http://arxiv.org/abs/2303.09331)

    本文提出了一种新的技术，通过各种解释技术，以空间特征的特征变化来表征概念漂移，可以适用于任何模型，并在多个数据集上评估和优化方法。

    

    概念漂移是指生成观测数据的分布随时间改变的现象。本文提出了一种新的技术，通过各种解释技术，以空间特征的特征变化来表征概念漂移。我们提出了一种将概念漂移的解释简化为重要特征及其空间特征识别的方法，该方法可以适用于任何机器学习模型。在多个数据集上评估了我们的方法，并显示它可以准确地表征概念漂移，并在可解释性方面优于现有方法。

    The notion of concept drift refers to the phenomenon that the distribution generating the observed data changes over time. If drift is present, machine learning models can become inaccurate and need adjustment. While there do exist methods to detect concept drift or to adjust models in the presence of observed drift, the question of explaining drift, i.e., describing the potentially complex and high dimensional change of distribution in a human-understandable fashion, has hardly been considered so far. This problem is of importance since it enables an inspection of the most prominent characteristics of how and where drift manifests itself. Hence, it enables human understanding of the change and it increases acceptance of life-long learning models. In this paper, we present a novel technology characterizing concept drift in terms of the characteristic change of spatial features based on various explanation techniques. To do so, we propose a methodology to reduce the explanation of conce
    
[^41]: 股票趋势预测：一种语义分割方法

    Stock Trend Prediction: A Semantic Segmentation Approach. (arXiv:2303.09323v1 [q-fin.ST])

    [http://arxiv.org/abs/2303.09323](http://arxiv.org/abs/2303.09323)

    本研究提出了一种采用语义分割和完全二维卷积编码器解码器预测股票价格未来趋势的方法

    

    市场金融预测是深度学习中的一个趋势领域。深度学习模型能够解决股市数据中的传统问题，如其极其复杂的动态和长期时间相关性。为了捕捉这些时间序列之间的时间关系，采用了循环神经网络。然而，对于循环模型来说，学习保持长期信息是困难的。卷积神经网络已经被用于更好地捕捉动态并提取短期和长期预测的特征。然而，语义分割及其设计良好的全卷积网络从未用于时间序列密集分类的研究。我们提出了一种使用完全二维卷积编码器解码器预测长期日常股价变动趋势的新方法。我们生成包含T天的每日价格的输入帧。目标是通过像素级的分类来预测未来的趋势。

    Market financial forecasting is a trending area in deep learning. Deep learning models are capable of tackling the classic challenges in stock market data, such as its extremely complicated dynamics as well as long-term temporal correlation. To capture the temporal relationship among these time series, recurrent neural networks are employed. However, it is difficult for recurrent models to learn to keep track of long-term information. Convolutional Neural Networks have been utilized to better capture the dynamics and extract features for both short- and long-term forecasting. However, semantic segmentation and its well-designed fully convolutional networks have never been studied for time-series dense classification. We present a novel approach to predict long-term daily stock price change trends with fully 2D-convolutional encoder-decoders. We generate input frames with daily prices for a time-frame of T days. The aim is to predict future trends by pixel-wise classification of the cur
    
[^42]: 一个新视角下的可解释性：将分层和领域知识与生物医学应用相结合。

    Interpretability from a new lens: Integrating Stratification and Domain knowledge for Biomedical Applications. (arXiv:2303.09322v1 [cs.LG])

    [http://arxiv.org/abs/2303.09322](http://arxiv.org/abs/2303.09322)

    本文提出了一种新的计算策略，将生物医学问题数据集分层为k折交叉验证(CVs)，并将领域知识解释技术嵌入到当前最先进的IML框架中。这种方法可以提高模型的稳定性、建立信任，并为IML模型生成的结果提供解释。

    

    机器学习(ML)技术在生物医学领域的应用日益重要，特别是在COVID-19大流行后所产生的大量数据中。然而，由于生物医学数据集的复杂性和黑盒ML模型的使用，领域专家可能会产生缺乏信任和采纳的情况。因此，出现了可解释的ML(IML)方法，但是生物医学数据集中的维度诅咒可能会导致模型不稳定。本文提出了一种新的计算策略，用于将生物医学问题数据集分层为k折交叉验证(CVs)，并将领域知识解释技术嵌入到当前最先进的IML框架中。这种方法可以提高模型的稳定性、建立信任，并为IML模型生成的结果提供解释。具体来说，模型结果，如聚合特征权重重要性，可以与更详细的领域特定知识相联系。

    The use of machine learning (ML) techniques in the biomedical field has become increasingly important, particularly with the large amounts of data generated by the aftermath of the COVID-19 pandemic. However, due to the complex nature of biomedical datasets and the use of black-box ML models, a lack of trust and adoption by domain experts can arise. In response, interpretable ML (IML) approaches have been developed, but the curse of dimensionality in biomedical datasets can lead to model instability. This paper proposes a novel computational strategy for the stratification of biomedical problem datasets into k-fold cross-validation (CVs) and integrating domain knowledge interpretation techniques embedded into the current state-of-the-art IML frameworks. This approach can improve model stability, establish trust, and provide explanations for outcomes generated by trained IML models. Specifically, the model outcome, such as aggregated feature weight importance, can be linked to further d
    
[^43]: 解释群体实例的反事实推理，用于可解释人工智能：群体反事实推理的使用案例、算法和用户研究。

    Explaining Groups of Instances Counterfactually for XAI: A Use Case, Algorithm and User Study for Group-Counterfactuals. (arXiv:2303.09297v1 [cs.AI])

    [http://arxiv.org/abs/2303.09297](http://arxiv.org/abs/2303.09297)

    该论文探索了一种新的用例，使用“群体反事实”集体解释类似实例的组，提出了一种新颖的群体反事实算法来生成高覆盖率的解释，适合人类对连贯、广泛解释的喜好。

    

    反事实解释是一种越来越受欢迎的事后解释形式，因为它们适用于各种问题领域、提供了法律合规性（例如符合《通用数据保护条例》），并且依赖于人类解释的对比性质。虽然反事实解释通常用于解释单个预测实例，但我们探索了一个新的用例，即使用“群体反事实”来集体解释类似实例的组（例如突出显示一组患者中疾病重复出现的模式）。这些群体反事实满足人们对包含多个事件/实例的连贯、广泛解释的偏好。我们提出了一种新颖的群体反事实算法来生成高覆盖率的解释，这种解释又忠实于待解释模型。还使用客观（即准确性）和主观（即信心、解释满意度）评估了该解释策略在大型控制用户研究（N=207）中的表现。

    Counterfactual explanations are an increasingly popular form of post hoc explanation due to their (i) applicability across problem domains, (ii) proposed legal compliance (e.g., with GDPR), and (iii) reliance on the contrastive nature of human explanation. Although counterfactual explanations are normally used to explain individual predictive-instances, we explore a novel use case in which groups of similar instances are explained in a collective fashion using ``group counterfactuals'' (e.g., to highlight a repeating pattern of illness in a group of patients). These group counterfactuals meet a human preference for coherent, broad explanations covering multiple events/instances. A novel, group-counterfactual algorithm is proposed to generate high-coverage explanations that are faithful to the to-be-explained model. This explanation strategy is also evaluated in a large, controlled user study (N=207), using objective (i.e., accuracy) and subjective (i.e., confidence, explanation satisfa
    
[^44]: 图像分类器泄露其类别的敏感属性

    Image Classifiers Leak Sensitive Attributes About Their Classes. (arXiv:2303.09289v1 [cs.LG])

    [http://arxiv.org/abs/2303.09289](http://arxiv.org/abs/2303.09289)

    该论文探讨了图像分类器存在的隐私泄露问题，提出的Class Attribute Inference Attack（Caia）能够从黑盒设置中准确地推断出敏感属性，包括个人的发色、性别和种族，这表明在鲁棒性和隐私之间存在权衡。

    

    基于神经网络的图像分类器是计算机视觉任务的有力工具，但它们无意中透露了有关其类别的敏感属性信息，引起了对它们的隐私的关注。为了研究这种隐私泄漏，我们引入了第一个Class Attribute Inference Attack（Caia），利用最近在文本到图像合成方面的进展，在黑盒设置中推断出单个类别的敏感属性，同时与相关的白盒攻击相竞争。在人脸识别领域进行的广泛实验表明，Caia能够准确地推断出未公开的敏感属性，例如个人的发色、性别和种族外貌，这些属性不属于训练标签。有趣的是，我们证明了对抗性鲁棒模型比标准模型更容易泄露隐私，表明在鲁棒性和隐私之间存在权衡。

    Neural network-based image classifiers are powerful tools for computer vision tasks, but they inadvertently reveal sensitive attribute information about their classes, raising concerns about their privacy. To investigate this privacy leakage, we introduce the first Class Attribute Inference Attack (Caia), which leverages recent advances in text-to-image synthesis to infer sensitive attributes of individual classes in a black-box setting, while remaining competitive with related white-box attacks. Our extensive experiments in the face recognition domain show that Caia can accurately infer undisclosed sensitive attributes, such as an individual's hair color, gender and racial appearance, which are not part of the training labels. Interestingly, we demonstrate that adversarial robust models are even more vulnerable to such privacy leakage than standard models, indicating that a trade-off between robustness and privacy exists.
    
[^45]: 使用设计多样性探索深度学习对自然图像污染的韧性

    Exploring Resiliency to Natural Image Corruptions in Deep Learning using Design Diversity. (arXiv:2303.09283v1 [cs.LG])

    [http://arxiv.org/abs/2303.09283](http://arxiv.org/abs/2303.09283)

    本文研究了深度学习图像分类器集合的多样性指标、准确性和对自然图像污染的韧性之间的关系，并发现实现设计选择的多样性可以降低故障模式的数量。

    

    本文研究了深度学习图像分类器集合的多样性指标、准确性和对自然图像污染的韧性之间的关系。我们调查了基于归因的多样性度量的潜力，以改善传统基于预测的多样性已知的准确性-多样性折衷所存在的问题。我们的动机基于设计多样性的分析研究，其显示如果实现设计选择的多样性，则可以降低常见故障模式的数量。我们通过将ResNet50用作比较基线，评估了多个单独的DL模型结构针对自然图像污染引起的数据集分布偏移的韧性。我们比较了通过神经结构搜索技术独立训练或训练的具有多样性模型结构的集合，并评估了基于预测和基于归因的多样性与最终集合准确性的相关性。我们评估了一组多样性度量，进一步说明了设计多样性的重要性。

    In this paper, we investigate the relationship between diversity metrics, accuracy, and resiliency to natural image corruptions of Deep Learning (DL) image classifier ensembles. We investigate the potential of an attribution-based diversity metric to improve the known accuracy-diversity trade-off of the typical prediction-based diversity. Our motivation is based on analytical studies of design diversity that have shown that a reduction of common failure modes is possible if diversity of design choices is achieved.  Using ResNet50 as a comparison baseline, we evaluate the resiliency of multiple individual DL model architectures against dataset distribution shifts corresponding to natural image corruptions. We compare ensembles created with diverse model architectures trained either independently or through a Neural Architecture Search technique and evaluate the correlation of prediction-based and attribution-based diversity to the final ensemble accuracy. We evaluate a set of diversity 
    
[^46]: 物理知识神经网络拓扑优化：应用于隐藏几何结构的非侵入式探测。

    Topology optimization with physics-informed neural networks: application to noninvasive detection of hidden geometries. (arXiv:2303.09280v1 [cs.LG])

    [http://arxiv.org/abs/2303.09280](http://arxiv.org/abs/2303.09280)

    该论文介绍了一种基于物理知识神经网络的拓扑优化方法，应用于无先验知识的几何结构检测，通过材料密度场表示任意解决方案拓扑，并通过Eikonal正则化实现。该方法可用于医疗和工业应用中的非侵入式成像技术。

    

    在医疗和工业应用中，通过电磁、声学或机械负载从表面测量中检测隐藏的几何结构是非侵入成像技术的目标。由于未知的拓扑和几何形状、数据的稀疏性以及物理规律的复杂性，解决逆问题是具有挑战性的。物理知识神经网络已经表现出许多优点，是一个简单而强大的问题反演工具，但它们尚未应用于具有先验未知拓扑的一般问题。在这里，我们介绍了一个基于PINNs的拓扑优化框架，它可以解决没有形状数量或类型先验知识的几何检测问题。我们允许任意的解决方案拓扑，通过使用材料密度场来表示几何形状，并通过新的Eikonal正则化接近二进制值。我们通过检测隐含虚空和包含物的数量、位置和形状来验证我们的框架。

    Detecting hidden geometrical structures from surface measurements under electromagnetic, acoustic, or mechanical loading is the goal of noninvasive imaging techniques in medical and industrial applications. Solving the inverse problem can be challenging due to the unknown topology and geometry, the sparsity of the data, and the complexity of the physical laws. Physics-informed neural networks (PINNs) have shown promise as a simple-yet-powerful tool for problem inversion, but they have yet to be applied to general problems with a priori unknown topology. Here, we introduce a topology optimization framework based on PINNs that solves geometry detection problems without prior knowledge of the number or types of shapes. We allow for arbitrary solution topology by representing the geometry using a material density field that approaches binary values thanks to a novel eikonal regularization. We validate our framework by detecting the number, locations, and shapes of hidden voids and inclusio
    
[^47]: 交通预测中的不确定性自适应建模

    Adaptive Modeling of Uncertainties for Traffic Forecasting. (arXiv:2303.09273v1 [cs.LG])

    [http://arxiv.org/abs/2303.09273](http://arxiv.org/abs/2303.09273)

    提出了QuanTraffic框架，可以自适应建模交通预测中的不确定性，生成预测区间并定义交通预测的真实值可能出现的范围。

    

    深度神经网络是开发交通预测模型的主要方法。然而，这些模型通常被训练为在平均测试案例上最小化误差并产生单点预测。我们提出了QuanTraffic，一种增强任意DNN模型不确定性建模能力的通用框架。QuanTraffic在DNN模型训练期间自动学习标准分位函数，以产生单点预测的预测区间，从而定义交通预测的真实值可能出现的范围。

    Deep neural networks (DNNs) have emerged as a dominant approach for developing traffic forecasting models. These models are typically trained to minimize error on averaged test cases and produce a single-point prediction, such as a scalar value for traffic speed or travel time. However, single-point predictions fail to account for prediction uncertainty that is critical for many transportation management scenarios, such as determining the best- or worst-case arrival time. We present QuanTraffic, a generic framework to enhance the capability of an arbitrary DNN model for uncertainty modeling. QuanTraffic requires little human involvement and does not change the base DNN architecture during deployment. Instead, it automatically learns a standard quantile function during the DNN model training to produce a prediction interval for the single-point prediction. The prediction interval defines a range where the true value of the traffic prediction is likely to fall. Furthermore, QuanTraffic d
    
[^48]: 生成式人工智能的版权保护和责任：攻击，水印和归属

    Copyright Protection and Accountability of Generative AI:Attack, Watermarking and Attribution. (arXiv:2303.09272v1 [cs.LG])

    [http://arxiv.org/abs/2303.09272](http://arxiv.org/abs/2303.09272)

    本文提出了一个评估框架来评估用于生成式人工智能版权保护的方法，结果显示针对输入图像，模型水印和归属网络的当前知识产权保护方法在广泛的GAN范围内基本上令人满意，但为保护训练集必须寻找更有效的方法。

    

    生成式人工智能（例如生成对抗网络-GAN）近年来变得越来越受欢迎。然而，生成式人工智能针对生成的图像（有毒图像）和模型（有毒模型）的知识产权保护（或模型的问责）引发了重大担忧。在本文中，我们提出了一种评估框架，以全面了解当前针对各种GAN架构的版权保护措施的性能，并确定影响它们的因素和未来的研究方向。我们的发现表明，针对输入图像，模型水印和归属网络的当前知识产权保护方法在广泛的GAN范围内基本上令人满意。我们强调，必须将进一步关注点集中在保护训练集上，因为当前的方法未能提供强有力的知识产权保护和知识产权来源证明。

    Generative AI (e.g., Generative Adversarial Networks - GANs) has become increasingly popular in recent years. However, Generative AI introduces significant concerns regarding the protection of Intellectual Property Rights (IPR) (resp. model accountability) pertaining to images (resp. toxic images) and models (resp. poisoned models) generated. In this paper, we propose an evaluation framework to provide a comprehensive overview of the current state of the copyright protection measures for GANs, evaluate their performance across a diverse range of GAN architectures, and identify the factors that affect their performance and future research directions. Our findings indicate that the current IPR protection methods for input images, model watermarking, and attribution networks are largely satisfactory for a wide range of GANs. We highlight that further attention must be directed towards protecting training sets, as the current approaches fail to provide robust IPR protection and provenance 
    
[^49]: 寻找树集成模型预测的最小代价解释

    Finding Minimum-Cost Explanations for Predictions made by Tree Ensembles. (arXiv:2303.09271v1 [cs.LG])

    [http://arxiv.org/abs/2303.09271](http://arxiv.org/abs/2303.09271)

    本研究提出了一种高效的oracle系统，能够寻找树集成模型预测的最小代价解释，该算法比目前最先进的替代方案的运行表现更好。m-MARCO算法可以计算每个预测的单个最小解释，并证明相对于枚举所有最小解释的MARCO算法，我们的方法具有两倍的总体加速比。

    

    当机器学习模型作为关键系统的决策支持时，能够解释为何模型做出特定预测的能力至关重要。提供的解释必须是可证明的，并且最好不包含冗余信息，即最小解释。本文旨在寻找树集成模型预测的解释，这些解释不仅是最小的，而且在成本函数方面也是最小的。为此，我们首先提出了一个高效的“神谕”系统，可以确定解释的正确性，在计算最小解释时超越了当前最先进的替代方案的运行表现数个数量级。其次，我们改编了来自相关工作的叫做MARCO的算法（将其称为m-MARCO），目的是计算每个预测的单个最小解释，并证明相对于枚举所有最小解释的MARCO算法，我们的方法具有两倍的总体加速比。

    The ability to explain why a machine learning model arrives at a particular prediction is crucial when used as decision support by human operators of critical systems. The provided explanations must be provably correct, and preferably without redundant information, called minimal explanations. In this paper, we aim at finding explanations for predictions made by tree ensembles that are not only minimal, but also minimum with respect to a cost function.  To this end, we first present a highly efficient oracle that can determine the correctness of explanations, surpassing the runtime performance of current state-of-the-art alternatives by several orders of magnitude when computing minimal explanations.  Secondly, we adapt an algorithm called MARCO from related works (calling it m-MARCO) for the purpose of computing a single minimum explanation per prediction, and demonstrate an overall speedup factor of two compared to the MARCO algorithm which enumerates all minimal explanations.  Final
    
[^50]: SmartBERT：用于加速BERT推理的动态早期退出机制的改进

    SmartBERT: A Promotion of Dynamic Early Exiting Mechanism for Accelerating BERT Inference. (arXiv:2303.09266v1 [cs.CL])

    [http://arxiv.org/abs/2303.09266](http://arxiv.org/abs/2303.09266)

    SmartBERT是一种改进的动态早期退出与层跳过机制，可以自适应地跳过一些层并自适应地选择是否退出，以加速BERT模型的推理速度。

    

    动态早期退出被证明可以提高预训练语言模型（如BERT）的推理速度。然而，所有样本在早期退出之前都必须经过所有连续层，较复杂的样本通常会经历更多的层，仍然存在冗余计算。本文提出了一种名为SmartBERT的Bert推理的新型动态早期退出与层跳过相结合的机制，它将跳过门和退出算子加入到BERT的每一层中。SmartBERT可以自适应地跳过一些层并自适应地选择是否退出。此外，我们提出了跨层对比学习，并将其结合到我们的训练阶段中，以提高中间层和分类器，这对于早期退出是有益的。为了保持训练和推理阶段跳过门的一致使用，我们在训练阶段提出了一种硬权重机制。我们在GLUE基准测试的八个分类数据集上进行了实验。

    Dynamic early exiting has been proven to improve the inference speed of the pre-trained language model like BERT. However, all samples must go through all consecutive layers before early exiting and more complex samples usually go through more layers, which still exists redundant computation. In this paper, we propose a novel dynamic early exiting combined with layer skipping for BERT inference named SmartBERT, which adds a skipping gate and an exiting operator into each layer of BERT. SmartBERT can adaptively skip some layers and adaptively choose whether to exit. Besides, we propose cross-layer contrastive learning and combine it into our training phases to boost the intermediate layers and classifiers which would be beneficial for early exiting. To keep the consistent usage of skipping gates between training and inference phases, we propose a hard weight mechanism during training phase. We conduct experiments on eight classification datasets of the GLUE benchmark. Experimental resul
    
[^51]: 基于生成对抗网络的黑色素瘤个性化艺术治疗方法

    Generative Adversarial Network for Personalized Art Therapy in Melanoma Disease Management. (arXiv:2303.09232v1 [eess.IV])

    [http://arxiv.org/abs/2303.09232](http://arxiv.org/abs/2303.09232)

    该论文提出了一种基于生成对抗网络的黑色素瘤个性化艺术治疗方法，该方法快速生成出个性化的艺术治疗内容，有效减轻患者心理压力。

    

    黑色素瘤是一种最致命的皮肤癌，患者容易患有心理健康疾病，这可能会降低癌症治疗的效果和患者用药计划的遵循性。因此，保护患者在接受治疗时的心理健康至关重要。然而，目前的艺术治疗方法并不是个性化和独特的。我们旨在提供一个训练有素的图像风格转换模型，它可以快速从个人皮肤镜黑色素瘤图像中生成独特的艺术品，作为辅助黑色素瘤病管理中的艺术治疗工具。视觉艺术欣赏是疾病管理中常见的艺术治疗形式，它可以明显减轻心理压力。我们基于循环一致生成对抗网络（cycle-consistent generative adversarial network）开发了一种基于风格转换的网络来生成来自皮肤镜黑色素瘤图像的个性化和独特艺术品。我们开发的模型将黑色素瘤图像转换为独特的花卉主题艺术品

    Melanoma is the most lethal type of skin cancer. Patients are vulnerable to mental health illnesses which can reduce the effectiveness of the cancer treatment and the patients adherence to drug plans. It is crucial to preserve the mental health of patients while they are receiving treatment. However, current art therapy approaches are not personal and unique to the patient. We aim to provide a well-trained image style transfer model that can quickly generate unique art from personal dermoscopic melanoma images as an additional tool for art therapy in disease management of melanoma. Visual art appreciation as a common form of art therapy in disease management that measurably reduces the degree of psychological distress. We developed a network based on the cycle-consistent generative adversarial network for style transfer that generates personalized and unique artworks from dermoscopic melanoma images. We developed a model that converts melanoma images into unique flower-themed artworks 
    
[^52]: 受控下降训练

    Controlled Descent Training. (arXiv:2303.09216v1 [math.OC])

    [http://arxiv.org/abs/2303.09216](http://arxiv.org/abs/2303.09216)

    本文提出了一种新颖的神经网络（ANN）训练方法，使用最优控制理论支持。通过增强训练标签，以可靠地保证训练损失收敛并提高训练收敛速率，实现动态标签增强。这种新颖的算法保证了局部收敛。

    

    本研究设计了一种新颖的、基于模型的人工神经网络（ANN）训练方法，该方法使用最优控制理论支持。该方法通过增强训练标签，以可靠地保证训练损失收敛并提高训练收敛速率。在梯度下降训练框架内提出了动态标签增强方法，以控制训练损失的收敛。首先，我们借助经验神经切向核（NTK）来捕捉训练行为，并从系统和控制理论中借鉴工具来分析局部和全局的训练动态（如稳定性、可达性）。其次，我们建议通过虚构的标签作为控制输入和最优状态反馈策略来动态改变梯度下降训练机制。通过这种方式，我们强制在本地实现$\mathcal{H}_2$最优和收敛的训练行为。这种新颖的算法“受控下降训练”（CDT）保证了局部收敛。

    In this work, a novel and model-based artificial neural network (ANN) training method is developed supported by optimal control theory. The method augments training labels in order to robustly guarantee training loss convergence and improve training convergence rate. Dynamic label augmentation is proposed within the framework of gradient descent training where the convergence of training loss is controlled. First, we capture the training behavior with the help of empirical Neural Tangent Kernels (NTK) and borrow tools from systems and control theory to analyze both the local and global training dynamics (e.g. stability, reachability). Second, we propose to dynamically alter the gradient descent training mechanism via fictitious labels as control inputs and an optimal state feedback policy. In this way, we enforce locally $\mathcal{H}_2$ optimal and convergent training behavior. The novel algorithm, \textit{Controlled Descent Training} (CDT), guarantees local convergence. CDT unleashes 
    
[^53]: 基于块的变压器模型的位压缩

    Block-wise Bit-Compression of Transformer-based Models. (arXiv:2303.09184v1 [cs.CL])

    [http://arxiv.org/abs/2303.09184](http://arxiv.org/abs/2303.09184)

    本论文提出了一种用于Transformer的块位压缩方法，称为BBCT。该方法可以更细粒度地压缩整个Transformer，包括嵌入、矩阵乘法、GELU、softmax、层归一化和所有中间结果。在GLUE数据集上测试结果表明，BBCT可以在大多数任务中实现少于1％的准确率下降。

    

    随着BERT、GPT-3和ChatGPT等近期基于Transformer的模型的流行，自然语言处理任务中取得了最先进的性能。然而，Transformer模型的巨大计算量、巨大的内存占用和高延迟是云计算中不可避免的挑战。为了解决这个问题，我们提出了BBCT方法，它是一种用于Transformer的块位压缩方法，无需重新训练。我们的方法实现了对整个Transformer的更细粒度的压缩，包括嵌入、矩阵乘法、GELU、softmax、层归一化和所有中间结果。我们以高效BERT为案例，使用BBCT方法进行压缩。我们在General Language Understanding Evaluation(GLUE)数据集上的测试结果表明，BBCT在大多数任务中的准确度下降小于1％。

    With the popularity of the recent Transformer-based models represented by BERT, GPT-3 and ChatGPT, there has been state-of-the-art performance in a range of natural language processing tasks. However, the massive computations, huge memory footprint, and thus high latency of Transformer-based models is an inevitable challenge for the cloud with high real-time requirement. To tackle the issue, we propose BBCT, a method of block-wise bit-compression for transformer without retraining. Our method achieves more fine-grained compression of the whole transformer, including embedding, matrix multiplication, GELU, softmax, layer normalization, and all the intermediate results. As a case, we compress an efficient BERT with the method of BBCT. Our benchmark test results on General Language Understanding Evaluation (GLUE) show that BBCT can achieve less than 1% accuracy drop in most tasks.
    
[^54]: POMCP中学习逻辑规范以实现软政策指导

    Learning Logic Specifications for Soft Policy Guidance in POMCP. (arXiv:2303.09172v1 [cs.AI])

    [http://arxiv.org/abs/2303.09172](http://arxiv.org/abs/2303.09172)

    论文提出了一种从POMCP执行中学习逻辑规范的方法，以实现软政策指导，代替手动定义的策略相关规则，并用于解决环境状态空间大的问题。

    

    部分可观察的蒙特卡洛规划（POMCP）是一种有效的部分可观察马尔可夫决策过程（POMDP）的解决器。它通过使用基于蒙特卡洛树搜索的策略，在本地和在线计算最优策略的近似，从而使得规模上的扩展成为可能。然而，POMCP在稀疏奖励函数方面存在问题，即仅在达到最终目标时获得奖励，尤其是在具有大状态空间和长时间跨度的环境中。最近，已经将逻辑规范集成到POMCP中，以指导探索并满足安全性要求。然而，在真实世界的情况下，这些与策略相关的规则需要由领域专家手动定义。在本文中，我们使用归纳逻辑编程从POMCP执行的跟踪中学习逻辑规范，即由规划器生成的信念-行为对集合。具体来说，我们学习了用答案集编程范式表示的规则。然后我们将它们集成到POMCP中利用它以实现软指导政策。

    Partially Observable Monte Carlo Planning (POMCP) is an efficient solver for Partially Observable Markov Decision Processes (POMDPs). It allows scaling to large state spaces by computing an approximation of the optimal policy locally and online, using a Monte Carlo Tree Search based strategy. However, POMCP suffers from sparse reward function, namely, rewards achieved only when the final goal is reached, particularly in environments with large state spaces and long horizons. Recently, logic specifications have been integrated into POMCP to guide exploration and to satisfy safety requirements. However, such policy-related rules require manual definition by domain experts, especially in real-world scenarios. In this paper, we use inductive logic programming to learn logic specifications from traces of POMCP executions, i.e., sets of belief-action pairs generated by the planner. Specifically, we learn rules expressed in the paradigm of answer set programming. We then integrate them inside
    
[^55]: 带有概念瓶颈结构和多任务组成的线性神经网络的贝叶斯泛化误差。

    Bayesian Generalization Error in Linear Neural Networks with Concept Bottleneck Structure and Multitask Formulation. (arXiv:2303.09154v1 [stat.ML])

    [http://arxiv.org/abs/2303.09154](http://arxiv.org/abs/2303.09154)

    本文数学上澄清了带有概念瓶颈结构和多任务组成的线性神经网络的贝叶斯泛化误差和自由能。

    

    概念瓶颈模型（CBM）是一种广泛使用的方法，可以使用概念解释神经网络。在CBM中，概念被插入到输出层和最后一个中间层之间作为可观察值。这有助于理解神经网络生成输出的原因：最后一个隐藏层到输出层的概念对应的权重。然而，在CBM中理解泛化误差行为尚不可能，因为神经网络通常是奇异的统计模型。当模型是奇异的时，从参数到概率分布的一一映射不能创建。这种不可识别性使得分析泛化性能变得困难。在本次研究中，我们数学上澄清了CBM的贝叶斯泛化误差和自由能，当其架构是三层的线性神经网络时。我们还考虑了一个多任务问题，在该问题中，神经网络的输出不再只是一个标签，而是一组任务。

    Concept bottleneck model (CBM) is a ubiquitous method that can interpret neural networks using concepts. In CBM, concepts are inserted between the output layer and the last intermediate layer as observable values. This helps in understanding the reason behind the outputs generated by the neural networks: the weights corresponding to the concepts from the last hidden layer to the output layer. However, it has not yet been possible to understand the behavior of the generalization error in CBM since a neural network is a singular statistical model in general. When the model is singular, a one to one map from the parameters to probability distributions cannot be created. This non-identifiability makes it difficult to analyze the generalization performance. In this study, we mathematically clarify the Bayesian generalization error and free energy of CBM when its architecture is three-layered linear neural networks. We also consider a multitask problem where the neural network outputs not on
    
[^56]: 用神经网络预测光纤中周期信号的非线性重塑

    Predicting nonlinear reshaping of periodic signals in optical fibre with a neural network. (arXiv:2303.09133v1 [physics.optics])

    [http://arxiv.org/abs/2303.09133](http://arxiv.org/abs/2303.09133)

    本研究利用神经网络预测光纤中周期信号的非线性重塑，探究了正常和异常二阶色散区域，可生成定制脉冲列或出现显着的时间或频谱聚焦。

    

    我们使用基于神经网络的监督式机器学习模型，预测简单正弦调制信号在光纤非线性传播时重塑成具有频域中具有一定结构脉冲列的时间和频谱。研究了光纤的正常和异常二阶色散区域，并利用神经网络的速度，探索输入参数空间以生成定制脉冲列或发生显着的时间或频谱聚焦。

    We deploy a supervised machine-learning model based on a neural network to predict the temporal and spectral reshaping of a simple sinusoidal modulation into a pulse train having a comb structure in the frequency domain, which occurs upon nonlinear propagation in an optical fibre. Both normal and anomalous second-order dispersion regimes of the fibre are studied, and the speed of the neural network is leveraged to probe the space of input parameters for the generation of custom combs or the occurrence of significant temporal or spectral focusing.
    
[^57]: 探索用于代码分析的大型语言模型中的分布偏移

    Exploring Distributional Shifts in Large Language Models for Code Analysis. (arXiv:2303.09128v1 [cs.CL])

    [http://arxiv.org/abs/2303.09128](http://arxiv.org/abs/2303.09128)

    研究了两种大型语言模型在代码分析中处理领域外数据的能力，提出了组织、项目和模块的自然边界分割方法，发现每个新领域的样本都会产生分布偏移的挑战，实现了多任务学习与少量微调相结合的解决方案。

    

    我们系统地研究了两种大型语言模型 CodeT5 和 Codex 的能力，以便推广到领域外数据。在本研究中，我们考虑了两种基本应用：代码摘要和代码生成。我们按照其自然边界（按组织、按项目和按软件项目中的模块）将数据分为不同的领域。这样，在部署时，识别领域内和领域外的数据变得简单。我们发现，来自每个新领域的样本都会给这两个模型带来分布偏移的重大挑战。我们研究了不同的方法如何适应模型以更好地推广到新领域。我们的实验表明，虽然多任务学习本身是一个合理的基线，但将其与从训练数据中检索的示例的少量微调相结合可以实现非常强的性能。事实上，根据我们的实验，这种解决方案可以在非常低的数据情况下优于直接调整微调。

    We systematically study the capacity of two large language models for code CodeT5 and Codex - to generalize to out-of-domain data. In this study, we consider two fundamental applications - code summarization, and code generation. We split data into domains following its natural boundaries - by an organization, by a project, and by a module within the software project. This makes recognition of in-domain vs out-of-domain data at the time of deployment trivial. We establish that samples from each new domain present both models with a significant challenge of distribution shift. We study how well different established methods can adapt models to better generalize to new domains. Our experiments show that while multitask learning alone is a reasonable baseline, combining it with few-shot finetuning on examples retrieved from training data can achieve very strong performance. In fact, according to our experiments, this solution can outperform direct finetuning for very low-data scenarios.
    
[^58]: 基于距离的取证比较方法评估：应用于手部气味取证

    Evaluation of distance-based approaches for forensic comparison: Application to hand odor evidence. (arXiv:2303.09126v1 [cs.LG])

    [http://arxiv.org/abs/2303.09126](http://arxiv.org/abs/2303.09126)

    本文提出了一种基于距离的法医学取证比较方法，同时评估并优化了直接方法和间接方法。间接方法更稳健，适用于机器学习，自动特征选择和降维。

    

    在法医学中，通过不同类型的痕迹区分相同来源和不同来源的假设是一个普遍的问题。这个问题通常使用贝叶斯方法来解决，它们能够提供似然比来量化支持两个竞争假设的证据之间的相对强度。在这里，我们集中于基于距离的方法，它们的鲁棒性，特别是处理高维度证据的能力非常不同，需要评估和优化。我们提出了一个直接方法和一个间接方法的统一框架，前者基于估计两个竞争假设下的痕迹距离的似然性，后者使用逻辑回归来区分同一来源和不同来源的距离分布。虽然直接方法更灵活，但间接方法更稳健，并且在机器学习中相当自然。此外，间接方法还能够实现自动的特征选择和降维。

    The issue of distinguishing between the same-source and different-source hypotheses based on various types of traces is a generic problem in forensic science. This problem is often tackled with Bayesian approaches, which are able to provide a likelihood ratio that quantifies the relative strengths of evidence supporting each of the two competing hypotheses. Here, we focus on distance-based approaches, whose robustness and specifically whose capacity to deal with high-dimensional evidence are very different, and need to be evaluated and optimized. A unified framework for direct methods based on estimating the likelihoods of the distance between traces under each of the two competing hypotheses, and indirect methods using logistic regression to discriminate between same-source and different-source distance distributions, is presented. Whilst direct methods are more flexible, indirect methods are more robust and quite natural in machine learning. Moreover, indirect methods also enable the
    
[^59]: SigVIC: 空间重要性指导的可变比图像压缩

    SigVIC: Spatial Importance Guided Variable-Rate Image Compression. (arXiv:2303.09112v1 [eess.IV])

    [http://arxiv.org/abs/2303.09112](http://arxiv.org/abs/2303.09112)

    SigVIC是一种空间重要性指导的可变比图像压缩方法，通过自适应学习空间重要性掩码指导特征缩放和比特分配，选择Top-K浅层特征来精细调整解码特征，实验结果表明其在速率失真性能和视觉质量方面均实现了最先进的性能。

    

    可变比机制提高了基于学习的图像压缩的灵活性和效率，该方法为不同的速率-失真权衡训练多个模型。变比率的最常见方法之一是按通道或空间均匀缩放内部特征。但是，空间重要性的多样性对于图像压缩的比特分配是有指导意义的。在本文中，我们介绍了一种空间重要性指导的可变比图像压缩方法（SigVIC），其中设计了一个空间门控单元（SGU），用于自适应学习空间重要性掩码。然后，一个空间缩放网络（SSN）使用空间重要性掩码来指导特征缩放和可变比率的比特分配。此外，为了提高解码图像的质量，选择Top-K浅层特征通过浅层特征融合模块（SFFM）来精细地调整解码特征。实验证明，我们的方法在多个基准数据集上优于其他基于学习的方法（无论是变比率还是非变比率），在速率失真性能和视觉质量方面均实现了最先进的性能。

    Variable-rate mechanism has improved the flexibility and efficiency of learning-based image compression that trains multiple models for different rate-distortion tradeoffs. One of the most common approaches for variable-rate is to channel-wisely or spatial-uniformly scale the internal features. However, the diversity of spatial importance is instructive for bit allocation of image compression. In this paper, we introduce a Spatial Importance Guided Variable-rate Image Compression (SigVIC), in which a spatial gating unit (SGU) is designed for adaptively learning a spatial importance mask. Then, a spatial scaling network (SSN) takes the spatial importance mask to guide the feature scaling and bit allocation for variable-rate. Moreover, to improve the quality of decoded image, Top-K shallow features are selected to refine the decoded features through a shallow feature fusion module (SFFM). Experiments show that our method outperforms other learning-based methods (whether variable-rate or 
    
[^60]: 基于机器学习的超声心动图图像处理研究

    Machine learning based biomedical image processing for echocardiographic images. (arXiv:2303.09103v1 [eess.IV])

    [http://arxiv.org/abs/2303.09103](http://arxiv.org/abs/2303.09103)

    本文介绍了一种通过KNN算法进行医学图像分割和分类的方法，使用灰度共生矩阵特征，可以提高超声心动图图像分析的准确性和效率。

    

    人工智能和机器学习的流行促使研究人员将其应用于最近的研究中。该论文提出了一种使用KNN算法进行医学图像分割和提取图像特征以进行分类分析的方法。分类对于医学成像中的图像非常重要，KNN算法是一种简单、概念化和计算的算法，在结果方面提供了非常好的准确性。KNN算法是一种独特的用户友好方法，广泛应用于机器学习算法中，用于各种图像处理应用，包括分类、分割和回归问题。该研究采用灰度共生矩阵特征。经过训练的神经网络已经成功地在一组超声心动图图像上进行了测试，通过回归分析比较了误差。研究结果表明，该方法具有高精度，可以有效地对超声心动图图像进行分割和分析。

    The popularity of Artificial intelligence and machine learning have prompted researchers to use it in the recent researches. The proposed method uses K-Nearest Neighbor (KNN) algorithm for segmentation of medical images, extracting of image features for analysis by classifying the data based on the neural networks. Classification of the images in medical imaging is very important, KNN is one suitable algorithm which is simple, conceptual and computational, which provides very good accuracy in results. KNN algorithm is a unique user-friendly approach with wide range of applications in machine learning algorithms which are majorly used for the various image processing applications including classification, segmentation and regression issues of the image processing. The proposed system uses gray level co-occurrence matrix features. The trained neural network has been tested successfully on a group of echocardiographic images, errors were compared using regression plot. The results of the 
    
[^61]: 视觉语言模型补丁-令牌对齐的贝叶斯提示学习

    Patch-Token Aligned Bayesian Prompt Learning for Vision-Language Models. (arXiv:2303.09100v1 [cs.CV])

    [http://arxiv.org/abs/2303.09100](http://arxiv.org/abs/2303.09100)

    本文提出了一种基于贝叶斯概率的视觉语言模型提示学习方法，通过将提示标记推向忠实捕捉标签特定的视觉概念，而不是过度拟合训练类别，解决了现有提示工程的问题。在各种视觉语言任务上的广泛实验表明，该方法优于现有的最先进模型。

    

    在视觉语言预训练模型的下游应用中，构建有效提示引起了极大关注。现有的提示工程方法要么需要费时费力的手动设计，要么将提示调优作为点估计问题进行优化，这可能无法描述类别的多样特征并限制了它们的应用。本文提出了一种基于贝叶斯概率的提示学习方法，其中通过从潜在分布中首先采样隐向量，然后采用轻量级生成模型来生成标签特定的随机提示。重要的是，我们将视觉知识与图像的语义规则化，并将图像和相应的提示视为补丁和令牌集，通过最优传输将提示标记推向忠实捕捉标签特定的视觉概念，而不是过度拟合训练类别。此外，所提出的模型还可以通过使用额外的基于文本的信息来生成更具信息量和准确性的提示。在各种视觉语言任务上的广泛实验表明，我们的补丁-令牌对齐的贝叶斯提示学习（PTBPL）优于现有的最先进模型。

    For downstream applications of vision-language pre-trained models, there has been significant interest in constructing effective prompts. Existing works on prompt engineering, which either require laborious manual designs or optimize the prompt tuning as a point estimation problem, may fail to describe diverse characteristics of categories and limit their applications. We introduce a Bayesian probabilistic resolution to prompt learning, where the label-specific stochastic prompts are generated hierarchically by first sampling a latent vector from an underlying distribution and then employing a lightweight generative model. Importantly, we semantically regularize prompt learning with the visual knowledge and view images and the corresponding prompts as patch and token sets under optimal transport, which pushes the prompt tokens to faithfully capture the label-specific visual concepts, instead of overfitting the training categories. Moreover, the proposed model can also be straightforwar
    
[^62]: 基于多模态和多模态学习的腰椎手术前低背痛和坐骨神经痛患者预后评估

    Preoperative Prognosis Assessment of Lumbar Spinal Surgery for Low Back Pain and Sciatica Patients based on Multimodalities and Multimodal Learning. (arXiv:2303.09085v1 [cs.LG])

    [http://arxiv.org/abs/2303.09085](http://arxiv.org/abs/2303.09085)

    本研究结合了中医和机器学习，开发了一种术前评估工具，组合了标准手术评估、中医体质评估和计划手术方法，可用于预测腰椎手术的预后。

    

    当患者出现严重疼痛症状时，低背痛和坐骨神经痛可能需要手术治疗，但目前没有有效的措施来预测手术结果。本研究结合了中医元素和机器学习，开发了一种术前评估工具，用于预测低背痛和坐骨神经痛患者腰椎手术的预后。收集并存储了标准手术评估、中医体质评估、计划手术方法和元音发音录音等不同的模态。我们的研究提供了利用模态组合、多模态和融合策略的见解。模型的可解释性和模态之间的相关性也得到了检查。通过招募105名患者，我们发现结合标准手术评估、体质评估和计划手术方法可以实现0.81的预测准确率。我们的方法是有效的。

    Low back pain (LBP) and sciatica may require surgical therapy when they are symptomatic of severe pain. However, there is no effective measures to evaluate the surgical outcomes in advance. This work combined elements of Eastern medicine and machine learning, and developed a preoperative assessment tool to predict the prognosis of lumbar spinal surgery in LBP and sciatica patients. Standard operative assessments, traditional Chinese medicine body constitution assessments, planned surgical approach, and vowel pronunciation recordings were collected and stored in different modalities. Our work provides insights into leveraging modality combinations, multimodals, and fusion strategies. The interpretability of models and correlations between modalities were also inspected. Based on the recruited 105 patients, we found that combining standard operative assessments, body constitution assessments, and planned surgical approach achieved the best performance in 0.81 accuracy. Our approach is ef
    
[^63]: SSL清理：自监督学习中的木马检测和缓解

    SSL-Cleanse: Trojan Detection and Mitigation in Self-Supervised Learning. (arXiv:2303.09079v1 [cs.CR])

    [http://arxiv.org/abs/2303.09079](http://arxiv.org/abs/2303.09079)

    本篇论文讨论了自监督学习中的木马攻击检测和缓解问题。由于这种攻击危险隐匿，且在下游分类器中很难检测出来。目前在超监督学习中的木马检测方法可以潜在地保护SSL下游分类器，但在其广泛传播之前识别和处理SSL编码器中的触发器是一项艰巨的任务。

    

    自监督学习（SSL）是一种常用的学习和编码数据表示的方法。通过使用预先训练的SSL图像编码器并在其顶部训练下游分类器，可以在各种任务上实现令人印象深刻的性能，而只需很少的标记数据。SSL的增加使用导致了与SSL编码器相关的安全研究和各种木马攻击的发展。在SSL编码器中插入木马攻击的危险在于它们能够隐蔽地操作并在各种用户和设备之间广泛传播。Trojaned编码器中的后门行为的存在可能会被下游分类器意外继承，使检测和缓解威胁变得更加困难。虽然超监督学习中当前的木马检测方法可以潜在地保护SSL下游分类器，但在其广泛传播之前识别和处理SSL编码器中的触发器是一项艰巨的任务。

    Self-supervised learning (SSL) is a commonly used approach to learning and encoding data representations. By using a pre-trained SSL image encoder and training a downstream classifier on top of it, impressive performance can be achieved on various tasks with very little labeled data. The increasing usage of SSL has led to an uptick in security research related to SSL encoders and the development of various Trojan attacks. The danger posed by Trojan attacks inserted in SSL encoders lies in their ability to operate covertly and spread widely among various users and devices. The presence of backdoor behavior in Trojaned encoders can inadvertently be inherited by downstream classifiers, making it even more difficult to detect and mitigate the threat. Although current Trojan detection methods in supervised learning can potentially safeguard SSL downstream classifiers, identifying and addressing triggers in the SSL encoder before its widespread dissemination is a challenging task. This is be
    
[^64]: VFP：考虑属性相关性将工业物联网表格数据转换为图像以供卷积神经网络使用。

    VFP: Converting Tabular Data for IIoT into Images Considering Correlations of Attributes for Convolutional Neural Networks. (arXiv:2303.09068v1 [cs.CV])

    [http://arxiv.org/abs/2303.09068](http://arxiv.org/abs/2303.09068)

    这篇论文提出了一种新的将工业物联网表格数据转换为图像的方法，考虑了属性之间的相关性，可以更好地利用卷积神经网络进行数据处理。

    

    对于从工业物联网设备生成的表格数据，传统基于决策树算法的机器学习（ML）技术已被采用。 但是，这些方法在处理真实数字属性占主导地位的表格数据时存在限制。 为了解决这个问题，提出了DeepInsight，REFINED和IGTD将表格数据转换为图像以利用卷积神经网络（CNN）。 他们在图像的某些特定位置收集相似的特征，使转换后的图像看起来像是实际图像。 收集类似的特征与传统的针对表格数据的ML技术形成对比，后者删除一些高度相关的属性以避免过度拟合。 此外，先前的转换方法固定了图像大小，根据表格数据的属性数量，会造成浪费或不足的像素。 因此，本文提出了一种新的转换方法Vortex特征定位（VFP）。 VFP考虑特征的相关性并放置类似的特征。

    For tabular data generated from IIoT devices, traditional machine learning (ML) techniques based on the decision tree algorithm have been employed. However, these methods have limitations in processing tabular data where real number attributes dominate. To address this issue, DeepInsight, REFINED, and IGTD were proposed to convert tabular data into images for utilizing convolutional neural networks (CNNs). They gather similar features in some specific spots of an image to make the converted image look like an actual image. Gathering similar features contrasts with traditional ML techniques for tabular data, which drops some highly correlated attributes to avoid overfitting. Also, previous converting methods fixed the image size, and there are wasted or insufficient pixels according to the number of attributes of tabular data. Therefore, this paper proposes a new converting method, Vortex Feature Positioning (VFP). VFP considers the correlation of features and places similar features fa
    
[^65]: 高维度惩罚伯恩斯坦支持向量机

    High-Dimensional Penalized Bernstein Support Vector Machines. (arXiv:2303.09066v1 [stat.ML])

    [http://arxiv.org/abs/2303.09066](http://arxiv.org/abs/2303.09066)

    提出一种适用于高维度情况下的平滑支持向量机铰链损失函数，即Bernstein支持向量机（BernSVM），并提出两种有效算法求解该方法，实验结果表明该方法在现有竞争对手中具有优越性。

    

    支持向量机(SVM)是一种用于二分类的强大分类器，以提高预测精度。然而，在高维设置中，SVM铰链损失函数的不可微性可能导致计算困难。为了克服这个问题，我们依赖伯恩斯坦多项式，提出了一种新的平滑的SVM铰链损失函数版本，称为Bernstein支持向量机（BernSVM），适用于高维$p>> n$情况。由于BernSVM目标损失函数属于$C^2$类，因此我们提出了两种计算惩罚BernSVM解的有效算法。第一个算法基于最大化-主导（MM）原理的坐标下降法，第二个算法是IRLS类型算法（迭代重新加权最小二乘法）。在标准假设下，我们推导出一个锥条件和一个限制性强凸性，以建立加权Lasso BernSVM估计器的上界。使用局部线性逼近，我们提出了两个模型选择标准，用于调整BernSVM超参数。进行了广泛的数值实验，以证明所提出的方法在现有竞争对手中具有优越性。

    The support vector machines (SVM) is a powerful classifier used for binary classification to improve the prediction accuracy. However, the non-differentiability of the SVM hinge loss function can lead to computational difficulties in high dimensional settings. To overcome this problem, we rely on Bernstein polynomial and propose a new smoothed version of the SVM hinge loss called the Bernstein support vector machine (BernSVM), which is suitable for the high dimension $p >> n$ regime. As the BernSVM objective loss function is of the class $C^2$, we propose two efficient algorithms for computing the solution of the penalized BernSVM. The first algorithm is based on coordinate descent with maximization-majorization (MM) principle and the second one is IRLS-type algorithm (iterative re-weighted least squares). Under standard assumptions, we derive a cone condition and a restricted strong convexity to establish an upper bound for the weighted Lasso BernSVM estimator. Using a local linear ap
    
[^66]: 基于t-SPN和滤波的细胞分类的最大间隔学习

    Maximum Margin Learning of t-SPNs for Cell Classification with Filtering. (arXiv:2303.09065v1 [cs.LG])

    [http://arxiv.org/abs/2303.09065](http://arxiv.org/abs/2303.09065)

    本研究提出了一种基于t-SPN算法和滤波技术的细胞分类方法，通过最大化边缘和L2正则化，该方法在HEp-2和Feulgen基准数据集上取得了最高的准确率。

    

    本文探讨了一种基于深度概率体系结构的算法，称为树形求和产品网络(t-SPN)，用于细胞分类。构建t-SPN的目的是表示未归一化概率作为最相似的细胞类别的条件概率。通过最大化边缘来学习构建的t-SPN体系结构，该边缘是真实标签和最有竞争力的错误标签之间的条件概率差。为了增强体系结构的泛化能力，在学习过程中考虑了L2正则化（REG）和最大间隔（MM）标准。为了突出细胞特征，本文探讨了两种通用的高通滤波器的有效性：理想高通滤波和拉普拉斯滤波(Log)。在HEp-2和Feulgen基准数据集上，基于最大间隔准则与正则化学习的t-SPN体系结构产生了最高的准确率。

    An algorithm based on a deep probabilistic architecture referred to as a tree-structured sum-product network (t-SPN) is considered for cell classification. The t-SPN is constructed such that the unnormalized probability is represented as conditional probabilities of a subset of most similar cell classes. The constructed t-SPN architecture is learned by maximizing the margin, which is the difference in the conditional probability between the true and the most competitive false label. To enhance the generalization ability of the architecture, L2-regularization (REG) is considered along with the maximum margin (MM) criterion in the learning process. To highlight cell features, this paper investigates the effectiveness of two generic high-pass filters: ideal high-pass filtering and the Laplacian of Gaussian (LOG) filtering. On both HEp-2 and Feulgen benchmark datasets, the t-SPN architecture learned based on the max-margin criterion with regularization produced the highest accuracy rate co
    
[^67]: 基于区域卷积神经网络的植物病害检测

    Plant Disease Detection using Region-Based Convolutional Neural Network. (arXiv:2303.09063v1 [cs.CV])

    [http://arxiv.org/abs/2303.09063](http://arxiv.org/abs/2303.09063)

    本文提出了一种新的基于区域卷积神经网络（R-CNN）的轻量级深度学习模型，可用于检测番茄植物的叶病害。与传统方法相比，该模型利用整个番茄植物图像的特征进行检测，具有更高的准确性和效率。

    

    农业在孟加拉国的食品和经济中起着重要作用，但细菌，病毒和真菌病害限制了作物生产。本论文旨在构建一个轻量级深度学习模型以预测番茄植物的叶病害。通过修改区域卷积神经网络（R-CNN）的结构，本文提出了一个架构，利用整个番茄植物图像的特征来检测植物病害，而不仅仅是感兴趣区域（ROI）。所提出的模型在检测四种常见的番茄植物病害方面表现出94.3％的准确性。

    Agriculture plays an important role in the food and economy of Bangladesh. The rapid growth of population over the years also has increased the demand for food production. One of the major reasons behind low crop production is numerous bacteria, virus and fungal plant diseases. Early detection of plant diseases and proper usage of pesticides and fertilizers are vital for preventing the diseases and boost the yield. Most of the farmers use generalized pesticides and fertilizers in the entire fields without specifically knowing the condition of the plants. Thus the production cost oftentimes increases, and, not only that, sometimes this becomes detrimental to the yield. Deep Learning models are found to be very effective to automatically detect plant diseases from images of plants, thereby reducing the need for human specialists. This paper aims at building a lightweight deep learning model for predicting leaf disease in tomato plants. By modifying the region-based convolutional neural n
    
[^68]: 来自低资源编程语言的伪代码生成的知识转移

    Knowledge Transfer for Pseudo-code Generation from Low Resource Programming Language. (arXiv:2303.09062v1 [cs.SE])

    [http://arxiv.org/abs/2303.09062](http://arxiv.org/abs/2303.09062)

    本文研究了将高资源编程语言中训练的编码器-解码器神经模型通过迭代回译的方法，将其知识转移到低资源编程语言中用于伪代码生成，从而解决了缺少低资源编程语言-伪代码平行数据的问题。

    

    生成遗留源代码的伪代码描述以实现软件维护是一项繁琐的任务。最近的编码器-解码器语言模型已经显示出在自动化高资源编程语言（如C++）的伪代码生成方面有潜力，但是它们严重依赖于大量的代码-伪代码语料库的可用性。针对在遗留编程语言（PL）中编写代码的伪代码注释是一项耗时且昂贵的工作，需要深入了解源PL。本文专注于通过使用平行代码-伪代码数据训练的编码器-解码器神经模型获取的知识来实现将这些知识转移到没有PL-伪代码平行数据用于训练的遗留PL（C）上。为了实现此目标，我们利用一种基于测试用例的过滤策略的迭代回译（IBT）方法，以将经过训练的C++-to-pseudocode模型调整为C-to-pseudocode模型。

    Generation of pseudo-code descriptions of legacy source code for software maintenance is a manually intensive task. Recent encoder-decoder language models have shown promise for automating pseudo-code generation for high resource programming languages such as C++, but are heavily reliant on the availability of a large code-pseudocode corpus. Soliciting such pseudocode annotations for codes written in legacy programming languages (PL) is a time consuming and costly affair requiring a thorough understanding of the source PL. In this paper, we focus on transferring the knowledge acquired by the code-to-pseudocode neural model trained on a high resource PL (C++) using parallel code-pseudocode data. We aim to transfer this knowledge to a legacy PL (C) with no PL-pseudocode parallel data for training. To achieve this, we utilize an Iterative Back Translation (IBT) approach with a novel test-cases based filtration strategy, to adapt the trained C++-to-pseudocode model to C-to-pseudocode model
    
[^69]: 扩散式对抗净化的鲁棒评估

    Robust Evaluation of Diffusion-Based Adversarial Purification. (arXiv:2303.09051v1 [cs.CV])

    [http://arxiv.org/abs/2303.09051](http://arxiv.org/abs/2303.09051)

    本文分析了对基于扩散式净化方法的评估方式，并提出了一个新的指导方针，以衡量净化方法对抗性攻击的鲁棒性。同时，我们提出了一种新的净化策略，展示了与最先进的对抗性训练方法相竞争的结果。

    

    我们质疑当前对基于扩散式净化方法的评估方式。扩散式净化方法旨在消除测试数据点中的对抗性影响。由于基于训练和测试的解耦，该方法越来越受到关注，作为对抗性训练的替代方法。为了测量净化方法的鲁棒性，通常采用众所周知的白盒攻击。然而，由于这些攻击通常是为对抗性训练而量身定制的，因此不知道这些攻击是否对扩散式净化最有效。我们分析了当前的实践，并提供了一个新的指导方针，以衡量净化方法对抗性攻击的鲁棒性。基于我们的分析，我们进一步提出了一种新的净化策略，展示了与最先进的对抗性训练方法相竞争的结果。

    We question the current evaluation practice on diffusion-based purification methods. Diffusion-based purification methods aim to remove adversarial effects from an input data point at test time. The approach gains increasing attention as an alternative to adversarial training due to the disentangling between training and testing. Well-known white-box attacks are often employed to measure the robustness of the purification. However, it is unknown whether these attacks are the most effective for the diffusion-based purification since the attacks are often tailored for adversarial training. We analyze the current practices and provide a new guideline for measuring the robustness of purification methods against adversarial attacks. Based on our analysis, we further propose a new purification strategy showing competitive results against the state-of-the-art adversarial training approaches.
    
[^70]: 在VoIP平台上提高知觉质量、可懂度和声学表现

    Improving Perceptual Quality, Intelligibility, and Acoustics on VoIP Platforms. (arXiv:2303.09048v1 [cs.SD])

    [http://arxiv.org/abs/2303.09048](http://arxiv.org/abs/2303.09048)

    本文提出一种基于VoIP通信平台进行DNS模型微调的方法，提高其在语音增强方面的性能。这种多任务学习框架能够将噪声抑制和VoIP特有的声学特征相结合，优于行业性能和最先进的方法。

    

    本文提出了一种方法，通过调整Deep Noise Suppression (DNS) 2020 Challenge模型在VoIP应用中的表现来提高其性能。我们的方法涉及将DNS 2020模型适应于VoIP通信的特定声学特征，包括因压缩、传输和平台特定处理而引起的失真和伪影。为此，我们提出了一种针对语音增强的VoIP-DNS多任务学习框架，共同优化降噪和VoIP特定的声学表现。我们在各种VoIP场景下评估了我们的方法，并表明它在VoIP应用的语音增强方面优于行业性能和最先进的方法。我们的结果证明了利用VoIP-DNS能够提高和定制DNS-2020训练的模型在不同的VoIP平台上的潜力，这项发现在语音识别、语音助理等领域有重要的应用。

    In this paper, we present a method for fine-tuning models trained on the Deep Noise Suppression (DNS) 2020 Challenge to improve their performance on Voice over Internet Protocol (VoIP) applications. Our approach involves adapting the DNS 2020 models to the specific acoustic characteristics of VoIP communications, which includes distortion and artifacts caused by compression, transmission, and platform-specific processing. To this end, we propose a multi-task learning framework for VoIP-DNS that jointly optimizes noise suppression and VoIP-specific acoustics for speech enhancement. We evaluate our approach on a diverse VoIP scenarios and show that it outperforms both industry performance and state-of-the-art methods for speech enhancement on VoIP applications. Our results demonstrate the potential of models trained on DNS-2020 to be improved and tailored to different VoIP platforms using VoIP-DNS, whose findings have important applications in areas such as speech recognition, voice assi
    
[^71]: 基于物联网和机器学习算法的选举管理Web和移动平台

    Web and Mobile Platforms for Managing Elections based on IoT And Machine Learning Algorithms. (arXiv:2303.09045v1 [cs.LG])

    [http://arxiv.org/abs/2303.09045](http://arxiv.org/abs/2303.09045)

    本研究使用IoT和ML技术降低选举成本并提高效率，解决E-voting系统安全性、准确性和可靠性问题。

    

    全球疫情严重影响各国。结果，几乎所有国家不得不调整在线技术以继续他们的流程。斯里兰卡每年在选举上花费100亿。为了解决现有问题并增加时间效率和降低成本，我们使用了IoT和ML技术。基于IoT的数据将识别、注册和用于防止欺诈，而ML算法将操纵选举数据并产生获胜预测、基于天气的选民出席率和选举暴力。所有数据将保存在云计算和标准数据库中以存储和访问数据。该研究主要关注E-voting系统的四个方面。 E-voting的最常见问题是系统的安全性，准确性和可靠性。提出的IoT和ML技术旨在提高E-voting系统的安全性，准确性和可靠性，同时降低成本并增加效率。

    The global pandemic situation has severely affected all countries. As a result, almost all countries had to adjust to online technologies to continue their processes. In addition, Sri Lanka is yearly spending ten billion on elections. We have examined a proper way of minimizing the cost of hosting these events online. To solve the existing problems and increase the time potency and cost reduction we have used IoT and ML-based technologies. IoT-based data will identify, register, and be used to secure from fraud, while ML algorithms manipulate the election data and produce winning predictions, weather-based voters attendance, and election violence. All the data will be saved in cloud computing and a standard database to store and access the data. This study mainly focuses on four aspects of an E-voting system. The most frequent problems across the world in E-voting are the security, accuracy, and reliability of the systems. E-government systems must be secured against various cyber-atta
    
[^72]: 嵌入式理论在洪泛计算中的应用及利用时间延迟减少洪泛网络规模

    Embedding Theory of Reservoir Computing and Reducing Reservoir Network Using Time Delays. (arXiv:2303.09042v1 [cs.LG])

    [http://arxiv.org/abs/2303.09042](http://arxiv.org/abs/2303.09042)

    本文结合延迟嵌入理论和广义嵌入理论，严谨证明了RC本质上是原始输入非线性动态系统的高维嵌入。我们进一步发现了时间延迟和网络神经元数量之间的权衡关系，并显着减小了洪泛计算网络的大小，实现了比全尺寸RC更好的性能。

    

    洪泛计算作为一种特殊的循环神经网络，由于在重构或/和预测复杂物理系统方面具有卓越的功效和高性能，因此正在爆炸性发展。然而，触发RC如此有效应用的机制仍不清楚，需要深入而系统的探索。本文结合延迟嵌入理论和广义嵌入理论，严谨证明了RC本质上是原始输入非线性动态系统的高维嵌入。因此，利用这种嵌入特性，我们将标准RC和时间延迟RC统一到一个通用框架中，并且我们对网络的输出层仅引入时间延迟，进一步发现了时间延迟和网络神经元数量之间的权衡关系。基于这一发现，我们显着减小了洪泛计算网络的大小，用于重构和预测一些代表性的物理系统，并且更让人惊讶的是，实现了比全尺寸RC更好的性能。

    Reservoir computing (RC), a particular form of recurrent neural network, is under explosive development due to its exceptional efficacy and high performance in reconstruction or/and prediction of complex physical systems. However, the mechanism triggering such effective applications of RC is still unclear, awaiting deep and systematic exploration. Here, combining the delayed embedding theory with the generalized embedding theory, we rigorously prove that RC is essentially a high dimensional embedding of the original input nonlinear dynamical system. Thus, using this embedding property, we unify into a universal framework the standard RC and the time-delayed RC where we novelly introduce time delays only into the network's output layer, and we further find a trade-off relation between the time delays and the number of neurons in RC. Based on this finding, we significantly reduce the network size of RC for reconstructing and predicting some representative physical systems, and, more surp
    
[^73]: 一种基于多模态数据驱动的焦虑筛查框架

    A Multimodal Data-driven Framework for Anxiety Screening. (arXiv:2303.09041v1 [cs.LG])

    [http://arxiv.org/abs/2303.09041](http://arxiv.org/abs/2303.09041)

    提出了一种基于多模态数据驱动的焦虑筛查框架（MMD-AS），可以提高模型准确性。

    

    针对焦虑筛查的需求，传统的方法往往通过专业的设备及医生的经验和判断力，但由于资源的有限性，无法同时满足高准确性和模型可解释性的需求。而多模态数据可以提供更客观的数据，以提高模型的准确性。但是由于多模态数据中存在大量的噪音，同时数据不平衡问题也容易导致过拟合。这使得现有基于机器学习和深度学习的焦虑筛查方法难以应用。 因此，我们提出了一种基于多模态数据驱动的焦虑筛查框架，即MMD-AS，并在收集了200个患者的健康数据上进行了实验。

    Early screening for anxiety and appropriate interventions are essential to reduce the incidence of self-harm and suicide in patients. Due to limited medical resources, traditional methods that overly rely on physician expertise and specialized equipment cannot simultaneously meet the needs for high accuracy and model interpretability. Multimodal data can provide more objective evidence for anxiety screening to improve the accuracy of models. The large amount of noise in multimodal data and the unbalanced nature of the data make the model prone to overfitting. However, it is a non-differentiable problem when high-dimensional and multimodal feature combinations are used as model inputs and incorporated into model training. This causes existing anxiety screening methods based on machine learning and deep learning to be inapplicable. Therefore, we propose a multimodal data-driven anxiety screening framework, namely MMD-AS, and conduct experiments on the collected health data of over 200 se
    
[^74]: 只针对不确定性支付代价：方差自适应汤普森采样

    Only Pay for What Is Uncertain: Variance-Adaptive Thompson Sampling. (arXiv:2303.09033v1 [cs.LG])

    [http://arxiv.org/abs/2303.09033](http://arxiv.org/abs/2303.09033)

    本文提出了一种针对多臂赌博机的方差自适应汤普森采样算法，通过考虑奖励方差的信息减少了遗憾，同时提高了鲁棒性

    

    大多数赌博算法都假设奖励方差或其上界已知。尽管方差高估通常是安全的，但它会增加遗憾。另一方面，低估的方差可能导致由于过早地选择了次优臂而导致的线性遗憾。这激发了关于方差感知频率算法的先前工作。我们为贝叶斯设置打下基础。特别是，我们研究了具有已知和未知异质奖励方差的多臂赌博机，并为两者开发了汤普森采样算法，并限制了它们的贝叶斯遗憾。我们的遗憾界随着较低奖励方差而减少，这使得学习更加容易。未知奖励方差的边界捕捉了先验对学习奖励方差的影响，是其类型中的首个。我们的实验表明了方差感知的贝叶斯算法的优越性，同时也突出了它们的鲁棒性。

    Most bandit algorithms assume that the reward variance or its upper bound is known. While variance overestimation is usually safe and sound, it increases regret. On the other hand, an underestimated variance may lead to linear regret due to committing early to a suboptimal arm. This motivated prior works on variance-aware frequentist algorithms. We lay foundations for the Bayesian setting. In particular, we study multi-armed bandits with known and \emph{unknown heterogeneous reward variances}, and develop Thompson sampling algorithms for both and bound their Bayes regret. Our regret bounds decrease with lower reward variances, which make learning easier. The bound for unknown reward variances captures the effect of the prior on learning reward variances and is the first of its kind. Our experiments show the superiority of variance-aware Bayesian algorithms and also highlight their robustness.
    
[^75]: 条件乐观探索用于深度多智能体强化学习

    Conditionally Optimistic Exploration for Cooperative Deep Multi-Agent Reinforcement Learning. (arXiv:2303.09032v1 [cs.LG])

    [http://arxiv.org/abs/2303.09032](http://arxiv.org/abs/2303.09032)

    本文提出了一种名为条件乐观探索(COE)的基于UCT的探索方法，通过结构依赖关系鼓励智能体进行基于乐观主义的协同探索。

    

    高效的探索对于协作深度多智能体强化学习(MARL)至关重要。本文提出了一种基于UCT(应用于树的置信上限)的探索方法，该方法有效地鼓励协同探索。高层次的思路是，为了进行基于乐观主义的探索，如果每个智能体的乐观估计捕获了与其他智能体的结构化依赖关系，则智能体将实现协作策略。在搜索树的每个节点（即动作）上，UCT使用通过对其父节点的访问计数进行条件推导的奖励来执行基于乐观主义的探索。我们提出了一种将MARL视为树搜索迭代的方法，并开发了一种名为条件乐观探索(COE)的方法。我们假设智能体按顺序执行动作，并将搜索树同一深度的节点视为一个单独智能体的动作。COE计算基于乐观主义的奖励，以鼓励智能体通过尝试新的行动来探索未知的状态。

    Efficient exploration is critical in cooperative deep Multi-Agent Reinforcement Learning (MARL). In this paper, we propose an exploration method that efficiently encourages cooperative exploration based on the idea of the theoretically justified tree search algorithm UCT (Upper Confidence bounds applied to Trees). The high-level intuition is that to perform optimism-based exploration, agents would achieve cooperative strategies if each agent's optimism estimate captures a structured dependency relationship with other agents. At each node (i.e., action) of the search tree, UCT performs optimism-based exploration using a bonus derived by conditioning on the visitation count of its parent node. We provide a perspective to view MARL as tree search iterations and develop a method called Conditionally Optimistic Exploration (COE). We assume agents take actions following a sequential order, and consider nodes at the same depth of the search tree as actions of one individual agent. COE compute
    
[^76]: 一幅图胜过千言万语：语言模型从像素中规划路径

    A Picture is Worth a Thousand Words: Language Models Plan from Pixels. (arXiv:2303.09031v1 [cs.CL])

    [http://arxiv.org/abs/2303.09031](http://arxiv.org/abs/2303.09031)

    本文探讨从像素中使用预训练的语言模型进行规划，相对于之前的方法在ALFWorld和VirtualHome基准测试中取得更好的表现。

    

    规划是人工智能代理执行实际环境中长时间跨度任务的重要能力。本文探讨使用预训练的语言模型（PLMs）来从文本指令中推理出规划序列的方法。之前通过PLM进行规划的方法要么假定观察结果以文本形式可获得（例如由字幕模型提供），要么仅从指令中理解规划，或者只有有限方式地整合了有关视觉环境的信息（例如预训练的可供性函数）。相反，我们展示了即使观察结果直接编码为PLM的输入提示，PLM也能够准确进行规划。我们在ALFWorld和VirtualHome基准测试中实验展示了这种简单方法优于以前的方法。

    Planning is an important capability of artificial agents that perform long-horizon tasks in real-world environments. In this work, we explore the use of pre-trained language models (PLMs) to reason about plan sequences from text instructions in embodied visual environments. Prior PLM based approaches for planning either assume observations are available in the form of text (e.g., provided by a captioning model), reason about plans from the instruction alone, or incorporate information about the visual environment in limited ways (such as a pre-trained affordance function). In contrast, we show that PLMs can accurately plan even when observations are directly encoded as input prompts for the PLM. We show that this simple approach outperforms prior approaches in experiments on the ALFWorld and VirtualHome benchmarks.
    
[^77]: 在深度强化学习中学习奖励以优化全局性能指标

    Learning Rewards to Optimize Global Performance Metrics in Deep Reinforcement Learning. (arXiv:2303.09027v1 [cs.LG])

    [http://arxiv.org/abs/2303.09027](http://arxiv.org/abs/2303.09027)

    该论文提出了一种名为LR4GPM的（深度）强化学习方法，它可以在问题描述中提供的全局性能度量的基础上优化。LR4GPM在学习奖励函数和训练策略时使用了几种训练技巧，能够避免进行奖励工程，其效率高于DAI'2020组织最近举办的一次自动驾驶竞赛的获胜者。

    

    当应用强化学习（RL）到一个新问题上时，奖励工程是一个必要但常常十分困难且容易出错的任务，系统设计师需要面对它。为了避免这一步，我们提出了LR4GPM，一种能够优化全局性能度量的新型（深度）RL方法，这个全局性能度量应该作为问题描述的一部分。LR4GPM交替执行两个阶段：（1）学习（可能是向量）的奖励函数，用于拟合性能度量，（2）训练策略以优化基于学习奖励的这个性能度量的近似值。这样的RL训练并不简单，因为奖励函数和策略都是使用非稳态数据进行训练的。为了克服这个问题，我们提出了几种训练技巧。我们在几个领域展示了LR4GPM的效率。值得注意的是，LR4GPM胜过了DAI'2020组织的最近一次自动驾驶竞赛的获胜者。

    When applying reinforcement learning (RL) to a new problem, reward engineering is a necessary, but often difficult and error-prone task a system designer has to face. To avoid this step, we propose LR4GPM, a novel (deep) RL method that can optimize a global performance metric, which is supposed to be available as part of the problem description. LR4GPM alternates between two phases: (1) learning a (possibly vector) reward function used to fit the performance metric, and (2) training a policy to optimize an approximation of this performance metric based on the learned rewards. Such RL training is not straightforward since both the reward function and the policy are trained using non-stationary data. To overcome this issue, we propose several training tricks. We demonstrate the efficiency of LR4GPM on several domains. Notably, LR4GPM outperforms the winner of a recent autonomous driving competition organized at DAI'2020.
    
[^78]: 使用深度Q网络强化学习的电厂无人机自检方法

    Self-Inspection Method of Unmanned Aerial Vehicles in Power Plants Using Deep Q-Network Reinforcement Learning. (arXiv:2303.09013v1 [cs.RO])

    [http://arxiv.org/abs/2303.09013](http://arxiv.org/abs/2303.09013)

    该论文提出了一种电厂无人机自检的方法，通过使用基于强化学习技术的自主机器人，包括感知、规划和行动，达到优化问题的解决，使用DQN框架进行训练，并考虑内外部因素，提高了检查效率和准确性。

    

    为了检查电厂，可以使用强化学习技术构建自主智能机器人。本方法复制环境并采用简单的强化学习（RL）算法，该策略可应用于多个领域，包括发电行业。研究提出了一个预训练模型，包括感知、规划和行动。为解决无人机导航等优化问题，Deepmind于2015年推出了基于深度学习和Q-learning的强化学习框架Deep Q-network（DQN）。为了克服当前程序存在的问题，研究提出了一种应用UAV自主导航和DQN强化学习的电厂检查系统。这些训练过程对参考状态设置奖励函数，并考虑内部和外部影响因素，这使其与其他强化学习训练有所区别。

    For the purpose of inspecting power plants, autonomous robots can be built using reinforcement learning techniques. The method replicates the environment and employs a simple reinforcement learning (RL) algorithm. This strategy might be applied in several sectors, including the electricity generation sector. A pre-trained model with perception, planning, and action is suggested by the research. To address optimization problems, such as the Unmanned Aerial Vehicle (UAV) navigation problem, Deep Q-network (DQN), a reinforcement learning-based framework that Deepmind launched in 2015, incorporates both deep learning and Q-learning. To overcome problems with current procedures, the research proposes a power plant inspection system incorporating UAV autonomous navigation and DQN reinforcement learning. These training processes set reward functions with reference to states and consider both internal and external effect factors, which distinguishes them from other reinforcement learning train
    
[^79]: 机器学习在流式细胞术数据分析中的应用

    Machine Learning for Flow Cytometry Data Analysis. (arXiv:2303.09007v1 [cs.LG])

    [http://arxiv.org/abs/2303.09007](http://arxiv.org/abs/2303.09007)

    本篇论文介绍了使用机器学习，特别是随机森林算法，来分析流式细胞计数数据的方法。通过此方法，可以提高识别感兴趣的细胞组群的精度和效率，并深入理解数据复杂关系。

    

    流式细胞术主要用于检测细胞中特定标记物的表达，特别适用于检测膜表面受体、抗原、离子或DNA/RNA 表达过程中。现代的流式细胞计数器可以同时快速分析数以万计的细胞，并从单个细胞中测量多个参数。然而，传统分析方法难以解释流式细胞计数数据。因此，本文提出了使用机器学习，特别是随机森林算法，来分析流式细胞计数数据的方法。通过利用机器学习技术，作者证明了在识别感兴趣的细胞组群方面的精度和效率得到了提高，同时提供了对数据复杂关系的深入理解。

    Flow cytometry mainly used for detecting the characteristics of a number of biochemical substances based on the expression of specific markers in cells. It is particularly useful for detecting membrane surface receptors, antigens, ions, or during DNA/RNA expression. Not only can it be employed as a biomedical research tool for recognising distinctive types of cells in mixed populations, but it can also be used as a diagnostic tool for classifying abnormal cell populations connected with disease. Modern flow cytometers can rapidly analyse tens of thousands of cells at the same time while also measuring multiple parameters from a single cell. However, the rapid development of flow cytometers makes it challenging for conventional analysis methods to interpret flow cytometry data. Researchers need to be able to distinguish interesting-looking cell populations manually in multi-dimensional data collected from millions of cells. Thus, it is essential to find a robust approach for analysing f
    
[^80]: 学习大规模容量扩展问题的时空聚合

    Learning Spatio-Temporal Aggregations for Large-Scale Capacity Expansion Problems. (arXiv:2303.08996v1 [cs.LG])

    [http://arxiv.org/abs/2303.08996](http://arxiv.org/abs/2303.08996)

    本文提出了一种新颖的方法，用于有效解决容量扩展问题，该方法通过时空聚合解决了由于网络规模大、节点特征异构等原因而导致的问题，并优于传统方法和现有基准。

    

    有效的投资规划决策对于确保网络物理基础设施在扩展期内满足性能要求至关重要。计算这些决策通常需要解决容量扩展问题。在区域规模的能源系统中，由于网络规模大、节点特征异构、操作周期众多等原因，这些问题往往难以解决。为了保持可行性，传统方法会聚合网络节点和/或选择一组代表性时段。然而，这些简化未能捕捉到供需变化对 CEP 成本和约束的重要影响，导致次优决策。本文提出了一种新颖的图卷积自编码器方法，用于到异构节点的泛型 CEP 的时空聚合。我们的架构利用图池化来识别具有相似特征的节点，并最小化一个多目标损失函数，该函数包括成本和性能目标。我们的方法在合成数据集和实际电网数据集上优于传统方法和现有基准。因此，所提出的方法为决策者提供了一种可扩展的工具，用于评估能源基础设施的容量扩展方案。

    Effective investment planning decisions are crucial to ensure cyber-physical infrastructures satisfy performance requirements over an extended time horizon. Computing these decisions often requires solving Capacity Expansion Problems (CEPs). In the context of regional-scale energy systems, these problems are prohibitively expensive to solve due to large network sizes, heterogeneous node characteristics, and a large number of operational periods. To maintain tractability, traditional approaches aggregate network nodes and/or select a set of representative time periods. Often, these reductions do not capture supply-demand variations that crucially impact CEP costs and constraints, leading to suboptimal decisions. Here, we propose a novel graph convolutional autoencoder approach for spatio-temporal aggregation of a generic CEP with heterogeneous nodes (CEPHN). Our architecture leverages graph pooling to identify nodes with similar characteristics and minimizes a multi-objective loss funct
    
[^81]: 物理信息神经网络用于时域模拟: 精度，计算成本和灵活性

    Physics-Informed Neural Networks for Time-Domain Simulations: Accuracy, Computational Cost, and Flexibility. (arXiv:2303.08994v1 [eess.SY])

    [http://arxiv.org/abs/2303.08994](http://arxiv.org/abs/2303.08994)

    物理信息神经网络被应用于电力系统动态模拟，可以比传统方法快10至1000倍，同时准确性和数值稳定性也足够满足需求。同时，他们也提出了一个基于梯度的方法来规范化NN训练。

    

    电力系统动态模拟是一个计算上昂贵的任务。考虑到发电和需求模式不确定性的增加，需要不断评估数千个场景以确保电力系统的安全性。近年来，物理信息神经网络（PINNs）已被提出作为加速非线性动力系统计算的有前途的解决方案。本文研究了这些方法在电力系统动力学中的适用性，重点关注对负载扰动的动态响应。通过将 PINNs 预测与传统求解器的解决方案进行比较，我们发现 PINNs 可以比传统求解器快 10 到 1000 倍。同时，我们发现它们即使对于大时间步长也足够准确和数值稳定。为了促进更深入的理解，本文还通过在损失函数中引入基于梯度的项，提出了神经网络（NN）训练的新规范化方法。

    The simulation of power system dynamics poses a computationally expensive task. Considering the growing uncertainty of generation and demand patterns, thousands of scenarios need to be continuously assessed to ensure the safety of power systems. Physics-Informed Neural Networks (PINNs) have recently emerged as a promising solution for drastically accelerating computations of non-linear dynamical systems. This work investigates the applicability of these methods for power system dynamics, focusing on the dynamic response to load disturbances. Comparing the prediction of PINNs to the solution of conventional solvers, we find that PINNs can be 10 to 1000 times faster than conventional solvers. At the same time, we find them to be sufficiently accurate and numerically stable even for large time steps. To facilitate a deeper understanding, this paper also presents a new regularisation of Neural Network (NN) training by introducing a gradient-based term in the loss function. The resulting NN
    
[^82]: RMT-SVD实现深度学习权重剪枝：提高准确性和减少过拟合

    Deep Learning Weight Pruning with RMT-SVD: Increasing Accuracy and Reducing Overfitting. (arXiv:2303.08986v1 [cs.LG])

    [http://arxiv.org/abs/2303.08986](http://arxiv.org/abs/2303.08986)

    本文提出了一种使用随机矩阵理论技术进行深度学习权重剪枝的方法，可以通过奇异值分解技术去除一些特定奇异值，从而减少过拟合和提高准确性。

    

    本文提出了使用随机矩阵理论(RMT)训练深度神经网络的应用。我们利用RMT技术来确定在训练DNN过程中应该去除哪些和多少奇异值，以通过奇异值分解(SVD)减少过拟合和提高准确性。在MNIST数据集上实验结果表明，这些技术可以减少全连接层的参数数量而保持或提高DNN的准确性。

    In this work, we present some applications of random matrix theory for the training of deep neural networks. Recently, random matrix theory (RMT) has been applied to the overfitting problem in deep learning. Specifically, it has been shown that the spectrum of the weight layers of a deep neural network (DNN) can be studied and understood using techniques from RMT. In this work, these RMT techniques will be used to determine which and how many singular values should be removed from the weight layers of a DNN during training, via singular value decomposition (SVD), so as to reduce overfitting and increase accuracy. We show the results on a simple DNN model trained on MNIST. In general, these techniques may be applied to any fully connected layer of a pretrained DNN to reduce the number of parameters in the layer while preserving and sometimes increasing the accuracy of the DNN.
    
[^83]: 利用Logistic LASSO回归预测粒子加速器中断

    Forecasting Particle Accelerator Interruptions Using Logistic LASSO Regression. (arXiv:2303.08984v1 [physics.acc-ph])

    [http://arxiv.org/abs/2303.08984](http://arxiv.org/abs/2303.08984)

    该论文提出了一种利用Logistic LASSO回归预测粒子加速器中断的二元分类模型。通过连续的评估指标测量保存的束流时间。

    

    不可预见的粒子加速器中断，也称为联锁，尽管是必要的安全措施，但会导致突然的操作更改。这可能导致大量的束流时间丧失，甚至可能造成设备损坏。我们提出了一个旨在预测Paul Scherrer Institut高强度质子加速器复合体中这种中断的简单而强大的二元分类模型。该模型被制定为逻辑回归最小绝对值收缩和选择算子惩罚，基于一种统计双样本测试来区分加速器的不稳定状态和稳定状态。接收联锁警报的主要目标是允许采取对策并减少束流时间的损失。因此，开发了一个连续的评估指标，用于衡量在任何时间段内节省的束流时间，假设联锁可以通过减少束流电流来规避。最佳表现的联锁-稳定分类

    Unforeseen particle accelerator interruptions, also known as interlocks, lead to abrupt operational changes despite being necessary safety measures. These may result in substantial loss of beam time and perhaps even equipment damage. We propose a simple yet powerful binary classification model aiming to forecast such interruptions, in the case of the High Intensity Proton Accelerator complex at the Paul Scherrer Institut. The model is formulated as logistic regression penalized by least absolute shrinkage and selection operator, based on a statistical two sample test to distinguish between unstable and stable states of the accelerator.  The primary objective for receiving alarms prior to interlocks is to allow for countermeasures and reduce beam time loss. Hence, a continuous evaluation metric is developed to measure the saved beam time in any period, given the assumption that interlocks could be circumvented by reducing the beam current. The best-performing interlock-to-stable classif
    
[^84]: 数据集增强：提高模型准确性和鲁棒性

    Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement. (arXiv:2303.08983v1 [cs.CV])

    [http://arxiv.org/abs/2303.08983](http://arxiv.org/abs/2303.08983)

    提出了一种名为数据集增强的策略，一次性改进数据集，从而提高任何经过增强的数据集训练的模型的准确性、鲁棒性和校准性。例如，使用ImageDataNet+训练的ResNet-50在ImageNet验证集上的准确率提高了1.7％，在ImageNetV2上提高了3.5％，在ImageNet-R上提高了10.0％。

    

    我们提出了一种名为数据集增强的策略，一次性改进数据集，从而提高任何经过增强的数据集训练的模型的准确性，对用户没有额外的训练成本。我们提出了一种基于数据增强和知识蒸馏的数据集增强策略。我们的通用策略是基于广泛的CNN和基于transformer的模型的分析，以及对带有各种数据增强的最先进模型进行大规模的蒸馏研究。我们创建了ImageDataNet+的增强版本，以及增强的数据集CIFAR-100+，Flowers-102+和Food-101+。使用ImageDataNet+训练的模型更准确、更有鲁棒性和校准性，并且对下游任务（例如分割和检测）具有很好的迁移能力。例如，ResNet-50在ImageNet验证集上的准确率提高了1.7％，在ImageNetV2上提高了3.5％，在ImageNet-R上提高了10.0％。在ImageDataNet+上测量的Expected Calibration Error（ECE）也有显著改进。

    We propose Dataset Reinforcement, a strategy to improve a dataset once such that the accuracy of any model architecture trained on the reinforced dataset is improved at no additional training cost for users. We propose a Dataset Reinforcement strategy based on data augmentation and knowledge distillation. Our generic strategy is designed based on extensive analysis across CNN- and transformer-based models and performing large-scale study of distillation with state-of-the-art models with various data augmentations. We create a reinforced version of the ImageNet training dataset, called ImageNet+, as well as reinforced datasets CIFAR-100+, Flowers-102+, and Food-101+. Models trained with ImageNet+ are more accurate, robust, and calibrated, and transfer well to downstream tasks (e.g., segmentation and detection). As an example, the accuracy of ResNet-50 improves by 1.7% on the ImageNet validation set, 3.5% on ImageNetV2, and 10.0% on ImageNet-R. Expected Calibration Error (ECE) on the Ima
    
[^85]: 通过探索每个样本的不确定性和一致性进行主动半监督学习

    Active Semi-Supervised Learning by Exploring Per-Sample Uncertainty and Consistency. (arXiv:2303.08978v1 [cs.CV])

    [http://arxiv.org/abs/2303.08978](http://arxiv.org/abs/2303.08978)

    该论文提出了一种称为主动半监督学习（ASSL）的方法，它结合了传统的主动学习和半监督学习，通过采用指数移动平均和上置信界等技术来解决在未标记数据中确定真实不确定性的问题。

    

    主动学习和半监督学习是两种通过使用少量标记数据和大量未标记数据来降低深度学习成本的技术。为了通过更低的成本提高模型的准确性，我们提出了一种称为主动半监督学习（ASSL）的方法，它结合了主动学习和半监督学习。为了最大化主动学习和半监督学习之间的协同作用，我们专注于ASSL和主动学习之间的差异。由于在训练过程中使用未标记数据，ASSL比主动学习涉及更多的动态模型更新，这导致未标记数据的预测概率存在时间不稳定性。这使得在ASSL中确定未标记数据的真实不确定性变得困难。为了解决这个问题，我们采用了强化学习中使用的指数移动平均（EMA）和上置信界（UCB）等技术。此外，我们通过使用弱和强数据扩充来分析标签噪声对无监督学习的影响。

    Active Learning (AL) and Semi-supervised Learning are two techniques that have been studied to reduce the high cost of deep learning by using a small amount of labeled data and a large amount of unlabeled data. To improve the accuracy of models at a lower cost, we propose a method called Active Semi-supervised Learning (ASSL), which combines AL and SSL. To maximize the synergy between AL and SSL, we focused on the differences between ASSL and AL. ASSL involves more dynamic model updates than AL due to the use of unlabeled data in the training process, resulting in the temporal instability of the predicted probabilities of the unlabeled data. This makes it difficult to determine the true uncertainty of the unlabeled data in ASSL. To address this, we adopted techniques such as exponential moving average (EMA) and upper confidence bound (UCB) used in reinforcement learning. Additionally, we analyzed the effect of label noise on unsupervised learning by using weak and strong augmentation p
    
[^86]: 用于高效的持续模型的门控压缩层

    Gated Compression Layers for Efficient Always-On Models. (arXiv:2303.08970v1 [cs.LG])

    [http://arxiv.org/abs/2303.08970](http://arxiv.org/abs/2303.08970)

    本文提出了一种Gated Compression层，可将现有的神经网络架构转化为Gated Neural Networks，可大幅度降低功耗、提高准确性并利用异构计算核心。实验结果在五个公共数据集上证明所提出的方法有效地压缩了正样本，同时保持或提高了模型准确性。

    

    移动和嵌入式机器学习开发人员经常需要在牺牲精度和大幅度压缩模型以运行在专用低功耗核心、或在更强大的计算核心（如神经处理单元或主应用程序处理器）上运行较大模型之间进行妥协。在本文中，我们提出了一种新颖的门控压缩层，可以将现有神经网络架构转化为门控神经网络。门控神经网络具有多个用于设备上场景的优点，可大大降低功耗、提高准确性并利用异构计算核心。我们在五个公共图像和音频数据集上提供了结果，证明所提出的门控压缩层可以有效地阻止高达96％的负样本，压缩97％的正样本，同时保持或提高模型准确性。

    Mobile and embedded machine learning developers frequently have to compromise between two inferior on-device deployment strategies: sacrifice accuracy and aggressively shrink their models to run on dedicated low-power cores; or sacrifice battery by running larger models on more powerful compute cores such as neural processing units or the main application processor. In this paper, we propose a novel Gated Compression layer that can be applied to transform existing neural network architectures into Gated Neural Networks. Gated Neural Networks have multiple properties that excel for on-device use cases that help significantly reduce power, boost accuracy, and take advantage of heterogeneous compute cores. We provide results across five public image and audio datasets that demonstrate the proposed Gated Compression layer effectively stops up to 96% of negative samples, compresses 97% of positive samples, while maintaining or improving model accuracy.
    
[^87]: 基于时间图神经网络的社群检索方法

    CS-TGN: Community Search via Temporal Graph Neural Networks. (arXiv:2303.08964v1 [cs.SI])

    [http://arxiv.org/abs/2303.08964](http://arxiv.org/abs/2303.08964)

    本论文提出一种基于查询驱动的时空图卷积网络（CS-TGN）方法，可以同时捕捉时间网络中灵活的社群模式和时态动态。

    

    在世界各种复杂网络（如万维网、社交网络、脑网络）中，寻找局部社群是一个挑战性的研究问题，可以支持个性化的社群发现和高级数据分析；网络的时空演化也催生了一些最近的研究，以在时间网络中识别局部社群。本文提出了一种基于查询驱动的时空图卷积网络（CS-TGN），它可以同时捕捉时间网络中灵活的社群模式和时态动态。我们的方法利用查询节点驱动社群搜索，并采用一种新颖的时间图神经网络来建模社群的时态动态。在合成和实际数据集上的广泛实验表明，在效率和准确度方面，CS-TGN优于现有方法。

    Searching for local communities is an important research challenge that allows for personalized community discovery and supports advanced data analysis in various complex networks, such as the World Wide Web, social networks, and brain networks. The evolution of these networks over time has motivated several recent studies to identify local communities in temporal networks. Given any query nodes, Community Search aims to find a densely connected subgraph containing query nodes. However, existing community search approaches in temporal networks have two main limitations: (1) they adopt pre-defined subgraph patterns to model communities, which cannot find communities that do not conform to these patterns in real-world networks, and (2) they only use the aggregation of disjoint structural information to measure quality, missing the dynamic of connections and temporal properties. In this paper, we propose a query-driven Temporal Graph Convolutional Network (CS-TGN) that can capture flexibl
    
[^88]: NESS：从静态子图学习节点嵌入

    NESS: Learning Node Embeddings from Static SubGraphs. (arXiv:2303.08958v1 [cs.LG])

    [http://arxiv.org/abs/2303.08958](http://arxiv.org/abs/2303.08958)

    NESS提出了一种在转导设置下使用图自编码器（GAE）从静态子图（NESS）中学习节点嵌入的新方法，并在多个基准数据集上达到了最新链接预测结果。

    

    我们提出了一个在转导设置下使用图自编码器（GAE）从静态子图（NESS）中学习节点嵌入的框架。此外，我们提出了一种新的对比学习方法。我们证明，与使用整个图或随机子图的当前自编码方法相比，在训练中使用静态子图加上GAE改善了节点表示，用于链接预测任务。NESS包括两个步骤：1）使用随机边缘拆分（RES）将训练图划分为子图，在数据预处理期间，2）聚合从每个子图学习的节点表示，以在测试时间获得图的联合表示。我们的实验表明，NESS改进了广泛的图编码器的性能，并在多个基准数据集上实现了链接预测的最新结果（SOTA）。

    We present a framework for learning Node Embeddings from Static Subgraphs (NESS) using a graph autoencoder (GAE) in a transductive setting. Moreover, we propose a novel approach for contrastive learning in the same setting. We demonstrate that using static subgraphs during training with a GAE improves node representation for link prediction tasks compared to current autoencoding methods using the entire graph or stochastic subgraphs. NESS consists of two steps: 1) Partitioning the training graph into subgraphs using random edge split (RES) during data pre-processing, and 2) Aggregating the node representations learned from each subgraph to obtain a joint representation of the graph at test time. Our experiments show that NESS improves the performance of a wide range of graph encoders and achieves state-of-the-art (SOTA) results for link prediction on multiple benchmark datasets.
    
[^89]: 分布式数据中心中硬盘寿命大规模预测

    Large-scale End-of-Life Prediction of Hard Disks in Distributed Datacenters. (arXiv:2303.08955v1 [cs.LG])

    [http://arxiv.org/abs/2303.08955](http://arxiv.org/abs/2303.08955)

    本文通过定制特征工程和序列学习器，提出一种编码器-解码器LSTM模型，用于大规模预测硬盘剩余寿命，从而降低运营成本

    

    数据中心每天处理海量数据，这些数据存储在价格便宜的硬盘中，用于金融、医疗和航天等重要领域。硬盘的过早损坏和数据的丢失可能会造成灾难性的后果。为了降低故障的风险，云存储提供商执行基于条件的监控并在故障之前更换硬盘。通过估计硬盘剩余寿命，可以预测特定设备的故障时间并在合适的时间内替换硬盘，确保最大利用率同时降低运营成本。本文使用定制的特征工程和一套序列学习器对极度偏斜的健康统计数据进行大规模预测分析。过去的工作表明，使用LSTM预测剩余寿命是一个很好的方法。因此，我们提出了一种编码器-解码器LSTM模型

    On a daily basis, data centers process huge volumes of data backed by the proliferation of inexpensive hard disks. Data stored in these disks serve a range of critical functional needs from financial, and healthcare to aerospace. As such, premature disk failure and consequent loss of data can be catastrophic. To mitigate the risk of failures, cloud storage providers perform condition-based monitoring and replace hard disks before they fail. By estimating the remaining useful life of hard disk drives, one can predict the time-to-failure of a particular device and replace it at the right time, ensuring maximum utilization whilst reducing operational costs. In this work, large-scale predictive analyses are performed using severely skewed health statistics data by incorporating customized feature engineering and a suite of sequence learners. Past work suggests using LSTMs as an excellent approach to predicting remaining useful life. To this end, we present an encoder-decoder LSTM model whe
    
[^90]: 微型时间序列Transformer：深度模型压缩实现低延迟高吞吐量天文瞬变分类

    The Tiny Time-series Transformer: Low-latency High-throughput Classification of Astronomical Transients using Deep Model Compression. (arXiv:2303.08951v1 [astro-ph.IM])

    [http://arxiv.org/abs/2303.08951](http://arxiv.org/abs/2303.08951)

    本论文提出了一种能够以实时、鲁棒和准确的方式处理天文事件的快速高效分类算法，且通过深度压缩方法显著减小模型大小，适应日益增长的实时及时域天文学数据。

    

    由于天文学大数据的出现，机器学习已成为现代科学处理流程中至关重要的组成部分。本文提出了一种新的深度压缩方法，可以将模型大小减小18倍，同时保持分类性能，并展示了该方法如何实现高效的实时分类算法，以适应日益增加的实时及时域天文学数据量。

    A new golden age in astronomy is upon us, dominated by data. Large astronomical surveys are broadcasting unprecedented rates of information, demanding machine learning as a critical component in modern scientific pipelines to handle the deluge of data. The upcoming Legacy Survey of Space and Time (LSST) of the Vera C. Rubin Observatory will raise the big-data bar for time-domain astronomy, with an expected 10 million alerts per-night, and generating many petabytes of data over the lifetime of the survey. Fast and efficient classification algorithms that can operate in real-time, yet robustly and accurately, are needed for time-critical events where additional resources can be sought for follow-up analyses. In order to handle such data, state-of-the-art deep learning architectures coupled with tools that leverage modern hardware accelerators are essential. We showcase how the use of modern deep compression methods can achieve a $18\times$ reduction in model size, whilst preserving class
    
[^91]: 通过ERM实现对贴片攻击的可验证（多）鲁棒性

    Certifiable (Multi)Robustness Against Patch Attacks Using ERM. (arXiv:2303.08944v1 [cs.LG])

    [http://arxiv.org/abs/2303.08944](http://arxiv.org/abs/2303.08944)

    该文研究了针对贴片攻击的防御方法，并提出了一种使用ERM算法学习可证明具有多种蒙版下都具有鲁棒性的预测模型算法，实现了严格的证明多鲁棒性对贴片攻击。

    

    考虑贴片攻击，即在测试时对测试图像进行贴片植入，以诱导有针对性的错误分类。我们关注最近针对这种攻击的一种防御方法——Patch-Cleanser算法。该算法要求预测模型具有“两个蒙版正确性”属性，意味着预测模型在任何时候用任意两个空白蒙版替换图像部分时都应正确分类。我们提出了一种使用ERM（经验风险最小化）算法可以证明多种蒙版下都具有鲁棒性的预测模型学习算法。我们的算法基于多鲁棒性问题的凸松弛和鲁棒优化与基于边界的分类器之间的联系。我们证明了我们的算法在满足一定大小和位置约束的前提下实现了严格的证明多鲁棒性对贴片攻击。我们还通过实验证明了我们的算法对各种类型的贴片攻击都是有效的。

    Consider patch attacks, where at test-time an adversary manipulates a test image with a patch in order to induce a targeted misclassification. We consider a recent defense to patch attacks, Patch-Cleanser (Xiang et al. [2022]). The Patch-Cleanser algorithm requires a prediction model to have a ``two-mask correctness'' property, meaning that the prediction model should correctly classify any image when any two blank masks replace portions of the image. Xiang et al. learn a prediction model to be robust to two-mask operations by augmenting the training set with pairs of masks at random locations of training images and performing empirical risk minimization (ERM) on the augmented dataset.  However, in the non-realizable setting when no predictor is perfectly correct on all two-mask operations on all images, we exhibit an example where ERM fails. To overcome this challenge, we propose a different algorithm that provably learns a predictor robust to all two-mask operations using an ERM orac
    
[^92]: 通过机器学习加强数据空间语义互操作性：一种前瞻性视角

    Enhancing Data Space Semantic Interoperability through Machine Learning: a Visionary Perspective. (arXiv:2303.08932v1 [cs.DB])

    [http://arxiv.org/abs/2303.08932](http://arxiv.org/abs/2303.08932)

    本文提出通过机器学习改善数据空间中的语义互操作性的计划，通过自动生成和更新元数据，产生更灵活的词汇，解决了传统数据交换的局限性，使数据对社区所有成员更具可访问性和价值。

    

    我们的这篇论文阐述了一种通过应用机器学习改善数据空间中语义互操作性的计划。数据空间，即数据在自我管理的环境中在成员之间交换日益受到欢迎。然而，当前在这些空间中管理元数据和词汇的手动实践耗时且容易出错，且可能无法满足所有利益相关者的需求。通过利用机器学习的力量，我们相信可以显著提高数据空间中的语义互操作性。这涉及自动生成和更新元数据，从而产生更灵活的词汇，以适应不同子群体使用的多样化术语。我们对数据空间未来的愿景解决了传统数据交换的局限性，使数据对社区所有成员更具可访问性和价值。

    Our vision paper outlines a plan to improve the future of semantic interoperability in data spaces through the application of machine learning. The use of data spaces, where data is exchanged among members in a self-regulated environment, is becoming increasingly popular. However, the current manual practices of managing metadata and vocabularies in these spaces are time-consuming, prone to errors, and may not meet the needs of all stakeholders. By leveraging the power of machine learning, we believe that semantic interoperability in data spaces can be significantly improved. This involves automatically generating and updating metadata, which results in a more flexible vocabulary that can accommodate the diverse terminologies used by different sub-communities. Our vision for the future of data spaces addresses the limitations of conventional data exchange and makes data more accessible and valuable for all members of the community.
    
[^93]: 应用无监督关键词方法对出院单中提取的概念进行处理

    Applying unsupervised keyphrase methods on concepts extracted from discharge sheets. (arXiv:2303.08928v1 [cs.CL])

    [http://arxiv.org/abs/2303.08928](http://arxiv.org/abs/2303.08928)

    本研究利用临床自然语言处理技术从出院单中提取概念，并应用无监督关键词方法识别重要概念，成功地解决了临床文本中的挑战。

    

    不同医疗保健提供者用各种科学水平和写作风格编写包含有价值患者信息的临床记录。对于处理广泛的电子医疗记录，理解什么信息是必要的可能对临床医生和研究人员有所帮助。实体识别和将其映射到标准术语是减少处理临床记录中的歧义的关键步骤。尽管命名实体识别和实体链接在临床自然语言处理中至关重要，但它们也可能导致产生重复和低价值的概念。因此，有必要确定每个内容记录的部分并识别关键概念以从临床文本中提取含义。本研究通过使用临床自然语言处理技术从出院单中提取概念，并应用无监督关键词方法来识别重要概念来解决这些挑战。结果表明，所提出的方法有效地从临床文本中识别出关键概念。

    Clinical notes containing valuable patient information are written by different health care providers with various scientific levels and writing styles. It might be helpful for clinicians and researchers to understand what information is essential when dealing with extensive electronic medical records. Entities recognizing and mapping them to standard terminologies is crucial in reducing ambiguity in processing clinical notes. Although named entity recognition and entity linking are critical steps in clinical natural language processing, they can also result in the production of repetitive and low-value concepts. In other hand, all parts of a clinical text do not share the same importance or content in predicting the patient's condition. As a result, it is necessary to identify the section in which each content is recorded and also to identify key concepts to extract meaning from clinical texts. In this study, these challenges have been addressed by using clinical natural language proc
    
[^94]: LRDB：基于长短期模型的LSTM原始数据DNA碱基识别器，用于主动学习环境

    LRDB: LSTM Raw data DNA Base-caller based on long-short term models in an active learning environment. (arXiv:2303.08915v1 [q-bio.GN])

    [http://arxiv.org/abs/2303.08915](http://arxiv.org/abs/2303.08915)

    本文提出了LRDB模型，一个轻量级的开源模型，可在关键应用中快速适应新的DNA样本，具有更好的读取标识，并可根据用户约束条件进行修改。

    

    提取DNA字符的第一个重要步骤是使用MinION设备的输出数据，以电流信号的形式呈现。各种尖端的碱基识别器使用这些数据来根据输入来检测DNA字符。本文讨论了先前碱基识别器在时间关键应用、隐私感知设计和灾难性遗忘问题等方面的若干不足。接着，我们提出了LRDB模型，这是一个轻量级开源模型，可用于私有开发，在本文中针对目标细菌样本具有更好的读取标识（增加了0.35%）。我们限制了训练数据的范围，并受益于迁移学习算法，使得LRDB在关键应用中的主动使用具有可行性。因此，适应新的DNA样本（在我们的案例中为细菌样本）需要更少的训练时间。此外，根据用户约束条件，可以修改LRDB，因为结果显示，在使用f的情况下，准确度损失可以忽略不计。

    The first important step in extracting DNA characters is using the output data of MinION devices in the form of electrical current signals. Various cutting-edge base callers use this data to detect the DNA characters based on the input. In this paper, we discuss several shortcomings of prior base callers in the case of time-critical applications, privacy-aware design, and the problem of catastrophic forgetting. Next, we propose the LRDB model, a lightweight open-source model for private developments with a better read-identity (0.35% increase) for the target bacterial samples in the paper. We have limited the extent of training data and benefited from the transfer learning algorithm to make the active usage of the LRDB viable in critical applications. Henceforth, less training time for adapting to new DNA samples (in our case, Bacterial samples) is needed. Furthermore, LRDB can be modified concerning the user constraints as the results show a negligible accuracy loss in case of using f
    
[^95]: 多目标深度强化学习中的潜在条件策略梯度

    Latent-Conditioned Policy Gradient for Multi-Objective Deep Reinforcement Learning. (arXiv:2303.08909v1 [cs.LG])

    [http://arxiv.org/abs/2303.08909](http://arxiv.org/abs/2303.08909)

    该论文提出了一种新的多目标深度强化学习算法，通过策略梯度训练单个神经网络，以在单次训练运行中近似获取整个帕累托集，而不依赖于目标的线性标量化。

    

    在现实世界中进行序列决策通常需要找到平衡相互矛盾的目标的良好平衡点。一般来说，存在大量的帕累托最优策略，它们体现了不同的目标权衡模式，并且使用深度神经网络全面获得它们具有技术挑战性。在本文中，我们提出了一种新的多目标强化学习（MORL）算法，通过策略梯度训练单个神经网络，以在单次训练运行中近似获取整个帕累托集，而不依赖于目标的线性标量化。该方法适用于连续和离散的行动空间，并且不需要修改策略网络的设计。在基准环境中的数字实验证明了我们的方法与标准MORL基线相比的实用性和有效性。

    Sequential decision making in the real world often requires finding a good balance of conflicting objectives. In general, there exist a plethora of Pareto-optimal policies that embody different patterns of compromises between objectives, and it is technically challenging to obtain them exhaustively using deep neural networks. In this work, we propose a novel multi-objective reinforcement learning (MORL) algorithm that trains a single neural network via policy gradient to approximately obtain the entire Pareto set in a single run of training, without relying on linear scalarization of objectives. The proposed method works in both continuous and discrete action spaces with no design change of the policy network. Numerical experiments in benchmark environments demonstrate the practicality and efficacy of our approach in comparison to standard MORL baselines.
    
[^96]: 用 Kernel 方法学习具有能隙的量子哈密顿量的基态

    Learning ground states of gapped quantum Hamiltonians with Kernel Methods. (arXiv:2303.08902v1 [quant-ph])

    [http://arxiv.org/abs/2303.08902](http://arxiv.org/abs/2303.08902)

    本文提出了一种利用 kernel 方法学习具有能隙的量子哈密顿量基态的统计学习方法，理论上需要多项式资源实现，通过数值模拟证明了该方法的有效性，并展示了方法的灵活性。

    

    近年来，利用神经网络来近似量子哈密顿量基态的方法需要解决高度非线性的优化问题。本文提出了一种利用 kernel 方法来使优化变得简单的统计学习方法。我们的方案是功率法的一种近似实现，其中通过监督学习来学习功率迭代的下一步。我们证明，假设监督学习是有效的，那么可以使用多项式资源实现对任意具有能隙的量子哈密顿量的基态性质的计算。我们使用 kernel ridge 回归，通过对一维和二维的几个典型相互作用多体量子系统进行基态的寻找，提供了基于数值模拟的证据，证明了学习假设的有效性，展示了我们方法的灵活性。

    Neural network approaches to approximate the ground state of quantum hamiltonians require the numerical solution of a highly nonlinear optimization problem. We introduce a statistical learning approach that makes the optimization trivial by using kernel methods. Our scheme is an approximate realization of the power method, where supervised learning is used to learn the next step of the power iteration. We show that the ground state properties of arbitrary gapped quantum hamiltonians can be reached with polynomial resources under the assumption that the supervised learning is efficient. Using kernel ridge regression, we provide numerical evidence that the learning assumption is verified by applying our scheme to find the ground states of several prototypical interacting many-body quantum systems, both in one and two dimensions, showing the flexibility of our approach.
    
[^97]: 多保真度深度算子网络方法适用于多尺度系统的封闭问题

    A Multifidelity deep operator network approach to closure for multiscale systems. (arXiv:2303.08893v1 [physics.comp-ph])

    [http://arxiv.org/abs/2303.08893](http://arxiv.org/abs/2303.08893)

    本文提出了一个基于多保真度深度算子网络框架和“内循环”训练方法解决多尺度系统的封闭问题的方法，并且在实验中得到了显著的改进。

    

    基于投影的降阶模型已经成功地用少量广义（或潜在）变量来表示多尺度系统的行为，但是由于未解析尺度和已解析尺度之间的相互作用（称为封闭问题）的不正确处理，导致降阶模型可能存在不准确性和甚至不稳定性。本文将封闭问题视为多保真度问题，并使用多保真度深度算子网络（DeepONet）框架来解决。此外，为了增强基于多保真度的封闭问题的稳定性和/或准确性，本文还采用了物理和机器学习模型耦合文献中最近开发的“内循环”训练方法。最终方法在一维粘性Burgers方程的激波输运和二维Navier-Stokes方程的漩涡合并方面进行了测试。数值实验表明了显著的改进。

    Projection-based reduced order models (PROMs) have shown promise in representing the behavior of multiscale systems using a small set of generalized (or latent) variables. Despite their success, PROMs can be susceptible to inaccuracies, even instabilities, due to the improper accounting of the interaction between the resolved and unresolved scales of the multiscale system (known as the closure problem). In the current work, we interpret closure as a multifidelity problem and use a multifidelity deep operator network (DeepONet) framework to address it. In addition, to enhance the stability and/or accuracy of the multifidelity-based closure, we employ the recently developed "in-the-loop" training approach from the literature on coupling physics and machine learning models. The resulting approach is tested on shock advection for the one-dimensional viscous Burgers equation and vortex merging for the two-dimensional Navier-Stokes equations. The numerical experiments show significant improv
    
[^98]: 物理准确的机器学习在非线性离散时间反馈线性化中的应用

    Discrete-Time Nonlinear Feedback Linearization via Physics-Informed Machine Learning. (arXiv:2303.08884v1 [cs.LG])

    [http://arxiv.org/abs/2303.08884](http://arxiv.org/abs/2303.08884)

    本文提出了一种物理准确的机器学习方法（PIML），用于非线性离散时间动态系统的反馈线性化，通过极点放置来确保稳定性，在非线性转换规律中存在陡峭的梯度时，提出了一种贪心训练方法，其优于传统的数值实现。

    

    本文提出了一种物理准确的机器学习方法（PIML），用于非线性离散时间动态系统的反馈线性化。PIML以一步找到非线性转换规律，从而通过极点放置来确保稳定性。为了在非线性转换规律中存在陡峭的梯度时促进收敛，我们提出了一种贪心训练方法。通过一个基准非线性离散映射，我们评估了所提出的PIML方法的性能，其中反馈线性化转换规律可以通过解析推导得出；由于存在奇点，在感兴趣的域中存在陡峭的梯度。我们展示了所提出的PIML方法在数值逼近精度方面优于传统的数值实现，后者涉及构造并解决一套同调方程的系数的幂级数展开以及实现。

    We present a physics-informed machine learning (PIML) scheme for the feedback linearization of nonlinear discrete-time dynamical systems. The PIML finds the nonlinear transformation law, thus ensuring stability via pole placement, in one step. In order to facilitate convergence in the presence of steep gradients in the nonlinear transformation law, we address a greedy-wise training procedure. We assess the performance of the proposed PIML approach via a benchmark nonlinear discrete map for which the feedback linearization transformation law can be derived analytically; the example is characterized by steep gradients, due to the presence of singularities, in the domain of interest. We show that the proposed PIML outperforms, in terms of numerical approximation accuracy, the traditional numerical implementation, which involves the construction--and the solution in terms of the coefficients of a power-series expansion--of a system of homological equations as well as the implementation of 
    
[^99]: 基于贝叶斯积分的神经网络集成搜索

    Bayesian Quadrature for Neural Ensemble Search. (arXiv:2303.08874v1 [stat.ML])

    [http://arxiv.org/abs/2303.08874](http://arxiv.org/abs/2303.08874)

    本论文介绍了一种使用贝叶斯积分的新方法，可以在架构似然表面有分散、狭窄峰时构建加权集成神经网络，相比当前同类方法，在测试似然性、准确性和期望校准误差方面更为优秀。

    

    集成方法可以提高神经网络的性能，但现有方法在架构似然表面有分散、狭窄峰时效果不佳。此外，现有方法构建均等加权的集成，这可能容易受到较弱架构的失效模式的影响。通过将集成视为近似边缘化架构，我们使用贝叶斯积分的工具构建集成方法——这些工具非常适合探索架构似然表面有分散、狭窄峰的情况。此外，由此产生的集成由体现其性能的架构加权权重组成。我们通过实证研究——在测试似然性、准确性和期望校准误差方面——表明我们的方法优于现有的基线，并通过削减研究验证其各成分的独立性能。

    Ensembling can improve the performance of Neural Networks, but existing approaches struggle when the architecture likelihood surface has dispersed, narrow peaks. Furthermore, existing methods construct equally weighted ensembles, and this is likely to be vulnerable to the failure modes of the weaker architectures. By viewing ensembling as approximately marginalising over architectures we construct ensembles using the tools of Bayesian Quadrature -tools which are well suited to the exploration of likelihood surfaces with dispersed, narrow peaks. Additionally, the resulting ensembles consist of architectures weighted commensurate with their performance. We show empirically -- in terms of test likelihood, accuracy, and expected calibration error -that our method outperforms state-of-the-art baselines, and verify via ablation studies that its components do so independently.
    
[^100]: 基于机器学习驱动的自适应OpenMP实现跨异构系统的可移植性

    Machine Learning-Driven Adaptive OpenMP For Portable Performance on Heterogeneous Systems. (arXiv:2303.08873v1 [cs.PL])

    [http://arxiv.org/abs/2303.08873](http://arxiv.org/abs/2303.08873)

    本文提出了基于机器学习的自适应OpenMP扩展，使用生产者-消费者模式动态地选择最快的代码变体，大大降低了用户在异构体系结构上编程自适应应用的工作量。

    

    异构架构已成为构建高性能计算系统的主流设计选择。但是，异构架构对于实现可执行性的性能可移植性提出了重大挑战。将程序调整到新异构平台上是耗时费力的，需要开发人员手动探索大量执行参数空间。为解决这些挑战，本文提出了新的OpenMP扩展，以实现自主的、机器学习驱动的自适应。我们的解决方案包括一组新颖的语言结构、编译器转换和运行时支持。我们提出了一种生产者消费者模式来灵活地定义多个不同变体的OpenMP代码区域以实现自适应。这些区域在运行时透明地进行分析，自主学习优化的机器学习模型来动态地选择最快的变体。我们的方法显著降低了用户在异构体系结构上编程自适应应用的工作量。

    Heterogeneity has become a mainstream architecture design choice for building High Performance Computing systems. However, heterogeneity poses significant challenges for achieving performance portability of execution. Adapting a program to a new heterogeneous platform is laborious and requires developers to manually explore a vast space of execution parameters. To address those challenges, this paper proposes new extensions to OpenMP for autonomous, machine learning-driven adaptation.  Our solution includes a set of novel language constructs, compiler transformations, and runtime support. We propose a producer-consumer pattern to flexibly define multiple, different variants of OpenMP code regions to enable adaptation. Those regions are transparently profiled at runtime to autonomously learn optimizing machine learning models that dynamically select the fastest variant. Our approach significantly reduces users' efforts of programming adaptive applications on heterogeneous architectures 
    
[^101]: EvalAttAI：一种综合评估鲁棒和非鲁棒模型中的归因映射方法的方法

    EvalAttAI: A Holistic Approach to Evaluating Attribution Maps in Robust and Non-Robust Models. (arXiv:2303.08866v1 [cs.LG])

    [http://arxiv.org/abs/2303.08866](http://arxiv.org/abs/2303.08866)

    该论文提出了一个综合方法EvalAttAI，旨在同时考虑归因映射在各种条件下的稳健性和保真度，以更好地评估归因映射的性能并选择适合特定应用的方法。

    

    可解释的人工智能作为一个研究领域的扩张，已经产生了许多可视化和理解机器学习模型黑盒的方法。归因映射通常用于突出显示影响模型做出特定决策的输入图像的部分。另一方面，机器学习模型对自然噪声和对抗攻击的鲁棒性也正在积极探索。本文重点评估归因映射方法，以找到鲁棒神经网络是否更可解释。我们将这个问题探索在医学成像的分类应用中。可解释性研究已经陷入了僵局。虽然有许多归因映射方法，但目前并没有共识如何评估它们并确定最好的方法。我们在多个数据集（自然和医学成像）和各种归因方法上进行的实验证明，两种流行的评估指标，删除和插入稳健性，不足以评估鲁棒模型中的归因映射。相反，我们提出了一种综合方法EvalAttAI，在各种条件下考虑归因映射的稳健性和保真度。EvalAttAI可以帮助研究人员和实践者更好地评估归因映射的性能，并选择适合其特定应用的方法。

    The expansion of explainable artificial intelligence as a field of research has generated numerous methods of visualizing and understanding the black box of a machine learning model. Attribution maps are generally used to highlight the parts of the input image that influence the model to make a specific decision. On the other hand, the robustness of machine learning models to natural noise and adversarial attacks is also being actively explored. This paper focuses on evaluating methods of attribution mapping to find whether robust neural networks are more explainable. We explore this problem within the application of classification for medical imaging. Explainability research is at an impasse. There are many methods of attribution mapping, but no current consensus on how to evaluate them and determine the ones that are the best. Our experiments on multiple datasets (natural and medical imaging) and various attribution methods reveal that two popular evaluation metrics, Deletion and Ins
    
[^102]: 基于类别引导的图像扩散：通过类别标签从亮场图像中细胞绘画

    Class-Guided Image-to-Image Diffusion: Cell Painting from Brightfield Images with Class Labels. (arXiv:2303.08863v1 [cs.CV])

    [http://arxiv.org/abs/2303.08863](http://arxiv.org/abs/2303.08863)

    该论文介绍了一种新的算法，将图像至图像和类别引导去噪扩散概率模型组合，用于有元数据标签的图像重建问题。实验结果表明，类别引导的图像至图像扩散可以提升图像重建的内容，优于未引导的模型。

    

    在生物医学图像领域，常常出现带有自由或廉价元数据形式的类别标签的图像重建问题。现有的文本引导或风格转移图像重建方法无法转化为提供离散类别的数据集。我们介绍并实现了一种模型，可将图像至图像和类别引导去噪扩散概率模型组合。我们使用真实世界的显微镜图像数据集进行训练，有和没有元数据标签。通过探索具有相关标签的图像至图像扩散的属性，我们展示了类别引导的图像至图像扩散可以提高重建图像的有意义内容，并在有用的下游任务中优于未引导的模型。

    Image-to-image reconstruction problems with free or inexpensive metadata in the form of class labels appear often in biological and medical image domains. Existing text-guided or style-transfer image-to-image approaches do not translate to datasets where additional information is provided as discrete classes. We introduce and implement a model which combines image-to-image and class-guided denoising diffusion probabilistic models. We train our model on a real-world dataset of microscopy images used for drug discovery, with and without incorporating metadata labels. By exploring the properties of image-to-image diffusion with relevant labels, we show that class-guided image-to-image diffusion can improve the meaningful content of the reconstructed images and outperform the unguided model in useful downstream tasks.
    
[^103]: 关于利用结构信息进行规划的好处

    On the Benefits of Leveraging Structural Information in Planning Over the Learned Model. (arXiv:2303.08856v1 [cs.LG])

    [http://arxiv.org/abs/2303.08856](http://arxiv.org/abs/2303.08856)

    研究了利用系统结构信息减少基于模型的强化学习中的样本复杂性的好处。

    

    基于模型的强化学习（RL）将学习和规划结合起来，在近年来受到越来越多的关注。然而，学习模型可能会产生显着的成本（样本复杂性），因为需要为每个状态-动作对获取足够的样本。本文研究了利用系统结构信息减少样本复杂性的好处。具体而言，我们考虑转移概率矩阵是一些结构化参数的已知函数的情况，这些参数的值最初是未知的。然后我们考虑基于与环境的交互来估计这些参数的问题。我们对Q值估计和最优Q值之间的差异进行了特征化，该差异是样本数的函数。我们的分析表明，利用模型的结构信息可以显著减少样本复杂度。我们通过实证研究展示了这些发现。

    Model-based Reinforcement Learning (RL) integrates learning and planning and has received increasing attention in recent years. However, learning the model can incur a significant cost (in terms of sample complexity), due to the need to obtain a sufficient number of samples for each state-action pair. In this paper, we investigate the benefits of leveraging structural information about the system in terms of reducing sample complexity. Specifically, we consider the setting where the transition probability matrix is a known function of a number of structural parameters, whose values are initially unknown. We then consider the problem of estimating those parameters based on the interactions with the environment. We characterize the difference between the Q estimates and the optimal Q value as a function of the number of samples. Our analysis shows that there can be a significant saving in sample complexity by leveraging structural information about the model. We illustrate the findings b
    
[^104]: 无线传感器网络中的机器学习异常检测：综述

    Wireless Sensor Networks anomaly detection using Machine Learning: A Survey. (arXiv:2303.08823v1 [cs.LG])

    [http://arxiv.org/abs/2303.08823](http://arxiv.org/abs/2303.08823)

    本篇综述论文介绍了机器学习技术在无线传感器网络中进行数据异常检测的最新应用。讲述了这种技术能够解决传感数据中异常模式检测的问题，并比较了不同方法的优缺点。

    

    无线传感器网络（WSN）已经在各种公民/军事应用中变得越来越有价值，如工业过程控制、建筑结构强度监测、环境监测、边境入侵、物联网和医疗保健等。然而，WSN产生的传感数据通常噪声和不可靠，这使得检测和诊断异常成为一项挑战。机器学习（ML）技术已广泛应用于解决此问题，通过检测和识别传感数据中的异常模式。本综述论文概述了ML技术在WSN领域数据异常检测方面的最新应用。我们首先介绍WSN的特征和WSN中异常检测的挑战。接着，我们回顾了各种应用于WSN数据异常检测的监督，无监督和半监督学习等ML技术。我们还比较了不同的ML异常检测方法，并强调了它们的优缺点。最后，我们讨论了一些开放的研究问题，并概述了这一领域的未来研究方向。

    Wireless Sensor Networks (WSNs) have become increasingly valuable in various civil/military applications like industrial process control, civil engineering applications such as buildings structural strength monitoring, environmental monitoring, border intrusion, IoT (Internet of Things), and healthcare. However, the sensed data generated by WSNs is often noisy and unreliable, making it a challenge to detect and diagnose anomalies. Machine learning (ML) techniques have been widely used to address this problem by detecting and identifying unusual patterns in the sensed data. This survey paper provides an overview of the state of the art applications of ML techniques for data anomaly detection in WSN domains. We first introduce the characteristics of WSNs and the challenges of anomaly detection in WSNs. Then, we review various ML techniques such as supervised, unsupervised, and semi-supervised learning that have been applied to WSN data anomaly detection. We also compare different ML-base
    
[^105]: 理解事后解释器：以Anchors为例

    Understanding Post-hoc Explainers: The Case of Anchors. (arXiv:2303.08806v1 [stat.ML])

    [http://arxiv.org/abs/2303.08806](http://arxiv.org/abs/2303.08806)

    本文对Anchors进行了理论分析，这是一种基于规则的可解释性方法，用于解释文本分类器的决策。

    

    在许多情况下，机器学习模型可解释性是一项高度要求但难以实现的任务。为了解释这些模型的个体预测，已经提出了本地模型无关方法。然而，产生解释的过程对于用户来说可能与要解释的预测一样神秘。此外，可解释性方法经常缺乏理论保证，并且它们在简单模型上的行为通常是未知的。本文对Anchors（Ribeiro等人，2018）进行理论分析：一种流行的基于规则的可解释性方法，它强调一小组单词以解释文本分类器的决策。

    In many scenarios, the interpretability of machine learning models is a highly required but difficult task. To explain the individual predictions of such models, local model-agnostic approaches have been proposed. However, the process generating the explanations can be, for a user, as mysterious as the prediction to be explained. Furthermore, interpretability methods frequently lack theoretical guarantees, and their behavior on simple models is frequently unknown. While it is difficult, if not impossible, to ensure that an explainer behaves as expected on a cutting-edge model, we can at least ensure that everything works on simple, already interpretable models. In this paper, we present a theoretical analysis of Anchors (Ribeiro et al., 2018): a popular rule-based interpretability method that highlights a small set of words to explain a text classifier's decision. After formalizing its algorithm and providing useful insights, we demonstrate mathematically that Anchors produces meaningf
    
[^106]: 用苏格拉底式的方法引导大型语言模型

    Prompting Large Language Models With the Socratic Method. (arXiv:2303.08769v1 [cs.LG])

    [http://arxiv.org/abs/2303.08769](http://arxiv.org/abs/2303.08769)

    本文提出了使用苏格拉底式方法开发提示模板的系统方法，以有效地与大型语言模型交互，并举出实例展示了这些方法在归纳、演绎和诱导推理方面的有效性。

    

    本论文概述了使用苏格拉底式方法开发提示模板的系统方法，以有效地与大型语言模型（包括GPT-3）交互。我们研究了各种方法，并确定了可以产生精确答案和证明的方法，同时在提高创意写作方面促进创造力和想象力的方法。具体来说，我们讨论了如何应用定义、elenchus、辩证法、母体、概括和反事实推理等技术来编写提示模板，并提供了实际示例，展示它们在归纳、演绎和诱导推理方面的有效性。

    This paper outlines a systematic approach to using the Socratic method in developing prompt templates that effectively interact with large language models, including GPT-3. We examine various methods and identify those that yield precise answers and justifications while simultaneously fostering creativity and imagination to enhance creative writing. Specifically, we discuss how techniques such as definition, elenchus, dialectic, maieutics, generalization, and counterfactual reasoning can be applied in engineering prompt templates, and provide practical examples that demonstrate their effectiveness in performing inductive, deductive, and abductive reasoning.
    
[^107]: 数据驱动的物理知识神经网络求解中子扩散本征值问题的不确定性分析

    On the uncertainty analysis of the data-enabled physics-informed neural network for solving neutron diffusion eigenvalue problem. (arXiv:2303.08455v1 [cs.LG])

    [http://arxiv.org/abs/2303.08455](http://arxiv.org/abs/2303.08455)

    本文针对数据不可避免带有噪声的情况，研究了DEPINN在求解中子扩散本征值问题方面的可行性，并提出了创新的区间损失函数用于减少噪声影响和提高先验数据利用率，此方法在两个基准问题上得到了验证。

    

    在实际工程实验中，通过探测器获得的数据不可避免地带有噪声。本文研究了当先验数据包含不同类型噪声时，已经提出的数据驱动的物理知识神经网络（DEPINN）在计算中子扩散本征值问题时的性能。此外，为了减少噪声的影响，提高噪声先验数据的利用率，本文提出了创新的区间损失函数，并给出了一些严格的数学证明。通过大量的数值结果，本文在两个典型的基准问题上检验了DEPINN的鲁棒性，并通过比较证明了所提出的区间损失函数的有效性。本文确认了改进的DEPINN在核反应堆物理实际工程应用中的可行性。

    In practical engineering experiments, the data obtained through detectors are inevitably noisy. For the already proposed data-enabled physics-informed neural network (DEPINN) \citep{DEPINN}, we investigate the performance of DEPINN in calculating the neutron diffusion eigenvalue problem from several perspectives when the prior data contain different scales of noise. Further, in order to reduce the effect of noise and improve the utilization of the noisy prior data, we propose innovative interval loss functions and give some rigorous mathematical proofs. The robustness of DEPINN is examined on two typical benchmark problems through a large number of numerical results, and the effectiveness of the proposed interval loss function is demonstrated by comparison. This paper confirms the feasibility of the improved DEPINN for practical engineering applications in nuclear reactor physics.
    
[^108]: 基于后训练量化的大型语言模型综合研究

    A Comprehensive Study on Post-Training Quantization for Large Language Models. (arXiv:2303.08302v1 [cs.LG])

    [http://arxiv.org/abs/2303.08302](http://arxiv.org/abs/2303.08302)

    本文基于数万个零-shot实验对基于后训练量化的大型语言模型的不同量化组件进行了综合研究，结果发现细粒度量化和后训练量化方法很重要，用粗粒度量化的更高位数比用非常细粒度的更低位数更强大。我们给出了如何为不同大小的\llms利用量化的建议。

    

    后训练量化是一种减少大型语言模型内存消耗和/或计算成本的权衡方法。然而，关于不同量化方案、不同模型族、不同后训练量化方法、不同量化位精度等的影响的全面研究仍缺失。本文通过数万个零-shot实验对这些组件进行了广泛的研究。我们的研究结果表明：(1)细粒度量化和后训练量化方法(而不是朴素的最近舍入量化)是实现良好精度的必要条件；(2) 用粗粒度量化的更高位数（如5位）比用非常细粒度的更低位数（如4位）（其有效位数与5位相似）更强大。我们还提出了如何为不同大小的\llms利用量化的建议，并留下未来机会和系统工作的建议。

    Post-training quantization (\ptq) had been recently shown as a compromising method to reduce the memory consumption and/or compute cost for large language models. However, a comprehensive study about the effect of different quantization schemes, different model families, different \ptq methods, different quantization bit precision, etc, is still missing. In this work, we provide an extensive study on those components over tens of thousands of zero-shot experiments. Our results show that (1) Fine-grained quantization and \ptq methods (instead of naive round-to-nearest quantization) are necessary to achieve good accuracy and (2) Higher bits (e.g., 5 bits) with coarse-grained quantization is more powerful than lower bits (e.g., 4 bits) with very fine-grained quantization (whose effective bits is similar to 5-bits). We also present recommendations about how to utilize quantization for \llms with different sizes, and leave suggestions of future opportunities and system work that are not res
    
[^109]: 基于图神经网络的公平图过滤替代方法

    Graph Neural Network Surrogates of Fair Graph Filtering. (arXiv:2303.08157v1 [cs.LG])

    [http://arxiv.org/abs/2303.08157](http://arxiv.org/abs/2303.08157)

    通过引入过滤器感知的通用近似框架，该方法定义了合适的图神经网络在运行时训练以满足统计平等约束，同时最小程度扰动原始后验情况下实现此目标。

    

    通过边传播将先前的节点值转换为后来的分数的图滤波器通常支持影响人类的图挖掘任务，例如推荐和排名。因此，重要的是在满足节点组之间的统计平等约束方面使它们公平（例如，按其代表性将分数质量在性别之间均衡分配）。为了在最小程度地扰动原始后验情况下实现此目标，我们引入了一个过滤器感知的通用近似框架，用于后验目标。这定义了适当的图神经网络，其在运行时训练，类似于过滤器，但也在本地优化包括公平感知在内的大类目标。在一组8个过滤器和5个图形的实验中，我们的方法在满足统计平等约束方面表现得不亚于替代品，同时保留基于分数的社区成员推荐的AUC并在传播先前节拍时创建最小实用损失。

    Graph filters that transform prior node values to posterior scores via edge propagation often support graph mining tasks affecting humans, such as recommendation and ranking. Thus, it is important to make them fair in terms of satisfying statistical parity constraints between groups of nodes (e.g., distribute score mass between genders proportionally to their representation). To achieve this while minimally perturbing the original posteriors, we introduce a filter-aware universal approximation framework for posterior objectives. This defines appropriate graph neural networks trained at runtime to be similar to filters but also locally optimize a large class of objectives, including fairness-aware ones. Experiments on a collection of 8 filters and 5 graphs show that our approach performs equally well or better than alternatives in meeting parity constraints while preserving the AUC of score-based community member recommendation and creating minimal utility loss in prior diffusion.
    
[^110]: 用调谐透镜从Transformer中获取潜在的预测能力

    Eliciting Latent Predictions from Transformers with the Tuned Lens. (arXiv:2303.08112v1 [cs.LG])

    [http://arxiv.org/abs/2303.08112](http://arxiv.org/abs/2303.08112)

    本文提出了一种改进版的“逻辑透镜”技术——“调谐透镜”，通过训练一个仿射探针，可以将每个隐藏状态解码成词汇分布。这个方法被应用于各种自回归语言模型上，比逻辑透镜更具有预测性、可靠性和无偏性，并且通过因果实验验证使用的特征与模型本身类似。同时，本文发现潜在预测的轨迹可以用于高精度地检测恶意输入。

    

    本文从迭代推理的角度分析了transformers模型，旨在了解模型预测是如何逐层进行精化的。为了实现这一目的，我们为冻结的预训练模型中的每个块训练一个仿射探针，使得可以将每个隐藏状态解码成词汇分布。我们的方法“调谐透镜”，是“逻辑透镜”技术的改进版本，前者给出了有用的见解，但常常易碎。我们将其应用于各种具有多达20B参数的自回归语言模型，表明其比逻辑透镜更具有预测性、可靠性和无偏性。通过因果实验显示，调谐透镜使用的特征与模型本身类似。我们还发现，潜在预测的轨迹可以用于高精度地检测恶意输入。我们的所有代码都可以在https://github.com/AlignmentResearch/tuned-lens 找到。

    We analyze transformers from the perspective of iterative inference, seeking to understand how model predictions are refined layer by layer. To do so, we train an affine probe for each block in a frozen pretrained model, making it possible to decode every hidden state into a distribution over the vocabulary. Our method, the \emph{tuned lens}, is a refinement of the earlier ``logit lens'' technique, which yielded useful insights but is often brittle.  We test our method on various autoregressive language models with up to 20B parameters, showing it to be more predictive, reliable and unbiased than the logit lens. With causal experiments, we show the tuned lens uses similar features to the model itself. We also find the trajectory of latent predictions can be used to detect malicious inputs with high accuracy. All code needed to reproduce our results can be found at https://github.com/AlignmentResearch/tuned-lens.
    
[^111]: 基于集成学习方法提升COVID-19重症分析

    Enhancing COVID-19 Severity Analysis through Ensemble Methods. (arXiv:2303.07130v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2303.07130](http://arxiv.org/abs/2303.07130)

    本文提出了一种基于领域知识的流程，使用图像处理算法和预训练的UNET模型结合从COVID-19患者中提取感染区域，并使用三种机器学习模型的集成将感染的严重程度分类为不同的类别，从而提高COVID-19重症分析。

    

    计算机断层扫描(CT)提供了肺部的详细图像，允许临床医生观察COVID-19所造成的损伤程度。基于CT的肺部受累程度评分(CTSS)方法用于识别CT扫描中观察到的肺部受累程度。本文提出了一种基于领域知识的流程，使用图像处理算法和预训练的UNET模型结合从COVID-19患者中提取感染区域。然后，使用三种机器学习模型的集成：极端梯度提升、极度随机化树和支持向量机将感染的严重程度分类为不同的类别。该系统在AI-Enabled医学图像分析研讨会和COVID-19诊断大赛(AI-MIA-COV19D)的验证数据集上进行评估，并获得了64\%的宏F1得分。这些结果展示了将领域知识与机器学习技术相结合的精确COVID-19诊断的潜力。

    Computed Tomography (CT) scans provide a detailed image of the lungs, allowing clinicians to observe the extent of damage caused by COVID-19. The CT severity score (CTSS) based scoring method is used to identify the extent of lung involvement observed on a CT scan. This paper presents a domain knowledge-based pipeline for extracting regions of infection in COVID-19 patients using a combination of image-processing algorithms and a pre-trained UNET model. The severity of the infection is then classified into different categories using an ensemble of three machine-learning models: Extreme Gradient Boosting, Extremely Randomized Trees, and Support Vector Machine. The proposed system was evaluated on a validation dataset in the AI-Enabled Medical Image Analysis Workshop and COVID-19 Diagnosis Competition (AI-MIA-COV19D) and achieved a macro F1 score of 64\%. These results demonstrate the potential of combining domain knowledge with machine learning techniques for accurate COVID-19 diagnosis
    
[^112]: 基于Transformer的符号回归规划

    Transformer-based Planning for Symbolic Regression. (arXiv:2303.06833v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06833](http://arxiv.org/abs/2303.06833)

    该论文提出了一种基于Transformer和蒙特卡罗树搜索的符号回归规划策略TPSR，可以将非可微的反馈作为知识的外部来源融入到方程生成过程中，提高方程生成的准确性和复杂性。

    

    符号回归是机器学习中一项具有挑战性的任务，它涉及基于函数值查找其数学表达式。最近，符号回归的一些进展表明，预训练的基于Transformer的模型对于生成方程序列是有效的，这些模型从合成数据集的大规模预训练中获益，并在推理时间方面比基于GP的方法具有显著优势。然而，这些模型关注的是借鉴文本生成的监督预训练目标，而忽略了方程的特定目标，如准确性和复杂性。为了解决这个问题，我们提出了TPSR，一种基于Transformer的符号回归规划策略，将蒙特卡罗树搜索融入到Transformer解码过程中。与传统的解码策略不同，TPSR允许将非可微的反馈（如拟合准确性和复杂性）作为知识的外部来源融入到方程生成过程中。

    Symbolic regression (SR) is a challenging task in machine learning that involves finding a mathematical expression for a function based on its values. Recent advancements in SR have demonstrated the efficacy of pretrained transformer-based models for generating equations as sequences, which benefit from large-scale pretraining on synthetic datasets and offer considerable advantages over GP-based methods in terms of inference time. However, these models focus on supervised pretraining goals borrowed from text generation and ignore equation-specific objectives like accuracy and complexity. To address this, we propose TPSR, a Transformer-based Planning strategy for Symbolic Regression that incorporates Monte Carlo Tree Search into the transformer decoding process. TPSR, as opposed to conventional decoding strategies, allows for the integration of non-differentiable feedback, such as fitting accuracy and complexity, as external sources of knowledge into the equation generation process. Ext
    
[^113]: ODIN：应对数据锁定的按需数据制定方法

    ODIN: On-demand Data Formulation to Mitigate Dataset Lock-in. (arXiv:2303.06832v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06832](http://arxiv.org/abs/2303.06832)

    ODIN采用生成AI模型，通过根据用户需求生成按需数据集，解决了传统零样本学习方法在数据集约束方面的限制，使得人工智能能够学习超出训练数据集的未见知识。

    

    ODIN是一种创新性的方法，通过整合生成式AI模型来解决数据集约束问题。传统的零样本学习方法受训练数据集的限制较大。为了从根本上克服这一限制，ODIN试图通过根据用户需求生成按需数据集来减轻数据集约束。ODIN由三个主要模块组成：提示生成器、文本到图像生成器和图像后处理器。为了生成高质量的提示和图像，我们采用了大型语言模型（例如ChatGPT）和文本到图像扩散模型（例如Stable Diffusion）。我们在各种数据集上对ODIN进行了模型准确性和数据多样性方面的评估以展示其潜力，并进行了进一步的后处理实验。总的来说，ODIN是一种可行的方法，使人工智能能够学习超出训练数据集的未见知识。

    ODIN is an innovative approach that addresses the problem of dataset constraints by integrating generative AI models. Traditional zero-shot learning methods are constrained by the training dataset. To fundamentally overcome this limitation, ODIN attempts to mitigate the dataset constraints by generating on-demand datasets based on user requirements. ODIN consists of three main modules: a prompt generator, a text-to-image generator, and an image post-processor. To generate high-quality prompts and images, we adopted a large language model (e.g., ChatGPT), and a text-to-image diffusion model (e.g., Stable Diffusion), respectively. We evaluated ODIN on various datasets in terms of model accuracy and data diversity to demonstrate its potential, and conducted post-experiments for further investigation. Overall, ODIN is a feasible approach that enables Al to learn unseen knowledge beyond the training dataset.
    
[^114]: 关于深度学习源分离中的神经网络架构：共信道OFDM信号的分离

    On Neural Architectures for Deep Learning-based Source Separation of Co-Channel OFDM Signals. (arXiv:2303.06438v1 [eess.SP])

    [http://arxiv.org/abs/2303.06438](http://arxiv.org/abs/2303.06438)

    本文研究了涉及OFDM信号的单通道源分离问题，通过原型问题评估了使用面向音频的神经网络架构在分离共信道OFDM波形方面的有效性，并提出了关键的领域知识修改网络参数化的解决方案。

    This paper studies the single-channel source separation problem involving OFDM signals and evaluates the efficacy of using audio-oriented neural architectures in separating co-channel OFDM waveforms. Critical domain-informed modifications to the network parameterization are proposed based on insights from OFDM structures.

    本文研究了涉及正交频分复用（OFDM）信号的单通道源分离问题，这种信号在许多现代数字通信系统中普遍存在。在单声道源分离方面已经进行了相关的努力，其中采用了最先进的神经网络架构来训练端到端的音频信号分离器（作为一维时间序列）。通过基于OFDM源模型的原型问题，我们评估并质疑了使用面向音频的神经网络架构在基于通信波形相关特征分离信号方面的有效性。也许令人惊讶的是，我们证明在某些配置中，即使在理论上可以实现完美分离的情况下，这些面向音频的神经网络架构在分离共信道OFDM波形方面表现不佳。然而，我们提出了关键的领域知识修改网络参数化，基于OFDM结构的洞察，可以共同解决这个问题。

    We study the single-channel source separation problem involving orthogonal frequency-division multiplexing (OFDM) signals, which are ubiquitous in many modern-day digital communication systems. Related efforts have been pursued in monaural source separation, where state-of-the-art neural architectures have been adopted to train an end-to-end separator for audio signals (as 1-dimensional time series). In this work, through a prototype problem based on the OFDM source model, we assess -- and question -- the efficacy of using audio-oriented neural architectures in separating signals based on features pertinent to communication waveforms. Perhaps surprisingly, we demonstrate that in some configurations, where perfect separation is theoretically attainable, these audio-oriented neural architectures perform poorly in separating co-channel OFDM waveforms. Yet, we propose critical domain-informed modifications to the network parameterization, based on insights from OFDM structures, that can co
    
[^115]: 神经图形的硬件加速

    Hardware Acceleration of Neural Graphics. (arXiv:2303.05735v2 [cs.AR] UPDATED)

    [http://arxiv.org/abs/2303.05735](http://arxiv.org/abs/2303.05735)

    本文研究了神经图形是否需要硬件支持，发现当前GPU性能无法满足对4K分辨率60FPS渲染的需求，且在增强现实/虚拟现实应用中性能缺口更大。作者确定输入编码和MLP内核是性能瓶颈。

    

    传统的计算机图形学渲染和反渲染算法已被神经表示（NR）所取代。NR最近被用于学习场景的几何和材质属性，并使用这些信息合成真实的图像，因此承诺用可伸缩的质量和可预测的性能替换传统的渲染算法。本文提出问题：神经图形（NG）是否需要硬件支持？我们研究了代表性的NG应用程序，发现如果我们要在当前的GPU上以60FPS渲染4K分辨率，则所需性能与当前GPU的实际性能存在1.5倍至55倍的差距。对于增强现实/虚拟现实应用程序，所需性能与所需系统功率之间存在更大的差距。我们确定输入编码和MLP内核是性能瓶颈，对于多分辨率哈希网格、多分辨率密集网格和低分辨率密集网格，它们占应用程序时间的72％、60％和59％。

    Rendering and inverse-rendering algorithms that drive conventional computer graphics have recently been superseded by neural representations (NR). NRs have recently been used to learn the geometric and the material properties of the scenes and use the information to synthesize photorealistic imagery, thereby promising a replacement for traditional rendering algorithms with scalable quality and predictable performance. In this work we ask the question: Does neural graphics (NG) need hardware support? We studied representative NG applications showing that, if we want to render 4k res. at 60FPS there is a gap of 1.5X-55X in the desired performance on current GPUs. For AR/VR applications, there is an even larger gap of 2-4 OOM between the desired performance and the required system power. We identify that the input encoding and the MLP kernels are the performance bottlenecks, consuming 72%,60% and 59% of application time for multi res. hashgrid, multi res. densegrid and low res. densegrid 
    
[^116]: Baldur：使用大语言模型进行全证明生成和修复

    Baldur: Whole-Proof Generation and Repair with Large Language Models. (arXiv:2303.04910v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04910](http://arxiv.org/abs/2303.04910)

    本文介绍了一种新的自动化形式验证方法，使用经过微调的大型语言模型一次性生成整个定理的证明，而不是一步步进行，从而提高证明效率。

    

    正式验证软件属性是一项非常可取但却需要耗费大量工作的任务。最近的工作已经开发出了使用证明助手（如Coq和Isabelle/HOL）自动化形式验证的方法，例如通过训练模型一次预测一个证明步骤，并使用该模型在可能的证明空间中进行搜索。本文介绍了一种新的方法来自动化形式验证：我们使用基于自然语言文本和代码的大型语言模型，并在证明上进行微调，一次性生成整个定理的证明，而不是一步步进行。我们将此证明生成模型与经过微调的修复模型相结合，以修复生成的证明，进一步增强证明能力。作为本文的主要贡献，本文首次表明：（1）使用transformers进行全证明生成是可能的，并且与不需要昂贵搜索的基于搜索的技术一样有效。 （2）给所学模型额外的上下文，例如之前失败的证明尝试，可以提高证明的效率。

    Formally verifying software properties is a highly desirable but labor-intensive task. Recent work has developed methods to automate formal verification using proof assistants, such as Coq and Isabelle/HOL, e.g., by training a model to predict one proof step at a time, and using that model to search through the space of possible proofs. This paper introduces a new method to automate formal verification: We use large language models, trained on natural language text and code and fine-tuned on proofs, to generate whole proofs for theorems at once, rather than one step at a time. We combine this proof generation model with a fine-tuned repair model to repair generated proofs, further increasing proving power. As its main contributions, this paper demonstrates for the first time that: (1) Whole-proof generation using transformers is possible and is as effective as search-based techniques without requiring costly search. (2) Giving the learned model additional context, such as a prior faile
    
[^117]: 使用不确定性估计指导伪标签的无源自适应域自适应方法研究

    Guiding Pseudo-labels with Uncertainty Estimation for Source-free Unsupervised Domain Adaptation. (arXiv:2303.03770v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.03770](http://arxiv.org/abs/2303.03770)

    本研究提出一种基于损失重新加权策略的无源自适应域自适应（SF-UDA）方法，用于适应目标域，其关键是通过估计伪标签的不确定性来指导其进一步精化，并利用自监督对比框架作为目标空间的正则化器以提高预测精度。

    

    标准的无监督域自适应方法假定在适应过程中同时可用源域和目标域数据。在这项工作中，我们研究了无源自适应域自适应（SF-UDA）方法，它是UDA的一个特殊情况，在该情况下，模型在没有访问源数据的情况下适应目标域。我们提出了一种新的方法来处理SF-UDA设置，基于损失重新加权策略，以增强对伪标签的噪声的鲁棒性。该分类损失基于估计其不确定性来重新加权，以指导伪标签的进一步精化，并通过聚集相邻样本的知识来逐步提高其准确性。此外，我们引入了自监督对比框架来作为目标空间的正则化器，以增强知识的聚合。同时，我们提出了一种负样本对排除策略，以识别和排除由共享相同特征的样本构成的负样本对。

    Standard Unsupervised Domain Adaptation (UDA) methods assume the availability of both source and target data during the adaptation. In this work, we investigate Source-free Unsupervised Domain Adaptation (SF-UDA), a specific case of UDA where a model is adapted to a target domain without access to source data. We propose a novel approach for the SF-UDA setting based on a loss reweighting strategy that brings robustness against the noise that inevitably affects the pseudo-labels. The classification loss is reweighted based on the reliability of the pseudo-labels that is measured by estimating their uncertainty. Guided by such reweighting strategy, the pseudo-labels are progressively refined by aggregating knowledge from neighbouring samples. Furthermore, a self-supervised contrastive framework is leveraged as a target space regulariser to enhance such knowledge aggregation. A novel negative pairs exclusion strategy is proposed to identify and exclude negative pairs made of samples shari
    
[^118]: 用答案启发式方式促使大型语言模型解决基于知识的视觉问答问题

    Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering. (arXiv:2303.01903v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.01903](http://arxiv.org/abs/2303.01903)

    本研究提出了一个名为Prophet的框架，使用答案启发式方式促使GPT-3解决基于知识的视觉问答问题。在特定的知识型VQA数据集上训练一个纯VQA模型，并从中提取出答案启发式，可提高模型的性能。

    

    基于知识的视觉问答需要超出图像范围的外部知识来回答问题。早期的研究从显式知识库（KBs）检索所需的知识，这经常会引入与问题无关的信息，从而限制了模型的性能。最近的研究试图将大型语言模型（即GPT-3）作为隐含式知识引擎来获取回答所需的必要知识。尽管这些方法取得了令人鼓舞的结果，但我们认为它们还没有充分发挥GPT-3的能力，因为提供的输入信息仍然不足。在本文中，我们提出了Prophet——一个概念上简单的框架，旨在通过回答启发式方式，促使GPT-3解决基于知识的VQA问题。具体来说，我们首先在特定的基于知识的VQA数据集上训练一个纯VQA模型，而不使用外部知识。之后，我们从模型中提取了两种互补的答案启发式：答案候选项。

    Knowledge-based visual question answering (VQA) requires external knowledge beyond the image to answer the question. Early studies retrieve required knowledge from explicit knowledge bases (KBs), which often introduces irrelevant information to the question, hence restricting the performance of their models. Recent works have sought to use a large language model (i.e., GPT-3) as an implicit knowledge engine to acquire the necessary knowledge for answering. Despite the encouraging results achieved by these methods, we argue that they have not fully activated the capacity of GPT-3 as the provided input information is insufficient. In this paper, we present Prophet -- a conceptually simple framework designed to prompt GPT-3 with answer heuristics for knowledge-based VQA. Specifically, we first train a vanilla VQA model on a specific knowledge-based VQA dataset without external knowledge. After that, we extract two types of complementary answer heuristics from the model: answer candidates 
    
[^119]: 自监督少样本学习用于缺血性卒中病灶分割

    Self-Supervised Few-Shot Learning for Ischemic Stroke Lesion Segmentation. (arXiv:2303.01332v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2303.01332](http://arxiv.org/abs/2303.01332)

    本文提出了一种利用少量标注数据进行缺血性卒中病灶分割的自监督少样本学习方法，其利用了计算机断层扫描的参数图生成颜色编码的参数图进行训练，可以达到与需要大量标注数据的最先进方法相当的分割效果。

    

    精准的缺血性病灶分割在改善诊断和治疗缺血性卒中等高死亡率疾病方面起着关键作用。然而，许多深度神经网络方法需要在训练期间大量的标注区域，这在医学领域中可能会不切实际。为此，我们提出了一种原型少样本分割方法，仅使用一个标注样本进行训练。所提出的方法利用计算机断层扫描的参数图，生成颜色编码的参数图，在自监督的训练机制中进行调整，使其针对缺血性卒中病灶分割任务。我们证明所提出的训练机制的优点，可以在少样本情况下显着提高性能。有了单个标记的患者扫描，我们的方法可以达到与需要大量标记样本的最先进方法相当的分割效果。

    Precise ischemic lesion segmentation plays an essential role in improving diagnosis and treatment planning for ischemic stroke, one of the prevalent diseases with the highest mortality rate. While numerous deep neural network approaches have recently been proposed to tackle this problem, these methods require large amounts of annotated regions during training, which can be impractical in the medical domain where annotated data is scarce. As a remedy, we present a prototypical few-shot segmentation approach for ischemic lesion segmentation using only one annotated sample during training. The proposed approach leverages a novel self-supervised training mechanism that is tailored to the task of ischemic stroke lesion segmentation by exploiting color-coded parametric maps generated from Computed Tomography Perfusion scans. We illustrate the benefits of our proposed training mechanism, leading to considerable improvements in performance in the few-shot setting. Given a single annotated pati
    
[^120]: AR3n: 一种基于强化学习的机器人康复辅助控制器

    AR3n: A Reinforcement Learning-based Assist-As-Needed Controller for Robotic Rehabilitation. (arXiv:2303.00085v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2303.00085](http://arxiv.org/abs/2303.00085)

    本文提出了一种基于强化学习的机器人康复辅助控制器AR3n，通过使用虚拟患者模型实现控制器的泛化，实时调节机器人辅助力度并最小化机器人辅助的量，该控制器在实验验证中表现出良好的效果。

    

    本文提出了AR3n（发音为Aaron），一种采用强化学习的辅助控制器，可在机器人辅助的书写康复任务中提供适应性辅助。与以往的辅助控制器不同，我们的方法不依赖于患者特定的控制器参数或物理模型。我们建议使用虚拟患者模型来使AR3n推广到多个受试者。该系统实时调节机器人辅助力度，同时最小化机器人辅助的量，基于被试的跟踪误差。通过一组仿真实验和人体受试实验对控制器进行实验验证。最后，进行了与传统基于规则的控制器的比较研究，以分析两种控制器的辅助机制的差异。

    In this paper, we present AR3n (pronounced as Aaron), an assist-as-needed (AAN) controller that utilizes reinforcement learning to supply adaptive assistance during a robot assisted handwriting rehabilitation task. Unlike previous AAN controllers, our method does not rely on patient specific controller parameters or physical models. We propose the use of a virtual patient model to generalize AR3n across multiple subjects. The system modulates robotic assistance in realtime based on a subject's tracking error, while minimizing the amount of robotic assistance. The controller is experimentally validated through a set of simulations and human subject experiments. Finally, a comparative study with a traditional rule-based controller is conducted to analyze differences in assistance mechanisms of the two controllers.
    
[^121]: 实现手术背景推理与手势转换的方法

    Towards Surgical Context Inference and Translation to Gestures. (arXiv:2302.14237v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.14237](http://arxiv.org/abs/2302.14237)

    本研究提出了一种自动推理手势并可解释生成手势转录的方法，利用图像分割的数据丰富性。通过检查工具和物体之间的距离和交点，使用分割蒙版检测手术背景。本方法可以高度一致地自动检测重要的手术状态，对机器人辅助手术具有实际应用价值。

    

    机器人辅助手术中手势的手动标注工作繁琐且容易出错，需要专业知识和培训。本文提出了一种自动推理手势并可解释生成手势转录的方法，利用图像分割的数据丰富性。通过检查工具和物体之间的距离和交点，使用分割蒙版检测手术背景。然后，使用基于知识的有限状态机和数据驱动的长期短期记忆模型将背景标签转换为手势转录。通过将结果与JIGSAWS数据集中的地面真实分割蒙版、共识背景标签和手势标签进行比较，评估了我们方法的每个阶段的表现。我们的结果表明，我们的分割模型在缝合时识别针和线的性能达到了最新水平，并且我们可以高度一致地自动检测重要的手术状态。

    Manual labeling of gestures in robot-assisted surgery is labor intensive, prone to errors, and requires expertise or training. We propose a method for automated and explainable generation of gesture transcripts that leverages the abundance of data for image segmentation. Surgical context is detected using segmentation masks by examining the distances and intersections between the tools and objects. Next, context labels are translated into gesture transcripts using knowledge-based Finite State Machine (FSM) and data-driven Long Short Term Memory (LSTM) models. We evaluate the performance of each stage of our method by comparing the results with the ground truth segmentation masks, the consensus context labels, and the gesture labels in the JIGSAWS dataset. Our results show that our segmentation models achieve state-of-the-art performance in recognizing needle and thread in Suturing and we can automatically detect important surgical states with high agreement with crowd-sourced labels (e
    
[^122]: KS-DETR: 知识共享的注意力学习用于检测Transformer。

    KS-DETR: Knowledge Sharing in Attention Learning for Detection Transformer. (arXiv:2302.11208v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.11208](http://arxiv.org/abs/2302.11208)

    本文提出了一个基于缩放点积注意力的知识共享DETR方法，通过使用真实的前景-背景蒙版进行权重/值学习，进而提高了DETR的准确性。

    

    缩放点积注意力（scaled dot-product attention）使用标准化后的点积对查询和键进行softmax计算来计算权重，然后乘以权重和值。本文研究如何改进缩放点积注意力的学习以提高DETR的准确性。我们的方法基于以下观察：使用真实的前景-背景蒙版（GT Fg-Bg Mask）作为额外的提示能够更好地学习权重/值；通过更好的权重/值，可以学习到更好的值/权重。我们提出了一个三重注意模块，其中第一个注意力是纯粹的缩放点积注意力，第二/三个注意力通过GT Fg-Bg Mask生成高质量的权重/值，并与第一个注意力共享值/权重以提高值/权重的质量。推断时移除第二和第三个注意力。我们称该方法为知识共享DETR（KS-DETR），它是对DETR的扩展。

    Scaled dot-product attention applies a softmax function on the scaled dot-product of queries and keys to calculate weights and then multiplies the weights and values. In this work, we study how to improve the learning of scaled dot-product attention to improve the accuracy of DETR. Our method is based on the following observations: using ground truth foreground-background mask (GT Fg-Bg Mask) as additional cues in the weights/values learning enables learning much better weights/values; with better weights/values, better values/weights can be learned. We propose a triple-attention module in which the first attention is a plain scaled dot-product attention, the second/third attention generates high-quality weights/values (with the assistance of GT Fg-Bg Mask) and shares the values/weights with the first attention to improve the quality of values/weights. The second and third attentions are removed during inference. We call our method knowledge-sharing DETR (KS-DETR), which is an extensio
    
[^123]: 采用以问题为中心的认知表示改进深度序列知识追踪模型的可解释性

    Improving Interpretability of Deep Sequential Knowledge Tracing Models with Question-centric Cognitive Representations. (arXiv:2302.06885v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06885](http://arxiv.org/abs/2302.06885)

    本文介绍了一种名为 QIKT 的基于问题的可解释知识追踪模型，通过采用以问题为中心的认知表示方法，明确地模拟学生的知识状态变化，解决了现有模型中同质化问题的假设对实际情况不准确以及解释预测结果具有挑战性的问题。

    

    知识追踪是预测学生未来表现的关键技术，通过观察他们的历史学习过程来实现。由于深度神经网络强大的表示能力，使用深度学习技术来解决知识追踪问题已取得了显著的进展。然而，现有方法中的大部分都依赖于“同质化问题”的假设，即如果问题具有相同的知识组件集，则它们的贡献是等价的。遗憾的是，这种假设在实际教育场景中是不准确的。此外，从现有的基于深度学习的知识追踪模型中解释预测结果非常具有挑战性。因此，在本文中，我们提出了一种名为QIKT的基于问题的可解释知识追踪模型来应对以上挑战。所提出的QIKT方法采用基于问题的认知表示，明确地模拟学生的知识状态变化，并从问题-答案交互中联合学习问题敏感的认知表示。

    Knowledge tracing (KT) is a crucial technique to predict students' future performance by observing their historical learning processes. Due to the powerful representation ability of deep neural networks, remarkable progress has been made by using deep learning techniques to solve the KT problem. The majority of existing approaches rely on the \emph{homogeneous question} assumption that questions have equivalent contributions if they share the same set of knowledge components. Unfortunately, this assumption is inaccurate in real-world educational scenarios. Furthermore, it is very challenging to interpret the prediction results from the existing deep learning based KT models. Therefore, in this paper, we present QIKT, a question-centric interpretable KT model to address the above challenges. The proposed QIKT approach explicitly models students' knowledge state variations at a fine-grained level with question-sensitive cognitive representations that are jointly learned from a question-c
    
[^124]: OpenHLS：适用于实验科学的低延迟深度神经网络高级综合

    OpenHLS: High-Level Synthesis for Low-Latency Deep Neural Networks for Experimental Science. (arXiv:2302.06751v3 [cs.AR] UPDATED)

    [http://arxiv.org/abs/2302.06751](http://arxiv.org/abs/2302.06751)

    OpenHLS是一个基于高级综合技术的开源编译器框架，将深度神经网络的高级表示转换为适用于近传感器设备的低级表示，解决了实验科学领域数据采集系统中低延迟处理问题。

    

    在许多实验驱动的科学领域，例如高能物理、材料科学和宇宙学中，高数据率实验对数据采集系统施加硬性约束：收集的数据必须无差别地存储以进行后处理和分析，从而需要大容量存储，或者在实时准确过滤时，从而需要低延迟处理。深度神经网络已被证明在其他过滤任务中非常有效，但由于设计和部署困难，尚未广泛应用于此类数据采集系统。我们提出了一个开源的、轻量级的编译器框架OpenHLS，基于高级综合技术，将深度神经网络的高级表示转换为适用于场可编程门阵列等近传感器设备的低级表示，其中没有任何专有依赖项。我们在各种工作负载上评估OpenHLS，并呈现了一个深度神经网络的案例研究实现。

    In many experiment-driven scientific domains, such as high-energy physics, material science, and cosmology, high data rate experiments impose hard constraints on data acquisition systems: collected data must either be indiscriminately stored for post-processing and analysis, thereby necessitating large storage capacity, or accurately filtered in real-time, thereby necessitating low-latency processing. Deep neural networks, effective in other filtering tasks, have not been widely employed in such data acquisition systems, due to design and deployment difficulties. We present an open source, lightweight, compiler framework, without any proprietary dependencies, OpenHLS, based on high-level synthesis techniques, for translating high-level representations of deep neural networks to low-level representations, suitable for deployment to near-sensor devices such as field-programmable gate arrays. We evaluate OpenHLS on various workloads and present a case-study implementation of a deep neural
    
[^125]: 针对帕金森病治疗的闭环深度脑电刺激控制器的离线学习

    Offline Learning of Closed-Loop Deep Brain Stimulation Controllers for Parkinson Disease Treatment. (arXiv:2302.02477v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02477](http://arxiv.org/abs/2302.02477)

    本研究针对DBS治疗帕金森氏症的问题，提出了一种基于离线强化学习框架的闭环深度脑电刺激控制器，以动态调整治疗幅度，减少能量使用，并检测其安全性和性能。

    

    深度脑电刺激（DBS）通过向脑的基底神经节区域传递电脉冲，显示出治疗帕金森氏症（PD）引起的运动症状的巨大潜力。然而，得到美国食品和药物管理局（FDA）批准的DBS仅能以固定幅度提供持续DBS（cDBS）刺激；这种能量低效的操作降低了设备的电池寿命，不能根据活动情况动态调整治疗，可能会引起显著的副作用（如步态障碍）。在本文中，我们介绍了一种离线强化学习（RL）框架，允许利用过去的临床数据来训练一个RL策略，从而在实时调整刺激幅度的同时，以减少能量使用，同时保持与cDBS相同水平的治疗效力（即控制）。此外，临床协议要求在将这种RL控制器部署到患者之前，需证明其安全性和性能。因此，我们还引入了基于模拟技术的安全检测流程。

    Deep brain stimulation (DBS) has shown great promise toward treating motor symptoms caused by Parkinson's disease (PD), by delivering electrical pulses to the Basal Ganglia (BG) region of the brain. However, DBS devices approved by the U.S. Food and Drug Administration (FDA) can only deliver continuous DBS (cDBS) stimuli at a fixed amplitude; this energy inefficient operation reduces battery lifetime of the device, cannot adapt treatment dynamically for activity, and may cause significant side-effects (e.g., gait impairment). In this work, we introduce an offline reinforcement learning (RL) framework, allowing the use of past clinical data to train an RL policy to adjust the stimulation amplitude in real time, with the goal of reducing energy use while maintaining the same level of treatment (i.e., control) efficacy as cDBS. Moreover, clinical protocols require the safety and performance of such RL controllers to be demonstrated ahead of deployments in patients. Thus, we also introduce
    
[^126]: FedPH: 隐私增强型异构联邦学习

    FedPH: Privacy-enhanced Heterogeneous Federated Learning. (arXiv:2301.11705v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11705](http://arxiv.org/abs/2301.11705)

    该论文提出了一种利用预训练模型作为本地模型的骨干，共享嵌入类向量来增强本地模型性能的异构联邦学习方法，并采用隐私保护的混合方法来保护隐私。

    

    联邦学习是一种分布式机器学习环境，允许客户端在不共享私有数据的情况下进行协作学习，通过交换参数来实现。然而，客户端之间的数据分布和计算资源差异使得相关研究变得困难。为了解决这些异构问题，我们提出了一种新的联邦学习方法。我们的方法利用预训练模型作为本地模型的骨干，完全连接的层构成头部。骨干提取头部特征，类的嵌入向量在客户端之间共享，以改善头部并增强本地模型的性能。通过共享类的嵌入向量而不是梯度参数，客户端可以更好地适应私有数据，服务器和客户端之间的通信更加有效。为了保护隐私，我们提出了一种隐私保护的混合方法，向类的嵌入向量添加噪声。

    Federated Learning is a distributed machine-learning environment that allows clients to learn collaboratively without sharing private data. This is accomplished by exchanging parameters. However, the differences in data distributions and computing resources among clients make related studies difficult. To address these heterogeneous problems, we propose a novel Federated Learning method. Our method utilizes a pre-trained model as the backbone of the local model, with fully connected layers comprising the head. The backbone extracts features for the head, and the embedding vector of classes is shared between clients to improve the head and enhance the performance of the local model. By sharing the embedding vector of classes instead of gradient-based parameters, clients can better adapt to private data, and communication between the server and clients is more effective. To protect privacy, we propose a privacy-preserving hybrid method that adds noise to the embedding vector of classes. 
    
[^127]: 学习漩涡动力学用于流体推理和预测

    Learning Vortex Dynamics for Fluid Inference and Prediction. (arXiv:2301.11494v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11494](http://arxiv.org/abs/2301.11494)

    该论文提出了一种基于可微漩涡粒子的方法，从单个视频中推断和预测流体动力学，通过学习一个低维的物理约束流特征表示，相对于现有方法具有更好的性能和稳健性。

    

    我们提出了一种新颖的可微漩涡粒子（DVP）方法，从单个视频中推断和预测流体动力学。其核心是基于粒子的潜在空间，用于包含支撑可观测的欧拉流现象的隐蔽的拉格朗日漩涡演化。我们的可微漩涡粒子与可学习的漩涡到速度动力学映射相结合，以有效地捕捉物理约束的复杂流特征、低维空间。该表示有助于学习针对输入视频的流体模拟器，可提供稳健的、长期的未来预测。我们方法的价值在于两个方面：首先，我们学习的模拟器使得可以纯粹通过视觉观察来推断隐含的物理量（例如速度场）；其次，它还支持未来预测，构造输入视频的序列以及其未来的动态演化。在合成和真实世界视频上，我们将我们的方法与一系列现有方法进行比较，并展示出优越的定量和定性性能，以及对噪声和不完整数据的稳健性。

    We propose a novel differentiable vortex particle (DVP) method to infer and predict fluid dynamics from a single video. Lying at its core is a particle-based latent space to encapsulate the hidden, Lagrangian vortical evolution underpinning the observable, Eulerian flow phenomena. Our differentiable vortex particles are coupled with a learnable, vortex-to-velocity dynamics mapping to effectively capture the complex flow features in a physically-constrained, low-dimensional space. This representation facilitates the learning of a fluid simulator tailored to the input video that can deliver robust, long-term future predictions. The value of our method is twofold: first, our learned simulator enables the inference of hidden physics quantities (e.g., velocity field) purely from visual observation; secondly, it also supports future prediction, constructing the input video's sequel along with its future dynamics evolution. We compare our method with a range of existing methods on both synthe
    
[^128]: 通过超参优化方案实现自主赛车系统的模型参数识别

    Model Parameter Identification via a Hyperparameter Optimization Scheme for Autonomous Racing Systems. (arXiv:2301.01470v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2301.01470](http://arxiv.org/abs/2301.01470)

    本文提出了基于超参优化方案的模型参数识别方法（MIHO），并实现了AV-21全尺寸自主赛车的模型参数识别。MIHO收敛速度比传统方法快13倍以上，参数模型具有良好适应性和泛化能力，车辆在场地测试中达到了217公里/小时的高速行驶和稳定避障性能。

    

    本文提出了一种通过超参数优化方案（MIHO）实现模型参数识别的方法。我们采用高效的探索-利用策略来以数据驱动的方式识别动态模型的参数。我们利用MIHO进行AV-21全尺寸自主赛车的模型参数识别。我们将优化后的参数融入我们平台的基于模型的规划和控制系统的设计中。实验结果显示，MIHO的收敛速度比传统的参数识别方法快13倍以上。此外，通过MIHO学习到的参数模型表现出良好的适应性和泛化能力。我们还进行了广泛的场地测试，证明了我们基于模型的系统具有稳定的避障性能和高速行驶性能，在印第安纳波利斯车速公园和拉斯维加斯车速公园的测试中达到了217公里/小时的高速行驶。

    In this letter, we propose a model parameter identification method via a hyperparameter optimization scheme (MIHO). Our method adopts an efficient explore-exploit strategy to identify the parameters of dynamic models in a data-driven optimization manner. We utilize MIHO for model parameter identification of the AV-21, a full-scaled autonomous race vehicle. We then incorporate the optimized parameters for the design of model-based planning and control systems of our platform. In experiments, MIHO exhibits more than 13 times faster convergence than traditional parameter identification methods. Furthermore, the parametric models learned via MIHO demonstrate good fitness to the given datasets and show generalization ability in unseen dynamic scenarios. We further conduct extensive field tests to validate our model-based system, demonstrating stable obstacle avoidance and high-speed driving up to 217 km/h at the Indianapolis Motor Speedway and Las Vegas Motor Speedway. The source code for M
    
[^129]: HTR模型训练的挑战：《数字时代的档案保护计划》项目反馈

    The Challenges of HTR Model Training: Feedback from the Project Donner le gout de l'archive a l'ere numerique. (arXiv:2212.11146v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.11146](http://arxiv.org/abs/2212.11146)

    本文介绍了使用Transkribus平台改进手写文本识别（HTR）模型性能的实践经验，包括创建转录协议、完整使用语言模型以及确定最佳使用基础模型的方法，这些方法可以将单个模型的性能提高20%以上（达到字符错误率低于5%），并讨论了HTR平台的合作性质和数据分享等挑战。

    

    手写识别技术的出现为文化遗产研究带来了新的可能性，但现在需要反思研究团队开发的经验和实践。我们自2018年以来使用Transkribus平台，致力于提高手写文本识别（HTR）模型的性能，用于转录17世纪的法语手写文本。本文报告了创建转录协议、完整使用语言模型以及确定最佳使用基础模型的影响，以帮助提高HTR模型的性能。将所有这些元素结合起来可以将单个模型的性能提高20%以上（达到字符错误率低于5%）。本文还讨论了HTR平台（如Transkribus）的合作性质以及研究人员如何分享其数据等方面的挑战。

    The arrival of handwriting recognition technologies offers new possibilities for research in heritage studies. However, it is now necessary to reflect on the experiences and the practices developed by research teams. Our use of the Transkribus platform since 2018 has led us to search for the most significant ways to improve the performance of our handwritten text recognition (HTR) models which are made to transcribe French handwriting dating from the 17th century. This article therefore reports on the impacts of creating transcribing protocols, using the language model at full scale and determining the best way to use base models in order to help increase the performance of HTR models. Combining all of these elements can indeed increase the performance of a single model by more than 20% (reaching a Character Error Rate below 5%). This article also discusses some challenges regarding the collaborative nature of HTR platforms such as Transkribus and the way researchers can share their da
    
[^130]: 自然语言处理中人类导向的公平分类

    Human-Guided Fair Classification for Natural Language Processing. (arXiv:2212.10154v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10154](http://arxiv.org/abs/2212.10154)

    本文提出了一种新方法，通过自动生成表达丰富的候选句子对并结合群众外包的成对人类判断，训练一个映射语义相似性到统计代理的个体公平性的模型，用于弥合人类直觉和公平分类规范之间的差距。该方法在表现能力、与人类直觉的一致性和两个基准数据集的分类准确性方面优于先前的方法。

    

    文本分类器在简历筛选和内容审核等高风险任务中具有广泛的应用。这些分类器必须是公平的，并通过对敏感属性（如性别或种族）的扰动不变来避免歧视性决策。然而，人类对这些扰动的直觉与捕捉它们的形式相似度规范之间存在差距。尽管现有研究已经开始解决这个问题，但当前的方法基于硬编码单词替换，导致规范的表达能力有限或者无法充分地与人类直觉相一致（例如，在不对称的反事实情况下）。本研究提出了新方法来弥合这一差距，发现具有表现力和直觉公平规范。我们展示了如何利用无监督式转换和GPT-3的零-shot能力自动生成语义上类似但在敏感属性上有所不同的表达丰富的候选句子对。然后，我们采用群众外包获得这些候选人的成对人类判断，并使用它们来训练模型，将语义相似性映射到统计代理的个体公平性。我们的实验表明，我们的方法在表现能力、与人类直觉的一致性和两个基准数据集的分类准确性方面优于先前的方法。

    Text classifiers have promising applications in high-stake tasks such as resume screening and content moderation. These classifiers must be fair and avoid discriminatory decisions by being invariant to perturbations of sensitive attributes such as gender or ethnicity. However, there is a gap between human intuition about these perturbations and the formal similarity specifications capturing them. While existing research has started to address this gap, current methods are based on hardcoded word replacements, resulting in specifications with limited expressivity or ones that fail to fully align with human intuition (e.g., in cases of asymmetric counterfactuals). This work proposes novel methods for bridging this gap by discovering expressive and intuitive individual fairness specifications. We show how to leverage unsupervised style transfer and GPT-3's zero-shot capabilities to automatically generate expressive candidate pairs of semantically similar sentences that differ along sensit
    
[^131]: 多分辨率在线确定性退火：一种分层和渐进学习架构

    Multi-Resolution Online Deterministic Annealing: A Hierarchical and Progressive Learning Architecture. (arXiv:2212.08189v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.08189](http://arxiv.org/abs/2212.08189)

    本文提出了一种基于逐渐增加子集数量的分区序列的通用的分层学习结构，并使用无梯度随机逼近更新进行在线解决优化问题的方法，可以定义函数逼近问题并使用双时间尺度随机逼近算法的理论解决，模拟了一种退火过程。

    

    随着时间和计算资源的限制，逐步逼近基于数据的优化问题的解决方案的分层学习算法对于决策系统至关重要。本研究提出了一种通用的分层学习结构，基于可能的多分辨率数据空间的渐进分区。最优分区通过解决一系列优化子问题逐步逼近，生成具有逐渐增加的子集数量的分区序列。我们展示对每个优化问题的解可以使用无梯度随机逼近更新进行在线估计。因此，可以在分区的每个子集中定义函数逼近问题，并使用双时间尺度随机逼近算法的理论解决。这模拟了一种退火过程，并定义了一种强大且可解释的启发式方法，逐步增加复杂性。

    Hierarchical learning algorithms that gradually approximate a solution to a data-driven optimization problem are essential to decision-making systems, especially under limitations on time and computational resources. In this study, we introduce a general-purpose hierarchical learning architecture that is based on the progressive partitioning of a possibly multi-resolution data space. The optimal partition is gradually approximated by solving a sequence of optimization sub-problems that yield a sequence of partitions with increasing number of subsets. We show that the solution of each optimization problem can be estimated online using gradient-free stochastic approximation updates. As a consequence, a function approximation problem can be defined within each subset of the partition and solved using the theory of two-timescale stochastic approximation algorithms. This simulates an annealing process and defines a robust and interpretable heuristic method to gradually increase the complexi
    
[^132]: 环面坐标：格点约化实现循环坐标解耦

    Toroidal Coordinates: Decorrelating Circular Coordinates With Lattice Reduction. (arXiv:2212.07201v2 [cs.CG] UPDATED)

    [http://arxiv.org/abs/2212.07201](http://arxiv.org/abs/2212.07201)

    本文研究了环面坐标算法的问题，提出了圆值图形的几何相关性概念，并描述了一个系统性的过程，用于构建最小能量的环面值图。

    

    这篇论文研究了应用环面坐标算法的问题，研究发现当应用于多个上同调类时，即使所选定的上同调类是线性无关的，输出的环面值图也可能会被几何相关。研究表明，通过适当的上同调类的整数线性组合可以获得较不相关的映射。本文提出了圆值图形的几何相关性概念，该概念在黎曼流形情况下对应于狄利克雷形式，这是从狄利克雷能量导出的双线性形式。作者描述了一个系统性的过程，用于构建最小能量的环面值图。

    The circular coordinates algorithm of de Silva, Morozov, and Vejdemo-Johansson takes as input a dataset together with a cohomology class representing a $1$-dimensional hole in the data; the output is a map from the data into the circle that captures this hole, and that is of minimum energy in a suitable sense. However, when applied to several cohomology classes, the output circle-valued maps can be "geometrically correlated" even if the chosen cohomology classes are linearly independent. It is shown in the original work that less correlated maps can be obtained with suitable integer linear combinations of the cohomology classes, with the linear combinations being chosen by inspection. In this paper, we identify a formal notion of geometric correlation between circle-valued maps which, in the Riemannian manifold case, corresponds to the Dirichlet form, a bilinear form derived from the Dirichlet energy. We describe a systematic procedure for constructing low energy torus-valued maps on d
    
[^133]: VideoCoCa: 从对比式字幕生成器实现零样本跨域视频文本建模

    VideoCoCa: Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners. (arXiv:2212.04979v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.04979](http://arxiv.org/abs/2212.04979)

    本文提出了一种名为VideoCoCa的基于对比式字幕生成技术的零样本视频文本建模方法，能够在最小的额外训练下，取得最先进的零样本视频分类和零样本文本到视频检索结果，并在轻微微调情况下也能取得强大的结果。

    

    本文探索了一种高效的方法来建立一个基础的视频-文本模型。我们提出了VideoCoCa，它最大限度地重用了预训练的图像-文本对比式字幕生成器（CoCa）模型，并通过最小的额外训练来适应视频-文本任务。我们发现，在以往的工作中，通过不同的帧间融合模块来适应图像-文本模型，而CoCa中的生成式注意力池化和对比式注意力池化层可以立即适应扁平化的帧嵌入，从而在零样本视频分类和零样本文本到视频检索方面取得了最先进的结果。此外，我们探索了在VideoCoCa之上进行轻微微调的方法，并在视频问答和视频字幕生成方面取得了强大的结果。

    We explore an efficient approach to establish a foundational video-text model. We present VideoCoCa that maximally reuses a pretrained image-text contrastive captioner (CoCa) model and adapt it to video-text tasks with minimal extra training. While previous works adapt image-text models with various cross-frame fusion modules, we find that the generative attentional pooling and contrastive attentional pooling layers in CoCa are instantly adaptable to flattened frame embeddings, yielding state-of-the-art results on zero-shot video classification and zero-shot text-to-video retrieval. Furthermore, we explore lightweight finetuning on top of VideoCoCa, and achieve strong results on video question-answering and video captioning.
    
[^134]: 视觉和结构化语言预训练用于跨模态食物检索

    Vision and Structured-Language Pretraining for Cross-Modal Food Retrieval. (arXiv:2212.04267v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.04267](http://arxiv.org/abs/2212.04267)

    本研究提出了一种新的策略VLPCook，利用视觉-语言预训练技术和结构化数据，实现基于结构化文本的计算料理任务，并在跨模态食物检索任务上取得了超越当前SoTA的优异表现。

    

    视觉-语言预训练（VLP）和基础模型一直是在通用基准上实现最佳结果的常规方法。然而，在更复杂的视觉-语言任务中，比如涉及更结构化输入数据的烹饪应用程序，利用这些强大的技术的研究还很少。在这项工作中，我们提出了一种利用这些技术进行基于结构化文本的计算料理任务的策略。我们的策略被称为VLPCook，首先将现有的图像-文本对转换为图像和结构化文本对。这使得我们可以使用适应于得到的数据集的结构化数据的VLP目标预训练我们的VLPCook模型，然后在下游计算烹饪任务上进行微调。在微调过程中，我们还利用预训练基础模型（例如CLIP）丰富视觉编码器，提供局部和全局文本上下文。VLPCook在跨模态食物检索任务上通过显著的较高比例（+3.3 Recall@1绝对改善）超越了当前的SoTA。

    Vision-Language Pretraining (VLP) and Foundation models have been the go-to recipe for achieving SoTA performance on general benchmarks. However, leveraging these powerful techniques for more complex vision-language tasks, such as cooking applications, with more structured input data, is still little investigated. In this work, we propose to leverage these techniques for structured-text based computational cuisine tasks. Our strategy, dubbed VLPCook, first transforms existing image-text pairs to image and structured-text pairs. This allows to pretrain our VLPCook model using VLP objectives adapted to the strutured data of the resulting datasets, then finetuning it on downstream computational cooking tasks. During finetuning, we also enrich the visual encoder, leveraging pretrained foundation models (e.g. CLIP) to provide local and global textual context. VLPCook outperforms current SoTA by a significant margin (+3.3 Recall@1 absolute improvement) on the task of Cross-Modal Food Retriev
    
[^135]: 深度孵化: 分而治之地训练大型模型

    Deep Incubation: Training Large Models by Divide-and-Conquering. (arXiv:2212.04129v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.04129](http://arxiv.org/abs/2212.04129)

    本文提出了一种称为深度孵化的训练大型模型的新方法，通过将大型模型分成较小的子模块进行训练并无缝地组装起来，从而实现了大型模型的高效、有效训练。

    

    近年来，大型深度学习模型取得了显著成功。然而，由于高计算成本、缓慢的收敛速度和过度拟合等问题，训练这些模型仍然具有挑战性。本文提出了一种称为深度孵化的新方法，通过将大型模型分成较小的子模块进行训练并无缝地组装起来，从而实现了大型模型的高效、有效训练。实现这个想法的一个关键挑战是确保独立训练的子模块的兼容性。为了解决这个问题，我们首先介绍了一个全局的共享元模型，它被用来隐式地将所有模块链接在一起，并且可以设计为一个具有可忽略计算开销的极小网络。然后我们提出了一个模块孵化算法，它训练每个子模块来替换元模型的相应部分并完成给定的学习任务。尽管简单，我们的方法有效地提高了大型模型的训练效率和效果。

    Recent years have witnessed a remarkable success of large deep learning models. However, training these models is challenging due to high computational costs, painfully slow convergence, and overfitting issues. In this paper, we present Deep Incubation, a novel approach that enables the efficient and effective training of large models by dividing them into smaller sub-modules that can be trained separately and assembled seamlessly. A key challenge for implementing this idea is to ensure the compatibility of the independently trained sub-modules. To address this issue, we first introduce a global, shared meta model, which is leveraged to implicitly link all the modules together, and can be designed as an extremely small network with negligible computational overhead. Then we propose a module incubation algorithm, which trains each sub-module to replace the corresponding component of the meta model and accomplish a given learning task. Despite the simplicity, our approach effectively enc
    
[^136]: 超越对象识别：面向对象概念学习的新基准

    Beyond Object Recognition: A New Benchmark towards Object Concept Learning. (arXiv:2212.02710v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.02710](http://arxiv.org/abs/2212.02710)

    本文提出了一个挑战性的 Object Concept Learning (OCL) 任务，涉及对象属性、作用及其因果关系。作者构建了密集注释的知识库以支持 OCL，提出了 Object Concept Reasoning Network (OCRN) 作为基线，提升了对象认知的发展。

    

    理解对象是人工智能的核心，特别是对于具有体验的人工智能而言。虽然深度学习在对象识别方面表现出色，但当前机器仍然难以学习更高层次的知识，例如对象具有哪些属性，以及我们能够使用对象做什么。在本文中，我们提出了一项具有挑战性的 Object Concept Learning (OCL) 任务，以推动对象理解的发展。它要求机器推理出对象的作用，并同时给出原因：是哪些属性使得一个对象具有这些作用。为了支持 OCL，我们构建了一个密集注释的知识库，包括三个层次的对象概念（类别、属性、作用），以及三个层次的因果关系。通过分析 OCL 的因果结构，我们提出了一种基线：Object Concept Reasoning Network (OCRN)。它利用因果干预和概念实例化来推断三个层次，遵循它们之间的因果关系。

    Understanding objects is a central building block of artificial intelligence, especially for embodied AI. Even though object recognition excels with deep learning, current machines still struggle to learn higher-level knowledge, e.g., what attributes an object has, and what can we do with an object. In this work, we propose a challenging Object Concept Learning (OCL) task to push the envelope of object understanding. It requires machines to reason out object affordances and simultaneously give the reason: what attributes make an object possesses these affordances. To support OCL, we build a densely annotated knowledge base including extensive labels for three levels of object concept (category, attribute, affordance), and the causal relations of three levels. By analyzing the causal structure of OCL, we present a baseline, Object Concept Reasoning Network (OCRN). It leverages causal intervention and concept instantiation to infer the three levels following their causal relations. In ex
    
[^137]: 分级策略混合作为最优输运

    Hierarchical Policy Blending As Optimal Transport. (arXiv:2212.01938v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2212.01938](http://arxiv.org/abs/2212.01938)

    提出了一种分层策略混合方法，通过不平衡的最优传输实现策略混合，巩固底层黎曼运动策略的比例，有效调整黎曼矩阵，决定专家和代理人之间的优先级，保证安全和任务成功，并在一系列机器人控制的应用场景中超过现有基线。

    

    我们提出了分级策略混合作为最优输运（HiPBOT）。这种分层框架通过对专家策略的低级反应适应权重，在专家策略和代理人的参数空间上添加了一个前瞻规划层。我们的高层规划者通过不平衡的最优传输实现策略混合，巩固底层黎曼运动策略的比例，有效调整它们的黎曼矩阵，并在专家和代理人之间决定优先级，保证安全和任务成功。我们在从低维导航到高维全身控制的一系列应用场景中的实验结果展示了HiPBOT的功效和效率，该方法优于执行概率推断或定义专家树结构的现有基线，为最优输运在机器人控制中的新应用铺平了道路。

    We present hierarchical policy blending as optimal transport (HiPBOT). This hierarchical framework adapts the weights of low-level reactive expert policies, adding a look-ahead planning layer on the parameter space of a product of expert policies and agents. Our high-level planner realizes a policy blending via unbalanced optimal transport, consolidating the scaling of underlying Riemannian motion policies, effectively adjusting their Riemannian matrix, and deciding over the priorities between experts and agents, guaranteeing safety and task success. Our experimental results in a range of application scenarios from low-dimensional navigation to high-dimensional whole-body control showcase the efficacy and efficiency of HiPBOT, which outperforms state-of-the-art baselines that either perform probabilistic inference or define a tree structure of experts, paving the way for new applications of optimal transport to robot control. More material at https://sites.google.com/view/hipobot
    
[^138]: 针对域泛化的直接影响风险最小化

    Direct-Effect Risk Minimization for Domain Generalization. (arXiv:2211.14594v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.14594](http://arxiv.org/abs/2211.14594)

    本文提出了一种针对域泛化的直接影响风险最小化算法，通过引入因果推断中的直接和间接影响概念解决了相关性转移问题。该算法与现有域泛化算法具有可比性。

    

    本文研究了域外推 generalization 中属性虚假相关性在训练和测试域间变化的问题，即相关性转移问题，该问题对机器学习的可靠性造成了影响。我们引入了因果推断中的直接和间接影响概念来解决域泛化问题。我们认为能够学习到直接影响的模型可以使最坏情况下的风险最小化。我们的算法分为两个阶段：第一阶段，我们通过最小化使用表示和类标签预测域标签的错误来学习间接影响表示；第二阶段，我们通过将训练和验证阶段中具有相似间接影响表示但标签不同的数据相匹配来消除在第一阶段中学习到的间接影响。我们的方法证明了其可与现有的域泛化算法相比较。

    We study the problem of out-of-distribution (o.o.d.) generalization where spurious correlations of attributes vary across training and test domains. This is known as the problem of correlation shift and has posed concerns on the reliability of machine learning. In this work, we introduce the concepts of direct and indirect effects from causal inference to the domain generalization problem. We argue that models that learn direct effects minimize the worst-case risk across correlation-shifted domains. To eliminate the indirect effects, our algorithm consists of two stages: in the first stage, we learn an indirect-effect representation by minimizing the prediction error of domain labels using the representation and the class labels; in the second stage, we remove the indirect effects learned in the first stage by matching each data with another data of similar indirect-effect representation but of different class labels in the training and validation phase. Our approach is shown to be com
    
[^139]: {\mu}Split: 显微镜数据的高效图像分解方法

    {\mu}Split: efficient image decomposition for microscopy data. (arXiv:2211.12872v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.12872](http://arxiv.org/abs/2211.12872)

    uSplit是一种适用于荧光显微镜图像的高效图像分解方法，集成了横向上下文化，帮助训练更深的分层模型，并有效地减少平铺伪影问题。

    

    我们提出了 uSplit，一种专门用于荧光显微镜图像中的训练图像分解的方法。我们发现，使用常规的深度结构体系结构在训练时使用大图像块会获得最佳结果，使内存消耗成为进一步提高性能的限制因素。因此，我们引入了横向上下文化（LC），一种内存高效的方法来训练强大的网络，并展示LC在处理任务时始终带来了显著的改进。我们将LC与U-Nets、分层自编码器和分层VAEs集成，为此我们制定了一种改进的ELBO loss。此外，LC使得训练比原本更深的分层模型成为可能，并且有助于减少使用分割VAE预测时不可避免的平铺伪影。我们将uSplit应用于五个分解任务，一个是合成数据集，另外四个来自实际显微镜数据。LC实现了SOTA的结果（平均im）

    We present uSplit, a dedicated approach for trained image decomposition in the context of fluorescence microscopy images. We find that best results using regular deep architectures are achieved when large image patches are used during training, making memory consumption the limiting factor to further improving performance. We therefore introduce lateral contextualization (LC), a memory efficient way to train powerful networks and show that LC leads to consistent and significant improvements on the task at hand. We integrate LC with U-Nets, Hierarchical AEs, and Hierarchical VAEs, for which we formulate a modified ELBO loss. Additionally, LC enables training deeper hierarchical models than otherwise possible and, interestingly, helps to reduce tiling artefacts that are inherently impossible to avoid when using tiled VAE predictions. We apply uSplit to five decomposition tasks, one on a synthetic dataset, four others derived from real microscopy data. LC achieves SOTA results (average im
    
[^140]: 从节点交互到跳跃交互：新的高效可扩展的图学习范式

    From Node Interaction to Hop Interaction: New Effective and Scalable Graph Learning Paradigm. (arXiv:2211.11761v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.11761](http://arxiv.org/abs/2211.11761)

    本文提出了一种新的跳跃交互范式，用于解决现有图神经网络（GNN）中存在的可扩展性限制和过度平滑问题。其核心思想是将节点之间的交互目标转换为节点内部经过预处理的多跳特征，并通过HopGNN框架实现跳跃交互。

    

    存在的图神经网络（GNN）遵循迭代地在节点之间进行信息交互的信息传递机制。然而，这种节点交互范式仍存在可扩展性限制，即在快速扩展的邻居之间进行节点交互会产生高计算和内存开销；以及过度平滑问题，即节点的判别能力受限，重复节点交互后不同类别节点的表示将会收敛到无法区分的状态。本文提出了一种新的跳跃交互范式，以同时解决这些限制。其核心思想是将节点之间的交互目标转换为节点内部经过预处理的多跳特征。我们设计了一个简单而有效的 HopGNN 框架，可以轻松地利用现有的GNN实现跳跃交往。

    Existing Graph Neural Networks (GNNs) follow the message-passing mechanism that conducts information interaction among nodes iteratively. While considerable progress has been made, such node interaction paradigms still have the following limitation. First, the scalability limitation precludes the broad application of GNNs in large-scale industrial settings since the node interaction among rapidly expanding neighbors incurs high computation and memory costs. Second, the over-smoothing problem restricts the discrimination ability of nodes, i.e., node representations of different classes will converge to indistinguishable after repeated node interactions. In this work, we propose a novel hop interaction paradigm to address these limitations simultaneously. The core idea is to convert the interaction target among nodes to pre-processed multi-hop features inside each node. We design a simple yet effective HopGNN framework that can easily utilize existing GNNs to achieve hop interaction. Fur
    
[^141]: 基于参数化分类的广义类别发现:一个基线研究

    Parametric Classification for Generalized Category Discovery: A Baseline Study. (arXiv:2211.11727v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.11727](http://arxiv.org/abs/2211.11727)

    该研究提出了一种简单而有效的参数化分类方法，该方法可以受益于熵正则化，在多个广义类别发现基准测试中实现最先进的性能，并对未知类别数量具有强大的稳健性。

    

    广义类别发现旨在利用从标注样本中学习到的知识，在未标注的数据集中发现新的类别。先前的研究认为，参数化分类器容易对已知类别过度拟合，并支持使用半监督k均值形成的非参数化分类器。然而，在本研究中，我们调查了参数化分类器的失败情况，验证了当有高质量的监督可用时先前的设计选择的有效性，并确定不可靠的伪标签是一个关键问题。我们证明了存在两种预测偏差：分类器更倾向于更频繁地预测已知的类别，并在已知和新颖类别之间产生一个不平衡的分布。基于这些发现，我们提出了一种简单而有效的参数化分类方法，可以受益于熵正则化，在多个广义类别发现基准测试中实现最先进的性能，并显示对未知类别数量具有强大的稳健性。我们希望这项研究成果能为未来更有效的GCD方法的发展做出贡献。

    Generalized Category Discovery (GCD) aims to discover novel categories in unlabelled datasets using knowledge learned from labelled samples. Previous studies argued that parametric classifiers are prone to overfitting to seen categories, and endorsed using a non-parametric classifier formed with semi-supervised k-means. However, in this study, we investigate the failure of parametric classifiers, verify the effectiveness of previous design choices when high-quality supervision is available, and identify unreliable pseudo-labels as a key problem. We demonstrate that two prediction biases exist: the classifier tends to predict seen classes more often, and produces an imbalanced distribution across seen and novel categories. Based on these findings, we propose a simple yet effective parametric classification method that benefits from entropy regularisation, achieves state-of-the-art performance on multiple GCD benchmarks and shows strong robustness to unknown class numbers. We hope the in
    
[^142]: 插入后门元素的文本编码器对文本生成图像的影响

    Rickrolling the Artist: Injecting Backdoors into Text Encoders for Text-to-Image Synthesis. (arXiv:2211.02408v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.02408](http://arxiv.org/abs/2211.02408)

    本文证明了文本生成图像模型中使用的文本编码器存在重大的篡改风险，并提出了一种基于反向门攻击的方法，可以插入一个单一字符触发器进提示中，从而触发模型生成具有预定义属性的图像或遵循隐藏的、潜在的恶意描述的图像。

    

    尽管文本生成图像技术在研究者和公众中越来越受欢迎，但这些模型的安全性一直被忽视。许多文本生成图像模型依赖于外部来源的预训练文本编码器，并且它们的用户相信检索到的模型会像承诺的那样运行。不幸的是，这可能不是这种情况。我们引入了反向门攻击文本引导的生成模型，并证明它们的文本编码器构成了重大的篡改风险。我们的攻击只是轻微地改变了编码器，使得对于带有干净提示的图像生成没有可疑的模型行为。然后，通过将一个单一字符触发器插入提示中，例如一个非拉丁字符或表情符号，攻击者就可以触发模型生成具有预定义属性的图像或遵循隐藏的、潜在的恶意描述的图像。我们在Stable Diffusion和highligh上经验性地证明了我们攻击的高效性。

    While text-to-image synthesis currently enjoys great popularity among researchers and the general public, the security of these models has been neglected so far. Many text-guided image generation models rely on pre-trained text encoders from external sources, and their users trust that the retrieved models will behave as promised. Unfortunately, this might not be the case. We introduce backdoor attacks against text-guided generative models and demonstrate that their text encoders pose a major tampering risk. Our attacks only slightly alter an encoder so that no suspicious model behavior is apparent for image generations with clean prompts. By then inserting a single character trigger into the prompt, e.g., a non-Latin character or emoji, the adversary can trigger the model to either generate images with pre-defined attributes or images following a hidden, potentially malicious description. We empirically demonstrate the high effectiveness of our attacks on Stable Diffusion and highligh
    
[^143]: 比较扩散生成方法和判别方法在语音复原中的效果

    Analysing Diffusion-based Generative Approaches versus Discriminative Approaches for Speech Restoration. (arXiv:2211.02397v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2211.02397](http://arxiv.org/abs/2211.02397)

    本文比较了生成扩散模型和判别方法在不同的语音恢复任务中的表现，结果发现生成扩散模型在语音降噪和带宽扩展等任务中表现较好。

    

    这篇论文系统地比较了生成扩散模型和判别方法在不同的语音恢复任务中的性能。研究发现，在语音降噪和带宽扩展任务中，生成扩散模型的表现优于判别方法，而在去混响任务中，它们的表现相当。

    Diffusion-based generative models have had a high impact on the computer vision and speech processing communities these past years. Besides data generation tasks, they have also been employed for data restoration tasks like speech enhancement and dereverberation. While discriminative models have traditionally been argued to be more powerful e.g. for speech enhancement, generative diffusion approaches have recently been shown to narrow this performance gap considerably. In this paper, we systematically compare the performance of generative diffusion models and discriminative approaches on different speech restoration tasks. For this, we extend our prior contributions on diffusion-based speech enhancement in the complex time-frequency domain to the task of bandwith extension. We then compare it to a discriminatively trained neural network with the same network architecture on three restoration tasks, namely speech denoising, dereverberation and bandwidth extension. We observe that the ge
    
[^144]: MPCFormer: 基于MPC的快速、高性能和私密的Transformer推理

    MPCFormer: fast, performant and private Transformer inference with MPC. (arXiv:2211.01452v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01452](http://arxiv.org/abs/2211.01452)

    MPCFormer使用MPC和KD技术实现私密的Transformer模型推理，并在速度和性能方面显著提升。

    

    实现私密的推理对于基于Transformer模型的云推理服务非常重要。然而，现有的私密推理解决方案可能会增加推理延迟60倍以上，或显著损害推理质量。在本文中，我们设计了MPCFORMER框架作为实用的解决方案，使用安全多方计算（MPC）和知识蒸馏（KD）。通过广泛的评估，我们证明了MPCFORMER在MPC设置中显著加速了Transformer推理，同时实现了类似于输入模型的ML性能。在IMDb数据集上，它实现了类似于BERTBASE的性能，同时速度快5.3倍。在GLUE基准测试中，它以2.2倍的速度提高了97%的BERTBASE性能。MPCFORMER对不同的经过训练的Transformer权重，如ROBERTABASE和更大的模型，如BERTLarge也保持有效。代码可在https://github.com/MccRee177/MPCFormer获取。

    Enabling private inference is crucial for many cloud inference services that are based on Transformer models. However, existing private inference solutions can increase the inference latency by more than 60x or significantly compromise the inference quality. In this paper, we design the framework MPCFORMER as a practical solution, using Secure Multi-Party Computation (MPC) and Knowledge Distillation (KD). Through extensive evaluations, we show that MPCFORMER significantly speeds up Transformer inference in MPC settings while achieving similar ML performance to the input model. On the IMDb dataset, it achieves similar performance to BERTBASE, while being 5.3x faster. On the GLUE benchmark, it achieves 97% performance of BERTBASE with a 2.2x speedup. MPCFORMER remains effective with different trained Transformer weights such as ROBERTABASE and larger models including BERTLarge. Code is available at https://github.com/MccRee177/MPCFormer.
    
[^145]: 使用图神经网络聚合多边形网格并应用于多重网格求解器

    Agglomeration of Polygonal Grids using Graph Neural Networks with applications to Multigrid solvers. (arXiv:2210.17457v2 [math.NA] UPDATED)

    [http://arxiv.org/abs/2210.17457](http://arxiv.org/abs/2210.17457)

    本文提出了使用图神经网络和 k-means 聚类算法来自动聚合多边形网格，实验结果表明这些策略比标准算法 METIS 更有效率并能显著提高求解器性能。

    

    聚合策略在自适应细化算法和构建可扩展的多级代数求解器中非常重要。为了自动执行多边形网格聚合，我们提出使用机器学习策略。这些策略可以自然地利用网格的几何信息来保持网格质量，提高数值方法的性能并降低总体计算成本。在具体方法上，我们采用了k-means聚类算法和图神经网络(GNNs)来划分计算网格的连接图，并且GNNs具有高速在线推理和处理网格结构和几何信息的优势。我们将这些技术与用于图分割的标准算法METIS进行了比较。实验表明，所提出的策略优于METIS，在计算效率和求解器性能方面都有显着改进。

    Agglomeration-based strategies are important both within adaptive refinement algorithms and to construct scalable multilevel algebraic solvers. In order to automatically perform agglomeration of polygonal grids, we propose the use of Machine Learning (ML) strategies, that can naturally exploit geometrical information about the mesh in order to preserve the grid quality, enhancing performance of numerical methods and reducing the overall computational cost. In particular, we employ the k-means clustering algorithm and Graph Neural Networks (GNNs) to partition the connectivity graph of a computational mesh. Moreover, GNNs have high online inference speed and the advantage to process naturally and simultaneously both the graph structure of mesh and the geometrical information, such as the areas of the elements or their barycentric coordinates. These techniques are compared with METIS, a standard algorithm for graph partitioning, which is meant to process only the graph information of the 
    
[^146]: 对比视觉语言模型中的感知分组

    Perceptual Grouping in Contrastive Vision-Language Models. (arXiv:2210.09996v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.09996](http://arxiv.org/abs/2210.09996)

    本文研究视觉语言模型是否能够理解物体在图像中的位置，并将视觉相关部分组合在一起。我们提出了一些修改，使模型独特地学习了语义和空间信息，并通过多个指标衡量了性能。

    

    最近零样本图像识别的进展表明，视觉语言模型学习了高度语义信息的通用视觉表示，可以通过自然语言短语进行任意探索。然而，理解图像不仅仅是理解图像中包含的内容，更重要的是了解这些内容所在的位置。本文研究了视觉语言模型能否理解物体在图像中的位置，并将视觉相关部分组合在一起。我们展示了基于对比损失和大规模网络数据的现代视觉和语言表示学习模型在有限的目标定位信息上的局限性。我们提出了一组最小的修改，使模型独特地学习了语义和空间信息。我们通过零样本图像识别、无监督自下而上和自上而下的语义分割以及鲁棒的目标定位来衡量模型的性能。

    Recent advances in zero-shot image recognition suggest that vision-language models learn generic visual representations with a high degree of semantic information that may be arbitrarily probed with natural language phrases. Understanding an image, however, is not just about understanding what content resides within an image, but importantly, where that content resides. In this work we examine how well vision-language models are able to understand where objects reside within an image and group together visually related parts of the imagery. We demonstrate how contemporary vision and language representation learning models based on contrastive losses and large web-based data capture limited object localization information. We propose a minimal set of modifications that results in models that uniquely learn both semantic and spatial information. We measure this performance in terms of zero-shot image recognition, unsupervised bottom-up and top-down semantic segmentations, as well as robu
    
[^147]: 语言模型解码作为似然度-效用对齐

    Language Model Decoding as Likelihood-Utility Alignment. (arXiv:2210.07228v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.07228](http://arxiv.org/abs/2210.07228)

    解码算法的选择需要考虑模型似然度和任务效用的匹配度问题，通过分类不匹配缓解策略（MMS）的视角，可以提高解码算法的通用性

    

    成功的语言生成流程中的关键组件是解码算法。然而，应该指导选择解码算法的一般原则仍不清楚。以前的研究仅在狭窄的情况下比较解码算法，他们的发现不能推广到跨任务。我们认为模型的似然和任务特定效用的不匹配是理解解码算法有效性的关键因素。为了结构化讨论，我们引入一种不匹配缓解策略（MMS）的分类法，提供解码作为对齐工具的统一视角。这个MMS分类法根据解码算法对似然度-效用不匹配的隐含假设对其进行分组，产生关于它们跨任务适用性的一般性声明。具体而言，通过分析跨多个任务的预测的似然度和效用之间的相关性，我们提供了实证证据

    A critical component of a successful language generation pipeline is the decoding algorithm. However, the general principles that should guide the choice of a decoding algorithm remain unclear. Previous works only compare decoding algorithms in narrow scenarios, and their findings do not generalize across tasks. We argue that the misalignment between the model's likelihood and the task-specific notion of utility is the key factor to understanding the effectiveness of decoding algorithms. To structure the discussion, we introduce a taxonomy of misalignment mitigation strategies (MMSs), providing a unifying view of decoding as a tool for alignment. The MMS taxonomy groups decoding algorithms based on their implicit assumptions about likelihood--utility misalignment, yielding general statements about their applicability across tasks. Specifically, by analyzing the correlation between the likelihood and the utility of predictions across a diverse set of tasks, we provide empirical evidence
    
[^148]: DISCOVER：通过增强的强化学习深度识别符号简洁的开放式PDE

    DISCOVER: Deep identification of symbolically concise open-form PDEs via enhanced reinforcement-learning. (arXiv:2210.02181v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.02181](http://arxiv.org/abs/2210.02181)

    本文提出一种增强的深度强化学习框架，通过结构感知的递归神经网络代理和稀疏回归方法，在少量先前知识的情况下发现符号简洁的开放式PDE，并通过优秀的奖励函数和基于模型的强化学习进行更新和优化。

    

    复杂自然系统的工作机制往往服从简洁深刻的偏微分方程(PDE)。直接从数据中挖掘方程的方法被称为PDE发现，它揭示了一致的物理定律，并促进了我们与自然界的适应性互动。本文提出了一个增强的深度强化学习框架，以少量先前知识发现符号简洁的开放式PDE。特别地，基于基本运算符和操作数的符号库，设计了一个结构感知的递归神经网络代理，并与稀疏回归方法无缝结合，生成简洁而开放的PDE表达式。通过平衡数据适应性和简洁性的完美设计的奖励函数，对所有生成的PDE进行评估，并通过基于模型的强化学习高效地进行更新。定制的约束和规则用于保证PDE在术语上的合理性。

    The working mechanisms of complex natural systems tend to abide by concise and profound partial differential equations (PDEs). Methods that directly mine equations from data are called PDE discovery, which reveals consistent physical laws and facilitates our adaptive interaction with the natural world. In this paper, an enhanced deep reinforcement-learning framework is proposed to uncover symbolically concise open-form PDEs with little prior knowledge. Particularly, based on a symbol library of basic operators and operands, a structure-aware recurrent neural network agent is designed and seamlessly combined with the sparse regression method to generate concise and open-form PDE expressions. All of the generated PDEs are evaluated by a meticulously designed reward function by balancing fitness to data and parsimony, and updated by the model-based reinforcement learning in an efficient way. Customized constraints and regulations are formulated to guarantee the rationality of PDEs in term
    
[^149]: 学习最小违反连续控制以实现不可行线性时态逻辑规范

    Learning Minimally-Violating Continuous Control for Infeasible Linear Temporal Logic Specifications. (arXiv:2210.01162v4 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.01162](http://arxiv.org/abs/2210.01162)

    本文提出了一个模型自由框架，使用深度强化学习来实现复杂高级任务的目标驱动导航。通过将先前的多目标DRL问题转化为一个单一目标问题，并使用基于采样的路径规划算法来指导DRL智能体，该方法可以满足不可行的线性时态逻辑任务并尽可能减少违规。

    

    本文研究了连续时间控制综合，以实现线性时态逻辑(LTL)表达的复杂高级任务的目标驱动导航。我们提出了一个模型自由框架，使用深度强化学习(DRL)，其中底层动态系统未知（透明盒子）。与先前的工作不同，本文考虑了给定的LTL规范可能是不可行的情况，因此无法全局完成。我们不修改给定的LTL公式，而是提供了一个通用的DRL方法，以最小违规满足它。为了做到这一点，我们将先前的多目标DRL问题转化为一个单一目标问题，该问题要求同时实现自动机满足和最小违规代价。通过使用基于采样的路径规划算法来指导可能不可行的LTL任务的DRL智能体，所提出的方法减轻了DRL的近视倾向，这在学习可以具有长或无限持续时间的一般LTL任务时经常是一个问题。

    This paper explores continuous-time control synthesis for target-driven navigation to satisfy complex high-level tasks expressed as linear temporal logic (LTL). We propose a model-free framework using deep reinforcement learning (DRL) where the underlying dynamic system is unknown (an opaque box). Unlike prior work, this paper considers scenarios where the given LTL specification might be infeasible and therefore cannot be accomplished globally. Instead of modifying the given LTL formula, we provide a general DRL-based approach to satisfy it with minimal violation. To do this, we transform a previously multi-objective DRL problem, which requires simultaneous automata satisfaction and minimum violation cost, into a single objective. By guiding the DRL agent with a sampling-based path planning algorithm for the potentially infeasible LTL task, the proposed approach mitigates the myopic tendencies of DRL, which are often an issue when learning general LTL tasks that can have long or infin
    
[^150]: 神经网络通过随机梯度下降有效地学习低维表示

    Neural Networks Efficiently Learn Low-Dimensional Representations with SGD. (arXiv:2209.14863v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2209.14863](http://arxiv.org/abs/2209.14863)

    本文研究使用随机梯度下降训练任意宽度的两层神经网络的问题，当输入为高斯分布，目标为多指数模型时，NN的第一层权重会收敛到真实模型中$k$维主子空间, 可以通过在子空间上使用均匀收敛建立广义误差边界为$O(\sqrt{{kd}/{T}})$, SGD训练的ReLU NN可以学习形如$y = f(\langle\boldsymbol{u},\boldsymbol{x}\rangle)$的单指数目标。

    

    本文研究了使用随机梯度下降训练任意宽度的两层神经网络(NN)的问题，其中输入$ \boldsymbol{x} \in \mathbb {R}^d $为高斯分布，目标$ y \in \mathbb {R}$遵循多指数模型，即 $ y = g(\langle\boldsymbol{u_1},\boldsymbol{x}\rangle,...,\langle\boldsymbol{u_k},\boldsymbol{x}\rangle)$ ，其中函数$g$为有噪声的连接函数，我们证明当使用带有权重衰减的在线SGD进行训练时，NN的第一层权重会收敛到真实模型中$ \boldsymbol{u_1},...,\boldsymbol{u_k}$的$k$维主子空间，当$k \ll d$ 时，该现象有几个重要的影响。首先，通过在这个更小的子空间上使用均匀收敛，我们建立了在SGD进行$T$次迭代后的广义误差边界为$ O(\sqrt{{kd}/{T}})$，这不依赖于NN的宽度。我们进一步证明，SGD训练的ReLU NN可以学习形如$y = f(\langle\boldsymbol{u},\boldsymbol{x}\rangle)$的单指数目标.

    We study the problem of training a two-layer neural network (NN) of arbitrary width using stochastic gradient descent (SGD) where the input $\boldsymbol{x}\in \mathbb{R}^d$ is Gaussian and the target $y \in \mathbb{R}$ follows a multiple-index model, i.e., $y=g(\langle\boldsymbol{u_1},\boldsymbol{x}\rangle,...,\langle\boldsymbol{u_k},\boldsymbol{x}\rangle)$ with a noisy link function $g$. We prove that the first-layer weights of the NN converge to the $k$-dimensional principal subspace spanned by the vectors $\boldsymbol{u_1},...,\boldsymbol{u_k}$ of the true model, when online SGD with weight decay is used for training. This phenomenon has several important consequences when $k \ll d$. First, by employing uniform convergence on this smaller subspace, we establish a generalization error bound of $O(\sqrt{{kd}/{T}})$ after $T$ iterations of SGD, which is independent of the width of the NN. We further demonstrate that, SGD-trained ReLU NNs can learn a single-index target of the form $y=f
    
[^151]: 自然语言处理方法识别在临床记录中高风险的癌症患者

    Natural Language Processing Methods to Identify Oncology Patients at High Risk for Acute Care with Clinical Notes. (arXiv:2209.13860v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.13860](http://arxiv.org/abs/2209.13860)

    本文研究了利用自然语言处理方法识别化疗后癌症患者急救护理风险的问题，与以往使用结构化卫生数据进行预测的模型相比较，结果表明两者差异不大，从而说明了在临床应用中采用语言模型的可行性以及不同患者群体风险偏差的存在。

    

    临床记录是健康记录中的重要组成部分。本篇论文评估了如何利用自然语言处理（NLP）识别化疗开始后癌症患者急救护理（ACU）的风险。使用结构化卫生数据（SHD）进行风险预测已成为标准，但使用自由文本格式进行预测更为复杂。本文探讨了使用自由文本笔记而非SHD进行ACU预测的方法。深度学习模型与手动构建的语言特征进行了比较。结果表明，SHD模型略胜于NLP模型； SHD的l1-罚项逻辑回归模型实现的C统计量为0.748（95％-CI：0.735，0.762），而具有语言特征的相同模型实现的C统计量为0.730（95％-CI：0.717，0.745），而基于变压器的模型实现的C统计量为0.702（95％-CI：0.688，0.717）。本文展示了语言模型在临床应用中的应用，并强调不同患者群体的风险偏差是不同的，即使只使用自由文本数据也是如此。

    Clinical notes are an essential component of a health record. This paper evaluates how natural language processing (NLP) can be used to identify the risk of acute care use (ACU) in oncology patients, once chemotherapy starts. Risk prediction using structured health data (SHD) is now standard, but predictions using free-text formats are complex. This paper explores the use of free-text notes for the prediction of ACU instead of SHD. Deep Learning models were compared to manually engineered language features. Results show that SHD models minimally outperform NLP models; an l1-penalised logistic regression with SHD achieved a C-statistic of 0.748 (95%-CI: 0.735, 0.762), while the same model with language features achieved 0.730 (95%-CI: 0.717, 0.745) and a transformer-based model achieved 0.702 (95%-CI: 0.688, 0.717). This paper shows how language models can be used in clinical applications and underlines how risk bias is different for diverse patient groups, even using only free-text dat
    
[^152]: 重新审视医学图像分割:极度有限标签下的医学图像分割

    Mine yOur owN Anatomy: Revisiting Medical Image Segmentation with Extremely Limited Labels. (arXiv:2209.13476v4 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2209.13476](http://arxiv.org/abs/2209.13476)

    提出了一种新的医学图像分割方法MOAN，该方法可以在极度有限的标签情况下实现高性能。MOAN由两个互补的流水线组成：协同邻域挖掘和自监督对比学习。实验结果表明，MOAN在Dice评分和其他评估指标方面优于现有最先进方法。

    

    最近，关于对比学习的研究在医学图像分割的背景下仅凭借几个标签取得了卓越的性能。现有方法主要集中在实体区分和不变映射上。然而，它们面临三个常见瓶颈：(1)尾部分布：医学图像数据通常遵循隐含的长尾类分布。盲目利用所有训练像素可能导致数据不平衡问题，并导致性能恶化；(2)一致性：由于不同解剖特征之间的类内变化，分割模型是否学会了有意义且一致的解剖特征仍不清楚；以及(3)多样性：整个数据集内部切片的相关性受到的关注显著较少。这促使我们寻找一个基于数据集本身的策略方法，从不同的解剖视图中发现相似但不同的样本。在本文中，我们介绍了一个新的框架，称为MOAN，用于极度有限标签下的医学图像分割。 MOAN由两个互补的流水线组成：协同邻域挖掘和自监督对比学习。前者提取共同依赖的像素区域，而后者强制网络学习有意义的解剖学表示。实验结果表明，MOAN在Dice评分和其他评估指标方面优于现有最先进方法。

    Recent studies on contrastive learning have achieved remarkable performance solely by leveraging few labels in the context of medical image segmentation. Existing methods mainly focus on instance discrimination and invariant mapping. However, they face three common pitfalls: (1) tailness: medical image data usually follows an implicit long-tail class distribution. Blindly leveraging all pixels in training hence can lead to the data imbalance issues, and cause deteriorated performance; (2) consistency: it remains unclear whether a segmentation model has learned meaningful and yet consistent anatomical features due to the intra-class variations between different anatomical features; and (3) diversity: the intra-slice correlations within the entire dataset have received significantly less attention. This motivates us to seek a principled approach for strategically making use of the dataset itself to discover similar yet distinct samples from different anatomical views. In this paper, we i
    
[^153]: LSAP: 重新思考GAN潜空间中反演的保真度、感知和可编辑性

    LSAP: Rethinking Inversion Fidelity, Perception and Editability in GAN Latent Space. (arXiv:2209.12746v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.12746](http://arxiv.org/abs/2209.12746)

    LSAP通过对潜空间实现对齐解决了反演和编辑结果中保真度、感知和可编辑性的问题，使得在保留重建保真度的前提下具有更好的感知和可编辑性。

    

    随着方法的发展，反演主要分为两个步骤。第一步是图像嵌入，在这个步骤中，编码器或者优化过程嵌入图像以获取相应的潜在码。之后，第二步旨在改善反演和编辑结果，我们称之为结果细化。尽管第二步显著提高了保真度，但感知和可编辑性几乎没有改变，深度依赖于在第一步中获得的反向潜在码。因此，重要的问题是在保留重建保真度的同时获得具有更好感知和可编辑性的潜在码。在这项工作中，我们首先指出这两个特征与反向码与合成分布的对齐（或不对齐）程度有关。然后，我们提出了潜空间对齐反演范例（LSAP），其中包括评估指标和解决此问题的解决方法。具体而言，我们引入了标准化风格空间（$\mathcal{S^N}$）和标准化内容空间（$\mathcal{C^N}$），分别在风格和内容上对齐正向和负向潜在码和合成分布。 LSAP在各种任务中都取得了最先进的结果，例如图像编辑、图像转换和图像合成。此外，我们证明了LSAP具有比以前方法更好的特性，如改进的可编辑性、视觉质量和更少的模式崩塌。

    As the methods evolve, inversion is mainly divided into two steps. The first step is Image Embedding, in which an encoder or optimization process embeds images to get the corresponding latent codes. Afterward, the second step aims to refine the inversion and editing results, which we named Result Refinement. Although the second step significantly improves fidelity, perception and editability are almost unchanged, deeply dependent on inverse latent codes attained in the first step. Therefore, a crucial problem is gaining the latent codes with better perception and editability while retaining the reconstruction fidelity. In this work, we first point out that these two characteristics are related to the degree of alignment (or disalignment) of the inverse codes with the synthetic distribution. Then, we propose Latent Space Alignment Inversion Paradigm (LSAP), which consists of evaluation metric and solution for this problem. Specifically, we introduce Normalized Style Space ($\mathcal{S^N
    
[^154]: 基于不确定性和自训练的要素对齐进行源自由无监督域自适应

    Feature Alignment by Uncertainty and Self-Training for Source-Free Unsupervised Domain Adaptation. (arXiv:2208.14888v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2208.14888](http://arxiv.org/abs/2208.14888)

    提出了一种新的源自由无监督域适应(UDA)方法，该方法仅使用预训练的源模型和无标签目标图像。方法对模型的特征生成器进行训练，通过捕获因素不确定性、一致性约束和两个不同的自训练阶段，增强了模型的适应能力，并在多个基准数据集上实现了优越性能。

    

    大多数无监督域自适应方法假定在模型适应期间有可用的标记源图像。然而，由于机密性问题或移动设备的内存限制，这种假设通常是不可行的。为了解决这些问题，我们提出了一种新颖的源自由无监督域自适应方法，仅使用预训练的源模型和无标签目标图像。我们的方法通过加入数据增强来捕获因素不确定性，并使用两个一致性目标来训练特征生成器。该特征生成器被鼓励在头分类器的决策边界之外学习一致的视觉特征。因此，适应模型对图像扰动的鲁棒性更强。受自监督学习的启发，我们的方法在促进预测空间和特征空间之间的内空间对齐的同时，保持数据分布的一致性，从而增强了适应能力。我们的方法采用了两个源主导和目标主导的不同自训练阶段，以增加源自适应模型的泛化能力。我们的方法在多个基准数据集上进行了实验，证明了其优越性。

    Most unsupervised domain adaptation (UDA) methods assume that labeled source images are available during model adaptation. However, this assumption is often infeasible owing to confidentiality issues or memory constraints on mobile devices. Some recently developed approaches do not require source images during adaptation, but they show limited performance on perturbed images. To address these problems, we propose a novel source-free UDA method that uses only a pre-trained source model and unlabeled target images. Our method captures the aleatoric uncertainty by incorporating data augmentation and trains the feature generator with two consistency objectives. The feature generator is encouraged to learn consistent visual features away from the decision boundaries of the head classifier. Thus, the adapted model becomes more robust to image perturbations. Inspired by self-supervised learning, our method promotes inter-space alignment between the prediction space and the feature space while
    
[^155]: 利用稳健分类器引导改进扩散式图像合成

    Enhancing Diffusion-Based Image Synthesis with Robust Classifier Guidance. (arXiv:2208.08664v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2208.08664](http://arxiv.org/abs/2208.08664)

    该论文提出利用来自时间相关的对手稳健分类器的梯度来指导生成扩散模型，以促进生成结果的改善。

    

    去噪扩散概率模型（DDPM）是一类最新的生成模型族，能够达到最先进的结果。为了获得类条件生成，建议利用来自时间相关分类器的梯度来指导扩散过程。尽管这个想法理论上很正确，但基于深度学习的分类器容易受到基于梯度的对手攻击的影响。因此，虽然传统分类器可能达到良好的准确性分数，但它们的梯度可能不可靠，可能会妨碍生成结果的改善。最近的研究发现，对手稳健分类器的梯度与人类感知一致，这些分类器可以更好地引导生成过程朝着语义相关的图像方向进行。我们利用这一观察结果，定义和训练一个时间相关的对手稳健分类器，并将其用作生成扩散模型的指导。在高度具有挑战性和多样性的自然图像数据集上进行实验，实验结果表明，我们的方法能够改进扩散模型的生成效果。

    Denoising diffusion probabilistic models (DDPMs) are a recent family of generative models that achieve state-of-the-art results. In order to obtain class-conditional generation, it was suggested to guide the diffusion process by gradients from a time-dependent classifier. While the idea is theoretically sound, deep learning-based classifiers are infamously susceptible to gradient-based adversarial attacks. Therefore, while traditional classifiers may achieve good accuracy scores, their gradients are possibly unreliable and might hinder the improvement of the generation results. Recent work discovered that adversarially robust classifiers exhibit gradients that are aligned with human perception, and these could better guide a generative process towards semantically meaningful images. We utilize this observation by defining and training a time-dependent adversarially robust classifier and use it as guidance for a generative diffusion model. In experiments on the highly challenging and di
    
[^156]: 使用基于模型的树和提升方法拟合低阶函数ANOVA模型

    Using Model-Based Trees with Boosting to Fit Low-Order Functional ANOVA Models. (arXiv:2207.06950v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.06950](http://arxiv.org/abs/2207.06950)

    本文提出了一种新算法GAMI-Tree，使用基于模型的树以及新的交互过滤方法，可以更好地拟合底层交互，具有更好的预测性能和更高的效率。

    

    低阶函数ANOVA模型已经被机器学习社区重新发现，并称之为内在可解释的机器学习。本文提出了一种新算法GAMI-Tree，类似于EBM，但具有一些趋向更好性能的特性。我们采用模型为基础的树，并融入一种新的交互过滤方法，提高了对底层交互的捕捉。此外，我们的迭代训练方法收敛于具有更好预测性能的模型，并确保相互作用在分层意义上正交于主效应。该算法不需要广泛的调整，并且实现快速高效。我们使用模拟和真实数据集进行比较。

    Low-order functional ANOVA (fANOVA) models have been rediscovered in the machine learning (ML) community under the guise of inherently interpretable machine learning. Explainable Boosting Machines or EBM (Lou et al. 2013) and GAMI-Net (Yang et al. 2021) are two recently proposed ML algorithms for fitting functional main effects and second-order interactions. We propose a new algorithm, called GAMI-Tree, that is similar to EBM, but has a number of features that lead to better performance. It uses model-based trees as base learners and incorporates a new interaction filtering method that is better at capturing the underlying interactions. In addition, our iterative training method converges to a model with better predictive performance, and the embedded purification ensures that interactions are hierarchically orthogonal to main effects. The algorithm does not need extensive tuning, and our implementation is fast and efficient. We use simulated and real datasets to compare the performanc
    
[^157]: 在文本数据上比较特征重要性和规则提取的可解释性

    Comparing Feature Importance and Rule Extraction for Interpretability on Text Data. (arXiv:2207.01420v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2207.01420](http://arxiv.org/abs/2207.01420)

    本文比较了文本数据上两类方法：计算每个特征的重要性分数和提取简单逻辑规则，发现在相同模型下产生的解释也不同。我们提出了一种比较解释差异的方法。

    

    复杂的机器学习算法在涉及文本数据的关键任务中越来越常见，这导致了可解释性方法的发展。在局部方法中，出现了两种族群：一种计算每个特征的重要性分数，另一种则提取简单的逻辑规则。在本文中，我们展示了使用不同方法可以导致意外不同的解释，即使应用于那些我们预计会有定性巧合的简单模型上。为了量化这种影响，我们提出一种比较不同方法产生的解释的新方法。

    Complex machine learning algorithms are used more and more often in critical tasks involving text data, leading to the development of interpretability methods. Among local methods, two families have emerged: those computing importance scores for each feature and those extracting simple logical rules. In this paper we show that using different methods can lead to unexpectedly different explanations, even when applied to simple models for which we would expect qualitative coincidence. To quantify this effect, we propose a new approach to compare explanations produced by different methods.
    
[^158]: FibeRed: 通过向量丛描述拓扑复杂数据的纤维降维方法

    FibeRed: Fiberwise Dimensionality Reduction of Topologically Complex Data with Vector Bundles. (arXiv:2206.06513v2 [cs.CG] UPDATED)

    [http://arxiv.org/abs/2206.06513](http://arxiv.org/abs/2206.06513)

    FibeRed提出了一种通过向量丛描述拓扑复杂数据的纤维降维方法，利用该方法可以降低数据维度，同时保留其大规模拓扑特征。算法包含从局部线性降维得到的局部表示与初始全局表示相结合的过程，并在动力学系统和化学领域的数据集上表现出良好的效果。

    

    具有非平凡大规模拓扑结构的数据集可能很难用现有的降维算法嵌入到低维欧几里得空间中。我们提出使用向量丛来模拟具有拓扑复杂性的数据集，这样基空间可以考虑到大规模拓扑，而纤维可以考虑到局部几何。这允许我们降低纤维的维度，同时保留大规模拓扑结构。我们形式化了这个观点，并作为一个应用，我们提出了一个算法，该算法将数据集及其在欧几里得空间中的初始表示作为输入，假设它能恢复部分大规模拓扑结构，并输出一个新表示，该表示将局部线性降维得到的局部表示与初始全局表示相结合。我们在动力学系统和化学领域的例子中证明了这个算法。在这些例子中，我们的算法能够学习拓扑结构和几何结构之间的联系。

    Datasets with non-trivial large scale topology can be hard to embed in low-dimensional Euclidean space with existing dimensionality reduction algorithms. We propose to model topologically complex datasets using vector bundles, in such a way that the base space accounts for the large scale topology, while the fibers account for the local geometry. This allows one to reduce the dimensionality of the fibers, while preserving the large scale topology. We formalize this point of view, and, as an application, we describe an algorithm which takes as input a dataset together with an initial representation of it in Euclidean space, assumed to recover part of its large scale topology, and outputs a new representation that integrates local representations, obtained through local linear dimensionality reduction, along the initial global representation. We demonstrate this algorithm on examples coming from dynamical systems and chemistry. In these examples, our algorithm is able to learn topologica
    
[^159]: WaveMix: 一种用于图像分析的资源高效神经网络

    WaveMix: A Resource-efficient Neural Network for Image Analysis. (arXiv:2205.14375v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2205.14375](http://arxiv.org/abs/2205.14375)

    WaveMix是一种资源高效的神经网络结构，可以在多项任务上达到可比或更好的准确率，并且需要更少的参数和GPU RAM，实现时间、成本和能量的节省。

    

    我们提出了WaveMix——一种新颖的神经网络结构，既具有资源效率性，又具有通用性和可扩展性。WaveMix网络在多项任务上达到了可比或更好的准确率，包括Cityscapes中的分割和Places-365、五个EMNIST数据集和iNAT-mini中的分类，并建立了新的基准。令人惊奇的是，与先前的最先进技术相比，WaveMix结构所需的参数更少。此外，当控制参数数量时，WaveMix所需的GPU RAM更少，这意味着节省时间、成本和能量。为了实现这些收益，我们在WaveMix块中使用了多级二维离散小波变换（2D-DWT），它具有以下优点:(1)它基于三种强图像先验条件重新组织空间信息——尺度不变性，位移不变性和边缘的稀疏性,(2) i

    We propose WaveMix -- a novel neural architecture for computer vision that is resource-efficient yet generalizable and scalable. WaveMix networks achieve comparable or better accuracy than the state-of-the-art convolutional neural networks, vision transformers, and token mixers for several tasks, establishing new benchmarks for segmentation on Cityscapes; and for classification on Places-365, five EMNIST datasets, and iNAT-mini. Remarkably, WaveMix architectures require fewer parameters to achieve these benchmarks compared to the previous state-of-the-art. Moreover, when controlled for the number of parameters, WaveMix requires lesser GPU RAM, which translates to savings in time, cost, and energy. To achieve these gains we used multi-level two-dimensional discrete wavelet transform (2D-DWT) in WaveMix blocks, which has the following advantages: (1) It reorganizes spatial information based on three strong image priors -- scale-invariance, shift-invariance, and sparseness of edges, (2) i
    
[^160]: 一片文字海：针对文本数据的 Anchors 深入分析

    A Sea of Words: An In-Depth Analysis of Anchors for Text Data. (arXiv:2205.13789v2 [stat.ML] CROSS LISTED)

    [http://arxiv.org/abs/2205.13789](http://arxiv.org/abs/2205.13789)

    Anchors 是一种后处理的规则性可解释性方法，它可以通过凸显出一个小组词语（锚点）来强调模型的决策，我们首次对 Anchors 进行了理论分析，考虑到寻找最佳锚点是详尽的，并通过 TF-IDF 向量化步骤以及模型层次的显式结果，探究其在不同类别模型中的行为特征。我们还发现，在神经网络中，最高偏导数所对应的词汇可以重新加权用作 Anchors 词汇。

    

    Anchors 是一种基于后处理的基于规则的可解释性方法，该方法旨在解释模型决策并强调一小组词语（锚点），这些词语存在于文档中时，模型输出类似。本文首次对 Anchors 进行了理论分析，考虑到寻找最佳锚点是详尽的。我们将文本分类的算法形式化后，结合不同类别模型的显式结果，探究了 Anchors 的行为特征。我们分别覆盖了基本 if-then 规则和线性分类器这两种模型。我们还利用这项分析，洞见任何可微分分类器的 Anchors 行为特征。对于神经网络，我们经验性地展示了模型对输入的最高偏导数所对应的词语，通过反向文件重新加权，可以作为 Anchors 词语。

    Anchors (Ribeiro et al., 2018) is a post-hoc, rule-based interpretability method. For text data, it proposes to explain a decision by highlighting a small set of words (an anchor) such that the model to explain has similar outputs when they are present in a document. In this paper, we present the first theoretical analysis of Anchors, considering that the search for the best anchor is exhaustive. After formalizing the algorithm for text classification, we present explicit results on different classes of models when the vectorization step is TF-IDF, and words are replaced by a fixed out-of-dictionary token when removed. Our inquiry covers models such as elementary if-then rules and linear classifiers. We then leverage this analysis to gain insights on the behavior of Anchors for any differentiable classifiers. For neural networks, we empirically show that the words corresponding to the highest partial derivatives of the model with respect to the input, reweighted by the inverse document
    
[^161]: 受限单调神经网络

    Constrained Monotonic Neural Networks. (arXiv:2205.11775v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.11775](http://arxiv.org/abs/2205.11775)

    本文针对实际应用场景需要的单调性，提出了一种通过在层中的一部分神经元中采用原始激活函数，同时在另一部分采用其点对称反射来解决构建单调深度神经网络的方法。实验证明，该方法的精度符合要求。

    

    深度神经网络越来越流行，可以逼近从嘈杂数据中得出的任意函数，但在推广过程中需要解释这些模型并对它们施加额外的限制，其中单调性是最受实际应用场景需要的属性之一，并且是该论文的重点。最早构建单调全连接神经网络的方法是将其权重约束为非负，同时采用单调激活函数。不幸的是，该方法无法与常用的非饱和激活函数（如ReLU，ELU，SELU等）一起使用，因为它只能逼近凸函数。我们通过在层中的一部分神经元中采用原始激活函数，同时在另一部分采用其点对称反射来解决这个问题。我们的实验证明，采用这种方法建立单调深度神经网络的精度与其他方法相当甚至更好，同时满足单调性约束。

    Deep neural networks are becoming increasingly popular in approximating arbitrary functions from noisy data. But wider adoption is being hindered by the need to explain such models and to impose additional constraints on them. Monotonicity constraint is one of the most requested properties in real-world scenarios and is the focus of this paper. One of the oldest ways to construct a monotonic fully connected neural network is to constrain its weights to be non-negative while employing a monotonic activation function. Unfortunately, this construction does not work with popular non-saturated activation functions such as ReLU, ELU, SELU etc, as it can only approximate convex functions. We show this shortcoming can be fixed by employing the original activation function for a part of the neurons in the layer, and employing its point reflection for the other part. Our experiments show this approach of building monotonic deep neural networks have matching or better accuracy when compared to ot
    
[^162]: 噪声低秩矩阵优化: 局部极小值的几何形状和收敛速度

    Noisy Low-rank Matrix Optimization: Geometry of Local Minima and Convergence Rate. (arXiv:2203.03899v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2203.03899](http://arxiv.org/abs/2203.03899)

    本文提出了一个新的数学框架来处理噪声低秩矩阵优化问题，对受限等距常数的限制要少得多，并且只要无噪声目标的受限等距条件小于1/3，任何错误的局部优化解必须接近于真实解，可以在多项式时间内找到近似解。

    

    本文关注于低秩矩阵优化问题，该问题在机器学习中应用广泛。在矩阵感知特例中，该问题已经通过受限等距性质的概念进行了广泛研究，导致了大量关于问题的几何景观和常见算法的收敛速度的结果。然而，现有结果仅在受限等距常数接近于0的情况下能够处理仅具有噪声数据的一般目标函数问题。在本文中，我们开发了一个新的数学框架来解决以上问题，该框架对受限等距常数的限制要少得多。我们证明，只要无噪声目标的受限等距条件小于1/3，任何错误的局部优化解必须接近于真实解。通过严格的鞍点特性，我们还证明在多项式时间内可以找到近似解。

    This paper is concerned with low-rank matrix optimization, which has found a wide range of applications in machine learning. This problem in the special case of matrix sensing has been studied extensively through the notion of Restricted Isometry Property (RIP), leading to a wealth of results on the geometric landscape of the problem and the convergence rate of common algorithms. However, the existing results can handle the problem in the case with a general objective function subject to noisy data only when the RIP constant is close to 0. In this paper, we develop a new mathematical framework to solve the above-mentioned problem with a far less restrictive RIP constant. We prove that as long as the RIP constant of the noiseless objective is less than $1/3$, any spurious local solution of the noisy optimization problem must be close to the ground truth solution. By working through the strict saddle property, we also show that an approximate solution can be found in polynomial time. We 
    
[^163]: 自监督视觉预训练中的损坏图像建模

    Corrupted Image Modeling for Self-Supervised Visual Pre-Training. (arXiv:2202.03382v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2202.03382](http://arxiv.org/abs/2202.03382)

    本文提出了一种损坏图像建模的自监督视觉预训练方法，通过协同训练生成器和增强网络来学习丰富的视觉表示，适用于多种网络架构，并在多个数据集上获得了优异的性能表现。

    

    本文介绍了一种自监督的视觉预训练方法——损坏图像建模，使用一个辅助生成器和一个小型的BEiT（Vision Transformer模型），将输入的图像进行破坏，而不是使用人工的[MASK]令牌，生成器在输出分布中采样恰当的备选项用于替换随机选择的一些图像片段。在此基础上，一个增强网络可以学习恢复原始图像或预测每个视觉令牌是否被生成器采样替换。生成器和增强网络同时进行训练，协同更新。预训练后，增强网络可用作下游任务的高容量视觉编码器。该方法适用于多种网络架构，首次证明了ViT和CNN可以使用统一的非孪生框架学习丰富的视觉表示。实验结果表明，本方法在ImageNet、COIL-100和PASCAL VOC 2007数据集上均取得了优异的性能。

    We introduce Corrupted Image Modeling (CIM) for self-supervised visual pre-training. CIM uses an auxiliary generator with a small trainable BEiT to corrupt the input image instead of using artificial [MASK] tokens, where some patches are randomly selected and replaced with plausible alternatives sampled from the BEiT output distribution. Given this corrupted image, an enhancer network learns to either recover all the original image pixels, or predict whether each visual token is replaced by a generator sample or not. The generator and the enhancer are simultaneously trained and synergistically updated. After pre-training, the enhancer can be used as a high-capacity visual encoder for downstream tasks. CIM is a general and flexible visual pre-training framework that is suitable for various network architectures. For the first time, CIM demonstrates that both ViT and CNN can learn rich visual representations using a unified, non-Siamese framework. Experimental results show that our appro
    
[^164]: Laplacian2Mesh：基于拉普拉斯的网格理解

    Laplacian2Mesh: Laplacian-Based Mesh Understanding. (arXiv:2202.00307v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2202.00307](http://arxiv.org/abs/2202.00307)

    Laplacian2Mesh是一种克服不规则三角形网格结构困难的方法，使用卷积神经网络来直接处理形状分析任务，并具有可扩展的感受野和个别特征学习的机制。

    

    几何深度学习引起了计算机图形学在形状理解任务上的兴趣，如形状分类和语义分割。当输入为多边形表面时，人们必须应对不规则网格结构。受到几何谱理论的启发，我们介绍Laplacian2Mesh，一种新颖且灵活的卷积神经网络（CNN）框架，用于应对不规则的三角形网格（顶点可能具有任何价值）。通过将输入网格表面映射到多维Laplacian-Beltrami空间，Laplacian2Mesh使人们能够直接使用成熟的CNN执行形状分析任务，而无需处理网格结构的不规则连接。我们进一步定义了一个网格汇集操作，以使网络的感受野可以扩展，同时保留原始顶点集以及它们之间的连接。此外，我们引入了一种通道级自我注意块来学习个别特征的相关性，从而有效地提高网络结果。

    Geometric deep learning has sparked a rising interest in computer graphics to perform shape understanding tasks, such as shape classification and semantic segmentation. When the input is a polygonal surface, one has to suffer from the irregular mesh structure. Motivated by the geometric spectral theory, we introduce Laplacian2Mesh, a novel and flexible convolutional neural network (CNN) framework for coping with irregular triangle meshes (vertices may have any valence). By mapping the input mesh surface to the multi-dimensional Laplacian-Beltrami space, Laplacian2Mesh enables one to perform shape analysis tasks directly using the mature CNNs, without the need to deal with the irregular connectivity of the mesh structure. We further define a mesh pooling operation such that the receptive field of the network can be expanded while retaining the original vertex set as well as the connections between them. Besides, we introduce a channel-wise self-attention block to learn the individual im
    
[^165]: 快速分布式k-Means算法，通信轮数较少

    Fast Distributed k-Means with a Small Number of Rounds. (arXiv:2201.13217v2 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2201.13217](http://arxiv.org/abs/2201.13217)

    该文提出了一种适用于分布式环境下聚类的快速算法，保证通信轮数较少且成本更低。

    

    我们提出了一种新的k-Means算法，用于分布式环境下的聚类，并保证误差近似因子和通信轮数仅取决于协调者的计算能力。此外，该算法包括内置的停止机制，可以尽可能地减少通信轮数。理论和实验证明，在许多实际情况下，只需要1-4轮即可完成。与k-means||算法相比，我们的方法允许利用更大的协调能力以获得更少的轮数。我们的实验表明，所提出的算法所得到的k-Means成本通常比k-means||算法更好，即使后者允许更多的通信轮数。此外，我们的算法在运行时间方面要比k-means ||短得多。

    We propose a new algorithm for k-means clustering in a distributed setting, where the data is distributed across many machines, and a coordinator communicates with these machines to calculate the output clustering. Our algorithm guarantees a cost approximation factor and a number of communication rounds that depend only on the computational capacity of the coordinator. Moreover, the algorithm includes a built-in stopping mechanism, which allows it to use fewer communication rounds whenever possible. We show both theoretically and empirically that in many natural cases, indeed 1-4 rounds suffice. In comparison with the popular k-means|| algorithm, our approach allows exploiting a larger coordinator capacity to obtain a smaller number of rounds. Our experiments show that the k-means cost obtained by the proposed algorithm is usually better than the cost obtained by k-means||, even when the latter is allowed a larger number of rounds. Moreover, the machine running time in our approach is 
    
[^166]: 测量非概率不确定性：已知和未知未知的认知、逻辑和计算评估

    Measuring Non-Probabilistic Uncertainty: A cognitive, logical and computational assessment of known and unknown unknowns. (arXiv:2201.05818v5 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2201.05818](http://arxiv.org/abs/2201.05818)

    该论文提出了一种测量非概率不确定性的方法，该方法可以通过分析文本来检测不确定性对决策因果关系的影响。

    

    不确定性无法用概率论充分描述的原因有两个。第一个原因是由于独特或几乎独特的事件，这些事件从未发生或发生得太少，以至于无法可靠地测量频率。第二个原因是当人们担心可能发生某些事情，而自己甚至无法想象，例如： "气候变化、金融危机、大流行、战争，下一个是什么？" 在这两种情况下，简单的一对一认知地图将最终崩溃。然而，这种破坏从具体、可识别和差异化的方式影响到企业高管、员工和其他利益相关者的不同讲述。特别是，可以通过分析诸如咨询报告或向股东的信函等文本，以检测这两种不确定性对通常指导决策的因果关系的影响。

    There are two reasons why uncertainty may not be adequately described by Probability Theory. The first one is due to unique or nearly-unique events, that either never realized or occurred too seldom for frequencies to be reliably measured. The second one arises when one fears that something may happen, that one is not even able to figure out, e.g., if one asks: "Climate change, financial crises, pandemic, war, what next?"  In both cases, simple one-to-one cognitive maps between available alternatives and possible consequences eventually melt down. However, such destructions reflect into the changing narratives of business executives, employees and other stakeholders in specific, identifiable and differential ways. In particular, texts such as consultants' reports or letters to shareholders can be analysed in order to detect the impact of both sorts of uncertainty onto the causal relations that normally guide decision-making.  We propose structural measures of cognitive maps as a means 
    
[^167]: 使用吸收比例缩放图的InfoMap算法在吸收随机漫步中的应用

    An adaptation of InfoMap to absorbing random walks using absorption-scaled graphs. (arXiv:2112.10953v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2112.10953](http://arxiv.org/abs/2112.10953)

    我们使用吸收比例缩放图和马尔可夫时间扫描改进了InfoMap算法，检测网络上密集连接的节点社区，此方法适应节点具有不同移除率的情况，社区结构与不考虑节点吸收率的方法可能有显著不同，并对易感-感染-恢复（SI）模型产生重要影响。

    

    InfoMap算法是一种用于检测网络上密集连接的“社区”节点的流行方法。本文将其应用与吸收随机漫步过程，并使用吸收比例缩放图和马尔可夫时间扫描来适应节点具有不同移除率的情况。改进后的InfoMap算法检测到的社区结构可能与不考虑节点吸收率的方法不同，并对易感-感染-恢复（SI）模型具有重要影响。

    InfoMap is a popular approach for detecting densely connected "communities" of nodes in networks. To detect such communities, InfoMap uses random walks and ideas from information theory. Motivated by the dynamics of disease spread on networks, whose nodes may have heterogeneous disease-removal rates, we adapt InfoMap to absorbing random walks. To do this, we use absorption-scaled graphs, in which the edge weights are scaled according to absorption rates, along with Markov time sweeping. One of our adaptations of InfoMap converges to the standard version of InfoMap in the limit in which the node-absorption rates approach $0$. The community structure that we obtain using our adaptations of InfoMap can differ markedly from the community structure that one detects using methods that do not take node-absorption rates into account. Additionally, we demonstrate that the community structure that is induced by local dynamics can have important implications for susceptible-infected-recovered (SI
    
[^168]: 用一比特神经网络逼近函数

    Approximation of functions with one-bit neural networks. (arXiv:2112.09181v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.09181](http://arxiv.org/abs/2112.09181)

    研究了一比特神经网络的逼近能力与参数范围，证明了任何满足条件的函数均可被逼近，并给出了实现方法。

    

    神经网络的通用逼近定理大致陈述了任何合理的函数都可以通过参数被适当选择的实数网络来任意逼近。本文研究了一比特神经网络的逼近能力，其非零参数为固定值 $±a$。其中一个主要定理显示，对于任何 $f \in C^s([0,1]^d)$，且 $\|f\|_\infty <1$ 和误差 $\varepsilon$，存在 $f_{NN}$，使得 $\boldsymbol{x}$ 不在 $[0,1]^d$ 边界处时，$|f(\boldsymbol{x})-f_{NN}(\boldsymbol{x})|\leq \varepsilon$，且当 $\varepsilon$ 趋近于零时，$f_{NN}$ 可以是由 $O(\varepsilon^{-2d/s})$ 个参数的 $\{\pm 1\}$ 二次网络实现或是由 $O(\varepsilon^{-2d/s}\log (1/\varepsilon))$ 个参数的 $\{\pm \frac 1 2 \}$ ReLU 网络实现。我们建立了迭代的多元 Bernstein 算子的新逼近结果，以及噪声整形量化的误差估计。

    The celebrated universal approximation theorems for neural networks roughly state that any reasonable function can be arbitrarily well-approximated by a network whose parameters are appropriately chosen real numbers. This paper examines the approximation capabilities of one-bit neural networks -- those whose nonzero parameters are $\pm a$ for some fixed $a\not=0$. One of our main theorems shows that for any $f\in C^s([0,1]^d)$ with $\|f\|_\infty<1$ and error $\varepsilon$, there is a $f_{NN}$ such that $|f(\boldsymbol{x})-f_{NN}(\boldsymbol{x})|\leq \varepsilon$ for all $\boldsymbol{x}$ away from the boundary of $[0,1]^d$, and $f_{NN}$ is either implementable by a $\{\pm 1\}$ quadratic network with $O(\varepsilon^{-2d/s})$ parameters or a $\{\pm \frac 1 2 \}$ ReLU network with $O(\varepsilon^{-2d/s}\log (1/\varepsilon))$ parameters, as $\varepsilon\to0$. We establish new approximation results for iterated multivariate Bernstein operators, error estimates for noise-shaping quantization 
    
[^169]: 核赌博中的协作纯探索问题

    Collaborative Pure Exploration in Kernel Bandit. (arXiv:2110.15771v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.15771](http://arxiv.org/abs/2110.15771)

    本论文提出了CoPE-KB模型，为多智能体多任务决策提供了一个新的模型，提出了两种最优算法CoopKernelFC和CoopKernelFB，成功地量化了任务相似性对学习加速度的影响并应用于核赌博问题的求解。

    

    本论文提出了协作纯探索在核赌博中的问题（CoPE-KB），为多智能体多任务决策提供了一个新的模型，并适用于许多在线学习任务，例如推荐系统和网络调度。我们考虑了CoPE-KB的两种设置，即固定置信度（FC）和固定预算（FB），并设计了两种最优算法CoopKernelFC（用于FC）和CoopKernelFB（用于FB）。我们的算法配备了创新的高效内核估计器，以同时实现计算和通信效率。在统计和通信度量下建立匹配的上下界，以证明我们的算法的最优性。理论界限成功地量化了任务相似性对学习加速度的影响，并且仅取决于内核化特征空间的有效维数。我们提出的分析技术在现有文献中将协作纯探索和非参数贝叶斯方法相结合，并将其应用于核赌博问题的求解。

    In this paper, we formulate a Collaborative Pure Exploration in Kernel Bandit problem (CoPE-KB), which provides a novel model for multi-agent multi-task decision making under limited communication and general reward functions, and is applicable to many online learning tasks, e.g., recommendation systems and network scheduling. We consider two settings of CoPE-KB, i.e., Fixed-Confidence (FC) and Fixed-Budget (FB), and design two optimal algorithms CoopKernelFC (for FC) and CoopKernelFB (for FB). Our algorithms are equipped with innovative and efficient kernelized estimators to simultaneously achieve computation and communication efficiency. Matching upper and lower bounds under both the statistical and communication metrics are established to demonstrate the optimality of our algorithms. The theoretical bounds successfully quantify the influences of task similarities on learning acceleration and only depend on the effective dimension of the kernelized feature space. Our analytical techn
    
[^170]: 基于深度学习的大规模验证项目因素分析的参数估计和拟合度检验方法研究

    Deep Learning-Based Estimation and Goodness-of-Fit for Large-Scale Confirmatory Item Factor Analysis. (arXiv:2109.09500v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2109.09500](http://arxiv.org/abs/2109.09500)

    本文研究了针对大规模验证项目因素分析的参数估计与拟合度检验方法，提出了基于深度学习的算法和扩展测试与指标，具有高效准确和有效性的特点。

    

    本论文研究了针对大规模验证项目因素分析中的参数估计和拟合度检验方法。对于参数估计，我们将Urban和Bauer（2021）的深度学习算法扩展到验证性因素分析领域，并展示了如何处理因子载荷和因子相关性的限制。对于拟合度检验，我们探索基于模拟的测试和指标，扩展了分类器两个样本测试（C2ST），该方法测试深度神经网络能否区分来自拟合的IFA模型的观测数据和合成数据。所提出的扩展包括近似拟合检验，其中用户指定观测数据和合成数据中应有多少占可区分部分的百分比，以及相对拟合指数（RFI），该指数类似于结构方程建模中使用的RFI。通过模拟研究，我们展示了：（1）Urban和Bauer的深度学习算法的验证性扩展即使存在高相关因子也可以准确地估计模型参数；（2）所提出的拟合度指标可以有效地检测模型不良拟合的重要信息，对于大规模验证IFA提供了有效的解决方案。

    We investigate novel parameter estimation and goodness-of-fit (GOF) assessment methods for large-scale confirmatory item factor analysis (IFA) with many respondents, items, and latent factors. For parameter estimation, we extend Urban and Bauer's (2021) deep learning algorithm for exploratory IFA to the confirmatory setting by showing how to handle constraints on loadings and factor correlations. For GOF assessment, we explore simulation-based tests and indices that extend the classifier two-sample test (C2ST), a method that tests whether a deep neural network can distinguish between observed data and synthetic data sampled from a fitted IFA model. Proposed extensions include a test of approximate fit wherein the user specifies what percentage of observed and synthetic data should be distinguishable as well as a relative fit index (RFI) that is similar in spirit to the RFIs used in structural equation modeling. Via simulation studies, we show that: (1) the confirmatory extension of Urb
    
[^171]: 使用分布感知词嵌入的命名实体识别性能的实证研究。

    Empirical Study of Named Entity Recognition Performance Using Distribution-aware Word Embedding. (arXiv:2109.01636v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2109.01636](http://arxiv.org/abs/2109.01636)

    研究开发了一种分布感知词嵌入，并实施了三种不同的方法来利用NER框架中的分布信息，实验表明将词的特异性融入NER方法可提高NER的性能。

    

    随着深度学习技术的快速发展，命名实体识别（NER）在信息提取任务中变得越来越重要。NER任务面临的最大困难是即使在NE类型和文档不熟悉的情况下仍然需要保持可检测性。意识到特定性信息可能包含单词的潜在含义并生成词嵌入的语义相关特征，我们开发了一个分布感知词嵌入，并实施了三种不同的方法来利用NER框架中的分布信息。结果表明，如果将词的特异性融入现有的NER方法中，NER的性能将得到提高。

    With the fast development of Deep Learning techniques, Named Entity Recognition (NER) is becoming more and more important in the information extraction task. The greatest difficulty that the NER task faces is to keep the detectability even when types of NE and documents are unfamiliar. Realizing that the specificity information may contain potential meanings of a word and generate semantic-related features for word embedding, we develop a distribution-aware word embedding and implement three different methods to make use of the distribution information in a NER framework. And the result shows that the performance of NER will be improved if the word specificity is incorporated into existing NER methods.
    
[^172]: 从高温吉布斯态中最优学习量子哈密顿量

    Optimal learning of quantum Hamiltonians from high-temperature Gibbs states. (arXiv:2108.04842v3 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2108.04842](http://arxiv.org/abs/2108.04842)

    本文研究了从已知高温吉布斯态中学习量子哈密顿量的问题，提出了可在多项式时间内实现的算法，并证明了算法的最优性。

    

    本文研究了学习哈密顿量$H$的问题，使得我们能够在已知反温度$\beta$下获得其吉布斯态$\rho=\exp(-\beta H) / \operatorname{Tr}(\exp(-\beta H))$的复制品，并期望精度为$\varepsilon$。我们研究了更一般类的哈密顿量，提出了一种算法，可以学习哈密顿量的系数，并保证在样本复杂度$S = O(\log N/(\beta\varepsilon)^{2})$和时间复杂度$O(S N)$的情况下，精度为$\varepsilon$。此外，还证明了与我们的算法样本复杂度相匹配的下限。

    We study the problem of learning a Hamiltonian $H$ to precision $\varepsilon$, supposing we are given copies of its Gibbs state $\rho=\exp(-\beta H)/\operatorname{Tr}(\exp(-\beta H))$ at a known inverse temperature $\beta$. Anshu, Arunachalam, Kuwahara, and Soleimanifar (Nature Physics, 2021, arXiv:2004.07266) recently studied the sample complexity (number of copies of $\rho$ needed) of this problem for geometrically local $N$-qubit Hamiltonians. In the high-temperature (low $\beta$) regime, their algorithm has sample complexity poly$(N, 1/\beta,1/\varepsilon)$ and can be implemented with polynomial, but suboptimal, time complexity.  In this paper, we study the same question for a more general class of Hamiltonians. We show how to learn the coefficients of a Hamiltonian to error $\varepsilon$ with sample complexity $S = O(\log N/(\beta\varepsilon)^{2})$ and time complexity linear in the sample size, $O(S N)$. Furthermore, we prove a matching lower bound showing that our algorithm's sam
    
[^173]: 通过组合嵌入进行开放集表示学习

    Open-Set Representation Learning through Combinatorial Embedding. (arXiv:2106.15278v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2106.15278](http://arxiv.org/abs/2106.15278)

    该论文提出了一种基于组合学习的方法来识别数据集中的新概念，扩展已知和新类别的识别性能。它使用多个监督元分类器给出的组成知识自然地对未见类别中的示例进行聚类，并通过无监督的成对关系学习让组合嵌入所提供的表示更加强健。

    

    由于剩余类别的标签不可用，视觉识别任务通常限于处理一小部分类别。我们通过基于有标签和无标签示例的表示学习来识别数据集中的新概念，并将识别扩展到已知和新的类别。为了应对这一具有挑战性的任务，我们提出了一种组合学习方法，该方法使用由多个异质标签空间的监督元分类器给出的组成知识自然地对未见类别中的示例进行聚类。通过无监督的成对关系学习，组合嵌入所提供的表示更加强健。所提出的算法通过联合优化来发现新概念，以增强未见类别的区分性，并学习对新类别具有泛化能力的已知类别的表示。我们广泛的实验展示了我们的方法在各种开放集识别基准测试中的卓越表现，表明了在发现新概念和提高已知和新类别的识别性能方面的有效性。

    Visual recognition tasks are often limited to dealing with a small subset of classes simply because the labels for the remaining classes are unavailable. We are interested in identifying novel concepts in a dataset through representation learning based on both labeled and unlabeled examples, and extending the horizon of recognition to both known and novel classes. To address this challenging task, we propose a combinatorial learning approach, which naturally clusters the examples in unseen classes using the compositional knowledge given by multiple supervised meta-classifiers on heterogeneous label spaces. The representations given by the combinatorial embedding are made more robust by unsupervised pairwise relation learning. The proposed algorithm discovers novel concepts via a joint optimization for enhancing the discrimitiveness of unseen classes as well as learning the representations of known classes generalizable to novel ones. Our extensive experiments demonstrate remarkable per
    
[^174]: 物理知识嵌入神经网络的暂态稳定性分析

    Transient Stability Analysis with Physics-Informed Neural Networks. (arXiv:2106.13638v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.13638](http://arxiv.org/abs/2106.13638)

    本文介绍了物理知识嵌入神经网络的暂态稳定性分析，该方法直接将电力系统微分代数方程嵌入神经网络的训练中，并大大减少了对训练数据的需求。

    

    本文探讨使用物理知识嵌入神经网络来大幅加速控制电力系统动态的常微分代数方程求解。在进行暂态稳定性计算方面，传统的方法要么计算负载较大、模型简化、或使用过度保守的代理模型。传统神经网络可以回避这些限制，但需要高质量的训练数据，而忽略了潜在的控制方程。物理知识嵌入神经网络不同：它们将电力系统的微分代数方程直接嵌入神经网络的训练中，并大大减少了对训练数据的需求。本文深入研究了物理知识嵌入神经网络在电力系统暂态稳定性评估方面的性能。引入了一种新的神经网络训练程序，以促进全面的解决方案评估。

    We explore the possibility to use physics-informed neural networks to drastically accelerate the solution of ordinary differential-algebraic equations that govern the power system dynamics. When it comes to transient stability assessment, the traditionally applied methods either carry a significant computational burden, require model simplifications, or use overly conservative surrogate models. Conventional neural networks can circumvent these limitations but are faced with high demand of high-quality training datasets, while they ignore the underlying governing equations. Physics-informed neural networks are different: they incorporate the power system differential algebraic equations directly into the neural network training and drastically reduce the need for training data. This paper takes a deep dive into the performance of physics-informed neural networks for power system transient stability assessment. Introducing a new neural network training procedure to facilitate a thorough 
    
[^175]: 关于 ReLU 神经网络深度下界的探究

    Towards Lower Bounds on the Depth of ReLU Neural Networks. (arXiv:2105.14835v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2105.14835](http://arxiv.org/abs/2105.14835)

    该研究运用数学和优化理论方法，就 ReLU 神经网络的深度下界做了探究，有助于更好地理解这种网络所能表示的函数类的性质。此外，该研究还肯定了一项旧的分段线性函数猜想。

    

    我们运用混合整数优化、多面体理论和热带几何学等技术，为理解具有 ReLU 激活和给定结构的神经网络所能表示的函数类做出了更好的贡献。尽管普适逼近定理认为单层隐藏层就足以学习任何函数，但我们提供了一个数学的对称性，并详细探讨了添加更多层（无大小限制）时是否严格增加了可表示函数的类。作为研究副产品，我们肯定了 Wang 和 Sun（2005）有关分段线性函数的一个旧猜想。我们还给出了表示具有对数深度函数所需的神经网络大小上界。

    We contribute to a better understanding of the class of functions that can be represented by a neural network with ReLU activations and a given architecture. Using techniques from mixed-integer optimization, polyhedral theory, and tropical geometry, we provide a mathematical counterbalance to the universal approximation theorems which suggest that a single hidden layer is sufficient for learning any function. In particular, we investigate whether the class of exactly representable functions strictly increases by adding more layers (with no restrictions on size). As a by-product of our investigations, we settle an old conjecture about piecewise linear functions by Wang and Sun (2005) in the affirmative. We also present upper bounds on the sizes of neural networks required to represent functions with logarithmic depth.
    
[^176]: 噪声低秩矩阵恢复的几何分析在精确参数化和过度参数化区间中的应用

    Geometric Analysis of Noisy Low-rank Matrix Recovery in the Exact Parameterized and the Overparameterized Regimes. (arXiv:2105.08232v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2105.08232](http://arxiv.org/abs/2105.08232)

    本文研究了噪声低秩矩阵恢复问题，提出了鲁棒错误定位几何分析算法和连续子空间优化算法，分别用于精确参数化和过度参数化的情况。通过约束等异性性质，我们提供了对全局最优解与局部解之间的最大距离的保证。

    

    矩阵感知问题是一种重要的低秩优化问题，在矩阵补全、相位同步/恢复、稳健PCA和电力系统状态估计等领域都有广泛应用。本文研究了通过线性测量损坏的噪声低秩矩阵感知问题。我们考虑了搜索秩r等于未知真实秩r*的情况（精确参数化情况），以及r大于r*的情况（过度参数化情况）。我们量化了约束等异性性质（restricted isometry property，RIP）在塑造非凸分解公式的整体景观和帮助局部搜索算法成功方面的作用。首先，我们在RIP常数小于 1/(1+sqrt(r*/r))的假设下，对非凸问题的任意局部极小值和真实值之间的最大距离进行了全局保证。然后，我们提出了一种新颖的方法，称为鲁棒错误定位几何分析（Robust Error-Locating Geometric Analysis，RELGA）算法，用于实现在存在噪声的情况下的精确低秩矩阵恢复。RELGA算法通过组合错误定位机制和几何分析，提供了理论保证，即使在噪声水平相对较大的情况下，也可以实现精确的矩阵恢复。对于过度参数化情况，我们提出了一种局部搜索算法，称为连续子空间优化（Successive Subspace Optimization，SSO）算法，在噪声水平和RIP常数的一定条件下，可以收敛到真实解。我们的分析揭示了SSO的成功取决于初始化、非退化性和几何条件的组合。

    The matrix sensing problem is an important low-rank optimization problem that has found a wide range of applications, such as matrix completion, phase synchornization/retrieval, robust PCA, and power system state estimation. In this work, we focus on the general matrix sensing problem with linear measurements that are corrupted by random noise. We investigate the scenario where the search rank $r$ is equal to the true rank $r^*$ of the unknown ground truth (the exact parametrized case), as well as the scenario where $r$ is greater than $r^*$ (the overparametrized case). We quantify the role of the restricted isometry property (RIP) in shaping the landscape of the non-convex factorized formulation and assisting with the success of local search algorithms. First, we develop a global guarantee on the maximum distance between an arbitrary local minimizer of the non-convex problem and the ground truth under the assumption that the RIP constant is smaller than $1/(1+\sqrt{r^*/r})$. We then p
    
[^177]: 《Bayesian停时问题的逆强化学习的充分必要条件》

    Necessary and Sufficient Conditions for Inverse Reinforcement Learning of Bayesian Stopping Time Problems. (arXiv:2007.03481v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2007.03481](http://arxiv.org/abs/2007.03481)

    本文提出了一个Bayesian停时问题的逆强化学习框架，结合微观经济学中的Bayesian揭示偏好思路，通过观察Bayesian决策者的行动，确定其的最优性。并且通过两个停时问题示例得到了验证，并且已在一个真实的例子中得到了高精度地预测用户参与度。

    

    本文提出了一个Bayesian停时问题的逆强化学习（IRL）框架。通过观察Bayesian决策者的行动，我们提供了一种必要且充分的条件来确定这些行动是否与优化成本函数一致。在Bayesian（部分观察）情况下，逆向学习者能够最好地确定针对观察到的策略的最优性。我们的IRL算法确定最优性，然后构建成本函数的估计值，是一个集合值。为了实现这样一个IRL目标，我们使用了来自微观经济学的Bayesian揭示偏好的新思路。我们通过两个重要的停时问题示例，即，顺序假设检验和Bayesian搜索，说明了所提议的IRL方案。作为一个真实世界的例子，我们使用来自190000个视频的元数据的YouTube数据集说明了所提议的IRL方法如何高精度地预测在线多媒体平台中用户的参与度。最后，对于该算法的未来研究方向做了最后讨论。

    This paper presents an inverse reinforcement learning~(IRL) framework for Bayesian stopping time problems. By observing the actions of a Bayesian decision maker, we provide a necessary and sufficient condition to identify if these actions are consistent with optimizing a cost function. In a Bayesian (partially observed) setting, the inverse learner can at best identify optimality wrt the observed strategies. Our IRL algorithm identifies optimality and then constructs set-valued estimates of the cost function.To achieve this IRL objective, we use novel ideas from Bayesian revealed preferences stemming from microeconomics. We illustrate the proposed IRL scheme using two important examples of stopping time problems, namely, sequential hypothesis testing and Bayesian search. As a real-world example, we illustrate using a YouTube dataset comprising metadata from 190000 videos how the proposed IRL method predicts user engagement in online multimedia platforms with high accuracy. Finally, for
    
[^178]: 基于频谱CUSUM的在线网络结构变化检测

    Spectral CUSUM for Online Network Structure Change Detection. (arXiv:1910.09083v8 [math.ST] UPDATED)

    [http://arxiv.org/abs/1910.09083](http://arxiv.org/abs/1910.09083)

    本文提出了一种在线变化检测算法Spectral-CUSUM，可用于从嘈杂的观测数据中检测网络社区结构的突变，并证明了其渐近最优性。

    

    从嘈杂的观测数据中检测网络社区结构的突变是统计学和机器学习中的一个基本问题。本文提出了一种名为Spectral-CUSUM的在线变化检测算法，通过广义似然比统计量检测未知网络结构的变化。我们表征了Spectral-CUSUM程序的平均运行长度（ARL）和预期检测延迟（EDD），并证明了其渐近最优性。最后，我们使用模拟和真实数据例子演示了Spectral-CUSUM程序的良好性能，并将其与几种基线方法进行了比较，其中使用传感器网络数据进行地震事件检测。

    Detecting abrupt changes in the community structure of a network from noisy observations is a fundamental problem in statistics and machine learning. This paper presents an online change detection algorithm called Spectral-CUSUM to detect unknown network structure changes through a generalized likelihood ratio statistic. We characterize the average run length (ARL) and the expected detection delay (EDD) of the Spectral-CUSUM procedure and prove its asymptotic optimality. Finally, we demonstrate the good performance of the Spectral-CUSUM procedure and compare it with several baseline methods using simulations and real data examples on seismic event detection using sensor network data.
    
[^179]: 用于在线学习非平稳函数的连续高斯过程

    Sequential Gaussian Processes for Online Learning of Nonstationary Functions. (arXiv:1905.10003v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1905.10003](http://arxiv.org/abs/1905.10003)

    本文提出了一种基于顺序蒙特卡罗算法的连续高斯过程模型，以解决高斯过程模型的计算复杂度高，难以在线顺序更新的问题，同时允许拟合具有非平稳性质的函数。方法优于现有最先进方法的性能。

    

    许多机器学习问题可以在估计函数的上下文中得到解决，通常这些函数是时间相关的函数，并且是实时地随着观测的到来而估计的。高斯过程是建模实值非线性函数的一个有吸引力的选择，由于其灵活性和不确定性量化。然而，典型的高斯过程回归模型存在若干不足：1）传统高斯过程推断的复杂度$O(N^{3})$随着观测值的个数N成增长；2）逐步更新高斯过程模型不容易；3）协方差核通常对函数施加平稳性约束，而具有非平稳协方差核的高斯过程通常难以在实践中使用。为了克服这些问题，我们提出了一个顺序蒙特卡罗算法来拟合无限混合高斯过程，以捕捉非平稳行为，同时允许在线、分布推断。我们的方法在实验中优于现有最先进方法的性能。

    Many machine learning problems can be framed in the context of estimating functions, and often these are time-dependent functions that are estimated in real-time as observations arrive. Gaussian processes (GPs) are an attractive choice for modeling real-valued nonlinear functions due to their flexibility and uncertainty quantification. However, the typical GP regression model suffers from several drawbacks: 1) Conventional GP inference scales $O(N^{3})$ with respect to the number of observations; 2) Updating a GP model sequentially is not trivial; and 3) Covariance kernels typically enforce stationarity constraints on the function, while GPs with non-stationary covariance kernels are often intractable to use in practice. To overcome these issues, we propose a sequential Monte Carlo algorithm to fit infinite mixtures of GPs that capture non-stationary behavior while allowing for online, distributed inference. Our approach empirically improves performance over state-of-the-art methods fo
    

