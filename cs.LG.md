# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Risk-Aware Accelerated Wireless Federated Learning with Heterogeneous Clients.](http://arxiv.org/abs/2401.09267) | 本文提出了一种考虑风险的加速无线联邦学习框架，有效应对了无线联邦学习在传输速率和传输错误方面的挑战，通过动态风险感知的全局模型聚合方案，使得客户端能够以最佳顺序参与。 |
| [^2] | [MSHyper: Multi-Scale Hypergraph Transformer for Long-Range Time Series Forecasting.](http://arxiv.org/abs/2401.09261) | MSHyper提出了一个多尺度超图转换器框架，用于精确的长期时间序列预测。通过引入多尺度超图和超边图，并使用三阶段的消息传递机制，实现了更全面的高阶模式相互作用建模，取得了领先于其他方法的最先进性能。 |
| [^3] | [A First-Order Multi-Gradient Algorithm for Multi-Objective Bi-Level Optimization.](http://arxiv.org/abs/2401.09257) | 本文提出了一种用于多目标双层优化的高效一阶多梯度算法FORUM，通过值函数方法将MOBLO问题重新形式化为约束多目标优化问题，并提出了一种新颖的多梯度聚合方法来解决挑战性的约束多目标优化问题。在理论和实证上的结果表明该方法的高效性和有效性。 |
| [^4] | [3D Scene Geometry Estimation from 360$^\circ$ Imagery: A Survey.](http://arxiv.org/abs/2401.09252) | 本文调查了基于全景图像的单图、双图或多图的3D场景几何估计方法。主要包括球面相机模型、全景图像获取技术和表示格式、单眼布局和深度推断方法、立体匹配在球面领域的应用，以及多视角摄像机设置下的立体匹配方法。同时还提供了关于数据集和评估尺度的综合列表。 |
| [^5] | [Bridging the Gap Between General and Down-Closed Convex Sets in Submodular Maximization.](http://arxiv.org/abs/2401.09251) | 本文提出了一种在非凸优化中的新方法，通过将凸体约束分解为下凸凸体和一般凸体，实现了一种平滑插值的效果。 |
| [^6] | [DiffClone: Enhanced Behaviour Cloning in Robotics with Diffusion-Driven Policy Learning.](http://arxiv.org/abs/2401.09243) | 本文介绍了DiffClone，一种通过扩散驱动的策略学习增强行为克隆代理的离线算法。在真实的在线物理机器人上的实验表明，采用MOCO微调的ResNet50的效果最好。 |
| [^7] | [Classification and Reconstruction Processes in Deep Predictive Coding Networks: Antagonists or Allies?.](http://arxiv.org/abs/2401.09237) | 本研究对深度学习架构中的分类和重建过程进行了批判性分析，发现分类驱动的信息与重建驱动的信息在共享层存在相互抑制的挑战。 |
| [^8] | [A Characterization Theorem for Equivariant Networks with Point-wise Activations.](http://arxiv.org/abs/2401.09235) | 本论文提出了一个定理，描述了所有可能的有限维表示、坐标选择和点状激活函数的组合，以获得一个完全等变的层，从而推广和加强了现有的特征定理。具有实际相关性的重要情况作为推论进行了讨论。 |
| [^9] | [A Real-Time Lyrics Alignment System Using Chroma And Phonetic Features For Classical Vocal Performance.](http://arxiv.org/abs/2401.09200) | 该论文提出了一个用于古典声乐表演的实时歌词对齐系统，通过优化音高和音素特征的组合来改进歌词对齐算法，并重新组织了舒伯特冬之旅数据集，为实时歌词对齐模型的评估提供了标准方法。 |
| [^10] | [Space and Time Continuous Physics Simulation From Partial Observations.](http://arxiv.org/abs/2401.09198) | 本研究提出了一种新颖的方法，可以从部分观测中进行连续的空间和时间物理模拟，并解决了基于固定支持网格的传统方法的缺点。这种方法通过在稀疏观测上进行训练，利用两个相互关联的动力系统在稀疏位置和连续域上进行预测和插值求解。 |
| [^11] | [GNN-LoFI: a Novel Graph Neural Network through Localized Feature-based Histogram Intersection.](http://arxiv.org/abs/2401.09193) | GNN-LoFI是一种新的图神经网络架构，通过分析局部邻域节点特征的分布来替代传统的消息传递方法。通过直方图交集核函数将相似性信息传播到其他节点，实现了类似于消息传递的机制，并在图分类和回归基准测试中表现出明显优势。 |
| [^12] | [Preparing Lessons for Progressive Training on Language Models.](http://arxiv.org/abs/2401.09192) | 提出了一种名为Apollo的方法，通过在低层训练期间学习高层功能，为渐进式训练语言模型设计了课程，实现了最先进的加速比率。 |
| [^13] | [An Optimal Transport Approach for Computing Adversarial Training Lower Bounds in Multiclass Classification.](http://arxiv.org/abs/2401.09191) | 本文提出了一个用于计算多类分类中对抗训练下界的最优输运方法，并利用该方法提出了计算最优对抗风险下界和确定最优分类器的算法。通过截断类之间的高阶相互作用，避免了组合运行时间的问题。 |
| [^14] | [Exploring the Role of Convolutional Neural Networks (CNN) in Dental Radiography Segmentation: A Comprehensive Systematic Literature Review.](http://arxiv.org/abs/2401.09190) | 本文综述了卷积神经网络在牙科放射学分割中的作用，强调深度学习技术在自动提取诊断数据方面的潜力，以及其在口腔健康问题的早期发现和治疗中的重要作用。 |
| [^15] | [A Two-Scale Complexity Measure for Deep Learning Models.](http://arxiv.org/abs/2401.09184) | 这篇论文介绍了一种用于统计模型的新容量测量2sED，可以可靠地限制泛化误差，并且与训练误差具有很好的相关性。此外，对于深度学习模型，我们展示了如何通过逐层迭代的方法有效地近似2sED，从而处理大量参数的情况。 |
| [^16] | [Beyond Anti-Forgetting: Multimodal Continual Instruction Tuning with Positive Forward Transfer.](http://arxiv.org/abs/2401.09181) | 本研究提出了一种名为Fwd-Prompt的方法，通过对输入嵌入进行奇异值分解，并在残差空间和预训练子空间中进行梯度投影，以解决多模态连续指导调优中的灾难性遗忘和负面的正向传递问题。 |
| [^17] | [Unsupervised Multiple Domain Translation through Controlled Disentanglement in Variational Autoencoder.](http://arxiv.org/abs/2401.09180) | 该论文提出了一种无监督多领域翻译的方法，通过修改后的变分自编码器实现了受控解缠的两个潜变量，其中一个仅与领域有关，另一个与数据的其他变化因素有关。实验证明该方法在不同的视觉数据集上提高了性能。 |
| [^18] | [ADCNet: a unified framework for predicting the activity of antibody-drug conjugates.](http://arxiv.org/abs/2401.09176) | ADCNet是一个统一框架，通过整合蛋白质和小分子的表示学习模型，能够预测抗体药物复合物的活性。其在手动调整的数据集上表现最佳。 |
| [^19] | [Asynchronous Local-SGD Training for Language Modeling.](http://arxiv.org/abs/2401.09135) | 本文通过异步Local-SGD训练语言模型，并进行了全面的实证研究。研究发现，尽管异步更新更频繁，但其收敛所需的迭代次数多于同步方法。作者还提出了一种利用延迟的Nesterov动量更新进行调整的新方法来解决异步更新的挑战。 |
| [^20] | [Understanding Heterophily for Graph Neural Networks.](http://arxiv.org/abs/2401.09125) | 本文通过提出的异质性随机块模型（HSBM）来提供对不同异质性模式对图神经网络（GNNs）影响的理论理解。研究发现，异质性对分类的影响需要与平均节点度一起评估，并且拓扑噪声对分类有负面影响。 |
| [^21] | [RWKV-TS: Beyond Traditional Recurrent Neural Network for Time Series Tasks.](http://arxiv.org/abs/2401.09093) | RWKV-TS是一个高效的RNN模型，它通过具有低时间复杂度和内存使用的新颖架构、增强的捕捉长期序列信息能力以及高计算效率的特点，超越了传统循环神经网络在时间序列任务中的应用，并在与最先进的Transformer或CNN模型的比较中展现出竞争力。 |
| [^22] | [Code Simulation Challenges for Large Language Models.](http://arxiv.org/abs/2401.09074) | 大型语言模型在模拟计算机代码和算法执行方面遇到挑战，性能随着代码长度的增加而迅速下降。在处理短程序或标准过程时，它们能以低错误率按顺序执行指令，但对于复杂的程序，特别是包含关键路径和冗余指令的程序，模拟效果较差。我们提出了一种逐行模拟代码执行的方法来解决这个问题。 |
| [^23] | [Fixed-Budget Differentially Private Best Arm Identification.](http://arxiv.org/abs/2401.09073) | 本论文研究了差分隐私约束下固定预算条件下的最佳臂识别问题，提出了满足差分隐私约束的策略DP-BAI，并得到了错误概率的上界和最小-最大下界的指数衰减关系。 |
| [^24] | [Rethinking Spectral Graph Neural Networks with Spatially Adaptive Filtering.](http://arxiv.org/abs/2401.09071) | 本文重新思考了谱图神经网络，并揭示了谱滤波和空间聚合之间的联系。该研究发现，谱滤波在隐含地将原始图转换成适应性新图，并明确计算用于空间聚合的新图。适应性新图展现出非局部性，并能够反映节点之间的标签一致性。 |
| [^25] | [DTMM: Deploying TinyML Models on Extremely Weak IoT Devices with Pruning.](http://arxiv.org/abs/2401.09068) | DTMM是一个库，旨在在弱物联网设备上高效部署和执行机器学习模型。之前的解决方案无法同时实现在不损害准确性的情况下深度压缩模型和高效执行的目标，而DTMM通过修剪单元选择解决了这个问题。 |
| [^26] | [Towards Continual Learning Desiderata via HSIC-Bottleneck Orthogonalization and Equiangular Embedding.](http://arxiv.org/abs/2401.09067) | 本文提出了一种方法，通过HSIC-Bottleneck正交化和平均角嵌入，实现了对持续学习中遗忘问题的解决，该方法在不使用先前任务的训练数据且模型大小相对恒定的条件下，通过限制逐层参数覆盖和决策边界畸变来避免遗忘。 |
| [^27] | [Consistent3D: Towards Consistent High-Fidelity Text-to-3D Generation with Deterministic Sampling Prior.](http://arxiv.org/abs/2401.09050) | 本论文提出了一种名为“Consistent3D”的方法，通过探索普通微分方程的确定性采样先验，解决了分数蒸馏采样（SDS）在文本到3D生成中容易出现几何崩溃和质量差纹理的问题。 |
| [^28] | [Data Attribution for Diffusion Models: Timestep-induced Bias in Influence Estimation.](http://arxiv.org/abs/2401.09031) | 本文研究了数据归因方法对扩散模型的影响，发现对于在引发大范数时间步骤上训练的样本，其损失梯度范数高度依赖于时间步骤，导致在影响估计中存在显著的偏差。为了解决这个问题，提出了Diffusion-ReTr方法。 |
| [^29] | [Residual Alignment: Uncovering the Mechanisms of Residual Networks.](http://arxiv.org/abs/2401.09018) | 本研究通过线性化残差模块并测量其奇异值分解，揭示了ResNet架构的成功机制，包括中间表示的等间隔分布、残差雅可比的对齐以及与类别数相关的残差雅可比秩的限制。 |
| [^30] | [Inductive Models for Artificial Intelligence Systems are Insufficient without Good Explanations.](http://arxiv.org/abs/2401.09011) | 本文讨论了机器学习模型的局限性，特别是深度神经网络的透明度和解释能力不足，提出AI系统不仅需要预测能力，还需要提供良好的解释能力和洞察力。 |
| [^31] | [Augmenting Math Word Problems via Iterative Question Composing.](http://arxiv.org/abs/2401.09003) | 本研究通过引入MMIQC数据集和迭代组合问题(IQC)的新颖增强方法，成功提高了大型语言模型的数学推理能力，在竞赛级数学问题上取得了优于先前最佳结果的准确率。 |
| [^32] | [Continuous Time Continuous Space Homeostatic Reinforcement Learning (CTCS-HRRL) : Towards Biological Self-Autonomous Agent.](http://arxiv.org/abs/2401.08999) | 本文提出了连续时间连续空间的稳态强化学习框架CTCS-HRRL，并通过模拟实验证明了该模型的有效性和与代理能力相关的证据。 |
| [^33] | [Attack and Reset for Unlearning: Exploiting Adversarial Noise toward Machine Unlearning through Parameter Re-initialization.](http://arxiv.org/abs/2401.08998) | 该论文提出了一种攻击和重置用于遗忘的方法（ARU），通过利用对抗性噪声生成参数蒙版，从而有效地重置模型中的特定参数，使其无法学习。该方法在面部机器遗忘任务上超越了现有的最先进结果，是在保护数据隐私方面的重要进展。 |
| [^34] | [MicroNAS: Zero-Shot Neural Architecture Search for MCUs.](http://arxiv.org/abs/2401.08996) | MicroNAS是一个针对边缘计算中的微控制器单元（MCUs）设计的硬件感知零样本神经架构搜索框架。与之前的方法相比，MicroNAS在搜索效率和MCU推理速度方面都取得了显著的提升，同时保持了相似的精度水平。 |
| [^35] | [Efficient Adapter Finetuning for Tail Languages in Streaming Multilingual ASR.](http://arxiv.org/abs/2401.08992) | 本研究通过在流式多语言ASR中采用简单而有效的适配器微调方法，提供了对边缘语种的支持。适配器仅占每种语言的模型的0.4%。该方法在级联的Conformer转录器框架下，通过教师伪标签增强了模型性能。 |
| [^36] | [Rigid Protein-Protein Docking via Equivariant Elliptic-Paraboloid Interface Prediction.](http://arxiv.org/abs/2401.08986) | 本文提出了一种名为ElliDock的新颖学习方法，通过预测椭抛体来表示蛋白质-蛋白质对接界面。实验评估表明，ElliDock在所有比较方法中拥有最快的推理时间，并且在当前的对接方法中具有强大的竞争力。 |
| [^37] | [A GAN-based data poisoning framework against anomaly detection in vertical federated learning.](http://arxiv.org/abs/2401.08984) | 这篇论文介绍了一种基于GAN的数据污染框架（P-GAN），用于对抗纵向联合学习中的异常检测。通过使用半监督学习训练一个替代目标模型，并使用GAN生成对抗性扰动来降低模型性能，最后通过深度自编码器开发的异常检测算法提供了强大的防御机制。 |
| [^38] | [FedLoGe: Joint Local and Generic Federated Learning under Long-tailed Data.](http://arxiv.org/abs/2401.08977) | 本文介绍了一种名为FedLoGe的方法，它通过在神经网络崩溃框架中集成表示学习和分类器对齐来提高区域和全局模型的性能，解决了在联邦长尾学习中忽视本地级别性能的问题。 |
| [^39] | [ACT-GAN: Radio map construction based on generative adversarial networks with ACT blocks.](http://arxiv.org/abs/2401.08976) | 本论文提出了一种基于生成对抗网络的无线电图构建方法，通过应用聚合上下文转换块、卷积块注意力模块和转置卷积块于生成器，有效提高了无线电图的重建准确性和局部纹理，并在多个场景中展示了其性能优势。 |
| [^40] | [DOO-RE: A dataset of ambient sensors in a meeting room for activity recognition.](http://arxiv.org/abs/2401.08962) | DOO-RE是一个来自会议室环境传感器的数据集，用于活动识别。该数据集包含了来自不同类型环境传感器的数据流，通过交叉验证注释过程获得了9种活动类型的标签。 |
| [^41] | [Cascading Reinforcement Learning.](http://arxiv.org/abs/2401.08961) | 本文提出了一个广义的级联强化学习框架，考虑了用户状态和状态转换对决策的影响，在级联强化学习中，我们需要选择不仅具有较大吸引概率的项目，还要选择能够导致良好后继状态的项目。 |
| [^42] | [Towards Off-Policy Reinforcement Learning for Ranking Policies with Human Feedback.](http://arxiv.org/abs/2401.08959) | 本文提出了一种新的离策略价值排名（VR）算法，通过统一的期望最大化（EM）框架，在不需要在线交互的情况下最大化用户的长期回报和优化排名指标，以提高样本效率。 |
| [^43] | [AntiPhishStack: LSTM-based Stacked Generalization Model for Optimized Phishing URLs Detection.](http://arxiv.org/abs/2401.08947) | 本文介绍了一种名为AntiPhishStack的LSTM-based堆叠泛化模型，用于优化网络钓鱼URL的检测。该模型通过对URL和字符级TF-IDF特征进行对称学习，提高了对新型网络钓鱼威胁的应对能力，并采用对抗性训练策略增加鲁棒性和对抗刚性网络钓鱼攻击。 |
| [^44] | [CEL: A Continual Learning Model for Disease Outbreak Prediction by Leveraging Domain Adaptation via Elastic Weight Consolidation.](http://arxiv.org/abs/2401.08940) | 本研究提出了一种名为CEL的模型，通过利用领域自适应和弹性权重整合，实现了对疾病爆发预测的持续学习。该模型通过惩罚重要参数的改变来缓解灾难性遗忘现象，并在多个疾病上取得了比其他模型更好的性能。 |
| [^45] | [DeLF: Designing Learning Environments with Foundation Models.](http://arxiv.org/abs/2401.08936) | DeLF是一种利用大型语言模型设计学习环境的方法，可以解决在实践中应用强化学习的困难。通过测试，证明DeLF可以为不同的学习环境获得可执行的代码。 |
| [^46] | [Partial Diacritization: A Context-Contrastive Inference Approach.](http://arxiv.org/abs/2401.08919) | 部分音标化是选择标记部分字符来提高阅读可读性和准确性的新方法。上下文对比的部分音标化（CCPD）集成了现有的阿拉伯音标化系统，并通过衡量部分音标化的新指标来判断需要标记哪些字符。 |
| [^47] | [Characterising Gradients for Unsupervised Accuracy Estimation under Distribution Shift.](http://arxiv.org/abs/2401.08909) | 本文研究了在分布偏移下，利用梯度信息对真实测试准确性进行预测的方法。通过分析分类层梯度范数，我们发现在无法泛化到测试数据集时，调整模型以获得更大的梯度范数是有效的。 |
| [^48] | [Herding LLaMaS: Using LLMs as an OS Module.](http://arxiv.org/abs/2401.08908) | LLaMaS使用大型语言模型（LLMs）从新设备的文本描述中提取有用的特征，并利用这些特征在运行时做出操作系统决策，为新设备提供高性能的操作系统。 |
| [^49] | [PPR: Enhancing Dodging Attacks while Maintaining Impersonation Attacks on Face Recognition Systems.](http://arxiv.org/abs/2401.08903) | 本文提出了一种名为PPR的新型攻击方法，旨在增强躲避攻击的性能同时避免冒名顶替攻击的降级。该方法利用对抗样本修剪，并通过嵌入对抗扰动来增强对抗人脸样本的躲避性能。 |
| [^50] | [Similar but Faster: Manipulation of Tempo in Music Audio Embeddings for Tempo Prediction and Search.](http://arxiv.org/abs/2401.08902) | 通过在音频嵌入中操作节奏，本研究实现了在维持其他属性不变的情况下，检索出在节奏上相似但在其他方面不同的音乐曲目。 |
| [^51] | [Bridging State and History Representations: Understanding Self-Predictive RL.](http://arxiv.org/abs/2401.08898) | 本论文研究了深度强化学习中状态和历史表示间的关系，发现了这些方法和框架实际上都基于自预测抽象的共同思想，并提供了理论洞见和简化算法来学习自预测表示。 |
| [^52] | [CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational AutoEncoder.](http://arxiv.org/abs/2401.08897) | CFASL是一种用于解缠学习的新方法，它将对称性学习与VAE集成，无需任何数据集因子信息的先验知识，具有三个新特征：对齐潜在向量维度到可学习对称代码簿中的对称性，学习复合对称性来表达未知因素的变化，以及引入群等变编码器和解码器来训练VAE。 |
| [^53] | [cedar: Composable and Optimized Machine Learning Input Data Pipelines.](http://arxiv.org/abs/2401.08895) | cedar是一个编程模型和框架，可以轻松构建、优化和执行机器学习输入数据管道。它提供了易于使用的编程接口和可组合运算符，支持任意ML框架和库。通过解决当前输入数据系统无法充分利用性能优化的问题，cedar提高了资源利用效率，满足了庞大数据量和高训练吞吐量的需求。 |
| [^54] | [MADA: Meta-Adaptive Optimizers through hyper-gradient Descent.](http://arxiv.org/abs/2401.08893) | MADA是一个统一的优化器框架，通过超梯度下降动态学习最适合的优化器。数值结果表明MADA在次优调整的超参数下是稳健的，并且在默认超参数下常常优于其他优化器。插值优化器可以改进收敛性能。 |
| [^55] | [Tempo estimation as fully self-supervised binary classification.](http://arxiv.org/abs/2401.08891) | 本文提出了一种完全自监督的方法，用于音乐音频中的全局节奏估计。该方法不依赖任何人工标记的数据，通过将任务重新定义为二分类问题，可以使用公开可用的音频数据源进行训练。 |
| [^56] | [On the Effect of Data-Augmentation on Local Embedding Properties in the Contrastive Learning of Music Audio Representations.](http://arxiv.org/abs/2401.08889) | 本研究通过对比学习音乐数据集上的音频表示，发现通过适当的数据增强策略，可以减少一首曲目中均匀的音乐特性在嵌入空间中的本地化，并提高其他属性的本地化。这对于音乐搜索和推荐中的最近邻算法应用是重要的。 |
| [^57] | [RiemannONets: Interpretable Neural Operators for Riemann Problems.](http://arxiv.org/abs/2401.08886) | 本文使用可解释的神经算子来解决高速流动中的Riemann问题，通过对DeepONet进行简单修改，在极端压力跳跃情况下获得了非常准确的解决方案。 |
| [^58] | [Evaluating the Utility of Conformal Prediction Sets for AI-Advised Image Labeling.](http://arxiv.org/abs/2401.08876) | 本研究评估了符合预测集在AI辅助图像标注中的效用，发现对于简单图像，预测集与Top-1和Top-k显示的准确性相当，但在标记分布外图像时特别有效，尤其是集合大小较小时。 |
| [^59] | [DCRMTA: Unbiased Causal Representation for Multi-touch Attribution.](http://arxiv.org/abs/2401.08875) | DCRMTA提出了一种无偏的多触点归因方法，通过建立转化预测模型和构建对照触点序列来减轻偏差的影响。 |
| [^60] | [MambaTab: A Simple Yet Effective Approach for Handling Tabular Data.](http://arxiv.org/abs/2401.08867) | MambaTab是一种简单而有效的方法，用于处理表格数据，通过利用结构化状态空间模型（SSM）的特性，在少量参数和最小预处理的情况下，实现了超过最先进基线模型的性能。 |
| [^61] | [The Effect of Intrinsic Dataset Properties on Generalization: Unraveling Learning Differences Between Natural and Medical Images.](http://arxiv.org/abs/2401.08865) | 本文研究了神经网络在自然图像和医学图像领域学习时的差异，提出了一个与训练集维度有关的泛化缩放定律，并认为医学图像数据集更高的固有“标签锐度”可能是两个领域之间显著差异的部分原因。 |
| [^62] | [Binaural Angular Separation Network.](http://arxiv.org/abs/2401.08864) | 我们提出了一个双耳角分离网络，能够使用两个麦克风在不同的角度区域分离目标语音源和干扰源，在各种混响环境下保持稳健，并在实时场景中表现出良好的性能。 |
| [^63] | [Robust Localization of Key Fob Using Channel Impulse Response of Ultra Wide Band Sensors for Keyless Entry Systems.](http://arxiv.org/abs/2401.08863) | 本文研究了使用神经网络和超宽带传感器对车辆中的钥匙扣进行定位的问题，并提出了一种新的鲁棒定位方法，该方法在没有对抗训练的情况下取得了比基准方法更好的性能。 |
| [^64] | [Semi-Supervised Learning Approach for Efficient Resource Allocation with Network Slicing in O-RAN.](http://arxiv.org/abs/2401.08861) | 本文提出了一种半监督学习方法，解决了O-RAN中网络切片和资源分配的问题。通过设计两个xAPPs，分别处理功率控制和物理资源块分配，我们的方法能够在用户设备之间实现最大化的加权吞吐量，并优先考虑增强型移动宽带和超可靠低延迟通信这两种服务类型。 |
| [^65] | [Shabari: Delayed Decision-Making for Faster and Efficient Serverless Function.](http://arxiv.org/abs/2401.08859) | Shabari是一个延迟决策的无服务器资源管理框架，通过对函数输入的延迟来减轻无服务器系统中的性能变异性和资源低利用率问题。 |
| [^66] | [Using i-vectors for subject-independent cross-session EEG transfer learning.](http://arxiv.org/abs/2401.08851) | 本文提出了使用基于i向量的神经网络分类器进行跨主体跨会话EEG迁移学习的方法，相对于等效的主体相关模型取得了18%的相对改进。此外，我们还展示了我们的主体无关模型在新主体上的竞争力和额外主体数据的增加对模型性能的改善，表明有效的认知负荷确定不需要主体相关的训练。 |
| [^67] | [REValueD: Regularised Ensemble Value-Decomposition for Factorisable Markov Decision Processes.](http://arxiv.org/abs/2401.08850) | REValueD是一种通过正则化集合值分解的新算法，针对高维离散动作空间的任务提供了优越性能，尤其在人形和狗类任务中表现出色。它遏制了Q-learning算法的高估偏差，并减轻了目标方差问题。 |
| [^68] | [RIDGE: Reproducibility, Integrity, Dependability, Generalizability, and Efficiency Assessment of Medical Image Segmentation Models.](http://arxiv.org/abs/2401.08847) | RIDGE是一个用于评估医学图像分割模型的可重复性、完整性、可靠性、泛化性和效率的框架，旨在通过提高工作质量和透明度，确保分割模型在科学可靠性和临床相关性上都具备优势。 |
| [^69] | [Stochastic Subnetwork Annealing: A Regularization Technique for Fine Tuning Pruned Subnetworks.](http://arxiv.org/abs/2401.08830) | 随机子网络退火是一种用于微调修剪子网络的正则化技术，通过在每次前向传播时使用随机掩码表示参数的包含或排除概率，以避免修剪过多参数导致的准确度下降和子网络过拟合的问题。 |
| [^70] | [AiGen-FoodReview: A Multimodal Dataset of Machine-Generated Restaurant Reviews and Images on Social Media.](http://arxiv.org/abs/2401.08825) | 本论文创建了一个多模态的机器生成的社交媒体餐厅评论和图像数据集AiGen-FoodReview，并开发了相应的检测模型，可用于鉴别真实和虚假的评论。通过使用可读性和摄影理论的属性评分，证明了这些属性作为手工特征在可伸缩和可解释的检测模型中的有效性。 |
| [^71] | [Surface-Enhanced Raman Spectroscopy and Transfer Learning Toward Accurate Reconstruction of the Surgical Zone.](http://arxiv.org/abs/2401.08821) | 本研究提出了一种利用表面增强拉曼光谱和迁移学习的机器人拉曼系统，可以可靠地重建手术区域中肿瘤的位置和边界，并取得了较高的验证分类准确性。 |
| [^72] | [Learning from Sparse Offline Datasets via Conservative Density Estimation.](http://arxiv.org/abs/2401.08819) | 本文提出了一种名为保守密度估计（CDE）的训练算法，通过明确约束状态-行为占据稳态分布来解决离线强化学习中的外推错误问题。在稀疏奖励或不足数据的任务中，CDE显示出明显优于基准方法的性能。 |
| [^73] | [Link Me Baby One More Time: Social Music Discovery on Spotify.](http://arxiv.org/abs/2401.08818) | 本研究探讨了在社交音乐推荐中影响音乐互动的社交和环境因素。研究发现，接收者与发送者音乐品味相似、分享的音轨适合接收者的品味、接收者与发送者具有更强和更亲密的联系以及分享的艺术家在接收者的关系中受欢迎，这些因素都会增加接收者与新艺术家的互动。 |
| [^74] | [Adversarial Supervision Makes Layout-to-Image Diffusion Models Thrive.](http://arxiv.org/abs/2401.08815) | 该论文提出了一种对抗监督的布局到图像扩散模型（ALDM），通过引入分割判别器和多步展开策略，可以提升布局到图像合成任务中的文本编辑能力和生成图像与输入布局之间的对齐准确性。 |
| [^75] | [Sample Relationship from Learning Dynamics Matters for Generalisation.](http://arxiv.org/abs/2401.08808) | 这项研究从近似样本之间的相互作用开始，揭示了标签对样本之间的影响，提出了带标签的伪神经切向核 (lpNTK)，并探讨了lpNTK如何帮助理解学习现象。 |
| [^76] | [The Impact of Differential Feature Under-reporting on Algorithmic Fairness.](http://arxiv.org/abs/2401.08788) | 本文研究了差异特征未报告对算法公平性的影响，并提出了一个可分析的模型进行刻画。 |
| [^77] | [Robust Anomaly Detection for Particle Physics Using Multi-Background Representation Learning.](http://arxiv.org/abs/2401.08777) | 这项工作提出了一种使用多背景表示学习进行鲁棒异常检测的方法，通过利用更多信息来提高检测相关性的估计，并推广了去相关性到多背景设置中。在大型强子对撞机的高维粒子衰变数据集上进行的实验证明了该方法的有效性。 |
| [^78] | [The weird and the wonderful in our Solar System: Searching for serendipity in the Legacy Survey of Space and Time.](http://arxiv.org/abs/2401.08763) | 该论文提出了一种用于太阳系天体数据异常检测的新方法，通过训练深度自编码器，并利用学习到的潜在空间来寻找有趣的天体。该方法在寻找星际物体等有趣类别的例子方面表现出很高的效果，同时也对经典无监督异常检测方法的局限性进行了研究，并评估了使用监督学习方法的可行性。未来的工作可以考虑扩展特征空间以增加发现的异常种类。 |
| [^79] | [MMToM-QA: Multimodal Theory of Mind Question Answering.](http://arxiv.org/abs/2401.08743) | MMToM-QA是一个多模态心智理论问答基准，用于评估机器对于人的心智理论的理解能力。我们提出了一种新的方法BIP-ALM用于实现多模态心智理论能力。 |
| [^80] | [Fixed Point Diffusion Models.](http://arxiv.org/abs/2401.08741) | 固定点扩散模型（FPDM）是一种将固定点求解引入扩散生成模型框架的新方法，通过嵌入固定点求解层和采用新的训练方法，显著减小了模型大小、内存使用量并加快了训练速度，并且提出了两种新的技术来提高采样效率。实验证明，与现有模型相比，FPDM在性能和效率上有明显改进。 |
| [^81] | [SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers.](http://arxiv.org/abs/2401.08740) | SiT是一种基于Diffusion Transformers骨干的生成模型，通过插值框架和各种设计选择的模块化研究，实现了在模型大小上超过DiT，在条件ImageNet基准测试中获得了较低的FID-50K评分。 |
| [^82] | [Machine Learning-Based Analysis of Ebola Virus' Impact on Gene Expression in Nonhuman Primates.](http://arxiv.org/abs/2401.08738) | 本研究引入了一种基于机器学习的方法用于分析感染伊波拉病毒的非人类灵长类动物的基因表达数据，发现IFI6和IFI27等基因作为关键生物标志物能有效分类不同阶段的伊波拉感染。 |
| [^83] | [A Framework for Scalable Ambient Air Pollution Concentration Estimation.](http://arxiv.org/abs/2401.08735) | 提出了一个可扩展的数据驱动的机器学习模型框架，用于填充空间和时间数据缺失，从而提供了2018年英格兰全面的空气污染浓度数据集。 |
| [^84] | [Bag of Tricks to Boost Adversarial Transferability.](http://arxiv.org/abs/2401.08734) | 本文通过对现有对抗性攻击的研究，提出了一系列技巧来增强对抗性转移能力，并在ImageNet数据集上进行了大量实验证实了其高效性。 |
| [^85] | [Bayes Conditional Distribution Estimation for Knowledge Distillation Based on Conditional Mutual Information.](http://arxiv.org/abs/2401.08732) | 本文介绍了一种基于条件互信息的贝叶斯条件分布估计方法，通过最大化教师的对数似然和条件互信息来改进知识蒸馏中的估计。实验证明，使用该方法训练的教师能够更好地捕捉图像聚类中的上下文信息。 |
| [^86] | [MA2GCN: Multi Adjacency relationship Attention Graph Convolutional Networks for Traffic Prediction using Trajectory data.](http://arxiv.org/abs/2401.08727) | 提出了一种新的交通拥堵预测模型，使用车辆轨迹数据以及多邻接关系注意力图卷积网络（MA2GCN）来预测交通拥堵情况，不依赖于传感器数据，提取灵活且准确的交通信息。 |
| [^87] | [HierSFL: Local Differential Privacy-aided Split Federated Learning in Mobile Edge Computing.](http://arxiv.org/abs/2401.08723) | HierSFL提出了一种基于局部差分隐私的分割联合学习算法，通过合并模型并定性指导最佳聚合时间框架，减少计算和通信开销，提高了隐私保护能力。 |
| [^88] | [Investigating Fouling Efficiency in Football Using Expected Booking (xB) Model.](http://arxiv.org/abs/2401.08718) | 本论文介绍了一种新的度量指标，期望罚牌 (xB) 模型，用于估算足球犯规导致黄牌的可能性，并通过实验证明了其在分析球队和球员犯规策略方面的有效性。 |
| [^89] | [Selecting Subsets of Source Data for Transfer Learning with Applications in Metal Additive Manufacturing.](http://arxiv.org/abs/2401.08715) | 该论文提出了一种系统的方法来寻找适当的源数据子集，用于金属增材制造中的迁移学习。该方法基于源数据和目标数据集之间的相似性来选择子集，并通过两个相似度距离度量定义的Pareto frontier进行迭代选择。 |
| [^90] | [Survival Analysis of Young Triple-Negative Breast Cancer Patients.](http://arxiv.org/abs/2401.08712) | 这项研究使用SEER数据集研究了年轻年龄对三阴性乳腺癌患者的生存能力的影响。实验结果显示，年轻年龄在TNBC患者的存活能力中起着显著作用。 |
| [^91] | [Decoupled Prototype Learning for Reliable Test-Time Adaptation.](http://arxiv.org/abs/2401.08703) | 提出了一种名为Decoupled Prototype Learning (DPL)的解耦样本学习方法，通过使用样本原型为中心的损失计算，来解决测试时自适应中噪声伪标签的问题。 |
| [^92] | [Do We Really Even Need Data?.](http://arxiv.org/abs/2401.08702) | 本文探讨了使用预训练算法的预测作为因变量的统计挑战，并着重阐述了三个可能的错误来源。 |
| [^93] | [Computationally Efficient Optimisation of Elbow-Type Draft Tube Using Neural Network Surrogates.](http://arxiv.org/abs/2401.08700) | 本研究提出了一种计算效率高的弯头型泄压管的优化工作流，利用神经网络替代品进行评估和设计。从评估结果中找到了最佳算法，并确定了单目标优化和多目标优化中目标的影响和综合影响对于泄压管设计的影响。 |
| [^94] | [Hierarchical Source-to-Post-Route QoR Prediction in High-Level Synthesis with GNNs.](http://arxiv.org/abs/2401.08696) | 该论文提出了一种在高层次综合中使用GNN进行源-后向路质量预测的分层方法。该方法通过建模流程、图构建方法和分层GNN训练和预测方法，能够有效预测QoR指标，并在设计空间探索中减少了运行时。 |
| [^95] | [Towards Responsible AI in Banking: Addressing Bias for Fair Decision-Making.](http://arxiv.org/abs/2401.08691) | 本论文探讨了在银行业中解决偏见以实现公平决策的问题。通过无缝整合公平、可解释性和人类监督，构建负责任人工智能文化，以遵守规定并符合人权标准。 |
| [^96] | [Contrastive Learning with Negative Sampling Correction.](http://arxiv.org/abs/2401.08690) | 对比学习是一种自监督表示学习方法，通过矫正负样本采样偏置来提高性能，我们提出了一种名为PUCL的新型对比学习方法。 |
| [^97] | [NODI: Out-Of-Distribution Detection with Noise from Diffusion.](http://arxiv.org/abs/2401.08689) | 本研究将扩散过程应用于外分布检测任务中，通过将整个训练集的信息集成到预测的噪声向量中，获得稳定的噪声向量，并将其转化为OOD分数。 |
| [^98] | [A Physics-informed machine learning model for time-dependent wave runup prediction.](http://arxiv.org/abs/2401.08684) | 本研究提出了一种基于物理信息的机器学习模型，通过结合Surfbeat和XBeach模型的计算效率和准确性，使用cGAN将波浪冲上的图像从XBSB模式映射到XBNH模式，实现了快速准确的时变波浪冲上预测。 |
| [^99] | [Zero-Shot RTL Code Generation with Attention Sink Augmented Large Language Models.](http://arxiv.org/abs/2401.08683) | 本文探讨了使用大型语言模型在硬件设计中快速生成RTL代码的可能性，并展示了新的注意力机制如何提供功能、优化和符合行业标准的RTL代码。 |
| [^100] | [Concept Alignment.](http://arxiv.org/abs/2401.08672) | 在AI对齐的讨论中，我们强调了概念对齐的重要性，认为在对齐价值观之前，AI系统和人类必须对其理解世界所使用的概念进行对齐。我们整合了哲学、认知科学和深度学习的思想，并提出了实现共享概念的机会和挑战。 |
| [^101] | [DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference.](http://arxiv.org/abs/2401.08671) | 本文介绍了DeepSpeed-FastGen，该系统采用了动态SplitFuse策略，通过MII和DeepSpeed-Inference实现了高吞吐量和低延迟的LLMs文本生成，相比于最先进的系统，实现了高达2.3倍的有效吞吐量提升，平均降低了2倍的延迟，并降低了高达3.7倍的尾延迟。 |
| [^102] | [Deep Reinforcement Learning for Multi-Truck Vehicle Routing Problems with Multi-Leg Demand Routes.](http://arxiv.org/abs/2401.08669) | 本研究针对多卡车多分段需求路径的车辆路径问题，通过对现有的编码器-解码器注意力模型进行扩展，实现了在工业供应链物流中使用深度强化学习的有效策略。 |
| [^103] | [Data-Driven Physics-Informed Neural Networks: A Digital Twin Perspective.](http://arxiv.org/abs/2401.08667) | 该论文研究了利用物理信息神经网络(PINNs)实现数字孪生(DT)的潜力。提出了适用于无网格框架的自适应采样方法，并验证了PINNs在参数化的Navier-Stokes方程中的可扩展性和多保真度的优势。 |
| [^104] | [An Integrated Imitation and Reinforcement Learning Methodology for Robust Agile Aircraft Control with Limited Pilot Demonstration Data.](http://arxiv.org/abs/2401.08663) | 本文提出了一种使用有限的飞行员示范数据的集成模仿和强化学习方法，用于建立适用于广泛条件的敏捷飞行器的数据驱动机动生成模型，以解决构建模型所需的大量真实数据的时间和成本问题，并能够泛化到其他飞行条件。 |
| [^105] | [Risk-anticipatory autonomous driving strategies considering vehicles' weights, based on hierarchical deep reinforcement learning.](http://arxiv.org/abs/2401.08661) | 本研究基于分层深度强化学习开发了一种风险预测自动驾驶策略，考虑车辆的重量，并将其纳入自动驾驶决策中，以降低潜在风险和事故后果。 |
| [^106] | [SAiD: Speech-driven Blendshape Facial Animation with Diffusion.](http://arxiv.org/abs/2401.08655) | 提出了一种使用扩散模型（SAiD）驱动的语音驱动的三维面部动画方法，通过轻量级的Transformer-based U-Net模型和音频与视觉的交叉模态对齐偏差，实现了较好的唇部同步和更多样化的唇部运动。 |
| [^107] | [Deep Pulse-Coupled Neural Networks.](http://arxiv.org/abs/2401.08649) | 本文提出了一种利用脉冲耦合神经网络（DPCNNs）来提高脉冲神经网络（SNNs）在视觉任务中的表达能力和识别性能的方法。 |
| [^108] | [One-Step Diffusion Distillation via Deep Equilibrium Models.](http://arxiv.org/abs/2401.08639) | 本文通过深度平衡模型实现了一步扩散蒸馏，将生成过程蒸馏到更快的网络中，取得了比现有一步方法更好的性能。 |
| [^109] | [Collaborative Inference via Dynamic Composition of Tiny AI Accelerators on MCUs.](http://arxiv.org/abs/2401.08637) | 该论文介绍了Synergy，一个通过动态组合微型AI加速器来进行协作推断的系统，有效地解决了在设备上AI需求不断增长时tinyML面临的关键挑战。Synergy通过提供虚拟计算空间和运行时编排模块，实现了资源的统一虚拟化视图和跨动态/异构加速器的最佳推断，其吞吐量平均提升了8.0倍。 |
| [^110] | [Synergizing Quality-Diversity with Descriptor-Conditioned Reinforcement Learning.](http://arxiv.org/abs/2401.08632) | 将质量多样性优化与描述符条件加强学习相结合，以克服进化算法的局限性，并在生成既多样又高性能的解决方案集合方面取得成功。 |
| [^111] | [Predicting and Interpreting Energy Barriers of Metallic Glasses with Graph Neural Networks.](http://arxiv.org/abs/2401.08627) | 本研究使用图神经网络建模原子图结构并预测金属玻璃的能垒，通过提出的对称图神经网络模型，实现了在结构正交变换下的不变性，为理解金属玻璃的局部结构与物理性质关系提供了新的方法。 |
| [^112] | [Wake-Sleep Consolidated Learning.](http://arxiv.org/abs/2401.08623) | Wake-Sleep Consolidated Learning（WSCL）是一种借鉴人脑觉醒-睡眠阶段的学习策略，用于改进深度神经网络在连续学习中的视觉分类任务。该方法通过觉醒和睡眠阶段之间的同步学习来实现持续学习，觉醒阶段中模型适应感官输入并利用动态参数冻结机制保持稳定，睡眠阶段根据NREM和REM阶段对模型的突触权重进行巩固和调整，强化重要连接并削弱不重要的连接。 |
| [^113] | [MATE-Pred: Multimodal Attention-based TCR-Epitope interaction Predictor.](http://arxiv.org/abs/2401.08619) | MATE-Pred是一种多模态基于注意力的TCR-Epitope相互作用预测器，通过整合进化特征和预训练的语言模型，准确预测T细胞受体和表位的结合亲和力。 |
| [^114] | [Representation Learning in a Decomposed Encoder Design for Bio-inspired Hebbian Learning.](http://arxiv.org/abs/2401.08603) | 这项研究探索了在生物启发式Hebbian学习中的表示学习，并提出了一个模块化框架，利用不同的不变视觉描述符作为归纳偏见。该框架在图像分类任务上展示了较好的鲁棒性和透明度。 |
| [^115] | [Learning with Chemical versus Electrical Synapses -- Does it Make a Difference?.](http://arxiv.org/abs/2401.08602) | 本文研究了在稀疏和全连接网络中使用化学合成与电合成的影响，并通过自动驾驶模拟器的实验来评估它们在不同条件下的性能。 |
| [^116] | [Nahid: AI-based Algorithm for operating fully-automatic surgery.](http://arxiv.org/abs/2401.08584) | 本文提出了一种基于软件和计算机视觉技术的全自动手术方法，能够自动诊断和治疗孤立性卵巢子宫内膜异位症。 |
| [^117] | [Temporal Embeddings: Scalable Self-Supervised Temporal Representation Learning from Spatiotemporal Data for Multimodal Computer Vision.](http://arxiv.org/abs/2401.08581) | 该论文提出了一种自监督的时间嵌入方法，可以将地理活动的时间模式与土地利用类型相对应。通过将时间序列信号转换到频域并压缩为语义分割所需的图像通道，时间嵌入可以有效地表示时间序列数据，并用于多个地理空间任务。 |
| [^118] | [Exploiting Inter-Layer Expert Affinity for Accelerating Mixture-of-Experts Model Inference.](http://arxiv.org/abs/2401.08383) | 本文提出了一种称为ExFlow的轻量级优化技术，通过利用层间专家亲和性，大大加速了混合专家模型推理的过程。 |
| [^119] | [An Explainable Proxy Model for Multiabel Audio Segmentation.](http://arxiv.org/abs/2401.08268) | 该论文提出了一种可解释的多标签音频分割代理模型，通过使用非负矩阵分解将嵌入映射到频域，实现了对语音活动、音乐、噪音和重叠语音的同时检测，并且具有强大的可解释性特性。 |
| [^120] | [Matrix Completion with Hypergraphs:Sharp Thresholds and Efficient Algorithms.](http://arxiv.org/abs/2401.08197) | 本论文研究了基于子采样矩阵条目、观察到的社交图和超图的矩阵补全问题。我们发现了一个尖锐阈值，可以精确补全评分矩阵。通过量化超图的“质量”函数，我们可以评估超图利用对样本概率的影响。通过开发高效算法，我们在高概率情况下成功地完成了矩阵补全任务。 |
| [^121] | [Carrying over algorithm in transformers.](http://arxiv.org/abs/2401.07993) | 本文研究了在Transformer模型中实现进位算法的方式，发现在两层的仅编码器模型中，第一层负责相同位置的数字相加，第二层根据注意力机制决定是否需要进位，并通过多层感知机来执行进位操作。 |
| [^122] | [Deep Evolutional Instant Interest Network for CTR Prediction in Trigger-Induced Recommendation.](http://arxiv.org/abs/2401.07769) | 这篇论文提出了一种基于深度进化的即时兴趣网络（DEI2N）来解决触发引导推荐（TIR）中的点击率预测问题。该方法考虑了用户行为的时间信息、即时兴趣的动态变化以及触发项和目标项之间的交互。 |
| [^123] | [CLSA-CIM: A Cross-Layer Scheduling Approach for Computing-in-Memory Architectures.](http://arxiv.org/abs/2401.07671) | CLSA-CIM是一种用于计算内存架构的跨层调度算法，能够提高平铺CIM架构的利用率，加速计算。 |
| [^124] | [E3x: $\mathrm{E}(3)$-Equivariant Deep Learning Made Easy.](http://arxiv.org/abs/2401.07595) | E3x是一种简化了$\mathrm{E}(3)$等变深度学习的软件包，通过内置等变性实现更高的数据效率和准确性。 |
| [^125] | [Use of Prior Knowledge to Discover Causal Additive Models with Unobserved Variables and its Application to Time Series Data.](http://arxiv.org/abs/2401.07231) | 本文提出了两种用于具有未观测变量的因果可加模型（CAM-UV）的方法，并扩展了这些方法以应用于时间序列数据。这些方法利用先验知识进行高效因果发现，并具有对因果关系顺序的特殊处理。 |
| [^126] | [Bounds on the price of feedback for mistake-bounded online learning.](http://arxiv.org/abs/2401.05794) | 改进了错误有界在线学习中反馈价格的上下界，还解决了多类学习中标准反馈与赌徒反馈的价格问题。 |
| [^127] | [AUTOACT: Automatic Agent Learning from Scratch via Self-Planning.](http://arxiv.org/abs/2401.05268) | AUTOACT是一个自动代理学习框架，通过自主规划合成轨迹，不依赖于大规模数据和闭源模型，能够实现更好或类似的性能。 |
| [^128] | [Tiny Time Mixers (TTMs): Fast Pretrained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series.](http://arxiv.org/abs/2401.03955) | 本论文介绍了一种名为微小时间混合器 (TTMs) 的预训练模型，该模型针对多变量时间序列的零/少样本预测进行了优化。与大型预训练模型相比，TTMs模型更小、更快，并考虑了跨通道相关性，能够在短时间内进行有效的预测。 |
| [^129] | [DiarizationLM: Speaker Diarization Post-Processing with Large Language Models.](http://arxiv.org/abs/2401.03506) | 本文介绍了DiarizationLM框架，利用大语言模型对说话人分离系统的输出进行后处理。实验证明，使用finetuned的PaLM 2-S模型可以显著减少分离错误率，对多种目标都有优化效果。 |
| [^130] | [The Rise of Diffusion Models in Time-Series Forecasting.](http://arxiv.org/abs/2401.03006) | 本文调查了扩散模型在时间序列预测中的应用，提供了对这些模型的全面背景信息和详细说明，同时也对它们在不同数据集上的有效性和彼此之间的比较进行了分析。其贡献包括对扩散模型在时间序列预测中应用的彻底探索和按时间顺序排序的模型概述。这是一份对人工智能和时间序列分析领域的研究人员来说具有价值的资源。 |
| [^131] | [Approximating Numerical Flux by Fourier Neural Operators for the Hyperbolic Conservation Laws.](http://arxiv.org/abs/2401.01783) | 该研究通过在传统的数值方案中运用神经网络来替换数值通量，解决了神经网络方法缺乏鲁棒性和泛化能力的问题，并展示了该方法在超声古典守恒定律方面具有优势。 |
| [^132] | [AIRI: Predicting Retention Indices and their Uncertainties using Artificial Intelligence.](http://arxiv.org/abs/2401.01506) | 使用深度神经网络预测化合物保留指数，并量化其不确定性，以改善化学鉴定方法和库的质量。 (Predict the retention index of compounds using a deep neural network and quantify the uncertainty to enhance chemical identification methods and library quality.) |
| [^133] | [Multi-Lattice Sampling of Quantum Field Theories via Neural Operator-based Flows.](http://arxiv.org/abs/2401.00828) | 本文提出了一种基于神经算子流的方法，通过近似时间相关算子，实现了在量子场论中从底层自由理论到目标理论的离散-连续归一化流。 |
| [^134] | [Matching of Users and Creators in Two-Sided Markets with Departures.](http://arxiv.org/abs/2401.00313) | 本论文提出了一个双边市场中匹配用户和创作者的模型，并展示了一个以用户为中心的贪心算法可能导致整体参与度下降的问题。 |
| [^135] | [Efficient Reinforcemen Learning with Decoupling Exploration and Utilization.](http://arxiv.org/abs/2312.15965) | 本研究提出了一种新的强化学习框架，通过分离探索和利用策略，并采用乐观与悲观演员的双演员方法，实现了更平衡和高效的优化策略。 |
| [^136] | [Self-Supervised Learning for Few-Shot Bird Sound Classification.](http://arxiv.org/abs/2312.15824) | 本研究展示了自监督学习在鸟鸣分类中的应用，通过无需标注的方式，从音频录音中获得有意义的鸟鸣表示，并展示了这些表示在少样本学习中的泛化能力。此外，使用预训练的音频神经网络选择高鸟活跃窗口进行自监督学习可以显著提升学习表示的质量。 |
| [^137] | [Beyond Empirical Windowing: An Attention-Based Approach for Trust Prediction in Autonomous Vehicles.](http://arxiv.org/abs/2312.10209) | 这项研究提出了一种基于注意力的方法，选择性窗口化注意网络（SWAN），通过窗口提示和掩码注意转换实现具有灵活长度的关注区间的选择，从而解决信任预测中窗口大小的挑选问题。 |
| [^138] | [Language Modeling on a SpiNNaker 2 Neuromorphic Chip.](http://arxiv.org/abs/2312.09084) | 该论文介绍了在SpiNNaker 2神经形态芯片上实现语言建模的首次尝试。通过利用基于事件的架构和大规模异步处理的硬件，该方法有望在减少能耗的同时保持竞争任务性能。 |
| [^139] | [TiMix: Text-aware Image Mixing for Effective Vision-Language Pre-training.](http://arxiv.org/abs/2312.08846) | TiMix是一种将文本感知的图像混合技术用于视觉语言预训练的方法，通过集成混合数据增强技术，并从互信息的角度进行理论分析，提高了数据效率并取得了可比较的性能。 |
| [^140] | [CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models.](http://arxiv.org/abs/2312.04350) | 该论文提出了一个新的NLP任务，评估语言模型在因果推理方面的能力。作者构建了一个大规模的数据集CLadder，并利用oracle因果推理引擎将符号问题转化为自然语言。研究结果表明多个LLMs在该数据集上的表现，并引入并评估了一种定制的链式推理机制。 |
| [^141] | [Caregiver Talk Shapes Toddler Vision: A Computational Study of Dyadic Play.](http://arxiv.org/abs/2312.04118) | 本研究通过计算模型探究了护理者的谈话对幼儿视觉表征的影响。研究发现，即使在语言输入有限的情境下，护理者的谈话仍然能够提升幼儿的视觉表征能力。 |
| [^142] | [A Comparison Between Invariant and Equivariant Classical and Quantum Graph Neural Networks.](http://arxiv.org/abs/2311.18672) | 该论文比较了不变和等变的经典和量子图神经网络，探讨了它们在高能物理数据分析中的应用，以及如何利用量子计算提供快速而高效的计算范式。同时，研究还指出通过使用不变输入和等变层，可以增强深度网络的有效性和鲁棒性。 |
| [^143] | [Dynamic Fault Characteristics Evaluation in Power Grid.](http://arxiv.org/abs/2311.16522) | 该论文提出了一种在电力系统中进行故障检测的新方法，通过图神经网络识别故障节点，并利用前后时间段内节点的状态来辅助当前故障检测。实验证明该方法准确可靠，并提供了对故障节点传播的定性分析。 |
| [^144] | [LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning.](http://arxiv.org/abs/2311.12023) | LQ-LoRA是一种低秩加量化矩阵分解方法，用于内存高效的语言模型微调。它通过将每个预训练矩阵分解为高精度低秩部分和内存高效的量化部分，实现了动态配置量化参数以及对重构目标进行加权的优化，并在微调实验中表现出了优于QLoRA和GPTQ-LoRA的效果。 |
| [^145] | [FIKIT: Priority-Based Real-time GPU Multi-tasking Scheduling with Kernel Identification.](http://arxiv.org/abs/2311.10359) | FIKIT是一种基于优先级的实时GPU多任务调度策略，具有内核识别功能，能够在高优先级任务的内核间填充空闲时间。 |
| [^146] | [Efficient Generalized Low-Rank Tensor Contextual Bandits.](http://arxiv.org/abs/2311.01771) | 本文提出了一种新颖的广义低秩张量情境赌博算法，并引入了G-LowTESTR算法来实现探索和利用之间的权衡。 |
| [^147] | [Patch-Based Deep Unsupervised Image Segmentation using Graph Cuts.](http://arxiv.org/abs/2311.01475) | 本文提出了一种基于补丁的深度无监督图像分割策略，将深度聚类方法和经典的图割方法相结合，通过训练简单的卷积神经网络对图像补丁进行分类，并利用图割算法进行迭代正则化，实现了最先进的分割效果。 |
| [^148] | [Stochastic Thermodynamics of Learning Generative Parametric Probabilistic Models.](http://arxiv.org/abs/2310.19802) | 本文将生成式机器学习问题视为参数概率模型的时间演化过程，通过研究模型参数与生成样本之间的热力学交换，发现模型通过耗散热量来学习，参数子系统充当热库存储学到的信息。这为超参数模型的泛化能力提供了有价值的热力学洞察。 |
| [^149] | [Understanding Addition in Transformers.](http://arxiv.org/abs/2310.13121) | 本文通过对经过训练进行整数加法的单层Transformer模型的深入分析，揭示了该模型将任务分为并行的、特定于数字的流，并针对不同的数字位置采用不同的算法，同时还发现了一种罕见的高损失的使用情况。这些发现对于机制可解释性、人工智能安全性和对齐性等方面的研究具有重要贡献。 |
| [^150] | [Generalizing Medical Image Representations via Quaternion Wavelet Networks.](http://arxiv.org/abs/2310.10224) | 本文提出了一种名为QUAVE的四元数小波网络，可以从医学图像中提取显著特征。该网络可以与现有的医学图像分析或综合任务结合使用，并推广了对单通道数据的采用。通过四元数小波变换和加权处理，QUAVE能够处理具有较大变化的医学数据。 |
| [^151] | [Learning from Label Proportions: Bootstrapping Supervised Learners via Belief Propagation.](http://arxiv.org/abs/2310.08056) | 本文提出了一种从标签比例中学习的算法框架，通过伪标签化和嵌入细化两个步骤迭代地提高有监督学习器性能。 |
| [^152] | [Efficient Adaptation of Large Vision Transformer via Adapter Re-Composing.](http://arxiv.org/abs/2310.06234) | 本研究提出了一种名为Adapter重组（ARC）的策略，旨在从一种新的角度解决高效预训练模型适应的问题。该方法通过考虑适应参数的可重用性和引入参数共享方案，利用对称的投影操作来构建共享的瓶颈操作，并通过学习低维度的重新缩放系数来有效重新组合层适应参数。 |
| [^153] | [Post-hoc Bias Scoring Is Optimal For Fair Classification.](http://arxiv.org/abs/2310.05725) | 本研究提出了一种后验偏差评分的方法，在满足公平性约束的情况下保持高准确性，并给出了基于偏差分数的修改规则。该方法适用于各种类型的公平性约束问题。 |
| [^154] | [Balancing stability and plasticity in continual learning: the readout-decomposition of activation change (RDAC) framework.](http://arxiv.org/abs/2310.04741) | 本文介绍了一个名为RDAC的框架，该框架解剖了持续学习中稳定性和可塑性之间的平衡，并详细分析了几种常用算法在处理任务时的稳定性和可塑性权衡。 |
| [^155] | [Chameleon: Increasing Label-Only Membership Leakage with Adaptive Poisoning.](http://arxiv.org/abs/2310.03838) | Chameleon是一种新的成员推理攻击方法，利用自适应数据污染策略和高效的查询选择方法，可在标签唯一设置中提高成员泄露的准确率。 |
| [^156] | [UniPredict: Large Language Models are Universal Tabular Predictors.](http://arxiv.org/abs/2310.03266) | 本文提出了UniPredict，一个基于大型语言模型的通用表格数据预测器，能够扩展到庞大的表格数据集，并具备理解多样化表格输入和根据输入指令预测目标变量的能力。实验结果表明，UniPredict模型在与其他模型相比时具有显著优势。 |
| [^157] | [Combining Spatial and Temporal Abstraction in Planning for Better Generalization.](http://arxiv.org/abs/2310.00229) | Skipper是一个基于模型的强化学习代理，利用时空抽象来在新情境中推广学到的技能。它自动将任务分解为子任务，实现稀疏决策和对环境相关部分的专注计算。实验结果表明，Skipper在零样本泛化方面具有显著优势。 |
| [^158] | [Implicit Gaussian process representation of vector fields over arbitrary latent manifolds.](http://arxiv.org/abs/2309.16746) | 这项研究通过引入RVGP方法，结合基于图的数据逼近方法对潜在流形上的向量信号进行学习，实现了超分辨率和修复向量场，并且在实验中证明了其具有全局规律性。 |
| [^159] | [Towards Best Practices of Activation Patching in Language Models: Metrics and Methods.](http://arxiv.org/abs/2309.16042) | 本研究系统地考察了激活路径修复中的方法细节对语言模型解释性结果的影响，并提出了最佳实践建议。 |
| [^160] | [Semantic similarity prediction is better than other semantic similarity measures.](http://arxiv.org/abs/2309.12697) | 本文提出了一种使用经过微调的模型直接预测语义相似性的方法，并将其与其他方法进行比较，结果表明所得到的相似性更加符合我们对鲁棒的语义相似性度量的预期。 |
| [^161] | [Regularized Contrastive Pre-training for Few-shot Bioacoustic Sound Detection.](http://arxiv.org/abs/2309.08971) | 本研究通过正则化对比式预训练，在小样本生物声音事件检测中实现了良好的迁移性能，使F-score提高至68.19%（0.75）。 |
| [^162] | [Wasserstein Distributionally Robust Policy Evaluation and Learning for Contextual Bandits.](http://arxiv.org/abs/2309.08748) | 通过使用Wasserstein距离而不是KL散度，我们提出了一种新颖的分布保证优化方法，用于解决上下文乐队中实际环境不匹配和最坏情况下过度拟合的问题。 |
| [^163] | [FedDCSR: Federated Cross-domain Sequential Recommendation via Disentangled Representation Learning.](http://arxiv.org/abs/2309.08420) | 提出了一种名为FedDCSR的联邦跨领域顺序推荐框架，通过解缠表示学习来处理不同领域之间的序列特征异质性，并保护数据隐私。 |
| [^164] | [A DenseNet-based method for decoding auditory spatial attention with EEG.](http://arxiv.org/abs/2309.07690) | 本论文提出了一种基于DenseNet的EEG辅助听觉空间注意力解码方法，该方法充分利用了EEG电极的空间分布，并通过深度卷积神经网络提取了时空特征，实现了较高的解码精度。 |
| [^165] | [Multimodal Transformer for Material Segmentation.](http://arxiv.org/abs/2309.04001) | 本文提出了一种新的多模态分割方法MMSFormer，该方法有效地融合四种不同模态的信息，并在MCubeS数据集上取得了显著的性能提升。 |
| [^166] | [How Safe Am I Given What I See? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy.](http://arxiv.org/abs/2308.12252) | 本文提出了一种基于生成世界模型的学习流水线族，通过克服学习安全知情表示和分布漂移下缺失安全标签的挑战，实现了在线安全预测。这些流水线具有统计校准保证的安全机会预测能力。 |
| [^167] | [A Probabilistic Fluctuation based Membership Inference Attack for Generative Models.](http://arxiv.org/abs/2308.12143) | 本研究针对生成模型提出了一种概率波动评估成员推断攻击方法(PFAMI)，通过检测概率分布的波动性来推断模型中是否存在某条训练记录的成员身份。 |
| [^168] | [KinSPEAK: Improving speech recognition for Kinyarwanda via semi-supervised learning methods.](http://arxiv.org/abs/2308.11863) | 通过自监督预训练、课程进度微调和半监督学习利用无标签语音数据，该论文提出了一种改善基尼亚兰达语音识别的方法，实现了最先进的结果。 |
| [^169] | [Federated Classification in Hyperbolic Spaces via Secure Aggregation of Convex Hulls.](http://arxiv.org/abs/2308.06895) | 本研究是关于在超几何空间中进行联邦分类的首个方法，通过使用分布式凸SVM分类器和凸包的安全聚合来解决在隐私保护环境中处理树状数据的问题。 |
| [^170] | [VertiBench: Advancing Feature Distribution Diversity in Vertical Federated Learning Benchmarks.](http://arxiv.org/abs/2307.02040) | 本文引入了两个影响VFL性能的关键因素：特征重要性和特征相关性，并提出了相关的评估指标和数据集划分方法。同时，通过引入真实的VFL数据集，填补了图像-图像VFL情景中的不足。研究对于未来的VFL研究提供了有价值的见解。 |
| [^171] | [Score-based Source Separation with Applications to Digital Communication Signals.](http://arxiv.org/abs/2306.14411) | 我们提出了一种基于评分的源分离方法，应用于数字通信信号，通过分离叠加源的扩散生成模型和最大后验估计，实现对离散源的恢复和编码比特的准确性改善。在实验中，与传统的和现有的基于学习的方法相比，我们的方法在比特错误率上提供了95％的减小，同时我们的方法可以渐近地接近基础离散分布的模式，并作为评分提取采样方案的多源扩展。 |
| [^172] | [GKD: Generalized Knowledge Distillation for Auto-regressive Sequence Models.](http://arxiv.org/abs/2306.13649) | 本文提出了广义知识蒸馏（GKD），通过从学生中采样输出序列来缓解分布不匹配，并在优化替代KL等离散度方面处理模型欠规范，达到了在摘要任务上最先进的性能。 |
| [^173] | [Last-Iterate Convergent Policy Gradient Primal-Dual Methods for Constrained MDPs.](http://arxiv.org/abs/2306.11700) | 本文研究了约束MDP问题，提出了两种基于策略的原始-对偶算法，实现了策略迭代的非渐进收敛到最优约束策略。 |
| [^174] | [Creating Multi-Level Skill Hierarchies in Reinforcement Learning.](http://arxiv.org/abs/2306.09980) | 本文提出了一种基于代理人与环境交互的图形结构的答案，使用分层图划分产生具有多个抽象层次的技能层次结构。技能能将代理人移动到状态空间中互相连接紧密但相互连接较弱的区域，有效提高了强化学习的效率。 |
| [^175] | [Intensity Profile Projection: A Framework for Continuous-Time Representation Learning for Dynamic Networks.](http://arxiv.org/abs/2306.06155) | 本文提出了一种连续时间网络表示学习框架，涵盖核平滑的强度函数估计、最小化强度重构误差的投影学习和归纳构造节点表示。这种表示保留了网络结构和时间一致性。 |
| [^176] | [Improved Probabilistic Image-Text Representations.](http://arxiv.org/abs/2305.18171) | 本论文提出了一种改进的概率图像-文本表示方法，通过引入新的概率距离和两种优化技术，解决了现有方法中的计算负担过重和损失饱和问题，取得了显著的性能提升。 |
| [^177] | [Causal Component Analysis.](http://arxiv.org/abs/2305.17225) | 本文介绍了一个中间问题：因果成分分析(CauCA)，它是独立成分分析(ICA)和因果表示学习(CRL)的泛化和特例，其目标是学习解混函数和因果机制，预设了因果图的知识。 |
| [^178] | [Demystifying Oversmoothing in Attention-Based Graph Neural Networks.](http://arxiv.org/abs/2305.16102) | 本文通过数学分析证明基于注意力的图神经网络并不能解决平滑过度问题，在实际应用中需要更多关注不对称、状态相关和有向图结构。 |
| [^179] | [A Scalable Neural Network for DSIC Affine Maximizer Auction Design.](http://arxiv.org/abs/2305.12162) | 该论文提出了一种可扩展的神经网络AMenuNet来构造AMAs参数和生成候选分配，解决了现有方法在占优策略激励兼容性和可扩展性方面的限制，其在协商一致的价值和社会残余价值方面优于强基线模型。 |
| [^180] | [Federated Learning Operations Made Simple with Flame.](http://arxiv.org/abs/2305.05118) | Flame 是一个工具，通过引入高级抽象——角色和通道，将联邦学习应用程序描述为拓扑抽象图，解耦 ML 应用程序逻辑与底层部署细节，使得可以专门定制部署，减少开发工作，改进自动化和调整。 |
| [^181] | [Segment Anything Model for Medical Images?.](http://arxiv.org/abs/2304.14660) | “Segment Anything Model”（SAM）是适用于常规图像分割的基础模型，可以实现零样本图像分割，但在医学图像分割方面具有更高的挑战性。作者通过构建一个大型医学分割数据集来验证SAM在该领域的潜力。 |
| [^182] | [ID-MixGCL: Identity Mixup for Graph Contrastive Learning.](http://arxiv.org/abs/2304.10045) | 本文提出了一种基于身份混合的图形对比学习方法，旨在解决通过图形增强得到的不同但相似的图形结构和标签的不匹配问题，以实现更好的表示捕获。 |
| [^183] | [HomPINNs: homotopy physics-informed neural networks for solving the inverse problems of nonlinear differential equations with multiple solutions.](http://arxiv.org/abs/2304.02811) | 本文提出了一种新的框架——基于同伦的物理知识神经网络，来解决具有多个解的非线性微分方程的反问题。该框架使用神经网络逼近已知观测结果并符合DEs的约束条件，通过同伦连续方法解决反问题。实验证明该方法可伸缩且适应性强，为解决具有多个解的DEs提供了有效解决方案。 |
| [^184] | [A Comparative Study of Deep Learning and Iterative Algorithms for Joint Channel Estimation and Signal Detection.](http://arxiv.org/abs/2303.03678) | 本研究对比了深度学习和迭代算法在联合信道估计和信号检测中的效果，提出了一个新的深度学习模型，通过展开迭代算法并使用超网络来估计超参数，同时适应了低信噪比环境。结果表明，该模型能够在不同的信道模型和信噪比范围内实现出色的性能。 |
| [^185] | [Supporting Safety Analysis of Image-processing DNNs through Clustering-based Approaches.](http://arxiv.org/abs/2301.13506) | 本文通过不同的分析管道对DNN失效的根本原因进行了实证评估，结果显示最佳管道结合了迁移学习，DBSCAN和UMAP，并几乎只产生了一个簇群。 |
| [^186] | [Online Loss Function Learning.](http://arxiv.org/abs/2301.13247) | 在线损失函数学习是一种新的元学习范例，旨在自动化为机器学习模型设计损失函数的重要任务。我们提出了一种新的损失函数学习技术，可以在每次更新基本模型参数后自适应地在线更新损失函数。实验结果表明，我们的方法在多个任务上稳定地优于现有技术。 |
| [^187] | [Degeneracy is OK: Logarithmic Regret for Network Revenue Management with Indiscrete Distributions.](http://arxiv.org/abs/2210.07996) | 该论文研究了具有不连续分布的网络收入管理问题，并提出了一种在线算法，其在此模型下实现了对数级别的遗憾。这是在不需要任何“退化”假设的情况下，首次在具有连续值的NRM模型中实现对数级别遗憾的结果。 |
| [^188] | [Exploring Contextual Representation and Multi-Modality for End-to-End Autonomous Driving.](http://arxiv.org/abs/2210.06758) | 本文旨在探索在端到端自动驾驶中应用上下文表达和多模态的方法。通过学习上下文和空间环境表示，可以增强自动车辆在复杂场景中的决策能力和威胁预测能力。我们提出了一个框架，通过整合多个摄像头的信息来实现全面的上下文感知。 |
| [^189] | [Lyapunov Function Consistent Adaptive Network Signal Control with Back Pressure and Reinforcement Learning.](http://arxiv.org/abs/2210.02612) | 这项研究提出了一种利用Lyapunov控制理论的统一框架，将基于流量和基于压力的交通信号控制方法相结合。通过引入背压方法和强化学习算法，实现对复杂交通网络的有效控制。 |
| [^190] | [Model-Informed Generative Adversarial Network (MI-GAN) for Learning Optimal Power Flow.](http://arxiv.org/abs/2206.01864) | 本文提出了一种基于优化模型信息的生成对抗网络 (MI-GAN) 框架，用于解决可再生能源不确定性对优化问题的影响，提高最优功率流问题的计算效率。 |
| [^191] | [A Dempster-Shafer approach to trustworthy AI with application to fetal brain MRI segmentation.](http://arxiv.org/abs/2204.02779) | 该论文提出了一种基于Dempster-Shafer方法的值得信赖的人工智能框架和实用系统，用于医学图像分割。通过检测和纠正深度学习模型的失败，增强了其可信性和安全性。 |

# 详细

[^1]: 考虑风险的异构客户无线联邦学习的加速方法

    Risk-Aware Accelerated Wireless Federated Learning with Heterogeneous Clients. (arXiv:2401.09267v1 [cs.LG])

    [http://arxiv.org/abs/2401.09267](http://arxiv.org/abs/2401.09267)

    本文提出了一种考虑风险的加速无线联邦学习框架，有效应对了无线联邦学习在传输速率和传输错误方面的挑战，通过动态风险感知的全局模型聚合方案，使得客户端能够以最佳顺序参与。

    

    无线联邦学习是一种新兴的分布式机器学习范式，在移动客户端上存在机密和私密数据的领域中得到了广泛应用。然而，考虑到传输速率和传输错误的位置相关性，对于无线联邦学习的收敛速度和准确性提出了重大挑战。尤其是在没有一个能够验证客户端数据质量和安全性的度量指标的敌对环境中，这一挑战更为严峻。针对这一情况，本文提出了一种新颖的风险感知加速无线联邦学习框架，考虑了客户端在拥有数据量、传输速率、传输错误和可信度方面的异质性。根据客户端的位置相关性性能和可信度，我们提出了一种动态风险感知的全局模型聚合方案，允许客户端以传输速率降序和传输错误升序的方式参与。

    Wireless Federated Learning (FL) is an emerging distributed machine learning paradigm, particularly gaining momentum in domains with confidential and private data on mobile clients. However, the location-dependent performance, in terms of transmission rates and susceptibility to transmission errors, poses major challenges for wireless FL's convergence speed and accuracy. The challenge is more acute for hostile environments without a metric that authenticates the data quality and security profile of the clients. In this context, this paper proposes a novel risk-aware accelerated FL framework that accounts for the clients heterogeneity in the amount of possessed data, transmission rates, transmission errors, and trustworthiness. Classifying clients according to their location-dependent performance and trustworthiness profiles, we propose a dynamic risk-aware global model aggregation scheme that allows clients to participate in descending order of their transmission rates and an ascending
    
[^2]: MSHyper：用于长期时间序列预测的多尺度超图转换器

    MSHyper: Multi-Scale Hypergraph Transformer for Long-Range Time Series Forecasting. (arXiv:2401.09261v1 [cs.LG])

    [http://arxiv.org/abs/2401.09261](http://arxiv.org/abs/2401.09261)

    MSHyper提出了一个多尺度超图转换器框架，用于精确的长期时间序列预测。通过引入多尺度超图和超边图，并使用三阶段的消息传递机制，实现了更全面的高阶模式相互作用建模，取得了领先于其他方法的最先进性能。

    

    解析不同尺度时间模式之间的相互作用对于精确的长期时间序列预测至关重要。然而，先前的工作缺乏对高阶相互作用的建模能力。为了促进更全面的模式相互作用建模用于长期时间序列预测，我们提出了一个多尺度超图转换器（MSHyper）框架。具体而言，引入了一个多尺度超图来提供高阶模式相互作用建模的基础。然后，通过将超边视为节点，我们还构建了一个超边图来增强超图建模。此外，引入了一个三阶段的消息传递机制来聚合模式信息并学习不同尺度时间模式之间的相互作用强度。在五个真实世界的数据集上进行了大量实验，结果表明，MSHyper在均方误差(MSE)和平均绝对误差(MAE)方面实现了最先进的性能，平均减小了8.73%和7.15%的预测误差。

    Demystifying interactions between temporal patterns of different scales is fundamental to precise long-range time series forecasting. However, previous works lack the ability to model high-order interactions. To promote more comprehensive pattern interaction modeling for long-range time series forecasting, we propose a Multi-Scale Hypergraph Transformer (MSHyper) framework. Specifically, a multi-scale hypergraph is introduced to provide foundations for modeling high-order pattern interactions. Then by treating hyperedges as nodes, we also build a hyperedge graph to enhance hypergraph modeling. In addition, a tri-stage message passing mechanism is introduced to aggregate pattern information and learn the interaction strength between temporal patterns of different scales. Extensive experiments on five real-world datasets demonstrate that MSHyper achieves state-of-the-art performance, reducing prediction errors by an average of 8.73% and 7.15% over the best baseline in MSE and MAE, respec
    
[^3]: 一种用于多目标双层优化的一阶多梯度算法

    A First-Order Multi-Gradient Algorithm for Multi-Objective Bi-Level Optimization. (arXiv:2401.09257v1 [cs.LG])

    [http://arxiv.org/abs/2401.09257](http://arxiv.org/abs/2401.09257)

    本文提出了一种用于多目标双层优化的高效一阶多梯度算法FORUM，通过值函数方法将MOBLO问题重新形式化为约束多目标优化问题，并提出了一种新颖的多梯度聚合方法来解决挑战性的约束多目标优化问题。在理论和实证上的结果表明该方法的高效性和有效性。

    

    本文研究了多目标双层优化（MOBLO）问题，其中上层子问题是一个多目标优化问题，下层子问题是一个标量优化问题。现有基于梯度的MOBLO算法需要计算Hessian矩阵，从而导致计算效率低下的问题。为解决这个问题，我们提出了一种高效的一阶多梯度算法FORUM用于MOBLO。具体地，我们通过值函数方法将MOBLO问题重新形式化为约束多目标优化（MOO）问题。然后，我们提出了一种新颖的多梯度聚合方法来解决具有挑战性的约束MOO问题。从理论上讲，我们提供了复杂度分析，展示了所提方法的高效性和非渐进收敛结果。从实证上讲，广泛的实验验证了所提FORUM方法在不同学习问题中的有效性和高效性。尤其值得注意的是，在一些应用中，它达到了当前最先进的水平。

    In this paper, we study the Multi-Objective Bi-Level Optimization (MOBLO) problem, where the upper-level subproblem is a multi-objective optimization problem and the lower-level subproblem is for scalar optimization. Existing gradient-based MOBLO algorithms need to compute the Hessian matrix, causing the computational inefficient problem. To address this, we propose an efficient first-order multi-gradient method for MOBLO, called FORUM. Specifically, we reformulate MOBLO problems as a constrained multi-objective optimization (MOO) problem via the value-function approach. Then we propose a novel multi-gradient aggregation method to solve the challenging constrained MOO problem. Theoretically, we provide the complexity analysis to show the efficiency of the proposed method and a non-asymptotic convergence result. Empirically, extensive experiments demonstrate the effectiveness and efficiency of the proposed FORUM method in different learning problems. In particular, it achieves state-of-
    
[^4]: 从360度图像中估计3D场景几何：一项调查

    3D Scene Geometry Estimation from 360$^\circ$ Imagery: A Survey. (arXiv:2401.09252v1 [cs.CV])

    [http://arxiv.org/abs/2401.09252](http://arxiv.org/abs/2401.09252)

    本文调查了基于全景图像的单图、双图或多图的3D场景几何估计方法。主要包括球面相机模型、全景图像获取技术和表示格式、单眼布局和深度推断方法、立体匹配在球面领域的应用，以及多视角摄像机设置下的立体匹配方法。同时还提供了关于数据集和评估尺度的综合列表。

    

    本文对基于单个、两个或多个在全景光学下捕获的图像的先驱和最新的3D场景几何估计方法进行了全面的调查。我们首先回顾了球面相机模型的基本概念，并回顾了适用于全景（也称为360度、球形或全景）图像和视频的最常见的采集技术和表示格式。然后，我们调查了单眼布局和深度推断方法，突出了适用于球形数据的基于学习的解决方案的最新进展。接着，在球面领域对经典的立体匹配进行了修订，其中检测和描述稀疏和稠密特征的方法变得至关重要。然后，立体匹配概念被推广到多视角摄像机设置中，将它们归类为光场、多视角立体匹配和结构运动（或视觉同时定位和建图）。我们还编制了一个关于数据集和评估尺度的综合列表。

    This paper provides a comprehensive survey on pioneer and state-of-the-art 3D scene geometry estimation methodologies based on single, two, or multiple images captured under the omnidirectional optics. We first revisit the basic concepts of the spherical camera model, and review the most common acquisition technologies and representation formats suitable for omnidirectional (also called 360$^\circ$, spherical or panoramic) images and videos. We then survey monocular layout and depth inference approaches, highlighting the recent advances in learning-based solutions suited for spherical data. The classical stereo matching is then revised on the spherical domain, where methodologies for detecting and describing sparse and dense features become crucial. The stereo matching concepts are then extrapolated for multiple view camera setups, categorizing them among light fields, multi-view stereo, and structure from motion (or visual simultaneous localization and mapping). We also compile and di
    
[^5]: 在子模泛化最大化中，弥合一般与下凸集之间的差距

    Bridging the Gap Between General and Down-Closed Convex Sets in Submodular Maximization. (arXiv:2401.09251v1 [cs.LG])

    [http://arxiv.org/abs/2401.09251](http://arxiv.org/abs/2401.09251)

    本文提出了一种在非凸优化中的新方法，通过将凸体约束分解为下凸凸体和一般凸体，实现了一种平滑插值的效果。

    

    在非凸优化领域中，DR-子模函数的优化在近期变得越来越重要。一些最近的研究针对非单调DR-子模函数在一般（不一定是下凸）约束集上进行最大化。然而，Mualem和Feldman的最新研究结果表明，使用任意可行解的最小l∞范数作为参数的方法无法在下凸和非下凸约束之间实现平滑插值。本文提出了一种新的离线和在线算法，基于将凸体约束分解为两个不同的凸体：一个下凸凸体和一个一般凸体，可证明地提供这种插值。我们还通过实验证明了这种插值的优越性。

    Optimization of DR-submodular functions has experienced a notable surge in significance in recent times, marking a pivotal development within the domain of non-convex optimization. Motivated by real-world scenarios, some recent works have delved into the maximization of non-monotone DR-submodular functions over general (not necessarily down-closed) convex set constraints. Up to this point, these works have all used the minimum $\ell_\infty$ norm of any feasible solution as a parameter. Unfortunately, a recent hardness result due to Mualem \& Feldman~\cite{mualem2023resolving} shows that this approach cannot yield a smooth interpolation between down-closed and non-down-closed constraints. In this work, we suggest novel offline and online algorithms that provably provide such an interpolation based on a natural decomposition of the convex body constraint into two distinct convex bodies: a down-closed convex body and a general convex body. We also empirically demonstrate the superiority o
    
[^6]: DiffClone: 使用扩散驱动的策略学习增强机器人行为克隆

    DiffClone: Enhanced Behaviour Cloning in Robotics with Diffusion-Driven Policy Learning. (arXiv:2401.09243v1 [cs.RO])

    [http://arxiv.org/abs/2401.09243](http://arxiv.org/abs/2401.09243)

    本文介绍了DiffClone，一种通过扩散驱动的策略学习增强行为克隆代理的离线算法。在真实的在线物理机器人上的实验表明，采用MOCO微调的ResNet50的效果最好。

    

    机器人学习任务在计算上非常密集且硬件特定。因此，通过使用多样化的离线演示数据集来训练机器人操作代理，来应对这些挑战的方式非常吸引人。Train-Offline-Test-Online（TOTO）基准提供了一个经过精心策划的开源离线训练数据集，主要由专家数据组成，并提供了常见离线强化学习和行为克隆代理的基准分数。在本文中，我们介绍了DiffClone，一种增强行为克隆代理的离线算法，采用基于扩散的策略学习，并在测试时在真实的在线物理机器人上评估了我们的方法的有效性。同时，这也是我们在NeurIPS 2023举办的Train-Offline-Test-Online（TOTO）基准挑战赛中的官方提交。我们尝试了预训练的视觉表示和代理策略。在实验中，我们发现MOCO微调的ResNet50相比其他微调方法表现最好。

    Robot learning tasks are extremely compute-intensive and hardware-specific. Thus the avenues of tackling these challenges, using a diverse dataset of offline demonstrations that can be used to train robot manipulation agents, is very appealing. The Train-Offline-Test-Online (TOTO) Benchmark provides a well-curated open-source dataset for offline training comprised mostly of expert data and also benchmark scores of the common offline-RL and behaviour cloning agents. In this paper, we introduce DiffClone, an offline algorithm of enhanced behaviour cloning agent with diffusion-based policy learning, and measured the efficacy of our method on real online physical robots at test time. This is also our official submission to the Train-Offline-Test-Online (TOTO) Benchmark Challenge organized at NeurIPS 2023. We experimented with both pre-trained visual representation and agent policies. In our experiments, we find that MOCO finetuned ResNet50 performs the best in comparison to other finetuned
    
[^7]: 深度预测编码网络中的分类和重建过程：对手还是盟友？

    Classification and Reconstruction Processes in Deep Predictive Coding Networks: Antagonists or Allies?. (arXiv:2401.09237v1 [cs.LG])

    [http://arxiv.org/abs/2401.09237](http://arxiv.org/abs/2401.09237)

    本研究对深度学习架构中的分类和重建过程进行了批判性分析，发现分类驱动的信息与重建驱动的信息在共享层存在相互抑制的挑战。

    

    预测编码启发的视觉计算深度网络将分类和重建过程整合在共享的中间层中。尽管通常认为这些过程之间存在协同效应，但尚未有令人信服的证据。在本研究中，我们对深度学习架构中的分类和重建交互进行了批判性分析。我们采用了一系列精心设计的模型架构，类似于自编码器，每个模型都配备了编码器、解码器和分类头部，具有不同的模块和复杂度。我们细致地分析了分类驱动和重建驱动信息在模型架构的共享潜在层中无缝共存的程度。我们的研究结果凸显出一个重要挑战：分类驱动的信息削弱了中间层共享表示中的重建驱动信息，反之亦然。同时，我们扩展了共享表示的能力。

    Predictive coding-inspired deep networks for visual computing integrate classification and reconstruction processes in shared intermediate layers. Although synergy between these processes is commonly assumed, it has yet to be convincingly demonstrated. In this study, we take a critical look at how classifying and reconstructing interact in deep learning architectures. Our approach utilizes a purposefully designed family of model architectures reminiscent of autoencoders, each equipped with an encoder, a decoder, and a classification head featuring varying modules and complexities. We meticulously analyze the extent to which classification- and reconstruction-driven information can seamlessly coexist within the shared latent layer of the model architectures. Our findings underscore a significant challenge: Classification-driven information diminishes reconstruction-driven information in intermediate layers' shared representations and vice versa. While expanding the shared representation
    
[^8]: 对具有点状激活的等变网络的一个特征定理

    A Characterization Theorem for Equivariant Networks with Point-wise Activations. (arXiv:2401.09235v1 [cs.LG])

    [http://arxiv.org/abs/2401.09235](http://arxiv.org/abs/2401.09235)

    本论文提出了一个定理，描述了所有可能的有限维表示、坐标选择和点状激活函数的组合，以获得一个完全等变的层，从而推广和加强了现有的特征定理。具有实际相关性的重要情况作为推论进行了讨论。

    

    等变神经网络在对称领域表现出了更好的性能、表达能力和样本复杂度。但对于某些特定的对称性、表示和坐标选择，最常见的点状激活函数（如ReLU）并不具备等变性，因此不能用于设计等变神经网络。本文介绍的定理描述了所有可能的有限维表示、坐标选择和点状激活函数的组合，以获得一个完全等变的层，从而推广和加强了现有的特征定理。我们讨论了具有实际相关性的重要情况作为推论。事实上，我们证明了旋转等变网络只能是不变的，就像对于任何对连通紧致群等变的网络一样。然后，我们讨论了将我们的结果应用于重要的完全等变网络实例时的影响。

    Equivariant neural networks have shown improved performance, expressiveness and sample complexity on symmetrical domains. But for some specific symmetries, representations, and choice of coordinates, the most common point-wise activations, such as ReLU, are not equivariant, hence they cannot be employed in the design of equivariant neural networks. The theorem we present in this paper describes all possible combinations of finite-dimensional representations, choice of coordinates and point-wise activations to obtain an exactly equivariant layer, generalizing and strengthening existing characterizations. Notable cases of practical relevance are discussed as corollaries. Indeed, we prove that rotation-equivariant networks can only be invariant, as it happens for any network which is equivariant with respect to connected compact groups. Then, we discuss implications of our findings when applied to important instances of exactly equivariant networks. First, we completely characterize permu
    
[^9]: 一个使用音高和音素特征进行古典声乐表演的实时歌词对齐系统

    A Real-Time Lyrics Alignment System Using Chroma And Phonetic Features For Classical Vocal Performance. (arXiv:2401.09200v1 [cs.SD])

    [http://arxiv.org/abs/2401.09200](http://arxiv.org/abs/2401.09200)

    该论文提出了一个用于古典声乐表演的实时歌词对齐系统，通过优化音高和音素特征的组合来改进歌词对齐算法，并重新组织了舒伯特冬之旅数据集，为实时歌词对齐模型的评估提供了标准方法。

    

    实时歌词对齐的目标是将现场演唱的音频作为输入，并在演唱过程中准确找到给定歌词的位置。该任务可以在现实世界的应用中受益，比如自动为现场音乐会或歌剧生成字幕。然而，设计一个实时模型面临着挑战，因为只能使用过去的输入并在最小延迟内运行。此外，由于缺乏用于实时歌词对齐模型的数据集，先前的研究大多使用私人内部数据集进行评估，导致缺乏标准评估方法。本文提出了一个用于古典声乐表演的实时歌词对齐系统，做出了两个贡献。首先，我们通过找到音高谱图和音素后验图（PPG）的最佳组合来改进歌词对齐算法，分别捕捉唱歌声音的旋律和语音特征。其次，我们重组了舒伯特冬之旅数据集，其中包含了...

    The goal of real-time lyrics alignment is to take live singing audio as input and to pinpoint the exact position within given lyrics on the fly. The task can benefit real-world applications such as the automatic subtitling of live concerts or operas. However, designing a real-time model poses a great challenge due to the constraints of only using past input and operating within a minimal latency. Furthermore, due to the lack of datasets for real-time models for lyrics alignment, previous studies have mostly evaluated with private in-house datasets, resulting in a lack of standard evaluation methods. This paper presents a real-time lyrics alignment system for classical vocal performances with two contributions. First, we improve the lyrics alignment algorithm by finding an optimal combination of chromagram and phonetic posteriorgram (PPG) that capture melodic and phonetics features of the singing voice, respectively. Second, we recast the Schubert Winterreise Dataset (SWD) which contain
    
[^10]: 从部分观测中进行空间和时间连续的物理模拟

    Space and Time Continuous Physics Simulation From Partial Observations. (arXiv:2401.09198v1 [cs.LG])

    [http://arxiv.org/abs/2401.09198](http://arxiv.org/abs/2401.09198)

    本研究提出了一种新颖的方法，可以从部分观测中进行连续的空间和时间物理模拟，并解决了基于固定支持网格的传统方法的缺点。这种方法通过在稀疏观测上进行训练，利用两个相互关联的动力系统在稀疏位置和连续域上进行预测和插值求解。

    

    现代物理模拟技术依赖于数值方案和网格细化方法来解决精度和复杂性之间的权衡，但这些手工解决方案繁琐且需要高计算能力。基于大规模机器学习的数据驱动方法通过更直接和高效地集成长距离依赖来实现高适应性。在这项工作中，我们主要关注流体动力学，并解决了大部分文献中存在的问题，即计算和预测形式为常规或非规则网格的固定支持。我们提出了一种新颖的设置，可以在连续的空间和时间域中进行预测，同时在稀疏观测上进行训练。我们将这个任务形式化为双观测问题，并提出了一个解决方案，其中在稀疏位置和连续域上定义了两个相互关联的动力系统，可以从初始条件进行预测和插值求解。

    Modern techniques for physical simulations rely on numerical schemes and mesh-refinement methods to address trade-offs between precision and complexity, but these handcrafted solutions are tedious and require high computational power. Data-driven methods based on large-scale machine learning promise high adaptivity by integrating long-range dependencies more directly and efficiently. In this work, we focus on fluid dynamics and address the shortcomings of a large part of the literature, which are based on fixed support for computations and predictions in the form of regular or irregular grids. We propose a novel setup to perform predictions in a continuous spatial and temporal domain while being trained on sparse observations. We formulate the task as a double observation problem and propose a solution with two interlinked dynamical systems defined on, respectively, the sparse positions and the continuous domain, which allows to forecast and interpolate a solution from the initial cond
    
[^11]: GNN-LoFI: 一种通过基于特征的局部直方图交集的新型图神经网络

    GNN-LoFI: a Novel Graph Neural Network through Localized Feature-based Histogram Intersection. (arXiv:2401.09193v1 [cs.LG])

    [http://arxiv.org/abs/2401.09193](http://arxiv.org/abs/2401.09193)

    GNN-LoFI是一种新的图神经网络架构，通过分析局部邻域节点特征的分布来替代传统的消息传递方法。通过直方图交集核函数将相似性信息传播到其他节点，实现了类似于消息传递的机制，并在图分类和回归基准测试中表现出明显优势。

    

    图神经网络越来越成为基于图的机器学习的首选框架。在本文中，我们提出了一种新的图神经网络架构，该架构用局部邻域节点特征的分布分析替代了传统的消息传递方法。为此，我们提取了每个局部邻域的egonet中特征的分布，并将其与一组学习到的标签分布进行直方图交集核函数的比较。然后，将相似性信息传播到网络中的其他节点，从而有效地创建了一种类似于消息传递的机制，其中消息由特征的集合确定。我们进行了消融研究，评估了网络在不同超参数选择下的性能。最后，我们在标准的图分类和回归基准测试中对模型进行了测试，并发现它的表现优于广泛使用的替代方法，包括图核和图神经网络。

    Graph neural networks are increasingly becoming the framework of choice for graph-based machine learning. In this paper, we propose a new graph neural network architecture that substitutes classical message passing with an analysis of the local distribution of node features. To this end, we extract the distribution of features in the egonet for each local neighbourhood and compare them against a set of learned label distributions by taking the histogram intersection kernel. The similarity information is then propagated to other nodes in the network, effectively creating a message passing-like mechanism where the message is determined by the ensemble of the features. We perform an ablation study to evaluate the network's performance under different choices of its hyper-parameters. Finally, we test our model on standard graph classification and regression benchmarks, and we find that it outperforms widely used alternative approaches, including both graph kernels and graph neural networks
    
[^12]: 为渐进式训练语言模型准备课程的方法

    Preparing Lessons for Progressive Training on Language Models. (arXiv:2401.09192v1 [cs.LG])

    [http://arxiv.org/abs/2401.09192](http://arxiv.org/abs/2401.09192)

    提出了一种名为Apollo的方法，通过在低层训练期间学习高层功能，为渐进式训练语言模型设计了课程，实现了最先进的加速比率。

    

    Transformers在人工智能领域的迅速发展带来了资源消耗和温室气体排放的增加，这是由于模型规模的增长。先前的研究表明使用预训练的小模型可以提高训练效率，但这种方法对于新的模型结构可能不适用。另一方面，从头开始训练可能很慢，并且渐进堆叠层往往无法实现显著的加速。为了解决这些挑战，我们提出了一种名为Apollo的新方法，它通过在低层训练期间学习高层功能来准备膨胀操作的课程。我们的方法涉及低值优先采样(LVPS)来训练不同深度，并引入权重共享以促进高效扩展。我们还介绍了一种插值方法来实现稳定的模型深度扩展。实验证明，Apollo实现了最先进的加速比率，甚至……

    The rapid progress of Transformers in artificial intelligence has come at the cost of increased resource consumption and greenhouse gas emissions due to growing model sizes. Prior work suggests using pretrained small models to improve training efficiency, but this approach may not be suitable for new model structures. On the other hand, training from scratch can be slow, and progressively stacking layers often fails to achieve significant acceleration. To address these challenges, we propose a novel method called Apollo, which prep\textbf{a}res lessons for ex\textbf{p}anding \textbf{o}perations by \textbf{l}earning high-\textbf{l}ayer functi\textbf{o}nality during training of low layers. Our approach involves low-value-prioritized sampling (LVPS) to train different depths and weight sharing to facilitate efficient expansion. We also introduce an interpolation method for stable model depth extension. Experiments demonstrate that Apollo achieves state-of-the-art acceleration ratios, even
    
[^13]: 一个用于计算多类分类中对抗训练下界的最优输运方法

    An Optimal Transport Approach for Computing Adversarial Training Lower Bounds in Multiclass Classification. (arXiv:2401.09191v1 [cs.LG])

    [http://arxiv.org/abs/2401.09191](http://arxiv.org/abs/2401.09191)

    本文提出了一个用于计算多类分类中对抗训练下界的最优输运方法，并利用该方法提出了计算最优对抗风险下界和确定最优分类器的算法。通过截断类之间的高阶相互作用，避免了组合运行时间的问题。

    

    尽管基于深度学习的算法取得了很大的成功，但广为人知的是神经网络可能缺乏鲁棒性。强制鲁棒性的流行范式是对抗训练（AT），然而，这引入了许多计算和理论上的困难。最近的研究在多类分类设置和多边际最优输运（MOT）之间建立了联系，为研究这个问题提供了一套新的工具。在本文中，我们利用MOT的连接，提出了计算上最简便可行的数值算法来计算最优对抗风险的普遍下界，并确定最优分类器。我们基于线性规划（LP）和熵正则化（Sinkhorn）提出了两个主要算法。我们的关键洞察是可以无害地截断类之间的高阶相互作用，从而避免了在MOT问题中通常遇到的组合运行时间。我们通过在MNIST和CI 上进行实验证实了这些结果

    Despite the success of deep learning-based algorithms, it is widely known that neural networks may fail to be robust. A popular paradigm to enforce robustness is adversarial training (AT), however, this introduces many computational and theoretical difficulties. Recent works have developed a connection between AT in the multiclass classification setting and multimarginal optimal transport (MOT), unlocking a new set of tools to study this problem. In this paper, we leverage the MOT connection to propose computationally tractable numerical algorithms for computing universal lower bounds on the optimal adversarial risk and identifying optimal classifiers. We propose two main algorithms based on linear programming (LP) and entropic regularization (Sinkhorn). Our key insight is that one can harmlessly truncate the higher order interactions between classes, preventing the combinatorial run times typically encountered in MOT problems. We validate these results with experiments on MNIST and CI
    
[^14]: 探索卷积神经网络在牙科放射学分割中的作用：一项综合系统性文献综述

    Exploring the Role of Convolutional Neural Networks (CNN) in Dental Radiography Segmentation: A Comprehensive Systematic Literature Review. (arXiv:2401.09190v1 [cs.CV])

    [http://arxiv.org/abs/2401.09190](http://arxiv.org/abs/2401.09190)

    本文综述了卷积神经网络在牙科放射学分割中的作用，强调深度学习技术在自动提取诊断数据方面的潜力，以及其在口腔健康问题的早期发现和治疗中的重要作用。

    

    在牙科领域，对诊断工具的精度要求越来越高，特别关注计算机断层扫描、锥束计算机断层扫描、磁共振成像、超声和传统口腔根尖X线等先进成像技术。深度学习在这个背景下成为一个关键工具，可以实现自动分割技术，从而提取必要的诊断数据。这种先进技术的整合解决了对牙科疾病的有效管理的迫切需求，如果不及时发现，可能对人体健康产生重大影响。深度学习在牙科等各个领域的出色成绩突出了它在口腔健康问题的早期发现和治疗中的潜力。目标：通过展示在诊断和预测方面取得显著成果，深度卷积神经网络（CNN）代表了一个新兴领域。

    In the field of dentistry, there is a growing demand for increased precision in diagnostic tools, with a specific focus on advanced imaging techniques such as computed tomography, cone beam computed tomography, magnetic resonance imaging, ultrasound, and traditional intra-oral periapical X-rays. Deep learning has emerged as a pivotal tool in this context, enabling the implementation of automated segmentation techniques crucial for extracting essential diagnostic data. This integration of cutting-edge technology addresses the urgent need for effective management of dental conditions, which, if left undetected, can have a significant impact on human health. The impressive track record of deep learning across various domains, including dentistry, underscores its potential to revolutionize early detection and treatment of oral health issues. Objective: Having demonstrated significant results in diagnosis and prediction, deep convolutional neural networks (CNNs) represent an emerging field 
    
[^15]: 深度学习模型的两尺度复杂度测量

    A Two-Scale Complexity Measure for Deep Learning Models. (arXiv:2401.09184v1 [stat.ML])

    [http://arxiv.org/abs/2401.09184](http://arxiv.org/abs/2401.09184)

    这篇论文介绍了一种用于统计模型的新容量测量2sED，可以可靠地限制泛化误差，并且与训练误差具有很好的相关性。此外，对于深度学习模型，我们展示了如何通过逐层迭代的方法有效地近似2sED，从而处理大量参数的情况。

    

    我们引入了一种基于有效维度的统计模型新容量测量2sED。这个新的数量在对模型进行温和假设的情况下，可以可靠地限制泛化误差。此外，对于标准数据集和流行的模型架构的模拟结果表明，2sED与训练误差具有很好的相关性。对于马尔可夫模型，我们展示了如何通过逐层迭代的方法有效地从下方近似2sED，从而解决具有大量参数的深度学习模型。模拟结果表明，这种近似对不同的突出模型和数据集都很好。

    We introduce a novel capacity measure 2sED for statistical models based on the effective dimension. The new quantity provably bounds the generalization error under mild assumptions on the model. Furthermore, simulations on standard data sets and popular model architectures show that 2sED correlates well with the training error. For Markovian models, we show how to efficiently approximate 2sED from below through a layerwise iterative approach, which allows us to tackle deep learning models with a large number of parameters. Simulation results suggest that the approximation is good for different prominent models and data sets.
    
[^16]: 超越反遗忘: 带有正向传递的多模态连续指导调优

    Beyond Anti-Forgetting: Multimodal Continual Instruction Tuning with Positive Forward Transfer. (arXiv:2401.09181v1 [cs.LG])

    [http://arxiv.org/abs/2401.09181](http://arxiv.org/abs/2401.09181)

    本研究提出了一种名为Fwd-Prompt的方法，通过对输入嵌入进行奇异值分解，并在残差空间和预训练子空间中进行梯度投影，以解决多模态连续指导调优中的灾难性遗忘和负面的正向传递问题。

    

    多模态连续指导调优（MCIT）使得多模态大型语言模型（MLLMs）可以满足不断出现的需求，而无需昂贵的重新训练。MCIT面临两个主要障碍：灾难性遗忘（旧知识被遗忘）和负面的正向传递（未来任务的性能下降）。虽然现有方法大大缓解了灾难性遗忘，但仍然遭受负面的正向传递。通过对输入嵌入进行奇异值分解（SVD），我们发现不同输入嵌入之间存在很大差异。这种差异导致模型学习与旧的和预训练的任务无关的信息，从而导致灾难性遗忘和负面的正向传递。为了解决这些问题，我们提出了Fwd-Prompt，这是一种基于提示的方法，将提示梯度投影到残差空间中，以减小任务之间的干扰，并投影到预训练子空间中以重用预训练的知识。

    Multimodal Continual Instruction Tuning (MCIT) enables Multimodal Large Language Models (MLLMs) to meet continuously emerging requirements without expensive retraining. MCIT faces two major obstacles: catastrophic forgetting (where old knowledge is forgotten) and negative forward transfer (where the performance of future tasks is degraded). Although existing methods have greatly alleviated catastrophic forgetting, they still suffer from negative forward transfer. By performing singular value decomposition (SVD) on input embeddings, we discover a large discrepancy in different input embeddings. The discrepancy results in the model learning irrelevant information for old and pre-trained tasks, which leads to catastrophic forgetting and negative forward transfer. To address these issues, we propose Fwd-Prompt, a prompt-based method projecting prompt gradient to the residual space to minimize the interference between tasks and to the pre-trained subspace for reusing pre-trained knowledge. 
    
[^17]: 通过可控的变分自编码器中的受控解缠来进行无监督多领域翻译

    Unsupervised Multiple Domain Translation through Controlled Disentanglement in Variational Autoencoder. (arXiv:2401.09180v1 [cs.LG])

    [http://arxiv.org/abs/2401.09180](http://arxiv.org/abs/2401.09180)

    该论文提出了一种无监督多领域翻译的方法，通过修改后的变分自编码器实现了受控解缠的两个潜变量，其中一个仅与领域有关，另一个与数据的其他变化因素有关。实验证明该方法在不同的视觉数据集上提高了性能。

    

    无监督多领域翻译是将数据从一个领域转换到其他领域，而无需配对数据来训练系统的任务。通常，基于生成对抗网络（GAN）的方法被用来解决这个任务。然而，我们的提议完全依赖于修改过的变分自编码器。这种修改包括通过设计以控制方式解缠两个潜变量。其中一个潜变量被强制仅依赖于领域，而另一个潜变量必须依赖于数据的其他变化因素。此外，对于领域潜变量的条件限制可以更好地控制和理解潜空间。我们从实证上证明了我们的方法适用于不同的视觉数据集，提高了其他众所周知的方法的性能。最后，我们证明了实际上一个潜变量存储了与领域和其他变化因素有关的所有信息。

    Unsupervised Multiple Domain Translation is the task of transforming data from one domain to other domains without having paired data to train the systems. Typically, methods based on Generative Adversarial Networks (GANs) are used to address this task. However, our proposal exclusively relies on a modified version of a Variational Autoencoder. This modification consists of the use of two latent variables disentangled in a controlled way by design. One of this latent variables is imposed to depend exclusively on the domain, while the other one must depend on the rest of the variability factors of the data. Additionally, the conditions imposed over the domain latent variable allow for better control and understanding of the latent space. We empirically demonstrate that our approach works on different vision datasets improving the performance of other well known methods. Finally, we prove that, indeed, one of the latent variables stores all the information related to the domain and the o
    
[^18]: ADCNet：预测抗体药物复合物活性的统一框架

    ADCNet: a unified framework for predicting the activity of antibody-drug conjugates. (arXiv:2401.09176v1 [cs.LG])

    [http://arxiv.org/abs/2401.09176](http://arxiv.org/abs/2401.09176)

    ADCNet是一个统一框架，通过整合蛋白质和小分子的表示学习模型，能够预测抗体药物复合物的活性。其在手动调整的数据集上表现最佳。

    

    抗体药物复合物（ADC）由于其精确靶向癌细胞并释放高效药物的能力，已经在精准医学时代彻底改变了癌症治疗领域。然而，由于ADC的结构与活性之间的关系难以理解，因此合理设计ADC非常困难。在本研究中，我们介绍了一个名为ADCNet的统一深度学习框架，帮助设计潜在的ADC。ADCNet集成了蛋白质表示学习语言模型ESM-2和小分子表示学习语言模型FG-BERT，通过学习ADC的抗原和抗体蛋白质序列、连接物和药物的SMILES序列，以及药物-抗体比（DAR）值的有意义特征，实现了活性预测。根据精心设计和手动调整的ADC数据集，广泛的评估结果表明，与基准相比，ADCNet在测试集上表现最佳。

    Antibody-drug conjugate (ADC) has revolutionized the field of cancer treatment in the era of precision medicine due to their ability to precisely target cancer cells and release highly effective drug. Nevertheless, the realization of rational design of ADC is very difficult because the relationship between their structures and activities is difficult to understand. In the present study, we introduce a unified deep learning framework called ADCNet to help design potential ADCs. The ADCNet highly integrates the protein representation learning language model ESM-2 and small-molecule representation learning language model FG-BERT models to achieve activity prediction through learning meaningful features from antigen and antibody protein sequences of ADC, SMILES strings of linker and payload, and drug-antibody ratio (DAR) value. Based on a carefully designed and manually tailored ADC data set, extensive evaluation results reveal that ADCNet performs best on the test set compared to baseline
    
[^19]: 异步Local-SGD训练语言建模

    Asynchronous Local-SGD Training for Language Modeling. (arXiv:2401.09135v1 [cs.LG])

    [http://arxiv.org/abs/2401.09135](http://arxiv.org/abs/2401.09135)

    本文通过异步Local-SGD训练语言模型，并进行了全面的实证研究。研究发现，尽管异步更新更频繁，但其收敛所需的迭代次数多于同步方法。作者还提出了一种利用延迟的Nesterov动量更新进行调整的新方法来解决异步更新的挑战。

    

    Local随机梯度下降(Local-SGD)，也称为联邦平均，是一种分布式优化方法，其中每个设备在通信中执行多个SGD更新。本文介绍了异步Local-SGD用于训练语言模型的经验证研究；即，每个工作节点在完成其SGD步骤后立即更新全局参数。我们通过考察工作节点硬件异构性、模型大小、工作节点数量和优化器等因素对学习性能的影响进行了全面调查。我们发现，尽管更频繁地更新（全局）模型参数，但异步Local-SGD比其同步对应物需要更多迭代才能收敛。我们确定了在工作节点梯度陈旧时全局参数的动量加速作为一个关键挑战。我们提出了一种利用延迟的Nesterov动量更新，根据工作节点的本地训练步骤进行调整的新方法。

    Local stochastic gradient descent (Local-SGD), also referred to as federated averaging, is an approach to distributed optimization where each device performs more than one SGD update per communication. This work presents an empirical study of {\it asynchronous} Local-SGD for training language models; that is, each worker updates the global parameters as soon as it has finished its SGD steps. We conduct a comprehensive investigation by examining how worker hardware heterogeneity, model size, number of workers, and optimizer could impact the learning performance. We find that with naive implementations, asynchronous Local-SGD takes more iterations to converge than its synchronous counterpart despite updating the (global) model parameters more frequently. We identify momentum acceleration on the global parameters when worker gradients are stale as a key challenge. We propose a novel method that utilizes a delayed Nesterov momentum update and adjusts the workers' local training steps based
    
[^20]: 理解图神经网络的异质性

    Understanding Heterophily for Graph Neural Networks. (arXiv:2401.09125v1 [cs.LG])

    [http://arxiv.org/abs/2401.09125](http://arxiv.org/abs/2401.09125)

    本文通过提出的异质性随机块模型（HSBM）来提供对不同异质性模式对图神经网络（GNNs）影响的理论理解。研究发现，异质性对分类的影响需要与平均节点度一起评估，并且拓扑噪声对分类有负面影响。

    

    具有异质性的图被认为是图神经网络（GNNs）面临挑战的情景，其中节点通过各种模式与不同的邻居相连接。本文通过将图卷积（GC）操作合并到完全连接的网络中，通过提出的异质性随机块模型（HSBM）来提供对不同异质性模式对GNNs影响的理论理解，HSBM是一个可以容纳多样的异质性模式的通用随机图模型。首先，我们展示了通过应用GC操作，可分性增益取决于两个因素，即邻域分布的欧氏距离和$\sqrt{\mathbb{E}\left[\operatorname{deg}\right]}$，其中$\mathbb{E}\left[\operatorname{deg}\right]$是平均节点度。它揭示了异质性对分类的影响需要与平均节点度一起评估。其次，我们展示了拓扑噪声具有负面影响

    Graphs with heterophily have been regarded as challenging scenarios for Graph Neural Networks (GNNs), where nodes are connected with dissimilar neighbors through various patterns. In this paper, we present theoretical understandings of the impacts of different heterophily patterns for GNNs by incorporating the graph convolution (GC) operations into fully connected networks via the proposed Heterophilous Stochastic Block Models (HSBM), a general random graph model that can accommodate diverse heterophily patterns. Firstly, we show that by applying a GC operation, the separability gains are determined by two factors, i.e., the Euclidean distance of the neighborhood distributions and $\sqrt{\mathbb{E}\left[\operatorname{deg}\right]}$, where $\mathbb{E}\left[\operatorname{deg}\right]$ is the averaged node degree. It reveals that the impact of heterophily on classification needs to be evaluated alongside the averaged node degree. Secondly, we show that the topological noise has a detrimenta
    
[^21]: RWKV-TS：超越传统循环神经网络在时间序列任务中的应用

    RWKV-TS: Beyond Traditional Recurrent Neural Network for Time Series Tasks. (arXiv:2401.09093v1 [cs.LG])

    [http://arxiv.org/abs/2401.09093](http://arxiv.org/abs/2401.09093)

    RWKV-TS是一个高效的RNN模型，它通过具有低时间复杂度和内存使用的新颖架构、增强的捕捉长期序列信息能力以及高计算效率的特点，超越了传统循环神经网络在时间序列任务中的应用，并在与最先进的Transformer或CNN模型的比较中展现出竞争力。

    

    传统的循环神经网络（RNN）模型如LSTM和GRU在时间序列任务中一直占据主导地位。然而，近年来它们在各种时间序列任务中的地位有所下降。因此，最近的时间序列预测研究已经发生了明显的转变，从RNN向Transformers、MLPs和CNNs等其他架构转变。为了超越传统RNN的局限性，我们设计了一种名为RWKV-TS的高效RNN模型，具有以下三个独特的特点：（i）具有$O(L)$的时间复杂度和内存使用的新颖RNN架构；（ii）相较于传统RNN，更能够捕捉长期的序列信息；（iii）高计算效率，能够有效地扩展。通过广泛的实验，我们提出的RWKV-TS模型在与基于Transformer类型或CNN类型的最先进模型比较时表现出竞争力。

    Traditional Recurrent Neural Network (RNN) architectures, such as LSTM and GRU, have historically held prominence in time series tasks. However, they have recently seen a decline in their dominant position across various time series tasks. As a result, recent advancements in time series forecasting have seen a notable shift away from RNNs towards alternative architectures such as Transformers, MLPs, and CNNs. To go beyond the limitations of traditional RNNs, we design an efficient RNN-based model for time series tasks, named RWKV-TS, with three distinctive features: (i) A novel RNN architecture characterized by $O(L)$ time complexity and memory usage. (ii) An enhanced ability to capture long-term sequence information compared to traditional RNNs. (iii) High computational efficiency coupled with the capacity to scale up effectively. Through extensive experimentation, our proposed RWKV-TS model demonstrates competitive performance when compared to state-of-the-art Transformer-based or CN
    
[^22]: 大型语言模型中的代码模拟挑战

    Code Simulation Challenges for Large Language Models. (arXiv:2401.09074v1 [cs.LG])

    [http://arxiv.org/abs/2401.09074](http://arxiv.org/abs/2401.09074)

    大型语言模型在模拟计算机代码和算法执行方面遇到挑战，性能随着代码长度的增加而迅速下降。在处理短程序或标准过程时，它们能以低错误率按顺序执行指令，但对于复杂的程序，特别是包含关键路径和冗余指令的程序，模拟效果较差。我们提出了一种逐行模拟代码执行的方法来解决这个问题。

    

    我们调查了大型语言模型（LLMs）在模拟计算机代码和算法执行方面的能力。我们首先研究了直线程序，并展示了当前LLMs在处理这样简单的程序时表现出的性能较差——性能随着代码长度的增加而迅速下降。接着，我们研究了LLMs在模拟包含关键路径和冗余指令的程序方面的能力。我们还通过排序算法和嵌套循环超越了直线程序的模拟，并展示了程序的计算复杂性直接影响LLMs模拟其执行的能力。我们观察到LLMs只有在处理短程序或标准过程时才能以低错误率按顺序执行指令。LLMs的代码模拟与它们的模式识别和记忆能力存在矛盾：在记忆对任务有害的情况下，我们提出了一种新的提示方法，逐行模拟代码的执行。

    We investigate the extent to which Large Language Models (LLMs) can simulate the execution of computer code and algorithms. We begin by looking straight line programs, and show that current LLMs demonstrate poor performance even with such simple programs -- performance rapidly degrades with the length of code. We then investigate the ability of LLMs to simulate programs that contain critical paths and redundant instructions. We also go beyond straight line program simulation with sorting algorithms and nested loops, and we show the computational complexity of a routine directly affects the ability of an LLM to simulate its execution. We observe that LLMs execute instructions sequentially and with a low error margin only for short programs or standard procedures. LLMs' code simulation is in tension with their pattern recognition and memorisation capabilities: on tasks where memorisation is detrimental, we propose a novel prompting method to simulate code execution line by line. Empirica
    
[^23]: 固定预算差分隐私最佳臂识别

    Fixed-Budget Differentially Private Best Arm Identification. (arXiv:2401.09073v1 [cs.LG])

    [http://arxiv.org/abs/2401.09073](http://arxiv.org/abs/2401.09073)

    本论文研究了差分隐私约束下固定预算条件下的最佳臂识别问题，提出了满足差分隐私约束的策略DP-BAI，并得到了错误概率的上界和最小-最大下界的指数衰减关系。

    

    我们在差分隐私约束下研究了线性赌博机中固定预算条件下的最佳臂识别问题，其中臂的奖励在单位区间上。给定一个有限的预算$T$和隐私参数$\varepsilon>0$，目标是在$T$个采样轮后最小化寻找平均值最大的臂的错误概率，同时满足决策者策略满足特定的$\varepsilon$-差分隐私($\varepsilon$-DP)约束条件。我们通过提出“最大绝对行列式”原则构建满足$\varepsilon$-DP约束的策略(称为DP-BAI)，并给出其错误概率的上界。此外，我们得到了错误概率的最小-最大下界，并证明这两个界在$T$上按指数衰减，界中的指数与(a)臂的次优间隙，(b)$\varepsilon$和(c)关联。

    We study best arm identification (BAI) in linear bandits in the fixed-budget regime under differential privacy constraints, when the arm rewards are supported on the unit interval. Given a finite budget $T$ and a privacy parameter $\varepsilon>0$, the goal is to minimise the error probability in finding the arm with the largest mean after $T$ sampling rounds, subject to the constraint that the policy of the decision maker satisfies a certain {\em $\varepsilon$-differential privacy} ($\varepsilon$-DP) constraint. We construct a policy satisfying the $\varepsilon$-DP constraint (called {\sc DP-BAI}) by proposing the principle of {\em maximum absolute determinants}, and derive an upper bound on its error probability. Furthermore, we derive a minimax lower bound on the error probability, and demonstrate that the lower and the upper bounds decay exponentially in $T$, with exponents in the two bounds matching order-wise in (a) the sub-optimality gaps of the arms, (b) $\varepsilon$, and (c) t
    
[^24]: 用空间自适应滤波重新思考谱图神经网络

    Rethinking Spectral Graph Neural Networks with Spatially Adaptive Filtering. (arXiv:2401.09071v1 [cs.LG])

    [http://arxiv.org/abs/2401.09071](http://arxiv.org/abs/2401.09071)

    本文重新思考了谱图神经网络，并揭示了谱滤波和空间聚合之间的联系。该研究发现，谱滤波在隐含地将原始图转换成适应性新图，并明确计算用于空间聚合的新图。适应性新图展现出非局部性，并能够反映节点之间的标签一致性。

    

    尽管谱图神经网络（GNN）在理论上在谱域中有很好的基础，但它们实际上依赖于多项式逼近，意味着它们与空间域有着深刻的联系。由于以前的研究很少从空间角度研究谱图GNN，因此它们在空间域的可解释性仍然难以捉摸，例如，谱图GNN在空间域中实际上编码了哪些信息？为了回答这个问题，本文在谱滤波和空间聚合之间建立了一个理论上的联系，揭示了谱滤波隐含地将原始图转换成适应性新图的内在交互作用，并明确地计算用于空间聚合的适应性新图。理论和经验研究表明，适应性新图不仅表现出非局部性，还能够容纳有符号的边权重以反映节点之间的标签一致性。因此，这些发现突显了谱图GNN在空间中的可解释性角色。

    Whilst spectral Graph Neural Networks (GNNs) are theoretically well-founded in the spectral domain, their practical reliance on polynomial approximation implies a profound linkage to the spatial domain. As previous studies rarely examine spectral GNNs from the spatial perspective, their spatial-domain interpretability remains elusive, e.g., what information is essentially encoded by spectral GNNs in the spatial domain? In this paper, to answer this question, we establish a theoretical connection between spectral filtering and spatial aggregation, unveiling an intrinsic interaction that spectral filtering implicitly leads the original graph to an adapted new graph, explicitly computed for spatial aggregation. Both theoretical and empirical investigations reveal that the adapted new graph not only exhibits non-locality but also accommodates signed edge weights to reflect label consistency between nodes. These findings thus highlight the interpretable role of spectral GNNs in the spatial 
    
[^25]: DTMM：利用修剪在极弱的物联网设备上部署TinyML模型

    DTMM: Deploying TinyML Models on Extremely Weak IoT Devices with Pruning. (arXiv:2401.09068v1 [cs.LG])

    [http://arxiv.org/abs/2401.09068](http://arxiv.org/abs/2401.09068)

    DTMM是一个库，旨在在弱物联网设备上高效部署和执行机器学习模型。之前的解决方案无法同时实现在不损害准确性的情况下深度压缩模型和高效执行的目标，而DTMM通过修剪单元选择解决了这个问题。

    

    DTMM是一个为弱物联网设备（如微控制器单元）上的机器学习模型的高效部署和执行而设计的库。设计DTMM的动机来自于新兴领域的小型机器学习（TinyML），它探索将机器学习扩展到许多低端物联网设备以实现普遍智能。由于嵌入式设备的能力较弱，需要在部署之前通过修剪足够的权重来压缩模型。尽管修剪已在许多计算平台上进行了广泛研究，但修剪方法在MCUs上面临两个关键问题的加剧：需要在不显著损害准确性的情况下深度压缩模型，并且修剪后的模型在执行效率上应具备高效性。目前的解决方案只能实现其中一个目标，而不能同时实现两者。在本文中，我们发现修剪后的模型在MCUs上具有高效部署和执行的巨大潜力。因此，我们提出了具有修剪单元选择的DTMM。

    DTMM is a library designed for efficient deployment and execution of machine learning models on weak IoT devices such as microcontroller units (MCUs). The motivation for designing DTMM comes from the emerging field of tiny machine learning (TinyML), which explores extending the reach of machine learning to many low-end IoT devices to achieve ubiquitous intelligence. Due to the weak capability of embedded devices, it is necessary to compress models by pruning enough weights before deploying. Although pruning has been studied extensively on many computing platforms, two key issues with pruning methods are exacerbated on MCUs: models need to be deeply compressed without significantly compromising accuracy, and they should perform efficiently after pruning. Current solutions only achieve one of these objectives, but not both. In this paper, we find that pruned models have great potential for efficient deployment and execution on MCUs. Therefore, we propose DTMM with pruning unit selection,
    
[^26]: 通过HSIC-Bottleneck正交化和平均角嵌入实现持续学习目标

    Towards Continual Learning Desiderata via HSIC-Bottleneck Orthogonalization and Equiangular Embedding. (arXiv:2401.09067v1 [cs.LG])

    [http://arxiv.org/abs/2401.09067](http://arxiv.org/abs/2401.09067)

    本文提出了一种方法，通过HSIC-Bottleneck正交化和平均角嵌入，实现了对持续学习中遗忘问题的解决，该方法在不使用先前任务的训练数据且模型大小相对恒定的条件下，通过限制逐层参数覆盖和决策边界畸变来避免遗忘。

    

    在顺序任务训练中，深度神经网络容易遭受灾难性遗忘。各种持续学习（CL）方法通常依赖于样本缓冲区和/或网络扩展，以平衡模型的稳定性和可塑性，但这会损害其实际价值，因为涉及到隐私和内存问题。相反，本文考虑了一个严格但现实的设置，即以前任务的训练数据不可用，且在顺序训练期间模型的大小保持相对恒定。为了实现这样的目标，我们提出了一种概念简单但有效的方法，将遗忘归因于逐层参数覆盖和由此产生的决策边界畸变。这通过两个关键组件之间的协同作用实现：HSIC-Bottleneck正交化（HBO）在正交空间中实现非覆盖参数的更新，通过Hilbert-Schmidt独立性准则进行中介；而平均角嵌入（EAE）则增强了决策边界的适应能力。

    Deep neural networks are susceptible to catastrophic forgetting when trained on sequential tasks. Various continual learning (CL) methods often rely on exemplar buffers or/and network expansion for balancing model stability and plasticity, which, however, compromises their practical value due to privacy and memory concerns. Instead, this paper considers a strict yet realistic setting, where the training data from previous tasks is unavailable and the model size remains relatively constant during sequential training. To achieve such desiderata, we propose a conceptually simple yet effective method that attributes forgetting to layer-wise parameter overwriting and the resulting decision boundary distortion. This is achieved by the synergy between two key components: HSIC-Bottleneck Orthogonalization (HBO) implements non-overwritten parameter updates mediated by Hilbert-Schmidt independence criterion in an orthogonal space and EquiAngular Embedding (EAE) enhances decision boundary adaptat
    
[^27]: Consistent3D: 实现一致高保真度的文本到3D生成方法，采用确定性采样先验

    Consistent3D: Towards Consistent High-Fidelity Text-to-3D Generation with Deterministic Sampling Prior. (arXiv:2401.09050v1 [cs.CV])

    [http://arxiv.org/abs/2401.09050](http://arxiv.org/abs/2401.09050)

    本论文提出了一种名为“Consistent3D”的方法，通过探索普通微分方程的确定性采样先验，解决了分数蒸馏采样（SDS）在文本到3D生成中容易出现几何崩溃和质量差纹理的问题。

    

    分数蒸馏采样（SDS）及其变种极大地推动了文本到3D生成领域的发展，但容易出现几何崩溃和质量差的纹理。为了解决这个问题，我们首先深入分析了SDS，并发现它的蒸馏采样过程实际上对应于随机微分方程（SDE）的轨迹采样：SDS沿着SDE轨迹进行采样，以产生一个更少带噪声的样本，这个样本则作为优化3D模型的指导。然而，SDE采样中的随机性经常导致样本多样且不可预测，不总是更少带噪声，因此不是一个一致正确的指导，这解释了SDS的易受攻击性。由于对于任何SDE，总是存在一个普通微分方程（ODE），其轨迹采样可以确定性和一致地收敛到所需目标点作为SDE，我们提出了一种新颖有效的“Consistent3D”方法，探索ODE的确定性采样先验以解决这个问题。

    Score distillation sampling (SDS) and its variants have greatly boosted the development of text-to-3D generation, but are vulnerable to geometry collapse and poor textures yet. To solve this issue, we first deeply analyze the SDS and find that its distillation sampling process indeed corresponds to the trajectory sampling of a stochastic differential equation (SDE): SDS samples along an SDE trajectory to yield a less noisy sample which then serves as a guidance to optimize a 3D model. However, the randomness in SDE sampling often leads to a diverse and unpredictable sample which is not always less noisy, and thus is not a consistently correct guidance, explaining the vulnerability of SDS. Since for any SDE, there always exists an ordinary differential equation (ODE) whose trajectory sampling can deterministically and consistently converge to the desired target point as the SDE, we propose a novel and effective "Consistent3D" method that explores the ODE deterministic sampling prior for
    
[^28]: 数据归因对扩散模型的影响：时间步引起的对影响估计的偏差

    Data Attribution for Diffusion Models: Timestep-induced Bias in Influence Estimation. (arXiv:2401.09031v1 [cs.LG])

    [http://arxiv.org/abs/2401.09031](http://arxiv.org/abs/2401.09031)

    本文研究了数据归因方法对扩散模型的影响，发现对于在引发大范数时间步骤上训练的样本，其损失梯度范数高度依赖于时间步骤，导致在影响估计中存在显著的偏差。为了解决这个问题，提出了Diffusion-ReTr方法。

    

    数据归因方法可以将模型行为追溯到其训练数据集，为理解“黑箱”神经网络提供了一种有效的方法。虽然先前的研究已经在各种情况下建立了模型输出与训练数据之间的可量化联系，但在与训练样本相关的扩散模型输出的解释方面仍然未被充分探索。特别是，扩散模型通过一系列时间步骤而不是之前的瞬时输入输出关系操作，对直接将现有框架扩展到扩散模型构成了重大挑战。值得注意的是，我们提出了Diffusion-TracIn，它包含了这种时间动力学，并观察到样本的损失梯度范数高度依赖于时间步骤。这种趋势导致影响估计中存在显著的偏差，对于在引发大范数时间步骤上训练的样本尤为明显，导致它们通常具有影响力。为了减轻这种影响，我们引入了Diffusion-ReTr方法。

    Data attribution methods trace model behavior back to its training dataset, offering an effective approach to better understand ``black-box'' neural networks. While prior research has established quantifiable links between model output and training data in diverse settings, interpreting diffusion model outputs in relation to training samples remains underexplored. In particular, diffusion models operate over a sequence of timesteps instead of instantaneous input-output relationships in previous contexts, posing a significant challenge to extend existing frameworks to diffusion models directly. Notably, we present Diffusion-TracIn that incorporates this temporal dynamics and observe that samples' loss gradient norms are highly dependent on timestep. This trend leads to a prominent bias in influence estimation, and is particularly noticeable for samples trained on large-norm-inducing timesteps, causing them to be generally influential. To mitigate this effect, we introduce Diffusion-ReTr
    
[^29]: 残差对齐：揭示残差网络的机制

    Residual Alignment: Uncovering the Mechanisms of Residual Networks. (arXiv:2401.09018v1 [cs.LG])

    [http://arxiv.org/abs/2401.09018](http://arxiv.org/abs/2401.09018)

    本研究通过线性化残差模块并测量其奇异值分解，揭示了ResNet架构的成功机制，包括中间表示的等间隔分布、残差雅可比的对齐以及与类别数相关的残差雅可比秩的限制。

    

    由于通过简单的跳跃连接显著提高了性能，ResNet架构在深度学习中被广泛采用，然而导致其成功的基本机制仍然大部分未知。在本文中，我们通过使用残差雅可比和测量其奇异值分解，对ResNet架构在分类任务中进行了深入的实证研究，线性化其组成的残差模块。我们的测量结果揭示了一个名为残差对齐的过程，其具有以下四个特性：（RA1）给定输入的中间表示在嵌入高维空间的线上等间隔，如Gai和Zhang [2021]观察到的一样；（RA2）残差雅可比的左上和右上奇异向量相互对齐，并且在不同深度上也相互对齐；（RA3）对于全连接的ResNet来说，残差雅可比至多是秩C，其中C是类别数；（RA4）残差雅可比的前N个奇异值与N相关联的数据集规模成反比例缩放。

    The ResNet architecture has been widely adopted in deep learning due to its significant boost to performance through the use of simple skip connections, yet the underlying mechanisms leading to its success remain largely unknown. In this paper, we conduct a thorough empirical study of the ResNet architecture in classification tasks by linearizing its constituent residual blocks using Residual Jacobians and measuring their singular value decompositions. Our measurements reveal a process called Residual Alignment (RA) characterized by four properties:  (RA1) intermediate representations of a given input are equispaced on a line, embedded in high dimensional space, as observed by Gai and Zhang [2021];  (RA2) top left and right singular vectors of Residual Jacobians align with each other and across different depths;  (RA3) Residual Jacobians are at most rank C for fully-connected ResNets, where C is the number of classes; and  (RA4) top singular values of Residual Jacobians scale inversely
    
[^30]: 机器学习模型对人工智能系统的归纳能力不足，缺乏良好的解释能力

    Inductive Models for Artificial Intelligence Systems are Insufficient without Good Explanations. (arXiv:2401.09011v1 [cs.LG])

    [http://arxiv.org/abs/2401.09011](http://arxiv.org/abs/2401.09011)

    本文讨论了机器学习模型的局限性，特别是深度神经网络的透明度和解释能力不足，提出AI系统不仅需要预测能力，还需要提供良好的解释能力和洞察力。

    

    本文讨论了机器学习（ML），特别是深度人工神经网络（ANNs）的局限性，它们在逼近复杂函数方面有效，但常常缺乏透明度和解释能力。本文强调了归纳问题：即过去的观察不一定能预测未来事件，这是ML模型在遭遇新的、未见过的数据时面临的挑战。本文主张重要的不仅是做出预测，还要提供良好的解释，而当前模型往往未能做到这一点。它建议为了AI的进步，我们必须寻求能够提供洞察力和解释能力的模型，而不仅仅是预测能力。

    This paper discusses the limitations of machine learning (ML), particularly deep artificial neural networks (ANNs), which are effective at approximating complex functions but often lack transparency and explanatory power. It highlights the `problem of induction' : the philosophical issue that past observations may not necessarily predict future events, a challenge that ML models face when encountering new, unseen data. The paper argues for the importance of not just making predictions but also providing good explanations, a feature that current models often fail to deliver. It suggests that for AI to progress, we must seek models that offer insights and explanations, not just predictions.
    
[^31]: 通过迭代组合问题来增强数学问题求解

    Augmenting Math Word Problems via Iterative Question Composing. (arXiv:2401.09003v1 [cs.CL])

    [http://arxiv.org/abs/2401.09003](http://arxiv.org/abs/2401.09003)

    本研究通过引入MMIQC数据集和迭代组合问题(IQC)的新颖增强方法，成功提高了大型语言模型的数学推理能力，在竞赛级数学问题上取得了优于先前最佳结果的准确率。

    

    尽管在改善大型语言模型(LLMs)的数学推理能力方面取得了一定进展，但在不使用外部工具的情况下解决竞赛级数学问题仍然对开源LLMs具有挑战性。在这项工作中，我们介绍了MMIQC数据集，这是一个混合处理的网络数据和合成问题-响应对的混合数据集，以提供基础模型更好的数学推理能力。通过在MMIQC上对Mistral-7B(arXiv:2310.06825)进行微调获得的模型Mistral-7B-MMIQC，在MATH(arXiv:2103.03874)上达到了36.0%的准确率，比之前(model size $\sim$7B)的最佳结果高出5.8%。我们的实验还表明，改进的一个重要部分归功于我们的新颖增强方法IQC(迭代组合问题)，其中我们迭代地要求LLM从给定的种子问题中组合新问题，并从另一个LLM中进行拒绝抽样。MMIQC现已在https://huggingface.co/datasets/Vivacem/MMIQC上发布。

    Despite recent progress in improving the mathematical reasoning ability of large language models(LLMs), solving competition-level math problems without the use of external tools remains challenging for open-source LLMs. In this work, we introduce the MMIQC dataset, a mixture of processed web data and synthetic question-response pairs, to equip base models with better mathematical reasoning skills. Mistral-7B-MMIQC, the model obtained by fine-tuning Mistral-7B(arXiv:2310.06825) on MMIQC, achieves 36.0\% accuracy on MATH(arXiv:2103.03874), 5.8\% higher than the previous (model size $\sim$7B) SOTA. Our experiments also show that a large part of the improvement attributes to our novel augmentation method IQC(Iterative Question Composing), where we iteratively ask an LLM to compose new questions from the given seed problems and do rejection sampling from another LLM. MMIQC has now been released on https://huggingface.co/datasets/Vivacem/MMIQC.
    
[^32]: 连续时间连续空间的稳态强化学习（CTCS-HRRL）：朝向生物自主代理

    Continuous Time Continuous Space Homeostatic Reinforcement Learning (CTCS-HRRL) : Towards Biological Self-Autonomous Agent. (arXiv:2401.08999v1 [cs.AI])

    [http://arxiv.org/abs/2401.08999](http://arxiv.org/abs/2401.08999)

    本文提出了连续时间连续空间的稳态强化学习框架CTCS-HRRL，并通过模拟实验证明了该模型的有效性和与代理能力相关的证据。

    

    稳态是生物维持内部平衡的生理过程。先前的研究表明，稳态是一种学习行为。最近引入的稳态调节的强化学习（HRRL）框架试图通过将驱动减少理论和强化学习相结合来解释这种学习的稳态行为。这种链接已经在离散时间空间上得到证明，但是在连续时间空间上尚未得到证明。在这项工作中，我们将HRRL框架推广到连续时间空间环境，并验证连续时间连续空间HRRL（CTCS-HRRL）框架。我们通过设计一个模型来模拟真实生物代理中的稳态机制来实现这一目标。该模型使用Hamilton-Jacobian Bellman方程以及基于神经网络和强化学习的函数近似。通过基于模拟的实验，我们展示了该模型的有效性，并揭示了与代理能力有关的证据。

    Homeostasis is a biological process by which living beings maintain their internal balance. Previous research suggests that homeostasis is a learned behaviour. Recently introduced Homeostatic Regulated Reinforcement Learning (HRRL) framework attempts to explain this learned homeostatic behavior by linking Drive Reduction Theory and Reinforcement Learning. This linkage has been proven in the discrete time-space, but not in the continuous time-space. In this work, we advance the HRRL framework to a continuous time-space environment and validate the CTCS-HRRL (Continuous Time Continuous Space HRRL) framework. We achieve this by designing a model that mimics the homeostatic mechanisms in a real-world biological agent. This model uses the Hamilton-Jacobian Bellman Equation, and function approximation based on neural networks and Reinforcement Learning. Through a simulation-based experiment we demonstrate the efficacy of this model and uncover the evidence linked to the agent's ability to dy
    
[^33]: 攻击与重置用于遗忘：通过参数重新初始化利用对抗性噪声实现机器遗忘

    Attack and Reset for Unlearning: Exploiting Adversarial Noise toward Machine Unlearning through Parameter Re-initialization. (arXiv:2401.08998v1 [cs.LG])

    [http://arxiv.org/abs/2401.08998](http://arxiv.org/abs/2401.08998)

    该论文提出了一种攻击和重置用于遗忘的方法（ARU），通过利用对抗性噪声生成参数蒙版，从而有效地重置模型中的特定参数，使其无法学习。该方法在面部机器遗忘任务上超越了现有的最先进结果，是在保护数据隐私方面的重要进展。

    

    随着隐私和监管合规方面的关注不断增加，机器遗忘的概念日益重要，旨在选择性地忘记或删除训练模型中的特定学习信息。针对这一迫切需求，我们引入了一种称为攻击和重置用于遗忘（ARU）的新方法。该算法利用精心设计的对抗性噪声生成参数蒙版，有效地重置某些参数并使其无法学习。ARU在两个面部机器遗忘基准数据集MUFAC和MUCAC上优于当前最先进的结果。我们特别介绍了攻击和蒙版的步骤，这些步骤过滤和重置对忘记集有偏见的网络参数。我们的工作通过利用对抗性噪声来设计蒙版，在通过参数重新初始化方面取得了使数据对深度学习模型不可利用的显著进展。

    With growing concerns surrounding privacy and regulatory compliance, the concept of machine unlearning has gained prominence, aiming to selectively forget or erase specific learned information from a trained model. In response to this critical need, we introduce a novel approach called Attack-and-Reset for Unlearning (ARU). This algorithm leverages meticulously crafted adversarial noise to generate a parameter mask, effectively resetting certain parameters and rendering them unlearnable. ARU outperforms current state-of-the-art results on two facial machine-unlearning benchmark datasets, MUFAC and MUCAC. In particular, we present the steps involved in attacking and masking that strategically filter and re-initialize network parameters biased towards the forget set. Our work represents a significant advancement in rendering data unexploitable to deep learning models through parameter re-initialization, achieved by harnessing adversarial noise to craft a mask.
    
[^34]: MicroNAS: 零样本神经架构搜索用于MCUs

    MicroNAS: Zero-Shot Neural Architecture Search for MCUs. (arXiv:2401.08996v1 [cs.LG])

    [http://arxiv.org/abs/2401.08996](http://arxiv.org/abs/2401.08996)

    MicroNAS是一个针对边缘计算中的微控制器单元（MCUs）设计的硬件感知零样本神经架构搜索框架。与之前的方法相比，MicroNAS在搜索效率和MCU推理速度方面都取得了显著的提升，同时保持了相似的精度水平。

    

    神经架构搜索 (NAS) 可以有效地发现新的卷积神经网络 (CNN) 架构，特别是用于精度优化。然而，之前的方法通常需要在超级网络上进行资源密集型训练或广泛的架构评估，限制了实际应用。为了解决这些挑战，我们提出了MicroNAS，这是一个专为边缘计算中的微控制器单元 (MCUs) 设计的硬件感知零样本NAS框架。MicroNAS在搜索过程中考虑了目标硬件优化性能，利用专门的性能指标来识别最佳的神经架构，而不需要高计算成本。与之前的工作相比，MicroNAS在搜索效率方面提高了1104倍，并且发现了在维持相似精度的情况下MCU推理速度提高了3.23倍的模型。

    Neural Architecture Search (NAS) effectively discovers new Convolutional Neural Network (CNN) architectures, particularly for accuracy optimization. However, prior approaches often require resource-intensive training on super networks or extensive architecture evaluations, limiting practical applications. To address these challenges, we propose MicroNAS, a hardware-aware zero-shot NAS framework designed for microcontroller units (MCUs) in edge computing. MicroNAS considers target hardware optimality during the search, utilizing specialized performance indicators to identify optimal neural architectures without high computational costs. Compared to previous works, MicroNAS achieves up to 1104x improvement in search efficiency and discovers models with over 3.23x faster MCU inference while maintaining similar accuracy
    
[^35]: 流式多语言ASR中针对边缘语种的高效适配器微调

    Efficient Adapter Finetuning for Tail Languages in Streaming Multilingual ASR. (arXiv:2401.08992v1 [cs.CL])

    [http://arxiv.org/abs/2401.08992](http://arxiv.org/abs/2401.08992)

    本研究通过在流式多语言ASR中采用简单而有效的适配器微调方法，提供了对边缘语种的支持。适配器仅占每种语言的模型的0.4%。该方法在级联的Conformer转录器框架下，通过教师伪标签增强了模型性能。

    

    在流式多语言情景中，人们通常希望使用端到端ASR模型，因为这样更容易部署，并且可以从预训练的语音模型中受益，例如强大的基础模型。与此同时，不同语言的异质性和数据丰富度的不平衡可能导致性能下降，从而在训练过程中不同语言的性能出现异步峰值，尤其是对于边缘语种。有时，数据本身甚至可能因为加强的隐私保护而不可用。现有的方法往往倾向于显著增加模型大小或学习语言特定的解码器来单独适应每种语言。本研究中，我们探索了一种简单且有效的Language-Dependent Adapter (LDA)微调方法，该方法运用了级联的Conformer转录器框架，通过教师伪标签增强了对流式多语言ASR中的边缘语种的支持。适配器仅占每种语言的完整模型的0.4%。它被插入到冻结的基础模型中。

    The end-to-end ASR model is often desired in the streaming multilingual scenario since it is easier to deploy and can benefit from pre-trained speech models such as powerful foundation models. Meanwhile, the heterogeneous nature and imbalanced data abundance of different languages may cause performance degradation, leading to asynchronous peak performance for different languages during training, especially on tail ones. Sometimes even the data itself may become unavailable as a result of the enhanced privacy protection. Existing work tend to significantly increase the model size or learn language-specific decoders to accommodate each language separately. In this study, we explore simple yet effective Language-Dependent Adapter (LDA) finetuning under a cascaded Conformer transducer framework enhanced by teacher pseudo-labeling for tail languages in the streaming multilingual ASR. The adapter only accounts for 0.4% of the full model per language. It is plugged into the frozen foundation 
    
[^36]: 刚性蛋白质-蛋白质对接：基于等变椭抛接口预测的方法

    Rigid Protein-Protein Docking via Equivariant Elliptic-Paraboloid Interface Prediction. (arXiv:2401.08986v1 [cs.LG])

    [http://arxiv.org/abs/2401.08986](http://arxiv.org/abs/2401.08986)

    本文提出了一种名为ElliDock的新颖学习方法，通过预测椭抛体来表示蛋白质-蛋白质对接界面。实验评估表明，ElliDock在所有比较方法中拥有最快的推理时间，并且在当前的对接方法中具有强大的竞争力。

    

    刚性蛋白质-蛋白质对接的研究在药物设计和蛋白质工程等各种任务中起着重要作用。最近，已经提出了几种基于学习的方法来进行对接任务，相比计算方法，这些方法表现出更快的对接速度。在本文中，我们提出了一种名为ElliDock的新颖学习方法，该方法通过预测椭抛体来表示蛋白质-蛋白质对接界面。具体来说，我们的模型分别为两个输入蛋白质估计椭抛体接口，并通过使两个接口重合来获得对接的旋转平移变换。通过其设计，ElliDock在蛋白质的任意旋转/平移下都是等变的，这是确保对接过程的泛化性的必要性质。实验评估表明，ElliDock在所有比较方法中拥有最快的推理时间，并且在当前的对接方法中具有强大的竞争力。

    The study of rigid protein-protein docking plays an essential role in a variety of tasks such as drug design and protein engineering. Recently, several learning-based methods have been proposed for the task, exhibiting much faster docking speed than those computational methods. In this paper, we propose a novel learning-based method called ElliDock, which predicts an elliptic paraboloid to represent the protein-protein docking interface. To be specific, our model estimates elliptic paraboloid interfaces for the two input proteins respectively, and obtains the roto-translation transformation for docking by making two interfaces coincide. By its design, ElliDock is independently equivariant with respect to arbitrary rotations/translations of the proteins, which is an indispensable property to ensure the generalization of the docking process. Experimental evaluations show that ElliDock achieves the fastest inference time among all compared methods and is strongly competitive with current 
    
[^37]: 基于GAN的数据污染框架对抗纵向联合学习中的异常检测

    A GAN-based data poisoning framework against anomaly detection in vertical federated learning. (arXiv:2401.08984v1 [cs.LG])

    [http://arxiv.org/abs/2401.08984](http://arxiv.org/abs/2401.08984)

    这篇论文介绍了一种基于GAN的数据污染框架（P-GAN），用于对抗纵向联合学习中的异常检测。通过使用半监督学习训练一个替代目标模型，并使用GAN生成对抗性扰动来降低模型性能，最后通过深度自编码器开发的异常检测算法提供了强大的防御机制。

    

    在纵向联合学习 (VFL) 中，商业实体在保护数据隐私的同时协作训练模型。然而，恶意参与者的污染攻击可能会降低这个协作模型的性能。实现污染攻击的主要挑战是缺乏对服务器端顶层模型的访问，使得恶意参与者没有明确的目标模型。为了应对这个挑战，我们引入了一种创新的端到端污染框架 P-GAN。具体而言，恶意参与者最初采用半监督学习训练一个替代目标模型。随后，该参与者采用基于GAN的方法产生对抗性扰动，以降低替代目标模型的性能。最后，生成器被获得并针对VFL污染进行了改进。此外，我们还基于深度自编码器 (DAE) 开发了一种异常检测算法，为VFL场景提供了强大的防御机制。通过大量实验，我们证明了…

    In vertical federated learning (VFL), commercial entities collaboratively train a model while preserving data privacy. However, a malicious participant's poisoning attack may degrade the performance of this collaborative model. The main challenge in achieving the poisoning attack is the absence of access to the server-side top model, leaving the malicious participant without a clear target model. To address this challenge, we introduce an innovative end-to-end poisoning framework P-GAN. Specifically, the malicious participant initially employs semi-supervised learning to train a surrogate target model. Subsequently, this participant employs a GAN-based method to produce adversarial perturbations to degrade the surrogate target model's performance. Finally, the generator is obtained and tailored for VFL poisoning. Besides, we develop an anomaly detection algorithm based on a deep auto-encoder (DAE), offering a robust defense mechanism to VFL scenarios. Through extensive experiments, we 
    
[^38]: FedLoGe: 长尾数据下的本地和通用联邦学习

    FedLoGe: Joint Local and Generic Federated Learning under Long-tailed Data. (arXiv:2401.08977v1 [cs.LG])

    [http://arxiv.org/abs/2401.08977](http://arxiv.org/abs/2401.08977)

    本文介绍了一种名为FedLoGe的方法，它通过在神经网络崩溃框架中集成表示学习和分类器对齐来提高区域和全局模型的性能，解决了在联邦长尾学习中忽视本地级别性能的问题。

    

    联邦长尾学习（Fed-LT）是一种在去中心化的本地客户端收集的数据呈现全球普遍存在的长尾分布的范例，近年来引起了相当大的关注。在Fed-LT的背景下，现有研究主要集中于解决数据不平衡问题，以提高通用全局模型的效能，而忽视了本地级别的性能。相比之下，常规的个性化联邦学习（pFL）技术主要是在平衡的全局数据分布的假设下，优化个性化的本地模型。本文提出了一种名为FedLoGe的方法，在Fed-LT中通过在神经网络崩溃框架中集成表示学习和分类器对齐，提高本地和通用模型的性能。我们的研究结果揭示了使用共享骨干作为基础框架的可行性。

    Federated Long-Tailed Learning (Fed-LT), a paradigm wherein data collected from decentralized local clients manifests a globally prevalent long-tailed distribution, has garnered considerable attention in recent times. In the context of Fed-LT, existing works have predominantly centered on addressing the data imbalance issue to enhance the efficacy of the generic global model while neglecting the performance at the local level. In contrast, conventional Personalized Federated Learning (pFL) techniques are primarily devised to optimize personalized local models under the presumption of a balanced global data distribution. This paper introduces an approach termed Federated Local and Generic Model Training in Fed-LT (FedLoGe), which enhances both local and generic model performance through the integration of representation learning and classifier alignment within a neural collapse framework. Our investigation reveals the feasibility of employing a shared backbone as a foundational framewor
    
[^39]: ACT-GAN：基于ACT块的生成对抗网络实现的无线电图构建

    ACT-GAN: Radio map construction based on generative adversarial networks with ACT blocks. (arXiv:2401.08976v1 [cs.LG])

    [http://arxiv.org/abs/2401.08976](http://arxiv.org/abs/2401.08976)

    本论文提出了一种基于生成对抗网络的无线电图构建方法，通过应用聚合上下文转换块、卷积块注意力模块和转置卷积块于生成器，有效提高了无线电图的重建准确性和局部纹理，并在多个场景中展示了其性能优势。

    

    无线电图作为电磁空间特征的可视化表示，在评估无线通信网络和无线电监测覆盖方面起着关键作用。本文针对当前无线电图构建存在的低准确性问题，提出了一种基于生成对抗网络（GAN）的新型无线电图构建方法，其中应用了聚合上下文转换（AOT）块、卷积块注意力模块（CBAM）和转置卷积（T-Conv）块于生成器，并命名为ACT-GAN。该方法显著提高了无线电图的重建准确性和局部纹理。在三种不同场景下，ACT-GAN的性能得到了验证。实验结果表明，在没有稀疏离散观测的场景中，与现有先进模型相比，所提出的方法将均方根误差（RMSE）降低了14.6%。

    The radio map, serving as a visual representation of electromagnetic spatial characteristics, plays a pivotal role in assessment of wireless communication networks and radio monitoring coverage. Addressing the issue of low accuracy existing in the current radio map construction, this paper presents a novel radio map construction method based on generative adversarial network (GAN) in which the Aggregated Contextual-Transformation (AOT) block, Convolutional Block Attention Module (CBAM), and Transposed Convolution (T-Conv) block are applied to the generator, and we name it as ACT-GAN. It significantly improves the reconstruction accuracy and local texture of the radio maps. The performance of ACT-GAN across three different scenarios is demonstrated. Experiment results reveal that in the scenario without sparse discrete observations, the proposed method reduces the root mean square error (RMSE) by 14.6% in comparison to the state-of-the-art models. In the scenario with sparse discrete ob
    
[^40]: DOO-RE: 一个会议室环境传感器数据集用于活动识别

    DOO-RE: A dataset of ambient sensors in a meeting room for activity recognition. (arXiv:2401.08962v1 [cs.HC])

    [http://arxiv.org/abs/2401.08962](http://arxiv.org/abs/2401.08962)

    DOO-RE是一个来自会议室环境传感器的数据集，用于活动识别。该数据集包含了来自不同类型环境传感器的数据流，通过交叉验证注释过程获得了9种活动类型的标签。

    

    随着物联网技术的进步，利用机器学习方法识别用户活动是提供各种智能服务给用户的一种有前景的方式。在真实世界中部署这样的服务需要具备质量高且具有隐私保护的数据。周围环境传感器的数据流非常适合这个需求。目前存在的环境传感器数据集只支持受限的私人空间，对于公共空间的数据集尚未被探索，尽管对此类研究越来越感兴趣。为了满足这一需求，我们构建了一个从装有环境传感器的会议室中收集的数据集，名为DOO-RE。该数据集包括来自各种环境传感器类型(如声音和投影仪)的数据流。每个传感器数据流都被划分为活动单元，并通过交叉验证注释过程，多个注释者提供了活动标签以提高注释质量。最终我们得到了9种类型的活动。据我们所知，DOO-RE是第一个支持公共空间的数据集。

    With the advancement of IoT technology, recognizing user activities with machine learning methods is a promising way to provide various smart services to users. High-quality data with privacy protection is essential for deploying such services in the real world. Data streams from surrounding ambient sensors are well suited to the requirement. Existing ambient sensor datasets only support constrained private spaces and those for public spaces have yet to be explored despite growing interest in research on them. To meet this need, we build a dataset collected from a meeting room equipped with ambient sensors. The dataset, DOO-RE, includes data streams from various ambient sensor types such as Sound and Projector. Each sensor data stream is segmented into activity units and multiple annotators provide activity labels through a cross-validation annotation process to improve annotation quality. We finally obtain 9 types of activities. To our best knowledge, DOO-RE is the first dataset to su
    
[^41]: 级联强化学习

    Cascading Reinforcement Learning. (arXiv:2401.08961v1 [cs.LG])

    [http://arxiv.org/abs/2401.08961](http://arxiv.org/abs/2401.08961)

    本文提出了一个广义的级联强化学习框架，考虑了用户状态和状态转换对决策的影响，在级联强化学习中，我们需要选择不仅具有较大吸引概率的项目，还要选择能够导致良好后继状态的项目。

    

    最近几年，级联赌博机在推荐系统和在线广告中应用广泛。在级联赌博机模型中，每个时刻，一个代理人从一组具有未知吸引概率的项目中推荐一个有序的项目子集（称为项目列表）。然后，用户检查列表，并点击第一个有吸引力的项目（如果有的话），之后，代理收到一个奖励。代理的目标是最大化预期的累积奖励。然而，以往的级联赌博机文献忽略了用户状态（例如历史行为）对推荐的影响以及会话进行过程中状态的变化。受此事实的启发，我们提出了一个广义的级联强化学习框架，考虑了用户状态和状态转换对决策的影响。在级联强化学习中，我们需要选择不仅具有较大吸引概率的项目，还要选择能够导致良好后继状态的项目。

    Cascading bandits have gained popularity in recent years due to their applicability to recommendation systems and online advertising. In the cascading bandit model, at each timestep, an agent recommends an ordered subset of items (called an item list) from a pool of items, each associated with an unknown attraction probability. Then, the user examines the list, and clicks the first attractive item (if any), and after that, the agent receives a reward. The goal of the agent is to maximize the expected cumulative reward. However, the prior literature on cascading bandits ignores the influences of user states (e.g., historical behaviors) on recommendations and the change of states as the session proceeds. Motivated by this fact, we propose a generalized cascading RL framework, which considers the impact of user states and state transition into decisions. In cascading RL, we need to select items not only with large attraction probabilities but also leading to good successor states. This im
    
[^42]: 面向带有人类反馈的排名策略的离策略强化学习

    Towards Off-Policy Reinforcement Learning for Ranking Policies with Human Feedback. (arXiv:2401.08959v1 [cs.LG])

    [http://arxiv.org/abs/2401.08959](http://arxiv.org/abs/2401.08959)

    本文提出了一种新的离策略价值排名（VR）算法，通过统一的期望最大化（EM）框架，在不需要在线交互的情况下最大化用户的长期回报和优化排名指标，以提高样本效率。

    

    概率学习排名（LTR）一直是优化排名指标的主要方法，但无法最大化长期回报。提出了强化学习模型来将推荐问题形式化为序贯决策问题，以最大化用户的长期回报，但在准确性方面与LTR方法相比仍然存在不足，主要原因是缺乏在线交互和排名特性。本文提出了一种新的离策略价值排名（VR）算法，可以在统一的期望最大化（EM）框架下同时最大化用户的长期回报和优化排名指标，从而提高样本效率。我们在理论上和实验证明了EM过程引导学习策略享受未来回报和排名指标融合的好处，而无需进行任何在线交互。大量的离线和在线实验证明了我们方法的有效性。

    Probabilistic learning to rank (LTR) has been the dominating approach for optimizing the ranking metric, but cannot maximize long-term rewards. Reinforcement learning models have been proposed to maximize user long-term rewards by formulating the recommendation as a sequential decision-making problem, but could only achieve inferior accuracy compared to LTR counterparts, primarily due to the lack of online interactions and the characteristics of ranking. In this paper, we propose a new off-policy value ranking (VR) algorithm that can simultaneously maximize user long-term rewards and optimize the ranking metric offline for improved sample efficiency in a unified Expectation-Maximization (EM) framework. We theoretically and empirically show that the EM process guides the leaned policy to enjoy the benefit of integration of the future reward and ranking metric, and learn without any online interactions. Extensive offline and online experiments demonstrate the effectiveness of our methods
    
[^43]: AntiPhishStack：基于LSTM的堆叠泛化模型用于优化网络钓鱼URL的检测

    AntiPhishStack: LSTM-based Stacked Generalization Model for Optimized Phishing URLs Detection. (arXiv:2401.08947v1 [cs.CR])

    [http://arxiv.org/abs/2401.08947](http://arxiv.org/abs/2401.08947)

    本文介绍了一种名为AntiPhishStack的LSTM-based堆叠泛化模型，用于优化网络钓鱼URL的检测。该模型通过对URL和字符级TF-IDF特征进行对称学习，提高了对新型网络钓鱼威胁的应对能力，并采用对抗性训练策略增加鲁棒性和对抗刚性网络钓鱼攻击。

    

    革命性的在线网络服务的不断依赖引入了更高的安全风险，尽管有广泛的安全措施，网络钓鱼仍然带来持续的挑战。传统的基于机器学习和手动特征的网络钓鱼系统在应对不断变化的策略上很困难。深度学习的最新进展为解决新型网络钓鱼和恶意URL挑战提供了有希望的途径。本文介绍了一种名为AntiPhishStack的两阶段堆叠泛化模型，旨在检测网络钓鱼网站。该模型对URL和字符级TF-IDF特征进行对称学习，增强了对新型网络钓鱼威胁的应对能力。第一阶段在基本的机器学习分类器上训练特征，采用K折交叉验证进行均值预测以提高鲁棒性。第二阶段采用两层堆叠LSTM网络，配合五个自适应优化器进行动态编译，确保在这些特征上获得出色的预测性能。此外，该模型使用对抗性训练策略来增加鲁棒性和对抗刚性网络钓鱼攻击。

    The escalating reliance on revolutionary online web services has introduced heightened security risks, with persistent challenges posed by phishing despite extensive security measures. Traditional phishing systems, reliant on machine learning and manual features, struggle with evolving tactics. Recent advances in deep learning offer promising avenues for tackling novel phishing challenges and malicious URLs. This paper introduces a two-phase stack generalized model named AntiPhishStack, designed to detect phishing sites. The model leverages the learning of URLs and character-level TF-IDF features symmetrically, enhancing its ability to combat emerging phishing threats. In Phase I, features are trained on a base machine learning classifier, employing K-fold cross-validation for robust mean prediction. Phase II employs a two-layered stacked-based LSTM network with five adaptive optimizers for dynamic compilation, ensuring premier prediction on these features. Additionally, the symmetrica
    
[^44]: CEL：通过弹性权重整合利用领域自适应来进行疾病爆发预测的持续学习模型

    CEL: A Continual Learning Model for Disease Outbreak Prediction by Leveraging Domain Adaptation via Elastic Weight Consolidation. (arXiv:2401.08940v1 [cs.LG])

    [http://arxiv.org/abs/2401.08940](http://arxiv.org/abs/2401.08940)

    本研究提出了一种名为CEL的模型，通过利用领域自适应和弹性权重整合，实现了对疾病爆发预测的持续学习。该模型通过惩罚重要参数的改变来缓解灾难性遗忘现象，并在多个疾病上取得了比其他模型更好的性能。

    

    连续学习是模型在学习过程中不忘记之前知识并能适应新数据的能力，在疾病爆发预测等动态领域尤为重要。深度神经网络，如LSTM，由于灾难性遗忘而容易出错。本研究通过弹性权重整合（EWC）利用领域自适应引入了一种新颖的CEL模型进行持续学习。该模型旨在缓解领域增量设置中的灾难性遗忘现象。利用EWC构建Fisher信息矩阵（FIM）以开发出一个惩罚对重要参数即重要先前知识的改变的正则化项。在评估和重新评估中，CEL的性能使用不同指标在三种不同的疾病（流感，痘疹和麻疹）上得到了很高的R-squared值，胜过其他最先进的模型，在多个上下文中表明CEL对增量数据具有适应能力。

    Continual learning, the ability of a model to learn over time without forgetting previous knowledge and, therefore, be adaptive to new data, is paramount in dynamic fields such as disease outbreak prediction. Deep neural networks, i.e., LSTM, are prone to error due to catastrophic forgetting. This study introduces a novel CEL model for continual learning by leveraging domain adaptation via Elastic Weight Consolidation (EWC). This model aims to mitigate the catastrophic forgetting phenomenon in a domain incremental setting. The Fisher Information Matrix (FIM) is constructed with EWC to develop a regularization term that penalizes changes to important parameters, namely, the important previous knowledge. CEL's performance is evaluated on three distinct diseases, Influenza, Mpox, and Measles, with different metrics. The high R-squared values during evaluation and reevaluation outperform the other state-of-the-art models in several contexts, indicating that CEL adapts to incremental data w
    
[^45]: 使用基础模型设计学习环境的 DeLF

    DeLF: Designing Learning Environments with Foundation Models. (arXiv:2401.08936v1 [cs.AI])

    [http://arxiv.org/abs/2401.08936](http://arxiv.org/abs/2401.08936)

    DeLF是一种利用大型语言模型设计学习环境的方法，可以解决在实践中应用强化学习的困难。通过测试，证明DeLF可以为不同的学习环境获得可执行的代码。

    

    强化学习（RL）为基本的顺序决策问题提供了一种能力强大且直观的结构。尽管取得了令人瞩目的突破，但在许多简单应用中实际应用RL仍然很困难。在本文中，我们通过引入一种用于为给定的、用户预期的应用设计RL环境组件的方法来解决这个问题。我们提供了RL组件设计问题的初始形式化，重点是设计观察和动作空间的良好表示。我们提出了一种名为DeLF：使用基础模型设计学习环境的方法，该方法利用大型语言模型来设计和编码用户预期的学习场景。通过在四个不同的学习环境上测试我们的方法，我们证明了DeLF可以为相应的RL问题获得可执行的环境代码。

    Reinforcement learning (RL) offers a capable and intuitive structure for the fundamental sequential decision-making problem. Despite impressive breakthroughs, it can still be difficult to employ RL in practice in many simple applications. In this paper, we try to address this issue by introducing a method for designing the components of the RL environment for a given, user-intended application. We provide an initial formalization for the problem of RL component design, that concentrates on designing a good representation for observation and action space. We propose a method named DeLF: Designing Learning Environments with Foundation Models, that employs large language models to design and codify the user's intended learning scenario. By testing our method on four different learning environments, we demonstrate that DeLF can obtain executable environment codes for the corresponding RL problems.
    
[^46]: 部分音标化：一种上下文对比推理方法

    Partial Diacritization: A Context-Contrastive Inference Approach. (arXiv:2401.08919v1 [cs.CL])

    [http://arxiv.org/abs/2401.08919](http://arxiv.org/abs/2401.08919)

    部分音标化是选择标记部分字符来提高阅读可读性和准确性的新方法。上下文对比的部分音标化（CCPD）集成了现有的阿拉伯音标化系统，并通过衡量部分音标化的新指标来判断需要标记哪些字符。

    

    音标化在提高阿拉伯文本可读性和消除歧义方面起着关键作用。目前的努力主要集中在标记每个符合条件的字符（全音标化）。相比之下，部分音标化（PD）是选择标记子集以在必要时提供帮助。研究表明，过多的音标符号会妨碍熟练读者，降低阅读速度和准确性。我们进行了一项行为实验，并显示出部分标记的文本通常比完全标记的文本更容易阅读，有时甚至比纯文本更容易。在这种情况下，我们介绍了上下文对比的部分音标化（CCPD）-一种与现有阿拉伯音标化系统无缝集成的新方法。CCPD对每个单词进行两次处理，一次有上下文，一次没有，并且只对两次推理之间存在差异的字符进行音标化。此外，我们还引入了衡量部分音标化的新指标。

    Diacritization plays a pivotal role in improving readability and disambiguating the meaning of Arabic texts. Efforts have so far focused on marking every eligible character (Full Diacritization). Comparatively overlooked, Partial Diacritzation (PD) is the selection of a subset of characters to be marked to aid comprehension where needed. Research has indicated that excessive diacritic marks can hinder skilled readers--reducing reading speed and accuracy. We conduct a behavioral experiment and show that partially marked text is often easier to read than fully marked text, and sometimes easier than plain text. In this light, we introduce Context-Contrastive Partial Diacritization (CCPD)--a novel approach to PD which integrates seamlessly with existing Arabic diacritization systems. CCPD processes each word twice, once with context and once without, and diacritizes only the characters with disparities between the two inferences. Further, we introduce novel indicators for measuring partial
    
[^47]: 无监督准确性估计下分布偏移的梯度特征化研究

    Characterising Gradients for Unsupervised Accuracy Estimation under Distribution Shift. (arXiv:2401.08909v1 [cs.LG])

    [http://arxiv.org/abs/2401.08909](http://arxiv.org/abs/2401.08909)

    本文研究了在分布偏移下，利用梯度信息对真实测试准确性进行预测的方法。通过分析分类层梯度范数，我们发现在无法泛化到测试数据集时，调整模型以获得更大的梯度范数是有效的。

    

    在变化的测试环境下，无法访问真实测试标签的情况下估计测试准确性是机器学习算法安全部署中一个具有挑战性但极其重要的问题。现有的方法依赖于神经网络的输出或提取特征的信息来建立与真实测试准确性相关的估计分数。本文通过实证和理论研究探讨了梯度信息如何在分布偏移下对真实测试准确性进行预测。具体而言，我们使用从经过一次梯度步长的交叉熵损失函数后反向传播的分类层梯度范数来进行研究。我们的关键思想是，在模型在分布偏移下无法泛化到测试数据集时，应当调整模型以获得更大的梯度范数。我们提供理论见解，突出了这种方法的主要要素。

    Estimating test accuracy without access to the ground-truth test labels under varying test environments is a challenging, yet extremely important problem in the safe deployment of machine learning algorithms. Existing works rely on the information from either the outputs or the extracted features of neural networks to formulate an estimation score correlating with the ground-truth test accuracy. In this paper, we investigate--both empirically and theoretically--how the information provided by the gradients can be predictive of the ground-truth test accuracy even under a distribution shift. Specifically, we use the norm of classification-layer gradients, backpropagated from the cross-entropy loss after only one gradient step over test data. Our key idea is that the model should be adjusted with a higher magnitude of gradients when it does not generalize to the test dataset with a distribution shift. We provide theoretical insights highlighting the main ingredients of such an approach en
    
[^48]: 驯服LLaMaS：将LLMs用作操作系统模块

    Herding LLaMaS: Using LLMs as an OS Module. (arXiv:2401.08908v1 [cs.OS])

    [http://arxiv.org/abs/2401.08908](http://arxiv.org/abs/2401.08908)

    LLaMaS使用大型语言模型（LLMs）从新设备的文本描述中提取有用的特征，并利用这些特征在运行时做出操作系统决策，为新设备提供高性能的操作系统。

    

    随着新的存储技术和计算设备的出现，计算机系统变得越来越异构化。 GPU和CPU并用已经变得普遍，而CXL有望成为云系统的主要支柱。操作系统负责管理这些硬件资源，每次发布新设备时都需要进行修改。多年的研究和开发投入到调整操作系统以实现高性能与每个新的异构设备。随着近年来内存技术和领域特定加速器的爆发，有一个可以轻松为新设备提供高性能的操作系统将是有益的。我们提出了LLaMaS，它可以轻松适应新设备。LLaMaS使用大型语言模型（LLMs）从新设备的文本描述中提取有用的特征，并利用这些特征在运行时做出操作系统决策。为LLaMaS提供对新设备的支持就像描述系统一样简单。

    Computer systems are becoming increasingly heterogeneous with the emergence of new memory technologies and compute devices. GPUs alongside CPUs have become commonplace and CXL is poised to be a mainstay of cloud systems. The operating system is responsible for managing these hardware resources, requiring modification every time a new device is released. Years of research and development are sunk into tuning the OS for high performance with each new heterogeneous device. With the recent explosion in memory technologies and domain-specific accelerators, it would be beneficial to have an OS that could provide high performance for new devices without significant effort.  We propose LLaMaS which can adapt to new devices easily. LLaMaS uses Large Language Models (LLMs) to extract the useful features of new devices from their textual description and uses these features to make operating system decisions at runtime. Adding support to LLaMaS for a new device is as simple as describing the syste
    
[^49]: PPR: 在维持冒名顶替攻击的同时增强躲避攻击对人脸识别系统的影响

    PPR: Enhancing Dodging Attacks while Maintaining Impersonation Attacks on Face Recognition Systems. (arXiv:2401.08903v1 [cs.CV])

    [http://arxiv.org/abs/2401.08903](http://arxiv.org/abs/2401.08903)

    本文提出了一种名为PPR的新型攻击方法，旨在增强躲避攻击的性能同时避免冒名顶替攻击的降级。该方法利用对抗样本修剪，并通过嵌入对抗扰动来增强对抗人脸样本的躲避性能。

    

    人脸识别系统上的对抗攻击可以分为两种类型：冒名顶替攻击和躲避攻击。我们观察到，在黑盒设置中成功进行冒名顶替攻击并不一定能保证在人脸识别系统上成功进行躲避攻击。本文提出了一种名为预训练修剪恢复攻击（PPR）的新型攻击方法，旨在增强躲避攻击的性能同时避免冒名顶替攻击的降级。我们的方法利用对抗样本修剪，可以将一部分对抗扰动设为零，并倾向于保持攻击性能。通过利用对抗样本修剪，我们可以修剪预训练的对抗样本，并有选择性地释放某些对抗扰动。然后，我们将对抗扰动嵌入到修剪区域，从而增强对抗人脸样本的躲避性能。通过实验证明了我们提出的攻击方法的有效性。

    Adversarial Attacks on Face Recognition (FR) encompass two types: impersonation attacks and evasion attacks. We observe that achieving a successful impersonation attack on FR does not necessarily ensure a successful dodging attack on FR in the black-box setting. Introducing a novel attack method named Pre-training Pruning Restoration Attack (PPR), we aim to enhance the performance of dodging attacks whilst avoiding the degradation of impersonation attacks. Our method employs adversarial example pruning, enabling a portion of adversarial perturbations to be set to zero, while tending to maintain the attack performance. By utilizing adversarial example pruning, we can prune the pre-trained adversarial examples and selectively free up certain adversarial perturbations. Thereafter, we embed adversarial perturbations in the pruned area, which enhances the dodging performance of the adversarial face examples. The effectiveness of our proposed attack method is demonstrated through our experim
    
[^50]: 类似但更快：音乐音频嵌入中的节奏操作用于节奏预测和搜索

    Similar but Faster: Manipulation of Tempo in Music Audio Embeddings for Tempo Prediction and Search. (arXiv:2401.08902v1 [cs.SD])

    [http://arxiv.org/abs/2401.08902](http://arxiv.org/abs/2401.08902)

    通过在音频嵌入中操作节奏，本研究实现了在维持其他属性不变的情况下，检索出在节奏上相似但在其他方面不同的音乐曲目。

    

    音频嵌入使得音频文件的相似性能够进行大规模比较，用于搜索和推荐等应用。由于音频相似性的主观性，设计能够回答音频是否相似以及相似的方式（例如，与节奏、情绪或流派相关）的系统可能更加理想。之前的研究已经提出了能够对特定属性进行加权以在后续任务中强调这些属性的分离嵌入空间。然而，关于这些子空间的独立性以及对其进行操作以检索在特定方式下相似但不同的曲目的研究尚未进行。在这里，我们探索了在嵌入空间中操作节奏作为这个目标的一个案例研究。我们提出了节奏转换函数，可以在现有嵌入空间中高效地操作节奏，同时保持流派等其他属性。

    Audio embeddings enable large scale comparisons of the similarity of audio files for applications such as search and recommendation. Due to the subjectivity of audio similarity, it can be desirable to design systems that answer not only whether audio is similar, but similar in what way (e.g., wrt. tempo, mood or genre). Previous works have proposed disentangled embedding spaces where subspaces representing specific, yet possibly correlated, attributes can be weighted to emphasize those attributes in downstream tasks. However, no research has been conducted into the independence of these subspaces, nor their manipulation, in order to retrieve tracks that are similar but different in a specific way. Here, we explore the manipulation of tempo in embedding spaces as a case-study towards this goal. We propose tempo translation functions that allow for efficient manipulation of tempo within a pre-existing embedding space whilst maintaining other properties such as genre. As this translation 
    
[^51]: 桥接状态和历史表示：理解自预测强化学习

    Bridging State and History Representations: Understanding Self-Predictive RL. (arXiv:2401.08898v1 [cs.LG])

    [http://arxiv.org/abs/2401.08898](http://arxiv.org/abs/2401.08898)

    本论文研究了深度强化学习中状态和历史表示间的关系，发现了这些方法和框架实际上都基于自预测抽象的共同思想，并提供了理论洞见和简化算法来学习自预测表示。

    

    表示是所有深度强化学习方法的核心，适用于马尔可夫决策过程（MDP）和部分可观察的马尔可夫决策过程（POMDP）。许多表示学习方法和理论框架被开发用于理解什么构成了有效的表示。然而，这些方法之间的关系和它们之间的共同属性仍然不清楚。在本文中，我们展示了许多看似不同的状态和历史抽象方法和框架实际上基于自预测抽象的共同思想。此外，我们提供了关于广泛采用的目标和优化（如停梯度技术）在学习自预测表示中的理论洞见。这些发现共同产生了一种简化的算法，用于学习状态和历史的自预测表示。我们通过将我们的算法应用于标准MDP、带有dist的MDP进行验证。

    Representations are at the core of all deep reinforcement learning (RL) methods for both Markov decision processes (MDPs) and partially observable Markov decision processes (POMDPs). Many representation learning methods and theoretical frameworks have been developed to understand what constitutes an effective representation. However, the relationships between these methods and the shared properties among them remain unclear. In this paper, we show that many of these seemingly distinct methods and frameworks for state and history abstractions are, in fact, based on a common idea of self-predictive abstraction. Furthermore, we provide theoretical insights into the widely adopted objectives and optimization, such as the stop-gradient technique, in learning self-predictive representations. These findings together yield a minimalist algorithm to learn self-predictive representations for states and histories. We validate our theories by applying our algorithm to standard MDPs, MDPs with dist
    
[^52]: CFASL：用于变分自编码器中的解缠学习的复合因子对齐对称学习

    CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational AutoEncoder. (arXiv:2401.08897v1 [cs.LG])

    [http://arxiv.org/abs/2401.08897](http://arxiv.org/abs/2401.08897)

    CFASL是一种用于解缠学习的新方法，它将对称性学习与VAE集成，无需任何数据集因子信息的先验知识，具有三个新特征：对齐潜在向量维度到可学习对称代码簿中的对称性，学习复合对称性来表达未知因素的变化，以及引入群等变编码器和解码器来训练VAE。

    

    输入和潜在向量的对称性为VAE中的解缠学习提供了宝贵的见解。然而，只有少数几篇论文提出了一种无监督方法，甚至这些方法在训练数据中也需要已知的因子信息。我们提出了一种新的方法，Composite Factor-Aligned Symmetry Learning (CFASL)，将其集成到VAE中，用于学习基于对称性的解缠，无监督学习中不需要任何数据集因子信息的知识。CFASL包括三个用于学习基于对称性的解缠的新特征：1)注入归纳偏置，将潜在向量维度对齐到明确可学习的对称代码簿中的因子对齐对称性；2)学习一个复合对称性，通过学习代码簿中的因子对齐对称性，来表达两个随机样本之间的未知因素的变化；3)在训练VAE时，引入具有群等变编码器和解码器的两个条件。此外，我们提出了一种扩展的评估指标。

    Symmetries of input and latent vectors have provided valuable insights for disentanglement learning in VAEs.However, only a few works were proposed as an unsupervised method, and even these works require known factor information in training data. We propose a novel method, Composite Factor-Aligned Symmetry Learning (CFASL), which is integrated into VAEs for learning symmetry-based disentanglement in unsupervised learning without any knowledge of the dataset factor information.CFASL incorporates three novel features for learning symmetry-based disentanglement: 1) Injecting inductive bias to align latent vector dimensions to factor-aligned symmetries within an explicit learnable symmetry codebook 2) Learning a composite symmetry to express unknown factors change between two random samples by learning factor-aligned symmetries within the codebook 3) Inducing group equivariant encoder and decoder in training VAEs with the two conditions. In addition, we propose an extended evaluation metri
    
[^53]: cedar：可组合和优化的机器学习输入数据管道

    cedar: Composable and Optimized Machine Learning Input Data Pipelines. (arXiv:2401.08895v1 [cs.LG])

    [http://arxiv.org/abs/2401.08895](http://arxiv.org/abs/2401.08895)

    cedar是一个编程模型和框架，可以轻松构建、优化和执行机器学习输入数据管道。它提供了易于使用的编程接口和可组合运算符，支持任意ML框架和库。通过解决当前输入数据系统无法充分利用性能优化的问题，cedar提高了资源利用效率，满足了庞大数据量和高训练吞吐量的需求。

    

    输入数据管道是每个机器学习（ML）训练任务的重要组成部分。它负责读取大量的训练数据，使用复杂的变换处理样本批次，并以低延迟和高吞吐量将其加载到训练节点上。高性能的输入数据系统变得越来越关键，原因是数据量急剧增加和训练吞吐量的要求。然而，当前的输入数据系统无法充分利用关键的性能优化，导致资源利用效率极低的基础设施，或者更糟糕地，浪费昂贵的加速器。为了满足这些需求，我们提出了cedar，一个编程模型和框架，允许用户轻松构建、优化和执行输入数据管道。cedar提供了易于使用的编程接口，允许用户使用可组合运算符来定义支持任意ML框架和库的输入数据管道。

    The input data pipeline is an essential component of each machine learning (ML) training job. It is responsible for reading massive amounts of training data, processing batches of samples using complex of transformations, and loading them onto training nodes at low latency and high throughput. Performant input data systems are becoming increasingly critical, driven by skyrocketing data volumes and training throughput demands. Unfortunately, current input data systems cannot fully leverage key performance optimizations, resulting in hugely inefficient infrastructures that require significant resources -- or worse -- underutilize expensive accelerators.  To address these demands, we present cedar, a programming model and framework that allows users to easily build, optimize, and execute input data pipelines. cedar presents an easy-to-use programming interface, allowing users to define input data pipelines using composable operators that support arbitrary ML frameworks and libraries. Mean
    
[^54]: MADA: 通过超梯度下降的元适应优化器

    MADA: Meta-Adaptive Optimizers through hyper-gradient Descent. (arXiv:2401.08893v1 [cs.LG])

    [http://arxiv.org/abs/2401.08893](http://arxiv.org/abs/2401.08893)

    MADA是一个统一的优化器框架，通过超梯度下降动态学习最适合的优化器。数值结果表明MADA在次优调整的超参数下是稳健的，并且在默认超参数下常常优于其他优化器。插值优化器可以改进收敛性能。

    

    自从Adam被引入以来，已经提出了几种用于深度学习的新型自适应优化器。这些优化器通常在某些任务上表现卓越，但可能无法在所有任务中均优于Adam。在这项工作中，我们引入了元适应优化器(MADA)，这是一个统一的优化器框架，可以概括多种已知的优化器，并在训练过程中动态学习最适合的优化器。MADA的关键思想是对优化器的空间进行参数化，并使用超梯度下降进行搜索。数值结果表明，MADA对于次优调整的超参数是稳健的，并且在默认超参数下常常优于Adam、Lion和Adan，甚至在优化超参数的情况下也是如此。我们还提出了AVGrad，它是AMSGrad的一个变种，在其中将最大操作符替换为平均操作符，并观察到它在MADA中的表现更好。最后，我们提供了收敛分析，以表明优化器的插值（特别是AVGrad和Adam）可以改进收敛性能。

    Since Adam was introduced, several novel adaptive optimizers for deep learning have been proposed. These optimizers typically excel in some tasks but may not outperform Adam uniformly across all tasks. In this work, we introduce Meta-Adaptive Optimizers (MADA), a unified optimizer framework that can generalize several known optimizers and dynamically learn the most suitable one during training. The key idea in MADA is to parameterize the space of optimizers and search through it using hyper-gradient descent. Numerical results suggest that MADA is robust against sub-optimally tuned hyper-parameters, and outperforms Adam, Lion, and Adan with their default hyper-parameters, often even with optimized hyper-parameters. We also propose AVGrad, a variant of AMSGrad where the maximum operator is replaced with averaging, and observe that it performs better within MADA. Finally, we provide a convergence analysis to show that interpolation of optimizers (specifically, AVGrad and Adam) can improve
    
[^55]: 动态估计作为完全自监督的二分类

    Tempo estimation as fully self-supervised binary classification. (arXiv:2401.08891v1 [cs.SD])

    [http://arxiv.org/abs/2401.08891](http://arxiv.org/abs/2401.08891)

    本文提出了一种完全自监督的方法，用于音乐音频中的全局节奏估计。该方法不依赖任何人工标记的数据，通过将任务重新定义为二分类问题，可以使用公开可用的音频数据源进行训练。

    

    本文解决了音乐音频中全局节奏估计的问题。考虑到标注节奏是耗时且需要一定的音乐专业知识，公开可用的数据源用于此任务的机器学习模型训练很少。为了解决这个问题，我们提出了一种完全自监督的方法，不依赖任何人工标记的数据。我们的方法基于通用（音乐）音频嵌入已经包含了各种属性的事实，包括关于节奏的信息，使其可以轻松地适应下游任务。虽然最近的自监督节奏估计的研究旨在学习一个特定节奏的表示，然后用于训练监督分类器，但我们将该任务重新定义为二分类问题，即预测目标曲目与参考曲目的节奏是否相同。虽然前者仍然需要有标签的训练数据用于最终的分类模型，我们的方法使用了一种自我监督的方法进行训练，并且可以使用公开可用的音频数据源。

    This paper addresses the problem of global tempo estimation in musical audio. Given that annotating tempo is time-consuming and requires certain musical expertise, few publicly available data sources exist to train machine learning models for this task. Towards alleviating this issue, we propose a fully self-supervised approach that does not rely on any human labeled data. Our method builds on the fact that generic (music) audio embeddings already encode a variety of properties, including information about tempo, making them easily adaptable for downstream tasks. While recent work in self-supervised tempo estimation aimed to learn a tempo specific representation that was subsequently used to train a supervised classifier, we reformulate the task into the binary classification problem of predicting whether a target track has the same or a different tempo compared to a reference. While the former still requires labeled training data for the final classification model, our approach uses a
    
[^56]: 对数据增强在对比学习音乐音频表示中的局部嵌入特性的影响

    On the Effect of Data-Augmentation on Local Embedding Properties in the Contrastive Learning of Music Audio Representations. (arXiv:2401.08889v1 [cs.SD])

    [http://arxiv.org/abs/2401.08889](http://arxiv.org/abs/2401.08889)

    本研究通过对比学习音乐数据集上的音频表示，发现通过适当的数据增强策略，可以减少一首曲目中均匀的音乐特性在嵌入空间中的本地化，并提高其他属性的本地化。这对于音乐搜索和推荐中的最近邻算法应用是重要的。

    

    音频嵌入是理解大量音乐目录的重要工具。通常，嵌入是根据它们在各种下游任务中的表现来评估的，然而少有研究调查嵌入空间本身的局部特性，而这些局部特性对于最近邻算法在音乐搜索和推荐中的应用至关重要。在这项工作中，我们展示了在通过对比学习学习音乐数据集上的音频表示时，通常在一首曲目中是均匀的音乐特性（如音调和速度）在所得到的嵌入空间的邻域的局部性中得到了体现。通过应用适当的数据增强策略，这些特性的本地化不仅可以减少，而且其他属性的本地化也会增加。例如，与非专业听众关系较小的音高和速度等特征的局部性可能会得到缓解，同时改善更显著特征的局部性。

    Audio embeddings are crucial tools in understanding large catalogs of music. Typically embeddings are evaluated on the basis of the performance they provide in a wide range of downstream tasks, however few studies have investigated the local properties of the embedding spaces themselves which are important in nearest neighbor algorithms, commonly used in music search and recommendation. In this work we show that when learning audio representations on music datasets via contrastive learning, musical properties that are typically homogeneous within a track (e.g., key and tempo) are reflected in the locality of neighborhoods in the resulting embedding space. By applying appropriate data augmentation strategies, localisation of such properties can not only be reduced but the localisation of other attributes is increased. For example, locality of features such as pitch and tempo that are less relevant to non-expert listeners, may be mitigated while improving the locality of more salient fea
    
[^57]: RiemannONets: 可解释的用于Riemann问题的神经算子

    RiemannONets: Interpretable Neural Operators for Riemann Problems. (arXiv:2401.08886v1 [cs.LG])

    [http://arxiv.org/abs/2401.08886](http://arxiv.org/abs/2401.08886)

    本文使用可解释的神经算子来解决高速流动中的Riemann问题，通过对DeepONet进行简单修改，在极端压力跳跃情况下获得了非常准确的解决方案。

    

    在数值分析中，如何为模拟具有强激波、稀疏化和接触间断的高速流动开发合适的表示一直是一个长期存在的问题。本文采用神经算子来解决在可压缩流中遇到的Riemann问题，其中包括极端压力跳跃（高达$10^{10}$的压力比）。特别地，在本研究中，我们首先考虑了DeepONet，它是在Lee和Shin的最新工作基础上进行的两阶段训练。在第一阶段，我们从主干网络中提取了一个基础并使其正交化，然后在第二阶段使用它来训练分支网络。这个对DeepONet的简单修改对其准确性、效率和鲁棒性有着深远影响，相比原始版本，它提供了非常准确的Riemann问题解决方案。此外，它还使我们能够在物理上对结果进行解释，因为层次化的数据驱动生成的基础反映了所有流动特征，否则将被引入。

    Developing the proper representations for simulating high-speed flows with strong shock waves, rarefactions, and contact discontinuities has been a long-standing question in numerical analysis. Herein, we employ neural operators to solve Riemann problems encountered in compressible flows for extreme pressure jumps (up to $10^{10}$ pressure ratio). In particular, we first consider the DeepONet that we train in a two-stage process, following the recent work of Lee and Shin, wherein the first stage, a basis is extracted from the trunk net, which is orthonormalized and subsequently is used in the second stage in training the branch net. This simple modification of DeepONet has a profound effect on its accuracy, efficiency, and robustness and leads to very accurate solutions to Riemann problems compared to the vanilla version. It also enables us to interpret the results physically as the hierarchical data-driven produced basis reflects all the flow features that would otherwise be introduce
    
[^58]: 评估用于AI辅助图像标注的符合预测集的效用

    Evaluating the Utility of Conformal Prediction Sets for AI-Advised Image Labeling. (arXiv:2401.08876v1 [cs.HC])

    [http://arxiv.org/abs/2401.08876](http://arxiv.org/abs/2401.08876)

    本研究评估了符合预测集在AI辅助图像标注中的效用，发现对于简单图像，预测集与Top-1和Top-k显示的准确性相当，但在标记分布外图像时特别有效，尤其是集合大小较小时。

    

    随着深度神经网络在高风险领域中越来越常见，它们的缺乏可解释性使得不确定性量化变得具有挑战性。我们研究了用于表示AI辅助决策中的不确定性的符合预测集的效果。通过一项大型预注册实验，我们比较了符合预测集和显示Top-1和Top-k预测在AI辅助图像标注中的效用。我们发现，对于简单的图像，预测集的准确性与Top-1和Top-k显示相当或稍低，但在标记分布外（OOD）图像时，尤其是当集合大小较小时，预测集在辅助人类标注方面表现出色。我们的结果在实践中强调了符合预测集的实际挑战，并提供了相关建议。

    As deep neural networks are more commonly deployed in high-stakes domains, their lack of interpretability makes uncertainty quantification challenging. We investigate the effects of presenting conformal prediction sets$\unicode{x2013}$a method for generating valid confidence sets in distribution-free uncertainty quantification$\unicode{x2013}$to express uncertainty in AI-advised decision-making. Through a large pre-registered experiment, we compare the utility of conformal prediction sets to displays of Top-1 and Top-k predictions for AI-advised image labeling. We find that the utility of prediction sets for accuracy varies with the difficulty of the task: while they result in accuracy on par with or less than Top-1 and Top-k displays for easy images, prediction sets excel at assisting humans in labeling out-of-distribution (OOD) images especially when the set size is small. Our results empirically pinpoint the practical challenges of conformal prediction sets and provide implications 
    
[^59]: DCRMTA: 无偏的多触点归因的因果表示

    DCRMTA: Unbiased Causal Representation for Multi-touch Attribution. (arXiv:2401.08875v1 [cs.LG])

    [http://arxiv.org/abs/2401.08875](http://arxiv.org/abs/2401.08875)

    DCRMTA提出了一种无偏的多触点归因方法，通过建立转化预测模型和构建对照触点序列来减轻偏差的影响。

    

    多触点归因（MTA）在实现对每个广告触点对于转化行为的贡献的公正估计方面起着关键作用，深刻影响预算分配和广告推荐。传统的多触点归因方法首先构建一个转化预测模型，通过历史数据学习触点序列和用户购买行为之间的内在关系。在此基础上，从原始序列子集中构建对照触点序列，并使用预测模型估计转化，从而计算广告贡献。这些方法的一个隐含假设是转化预测模型的无偏性。然而，由于用户偏好和互联网推荐机制（如过去的购物记录导致的广告推荐同质化）引起的混杂变量因素，转化中很容易产生偏差。

    Multi-touch attribution (MTA) currently plays a pivotal role in achieving a fair estimation of the contributions of each advertising touchpoint to-wards conversion behavior, deeply influencing budget allocation and advertising recommenda-tion. Traditional multi-touch attribution methods initially build a conversion prediction model, an-ticipating learning the inherent relationship be-tween touchpoint sequences and user purchasing behavior through historical data. Based on this, counterfactual touchpoint sequences are con-structed from the original sequence subset, and conversions are estimated using the prediction model, thus calculating advertising contributions. A covert assumption of these methods is the un-biased nature of conversion prediction models. However, due to confounding variables factors arising from user preferences and internet recom-mendation mechanisms such as homogenization of ad recommendations resulting from past shop-ping records, bias can easily occur in conversi
    
[^60]: MambaTab：一种处理表格数据的简单而有效的方法

    MambaTab: A Simple Yet Effective Approach for Handling Tabular Data. (arXiv:2401.08867v1 [cs.LG])

    [http://arxiv.org/abs/2401.08867](http://arxiv.org/abs/2401.08867)

    MambaTab是一种简单而有效的方法，用于处理表格数据，通过利用结构化状态空间模型（SSM）的特性，在少量参数和最小预处理的情况下，实现了超过最先进基线模型的性能。

    

    尽管图像和文本在机器学习中的应用越来越多，但表格数据仍然在各个领域中普遍存在。深度学习模型如卷积神经网络和transformers在处理表格数据上取得了很好的性能，但它们需要大量的数据预处理、调优和资源，限制了其可访问性和可扩展性。本研究基于结构化状态空间模型（SSM）开发了一种创新的方法，称为MambaTab，用于处理表格数据。SSM对于从具有长程依赖的数据中高效提取有效表示具有很强的能力。MambaTab利用了Mamba，一种新兴的SSM变体，在表格上进行端到端的有监督学习。与最先进的基线模型相比，MambaTab在多样化的基准数据集上经验验证，表现出卓越的性能，并且需要更少的参数和较少的预处理步骤。MambaTab的效率、可扩展性、普适性和预测性能使其成为一种轻量级的、即开即用的方法。

    Tabular data remains ubiquitous across domains despite growing use of images and texts for machine learning. While deep learning models like convolutional neural networks and transformers achieve strong performance on tabular data, they require extensive data preprocessing, tuning, and resources, limiting accessibility and scalability. This work develops an innovative approach based on a structured state-space model (SSM), MambaTab, for tabular data. SSMs have strong capabilities for efficiently extracting effective representations from data with long-range dependencies. MambaTab leverages Mamba, an emerging SSM variant, for end-to-end supervised learning on tables. Compared to state-of-the-art baselines, MambaTab delivers superior performance while requiring significantly fewer parameters and minimal preprocessing, as empirically validated on diverse benchmark datasets. MambaTab's efficiency, scalability, generalizability, and predictive gains signify it as a lightweight, "out-of-the-
    
[^61]: Intrinsic Dataset Properties对泛化能力的影响：揭示自然图像和医学图像之间的学习差异

    The Effect of Intrinsic Dataset Properties on Generalization: Unraveling Learning Differences Between Natural and Medical Images. (arXiv:2401.08865v1 [cs.CV])

    [http://arxiv.org/abs/2401.08865](http://arxiv.org/abs/2401.08865)

    本文研究了神经网络在自然图像和医学图像领域学习时的差异，提出了一个与训练集维度有关的泛化缩放定律，并认为医学图像数据集更高的固有“标签锐度”可能是两个领域之间显著差异的部分原因。

    

    本文研究了神经网络在不同图像领域学习时的差异，这在从自然图像到其他专门领域（如医学图像）采用计算机视觉技术时通常被忽视。最近的研究发现，训练集的固有维度($d_{data}$)与网络的泛化错误一般会增加。然而，医学（放射学）和自然图像领域之间的这种关系的陡峭程度存在显著差异，且无现有的理论解释。我们通过建立并经验证一个与$d_{data}$相关的泛化缩放定律来解决这个知识空白，并提出考虑到医学图像数据集更高的固有“标签锐度”($K_F$)这一度量指标可以部分解释这两个领域之间的显著缩放差异。接下来，我们展示了利用测量这一指标可以提供的额外好处。

    This paper investigates discrepancies in how neural networks learn from different imaging domains, which are commonly overlooked when adopting computer vision techniques from the domain of natural images to other specialized domains such as medical images. Recent works have found that the generalization error of a trained network typically increases with the intrinsic dimension ($d_{data}$) of its training set. Yet, the steepness of this relationship varies significantly between medical (radiological) and natural imaging domains, with no existing theoretical explanation. We address this gap in knowledge by establishing and empirically validating a generalization scaling law with respect to $d_{data}$, and propose that the substantial scaling discrepancy between the two considered domains may be at least partially attributed to the higher intrinsic "label sharpness" ($K_F$) of medical imaging datasets, a metric which we propose. Next, we demonstrate an additional benefit of measuring th
    
[^62]: 双耳角分离网络

    Binaural Angular Separation Network. (arXiv:2401.08864v1 [eess.AS])

    [http://arxiv.org/abs/2401.08864](http://arxiv.org/abs/2401.08864)

    我们提出了一个双耳角分离网络，能够使用两个麦克风在不同的角度区域分离目标语音源和干扰源，在各种混响环境下保持稳健，并在实时场景中表现出良好的性能。

    

    我们提出了一个神经网络模型，可以使用两个麦克风在不同的角度区域分离目标语音源和干扰源。该模型使用模拟的房间脉冲响应（RIRs）进行训练，无需收集真实的RIRs。通过依赖特定的角度区域和多个房间模拟，该模型利用一致的到达时间差（TDOA）线索，或者我们称之为延迟对比，将目标和干扰源分离，同时在各种混响环境下保持稳健。我们证明了该模型不仅适用于具有略有不同麦克风几何形状的商用设备，而且胜过了我们之前在相同设备上使用的额外麦克风的工作。该模型可以在设备上实时运行，适用于低延迟的流媒体应用，如电话和视频会议。

    We propose a neural network model that can separate target speech sources from interfering sources at different angular regions using two microphones. The model is trained with simulated room impulse responses (RIRs) using omni-directional microphones without needing to collect real RIRs. By relying on specific angular regions and multiple room simulations, the model utilizes consistent time difference of arrival (TDOA) cues, or what we call delay contrast, to separate target and interference sources while remaining robust in various reverberation environments. We demonstrate the model is not only generalizable to a commercially available device with a slightly different microphone geometry, but also outperforms our previous work which uses one additional microphone on the same device. The model runs in real-time on-device and is suitable for low-latency streaming applications such as telephony and video conferencing.
    
[^63]: 使用超宽带传感器的通道脉冲响应来实现钥匙无线进入系统中的密钥扣的鲁棒定位

    Robust Localization of Key Fob Using Channel Impulse Response of Ultra Wide Band Sensors for Keyless Entry Systems. (arXiv:2401.08863v1 [cs.LG])

    [http://arxiv.org/abs/2401.08863](http://arxiv.org/abs/2401.08863)

    本文研究了使用神经网络和超宽带传感器对车辆中的钥匙扣进行定位的问题，并提出了一种新的鲁棒定位方法，该方法在没有对抗训练的情况下取得了比基准方法更好的性能。

    

    使用神经网络对车辆内部和周围的钥匙扣进行定位是一种新兴的车辆安全功能。本文研究了以下问题：1）基于神经网络的超宽带（UWB）定位分类的预计算特征的性能作为我们实验的基准。2）研究各种神经网络的鲁棒性，包括没有任何对抗训练的对抗示例的鲁棒性。3）提出了一种多头自监督神经网络架构，该架构在没有任何对抗训练的情况下优于基准神经网络。模型的性能在快速梯度符号方法的某些对抗强度范围内提高了67％，对于基本迭代方法和投影梯度下降方法分别提高了37％。

    Using neural networks for localization of key fob within and surrounding a car as a security feature for keyless entry is fast emerging. In this paper we study: 1) the performance of pre-computed features of neural networks based UWB (ultra wide band) localization classification forming the baseline of our experiments. 2) Investigate the inherent robustness of various neural networks; therefore, we include the study of robustness of the adversarial examples without any adversarial training in this work. 3) Propose a multi-head self-supervised neural network architecture which outperforms the baseline neural networks without any adversarial training. The model's performance improved by 67% at certain ranges of adversarial magnitude for fast gradient sign method and 37% each for basic iterative method and projected gradient descent method.
    
[^64]: O-RAN中利用半监督学习方法进行网络切片的资源分配的研究

    Semi-Supervised Learning Approach for Efficient Resource Allocation with Network Slicing in O-RAN. (arXiv:2401.08861v1 [cs.NI])

    [http://arxiv.org/abs/2401.08861](http://arxiv.org/abs/2401.08861)

    本文提出了一种半监督学习方法，解决了O-RAN中网络切片和资源分配的问题。通过设计两个xAPPs，分别处理功率控制和物理资源块分配，我们的方法能够在用户设备之间实现最大化的加权吞吐量，并优先考虑增强型移动宽带和超可靠低延迟通信这两种服务类型。

    

    开放式无线接入网络（O-RAN）技术作为一种有前景的解决方案，为网络运营商提供了一个开放和有利的环境。在O-RAN内确保有效地协调x应用程序（xAPPs）对于网络切片和资源分配至关重要。本文介绍了一种创新的资源分配方法，旨在协调O-RAN中多个独立xAPPs的协调。我们的方法侧重于在用户设备（UE）之间最大化加权吞吐量，并分配物理资源块（PRBs）。我们优先考虑增强型移动宽带和超可靠低延迟通信这两种服务类型。为此，我们设计了两个xAPPs：每个UE的功率控制xAPP和PRB分配xAPP。所提出的方法包括两个部分的训练阶段，其中第一部分使用带有变分自动编码器的监督学习进行训练。

    The Open Radio Access Network (O-RAN) technology has emerged as a promising solution for network operators, providing them with an open and favorable environment. Ensuring effective coordination of x-applications (xAPPs) is crucial to enhance flexibility and optimize network performance within the O-RAN. In this paper, we introduce an innovative approach to the resource allocation problem, aiming to coordinate multiple independent xAPPs for network slicing and resource allocation in O-RAN. Our proposed method focuses on maximizing the weighted throughput among user equipments (UE), as well as allocating physical resource blocks (PRBs). We prioritize two service types, namely enhanced Mobile Broadband and Ultra Reliable Low Latency Communication. To achieve this, we have designed two xAPPs: a power control xAPP for each UE and a PRB allocation xAPP. The proposed method consists of a two-part training phase, where the first part uses supervised learning with a Variational Autoencoder tra
    
[^65]: Shabari: 延迟决策以实现更快、更高效的无服务器函数

    Shabari: Delayed Decision-Making for Faster and Efficient Serverless Function. (arXiv:2401.08859v1 [cs.DC])

    [http://arxiv.org/abs/2401.08859](http://arxiv.org/abs/2401.08859)

    Shabari是一个延迟决策的无服务器资源管理框架，通过对函数输入的延迟来减轻无服务器系统中的性能变异性和资源低利用率问题。

    

    无服务器计算减轻了开发人员对资源管理的负担，为用户提供了易用性，并为提供者优化资源利用率提供了机会。然而，今天的无服务器系统在函数调用方面缺乏性能保证，从而限制了对性能关键应用程序的支持：我们观察到严重的性能变异性（高达6倍）。提供者缺乏对用户函数的可见性，因此很难对其进行合适的资源规模化：我们观察到严重的资源低利用率（高达80%）。为了理解性能变异性和低利用率背后的原因，我们对常见部署的无服务器函数进行了测量研究，并了解到函数性能和资源利用率关键取决于函数语义和输入。我们的主要认识是在函数输入可用之后延迟资源分配决策。我们引入了Shabari，一个用于无服务器的资源管理框架。

    Serverless computing relieves developers from the burden of resource management, thus providing ease-of-use to the users and the opportunity to optimize resource utilization for the providers. However, today's serverless systems lack performance guarantees for function invocations, thus limiting support for performance-critical applications: we observed severe performance variability (up to 6x). Providers lack visibility into user functions and hence find it challenging to right-size them: we observed heavy resource underutilization (up to 80%). To understand the causes behind the performance variability and underutilization, we conducted a measurement study of commonly deployed serverless functions and learned that the function performance and resource utilization depend crucially on function semantics and inputs. Our key insight is to delay making resource allocation decisions until after the function inputs are available. We introduce Shabari, a resource management framework for ser
    
[^66]: 使用i向量进行主体无关的跨会话EEG迁移学习

    Using i-vectors for subject-independent cross-session EEG transfer learning. (arXiv:2401.08851v1 [cs.LG])

    [http://arxiv.org/abs/2401.08851](http://arxiv.org/abs/2401.08851)

    本文提出了使用基于i向量的神经网络分类器进行跨主体跨会话EEG迁移学习的方法，相对于等效的主体相关模型取得了18%的相对改进。此外，我们还展示了我们的主体无关模型在新主体上的竞争力和额外主体数据的增加对模型性能的改善，表明有效的认知负荷确定不需要主体相关的训练。

    

    认知负荷分类是根据生理测量如脑电图（EEG）自动确定个体在执行任务时利用工作记忆资源的任务。本文采用了跨学科的方法，利用语音处理的工具和方法来解决这个问题。我们使用的语料库作为第一次被动脑机接口竞赛的一部分，在2021年公开发布。我们提出了使用基于i向量的神经网络分类器进行跨主体跨会话EEG迁移学习的方法，相较于等效的主体相关模型，取得了18%的相对改进。我们还报道了实验结果，显示我们的主体无关模型在新主体上具有竞争力，并且随着额外主体数据的增加而改善，表明有效的认知负荷确定不需要主体相关的训练。

    Cognitive load classification is the task of automatically determining an individual's utilization of working memory resources during performance of a task based on physiologic measures such as electroencephalography (EEG). In this paper, we follow a cross-disciplinary approach, where tools and methodologies from speech processing are used to tackle this problem. The corpus we use was released publicly in 2021 as part of the first passive brain-computer interface competition on cross-session workload estimation. We present our approach which used i-vector-based neural network classifiers to accomplish inter-subject cross-session EEG transfer learning, achieving 18% relative improvement over equivalent subject-dependent models. We also report experiments showing how our subject-independent models perform competitively on held-out subjects and improve with additional subject data, suggesting that subject-dependent training is not required for effective cognitive load determination.
    
[^67]: REValueD: 对可分解的马尔可夫决策过程进行正则化集合值分解

    REValueD: Regularised Ensemble Value-Decomposition for Factorisable Markov Decision Processes. (arXiv:2401.08850v1 [cs.LG])

    [http://arxiv.org/abs/2401.08850](http://arxiv.org/abs/2401.08850)

    REValueD是一种通过正则化集合值分解的新算法，针对高维离散动作空间的任务提供了优越性能，尤其在人形和狗类任务中表现出色。它遏制了Q-learning算法的高估偏差，并减轻了目标方差问题。

    

    由于可能的动作数量庞大，离散动作强化学习算法在具有高维离散动作空间的任务中经常失败。最近的一项进展利用了来自多智能体强化学习的概念——值分解，来解决这个问题。这项研究深入探讨了值分解的影响，揭示了它虽然可以遏制Q学习算法固有的高估偏差，但也会放大目标方差。为了应对这个问题，我们提出了一个评论家的集合以减轻目标方差。此外，我们引入了一个正则化损失，有助于减轻一个维度上的探索性动作对其他维度上最优动作价值的影响。我们的新颖算法REValueD，在经过离散化的DeepMind控制套件任务上进行了测试，展示了卓越的性能，特别是在困难的人形和狗类任务中。我们进一步分析了影响REValueD表现的因素。

    Discrete-action reinforcement learning algorithms often falter in tasks with high-dimensional discrete action spaces due to the vast number of possible actions. A recent advancement leverages value-decomposition, a concept from multi-agent reinforcement learning, to tackle this challenge. This study delves deep into the effects of this value-decomposition, revealing that whilst it curtails the over-estimation bias inherent to Q-learning algorithms, it amplifies target variance. To counteract this, we present an ensemble of critics to mitigate target variance. Moreover, we introduce a regularisation loss that helps to mitigate the effects that exploratory actions in one dimension can have on the value of optimal actions in other dimensions. Our novel algorithm, REValueD, tested on discretised versions of the DeepMind Control Suite tasks, showcases superior performance, especially in the challenging humanoid and dog tasks. We further dissect the factors influencing REValueD's performance
    
[^68]: RIDGE: 医学图像分割模型的可重复性、完整性、可靠性、泛化性和效率评估

    RIDGE: Reproducibility, Integrity, Dependability, Generalizability, and Efficiency Assessment of Medical Image Segmentation Models. (arXiv:2401.08847v1 [eess.IV])

    [http://arxiv.org/abs/2401.08847](http://arxiv.org/abs/2401.08847)

    RIDGE是一个用于评估医学图像分割模型的可重复性、完整性、可靠性、泛化性和效率的框架，旨在通过提高工作质量和透明度，确保分割模型在科学可靠性和临床相关性上都具备优势。

    

    深度学习技术尽管具有潜力，但往往缺乏可重复性和泛化性，限制了其在临床中的应用。图像分割是医学图像分析中的关键任务之一，需要对一个或多个感兴趣的区域/体积进行注释。本文介绍了RIDGE清单，这是一个用于评估基于深度学习的医学图像分割模型的可重复性、完整性、可靠性、泛化性和效率的框架。该清单为研究人员提供了指导，以提高其工作的质量和透明度，确保分割模型不仅具有科学的可靠性，还具有临床的相关性。

    Deep learning techniques, despite their potential, often suffer from a lack of reproducibility and generalizability, impeding their clinical adoption. Image segmentation is one of the critical tasks in medical image analysis, in which one or several regions/volumes of interest should be annotated. This paper introduces the RIDGE checklist, a framework for assessing the Reproducibility, Integrity, Dependability, Generalizability, and Efficiency of deep learning-based medical image segmentation models. The checklist serves as a guide for researchers to enhance the quality and transparency of their work, ensuring that segmentation models are not only scientifically sound but also clinically relevant.
    
[^69]: 随机子网络退火：一种用于微调修剪子网络的正则化技术

    Stochastic Subnetwork Annealing: A Regularization Technique for Fine Tuning Pruned Subnetworks. (arXiv:2401.08830v1 [cs.LG])

    [http://arxiv.org/abs/2401.08830](http://arxiv.org/abs/2401.08830)

    随机子网络退火是一种用于微调修剪子网络的正则化技术，通过在每次前向传播时使用随机掩码表示参数的包含或排除概率，以避免修剪过多参数导致的准确度下降和子网络过拟合的问题。

    

    最近，修剪方法因其降低深度神经网络的大小和计算复杂度的有效性而变得流行起来。经过少数训练轮次后，大量参数可以从训练模型中移除，而准确度几乎没有明显损失。然而，一次性修剪太多参数通常会导致准确度的陡然下降，这可能破坏收敛质量。迭代修剪方法通过在多个轮次中逐渐移除少量参数来缓解这个问题。然而，这仍可能导致子网络过拟合损失函数的局部区域。我们引入了一种新颖而有效的方法，通过一种名为随机子网络退火的正则化技术来调整子网络。我们使用随机掩码来表示子网络，其中每个参数在任何前向传播中都有被包含或排除的概率。

    Pruning methods have recently grown in popularity as an effective way to reduce the size and computational complexity of deep neural networks. Large numbers of parameters can be removed from trained models with little discernible loss in accuracy after a small number of continued training epochs. However, pruning too many parameters at once often causes an initial steep drop in accuracy which can undermine convergence quality. Iterative pruning approaches mitigate this by gradually removing a small number of parameters over multiple epochs. However, this can still lead to subnetworks that overfit local regions of the loss landscape. We introduce a novel and effective approach to tuning subnetworks through a regularization technique we call Stochastic Subnetwork Annealing. Instead of removing parameters in a discrete manner, we instead represent subnetworks with stochastic masks where each parameter has a probabilistic chance of being included or excluded on any given forward pass. We a
    
[^70]: AiGen-FoodReview:一种多模态的机器生成的社交媒体餐厅评论和图像数据集

    AiGen-FoodReview: A Multimodal Dataset of Machine-Generated Restaurant Reviews and Images on Social Media. (arXiv:2401.08825v1 [cs.LG])

    [http://arxiv.org/abs/2401.08825](http://arxiv.org/abs/2401.08825)

    本论文创建了一个多模态的机器生成的社交媒体餐厅评论和图像数据集AiGen-FoodReview，并开发了相应的检测模型，可用于鉴别真实和虚假的评论。通过使用可读性和摄影理论的属性评分，证明了这些属性作为手工特征在可伸缩和可解释的检测模型中的有效性。

    

    在线评论作为用户生成内容（UGC）显著影响消费者决策。然而，不仅人为制造的虚假内容，还有机器生成的内容都挑战了UGC的可靠性。借助于OpenAI的GPT-4-Turbo和DALL-E-2模型，我们创建了AiGen-FoodReview，这是一个包含20,144个餐厅评论-图像对的多模态数据集，分为真实和机器生成的部分。我们探索了单模态和多模态的检测模型，在FLAVA上实现了99.80%的多模态准确度。我们使用可读性和摄影理论的属性分别对评论和图像进行评分，证明了它们作为手工特征在可伸缩和可解释的检测模型中的实用性，其性能与对比相当。本文通过开源数据集和发布虚假评论检测器做出了贡献，推荐使用该数据集以增强可靠的UGC。

    Online reviews in the form of user-generated content (UGC) significantly impact consumer decision-making. However, the pervasive issue of not only human fake content but also machine-generated content challenges UGC's reliability. Recent advances in Large Language Models (LLMs) may pave the way to fabricate indistinguishable fake generated content at a much lower cost. Leveraging OpenAI's GPT-4-Turbo and DALL-E-2 models, we craft AiGen-FoodReview, a multi-modal dataset of 20,144 restaurant review-image pairs divided into authentic and machine-generated. We explore unimodal and multimodal detection models, achieving 99.80% multimodal accuracy with FLAVA. We use attributes from readability and photographic theories to score reviews and images, respectively, demonstrating their utility as hand-crafted features in scalable and interpretable detection models, with comparable performance. The paper contributes by open-sourcing the dataset and releasing fake review detectors, recommending its
    
[^71]: 表面增强拉曼光谱和迁移学习在手术区域准确重建方面的应用

    Surface-Enhanced Raman Spectroscopy and Transfer Learning Toward Accurate Reconstruction of the Surgical Zone. (arXiv:2401.08821v1 [eess.IV])

    [http://arxiv.org/abs/2401.08821](http://arxiv.org/abs/2401.08821)

    本研究提出了一种利用表面增强拉曼光谱和迁移学习的机器人拉曼系统，可以可靠地重建手术区域中肿瘤的位置和边界，并取得了较高的验证分类准确性。

    

    拉曼光谱是一种基于相干光的非弹性背向散射的光子模态，对于术中感知空间来说是一种宝贵的资产，提供了非电离潜力和高度特异性的分子指纹样光谱特征，可用于动态手术场中病理组织的诊断。尽管拉曼光谱在强度上存在缺陷，但利用金属纳米结构来放大拉曼信号的表面增强拉曼光谱（SERS）可以达到与传统光子模态相媲美的检测灵敏度。本研究概述了一种能可靠地确定健康组织中嵌入肿瘤的位置和边界的机器人拉曼系统，该系统以组织模拟幻影为模型，其中有选择地注入了金纳米星区域。此外，由于收集的生物SERS或拉曼数据相对不足，我们实施迁移学习，实现了与金纳米星相比的100%验证分类准确性。

    Raman spectroscopy, a photonic modality based on the inelastic backscattering of coherent light, is a valuable asset to the intraoperative sensing space, offering non-ionizing potential and highly-specific molecular fingerprint-like spectroscopic signatures that can be used for diagnosis of pathological tissue in the dynamic surgical field. Though Raman suffers from weakness in intensity, Surface-Enhanced Raman Spectroscopy (SERS), which uses metal nanostructures to amplify Raman signals, can achieve detection sensitivities that rival traditional photonic modalities. In this study, we outline a robotic Raman system that can reliably pinpoint the location and boundaries of a tumor embedded in healthy tissue, modeled here as a tissue-mimicking phantom with selectively infused Gold Nanostar regions. Further, due to the relative dearth of collected biological SERS or Raman data, we implement transfer learning to achieve 100% validation classification accuracy for Gold Nanostars compared to
    
[^72]: 通过保守密度估计从稀疏离线数据集中学习

    Learning from Sparse Offline Datasets via Conservative Density Estimation. (arXiv:2401.08819v1 [cs.LG])

    [http://arxiv.org/abs/2401.08819](http://arxiv.org/abs/2401.08819)

    本文提出了一种名为保守密度估计（CDE）的训练算法，通过明确约束状态-行为占据稳态分布来解决离线强化学习中的外推错误问题。在稀疏奖励或不足数据的任务中，CDE显示出明显优于基准方法的性能。

    

    离线强化学习（RL）为从预先收集的数据集中学习策略提供了一种有前景的方向，而无需与环境进一步交互。然而，现有的方法在处理分布外（OOD）外推错误方面存在困难，特别是在稀疏奖励或数据稀缺的情况下。在本文中，我们提出了一种名为保守密度估计（CDE）的新的训练算法，通过明确约束状态-行为占据稳态分布来解决这个挑战。CDE通过解决边际重要性抽样中的支持不匹配问题，克服了现有方法的局限性，如稳态分布校正方法。我们的方法在D4RL基准测试中实现了最先进的性能。值得注意的是，CDE在具有稀疏奖励或不足数据的挑战性任务中持续优于基准方法，证明了我们的方法在解决外推错误问题上的优势。

    Offline reinforcement learning (RL) offers a promising direction for learning policies from pre-collected datasets without requiring further interactions with the environment. However, existing methods struggle to handle out-of-distribution (OOD) extrapolation errors, especially in sparse reward or scarce data settings. In this paper, we propose a novel training algorithm called Conservative Density Estimation (CDE), which addresses this challenge by explicitly imposing constraints on the state-action occupancy stationary distribution. CDE overcomes the limitations of existing approaches, such as the stationary distribution correction method, by addressing the support mismatch issue in marginal importance sampling. Our method achieves state-of-the-art performance on the D4RL benchmark. Notably, CDE consistently outperforms baselines in challenging tasks with sparse rewards or insufficient data, demonstrating the advantages of our approach in addressing the extrapolation error problem i
    
[^73]: Link Me Baby One More Time: 在 Spotify 上的社交音乐发现

    Link Me Baby One More Time: Social Music Discovery on Spotify. (arXiv:2401.08818v1 [cs.SI])

    [http://arxiv.org/abs/2401.08818](http://arxiv.org/abs/2401.08818)

    本研究探讨了在社交音乐推荐中影响音乐互动的社交和环境因素。研究发现，接收者与发送者音乐品味相似、分享的音轨适合接收者的品味、接收者与发送者具有更强和更亲密的联系以及分享的艺术家在接收者的关系中受欢迎，这些因素都会增加接收者与新艺术家的互动。

    

    我们探讨影响个人之间音乐推荐和发现结果的社交和环境因素。具体来说，我们使用 Spotify 的数据来研究用户之间发送链接导致接收者与分享的艺术家的音乐互动。我们考虑了几个可能影响这一过程的因素，如发送者与接收者的关系强度，用户在 Spotify 社交网络中的角色，他们的音乐社交凝聚力，以及新艺术家与接收者的品味相似程度。我们发现，当接收者与发送者的音乐品味相似且分享的音轨适合他们的品味时，他们更有可能与新艺术家互动；当他们与发送者有更强和更亲密的联系时，也更有可能互动；以及当分享的艺术家在接收者的关系中受欢迎时，也更有可能互动。最后，我们利用这些发现构建了一个随机森林分类器，用于预测分享的音乐轨道是否会导致接收者的互动。

    We explore the social and contextual factors that influence the outcome of person-to-person music recommendations and discovery. Specifically, we use data from Spotify to investigate how a link sent from one user to another results in the receiver engaging with the music of the shared artist. We consider several factors that may influence this process, such as the strength of the sender-receiver relationship, the user's role in the Spotify social network, their music social cohesion, and how similar the new artist is to the receiver's taste. We find that the receiver of a link is more likely to engage with a new artist when (1) they have similar music taste to the sender and the shared track is a good fit for their taste, (2) they have a stronger and more intimate tie with the sender, and (3) the shared artist is popular with the receiver's connections. Finally, we use these findings to build a Random Forest classifier to predict whether a shared music track will result in the receiver
    
[^74]: 对抗监督使布局到图像扩散模型蓬勃发展

    Adversarial Supervision Makes Layout-to-Image Diffusion Models Thrive. (arXiv:2401.08815v1 [cs.CV])

    [http://arxiv.org/abs/2401.08815](http://arxiv.org/abs/2401.08815)

    该论文提出了一种对抗监督的布局到图像扩散模型（ALDM），通过引入分割判别器和多步展开策略，可以提升布局到图像合成任务中的文本编辑能力和生成图像与输入布局之间的对齐准确性。

    

    尽管大规模扩散模型取得了近期的进展，但布局到图像合成任务的进展较小。当前的布局到图像模型要么在文本编辑能力上欠佳，要么在生成的图像与输入布局之间的对齐上较弱，这限制了它们的实际应用性。为了缓解这一问题，我们提出将对抗监督集成到传统布局到图像扩散模型的训练流程中（ALDM）。具体而言，我们采用基于分割的判别器，该判别器在像素级别上为扩散生成器提供明确的反馈，用于指导去噪图像与输入布局之间的对齐。为了鼓励在采样步骤中对输入布局的一致依从，我们进一步引入了多步展开策略。我们不是只关注单个时间步，而是递归地展开几个步骤来模拟推理过程，并要求判别器在一定时间窗口内评估去噪图像与布局的对齐情况。

    Despite the recent advances in large-scale diffusion models, little progress has been made on the layout-to-image (L2I) synthesis task. Current L2I models either suffer from poor editability via text or weak alignment between the generated image and the input layout. This limits their usability in practice. To mitigate this, we propose to integrate adversarial supervision into the conventional training pipeline of L2I diffusion models (ALDM). Specifically, we employ a segmentation-based discriminator which provides explicit feedback to the diffusion generator on the pixel-level alignment between the denoised image and the input layout. To encourage consistent adherence to the input layout over the sampling steps, we further introduce the multistep unrolling strategy. Instead of looking at a single timestep, we unroll a few steps recursively to imitate the inference process, and ask the discriminator to assess the alignment of denoised images with the layout over a certain time window. 
    
[^75]: 学习动力学对泛化的影响：从样本关系开始

    Sample Relationship from Learning Dynamics Matters for Generalisation. (arXiv:2401.08808v1 [cs.LG])

    [http://arxiv.org/abs/2401.08808](http://arxiv.org/abs/2401.08808)

    这项研究从近似样本之间的相互作用开始，揭示了标签对样本之间的影响，提出了带标签的伪神经切向核 (lpNTK)，并探讨了lpNTK如何帮助理解学习现象。

    

    尽管在改进人工神经网络（ANN）的泛化性能方面，已经有很多关于提出新模型或损失函数的研究，但对训练数据对泛化的影响却关注较少。在这项工作中，我们从近似样本之间的相互作用开始，即学习一个样本如何影响模型对其他样本的预测。通过分析监督学习中涉及的权重更新项，我们发现标签会影响样本之间的相互作用。因此，我们提出了带标签的伪神经切向核（lpNTK），在测量样本之间的相互作用时考虑标签信息。我们首先证明，在某些假设下，lpNTK在Frobenius范数下渐近收敛于经验神经切向核。其次，我们说明了lpNTK如何帮助理解先前工作中发现的学习现象，特别是样本学习困难的现象。

    Although much research has been done on proposing new models or loss functions to improve the generalisation of artificial neural networks (ANNs), less attention has been directed to the impact of the training data on generalisation. In this work, we start from approximating the interaction between samples, i.e. how learning one sample would modify the model's prediction on other samples. Through analysing the terms involved in weight updates in supervised learning, we find that labels influence the interaction between samples. Therefore, we propose the labelled pseudo Neural Tangent Kernel (lpNTK) which takes label information into consideration when measuring the interactions between samples. We first prove that lpNTK asymptotically converges to the empirical neural tangent kernel in terms of the Frobenius norm under certain assumptions. Secondly, we illustrate how lpNTK helps to understand learning phenomena identified in previous work, specifically the learning difficulty of sample
    
[^76]: 差异特征未报告对算法公平性的影响

    The Impact of Differential Feature Under-reporting on Algorithmic Fairness. (arXiv:2401.08788v1 [cs.LG])

    [http://arxiv.org/abs/2401.08788](http://arxiv.org/abs/2401.08788)

    本文研究了差异特征未报告对算法公平性的影响，并提出了一个可分析的模型进行刻画。

    

    公共部门的预测风险模型通常使用更完整的行政数据来开发，这些数据对于更大程度依赖公共服务的亚群体更为完整。例如，在美国，对于由医疗补助和医疗保险支持的个人，政府机构常常可以获得有关医疗保健利用的信息，但对于私人保险的人则没有。对公共部门算法的批评指出，差异特征未报告导致算法决策中的不公平。然而，这种数据偏见在技术视角下仍然研究不足。虽然以前的研究已经考察了添加特征噪声和明确标记为缺失的特征对公平性的影响，但缺失指标的数据缺失情况（即差异特征未报告）尚未得到研究的关注。在本研究中，我们提出了一个可分析的差异特征未报告模型，并将其应用于特征未报告对算法公平性的刻画。

    Predictive risk models in the public sector are commonly developed using administrative data that is more complete for subpopulations that more greatly rely on public services. In the United States, for instance, information on health care utilization is routinely available to government agencies for individuals supported by Medicaid and Medicare, but not for the privately insured. Critiques of public sector algorithms have identified such differential feature under-reporting as a driver of disparities in algorithmic decision-making. Yet this form of data bias remains understudied from a technical viewpoint. While prior work has examined the fairness impacts of additive feature noise and features that are clearly marked as missing, the setting of data missingness absent indicators (i.e. differential feature under-reporting) has been lacking in research attention. In this work, we present an analytically tractable model of differential feature under-reporting which we then use to charac
    
[^77]: 使用多背景表示学习进行粒子物理的鲁棒异常检测

    Robust Anomaly Detection for Particle Physics Using Multi-Background Representation Learning. (arXiv:2401.08777v1 [hep-ex])

    [http://arxiv.org/abs/2401.08777](http://arxiv.org/abs/2401.08777)

    这项工作提出了一种使用多背景表示学习进行鲁棒异常检测的方法，通过利用更多信息来提高检测相关性的估计，并推广了去相关性到多背景设置中。在大型强子对撞机的高维粒子衰变数据集上进行的实验证明了该方法的有效性。

    

    异常或超出分布的检测是在粒子物理中发现新粒子或过程的有望工具。在这项工作中，我们确定并解决了改进高能物理异常检测的两个被忽视的机会。首先，我们不再对单一最主要的背景过程训练生成模型，而是使用多个背景类型的表示学习来构建检测算法，从而利用更多信息来提高检测相关性的估计。其次，我们将去相关性推广到多背景设置中，从而直接强化了异常检测的更完整定义的鲁棒性。我们在大型强子对撞机上的高维粒子衰变数据集上展示了提出的鲁棒多背景异常检测算法的好处。

    Anomaly, or out-of-distribution, detection is a promising tool for aiding discoveries of new particles or processes in particle physics. In this work, we identify and address two overlooked opportunities to improve anomaly detection for high-energy physics. First, rather than train a generative model on the single most dominant background process, we build detection algorithms using representation learning from multiple background types, thus taking advantage of more information to improve estimation of what is relevant for detection. Second, we generalize decorrelation to the multi-background setting, thus directly enforcing a more complete definition of robustness for anomaly detection. We demonstrate the benefit of the proposed robust multi-background anomaly detection algorithms on a high-dimensional dataset of particle decays at the Large Hadron Collider.
    
[^78]: 太阳系中的奇怪和奇妙之处：在时空遗产调查中寻找意外发现

    The weird and the wonderful in our Solar System: Searching for serendipity in the Legacy Survey of Space and Time. (arXiv:2401.08763v1 [astro-ph.EP])

    [http://arxiv.org/abs/2401.08763](http://arxiv.org/abs/2401.08763)

    该论文提出了一种用于太阳系天体数据异常检测的新方法，通过训练深度自编码器，并利用学习到的潜在空间来寻找有趣的天体。该方法在寻找星际物体等有趣类别的例子方面表现出很高的效果，同时也对经典无监督异常检测方法的局限性进行了研究，并评估了使用监督学习方法的可行性。未来的工作可以考虑扩展特征空间以增加发现的异常种类。

    

    我们提出了一种新颖的太阳系天体数据异常检测方法，为时空遗产调查做准备。我们训练了一个深度自编码器来进行异常检测，并利用学习到的潜在空间来搜索其他有趣的天体。我们通过找到星际物体等有趣的例子来证明自编码器方法的有效性，并展示了使用自编码器可以找到更多有趣类别的例子。我们还通过生成合成异常来探究经典无监督异常检测方法的局限性，并评估了使用监督学习方法的可行性。未来的工作应考虑通过扩展特征空间来增加在调查中可以发现的异常的种类，使用自编码器。

    We present a novel method for anomaly detection in Solar System object data, in preparation for the Legacy Survey of Space and Time. We train a deep autoencoder for anomaly detection and use the learned latent space to search for other interesting objects. We demonstrate the efficacy of the autoencoder approach by finding interesting examples, such as interstellar objects, and show that using the autoencoder, further examples of interesting classes can be found. We also investigate the limits of classic unsupervised approaches to anomaly detection through the generation of synthetic anomalies and evaluate the feasibility of using a supervised learning approach. Future work should consider expanding the feature space to increase the variety of anomalies that can be uncovered during the survey using an autoencoder.
    
[^79]: MMToM-QA: 多模态心智理论问答

    MMToM-QA: Multimodal Theory of Mind Question Answering. (arXiv:2401.08743v1 [cs.AI])

    [http://arxiv.org/abs/2401.08743](http://arxiv.org/abs/2401.08743)

    MMToM-QA是一个多模态心智理论问答基准，用于评估机器对于人的心智理论的理解能力。我们提出了一种新的方法BIP-ALM用于实现多模态心智理论能力。

    

    理解人们的心智是开发具有人类水平社交智能的机器的一个重要组成部分。最近的机器学习模型，特别是大型语言模型，似乎展现出某些心智理解的方面。然而，现有的心智理论基准使用单模态数据集-或者视频或者文本。然而，人类的心智理论不仅仅是视频或文本理解。人们可以根据从任何可用数据中提取的概念表示（例如目标，信念，计划）灵活地推理另一个人的心智，这些数据可以包括视觉线索，语言叙事或两者兼有。为了解决这个问题，我们引入了一个多模态的心智理论问答（MMToM-QA）基准。MMToM-QA在多模态数据和关于一个人在家庭环境中的活动的不同种类的单模态数据上全面评估机器的心智理论。为了实现多模态心智理论能力，我们提出了一种新的方法，BIP-ALM（贝叶斯逆向规划）。

    Theory of Mind (ToM), the ability to understand people's minds, is an essential ingredient for developing machines with human-level social intelligence. Recent machine learning models, particularly large language models, seem to show some aspects of ToM understanding. However, existing ToM benchmarks use unimodal datasets - either video or text. Human ToM, on the other hand, is more than video or text understanding. People can flexibly reason about another person's mind based on conceptual representations (e.g., goals, beliefs, plans) extracted from any available data, which can include visual cues, linguistic narratives, or both. To address this, we introduce a multimodal Theory of Mind question answering (MMToM-QA) benchmark. MMToM-QA comprehensively evaluates machine ToM both on multimodal data and on different kinds of unimodal data about a person's activity in a household environment. To engineer multimodal ToM capacity, we propose a novel method, BIP-ALM (Bayesian Inverse Plannin
    
[^80]: 固定点扩散模型

    Fixed Point Diffusion Models. (arXiv:2401.08741v1 [cs.CV])

    [http://arxiv.org/abs/2401.08741](http://arxiv.org/abs/2401.08741)

    固定点扩散模型（FPDM）是一种将固定点求解引入扩散生成模型框架的新方法，通过嵌入固定点求解层和采用新的训练方法，显著减小了模型大小、内存使用量并加快了训练速度，并且提出了两种新的技术来提高采样效率。实验证明，与现有模型相比，FPDM在性能和效率上有明显改进。

    

    我们引入了固定点扩散模型（FPDM），这是一种将固定点求解的概念融入基于扩散的生成模型框架的新方法。我们的方法将一个隐式的固定点求解层嵌入到扩散模型的去噪网络中，将扩散过程转化为一系列相关的固定点问题。结合一种新的随机训练方法，这种方法显著减少了模型大小，减小了内存使用量，并加快了训练速度。此外，它还能够开发出两种新的技术来提高采样效率：在时间步之间重新分配计算和重用固定点解。我们对ImageNet、FFHQ、CelebA-HQ和LSUN-Church等最先进的模型进行了大量实验，证明了性能和效率的显著改进。与最先进的DiT模型相比，FPDM参数减少了87%，内存消耗减少了60%。

    We introduce the Fixed Point Diffusion Model (FPDM), a novel approach to image generation that integrates the concept of fixed point solving into the framework of diffusion-based generative modeling. Our approach embeds an implicit fixed point solving layer into the denoising network of a diffusion model, transforming the diffusion process into a sequence of closely-related fixed point problems. Combined with a new stochastic training method, this approach significantly reduces model size, reduces memory usage, and accelerates training. Moreover, it enables the development of two new techniques to improve sampling efficiency: reallocating computation across timesteps and reusing fixed point solutions between timesteps. We conduct extensive experiments with state-of-the-art models on ImageNet, FFHQ, CelebA-HQ, and LSUN-Church, demonstrating substantial improvements in performance and efficiency. Compared to the state-of-the-art DiT model, FPDM contains 87% fewer parameters, consumes 60%
    
[^81]: SiT:使用可扩展的插值仿射变换探索基于流动和扩散的生成模型

    SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers. (arXiv:2401.08740v1 [cs.CV])

    [http://arxiv.org/abs/2401.08740](http://arxiv.org/abs/2401.08740)

    SiT是一种基于Diffusion Transformers骨干的生成模型，通过插值框架和各种设计选择的模块化研究，实现了在模型大小上超过DiT，在条件ImageNet基准测试中获得了较低的FID-50K评分。

    

    我们提出了一种基于扩散变换器（DiT）骨干的可扩展插值仿射变换器（SiT），这是一种生成模型的系列。插值框架允许以比标准扩散模型更灵活的方式连接两个分布，使得可以对建立在动态传输上的生成模型的各种设计选择进行模块化研究：使用离散时间学习还是连续时间学习，决定模型学习的目标，选择连接分布的插值器，以及部署确定性还是随机采样器。通过精心引入上述元素，SiT在具有相同骨干、参数数量和GFLOPs的条件ImageNet 256x256基准测试中，在模型大小上全面超过了DiT。通过探索可以与学习分开调整的各种扩散系数，SiT在FID-50K评分上达到了2.06。

    We present Scalable Interpolant Transformers (SiT), a family of generative models built on the backbone of Diffusion Transformers (DiT). The interpolant framework, which allows for connecting two distributions in a more flexible way than standard diffusion models, makes possible a modular study of various design choices impacting generative models built on dynamical transport: using discrete vs. continuous time learning, deciding the objective for the model to learn, choosing the interpolant connecting the distributions, and deploying a deterministic or stochastic sampler. By carefully introducing the above ingredients, SiT surpasses DiT uniformly across model sizes on the conditional ImageNet 256x256 benchmark using the exact same backbone, number of parameters, and GFLOPs. By exploring various diffusion coefficients, which can be tuned separately from learning, SiT achieves an FID-50K score of 2.06.
    
[^82]: 基于机器学习的非人类灵长类动物中伊波拉病毒对基因表达影响的分析

    Machine Learning-Based Analysis of Ebola Virus' Impact on Gene Expression in Nonhuman Primates. (arXiv:2401.08738v1 [q-bio.GN])

    [http://arxiv.org/abs/2401.08738](http://arxiv.org/abs/2401.08738)

    本研究引入了一种基于机器学习的方法用于分析感染伊波拉病毒的非人类灵长类动物的基因表达数据，发现IFI6和IFI27等基因作为关键生物标志物能有效分类不同阶段的伊波拉感染。

    

    本研究引入了一种基于机器学习的方法，即监督型幅度-高度评分 (SMAS) 方法，用于分析感染伊波拉病毒 (EBOV) 的非人类灵长类动物 (NHPs) 的基因表达数据。我们利用了来自感染伊波拉病毒的 NHPs 的NanoString基因表达数据集，采用SMAS系统进行微妙的宿主-病原体相互作用分析。SMAS有效地结合了基于统计学显著性和表达变化的基因选择，采用线性分类器如逻辑回归以准确区分RT-qPCR阳性和阴性NHP样本。我们研究的一项重要发现是鉴定了IFI6和IFI27作为关键生物标志物，表现出100%的准确性和曲线下面积 (AUC) 指标在分类不同阶段的伊波拉感染中。除IFI6和IFI27外，基因如MX1，OAS1和ISG15显著上调，突出显示了...

    This study introduces the Supervised Magnitude-Altitude Scoring (SMAS) methodology, a machine learning-based approach, for analyzing gene expression data obtained from nonhuman primates (NHPs) infected with Ebola virus (EBOV). We utilize a comprehensive dataset of NanoString gene expression profiles from Ebola-infected NHPs, deploying the SMAS system for nuanced host-pathogen interaction analysis. SMAS effectively combines gene selection based on statistical significance and expression changes, employing linear classifiers such as logistic regression to accurately differentiate between RT-qPCR positive and negative NHP samples. A key finding of our research is the identification of IFI6 and IFI27 as critical biomarkers, demonstrating exceptional predictive performance with 100% accuracy and Area Under the Curve (AUC) metrics in classifying various stages of Ebola infection. Alongside IFI6 and IFI27, genes, including MX1, OAS1, and ISG15, were significantly upregulated, highlighting the
    
[^83]: 一个可扩展的环境空气污染浓度估计框架

    A Framework for Scalable Ambient Air Pollution Concentration Estimation. (arXiv:2401.08735v1 [stat.AP])

    [http://arxiv.org/abs/2401.08735](http://arxiv.org/abs/2401.08735)

    提出了一个可扩展的数据驱动的机器学习模型框架，用于填充空间和时间数据缺失，从而提供了2018年英格兰全面的空气污染浓度数据集。

    

    环境空气污染仍然是英国的一个关键问题，空气污染浓度数据是改善空气质量的干预措施的基础。然而，英国当前的空气污染监测站网络存在空间稀疏、异质布局和频繁的时间数据缺失问题，通常是由于断电等问题。我们引入了一个可扩展的数据驱动的监督机器学习模型框架，旨在通过填充缺失的测量数据来解决时间和空间数据缺失问题。该方法提供了2018年英格兰1kmx1km的全面数据集，分辨率为每小时一次。我们利用机器学习技术和来自分布不均匀的监测站的实际数据，在研究区域内生成了355,827个合成监测站，产生了价值约700亿英镑的数据。通过验证，评估了模型在预测和估计缺失数据中的性能。

    Ambient air pollution remains a critical issue in the United Kingdom, where data on air pollution concentrations form the foundation for interventions aimed at improving air quality. However, the current air pollution monitoring station network in the UK is characterized by spatial sparsity, heterogeneous placement, and frequent temporal data gaps, often due to issues such as power outages. We introduce a scalable data-driven supervised machine learning model framework designed to address temporal and spatial data gaps by filling missing measurements. This approach provides a comprehensive dataset for England throughout 2018 at a 1kmx1km hourly resolution. Leveraging machine learning techniques and real-world data from the sparsely distributed monitoring stations, we generate 355,827 synthetic monitoring stations across the study area, yielding data valued at approximately \pounds70 billion. Validation was conducted to assess the model's performance in forecasting, estimating missing l
    
[^84]: 提高对抗转移能力的一系列技巧

    Bag of Tricks to Boost Adversarial Transferability. (arXiv:2401.08734v1 [cs.CV])

    [http://arxiv.org/abs/2401.08734](http://arxiv.org/abs/2401.08734)

    本文通过对现有对抗性攻击的研究，提出了一系列技巧来增强对抗性转移能力，并在ImageNet数据集上进行了大量实验证实了其高效性。

    

    深度神经网络广为人知的是对抗性样本的脆弱性。然而，在白盒设置下生成的纯粹对抗性样本在不同模型间的传递能力通常较低。由于对抗性转移对实际应用造成更严重的威胁，因此已提出了各种方法来改善转移能力，包括基于梯度、基于输入转换和基于模型的攻击等。在这项工作中，我们发现现有对抗性攻击中的几个微小改变可以显著影响攻击性能，例如迭代次数和步长。基于对现有的对抗性攻击进行仔细研究，我们提出了一系列技巧来增强对抗性转移能力，包括动量初始化、定期调整步长、对抗示例、基于谱的输入转换以及几种集成策略。在ImageNet数据集上进行的大量实验证实了我们提出的技巧的高效性。

    Deep neural networks are widely known to be vulnerable to adversarial examples. However, vanilla adversarial examples generated under the white-box setting often exhibit low transferability across different models. Since adversarial transferability poses more severe threats to practical applications, various approaches have been proposed for better transferability, including gradient-based, input transformation-based, and model-related attacks, \etc. In this work, we find that several tiny changes in the existing adversarial attacks can significantly affect the attack performance, \eg, the number of iterations and step size. Based on careful studies of existing adversarial attacks, we propose a bag of tricks to enhance adversarial transferability, including momentum initialization, scheduled step size, dual example, spectral-based input transformation, and several ensemble strategies. Extensive experiments on the ImageNet dataset validate the high effectiveness of our proposed tricks a
    
[^85]: 基于条件互信息的贝叶斯条件分布估计用于知识蒸馏

    Bayes Conditional Distribution Estimation for Knowledge Distillation Based on Conditional Mutual Information. (arXiv:2401.08732v1 [cs.LG])

    [http://arxiv.org/abs/2401.08732](http://arxiv.org/abs/2401.08732)

    本文介绍了一种基于条件互信息的贝叶斯条件分布估计方法，通过最大化教师的对数似然和条件互信息来改进知识蒸馏中的估计。实验证明，使用该方法训练的教师能够更好地捕捉图像聚类中的上下文信息。

    

    在知识蒸馏中，通常认为教师的角色是提供用于学生训练过程中的未知贝叶斯条件概率分布（BCPD）的估计。传统上，通过使用最大对数似然（MLL）方法训练教师来获得此估计。为了改进知识蒸馏中的估计，本文将条件互信息（CMI）的概念引入到BCPD的估计中，提出了一种新的估计方法，称为最大CMI（MCMI）方法。具体而言，在MCMI估计中，教师在训练时同时最大化对数似然和条件互信息。通过Eigen-CAM，进一步展示了最大化教师的CMI值可以使教师在图像聚类中捕捉更多的上下文信息。通过进行一系列彻底的实验，我们展示了通过使用通过MCMI估计训练的教师而不是通过MLL估计训练的教师，在各种状态下的性能提升。

    It is believed that in knowledge distillation (KD), the role of the teacher is to provide an estimate for the unknown Bayes conditional probability distribution (BCPD) to be used in the student training process. Conventionally, this estimate is obtained by training the teacher using maximum log-likelihood (MLL) method. To improve this estimate for KD, in this paper we introduce the concept of conditional mutual information (CMI) into the estimation of BCPD and propose a novel estimator called the maximum CMI (MCMI) method. Specifically, in MCMI estimation, both the log-likelihood and CMI of the teacher are simultaneously maximized when the teacher is trained. Through Eigen-CAM, it is further shown that maximizing the teacher's CMI value allows the teacher to capture more contextual information in an image cluster. Via conducting a thorough set of experiments, we show that by employing a teacher trained via MCMI estimation rather than one trained via MLL estimation in various state-of-t
    
[^86]: MA2GCN: 使用轨迹数据进行交通预测的多邻接关系注意力图卷积网络

    MA2GCN: Multi Adjacency relationship Attention Graph Convolutional Networks for Traffic Prediction using Trajectory data. (arXiv:2401.08727v1 [cs.LG])

    [http://arxiv.org/abs/2401.08727](http://arxiv.org/abs/2401.08727)

    提出了一种新的交通拥堵预测模型，使用车辆轨迹数据以及多邻接关系注意力图卷积网络（MA2GCN）来预测交通拥堵情况，不依赖于传感器数据，提取灵活且准确的交通信息。

    

    交通拥堵问题不仅导致巨大的经济损失，而且严重危害城市环境。预测交通拥堵具有重要的实际意义。迄今为止，大多数研究都是基于不同路段上的传感器的历史数据来预测未来的交通流量和速度，分析某个道路段的交通拥堵情况。然而，由于传感器的固定位置，很难挖掘新的信息。另一方面，车辆轨迹数据更加灵活，可以根据需要提取交通信息。因此，我们提出了一种新的交通拥堵预测模型——多邻接关系注意力图卷积网络（MA2GCN）。该模型将车辆轨迹数据转化为网格形式的图结构化数据，并基于不同网格之间的流动性提出了车辆进出矩阵。同时，为了提高模型的性能，

    The problem of traffic congestion not only causes a large amount of economic losses, but also seriously endangers the urban environment. Predicting traffic congestion has important practical significance. So far, most studies have been based on historical data from sensors placed on different roads to predict future traffic flow and speed, to analyze the traffic congestion conditions of a certain road segment. However, due to the fixed position of sensors, it is difficult to mine new information. On the other hand, vehicle trajectory data is more flexible and can extract traffic information as needed. Therefore, we proposed a new traffic congestion prediction model - Multi Adjacency relationship Attention Graph Convolutional Networks(MA2GCN). This model transformed vehicle trajectory data into graph structured data in grid form, and proposed a vehicle entry and exit matrix based on the mobility between different grids. At the same time, in order to improve the performance of the model,
    
[^87]: HierSFL：移动边缘计算中基于局部差分隐私的分割联合学习

    HierSFL: Local Differential Privacy-aided Split Federated Learning in Mobile Edge Computing. (arXiv:2401.08723v1 [cs.CR])

    [http://arxiv.org/abs/2401.08723](http://arxiv.org/abs/2401.08723)

    HierSFL提出了一种基于局部差分隐私的分割联合学习算法，通过合并模型并定性指导最佳聚合时间框架，减少计算和通信开销，提高了隐私保护能力。

    

    联邦学习是一种在保护数据隐私的同时利用用户数据进行学习的有前途的方法。然而，模型训练过程的高要求使得内存或带宽有限的客户端参与变得困难。为了解决这个问题，使用了分割联合学习，其中客户端将中间模型训练结果上传到云服务器进行协同的服务器-客户端模型训练。这种方法促进了资源受限客户端的参与，但也增加了训练时间和通信开销。为了克服这些限制，我们提出了一种新的算法，称为Hierarchical Split Federated Learning（HierSFL），它在边缘和云阶段合并模型，并提供了定性指导来确定最佳聚合时间框架，从而减少计算和通信开销。通过在客户端和边缘服务器级别实施局部差分隐私，我们提高了隐私保护能力。

    Federated Learning is a promising approach for learning from user data while preserving data privacy. However, the high requirements of the model training process make it difficult for clients with limited memory or bandwidth to participate. To tackle this problem, Split Federated Learning is utilized, where clients upload their intermediate model training outcomes to a cloud server for collaborative server-client model training. This methodology facilitates resource-constrained clients' participation in model training but also increases the training time and communication overhead. To overcome these limitations, we propose a novel algorithm, called Hierarchical Split Federated Learning (HierSFL), that amalgamates models at the edge and cloud phases, presenting qualitative directives for determining the best aggregation timeframes to reduce computation and communication expenses. By implementing local differential privacy at the client and edge server levels, we enhance privacy during 
    
[^88]: 使用期望罚牌 (xB) 模型研究足球犯规效率

    Investigating Fouling Efficiency in Football Using Expected Booking (xB) Model. (arXiv:2401.08718v1 [cs.LG])

    [http://arxiv.org/abs/2401.08718](http://arxiv.org/abs/2401.08718)

    本论文介绍了一种新的度量指标，期望罚牌 (xB) 模型，用于估算足球犯规导致黄牌的可能性，并通过实验证明了其在分析球队和球员犯规策略方面的有效性。

    

    本文介绍了期望罚牌 (xB) 模型，这是一种用于估算足球犯规导致黄牌的可能性的新度量指标。通过三个迭代实验，采用集成方法，模型在增加特征和扩展数据集的情况下展示了改进的性能。对2022年FIFA世界杯数据的分析验证了该模型在提供关于球队和球员犯规策略的洞察力方面的有效性，并与实际的防守表现相一致。xB模型填补了犯规效率研究中的空白，强调了常常被忽视的防守策略。通过整合全面的数据和空间特征，可以进一步改进。

    This paper introduces the Expected Booking (xB) model, a novel metric designed to estimate the likelihood of a foul resulting in a yellow card in football. Through three iterative experiments, employing ensemble methods, the model demonstrates improved performance with additional features and an expanded dataset. Analysis of FIFA World Cup 2022 data validates the model's efficacy in providing insights into team and player fouling tactics, aligning with actual defensive performance. The xB model addresses a gap in fouling efficiency examination, emphasizing defensive strategies which often overlooked. Further enhancements are suggested through the incorporation of comprehensive data and spatial features.
    
[^89]: 用于金属增材制造的迁移学习中的源数据子集选择

    Selecting Subsets of Source Data for Transfer Learning with Applications in Metal Additive Manufacturing. (arXiv:2401.08715v1 [cs.LG])

    [http://arxiv.org/abs/2401.08715](http://arxiv.org/abs/2401.08715)

    该论文提出了一种系统的方法来寻找适当的源数据子集，用于金属增材制造中的迁移学习。该方法基于源数据和目标数据集之间的相似性来选择子集，并通过两个相似度距离度量定义的Pareto frontier进行迭代选择。

    

    鉴于金属增材制造中数据不足的问题，已经采用迁移学习（TL）从源领域（例如完成的打印）中提取知识，以改善目标领域（例如新的打印）中的建模性能。当前的应用程序直接在迁移学习中使用所有可访问的源数据，而不考虑源数据和目标数据之间的相似性。本文提出了一种系统的方法，基于源数据和目标数据集之间的相似性来寻找适当的源数据子集，针对一组有限的目标领域数据。这种相似性通过空间和模型距离度量来刻画。基于Pareto frontier的源数据选择方法被开发出来，通过两个相似度距离度量定义的Pareto frontier上的源数据被迭代选择。该方法被集成到基于实例的TL方法（决策树回归模型）和基于模型的TL方法（微调的人工神经网络）中。

    Considering data insufficiency in metal additive manufacturing (AM), transfer learning (TL) has been adopted to extract knowledge from source domains (e.g., completed printings) to improve the modeling performance in target domains (e.g., new printings). Current applications use all accessible source data directly in TL with no regard to the similarity between source and target data. This paper proposes a systematic method to find appropriate subsets of source data based on similarities between the source and target datasets for a given set of limited target domain data. Such similarity is characterized by the spatial and model distance metrics. A Pareto frontier-based source data selection method is developed, where the source data located on the Pareto frontier defined by two similarity distance metrics are selected iteratively. The method is integrated into an instance-based TL method (decision tree regression model) and a model-based TL method (fine-tuned artificial neural network)
    
[^90]: 年轻三阴性乳腺癌患者的生存分析

    Survival Analysis of Young Triple-Negative Breast Cancer Patients. (arXiv:2401.08712v1 [q-bio.QM])

    [http://arxiv.org/abs/2401.08712](http://arxiv.org/abs/2401.08712)

    这项研究使用SEER数据集研究了年轻年龄对三阴性乳腺癌患者的生存能力的影响。实验结果显示，年轻年龄在TNBC患者的存活能力中起着显著作用。

    

    乳腺癌预后对于有效治疗至关重要，这种疾病在40岁以上的妇女中更为常见，但在40岁以下的妇女中罕见，美国只有不到5％的病例出现。研究表明，年轻妇女的预后较差，这种差异因种族而异。乳腺癌根据雌激素、孕激素和HER2等受体进行分类。三阴性乳腺癌（TNBC）缺乏这些受体，约占病例的15％，在年轻患者中更常见，通常导致较差的结果。然而，年龄对TNBC预后的影响尚不清楚。年龄、种族、肿瘤分级、大小和淋巴结状态等因素被研究其对TNBC临床结果的作用，但目前的研究对于年龄相关差异的结论不一致。本研究使用SEER数据集来研究年轻年龄对TNBC患者的存活能力的影响，旨在确定年龄是否是一个重要的预后因素。我们的实验结果表明，年轻年龄在TNBC患者的存活能力中有显著影响。

    Breast cancer prognosis is crucial for effective treatment, with the disease more common in women over 40 years old but rare under 40 years old, where less than 5 percent of cases occur in the U.S. Studies indicate a worse prognosis in younger women, which varies by ethnicity. Breast cancers are classified based on receptors like estrogen, progesterone, and HER2. Triple-negative breast cancer (TNBC), lacking these receptors, accounts for about 15 percent of cases and is more prevalent in younger patients, often resulting in poorer outcomes. Nevertheless, the impact of age on TNBC prognosis remains unclear. Factors like age, race, tumor grade, size, and lymph node status are studied for their role in TNBC's clinical outcomes, but current research is inconclusive about age-related differences. This study uses SEER data set to examine the influence of younger age on survivability in TNBC patients, aiming to determine if age is a significant prognostic factor. Our experimental results on S
    
[^91]: 可靠的测试时自适应的解耦样本学习

    Decoupled Prototype Learning for Reliable Test-Time Adaptation. (arXiv:2401.08703v1 [cs.LG])

    [http://arxiv.org/abs/2401.08703](http://arxiv.org/abs/2401.08703)

    提出了一种名为Decoupled Prototype Learning (DPL)的解耦样本学习方法，通过使用样本原型为中心的损失计算，来解决测试时自适应中噪声伪标签的问题。

    

    测试时自适应（TTA）是在推理过程中持续将预训练的源模型适应到目标域的任务。一种常见的方法是使用估计的伪标签使用交叉熵损失微调模型。然而，该方法的性能很容易受到噪声伪标签的影响。本研究发现，将每个样本的分类错误最小化会使交叉熵损失对标签噪声非常敏感。为解决这个问题，我们提出了一种新颖的解耦样本学习（DPL）方法，采用样本原型为中心的损失计算。首先，我们解耦了类原型的优化。对于每个类原型，我们采用对比方式减小其与正样本的距离，并增加其与负样本的距离。这种策略可以防止模型对噪声伪标签过拟合。其次，我们提出了一种基于记忆的策略，以增强DPL在TTA中经常遇到的小批量情况下的鲁棒性。我们更新每个类别的原型时使用记忆策略。

    Test-time adaptation (TTA) is a task that continually adapts a pre-trained source model to the target domain during inference. One popular approach involves fine-tuning model with cross-entropy loss according to estimated pseudo-labels. However, its performance is significantly affected by noisy pseudo-labels. This study reveals that minimizing the classification error of each sample causes the cross-entropy loss's vulnerability to label noise. To address this issue, we propose a novel Decoupled Prototype Learning (DPL) method that features prototype-centric loss computation. First, we decouple the optimization of class prototypes. For each class prototype, we reduce its distance with positive samples and enlarge its distance with negative samples in a contrastive manner. This strategy prevents the model from overfitting to noisy pseudo-labels. Second, we propose a memory-based strategy to enhance DPL's robustness for the small batch sizes often encountered in TTA. We update each class
    
[^92]: 我们真的需要数据吗？

    Do We Really Even Need Data?. (arXiv:2401.08702v1 [stat.ME])

    [http://arxiv.org/abs/2401.08702](http://arxiv.org/abs/2401.08702)

    本文探讨了使用预训练算法的预测作为因变量的统计挑战，并着重阐述了三个可能的错误来源。

    

    随着人工智能和机器学习工具的普及，科学家在数据收集方面面临着新的障碍（例如成本上升、调查响应率下降），研究人员越来越多地使用预训练算法的预测作为因变量。虽然从财务和后勤的角度来看这样做具有吸引力，但使用标准的推论工具可能会在替换真实的未观察到的结果值时误代表自变量与所关心的结果之间的联系。本文对这种所谓“后预测推断”问题的统计挑战进行了表征，并阐明了可能存在的三个错误来源：（i）预测结果与其真实未观察到的对应物之间的关系，（ii）机器学习模型对重新采样或对训练数据的不确定性的鲁棒性，以及（iii）适当地将预测结果的偏差和不确定性传播到最终的推断中。

    As artificial intelligence and machine learning tools become more accessible, and scientists face new obstacles to data collection (e.g. rising costs, declining survey response rates), researchers increasingly use predictions from pre-trained algorithms as outcome variables. Though appealing for financial and logistical reasons, using standard tools for inference can misrepresent the association between independent variables and the outcome of interest when the true, unobserved outcome is replaced by a predicted value. In this paper, we characterize the statistical challenges inherent to this so-called ``post-prediction inference'' problem and elucidate three potential sources of error: (i) the relationship between predicted outcomes and their true, unobserved counterparts, (ii) robustness of the machine learning model to resampling or uncertainty about the training data, and (iii) appropriately propagating not just bias but also uncertainty from predictions into the ultimate inference
    
[^93]: 使用神经网络替代品的计算效率高的弯头型泄压管优化

    Computationally Efficient Optimisation of Elbow-Type Draft Tube Using Neural Network Surrogates. (arXiv:2401.08700v1 [math.OC])

    [http://arxiv.org/abs/2401.08700](http://arxiv.org/abs/2401.08700)

    本研究提出了一种计算效率高的弯头型泄压管的优化工作流，利用神经网络替代品进行评估和设计。从评估结果中找到了最佳算法，并确定了单目标优化和多目标优化中目标的影响和综合影响对于泄压管设计的影响。

    

    本研究旨在综合评估单目标和多目标优化算法用于设计弯头型泄压管，并引入一种计算效率高的优化工作流。所提出的工作流利用从数值模拟获得的数据训练深度神经网络替代品。使用替代品能够更灵活、更快速地评估新颖设计。成功的基于历史记录的自适应差分进化与线性减少和基于分解的多目标进化算法被确定为表现最佳的算法，并用于确定单目标优化中不同目标的影响，以及多目标优化中这些目标的综合影响对于泄压管设计的影响。单目标算法的结果与多目标算法在分别考虑目标时一致。

    This study aims to provide a comprehensive assessment of single-objective and multi-objective optimisation algorithms for the design of an elbow-type draft tube, as well as to introduce a computationally efficient optimisation workflow. The proposed workflow leverages deep neural network surrogates trained on data obtained from numerical simulations. The use of surrogates allows for a more flexible and faster evaluation of novel designs. The success history-based adaptive differential evolution with linear reduction and the multi-objective evolutionary algorithm based on decomposition were identified as the best-performing algorithms and used to determine the influence of different objectives in the single-objective optimisation and their combined impact on the draft tube design in the multi-objective optimisation. The results for the single-objective algorithm are consistent with those of the multi-objective algorithm when the objectives are considered separately. Multi-objective appr
    
[^94]: 在高层次综合中使用GNN进行源-后向路质量预测的分层方法

    Hierarchical Source-to-Post-Route QoR Prediction in High-Level Synthesis with GNNs. (arXiv:2401.08696v1 [cs.AR])

    [http://arxiv.org/abs/2401.08696](http://arxiv.org/abs/2401.08696)

    该论文提出了一种在高层次综合中使用GNN进行源-后向路质量预测的分层方法。该方法通过建模流程、图构建方法和分层GNN训练和预测方法，能够有效预测QoR指标，并在设计空间探索中减少了运行时。

    

    高层次综合（HLS）通过避免RTL编程显着加快了硬件设计过程。然而，在优化过程中考虑后向路结果质量（QoR）时，HLS的周转时间显著增加。为了解决这个问题，我们提出了一种用于FPGA HLS的分层后向路QoR预测方法，具有以下特点：（1）直接从C / C ++程序估计延迟和后向路资源使用的建模流程；（2）有效表示源代码的控制和数据流图以及HLS伪指令的影响的图构建方法；（3）能够捕获循环层次影响的分层GNN训练和预测方法。实验结果显示，与最先进的GNN方法相比，我们的方法在不同类型的QoR指标下的预测误差小于10%，取得了巨大的改进。通过采用我们提出的方法论，在HLS的设计空间探索中减少了运行时。

    High-level synthesis (HLS) notably speeds up the hardware design process by avoiding RTL programming. However, the turnaround time of HLS increases significantly when post-route quality of results (QoR) are considered during optimization. To tackle this issue, we propose a hierarchical post-route QoR prediction approach for FPGA HLS, which features: (1) a modeling flow that directly estimates latency and post-route resource usage from C/C++ programs; (2) a graph construction method that effectively represents the control and data flow graph of source code and effects of HLS pragmas; and (3) a hierarchical GNN training and prediction method capable of capturing the impact of loop hierarchies. Experimental results show that our method presents a prediction error of less than 10% for different types of QoR metrics, which gains tremendous improvement compared with the state-of-the-art GNN methods. By adopting our proposed methodology, the runtime for design space exploration in HLS is shor
    
[^95]: 在银行业实现负责任的人工智能：解决偏见以实现公平决策

    Towards Responsible AI in Banking: Addressing Bias for Fair Decision-Making. (arXiv:2401.08691v1 [stat.ML])

    [http://arxiv.org/abs/2401.08691](http://arxiv.org/abs/2401.08691)

    本论文探讨了在银行业中解决偏见以实现公平决策的问题。通过无缝整合公平、可解释性和人类监督，构建负责任人工智能文化，以遵守规定并符合人权标准。

    

    在人工智能广泛应用于各行各业的决策过程的时代，对信任的需求变得更加强烈。本论文对偏见和公平进行了全面的探讨，特别关注它们在银行业内的影响，因为以人工智能驱动的决策对社会产生了重大影响。在这个背景下，公平、可解释性和人类监督的无缝整合变得至关重要，最终形成了所谓的“负责任人工智能”。这强调了在开发符合人工智能规定和普世人权标准的企业文化时，解决偏见的重要性，特别是在自动决策系统领域。如今，将伦理原则融入到人工智能模型的开发、训练和部署中对于遵守即将到来的规定非常关键。

    In an era characterized by the pervasive integration of artificial intelligence into decision-making processes across diverse industries, the demand for trust has never been more pronounced. This thesis embarks on a comprehensive exploration of bias and fairness, with a particular emphasis on their ramifications within the banking sector, where AI-driven decisions bear substantial societal consequences. In this context, the seamless integration of fairness, explainability, and human oversight is of utmost importance, culminating in the establishment of what is commonly referred to as "Responsible AI". This emphasizes the critical nature of addressing biases within the development of a corporate culture that aligns seamlessly with both AI regulations and universal human rights standards, particularly in the realm of automated decision-making systems. Nowadays, embedding ethical principles into the development, training, and deployment of AI models is crucial for compliance with forthcom
    
[^96]: 负采样矫正的对比学习

    Contrastive Learning with Negative Sampling Correction. (arXiv:2401.08690v1 [cs.LG])

    [http://arxiv.org/abs/2401.08690](http://arxiv.org/abs/2401.08690)

    对比学习是一种自监督表示学习方法，通过矫正负样本采样偏置来提高性能，我们提出了一种名为PUCL的新型对比学习方法。

    

    对比学习是一种有效的自监督表示学习方法，它依赖于多个负样本与正样本进行对比。在标准的对比学习中，使用数据增强方法生成正负样本。然而，现有的研究一直致力于改进正样本采样，而负样本采样则常常被忽视。事实上，生成的负样本通常会受到正样本的污染，这导致了偏置损失和性能下降。为了矫正负样本采样偏置，我们提出了一种名为正负样本对比学习（PUCL）的新型对比学习方法。PUCL将生成的负样本视为未标记样本，并使用正样本的信息来纠正对比损失中的偏差。我们证明了在PUCL中使用的校正损失与无偏差的对比损失相比，只引入了可忽略的偏差。PUCL可以应用于... （原文省略）

    As one of the most effective self-supervised representation learning methods, contrastive learning (CL) relies on multiple negative pairs to contrast against each positive pair. In the standard practice of contrastive learning, data augmentation methods are utilized to generate both positive and negative pairs. While existing works have been focusing on improving the positive sampling, the negative sampling process is often overlooked. In fact, the generated negative samples are often polluted by positive samples, which leads to a biased loss and performance degradation. To correct the negative sampling bias, we propose a novel contrastive learning method named Positive-Unlabeled Contrastive Learning (PUCL). PUCL treats the generated negative samples as unlabeled samples and uses information from positive samples to correct bias in contrastive loss. We prove that the corrected loss used in PUCL only incurs a negligible bias compared to the unbiased contrastive loss. PUCL can be applied
    
[^97]: 使用扩散噪声进行ODI（外分布检测）

    NODI: Out-Of-Distribution Detection with Noise from Diffusion. (arXiv:2401.08689v1 [cs.CV])

    [http://arxiv.org/abs/2401.08689](http://arxiv.org/abs/2401.08689)

    本研究将扩散过程应用于外分布检测任务中，通过将整个训练集的信息集成到预测的噪声向量中，获得稳定的噪声向量，并将其转化为OOD分数。

    

    外分布（OOD）检测是安全部署机器学习模型的关键部分。在文献中已经广泛研究并开发了大量方法来解决这个问题。然而，以前的方法在计算OOD分数时对内分布数据集的使用有限。例如，OOD分数是根据内分布数据的一小部分信息计算的。此外，这些方法使用神经图像编码器对图像进行编码。然而，很少有人检查这些方法对不同训练方法和架构的图像编码器的鲁棒性。在这项工作中，我们将扩散过程引入到ODD任务中。扩散模型将整个训练集的信息集成到预测的噪声向量中。此外，我们推导出噪声向量（稳定点）的闭式解。然后，将噪声向量转化为我们的OOD分数，我们测试了深度模型预测的...

    Out-of-distribution (OOD) detection is a crucial part of deploying machine learning models safely. It has been extensively studied with a plethora of methods developed in the literature. This problem is tackled with an OOD score computation, however, previous methods compute the OOD scores with limited usage of the in-distribution dataset. For instance, the OOD scores are computed with information from a small portion of the in-distribution data. Furthermore, these methods encode images with a neural image encoder. The robustness of these methods is rarely checked with respect to image encoders of different training methods and architectures. In this work, we introduce the diffusion process into the OOD task. The diffusion model integrates information on the whole training set into the predicted noise vectors. What's more, we deduce a closed-form solution for the noise vector (stable point). Then the noise vector is converted into our OOD score, we test both the deep model predicted no
    
[^98]: 一种基于物理信息的机器学习模型用于时变波浪冲上预测

    A Physics-informed machine learning model for time-dependent wave runup prediction. (arXiv:2401.08684v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2401.08684](http://arxiv.org/abs/2401.08684)

    本研究提出了一种基于物理信息的机器学习模型，通过结合Surfbeat和XBeach模型的计算效率和准确性，使用cGAN将波浪冲上的图像从XBSB模式映射到XBNH模式，实现了快速准确的时变波浪冲上预测。

    

    波浪冲上是影响海滩洪水、海岸线变化和沿海结构损坏的关键因素。气候变化还预计将增加波浪冲上对沿海地区的影响。因此，快速准确地估计波浪冲上对于有效的沿海工程设计和管理至关重要。然而，由于过程的固有非线性和非平稳性，即使使用最先进的机器学习技术，预测时变的波浪冲上也具有挑战性。在本研究中，提出了一种基于物理信息的机器学习方法，以高效准确地模拟时序波浪冲上。该方法将Surfbeat (XBSB)模式的计算效率与XBeach模型的非静水压（XBNH）模式的准确性相结合。具体而言，采用条件生成对抗网络（cGAN）将XBSB的波浪冲上图像表示映射到相应的XBNH图像表示。这些图像表示使用光流方法进行时间变换并进行机器学习模型训练以实现波浪冲上预测。

    Wave runup is a critical factor affecting coastal flooding, shoreline changes, and damage to coastal structures. Climate change is also expected to amplify wave runup's impact on coastal areas. Therefore, fast and accurate wave runup estimation is essential for effective coastal engineering design and management. However, predicting the time-dependent wave runup is challenging due to the intrinsic nonlinearities and non-stationarity of the process, even with the use of the most advanced machine learning techniques. In this study, a physics-informed machine learning-based approach is proposed to efficiently and accurately simulate time-series wave runup. The methodology combines the computational efficiency of the Surfbeat (XBSB) mode with the accuracy of the nonhydrostatic (XBNH) mode of the XBeach model. Specifically, a conditional generative adversarial network (cGAN) is used to map the image representation of wave runup from XBSB to the corresponding image from XBNH. These images ar
    
[^99]: 使用增强型大型语言模型实现零样本RTL代码生成

    Zero-Shot RTL Code Generation with Attention Sink Augmented Large Language Models. (arXiv:2401.08683v1 [cs.AR])

    [http://arxiv.org/abs/2401.08683](http://arxiv.org/abs/2401.08683)

    本文探讨了使用大型语言模型在硬件设计中快速生成RTL代码的可能性，并展示了新的注意力机制如何提供功能、优化和符合行业标准的RTL代码。

    

    传统上，硬件设计和优化需耗费大量资源，需要相当专业知识，并依赖于已建立的设计自动化工具。本文讨论了利用大型语言模型来简化硬件设计中的代码生成过程的可能性。与之前的研究不同，本文旨在使用大型语言模型，通过一个单一提示接受高层设计规范，生成相应的寄存器传输级（RTL）代码。能够在RTL代码生成中使用大型语言模型不仅加快了设计迭代周期，还便于探索传统技术难以处理的设计空间的计算挑战。通过我们的评估，我们展示了现有注意力机制的不足，并介绍了语言模型在使用一种新的注意力机制时产生功能、优化且符合行业标准的RTL代码的能力。

    The design and optimization of hardware have traditionally been resource-intensive, demanding considerable expertise and dependence on established design automation tools. This paper discusses the possibility of exploiting large language models to streamline the code generation process in hardware design. In contrast to earlier studies, this paper aims to use large language models that accepts high-level design specifications through a single prompt to generate corresponding Register-Transfer Level (RTL) code. The ability to use large language models on RTL code generation not only expedites design iteration cycles but also facilitates the exploration of design spaces that have computational challenges for conventional techniques. Through our evaluation, we demonstrate the shortcoming of existing attention mechanisms, and present the abilities of language models to produce functional, optimized, and industry-standard compliant RTL code when a novel attention mechanism is used. These fi
    
[^100]: 概念对齐（arXiv：2401.08672v1 [cs.LG]）

    Concept Alignment. (arXiv:2401.08672v1 [cs.LG])

    [http://arxiv.org/abs/2401.08672](http://arxiv.org/abs/2401.08672)

    在AI对齐的讨论中，我们强调了概念对齐的重要性，认为在对齐价值观之前，AI系统和人类必须对其理解世界所使用的概念进行对齐。我们整合了哲学、认知科学和深度学习的思想，并提出了实现共享概念的机会和挑战。

    

    AI对齐（人类和AI系统之间的对齐）的讨论主要集中在价值对齐上，广义上指的是创建与人类价值观相同的AI系统。我们认为，在我们尝试对齐价值观之前，AI系统和人类对于理解世界所使用的概念必须首先对齐。我们整合了哲学、认知科学和深度学习的思想，解释了人类和机器之间需要概念对齐而不仅仅是价值对齐的必要性。我们总结了目前人类和机器学习概念的现有方法，并概述了实现共享概念的机会和挑战。最后，我们解释了如何利用认知科学和AI研究中已经开发的工具加速概念对齐的进展。

    Discussion of AI alignment (alignment between humans and AI systems) has focused on value alignment, broadly referring to creating AI systems that share human values. We argue that before we can even attempt to align values, it is imperative that AI systems and humans align the concepts they use to understand the world. We integrate ideas from philosophy, cognitive science, and deep learning to explain the need for concept alignment, not just value alignment, between humans and machines. We summarize existing accounts of how humans and machines currently learn concepts, and we outline opportunities and challenges in the path towards shared concepts. Finally, we explain how we can leverage the tools already being developed in cognitive science and AI research to accelerate progress towards concept alignment.
    
[^101]: DeepSpeed-FastGen: 通过MII和DeepSpeed-Inference实现高吞吐量的LLMs文本生成

    DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference. (arXiv:2401.08671v1 [cs.PF])

    [http://arxiv.org/abs/2401.08671](http://arxiv.org/abs/2401.08671)

    本文介绍了DeepSpeed-FastGen，该系统采用了动态SplitFuse策略，通过MII和DeepSpeed-Inference实现了高吞吐量和低延迟的LLMs文本生成，相比于最先进的系统，实现了高达2.3倍的有效吞吐量提升，平均降低了2倍的延迟，并降低了高达3.7倍的尾延迟。

    

    部署和扩展大型语言模型（LLMs）已经变得至关重要，因为它们在各种应用中渗透，并要求高吞吐量和低延迟的服务系统。现有框架对于具有长提示的工作负载很难平衡这些要求。本文介绍了DeepSpeed-FastGen，这是一个采用动态SplitFuse的系统，它采用了一种新颖的提示和生成组合策略，相比于vLLM等最先进的系统，实现了高达2.3倍的有效吞吐量提升，平均降低了2倍的延迟，并降低了高达3.7倍（令牌级）的尾延迟。我们利用DeepSpeed-MII和DeepSpeed-Inference的协同组合，为LLMs提供了高效易用的服务系统。DeepSpeed-FastGen的先进实现支持一系列模型，并提供非持久和持久部署选项，适用于从交互式会话到长时间运行应用的各种用户场景。我们提供了详细的基准测试。

    The deployment and scaling of large language models (LLMs) have become critical as they permeate various applications, demanding high-throughput and low-latency serving systems. Existing frameworks struggle to balance these requirements, especially for workloads with long prompts. This paper introduces DeepSpeed-FastGen, a system that employs Dynamic SplitFuse, a novel prompt and generation composition strategy, to deliver up to 2.3x higher effective throughput, 2x lower latency on average, and up to 3.7x lower (token-level) tail latency, compared to state-of-the-art systems like vLLM. We leverage a synergistic combination of DeepSpeed-MII and DeepSpeed-Inference to provide an efficient and easy-to-use serving system for LLMs. DeepSpeed-FastGen's advanced implementation supports a range of models and offers both non-persistent and persistent deployment options, catering to diverse user scenarios from interactive sessions to long-running applications. We present a detailed benchmarking 
    
[^102]: 多卡车多分段需求路径的深度强化学习在车辆路径问题中的应用

    Deep Reinforcement Learning for Multi-Truck Vehicle Routing Problems with Multi-Leg Demand Routes. (arXiv:2401.08669v1 [cs.LG])

    [http://arxiv.org/abs/2401.08669](http://arxiv.org/abs/2401.08669)

    本研究针对多卡车多分段需求路径的车辆路径问题，通过对现有的编码器-解码器注意力模型进行扩展，实现了在工业供应链物流中使用深度强化学习的有效策略。

    

    深度强化学习已被证明在解决一些车辆路径问题时非常有效，特别是在使用编码器-解码器注意力机制生成的策略时。然而，对于一些相对简单的问题实例，这些技术已经取得了相当大的成功，但对于一些仍未得到研究和非常复杂的车辆路径问题变体，尚未证明有有效的强化学习方法可用。本文聚焦于一种这样的车辆路径问题变体，其中包含多辆卡车和多分段路径要求。在这些问题中，需求需要沿着节点序列移动，而不仅仅是从起点到终点。为了使深度强化学习成为适用于实际工业规模的供应链物流的有效策略，我们对现有的编码器-解码器注意力模型进行了新扩展，使其能处理多卡车和多分段路径要求。我们的模型具有这样的优势，可以在小规模数据训练下进行，并能在工业供应链物流中进行实际应用。

    Deep reinforcement learning (RL) has been shown to be effective in producing approximate solutions to some vehicle routing problems (VRPs), especially when using policies generated by encoder-decoder attention mechanisms. While these techniques have been quite successful for relatively simple problem instances, there are still under-researched and highly complex VRP variants for which no effective RL method has been demonstrated. In this work we focus on one such VRP variant, which contains multiple trucks and multi-leg routing requirements. In these problems, demand is required to move along sequences of nodes, instead of just from a start node to an end node. With the goal of making deep RL a viable strategy for real-world industrial-scale supply chain logistics, we develop new extensions to existing encoder-decoder attention models which allow them to handle multiple trucks and multi-leg routing requirements. Our models have the advantage that they can be trained for a small number 
    
[^103]: 数据驱动的物理信息神经网络：数字孪生的观点

    Data-Driven Physics-Informed Neural Networks: A Digital Twin Perspective. (arXiv:2401.08667v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2401.08667](http://arxiv.org/abs/2401.08667)

    该论文研究了利用物理信息神经网络(PINNs)实现数字孪生(DT)的潜力。提出了适用于无网格框架的自适应采样方法，并验证了PINNs在参数化的Navier-Stokes方程中的可扩展性和多保真度的优势。

    

    本研究从不同角度探索了利用物理信息神经网络(PINNs)实现数字孪生(DT)的潜力。首先，研究了用于配点的各种自适应采样方法，以验证它们在无网格框架的PINNs中的有效性，该框架允许自动构建虚拟表示，无需手动生成网格。然后，检验了数据驱动的PINNs(DD-PINNs)框架的整体性能，该框架可以利用在DT场景中获得的数据集。其对参数化的Navier-Stokes方程的更一般物理性的可扩展性得到了验证，其中PINNs在雷诺数变化时无需重新训练。此外，由于实际上数据集经常可以在不同的保真度/稀疏度下收集，还提出并评估了多保真度的DD-PINNs。它们在外推任务中表现出了显著的预测性能，相对于单保真度方法提高了42％到62％。

    This study explores the potential of physics-informed neural networks (PINNs) for the realization of digital twins (DT) from various perspectives. First, various adaptive sampling approaches for collocation points are investigated to verify their effectiveness in the mesh-free framework of PINNs, which allows automated construction of virtual representation without manual mesh generation. Then, the overall performance of the data-driven PINNs (DD-PINNs) framework is examined, which can utilize the acquired datasets in DT scenarios. Its scalability to more general physics is validated within parametric Navier-Stokes equations, where PINNs do not need to be retrained as the Reynolds number varies. In addition, since datasets can be often collected from different fidelity/sparsity in practice, multi-fidelity DD-PINNs are also proposed and evaluated. They show remarkable prediction performance even in the extrapolation tasks, with $42\sim62\%$ improvement over the single-fidelity approach.
    
[^104]: 用有限的飞行员示范数据进行稳健敏捷飞行器控制的集成模仿和强化学习方法

    An Integrated Imitation and Reinforcement Learning Methodology for Robust Agile Aircraft Control with Limited Pilot Demonstration Data. (arXiv:2401.08663v1 [cs.AI])

    [http://arxiv.org/abs/2401.08663](http://arxiv.org/abs/2401.08663)

    本文提出了一种使用有限的飞行员示范数据的集成模仿和强化学习方法，用于建立适用于广泛条件的敏捷飞行器的数据驱动机动生成模型，以解决构建模型所需的大量真实数据的时间和成本问题，并能够泛化到其他飞行条件。

    

    本文提出了一种方法，用于构建适用于敏捷飞行器在广泛的平衡条件和飞行器模型参数下可以泛化的数据驱动机动生成模型。机动生成模型在飞行器原型的测试和评估中起着关键作用，能够提供关于飞行器的机动性和敏捷性的见解。然而，构建这些模型通常需要大量的真实飞行员数据，这可能耗时且昂贵。此外，使用有限数据构建的模型往往难以在原始数据集所包含的特定飞行条件之外进行泛化。为了解决这些挑战，我们提出了一种混合架构，利用一个称为源模型的仿真模型。这个开源敏捷飞行器模拟器与目标飞行器具有相似的动力学特性，并允许我们生成无限的数据来构建代理机动生成模型。然后，我们对该模型进行微调。

    In this paper, we present a methodology for constructing data-driven maneuver generation models for agile aircraft that can generalize across a wide range of trim conditions and aircraft model parameters. Maneuver generation models play a crucial role in the testing and evaluation of aircraft prototypes, providing insights into the maneuverability and agility of the aircraft. However, constructing the models typically requires extensive amounts of real pilot data, which can be time-consuming and costly to obtain. Moreover, models built with limited data often struggle to generalize beyond the specific flight conditions covered in the original dataset. To address these challenges, we propose a hybrid architecture that leverages a simulation model, referred to as the source model. This open-source agile aircraft simulator shares similar dynamics with the target aircraft and allows us to generate unlimited data for building a proxy maneuver generation model. We then fine-tune this model t
    
[^105]: 考虑车辆重量的风险预测自动驾驶策略，基于分层深度强化学习

    Risk-anticipatory autonomous driving strategies considering vehicles' weights, based on hierarchical deep reinforcement learning. (arXiv:2401.08661v1 [cs.RO])

    [http://arxiv.org/abs/2401.08661](http://arxiv.org/abs/2401.08661)

    本研究基于分层深度强化学习开发了一种风险预测自动驾驶策略，考虑车辆的重量，并将其纳入自动驾驶决策中，以降低潜在风险和事故后果。

    

    自动驾驶车辆具有减少人为错误导致的事故和降低道路交通风险的潜力。然而，由于重型车辆的性质，其碰撞会导致更严重的事故，因此在自动驾驶的背景下，需要考虑车辆的重量来制定降低潜在风险和后果的驾驶策略。本研究基于风险预测开发了一种自动驾驶策略，考虑周围车辆的重量，并使用分层深度强化学习。通过风险场理论提出了一个综合周围车辆重量的风险指标，并将其纳入自动驾驶决策中。设计了一个混合行动空间，允许左车道变道、右车道变道和跟车行为，使自动驾驶车辆能够在可能的情况下更自由、更真实地行动。为了解决上述混合决策问题，采用了分层近端策略优化（HPPO）算法。

    Autonomous vehicles (AVs) have the potential to prevent accidents caused by drivers' error and reduce road traffic risks. Due to the nature of heavy vehicles, whose collisions cause more serious crashes, the weights of vehicles need to be considered when making driving strategies aimed at reducing the potential risks and their consequences in the context of autonomous driving. This study develops an autonomous driving strategy based on risk anticipation, considering the weights of surrounding vehicles and using hierarchical deep reinforcement learning. A risk indicator integrating surrounding vehicles' weights, based on the risk field theory, is proposed and incorporated into autonomous driving decisions. A hybrid action space is designed to allow for left lane changes, right lane changes and car-following, which enables AVs to act more freely and realistically whenever possible. To solve the above hybrid decision-making problem, a hierarchical proximal policy optimization (HPPO) algor
    
[^106]: SAiD: 使用扩散方法驱动的语音驱动表情动画

    SAiD: Speech-driven Blendshape Facial Animation with Diffusion. (arXiv:2401.08655v1 [cs.CV])

    [http://arxiv.org/abs/2401.08655](http://arxiv.org/abs/2401.08655)

    提出了一种使用扩散模型（SAiD）驱动的语音驱动的三维面部动画方法，通过轻量级的Transformer-based U-Net模型和音频与视觉的交叉模态对齐偏差，实现了较好的唇部同步和更多样化的唇部运动。

    

    尽管进行了大量研究，但由于缺乏大规模的视听数据集，语音驱动的三维面部动画仍然具有挑战性。大多数过去的工作通常采用最小二乘法在小数据集上学习回归模型，但在从语音生成各种唇部动作方面遇到困难，并且需要大量精细调整生成的输出结果。为了解决这些问题，我们提出了一种使用扩散模型（SAiD）驱动的语音驱动的三维面部动画，这是一种轻量级的基于Transformer的U-Net模型，具有音频和视觉之间的交叉模态对齐偏差，以增强唇部同步。此外，我们还介绍了BlendVOCA，这是一种语音音频和混合形状面部模型参数对的基准数据集，以解决公共资源的缺乏问题。我们的实验结果表明，所提出的方法在唇部同步方面达到了与基线相当或更好的性能，确保了更多样化的唇部运动，并简化了动画流程。

    Speech-driven 3D facial animation is challenging due to the scarcity of large-scale visual-audio datasets despite extensive research. Most prior works, typically focused on learning regression models on a small dataset using the method of least squares, encounter difficulties generating diverse lip movements from speech and require substantial effort in refining the generated outputs. To address these issues, we propose a speech-driven 3D facial animation with a diffusion model (SAiD), a lightweight Transformer-based U-Net with a cross-modality alignment bias between audio and visual to enhance lip synchronization. Moreover, we introduce BlendVOCA, a benchmark dataset of pairs of speech audio and parameters of a blendshape facial model, to address the scarcity of public resources. Our experimental results demonstrate that the proposed approach achieves comparable or superior performance in lip synchronization to baselines, ensures more diverse lip movements, and streamlines the animati
    
[^107]: 深度脉冲耦合神经网络

    Deep Pulse-Coupled Neural Networks. (arXiv:2401.08649v1 [cs.NE])

    [http://arxiv.org/abs/2401.08649](http://arxiv.org/abs/2401.08649)

    本文提出了一种利用脉冲耦合神经网络（DPCNNs）来提高脉冲神经网络（SNNs）在视觉任务中的表达能力和识别性能的方法。

    

    脉冲神经网络（SNNs）利用脉冲神经元（如泄漏积分类神经元）捕捉大脑的信息处理机制，该神经元包含时间动力学，通过离散和异步的脉冲传递信息。然而，泄漏积分类神经元的简化生物特性忽略了真实神经元的神经元耦合和树突结构，这限制了神经元的时空动力学，从而降低了结果SNNs的表达能力。在这项工作中，我们利用一种更具生物学可行性的神经模型，即脉冲耦合神经网络（PCNN），来改进SNNs在视觉任务中的表达能力和识别性能。PCNN是一种能够模拟主要视觉皮层中复杂神经活动的皮层模型。我们通过用PCNN神经元取代SNNs中常用的泄漏积分类神经元构建了深度脉冲耦合神经网络（DPCNNs）。

    Spiking Neural Networks (SNNs) capture the information processing mechanism of the brain by taking advantage of spiking neurons, such as the Leaky Integrate-and-Fire (LIF) model neuron, which incorporates temporal dynamics and transmits information via discrete and asynchronous spikes. However, the simplified biological properties of LIF ignore the neuronal coupling and dendritic structure of real neurons, which limits the spatio-temporal dynamics of neurons and thus reduce the expressive power of the resulting SNNs. In this work, we leverage a more biologically plausible neural model with complex dynamics, i.e., a pulse-coupled neural network (PCNN), to improve the expressiveness and recognition performance of SNNs for vision tasks. The PCNN is a type of cortical model capable of emulating the complex neuronal activities in the primary visual cortex. We construct deep pulse-coupled neural networks (DPCNNs) by replacing commonly used LIF neurons in SNNs with PCNN neurons. The intra-cou
    
[^108]: 一步扩散蒸馏：通过深度平衡模型

    One-Step Diffusion Distillation via Deep Equilibrium Models. (arXiv:2401.08639v1 [cs.CV])

    [http://arxiv.org/abs/2401.08639](http://arxiv.org/abs/2401.08639)

    本文通过深度平衡模型实现了一步扩散蒸馏，将生成过程蒸馏到更快的网络中，取得了比现有一步方法更好的性能。

    

    扩散模型在生成高质量样本方面表现出色，但需要大量迭代，因此需要尝试将生成过程蒸馏到更快的网络中。然而，许多现有方法存在各种挑战：蒸馏训练过程复杂，通常需要多个训练阶段，而生成应用中使用这些模型的性能较差。本文介绍了一种简单而有效的方法，将扩散模型直接从初始噪声蒸馏到生成的图像。我们的方法中一个重要的方面是利用一种新的深度平衡模型作为蒸馏结构：生成型平衡变压器（GET）。我们的方法可以完全离线训练，只需使用扩散模型的噪声/图像对，而且在相当的训练预算下，实现了比现有一步方法更好的性能。我们证明了DEQ架构是有效的。

    Diffusion models excel at producing high-quality samples but naively require hundreds of iterations, prompting multiple attempts to distill the generation process into a faster network. However, many existing approaches suffer from a variety of challenges: the process for distillation training can be complex, often requiring multiple training stages, and the resulting models perform poorly when utilized in single-step generative applications. In this paper, we introduce a simple yet effective means of distilling diffusion models directly from initial noise to the resulting image. Of particular importance to our approach is to leverage a new Deep Equilibrium (DEQ) model as the distilled architecture: the Generative Equilibrium Transformer (GET). Our method enables fully offline training with just noise/image pairs from the diffusion model while achieving superior performance compared to existing one-step methods on comparable training budgets. We demonstrate that the DEQ architecture is
    
[^109]: 通过MCU上微型AI加速器的动态组合实现协作推断

    Collaborative Inference via Dynamic Composition of Tiny AI Accelerators on MCUs. (arXiv:2401.08637v1 [cs.DC])

    [http://arxiv.org/abs/2401.08637](http://arxiv.org/abs/2401.08637)

    该论文介绍了Synergy，一个通过动态组合微型AI加速器来进行协作推断的系统，有效地解决了在设备上AI需求不断增长时tinyML面临的关键挑战。Synergy通过提供虚拟计算空间和运行时编排模块，实现了资源的统一虚拟化视图和跨动态/异构加速器的最佳推断，其吞吐量平均提升了8.0倍。

    

    微型AI加速器的出现为深度神经网络在极限边缘上的部署提供了机会，提供了较低的延迟、较低的功耗成本和改进的隐私保护。尽管取得了这些进展，但由于这些加速器的固有限制，如有限的内存和单设备焦点，仍存在挑战。本文介绍了Synergy，一个能够为多租户模型动态组合微型AI加速器的系统，有效解决了对于设备上AI的需求不断增长时tinyML面临的关键挑战。Synergy的一个关键特性是其提供了虚拟计算空间，为资源提供了统一的虚拟化视图，从而实现了对物理设备的高效任务映射。Synergy的运行时编排模块确保了跨动态和异构加速器的最佳推断。我们的评估结果显示，与基准相比，Synergy的吞吐量平均提升了8.0倍。

    The advent of tiny AI accelerators opens opportunities for deep neural network deployment at the extreme edge, offering reduced latency, lower power cost, and improved privacy in on-device ML inference. Despite these advancements, challenges persist due to inherent limitations of these accelerators, such as restricted onboard memory and single-device focus. This paper introduces Synergy, a system that dynamically composes tiny AI accelerators for multi-tenant models, effectively addressing tinyML's critical challenges for the increasing demand for on-device AI. A key feature of Synergy is its virtual computing space, providing a unified, virtualized view of resources and enabling efficient task mapping to physical devices. Synergy's runtime orchestration module ensures optimal inference across dynamic and heterogeneous accelerators. Our evaluations with 7 baselines and 8 models demonstrate that Synergy improves throughput by an average of 8.0X compared to baselines.
    
[^110]: 将质量多样性与描述符条件加强学习相结合

    Synergizing Quality-Diversity with Descriptor-Conditioned Reinforcement Learning. (arXiv:2401.08632v1 [cs.NE])

    [http://arxiv.org/abs/2401.08632](http://arxiv.org/abs/2401.08632)

    将质量多样性优化与描述符条件加强学习相结合，以克服进化算法的局限性，并在生成既多样又高性能的解决方案集合方面取得成功。

    

    智能的基本特征之一是找到新颖和有创造性的解决方案来解决给定的挑战或适应未预料到的情况。质量多样性优化是一类进化算法，可以生成既多样又高性能的解决方案集合。其中，MAP-Elites是一个著名的例子，已成功应用于各种领域，包括进化机器人学。然而，MAP-Elites通过遗传算法的随机突变进行发散搜索，因此仅限于进化低维解决方案的种群。PGA-MAP-Elites通过受深度强化学习启发的基于梯度的变异算子克服了这一限制，从而实现了大型神经网络的进化。尽管在许多环境中性能优秀，但PGA-MAP-Elites在一些任务中失败，其中基于梯度的变异算子的收敛搜索阻碍了多样性。在这项工作中，我们...

    A fundamental trait of intelligence involves finding novel and creative solutions to address a given challenge or to adapt to unforeseen situations. Reflecting this, Quality-Diversity optimization is a family of Evolutionary Algorithms, that generates collections of both diverse and high-performing solutions. Among these, MAP-Elites is a prominent example, that has been successfully applied to a variety of domains, including evolutionary robotics. However, MAP-Elites performs a divergent search with random mutations originating from Genetic Algorithms, and thus, is limited to evolving populations of low-dimensional solutions. PGA-MAP-Elites overcomes this limitation using a gradient-based variation operator inspired by deep reinforcement learning which enables the evolution of large neural networks. Although high-performing in many environments, PGA-MAP-Elites fails on several tasks where the convergent search of the gradient-based variation operator hinders diversity. In this work, we
    
[^111]: 使用图神经网络预测和解释金属玻璃的能垒

    Predicting and Interpreting Energy Barriers of Metallic Glasses with Graph Neural Networks. (arXiv:2401.08627v1 [cond-mat.dis-nn])

    [http://arxiv.org/abs/2401.08627](http://arxiv.org/abs/2401.08627)

    本研究使用图神经网络建模原子图结构并预测金属玻璃的能垒，通过提出的对称图神经网络模型，实现了在结构正交变换下的不变性，为理解金属玻璃的局部结构与物理性质关系提供了新的方法。

    

    金属玻璃是广泛使用的无序材料。理解金属玻璃的局部结构与物理性质之间的关系是材料科学和凝聚态物理学的最大挑战之一。本研究利用图神经网络来建模原子图结构，并研究结构与相应的局部能垒之间的联系，据信能够控制金属玻璃的许多关键物理性质。我们的一个重要贡献是提出了一种用于预测能垒的新型对称图神经网络（SymGNN）模型，该模型在结构的正交变换下具有不变性，例如旋转和反射。这种不变性是标准图神经网络如图卷积网络无法捕捉的性质。SymGNN通过聚合图结构的正交变换来进行表示学习，并对所有三维正交变换进行最优分布处理。

    Metallic Glasses (MGs) are widely used disordered materials. Understanding the relationship between the local structure and physical properties of MGs is one of the greatest challenges for both material science and condensed matter physics. In this work, we utilize Graph Neural Networks (GNNs) to model the atomic graph structure and study the connection between the structure and the corresponding local energy barrier, which is believed to govern many critical physical properties in MGs. One of our key contributions is to propose a novel Symmetrized GNN (SymGNN) model for predicting the energy barriers, which is invariant under orthogonal transformations of the structure, e.g., rotations and reflections. Such invariance is a desired property that standard GNNs like Graph Convolutional Networks cannot capture. SymGNNs handle the invariance by aggregating over orthogonal transformations of the graph structure for representation learning, and an optimal distribution over all 3D orthogonal 
    
[^112]: Wake-Sleep Consolidated Learning（WSCL）的研究

    Wake-Sleep Consolidated Learning. (arXiv:2401.08623v1 [cs.NE])

    [http://arxiv.org/abs/2401.08623](http://arxiv.org/abs/2401.08623)

    Wake-Sleep Consolidated Learning（WSCL）是一种借鉴人脑觉醒-睡眠阶段的学习策略，用于改进深度神经网络在连续学习中的视觉分类任务。该方法通过觉醒和睡眠阶段之间的同步学习来实现持续学习，觉醒阶段中模型适应感官输入并利用动态参数冻结机制保持稳定，睡眠阶段根据NREM和REM阶段对模型的突触权重进行巩固和调整，强化重要连接并削弱不重要的连接。

    

    我们提出了 Wake-Sleep Consolidated Learning（WSCL）的学习策略，利用互补学习系统理论和人脑的觉醒-睡眠阶段来改进深度神经网络在连续学习设置中的视觉分类任务表现。我们的方法通过觉醒和睡眠阶段之间的同步学习来实现持续学习。在觉醒阶段，模型暴露于感官输入并调整其表示，通过动态参数冻结机制确保稳定性，并将情节记忆存储在短期临时记忆中（类似于海马体中的情况）。在睡眠阶段，训练过程分为NREM和REM阶段。在NREM阶段，模型的突触权重利用来自短期和长期记忆的回放样本进行巩固，并激活突触可塑性机制，强化重要连接并削弱不重要的连接。在REM阶段，模型...

    We propose Wake-Sleep Consolidated Learning (WSCL), a learning strategy leveraging Complementary Learning System theory and the wake-sleep phases of the human brain to improve the performance of deep neural networks for visual classification tasks in continual learning settings. Our method learns continually via the synchronization between distinct wake and sleep phases. During the wake phase, the model is exposed to sensory input and adapts its representations, ensuring stability through a dynamic parameter freezing mechanism and storing episodic memories in a short-term temporary memory (similarly to what happens in the hippocampus). During the sleep phase, the training process is split into NREM and REM stages. In the NREM stage, the model's synaptic weights are consolidated using replayed samples from the short-term and long-term memory and the synaptic plasticity mechanism is activated, strengthening important connections and weakening unimportant ones. In the REM stage, the model
    
[^113]: MATE-Pred: 多模态基于注意力的TCR-Epitope相互作用预测器

    MATE-Pred: Multimodal Attention-based TCR-Epitope interaction Predictor. (arXiv:2401.08619v1 [cs.LG])

    [http://arxiv.org/abs/2401.08619](http://arxiv.org/abs/2401.08619)

    MATE-Pred是一种多模态基于注意力的TCR-Epitope相互作用预测器，通过整合进化特征和预训练的语言模型，准确预测T细胞受体和表位的结合亲和力。

    

    准确地预测T细胞受体和表位之间的结合亲和力对于开发成功的免疫疗法策略至关重要。一些最先进的计算方法通过将细胞受体和表位序列的氨基酸残基转化为数值来实现深度学习技术，并集成了进化特征，而另一些方法则利用预训练的语言模型，在氨基酸残基层面上总结嵌入向量以获取序列级表示。在这里，我们提出了一种高度可靠的新方法MATE-Pred，它通过多模态注意力进行T细胞受体和表位结合亲和力的预测。MATE-Pred与其他利用T细胞受体和表位的多模态表示的深度学习模型进行了比较和基准测试。在提出的方法中，蛋白质的文本表示以预训练的双向编码器模型进行嵌入，并与两种附加特征进行融合。

    An accurate binding affinity prediction between T-cell receptors and epitopes contributes decisively to develop successful immunotherapy strategies. Some state-of-the-art computational methods implement deep learning techniques by integrating evolutionary features to convert the amino acid residues of cell receptors and epitope sequences into numerical values, while some other methods employ pre-trained language models to summarize the embedding vectors at the amino acid residue level to obtain sequence-wise representations.  Here, we propose a highly reliable novel method, MATE-Pred, that performs multi-modal attention-based prediction of T-cell receptors and epitopes binding affinity. The MATE-Pred is compared and benchmarked with other deep learning models that leverage multi-modal representations of T-cell receptors and epitopes. In the proposed method, the textual representation of proteins is embedded with a pre-trained bi-directional encoder model and combined with two additiona
    
[^114]: 基于分解编码器设计的生物启发式Hebbian学习中的表示学习

    Representation Learning in a Decomposed Encoder Design for Bio-inspired Hebbian Learning. (arXiv:2401.08603v1 [cs.NE])

    [http://arxiv.org/abs/2401.08603](http://arxiv.org/abs/2401.08603)

    这项研究探索了在生物启发式Hebbian学习中的表示学习，并提出了一个模块化框架，利用不同的不变视觉描述符作为归纳偏见。该框架在图像分类任务上展示了较好的鲁棒性和透明度。

    

    现代数据驱动的机器学习系统设计利用了对架构结构的归纳偏见、不变性和等变性要求、任务特定的损失函数以及计算优化工具。先前的工作表明，编码器的早期层中的归纳偏见，以人为指定的准不变滤波器的形式，可以作为一种强大的归纳偏见，实现更好的鲁棒性和透明度。本文在生物启发式Hebbian学习的表示学习上进一步探索了这一点。我们提出了一个模块化框架，使用生物启发式的对比预测编码（Hinge CLAPP Loss）进行训练。我们的框架由多个并行编码器组成，每个编码器利用不同的不变视觉描述符作为归纳偏见。我们在不同难度的图像数据上的分类场景中评估了我们系统的表示学习能力（GTSRB, STL10, CODEBR）

    Modern data-driven machine learning system designs exploit inductive biases on architectural structure, invariance and equivariance requirements, task specific loss functions, and computational optimization tools. Previous works have illustrated that inductive bias in the early layers of the encoder in the form of human specified quasi-invariant filters can serve as a powerful inductive bias to attain better robustness and transparency in learned classifiers. This paper explores this further in the context of representation learning with local plasticity rules i.e. bio-inspired Hebbian learning . We propose a modular framework trained with a bio-inspired variant of contrastive predictive coding (Hinge CLAPP Loss). Our framework is composed of parallel encoders each leveraging a different invariant visual descriptor as an inductive bias. We evaluate the representation learning capacity of our system in a classification scenario on image data of various difficulties (GTSRB, STL10, CODEBR
    
[^115]: 学习化学合成与电合成——有区别吗?

    Learning with Chemical versus Electrical Synapses -- Does it Make a Difference?. (arXiv:2401.08602v1 [cs.NE])

    [http://arxiv.org/abs/2401.08602](http://arxiv.org/abs/2401.08602)

    本文研究了在稀疏和全连接网络中使用化学合成与电合成的影响，并通过自动驾驶模拟器的实验来评估它们在不同条件下的性能。

    

    生物启发的神经网络有助于推动我们对神经计算的理解，并改进人工智能系统的现状。生物电化学合成直接通过使神经元之间的电流快速流动来传递神经信号。相反，生物化学合成通过神经递质间接传递神经信号。之前的研究表明，在稀疏的、生物启发的架构——神经回路策略(NCPs)中使用化学合成可以实现复杂机器人控制的可解释性动态。然而，在相同的架构中比较这两种突触模型的影响尚未被探索。在这项研究中，我们旨在确定在稀疏和全连接网络中使用化学合成与电合成的影响。我们通过逼真的自动驾驶模拟器进行自主车道保持的实验，在不同条件下以及存在噪声情况下评估它们的性能。

    Bio-inspired neural networks have the potential to advance our understanding of neural computation and improve the state-of-the-art of AI systems. Bio-electrical synapses directly transmit neural signals, by enabling fast current flow between neurons. In contrast, bio-chemical synapses transmit neural signals indirectly, through neurotransmitters. Prior work showed that interpretable dynamics for complex robotic control, can be achieved by using chemical synapses, within a sparse, bio-inspired architecture, called Neural Circuit Policies (NCPs). However, a comparison of these two synaptic models, within the same architecture, remains an unexplored area. In this work we aim to determine the impact of using chemical synapses compared to electrical synapses, in both sparse and all-to-all connected networks. We conduct experiments with autonomous lane-keeping through a photorealistic autonomous driving simulator to evaluate their performance under diverse conditions and in the presence of 
    
[^116]: Nahid: 基于人工智能算法的完全自动手术

    Nahid: AI-based Algorithm for operating fully-automatic surgery. (arXiv:2401.08584v1 [cs.CV])

    [http://arxiv.org/abs/2401.08584](http://arxiv.org/abs/2401.08584)

    本文提出了一种基于软件和计算机视觉技术的全自动手术方法，能够自动诊断和治疗孤立性卵巢子宫内膜异位症。

    

    在本文中，首次提出了一种基于软件和计算机视觉技术的全自动手术方法。然后，对医疗手术的计算机化优势和挑战进行了研究。最后，对孤立性卵巢子宫内膜异位症相关的手术进行了研究，并基于提出的方法，提出了一种更详细的算法，能够在手术过程中自动诊断和治疗该疾病，其中使用了一个U-net模型来检测手术过程中的子宫内膜异位症。

    In this paper, for the first time, a method is presented that can provide a fully automated surgery based on software and computer vision techniques. Then, the advantages and challenges of computerization of medical surgery are examined. Finally, the surgery related to isolated ovarian endometriosis disease has been examined, and based on the presented method, a more detailed algorithm is presented that is capable of automatically diagnosing and treating this disease during surgery as proof of our proposed method where a U-net is trained to detect the endometriosis during surgery.
    
[^117]: 时间嵌入：从时空数据中进行可扩展的自监督时序表示学习，用于多模态计算机视觉

    Temporal Embeddings: Scalable Self-Supervised Temporal Representation Learning from Spatiotemporal Data for Multimodal Computer Vision. (arXiv:2401.08581v1 [cs.CV])

    [http://arxiv.org/abs/2401.08581](http://arxiv.org/abs/2401.08581)

    该论文提出了一种自监督的时间嵌入方法，可以将地理活动的时间模式与土地利用类型相对应。通过将时间序列信号转换到频域并压缩为语义分割所需的图像通道，时间嵌入可以有效地表示时间序列数据，并用于多个地理空间任务。

    

    地理活动的时间模式与土地利用类型之间存在相关性。提出了一种新颖的自监督方法，通过压缩自编码器将时间序列信号转换到频域，并将其转换为图像通道，用于下游地理空间任务的深度语义分割。实验表明，时间嵌入是时间序列数据的语义有意义的表示，对于分类住宅区和商业区等不同任务具有有效性。

    There exists a correlation between geospatial activity temporal patterns and type of land use. A novel self-supervised approach is proposed to stratify landscape based on mobility activity time series. First, the time series signal is transformed to the frequency domain and then compressed into task-agnostic temporal embeddings by a contractive autoencoder, which preserves cyclic temporal patterns observed in time series. The pixel-wise embeddings are converted to image-like channels that can be used for task-based, multimodal modeling of downstream geospatial tasks using deep semantic segmentation. Experiments show that temporal embeddings are semantically meaningful representations of time series data and are effective across different tasks such as classifying residential area and commercial areas. Temporal embeddings transform sequential, spatiotemporal motion trajectory data into semantically meaningful image-like tensor representations that can be combined (multimodal fusion) wit
    
[^118]: 利用层间专家亲和性加速混合专家模型推理

    Exploiting Inter-Layer Expert Affinity for Accelerating Mixture-of-Experts Model Inference. (arXiv:2401.08383v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.08383](http://arxiv.org/abs/2401.08383)

    本文提出了一种称为ExFlow的轻量级优化技术，通过利用层间专家亲和性，大大加速了混合专家模型推理的过程。

    

    在像生成预训练变压器这样的大型语言模型中，混合专家范式已经成为增强模型表达能力和准确性的强大技术。然而，将GPT MoE模型部署到分布式系统上进行并行推理面临着重大挑战，主要是由于专家路由和聚合所需的广泛Alltoall通信。这种通信瓶颈加剧了已经复杂的计算环境，从而妨碍了高性能计算资源的高效利用。在本文中，我们提出了一种轻量级的优化技术ExFlow，以大大加速这些MoE模型的推理。我们从利用层间专家亲和性的新视角来减轻通信开销。与之前的方法不同的是，我们的解决方案可以直接应用于预训练的MoE模型，无需任何微调或精度下降。通过提出一个上下文连贯的专家并行性

    In large language models like the Generative Pre-trained Transformer, the Mixture of Experts paradigm has emerged as a powerful technique for enhancing model expressiveness and accuracy. However, deploying GPT MoE models for parallel inference on distributed systems presents significant challenges, primarily due to the extensive Alltoall communication required for expert routing and aggregation. This communication bottleneck exacerbates the already complex computational landscape, hindering the efficient utilization of high-performance computing resources. In this paper, we propose a lightweight optimization technique called ExFlow, to largely accelerate the inference of these MoE models. We take a new perspective on alleviating the communication overhead by exploiting the inter-layer expert affinity. Unlike previous methods, our solution can be directly applied to pre-trained MoE models without any fine-tuning or accuracy degradation. By proposing a context-coherent expert parallelism
    
[^119]: 一种可解释的多标签音频分割代理模型

    An Explainable Proxy Model for Multiabel Audio Segmentation. (arXiv:2401.08268v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2401.08268](http://arxiv.org/abs/2401.08268)

    该论文提出了一种可解释的多标签音频分割代理模型，通过使用非负矩阵分解将嵌入映射到频域，实现了对语音活动、音乐、噪音和重叠语音的同时检测，并且具有强大的可解释性特性。

    

    音频信号分割是自动音频索引的关键任务。它包括在信号中检测类同质片段的边界。在许多应用中，可解释的人工智能是透明决策的重要过程。在本文中，我们提出了一种可解释的多标签分割模型，同时解决了语音活动（SAD）、音乐（MD）、噪音（ND）和重叠语音检测（OSD）。该代理模型使用非负矩阵分解（NMF）将用于分割的嵌入映射到频域。在两个数据集上进行的实验表明，与预训练的黑盒模型相比表现相似，同时显示出强大的可解释性功能。具体而言，决策所使用的频率区间可以在片段级别（局部解释）和全局级别（类原型）上轻松识别出来。

    Audio signal segmentation is a key task for automatic audio indexing. It consists of detecting the boundaries of class-homogeneous segments in the signal. In many applications, explainable AI is a vital process for transparency of decision-making with machine learning. In this paper, we propose an explainable multilabel segmentation model that solves speech activity (SAD), music (MD), noise (ND), and overlapped speech detection (OSD) simultaneously. This proxy uses the non-negative matrix factorization (NMF) to map the embedding used for the segmentation to the frequency domain. Experiments conducted on two datasets show similar performances as the pre-trained black box model while showing strong explainability features. Specifically, the frequency bins used for the decision can be easily identified at both the segment level (local explanations) and global level (class prototypes).
    
[^120]: 基于超图的矩阵补全：尖锐阈值和高效算法

    Matrix Completion with Hypergraphs:Sharp Thresholds and Efficient Algorithms. (arXiv:2401.08197v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.08197](http://arxiv.org/abs/2401.08197)

    本论文研究了基于子采样矩阵条目、观察到的社交图和超图的矩阵补全问题。我们发现了一个尖锐阈值，可以精确补全评分矩阵。通过量化超图的“质量”函数，我们可以评估超图利用对样本概率的影响。通过开发高效算法，我们在高概率情况下成功地完成了矩阵补全任务。

    

    该论文研究了基于子采样矩阵条目以及观察到的社交图和超图的补全评分矩阵的问题。我们证明了在样本概率上存在一个尖锐阈值，用于精确完成评分矩阵的任务，当样本概率高于阈值时，任务可完成，反之则不可能，这展示了一个相变现象。阈值可以作为超图的“质量”函数来表示，从而使我们能够量化由于超图利用而导致的样本概率减少量，这也突显了超图在矩阵补全问题中的有用性。在发现尖锐阈值的过程中，我们开发了一种计算高效的矩阵补全算法，该算法有效地利用了观察到的图和超图。理论分析表明，只要样本概率高于某个阈值，我们的算法可以高概率地成功。

    This paper considers the problem of completing a rating matrix based on sub-sampled matrix entries as well as observed social graphs and hypergraphs. We show that there exists a \emph{sharp threshold} on the sample probability for the task of exactly completing the rating matrix -- the task is achievable when the sample probability is above the threshold, and is impossible otherwise -- demonstrating a phase transition phenomenon. The threshold can be expressed as a function of the ``quality'' of hypergraphs, enabling us to \emph{quantify} the amount of reduction in sample probability due to the exploitation of hypergraphs. This also highlights the usefulness of hypergraphs in the matrix completion problem. En route to discovering the sharp threshold, we develop a computationally efficient matrix completion algorithm that effectively exploits the observed graphs and hypergraphs. Theoretical analyses show that our algorithm succeeds with high probability as long as the sample probability
    
[^121]: 在Transformer模型中实现进位算法

    Carrying over algorithm in transformers. (arXiv:2401.07993v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.07993](http://arxiv.org/abs/2401.07993)

    本文研究了在Transformer模型中实现进位算法的方式，发现在两层的仅编码器模型中，第一层负责相同位置的数字相加，第二层根据注意力机制决定是否需要进位，并通过多层感知机来执行进位操作。

    

    加法可能是最简单的算术任务之一，通常使用进位算法进行计算。本文研究了Transformer模型如何实现这个算法，以及如何将两个任务分配给网络的不同部分。我们首先关注两层的仅编码器模型，发现进位算法以一种模块化的方式实现。第一层主要负责在相同位置上添加数字。第二层首先在注意力机制中决定哪些位置需要进位，然后在最终的多层感知机中执行进位操作。我们提供了一种简单的方法来精确定位负责这个任务的神经元。进位算法的这种实现在两层和三层模型的一系列超参数中都存在。对于小型的仅解码器模型

    Addition is perhaps one of the simplest arithmetic tasks one can think of and is usually performed using the carrying over algorithm. This algorithm consists of two tasks: adding digits in the same position and carrying over a one whenever necessary. We study how transformer models implement this algorithm and how the two aforementioned tasks are allocated to different parts of the network. We first focus on two-layer encoder-only models and show that the carrying over algorithm is implemented in a modular fashion. The first layer is mostly responsible for adding digits in the same position. The second layer first decides, in the attention, which positions need a carried one or not, and then performs the carrying of the one in the final MLP. We provide a simple way of precisely identifying which neurons are responsible for that task. This implementation of the carrying over algorithm occurs across a range of hyperparameters for two as well as three-layer models. For small decoder-only 
    
[^122]: 基于深度进化的即时兴趣网络用于触发引导推荐中的CTR预测

    Deep Evolutional Instant Interest Network for CTR Prediction in Trigger-Induced Recommendation. (arXiv:2401.07769v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2401.07769](http://arxiv.org/abs/2401.07769)

    这篇论文提出了一种基于深度进化的即时兴趣网络（DEI2N）来解决触发引导推荐（TIR）中的点击率预测问题。该方法考虑了用户行为的时间信息、即时兴趣的动态变化以及触发项和目标项之间的交互。

    

    推荐在许多行业中发挥着关键作用，例如电子商务、流媒体、社交媒体等。最近，出现了一种新的推荐场景，称为触发引导推荐（TIR），用户可以通过触发项明确表达他们的即时兴趣，在许多电子商务平台（如阿里巴巴和亚马逊）中起着重要作用。传统的推荐方法通常无法明确建模用户的即时兴趣，因此在TIR中获得次优结果。尽管有一些同时考虑触发项和目标项的方法来解决这个问题，但它们仍未考虑用户行为的时间信息、用户向下滚动时即时兴趣的动态变化以及触发项和目标项之间的交互。为了解决这些问题，我们提出了一种新的方法--深度进化的即时兴趣网络（DEI2N），用于TIR场景中的点击率预测。

    The recommendation has been playing a key role in many industries, e.g., e-commerce, streaming media, social media, etc. Recently, a new recommendation scenario, called Trigger-Induced Recommendation (TIR), where users are able to explicitly express their instant interests via trigger items, is emerging as an essential role in many e-commerce platforms, e.g., Alibaba.com and Amazon. Without explicitly modeling the user's instant interest, traditional recommendation methods usually obtain sub-optimal results in TIR. Even though there are a few methods considering the trigger and target items simultaneously to solve this problem, they still haven't taken into account temporal information of user behaviors, the dynamic change of user instant interest when the user scrolls down and the interactions between the trigger and target items. To tackle these problems, we propose a novel method -- Deep Evolutional Instant Interest Network (DEI2N), for click-through rate prediction in TIR scenarios
    
[^123]: CLSA-CIM: 一种用于计算内存架构的跨层调度方法

    CLSA-CIM: A Cross-Layer Scheduling Approach for Computing-in-Memory Architectures. (arXiv:2401.07671v2 [cs.AR] UPDATED)

    [http://arxiv.org/abs/2401.07671](http://arxiv.org/abs/2401.07671)

    CLSA-CIM是一种用于计算内存架构的跨层调度算法，能够提高平铺CIM架构的利用率，加速计算。

    

    高效的机器学习加速器需求迅速增长，推动了新型计算概念的发展，如基于阻性随机存取存储器（RRAM）的平铺计算内存（CIM）架构。CIM允许在内存单元内进行计算，从而加快数据处理速度，降低功耗。高效的编译器算法对于充分利用平铺CIM架构的潜力至关重要。传统的机器学习编译器专注于为CPU、GPU和其他冯·诺伊曼架构生成代码，需要进行适应，以覆盖CIM架构。跨层调度是一种有前途的方法，它提高了CIM核心的利用率，从而加速了计算。尽管类似的概念在以前的工作中隐含地使用，但对于平铺CIM架构的跨层调度缺乏清晰且可量化的算法定义。为了弥补这个空白，我们提出了CLSA-CIM，一种适用于平铺CIM架构的跨层调度算法。

    The demand for efficient machine learning (ML) accelerators is growing rapidly, driving the development of novel computing concepts such as resistive random access memory (RRAM)-based tiled computing-in-memory (CIM) architectures. CIM allows to compute within the memory unit, resulting in faster data processing and reduced power consumption. Efficient compiler algorithms are essential to exploit the potential of tiled CIM architectures. While conventional ML compilers focus on code generation for CPUs, GPUs, and other von Neumann architectures, adaptations are needed to cover CIM architectures. Cross-layer scheduling is a promising approach, as it enhances the utilization of CIM cores, thereby accelerating computations. Although similar concepts are implicitly used in previous work, there is a lack of clear and quantifiable algorithmic definitions for cross-layer scheduling for tiled CIM architectures. To close this gap, we present CLSA-CIM, a cross-layer scheduling algorithm for tiled
    
[^124]: E3x：简化的$\mathrm{E}(3)$等变深度学习

    E3x: $\mathrm{E}(3)$-Equivariant Deep Learning Made Easy. (arXiv:2401.07595v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.07595](http://arxiv.org/abs/2401.07595)

    E3x是一种简化了$\mathrm{E}(3)$等变深度学习的软件包，通过内置等变性实现更高的数据效率和准确性。

    

    本文介绍了E3x，一种用于构建神经网络的软件包，该网络在三维空间的平移、旋转和反射方面等变。与普通神经网络相比，$\mathrm{E}(3)$-等变模型在输入和/或输出数据是与三维对象相关的数量时具有优势。这是因为此类数量（例如位置）的数值通常取决于所选择的坐标系统。在参考系的变换下，这些值会可预测地发生变化，但对于普通的机器学习模型来说，学习其潜在规则可能很困难。使用内置的$\mathrm{E}(3)$-等变性，神经网络可以保证完全满足相关的变换规则，从而实现更高的数据效率和准确性。E3x的代码可从https://github.com/google-research/e3x获得，还提供了详细的文档和使用示例。

    This work introduces E3x, a software package for building neural networks that are equivariant with respect to the Euclidean group $\mathrm{E}(3)$, consisting of translations, rotations, and reflections of three-dimensional space. Compared to ordinary neural networks, $\mathrm{E}(3)$-equivariant models promise benefits whenever input and/or output data are quantities associated with three-dimensional objects. This is because the numeric values of such quantities (e.g. positions) typically depend on the chosen coordinate system. Under transformations of the reference frame, the values change predictably, but the underlying rules can be difficult to learn for ordinary machine learning models. With built-in $\mathrm{E}(3)$-equivariance, neural networks are guaranteed to satisfy the relevant transformation rules exactly, resulting in superior data efficiency and accuracy. The code for E3x is available from https://github.com/google-research/e3x, detailed documentation and usage examples ca
    
[^125]: 利用先验知识发现具有未观测变量的因果可加模型及其在时间序列数据中的应用

    Use of Prior Knowledge to Discover Causal Additive Models with Unobserved Variables and its Application to Time Series Data. (arXiv:2401.07231v1 [cs.LG])

    [http://arxiv.org/abs/2401.07231](http://arxiv.org/abs/2401.07231)

    本文提出了两种用于具有未观测变量的因果可加模型（CAM-UV）的方法，并扩展了这些方法以应用于时间序列数据。这些方法利用先验知识进行高效因果发现，并具有对因果关系顺序的特殊处理。

    

    本文提出了两种用于具有未观测变量的因果可加模型（CAM-UV）的方法。CAM-UV假设因果函数采用广义可加模型的形式，并存在潜在的混淆变量。首先，我们提出了一种利用先验知识进行高效因果发现的方法。然后，我们扩展了这种方法，用于推断时间序列数据的因果关系。与其他现有的因果函数模型不同，原始的CAM-UV算法不寻求观测变量之间的因果顺序，而是旨在确定每个观测变量的原因。因此，本文中提出的第一种方法利用先验知识，例如理解某些变量不能成为特定变量的原因。此外，通过融入因果在时间上的先验知识，我们将第一个算法扩展为第二种用于时间序列数据中的因果发现的方法。我们验证了第一个提出的方法。

    This paper proposes two methods for causal additive models with unobserved variables (CAM-UV). CAM-UV assumes that the causal functions take the form of generalized additive models and that latent confounders are present. First, we propose a method that leverages prior knowledge for efficient causal discovery. Then, we propose an extension of this method for inferring causality in time series data. The original CAM-UV algorithm differs from other existing causal function models in that it does not seek the causal order between observed variables, but rather aims to identify the causes for each observed variable. Therefore, the first proposed method in this paper utilizes prior knowledge, such as understanding that certain variables cannot be causes of specific others. Moreover, by incorporating the prior knowledge that causes precedes their effects in time, we extend the first algorithm to the second method for causal discovery in time series data. We validate the first proposed method
    
[^126]: 错误有界在线学习中反馈价格的界限

    Bounds on the price of feedback for mistake-bounded online learning. (arXiv:2401.05794v1 [cs.LG])

    [http://arxiv.org/abs/2401.05794](http://arxiv.org/abs/2401.05794)

    改进了错误有界在线学习中反馈价格的上下界，还解决了多类学习中标准反馈与赌徒反馈的价格问题。

    

    我们改进了(Auer和Long，Machine Learning，1999)中各种在线学习场景的若干最坏情况界限。特别地，我们将延迟模棱两可强化学习的上界缩小了2倍，将函数族组合学习的上界缩小了2.41倍，将不知性学习的上界缩小了1.09倍。我们还改进了同一论文中函数族组合学习的下界，将其缩小到Θ(ln{k})的因子，与上界相匹配。此外，我们解决了(Lon，Theoretical Computer Science，2020)中关于多类学习标准反馈与赌徒反馈价格的问题，并将(Feng等人，Theoretical Computer Science，2023)中$r$-输入延迟模棱两可强化学习的上界缩小了$r$倍，与同一论文中的下界相匹配。

    We improve several worst-case bounds for various online learning scenarios from (Auer and Long, Machine Learning, 1999). In particular, we sharpen an upper bound for delayed ambiguous reinforcement learning by a factor of 2, an upper bound for learning compositions of families of functions by a factor of 2.41, and an upper bound for agnostic learning by a factor of 1.09. We also improve a lower bound from the same paper for learning compositions of $k$ families of functions by a factor of $\Theta(\ln{k})$, matching the upper bound up to a constant factor. In addition, we solve a problem from (Long, Theoretical Computer Science, 2020) on the price of bandit feedback with respect to standard feedback for multiclass learning, and we improve an upper bound from (Feng et al., Theoretical Computer Science, 2023) on the price of $r$-input delayed ambiguous reinforcement learning by a factor of $r$, matching a lower bound from the same paper up to the leading term.
    
[^127]: AUTOACT：通过自主规划实现的自动代理学习

    AUTOACT: Automatic Agent Learning from Scratch via Self-Planning. (arXiv:2401.05268v1 [cs.CL])

    [http://arxiv.org/abs/2401.05268](http://arxiv.org/abs/2401.05268)

    AUTOACT是一个自动代理学习框架，通过自主规划合成轨迹，不依赖于大规模数据和闭源模型，能够实现更好或类似的性能。

    

    语言代理在各种复杂任务上取得了相当的性能。尽管在这个领域进行了不断的探索，但现有的语言代理系统仍然面临昂贵、不可重复的数据依赖问题，并且面临将单一模型应用于多个功能的挑战。为此，我们介绍了AutoAct，这是一个自动代理学习框架，不依赖于大规模带注释的数据和来自闭源模型（如GPT-4）的合成轨迹。给定有限的数据和工具库，AutoAct首先自动合成规划轨迹，不需要人类或强闭源模型的任何辅助。然后，AutoAct利用分工策略，根据目标任务信息和合成轨迹自动区分，产生一个子代理组来完成任务。我们进行了多种LLMs的广泛实验，结果显示AutoAct在性能上优于或与其相当。

    Language agents have achieved considerable performance on various complex tasks. Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions. To this end, we introduce AutoAct, an automatic agent learning framework that does not rely on large-scale annotated data and synthetic trajectories from closed-source models (e.g., GPT-4). Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models. Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task. We conduct comprehensive experiments with different LLMs, which demonstrates that AutoAct yields better or parallel performance compared to var
    
[^128]: 微小时间混合器 (TTMs): 针对多变量时间序列的增强零/少样本预测的快速预训练模型

    Tiny Time Mixers (TTMs): Fast Pretrained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series. (arXiv:2401.03955v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.03955](http://arxiv.org/abs/2401.03955)

    本论文介绍了一种名为微小时间混合器 (TTMs) 的预训练模型，该模型针对多变量时间序列的零/少样本预测进行了优化。与大型预训练模型相比，TTMs模型更小、更快，并考虑了跨通道相关性，能够在短时间内进行有效的预测。

    

    零/少样本学习的大型预训练模型在语言和视觉领域表现出色，但在多变量时间序列 (TS) 中面临着多样性和公开预训练数据稀缺的挑战。因此，最近在时间序列预测中使用预训练的大型语言模型 (LLMs) 进行各种适应的趋势逐渐增加。这些方法利用跨领域迁移学习，出奇地取得了令人印象深刻的结果。然而，这些模型通常非常缓慢且庞大（大约十亿个参数），并且不考虑跨通道相关性。为了解决这个问题，我们提出了多层微小时间混合器 (TTM)，这是一种基于轻量级 TSMixer 结构的显著小型模型。TTM 是首个成功开发的微型通用预训练模型（≤100万个参数），专门在公开TS数据集上进行快速训练（仅需4-8小时），具有有效的迁移学习能力进行预测。

    Large Pretrained models for zero/few-shot learning excel in language and vision domains but encounter challenges in multivariate time series (TS) due to the diverse nature and scarcity of publicly available pretraining data. Consequently, there has been a recent surge in utilizing pretrained large language models (LLMs) with various adaptations for time series forecasting. These approaches employ cross-domain transfer learning and surprisingly yield impressive results. However, these models are typically very slow and large ($\sim$billion parameters) and do not consider cross-channel correlations. To address this, we present Multi-level Tiny Time Mixers (TTM), a significantly small model based on the lightweight TSMixer architecture. TTM marks the first success in developing tiny general-pretrained models ($\le$1 million parameters), exclusively trained on public TS datasets in a flash of just 4-8 hrs with effective transfer learning capabilities for forecasting. To tackle the complexi
    
[^129]: DiarizationLM: 基于大语言模型的说话人分离后处理

    DiarizationLM: Speaker Diarization Post-Processing with Large Language Models. (arXiv:2401.03506v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2401.03506](http://arxiv.org/abs/2401.03506)

    本文介绍了DiarizationLM框架，利用大语言模型对说话人分离系统的输出进行后处理。实验证明，使用finetuned的PaLM 2-S模型可以显著减少分离错误率，对多种目标都有优化效果。

    

    本文介绍了DiarizationLM，一个利用大语言模型（LLM）对说话人分离系统输出进行后处理的框架。这个框架可以实现多种目标，如改善分离对话转录的可读性，或减少词级分离错误率（WDER）。在这个框架中，自动语音识别（ASR）和说话人分离系统的输出被表示为一种紧凑的文本格式，其包含在一个可选择调整的LLM的提示中。LLM的输出可以作为所需改进的精细化分离结果。作为后处理步骤，该框架可以轻松应用于任何现有的ASR和说话人分离系统，无需重新训练现有的组件。我们的实验证明，finetuned的PaLM 2-S模型可以在Fisher电话对话数据集上将WDER降低55.5%，在Callhome英语数据集上降低44.9%。

    In this paper, we introduce DiarizationLM, a framework to leverage large language models (LLM) to post-process the outputs from a speaker diarization system. Various goals can be achieved with the proposed framework, such as improving the readability of the diarized transcript, or reducing the word diarization error rate (WDER). In this framework, the outputs of the automatic speech recognition (ASR) and speaker diarization systems are represented as a compact textual format, which is included in the prompt to an optionally finetuned LLM. The outputs of the LLM can be used as the refined diarization results with the desired enhancement. As a post-processing step, this framework can be easily applied to any off-the-shelf ASR and speaker diarization systems without retraining existing components. Our experiments show that a finetuned PaLM 2-S model can reduce the WDER by rel. 55.5% on the Fisher telephone conversation dataset, and rel. 44.9% on the Callhome English dataset.
    
[^130]: 时间序列预测中扩散模型的兴起

    The Rise of Diffusion Models in Time-Series Forecasting. (arXiv:2401.03006v1 [cs.LG])

    [http://arxiv.org/abs/2401.03006](http://arxiv.org/abs/2401.03006)

    本文调查了扩散模型在时间序列预测中的应用，提供了对这些模型的全面背景信息和详细说明，同时也对它们在不同数据集上的有效性和彼此之间的比较进行了分析。其贡献包括对扩散模型在时间序列预测中应用的彻底探索和按时间顺序排序的模型概述。这是一份对人工智能和时间序列分析领域的研究人员来说具有价值的资源。

    

    本调查探讨了扩散模型在时间序列预测中的应用。扩散模型在生成型人工智能的各个领域中展示出最先进的结果。本文包括对扩散模型的全面背景信息，详细介绍其条件方法，并审查了其在时间序列预测中的应用。分析涵盖了11个具体的时间序列实现，它们的直觉和理论基础，不同数据集上的有效性以及彼此之间的比较。该工作的关键贡献是对扩散模型在时间序列预测中应用的彻底探索，并提供了一个按时间顺序排序的模型概述。此外，本文对该领域的最新技术水平进行了深入讨论，并概述了潜在的未来研究方向。这对于人工智能和时间序列分析领域的研究人员来说是一份宝贵的资源，提供了对最新进展和未来发展的清晰视图。

    This survey delves into the application of diffusion models in time-series forecasting. Diffusion models are demonstrating state-of-the-art results in various fields of generative AI. The paper includes comprehensive background information on diffusion models, detailing their conditioning methods and reviewing their use in time-series forecasting. The analysis covers 11 specific time-series implementations, the intuition and theory behind them, the effectiveness on different datasets, and a comparison among each other. Key contributions of this work are the thorough exploration of diffusion models' applications in time-series forecasting and a chronologically ordered overview of these models. Additionally, the paper offers an insightful discussion on the current state-of-the-art in this domain and outlines potential future research directions. This serves as a valuable resource for researchers in AI and time-series analysis, offering a clear view of the latest advancements and future p
    
[^131]: 使用傅里叶神经算子逼近数值通量的超波古典守恒定律

    Approximating Numerical Flux by Fourier Neural Operators for the Hyperbolic Conservation Laws. (arXiv:2401.01783v1 [math.NA])

    [http://arxiv.org/abs/2401.01783](http://arxiv.org/abs/2401.01783)

    该研究通过在传统的数值方案中运用神经网络来替换数值通量，解决了神经网络方法缺乏鲁棒性和泛化能力的问题，并展示了该方法在超声古典守恒定律方面具有优势。

    

    传统的数值方案用于数值解PDE，最近发展了基于神经网络的方法。然而，使用神经网络的方法，如PINN和神经算子，缺乏鲁棒性和泛化能力。为了弥补这些缺点，有很多种类型的研究将传统的数值方案和机器学习方法结合起来，通过用神经网络替代数值方案中的一小部分来实现。在本文中，我们专注于超声古典守恒定律，将数值方案中的数值通量替换为神经算子。为此，我们构造了受数值方案启发的损失函数，并通过FNO逼近数值通量。通过实验证明，我们的方法通过与原始方法的比较具有数值方案和FNO的优势。例如，我们演示了我们的方法具有鲁棒性，分辨率不变性和数据驱动方法的可行性。

    Classical numerical schemes exist for solving PDEs numerically, and recently, neural network-based methods have been developed. However, methodologies using neural networks, such as PINN and neural operators, lack robustness and generalization power. To compensate for such drawbacks, there are many types of research combining classical numerical schemes and machine learning methods by replacing a small portion of the numerical schemes with neural networks. In this work, we focus on hyperbolic conservation laws and replace numerical fluxes in the numerical schemes by neural operator. For this, we construct losses that are motivated by numerical schemes for conservation laws and approximate numerical flux by FNO. Through experiments, we show that our methodology has advantages of both numerical schemes and FNO by comparing with original methods. For instance, we demonstrate our method gains robustness, resolution invariance property, and feasibility of a data-driven method. Our method es
    
[^132]: AIRI: 使用人工智能预测保留指数及其不确定性

    AIRI: Predicting Retention Indices and their Uncertainties using Artificial Intelligence. (arXiv:2401.01506v1 [cs.LG])

    [http://arxiv.org/abs/2401.01506](http://arxiv.org/abs/2401.01506)

    使用深度神经网络预测化合物保留指数，并量化其不确定性，以改善化学鉴定方法和库的质量。 (Predict the retention index of compounds using a deep neural network and quantify the uncertainty to enhance chemical identification methods and library quality.)

    

    Kovát's保留指数（RI）是使用气相色谱测量的化学结构鉴定中常用的指标。由于创建观察到的RI值的库是一项费时费力的任务，因此我们探索了使用深度神经网络从结构预测标准半极性柱的RI值。该网络生成的预测的平均绝对误差为15.1，在误差分布尾部的量化中，95%的绝对误差为46.5。由于人工智能保留指数（AIRI）网络的准确性，它被用于预测NIST EI-MS光谱库的RI值。这些RI值用于改进化学鉴定方法和提高库的质量。在使用预测模型时，估算不确定性是一项重要的实际需求。为了量化我们网络对每个单独预测的不确定性，我们使用了8个网络的输出来计算预测的标准

    The Kov\'ats Retention index (RI) is a quantity measured using gas chromatography and commonly used in the identification of chemical structures. Creating libraries of observed RI values is a laborious task, so we explore the use of a deep neural network for predicting RI values from structure for standard semipolar columns. This network generated predictions with a mean absolute error of 15.1 and, in a quantification of the tail of the error distribution, a 95th percentile absolute error of 46.5. Because of the Artificial Intelligence Retention Indices (AIRI) network's accuracy, it was used to predict RI values for the NIST EI-MS spectral libraries. These RI values are used to improve chemical identification methods and the quality of the library. Estimating uncertainty is an important practical need when using prediction models. To quantify the uncertainty of our network for each individual prediction, we used the outputs of an ensemble of 8 networks to calculate a predicted standard
    
[^133]: 基于神经算子流的量子场论多格采样方法

    Multi-Lattice Sampling of Quantum Field Theories via Neural Operator-based Flows. (arXiv:2401.00828v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.00828](http://arxiv.org/abs/2401.00828)

    本文提出了一种基于神经算子流的方法，通过近似时间相关算子，实现了在量子场论中从底层自由理论到目标理论的离散-连续归一化流。

    

    本文考虑从玻尔兹曼分布中采样离散场配置$\phi$的问题，其中$S$是某个量子场论连续欧几里得作用$\mathcal S$的格点离散化。我们将该密度近似视为底层函数密度$[\mathcal D\phi(x)]\mathcal Z^{-1}e^{-\mathcal S[\phi(x)]}$的学习算子实例。具体而言，我们提出了近似时间相关算子$\mathcal V_t$的方法，其时间积分提供了自由理论$[\mathcal D\phi(x)]\mathcal Z_0^{-1}e^{-\mathcal S_{0}[\phi(x)]}$的函数分布与目标理论$[\mathcal D\phi(x)]\mathcal Z^{-1}e^{-\mathcal S[\phi(x)]}$之间的映射。当选择特定的格点时，算子$\mathcal V_t$可以离散化为有限维的时间相关矢量场$V_t$，从而在离散格点上实现了连续的归一化流。

    We consider the problem of sampling discrete field configurations $\phi$ from the Boltzmann distribution $[d\phi] Z^{-1} e^{-S[\phi]}$, where $S$ is the lattice-discretization of the continuous Euclidean action $\mathcal S$ of some quantum field theory. Since such densities arise as the approximation of the underlying functional density $[\mathcal D\phi(x)] \mathcal Z^{-1} e^{-\mathcal S[\phi(x)]}$, we frame the task as an instance of operator learning. In particular, we propose to approximate a time-dependent operator $\mathcal V_t$ whose time integral provides a mapping between the functional distributions of the free theory $[\mathcal D\phi(x)] \mathcal Z_0^{-1} e^{-\mathcal S_{0}[\phi(x)]}$ and of the target theory $[\mathcal D\phi(x)]\mathcal Z^{-1}e^{-\mathcal S[\phi(x)]}$. Whenever a particular lattice is chosen, the operator $\mathcal V_t$ can be discretized to a finite dimensional, time-dependent vector field $V_t$ which in turn induces a continuous normalizing flow between fi
    
[^134]: 在双边市场中匹配用户和创作者的研究

    Matching of Users and Creators in Two-Sided Markets with Departures. (arXiv:2401.00313v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2401.00313](http://arxiv.org/abs/2401.00313)

    本论文提出了一个双边市场中匹配用户和创作者的模型，并展示了一个以用户为中心的贪心算法可能导致整体参与度下降的问题。

    

    现今许多在线平台，包括社交媒体网站，都是桥接内容创作者与用户的双边市场。现有的关于平台推荐算法的文献主要集中在用户偏好和决策上，并没有同时考虑到创作者的动机。我们提出了一个内容推荐模型，明确关注用户-内容匹配的动态过程，其新颖之处在于如果用户和创作者没有足够的参与感，他们都可能永久离开平台。在我们的模型中，每个参与者根据当前匹配的实用性决定是否在每个时间步参与：用户基于推荐内容与其偏好的一致性，而创作者则基于其受众规模。我们证明了一种以用户为中心的贪心算法，如果不考虑创作者的离开，可能导致整体参与度任意下降，相对于考虑到双方利益最大化整体参与度的算法。

    Many online platforms of today, including social media sites, are two-sided markets bridging content creators and users. Most of the existing literature on platform recommendation algorithms largely focuses on user preferences and decisions, and does not simultaneously address creator incentives. We propose a model of content recommendation that explicitly focuses on the dynamics of user-content matching, with the novel property that both users and creators may leave the platform permanently if they do not experience sufficient engagement. In our model, each player decides to participate at each time step based on utilities derived from the current match: users based on alignment of the recommended content with their preferences, and creators based on their audience size. We show that a user-centric greedy algorithm that does not consider creator departures can result in arbitrarily poor total engagement, relative to an algorithm that maximizes total engagement while accounting for two
    
[^135]: 有效的强化学习方法：探索与利用分离

    Efficient Reinforcemen Learning with Decoupling Exploration and Utilization. (arXiv:2312.15965v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.15965](http://arxiv.org/abs/2312.15965)

    本研究提出了一种新的强化学习框架，通过分离探索和利用策略，并采用乐观与悲观演员的双演员方法，实现了更平衡和高效的优化策略。

    

    当前的离线强化学习技术过于保守，对现有数据集的处理限制了深度神经网络(DNN)的泛化能力。这种方法常常导致算法只适应某个特定数据集的次优解。同样，在在线强化学习中，之前的惩罚性悲观主义也剥夺了模型的探索潜力。我们的研究提出了一种新的框架，乐观与悲观演员强化学习（OPARL）。OPARL采用独特的双演员方法：乐观演员专注于探索，悲观演员专注于利用，从而有效区分探索和利用策略。这种组合在强化学习方法中促进了更平衡和高效的方法。它通过悲观的利用策略优化着重于产生高奖励动作的策略。

    Deep neural network(DNN) generalization is limited by the over-reliance of current offline reinforcement learning techniques on conservative processing of existing datasets. This method frequently results in algorithms that settle for suboptimal solutions that only adjust to a certain dataset. Similarly, in online reinforcement learning, the previously imposed punitive pessimism also deprives the model of its exploratory potential. Our research proposes a novel framework, Optimistic and Pessimistic Actor Reinforcement Learning (OPARL). OPARL employs a unique dual-actor approach: an optimistic actor dedicated to exploration and a pessimistic actor focused on utilization, thereby effectively differentiating between exploration and utilization strategies. This unique combination in reinforcement learning methods fosters a more balanced and efficient approach. It enables the optimization of policies that focus on actions yielding high rewards through pessimistic utilization strategies, whi
    
[^136]: 自监督学习用于少样本鸟鸣分类

    Self-Supervised Learning for Few-Shot Bird Sound Classification. (arXiv:2312.15824v3 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2312.15824](http://arxiv.org/abs/2312.15824)

    本研究展示了自监督学习在鸟鸣分类中的应用，通过无需标注的方式，从音频录音中获得有意义的鸟鸣表示，并展示了这些表示在少样本学习中的泛化能力。此外，使用预训练的音频神经网络选择高鸟活跃窗口进行自监督学习可以显著提升学习表示的质量。

    

    音频中的自监督学习（SSL）在各个领域有着巨大的潜力，尤其是在存在大量无标签数据的情况下。这在生物声学领域尤为重要，生物学家经常从自然环境中收集大量的声音数据集。本研究证明了无需标注就能够从音频录音中获得有意义的鸟鸣表示的自监督学习的能力。我们的实验显示，这些学习到的表示能够在少样本学习（FSL）场景中泛化到新的鸟类。此外，我们还展示了使用预训练的音频神经网络选择高鸟活跃的窗口进行自监督学习可以显著提升学习表示的质量。

    Self-supervised learning (SSL) in audio holds significant potential across various domains, particularly in situations where abundant, unlabeled data is readily available at no cost. This is particularly pertinent in bioacoustics, where biologists routinely collect extensive sound datasets from the natural environment. In this study, we demonstrate that SSL is capable of acquiring meaningful representations of bird sounds from audio recordings without the need for annotations. Our experiments showcase that these learned representations exhibit the capacity to generalize to new bird species in few-shot learning (FSL) scenarios. Additionally, we show that selecting windows with high bird activation for self-supervised learning, using a pretrained audio neural network, significantly enhances the quality of the learned representations.
    
[^137]: 超越经验窗口化：一种基于注意力的方法用于自动驾驶中的信任预测

    Beyond Empirical Windowing: An Attention-Based Approach for Trust Prediction in Autonomous Vehicles. (arXiv:2312.10209v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2312.10209](http://arxiv.org/abs/2312.10209)

    这项研究提出了一种基于注意力的方法，选择性窗口化注意网络（SWAN），通过窗口提示和掩码注意转换实现具有灵活长度的关注区间的选择，从而解决信任预测中窗口大小的挑选问题。

    

    人类的内部状态在人机交互中起着关键作用，引发了人类状态估计作为一个重要领域的兴起。相比于诸如惊讶和烦恼等快速状态变化，建模像信任和满意度这样渐进状态面临更大的挑战，因为标签稀疏性：长时间序列信号通常只与一个标签相关联，导致很难确定状态变化的关键时刻。窗口化是一种广泛使用的技术，用于对长时间序列数据进行局部分析。然而，下游模型的性能可能对窗口大小敏感，确定最佳窗口大小需要领域专业知识和大量搜索。为了解决这个问题，我们提出了一种选择性窗口化注意网络（SWAN），它使用窗口提示和掩码注意转换，以实现选择具有灵活长度的关注区间。我们在一个新的多模态驾驶信任预测任务上评估了SWAN。

    Humans' internal states play a key role in human-machine interaction, leading to the rise of human state estimation as a prominent field. Compared to swift state changes such as surprise and irritation, modeling gradual states like trust and satisfaction are further challenged by label sparsity: long time-series signals are usually associated with a single label, making it difficult to identify the critical span of state shifts. Windowing has been one widely-used technique to enable localized analysis of long time-series data. However, the performance of downstream models can be sensitive to the window size, and determining the optimal window size demands domain expertise and extensive search. To address this challenge, we propose a Selective Windowing Attention Network (SWAN), which employs window prompts and masked attention transformation to enable the selection of attended intervals with flexible lengths. We evaluate SWAN on the task of trust prediction on a new multimodal driving 
    
[^138]: 在SpiNNaker 2神经形态芯片上进行语言建模

    Language Modeling on a SpiNNaker 2 Neuromorphic Chip. (arXiv:2312.09084v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2312.09084](http://arxiv.org/abs/2312.09084)

    该论文介绍了在SpiNNaker 2神经形态芯片上实现语言建模的首次尝试。通过利用基于事件的架构和大规模异步处理的硬件，该方法有望在减少能耗的同时保持竞争任务性能。

    

    随着大型语言模型的规模迅速增长，所需的计算能力也在增加。基于神经形态设备上的事件驱动网络提供了一种显著降低推理能耗的潜在方式。然而，迄今为止，大多数可以在神经形态硬件上运行的基于事件的网络，包括脉冲神经网络(SNN)，在语言建模方面的任务性能甚至不能与LSTM模型相媲美。因此，在神经形态设备上进行语言建模似乎是一个遥远的可能性。在这项工作中，我们首次在神经形态设备上实现了一个语言模型 - 具体来说是基于最近发布的名为EGRU的基于事件的架构的SpiNNaker 2芯片。SpiNNaker 2是一个设计用于大规模异步处理的众核神经形态芯片，而EGRU是为了在保持竞争任务性能的同时高效利用这种硬件而设计的。这个实现标志着在神经形态设备上进行语言建模的第一个

    As large language models continue to scale in size rapidly, so too does the computational power required to run them. Event-based networks on neuromorphic devices offer a potential way to reduce energy consumption for inference significantly. However, to date, most event-based networks that can run on neuromorphic hardware, including spiking neural networks (SNNs), have not achieved task performance even on par with LSTM models for language modeling. As a result, language modeling on neuromorphic devices has seemed a distant prospect. In this work, we demonstrate the first-ever implementation of a language model on a neuromorphic device - specifically the SpiNNaker 2 chip based on a recently published event-based architecture called the EGRU. SpiNNaker 2 is a many-core neuromorphic chip designed for large-scale asynchronous processing, while the EGRU is architected to leverage such hardware efficiently while maintaining competitive task performance. This implementation marks the firs
    
[^139]: TiMix: 文本感知图像混合用于有效的视觉语言预训练

    TiMix: Text-aware Image Mixing for Effective Vision-Language Pre-training. (arXiv:2312.08846v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.08846](http://arxiv.org/abs/2312.08846)

    TiMix是一种将文本感知的图像混合技术用于视觉语言预训练的方法，通过集成混合数据增强技术，并从互信息的角度进行理论分析，提高了数据效率并取得了可比较的性能。

    

    自监督的多模态对比学习（SMCL）通过对齐视觉和语言模态，显著推进了现代视觉语言预训练（VLP）模型的发展。然而，由于网络收集的文本-图像对中存在噪声，扩大SMCL的训练数据量在计算成本和数据效率方面面临着相当大的障碍。为了提高VLP的数据效率，我们提出了文本感知图像混合（TiMix），将基于混合的数据增强技术集成到SMCL中，显著提升了性能，而不会显著增加计算开销。我们从互信息（MI）的角度对TiMix进行了理论分析，表明跨模态对比学习的混合数据样本隐式地作为对比损失的正则化器。实验结果表明，即使使用较少的训练数据和较短的训练时间，TiMix在下游任务上表现出可比较的性能。

    Self-supervised Multi-modal Contrastive Learning (SMCL) remarkably advances modern Vision-Language Pre-training (VLP) models by aligning visual and linguistic modalities. Due to noises in web-harvested text-image pairs, however, scaling up training data volume in SMCL presents considerable obstacles in terms of computational cost and data inefficiency. To improve data efficiency in VLP, we propose Text-aware Image Mixing (TiMix), which integrates mix-based data augmentation techniques into SMCL, yielding significant performance improvements without significantly increasing computational overhead. We provide a theoretical analysis of TiMixfrom a mutual information (MI) perspective, showing that mixed data samples for cross-modal contrastive learning implicitly serve as a regularizer for the contrastive loss. The experimental results demonstrate that TiMix exhibits a comparable performance on downstream tasks, even with a reduced amount of training data and shorter training time, when be
    
[^140]: CLadder: 评估语言模型因果推理能力的基准测试

    CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models. (arXiv:2312.04350v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.04350](http://arxiv.org/abs/2312.04350)

    该论文提出了一个新的NLP任务，评估语言模型在因果推理方面的能力。作者构建了一个大规模的数据集CLadder，并利用oracle因果推理引擎将符号问题转化为自然语言。研究结果表明多个LLMs在该数据集上的表现，并引入并评估了一种定制的链式推理机制。

    

    进行因果推理的能力被广泛视为智能的核心特征。本文研究了大型语言模型(LLMs)能否连贯地推理因果关系。现有的自然语言处理(NLP)工作主要关注评估LLMs中的常识因果推理，未能评估模型是否能够按照一组明确定义的形式规则执行因果推断。为了解决这个问题，我们提出了一个新的NLP任务，自然语言中的因果推断，受到Judea Pearl等人提出的“因果推断引擎”的启发。我们构建了一个包含10K个样本的大型数据集CLadder，通过一种oracle因果推理引擎，基于一组因果图和查询(联合、干预和反事实)，得到符号问题和真实答案，并将其翻译为自然语言。我们对数据集上的多个LLMs进行评估，并引入和评估了一种定制的链式推理机制。

    The ability to perform causal reasoning is widely considered a core feature of intelligence. In this work, we investigate whether large language models (LLMs) can coherently reason about causality. Much of the existing work in natural language processing (NLP) focuses on evaluating commonsense causal reasoning in LLMs, thus failing to assess whether a model can perform causal inference in accordance with a set of well-defined formal rules. To address this, we propose a new NLP task, causal inference in natural language, inspired by the "causal inference engine" postulated by Judea Pearl et al. We compose a large dataset, CLadder, with 10K samples: based on a collection of causal graphs and queries (associational, interventional, and counterfactual), we obtain symbolic questions and ground-truth answers, through an oracle causal inference engine. These are then translated into natural language. We evaluate multiple LLMs on our dataset, and we introduce and evaluate a bespoke chain-of-th
    
[^141]: 护理者的谈话塑造幼儿视觉：一项关于双参与游戏的计算研究

    Caregiver Talk Shapes Toddler Vision: A Computational Study of Dyadic Play. (arXiv:2312.04118v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.04118](http://arxiv.org/abs/2312.04118)

    本研究通过计算模型探究了护理者的谈话对幼儿视觉表征的影响。研究发现，即使在语言输入有限的情境下，护理者的谈话仍然能够提升幼儿的视觉表征能力。

    

    婴儿识别和分类物体的能力逐渐发展。生命的第二年标志着更多语义视觉表征的出现和对词汇含义的更好理解。这表明语言输入可能在塑造视觉表征中起重要作用。然而，即使在适合学习单词的情境下，如双参与游戏会话中，护理者的话语也是稀少和不明确的，常常指的是与儿童注意的物体不同的物体。在这里，我们系统地研究护理者的话语到底能够在多大程度上增强视觉表征。为此，我们提出了一个计算模型，用于在双参与游戏过程中学习视觉表征。我们引入了一个合成数据集，其中包含了由幼儿代理人感知到的以自我为中心的图像，在不同的家庭环境中移动和旋转玩具物体，并同时听到被建模为字幕的护理者的话语。

    Infants' ability to recognize and categorize objects develops gradually. The second year of life is marked by both the emergence of more semantic visual representations and a better understanding of word meaning. This suggests that language input may play an important role in shaping visual representations. However, even in suitable contexts for word learning like dyadic play sessions, caregivers utterances are sparse and ambiguous, often referring to objects that are different from the one to which the child attends. Here, we systematically investigate to what extent caregivers' utterances can nevertheless enhance visual representations. For this we propose a computational model of visual representation learning during dyadic play. We introduce a synthetic dataset of ego-centric images perceived by a toddler-agent that moves and rotates toy objects in different parts of its home environment while hearing caregivers' utterances, modeled as captions. We propose to model toddlers' learni
    
[^142]: 不变和等变的经典和量子图神经网络的比较

    A Comparison Between Invariant and Equivariant Classical and Quantum Graph Neural Networks. (arXiv:2311.18672v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2311.18672](http://arxiv.org/abs/2311.18672)

    该论文比较了不变和等变的经典和量子图神经网络，探讨了它们在高能物理数据分析中的应用，以及如何利用量子计算提供快速而高效的计算范式。同时，研究还指出通过使用不变输入和等变层，可以增强深度网络的有效性和鲁棒性。

    

    机器学习算法在理解CERN大型强子对撞机(LHC)上产生的大量高能粒子碰撞数据时起着重要作用。这些碰撞事件的数据可以自然地用图结构表示。因此，深度几何方法，如图神经网络(GNNs)，已经在高能物理数据分析的各种任务中得到应用。一个典型的任务是喷注标记，其中喷注被视为具有不同特征和其组成粒子之间的边连接的点云。LHC粒子数据集的规模和复杂性的增加，以及用于其分析的计算模型，大大促进了开发替代快速且高效的计算范式，如量子计算。此外，为了增强深度网络的有效性和鲁棒性，可以通过使用不变输入和等变层来利用数据中存在的基本对称性。

    Machine learning algorithms are heavily relied on to understand the vast amounts of data from high-energy particle collisions at the CERN Large Hadron Collider (LHC). The data from such collision events can naturally be represented with graph structures. Therefore, deep geometric methods, such as graph neural networks (GNNs), have been leveraged for various data analysis tasks in high-energy physics. One typical task is jet tagging, where jets are viewed as point clouds with distinct features and edge connections between their constituent particles. The increasing size and complexity of the LHC particle datasets, as well as the computational models used for their analysis, greatly motivate the development of alternative fast and efficient computational paradigms such as quantum computation. In addition, to enhance the validity and robustness of deep networks, one can leverage the fundamental symmetries present in the data through the use of invariant inputs and equivariant layers. In t
    
[^143]: 电力系统中动态故障特性评估

    Dynamic Fault Characteristics Evaluation in Power Grid. (arXiv:2311.16522v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.16522](http://arxiv.org/abs/2311.16522)

    该论文提出了一种在电力系统中进行故障检测的新方法，通过图神经网络识别故障节点，并利用前后时间段内节点的状态来辅助当前故障检测。实验证明该方法准确可靠，并提供了对故障节点传播的定性分析。

    

    为了增强运维的智能度，提出了一种在电力系统中进行故障检测的新方法。该方法基于图神经网络，通过专门的特征提取方法和知识图谱来识别故障节点。通过引入时间数据，该方法利用前后时间段内节点的状态来辅助当前故障检测。为了验证节点特征的有效性，还进行了每个节点输出特征的相关性分析。实验证明，该方法可以在仿真场景中准确地定位故障节点，并具有显著的准确性。此外，基于图神经网络的特征建模可以定性地考察故障如何在节点间传播，为分析故障节点提供了有价值的见解。

    To enhance the intelligence degree in operation and maintenance, a novel method for fault detection in power grids is proposed. The proposed GNN-based approach first identifies fault nodes through a specialized feature extraction method coupled with a knowledge graph. By incorporating temporal data, the method leverages the status of nodes from preceding and subsequent time periods to help current fault detection. To validate the effectiveness of the node features, a correlation analysis of the output features from each node was conducted. The results from experiments show that this method can accurately locate fault nodes in simulation scenarios with a remarkable accuracy. Additionally, the graph neural network based feature modeling allows for a qualitative examination of how faults spread across nodes, which provides valuable insights for analyzing fault nodes.
    
[^144]: LQ-LoRA: 低秩加量化矩阵分解用于有效的语言模型微调

    LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning. (arXiv:2311.12023v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2311.12023](http://arxiv.org/abs/2311.12023)

    LQ-LoRA是一种低秩加量化矩阵分解方法，用于内存高效的语言模型微调。它通过将每个预训练矩阵分解为高精度低秩部分和内存高效的量化部分，实现了动态配置量化参数以及对重构目标进行加权的优化，并在微调实验中表现出了优于QLoRA和GPTQ-LoRA的效果。

    

    我们提出了一种简单的方法，用于对预训练语言模型进行内存高效的自适应。我们的方法使用迭代算法将每个预训练矩阵分解为高精度低秩部分和内存高效的量化部分。在微调过程中，量化部分保持固定，只有低秩部分被更新。我们提出了量化部分的整数线性规划表达，可以根据总体内存预算动态配置量化参数（例如比特宽度、块大小）给定每个矩阵。我们进一步探索了数据感知版本的算法，该算法使用Fisher信息矩阵的近似来加权矩阵分解过程中的重构目标。在RoBERTa和LLaMA-2（7B和70B）的微调实验中，我们的低秩加量化矩阵分解方法（LQ-LoRA）优于强基线方法QLoRA和GPTQ-LoRA，并实现了激进的量化。

    We propose a simple approach for memory-efficient adaptation of pretrained language models. Our approach uses an iterative algorithm to decompose each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. During finetuning, the quantized component remains fixed and only the low-rank component is updated. We present an integer linear programming formulation of the quantization component which enables dynamic configuration of quantization parameters (e.g., bit-width, block size) for each matrix given an overall target memory budget. We further explore a data-aware version of the algorithm which uses an approximation of the Fisher information matrix to weight the reconstruction objective during matrix decomposition. Experiments on finetuning RoBERTa and LLaMA-2 (7B and 70B) demonstrate that our low-rank plus quantized matrix decomposition approach (LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and enables aggressive quantization
    
[^145]: FIKIT：基于优先级的实时GPU多任务调度与内核识别

    FIKIT: Priority-Based Real-time GPU Multi-tasking Scheduling with Kernel Identification. (arXiv:2311.10359v3 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2311.10359](http://arxiv.org/abs/2311.10359)

    FIKIT是一种基于优先级的实时GPU多任务调度策略，具有内核识别功能，能够在高优先级任务的内核间填充空闲时间。

    

    高并行工作负载，如机器学习训练、推断和一般HPC任务，通过使用GPU设备得到了极大的加速。在云计算集群中，通过多任务共享来提供GPU的计算能力是非常需要的，因为总是有更多的任务请求而不是可用的GPU数量。现有的GPU共享解决方案着重于减少多个作业争夺单个GPU时的任务级等待时间或任务级切换成本。连续计算请求具有不同的优先级，对于共享GPU设备，对QoS产生了非对称的影响。现有工作没有充分利用这种情况带来的内核级优化机会。为了解决这个问题，我们提出了一种新颖的内核级调度策略FIKIT：填充内核间空闲时间。FIKIT包含任务级优先级信息、细粒度内核识别和内核测量，允许低优先级任务在高优先级任务的内核间执行。

    Highly parallelized workloads like machine learning training, inferences and general HPC tasks are greatly accelerated using GPU devices. In a cloud computing cluster, serving a GPU's computation power through multi-tasks sharing is highly demanded since there are always more task requests than the number of GPU available. Existing GPU sharing solutions focus on reducing task-level waiting time or task-level switching costs when multiple jobs competing for a single GPU. Non-stopped computation requests come with different priorities, having non-symmetric impact on QoS for sharing a GPU device. Existing work missed the kernel-level optimization opportunity brought by this setting. To address this problem, we present a novel kernel-level scheduling strategy called FIKIT: Filling Inter-kernel Idle Time. FIKIT incorporates task-level priority information, fine-grained kernel identification, and kernel measurement, allowing low priorities task's execution during high priority task's inter-k
    
[^146]: 高效的广义低秩张量情境赌博算法

    Efficient Generalized Low-Rank Tensor Contextual Bandits. (arXiv:2311.01771v1 [cs.LG])

    [http://arxiv.org/abs/2311.01771](http://arxiv.org/abs/2311.01771)

    本文提出了一种新颖的广义低秩张量情境赌博算法，并引入了G-LowTESTR算法来实现探索和利用之间的权衡。

    

    本文旨在构建一种新颖的赌博算法，能够充分利用多维数据和奖励函数的固有非线性特性，提供高可用和负责任的决策服务。为此，我们引入了一种广义低秩张量情境赌博模型，其中一个动作由三个特征向量组成，因此可以用张量表示。在这个模型中，奖励是通过将动作的特征张量与一个固定但未知的参数张量的内积应用于广义线性函数来确定的，而这个参数张量具有较低的管状秩。为了实现探索和利用之间的权衡，我们引入了一种名为“广义低秩张量探索子空间然后细化”的新算法（G-LowTESTR）。该算法首先收集原始数据，以探索嵌入在决策情境中的本质低秩张量子空间信息，然后将原始概率转换为可解释的结构化概率。

    In this paper, we aim to build a novel bandits algorithm that is capable of fully harnessing the power of multi-dimensional data and the inherent non-linearity of reward functions to provide high-usable and accountable decision-making services. To this end, we introduce a generalized low-rank tensor contextual bandits model in which an action is formed from three feature vectors, and thus can be represented by a tensor. In this formulation, the reward is determined through a generalized linear function applied to the inner product of the action's feature tensor and a fixed but unknown parameter tensor with a low tubal rank. To effectively achieve the trade-off between exploration and exploitation, we introduce a novel algorithm called "Generalized Low-Rank Tensor Exploration Subspace then Refine" (G-LowTESTR). This algorithm first collects raw data to explore the intrinsic low-rank tensor subspace information embedded in the decision-making scenario, and then converts the original prob
    
[^147]: 基于图割的基于补丁的深度无监督图像分割

    Patch-Based Deep Unsupervised Image Segmentation using Graph Cuts. (arXiv:2311.01475v1 [cs.CV])

    [http://arxiv.org/abs/2311.01475](http://arxiv.org/abs/2311.01475)

    本文提出了一种基于补丁的深度无监督图像分割策略，将深度聚类方法和经典的图割方法相结合，通过训练简单的卷积神经网络对图像补丁进行分类，并利用图割算法进行迭代正则化，实现了最先进的分割效果。

    

    无监督图像分割旨在在不使用人工注释的情况下将图像中的不同语义模式分组。类似地，图像聚类根据它们的语义内容搜索图像的分组，也不需要监督。经典地，这两个问题吸引了研究人员的关注，因为它们从坚实的数学概念中产生了具体的应用。随着深度学习的出现，科学界将注意力转向了基于复杂神经网络的解决方法，在这些领域取得了令人印象深刻的结果，但很少利用经典方法取得的进展。在这项工作中，我们提出了一种基于补丁的无监督图像分割策略，将深度聚类方法的无监督特征提取进展与经典的基于图的方法相结合。我们展示了一个简单的卷积神经网络，通过图割进行迭代正则化，可以自然地实现最先进的分割效果。

    Unsupervised image segmentation aims at grouping different semantic patterns in an image without the use of human annotation. Similarly, image clustering searches for groupings of images based on their semantic content without supervision. Classically, both problems have captivated researchers as they drew from sound mathematical concepts to produce concrete applications. With the emergence of deep learning, the scientific community turned its attention to complex neural network-based solvers that achieved impressive results in those domains but rarely leveraged the advances made by classical methods. In this work, we propose a patch-based unsupervised image segmentation strategy that bridges advances in unsupervised feature extraction from deep clustering methods with the algorithmic help of classical graph-based methods. We show that a simple convolutional neural network, trained to classify image patches and iteratively regularized using graph cuts, naturally leads to a state-of-the
    
[^148]: 学习生成参数概率模型的随机热力学

    Stochastic Thermodynamics of Learning Generative Parametric Probabilistic Models. (arXiv:2310.19802v1 [cs.LG])

    [http://arxiv.org/abs/2310.19802](http://arxiv.org/abs/2310.19802)

    本文将生成式机器学习问题视为参数概率模型的时间演化过程，通过研究模型参数与生成样本之间的热力学交换，发现模型通过耗散热量来学习，参数子系统充当热库存储学到的信息。这为超参数模型的泛化能力提供了有价值的热力学洞察。

    

    我们将生成式机器学习问题形式化为参数化概率模型（PPM）的时间演化，从本质上来说，这是一个热力学过程。然后，我们研究了模型参数（记为$\Theta$）与模型生成样本（记为$X$）之间的热力学交换。我们证明了训练数据集和随机梯度下降（SGD）优化器的作用是驱动这两个子系统的时间演化的能源。我们的发现表明，在生成样本$X$的过程中，模型通过耗散热量来学习，导致模型参数$\Theta$的熵增加。因此，参数子系统充当了一个热库，有效地存储了学到的信息。此外，模型参数作为热库的角色为超参数模型的泛化能力提供了有价值的热力学洞察。这种方法提供了一个明确且一致的方式来理解生成模型学习过程中的热力学行为。

    We have formulated generative machine learning problems as the time evolution of Parametric Probabilistic Models (PPMs), inherently rendering a thermodynamic process. Then, we have studied the thermodynamic exchange between the model's parameters, denoted as $\Theta$, and the model's generated samples, denoted as $X$. We demonstrate that the training dataset and the action of the Stochastic Gradient Descent (SGD) optimizer serve as a work source that governs the time evolution of these two subsystems. Our findings reveal that the model learns through the dissipation of heat during the generation of samples $X$, leading to an increase in the entropy of the model's parameters, $\Theta$. Thus, the parameter subsystem acts as a heat reservoir, effectively storing the learned information. Furthermore, the role of the model's parameters as a heat reservoir provides valuable thermodynamic insights into the generalization power of over-parameterized models. This approach offers an unambiguous 
    
[^149]: 理解Transformer中的加法

    Understanding Addition in Transformers. (arXiv:2310.13121v1 [cs.LG])

    [http://arxiv.org/abs/2310.13121](http://arxiv.org/abs/2310.13121)

    本文通过对经过训练进行整数加法的单层Transformer模型的深入分析，揭示了该模型将任务分为并行的、特定于数字的流，并针对不同的数字位置采用不同的算法，同时还发现了一种罕见的高损失的使用情况。这些发现对于机制可解释性、人工智能安全性和对齐性等方面的研究具有重要贡献。

    

    了解像Transformer这样的机器学习模型的内部工作方式对于其安全和道德使用至关重要。本文对经过训练进行整数加法的单层Transformer模型进行了深入分析。我们揭示了该模型将任务分为并行的、特定于数字的流，并针对不同的数字位置采用不同的算法。我们的研究还发现该模型开始计算较晚，但执行速度非常快。我们还发现了一种罕见的高损失的使用情况，并予以解释。总体而言，我们详细解释了该模型的算法。这些发现通过严格测试和数学建模得到了验证，对于机制可解释性、人工智能安全性和对齐性等广泛研究做出了贡献。我们的方法为分析更复杂的任务和多层Transformer模型打开了大门。

    Understanding the inner workings of machine learning models like Transformers is vital for their safe and ethical use. This paper presents an in-depth analysis of a one-layer Transformer model trained for integer addition. We reveal that the model divides the task into parallel, digit-specific streams and employs distinct algorithms for different digit positions. Our study also finds that the model starts calculations late but executes them rapidly. A rare use case with high loss is identified and explained. Overall, the model's algorithm is explained in detail. These findings are validated through rigorous testing and mathematical modeling, contributing to the broader works in Mechanistic Interpretability, AI safety, and alignment. Our approach opens the door for analyzing more complex tasks and multi-layer Transformer models.
    
[^150]: 通过四元数小波网络推广医学图像表示

    Generalizing Medical Image Representations via Quaternion Wavelet Networks. (arXiv:2310.10224v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2310.10224](http://arxiv.org/abs/2310.10224)

    本文提出了一种名为QUAVE的四元数小波网络，可以从医学图像中提取显著特征。该网络可以与现有的医学图像分析或综合任务结合使用，并推广了对单通道数据的采用。通过四元数小波变换和加权处理，QUAVE能够处理具有较大变化的医学数据。

    

    鉴于来自不同来源和各种任务的数据集日益增加，神经网络的普适性成为一个广泛研究的领域。当处理医学数据时，这个问题尤为广泛，因为缺乏方法论标准导致不同的成像中心或使用不同设备和辅助因素获取的数据存在较大变化。为了克服这些限制，我们引入了一种新颖的、普适的、数据-和任务不可知的框架，能够从医学图像中提取显著特征。所提出的四元数小波网络（QUAVE）可以很容易地与任何现有的医学图像分析或综合任务相结合，并且可以结合实际、四元数或超复值模型，推广它们对单通道数据的采用。QUAVE首先通过四元数小波变换提取不同的子带，得到低频/近似频带和高频/细粒度特征。然后，它对最有代表性的特征进行加权处理，从而减少了特征重要性不均匀性。

    Neural network generalizability is becoming a broad research field due to the increasing availability of datasets from different sources and for various tasks. This issue is even wider when processing medical data, where a lack of methodological standards causes large variations being provided by different imaging centers or acquired with various devices and cofactors. To overcome these limitations, we introduce a novel, generalizable, data- and task-agnostic framework able to extract salient features from medical images. The proposed quaternion wavelet network (QUAVE) can be easily integrated with any pre-existing medical image analysis or synthesis task, and it can be involved with real, quaternion, or hypercomplex-valued models, generalizing their adoption to single-channel data. QUAVE first extracts different sub-bands through the quaternion wavelet transform, resulting in both low-frequency/approximation bands and high-frequency/fine-grained features. Then, it weighs the most repr
    
[^151]: 从标签比例中学习：通过信念传播对有监督学习器进行引导

    Learning from Label Proportions: Bootstrapping Supervised Learners via Belief Propagation. (arXiv:2310.08056v1 [cs.LG])

    [http://arxiv.org/abs/2310.08056](http://arxiv.org/abs/2310.08056)

    本文提出了一种从标签比例中学习的算法框架，通过伪标签化和嵌入细化两个步骤迭代地提高有监督学习器性能。

    

    标签比例学习（LLP）是一个学习问题，在训练过程中，只有针对一组实例（称为包）的聚合级别标签可用，并且目的是在测试数据的实例级别上获得最佳性能。这种设置在广告和医学等领域由于隐私考虑而出现。我们提出了一个新颖的算法框架来解决这个问题，它通过两个主要步骤进行迭代。在每次迭代的第一步（伪标签化）中，我们定义了一个基于二进制实例标签的吉布斯分布，该分布通过以下约束将covariate信息（协变量信息）合并进去：具有相似covariates的实例应该具有相似的标签，并且通过包级别的聚合标签来综合covariate信息。然后，我们使用信念传播（BP）来边缘化吉布斯分布以获得伪标签。在第二步（嵌入细化）中，我们使用伪标签为学习器提供监督，从而获得更好的嵌入。此后，我们对这两个步骤进行迭代。

    Learning from Label Proportions (LLP) is a learning problem where only aggregate level labels are available for groups of instances, called bags, during training, and the aim is to get the best performance at the instance-level on the test data. This setting arises in domains like advertising and medicine due to privacy considerations. We propose a novel algorithmic framework for this problem that iteratively performs two main steps. For the first step (Pseudo Labeling) in every iteration, we define a Gibbs distribution over binary instance labels that incorporates a) covariate information through the constraint that instances with similar covariates should have similar labels and b) the bag level aggregated label. We then use Belief Propagation (BP) to marginalize the Gibbs distribution to obtain pseudo labels. In the second step (Embedding Refinement), we use the pseudo labels to provide supervision for a learner that yields a better embedding. Further, we iterate on the two steps ag
    
[^152]: 大型Vision Transformer的高效适应性通过Adapter重组

    Efficient Adaptation of Large Vision Transformer via Adapter Re-Composing. (arXiv:2310.06234v1 [cs.CV])

    [http://arxiv.org/abs/2310.06234](http://arxiv.org/abs/2310.06234)

    本研究提出了一种名为Adapter重组（ARC）的策略，旨在从一种新的角度解决高效预训练模型适应的问题。该方法通过考虑适应参数的可重用性和引入参数共享方案，利用对称的投影操作来构建共享的瓶颈操作，并通过学习低维度的重新缩放系数来有效重新组合层适应参数。

    

    高容量预训练模型的出现彻底改变了计算机视觉问题解决的方式，将焦点从训练特定任务模型转向了适应预训练模型。因此，以高效的方式适应大型预训练模型到下游任务已成为一个重要的研究领域。现有的解决方案主要集中在设计轻量级的适配器及其与预训练模型的交互，目标是最小化需要更新的参数数量。本研究提出了一种新颖的Adapter重组（ARC）策略，从一个新的角度解决了高效预训练模型适应的问题。我们的方法考虑了适应参数的可重用性，并引入了参数共享方案。具体而言，我们利用对称的向下/向上投影来构建瓶颈操作，这些操作在不同层之间共享。通过学习低维度的重新缩放系数，我们可以有效地重新组合层适应参数。

    The advent of high-capacity pre-trained models has revolutionized problem-solving in computer vision, shifting the focus from training task-specific models to adapting pre-trained models. Consequently, effectively adapting large pre-trained models to downstream tasks in an efficient manner has become a prominent research area. Existing solutions primarily concentrate on designing lightweight adapters and their interaction with pre-trained models, with the goal of minimizing the number of parameters requiring updates. In this study, we propose a novel Adapter Re-Composing (ARC) strategy that addresses efficient pre-trained model adaptation from a fresh perspective. Our approach considers the reusability of adaptation parameters and introduces a parameter-sharing scheme. Specifically, we leverage symmetric down-/up-projections to construct bottleneck operations, which are shared across layers. By learning low-dimensional re-scaling coefficients, we can effectively re-compose layer-adapti
    
[^153]: 后验偏差评分对公平分类最优

    Post-hoc Bias Scoring Is Optimal For Fair Classification. (arXiv:2310.05725v1 [stat.ML])

    [http://arxiv.org/abs/2310.05725](http://arxiv.org/abs/2310.05725)

    本研究提出了一种后验偏差评分的方法，在满足公平性约束的情况下保持高准确性，并给出了基于偏差分数的修改规则。该方法适用于各种类型的公平性约束问题。

    

    我们考虑了一个在群体公平性约束下的二元分类问题，该问题可以是人口统计学公平性（DP），机会均等（EOp）或等概率（EO）之一。我们提出了在公平性约束下贝叶斯最优分类器的明确特征化，结果是不受约束分类器的简单修改规则。即，我们引入了一种新的实例级别的偏差度量，称为偏差分数，而修改规则则是在有限量的偏差分数之上的简单线性规则。基于这个特征化，我们开发了一种后验方法，使我们能够适应公平性约束同时保持较高的准确性。在DP和EOp约束的情况下，修改规则是基于单个偏差分数的阈值选择，而在EO约束的情况下，我们需要调整具有2个参数的线性修改规则。该方法还可以用于包含多个敏感属性的复合群体公平性标准的情况。

    We consider a binary classification problem under group fairness constraints, which can be one of Demographic Parity (DP), Equalized Opportunity (EOp), or Equalized Odds (EO). We propose an explicit characterization of Bayes optimal classifier under the fairness constraints, which turns out to be a simple modification rule of the unconstrained classifier. Namely, we introduce a novel instance-level measure of bias, which we call bias score, and the modification rule is a simple linear rule on top of the finite amount of bias scores. Based on this characterization, we develop a post-hoc approach that allows us to adapt to fairness constraints while maintaining high accuracy. In the case of DP and EOp constraints, the modification rule is thresholding a single bias score, while in the case of EO constraints we are required to fit a linear modification rule with 2 parameters. The method can also be applied for composite group-fairness criteria, such as ones involving several sensitive att
    
[^154]: 在持续学习中平衡稳定性和可塑性：激活变化的读出分解（RDAC）框架。

    Balancing stability and plasticity in continual learning: the readout-decomposition of activation change (RDAC) framework. (arXiv:2310.04741v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.04741](http://arxiv.org/abs/2310.04741)

    本文介绍了一个名为RDAC的框架，该框架解剖了持续学习中稳定性和可塑性之间的平衡，并详细分析了几种常用算法在处理任务时的稳定性和可塑性权衡。

    

    持续学习算法旨在获取新知识的同时保留先前的信息。然而，稳定性和可塑性之间的平衡仍然是一个中心挑战。本文介绍了一个框架，对这种平衡进行了解剖，提供了关于持续学习算法的宝贵见解。激活变化的读出分解（RDAC）框架首先解决了稳定性和可塑性困境及其与灾难性遗忘的关系。它将学习诱导的激活变化与先前读出范围内的稳定性程度和零空间的变化与可塑性程度相关联。在处理分裂CIFAR-110任务的深度非线性网络中，该框架阐明了常用正则化算法（SI、EWC和LwF）以及重放算法（GEM和数据重放）的稳定性和可塑性的权衡。

    Continual learning (CL) algorithms strive to acquire new knowledge while preserving prior information. However, this stability-plasticity trade-off remains a central challenge. This paper introduces a framework that dissects this trade-off, offering valuable insights into CL algorithms. The Readout-Decomposition of Activation Change (RDAC) framework first addresses the stability-plasticity dilemma and its relation to catastrophic forgetting. It relates learning-induced activation changes in the range of prior readouts to the degree of stability and changes in the null space to the degree of plasticity. In deep non-linear networks tackling split-CIFAR-110 tasks, the framework clarifies the stability-plasticity trade-offs of the popular regularization algorithms Synaptic intelligence (SI), Elastic-weight consolidation (EWC), and learning without Forgetting (LwF), and replay-based algorithms Gradient episodic memory (GEM), and data replay. GEM and data replay preserved stability and plast
    
[^155]: Chameleon: 使用自适应污染增强标签唯一成员泄露

    Chameleon: Increasing Label-Only Membership Leakage with Adaptive Poisoning. (arXiv:2310.03838v1 [cs.LG])

    [http://arxiv.org/abs/2310.03838](http://arxiv.org/abs/2310.03838)

    Chameleon是一种新的成员推理攻击方法，利用自适应数据污染策略和高效的查询选择方法，可在标签唯一设置中提高成员泄露的准确率。

    

    在众多关键应用中引入机器学习(ML)后，个人数据提供者面临多种隐私问题。其中之一是成员推理(MI)，攻击者试图确定特定数据样本是否包含在模型的训练数据集中。目前的MI攻击利用模型的预测置信度分数来成功进行成员推理，并通过数据污染进一步提高效果。在这项工作中，我们关注较少探索且更现实的仅标签设置中，模型仅在查询样本上提供预测的标签。我们表明现有的仅标签MI攻击在低误报率(FPR)情况下无法有效推断成员身份。为了应对这一挑战，我们提出了一种新的攻击方法Chameleon，它利用一种新颖的自适应数据污染策略和高效的查询选择方法。

    The integration of machine learning (ML) in numerous critical applications introduces a range of privacy concerns for individuals who provide their datasets for model training. One such privacy risk is Membership Inference (MI), in which an attacker seeks to determine whether a particular data sample was included in the training dataset of a model. Current state-of-the-art MI attacks capitalize on access to the model's predicted confidence scores to successfully perform membership inference, and employ data poisoning to further enhance their effectiveness. In this work, we focus on the less explored and more realistic label-only setting, where the model provides only the predicted label on a queried sample. We show that existing label-only MI attacks are ineffective at inferring membership in the low False Positive Rate (FPR) regime. To address this challenge, we propose a new attack Chameleon that leverages a novel adaptive data poisoning strategy and an efficient query selection meth
    
[^156]: UniPredict：大型语言模型是通用的表格预测器

    UniPredict: Large Language Models are Universal Tabular Predictors. (arXiv:2310.03266v1 [cs.LG])

    [http://arxiv.org/abs/2310.03266](http://arxiv.org/abs/2310.03266)

    本文提出了UniPredict，一个基于大型语言模型的通用表格数据预测器，能够扩展到庞大的表格数据集，并具备理解多样化表格输入和根据输入指令预测目标变量的能力。实验结果表明，UniPredict模型在与其他模型相比时具有显著优势。

    

    表格数据预测是许多应用中的基础机器学习任务。现有方法主要采用判别建模，并在假设固定目标列的情况下进行运算，需要为每个新的预测任务重新训练。本文受到大型语言模型(LLMs)生成能力的启发，提出了基于生成建模的通用表格数据预测器UniPredict。我们展示了将LLM扩展到庞大的表格数据集的方法，能够理解不同的表格输入并根据输入指令预测目标变量。具体地，我们在169个具有不同目标的表格数据集上训练了一个单一的LLM，并将其性能与分别在每个数据集上训练的基准模型进行了比较。我们观察到这个多功能的UniPredict模型在与最佳的树提升模型相比时表现出5.4%到13.4%的优势。

    Tabular data prediction is a fundamental machine learning task for many applications. Existing methods predominantly employ discriminative modeling and operate under the assumption of a fixed target column, necessitating re-training for every new predictive task. Inspired by the generative power of large language models (LLMs), this paper exploits the idea of building universal tabular data predictors based on generative modeling, namely UniPredict. Here, we show that scaling up an LLM to extensive tabular datasets with the capability of comprehending diverse tabular inputs and predicting for target variables following the input instructions. Specifically, we train a single LLM on an aggregation of 169 tabular datasets with diverse targets and compare its performance against baselines that are trained on each dataset separately. We observe this versatile UniPredict model demonstrates an advantage over other models, ranging from 5.4% to 13.4%, when compared with the best tree-boosting b
    
[^157]: 在规划中结合空间和时间抽象以实现更好的泛化

    Combining Spatial and Temporal Abstraction in Planning for Better Generalization. (arXiv:2310.00229v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.00229](http://arxiv.org/abs/2310.00229)

    Skipper是一个基于模型的强化学习代理，利用时空抽象来在新情境中推广学到的技能。它自动将任务分解为子任务，实现稀疏决策和对环境相关部分的专注计算。实验结果表明，Skipper在零样本泛化方面具有显著优势。

    

    受到人类有意识规划的启发，我们提出了Skipper，这是一个利用时空抽象来推广在新情境中学到的技能的基于模型的强化学习代理。它自动将给定任务分解为更小、更可管理的子任务，从而实现稀疏决策和对环境相关部分的专注计算。这依赖于从回溯中学习得到的表示为有向图的抽象代理问题的提取。我们的理论分析在适当的假设下提供了性能保证，并确定了我们的方法在哪些方面有望提供帮助。针对泛化的实验验证了Skipper在零样本泛化方面与现有最先进的分层规划方法相比的显著优势。

    Inspired by human conscious planning, we propose Skipper, a model-based reinforcement learning agent utilizing spatio-temporal abstractions to generalize learned skills in novel situations. It automatically decomposes the given task into smaller, more manageable subtasks, and hence enables sparse decision-making and focused computation on the relevant parts of the environment. This relies on the extraction of an abstracted proxy problem represented as a directed graph, in which vertices and edges are learned end-to-end from hindsight. Our theoretical analyses provide performance guarantees under appropriate assumptions and establish where our approach is expected to be helpful. Generalization-focused experiments validate Skipper's significant advantage in zero-shot generalization, compared to existing state-of-the-art hierarchical planning methods.
    
[^158]: 隐性高斯过程表示任意潜在流形上的向量场

    Implicit Gaussian process representation of vector fields over arbitrary latent manifolds. (arXiv:2309.16746v1 [cs.LG])

    [http://arxiv.org/abs/2309.16746](http://arxiv.org/abs/2309.16746)

    这项研究通过引入RVGP方法，结合基于图的数据逼近方法对潜在流形上的向量信号进行学习，实现了超分辨率和修复向量场，并且在实验中证明了其具有全局规律性。

    

    高斯过程（GPs）是用于学习未知函数和量化数据中的时空不确定性的流行非参数统计模型。最近的研究扩展了GPs，用于建模分布在非欧几里得域上的标量和向量数据，包括出现在计算机视觉、动力系统和神经科学等众多领域中的平滑流形。然而，这些方法假设数据的潜在流形是已知的，限制了它们的实际效用。我们引入了RVGP，一种用于学习潜在黎曼流形上的向量信号的GP的推广。我们的方法使用与切向丛关联的连接Laplacian的特征函数进行位置编码，这些特征函数可以从基于图的常见数据近似中轻松推导出来。我们证明了RVGP在流形上具有全局规律性，使得其能够在保留奇异性的同时超分辨率和修复向量场。此外，我们使用RVGP来重构高密度数据。

    Gaussian processes (GPs) are popular nonparametric statistical models for learning unknown functions and quantifying the spatiotemporal uncertainty in data. Recent works have extended GPs to model scalar and vector quantities distributed over non-Euclidean domains, including smooth manifolds appearing in numerous fields such as computer vision, dynamical systems, and neuroscience. However, these approaches assume that the manifold underlying the data is known, limiting their practical utility. We introduce RVGP, a generalisation of GPs for learning vector signals over latent Riemannian manifolds. Our method uses positional encoding with eigenfunctions of the connection Laplacian, associated with the tangent bundle, readily derived from common graph-based approximation of data. We demonstrate that RVGP possesses global regularity over the manifold, which allows it to super-resolve and inpaint vector fields while preserving singularities. Furthermore, we use RVGP to reconstruct high-dens
    
[^159]: 《语言模型中激活路径修复的最佳实践：度量和方法》的论文翻译

    Towards Best Practices of Activation Patching in Language Models: Metrics and Methods. (arXiv:2309.16042v1 [cs.LG])

    [http://arxiv.org/abs/2309.16042](http://arxiv.org/abs/2309.16042)

    本研究系统地考察了激活路径修复中的方法细节对语言模型解释性结果的影响，并提出了最佳实践建议。

    

    机械解释性旨在理解机器学习模型的内部机制，其中定位-识别重要的模型组件是关键步骤。激活路径修复，也称为因果追踪或交换干预，是完成这一任务的标准技术，但文献中存在许多变体，对超参数或方法选择缺乏一致性。在这项工作中，我们系统地考察了激活路径修复中的方法细节对结果的影响，包括评估指标和损坏方法。在语言模型的定位和电路发现的几种设置中，我们发现不同的超参数可能导致不同的解释结果。通过经验观察支持，我们提出了为什么某些指标或方法可能更受欢迎的概念性论证。最后，我们提出了激活路径修复的最佳实践建议。

    Mechanistic interpretability seeks to understand the internal mechanisms of machine learning models, where localization -- identifying the important model components -- is a key step. Activation patching, also known as causal tracing or interchange intervention, is a standard technique for this task (Vig et al., 2020), but the literature contains many variants with little consensus on the choice of hyperparameters or methodology. In this work, we systematically examine the impact of methodological details in activation patching, including evaluation metrics and corruption methods. In several settings of localization and circuit discovery in language models, we find that varying these hyperparameters could lead to disparate interpretability results. Backed by empirical observations, we give conceptual arguments for why certain metrics or methods may be preferred. Finally, we provide recommendations for the best practices of activation patching going forwards.
    
[^160]: 语义相似性预测优于其他语义相似性度量方法

    Semantic similarity prediction is better than other semantic similarity measures. (arXiv:2309.12697v1 [cs.CL])

    [http://arxiv.org/abs/2309.12697](http://arxiv.org/abs/2309.12697)

    本文提出了一种使用经过微调的模型直接预测语义相似性的方法，并将其与其他方法进行比较，结果表明所得到的相似性更加符合我们对鲁棒的语义相似性度量的预期。

    

    自然语言文本之间的语义相似性通常通过检查子序列的重叠（例如BLEU）或使用嵌入（例如BERTScore，S-BERT）来衡量。在本文中，我们认为当我们仅对衡量语义相似性感兴趣时，直接使用经过微调的模型来预测相似性比其他方法更好。我们使用从GLUE基准测试中微调的STS-B模型，定义了STSScore方法，并且显示出所得到的相似性与我们对鲁棒的语义相似性度量的预期更加一致。

    Semantic similarity between natural language texts is typically measured either by looking at the overlap between subsequences (e.g., BLEU) or by using embeddings (e.g., BERTScore, S-BERT). Within this paper, we argue that when we are only interested in measuring the semantic similarity, it is better to directly predict the similarity using a fine-tuned model for such a task. Using a fine-tuned model for the STS-B from the GLUE benchmark, we define the STSScore approach and show that the resulting similarity is better aligned with our expectations on a robust semantic similarity measure than other approaches.
    
[^161]: 正则化对比式预训练用于小样本生物声音检测

    Regularized Contrastive Pre-training for Few-shot Bioacoustic Sound Detection. (arXiv:2309.08971v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2309.08971](http://arxiv.org/abs/2309.08971)

    本研究通过正则化对比式预训练，在小样本生物声音事件检测中实现了良好的迁移性能，使F-score提高至68.19%（0.75）。

    

    生物声音事件检测可以更好地理解动物行为，利用声音进行生物多样性监测。深度学习系统可以帮助实现这一目标，然而很难获得足够的有标注数据以从头开始训练这些系统。为了解决这个限制，Detection and Classification of Acoustic Scenes and Events (DCASE)社区将问题重新框架为小样本学习，并定期举办一个关于从仅有五个带标注示例中学习检测动物声音的挑战。在这项工作中，我们对有监督的对比式预训练进行正则化，学习可以在未见过的新目标任务中进行良好迁移的特征，在没有应用特征适应的情况下实现了61.52%（0.48）的高F-score，当我们进一步为每个新目标任务适应学习到的特征时，F-score提高至68.19%（0.75）。本研究旨在降低小样本生物声音事件检测的入门门槛。

    Bioacoustic sound event detection allows for better understanding of animal behavior and for better monitoring biodiversity using audio. Deep learning systems can help achieve this goal, however it is difficult to acquire sufficient annotated data to train these systems from scratch. To address this limitation, the Detection and Classification of Acoustic Scenes and Events (DCASE) community has recasted the problem within the framework of few-shot learning and organize an annual challenge for learning to detect animal sounds from only five annotated examples. In this work, we regularize supervised contrastive pre-training to learn features that can transfer well on new target tasks with animal sounds unseen during training, achieving a high F-score of 61.52%(0.48) when no feature adaptation is applied, and an F-score of 68.19%(0.75) when we further adapt the learned features for each new target task. This work aims to lower the entry bar to few-shot bioacoustic sound event detection by
    
[^162]: Wasserstein分布保证的策略评估和学习在上下文乐队中

    Wasserstein Distributionally Robust Policy Evaluation and Learning for Contextual Bandits. (arXiv:2309.08748v1 [cs.LG])

    [http://arxiv.org/abs/2309.08748](http://arxiv.org/abs/2309.08748)

    通过使用Wasserstein距离而不是KL散度，我们提出了一种新颖的分布保证优化方法，用于解决上下文乐队中实际环境不匹配和最坏情况下过度拟合的问题。

    

    在没有与环境直接互动的情况下，数据收集的环境通常与学习的策略应用的环境不同。为了在学习和执行过程中考虑不同环境的影响，我们提出了一种使用Wasserstein距离的新型分布保证优化(DRO)方法，该方法在假设新环境的分布位于不确定集合内时，计算策略值的最坏情况下界。典型地，这个不确定集合是基于从日志数据集中计算的经验分布的KL散度定义的。然而，KL不确定集合无法包含具有不同支持的分布，也缺乏对分布支持的几何感知。结果，KL方法在解决实际环境不匹配和导致过度拟合最坏情况方面存在不足。为了克服这些限制，我们提出了一种使用Wasserstein距离的新型DRO方法。

    Without direct interaction with the environment. Often, the environment in which the data are collected differs from the environment in which the learned policy is applied. To account for the effect of different environments during learning and execution, distributionally robust optimization (DRO) methods have been developed that compute worst-case bounds on the policy values assuming that the distribution of the new environment lies within an uncertainty set. Typically, this uncertainty set is defined based on the KL divergence around the empirical distribution computed from the logging dataset. However, the KL uncertainty set fails to encompass distributions with varying support and lacks awareness of the geometry of the distribution support. As a result, KL approaches fall short in addressing practical environment mismatches and lead to over-fitting to worst-case scenarios. To overcome these limitations, we propose a novel DRO approach that employs the Wasserstein distance instead. 
    
[^163]: FedDCSR: 通过解缠表示学习实现联邦跨领域顺序推荐

    FedDCSR: Federated Cross-domain Sequential Recommendation via Disentangled Representation Learning. (arXiv:2309.08420v1 [cs.LG])

    [http://arxiv.org/abs/2309.08420](http://arxiv.org/abs/2309.08420)

    提出了一种名为FedDCSR的联邦跨领域顺序推荐框架，通过解缠表示学习来处理不同领域之间的序列特征异质性，并保护数据隐私。

    

    近年来，利用来自多个领域的用户序列数据的跨领域顺序推荐(CSR)受到了广泛关注。然而，现有的CSR方法需要在领域之间共享原始用户数据，这违反了《通用数据保护条例》(GDPR)。因此，有必要将联邦学习(FL)和CSR相结合，充分利用不同领域的知识，同时保护数据隐私。然而，不同领域之间的序列特征异质性对FL的整体性能有显著影响。在本文中，我们提出了FedDCSR，这是一种通过解缠表示学习的新型联邦跨领域顺序推荐框架。具体而言，为了解决不同领域之间的序列特征异质性，我们引入了一种称为领域内-领域间序列表示解缠(SRD)的方法，将用户序列特征解缠成领域共享和领域专属特征。

    Cross-domain Sequential Recommendation (CSR) which leverages user sequence data from multiple domains has received extensive attention in recent years. However, the existing CSR methods require sharing origin user data across domains, which violates the General Data Protection Regulation (GDPR). Thus, it is necessary to combine federated learning (FL) and CSR to fully utilize knowledge from different domains while preserving data privacy. Nonetheless, the sequence feature heterogeneity across different domains significantly impacts the overall performance of FL. In this paper, we propose FedDCSR, a novel federated cross-domain sequential recommendation framework via disentangled representation learning. Specifically, to address the sequence feature heterogeneity across domains, we introduce an approach called inter-intra domain sequence representation disentanglement (SRD) to disentangle the user sequence features into domain-shared and domain-exclusive features. In addition, we design
    
[^164]: 基于DenseNet的EEG辅助听觉空间注意力解码方法

    A DenseNet-based method for decoding auditory spatial attention with EEG. (arXiv:2309.07690v1 [eess.SP])

    [http://arxiv.org/abs/2309.07690](http://arxiv.org/abs/2309.07690)

    本论文提出了一种基于DenseNet的EEG辅助听觉空间注意力解码方法，该方法充分利用了EEG电极的空间分布，并通过深度卷积神经网络提取了时空特征，实现了较高的解码精度。

    

    听觉空间注意力检测（ASAD）旨在利用EEG在多人演讲者环境中解码被注意的空间位置。ASAD方法受到大脑皮层神经元在处理听觉空间注意力时的侧化启发，并在神经记录的听觉注意力解码（AAD）任务中显示出有希望的性能。在以往的ASAD方法中，尚未充分利用EEG电极的空间分布，这可能限制了这些方法的性能。在本研究中，通过将原始EEG信道转换为二维空间拓扑图，将EEG数据转换为包含时空信息的三维排列。然后使用三维深度卷积神经网络（DenseNet-3D）来提取所关注位置的神经表征的时空特征。结果表明，所提出的方法比传统方法实现了更高的解码精度。

    Auditory spatial attention detection (ASAD) aims to decode the attended spatial location with EEG in a multiple-speaker setting. ASAD methods are inspired by the brain lateralization of cortical neural responses during the processing of auditory spatial attention, and show promising performance for the task of auditory attention decoding (AAD) with neural recordings. In the previous ASAD methods, the spatial distribution of EEG electrodes is not fully exploited, which may limit the performance of these methods. In the present work, by transforming the original EEG channels into a two-dimensional (2D) spatial topological map, the EEG data is transformed into a three-dimensional (3D) arrangement containing spatial-temporal information. And then a 3D deep convolutional neural network (DenseNet-3D) is used to extract temporal and spatial features of the neural representation for the attended locations. The results show that the proposed method achieves higher decoding accuracy than the sta
    
[^165]: 多模态变换器用于材料分割

    Multimodal Transformer for Material Segmentation. (arXiv:2309.04001v1 [cs.CV])

    [http://arxiv.org/abs/2309.04001](http://arxiv.org/abs/2309.04001)

    本文提出了一种新的多模态分割方法MMSFormer，该方法有效地融合四种不同模态的信息，并在MCubeS数据集上取得了显著的性能提升。

    

    利用不同模态的信息可以提高多模态分割任务的性能。然而，由于每个模态的独特特性，有效地融合不同模态的信息仍然具有挑战性。在本文中，我们提出了一种新的融合策略，可以有效地融合四种不同模态的信息：RGB、线性偏振角（AoLP）、线性偏振度（DoLP）和近红外（NIR）。我们还提出了一种名为多模态分割变换器（MMSFormer）的新模型，该模型将所提出的融合策略结合起来进行多模态材料分割。MMSFormer在多模态材料分割（MCubeS）数据集上取得了52.05％的mIoU，超过了当前最先进的方法。例如，我们的方法在检测砾石（+10.4％）和人类（+9.1％）类上提供了显着的改进。消融研究表明融合块中的不同模块对结果至关重要。

    Leveraging information across diverse modalities is known to enhance performance on multimodal segmentation tasks. However, effectively fusing information from different modalities remains challenging due to the unique characteristics of each modality. In this paper, we propose a novel fusion strategy that can effectively fuse information from different combinations of four different modalities: RGB, Angle of Linear Polarization (AoLP), Degree of Linear Polarization (DoLP) and Near-Infrared (NIR). We also propose a new model named Multi-Modal Segmentation Transformer (MMSFormer) that incorporates the proposed fusion strategy to perform multimodal material segmentation. MMSFormer achieves 52.05% mIoU outperforming the current state-of-the-art on Multimodal Material Segmentation (MCubeS) dataset. For instance, our method provides significant improvement in detecting gravel (+10.4%) and human (+9.1%) classes. Ablation studies show that different modules in the fusion block are crucial for
    
[^166]: 我看到的东西有多安全？基于图像控制的自治安全性预测的校准预测

    How Safe Am I Given What I See? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy. (arXiv:2308.12252v1 [cs.LG])

    [http://arxiv.org/abs/2308.12252](http://arxiv.org/abs/2308.12252)

    本文提出了一种基于生成世界模型的学习流水线族，通过克服学习安全知情表示和分布漂移下缺失安全标签的挑战，实现了在线安全预测。这些流水线具有统计校准保证的安全机会预测能力。

    

    端到端学习已经成为开发自治系统的主要范 paradigm。不幸的是，随着其性能和便利性，安全保证面临着更大的挑战。挑战的一个关键因素是缺乏低维可解释动态状态的概念，传统的保证方法都围绕这一概念展开。本文针对在线安全预测问题，提出了一种基于生成世界模型的可配置学习流水线族，不需要低维状态。为了实现这些流水线，我们克服了学习安全知情潜在表示和预测引起的分布漂移下的缺失安全标签的挑战。这些流水线基于符合性预测，对其安全机会预测提供了统计校准保证。我们对提出的学习流水线在两个图像控制系统的案例研究上进行了广泛评估：赛车和汽车。

    End-to-end learning has emerged as a major paradigm for developing autonomous systems. Unfortunately, with its performance and convenience comes an even greater challenge of safety assurance. A key factor of this challenge is the absence of the notion of a low-dimensional and interpretable dynamical state, around which traditional assurance methods revolve. Focusing on the online safety prediction problem, this paper proposes a configurable family of learning pipelines based on generative world models, which do not require low-dimensional states. To implement these pipelines, we overcome the challenges of learning safety-informed latent representations and missing safety labels under prediction-induced distribution shift. These pipelines come with statistical calibration guarantees on their safety chance predictions based on conformal prediction. We perform an extensive evaluation of the proposed learning pipelines on two case studies of image-controlled systems: a racing car and a car
    
[^167]: 一种基于概率波动的生成模型成员推断攻击方法

    A Probabilistic Fluctuation based Membership Inference Attack for Generative Models. (arXiv:2308.12143v1 [cs.LG])

    [http://arxiv.org/abs/2308.12143](http://arxiv.org/abs/2308.12143)

    本研究针对生成模型提出了一种概率波动评估成员推断攻击方法(PFAMI)，通过检测概率分布的波动性来推断模型中是否存在某条训练记录的成员身份。

    

    成员推断攻击(MIA)通过查询模型来识别机器学习模型的训练集中是否存在某条记录。对经典分类模型的MIA已有很多研究，最近的工作开始探索如何将MIA应用到生成模型上。我们的研究表明，现有的面向生成模型的MIA主要依赖于目标模型的过拟合现象。然而，过拟合可以通过采用各种正则化技术来避免，而现有的MIA在实践中表现不佳。与过拟合不同，记忆对于深度学习模型实现最佳性能是至关重要的，使其成为一种更为普遍的现象。生成模型中的记忆导致生成记录的概率分布呈现出增长的趋势。因此，我们提出了一种基于概率波动的成员推断攻击方法(PFAMI)，它是一种黑盒MIA，通过检测概率波动来推断成员身份。

    Membership Inference Attack (MIA) identifies whether a record exists in a machine learning model's training set by querying the model. MIAs on the classic classification models have been well-studied, and recent works have started to explore how to transplant MIA onto generative models. Our investigation indicates that existing MIAs designed for generative models mainly depend on the overfitting in target models. However, overfitting can be avoided by employing various regularization techniques, whereas existing MIAs demonstrate poor performance in practice. Unlike overfitting, memorization is essential for deep learning models to attain optimal performance, making it a more prevalent phenomenon. Memorization in generative models leads to an increasing trend in the probability distribution of generating records around the member record. Therefore, we propose a Probabilistic Fluctuation Assessing Membership Inference Attack (PFAMI), a black-box MIA that infers memberships by detecting t
    
[^168]: KinSPEAK: 通过半监督学习方法提高基尼亚兰达语语音识别的准确性

    KinSPEAK: Improving speech recognition for Kinyarwanda via semi-supervised learning methods. (arXiv:2308.11863v1 [eess.AS])

    [http://arxiv.org/abs/2308.11863](http://arxiv.org/abs/2308.11863)

    通过自监督预训练、课程进度微调和半监督学习利用无标签语音数据，该论文提出了一种改善基尼亚兰达语音识别的方法，实现了最先进的结果。

    

    尽管最近具备了大规模记录的基尼亚兰达语语音数据，但是实现基尼亚兰达语的强大语音识别仍然具有挑战性。本研究表明，使用自监督预训练，遵循简单的课程进度进行微调，以及使用半监督学习来利用大规模无标签语音数据，显著提高了基尼亚兰达语的语音识别性能。我们的方法仅关注使用公共领域数据。我们从公共网站收集了一个新的制作室级别的语音数据集，然后使用该数据集训练一个干净的基准模型。然后，使用该干净的基准模型对来自更多多样和嘈杂的公共数据集的样本进行排序，定义一个简单的课程训练进度。最后，我们将半监督学习应用于连续四代对大规模无标签数据进行标记和学习。我们的最终模型在新数据集上实现了3.2％的字错误率（WER），在Mozilla Common Voice基准测试中实现了15.9％的WER，达到了最先进的水平。

    Despite recent availability of large transcribed Kinyarwanda speech data, achieving robust speech recognition for Kinyarwanda is still challenging. In this work, we show that using self-supervised pre-training, following a simple curriculum schedule during fine-tuning and using semi-supervised learning to leverage large unlabelled speech data significantly improve speech recognition performance for Kinyarwanda. Our approach focuses on using public domain data only. A new studio-quality speech dataset is collected from a public website, then used to train a clean baseline model. The clean baseline model is then used to rank examples from a more diverse and noisy public dataset, defining a simple curriculum training schedule. Finally, we apply semi-supervised learning to label and learn from large unlabelled data in four successive generations. Our final model achieves 3.2% word error rate (WER) on the new dataset and 15.9% WER on Mozilla Common Voice benchmark, which is state-of-the-art
    
[^169]: 超几何空间中的联邦分类通过凸包的安全聚合

    Federated Classification in Hyperbolic Spaces via Secure Aggregation of Convex Hulls. (arXiv:2308.06895v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.06895](http://arxiv.org/abs/2308.06895)

    本研究是关于在超几何空间中进行联邦分类的首个方法，通过使用分布式凸SVM分类器和凸包的安全聚合来解决在隐私保护环境中处理树状数据的问题。

    

    层级和树状数据集在许多应用中出现，包括语言处理、图数据挖掘、系统发育和基因组学。已知树状数据无法以小失真嵌入到有限维的欧几里得空间中。通过使用超几何空间可以缓解这个问题。当这样的数据还必须在分布式和私有化的环境中进行处理时，需要使用针对超几何空间量身定制的新的联邦学习方法。作为发展超几何空间中联邦学习领域的初始步骤，我们提出了已知的第一种在超几何空间中进行联邦分类的方法。我们的贡献如下。首先，我们为Poincar\'e图中开发了凸SVM分类器的分布式版本。在这个设置中，从客户端向全局分类器传递的信息是各个客户端数据中存在的聚类的凸包。其次，为了避免标签切换问题，我们引入了一种新的机制以约束标签切换。

    Hierarchical and tree-like data sets arise in many applications, including language processing, graph data mining, phylogeny and genomics. It is known that tree-like data cannot be embedded into Euclidean spaces of finite dimension with small distortion. This problem can be mitigated through the use of hyperbolic spaces. When such data also has to be processed in a distributed and privatized setting, it becomes necessary to work with new federated learning methods tailored to hyperbolic spaces. As an initial step towards the development of the field of federated learning in hyperbolic spaces, we propose the first known approach to federated classification in hyperbolic spaces. Our contributions are as follows. First, we develop distributed versions of convex SVM classifiers for Poincar\'e discs. In this setting, the information conveyed from clients to the global classifier are convex hulls of clusters present in individual client data. Second, to avoid label switching issues, we intro
    
[^170]: VertiBench: 在垂直联邦学习基准中推进特征分布多样性

    VertiBench: Advancing Feature Distribution Diversity in Vertical Federated Learning Benchmarks. (arXiv:2307.02040v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.02040](http://arxiv.org/abs/2307.02040)

    本文引入了两个影响VFL性能的关键因素：特征重要性和特征相关性，并提出了相关的评估指标和数据集划分方法。同时，通过引入真实的VFL数据集，填补了图像-图像VFL情景中的不足。研究对于未来的VFL研究提供了有价值的见解。

    

    垂直联邦学习（VFL）是在特征划分的分布式数据上训练机器学习模型的重要范例。然而，由于隐私限制，很少有公开的真实世界VFL数据集用于算法评估，而这些数据集只代表了有限的特征分布。现有的基准通常采用从全局集合中的任意特征划分导出的合成数据集，这只捕捉到了一部分特征分布，导致算法性能评估不足。本文通过引入影响VFL性能的两个关键因素——特征重要性和特征相关性，并提出相关的评估指标和数据集划分方法，解决了这些问题。此外，我们还引入了一个真实的VFL数据集来弥补图像-图像VFL情景中的不足。我们对尖端VFL算法进行全面评估，为未来研究提供了有价值的见解。

    Vertical Federated Learning (VFL) is a crucial paradigm for training machine learning models on feature-partitioned, distributed data. However, due to privacy restrictions, few public real-world VFL datasets exist for algorithm evaluation, and these represent a limited array of feature distributions. Existing benchmarks often resort to synthetic datasets, derived from arbitrary feature splits from a global set, which only capture a subset of feature distributions, leading to inadequate algorithm performance assessment. This paper addresses these shortcomings by introducing two key factors affecting VFL performance - feature importance and feature correlation - and proposing associated evaluation metrics and dataset splitting methods. Additionally, we introduce a real VFL dataset to address the deficit in image-image VFL scenarios. Our comprehensive evaluation of cutting-edge VFL algorithms provides valuable insights for future research in the field.
    
[^171]: 基于评分的源分离方法及其在数字通信信号中的应用

    Score-based Source Separation with Applications to Digital Communication Signals. (arXiv:2306.14411v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.14411](http://arxiv.org/abs/2306.14411)

    我们提出了一种基于评分的源分离方法，应用于数字通信信号，通过分离叠加源的扩散生成模型和最大后验估计，实现对离散源的恢复和编码比特的准确性改善。在实验中，与传统的和现有的基于学习的方法相比，我们的方法在比特错误率上提供了95％的减小，同时我们的方法可以渐近地接近基础离散分布的模式，并作为评分提取采样方案的多源扩展。

    

    我们提出了一种基于扩散生成模型的分离叠加源的新方法。我们的方法仅依赖于单独训练的独立源的统计先验，通过最大后验估计引导一个由多层高斯平滑引导的新目标函数。受射频（RF）系统应用的启发，我们对具有基础离散性质的源以及从感兴趣信号中恢复编码比特的能力感兴趣，如比特错误率（BER）所衡量。通过RF混合的实验结果表明，我们的方法在BER上比经典的和现有的基于学习的方法减小了95％。我们的分析表明，我们提出的方法可以渐近地接近基础离散分布的模式。此外，我们的方法可以看作是最近提出的评分提取采样方案的多源扩展，揭示出了一些重要性贡献。

    We propose a new method for separating superimposed sources using diffusion-based generative models. Our method relies only on separately trained statistical priors of independent sources to establish a new objective function guided by maximum a posteriori estimation with an $\alpha$-posterior, across multiple levels of Gaussian smoothing. Motivated by applications in radio-frequency (RF) systems, we are interested in sources with underlying discrete nature and the recovery of encoded bits from a signal of interest, as measured by the bit error rate (BER). Experimental results with RF mixtures demonstrate that our method results in a BER reduction of 95% over classical and existing learning-based methods. Our analysis demonstrates that our proposed method yields solutions that asymptotically approach the modes of an underlying discrete distribution. Furthermore, our method can be viewed as a multi-source extension to the recently proposed score distillation sampling scheme, shedding ad
    
[^172]: GKD：自回归序列模型的广义知识蒸馏

    GKD: Generalized Knowledge Distillation for Auto-regressive Sequence Models. (arXiv:2306.13649v1 [cs.LG])

    [http://arxiv.org/abs/2306.13649](http://arxiv.org/abs/2306.13649)

    本文提出了广义知识蒸馏（GKD），通过从学生中采样输出序列来缓解分布不匹配，并在优化替代KL等离散度方面处理模型欠规范，达到了在摘要任务上最先进的性能。

    

    知识蒸馏通常用于压缩神经网络，以减少推理成本和内存占用。然而，当前针对自回归模型（如生成语言模型）的蒸馏方法存在两个关键问题：（1）训练期间输出序列和部署时由学生模型生成的序列之间分布不匹配，（2）模型欠规范，学生模型可能不够表达老师分布。为了解决这些问题，我们提出了广义知识蒸馏（GKD）。GKD通过在训练期间从学生中采样输出序列来缓解分布不匹配。此外，GKD通过优化替代KL等离散度来处理模型欠规范，这些离散度集中于生成可能符合老师分布的学生样本。我们证明，在摘要任务上，GKD优于常用的LLM蒸馏方法，在几个基准数据集上实现了最先进的性能。

    Knowledge distillation is commonly used for compressing neural networks to reduce their inference cost and memory footprint. However, current distillation methods for auto-regressive models, such as generative language models (LMs), suffer from two key issues: (1) distribution mismatch between output sequences during training and the sequences generated by the student during its deployment, and (2) model under-specification, where the student model may not be expressive enough to fit the teacher's distribution. To address these issues, we propose Generalized Knowledge Distillation (GKD). GKD mitigates distribution mismatch by sampling output sequences from the student during training. Furthermore, GKD handles model under-specification by optimizing alternative divergences, such as reverse KL, that focus on generating samples from the student that are likely under the teacher's distribution. We demonstrate that GKD outperforms commonly-used approaches for distilling LLMs on summarizatio
    
[^173]: 为约束MDP问题设计的最后迭代收敛的策略梯度原始-对偶方法

    Last-Iterate Convergent Policy Gradient Primal-Dual Methods for Constrained MDPs. (arXiv:2306.11700v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2306.11700](http://arxiv.org/abs/2306.11700)

    本文研究了约束MDP问题，提出了两种基于策略的原始-对偶算法，实现了策略迭代的非渐进收敛到最优约束策略。

    

    本文研究无限时间折扣约束马尔可夫决策过程（约束MDP）的最优策略计算问题。尽管在实践中使用基于Lagrangian的政策搜索方法很受欢迎，但这些方法中策略迭代的振荡尚未完全解释清楚，从而引出了诸如约束违规和对超参数的敏感性等问题。为了填补这一空白，我们采用Lagrangian方法将约束MDP转化为一个约束鞍点问题，其中最大/最小玩家分别对应原始/对偶变量，并开发了两种单时间尺度的基于策略的原始-对偶算法，其策略迭代非渐进收敛到最优约束策略。具体而言，我们首先提出了一种正则化策略梯度原始-对偶（RPG-PD）方法，该方法使用熵正则化的策略梯度更新策略，同时使用二次正则化的梯度上升更新对偶变量。我们证明了 ...

    We study the problem of computing an optimal policy of an infinite-horizon discounted constrained Markov decision process (constrained MDP). Despite the popularity of Lagrangian-based policy search methods used in practice, the oscillation of policy iterates in these methods has not been fully understood, bringing out issues such as violation of constraints and sensitivity to hyper-parameters. To fill this gap, we employ the Lagrangian method to cast a constrained MDP into a constrained saddle-point problem in which max/min players correspond to primal/dual variables, respectively, and develop two single-time-scale policy-based primal-dual algorithms with non-asymptotic convergence of their policy iterates to an optimal constrained policy. Specifically, we first propose a regularized policy gradient primal-dual (RPG-PD) method that updates the policy using an entropy-regularized policy gradient, and the dual variable via a quadratic-regularized gradient ascent, simultaneously. We prove
    
[^174]: 论文标题：在强化学习中创建多级技能层次结构。

    Creating Multi-Level Skill Hierarchies in Reinforcement Learning. (arXiv:2306.09980v1 [cs.LG])

    [http://arxiv.org/abs/2306.09980](http://arxiv.org/abs/2306.09980)

    本文提出了一种基于代理人与环境交互的图形结构的答案，使用分层图划分产生具有多个抽象层次的技能层次结构。技能能将代理人移动到状态空间中互相连接紧密但相互连接较弱的区域，有效提高了强化学习的效率。

    

    摘要：什么样的技能层次结构对于自主代理人是有用的？我们提出了一种基于代理人与环境交互的图形结构的答案。我们的方法使用分层图划分来揭示图在不同时间尺度上的结构，从而产生具有多个抽象层次的技能层次结构。在层次结构的每个层次上，技能将代理人移动到状态空间中互相连接紧密但相互连接较弱的区域。我们在强化学习的广泛领域中展示了所提出的技能层次结构的效用。

    What is a useful skill hierarchy for an autonomous agent? We propose an answer based on the graphical structure of an agent's interaction with its environment. Our approach uses hierarchical graph partitioning to expose the structure of the graph at varying timescales, producing a skill hierarchy with multiple levels of abstraction. At each level of the hierarchy, skills move the agent between regions of the state space that are well connected within themselves but weakly connected to each other. We illustrate the utility of the proposed skill hierarchy in a wide variety of domains in the context of reinforcement learning.
    
[^175]: 强度轮廓投影：用于动态网络的连续时间表示学习框架。

    Intensity Profile Projection: A Framework for Continuous-Time Representation Learning for Dynamic Networks. (arXiv:2306.06155v1 [cs.LG])

    [http://arxiv.org/abs/2306.06155](http://arxiv.org/abs/2306.06155)

    本文提出了一种连续时间网络表示学习框架，涵盖核平滑的强度函数估计、最小化强度重构误差的投影学习和归纳构造节点表示。这种表示保留了网络结构和时间一致性。

    

    我们提出了一种名为“强度轮廓投影”的新算法框架，用于学习动态网络节点的连续时间表示，该动态网络由节点集和在连续时间内发生的瞬时交互事件的集合所特征化。我们的框架包括三个阶段：通过核平滑等方法估计节点对之间交互的强度函数；学习一个最小化某种强度重构误差的投影；通过学习的投影归纳构造出不断发展的节点表示。我们展示了我们的表示保留了网络的基本结构，并具有时间一致性，这意味着节点表示可以在不同的时间点上进行有意义的比较。同时，我们也构建了估计理论来阐明平滑作为偏差方差折衷的作用，并展示了如何随着信噪比的增加而减少平滑程度以获得更好的性能。

    We present a new algorithmic framework, Intensity Profile Projection, for learning continuous-time representations of the nodes of a dynamic network, characterised by a node set and a collection of instantaneous interaction events which occur in continuous time. Our framework consists of three stages: estimating the intensity functions underlying the interactions between pairs of nodes, e.g. via kernel smoothing; learning a projection which minimises a notion of intensity reconstruction error; and inductively constructing evolving node representations via the learned projection. We show that our representations preserve the underlying structure of the network, and are temporally coherent, meaning that node representations can be meaningfully compared at different points in time. We develop estimation theory which elucidates the role of smoothing as a bias-variance trade-off, and shows how we can reduce smoothing as the signal-to-noise ratio increases on account of the algorithm `borrow
    
[^176]: 改进的概率图像-文本表示方法

    Improved Probabilistic Image-Text Representations. (arXiv:2305.18171v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.18171](http://arxiv.org/abs/2305.18171)

    本论文提出了一种改进的概率图像-文本表示方法，通过引入新的概率距离和两种优化技术，解决了现有方法中的计算负担过重和损失饱和问题，取得了显著的性能提升。

    

    图像-文本匹配是一种基本的视觉-语言任务，由于多样性和不完美注释导致的固有歧义使其受到困扰。确定性函数无法足够强大地捕捉到这种歧义，因此需要探索概率嵌入来解决这个挑战。然而，现有的概率图像-文本匹配方法存在两个关键缺点：蒙特卡洛逼近导致计算负担较重，且在大量误检情况下容易出现损失饱和问题。为了克服这些问题，本文提出了一种改进的概率跨模态嵌入方法（PCME++），通过引入具有闭合形式解的新的概率距离。此外，还提出了两种优化技术进一步增强PCME++：首先，引入伪正样本以防止大量误检情况下的损失饱和问题；其次，采用混合样本数据增强进行概率匹配。实验结果表明，PCME++在ITM任务中取得了显著的性能提升。

    Image-Text Matching (ITM) task, a fundamental vision-language (VL) task, suffers from the inherent ambiguity arising from multiplicity and imperfect annotations. Deterministic functions are not sufficiently powerful to capture ambiguity, prompting the exploration of probabilistic embeddings to tackle the challenge. However, the existing probabilistic ITM approach encounters two key shortcomings; the burden of heavy computations due to the Monte Carlo approximation, and the loss saturation issue in the face of abundant false negatives. To overcome the issues, this paper presents an improved Probabilistic Cross-Modal Embeddings (named PCME++) by introducing a new probabilistic distance with a closed-form solution. In addition, two optimization techniques are proposed to enhance PCME++ further; first, the incorporation of pseudo-positives to prevent the loss saturation problem under massive false negatives; second, mixed sample data augmentation for probabilistic matching. Experimental re
    
[^177]: 因果成分分析

    Causal Component Analysis. (arXiv:2305.17225v1 [stat.ML])

    [http://arxiv.org/abs/2305.17225](http://arxiv.org/abs/2305.17225)

    本文介绍了一个中间问题：因果成分分析(CauCA)，它是独立成分分析(ICA)和因果表示学习(CRL)的泛化和特例，其目标是学习解混函数和因果机制，预设了因果图的知识。

    

    独立成分分析(ICA)的目标是从混合观测到的变量中恢复独立的潜在变量。而因果表示学习(CRL)的目标是推断因果关系强相关性的潜在变量，以及编码它们的因果关系的未知图。我们引入了一个中间问题，称为因果成分分析(CauCA)。CauCA可以被看作是ICA的一种推广，对潜在成分之间的因果依赖建模，也是CRL的一个特例。与CRL不同的是，它预设了因果图的知识，仅关注于学习解混函数和因果机制。所有关于CauCA回收基础真相的不可能结果也适用于CRL，而可能性结果可以作为扩展CRL的基础。我们将从对潜在因果变量实施不同类型干预的多个数据集中表征CauCA的可识别性。

    Independent Component Analysis (ICA) aims to recover independent latent variables from observed mixtures thereof. Causal Representation Learning (CRL) aims instead to infer causally related (thus often statistically dependent) latent variables, together with the unknown graph encoding their causal relationships. We introduce an intermediate problem termed Causal Component Analysis (CauCA). CauCA can be viewed as a generalization of ICA, modelling the causal dependence among the latent components, and as a special case of CRL. In contrast to CRL, it presupposes knowledge of the causal graph, focusing solely on learning the unmixing function and the causal mechanisms. Any impossibility results regarding the recovery of the ground truth in CauCA also apply for CRL, while possibility results may serve as a stepping stone for extensions to CRL. We characterize CauCA identifiability from multiple datasets generated through different types of interventions on the latent causal variables. As a
    
[^178]: 揭示基于注意力的图神经网络中的平滑过度现象

    Demystifying Oversmoothing in Attention-Based Graph Neural Networks. (arXiv:2305.16102v1 [cs.LG])

    [http://arxiv.org/abs/2305.16102](http://arxiv.org/abs/2305.16102)

    本文通过数学分析证明基于注意力的图神经网络并不能解决平滑过度问题，在实际应用中需要更多关注不对称、状态相关和有向图结构。

    

    图神经网络中的平滑过度指的是增加网络深度导致节点表示变得相同的现象。尽管之前的研究已经证实了图卷积网络(GCN)会指数级失去表达能力，但是图注意力机制是否可以缓解平滑过度问题还存在争议。本文通过将基于注意力的图神经网络视为非线性时变动态系统，并结合非齐次矩阵乘积和联合谱半径理论的工具和技术，对这个问题进行了严格的数学分析，提出了一个明确的答案。我们证明了与流行观点相反，图注意力机制不能防止平滑过度现象，并且呈指数级失去表达能力。所提出的框架将对称GCN的平滑过度问题扩展到了更广泛的GNN模型类别中。特别地，我们的分析考虑了在现实应用中普遍存在的不对称、状态相关和有向图结构。

    Oversmoothing in Graph Neural Networks (GNNs) refers to the phenomenon where increasing network depth leads to homogeneous node representations. While previous work has established that Graph Convolutional Networks (GCNs) exponentially lose expressive power, it remains controversial whether the graph attention mechanism can mitigate oversmoothing. In this work, we provide a definitive answer to this question through a rigorous mathematical analysis, by viewing attention-based GNNs as nonlinear time-varying dynamical systems and incorporating tools and techniques from the theory of products of inhomogeneous matrices and the joint spectral radius. We establish that, contrary to popular belief, the graph attention mechanism cannot prevent oversmoothing and loses expressive power exponentially. The proposed framework extends the existing results on oversmoothing for symmetric GCNs to a significantly broader class of GNN models. In particular, our analysis accounts for asymmetric, state-dep
    
[^179]: 一种可扩展的神经网络用于DSIC仿射极大价拍卖设计

    A Scalable Neural Network for DSIC Affine Maximizer Auction Design. (arXiv:2305.12162v1 [cs.GT])

    [http://arxiv.org/abs/2305.12162](http://arxiv.org/abs/2305.12162)

    该论文提出了一种可扩展的神经网络AMenuNet来构造AMAs参数和生成候选分配，解决了现有方法在占优策略激励兼容性和可扩展性方面的限制，其在协商一致的价值和社会残余价值方面优于强基线模型。

    

    自动拍卖设计旨在通过机器学习寻找经验上高收入的机制。现有的多物品拍卖情景的工作可以粗略地分为RegretNet类和仿射极大价（AMAs）方法。然而，前者不能严格保证占优策略激励兼容性（DSIC），而后者因为分配候选人数过多而面临可扩展性问题。为解决这些限制，我们提出了AMenuNet，一种可扩展的神经网络，它从出价人和物品表示中构造AMA参数（甚至包括分配菜单）。由于AMA的属性，AMenuNet始终是DSIC和个人理性（IR）的，通过神经网络生成候选分配来增强可伸缩性。此外，AMenuNet是置换等变的，其参数数量不受拍卖规模的影响。我们进行了大量实验，证明AMenuNet在协商一致的价值和社会残余价值方面优于强基线模型。

    Automated auction design aims to find empirically high-revenue mechanisms through machine learning. Existing works on multi item auction scenarios can be roughly divided into RegretNet-like and affine maximizer auctions (AMAs) approaches. However, the former cannot strictly ensure dominant strategy incentive compatibility (DSIC), while the latter faces scalability issue due to the large number of allocation candidates. To address these limitations, we propose AMenuNet, a scalable neural network that constructs the AMA parameters (even including the allocation menu) from bidder and item representations. AMenuNet is always DSIC and individually rational (IR) due to the properties of AMAs, and it enhances scalability by generating candidate allocations through a neural network. Additionally, AMenuNet is permutation equivariant, and its number of parameters is independent of auction scale. We conduct extensive experiments to demonstrate that AMenuNet outperforms strong baselines in both co
    
[^180]: Flame：简化联邦学习操作的工具

    Federated Learning Operations Made Simple with Flame. (arXiv:2305.05118v1 [cs.LG])

    [http://arxiv.org/abs/2305.05118](http://arxiv.org/abs/2305.05118)

    Flame 是一个工具，通过引入高级抽象——角色和通道，将联邦学习应用程序描述为拓扑抽象图，解耦 ML 应用程序逻辑与底层部署细节，使得可以专门定制部署，减少开发工作，改进自动化和调整。

    

    分布式机器学习方法，包括广泛的联邦学习技术，在广泛分布的基础架构上部署机器学习应用程序时带来了许多优点。然而，为了实现预期的效益，需要进行应用程序和配置级别的更改，这涉及部署特定的细节。通过引入更高级别的抽象——角色和通道，可以大大减少这些复杂性，并将联邦学习应用程序描述为拓扑抽象图（TAG）。TAG将ML应用程序逻辑与底层部署细节解耦，使得可以专门定制应用程序部署，从而降低开发工作量，并为改进自动化和调整铺平道路。我们推出了Flame，这是第一个支持这些抽象概念的系统，并演示了它对多个用例的好处。

    Distributed machine learning approaches, including a broad class of federated learning techniques, present a number of benefits when deploying machine learning applications over widely distributed infrastructures. To realize the expected benefits, however, introduces substantial operational challenges due to required application and configuration-level changes related to deployment-specific details. Such complexities can be greatly reduced by introducing higher-level abstractions -- role and channel -- using which federated learning applications are described as Topology Abstraction Graphs (TAGs). TAGs decouple the ML application logic from the underlying deployment details, making it possible to specialize the application deployment, thus reducing development effort and paving the way for improved automation and tuning. We present Flame, the first system that supports these abstractions, and demonstrate its benefits for several use cases.
    
[^181]: 医学图像的“Segment Anything Model”模型？

    Segment Anything Model for Medical Images?. (arXiv:2304.14660v1 [eess.IV])

    [http://arxiv.org/abs/2304.14660](http://arxiv.org/abs/2304.14660)

    “Segment Anything Model”（SAM）是适用于常规图像分割的基础模型，可以实现零样本图像分割，但在医学图像分割方面具有更高的挑战性。作者通过构建一个大型医学分割数据集来验证SAM在该领域的潜力。

    

    “Segment Anything Model”（SAM）是第一个适用于常规图像分割的基础模型。它设计了一种新颖的可推广分割任务，通过自动和手动两种模式实现了使用预训练模型进行零样本图像分割。SAM在各种自然图像分割任务中取得了显着的成果。然而，由于复杂的模态、细微的解剖结构、不确定的复杂对象边界和广泛的对象尺度，医学图像分割（MIS）更具挑战性。SAM在各种自然图像分割任务中取得了显着的成果。同时，零样本和高效的MIS可以很好地减少注释时间并促进医学图像分析的发展。因此，SAM似乎是一个潜在的工具，并且其在大型医学数据集上的表现应该进一步验证。我们收集和整理了52个开源数据集，并建立了一个具有16个模态和68个对象的大型医学分割数据集。

    The Segment Anything Model (SAM) is the first foundation model for general image segmentation. It designed a novel promotable segmentation task, ensuring zero-shot image segmentation using the pre-trained model via two main modes including automatic everything and manual prompt. SAM has achieved impressive results on various natural image segmentation tasks. However, medical image segmentation (MIS) is more challenging due to the complex modalities, fine anatomical structures, uncertain and complex object boundaries, and wide-range object scales. SAM has achieved impressive results on various natural image segmentation tasks. Meanwhile, zero-shot and efficient MIS can well reduce the annotation time and boost the development of medical image analysis. Hence, SAM seems to be a potential tool and its performance on large medical datasets should be further validated. We collected and sorted 52 open-source datasets, and build a large medical segmentation dataset with 16 modalities, 68 obje
    
[^182]: ID-MixGCL: 基于身份混合的图形对比学习

    ID-MixGCL: Identity Mixup for Graph Contrastive Learning. (arXiv:2304.10045v1 [cs.LG])

    [http://arxiv.org/abs/2304.10045](http://arxiv.org/abs/2304.10045)

    本文提出了一种基于身份混合的图形对比学习方法，旨在解决通过图形增强得到的不同但相似的图形结构和标签的不匹配问题，以实现更好的表示捕获。

    

    最近发展的图形对比学习（GCL）方法是比较同一个图形的两个不同的“视图”以学习节点/图形表示。这些方法的核心假设是通过图形增强，可以生成几个结构不同但语义相似的图形结构，因此原始和增强的图形/节点的身份标签应该是相同的。然而，在本文中，我们发现这个假设并不总是成立，例如分子图中对节点或边的任何扰动都会在一定程度上改变图形标签。因此，我们认为增强图形结构应该伴随着对对比损失使用的标签的适应。基于这个想法，我们提出了 ID-MixGCL，它允许同时调节输入图形和相应的身份标签，具有可控的改变程度，从而捕获细粒度的表示。

    Recently developed graph contrastive learning (GCL) approaches compare two different "views" of the same graph in order to learn node/graph representations. The core assumption of these approaches is that by graph augmentation, it is possible to generate several structurally different but semantically similar graph structures, and therefore, the identity labels of the original and augmented graph/nodes should be identical. However, in this paper, we observe that this assumption does not always hold, for example, any perturbation to nodes or edges in a molecular graph will change the graph labels to some degree. Therefore, we believe that augmenting the graph structure should be accompanied by an adaptation of the labels used for the contrastive loss. Based on this idea, we propose ID-MixGCL, which allows for simultaneous modulation of both the input graph and the corresponding identity labels, with a controllable degree of change, leading to the capture of fine-grained representations 
    
[^183]: HomPINNs：基于同伦的物理知识神经网络用于解决具有多个解的非线性微分方程的反问题

    HomPINNs: homotopy physics-informed neural networks for solving the inverse problems of nonlinear differential equations with multiple solutions. (arXiv:2304.02811v1 [cs.LG])

    [http://arxiv.org/abs/2304.02811](http://arxiv.org/abs/2304.02811)

    本文提出了一种新的框架——基于同伦的物理知识神经网络，来解决具有多个解的非线性微分方程的反问题。该框架使用神经网络逼近已知观测结果并符合DEs的约束条件，通过同伦连续方法解决反问题。实验证明该方法可伸缩且适应性强，为解决具有多个解的DEs提供了有效解决方案。

    

    由于解空间中的非唯一性、对称性和分岔等复杂行为，解决具有多个解的非线性微分方程（DEs）的反问题是一项具有挑战性的任务。为了解决这个问题，我们提出了同伦物理知识神经网络（HomPINNs），这是一种利用同伦连续和神经网络（NNs）来解决反问题的新框架。所提出的框架首先使用NNs同时逼近已知观测结果和符合DEs的约束条件。通过利用同伦连续方法，逼近可追踪观察结果以确定多个解并解决反问题。实验涵盖在一维DEs上测试所提出的方法的性能，并应用它来解决二维Gray-Scott模拟。我们的研究结果表明，所提出的方法是可伸缩且适应性强的，为解决具有多个解的DEs提供了有效的解决方案。

    Due to the complex behavior arising from non-uniqueness, symmetry, and bifurcations in the solution space, solving inverse problems of nonlinear differential equations (DEs) with multiple solutions is a challenging task. To address this issue, we propose homotopy physics-informed neural networks (HomPINNs), a novel framework that leverages homotopy continuation and neural networks (NNs) to solve inverse problems. The proposed framework begins with the use of a NN to simultaneously approximate known observations and conform to the constraints of DEs. By utilizing the homotopy continuation method, the approximation traces the observations to identify multiple solutions and solve the inverse problem. The experiments involve testing the performance of the proposed method on one-dimensional DEs and applying it to solve a two-dimensional Gray-Scott simulation. Our findings demonstrate that the proposed method is scalable and adaptable, providing an effective solution for solving DEs with mul
    
[^184]: 深度学习和迭代算法在联合信道估计和信号检测中的比较研究

    A Comparative Study of Deep Learning and Iterative Algorithms for Joint Channel Estimation and Signal Detection. (arXiv:2303.03678v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2303.03678](http://arxiv.org/abs/2303.03678)

    本研究对比了深度学习和迭代算法在联合信道估计和信号检测中的效果，提出了一个新的深度学习模型，通过展开迭代算法并使用超网络来估计超参数，同时适应了低信噪比环境。结果表明，该模型能够在不同的信道模型和信噪比范围内实现出色的性能。

    

    无线通信系统中的联合信道估计和信号检测是一个关键且具有挑战性的任务，特别是因为它固有地涉及非线性逆问题。在低信噪比的情况下，传统算法往往效果不佳，这进一步凸显了这一挑战。深度学习方法已经得到了研究，但是人们对计算费用和在低信噪比环境中缺乏验证的担忧依然存在。因此，开发一个强大且低复杂度的模型，能够在广泛的信噪比范围内提供出色的性能，非常有意义。在本文中，我们旨在建立一个基准，在不同的信道模型、多普勒和信噪比设置下验证传统算法和深度学习方法。特别地，我们提出了一个新的深度学习模型，其中的骨架网络由展开迭代算法形成，并通过超网络估计超参数。此外，我们还将轻量级的DenseNet调整为适应联合信道估计和信号检测任务。

    Joint channel estimation and signal detection (JCESD) in wireless communication systems is a crucial and challenging task, especially since it inherently poses a nonlinear inverse problem. This challenge is further highlighted in low signal-to-noise ratio (SNR) scenarios, where traditional algorithms often perform poorly. Deep learning (DL) methods have been investigated, but concerns regarding computational expense and lack of validation in low-SNR settings remain. Hence, the development of a robust and low-complexity model that can deliver excellent performance across a wide range of SNRs is highly desirable. In this paper, we aim to establish a benchmark where traditional algorithms and DL methods are validated on different channel models, Doppler, and SNR settings. In particular, we propose a new DL model where the backbone network is formed by unrolling the iterative algorithm, and the hyperparameters are estimated by hypernetworks. Additionally, we adapt a lightweight DenseNet to
    
[^185]: 通过基于聚类的方法支持图像处理DNN的安全分析

    Supporting Safety Analysis of Image-processing DNNs through Clustering-based Approaches. (arXiv:2301.13506v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2301.13506](http://arxiv.org/abs/2301.13506)

    本文通过不同的分析管道对DNN失效的根本原因进行了实证评估，结果显示最佳管道结合了迁移学习，DBSCAN和UMAP，并几乎只产生了一个簇群。

    

    在安全关键的环境中，由于缺乏有效的手段解释其结果（尤其是当结果错误时），深度神经网络（DNN）往往无法应用。在我们之前的工作中，我们提出了一种白盒方法（HUDD）和一种黑盒方法（SAFE）来自动化表征DNN的失效情况。它们都可以从一组潜在庞大的导致DNN失效的图像中识别相似的图像集群。然而，HUDD和SAFE的分析管道是按照常规实践实例化的，将其他管道的分析推迟到将来的工作中。在本文中，我们报告了对99种不同的用于根本原因分析DNN失效的管道进行的实证评估结果。它们结合了迁移学习，自动编码器，神经元相关性的热图，降维技术和不同的聚类算法。我们的结果表明，最佳的管道是结合迁移学习，DBSCAN和UMAP，几乎只产生了簇群。

    The adoption of deep neural networks (DNNs) in safety-critical contexts is often prevented by the lack of effective means to explain their results, especially when they are erroneous. In our previous work, we proposed a white-box approach (HUDD) and a black-box approach (SAFE) to automatically characterize DNN failures. They both identify clusters of similar images from a potentially large set of images leading to DNN failures. However, the analysis pipelines for HUDD and SAFE were instantiated in specific ways according to common practices, deferring the analysis of other pipelines to future work. In this paper, we report on an empirical evaluation of 99 different pipelines for root cause analysis of DNN failures. They combine transfer learning, autoencoders, heatmaps of neuron relevance, dimensionality reduction techniques, and different clustering algorithms. Our results show that the best pipeline combines transfer learning, DBSCAN, and UMAP. It leads to clusters almost exclusively
    
[^186]: 在线损失函数学习

    Online Loss Function Learning. (arXiv:2301.13247v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13247](http://arxiv.org/abs/2301.13247)

    在线损失函数学习是一种新的元学习范例，旨在自动化为机器学习模型设计损失函数的重要任务。我们提出了一种新的损失函数学习技术，可以在每次更新基本模型参数后自适应地在线更新损失函数。实验结果表明，我们的方法在多个任务上稳定地优于现有技术。

    

    损失函数学习是一种新的元学习范例，旨在自动化为机器学习模型设计损失函数的重要任务。现有的损失函数学习技术已经显示出有希望的结果，经常改善模型的训练动态和最终推理性能。然而，这些技术的一个重要限制是损失函数以线下方式进行元学习，元目标仅考虑训练的前几个步骤，这与训练深度神经网络通常使用的时间范围相比显著较短。这导致对于在训练开始时表现良好但在训练结束时表现不佳的损失函数存在明显的偏差。为了解决这个问题，我们提出了一种新的损失函数学习技术，可以在每次更新基本模型参数后自适应地在线更新损失函数。实验结果表明，我们提出的方法在多个任务上稳定地优于现有技术。

    Loss function learning is a new meta-learning paradigm that aims to automate the essential task of designing a loss function for a machine learning model. Existing techniques for loss function learning have shown promising results, often improving a model's training dynamics and final inference performance. However, a significant limitation of these techniques is that the loss functions are meta-learned in an offline fashion, where the meta-objective only considers the very first few steps of training, which is a significantly shorter time horizon than the one typically used for training deep neural networks. This causes significant bias towards loss functions that perform well at the very start of training but perform poorly at the end of training. To address this issue we propose a new loss function learning technique for adaptively updating the loss function online after each update to the base model parameters. The experimental results show that our proposed method consistently out
    
[^187]: 退化是可以接受的：带有不连续分布的网络收入管理中的对数遗憾

    Degeneracy is OK: Logarithmic Regret for Network Revenue Management with Indiscrete Distributions. (arXiv:2210.07996v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.07996](http://arxiv.org/abs/2210.07996)

    该论文研究了具有不连续分布的网络收入管理问题，并提出了一种在线算法，其在此模型下实现了对数级别的遗憾。这是在不需要任何“退化”假设的情况下，首次在具有连续值的NRM模型中实现对数级别遗憾的结果。

    

    我们研究了具有接受/拒绝决策和T次独立同分布到达的经典网络收入管理（NRM）问题。我们考虑了一个分布形式，每个到达必须属于有限数量的可能类别之一，每个类别具有确定的资源消耗向量，但是一个在区间上连续分布的随机值。我们开发了一种在线算法，该算法在此模型下实现了O(log^2 T)的遗憾，唯一（必要）的假设是概率密度远离0。我们得到了第二个结果，在二阶增长的额外假设下，实现了O(log T)的遗憾。据我们所知，这是第一个在没有任何“退化”假设的情况下，在具有连续值的NRM模型中实现对数级别遗憾的结果。我们的结果是通过包括一种新的边界mypopic regret，离线分配的“半流体”放松以及改进边界的新技术实现的。

    We study the classical Network Revenue Management (NRM) problem with accept/reject decisions and $T$ IID arrivals. We consider a distributional form where each arrival must fall under a finite number of possible categories, each with a deterministic resource consumption vector, but a random value distributed continuously over an interval. We develop an online algorithm that achieves $O(\log^2 T)$ regret under this model, with the only (necessary) assumption being that the probability densities are bounded away from 0. We derive a second result that achieves $O(\log T)$ regret under an additional assumption of second-order growth. To our knowledge, these are the first results achieving logarithmic-level regret in an NRM model with continuous values that do not require any kind of ``non-degeneracy'' assumptions. Our results are achieved via new techniques including a new method of bounding myopic regret, a ``semi-fluid'' relaxation of the offline allocation, and an improved bound on the 
    
[^188]: 探索上下文表达和多模态在端到端自动驾驶中的应用

    Exploring Contextual Representation and Multi-Modality for End-to-End Autonomous Driving. (arXiv:2210.06758v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.06758](http://arxiv.org/abs/2210.06758)

    本文旨在探索在端到端自动驾驶中应用上下文表达和多模态的方法。通过学习上下文和空间环境表示，可以增强自动车辆在复杂场景中的决策能力和威胁预测能力。我们提出了一个框架，通过整合多个摄像头的信息来实现全面的上下文感知。

    

    学习上下文和空间环境表示可以增强自动车辆在复杂场景中对威胁的预测和决策能力。最近的感知系统通过传感器融合增强了空间理解能力，但往往缺乏完整的环境上下文。人类在驾驶时自然地使用神经地图，将历史数据、情境细节和其他道路使用者的行为预测等各种因素整合起来，形成对周围环境的丰富上下文理解。这种基于神经地图的理解对于在道路上做出明智决策至关重要。相比之下，尽管自动系统取得了显著进展，但仍然没有完全利用人类般深度的上下文理解能力。受此启发，我们的工作借鉴人类驾驶模式，旨在在端到端自动驾驶框架中形式化传感器融合方法。我们提出了一个框架，将三个摄像头（左、右、后）的信息整合在一起，以实现全面的上下文感知。

    Learning contextual and spatial environmental representations enhances autonomous vehicle's hazard anticipation and decision-making in complex scenarios. Recent perception systems enhance spatial understanding with sensor fusion but often lack full environmental context. Humans, when driving, naturally employ neural maps that integrate various factors such as historical data, situational subtleties, and behavioral predictions of other road users to form a rich contextual understanding of their surroundings. This neural map-based comprehension is integral to making informed decisions on the road. In contrast, even with their significant advancements, autonomous systems have yet to fully harness this depth of human-like contextual understanding. Motivated by this, our work draws inspiration from human driving patterns and seeks to formalize the sensor fusion approach within an end-to-end autonomous driving framework. We introduce a framework that integrates three cameras (left, right, an
    
[^189]: 具有背压和强化学习的Lyapunov函数一致自适应网络信号控制

    Lyapunov Function Consistent Adaptive Network Signal Control with Back Pressure and Reinforcement Learning. (arXiv:2210.02612v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2210.02612](http://arxiv.org/abs/2210.02612)

    这项研究提出了一种利用Lyapunov控制理论的统一框架，将基于流量和基于压力的交通信号控制方法相结合。通过引入背压方法和强化学习算法，实现对复杂交通网络的有效控制。

    

    在交通信号控制中，基于流量和基于压力的方法通常分别使用，但往往被单独考虑。本研究引入了一个统一的框架，利用Lyapunov控制理论，分别为这些方法定义了特定的Lyapunov函数。我们发现了有趣的结果。例如，著名的背压方法等于交叉口车道饱和流量加权的差分队列长度。我们进一步通过添加基本的交通流理论来改进它。控制系统不仅应该确保稳定，还应该能够适应各种性能指标。基于Lyapunov理论的启示，本研究为基于强化学习的网络信号控制设计了一个奖励函数，该函数使用Double Deep Q-Network (DDQN)训练智能体，以有效控制复杂的交通网络。所提出的算法与几种常用的控制算法进行了比较。

    In traffic signal control, flow-based (optimizing the overall flow) and pressure-based methods (equalizing and alleviating congestion) are commonly used but often considered separately. This study introduces a unified framework using Lyapunov control theory, defining specific Lyapunov functions respectively for these methods. We have found interesting results. For example, the well-recognized back-pressure method is equal to differential queue lengths weighted by intersection lane saturation flows. We further improve it by adding basic traffic flow theory. Rather than ensuring that the control system be stable, the system should be also capable of adaptive to various performance metrics. Building on insights from Lyapunov theory, this study designs a reward function for the Reinforcement Learning (RL)-based network signal control, whose agent is trained with Double Deep Q-Network (DDQN) for effective control over complex traffic networks. The proposed algorithm is compared with several
    
[^190]: 基于模型信息的生成对抗网络 (MI-GAN) 用于学习最优功率流

    Model-Informed Generative Adversarial Network (MI-GAN) for Learning Optimal Power Flow. (arXiv:2206.01864v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.01864](http://arxiv.org/abs/2206.01864)

    本文提出了一种基于优化模型信息的生成对抗网络 (MI-GAN) 框架，用于解决可再生能源不确定性对优化问题的影响，提高最优功率流问题的计算效率。

    

    最优功率流 (OPF) 问题作为电力系统运营的关键组成部分，由于可再生能源的变异性、间歇性和不可预测性给解决带来了越来越大的困难。虽然传统的优化技术，如随机化和鲁棒性优化方法，可以用来解决OPF问题，但面对可再生能源的不确定性，即优化模型中的动态系数，它们在处理大规模问题方面的效果仍然有限。因此，近年来已经开发出深度学习技术，如神经网络，以提高利用数据解决OPF问题的计算效率。然而，解决方案的可行性和最优性可能无法得到保证，系统动态也无法得到适当的处理。在本文中，我们提出了一种基于优化模型信息的生成对抗网络 (MI-GAN) 框架，以解决这些问题。

    The optimal power flow (OPF) problem, as a critical component of power system operations, becomes increasingly difficult to solve due to the variability, intermittency, and unpredictability of renewable energy brought to the power system. Although traditional optimization techniques, such as stochastic and robust optimization approaches, could be leveraged to address the OPF problem, in the face of renewable energy uncertainty, i.e., the dynamic coefficients in the optimization model, their effectiveness in dealing with large-scale problems remains limited. As a result, deep learning techniques, such as neural networks, have recently been developed to improve computational efficiency in solving OPF problems with the utilization of data. However, the feasibility and optimality of the solution may not be guaranteed, and the system dynamics cannot be properly addressed as well. In this paper, we propose an optimization model-informed generative adversarial network (MI-GAN) framework to so
    
[^191]: 一种基于Dempster-Shafer方法的值得信赖的人工智能：胎儿脑MRI分割应用

    A Dempster-Shafer approach to trustworthy AI with application to fetal brain MRI segmentation. (arXiv:2204.02779v4 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2204.02779](http://arxiv.org/abs/2204.02779)

    该论文提出了一种基于Dempster-Shafer方法的值得信赖的人工智能框架和实用系统，用于医学图像分割。通过检测和纠正深度学习模型的失败，增强了其可信性和安全性。

    

    医学图像分割的深度学习模型在病理情况和与训练图像拍摄在不同中心的图像中可能出现意外和显著的失败，其标签错误违反了专家知识。这些错误削弱了深度学习模型在医学图像分割中的可信性。检测和纠正这些失败的机制对于安全地将这项技术应用于临床是必不可少的，并且可能成为未来关于人工智能的法规的要求。在这项工作中，我们提出了一个值得信赖的人工智能理论框架和一个实用系统，可以使用Dempster-Shafer理论的备用方法和故障安全机制来增强任何骨干人工智能系统。我们的方法依赖于对值得信赖的人工智能的可操作定义。我们的方法自动丢弃骨干人工智能预测的体素级标签，这些标签违反了专家知识，并对这些体素使用备用方法。我们展示了该方法的有效性。

    Deep learning models for medical image segmentation can fail unexpectedly and spectacularly for pathological cases and images acquired at different centers than training images, with labeling errors that violate expert knowledge. Such errors undermine the trustworthiness of deep learning models for medical image segmentation. Mechanisms for detecting and correcting such failures are essential for safely translating this technology into clinics and are likely to be a requirement of future regulations on artificial intelligence (AI). In this work, we propose a trustworthy AI theoretical framework and a practical system that can augment any backbone AI system using a fallback method and a fail-safe mechanism based on Dempster-Shafer theory. Our approach relies on an actionable definition of trustworthy AI. Our method automatically discards the voxel-level labeling predicted by the backbone AI that violate expert knowledge and relies on a fallback for those voxels. We demonstrate the effec
    

