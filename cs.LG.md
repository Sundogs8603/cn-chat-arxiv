# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [LightZero: A Unified Benchmark for Monte Carlo Tree Search in General Sequential Decision Scenarios.](http://arxiv.org/abs/2310.08348) | 本文介绍了LightZero，一个用于在普适顺序决策场景中部署MCTS/MuZero的统一基准。通过解决普通MCTS风格决策求解器面临的关键挑战并进行模块化设计，结合更合适的探索和优化策略，我们可以构建强大的LightZero代理来应对各种任务。 |
| [^2] | [A Generic Software Framework for Distributed Topological Analysis Pipelines.](http://arxiv.org/abs/2310.08339) | 本文介绍了一个通用的软件框架，用于支持分布式内存模型下的拓扑分析管线。该框架能够实现不同的拓扑算法之间的协作，并提供了性能分析和示例。 |
| [^3] | [Impact of multi-armed bandit strategies on deep recurrent reinforcement learning.](http://arxiv.org/abs/2310.08331) | 本研究在自动驾驶场景中使用部分可观测系统，通过部署和测试多种技术来平衡探索和利用的权衡，以预测方向盘操作。 |
| [^4] | [Defending Our Privacy With Backdoors.](http://arxiv.org/abs/2310.08320) | 本研究提出了一种基于后门攻击的防御方法，通过对模型进行策略性插入后门，对齐敏感短语与中性术语的嵌入，以删除训练数据中的私人信息。实证结果显示该方法的有效性。 |
| [^5] | [GePSAn: Generative Procedure Step Anticipation in Cooking Videos.](http://arxiv.org/abs/2310.08312) | 这篇论文研究了在烹饪视频中未来步骤预测的问题，通过设计一个生成模型，能够生成多个合理且多样的候选步骤，解决了之前工作中忽视的考虑多个可能实现的挑战。 |
| [^6] | [CHIP: Contrastive Hierarchical Image Pretraining.](http://arxiv.org/abs/2310.08304) | 我们提出了一种基于对比损失的层次化分类模型，用于少样本目标分类任务。该模型可以将未见过的物体分类到相对通用的类别中，通过从图像嵌入中提取的特征进行分类。 |
| [^7] | [A Symmetry-Aware Exploration of Bayesian Neural Network Posteriors.](http://arxiv.org/abs/2310.08287) | 本文对贝叶斯神经网络后验进行了大规模探索，研究了逼近后验的最佳方法和后验质量与不确定性量化的关系，同时发现了权重空间对称性对后验的影响。 |
| [^8] | [Data driven modeling of self-similar dynamics.](http://arxiv.org/abs/2310.08282) | 本文介绍了一个多尺度神经网络框架，利用自相似性作为先验知识，对自相似动力系统进行建模。对于确定性动力学，框架可以判断动力学是否自相似；对于不确定性动力学，它可以确定哪个参数集更接近自相似性。方法可以提取与尺度无关的核进行任意尺度的建模，并识别自相似系统中的幂律指数。初步测试表明，方法对Ising模型的临界指数具有理论一致性。 |
| [^9] | [Lag-Llama: Towards Foundation Models for Time Series Forecasting.](http://arxiv.org/abs/2310.08278) | Lag-Llama是一个基于大量时间序列数据训练的通用预测模型，在未见过的数据集上展现出强大的零样本预测能力，并使用光滑断裂幂律模型来拟合和预测扩展行为。 |
| [^10] | [Invisible Threats: Backdoor Attack in OCR Systems.](http://arxiv.org/abs/2310.08259) | 该论文介绍了一种针对OCR系统的后门攻击方法，通过插入非可读字符的恶意输入图像，在不影响模型性能的情况下干扰训练阶段。实验证明，该攻击方法对OCR的现有技术造成了威胁。 |
| [^11] | [Impact of Co-occurrence on Factual Knowledge of Large Language Models.](http://arxiv.org/abs/2310.08256) | 大型语言模型在回答问题时常常出现事实错误，这主要是因为过度依赖预训练语料库的共现统计。研究结果表明，大型语言模型容易受到共现偏见的影响，导致难以回忆起预训练数据集中很少共现的事实。建议使用去偏数据集进行微调来减轻偏见，但这对于微调期间未见过的稀有事实的回忆效果并不明显。 |
| [^12] | [MetaBox: A Benchmark Platform for Meta-Black-Box Optimization with Reinforcement Learning.](http://arxiv.org/abs/2310.08252) | MetaBox是一种用于开发和评估元黑箱优化与强化学习方法的基准平台，提供灵活的算法模板、广泛的问题实例和基线方法，并引入了三个标准化的性能指标，以促进方法的严格评估。 |
| [^13] | [Towards a Unified Analysis of Kernel-based Methods Under Covariate Shift.](http://arxiv.org/abs/2310.08237) | 该论文提出了一个在协变量漂移下对一般非参数方法进行统一分析的方法，通过建立收敛速度为一般损失函数提供了统一的理论分析。 |
| [^14] | [GROOT: Learning to Follow Instructions by Watching Gameplay Videos.](http://arxiv.org/abs/2310.08235) | GROOT是一个通过观看游戏视频学习遵循指令的控制器，它通过产生一个结构化的目标空间来消除昂贵的文本-游戏注释的需求，并在Minecraft SkillForge基准测试中展现出与人类玩家相当的表现水平。 |
| [^15] | [Emergence of Latent Binary Encoding in Deep Neural Network Classifiers.](http://arxiv.org/abs/2310.08224) | 这篇论文观察到在深度神经网络分类器的潜在空间中出现了二进制编码，这种编码通过引入线性倒数第二层和指数增长的损失函数产生，并且加速了收敛和提高了分类准确率。 |
| [^16] | [SimCKP: Simple Contrastive Learning of Keyphrase Representations.](http://arxiv.org/abs/2310.08221) | SimCKP是一个简单的对比学习框架，通过学习上下文感知的短语级表示来提取关键词短语，并通过重新排序来调整生成的短语的分数。 |
| [^17] | [TriRE: A Multi-Mechanism Learning Paradigm for Continual Knowledge Retention and Promotion.](http://arxiv.org/abs/2310.08217) | TriRE是一个新的持续学习范式，受到大脑如何利用多种机制进行学习的启发。它通过保持每个任务中最显著的神经元，修订和巩固这些神经元之间的联系，解决了深度神经网络中的灾难性遗忘问题。 |
| [^18] | [Trustworthy Machine Learning.](http://arxiv.org/abs/2310.08215) | 该论文讨论了当前机器学习技术面临的可信度问题，并提出了分布之外的泛化、可解释性、不确定性量化和可信度评估四个关键主题的解决方案。 |
| [^19] | [Conformal inference for regression on Riemannian Manifolds.](http://arxiv.org/abs/2310.08209) | 本文研究了在黎曼流形上进行回归场景的预测集，并证明了这些区域的经验版本在大样本下的收敛性。 |
| [^20] | [Lifelong Audio-video Masked Autoencoder with Forget-robust Localized Alignments.](http://arxiv.org/abs/2310.08204) | 这项研究提出了一种终身音视频掩码自编码器，通过引入本地化对齐和抗遗忘的多模态块选择，在不断变化的音视频分布中学习准确的多模态关系。 |
| [^21] | [Beyond Traditional DoE: Deep Reinforcement Learning for Optimizing Experiments in Model Identification of Battery Dynamics.](http://arxiv.org/abs/2310.08198) | 本论文提出了一种基于深度强化学习的新颖DoE方法，用于优化电池动力学模型识别中的实验设计。该方法根据过去实验的统计信息动态地改变实验配置，从而提高了实验效率和准确性。 |
| [^22] | [Learn From Model Beyond Fine-Tuning: A Survey.](http://arxiv.org/abs/2310.08184) | 这项研究以Learn From Model (LFM)为名，探索了超越微调的模型学习技术，旨在通过对模型接口进行研究和设计，将基于模型的学习推广到下游任务中。 |
| [^23] | [XIMAGENET-12: An Explainable AI Benchmark Dataset for Model Robustness Evaluation.](http://arxiv.org/abs/2310.08182) | XIMAGENET-12是一个可解释的AI基准数据集，包含12个常见物体类别的超过200,000张图像和15,600个手动语义注释。它通过模拟六个不同的场景，提出了一种超越模型生成能力评估的新的稳健性准则。 |
| [^24] | [Improving Fast Minimum-Norm Attacks with Hyperparameter Optimization.](http://arxiv.org/abs/2310.08177) | 本研究使用超参数优化方法，改善了快速最小范数攻击的效果，通过自动选择损失函数、优化器和步长调度器，以及相应的超参数，该方法在多个鲁棒模型下得到了验证。 |
| [^25] | [Infinite Width Graph Neural Networks for Node Regression/ Classification.](http://arxiv.org/abs/2310.08176) | 本研究分析了无限宽度图神经网络在图结构化数据上的应用。通过连接深度学习和高斯过程/核方法，研究推广了神经网络，并推导出了闭式形式的核函数和高斯过程。研究结果表明，高斯过程和核方法在不确定性估计方面更加用户友好，并且可以在多种架构和数据集上进行回归/分类任务。 |
| [^26] | [COVID-19 Detection Using Swin Transformer Approach from Computed Tomography Images.](http://arxiv.org/abs/2310.08165) | 利用Swin Transformer模型从CT图像中诊断COVID-19，方法在患者级别上进行预测，取得了优越的准确性，在评估指标上超越基线模型和其他方法，为准确诊断提供了稳健的解决方案。 |
| [^27] | [Interpreting Reward Models in RLHF-Tuned Language Models Using Sparse Autoencoders.](http://arxiv.org/abs/2310.08164) | 通过使用稀疏自编码器，我们提出了一种解释RLHF调整的语言模型中学习的奖励函数的新方法。这个方法能够通过比较自编码器隐藏空间来识别学习奖励模型准确性的独特特征，并提供了奖励完整性的抽象近似值。 |
| [^28] | [On Extreme Value Asymptotics of Projected Sample Covariances in High Dimensions with Applications in Finance and Convolutional Networks.](http://arxiv.org/abs/2310.08150) | 本文研究了高维向量时间序列样本协方差矩阵的函数的最大型统计量，推广了样本自协方差函数的最大偏差情况，证明了Gumbel型极值渐近性成立，并将其应用于金融和卷积网络领域。 |
| [^29] | [Open-Set Knowledge-Based Visual Question Answering with Inference Paths.](http://arxiv.org/abs/2310.08148) | 本文提出了一种基于开放集知识的视觉问答系统，解决了现有模型中的分类限制和缺乏推理路径的问题。 |
| [^30] | [Multi-Scale Spatial-Temporal Recurrent Networks for Traffic Flow Prediction.](http://arxiv.org/abs/2310.08138) | 本论文提出了一种名为MSSTRN的多尺度时空循环网络，用于交通流预测。该网络通过两种不同的循环神经网络完全捕获了交通数据中不同时间窗口下的复杂时空信息，解决了现有方法中忽视图结构、捕捉不充分以及缺乏关注不同时间尺度上时空信息的问题。 |
| [^31] | [Counterfactual Explanations for Time Series Forecasting.](http://arxiv.org/abs/2310.08137) | 本论文提出了一种针对时间序列预测的逆向解释方法，通过施加基于梯度的扰动，并利用约束条件指导扰动，从而获得所需的预测结果。实验结果表明，该方法在四种最先进的深度模型架构上表现良好。 |
| [^32] | [Core-sets for Fair and Diverse Data Summarization.](http://arxiv.org/abs/2310.08122) | 该论文研究了在公平/划分约束下多样性最大化任务的核心集构造算法，提出了与两种多样性度量相关的改进算法，并展示了核心集方法的有效性。 |
| [^33] | [Generative Intrinsic Optimization: Intrisic Control with Model Learning.](http://arxiv.org/abs/2310.08100) | 这项工作提出了一种生成内在优化的方法，通过结合模型学习和内在控制，实现了对不同形式结果的综合处理。这种方法保证了收敛到最优策略，有助于提高样本效率并考虑环境不确定性。 |
| [^34] | [ClimateBERT-NetZero: Detecting and Assessing Net Zero and Reduction Targets.](http://arxiv.org/abs/2310.08096) | ClimateBERT-NetZero是一个用于自动检测净零和减排目标的工具，它通过专家标注数据集和自然语言分类器的训练实现。该工具可以与问答模型结合使用，分析净零和减排目标中的雄心，并通过分析通信模式的演变提供有希望的途径。 |
| [^35] | [Discerning Temporal Difference Learning.](http://arxiv.org/abs/2310.08091) | 该论文提出了一种名为分辨TD学习(DTD)的新型TD算法，通过灵活的强调函数来分配资源，以改善状态之间的轻重分配，解决了传统TD学习中忽视历史状态重要性和TD误差传播的问题。实证结果表明，使用合理的强调函数不仅改进了值估计，还加速了学习过程。 |
| [^36] | [Dealing with zero-inflated data: achieving SOTA with a two-fold machine learning approach.](http://arxiv.org/abs/2310.08088) | 通过应用双重机器学习方法，本文解决了处理零膨胀数据的问题，并在两个实际应用案例中取得了出色的结果。 |
| [^37] | [A Carbon Tracking Model for Federated Learning: Impact of Quantization and Sparsification.](http://arxiv.org/abs/2310.08087) | 本文提出了一个碳追踪模型，用于实时监测联邦学习系统的能源消耗和碳足迹影响。通过对不同的计算和通信高效联邦学习方法进行定量评估，为减少能源消耗和碳排放提供了参考。 |
| [^38] | [To token or not to token: A Comparative Study of Text Representations for Cross-Lingual Transfer.](http://arxiv.org/abs/2310.08078) | 本文进行了针对跨语言转换的文本表示方案的比较研究，发现图像模型在相关且脚本相似的语言之间的转换中表现优秀，而基于分割的模型在偏向单词含义的任务中表现更好。 |
| [^39] | [Samples on Thin Ice: Re-Evaluating Adversarial Pruning of Neural Networks.](http://arxiv.org/abs/2310.08073) | 本研究重新评估了对抗性剪枝方法，发现其鲁棒性被高估。剪枝后，接近未剪枝模型决策边界的样本通常被错误分类。这些结果对未来设计更有效的对抗性剪枝方法具有指导意义。 |
| [^40] | [Learning Transferable Conceptual Prototypes for Interpretable Unsupervised Domain Adaptation.](http://arxiv.org/abs/2310.08071) | 本文提出了一种名为Transferable Conceptual Prototype Learning（TCPL）的可迁移概念原型学习方法，它能够同时解释和改进无监督领域适应中的知识转移和决策过程。 |
| [^41] | [Tight Time-Space Lower Bounds for Constant-Pass Learning.](http://arxiv.org/abs/2310.08070) | 该论文证明了对于常数次通行的任何奇偶学习算法，需要要么Ω(n^2)的内存大小，要么至少需要2^的样本数量。 |
| [^42] | [Rethinking Negative Pairs in Code Search.](http://arxiv.org/abs/2310.08069) | 本文提出了一种简单而有效的Soft-InfoNCE损失函数，通过在InfoNCE中插入权重项来解决代码搜索中负样本的问题，包括大型代码库中的虚假负样本和未能区分负样本的潜在相关性。 |
| [^43] | [ETDock: A Novel Equivariant Transformer for Protein-Ligand Docking.](http://arxiv.org/abs/2310.08061) | 提出了一种新颖的等变Transformer神经网络用于蛋白质-配体对接，通过融合配体的图层特征，并使用TAMformer模块学习配体和蛋白质的表示，实现了对配体位姿的准确预测，并通过迭代优化方法生成精炼的配体位姿。 |
| [^44] | [Learning from Label Proportions: Bootstrapping Supervised Learners via Belief Propagation.](http://arxiv.org/abs/2310.08056) | 本文提出了一种从标签比例中学习的算法框架，通过伪标签化和嵌入细化两个步骤迭代地提高有监督学习器性能。 |
| [^45] | [LGL-BCI: A Lightweight Geometric Learning Framework for Motor Imagery-Based Brain-Computer Interfaces.](http://arxiv.org/abs/2310.08051) | LGL-BCI是一种轻量级几何学习框架，通过处理EEG数据在非欧几里德度量空间中捕捉运动想象任务的空间相关性，并通过特征分解算法进行EEG通道选择以提高推断速度。实验证明LGL-BCI相比现有解决方案具有更高的准确性和效率。 |
| [^46] | [Exploring the Relationship Between Model Architecture and In-Context Learning Ability.](http://arxiv.org/abs/2310.08049) | 现有的模型架构在上下文学习中表现最好，特别是在任务复杂性增加的情况下。不同架构对超参数设置的敏感度有所差异，且一些架构展现出平稳的学习轨迹。 |
| [^47] | [QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models.](http://arxiv.org/abs/2310.08041) | QLLM是一种为大规模语言模型设计的准确高效的低位宽后训练量化方法，通过引入自适应通道重组技术，将离群值的大小重新分配给其他通道，从而减轻它们对量化范围的影响。 |
| [^48] | [SEE-OoD: Supervised Exploration For Enhanced Out-of-Distribution Detection.](http://arxiv.org/abs/2310.08040) | 本研究提出了一种基于生成对抗训练的方式，利用有限的离域样本进行数据增强和探索，以提高离域检测的准确性。 |
| [^49] | [Rethinking Large-scale Pre-ranking System: Entire-chain Cross-domain Models.](http://arxiv.org/abs/2310.08039) | 本文提出了一种重新思考大规模预排序系统的方法，通过整体链路跨域模型和细粒度神经结构来解决样本选择偏差问题，并改进预排序的准确性。 |
| [^50] | [Continual Learning via Manifold Expansion Replay.](http://arxiv.org/abs/2310.08038) | 该论文提出了一种名为Manifold Expansion Replay（MaER）的新型回放策略，通过扩展知识表示中的隐式流形来减少灾难性遗忘。采用贪心策略增加缓冲区中知识表示的流形直径，并使用Wasserstein距离作为距离度量，以提高模型的鲁棒性和表达能力。 |
| [^51] | [ZEST: Attention-based Zero-Shot Learning for Unseen IoT Device Classification.](http://arxiv.org/abs/2310.08036) | ZEST是一种基于注意力机制的零样本学习框架，用于分类未见过的物联网设备。通过使用自注意力机制提取特征并利用生成模型生成伪数据，ZEST能够在性能上取得显著的改进。 |
| [^52] | [Local Graph Clustering with Noisy Labels.](http://arxiv.org/abs/2310.08031) | 本论文研究了使用噪声节点标签作为额外节点信息的局部图聚类方法，并探究了将噪声标签纳入局部图聚类的好处。 |
| [^53] | [Robust 1-bit Compressed Sensing with Iterative Hard Thresholding.](http://arxiv.org/abs/2310.08019) | 本文研究了鲁棒的一比特压缩感知问题，并提出了二元迭代硬阈值化（BIHT）算法，在噪声情况下比目前已知方法表现更好。 |
| [^54] | [Why Train More? Effective and Efficient Membership Inference via Memorization.](http://arxiv.org/abs/2310.08015) | 本文通过策略性选择样本，最大化攻击成功并最小化影子模型的数量，从而提出了一种通过记忆进行有效和高效的成员推断攻击的方法。 |
| [^55] | [AutoFHE: Automated Adaption of CNNs for Efficient Evaluation over FHE.](http://arxiv.org/abs/2310.08012) | AutoFHE针对深度卷积神经网络的安全推理提出了一种自动方法，通过采用逐层混合次数多项式激活函数来解决现有方法的灵活性、次优逼近和限制性设计限制。 |
| [^56] | [LEMON: Lossless model expansion.](http://arxiv.org/abs/2310.07999) | LEMON是一种无损模型扩展方法，在深度神经网络中能够通过利用小型预训练模型的知识来初始化和训练大型模型，从而大大减少训练时间，同时具有通用性适用于各种网络结构。 |
| [^57] | [Reset It and Forget It: Relearning Last-Layer Weights Improves Continual and Transfer Learning.](http://arxiv.org/abs/2310.07996) | 本研究发展了一种重置最后一层权重的方法，称为"zapping"，通过这种方法可以提供更好的持续和迁移学习效果，同时具备简单实施和高效计算的特点。 |
| [^58] | [Multi-View Variational Autoencoder for Missing Value Imputation in Untargeted Metabolomics.](http://arxiv.org/abs/2310.07990) | 本文提出了一种新的方法，利用多视图变分自动编码器来填充非目标代谢组学中的缺失值，该方法利用了全基因组测序数据和参考代谢物的信息，可以有效地根据基因组信息填充缺失的代谢组学值。 |
| [^59] | [Semantic-Forward Relaying: A Novel Framework Towards 6G Cooperative Communications.](http://arxiv.org/abs/2310.07987) | 该论文提出了一种面向6G合作通信的新中继框架，通过提取传输语义特征减少转发负载，并提高网络的鲁棒性。设计了一种联合源信道编码算法，通过迭代交换外在信息来增强解码增益。仿真结果表明，即使在恶劣的信道条件下，该中继框架仍然可以有效地改善恢复的信息质量。 |
| [^60] | [Neural Combinatorial Optimization with Heavy Decoder: Toward Large Scale Generalization.](http://arxiv.org/abs/2310.07985) | 提出了一种具有强大通用性的轻编码器和重解码器（LEHD）模型，通过动态学习节点间关系，解决了神经组合优化在大规模实例上的问题，并应用于旅行商问题和容量限制车辆路径规划问题。 |
| [^61] | [RandCom: Random Communication Skipping Method for Decentralized Stochastic Optimization.](http://arxiv.org/abs/2310.07983) | RandCom是一种去中心化的随机通信跳跃方法，能够在分布式优化中通过概率性本地更新减少通信开销，并在不同的设置中实现线性加速。 |
| [^62] | [Reinforcement Learning of Display Transfer Robots in Glass Flow Control Systems: A Physical Simulation-Based Approach.](http://arxiv.org/abs/2310.07981) | 该论文讨论了使用深度强化学习的方法解决流控制系统调度优化问题的可能性，并提出了一种基于物理仿真的方法来验证该方法的有效性。 |
| [^63] | [GRASP: Accelerating Shortest Path Attacks via Graph Attention.](http://arxiv.org/abs/2310.07980) | GRASP是一个通过使用图注意力网络识别子图来加速最短路径攻击的优化算法，能够运行速度快达到10倍而仍保持解决方案的质量。 |
| [^64] | [Graph-SCP: Accelerating Set Cover Problems with Graph Neural Networks.](http://arxiv.org/abs/2310.07979) | 图形-SCP是一种使用图神经网络加速集合覆盖问题的方法，通过学习识别包含解空间的较小子问题来提高优化求解器的性能，实验结果表明，图形-SCP能够将问题大小减少30-70%，和商业求解器相比加速高达25倍，并且能够在给定的最优性阈值下改进或实现100%的最优性。 |
| [^65] | [Interpretable Diffusion via Information Decomposition.](http://arxiv.org/abs/2310.07972) | 本研究通过观察扩散和信息分解之间的关系，揭示了扩散模型学习到的细粒度关系，进一步解决了高维空间中信息携带变量的问题。 |
| [^66] | [Hyperparameter Adaptive Search for Surrogate Optimization: A Self-Adjusting Approach.](http://arxiv.org/abs/2310.07970) | 这个论文研究了代理优化算法中超参数的影响，并提出了一种自适应搜索的代理优化方法（HASSO），该方法可以动态调整超参数而不需要额外的评估。该方法旨在提高代理优化算法的可访问性、效果和收敛速度。 |
| [^67] | [CleftGAN: Adapting A Style-Based Generative Adversarial Network To Create Images Depicting Cleft Lip Deformity.](http://arxiv.org/abs/2310.07969) | 本研究提出了CleftGAN，即将基于风格的生成对抗网络适应于创建展示唇裂畸形的图像。通过使用少量训练图像，该模型可以产生高质量、多样化的唇裂图像，为面部唇裂评估系统的训练提供了解决方案。 |
| [^68] | [Towards Causal Deep Learning for Vulnerability Detection.](http://arxiv.org/abs/2310.07958) | 本文提出了一种针对漏洞检测的因果深度学习方法CausalVul，通过引入因果性，并设计新的扰动，解决了深度学习漏洞检测中模型不稳定和泛化性能差的问题。 |
| [^69] | [Cost-Driven Hardware-Software Co-Optimization of Machine Learning Pipelines.](http://arxiv.org/abs/2310.07940) | 该论文探索了在物联网设备中使用深度神经网络时的硬件软件协同优化，重点考虑了成本、延迟和用户体验，并研究了量化、模型缩放和多模态等方法与系统组件的相互作用。 |
| [^70] | [D2 Pruning: Message Passing for Balancing Diversity and Difficulty in Data Pruning.](http://arxiv.org/abs/2310.07931) | D2修剪是一种平衡数据多样性和困难度的方法，在coreset选择中同时考虑数据多样性和重要性评分。 |
| [^71] | [Enhanced sampling of Crystal Nucleation with Graph Representation Learnt Variables.](http://arxiv.org/abs/2310.07927) | 本研究提出了一种基于图神经网络的学习方法，使用自编码器从实验晶体结构中观察到的特征中推导出低维变量，并在增强抽样中偏置这些变量以实现可靠的状态转换和热力学权重。该方法在研究铁和甘氨酸的晶体成核过程中取得了准确的自由能计算结果，展示了改进抽样的潜力。en_tdlr: This study presents a graph neural network-based learning approach to derive low-dimensional variables from experimental crystal structures and bias them in enhanced sampling for reliable state transitions and thermodynamic weights. The approach achieves accurate free energy calculations in the nucleation processes of iron and glycine crystals, demonstrating the potential for improved sampling accuracy. |
| [^72] | [First-Order Dynamic Optimization for Streaming Convex Costs.](http://arxiv.org/abs/2310.07925) | 本文提出了一种新型优化算法，用于解决具有时变流式成本函数的凸优化问题，通过使用一阶导数进行计算，实现了高效的优化。通过与梯度下降算法比较和几个示例的验证，证明了该算法的有效性。 |
| [^73] | [The Expresssive Power of Transformers with Chain of Thought.](http://arxiv.org/abs/2310.07923) | 本论文研究基于思维链的Transformer的表达能力，通过允许使用中间生成的方式提高了Transformer的推理能力，并发现线性数量的解码步骤在标准计算复杂度下增加了明显的新能力。 |
| [^74] | [Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning.](http://arxiv.org/abs/2310.07918) | 本论文提出了一种上下文化政策恢复方法用于建模复杂的医疗决策过程，以解决现有模型在准确性和可解释性之间的权衡问题。该方法将决策策略拆分为上下文特定策略，通过多任务学习来实现建模，并提供复杂行为的简洁描述。 |
| [^75] | [A Review of Machine Learning Techniques in Imbalanced Data and Future Trends.](http://arxiv.org/abs/2310.07917) | 该论文综述了在非平衡数据中使用的各种机器学习方法，并提供了一个通用指南，旨在帮助研究人员在大规模非平衡数据中进行机器学习。 |
| [^76] | [Unraveling the Single Tangent Space Fallacy: An Analysis and Clarification for Applying Riemannian Geometry in Robot Learning.](http://arxiv.org/abs/2310.07902) | "Unraveling the Single Tangent Space Fallacy"论文分析和澄清了在机器人学习中应用黎曼几何的误区，该误区是指将数据仅投影到单一切空间中的方法。 |
| [^77] | [NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration.](http://arxiv.org/abs/2310.07896) | 本文提出了NoMaD，一种目标遮蔽扩散策略，用于同时处理目标导向导航和目标不可知探索。实验证明，相对于使用子目标提议或潜变量模型的方法，使用此统一策略具有更好的导航性能。 |
| [^78] | [Precise localization within the GI tract by combining classification of CNNs and time-series analysis of HMMs.](http://arxiv.org/abs/2310.07895) | 本文提出了一种通过使用CNN进行分类和HMM的时间序列分析，高效地将胃肠造影的图像进行分类，并通过连续的时间序列分析纠正CNN输出来实现精确定位。研究结果表明，该方法在Rhode Island胃肠病学数据集上达到了98.04％的准确率，可以在低功耗设备上使用。 |
| [^79] | [Efficient Integrators for Diffusion Generative Models.](http://arxiv.org/abs/2310.07894) | 本文提出了共轭积分器和分裂积分器两种框架，用于加速预训练模型中的样本生成。经过实证和理论研究，提出了一种最佳性能的混合方法，能够应用于扩散生成模型。 |
| [^80] | [ASV Station Keeping under Wind Disturbances using Neural Network Simulation Error Minimization Model Predictive Control.](http://arxiv.org/abs/2310.07892) | 本研究提出了一种通过神经网络仿真误差最小化模型预测控制实现在风扰动下ASV保持站位的方法，并在ROS和Gazebo仿真环境下进行了测试和比较。 |
| [^81] | [A Theory of Non-Linear Feature Learning with One Gradient Step in Two-Layer Neural Networks.](http://arxiv.org/abs/2310.07891) | 这篇论文提出了一种关于两层神经网络中非线性特征学习的理论。通过一步梯度下降训练的过程中引入不同的多项式特征，该方法能够学习到目标函数的非线性组件，而更新的神经网络的性能则由这些特征所决定。 |
| [^82] | [Leader-Follower Neural Networks with Local Error Signals Inspired by Complex Collectives.](http://arxiv.org/abs/2310.07885) | 领导者-跟随者神经网络是受到自然群体集合中观察到的规则启发的一种神经网络架构，利用局部误差信号训练并可选择性地结合反向传播和全局损失。通过大量实验研究工作人员行为和评估，这种神经网络具有高度的自组织和复杂性。 |
| [^83] | [The Thousand Faces of Explainable AI Along the Machine Learning Life Cycle: Industrial Reality and Current State of Research.](http://arxiv.org/abs/2310.07882) | 本文调查了可解释人工智能（XAI）在生产行业中的实际相关性，并将其与当前学术界XAI研究的现状相联系。通过对广泛的访谈和文献回顾的分析，发现了访谈结果与当前的研究方法之间存在差异。 |
| [^84] | [DeePref: Deep Reinforcement Learning For Video Prefetching In Content Delivery Networks.](http://arxiv.org/abs/2310.07881) | 本论文介绍了DeePref，一种用于在内容交付网络中进行在线视频内容预取的深度强化学习代理。传统的预取技术难以适应工作负载中的变化，而DeePref利用强化学习算法，可以自动适应不断变化的用户访问模式。 |
| [^85] | [TabLib: A Dataset of 627M Tables with Context.](http://arxiv.org/abs/2310.07875) | TabLib是一个包含上亿表格和上百亿上下文的数据集，规模和多样性使其在表格模态下具有巨大的潜力。 |
| [^86] | [Refined Mechanism Design for Approximately Structured Priors via Active Regression.](http://arxiv.org/abs/2310.07874) | 本研究提出了一种通过主动学习和机制设计相结合的方法，在拍卖中处理大量商品和策略招标人的问题。通过使用主题模型来近似投标人的先验分布，并设计出相应的机制，以提高机制的稳定性和适应性。 |
| [^87] | [QArchSearch: A Scalable Quantum Architecture Search Package.](http://arxiv.org/abs/2310.07858) | QArchSearch是一个基于人工智能的量子架构搜索软件包，通过使用QTensor库作为后端，它能够自动化地找到最佳的适用于特定任务和输入量子态的模型。它能够扩展到大型量子电路，并且能够探索不同的复杂模型，为不同的量子应用提供支持。 |
| [^88] | [CrIBo: Self-Supervised Learning via Cross-Image Object-Level Bootstrapping.](http://arxiv.org/abs/2310.07855) | CrIBo通过跨图像对象级引导进行自监督学习，可以提高密集视觉表示学习的性能，并在自然理解应用中表现出领先的性能。 |
| [^89] | [On the Computational Complexity of Private High-dimensional Model Selection via the Exponential Mechanism.](http://arxiv.org/abs/2310.07852) | 本文研究了在高维稀疏线性回归模型中的差分隐私模型选择问题。我们使用指数机制进行模型选择，并提出了Metropolis-Hastings算法来克服指数搜索空间的计算复杂性。我们的算法在一定边界条件下能够实现强模型恢复性质，并具有多项式混合时间和近似差分隐私性质。 |
| [^90] | [Towards the Fundamental Limits of Knowledge Transfer over Finite Domains.](http://arxiv.org/abs/2310.07838) | 本论文研究了在有限领域中从教师到学生分类器进行知识传递的统计效率，发现特权信息会加速传递，通过使用一种新颖的损失函数达到了知识传递的基本限制。 |
| [^91] | [Measuring Feature Sparsity in Language Models.](http://arxiv.org/abs/2310.07837) | 这项研究开发了度量方法来评估语言模型中特征稀疏性的成功，并测试了线性性和稀疏性假设的有效性。研究结果表明，语言模型的激活可以准确地建模为特征的稀疏线性组合，并且在第一层和最后一层中呈最稀疏状态。 |
| [^92] | [When, Why and How Much? Adaptive Learning Rate Scheduling by Refinement.](http://arxiv.org/abs/2310.07831) | 该论文介绍了一种通过细化分析学习率调度来解决实践中学习率调整与理论的不一致的方法，通过对观察到的梯度范数进行分析，得到了适应于特定任务的细化调度。该方法能够改善优化算法的收敛性能。 |
| [^93] | [Does Synthetic Data Make Large Language Models More Efficient?.](http://arxiv.org/abs/2310.07830) | 本文探讨了在NLP中合成数据生成的细微差别，重点关注基于模板的问题生成。通过评估其优势和固有限制，本研究揭示了合成数据对现代Transformer模型性能的影响，并强调了合成数据与真实世界数据之间所需的微妙平衡。 |
| [^94] | [Large Language Models Are Zero-Shot Time Series Forecasters.](http://arxiv.org/abs/2310.07820) | 大型语言模型（LLMs）如GPT-3和LLaMA-2能够令人惊讶地零-shot外推时间序列，其性能可与或超过专门设计的时间序列模型的性能相媲美。这是因为LLMs具有自然地表示多模态分布的能力，并且具有与许多时间序列的重复季节趋势特征相一致的简单性和重复性偏好。 |
| [^95] | [Faithfulness Measurable Masked Language Models.](http://arxiv.org/abs/2310.07819) | 本论文提出了一种可度量忠实性的掩码语言模型，通过使用一种新颖的微调方法，将屏蔽令牌作为设计使其成为分布内，以解决解释自然语言处理模型时常见的问题。 |
| [^96] | [Language Models As Semantic Indexers.](http://arxiv.org/abs/2310.07815) | 本文介绍了一种使用生成性语言模型学习语义ID的自监督框架LMINDEXER。 |
| [^97] | [Explorable Mesh Deformation Subspaces from Unstructured Generative Models.](http://arxiv.org/abs/2310.07814) | 本文介绍了一种方法，通过构建从易于导航的2D探索空间到预训练生成模型的子空间的映射，来探索一组标志性形状的变化。 |
| [^98] | [Online RL in Linearly $q^\pi$-Realizable MDPs Is as Easy as in Linear MDPs If You Learn What to Ignore.](http://arxiv.org/abs/2310.07811) | 该论文研究了在线强化学习中在线性$q^\pi$可实现的MDPs和线性MDPs的差异，并提出了一种新颖的学习算法，可以通过学习忽略某些状态将问题转化为线性MDP。 |
| [^99] | [FedSym: Unleashing the Power of Entropy for Benchmarking the Algorithms for Federated Learning.](http://arxiv.org/abs/2310.07807) | 本文提出了一种利用熵和对称性构建数据分布的方法，用于基于联邦学习的算法基准测试。该方法解决了数据分区技术中存在的精度不足和无法增量挑战算法的问题。 |
| [^100] | [Generative Modeling with Phase Stochastic Bridges.](http://arxiv.org/abs/2310.07805) | 通过在相位空间中构建路径测度，我们提出了一种新颖的生成建模框架，可以在动力传播的早期阶段生成逼真的数据点，并利用额外的速度信息实现高效的数据生成。 |
| [^101] | [Explainable Attention for Few-shot Learning and Beyond.](http://arxiv.org/abs/2310.07800) | 本研究介绍了一种用于少样本学习的解释性注意力框架，旨在通过识别输入数据的关键部分来提高模型性能。 |
| [^102] | [A Transfer-Learning-Based Prognosis Prediction Paradigm that Bridges Data Distribution Shift across EMR Datasets.](http://arxiv.org/abs/2310.07799) | 本研究提出了一种基于迁移学习的EMR数据集之间数据分布变化的预后预测模型，通过构建过渡模型解决了不同数据集的特征不匹配问题，提高了深度学习模型的效率。 |
| [^103] | [GenTKG: Generative Forecasting on Temporal Knowledge Graph.](http://arxiv.org/abs/2310.07793) | 研究提出了一种名为GenTKG的生成模型，用于在时间知识图谱上进行预测。该模型通过结合基于时间逻辑规则的检索策略和轻量级的参数效率指导，克服了复杂的时间图数据结构和庞大的数据量所带来的挑战。 |
| [^104] | [Using Spark Machine Learning Models to Perform Predictive Analysis on Flight Ticket Pricing Data.](http://arxiv.org/abs/2310.07787) | 本文使用Spark机器学习模型对航班票价数据进行预测分析，发现了一些关键的商业洞察，并讨论了过程和工具的使用。 |
| [^105] | [Non-Stationary Contextual Bandit Learning via Neural Predictive Ensemble Sampling.](http://arxiv.org/abs/2310.07786) | 本文介绍了一种新颖的非稳态情境赌博算法，通过将可扩展的基于深度神经网络的架构与精心设计的探索机制相结合，在非稳态环境中优先收集持久价值信息，从而显著提高了性能。 |
| [^106] | [Promoting Robustness of Randomized Smoothing: Two Cost-Effective Approaches.](http://arxiv.org/abs/2310.07780) | 本文提出了两种成本效益的方法来提高随机平滑的鲁棒性，一种是结合对抗训练和鲁棒性认证的新的鲁棒训练方法，另一种是后处理方法，这些方法在保持良好性能的同时能够提高随机平滑的鲁棒性。 |
| [^107] | [Feature Learning and Generalization in Deep Networks with Orthogonal Weights.](http://arxiv.org/abs/2310.07765) | 我们通过使用正交矩阵集合初始化权重并使用tanh激活函数，解决了全连接深度神经网络在初始化中具有与深度无关的线性波动问题。此外，我们发现神经切向核（NTK）及其后代的所有相关函数在逆宽度的主导阶段在深度约为20的位置饱和，而不是不断增长。 |
| [^108] | [Self-supervised Representation Learning From Random Data Projectors.](http://arxiv.org/abs/2310.07756) | 本文提出了一种无监督表示学习（SSRL）方法，通过重建随机数据投影来学习高质量的数据表示，不依赖于增强或掩蔽技术，可以应用于任何数据模态和网络架构。实验结果表明该方法在各种任务中优于其他SSRL算法。 |
| [^109] | [Accountability in Offline Reinforcement Learning: Explaining Decisions with a Corpus of Examples.](http://arxiv.org/abs/2310.07747) | 本论文介绍了一种可解释的离线控制器方法，通过使用离线数据集作为决策语料库，在低数据场景中实现了问责制的控制，并在医疗保健领域展示了良好的性能。 |
| [^110] | [Deep Reinforcement Learning for Autonomous Cyber Operations: A Survey.](http://arxiv.org/abs/2310.07745) | 深度强化学习被应用于自主网络安全行动有很大潜力，但在实际应用中还面临许多挑战。 |
| [^111] | [Spiral-Elliptical automated galaxy morphology classification from telescope images.](http://arxiv.org/abs/2310.07740) | 本研究发展了两个新颖的星系形态统计方法，并提出了简化版本的现有图像统计方法，通过提取望远镜图像中的特征，实现了自动分类螺旋-椭圆星系形态。 |
| [^112] | [Observatory: Characterizing Embeddings of Relational Tables.](http://arxiv.org/abs/2310.07736) | Observatory提出了一个正式框架来分析关系表的嵌入表示，以帮助研究人员和实践者更好地理解和选择适合特定任务的模型。 |
| [^113] | [Extreme Image Transformations Facilitate Robust Latent Object Representations.](http://arxiv.org/abs/2310.07725) | 本论文展示了使用极端图像转换（EIT）对预训练网络进行微调可以学习到稳健的潜在对象表示，并且可以提高网络对各种强度的常见对抗攻击的性能。 |
| [^114] | [Visual Forecasting as a Mid-level Representation for Avoidance.](http://arxiv.org/abs/2310.07724) | 本研究提出了一种创新的替代方法——视觉预测，通过引入直观的视觉提示，投射动态物体的未来轨迹，提高智能体的感知能力并实现预期的动作。研究结果验证了视觉预测作为导航问题解决方案的可行性。 |
| [^115] | [Parametric Leaky Tanh: A New Hybrid Activation Function for Deep Learning.](http://arxiv.org/abs/2310.07720) | 本文提出了一种新颖的激活函数Parametric Leaky Tanh (PLTanh)，结合了Tanh和Leaky ReLU (LReLU)的优点，解决了'dying ReLU'问题，有助于在网络内部学习更复杂的非线性关系。 |
| [^116] | [Rethinking the BERT-like Pretraining for DNA Sequences.](http://arxiv.org/abs/2310.07644) | 重新考虑了基于DNA序列的BERT-like预训练方法，通过使用K-mer重叠标记化，在下游任务的微调阶段和预训练过程中都取得了一致的性能改善。 |
| [^117] | [In-Context Unlearning: Language Models as Few Shot Unlearners.](http://arxiv.org/abs/2310.07579) | 这项工作提出了一种新的LLM消除方法，称为“基于上下文的消除”，它提供了上下文的输入且无需更新模型参数。这种方法解决了消除对于很大模型来说在计算上的困难，并在实践中具有更高的可行性和便捷性。 |
| [^118] | [Imitation Learning from Observation with Automatic Discount Scheduling.](http://arxiv.org/abs/2310.07433) | 我们提出了一个新颖的观察学习模仿(ILfO)框架，能够解决由于奖励信号分配错误导致代理无法学习初始行为的问题。 |
| [^119] | [Quantum-Enhanced Forecasting: Leveraging Quantum Gramian Angular Field and CNNs for Stock Return Predictions.](http://arxiv.org/abs/2310.07427) | 该论文提出了一种名为Quantum Gramian Angular Field (QGAF)的新方法，通过将量子计算技术与深度学习相结合，成功将股票回报时间序列数据转化为适合卷积神经网络（CNN）训练的二维图像，从而提高了预测的精度。实验证明，相比传统方法，QGAF方法具有显著的性能优势。 |
| [^120] | [NuTime: Numerically Multi-Scaled Embedding for Large-Scale Time Series Pretraining.](http://arxiv.org/abs/2310.07402) | 本研究通过采用Transformer架构和数值多尺度嵌入模块，使时间序列自监督模型能够扩展到大规模数据集，并在大规模数据集上进行预训练。 |
| [^121] | [GraphControl: Adding Conditional Control to Universal Graph Pre-trained Models for Graph Domain Transfer Learning.](http://arxiv.org/abs/2310.07365) | 在图领域迁移学习中，GraphControl通过添加条件控制实现了对通用图预训练模型的有效迁移，克服了不同图域间的属性语义差异问题。 |
| [^122] | [WiGenAI: The Symphony of Wireless and Generative AI via Diffusion Models.](http://arxiv.org/abs/2310.07312) | WiGenAI通过引入扩散模型，将生成式人工智能应用于无线通信系统中，为研究奠定基础。这篇文章介绍了扩散模型作为生成模型的最新范式，并讨论了它在无线通信系统中的应用。通过两个案例研究展示了扩散模型在开发韧性的AI本地通信系统中的潜力。 |
| [^123] | [Score Regularized Policy Optimization through Diffusion Behavior.](http://arxiv.org/abs/2310.07297) | 通过利用扩散行为模型，我们提出了一种在离线强化学习中用于优化策略的得分正则化方法，从而避免了耗时且计算密集的扩散采样方案，并在D4RL任务上实现了超过25倍的动作采样速度提升。 |
| [^124] | [Federated Generalization via Information-Theoretic Distribution Diversification.](http://arxiv.org/abs/2310.07171) | 该论文研究了联邦学习中泛化能力的挑战，特别关注训练分布和测试分布的不匹配。提出了一种信息论的泛化方法来解决这个问题。 |
| [^125] | [Flood and Echo: Algorithmic Alignment of GNNs with Distributed Computing.](http://arxiv.org/abs/2310.06970) | 本论文提出了一个基于分布式算法设计原则的新的执行框架：洪水和回声网络。通过波状激活模式，它可以在整个图中传递消息，并且可以有效地处理更大的图形。 |
| [^126] | [NECO: NEural Collapse Based Out-of-distribution detection.](http://arxiv.org/abs/2310.06823) | NECO是一种基于神经坍塌的新颖的超出分布检测方法，利用几何属性和主成分空间识别OOD数据，在小规模和大规模任务上取得了最先进的结果，并展示了强大的泛化能力。 |
| [^127] | [FABind: Fast and Accurate Protein-Ligand Binding.](http://arxiv.org/abs/2310.06763) | FABind是一个结合了口袋预测和对接的端到端模型，旨在实现快速准确的蛋白-配体结合预测。 |
| [^128] | [SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural Network.](http://arxiv.org/abs/2310.06488) | 本论文引入了一种名为SpikeCLIP的新框架，通过对比语言-图像预训练实现了脉冲神经网络的多模态扩展，并在能源效率和性能方面取得了可比较的结果。 |
| [^129] | [GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using Large Language Models.](http://arxiv.org/abs/2310.06225) | 本研究全面评估了大型语言模型（LLMs）在回答农业相关问题方面的能力，并使用RAG和ER技术提高了LLMs的性能。其中，GPT-4在农业考试中取得了及格分数以获得认证。 |
| [^130] | [Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts.](http://arxiv.org/abs/2310.05898) | Lion是通过程序搜索发现的新优化器，在训练大型AI模型方面表现出有希望的结果，具有更高的内存效率。尽管其理论基础不明确，但基于连续时间和离散时间分析，我们证明Lion是一种理论上新颖且有原则的方法，可在最小化一般损失函数的同时强制执行边界约束。 |
| [^131] | [Locality-Aware Generalizable Implicit Neural Representation.](http://arxiv.org/abs/2310.05624) | 本文提出了一种结合Transformer编码器和具有本地感知的INR解码器的框架，用于解决泛化的隐式神经表示中无法定位和捕获细粒度细节的问题。 |
| [^132] | [Clustering Three-Way Data with Outliers.](http://arxiv.org/abs/2310.05288) | 这项研究提出了一种用于聚类矩阵形式数据的方法，可以处理其中的异常值。 |
| [^133] | [TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting.](http://arxiv.org/abs/2310.04948) | 本文提出了一个新的框架 TEMPO，通过利用时间序列任务的两个重要归纳偏差，即将复杂交互分解和引入基于选择的提示来有效学习时间序列表示。 |
| [^134] | [DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery through Sophisticated AI System Technologies.](http://arxiv.org/abs/2310.04610) | DeepSpeed4Science计划旨在通过先进的AI系统技术加速科学发现，提供针对独特复杂性的解决方案，为自然科学带来重大进展。 |
| [^135] | [Beyond Uniform Sampling: Offline Reinforcement Learning with Imbalanced Datasets.](http://arxiv.org/abs/2310.04413) | 该论文提出了一种解决离线强化学习中不平衡数据集问题的方法，通过采样策略使策略只受``好数据"限制，而不是所有的动作，提高了离线RL算法的性能。 |
| [^136] | [On Training Derivative-Constrained Neural Networks.](http://arxiv.org/abs/2310.01649) | 该论文研究了导数约束神经网络（DC NN），提出了一种整合RELU激活函数（IReLU）来改进DC NN的训练，并探究了去归一化和标签缩放对于稳定DC训练的作用。研究还评估了这些方法在物理相关设置中的效果，证明了使用IReLU激活函数、去归一化和标签缩放的现有架构更好地融入了导数约束提供的训练信号。 |
| [^137] | [Memorization with neural nets: going beyond the worst case.](http://arxiv.org/abs/2310.00327) | 本文研究了神经网络的插值问题，提出了一种简单的随机算法，在给定的数据集和两个类的情况下，能够以很高的概率构建一个插值的神经网络。这些结果与训练数据规模无关。 |
| [^138] | [A Neural-preconditioned Poisson Solver for Mixed Dirichlet and Neumann Boundary Conditions.](http://arxiv.org/abs/2310.00177) | 我们引入了一个用于混合边界条件的泊松方程的神经预处理迭代求解器，核心是一个神经网络，能够近似逆离散结构网格拉普拉斯算子，并且在训练集之外的边界条件下仍然有效。 |
| [^139] | [A Design Toolbox for the Development of Collaborative Distributed Machine Learning Systems.](http://arxiv.org/abs/2309.16584) | 我们开发了一个CDML设计工具箱，可以指导开发者设计满足用例要求的协作分布式机器学习系统。 |
| [^140] | [A Comprehensive Review on Tree Detection Methods Using Point Cloud and Aerial Imagery from Unmanned Aerial Vehicles.](http://arxiv.org/abs/2309.16375) | 本文综述了无人机在树木检测中的应用方法，包括激光雷达和航拍图像的点云和图像数据。通过对这些方法的分类和对比分析，总结了它们的性能、优势和应用场景。 |
| [^141] | [Finite Scalar Quantization: VQ-VAE Made Simple.](http://arxiv.org/abs/2309.15505) | 该论文提出了有限标量量化 (FSQ) 方法，用来简化 VQ-VAE 方法中的向量量化 (VQ)。通过投影和量化 VAE 表示，我们得到与 VQ 相同大小的码本。在这种离散表示上，我们可以训练相同的模型，并在图像生成、多模态生成和计算机视觉任务中取得竞争性能。 |
| [^142] | [Model-Free, Regret-Optimal Best Policy Identification in Online CMDPs.](http://arxiv.org/abs/2309.15395) | 本文提出了一种无模型的算法，名为PRI，用于在线CMDPs中的最佳策略识别问题。该算法基于CMDPs的有限随机性属性，能够以低遗憾并以高概率识别出最优策略。 |
| [^143] | [PRiSM: Enhancing Low-Resource Document-Level Relation Extraction with Relation-Aware Score Calibration.](http://arxiv.org/abs/2309.13869) | PRiSM是一种增强低资源文档级关系抽取的方法，通过关系感知分数校准来提高模型性能，成功地降低了在低资源环境下训练模型时的校准误差。 |
| [^144] | [MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback.](http://arxiv.org/abs/2309.10691) | MINT是一个评估LLMs在多轮交互中解决任务能力的基准，通过使用工具和利用用户的自然语言反馈。它解决了当前评估协议忽略细致互动和低估自然语言反馈的问题，促进了研究基准评估和实际应用之间的一致性。 |
| [^145] | [On Regularized Sparse Logistic Regression.](http://arxiv.org/abs/2309.05925) | 本文提出了解决正则稀疏逻辑回归的方法，包括$\ell_1$正则化稀疏逻辑回归和一些满足先决条件的非凸惩罚正则化稀疏逻辑回归。经验实验表明，这些算法能够以较低的计算成本有效地进行分类和特征选择。 |
| [^146] | [DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning.](http://arxiv.org/abs/2309.05173) | DePT通过将软提示分解为较短的软提示和一对低秩矩阵，并用两个不同的学习率来优化，以解决提示调整对训练和推理时间以及内存使用的影响，从而实现更好的性能。 |
| [^147] | [Seeing-Eye Quadruped Navigation with Force Responsive Locomotion Control.](http://arxiv.org/abs/2309.04370) | 本研究提出了一种带有力响应式运动控制的导盲四足机器人导航系统，通过同时训练运动控制器和外力估计器，实现对外部拉扯力的稳健感知和响应。实验结果表明该系统能够准确导航并绕过障碍物，具有较强的鲁棒性。 |
| [^148] | [Pure Monte Carlo Counterfactual Regret Minimization.](http://arxiv.org/abs/2309.03084) | 纯蒙特卡洛反事实遗憾最小化算法（PCFR）是一种结合了反事实遗憾最小化（CFR）和虚拟游戏（FP）概念的新算法，能够与各种CFR变体相结合，包括蒙特卡洛CFR（MCCFR）。PCFR具有更好的性能和较快的收敛速度，同时降低了时间和空间复杂度。 |
| [^149] | [Theoretical Explanation of Activation Sparsity through Flat Minima and Adversarial Robustness.](http://arxiv.org/abs/2309.03004) | 提出了一个理论解释，将梯度稀疏性和激活稀疏性解释为对抗性鲁棒性的必要步骤，以隐藏特征和参数而言，这大致等于对学习良好模型的极小值平坦性。 |
| [^150] | [PromptTTS 2: Describing and Generating Voices with Text Prompt.](http://arxiv.org/abs/2309.02285) | PromptTTS 2是一种使用文本提示来描述和生成声音的方法，通过变化网络提供声音的可变性信息，并利用大型语言模型（LLM）来生成高质量的文本提示。 |
| [^151] | [Bengali Document Layout Analysis -- A YOLOV8 Based Ensembling Approach.](http://arxiv.org/abs/2309.00848) | 本文提出了一种基于YOLOv8模型和创新的后处理技术的孟加拉文档布局分析方法，通过数据增强和两阶段预测策略实现了准确的元素分割。该方法优于单个基础架构，并解决了BaDLAD数据集中的问题，有助于提高OCR和文档理解能力。 |
| [^152] | [Learning Collaborative Information Dissemination with Graph-based Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2308.16198) | 本论文介绍了一种使用多智能体强化学习的方法来实现协作信息传播。通过提出分布式POMDP形式，在消息转发上实现了每个智能体的独立决策，相比传统的基于多点中继选择的启发式方法具有重大创新和贡献。同时，该方法利用图卷积强化学习和动态注意力机制捕捉关键网络特征，并提出了不同信息交换方式的两种方法进行评估。 |
| [^153] | [Multi-Objective Optimization for Sparse Deep Neural Network Training.](http://arxiv.org/abs/2308.12243) | 这项工作提出了一种用于稀疏深度神经网络训练的多目标优化算法，通过加权Chebyshev标量化和增广Lagrangian方法，解决了同时优化多个任务时的挑战。 |
| [^154] | [LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs.](http://arxiv.org/abs/2308.08469) | 这项工作提出了LLM4TS方法，利用预训练的LLMs增强时间序列预测能力。通过两阶段微调和参数高效微调，提高了LLMs处理时间序列数据的能力。 |
| [^155] | [BarlowRL: Barlow Twins for Data-Efficient Reinforcement Learning.](http://arxiv.org/abs/2308.04263) | BarlowRL通过将Barlow Twins和DER相结合，实现了数据效率强化学习，并在Atari 100k基准测试上优于其他算法。它通过信息扩散避免了维度折叠，使得RL算法能够利用均匀分布的状态表示，从而实现卓越的性能。 |
| [^156] | [Asynchronous Evolution of Deep Neural Network Architectures.](http://arxiv.org/abs/2308.04102) | 本文提出了一种通用的异步评估策略，用于增加进化神经网络架构搜索的吞吐量。该策略维护一个个体队列，并在适当数量的个体被评估后立即进入下一代，平衡多样性和效率。 |
| [^157] | [Nest-DGIL: Nesterov-optimized Deep Geometric Incremental Learning for CS Image Reconstruction.](http://arxiv.org/abs/2308.03807) | 提出了一种基于第二Nesterov近端梯度优化的深度几何增量学习框架，能够在图像重建时减轻伪影，同时实现快速收敛。 |
| [^158] | [Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning.](http://arxiv.org/abs/2307.03486) | 通过对比学习方法，我们在强化学习中提出了新的成就蒸馏方法，可以加强代理对下一个解锁成就的预测能力，并优于先前的模型驱动和层次化方法。 |
| [^159] | [Distilling Large Vision-Language Model with Out-of-Distribution Generalizability.](http://arxiv.org/abs/2307.03135) | 本文研究了针对大型视觉语言模型的模型压缩方法，将教师模型的视觉表示压缩到学生模型中。研究重点在于超出分布可泛化的问题，并提出了两个原则来增强学生模型的性能。 |
| [^160] | [Elastic Decision Transformer.](http://arxiv.org/abs/2307.02484) | 弹性决策变压器（EDT）通过在测试时间进行动作推断时调整历史长度来实现轨迹拼接，填补了决策变压器（DT）在这一方面的性能差距，并且在多任务情况下胜过基于Q-Learning的方法。 |
| [^161] | [Emulating the dynamics of complex systems using autoregressive models on manifolds (mNARX).](http://arxiv.org/abs/2306.16335) | 本研究提出了一种名为mNARX的方法，通过构建流形和自回归模型，以高效准确地近似复杂系统的动力学响应。这种方法能够将整个问题分解成较小的子问题，具有良好的可扩展性，并且与传统的降维技术相结合，适用于建模复杂系统。 |
| [^162] | [Smoothed $f$-Divergence Distributionally Robust Optimization: Exponential Rate Efficiency and Complexity-Free Calibration.](http://arxiv.org/abs/2306.14041) | 分布鲁棒优化在实现统计保证界限时存在限制和保守性问题，但平滑的$f$-散度分布鲁棒优化可在指数衰减率方面实现最紧密的统计保证。 |
| [^163] | [GIO: Gradient Information Optimization for Training Dataset Selection.](http://arxiv.org/abs/2306.11670) | GIO是一种可伸缩且任务不可知的方法，通过对目标的简单放松和高效实现，可以在非常小的训练集上产生出色的结果。 |
| [^164] | [Physics Constrained Unsupervised Deep Learning for Rapid, High Resolution Scanning Coherent Diffraction Reconstruction.](http://arxiv.org/abs/2306.11014) | 提出了一种无监督的物理信息神经网络重建方法，PtychoPINN，通过将衍射正演图与重叠测量的实空间约束相结合，显著提高了重建质量和空间分辨率。 |
| [^165] | [Theoretical Hardness and Tractability of POMDPs in RL with Partial Online State Information.](http://arxiv.org/abs/2306.08762) | 本论文研究了具有部分在线状态信息的POMDP问题的理论困难性和可计算性。作者通过建立下界得出一个惊人的难度结果：除非具有完整的在线状态信息，否则需要指数级的样本复杂度才能得到POMDP的最优策略解。然而，作者还发现了具有部分在线状态信息下的可计算POMDP类别，并提出了新的算法来证明其接近最优性。 |
| [^166] | [Variational Imbalanced Regression.](http://arxiv.org/abs/2306.06599) | 本文提出的变分不平衡回归（VIR）模型通过引入Probabilistic Reweighting方法，可以在不平衡回归方面表现良好，并自然产生合理的不确定性估计。 |
| [^167] | [Learning Joint Latent Space EBM Prior Model for Multi-layer Generator.](http://arxiv.org/abs/2306.06323) | 本文研究了学习多层生成器模型的基本问题，并提出了一种以多层生成器为骨干的联合潜在空间EBM先验模型，该模型可以更好地学习复杂的数据分布和分层表示，实现更好的生成质量和修复结果。 |
| [^168] | [Learning to Simulate Tree-Branch Dynamics for Manipulation.](http://arxiv.org/abs/2306.03410) | 本论文提出了一种仿真驱动的逆推理方法来学习树枝动态，并可以操纵可变形的植被，以解决密集植被中容易遮挡的任务，算法结合了生物学上的假设和传统参数推理方法的有限差分方案。 |
| [^169] | [Memorization Capacity of Multi-Head Attention in Transformers.](http://arxiv.org/abs/2306.02010) | 本文研究了多头注意力机制的记忆能力，发现在特定的假设下，注意力层可以记忆Ω(Hn)个示例序列，这对于理解transformers的记忆容量有重要意义。 |
| [^170] | [Relaxing the Additivity Constraints in Decentralized No-Regret High-Dimensional Bayesian Optimization.](http://arxiv.org/abs/2305.19838) | 本文提出了一种放松去中心化无遗憾高维贝叶斯优化中加法约束的方法，并且解决了过度探索问题。该方法称为DumBO，并且在实验中展示了竞争性能。 |
| [^171] | [OWAdapt: An adaptive loss function for deep learning using OWA operators.](http://arxiv.org/abs/2305.19443) | 本文提出了一种基于OWA算子的模糊自适应损失函数，通过迭代加权策略应对类别级别噪声条件，提高深度学习在分类任务中的性能。实验证明该方法在各种分类任务中均优于传统的常用损失函数。 |
| [^172] | [Differentially-Private Decision Trees and Provable Robustness to Data Poisoning.](http://arxiv.org/abs/2305.15394) | 本论文提出了一种名为PrivaTree的差分隐私决策树方法，通过使用私有直方图选择分割点来在隐私保护与模型效用之间取得更好的平衡。这种方法能够接收混合的数值和类别数据，并且能够在数据篡改方面表现出可靠性。 |
| [^173] | [Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery.](http://arxiv.org/abs/2305.14259) | 本文介绍了一种基于文献的发现方法，通过上下文化的学习生成新的科学方向，克服了标准方法在预测关联、忽略上下文等方面的局限性。模型使用了引文和知识图关系的网络，并使用大型语言模型进行评估，发现GPT4在生成创新思想方面表现出色。 |
| [^174] | [Conditional Mutual Information for Disentangled Representations in Reinforcement Learning.](http://arxiv.org/abs/2305.14133) | 本论文提出了一种用于解决强化学习中训练数据相关性问题的方法，通过最小化表示中特征之间的条件互信息，学习具有相关特征的解耦表示。实验证明该方法可以提高泛化性能并处理密集观测的相关特征。 |
| [^175] | [Generalization bounds for neural ordinary differential equations and deep residual networks.](http://arxiv.org/abs/2305.06648) | 本研究提出了神经常微分方程及其变体的泛化界限，涵盖了深度残差网络，其泛化界限与连续权重差异大小有关。 |
| [^176] | [Joint Metrics Matter: A Better Standard for Trajectory Forecasting.](http://arxiv.org/abs/2305.06292) | 这项研究展示了边际度量标准不足以完全捕捉多个互动代理的联合表现，提出使用联合度量标准进行轨迹预测方法评估的重要性。 |
| [^177] | [Limits of Model Selection under Transfer Learning.](http://arxiv.org/abs/2305.00152) | 这篇论文介绍了在转移学习下模型选择存在的限制，其转移距离会影响自适应速率，可能导致速率较慢。 |
| [^178] | [Diffusion-based Generative AI for Exploring Transition States from 2D Molecular Graphs.](http://arxiv.org/abs/2304.12233) | 本文提出了一种基于扩散的生成式方法(TSDiff)，用于从二维分子图中预测过渡态几何结构。与现有具有 3D 几何结构机器学习模型相比，TSdiff 的准确性和效率都更高。TSDiff 能够找到比参考数据库更优化的反应途径，在反应势垒更低的情况下找到更为优化的反应路径。 |
| [^179] | [Generative Modeling of Time-Dependent Densities via Optimal Transport and Projection Pursuit.](http://arxiv.org/abs/2304.09663) | 本文提出了一种基于最优输运和投影追踪的方法，用于便宜、高效地生成时态密度建模，其最优映射与恒等映射接近，训练过程高度并行化。 |
| [^180] | [Variational operator learning: A unified paradigm for training neural operators and solving partial differential equations.](http://arxiv.org/abs/2304.04234) | 本文提出了变分算子学习（VOL）的范式，同时训练神经算子和解决偏微分方程（PDE）。使用正反传递循环和自动微分实现了变分操作，通过最速下降法和共轭梯度法进行神经算子的简单但有效的训练。实验结果非常好。 |
| [^181] | [High-fidelity Pseudo-labels for Boosting Weakly-Supervised Segmentation.](http://arxiv.org/abs/2304.02621) | 本文提出了一种使用马尔可夫随机场增强弱监督分割中的高保真伪标签生成方法，能够生成更准确的伪标签，并使用新的训练策略来实现更好的收敛。实验结果表明该方法达到了弱监督分割方法的最佳性能。 |
| [^182] | [LLMMaps -- A Visual Metaphor for Stratified Evaluation of Large Language Models.](http://arxiv.org/abs/2304.00457) | LLMMaps是一种分层评估大型语言模型性能的可视化技术，能够揭示取得高准确度和产生幻觉的子领域，并指导模型的进一步发展。 |
| [^183] | [An interpretable neural network-based non-proportional odds model for ordinal regression with continuous response.](http://arxiv.org/abs/2303.17823) | 本文提出了一种基于可解释神经网络的非比例赔率模型 (N$^3$POM) 用于有序回归，可以对连续变量进行预测，同时保留了可解释性，具有灵活性。 |
| [^184] | [Data-Centric Learning from Unlabeled Graphs with Diffusion Model.](http://arxiv.org/abs/2303.10108) | 本文提出了一种从无标签图中提取知识并增强属性预测模型的数据中心方法，使用扩散模型和两个新目标进行去噪，实验证明其效果比14种现有方法更好。 |
| [^185] | [Only Pay for What Is Uncertain: Variance-Adaptive Thompson Sampling.](http://arxiv.org/abs/2303.09033) | 本文提出了一种针对多臂赌博机的方差自适应汤普森采样算法，通过考虑奖励方差的信息减少了遗憾，同时提高了鲁棒性 |
| [^186] | [Optimizing Convolutional Neural Networks for Chronic Obstructive Pulmonary Disease Detection in Clinical Computed Tomography Imaging.](http://arxiv.org/abs/2303.07189) | 本文旨在通过探索手动调整和自动化窗口设置优化，利用卷积神经网络在临床计算机断层扫描图像中检测慢性阻塞性肺疾病。研究结果表明，通过添加自定义层实现的自动化窗口设置优化可以改善检测性能。 |
| [^187] | [A Complete Recipe for Diffusion Generative Models.](http://arxiv.org/abs/2303.01748) | 本文提出了一种完整的扩散生成模型的配方，利用可扩展的贝叶斯后验采样器的见解，确保收敛到所需的目标分布。在这个方法的基础上，引入了相空间Langevin扩散（PSLD），在扩展空间中进行基于评分的建模，展现出更优质的样本质量和改进的速度-质量权衡。 |
| [^188] | [Analyzing And Editing Inner Mechanisms Of Backdoored Language Models.](http://arxiv.org/abs/2302.12461) | 本研究分析并编辑暗藏后门的语言模型的内部机制，发现早期层的MLP模块和初始嵌入投影是后门机制中最重要的部分。通过使用PCP消融技术替换变压器模块，我们成功删除、插入和修改后门机制，并显著改善了后门的输出效果。 |
| [^189] | [Participatory Personalization in Classification.](http://arxiv.org/abs/2302.03874) | 本研究引入了一类名为参与性系统的分类模型，允许个体在预测时选择个性化，从而促进并知情同意、提高性能和数据使用。 |
| [^190] | [Provably Efficient Offline Goal-Conditioned Reinforcement Learning with General Function Approximation and Single-Policy Concentrability.](http://arxiv.org/abs/2302.03770) | 本文提供了关于离线目标条件强化学习算法的理论分析，证明了经过轻微修改后，该算法在满足通用函数逼近时具有 O(poly(1/ε)) 的样本复杂度。 |
| [^191] | [Bitrate-Constrained DRO: Beyond Worst Case Robustness To Unknown Group Shifts.](http://arxiv.org/abs/2302.02931) | 本文提出了一种针对分布变化不确定性的受比特率限制的DRO方法，通过假设简单群体函数的条件下，以保持高准确性的模型在低比特率特征上的表现。 |
| [^192] | [ImageNomer: description of a functional connectivity and omics analysis tool and case study identifying a race confound.](http://arxiv.org/abs/2302.00767) | ImageNomer是一个功能连接和组学分析工具，通过提供易于导航的GUI前端以解决fMRI和基因组数据分析中的人口统计学混淆和质量控制问题。 |
| [^193] | [A Framework for Adapting Offline Algorithms to Solve Combinatorial Multi-Armed Bandit Problems with Bandit Feedback.](http://arxiv.org/abs/2301.13326) | 这个论文介绍了一个框架，用于将离线算法调整为只需要赌徒反馈的亚线性α-后悔方法来解决组合多臂赌博问题，该框架可以实现对于时间界T的预期累积α-后悔依赖度为O(T^（2/3）log（T^（1/3））)。 |
| [^194] | [Efficient Hyperdimensional Computing.](http://arxiv.org/abs/2301.10902) | 本论文研究了高维计算中高维度超向量的必要性，并通过理论分析发现，超向量维度的增加会降低高维计算的预测准确度。基于此，研究人员开发了使用维度更低的二进制超向量的高维计算模型，同时保持相等甚至更优的准确度。 |
| [^195] | [Automatic Intrinsic Reward Shaping for Exploration in Deep Reinforcement Learning.](http://arxiv.org/abs/2301.10886) | 本文提出了一种名为AIRS的自动内在奖励塑造探索方法，可以提供高质量的内在激励以增强强化学习中的探索性能；并开发了高效可靠的内在奖励工具包。实验表明，AIRS性能卓越，能够胜过基准方案。 |
| [^196] | [Identifying latent distances with Finslerian geometry.](http://arxiv.org/abs/2212.10010) | 本文提出了一种新的度量方法，可以明确地最小化期望长度，用于在生成模型的潜在空间中导航。这种方法定义了一个 Finsler 度量，在实践中可以替代现有的随机拉回度量的近似方法。 |
| [^197] | [On the Security Vulnerabilities of Text-to-SQL Models.](http://arxiv.org/abs/2211.15363) | 该论文揭示了Text-to-SQL模型存在的安全漏洞，并证明了这些漏洞能够被恶意利用产生攻击，通过对商业应用和开源语言模型的实验验证。该研究意在引起学术界对NLP算法相关的软件安全问题的关注和进一步研究。 |
| [^198] | [Towards Data-and Knowledge-Driven Artificial Intelligence: A Survey on Neuro-Symbolic Computing.](http://arxiv.org/abs/2210.15889) | 神经符号计算是将符号和统计认知范式整合的研究领域，在推理和解释性以及神经网络学习等方面具有潜力。本文综述了该领域的最新进展和重要贡献，并探讨了其成功应用案例和未来发展方向。 |
| [^199] | [SpacePhish: The Evasion-space of Adversarial Attacks against Phishing Website Detectors using Machine Learning.](http://arxiv.org/abs/2210.13660) | 该论文研究了对抗机器学习中钓鱼网站检测的攻击以及针对这些攻击的逃避防御空间，并提出了一种现实威胁模型。 |
| [^200] | [Network Synthetic Interventions: A Causal Framework for Panel Data Under Network Interference.](http://arxiv.org/abs/2210.11355) | 本研究提出了一种网络干扰下的面板数据因果框架，其中包括网络干扰的合成控制和合成干预方法。通过引入新颖的潜在因子模型，我们的方法能够在解决溢出和未观测混杂的情况下提供准确的反事实估计。 |
| [^201] | [MMTSA: Multimodal Temporal Segment Attention Network for Efficient Human Activity Recognition.](http://arxiv.org/abs/2210.09222) | MMTSA是一种多模式神经网络架构，用于高效的人体活动识别。它利用多模式传感器的互补信息，通过将IMU数据转换为时序结构保持的灰度图像，并采用多模式稀疏采样和段间关注模块实现了高效的多模式融合。在评估中显示，该方法在人体活动识别方面取得了显著的性能提升。 |
| [^202] | [Efficient probabilistic reconciliation of forecasts for real-valued and count time series.](http://arxiv.org/abs/2210.02286) | 本论文提出了一种基于条件方法来调和任何类型的预测分布的新方法，并引入了一种名为“自下而上重要性抽样”的高效抽样算法。这种方法在多个时间层次的实验中显示出与基本概率预测相比的显著改进。 |
| [^203] | [GP-net: Flexible Viewpoint Grasp Proposal.](http://arxiv.org/abs/2209.10404) | GP-net 可以从灵活的视角，例如移动机械臂所体验的视角，生成六自由度的抓取，在真实世界的实验中，它实现了 51.8% 的抓取成功率，相比之下，机器人抓取技术的最新方法成功率更低，需要定义工作空间。 |
| [^204] | [Quasi-Arithmetic Mixtures, Divergence Minimization, and Bregman Information.](http://arxiv.org/abs/2209.07481) | 本文提供了对准算术混合、散度最小化和Bregman信息的全面分析。通过在密度函数的单调嵌入下使用Bregman散度，我们将常见的散度函数与退火路径上的中间密度关联起来。 |
| [^205] | [A general framework for multi-step ahead adaptive conformal heteroscedastic time series forecasting.](http://arxiv.org/abs/2207.14219) | 本文介绍了一种名为AEnbMIMOCQR的新颖算法，通过自适应集成的方式，在不需要数据拆分的情况下，以分布无关的方式生成多步鲍型预测区间。该方法考虑了异方差性，并对分布转变具有鲁棒性，在实验中表现优于其他竞争方法。 |
| [^206] | [MemSAC: Memory Augmented Sample Consistency for Large Scale Unsupervised Domain Adaptation.](http://arxiv.org/abs/2207.12389) | MemSAC提出了一种记忆增强样本一致性方法，通过利用源域和目标域之间的样本级相似性实现判别转移，并且在大规模数据集上表现出明显的优势。 |
| [^207] | [Federated Learning from Small Datasets.](http://arxiv.org/abs/2110.03469) | 本文介绍了一种在小数据集上实现联合学习的新方法，通过将模型聚合与本地模型的置换相结合，可以更高效地在数据稀疏的领域进行训练。 |
| [^208] | [Adaptive Optimizers with Sparse Group Lasso for Neural Networks in CTR Prediction.](http://arxiv.org/abs/2107.14432) | 本论文提出了一种新的框架，在神经网络的CTR预测中加入了稀疏分组Lasso的正则项，并创建了一类新的自适应优化器。实验证明，这些优化器在相同稀疏水平下可以显著提升模型性能，并且能够实现极高的稀疏性。 |
| [^209] | [Conditional Sig-Wasserstein GANs for Time Series Generation.](http://arxiv.org/abs/2006.05421) | 本论文提出了一种有条件的Sig-Wasserstein GANs框架，通过将WGANs与路径特征提取方法相结合，用于解决时间序列生成中的挑战。路径的签名作为一种通用描述数据流的统计特征，可以刻画时序模型的分布特征。 |
| [^210] | [Dynamic Subgoal-based Exploration via Bayesian Optimization.](http://arxiv.org/abs/1910.09143) | 本论文提出了一种基于贝叶斯优化的成本感知探索方法，能够针对稀疏奖励导航环境中的复杂任务，高效地搜索动态子目标的探索策略。 |
| [^211] | [L2P: Learning to Place for Estimating Heavy-Tailed Distributed Outcomes.](http://arxiv.org/abs/1908.04628) | L2P是一种利用实例之间成对关系进行学习的方法，用于解决具有重尾分布特征的预测任务。实验证明，L2P在准确度和能力方面优于竞争方法。 |

# 详细

[^1]: LightZero：一个用于普适顺序决策场景的蒙特卡洛树搜索统一基准

    LightZero: A Unified Benchmark for Monte Carlo Tree Search in General Sequential Decision Scenarios. (arXiv:2310.08348v1 [cs.LG])

    [http://arxiv.org/abs/2310.08348](http://arxiv.org/abs/2310.08348)

    本文介绍了LightZero，一个用于在普适顺序决策场景中部署MCTS/MuZero的统一基准。通过解决普通MCTS风格决策求解器面临的关键挑战并进行模块化设计，结合更合适的探索和优化策略，我们可以构建强大的LightZero代理来应对各种任务。

    

    基于学习模型的树搜索规划能力的代理在经典决策问题，如围棋和Atari上取得了显著的成功。然而，在涉及复杂行动空间和显著仿真成本或固有随机性的真实世界应用中，将蒙特卡洛树搜索（MCTS）算法扩展到具有挑战性甚至不可行。在这项工作中，我们介绍了LightZero，第一个用于在普遍顺序决策场景中部署MCTS/MuZero的统一基准。具体而言，我们总结了在设计普通MCTS风格决策求解器时所面临的最关键挑战，然后将树搜索RL方法的紧密耦合算法和系统设计分解为不同的子模块。通过结合更合适的探索和优化策略，我们可以明显增强这些子模块，并构建强大的LightZero代理以应对各种任务。

    Building agents based on tree-search planning capabilities with learned models has achieved remarkable success in classic decision-making problems, such as Go and Atari. However, it has been deemed challenging or even infeasible to extend Monte Carlo Tree Search (MCTS) based algorithms to diverse real-world applications, especially when these environments involve complex action spaces and significant simulation costs, or inherent stochasticity. In this work, we introduce LightZero, the first unified benchmark for deploying MCTS/MuZero in general sequential decision scenarios. Specificially, we summarize the most critical challenges in designing a general MCTS-style decision-making solver, then decompose the tightly-coupled algorithm and system design of tree-search RL methods into distinct sub-modules. By incorporating more appropriate exploration and optimization strategies, we can significantly enhance these sub-modules and construct powerful LightZero agents to tackle tasks across a
    
[^2]: 一个用于分布式拓扑分析管线的通用软件框架

    A Generic Software Framework for Distributed Topological Analysis Pipelines. (arXiv:2310.08339v1 [cs.DC])

    [http://arxiv.org/abs/2310.08339](http://arxiv.org/abs/2310.08339)

    本文介绍了一个通用的软件框架，用于支持分布式内存模型下的拓扑分析管线。该框架能够实现不同的拓扑算法之间的协作，并提供了性能分析和示例。

    

    本文介绍了一种用于支持分布式内存模型下拓扑分析管线的通用软件框架。与最近的一些论文针对分布式内存环境引入了基于拓扑的方法不同，这些论文报告的实验结果是通过定制的单一算法实现得到的。相比之下，我们在本文中描述了一个通用、泛化的拓扑分析管线框架，即一系列相互作用的拓扑算法，可能在不同的进程数量上运行。具体地，我们在Topology ToolKit (TTK)中使用MPI模型来实例化我们的框架。在开发这个框架的过程中，我们遇到了一些算法和软件工程方面的挑战，本文对此进行了记录。我们提供了TTK支持的分布式内存拓扑算法分类，根据它们的通信需求，并提供了MPI+线程并行化的示例。详细的性能分析显示，p

    This system paper presents a software framework for the support of topological analysis pipelines in a distributed-memory model. While several recent papers introduced topology-based approaches for distributed-memory environments, these were reporting experiments obtained with tailored, mono-algorithm implementations. In contrast, we describe in this paper a general-purpose, generic framework for topological analysis pipelines, i.e. a sequence of topological algorithms interacting together, possibly on distinct numbers of processes. Specifically, we instantiated our framework with the MPI model, within the Topology ToolKit (TTK). While developing this framework, we faced several algorithmic and software engineering challenges, which we document in this paper. We provide a taxonomy for the distributed-memory topological algorithms supported by TTK, depending on their communication needs and provide examples of hybrid MPI+thread parallelizations. Detailed performance analyses show that p
    
[^3]: 多臂赌博策略对深度循环强化学习的影响

    Impact of multi-armed bandit strategies on deep recurrent reinforcement learning. (arXiv:2310.08331v1 [stat.ML])

    [http://arxiv.org/abs/2310.08331](http://arxiv.org/abs/2310.08331)

    本研究在自动驾驶场景中使用部分可观测系统，通过部署和测试多种技术来平衡探索和利用的权衡，以预测方向盘操作。

    

    对环境的不完全了解导致智能体在不确定性下做出决策。强化学习中一个重要的困境是，在做出决策时，智能体需要在利用当前环境知识最大化累积奖励和探索行动以提高环境知识的之间进行权衡（探索-利用的平衡）。同时，另一个相关问题是状态的完全可观测性，不是所有应用都能假定。例如，当只将2D图像作为输入用于在3D模拟环境中找到最佳行动时，就存在这个问题。在本研究中，我们通过部署和测试多种技术来解决部分可观测系统中探索和利用的平衡问题，以预测自动驾驶场景中的方向盘操作。

    Incomplete knowledge of the environment leads an agent to make decisions under uncertainty. One of the major dilemmas in Reinforcement Learning (RL) where an autonomous agent has to balance two contrasting needs in making its decisions is: exploiting the current knowledge of the environment to maximize the cumulative reward as well as exploring actions that allow improving the knowledge of the environment, hopefully leading to higher reward values (exploration-exploitation trade-off). Concurrently, another relevant issue regards the full observability of the states, which may not be assumed in all applications. Such as when only 2D images are considered as input in a RL approach used for finding the optimal action within a 3D simulation environment. In this work, we address these issues by deploying and testing several techniques to balance exploration and exploitation trade-off on partially observable systems for predicting steering wheels in autonomous driving scenario. More precisel
    
[^4]: 使用后门技术保护我们的隐私

    Defending Our Privacy With Backdoors. (arXiv:2310.08320v1 [cs.LG])

    [http://arxiv.org/abs/2310.08320](http://arxiv.org/abs/2310.08320)

    本研究提出了一种基于后门攻击的防御方法，通过对模型进行策略性插入后门，对齐敏感短语与中性术语的嵌入，以删除训练数据中的私人信息。实证结果显示该方法的有效性。

    

    在使用未经筛选、常常包含敏感信息的网页数据训练大型人工智能模型的情况下，隐私问题成为了一个重要的关注点。其中一个问题是，攻击者可以利用隐私攻击的方法提取出训练数据的信息。然而，如何在不降低模型性能的情况下去除特定信息是一个不容易解决且具有挑战性的问题。我们提出了一个基于后门攻击的简单而有效的防御方法，用于从模型中删除私人信息，如个人姓名，特别是针对文本编码器的。具体而言，通过策略性地插入后门，我们将敏感短语的嵌入与中性术语的嵌入对齐，例如用"a person"代替人名。我们的实证结果通过对零样本分类器使用专门的隐私攻击测试表明了我们基于后门的防御方法的效果。我们的方法提供了一个新的"双重用途"的视角。

    The proliferation of large AI models trained on uncurated, often sensitive web-scraped data has raised significant privacy concerns. One of the concerns is that adversaries can extract information about the training data using privacy attacks. Unfortunately, the task of removing specific information from the models without sacrificing performance is not straightforward and has proven to be challenging. We propose a rather easy yet effective defense based on backdoor attacks to remove private information such as names of individuals from models, and focus in this work on text encoders. Specifically, through strategic insertion of backdoors, we align the embeddings of sensitive phrases with those of neutral terms-"a person" instead of the person's name. Our empirical results demonstrate the effectiveness of our backdoor-based defense on CLIP by assessing its performance using a specialized privacy attack for zero-shot classifiers. Our approach provides not only a new "dual-use" perspecti
    
[^5]: GePSAn: 烹饪视频中的生成式步骤预测

    GePSAn: Generative Procedure Step Anticipation in Cooking Videos. (arXiv:2310.08312v1 [cs.CV])

    [http://arxiv.org/abs/2310.08312](http://arxiv.org/abs/2310.08312)

    这篇论文研究了在烹饪视频中未来步骤预测的问题，通过设计一个生成模型，能够生成多个合理且多样的候选步骤，解决了之前工作中忽视的考虑多个可能实现的挑战。

    

    我们研究了在过程视频中未来步骤预测的问题。给定一个正在进行的过程活动的视频，我们预测一个合理的下一个步骤，用丰富的自然语言描述。虽然大多数先前的工作关注的是过程视频数据集中的数据稀缺问题，但未来预测的另一个核心挑战是如何考虑到自然环境中多个可能的未来实现。这个问题在以前的工作中被大部分忽视了。为了解决这个挑战，我们将未来步骤预测制定为建模下一个步骤的所有可能候选的分布。具体地，我们设计了一个生成模型，它以一系列视频片段作为输入，并生成多个下一个步骤的合理和多样化的候选（用自然语言表示）。遵循先前的工作，我们通过在大型基于文本的过程活动语料库上预训练模型，然后将模型转移到视频领域来避开视频注释的稀缺性。

    We study the problem of future step anticipation in procedural videos. Given a video of an ongoing procedural activity, we predict a plausible next procedure step described in rich natural language. While most previous work focus on the problem of data scarcity in procedural video datasets, another core challenge of future anticipation is how to account for multiple plausible future realizations in natural settings. This problem has been largely overlooked in previous work. To address this challenge, we frame future step prediction as modelling the distribution of all possible candidates for the next step. Specifically, we design a generative model that takes a series of video clips as input, and generates multiple plausible and diverse candidates (in natural language) for the next step. Following previous work, we side-step the video annotation scarcity by pretraining our model on a large text-based corpus of procedural activities, and then transfer the model to the video domain. Our 
    
[^6]: CHIP：对比层次化图像预训练

    CHIP: Contrastive Hierarchical Image Pretraining. (arXiv:2310.08304v1 [cs.CV])

    [http://arxiv.org/abs/2310.08304](http://arxiv.org/abs/2310.08304)

    我们提出了一种基于对比损失的层次化分类模型，用于少样本目标分类任务。该模型可以将未见过的物体分类到相对通用的类别中，通过从图像嵌入中提取的特征进行分类。

    

    少样本目标分类是在有限数量的样本作为监督时对图像中的物体进行分类的任务。我们提出了一种一次/少次分类模型，可以将任何未见过的类别的物体分类到相对通用的类别中，基于层次化的分类。我们的模型使用了基于三级层次对比损失的ResNet152分类器，根据从图像嵌入中提取的特征对物体进行分类，而这些特征在训练阶段未被使用。我们的实验使用了ImageNet（ILSVRC-12）数据集的一个子集，该子集仅包含动物类别，用于训练我们的模型，并创建了我们自己的未见类别数据集，用于评估我们训练的模型。我们的模型在将未知物体分类为一个通用类别方面提供了令人满意的结果，这在后文中有更详细的讨论。

    Few-shot object classification is the task of classifying objects in an image with limited number of examples as supervision. We propose a one-shot/few-shot classification model that can classify an object of any unseen class into a relatively general category in an hierarchically based classification. Our model uses a three-level hierarchical contrastive loss based ResNet152 classifier for classifying an object based on its features extracted from Image embedding, not used during the training phase. For our experimentation, we have used a subset of the ImageNet (ILSVRC-12) dataset that contains only the animal classes for training our model and created our own dataset of unseen classes for evaluating our trained model. Our model provides satisfactory results in classifying the unknown objects into a generic category which has been later discussed in greater detail.
    
[^7]: 对贝叶斯神经网络后验的对称感知探索

    A Symmetry-Aware Exploration of Bayesian Neural Network Posteriors. (arXiv:2310.08287v1 [stat.ML])

    [http://arxiv.org/abs/2310.08287](http://arxiv.org/abs/2310.08287)

    本文对贝叶斯神经网络后验进行了大规模探索，研究了逼近后验的最佳方法和后验质量与不确定性量化的关系，同时发现了权重空间对称性对后验的影响。

    

    现代深度神经网络(DNNs)的权重分布对不确定性量化和鲁棒性非常重要，由于其极高的维度，其本质非常复杂。本文提出了对深度贝叶斯神经网络(BNNs)后验分布的首次大规模探索，将其研究扩展到现实世界的视觉任务和架构。具体而言，我们研究了逼近后验的最佳方法，分析了后验质量和不确定性量化之间的关系，深入研究了后验中模式的影响，并探索了可视化后验的方法。此外，我们发现权重空间的对称性是理解后验的关键方面。为此，我们对置换和缩放对称性的影响进行了深入评估，这些对称性往往会使贝叶斯后验变得模糊。尽管第一种变换已知会复制模式，但我们还是对其进行了探索。

    The distribution of the weights of modern deep neural networks (DNNs) crucial for uncertainty quantification and robustness - is an eminently complex object due to its extremely high dimensionality. This paper proposes one of the first large-scale explorations of the posterior distribution of deep Bayesian Neural Networks (BNNs), expanding its study to real-world vision tasks and architectures. Specifically, we investigate the optimal approach for approximating the posterior, analyze the connection between posterior quality and uncertainty quantification, delve into the impact of modes on the posterior, and explore methods for visualizing the posterior. Moreover, we uncover weight-space symmetries as a critical aspect for understanding the posterior. To this extent, we develop an in-depth assessment of the impact of both permutation and scaling symmetries that tend to obfuscate the Bayesian posterior. While the first type of transformation is known for duplicating modes, we explore t
    
[^8]: 数据驱动建模自相似动力学

    Data driven modeling of self-similar dynamics. (arXiv:2310.08282v1 [cs.LG])

    [http://arxiv.org/abs/2310.08282](http://arxiv.org/abs/2310.08282)

    本文介绍了一个多尺度神经网络框架，利用自相似性作为先验知识，对自相似动力系统进行建模。对于确定性动力学，框架可以判断动力学是否自相似；对于不确定性动力学，它可以确定哪个参数集更接近自相似性。方法可以提取与尺度无关的核进行任意尺度的建模，并识别自相似系统中的幂律指数。初步测试表明，方法对Ising模型的临界指数具有理论一致性。

    

    多尺度建模复杂系统对于理解其复杂性至关重要。数据驱动多尺度建模已成为解决复杂系统挑战的一种有希望的方法。另一方面，自相似性在复杂系统中普遍存在，这表明大规模复杂系统可以以较低成本进行建模。本文引入了一个多尺度神经网络框架，将自相似性作为先验知识，并实现了对自相似动力系统的建模。对于确定性动力学，我们的框架可以判断动力学是否自相似。对于不确定性动力学，它可以比较和确定哪个参数集更接近自相似性。该框架允许我们从动力学中提取与尺度无关的核进行任意尺度的建模。此外，我们的方法可以识别自相似系统中的幂律指数。对Ising模型的初步测试产生了与理论一致的临界指数。

    Multiscale modeling of complex systems is crucial for understanding their intricacies. Data-driven multiscale modeling has emerged as a promising approach to tackle challenges associated with complex systems. On the other hand, self-similarity is prevalent in complex systems, hinting that large-scale complex systems can be modeled at a reduced cost. In this paper, we introduce a multiscale neural network framework that incorporates self-similarity as prior knowledge, facilitating the modeling of self-similar dynamical systems. For deterministic dynamics, our framework can discern whether the dynamics are self-similar. For uncertain dynamics, it can compare and determine which parameter set is closer to self-similarity. The framework allows us to extract scale-invariant kernels from the dynamics for modeling at any scale. Moreover, our method can identify the power law exponents in self-similar systems. Preliminary tests on the Ising model yielded critical exponents consistent with theo
    
[^9]: Lag-Llama: 用于时间序列预测的基础模型

    Lag-Llama: Towards Foundation Models for Time Series Forecasting. (arXiv:2310.08278v1 [cs.LG])

    [http://arxiv.org/abs/2310.08278](http://arxiv.org/abs/2310.08278)

    Lag-Llama是一个基于大量时间序列数据训练的通用预测模型，在未见过的数据集上展现出强大的零样本预测能力，并使用光滑断裂幂律模型来拟合和预测扩展行为。

    

    为了构建时间序列预测的基础模型并研究其扩展行为，我们在这里介绍了我们正在进行中的 Lag-Llama 工作，这是一个在大量时间序列数据上训练的通用单变量概率时间序列预测模型。该模型在未见过的“分布外”时间序列数据集上展现出良好的零样本预测能力，优于有监督基线方法。我们使用光滑断裂幂律来拟合和预测模型的扩展行为。开源代码可在 https://github.com/kashif/pytorch-transformer-ts 上获得。

    Aiming to build foundation models for time-series forecasting and study their scaling behavior, we present here our work-in-progress on Lag-Llama, a general-purpose univariate probabilistic time-series forecasting model trained on a large collection of time-series data. The model shows good zero-shot prediction capabilities on unseen "out-of-distribution" time-series datasets, outperforming supervised baselines. We use smoothly broken power-laws to fit and predict model scaling behavior. The open source code is made available at https://github.com/kashif/pytorch-transformer-ts.
    
[^10]: 隐形威胁：OCR系统中的后门攻击

    Invisible Threats: Backdoor Attack in OCR Systems. (arXiv:2310.08259v1 [cs.CR])

    [http://arxiv.org/abs/2310.08259](http://arxiv.org/abs/2310.08259)

    该论文介绍了一种针对OCR系统的后门攻击方法，通过插入非可读字符的恶意输入图像，在不影响模型性能的情况下干扰训练阶段。实验证明，该攻击方法对OCR的现有技术造成了威胁。

    

    光学字符识别（OCR）是一种广泛用于从扫描文档中提取文本的工具。目前，通过利用深度神经网络来实现最先进的性能。然而，这种性能的代价是系统容易受到威胁。例如，在后门攻击中，攻击者通过在受害者模型中插入后门，在特定模式下激活，从而干扰训练阶段。本文提出了一种针对OCR的后门攻击，通过恶意输入图像中注入非可读字符。这种简单但有效的攻击揭示了OCR的先进技术的弱点，使提取出的文本对人眼正确可读，但对使用OCR作为预处理步骤的自然语言处理应用来说无用。实验结果显示，被攻击的模型在大约90%的污染实例中成功输出非可读字符，而不影响其性能。

    Optical Character Recognition (OCR) is a widely used tool to extract text from scanned documents. Today, the state-of-the-art is achieved by exploiting deep neural networks. However, the cost of this performance is paid at the price of system vulnerability. For instance, in backdoor attacks, attackers compromise the training phase by inserting a backdoor in the victim's model that will be activated at testing time by specific patterns while leaving the overall model performance intact. This work proposes a backdoor attack for OCR resulting in the injection of non-readable characters from malicious input images. This simple but effective attack exposes the state-of-the-art OCR weakness, making the extracted text correct to human eyes but simultaneously unusable for the NLP application that uses OCR as a preprocessing step. Experimental results show that the attacked models successfully output non-readable characters for around 90% of the poisoned instances without harming their performa
    
[^11]: 大型语言模型中共现对事实知识的影响

    Impact of Co-occurrence on Factual Knowledge of Large Language Models. (arXiv:2310.08256v1 [cs.CL])

    [http://arxiv.org/abs/2310.08256](http://arxiv.org/abs/2310.08256)

    大型语言模型在回答问题时常常出现事实错误，这主要是因为过度依赖预训练语料库的共现统计。研究结果表明，大型语言模型容易受到共现偏见的影响，导致难以回忆起预训练数据集中很少共现的事实。建议使用去偏数据集进行微调来减轻偏见，但这对于微调期间未见过的稀有事实的回忆效果并不明显。

    

    尽管大型语言模型在各种应用中取得了成功，但它们经常在事实上做出错误的回答。本文假设过度依赖于预训练语料库的简单共现统计是导致事实错误的主要因素之一。我们的结果表明，大型语言模型容易受到共现偏见的影响，即更倾向于选择频繁共现的词而不是正确答案。因此，尽管在微调期间已经见过这些事实的主题和对象在预训练数据集中很少共现，大型语言模型仍然难以回忆起这些事实。我们展示了即使扩大模型规模或进行微调，共现偏见仍然存在。因此，我们建议在去偏数据集上进行微调，通过过滤掉主题-对象共现计数高的偏见样本来减轻偏见。尽管去偏微调允许大型语言模型记忆训练集中的稀有事实，但在微调期间未见过的稀有事实的回忆效果并不明显。

    Large language models (LLMs) often make factually incorrect responses despite their success in various applications. In this paper, we hypothesize that relying heavily on simple co-occurrence statistics of the pre-training corpora is one of the main factors that cause factual errors. Our results reveal that LLMs are vulnerable to the co-occurrence bias, defined as preferring frequently co-occurred words over the correct answer. Consequently, LLMs struggle to recall facts whose subject and object rarely co-occur in the pre-training dataset although they are seen during finetuning. We show that co-occurrence bias remains despite scaling up model sizes or finetuning. Therefore, we suggest finetuning on a debiased dataset to mitigate the bias by filtering out biased samples whose subject-object co-occurrence count is high. Although debiased finetuning allows LLMs to memorize rare facts in the training set, it is not effective in recalling rare facts unseen during finetuning. Further resear
    
[^12]: MetaBox：一种用于元黑箱优化与强化学习的基准平台

    MetaBox: A Benchmark Platform for Meta-Black-Box Optimization with Reinforcement Learning. (arXiv:2310.08252v1 [cs.LG])

    [http://arxiv.org/abs/2310.08252](http://arxiv.org/abs/2310.08252)

    MetaBox是一种用于开发和评估元黑箱优化与强化学习方法的基准平台，提供灵活的算法模板、广泛的问题实例和基线方法，并引入了三个标准化的性能指标，以促进方法的严格评估。

    

    最近，元黑箱优化与强化学习（MetaBBO-RL）展示了在元级别上利用强化学习来减少对低级黑箱优化器的手动微调的能力。然而，这个领域由于缺乏统一的基准而受到阻碍。为了填补这个空白，我们介绍了MetaBox，这是一个专门为开发和评估MetaBBO-RL方法而设计的第一个基准平台。MetaBox提供了一个灵活的算法模板，让用户可以轻松地在平台内实现自己的独特设计。此外，它提供了超过300个问题实例，从合成到真实场景的广泛范围，并且包含了19种基线方法的详尽库，包括传统黑箱优化器和最近的MetaBBO-RL方法。此外，MetaBox引入了三个标准化的性能指标，使方法的评估更加全面。

    Recently, Meta-Black-Box Optimization with Reinforcement Learning (MetaBBO-RL) has showcased the power of leveraging RL at the meta-level to mitigate manual fine-tuning of low-level black-box optimizers. However, this field is hindered by the lack of a unified benchmark. To fill this gap, we introduce MetaBox, the first benchmark platform expressly tailored for developing and evaluating MetaBBO-RL methods. MetaBox offers a flexible algorithmic template that allows users to effortlessly implement their unique designs within the platform. Moreover, it provides a broad spectrum of over 300 problem instances, collected from synthetic to realistic scenarios, and an extensive library of 19 baseline methods, including both traditional black-box optimizers and recent MetaBBO-RL methods. Besides, MetaBox introduces three standardized performance metrics, enabling a more thorough assessment of the methods. In a bid to illustrate the utility of MetaBox for facilitating rigorous evaluation and in-
    
[^13]: 在协变量漂移下基于核方法的统一分析

    Towards a Unified Analysis of Kernel-based Methods Under Covariate Shift. (arXiv:2310.08237v1 [stat.ML])

    [http://arxiv.org/abs/2310.08237](http://arxiv.org/abs/2310.08237)

    该论文提出了一个在协变量漂移下对一般非参数方法进行统一分析的方法，通过建立收敛速度为一般损失函数提供了统一的理论分析。

    

    在实际应用中，协变量漂移是普遍存在的，即源数据和目标数据的输入分布存在显著差异。尽管在各种学习问题中具有实际重要性，但现有的大多数方法只关注于一些特定的学习任务，并没有在理论上和数值上得到很好的验证。为了解决这个问题，我们提出了一个在协变量漂移下对一般非参数方法进行统一分析的方法。我们的理论结果适用于属于一个丰富的损失函数家族的一般损失，其中包括许多常用的方法，如均值回归、分位数回归、基于似然的分类和基于边缘的分类。本文重点研究了两类协变量漂移问题，并为一般损失函数建立了尖锐的收敛速度以提供一个统一的理论分析，该结果与文献中的最优结果一致。

    Covariate shift occurs prevalently in practice, where the input distributions of the source and target data are substantially different. Despite its practical importance in various learning problems, most of the existing methods only focus on some specific learning tasks and are not well validated theoretically and numerically. To tackle this problem, we propose a unified analysis of general nonparametric methods in a reproducing kernel Hilbert space (RKHS) under covariate shift. Our theoretical results are established for a general loss belonging to a rich loss function family, which includes many commonly used methods as special cases, such as mean regression, quantile regression, likelihood-based classification, and margin-based classification. Two types of covariate shift problems are the focus of this paper and the sharp convergence rates are established for a general loss function to provide a unified theoretical analysis, which concurs with the optimal results in literature wher
    
[^14]: GROOT: 通过观看游戏视频学习遵循指令

    GROOT: Learning to Follow Instructions by Watching Gameplay Videos. (arXiv:2310.08235v1 [cs.AI])

    [http://arxiv.org/abs/2310.08235](http://arxiv.org/abs/2310.08235)

    GROOT是一个通过观看游戏视频学习遵循指令的控制器，它通过产生一个结构化的目标空间来消除昂贵的文本-游戏注释的需求，并在Minecraft SkillForge基准测试中展现出与人类玩家相当的表现水平。

    

    我们研究在开放世界环境中构建一个可以遵循开放式指令的控制器的问题。我们提出了通过观看视频作为指令的方式，这种方式提供了表达能力强的目标规范，同时消除了昂贵的文本-游戏注释的需求。我们提出了一个新的学习框架，可以从游戏视频中学习这种指令遵循控制器，并产生一个能产生结构化目标空间的视频指令编码器。我们在基于因果变压器的简单而有效的编码器-解码器架构中实现了我们的代理程序GROOT。我们在一个提出的Minecraft SkillForge基准测试中对GROOT进行了与开放世界对手和人类玩家的评估。Elo评级清楚地显示GROOT正在缩小人机差距，并且对最好的通用代理程序基线具有70%的胜率。对所产生的目标空间的定性分析进一步展示了一些有趣的新颖性质，包括目标的组成和...

    We study the problem of building a controller that can follow open-ended instructions in open-world environments. We propose to follow reference videos as instructions, which offer expressive goal specifications while eliminating the need for expensive text-gameplay annotations. A new learning framework is derived to allow learning such instruction-following controllers from gameplay videos while producing a video instruction encoder that induces a structured goal space. We implement our agent GROOT in a simple yet effective encoder-decoder architecture based on causal transformers. We evaluate GROOT against open-world counterparts and human players on a proposed Minecraft SkillForge benchmark. The Elo ratings clearly show that GROOT is closing the human-machine gap as well as exhibiting a 70% winning rate over the best generalist agent baseline. Qualitative analysis of the induced goal space further demonstrates some interesting emergent properties, including the goal composition and 
    
[^15]: 深度神经网络分类器中潜在二进制编码的出现

    Emergence of Latent Binary Encoding in Deep Neural Network Classifiers. (arXiv:2310.08224v1 [cs.LG])

    [http://arxiv.org/abs/2310.08224](http://arxiv.org/abs/2310.08224)

    这篇论文观察到在深度神经网络分类器的潜在空间中出现了二进制编码，这种编码通过引入线性倒数第二层和指数增长的损失函数产生，并且加速了收敛和提高了分类准确率。

    

    我们观察到深度神经网络分类器的潜在空间中出现了二进制编码。通过引入一个线性倒数第二层，并在训练过程中配备一个损失函数，该函数随着潜在空间中坐标$\vec{x}$的平方指数增长，诱导出了二进制编码。我们描述的现象是已知的一种被称为"神经崩溃"的特殊情况，它在训练的最后阶段出现，并导致潜在类均值崩溃为简单等角紧框架（ETF）的顶点。我们展示了二进制编码加速了收敛到简单等角紧框架的过程，并提高了分类准确率。

    We observe the emergence of binary encoding within the latent space of deep-neural-network classifiers. Such binary encoding is induced by introducing a linear penultimate layer, which is equipped during training with a loss function that grows as $\exp(\vec{x}^2)$, where $\vec{x}$ are the coordinates in the latent space. The phenomenon we describe represents a specific instance of a well-documented occurrence known as \textit{neural collapse}, which arises in the terminal phase of training and entails the collapse of latent class means to the vertices of a simplex equiangular tight frame (ETF). We show that binary encoding accelerates convergence toward the simplex ETF and enhances classification accuracy.
    
[^16]: SimCKP: 简单对比学习关键词短语表示

    SimCKP: Simple Contrastive Learning of Keyphrase Representations. (arXiv:2310.08221v1 [cs.CL])

    [http://arxiv.org/abs/2310.08221](http://arxiv.org/abs/2310.08221)

    SimCKP是一个简单的对比学习框架，通过学习上下文感知的短语级表示来提取关键词短语，并通过重新排序来调整生成的短语的分数。

    

    关键词生成（KG）旨在生成一组总结性词语或短语，给定一个源文档，而关键词提取（KE）旨在从文本中识别它们。由于在KE中搜索空间较小，通常将其与KG相结合，预测可能存在或不存在于相应文档中的关键词短语。然而，目前的统一方法采用序列标注和基于最大化的生成，主要在令牌级别上操作，不能很好地观察和评分关键词短语作为一个整体。在这项工作中，我们提出了SimCKP，一个简单的对比学习框架，包括两个阶段：1）提取器-生成器，通过对比学习上下文感知的短语级表示来提取关键词短语，同时生成不出现在文档中的关键词短语；2）重新排序器，通过将它们的表示与相应文档对齐，同样调整每个生成的短语的分数。在多个基准上进行实验证明

    Keyphrase generation (KG) aims to generate a set of summarizing words or phrases given a source document, while keyphrase extraction (KE) aims to identify them from the text. Because the search space is much smaller in KE, it is often combined with KG to predict keyphrases that may or may not exist in the corresponding document. However, current unified approaches adopt sequence labeling and maximization-based generation that primarily operate at a token level, falling short in observing and scoring keyphrases as a whole. In this work, we propose SimCKP, a simple contrastive learning framework that consists of two stages: 1) An extractor-generator that extracts keyphrases by learning context-aware phrase-level representations in a contrastive manner while also generating keyphrases that do not appear in the document; 2) A reranker that adapts scores for each generated phrase by likewise aligning their representations with the corresponding document. Experimental results on multiple ben
    
[^17]: TriRE: 一种用于持续知识保持和推进的多机制学习范式

    TriRE: A Multi-Mechanism Learning Paradigm for Continual Knowledge Retention and Promotion. (arXiv:2310.08217v1 [cs.AI])

    [http://arxiv.org/abs/2310.08217](http://arxiv.org/abs/2310.08217)

    TriRE是一个新的持续学习范式，受到大脑如何利用多种机制进行学习的启发。它通过保持每个任务中最显著的神经元，修订和巩固这些神经元之间的联系，解决了深度神经网络中的灾难性遗忘问题。

    

    持续学习（CL）对于深度神经网络来说一直是一个持久的挑战，原因是先前学习的任务会发生灾难性遗忘（CF）。已经提出了一些技术来减轻CF，例如权重正则化，经验重演和参数隔离。尽管相对成功，但这些研究方向主要是相互独立的，存在一些缺点，并且错过了竞争策略的优势。相反，大脑通过同时利用多种神经生理过程（包括神经发生，主动遗忘，神经调节，元可塑性，经验重演和上下文相关的门控），不常导致CF，而是持续学习，适应和跨任务传递知识。受大脑如何同时利用多个机制的启发，我们提出了TriRE，一种新的CL范式，旨在保留每个任务中最显著的神经元，修订和巩固这些神经元之间的联系。

    Continual learning (CL) has remained a persistent challenge for deep neural networks due to catastrophic forgetting (CF) of previously learned tasks. Several techniques such as weight regularization, experience rehearsal, and parameter isolation have been proposed to alleviate CF. Despite their relative success, these research directions have predominantly remained orthogonal and suffer from several shortcomings, while missing out on the advantages of competing strategies. On the contrary, the brain continually learns, accommodates, and transfers knowledge across tasks by simultaneously leveraging several neurophysiological processes, including neurogenesis, active forgetting, neuromodulation, metaplasticity, experience rehearsal, and context-dependent gating, rarely resulting in CF. Inspired by how the brain exploits multiple mechanisms concurrently, we propose TriRE, a novel CL paradigm that encompasses retaining the most prominent neurons for each task, revising and solidifying the 
    
[^18]: 可信任的机器学习

    Trustworthy Machine Learning. (arXiv:2310.08215v1 [cs.LG])

    [http://arxiv.org/abs/2310.08215](http://arxiv.org/abs/2310.08215)

    该论文讨论了当前机器学习技术面临的可信度问题，并提出了分布之外的泛化、可解释性、不确定性量化和可信度评估四个关键主题的解决方案。

    

    随着机器学习技术应用于实际产品和解决方案，出现了新的挑战。模型在分布的微小变化下意外地无法泛化，对于他们从未见过的新数据表现出自信，或者无法有效地向最终用户解释其决策的原理。总而言之，我们面临着当前机器学习技术的可信度问题。这本《可信任的机器学习》教材涵盖了可信任的机器学习的四个关键主题的理论和技术背景：分布之外的泛化、可解释性、不确定性量化和可信度评估。我们讨论了重要的经典和现代研究论文，并揭示并连接了它们的基本直觉。这本书是从2022/23冬季学期开始在图宾根大学开设的同名课程发展而来。它旨在成为一本独立的产品，附有代码片段。

    As machine learning technology gets applied to actual products and solutions, new challenges have emerged. Models unexpectedly fail to generalize to small changes in the distribution, tend to be confident on novel data they have never seen, or cannot communicate the rationale behind their decisions effectively with the end users. Collectively, we face a trustworthiness issue with the current machine learning technology. This textbook on Trustworthy Machine Learning (TML) covers a theoretical and technical background of four key topics in TML: Out-of-Distribution Generalization, Explainability, Uncertainty Quantification, and Evaluation of Trustworthiness. We discuss important classical and contemporary research papers of the aforementioned fields and uncover and connect their underlying intuitions. The book evolved from the homonymous course at the University of T\"ubingen, first offered in the Winter Semester of 2022/23. It is meant to be a stand-alone product accompanied by code snip
    
[^19]: 在黎曼流形上进行回归的一致推断

    Conformal inference for regression on Riemannian Manifolds. (arXiv:2310.08209v1 [stat.ML])

    [http://arxiv.org/abs/2310.08209](http://arxiv.org/abs/2310.08209)

    本文研究了在黎曼流形上进行回归场景的预测集，并证明了这些区域的经验版本在大样本下的收敛性。

    

    在流形上进行回归，以及更广泛地说，对流形上的统计学有了重要的关注，因为这种类型的数据有大量的应用。圆形数据是一个经典示例，但协方差矩阵空间上的数据、主成分分析得到的Grassmann流形上的数据等也是如此。在本文中，我们研究了当响应变量$Y$位于流形上，而协变量$X$位于欧几里德空间时，回归场景的预测集。这扩展了[Lei and Wasserman, 2014]中在这一新领域中概述的概念。与一致推断中的传统原则一致，这些预测集是无分布的，表明对$(X, Y)$的联合分布没有施加特定的假设，而且它们保持非参数性质。我们证明了这些区域的经验版本在几乎必然收敛于无穷大时的收敛性。

    Regression on manifolds, and, more broadly, statistics on manifolds, has garnered significant importance in recent years due to the vast number of applications for this type of data. Circular data is a classic example, but so is data in the space of covariance matrices, data on the Grassmannian manifold obtained as a result of principal component analysis, among many others. In this work we investigate prediction sets for regression scenarios when the response variable, denoted by $Y$, resides in a manifold, and the covariable, denoted by X, lies in Euclidean space. This extends the concepts delineated in [Lei and Wasserman, 2014] to this novel context. Aligning with traditional principles in conformal inference, these prediction sets are distribution-free, indicating that no specific assumptions are imposed on the joint distribution of $(X, Y)$, and they maintain a non-parametric character. We prove the asymptotic almost sure convergence of the empirical version of these regions on th
    
[^20]: 终身音视频掩码自编码器与抗遗忘的本地化对齐

    Lifelong Audio-video Masked Autoencoder with Forget-robust Localized Alignments. (arXiv:2310.08204v1 [cs.CV])

    [http://arxiv.org/abs/2310.08204](http://arxiv.org/abs/2310.08204)

    这项研究提出了一种终身音视频掩码自编码器，通过引入本地化对齐和抗遗忘的多模态块选择，在不断变化的音视频分布中学习准确的多模态关系。

    

    我们提出了一种终身音视频掩码自编码器，它不断地从包含音视频对的视频流中学习多模态表示，同时其分布随着时间不断变化。具体而言，我们提出了两个新颖的想法来解决这个问题：（1）本地化对齐：引入一个小型可训练的多模态编码器，预测彼此之间良好对齐的音频和视频令牌。这使得模型只学习具有准确多模态关系的高度相关的音频视觉块。（2）抗遗忘的多模态块选择：比较当前和过去数据对之间每个音频视频块的相对重要性，以减轻先前学习的音视频表示的意外漂移。因此，我们提出的方法FLAVA（抗遗忘的本地化音视频对齐）在一系列预训练任务的训练过程中捕捉到音频和视频模态之间的复杂关系。

    We present a lifelong audio-video masked autoencoder that continually learns the multimodal representations from a video stream containing audio-video pairs, while its distribution continually shifts over time. Specifically, we propose two novel ideas to tackle the problem: (1) Localized Alignment: We introduce a small trainable multimodal encoder that predicts the audio and video tokens that are well-aligned with each other. This allows the model to learn only the highly correlated audiovisual patches with accurate multimodal relationships. (2) Forget-robust multimodal patch selection: We compare the relative importance of each audio-video patch between the current and past data pair to mitigate unintended drift of the previously learned audio-video representations. Our proposed method, FLAVA (Forget-robust Localized Audio-Video Alignment), therefore, captures the complex relationships between the audio and video modalities during training on a sequence of pre-training tasks while all
    
[^21]: 超越传统DoE：深度强化学习用于优化电池动力学模型识别中的实验设计

    Beyond Traditional DoE: Deep Reinforcement Learning for Optimizing Experiments in Model Identification of Battery Dynamics. (arXiv:2310.08198v1 [cs.LG])

    [http://arxiv.org/abs/2310.08198](http://arxiv.org/abs/2310.08198)

    本论文提出了一种基于深度强化学习的新颖DoE方法，用于优化电池动力学模型识别中的实验设计。该方法根据过去实验的统计信息动态地改变实验配置，从而提高了实验效率和准确性。

    

    电池动力学模型识别是能源研究中的一个核心问题；许多能源管理系统和设计过程依赖于准确的电池模型进行效率优化。传统的电池建模方法是利用传统的实验设计（DoE），其中通过许多不同的电流配置来激发电池动力学，并利用测量输出来估计系统动力学。然而，虽然传统方法可以获得有用的模型，但由于需要扫描许多不同的电流配置，这一过程时间长且昂贵。在本研究中，基于深度强化学习开发了一种新的DoE方法，该方法根据过去实验的统计信息动态地改变实验配置。与坚持预定义电流配置的库不同，提出的方法通过更新输出空间来动态修改电流配置。

    Model identification of battery dynamics is a central problem in energy research; many energy management systems and design processes rely on accurate battery models for efficiency optimization. The standard methodology for battery modelling is traditional design of experiments (DoE), where the battery dynamics are excited with many different current profiles and the measured outputs are used to estimate the system dynamics. However, although it is possible to obtain useful models with the traditional approach, the process is time consuming and expensive because of the need to sweep many different current-profile configurations. In the present work, a novel DoE approach is developed based on deep reinforcement learning, which alters the configuration of the experiments on the fly based on the statistics of past experiments. Instead of sticking to a library of predefined current profiles, the proposed approach modifies the current profiles dynamically by updating the output space covere
    
[^22]: 超越微调的模型学习：一项调查

    Learn From Model Beyond Fine-Tuning: A Survey. (arXiv:2310.08184v1 [cs.AI])

    [http://arxiv.org/abs/2310.08184](http://arxiv.org/abs/2310.08184)

    这项研究以Learn From Model (LFM)为名，探索了超越微调的模型学习技术，旨在通过对模型接口进行研究和设计，将基于模型的学习推广到下游任务中。

    

    基于模型的学习（LFM）是一种新的研究趋势，它专注于通过对模型接口进行研究、修改和设计来更好地理解模型的结构和权重（在黑匣子环境中），并将模型泛化到下游任务中。本文将LFM技术的研究分为五个主要领域。

    Foundation models (FM) have demonstrated remarkable performance across a wide range of tasks (especially in the fields of natural language processing and computer vision), primarily attributed to their ability to comprehend instructions and access extensive, high-quality data. This not only showcases their current effectiveness but also sets a promising trajectory towards the development of artificial general intelligence. Unfortunately, due to multiple constraints, the raw data of the model used for large model training are often inaccessible, so the use of end-to-end models for downstream tasks has become a new research trend, which we call Learn From Model (LFM) in this article. LFM focuses on the research, modification, and design of FM based on the model interface, so as to better understand the model structure and weights (in a black box environment), and to generalize the model to downstream tasks. The study of LFM techniques can be broadly categorized into five major areas: mod
    
[^23]: XIMAGENET-12：一种可解释的AI基准数据集，用于模型的稳健性评估

    XIMAGENET-12: An Explainable AI Benchmark Dataset for Model Robustness Evaluation. (arXiv:2310.08182v1 [cs.CV])

    [http://arxiv.org/abs/2310.08182](http://arxiv.org/abs/2310.08182)

    XIMAGENET-12是一个可解释的AI基准数据集，包含12个常见物体类别的超过200,000张图像和15,600个手动语义注释。它通过模拟六个不同的场景，提出了一种超越模型生成能力评估的新的稳健性准则。

    

    缺乏标准化的稳健性评估指标以及广泛依赖各种无关的基准数据集的测试方法，导致学术验证的稳健模型与实际应用中存在的问题之间存在差距。为了解决这个问题，我们引入了XIMAGENET-12，一个可解释的基准数据集，包含超过200,000张图像和15,600个手动语义注释。该数据集涵盖了ImageNet的12个类别，以代表在实际生活中常见的物体，并模拟了六个不同的场景，包括过曝、模糊、颜色变化等。我们进一步提出了一种超越模型生成能力评估的新的稳健性准则。该基准数据集以及相关代码可在https://sites.google.com/view/ximagenet-12/home获取。研究人员和实践者可以利用这一资源，在具有挑战性条件下评估他们的视觉模型的稳健性，从而从实际计算机视觉系统的需求中获益。

    The lack of standardized robustness metrics and the widespread reliance on numerous unrelated benchmark datasets for testing have created a gap between academically validated robust models and their often problematic practical adoption. To address this, we introduce XIMAGENET-12, an explainable benchmark dataset with over 200K images and 15,600 manual semantic annotations. Covering 12 categories from ImageNet to represent objects commonly encountered in practical life and simulating six diverse scenarios, including overexposure, blurring, color changing, etc., we further propose a novel robustness criterion that extends beyond model generation ability assessment. This benchmark dataset, along with related code, is available at https://sites.google.com/view/ximagenet-12/home. Researchers and practitioners can leverage this resource to evaluate the robustness of their visual models under challenging conditions and ultimately benefit from the demands of practical computer vision systems.
    
[^24]: 用超参数优化提升快速最小范数攻击

    Improving Fast Minimum-Norm Attacks with Hyperparameter Optimization. (arXiv:2310.08177v1 [cs.LG])

    [http://arxiv.org/abs/2310.08177](http://arxiv.org/abs/2310.08177)

    本研究使用超参数优化方法，改善了快速最小范数攻击的效果，通过自动选择损失函数、优化器和步长调度器，以及相应的超参数，该方法在多个鲁棒模型下得到了验证。

    

    使用梯度攻击评估机器学习模型的对抗鲁棒性具有挑战性。本研究表明，通过自动选择损失函数、优化器和步长调度器以及相应的超参数，超参数优化可以提高快速最小范数攻击的效果。我们进行了详尽的评估，涉及多个鲁棒模型，证明了在超参数优化的情况下，快速最小范数攻击的有效性得到了改进。我们在https://github.com/pralab/HO-FMN上发布了我们的开源代码。

    Evaluating the adversarial robustness of machine learning models using gradient-based attacks is challenging. In this work, we show that hyperparameter optimization can improve fast minimum-norm attacks by automating the selection of the loss function, the optimizer and the step-size scheduler, along with the corresponding hyperparameters. Our extensive evaluation involving several robust models demonstrates the improved efficacy of fast minimum-norm attacks when hyper-up with hyperparameter optimization. We release our open-source code at https://github.com/pralab/HO-FMN.
    
[^25]: 无限宽度图神经网络用于节点回归/分类

    Infinite Width Graph Neural Networks for Node Regression/ Classification. (arXiv:2310.08176v1 [cs.LG])

    [http://arxiv.org/abs/2310.08176](http://arxiv.org/abs/2310.08176)

    本研究分析了无限宽度图神经网络在图结构化数据上的应用。通过连接深度学习和高斯过程/核方法，研究推广了神经网络，并推导出了闭式形式的核函数和高斯过程。研究结果表明，高斯过程和核方法在不确定性估计方面更加用户友好，并且可以在多种架构和数据集上进行回归/分类任务。

    

    本研究分析了图神经网络，在每个全连接层的节点数量趋近无穷大时，它是对图结构化数据上全连接深度神经网的一种推广。无限宽度神经网络将深度学习与高斯过程和核方法相连接，后者都是具有悠久传统和丰富理论基础的机器学习框架。高斯过程和核方法的超参数较少，可用于不确定性估计，使其在应用中更加用户友好。本研究扩展了将高斯过程和核方法与神经网络相连接的研究数量不断增加的趋势。对于多种架构（包括标准图神经网络、具有跳跃连接的图神经网络和图注意力神经网络），推导出了核函数和高斯过程的闭式形式。对这些架构在各种数据集上进行了评估，并进行了回归/分类任务。

    This work analyzes Graph Neural Networks, a generalization of Fully-Connected Deep Neural Nets on Graph structured data, when their width, that is the number of nodes in each fullyconnected layer is increasing to infinity. Infinite Width Neural Networks are connecting Deep Learning to Gaussian Processes and Kernels, both Machine Learning Frameworks with long traditions and extensive theoretical foundations. Gaussian Processes and Kernels have much less hyperparameters then Neural Networks and can be used for uncertainty estimation, making them more user friendly for applications. This works extends the increasing amount of research connecting Gaussian Processes and Kernels to Neural Networks. The Kernel and Gaussian Process closed forms are derived for a variety of architectures, namely the standard Graph Neural Network, the Graph Neural Network with Skip-Concatenate Connections and the Graph Attention Neural Network. All architectures are evaluated on a variety of datasets on the task
    
[^26]: 利用Swin Transformer模型从计算机断层扫描图像中检测COVID-19

    COVID-19 Detection Using Swin Transformer Approach from Computed Tomography Images. (arXiv:2310.08165v1 [eess.IV])

    [http://arxiv.org/abs/2310.08165](http://arxiv.org/abs/2310.08165)

    利用Swin Transformer模型从CT图像中诊断COVID-19，方法在患者级别上进行预测，取得了优越的准确性，在评估指标上超越基线模型和其他方法，为准确诊断提供了稳健的解决方案。

    

    准确高效地诊断COVID-19对于大规模医学影像数据集非常重要。在本预印论文中，我们提出了一种利用CT图像进行COVID-19诊断的新方法，该方法利用了计算机视觉任务中最先进的Swin Transformer模型。我们的方法包括一种系统的患者级预测方法，其中将每个CT切片分类为COVID-19或非COVID-19，并通过多数表决确定患者的整体诊断结果。在这个背景下，应用Swin Transformer结果表现出异常的诊断准确性。在评估指标方面，我们的方法始终优于基线模型和许多竞争方法，展示了它在COVID-19诊断中的有效性。我们模型达到的宏F1分数超过基线，并提供了一个准确诊断的强有力解决方案。

    The accurate and efficient diagnosis of COVID-19 is of paramount importance, particularly in the context of large-scale medical imaging datasets. In this preprint paper, we propose a novel approach for COVID-19 diagnosis using CT images that leverages the power of Swin Transformer models, state-of-the-art solutions in computer vision tasks. Our method includes a systematic approach for patient-level predictions, where individual CT slices are classified as COVID-19 or non-COVID, and the patient's overall diagnosis is determined through majority voting. The application of the Swin Transformer in this context results in patient-level predictions that demonstrate exceptional diagnostic accuracy. In terms of evaluation metrics, our approach consistently outperforms the baseline, as well as numerous competing methods, showcasing its effectiveness in COVID-19 diagnosis. The macro F1 score achieved by our model exceeds the baseline and offers a robust solution for accurate diagnosis.
    
[^27]: 使用稀疏自编码器解释RLHF调整的语言模型中的奖励模型

    Interpreting Reward Models in RLHF-Tuned Language Models Using Sparse Autoencoders. (arXiv:2310.08164v1 [cs.LG])

    [http://arxiv.org/abs/2310.08164](http://arxiv.org/abs/2310.08164)

    通过使用稀疏自编码器，我们提出了一种解释RLHF调整的语言模型中学习的奖励函数的新方法。这个方法能够通过比较自编码器隐藏空间来识别学习奖励模型准确性的独特特征，并提供了奖励完整性的抽象近似值。

    

    通过稀疏自编码器，我们提出了一种解释RLHF调整的语言模型中学习的奖励函数的新方法。我们的方法利用基本语言模型和经过RLHF调整的版本的激活来训练自编码器集合，并通过比较自编码器隐藏空间来识别反映学习奖励模型准确性的独特特征。为了量化这一点，我们构建了一个情景，调整的语言模型学习令牌-奖励映射以最大化奖励。这是首次应用稀疏自编码器来解释学习奖励和广泛检查语言模型中的奖励学习。我们的方法提供了奖励完整性的抽象近似值，这为确保指定目标和模型行为之间的一致性提供了一个有前景的技术。

    Large language models (LLMs) aligned to human preferences via reinforcement learning from human feedback (RLHF) underpin many commercial applications. However, how RLHF impacts LLM internals remains opaque. We propose a novel method to interpret learned reward functions in RLHF-tuned LLMs using sparse autoencoders. Our approach trains autoencoder sets on activations from a base LLM and its RLHF-tuned version. By comparing autoencoder hidden spaces, we identify unique features that reflect the accuracy of the learned reward model. To quantify this, we construct a scenario where the tuned LLM learns token-reward mappings to maximize reward. This is the first application of sparse autoencoders for interpreting learned rewards and broadly inspecting reward learning in LLMs. Our method provides an abstract approximation of reward integrity. This presents a promising technique for ensuring alignment between specified objectives and model behaviors.
    
[^28]: 在高维空间中研究投影样本协方差的极值渐近性，并在金融和卷积网络中应用

    On Extreme Value Asymptotics of Projected Sample Covariances in High Dimensions with Applications in Finance and Convolutional Networks. (arXiv:2310.08150v1 [math.ST])

    [http://arxiv.org/abs/2310.08150](http://arxiv.org/abs/2310.08150)

    本文研究了高维向量时间序列样本协方差矩阵的函数的最大型统计量，推广了样本自协方差函数的最大偏差情况，证明了Gumbel型极值渐近性成立，并将其应用于金融和卷积网络领域。

    

    本文研究了高维向量时间序列样本协方差矩阵的某些函数的最大型统计量，以统计地确认或拒绝数据集在正常条件下被收集的零假设。本方法推广了样本自协方差函数的最大偏差情况。在线性时间序列框架下，证明了Gumbel型极值渐近性成立。作为应用，我们讨论了仅做多的最小风险组合优化、由稀疏跟踪组合进行ETF指数跟踪、用于图像分析的卷积深度学习和阵列传感器数据的分析。

    Maximum-type statistics of certain functions of the sample covariance matrix of high-dimensional vector time series are studied to statistically confirm or reject the null hypothesis that a data set has been collected under normal conditions. The approach generalizes the case of the maximal deviation of the sample autocovariances function from its assumed values. Within a linear time series framework it is shown that Gumbel-type extreme value asymptotics holds true. As applications we discuss long-only mimimal-variance portfolio optimization and subportfolio analysis with respect to idiosyncratic risks, ETF index tracking by sparse tracking portfolios, convolutional deep learners for image analysis and the analysis of array-of-sensors data.
    
[^29]: 基于开放集知识的视觉问答系统及推理路径

    Open-Set Knowledge-Based Visual Question Answering with Inference Paths. (arXiv:2310.08148v1 [cs.LG])

    [http://arxiv.org/abs/2310.08148](http://arxiv.org/abs/2310.08148)

    本文提出了一种基于开放集知识的视觉问答系统，解决了现有模型中的分类限制和缺乏推理路径的问题。

    

    在给定一张图片和相关的文本问题的情况下，基于知识的视觉问答系统的目的是通过外部知识库提供正确的答案。先前的基于知识的视觉问答系统通常被形式化为一个检索器-分类器框架，在这个框架中，一个预训练的检索器从知识图谱中提取文本或视觉信息，然后在候选答案中进行预测。尽管取得了一些进展，但现有模型存在两个缺点。首先，将问题回答建模为多类别分类将答案空间限制在预设的语料库中，缺乏灵活推理的能力。其次，分类器仅仅考虑"答案是什么"而没有考虑"如何得到答案"，无法将答案与明确的推理路径进行关联。在本文中，我们面对着解释性开放集知识视觉问答系统的挑战，该系统要求能够回答涉及任意实体的问题，并保留可解释的推理路径。

    Given an image and an associated textual question, the purpose of Knowledge-Based Visual Question Answering (KB-VQA) is to provide a correct answer to the question with the aid of external knowledge bases. Prior KB-VQA models are usually formulated as a retriever-classifier framework, where a pre-trained retriever extracts textual or visual information from knowledge graphs and then makes a prediction among the candidates. Despite promising progress, there are two drawbacks with existing models. Firstly, modeling question-answering as multi-class classification limits the answer space to a preset corpus and lacks the ability of flexible reasoning. Secondly, the classifier merely consider "what is the answer" without "how to get the answer", which cannot ground the answer to explicit reasoning paths. In this paper, we confront the challenge of \emph{explainable open-set} KB-VQA, where the system is required to answer questions with entities at wild and retain an explainable reasoning pa
    
[^30]: 多尺度时空循环网络用于交通流预测

    Multi-Scale Spatial-Temporal Recurrent Networks for Traffic Flow Prediction. (arXiv:2310.08138v1 [cs.LG])

    [http://arxiv.org/abs/2310.08138](http://arxiv.org/abs/2310.08138)

    本论文提出了一种名为MSSTRN的多尺度时空循环网络，用于交通流预测。该网络通过两种不同的循环神经网络完全捕获了交通数据中不同时间窗口下的复杂时空信息，解决了现有方法中忽视图结构、捕捉不充分以及缺乏关注不同时间尺度上时空信息的问题。

    

    交通流预测是智能交通系统中最基本的任务之一。复杂而动态的时空依赖关系使得交通流预测变得非常具有挑战性。尽管现有的时空图神经网络在交通流预测中有显著优势，但它们常常面临以下挑战：（1）忽视了固定的图结构，限制了模型的预测性能；（2）不能同时充分捕捉复杂的时空依赖关系；（3）缺乏对不同时间尺度上的时空信息的关注。本文提出了一种多尺度时空循环网络（MSSTRN）用于交通流预测，它由两种不同的循环神经网络组成：单步门循环单元和多步门循环单元，以完全捕获交通数据中不同时间窗口下的复杂时空信息。此外，我们还提出了一种时空同步注意机制。

    Traffic flow prediction is one of the most fundamental tasks of intelligent transportation systems. The complex and dynamic spatial-temporal dependencies make the traffic flow prediction quite challenging. Although existing spatial-temporal graph neural networks hold prominent, they often encounter challenges such as (1) ignoring the fixed graph that limits the predictive performance of the model, (2) insufficiently capturing complex spatial-temporal dependencies simultaneously, and (3) lacking attention to spatial-temporal information at different time lengths. In this paper, we propose a Multi-Scale Spatial-Temporal Recurrent Network for traffic flow prediction, namely MSSTRN, which consists of two different recurrent neural networks: the single-step gate recurrent unit and the multi-step gate recurrent unit to fully capture the complex spatial-temporal information in the traffic data under different time windows. Moreover, we propose a spatial-temporal synchronous attention mechanis
    
[^31]: 逆向解释用于时间序列预测的论文

    Counterfactual Explanations for Time Series Forecasting. (arXiv:2310.08137v1 [cs.LG])

    [http://arxiv.org/abs/2310.08137](http://arxiv.org/abs/2310.08137)

    本论文提出了一种针对时间序列预测的逆向解释方法，通过施加基于梯度的扰动，并利用约束条件指导扰动，从而获得所需的预测结果。实验结果表明，该方法在四种最先进的深度模型架构上表现良好。

    

    在时间序列预测方法的最新发展中，深度预测模型因能利用时间序列中的隐藏特征模式来提升预测性能而受到关注。然而，当前大部分深度预测模型都是不透明的，因此结果的解释具有挑战性。虽然逆向解释已广泛应用于解释分类模型的后处理方法，但其在预测模型中的应用仍相对较少。本文提出了一种新颖的时间序列预测逆向生成问题，并提出了一种名为ForecastCF的算法，通过对原始时间序列施加基于梯度的扰动来解决这一问题。ForecastCF通过对预测值施加约束指导扰动以获得所需的预测结果。我们使用四种最先进的深度模型架构对ForecastCF进行实验评估，并与现有的方法进行比较。

    Among recent developments in time series forecasting methods, deep forecasting models have gained popularity as they can utilize hidden feature patterns in time series to improve forecasting performance. Nevertheless, the majority of current deep forecasting models are opaque, hence making it challenging to interpret the results. While counterfactual explanations have been extensively employed as a post-hoc approach for explaining classification models, their application to forecasting models still remains underexplored. In this paper, we formulate the novel problem of counterfactual generation for time series forecasting, and propose an algorithm, called ForecastCF, that solves the problem by applying gradient-based perturbations to the original time series. ForecastCF guides the perturbations by applying constraints to the forecasted values to obtain desired prediction outcomes. We experimentally evaluate ForecastCF using four state-of-the-art deep model architectures and compare to 
    
[^32]: 公平和多样性数据汇总的核心集构造

    Core-sets for Fair and Diverse Data Summarization. (arXiv:2310.08122v1 [cs.DS])

    [http://arxiv.org/abs/2310.08122](http://arxiv.org/abs/2310.08122)

    该论文研究了在公平/划分约束下多样性最大化任务的核心集构造算法，提出了与两种多样性度量相关的改进算法，并展示了核心集方法的有效性。

    

    我们研究了在公平/划分约束下多样性最大化任务的核心集构造算法。给定一个分成m组的度量空间中的点集P，并给定k1,...,km，该问题的目标是从每个组i中选择k_i个点，使得选出的k=\sum_i k_i个点的整体多样性最大化。我们考虑了两种自然的多样性度量：对两点之间距离求和和对最近邻距离求和，并展示了相对于这些度量的改进的核心集构造算法。具体地说，我们展示了第一个与对两点之间距离求和无关的核心集，其大小与数据集的大小和纵横比无关。其次，我们展示了第一个与最近邻距离之和相关的核心集。最后，我们进行了几个实验，展示了我们的核心集方法的有效性。特别地，我们应用了约束多样性最大化来汇总一组定时消息。

    We study core-set construction algorithms for the task of Diversity Maximization under fairness/partition constraint. Given a set of points $P$ in a metric space partitioned into $m$ groups, and given $k_1,\ldots,k_m$, the goal of this problem is to pick $k_i$ points from each group $i$ such that the overall diversity of the $k=\sum_i k_i$ picked points is maximized. We consider two natural diversity measures: sum-of-pairwise distances and sum-of-nearest-neighbor distances, and show improved core-set construction algorithms with respect to these measures. More precisely, we show the first constant factor core-set w.r.t. sum-of-pairwise distances whose size is independent of the size of the dataset and the aspect ratio. Second, we show the first core-set w.r.t. the sum-of-nearest-neighbor distances. Finally, we run several experiments showing the effectiveness of our core-set approach. In particular, we apply constrained diversity maximization to summarize a set of timed messages that t
    
[^33]: 生成内在优化：具有模型学习的内在控制

    Generative Intrinsic Optimization: Intrisic Control with Model Learning. (arXiv:2310.08100v1 [cs.LG])

    [http://arxiv.org/abs/2310.08100](http://arxiv.org/abs/2310.08100)

    这项工作提出了一种生成内在优化的方法，通过结合模型学习和内在控制，实现了对不同形式结果的综合处理。这种方法保证了收敛到最优策略，有助于提高样本效率并考虑环境不确定性。

    

    未来序列代表在环境中执行动作后的结果。当基于信息论概念的相互信息驱动时，它寻求最具信息量的结果。显式结果可能因状态、回报或轨迹而异，用于不同目的，如学分分配或模仿学习。然而，将内在动机与奖励最大化结合的固有性质往往被忽视。在这项工作中，我们提出了一种变分方法，共同学习估计相互信息和动力学模型的必要数量，为合并不同形式的感兴趣结果提供了一个通用框架。结合到策略迭代方案中，我们的方法保证收敛到最优策略。虽然我们主要关注理论分析，但我们的方法打开了利用带有模型学习的内在控制以提高样本效率并纳入环境不确定性的可能性。

    Future sequence represents the outcome after executing the action into the environment. When driven by the information-theoretic concept of mutual information, it seeks maximally informative consequences. Explicit outcomes may vary across state, return, or trajectory serving different purposes such as credit assignment or imitation learning. However, the inherent nature of incorporating intrinsic motivation with reward maximization is often neglected. In this work, we propose a variational approach to jointly learn the necessary quantity for estimating the mutual information and the dynamics model, providing a general framework for incorporating different forms of outcomes of interest. Integrated into a policy iteration scheme, our approach guarantees convergence to the optimal policy. While we mainly focus on theoretical analysis, our approach opens the possibilities of leveraging intrinsic control with model learning to enhance sample efficiency and incorporate uncertainty of the env
    
[^34]: ClimateBERT-NetZero:检测和评估净零和减排目标

    ClimateBERT-NetZero: Detecting and Assessing Net Zero and Reduction Targets. (arXiv:2310.08096v1 [cs.LG])

    [http://arxiv.org/abs/2310.08096](http://arxiv.org/abs/2310.08096)

    ClimateBERT-NetZero是一个用于自动检测净零和减排目标的工具，它通过专家标注数据集和自然语言分类器的训练实现。该工具可以与问答模型结合使用，分析净零和减排目标中的雄心，并通过分析通信模式的演变提供有希望的途径。

    

    公共和私人参与者在评估各种机构可持续发展承诺的大量信息时面临困难。为了解决这个问题，我们创建了一个新颖的工具，用于自动检测企业、国家和地区的净零和减排目标，这个工具分为三个步骤。首先，我们引入了一个包含3.5K个文本样本的专家标注数据集。其次，我们训练并发布ClimateBERT-NetZero，一个自然语言分类器，用于检测文本是否包含净零或减排目标。第三，我们展示了它在两个使用案例中的分析潜力：首先，我们演示了如何将ClimateBERT-NetZero与传统的问答模型相结合，分析净零和减排目标中展示的雄心。此外，我们在季度财报电话会议记录上使用ClimateBERT-NetZero模型，并概述了通信模式随时间的演变。我们的实验展示了提取和分析净零和排放目标的有希望的途径。

    Public and private actors struggle to assess the vast amounts of information about sustainability commitments made by various institutions. To address this problem, we create a novel tool for automatically detecting corporate, national, and regional net zero and reduction targets in three steps. First, we introduce an expert-annotated data set with 3.5K text samples. Second, we train and release ClimateBERT-NetZero, a natural language classifier to detect whether a text contains a net zero or reduction target. Third, we showcase its analysis potential with two use cases: We first demonstrate how ClimateBERT-NetZero can be combined with conventional question-answering (Q&A) models to analyze the ambitions displayed in net zero and reduction targets. Furthermore, we employ the ClimateBERT-NetZero model on quarterly earning call transcripts and outline how communication patterns evolve over time. Our experiments demonstrate promising pathways for extracting and analyzing net zero and emis
    
[^35]: 分辨时间差分学习

    Discerning Temporal Difference Learning. (arXiv:2310.08091v1 [cs.LG])

    [http://arxiv.org/abs/2310.08091](http://arxiv.org/abs/2310.08091)

    该论文提出了一种名为分辨TD学习(DTD)的新型TD算法，通过灵活的强调函数来分配资源，以改善状态之间的轻重分配，解决了传统TD学习中忽视历史状态重要性和TD误差传播的问题。实证结果表明，使用合理的强调函数不仅改进了值估计，还加速了学习过程。

    

    时间差分学习(TD)是强化学习中的基本概念，旨在高效评估策略的价值函数。TD($\lambda$)是一种有效的变体，通过引入记忆轨迹将预测误差分散到历史上下文中。然而，这种方法经常忽视历史状态的重要性以及传播TD误差的相对重要性，这受到访问失衡或结果噪声等挑战的影响。为了解决这个问题，我们提出了一种名为分辨TD学习(DTD)的新型TD算法，它允许灵活的强调函数-在训练过程中预先确定或自适应地分配资源以提高状态之间的效果。我们在特定类别的强调函数内建立了我们方法的收敛性质，并展示了它在深度RL环境中的潜在应用。实证结果表明，使用合理的强调函数不仅可以改进值估计，还可以加速学习。

    Temporal difference learning (TD) is a foundational concept in reinforcement learning (RL), aimed at efficiently assessing a policy's value function. TD($\lambda$), a potent variant, incorporates a memory trace to distribute the prediction error into the historical context. However, this approach often neglects the significance of historical states and the relative importance of propagating the TD error, influenced by challenges such as visitation imbalance or outcome noise. To address this, we propose a novel TD algorithm named discerning TD learning (DTD), which allows flexible emphasis functions$-$predetermined or adapted during training$-$to allocate efforts effectively across states. We establish the convergence properties of our method within a specific class of emphasis functions and showcase its promising potential for adaptation to deep RL contexts. Empirical results underscore that employing a judicious emphasis function not only improves value estimation but also expedites l
    
[^36]: 处理零膨胀数据：用双重机器学习方法实现SOTA水平

    Dealing with zero-inflated data: achieving SOTA with a two-fold machine learning approach. (arXiv:2310.08088v1 [cs.LG])

    [http://arxiv.org/abs/2310.08088](http://arxiv.org/abs/2310.08088)

    通过应用双重机器学习方法，本文解决了处理零膨胀数据的问题，并在两个实际应用案例中取得了出色的结果。

    

    在许多情况下，机器学习模型必须学会在较广范围的数据中正确预测一些具有特定兴趣值的数据点，而其中许多目标值为零。零膨胀数据可以在各种场景中找到，比如颗粒状和间歇性需求、家用电器的功率消耗（开关开启和关闭）、蒸馏过程中的杂质测量，甚至是机场班车需求预测等。零的存在影响了模型的学习并可能导致性能不佳。此外，零也扭曲了用于计算模型预测质量的指标。本文展示了两个实际应用案例（家用电器分类和机场班车需求预测），其中在零膨胀数据背景下应用层次模型取得了出色的结果。特别是对于家用电器分类，加权平均精确度、召回率、F1值和AUC ROC分别提高了27%、34%、49%和27%。

    In many cases, a machine learning model must learn to correctly predict a few data points with particular values of interest in a broader range of data where many target values are zero. Zero-inflated data can be found in diverse scenarios, such as lumpy and intermittent demands, power consumption for home appliances being turned on and off, impurities measurement in distillation processes, and even airport shuttle demand prediction. The presence of zeroes affects the models' learning and may result in poor performance. Furthermore, zeroes also distort the metrics used to compute the model's prediction quality. This paper showcases two real-world use cases (home appliances classification and airport shuttle demand prediction) where a hierarchical model applied in the context of zero-inflated data leads to excellent results. In particular, for home appliances classification, the weighted average of Precision, Recall, F1, and AUC ROC was increased by 27%, 34%, 49%, and 27%, respectively.
    
[^37]: 一种用于联邦学习的碳追踪模型：量化和稀疏化的影响

    A Carbon Tracking Model for Federated Learning: Impact of Quantization and Sparsification. (arXiv:2310.08087v1 [eess.SP])

    [http://arxiv.org/abs/2310.08087](http://arxiv.org/abs/2310.08087)

    本文提出了一个碳追踪模型，用于实时监测联邦学习系统的能源消耗和碳足迹影响。通过对不同的计算和通信高效联邦学习方法进行定量评估，为减少能源消耗和碳排放提供了参考。

    

    联邦学习方法采用高效的通信技术将机器学习任务分布在边缘设备上，与集中式解决方案相比，在数据存储和计算复杂性方面减少了开销。联邦学习为解决从生产者（传感器、机器）到能耗高的数据中心大量传输数据引起的资源需求而引发的环境问题提供了替代解决方案，同时使得新的物联网人工智能（AIoT）应用成为可能。本文提出了一个用于实时监测联邦学习系统能源消耗和碳足迹影响的框架。该碳追踪工具对共识（完全分散）和传统联邦学习策略进行了评估。我们首次从能耗和等效碳排放的角度定量评估了不同的计算和通信高效联邦学习方法。

    Federated Learning (FL) methods adopt efficient communication technologies to distribute machine learning tasks across edge devices, reducing the overhead in terms of data storage and computational complexity compared to centralized solutions. Rather than moving large data volumes from producers (sensors, machines) to energy-hungry data centers, raising environmental concerns due to resource demands, FL provides an alternative solution to mitigate the energy demands of several learning tasks while enabling new Artificial Intelligence of Things (AIoT) applications. This paper proposes a framework for real-time monitoring of the energy and carbon footprint impacts of FL systems. The carbon tracking tool is evaluated for consensus (fully decentralized) and classical FL policies. For the first time, we present a quantitative evaluation of different computationally and communication efficient FL methods from the perspectives of energy consumption and carbon equivalent emissions, suggesting 
    
[^38]: 是否进行词元化：用于跨语言转换的文本表示的比较研究

    To token or not to token: A Comparative Study of Text Representations for Cross-Lingual Transfer. (arXiv:2310.08078v1 [cs.CL])

    [http://arxiv.org/abs/2310.08078](http://arxiv.org/abs/2310.08078)

    本文进行了针对跨语言转换的文本表示方案的比较研究，发现图像模型在相关且脚本相似的语言之间的转换中表现优秀，而基于分割的模型在偏向单词含义的任务中表现更好。

    

    在资源匮乏的跨语言转换中，选择适当的词元化方案往往是一个瓶颈。为了理解文本表示选择的下游影响，我们对具有不同文本表示模态的语言模型进行了比较分析，包括2个基于分割的模型（BERT，mBERT），1个基于图像的模型（PIXEL），和1个字符级模型（CANINE）。首先，我们提出了一个评分语言商数（LQ）指标，能够提供零射击和少射击评估的加权表示。利用这个指标，我们在三个任务（词性标注，依存句法分析和命名实体识别）上进行了包含19个源语言和133个目标语言的实验。我们的分析表明，当语言之间关系密切且具有相似的视觉脚本时，基于图像的模型在跨语言转换中表现出色。然而，对于偏向于单词含义的任务（词性标注，命名实体识别），基于分割的模型证明是更好的选择。

    Choosing an appropriate tokenization scheme is often a bottleneck in low-resource cross-lingual transfer. To understand the downstream implications of text representation choices, we perform a comparative analysis on language models having diverse text representation modalities including 2 segmentation-based models (\texttt{BERT}, \texttt{mBERT}), 1 image-based model (\texttt{PIXEL}), and 1 character-level model (\texttt{CANINE}). First, we propose a scoring Language Quotient (LQ) metric capable of providing a weighted representation of both zero-shot and few-shot evaluation combined. Utilizing this metric, we perform experiments comprising 19 source languages and 133 target languages on three tasks (POS tagging, Dependency parsing, and NER). Our analysis reveals that image-based models excel in cross-lingual transfer when languages are closely related and share visually similar scripts. However, for tasks biased toward word meaning (POS, NER), segmentation-based models prove to be sup
    
[^39]: 薄冰样本：重新评估神经网络的对抗剪枝

    Samples on Thin Ice: Re-Evaluating Adversarial Pruning of Neural Networks. (arXiv:2310.08073v1 [cs.LG])

    [http://arxiv.org/abs/2310.08073](http://arxiv.org/abs/2310.08073)

    本研究重新评估了对抗性剪枝方法，发现其鲁棒性被高估。剪枝后，接近未剪枝模型决策边界的样本通常被错误分类。这些结果对未来设计更有效的对抗性剪枝方法具有指导意义。

    

    神经网络剪枝已被证明是一种有效的减小网络大小的技术，它通过增加稀疏度来换取良好的泛化能力和对抗攻击的鲁棒性。最近的研究声称，对抗性剪枝方法可以在产生稀疏网络的同时保持对抗性示例的鲁棒性。在本文中，我们首先重新评估了三种最先进的对抗性剪枝方法，发现它们的鲁棒性的确被高估了。然后，我们比较了剪枝和密集模型的版本，发现在剪枝之后，接近未剪枝模型决策边界的样本通常被错误分类。最后，我们讨论了这种直觉可能如何在未来的工作中导致设计更有效的对抗性剪枝方法。

    Neural network pruning has shown to be an effective technique for reducing the network size, trading desirable properties like generalization and robustness to adversarial attacks for higher sparsity. Recent work has claimed that adversarial pruning methods can produce sparse networks while also preserving robustness to adversarial examples. In this work, we first re-evaluate three state-of-the-art adversarial pruning methods, showing that their robustness was indeed overestimated. We then compare pruned and dense versions of the same models, discovering that samples on thin ice, i.e., closer to the unpruned model's decision boundary, are typically misclassified after pruning. We conclude by discussing how this intuition may lead to designing more effective adversarial pruning methods in future work.
    
[^40]: 学习可迁移的概念原型以解释无监督领域适应

    Learning Transferable Conceptual Prototypes for Interpretable Unsupervised Domain Adaptation. (arXiv:2310.08071v1 [cs.LG])

    [http://arxiv.org/abs/2310.08071](http://arxiv.org/abs/2310.08071)

    本文提出了一种名为Transferable Conceptual Prototype Learning（TCPL）的可迁移概念原型学习方法，它能够同时解释和改进无监督领域适应中的知识转移和决策过程。

    

    尽管深度神经网络在无监督领域适应（UDA）方面取得了巨大进展，但目前的UDA模型不透明，无法提供令人满意的解释，限制了其在需要安全和可控模型决策的场景中的应用。目前，有大量的研究专注于设计具有充分数据注释的深度可解释方法，只有少数方法考虑了分布偏移问题。大多数现有的可解释UDA方法是事后方法，不能促进模型学习过程以提升性能。在本文中，我们提出了一种名为Transferable Conceptual Prototype Learning（TCPL）的本质上可解释的方法，它可以同时解释和改进UDA中的知识转移和决策过程。为了实现这一目标，我们设计了一个层次化的原型模块，从源域传输分类基本概念到目标域并学习领域共享的原型。

    Despite the great progress of unsupervised domain adaptation (UDA) with the deep neural networks, current UDA models are opaque and cannot provide promising explanations, limiting their applications in the scenarios that require safe and controllable model decisions. At present, a surge of work focuses on designing deep interpretable methods with adequate data annotations and only a few methods consider the distributional shift problem. Most existing interpretable UDA methods are post-hoc ones, which cannot facilitate the model learning process for performance enhancement. In this paper, we propose an inherently interpretable method, named Transferable Conceptual Prototype Learning (TCPL), which could simultaneously interpret and improve the processes of knowledge transfer and decision-making in UDA. To achieve this goal, we design a hierarchically prototypical module that transfers categorical basic concepts from the source domain to the target domain and learns domain-shared prototyp
    
[^41]: 紧密时间空间下对于常数通行学习的下界

    Tight Time-Space Lower Bounds for Constant-Pass Learning. (arXiv:2310.08070v1 [cs.LG])

    [http://arxiv.org/abs/2310.08070](http://arxiv.org/abs/2310.08070)

    该论文证明了对于常数次通行的任何奇偶学习算法，需要要么Ω(n^2)的内存大小，要么至少需要2^的样本数量。

    

    在他的突破性论文中，Raz证明了任何奇偶学习算法要么需要二次内存，要么需要指数数量的样本。随后的一系列工作将此结果扩展到了大类学习问题。直到最近，所有这些结果都考虑了流式模型中的学习，其中每个样本都是独立绘制的，而学习者被允许在样本流上进行单次通行。Garg、Raz和Tal则考虑了一个更强的模型，允许对样本流进行多次通行。在2次通行模型中，他们证明了大小为n的奇偶学习需要n^1.5的内存大小或者至少2^(n^0.5)个样本数量。在这项工作中，对于任意常数q，我们证明了对于在样本流上进行q次通行的任何奇偶学习算法的紧密内存-样本下界。我们证明这样的学习者要么需要Ω(n^2)的内存大小，要么至少需要2^的样本数量。

    In his breakthrough paper, Raz showed that any parity learning algorithm requires either quadratic memory or an exponential number of samples [FOCS'16, JACM'19]. A line of work that followed extended this result to a large class of learning problems. Until recently, all these results considered learning in the streaming model, where each sample is drawn independently, and the learner is allowed a single pass over the stream of samples. Garg, Raz, and Tal [CCC'19] considered a stronger model, allowing multiple passes over the stream. In the $2$-pass model, they showed that learning parities of size $n$ requires either a memory of size $n^{1.5}$ or at least $2^{\sqrt{n}}$ samples. (Their result also generalizes to other learning problems.)  In this work, for any constant $q$, we prove tight memory-sample lower bounds for any parity learning algorithm that makes $q$ passes over the stream of samples. We show that such a learner requires either $\Omega(n^{2})$ memory size or at least $2^{\
    
[^42]: 重新思考代码搜索中的负样本对

    Rethinking Negative Pairs in Code Search. (arXiv:2310.08069v1 [cs.SE])

    [http://arxiv.org/abs/2310.08069](http://arxiv.org/abs/2310.08069)

    本文提出了一种简单而有效的Soft-InfoNCE损失函数，通过在InfoNCE中插入权重项来解决代码搜索中负样本的问题，包括大型代码库中的虚假负样本和未能区分负样本的潜在相关性。

    

    最近，对比学习成为细化代码搜索模型以提高软件开发效率和效果的关键组成部分。它将正样本代码片段聚集在一起，同时将与搜索查询不相关的负样本推开。在对比学习中，InfoNCE是最常用的损失函数，因为它具有更好的性能。然而，InfoNCE负样本存在以下问题可能会损害其表示学习的效果：1）由于重复，大型代码库中存在虚假负样本。2）未能明确区分负样本的潜在相关性。例如，对于快速排序算法查询，冒泡排序算法示例要比文件保存函数“更负面”。在本文中，我们通过提出一种简单而有效的Soft-InfoNCE损失来解决上述问题。在我们提出的损失函数中，我们采用了三种方法来估计权重...

    Recently, contrastive learning has become a key component in fine-tuning code search models for software development efficiency and effectiveness. It pulls together positive code snippets while pushing negative samples away given search queries. Among contrastive learning, InfoNCE is the most widely used loss function due to its better performance. However, the following problems in negative samples of InfoNCE may deteriorate its representation learning: 1) The existence of false negative samples in large code corpora due to duplications. 2). The failure to explicitly differentiate between the potential relevance of negative samples. As an example, a bubble sorting algorithm example is less ``negative'' than a file saving function for the quick sorting algorithm query. In this paper, we tackle the above problems by proposing a simple yet effective Soft-InfoNCE loss that inserts weight terms into InfoNCE. In our proposed loss function, we apply three methods to estimate the weights of n
    
[^43]: ETDock: 一种新颖的等变Transformer用于蛋白质-配体对接

    ETDock: A Novel Equivariant Transformer for Protein-Ligand Docking. (arXiv:2310.08061v1 [q-bio.BM])

    [http://arxiv.org/abs/2310.08061](http://arxiv.org/abs/2310.08061)

    提出了一种新颖的等变Transformer神经网络用于蛋白质-配体对接，通过融合配体的图层特征，并使用TAMformer模块学习配体和蛋白质的表示，实现了对配体位姿的准确预测，并通过迭代优化方法生成精炼的配体位姿。

    

    预测蛋白质和配体之间的对接是药物开发中关键且具有挑战性的任务。然而，传统的对接方法主要依赖于评分函数，而基于深度学习的对接方法通常忽略了蛋白质和配体的3D空间信息以及配体的图层特征，限制了它们的性能。为了解决这些限制，我们提出了一种蛋白质-配体对接位姿预测的等变Transformer神经网络。我们的方法通过特征处理来融合配体的图层特征，然后使用我们提出的TAMformer模块学习配体和蛋白质的表示。此外，我们采用基于预测的距离矩阵的迭代优化方法来生成精炼的配体位姿。实验结果表明，我们的模型可以达到最先进的性能水平。

    Predicting the docking between proteins and ligands is a crucial and challenging task for drug discovery. However, traditional docking methods mainly rely on scoring functions, and deep learning-based docking approaches usually neglect the 3D spatial information of proteins and ligands, as well as the graph-level features of ligands, which limits their performance. To address these limitations, we propose an equivariant transformer neural network for protein-ligand docking pose prediction. Our approach involves the fusion of ligand graph-level features by feature processing, followed by the learning of ligand and protein representations using our proposed TAMformer module. Additionally, we employ an iterative optimization approach based on the predicted distance matrix to generate refined ligand poses. The experimental results on real datasets show that our model can achieve state-of-the-art performance.
    
[^44]: 从标签比例中学习：通过信念传播对有监督学习器进行引导

    Learning from Label Proportions: Bootstrapping Supervised Learners via Belief Propagation. (arXiv:2310.08056v1 [cs.LG])

    [http://arxiv.org/abs/2310.08056](http://arxiv.org/abs/2310.08056)

    本文提出了一种从标签比例中学习的算法框架，通过伪标签化和嵌入细化两个步骤迭代地提高有监督学习器性能。

    

    标签比例学习（LLP）是一个学习问题，在训练过程中，只有针对一组实例（称为包）的聚合级别标签可用，并且目的是在测试数据的实例级别上获得最佳性能。这种设置在广告和医学等领域由于隐私考虑而出现。我们提出了一个新颖的算法框架来解决这个问题，它通过两个主要步骤进行迭代。在每次迭代的第一步（伪标签化）中，我们定义了一个基于二进制实例标签的吉布斯分布，该分布通过以下约束将covariate信息（协变量信息）合并进去：具有相似covariates的实例应该具有相似的标签，并且通过包级别的聚合标签来综合covariate信息。然后，我们使用信念传播（BP）来边缘化吉布斯分布以获得伪标签。在第二步（嵌入细化）中，我们使用伪标签为学习器提供监督，从而获得更好的嵌入。此后，我们对这两个步骤进行迭代。

    Learning from Label Proportions (LLP) is a learning problem where only aggregate level labels are available for groups of instances, called bags, during training, and the aim is to get the best performance at the instance-level on the test data. This setting arises in domains like advertising and medicine due to privacy considerations. We propose a novel algorithmic framework for this problem that iteratively performs two main steps. For the first step (Pseudo Labeling) in every iteration, we define a Gibbs distribution over binary instance labels that incorporates a) covariate information through the constraint that instances with similar covariates should have similar labels and b) the bag level aggregated label. We then use Belief Propagation (BP) to marginalize the Gibbs distribution to obtain pseudo labels. In the second step (Embedding Refinement), we use the pseudo labels to provide supervision for a learner that yields a better embedding. Further, we iterate on the two steps ag
    
[^45]: LGL-BCI：一种轻量级几何学习框架用于基于运动想象的脑机接口

    LGL-BCI: A Lightweight Geometric Learning Framework for Motor Imagery-Based Brain-Computer Interfaces. (arXiv:2310.08051v1 [cs.LG])

    [http://arxiv.org/abs/2310.08051](http://arxiv.org/abs/2310.08051)

    LGL-BCI是一种轻量级几何学习框架，通过处理EEG数据在非欧几里德度量空间中捕捉运动想象任务的空间相关性，并通过特征分解算法进行EEG通道选择以提高推断速度。实验证明LGL-BCI相比现有解决方案具有更高的准确性和效率。

    

    脑机接口是一种使用脑信号与外部设备进行交互的开创性技术。尽管有所进展，基于脑电图（EEG）的运动想象任务面临挑战，如幅度和相位变异，以及复杂的空间相关性，需要更小的模型大小和更快的推断。本研究介绍了LGL-BCI框架，采用几何深度学习框架处理非欧几里德度量空间中的EEG，特别是对称正定（SPD）流形空间。LGL-BCI提供了稳健的EEG数据表示，并捕捉了空间相关性。我们提出了一种通过特征分解算法进行EEG通道选择的解决方案，以减少SPD矩阵的维度，同时提高了推断速度。广泛的实验显示，与当前解决方案相比，LGL-BCI具有更高的准确性和效率，突出了几何深度学习在运动想象-脑机接口应用中的潜力。

    Brain-Computer Interfaces (BCIs) are a groundbreaking technology for interacting with external devices using brain signals. Despite advancements, electroencephalogram (EEG)-based Motor Imagery (MI) tasks face challenges like amplitude and phase variability, and complex spatial correlations, with a need for smaller model size and faster inference. This study introduces the LGL-BCI framework, employing a Geometric Deep Learning Framework for EEG processing in non-Euclidean metric spaces, particularly the Symmetric Positive Definite (SPD) Manifold space. LGL-BCI offers robust EEG data representation and captures spatial correlations. We propose an EEG channel selection solution via a feature decomposition algorithm to reduce SPD matrix dimensionality, with a lossless transformation boosting inference speed. Extensive experiments show LGL-BCI's superior accuracy and efficiency compared to current solutions, highlighting geometric deep learning's potential in MI-BCI applications. The effici
    
[^46]: 探索模型架构与上下文学习能力之间的关系

    Exploring the Relationship Between Model Architecture and In-Context Learning Ability. (arXiv:2310.08049v1 [cs.LG])

    [http://arxiv.org/abs/2310.08049](http://arxiv.org/abs/2310.08049)

    现有的模型架构在上下文学习中表现最好，特别是在任务复杂性增加的情况下。不同架构对超参数设置的敏感度有所差异，且一些架构展现出平稳的学习轨迹。

    

    模型架构和执行上下文学习任务的能力之间有什么关联？在这项实证研究中，我们首次尝试回答这个问题。具体而言，我们评估了十五种模型架构在一套合成的上下文学习任务中的表现。所选的架构代表了各种范式，包括循环和卷积神经网络，变换器以及新兴的注意力替代方案。我们发现，在特定条件下，所有考虑的架构都能够执行上下文学习任务。然而，当任务复杂性增加时，当代架构表现最好。此外，我们的后续实验探索了一些影响上下文学习的因素。我们观察到，不同架构对超参数设置有不同的敏感性。我们对训练动态的研究揭示了某些架构呈现出平稳、渐进的学习轨迹，而...

    What is the relationship between model architecture and the ability to perform in-context learning? In this empirical study, we take the first steps towards answering this question. In particular, we evaluate fifteen model architectures across a suite of synthetic in-context learning tasks. The selected architectures represent a broad range of paradigms, including recurrent and convolution-based neural networks, transformers, and emerging attention alternatives. We discover that all considered architectures can perform in-context learning under certain conditions. However, contemporary architectures are found to be the best performing, especially as task complexity grows. Additionally, our follow-up experiments delve into various factors that influence in-context learning. We observe varied sensitivities among architectures with respect to hyperparameter settings. Our study of training dynamics reveals that certain architectures exhibit a smooth, progressive learning trajectory, while 
    
[^47]: QLLM: 大规模语言模型的准确高效低位宽量化

    QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models. (arXiv:2310.08041v1 [cs.CL])

    [http://arxiv.org/abs/2310.08041](http://arxiv.org/abs/2310.08041)

    QLLM是一种为大规模语言模型设计的准确高效的低位宽后训练量化方法，通过引入自适应通道重组技术，将离群值的大小重新分配给其他通道，从而减轻它们对量化范围的影响。

    

    大规模语言模型在自然语言处理领域表现出色，但由于其所需资源过大，限制了其广泛应用。虽然量化感知训练（Quantization-Aware Training，QAT）提供了一种解决方案，但它的训练成本过高，因此后训练量化（Post-Training Quantization，PTQ）成为大规模语言模型更实际的方法。在现有研究中，特定通道中的激活离群值被认为是导致后训练量化准确性下降的瓶颈。本文提出了QLLM，一种为大规模语言模型设计的准确高效的低位宽后训练量化方法。QLLM引入了一种自适应通道重组技术，将离群值的大小重新分配给其他通道，从而减轻它们对量化范围的影响。具体来说，通过通道拆分和通道组装，在保证低位宽的情况下将离群通道分解成多个子通道。

    Large Language Models (LLMs) excel in NLP, but their demands hinder their widespread deployment. While Quantization-Aware Training (QAT) offers a solution, its extensive training costs make Post-Training Quantization (PTQ) a more practical approach for LLMs. In existing studies, activation outliers in particular channels are identified as the bottleneck to PTQ accuracy. They propose to transform the magnitudes from activations to weights, which however offers limited alleviation or suffers from unstable gradients, resulting in a severe performance drop at low-bitwidth. In this paper, we propose QLLM, an accurate and efficient low-bitwidth PTQ method designed for LLMs. QLLM introduces an adaptive channel reassembly technique that reallocates the magnitude of outliers to other channels, thereby mitigating their impact on the quantization range. This is achieved by channel disassembly and channel assembly, which first breaks down the outlier channels into several sub-channels to ensure a 
    
[^48]: SEE-OoD: 增强型离域检测的有监督探索

    SEE-OoD: Supervised Exploration For Enhanced Out-of-Distribution Detection. (arXiv:2310.08040v1 [cs.LG])

    [http://arxiv.org/abs/2310.08040](http://arxiv.org/abs/2310.08040)

    本研究提出了一种基于生成对抗训练的方式，利用有限的离域样本进行数据增强和探索，以提高离域检测的准确性。

    

    当前的离域检测技术主要依赖于量化预测不确定性并在训练阶段通过模型正则化来使用真实或合成的离域样本。然而，利用真实离域样本的方法缺乏探索，容易过拟合手头的离域样本。而合成样本通常是基于从训练数据中提取的特征生成的，当训练数据和离域数据在特征空间中高度重叠时，它们的效果较差。在这项工作中，我们提出了一种基于Wasserstein分数的生成对抗训练方案，以提高离域检测的准确性，这是首次在有限离域样本的监督下同时进行数据增强和探索。具体而言，生成器利用鉴别器的反馈来探索离域空间并生成合成的离域样本，而鉴别器则利用观察到的和合成的样本来进行离域检测。

    Current techniques for Out-of-Distribution (OoD) detection predominantly rely on quantifying predictive uncertainty and incorporating model regularization during the training phase, using either real or synthetic OoD samples. However, methods that utilize real OoD samples lack exploration and are prone to overfit the OoD samples at hand. Whereas synthetic samples are often generated based on features extracted from training data, rendering them less effective when the training and OoD data are highly overlapped in the feature space. In this work, we propose a Wasserstein-score-based generative adversarial training scheme to enhance OoD detection accuracy, which, for the first time, performs data augmentation and exploration simultaneously under the supervision of limited OoD samples. Specifically, the generator explores OoD spaces and generates synthetic OoD samples using feedback from the discriminator, while the discriminator exploits both the observed and synthesized samples for OoD
    
[^49]: 重新思考大规模预排序系统：整体链路跨域模型

    Rethinking Large-scale Pre-ranking System: Entire-chain Cross-domain Models. (arXiv:2310.08039v1 [cs.IR])

    [http://arxiv.org/abs/2310.08039](http://arxiv.org/abs/2310.08039)

    本文提出了一种重新思考大规模预排序系统的方法，通过整体链路跨域模型和细粒度神经结构来解决样本选择偏差问题，并改进预排序的准确性。

    

    工业系统，如推荐系统和在线广告，已广泛配备了多阶段架构，包括匹配、预排序、排序和再排序。作为匹配和排序之间的关键桥梁，现有的预排序方法主要忽视了整个链路数据的依赖性，导致子优化性能。在本文中，我们从整体样本空间的角度重新思考预排序系统，并提出了整体链路跨域模型（ECM），利用整个级联阶段的样本来有效减轻样本选择偏差（SSB）问题。此外，我们设计了一种细粒度神经结构，名为ECMM，进一步提高了预排序的准确性。具体来说，我们提出了一个跨域多塔神经网络来综合预测每个阶段的结果，并引入$L0$正则化的子网络路由策略来调整每个阶段的贡献权重。

    Industrial systems such as recommender systems and online advertising, have been widely equipped with multi-stage architectures, which are divided into several cascaded modules, including matching, pre-ranking, ranking and re-ranking. As a critical bridge between matching and ranking, existing pre-ranking approaches mainly endure sample selection bias (SSB) problem owing to ignoring the entire-chain data dependence, resulting in sub-optimal performances. In this paper, we rethink pre-ranking system from the perspective of the entire sample space, and propose Entire-chain Cross-domain Models (ECM), which leverage samples from the whole cascaded stages to effectively alleviate SSB problem. Besides, we design a fine-grained neural structure named ECMM to further improve the pre-ranking accuracy. Specifically, we propose a cross-domain multi-tower neural network to comprehensively predict for each stage result, and introduce the sub-networking routing strategy with $L0$ regularization to r
    
[^50]: 基于流形扩展回放的持续学习

    Continual Learning via Manifold Expansion Replay. (arXiv:2310.08038v1 [cs.LG])

    [http://arxiv.org/abs/2310.08038](http://arxiv.org/abs/2310.08038)

    该论文提出了一种名为Manifold Expansion Replay（MaER）的新型回放策略，通过扩展知识表示中的隐式流形来减少灾难性遗忘。采用贪心策略增加缓冲区中知识表示的流形直径，并使用Wasserstein距离作为距离度量，以提高模型的鲁棒性和表达能力。

    

    在持续学习中，学习者按顺序学习多个任务，每个任务只获取一次数据。灾难性遗忘是持续学习的主要挑战。为了减少遗忘，一些现有的基于回放的方法使用情境记忆来重新播放先前任务的样本。然而，在学习新任务时进行知识整合的过程中，由于旧知识和新知识之间的不平衡，这种策略也会遭受灾难性遗忘。为了解决这个问题，我们提出了一种称为Manifold Expansion Replay（MaER）的新型回放策略。我们认为，在情境记忆中扩展知识表示的隐式流形有助于提高模型的鲁棒性和表达能力。为此，我们提出了一种贪心策略，在内存管理过程中，不断增加由缓冲区中的知识表示的隐式流形的直径。此外，我们将Wasserstein距离引入替代交叉熵作为距离度量。

    In continual learning, the learner learns multiple tasks in sequence, with data being acquired only once for each task. Catastrophic forgetting is a major challenge to continual learning. To reduce forgetting, some existing rehearsal-based methods use episodic memory to replay samples of previous tasks. However, in the process of knowledge integration when learning a new task, this strategy also suffers from catastrophic forgetting due to an imbalance between old and new knowledge. To address this problem, we propose a novel replay strategy called Manifold Expansion Replay (MaER). We argue that expanding the implicit manifold of the knowledge representation in the episodic memory helps to improve the robustness and expressiveness of the model. To this end, we propose a greedy strategy to keep increasing the diameter of the implicit manifold represented by the knowledge in the buffer during memory management. In addition, we introduce Wasserstein distance instead of cross entropy as dis
    
[^51]: ZEST:基于注意力机制的零样本学习用于未见过的物联网设备分类

    ZEST: Attention-based Zero-Shot Learning for Unseen IoT Device Classification. (arXiv:2310.08036v1 [cs.NI])

    [http://arxiv.org/abs/2310.08036](http://arxiv.org/abs/2310.08036)

    ZEST是一种基于注意力机制的零样本学习框架，用于分类未见过的物联网设备。通过使用自注意力机制提取特征并利用生成模型生成伪数据，ZEST能够在性能上取得显著的改进。

    

    最近的研究工作提出了用于分类与网络连接的物联网设备的机器学习模型。然而，在模型训练期间，由于没有所有设备（因此没有它们的流量），仍然存在无法使用所有设备进行训练的实际挑战。这意味着在操作阶段需要对在训练阶段未见过的新设备进行分类。为了解决这个挑战，我们提出了一种基于自注意力机制的零样本学习框架ZEST，用于分类已知和未知的设备。ZEST包括：i）基于自注意力机制的网络特征提取器SANE，用于提取物联网流量的潜在空间表示；ii）一个生成模型，利用潜在特征训练解码器生成伪数据；iii）一个监督模型，用于基于生成的伪数据进行设备分类。我们对真实的物联网流量数据进行了大量实验；实验结果表明ZEST在性能上取得了显著的改进。

    Recent research works have proposed machine learning models for classifying IoT devices connected to a network. However, there is still a practical challenge of not having all devices (and hence their traffic) available during the training of a model. This essentially means, during the operational phase, we need to classify new devices not seen during the training phase. To address this challenge, we propose ZEST -- a ZSL (zero-shot learning) framework based on self-attention for classifying both seen and unseen devices. ZEST consists of i) a self-attention based network feature extractor, termed SANE, for extracting latent space representations of IoT traffic, ii) a generative model that trains a decoder using latent features to generate pseudo data, and iii) a supervised model that is trained on the generated pseudo data for classifying devices. We carry out extensive experiments on real IoT traffic data; our experiments demonstrate i) ZEST achieves significant improvement (in terms 
    
[^52]: 带有噪声标签的局部图聚类

    Local Graph Clustering with Noisy Labels. (arXiv:2310.08031v1 [cs.LG])

    [http://arxiv.org/abs/2310.08031](http://arxiv.org/abs/2310.08031)

    本论文研究了使用噪声节点标签作为额外节点信息的局部图聚类方法，并探究了将噪声标签纳入局部图聚类的好处。

    

    在机器学习问题中，对于带有额外节点信息（如文本、图像或标签）的图形的增加兴趣，促使了需要耗费大量资源处理整个图形的方法的流行。然而，对于从这样的数据中提取有用信息的快速局部方法（即不需要访问整个图形）的发展还很少。为此，我们提出了使用噪声节点标签作为额外节点信息的局部图聚类的研究。在这种设置下，节点根据所属簇的联属关系接收初始二进制标签：如果它们属于目标簇，则为1；否则为0。随后，这些标签的一部分会被翻转。我们研究了将噪声标签纳入局部图聚类的好处。通过构建带有这些标签的加权图形，我们研究了基于图扩散的局部聚类方法在原始图形和加权图形上的性能。从理论角度出发，

    The growing interest in machine learning problems over graphs with additional node information such as texts, images, or labels has popularized methods that require the costly operation of processing the entire graph. Yet, little effort has been made to the development of fast local methods (i.e. without accessing the entire graph) that extract useful information from such data. To that end, we propose a study of local graph clustering using noisy node labels as a proxy for additional node information. In this setting, nodes receive initial binary labels based on cluster affiliation: 1 if they belong to the target cluster and 0 otherwise. Subsequently, a fraction of these labels is flipped. We investigate the benefits of incorporating noisy labels for local graph clustering. By constructing a weighted graph with such labels, we study the performance of graph diffusion-based local clustering method on both the original and the weighted graphs. From a theoretical perspective, we consider
    
[^53]: 鲁棒的一比特压缩感知与迭代硬阈值化

    Robust 1-bit Compressed Sensing with Iterative Hard Thresholding. (arXiv:2310.08019v1 [cs.IT])

    [http://arxiv.org/abs/2310.08019](http://arxiv.org/abs/2310.08019)

    本文研究了鲁棒的一比特压缩感知问题，并提出了二元迭代硬阈值化（BIHT）算法，在噪声情况下比目前已知方法表现更好。

    

    在一比特压缩感知中，目标是从仅有正负符号量化的线性测量中恢复一个k-稀疏单位向量x，使其相对于最小二乘误差ϵ（在ℓ2范数下）内。本文研究了一种带噪声的情况，即部分测量值可能被对手翻转的情况。具体地，我们分析了一种名为二元迭代硬阈值化（Binary Iterative Hard Thresholding，BIHT）的算法，在这个噪声情况下进行一比特压缩感知。最近的研究结果表明，使用近似O(k/ϵ)个无噪声测量，BIHT算法可以提供一个ϵ误差范围内的估计。这个结果是最优的和通用的，意味着一组测量可以适用于所有稀疏向量。本文还展示了在噪声情况下，BIHT算法比目前已知的所有方法都提供更好的结果。

    In 1-bit compressed sensing, the aim is to estimate a $k$-sparse unit vector $x\in S^{n-1}$ within an $\epsilon$ error (in $\ell_2$) from minimal number of linear measurements that are quantized to just their signs, i.e., from measurements of the form $y = \mathrm{Sign}(\langle a, x\rangle).$ In this paper, we study a noisy version where a fraction of the measurements can be flipped, potentially by an adversary. In particular, we analyze the Binary Iterative Hard Thresholding (BIHT) algorithm, a proximal gradient descent on a properly defined loss function used for 1-bit compressed sensing, in this noisy setting. It is known from recent results that, with $\tilde{O}(\frac{k}{\epsilon})$ noiseless measurements, BIHT provides an estimate within $\epsilon$ error. This result is optimal and universal, meaning one set of measurements work for all sparse vectors. In this paper, we show that BIHT also provides better results than all known methods for the noisy setting. We show that when up t
    
[^54]: 为什么要训练更多？通过记忆进行有效和高效的成员推断攻击。

    Why Train More? Effective and Efficient Membership Inference via Memorization. (arXiv:2310.08015v1 [cs.LG])

    [http://arxiv.org/abs/2310.08015](http://arxiv.org/abs/2310.08015)

    本文通过策略性选择样本，最大化攻击成功并最小化影子模型的数量，从而提出了一种通过记忆进行有效和高效的成员推断攻击的方法。

    

    成员推断攻击（MIAs）旨在识别机器学习模型私有训练数据集中的特定数据样本，从而造成严重的隐私侵犯和其他复杂的威胁。许多实际的黑盒MIAs需要对数据分布进行查询访问（与私有数据绘制的相同分布），以训练影子模型。通过这样做，攻击者获得在数据分布中使用或不使用样本训练的模型，并分析所考虑样本的特征。攻击者通常需要训练超过数百个影子模型来提取MIAs所需的信号，这成为MIAs的计算开销。

    Membership Inference Attacks (MIAs) aim to identify specific data samples within the private training dataset of machine learning models, leading to serious privacy violations and other sophisticated threats. Many practical black-box MIAs require query access to the data distribution (the same distribution where the private data is drawn) to train shadow models. By doing so, the adversary obtains models trained "with" or "without" samples drawn from the distribution, and analyzes the characteristics of the samples under consideration. The adversary is often required to train more than hundreds of shadow models to extract the signals needed for MIAs; this becomes the computational overhead of MIAs. In this paper, we propose that by strategically choosing the samples, MI adversaries can maximize their attack success while minimizing the number of shadow models. First, our motivational experiments suggest memorization as the key property explaining disparate sample vulnerability to MIAs. 
    
[^55]: AutoFHE: 用于FHE高效评估的CNN自动适应方法

    AutoFHE: Automated Adaption of CNNs for Efficient Evaluation over FHE. (arXiv:2310.08012v1 [cs.LG])

    [http://arxiv.org/abs/2310.08012](http://arxiv.org/abs/2310.08012)

    AutoFHE针对深度卷积神经网络的安全推理提出了一种自动方法，通过采用逐层混合次数多项式激活函数来解决现有方法的灵活性、次优逼近和限制性设计限制。

    

    在RNS-CKKS下，对于深度卷积神经网络（CNN）的安全推理，需要对不支持的非线性激活函数进行多项式逼近。然而，现有方法存在三个主要限制：1）不灵活：多项式逼近和相关的同态评估架构是针对每个CNN架构手动定制的，并且无法推广到其他网络。2）次优逼近：对于CNN表示的每个激活函数进行近似，而不是近似整个函数。3）限制性设计：使用高次或低次多项式逼近。前者保留了较高的准确性，但由于引导操作而减慢了推理速度，而后者加快了密文推理，但损害了准确性。为了解决这些限制，我们提出了AutoFHE，它可以自动适应RNS-CKKS下标准CNN进行安全推理。关键思想是采用逐层混合次数多项式激活函数。

    Secure inference of deep convolutional neural networks (CNNs) under RNS-CKKS involves polynomial approximation of unsupported non-linear activation functions. However, existing approaches have three main limitations: 1) Inflexibility: The polynomial approximation and associated homomorphic evaluation architecture are customized manually for each CNN architecture and do not generalize to other networks. 2) Suboptimal Approximation: Each activation function is approximated instead of the function represented by the CNN. 3) Restricted Design: Either high-degree or low-degree polynomial approximations are used. The former retains high accuracy but slows down inference due to bootstrapping operations, while the latter accelerates ciphertext inference but compromises accuracy. To address these limitations, we present AutoFHE, which automatically adapts standard CNNs for secure inference under RNS-CKKS. The key idea is to adopt layerwise mixed-degree polynomial activation functions, which are
    
[^56]: LEMON：无损模型扩展

    LEMON: Lossless model expansion. (arXiv:2310.07999v1 [cs.LG])

    [http://arxiv.org/abs/2310.07999](http://arxiv.org/abs/2310.07999)

    LEMON是一种无损模型扩展方法，在深度神经网络中能够通过利用小型预训练模型的知识来初始化和训练大型模型，从而大大减少训练时间，同时具有通用性适用于各种网络结构。

    

    深度神经网络（特别是Transformer）的扩展对于它们的出色性能至关重要，并且进一步导致了基础模型中复杂的推理能力的出现。这种扩展通常需要从头开始训练大型模型，并使用随机初始化，而无法利用已有的小型模型所获得的知识，这些小型模型已经耗费了大量资源。为了解决这种低效率问题，我们提出了无损模型扩展（LEMON），一种使用小型但已经预训练的模型的权重来初始化扩展模型的方法。然后，我们使用专门为扩展模型定制的优化学习率调度器进行模型训练，与从头训练相比，大大减少了训练时间。值得注意的是，LEMON具有通用性，能够与各种网络结构兼容，包括Vision Transformer和BERT等模型。我们的实证结果证明了LEMON的效果。

    Scaling of deep neural networks, especially Transformers, is pivotal for their surging performance and has further led to the emergence of sophisticated reasoning capabilities in foundation models. Such scaling generally requires training large models from scratch with random initialization, failing to leverage the knowledge acquired by their smaller counterparts, which are already resource-intensive to obtain. To tackle this inefficiency, we present $\textbf{L}$ossl$\textbf{E}$ss $\textbf{MO}$del Expansio$\textbf{N}$ (LEMON), a recipe to initialize scaled models using the weights of their smaller but pre-trained counterparts. This is followed by model training with an optimized learning rate scheduler tailored explicitly for the scaled models, substantially reducing the training time compared to training from scratch. Notably, LEMON is versatile, ensuring compatibility with various network structures, including models like Vision Transformers and BERT. Our empirical results demonstrat
    
[^57]: 重置并忘却：重新学习最后一层权重改善持续和迁移学习

    Reset It and Forget It: Relearning Last-Layer Weights Improves Continual and Transfer Learning. (arXiv:2310.07996v1 [cs.LG])

    [http://arxiv.org/abs/2310.07996](http://arxiv.org/abs/2310.07996)

    本研究发展了一种重置最后一层权重的方法，称为"zapping"，通过这种方法可以提供更好的持续和迁移学习效果，同时具备简单实施和高效计算的特点。

    

    本研究发现了一种简单的预训练机制，能够导致具有更好的持续和迁移学习表征。这种机制——在最后一层权重中反复重置，我们称之为“zapping”——最初设计用于元持续学习过程，但我们发现它在许多不同于元学习和持续学习的情况下也非常适用。在我们的实验中，我们希望将预训练的图像分类器迁移到一组新的类别，仅使用少量样本。我们展示了我们的zapping过程在标准微调和持续学习设置中能够获得更好的迁移准确性和/或更快的适应性，同时实现简单的实施和高效的计算。在许多情况下，通过使用zapping和顺序学习的组合，我们可以达到与最先进的元学习相当的性能而无需昂贵的高阶梯度。

    This work identifies a simple pre-training mechanism that leads to representations exhibiting better continual and transfer learning. This mechanism -- the repeated resetting of weights in the last layer, which we nickname "zapping" -- was originally designed for a meta-continual-learning procedure, yet we show it is surprisingly applicable in many settings beyond both meta-learning and continual learning. In our experiments, we wish to transfer a pre-trained image classifier to a new set of classes, in a few shots. We show that our zapping procedure results in improved transfer accuracy and/or more rapid adaptation in both standard fine-tuning and continual learning settings, while being simple to implement and computationally efficient. In many cases, we achieve performance on par with state of the art meta-learning without needing the expensive higher-order gradients, by using a combination of zapping and sequential learning. An intuitive explanation for the effectiveness of this za
    
[^58]: 多视图变分自动编码器在非目标代谢组学中缺失值填充中的应用

    Multi-View Variational Autoencoder for Missing Value Imputation in Untargeted Metabolomics. (arXiv:2310.07990v1 [q-bio.GN])

    [http://arxiv.org/abs/2310.07990](http://arxiv.org/abs/2310.07990)

    本文提出了一种新的方法，利用多视图变分自动编码器来填充非目标代谢组学中的缺失值，该方法利用了全基因组测序数据和参考代谢物的信息，可以有效地根据基因组信息填充缺失的代谢组学值。

    

    背景：在基于质谱的代谢组学中，缺失数据是一个常见的挑战，可能导致偏倚和不完整的分析。将全基因组测序（WGS）数据与代谢组学数据整合起来，已经成为增强代谢组学研究中数据填充准确性的一种有前景的方法。方法：在本研究中，我们提出了一种新的方法，利用来自WGS数据和参考代谢物的信息来填充未知代谢物。我们的方法利用多视图变分自动编码器共同对负担评分、多基因风险评分（PGS）和连锁不平衡（LD）删减的单核苷酸多态性（SNPs）进行特征提取和缺失代谢组学数据的填充。通过学习两种组学数据的潜在表示，我们的方法可以根据基因组信息有效地填充缺失的代谢组学值。结果：我们在具有缺失值和不完整数据的实验代谢组学数据集上评估了我们方法的性能。

    Background: Missing data is a common challenge in mass spectrometry-based metabolomics, which can lead to biased and incomplete analyses. The integration of whole-genome sequencing (WGS) data with metabolomics data has emerged as a promising approach to enhance the accuracy of data imputation in metabolomics studies. Method: In this study, we propose a novel method that leverages the information from WGS data and reference metabolites to impute unknown metabolites. Our approach utilizes a multi-view variational autoencoder to jointly model the burden score, polygenetic risk score (PGS), and linkage disequilibrium (LD) pruned single nucleotide polymorphisms (SNPs) for feature extraction and missing metabolomics data imputation. By learning the latent representations of both omics data, our method can effectively impute missing metabolomics values based on genomic information. Results: We evaluate the performance of our method on empirical metabolomics datasets with missing values and de
    
[^59]: 语义前向中继：面向6G合作通信的新框架

    Semantic-Forward Relaying: A Novel Framework Towards 6G Cooperative Communications. (arXiv:2310.07987v1 [cs.NI])

    [http://arxiv.org/abs/2310.07987](http://arxiv.org/abs/2310.07987)

    该论文提出了一种面向6G合作通信的新中继框架，通过提取传输语义特征减少转发负载，并提高网络的鲁棒性。设计了一种联合源信道编码算法，通过迭代交换外在信息来增强解码增益。仿真结果表明，即使在恶劣的信道条件下，该中继框架仍然可以有效地改善恢复的信息质量。

    

    本文提出了一种新的中继框架，语义前向（SF），用于面向第六代（6G）无线网络的合作通信。SF中继提取并传输语义特征，减少了转发负载，并提高了网络对内链路错误的鲁棒性。基于具有辅助信息的合作通信的理论基础和TURBO原理，我们设计了一种联合源信道编码算法，通过迭代交换外在信息来增强目的地的解码增益。令人惊讶的是，仿真结果表明，即使在恶劣的信道条件下，SF中继仍然可以有效地改善恢复的信息质量。

    This letter proposes a novel relaying framework, semantic-forward (SF), for cooperative communications towards the sixth-generation (6G) wireless networks. The SF relay extracts and transmits the semantic features, which reduces forwarding payload, and also improves the network robustness against intra-link errors. Based on the theoretical basis for cooperative communications with side information and the turbo principle, we design a joint source-channel coding algorithm to iteratively exchange the extrinsic information for enhancing the decoding gains at the destination. Surprisingly, simulation results indicate that even in bad channel conditions, SF relaying can still effectively improve the recovered information quality.
    
[^60]: 具有重编码器的神经组合优化：朝着大规模通用化迈进

    Neural Combinatorial Optimization with Heavy Decoder: Toward Large Scale Generalization. (arXiv:2310.07985v1 [cs.LG])

    [http://arxiv.org/abs/2310.07985](http://arxiv.org/abs/2310.07985)

    提出了一种具有强大通用性的轻编码器和重解码器（LEHD）模型，通过动态学习节点间关系，解决了神经组合优化在大规模实例上的问题，并应用于旅行商问题和容量限制车辆路径规划问题。

    

    神经组合优化（NCO）是一种有前景的基于学习的方法，可以通过传统的专家算法设计来解决具有挑战性的组合优化问题。然而，大多数构造性的NCO方法不能解决大规模实例大小的问题，这显著降低了它们在实际应用中的实用性。在这项工作中，我们提出了一种新颖的轻编码器和重解码器（LEHD）模型，具有强大的通用性，以解决这个关键问题。LEHD模型可以动态地学习到不同尺寸的所有可用节点之间的关系，这有利于模型对各种规模的问题进行泛化。此外，我们还为LEHD模型开发了一种数据高效的训练方案和灵活的解决方案构建机制。通过在小规模问题实例上进行训练，LEHD模型可以生成近乎最优的旅行商问题（TSP）和容量限制车辆路径规划问题

    Neural combinatorial optimization (NCO) is a promising learning-based approach for solving challenging combinatorial optimization problems without specialized algorithm design by experts. However, most constructive NCO methods cannot solve problems with large-scale instance sizes, which significantly diminishes their usefulness for real-world applications. In this work, we propose a novel Light Encoder and Heavy Decoder (LEHD) model with a strong generalization ability to address this critical issue. The LEHD model can learn to dynamically capture the relationships between all available nodes of varying sizes, which is beneficial for model generalization to problems of various scales. Moreover, we develop a data-efficient training scheme and a flexible solution construction mechanism for the proposed LEHD model. By training on small-scale problem instances, the LEHD model can generate nearly optimal solutions for the Travelling Salesman Problem (TSP) and the Capacitated Vehicle Routing
    
[^61]: RandCom：去中心化随机通信跳跃方法用于分布式随机优化

    RandCom: Random Communication Skipping Method for Decentralized Stochastic Optimization. (arXiv:2310.07983v1 [cs.LG])

    [http://arxiv.org/abs/2310.07983](http://arxiv.org/abs/2310.07983)

    RandCom是一种去中心化的随机通信跳跃方法，能够在分布式优化中通过概率性本地更新减少通信开销，并在不同的设置中实现线性加速。

    

    具有随机通信跳过的分布式优化方法因其在加速通信复杂性方面具有的优势而受到越来越多的关注。然而，现有的研究主要集中在强凸确定性设置的集中式通信协议上。在本研究中，我们提出了一种名为RandCom的分布式优化方法，它采用了概率性的本地更新。我们分析了RandCom在随机非凸、凸和强凸设置中的性能，并证明了它能够通过通信概率来渐近地减少通信开销。此外，我们证明当节点数量增加时，RandCom能够实现线性加速。在随机强凸设置中，我们进一步证明了RandCom可以通过独立于网络的步长实现线性加速。此外，我们将RandCom应用于联邦学习，并提供了关于实现线性加速的潜力的积极结果。

    Distributed optimization methods with random communication skips are gaining increasing attention due to their proven benefits in accelerating communication complexity. Nevertheless, existing research mainly focuses on centralized communication protocols for strongly convex deterministic settings. In this work, we provide a decentralized optimization method called RandCom, which incorporates probabilistic local updates. We analyze the performance of RandCom in stochastic non-convex, convex, and strongly convex settings and demonstrate its ability to asymptotically reduce communication overhead by the probability of communication. Additionally, we prove that RandCom achieves linear speedup as the number of nodes increases. In stochastic strongly convex settings, we further prove that RandCom can achieve linear speedup with network-independent stepsizes. Moreover, we apply RandCom to federated learning and provide positive results concerning the potential for achieving linear speedup and
    
[^62]: 用于玻璃流控制系统中显示转移机器人的强化学习：基于物理仿真的方法

    Reinforcement Learning of Display Transfer Robots in Glass Flow Control Systems: A Physical Simulation-Based Approach. (arXiv:2310.07981v1 [cs.LG])

    [http://arxiv.org/abs/2310.07981](http://arxiv.org/abs/2310.07981)

    该论文讨论了使用深度强化学习的方法解决流控制系统调度优化问题的可能性，并提出了一种基于物理仿真的方法来验证该方法的有效性。

    

    流控制系统是增加制造系统产能的关键概念。为了解决与流控制相关的调度优化问题，目前的方法依赖领域人工专家的启发式设计。因此，这些方法需要使用真实设备进行校正、监视和验证。随着系统设计的复杂性增加，监视时间增加，导致达到最佳设计的概率降低。作为启发式流控制系统设计的替代方法，考虑使用深度强化学习来解决调度优化问题。尽管现有的强化学习研究在某些领域的性能表现出色，但其结果在实际的显示和半导体制造过程等实际工厂自动化( FAB)中的适用性尚不明确。为此，我们提出了一种实现物理仿真的方法

    A flow control system is a critical concept for increasing the production capacity of manufacturing systems. To solve the scheduling optimization problem related to the flow control with the aim of improving productivity, existing methods depend on a heuristic design by domain human experts. Therefore, the methods require correction, monitoring, and verification by using real equipment. As system designs increase in complexity, the monitoring time increases, which decreases the probability of arriving at the optimal design. As an alternative approach to the heuristic design of flow control systems, the use of deep reinforcement learning to solve the scheduling optimization problem has been considered. Although the existing research on reinforcement learning has yielded excellent performance in some areas, the applicability of the results to actual FAB such as display and semiconductor manufacturing processes is not evident so far. To this end, we propose a method to implement a physica
    
[^63]: GRASP：通过图注意力加速最短路径攻击

    GRASP: Accelerating Shortest Path Attacks via Graph Attention. (arXiv:2310.07980v1 [cs.LG])

    [http://arxiv.org/abs/2310.07980](http://arxiv.org/abs/2310.07980)

    GRASP是一个通过使用图注意力网络识别子图来加速最短路径攻击的优化算法，能够运行速度快达到10倍而仍保持解决方案的质量。

    

    最近机器学习在辅助和加速传统组合优化算法方面展示出了潜力。以直接输出解决方案的端到端学习为目标的机器学习加速往往会在运行时间和解决质量之间做出权衡。因此，能够在保持性能保证的同时加速现有求解器的解决方案备受关注。我们考虑一个APX困难问题，其中对手通过删除最小数量的边来攻击图中的最短路径。我们提出了GRASP算法：图注意力加速最短路径攻击，这是一个通过ML辅助的优化算法，实现了最高10倍的运行时间加速，同时保持了生成的解决方案的质量。GRASP使用图注意力网络识别包含组合解决方案的更小子图，从而有效地减小了输入问题的大小。此外，我们展示了如何精确表示输入图的方法，

    Recent advances in machine learning (ML) have shown promise in aiding and accelerating classical combinatorial optimization algorithms. ML-based speed ups that aim to learn in an end to end manner (i.e., directly output the solution) tend to trade off run time with solution quality. Therefore, solutions that are able to accelerate existing solvers while maintaining their performance guarantees, are of great interest. We consider an APX-hard problem, where an adversary aims to attack shortest paths in a graph by removing the minimum number of edges. We propose the GRASP algorithm: Graph Attention Accelerated Shortest Path Attack, an ML aided optimization algorithm that achieves run times up to 10x faster, while maintaining the quality of solution generated. GRASP uses a graph attention network to identify a smaller subgraph containing the combinatorial solution, thus effectively reducing the input problem size. Additionally, we demonstrate how careful representation of the input graph, 
    
[^64]: 图形-SCP: 用图神经网络加速集合覆盖问题

    Graph-SCP: Accelerating Set Cover Problems with Graph Neural Networks. (arXiv:2310.07979v1 [cs.LG])

    [http://arxiv.org/abs/2310.07979](http://arxiv.org/abs/2310.07979)

    图形-SCP是一种使用图神经网络加速集合覆盖问题的方法，通过学习识别包含解空间的较小子问题来提高优化求解器的性能，实验结果表明，图形-SCP能够将问题大小减少30-70%，和商业求解器相比加速高达25倍，并且能够在给定的最优性阈值下改进或实现100%的最优性。

    

    机器学习方法越来越多地用于加速组合优化问题。我们特别关注集合覆盖问题（SCP），提出了一种名为图形-SCP的图神经网络方法，可以通过学习识别包含解空间的大大较小的子问题来增强现有的优化求解器。我们在具有不同问题特征和复杂度的合成加权和非加权SCP实例上评估了图形-SCP的性能，并在OR Library的实例上进行了评估，这是SCP的一个经典基准。我们展示了图形-SCP将问题大小减少了30-70%，并且相对于商业求解器（Gurobi）实现了高达25倍的运行时间加速。在给定所需的最优性阈值的情况下，图形-SCP将改进或甚至实现100%的最优性。这与快速贪婪解决方案形成了对比，后者在保证多项式运行时间的同时明显损害了解决方案的质量。图形-SCP可以推广到更大的问题规模。

    Machine learning (ML) approaches are increasingly being used to accelerate combinatorial optimization (CO) problems. We look specifically at the Set Cover Problem (SCP) and propose Graph-SCP, a graph neural network method that can augment existing optimization solvers by learning to identify a much smaller sub-problem that contains the solution space. We evaluate the performance of Graph-SCP on synthetic weighted and unweighted SCP instances with diverse problem characteristics and complexities, and on instances from the OR Library, a canonical benchmark for SCP. We show that Graph-SCP reduces the problem size by 30-70% and achieves run time speedups up to~25x when compared to commercial solvers (Gurobi). Given a desired optimality threshold, Graph-SCP will improve upon it or even achieve 100% optimality. This is in contrast to fast greedy solutions that significantly compromise solution quality to achieve guaranteed polynomial run time. Graph-SCP can generalize to larger problem sizes
    
[^65]: 可解释的扩散模型通过信息分解

    Interpretable Diffusion via Information Decomposition. (arXiv:2310.07972v1 [cs.LG])

    [http://arxiv.org/abs/2310.07972](http://arxiv.org/abs/2310.07972)

    本研究通过观察扩散和信息分解之间的关系，揭示了扩散模型学习到的细粒度关系，进一步解决了高维空间中信息携带变量的问题。

    

    去噪扩散模型能够用于复杂关系的条件生成和密度建模，如图像和文本。然而，学习到的关系的本质是不透明的，因此很难准确理解单词和图像部分之间的关系，或者预测干预的效果。我们通过观察扩散和信息分解之间的精确关系，揭示了扩散模型学习到的细粒度关系。互信息和条件互信息的精确表达可以通过去噪模型来计算。此外，也可以轻松估计在特定图像和标题之间的关系。进一步对信息进行分解，以理解高维空间中哪些变量携带信息，是一个长期存在的问题。对于扩散模型，我们展示了一种自然的非负信息分解方法。

    Denoising diffusion models enable conditional generation and density modeling of complex relationships like images and text. However, the nature of the learned relationships is opaque making it difficult to understand precisely what relationships between words and parts of an image are captured, or to predict the effect of an intervention. We illuminate the fine-grained relationships learned by diffusion models by noticing a precise relationship between diffusion and information decomposition. Exact expressions for mutual information and conditional mutual information can be written in terms of the denoising model. Furthermore, pointwise estimates can be easily estimated as well, allowing us to ask questions about the relationships between specific images and captions. Decomposing information even further to understand which variables in a high-dimensional space carry information is a long-standing problem. For diffusion models, we show that a natural non-negative decomposition of mutu
    
[^66]: 超参数自适应搜索用于代理优化：一种自调整方法

    Hyperparameter Adaptive Search for Surrogate Optimization: A Self-Adjusting Approach. (arXiv:2310.07970v1 [cs.LG])

    [http://arxiv.org/abs/2310.07970](http://arxiv.org/abs/2310.07970)

    这个论文研究了代理优化算法中超参数的影响，并提出了一种自适应搜索的代理优化方法（HASSO），该方法可以动态调整超参数而不需要额外的评估。该方法旨在提高代理优化算法的可访问性、效果和收敛速度。

    

    代理优化算法在优化昂贵的黑盒函数方面表现出了潜力。然而，它们的性能受与采样和代理拟合相关的超参数的影响很大，这对于它们的广泛应用构成挑战。我们研究了各种代理优化算法中超参数的影响，并提出了一种超参数自适应搜索的代理优化方法（HASSO）。HASSO不是一个超参数调整算法，而是一种通用的自调整代理优化算法，它在同时优化主要目标函数的过程中动态调整自己的超参数，而不需要额外的评估。其目标是提高代理优化算法对于从业者的可访问性、效果和收敛速度。我们的方法识别并修改了每个问题和代理优化方法特定的最有影响力的超参数，减少了手动调整的需求，同时不显著增加计算负担。实验结果演示了我们方法的有效性。

    Surrogate Optimization (SO) algorithms have shown promise for optimizing expensive black-box functions. However, their performance is heavily influenced by hyperparameters related to sampling and surrogate fitting, which poses a challenge to their widespread adoption. We investigate the impact of hyperparameters on various SO algorithms and propose a Hyperparameter Adaptive Search for SO (HASSO) approach. HASSO is not a hyperparameter tuning algorithm, but a generic self-adjusting SO algorithm that dynamically tunes its own hyperparameters while concurrently optimizing the primary objective function, without requiring additional evaluations. The aim is to improve the accessibility, effectiveness, and convergence speed of SO algorithms for practitioners. Our approach identifies and modifies the most influential hyperparameters specific to each problem and SO approach, reducing the need for manual tuning without significantly increasing the computational burden. Experimental results demo
    
[^67]: CleftGAN: 将基于风格的生成对抗网络适应于创建展示唇裂畸形的图像

    CleftGAN: Adapting A Style-Based Generative Adversarial Network To Create Images Depicting Cleft Lip Deformity. (arXiv:2310.07969v1 [cs.CV])

    [http://arxiv.org/abs/2310.07969](http://arxiv.org/abs/2310.07969)

    本研究提出了CleftGAN，即将基于风格的生成对抗网络适应于创建展示唇裂畸形的图像。通过使用少量训练图像，该模型可以产生高质量、多样化的唇裂图像，为面部唇裂评估系统的训练提供了解决方案。

    

    在训练面部唇裂评估机器学习系统时的一个主要障碍是缺乏大量高质量且经道德委员会批准的患者图像数据集。为了应对这个问题，我们构建了一个基于深度学习的唇裂生成器，旨在产生出具有高保真度的唇裂图像的几乎无限数量，且具有广泛的变化。我们进行了一项转移学习协议，测试了基于StyleGAN-ADA的不同版本（一种包含自适应数据增强(ADA)的生成对抗网络图像生成器）作为基础模型。通过对描绘不同唇裂畸形的训练图像进行预处理，调整了旋转、缩放、颜色调整和背景模糊。主要算法的ADA修改允许构建我们的新生成模型，同时需要输入相对较少的训练图像。通过使用514张独特的正脸唇裂照片进行对抗性训练，验证了生成模型的效果。

    A major obstacle when attempting to train a machine learning system to evaluate facial clefts is the scarcity of large datasets of high-quality, ethics board-approved patient images. In response, we have built a deep learning-based cleft lip generator designed to produce an almost unlimited number of artificial images exhibiting high-fidelity facsimiles of cleft lip with wide variation. We undertook a transfer learning protocol testing different versions of StyleGAN-ADA (a generative adversarial network image generator incorporating adaptive data augmentation (ADA)) as the base model. Training images depicting a variety of cleft deformities were pre-processed to adjust for rotation, scaling, color adjustment and background blurring. The ADA modification of the primary algorithm permitted construction of our new generative model while requiring input of a relatively small number of training images. Adversarial training was carried out using 514 unique frontal photographs of cleft-affect
    
[^68]: 迈向针对漏洞检测的因果深度学习

    Towards Causal Deep Learning for Vulnerability Detection. (arXiv:2310.07958v1 [cs.SE])

    [http://arxiv.org/abs/2310.07958](http://arxiv.org/abs/2310.07958)

    本文提出了一种针对漏洞检测的因果深度学习方法CausalVul，通过引入因果性，并设计新的扰动，解决了深度学习漏洞检测中模型不稳定和泛化性能差的问题。

    

    近年来，深度学习的漏洞检测取得了有希望的成果。然而，一个阻碍其在实践中非常有用的重要挑战是模型在扰动下不稳定，并且不能很好地泛化到超出分布（OOD）的数据，例如，在真实世界中将训练好的模型应用到未见过的项目上。我们假设这是因为模型学习到了非稳定的特征，例如变量名，与标签具有虚假相关性。当扰动和OOD数据集不再具有相同的虚假特征时，模型预测失败。为了解决这个挑战，在本文中，我们将因果性引入了深度学习漏洞检测中。我们的方法CausalVul分为两个阶段。首先，我们设计了新的扰动来发现模型可能用于进行预测的虚假特征。其次，我们在现有的深度学习模型之上应用了因果学习算法，特别是do-计算，来解决这个问题。

    Deep learning vulnerability detection has shown promising results in recent years. However, an important challenge that still blocks it from being very useful in practice is that the model is not robust under perturbation and it cannot generalize well over the out-of-distribution (OOD) data, e.g., applying a trained model to unseen projects in real world. We hypothesize that this is because the model learned non-robust features, e.g., variable names, that have spurious correlations with labels. When the perturbed and OOD datasets no longer have the same spurious features, the model prediction fails. To address the challenge, in this paper, we introduced causality into deep learning vulnerability detection. Our approach CausalVul consists of two phases. First, we designed novel perturbations to discover spurious features that the model may use to make predictions. Second, we applied the causal learning algorithms, specifically, do-calculus, on top of existing deep learning models to sys
    
[^69]: 成本驱动的机器学习流水线硬件软件协同优化

    Cost-Driven Hardware-Software Co-Optimization of Machine Learning Pipelines. (arXiv:2310.07940v1 [cs.LG])

    [http://arxiv.org/abs/2310.07940](http://arxiv.org/abs/2310.07940)

    该论文探索了在物联网设备中使用深度神经网络时的硬件软件协同优化，重点考虑了成本、延迟和用户体验，并研究了量化、模型缩放和多模态等方法与系统组件的相互作用。

    

    研究人员长期以来一直宣扬着由物联网设备（包括智能传感器，家居和城市）推动的未来愿景。越来越多地，将智能嵌入这些设备中涉及到深度神经网络的使用。然而，它们的存储和处理需求使它们对于廉价的现成平台来说是禁止的。克服这些要求对于实现广泛适用的智能设备至关重要。虽然已经开发出了许多使模型变得更小更高效的方法，但对于特定场景最适合的方法缺乏理解。更重要的是对于边缘平台，这些选择不能与成本和用户体验相割离地进行分析。在这项工作中，我们从成本、延迟和用户体验的角度全面探索了量化、模型缩放和多模态与存储、传感器和处理器等系统组件的相互作用。我们从硬件/软件协同设计的角度进行，考虑成本、延迟和用户体验的因素。

    Researchers have long touted a vision of the future enabled by a proliferation of internet-of-things devices, including smart sensors, homes, and cities. Increasingly, embedding intelligence in such devices involves the use of deep neural networks. However, their storage and processing requirements make them prohibitive for cheap, off-the-shelf platforms. Overcoming those requirements is necessary for enabling widely-applicable smart devices. While many ways of making models smaller and more efficient have been developed, there is a lack of understanding of which ones are best suited for particular scenarios. More importantly for edge platforms, those choices cannot be analyzed in isolation from cost and user experience. In this work, we holistically explore how quantization, model scaling, and multi-modality interact with system components such as memory, sensors, and processors. We perform this hardware/software co-design from the cost, latency, and user-experience perspective, and d
    
[^70]: D2修剪：信息传递平衡数据修剪中的多样性和困难

    D2 Pruning: Message Passing for Balancing Diversity and Difficulty in Data Pruning. (arXiv:2310.07931v1 [cs.LG])

    [http://arxiv.org/abs/2310.07931](http://arxiv.org/abs/2310.07931)

    D2修剪是一种平衡数据多样性和困难度的方法，在coreset选择中同时考虑数据多样性和重要性评分。

    

    分析理论表明，在固定数据预算上训练的模型中，更高质量的数据可以导致更低的测试错误。此外，如果数据集可以剥离冗余项，则可以在较低的计算预算上训练模型而不降低性能。Coreset选择（或数据修剪）寻求选择训练数据的子集，以最大程度地提高在该子集上训练的模型的性能，也称为coreset。有两种主要方法：（1）基于几何的数据选取，以最大程度地提高coreset中的数据多样性，和（2）根据训练动态为样本分配困难度分数的函数。为数据多样性进行优化会导致偏向较容易样本的coreset，而难度排名选择会忽略深度学习模型训练所必需的容易样本。这表明数据多样性和重要性评分是两个互补因素，在coreset选择中需要同时考虑。

    Analytical theories suggest that higher-quality data can lead to lower test errors in models trained on a fixed data budget. Moreover, a model can be trained on a lower compute budget without compromising performance if a dataset can be stripped of its redundancies. Coreset selection (or data pruning) seeks to select a subset of the training data so as to maximize the performance of models trained on this subset, also referred to as coreset. There are two dominant approaches: (1) geometry-based data selection for maximizing data diversity in the coreset, and (2) functions that assign difficulty scores to samples based on training dynamics. Optimizing for data diversity leads to a coreset that is biased towards easier samples, whereas, selection by difficulty ranking omits easy samples that are necessary for the training of deep learning models. This demonstrates that data diversity and importance scores are two complementary factors that need to be jointly considered during coreset sel
    
[^71]: 基于图表示学习变量的晶体成核增强抽样研究

    Enhanced sampling of Crystal Nucleation with Graph Representation Learnt Variables. (arXiv:2310.07927v1 [cond-mat.stat-mech])

    [http://arxiv.org/abs/2310.07927](http://arxiv.org/abs/2310.07927)

    本研究提出了一种基于图神经网络的学习方法，使用自编码器从实验晶体结构中观察到的特征中推导出低维变量，并在增强抽样中偏置这些变量以实现可靠的状态转换和热力学权重。该方法在研究铁和甘氨酸的晶体成核过程中取得了准确的自由能计算结果，展示了改进抽样的潜力。en_tdlr: This study presents a graph neural network-based learning approach to derive low-dimensional variables from experimental crystal structures and bias them in enhanced sampling for reliable state transitions and thermodynamic weights. The approach achieves accurate free energy calculations in the nucleation processes of iron and glycine crystals, demonstrating the potential for improved sampling accuracy.

    

    在本研究中，我们提出了一种基于图神经网络的学习方法，使用自编码器来从实验晶体结构中观察到的特征中推导出低维变量。然后，在增强抽样中对这些变量进行偏置，以观察状态转换和可靠的热力学权重。我们的方法使用简单的卷积和池化方法。为了验证我们的协议的有效性，我们研究了铁和甘氨酸的各种同素异形体和多型体从熔融状态到成核状态的转变。当我们的图潜变量在良温元动力学中进行偏置时，始终显示出状态之间的转变，并且实现了与实验一致的准确自由能计算，这些都是可靠抽样的指标。这凸显了我们的图神经网络变量在改进抽样方面的优势和潜力。这里展示的协议应该适用于其他系统和其他抽样方法。

    In this study, we present a graph neural network-based learning approach using an autoencoder setup to derive low-dimensional variables from features observed in experimental crystal structures. These variables are then biased in enhanced sampling to observe state-to-state transitions and reliable thermodynamic weights. Our approach uses simple convolution and pooling methods. To verify the effectiveness of our protocol, we examined the nucleation of various allotropes and polymorphs of iron and glycine from their molten states. Our graph latent variables when biased in well-tempered metadynamics consistently show transitions between states and achieve accurate free energy calculations in agreement with experiments, both of which are indicators of dependable sampling. This underscores the strength and promise of our graph neural net variables for improved sampling. The protocol shown here should be applicable for other systems and with other sampling methods.
    
[^72]: 针对流式凸成本的一阶动态优化

    First-Order Dynamic Optimization for Streaming Convex Costs. (arXiv:2310.07925v1 [math.OC])

    [http://arxiv.org/abs/2310.07925](http://arxiv.org/abs/2310.07925)

    本文提出了一种新型优化算法，用于解决具有时变流式成本函数的凸优化问题，通过使用一阶导数进行计算，实现了高效的优化。通过与梯度下降算法比较和几个示例的验证，证明了该算法的有效性。

    

    本文提出了一组用于解决一类带有时变流式成本函数的凸优化问题的新型优化算法。我们开发了一种方法来跟踪最优解，并保持有界误差。与现有结果不同，我们的算法仅使用成本函数的一阶导数执行，这使得它在处理时变成本函数的优化问题时具有计算效率。我们将我们的算法与梯度下降算法进行比较，并展示了为什么梯度下降算法对于具有时变成本的优化问题不是一个有效的解决方案。通过几个示例，包括使用流式时变成本函数建模预测控制问题的凸优化问题的解决，证明了我们的结果。

    This paper proposes a set of novel optimization algorithms for solving a class of convex optimization problems with time-varying streaming cost function. We develop an approach to track the optimal solution with a bounded error. Unlike the existing results, our algorithm is executed only by using the first-order derivatives of the cost function which makes it computationally efficient for optimization with time-varying cost function. We compare our algorithms to the gradient descent algorithm and show why gradient descent is not an effective solution for optimization problems with time-varying cost. Several examples including solving a model predictive control problem cast as a convex optimization problem with a streaming time-varying cost function demonstrate our results.
    
[^73]: 基于思维链的Transformer的表达能力

    The Expresssive Power of Transformers with Chain of Thought. (arXiv:2310.07923v1 [cs.LG])

    [http://arxiv.org/abs/2310.07923](http://arxiv.org/abs/2310.07923)

    本论文研究基于思维链的Transformer的表达能力，通过允许使用中间生成的方式提高了Transformer的推理能力，并发现线性数量的解码步骤在标准计算复杂度下增加了明显的新能力。

    

    最近的理论研究发现了一些出人意料地简单的推理问题，例如检查图中是否存在连接的两个节点，或模拟有限状态机，这些问题被证明无法由立即读取输入后回答的标准Transformer解决。然而，在实践中，通过允许Transformer使用“思维链”或“草稿纸”，即在回答之前生成并依赖一系列中间token，可以改善其推理能力。基于此，我们问：这种中间生成是否从根本上扩展了仅有解码器的Transformer的计算能力？我们表明答案是肯定的，但增加的程度关键取决于中间生成的数量。例如，我们发现相对于输入长度来说，具有对数级解码步骤的Transformer解码器仅略微推动了标准Transformer的极限，而线性数量的解码步骤则增加了明显的新能力（在标准计算复杂度下）。

    Recent theoretical work has identified surprisingly simple reasoning problems, such as checking if two nodes in a graph are connected or simulating finite-state machines, that are provably unsolvable by standard transformers that answer immediately after reading their input. However, in practice, transformers' reasoning can be improved by allowing them to use a "chain of thought" or "scratchpad", i.e., generate and condition on a sequence of intermediate tokens before answering. Motivated by this, we ask: Does such intermediate generation fundamentally extend the computational power of a decoder-only transformer? We show that the answer is yes, but the amount of increase depends crucially on the amount of intermediate generation. For instance, we find that transformer decoders with a logarithmic number of decoding steps (w.r.t. the input length) push the limits of standard transformers only slightly, while a linear number of decoding steps adds a clear new ability (under standard compl
    
[^74]: 上下文化政策恢复：通过自适应模仿学习对医疗决策进行建模和解释

    Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning. (arXiv:2310.07918v1 [cs.LG])

    [http://arxiv.org/abs/2310.07918](http://arxiv.org/abs/2310.07918)

    本论文提出了一种上下文化政策恢复方法用于建模复杂的医疗决策过程，以解决现有模型在准确性和可解释性之间的权衡问题。该方法将决策策略拆分为上下文特定策略，通过多任务学习来实现建模，并提供复杂行为的简洁描述。

    

    可解释的策略学习旨在从观察到的行为中估计可理解的决策策略；然而，现有模型在准确性和可解释性之间存在权衡。这种权衡限制了基于数据驱动的对人类决策过程的解释，例如，审计医疗决策的偏见和次优实践，我们需要决策过程的模型，能够提供复杂行为的简洁描述。现有方法基本上由于将潜在决策过程表示为通用策略而负担了这种权衡，而实际上人类决策是动态的，可以随上下文信息而大幅改变。因此，我们提出了上下文化政策恢复（CPR），将建模复杂决策过程的问题重新定义为多任务学习问题，其中复杂决策策略由特定上下文的策略组成。CPR将每个上下文特定策略建模为线性的观察-动作映射

    Interpretable policy learning seeks to estimate intelligible decision policies from observed actions; however, existing models fall short by forcing a tradeoff between accuracy and interpretability. This tradeoff limits data-driven interpretations of human decision-making process. e.g. to audit medical decisions for biases and suboptimal practices, we require models of decision processes which provide concise descriptions of complex behaviors. Fundamentally, existing approaches are burdened by this tradeoff because they represent the underlying decision process as a universal policy, when in fact human decisions are dynamic and can change drastically with contextual information. Thus, we propose Contextualized Policy Recovery (CPR), which re-frames the problem of modeling complex decision processes as a multi-task learning problem in which complex decision policies are comprised of context-specific policies. CPR models each context-specific policy as a linear observation-to-action mapp
    
[^75]: 在非平衡数据和未来趋势中的机器学习技术综述

    A Review of Machine Learning Techniques in Imbalanced Data and Future Trends. (arXiv:2310.07917v1 [cs.LG])

    [http://arxiv.org/abs/2310.07917](http://arxiv.org/abs/2310.07917)

    该论文综述了在非平衡数据中使用的各种机器学习方法，并提供了一个通用指南，旨在帮助研究人员在大规模非平衡数据中进行机器学习。

    

    在过去的二十年里，检测罕见事件一直是数据挖掘和机器学习领域的一个挑战性任务。现实生活中的问题激发了研究人员进一步改进数据处理和算法方法，以实现有效和计算效率高的非平衡学习方法。本论文收集和审查了258篇来自期刊和会议论文的同行评审论文，旨在从技术和应用角度深入审查非平衡学习中的各种方法。该工作旨在为在学术界或工业界希望深入学习大规模非平衡数据下的机器学习领域的研究人员提供一个结构化的方法综述，并为他们提供一个通用指南。

    For over two decades, detecting rare events has been a challenging task among researchers in the data mining and machine learning domain. Real-life problems inspire researchers to navigate and further improve data processing and algorithmic approaches to achieve effective and computationally efficient methods for imbalanced learning. In this paper, we have collected and reviewed 258 peer-reviewed papers from archival journals and conference papers in an attempt to provide an in-depth review of various approaches in imbalanced learning from technical and application perspectives. This work aims to provide a structured review of methods used to address the problem of imbalanced data in various domains and create a general guideline for researchers in academia or industry who want to dive into the broad field of machine learning using large-scale imbalanced data.
    
[^76]: 揭示单切平面误区：在机器人学习中应用黎曼几何的分析和澄清

    Unraveling the Single Tangent Space Fallacy: An Analysis and Clarification for Applying Riemannian Geometry in Robot Learning. (arXiv:2310.07902v1 [cs.RO])

    [http://arxiv.org/abs/2310.07902](http://arxiv.org/abs/2310.07902)

    "Unraveling the Single Tangent Space Fallacy"论文分析和澄清了在机器人学习中应用黎曼几何的误区，该误区是指将数据仅投影到单一切空间中的方法。

    

    在机器人领域，许多后续的机器人任务利用机器学习方法来处理、建模或合成数据。这些数据通常包含固体方向表示四元数的单位范数条件或刚度和可操纵性椭球的正定性等几何约束。有效处理这样的几何约束需要将微分几何工具纳入机器学习方法的制定中。在这个背景下，黎曼流形成为处理这种几何约束的强大数学框架。然而，最近在机器人学习中对其采用过程中存在的一个数学上的缺陷化简现象，被称为“单切平面误区”。这种方法仅涉及将感兴趣的数据投影到一个单一切空间（欧几里得空间）上，然后使用现成的学习算法

    In the realm of robotics, numerous downstream robotics tasks leverage machine learning methods for processing, modeling, or synthesizing data. Often, this data comprises variables that inherently carry geometric constraints, such as the unit-norm condition of quaternions representing rigid-body orientations or the positive definiteness of stiffness and manipulability ellipsoids. Handling such geometric constraints effectively requires the incorporation of tools from differential geometry into the formulation of machine learning methods. In this context, Riemannian manifolds emerge as a powerful mathematical framework to handle such geometric constraints. Nevertheless, their recent adoption in robot learning has been largely characterized by a mathematically-flawed simplification, hereinafter referred to as the ``single tangent space fallacy". This approach involves merely projecting the data of interest onto a single tangent (Euclidean) space, over which an off-the-shelf learning algor
    
[^77]: NoMaD: 目标遮蔽扩散策略用于导航与探索

    NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration. (arXiv:2310.07896v1 [cs.RO])

    [http://arxiv.org/abs/2310.07896](http://arxiv.org/abs/2310.07896)

    本文提出了NoMaD，一种目标遮蔽扩散策略，用于同时处理目标导向导航和目标不可知探索。实验证明，相对于使用子目标提议或潜变量模型的方法，使用此统一策略具有更好的导航性能。

    

    在不熟悉的环境中，机器人学习导航需要提供任务导向的导航策略（即到达机器人定位的目标）和任务不可知的探索策略（即在新的环境中搜索目标）。通常情况下，这些角色由不同的模型处理，例如使用子目标提议、规划或分离的导航策略。本文描述了如何训练一个统一的扩散策略来同时处理目标导向导航和目标不可知探索，后者提供了在新环境中搜索目标的能力，前者提供了一旦目标被定位后到达用户指定目标的能力。我们展示了相对于使用生成模型的子目标提议或基于潜变量模型的先前方法，这种统一策略在导航到视觉指示目标的新环境中具有更好的整体性能。我们通过使用...

    Robotic learning for navigation in unfamiliar environments needs to provide policies for both task-oriented navigation (i.e., reaching a goal that the robot has located), and task-agnostic exploration (i.e., searching for a goal in a novel setting). Typically, these roles are handled by separate models, for example by using subgoal proposals, planning, or separate navigation strategies. In this paper, we describe how we can train a single unified diffusion policy to handle both goal-directed navigation and goal-agnostic exploration, with the latter providing the ability to search novel environments, and the former providing the ability to reach a user-specified goal once it has been located. We show that this unified policy results in better overall performance when navigating to visually indicated goals in novel environments, as compared to approaches that use subgoal proposals from generative models, or prior methods based on latent variable models. We instantiate our method by using
    
[^78]: 通过结合CNN的分类和HMM的时间序列分析在GI道中进行精确定位的方法

    Precise localization within the GI tract by combining classification of CNNs and time-series analysis of HMMs. (arXiv:2310.07895v1 [cs.LG])

    [http://arxiv.org/abs/2310.07895](http://arxiv.org/abs/2310.07895)

    本文提出了一种通过使用CNN进行分类和HMM的时间序列分析，高效地将胃肠造影的图像进行分类，并通过连续的时间序列分析纠正CNN输出来实现精确定位。研究结果表明，该方法在Rhode Island胃肠病学数据集上达到了98.04％的准确率，可以在低功耗设备上使用。

    

    本文介绍了一种通过探索卷积神经网络（CNN）进行分类和隐马尔可夫模型（HMM）的时间序列分析属性的组合来高效地对视频胶囊内镜（VCE）研究中基于图像的胃肠学部分进行分类的方法。实验证明连续的时间序列分析可以识别和纠正CNN输出中的错误。我们的方法在Rhode Island（RI）胃肠病学数据集上实现了98.04％的准确率。这使得在胃肠道内实现精确定位成为可能，同时只需要约1M个参数，因此适用于低功耗设备。

    This paper presents a method to efficiently classify the gastroenterologic section of images derived from Video Capsule Endoscopy (VCE) studies by exploring the combination of a Convolutional Neural Network (CNN) for classification with the time-series analysis properties of a Hidden Markov Model (HMM). It is demonstrated that successive time-series analysis identifies and corrects errors in the CNN output. Our approach achieves an accuracy of $98.04\%$ on the Rhode Island (RI) Gastroenterology dataset. This allows for precise localization within the gastrointestinal (GI) tract while requiring only approximately 1M parameters and thus, provides a method suitable for low power devices
    
[^79]: 高效扩散生成模型的积分器

    Efficient Integrators for Diffusion Generative Models. (arXiv:2310.07894v1 [cs.LG])

    [http://arxiv.org/abs/2310.07894](http://arxiv.org/abs/2310.07894)

    本文提出了共轭积分器和分裂积分器两种框架，用于加速预训练模型中的样本生成。经过实证和理论研究，提出了一种最佳性能的混合方法，能够应用于扩散生成模型。

    

    扩散模型在推理时间生成样本速度较慢。因此，为更广泛的扩散模型开发快速确定性/随机采样的原则性框架是一个有希望的方向。我们提出了两种互补的加速预训练模型样本生成的框架：共轭积分器和分裂积分器。共轭积分器将DDIM泛化，将反向扩散动力学映射到更易于采样的空间。相反，基于分裂的积分器，常用于分子动力学，通过巧妙地在涉及数据和辅助变量的数值更新之间交替，减少了数值模拟误差。经过广泛的实证和理论研究，我们提出了一种混合方法，该方法在增强空间中获得了报告的扩散模型的最佳性能。我们在CIFAR-10上应用相空间朗之万扩散[Pandey＆Mandt，2023]，我们的确定性和随机样本生成获得了最好的性能。

    Diffusion models suffer from slow sample generation at inference time. Therefore, developing a principled framework for fast deterministic/stochastic sampling for a broader class of diffusion models is a promising direction. We propose two complementary frameworks for accelerating sample generation in pre-trained models: Conjugate Integrators and Splitting Integrators. Conjugate integrators generalize DDIM, mapping the reverse diffusion dynamics to a more amenable space for sampling. In contrast, splitting-based integrators, commonly used in molecular dynamics, reduce the numerical simulation error by cleverly alternating between numerical updates involving the data and auxiliary variables. After extensively studying these methods empirically and theoretically, we present a hybrid method that leads to the best-reported performance for diffusion models in augmented spaces. Applied to Phase Space Langevin Diffusion [Pandey & Mandt, 2023] on CIFAR-10, our deterministic and stochastic samp
    
[^80]: ASV通过神经网络仿真误差最小化模型预测控制实现在风扰动下的保持站位

    ASV Station Keeping under Wind Disturbances using Neural Network Simulation Error Minimization Model Predictive Control. (arXiv:2310.07892v1 [cs.RO])

    [http://arxiv.org/abs/2310.07892](http://arxiv.org/abs/2310.07892)

    本研究提出了一种通过神经网络仿真误差最小化模型预测控制实现在风扰动下ASV保持站位的方法，并在ROS和Gazebo仿真环境下进行了测试和比较。

    

    保持站位对于自主水面车辆（ASVs）来说是一项重要的操纵动作，特别是在狭小空间中使用，用于进行需要ASV保持位置或与其他车辆协作的调查任务。然而，由于对ASV动力学和环境干扰的精确建模的需求，这种操纵动作对于经典的反馈控制器而言可能变得具有挑战性。本研究提出了一种使用神经网络仿真误差最小化（NNSEM-MPC）的模型预测控制器，用于准确预测在风扰动下ASV的动力学。在机器人操作系统（ROS）和多用途仿真环境Gazebo上进行了对所提出方案在风扰动下的性能进行仿真测试，并与其他控制器进行了比较。通过组合两个风速（3 m/s和6 m/s）和三个风向（0°、90°和180°），进行了一系列六个测试。

    Station keeping is an essential maneuver for Autonomous Surface Vehicles (ASVs), mainly when used in confined spaces, to carry out surveys that require the ASV to keep its position or in collaboration with other vehicles where the relative position has an impact over the mission. However, this maneuver can become challenging for classic feedback controllers due to the need for an accurate model of the ASV dynamics and the environmental disturbances. This work proposes a Model Predictive Controller using Neural Network Simulation Error Minimization (NNSEM-MPC) to accurately predict the dynamics of the ASV under wind disturbances. The performance of the proposed scheme under wind disturbances is tested and compared against other controllers in simulation, using the Robotics Operating System (ROS) and the multipurpose simulation environment Gazebo. A set of six tests were conducted by combining two wind speeds (3 m/s and 6 m/s) and three wind directions (0$^\circ$, 90$^\circ$, and 180$^\c
    
[^81]: 两层神经网络中一次梯度下降的非线性特征学习理论

    A Theory of Non-Linear Feature Learning with One Gradient Step in Two-Layer Neural Networks. (arXiv:2310.07891v1 [stat.ML])

    [http://arxiv.org/abs/2310.07891](http://arxiv.org/abs/2310.07891)

    这篇论文提出了一种关于两层神经网络中非线性特征学习的理论。通过一步梯度下降训练的过程中引入不同的多项式特征，该方法能够学习到目标函数的非线性组件，而更新的神经网络的性能则由这些特征所决定。

    

    特征学习被认为是深度神经网络成功的基本原因之一。在特定条件下已经严格证明，在两层全连接神经网络中，第一层进行一步梯度下降，然后在第二层进行岭回归可以导致特征学习；特征矩阵的谱中会出现分离的一维组件，称为“spike”。然而，使用固定梯度下降步长时，这个“spike”仅提供了目标函数的线性组件的信息，因此学习非线性组件是不可能的。我们展示了当学习率随样本大小增长时，这样的训练实际上引入了多个一维组件，每个组件对应一个特定的多项式特征。我们进一步证明了更新的神经网络的极限大维度和大样本训练和测试误差完全由这些“spike”所决定。

    Feature learning is thought to be one of the fundamental reasons for the success of deep neural networks. It is rigorously known that in two-layer fully-connected neural networks under certain conditions, one step of gradient descent on the first layer followed by ridge regression on the second layer can lead to feature learning; characterized by the appearance of a separated rank-one component -- spike -- in the spectrum of the feature matrix. However, with a constant gradient descent step size, this spike only carries information from the linear component of the target function and therefore learning non-linear components is impossible. We show that with a learning rate that grows with the sample size, such training in fact introduces multiple rank-one components, each corresponding to a specific polynomial feature. We further prove that the limiting large-dimensional and large sample training and test errors of the updated neural networks are fully characterized by these spikes. By 
    
[^82]: 基于复杂群体的局部误差信号启发的领导者-跟随者神经网络

    Leader-Follower Neural Networks with Local Error Signals Inspired by Complex Collectives. (arXiv:2310.07885v1 [cs.LG])

    [http://arxiv.org/abs/2310.07885](http://arxiv.org/abs/2310.07885)

    领导者-跟随者神经网络是受到自然群体集合中观察到的规则启发的一种神经网络架构，利用局部误差信号训练并可选择性地结合反向传播和全局损失。通过大量实验研究工作人员行为和评估，这种神经网络具有高度的自组织和复杂性。

    

    网络中具有异质、资源有限的信息处理单元（例如，一群鱼、一群鸟或一组神经元网络）的集体行为表现出高度的自组织和复杂性。这些 emergent properties是通过简单的相互作用规则产生的，其中某些个体能够表现出领导行为并影响群体的活动。受到这些群体的复杂性的启发，我们提出了一种神经网络（NN）架构，该架构受到自然群体集合中观察到的规则的启发。这个NN结构包含了包含一个或多个信息处理单元（如神经元、滤波器、层或层块）的工作人员。工作人员可以是领导者或跟随者，我们通过利用局部误差信号训练了一个领导者-跟随者神经网络（LFNN），并选择性地结合反向传播（BP）和全局损失。我们通过大量的实验研究工作人员的行为并评估了LFNN。我们训练了使用局部误差信号的LFNNs，可选择性地将反向传播和全局损失结合在一起。

    The collective behavior of a network with heterogeneous, resource-limited information processing units (e.g., group of fish, flock of birds, or network of neurons) demonstrates high self-organization and complexity. These emergent properties arise from simple interaction rules where certain individuals can exhibit leadership-like behavior and influence the collective activity of the group. Motivated by the intricacy of these collectives, we propose a neural network (NN) architecture inspired by the rules observed in nature's collective ensembles. This NN structure contains workers that encompass one or more information processing units (e.g., neurons, filters, layers, or blocks of layers). Workers are either leaders or followers, and we train a leader-follower neural network (LFNN) by leveraging local error signals and optionally incorporating backpropagation (BP) and global loss. We investigate worker behavior and evaluate LFNNs through extensive experimentation. Our LFNNs trained wit
    
[^83]: 可解释人工智能在机器学习生命周期中的众多面貌：工业现实与当前研究状况

    The Thousand Faces of Explainable AI Along the Machine Learning Life Cycle: Industrial Reality and Current State of Research. (arXiv:2310.07882v1 [cs.LG])

    [http://arxiv.org/abs/2310.07882](http://arxiv.org/abs/2310.07882)

    本文调查了可解释人工智能（XAI）在生产行业中的实际相关性，并将其与当前学术界XAI研究的现状相联系。通过对广泛的访谈和文献回顾的分析，发现了访谈结果与当前的研究方法之间存在差异。

    

    本文研究了可解释人工智能（XAI）在生产行业中的实际相关性，并将其与当前学术界XAI研究的现状相联系。我们的发现基于对当前工业实践中XAI在机器学习（ML）生命周期中的角色和适用性以及未来的预期重要性进行了广泛的访谈。访谈对象包括来自不同行业部门的各种角色和关键利益相关者。此外，我们通过提供对相关文献的简明回顾，概述了XAI研究的现状。这使我们能够提供一个全面的概述，涵盖了被调查者的意见以及当前学术研究的现状。通过将我们的访谈结果与当前的研究方法进行比较，我们揭示了几个差异。尽管存在许多不同的XAI方法，但大多数方法都集中在...

    In this paper, we investigate the practical relevance of explainable artificial intelligence (XAI) with a special focus on the producing industries and relate them to the current state of academic XAI research. Our findings are based on an extensive series of interviews regarding the role and applicability of XAI along the Machine Learning (ML) lifecycle in current industrial practice and its expected relevance in the future. The interviews were conducted among a great variety of roles and key stakeholders from different industry sectors. On top of that, we outline the state of XAI research by providing a concise review of the relevant literature. This enables us to provide an encompassing overview covering the opinions of the surveyed persons as well as the current state of academic research. By comparing our interview results with the current research approaches we reveal several discrepancies. While a multitude of different XAI approaches exists, most of them are centered around the
    
[^84]: DeePref: 在内容交付网络中的视频预取中应用深度强化学习

    DeePref: Deep Reinforcement Learning For Video Prefetching In Content Delivery Networks. (arXiv:2310.07881v1 [cs.NI])

    [http://arxiv.org/abs/2310.07881](http://arxiv.org/abs/2310.07881)

    本论文介绍了DeePref，一种用于在内容交付网络中进行在线视频内容预取的深度强化学习代理。传统的预取技术难以适应工作负载中的变化，而DeePref利用强化学习算法，可以自动适应不断变化的用户访问模式。

    

    内容交付网络承载了大部分的互联网流量，对视频内容的需求不断增加使得缓存和预取优化算法的重要性日益突出。预取的目标是在请求者发出请求之前将数据提前放入缓存中，以减少访问时间并改善用户体验。传统的预取技术适应特定的访问模式，但很难适应工作负载中的突变或随机化。本文探讨了使用强化学习来应对用户访问模式的变化，并随时间自动适应的方法。为此，我们提出了DeePref，一种用于在内容交付网络中在线视频内容预取的深度强化学习代理。

    Content Delivery Networks carry the majority of Internet traffic, and the increasing demand for video content as a major IP traffic across the Internet highlights the importance of caching and prefetching optimization algorithms. Prefetching aims to make data available in the cache before the requester places its request to reduce access time and improve the Quality of Experience on the user side. Prefetching is well investigated in operating systems, compiler instructions, in-memory cache, local storage systems, high-speed networks, and cloud systems. Traditional prefetching techniques are well adapted to a particular access pattern, but fail to adapt to sudden variations or randomization in workloads. This paper explores the use of reinforcement learning to tackle the changes in user access patterns and automatically adapt over time. To this end, we propose, DeePref, a Deep Reinforcement Learning agent for online video content prefetching in Content Delivery Networks. DeePref is a pr
    
[^85]: TabLib：一个包含上亿表格和上百亿上下文的数据集

    TabLib: A Dataset of 627M Tables with Context. (arXiv:2310.07875v1 [cs.CL])

    [http://arxiv.org/abs/2310.07875](http://arxiv.org/abs/2310.07875)

    TabLib是一个包含上亿表格和上百亿上下文的数据集，规模和多样性使其在表格模态下具有巨大的潜力。

    

    巨大多样的数据集在提升现代AI系统在文本和图像模态下的性能方面起着关键作用。然而，对于表格数据，目前还没有与文本和图像可比拟的规模和多样性的数据集。因此，我们提出了"TabLib"，这是一个由6.27亿个表格和86.7亿个上下文令牌总共达到69 TiB的编译数据集。TabLib的数据来自多个文件格式，包括CSV、HTML、SQLite、PDF、Excel等，这些数据源自GitHub和Common Crawl。TabLib的规模和多样性在表格模态下具有巨大的潜力，如同原始的用于文本和图像的基础数据集，例如The Pile和LAION。

    It is well-established that large, diverse datasets play a pivotal role in the performance of modern AI systems for text and image modalities. However, there are no datasets for tabular data of comparable size and diversity to those available for text and images. Thus we present "TabLib'', a compilation of 627 million tables totaling 69 TiB, along with 867B tokens of context. TabLib was extracted from numerous file formats, including CSV, HTML, SQLite, PDF, Excel, and others, sourced from GitHub and Common Crawl. The size and diversity of TabLib offer considerable promise in the table modality, reminiscent of the original promise of foundational datasets for text and images, such as The Pile and LAION.
    
[^86]: 通过主动回归对近似结构的先验进行精细机制设计

    Refined Mechanism Design for Approximately Structured Priors via Active Regression. (arXiv:2310.07874v1 [cs.GT])

    [http://arxiv.org/abs/2310.07874](http://arxiv.org/abs/2310.07874)

    本研究提出了一种通过主动学习和机制设计相结合的方法，在拍卖中处理大量商品和策略招标人的问题。通过使用主题模型来近似投标人的先验分布，并设计出相应的机制，以提高机制的稳定性和适应性。

    

    我们考虑了一个有大量商品m出售给n个策略招标人的最大化收入卖方的问题，他们的估值是从高维未知的先验分布中独立抽取的。众所周知，这种情况下的最优甚至近似最优的机制很难表达或计算，而且即使找到了，通常也具有各种反直觉的特性。在本文中，根据Cai和Daskalakis最近提出的模型，我们考虑投标人的先验分布可以被一个主题模型很好地近似。我们设计了一个负责与投标人进行交互并输出其类型的低维近似的主动学习组件，以及一个负责为低维模型设计机制以适应前一组件的近似类型的机制设计组件。

    We consider the problem of a revenue-maximizing seller with a large number of items $m$ for sale to $n$ strategic bidders, whose valuations are drawn independently from high-dimensional, unknown prior distributions. It is well-known that optimal and even approximately-optimal mechanisms for this setting are notoriously difficult to characterize or compute, and, even when they can be found, are often rife with various counter-intuitive properties. In this paper, following a model introduced recently by Cai and Daskalakis~\cite{cai2022recommender}, we consider the case that bidders' prior distributions can be well-approximated by a topic model. We design an active learning component, responsible for interacting with the bidders and outputting low-dimensional approximations of their types, and a mechanism design component, responsible for robustifying mechanisms for the low-dimensional model to work for the approximate types of the former component. On the active learning front, we cast o
    
[^87]: QArchSearch:一种可扩展的量子架构搜索软件包

    QArchSearch: A Scalable Quantum Architecture Search Package. (arXiv:2310.07858v1 [quant-ph])

    [http://arxiv.org/abs/2310.07858](http://arxiv.org/abs/2310.07858)

    QArchSearch是一个基于人工智能的量子架构搜索软件包，通过使用QTensor库作为后端，它能够自动化地找到最佳的适用于特定任务和输入量子态的模型。它能够扩展到大型量子电路，并且能够探索不同的复杂模型，为不同的量子应用提供支持。

    

    当前的量子计算时代产生了几种算法，承诺高计算效率。虽然这些算法在理论上是可行的，并且可以提供潜在的指数级加速，但关于如何设计合适的量子电路来实现适当的幺正变换应用于输入的量子态却没有太多指导。在本文中，我们提出了一个基于人工智能的量子架构搜索软件包\texttt{QArchSearch}，其使用\texttt{QTensor}库作为后端，为给定任务和输入的量子态寻找最佳模型提供了一个有原则的自动化方法。我们展示了该搜索软件包能够将搜索有效扩展到大型量子电路，并且能够探索不同量子应用的更复杂模型。\texttt{QArchSearch}在高性能计算系统上通过CPU和GPU的两级并行化方案以规模和高效率运行，这已经在Polari上得到了证明。

    The current era of quantum computing has yielded several algorithms that promise high computational efficiency. While the algorithms are sound in theory and can provide potentially exponential speedup, there is little guidance on how to design proper quantum circuits to realize the appropriate unitary transformation to be applied to the input quantum state. In this paper, we present \texttt{QArchSearch}, an AI based quantum architecture search package with the \texttt{QTensor} library as a backend that provides a principled and automated approach to finding the best model given a task and input quantum state. We show that the search package is able to efficiently scale the search to large quantum circuits and enables the exploration of more complex models for different quantum applications. \texttt{QArchSearch} runs at scale and high efficiency on high-performance computing systems using a two-level parallelization scheme on both CPUs and GPUs, which has been demonstrated on the Polari
    
[^88]: CrIBo: 通过跨图像对象级引导进行自监督学习

    CrIBo: Self-Supervised Learning via Cross-Image Object-Level Bootstrapping. (arXiv:2310.07855v1 [cs.CV])

    [http://arxiv.org/abs/2310.07855](http://arxiv.org/abs/2310.07855)

    CrIBo通过跨图像对象级引导进行自监督学习，可以提高密集视觉表示学习的性能，并在自然理解应用中表现出领先的性能。

    

    在利用最近邻检索进行自监督表示学习时，以对象为中心的图像已被证明是有益的。然而，当应用于以场景为中心的数据集时，这种方法面临限制，在图像中的多个对象仅在全局表示中被隐含地捕获。这种全局引导可能导致对象表示的不可取的纠缠。此外，即使是以对象为中心的数据集，也可以受益于更细粒度的引导方法。为了应对这些挑战，我们引入了一种新的适用于增强密集视觉表示学习的跨图像对象级引导方法。通过在整个训练过程中采用对象级最近邻引导，CrIBo成为一个明显强大和适当的候选方案，用于上下文学习，在测试时利用最近邻检索。CrIBo在后一个任务上显示出最先进的性能，同时在更标准的下游自然理解应用中具有很高的竞争力。

    Leveraging nearest neighbor retrieval for self-supervised representation learning has proven beneficial with object-centric images. However, this approach faces limitations when applied to scene-centric datasets, where multiple objects within an image are only implicitly captured in the global representation. Such global bootstrapping can lead to undesirable entanglement of object representations. Furthermore, even object-centric datasets stand to benefit from a finer-grained bootstrapping approach. In response to these challenges, we introduce a novel Cross-Image Object-Level Bootstrapping method tailored to enhance dense visual representation learning. By employing object-level nearest neighbor bootstrapping throughout the training, CrIBo emerges as a notably strong and adequate candidate for in-context learning, leveraging nearest neighbor retrieval at test time. CrIBo shows state-of-the-art performance on the latter task while being highly competitive in more standard downstream se
    
[^89]: 关于通过指数机制进行高维私有模型选择的计算复杂性

    On the Computational Complexity of Private High-dimensional Model Selection via the Exponential Mechanism. (arXiv:2310.07852v1 [stat.ML])

    [http://arxiv.org/abs/2310.07852](http://arxiv.org/abs/2310.07852)

    本文研究了在高维稀疏线性回归模型中的差分隐私模型选择问题。我们使用指数机制进行模型选择，并提出了Metropolis-Hastings算法来克服指数搜索空间的计算复杂性。我们的算法在一定边界条件下能够实现强模型恢复性质，并具有多项式混合时间和近似差分隐私性质。

    

    在差分隐私框架下，我们考虑了高维稀疏线性回归模型中的模型选择问题。具体而言，我们考虑了差分隐私最佳子集选择的问题，并研究了其效用保证。我们采用了广为人知的指数机制来选择最佳模型，并在一定边界条件下，建立了其强模型恢复性质。然而，指数机制的指数搜索空间导致了严重的计算瓶颈。为了克服这个挑战，我们提出了Metropolis-Hastings算法来进行采样步骤，并在问题参数$n$、$p$和$s$中建立了其到稳态分布的多项式混合时间。此外，我们还利用其混合性质建立了Metropolis-Hastings随机行走的最终估计的近似差分隐私性质。最后，我们还进行了一些说明性模拟，印证了我们主要结果的理论发现。

    We consider the problem of model selection in a high-dimensional sparse linear regression model under the differential privacy framework. In particular, we consider the problem of differentially private best subset selection and study its utility guarantee. We adopt the well-known exponential mechanism for selecting the best model, and under a certain margin condition, we establish its strong model recovery property. However, the exponential search space of the exponential mechanism poses a serious computational bottleneck. To overcome this challenge, we propose a Metropolis-Hastings algorithm for the sampling step and establish its polynomial mixing time to its stationary distribution in the problem parameters $n,p$, and $s$. Furthermore, we also establish approximate differential privacy for the final estimates of the Metropolis-Hastings random walk using its mixing property. Finally, we also perform some illustrative simulations that echo the theoretical findings of our main results
    
[^90]: 探索有限领域知识传递的基本限制

    Towards the Fundamental Limits of Knowledge Transfer over Finite Domains. (arXiv:2310.07838v1 [cs.LG])

    [http://arxiv.org/abs/2310.07838](http://arxiv.org/abs/2310.07838)

    本论文研究了在有限领域中从教师到学生分类器进行知识传递的统计效率，发现特权信息会加速传递，通过使用一种新颖的损失函数达到了知识传递的基本限制。

    

    我们对通过从教师到概率化学生分类器的n个样本进行知识传递的统计效率进行了表征，其中输入空间S和标签A为有限域。我们发现，在三个渐进级别上的特权信息可以加快传递的速度。在第一级别上，只有具有困难标签的样本是已知的，最大似然估计器能够达到最小化速率sqrt(|S||A|/n)。第二级别上，除了已知的困难标签样本外，还有采样标签的教师概率可用，这将收敛速度的下界提高到|S||A|/n。然而，在第二个数据采集协议下，最小化交叉熵损失的朴素适应会导致渐近偏差的学生。我们克服了这个限制，并通过使用一种新颖的经验变体的平方误差逻辑损失来实现了基本限制。第三级别进一步赋予学生软标签。

    We characterize the statistical efficiency of knowledge transfer through $n$ samples from a teacher to a probabilistic student classifier with input space $\mathcal S$ over labels $\mathcal A$. We show that privileged information at three progressive levels accelerates the transfer. At the first level, only samples with hard labels are known, via which the maximum likelihood estimator attains the minimax rate $\sqrt{{|{\mathcal S}||{\mathcal A}|}/{n}}$. The second level has the teacher probabilities of sampled labels available in addition, which turns out to boost the convergence rate lower bound to ${{|{\mathcal S}||{\mathcal A}|}/{n}}$. However, under this second data acquisition protocol, minimizing a naive adaptation of the cross-entropy loss results in an asymptotically biased student. We overcome this limitation and achieve the fundamental limit by using a novel empirical variant of the squared error logit loss. The third level further equips the student with the soft labels (com
    
[^91]: 在语言模型中衡量特征稀疏性

    Measuring Feature Sparsity in Language Models. (arXiv:2310.07837v1 [cs.LG])

    [http://arxiv.org/abs/2310.07837](http://arxiv.org/abs/2310.07837)

    这项研究开发了度量方法来评估语言模型中特征稀疏性的成功，并测试了线性性和稀疏性假设的有效性。研究结果表明，语言模型的激活可以准确地建模为特征的稀疏线性组合，并且在第一层和最后一层中呈最稀疏状态。

    

    最近的研究提出，语言模型中的激活可以被建模为与输入文本特征对应的向量的稀疏线性组合。在这个假设下，这些研究旨在使用稀疏编码重构特征方向。我们开发了度量方法来评估这些稀疏编码技术的成功，并测试线性性和稀疏性假设的有效性。我们展示了我们的度量方法可以预测合成稀疏线性激活的稀疏程度，并能够区分稀疏线性数据和其他几种分布。我们使用我们的度量方法来衡量几个语言模型的稀疏程度。我们发现，与对照数据集相比，语言模型的激活可以准确地建模为特征的稀疏线性组合，并且在第一层和最后一层中呈最稀疏状态。

    Recent works have proposed that activations in language models can be modelled as sparse linear combinations of vectors corresponding to features of input text. Under this assumption, these works aimed to reconstruct feature directions using sparse coding. We develop metrics to assess the success of these sparse coding techniques and test the validity of the linearity and sparsity assumptions. We show our metrics can predict the level of sparsity on synthetic sparse linear activations, and can distinguish between sparse linear data and several other distributions. We use our metrics to measure levels of sparsity in several language models. We find evidence that language model activations can be accurately modelled by sparse linear combinations of features, significantly more so than control datasets. We also show that model activations appear to be sparsest in the first and final layers.
    
[^92]: 何时，为什么以及多少？通过细化进行的自适应学习率调度

    When, Why and How Much? Adaptive Learning Rate Scheduling by Refinement. (arXiv:2310.07831v1 [cs.LG])

    [http://arxiv.org/abs/2310.07831](http://arxiv.org/abs/2310.07831)

    该论文介绍了一种通过细化分析学习率调度来解决实践中学习率调整与理论的不一致的方法，通过对观察到的梯度范数进行分析，得到了适应于特定任务的细化调度。该方法能够改善优化算法的收敛性能。

    

    实践中使用的学习率调度与理论推荐的几乎完全不同。我们缩小了大部分理论与实践之间的差距，并因此能够推导出新的问题自适应学习率调度。我们的关键技术贡献是对广泛类别的优化算法（包括SGD）的学习率调度进行细化分析。与大多数前期研究只研究平均迭代的收敛性不同，我们研究最后一次迭代，这是大多数人在实践中使用的。当仅考虑最坏情况分析时，我们的理论预测最佳选择是线性衰减调度：这是一种实践中常用的选择，其将步长与当前迭代次数t和总步数T成比例地设置为1 - t/T。为了超越这种最坏情况分析，我们使用观察到的梯度范数来推导适应于特定任务的细化调度。这些细化调度表现出学习率逐渐增加和学习率迅速退火。

    Learning rate schedules used in practice bear little resemblance to those recommended by theory. We close much of this theory/practice gap, and as a consequence are able to derive new problem-adaptive learning rate schedules. Our key technical contribution is a refined analysis of learning rate schedules for a wide class of optimization algorithms (including SGD). In contrast to most prior works that study the convergence of the average iterate, we study the last iterate, which is what most people use in practice. When considering only worst-case analysis, our theory predicts that the best choice is the linear decay schedule: a popular choice in practice that sets the stepsize proportionally to $1 - t/T$, where $t$ is the current iteration and $T$ is the total number of steps. To go beyond this worst-case analysis, we use the observed gradient norms to derive schedules refined for any particular task. These refined schedules exhibit learning rate warm-up and rapid learning rate anneali
    
[^93]: 合成数据是否能使大型语言模型更高效？

    Does Synthetic Data Make Large Language Models More Efficient?. (arXiv:2310.07830v1 [cs.CL])

    [http://arxiv.org/abs/2310.07830](http://arxiv.org/abs/2310.07830)

    本文探讨了在NLP中合成数据生成的细微差别，重点关注基于模板的问题生成。通过评估其优势和固有限制，本研究揭示了合成数据对现代Transformer模型性能的影响，并强调了合成数据与真实世界数据之间所需的微妙平衡。

    

    随着深度学习方法的出现，自然语言处理(NLP)经历了深刻的变革。研究人员持续面临的一个挑战是驱动这些模型的高质量标注数据集的稀缺。本文探讨了在NLP中合成数据生成的细微差别，重点关注基于模板的问题生成。通过评估其优势，包括数据扩充潜力和结构多样性的引入，我们将这些优点与固有限制进行了对比，如过拟合风险和预定义模板所带来的限制。通过经验评估，我们展示了基于模板的合成数据对现代Transformer模型性能的影响。我们通过强调合成数据和真实世界数据之间所需的微妙平衡及将合成数据整合到模型训练流程中的未来轨迹来总结。这些发现旨在指导NLP从业者。

    Natural Language Processing (NLP) has undergone transformative changes with the advent of deep learning methodologies. One challenge persistently confronting researchers is the scarcity of high-quality, annotated datasets that drive these models. This paper explores the nuances of synthetic data generation in NLP, with a focal point on template-based question generation. By assessing its advantages, including data augmentation potential and the introduction of structured variety, we juxtapose these benefits against inherent limitations, such as the risk of overfitting and the constraints posed by pre-defined templates. Drawing from empirical evaluations, we demonstrate the impact of template-based synthetic data on the performance of modern transformer models. We conclude by emphasizing the delicate balance required between synthetic and real-world data, and the future trajectories of integrating synthetic data in model training pipelines. The findings aim to guide NLP practitioners in
    
[^94]: 大型语言模型能够进行零-shot时间序列预测

    Large Language Models Are Zero-Shot Time Series Forecasters. (arXiv:2310.07820v1 [cs.LG])

    [http://arxiv.org/abs/2310.07820](http://arxiv.org/abs/2310.07820)

    大型语言模型（LLMs）如GPT-3和LLaMA-2能够令人惊讶地零-shot外推时间序列，其性能可与或超过专门设计的时间序列模型的性能相媲美。这是因为LLMs具有自然地表示多模态分布的能力，并且具有与许多时间序列的重复季节趋势特征相一致的简单性和重复性偏好。

    

    通过将时间序列编码为一系列数字，我们可以将时间序列预测视为文本中的下一个标记预测。在开发这种方法时，我们发现大型语言模型（LLMs）例如GPT-3和LLaMA-2可以令人惊讶地零-shot外推时间序列，其性能可与或超过针对下游任务训练的专门设计的时间序列模型的性能相媲美。为了促进这种性能，我们提出了有效标记化时间序列数据并将离散分布转换为高度灵活的连续值密度的方法。我们认为，LLMs在时间序列中的成功源于它们自然地表示多模态分布的能力，结合了简单性和重复性的偏见，这与许多时间序列的重复季节趋势等显著特征相一致。我们还展示了LLMs如何通过非数字文本处理缺失数据，以及如何适应文本附加信息。

    By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text. Developing this approach, we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zero-shot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on the downstream tasks. To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values. We argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trends. We also show how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side in
    
[^95]: 可度量忠实性的掩码语言模型

    Faithfulness Measurable Masked Language Models. (arXiv:2310.07819v1 [cs.CL])

    [http://arxiv.org/abs/2310.07819](http://arxiv.org/abs/2310.07819)

    本论文提出了一种可度量忠实性的掩码语言模型，通过使用一种新颖的微调方法，将屏蔽令牌作为设计使其成为分布内，以解决解释自然语言处理模型时常见的问题。

    

    解释自然语言处理模型的常见方法是使用重要性度量来表达哪些令牌对于预测很重要。然而，尽管这些解释具有说服力，但往往是错误的。因此，测量它们的忠实性至关重要。其中一种度量标准是如果令牌确实很重要，那么屏蔽它们应该导致模型性能变差。然而，令牌屏蔽会引入区域外问题，而现有的解决方案在计算上很昂贵并且使用代理模型。此外，其他指标的适用范围非常有限。在这项工作中，我们提出了一种固有的忠实性可度量模型来应对这些挑战。通过使用一种新颖的微调方法来实现这一目标，该方法将屏蔽令牌作为设计使其成为分布内。这与现有方法不同，现有方法完全与模型无关，但在实践中不适用。我们通过将其应用于各种任务和数据集来证明我们方法的普适性。

    A common approach to explain NLP models, is to use importance measures that express which tokens are important for a prediction. Unfortunately, such explanations are often wrong despite being persuasive. Therefore, it is essential to measure their faithfulness. One such metric is if tokens are truly important, then masking them should result in worse model performance. However, token masking introduces out-of-distribution issues and existing solutions are computationally expensive and employ proxy-models. Furthermore, other metrics are very limited in scope. In this work, we propose an inherently faithfulness measurable model that addresses these challenges. This is achieved by using a novel fine-tuning method that incorporates masking, such that masking tokens become in-distribution by design. This differs from existing approaches, which are completely model-agnostic but are inapplicable in practice. We demonstrate the generality of our approach by applying it to various tasks and val
    
[^96]: 语言模型作为语义索引器

    Language Models As Semantic Indexers. (arXiv:2310.07815v1 [cs.IR])

    [http://arxiv.org/abs/2310.07815](http://arxiv.org/abs/2310.07815)

    本文介绍了一种使用生成性语言模型学习语义ID的自监督框架LMINDEXER。

    

    语义标识符（ID）是信息检索中的一个重要概念，旨在保留对象（如文档和项）内部的语义。先前的研究通常采用两阶段流程来学习语义ID，首先使用现成的文本编码器获取嵌入，并根据嵌入来推导ID。然而，每个步骤都会引入潜在的信息损失，并且文本编码器生成的潜在空间内的嵌入分布通常与语义索引所需的预期分布存在固有的不匹配。然而，设计一个既能学习文档的语义表示又能同时学习其分层结构的方法并不容易，因为语义ID是离散和顺序结构的，并且语义监督是不充分的。在本文中，我们引入了LMINDEXER，它是一个自监督框架，用于使用生成性语言模型学习语义ID。

    Semantic identifier (ID) is an important concept in information retrieval that aims to preserve the semantics of objects such as documents and items inside their IDs. Previous studies typically adopt a two-stage pipeline to learn semantic IDs by first procuring embeddings using off-the-shelf text encoders and then deriving IDs based on the embeddings. However, each step introduces potential information loss and there is usually an inherent mismatch between the distribution of embeddings within the latent space produced by text encoders and the anticipated distribution required for semantic indexing. Nevertheless, it is non-trivial to design a method that can learn the document's semantic representations and its hierarchical structure simultaneously, given that semantic IDs are discrete and sequentially structured, and the semantic supervision is deficient. In this paper, we introduce LMINDEXER, a self-supervised framework to learn semantic IDs with a generative language model. We tackl
    
[^97]: 从非结构化生成模型中可探索的网格变形子空间

    Explorable Mesh Deformation Subspaces from Unstructured Generative Models. (arXiv:2310.07814v1 [cs.GR])

    [http://arxiv.org/abs/2310.07814](http://arxiv.org/abs/2310.07814)

    本文介绍了一种方法，通过构建从易于导航的2D探索空间到预训练生成模型的子空间的映射，来探索一组标志性形状的变化。

    

    在传统的3D建模工具中，探索3D形状的变化是一个耗时的过程。3D形状的深度生成模型通常具有连续的潜在空间，可以用来从一组输入形状开始探索潜在的变化。然而，在实践中，这样做可能存在问题：潜在空间是高维的，很难可视化，包含与输入形状无关的形状，并且通过线性路径进行探索往往会导致次优的形状过渡。此外，理想情况下，我们希望能够在用于训练生成模型的原始高质量网格中探索变化，而不是其较低质量的输出几何形状。在本文中，我们提出了一种方法，通过构建从易于导航的2D探索空间到预训练生成模型的子空间的映射，来探索给定一组标志性形状的变化。我们首先描述如何找到覆盖输入标志性形状并呈现平滑变化的映射。

    Exploring variations of 3D shapes is a time-consuming process in traditional 3D modeling tools. Deep generative models of 3D shapes often feature continuous latent spaces that can, in principle, be used to explore potential variations starting from a set of input shapes. In practice, doing so can be problematic: latent spaces are high dimensional and hard to visualize, contain shapes that are not relevant to the input shapes, and linear paths through them often lead to sub-optimal shape transitions. Furthermore, one would ideally be able to explore variations in the original high-quality meshes used to train the generative model, not its lower-quality output geometry. In this paper, we present a method to explore variations among a given set of landmark shapes by constructing a mapping from an easily-navigable 2D exploration space to a subspace of a pre-trained generative model. We first describe how to find a mapping that spans the set of input landmark shapes and exhibits smooth vari
    
[^98]: 在线RL在线性$q^\pi$可实现的MDPs中和线性MDPs一样容易，只要你学会忽略。 (arXiv:2310.07811v1 [cs.LG])

    Online RL in Linearly $q^\pi$-Realizable MDPs Is as Easy as in Linear MDPs If You Learn What to Ignore. (arXiv:2310.07811v1 [cs.LG])

    [http://arxiv.org/abs/2310.07811](http://arxiv.org/abs/2310.07811)

    该论文研究了在线强化学习中在线性$q^\pi$可实现的MDPs和线性MDPs的差异，并提出了一种新颖的学习算法，可以通过学习忽略某些状态将问题转化为线性MDP。

    

    我们考虑在线强化学习（RL）在离散的马尔可夫决策过程（MDPs）中，在线性$q^\pi$可实现的假设下，假设所有策略的动作值可以表示为状态-动作特征的线性函数。这个类别被认为比线性MDPs更一般化，其中转移内核和奖励函数被假设为特征向量的线性函数。我们的第一个贡献是展示了这两个类别之间的差异是在线性$q^\pi$可实现的MDPs中存在一些状态，在这些状态中，对于任何策略，所有的动作值都近似相等，通过跳过这些状态并按照任意固定策略在这些状态中进行转换，我们可以将问题转化为线性MDP。基于这个观察，我们推导了一种新颖（计算效率较低）的学习算法，用于在线性$q^\pi$可实现的MDPs中，该算法同时学习了应该跳过的状态，并运行另一个学习算法。

    We consider online reinforcement learning (RL) in episodic Markov decision processes (MDPs) under the linear $q^\pi$-realizability assumption, where it is assumed that the action-values of all policies can be expressed as linear functions of state-action features. This class is known to be more general than linear MDPs, where the transition kernel and the reward function are assumed to be linear functions of the feature vectors. As our first contribution, we show that the difference between the two classes is the presence of states in linearly $q^\pi$-realizable MDPs where for any policy, all the actions have approximately equal values, and skipping over these states by following an arbitrarily fixed policy in those states transforms the problem to a linear MDP. Based on this observation, we derive a novel (computationally inefficient) learning algorithm for linearly $q^\pi$-realizable MDPs that simultaneously learns what states should be skipped over and runs another learning algorith
    
[^99]: FedSym: 发挥熵的力量对联邦学习算法进行基准测试

    FedSym: Unleashing the Power of Entropy for Benchmarking the Algorithms for Federated Learning. (arXiv:2310.07807v1 [cs.LG])

    [http://arxiv.org/abs/2310.07807](http://arxiv.org/abs/2310.07807)

    本文提出了一种利用熵和对称性构建数据分布的方法，用于基于联邦学习的算法基准测试。该方法解决了数据分区技术中存在的精度不足和无法增量挑战算法的问题。

    

    联邦学习是一种去中心化的机器学习方法，独立学习者会私下处理数据，通过聚合和重新训练本地模型来创建稳健准确的模型。然而，联邦学习面临着数据异质性和模型聚合效果的挑战。为了模拟真实世界的数据，研究人员使用数据分区方法，将为集中式学习设计的数据集转化为适用于具有不同数据异质性的分布式机器学习的一组子数据集。本文研究了目前流行的数据分区技术，并可视化其主要缺点：数据多样性的精度不足，导致不可靠的异质性指标，以及无法增量挑战联邦学习算法。为了解决这个问题，我们提出了一种利用熵和对称性构建“最具挑战性”和可控数据分布的方法。

    Federated learning (FL) is a decentralized machine learning approach where independent learners process data privately. Its goal is to create a robust and accurate model by aggregating and retraining local models over multiple rounds. However, FL faces challenges regarding data heterogeneity and model aggregation effectiveness. In order to simulate real-world data, researchers use methods for data partitioning that transform a dataset designated for centralized learning into a group of sub-datasets suitable for distributed machine learning with different data heterogeneity. In this paper, we study the currently popular data partitioning techniques and visualize their main disadvantages: the lack of precision in the data diversity, which leads to unreliable heterogeneity indexes, and the inability to incrementally challenge the FL algorithms. To resolve this problem, we propose a method that leverages entropy and symmetry to construct 'the most challenging' and controllable data distrib
    
[^100]: 具有相位随机桥的生成建模

    Generative Modeling with Phase Stochastic Bridges. (arXiv:2310.07805v1 [cs.LG])

    [http://arxiv.org/abs/2310.07805](http://arxiv.org/abs/2310.07805)

    通过在相位空间中构建路径测度，我们提出了一种新颖的生成建模框架，可以在动力传播的早期阶段生成逼真的数据点，并利用额外的速度信息实现高效的数据生成。

    

    扩散模型（DMs）是用于连续输入的最先进的生成模型。DMs通过在输入空间（即位置空间）中构建随机微分方程（SDE），并使用神经网络进行反演来工作。在这项工作中，我们介绍了一种基于相位空间动力学的新型生成建模框架，其中相位空间被定义为一个包括位置和速度的增强空间。利用随机最优控制的洞察力，我们构建了相位空间中的路径测度，实现了高效的采样。与DMs相比，我们的框架在动力传播的早期阶段就能够生成逼真的数据点。这种早期预测为通过沿轨迹利用额外的速度信息实现高效的数据生成奠定了基础。在标准图像生成基准测试中，我们的模型在小函数评估数量的范围内表现出优秀的性能。

    Diffusion models (DMs) represent state-of-the-art generative models for continuous inputs. DMs work by constructing a Stochastic Differential Equation (SDE) in the input space (ie, position space), and using a neural network to reverse it. In this work, we introduce a novel generative modeling framework grounded in \textbf{phase space dynamics}, where a phase space is defined as {an augmented space encompassing both position and velocity.} Leveraging insights from Stochastic Optimal Control, we construct a path measure in the phase space that enables efficient sampling. {In contrast to DMs, our framework demonstrates the capability to generate realistic data points at an early stage of dynamics propagation.} This early prediction sets the stage for efficient data generation by leveraging additional velocity information along the trajectory. On standard image generation benchmarks, our model yields favorable performance over baselines in the regime of small Number of Function Evaluation
    
[^101]: 解释性注意力用于少样本学习及其它领域

    Explainable Attention for Few-shot Learning and Beyond. (arXiv:2310.07800v1 [cs.AI])

    [http://arxiv.org/abs/2310.07800](http://arxiv.org/abs/2310.07800)

    本研究介绍了一种用于少样本学习的解释性注意力框架，旨在通过识别输入数据的关键部分来提高模型性能。

    

    注意力机制在增强学习模型中显示出了有希望的潜力，它可以通过识别输入数据中显著的部分来提高模型的性能。这在数据收集和标记方面存在挑战，导致训练样本有限的情况下尤其有价值。受人类认知过程启发，我们认为，如果将AI基线暴露于原始数据的关键部分而不是整个输入数据集，类似于人类的感知，那么它的性能可能会更准确、更可靠。然而，选择这些信息性数据部分的任务，即硬注意力寻找，是一个巨大的挑战。在少量训练样本的情况下，现有的研究很难找到这些信息性区域，原因是大量的训练参数无法从有限的样本中有效学习。在本研究中，我们介绍了一种实用的框架，用于实现可解释的硬注意力寻找。

    Attention mechanisms have exhibited promising potential in enhancing learning models by identifying salient portions of input data. This is particularly valuable in scenarios where limited training samples are accessible due to challenges in data collection and labeling. Drawing inspiration from human recognition processes, we posit that an AI baseline's performance could be more accurate and dependable if it is exposed to essential segments of raw data rather than the entire input dataset, akin to human perception. However, the task of selecting these informative data segments, referred to as hard attention finding, presents a formidable challenge. In situations with few training samples, existing studies struggle to locate such informative regions due to the large number of training parameters that cannot be effectively learned from the available limited samples. In this study, we introduce a novel and practical framework for achieving explainable hard attention finding, specifically
    
[^102]: 基于迁移学习的EMR数据集之间数据分布变化的预后预测模型

    A Transfer-Learning-Based Prognosis Prediction Paradigm that Bridges Data Distribution Shift across EMR Datasets. (arXiv:2310.07799v1 [cs.LG])

    [http://arxiv.org/abs/2310.07799](http://arxiv.org/abs/2310.07799)

    本研究提出了一种基于迁移学习的EMR数据集之间数据分布变化的预后预测模型，通过构建过渡模型解决了不同数据集的特征不匹配问题，提高了深度学习模型的效率。

    

    由于对新兴疾病的信息有限，症状很难被察觉和认识到，因此可能忽视临床干预的窗口。期望能够建立一个有效的预后模型，辅助医生进行正确诊断和制定个性化治疗方案，从而及时预防不利结果。然而，在疾病早期阶段，由于数据收集和临床经验有限，再加上对隐私和伦理的考虑，导致可供参考的数据受限，甚至难以正确标记数据标签。此外，不同疾病或同一疾病不同来源的电子医疗记录（EMR）数据可能存在严重的跨数据集特征不匹配问题，严重影响深度学习模型的效率。本文介绍了一种迁移学习方法，建立一个从源数据集到目标数据集的过渡模型，通过对特征进行约束，来解决数据分布变化的问题。

    Due to the limited information about emerging diseases, symptoms are hard to be noticed and recognized, so that the window for clinical intervention could be ignored. An effective prognostic model is expected to assist doctors in making right diagnosis and designing personalized treatment plan, so to promptly prevent unfavorable outcomes. However, in the early stage of a disease, limited data collection and clinical experiences, plus the concern out of privacy and ethics, may result in restricted data availability for reference, to the extent that even data labels are difficult to mark correctly. In addition, Electronic Medical Record (EMR) data of different diseases or of different sources of the same disease can prove to be having serious cross-dataset feature misalignment problems, greatly mutilating the efficiency of deep learning models. This article introduces a transfer learning method to build a transition model from source dataset to target dataset. By way of constraining the 
    
[^103]: GenTKG: 基于生成模型的时间知识图谱预测

    GenTKG: Generative Forecasting on Temporal Knowledge Graph. (arXiv:2310.07793v1 [cs.CL])

    [http://arxiv.org/abs/2310.07793](http://arxiv.org/abs/2310.07793)

    研究提出了一种名为GenTKG的生成模型，用于在时间知识图谱上进行预测。该模型通过结合基于时间逻辑规则的检索策略和轻量级的参数效率指导，克服了复杂的时间图数据结构和庞大的数据量所带来的挑战。

    

    大规模语言模型(LLM)的快速发展引发了对时间知识图谱(tKG)领域的兴趣，其中传统的基于嵌入和规则的模型占主导地位。目前仍然存在一个问题，即预训练的LLM是否能够理解结构化的时间关系数据，并取代它们成为时间关系预测的基础模型。因此，我们将时间知识预测引入生成模式。然而，在复杂的时间图数据结构和LLM可以处理的序列自然表达之间存在巨大的鸿沟，在tKG的庞大数据量和微调LLM的巨大计算成本之间也存在挑战。为了解决这些挑战，我们提出了一种新颖的检索增强生成框架，称为GenTKG，它在tKG上执行生成式预测，结合了基于时间逻辑规则的检索策略和轻量级的参数效率指导。通过大量实验证明了GenTKG的有效性。

    The rapid advancements in large language models (LLMs) have ignited interest in the temporal knowledge graph (tKG) domain, where conventional carefully designed embedding-based and rule-based models dominate. The question remains open of whether pre-trained LLMs can understand structured temporal relational data and replace them as the foundation model for temporal relational forecasting. Therefore, we bring temporal knowledge forecasting into the generative setting. However, challenges occur in the huge chasms between complex temporal graph data structure and sequential natural expressions LLMs can handle, and between the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs. To address these challenges, we propose a novel retrieval augmented generation framework that performs generative forecasting on tKGs named GenTKG, which combines a temporal logical rule-based retrieval strategy and lightweight parameter-efficient instruction tuning. Extensive experiments hav
    
[^104]: 使用Spark机器学习模型对航班票价数据进行预测分析

    Using Spark Machine Learning Models to Perform Predictive Analysis on Flight Ticket Pricing Data. (arXiv:2310.07787v1 [cs.LG])

    [http://arxiv.org/abs/2310.07787](http://arxiv.org/abs/2310.07787)

    本文使用Spark机器学习模型对航班票价数据进行预测分析，发现了一些关键的商业洞察，并讨论了过程和工具的使用。

    

    本文讨论了在航班票价数据上进行预测性能和流程的问题，利用r2(r-square)和RMSE来利用原始自Expedia.com的大型数据集进行分析，数据集包含大约2000万条记录或4.68GB。该项目旨在确定在现实世界中可用的最佳模型，以预测美国境内的非停航班机票价格。因此，模型的良好泛化能力和优化处理时间是重要的指标。我们将利用特征重要性发现关键的商业洞察，并讨论我们分析所使用的过程和工具。使用了四个回归机器学习算法：随机森林、梯度提升树、决策树和分解机，利用交叉验证器和训练验证器函数来评估性能和泛化能力。

    This paper discusses predictive performance and processes undertaken on flight pricing data utilizing r2(r-square) and RMSE that leverages a large dataset, originally from Expedia.com, consisting of approximately 20 million records or 4.68 gigabytes. The project aims to determine the best models usable in the real world to predict airline ticket fares for non-stop flights across the US. Therefore, good generalization capability and optimized processing times are important measures for the model.  We will discover key business insights utilizing feature importance and discuss the process and tools used for our analysis. Four regression machine learning algorithms were utilized: Random Forest, Gradient Boost Tree, Decision Tree, and Factorization Machines utilizing Cross Validator and Training Validator functions for assessing performance and generalization capability.
    
[^105]: 非稳态环境下基于神经预测集成抽样的情境赌博学习

    Non-Stationary Contextual Bandit Learning via Neural Predictive Ensemble Sampling. (arXiv:2310.07786v1 [cs.LG])

    [http://arxiv.org/abs/2310.07786](http://arxiv.org/abs/2310.07786)

    本文介绍了一种新颖的非稳态情境赌博算法，通过将可扩展的基于深度神经网络的架构与精心设计的探索机制相结合，在非稳态环境中优先收集持久价值信息，从而显著提高了性能。

    

    实际世界中的情境赌博应用常常因季节性、偶然性和不断变化的社交趋势而呈非稳态。尽管文献中已提出了许多非稳态情境赌博学习算法，但由于缺乏对持久价值信息的优先考虑，这些算法在探索时过度，或者设计方式难以在具有高维用户特定特征和大规模动作集的现代应用中扩展，或者两者都有。在本文中，我们介绍了一种新颖的非稳态情境赌博算法，它解决了这些问题。它将可扩展的基于深度神经网络的架构与一个精心设计的探索机制相结合，在非稳态环境中战略性地优先收集具有最持久价值的信息。通过在展示明显非稳态的两个实际推荐数据集上进行实证评估，我们证明了我们的方法显著胜过现有的算法。

    Real-world applications of contextual bandits often exhibit non-stationarity due to seasonality, serendipity, and evolving social trends. While a number of non-stationary contextual bandit learning algorithms have been proposed in the literature, they excessively explore due to a lack of prioritization for information of enduring value, or are designed in ways that do not scale in modern applications with high-dimensional user-specific features and large action set, or both. In this paper, we introduce a novel non-stationary contextual bandit algorithm that addresses these concerns. It combines a scalable, deep-neural-network-based architecture with a carefully designed exploration mechanism that strategically prioritizes collecting information with the most lasting value in a non-stationary environment. Through empirical evaluations on two real-world recommendation datasets, which exhibit pronounced non-stationarity, we demonstrate that our approach significantly outperforms the state
    
[^106]: 提升随机平滑的鲁棒性：两种成本效益的方法

    Promoting Robustness of Randomized Smoothing: Two Cost-Effective Approaches. (arXiv:2310.07780v1 [cs.LG])

    [http://arxiv.org/abs/2310.07780](http://arxiv.org/abs/2310.07780)

    本文提出了两种成本效益的方法来提高随机平滑的鲁棒性，一种是结合对抗训练和鲁棒性认证的新的鲁棒训练方法，另一种是后处理方法，这些方法在保持良好性能的同时能够提高随机平滑的鲁棒性。

    

    随机平滑最近在对抗鲁棒性领域吸引了注意，它可以为平滑的神经网络分类器提供可证明的鲁棒性保证。然而，现有的研究表明，普通的随机平滑通常不能提供良好的鲁棒性性能，并且经常需要在基本分类器上进行（重新）训练技术来提高平滑分类器的鲁棒性。在这项工作中，我们提出了两种成本效益的方法来提高随机平滑的鲁棒性，同时保持其良好的性能。第一种方法引入了一种新的鲁棒训练方法AdvMacer，结合对抗训练和最大化随机平滑的鲁棒性认证，我们展示了AdvMacer相对于SOTA基准可以提高随机平滑分类器的鲁棒性性能，同时训练速度比MACER基准快3倍。第二个方法引入了一种后处理方法EsbRS，它可以显著改善随机平滑的鲁棒性性能。

    Randomized smoothing has recently attracted attentions in the field of adversarial robustness to provide provable robustness guarantees on smoothed neural network classifiers. However, existing works show that vanilla randomized smoothing usually does not provide good robustness performance and often requires (re)training techniques on the base classifier in order to boost the robustness of the resulting smoothed classifier. In this work, we propose two cost-effective approaches to boost the robustness of randomized smoothing while preserving its clean performance. The first approach introduces a new robust training method AdvMacerwhich combines adversarial training and robustness certification maximization for randomized smoothing. We show that AdvMacer can improve the robustness performance of randomized smoothing classifiers compared to SOTA baselines, while being 3x faster to train than MACER baseline. The second approach introduces a post-processing method EsbRS which greatly impr
    
[^107]: 具有正交权重的深度网络中的特征学习与泛化

    Feature Learning and Generalization in Deep Networks with Orthogonal Weights. (arXiv:2310.07765v1 [cs.LG])

    [http://arxiv.org/abs/2310.07765](http://arxiv.org/abs/2310.07765)

    我们通过使用正交矩阵集合初始化权重并使用tanh激活函数，解决了全连接深度神经网络在初始化中具有与深度无关的线性波动问题。此外，我们发现神经切向核（NTK）及其后代的所有相关函数在逆宽度的主导阶段在深度约为20的位置饱和，而不是不断增长。

    

    通过使用从正交矩阵集合初始化的权重和tanh激活函数，我们展示了全连接深度神经网络在初始化时具有与宽度无关的前激活波动，这是通过计算证明的。此外，我们通过数值实验证明，在初始化时，涉及神经切向核（NTK）及其后代的所有相关函数在逆宽度的主导阶段饱和在深度约为20的位置，而不是像高斯初始化的情况那样不断增长。

    Fully-connected deep neural networks with weights initialized from independent Gaussian distributions can be tuned to criticality, which prevents the exponential growth or decay of signals propagating through the network. However, such networks still exhibit fluctuations that grow linearly with the depth of the network, which may impair the training of networks with width comparable to depth. We show analytically that rectangular networks with tanh activations and weights initialized from the ensemble of orthogonal matrices have corresponding preactivation fluctuations which are independent of depth, to leading order in inverse width. Moreover, we demonstrate numerically that, at initialization, all correlators involving the neural tangent kernel (NTK) and its descendants at leading order in inverse width -- which govern the evolution of observables during training -- saturate at a depth of $\sim 20$, rather than growing without bound as in the case of Gaussian initializations. We spec
    
[^108]: 从随机数据投影器进行无监督表示学习

    Self-supervised Representation Learning From Random Data Projectors. (arXiv:2310.07756v1 [cs.LG])

    [http://arxiv.org/abs/2310.07756](http://arxiv.org/abs/2310.07756)

    本文提出了一种无监督表示学习（SSRL）方法，通过重建随机数据投影来学习高质量的数据表示，不依赖于增强或掩蔽技术，可以应用于任何数据模态和网络架构。实验结果表明该方法在各种任务中优于其他SSRL算法。

    

    通过利用人工设计的数据增强方法下的变换不变性假设，自监督表示学习（SSRL）已经取得了显著的进展。虽然基于增强的SSRL算法在计算机视觉和自然语言处理中推动了性能的提升，但它们通常不适用于其他数据模态，并且可能与应用特定的数据增强约束冲突。本文提出了一种SSRL方法，可以应用于任何数据模态和网络架构，因为它不依赖于增强或掩蔽。具体而言，我们通过重建随机数据投影来学习高质量的数据表示。我们对跨多种模态和实际应用的表示学习任务进行了评估，结果表明它优于多个最先进的SSRL基线模型。由于其广泛适用性和强大的实证结果，我们认为...

    Self-supervised representation learning~(SSRL) has advanced considerably by exploiting the transformation invariance assumption under artificially designed data augmentations. While augmentation-based SSRL algorithms push the boundaries of performance in computer vision and natural language processing, they are often not directly applicable to other data modalities, and can conflict with application-specific data augmentation constraints. This paper presents an SSRL approach that can be applied to any data modality and network architecture because it does not rely on augmentations or masking. Specifically, we show that high-quality data representations can be learned by reconstructing random data projections. We evaluate the proposed approach on a wide range of representation learning tasks that span diverse modalities and real-world applications. We show that it outperforms multiple state-of-the-art SSRL baselines. Due to its wide applicability and strong empirical results, we argue t
    
[^109]: 离线强化学习中的问责制：用语料库的例子解释决策

    Accountability in Offline Reinforcement Learning: Explaining Decisions with a Corpus of Examples. (arXiv:2310.07747v1 [cs.LG])

    [http://arxiv.org/abs/2310.07747](http://arxiv.org/abs/2310.07747)

    本论文介绍了一种可解释的离线控制器方法，通过使用离线数据集作为决策语料库，在低数据场景中实现了问责制的控制，并在医疗保健领域展示了良好的性能。

    

    在决策系统中使用离线数据学习透明、可解释的控制器是一个重要的研究领域，因为它有潜力降低在现实世界系统中应用的风险。然而，在责任敏感的设置（如医疗保健）中，决策问责制非常重要，但目前的文献尚未充分解决这个问题。本文介绍了一种名为Accountable Offline Controller（AOC）的方法，它将离线数据集作为决策语料库，并根据一组定制的例子（称为语料库子集）进行问责制的控制。AOC在低数据场景中有效地运行，可以扩展到严格的离线模仿设置，并表现出保护和适应性的特点。我们在模拟和真实的医疗保健场景中评估了AOC的性能，强调了它在保持问责制的同时能够管理高水平的离线控制任务。

    Learning transparent, interpretable controllers with offline data in decision-making systems is an essential area of research due to its potential to reduce the risk of applications in real-world systems. However, in responsibility-sensitive settings such as healthcare, decision accountability is of paramount importance, yet has not been adequately addressed by the literature. This paper introduces the Accountable Offline Controller (AOC) that employs the offline dataset as the Decision Corpus and performs accountable control based on a tailored selection of examples, referred to as the Corpus Subset. ABC operates effectively in low-data scenarios, can be extended to the strictly offline imitation setting, and displays qualities of both conservation and adaptability. We assess ABC's performance in both simulated and real-world healthcare scenarios, emphasizing its capability to manage offline control tasks with high levels of performance while maintaining accountability.  Keywords: Int
    
[^110]: 深度强化学习用于自主网络安全行动的综述

    Deep Reinforcement Learning for Autonomous Cyber Operations: A Survey. (arXiv:2310.07745v1 [cs.LG])

    [http://arxiv.org/abs/2310.07745](http://arxiv.org/abs/2310.07745)

    深度强化学习被应用于自主网络安全行动有很大潜力，但在实际应用中还面临许多挑战。

    

    近年来网络攻击数量的快速增加引发了对针对恶意行为的网络防御方法的需求。深度强化学习（DRL）已经成为缓解这些攻击的一种有希望的方法。然而，尽管DRL在网络防御方面显示出了很大的潜力，但在将DRL应用于大规模自主网络安全行动（ACO）之前，还需要克服许多挑战。需要为与学习者面对非常高维度状态空间、大规模多离散操作空间和对抗学习相遇的环境提供有原则的方法。最近的研究在解决这些问题方面取得了成功。针对实时策略游戏也进行了印象深刻的工程努力。然而，将DRL应用于完整的ACO问题仍然是一个未解决的挑战。在这里，我们对相关的DRL文献进行了调查，并构想了一个理想化的ACO-DRL代理。我们提供了：i.) 领域特性的总结

    The rapid increase in the number of cyber-attacks in recent years raises the need for principled methods for defending networks against malicious actors. Deep reinforcement learning (DRL) has emerged as a promising approach for mitigating these attacks. However, while DRL has shown much potential for cyber-defence, numerous challenges must be overcome before DRL can be applied to autonomous cyber-operations (ACO) at scale. Principled methods are required for environments that confront learners with very high-dimensional state spaces, large multi-discrete action spaces, and adversarial learning. Recent works have reported success in solving these problems individually. There have also been impressive engineering efforts towards solving all three for real-time strategy games. However, applying DRL to the full ACO problem remains an open challenge. Here, we survey the relevant DRL literature and conceptualize an idealised ACO-DRL agent. We provide: i.) A summary of the domain properties t
    
[^111]: 从望远镜图像中自动分类螺旋-椭圆星系形态

    Spiral-Elliptical automated galaxy morphology classification from telescope images. (arXiv:2310.07740v1 [astro-ph.IM])

    [http://arxiv.org/abs/2310.07740](http://arxiv.org/abs/2310.07740)

    本研究发展了两个新颖的星系形态统计方法，并提出了简化版本的现有图像统计方法，通过提取望远镜图像中的特征，实现了自动分类螺旋-椭圆星系形态。

    

    星系形态分类是研究分级结构形成理论的重要步骤。虽然人工专家的视觉分类仍然有效准确，但无法应对新兴天空调查的海量数据。提出了各种分类大量星系的方法，包括众包视觉分类、基于设计的形态统计和深度学习的机器学习方法。本研究开发了两个新颖的星系形态统计，降落平均值和降落方差，可以有效地从望远镜星系图像中提取。进一步提出了现有图像统计集中度、不对称性和杂乱性的简化版本，这些方法在星系形态的文献中被广泛使用。通过利用SDSS的星系图像数据进行演示。

    The classification of galaxy morphologies is an important step in the investigation of theories of hierarchical structure formation. While human expert visual classification remains quite effective and accurate, it cannot keep up with the massive influx of data from emerging sky surveys. A variety of approaches have been proposed to classify large numbers of galaxies; these approaches include crowdsourced visual classification, and automated and computational methods, such as machine learning methods based on designed morphology statistics and deep learning. In this work, we develop two novel galaxy morphology statistics, descent average and descent variance, which can be efficiently extracted from telescope galaxy images. We further propose simplified versions of the existing image statistics concentration, asymmetry, and clumpiness, which have been widely used in the literature of galaxy morphologies. We utilize the galaxy image data from the Sloan Digital Sky Survey to demonstrate t
    
[^112]: Observatory: 刻画关系表嵌入的研究

    Observatory: Characterizing Embeddings of Relational Tables. (arXiv:2310.07736v1 [cs.DB])

    [http://arxiv.org/abs/2310.07736](http://arxiv.org/abs/2310.07736)

    Observatory提出了一个正式框架来分析关系表的嵌入表示，以帮助研究人员和实践者更好地理解和选择适合特定任务的模型。

    

    最近，语言模型和专门的表嵌入模型在许多表格数据任务上展示出了强大的性能。研究人员和实践者都渴望在许多新的应用场景中利用这些模型；但是对于这些模型的优势和缺点以及它们生成的表格表示的理解有限，导致在寻找适合特定任务的模型的过程中依赖于试错。迫切需要全面了解这些模型，以减少下游使用中的低效率和失败。为了解决这个问题，我们提出了一个名为Observatory的正式框架，以系统地分析关系表的嵌入表示。在关系数据模型的不变性和关于数据分布的统计考虑的基础上，我们定义了八个原始属性，以及相应的度量来定量地刻画这些属性的表格嵌入。

    Language models and specialized table embedding models have recently demonstrated strong performance on many tasks over tabular data. Researchers and practitioners are keen to leverage these models in many new application contexts; but limited understanding of the strengths and weaknesses of these models, and the table representations they generate, makes the process of finding a suitable model for a given task reliant on trial and error. There is an urgent need to gain a comprehensive understanding of these models to minimize inefficiency and failures in downstream usage.  To address this need, we propose Observatory, a formal framework to systematically analyze embedding representations of relational tables. Motivated both by invariants of the relational data model and by statistical considerations regarding data distributions, we define eight primitive properties, and corresponding measures to quantitatively characterize table embeddings for these properties. Based on these properti
    
[^113]: 极端图像转换促进稳健的潜在对象表示

    Extreme Image Transformations Facilitate Robust Latent Object Representations. (arXiv:2310.07725v1 [cs.LG])

    [http://arxiv.org/abs/2310.07725](http://arxiv.org/abs/2310.07725)

    本论文展示了使用极端图像转换（EIT）对预训练网络进行微调可以学习到稳健的潜在对象表示，并且可以提高网络对各种强度的常见对抗攻击的性能。

    

    对抗攻击可能会影响机器在野外的物体识别能力。这些攻击通常是由输入和类标签之间的虚假关联引起的，并且容易在大型网络中发生记忆。虽然网络被期望进行自动化特征选择，但在对象的规模上不是很有效。然而，人类能够选择形成物体稳健表示所需的最小特征集。在这项工作中，我们展示了使用极端图像转换（EIT）对任何预训练的现成网络进行微调不仅有助于学习稳健的潜在表示，还改善了这些网络对不同强度的常见对抗攻击的性能。我们的EIT训练网络在测试中即使受到更强烈的噪声干扰时也表现出强烈的激活，显示出在不同类型的对抗攻击中的有希望的泛化能力。

    Adversarial attacks can affect the object recognition capabilities of machines in wild. These can often result from spurious correlations between input and class labels, and are prone to memorization in large networks. While networks are expected to do automated feature selection, it is not effective at the scale of the object. Humans, however, are able to select the minimum set of features required to form a robust representation of an object. In this work, we show that finetuning any pretrained off-the-shelf network with Extreme Image Transformations (EIT) not only helps in learning a robust latent representation, it also improves the performance of these networks against common adversarial attacks of various intensities. Our EIT trained networks show strong activations in the object regions even when tested with more intense noise, showing promising generalizations across different kinds of adversarial attacks.
    
[^114]: 视觉预测作为避障的中层表示

    Visual Forecasting as a Mid-level Representation for Avoidance. (arXiv:2310.07724v1 [cs.RO])

    [http://arxiv.org/abs/2310.07724](http://arxiv.org/abs/2310.07724)

    本研究提出了一种创新的替代方法——视觉预测，通过引入直观的视觉提示，投射动态物体的未来轨迹，提高智能体的感知能力并实现预期的动作。研究结果验证了视觉预测作为导航问题解决方案的可行性。

    

    在自主智能体研究中，在具有动态物体的环境中进行导航的挑战仍然是一个核心问题。虽然预测方法很有前景，但它们对精确状态信息的依赖使其在实际世界中的应用不太实用。本研究提出了视觉预测作为一种创新的替代方法。通过引入直观的视觉提示，这种方法可以投射动态物体的未来轨迹，以提高智能体的感知能力并实现预期的动作。我们的研究探索了两种不同的策略来通过视觉预测传达预测信息：（1）边界框序列和（2）增强路径。为了验证所提出的视觉预测策略，我们在Unity引擎的模拟环境中进行评估，并将这些评估扩展到实际场景中，以评估其实用性和有效性。结果证实了视觉预测作为导航问题的有前途的解决方案的可行性。

    The challenge of navigation in environments with dynamic objects continues to be a central issue in the study of autonomous agents. While predictive methods hold promise, their reliance on precise state information makes them less practical for real-world implementation. This study presents visual forecasting as an innovative alternative. By introducing intuitive visual cues, this approach projects the future trajectories of dynamic objects to improve agent perception and enable anticipatory actions. Our research explores two distinct strategies for conveying predictive information through visual forecasting: (1) sequences of bounding boxes, and (2) augmented paths. To validate the proposed visual forecasting strategies, we initiate evaluations in simulated environments using the Unity engine and then extend these evaluations to real-world scenarios to assess both practicality and effectiveness. The results confirm the viability of visual forecasting as a promising solution for navigat
    
[^115]: Parametric Leaky Tanh：一种新的深度学习混合激活函数

    Parametric Leaky Tanh: A New Hybrid Activation Function for Deep Learning. (arXiv:2310.07720v1 [cs.LG])

    [http://arxiv.org/abs/2310.07720](http://arxiv.org/abs/2310.07720)

    本文提出了一种新颖的激活函数Parametric Leaky Tanh (PLTanh)，结合了Tanh和Leaky ReLU (LReLU)的优点，解决了'dying ReLU'问题，有助于在网络内部学习更复杂的非线性关系。

    

    激活函数对于深度神经网络的性能有着重要的影响。本文提出了Parametric Leaky Tanh (PLTanh)，一种新颖的混合激活函数，旨在结合Tanh和Leaky ReLU (LReLU)激活函数的优点。PLTanh在所有点上可微分，并通过保证负输入的非零梯度，解决了'dying ReLU'问题，与LReLU的行为一致。通过整合这两种不同的激活函数的独特优势，PLTanh有助于在网络内部学习更复杂的非线性关系。本文通过在五个不同的数据集上对比了PLTanh与常见的激活函数ReLU、LReLU和ALReLU的实证评估。

    Activation functions (AFs) are crucial components of deep neural networks (DNNs), having a significant impact on their performance. An activation function in a DNN is typically a smooth, nonlinear function that transforms an input signal into an output signal for the subsequent layer. In this paper, we propose the Parametric Leaky Tanh (PLTanh), a novel hybrid activation function designed to combine the strengths of both the Tanh and Leaky ReLU (LReLU) activation functions. PLTanh is differentiable at all points and addresses the 'dying ReLU' problem by ensuring a non-zero gradient for negative inputs, consistent with the behavior of LReLU. By integrating the unique advantages of these two diverse activation functions, PLTanh facilitates the learning of more intricate nonlinear relationships within the network. This paper presents an empirical evaluation of PLTanh against established activation functions, namely ReLU, LReLU, and ALReLU utilizing five diverse datasets.
    
[^116]: 重新考虑基于DNA序列的BERT-like预训练

    Rethinking the BERT-like Pretraining for DNA Sequences. (arXiv:2310.07644v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.07644](http://arxiv.org/abs/2310.07644)

    重新考虑了基于DNA序列的BERT-like预训练方法，通过使用K-mer重叠标记化，在下游任务的微调阶段和预训练过程中都取得了一致的性能改善。

    

    随着在自然语言处理领域中大规模预训练的成功，将其应用于生命科学领域的趋势日益增长。特别是基于DNA序列的预训练方法因其捕捉基因的通用信息的潜力而受到关注。然而，现有的DNA序列预训练方法主要依赖于从自然语言处理领域直接引入的BERT预训练方法，缺乏全面的理解和专门定制的方法。为了填补这一研究空白，我们首先进行了一系列的探索性实验，并获得了几个有启发性的观察结果：1）在下游任务的微调阶段，使用K-mer重叠标记化而不是K-mer非重叠标记化时，重叠和非重叠的预训练权重均表现出一致的性能改善。2）在预训练过程中，使用K-mer重叠标记化会迅速产生清晰的K-mer嵌入，并将损失降低到非常低的水平。

    With the success of large-scale pretraining in NLP, there is an increasing trend of applying it to the domain of life sciences. In particular, pretraining methods based on DNA sequences have garnered growing attention due to their potential to capture generic information about genes. However, existing pretraining methods for DNA sequences largely rely on direct adoptions of BERT pretraining from NLP, lacking a comprehensive understanding and a specifically tailored approach. To address this research gap, we first conducted a series of exploratory experiments and gained several insightful observations: 1) In the fine-tuning phase of downstream tasks, when using K-mer overlapping tokenization instead of K-mer non-overlapping tokenization, both overlapping and non-overlapping pretraining weights show consistent performance improvement.2) During the pre-training process, using K-mer overlapping tokenization quickly produces clear K-mer embeddings and reduces the loss to a very low level, w
    
[^117]: In-Context Unlearning: 基于少样本学习的语言模型的消除研究

    In-Context Unlearning: Language Models as Few Shot Unlearners. (arXiv:2310.07579v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.07579](http://arxiv.org/abs/2310.07579)

    这项工作提出了一种新的LLM消除方法，称为“基于上下文的消除”，它提供了上下文的输入且无需更新模型参数。这种方法解决了消除对于很大模型来说在计算上的困难，并在实践中具有更高的可行性和便捷性。

    

    机器消除学习是研究如何高效地去除特定训练数据对训练模型的影响，近来引起了更多的关注，主要是由于需要遵守诸如被遗忘权等隐私法规的需求。尽管在版权问题上LLM（语言模型）尤其相关，但在非常大的模型上实现精确消除是计算上不可行的。为此，最近的研究提出了几种算法，可以在不重新训练模型的情况下近似消除训练数据。这些算法关键依赖于对模型参数的访问来更新它们，但在实践中可能由于计算约束或通过API访问LLM而无法满足这种假设。在这项工作中，我们提出了一种新的LLM消除方法，称为“基于上下文的消除”，它提供了上下文的输入且无需更新模型参数。为了消除特定的训练实例，我们提供了i

    Machine unlearning, the study of efficiently removing the impact of specific training points on the trained model, has garnered increased attention of late, driven by the need to comply with privacy regulations like the Right to be Forgotten. Although unlearning is particularly relevant for LLMs in light of the copyright issues they raise, achieving precise unlearning is computationally infeasible for very large models. To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or when the LLM is accessed via API. In this work, we propose a new class of unlearning methods for LLMs we call ''In-Context Unlearning'', providing inputs in context and without having to update model parameters. To unlearn a particular training instance, we provide the i
    
[^118]: 通过自动折扣调度从观察中进行模仿学习

    Imitation Learning from Observation with Automatic Discount Scheduling. (arXiv:2310.07433v1 [cs.RO])

    [http://arxiv.org/abs/2310.07433](http://arxiv.org/abs/2310.07433)

    我们提出了一个新颖的观察学习模仿(ILfO)框架，能够解决由于奖励信号分配错误导致代理无法学习初始行为的问题。

    

    人类通常通过观察和模仿来获得新的技能。对于机器人代理，从互联网上可用的大量无标签视频演示数据中进行学习，需要在没有访问其动作的情况下模仿专家，这是一种称为观察学习模仿（ILfO）的挑战。解决ILfO问题的常见方法是将其转化为逆向强化学习问题，利用从代理和专家观察中计算出的代理奖励。然而，我们发现在具有进展依赖性属性的任务中，这样的方法面临重大挑战；在这些任务中，代理需要在掌握后续行为之前先学习专家的前序行为。我们的研究表明，主要原因是分配给后续步骤的奖励信号妨碍了对初始行为的学习。为了解决这个挑战，我们提出了一个新颖的ILfO框架，使代理能够掌握早期行为。

    Humans often acquire new skills through observation and imitation. For robotic agents, learning from the plethora of unlabeled video demonstration data available on the Internet necessitates imitating the expert without access to its action, presenting a challenge known as Imitation Learning from Observations (ILfO). A common approach to tackle ILfO problems is to convert them into inverse reinforcement learning problems, utilizing a proxy reward computed from the agent's and the expert's observations. Nonetheless, we identify that tasks characterized by a progress dependency property pose significant challenges for such approaches; in these tasks, the agent needs to initially learn the expert's preceding behaviors before mastering the subsequent ones. Our investigation reveals that the main cause is that the reward signals assigned to later steps hinder the learning of initial behaviors. To address this challenge, we present a novel ILfO framework that enables the agent to master earl
    
[^119]: 增强量子预测能力：利用量子Gramian角度场和CNN进行股票回报预测

    Quantum-Enhanced Forecasting: Leveraging Quantum Gramian Angular Field and CNNs for Stock Return Predictions. (arXiv:2310.07427v1 [cs.LG])

    [http://arxiv.org/abs/2310.07427](http://arxiv.org/abs/2310.07427)

    该论文提出了一种名为Quantum Gramian Angular Field (QGAF)的新方法，通过将量子计算技术与深度学习相结合，成功将股票回报时间序列数据转化为适合卷积神经网络（CNN）训练的二维图像，从而提高了预测的精度。实验证明，相比传统方法，QGAF方法具有显著的性能优势。

    

    我们提出了一种名为Quantum Gramian Angular Field (QGAF)的时间序列预测方法。该方法将量子计算技术与深度学习相结合，旨在提高时间序列分类和预测的精度。通过设计特定的量子电路，我们成功地将股票回报时间序列数据转化为适合卷积神经网络（CNN）训练的二维图像。与经典的Gramian Angular Field (GAF)方法不同，QGAF的独特之处在于消除了数据归一化和反余弦计算的需求，简化了从时间序列数据到二维图像的转换过程。为了验证该方法的有效性，我们在中国A股市场、香港股市和美国股市的数据集上进行了实验。实验结果表明，与经典的GAF方法相比，QGAF方法显著改善了预测性能。

    We propose a time series forecasting method named Quantum Gramian Angular Field (QGAF). This approach merges the advantages of quantum computing technology with deep learning, aiming to enhance the precision of time series classification and forecasting. We successfully transformed stock return time series data into two-dimensional images suitable for Convolutional Neural Network (CNN) training by designing specific quantum circuits. Distinct from the classical Gramian Angular Field (GAF) approach, QGAF's uniqueness lies in eliminating the need for data normalization and inverse cosine calculations, simplifying the transformation process from time series data to two-dimensional images. To validate the effectiveness of this method, we conducted experiments on datasets from three major stock markets: the China A-share market, the Hong Kong stock market, and the US stock market. Experimental results revealed that compared to the classical GAF method, the QGAF approach significantly improv
    
[^120]: NuTime: 大规模时间序列预训练的数值多尺度嵌入

    NuTime: Numerically Multi-Scaled Embedding for Large-Scale Time Series Pretraining. (arXiv:2310.07402v1 [cs.LG])

    [http://arxiv.org/abs/2310.07402](http://arxiv.org/abs/2310.07402)

    本研究通过采用Transformer架构和数值多尺度嵌入模块，使时间序列自监督模型能够扩展到大规模数据集，并在大规模数据集上进行预训练。

    

    最近关于时间序列自监督模型的研究显示出学习语义表示的巨大潜力，然而，这些研究仅限于小规模数据集，例如数千个时间序列。本文的关键技术贡献针对时间序列数据的数值特性，使模型能够扩展到大规模数据集，例如百万个时间序列。我们采用Transformer架构，首先将输入划分为非重叠窗口。然后，通过窗口的标准化形状和两个标量值表示每个窗口内的均值和标准差。为了将可能具有任意数值尺度的标量值嵌入到高维向量中，我们提出了一个数值多尺度嵌入模块，枚举所有可能的标量值尺度。该模型使用提出的数值多尺度嵌入在大规模数据集上进行预训练，采用简单的对比损失函数。

    Recent research on time-series self-supervised models shows great promise in learning semantic representations. However, it has been limited to small-scale datasets, e.g., thousands of temporal sequences. In this work, we make key technical contributions that are tailored to the numerical properties of time-series data and allow the model to scale to large datasets, e.g., millions of temporal sequences. We adopt the Transformer architecture by first partitioning the input into non-overlapping windows. Each window is then characterized by its normalized shape and two scalar values denoting the mean and standard deviation within each window. To embed scalar values that may possess arbitrary numerical scales to high-dimensional vectors, we propose a numerically multi-scaled embedding module enumerating all possible scales for the scalar values. The model undergoes pretraining using the proposed numerically multi-scaled embedding with a simple contrastive objective on a large-scale dataset
    
[^121]: GraphControl:为图领域迁移学习中的通用图预训练模型添加条件控制

    GraphControl: Adding Conditional Control to Universal Graph Pre-trained Models for Graph Domain Transfer Learning. (arXiv:2310.07365v1 [cs.LG])

    [http://arxiv.org/abs/2310.07365](http://arxiv.org/abs/2310.07365)

    在图领域迁移学习中，GraphControl通过添加条件控制实现了对通用图预训练模型的有效迁移，克服了不同图域间的属性语义差异问题。

    

    图结构化数据在世界中无处不在，这种数据模型了对象之间的复杂关系，为各种Web应用提供了可能。Web上每天涌现的无标签图数据为这些应用提供了巨大的潜力。图自监督算法在从丰富的无标签图数据中获得通用知识方面取得了显著成功。这些预训练模型可以应用于各种下游Web应用，节省训练时间，提高下游（目标）性能。然而，即使在表面上看起来相似的领域中，不同的图在属性语义方面也可能存在显着差异，这给将预训练模型迁移到下游任务中带来了困难，甚至是不可行性。具体而言，例如，下游任务中的附加特定任务节点信息（特异性）通常会被有意省略，以便利用预训练表示（可迁移性）。这种权衡被称为

    Graph-structured data is ubiquitous in the world which models complex relationships between objects, enabling various Web applications. Daily influxes of unlabeled graph data on the Web offer immense potential for these applications. Graph self-supervised algorithms have achieved significant success in acquiring generic knowledge from abundant unlabeled graph data. These pre-trained models can be applied to various downstream Web applications, saving training time and improving downstream (target) performance. However, different graphs, even across seemingly similar domains, can differ significantly in terms of attribute semantics, posing difficulties, if not infeasibility, for transferring the pre-trained models to downstream tasks. Concretely speaking, for example, the additional task-specific node information in downstream tasks (specificity) is usually deliberately omitted so that the pre-trained representation (transferability) can be leveraged. The trade-off as such is termed as 
    
[^122]: WiGenAI: 通过扩散模型实现无线和生成式人工智能的交织

    WiGenAI: The Symphony of Wireless and Generative AI via Diffusion Models. (arXiv:2310.07312v1 [cs.IT])

    [http://arxiv.org/abs/2310.07312](http://arxiv.org/abs/2310.07312)

    WiGenAI通过引入扩散模型，将生成式人工智能应用于无线通信系统中，为研究奠定基础。这篇文章介绍了扩散模型作为生成模型的最新范式，并讨论了它在无线通信系统中的应用。通过两个案例研究展示了扩散模型在开发韧性的AI本地通信系统中的潜力。

    

    创新的基础模型，如GPT-3和稳定的扩散模型，已经在人工智能领域实现了范式转变，向生成式人工智能系统发展。从数据通信和网络的角度来看，人工智能和机器学习算法预计将广泛应用于未来无线通信系统的新一代中，强调了在新兴通信场景中需要新颖的AI本地解决方案。本文介绍生成式人工智能在无线通信系统中的应用，为该领域的研究奠定基础。介绍了扩散型生成模型作为生成模型的最新范式，并讨论了它们在无线通信系统中的应用。还提供了两个案例研究，展示了如何利用扩散模型开发具有韧性的AI本地通信系统。具体而言，我们提出了一种基于扩散模型的生成模型，以展示其在生成模型的应用中的优势。

    Innovative foundation models, such as GPT-3 and stable diffusion models, have made a paradigm shift in the realm of artificial intelligence (AI) towards generative AI-based systems. In unison, from data communication and networking perspective, AI and machine learning (AI/ML) algorithms are envisioned to be pervasively incorporated into the future generations of wireless communications systems, highlighting the need for novel AI-native solutions for the emergent communication scenarios. In this article, we outline the applications of generative AI in wireless communication systems to lay the foundations for research in this field. Diffusion-based generative models, as the new state-of-the-art paradigm of generative models, are introduced, and their applications in wireless communication systems are discussed. Two case studies are also presented to showcase how diffusion models can be exploited for the development of resilient AI-native communication systems. Specifically, we propose de
    
[^123]: 通过扩散行为实现得分正则化策略优化

    Score Regularized Policy Optimization through Diffusion Behavior. (arXiv:2310.07297v1 [cs.LG])

    [http://arxiv.org/abs/2310.07297](http://arxiv.org/abs/2310.07297)

    通过利用扩散行为模型，我们提出了一种在离线强化学习中用于优化策略的得分正则化方法，从而避免了耗时且计算密集的扩散采样方案，并在D4RL任务上实现了超过25倍的动作采样速度提升。

    

    最近的离线强化学习研究展示了扩散建模的巨大潜力，这充分展现了其在表达异质行为策略方面的优越性。然而，从扩散策略中采样非常缓慢，因为需要数十到数百次迭代推理步骤来进行一次动作采样。为了解决这个问题，我们提出了一种从评论家模型和预训练的扩散行为模型中提取高效确定性推理策略的方法，在优化过程中利用后者直接对策略梯度进行正则化，使用行为分布的得分函数。我们的方法在训练和评估过程中充分发挥了扩散建模的强大生成能力，同时完全绕过了计算密集和耗时的扩散采样方案。在D4RL任务上的广泛结果显示，我们的方法将动作采样速度提高了超过25倍，相比于各种领先的基于扩散的方法在运动任务中。

    Recent developments in offline reinforcement learning have uncovered the immense potential of diffusion modeling, which excels at representing heterogeneous behavior policies. However, sampling from diffusion policies is considerably slow because it necessitates tens to hundreds of iterative inference steps for one action. To address this issue, we propose to extract an efficient deterministic inference policy from critic models and pretrained diffusion behavior models, leveraging the latter to directly regularize the policy gradient with the behavior distribution's score function during optimization. Our method enjoys powerful generative capabilities of diffusion modeling while completely circumventing the computationally intensive and time-consuming diffusion sampling scheme, both during training and evaluation. Extensive results on D4RL tasks show that our method boosts action sampling speed by more than 25 times compared with various leading diffusion-based methods in locomotion ta
    
[^124]: 通过信息论分布多样化实现联邦泛化能力

    Federated Generalization via Information-Theoretic Distribution Diversification. (arXiv:2310.07171v1 [cs.LG])

    [http://arxiv.org/abs/2310.07171](http://arxiv.org/abs/2310.07171)

    该论文研究了联邦学习中泛化能力的挑战，特别关注训练分布和测试分布的不匹配。提出了一种信息论的泛化方法来解决这个问题。

    

    联邦学习（FL）因其在无需直接数据共享的情况下进行协同模型训练的能力而日益突出。然而，客户端之间本地数据分布的巨大差异，通常被称为非独立同分布（non-IID）挑战，对FL的泛化能力构成了重大障碍。当并非所有客户端都参与训练过程时，情况变得更加复杂，这是由于不稳定的网络连接或有限的计算能力而常见。这可能极大地复杂化了对训练模型的泛化能力的评估。尽管最近的大量研究集中在涉及具有不同分布的参与客户端的未见数据的泛化差距问题上，但参与客户端的训练分布和非参与客户端的测试分布之间的差异却被大部分忽视了。为此，我们的论文揭示了一种基于信息论的泛化方法。

    Federated Learning (FL) has surged in prominence due to its capability of collaborative model training without direct data sharing. However, the vast disparity in local data distributions among clients, often termed the non-Independent Identically Distributed (non-IID) challenge, poses a significant hurdle to FL's generalization efficacy. The scenario becomes even more complex when not all clients participate in the training process, a common occurrence due to unstable network connections or limited computational capacities. This can greatly complicate the assessment of the trained models' generalization abilities. While a plethora of recent studies has centered on the generalization gap pertaining to unseen data from participating clients with diverse distributions, the divergence between the training distributions of participating clients and the testing distributions of non-participating ones has been largely overlooked. In response, our paper unveils an information-theoretic genera
    
[^125]: 洪水和回声：图神经网络与分布式计算的算法对齐

    Flood and Echo: Algorithmic Alignment of GNNs with Distributed Computing. (arXiv:2310.06970v1 [cs.LG])

    [http://arxiv.org/abs/2310.06970](http://arxiv.org/abs/2310.06970)

    本论文提出了一个基于分布式算法设计原则的新的执行框架：洪水和回声网络。通过波状激活模式，它可以在整个图中传递消息，并且可以有效地处理更大的图形。

    

    图神经网络是学习算法的自然选择。它们可以通过抽象而多功能的图结构直接表示任务，并处理不同规模的输入。这为算法的扩展和外推到更大的图形提供了可能性，这是一种最重要的优势。然而，这提出了两个核心问题：i）如何使节点能够在给定的图中收集所需的信息（即“信息交换”），即使节点距离很远；ii）我们如何设计一个执行框架，以实现该信息交换以便外推到更大的图大小（即“外推时的算法对齐”）。我们提出了一个新的执行框架，受分布式算法的设计原则启发：洪水和回声网络。它以波状激活模式将消息传播到整个图中，自然地推广到更大的实例。通过它的稀疏但并行激活，可以证明它更加高效。

    Graph Neural Networks are a natural fit for learning algorithms. They can directly represent tasks through an abstract but versatile graph structure and handle inputs of different sizes. This opens up the possibility for scaling and extrapolation to larger graphs, one of the most important advantages of an algorithm. However, this raises two core questions i) How can we enable nodes to gather the required information in a given graph ($\textit{information exchange}$), even if is far away and ii) How can we design an execution framework which enables this information exchange for extrapolation to larger graph sizes ($\textit{algorithmic alignment for extrapolation}$). We propose a new execution framework that is inspired by the design principles of distributed algorithms: Flood and Echo Net. It propagates messages through the entire graph in a wave like activation pattern, which naturally generalizes to larger instances. Through its sparse but parallel activations it is provably more ef
    
[^126]: NECO: 基于神经坍塌的超出分布检测

    NECO: NEural Collapse Based Out-of-distribution detection. (arXiv:2310.06823v1 [stat.ML])

    [http://arxiv.org/abs/2310.06823](http://arxiv.org/abs/2310.06823)

    NECO是一种基于神经坍塌的新颖的超出分布检测方法，利用几何属性和主成分空间识别OOD数据，在小规模和大规模任务上取得了最先进的结果，并展示了强大的泛化能力。

    

    由于模型过于自信并且没有意识到其认识论限制，检测超出分布（OOD）数据是机器学习中的一个重要挑战。我们假设“神经坍塌”，一种影响超出分布数据的现象，也会影响超出分布数据。为了从这种相互作用中受益，我们引入了NECO，一种用于OOD检测的新颖的事后方法，它利用“神经坍塌”和主成分空间的几何属性来识别OOD数据。我们的大量实验表明，NECO在小规模和大规模OOD检测任务上取得了最先进的结果，同时在不同的网络架构上展示了强大的泛化能力。此外，我们还对我们的方法在OOD检测中的有效性提供了理论解释。我们计划在匿名期结束后发布代码。

    Detecting out-of-distribution (OOD) data is a critical challenge in machine learning due to model overconfidence, often without awareness of their epistemological limits. We hypothesize that ``neural collapse'', a phenomenon affecting in-distribution data for models trained beyond loss convergence, also influences OOD data. To benefit from this interplay, we introduce NECO, a novel post-hoc method for OOD detection, which leverages the geometric properties of ``neural collapse'' and of principal component spaces to identify OOD data. Our extensive experiments demonstrate that NECO achieves state-of-the-art results on both small and large-scale OOD detection tasks while exhibiting strong generalization capabilities across different network architectures. Furthermore, we provide a theoretical explanation for the effectiveness of our method in OOD detection. We plan to release the code after the anonymity period.
    
[^127]: FABind: 快速准确的蛋白-配体结合

    FABind: Fast and Accurate Protein-Ligand Binding. (arXiv:2310.06763v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.06763](http://arxiv.org/abs/2310.06763)

    FABind是一个结合了口袋预测和对接的端到端模型，旨在实现快速准确的蛋白-配体结合预测。

    

    在药物发现中，对蛋白质和配体之间的相互作用进行建模并准确预测其结合结构是一项关键但具有挑战性的任务。深度学习的最新进展在应对这一挑战方面显示出了希望，采样法和回归法成为两种突出的方法。然而，这些方法都存在明显的局限性。采样法通常由于需要生成多个候选结构来进行选择而效率较低。而回归法提供了快速的预测，但可能会导致准确性降低。另外，蛋白质大小的变化通常需要外部模块来选择合适的结合口袋，进一步影响效率。在这项工作中，我们提出了FABind，一个将口袋预测和对接相结合的端到端模型，以实现准确和快速的蛋白-配体结合。

    Modeling the interaction between proteins and ligands and accurately predicting their binding structures is a critical yet challenging task in drug discovery. Recent advancements in deep learning have shown promise in addressing this challenge, with sampling-based and regression-based methods emerging as two prominent approaches. However, these methods have notable limitations. Sampling-based methods often suffer from low efficiency due to the need for generating multiple candidate structures for selection. On the other hand, regression-based methods offer fast predictions but may experience decreased accuracy. Additionally, the variation in protein sizes often requires external modules for selecting suitable binding pockets, further impacting efficiency. In this work, we propose $\mathbf{FABind}$, an end-to-end model that combines pocket prediction and docking to achieve accurate and fast protein-ligand binding. $\mathbf{FABind}$ incorporates a unique ligand-informed pocket prediction
    
[^128]: SpikeCLIP：一种对比语言-图像预训练脉冲神经网络

    SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural Network. (arXiv:2310.06488v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2310.06488](http://arxiv.org/abs/2310.06488)

    本论文引入了一种名为SpikeCLIP的新框架，通过对比语言-图像预训练实现了脉冲神经网络的多模态扩展，并在能源效率和性能方面取得了可比较的结果。

    

    脉冲神经网络（SNNs）已经证明其在视觉和语言领域中能够实现与深度神经网络（DNNs）相当的性能，同时具有能效提高和符合生物合理性的优势。然而，将这种单模态的SNNs扩展到多模态的情景仍然是一个未开发的领域。受到对比语言-图像预训练（CLIP）概念的启发，我们引入了一个名为SpikeCLIP的新框架，通过“对齐预训练+双损失微调”的两步骤配方，来解决脉冲计算背景下两种模态之间的差距。广泛的实验证明，在常用的用于多模态模型评估的各种数据集上，SNNs取得了与其DNNs对应物相当的结果，同时显著降低了能源消耗。此外，SpikeCLIP在图像分类方面保持了稳定的性能。

    Spiking neural networks (SNNs) have demonstrated the capability to achieve comparable performance to deep neural networks (DNNs) in both visual and linguistic domains while offering the advantages of improved energy efficiency and adherence to biological plausibility. However, the extension of such single-modality SNNs into the realm of multimodal scenarios remains an unexplored territory. Drawing inspiration from the concept of contrastive language-image pre-training (CLIP), we introduce a novel framework, named SpikeCLIP, to address the gap between two modalities within the context of spike-based computing through a two-step recipe involving ``Alignment Pre-training + Dual-Loss Fine-tuning". Extensive experiments demonstrate that SNNs achieve comparable results to their DNN counterparts while significantly reducing energy consumption across a variety of datasets commonly used for multimodal model evaluation. Furthermore, SpikeCLIP maintains robust performance in image classification 
    
[^129]: GPT-4作为农学助手？使用大型语言模型回答农业考试

    GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using Large Language Models. (arXiv:2310.06225v1 [cs.AI])

    [http://arxiv.org/abs/2310.06225](http://arxiv.org/abs/2310.06225)

    本研究全面评估了大型语言模型（LLMs）在回答农业相关问题方面的能力，并使用RAG和ER技术提高了LLMs的性能。其中，GPT-4在农业考试中取得了及格分数以获得认证。

    

    大型语言模型（LLM）在各个领域，包括医疗保健和金融领域，展示了卓越的自然语言理解能力。在某些任务上，LLM的性能与训练有素的人类相似甚至更好，因此合理地使用人类考试（例如认证考试）来评估LLM的性能。我们对流行的LLM（如Llama 2和GPT）在回答农业相关问题的能力进行了全面评估。在评估过程中，我们还运用了RAG（检索增强生成）和ER（集合细化）技术，结合信息检索、生成能力和提示策略，提高LLM的性能。为了展示LLM的能力，我们选择了来自巴西、印度和美国三个最大的农业生产国的农业考试和基准数据集。我们的分析突出了GPT-4在考试中取得及格分数以获得认证的能力。

    Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding across various domains, including healthcare and finance. For some tasks, LLMs achieve similar or better performance than trained human beings, therefore it is reasonable to employ human exams (e.g., certification tests) to assess the performance of LLMs. We present a comprehensive evaluation of popular LLMs, such as Llama 2 and GPT, on their ability to answer agriculture-related questions. In our evaluation, we also employ RAG (Retrieval-Augmented Generation) and ER (Ensemble Refinement) techniques, which combine information retrieval, generation capabilities, and prompting strategies to improve the LLMs' performance. To demonstrate the capabilities of LLMs, we selected agriculture exams and benchmark datasets from three of the largest agriculture producer countries: Brazil, India, and the USA. Our analysis highlights GPT-4's ability to achieve a passing score on exams to earn cred
    
[^130]: 狮子秘密地解决受限制优化问题：正如李雅普诺夫所预测的。

    Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts. (arXiv:2310.05898v1 [cs.LG])

    [http://arxiv.org/abs/2310.05898](http://arxiv.org/abs/2310.05898)

    Lion是通过程序搜索发现的新优化器，在训练大型AI模型方面表现出有希望的结果，具有更高的内存效率。尽管其理论基础不明确，但基于连续时间和离散时间分析，我们证明Lion是一种理论上新颖且有原则的方法，可在最小化一般损失函数的同时强制执行边界约束。

    

    通过程序搜索发现的新优化器Lion（进化的符号动量）在训练大型AI模型方面显示出有希望的结果。它在训练效果上与AdamW相当或更好，并具有更高的内存效率。正如我们可以从随机搜索程序的结果中期待的，Lion集成了几个现有算法的元素，包括符号动量、独立的权重衰减、Polak和Nesterov动量，但又不属于任何现有的理论基础优化器类别。因此，尽管Lion作为广泛任务的通用优化器表现良好，但其理论基础仍然不明确。这种缺乏理论的明确性限制了进一步增强和扩展Lion的可能性。本文旨在揭开Lion的神秘面纱。基于连续时间和离散时间分析，我们证明Lion是一种理论上新颖且有原则的方法，可在最小化一般损失函数$f(x)$的同时强制执行边界约束。

    Lion (Evolved Sign Momentum), a new optimizer discovered through program search, has shown promising results in training large AI models. It performs comparably or favorably to AdamW but with greater memory efficiency. As we can expect from the results of a random search program, Lion incorporates elements from several existing algorithms, including signed momentum, decoupled weight decay, Polak, and Nesterov momentum, but does not fit into any existing category of theoretically grounded optimizers. Thus, even though Lion appears to perform well as a general-purpose optimizer for a wide range of tasks, its theoretical basis remains uncertain. This lack of theoretical clarity limits opportunities to further enhance and expand Lion's efficacy.  This work aims to demystify Lion. Based on both continuous-time and discrete-time analysis, we demonstrate that Lion is a theoretically novel and principled approach for minimizing a general loss function $f(x)$ while enforcing a bound constraint 
    
[^131]: 可适应本地性感知的泛化隐式神经表示

    Locality-Aware Generalizable Implicit Neural Representation. (arXiv:2310.05624v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.05624](http://arxiv.org/abs/2310.05624)

    本文提出了一种结合Transformer编码器和具有本地感知的INR解码器的框架，用于解决泛化的隐式神经表示中无法定位和捕获细粒度细节的问题。

    

    泛化的隐式神经表示（INR）通过使用潜在编码来调节其权重或中间特征，使单个连续函数（即基于坐标的神经网络）能够表示多个数据实例。然而，最先进的调制方法的表达能力有限，因为它无法定位和捕获数据实体（如特定像素和光线）的细粒度细节。为了解决这个问题，我们提出了一个新的框架，将Transformer编码器与具有本地感知INR解码器相结合。Transformer编码器从数据实例中预测一组潜在令牌，将本地信息编码到每个潜在令牌中。具有本地感知的INR解码器通过交叉注意力有选择地聚合潜在令牌以获取坐标输入的调制向量，随后通过多个频带逐步解码以实现粗细调制。

    Generalizable implicit neural representation (INR) enables a single continuous function, i.e., a coordinate-based neural network, to represent multiple data instances by modulating its weights or intermediate features using latent codes. However, the expressive power of the state-of-the-art modulation is limited due to its inability to localize and capture fine-grained details of data entities such as specific pixels and rays. To address this issue, we propose a novel framework for generalizable INR that combines a transformer encoder with a locality-aware INR decoder. The transformer encoder predicts a set of latent tokens from a data instance to encode local information into each latent token. The locality-aware INR decoder extracts a modulation vector by selectively aggregating the latent tokens via cross-attention for a coordinate input and then predicts the output by progressively decoding with coarse-to-fine modulation through multiple frequency bandwidths. The selective token ag
    
[^132]: 带有异常值的三元数据聚类

    Clustering Three-Way Data with Outliers. (arXiv:2310.05288v1 [stat.ML])

    [http://arxiv.org/abs/2310.05288](http://arxiv.org/abs/2310.05288)

    这项研究提出了一种用于聚类矩阵形式数据的方法，可以处理其中的异常值。

    

    矩阵变量分布是模型聚类领域的最新添加，从而可以分析具有复杂结构（如图像和时间序列）的矩阵形式数据。由于其最近的出现，关于矩阵变量数据的文献有限，对于处理这些模型中的异常值的文献更少。本文讨论了一种用于聚类矩阵变量正态数据的方法。该方法使用子集对数似然的分布，将OCLUST算法扩展到矩阵变量正态数据，并使用迭代方法检测和剪裁异常值。

    Matrix-variate distributions are a recent addition to the model-based clustering field, thereby making it possible to analyze data in matrix form with complex structure such as images and time series. Due to its recent appearance, there is limited literature on matrix-variate data, with even less on dealing with outliers in these models. An approach for clustering matrix-variate normal data with outliers is discussed. The approach, which uses the distribution of subset log-likelihoods, extends the OCLUST algorithm to matrix-variate normal data and uses an iterative approach to detect and trim outliers.
    
[^133]: TEMPO: 基于提示的生成式预训练变换器模型用于时间序列预测

    TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting. (arXiv:2310.04948v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.04948](http://arxiv.org/abs/2310.04948)

    本文提出了一个新的框架 TEMPO，通过利用时间序列任务的两个重要归纳偏差，即将复杂交互分解和引入基于选择的提示来有效学习时间序列表示。

    

    在过去的十年中，深度学习在时间序列建模方面取得了显著进展。尽管在取得最先进的结果的同时，最好的架构在不同应用和领域之间差异很大。与此同时，在自然语言处理方面，生成式预训练变换器(GPT)通过训练一个通用模型在各种文本数据集上展现出了令人印象深刻的性能。有趣的是，探索是否GPT类型的架构可以对时间序列产生有效的影响，捕捉其内在动态属性并显著提高准确性。在本文中，我们提出了一个新颖的框架TEMPO，可以有效地学习时间序列表示。我们专注于利用时间序列任务的两种重要归纳偏差来预训练模型：(i) 对趋势、季节和残差成分复杂交互的分解；和(ii) 提出基于选择的提示以便于非分布自适应。

    The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the selection-based prompts to facilitate distribution adaptation in non-
    
[^134]: DeepSpeed4Science计划：通过先进的AI系统技术实现大规模科学发现

    DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery through Sophisticated AI System Technologies. (arXiv:2310.04610v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.04610](http://arxiv.org/abs/2310.04610)

    DeepSpeed4Science计划旨在通过先进的AI系统技术加速科学发现，提供针对独特复杂性的解决方案，为自然科学带来重大进展。

    

    在即将到来的十年中，深度学习可能会彻底改变自然科学，增强我们对自然现象建模和预测的能力。这可能预示着科学探索的新时代，从药物开发到可再生能源等领域都将取得重大进展。为了回应这一呼吁，我们提出了DeepSpeed4Science计划（deepspeed4science.ai），旨在通过AI系统技术创新构建独特的能力，帮助领域专家解开今天最大的科学之谜。通过利用DeepSpeed当前的技术支柱（训练、推理和压缩）作为基础技术支持，DeepSpeed4Science将创建一套针对加速科学发现的AI系统技术，以应对其独特的复杂性，超越常见的用于加速通用大语言模型（LLM）的技术方法。在本文中，我们展示了DeepSpeed4Science在解决两个关键问题上的早期进展。

    In the upcoming decade, deep learning may revolutionize the natural sciences, enhancing our capacity to model and predict natural occurrences. This could herald a new era of scientific exploration, bringing significant advancements across sectors from drug development to renewable energy. To answer this call, we present DeepSpeed4Science initiative (deepspeed4science.ai) which aims to build unique capabilities through AI system technology innovations to help domain experts to unlock today's biggest science mysteries. By leveraging DeepSpeed's current technology pillars (training, inference and compression) as base technology enablers, DeepSpeed4Science will create a new set of AI system technologies tailored for accelerating scientific discoveries by addressing their unique complexity beyond the common technical approaches used for accelerating generic large language models (LLMs). In this paper, we showcase the early progress we made with DeepSpeed4Science in addressing two of the cri
    
[^135]: 超越均匀采样：使用不平衡数据集的离线强化学习

    Beyond Uniform Sampling: Offline Reinforcement Learning with Imbalanced Datasets. (arXiv:2310.04413v1 [cs.LG])

    [http://arxiv.org/abs/2310.04413](http://arxiv.org/abs/2310.04413)

    该论文提出了一种解决离线强化学习中不平衡数据集问题的方法，通过采样策略使策略只受``好数据"限制，而不是所有的动作，提高了离线RL算法的性能。

    

    离线策略学习旨在利用现有的轨迹数据集来学习决策策略，而无需收集额外的数据。与行为克隆等监督学习技术相比，使用强化学习（RL）的主要动机是找到一个比数据集中的轨迹达到更高平均收益的策略。然而，我们在经验上发现，当一个数据集被次优轨迹所主导时，当前最先进的离线RL算法在平均收益上没有显著提高。我们认为这是由于当前离线RL算法假设与数据集中的轨迹保持接近。如果数据集主要由次优轨迹组成，这个假设将强制策略模仿次优动作。我们通过提出一种采样策略来克服这个问题，使策略只受``好数据"限制，而不是所有的动作。

    Offline policy learning is aimed at learning decision-making policies using existing datasets of trajectories without collecting additional data. The primary motivation for using reinforcement learning (RL) instead of supervised learning techniques such as behavior cloning is to find a policy that achieves a higher average return than the trajectories constituting the dataset. However, we empirically find that when a dataset is dominated by suboptimal trajectories, state-of-the-art offline RL algorithms do not substantially improve over the average return of trajectories in the dataset. We argue this is due to an assumption made by current offline RL algorithms of staying close to the trajectories in the dataset. If the dataset primarily consists of sub-optimal trajectories, this assumption forces the policy to mimic the suboptimal actions. We overcome this issue by proposing a sampling strategy that enables the policy to only be constrained to ``good data" rather than all actions in t
    
[^136]: 关于训练导数约束神经网络

    On Training Derivative-Constrained Neural Networks. (arXiv:2310.01649v1 [cs.LG])

    [http://arxiv.org/abs/2310.01649](http://arxiv.org/abs/2310.01649)

    该论文研究了导数约束神经网络（DC NN），提出了一种整合RELU激活函数（IReLU）来改进DC NN的训练，并探究了去归一化和标签缩放对于稳定DC训练的作用。研究还评估了这些方法在物理相关设置中的效果，证明了使用IReLU激活函数、去归一化和标签缩放的现有架构更好地融入了导数约束提供的训练信号。

    

    我们将神经网络对输入的预测相对于输入的（部分）导数作为额外的训练信号的情况称之为导数约束神经网络（DC NN）。这种情况在自然科学中的物理相关设置中很常见。我们提出了一种整合RELU (IReLU)激活函数，以改进DC NN的训练。我们还研究了去归一化和标签缩放以帮助稳定DC训练。我们在包括量子化学和科学机器学习（SciML）任务在内的物理相关设置上评估了我们的方法。我们证明了使用IReLU激活函数结合去归一化和标签缩放的现有架构更好地融入了导数约束提供的训练信号。

    We refer to the setting where the (partial) derivatives of a neural network's (NN's) predictions with respect to its inputs are used as additional training signal as a derivative-constrained (DC) NN. This situation is common in physics-informed settings in the natural sciences. We propose an integrated RELU (IReLU) activation function to improve training of DC NNs. We also investigate denormalization and label rescaling to help stabilize DC training. We evaluate our methods on physics-informed settings including quantum chemistry and Scientific Machine Learning (SciML) tasks. We demonstrate that existing architectures with IReLU activations combined with denormalization and label rescaling better incorporate training signal provided by derivative constraints.
    
[^137]: 神经网络的记忆化：超越最坏情况

    Memorization with neural nets: going beyond the worst case. (arXiv:2310.00327v1 [stat.ML])

    [http://arxiv.org/abs/2310.00327](http://arxiv.org/abs/2310.00327)

    本文研究了神经网络的插值问题，提出了一种简单的随机算法，在给定的数据集和两个类的情况下，能够以很高的概率构建一个插值的神经网络。这些结果与训练数据规模无关。

    

    在实践中，深度神经网络通常能够轻松地插值其训练数据。为了理解这一现象，许多研究都旨在量化神经网络架构的记忆能力：即在任意放置这些点并任意分配标签的情况下，架构能够插值的最大点数。然而，对于实际数据，人们直觉地期望存在一种良性结构，使得插值在比记忆能力建议的较小网络尺寸上已经发生。在本文中，我们通过采用实例特定的观点来研究插值。我们引入了一个简单的随机算法，它可以在多项式时间内给定一个固定的有限数据集和两个类的情况下，以很高的概率构建出一个插值三层神经网络。所需的参数数量与这两个类的几何特性及其相互排列有关。因此，我们获得了与训练数据规模无关的保证。

    In practice, deep neural networks are often able to easily interpolate their training data. To understand this phenomenon, many works have aimed to quantify the memorization capacity of a neural network architecture: the largest number of points such that the architecture can interpolate any placement of these points with any assignment of labels. For real-world data, however, one intuitively expects the presence of a benign structure so that interpolation already occurs at a smaller network size than suggested by memorization capacity. In this paper, we investigate interpolation by adopting an instance-specific viewpoint. We introduce a simple randomized algorithm that, given a fixed finite dataset with two classes, with high probability constructs an interpolating three-layer neural network in polynomial time. The required number of parameters is linked to geometric properties of the two classes and their mutual arrangement. As a result, we obtain guarantees that are independent of t
    
[^138]: 一个用于混合迪里切特和诺曼边界条件的神经预处理泊松求解器

    A Neural-preconditioned Poisson Solver for Mixed Dirichlet and Neumann Boundary Conditions. (arXiv:2310.00177v1 [math.NA])

    [http://arxiv.org/abs/2310.00177](http://arxiv.org/abs/2310.00177)

    我们引入了一个用于混合边界条件的泊松方程的神经预处理迭代求解器，核心是一个神经网络，能够近似逆离散结构网格拉普拉斯算子，并且在训练集之外的边界条件下仍然有效。

    

    我们引入了一个神经预处理的迭代求解器，用于具有混合边界条件的泊松方程。泊松方程在科学计算中是普遍存在的：它控制着广泛的物理现象，在许多数值算法中作为子问题出现，并且作为更广泛的椭圆PDE类的模型问题。最流行的泊松离散化方法可以产生大型稀疏线性系统。在高分辨率和对性能至关重要的应用中，迭代求解器结合强大的预处理器可以提供优势。我们求解器的核心是一个神经网络，该网络经过训练可以近似离散结构网格拉普拉斯算子的逆算子，适用于任意形状的域和混合边界条件。我们展示了该问题的结构激发了一种新颖的网络架构，即使在训练集之外的边界条件下，该架构也表现出高效的预处理器。我们展示了在具有挑战性的测试案例上的效果。

    We introduce a neural-preconditioned iterative solver for Poisson equations with mixed boundary conditions. The Poisson equation is ubiquitous in scientific computing: it governs a wide array of physical phenomena, arises as a subproblem in many numerical algorithms, and serves as a model problem for the broader class of elliptic PDEs. The most popular Poisson discretizations yield large sparse linear systems. At high resolution, and for performance-critical applications, iterative solvers can be advantageous for these -- but only when paired with powerful preconditioners. The core of our solver is a neural network trained to approximate the inverse of a discrete structured-grid Laplace operator for a domain of arbitrary shape and with mixed boundary conditions. The structure of this problem motivates a novel network architecture that we demonstrate is highly effective as a preconditioner even for boundary conditions outside the training set. We show that on challenging test cases aris
    
[^139]: 用于开发协作分布式机器学习系统的设计工具箱

    A Design Toolbox for the Development of Collaborative Distributed Machine Learning Systems. (arXiv:2309.16584v1 [cs.MA])

    [http://arxiv.org/abs/2309.16584](http://arxiv.org/abs/2309.16584)

    我们开发了一个CDML设计工具箱，可以指导开发者设计满足用例要求的协作分布式机器学习系统。

    

    为了在保护机器学习模型的机密性的同时利用来自多方的训练数据对模型进行充分训练，研究人员开发了各种协作分布式机器学习（CDML）系统设计，例如辅助学习、联邦学习和分裂学习。CDML系统设计展示了不同的特征，例如高度的代理人自治性、机器学习模型的机密性和容错性。面对不同特征的各种CDML系统设计，开发者很难有针对性地设计满足用例要求的CDML系统。然而，不合适的CDML系统设计可能导致CDML系统无法实现其预期目的。我们开发了一个CDML设计工具箱，可以指导CDML系统的开发。基于CDML设计工具箱，我们提出了具有不同关键特征的CDML系统典型，可以支持设计满足用例要求的CDML系统。

    To leverage training data for the sufficient training of ML models from multiple parties in a confidentiality-preserving way, various collaborative distributed machine learning (CDML) system designs have been developed, for example, to perform assisted learning, federated learning, and split learning. CDML system designs show different traits, for example, high agent autonomy, machine learning (ML) model confidentiality, and fault tolerance. Facing a wide variety of CDML system designs with different traits, it is difficult for developers to design CDML systems with traits that match use case requirements in a targeted way. However, inappropriate CDML system designs may result in CDML systems failing their envisioned purposes. We developed a CDML design toolbox that can guide the development of CDML systems. Based on the CDML design toolbox, we present CDML system archetypes with distinct key traits that can support the design of CDML systems to meet use case requirements.
    
[^140]: 无人机激光雷达和航拍图像在树木检测中的综述

    A Comprehensive Review on Tree Detection Methods Using Point Cloud and Aerial Imagery from Unmanned Aerial Vehicles. (arXiv:2309.16375v2 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2309.16375](http://arxiv.org/abs/2309.16375)

    本文综述了无人机在树木检测中的应用方法，包括激光雷达和航拍图像的点云和图像数据。通过对这些方法的分类和对比分析，总结了它们的性能、优势和应用场景。

    

    无人机是具有高效低成本和灵活应用场景的前沿技术。虽然许多文章已经对无人机在农业中的应用进行了综述，但对于树木检测的综述还不足。本文主要关注应用于无人机数据的树木检测方法，其中包括由激光雷达传感器获取的点云数据和由相机获取的图像数据。针对使用点云数据的检测方法，本文主要根据使用激光雷达和数字航空摄影术（DAP）的方法进行分类。而对于直接使用图像的检测方法，本文则根据是否使用深度学习（DL）方法进行综述。综述总结并分析了基于激光雷达和DAP的点云数据应用之间的比较和组合，以及它们的性能、优势和应用领域。

    Unmanned Aerial Vehicles (UAVs) are considered cutting-edge technology with highly cost-effective and flexible usage scenarios. Although many papers have reviewed the application of UAVs in agriculture, the review of the application for tree detection is still insufficient. This paper focuses on tree detection methods applied to UAV data collected by UAVs. There are two kinds of data, the point cloud and the images, which are acquired by the Light Detection and Ranging (LiDAR) sensor and camera, respectively. Among the detection methods using point-cloud data, this paper mainly classifies these methods according to LiDAR and Digital Aerial Photography (DAP). For the detection methods using images directly, this paper reviews these methods by whether or not to use the Deep Learning (DL) method. Our review concludes and analyses the comparison and combination between the application of LiDAR-based and DAP-based point cloud data. The performance, relative merits, and application fields of
    
[^141]: 有限标量量化: 简化 VQ-VAE 方法

    Finite Scalar Quantization: VQ-VAE Made Simple. (arXiv:2309.15505v1 [cs.CV])

    [http://arxiv.org/abs/2309.15505](http://arxiv.org/abs/2309.15505)

    该论文提出了有限标量量化 (FSQ) 方法，用来简化 VQ-VAE 方法中的向量量化 (VQ)。通过投影和量化 VAE 表示，我们得到与 VQ 相同大小的码本。在这种离散表示上，我们可以训练相同的模型，并在图像生成、多模态生成和计算机视觉任务中取得竞争性能。

    

    我们提出用有限标量量化 (FSQ) 替代 VQ-VAE 潜在表示中的向量量化 (VQ)。在 FSQ 中，我们将 VAE 表示投影到几个维度 (通常少于10个)，每个维度被量化为一组固定的值，从而形成一个（隐式的）码本，由这些值的乘积组成。通过合适地选择维度和每个维度可以取的值的数量，我们获得与 VQ 中相同的码本大小。在这样的离散表示上，我们可以训练已经在 VQ-VAE 表示上训练过的相同模型，例如用于图像生成、多模态生成和密集预测计算机视觉任务的自回归和掩码变换器模型。具体而言，我们在图像生成中使用 FSQ 和 MaskGIT，在深度估计、着色和全景分割中使用 FSQ 和 UViM。尽管 FSQ 的设计要简单得多，我们在所有这些任务中获得了有竞争力的性能。

    We propose to replace vector quantization (VQ) in the latent representation of VQ-VAEs with a simple scheme termed finite scalar quantization (FSQ), where we project the VAE representation down to a few dimensions (typically less than 10). Each dimension is quantized to a small set of fixed values, leading to an (implicit) codebook given by the product of these sets. By appropriately choosing the number of dimensions and values each dimension can take, we obtain the same codebook size as in VQ. On top of such discrete representations, we can train the same models that have been trained on VQ-VAE representations. For example, autoregressive and masked transformer models for image generation, multimodal generation, and dense prediction computer vision tasks. Concretely, we employ FSQ with MaskGIT for image generation, and with UViM for depth estimation, colorization, and panoptic segmentation. Despite the much simpler design of FSQ, we obtain competitive performance in all these tasks. W
    
[^142]: 在在线CMDPs中，无模型、遗憾最优的最佳策略识别

    Model-Free, Regret-Optimal Best Policy Identification in Online CMDPs. (arXiv:2309.15395v1 [cs.LG])

    [http://arxiv.org/abs/2309.15395](http://arxiv.org/abs/2309.15395)

    本文提出了一种无模型的算法，名为PRI，用于在线CMDPs中的最佳策略识别问题。该算法基于CMDPs的有限随机性属性，能够以低遗憾并以高概率识别出最优策略。

    

    本文考虑了在线约束马尔科夫决策过程（CMDPs）中的最佳策略识别（BPI）问题。我们对具有低遗憾并且以高概率识别最优策略的无模型算法感兴趣。现有的在线CMDPs的无模型算法在次线性遗憾和违约时没有提供任何对最优策略的收敛保证，并且只在从以前使用的策略中随机均匀抽样时提供平均性能保证。本文提出了一种新的算法，名为PRUNING-REFINEMENT-IDENTIFICATION（PRI），基于我们发现的CMDPs的一个基本结构性质，称为有限随机性。该属性表明对于具有N约束的CMDP，存在一个最优策略，其中至多有N个随机决策。所提出的算法首先识别出在哪个步骤和哪个状态需要进行随机决策，然后对这些决策的分布进行微调。

    This paper considers the best policy identification (BPI) problem in online Constrained Markov Decision Processes (CMDPs). We are interested in algorithms that are model-free, have low regret, and identify an optimal policy with a high probability. Existing model-free algorithms for online CMDPs with sublinear regret and constraint violation do not provide any convergence guarantee to an optimal policy and provide only average performance guarantees when a policy is uniformly sampled at random from all previously used policies. In this paper, we develop a new algorithm, named Pruning-Refinement-Identification (PRI), based on a fundamental structural property of CMDPs we discover, called limited stochasticity. The property says for a CMDP with $N$ constraints, there exists an optimal policy with at most $N$ stochastic decisions.  The proposed algorithm first identifies at which step and in which state a stochastic decision has to be taken and then fine-tunes the distributions of these s
    
[^143]: PRiSM: 使用关系感知分数校准增强低资源文档级关系抽取

    PRiSM: Enhancing Low-Resource Document-Level Relation Extraction with Relation-Aware Score Calibration. (arXiv:2309.13869v1 [cs.CL] CROSS LISTED)

    [http://arxiv.org/abs/2309.13869](http://arxiv.org/abs/2309.13869)

    PRiSM是一种增强低资源文档级关系抽取的方法，通过关系感知分数校准来提高模型性能，成功地降低了在低资源环境下训练模型时的校准误差。

    

    文档级关系抽取（DocRE）旨在提取文档中所有实体对的关系。在DocRE中的一个关键挑战是注释这类数据的成本，需要大量的人力投入。因此，我们调查了低资源环境中的DocRE情况，并发现现有的在少量数据上训练的模型过高估计了NA（"no relation"）标签，导致性能受限。在这项工作中，我们从校准的角度来解决这个问题，提出了PRiSM，它可以根据关系语义信息来适应logits。我们在三个DocRE数据集上评估了我们的方法，并证明了将现有模型与PRiSM集成可以提高性能，F1分数提高了26.38%，而当用约3%的数据进行训练时，校准误差下降了36倍。代码可以在https://github.com/brightjade/PRiSM公开获取。

    Document-level relation extraction (DocRE) aims to extract relations of all entity pairs in a document. A key challenge in DocRE is the cost of annotating such data which requires intensive human effort. Thus, we investigate the case of DocRE in a low-resource setting, and we find that existing models trained on low data overestimate the NA ("no relation") label, causing limited performance. In this work, we approach the problem from a calibration perspective and propose PRiSM, which learns to adapt logits based on relation semantic information. We evaluate our method on three DocRE datasets and demonstrate that integrating existing models with PRiSM improves performance by as much as 26.38 F1 score, while the calibration error drops as much as 36 times when trained with about 3% of data. The code is publicly available at https://github.com/brightjade/PRiSM.
    
[^144]: MINT: 评估在与工具和语言反馈进行多轮交互中的LLMs的能力

    MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback. (arXiv:2309.10691v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.10691](http://arxiv.org/abs/2309.10691)

    MINT是一个评估LLMs在多轮交互中解决任务能力的基准，通过使用工具和利用用户的自然语言反馈。它解决了当前评估协议忽略细致互动和低估自然语言反馈的问题，促进了研究基准评估和实际应用之间的一致性。

    

    为了解决复杂任务，大语言模型（LLMs）通常需要与用户进行多轮交互，有时候辅以外部工具的帮助。然而，当前的评估协议常常强调用单轮交流的基准性能，忽略了用户、LLMs和外部工具之间的细致互动，并低估了用户的自然语言反馈的重要性。这些疏忽导致了研究基准评估结果与实际应用情况之间的差异。我们引入了MINT，这是一个通过使用工具和利用用户的自然语言反馈来评估LLMs解决多轮交互任务能力的基准。为了保证可重复性，我们提供了一个评估框架，在这个框架中，LLMs可以通过执行Python代码来访问工具，并接收由GPT-4模拟的用户的自然语言反馈。我们重新利用了一系列多样的已建立评估数据集，重点关注推理、编码和决策方面。

    To solve complex tasks, large language models (LLMs) often require multiple rounds of interactions with the user, sometimes assisted by external tools. However, current evaluation protocols often emphasize benchmark performance with single-turn exchanges, neglecting the nuanced interactions among the user, LLMs, and external tools, while also underestimating the importance of natural language feedback from users. These oversights contribute to discrepancies between research benchmark evaluations and real-world use cases. We introduce MINT, a benchmark that evaluates LLMs' ability to solve tasks with multi-turn interactions by (1) using tools and (2) leveraging natural language feedback. To ensure reproducibility, we provide an evaluation framework where LLMs can access tools by executing Python code and receive users' natural language feedback simulated by GPT-4. We repurpose a diverse set of established evaluation datasets focusing on reasoning, coding, and decision-making and careful
    
[^145]: 关于正则稀疏逻辑回归的研究

    On Regularized Sparse Logistic Regression. (arXiv:2309.05925v1 [cs.LG])

    [http://arxiv.org/abs/2309.05925](http://arxiv.org/abs/2309.05925)

    本文提出了解决正则稀疏逻辑回归的方法，包括$\ell_1$正则化稀疏逻辑回归和一些满足先决条件的非凸惩罚正则化稀疏逻辑回归。经验实验表明，这些算法能够以较低的计算成本有效地进行分类和特征选择。

    

    稀疏逻辑回归旨在同时进行高维数据的分类和特征选择。虽然有许多研究解决了$\ell_1$正则化逻辑回归问题，但对于与非凸惩罚相关的稀疏逻辑回归解决方案并没有等量的文献。本文提出了解决$\ell_1$正则化稀疏逻辑回归和一些满足一定先决条件的非凸惩罚正则化稀疏逻辑回归的方法，并采用类似的优化框架。在提出的优化框架中，我们利用不同的线搜索准则来保证不同正则化项的良好收敛性能。通过对真实世界数据集的二元分类任务进行经验实验，我们证明了我们提出的算法能够以较低的计算成本有效地进行分类和特征选择。

    Sparse logistic regression aims to perform classification and feature selection simultaneously for high-dimensional data. Although many studies have been done to solve $\ell_1$-regularized logistic regression, there is no equivalently abundant literature about solving sparse logistic regression associated with nonconvex penalties. In this paper, we propose to solve $\ell_1$-regularized sparse logistic regression and some nonconvex penalties-regularized sparse logistic regression, when the nonconvex penalties satisfy some prerequisites, with similar optimization frameworks. In the proposed optimization frameworks, we utilize different line search criteria to guarantee good convergence performance for different regularization terms. Empirical experiments on binary classification tasks with real-world datasets demonstrate our proposed algorithms are capable of performing classification and feature selection effectively with a lower computational cost.
    
[^146]: DePT:分解提示调整以实现参数高效微调

    DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning. (arXiv:2309.05173v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.05173](http://arxiv.org/abs/2309.05173)

    DePT通过将软提示分解为较短的软提示和一对低秩矩阵，并用两个不同的学习率来优化，以解决提示调整对训练和推理时间以及内存使用的影响，从而实现更好的性能。

    

    提示调整（PT）是一种将可训练的少量软提示向量附加到语言模型（LM）输入中的参数高效微调（PEFT）方法，已在各种任务和模型中显示出了有希望的结果。 与其他PEFT方法相比，PT的竞争性能可以在可训练参数更少的情况下保持，并且随着模型规模的扩大，其参数并不会显著增加。 但是，PT引入了额外的软提示标记，导致输入序列变长，这对于Transformer的二次复杂度而言，在训练和推理时间以及内存使用方面会产生显著影响。 这对于面临大量每日查询的大型语言模型（LLMs）尤其令人担忧。

    Prompt tuning (PT), where a small amount of trainable soft (continuous) prompt vectors is affixed to the input of language models (LM), has shown promising results across various tasks and models for parameter-efficient fine-tuning (PEFT). PT stands out from other PEFT approaches because it maintains competitive performance with fewer trainable parameters and does not drastically scale up its parameters as the model size expands. However, PT introduces additional soft prompt tokens, leading to longer input sequences, which significantly impacts training and inference time and memory usage due to the Transformer's quadratic complexity. Particularly concerning for Large Language Models (LLMs) that face heavy daily querying. To address this issue, we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt into a shorter soft prompt and a pair of low-rank matrices that are then optimised with two different learning rates. This allows DePT to achieve better performance whi
    
[^147]: 带有力响应式运动控制的导盲四足机器人导航

    Seeing-Eye Quadruped Navigation with Force Responsive Locomotion Control. (arXiv:2309.04370v1 [cs.RO])

    [http://arxiv.org/abs/2309.04370](http://arxiv.org/abs/2309.04370)

    本研究提出了一种带有力响应式运动控制的导盲四足机器人导航系统，通过同时训练运动控制器和外力估计器，实现对外部拉扯力的稳健感知和响应。实验结果表明该系统能够准确导航并绕过障碍物，具有较强的鲁棒性。

    

    导盲机器人是为导引视障人士而设计的非常有用的工具，鉴于真正导盲犬的可获得性低且高成本，其可能产生巨大的社会影响。尽管已经演示了几个导盲机器人系统，但没有考虑到导盲犬实际环境中经常发生的来自人类的外部拉扯。本文通过强化学习（RL）同时训练了一个对外部拉扯力具有鲁棒性的运动控制器和一个通过监督学习的外力估计器。控制器确保了稳定的行走，外力估计器使机器人能够对人类施加的外部力做出响应。这些力用于将机器人引导到未知的全局目标，同时机器人通过局部规划器将人类引导绕过附近的障碍物。在模拟和硬件上的实验结果表明，我们的控制器对外部力具有鲁棒性，并且我们的导盲系统能够准确检测...

    Seeing-eye robots are very useful tools for guiding visually impaired people, potentially producing a huge societal impact given the low availability and high cost of real guide dogs. Although a few seeing-eye robot systems have already been demonstrated, none considered external tugs from humans, which frequently occur in a real guide dog setting. In this paper, we simultaneously train a locomotion controller that is robust to external tugging forces via Reinforcement Learning (RL), and an external force estimator via supervised learning. The controller ensures stable walking, and the force estimator enables the robot to respond to the external forces from the human. These forces are used to guide the robot to the global goal, which is unknown to the robot, while the robot guides the human around nearby obstacles via a local planner. Experimental results in simulation and on hardware show that our controller is robust to external forces, and our seeing-eye system can accurately detect
    
[^148]: 纯蒙特卡洛反事实遗憾最小化

    Pure Monte Carlo Counterfactual Regret Minimization. (arXiv:2309.03084v1 [cs.AI])

    [http://arxiv.org/abs/2309.03084](http://arxiv.org/abs/2309.03084)

    纯蒙特卡洛反事实遗憾最小化算法（PCFR）是一种结合了反事实遗憾最小化（CFR）和虚拟游戏（FP）概念的新算法，能够与各种CFR变体相结合，包括蒙特卡洛CFR（MCCFR）。PCFR具有更好的性能和较快的收敛速度，同时降低了时间和空间复杂度。

    

    反事实遗憾最小化（CFR）及其变体是目前解决大规模不完全信息博弈的最佳算法。本文在CFR的基础上提出了一种名为纯CFR（PCFR）的新算法，以实现更好的性能。PCFR可以看作是CFR和虚拟游戏（FP）的结合，继承了CFR的反事实遗憾（值）的概念，并在下一次迭代中使用最佳响应策略而不是遗憾匹配策略。我们的理论证明了PCFR可以实现Blackwell可达性，使PCFR能够与包括蒙特卡洛CFR（MCCFR）在内的任何CFR变体相结合。由此产生的纯MCCFR（PMCCFR）可以大大降低时间和空间复杂度。特别地，PMCCFR的收敛速度至少比MCCFR快三倍。此外，由于PMCCFR不通过严格被支配策略的路径，我们开发了一种新的启动算法，受到了严格被支配策略的启示。

    Counterfactual Regret Minimization (CFR) and its variants are the best algorithms so far for solving large-scale incomplete information games. Building upon CFR, this paper proposes a new algorithm named Pure CFR (PCFR) for achieving better performance. PCFR can be seen as a combination of CFR and Fictitious Play (FP), inheriting the concept of counterfactual regret (value) from CFR, and using the best response strategy instead of the regret matching strategy for the next iteration. Our theoretical proof that PCFR can achieve Blackwell approachability enables PCFR's ability to combine with any CFR variant including Monte Carlo CFR (MCCFR). The resultant Pure MCCFR (PMCCFR) can significantly reduce time and space complexity. Particularly, the convergence speed of PMCCFR is at least three times more than that of MCCFR. In addition, since PMCCFR does not pass through the path of strictly dominated strategies, we developed a new warm-start algorithm inspired by the strictly dominated strat
    
[^149]: 通过平坦极小值和对抗鲁棒性解释激活稀疏性的理论解释

    Theoretical Explanation of Activation Sparsity through Flat Minima and Adversarial Robustness. (arXiv:2309.03004v1 [cs.LG])

    [http://arxiv.org/abs/2309.03004](http://arxiv.org/abs/2309.03004)

    提出了一个理论解释，将梯度稀疏性和激活稀疏性解释为对抗性鲁棒性的必要步骤，以隐藏特征和参数而言，这大致等于对学习良好模型的极小值平坦性。

    

    最近对MLP层中的激活稀疏性的实证观察为大幅降低计算成本提供了机会。尽管有几项研究将其归因于训练动力学，但激活稀疏性的理论解释仅限于浅层网络、小训练步长以及修改的训练，尽管这种稀疏性已在通过vanilla协议进行大步骤训练的深层模型中被发现。为了填补这三个差距，我们提出了梯度稀疏性的概念作为激活稀疏性的源头，并基于此提出了一个理论解释，该解释将梯度稀疏性和激活稀疏性解释为对抗性鲁棒性的必要步骤，以隐藏特征和参数而言，这大致等于对学习良好模型的极小值平坦性。这个理论适用于经过LayerNorm标准训练的纯MLP，并且如果在训练过程中给权重添加噪声，还适用于Transformers或其他架构。为了消除其他来源的激活稀疏性，我们还进行了进一步的实证研究。

    A recent empirical observation of activation sparsity in MLP layers offers an opportunity to drastically reduce computation costs for free. Despite several works attributing it to training dynamics, the theoretical explanation of activation sparsity's emergence is restricted to shallow networks, small training steps well as modified training, even though the sparsity has been found in deep models trained by vanilla protocols for large steps. To fill the three gaps, we propose the notion of gradient sparsity as the source of activation sparsity and a theoretical explanation based on it that explains gradient sparsity and then activation sparsity as necessary steps to adversarial robustness w.r.t. hidden features and parameters, which is approximately the flatness of minima for well-learned models. The theory applies to standardly trained LayerNorm-ed pure MLPs, and further to Transformers or other architectures if noises are added to weights during training. To eliminate other sources o
    
[^150]: PromptTTS 2: 使用文本提示描述和生成声音

    PromptTTS 2: Describing and Generating Voices with Text Prompt. (arXiv:2309.02285v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2309.02285](http://arxiv.org/abs/2309.02285)

    PromptTTS 2是一种使用文本提示来描述和生成声音的方法，通过变化网络提供声音的可变性信息，并利用大型语言模型（LLM）来生成高质量的文本提示。

    

    语音传达的信息比文字更丰富，因为相同的词可以以不同的声音表达不同的信息。与依赖语音提示（参考语音）来实现声音可变性的传统文本转语音（TTS）方法相比，使用文本提示（描述）更加用户友好，因为语音提示可能难以找到或根本不存在。基于文本提示的TTS方法面临两个主要挑战：1）一对多问题，即文本提示无法描述声音可变性的所有细节；2）文本提示数据集的有限可用性，需要供应商和大量数据标注成本来编写语音的文本提示。在这项工作中，我们介绍了PromptTTS 2来解决这些挑战，该系统使用变化网络提供文本提示无法捕捉的声音可变性信息，并使用大型语言模型（LLM）来生成高质量的文本提示。

    Speech conveys more information than text, as the same word can be uttered in various voices to convey diverse information. Compared to traditional text-to-speech (TTS) methods relying on speech prompts (reference speech) for voice variability, using text prompts (descriptions) is more user-friendly since speech prompts can be hard to find or may not exist at all. TTS approaches based on the text prompt face two main challenges: 1) the one-to-many problem, where not all details about voice variability can be described in the text prompt, and 2) the limited availability of text prompt datasets, where vendors and large cost of data labeling are required to write text prompts for speech. In this work, we introduce PromptTTS 2 to address these challenges with a variation network to provide variability information of voice not captured by text prompts, and a prompt generation pipeline to utilize the large language models (LLM) to compose high quality text prompts. Specifically, the variatio
    
[^151]: 孟加拉文档布局分析-一种基于YOLOv8的集成方法

    Bengali Document Layout Analysis -- A YOLOV8 Based Ensembling Approach. (arXiv:2309.00848v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.00848](http://arxiv.org/abs/2309.00848)

    本文提出了一种基于YOLOv8模型和创新的后处理技术的孟加拉文档布局分析方法，通过数据增强和两阶段预测策略实现了准确的元素分割。该方法优于单个基础架构，并解决了BaDLAD数据集中的问题，有助于提高OCR和文档理解能力。

    

    本文侧重于利用YOLOv8模型和创新的后处理技术提升孟加拉文档布局分析（DLA）。我们通过数据增强以应对孟加拉复杂文字独特的挑战，经过严格的验证集评估，对完整数据集进行微调，实现准确的元素分割的两阶段预测策略。我们的集成模型结合后处理性能优于单个基础架构，解决了BaDLAD数据集中的问题。通过利用这种方法，我们旨在推动孟加拉文档分析的发展，提高OCR和文档理解能力，同时BaDLAD作为基础资源有助于未来的研究。此外，我们的实验为将新策略纳入现有解决方案提供了关键见解。

    This paper focuses on enhancing Bengali Document Layout Analysis (DLA) using the YOLOv8 model and innovative post-processing techniques. We tackle challenges unique to the complex Bengali script by employing data augmentation for model robustness. After meticulous validation set evaluation, we fine-tune our approach on the complete dataset, leading to a two-stage prediction strategy for accurate element segmentation. Our ensemble model, combined with post-processing, outperforms individual base architectures, addressing issues identified in the BaDLAD dataset. By leveraging this approach, we aim to advance Bengali document analysis, contributing to improved OCR and document comprehension and BaDLAD serves as a foundational resource for this endeavor, aiding future research in the field. Furthermore, our experiments provided key insights to incorporate new strategies into the established solution.
    
[^152]: 使用基于图的多智能体强化学习学习协作信息传播

    Learning Collaborative Information Dissemination with Graph-based Multi-Agent Reinforcement Learning. (arXiv:2308.16198v1 [cs.LG])

    [http://arxiv.org/abs/2308.16198](http://arxiv.org/abs/2308.16198)

    本论文介绍了一种使用多智能体强化学习的方法来实现协作信息传播。通过提出分布式POMDP形式，在消息转发上实现了每个智能体的独立决策，相比传统的基于多点中继选择的启发式方法具有重大创新和贡献。同时，该方法利用图卷积强化学习和动态注意力机制捕捉关键网络特征，并提出了不同信息交换方式的两种方法进行评估。

    

    在现代通信系统中，高效可靠的信息传播对支持关键操作至关重要，如灾难响应、自动驾驶车辆和传感器网络。本文介绍了一种多智能体强化学习（MARL）方法，作为实现更为分散、高效和协作解决方案的重要进展。我们提出了一种用于信息传播的分布式POMDP（Decentralized-POMDP）形式，使得每个智能体可以独立决定消息的转发。这构成了一种从传统基于多点中继（MPR）选择的启发式方法的重大范式转移。我们的方法利用图卷积强化学习，采用具有动态注意力的图注意力网络（GAT）来捕捉关键网络特征。我们提出了两种方法，L-DGN和HL-DGN，它们在智能体之间交换的信息上有所不同。通过将我们的分散方法与基于MPR的方法进行比较，我们评估了其性能。

    In modern communication systems, efficient and reliable information dissemination is crucial for supporting critical operations across domains like disaster response, autonomous vehicles, and sensor networks. This paper introduces a Multi-Agent Reinforcement Learning (MARL) approach as a significant step forward in achieving more decentralized, efficient, and collaborative solutions. We propose a Decentralized-POMDP formulation for information dissemination, empowering each agent to independently decide on message forwarding. This constitutes a significant paradigm shift from traditional heuristics based on Multi-Point Relay (MPR) selection. Our approach harnesses Graph Convolutional Reinforcement Learning, employing Graph Attention Networks (GAT) with dynamic attention to capture essential network features. We propose two approaches, L-DGN and HL-DGN, which differ in the information that is exchanged among agents. We evaluate the performance of our decentralized approaches, by compari
    
[^153]: 用于稀疏深度神经网络训练的多目标优化

    Multi-Objective Optimization for Sparse Deep Neural Network Training. (arXiv:2308.12243v1 [cs.LG])

    [http://arxiv.org/abs/2308.12243](http://arxiv.org/abs/2308.12243)

    这项工作提出了一种用于稀疏深度神经网络训练的多目标优化算法，通过加权Chebyshev标量化和增广Lagrangian方法，解决了同时优化多个任务时的挑战。

    

    在深度学习的各种场景中，会自然地出现不同的冲突优化准则。这些准则可以解决不同的主任务（如多任务学习设置），也可以解决主要任务和次要任务，例如损失最小化与稀疏性。通常的方法是简单地加权准则，但在凸设置中才有效。本文提出了一种多目标优化算法，对多任务深度神经网络（DNNs）进行训练，使用改进的加权Chebyshev标量化方法。通过使用这种标量化技术，算法可以识别原始问题的所有最优解，同时将其复杂性降低为一系列单目标问题。然后，使用增广Lagrangian方法来解决简化后的问题，从而可以使用常见的优化技术，如Adam和随机梯度下降，同时有效地处理约束条件。我们的工作旨在解决经济化问题。

    Different conflicting optimization criteria arise naturally in various Deep Learning scenarios. These can address different main tasks (i.e., in the setting of Multi-Task Learning), but also main and secondary tasks such as loss minimization versus sparsity. The usual approach is a simple weighting of the criteria, which formally only works in the convex setting. In this paper, we present a Multi-Objective Optimization algorithm using a modified Weighted Chebyshev scalarization for training Deep Neural Networks (DNNs) with respect to several tasks. By employing this scalarization technique, the algorithm can identify all optimal solutions of the original problem while reducing its complexity to a sequence of single-objective problems. The simplified problems are then solved using an Augmented Lagrangian method, enabling the use of popular optimization techniques such as Adam and Stochastic Gradient Descent, while efficaciously handling constraints. Our work aims to address the (economi
    
[^154]: LLM4TS:使用预训练的LLM进行两阶段微调用于时间序列预测

    LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs. (arXiv:2308.08469v1 [cs.LG])

    [http://arxiv.org/abs/2308.08469](http://arxiv.org/abs/2308.08469)

    这项工作提出了LLM4TS方法，利用预训练的LLMs增强时间序列预测能力。通过两阶段微调和参数高效微调，提高了LLMs处理时间序列数据的能力。

    

    在这项工作中，我们利用预训练的大型语言模型（LLMs）来增强时间序列预测。借鉴了自然语言处理和计算机视觉统一模型的日益增长的兴趣，我们设想创建一个类似的模型用于长期时间序列预测。由于缺乏大规模的时间序列数据来构建稳健的基础模型，我们的方法LLM4TS专注于利用预训练的LLMs的优势。通过将时间序列修补与时间编码相结合，我们提高了LLMs处理时间序列数据的能力。受到聊天机器人领域的有监督微调的启发，我们优先进行两阶段的微调过程：首先进行有监督微调以使LLMs适应时间序列数据，然后进行任务特定的下游微调。此外，为了在不进行大量参数调整的情况下发挥预训练LLMs的灵活性，我们采用了几种参数高效微调（PEFT）技术。

    In this work, we leverage pre-trained Large Language Models (LLMs) to enhance time-series forecasting. Mirroring the growing interest in unifying models for Natural Language Processing and Computer Vision, we envision creating an analogous model for long-term time-series forecasting. Due to limited large-scale time-series data for building robust foundation models, our approach LLM4TS focuses on leveraging the strengths of pre-trained LLMs. By combining time-series patching with temporal encoding, we have enhanced the capability of LLMs to handle time-series data effectively. Inspired by the supervised fine-tuning in chatbot domains, we prioritize a two-stage fine-tuning process: first conducting supervised fine-tuning to orient the LLM towards time-series data, followed by task-specific downstream fine-tuning. Furthermore, to unlock the flexibility of pre-trained LLMs without extensive parameter adjustments, we adopt several Parameter-Efficient Fine-Tuning (PEFT) techniques. Drawing o
    
[^155]: BarlowRL: Barlow Twins用于数据效率强化学习

    BarlowRL: Barlow Twins for Data-Efficient Reinforcement Learning. (arXiv:2308.04263v1 [cs.LG])

    [http://arxiv.org/abs/2308.04263](http://arxiv.org/abs/2308.04263)

    BarlowRL通过将Barlow Twins和DER相结合，实现了数据效率强化学习，并在Atari 100k基准测试上优于其他算法。它通过信息扩散避免了维度折叠，使得RL算法能够利用均匀分布的状态表示，从而实现卓越的性能。

    

    本文介绍了BarlowRL，一种将Barlow Twins自监督学习框架与DER（Data-Efficient Rainbow）算法相结合的数据效率强化学习代理。BarlowRL在Atari 100k基准测试中优于DER和对比算法CURL。BarlowRL通过确保信息扩散到整个空间来避免维度折叠。这有助于RL算法利用均匀分布的状态表示，最终实现卓越的性能。Barlow Twins与DER的整合增强了数据效率，并在RL任务中实现了卓越的性能。BarlowRL展示了将自监督学习技术纳入强化学习算法以提高性能的潜力。

    This paper introduces BarlowRL, a data-efficient reinforcement learning agent that combines the Barlow Twins self-supervised learning framework with DER (Data-Efficient Rainbow) algorithm. BarlowRL outperforms both DER and its contrastive counterpart CURL on the Atari 100k benchmark. BarlowRL avoids dimensional collapse by enforcing information spread to the whole space. This helps RL algorithms to utilize uniformly spread state representation that eventually results in a remarkable performance. The integration of Barlow Twins with DER enhances data efficiency and achieves superior performance in the RL tasks. BarlowRL demonstrates the potential of incorporating self-supervised learning techniques to improve RL algorithms.
    
[^156]: 深度神经网络架构的异步进化

    Asynchronous Evolution of Deep Neural Network Architectures. (arXiv:2308.04102v1 [cs.NE])

    [http://arxiv.org/abs/2308.04102](http://arxiv.org/abs/2308.04102)

    本文提出了一种通用的异步评估策略，用于增加进化神经网络架构搜索的吞吐量。该策略维护一个个体队列，并在适当数量的个体被评估后立即进入下一代，平衡多样性和效率。

    

    许多进化算法(EAs)利用候选解的并行评估。然而，如果评估时间差异很大，许多工作节点(即计算客户端)大部分时间都处于闲置状态，等待下一代的创建。进化神经网络架构搜索(ENAS)是一类优化深度神经网络架构和超参数的EA，特别容易受到这个问题的影响。本文提出了一种通用的异步评估策略(AES)，然后将其适配到ENAS上。AES通过维护一个多达$K$个个体的队列，这些个体已准备好被发送到工作器进行评估，并在由工作器评估了$M<<K$个个体之后立即进入下一代。合适的$M$值是通过实验确定的，平衡多样性和效率。为了展示AES的普适性和能力，首先在11位多路复用器设计(一个单个种群可验证的发现任务)上进行了评估。

    Many evolutionary algorithms (EAs) take advantage of parallel evaluation of candidates. However, if evaluation times vary significantly, many worker nodes (i.e.,\ compute clients) are idle much of the time, waiting for the next generation to be created. Evolutionary neural architecture search (ENAS), a class of EAs that optimizes the architecture and hyperparameters of deep neural networks, is particularly vulnerable to this issue. This paper proposes a generic asynchronous evaluation strategy (AES) that is then adapted to work with ENAS. AES increases throughput by maintaining a queue of upto $K$ individuals ready to be sent to the workers for evaluation and proceeding to the next generation as soon as $M<<K$ individuals have been evaluated by the workers. A suitable value for $M$ is determined experimentally, balancing diversity and efficiency. To showcase the generality and power of AES, it was first evaluated in 11-bit multiplexer design (a single-population verifiable discovery ta
    
[^157]: Nest-DGIL: Nesterov优化的深度几何增量学习用于CS图像重构

    Nest-DGIL: Nesterov-optimized Deep Geometric Incremental Learning for CS Image Reconstruction. (arXiv:2308.03807v1 [eess.IV])

    [http://arxiv.org/abs/2308.03807](http://arxiv.org/abs/2308.03807)

    提出了一种基于第二Nesterov近端梯度优化的深度几何增量学习框架，能够在图像重建时减轻伪影，同时实现快速收敛。

    

    近端梯度优化是解决图像反问题的常用策略之一，易于实现。然而，这些技术在图像重构中通常会产生严重的伪影。一种常用的改进方法是通过微调正则化参数来减轻这些伪影，但由于增加了计算成本，这种方法并不总是足够或适用。在这项工作中，我们提出了一种基于第二Nesterov近端梯度优化的深度几何增量学习框架。所提出的端到端网络不仅具有对高/低频图像特征的强大学习能力，还可以在初步线性重建中从理论上保证几何纹理细节的重建。此外，它可以避免中间重建结果超出几何分解域的风险，并实现快速收敛。我们的重建框架被分解为两个子问题:图像初步线性重建和深度增量学习。

    Proximal gradient-based optimization is one of the most common strategies for solving image inverse problems as well as easy to implement. However, these techniques often generate heavy artifacts in image reconstruction. One of the most popular refinement methods is to fine-tune the regularization parameter to alleviate such artifacts, but it may not always be sufficient or applicable due to increased computational costs. In this work, we propose a deep geometric incremental learning framework based on second Nesterov proximal gradient optimization. The proposed end-to-end network not only has the powerful learning ability for high/low frequency image features,but also can theoretically guarantee that geometric texture details will be reconstructed from preliminary linear reconstruction.Furthermore, it can avoid the risk of intermediate reconstruction results falling outside the geometric decomposition domains and achieve fast convergence. Our reconstruction framework is decomposed int
    
[^158]: 通过对比学习在强化学习中发现层次化成就

    Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning. (arXiv:2307.03486v1 [cs.LG])

    [http://arxiv.org/abs/2307.03486](http://arxiv.org/abs/2307.03486)

    通过对比学习方法，我们在强化学习中提出了新的成就蒸馏方法，可以加强代理对下一个解锁成就的预测能力，并优于先前的模型驱动和层次化方法。

    

    在生成环境中发现具有层次结构的成就是一个重大挑战。这需要智能体具备广泛的能力，包括泛化和长期推理。许多先前的方法基于模型驱动或层次化方法，认为显式的长期规划模块对于学习层次化成就是有益的。然而，这些方法需要大量的环境交互或大型模型，限制了它们的实用性。在这项工作中，我们发现近期实施实践中的近端策略优化（PPO）算法优于先前的方法。此外，我们发现PPO智能体可以在一定程度上预测下一个要解锁的成就，尽管预测的置信度较低。基于这一观察，我们提出了一种新颖的对比学习方法，称为成就蒸馏，可以加强PPO智能体对下一个解锁成就的预测能力。

    Discovering achievements with a hierarchical structure on procedurally generated environments poses a significant challenge. This requires agents to possess a broad range of abilities, including generalization and long-term reasoning. Many prior methods are built upon model-based or hierarchical approaches, with the belief that an explicit module for long-term planning would be beneficial for learning hierarchical achievements. However, these methods require an excessive amount of environment interactions or large model sizes, limiting their practicality. In this work, we identify that proximal policy optimization (PPO), a simple and versatile model-free algorithm, outperforms the prior methods with recent implementation practices. Moreover, we find that the PPO agent can predict the next achievement to be unlocked to some extent, though with low confidence. Based on this observation, we propose a novel contrastive learning method, called achievement distillation, that strengthens the 
    
[^159]: 用于超出分布可泛化性的大型视觉语言模型压缩

    Distilling Large Vision-Language Model with Out-of-Distribution Generalizability. (arXiv:2307.03135v1 [cs.CV])

    [http://arxiv.org/abs/2307.03135](http://arxiv.org/abs/2307.03135)

    本文研究了针对大型视觉语言模型的模型压缩方法，将教师模型的视觉表示压缩到学生模型中。研究重点在于超出分布可泛化的问题，并提出了两个原则来增强学生模型的性能。

    

    大型视觉语言模型取得了出色的性能，但其规模和计算要求使它们在资源受限设备和时间敏感任务上的部署变得不切实际。模型压缩是创建更小、更快的模型以保持较大模型性能的有希望的方法。本文研究了将大型视觉语言模型中的视觉表示压缩到轻量级学生模型中的过程，使用小型或中型数据集。值得注意的是，本研究关注的是超出分布（OOD）可泛化的开放词汇问题，这在以往的模型压缩研究中被忽视了。我们从视觉和语言的角度提出了两个原则来增强学生模型的OOD可泛化性：（1）更好地模仿教师的视觉表示空间，并在视觉语言对齐方面谨慎地促进更好的一致性；（2）通过丰富学生模型的自举学习和数据扩充来提高OOD可泛化性。

    Large vision-language models have achieved outstanding performance, but their size and computational requirements make their deployment on resource-constrained devices and time-sensitive tasks impractical. Model distillation, the process of creating smaller, faster models that maintain the performance of larger models, is a promising direction towards the solution. This paper investigates the distillation of visual representations in large teacher vision-language models into lightweight student models using a smallor mid-scale dataset. Notably, this study focuses on open-vocabulary out-of-distribution (OOD) generalization, a challenging problem that has been overlooked in previous model distillation literature. We propose two principles from vision and language modality perspectives to enhance student's OOD generalization: (1) by better imitating teacher's visual representation space, and carefully promoting better coherence in vision-language alignment with the teacher; (2) by enric
    
[^160]: 弹性决策变压器

    Elastic Decision Transformer. (arXiv:2307.02484v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.02484](http://arxiv.org/abs/2307.02484)

    弹性决策变压器（EDT）通过在测试时间进行动作推断时调整历史长度来实现轨迹拼接，填补了决策变压器（DT）在这一方面的性能差距，并且在多任务情况下胜过基于Q-Learning的方法。

    

    本文介绍了弹性决策变压器（EDT），它是现有决策变压器（DT）及其变体的重大进展。尽管DT声称能够生成最佳轨迹，但实证证据表明它在轨迹拼接方面存在困难，轨迹拼接是指从一组次优轨迹中生成最优或接近最优轨迹的过程。提出的EDT通过在测试时间进行动作推断时调整DT中维护的历史长度来实现轨迹拼接，从而使自己与众不同。此外，当前轨迹是最优的时候，EDT通过保持较长的历史，当当前轨迹是次优的时候，EDT通过保持较短的历史来优化轨迹，使其能够与更优的轨迹进行“拼接”。广泛的实验表明，EDT能够填补基于DT和基于Q-Learning方法之间的性能差距。特别是，EDT在多任务情况下胜过基于Q-Learning的方法。

    This paper introduces Elastic Decision Transformer (EDT), a significant advancement over the existing Decision Transformer (DT) and its variants. Although DT purports to generate an optimal trajectory, empirical evidence suggests it struggles with trajectory stitching, a process involving the generation of an optimal or near-optimal trajectory from the best parts of a set of sub-optimal trajectories. The proposed EDT differentiates itself by facilitating trajectory stitching during action inference at test time, achieved by adjusting the history length maintained in DT. Further, the EDT optimizes the trajectory by retaining a longer history when the previous trajectory is optimal and a shorter one when it is sub-optimal, enabling it to "stitch" with a more optimal trajectory. Extensive experimentation demonstrates EDT's ability to bridge the performance gap between DT-based and Q Learning-based approaches. In particular, the EDT outperforms Q Learning-based methods in a multi-task regi
    
[^161]: 使用流形上的自回归模型（mNARX）模拟复杂系统的动力学

    Emulating the dynamics of complex systems using autoregressive models on manifolds (mNARX). (arXiv:2306.16335v1 [stat.CO])

    [http://arxiv.org/abs/2306.16335](http://arxiv.org/abs/2306.16335)

    本研究提出了一种名为mNARX的方法，通过构建流形和自回归模型，以高效准确地近似复杂系统的动力学响应。这种方法能够将整个问题分解成较小的子问题，具有良好的可扩展性，并且与传统的降维技术相结合，适用于建模复杂系统。

    

    在这项研究中，我们提出了一种新颖的替代建模方法，可以高效准确地近似复杂动态系统对随时间变化的外部刺激的响应，并且可以延长时间段。我们的方法被称为“带外部输入的流形非线性自回归建模”（mNARX），它涉及构建一个问题特定的外部输入流形，以便构造自回归替代模型。这个流形是mNARX的核心，通过结合系统的物理性质以及先前的专家和领域知识来逐步构建。因为mNARX将整个问题分解成一系列更小的子问题，每个子问题的复杂度比原来的低，所以它在问题的复杂性方面具有良好的可扩展性，无论是最终替代模型的训练成本还是评估成本。此外，mNARX与传统的降维技术非常契合，使其非常适合建模。

    In this study, we propose a novel surrogate modelling approach to efficiently and accurately approximate the response of complex dynamical systems driven by time-varying exogenous excitations over extended time periods. Our approach, that we name \emph{manifold nonlinear autoregressive modelling with exogenous input} (mNARX), involves constructing a problem-specific exogenous input manifold that is optimal for constructing autoregressive surrogates. The manifold, which forms the core of mNARX, is constructed incrementally by incorporating the physics of the system, as well as prior expert- and domainknowledge. Because mNARX decomposes the full problem into a series of smaller sub-problems, each with a lower complexity than the original, it scales well with the complexity of the problem, both in terms of training and evaluation costs of the final surrogate. Furthermore, mNARX synergizes well with traditional dimensionality reduction techniques, making it highly suitable for modelling 
    
[^162]: 平滑的$f$-散度分布鲁棒优化：指数率效率和不带复杂性的校准。

    Smoothed $f$-Divergence Distributionally Robust Optimization: Exponential Rate Efficiency and Complexity-Free Calibration. (arXiv:2306.14041v1 [math.OC])

    [http://arxiv.org/abs/2306.14041](http://arxiv.org/abs/2306.14041)

    分布鲁棒优化在实现统计保证界限时存在限制和保守性问题，但平滑的$f$-散度分布鲁棒优化可在指数衰减率方面实现最紧密的统计保证。

    

    在数据驱动的优化中，样本平均逼近已知存在一个所谓的优化者诅咒，会导致在评估解决方案性能时产生乐观偏差。可以通过在估计的目标值中增加“保证空间”或通过分布鲁棒优化（DRO）来解决这个问题，后者是一种快速增长的方法，基于最坏情况分析，为获得的目标价值提供了保护界限。然而，在所有这些现有方法中，对真实解决方案性能的统计保证界限要么需要对目标函数复杂性有限制性条件和知识，要么会表现出取决于分布维度的过于保守的速率。我们认为，在这些挑战方面，一种特殊类型的DRO在理论上提供了强大的优势：对于一大类目标函数，它获得了对真实解的解决方案性能的统计界限，这在指数衰减率方面是可能的，就其紧缩程度而言，要紧密得多。

    In data-driven optimization, sample average approximation is known to suffer from the so-called optimizer's curse that causes optimistic bias in evaluating the solution performance. This can be tackled by adding a "margin" to the estimated objective value, or via distributionally robust optimization (DRO), a fast-growing approach based on worst-case analysis, which gives a protective bound on the attained objective value. However, in all these existing approaches, a statistically guaranteed bound on the true solution performance either requires restrictive conditions and knowledge on the objective function complexity, or otherwise exhibits an over-conservative rate that depends on the distribution dimension. We argue that a special type of DRO offers strong theoretical advantages in regard to these challenges: It attains a statistical bound on the true solution performance that is the tightest possible in terms of exponential decay rate, for a wide class of objective functions that not
    
[^163]: GIO：用于训练数据集选择的梯度信息优化

    GIO: Gradient Information Optimization for Training Dataset Selection. (arXiv:2306.11670v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.11670](http://arxiv.org/abs/2306.11670)

    GIO是一种可伸缩且任务不可知的方法，通过对目标的简单放松和高效实现，可以在非常小的训练集上产生出色的结果。

    

    在训练模型时，通常有利于在可用训练样本的子集上进行训练，因为这些样本具有不同的质量，或者希望在不影响性能的情况下使用更少的样本进行训练。我们提出了梯度信息优化（GIO），这是一种可伸缩且任务不可知的方法，用于解决这个数据选择问题，它只需要一小组（未标记的）样本来代表目标分布。GIO从一个实践中难以处理的自然的信息理论目标开始。我们的贡献在于展示出通过对目标进行简单的放松和高效的实现，它可以被高度扩展。在机器翻译、拼写纠正和图像识别等实验中，我们证明了GIO在非常小的训练集上产生了出色的结果。这些发现对于GIO本身的不同表示模型和超参数是稳健的。GIO是任务和领域无关的，可以直接应用于新领域。

    It is often advantageous to train models on a subset of the available train examples, because the examples are of variable quality or because one would like to train with fewer examples, without sacrificing performance. We present Gradient Information Optimization (GIO), a scalable, task-agnostic approach to this data selection problem that requires only a small set of (unlabeled) examples representing a target distribution. GIO begins from a natural, information-theoretic objective that is intractable in practice. Our contribution is in showing that it can be made highly scalable through a simple relaxation of the objective and a highly efficient implementation. In experiments with machine translation, spelling correction, and image recognition, we show that GIO delivers outstanding results with very small train sets. These findings are robust to different representation models and hyperparameters for GIO itself. GIO is task- and domain-agnostic and can be applied out-of-the-box to ne
    
[^164]: 物理约束的无监督深度学习用于快速高分辨率扫描相干衍射重建

    Physics Constrained Unsupervised Deep Learning for Rapid, High Resolution Scanning Coherent Diffraction Reconstruction. (arXiv:2306.11014v2 [physics.comp-ph] UPDATED)

    [http://arxiv.org/abs/2306.11014](http://arxiv.org/abs/2306.11014)

    提出了一种无监督的物理信息神经网络重建方法，PtychoPINN，通过将衍射正演图与重叠测量的实空间约束相结合，显著提高了重建质量和空间分辨率。

    

    通过规避光学的分辨率限制，相干衍射成像（CDI）和全息拼补技术正逐渐应用于从X射线成像到天文学等科学领域。然而，耗时的迭代相位恢复限制了实时成像的速度。虽然有监督的深度学习方法提高了重建速度，但牺牲了图像质量。此外，这些方法对大量标记训练数据的需求在实验上很繁琐。在这里，我们提出了一种无监督的物理信息神经网络重建方法，PtychoPINN，它在保持深度学习重建的100到1000倍速度提升的同时，通过将衍射正演图与重叠测量的实空间约束相结合，改善了重建质量。特别是，PtychoPINN在泛化能力、准确性（典型的信噪比增加了10 dB）和线性分辨率（2-6倍提升）方面取得了显著进展。这种性能和空间分辨率的结合是一种创新性和贡献。

    By circumventing the resolution limitations of optics, coherent diffractive imaging (CDI) and ptychography are making their way into scientific fields ranging from X-ray imaging to astronomy. Yet, the need for time consuming iterative phase recovery hampers real-time imaging. While supervised deep learning strategies have increased reconstruction speed, they sacrifice image quality. Furthermore, these methods' demand for extensive labeled training data is experimentally burdensome. Here, we propose an unsupervised physics-informed neural network reconstruction method, PtychoPINN, that retains the factor of 100-to-1000 speedup of deep learning-based reconstruction while improving reconstruction quality by combining the diffraction forward map with real-space constraints from overlapping measurements. In particular, PtychoPINN significantly advances generalizability, accuracy (with a typical 10 dB PSNR increase), and linear resolution (2- to 6-fold gain). This blend of performance and sp
    
[^165]: 在具有部分在线状态信息的强化学习中，POMDP的理论难度和可计算性

    Theoretical Hardness and Tractability of POMDPs in RL with Partial Online State Information. (arXiv:2306.08762v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.08762](http://arxiv.org/abs/2306.08762)

    本论文研究了具有部分在线状态信息的POMDP问题的理论困难性和可计算性。作者通过建立下界得出一个惊人的难度结果：除非具有完整的在线状态信息，否则需要指数级的样本复杂度才能得到POMDP的最优策略解。然而，作者还发现了具有部分在线状态信息下的可计算POMDP类别，并提出了新的算法来证明其接近最优性。

    

    部分可观察的马尔可夫决策过程（POMDP）被广泛应用于捕捉许多现实世界的应用。然而，现有的理论结果已经表明，在一般的POMDP中学习可能是不可计算的，主要挑战在于缺乏潜在的状态信息。一个关键的基本问题是有多少在线状态信息（OSI）足以实现可计算性。在本文中，我们建立了一个下界，揭示了一个惊人的难度结果：除非我们具有完整的OSI，否则我们需要指数级的采样复杂度才能获得POMDP的$\epsilon$-最优策略解。尽管如此，受到我们下界设计的关键见解的启发，我们发现即使只有部分OSI，也存在重要的可计算的POMDP类别。特别地，对于具有部分OSI的两个新颖的POMDP类别，我们通过建立新的遗憾上下界证明了新的算法是接近最优的。

    Partially observable Markov decision processes (POMDPs) have been widely applied to capture many real-world applications. However, existing theoretical results have shown that learning in general POMDPs could be intractable, where the main challenge lies in the lack of latent state information. A key fundamental question here is how much online state information (OSI) is sufficient to achieve tractability. In this paper, we establish a lower bound that reveals a surprising hardness result: unless we have full OSI, we need an exponentially scaling sample complexity to obtain an $\epsilon$-optimal policy solution for POMDPs. Nonetheless, inspired by the key insights in our lower bound design, we find that there exist important tractable classes of POMDPs even with only partial OSI. In particular, for two novel classes of POMDPs with partial OSI, we provide new algorithms that are proved to be near-optimal by establishing new regret upper and lower bounds.
    
[^166]: 变分不平衡回归(Variational Imbalanced Regression)

    Variational Imbalanced Regression. (arXiv:2306.06599v1 [cs.LG])

    [http://arxiv.org/abs/2306.06599](http://arxiv.org/abs/2306.06599)

    本文提出的变分不平衡回归（VIR）模型通过引入Probabilistic Reweighting方法，可以在不平衡回归方面表现良好，并自然产生合理的不确定性估计。

    

    当标签分布不平衡时，现有的回归模型往往在准确性和不确定性估计方面表现不佳。本文提出了一种概率深度学习模型——变分不平衡回归（VIR），它不仅在不平衡回归方面表现出色，而且自然地产生合理的不确定性估计。与典型的变分自编码器假设I.I.D.表示（数据点的表示不直接受其他数据点的影响）不同，我们的VIR借用具有类似回归标签的数据来计算潜在表示的变分分布；此外，不同于产生点估计的确定性回归模型， VIR预测整个正态反-伽玛分布并调节相关联的共轭分布，对不平衡数据施加概率重新加权，从而提供更好的不确定性估计。在几个真实世界的数据集上进行了实验。

    Existing regression models tend to fall short in both accuracy and uncertainty estimation when the label distribution is imbalanced. In this paper, we propose a probabilistic deep learning model, dubbed variational imbalanced regression (VIR), which not only performs well in imbalanced regression but naturally produces reasonable uncertainty estimation as a byproduct. Different from typical variational autoencoders assuming I.I.D. representations (a data point's representation is not directly affected by other data points), our VIR borrows data with similar regression labels to compute the latent representation's variational distribution; furthermore, different from deterministic regression models producing point estimates, VIR predicts the entire normal-inverse-gamma distributions and modulates the associated conjugate distributions to impose probabilistic reweighting on the imbalanced data, thereby providing better uncertainty estimation. Experiments in several real-world datasets sh
    
[^167]: 学习多层生成器的联合潜在空间EBM先验模型

    Learning Joint Latent Space EBM Prior Model for Multi-layer Generator. (arXiv:2306.06323v1 [cs.CV])

    [http://arxiv.org/abs/2306.06323](http://arxiv.org/abs/2306.06323)

    本文研究了学习多层生成器模型的基本问题，并提出了一种以多层生成器为骨干的联合潜在空间EBM先验模型，该模型可以更好地学习复杂的数据分布和分层表示，实现更好的生成质量和修复结果。

    

    本文研究了学习多层生成器模型的基本问题。多层生成器模型在生成器之上构建多层潜在变量作为先验模型，有利于学习复杂的数据分布和分层表示。然而，这样的先验模型通常通过假设非信息（条件）高斯分布来专注于建模潜在变量之间的层间关系，并且在模型表达能力方面存在局限性。为了解决这个问题并学习更具表现力的先验模型，我们在所有潜在变量的联合潜在空间上提出了一种以多层生成器为骨干的能量基模型（EBM）的先验模型。这种联合潜在空间EBM 先验模型通过层间能量项捕获每层内的内部关系，并对不同层的潜在变量进行联合修正。我们通过最大似然估计开发了一种联合训练方案，其中同时学习生成器和EBM先验模型。在生成图像建模和修复任务上的实验表明，与几个基线相比，我们提出的方法可以学习更具表现力和可解释性的先验模型，从而实现更好的生成质量和修复结果。

    This paper studies the fundamental problem of learning multi-layer generator models. The multi-layer generator model builds multiple layers of latent variables as a prior model on top of the generator, which benefits learning complex data distribution and hierarchical representations. However, such a prior model usually focuses on modeling inter-layer relations between latent variables by assuming non-informative (conditional) Gaussian distributions, which can be limited in model expressivity. To tackle this issue and learn more expressive prior models, we propose an energy-based model (EBM) on the joint latent space over all layers of latent variables with the multi-layer generator as its backbone. Such joint latent space EBM prior model captures the intra-layer contextual relations at each layer through layer-wise energy terms, and latent variables across different layers are jointly corrected. We develop a joint training scheme via maximum likelihood estimation (MLE), which involves
    
[^168]: 学习模拟树枝动力学以进行操纵

    Learning to Simulate Tree-Branch Dynamics for Manipulation. (arXiv:2306.03410v1 [cs.RO])

    [http://arxiv.org/abs/2306.03410](http://arxiv.org/abs/2306.03410)

    本论文提出了一种仿真驱动的逆推理方法来学习树枝动态，并可以操纵可变形的植被，以解决密集植被中容易遮挡的任务，算法结合了生物学上的假设和传统参数推理方法的有限差分方案。

    

    我们提出使用仿真驱动的逆推理方法来模拟操纵下的树枝关节动力学。学习枝干动态并获得操纵可变形植被的能力可帮助处理容易遮挡的任务，例如在密集树叶中采摘水果、移动悬垂的藤蔓和树枝，以便在密集植被中导航。植物的可变形几何形状通过在并行、不可微模拟器上执行的粗略弹簧抽象来实现。由模拟器定义的隐式统计模型、通过主动探测的地面真实情况获得的参考轨迹和贝叶斯形式主义一起指导弹簧参数后验密度估计。我们的无参数推理算法基于斯坦变分梯度下降，并将生物学上的假设作为神经网络驱动的学习联合先验合并到推理过程中。此外，它利用了有限差分方案来对函数梯度进行估计，从而克服了传统参数推理方法中的梯度计算困难和维度灾难问题。

    We propose to use a simulation driven inverse inference approach to model the joint dynamics of tree branches under manipulation. Learning branch dynamics and gaining the ability to manipulate deformable vegetation can help with occlusion-prone tasks, such as fruit picking in dense foliage, as well as moving overhanging vines and branches for navigation in dense vegetation. The underlying deformable tree geometry is encapsulated as coarse spring abstractions executed on parallel, non-differentiable simulators. The implicit statistical model defined by the simulator, reference trajectories obtained by actively probing the ground truth, and the Bayesian formalism, together guide the spring parameter posterior density estimation. Our non-parametric inference algorithm, based on Stein Variational Gradient Descent, incorporates biologically motivated assumptions into the inference process as neural network driven learnt joint priors; moreover, it leverages the finite difference scheme for g
    
[^169]: Transformers中多头注意力的记忆容量

    Memorization Capacity of Multi-Head Attention in Transformers. (arXiv:2306.02010v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02010](http://arxiv.org/abs/2306.02010)

    本文研究了多头注意力机制的记忆能力，发现在特定的假设下，注意力层可以记忆Ω(Hn)个示例序列，这对于理解transformers的记忆容量有重要意义。

    

    Transformers已成为语言和视觉任务的首选架构，但其理论属性，特别是记忆容量，仍然难以捉摸。本文研究了多头注意力机制的记忆能力，研究了它们能够记忆多少个示例序列，作为头数和序列长度的函数。在对视觉transformers的实验结果的启发下，我们引入了关于输入数据线性独立性的新假设，不同于通常使用的一般位置假设。在这些假设下，我们证明了具有H个头，维度d，上下文大小n < d的注意力层，具有Θ(Hd^2)个参数，可以记住Ω(Hn)个示例。我们的分析揭示了不同的注意力头如何处理不同的示例序列，受到softmax运算符的饱和特性的帮助。我们通过合成数据的实验验证了我们的发现。

    Transformers have become the go-to architecture for language and vision tasks, yet their theoretical properties, especially memorization capacity, remain elusive. This paper investigates the memorization abilities of multi-head attention mechanisms, examining how many example sequences they can memorize, as a function of the number of heads and sequence length. Motivated by experimental findings on vision transformers, we introduce novel assumptions about the linear independence of input data, distinct from the commonly used general-position assumption. Under these assumptions, we demonstrate that an attention layer with $H$ heads, dimension $d$, and context size $n < d$, featuring $\Theta(Hd^2)$ parameters, can memorize $\Omega(Hn)$ examples. Our analysis sheds light on how different attention heads handle various example sequences, aided by the softmax operator's saturation property. We validate our findings through experiments on synthetic data.
    
[^170]: 放松去中心化无遗憾高维贝叶斯优化中的加法约束

    Relaxing the Additivity Constraints in Decentralized No-Regret High-Dimensional Bayesian Optimization. (arXiv:2305.19838v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19838](http://arxiv.org/abs/2305.19838)

    本文提出了一种放松去中心化无遗憾高维贝叶斯优化中加法约束的方法，并且解决了过度探索问题。该方法称为DumBO，并且在实验中展示了竞争性能。

    

    贝叶斯优化常用于优化一个未知函数$f$，该函数存在噪声且评估成本高昂，通过利用必须在每个优化步骤中最大化的收获函数来实现。尽管可证明渐进最优的BO算法在优化低维函数方面效率很高，但将其扩展到高维空间仍然是一个待解决的问题，通常通过假设$f$具有加法结构来解决。通过这样做，BO算法通常引入了对加法结构的额外限制性假设，降低了它们的适用范围。本文包含两个主要贡献：（i）放松对$f$加法结构的限制性假设，以减弱收获函数的最大化保证；（ii）解决去中心化BO算法中的过度探索问题。为此，我们提出了DumBO，一种渐进最优的去中心化BO算法，具有非常竞争的性能。

    Bayesian Optimization (BO) is typically used to optimize an unknown function $f$ that is noisy and costly to evaluate, by exploiting an acquisition function that must be maximized at each optimization step. Even if provably asymptotically optimal BO algorithms are efficient at optimizing low-dimensional functions, scaling them to high-dimensional spaces remains an open problem, often tackled by assuming an additive structure for $f$. By doing so, BO algorithms typically introduce additional restrictive assumptions on the additive structure that reduce their applicability domain. This paper contains two main contributions: (i) we relax the restrictive assumptions on the additive structure of $f$, at the expense of weakening the maximization guarantees of the acquisition function, and (ii) we address the over-exploration problem for decentralized BO algorithms. To these ends, we propose DumBO, an asymptotically optimal decentralized BO algorithm that achieves very competitive performance
    
[^171]: OWAdapt：使用OWA算子的深度学习自适应损失函数

    OWAdapt: An adaptive loss function for deep learning using OWA operators. (arXiv:2305.19443v1 [cs.LG])

    [http://arxiv.org/abs/2305.19443](http://arxiv.org/abs/2305.19443)

    本文提出了一种基于OWA算子的模糊自适应损失函数，通过迭代加权策略应对类别级别噪声条件，提高深度学习在分类任务中的性能。实验证明该方法在各种分类任务中均优于传统的常用损失函数。

    

    本文提出了一种模糊自适应损失函数，用于提升分类任务中深度学习的性能。具体而言，我们重新定义了交叉熵损失，以有效应对类别级别噪声条件，包括类别不平衡这一难题。我们的方法引入了聚合算子，利用模糊逻辑的能力提高了分类精度。我们提出的方法的理论基础在于损失函数内类别级别组件的迭代加权，重点关注那些存在较大误差的组件。为此，我们采用了有序加权平均（OWA）算子，并将其与基于梯度的学习的自适应方案结合在一起。通过大量实验，我们的方法在各种二元和多元分类任务中优于其他常用的损失函数，如标准交叉熵或聚焦损失。此外，我们还探讨了与OWA算子相关的超参数的影响。

    In this paper, we propose a fuzzy adaptive loss function for enhancing deep learning performance in classification tasks. Specifically, we redefine the cross-entropy loss to effectively address class-level noise conditions, including the challenging problem of class imbalance. Our approach introduces aggregation operators, leveraging the power of fuzzy logic to improve classification accuracy. The rationale behind our proposed method lies in the iterative up-weighting of class-level components within the loss function, focusing on those with larger errors. To achieve this, we employ the ordered weighted average (OWA) operator and combine it with an adaptive scheme for gradient-based learning. Through extensive experimentation, our method outperforms other commonly used loss functions, such as the standard cross-entropy or focal loss, across various binary and multiclass classification tasks. Furthermore, we explore the influence of hyperparameters associated with the OWA operators and 
    
[^172]: 差分隐私决策树与对数据篡改的可靠性证明

    Differentially-Private Decision Trees and Provable Robustness to Data Poisoning. (arXiv:2305.15394v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.15394](http://arxiv.org/abs/2305.15394)

    本论文提出了一种名为PrivaTree的差分隐私决策树方法，通过使用私有直方图选择分割点来在隐私保护与模型效用之间取得更好的平衡。这种方法能够接收混合的数值和类别数据，并且能够在数据篡改方面表现出可靠性。

    

    决策树是适用于非线性学习问题的可解释模型。关于将差分隐私引入决策树学习算法的研究已经很多，差分隐私能够确保训练数据中样本的隐私性。然而，目前用于此目的的最先进算法在获得一点点隐私保护的同时牺牲了较多的模型效用。这些解决方案引入了随机决策节点，降低了决策树的准确性，或者在标记叶子节点上使用过多的隐私预算。此外，很多方法不支持连续特征或者泄露与连续特征相关的信息。我们提出了一种基于私有直方图的新方法，称为PrivaTree，它在消耗一小部分隐私预算的同时选择合适的分割点。由此产生的决策树在隐私效用权衡方面取得了显著的提升，而且能够接受混合的数值和类别数据而不泄露与数值特征相关的信息。最后，尽管给出可靠性保证一直很难，我们的方法在数据篡改方面表现出了可靠性。

    Decision trees are interpretable models that are well-suited to non-linear learning problems. Much work has been done on extending decision tree learning algorithms with differential privacy, a system that guarantees the privacy of samples within the training data. However, current state-of-the-art algorithms for this purpose sacrifice much utility for a small privacy benefit. These solutions create random decision nodes that reduce decision tree accuracy or spend an excessive share of the privacy budget on labeling leaves. Moreover, many works do not support continuous features or leak information about them. We propose a new method called PrivaTree based on private histograms that chooses good splits while consuming a small privacy budget. The resulting trees provide a significantly better privacy-utility trade-off and accept mixed numerical and categorical data without leaking information about numerical features. Finally, while it is notoriously hard to give robustness guarantees a
    
[^173]: 使用基于文献的语境化学习生成新的科学方向

    Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery. (arXiv:2305.14259v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14259](http://arxiv.org/abs/2305.14259)

    本文介绍了一种基于文献的发现方法，通过上下文化的学习生成新的科学方向，克服了标准方法在预测关联、忽略上下文等方面的局限性。模型使用了引文和知识图关系的网络，并使用大型语言模型进行评估，发现GPT4在生成创新思想方面表现出色。

    

    基于文献的发现（LBD）旨在通过挖掘论文并生成假设来发现新的科学知识。标准的LBD仅限于预测离散概念之间的两两关系（例如，药物和疾病的关联）。LBD还忽略了关键的上下文，例如实验设置（例如，药物评估的特定患者群体）和人类科学家考虑的背景知识和动机（例如，找到没有特定副作用的药物候选）。我们通过一种新颖的上下文化LBD（C-LBD）表述来解决这些局限性：以自然语言生成科学假设，同时将它们联系到控制假设搜索空间的上下文中。我们提出了一个建模框架，使用获得的引文和知识图关系的异构网络中的“灵感”，并创建了一个从论文中派生的新数据集。我们使用强大的大型语言模型（LLM）进行评估，发现GPT4倾向于生成具有创新性的思想。

    Literature-Based Discovery (LBD) aims to discover new scientific knowledge by mining papers and generating hypotheses. Standard LBD is limited to predicting pairwise relations between discrete concepts (e.g., drug-disease links). LBD also ignores critical contexts like experimental settings (e.g., a specific patient population where a drug is evaluated) and background knowledge and motivations that human scientists consider (e.g., to find a drug candidate without specific side effects). We address these limitations with a novel formulation of contextualized-LBD (C-LBD): generating scientific hypotheses in natural language, while grounding them in a context that controls the hypothesis search space. We present a modeling framework using retrieval of ``inspirations'' from a heterogeneous network of citations and knowledge graph relations, and create a new dataset derived from papers. Our evaluations with powerful large language models (LLMs) reveal that GPT4 tends to generate ideas with 
    
[^174]: 强化学习中用于解耦表示的条件互信息

    Conditional Mutual Information for Disentangled Representations in Reinforcement Learning. (arXiv:2305.14133v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.14133](http://arxiv.org/abs/2305.14133)

    本论文提出了一种用于解决强化学习中训练数据相关性问题的方法，通过最小化表示中特征之间的条件互信息，学习具有相关特征的解耦表示。实验证明该方法可以提高泛化性能并处理密集观测的相关特征。

    

    强化学习环境可以产生具有假相关性的训练数据，这是由于训练数据量或特征覆盖的有限性所导致的。这可能导致强化学习代理将这些误导性相关性编码到其潜在表示中，如果环境内的相关性发生变化或在真实世界中部署时，则会阻止代理进行泛化。解耦表示可以提高鲁棒性，但现有的最小化特征之间互信息的解耦技术要求特征之间是独立的，因此无法解耦相关特征。我们提出了一种用于强化学习算法的辅助任务，通过最小化表示中特征之间的条件互信息，学习具有相关特征的高维观测的解耦表示。我们在连续控制任务中进行了实验证明，我们的方法可以改善在相关性变化下的泛化性能，同时能够处理密集观测的相关特征。

    Reinforcement Learning (RL) environments can produce training data with spurious correlations between features due to the amount of training data or its limited feature coverage. This can lead to RL agents encoding these misleading correlations in their latent representation, preventing the agent from generalising if the correlation changes within the environment or when deployed in the real world. Disentangled representations can improve robustness, but existing disentanglement techniques that minimise mutual information between features require independent features, thus they cannot disentangle correlated features. We propose an auxiliary task for RL algorithms that learns a disentangled representation of high-dimensional observations with correlated features by minimising the conditional mutual information between features in the representation. We demonstrate experimentally, using continuous control tasks, that our approach improves generalisation under correlation shifts, as well 
    
[^175]: 神经常微分方程与深度残差网络的泛化界限

    Generalization bounds for neural ordinary differential equations and deep residual networks. (arXiv:2305.06648v1 [stat.ML])

    [http://arxiv.org/abs/2305.06648](http://arxiv.org/abs/2305.06648)

    本研究提出了神经常微分方程及其变体的泛化界限，涵盖了深度残差网络，其泛化界限与连续权重差异大小有关。

    

    神经常微分方程（Neural ODEs）是一类流行的连续深度深度学习模型。本文考虑了一个由连续时间参数化的ODE及时变的神经ODE组成的大类。我们通过Lipschitz方法推导了这个类别的泛化界限。通过利用神经ODE和深度残差网络之间的类比，我们的方法得到了一个深度残差网络的泛化界限。这个界限与连续权重之间的差异的大小有关。我们通过数值结果演示了这个量是如何影响神经网络的泛化能力的。

    Neural ordinary differential equations (neural ODEs) are a popular family of continuous-depth deep learning models. In this work, we consider a large family of parameterized ODEs with continuous-in-time parameters, which include time-dependent neural ODEs. We derive a generalization bound for this class by a Lipschitz-based argument. By leveraging the analogy between neural ODEs and deep residual networks, our approach yields in particular a generalization bound for a class of deep residual networks. The bound involves the magnitude of the difference between successive weight matrices. We illustrate numerically how this quantity affects the generalization capability of neural networks.
    
[^176]: 联合度量重要性：轨迹预测的更好标准

    Joint Metrics Matter: A Better Standard for Trajectory Forecasting. (arXiv:2305.06292v1 [cs.RO])

    [http://arxiv.org/abs/2305.06292](http://arxiv.org/abs/2305.06292)

    这项研究展示了边际度量标准不足以完全捕捉多个互动代理的联合表现，提出使用联合度量标准进行轨迹预测方法评估的重要性。

    

    多模态轨迹预测方法通常使用单个代理度量标准（边际度量标准），例如最小平均位移误差（ADE）和最终位移误差（FDE）进行评估。然而，这些度量标准无法捕捉多个互动代理的联合表现。仅关注边际度量标准可能会导致不自然的预测，如碰撞轨迹或明显一起行走的人的发散轨迹。因此，针对边际度量标准优化的方法会导致过于乐观的性能估计，这对轨迹预测研究的进展有害。为了应对边际度量标准的局限性，我们首次全面评估了最先进的轨迹预测方法与多代理度量标准（联合度量标准）之间的关系：JADE、JFDE和碰撞率。我们通过量化证据和定性示例展示了联合度量标准的重要性，而不是边际度量标准。

    Multi-modal trajectory forecasting methods commonly evaluate using single-agent metrics (marginal metrics), such as minimum Average Displacement Error (ADE) and Final Displacement Error (FDE), which fail to capture joint performance of multiple interacting agents. Only focusing on marginal metrics can lead to unnatural predictions, such as colliding trajectories or diverging trajectories for people who are clearly walking together as a group. Consequently, methods optimized for marginal metrics lead to overly-optimistic estimations of performance, which is detrimental to progress in trajectory forecasting research. In response to the limitations of marginal metrics, we present the first comprehensive evaluation of state-of-the-art (SOTA) trajectory forecasting methods with respect to multi-agent metrics (joint metrics): JADE, JFDE, and collision rate. We demonstrate the importance of joint metrics as opposed to marginal metrics with quantitative evidence and qualitative examples drawn 
    
[^177]: 转移学习下的模型选择限制

    Limits of Model Selection under Transfer Learning. (arXiv:2305.00152v1 [stat.ML])

    [http://arxiv.org/abs/2305.00152](http://arxiv.org/abs/2305.00152)

    这篇论文介绍了在转移学习下模型选择存在的限制，其转移距离会影响自适应速率，可能导致速率较慢。

    

    目前，关于转移学习或领域自适应的理论研究主要关注已知假设类或模型的情况；然而，在实践中，通常涉及一定程度的模型选择，这经常出现在超参数调整的总体范畴下：例如，我们可以考虑调整针对目标任务的正确神经网络架构的问题，同时利用来自相关源任务的数据。除了与模型选择有关的近似与估计误差的通常权衡之外，这个问题还带来了新的复杂度，即源分布与目标分布之间的转移距离，这个距离随着假设类的选择而发生变化。我们首次研究了这个问题，重点关注分类问题。特别的，分析揭示了一些引人注目的现象：自适应速率，即没有分布式信息时可达到的速率，可以任意慢于oracle速率，即在给定知识的情况下。

    Theoretical studies on transfer learning or domain adaptation have so far focused on situations with a known hypothesis class or model; however in practice, some amount of model selection is usually involved, often appearing under the umbrella term of hyperparameter-tuning: for example, one may think of the problem of tuning for the right neural network architecture towards a target task, while leveraging data from a related source task.  Now, in addition to the usual tradeoffs on approximation vs estimation errors involved in model selection, this problem brings in a new complexity term, namely, the transfer distance between source and target distributions, which is known to vary with the choice of hypothesis class.  We present a first study of this problem, focusing on classification; in particular, the analysis reveals some remarkable phenomena: adaptive rates, i.e., those achievable with no distributional information, can be arbitrarily slower than oracle rates, i.e., when given kn
    
[^178]: 基于扩散的生成式人工智能在探索二维分子图的过渡态上的应用

    Diffusion-based Generative AI for Exploring Transition States from 2D Molecular Graphs. (arXiv:2304.12233v2 [physics.chem-ph] UPDATED)

    [http://arxiv.org/abs/2304.12233](http://arxiv.org/abs/2304.12233)

    本文提出了一种基于扩散的生成式方法(TSDiff)，用于从二维分子图中预测过渡态几何结构。与现有具有 3D 几何结构机器学习模型相比，TSdiff 的准确性和效率都更高。TSDiff 能够找到比参考数据库更优化的反应途径，在反应势垒更低的情况下找到更为优化的反应路径。

    

    探究过渡态几何结构对于阐明化学反应机理和模拟反应动力学至关重要。最近，机器学习模型在预测过渡态几何结构方面表现出了出色的性能。然而，它们需要反应物和产物的 3D 形态和方向作为输入，这需要大量的努力和计算成本。在这里，我们提出了一种基于随机扩散方法的生成式方法(TSDiff)，用于从二维分子图中预测过渡态几何结构。TSDiff在准确性和效率方面优于现有的具有 3D 几何结构的机器学习模型。此外，它可以从训练中了解到各种反应的过渡态几何分布，从而能够采样各种过渡态构象。因此，TSDiff 能够找到比参考数据库中更为有利的反应途径，在反应势垒更低的情况下找到更为优化的反应路径。这些结果表明，TSDiff 在加速化学反应和反应途径的发现方面具有潜在的价值。

    The exploration of transition state (TS) geometries is crucial for elucidating chemical reaction mechanisms and modeling their kinetics. Recently, machine learning (ML) models have shown remarkable performance for prediction of TS geometries. However, they require 3D conformations of reactants and products often with their appropriate orientations as input, which demands substantial efforts and computational cost. Here, we propose a generative approach based on the stochastic diffusion method, namely TSDiff, for prediction of TS geometries just from 2D molecular graphs. TSDiff outperformed the existing ML models with 3D geometries in terms of both accuracy and efficiency. Moreover, it enables to sample various TS conformations, because it learned the distribution of TS geometries for diverse reactions in training. Thus, TSDiff was able to find more favorable reaction pathways with lower barrier heights than those in the reference database. These results demonstrate that TSDiff shows pr
    
[^179]: 通过最优输运和投影追踪的时变密度生成建模

    Generative Modeling of Time-Dependent Densities via Optimal Transport and Projection Pursuit. (arXiv:2304.09663v1 [stat.ML])

    [http://arxiv.org/abs/2304.09663](http://arxiv.org/abs/2304.09663)

    本文提出了一种基于最优输运和投影追踪的方法，用于便宜、高效地生成时态密度建模，其最优映射与恒等映射接近，训练过程高度并行化。

    

    受到流行的深度学习算法对于时态密度生成建模所带来的计算困难的启发，我们提出了一种便宜的替代方案，它需要最少的超参数调整，并且可以很好地扩展到高维问题。具体地，我们使用基于投影的最优输运求解器 [Meng等，2019] 来连接连续的样本，然后使用传输样条 [Chewi等，2020] 来插值演化的密度。当采样频率足够高时，最优映射接近于恒等映射，因此计算效率高。此外，训练过程可以高度并行化，因为所有最优映射是独立的，因此可以同时学习。最后，该方法仅基于数值线性代数而不是最小化非凸目标函数，这使我们能够轻松分析和控制算法。我们在合成和真实数据集上进行了几个数值实验。

    Motivated by the computational difficulties incurred by popular deep learning algorithms for the generative modeling of temporal densities, we propose a cheap alternative which requires minimal hyperparameter tuning and scales favorably to high dimensional problems. In particular, we use a projection-based optimal transport solver [Meng et al., 2019] to join successive samples and subsequently use transport splines [Chewi et al., 2020] to interpolate the evolving density. When the sampling frequency is sufficiently high, the optimal maps are close to the identity and are thus computationally efficient to compute. Moreover, the training process is highly parallelizable as all optimal maps are independent and can thus be learned simultaneously. Finally, the approach is based solely on numerical linear algebra rather than minimizing a nonconvex objective function, allowing us to easily analyze and control the algorithm. We present several numerical experiments on both synthetic and real-w
    
[^180]: 变分算子学习：一种训练神经算子和解决偏微分方程的统一方法

    Variational operator learning: A unified paradigm for training neural operators and solving partial differential equations. (arXiv:2304.04234v1 [cs.LG])

    [http://arxiv.org/abs/2304.04234](http://arxiv.org/abs/2304.04234)

    本文提出了变分算子学习（VOL）的范式，同时训练神经算子和解决偏微分方程（PDE）。使用正反传递循环和自动微分实现了变分操作，通过最速下降法和共轭梯度法进行神经算子的简单但有效的训练。实验结果非常好。

    

    本论文提出了一种基于变分方法的新范式，为训练神经算子和用变分形式解决偏微分方程（PDE）提供了一个统一的框架，称为变分算子学习（VOL）。我们首先从神经算子给出的节点解预测中推导出系统的函数逼近，并通过自动微分进行变分操作，构建正反传递循环来推导线性系统的残差。在每次迭代中，我们提供最速下降法（SD）和共轭梯度法（CG）的一个或多个更新步骤，作为训练神经算子的一种简单而有效的更新方法。实验结果显示，所提出的VOL可以学习到在稳定传热和变刚度弹性PDE中各种解算子，结果令人满意，误差较小。该方法几乎实现无标签训练。

    Based on the variational method, we propose a novel paradigm that provides a unified framework of training neural operators and solving partial differential equations (PDEs) with the variational form, which we refer to as the variational operator learning (VOL). We first derive the functional approximation of the system from the node solution prediction given by neural operators, and then conduct the variational operation by automatic differentiation, constructing a forward-backward propagation loop to derive the residual of the linear system. One or several update steps of the steepest decent method (SD) and the conjugate gradient method (CG) are provided in every iteration as a cheap yet effective update for training the neural operators. Experimental results show the proposed VOL can learn a variety of solution operators in PDEs of the steady heat transfer and the variable stiffness elasticity with satisfactory results and small error. The proposed VOL achieves nearly label-free tra
    
[^181]: 增强弱监督分割的高保真伪标签生成方法

    High-fidelity Pseudo-labels for Boosting Weakly-Supervised Segmentation. (arXiv:2304.02621v1 [cs.CV])

    [http://arxiv.org/abs/2304.02621](http://arxiv.org/abs/2304.02621)

    本文提出了一种使用马尔可夫随机场增强弱监督分割中的高保真伪标签生成方法，能够生成更准确的伪标签，并使用新的训练策略来实现更好的收敛。实验结果表明该方法达到了弱监督分割方法的最佳性能。

    

    近年来，图像级别的弱监督语义分割（WSSS）任务因为其可减少大量数据标注成本而变得流行。WSSS的典型方法是使用全局平均池化（GAP）在卷积特征映射上训练图像分类网络。这使得可以基于类别激活图（CAMs）估计对象位置，CAMs识别图像区域的重要性。然后使用CAMs生成伪标签，以形式化的分割掩码的方式在缺乏像素级标签的情况下对分割模型进行监督。在SEAM基线的情况下，一个先前的工作提出了提高CAM学习的两种方法：（1）重要性抽样，它是GAP的替代方法；（2）特征相似性损失，它使用一种启发式方法，即对象轮廓几乎仅与图像中的颜色边缘对齐。在这项工作中，我们为这些任务提出了一种不同的概率解释CAM的方法，从而生成更精确的伪标签。具体而言，我们采用马尔可夫随机场将局部空间一致性约束融入CAM学习中。我们还提出了一种新的训练策略，交替更新CAM和分割模型以实现更好的收敛。在基准数据集上的实验结果表明，我们的方法在弱监督分割方法中实现了最先进的性能。

    The task of image-level weakly-supervised semantic segmentation (WSSS) has gained popularity in recent years, as it reduces the vast data annotation cost for training segmentation models. The typical approach for WSSS involves training an image classification network using global average pooling (GAP) on convolutional feature maps. This enables the estimation of object locations based on class activation maps (CAMs), which identify the importance of image regions. The CAMs are then used to generate pseudo-labels, in the form of segmentation masks, to supervise a segmentation model in the absence of pixel-level ground truth. In case of the SEAM baseline, a previous work proposed to improve CAM learning in two ways: (1) Importance sampling, which is a substitute for GAP, and (2) the feature similarity loss, which utilizes a heuristic that object contours almost exclusively align with color edges in images. In this work, we propose a different probabilistic interpretation of CAMs for thes
    
[^182]: LLMMaps——大型语言模型分层评价的可视化隐喻

    LLMMaps -- A Visual Metaphor for Stratified Evaluation of Large Language Models. (arXiv:2304.00457v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.00457](http://arxiv.org/abs/2304.00457)

    LLMMaps是一种分层评估大型语言模型性能的可视化技术，能够揭示取得高准确度和产生幻觉的子领域，并指导模型的进一步发展。

    

    大型语言模型(LLMs)在自然语言处理中取得了革命性的进展，并在各种任务中展示了惊人的能力。然而，它们容易产生幻觉，即模型在响应中暴露出不正确或错误的信息，这使得必须采用勤奋的评估方法。虽然LLM在特定知识领域中的表现通常是基于问答(Q&A)数据集进行评估，但这些评估通常仅报告整个领域的单个准确度数字，这一程序在透明度和模型改进方面存在问题。分层评估可以揭示可能更容易发生幻觉的子领域，从而有助于更好地评估LLMs的风险并指导它们的进一步发展。为支持这样的分层评估，我们提出了LLMMaps作为一种新的可视化技术，使用户能够根据Q&A数据集评估LLMs的性能。LLMMaps提供了对LLMs在不同子领域中的知识分布的详细洞察，允许用户放大领域的特定部分并探索模型性能上的差异。我们的实验证明，LLMMaps有助于识别出更容易出现LLM幻觉的子领域，并可以指导模型的发展，以改善这些领域的准确性。

    Large Language Models (LLMs) have revolutionized natural language processing and demonstrated impressive capabilities in various tasks. Unfortunately, they are prone to hallucinations, where the model exposes incorrect or false information in its responses, which renders diligent evaluation approaches mandatory. While LLM performance in specific knowledge fields is often evaluated based on question and answer (Q&A) datasets, such evaluations usually report only a single accuracy number for the entire field, a procedure which is problematic with respect to transparency and model improvement. A stratified evaluation could instead reveal subfields, where hallucinations are more likely to occur and thus help to better assess LLMs' risks and guide their further development. To support such stratified evaluations, we propose LLMMaps as a novel visualization technique that enables users to evaluate LLMs' performance with respect to Q&A datasets. LLMMaps provide detailed insights into LLMs' kn
    
[^183]: 一种基于可解释神经网络的连续回应有序回归非比例赔率模型

    An interpretable neural network-based non-proportional odds model for ordinal regression with continuous response. (arXiv:2303.17823v1 [stat.ME])

    [http://arxiv.org/abs/2303.17823](http://arxiv.org/abs/2303.17823)

    本文提出了一种基于可解释神经网络的非比例赔率模型 (N$^3$POM) 用于有序回归，可以对连续变量进行预测，同时保留了可解释性，具有灵活性。

    

    本文提出了一种基于可解释神经网络的非比例赔率模型（N$^3$POM) 用于有序回归，其中反应变量不仅可以取离散值，也可以取连续值，而回归系数根据预测顺序反应也不同。与传统方法直接从离散反应估计线性系数不同，我们训练了一个非线性的神经网络，通过以反应为输入产生线性系数。由于神经网络的优势，N$^3$POM可以在保留传统有序回归的可解释性的同时具有灵活性。我们给出了充分的条件，使得在指定的用户区域内，预测的条件累积概率（CCP）满足局部单调性约束。我们还提供了一种保持单调性的随机（MPS）算法来充分训练神经网络。

    This paper proposes an interpretable neural network-based non-proportional odds model (N$^3$POM) for ordinal regression, where the response variable can take not only discrete but also continuous values, and the regression coefficients vary depending on the predicting ordinal response. In contrast to conventional approaches estimating the linear coefficients of regression directly from the discrete response, we train a non-linear neural network that outputs the linear coefficients by taking the response as its input. By virtue of the neural network, N$^3$POM may have flexibility while preserving the interpretability of the conventional ordinal regression. We show a sufficient condition so that the predicted conditional cumulative probability~(CCP) satisfies the monotonicity constraint locally over a user-specified region in the covariate space; we also provide a monotonicity-preserving stochastic (MPS) algorithm for training the neural network adequately.
    
[^184]: 基于扩散模型的无标签图数据中心学习

    Data-Centric Learning from Unlabeled Graphs with Diffusion Model. (arXiv:2303.10108v1 [cs.LG])

    [http://arxiv.org/abs/2303.10108](http://arxiv.org/abs/2303.10108)

    本文提出了一种从无标签图中提取知识并增强属性预测模型的数据中心方法，使用扩散模型和两个新目标进行去噪，实验证明其效果比14种现有方法更好。

    

    图属性预测任务非常重要和众多。虽然每个任务只提供少量的标记示例，但无标签图已从各种渠道并大规模收集。传统方法是在自监督任务的无标签图上训练模型，然后在预测任务上微调模型。然而，自监督任务的知识可能无法与预测任务需要的知识相吻合或有时会产生冲突。本文提出了一种从大量无标签图中提取知识作为特定有用数据点的方法，以增强每个属性预测模型。我们使用扩散模型充分利用无标签图，并设计了两个新的目标来指导模型的去噪过程，使用每个任务的标记数据生成任务特定图形示例及其标签。实验证明，我们的数据中心方法比14种现有方法在fi上表现显着优异。

    Graph property prediction tasks are important and numerous. While each task offers a small size of labeled examples, unlabeled graphs have been collected from various sources and at a large scale. A conventional approach is training a model with the unlabeled graphs on self-supervised tasks and then fine-tuning the model on the prediction tasks. However, the self-supervised task knowledge could not be aligned or sometimes conflicted with what the predictions needed. In this paper, we propose to extract the knowledge underlying the large set of unlabeled graphs as a specific set of useful data points to augment each property prediction model. We use a diffusion model to fully utilize the unlabeled graphs and design two new objectives to guide the model's denoising process with each task's labeled data to generate task-specific graph examples and their labels. Experiments demonstrate that our data-centric approach performs significantly better than fourteen existing various methods on fi
    
[^185]: 只针对不确定性支付代价：方差自适应汤普森采样

    Only Pay for What Is Uncertain: Variance-Adaptive Thompson Sampling. (arXiv:2303.09033v1 [cs.LG])

    [http://arxiv.org/abs/2303.09033](http://arxiv.org/abs/2303.09033)

    本文提出了一种针对多臂赌博机的方差自适应汤普森采样算法，通过考虑奖励方差的信息减少了遗憾，同时提高了鲁棒性

    

    大多数赌博算法都假设奖励方差或其上界已知。尽管方差高估通常是安全的，但它会增加遗憾。另一方面，低估的方差可能导致由于过早地选择了次优臂而导致的线性遗憾。这激发了关于方差感知频率算法的先前工作。我们为贝叶斯设置打下基础。特别是，我们研究了具有已知和未知异质奖励方差的多臂赌博机，并为两者开发了汤普森采样算法，并限制了它们的贝叶斯遗憾。我们的遗憾界随着较低奖励方差而减少，这使得学习更加容易。未知奖励方差的边界捕捉了先验对学习奖励方差的影响，是其类型中的首个。我们的实验表明了方差感知的贝叶斯算法的优越性，同时也突出了它们的鲁棒性。

    Most bandit algorithms assume that the reward variance or its upper bound is known. While variance overestimation is usually safe and sound, it increases regret. On the other hand, an underestimated variance may lead to linear regret due to committing early to a suboptimal arm. This motivated prior works on variance-aware frequentist algorithms. We lay foundations for the Bayesian setting. In particular, we study multi-armed bandits with known and \emph{unknown heterogeneous reward variances}, and develop Thompson sampling algorithms for both and bound their Bayes regret. Our regret bounds decrease with lower reward variances, which make learning easier. The bound for unknown reward variances captures the effect of the prior on learning reward variances and is the first of its kind. Our experiments show the superiority of variance-aware Bayesian algorithms and also highlight their robustness.
    
[^186]: 在临床计算机断层扫描成像中优化卷积神经网络用于慢性阻塞性肺疾病检测

    Optimizing Convolutional Neural Networks for Chronic Obstructive Pulmonary Disease Detection in Clinical Computed Tomography Imaging. (arXiv:2303.07189v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2303.07189](http://arxiv.org/abs/2303.07189)

    本文旨在通过探索手动调整和自动化窗口设置优化，利用卷积神经网络在临床计算机断层扫描图像中检测慢性阻塞性肺疾病。研究结果表明，通过添加自定义层实现的自动化窗口设置优化可以改善检测性能。

    

    目的：通过探索手动调整和自动化窗口设置优化，利用卷积神经网络（CNN）在肺部计算机断层扫描（CT）图像中检测慢性阻塞性肺疾病（COPD）的存在，来优化二进制COPD的检测。方法：回顾性选择了78名受试者（43名COPD患者；35名健康对照组）的7,194个CT图像（3,597个COPD；3,597个健康对照组）（2018年10月至2019年12月）。对每个图像，将强度值手动裁剪到肺气肿窗口设置和基准的“全范围”窗口设置。类平衡的训练、验证和测试集包含了3,392、1,114和2,688个图像。通过比较不同的CNN架构来优化网络主干。此外，还通过向模型添加自定义层来实现自动化的窗口设置优化。根据受试者工作特征曲线（ROC）下面积（AUC）的图像水平，计算出P值来评估性能。

    Purpose: To optimize the binary detection of Chronic Obstructive Pulmonary Disease (COPD) based on emphysema presence in the lung with convolutional neural networks (CNN) by exploring manually adjusted versus automated window-setting optimization (WSO) on computed tomography (CT) images.  Methods: 7,194 CT images (3,597 with COPD; 3,597 healthy controls) from 78 subjects (43 with COPD; 35 healthy controls) were selected retrospectively (10.2018-12.2019) and preprocessed. For each image, intensity values were manually clipped to the emphysema window setting and a baseline 'full-range' window setting. Class-balanced train, validation, and test sets contained 3,392, 1,114, and 2,688 images. The network backbone was optimized by comparing various CNN architectures. Furthermore, automated WSO was implemented by adding a customized layer to the model. The image-level area under the Receiver Operating Characteristics curve (AUC) [lower, upper limit 95% confidence] and P-values calculated from
    
[^187]: 一种完整的扩散生成模型的配方

    A Complete Recipe for Diffusion Generative Models. (arXiv:2303.01748v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01748](http://arxiv.org/abs/2303.01748)

    本文提出了一种完整的扩散生成模型的配方，利用可扩展的贝叶斯后验采样器的见解，确保收敛到所需的目标分布。在这个方法的基础上，引入了相空间Langevin扩散（PSLD），在扩展空间中进行基于评分的建模，展现出更优质的样本质量和改进的速度-质量权衡。

    

    基于评分的生成模型（SGMs）在各种任务中展示了出色的合成结果。然而，目前前向扩散过程的设计领域仍然较少开发，并且通常依赖物理启发法或简化假设。利用可扩展的贝叶斯后验采样器的见解，我们提出了一种完整的配方，用于在SGMs中制定前向过程，确保收敛到所需的目标分布。我们的方法揭示了几个现有的SGMs可以被看作是我们框架的特定表现形式。在这个方法的基础上，我们引入了相空间Langevin扩散（PSLD），它依赖于基于评分的建模，在由辅助变量增强的类似物理相空间的扩展空间中进行。实证结果表明，与已建立的图像合成基准上的各种竞争方法相比，PSLD展现了更优质的样本质量和改进的速度-质量权衡。

    Score-based Generative Models (SGMs) have demonstrated exceptional synthesis outcomes across various tasks. However, the current design landscape of the forward diffusion process remains largely untapped and often relies on physical heuristics or simplifying assumptions. Utilizing insights from the development of scalable Bayesian posterior samplers, we present a complete recipe for formulating forward processes in SGMs, ensuring convergence to the desired target distribution. Our approach reveals that several existing SGMs can be seen as specific manifestations of our framework. Building upon this method, we introduce Phase Space Langevin Diffusion (PSLD), which relies on score-based modeling within an augmented space enriched by auxiliary variables akin to physical phase space. Empirical results exhibit the superior sample quality and improved speed-quality trade-off of PSLD compared to various competing approaches on established image synthesis benchmarks. Remarkably, PSLD achieves 
    
[^188]: 分析和编辑暗藏后门的语言模型的内部机制

    Analyzing And Editing Inner Mechanisms Of Backdoored Language Models. (arXiv:2302.12461v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12461](http://arxiv.org/abs/2302.12461)

    本研究分析并编辑暗藏后门的语言模型的内部机制，发现早期层的MLP模块和初始嵌入投影是后门机制中最重要的部分。通过使用PCP消融技术替换变压器模块，我们成功删除、插入和修改后门机制，并显著改善了后门的输出效果。

    

    数据集中的毒化是对大型语言模型的潜在安全威胁，可能导致暗藏后门的模型。关于暗藏后门语言模型的内部机制以及它们如何处理触发输入（例如，切换至有毒语言）的描述尚未找到。本文研究基于Transformer的暗藏后门语言模型的内部表示，并确定早期层的MLP模块与初始嵌入投影结合是后门机制中最重要的部分。我们利用这些知识来删除、插入和修改后门机制，并用工程化替代物降低MLP模块输出的重要性。为此，我们引入了基于主要成分的低秩矩阵的PCP消融技术，用其替换变压器模块。我们在暗藏后门的玩具模型、暗藏后门的大型模型和非暗藏后门的开源模型上展示了我们的结果。我们表明我们可以改善后门的输出效果。

    Poisoning of data sets is a potential security threat to large language models that can lead to backdoored models. A description of the internal mechanisms of backdoored language models and how they process trigger inputs, e.g., when switching to toxic language, has yet to be found. In this work, we study the internal representations of transformer-based backdoored language models and determine early-layer MLP modules as most important for the backdoor mechanism in combination with the initial embedding projection. We use this knowledge to remove, insert, and modify backdoor mechanisms with engineered replacements that reduce the MLP module outputs to essentials for the backdoor mechanism. To this end, we introduce PCP ablation, where we replace transformer modules with low-rank matrices based on the principal components of their activations. We demonstrate our results on backdoored toy, backdoored large, and non-backdoored open-source models. We show that we can improve the backdoor r
    
[^189]: 分类中的参与个性化

    Participatory Personalization in Classification. (arXiv:2302.03874v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03874](http://arxiv.org/abs/2302.03874)

    本研究引入了一类名为参与性系统的分类模型，允许个体在预测时选择个性化，从而促进并知情同意、提高性能和数据使用。

    

    机器学习模型通常使用受保护的、敏感的、自报告的或昂贵的信息进行个性化。这些模型使用关于人的信息，但不促进也不告知他们的同意。个体不能选择不将个人信息报告给模型，也无法确定他们是否从个性化中受益。我们引入了一类名为参与性系统的分类模型，在预测时允许个体选择个性化。我们提出了一个模型无关的算法来学习用于个性化的参与性系统，其中包含分类组属性。我们在临床预测任务中进行了全面的实证研究，将参与性系统与常用的个性化和补全方法进行了基准测试。我们的结果表明，参与性系统可以促进和知情同意，并在所有报告个人数据的群体中提高性能和数据使用。

    Machine learning models are often personalized with information that is protected, sensitive, self-reported, or costly to acquire. These models use information about people but do not facilitate nor inform their consent. Individuals cannot opt out of reporting personal information to a model, nor tell if they benefit from personalization in the first place. We introduce a family of classification models, called participatory systems, that let individuals opt into personalization at prediction time. We present a model-agnostic algorithm to learn participatory systems for personalization with categorical group attributes. We conduct a comprehensive empirical study of participatory systems in clinical prediction tasks, benchmarking them with common approaches for personalization and imputation. Our results demonstrate that participatory systems can facilitate and inform consent while improving performance and data use across all groups who report personal data.
    
[^190]: 可证明高效的离线目标条件强化学习与通用函数逼近和单一策略集中性研究

    Provably Efficient Offline Goal-Conditioned Reinforcement Learning with General Function Approximation and Single-Policy Concentrability. (arXiv:2302.03770v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03770](http://arxiv.org/abs/2302.03770)

    本文提供了关于离线目标条件强化学习算法的理论分析，证明了经过轻微修改后，该算法在满足通用函数逼近时具有 O(poly(1/ε)) 的样本复杂度。

    

    目标条件强化学习（GCRL）是指学习通用技能，以达到不同的目标。特别是，离线GCRL只需要纯预先收集的数据集来执行训练任务，而无需与环境进行额外的交互。尽管离线GCRL越来越普遍，并且许多之前的工作已经证明了其实证成功，但对于大状态空间且离线数据集仅覆盖我们希望学习的策略的高效离线GCRL算法的理论理解尚未建立起来。本文对一种现有经验成功的离线GCRL算法进行了严格的理论分析。我们证明，通过轻微修改，该算法在具有通用函数逼近的情况下，具有$\widetilde{O}(\text{poly}(1/\epsilon))$样本复杂度（其中$\epsilon$是学习策略的期望非最优性）。

    Goal-conditioned reinforcement learning (GCRL) refers to learning general-purpose skills that aim to reach diverse goals. In particular, offline GCRL only requires purely pre-collected datasets to perform training tasks without additional interactions with the environment. Although offline GCRL has become increasingly prevalent and many previous works have demonstrated its empirical success, the theoretical understanding of efficient offline GCRL algorithms is not well established, especially when the state space is huge and the offline dataset only covers the policy we aim to learn. In this paper, we provide a rigorous theoretical analysis of an existing empirically successful offline GCRL algorithm. We prove that under slight modification, this algorithm enjoys an $\widetilde{O}(\text{poly}(1/\epsilon))$ sample complexity (where $\epsilon$ is the desired suboptimality of the learned policy) with general function approximation thanks to the property of (semi-)strong convexity of the o
    
[^191]: 受比特率限制的DRO: 超越最坏情况鲁棒性对未知群体转变

    Bitrate-Constrained DRO: Beyond Worst Case Robustness To Unknown Group Shifts. (arXiv:2302.02931v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02931](http://arxiv.org/abs/2302.02931)

    本文提出了一种针对分布变化不确定性的受比特率限制的DRO方法，通过假设简单群体函数的条件下，以保持高准确性的模型在低比特率特征上的表现。

    

    对于真实世界应用来说，训练对分布变化具有鲁棒性的机器学习模型非常重要。一些鲁棒训练算法（例如Group DRO）专注于群体变化，并需要在所有训练点上提供群体信息。其他不需要群体注释的方法（例如CVaR DRO）可能过于保守，因为它们会简单地加权计算损失较大的点，而这些点可能是一个虚构的集合，不对应真实世界中的任何有意义的群体（例如，当高损失点是随机错标的训练点时）。在这项研究中，我们通过假设一种更微妙的群体转变形式来解决先前方法的局限性：在给定标签的条件下，我们假设真实的群体函数（群体指示器）是简单的。例如，我们可能期望群体转变发生在低比特率特征上（例如图像背景、光照）。因此，我们的目标是学习一个能够在由这些低比特率特征实现的简单群体函数上保持高准确性的模型。

    Training machine learning models robust to distribution shifts is critical for real-world applications. Some robust training algorithms (e.g., Group DRO) specialize to group shifts and require group information on all training points. Other methods (e.g., CVaR DRO) that do not need group annotations can be overly conservative, since they naively upweight high loss points which may form a contrived set that does not correspond to any meaningful group in the real world (e.g., when the high loss points are randomly mislabeled training points). In this work, we address limitations in prior approaches by assuming a more nuanced form of group shift: conditioned on the label, we assume that the true group function (indicator over group) is simple. For example, we may expect that group shifts occur along low bitrate features (e.g., image background, lighting). Thus, we aim to learn a model that maintains high accuracy on simple group functions realized by these low bitrate features, that need 
    
[^192]: ImageNomer: 一种功能连接和组学分析工具的描述和案例研究，识别出一种种族混淆

    ImageNomer: description of a functional connectivity and omics analysis tool and case study identifying a race confound. (arXiv:2302.00767v2 [q-bio.PE] UPDATED)

    [http://arxiv.org/abs/2302.00767](http://arxiv.org/abs/2302.00767)

    ImageNomer是一个功能连接和组学分析工具，通过提供易于导航的GUI前端以解决fMRI和基因组数据分析中的人口统计学混淆和质量控制问题。

    

    大多数用于基于fMRI的功能连接和基因组数据分析的软件包都使用编程语言界面，缺乏易于导航的GUI前端。这加剧了这些类型数据中存在的两个问题：人口统计学混淆和在高维特征面前的质量控制。原因是使用编程界面来创建必要的可视化图形来识别数据集中的所有相关性、混淆效应或质量控制问题太慢且复杂。为了解决这个问题，我们开发了ImageNomer，一种数据可视化和分析工具，允许检查主体级和队列级的人口统计学、基因组学和成像特征。软件基于Python，运行在一个自包含的Docker镜像中，并包含一个基于浏览器的GUI前端。我们通过识别在Ph。

    Most packages for the analysis of fMRI-based functional connectivity (FC) and genomic data are used with a programming language interface, lacking an easy-to-navigate GUI frontend. This exacerbates two problems found in these types of data: demographic confounds and quality control in the face of high dimensionality of features. The reason is that it is too slow and cumbersome to use a programming interface to create all the necessary visualizations required to identify all correlations, confounding effects, or quality control problems in a dataset. To remedy this situation, we have developed ImageNomer, a data visualization and analysis tool that allows inspection of both subject-level and cohort-level demographic, genomic, and imaging features. The software is Python-based, runs in a self-contained Docker image, and contains a browser-based GUI frontend. We demonstrate the usefulness of ImageNomer by identifying an unexpected race confound when predicting achievement scores in the Ph
    
[^193]: 适应离线算法解决带有赌徒反馈的组合多臂赌博问题的框架

    A Framework for Adapting Offline Algorithms to Solve Combinatorial Multi-Armed Bandit Problems with Bandit Feedback. (arXiv:2301.13326v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13326](http://arxiv.org/abs/2301.13326)

    这个论文介绍了一个框架，用于将离线算法调整为只需要赌徒反馈的亚线性α-后悔方法来解决组合多臂赌博问题，该框架可以实现对于时间界T的预期累积α-后悔依赖度为O(T^（2/3）log（T^（1/3））)。

    

    我们研究了随机的组合多臂赌博问题，其中学习者只能访问赌徒反馈，并且奖励函数可以是非线性的。我们提供了一个通用的框架，将离线离散逼近算法调整为只需要赌徒反馈的亚线性α-后悔方法，实现了对于时间界T的预期累积α-后悔依赖度为O(T^（2/3）log（T^（1/3））)。该框架只需要离线算法对函数评估中的小误差具有鲁棒性。适应过程甚至不需要显式地知道离线逼近算法--离线算法可以被用作黑盒子子程序。为了证明所提出的框架的实用性，将该框架应用于子模最大化的不同应用中。在具有背包约束的子模最大化中，新的CMAB算法优于开发的完全赌徒方法。

    We investigate the problem of stochastic, combinatorial multi-armed bandits where the learner only has access to bandit feedback and the reward function can be non-linear. We provide a general framework for adapting discrete offline approximation algorithms into sublinear $\alpha$-regret methods that only require bandit feedback, achieving $\mathcal{O}\left(T^\frac{2}{3}\log(T)^\frac{1}{3}\right)$ expected cumulative $\alpha$-regret dependence on the horizon $T$. The framework only requires the offline algorithms to be robust to small errors in function evaluation. The adaptation procedure does not even require explicit knowledge of the offline approximation algorithm -- the offline algorithm can be used as a black box subroutine. To demonstrate the utility of the proposed framework, the proposed framework is applied to diverse applications in submodular maximization. The new CMAB algorithms for submodular maximization with knapsack constraints outperform a full-bandit method developed
    
[^194]: 高效的高维计算

    Efficient Hyperdimensional Computing. (arXiv:2301.10902v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10902](http://arxiv.org/abs/2301.10902)

    本论文研究了高维计算中高维度超向量的必要性，并通过理论分析发现，超向量维度的增加会降低高维计算的预测准确度。基于此，研究人员开发了使用维度更低的二进制超向量的高维计算模型，同时保持相等甚至更优的准确度。

    

    高维计算是一种使用高维二进制向量和多数规则进行分类的方法。这种方法由于其简单性和大规模并行性，具有节能的潜力，因此被认为适用于资源受限平台。然而，为了获得高准确度，高维计算有时会使用成千上万维的超向量，这可能抵消了其效率优势。本文研究了这种高维度的必要性，并对超向量维度与准确度之间的关系进行了详细的理论分析。我们的结果表明，随着超向量维度的增加，采用多数规则的高维计算在最坏情况和平均情况下的预测准确度会降低。基于这一观察，我们开发了高维计算模型，使用维度比最先进的模型低几个数量级的二进制超向量，同时保持等效甚至更优的准确度。

    Hyperdimensional computing (HDC) is a method to perform classification that uses binary vectors with high dimensions and the majority rule. This approach has the potential to be energy-efficient and hence deemed suitable for resource-limited platforms due to its simplicity and massive parallelism. However, in order to achieve high accuracy, HDC sometimes uses hypervectors with tens of thousands of dimensions. This potentially negates its efficiency advantage. In this paper, we examine the necessity of such high dimensions and conduct a detailed theoretical analysis of the relationship between hypervector dimensions and accuracy. Our results demonstrate that as the dimension of the hypervectors increases, the worst-case/average-case HDC prediction accuracy with the majority rule decreases. Building on this insight, we develop HDC models that use binary hypervectors with dimensions orders of magnitude lower than those of state-of-the-art HDC models while maintaining equivalent or even im
    
[^195]: 深度强化学习中的自动内在奖励塑造探索方法研究

    Automatic Intrinsic Reward Shaping for Exploration in Deep Reinforcement Learning. (arXiv:2301.10886v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10886](http://arxiv.org/abs/2301.10886)

    本文提出了一种名为AIRS的自动内在奖励塑造探索方法，可以提供高质量的内在激励以增强强化学习中的探索性能；并开发了高效可靠的内在奖励工具包。实验表明，AIRS性能卓越，能够胜过基准方案。

    

    本文提出了一种名为AIRS的自动内在奖励塑造方法，通过智能和适应性的塑造函数，提供高质量的内在激励以增强强化学习中的探索性能。AIRS可以根据实时估计的任务回报从预定义的函数集中选择塑造函数，提供可靠的探索激励并解决偏置目标问题。此外，我们开发了一个内在奖励工具包，提供多种内在奖励方法的高效可靠实现方式。我们将AIRS应用在MiniGrid、Procgen和DeepMind控制套件的多项任务中进行测试。大量仿真结果表明，AIRS可以胜过基准方案，并具有简单的架构和卓越的性能。

    We present AIRS: Automatic Intrinsic Reward Shaping that intelligently and adaptively provides high-quality intrinsic rewards to enhance exploration in reinforcement learning (RL). More specifically, AIRS selects shaping function from a predefined set based on the estimated task return in real-time, providing reliable exploration incentives and alleviating the biased objective problem. Moreover, we develop an intrinsic reward toolkit to provide efficient and reliable implementations of diverse intrinsic reward approaches. We test AIRS on various tasks of MiniGrid, Procgen, and DeepMind Control Suite. Extensive simulation demonstrates that AIRS can outperform the benchmarking schemes and achieve superior performance with simple architecture.
    
[^196]: 用 Finslerian 几何识别潜在距离

    Identifying latent distances with Finslerian geometry. (arXiv:2212.10010v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.10010](http://arxiv.org/abs/2212.10010)

    本文提出了一种新的度量方法，可以明确地最小化期望长度，用于在生成模型的潜在空间中导航。这种方法定义了一个 Finsler 度量，在实践中可以替代现有的随机拉回度量的近似方法。

    

    黎曼几何为我们提供了强大的工具，可以在保持数据底层结构的同时探索生成模型的潜在空间。潜在空间可以配备从数据流形上拉回的黎曼度量。利用这个度量，我们可以系统地在空间中导航，依赖于定义为两点之间最短曲线的测地线。生成模型通常是随机的，导致数据空间、黎曼度量和测地线也是随机的。随机对象最多是不切实际的，最坏的情况下是不可能操作的。一个常见的解决方案是通过期望近似随机拉回度量。但是从这个期望黎曼度量导出的测地线不对应于期望的长度最小化曲线。在这项工作中，我们提出了另一种度量，其测地线明确地最小化了拉回度量的期望长度。我们证明这个度量定义了一个 Finsler 度量，并将其与其他度量进行比较。

    Riemannian geometry provides us with powerful tools to explore the latent space of generative models while preserving the underlying structure of the data. The latent space can be equipped it with a Riemannian metric, pulled back from the data manifold. With this metric, we can systematically navigate the space relying on geodesics defined as the shortest curves between two points. Generative models are often stochastic, causing the data space, the Riemannian metric, and the geodesics, to be stochastic as well. Stochastic objects are at best impractical, and at worst impossible, to manipulate. A common solution is to approximate the stochastic pullback metric by its expectation. But the geodesics derived from this expected Riemannian metric do not correspond to the expected length-minimising curves. In this work, we propose another metric whose geodesics explicitly minimise the expected length of the pullback metric. We show this metric defines a Finsler metric, and we compare it with 
    
[^197]: 关于Text-to-SQL模型的安全漏洞

    On the Security Vulnerabilities of Text-to-SQL Models. (arXiv:2211.15363v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.15363](http://arxiv.org/abs/2211.15363)

    该论文揭示了Text-to-SQL模型存在的安全漏洞，并证明了这些漏洞能够被恶意利用产生攻击，通过对商业应用和开源语言模型的实验验证。该研究意在引起学术界对NLP算法相关的软件安全问题的关注和进一步研究。

    

    尽管已经证明自然语言处理（NLP）算法容易受到恶意攻击，但这些弱点是否可能导致软件安全威胁尚未深入研究。为了弥补这一差距，我们对常用于创建自然语言数据库接口的Text-to-SQL系统进行了漏洞测试。我们展示了六个商业应用中的Text-to-SQL模块可以被操纵以产生恶意代码，潜在地导致数据泄漏和拒绝服务攻击。这是第一个证明NLP模型可以被利用为攻击向量的示例。此外，使用四个开源语言模型的实验验证了对Text-to-SQL系统进行直接后门攻击可以达到100％的成功率，而不影响其性能。本研究旨在引起学术界对与NLP算法相关的潜在软件安全问题的关注，并鼓励进一步探索。

    Although it has been demonstrated that Natural Language Processing (NLP) algorithms are vulnerable to deliberate attacks, the question of whether such weaknesses can lead to software security threats is under-explored. To bridge this gap, we conducted vulnerability tests on Text-to-SQL systems that are commonly used to create natural language interfaces to databases. We showed that the Text-to-SQL modules within six commercial applications can be manipulated to produce malicious code, potentially leading to data breaches and Denial of Service attacks. This is the first demonstration that NLP models can be exploited as attack vectors in the wild. In addition, experiments using four open-source language models verified that straightforward backdoor attacks on Text-to-SQL systems achieve a 100% success rate without affecting their performance. The aim of this work is to draw the community's attention to potential software security issues associated with NLP algorithms and encourage explor
    
[^198]: 走向数据和知识驱动的人工智能：神经符号计算综述

    Towards Data-and Knowledge-Driven Artificial Intelligence: A Survey on Neuro-Symbolic Computing. (arXiv:2210.15889v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2210.15889](http://arxiv.org/abs/2210.15889)

    神经符号计算是将符号和统计认知范式整合的研究领域，在推理和解释性以及神经网络学习等方面具有潜力。本文综述了该领域的最新进展和重要贡献，并探讨了其成功应用案例和未来发展方向。

    

    多年来，神经符号计算（NeSy）一直是人工智能（AI）领域的研究热点，追求符号和统计认知范式的整合。由于NeSy在符号表示的推理和可解释性以及神经网络的强大学习中具有潜力，它可能成为下一代AI的催化剂。本文系统综述了NeSy研究的最新进展和重要贡献。首先，我们介绍了该领域的研究历史，涵盖了早期工作和基础知识。我们进一步讨论了背景概念，并确定了推动NeSy发展的关键因素。随后，我们按照几个主要特征对近期的里程碑方法进行分类，包括神经符号整合、知识表示、知识嵌入和功能性。接下来，我们简要讨论了成功的应用案例，以及NeSy领域的挑战和未来发展方向。

    Neural-symbolic computing (NeSy), which pursues the integration of the symbolic and statistical paradigms of cognition, has been an active research area of Artificial Intelligence (AI) for many years. As NeSy shows promise of reconciling the advantages of reasoning and interpretability of symbolic representation and robust learning in neural networks, it may serve as a catalyst for the next generation of AI. In the present paper, we provide a systematic overview of the recent developments and important contributions of NeSy research. Firstly, we introduce study history of this area, covering early work and foundations. We further discuss background concepts and identify key driving factors behind the development of NeSy. Afterward, we categorize recent landmark approaches along several main characteristics that underline this research paradigm, including neural-symbolic integration, knowledge representation, knowledge embedding, and functionality. Next, we briefly discuss the successfu
    
[^199]: SpacePhish: 使用机器学习对抗钓鱼网站检测器的攻击的逃避空间

    SpacePhish: The Evasion-space of Adversarial Attacks against Phishing Website Detectors using Machine Learning. (arXiv:2210.13660v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2210.13660](http://arxiv.org/abs/2210.13660)

    该论文研究了对抗机器学习中钓鱼网站检测的攻击以及针对这些攻击的逃避防御空间，并提出了一种现实威胁模型。

    

    现有的关于对抗机器学习的文献要么展示能够破坏所有机器学习模型的攻击，要么展示能够抵御大部分攻击的防御措施。然而，很少考虑攻击或防御的实际可行性。此外，对抗样本通常是在“特征空间”中生成的，这使得相关评估的价值值得怀疑。简而言之，目前的情况不允许估计对抗攻击所带来的真实威胁，这导致缺乏安全的机器学习系统。本文旨在澄清这种困惑。通过考虑机器学习用于钓鱼网站检测的应用，我们将“逃避空间”形式化为一种能够引入对抗扰动以欺骗机器学习-钓鱼网站检测的空间-证明即使在“特征空间”中的扰动也是有用的。然后，我们提出了一个描述针对机器学习-钓鱼网站检测的逃避攻击的现实威胁模型，这种攻击容易实施，因此更具吸引力。

    Existing literature on adversarial Machine Learning (ML) focuses either on showing attacks that break every ML model, or defenses that withstand most attacks. Unfortunately, little consideration is given to the actual feasibility of the attack or the defense. Moreover, adversarial samples are often crafted in the "feature-space", making the corresponding evaluations of questionable value. Simply put, the current situation does not allow to estimate the actual threat posed by adversarial attacks, leading to a lack of secure ML systems.  We aim to clarify such confusion in this paper. By considering the application of ML for Phishing Website Detection (PWD), we formalize the "evasion-space" in which an adversarial perturbation can be introduced to fool a ML-PWD -- demonstrating that even perturbations in the "feature-space" are useful. Then, we propose a realistic threat model describing evasion attacks against ML-PWD that are cheap to stage, and hence intrinsically more attractive for r
    
[^200]: 网络合成干预：在网络干扰下进行面板数据的因果框架

    Network Synthetic Interventions: A Causal Framework for Panel Data Under Network Interference. (arXiv:2210.11355v2 [econ.EM] UPDATED)

    [http://arxiv.org/abs/2210.11355](http://arxiv.org/abs/2210.11355)

    本研究提出了一种网络干扰下的面板数据因果框架，其中包括网络干扰的合成控制和合成干预方法。通过引入新颖的潜在因子模型，我们的方法能够在解决溢出和未观测混杂的情况下提供准确的反事实估计。

    

    我们提出了一种将合成控制和合成干预方法推广到网络干扰的方法。我们考虑在面板数据中估计单元特定的潜在结果，在单位之间存在溢出和未观测混杂的情况下。我们的方法的关键是使用一种新颖的潜在因子模型，它考虑了网络干扰，并推广了通常在面板数据环境中使用的因子模型。我们提出了一个估计器，名为网络合成干预 (NSI)，并且证明它在任意一组对网络进行反事实处理时一致地估计单位的平均结果。我们进一步证明了该估计器在渐近意义下是正态分布的。我们提供了两个有效性测试，以确定NSI估计器是否可靠地推广以产生准确的反事实估计。我们提供了一种基于图的实验设计，保证了NSI估计器产生准确的反事实估计，并分析了样本。

    We propose a generalization of the synthetic controls and synthetic interventions methodology to incorporate network interference. We consider the estimation of unit-specific potential outcomes from panel data in the presence of spillover across units and unobserved confounding. Key to our approach is a novel latent factor model that takes into account network interference and generalizes the factor models typically used in panel data settings. We propose an estimator, Network Synthetic Interventions (NSI), and show that it consistently estimates the mean outcomes for a unit under an arbitrary set of counterfactual treatments for the network. We further establish that the estimator is asymptotically normal. We furnish two validity tests for whether the NSI estimator reliably generalizes to produce accurate counterfactual estimates. We provide a novel graph-based experiment design that guarantees the NSI estimator produces accurate counterfactual estimates, and also analyze the sample c
    
[^201]: MMTSA: 多模式时序段注意力网络用于高效的人体活动识别

    MMTSA: Multimodal Temporal Segment Attention Network for Efficient Human Activity Recognition. (arXiv:2210.09222v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.09222](http://arxiv.org/abs/2210.09222)

    MMTSA是一种多模式神经网络架构，用于高效的人体活动识别。它利用多模式传感器的互补信息，通过将IMU数据转换为时序结构保持的灰度图像，并采用多模式稀疏采样和段间关注模块实现了高效的多模式融合。在评估中显示，该方法在人体活动识别方面取得了显著的性能提升。

    

    多模式传感器为开发准确的人体活动识别的机器学习方法提供了互补信息，但也引入了显著的计算负荷，从而降低了效率。本文提出了一种用于人体活动识别的高效多模式神经架构，称为多模式时序段注意力网络（MMTSA）。MMTSA首先使用格拉姆角场（GAF）将惯性测量单元（IMU）传感器数据转换为时序和结构保持的灰度图像，表示人体活动的固有属性。然后，MMTSA应用多模式稀疏采样方法来减少数据冗余。最后，MMTSA采用一种段间关注模块来实现高效多模式融合。使用三个公开数据集，我们评估了MMTSA在人体活动识别中的有效性和效率。结果表明，我们的方法在MMAct数据集上跨受试者F1分数方面取得了卓越的性能提升，达到11.13%。

    Multimodal sensors provide complementary information to develop accurate machine-learning methods for human activity recognition (HAR), but introduce significantly higher computational load, which reduces efficiency. This paper proposes an efficient multimodal neural architecture for HAR using an RGB camera and inertial measurement units (IMUs) called Multimodal Temporal Segment Attention Network (MMTSA). MMTSA first transforms IMU sensor data into a temporal and structure-preserving gray-scale image using the Gramian Angular Field (GAF), representing the inherent properties of human activities. MMTSA then applies a multimodal sparse sampling method to reduce data redundancy. Lastly, MMTSA adopts an inter-segment attention module for efficient multimodal fusion. Using three well-established public datasets, we evaluated MMTSA's effectiveness and efficiency in HAR. Results show that our method achieves superior performance improvements 11.13% of cross-subject F1-score on the MMAct datas
    
[^202]: 实数值和计数时间序列的高效概率调和预测

    Efficient probabilistic reconciliation of forecasts for real-valued and count time series. (arXiv:2210.02286v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.02286](http://arxiv.org/abs/2210.02286)

    本论文提出了一种基于条件方法来调和任何类型的预测分布的新方法，并引入了一种名为“自下而上重要性抽样”的高效抽样算法。这种方法在多个时间层次的实验中显示出与基本概率预测相比的显著改进。

    

    层次时间序列在许多应用领域中很常见。这些时间序列的预测需要是一致的，也就是满足层次结构的约束条件。强制一致性的最流行技术被称为调和，它调整了为每个时间序列计算的基本预测。然而，最近关于概率调和的研究存在几个局限性。在本文中，我们提出了一种新的基于条件方法来调和任何类型的预测分布。然后，我们引入了一种名为“自下而上重要性抽样”的新算法，来高效地从调和后的分布中进行抽样。它可以用于任何基本预测分布：离散的、连续的，或者以样本形式提供的，相对于当前方法，它提供了很大的速度提升。在几个时间层次的实验中，显示出与基本概率预测相比的显著改进。

    Hierarchical time series are common in several applied fields. The forecasts for these time series are required to be coherent, that is, to satisfy the constraints given by the hierarchy. The most popular technique to enforce coherence is called reconciliation, which adjusts the base forecasts computed for each time series. However, recent works on probabilistic reconciliation present several limitations. In this paper, we propose a new approach based on conditioning to reconcile any type of forecast distribution. We then introduce a new algorithm, called Bottom-Up Importance Sampling, to efficiently sample from the reconciled distribution. It can be used for any base forecast distribution: discrete, continuous, or in the form of samples, providing a major speedup compared to the current methods. Experiments on several temporal hierarchies show a significant improvement over base probabilistic forecasts.
    
[^203]: GP-net: 灵活的视角抓取提案

    GP-net: Flexible Viewpoint Grasp Proposal. (arXiv:2209.10404v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2209.10404](http://arxiv.org/abs/2209.10404)

    GP-net 可以从灵活的视角，例如移动机械臂所体验的视角，生成六自由度的抓取，在真实世界的实验中，它实现了 51.8% 的抓取成功率，相比之下，机器人抓取技术的最新方法成功率更低，需要定义工作空间。

    

    我们提出了一种名为 Grasp Proposal Network (GP-net) 的卷积神经网络模型，可以从灵活的视角，例如移动机械臂所体验的视角，生成六自由度的抓取。我们通过合成生成深度图像和标注抓取信息的数据集来训练 GP-net。在真实世界的实验中，我们使用 EGAD! 抓取基准测试对 GP-net 进行评估，并将其与两种常用算法——Volumetric Grasping Network (VGN) 和 Grasp Pose Detection package (GPD) 进行比较，在 PAL TIAGo 移动机器人上。与机器人抓取技术的最新方法相比，GP-net 可以用于从灵活的未知视角抓取对象，而无需定义工作空间，并且抓取成功率达到 51.8%，相比之下，VGN 为 51.1%，GPD 为 33.6%。我们提供了一个 ROS 包，以及我们的代码和预训练模型，网址为 https://aucoroboticsmu.github.io/GP-net/。

    We present the Grasp Proposal Network (GP-net), a Convolutional Neural Network model which can generate 6-DOF grasps from flexible viewpoints, e.g. as experienced by mobile manipulators. To train GP-net, we synthetically generate a dataset containing depth-images and ground-truth grasp information. In real-world experiments we use the EGAD! grasping benchmark to evaluate GP-net against two commonly used algorithms, the Volumetric Grasping Network (VGN) and the Grasp Pose Detection package (GPD), on a PAL TIAGo mobile manipulator. In contrast to the state-of-the-art methods in robotic grasping, GP-net can be used for grasping objects from flexible, unknown viewpoints without the need to define the workspace and achieves a grasp success of 51.8% compared to 51.1% for VGN and 33.6% for GPD. We provide a ROS package along with our code and pre-trained models at https://aucoroboticsmu.github.io/GP-net/.
    
[^204]: 准算术混合、散度最小化和Bregman信息

    Quasi-Arithmetic Mixtures, Divergence Minimization, and Bregman Information. (arXiv:2209.07481v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.07481](http://arxiv.org/abs/2209.07481)

    本文提供了对准算术混合、散度最小化和Bregman信息的全面分析。通过在密度函数的单调嵌入下使用Bregman散度，我们将常见的散度函数与退火路径上的中间密度关联起来。

    

    马尔可夫链蒙特卡洛方法用于从复杂分布中采样和估计归一化常数通常模拟沿着连接可跟踪初始分布和目标密度的退火路径的中间分布的样本。先前的工作使用准算术平均构建了退火路径，并解释了由此产生的中间密度是最小化期望散度到端点的。我们通过在密度函数的单调嵌入下使用Bregman散度对这个“质心”性质进行了全面分析，从而将常见的散度（如Amari和Renyi的alpha散度、（alpha，beta）散度和Jensen-Shannon散度）与沿着退火路径的中间密度关联起来。我们的分析突出了参数化族、准算术平均和散度函数之间的相互作用，使用了Zhang的rho-tau Bregman散度框架。

    Markov Chain Monte Carlo methods for sampling from complex distributions and estimating normalization constants often simulate samples from a sequence of intermediate distributions along an annealing path, which bridges between a tractable initial distribution and a target density of interest. Prior work has constructed annealing paths using quasi-arithmetic means, and interpreted the resulting intermediate densities as minimizing an expected divergence to the endpoints. We provide a comprehensive analysis of this 'centroid' property using Bregman divergences under a monotonic embedding of the density function, thereby associating common divergences such as Amari's and Renyi's ${\alpha}$-divergences, ${(\alpha,\beta)}$-divergences, and the Jensen-Shannon divergence with intermediate densities along an annealing path. Our analysis highlights the interplay between parametric families, quasi-arithmetic means, and divergence functions using the rho-tau Bregman divergence framework of Zhang
    
[^205]: 一种用于多步鲍型自适应异方差时间序列预测的通用框架

    A general framework for multi-step ahead adaptive conformal heteroscedastic time series forecasting. (arXiv:2207.14219v7 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.14219](http://arxiv.org/abs/2207.14219)

    本文介绍了一种名为AEnbMIMOCQR的新颖算法，通过自适应集成的方式，在不需要数据拆分的情况下，以分布无关的方式生成多步鲍型预测区间。该方法考虑了异方差性，并对分布转变具有鲁棒性，在实验中表现优于其他竞争方法。

    

    本文介绍了一种新颖的模型无关算法，名为自适应集成批量多输入多输出鲍型分位数回归（AEnbMIMOCQR），使得预测者能够以分布无关的方式生成固定预设失配率的多步鲍型预测区间。我们的方法基于鲍型预测原理，但不需要数据拆分，并且即使在数据不可互换的情况下也能提供接近精确的覆盖率。此外，所得到的预测区间在预测时间范围内经验证明有效，并且考虑了异方差性。AEnbMIMOCQR被设计成对分布转变具有鲁棒性，这意味着其预测区间在无限的时间范围内保持可靠，而无需重新训练或对数据生成过程进行不切实际的严格假设。通过系统实验，我们证明了我们的方法在鲍型预测中优于其他竞争方法。

    This paper introduces a novel model-agnostic algorithm called adaptive ensemble batch multi-input multi-output conformalized quantile regression (AEnbMIMOCQR} that enables forecasters to generate multi-step ahead prediction intervals for a fixed pre-specified miscoverage rate in a distribution-free manner. Our method is grounded on conformal prediction principles, however, it does not require data splitting and provides close to exact coverage even when the data is not exchangeable. Moreover, the resulting prediction intervals, besides being empirically valid along the forecast horizon, do not neglect heteroscedasticity. AEnbMIMOCQR is designed to be robust to distribution shifts, which means that its prediction intervals remain reliable over an unlimited period of time, without entailing retraining or imposing unrealistic strict assumptions on the data-generating process. Through methodically experimentation, we demonstrate that our approach outperforms other competitive methods on bo
    
[^206]: MemSAC: 大规模无监督领域自适应的记忆增强样本一致性

    MemSAC: Memory Augmented Sample Consistency for Large Scale Unsupervised Domain Adaptation. (arXiv:2207.12389v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2207.12389](http://arxiv.org/abs/2207.12389)

    MemSAC提出了一种记忆增强样本一致性方法，通过利用源域和目标域之间的样本级相似性实现判别转移，并且在大规模数据集上表现出明显的优势。

    

    实际的现实世界数据集引入了无监督领域自适应的新挑战，如类间区分度小，现有的仅依赖于域不变性的方法无法很好地处理。在这项工作中，我们提出了MemSAC，它利用源域和目标域之间的样本级相似性实现判别转移，并可扩展到大量类别。为此，我们首先引入了一种记忆增强方法，以有效地提取标记源域和未标记目标域实例之间的成对相似性关系，适合处理任意数量的类别。接下来，我们提出并从理论上证明了一种新颖的对比损失的变体，以促进类内跨域样本之间的局部一致性，同时确保类别之间的分离，从而保持从源域到目标域的判别转移。我们验证了MemSAC的优势和显著性。

    Practical real world datasets with plentiful categories introduce new challenges for unsupervised domain adaptation like small inter-class discriminability, that existing approaches relying on domain invariance alone cannot handle sufficiently well. In this work we propose MemSAC, which exploits sample level similarity across source and target domains to achieve discriminative transfer, along with architectures that scale to a large number of categories. For this purpose, we first introduce a memory augmented approach to efficiently extract pairwise similarity relations between labeled source and unlabeled target domain instances, suited to handle an arbitrary number of classes. Next, we propose and theoretically justify a novel variant of the contrastive loss to promote local consistency among within-class cross domain samples while enforcing separation between classes, thus preserving discriminative transfer from source to target. We validate the advantages of MemSAC with significant
    
[^207]: 从小数据集中实现联合学习

    Federated Learning from Small Datasets. (arXiv:2110.03469v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.03469](http://arxiv.org/abs/2110.03469)

    本文介绍了一种在小数据集上实现联合学习的新方法，通过将模型聚合与本地模型的置换相结合，可以更高效地在数据稀疏的领域进行训练。

    

    联合学习允许多个参与方在不共享本地数据的情况下协同训练一个联合模型。这在医疗领域等数据本身分布分散、无法公开的情况下，为机器学习应用提供了可能。在实践中，通常通过聚合本地模型实现联合训练，而本地训练目标的期望与全局目标相似。然而，由于本地数据集通常很小，导致本地目标与全局目标差异很大，从而使联合学习失败。我们提出了一种新颖的方法，将模型聚合与本地模型的置换相结合。这种置换将每个本地模型暴露给一系列本地数据集，从而在数据稀疏的领域中实现更高效的训练。这使得可以在极小的本地数据集上进行训练，例如跨医院的患者数据，同时保持联合学习的训练效率和隐私保护的好处。

    Federated learning allows multiple parties to collaboratively train a joint model without sharing local data. This enables applications of machine learning in settings of inherently distributed, undisclosable data such as in the medical domain. In practice, joint training is usually achieved by aggregating local models, for which local training objectives have to be in expectation similar to the joint (global) objective. Often, however, local datasets are so small that local objectives differ greatly from the global objective, resulting in federated learning to fail. We propose a novel approach that intertwines model aggregations with permutations of local models. The permutations expose each local model to a daisy chain of local datasets resulting in more efficient training in data-sparse domains. This enables training on extremely small local datasets, such as patient data across hospitals, while retaining the training efficiency and privacy benefits of federated learning.
    
[^208]: CTR预测中基于稀疏分组Lasso的神经网络自适应优化器

    Adaptive Optimizers with Sparse Group Lasso for Neural Networks in CTR Prediction. (arXiv:2107.14432v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.14432](http://arxiv.org/abs/2107.14432)

    本论文提出了一种新的框架，在神经网络的CTR预测中加入了稀疏分组Lasso的正则项，并创建了一类新的自适应优化器。实验证明，这些优化器在相同稀疏水平下可以显著提升模型性能，并且能够实现极高的稀疏性。

    

    我们在深度学习中开发了一个新的框架，将稀疏分组Lasso的正则项加入到一系列自适应优化器中，如Momentum、Adagrad、Adam、AMSGrad、AdaHessian等，并创建了一类新的优化器，分别命名为Group Momentum、Group Adagrad、Group Adam、Group AMSGrad和Group AdaHessian等。我们基于原始-对偶方法在随机凸设置下建立了理论上的收敛保证。我们使用最先进的深度学习模型，在三个大规模真实广告点击数据集上评估了我们新优化器的正则效果。实验结果表明，与使用幅度修剪方法的原始优化器相比，模型在相同稀疏水平上的性能可以显著提升。此外，与没有幅度修剪的情况相比，我们的方法可以实现极高的稀疏性，同时具有更好或更高的性能。

    We develop a novel framework that adds the regularizers of the sparse group lasso to a family of adaptive optimizers in deep learning, such as Momentum, Adagrad, Adam, AMSGrad, AdaHessian, and create a new class of optimizers, which are named Group Momentum, Group Adagrad, Group Adam, Group AMSGrad and Group AdaHessian, etc., accordingly. We establish theoretically proven convergence guarantees in the stochastic convex settings, based on primal-dual methods. We evaluate the regularized effect of our new optimizers on three large-scale real-world ad click datasets with state-of-the-art deep learning models. The experimental results reveal that compared with the original optimizers with the post-processing procedure which uses the magnitude pruning method, the performance of the models can be significantly improved on the same sparsity level. Furthermore, in comparison to the cases without magnitude pruning, our methods can achieve extremely high sparsity with significantly better or hig
    
[^209]: 有条件Sig-Wasserstein GANs用于时间序列生成

    Conditional Sig-Wasserstein GANs for Time Series Generation. (arXiv:2006.05421v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2006.05421](http://arxiv.org/abs/2006.05421)

    本论文提出了一种有条件的Sig-Wasserstein GANs框架，通过将WGANs与路径特征提取方法相结合，用于解决时间序列生成中的挑战。路径的签名作为一种通用描述数据流的统计特征，可以刻画时序模型的分布特征。

    

    生成对抗网络（GANs）在生成样本方面取得了极大的成功，从看似高维的概率分布中生成样本。然而，这些方法在捕捉由时间序列数据引起的联合概率分布的时序依赖性方面存在困难。此外，长时间序列数据流会极大地增加目标空间的维度，可能使生成建模变得不可行。为了克服这些挑战，受计量经济学中自回归模型的启发，我们对给定过去信息的未来时间序列的条件分布非常感兴趣。我们提出了一种通用的条件Sig-WGAN框架，通过将Wasserstein-GANs（WGANs）与数学上有原则且高效的路径特征提取技术——路径的签名相结合。路径的签名是一系列分级统计量，为数据流提供了一个通用的描述，其期望值刻画了时间序列模型的分布特征。

    Generative adversarial networks (GANs) have been extremely successful in generating samples, from seemingly high dimensional probability measures. However, these methods struggle to capture the temporal dependence of joint probability distributions induced by time-series data. Furthermore, long time-series data streams hugely increase the dimension of the target space, which may render generative modelling infeasible. To overcome these challenges, motivated by the autoregressive models in econometric, we are interested in the conditional distribution of future time series given the past information. We propose the generic conditional Sig-WGAN framework by integrating Wasserstein-GANs (WGANs) with mathematically principled and efficient path feature extraction called the signature of a path. The signature of a path is a graded sequence of statistics that provides a universal description for a stream of data, and its expected value characterises the law of the time-series model. In parti
    
[^210]: 基于贝叶斯优化的动态子目标导向的探索

    Dynamic Subgoal-based Exploration via Bayesian Optimization. (arXiv:1910.09143v4 [math.OC] UPDATED)

    [http://arxiv.org/abs/1910.09143](http://arxiv.org/abs/1910.09143)

    本论文提出了一种基于贝叶斯优化的成本感知探索方法，能够针对稀疏奖励导航环境中的复杂任务，高效地搜索动态子目标的探索策略。

    

    在稀疏奖励导航环境中，通过昂贵和有限的交互中进行强化学习是具有挑战性的，需要有效的探索策略。针对需要现实世界训练的复杂导航任务（当廉价模拟器不可用时），我们考虑一个面临未知环境分布的代理，并且必须决定一种探索策略。在从相同环境分布中选择的测试环境中评估之前，它可以利用一系列训练环境来改进其策略。大多数现有方法关注固定的探索策略，而将探索视为元优化问题的少数方法往往忽视了对成本高效的探索的需求。我们提出了一种成本感知的贝叶斯优化方法，可以高效地搜索一类基于动态子目标的探索策略。该算法调整多个杠杆--子目标的位置，每个episode的长度以及nu的值。

    Reinforcement learning in sparse-reward navigation environments with expensive and limited interactions is challenging and poses a need for effective exploration. Motivated by complex navigation tasks that require real-world training (when cheap simulators are not available), we consider an agent that faces an unknown distribution of environments and must decide on an exploration strategy. It may leverage a series of training environments to improve its policy before it is evaluated in a test environment drawn from the same environment distribution. Most existing approaches focus on fixed exploration strategies, while the few that view exploration as a meta-optimization problem tend to ignore the need for cost-efficient exploration. We propose a cost-aware Bayesian optimization approach that efficiently searches over a class of dynamic subgoal-based exploration strategies. The algorithm adjusts a variety of levers -- the locations of the subgoals, the length of each episode, and the nu
    
[^211]: L2P: 学习放置用于估计重尾分布结果

    L2P: Learning to Place for Estimating Heavy-Tailed Distributed Outcomes. (arXiv:1908.04628v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1908.04628](http://arxiv.org/abs/1908.04628)

    L2P是一种利用实例之间成对关系进行学习的方法，用于解决具有重尾分布特征的预测任务。实验证明，L2P在准确度和能力方面优于竞争方法。

    

    许多现实世界的预测任务具有具有重尾分布特征的结果变量。例如，销售的图书副本，艺术品拍卖价格，仓库中商品的需求等。通过学习重尾分布，"大而罕见"的实例（例如，畅销书）将具有准确的预测。大多数现有方法并不专注于学习重尾分布；因此，它们往往会对这些实例进行严重低估。为了解决这个问题，我们引入了学习放置（L2P）方法，它利用实例之间的成对关系进行学习。在训练阶段，L2P学习一对一偏好分类器：实例A是否大于实例B？在放置阶段，L2P通过将新实例放置在已知实例之间来获得预测结果。根据其放置位置，新实例被分配一个结果变量的值。实验结果表明，L2P在准确度和能力方面优于竞争方法。

    Many real-world prediction tasks have outcome variables that have characteristic heavy-tail distributions. Examples include copies of books sold, auction prices of art pieces, demand for commodities in warehouses, etc. By learning heavy-tailed distributions, "big and rare" instances (e.g., the best-sellers) will have accurate predictions. Most existing approaches are not dedicated to learning heavy-tailed distribution; thus, they heavily under-predict such instances. To tackle this problem, we introduce Learning to Place (L2P), which exploits the pairwise relationships between instances for learning. In its training phase, L2P learns a pairwise preference classifier: is instance A > instance B? In its placing phase, L2P obtains a prediction by placing the new instance among the known instances. Based on its placement, the new instance is then assigned a value for its outcome variable. Experiments on real data show that L2P outperforms competing approaches in terms of accuracy and abili
    

