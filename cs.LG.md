# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [InceptionNeXt: When Inception Meets ConvNeXt.](http://arxiv.org/abs/2303.16900) | 本论文提出了一种名为InceptionNeXt的新型神经网络，通过将大内核卷积沿通道维度分解为四个平行分支来提高模型效率，解决了保持性能的同时加快基于大内核的CNN模型的问题。 |
| [^2] | [Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos.](http://arxiv.org/abs/2303.16897) | 该论文提出了一种物理驱动扩散模型，可以为silent视频剪辑合成高保真的冲击声，并使用额外的物理先验知识来指导冲击声合成过程。 |
| [^3] | [Towards Understanding the Effect of Pretraining Label Granularity.](http://arxiv.org/abs/2303.16887) | 本文探究了预训练标签粒度对深度神经网络图像分类任务的泛化能力的影响，并在iNaturalist 2021与ImageNet数据集中进行了实验证明，在预训练数据集具有强有力的标签层次结构，标签功能与目标任务对齐，以及选择适当的预训练标签粒度时，能有效提高模型的性能。 |
| [^4] | [The Hidden-Manifold Hopfield Model and a learning phase transition.](http://arxiv.org/abs/2303.16880) | 提出一种称为隐藏流形Hopfield模型的广义Hopfield模型，重点研究了在复杂数据上的应用，发现其中存在着一种学习相变的现象。 |
| [^5] | [Application of probabilistic modeling and automated machine learning framework for high-dimensional stress field.](http://arxiv.org/abs/2303.16869) | 本文探讨了数据驱动的代理建模在高维物理问题中的应用，提出了一种基于自动化机器学习框架的概率建模方法，以降低计算成本和提高预测准确性和精度。 |
| [^6] | [ALUM: Adversarial Data Uncertainty Modeling from Latent Model Uncertainty Compensation.](http://arxiv.org/abs/2303.16866) | 提出了一种新方法ALUM，可以同时处理模型不确定性和数据不确定性。通过探索开采的对抗三元组来促进数据不确定性建模和非参数不确定性估计，以弥补潜在教练层的不足。该方法是模型不可知的，可以很容易地整合到任何现有的深度模型中，以提高模型的稳健性。 |
| [^7] | [Beyond Empirical Risk Minimization: Local Structure Preserving Regularization for Improving Adversarial Robustness.](http://arxiv.org/abs/2303.16861) | 提出一种新颖的局部结构保持正则化方法，旨在在学习的嵌入空间中保持输入空间的局部结构，与基线相比，该方法显著提高了对抗鲁棒性的性能。 |
| [^8] | [Physical Deep Reinforcement Learning Towards Safety Guarantee.](http://arxiv.org/abs/2303.16860) | Phy-DRL是一种物理深度强化学习框架，采用类李雅普诺夫奖赏和残差控制的架构设计，具备可证明的安全性和稳定性保障，实验结果表明其能够提升训练速度和奖励，具有良好的鲁棒性。 |
| [^9] | [Diffusion Schr\"odinger Bridge Matching.](http://arxiv.org/abs/2303.16852) | 本文介绍了一种新的方法 Iterative Markovian Fitting，用于解决高维度 Schr\"odinger桥（SBs）问题，该方法的数值实验表现出在准确性和性能方面的显著优势。 |
| [^10] | [Randomly Projected Convex Clustering Model: Motivation, Realization, and Cluster Recovery Guarantees.](http://arxiv.org/abs/2303.16841) | 本文提出了一种随机投影凸聚类模型，具有簇恢复保证和良好的鲁棒性，在聚类精度和可伸缩性方面优于许多最先进的聚类算法。 |
| [^11] | [MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks.](http://arxiv.org/abs/2303.16839) | 提出了一种名为MaMMUT的简单模型，可以通过两步方法容纳对比和生成学习，并在联合训练不同的视觉语言任务时表现出很高的效力。 |
| [^12] | [Zero-shot Entailment of Leaderboards for Empirical AI Research.](http://arxiv.org/abs/2303.16835) | 本文探讨了自动挖掘 Empirical AI Research 领域排行榜这一任务类别中的零样本学习现象。实验测试了先前报告的最新技术模型在其未见过的排行榜标签上的泛化能力或 entailment 能力。本文创建了一个零样本标记的数据集。 |
| [^13] | [Decision Making for Autonomous Driving in Interactive Merge Scenarios via Learning-based Prediction.](http://arxiv.org/abs/2303.16821) | 本研究提出了在自动驾驶车辆与其他车辆交互合流的情况下，基于学习的预测的决策制定方法来解决高度挑战的决策问题和不确定性，并采用部分可观察的马尔可夫决策过程（POMDP）和蒙特卡罗树搜索算法来执行高级驾驶操作。 |
| [^14] | [PAC-Bayesian bounds for learning LTI-ss systems with input from empirical loss.](http://arxiv.org/abs/2303.16816) | 本文推导出应用于有限数据点学习模型的PAC-Bayesian误差界限，使我们能够为广泛的学习/系统辨识算法提供有限样本误差边界。 |
| [^15] | [Optimal approximation of $C^k$-functions using shallow complex-valued neural networks.](http://arxiv.org/abs/2303.16813) | 本文证明了对于$C^k$（在实变量意义下）的函数，使用具有单层隐藏层和$m$个神经元的神经网络可以以错误率$m^{-k/(2n)}$将其逼近。此外，如果选取权值$\sigma_j,b_j\in\mathbb{C}$和$\rho_j\in\mathbb{C}^n$对$f$连续，那么获得的逼近速率是最优的。 |
| [^16] | [Module-based regularization improves Gaussian graphical models when observing noisy data.](http://arxiv.org/abs/2303.16796) | 建议将推断网络的模块化结构整合到正则化强度的交叉验证中，以改善高斯图模型在观察含噪数据时的表现。 |
| [^17] | [GRAF: Graph Attention-aware Fusion Networks.](http://arxiv.org/abs/2303.16781) | 本文提出了一个名为GRAF的方法，使用注意机制和网络融合在多个网络上使用基于GNN的方法。通过attention-based neighborhood aggregation，GRAF能够学习每个节点和关联的重要性，并根据它们在网络融合中进行边缘加权。 |
| [^18] | [Language Models Trained on Media Diets Can Predict Public Opinion.](http://arxiv.org/abs/2303.16779) | 该论文介绍了一种新的方法来探索媒体偏好模型——适用于在线新闻、电视广播或广播节目内容的语言模型，可以预测公众舆论并具有实际应用价值。 |
| [^19] | [Assorted, Archetypal and Annotated Two Million (3A2M) Cooking Recipes Dataset based on Active Learning.](http://arxiv.org/abs/2303.16778) | 本研究利用领域专家的知识和主动学习技术呈现了一个两百万份烹饪食谱新数据集，通过将30万份食谱按照命名实体识别进行分类到9个类别中，然后使用混合方法对其余的1900K份食谱进行分类。 |
| [^20] | [Not cool, calm or collected: Using emotional language to detect COVID-19 misinformation.](http://arxiv.org/abs/2303.16777) | 本论文提出一种使用情感语言检测COVID-19错误信息的模型，该模型相比于单一的错误信息分类器具有更好的效果，但该研究的主要限制因素是低质量标签和不匹配的标签分布。 |
| [^21] | [Supervised Learning for Table Tennis Match Prediction.](http://arxiv.org/abs/2303.16776) | 本研究研究了使用机器学习预测乒乓球单打比赛结果，使用球员和比赛统计数据作为特征。通过消融研究，我们发现5折交叉验证和超参数调整是提高模型性能的关键。我们证明了不同的特征聚合策略可以提高模型的鲁棒性。最终，我们的模型与最新的类似研究相匹配。 |
| [^22] | [Computationally Efficient Labeling of Cancer Related Forum Posts by Non-Clinical Text Information Retrieval.](http://arxiv.org/abs/2303.16766) | 本研究基于非临床和免费可用的信息，结合分布式计算、文本检索、聚类和分类方法开发了一个能够检索、聚类和展示关于癌症病程信息的计算有效系统。 |
| [^23] | [ACO-tagger: A Novel Method for Part-of-Speech Tagging using Ant Colony Optimization.](http://arxiv.org/abs/2303.16760) | 本研究提出了一种基于蚁群优化的高效词性标注方法ACO-tagger，实现了高达96.867％的准确率，优于几种最先进的方法。 |
| [^24] | [Exploring celebrity influence on public attitude towards the COVID-19 pandemic: social media shared sentiment analysis.](http://arxiv.org/abs/2303.16759) | 本文研究了公众人物在社交媒体上共享的信息对 COVID-19 疫情中的公众情感和大众意见的影响。通过收集和分析推文，发现公众人物的信息对公众情感和大众意见具有显著的影响。 |
| [^25] | [Training Language Models with Language Feedback at Scale.](http://arxiv.org/abs/2303.16755) | 本文提出一种新方法，即利用更丰富的语言反馈进行模仿学习，通过三个迭代步骤对语言模型进行训练以生成更符合人类偏好的输出。 |
| [^26] | [Deep transfer learning for detecting Covid-19, Pneumonia and Tuberculosis using CXR images -- A Review.](http://arxiv.org/abs/2303.16754) | 本综述研究了使用深度迁移学习技术检测胸部X射线图像中COVID-19、肺炎和结核病的情况。文章调查了现有的图像分类技术并讨论了将迁移学习应用于这一领域的挑战和机会。文章重点考察了近期使用深度迁移学习算法进行COVID-19、肺炎和结核病检测的研究，并探讨了未来深度迁移学习在CX图像分类中的研究方向和在肺部疾病诊断和治疗中的潜力。 |
| [^27] | [A Gold Standard Dataset for the Reviewer Assignment Problem.](http://arxiv.org/abs/2303.16750) | 该论文提出了一个用于审稿人分配问题的新数据集，解决了当前算法难以进行原则比较的问题，并提供了基于此数据集的算法比较结果，为利益相关者在选择算法方面提供了一个基础。 |
| [^28] | [Improving Code Generation by Training with Natural Language Feedback.](http://arxiv.org/abs/2303.16749) | 该论文提出了一种新算法ILF，通过从自然语言反馈中进行学习来显著提高代码生成模型的性能，即使只有少量反馈，也可以获得很好的效果。 |
| [^29] | [Multi-View Clustering via Semi-non-negative Tensor Factorization.](http://arxiv.org/abs/2303.16748) | 该论文提出了基于半非负张量分解的多视图聚类方法，该方法采用单侧正交约束来考虑视图间的关系，并使用张量Schatten p-范数正则化来表征多视图数据的簇结构和利用视图间的互补信息。 |
| [^30] | [Maximum likelihood method revisited: Gauge symmetry in Kullback -- Leibler divergence and performance-guaranteed regularization.](http://arxiv.org/abs/2303.16721) | 本文提出了一种在最大似然方法中进行正则化的理论上保证的方法，通过关注 Kullback - Leibler 散度中的规范对称性，可以获得最优的模型。该方法不需要频繁搜索正则化的超参数。 |
| [^31] | [Topological Point Cloud Clustering.](http://arxiv.org/abs/2303.16716) | 本文提出一种新的基于拓扑的点聚类方法，该方法可以利用拓扑特征描述点云内的数据点，相较于传统图模型方法更具有健壮性和效率。 |
| [^32] | [TraVaG: Differentially Private Trace Variant Generation Using GANs.](http://arxiv.org/abs/2303.16704) | 本文提出一种基于GAN的新方法TraVaG，能够生成多样和具有代表性的新变体，同时提供差分隐私，并在出现高比例稀有变体时提供工业规模的好处和提高隐私保证水平。 |
| [^33] | [Probabilistic inverse optimal control with local linearization for non-linear partially observable systems.](http://arxiv.org/abs/2303.16698) | 本文介绍了一种针对非线性部分可观测系统的局部线性化概率逆优化控制方法，可用于特征化顺序决策任务中的行为，并且具有广泛的适用性。 |
| [^34] | [Communication Load Balancing via Efficient Inverse Reinforcement Learning.](http://arxiv.org/abs/2303.16686) | 本文使用逆强化学习方法成功解决了通信负载均衡问题，首次将逆强化学习应用于该领域。 |
| [^35] | [Policy Reuse for Communication Load Balancing in Unseen Traffic Scenarios.](http://arxiv.org/abs/2303.16685) | 提出了一种策略重用框架，该框架通过选择最适合执行的预训练强化学习策略来解决通信负载平衡问题，其根据不同流量场景下的策略训练库进行选择，可以胜过传统的负载平衡算法。 |
| [^36] | [Neuro-symbolic Rule Learning in Real-world Classification Tasks.](http://arxiv.org/abs/2303.16674) | 本研究通过将神经范式模块扩展到多类和多标签实际分类任务中，以及强制实现符号互斥属性，提高了神经符号规则学习模型的准确性和可解释性。 |
| [^37] | [A Byzantine-Resilient Aggregation Scheme for Federated Learning via Matrix Autoregression on Client Updates.](http://arxiv.org/abs/2303.16668) | 本文提出了FLANDERS，一种基于矩阵自回归的联邦学习聚合方案，可以识别恶意客户端，并提供了强大的拜占庭攻击防御。 |
| [^38] | [Learning Flow Functions from Data with Applications to Nonlinear Oscillators.](http://arxiv.org/abs/2303.16656) | 本文提出一种基于循环神经网络的架构，通过数据学习了一个因果、时不变且连续时间控制系统的流函数，并证明了该架构能够近似表示流函数，在实验中应用于Van der Pol和FitzHugh Nagumo振荡器的轨迹重现和最优控制输入计算。 |
| [^39] | [A Hierarchical Game-Theoretic Decision-Making for Cooperative Multi-Agent Systems Under the Presence of Adversarial Agents.](http://arxiv.org/abs/2303.16641) | 本文提出了一种名为GUT的新型分层式博弈论实用树模型，在多智能体决策中实现高低层次策略分解，使用代理需求作为新的收益度量，通过广泛的数值模拟表明可以帮助MAS以更低的成本和更高的获胜率实现具有挑战性的任务。 |
| [^40] | [Targeted Adversarial Attacks on Wind Power Forecasts.](http://arxiv.org/abs/2303.16633) | 本研究检测了两个深度学习预测模型的脆弱性，并提出了一种新的评估指标。这些模型可以被针对性攻击，因此保护风电功率预测结果对现代电力系统的稳定性至关重要。 |
| [^41] | [Fairlearn: Assessing and Improving Fairness of AI Systems.](http://arxiv.org/abs/2303.16626) | Fairlearn是一个有助于评估和提高人工智能系统公平性的开源项目，通过可视化地评估受影响人群的模型输出，并提供多种算法来缓解不公平问题，同时还在考虑系统的更广泛社会背景方面提供了学习资源。 |
| [^42] | [Personalised Language Modelling of Screen Characters Using Rich Metadata Annotations.](http://arxiv.org/abs/2303.16618) | 本篇论文研究了如何使用丰富的元数据注释的信息进行屏幕角色的个性化语言建模，测试表明这样可以有效地进行个性化语言模型的构建，即使对于零样本的演说家也可以应用。 |
| [^43] | [Bi-directional Training for Composed Image Retrieval via Text Prompt Learning.](http://arxiv.org/abs/2303.16604) | 本文提出了一种基于文本提示学习和双向训练的组成图像检索方法，可以应用于现有的体系结构，并且在修改文本存在噪声或歧义的情况下特别有效。 |
| [^44] | [Federated Learning in MIMO Satellite Broadcast System.](http://arxiv.org/abs/2303.16603) | 本文介绍将联邦学习应用于多输入多输出卫星广播系统，以解决现有联邦学习方法中存在的推断攻击和差分隐私精度降低的问题。 |
| [^45] | [Poster: Link between Bias, Node Sensitivity and Long-Tail Distribution in trained DNNs.](http://arxiv.org/abs/2303.16589) | DNN训练中长尾分布的数据集将给不同输出类别提供不同的分类性能，本文首次指出导致节点敏感性变化的节点偏差，提出了开放性挑战。 |
| [^46] | [Quantum Deep Hedging.](http://arxiv.org/abs/2303.16585) | 本论文针对对冲问题提出了基于量子神经网络的深度强化学习方法，相比于其他经典或量子的标准方法，其分布式方法可以获得更高的性能。 |
| [^47] | [PMAA: A Progressive Multi-scale Attention Autoencoder Model for High-Performance Cloud Removal from Multi-temporal Satellite Imagery.](http://arxiv.org/abs/2303.16565) | PMAA是一种利用全局和局部信息的高性能卫星遥感云去除架构。其中，独特的多尺度注意力模块和局部交互模块能够同时表示细粒度和粗粒度特征，相比现有方法表现更优。 |
| [^48] | [Plan4MC: Skill Reinforcement Learning and Planning for Open-World Minecraft Tasks.](http://arxiv.org/abs/2303.16563) | 本论文提出了一种基于技能强化学习和规划的 Minecraft 开放式任务解决方案，通过学习基本技能和技能规划的方法有效提高了任务解决效率，实现了24个不同的任务并在大多数任务中优于基线算法。 |
| [^49] | [Policy Gradient Methods for Discrete Time Linear Quadratic Regulator With Random Parameters.](http://arxiv.org/abs/2303.16548) | 本文使用增强学习技术中的策略梯度方法解决了带随机参数的离散时间线性二次调节器的最优控制问题，并建立了全局线性收敛保证。 |
| [^50] | [Futures Quantitative Investment with Heterogeneous Continual Graph Neural Network.](http://arxiv.org/abs/2303.16532) | 为了预测期货价格趋势，本文提出了一种基于异构任务设计和连续训练的时空图神经网络模型，可以捕捉长期和短期特征。 |
| [^51] | [Importance Sampling for Stochastic Gradient Descent in Deep Neural Networks.](http://arxiv.org/abs/2303.16529) | 本文回顾了深度学习中的重要性采样理论，并总结了利用重要性采样来训练深度神经网络所面临的挑战；本文提出了一种度量方法，用于评估给定的采样策略的质量，研究了采样方案和所使用的优化器之间的相互作用。 |
| [^52] | [Ensemble Learning Model on Artificial Neural Network-Backpropagation (ANN-BP) Architecture for Coal Pillar Stability Classification.](http://arxiv.org/abs/2303.16524) | 本文提出一种新颖的人工神经网络反向传播（ANN-BP）和深度集成学习的方法，用于煤柱稳定性的分类。通过使用不同的ANN-BP激活函数和新的标签替代方案，将柱子稳定性扩展到四个类别，成功预测了柱子的稳定性。 |
| [^53] | [Hard Regularization to Prevent Collapse in Online Deep Clustering without Data Augmentation.](http://arxiv.org/abs/2303.16521) | 该论文提出了一种不需要数据增强的在线深度聚类方法，通过加强正则化来避免崩溃，相比于其他方法，具有更高的稳定性。 |
| [^54] | [Fair Federated Medical Image Segmentation via Client Contribution Estimation.](http://arxiv.org/abs/2303.16520) | 本论文提出了一种新方法，在客户贡献评估下同时优化协作公平性和性能公平性，以获得高质量的全局模型。 |
| [^55] | [Infeasible Deterministic, Stochastic, and Variance-Reduction Algorithms for Optimization under Orthogonality Constraints.](http://arxiv.org/abs/2303.16510) | 本研究提出了一种简单迭代的Landing算法，可以在不强制执行正交约束的同时顺畅地吸引到正交约束流形上。我们扩展了这种算法以支持斯托菲尔（Stiefel）流形，并提供了随机和方差约减算法，这些方法与黎曼优化算法的收敛速度相同但需要更少的计算。 |
| [^56] | [Local Interpretability of Random Forests for Multi-Target Regression.](http://arxiv.org/abs/2303.16506) | 本文提出了一种针对随机森林模型在多目标回归中产生基于规则解释的技术，并在广泛实验评估中证明其具有竞争力。 |
| [^57] | [An Over-parameterized Exponential Regression.](http://arxiv.org/abs/2303.16504) | 该论文介绍了一种使用指数激活函数定义的神经函数来实现过参数化指数回归模型，并提出了一种新的注意力机制。 |
| [^58] | [Unified analysis of SGD-type methods.](http://arxiv.org/abs/2303.16502) | 本文从统一分析的角度探讨了SGD类型方法，在强凸光滑优化问题中具有重要作用。 |
| [^59] | [Lipschitzness Effect of a Loss Function on Generalization Performance of Deep Neural Networks Trained by Adam and AdamW Optimizers.](http://arxiv.org/abs/2303.16464) | 本文理论证明了损失函数的Lipschitz常数是降低Adam或AdamW获得输出模型的泛化误差的一个重要因素。本文的选择损失函数方针为Adam或AdamW优化算法的使用提供了指导。实验结果表明了Lipschitz常数较低且最大值较小的损失函数可以提高模型的泛化能力。 |
| [^60] | [GNNBuilder: An Automated Framework for Generic Graph Neural Network Accelerator Generation, Simulation, and Optimization.](http://arxiv.org/abs/2303.16459) | GNNBuilder是一个自动化的、通用的、端到端的GNN加速器生成框架，它可以为用户任意定义的广泛的GNN模型自动生成加速器，且运行速度比软件基准快多达12.95倍。 |
| [^61] | [When to Pre-Train Graph Neural Networks? An Answer from Data Generation Perspective!.](http://arxiv.org/abs/2303.16458) | 本文提出了一个通用框架W2PGNN，旨在回答何时预训练图神经网络的关键问题。使用新的角度探索了从预训练数据到下游数据的复杂生成机制。 |
| [^62] | [Conductivity Imaging from Internal Measurements with Mixed Least-Squares Deep Neural Networks.](http://arxiv.org/abs/2303.16454) | 本论文提出了一种新的通过深度神经网络从内部测量中重构椭圆问题中的导电率分布的方法，并在连续和经验损失的神经网络逼近上进行了深入的分析，展示了该方法对于数据噪声的优异稳定性以及解决高维问题的能力。 |
| [^63] | [ProtFIM: Fill-in-Middle Protein Sequence Design via Protein Language Models.](http://arxiv.org/abs/2303.16452) | 本论文介绍了一种基于蛋白质语言模型的填充中间蛋白序列设计方法，并设计了新的基准测试SEIFER，揭示了现有语言模型的弱点，证明了使用填充中间变换训练的语言模型(ProtFIM)，更适合于蛋白质工程。 |
| [^64] | [ProductAE: Toward Deep Learning Driven Error-Correction Codes of Large Dimensions.](http://arxiv.org/abs/2303.16424) | 本文提出了Product Autoencoder来通过可管理的训练复杂性实现相对较大的代码的训练。 |
| [^65] | [Problems and shortcuts in deep learning for screening mammography.](http://arxiv.org/abs/2303.16417) | 本研究发现在乳腺X光检查中，深度学习模型存在问题和快捷方式，建议利用采样方式平衡训练数据，并对相应的评估问题提出了解决方案 |
| [^66] | [A Comprehensive and Versatile Multimodal Deep Learning Approach for Predicting Diverse Properties of Advanced Materials.](http://arxiv.org/abs/2303.16412) | 本文提出了一种多模态深度学习方法，利用物理属性和化学数据预测先进材料的特性，在处理高维信息空间方面具有很大优势，成功预测了114210种组成条件下的913680个属性数据点，该方法适用于不同材料和尺度。 |
| [^67] | [LMDA-Net:A lightweight multi-dimensional attention network for general EEG-based brain-computer interface paradigms and interpretability.](http://arxiv.org/abs/2303.16407) | 本论文提出了一种名为LMDA-Net的轻量级多维注意力网络，旨在提高通用EEG脑机接口范例的分类性能。该网络使用两个新型注意力模块——通道注意力模块和深度注意力模块，并在多个公开高影响力数据集上进行了评估。实验结果表明，LMDA-Net表现优于其他代表模型，实现了最先进的性能，并展示了其决策过程的高可解释性。 |
| [^68] | [Hierarchical Video-Moment Retrieval and Step-Captioning.](http://arxiv.org/abs/2303.16406) | 这篇论文提出了HiREST数据集和一个新的基准，将分层信息检索和视觉/文本逐步总结从教学视频语料库中推广，使得在一个端到端的设置下可以共同搜索视频语料库，并生成摘要。 |
| [^69] | [Using Connected Vehicle Trajectory Data to Evaluate the Effects of Speeding.](http://arxiv.org/abs/2303.16396) | 本研究利用连接车辆轨迹数据探究了道路特征对个人超速行为的影响，建立模型预测超速比例。研究结果可为制定更有效的速度管理策略提供参考。 |
| [^70] | [Are Data-driven Explanations Robust against Out-of-distribution Data?.](http://arxiv.org/abs/2303.16390) | 随着黑盒模型在高风险应用中越来越被广泛应用，各种数据驱动的解释方法应运而生，但是，实证结果表明，即使模型预测正确，也可能在分布转移下产生不可靠的解释。为了解决这个问题，我们提出了一个模型无关的解释学习框架：分布稳健解释(DRE)，它可以在越界数据的情况下提供强健的解释信号，并提高模型的泛化能力。 |
| [^71] | [A Unified Single-stage Learning Model for Estimating Fiber Orientation Distribution Functions on Heterogeneous Multi-shell Diffusion-weighted MRI.](http://arxiv.org/abs/2303.16376) | 该论文提出了一种适用于异质多壳扩散加权MRI估计纤维定向分布函数的单阶段学习模型，使用深度学习技术可以提高推理速度和扫描一致性。 |
| [^72] | [Non-Asymptotic Lower Bounds For Training Data Reconstruction.](http://arxiv.org/abs/2303.16372) | 本文通过研究差分隐私和度量隐私学习器在对抗者重构错误方面的鲁棒性，得出了非渐进性下界，覆盖了高维情况，且扩展了深度学习算法的隐私分析 |
| [^73] | [Maximum likelihood smoothing estimation in state-space models: An incomplete-information based approach.](http://arxiv.org/abs/2303.16364) | 本文提出了一种新方法可以从随机状态空间系统的不完整信息/数据中进行极大似然（ML）平滑估计，所得到的估计结果比传统方法精度更高。算法基于EM梯度粒子算法，递归地进行估计。 |
| [^74] | [ChatGPT or academic scientist? Distinguishing authorship with over 99% accuracy using off-the-shelf machine learning tools.](http://arxiv.org/abs/2303.16352) | 该论文设计了一种用于区分ChatGPT生成文本和学术科学家撰写文本的方法，并在2.5k个样本上测试，实现了99.2%的准确率。 |
| [^75] | [Facial recognition technology can expose political orientation from facial images even when controlling for demographics and self-presentation.](http://arxiv.org/abs/2303.16343) | 本研究使用面部识别技术，通过特定的面部特征发现了人们的政治取向，甚至可以从自然图像中推广。这种预测的精度比人类评分者高，相当于一些工作面试的预测效果。 |
| [^76] | [On the Local Cache Update Rules in Streaming Federated Learning.](http://arxiv.org/abs/2303.16340) | 本文提出了三种本地缓存更新规则来管理动态数据分布和有限的缓存容量，以应对流式联邦学习中本地训练数据集与长期数据分布之间的差异。我们还推导出了该算法的收敛界限。我们在两个数据集上进行了测试，结果表明我们的算法效果良好。 |
| [^77] | [Operator learning with PCA-Net: upper and lower complexity bounds.](http://arxiv.org/abs/2303.16317) | 本文发展了PCA-Net的近似理论，得出了通用逼近结果，并识别出了使用PCA-Net进行高效操作学习的潜在障碍：输出分布的复杂性和算子空间的内在复杂性。 |
| [^78] | [Crime Prediction Using Machine Learning and Deep Learning: A Systematic Review and Future Directions.](http://arxiv.org/abs/2303.16310) | 该综述文章调查了超过150篇文章，探讨了各种机器学习和深度学习算法在预测犯罪方面的应用，为研究人员提供有价值的参考资料。 |
| [^79] | [Provable Robustness for Streaming Models with a Sliding Window.](http://arxiv.org/abs/2303.16308) | 本文关注于流数据上机器学习模型的可证明鲁棒性，为使用固定大小滑动窗口的模型提供了强稳健性证明。 |
| [^80] | [A Machine Learning Outlook: Post-processing of Global Medium-range Forecasts.](http://arxiv.org/abs/2303.16301) | 本研究探讨了使用非线性神经网络神经网络算法对全球中长期天气预报进行后处理的潜在好处和挑战，并展示了在特定领域上可提高高达12%的准确性。然而，需要加强对天气预报精度测量的研究，将有助于更好的理解这些模型，并进一步优化这些算法。 |
| [^81] | [Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels.](http://arxiv.org/abs/2303.16296) | 本文提出的Dice半度量损失函数可在软标签设置中使用，在医疗成像领域的分割方案中与使用软标签的研究相结合，可以获得更好的Dice分数和模型校准。 |
| [^82] | [XAIR: A Framework of Explainable AI in Augmented Reality.](http://arxiv.org/abs/2303.16292) | XAIR是一个解决AR中XAI输出解释的设计框架，可以为设计者提供指南和支持，实现有效的XAI设计。 |
| [^83] | [A Perspectival Mirror of the Elephant: Investigating Language Bias on Google, ChatGPT, Wikipedia, and YouTube.](http://arxiv.org/abs/2303.16281) | 研究发现在Google、ChatGPT、维基百科和YouTube上，搜索结果受限于语言，反映了与复杂主题相关的文化刻板印象，缺乏跨文化视角。 |
| [^84] | [Accelerated wind farm yaw and layout optimisation with multi-fidelity deep transfer learning wake models.](http://arxiv.org/abs/2303.16274) | 本文提出了一种名为 WakeNet 的机器学习框架，它利用多保真度转移学习将低保真度高斯涡模型微调生成中高保真度卷曲涡模型的精确涡流结果，以加速基于偏航和布局优化的涡流控制，取得了不错的效果。 |
| [^85] | [Communication-Efficient Vertical Federated Learning with Limited Overlapping Samples.](http://arxiv.org/abs/2303.16270) | 提出了一种名叫one-shot VFL的实用VFL框架，可以同时解决通信瓶颈和有限重叠样本的问题。few-shot VFL则可以在进行一次或仅少量通信的情况下进一步提高准确性。 |
| [^86] | [TimeBalance: Temporally-Invariant and Temporally-Distinctive Video Representations for Semi-Supervised Action Recognition.](http://arxiv.org/abs/2303.16268) | TimeBalance是一个半监督学习框架，通过利用时间不变和时间不同的自监督视频表示来学习准确的行为识别特征。实验结果表明该方法在半监督设置下超越了现有技术水平。 |
| [^87] | [Reinforcement learning for optimization of energy trading strategy.](http://arxiv.org/abs/2303.16266) | 本文使用强化学习算法优化了一种黑盒交易策略，该策略通过在马尔可夫决策过程中使用真实数据进行优化，在 DA 能源市场上由中型生产者自动进行交易。 |
| [^88] | [Zero-Shot Generalizable End-to-End Task-Oriented Dialog System using Context Summarization and Domain Schema.](http://arxiv.org/abs/2303.16252) | 本论文提出了一种使用领域架构和对话历史摘要实现泛化的零样本端到端任务导向对话系统，并在实验中证明其在标准领域转移和少样本学习中优于现有方法。 |
| [^89] | [Function Approximation with Randomly Initialized Neural Networks for Approximate Model Reference Adaptive Control.](http://arxiv.org/abs/2303.16251) | 本论文提出一种新的构造方法——平滑的积分表示，使得可以使用随机初始化的神经网络保证逼近精度，并且可以适用于更广泛的激活函数。 |
| [^90] | [ytopt: Autotuning Scientific Applications for Energy Efficiency at Large Scales.](http://arxiv.org/abs/2303.16245) | 本研究提出了ytopt，一种低延迟的自动调优框架，可用于大规模混合MPI / OpenMP科学应用，实现对性能和能源的自动调优，同时优化应用程序的能源消耗，实验结果表明，应用框架可实现高达34％的能源节省，且具有可伸缩性。 |
| [^91] | [Tetra-AML: Automatic Machine Learning via Tensor Networks.](http://arxiv.org/abs/2303.16214) | Tetra-AML 是一套基于张量网络的自动机器学习工具，可以自动完成神经网络结构搜索和超参数优化，还提供了模型压缩功能，通过量化、剪枝和张量网络来实现。在计算机视觉任务中，该工具箱的表现优于贝叶斯优化，同时可以将内存使用压缩至原来的 1/14.5，仅损失 3.2％ 的准确度。 |
| [^92] | [An EMO Joint Pruning with Multiple Sub-networks: Fast and Effect.](http://arxiv.org/abs/2303.16212) | 本文提出了一种基于多子网络的EMO联合剪枝算法，该算法可以减少空间和资源消耗。该算法采用分治的EMO网络剪枝框架，将整个网络上复杂的EMO剪枝任务分解为多个子网络上更简单的子任务。基于跨网络约束的子网络训练方法可以进一步提高性能。 |
| [^93] | [Combinatorial Convolutional Neural Networks for Words.](http://arxiv.org/abs/2303.16211) | 本文研究了深度学习模型在识别和利用数据项上的双射变换下保持不变的特征方面的局限性，并提出了一种组合卷积神经网络用于单词分类。 |
| [^94] | [Towards Quantifying Calibrated Uncertainty via Deep Ensembles in Multi-output Regression Task.](http://arxiv.org/abs/2303.16210) | 本研究探究了在多输出回归任务中应用深度集合量化校准不确定性的方法，提出了该方法的改进框架，其在回归准确性、不确定性估计可靠性和训练效率方面具有优越表现。 |
| [^95] | [AmorProt: Amino Acid Molecular Fingerprints Repurposing based Protein Fingerprint.](http://arxiv.org/abs/2303.16209) | 本文提出了一种基于氨基酸分子指纹重用的蛋白质指纹算法AmorProt，使用树形机器学习和人工神经网络模型进行淀粉样蛋白分组和等电点回归，并展示了该平台的适用性和优势。 |
| [^96] | [Lifting uniform learners via distributional decomposition.](http://arxiv.org/abs/2303.16208) | 本文介绍了一种方法，可以将任何在均匀分布下有效的PAC学习算法转换成一个在任意未知分布下有效的算法，而且对于单调分布，只需要用$\mathcal{D}$中的样本。算法的核心是通过一个算法将$\mathcal{D}$逼近成由子立方体混合而成的混合均匀分布。 |
| [^97] | [Your Diffusion Model is Secretly a Zero-Shot Classifier.](http://arxiv.org/abs/2303.16203) | 扩散模型的密度估计可以被用作零样本分类，作者的生成式分类方法在各种基准测试中取得强大的结果，并具有更强的多模式关系推理能力。 |
| [^98] | [Invariant preservation in machine learned PDE solvers via error correction.](http://arxiv.org/abs/2303.16110) | 本文研究通过保持基本偏微分方程的离散模拟中的连续不变量来设计更可靠的机器学习PDE求解器，关键在于通过纠错保持不变量。 |
| [^99] | [Large-scale Pre-trained Models are Surprisingly Strong in Incremental Novel Class Discovery.](http://arxiv.org/abs/2303.15975) | 本论文提出了一种更加挑战性和实用性的学习方法MSc-iNCD，通过在连续而无人监督的学习中利用大规模预训练模型的丰富先验知识，该方法在增量式新类别发现中表现出出乎意料的强大实力。 |
| [^100] | [Foundation Models and Fair Use.](http://arxiv.org/abs/2303.15715) | 基础模型开发和部署需确保在合理使用的范围内，但这并不保证。在使用有版权内容时需要注意风险并可能需要进一步的工作。 |
| [^101] | [TextMI: Textualize Multimodal Information for Integrating Non-verbal Cues in Pre-trained Language Models.](http://arxiv.org/abs/2303.15430) | 本文提出了一种将声学和视觉信息转化为文本描述并与口语文本串联，从而将非语言信息纳入预训练语言模型中的方法。该方法可以不需要昂贵的多模态数据收集和建模，并且可以使非语言线索与语言无缝集成，对于多模态行为理解任务具有实际应用意义。 |
| [^102] | [Inflation forecasting with attention based transformer neural networks.](http://arxiv.org/abs/2303.15364) | 本文研究了基于注意力机制变压器神经网络用于预测不同通货膨胀率的潜力，结果表明其可以超越传统模型，成为金融决策中的有用工具。 |
| [^103] | [Generalizable Denoising of Microscopy Images using Generative Adversarial Networks and Contrastive Learning.](http://arxiv.org/abs/2303.15214) | 本文提出一种结合生成对抗网络和对比学习的方法，可以使用较少的数据量有效地去噪显微镜图像，减轻数据获取负担，实现少样本学习。 |
| [^104] | [Diffusion Denoised Smoothing for Certified and Adversarial Robust Out-Of-Distribution Detection.](http://arxiv.org/abs/2303.14961) | 本研究提出了一个新的方法来证明$\ell_2$-norm下的样本外检测的鲁棒性，在不考虑网络架构和具体组件的情况下，改善了对抗攻击的检测技术。 |
| [^105] | [Efficient Scale-Invariant Generator with Column-Row Entangled Pixel Synthesis.](http://arxiv.org/abs/2303.14157) | 提出了一种高效尺度不变的生成模型，不使用空间卷积或分层架构，通过将层内特征图分解为局部和全局特征并通过插值这些特征生成图像，使得模型具有较小的内存占用和更快的推理速度。 |
| [^106] | [Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration.](http://arxiv.org/abs/2303.11435) | InDI是一种新的监督式图像修复公式，通过逐步改进图像质量来生成比现有回归方法更真实和详细的图像，修复效果更具有感知质量。 |
| [^107] | [Class-Guided Image-to-Image Diffusion: Cell Painting from Brightfield Images with Class Labels.](http://arxiv.org/abs/2303.08863) | 该论文介绍了一种新的算法，将图像至图像和类别引导去噪扩散概率模型组合，用于有元数据标签的图像重建问题。实验结果表明，类别引导的图像至图像扩散可以提升图像重建的内容，优于未引导的模型。 |
| [^108] | [Training, Architecture, and Prior for Deterministic Uncertainty Methods.](http://arxiv.org/abs/2303.05796) | 本文研究了确定性不确定性方法中的重要设计选择，包括解耦核心架构和不确定性头方案的训练、核心架构表达能力、避免特征崩溃的额外架构约束和先验影响等。研究发现，训练方案和核心架构表达能力对不确定性性能至关重要，而避免特征崩溃的额外架构约束可能会破坏OOD泛化和检测之间的折衷，DUMs定义的先验则对最终性能影响较小。 |
| [^109] | [PreFallKD: Pre-Impact Fall Detection via CNN-ViT Knowledge Distillation.](http://arxiv.org/abs/2303.03634) | 本文提出了一种基于CNN-ViT知识蒸馏的预防跌倒系统PreFallKD，通过将预训练的教师模型的检测知识转移到轻量级卷积神经网络的学生模型上，实现了检测性能和计算复杂性的平衡。 |
| [^110] | [Coupled Multiwavelet Neural Operator Learning for Coupled Partial Differential Equations.](http://arxiv.org/abs/2303.02304) | 本论文提出一种耦合多小波神经算子学习的方案，解决了处理耦合多变量映射问题的难点，能够显著提高解决耦合偏微分方程的准确性，并在实验中得到了验证。 |
| [^111] | [GBMST: An Efficient Minimum Spanning Tree Clustering Based on Granular-Ball Computing.](http://arxiv.org/abs/2303.01082) | 该论文提出了一种基于多粒度颗粒球和最小生成树的聚类算法，通过实现基于“大规模优先级”的聚类方法，可以大大避免异常值的影响，并加速了MST的构建过程。 |
| [^112] | [FuNVol: A Multi-Asset Implied Volatility Market Simulator using Functional Principal Components and Neural SDEs.](http://arxiv.org/abs/2303.00859) | FuNVol是一个多资产隐含波动率市场模拟器，使用函数主成分和神经SDE生成真实历史价格的IV表面序列，并在无静态套利的表面次流形内产生一致的市场情景。同时，使用模拟表面进行对冲可以生成与实现P＆L一致的损益分布。 |
| [^113] | [Mixtures of All Trees.](http://arxiv.org/abs/2302.14202) | 提出了一种新的生成模型类别——所有树的混合模型——它在处理$n$个变量的情况时能实现最先进的性能，尽管其边际估计的精确计算是NP难问题。 |
| [^114] | [On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective.](http://arxiv.org/abs/2302.12095) | 本研究评估了ChatGPT的鲁棒性，发现其在对抗性和超出分布任务上有一致的优势，但绝对表现仍有提高空间，鲁棒性仍是一个重要的挑战。 |
| [^115] | [Selective experience replay compression using coresets for lifelong deep reinforcement learning in medical imaging.](http://arxiv.org/abs/2302.11510) | 本文提出了一种使用核心集进行选择性经验回放压缩的技术，可以提升终身学习的效率，应用于医学影像领域。 |
| [^116] | [A Complete Expressiveness Hierarchy for Subgraph GNNs via Subgraph Weisfeiler-Lehman Tests.](http://arxiv.org/abs/2302.07090) | 本文提出了一个通过子图Weisfeiler-Lehman Tests（SWL）的视角对一般基于节点的子图GNN进行系统研究的方法。其中，我们建立了一个严格增长的SWL完整表达力层次结构，证明了任何基于节点的子图GNN都属于六个SWL等价类之一，其中SSWL实现了最大的表达能力。 |
| [^117] | [Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels.](http://arxiv.org/abs/2302.05666) | 本文提出了Jaccard度量损失（JMLs）来优化Jaccard指数，该损失在软标签下仍然有效。 |
| [^118] | [Machine Learning for Synthetic Data Generation: A Review.](http://arxiv.org/abs/2302.04062) | 机器学习用于合成数据生成的综述，探讨了合成数据生成的应用（计算机视觉、语音、自然语言、医疗保健和商业）、机器学习方法（神经网络架构和深度生成模型）以及隐私和公平问题，并提出了未来的研究方向。 |
| [^119] | [Parameterizing the cost function of Dynamic Time Warping with application to time series classification.](http://arxiv.org/abs/2301.10350) | 本研究参数化了动态时间规整的代价函数并在时间序列分类任务中得到应用，相比原始的绝对值代价函数，研究还提出了可调整参数的代价函数，通过调整参数可以更好地适应不同的任务，且在多个标准时间序列分类数据集上得到验证。 |
| [^120] | [Improved Kernel Alignment Regret Bound for Online Kernel Learning.](http://arxiv.org/abs/2212.12989) | 本文提出了一种新的算法，在线核学习中的新算法遗憾界和计算复杂度优于以前的结果。该算法的遗憾界和计算复杂度取决于核矩阵特征值的衰减率。 |
| [^121] | [HAC-Net: A Hybrid Attention-Based Convolutional Neural Network for Highly Accurate Protein-Ligand Binding Affinity Prediction.](http://arxiv.org/abs/2212.12440) | 本文提出了一种新的深度学习架构HAC-Net，结合了三维卷积神经网络和两个图卷积网络的注意力机制，能够高精度地预测蛋白质与配体的结合亲和力，并在公认的基准测试集上获得最先进的结果。 |
| [^122] | [Editing Models with Task Arithmetic.](http://arxiv.org/abs/2212.04089) | 本文提出了一种使用任务向量进行模型编辑的新范式，任务向量可通过算术操作进行修改和组合，可以提高目标任务性能且对控制任务影响较小。 |
| [^123] | [Link Prediction with Non-Contrastive Learning.](http://arxiv.org/abs/2211.14394) | 本文介绍了一种新的非对比学习方法，用于链路预测任务，该方法可以在提高节点表示性能的同时减少负样本采样的挑战。 |
| [^124] | [Motif-aware temporal GCN for fraud detection in signed cryptocurrency trust networks.](http://arxiv.org/abs/2211.13123) | 本文提出了一种基于图卷积网络和平衡理论的加密货币信任网络欺诈检测方法，并使用模式矩阵捕捉局部拓扑信息。实验结果表明，该方法在真实数据和合成数据集上均优于现有方法。 |
| [^125] | [Single-Pass Contrastive Learning Can Work for Both Homophilic and Heterophilic Graph.](http://arxiv.org/abs/2211.10890) | 该论文介绍了一种单次对比学习方法，可适用于同构和异构图，并给出了性能保证。 |
| [^126] | [Environmental Sensor Placement with Convolutional Gaussian Neural Processes.](http://arxiv.org/abs/2211.10381) | 本论文提出了一种新的方式——卷积高斯神经过程（ConvGNP），用于提高环境传感器的放置效率。ConvGNP使用神经网络来参数化联合高斯分布，通过学习空间和季节性非平稳性，优于传统的非平稳高斯过程模型。 |
| [^127] | [Deep Reinforcement Learning Based Joint Downlink Beamforming and RIS Configuration in RIS-aided MU-MISO Systems Under Hardware Impairments and Imperfect CSI.](http://arxiv.org/abs/2211.09702) | 本文提出了一种DRL方法，联合优化了MU-MISO系统中的波束成形和RIS相位移位，以最大化总下行速率。在解决了不完美的CSI和硬件失调的挑战后，该方法显著优于基础DRL代理，并且是第一篇应用DRL优化波束成形和RIS配置的工作。 |
| [^128] | [Emergent Linguistic Structures in Neural Networks are Fragile.](http://arxiv.org/abs/2210.17406) | 本文提出了一个框架来评估语言模型对于语法的表述的一致性和稳健性，通过多项实验证据表明，神经网络中 emergent 语言结构是脆弱的。 |
| [^129] | [A Primal-dual Approach for Solving Variational Inequalities with General-form Constraints.](http://arxiv.org/abs/2210.15659) | 本文介绍了一种解决具有一般形式约束的变分不等式的原始-对偶方法，并使用一种温启动技术，在每次迭代中近似地解决子问题，我们证明了它的收敛性，并展示了它的收敛速度比其精确对应物更快。 |
| [^130] | [Multi-Viewpoint and Multi-Evaluation with Felicitous Inductive Bias Boost Machine Abstract Reasoning Ability.](http://arxiv.org/abs/2210.14914) | 本文研究了机器在抽象推理方面的能力，证明了具有适宜的归纳偏差的神经网络可以优雅地解决RPM问题，并揭示了多视角和多评估是成功推理的关键学习策略。 |
| [^131] | [Multi-lingual Evaluation of Code Generation Models.](http://arxiv.org/abs/2210.14868) | 本研究提出了多种用于评估代码生成模型的新基准测试，能够以多语言方式评估模型性能，并探讨了多语言模型优于单语言模型、少量提示能教授模型新语言以及在单语言设置下拥有零-shot翻译能力等问题。 |
| [^132] | [Interpolating Discriminant Functions in High-Dimensional Gaussian Latent Mixtures.](http://arxiv.org/abs/2210.14347) | 本文提出一种在高维特征下进行二元分类的方法，使用广义最小二乘估计器估计最佳分离超平面的方向，惊人地发现超平面的插值取决于标签的编码方式。 |
| [^133] | [Instance-Aware Image Completion.](http://arxiv.org/abs/2210.12350) | 本文提出了一个实例感知图像修复模型ImComplete，相比现有方法，它可以幻象出与环境背景相协调的视觉实例，提供了基于语义和结构的像素级指导。 |
| [^134] | [Improving Transfer Learning with a Dual Image and Video Transformer for Multi-label Movie Trailer Genre Classification.](http://arxiv.org/abs/2210.07983) | 本文研究了ImageNet和Kinetics预先训练的模型在多标签电影预告片类型分类中的可迁移性，通过Dual Image and Video Transformer Architecture提高了可迁移性。 |
| [^135] | [Fast Lifelong Adaptive Inverse Reinforcement Learning from Demonstrations.](http://arxiv.org/abs/2209.11908) | 本文提出了一种快速生涯适应性逆强化学习框架，从学习的策略中构建多样策略的组合实现了对新的演示的快速适应，同时整合演示中的共性知识，实现准确的任务推断，还能够在大规模部署中通过维护一个精简的原型策略集合并通过策略组合来逼近所有行为。 |
| [^136] | [Quadratic Gradient: Combining Gradient Algorithms and Newton's Method as One.](http://arxiv.org/abs/2209.03282) | 本文提出了一种基于对角矩阵的二次梯度，可以加速梯度的收敛速度，在实验中表现良好。研究者还推测海森矩阵与学习率之间可能存在关系。 |
| [^137] | [What Does the Gradient Tell When Attacking the Graph Structure.](http://arxiv.org/abs/2208.12815) | 本文研究了图神经网络中针对图形结构的对抗攻击，发现攻击者更倾向于增加类间边缘，通过连接不同类的节点来更有效地破坏节点特征。然而，GNN 的固有平滑性会导致在前向过程中丢失关键信息。为此，我们提出了一个具有多级传播的新型代理模型来解决这个问题。 |
| [^138] | [Multinomial Logistic Regression Algorithms via Quadratic Gradient.](http://arxiv.org/abs/2208.06828) | 本文扩展了二次梯度的方法到多类逻辑回归，提出了一个增强的自适应梯度算法，实验结果表明，这两种增强方法能够分别比它们的原始方法更快地收敛。 |
| [^139] | [Finite-time High-probability Bounds for Polyak-Ruppert Averaged Iterates of Linear Stochastic Approximation.](http://arxiv.org/abs/2207.04475) | 本文提供了关于带有固定步长的线性随机逼近（LSA）算法的有限时间分析，得到了LSA及其Polyak-Ruppert平均化版本定义的迭代的$p$阶矩和高概率偏差界的推导。本文得到的有限时间实例相关的平均LSA迭代的界限是尖锐的，并与局部渐近极小极值限制相同，同时剩余项对混合时间有着紧密的依赖关系。 |
| [^140] | [Cooperative Retriever and Ranker in Deep Recommenders.](http://arxiv.org/abs/2206.14649) | 本文介绍了深度推荐系统中的检索和排名两阶段工作流程。传统方法中，这两个组件都是独立训练或使用简单的级联管道，效果不佳。最近一些工作提出联合训练检索器和排名器，但仍存在许多限制。因此，还需要探索更有效的协作方法。 |
| [^141] | [NovelCraft: A Dataset for Novelty Detection and Discovery in Open Worlds.](http://arxiv.org/abs/2206.11736) | NovelCraft数据集提供了开放世界中新颖性检测与发现任务的挑战。在复杂的场景中插入新颖物体的检测需要更好的基准，并发现了控制假阳性时更简单的方法可能比复杂的方法更出色。 |
| [^142] | [Beyond RMSE: Do machine-learned models of road user interaction produce human-like behavior?.](http://arxiv.org/abs/2206.11110) | 本文研究了机器学习模型在道路用户交互预测方面的行为，并发现尽管这些模型在RMSE方面表现良好，但它们的行为可能与人类不同，需要更细致的分析。 |
| [^143] | [Compressed-VFL: Communication-Efficient Learning with Vertically Partitioned Data.](http://arxiv.org/abs/2206.08330) | 提出了一种用于纵向分割数据的通信高效学习方法，通过周期性共享压缩的中间结果，实现了超过90%的通信量减少，且不影响模型准确性。 |
| [^144] | [Look, Radiate, and Learn: Self-Supervised Localisation via Radio-Visual Correspondence.](http://arxiv.org/abs/2206.06424) | 本文提出了一个合成的无线电-视觉数据集和基准(MaxRay)，通过从无线电-视觉对应中提取自坐标来自主学习目标在无线电中的定位，最终实现了无需标签的准确无线电目标定位。 |
| [^145] | [Squeeze All: Novel Estimator and Self-Normalized Bound for Linear Contextual Bandits.](http://arxiv.org/abs/2206.05404) | 我们提出了一种具有新颖估计量和自标准化界限的线性上下文Bandit算法，具有$O(\sqrt{dT\log T})$遗憾界，维度相关的加法项分解遗憾累积，性能优于现有方法。 |
| [^146] | [Global Convergence of Over-parameterized Deep Equilibrium Models.](http://arxiv.org/abs/2205.13814) | 本研究探讨了深度均衡模型的训练动态，提出唯一平衡点始终存在且梯度下降的收敛速率为线性收敛到全局最优解，可通过轻微过参数化得到满足。 |
| [^147] | [From {Solution Synthesis} to {Student Attempt Synthesis} for Block-Based Visual Programming Tasks.](http://arxiv.org/abs/2205.01265) | 研究自动推断学生误解的关键组成部分，引入了一个基准测试——StudentSyn，通过观察学生在一个任务上的尝试，合成他们对另一个任务的学习尝试，以提高人工智能驱动的编程导师的效果。 |
| [^148] | [IOP-FL: Inside-Outside Personalization for Federated Medical Image Segmentation.](http://arxiv.org/abs/2204.08467) | 该论文提出了IOP-FL，一种联邦学习的内外个性化方法，以增强医学成像任务中单个客户的预测准确性。 |
| [^149] | [Rethinking Reconstruction Autoencoder-Based Out-of-Distribution Detection.](http://arxiv.org/abs/2203.02194) | 本文重新考虑了基于重构自编码器方法的外样本检测，在最大压缩自编码器的潜空间和保证重构能力的基础上，通过引入语义重构、数据确定性分解和标准化L2距离等策略，本文提出的方法在各个基准测试中都取得了最先进的性能表现，且不需要额外的标记外样本数据。 |
| [^150] | [Privacy-Preserving Logistic Regression Training with A Faster Gradient Variant.](http://arxiv.org/abs/2201.10838) | 本文提出了一种更快的梯度变种——二次梯度，用于在同态加密领域实现隐私保护的逻辑回归训练，并成功提升了收敛速度，实现了同态逻辑回归训练仅需较少的迭代次数。 |
| [^151] | [The Prominence of Artificial Intelligence in COVID-19.](http://arxiv.org/abs/2111.09537) | 本论文研究人工智能在 COVID-19 中的应用，探讨了提出的方法，可帮助早期和廉价地诊断该病，有助于医生和研究人员对抗该病。 |
| [^152] | [Compute and Energy Consumption Trends in Deep Learning Inference.](http://arxiv.org/abs/2109.05472) | 本篇论文研究了深度学习推理中的计算和能量消耗趋势，重点关注推理成本而非训练成本。结果显示，除了算法创新外，更具体和强大的硬件通常伴随着重要的能量效率优化，导致推理成本的提高呈现柔和的趋势。 |
| [^153] | [Systematic human learning and generalization from a brief tutorial with explanatory feedback.](http://arxiv.org/abs/2107.06994) | 本文研究了人类如何通过简要教程和解释性反馈从少量的训练示例中学习抽象推理任务，并能成功地将其推广到训练范围之外的情况。结果表明，实现人类学习机制对于人工智能的发展非常重要。 |
| [^154] | [Learning Identity-Preserving Transformations on Data Manifolds.](http://arxiv.org/abs/2106.12096) | 本论文提出了一种新方法，可以直接从数据流形中学习保持身份的变换，而无需在训练期间标记变换数据。该方法基于变分自编码器，可以学习一组本地自适应的保持身份的变换，可以提高模型在具有挑战性的图像分类和少样本学习任务上的推广性能。 |
| [^155] | [Imbalanced Gradients: A Subtle Cause of Overestimated Adversarial Robustness.](http://arxiv.org/abs/2006.13726) | 本文研究了对敌对鲁棒性估计过高的微妙原因，发现不平衡梯度也会导致鲁棒性估计过高，我们提出了一种边界分解攻击，并证明11个防御模型在某种程度上容易受到不平衡梯度的影响，MD攻击可以降低它们的鲁棒性。 |

# 详细

[^1]: InceptionNeXt：当Inception遇到ConvNeXt

    InceptionNeXt: When Inception Meets ConvNeXt. (arXiv:2303.16900v1 [cs.CV])

    [http://arxiv.org/abs/2303.16900](http://arxiv.org/abs/2303.16900)

    本论文提出了一种名为InceptionNeXt的新型神经网络，通过将大内核卷积沿通道维度分解为四个平行分支来提高模型效率，解决了保持性能的同时加快基于大内核的CNN模型的问题。

    

    受ViTs长程建模能力的启发，近期广泛研究和采用了大内核卷积来扩大感受野和提高模型性能，例如ConvNeXt采用了7x7深度卷积。虽然这种深度操作仅消耗少量FLOPs，但由于高内存访问成本，这在功能强大的计算设备上大大损害了模型效率。尽管缩小ConvNeXt的内核大小能提高速度，但会导致性能显着下降。如何在保持性能的同时加快基于大内核的CNN模型仍不清楚。为了解决这个问题，受Inceptions的启发，我们提出将大内核深度卷积沿通道维度分解为四个平行分支，即小方内核、两个正交带内核和一个互补内核。

    Inspired by the long-range modeling ability of ViTs, large-kernel convolutions are widely studied and adopted recently to enlarge the receptive field and improve model performance, like the remarkable work ConvNeXt which employs 7x7 depthwise convolution. Although such depthwise operator only consumes a few FLOPs, it largely harms the model efficiency on powerful computing devices due to the high memory access costs. For example, ConvNeXt-T has similar FLOPs with ResNet-50 but only achieves 60% throughputs when trained on A100 GPUs with full precision. Although reducing the kernel size of ConvNeXt can improve speed, it results in significant performance degradation. It is still unclear how to speed up large-kernel-based CNN models while preserving their performance. To tackle this issue, inspired by Inceptions, we propose to decompose large-kernel depthwise convolution into four parallel branches along channel dimension, i.e. small square kernel, two orthogonal band kernels, and an ide
    
[^2]: 物理驱动的扩散模型用于从视频中合成冲击声

    Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos. (arXiv:2303.16897v1 [cs.CV])

    [http://arxiv.org/abs/2303.16897](http://arxiv.org/abs/2303.16897)

    该论文提出了一种物理驱动扩散模型，可以为silent视频剪辑合成高保真的冲击声，并使用额外的物理先验知识来指导冲击声合成过程。

    

    对物体相互作用发出的声音进行建模对于实际世界和虚拟世界中的沉浸式感官体验至关重要。传统的冲击声合成方法使用物理模拟来获得一组能够表示和合成声音的物理参数。然而，它们需要物体的细节和冲击位置，这在真实世界中很少可用，并且无法应用于从普通视频中合成冲击声。另一方面，现有的视频驱动深度学习方法只能捕捉到视觉内容和冲击声之间的弱对应关系，因为它们缺乏物理知识。在这项工作中，我们提出了一种物理驱动的扩散模型，可以为静态视频剪辑合成高保真的冲击声。除了视频内容外，我们还提出使用额外的物理先验知识来指导冲击声合成过程，这些先验包括既可控制物理参数，同时也能保证音效质量的噪声扰动。

    Modeling sounds emitted from physical object interactions is critical for immersive perceptual experiences in real and virtual worlds. Traditional methods of impact sound synthesis use physics simulation to obtain a set of physics parameters that could represent and synthesize the sound. However, they require fine details of both the object geometries and impact locations, which are rarely available in the real world and can not be applied to synthesize impact sounds from common videos. On the other hand, existing video-driven deep learning-based approaches could only capture the weak correspondence between visual content and impact sounds since they lack of physics knowledge. In this work, we propose a physics-driven diffusion model that can synthesize high-fidelity impact sound for a silent video clip. In addition to the video content, we propose to use additional physics priors to guide the impact sound synthesis procedure. The physics priors include both physics parameters that are
    
[^3]: 探究预训练标签粒度的影响

    Towards Understanding the Effect of Pretraining Label Granularity. (arXiv:2303.16887v1 [cs.CV])

    [http://arxiv.org/abs/2303.16887](http://arxiv.org/abs/2303.16887)

    本文探究了预训练标签粒度对深度神经网络图像分类任务的泛化能力的影响，并在iNaturalist 2021与ImageNet数据集中进行了实验证明，在预训练数据集具有强有力的标签层次结构，标签功能与目标任务对齐，以及选择适当的预训练标签粒度时，能有效提高模型的性能。

    

    本文研究了预训练标签粒度如何影响深度神经网络在图像分类任务中的泛化能力。我们关注“细到粗”的迁移学习设置，其中预训练标签比目标问题更细粒度。我们使用iNaturalist 2021的标签层次结构进行了该方法的实验，并观察到相对于基线错误率有8.76％的相对改进。我们发现以下条件对于改进非常关键：1）预训练数据集具有强大且有意义的标签层次结构，2）其标签功能与目标任务的功能强烈对齐，最重要的是，3）选择了适当级别的预训练标签粒度。我们在ImageNet上的迁移学习实验进一步证明了预训练标签粒度的重要性。值得注意的是，我们展示了在ImageNet21k上的叶标签预训练产生了比其他合作标签更好的ImageNet1k迁移结果。

    In this paper, we study how pretraining label granularity affects the generalization of deep neural networks in image classification tasks. We focus on the "fine-to-coarse" transfer learning setting where the pretraining label is more fine-grained than that of the target problem. We experiment with this method using the label hierarchy of iNaturalist 2021, and observe a 8.76% relative improvement of the error rate over the baseline. We find the following conditions are key for the improvement: 1) the pretraining dataset has a strong and meaningful label hierarchy, 2) its label function strongly aligns with that of the target task, and most importantly, 3) an appropriate level of pretraining label granularity is chosen. The importance of pretraining label granularity is further corroborated by our transfer learning experiments on ImageNet. Most notably, we show that pretraining at the leaf labels of ImageNet21k produces better transfer results on ImageNet1k than pretraining at other coa
    
[^4]: 隐藏流形Hopfield模型及其学习相变

    The Hidden-Manifold Hopfield Model and a learning phase transition. (arXiv:2303.16880v1 [cond-mat.dis-nn])

    [http://arxiv.org/abs/2303.16880](http://arxiv.org/abs/2303.16880)

    提出一种称为隐藏流形Hopfield模型的广义Hopfield模型，重点研究了在复杂数据上的应用，发现其中存在着一种学习相变的现象。

    

    Hopfield模型在统计物理学中有着悠久的传统，是少数具有理论基础的神经网络之一。通过将Hopfield模型的理论拓展到相关数据上，可以帮助我们理解深度神经网络的成功，例如描述它们如何从数据中提取特征。出于这个动机，我们提出并研究了一种广义的Hopfield模型，称为隐藏流形Hopfield模型：我们使用称为因子的$D=\alpha_D N$个随机向量的非线性变换，使用来自$P=\alpha N$个示例的Hebb规则生成耦合，其中$N$是神经元的数量。使用重复方法，我们获得了该模型的相图，显示出一个相变，其中在示例中隐藏的因子成为动态学的吸引子；这种相存在于关键的$\alpha$值以上和$\alpha_D$关键的值以下。我们将这种行为称为学习相变。

    The Hopfield model has a long-standing tradition in statistical physics, being one of the few neural networks for which a theory is available. Extending the theory of Hopfield models for correlated data could help understand the success of deep neural networks, for instance describing how they extract features from data. Motivated by this, we propose and investigate a generalized Hopfield model that we name Hidden-Manifold Hopfield Model: we generate the couplings from $P=\alpha N$ examples with the Hebb rule using a non-linear transformation of $D=\alpha_D N$ random vectors that we call factors, with $N$ the number of neurons. Using the replica method, we obtain a phase diagram for the model that shows a phase transition where the factors hidden in the examples become attractors of the dynamics; this phase exists above a critical value of $\alpha$ and below a critical value of $\alpha_D$. We call this behaviour learning transition.
    
[^5]: 概率建模与自动化机器学习框架在高维应力场中的应用

    Application of probabilistic modeling and automated machine learning framework for high-dimensional stress field. (arXiv:2303.16869v1 [cs.CE])

    [http://arxiv.org/abs/2303.16869](http://arxiv.org/abs/2303.16869)

    本文探讨了数据驱动的代理建模在高维物理问题中的应用，提出了一种基于自动化机器学习框架的概率建模方法，以降低计算成本和提高预测准确性和精度。

    

    现代计算方法采用高度复杂的数学公式，使得建模复杂的物理现象、预测关键性能和设计优化成为可能。然而，这些计算模型的高保真度使得查询成本极高，通常会采用简化模型以换取预测精度和精确度的代价。为解决这一问题，数据驱动的代理建模方法在仿真昂贵的计算模型行为方面表现出了很大的优势。然而，这种方法的一个主要瓶颈是无法处理高维度的输入和输出量，需要相对较大的数据集。对于此类问题，常用的代理建模方法会要求大量的计算评估，使得其他数值处理变得困难。

    Modern computational methods, involving highly sophisticated mathematical formulations, enable several tasks like modeling complex physical phenomenon, predicting key properties and design optimization. The higher fidelity in these computer models makes it computationally intensive to query them hundreds of times for optimization and one usually relies on a simplified model albeit at the cost of losing predictive accuracy and precision. Towards this, data-driven surrogate modeling methods have shown a lot of promise in emulating the behavior of the expensive computer models. However, a major bottleneck in such methods is the inability to deal with high input dimensionality and the need for relatively large datasets. With such problems, the input and output quantity of interest are tensors of high dimensionality. Commonly used surrogate modeling methods for such problems, suffer from requirements like high number of computational evaluations that precludes one from performing other nume
    
[^6]: ALUM: 从潜在模型的不确定性补偿对抗数据不确定性建模

    ALUM: Adversarial Data Uncertainty Modeling from Latent Model Uncertainty Compensation. (arXiv:2303.16866v1 [cs.LG])

    [http://arxiv.org/abs/2303.16866](http://arxiv.org/abs/2303.16866)

    提出了一种新方法ALUM，可以同时处理模型不确定性和数据不确定性。通过探索开采的对抗三元组来促进数据不确定性建模和非参数不确定性估计，以弥补潜在教练层的不足。该方法是模型不可知的，可以很容易地整合到任何现有的深度模型中，以提高模型的稳健性。

    

    模型不确定度的预测对于建立可信的AI至关重要。由于数据噪声引起的模型不确定性，使深度模型出现不确定的预测结果，引起重要的担忧。为了探索和处理由于固有数据噪声引起的不确定性，我们提出了一种称为ALUM的新方法，可以同时处理模型不确定性和数据不确定性。我们不是仅在深度模型的最终层中根据随机选择的训练数据建模数据不确定性，而是提出探索开采的对抗三元组以促进数据不确定性建模和非参数不确定性估计来补偿训练不足的潜伏模型层。因此，由于噪声数据引起的关键数据不确定性和模型不确定性可以被轻松地量化以提高模型的鲁棒性。我们提出的ALUM是模型不可知的，可以很容易地用很少的额外计算实现到任何现有的深度模型中。

    It is critical that the models pay attention not only to accuracy but also to the certainty of prediction. Uncertain predictions of deep models caused by noisy data raise significant concerns in trustworthy AI areas. To explore and handle uncertainty due to intrinsic data noise, we propose a novel method called ALUM to simultaneously handle the model uncertainty and data uncertainty in a unified scheme. Rather than solely modeling data uncertainty in the ultimate layer of a deep model based on randomly selected training data, we propose to explore mined adversarial triplets to facilitate data uncertainty modeling and non-parametric uncertainty estimations to compensate for the insufficiently trained latent model layers. Thus, the critical data uncertainty and model uncertainty caused by noisy data can be readily quantified for improving model robustness. Our proposed ALUM is model-agnostic which can be easily implemented into any existing deep model with little extra computation overhe
    
[^7]: 超越经验风险最小化：局部结构保持正则化提高对抗鲁棒性

    Beyond Empirical Risk Minimization: Local Structure Preserving Regularization for Improving Adversarial Robustness. (arXiv:2303.16861v1 [cs.LG])

    [http://arxiv.org/abs/2303.16861](http://arxiv.org/abs/2303.16861)

    提出一种新颖的局部结构保持正则化方法，旨在在学习的嵌入空间中保持输入空间的局部结构，与基线相比，该方法显著提高了对抗鲁棒性的性能。

    

    深度神经网络容易被对人类不可感知的扰动所欺骗。许多防御措施已被提出以提高对抗鲁棒性，其中对抗训练方法最为有效。然而，大多数这些方法独立地处理训练样本并要求大量的样本来训练鲁棒性网络，同时忽略这些样本之间的潜在结构信息。在本文中，我们提出了一种新颖的局部结构保持（LSP）正则化方法，旨在在学习的嵌入空间中保持输入空间的局部结构。这样，可缓解干净样本附近存在的对抗样本的攻击效果。我们展示了强有力的实证证据，在几个图像分类数据集中，我们的方法不论是否进行对抗训练，均相对于基线显著提高了对抗鲁棒性的性能。

    It is broadly known that deep neural networks are susceptible to being fooled by adversarial examples with perturbations imperceptible by humans. Various defenses have been proposed to improve adversarial robustness, among which adversarial training methods are most effective. However, most of these methods treat the training samples independently and demand a tremendous amount of samples to train a robust network, while ignoring the latent structural information among these samples. In this work, we propose a novel Local Structure Preserving (LSP) regularization, which aims to preserve the local structure of the input space in the learned embedding space. In this manner, the attacking effect of adversarial samples lying in the vicinity of clean samples can be alleviated. We show strong empirical evidence that with or without adversarial training, our method consistently improves the performance of adversarial robustness on several image classification datasets compared to the baseline
    
[^8]: 物理深度强化学习保障安全

    Physical Deep Reinforcement Learning Towards Safety Guarantee. (arXiv:2303.16860v1 [cs.LG])

    [http://arxiv.org/abs/2303.16860](http://arxiv.org/abs/2303.16860)

    Phy-DRL是一种物理深度强化学习框架，采用类李雅普诺夫奖赏和残差控制的架构设计，具备可证明的安全性和稳定性保障，实验结果表明其能够提升训练速度和奖励，具有良好的鲁棒性。

    

    深度强化学习（DRL）在处理高维状态和/或行动空间的自主系统的复杂决策任务方面取得了巨大成功。然而，安全和稳定性仍然是阻碍DRL用于安全关键自主系统的主要问题。为解决这些问题，我们提出了Phy-DRL：一种物理深度强化学习框架。该框架涵盖两个新颖的架构设计：i）类李雅普诺夫奖赏，ii）残差控制（即物理模型控制和数据驱动控制的集成）。同时将物理奖励和残差控制集成于Phy-DRL中，使其具备可证明的安全性和稳定性保障。通过对倒立摆的实验，我们展示了Phy-DRL的保障安全和稳定以及增强的鲁棒性，同时具有明显的训练加速和增加奖励的效果。

    Deep reinforcement learning (DRL) has achieved tremendous success in many complex decision-making tasks of autonomous systems with high-dimensional state and/or action spaces. However, the safety and stability still remain major concerns that hinder the applications of DRL to safety-critical autonomous systems. To address the concerns, we proposed the Phy-DRL: a physical deep reinforcement learning framework. The Phy-DRL is novel in two architectural designs: i) Lyapunov-like reward, and ii) residual control (i.e., integration of physics-model-based control and data-driven control). The concurrent physical reward and residual control empower the Phy-DRL the (mathematically) provable safety and stability guarantees. Through experiments on the inverted pendulum, we show that the Phy-DRL features guaranteed safety and stability and enhanced robustness, while offering remarkably accelerated training and enlarged reward.
    
[^9]: 扩散Schr\"odinger桥匹配

    Diffusion Schr\"odinger Bridge Matching. (arXiv:2303.16852v1 [stat.ML])

    [http://arxiv.org/abs/2303.16852](http://arxiv.org/abs/2303.16852)

    本文介绍了一种新的方法 Iterative Markovian Fitting，用于解决高维度 Schr\"odinger桥（SBs）问题，该方法的数值实验表现出在准确性和性能方面的显著优势。

    

    解决运输问题，在机器学习中有着许多应用，例如新型的质量传输方法，如去噪扩散模型（DDMs）和流匹配模型（FMMs），通过随机微分方程（SDE）或常微分方程（ODE）实现这样的传输。然而，虽然在许多应用中，近似确定性动态最优传输（OT）映射是可取的，因为具有吸引人的性质，但 DDMs 和 FMMs 并不能保证提供接近 OT 映射的传输。相反，Schr\"odinger桥（SBs）计算随机动态映射，可以恢复正则熵版本的 OT。不幸的是，现有的数值方法近似 SBs 的维度缩放差或在迭代中积累误差。在这项工作中，我们介绍了迭代马尔科夫拟合，一种解决高维度 SB 问题的新方法。我们将这个方法设计为一个迭代过程，将置信传播扩展到 KL 散度，利用条件独立性降低计算复杂度，并确保一致性和收敛性质。我们的数值实验证明了相对于现有成果方法，在准确性和性能方面都有显著优势。

    Solving transport problems, i.e. finding a map transporting one given distribution to another, has numerous applications in machine learning. Novel mass transport methods motivated by generative modeling have recently been proposed, e.g. Denoising Diffusion Models (DDMs) and Flow Matching Models (FMMs) implement such a transport through a Stochastic Differential Equation (SDE) or an Ordinary Differential Equation (ODE). However, while it is desirable in many applications to approximate the deterministic dynamic Optimal Transport (OT) map which admits attractive properties, DDMs and FMMs are not guaranteed to provide transports close to the OT map. In contrast, Schr\"odinger bridges (SBs) compute stochastic dynamic mappings which recover entropy-regularized versions of OT. Unfortunately, existing numerical methods approximating SBs either scale poorly with dimension or accumulate errors across iterations. In this work, we introduce Iterative Markovian Fitting, a new methodology for solv
    
[^10]: 随机投影的凸聚类模型：动机，实现和簇恢复保证

    Randomly Projected Convex Clustering Model: Motivation, Realization, and Cluster Recovery Guarantees. (arXiv:2303.16841v1 [cs.LG])

    [http://arxiv.org/abs/2303.16841](http://arxiv.org/abs/2303.16841)

    本文提出了一种随机投影凸聚类模型，具有簇恢复保证和良好的鲁棒性，在聚类精度和可伸缩性方面优于许多最先进的聚类算法。

    

    本文提出了一种随机投影凸聚类模型，用于对$\mathbb{R}^d$中的$n$个高维数据点进行聚类，并假设这些点存在$K$个隐藏的簇。与用于聚类原始数据的凸聚类模型相比($d$维)，我们证明，在某些温和的条件下，如果存在凸聚类模型的完美拆分，那么通过嵌入维度$m=O(\epsilon^{-2}\log(n))$的随机投影凸聚类模型可以保留拆分结果，其中$0 < \epsilon < 1$是一些给定的参数。我们进一步证明，嵌入维度可以改进为$O(\epsilon^{-2}\log(K))$，不受数据点数量的影响。本文还通过广泛的数字实验结果展示了随机投影凸聚类模型的鲁棒性和卓越性能。本文中呈现的数字实验结果还表明，随机投影凸聚类模型在聚类精度和可伸缩性方面优于许多最先进的聚类算法。

    In this paper, we propose a randomly projected convex clustering model for clustering a collection of $n$ high dimensional data points in $\mathbb{R}^d$ with $K$ hidden clusters. Compared to the convex clustering model for clustering original data with dimension $d$, we prove that, under some mild conditions, the perfect recovery of the cluster membership assignments of the convex clustering model, if exists, can be preserved by the randomly projected convex clustering model with embedding dimension $m = O(\epsilon^{-2}\log(n))$, where $0 < \epsilon < 1$ is some given parameter. We further prove that the embedding dimension can be improved to be $O(\epsilon^{-2}\log(K))$, which is independent of the number of data points. Extensive numerical experiment results will be presented in this paper to demonstrate the robustness and superior performance of the randomly projected convex clustering model. The numerical results presented in this paper also demonstrate that the randomly projected 
    
[^11]: MaMMUT: 一种用于多模态任务联合学习的简单架构

    MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks. (arXiv:2303.16839v1 [cs.CV])

    [http://arxiv.org/abs/2303.16839](http://arxiv.org/abs/2303.16839)

    提出了一种名为MaMMUT的简单模型，可以通过两步方法容纳对比和生成学习，并在联合训练不同的视觉语言任务时表现出很高的效力。

    

    语言模型的发展已从编码-解码转向仅解码的设计。此外，普遍认为，最流行的两种多模态任务，生成任务和对比任务，往往互相冲突，难以在一个架构中容纳，并进一步需要用于下游任务的复杂调整。我们提出了一种新的培训范式，用于多模态任务的仅解码模型，这在联合学习这些不同的视觉语言任务方面非常有效。这是通过一个简单的模型MaMMUT实现的。它由单一的视觉编码器和一个文本解码器组成，并能够通过文本解码器上的新的两步方法容纳对比和生成学习。我们证明这些不同目标任务的联合训练是简单的，有效的，并最大化了模型的权重共享。此外，相同的架构使得对开放词汇对象检测的简单扩展成为可能。

    The development of language models have moved from encoder-decoder to decoder-only designs. In addition, the common knowledge has it that the two most popular multimodal tasks, the generative and contrastive tasks, tend to conflict with one another, are hard to accommodate in one architecture, and further need complex adaptations for downstream tasks. We propose a novel paradigm of training with a decoder-only model for multimodal tasks, which is surprisingly effective in jointly learning of these disparate vision-language tasks. This is done with a simple model, called MaMMUT. It consists of a single vision encoder and a text decoder, and is able to accommodate contrastive and generative learning by a novel two-pass approach on the text decoder. We demonstrate that joint training of these diverse-objective tasks is simple, effective, and maximizes the weight-sharing of the model. Furthermore, the same architecture enables straightforward extensions to open-vocabulary object detection 
    
[^12]: 零样本 Entrailment 用于 Empirical AI Research 领域排行榜

    Zero-shot Entailment of Leaderboards for Empirical AI Research. (arXiv:2303.16835v1 [cs.CL])

    [http://arxiv.org/abs/2303.16835](http://arxiv.org/abs/2303.16835)

    本文探讨了自动挖掘 Empirical AI Research 领域排行榜这一任务类别中的零样本学习现象。实验测试了先前报告的最新技术模型在其未见过的排行榜标签上的泛化能力或 entailment 能力。本文创建了一个零样本标记的数据集。

    

    本文在一个特定的文本蕴含（RTE）任务类别中进行了零样本学习现象的大规模实证研究，即自动挖掘 Empirical AI Research 领域排行榜。该领域的排行榜提取先前报告的最新技术模型，在非零样本设置下表现良好，报告了高于90%的性能。然而，一个核心的研究问题仍未被检验：这些模型真的学习了 entailment 吗？因此，在本文的实验中，测试了两个先前报告的最新技术模型，在其训练过程中没有见过的排行榜标签上，测试它们的泛化能力或 entailment 能力。我们假设，如果模型学习了 entailment，它们的零样本性能也可能是中等的，或者具体来说，好于随机猜测。本文通过远程标注创建了零样本标记数据集。

    We present a large-scale empirical investigation of the zero-shot learning phenomena in a specific recognizing textual entailment (RTE) task category, i.e. the automated mining of leaderboards for Empirical AI Research. The prior reported state-of-the-art models for leaderboards extraction formulated as an RTE task, in a non-zero-shot setting, are promising with above 90% reported performances. However, a central research question remains unexamined: did the models actually learn entailment? Thus, for the experiments in this paper, two prior reported state-of-the-art models are tested out-of-the-box for their ability to generalize or their capacity for entailment, given leaderboard labels that were unseen during training. We hypothesize that if the models learned entailment, their zero-shot performances can be expected to be moderately high as well--perhaps, concretely, better than chance. As a result of this work, a zero-shot labeled dataset is created via distant labeling formulating
    
[^13]: 基于学习的预测的交互合流情况下自动驾驶决策制定

    Decision Making for Autonomous Driving in Interactive Merge Scenarios via Learning-based Prediction. (arXiv:2303.16821v1 [cs.RO])

    [http://arxiv.org/abs/2303.16821](http://arxiv.org/abs/2303.16821)

    本研究提出了在自动驾驶车辆与其他车辆交互合流的情况下，基于学习的预测的决策制定方法来解决高度挑战的决策问题和不确定性，并采用部分可观察的马尔可夫决策过程（POMDP）和蒙特卡罗树搜索算法来执行高级驾驶操作。

    

    在与人类驾驶员共享道路的自动代理方面，必须考虑交通参与者之间微妙的互动。这是一个非常具有挑战性的决策问题，因为人类行为受到难以建模的多种因素（例如人类意图和情绪）的影响。本文提出了一种自动驾驶的决策方法，重点关注复杂的合流交通任务，其中不确定性来自其他驾驶员的行为和不完美的传感器测量。我们将问题框架化为部分可观察的马尔可夫决策过程（POMDP），并使用蒙特卡罗树搜索在线求解。 POMDP的解决方案是执行高级驾驶操作的策略，例如让道给逼近的车辆，与前面的车辆保持安全距离或合并到交通中。我们的方法利用从数据中学习的模型来预测未来的交通状态，同时明确考虑交互作用。

    Autonomous agents that drive on roads shared with human drivers must reason about the nuanced interactions among traffic participants. This poses a highly challenging decision making problem since human behavior is influenced by a multitude of factors (e.g., human intentions and emotions) that are hard to model. This paper presents a decision making approach for autonomous driving, focusing on the complex task of merging into moving traffic where uncertainty emanates from the behavior of other drivers and imperfect sensor measurements. We frame the problem as a partially observable Markov decision process (POMDP) and solve it online with Monte Carlo tree search. The solution to the POMDP is a policy that performs high-level driving maneuvers, such as giving way to an approaching car, keeping a safe distance from the vehicle in front or merging into traffic. Our method leverages a model learned from data to predict the future states of traffic while explicitly accounting for interaction
    
[^14]: 从经验损失中学习LTI-ss系统的PAC-Bayesian界限

    PAC-Bayesian bounds for learning LTI-ss systems with input from empirical loss. (arXiv:2303.16816v1 [stat.ML])

    [http://arxiv.org/abs/2303.16816](http://arxiv.org/abs/2303.16816)

    本文推导出应用于有限数据点学习模型的PAC-Bayesian误差界限，使我们能够为广泛的学习/系统辨识算法提供有限样本误差边界。

    

    本文针对带输入的线性时不变系统（LTI）随机动力系统，推导出一种概率近似正确（PAC）-Bayesian误差界限。该界限广泛用于机器学习中，用于表征从有限数据点学习的模型的预测能力。特别地，本文导出的界限将未来的平均预测误差与模型在学习数据上生成的预测误差联系起来。因此，这使我们能够为广泛的学习/系统辨识算法提供有限样本误差界限。此外，由于LTI系统是递归神经网络（RNN）的一个子类，因此这些误差界限可能是PAC-Bayesian界限适用于RNN的第一步。

    In this paper we derive a Probably Approxilmately Correct(PAC)-Bayesian error bound for linear time-invariant (LTI) stochastic dynamical systems with inputs. Such bounds are widespread in machine learning, and they are useful for characterizing the predictive power of models learned from finitely many data points. In particular, with the bound derived in this paper relates future average prediction errors with the prediction error generated by the model on the data used for learning. In turn, this allows us to provide finite-sample error bounds for a wide class of learning/system identification algorithms. Furthermore, as LTI systems are a sub-class of recurrent neural networks (RNNs), these error bounds could be a first step towards PAC-Bayesian bounds for RNNs.
    
[^15]: 浅层复值神经网络对$C^k$-函数的最优逼近

    Optimal approximation of $C^k$-functions using shallow complex-valued neural networks. (arXiv:2303.16813v1 [math.FA])

    [http://arxiv.org/abs/2303.16813](http://arxiv.org/abs/2303.16813)

    本文证明了对于$C^k$（在实变量意义下）的函数，使用具有单层隐藏层和$m$个神经元的神经网络可以以错误率$m^{-k/(2n)}$将其逼近。此外，如果选取权值$\sigma_j,b_j\in\mathbb{C}$和$\rho_j\in\mathbb{C}^n$对$f$连续，那么获得的逼近速率是最优的。

    

    本文证明了使用浅层复值神经网络对复立方体上$C^k$（在实变量意义下）的函数进行逼近的量化结果。具体而言，我们考虑具有单层隐藏层和$m$个神经元的神经网络，即形如$z \mapsto \sum_{j=1}^m \sigma_j \cdot \phi\big(\rho_j^T z + b_j\big)$的网络，并且证明了可以使用这种形式的函数逼近$C^k \left(\Omega_n;\mathbb{C}\right)$中的任何函数，当$m\to\infty$时误差为$m^{-k/(2n)}$.此外，我们还证明选取权值$\sigma_j,b_j\in\mathbb{C}$和$\rho_j\in\mathbb{C}^n$对$f$连续并且在这种连续性假设下获得的逼近速率是最优的。

    We prove a quantitative result for the approximation of functions of regularity $C^k$ (in the sense of real variables) defined on the complex cube $\Omega_n := [-1,1]^n +i[-1,1]^n\subseteq \mathbb{C}^n$ using shallow complex-valued neural networks. Precisely, we consider neural networks with a single hidden layer and $m$ neurons, i.e., networks of the form $z \mapsto \sum_{j=1}^m \sigma_j \cdot \phi\big(\rho_j^T z + b_j\big)$ and show that one can approximate every function in $C^k \left( \Omega_n; \mathbb{C}\right)$ using a function of that form with error of the order $m^{-k/(2n)}$ as $m \to \infty$, provided that the activation function $\phi: \mathbb{C} \to \mathbb{C}$ is smooth but not polyharmonic on some non-empty open set. Furthermore, we show that the selection of the weights $\sigma_j, b_j \in \mathbb{C}$ and $\rho_j \in \mathbb{C}^n$ is continuous with respect to $f$ and prove that the derived rate of approximation is optimal under this continuity assumption. We also discuss
    
[^16]: 基于模块化正则化可改善观察含噪数据的高斯图模型

    Module-based regularization improves Gaussian graphical models when observing noisy data. (arXiv:2303.16796v1 [physics.data-an])

    [http://arxiv.org/abs/2303.16796](http://arxiv.org/abs/2303.16796)

    建议将推断网络的模块化结构整合到正则化强度的交叉验证中，以改善高斯图模型在观察含噪数据时的表现。

    

    研究人员经常使用高斯图模型表示多变量相关数据中的关系，这些模型需要正则化来稀疏模型。我们建议在正则化强度的交叉验证中，将推断网络的模块化结构整合起来以平衡欠拟合和过拟合。使用合成和真实数据，我们发现与使用高斯对数似然进行交叉验证的标准方法（图形套索法）相比，这种方法可以更好地恢复和推断含噪声数据中的模块化结构。

    Researchers often represent relations in multi-variate correlational data using Gaussian graphical models, which require regularization to sparsify the models. Acknowledging that they often study the modular structure of the inferred network, we suggest integrating it in the cross-validation of the regularization strength to balance under- and overfitting. Using synthetic and real data, we show that this approach allows us to better recover and infer modular structure in noisy data compared with the graphical lasso, a standard approach using the Gaussian log-likelihood when cross-validating the regularization strength.
    
[^17]: GRAF：图形注意融合网络

    GRAF: Graph Attention-aware Fusion Networks. (arXiv:2303.16781v1 [cs.LG])

    [http://arxiv.org/abs/2303.16781](http://arxiv.org/abs/2303.16781)

    本文提出了一个名为GRAF的方法，使用注意机制和网络融合在多个网络上使用基于GNN的方法。通过attention-based neighborhood aggregation，GRAF能够学习每个节点和关联的重要性，并根据它们在网络融合中进行边缘加权。

    

    许多现实世界的网络包含多种类型的节点和边。图神经网络(GNN)作为一种利用图结构数据上的节点特征的深度学习框架已经证明了卓越的性能。然而，流行的基于GNN的架构只能处理一个同构网络。使它们能够处理多个网络带来了额外的挑战，因为网络的异构性和现有关联的多样性。在本研究中，我们提出了一个名为GRAF的计算方法，利用GNN的方法在多个网络上使用注意机制和网络融合。使用基于注意力的邻域聚合，GRAF学习每个节点的邻居节点的重要性（称为节点级注意力）以及分层方式下的关联的重要性（称为关联级注意力）。然后，GRAF处理一个网络融合步骤，根据学习到的节点和关联级注意力加权每条边。

    A large number of real-world networks include multiple types of nodes and edges. Graph Neural Network (GNN) emerged as a deep learning framework to utilize node features on graph-structured data showing superior performance. However, popular GNN-based architectures operate on one homogeneous network. Enabling them to work on multiple networks brings additional challenges due to the heterogeneity of the networks and the multiplicity of the existing associations. In this study, we present a computational approach named GRAF utilizing GNN-based approaches on multiple networks with the help of attention mechanisms and network fusion. Using attention-based neighborhood aggregation, GRAF learns the importance of each neighbor per node (called node-level attention) followed by the importance of association (called association-level attention) in a hierarchical way. Then, GRAF processes a network fusion step weighing each edge according to learned node- and association-level attention, which r
    
[^18]: 基于媒体偏好训练的语言模型可以预测公众舆论

    Language Models Trained on Media Diets Can Predict Public Opinion. (arXiv:2303.16779v1 [cs.CL])

    [http://arxiv.org/abs/2303.16779](http://arxiv.org/abs/2303.16779)

    该论文介绍了一种新的方法来探索媒体偏好模型——适用于在线新闻、电视广播或广播节目内容的语言模型，可以预测公众舆论并具有实际应用价值。

    

    公众舆论反映和塑造社会行为，但传统的基于调查的工具存在局限性。我们介绍一种新的方法来探索媒体偏好模型——适用于在线新闻、电视广播或广播节目内容的语言模型，可以模拟已消费一组媒体的亚群体的意见。为了验证这种方法，我们使用美国全国代表性调查中针对COVID-19和消费者信心的观点表达作为基本事实。我们的研究表明，这种方法（1）可以预测调查响应分布中的人类判断，并且对措辞和媒体曝光渠道具有鲁棒性，（2）在建模更密切关注媒体的人方面更准确，（3）符合关于哪些类型的观点受媒体消费影响的文献。探测语言模型提供了一种强大的新方法来研究媒体效应，具有在补充调查和预测公众舆论方面的实际应用，也涉及解决与媒体偏见有关的问题。

    Public opinion reflects and shapes societal behavior, but the traditional survey-based tools to measure it are limited. We introduce a novel approach to probe media diet models -- language models adapted to online news, TV broadcast, or radio show content -- that can emulate the opinions of subpopulations that have consumed a set of media. To validate this method, we use as ground truth the opinions expressed in U.S. nationally representative surveys on COVID-19 and consumer confidence. Our studies indicate that this approach is (1) predictive of human judgements found in survey response distributions and robust to phrasing and channels of media exposure, (2) more accurate at modeling people who follow media more closely, and (3) aligned with literature on which types of opinions are affected by media consumption. Probing language models provides a powerful new method for investigating media effects, has practical applications in supplementing polls and forecasting public opinion, and 
    
[^19]: 基于主动学习策略的两 百万份标记美食食谱数据集 - 3A2M

    Assorted, Archetypal and Annotated Two Million (3A2M) Cooking Recipes Dataset based on Active Learning. (arXiv:2303.16778v1 [cs.CL])

    [http://arxiv.org/abs/2303.16778](http://arxiv.org/abs/2303.16778)

    本研究利用领域专家的知识和主动学习技术呈现了一个两百万份烹饪食谱新数据集，通过将30万份食谱按照命名实体识别进行分类到9个类别中，然后使用混合方法对其余的1900K份食谱进行分类。

    

    烹饪食谱可以交换烹饪思想，并提供食品的制作说明。然而，在该领域内由于缺乏足够的标记数据，将在线找到的原始食谱分类到合适的食品类型是一项具有挑战性的任务。本研究利用领域专家的知识将食谱分类可能是一种解决方案。我们基于一个主动学习技术呈现了一个两百万份烹饪食谱的新数据集，通过利用领域专家的知识将其标记在各自的类别中。为了构建数据集，我们从RecipeNLG数据集中获取食谱。然后，我们使用三个可信度得分高于86.667％的人类专家按照其命名实体识别（NER）将30万份食谱分类到九个类别之一：烘焙、饮料、荤菜、蔬菜、快餐、麦片、餐点、配菜和融合。最后，我们使用Query-by-Committee和Human的混合方法，将剩余的1900K份食谱进行分类。

    Cooking recipes allow individuals to exchange culinary ideas and provide food preparation instructions. Due to a lack of adequate labeled data, categorizing raw recipes found online to the appropriate food genres is a challenging task in this domain. Utilizing the knowledge of domain experts to categorize recipes could be a solution. In this study, we present a novel dataset of two million culinary recipes labeled in respective categories leveraging the knowledge of food experts and an active learning technique. To construct the dataset, we collect the recipes from the RecipeNLG dataset. Then, we employ three human experts whose trustworthiness score is higher than 86.667% to categorize 300K recipe by their Named Entity Recognition (NER) and assign it to one of the nine categories: bakery, drinks, non-veg, vegetables, fast food, cereals, meals, sides and fusion. Finally, we categorize the remaining 1900K recipes using Active Learning method with a blend of Query-by-Committee and Human 
    
[^20]: 不冷静，不冷静，也不镇定：使用情感语言检测COVID-19的错误信息。

    Not cool, calm or collected: Using emotional language to detect COVID-19 misinformation. (arXiv:2303.16777v1 [cs.CL])

    [http://arxiv.org/abs/2303.16777](http://arxiv.org/abs/2303.16777)

    本论文提出一种使用情感语言检测COVID-19错误信息的模型，该模型相比于单一的错误信息分类器具有更好的效果，但该研究的主要限制因素是低质量标签和不匹配的标签分布。

    

    社交媒体平台如Twitter上的COVID-19错误信息对有效的疫情管理构成威胁。先前在推特上的COVID-19错误信息的工作否认了推特上普遍存在的诸如带电情感的语义特征的作用。因此，我们提出了一种新的COVID-19错误信息模型，该模型使用推特情感编码器和COVID-19错误信息编码器来预测推文是否包含COVID-19错误信息。我们的情感编码器在一组新的注释数据集上进行了微调，我们的COVID-19错误信息编码器在COVID-HeRA数据集的子集上进行了微调。实验结果表明，使用情感和错误信息编码器的组合比单独的错误信息分类器产生了更好的结果。此外，进行了广泛的结果分析，强调了低质量标签和不匹配的标签分布是我们研究的主要限制因素。

    COVID-19 misinformation on social media platforms such as twitter is a threat to effective pandemic management. Prior works on tweet COVID-19 misinformation negates the role of semantic features common to twitter such as charged emotions. Thus, we present a novel COVID-19 misinformation model, which uses both a tweet emotion encoder and COVID-19 misinformation encoder to predict whether a tweet contains COVID-19 misinformation. Our emotion encoder was fine-tuned on a novel annotated dataset and our COVID-19 misinformation encoder was fine-tuned on a subset of the COVID-HeRA dataset. Experimental results show superior results using the combination of emotion and misinformation encoders as opposed to a misinformation classifier alone. Furthermore, extensive result analysis was conducted, highlighting low quality labels and mismatched label distributions as key limitations to our study.
    
[^21]: 乒乓球比赛预测的监督学习

    Supervised Learning for Table Tennis Match Prediction. (arXiv:2303.16776v1 [cs.LG])

    [http://arxiv.org/abs/2303.16776](http://arxiv.org/abs/2303.16776)

    本研究研究了使用机器学习预测乒乓球单打比赛结果，使用球员和比赛统计数据作为特征。通过消融研究，我们发现5折交叉验证和超参数调整是提高模型性能的关键。我们证明了不同的特征聚合策略可以提高模型的鲁棒性。最终，我们的模型与最新的类似研究相匹配。

    

    机器学习、分类和预测模型在各种领域都有应用。运动分析是其中一个越来越受欢迎的应用领域，但大多数现有工作都集中在主流运动的自动裁判和伤害预防上。对于其他运动，例如乒乓球，研究才刚刚开始受到更多关注。本文提出使用机器学习来预测乒乓球单打比赛的结果。我们使用球员和比赛统计数据作为特征，并通过消融研究评估它们的相对重要性。在模型方面，我们探索了许多流行的模型。我们发现，5折交叉验证和超参数调整对于提高模型性能至关重要。我们在消融研究中调查了不同的特征聚合策略，以证明模型的鲁棒性。不同模型的表现相当，结果的准确率（61-70%）与类似运动中的最新模型相匹配，例如...

    Machine learning, classification and prediction models have applications across a range of fields. Sport analytics is an increasingly popular application, but most existing work is focused on automated refereeing in mainstream sports and injury prevention. Research on other sports, such as table tennis, has only recently started gaining more traction. This paper proposes the use of machine learning to predict the outcome of table tennis single matches. We use player and match statistics as features and evaluate their relative importance in an ablation study. In terms of models, a number of popular models were explored. We found that 5-fold cross-validation and hyperparameter tuning was crucial to improve model performance. We investigated different feature aggregation strategies in our ablation study to demonstrate the robustness of the models. Different models performed comparably, with the accuracy of the results (61-70%) matching state-of-the-art models in comparable sports, such as
    
[^22]: 用非临床文本信息检索实现肿瘤相关论坛帖子的计算有效标记

    Computationally Efficient Labeling of Cancer Related Forum Posts by Non-Clinical Text Information Retrieval. (arXiv:2303.16766v1 [cs.IR])

    [http://arxiv.org/abs/2303.16766](http://arxiv.org/abs/2303.16766)

    本研究基于非临床和免费可用的信息，结合分布式计算、文本检索、聚类和分类方法开发了一个能够检索、聚类和展示关于癌症病程信息的计算有效系统。

    

    在线上存在着大量关于癌症的信息，但分类和提取有用信息很困难。几乎所有的医疗保健数据处理研究都涉及正式的临床数据，但非临床数据中也有有价值的信息。本研究将分布式计算、文本检索、聚类和分类方法结合成一个连贯、计算有效的系统，基于非临床和免费可用的信息，可以澄清癌症患者的病程。我们开发了一个完全功能的原型，可以从非临床论坛帖子中检索、聚类和展示关于癌症病程的信息。我们评估了三种聚类算法（MR-DBSCAN、DBSCAN和HDBSCAN），并比较了它们在调整后的兰德指数和总运行时间方面的表现，作为检索的帖子数量和邻域半径函数。聚类结果显示，邻域半径对聚类结果有最显著的影响。

    An abundance of information about cancer exists online, but categorizing and extracting useful information from it is difficult. Almost all research within healthcare data processing is concerned with formal clinical data, but there is valuable information in non-clinical data too. The present study combines methods within distributed computing, text retrieval, clustering, and classification into a coherent and computationally efficient system, that can clarify cancer patient trajectories based on non-clinical and freely available information. We produce a fully-functional prototype that can retrieve, cluster and present information about cancer trajectories from non-clinical forum posts. We evaluate three clustering algorithms (MR-DBSCAN, DBSCAN, and HDBSCAN) and compare them in terms of Adjusted Rand Index and total run time as a function of the number of posts retrieved and the neighborhood radius. Clustering results show that neighborhood radius has the most significant impact on c
    
[^23]: 使用蚁群优化的新型词性标注方法ACO-tagger

    ACO-tagger: A Novel Method for Part-of-Speech Tagging using Ant Colony Optimization. (arXiv:2303.16760v1 [cs.CL])

    [http://arxiv.org/abs/2303.16760](http://arxiv.org/abs/2303.16760)

    本研究提出了一种基于蚁群优化的高效词性标注方法ACO-tagger，实现了高达96.867％的准确率，优于几种最先进的方法。

    

    近年来，群智能算法因其解决复杂和非确定性问题的能力而备受关注。这些算法受自然生物的集体行为启发，模拟这种行为以开发用于计算任务的智能 agent。其中一种算法是受到蚂蚁觅食行为及其信息素释放机制启发的蚁群优化（ACO）算法，用于解决离散和组合性的困难问题。词性标注是自然语言处理中的基础任务，旨在为句子中的每个单词分配一个词性角色。本研究提出了一种基于ACO的高性能词性标注方法ACO-tagger。该方法实现了高达96.867％的准确率，优于几种最先进的方法。该方法快速高效，是实际应用的可行选择。

    Swarm Intelligence algorithms have gained significant attention in recent years as a means of solving complex and non-deterministic problems. These algorithms are inspired by the collective behavior of natural creatures, and they simulate this behavior to develop intelligent agents for computational tasks. One such algorithm is Ant Colony Optimization (ACO), which is inspired by the foraging behavior of ants and their pheromone laying mechanism. ACO is used for solving difficult problems that are discrete and combinatorial in nature. Part-of-Speech (POS) tagging is a fundamental task in natural language processing that aims to assign a part-of-speech role to each word in a sentence. In this research paper, proposed a high-performance POS-tagging method based on ACO called ACO-tagger. This method achieved a high accuracy rate of 96.867%, outperforming several state-of-the-art methods. The proposed method is fast and efficient, making it a viable option for practical applications.
    
[^24]: 探究名人对公众态度影响的研究：基于社交媒体情感分析的 COVID-19 研究

    Exploring celebrity influence on public attitude towards the COVID-19 pandemic: social media shared sentiment analysis. (arXiv:2303.16759v1 [cs.CL])

    [http://arxiv.org/abs/2303.16759](http://arxiv.org/abs/2303.16759)

    本文研究了公众人物在社交媒体上共享的信息对 COVID-19 疫情中的公众情感和大众意见的影响。通过收集和分析推文，发现公众人物的信息对公众情感和大众意见具有显著的影响。

    

    COVID-19 疫情为健康沟通带来了新机遇，增加了公众使用在线渠道获取与健康相关情绪的机会。人们已经转向社交媒体网络分享与 COVID-19 疫情影响相关的情感。本文研究了公众人物（即运动员、政治家、新闻工作者）共享的社交信息在决定整体公共话语方向中的作用。我们从 2020 年 1 月 1 日到 2022 年 3 月 1 日收集了约 1300 万条推特。使用一个经过调优的 DistilRoBERTa 模型计算了每条推文的情绪，该模型用于比较与公众人物提及同时出现的 COVID-19 疫苗相关推特发布。我们的发现表明，在 COVID-19 疫情的前两年里，与公众人物共享的信息同时出现的情感内容具有一致的模式，影响了公众舆论和大众。

    The COVID-19 pandemic has introduced new opportunities for health communication, including an increase in the public use of online outlets for health-related emotions. People have turned to social media networks to share sentiments related to the impacts of the COVID-19 pandemic. In this paper we examine the role of social messaging shared by Persons in the Public Eye (i.e. athletes, politicians, news personnel) in determining overall public discourse direction. We harvested approximately 13 million tweets ranging from 1 January 2020 to 1 March 2022. The sentiment was calculated for each tweet using a fine-tuned DistilRoBERTa model, which was used to compare COVID-19 vaccine-related Twitter posts (tweets) that co-occurred with mentions of People in the Public Eye. Our findings suggest the presence of consistent patterns of emotional content co-occurring with messaging shared by Persons in the Public Eye for the first two years of the COVID-19 pandemic influenced public opinion and larg
    
[^25]: 使用语言反馈规模化训练语言模型

    Training Language Models with Language Feedback at Scale. (arXiv:2303.16755v1 [cs.CL])

    [http://arxiv.org/abs/2303.16755](http://arxiv.org/abs/2303.16755)

    本文提出一种新方法，即利用更丰富的语言反馈进行模仿学习，通过三个迭代步骤对语言模型进行训练以生成更符合人类偏好的输出。

    

    预训练的语言模型经常生成不符合人类偏好的输出，例如有害的文本或事实不正确的摘要。最近的研究尝试通过学习一种简单的人类反馈形式（即模型生成输出之间的比较）来解决这些问题。但是，比较反馈只能传达有限的关于人类偏好的信息。在本文中，我们介绍了一种新的方法——使用语言反馈进行模仿学习（ILF），它利用了更丰富的语言反馈。ILF由三个迭代步骤组成：第一步，根据输入，初始LM输出和反馈对语言模型进行调节以生成改进。第二步，选择最多反馈的改进。第三步，微调语言模型，以最大化在给定输入的情况下选择的改进的可能性。我们在理论上证明了ILF可以被看作是贝叶斯推断，类似于从人类反馈中进行强化学习。我们还评估了ILF在各种基准测试中的性能。

    Pretrained language models often generate outputs that are not in line with human preferences, such as harmful text or factually incorrect summaries. Recent work approaches the above issues by learning from a simple form of human feedback: comparisons between pairs of model-generated outputs. However, comparison feedback only conveys limited information about human preferences. In this paper, we introduce Imitation learning from Language Feedback (ILF), a new approach that utilizes more informative language feedback. ILF consists of three steps that are applied iteratively: first, conditioning the language model on the input, an initial LM output, and feedback to generate refinements. Second, selecting the refinement incorporating the most feedback. Third, finetuning the language model to maximize the likelihood of the chosen refinement given the input. We show theoretically that ILF can be viewed as Bayesian Inference, similar to Reinforcement Learning from human feedback. We evaluate
    
[^26]: COVID-19、肺炎和结核的深度迁移学习——基于胸部X射线图像的检测综述

    Deep transfer learning for detecting Covid-19, Pneumonia and Tuberculosis using CXR images -- A Review. (arXiv:2303.16754v1 [eess.IV])

    [http://arxiv.org/abs/2303.16754](http://arxiv.org/abs/2303.16754)

    本综述研究了使用深度迁移学习技术检测胸部X射线图像中COVID-19、肺炎和结核病的情况。文章调查了现有的图像分类技术并讨论了将迁移学习应用于这一领域的挑战和机会。文章重点考察了近期使用深度迁移学习算法进行COVID-19、肺炎和结核病检测的研究，并探讨了未来深度迁移学习在CX图像分类中的研究方向和在肺部疾病诊断和治疗中的潜力。

    

    胸部X光仍然是用于诊断肺部疾病的最常见成像模式。然而，它们需要少数专家（放射科医师和肺科医师）的解读。本综述论文研究了使用深度迁移学习技术来检测胸部X射线图像中的COVID-19、肺炎和结核病的情况。本文概述了当前最先进的CX图像分类技术，并讨论了将迁移学习应用于该领域的挑战和机会。本文全面考察了最近使用深度迁移学习算法进行COVID-19、肺炎和结核病检测的研究，突出了这些方法的优缺点。最后，本综述论文讨论了深度迁移学习用于CX图像分类的未来研究方向，以及这些技术在诊断和治疗肺部疾病方面的潜力。

    Chest X-rays remains to be the most common imaging modality used to diagnose lung diseases. However, they necessitate the interpretation of experts (radiologists and pulmonologists), who are few. This review paper investigates the use of deep transfer learning techniques to detect COVID-19, pneumonia, and tuberculosis in chest X-ray (CXR) images. It provides an overview of current state-of-the-art CXR image classification techniques and discusses the challenges and opportunities in applying transfer learning to this domain. The paper provides a thorough examination of recent research studies that used deep transfer learning algorithms for COVID-19, pneumonia, and tuberculosis detection, highlighting the advantages and disadvantages of these approaches. Finally, the review paper discusses future research directions in the field of deep transfer learning for CXR image classification, as well as the potential for these techniques to aid in the diagnosis and treatment of lung diseases.
    
[^27]: 一种用于审稿人分配问题的黄金标准数据集

    A Gold Standard Dataset for the Reviewer Assignment Problem. (arXiv:2303.16750v1 [cs.IR])

    [http://arxiv.org/abs/2303.16750](http://arxiv.org/abs/2303.16750)

    该论文提出了一个用于审稿人分配问题的新数据集，解决了当前算法难以进行原则比较的问题，并提供了基于此数据集的算法比较结果，为利益相关者在选择算法方面提供了一个基础。

    

    许多同行评审期刊或会议正在使用或试图使用算法将投稿分配给审稿人。这些自动化方法的关键是“相似度分数”，即对审稿人在审查论文中的专业水平的数值估计，已经提出了许多算法来计算这些分数。然而，这些算法尚未经过有原则的比较，这使得利益相关者难以以基于证据的方式选择算法。比较现有算法和开发更好算法的关键挑战是缺乏公开可用的黄金标准数据，这些数据将用于进行可重复研究。我们通过收集一组新的相似度得分数据来解决这个问题，并将其发布给研究社区。我们的数据集由58位研究人员提供的477个自我报告的专业水平分数组成，用于评估他们先前阅读的论文的审查经验。我们使用这些数据来比较各种算法，并对标准数据集的设计提出了建议。

    Many peer-review venues are either using or looking to use algorithms to assign submissions to reviewers. The crux of such automated approaches is the notion of the "similarity score"--a numerical estimate of the expertise of a reviewer in reviewing a paper--and many algorithms have been proposed to compute these scores. However, these algorithms have not been subjected to a principled comparison, making it difficult for stakeholders to choose the algorithm in an evidence-based manner. The key challenge in comparing existing algorithms and developing better algorithms is the lack of the publicly available gold-standard data that would be needed to perform reproducible research. We address this challenge by collecting a novel dataset of similarity scores that we release to the research community. Our dataset consists of 477 self-reported expertise scores provided by 58 researchers who evaluated their expertise in reviewing papers they have read previously.  We use this data to compare s
    
[^28]: 利用自然语言反馈进行代码生成的改进

    Improving Code Generation by Training with Natural Language Feedback. (arXiv:2303.16749v1 [cs.SE])

    [http://arxiv.org/abs/2303.16749](http://arxiv.org/abs/2303.16749)

    该论文提出了一种新算法ILF，通过从自然语言反馈中进行学习来显著提高代码生成模型的性能，即使只有少量反馈，也可以获得很好的效果。

    

    预先训练好的大型语言模型（LLM）在推理时使用自然语言反馈的潜力是最近的一个令人兴奋的发展。我们在此基础上提出一种名为Language Feedback（ILF）的算法，用于从自然语言反馈中进行学习。ILF在训练期间仅需要少量的人工编写反馈，并且在测试时不需要相同的反馈，因此使用起来既方便又高效。此外，我们进一步证明ILF可以被视为最小化与基准分布的KL散度的一种形式，并在神经程序合成任务上进行了概念验证。我们使用ILF在Mostly Basic Python Problems(MBPP)基准测试上将Codegen-Mono 6.1B模型的pass @ 1覆盖率相对提高了38%（绝对提高了10%），胜过了在MBPP上微调和在人类修复的程序上微调的模型。总的来说，我们的结果表明，即使只有少量反馈，从人类编写的自然语言反馈中进行学习也可以显著改进代码生成模型。

    The potential for pre-trained large language models (LLMs) to use natural language feedback at inference time has been an exciting recent development. We build upon this observation by formalizing an algorithm for learning from natural language feedback at training time instead, which we call Imitation learning from Language Feedback (ILF). ILF requires only a small amount of human-written feedback during training and does not require the same feedback at test time, making it both user-friendly and sample-efficient. We further show that ILF can be seen as a form of minimizing the KL divergence to the ground truth distribution and demonstrate a proof-of-concept on a neural program synthesis task. We use ILF to improve a Codegen-Mono 6.1B model's pass@1 rate by 38% relative (and 10% absolute) on the Mostly Basic Python Problems (MBPP) benchmark, outperforming both fine-tuning on MBPP and fine-tuning on repaired programs written by humans. Overall, our results suggest that learning from h
    
[^29]: 基于半非负张量分解的多视图聚类

    Multi-View Clustering via Semi-non-negative Tensor Factorization. (arXiv:2303.16748v1 [cs.LG])

    [http://arxiv.org/abs/2303.16748](http://arxiv.org/abs/2303.16748)

    该论文提出了基于半非负张量分解的多视图聚类方法，该方法采用单侧正交约束来考虑视图间的关系，并使用张量Schatten p-范数正则化来表征多视图数据的簇结构和利用视图间的互补信息。

    

    非负矩阵分解及其变体被广泛应用于多视图聚类(MVC)。然而，现有基于非负矩阵分解的MVC方法分别对每个视图数据进行矩阵分解，忽略了视图间的影响，不能很好地利用视图内部空间结构和视 图间的互补信息。为解决这个问题，提出了半非负张量分解(Semi-NTF)，并开发了一种基于Semi-NTF的新型多视图聚类方法，其中约束条件为单侧正交约束。我们的模型直接在第3阶张量上执行Semi-NTF，该张量由视图的锚图组成，因此我们的模型直接考虑视图间的关系。此外，我们使用张量Schatten p-范数正则化作为对第3阶张量的秩近似，来表征多视图数据的簇结构，并利用视图间的互补信息。

    Multi-view clustering (MVC) based on non-negative matrix factorization (NMF) and its variants have received a huge amount of attention in recent years due to their advantages in clustering interpretability. However, existing NMF-based multi-view clustering methods perform NMF on each view data respectively and ignore the impact of between-view. Thus, they can't well exploit the within-view spatial structure and between-view complementary information. To resolve this issue, we present semi-non-negative tensor factorization (Semi-NTF) and develop a novel multi-view clustering based on Semi-NTF with one-side orthogonal constraint. Our model directly performs Semi-NTF on the 3rd-order tensor which is composed of anchor graphs of views. Thus, our model directly considers the between-view relationship. Moreover, we use the tensor Schatten p-norm regularization as a rank approximation of the 3rd-order tensor which characterizes the cluster structure of multi-view data and exploits the between
    
[^30]: 最大似然方法再探：Kullback - Leibler 散度中的规范对称性和性能保证的正则化

    Maximum likelihood method revisited: Gauge symmetry in Kullback -- Leibler divergence and performance-guaranteed regularization. (arXiv:2303.16721v1 [stat.ML])

    [http://arxiv.org/abs/2303.16721](http://arxiv.org/abs/2303.16721)

    本文提出了一种在最大似然方法中进行正则化的理论上保证的方法，通过关注 Kullback - Leibler 散度中的规范对称性，可以获得最优的模型。该方法不需要频繁搜索正则化的超参数。

    

    最大似然方法是估计数据背后概率的最知名方法。然而，传统方法获得与经验分布最接近的概率模型，导致过度拟合。然后，正则化方法可以防止模型过度接近错误的概率，但是对它们的性能知之甚少。正则化的思想类似于纠错代码，通过将次优解与错误接收到的代码混合，获得最优解码。纠错代码中的最优解码是基于规范对称性实现的。我们通过关注 Kullback - Leibler 散度中的规范对称性，提出了最大似然方法中的理论上保证的正则化。在我们的方法中，我们可以获得最优的模型，而无需频繁搜索正则化中经常出现的超参数。

    The maximum likelihood method is the best-known method for estimating the probabilities behind the data. However, the conventional method obtains the probability model closest to the empirical distribution, resulting in overfitting. Then regularization methods prevent the model from being excessively close to the wrong probability, but little is known systematically about their performance. The idea of regularization is similar to error-correcting codes, which obtain optimal decoding by mixing suboptimal solutions with an incorrectly received code. The optimal decoding in error-correcting codes is achieved based on gauge symmetry. We propose a theoretically guaranteed regularization in the maximum likelihood method by focusing on a gauge symmetry in Kullback -- Leibler divergence. In our approach, we obtain the optimal model without the need to search for hyperparameters frequently appearing in regularization.
    
[^31]: 基于拓扑的点云聚类方法

    Topological Point Cloud Clustering. (arXiv:2303.16716v1 [math.AT])

    [http://arxiv.org/abs/2303.16716](http://arxiv.org/abs/2303.16716)

    本文提出一种新的基于拓扑的点聚类方法，该方法可以利用拓扑特征描述点云内的数据点，相较于传统图模型方法更具有健壮性和效率。

    

    本文提出了一种叫做拓扑点云聚类（TPCC）的新方法，它基于点云对于全局拓扑特征的贡献来聚类点。TPCC从谱聚类和拓扑数据分析中综合了有利的特征，基于考虑与所考虑的点云相关联的一个单形复合体的谱特性。由于它基于考虑稀疏特征向量计算，TPCC同样容易解释和实现，就像谱聚类一样。然而，通过不仅关注与从点云数据创建的图相关联的单个矩阵，而是关注与恰当构造的单形复合体相关联的整个Hodge-Laplacian的一整套矩阵，我们可以利用更丰富的拓扑特征来描述点云内的数据点，并受益于拓扑技术相对于噪声的相对健壮性。我们在合成和真实世界数据上测试了TPCC的性能。

    We present Topological Point Cloud Clustering (TPCC), a new method to cluster points in an arbitrary point cloud based on their contribution to global topological features. TPCC synthesizes desirable features from spectral clustering and topological data analysis and is based on considering the spectral properties of a simplicial complex associated to the considered point cloud. As it is based on considering sparse eigenvector computations, TPCC is similarly easy to interpret and implement as spectral clustering. However, by focusing not just on a single matrix associated to a graph created from the point cloud data, but on a whole set of Hodge-Laplacians associated to an appropriately constructed simplicial complex, we can leverage a far richer set of topological features to characterize the data points within the point cloud and benefit from the relative robustness of topological techniques against noise. We test the performance of TPCC on both synthetic and real-world data and compa
    
[^32]: 使用GAN实现差分隐私的日志变体生成

    TraVaG: Differentially Private Trace Variant Generation Using GANs. (arXiv:2303.16704v1 [cs.LG])

    [http://arxiv.org/abs/2303.16704](http://arxiv.org/abs/2303.16704)

    本文提出一种基于GAN的新方法TraVaG，能够生成多样和具有代表性的新变体，同时提供差分隐私，并在出现高比例稀有变体时提供工业规模的好处和提高隐私保证水平。

    

    流程挖掘在工业领域快速发展，然而事件数据中包括的敏感和私人信息的隐私问题也越来越受关注。现有研究主要关注为主要流程挖掘技术，如流程发现，提供差分隐私等隐私保证的变体。然而，释放变体的隐私保护技术仍未满足工业规模使用的所有要求，并且在出现高频稀有变体时提供隐私保证仍然是一个挑战。本文提出一种基于生成的对抗网络（GAN）的新方法TraVaG，用于发布具有差分隐私的不同变体，并在出现高比例稀有变体时提供工业规模的好处和提高隐私保证水平。此外，TraVaG通过生成多样和具有代表性的新变体来克服以往技术的局限性，同时保护隐私。

    Process mining is rapidly growing in the industry. Consequently, privacy concerns regarding sensitive and private information included in event data, used by process mining algorithms, are becoming increasingly relevant. State-of-the-art research mainly focuses on providing privacy guarantees, e.g., differential privacy, for trace variants that are used by the main process mining techniques, e.g., process discovery. However, privacy preservation techniques for releasing trace variants still do not fulfill all the requirements of industry-scale usage. Moreover, providing privacy guarantees when there exists a high rate of infrequent trace variants is still a challenge. In this paper, we introduce TraVaG as a new approach for releasing differentially private trace variants based on \text{Generative Adversarial Networks} (GANs) that provides industry-scale benefits and enhances the level of privacy guarantees when there exists a high ratio of infrequent variants. Moreover, TraVaG overcome
    
[^33]: 针对非线性部分可观测系统的局部线性化概率逆优化控制方法

    Probabilistic inverse optimal control with local linearization for non-linear partially observable systems. (arXiv:2303.16698v1 [cs.LG])

    [http://arxiv.org/abs/2303.16698](http://arxiv.org/abs/2303.16698)

    本文介绍了一种针对非线性部分可观测系统的局部线性化概率逆优化控制方法，可用于特征化顺序决策任务中的行为，并且具有广泛的适用性。

    

    逆优化控制方法可以用于特征化顺序决策任务中的行为。然而，大多数现有的工作要求已知控制信号，或者仅限于完全可观测或线性系统。本文介绍了一种概率逆优化控制方法，用于非线性随机系统的丢失控制信号和部分可观测性，该方法统一了现有方法。通过使用代理的感觉和控制系统的噪声特征的显式模型以及局部线性化技术，我们推导出了模型参数的近似似然函数，可以在单个正向传递中计算。我们在随机和部分可观测版本的经典控制任务，导航任务和手动达到任务上评估了我们提出的方法。该方法具有广泛的适用性，可用于模仿学习到感觉运动神经科学。

    Inverse optimal control methods can be used to characterize behavior in sequential decision-making tasks. Most existing work, however, requires the control signals to be known, or is limited to fully-observable or linear systems. This paper introduces a probabilistic approach to inverse optimal control for stochastic non-linear systems with missing control signals and partial observability that unifies existing approaches. By using an explicit model of the noise characteristics of the sensory and control systems of the agent in conjunction with local linearization techniques, we derive an approximate likelihood for the model parameters, which can be computed within a single forward pass. We evaluate our proposed method on stochastic and partially observable version of classic control tasks, a navigation task, and a manual reaching task. The proposed method has broad applicability, ranging from imitation learning to sensorimotor neuroscience.
    
[^34]: 通过高效的逆强化学习实现通信负载均衡

    Communication Load Balancing via Efficient Inverse Reinforcement Learning. (arXiv:2303.16686v1 [cs.NI])

    [http://arxiv.org/abs/2303.16686](http://arxiv.org/abs/2303.16686)

    本文使用逆强化学习方法成功解决了通信负载均衡问题，首次将逆强化学习应用于该领域。

    

    通信负载均衡旨在平衡不同可用资源之间的负载，从而提高网络系统的服务质量。本文将负载均衡问题描述为马尔可夫决策过程，并利用强化学习有效解决负载均衡问题。然而，为了利用经典强化学习解决负载均衡问题，我们需要明确的奖励定义。构建这种奖励函数是具有挑战性的，因为它涉及到专家知识的需求，而且对于最优奖励函数的形式缺乏共识。本文采用逆强化学习方法，从一组演示中推断奖励函数，成功地解决了通信负载均衡问题。据我们所知，这是第一次在通信负载均衡领域成功应用逆强化学习。具体而言，我们首先从一组演示中推断奖励函数，然后学习强化学习负载均衡策略。

    Communication load balancing aims to balance the load between different available resources, and thus improve the quality of service for network systems. After formulating the load balancing (LB) as a Markov decision process problem, reinforcement learning (RL) has recently proven effective in addressing the LB problem. To leverage the benefits of classical RL for load balancing, however, we need an explicit reward definition. Engineering this reward function is challenging, because it involves the need for expert knowledge and there lacks a general consensus on the form of an optimal reward function. In this work, we tackle the communication load balancing problem from an inverse reinforcement learning (IRL) approach. To the best of our knowledge, this is the first time IRL has been successfully applied in the field of communication load balancing. Specifically, first, we infer a reward function from a set of demonstrations, and then learn a reinforcement learning load balancing polic
    
[^35]: 未知流量场景下的通信负载均衡中的策略重用

    Policy Reuse for Communication Load Balancing in Unseen Traffic Scenarios. (arXiv:2303.16685v1 [cs.NI])

    [http://arxiv.org/abs/2303.16685](http://arxiv.org/abs/2303.16685)

    提出了一种策略重用框架，该框架通过选择最适合执行的预训练强化学习策略来解决通信负载平衡问题，其根据不同流量场景下的策略训练库进行选择，可以胜过传统的负载平衡算法。

    

    随着通信网络复杂性和流量增长的持续增加，通信负载平衡解决方案越来越受到关注。具体来说，基于强化学习（RL）的方法与传统的基于规则的方法相比，表现出了令人印象深刻的性能。然而，标准的RL方法通常需要大量的数据进行训练，并且对于在训练过程中未遇到的场景的泛化能力较差。我们提出了一个策略重用框架，其中策略选择器基于当前的流量状况选择最适合的预训练RL策略来执行。我们的方法依赖于一个策略库，其中包含在不同流量场景下训练的策略。在部署到未知的流量场景时，我们根据当前场景的前一天的流量与训练期间观察到的流量之间的相似度从策略库中选择策略。实验表明，该框架可以胜过传统的负载平衡算法。

    With the continuous growth in communication network complexity and traffic volume, communication load balancing solutions are receiving increasing attention. Specifically, reinforcement learning (RL)-based methods have shown impressive performance compared with traditional rule-based methods. However, standard RL methods generally require an enormous amount of data to train, and generalize poorly to scenarios that are not encountered during training. We propose a policy reuse framework in which a policy selector chooses the most suitable pre-trained RL policy to execute based on the current traffic condition. Our method hinges on a policy bank composed of policies trained on a diverse set of traffic scenarios. When deploying to an unknown traffic scenario, we select a policy from the policy bank based on the similarity between the previous-day traffic of the current scenario and the traffic observed during training. Experiments demonstrate that this framework can outperform classical a
    
[^36]: 实际分类任务中的神经符号规则学习

    Neuro-symbolic Rule Learning in Real-world Classification Tasks. (arXiv:2303.16674v1 [cs.LG])

    [http://arxiv.org/abs/2303.16674](http://arxiv.org/abs/2303.16674)

    本研究通过将神经范式模块扩展到多类和多标签实际分类任务中，以及强制实现符号互斥属性，提高了神经符号规则学习模型的准确性和可解释性。

    

    神经符号规则学习因其比纯神经模型具有更好的可解释性且比符号规则学习扩展性更好而受到关注。最近提出了一种名为 pix2rule 的方法，引入了一个神经范式模块来使用前向层学习符号规则。尽管在合成二分类方面证明了其有效性，但 pix2rule 尚未应用于更具挑战性的任务，如基于实际数据的多标签和多类分类。本文通过将神经范式模块扩展到以下三个方面来解决这一限制：(i) 支持实际多类和多标签分类任务的规则学习，(ii) 在多类分类中强制符号互斥属性(即预测恰好一个类)，以及(iii) 探索其在大输入输出方面的扩展性。我们使用类似于 pix2rule 的神经 DNF 模块训练了一个普通的神经 DNF 模型，用于多标签分类，同时提出了一种新方法来通过改进现有的神经 DNF 模块实现多类分类中的互斥性。在多个基准数据集上的实验结果表明，我们扩展的神经 DNF 模型在准确性和可解释性方面明显优于现有的神经符号规则学习模型以及纯神经模型和符号规则学习模型。

    Neuro-symbolic rule learning has attracted lots of attention as it offers better interpretability than pure neural models and scales better than symbolic rule learning. A recent approach named pix2rule proposes a neural Disjunctive Normal Form (neural DNF) module to learn symbolic rules with feed-forward layers. Although proved to be effective in synthetic binary classification, pix2rule has not been applied to more challenging tasks such as multi-label and multi-class classifications over real-world data. In this paper, we address this limitation by extending the neural DNF module to (i) support rule learning in real-world multi-class and multi-label classification tasks, (ii) enforce the symbolic property of mutual exclusivity (i.e. predicting exactly one class) in multi-class classification, and (iii) explore its scalability over large inputs and outputs. We train a vanilla neural DNF model similar to pix2rule's neural DNF module for multi-label classification, and we propose a nove
    
[^37]: 基于矩阵自回归的联邦学习拜占庭容错聚合方案

    A Byzantine-Resilient Aggregation Scheme for Federated Learning via Matrix Autoregression on Client Updates. (arXiv:2303.16668v1 [cs.LG])

    [http://arxiv.org/abs/2303.16668](http://arxiv.org/abs/2303.16668)

    本文提出了FLANDERS，一种基于矩阵自回归的联邦学习聚合方案，可以识别恶意客户端，并提供了强大的拜占庭攻击防御。

    

    本文提出了FLANDERS，一种新颖的联邦学习（FL）聚合方案，可以抵御拜占庭攻击。FLANDERS将每个FL轮次中由客户端发送的本地模型更新视为矩阵值时间序列。然后，通过将实际观测与由矩阵自回归预测模型估计的观测进行比较，识别恶意客户端作为这个时间序列的异常值。在不同FL设置下对多个数据集进行的实验证明，FLANDERS在抵御拜占庭攻击方面与最强大的基线相匹配。此外，与现有的防御策略相比， FLANDERS即使在极其严重的攻击场景下仍然非常有效。

    In this work, we propose FLANDERS, a novel federated learning (FL) aggregation scheme robust to Byzantine attacks. FLANDERS considers the local model updates sent by clients at each FL round as a matrix-valued time series. Then, it identifies malicious clients as outliers of this time series by comparing actual observations with those estimated by a matrix autoregressive forecasting model. Experiments conducted on several datasets under different FL settings demonstrate that FLANDERS matches the robustness of the most powerful baselines against Byzantine clients. Furthermore, FLANDERS remains highly effective even under extremely severe attack scenarios, as opposed to existing defense strategies.
    
[^38]: 从数据中学习流函数及其在非线性振荡器中的应用

    Learning Flow Functions from Data with Applications to Nonlinear Oscillators. (arXiv:2303.16656v1 [eess.SY])

    [http://arxiv.org/abs/2303.16656](http://arxiv.org/abs/2303.16656)

    本文提出一种基于循环神经网络的架构，通过数据学习了一个因果、时不变且连续时间控制系统的流函数，并证明了该架构能够近似表示流函数，在实验中应用于Van der Pol和FitzHugh Nagumo振荡器的轨迹重现和最优控制输入计算。

    

    本文提出了一种基于循环神经网络（RNN）的架构来从轨迹数据中学习一个因果、时不变且连续时间控制系统的流函数。通过将控制输入的类别限制为分段常数函数，我们展示了学习流函数等价于学习离散时间动态系统的状态映射的输入输出映射关系。因此，提出一个由编码器和解码器网络构成的RNN，以将系统状态映射到RNN的隐藏状态以及将隐藏状态映射回系统状态。通过利用系统的因果性和时不变性，我们证明了提出的架构能够近似地表示流函数，且可以在任意时间查询学习到的流函数。我们通过Van der Pol和FitzHugh Nagumo振荡器的模型验证了所提出方法的有效性，结果表明该架构能够准确地重现这两个系统的轨迹。对于Van der Pol振荡器，我们展示了该架构如何用于计算将系统引导到所需轨迹的最优控制输入。

    We describe a recurrent neural network (RNN) based architecture to learn the flow function of a causal, time-invariant and continuous-time control system from trajectory data. By restricting the class of control inputs to piecewise constant functions, we show that learning the flow function is equivalent to learning the input-to-state map of a discrete-time dynamical system. This motivates the use of an RNN together with encoder and decoder networks which map the state of the system to the hidden state of the RNN and back. We show that the proposed architecture is able to approximate the flow function by exploiting the system's causality and time-invariance. The output of the learned flow function model can be queried at any time instant. We experimentally validate the proposed method using models of the Van der Pol and FitzHugh Nagumo oscillators. In both cases, the results demonstrate that the architecture is able to closely reproduce the trajectories of these two systems. For the Va
    
[^39]: 多智能体系统中博弈论决策的分层式实用树模型

    A Hierarchical Game-Theoretic Decision-Making for Cooperative Multi-Agent Systems Under the Presence of Adversarial Agents. (arXiv:2303.16641v1 [cs.MA])

    [http://arxiv.org/abs/2303.16641](http://arxiv.org/abs/2303.16641)

    本文提出了一种名为GUT的新型分层式博弈论实用树模型，在多智能体决策中实现高低层次策略分解，使用代理需求作为新的收益度量，通过广泛的数值模拟表明可以帮助MAS以更低的成本和更高的获胜率实现具有挑战性的任务。

    

    在危险情境中的多智能体系统（MAS）中，基础的关系可以被表示成博弈论模型。本文提出了一种新的基于网络的分层模型，称为博弈论实用树（GUT），它将高层次策略分解为可执行的低层次动作，以实现合作MAS决策。它结合了一种基于代理需求的实时策略游戏的新收益度量。我们在Explore游戏领域中，从平衡成功概率和系统成本的角度衡量了MAS的绩效，评估了GUT方法相对于贪心地依赖于组合动作奖励的最新方法。广泛的数值模拟的确定结果表明GUT可以组织更复杂的MAS协作关系，帮助团队以更低的成本和更高的获胜率实现具有挑战性的任务。此外，我们演示了使用模拟应用GUT的可行性。

    Underlying relationships among Multi-Agent Systems (MAS) in hazardous scenarios can be represented as Game-theoretic models. This paper proposes a new hierarchical network-based model called Game-theoretic Utility Tree (GUT), which decomposes high-level strategies into executable low-level actions for cooperative MAS decisions. It combines with a new payoff measure based on agent needs for real-time strategy games. We present an Explore game domain, where we measure the performance of MAS achieving tasks from the perspective of balancing the success probability and system costs. We evaluate the GUT approach against state-of-the-art methods that greedily rely on rewards of the composite actions. Conclusive results on extensive numerical simulations indicate that GUT can organize more complex relationships among MAS cooperation, helping the group achieve challenging tasks with lower costs and higher winning rates. Furthermore, we demonstrated the applicability of the GUT using the simula
    
[^40]: 针对风电功率预测的有目标对抗攻击

    Targeted Adversarial Attacks on Wind Power Forecasts. (arXiv:2303.16633v1 [cs.LG])

    [http://arxiv.org/abs/2303.16633](http://arxiv.org/abs/2303.16633)

    本研究检测了两个深度学习预测模型的脆弱性，并提出了一种新的评估指标。这些模型可以被针对性攻击，因此保护风电功率预测结果对现代电力系统的稳定性至关重要。

    

    近年来，研究人员提出了多种用于风电功率预测的深度学习模型。这些模型比传统的机器学习算法或物理模型更准确地预测了风电场或整个地区的风力发电。然而，最新的研究表明，深度学习模型常常会受到对抗性攻击的影响。由于风电功率预测对现代电力系统的稳定性至关重要，因此保护预测结果免受这种威胁非常重要。本文研究了两种不同预测模型对有目标、半有目标和无目标对抗攻击的脆弱性。我们考虑了一种用于预测风电场功率发电的长短期记忆（LSTM）网络和一种用于预测德国整个地区的风力发电的卷积神经网络（CNN）。此外，我们提出了总对抗鲁棒性评分（TARS），一种用于量化回归模型稳健性的评估指标。

    In recent years, researchers proposed a variety of deep learning models for wind power forecasting. These models predict the wind power generation of wind farms or entire regions more accurately than traditional machine learning algorithms or physical models. However, latest research has shown that deep learning models can often be manipulated by adversarial attacks. Since wind power forecasts are essential for the stability of modern power systems, it is important to protect them from this threat. In this work, we investigate the vulnerability of two different forecasting models to targeted, semitargeted, and untargeted adversarial attacks. We consider a Long Short-Term Memory (LSTM) network for predicting the power generation of a wind farm and a Convolutional Neural Network (CNN) for forecasting the wind power generation throughout Germany. Moreover, we propose the Total Adversarial Robustness Score (TARS), an evaluation metric for quantifying the robustness of regression models to 
    
[^41]: Fairlearn: 评估和提升人工智能系统的公平性

    Fairlearn: Assessing and Improving Fairness of AI Systems. (arXiv:2303.16626v1 [cs.LG])

    [http://arxiv.org/abs/2303.16626](http://arxiv.org/abs/2303.16626)

    Fairlearn是一个有助于评估和提高人工智能系统公平性的开源项目，通过可视化地评估受影响人群的模型输出，并提供多种算法来缓解不公平问题，同时还在考虑系统的更广泛社会背景方面提供了学习资源。

    

    Fairlearn是一个开源项目，旨在帮助从业人员评估和提高人工智能（AI）系统的公平性。该相关的Python库，也名为Fairlearn，支持跨受影响的人口评估模型的输出，并包括几种算法以缓解公平性问题。基于公平是一个社会技术挑战的认识，该项目集成了学习资源，以帮助从业人员考虑一个系统的更广泛社会背景。

    Fairlearn is an open source project to help practitioners assess and improve fairness of artificial intelligence (AI) systems. The associated Python library, also named fairlearn, supports evaluation of a model's output across affected populations and includes several algorithms for mitigating fairness issues. Grounded in the understanding that fairness is a sociotechnical challenge, the project integrates learning resources that aid practitioners in considering a system's broader societal context.
    
[^42]: 使用丰富的元数据注释的屏幕角色的个性化语言建模

    Personalised Language Modelling of Screen Characters Using Rich Metadata Annotations. (arXiv:2303.16618v1 [cs.CL])

    [http://arxiv.org/abs/2303.16618](http://arxiv.org/abs/2303.16618)

    本篇论文研究了如何使用丰富的元数据注释的信息进行屏幕角色的个性化语言建模，测试表明这样可以有效地进行个性化语言模型的构建，即使对于零样本的演说家也可以应用。

    

    语言模型的个性化为对话敏感，能更好地捕捉特定特征的人员和/或特定环境中的说话模式。然而，丰富的角色注释难以得到和成功利用。在此工作中，我们发布并描述了一组新颖的手动注释，涵盖了来自流行的 Cornell 电影对话语料库的 863 名演讲者，包括特征引用和角色描述，以及超过 95％ 的特色电影的一组自动提取的六个元数据。我们对两个语料库进行了广泛的实验，并表明可以有效地使用此类注释来个性化语言模型，将困惑减少高达 8.5％。我们的方法甚至可以应用于零样本的演讲者，即对于没有先前培训数据的演讲者，依赖于角色的人口特征的组合。由于收集此类元数据成本高昂，因此我们还贡献了一项成本效益分析，以突出显示

    Personalisation of language models for dialogue sensitises them to better capture the speaking patterns of people of specific characteristics, and/or in specific environments. However, rich character annotations are difficult to come by and to successfully leverage. In this work, we release and describe a novel set of manual annotations for 863 speakers from the popular Cornell Movie Dialog Corpus, including features like characteristic quotes and character descriptions, and a set of six automatically extracted metadata for over 95% of the featured films. We perform extensive experiments on two corpora and show that such annotations can be effectively used to personalise language models, reducing perplexity by up to 8.5%. Our method can be applied even zero-shot for speakers for whom no prior training data is available, by relying on combinations of characters' demographic characteristics. Since collecting such metadata is costly, we also contribute a cost-benefit analysis to highlight
    
[^43]: 基于文本提示学习和双向训练的组成图像检索方法

    Bi-directional Training for Composed Image Retrieval via Text Prompt Learning. (arXiv:2303.16604v1 [cs.CV])

    [http://arxiv.org/abs/2303.16604](http://arxiv.org/abs/2303.16604)

    本文提出了一种基于文本提示学习和双向训练的组成图像检索方法，可以应用于现有的体系结构，并且在修改文本存在噪声或歧义的情况下特别有效。

    

    组成图像检索是根据包含参考图像和描述所需更改的修改文本的多模态用户查询来搜索目标图像的方法。现有的解决这个具有挑战性的任务的方法学习从（参考图像，修改文本）对到图像嵌入的映射，然后将其与大型图像语料库进行匹配。本文提出了一种双向训练方案，利用了这种反向查询，并可应用于现有的组成图像检索体系结构。为了编码双向查询，我们在修改文本前面添加一个可学习的令牌，指定查询的方向，然后微调文本嵌入模块的参数。我们没有对网络架构进行其他更改。在两个标准数据集上的实验表明，双向训练在提高组成图像检索性能方面是有效的，特别是在修改文本存在噪声或歧义的情况下。

    Composed image retrieval searches for a target image based on a multi-modal user query comprised of a reference image and modification text describing the desired changes. Existing approaches to solving this challenging task learn a mapping from the (reference image, modification text)-pair to an image embedding that is then matched against a large image corpus. One area that has not yet been explored is the reverse direction, which asks the question, what reference image when modified as describe by the text would produce the given target image? In this work we propose a bi-directional training scheme that leverages such reversed queries and can be applied to existing composed image retrieval architectures. To encode the bi-directional query we prepend a learnable token to the modification text that designates the direction of the query and then finetune the parameters of the text embedding module. We make no other changes to the network architecture. Experiments on two standard datas
    
[^44]: 多输入多输出卫星广播系统中的联邦学习

    Federated Learning in MIMO Satellite Broadcast System. (arXiv:2303.16603v1 [eess.SP])

    [http://arxiv.org/abs/2303.16603](http://arxiv.org/abs/2303.16603)

    本文介绍将联邦学习应用于多输入多输出卫星广播系统，以解决现有联邦学习方法中存在的推断攻击和差分隐私精度降低的问题。

    

    联邦学习（FL）是一种分布式机器学习技术，可以在无线边缘保护客户端数据安全，使之免受攻击甚至能够不将数据上传到中央服务器。现有的联邦学习方法要么使用安全多方计算（SMC），但对于推断攻击容易受到威胁；要么采用差分隐私，但对于相对较少数据的大量客户端，可能会降低测试精度。为了解决现有方法面临的问题，本文在多输入多输出系统的内部引入了联邦学习。

    Federated learning (FL) is a type of distributed machine learning at the wireless edge that preserves the privacy of clients' data from adversaries and even the central server. Existing federated learning approaches either use (i) secure multiparty computation (SMC) which is vulnerable to inference or (ii) differential privacy which may decrease the test accuracy given a large number of parties with relatively small amounts of data each. To tackle the problem with the existing methods in the literature, In this paper, we introduce incorporate federated learning in the inner-working of MIMO systems.
    
[^45]: 论文海报：训练DNN中偏差、节点敏感性和长尾分布之间的链接 (arXiv:2303.16589v1 [cs.LG])

    Poster: Link between Bias, Node Sensitivity and Long-Tail Distribution in trained DNNs. (arXiv:2303.16589v1 [cs.LG])

    [http://arxiv.org/abs/2303.16589](http://arxiv.org/abs/2303.16589)

    DNN训练中长尾分布的数据集将给不同输出类别提供不同的分类性能，本文首次指出导致节点敏感性变化的节点偏差，提出了开放性挑战。

    

    深度神经网络(DNNs)由于其卓越的学习(和重新学习)能力，被广泛应用于各种现实世界的应用场景。然而，这些数据驱动的机器学习模型的学习效果一般取决于数据的质量和分布。因此，分布呈现长尾分布的训练数据集对DNNs构成了挑战，因为这些训练的DNNs可能对不同的输出类别提供不同程度的分类性能。虽然现有的研究已经强调了这些网络的整体偏差，但本文首次指出了导致节点对不同输出类别敏感性变化的节点偏差。据我们所知，这是第一篇强调DNNs中这种独特挑战的工作，讨论其可能的原因，并为这个新的研究方向提供了开放性挑战。我们使用真实情境数据集上训练的网络的实证案例来支持我们的推理。

    Owing to their remarkable learning (and relearning) capabilities, deep neural networks (DNNs) find use in numerous real-world applications. However, the learning of these data-driven machine learning models is generally as good as the data available to them for training. Hence, training datasets with long-tail distribution pose a challenge for DNNs, since the DNNs trained on them may provide a varying degree of classification performance across different output classes. While the overall bias of such networks is already highlighted in existing works, this work identifies the node bias that leads to a varying sensitivity of the nodes for different output classes. To the best of our knowledge, this is the first work highlighting this unique challenge in DNNs, discussing its probable causes, and providing open challenges for this new research direction. We support our reasoning using an empirical case study of the networks trained on a real-world dataset.
    
[^46]: 量子深度对冲

    Quantum Deep Hedging. (arXiv:2303.16585v1 [quant-ph])

    [http://arxiv.org/abs/2303.16585](http://arxiv.org/abs/2303.16585)

    本论文针对对冲问题提出了基于量子神经网络的深度强化学习方法，相比于其他经典或量子的标准方法，其分布式方法可以获得更高的性能。

    

    量子机器学习在金融等行业有着潜在变革性的影响。本论文研究对冲问题，深度强化学习提供了实现真实市场的有力框架。我们发展了基于策略搜索和分布式Actor-Critic算法的量子强化学习方法，使用正交和复合层的量子神经网络架构来处理策略和价值函数。我们证明了我们使用的量子神经网络是可训练的，并进行了广泛的模拟，表明量子模型可以减少可训练参数的数量，同时实现可比较的性能，分布式方法得到的性能比标准方法更好，无论是经典的还是量子的。我们成功地将所提出模型实现在一个最多有16个量子比特的离子陷阱量子处理器上，并观察到与无噪声情况下的性能相符。

    Quantum machine learning has the potential for a transformative impact across industry sectors and in particular in finance. In our work we look at the problem of hedging where deep reinforcement learning offers a powerful framework for real markets. We develop quantum reinforcement learning methods based on policy-search and distributional actor-critic algorithms that use quantum neural network architectures with orthogonal and compound layers for the policy and value functions. We prove that the quantum neural networks we use are trainable, and we perform extensive simulations that show that quantum models can reduce the number of trainable parameters while achieving comparable performance and that the distributional approach obtains better performance than other standard approaches, both classical and quantum. We successfully implement the proposed models on a trapped-ion quantum processor, utilizing circuits with up to $16$ qubits, and observe performance that agrees well with nois
    
[^47]: PMAA：一种基于渐进式多尺度注意力自编码器模型的高性能卫星遥感云去除方法

    PMAA: A Progressive Multi-scale Attention Autoencoder Model for High-Performance Cloud Removal from Multi-temporal Satellite Imagery. (arXiv:2303.16565v1 [cs.CV])

    [http://arxiv.org/abs/2303.16565](http://arxiv.org/abs/2303.16565)

    PMAA是一种利用全局和局部信息的高性能卫星遥感云去除架构。其中，独特的多尺度注意力模块和局部交互模块能够同时表示细粒度和粗粒度特征，相比现有方法表现更优。

    

    卫星遥感图像分析在远程感知中起着至关重要的作用，但由云层引起的信息丢失严重阻碍了其应用。本研究提出了一种名为“渐进式多尺度注意力自编码器”（PMAA）的高性能云去除架构，同时利用全局和局部信息。其主要包括云检测和云去除模块。云检测通过云掩模加强云区域以促进云去除。云去除模块主要包括新颖的多尺度注意力模块（MAM）和局部交互模块（LIM）。PMAA通过MAM建立多尺度特征的长程依赖性，并通过LIM调节细粒度细节的重建，从而实现在同一级别上同时呈现细粒度和粗粒度特征。在各种数据集上，借助不同和多尺度特征表示的帮助下，PMAA在定量和视觉评估方面均优于先前的现有方法，展示了其高效性和鲁棒性。

    Satellite imagery analysis plays a vital role in remote sensing, but the information loss caused by cloud cover seriously hinders its application. This study presents a high-performance cloud removal architecture called Progressive Multi-scale Attention Autoencoder (PMAA), which simultaneously leverages global and local information. It mainly consists of a cloud detection backbone and a cloud removal module. The cloud detection backbone uses cloud masks to reinforce cloudy areas to prompt the cloud removal module. The cloud removal module mainly comprises a novel Multi-scale Attention Module (MAM) and a Local Interaction Module (LIM). PMAA establishes the long-range dependency of multi-scale features using MAM and modulates the reconstruction of the fine-grained details using LIM, allowing for the simultaneous representation of fine- and coarse-grained features at the same level. With the help of diverse and multi-scale feature representation, PMAA outperforms the previous state-of-the
    
[^48]: Plan4MC: 基于技能强化学习和规划的 Minecraft 开放式任务解决方案

    Plan4MC: Skill Reinforcement Learning and Planning for Open-World Minecraft Tasks. (arXiv:2303.16563v1 [cs.LG])

    [http://arxiv.org/abs/2303.16563](http://arxiv.org/abs/2303.16563)

    本论文提出了一种基于技能强化学习和规划的 Minecraft 开放式任务解决方案，通过学习基本技能和技能规划的方法有效提高了任务解决效率，实现了24个不同的任务并在大多数任务中优于基线算法。

    

    本论文研究在 Minecraft 中构建一个多任务智能体。在没有人工演示的情况下，用强化学习（RL）解决这个开放式环境中的长程任务是极其样本低效的。为了解决这个问题，我们将 Minecraft 任务的解决分解成学习基本技能和基于技能进行规划两个阶段。我们在 Minecraft 中提出了三种类型的细粒度基本技能，并使用具有内在奖励的 RL 方法来实现成功率高的基本技能学习。在技能规划方面，我们使用大型语言模型来发现技能之间的关系，预先构建技能图。当智能体解决任务时，我们的技能搜索算法在技能图上行走并生成适当的技能计划。在实验中，我们的方法解决了 24 个不同的 Minecraft 任务，其中许多任务需要连续执行超过 10 个技能。我们的方法在大多数任务中优于基线算法。项目的网址和代码可以在 https://www.rocwang.me/plan4mc.html 找到。

    We study building a multi-task agent in Minecraft. Without human demonstrations, solving long-horizon tasks in this open-ended environment with reinforcement learning (RL) is extremely sample inefficient. To tackle the challenge, we decompose solving Minecraft tasks into learning basic skills and planning over the skills. We propose three types of fine-grained basic skills in Minecraft, and use RL with intrinsic rewards to accomplish basic skills with high success rates. For skill planning, we use Large Language Models to find the relationships between skills and build a skill graph in advance. When the agent is solving a task, our skill search algorithm walks on the skill graph and generates the proper skill plans for the agent. In experiments, our method accomplishes 24 diverse Minecraft tasks, where many tasks require sequentially executing for more than 10 skills. Our method outperforms baselines in most tasks by a large margin. The project's website and code can be found at https:
    
[^49]: 离散时间线性二次调节器中带随机参数的策略梯度方法

    Policy Gradient Methods for Discrete Time Linear Quadratic Regulator With Random Parameters. (arXiv:2303.16548v1 [math.OC])

    [http://arxiv.org/abs/2303.16548](http://arxiv.org/abs/2303.16548)

    本文使用增强学习技术中的策略梯度方法解决了带随机参数的离散时间线性二次调节器的最优控制问题，并建立了全局线性收敛保证。

    

    本文研究了一个离散时间的线性系统和二次准则的无限时域最优控制问题，其中参数是独立同分布于时间的随机变量。在这种一般情况下，我们应用增强学习技术中的策略梯度方法来寻找最优控制，而不需要了解参数的统计信息。我们研究了状态过程的次高斯性质，并根据比现有结果更弱且更易验证的假设，建立了此方法的全局线性收敛保证。我们进行了数值实验，以说明我们的结果。

    This paper studies an infinite horizon optimal control problem for discrete-time linear system and quadratic criteria, both with random parameters which are independent and identically distributed with respect to time. In this general setting, we apply the policy gradient method, a reinforcement learning technique, to search for the optimal control without requiring knowledge of statistical information of the parameters. We investigate the sub-Gaussianity of the state process and establish global linear convergence guarantee for this approach based on assumptions that are weaker and easier to verify compared to existing results. Numerical experiments are presented to illustrate our result.
    
[^50]: 异构时空图神经网络在期货量化投资中的应用研究

    Futures Quantitative Investment with Heterogeneous Continual Graph Neural Network. (arXiv:2303.16532v1 [cs.LG])

    [http://arxiv.org/abs/2303.16532](http://arxiv.org/abs/2303.16532)

    为了预测期货价格趋势，本文提出了一种基于异构任务设计和连续训练的时空图神经网络模型，可以捕捉长期和短期特征。

    

    传统计量模型预测期货价格趋势是一个具有挑战性的问题，因为需要考虑到期货历史数据以及不同期货之间的关联。时空图神经网络在处理此类空间时间数据方面具有很大的优势。本研究通过设计四个异构任务来捕捉长期和短期特征：价格回归、移动平均价格回归、短时间内的价格差回归和变化点检测。为了充分利用这些标签，我们采用连续训练的方式对模型进行训练。

    It is a challenging problem to predict trends of futures prices with traditional econometric models as one needs to consider not only futures' historical data but also correlations among different futures. Spatial-temporal graph neural networks (STGNNs) have great advantages in dealing with such kind of spatial-temporal data. However, we cannot directly apply STGNNs to high-frequency future data because future investors have to consider both the long-term and short-term characteristics when doing decision-making. To capture both the long-term and short-term features, we exploit more label information by designing four heterogeneous tasks: price regression, price moving average regression, price gap regression (within a short interval), and change-point detection, which involve both long-term and short-term scenes. To make full use of these labels, we train our model in a continual manner. Traditional continual GNNs define the gradient of prices as the parameter important to overcome ca
    
[^51]: 深度神经网络中的随机梯度下降重要性采样

    Importance Sampling for Stochastic Gradient Descent in Deep Neural Networks. (arXiv:2303.16529v1 [cs.LG])

    [http://arxiv.org/abs/2303.16529](http://arxiv.org/abs/2303.16529)

    本文回顾了深度学习中的重要性采样理论，并总结了利用重要性采样来训练深度神经网络所面临的挑战；本文提出了一种度量方法，用于评估给定的采样策略的质量，研究了采样方案和所使用的优化器之间的相互作用。

    

    随机梯度下降通过对训练集进行均匀采样，利用有限的样本来构建无偏梯度估计。然而，在训练过程的某个阶段，某些数据比其他数据更有助于继续学习。近年来，已经广泛研究了利用重要性采样来训练深度神经网络的方法，以提出比均匀采样方案更好的采样策略。在回顾深度学习中的重要性采样理论后，本文总结了这一研究领域所面临的挑战。特别是，我们提出了一种度量方法，用于评估给定的采样策略的质量，并研究了采样方案和所使用的优化器之间的相互作用。

    Stochastic gradient descent samples uniformly the training set to build an unbiased gradient estimate with a limited number of samples. However, at a given step of the training process, some data are more helpful than others to continue learning. Importance sampling for training deep neural networks has been widely studied to propose sampling schemes yielding better performance than the uniform sampling scheme. After recalling the theory of importance sampling for deep learning, this paper reviews the challenges inherent to this research area. In particular, we propose a metric allowing the assessment of the quality of a given sampling scheme; and we study the interplay between the sampling scheme and the optimizer used.
    
[^52]: 基于模拟神经网络BP架构的集成学习模型用于煤柱稳定性分类

    Ensemble Learning Model on Artificial Neural Network-Backpropagation (ANN-BP) Architecture for Coal Pillar Stability Classification. (arXiv:2303.16524v1 [cs.LG])

    [http://arxiv.org/abs/2303.16524](http://arxiv.org/abs/2303.16524)

    本文提出一种新颖的人工神经网络反向传播（ANN-BP）和深度集成学习的方法，用于煤柱稳定性的分类。通过使用不同的ANN-BP激活函数和新的标签替代方案，将柱子稳定性扩展到四个类别，成功预测了柱子的稳定性。

    

    煤柱是确保地下硬岩矿山安全的重要结构单元。因此，需要对地下柱子的稳定性进行精确的预测。一个常用的评估柱子稳定性的指标是安全系数（SF）。不幸的是，使用SF进行柱子稳定性评估时，常常出现清晰的边界不可靠的情况。本文提出了一种新颖的人工神经网络反向传播（ANN-BP）和深度集成学习在柱子稳定性分类中的应用。柱子稳定性的分类有三种ANN-BP，分别由其激活函数区分：ANN-BP ReLU、ANN-BP ELU和ANN-BP GELU。本研究还提出了一种新的标签替代方案，通过考虑其与SF的适应性来考虑柱子的稳定性。因此，柱子稳定性分为四个类别：具有适当的安全系数而失败、具有适当的安全系数而完好、不具有适当的安全系数而失败和不具有适当的安全系数而完好。

    Pillars are important structural units used to ensure mining safety in underground hard rock mines. Therefore, precise predictions regarding the stability of underground pillars are required. One common index that is often used to assess pillar stability is the Safety Factor (SF). Unfortunately, such crisp boundaries in pillar stability assessment using SF are unreliable. This paper presents a novel application of Artificial Neural Network-Backpropagation (ANN-BP) and Deep Ensemble Learning for pillar stability classification. There are three types of ANN-BP used for the classification of pillar stability distinguished by their activation functions: ANN-BP ReLU, ANN-BP ELU, and ANN-BP GELU. This research also presents a new labeling alternative for pillar stability by considering its suitability with the SF. Thus, pillar stability is expanded into four categories: failed with a suitable safety factor, intact with a suitable safety factor, failed without a suitable safety factor, and in
    
[^53]: 在不使用数据增强的情况下加强正则化来防止在线深度聚类中的崩溃

    Hard Regularization to Prevent Collapse in Online Deep Clustering without Data Augmentation. (arXiv:2303.16521v1 [cs.LG])

    [http://arxiv.org/abs/2303.16521](http://arxiv.org/abs/2303.16521)

    该论文提出了一种不需要数据增强的在线深度聚类方法，通过加强正则化来避免崩溃，相比于其他方法，具有更高的稳定性。

    

    在线深度聚类是指联合使用特征提取网络和聚类模型，以将每个新数据点或批处理分配到聚类标签中。尽管比离线方法更快速和更灵活，但在线聚类很容易达到崩溃解，其中编码器将所有输入映射到同一点，并将所有输入放入单个聚类中。现有成功模型采用了各种技术来避免这个问题，其中大多数需要数据增强或旨在使数据集中每个聚类的平均软分配相同。我们提出了一种不需要数据增强的方法，与现有方法不同，它对硬分配进行了规则化。我们使用贝叶斯框架，导出一个直观的优化目标，可以直接包含在编码器网络的训练中。在四个图像数据集上进行测试，我们证明它比其他方法更加稳定地避免了崩溃。

    Online deep clustering refers to the joint use of a feature extraction network and a clustering model to assign cluster labels to each new data point or batch as it is processed. While faster and more versatile than offline methods, online clustering can easily reach the collapsed solution where the encoder maps all inputs to the same point and all are put into a single cluster. Successful existing models have employed various techniques to avoid this problem, most of which require data augmentation or which aim to make the average soft assignment across the dataset the same for each cluster. We propose a method that does not require data augmentation, and that, differently from existing methods, regularizes the hard assignments. Using a Bayesian framework, we derive an intuitive optimization objective that can be straightforwardly included in the training of the encoder network. Tested on four image datasets, we show that it consistently avoids collapse more robustly than other method
    
[^54]: 客户贡献评估下的公平联邦医学图像分割

    Fair Federated Medical Image Segmentation via Client Contribution Estimation. (arXiv:2303.16520v1 [cs.LG])

    [http://arxiv.org/abs/2303.16520](http://arxiv.org/abs/2303.16520)

    本论文提出了一种新方法，在客户贡献评估下同时优化协作公平性和性能公平性，以获得高质量的全局模型。

    

    如何确保联邦学习中的公平性是一个重要的话题。最近的研究探讨了如何根据客户的贡献（协作公平性）来奖励客户，以及如何实现客户之间的性能均衡（性能公平性）。尽管已经在其中一个方面取得了进展，但我们认为将二者考虑在一起是至关重要的，以吸引和激励更多不同类型的客户加入联邦学习，以获得高质量的全局模型。在这项研究中，我们提出了一种同时优化两种公平性的新方法。具体而言，我们提出了一个梯度和数据空间中的客户贡献评估方法。在梯度空间中，我们监测每个客户相对于其他客户的梯度方向差异。在数据空间中，我们使用辅助模型来测量客户数据的预测误差。基于这种贡献评估，我们提出了一种联邦学习方法，即通过贡献评估进行的联邦训练（FedCE），即使用估计的方式作为全局模型聚合。

    How to ensure fairness is an important topic in federated learning (FL). Recent studies have investigated how to reward clients based on their contribution (collaboration fairness), and how to achieve uniformity of performance across clients (performance fairness). Despite achieving progress on either one, we argue that it is critical to consider them together, in order to engage and motivate more diverse clients joining FL to derive a high-quality global model. In this work, we propose a novel method to optimize both types of fairness simultaneously. Specifically, we propose to estimate client contribution in gradient and data space. In gradient space, we monitor the gradient direction differences of each client with respect to others. And in data space, we measure the prediction error on client data using an auxiliary model. Based on this contribution estimation, we propose a FL method, federated training via contribution estimation (FedCE), i.e., using estimation as global model agg
    
[^55]: 在正交约束下的优化问题中的不可行确定性、随机和方差约减算法

    Infeasible Deterministic, Stochastic, and Variance-Reduction Algorithms for Optimization under Orthogonality Constraints. (arXiv:2303.16510v1 [stat.ML])

    [http://arxiv.org/abs/2303.16510](http://arxiv.org/abs/2303.16510)

    本研究提出了一种简单迭代的Landing算法，可以在不强制执行正交约束的同时顺畅地吸引到正交约束流形上。我们扩展了这种算法以支持斯托菲尔（Stiefel）流形，并提供了随机和方差约减算法，这些方法与黎曼优化算法的收敛速度相同但需要更少的计算。

    

    正交约束在许多机器学习问题中都会自然地出现，从主成分分析到鲁棒性神经网络训练。传统上，这些问题需要使用黎曼优化算法来求解，该算法在强制执行约束时最耗费时间。最近，Ablin＆Peyr\'e（2022）提出了Landing算法，这是一种廉价迭代方法，它不强制执行正交约束，但会以平滑的方式吸引到流形上。在本文中，我们为Landing算法提供了新的实用和理论发展。首先，该方法被扩展到斯托菲尔流形，即矩形正交矩阵的集合。当成本函数是许多函数的平均值时，我们还考虑随机和方差约减算法。我们证明了所有这些方法的收敛速度与它们的黎曼优化算法相同，同时需要更少的计算。

    Orthogonality constraints naturally appear in many machine learning problems, from Principal Components Analysis to robust neural network training. They are usually solved using Riemannian optimization algorithms, which minimize the objective function while enforcing the constraint. However, enforcing the orthogonality constraint can be the most time-consuming operation in such algorithms. Recently, Ablin & Peyr\'e (2022) proposed the Landing algorithm, a method with cheap iterations that does not enforce the orthogonality constraint but is attracted towards the manifold in a smooth manner. In this article, we provide new practical and theoretical developments for the landing algorithm. First, the method is extended to the Stiefel manifold, the set of rectangular orthogonal matrices. We also consider stochastic and variance reduction algorithms when the cost function is an average of many functions. We demonstrate that all these methods have the same rate of convergence as their Rieman
    
[^56]: 随机森林在多目标回归中的局部可解释性

    Local Interpretability of Random Forests for Multi-Target Regression. (arXiv:2303.16506v1 [cs.LG])

    [http://arxiv.org/abs/2303.16506](http://arxiv.org/abs/2303.16506)

    本文提出了一种针对随机森林模型在多目标回归中产生基于规则解释的技术，并在广泛实验评估中证明其具有竞争力。

    

    多目标回归在众多应用中非常有用。虽然随机森林模型在这些任务中表现良好，但往往难以解释。可解释性在机器学习中非常重要，特别是当它能直接影响人类福祉时更为重要。虽然存在针对多目标回归的模型无关解释技术，但没有特定于随机森林模型的技术。为了解决这个问题，我们提出了一种技术，用于为随机森林模型在多目标回归中产生基于规则的解释，受最近用于随机森林解释性的模型特定技术的影响。通过广泛的实验评估，证明了所提出的技术在解释方面与最先进的技术相比具有竞争力。

    Multi-target regression is useful in a plethora of applications. Although random forest models perform well in these tasks, they are often difficult to interpret. Interpretability is crucial in machine learning, especially when it can directly impact human well-being. Although model-agnostic techniques exist for multi-target regression, specific techniques tailored to random forest models are not available. To address this issue, we propose a technique that provides rule-based interpretations for instances made by a random forest model for multi-target regression, influenced by a recent model-specific technique for random forest interpretability. The proposed technique was evaluated through extensive experiments and shown to offer competitive interpretations compared to state-of-the-art techniques.
    
[^57]: 一种过参数化指数回归模型

    An Over-parameterized Exponential Regression. (arXiv:2303.16504v1 [cs.LG])

    [http://arxiv.org/abs/2303.16504](http://arxiv.org/abs/2303.16504)

    该论文介绍了一种使用指数激活函数定义的神经函数来实现过参数化指数回归模型，并提出了一种新的注意力机制。

    

    近年来，对ReLU激活函数的研究引发了许多关注，旨在通过过参数化实现神经网络收敛。然而，最近在大型语言模型 (LLM) 领域的发展引起了人们对指数激活函数的兴趣，特别是在注意力机制中的应用。在数学上，我们使用指数激活函数定义神经函数 $F: \mathbb{R}^{d \times m} \times \mathbb{R}^d \rightarrow \mathbb{R}$。给定一组带标签的数据点 $\{(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)\} \subset \mathbb{R}^d \times \mathbb{R}$，其中 $n$ 表示数据的数量。这里 $F(W(t),x)$ 可以用 $F(W(t),x) := \sum_{r=1}^m a_r \exp(\langle w_r, x \rangle)$ 表示，其中 $m$ 表示神经元的数量，$w_r(t)$ 是时间 $t$ 上的权重。固定权重 $a_r$ 在训练期间不会改变，这是文献中的标准。

    Over the past few years, there has been a significant amount of research focused on studying the ReLU activation function, with the aim of achieving neural network convergence through over-parametrization. However, recent developments in the field of Large Language Models (LLMs) have sparked interest in the use of exponential activation functions, specifically in the attention mechanism.  Mathematically, we define the neural function $F: \mathbb{R}^{d \times m} \times \mathbb{R}^d \rightarrow \mathbb{R}$ using an exponential activation function. Given a set of data points with labels $\{(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)\} \subset \mathbb{R}^d \times \mathbb{R}$ where $n$ denotes the number of the data. Here $F(W(t),x)$ can be expressed as $F(W(t),x) := \sum_{r=1}^m a_r \exp(\langle w_r, x \rangle)$, where $m$ represents the number of neurons, and $w_r(t)$ are weights at time $t$. It's standard in literature that $a_r$ are the fixed weights and it's never changed during the trai
    
[^58]: SGD类型方法的统一分析

    Unified analysis of SGD-type methods. (arXiv:2303.16502v1 [math.OC])

    [http://arxiv.org/abs/2303.16502](http://arxiv.org/abs/2303.16502)

    本文从统一分析的角度探讨了SGD类型方法，在强凸光滑优化问题中具有重要作用。

    

    本文着重探讨了从（Gorbunov et al.，2020）对于强凸光滑优化问题中SGD类型方法的统一分析方法。讨论了不同随机一阶方法分析的相似之处以及现有框架的扩展。同时提到了分析的局限性以及几种替代方法。

    This note focuses on a simple approach to the unified analysis of SGD-type methods from (Gorbunov et al., 2020) for strongly convex smooth optimization problems. The similarities in the analyses of different stochastic first-order methods are discussed along with the existing extensions of the framework. The limitations of the analysis and several alternative approaches are mentioned as well.
    
[^59]: Adam和AdamW优化器训练的深度神经网络损失函数的Lipschitz效应对泛化性能的影响

    Lipschitzness Effect of a Loss Function on Generalization Performance of Deep Neural Networks Trained by Adam and AdamW Optimizers. (arXiv:2303.16464v1 [cs.LG])

    [http://arxiv.org/abs/2303.16464](http://arxiv.org/abs/2303.16464)

    本文理论证明了损失函数的Lipschitz常数是降低Adam或AdamW获得输出模型的泛化误差的一个重要因素。本文的选择损失函数方针为Adam或AdamW优化算法的使用提供了指导。实验结果表明了Lipschitz常数较低且最大值较小的损失函数可以提高模型的泛化能力。

    

    机器学习中一个主要关注点是深度神经网络的泛化性能与优化算法之间的关系。本文证明了损失函数的Lipschitz常数是降低Adam或AdamW获得输出模型的泛化误差的一个重要因素。这些结果可作为选择损失函数时优化算法为Adam或AdamW的指导方针。本文选择了计算机视觉中的人脸年龄评估问题来评估理论界限在实际环境下的表现。为了更好地评估泛化能力，训练集和测试集从不同分布中选择。实验结果表明，Lipschitz常数较低且最大值较小的损失函数可以提高Adam或AdamW训练的模型的泛化能力。

    The generalization performance of deep neural networks with regard to the optimization algorithm is one of the major concerns in machine learning. This performance can be affected by various factors. In this paper, we theoretically prove that the Lipschitz constant of a loss function is an important factor to diminish the generalization error of the output model obtained by Adam or AdamW. The results can be used as a guideline for choosing the loss function when the optimization algorithm is Adam or AdamW. In addition, to evaluate the theoretical bound in a practical setting, we choose the human age estimation problem in computer vision. For assessing the generalization better, the training and test datasets are drawn from different distributions. Our experimental evaluation shows that the loss function with lower Lipschitz constant and maximum value improves the generalization of the model trained by Adam or AdamW.
    
[^60]: GNNBuilder：通用图神经网络加速器生成、模拟和优化的自动化框架

    GNNBuilder: An Automated Framework for Generic Graph Neural Network Accelerator Generation, Simulation, and Optimization. (arXiv:2303.16459v1 [cs.AR])

    [http://arxiv.org/abs/2303.16459](http://arxiv.org/abs/2303.16459)

    GNNBuilder是一个自动化的、通用的、端到端的GNN加速器生成框架，它可以为用户任意定义的广泛的GNN模型自动生成加速器，且运行速度比软件基准快多达12.95倍。

    

    目前有很多图神经网络（GNN）加速器被提出。然而，它们高度依赖于用户的硬件专业知识，并且通常针对一种特定的GNN模型进行优化，使它们难以实际使用。因此，在这项工作中，我们提出了GNNBuilder，这是第一个自动化的、通用的、端到端的GNN加速器生成框架。它具有四个优点：（1）GNNBuilder可以自动为用户任意定义的广泛的GNN模型生成GNN加速器；（2）GNNBuilder采用标准的PyTorch编程接口，为算法开发人员提供零开销；（3）GNNBuilder支持端到端的代码生成、模拟、加速器优化和硬件部署，实现了GNN加速器设计的一键式操作；（4）GNNBuilder配备了其所生成的加速器的准确性能模型，使得设计空间探索（DSE）快速而灵活。在实验中，首先我们展示了我们的加速器在6个基准数据集上与最先进的GNN加速器相比表现出色，运行速度比软件基准快了多达12.95倍。其次，对不规则图处理的案例研究展示了GNNBuilder的出色可扩展性和适应性。最后，在单个GPU的服务器上，我们对400个GNN模型进行了30分钟的DSE研究，验证了GNNBuilder的效率和有效性。

    There are plenty of graph neural network (GNN) accelerators being proposed. However, they highly rely on users' hardware expertise and are usually optimized for one specific GNN model, making them challenging for practical use . Therefore, in this work, we propose GNNBuilder, the first automated, generic, end-to-end GNN accelerator generation framework. It features four advantages: (1) GNNBuilder can automatically generate GNN accelerators for a wide range of GNN models arbitrarily defined by users; (2) GNNBuilder takes standard PyTorch programming interface, introducing zero overhead for algorithm developers; (3) GNNBuilder supports end-to-end code generation, simulation, accelerator optimization, and hardware deployment, realizing a push-button fashion for GNN accelerator design; (4) GNNBuilder is equipped with accurate performance models of its generated accelerator, enabling fast and flexible design space exploration (DSE). In the experiments, first, we show that our accelerator pe
    
[^61]: 何时预训练图神经网络？基于数据生成视角的回答！

    When to Pre-Train Graph Neural Networks? An Answer from Data Generation Perspective!. (arXiv:2303.16458v1 [cs.LG])

    [http://arxiv.org/abs/2303.16458](http://arxiv.org/abs/2303.16458)

    本文提出了一个通用框架W2PGNN，旨在回答何时预训练图神经网络的关键问题。使用新的角度探索了从预训练数据到下游数据的复杂生成机制。

    

    最近，图预训练在学术界引起了广泛关注，旨在从未标记的图数据中学习可转移知识，以提高下游性能。尽管最近的尝试，但负面迁移是将图预训练模型应用于下游任务时的重大问题。现有工作通过设计多种图预训练和微调策略，致力于解决何时预训练和如何预训练的问题。然而，有时候无论策略如何先进，“预训练和微调”范式仍然无法带来明显的好处。本文引入了一个通用框架W2PGNN来回答何时预训练的关键问题（即我们在什么情况下可以利用图预训练），然后再进行费力的预训练或微调。我们从一个新的角度探索了从预训练数据到下游数据的复杂生成机制。

    Recently, graph pre-training has attracted wide research attention, which aims to learn transferable knowledge from unlabeled graph data so as to improve downstream performance. Despite these recent attempts, the negative transfer is a major issue when applying graph pre-trained models to downstream tasks. Existing works made great efforts on the issue of what to pre-train and how to pre-train by designing a number of graph pre-training and fine-tuning strategies. However, there are indeed cases where no matter how advanced the strategy is, the "pre-train and fine-tune" paradigm still cannot achieve clear benefits. This paper introduces a generic framework W2PGNN to answer the crucial question of when to pre-train (i.e., in what situations could we take advantage of graph pre-training) before performing effortful pre-training or fine-tuning. We start from a new perspective to explore the complex generative mechanisms from the pre-training data to downstream data. In particular, W2PGNN 
    
[^62]: 混合最小二乘深度神经网络在内部测量下的导电率成像

    Conductivity Imaging from Internal Measurements with Mixed Least-Squares Deep Neural Networks. (arXiv:2303.16454v1 [math.NA])

    [http://arxiv.org/abs/2303.16454](http://arxiv.org/abs/2303.16454)

    本论文提出了一种新的通过深度神经网络从内部测量中重构椭圆问题中的导电率分布的方法，并在连续和经验损失的神经网络逼近上进行了深入的分析，展示了该方法对于数据噪声的优异稳定性以及解决高维问题的能力。

    

    在这项工作中，我们使用深度神经网络开发了一种新的方法，从一个内部测量中重构椭圆问题中的导电率分布。该方法基于控制方程的混合改造，并利用标准的最小二乘目标函数同时近似导电率和通量，以深度神经网络作为试探函数。我们对连续和经验损失的神经网络逼近进行了深入的分析，包括通过噪声水平、各种惩罚参数和神经网络结构参数（深度、宽度和参数边界）显式地估计误差的严格估计。我们还进行了广泛的数值实验，以展示该方法的不同特点，例如对于数据噪声的优异稳定性以及解决高维问题的能力。

    In this work we develop a novel approach using deep neural networks to reconstruct the conductivity distribution in elliptic problems from one internal measurement. The approach is based on a mixed reformulation of the governing equation and utilizes the standard least-squares objective to approximate the conductivity and flux simultaneously, with deep neural networks as ansatz functions. We provide a thorough analysis of the neural network approximations for both continuous and empirical losses, including rigorous error estimates that are explicit in terms of the noise level, various penalty parameters and neural network architectural parameters (depth, width and parameter bound). We also provide extensive numerical experiments in two- and multi-dimensions to illustrate distinct features of the approach, e.g., excellent stability with respect to data noise and capability of solving high-dimensional problems.
    
[^63]: ProtFIM：基于蛋白质语言模型的填充中间蛋白序列设计

    ProtFIM: Fill-in-Middle Protein Sequence Design via Protein Language Models. (arXiv:2303.16452v1 [cs.LG])

    [http://arxiv.org/abs/2303.16452](http://arxiv.org/abs/2303.16452)

    本论文介绍了一种基于蛋白质语言模型的填充中间蛋白序列设计方法，并设计了新的基准测试SEIFER，揭示了现有语言模型的弱点，证明了使用填充中间变换训练的语言模型(ProtFIM)，更适合于蛋白质工程。

    

    基于蛋白质序列的因果语言建模，蛋白质语言模型(PLMs)已成为一种有前途的蛋白质序列设计工具。但是在实际的蛋白质工程中，有许多情况是在保持其它残基的情况下优化蛋白质中间的氨基酸。由于PLMs的从左到右的性质，现有的PLMs通过促使前缀残基来修改后缀残基，这对于考虑整个周围背景的填充任务是不足够的。为了找到更有效的PLMs用于蛋白质工程，我们设计了一个新的基准测试——二级结构中间填充——SEIFER，它近似于填充序列设计场景。通过对现有模型在基准测试中的评估，我们揭示了现有语言模型的弱点，证明了通过填充中间变换训练的语言模型(称为ProtFIM)，更适合于蛋白质工程。此外，我们证明了ProtFIM生成的蛋白质序列的中间部分，具有更准确的结构特征。

    Protein language models (pLMs), pre-trained via causal language modeling on protein sequences, have been a promising tool for protein sequence design. In real-world protein engineering, there are many cases where the amino acids in the middle of a protein sequence are optimized while maintaining other residues. Unfortunately, because of the left-to-right nature of pLMs, existing pLMs modify suffix residues by prompting prefix residues, which are insufficient for the infilling task that considers the whole surrounding context. To find the more effective pLMs for protein engineering, we design a new benchmark, Secondary structureE InFilling rEcoveRy, SEIFER, which approximates infilling sequence design scenarios. With the evaluation of existing models on the benchmark, we reveal the weakness of existing language models and show that language models trained via fill-in-middle transformation, called ProtFIM, are more appropriate for protein engineering. Also, we prove that ProtFIM generate
    
[^64]: ProductAE：面向大维数的深度学习驱动纠错码设计

    ProductAE: Toward Deep Learning Driven Error-Correction Codes of Large Dimensions. (arXiv:2303.16424v1 [cs.IT])

    [http://arxiv.org/abs/2303.16424](http://arxiv.org/abs/2303.16424)

    本文提出了Product Autoencoder来通过可管理的训练复杂性实现相对较大的代码的训练。

    

    尽管几十年的理论研究已经发明了几个纠错码类别，但这些码的设计却是一项极具挑战性的任务，主要依靠人类智慧。最近的研究表明，这样的设计可以通过机器学习（ML）工具有效地自动化和加速，从而实现与经典设计相比具有良好性能增益的ML驱动的纠错码类别。然而，一个根本性的挑战是，对于大码维度来说，设计和训练完全的ML驱动的编码器和解码器对来说是非常复杂的，如果不是不可能的的话。在本文中，我们提出了Product Autoencoder（ProductAE）-一种计算效率高的深度学习驱动（编码器，解码器）对的系列-旨在通过可管理的训练复杂性实现相对较大的代码（编码器和解码器）的训练。我们借鉴了经典乘积码的思想，并提出了使用ProductAE构建大型神经网络的编码技术。

    While decades of theoretical research have led to the invention of several classes of error-correction codes, the design of such codes is an extremely challenging task, mostly driven by human ingenuity. Recent studies demonstrate that such designs can be effectively automated and accelerated via tools from machine learning (ML), thus enabling ML-driven classes of error-correction codes with promising performance gains compared to classical designs. A fundamental challenge, however, is that it is prohibitively complex, if not impossible, to design and train fully ML-driven encoder and decoder pairs for large code dimensions. In this paper, we propose Product Autoencoder (ProductAE) -- a computationally-efficient family of deep learning driven (encoder, decoder) pairs -- aimed at enabling the training of relatively large codes (both encoder and decoder) with a manageable training complexity. We build upon ideas from classical product codes and propose constructing large neural codes usin
    
[^65]: 深度学习在乳腺X光检查中的问题和快捷方式

    Problems and shortcuts in deep learning for screening mammography. (arXiv:2303.16417v1 [cs.CV])

    [http://arxiv.org/abs/2303.16417](http://arxiv.org/abs/2303.16417)

    本研究发现在乳腺X光检查中，深度学习模型存在问题和快捷方式，建议利用采样方式平衡训练数据，并对相应的评估问题提出了解决方案

    

    本研究揭示了深度学习模型在性能和泛化能力方面未被发现的挑战。我们（1）识别出可能会提高性能的问题以及评估问题，并（2）提出了训练和分析方法来解决这些问题。我们利用120,112个美国乳腺检查和16,693个英国乳腺检查进行了癌症分类的AI模型训练和评估。我们发现，一个仅使用无乳房的图像进行训练的模型在特定测试集上有较高的性能表现。我们还发现，AUC分数的悖论现象出现在了两个国家的测试集上，（0.838和0.892），但是在联合测试集上AUC分数高达0.945。通过在训练过程中等量采样两个数据集中的癌症数据可以减轻这种情况。

    This work reveals undiscovered challenges in the performance and generalizability of deep learning models. We (1) identify spurious shortcuts and evaluation issues that can inflate performance and (2) propose training and analysis methods to address them.  We trained an AI model to classify cancer on a retrospective dataset of 120,112 US exams (3,467 cancers) acquired from 2008 to 2017 and 16,693 UK exams (5,655 cancers) acquired from 2011 to 2015.  We evaluated on a screening mammography test set of 11,593 US exams (102 cancers; 7,594 women; age 57.1 \pm 11.0) and 1,880 UK exams (590 cancers; 1,745 women; age 63.3 \pm 7.2). A model trained on images of only view markers (no breast) achieved a 0.691 AUC. The original model trained on both datasets achieved a 0.945 AUC on the combined US+UK dataset but paradoxically only 0.838 and 0.892 on the US and UK datasets, respectively. Sampling cancers equally from both datasets during training mitigated this shortcut. A similar AUC paradox (0.9
    
[^66]: 一种用于预测先进材料多种特性的综合多模态深度学习方法

    A Comprehensive and Versatile Multimodal Deep Learning Approach for Predicting Diverse Properties of Advanced Materials. (arXiv:2303.16412v1 [cond-mat.soft])

    [http://arxiv.org/abs/2303.16412](http://arxiv.org/abs/2303.16412)

    本文提出了一种多模态深度学习方法，利用物理属性和化学数据预测先进材料的特性，在处理高维信息空间方面具有很大优势，成功预测了114210种组成条件下的913680个属性数据点，该方法适用于不同材料和尺度。

    

    我们提出了一种多模态深度学习（MDL）框架，通过合并物理属性和化学数据来预测10维丙烯酸聚合物复合材料的物理特性。我们的MDL模型包括四个模块，其中三个模块是用于材料结构表征的生成式深度学习模型，第四个模块用于属性预测。我们的方法处理了18维的复杂性，包括10个组成输入和8个属性输出，成功预测了114210种组成条件下的913680个属性数据点。对于计算材料科学特别是对于结构未定义的材料，这种复杂性是前所未有的。我们提出了一个框架来分析高维信息空间，以实现逆向材料设计，展示了对各种材料和尺度的灵活性和适应性，前提是有足够的数据。这项研究推动了未来研究不同材料和开发更复杂的MDL模型。

    We present a multimodal deep learning (MDL) framework for predicting physical properties of a 10-dimensional acrylic polymer composite material by merging physical attributes and chemical data. Our MDL model comprises four modules, including three generative deep learning models for material structure characterization and a fourth model for property prediction. Our approach handles an 18-dimensional complexity, with 10 compositional inputs and 8 property outputs, successfully predicting 913,680 property data points across 114,210 composition conditions. This level of complexity is unprecedented in computational materials science, particularly for materials with undefined structures. We propose a framework to analyze the high-dimensional information space for inverse material design, demonstrating flexibility and adaptability to various materials and scales, provided sufficient data is available. This study advances future research on different materials and the development of more soph
    
[^67]: LMDA-Net: 用于通用EEG脑机接口范例的轻量级多维注意力网络及其可解释性

    LMDA-Net:A lightweight multi-dimensional attention network for general EEG-based brain-computer interface paradigms and interpretability. (arXiv:2303.16407v1 [cs.LG])

    [http://arxiv.org/abs/2303.16407](http://arxiv.org/abs/2303.16407)

    本论文提出了一种名为LMDA-Net的轻量级多维注意力网络，旨在提高通用EEG脑机接口范例的分类性能。该网络使用两个新型注意力模块——通道注意力模块和深度注意力模块，并在多个公开高影响力数据集上进行了评估。实验结果表明，LMDA-Net表现优于其他代表模型，实现了最先进的性能，并展示了其决策过程的高可解释性。

    

    基于EEG的活动和状态识别需要使用神经科学先前知识生成定量EEG特征，这可能限制BCI的性能。虽然基于神经网络的方法可以有效地提取特征，但它们经常会遇到诸如跨数据集的通用性差、高预测波动性和低模型可解释性等问题。因此，我们提出了一种新的轻量级多维注意力网络，称为LMDA-Net。通过结合两个专为EEG信号设计的新型注意力模块——通道注意力模块和深度注意力模块，LMDA-Net可以有效地整合来自多个维度的特征，从而提高了在各种BCI任务上的分类性能。 LMDA-Net在包括运动想象（MI）和P300-Speller范例在内的四个公开高影响力数据集上进行了评估，并与其他代表模型进行了比较。实验结果表明，LMDA-Net表现优于其他代表模型，实现了最先进的性能，并展示了其决策过程的高可解释性。

    EEG-based recognition of activities and states involves the use of prior neuroscience knowledge to generate quantitative EEG features, which may limit BCI performance. Although neural network-based methods can effectively extract features, they often encounter issues such as poor generalization across datasets, high predicting volatility, and low model interpretability. Hence, we propose a novel lightweight multi-dimensional attention network, called LMDA-Net. By incorporating two novel attention modules designed specifically for EEG signals, the channel attention module and the depth attention module, LMDA-Net can effectively integrate features from multiple dimensions, resulting in improved classification performance across various BCI tasks. LMDA-Net was evaluated on four high-impact public datasets, including motor imagery (MI) and P300-Speller paradigms, and was compared with other representative models. The experimental results demonstrate that LMDA-Net outperforms other represen
    
[^68]: 层次化视频瞬间检索和分步字幕

    Hierarchical Video-Moment Retrieval and Step-Captioning. (arXiv:2303.16406v1 [cs.CV])

    [http://arxiv.org/abs/2303.16406](http://arxiv.org/abs/2303.16406)

    这篇论文提出了HiREST数据集和一个新的基准，将分层信息检索和视觉/文本逐步总结从教学视频语料库中推广，使得在一个端到端的设置下可以共同搜索视频语料库，并生成摘要。

    

    目前人们在寻找大型视频语料库中的信息方面越来越感兴趣。先前的工作独立研究了相关任务，如基于文本的视频检索、瞬间检索、视频摘要和视频字幕，没有一个端到端的设置可以共同搜索视频语料库，并生成摘要。这样的端到端设置将允许许多有趣的应用程序，例如基于文本的搜索，从视频语料库中找到相关的视频，提取最相关的瞬间，并将瞬间分成重要的步骤，并加上字幕。为了解决这个问题，我们提出了HiREST(Hierarchical REtrieval and STep-captioning)数据集，并提出了一个新的基准，涵盖了来自教学视频语料库的分层信息检索和视觉/文本分阶段总结。HiREST由来自教学视频数据集的3.4K个文本-视频对组成，其中1.1K个视频具有与文本查询相关的瞬间跨度注释和细分。

    There is growing interest in searching for information from large video corpora. Prior works have studied relevant tasks, such as text-based video retrieval, moment retrieval, video summarization, and video captioning in isolation, without an end-to-end setup that can jointly search from video corpora and generate summaries. Such an end-to-end setup would allow for many interesting applications, e.g., a text-based search that finds a relevant video from a video corpus, extracts the most relevant moment from that video, and segments the moment into important steps with captions. To address this, we present the HiREST (HIerarchical REtrieval and STep-captioning) dataset and propose a new benchmark that covers hierarchical information retrieval and visual/textual stepwise summarization from an instructional video corpus. HiREST consists of 3.4K text-video pairs from an instructional video dataset, where 1.1K videos have annotations of moment spans relevant to text query and breakdown of e
    
[^69]: 利用连接车辆轨迹数据评估超速驾驶影响

    Using Connected Vehicle Trajectory Data to Evaluate the Effects of Speeding. (arXiv:2303.16396v1 [cs.LG])

    [http://arxiv.org/abs/2303.16396](http://arxiv.org/abs/2303.16396)

    本研究利用连接车辆轨迹数据探究了道路特征对个人超速行为的影响，建立模型预测超速比例。研究结果可为制定更有效的速度管理策略提供参考。

    

    超速驾驶是交通事故中的主要因素之一。各个交通部门已提出速度管理策略，以减少主干道上的超速行驶。虽然已有许多关于分析超速比例的研究，但很少研究考虑了超速行为对个人旅程的影响。许多研究所利用的探测器速度数据有限，无法了解驾驶员所采取的路径信息。本研究旨在探究给定旅程中个人经历的各种道路特征对超速比例的影响。利用连接车辆轨迹数据确定驾驶员所采取的路径，以及与车辆相关的变量。使用多个学习模型预测超速比例的水平。表现最佳的模型为极端梯度提升，准确度为0.756。我们的研究有助于深入了解道路特征对个人超速行为的影响，并为制定更有效的速度管理策略提供参考。

    Speeding has been and continues to be a major contributing factor to traffic fatalities. Various transportation agencies have proposed speed management strategies to reduce the amount of speeding on arterials. While there have been various studies done on the analysis of speeding proportions above the speed limit, few studies have considered the effect on the individual's journey. Many studies utilized speed data from detectors, which is limited in that there is no information of the route that the driver took. This study aims to explore the effects of various roadway features an individual experiences for a given journey on speeding proportions. Connected vehicle trajectory data was utilized to identify the path that a driver took, along with the vehicle related variables. The level of speeding proportion is predicted using multiple learning models. The model with the best performance, Extreme Gradient Boosting, achieved an accuracy of 0.756. The proposed model can be used to understa
    
[^70]: 数据驱动的解释是否对越界数据具有稳健性？

    Are Data-driven Explanations Robust against Out-of-distribution Data?. (arXiv:2303.16390v1 [cs.LG])

    [http://arxiv.org/abs/2303.16390](http://arxiv.org/abs/2303.16390)

    随着黑盒模型在高风险应用中越来越被广泛应用，各种数据驱动的解释方法应运而生，但是，实证结果表明，即使模型预测正确，也可能在分布转移下产生不可靠的解释。为了解决这个问题，我们提出了一个模型无关的解释学习框架：分布稳健解释(DRE)，它可以在越界数据的情况下提供强健的解释信号，并提高模型的泛化能力。

    

    随着黑盒模型越来越多地应用于高风险应用，引入了各种数据驱动的解释方法。与此同时，机器学习模型不断受到分布转移的挑战。自然而然地出现了一个问题：数据驱动的解释是否对越界数据具有稳健性？我们的实证结果表明，即使模型预测正确，也可能在分布转移下产生不可靠的解释。如何开发针对越界数据的强健解释？为了解决这个问题，我们提出了一个端到端的模型无关学习框架：分布式稳健解释(DRE)。关键思路是受到自监督学习的启发，充分利用相互分布的信息，为解释的学习提供监督信号，无需人类注释。强健的解释能否提高模型的泛化能力？我们在广泛的任务和数据类型上进行了大量实验。

    As black-box models increasingly power high-stakes applications, a variety of data-driven explanation methods have been introduced. Meanwhile, machine learning models are constantly challenged by distributional shifts. A question naturally arises: Are data-driven explanations robust against out-of-distribution data? Our empirical results show that even though predict correctly, the model might still yield unreliable explanations under distributional shifts. How to develop robust explanations against out-of-distribution data? To address this problem, we propose an end-to-end model-agnostic learning framework Distributionally Robust Explanations (DRE). The key idea is, inspired by self-supervised learning, to fully utilizes the inter-distribution information to provide supervisory signals for the learning of explanations without human annotation. Can robust explanations benefit the model's generalization capability? We conduct extensive experiments on a wide range of tasks and data types
    
[^71]: 适用于异质多壳扩散加权MRI估计纤维定向分布函数的单阶段学习模型

    A Unified Single-stage Learning Model for Estimating Fiber Orientation Distribution Functions on Heterogeneous Multi-shell Diffusion-weighted MRI. (arXiv:2303.16376v1 [cs.LG])

    [http://arxiv.org/abs/2303.16376](http://arxiv.org/abs/2303.16376)

    该论文提出了一种适用于异质多壳扩散加权MRI估计纤维定向分布函数的单阶段学习模型，使用深度学习技术可以提高推理速度和扫描一致性。

    

    扩散加权MRI通过其在q空间中的频谱测量每个体素中本地扩散过程的方向和尺度，通常在一个或多个shell中获取。 最近在微结构成像和多组织分解方面的发展引起了人们对信号的径向b值依赖性的关注。 因此，在组织分类和微观结构估计方面，需要扩展径向和角向域的信号表示。 提出了多种方法，可以模拟DW-MRI信号与生物微结构之间的非线性关系。 在过去几年中，许多基于深度学习的方法已经开发出来，相比传统的基于模型的方法（例如多壳多组织约束球形去卷积）具有更快的推理速度和更高的扫描一致性。 然而，由于学习过程在很大程度上依赖于扩散过程和组织微结构的先验信息，因此通常需要多阶段的学习策略。

    Diffusion-weighted (DW) MRI measures the direction and scale of the local diffusion process in every voxel through its spectrum in q-space, typically acquired in one or more shells. Recent developments in micro-structure imaging and multi-tissue decomposition have sparked renewed attention to the radial b-value dependence of the signal. Applications in tissue classification and micro-architecture estimation, therefore, require a signal representation that extends over the radial as well as angular domain. Multiple approaches have been proposed that can model the non-linear relationship between the DW-MRI signal and biological microstructure. In the past few years, many deep learning-based methods have been developed towards faster inference speed and higher inter-scan consistency compared with traditional model-based methods (e.g., multi-shell multi-tissue constrained spherical deconvolution). However, a multi-stage learning strategy is typically required since the learning process rel
    
[^72]: 训练数据重构的非渐进性下界

    Non-Asymptotic Lower Bounds For Training Data Reconstruction. (arXiv:2303.16372v1 [cs.LG])

    [http://arxiv.org/abs/2303.16372](http://arxiv.org/abs/2303.16372)

    本文通过研究差分隐私和度量隐私学习器在对抗者重构错误方面的鲁棒性，得出了非渐进性下界，覆盖了高维情况，且扩展了深度学习算法的隐私分析

    

    本文研究了专业对手进行训练数据重构攻击时私有学习算法的语义保证强度。我们通过导出非渐进量级下界来研究了满足差分隐私（DP）和度量隐私（mDP）的学习器对抗者重构错误的鲁棒性。此外，我们还证明了我们对mDP的分析覆盖了高维情况。本文进一步对流行的深度学习算法，如DP-SGD和Projected Noisy SGD进行了度量差分隐私的扩展隐私分析。

    We investigate semantic guarantees of private learning algorithms for their resilience to training Data Reconstruction Attacks (DRAs) by informed adversaries. To this end, we derive non-asymptotic minimax lower bounds on the adversary's reconstruction error against learners that satisfy differential privacy (DP) and metric differential privacy (mDP). Furthermore, we demonstrate that our lower bound analysis for the latter also covers the high dimensional regime, wherein, the input data dimensionality may be larger than the adversary's query budget. Motivated by the theoretical improvements conferred by metric DP, we extend the privacy analysis of popular deep learning algorithms such as DP-SGD and Projected Noisy SGD to cover the broader notion of metric differential privacy.
    
[^73]: 隐藏信息下基于极大似然的状态空间模型平滑估计方法研究

    Maximum likelihood smoothing estimation in state-space models: An incomplete-information based approach. (arXiv:2303.16364v1 [stat.ME])

    [http://arxiv.org/abs/2303.16364](http://arxiv.org/abs/2303.16364)

    本文提出了一种新方法可以从随机状态空间系统的不完整信息/数据中进行极大似然（ML）平滑估计，所得到的估计结果比传统方法精度更高。算法基于EM梯度粒子算法，递归地进行估计。

    

    本文重新审视了Rauch （1963年）和et al.（1965年）的经典作品，提出了一种从随机状态空间系统的不完整信息/数据中进行极大似然（ML）平滑估计的新方法。介绍了不完整数据的得分函数和条件观测信息矩阵，并建立了它们的分布式身份。利用这些身份，提出了ML smoother $\widehat{x}_{k\vert n}^s =\argmax_{x_k} \log f(x_k,\widehat{x}_{k+1\vert n}^s, y_{0:n}\vert\theta)$，$k\leq n-1$。结果表明，和ML状态估计$\widehat{x}_k=\argmax_{x_k} \log f(x_k,y_{0:k}\vert\theta)$相比，ML平滑估计器给出的状态$x_k$估计具有更少的标准误差，并且更符合对数似然。zhi。基于EM梯度粒子算法给出递归估计，该算法扩展了\cite{Lange}的ML平滑估计工作。该算法具有明确的迭代更新。

    This paper revisits classical works of Rauch (1963, et al. 1965) and develops a novel method for maximum likelihood (ML) smoothing estimation from incomplete information/data of stochastic state-space systems. Score function and conditional observed information matrices of incomplete data are introduced and their distributional identities are established. Using these identities, the ML smoother $\widehat{x}_{k\vert n}^s =\argmax_{x_k} \log f(x_k,\widehat{x}_{k+1\vert n}^s, y_{0:n}\vert\theta)$, $k\leq n-1$, is presented. The result shows that the ML smoother gives an estimate of state $x_k$ with more adherence of loglikehood having less standard errors than that of the ML state estimator $\widehat{x}_k=\argmax_{x_k} \log f(x_k,y_{0:k}\vert\theta)$, with $\widehat{x}_{n\vert n}^s=\widehat{x}_n$. Recursive estimation is given in terms of an EM-gradient-particle algorithm which extends the work of \cite{Lange} for ML smoothing estimation. The algorithm has an explicit iteration update whi
    
[^74]: 用现有机器学习工具以超过99%的准确度区分ChatGPT和学术科学家的作者身份

    ChatGPT or academic scientist? Distinguishing authorship with over 99% accuracy using off-the-shelf machine learning tools. (arXiv:2303.16352v1 [cs.LG])

    [http://arxiv.org/abs/2303.16352](http://arxiv.org/abs/2303.16352)

    该论文设计了一种用于区分ChatGPT生成文本和学术科学家撰写文本的方法，并在2.5k个样本上测试，实现了99.2%的准确率。

    

    ChatGPT使广大群众能够访问AI生成的文章，在短短几个月内，这个产品颠覆了知识经济，引发了人们工作、学习和写作方式的文化变革。现在区分人类写作和AI写作的需求变得至关重要和紧迫，特别是在高等教育和学术写作等领域，AI在之前不曾是一个重要的威胁或者贡献者。针对这一需求，我们开发了一种区分ChatGPT生成文本和（人类）学术科学家撰写文本的方法，依靠普及且易于获取的监督分类方法。我们的重点在于，学术科学家这一特定人群的写作与ChatGPT有何不同，这种有针对性的方法导致我们发现了用于区分（这些）人类和AI的新特征。例如，科学家经常写长段落，喜欢使用模糊语言，经常使用but、however和although等词汇。我们训练和测试了这种方法，使用了2.5k个样本，成功实现了99.2%的准确率，区分ChatGPT和学术科学家的写作。

    ChatGPT has enabled access to AI-generated writing for the masses, and within just a few months, this product has disrupted the knowledge economy, initiating a culture shift in the way people work, learn, and write. The need to discriminate human writing from AI is now both critical and urgent, particularly in domains like higher education and academic writing, where AI had not been a significant threat or contributor to authorship. Addressing this need, we developed a method for discriminating text generated by ChatGPT from (human) academic scientists, relying on prevalent and accessible supervised classification methods. We focused on how a particular group of humans, academic scientists, write differently than ChatGPT, and this targeted approach led to the discovery of new features for discriminating (these) humans from AI; as examples, scientists write long paragraphs and have a penchant for equivocal language, frequently using words like but, however, and although. With a set of 2
    
[^75]: 面部识别技术可以从面部图像中显示政治取向，即使控制社会人口统计和自我表现。(arXiv: 2303.16343v1 [cs.CV])

    Facial recognition technology can expose political orientation from facial images even when controlling for demographics and self-presentation. (arXiv:2303.16343v1 [cs.CV])

    [http://arxiv.org/abs/2303.16343](http://arxiv.org/abs/2303.16343)

    本研究使用面部识别技术，通过特定的面部特征发现了人们的政治取向，甚至可以从自然图像中推广。这种预测的精度比人类评分者高，相当于一些工作面试的预测效果。

    

    本论文运用面部识别算法，从实验室设置下拍摄的591张中性面部图像中提取面部描述符。在控制年龄、性别和种族的情况下，通过交叉验证的线性回归模型来预测参与者在政治取向量表上的得分(Cronbach的α=0.94)。模型的性能超过了r = 0.20，远优于人类评分者，与工作面试预测工作成功、酒精驱动攻击性或心理治疗改善心理健康的效果相当。此外，从标准化图像衍生出的模型在3,401名来自美国、英国和加拿大的政治人物的自然图像样本中表现良好(r = 0.12)，表明面部外貌和政治取向之间的关联可推广到我们之外的人群。面部特征与政治取向相关的分析发现，保守派的下半脸部分更大，虽然政治取向不能准确地预测个体面部特征的所有变化，但是这种发现还是富有启发性的。

    A facial recognition algorithm was used to extract face descriptors from carefully standardized images of 591 neutral faces taken in the laboratory setting. Face descriptors were entered into a cross-validated linear regression to predict participants' scores on a political orientation scale (Cronbach's alpha=.94) while controlling for age, gender, and ethnicity. The model's performance exceeded r=.20: much better than that of human raters and on par with how well job interviews predict job success, alcohol drives aggressiveness, or psychological therapy improves mental health. Moreover, the model derived from standardized images performed well (r=.12) in a sample of naturalistic images of 3,401 politicians from the U.S., UK, and Canada, suggesting that the associations between facial appearance and political orientation generalize beyond our sample. The analysis of facial features associated with political orientation revealed that conservatives had larger lower faces, although politi
    
[^76]: 关于流式联邦学习中本地缓存更新规则的研究

    On the Local Cache Update Rules in Streaming Federated Learning. (arXiv:2303.16340v1 [cs.LG])

    [http://arxiv.org/abs/2303.16340](http://arxiv.org/abs/2303.16340)

    本文提出了三种本地缓存更新规则来管理动态数据分布和有限的缓存容量，以应对流式联邦学习中本地训练数据集与长期数据分布之间的差异。我们还推导出了该算法的收敛界限。我们在两个数据集上进行了测试，结果表明我们的算法效果良好。

    

    本研究针对流式联邦学习（SFL）领域，提出了本地缓存更新规则，以管理动态数据分布和有限的缓存容量。传统的联邦学习依赖于固定的数据集，而在SFL中，数据是流式的，并且其分布随时间而变化，导致本地训练数据集与长期分布之间存在差异。为了解决这个问题，我们提出了三个本地缓存更新规则——先进先出（FIFO）、静态比例选择性替换（SRSR）和动态比例选择性替换（DRSR）——在考虑有限的缓存容量的同时更新每个客户端的本地缓存。此外，我们还推导出了基于长期数据分布和客户端本地训练数据集之间分布不一致度的收敛界限。然后，我们在两个数据集上评估了我们提出的算法：网络流量分类数据集和图像分类数据集。

    In this study, we address the emerging field of Streaming Federated Learning (SFL) and propose local cache update rules to manage dynamic data distributions and limited cache capacity. Traditional federated learning relies on fixed data sets, whereas in SFL, data is streamed, and its distribution changes over time, leading to discrepancies between the local training dataset and long-term distribution. To mitigate this problem, we propose three local cache update rules - First-In-First-Out (FIFO), Static Ratio Selective Replacement (SRSR), and Dynamic Ratio Selective Replacement (DRSR) - that update the local cache of each client while considering the limited cache capacity. Furthermore, we derive a convergence bound for our proposed SFL algorithm as a function of the distribution discrepancy between the long-term data distribution and the client's local training dataset. We then evaluate our proposed algorithm on two datasets: a network traffic classification dataset and an image class
    
[^77]: PCA-Net：操作学习的复杂性上下界

    Operator learning with PCA-Net: upper and lower complexity bounds. (arXiv:2303.16317v1 [cs.LG])

    [http://arxiv.org/abs/2303.16317](http://arxiv.org/abs/2303.16317)

    本文发展了PCA-Net的近似理论，得出了通用逼近结果，并识别出了使用PCA-Net进行高效操作学习的潜在障碍：输出分布的复杂性和算子空间的内在复杂性。

    

    神经算子在计算科学和工程中备受关注。PCA-Net是一种最近提出的神经算子架构，它将主成分分析(PCA)与神经网络相结合，以逼近潜在的算子。本文对这种方法进行了近似理论的发展，改进并显着扩展了此方向的以前的工作。在定性界限方面，本文得出了新颖的通用逼近结果，在对潜在算子和数据生成分布的最小假设的前提下。在定量限制方面，本文识别了使用PCA-Net进行高效操作学习的两个潜在障碍，通过导出下界进行了严格证明，第一个障碍与输出分布的复杂性有关，由PCA特征值的缓慢衰减来衡量；另一个障碍涉及无限维输入和输出空间之间的算子空间的内在复杂性。

    Neural operators are gaining attention in computational science and engineering. PCA-Net is a recently proposed neural operator architecture which combines principal component analysis (PCA) with neural networks to approximate an underlying operator. The present work develops approximation theory for this approach, improving and significantly extending previous work in this direction. In terms of qualitative bounds, this paper derives a novel universal approximation result, under minimal assumptions on the underlying operator and the data-generating distribution. In terms of quantitative bounds, two potential obstacles to efficient operator learning with PCA-Net are identified, and made rigorous through the derivation of lower complexity bounds; the first relates to the complexity of the output distribution, measured by a slow decay of the PCA eigenvalues. The other obstacle relates the inherent complexity of the space of operators between infinite-dimensional input and output spaces, 
    
[^78]: 机器学习和深度学习应用于犯罪预测：系统综述与未来方向

    Crime Prediction Using Machine Learning and Deep Learning: A Systematic Review and Future Directions. (arXiv:2303.16310v1 [cs.LG])

    [http://arxiv.org/abs/2303.16310](http://arxiv.org/abs/2303.16310)

    该综述文章调查了超过150篇文章，探讨了各种机器学习和深度学习算法在预测犯罪方面的应用，为研究人员提供有价值的参考资料。

    

    预测犯罪在最近几年引起了研究人员的相当关注，主要关注识别犯罪事件中的模式和趋势。本综述文章调查了超过150篇文章，探讨了各种机器学习和深度学习算法应用于预测犯罪的情况。该研究提供了研究人员用于犯罪预测的数据集，并分析了应用于预测犯罪的机器学习和深度学习算法的杰出方法，提供了有关犯罪活动相关不同趋势和因素的见解。此外，本文还强调了可能存在的差距和未来方向，以增强犯罪预测的准确性。最后，本文介绍的关于应用机器学习和深度学习方法预测犯罪的研究综述，为这一领域的研究人员提供了有价值的参考资料。

    Predicting crime using machine learning and deep learning techniques has gained considerable attention from researchers in recent years, focusing on identifying patterns and trends in crime occurrences. This review paper examines over 150 articles to explore the various machine learning and deep learning algorithms applied to predict crime. The study provides access to the datasets used for crime prediction by researchers and analyzes prominent approaches applied in machine learning and deep learning algorithms to predict crime, offering insights into different trends and factors related to criminal activities. Additionally, the paper highlights potential gaps and future directions that can enhance the accuracy of crime prediction. Finally, the comprehensive overview of research discussed in this paper on crime prediction using machine learning and deep learning approaches serves as a valuable reference for researchers in this field. By gaining a deeper understanding of crime predictio
    
[^79]: 滑动窗口流模型的可证明稳健性

    Provable Robustness for Streaming Models with a Sliding Window. (arXiv:2303.16308v1 [cs.LG])

    [http://arxiv.org/abs/2303.16308](http://arxiv.org/abs/2303.16308)

    本文关注于流数据上机器学习模型的可证明鲁棒性，为使用固定大小滑动窗口的模型提供了强稳健性证明。

    

    机器学习中，有关可证明的稳健性的文献主要关注静态预测问题，如图像分类等，其中假定输入样本是独立的，并且模型性能是在输入分布上的期望。对于单个输入实例，可以得出稳健性证明，但是假设该模型对每个实例单独进行评估的稳健性证明不能直接应用于许多深度学习应用中。本文关注于数据流上机器学习模型的可证明鲁棒性，其中输入作为一系列可能相关的项呈现。我们为使用固定大小的滑动窗口的模型推导出了强稳健性证明，保证了整个输入序列，而不是单个样本，并为流机器学习模型提供了强稳健性保证。

    The literature on provable robustness in machine learning has primarily focused on static prediction problems, such as image classification, in which input samples are assumed to be independent and model performance is measured as an expectation over the input distribution. Robustness certificates are derived for individual input instances with the assumption that the model is evaluated on each instance separately. However, in many deep learning applications such as online content recommendation and stock market analysis, models use historical data to make predictions. Robustness certificates based on the assumption of independent input samples are not directly applicable in such scenarios. In this work, we focus on the provable robustness of machine learning models in the context of data streams, where inputs are presented as a sequence of potentially correlated items. We derive robustness certificates for models that use a fixed-size sliding window over the input stream. Our guarante
    
[^80]: 一份机器学习报告: 全球中长期预报的后处理技术

    A Machine Learning Outlook: Post-processing of Global Medium-range Forecasts. (arXiv:2303.16301v1 [cs.LG])

    [http://arxiv.org/abs/2303.16301](http://arxiv.org/abs/2303.16301)

    本研究探讨了使用非线性神经网络神经网络算法对全球中长期天气预报进行后处理的潜在好处和挑战，并展示了在特定领域上可提高高达12%的准确性。然而，需要加强对天气预报精度测量的研究，将有助于更好的理解这些模型，并进一步优化这些算法。

    

    后处理技术通常是将数值天气预报(Numerical Weather Prediction, NWP)模型的输出作为输入，应用于线性统计方法，通过包含附加的观测数据或决定更细致尺度的系统性错误，产生了改进的本地化预报。在本次试点研究中，我们研究了使用基于非线性神经网络(NN)的方法来后处理全球30个垂直层次上的多种天气特征-Temperature, moisture, wind, geopotential height, precipitable water-以及到达时间长达7天的领先水平的好处和挑战。我们表明，我们可以在温度850hPa的领域中实现高达12% (RMSE)的精度提高，对于7天的预报。但是，我们认识到需要加强对尖锐和正确天气预报的客观度量的基础性研究。我们讨论了从线性统计模型到更复杂模型的挑战，如何使用标准指标例如均方根误差(RMSE)或异常相关系数(ACC)。

    Post-processing typically takes the outputs of a Numerical Weather Prediction (NWP) model and applies linear statistical techniques to produce improve localized forecasts, by including additional observations, or determining systematic errors at a finer scale. In this pilot study, we investigate the benefits and challenges of using non-linear neural network (NN) based methods to post-process multiple weather features -- temperature, moisture, wind, geopotential height, precipitable water -- at 30 vertical levels, globally and at lead times up to 7 days. We show that we can achieve accuracy improvements of up to 12% (RMSE) in a field such as temperature at 850hPa for a 7 day forecast. However, we recognize the need to strengthen foundational work on objectively measuring a sharp and correct forecast. We discuss the challenges of using standard metrics such as root mean squared error (RMSE) or anomaly correlation coefficient (ACC) as we move from linear statistical models to more complex
    
[^81]: Dice半度量损失函数：用软标签优化Dice分数

    Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels. (arXiv:2303.16296v1 [cs.CV])

    [http://arxiv.org/abs/2303.16296](http://arxiv.org/abs/2303.16296)

    本文提出的Dice半度量损失函数可在软标签设置中使用，在医疗成像领域的分割方案中与使用软标签的研究相结合，可以获得更好的Dice分数和模型校准。

    

    在医学成像领域的许多自动分割方案中，软Dice损失（SDL）发挥了关键作用。在过去几年中，人们已经揭示了其优越性能背后的一些原因并进一步探索了其优化。然而，目前还没有实现支持直接在软标签设置中使用它的方案。因此，在使用SDL和研究利用软标签的同时进行模型校准的协同作用仍然缺失。在本文中，我们介绍了Dice半度量损失函数（DMLs），它们（i）在硬标签的标准设置下与SDL相同，但（ii）也可在软标签设置中使用。我们在公共的QUBIQ、LiTS和KiTS基准测试上的实验证实了DMLs与软标签（如平均、标签平滑和知识蒸馏）的潜在协同作用，而DMLs与硬标签（如大多数投票和随机选择）相比，产生了更优秀的Dice分数和模型校准。

    The soft Dice loss (SDL) has taken a pivotal role in many automated segmentation pipelines in the medical imaging community. Over the last years, some reasons behind its superior functioning have been uncovered and further optimizations have been explored. However, there is currently no implementation that supports its direct use in settings with soft labels. Hence, a synergy between the use of SDL and research leveraging the use of soft labels, also in the context of model calibration, is still missing. In this work, we introduce Dice semimetric losses (DMLs), which (i) are by design identical to SDL in a standard setting with hard labels, but (ii) can be used in settings with soft labels. Our experiments on the public QUBIQ, LiTS and KiTS benchmarks confirm the potential synergy of DMLs with soft labels (e.g. averaging, label smoothing, and knowledge distillation) over hard labels (e.g. majority voting and random selection). As a result, we obtain superior Dice scores and model calib
    
[^82]: XAIR：增强现实中可解释AI框架

    XAIR: A Framework of Explainable AI in Augmented Reality. (arXiv:2303.16292v1 [cs.HC])

    [http://arxiv.org/abs/2303.16292](http://arxiv.org/abs/2303.16292)

    XAIR是一个解决AR中XAI输出解释的设计框架，可以为设计者提供指南和支持，实现有效的XAI设计。

    

    可解释的人工智能（XAI）已经成为AI驱动交互式系统的重要组成部分。随着增强现实（AR）越来越多地融入日常生活，XAI在AR中的作用也变得至关重要，因为最终用户将经常与智能服务互动。然而，如何设计有效的AR XAI体验尚不清楚。我们提出了XAIR，这是一个设计框架，解决了何时，什么和如何在AR中提供AI输出解释的问题。该框架是基于对XAI和HCI研究的跨学科文献综述、对500多名最终用户对基于AR的解释偏好的大规模调查以及对12位专家收集他们在AR XAI设计方面的见解的三个研讨会而构建的。通过与10名设计师和另外12名最终用户进行的研究验证了XAIR的效用和有效性。XAIR可以为设计师提供指南，激发他们发现新的设计机会，并在AR中实现有效的XAI设计。

    Explainable AI (XAI) has established itself as an important component of AI-driven interactive systems. With Augmented Reality (AR) becoming more integrated in daily lives, the role of XAI also becomes essential in AR because end-users will frequently interact with intelligent services. However, it is unclear how to design effective XAI experiences for AR. We propose XAIR, a design framework that addresses "when", "what", and "how" to provide explanations of AI output in AR. The framework was based on a multi-disciplinary literature review of XAI and HCI research, a large-scale survey probing 500+ end-users' preferences for AR-based explanations, and three workshops with 12 experts collecting their insights about XAI design in AR. XAIR's utility and effectiveness was verified via a study with 10 designers and another study with 12 end-users. XAIR can provide guidelines for designers, inspiring them to identify new design opportunities and achieve effective XAI designs in AR.
    
[^83]: 大象的透视镜：调查谷歌、ChatGPT、维基百科和YouTube上的语言偏见

    A Perspectival Mirror of the Elephant: Investigating Language Bias on Google, ChatGPT, Wikipedia, and YouTube. (arXiv:2303.16281v1 [cs.CY])

    [http://arxiv.org/abs/2303.16281](http://arxiv.org/abs/2303.16281)

    研究发现在Google、ChatGPT、维基百科和YouTube上，搜索结果受限于语言，反映了与复杂主题相关的文化刻板印象，缺乏跨文化视角。

    

    与谷歌搜索“从多个角度获取信息，以便你可以形成自己对世界的理解”的任务相反，我们发现谷歌及其最突出的搜索结果 - 维基百科和YouTube，仅反映与“佛教”、“自由主义”、“殖民化”、“伊朗”和“美国”等复杂主题相关的文化刻板印象。简单地说，在不同语言的相同搜索中，它们以不同程度呈现不同的信息（我们称之为“语言偏见”），而不是呈现复杂主题的全球图片。我们的在线搜索使我们成为谚语中的盲人，仅触摸小象的一小部分，不知道其他文化的视角的存在。我们用于搜索的语言最终成为促进本族中心主义观点的文化过滤器，其中一个人根据自己的文化评估其他人或思想。我们还发现ChatGPT中深深嵌入了语言偏见。

    Contrary to Google Search's mission of delivering information from "many angles so you can form your own understanding of the world," we find that Google and its most prominent returned results -- Wikipedia and YouTube, simply reflect the narrow set of cultural stereotypes tied to the search language for complex topics like "Buddhism," "Liberalism," "colonization," "Iran" and "America." Simply stated, they present, to varying degrees, distinct information across the same search in different languages (we call it 'language bias'). Instead of presenting a global picture of a complex topic, our online searches turn us into the proverbial blind person touching a small portion of an elephant, ignorant of the existence of other cultural perspectives. The language we use to search ends up as a cultural filter to promote ethnocentric views, where a person evaluates other people or ideas based on their own culture. We also find that language bias is deeply embedded in ChatGPT. As it is primaril
    
[^84]: 基于多保真深度转移学习涡模型的风电场偏航和布局优化加速方法

    Accelerated wind farm yaw and layout optimisation with multi-fidelity deep transfer learning wake models. (arXiv:2303.16274v1 [cs.LG])

    [http://arxiv.org/abs/2303.16274](http://arxiv.org/abs/2303.16274)

    本文提出了一种名为 WakeNet 的机器学习框架，它利用多保真度转移学习将低保真度高斯涡模型微调生成中高保真度卷曲涡模型的精确涡流结果，以加速基于偏航和布局优化的涡流控制，取得了不错的效果。

    

    风电场建模是一个日益受到关注的领域，已经开发出众多基于分析及计算的方法来拓展风电场效率边界、最大化发电量。本文提出了一种新的机器学习框架 WakeNet，可以在多种偏航角度、风速和湍流强度下，以99.8%的平均精度生成泛化的 2D 涡流速场，与 FLORIS 作为最先进的风电场模拟软件计算出的解决方案相比。由于高保真度数据生成成本高昂，本文还研究了多保真度转移学习的实用性。具体而言，对低保真度高斯涡模型进行预训练后微调，以便获得中等保真度卷曲涡模型的精确涡流结果。通过偏航和布局优化等多种涡流控制问题的演示，展示了 WakeNet 的健壮性和整体性能。结果显示，WakeNet 能够实现比基准场景增加 12.8% 的发电量，并比 FLORIS 提高 3.2%，且计算占用小。

    Wind farm modelling has been an area of rapidly increasing interest with numerous analytical as well as computational-based approaches developed to extend the margins of wind farm efficiency and maximise power production. In this work, we present the novel ML framework WakeNet, which can reproduce generalised 2D turbine wake velocity fields at hub-height over a wide range of yaw angles, wind speeds and turbulence intensities (TIs), with a mean accuracy of 99.8% compared to the solution calculated using the state-of-the-art wind farm modelling software FLORIS. As the generation of sufficient high-fidelity data for network training purposes can be cost-prohibitive, the utility of multi-fidelity transfer learning has also been investigated. Specifically, a network pre-trained on the low-fidelity Gaussian wake model is fine-tuned in order to obtain accurate wake results for the mid-fidelity Curl wake model. The robustness and overall performance of WakeNet on various wake steering control 
    
[^85]: 在有限重叠样本情况下的通信高效的垂直联合学习方法

    Communication-Efficient Vertical Federated Learning with Limited Overlapping Samples. (arXiv:2303.16270v1 [cs.LG])

    [http://arxiv.org/abs/2303.16270](http://arxiv.org/abs/2303.16270)

    提出了一种名叫one-shot VFL的实用VFL框架，可以同时解决通信瓶颈和有限重叠样本的问题。few-shot VFL则可以在进行一次或仅少量通信的情况下进一步提高准确性。

    

    垂直联合学习(VFL)是一种流行的协作学习方法，使客户端能够在不共享本地数据的情况下训练全局模型。VFL处理的是客户端数据具有不同的特征空间但共享一些重叠样本的情况。现有的VFL方法存在通信成本高和无法有效处理现实世界中常见的有限重叠样本问题。我们提出了一种名叫 \textbf{one-shot VFL} 的实用VFL框架，基于半监督学习，可以同时解决通信瓶颈和有限重叠样本的问题。我们还提出了 \textbf{few-shot VFL}，在只进行一次或仅少量通信的情况下，进一步提高准确性。在我们提出的框架中，客户端只需要与服务器进行一次或仅少量通信。我们在图像和表格数据集上评估了所提出的VFL框架。

    Federated learning is a popular collaborative learning approach that enables clients to train a global model without sharing their local data. Vertical federated learning (VFL) deals with scenarios in which the data on clients have different feature spaces but share some overlapping samples. Existing VFL approaches suffer from high communication costs and cannot deal efficiently with limited overlapping samples commonly seen in the real world. We propose a practical vertical federated learning (VFL) framework called \textbf{one-shot VFL} that can solve the communication bottleneck and the problem of limited overlapping samples simultaneously based on semi-supervised learning. We also propose \textbf{few-shot VFL} to improve the accuracy further with just one more communication round between the server and the clients. In our proposed framework, the clients only need to communicate with the server once or only a few times. We evaluate the proposed VFL framework on both image and tabular
    
[^86]: TimeBalance：面向半监督行为识别的时间不变和时间不同视频表示

    TimeBalance: Temporally-Invariant and Temporally-Distinctive Video Representations for Semi-Supervised Action Recognition. (arXiv:2303.16268v1 [cs.CV])

    [http://arxiv.org/abs/2303.16268](http://arxiv.org/abs/2303.16268)

    TimeBalance是一个半监督学习框架，通过利用时间不变和时间不同的自监督视频表示来学习准确的行为识别特征。实验结果表明该方法在半监督设置下超越了现有技术水平。

    

    与图像相比，半监督学习对于视频领域更具有优势，因为其注释成本和维度更高。为了学习半监督动作识别任务的静态和运动相关特征，现有方法依赖于硬输入归纳偏差，如使用两种模式（RGB和Optical-flow）或不同播放速率的两个流。我们不使用不同的输入流利用未标记的视频，相反，我们依赖于自监督视频表示，尤其是我们使用时间不变和时间不同的表示。我们观察到，这些表示根据动作的性质相互补充。基于这个观察，我们提出了一种名为TimeBalance的师生半监督学习框架，我们将来自时间不变和时间不同的教师模型的知识提炼到一个学生模型中，以学习准确的行为识别特征。在UCF101，HMDB51和Kinetics等标准基准测试中的实验评估表明了我们方法的有效性，在半监督设置下超越了现有技术水平。

    Semi-Supervised Learning can be more beneficial for the video domain compared to images because of its higher annotation cost and dimensionality. Besides, any video understanding task requires reasoning over both spatial and temporal dimensions. In order to learn both the static and motion related features for the semi-supervised action recognition task, existing methods rely on hard input inductive biases like using two-modalities (RGB and Optical-flow) or two-stream of different playback rates. Instead of utilizing unlabeled videos through diverse input streams, we rely on self-supervised video representations, particularly, we utilize temporally-invariant and temporally-distinctive representations. We observe that these representations complement each other depending on the nature of the action. Based on this observation, we propose a student-teacher semi-supervised learning framework, TimeBalance, where we distill the knowledge from a temporally-invariant and a temporally-distincti
    
[^87]: 强化学习用于能源交易策略的优化

    Reinforcement learning for optimization of energy trading strategy. (arXiv:2303.16266v1 [cs.LG])

    [http://arxiv.org/abs/2303.16266](http://arxiv.org/abs/2303.16266)

    本文使用强化学习算法优化了一种黑盒交易策略，该策略通过在马尔可夫决策过程中使用真实数据进行优化，在 DA 能源市场上由中型生产者自动进行交易。

    

    越来越多的能源来自大量小型生产者的可再生能源，这些来源的效率是不稳定的，在某种程度上也是随机的，加剧了能源市场平衡问题。在许多国家，这种平衡是在预测日（DA）能源市场上完成的。本文考虑由中型生产者在DA能源市场上的自动化交易。我们将此活动建模为马尔可夫决策过程，并规范了一个框架，其中可以使用现实数据优化即用策略。我们合成参数化交易策略，并使用进化算法优化它们。我们还使用最先进的强化学习算法优化一个黑盒交易策略，该策略利用来自环境的可用信息来影响未来价格。

    An increasing part of energy is produced from renewable sources by a large number of small producers. The efficiency of these sources is volatile and, to some extent, random, exacerbating the energy market balance problem. In many countries, that balancing is performed on day-ahead (DA) energy markets. In this paper, we consider automated trading on a DA energy market by a medium size prosumer. We model this activity as a Markov Decision Process and formalize a framework in which a ready-to-use strategy can be optimized with real-life data. We synthesize parametric trading strategies and optimize them with an evolutionary algorithm. We also use state-of-the-art reinforcement learning algorithms to optimize a black-box trading strategy fed with available information from the environment that can impact future prices.
    
[^88]: 使用上下文摘要和领域架构的零样本泛化端到端任务导向对话系统

    Zero-Shot Generalizable End-to-End Task-Oriented Dialog System using Context Summarization and Domain Schema. (arXiv:2303.16252v1 [cs.CL])

    [http://arxiv.org/abs/2303.16252](http://arxiv.org/abs/2303.16252)

    本论文提出了一种使用领域架构和对话历史摘要实现泛化的零样本端到端任务导向对话系统，并在实验中证明其在标准领域转移和少样本学习中优于现有方法。

    

    任务导向的对话系统通过促进直观和表达性的自然语言交互，使用户能够完成他们的目标。 在任务导向的对话系统中，最先进的方法将问题制定为条件序列生成任务，并在监督设置中微调预先训练的因果语言模型。 这需要为每个新领域或任务提供标记训练数据，并获取这样的数据是极其费力和昂贵的，从而使其成为扩展系统到各种领域的瓶颈。 为了克服这个挑战，我们引入了一种新的零样本泛化端到端任务导向对话系统ZS-ToD，它利用领域架构，允许对未知领域进行强大的泛化，并利用对话历史的有效摘要。 我们使用GPT-2作为骨干模型，并引入了一个两步训练过程，其中第一步的目标是学习对话数据的一般结构，第二步通过最大似然估计和强化学习的组合来优化端到端任务导向对话系统。 实验结果展示了所提出的方法在标准领域转移和少样本学习基准测试中优于现有的零样本任务导向对话系统。

    Task-oriented dialog systems empower users to accomplish their goals by facilitating intuitive and expressive natural language interactions. State-of-the-art approaches in task-oriented dialog systems formulate the problem as a conditional sequence generation task and fine-tune pre-trained causal language models in the supervised setting. This requires labeled training data for each new domain or task, and acquiring such data is prohibitively laborious and expensive, thus making it a bottleneck for scaling systems to a wide range of domains. To overcome this challenge, we introduce a novel Zero-Shot generalizable end-to-end Task-oriented Dialog system, ZS-ToD, that leverages domain schemas to allow for robust generalization to unseen domains and exploits effective summarization of the dialog history. We employ GPT-2 as a backbone model and introduce a two-step training process where the goal of the first step is to learn the general structure of the dialog data and the second step opti
    
[^89]: 随机初始化神经网络进行模型参考自适应控制的函数逼近

    Function Approximation with Randomly Initialized Neural Networks for Approximate Model Reference Adaptive Control. (arXiv:2303.16251v1 [math.OC])

    [http://arxiv.org/abs/2303.16251](http://arxiv.org/abs/2303.16251)

    本论文提出一种新的构造方法——平滑的积分表示，使得可以使用随机初始化的神经网络保证逼近精度，并且可以适用于更广泛的激活函数。

    

    神经网络逼近理论中的经典结果表明，在激活函数满足某些温和的假设条件下，可以通过单层隐藏层的网络逼近任意连续函数。然而，经典理论并没有给出一种构造性方法来生成网络参数以实现所需的精度。最近的研究结果表明，对于专门的激活函数（如ReLU函数和某些类的解析函数），可以通过随机初始化激活的线性组合实现高精度逼近。这些最近的工作利用了特殊的积分表示，这些表示依赖于所使用的具体的激活函数。本文定义了平滑的积分表示，提供了一种使用激活形成目标函数积分表示的方法，而对于这些激活，目前还没有直接的积分表示。新的构造使得可以为随机初始化网络提供逼近保证。

    Classical results in neural network approximation theory show how arbitrary continuous functions can be approximated by networks with a single hidden layer, under mild assumptions on the activation function. However, the classical theory does not give a constructive means to generate the network parameters that achieve a desired accuracy. Recent results have demonstrated that for specialized activation functions, such as ReLUs and some classes of analytic functions, high accuracy can be achieved via linear combinations of randomly initialized activations. These recent works utilize specialized integral representations of target functions that depend on the specific activation functions used. This paper defines mollified integral representations, which provide a means to form integral representations of target functions using activations for which no direct integral representation is currently known. The new construction enables approximation guarantees for randomly initialized networks
    
[^90]: ytopt：自动调优在大规模能源效率科学应用中的应用

    ytopt: Autotuning Scientific Applications for Energy Efficiency at Large Scales. (arXiv:2303.16245v1 [cs.DC])

    [http://arxiv.org/abs/2303.16245](http://arxiv.org/abs/2303.16245)

    本研究提出了ytopt，一种低延迟的自动调优框架，可用于大规模混合MPI / OpenMP科学应用，实现对性能和能源的自动调优，同时优化应用程序的能源消耗，实验结果表明，应用框架可实现高达34％的能源节省，且具有可伸缩性。

    

    进入异构计算时代后，合理优化能源和性能已经成为提高科学应用运行效率和优化管理的关键问题。本文提出一种低延迟的自动调优框架，可对各种大规模混合MPI / OpenMP科学应 民进行性能和能源自动调优，并探究面临能源约束情况下应用运行时间和能源耗 散之间的权衡，并在Argonne国家实验室的Theta和Oak Ridge国家实验室的Summit两个 生产系统上自动调优了四个ECP代理应用- XSBench，AMG，SWFFT和SW4lite。本方法使 用基于随机森林代理模型的贝叶斯优化来有效地搜索多达6百万个不同配置的参数空 间。实验结果表明，本文提出的大规模自动调优框架具有低开销和良好的可伸缩性。通 过所提出的自动调优框架，我们能够在不牺牲性能的情况下，使某些ECP代理应用的能源消耗降低高达34％。

    As we enter the exascale computing era, efficiently utilizing power and optimizing the performance of scientific applications under power and energy constraints has become critical and challenging. We propose a low-overhead autotuning framework to autotune performance and energy for various hybrid MPI/OpenMP scientific applications at large scales and to explore the tradeoffs between application runtime and power/energy for energy efficient application execution, then use this framework to autotune four ECP proxy applications -XSBench, AMG, SWFFT, and SW4lite. Our approach uses Bayesian optimization with a Random Forest surrogate model to effectively search parameter spaces with up to 6 million different configurations on two large-scale production systems, Theta at Argonne National Laboratory and Summit at Oak Ridge National Laboratory. The experimental results show that our autotuning framework at large scales has low overhead and achieves good scalability. Using the proposed autot
    
[^91]: Tetra-AML: 基于张量网络的自动机器学习

    Tetra-AML: Automatic Machine Learning via Tensor Networks. (arXiv:2303.16214v1 [cs.LG])

    [http://arxiv.org/abs/2303.16214](http://arxiv.org/abs/2303.16214)

    Tetra-AML 是一套基于张量网络的自动机器学习工具，可以自动完成神经网络结构搜索和超参数优化，还提供了模型压缩功能，通过量化、剪枝和张量网络来实现。在计算机视觉任务中，该工具箱的表现优于贝叶斯优化，同时可以将内存使用压缩至原来的 1/14.5，仅损失 3.2％ 的准确度。

    

    神经网络在许多领域中都有深刻的应用，但在具有数十亿个参数的巨型模型时代，针对商业应用进行优化和部署可能需要大量的计算和财务资源。为了解决这些挑战，我们引入了 Tetra-AML 工具箱，通过自定义的黑盒张量矩阵优化算法 TetraOpt 自动完成神经网络结构搜索和超参数优化。该工具箱还提供模型压缩功能，包括量化和剪枝，并使用张量网络来实现压缩。在这里，我们分析了用于在计算机视觉任务中优化神经网络的统一基准，并展示了我们的方法在 CIFAR-10 数据集上相对于贝叶斯优化的卓越性能。我们还展示了 ResNet-18 神经网络的压缩，其中我们使用的内存只有原来的 1/14.5，同时仅损失了 3.2％ 的准确度。所呈现的框架是通用的，不仅限于计算机视觉问题。

    Neural networks have revolutionized many aspects of society but in the era of huge models with billions of parameters, optimizing and deploying them for commercial applications can require significant computational and financial resources. To address these challenges, we introduce the Tetra-AML toolbox, which automates neural architecture search and hyperparameter optimization via a custom-developed black-box Tensor train Optimization algorithm, TetraOpt. The toolbox also provides model compression through quantization and pruning, augmented by compression using tensor networks. Here, we analyze a unified benchmark for optimizing neural networks in computer vision tasks and show the superior performance of our approach compared to Bayesian optimization on the CIFAR-10 dataset. We also demonstrate the compression of ResNet-18 neural networks, where we use 14.5 times less memory while losing just 3.2% of accuracy. The presented framework is generic, not limited by computer vision problem
    
[^92]: 基于多子网络的EMO联合剪枝：高效快速

    An EMO Joint Pruning with Multiple Sub-networks: Fast and Effect. (arXiv:2303.16212v1 [cs.LG])

    [http://arxiv.org/abs/2303.16212](http://arxiv.org/abs/2303.16212)

    本文提出了一种基于多子网络的EMO联合剪枝算法，该算法可以减少空间和资源消耗。该算法采用分治的EMO网络剪枝框架，将整个网络上复杂的EMO剪枝任务分解为多个子网络上更简单的子任务。基于跨网络约束的子网络训练方法可以进一步提高性能。

    

    基于进化多目标（EMO）的网络剪枝算法可以平衡网络的剪枝率和性能。然而，由于基于种群的特性，它经常受到复杂的剪枝优化空间和高度资源消耗的剪枝结构验证过程的限制，从而限制了它的应用。为此，本文提出了一种基于多子网络的EMO联合剪枝（EMO-PMS），以减少空间复杂度和资源消耗。首先，提出了一种分治的EMO网络剪枝框架，将整个网络上复杂的EMO剪枝任务分解为多个子网络上更简单的子任务。一方面，这种分解减少了剪枝优化空间并降低了优化难度；另一方面，较小的网络结构收敛更快，因此所提出的算法的计算资源消耗较低。其次，基于跨网络约束的子网络训练方法。

    The network pruning algorithm based on evolutionary multi-objective (EMO) can balance the pruning rate and performance of the network. However, its population-based nature often suffers from the complex pruning optimization space and the highly resource-consuming pruning structure verification process, which limits its application. To this end, this paper proposes an EMO joint pruning with multiple sub-networks (EMO-PMS) to reduce space complexity and resource consumption. First, a divide-and-conquer EMO network pruning framework is proposed, which decomposes the complex EMO pruning task on the whole network into easier sub-tasks on multiple sub-networks. On the one hand, this decomposition reduces the pruning optimization space and decreases the optimization difficulty; on the other hand, the smaller network structure converges faster, so the computational resource consumption of the proposed algorithm is lower. Secondly, a sub-network training method based on cross-network constraint
    
[^93]: 用于单词的组合卷积神经网络

    Combinatorial Convolutional Neural Networks for Words. (arXiv:2303.16211v1 [cs.LG])

    [http://arxiv.org/abs/2303.16211](http://arxiv.org/abs/2303.16211)

    本文研究了深度学习模型在识别和利用数据项上的双射变换下保持不变的特征方面的局限性，并提出了一种组合卷积神经网络用于单词分类。

    

    本文讨论了深度学习模型在识别和利用数据项上的双射变换下保持不变的特征（我们称其为组合模式）方面的局限性。我们认为在某些应用中识别出这样的模式可能很重要，并建议为神经网络提供充分描述输入项组合模式的信息，让网络确定预测相关内容。为了证明这种方法的可行性，我们提出了一种用于单词分类的组合卷积神经网络。

    The paper discusses the limitations of deep learning models in identifying and utilizing features that remain invariant under a bijective transformation on the data entries, which we refer to as combinatorial patterns. We argue that the identification of such patterns may be important for certain applications and suggest providing neural networks with information that fully describes the combinatorial patterns of input entries and allows the network to determine what is relevant for prediction. To demonstrate the feasibility of this approach, we present a combinatorial convolutional neural network for word classification.
    
[^94]: 深度集合在多输出回归任务中量化校准不确定性的探究

    Towards Quantifying Calibrated Uncertainty via Deep Ensembles in Multi-output Regression Task. (arXiv:2303.16210v1 [cs.LG])

    [http://arxiv.org/abs/2303.16210](http://arxiv.org/abs/2303.16210)

    本研究探究了在多输出回归任务中应用深度集合量化校准不确定性的方法，提出了该方法的改进框架，其在回归准确性、不确定性估计可靠性和训练效率方面具有优越表现。

    

    深度集合是逼近贝叶斯推断的一种简单直接的方法，已被成功应用于许多分类任务。本研究旨在全面探究该方法在多输出回归任务中的应用，以预测导弹结构的空气动力性能。通过仔细研究集合中神经网络数量的影响，观察到估计的不确定性普遍存在低估的趋势。在此背景下，提出了一种应用事后校准的深度集合框架，并证明其改进的不确定性量化性能。直观地将其与高斯过程回归进行比较，这是工程中最常用的不确定性量化模型，结果表明在回归准确性、估计不确定性的可靠性和训练效率方面具有卓越的表现。最后，本文也研究了所提出框架对贝叶斯优化结果的影响。

    Deep ensemble is a simple and straightforward approach for approximating Bayesian inference and has been successfully applied to many classification tasks. This study aims to comprehensively investigate this approach in the multi-output regression task to predict the aerodynamic performance of a missile configuration. By scrutinizing the effect of the number of neural networks used in the ensemble, an obvious trend toward underconfidence in estimated uncertainty is observed. In this context, we propose the deep ensemble framework that applies the post-hoc calibration method, and its improved uncertainty quantification performance is demonstrated. It is compared with Gaussian process regression, the most prevalent model for uncertainty quantification in engineering, and is proven to have superior performance in terms of regression accuracy, reliability of estimated uncertainty, and training efficiency. Finally, the impact of the suggested framework on the results of Bayesian optimizatio
    
[^95]: AmorProt：基于氨基酸分子指纹重用的蛋白质指纹算法

    AmorProt: Amino Acid Molecular Fingerprints Repurposing based Protein Fingerprint. (arXiv:2303.16209v1 [q-bio.QM])

    [http://arxiv.org/abs/2303.16209](http://arxiv.org/abs/2303.16209)

    本文提出了一种基于氨基酸分子指纹重用的蛋白质指纹算法AmorProt，使用树形机器学习和人工神经网络模型进行淀粉样蛋白分组和等电点回归，并展示了该平台的适用性和优势。

    

    随着蛋白质治疗在几乎所有医疗领域中的重要作用，已经有许多使用人工智能研究蛋白质的相关工作。人工智能使得在不需要昂贵的实验的情况下可以进行含有大量数据的预测。然而，与开发的各种分子指纹算法不同的是，蛋白质指纹算法很少被研究。在本研究中，我们提出了基于氨基酸分子指纹重用的蛋白质指纹算法（AmorProt），这是一种有效利用对应于20种氨基酸的分子指纹的蛋白质序列表示方法。随后，我们使用（1）淀粉样蛋白分组和（2）等电点回归来比较了基于树的机器学习和人工神经网络模型的表现。最后，通过案例研究和以下实验展示了开发平台的适用性和优势：（3）数据集比较。

    As protein therapeutics play an important role in almost all medical fields, numerous studies have been conducted on proteins using artificial intelligence. Artificial intelligence has enabled data driven predictions without the need for expensive experiments. Nevertheless, unlike the various molecular fingerprint algorithms that have been developed, protein fingerprint algorithms have rarely been studied. In this study, we proposed the amino acid molecular fingerprints repurposing based protein (AmorProt) fingerprint, a protein sequence representation method that effectively uses the molecular fingerprints corresponding to 20 amino acids. Subsequently, the performances of the tree based machine learning and artificial neural network models were compared using (1) amyloid classification and (2) isoelectric point regression. Finally, the applicability and advantages of the developed platform were demonstrated through a case study and the following experiments: (3) comparison of dataset 
    
[^96]: 利用分布分解提高均匀学习算法的性能

    Lifting uniform learners via distributional decomposition. (arXiv:2303.16208v1 [stat.ML])

    [http://arxiv.org/abs/2303.16208](http://arxiv.org/abs/2303.16208)

    本文介绍了一种方法，可以将任何在均匀分布下有效的PAC学习算法转换成一个在任意未知分布下有效的算法，而且对于单调分布，只需要用$\mathcal{D}$中的样本。算法的核心是通过一个算法将$\mathcal{D}$逼近成由子立方体混合而成的混合均匀分布。

    

    我们展示了如何将任何在均匀分布下有效的PAC学习算法转换成一个在任意未知分布$\mathcal{D}$下有效的算法。我们的转换效率随$\mathcal{D}$的固有复杂性而变化，对于在$\{\pm 1\}^n$上的分布，其pmf由深度为$d$的决策树计算，则时间复杂度为$\mathrm{poly}(n, (md)^d)$，其中$m$是原始算法的样本复杂度。对于单调分布，我们的转换仅使用$\mathcal{D}$中的样本，而对于一般分布，我们使用子立方体条件样本。其中一个关键技术是一个算法，它在给出$\mathcal{D}$的访问权限的情况下，产生了一个最优决策树分解$\mathcal{D}$：一个逼近了$\mathcal{D}$的混合均匀分布的分离子立方体。通过这个分解，我们在每个子立方体上运行均匀分布学习器，并将结果合并起来。

    We show how any PAC learning algorithm that works under the uniform distribution can be transformed, in a blackbox fashion, into one that works under an arbitrary and unknown distribution $\mathcal{D}$. The efficiency of our transformation scales with the inherent complexity of $\mathcal{D}$, running in $\mathrm{poly}(n, (md)^d)$ time for distributions over $\{\pm 1\}^n$ whose pmfs are computed by depth-$d$ decision trees, where $m$ is the sample complexity of the original algorithm. For monotone distributions our transformation uses only samples from $\mathcal{D}$, and for general ones it uses subcube conditioning samples.  A key technical ingredient is an algorithm which, given the aforementioned access to $\mathcal{D}$, produces an optimal decision tree decomposition of $\mathcal{D}$: an approximation of $\mathcal{D}$ as a mixture of uniform distributions over disjoint subcubes. With this decomposition in hand, we run the uniform-distribution learner on each subcube and combine the 
    
[^97]: 您的扩散模型暗中是一种零样本分类器。

    Your Diffusion Model is Secretly a Zero-Shot Classifier. (arXiv:2303.16203v1 [cs.LG])

    [http://arxiv.org/abs/2303.16203](http://arxiv.org/abs/2303.16203)

    扩散模型的密度估计可以被用作零样本分类，作者的生成式分类方法在各种基准测试中取得强大的结果，并具有更强的多模式关系推理能力。

    

    最近大规模的文本到图像扩散模型极大地增强了我们的基于文本生成图像的能力。这些模型可以为大量提示生成逼真的图像，并展示出令人印象深刻的组合泛化能力。几乎所有的用例到目前为止都只关注抽样，然而，扩散模型还可以提供有用于图像生成之外的条件密度估计。在本文中，我们展示了类似于Stable Diffusion的大规模文本到图像扩散模型的密度估计可以被利用来执行零样本分类，而无需额外的训练。我们的生成式分类方法在各种基准测试中取得了强大的结果，并优于从扩散模型中提取知识的替代方法。我们还发现，我们基于扩散的方法比竞争性的对比方法具有更强的多模式关系推理能力。最后，我们评估了我们方法的可解释性，并呈现了定性结果，证明它学习了有意义的图像-文本对齐。

    The recent wave of large-scale text-to-image diffusion models has dramatically increased our text-based image generation abilities. These models can generate realistic images for a staggering variety of prompts and exhibit impressive compositional generalization abilities. Almost all use cases thus far have solely focused on sampling; however, diffusion models can also provide conditional density estimates, which are useful for tasks beyond image generation. In this paper, we show that the density estimates from large-scale text-to-image diffusion models like Stable Diffusion can be leveraged to perform zero-shot classification without any additional training. Our generative approach to classification attains strong results on a variety of benchmarks and outperforms alternative methods of extracting knowledge from diffusion models. We also find that our diffusion-based approach has stronger multimodal relational reasoning abilities than competing contrastive approaches. Finally, we eva
    
[^98]: 通过纠错保持不变量在机器学习PDE求解器中的应用

    Invariant preservation in machine learned PDE solvers via error correction. (arXiv:2303.16110v1 [math.NA])

    [http://arxiv.org/abs/2303.16110](http://arxiv.org/abs/2303.16110)

    本文研究通过保持基本偏微分方程的离散模拟中的连续不变量来设计更可靠的机器学习PDE求解器，关键在于通过纠错保持不变量。

    

    机器学习的偏微分方程求解器在可能带来的准确性和/或速度上的潜在收益换取标准数值方法的可靠性。保证求解器输出精确解的唯一方法是在网格间距$\Delta x$和时间步长$\Delta t$趋近于零的极限下使用收敛方法。学会在大$\Delta x$和/或$\Delta t$下更新解的机器学习求解器永远无法保证完美的准确性。一定程度的误差是不可避免的，因此问题是：我们如何限制机器学习求解器以给出我们愿意容忍的错误？在本文中，我们通过保持基本偏微分方程的离散模拟中的连续不变量来设计更可靠的机器学习PDE求解器。这类不变量的示例包括质量守恒、能量守恒、热力学第二定律和（或）非负密度。我们的关键见解很简单：为了保持不变量，我们强制求解器的误差与满足相关不变量的离散函数空间正交。我们在几个示例PDE上演示了这一思想，包括非线性Burgers方程、Navier-Stokes方程和不可压缩的Euler方程。

    Machine learned partial differential equation (PDE) solvers trade the reliability of standard numerical methods for potential gains in accuracy and/or speed. The only way for a solver to guarantee that it outputs the exact solution is to use a convergent method in the limit that the grid spacing $\Delta x$ and timestep $\Delta t$ approach zero. Machine learned solvers, which learn to update the solution at large $\Delta x$ and/or $\Delta t$, can never guarantee perfect accuracy. Some amount of error is inevitable, so the question becomes: how do we constrain machine learned solvers to give us the sorts of errors that we are willing to tolerate? In this paper, we design more reliable machine learned PDE solvers by preserving discrete analogues of the continuous invariants of the underlying PDE. Examples of such invariants include conservation of mass, conservation of energy, the second law of thermodynamics, and/or non-negative density. Our key insight is simple: to preserve invariants,
    
[^99]: 大规模预训练模型在增量式新类别发现中具有出乎意料的强大表现。

    Large-scale Pre-trained Models are Surprisingly Strong in Incremental Novel Class Discovery. (arXiv:2303.15975v1 [cs.CV])

    [http://arxiv.org/abs/2303.15975](http://arxiv.org/abs/2303.15975)

    本论文提出了一种更加挑战性和实用性的学习方法MSc-iNCD，通过在连续而无人监督的学习中利用大规模预训练模型的丰富先验知识，该方法在增量式新类别发现中表现出出乎意料的强大实力。

    

    在生命长学习者中，从未标记的数据中连续地发现新概念是一个重要的期望。在文献中，这类问题在非常受限的情况下得到了部分解决，其中要么为发现新概念提供有标号的数据（例如 NCD），要么学习在有限数量的增量步骤中发生（例如类 iNCD）。在这项工作中，我们挑战现状，提出了一种更具挑战性和实用性的学习范式，称为 MSc-iNCD，其中学习连续而无人监督，并利用大规模预训练模型的丰富先验知识。为此，我们提出了简单的基线，不仅在较长的学习情境下具有弹性，而且与复杂的最先进方法相比，表现出出乎意料的强大实力。我们在多个基准测试中进行了广泛的实证评估，并展示了我们提出的基线的有效性，大大提升了基准要求。

    Discovering novel concepts from unlabelled data and in a continuous manner is an important desideratum of lifelong learners. In the literature such problems have been partially addressed under very restricted settings, where either access to labelled data is provided for discovering novel concepts (e.g., NCD) or learning occurs for a limited number of incremental steps (e.g., class-iNCD). In this work we challenge the status quo and propose a more challenging and practical learning paradigm called MSc-iNCD, where learning occurs continuously and unsupervisedly, while exploiting the rich priors from large-scale pre-trained models. To this end, we propose simple baselines that are not only resilient under longer learning scenarios, but are surprisingly strong when compared with sophisticated state-of-the-art methods. We conduct extensive empirical evaluation on a multitude of benchmarks and show the effectiveness of our proposed baselines, which significantly raises the bar.
    
[^100]: 基础模型与合理使用

    Foundation Models and Fair Use. (arXiv:2303.15715v1 [cs.CY] CROSS LISTED)

    [http://arxiv.org/abs/2303.15715](http://arxiv.org/abs/2303.15715)

    基础模型开发和部署需确保在合理使用的范围内，但这并不保证。在使用有版权内容时需要注意风险并可能需要进一步的工作。

    

    现有的基础模型是基于有版权的材料训练出来的。在数据创建者没有得到适当的归属或赔偿时部署这些模型可能带来法律和道德风险。在美国和其他几个国家，版权内容可能被用于构建基础模型而不会产生法律责任，这是因为合理使用原则的存在。但是需要注意的是，如果这些模型的输出产生了与版权数据相似的结果，尤其是在影响到这些数据市场的情况下，合理使用原则将不再适用于模型的输出。本文强调合理使用原则并不保证，可能需要进一步的工作才能确保模型的开发和部署在合理使用的范围内。首先，我们调查了基于有版权内容的开发和部署基础模型的潜在风险。我们回顾了相关的美国案例法，并类比生成文本、源代码和可视化等可能的应用。

    Existing foundation models are trained on copyrighted material. Deploying these models can pose both legal and ethical risks when data creators fail to receive appropriate attribution or compensation. In the United States and several other countries, copyrighted content may be used to build foundation models without incurring liability due to the fair use doctrine. However, there is a caveat: If the model produces output that is similar to copyrighted data, particularly in scenarios that affect the market of that data, fair use may no longer apply to the output of the model. In this work, we emphasize that fair use is not guaranteed, and additional work may be necessary to keep model development and deployment squarely in the realm of fair use. First, we survey the potential risks of developing and deploying foundation models based on copyrighted content. We review relevant U.S. case law, drawing parallels to existing and potential applications for generating text, source code, and vis
    
[^101]: TextMI:将多模态信息转化为文本形式，使预训练语言模型集成非语言线索

    TextMI: Textualize Multimodal Information for Integrating Non-verbal Cues in Pre-trained Language Models. (arXiv:2303.15430v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.15430](http://arxiv.org/abs/2303.15430)

    本文提出了一种将声学和视觉信息转化为文本描述并与口语文本串联，从而将非语言信息纳入预训练语言模型中的方法。该方法可以不需要昂贵的多模态数据收集和建模，并且可以使非语言线索与语言无缝集成，对于多模态行为理解任务具有实际应用意义。

    

    预训练大型语言模型最近在各种语言理解任务中取得了突破性的性能。然而，同一模型无法应用于多模式行为理解任务（例如，视频情感/幽默检测），除非可以将非语言特征（例如，声学和视觉）与语言集成。联合建模多个模态显着增加了模型复杂性，并使训练过程变得需数据量大。虽然通过网络可以获得大量的文本数据，但收集大规模的多模态行为视频数据集极其昂贵，无论是时间上还是金钱上。在本文中，我们研究了当以文本形式呈现的情况下，大型语言模型是否可以成功地将非语言信息纳入其中。我们提出了一种将声音和视觉信息转化为对应的文本描述，并将其与口语文本串联的方法。我们将这种增强的输入馈送给预先训练的语言模型，并显示其性能与直接使用非语言特征的相同模型相当。我们的方法TextMI不仅避免了昂贵的多模态数据收集和建模过程，而且实现了非语言线索与语言的无缝集成，这对于实际应用至关重要。

    Pre-trained large language models have recently achieved ground-breaking performance in a wide variety of language understanding tasks. However, the same model can not be applied to multimodal behavior understanding tasks (e.g., video sentiment/humor detection) unless non-verbal features (e.g., acoustic and visual) can be integrated with language. Jointly modeling multiple modalities significantly increases the model complexity, and makes the training process data-hungry. While an enormous amount of text data is available via the web, collecting large-scale multimodal behavioral video datasets is extremely expensive, both in terms of time and money. In this paper, we investigate whether large language models alone can successfully incorporate non-verbal information when they are presented in textual form. We present a way to convert the acoustic and visual information into corresponding textual descriptions and concatenate them with the spoken text. We feed this augmented input to a pr
    
[^102]: 基于注意力机制变压器神经网络的通胀预测

    Inflation forecasting with attention based transformer neural networks. (arXiv:2303.15364v1 [econ.EM])

    [http://arxiv.org/abs/2303.15364](http://arxiv.org/abs/2303.15364)

    本文研究了基于注意力机制变压器神经网络用于预测不同通货膨胀率的潜力，结果表明其可以超越传统模型，成为金融决策中的有用工具。

    

    通胀是资金配置决策的重要因素，其预测是政府和中央银行的基本目标。然而，由于其预测取决于低频高波动数据且缺乏清晰的解释变量，因此预测通胀并不是一项简单的任务。最近，（深度）神经网络在许多应用中已显示出惊人的结果，逐渐成为新的技术水平标杆。本文研究了变压器深度神经网络架构用于预测不同通胀率的潜力。结果与经典时间序列和机器学习模型进行了比较。我们发现，我们改进后的变压器模型在16个实验中平均超过基准模型6个实验，在研究的4个通货膨胀率中表现最佳。我们的结果表明，基于变压器的通胀预测模型有超越传统模型的潜力，并可以成为金融决策中的有用工具。

    Inflation is a major determinant for allocation decisions and its forecast is a fundamental aim of governments and central banks. However, forecasting inflation is not a trivial task, as its prediction relies on low frequency, highly fluctuating data with unclear explanatory variables. While classical models show some possibility of predicting inflation, reliably beating the random walk benchmark remains difficult. Recently, (deep) neural networks have shown impressive results in a multitude of applications, increasingly setting the new state-of-the-art. This paper investigates the potential of the transformer deep neural network architecture to forecast different inflation rates. The results are compared to a study on classical time series and machine learning models. We show that our adapted transformer, on average, outperforms the baseline in 6 out of 16 experiments, showing best scores in two out of four investigated inflation rates. Our results demonstrate that a transformer based
    
[^103]: 利用生成对抗网络和对比学习的通用显微镜图像去噪

    Generalizable Denoising of Microscopy Images using Generative Adversarial Networks and Contrastive Learning. (arXiv:2303.15214v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2303.15214](http://arxiv.org/abs/2303.15214)

    本文提出一种结合生成对抗网络和对比学习的方法，可以使用较少的数据量有效地去噪显微镜图像，减轻数据获取负担，实现少样本学习。

    

    显微镜图像往往受到高水平的噪声干扰，这会阻碍进一步的分析和解释。为了解决这个问题，提出了一种新的方法，将生成对抗网络（GAN）与结构保持的损失项相结合，通过对比学习（CL）训练，使用少量数据进一步提高去噪图像的质量。在三个著名的显微镜成像数据集上展示了方法的有效性，我们可以大幅减少训练数据量，同时保持去噪质量，减轻获取成对数据的负担，并实现少样本学习。该框架可轻松扩展到显微镜图像以外的其他图像恢复任务。

    Microscopy images often suffer from high levels of noise, which can hinder further analysis and interpretation. Content-aware image restoration (CARE) methods have been proposed to address this issue, but they often require large amounts of training data and suffer from over-fitting. To overcome these challenges, we propose a novel framework for few-shot microscopy image denoising. Our approach combines a generative adversarial network (GAN) trained via contrastive learning (CL) with two structure preserving loss terms (Structural Similarity Index and Total Variation loss) to further improve the quality of the denoised images using little data. We demonstrate the effectiveness of our method on three well-known microscopy imaging datasets, and show that we can drastically reduce the amount of training data while retaining the quality of the denoising, thus alleviating the burden of acquiring paired data and enabling few-shot learning. The proposed framework can be easily extended to oth
    
[^104]: 通过扩散去噪平滑进行认证和对抗性的鲁棒的样本外检测

    Diffusion Denoised Smoothing for Certified and Adversarial Robust Out-Of-Distribution Detection. (arXiv:2303.14961v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.14961](http://arxiv.org/abs/2303.14961)

    本研究提出了一个新的方法来证明$\ell_2$-norm下的样本外检测的鲁棒性，在不考虑网络架构和具体组件的情况下，改善了对抗攻击的检测技术。

    

    随着机器学习的应用不断扩展，确保其安全性变得尤为重要。其中一个主要关注点是识别给定样本是否来自训练分布，或者是一个“样本外”（OOD）样本。此外，对手可以以一种导致分类器做出自信预测的方式操纵OOD样本。本研究提出了一种新颖的方法，用于在输入的L2范围内证明在不考虑网络架构以及不需要特定组件或额外训练的情况下，对OOD检测的鲁棒性。此外，我们改进了检测OOD样本的对抗攻击的技术，同时提供了对于分布样本的高水平的认证和对抗的结果。在CIFAR10/100的所有OOD检测指标的平均值显示，与以前的方法相比提高了约13％/ 5％。

    As the use of machine learning continues to expand, the importance of ensuring its safety cannot be overstated. A key concern in this regard is the ability to identify whether a given sample is from the training distribution, or is an "Out-Of-Distribution" (OOD) sample. In addition, adversaries can manipulate OOD samples in ways that lead a classifier to make a confident prediction. In this study, we present a novel approach for certifying the robustness of OOD detection within a $\ell_2$-norm around the input, regardless of network architecture and without the need for specific components or additional training. Further, we improve current techniques for detecting adversarial attacks on OOD samples, while providing high levels of certified and adversarial robustness on in-distribution samples. The average of all OOD detection metrics on CIFAR10/100 shows an increase of $\sim 13 \% / 5\%$ relative to previous approaches.
    
[^105]: 一种具有行列交错像素合成的高效尺度不变生成器

    Efficient Scale-Invariant Generator with Column-Row Entangled Pixel Synthesis. (arXiv:2303.14157v1 [cs.CV])

    [http://arxiv.org/abs/2303.14157](http://arxiv.org/abs/2303.14157)

    提出了一种高效尺度不变的生成模型，不使用空间卷积或分层架构，通过将层内特征图分解为局部和全局特征并通过插值这些特征生成图像，使得模型具有较小的内存占用和更快的推理速度。

    

    任意比例图像合成为合成在任意比例下合成逼真图像提供了一种高效且可扩展的解决方案，即使超出了2K分辨率范围。然而，现有的基于GAN的解决方案过度依赖于卷积和分层架构，在缩放输出分辨率时会引入不一致性和“纹理粘连”问题。从另一个角度来看，基于INR的生成器从设计上是尺度等变的，但它们巨大的内存占用和缓慢的推理妨碍了这些网络在大规模或实时系统中的应用。在本研究中，我们提出了一种新的生成模型：具有行列交错像素合成的列-行交错像素合成（$\textbf{CREPS}$）。不使用任何空间卷积或从粗到细的设计。为了节省内存占用并使系统可扩展，我们采用了一种新颖的双线表示法，将层内特征图分解为局部和全局特征的独立“厚条”插值这些条带的融合来生成图像。我们的实验表明，$\textbf{CREPS}$在图像质量和可扩展性方面优于现有技术，在保持更小的内存占用和更快的推理速度的同时。

    Any-scale image synthesis offers an efficient and scalable solution to synthesize photo-realistic images at any scale, even going beyond 2K resolution. However, existing GAN-based solutions depend excessively on convolutions and a hierarchical architecture, which introduce inconsistency and the $``$texture sticking$"$ issue when scaling the output resolution. From another perspective, INR-based generators are scale-equivariant by design, but their huge memory footprint and slow inference hinder these networks from being adopted in large-scale or real-time systems. In this work, we propose $\textbf{C}$olumn-$\textbf{R}$ow $\textbf{E}$ntangled $\textbf{P}$ixel $\textbf{S}$ynthesis ($\textbf{CREPS}$), a new generative model that is both efficient and scale-equivariant without using any spatial convolutions or coarse-to-fine design. To save memory footprint and make the system scalable, we employ a novel bi-line representation that decomposes layer-wise feature maps into separate $``$thick
    
[^106]: 直接迭代反演：图像修复的替代方法

    Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration. (arXiv:2303.11435v1 [eess.IV])

    [http://arxiv.org/abs/2303.11435](http://arxiv.org/abs/2303.11435)

    InDI是一种新的监督式图像修复公式，通过逐步改进图像质量来生成比现有回归方法更真实和详细的图像，修复效果更具有感知质量。

    

    直接迭代反演（InDI）是一种新的监督式图像修复公式，它避免了所谓的“均值回归”效应，并生成比现有回归方法更真实和详细的图像。它通过逐步改进图像质量来实现，类似于生成式去噪扩散模型。图像修复是一个欠定问题，多个高质量图像都可能是给定低质量输入的可行重构。因此，单步回归模型的结果通常是所有可能解释的聚合结果，因此缺乏细节和真实感。

    Inversion by Direct Iteration (InDI) is a new formulation for supervised image restoration that avoids the so-called ``regression to the mean'' effect and produces more realistic and detailed images than existing regression-based methods. It does this by gradually improving image quality in small steps, similar to generative denoising diffusion models.  Image restoration is an ill-posed problem where multiple high-quality images are plausible reconstructions of a given low-quality input. Therefore, the outcome of a single step regression model is typically an aggregate of all possible explanations, therefore lacking details and realism. % The main advantage of InDI is that it does not try to predict the clean target image in a single step but instead gradually improves the image in small steps, resulting in better perceptual quality.  While generative denoising diffusion models also work in small steps, our formulation is distinct in that it does not require knowledge of any analytic f
    
[^107]: 基于类别引导的图像扩散：通过类别标签从亮场图像中细胞绘画

    Class-Guided Image-to-Image Diffusion: Cell Painting from Brightfield Images with Class Labels. (arXiv:2303.08863v1 [cs.CV])

    [http://arxiv.org/abs/2303.08863](http://arxiv.org/abs/2303.08863)

    该论文介绍了一种新的算法，将图像至图像和类别引导去噪扩散概率模型组合，用于有元数据标签的图像重建问题。实验结果表明，类别引导的图像至图像扩散可以提升图像重建的内容，优于未引导的模型。

    

    在生物医学图像领域，常常出现带有自由或廉价元数据形式的类别标签的图像重建问题。现有的文本引导或风格转移图像重建方法无法转化为提供离散类别的数据集。我们介绍并实现了一种模型，可将图像至图像和类别引导去噪扩散概率模型组合。我们使用真实世界的显微镜图像数据集进行训练，有和没有元数据标签。通过探索具有相关标签的图像至图像扩散的属性，我们展示了类别引导的图像至图像扩散可以提高重建图像的有意义内容，并在有用的下游任务中优于未引导的模型。

    Image-to-image reconstruction problems with free or inexpensive metadata in the form of class labels appear often in biological and medical image domains. Existing text-guided or style-transfer image-to-image approaches do not translate to datasets where additional information is provided as discrete classes. We introduce and implement a model which combines image-to-image and class-guided denoising diffusion probabilistic models. We train our model on a real-world dataset of microscopy images used for drug discovery, with and without incorporating metadata labels. By exploring the properties of image-to-image diffusion with relevant labels, we show that class-guided image-to-image diffusion can improve the meaningful content of the reconstructed images and outperform the unguided model in useful downstream tasks.
    
[^108]: 确定性不确定性方法中的训练、架构和先验

    Training, Architecture, and Prior for Deterministic Uncertainty Methods. (arXiv:2303.05796v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.05796](http://arxiv.org/abs/2303.05796)

    本文研究了确定性不确定性方法中的重要设计选择，包括解耦核心架构和不确定性头方案的训练、核心架构表达能力、避免特征崩溃的额外架构约束和先验影响等。研究发现，训练方案和核心架构表达能力对不确定性性能至关重要，而避免特征崩溃的额外架构约束可能会破坏OOD泛化和检测之间的折衷，DUMs定义的先验则对最终性能影响较小。

    

    准确高效的不确定性估计对于构建可靠的机器学习模型至关重要，而确定性不确定性方法是一种有前途的模型，能够在单次前向传播中执行不确定性估计。本文研究了DUMs的重要设计选择：（1）我们展示了将核心架构和不确定性头方案解耦的训练方案，可以显著提高不确定性性能。（2）我们证明了核心架构表达能力对不确定性性能至关重要，并且避免特征崩溃的额外架构约束可能破坏OOD泛化和检测之间的折衷。 （3）与其他贝叶斯模型相反，我们展示了DUMs定义的先验对最终性能没有强烈影响。

    Accurate and efficient uncertainty estimation is crucial to build reliable Machine Learning (ML) models capable to provide calibrated uncertainty estimates, generalize and detect Out-Of-Distribution (OOD) datasets. To this end, Deterministic Uncertainty Methods (DUMs) is a promising model family capable to perform uncertainty estimation in a single forward pass. This work investigates important design choices in DUMs: (1) we show that training schemes decoupling the core architecture and the uncertainty head schemes can significantly improve uncertainty performances. (2) we demonstrate that the core architecture expressiveness is crucial for uncertainty performance and that additional architecture constraints to avoid feature collapse can deteriorate the trade-off between OOD generalization and detection. (3) Contrary to other Bayesian models, we show that the prior defined by DUMs do not have a strong effect on the final performances.
    
[^109]: PreFallKD: 基于CNN-ViT知识蒸馏的预防跌倒系统

    PreFallKD: Pre-Impact Fall Detection via CNN-ViT Knowledge Distillation. (arXiv:2303.03634v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2303.03634](http://arxiv.org/abs/2303.03634)

    本文提出了一种基于CNN-ViT知识蒸馏的预防跌倒系统PreFallKD，通过将预训练的教师模型的检测知识转移到轻量级卷积神经网络的学生模型上，实现了检测性能和计算复杂性的平衡。

    This paper proposes a pre-impact fall detection system called PreFallKD, which uses CNN-ViT knowledge distillation to transfer detection knowledge from a pre-trained teacher model to a lightweight convolutional neural network student model. The system achieves a balance between detection performance and computational complexity.

    跌倒事故是老龄化社会中的重要问题。近年来，许多研究人员使用深度学习开发了预防跌倒系统，以支持基于可穿戴设备的跌倒保护系统，以预防严重的伤害。然而，大多数工作只使用简单的神经网络模型，而不是考虑到资源受限的移动设备和严格的延迟要求的复杂模型的可用性。在这项工作中，我们提出了一种新颖的基于CNN-ViT知识蒸馏的预防跌倒系统，即PreFallKD，以在检测性能和计算复杂性之间取得平衡。所提出的PreFallKD将检测知识从预训练的教师模型（视觉变换器）转移到学生模型（轻量级卷积神经网络）。此外，我们应用数据增强技术来解决数据不平衡的问题。我们在KFall公共数据集上进行实验，并将PreFallKD与其他最先进的模型进行比较。

    Fall accidents are critical issues in an aging and aged society. Recently, many researchers developed pre-impact fall detection systems using deep learning to support wearable-based fall protection systems for preventing severe injuries. However, most works only employed simple neural network models instead of complex models considering the usability in resource-constrained mobile devices and strict latency requirements. In this work, we propose a novel pre-impact fall detection via CNN-ViT knowledge distillation, namely PreFallKD, to strike a balance between detection performance and computational complexity. The proposed PreFallKD transfers the detection knowledge from the pre-trained teacher model (vision transformer) to the student model (lightweight convolutional neural networks). Additionally, we apply data augmentation techniques to tackle issues of data imbalance. We conduct the experiment on the KFall public dataset and compare PreFallKD with other state-of-the-art models. The
    
[^110]: 针对耦合偏微分方程的耦合多小波神经算子学习

    Coupled Multiwavelet Neural Operator Learning for Coupled Partial Differential Equations. (arXiv:2303.02304v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02304](http://arxiv.org/abs/2303.02304)

    本论文提出一种耦合多小波神经算子学习的方案，解决了处理耦合多变量映射问题的难点，能够显著提高解决耦合偏微分方程的准确性，并在实验中得到了验证。

    

    耦合偏微分方程是描述许多物理过程复杂动态的关键任务。最近，神经算子已经展示出通过在傅里叶/小波空间直接学习积分核来解决PDE的能力。对于耦合PDE的解决方法，难点在于处理函数之间的耦合映射。为此，我们提出了一种耦合多小波神经算子（CMWNO）学习方案，通过在小波空间中进行多小波分解和重构过程中解耦合积分核。在解决Gray-Scott（GS）方程和非局部均场博弈（MFG）问题等耦合PDE方面，所提出的模型相对于先前基于学习的求解器实现了显著提高的准确性。根据我们的实验结果，所提出的模型相对于最先进模型的$L^2$误差表现出了$2\times \sim 4\times$的改进。

    Coupled partial differential equations (PDEs) are key tasks in modeling the complex dynamics of many physical processes. Recently, neural operators have shown the ability to solve PDEs by learning the integral kernel directly in Fourier/Wavelet space, so the difficulty for solving the coupled PDEs depends on dealing with the coupled mappings between the functions. Towards this end, we propose a \textit{coupled multiwavelets neural operator} (CMWNO) learning scheme by decoupling the coupled integral kernels during the multiwavelet decomposition and reconstruction procedures in the Wavelet space. The proposed model achieves significantly higher accuracy compared to previous learning-based solvers in solving the coupled PDEs including Gray-Scott (GS) equations and the non-local mean field game (MFG) problem. According to our experimental results, the proposed model exhibits a $2\times \sim 4\times$ improvement relative $L$2 error compared to the best results from the state-of-the-art mode
    
[^111]: 基于颗粒球计算的高效最小生成树聚类算法GBMST

    GBMST: An Efficient Minimum Spanning Tree Clustering Based on Granular-Ball Computing. (arXiv:2303.01082v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01082](http://arxiv.org/abs/2303.01082)

    该论文提出了一种基于多粒度颗粒球和最小生成树的聚类算法，通过实现基于“大规模优先级”的聚类方法，可以大大避免异常值的影响，并加速了MST的构建过程。

    

    大多数现有的聚类方法都基于单一粒度的信息，如每个数据的距离和密度。这种最细粒度的方法通常效率低下且容易受到噪声的影响。因此，我们提出了一种聚类算法，它结合了多粒度颗粒球和最小生成树(MST)。我们构建了粗粒度颗粒球，然后使用颗粒球和MST实现基于“大规模优先级”的聚类方法，可以大大避免异常值的影响，并加快了MST的构建过程。在几个数据集上的实验结果证明了该算法的效果。所有代码已在https://github.com/xjnine/GBMST发布。

    Most of the existing clustering methods are based on a single granularity of information, such as the distance and density of each data. This most fine-grained based approach is usually inefficient and susceptible to noise. Therefore, we propose a clustering algorithm that combines multi-granularity Granular-Ball and minimum spanning tree (MST). We construct coarsegrained granular-balls, and then use granular-balls and MST to implement the clustering method based on "large-scale priority", which can greatly avoid the influence of outliers and accelerate the construction process of MST. Experimental results on several data sets demonstrate the power of the algorithm. All codes have been released at https://github.com/xjnine/GBMST.
    
[^112]: FuNVol：使用函数主成分和神经SDE的多资产隐含波动率市场模拟器

    FuNVol: A Multi-Asset Implied Volatility Market Simulator using Functional Principal Components and Neural SDEs. (arXiv:2303.00859v2 [q-fin.CP] UPDATED)

    [http://arxiv.org/abs/2303.00859](http://arxiv.org/abs/2303.00859)

    FuNVol是一个多资产隐含波动率市场模拟器，使用函数主成分和神经SDE生成真实历史价格的IV表面序列，并在无静态套利的表面次流形内产生一致的市场情景。同时，使用模拟表面进行对冲可以生成与实现P＆L一致的损益分布。

    

    我们介绍了一种新的方法，使用函数数据分析和神经随机微分方程，结合概率积分变换惩罚来生成多个资产的隐含波动率表面序列，该方法忠实于历史价格。我们证明了学习IV表面和价格的联合动态产生的市场情景与历史特征一致，并且在没有静态套利的表面次流形内。最后，我们证明使用模拟表面进行对冲会生成与实现P＆L一致的损益分布。

    Here, we introduce a new approach for generating sequences of implied volatility (IV) surfaces across multiple assets that is faithful to historical prices. We do so using a combination of functional data analysis and neural stochastic differential equations (SDEs) combined with a probability integral transform penalty to reduce model misspecification. We demonstrate that learning the joint dynamics of IV surfaces and prices produces market scenarios that are consistent with historical features and lie within the sub-manifold of surfaces that are essentially free of static arbitrage. Finally, we demonstrate that delta hedging using the simulated surfaces generates profit and loss (P&L) distributions that are consistent with realised P&Ls.
    
[^113]: 所有树的混合模型

    Mixtures of All Trees. (arXiv:2302.14202v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14202](http://arxiv.org/abs/2302.14202)

    提出了一种新的生成模型类别——所有树的混合模型——它在处理$n$个变量的情况时能实现最先进的性能，尽管其边际估计的精确计算是NP难问题。

    

    树形图模型因可计算性而被广泛使用，但不幸的是，它们在表达能力上存在局限性，因为它们需要承诺于特定的稀疏依赖结构中。我们提出了一种新的生成模型类别，称为所有树的混合模型：即对$n$个变量上的所有可能($n^{n-2}$)的树形图模型进行混合。我们展示了如何在可计算的方式下对混合模型(MoAT)进行紧凑的参数化（使用多项式大小的表示方式），从而实现了通过随机梯度下降优化的可计算似然值。此外，通过充分利用树形图模型的可计算性，我们设计了快速收敛的条件采样算法进行近似推理，尽管我们的理论分析表明，MoAT模型中边际的精确计算是NP难问题。实际上，在与强大的概率密度估计基准的比较中，MoAT实现了最先进的性能。

    Tree-shaped graphical models are widely used for their tractability. However, they unfortunately lack expressive power as they require committing to a particular sparse dependency structure. We propose a novel class of generative models called mixtures of all trees: that is, a mixture over all possible ($n^{n-2}$) tree-shaped graphical models over $n$ variables. We show that it is possible to parameterize this Mixture of All Trees (MoAT) model compactly (using a polynomial-size representation) in a way that allows for tractable likelihood computation and optimization via stochastic gradient descent. Furthermore, by leveraging the tractability of tree-shaped models, we devise fast-converging conditional sampling algorithms for approximate inference, even though our theoretical analysis suggests that exact computation of marginals in the MoAT model is NP-hard. Empirically, MoAT achieves state-of-the-art performance on density estimation benchmarks when compared against powerful probabili
    
[^114]: 论ChatGPT的鲁棒性：对抗性和超出分布的视角。

    On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective. (arXiv:2302.12095v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.12095](http://arxiv.org/abs/2302.12095)

    本研究评估了ChatGPT的鲁棒性，发现其在对抗性和超出分布任务上有一致的优势，但绝对表现仍有提高空间，鲁棒性仍是一个重要的挑战。

    

    ChatGPT是OpenAI最近发布的聊天机器人服务，并在过去几个月中受到越来越多的关注。虽然已对ChatGPT的各个方面进行了评估，但其鲁棒性，即对于未预期输入的表现，仍不清楚。鲁棒性在负责任的AI中特别受关注，特别是对于安全关键应用程序。在本文中，我们从对抗性和超出分布（OOD）的角度对ChatGPT的鲁棒性进行了彻底评估。为此，我们采用了AdvGLUE和ANLI基准来评估对抗性鲁棒性，采用Flipkart评论和DDXPlus医学诊断数据集进行OOD评估。我们选择了几个流行的基础模型作为基准。结果表明，ChatGPT在大多数对抗性和OOD分类和翻译任务上表现出一致的优势。但是，绝对的表现远非完美，这表明对抗性和OOD鲁棒性仍然是一个重要的威胁。

    ChatGPT is a recent chatbot service released by OpenAI and is receiving increasing attention over the past few months. While evaluations of various aspects of ChatGPT have been done, its robustness, i.e., the performance to unexpected inputs, is still unclear to the public. Robustness is of particular concern in responsible AI, especially for safety-critical applications. In this paper, we conduct a thorough evaluation of the robustness of ChatGPT from the adversarial and out-of-distribution (OOD) perspective. To do so, we employ the AdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart review and DDXPlus medical diagnosis datasets for OOD evaluation. We select several popular foundation models as baselines. Results show that ChatGPT shows consistent advantages on most adversarial and OOD classification and translation tasks. However, the absolute performance is far from perfection, which suggests that adversarial and OOD robustness remains a significant threat 
    
[^115]: 使用核心集的选择性经验回放压缩用于医学影像领域中的终身深度强化学习

    Selective experience replay compression using coresets for lifelong deep reinforcement learning in medical imaging. (arXiv:2302.11510v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11510](http://arxiv.org/abs/2302.11510)

    本文提出了一种使用核心集进行选择性经验回放压缩的技术，可以提升终身学习的效率，应用于医学影像领域。

    

    选择性经验回放是将深度强化学习与终身学习结合的一种流行策略。选择性经验回放旨在重述以前任务中的选择性经验，以避免灾难性遗忘。此外，基于选择性经验回放的技术是模型不可知的，并允许在不同模型之间共享经验。然而，存储所有以前任务的经验会使得使用选择性经验回放的终身学习变得计算上非常昂贵和不切实际，特别是随着任务数量的增加。

    Selective experience replay is a popular strategy for integrating lifelong learning with deep reinforcement learning. Selective experience replay aims to recount selected experiences from previous tasks to avoid catastrophic forgetting. Furthermore, selective experience replay based techniques are model agnostic and allow experiences to be shared across different models. However, storing experiences from all previous tasks make lifelong learning using selective experience replay computationally very expensive and impractical as the number of tasks increase. To that end, we propose a reward distribution-preserving coreset compression technique for compressing experience replay buffers stored for selective experience replay.  We evaluated the coreset compression technique on the brain tumor segmentation (BRATS) dataset for the task of ventricle localization and on the whole-body MRI for localization of left knee cap, left kidney, right trochanter, left lung, and spleen. The coreset lifel
    
[^116]: 基于子图Weisfeiler-Lehman Tests的子图GNN完整表达力层次结构

    A Complete Expressiveness Hierarchy for Subgraph GNNs via Subgraph Weisfeiler-Lehman Tests. (arXiv:2302.07090v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07090](http://arxiv.org/abs/2302.07090)

    本文提出了一个通过子图Weisfeiler-Lehman Tests（SWL）的视角对一般基于节点的子图GNN进行系统研究的方法。其中，我们建立了一个严格增长的SWL完整表达力层次结构，证明了任何基于节点的子图GNN都属于六个SWL等价类之一，其中SSWL实现了最大的表达能力。

    

    最近，子图GNN已成为开发具有表达力的图神经网络的重要方向。虽然已经提出了许多架构，但目前对各种设计范例的表达能力差异仍存在有限的理解，也不清楚什么设计原则能以最小的架构复杂度实现最大的表达能力。为了回答这些基本问题，本文通过子图Weisfeiler-Lehman Tests（SWL）的视角对一般基于节点的子图GNN进行了系统研究。我们的核心结果是建立了一个SWL完整层次结构，其表达能力是严格增长的。具体来说，我们证明了任何基于节点的子图GNN都属于六个SWL等价类之一，其中 $\mathsf{SSWL}$ 实现了最大的表达能力。我们还研究了这些等价类在编码图距离和双联通性等实际表达能力方面的差异。此外，我们提供了一个简单的方法来计算任何节点的SWL等价类以及整个图形的编码。

    Recently, subgraph GNNs have emerged as an important direction for developing expressive graph neural networks (GNNs). While numerous architectures have been proposed, so far there is still a limited understanding of how various design paradigms differ in terms of expressive power, nor is it clear what design principle achieves maximal expressiveness with minimal architectural complexity. To address these fundamental questions, this paper conducts a systematic study of general node-based subgraph GNNs through the lens of Subgraph Weisfeiler-Lehman Tests (SWL). Our central result is to build a complete hierarchy of SWL with strictly growing expressivity. Concretely, we prove that any node-based subgraph GNN falls into one of the six SWL equivalence classes, among which $\mathsf{SSWL}$ achieves the maximal expressive power. We also study how these equivalence classes differ in terms of their practical expressiveness such as encoding graph distance and biconnectivity. Furthermore, we give
    
[^117]: Jaccard度量损失：使用软标签优化Jaccard指数

    Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels. (arXiv:2302.05666v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.05666](http://arxiv.org/abs/2302.05666)

    本文提出了Jaccard度量损失（JMLs）来优化Jaccard指数，该损失在软标签下仍然有效。

    

    IoU损失是直接优化Jaccard指数的替代品。在语义分割中，将IoU损失作为损失函数的一部分，与仅优化像素损失（如交叉熵损失）相比，对于Jaccard指数测量表现更好。最显着的IoU损失是软Jaccard损失和Lovasz-Softmax损失。然而，这些损失与机器学习中普遍存在的软标签不兼容。在本文中，我们提出了Jaccard度量损失（JMLs），它们在标准设置下与软标签兼容，与软Jaccard损失相同。使用JMLs，我们研究了两种最流行的软标签用例：标签平滑和知识蒸馏。在三个语义分割数据集（Cityscapes、PASCAL VOC和DeepGlobe Land）上，我们的实验表明，与交叉熵损失相比，我们的简单方法显著提高了性能，并且在DeepGlobe Land数据集上超过了最先进的方法。

    IoU losses are surrogates that directly optimize the Jaccard index. In semantic segmentation, leveraging IoU losses as part of the loss function is shown to perform better with respect to the Jaccard index measure than optimizing pixel-wise losses such as the cross-entropy loss alone. The most notable IoU losses are the soft Jaccard loss and the Lovasz-Softmax loss. However, these losses are incompatible with soft labels which are ubiquitous in machine learning. In this paper, we propose Jaccard metric losses (JMLs), which are identical to the soft Jaccard loss in a standard setting with hard labels, but are compatible with soft labels. With JMLs, we study two of the most popular use cases of soft labels: label smoothing and knowledge distillation. With a variety of architectures, our experiments show significant improvements over the cross-entropy loss on three semantic segmentation datasets (Cityscapes, PASCAL VOC and DeepGlobe Land), and our simple approach outperforms state-of-the-
    
[^118]: 机器学习用于合成数据生成的综述

    Machine Learning for Synthetic Data Generation: A Review. (arXiv:2302.04062v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04062](http://arxiv.org/abs/2302.04062)

    机器学习用于合成数据生成的综述，探讨了合成数据生成的应用（计算机视觉、语音、自然语言、医疗保健和商业）、机器学习方法（神经网络架构和深度生成模型）以及隐私和公平问题，并提出了未来的研究方向。

    

    数据在机器学习中发挥着关键作用。然而，在实际应用中，数据存在多种问题，如数据质量低，有限的数据点导致机器学习模型欠拟合，由于隐私、安全和监管问题难以访问数据。合成数据生成提供了一种有前途的新途径，因为它可以以真实世界数据无法做到的方式进行共享和使用。本文系统地回顾了利用机器学习模型进行合成数据生成的现有工作。具体而言，我们从以下几个方面讨论合成数据生成的工作：（i）应用，包括计算机视觉、语音、自然语言、医疗保健和商业；（ii）机器学习方法，特别是神经网络架构和深度生成模型；（iii）隐私和公平问题。此外，我们还确定了这一新兴领域的挑战和机遇，并提出了未来的研究方向。

    Data plays a crucial role in machine learning. However, in real-world applications, there are several problems with data, e.g., data are of low quality; a limited number of data points lead to under-fitting of the machine learning model; it is hard to access the data due to privacy, safety and regulatory concerns. Synthetic data generation offers a promising new avenue, as it can be shared and used in ways that real-world data cannot. This paper systematically reviews the existing works that leverage machine learning models for synthetic data generation. Specifically, we discuss the synthetic data generation works from several perspectives: (i) applications, including computer vision, speech, natural language, healthcare, and business; (ii) machine learning methods, particularly neural network architectures and deep generative models; (iii) privacy and fairness issue. In addition, we identify the challenges and opportunities in this emerging field and suggest future research directions
    
[^119]: 动态时间规整的代价函数参数化及其在时间序列分类中的应用

    Parameterizing the cost function of Dynamic Time Warping with application to time series classification. (arXiv:2301.10350v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10350](http://arxiv.org/abs/2301.10350)

    本研究参数化了动态时间规整的代价函数并在时间序列分类任务中得到应用，相比原始的绝对值代价函数，研究还提出了可调整参数的代价函数，通过调整参数可以更好地适应不同的任务，且在多个标准时间序列分类数据集上得到验证。

    

    动态时间规整（DTW）是一种流行的时间序列距离度量方法，它将两个序列中的点进行对齐。这些对齐支持时间维度的扭曲，以允许以不同速度展开的过程。距离是任何可允许时间维度扭曲的最小对齐代价之和。两个点之间的对齐代价是这些点值之间差异的函数。原始代价函数是这种差异的绝对值。提出了其他代价函数。一种流行的替代方法是差值的平方。然而，据我们所知，这是对使用不同代价函数的相对影响和调整代价函数以适应不同任务潜力进行的第一次研究。本文使用可调整的代价函数{\lambda}{\gamma}，其中{\gamma}是参数，来实现此目的。我们表明，{\gamma}值越高，对较大的成对差异赋予更大的重要性，而{\gamma}值越低，则更加强调较小的差异。我们证明了一个调整的代价函数在几个基准时间序列分类数据集上，提高了使用DTW作为距离度量的最近邻分类器的准确性。

    Dynamic Time Warping (DTW) is a popular time series distance measure that aligns the points in two series with one another. These alignments support warping of the time dimension to allow for processes that unfold at differing rates. The distance is the minimum sum of costs of the resulting alignments over any allowable warping of the time dimension. The cost of an alignment of two points is a function of the difference in the values of those points. The original cost function was the absolute value of this difference. Other cost functions have been proposed. A popular alternative is the square of the difference. However, to our knowledge, this is the first investigation of both the relative impacts of using different cost functions and the potential to tune cost functions to different tasks. We do so in this paper by using a tunable cost function {\lambda}{\gamma} with parameter {\gamma}. We show that higher values of {\gamma} place greater weight on larger pairwise differences, while
    
[^120]: 在线核学习中的改进核对齐遗憾界

    Improved Kernel Alignment Regret Bound for Online Kernel Learning. (arXiv:2212.12989v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.12989](http://arxiv.org/abs/2212.12989)

    本文提出了一种新的算法，在线核学习中的新算法遗憾界和计算复杂度优于以前的结果。该算法的遗憾界和计算复杂度取决于核矩阵特征值的衰减率。

    

    本文针对Hinge损失函数下的在线核学习，改进了核对齐遗憾界。我们提出了一种算法，其遗憾界和计算复杂度优于以前的结果。如果核矩阵的特征值呈指数衰减，则我们的算法在计算复杂度为$O(\ln^2{T})$，遗憾界为$O(\sqrt{\mathcal{A}_T})$。否则，我们的算法在计算复杂度为$O(\sqrt{\mathcal{A}_TT})$，遗憾界为$O((\mathcal{A}_TT)^{\frac{1}{4}})$。我们将算法扩展到批量学习，并获得了$O(\frac{1}{T}\sqrt{\mathbb{E}[\mathcal{A}_T]})$的余量风险界，取得了改进。

    In this paper, we improve the kernel alignment regret bound for online kernel learning in the regime of the Hinge loss function. Previous algorithm achieves a regret of $O((\mathcal{A}_TT\ln{T})^{\frac{1}{4}})$ at a computational complexity (space and per-round time) of $O(\sqrt{\mathcal{A}_TT\ln{T}})$, where $\mathcal{A}_T$ is called \textit{kernel alignment}. We propose an algorithm whose regret bound and computational complexity are better than previous results. Our results depend on the decay rate of eigenvalues of the kernel matrix. If the eigenvalues of the kernel matrix decay exponentially, then our algorithm enjoys a regret of $O(\sqrt{\mathcal{A}_T})$ at a computational complexity of $O(\ln^2{T})$. Otherwise, our algorithm enjoys a regret of $O((\mathcal{A}_TT)^{\frac{1}{4}})$ at a computational complexity of $O(\sqrt{\mathcal{A}_TT})$. We extend our algorithm to batch learning and obtain a $O(\frac{1}{T}\sqrt{\mathbb{E}[\mathcal{A}_T]})$ excess risk bound which improves the p
    
[^121]: HAC-Net:一种基于注意力机制的混合卷积神经网络，用于高精度的蛋白质配体结合亲和力预测。

    HAC-Net: A Hybrid Attention-Based Convolutional Neural Network for Highly Accurate Protein-Ligand Binding Affinity Prediction. (arXiv:2212.12440v3 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2212.12440](http://arxiv.org/abs/2212.12440)

    本文提出了一种新的深度学习架构HAC-Net，结合了三维卷积神经网络和两个图卷积网络的注意力机制，能够高精度地预测蛋白质与配体的结合亲和力，并在公认的基准测试集上获得最先进的结果。

    

    应用图像检测和图论中的深度学习概念极大地推进了蛋白-配体结合亲和力的预测，这是药物发现和蛋白工程领域的重要挑战。我们设计了一种新型深度学习架构HAC-Net，包括一个三维卷积神经网络和两个基于注意力聚合节点特征的图卷积网络。在蛋白质配体结合亲和力预测的领域公认的基准测试集PDBbind v.2016 core set上，HAC-Net获得了最先进的结果。我们还使用多个训练-测试划分全面评估模型的普适性，每个划分都最大化了训练集和测试集复合物的蛋白结构、蛋白序列或配体扩展连接指纹之间的差异。此外，我们使用相似性测度进行了十倍交叉验证。

    Applying deep learning concepts from image detection and graph theory has greatly advanced protein-ligand binding affinity prediction, a challenge with enormous ramifications for both drug discovery and protein engineering. We build upon these advances by designing a novel deep learning architecture consisting of a 3-dimensional convolutional neural network utilizing channel-wise attention and two graph convolutional networks utilizing attention-based aggregation of node features. HAC-Net (Hybrid Attention-Based Convolutional Neural Network) obtains state-of-the-art results on the PDBbind v.2016 core set, the most widely recognized benchmark in the field. We extensively assess the generalizability of our model using multiple train-test splits, each of which maximizes differences between either protein structures, protein sequences, or ligand extended-connectivity fingerprints of complexes in the training and test sets. Furthermore, we perform 10-fold cross-validation with a similarity 
    
[^122]: 使用任务算术编辑模型

    Editing Models with Task Arithmetic. (arXiv:2212.04089v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.04089](http://arxiv.org/abs/2212.04089)

    本文提出了一种使用任务向量进行模型编辑的新范式，任务向量可通过算术操作进行修改和组合，可以提高目标任务性能且对控制任务影响较小。

    

    改变预训练模型的行为方式（比如提高其在下游任务上的表现或减轻预训练期间学习到的偏差）是开发机器学习系统时常见的做法。本文提出了一种围绕“任务向量”来引导神经网络行为的新范式。任务向量指定了一个方向，即预训练模型权重空间中的方向，沿着该方向移动可以提高任务的表现。我们通过从经过微调任务后的相同模型的权重中减去预训练模型的权重来构建任务向量。我们展示了这些任务向量可以通过否定和加法等算术操作进行修改和组合，从而引导生成模型的行为。否定任务向量会降低目标任务的性能，而对控制任务的模型行为影响不大。此外，将任务向量相加可以提高目标任务的性能和控制任务的模型行为。

    Changing how pre-trained models behave -- e.g., improving their performance on a downstream task or mitigating biases learned during pre-training -- is a common practice when developing machine learning systems. In this work, we propose a new paradigm for steering the behavior of neural networks, centered around \textit{task vectors}. A task vector specifies a direction in the weight space of a pre-trained model, such that movement in that direction improves performance on the task. We build task vectors by subtracting the weights of a pre-trained model from the weights of the same model after fine-tuning on a task. We show that these task vectors can be modified and combined together through arithmetic operations such as negation and addition, and the behavior of the resulting model is steered accordingly. Negating a task vector decreases performance on the target task, with little change in model behavior on control tasks. Moreover, adding task vectors together can improve performanc
    
[^123]: 非对比学习的链路预测

    Link Prediction with Non-Contrastive Learning. (arXiv:2211.14394v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.14394](http://arxiv.org/abs/2211.14394)

    本文介绍了一种新的非对比学习方法，用于链路预测任务，该方法可以在提高节点表示性能的同时减少负样本采样的挑战。

    

    图神经网络（GNN）的一个最新热点是图自监督学习（SSL），旨在推导出没有标签数据的有用节点表示。尽管许多最先进的图形SSL方法都是对比方法，但由于负样本采样的挑战（速度缓慢和模型敏感性），最近的文献引入了非对比学习方法，这些方法只使用正样本。本文评估了现有的非对比方法在链路预测中的性能，包括传递性和归纳性。虽然大多数现有的非对比方法表现不佳，但我们提出了一种新的SSL方法来在链路预测中提高节点表示的性能。

    A recent focal area in the space of graph neural networks (GNNs) is graph self-supervised learning (SSL), which aims to derive useful node representations without labeled data. Notably, many state-of-the-art graph SSL methods are contrastive methods, which use a combination of positive and negative samples to learn node representations. Owing to challenges in negative sampling (slowness and model sensitivity), recent literature introduced non-contrastive methods, which instead only use positive samples. Though such methods have shown promising performance in node-level tasks, their suitability for link prediction tasks, which are concerned with predicting link existence between pairs of nodes (and have broad applicability to recommendation systems contexts) is yet unexplored. In this work, we extensively evaluate the performance of existing non-contrastive methods for link prediction in both transductive and inductive settings. While most existing non-contrastive methods perform poorly
    
[^124]: 基于图卷积网络的密码货币信任网络中的欺诈检测方法

    Motif-aware temporal GCN for fraud detection in signed cryptocurrency trust networks. (arXiv:2211.13123v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.13123](http://arxiv.org/abs/2211.13123)

    本文提出了一种基于图卷积网络和平衡理论的加密货币信任网络欺诈检测方法，并使用模式矩阵捕捉局部拓扑信息。实验结果表明，该方法在真实数据和合成数据集上均优于现有方法。

    

    图卷积网络是一种处理可表示为图的数据的人工神经网络。由于金融交易可以自然地构造成图形，因此GCN在金融行业中得到了广泛应用，特别是在金融欺诈检测方面。本文重点研究加密货币信任网络中的欺诈检测。在现有文献中，大部分工作都集中在静态图上。而在本研究中，我们考虑了加密货币网络的演化特性，并利用本地结构和平衡理论来指导训练过程。具体而言，我们计算动态网络中的模式矩阵来捕捉局部拓扑信息，然后在GCN聚合过程中使用它们。每个快照生成的嵌入是时间窗口内嵌入的加权平均值，其中权重是可学习的参数。由于信任网络在每个边缘上都有签名，因此使用平衡理论来指导训练过程。实验结果表明，我们提出的基于模式感知的时态GCN在真实数据和合成数据集上均比现有的方法具有优异的性能。

    Graph convolutional networks (GCNs) is a class of artificial neural networks for processing data that can be represented as graphs. Since financial transactions can naturally be constructed as graphs, GCNs are widely applied in the financial industry, especially for financial fraud detection. In this paper, we focus on fraud detection on cryptocurrency truct networks. In the literature, most works focus on static networks. Whereas in this study, we consider the evolving nature of cryptocurrency networks, and use local structural as well as the balance theory to guide the training process. More specifically, we compute motif matrices to capture the local topological information, then use them in the GCN aggregation process. The generated embedding at each snapshot is a weighted average of embeddings within a time window, where the weights are learnable parameters. Since the trust networks is signed on each edge, balance theory is used to guide the training process. Experimental results 
    
[^125]: 单次对比学习可以适用于同构和异构图

    Single-Pass Contrastive Learning Can Work for Both Homophilic and Heterophilic Graph. (arXiv:2211.10890v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.10890](http://arxiv.org/abs/2211.10890)

    该论文介绍了一种单次对比学习方法，可适用于同构和异构图，并给出了性能保证。

    

    现有的图对比学习（GCL）技术通常需要两次前向传递才能构建产生对比的损失，这对于捕捉节点特征的低频信号是有效的。这种双通道设计已经在同构图上表现出实证的成功，但它在异构图上的有效性，其中直接连接的节点通常具有不同的标签，尚不清楚。此外，现有的GCL方法无法提供强有力的性能保证。受到GCL方法在异构图上的不可预测性的影响，它们在实际应用中的适用性受到限制。于是，一个自然的问题就出现了：我们能否设计一种适用于同构和异构图的GCL方法，并具有性能保证？为了回答这个问题，我们在理论上研究了在同构和异构图上通过邻域聚合得到的特征的集中性质，引入了单通道图对比学习损失。

    Existing graph contrastive learning (GCL) techniques typically require two forward passes for a single instance to construct the contrastive loss, which is effective for capturing the low-frequency signals of node features. Such a dual-pass design has shown empirical success on homophilic graphs, but its effectiveness on heterophilic graphs, where directly connected nodes typically have different labels, is unknown. In addition, existing GCL approaches fail to provide strong performance guarantees. Coupled with the unpredictability of GCL approaches on heterophilic graphs, their applicability in real-world contexts is limited. Then, a natural question arises: Can we design a GCL method that works for both homophilic and heterophilic graphs with a performance guarantee? To answer this question, we theoretically study the concentration property of features obtained by neighborhood aggregation on homophilic and heterophilic graphs, introduce the single-pass graph contrastive learning loss
    
[^126]: 带有卷积高斯神经过程的环境传感器放置

    Environmental Sensor Placement with Convolutional Gaussian Neural Processes. (arXiv:2211.10381v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.10381](http://arxiv.org/abs/2211.10381)

    本论文提出了一种新的方式——卷积高斯神经过程（ConvGNP），用于提高环境传感器的放置效率。ConvGNP使用神经网络来参数化联合高斯分布，通过学习空间和季节性非平稳性，优于传统的非平稳高斯过程模型。

    

    环境传感器对于监测天气和气候变化的影响至关重要。然而，在像南极这样的偏远地区，最大化测量信息和有效放置传感器是具有挑战性的。概率机器学习模型可以通过预测新传感器提供的不确定性减少来评估放置信息。高斯过程模型广泛用于此目的，但难以捕捉复杂的非平稳行为并缩放到大型数据集。本文提出使用卷积高斯神经过程（ConvGNP）来解决这些问题。ConvGNP使用神经网络来参数化任意目标位置的联合高斯分布，实现了灵活性和可扩展性。使用模拟的南极地区地面温度异常作为真实数据，ConvGNP学习了空间和季节性非平稳性，并优于非平稳GP基线。在模拟的s中，

    Environmental sensors are crucial for monitoring weather conditions and the impacts of climate change. However, it is challenging to maximise measurement informativeness and place sensors efficiently, particularly in remote regions like Antarctica. Probabilistic machine learning models can evaluate placement informativeness by predicting the uncertainty reduction provided by a new sensor. Gaussian process (GP) models are widely used for this purpose, but they struggle with capturing complex non-stationary behaviour and scaling to large datasets. This paper proposes using a convolutional Gaussian neural process (ConvGNP) to address these issues. A ConvGNP uses neural networks to parameterise a joint Gaussian distribution at arbitrary target locations, enabling flexibility and scalability. Using simulated surface air temperature anomaly over Antarctica as ground truth, the ConvGNP learns spatial and seasonal non-stationarities, outperforming a non-stationary GP baseline. In a simulated s
    
[^127]: 基于深度强化学习的RIS辅助MU-MISO系统中下行波束成形和RIS配置

    Deep Reinforcement Learning Based Joint Downlink Beamforming and RIS Configuration in RIS-aided MU-MISO Systems Under Hardware Impairments and Imperfect CSI. (arXiv:2211.09702v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2211.09702](http://arxiv.org/abs/2211.09702)

    本文提出了一种DRL方法，联合优化了MU-MISO系统中的波束成形和RIS相位移位，以最大化总下行速率。在解决了不完美的CSI和硬件失调的挑战后，该方法显著优于基础DRL代理，并且是第一篇应用DRL优化波束成形和RIS配置的工作。

    

    本文提出一种新颖的深度强化学习(DRL)方法，用于在相位依赖反射振幅模型下，联合优化多用户多输入单输出(MU-MISO)系统中的发射波束成形和可重构智能表面(RIS)相位移位，以最大化总下行速率。我们的方法通过考虑实际的RIS幅度模型，解决了不完美的信道状态信息(CSI)和硬件失调的挑战。我们将我们的方法的性能与一个基础的DRL代理在两种情况下进行了比较：完美的CSI和相位依赖的RIS幅度，以及不匹配的CSI和理想的RIS反射。结果表明，在不匹配的情况下，所提出的框架显著优于基础的DRL代理，并接近黄金标准。我们的贡献包括修改DRL方法，以解决发射波束成形和相位移位的联合设计以及相位依赖幅度模型。据我们所知，这是第一篇将DRL应用于处理硬件失调和不完美CSI的MU-MISO系统中，联合优化波束成形和RIS配置的工作。

    We introduce a novel deep reinforcement learning (DRL) approach to jointly optimize transmit beamforming and reconfigurable intelligent surface (RIS) phase shifts in a multiuser multiple input single output (MU-MISO) system to maximize the sum downlink rate under the phase-dependent reflection amplitude model. Our approach addresses the challenge of imperfect channel state information (CSI) and hardware impairments by considering a practical RIS amplitude model. We compare the performance of our approach against a vanilla DRL agent in two scenarios: perfect CSI and phase-dependent RIS amplitudes, and mismatched CSI and ideal RIS reflections. The results demonstrate that the proposed framework significantly outperforms the vanilla DRL agent under mismatch and approaches the golden standard. Our contributions include modifications to the DRL approach to address the joint design of transmit beamforming and phase shifts and the phase-dependent amplitude model. To the best of our knowledge,
    
[^128]: 神经网络中的 emergent 语言结构是脆弱的

    Emergent Linguistic Structures in Neural Networks are Fragile. (arXiv:2210.17406v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.17406](http://arxiv.org/abs/2210.17406)

    本文提出了一个框架来评估语言模型对于语法的表述的一致性和稳健性，通过多项实验证据表明，神经网络中 emergent 语言结构是脆弱的。

    

    大型语言模型（LLM）在自然语言处理任务中表现强劲。然而，准确度等性能指标并不能衡量模型在代表复杂语言结构方面的质量。本文针对语言模型代表语法的能力，提出了一个评估语言表述的一致性和稳健性的框架。为此，我们介绍了一些稳健性的神经网络模型度量方式，这些度量方式利用最近在通过探测任务从LLM中提取语言结构的先进技术，即用于从语言模型中提取有意义信息的简单任务，如语法重构和根识别。实证上，我们通过分析四种LLM在六个不同的语料库上对语法保持扰动的性能和稳健性来研究所提出的稳健度量方式的表现。我们提供了证据

    Large Language Models (LLMs) have been reported to have strong performance on natural language processing tasks. However, performance metrics such as accuracy do not measure the quality of the model in terms of its ability to robustly represent complex linguistic structure. In this paper, focusing on the ability of language models to represent syntax, we propose a framework to assess the consistency and robustness of linguistic representations. To this end, we introduce measures of robustness of neural network models that leverage recent advances in extracting linguistic constructs from LLMs via probing tasks, i.e., simple tasks used to extract meaningful information about a single facet of a language model, such as syntax reconstruction and root identification. Empirically, we study the performance of four LLMs across six different corpora on the proposed robustness measures by analysing their performance and robustness with respect to syntax-preserving perturbations. We provide evide
    
[^129]: 解决具有一般形式约束的变分不等式的原始-对偶方法

    A Primal-dual Approach for Solving Variational Inequalities with General-form Constraints. (arXiv:2210.15659v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.15659](http://arxiv.org/abs/2210.15659)

    本文介绍了一种解决具有一般形式约束的变分不等式的原始-对偶方法，并使用一种温启动技术，在每次迭代中近似地解决子问题，我们证明了它的收敛性，并展示了它的收敛速度比其精确对应物更快。

    

    Yang等人最近通过一种一阶梯度方法解决了具有等式和不等式约束的变分不等式（VI）的问题。但是，提出的原始-对偶方法称为ACVI仅适用于可以计算其子问题的解析解的情况，因此一般情况仍然是一个开放的问题。在本文中，我们采用一种温启动技术，在每次迭代中近似地解决子问题，并使用在先前迭代中找到的近似解初始化变量。我们证明了它的收敛性，并表明当算子为$L$-Lipschitz且单调时，这种不精确的ACVI方法的最后一次迭代的间隙函数下降的速度为$\mathcal{O}(\frac{1}{\sqrt{K}})$，前提是错误以适当的速度下降。有趣的是，我们展示了在数值实验中，通常这种技术比其精确对应物收敛更快。此外，对于不等式约束的情况

    Yang et al. (2023) recently addressed the open problem of solving Variational Inequalities (VIs) with equality and inequality constraints through a first-order gradient method. However, the proposed primal-dual method called ACVI is applicable when we can compute analytic solutions of its subproblems; thus, the general case remains an open problem. In this paper, we adopt a warm-starting technique where we solve the subproblems approximately at each iteration and initialize the variables with the approximate solution found at the previous iteration. We prove its convergence and show that the gap function of the last iterate of this inexact-ACVI method decreases at a rate of $\mathcal{O}(\frac{1}{\sqrt{K}})$ when the operator is $L$-Lipschitz and monotone, provided that the errors decrease at appropriate rates. Interestingly, we show that often in numerical experiments, this technique converges faster than its exact counterpart. Furthermore, for the cases when the inequality constraints
    
[^130]: 带有适宜的归纳偏差增强机器抽象推理能力的多视角和多评估方法

    Multi-Viewpoint and Multi-Evaluation with Felicitous Inductive Bias Boost Machine Abstract Reasoning Ability. (arXiv:2210.14914v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14914](http://arxiv.org/abs/2210.14914)

    本文研究了机器在抽象推理方面的能力，证明了具有适宜的归纳偏差的神经网络可以优雅地解决RPM问题，并揭示了多视角和多评估是成功推理的关键学习策略。

    

    长期以来，人们一直致力于研究人工智能在抽象推理方面的能力，RAVEN渐进矩阵（RPM）的不同版本被提出作为基准测试。以往的工作表明，如果没有复杂的设计或包含语义信息的额外元数据，神经网络在训练后在RPM问题的决策方面仍可能犹豫不决。通过彻底的实验和消融研究，我们展示了具有适宜的归纳偏差的端到端神经网络可以优雅地解决RPM问题，而无需增加任何额外的元数据或特定骨干的偏好。我们的工作还揭示了多视角和多评估是成功推理的关键学习策略。最后，我们提供了关于连接模型在泛化方面失败的潜在解释。我们希望这些结果将作为人工智能能力检查的指标。

    Great endeavors have been made to study AI's ability in abstract reasoning, along with which different versions of RAVEN's progressive matrices (RPM) are proposed as benchmarks. Previous works give inkling that without sophisticated design or extra meta-data containing semantic information, neural networks may still be indecisive in making decisions regarding RPM problems, after relentless training. Evidenced by thorough experiments and ablation studies, we showcase that end-to-end neural networks embodied with felicitous inductive bias, intentionally design or serendipitously match, can solve RPM problems elegantly, without the augment of any extra meta-data or preferences of any specific backbone. Our work also reveals that multi-viewpoint with multi-evaluation is a key learning strategy for successful reasoning. Finally, potential explanations for the failure of connectionist models in generalization are provided. We hope that these results will serve as inspections of AI's ability 
    
[^131]: 代码生成模型的多语言评估

    Multi-lingual Evaluation of Code Generation Models. (arXiv:2210.14868v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14868](http://arxiv.org/abs/2210.14868)

    本研究提出了多种用于评估代码生成模型的新基准测试，能够以多语言方式评估模型性能，并探讨了多语言模型优于单语言模型、少量提示能教授模型新语言以及在单语言设置下拥有零-shot翻译能力等问题。

    

    我们提出了新的基准测试，用于评估代码生成模型：MBXP和Multilingual HumanEval，以及MathQA-X。这些数据集涵盖了10种以上的编程语言，并使用可扩展的转换框架将原始Python数据集中的提示和测试用例转译成目标语言中的相应数据。利用这些基准测试，我们能够以多语言方式评估代码生成模型的性能，并发现了语言模型在跨领域语言上的泛化能力、多语言模型在单语言模型上的优势、少量提示教授模型新语言的能力，以及在单语言设置下的零-shot翻译能力。此外，我们使用我们的代码生成模型进行大规模引导，以获取多种语言的合成规范解，这些解可用于其他与代码相关的评估，如代码插入、鲁棒性或摘要任务。总的来说，

    We present new benchmarks on evaluation code generation models: MBXP and Multilingual HumanEval, and MathQA-X. These datasets cover over 10 programming languages and are generated using a scalable conversion framework that transpiles prompts and test cases from the original Python datasets into the corresponding data in the target language. Using these benchmarks, we are able to assess the performance of code generation models in a multi-lingual fashion, and discovered generalization ability of language models on out-of-domain languages, advantages of multi-lingual models over mono-lingual, the ability of few-shot prompting to teach the model new languages, and zero-shot translation abilities even on mono-lingual settings. Furthermore, we use our code generation model to perform large-scale bootstrapping to obtain synthetic canonical solutions in several languages, which can be used for other code-related evaluations such as code insertion, robustness, or summarization tasks. Overall, 
    
[^132]: 在高维高斯潜在混合模型中插值判别函数

    Interpolating Discriminant Functions in High-Dimensional Gaussian Latent Mixtures. (arXiv:2210.14347v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.14347](http://arxiv.org/abs/2210.14347)

    本文提出一种在高维特征下进行二元分类的方法，使用广义最小二乘估计器估计最佳分离超平面的方向，惊人地发现超平面的插值取决于标签的编码方式。

    

    本文考虑在低维潜在高斯混合结构和非零噪声的假设模型下针对高维特征的二元分类问题。使用广义最小二乘估计器来估计最佳分离超平面的方向。所估计的超平面在训练数据上被证明是插值的。虽然正如线性回归的最近结果所预期的那样，方向向量可以一致地估计，但一个天真的插件估计未能一致地估计拦截。一个简单的校正需要一个独立的保留样本，在许多场景中可以使过程保持最小化最大值最优。后续过程的插值性质可以保留，但令人惊讶的是，它取决于标签的编码方式。

    This paper considers binary classification of high-dimensional features under a postulated model with a low-dimensional latent Gaussian mixture structure and non-vanishing noise. A generalized least squares estimator is used to estimate the direction of the optimal separating hyperplane. The estimated hyperplane is shown to interpolate on the training data. While the direction vector can be consistently estimated as could be expected from recent results in linear regression, a naive plug-in estimate fails to consistently estimate the intercept. A simple correction, that requires an independent hold-out sample, renders the procedure minimax optimal in many scenarios. The interpolation property of the latter procedure can be retained, but surprisingly depends on the way the labels are encoded.
    
[^133]: 实例感知图像修复

    Instance-Aware Image Completion. (arXiv:2210.12350v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.12350](http://arxiv.org/abs/2210.12350)

    本文提出了一个实例感知图像修复模型ImComplete，相比现有方法，它可以幻象出与环境背景相协调的视觉实例，提供了基于语义和结构的像素级指导。

    

    图像修复是一项旨在填补带有缺失区域的图像的任务，使它们具有合理的内容。然而，现有的图像修复方法往往通过填充周围纹理来填补缺失区域，而不是去幻象一个与环境背景相协调的视觉实例。在本研究中，我们提出了一种新的图像修复模型，名为ImComplete，该模型可以幻象缺失的实例，从而与原始背景协调。ImComplete首先采用了一个变压器架构，考虑到可见实例和缺失区域的位置。然后，ImComplete完成了缺失区域内的语义分割掩模，提供像素级的语义和结构指导。最后，图像合成块生成了逼真的内容。

    Image completion is a task that aims to fill in the missing region of a masked image with plausible contents. However, existing image completion methods tend to fill in the missing region with the surrounding texture instead of hallucinating a visual instance that is suitable in accordance with the context of the scene. In this work, we propose a novel image completion model, dubbed ImComplete, that hallucinates the missing instance that harmonizes well with - and thus preserves - the original context. ImComplete first adopts a transformer architecture that considers the visible instances and the location of the missing region. Then, ImComplete completes the semantic segmentation masks within the missing region, providing pixel-level semantic and structural guidance. Finally, the image synthesis blocks generate photo-realistic content. We perform a comprehensive evaluation of the results in terms of visual quality (LPIPS and FID) and contextual preservation scores (CLIPscore and object
    
[^134]: 双重图像和视频Transformer的应用在多标签电影预告片类型分类中，提升了迁移学习

    Improving Transfer Learning with a Dual Image and Video Transformer for Multi-label Movie Trailer Genre Classification. (arXiv:2210.07983v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.07983](http://arxiv.org/abs/2210.07983)

    本文研究了ImageNet和Kinetics预先训练的模型在多标签电影预告片类型分类中的可迁移性，通过Dual Image and Video Transformer Architecture提高了可迁移性。

    

    本文研究了ImageNet空间和Kinetics时空表示对于多标签电影预告片类型分类（MTGC）的可迁移性。我们评估了在ImageNet和Kinetics预先训练的ConvNet和Transformer模型的可迁移性，并在Trailers12k上进行了展示，Trailers12k是一个由12,000个视频组成的手工标注的电影预告片数据集，并带有10种不同的类型和相关元数据的标签。我们分析了可以影响可迁移性的不同方面，例如帧率，输入视频扩展和时空建模。为了缩小ImageNet/Kinetics和Trailers12k之间的时空结构差距，我们提出了Dual Image and Video Transformer Architecture (DIViTA)，该架构执行拍摄检测，将预告片分割成高度相关的片段，为预先训练的骨干提供更连贯的输入，提高了可迁移性（ImageNet提高了1.83%，Kinetics提高了3.75%）。我们的结果...

    In this paper, we study the transferability of ImageNet spatial and Kinetics spatio-temporal representations to multi-label Movie Trailer Genre Classification (MTGC). In particular, we present an extensive evaluation of the transferability of ConvNet and Transformer models pretrained on ImageNet and Kinetics to Trailers12k, a new manually-curated movie trailer dataset composed of 12,000 videos labeled with 10 different genres and associated metadata. We analyze different aspects that can influence transferability, such as frame rate, input video extension, and spatio-temporal modeling. In order to reduce the spatio-temporal structure gap between ImageNet/Kinetics and Trailers12k, we propose Dual Image and Video Transformer Architecture (DIViTA), which performs shot detection so as to segment the trailer into highly correlated clips, providing a more cohesive input for pretrained backbones and improving transferability (a 1.83% increase for ImageNet and 3.75% for Kinetics). Our results 
    
[^135]: 来自演示的快速生涯适应性逆强化学习

    Fast Lifelong Adaptive Inverse Reinforcement Learning from Demonstrations. (arXiv:2209.11908v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.11908](http://arxiv.org/abs/2209.11908)

    本文提出了一种快速生涯适应性逆强化学习框架，从学习的策略中构建多样策略的组合实现了对新的演示的快速适应，同时整合演示中的共性知识，实现准确的任务推断，还能够在大规模部署中通过维护一个精简的原型策略集合并通过策略组合来逼近所有行为。

    

    演示学习（LfD）方法使终端用户通过所需行为的演示来教授机器人新任务，从而使机器人技术的使用面更广。然而，当前的LfD框架无法快速适应异构的人类演示，也不能在普适的机器人应用中进行大规模部署。在本文中，我们提出了一种新的LfD框架——快速生涯适应性逆强化学习（FLAIR）。我们的方法(1)利用学习策略构建多样策略的组合，从而快速适应新的演示，允许快速的终端用户个性化，(2)整合演示中的共性知识，实现准确的任务推断；(3)在终身部署中只在需要时扩展其模型，通过策略组合维护一个精简的原型策略集合，并能够逼近所有行为。我们在实验证明，FLAIR实现了适应性（即机器人适应了异构的、特定于用户的任务）并且在大规模部署中节省了模型大小。

    Learning from Demonstration (LfD) approaches empower end-users to teach robots novel tasks via demonstrations of the desired behaviors, democratizing access to robotics. However, current LfD frameworks are not capable of fast adaptation to heterogeneous human demonstrations nor the large-scale deployment in ubiquitous robotics applications. In this paper, we propose a novel LfD framework, Fast Lifelong Adaptive Inverse Reinforcement learning (FLAIR). Our approach (1) leverages learned strategies to construct policy mixtures for fast adaptation to new demonstrations, allowing for quick end-user personalization, (2) distills common knowledge across demonstrations, achieving accurate task inference; and (3) expands its model only when needed in lifelong deployments, maintaining a concise set of prototypical strategies that can approximate all behaviors via policy mixtures. We empirically validate that FLAIR achieves adaptability (i.e., the robot adapts to heterogeneous, user-specific task
    
[^136]: 二次梯度：将梯度算法和牛顿法融合为一体

    Quadratic Gradient: Combining Gradient Algorithms and Newton's Method as One. (arXiv:2209.03282v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2209.03282](http://arxiv.org/abs/2209.03282)

    本文提出了一种基于对角矩阵的二次梯度，可以加速梯度的收敛速度，在实验中表现良好。研究者还推测海森矩阵与学习率之间可能存在关系。

    

    使用一列与梯度相同大小的列向量，而不是仅使用一个浮点数来加速每个梯度元素的不同速率，可能对牛顿法的线搜索技术不足。此外，使用一个与海森矩阵大小相同的正方形矩阵来纠正海森矩阵可能是有用的。Chiang提出了一种介于列向量和正方形矩阵之间的东西，即对角矩阵，来加速梯度，并进一步提出了一种更快的梯度变体，称为二次梯度。在本文中，我们提出一种构建新版本的二次梯度的新方法。这个新的二次梯度不满足固定海森牛顿法的收敛条件。然而，实验结果显示，它有时比原始方法的收敛速度更快。此外，Chiang推测海森矩阵与学习率f之间可能存在关系。

    It might be inadequate for the line search technique for Newton's method to use only one floating point number. A column vector of the same size as the gradient might be better than a mere float number to accelerate each of the gradient elements with different rates. Moreover, a square matrix of the same order as the Hessian matrix might be helpful to correct the Hessian matrix. Chiang applied something between a column vector and a square matrix, namely a diagonal matrix, to accelerate the gradient and further proposed a faster gradient variant called quadratic gradient. In this paper, we present a new way to build a new version of the quadratic gradient. This new quadratic gradient doesn't satisfy the convergence conditions of the fixed Hessian Newton's method. However, experimental results show that it sometimes has a better performance than the original one in convergence rate. Also, Chiang speculates that there might be a relation between the Hessian matrix and the learning rate f
    
[^137]: 当攻击图形结构时，梯度告诉我们什么

    What Does the Gradient Tell When Attacking the Graph Structure. (arXiv:2208.12815v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.12815](http://arxiv.org/abs/2208.12815)

    本文研究了图神经网络中针对图形结构的对抗攻击，发现攻击者更倾向于增加类间边缘，通过连接不同类的节点来更有效地破坏节点特征。然而，GNN 的固有平滑性会导致在前向过程中丢失关键信息。为此，我们提出了一个具有多级传播的新型代理模型来解决这个问题。

    

    最近的研究表明，图神经网络 (GNNs) 容易受到针对图形结构的对抗攻击。一个恶意攻击者可以在有限的边缘范围内，通过给出的训练标签，来破坏受害模型的性能。之前的经验性研究表明，基于梯度的攻击者更倾向于添加边缘而不是删除。本文提出了一个理论证明，揭示了攻击者倾向于增加类间边缘，这是由于 GNN 的消息传递机制，这也解释了之前的一些经验观察。通过连接不同类的节点，攻击者可以更有效地破坏节点特征，从而使此类攻击更具优势。但是，我们证明了 GNN 消息传递的固有平滑性会将特征空间中的节点差异模糊化，导致在前向过程中丢失关键信息。为了解决这个问题，我们提出了一个具有多级传播的新型代理模型。

    Recent research has revealed that Graph Neural Networks (GNNs) are susceptible to adversarial attacks targeting the graph structure. A malicious attacker can manipulate a limited number of edges, given the training labels, to impair the victim model's performance. Previous empirical studies indicate that gradient-based attackers tend to add edges rather than remove them. In this paper, we present a theoretical demonstration revealing that attackers tend to increase inter-class edges due to the message passing mechanism of GNNs, which explains some previous empirical observations. By connecting dissimilar nodes, attackers can more effectively corrupt node features, making such attacks more advantageous. However, we demonstrate that the inherent smoothness of GNN's message passing tends to blur node dissimilarity in the feature space, leading to the loss of crucial information during the forward process. To address this issue, we propose a novel surrogate model with multi-level propagati
    
[^138]: 通过二次梯度的多项式逻辑回归算法

    Multinomial Logistic Regression Algorithms via Quadratic Gradient. (arXiv:2208.06828v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.06828](http://arxiv.org/abs/2208.06828)

    本文扩展了二次梯度的方法到多类逻辑回归，提出了一个增强的自适应梯度算法，实验结果表明，这两种增强方法能够分别比它们的原始方法更快地收敛。

    

    多项式逻辑回归是一种基础的分类方法，它将二元逻辑回归推广到多类问题。最近，一项工作提出了称为“二次梯度”的更快梯度，可以加速二元逻辑回归训练，并提出了一种增强的Nesterov加速梯度（NAG）方法用于二元逻辑回归。本文将这项工作扩展到多类逻辑回归，并提出了一种增强的自适应梯度算法(Adagrad)，可以加速原始Adagrad方法。我们在一些多类问题数据集上测试了增强的NAG方法和增强的Adagrad方法。实验结果显示，这两种增强方法分别比它们的原始方法更快地收敛。

    Multinomial logistic regression, also known by other names such as multiclass logistic regression and softmax regression, is a fundamental classification method that generalizes binary logistic regression to multiclass problems. A recently work proposed a faster gradient called $\texttt{quadratic gradient}$ that can accelerate the binary logistic regression training, and presented an enhanced Nesterov's accelerated gradient (NAG) method for binary logistic regression.  In this paper, we extend this work to multiclass logistic regression and propose an enhanced Adaptive Gradient Algorithm (Adagrad) that can accelerate the original Adagrad method. We test the enhanced NAG method and the enhanced Adagrad method on some multiclass-problem datasets. Experimental results show that both enhanced methods converge faster than their original ones respectively.
    
[^139]: 线性随机逼近的Polyak-Ruppert平均迭代的有限时间高概率界限

    Finite-time High-probability Bounds for Polyak-Ruppert Averaged Iterates of Linear Stochastic Approximation. (arXiv:2207.04475v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.04475](http://arxiv.org/abs/2207.04475)

    本文提供了关于带有固定步长的线性随机逼近（LSA）算法的有限时间分析，得到了LSA及其Polyak-Ruppert平均化版本定义的迭代的$p$阶矩和高概率偏差界的推导。本文得到的有限时间实例相关的平均LSA迭代的界限是尖锐的，并与局部渐近极小极值限制相同，同时剩余项对混合时间有着紧密的依赖关系。

    

    本文提供了关于带有固定步长的线性随机逼近（LSA）算法的有限时间分析，这是统计学和机器学习中的一个核心方法。LSA被用来计算$d$维线性系统$\bar{\mathbf{A}}\theta = \bar{\mathbf{b}}$的近似解，其中$(\bar{\mathbf{A}},\bar{\mathbf{b}})$仅能通过（渐近）无偏观测$\{(\mathbf{A}(Z_n),\mathbf{b}(Z_n))\}_{n \in \mathbb{N}}$来估计。我们在这里考虑的情况是$\{Z_n\}_{n \in \mathbb{N}}$是i.i.d.序列或均匀几何遍历马尔可夫链的情况。我们对LSA及其Polyak-Ruppert平均化版本定义的迭代进行$p$阶矩和高概率偏差界的推导。我们获得的有限时间实例相关的平均LSA迭代的界限是尖锐的，因为我们获得的主项与局部渐近极小极值限制相同。此外，我们的界限的剩余项对混合时间有着紧密的依赖关系。

    This paper provides a finite-time analysis of linear stochastic approximation (LSA) algorithms with fixed step size, a core method in statistics and machine learning. LSA is used to compute approximate solutions of a $d$-dimensional linear system $\bar{\mathbf{A}} \theta = \bar{\mathbf{b}}$ for which $(\bar{\mathbf{A}}, \bar{\mathbf{b}})$ can only be estimated by (asymptotically) unbiased observations $\{(\mathbf{A}(Z_n),\mathbf{b}(Z_n))\}_{n \in \mathbb{N}}$. We consider here the case where $\{Z_n\}_{n \in \mathbb{N}}$ is an i.i.d. sequence or a uniformly geometrically ergodic Markov chain. We derive $p$-th moment and high-probability deviation bounds for the iterates defined by LSA and its Polyak-Ruppert-averaged version. Our finite-time instance-dependent bounds for the averaged LSA iterates are sharp in the sense that the leading term we obtain coincides with the local asymptotic minimax limit. Moreover, the remainder terms of our bounds admit a tight dependence on the mixing time 
    
[^140]: 深度推荐系统中的协作检索器和排名器

    Cooperative Retriever and Ranker in Deep Recommenders. (arXiv:2206.14649v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2206.14649](http://arxiv.org/abs/2206.14649)

    本文介绍了深度推荐系统中的检索和排名两阶段工作流程。传统方法中，这两个组件都是独立训练或使用简单的级联管道，效果不佳。最近一些工作提出联合训练检索器和排名器，但仍存在许多限制。因此，还需要探索更有效的协作方法。

    

    深度推荐系统(DRS)在现代网络服务中被广泛应用。为了处理海量网络内容，DRS采用了两阶段工作流程：检索和排名，以生成其推荐结果。检索器旨在高效地从整个项目中选择一小组相关候选项；而排名器通常更精确但时间消耗更大，应进一步从检索候选项中优化最佳项目。传统上，两个组件要么独立训练，要么在简单的级联管道内训练，这容易产生合作效果差的问题。尽管最近一些工作建议联合训练检索器和排名器，但仍存在许多严重限制：训练和推理中的项分布转移、假阴性和排名顺序不对齐等。因此，探索检索器和排名器之间的有效协作仍然是必要的。

    Deep recommender systems (DRS) are intensively applied in modern web services. To deal with the massive web contents, DRS employs a two-stage workflow: retrieval and ranking, to generate its recommendation results. The retriever aims to select a small set of relevant candidates from the entire items with high efficiency; while the ranker, usually more precise but time-consuming, is supposed to further refine the best items from the retrieved candidates. Traditionally, the two components are trained either independently or within a simple cascading pipeline, which is prone to poor collaboration effect. Though some latest works suggested to train retriever and ranker jointly, there still exist many severe limitations: item distribution shift between training and inference, false negative, and misalignment of ranking order. As such, it remains to explore effective collaborations between retriever and ranker.
    
[^141]: NovelCraft：开放世界中的新颖性检测和发现数据集

    NovelCraft: A Dataset for Novelty Detection and Discovery in Open Worlds. (arXiv:2206.11736v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.11736](http://arxiv.org/abs/2206.11736)

    NovelCraft数据集提供了开放世界中新颖性检测与发现任务的挑战。在复杂的场景中插入新颖物体的检测需要更好的基准，并发现了控制假阳性时更简单的方法可能比复杂的方法更出色。

    

    为了让人工智能代理在不断变化的环境中成功执行任务，必须能够检测和适应新颖性。然而，视觉新颖性检测研究通常只评估旨在进行对象分类的重复利用数据集（如CIFAR-10），其中图像聚焦于一个明显、居中的对象。需要新的基准来代表在开放世界中导航复杂场景的挑战。我们的新NovelCraft数据集包含完成修改后的Minecraft环境中的跳跳球装配任务的代理所看到的图像和符号世界状态的多模式情节数据。在某些情节中，我们在复杂的3D场景中插入新颖物体，这些物体可能影响游戏玩法并出现在各种大小和位置中。我们的视觉新颖性检测基准发现，控制假阳性时，最好的面积下曲线度量的方法可能会被更简单的替代方法超过。

    In order for artificial agents to successfully perform tasks in changing environments, they must be able to both detect and adapt to novelty. However, visual novelty detection research often only evaluates on repurposed datasets such as CIFAR-10 originally intended for object classification, where images focus on one distinct, well-centered object. New benchmarks are needed to represent the challenges of navigating the complex scenes of an open world. Our new NovelCraft dataset contains multimodal episodic data of the images and symbolic world-states seen by an agent completing a pogo stick assembly task within a modified Minecraft environment. In some episodes, we insert novel objects within the complex 3D scene that may impact gameplay and appear in a variety of sizes and positions. Our visual novelty detection benchmark finds that methods that rank best on popular area-under-the-curve metrics may be outperformed by simpler alternatives when controlling false positives matters most. 
    
[^142]: 超越RMSE：机器学习模型的道路用户交互是否产生类人行为？

    Beyond RMSE: Do machine-learned models of road user interaction produce human-like behavior?. (arXiv:2206.11110v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.11110](http://arxiv.org/abs/2206.11110)

    本文研究了机器学习模型在道路用户交互预测方面的行为，并发现尽管这些模型在RMSE方面表现良好，但它们的行为可能与人类不同，需要更细致的分析。

    

    自动驾驶汽车使用各种传感器和机器学习模型来预测周围道路用户的行为。大多数文献中的机器学习模型侧重于学习和报告模型的能力的定量误差度量，如均方根误差(RMSE)。这种侧重于定量误差度量的做法往往忽视了模型的更重要的行为方面，引发了这样一个问题：这些模型是否确实预测了类人行为。因此，我们建议以类似于传统行为研究中分析人类数据的方式来分析机器学习模型的输出。我们介绍定量度量来证明自然行驶数据集中存在三种不同行为现象的存在：1）谁先通过合并点取决于运动学 2）在高速公路上，车道变更以适应匝道车辆 3）在高速公路上避免前车冲突的车辆变道。然后，我们分析了三种不同的机器学习模型在执行这三种现象时与人类行为的表现。我们的研究结果表明，尽管机器学习模型在RMSE方面表现良好，但它们可能表现出不同于人类行为的行为，这凸显了在道路用户交互环境中对机器学习模型进行更细致分析的需求。

    Autonomous vehicles use a variety of sensors and machine-learned models to predict the behavior of surrounding road users. Most of the machine-learned models in the literature focus on quantitative error metrics like the root mean square error (RMSE) to learn and report their models' capabilities. This focus on quantitative error metrics tends to ignore the more important behavioral aspect of the models, raising the question of whether these models really predict human-like behavior. Thus, we propose to analyze the output of machine-learned models much like we would analyze human data in conventional behavioral research. We introduce quantitative metrics to demonstrate presence of three different behavioral phenomena in a naturalistic highway driving dataset: 1) The kinematics-dependence of who passes a merging point first 2) Lane change by an on-highway vehicle to accommodate an on-ramp vehicle 3) Lane changes by vehicles on the highway to avoid lead vehicle conflicts. Then, we analyz
    
[^143]: Compressed-VFL: 用于具有纵向分割数据的通信高效学习

    Compressed-VFL: Communication-Efficient Learning with Vertically Partitioned Data. (arXiv:2206.08330v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.08330](http://arxiv.org/abs/2206.08330)

    提出了一种用于纵向分割数据的通信高效学习方法，通过周期性共享压缩的中间结果，实现了超过90%的通信量减少，且不影响模型准确性。

    

    我们提出了一种用于纵向分割数据的通信高效训练的压缩式纵向联邦学习（C-VFL）。在C-VFL中，服务器和多个参与方利用几个本地迭代协同训练各自特征上的模型，并周期性地共享压缩后的中间结果。我们的工作首次提供了压缩对于纵向分割数据分布式训练的影响的理论分析。我们证明了当训练过程中的压缩误差被限制时，非凸目标的收敛速率为 $O(\frac{1}{\sqrt{T}})$。我们提供了使用常用压缩技术（例如量化和前$k$大稀疏化）进行收敛所需的具体要求。最后，我们在实验中展示了，在不明显降低VFL准确性的情况下，压缩可以将通信量减少超过 $90\%$。

    We propose Compressed Vertical Federated Learning (C-VFL) for communication-efficient training on vertically partitioned data. In C-VFL, a server and multiple parties collaboratively train a model on their respective features utilizing several local iterations and sharing compressed intermediate results periodically. Our work provides the first theoretical analysis of the effect message compression has on distributed training over vertically partitioned data. We prove convergence of non-convex objectives at a rate of $O(\frac{1}{\sqrt{T}})$ when the compression error is bounded over the course of training. We provide specific requirements for convergence with common compression techniques, such as quantization and top-$k$ sparsification. Finally, we experimentally show compression can reduce communication by over $90\%$ without a significant decrease in accuracy over VFL without compression.
    
[^144]: 看、辐射和学习：基于无监督的无线电-视觉对应的自助定位

    Look, Radiate, and Learn: Self-Supervised Localisation via Radio-Visual Correspondence. (arXiv:2206.06424v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.06424](http://arxiv.org/abs/2206.06424)

    本文提出了一个合成的无线电-视觉数据集和基准(MaxRay)，通过从无线电-视觉对应中提取自坐标来自主学习目标在无线电中的定位，最终实现了无需标签的准确无线电目标定位。

    

    下一代移动通信网络将在惯常通信的基础上实现射频感知功能，从而使室外的感知范围空前。深度学习已经彻底改变了计算机视觉，但对于无线电感知任务的应用还受到限制，部分原因是缺乏系统性的数据集和基准以研究无线电感知的性能和前景。为了填补这一空白，我们提出了MaxRay：一个合成的无线电-视觉数据集和基准，有助于精确的无线电目标定位。我们进一步提出通过从无线电-视觉对应中提取自坐标来学习无线电目标的无监督定位。我们使用这种自助定位的坐标来训练一个无线电定位器网络，并与许多最先进的基准进行性能比较。我们的结果表明，通过配对的无线电-视觉数据，可以无需标签来自动学习准确的无线电目标定位。

    Next generation cellular networks will implement radio sensing functions alongside customary communications, thereby enabling unprecedented worldwide sensing coverage outdoors. Deep learning has revolutionised computer vision but has had limited application to radio perception tasks, in part due to lack of systematic datasets and benchmarks dedicated to the study of the performance and promise of radio sensing. To address this gap, we present MaxRay: a synthetic radio-visual dataset and benchmark that facilitate precise target localisation in radio. We further propose to learn to localise targets in radio without supervision by extracting self-coordinates from radio-visual correspondence. We use such self-supervised coordinates to train a radio localiser network. We characterise our performance against a number of state-of-the-art baselines. Our results indicate that accurate radio target localisation can be automatically learned from paired radio-visual data without labels, which is i
    
[^145]: Squeeze All：线性上下文Bandit的新估计器和自标准化界限

    Squeeze All: Novel Estimator and Self-Normalized Bound for Linear Contextual Bandits. (arXiv:2206.05404v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.05404](http://arxiv.org/abs/2206.05404)

    我们提出了一种具有新颖估计量和自标准化界限的线性上下文Bandit算法，具有$O(\sqrt{dT\log T})$遗憾界，维度相关的加法项分解遗憾累积，性能优于现有方法。

    

    我们提出了一种具有$O(\sqrt{dT\log T})$遗憾界的线性上下文Bandit算法，其中$d$是上下文的维数，$T$是时间跨度。我们的算法具有一种新颖的估计量，其中通过显式随机化嵌入了探索。根据随机性，我们的建议估计器可以从所有武器的上下文或从选定的上下文中获得贡献。我们为我们的估计器建立了一个自标准化界限，它允许将累积遗憾分解为基于维度的\textit{可加}项，而不是乘法项。我们还证明了在我们的问题设置下的$\Omega(\sqrt{dT})$的新的下限。因此，我们提出的算法的遗憾匹配下限，直到对数因子。数值实验支持理论保证，并表明我们的方法优于现有的线性Bandit算法。

    We propose a linear contextual bandit algorithm with $O(\sqrt{dT\log T})$ regret bound, where $d$ is the dimension of contexts and $T$ isthe time horizon. Our proposed algorithm is equipped with a novel estimator in which exploration is embedded through explicit randomization. Depending on the randomization, our proposed estimator takes contributions either from contexts of all arms or from selected contexts. We establish a self-normalized bound for our estimator, which allows a novel decomposition of the cumulative regret into \textit{additive} dimension-dependent terms instead of multiplicative terms. We also prove a novel lower bound of $\Omega(\sqrt{dT})$ under our problem setting. Hence, the regret of our proposed algorithm matches the lower bound up to logarithmic factors. The numerical experiments support the theoretical guarantees and show that our proposed method outperforms the existing linear bandit algorithms.
    
[^146]: 深度均衡模型全局收敛性研究

    Global Convergence of Over-parameterized Deep Equilibrium Models. (arXiv:2205.13814v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.13814](http://arxiv.org/abs/2205.13814)

    本研究探讨了深度均衡模型的训练动态，提出唯一平衡点始终存在且梯度下降的收敛速率为线性收敛到全局最优解，可通过轻微过参数化得到满足。

    

    深度均衡模型（DEQ）通过无限深度的加权-绑定模型中的平衡点与输入注入来隐式定义。它通过根查找直接求解平衡点，并通过隐式微分计算梯度，避免了无限运算。本研究探讨了过参数化DEQ的训练动态。通过对初始平衡点施加一定条件，我们表明唯一平衡点在训练过程中始终存在，并且针对二次损失函数，梯度下降的收敛速率证明为线性收敛到全局最优解。为了展示所需起始条件通过轻微过参数化得到满足，我们在随机DEQ上进行了细粒度的分析。我们提出了一种新的概率框架，以克服非渐近分析无限深度加权绑定模型的技术困难。

    A deep equilibrium model (DEQ) is implicitly defined through an equilibrium point of an infinite-depth weight-tied model with an input-injection. Instead of infinite computations, it solves an equilibrium point directly with root-finding and computes gradients with implicit differentiation. The training dynamics of over-parameterized DEQs are investigated in this study. By supposing a condition on the initial equilibrium point, we show that the unique equilibrium point always exists during the training process, and the gradient descent is proved to converge to a globally optimal solution at a linear convergence rate for the quadratic loss function. In order to show that the required initial condition is satisfied via mild over-parameterization, we perform a fine-grained analysis on random DEQs. We propose a novel probabilistic framework to overcome the technical difficulty in the non-asymptotic analysis of infinite-depth weight-tied models.
    
[^147]: 从“解决方案合成”到“学生尝试合成”：面向基于块的可视化编程任务的研究

    From {Solution Synthesis} to {Student Attempt Synthesis} for Block-Based Visual Programming Tasks. (arXiv:2205.01265v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2205.01265](http://arxiv.org/abs/2205.01265)

    研究自动推断学生误解的关键组成部分，引入了一个基准测试——StudentSyn，通过观察学生在一个任务上的尝试，合成他们对另一个任务的学习尝试，以提高人工智能驱动的编程导师的效果。

    

    基于块的可视化编程环境越来越被用来介绍计算概念给初学者。鉴于编程任务是开放式和概念性的，初学者在这些环境中学习时常常遇到困难。人工智能驱动的编程导师有着帮助挣扎的学生的巨大潜力，并且需要几个组成部分来实现这一潜力。我们研究了学生建模这一关键组成部分，特别是自动推断学生误解的能力，以便预测（合成）他们的行为。我们介绍了一个新颖的基准，StudentSyn，围绕以下挑战：为一个给定的学生，观察他们在一个固定的引用任务上的尝试后，合成他们对新目标任务的尝试。这个挑战类似于程序合成；但是，这里的目标不是合成一个“解决方案”（即专家编写的程序），而是合成一个“学生尝试”（即一个学生在学习过程中记录的程序）。我们提供了StudentSyn的详细说明，并描述了基准测试的几个潜在用例。我们还提出了一种模块化方法来解决这个挑战，并通过对初学者和有经验的参与者的实验分析了挑战的难度。

    Block-based visual programming environments are increasingly used to introduce computing concepts to beginners. Given that programming tasks are open-ended and conceptual, novice students often struggle when learning in these environments. AI-driven programming tutors hold great promise in automatically assisting struggling students, and need several components to realize this potential. We investigate the crucial component of student modeling, in particular, the ability to automatically infer students' misconceptions for predicting (synthesizing) their behavior. We introduce a novel benchmark, StudentSyn, centered around the following challenge: For a given student, synthesize the student's attempt on a new target task after observing the student's attempt on a fixed reference task. This challenge is akin to that of program synthesis; however, instead of synthesizing a {solution} (i.e., program an expert would write), the goal here is to synthesize a {student attempt} (i.e., program t
    
[^148]: IOP-FL: Inside-Outside Personalization for Federated Medical Image Segmentation（联邦医学图像分割的内外个性化方法）

    IOP-FL: Inside-Outside Personalization for Federated Medical Image Segmentation. (arXiv:2204.08467v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2204.08467](http://arxiv.org/abs/2204.08467)

    该论文提出了IOP-FL，一种联邦学习的内外个性化方法，以增强医学成像任务中单个客户的预测准确性。

    

    联邦学习（FL）允许多个医疗机构在未集中客户数据的情况下协同学习全局模型。由于来自各种扫描仪和患者人口统计学的医学图像的异质性，这种全局模型普遍实现每个客户的最佳性能是困难的，如果可能的话。当将全局模型部署到在FL期间未呈现的分布上看不见的客户时，这个问题变得更为重要。为了优化每个客户在医学成像任务中的预测准确性，我们提出了一个新的统一框架，用于FL中的“内部和外部模型个性化”（IOP-FL）。我们的内部个性化使用一种轻量级的基于梯度的方法，通过累积通用知识的全局梯度和客户特定优化的本地梯度，利用每个客户的本地适应模型。此外，我们还提出了一个外部个性化解决方案，通过一个客户自适应机制来应对来自未知客户的图像。实验结果表明，我们的方法可以大大提高单个客户的预测准确性，并且可以同时保持在不同客户之间的通用性。

    Federated learning (FL) allows multiple medical institutions to collaboratively learn a global model without centralizing client data. It is difficult, if possible at all, for such a global model to commonly achieve optimal performance for each individual client, due to the heterogeneity of medical images from various scanners and patient demographics. This problem becomes even more significant when deploying the global model to unseen clients outside the FL with unseen distributions not presented during federated training. To optimize the prediction accuracy of each individual client for medical imaging tasks, we propose a novel unified framework for both \textit{Inside and Outside model Personalization in FL} (IOP-FL). Our inside personalization uses a lightweight gradient-based approach that exploits the local adapted model for each client, by accumulating both the global gradients for common knowledge and the local gradients for client-specific optimization. Moreover, and important
    
[^149]: 重新思考基于重构自编码器的外样本检测

    Rethinking Reconstruction Autoencoder-Based Out-of-Distribution Detection. (arXiv:2203.02194v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2203.02194](http://arxiv.org/abs/2203.02194)

    本文重新考虑了基于重构自编码器方法的外样本检测，在最大压缩自编码器的潜空间和保证重构能力的基础上，通过引入语义重构、数据确定性分解和标准化L2距离等策略，本文提出的方法在各个基准测试中都取得了最先进的性能表现，且不需要额外的标记外样本数据。

    

    在某些情况下，分类器需要检测远离其训练数据的外样本。重构自编码器方法利用输入重构误差作为新颖性与正常性的度量来解决这个问题。我们将这种方法的本质表述为具有对条件数据不确定性的代理查询的四元组域转换，其有内在偏见。因此，一种改进方向被形式化为最大压缩自编码器的潜空间，同时确保其重构能力，以充当所描述的域转换器。从中，引入了策略，包括语义重构、数据确定性分解和标准化L2距离，以实质性改善原始方法，这些方法共同在各种基准测试中建立了最先进的性能，例如，在Wide-ResNet上，CIFAR-100与TinyImagenet-crop的FPR@95%TPR为0.2%。重要的是，我们的方法不需要任何额外的标记外样本数据。

    In some scenarios, classifier requires detecting out-of-distribution samples far from its training data. With desirable characteristics, reconstruction autoencoder-based methods deal with this problem by using input reconstruction error as a metric of novelty vs. normality. We formulate the essence of such approach as a quadruplet domain translation with an intrinsic bias to only query for a proxy of conditional data uncertainty. Accordingly, an improvement direction is formalized as maximumly compressing the autoencoder's latent space while ensuring its reconstructive power for acting as a described domain translator. From it, strategies are introduced including semantic reconstruction, data certainty decomposition and normalized L2 distance to substantially improve original methods, which together establish state-of-the-art performance on various benchmarks, e.g., the FPR@95%TPR of CIFAR-100 vs. TinyImagenet-crop on Wide-ResNet is 0.2%. Importantly, our method works without any addit
    
[^150]: 隐私保护的逻辑回归训练及其更快的梯度变种

    Privacy-Preserving Logistic Regression Training with A Faster Gradient Variant. (arXiv:2201.10838v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2201.10838](http://arxiv.org/abs/2201.10838)

    本文提出了一种更快的梯度变种——二次梯度，用于在同态加密领域实现隐私保护的逻辑回归训练，并成功提升了收敛速度，实现了同态逻辑回归训练仅需较少的迭代次数。

    

    多年来，加密数据上的逻辑回归训练一直是一个有吸引力的安全解决方案。本文提出了一种更快的梯度变种——二次梯度，用于在同态加密领域实现逻辑回归训练，其核心可以看作是简化的固定Hessian的扩展。我们分别向Nesterov的加速梯度方法（NAG）和自适应梯度算法（Adagrad）增强了该梯度变种，并在多个数据集上评估了增强算法。实验结果表明，增强方法在收敛速度上比朴素的一阶梯度方法具有最先进的性能。然后，我们采用增强的NAG方法来实现同态逻辑回归训练，并在仅3次迭代中获得了可比较的结果。

    Logistic regression training over encrypted data has been an attractive idea to security concerns for years. In this paper, we propose a faster gradient variant called $\texttt{quadratic gradient}$ to implement logistic regression training in a homomorphic encryption domain, the core of which can be seen as an extension of the simplified fixed Hessian.  We enhance Nesterov's accelerated gradient (NAG) and Adaptive Gradient Algorithm (Adagrad) respectively with this gradient variant and evaluate the enhanced algorithms on several datasets. Experimental results show that the enhanced methods have a state-of-the-art performance in convergence speed compared to the naive first-order gradient methods. We then adopt the enhanced NAG method to implement homomorphic logistic regression training and obtain a comparable result by only $3$ iterations.
    
[^151]: 新冠肺炎中的人工智能应用

    The Prominence of Artificial Intelligence in COVID-19. (arXiv:2111.09537v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.09537](http://arxiv.org/abs/2111.09537)

    本论文研究人工智能在 COVID-19 中的应用，探讨了提出的方法，可帮助早期和廉价地诊断该病，有助于医生和研究人员对抗该病。

    

    2019年12月，一种新型病毒，COVID-19已经导致了极多的死亡。与西班牙流感1918年相比，与这种新冠病毒的斗争令人困惑和恐惧。而在前线医生和医学研究人员在控制这种高度传染病毒的传播方面取得了重要进展的同时，技术在这场战斗中也证明了其重要性。此外，人工智能在许多医疗应用中得到了采用，可以诊断许多疾病，甚至使经验丰富的医生感到困惑。因此，本调查论文探讨了提出的方法，可帮助医生和研究人员早期和廉价地诊断该病。大多数发展中国家难以使用传统方式进行测试，但可以采用机器学习和深度学习的方法。另一方面，获得不同类型的医学图像也激发了研究人员的动力。因此，大量的技术创新被引入并应用于实践中。

    In December 2019, a novel virus called COVID-19 had caused an enormous number of causalities to date. The battle with the novel Coronavirus is baffling and horrifying after the Spanish Flu 2019. While the front-line doctors and medical researchers have made significant progress in controlling the spread of the highly contiguous virus, technology has also proved its significance in the battle. Moreover, Artificial Intelligence has been adopted in many medical applications to diagnose many diseases, even baffling experienced doctors. Therefore, this survey paper explores the methodologies proposed that can aid doctors and researchers in early and inexpensive methods of diagnosis of the disease. Most developing countries have difficulties carrying out tests using the conventional manner, but a significant way can be adopted with Machine and Deep Learning. On the other hand, the access to different types of medical images has motivated the researchers. As a result, a mammoth number of tech
    
[^152]: 深度学习推理中的计算和能量消耗趋势

    Compute and Energy Consumption Trends in Deep Learning Inference. (arXiv:2109.05472v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.05472](http://arxiv.org/abs/2109.05472)

    本篇论文研究了深度学习推理中的计算和能量消耗趋势，重点关注推理成本而非训练成本。结果显示，除了算法创新外，更具体和强大的硬件通常伴随着重要的能量效率优化，导致推理成本的提高呈现柔和的趋势。

    

    一些人认为，深度学习等AI范式的进展与参数数量指数增长有关。有许多研究证实这些趋势，但这是否意味着能量消耗呈指数增长？为了回答这个问题，我们将重点放在推理成本上，而不是训练成本，因为前者占据了大部分的计算工作量，仅因为有乘法因素存在。此外，除了算法创新外，我们还考虑了更具体和强大的硬件（导致更高的FLOPS），这通常伴随着重要的能量效率优化。我们还将焦点从突破性论文的第一次实现转移到了一两年后的技术版本的巩固版本。在这个独特和全面的视角下，我们研究了计算机视觉和自然语言处理领域的相关模型：对于持续提高性能，我们看到一个更柔和的趋势。

    The progress of some AI paradigms such as deep learning is said to be linked to an exponential growth in the number of parameters. There are many studies corroborating these trends, but does this translate into an exponential increase in energy consumption? In order to answer this question we focus on inference costs rather than training costs, as the former account for most of the computing effort, solely because of the multiplicative factors. Also, apart from algorithmic innovations, we account for more specific and powerful hardware (leading to higher FLOPS) that is usually accompanied with important energy efficiency optimisations. We also move the focus from the first implementation of a breakthrough paper towards the consolidated version of the techniques one or two year later. Under this distinctive and comprehensive perspective, we study relevant models in the areas of computer vision and natural language processing: for a sustained increase in performance we see a much softer 
    
[^153]: 系统化的人类学习与推广：通过简要教程和解释性反馈实现

    Systematic human learning and generalization from a brief tutorial with explanatory feedback. (arXiv:2107.06994v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.06994](http://arxiv.org/abs/2107.06994)

    本文研究了人类如何通过简要教程和解释性反馈从少量的训练示例中学习抽象推理任务，并能成功地将其推广到训练范围之外的情况。结果表明，实现人类学习机制对于人工智能的发展非常重要。

    

    神经网络长期以来被用来模拟人类智能，捕捉行为和认知的元素及其神经基础。 深度学习的最新进展使得神经网络模型在许多方面达到甚至超过人类的智能水平，然而与人类不同的是，它们快速学习新任务的能力仍然是一个挑战。人们不仅可以在熟悉的领域进行推理，而且还可以快速学习推理新问题和情况，这就引发了一个问题：现代神经网络模型如何捕捉人类智能以及它们的哪些方面与人类不同。在这项工作中，我们通过研究成年人从一个基于数独的抽象推理任务的简要教学中学习的能力、通过解释性反馈纠正错误答案和狭窄范围的训练示例，来探讨这个差距。我们发现，掌握该任务的参与者可以在少数几次试验中实现，并可以推广到训练范围外的谜题。我们的结果突出了在人工智能系统中实现人类学习机制的重要性。

    Neural networks have long been used to model human intelligence, capturing elements of behavior and cognition, and their neural basis. Recent advancements in deep learning have enabled neural network models to reach and even surpass human levels of intelligence in many respects, yet unlike humans, their ability to learn new tasks quickly remains a challenge. People can reason not only in familiar domains, but can also rapidly learn to reason through novel problems and situations, raising the question of how well modern neural network models capture human intelligence and in which ways they diverge. In this work, we explore this gap by investigating human adults' ability to learn an abstract reasoning task based on Sudoku from a brief instructional tutorial with explanatory feedback for incorrect responses using a narrow range of training examples. We find that participants who master the task do so within a small number of trials and generalize well to puzzles outside of the training r
    
[^154]: 学习数据流形上的保持身份的变换

    Learning Identity-Preserving Transformations on Data Manifolds. (arXiv:2106.12096v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.12096](http://arxiv.org/abs/2106.12096)

    本论文提出了一种新方法，可以直接从数据流形中学习保持身份的变换，而无需在训练期间标记变换数据。该方法基于变分自编码器，可以学习一组本地自适应的保持身份的变换，可以提高模型在具有挑战性的图像分类和少样本学习任务上的推广性能。

    

    许多机器学习技术将保持身份的变换纳入其模型中，以将其性能推广到以前未见过的数据。然而，这些变换通常是从一组已知可以维持输入身份的函数中选择的（例如旋转、平移、翻转和缩放）。然而，有许多自然变化无法进行标记监督或通过数据检查来定义。如浸入式学习假设所示，许多这些自然变化存在于或靠近低维非线性流形上。几种技术通过一组学习的李群算子来表示流形变化，这些算子定义了流形上的运动方向。然而，这些方法的局限在于它们在训练其模型时需要变换标签，并且缺乏一种确定每个特定算子适用于流形的哪些区域的方法。我们通过提出一种新的方法来解决这些限制，该方法直接从数据流形中学习保持身份的变换，而无需在训练过程中标记变换数据。我们的方法基于一种修改的变分自编码器，该自编码器在端到端训练中学习生成模型和一组本地自适应的保持身份的变换。我们的实验表明，我们的方法能够学习有用的变换，从而改善模型在具有挑战性的图像分类和少样本学习任务中的推广性能。

    Many machine learning techniques incorporate identity-preserving transformations into their models to generalize their performance to previously unseen data. These transformations are typically selected from a set of functions that are known to maintain the identity of an input when applied (e.g., rotation, translation, flipping, and scaling). However, there are many natural variations that cannot be labeled for supervision or defined through examination of the data. As suggested by the manifold hypothesis, many of these natural variations live on or near a low-dimensional, nonlinear manifold. Several techniques represent manifold variations through a set of learned Lie group operators that define directions of motion on the manifold. However, these approaches are limited because they require transformation labels when training their models and they lack a method for determining which regions of the manifold are appropriate for applying each specific operator. We address these limitati
    
[^155]: 不平衡梯度：对敌对鲁棒性估计过高的微妙原因

    Imbalanced Gradients: A Subtle Cause of Overestimated Adversarial Robustness. (arXiv:2006.13726v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2006.13726](http://arxiv.org/abs/2006.13726)

    本文研究了对敌对鲁棒性估计过高的微妙原因，发现不平衡梯度也会导致鲁棒性估计过高，我们提出了一种边界分解攻击，并证明11个防御模型在某种程度上容易受到不平衡梯度的影响，MD攻击可以降低它们的鲁棒性。

    

    在敌对鲁棒性研究中，评估防御模型的鲁棒性是一项具有挑战性的任务。以往发现在许多防御方法中存在隐晦的梯度，这些梯度会导致错误的鲁棒性信号。在本文中，我们确定了一种更微妙的情况，称为不平衡梯度，它也会导致对敌对鲁棒性的过高估计。不平衡梯度现象发生在边界损失的一个术语的梯度占主导地位并将攻击推向次优方向时。为了利用不平衡梯度，我们制定了一种边界分解(MD)攻击，将边界损失分解为单独的术语，然后通过两个阶段的过程分别探索这些术语的攻击性。我们还提出了我们的MD攻击的多目标和集合版本。通过调查自2018年以来提出的24个防御模型，我们发现11个模型在某种程度上容易受到不平衡梯度的影响，而我们的MD攻击可以降低它们的鲁棒性。

    Evaluating the robustness of a defense model is a challenging task in adversarial robustness research. Obfuscated gradients have previously been found to exist in many defense methods and cause a false signal of robustness. In this paper, we identify a more subtle situation called Imbalanced Gradients that can also cause overestimated adversarial robustness. The phenomenon of imbalanced gradients occurs when the gradient of one term of the margin loss dominates and pushes the attack towards to a suboptimal direction. To exploit imbalanced gradients, we formulate a Margin Decomposition (MD) attack that decomposes a margin loss into individual terms and then explores the attackability of these terms separately via a two-stage process. We also propose a multi-targeted and ensemble version of our MD attack. By investigating 24 defense models proposed since 2018, we find that 11 models are susceptible to a certain degree of imbalanced gradients and our MD attack can decrease their robustnes
    

