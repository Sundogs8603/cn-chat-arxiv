# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [New Foggy Object Detecting Model.](http://arxiv.org/abs/2401.15455) | 本文提出了一种新的模糊目标检测方法，并通过区域识别和目标检测两个阶段的架构实现。该方法在准确性和检测时间方面表现出显著的改进。 |
| [^2] | [Continuous Treatment Effect Estimation Using Gradient Interpolation and Kernel Smoothing.](http://arxiv.org/abs/2401.15447) | 本文提出了一种使用梯度插值和核平滑的方法，用于个性化连续处理效果估计。通过增加独立采样的处理和推断的反事实结果来处理训练数据中的混淆问题。实验证明该方法在反事实预测性能上优于其他六种最先进的方法。 |
| [^3] | [Towards Causal Classification: A Comprehensive Study on Graph Neural Networks.](http://arxiv.org/abs/2401.15444) | 这项研究通过对九种基准图分类模型的测试，揭示了因果性对图神经网络的预测能力的影响，为进一步发展和应用GNN提供了重要的见解。 |
| [^4] | [Decentralized Gossip Mutual Learning (GML) for brain tumor segmentation on multi-parametric MRI.](http://arxiv.org/abs/2401.15434) | 本论文提出了一种基于去中心化流言传播共学习的多参数MRI脑肿瘤分割框架，该框架实现了在不共享私密数据的情况下医疗中心之间的协作模型训练，并通过共学习来优化不同站点之间的数据变化。 |
| [^5] | [A Survey on Data Augmentation in Large Model Era.](http://arxiv.org/abs/2401.15422) | 这篇论文综述了大模型驱动的数据增强方法，包括图像增强、文本增强和配对数据增强。这些方法利用大模型的能力，有效提高了数据增强的效果，是解决大模型训练中数据质量不足的重要研究方向。 |
| [^6] | [Fault Diagnosis on Induction Motor using Machine Learning and Signal Processing.](http://arxiv.org/abs/2401.15417) | 本研究利用机器学习和信号处理方法，研究了在Industry 4.0背景下利用MATLAB Simulink对感应电机故障进行检测和识别。生成了包含四种故障类型的数据集，通过应用快速傅里叶变换进行故障检测和识别。 |
| [^7] | [Validation of artificial neural networks to model the acoustic behaviour of induction motors.](http://arxiv.org/abs/2401.15377) | 本研究旨在评估多任务人工神经网络作为感应电动机声学参数预测的建模技术，通过使用额定声压、响度、粗糙度和尖锐度作为输出。 |
| [^8] | [Deep Learning with Information Fusion and Model Interpretation for Health Monitoring of Fetus based on Long-term Prenatal Electronic Fetal Heart Rate Monitoring Data.](http://arxiv.org/abs/2401.15337) | 本研究通过结合深度学习和信息融合方法，开发了一个名为LARA的自动分析系统，用于长期产前电子胎儿心率监测。该系统通过卷积神经网络模型处理长期的FHR数据，提供了更全面的对胎儿状态的理解。 |
| [^9] | [L-AutoDA: Leveraging Large Language Models for Automated Decision-based Adversarial Attacks.](http://arxiv.org/abs/2401.15335) | 本文介绍了一种名为L-AutoDA的创新方法，利用大型语言模型自动设计决策型对抗攻击。通过与大型语言模型的迭代交互，L-AutoDA能够高效地生成竞争性的攻击算法，显示出在成功率和计算效率方面的显著改进。 |
| [^10] | [Optimal Sparse Survival Trees.](http://arxiv.org/abs/2401.15330) | 本研究提出了一种动态规划边界法，可以在几秒钟内找到可证明最优稀疏生存树模型，对于涉及人类健康的高风险问题的分析和决策具有重要意义。 |
| [^11] | [Localization of Dummy Data Injection Attacks in Power Systems Considering Incomplete Topological Information: A Spatio-Temporal Graph Wavelet Convolutional Neural Network Approach.](http://arxiv.org/abs/2401.15321) | 本文研究了新型虚假数据注入攻击在电力系统中的定位问题，通过考虑电力网数据的非欧氏空间属性中的固有拓扑相关性，提出了一种基于时空图波卷积神经网络的方法，以提高攻击定位的准确性。 |
| [^12] | [Gaussian Splashing: Dynamic Fluid Synthesis with Gaussian Splatting.](http://arxiv.org/abs/2401.15318) | 高斯喷溅技术相结合物理基础动画和3D高斯喷溅，可以在虚拟场景中创造出无可比拟的效果，同时实现渲染、视图合成以及固体和流体的动态管理和交互。 |
| [^13] | [A Practical Probabilistic Benchmark for AI Weather Models.](http://arxiv.org/abs/2401.15305) | 这篇论文提出了一个实用的概率基准，用于比较AI天气模型的概率技能。通过使用滞后集合，可以实现对多个模型的比较，并与操作基准进行对比。 |
| [^14] | [Adaptive Least Mean Squares Graph Neural Networks and Online Graph Signal Estimation.](http://arxiv.org/abs/2401.15304) | 我们提出了一种名为自适应最小均方图神经网络（LMS-GNN）的神经网络架构，用于在线估计时变图信号。实验结果表明，与基于图的方法相比，LMS-GNN实现了更准确的在线预测。 |
| [^15] | [SupplyGraph: A Benchmark Dataset for Supply Chain Planning using Graph Neural Networks.](http://arxiv.org/abs/2401.15299) | SupplyGraph是一个基准数据集，用于使用图神经网络进行供应链规划。该数据集包含了来自孟加拉国一家领先快速消费品公司的实际数据，用于优化、预测和解决供应链问题。数据集中的时间数据作为节点特征，可用于销售预测、生产计划和故障识别。 |
| [^16] | [Multi-Trigger Backdoor Attacks: More Triggers, More Threats.](http://arxiv.org/abs/2401.15295) | 本文主要研究了多触发后门攻击对深度神经网络的威胁。通过提出并研究了三种类型的多触发攻击，包括并行、顺序和混合攻击，文章揭示了不同触发器对同一数据集的共存、覆写和交叉激活效果。结果表明单触发攻击容易引起覆写问题。 |
| [^17] | [Integral Operator Approaches for Scattered Data Fitting on Spheres.](http://arxiv.org/abs/2401.15294) | 本文提出了一种积分算子方法来解决球面上的散点数据拟合问题，通过研究加权谱滤波算法的逼近性能，成功推导出了带权重谱滤波算法的最优误差估计。这种方法可以避免一些现有方法中存在的问题，同时提供了一种优化算法的解决方案。 |
| [^18] | [SkipViT: Speeding Up Vision Transformers with a Token-Level Skip Connection.](http://arxiv.org/abs/2401.15293) | SkipViT通过令牌级跳跃连接将不重要的图像令牌分离，以提高Vision Transformers的训练速度，而不影响最终模型的准确率。 |
| [^19] | [Adaptive Block sparse regularization under arbitrary linear transform.](http://arxiv.org/abs/2401.15292) | 我们提出了一种在任意线性变换下重构具有块稀疏性的信号的方法，相比现有方法扩大了应用范围，并通过数值实验证明了其有效性。 |
| [^20] | [Benchmarking with MIMIC-IV, an irregular, spare clinical time series dataset.](http://arxiv.org/abs/2401.15290) | 这项工作旨在为最新版本的MIMIC数据集（MIMIC-IV）提供基准测试，填补该领域在深度学习和时间序列标签数据方面的研究尚未解决的问题。 |
| [^21] | [Ransomware threat mitigation through network traffic analysis and machine learning techniques.](http://arxiv.org/abs/2401.15285) | 本文介绍了一种通过分析网络流量和使用机器学习算法来识别和检测勒索软件的方法，该方法在实践中表现出高水平的精度和准确性。 |
| [^22] | [Finite-Time Analysis of On-Policy Heterogeneous Federated Reinforcement Learning.](http://arxiv.org/abs/2401.15273) | 本论文介绍了一种新颖的联邦政策强化学习方案（FedSARSA），利用线性函数逼近来解决马尔可夫取样、多个本地更新等技术挑战，从而提供了关于有限时间性能的全面分析。 |
| [^23] | [SimFair: Physics-Guided Fairness-Aware Learning with Simulation Models.](http://arxiv.org/abs/2401.15270) | SimFair是一种物理引导的公平感知学习框架，通过集成模拟和逆向建模来解决纯数据驱动的公平性问题，有效保持公平性。 |
| [^24] | [Towards Stable Preferences for Stakeholder-aligned Machine Learning.](http://arxiv.org/abs/2401.15268) | 本研究旨在通过数据驱动的方法，整合个人和团体的偏好，解决肾脏分配中的紧迫挑战，并评估偏好的稳定性。 |
| [^25] | [Asymptotic Behavior of Adversarial Training Estimator under $\ell_\infty$-Perturbation.](http://arxiv.org/abs/2401.15262) | 本文研究了在$\ell_\infty$-扰动下的对抗性训练，证明当真实参数为0时，对抗性训练估计器在该扰动下的极限分布可能在0处有一个正概率质量，提供了稀疏性恢复能力的理论保证，并提出了一种两步过程——自适应对抗性训练，可以进一步提高性能。 |
| [^26] | [Finite Sample Confidence Regions for Linear Regression Parameters Using Arbitrary Predictors.](http://arxiv.org/abs/2401.15254) | 本文提出了一种新的方法，利用任意预测器构建线性模型参数的有限样本置信区间。该方法对噪声的要求很少，并且适用于严格线性函数偏差一定阈值的函数。这种方法能够进行鲁棒优化，并提取特定参数坐标的置信区间，也能用于假设检验。 |
| [^27] | [Better Representations via Adversarial Training in Pre-Training: A Theoretical Perspective.](http://arxiv.org/abs/2401.15248) | 该论文从理论角度探讨了在预训练中通过对抗训练改进表示的思路，并证明了特征净化在预训练模型的对抗鲁棒性和下游任务之间起到重要作用。 |
| [^28] | [Training Differentially Private Ad Prediction Models with Semi-Sensitive Features.](http://arxiv.org/abs/2401.15246) | 我们介绍了一种新的算法，用于训练具有半敏感特征的差分隐私广告预测模型，并在真实广告数据集上证明了其优于传统方法的效果。 |
| [^29] | [GenPluSSS: A Genetic Algorithm Based Plugin for Measured Subsurface Scattering Representation.](http://arxiv.org/abs/2401.15245) | 本文介绍了一种基于遗传算法的插件，可以在Blender 3D建模工具上添加次表面散射的表示方法，并使用Mitsuba渲染器进行验证。实验证明该插件能够准确、紧密和高效地可视化均匀和异质次表面散射效果。 |
| [^30] | [Near-Optimal Policy Optimization for Correlated Equilibrium in General-Sum Markov Games.](http://arxiv.org/abs/2401.15240) | 本文提出了一个协方差均衡的近最优策略优化算法，通过结合平滑价值更新和乐观实行者算法，以及对数障碍正则化器，实现了在一般性和的马尔可夫博弈中计算协方差均衡的近最优收敛速度$\tilde{O}(T^{-1})$。 |
| [^31] | [MEA-Defender: A Robust Watermark against Model Extraction Attack.](http://arxiv.org/abs/2401.15239) | MEA-Defender是一种抵御模型提取攻击的强大水印，通过将两个源类的样本组合在一起产生水印，并确保水印的输出域与主任务样本的输出域相同，实现了对DNN模型知识产权的保护。 |
| [^32] | [Deep Learning with Tabular Data: A Self-supervised Approach.](http://arxiv.org/abs/2401.15238) | 我们提出了一种使用自监督学习和TabTransformer模型进行表格数据训练的新方法，该方法能够捕捉表格数据中的复杂关系和依赖关系。相比传统的机器学习模型，我们的方法能够消除对标记数据的需求。 |
| [^33] | [Adaptive Deep Learning for Efficient Visual Pose Estimation aboard Ultra-low-power Nano-drones.](http://arxiv.org/abs/2401.15236) | 本研究提出了一种自适应深度学习机制，用于在超低功耗纳米无人机上进行高效的视觉姿态估计。通过将两种具有不同性能和成本权衡的卷积神经网络与自适应分类模块结合使用，我们能够根据计算资源的可用性选择合适的网络以实现高效的姿态估计。 |
| [^34] | [CascadedGaze: Efficiency in Global Context Extraction for Image Restoration.](http://arxiv.org/abs/2401.15235) | 本文提出了一种名为CascadedGaze的网络架构，使用了一种新颖而高效的全局上下文提取方法，以解决图像恢复中全局信息的问题。通过在卷积层之间引入小的卷积核，该方法可以学习到全局的依赖关系，而无需使用自注意力机制。实验结果表明，该方法在多种图像去噪任务上优于其他先进方法。 |
| [^35] | [Biological Valuation Map of Flanders: A Sentinel-2 Imagery Analysis.](http://arxiv.org/abs/2401.15223) | 本文介绍了一个综合的方法来填补佛兰德地区生物评估地图领域的研究差距，并提供了一张密集标记的真实地面地图。 |
| [^36] | [Transfer Learning for the Prediction of Entity Modifiers in Clinical Text: Application to Opioid Use Disorder Case Detection.](http://arxiv.org/abs/2401.15222) | 本研究提出了一种使用多任务学习和迁移学习的方法，在临床文本中预测实体修饰语。实验结果表明，该方法在公开数据集和新数据集上均取得了最新技术的结果。 |
| [^37] | [HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy.](http://arxiv.org/abs/2401.15207) | HiFT是一种分层全参数微调策略，通过仅在每个训练步骤中更新参数的子集，可以显著减少GPU内存使用，并实现与参数高效微调和标准全参数微调相当的性能。 |
| [^38] | [FedGT: Federated Node Classification with Scalable Graph Transformer.](http://arxiv.org/abs/2401.15203) | 本论文提出了一种可扩展的联邦图形变压器（FedGT），用于解决子图联邦学习中缺少链接和子图异构性的挑战。 |
| [^39] | [SCANIA Component X Dataset: A Real-World Multivariate Time Series Dataset for Predictive Maintenance.](http://arxiv.org/abs/2401.15199) | 这个论文介绍了一种来自SCANIA公司的真实世界多变量时间序列数据集，该数据集适用于各种机器学习应用，尤其是预测性维护场景。它具有庞大的样本数量和多样化的特征，以及时间信息，为研究者提供了一个使用真实世界数据的标准基准。 |
| [^40] | [AMuSE: Adaptive Multimodal Analysis for Speaker Emotion Recognition in Group Conversations.](http://arxiv.org/abs/2401.15164) | 本文提出了一种AMuSE模型，通过多模态注意力网络在群体对话中捕捉并分析说话者的情感。模型通过联合学习多个模态的外围和中央网络实现了不同级别的跨模态交互。 |
| [^41] | [FDR-Controlled Portfolio Optimization for Sparse Financial Index Tracking.](http://arxiv.org/abs/2401.15139) | 本论文提出了一种扩展的T-Rex框架，用于在稀疏金融指数跟踪中选择少数相关变量，并通过集成最近邻惩罚机制，可靠控制误发现率（FDR）。实验证明了该方法在过去20年内基于少量股票准确跟踪标准普尔500指数的能力。 |
| [^42] | [Evaluation of LLM Chatbots for OSINT-based Cyberthreat Awareness.](http://arxiv.org/abs/2401.15127) | 本研究评估了LLM聊天机器人在基于OSINT的网络威胁意识中的应用能力，并发现聊天机器人在网络安全的二分类和命名实体识别任务方面表现出良好的性能。 |
| [^43] | [Large Language Model Guided Knowledge Distillation for Time Series Anomaly Detection.](http://arxiv.org/abs/2401.15123) | 提出了一种基于大型语言模型引导的知识蒸馏的时间序列异常检测方法AnomalyLLM，通过训练学生网络模仿预训练的大型语言模型的特征，在测试阶段通过比较学生网络和教师网络的特征差异来检测异常。 |
| [^44] | [A Multi-Grained Symmetric Differential Equation Model for Learning Protein-Ligand Binding Dynamics.](http://arxiv.org/abs/2401.15122) | 提出了一种能够促进数值MD模拟并有效模拟蛋白质-配体结合动力学的NeuralMD方法，采用物理信息多级对称框架，实现了准确建模多级蛋白质-配体相互作用。 |
| [^45] | [Expressive Power of ReLU and Step Networks under Floating-Point Operations.](http://arxiv.org/abs/2401.15121) | 该论文研究了在浮点运算下神经网络的表达能力，证明了使用二进制阈值单元或ReLU的神经网络可以记忆任何实数输入/输出对，并且可以在小误差内逼近任何连续函数。 |
| [^46] | [Interpreting Time Series Transformer Models and Sensitivity Analysis of Population Age Groups to COVID-19 Infections.](http://arxiv.org/abs/2401.15119) | 该论文研究了解释深度学习时间序列模型的重要性，并通过局部解释方法解释了最先进的Transformer模型。通过将13个输入特征与3,142个美国县的三年日案例数据相结合，最佳预测模型能够在过去两周的基础上预测接下来两周的COVID-19感染情况。此外，该研究还提出了一种创新的评估方法，用于评估感染对8个人口年龄组的敏感性。 |
| [^47] | [Efficient Online Crowdsourcing with Complex Annotations.](http://arxiv.org/abs/2401.15116) | 本文提出了一种有效的在线众包算法，能够在多个标注者中聚合复杂注释，通过推断标注者的准确性来改善成本-质量权衡。基于实验结果表明该算法在真实众包数据上取得了很好的效果。 |
| [^48] | [Towards Global Glacier Mapping with Deep Learning and Open Earth Observation Data.](http://arxiv.org/abs/2401.15113) | 本研究提出了一种使用深度学习和开放地球观测数据进行全球冰川制图的方法，通过新的模型和策略，在多种地形和传感器上实现了较高的准确性。通过添加合成孔径雷达数据，并报告冰川范围的校准置信度，提高了预测的可靠性和可解释性。 |
| [^49] | [Improving Fairness of Automated Chest X-ray Diagnosis by Contrastive Learning.](http://arxiv.org/abs/2401.15111) | 该论文提出了一种利用对比学习来改善胸部X射线诊断公平性的方法，通过对两个数据集进行评估，利用精心选择的正负样本生成公平的图像嵌入。 |
| [^50] | [Multi-agent Deep Reinforcement Learning for Dynamic Pricing by Fast-charging Electric Vehicle Hubs in ccompetition.](http://arxiv.org/abs/2401.15108) | 本文提出了一个多智能体深度强化学习的方法，应用于快速充电电动车中心的动态定价竞争。通过预测性购买电力需求和设定竞争性价格策略，充电站可以在竞争中进行有效定价。 |
| [^51] | [Optimal Potential Shaping on SE(3) via Neural ODEs on Lie Groups.](http://arxiv.org/abs/2401.15107) | 本文提出了一种在有限维李群上优化动态系统的新方法，通过将动态系统表示为神经常微分方程，并在李群上制定优化问题。提出了一种可扩展的梯度下降算法来解决优化问题，并通过在李代数级别表示系统来降低计算成本。在一个例子中，处理了刚体控制的最优势能塑形，并通过迭代优化控制器来验证最终结果。 |
| [^52] | [Diffusion Enhancement for Cloud Removal in Ultra-Resolution Remote Sensing Imagery.](http://arxiv.org/abs/2401.15105) | 本文提出了一种基于扩散增强的云去除方法，通过在数据和方法上进行改进，实现了对超高分辨率遥感图像中云层的准确去除和详细语义内容恢复。 |
| [^53] | [PruneSymNet: A Symbolic Neural Network and Pruning Algorithm for Symbolic Regression.](http://arxiv.org/abs/2401.15103) | PruneSymNet是一种用于符号回归的新颖神经网络，可以通过贪婪修剪算法提取子网络以获得所需的符号表达式。 |
| [^54] | [Hi-Core: Hierarchical Knowledge Transfer for Continual Reinforcement Learning.](http://arxiv.org/abs/2401.15098) | Hi-Core提出了一种新的框架，通过层次化的知识迁移来增强连续强化学习。该框架包括利用大型语言模型的推理能力设定目标的高层策略制定和通过强化学习按照高层目标导向的低层策略学习。在实验中，Hi-Core展现了较强的知识迁移能力。 |
| [^55] | [A note on the capacity of the binary perceptron.](http://arxiv.org/abs/2401.15092) | 该论文研究了二进制感知机的容量问题，在确定了上界和下界后，给出了证明该容量小于0.847的条件一阶矩方法与已知结果的结合。 |
| [^56] | [Accelerating Material Property Prediction using Generically Complete Isometry Invariants.](http://arxiv.org/abs/2401.15089) | 本研究提出了一种使用通用完全等变量加速材料属性预测的方法，通过采用点距离分布(PDD)作为学习算法的表示，并开发了一个修改的自注意机制的变压器模型来利用PDD。 |
| [^57] | [Design & Implementation of Automatic Machine Condition Monitoring and Maintenance System in Limited Resource Situations.](http://arxiv.org/abs/2401.15088) | 本研究设计和实施了一种在资源有限情况下的自动机器状态监测和维护系统，通过开发成本效益的数据采集系统和特征工程和数据降维方法，解决了发展中国家缺乏预测性维护和职业健康安全文化的问题。 |
| [^58] | [Towards Interpretable Physical-Conceptual Catchment-Scale Hydrological Modeling using the Mass-Conserving-Perceptron.](http://arxiv.org/abs/2401.14521) | 本研究通过利用质量守恒感知器构建基于有向图结构的水文模型，实现了对集水区尺度水文过程的解释能力，在保持简洁性的同时能够准确地模拟各种流量动力学行为，并通过引入输入旁路机制进一步优化了模型的表现。 |
| [^59] | [Discovering Mathematical Formulas from Data via GPT-guided Monte Carlo Tree Search.](http://arxiv.org/abs/2401.14424) | 通过结合MCTS和生成式预训练模型，我们提出了一种新的符号回归算法SR-GPT，在发现数据中的数学公式方面取得了显著的改进。 |
| [^60] | [Adaptive Mobile Manipulation for Articulated Objects In the Open World.](http://arxiv.org/abs/2401.14403) | 本文介绍了一种针对开放环境中关节物体操作的全栈方法，机器人通过自适应学习框架从少量数据中学习，并通过在线实践学习适应训练分布之外的新对象。同时，还开发了低成本的移动操作硬件平台。 |
| [^61] | [Estimation of partially known Gaussian graphical models with score-based structural priors.](http://arxiv.org/abs/2401.14340) | 本论文提出了一种基于得分结构先验的算法，用于估计部分已知高斯图模型。通过使用图神经网络来估计图的得分函数，我们可以在生成样本时利用退火朗格维能扩散，从而更准确地估计后验分布。数值实验表明，我们的方法具有明显的优势。 |
| [^62] | [Communication-Efficient Federated Learning through Adaptive Weight Clustering and Server-Side Distillation.](http://arxiv.org/abs/2401.14211) | 本论文提出了一种名为FedCompress的新方法，通过动态权重聚类和服务器端知识蒸馏的结合，实现了高效通信的联邦学习。该方法在降低通信成本的同时，能够学习到高度可泛化的模型。 |
| [^63] | [Networked Multiagent Reinforcement Learning for Peer-to-Peer Energy Trading.](http://arxiv.org/abs/2401.13947) | 本文提出了一个利用多智能体强化学习框架来实现点对点能源交易的方法，该方法帮助自动化消费者的竞标和管理，并解决了可再生能源零边际成本和物理约束的问题。 |
| [^64] | [Scaling NVIDIA's multi-speaker multi-lingual TTS systems with voice cloning to Indic Languages.](http://arxiv.org/abs/2401.13851) | 本文介绍了NVIDIA开发的TTS模型，利用RAD-MMM和P-Flow实现了多语言TTS的训练，其中P-Flow在零样本TTS方面表现出色，在2024挑战中获得了第一名。 |
| [^65] | [Investigating the Efficacy of Large Language Models for Code Clone Detection.](http://arxiv.org/abs/2401.13802) | 这项研究探索了大型语言模型在代码克隆检测任务中的应用。 |
| [^66] | [Detection of Correlated Random Vectors.](http://arxiv.org/abs/2401.13429) | 本文研究了判断两个标准正态随机向量是否相关的问题，提出了一种新的方法来评估似然比的二阶矩，并发现了与整数分割函数之间的联系。 |
| [^67] | [DittoGym: Learning to Control Soft Shape-Shifting Robots.](http://arxiv.org/abs/2401.13231) | 这篇论文介绍了一种学习控制软形变机器人的方法，并且提出了一个全面的强化学习基准系统DittoGym，该系统需要对机器人的形态进行细粒度变化来完成任务。 |
| [^68] | [Gravity-Informed Deep Learning Framework for Predicting Ship Traffic Flow and Invasion Risk of Non-Indigenous Species via Ballast Water Discharge.](http://arxiv.org/abs/2401.13098) | 通过考虑航运通量密度、港口距离、贸易流量和交通枢纽的中心性指标等因素，本研究开发了一个受物理启发的模型来预测海事航运流量，并用于指导全球交通网络中入侵物种的风险评估和管理。 |
| [^69] | [Locality Sensitive Sparse Encoding for Learning World Models Online.](http://arxiv.org/abs/2401.13034) | 本文提出了一种基于局部敏感稀疏编码的线性回归模型，通过非线性随机特征实现对复杂环境的拟合。这种模型能够高效地进行稀疏更新，实现了优化拟合先前经验的Follow-The-Leader（FTL）世界模型。 |
| [^70] | [Deep multitask neural networks for solving some stochastic optimal control problems.](http://arxiv.org/abs/2401.12923) | 本文针对某些难以模拟底层状态变量的随机最优控制问题，引入了使用多任务神经网络的有效解决方案，并通过实验证明了该方法的优越性。 |
| [^71] | [Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios.](http://arxiv.org/abs/2401.12729) | 通过生成合成数据和比例类平衡技术，提高了针对小目标的物体检测性能。这项研究解决了工业场景中收集和注释小目标数据的难题，并讨论了比例类平衡技术的效果。 |
| [^72] | [NeuroSynt: A Neuro-symbolic Portfolio Solver for Reactive Synthesis.](http://arxiv.org/abs/2401.12131) | NeuroSynt是一个用于反应合成的神经符号组合求解器，它通过将神经和符号方法无缝集成来解决问题。NeuroSynt在处理具有挑战性的规范方面表现出色，为当前SYNTCOMP基准提供了新的解决方案。 |
| [^73] | [TurboSVM-FL: Boosting Federated Learning through SVM Aggregation for Lazy Clients.](http://arxiv.org/abs/2401.12012) | TurboSVM-FL是一种新颖的联邦聚合策略，通过SVM聚合为懒惰客户端增强联邦学习。这种策略在不增加客户端计算负担的情况下解决了联邦学习中的收敛速度慢的问题。 |
| [^74] | [Tensor-view Topological Graph Neural Network.](http://arxiv.org/abs/2401.12007) | 提出了一种新颖的Tensor视图拓扑图神经网络（TTG-NN），该方法结合了持久同调、图卷积和张量运算，同时捕捉了局部和全局层面上的Tensor视图拓扑（TT）和Tensor视图图（TG）结构信息。 |
| [^75] | [Knowledge Distillation on Spatial-Temporal Graph Convolutional Network for Traffic Prediction.](http://arxiv.org/abs/2401.11798) | 本论文研究了在交通预测中应用空间-时间图卷积网络和知识蒸馏的方法。知识蒸馏的思想能够实现在减少参数和保持准确性的同时提高执行效率。通过引入教师网络的空间-时间相关性，我们的方法能够使学生网络学习到复杂的交通模式。 |
| [^76] | [Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations.](http://arxiv.org/abs/2401.11792) | 本文介绍了一种安全且广义的端到端自主驾驶系统 (SGADS)，使用强化学习和示范相结合的方法解决了现有方法的低安全性、泛化能力差和采样效率低的问题，同时引入了变分推理和归一化流以准确预测驾驶轨迹，并提出了鲁棒性安全约束的制定方法。 |
| [^77] | [Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal Contrastive EHR Modelling with Hierarchical Regularisation.](http://arxiv.org/abs/2401.11648) | 通过医学代码中心的多模态对比EHR建模预测下次就诊诊断，并通过分层正则化提高性能。 |
| [^78] | [SPAND: Sleep Prediction Architecture using Network Dynamics.](http://arxiv.org/abs/2401.11113) | SPAND是一个利用网络动态的睡眠预测架构，可以通过图网络和移动设备数据来预测下一天的睡眠持续时间标签。 |
| [^79] | [Estimation of AMOC transition probabilities using a machine learning based rare-event algorithm.](http://arxiv.org/abs/2401.10800) | 本研究通过结合TAMS和Next-Generation Reservoir Computing技术，利用稀事件算法估计来源于数据的确定函数，来计算大西洋经度翻转环流（AMOC）在指定时间窗口内崩溃的概率。 |
| [^80] | [BioDiffusion: A Versatile Diffusion Model for Biomedical Signal Synthesis.](http://arxiv.org/abs/2401.10282) | BioDiffusion是一种用于生物医学信号合成的多功能扩散模型，能够产生高保真度、非稳态的多变量信号。通过利用这些合成的信号，可以有效解决生物医学信号机器学习任务中的数据不足、数据不平衡和标签复杂性等问题，提高准确性。 |
| [^81] | [Explaining Time Series via Contrastive and Locally Sparse Perturbations.](http://arxiv.org/abs/2401.08552) | 这篇论文提出了一个局部稀疏模型ContraLSP，通过引入对立样本和对比学习来解释时间序列。实证研究表明，ContraLSP在解释时间序列数据的质量上取得了实质性的改进。 |
| [^82] | [Enhancing Blood Flow Assessment in Diffuse Correlation Spectroscopy: A Transfer Learning Approach with Noise Robustness Analysis.](http://arxiv.org/abs/2401.05580) | 本研究提出了一种迁移学习方法，用于增强扩散相关光谱学中的血流评估，并通过噪声鲁棒性分析展示了其鲁棒性。 |
| [^83] | [Fast Cerebral Blood Flow Analysis via Extreme Learning Machine.](http://arxiv.org/abs/2401.05578) | 本论文提出了一种通过极限学习机快速精确分析脑血流的方法，并通过与现有算法的综合比较验证了其优越性。它展示了强大的泛化能力，在各种噪声和光学参数下都具有更高的准确性。同时，与计算效率高的神经网络相比，该方法具有较短的训练和推理时间。这种策略适用于在线训练的边缘计算应用。 |
| [^84] | [An improved genetic programming for predicting semi autogenous grinding mill throughput.](http://arxiv.org/abs/2401.05382) | 该论文介绍了一种改进的遗传编程方法，应用于预测半自磨磨机的产量。新的遗传编程变体可以提取多个方程式，从而精确预测不同训练数据群集的产量。 |
| [^85] | [Generalizable Sleep Staging via Multi-level Domain Alignment.](http://arxiv.org/abs/2401.05363) | 本文提出了一种通用的睡眠分期方法，通过引入域泛化概念，结合多级特征对齐的思想，提高了模型对未见过数据集的泛化能力。 |
| [^86] | [A Good Score Does not Lead to A Good Generative Model.](http://arxiv.org/abs/2401.04856) | 本文通过反例证明，在某些情况下，即使评分函数学习良好，基于评分的生成模型（SGMs）仍然无法生成接近真实数据分布的样本，并且只能产生训练数据点的高斯模糊样本。 |
| [^87] | [Optimization Over Trained Neural Networks: Taking a Relaxing Walk.](http://arxiv.org/abs/2401.03451) | 本文提出了一种更可扩展的启发式方法来优化训练过的神经网络，通过探索全局和局部线性松弛来产生更好的解决方案。 |
| [^88] | [A quatum inspired neural network for geometric modeling.](http://arxiv.org/abs/2401.01801) | 这个论文介绍了一种创新的矩阵乘积态(MPS)的消息传递策略，通过这种策略可以更好地捕捉几何图中的复杂关系。 |
| [^89] | [Backstepping Neural Operators for $2\times 2$ Hyperbolic PDEs.](http://arxiv.org/abs/2312.16762) | 本文介绍了一种用于$2\times 2$双曲PDE的Backstepping神经操作员方法。通过考虑耦合的Goursat形式PDE，并建立了从植被PDE功能系数到核PDE解的映射的连续性，证明了DeepONet逼近核PDE解的存在性 |
| [^90] | [A Theoretical Analysis of Efficiency Constrained Utility-Privacy Bi-Objective Optimization in Federated Learning.](http://arxiv.org/abs/2312.16554) | 本文从理论上分析了联邦学习中受效率限制的效用-隐私双目标优化。先前的研究主要关注效用-隐私的权衡，忽视了训练效率和其他影响因素。该研究对差分隐私联邦学习中的关键问题进行了系统分析。 |
| [^91] | [Context-aware Communication for Multi-agent Reinforcement Learning.](http://arxiv.org/abs/2312.15600) | 这项研究针对多智能体强化学习提出了一种上下文感知的通信方案，通过两个阶段的交流，使智能体能够发送个性化的消息，从而提高合作和团队性能。 |
| [^92] | [The role of data embedding in equivariant quantum convolutional neural networks.](http://arxiv.org/abs/2312.13250) | 本论文研究了数据嵌入对等变量量子卷积神经网络的性能的影响。通过分析数据嵌入方法与对称群表示之间的关系以及不同表示对网络可表达性的影响，我们发现分类准确率与嵌入方法有明确的相关性。 |
| [^93] | [Stronger Graph Transformer with Regularized Attention Scores.](http://arxiv.org/abs/2312.11730) | 本论文提出了一种新颖的边缘正则化技术版本，用于缓解图神经网络在内存问题上存在的困扰。与没有位置编码的Graph Transformer相比，应用了边缘正则化技术确实可以稳定地提高性能。 |
| [^94] | [Time-Transformer: Integrating Local and Global Features for Better Time Series Generation.](http://arxiv.org/abs/2312.11714) | 本文提出了一种新的时间序列生成模型，通过时间变换器同时学习本地和全局特征，实现了对时间序列数据的更好生成能力。 |
| [^95] | [Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint.](http://arxiv.org/abs/2312.11456) | 该论文研究了在KL约束下的反馈强化学习的理论框架，并提出了有效的算法和实践。实证评估表明，该框架在大型语言模型的对齐实验中表现出良好的效果。 |
| [^96] | [Unraveling Batch Normalization for Realistic Test-Time Adaptation.](http://arxiv.org/abs/2312.09486) | 本文研究了测试时领域适应的问题，通过揭示批次归一化的内部机制，并介绍了测试时指数移动平均（TEMA）方法来弥补训练和测试批次之间的类别多样性差距，从而提高了准确的目标估计。 |
| [^97] | [Evolving Reservoirs for Meta Reinforcement Learning.](http://arxiv.org/abs/2312.06695) | 本论文提出了一种进化沉积池的计算模型，用于研究动物适应环境的机制。这种模型基于元增强学习框架，通过演化和发展之间的相互作用，利用进化沉积池来加速和引导强化学习过程。 |
| [^98] | [The sample complexity of multi-distribution learning.](http://arxiv.org/abs/2312.04027) | 本文解决了多分布学习的样本复杂度问题，并给出了匹配下界的样本复杂度算法。 |
| [^99] | [A Pseudo-Semantic Loss for Autoregressive Models with Logical Constraints.](http://arxiv.org/abs/2312.03905) | 本论文提出了一种针对具有逻辑约束的自回归模型的伪语义损失方法，通过在模型输出的局部近似上优化约束的似然，提高了神经符号学习的效率和适用性。 |
| [^100] | [MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs.](http://arxiv.org/abs/2312.03731) | 本文提出了一种名为MultiGPrompt的多任务预训练和提示框架，用于在图形表示学习中提高鲁棒性和减少标注成本。 |
| [^101] | [Automatic Functional Differentiation in JAX.](http://arxiv.org/abs/2311.18727) | 我们在JAX中扩展了自动微分功能，使其能够自动微分高阶函数，通过引入一组原始算子，我们实现了函数微分的线性化和转置规则，并展示了该工具在函数导数应用中的效果和简单性。 |
| [^102] | [Imputation using training labels and classification via label imputation.](http://arxiv.org/abs/2311.16877) | 本论文提出一种在填充缺失数据时将标签与输入堆叠的方法，能够显著提高填充效果，并同时填充标签和输入。该方法适用于各种类型的数据，且在实验证明具有有希望的准确性结果。 |
| [^103] | [Dynamic Fault Characteristics Evaluation in Power Grid.](http://arxiv.org/abs/2311.16522) | 该论文提出了一种在电力系统中进行故障检测的新方法，通过图神经网络识别故障节点，并利用前后时间段内节点的状态来辅助当前故障检测。实验证明该方法准确可靠，并提供了对故障节点传播的定性分析。 |
| [^104] | [Automatic Time Signature Determination for New Scores Using Lyrics for Latent Rhythmic Structure.](http://arxiv.org/abs/2311.15480) | 本文提出了一种新颖的方法，通过仅使用歌词作为输入，自动生成适合歌词歌曲的节拍记号，并揭示潜在的节奏结构。 |
| [^105] | [Achieving Margin Maximization Exponentially Fast via Progressive Norm Rescaling.](http://arxiv.org/abs/2311.14387) | 通过PRGD算法，我们在分类线性可分数据时实现了指数级快速边界最大化，与现有的算法相比，取得了显著的改进。 |
| [^106] | [Knowledge Graph Construction in Power Distribution Networks.](http://arxiv.org/abs/2311.08724) | 本文提出了一种在电力分配网络中构建知识图谱的方法，该方法利用实体特征，在分配网络的知识图谱和分配文本中进行匹配，通过实验证明了其在电力分配知识图谱构建任务中的高准确性。 |
| [^107] | [Modeling Complex Disease Trajectories using Deep Generative Models with Semi-Supervised Latent Processes.](http://arxiv.org/abs/2311.08149) | 本文提出了一种深度生成模型的时间序列方法，利用潜在时间过程来模拟复杂疾病轨迹。通过结合医学知识和半监督方法，该方法可以解释和全面分析疾病轨迹，并用于进一步的数据分析和临床假设测试。 |
| [^108] | [Communication-Constrained Bayesian Active Knowledge Distillation.](http://arxiv.org/abs/2311.08053) | 本研究提出了一种名为通信受限的贝叶斯主动知识蒸馏（CC-BAKD）的新协议，通过使用线性混合机制将贝叶斯主动学习与压缩相结合，解决了在学习者与教师之间进行通信时关于批次选择和批次编码的重要问题。 |
| [^109] | [Language Models are Better Bug Detector Through Code-Pair Classification.](http://arxiv.org/abs/2311.07957) | 本研究提出了一种代码对分类任务，通过给语言模型同时提供有错误和无错误版本的代码，实现更好的漏洞检测。研究结果表明，这个任务相比于给出代码片段并判断是否存在错误及其位置要容易得多。 |
| [^110] | [Tabdoor: Backdoor Vulnerabilities in Transformer-based Neural Networks for Tabular Data.](http://arxiv.org/abs/2311.07550) | 这项研究全面分析了使用DNNs对表格数据进行后门攻击，揭示了基于转换器的DNNs对表格数据非常容易受到后门攻击，甚至只需最小的特征值修改。该攻击还可以推广到其他模型。 |
| [^111] | [GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling.](http://arxiv.org/abs/2311.01927) | GateLoop是一种完全数据控制的线性递归序列模型，优于现有模型，可以提供数据控制的相对位置信息给Attention。 |
| [^112] | [GOPlan: Goal-conditioned Offline Reinforcement Learning by Planning with Learned Models.](http://arxiv.org/abs/2310.20025) | GOPlan是一个使用学习模型进行计划的目标条件下的离线强化学习方法，通过预训练先验策略和使用重新分析方法生成虚构轨迹，用以提高性能和处理有限数据预算和未见目标泛化的能力。 |
| [^113] | [Feature Aggregation in Joint Sound Classification and Localization Neural Networks.](http://arxiv.org/abs/2310.19063) | 本研究通过在声音分类和定位网络中引入特征聚合技术，提升了模型性能和特征的鲁棒性，特别适用于区分直接和间接声音信号的SSL网络。 |
| [^114] | [Towards Zero Shot Learning in Restless Multi-armed Bandits.](http://arxiv.org/abs/2310.14526) | 通过开发一个基于神经网络的预训练模型，我们实现了在不断变化的多臂赌博机中的零样本学习，该模型具有泛化能力，并且能够在特定实例上进行高效微调，同时适用于多行为设置和离散或连续状态空间。 |
| [^115] | [GraphMaker: Can Diffusion Models Generate Large Attributed Graphs?.](http://arxiv.org/abs/2310.13833) | GraphMaker是一种专门设计用于生成大型带属性图的新颖扩散模型。 |
| [^116] | [Uncertainty-aware transfer across tasks using hybrid model-based successor feature reinforcement learning.](http://arxiv.org/abs/2310.10818) | 该论文提出了一种利用混合模型基于后继特征强化学习方法，能够在具有不同转移动力学和奖励函数的任务之间实现样本高效的不确定性感知知识传递。 |
| [^117] | [Machine Learning Techniques for Identifying the Defective Patterns in Semiconductor Wafer Maps: A Survey, Empirical, and Experimental Evaluations.](http://arxiv.org/abs/2310.10705) | 本文综述了利用机器学习技术在半导体制造中识别晶圆缺陷的方法，提出了一种创新的分类体系，为不同算法和技术之间的关系提供了深入分析，并通过严谨的评估验证了算法性能。 |
| [^118] | [Passive Inference Attacks on Split Learning via Adversarial Regularization.](http://arxiv.org/abs/2310.10483) | 该论文介绍了一种针对拆分学习的被动推理攻击框架SDAR，通过利用辅助数据和对抗性正则化来推断客户端的私有特征和标签，在实验中取得了与主动攻击相当的攻击性能。 |
| [^119] | [Taming the Sigmoid Bottleneck: Provably Argmaxable Sparse Multi-Label Classification.](http://arxiv.org/abs/2310.10443) | 本文研究了多标签分类任务中的S形瓶颈问题，并提出了通过引入离散傅立叶变换（DFT）输出层来解决这个问题的方法。实验结果表明，该方法能够有效地预测具有稀疏标签组合的输入数据。 |
| [^120] | [Observatory: Characterizing Embeddings of Relational Tables.](http://arxiv.org/abs/2310.07736) | Observatory提出了一个正式框架来分析关系表的嵌入表示，以帮助研究人员和实践者更好地理解和选择适合特定任务的模型。 |
| [^121] | [BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations.](http://arxiv.org/abs/2310.07276) | BioT5是一个全面的预训练框架，在生物学中利用化学知识和自然语言关联丰富了跨模态整合，通过鲁棒的分子表示和上下文知识提取，实现了更有效的信息利用，展现出卓越的性能。 |
| [^122] | [The Lattice Overparametrization Paradigm for the Machine Learning of Lattice Operators.](http://arxiv.org/abs/2310.06639) | 本文介绍了一种通过过参数化格子，利用格子函数最小化算法进行学习的范式，以克服格子操作器机器学习中的三个潜在瓶颈。 |
| [^123] | [LARA: A Light and Anti-overfitting Retraining Approach for Unsupervised Anomaly Detection.](http://arxiv.org/abs/2310.05668) | LARA是一种轻量级且抗过拟合的无监督异常检测再训练方法，它将重新训练过程形式化为一个凸问题，并设计了一个反思模块以利用历史数据，同时数学证明了在微调后可以获得更好的性能。 |
| [^124] | [Surgical Gym: A high-performance GPU-based platform for reinforcement learning with surgical robots.](http://arxiv.org/abs/2310.04676) | 这项工作提出了一个高性能GPU平台，用于增强学习与手术机器人的协作。通过提高模拟器效率和训练数据的易获取性，为手术自动化的深度强化学习提供了可扩展的解决方案。 |
| [^125] | [The Cadenza ICASSP 2024 Grand Challenge.](http://arxiv.org/abs/2310.03480) | Cadenza项目组织了ICASSP SP Cadenza Challenge，旨在通过音乐分解/混音来提升助听器音质，处理过程考虑音乐、增益和听力损失。 |
| [^126] | [Low-Resource Languages Jailbreak GPT-4.](http://arxiv.org/abs/2310.02446) | 通过翻译不安全的英文输入成低资源语言，我们成功绕过了GPT-4的安全机制，并展示了这种跨语言漏洞。这一方法在实验中取得了与甚至超过了最先进的越狱攻击的效果，揭示了低资源语言在AI安全性中的薄弱环节。 |
| [^127] | [Time-LLM: Time Series Forecasting by Reprogramming Large Language Models.](http://arxiv.org/abs/2310.01728) | 这项工作介绍了Time-LLM，一个重新编程的框架，通过重新利用大型语言模型，可以进行一般的时间序列预测。 |
| [^128] | [Generalized Activation via Multivariate Projection.](http://arxiv.org/abs/2309.17194) | 通过将ReLU视为从R投影到非负半线R+的操作，我们将其通过用凸锥的广义投影算子替代，扩展为具有多个输入和多个输出的多变量投影单元 (MPU)激活函数，并证明其在表达能力方面优于ReLU激活的FNN。 |
| [^129] | [Supervised Learning Models for Early Detection of Albuminuria Risk in Type-2 Diabetes Mellitus Patients.](http://arxiv.org/abs/2309.16742) | 该论文研究了2型糖尿病患者白蛋白尿的早期预测问题，并开发了一种监督学习模型。通过使用不同的监督学习算法对184条数据进行训练，得出了预测结果。 |
| [^130] | [Identifiability Matters: Revealing the Hidden Recoverable Condition in Unbiased Learning to Rank.](http://arxiv.org/abs/2309.15560) | 研究揭示在无偏学习排名中，当点击数据不能完全拟合时，无法恢复真实相关性，导致排名性能显著降低，提出了可识别性图模型作为解决方案。 |
| [^131] | [Revisiting LARS for Large Batch Training Generalization of Neural Networks.](http://arxiv.org/abs/2309.14053) | 本文通过对大批量训练技术的研究，提出了一种新的算法TVLARS，该算法利用可配置的函数替代了热身阶段，以实现对于神经网络的稳健训练。实验证明，在大多数情况下，TVLARS比LARS和LAMB都有更好的性能表现，特别是在自监督学习方面。 |
| [^132] | [Enhancing Student Performance Prediction on Learnersourced Questions with SGNN-LLM Synergy.](http://arxiv.org/abs/2309.13500) | 这项研究介绍了一种创新的策略，将有符号图神经网络（SGNNs）和大型语言模型（LLM）的潜力协同起来，用于预测学生在学习者提供的问题上的表现。该方法利用有符号二分图全面建模学生回答，并采用对比学习框架增强了噪声的鲁棒性。 |
| [^133] | [Towards LLM-guided Causal Explainability for Black-box Text Classifiers.](http://arxiv.org/abs/2309.13340) | 本文提出了一种利用大型语言模型（LLM）引导黑盒文本分类器的因果可解释性的方法，通过生成反事实解释来解决这一挑战。 |
| [^134] | [Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation.](http://arxiv.org/abs/2309.11765) | 本论文提出了一种隐私保护下的上下文学习算法，通过生成具有差分隐私保证的合成少量示范，实现了有效的ICL。实验证明该算法在强隐私级别下能够取得竞争性能，为广泛应用领域的隐私保护下ICL开辟了新的可能性。 |
| [^135] | [Scalable neural network models and terascale datasets for particle-flow reconstruction.](http://arxiv.org/abs/2309.06782) | 本研究针对高能电子-正电子碰撞中的粒子流重建，使用可扩展的机器学习模型，并通过超参数调优和硬件处理器的高度可移植性，取得了真实且具有竞争力的物理性能。 |
| [^136] | [How does representation impact in-context learning: A exploration on a synthetic task.](http://arxiv.org/abs/2309.06054) | 本研究通过探索表示学习的角度，研究了表示对上下文学习的影响。实验结果表明，在上下文学习中，上下文内部成分对学习性能起到重要作用。 |
| [^137] | [Reinforcement Learning-assisted Evolutionary Algorithm: A Survey and Research Opportunities.](http://arxiv.org/abs/2308.13420) | 本文调查了强化学习辅助进化算法（RL-EA），该算法将强化学习与进化算法结合，以提高优化性能。对各种RL-EA的结构、操作符和搜索模式进行了分类和概述。 |
| [^138] | [Bayesian low-rank adaptation for large language models.](http://arxiv.org/abs/2308.13111) | 本研究提出了一种名为Laplace-LoRA的贝叶斯方法，通过应用拉普拉斯近似来增强经过微调的大型语言模型的校准能力。 |
| [^139] | [HypBO: Expert-Guided Chemist-in-the-Loop Bayesian Search for New Materials.](http://arxiv.org/abs/2308.11787) | HypBO是一种利用专家人类知识引导贝叶斯搜索的方法，通过生成改进的样本种子来更快地找到有希望的化学空间区域。 |
| [^140] | [Learning Logic Programs by Discovering Higher-Order Abstractions.](http://arxiv.org/abs/2308.08334) | 本论文介绍了一种通过发现高阶抽象来学习逻辑程序的方法，并且在多个领域的实验结果表明，该方法能够显著提高预测精度并减少学习时间。 |
| [^141] | [Adaptive Tracking of a Single-Rigid-Body Character in Various Environments.](http://arxiv.org/abs/2308.07491) | 本研究提出了一种基于单刚体角色仿真的深度强化学习方法，通过训练一个能够自适应各种环境变化的策略，实现在不需要额外学习的情况下完成各种任务。 |
| [^142] | [Rating-based Reinforcement Learning.](http://arxiv.org/abs/2307.16348) | 本文提出了一种基于评分的强化学习方法，通过利用人类评分来获得人类指导，该方法不同于现有的强化学习方法，它通过对样本轨迹的评估来进行学习。研究结果表明，该方法在实验中取得了良好的效果和收益。 |
| [^143] | [Are Transformers with One Layer Self-Attention Using Low-Rank Weight Matrices Universal Approximators?.](http://arxiv.org/abs/2307.14023) | 通过澄清softmax函数与Boltzmann算符之间的关系，我们证明了单层具有低秩权重矩阵的自注意力具备完全捕获整个输入序列上下文的能力，单层Transformer对于有限样本的记忆能力，单层自注意力Transformer是紧凑域上连续函数的通用逼近器。 |
| [^144] | [REX: Rapid Exploration and eXploitation for AI Agents.](http://arxiv.org/abs/2307.08962) | 本文提出了一种增强型的快速探索与利用的AI代理方法REX，它通过引入额外的奖励层和类似于UCB分数的概念，实现了更强大和高效的AI代理性能，并且具有离线行为利用和与基础模型无缝集成的优势。 |
| [^145] | [Accelerating Distributed ML Training via Selective Synchronization.](http://arxiv.org/abs/2307.07950) | 本文介绍了一种名为SelSync的方法，通过选择性同步，在保持准确性的前提下减少了分布式深度神经网络训练的时间开销。该方法根据每一步的重要性动态选择是否进行通信，达到了与批量同步并行（BSP）相同或更高的准确性。 |
| [^146] | [Unified Transfer Learning Models for High-Dimensional Linear Regression.](http://arxiv.org/abs/2307.00238) | UTrans是一种统一转移学习模型，它能检测可转移变量和源数据，并具有较低的估计和预测误差，同时保持可解释性。 |
| [^147] | [Black-Box Prediction of Flaky Test Fix Categories Using Language Models.](http://arxiv.org/abs/2307.00012) | 本文提出了一个使用语言模型的框架，可以自动生成易出错测试的标记数据集，并通过分析测试代码来预测测试的修复类别。实验结果表明UniXcoder优于CodeBERT。 |
| [^148] | [To Spike or Not To Spike: A Digital Hardware Perspective on Deep Learning Acceleration.](http://arxiv.org/abs/2306.15749) | 神经形态计算旨在通过仿真脑部操作来提高深度学习模型的效率，但是在SNNs的高效硬件后端设计上仍需进一步研究。 |
| [^149] | [SPRINT: Scalable Policy Pre-Training via Language Instruction Relabeling.](http://arxiv.org/abs/2306.11886) | SPRINT 提出了一种离线策略预训练方法，通过指令重标记及离线强化学习实现可扩展的预训练任务，大大减少了预训练所需的人力，同时使机器人能够获取更丰富的技能库，相较于之前的预训练方法，能够更快地学习新的长时间跨度任务。 |
| [^150] | [AdaStop: sequential testing for efficient and reliable comparisons of Deep RL Agents.](http://arxiv.org/abs/2306.10882) | AdaStop是一种基于多组序列测试的新统计测试方法，可用于比较多个深度强化学习算法来解决实验结果可复制性的问题。 |
| [^151] | [High-Resolution Convolutional Neural Networks on Homomorphically Encrypted Data via Sharding Ciphertexts.](http://arxiv.org/abs/2306.09189) | 通过分片密文上的同态加密数据，我们扩展了在大尺寸和多通道图像上评估深度卷积神经网络的方法，并简化了多路复用图像格式的效率。我们还展示了如何正则化现有模型以提高效率和准确性。 |
| [^152] | [DiffECG: A Generalized Probabilistic Diffusion Model for ECG Signals Synthesis.](http://arxiv.org/abs/2306.01875) | 本文介绍了一种新颖的ECG信号合成方法——基于去噪扩散概率模型的DiffECG，能够涵盖三种情形，并且是ECG合成的第一个广义条件方法。实验证明该方法的有效性以及优于其他ECG生成模型并可提高分类器性能。 |
| [^153] | [Improving Expressivity of Graph Neural Networks using Localization.](http://arxiv.org/abs/2305.19659) | 本文提出了Weisfeiler-Leman (WL)算法的局部版本，用于解决子图计数问题并提高图神经网络的表达能力，同时，也给出了一些时间和空间效率更高的$k-$WL变体和分裂技术。 |
| [^154] | [Exploring Weight Balancing on Long-Tailed Recognition Problem.](http://arxiv.org/abs/2305.16573) | 研究分析了新提出的权重平衡方法在长尾识别问题中有效的原因，发现其能够缓解神经崩溃和圆锥效应，从而提高识别性能。 |
| [^155] | [Candidate Set Re-ranking for Composed Image Retrieval with Dual Multi-modal Encoder.](http://arxiv.org/abs/2305.16304) | 本论文提出了一种使用两阶段模式结合预先计算图像嵌入和参考文本-候选项三元组交互选择的方式进行组合图像检索候选集重排序的方法。 |
| [^156] | [Neural Cellular Automata Can Respond to Signals.](http://arxiv.org/abs/2305.12971) | 神经元元胞自动机对信号作出响应的能力，为神经元元胞自动机作为人工形态发生模型的发展提供了基础，并且为将动态行为嵌入模型铺平了道路。 |
| [^157] | [GraVAC: Adaptive Compression for Communication-Efficient Distributed DL Training.](http://arxiv.org/abs/2305.12201) | GraVAC提出了一个动态调整压缩因子的框架，通过评估模型进展和评估与压缩相关的梯度信息损失来进行训练。GraVAC可以在不需要任何关于模型或其超参数的先前假设的情况下，达到与先前最先进的压缩方法相同或更好的翻译准确性。在CIFAR-10和ImageNet数据集上，相对于静态压缩对应物，GraVAC可以将通信减少高达87％和75％。 |
| [^158] | [Effects of Real-Life Traffic Sign Alteration on YOLOv7- an Object Recognition Model.](http://arxiv.org/abs/2305.05499) | 本研究调查了改变后的交通标志对目标识别模型准确性和性能的影响，结果表明当暴露于不太可能的条件下修改后的交通标志时，物体检测模型的准确率显著降低。 |
| [^159] | [Self-Repellent Random Walks on General Graphs - Achieving Minimal Sampling Variance via Nonlinear Markov Chains.](http://arxiv.org/abs/2305.05097) | 本文设计了一种自我排斥随机游走模型，可实现较小的渐近采样方差，适用于网络拓扑的采样和邻域探索。 |
| [^160] | [On Preimage Approximation for Neural Networks.](http://arxiv.org/abs/2305.03686) | 本文提出了一种基于线性松弛的高效实用的任意时刻算法，用于生成神经网络原像的符号下近似，以实现更快的改进和更高的压缩度。 |
| [^161] | [AutoColor: Learned Light Power Control for Multi-Color Holograms.](http://arxiv.org/abs/2305.01611) | AutoColor 是第一个学习正确照明多色全息图所需光源功率的方法，将优化多色全息图所需的步骤数从超过1000个降至70个迭代步骤。 |
| [^162] | [Feasible Policy Iteration.](http://arxiv.org/abs/2304.08845) | 可行性策略迭代 (FPI) 是一个间接的安全强化学习方法，使用上一个策略的可行域来迭代地限制当前策略。可行性策略改进是其核心，它在可行域内最大化回报，在可行域外最小化约束衰减函数 (CDF). |
| [^163] | [Model sparsification can simplify machine unlearning.](http://arxiv.org/abs/2304.04934) | 本文提出了一种基于模型稀疏化的机器反学习方案，称为prune first, then unlearn和sparsity-aware unlearning。此方案可以提高近似反学习器的多标准反学习性能，并在不同的场景中表现出一致的效果。 |
| [^164] | [Cube-Based 3D Denoising Diffusion Probabilistic Model for Cone Beam Computed Tomography Reconstruction with Incomplete Data.](http://arxiv.org/abs/2303.12861) | 本文提出了一种基于立方体的3D去噪扩散概率模型（DDPM）来重建CBCT并解决了存储整个正弦图的内存问题。通过将整个CBCT volume分成多个小立方体，该模型能够实现高效的计算并在视觉和定量评价方面优于现有方法。 |
| [^165] | [Magnushammer: A Transformer-based Approach to Premise Selection.](http://arxiv.org/abs/2303.04488) | Magnushammer是一种基于Transformer的前提选择方法，通过在PISA基准上的测试表明，它可以大幅度超越传统符号系统，并将先前最先进的证明率从57.0％提高到71.0％。 |
| [^166] | [Evaluating explainability for machine learning predictions using model-agnostic metrics.](http://arxiv.org/abs/2302.12094) | 本文提出了一种使用模型无关的度量标准，用于评估机器学习模型的预测结果的可解释性。这些度量标准将各个解释能力方面总结成标量，提供全面的理解并促进决策者和利益相关者之间的沟通，从而提高整体的透明度。 |
| [^167] | [Modular Deep Learning.](http://arxiv.org/abs/2302.11529) | 模块化深度学习是一种有前景的解决方案，通过将计算与路由和局部更新模块分离，实现了积极迁移和系统化的推广。 |
| [^168] | [Detecting Reddit Users with Depression Using a Hybrid Neural Network SBERT-CNN.](http://arxiv.org/abs/2302.02759) | 本论文介绍了一种混合神经网络模型，结合了预训练的SBERT和CNN，用于通过分析Reddit用户的帖子自动识别抑郁症患者。 |
| [^169] | [Aligning Robot and Human Representations.](http://arxiv.org/abs/2302.01928) | 本文研究了机器人与人类表征之间的对齐问题，指出了当前学习方法存在的表示不对齐的困境，并建议应将机器人表征学习方法从实现任务目标的角度转向与人类表征对齐的问题。 |
| [^170] | [AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning.](http://arxiv.org/abs/2301.12132) | AutoPEFT是一个自动化的PEFT（参数高效微调）配置搜索方法，它能够自动地找到最佳的PEFT模块和体系结构，以优化任务的性能和参数效率。在典型的NLP任务中，AutoPEFT表现出比手动设计更好的性能。 |
| [^171] | [ScaDLES: Scalable Deep Learning over Streaming data at the Edge.](http://arxiv.org/abs/2301.08897) | ScaDLES是一种用于边缘端流式数据上的可扩展深度学习方法，解决了系统和统计异质性的挑战。 |
| [^172] | [Gamma-convergence of a nonlocal perimeter arising in adversarial machine learning.](http://arxiv.org/abs/2211.15223) | 本文研究了在对抗机器学习中出现的非局部周长的伽玛收敛性问题，证明了其与局部各向异性周长的伽玛收敛。该工作对于理解对抗性训练在二元分类中的正则化效果以及相关优化问题具有重要意义。 |
| [^173] | [Compressing Transformer-based self-supervised models for speech processing.](http://arxiv.org/abs/2211.09949) | 本文研究了对基于Transformer的自监督模型进行压缩的方法，包括权重修剪、头部修剪、低秩逼近和知识蒸馏。结果发现，基本的压缩技术是强大的基准，可以改善模型的压缩效果。 |
| [^174] | [Offline Estimation of Controlled Markov Chains: Minimaxity and Sample Complexity.](http://arxiv.org/abs/2211.07092) | 本论文研究了离线估计有限控制马尔可夫链的转移概率矩阵的非参数估计器，并通过记录策略的混合特性建立了样本复杂性界限和最小化条件。结果表明，实现特定的统计风险界限涉及到混合特性的强度和样本数量之间微妙而有趣的权衡。还使用这些样本复杂性界限建立了离线评估恒定马尔可夫控制策略的相关界限。 |
| [^175] | [Efficient Deep Reinforcement Learning with Predictive Processing Proximal Policy Optimization.](http://arxiv.org/abs/2211.06236) | 该论文介绍了一种名为预测处理近端策略优化（P4O）的深度强化学习方法，通过利用递归神经网络预测自身感觉状态来最小化惊异，从而显著提高累积奖励。 |
| [^176] | [No-Box Attacks on 3D Point Cloud Classification.](http://arxiv.org/abs/2210.14164) | 该论文介绍了一种新的方法，可以在不访问目标DNN模型的情况下预测3D点云中的对抗点，提供了无盒子攻击的新视角。 |
| [^177] | [Learning Ultrametric Trees for Optimal Transport Regression.](http://arxiv.org/abs/2210.12288) | 本文提出了一种学习超度量树的方法，以逼近原始空间中的最优输运距离。通过在超度量空间上进行投影梯度下降优化，我们能够找到具有最优树结构的树-瓦瑟斯坦距离。 |
| [^178] | [On the Relation between Sensitivity and Accuracy in In-context Learning.](http://arxiv.org/abs/2209.07661) | 在上下文学习中，我们发现ICL对多种扰动类型具有敏感性，标签偏差导致过去的研究低估了ICL的敏感性。同时，我们观察到ICL的敏感性和准确性之间呈现负相关关系。基于这些发现，我们提出了一种少样本选择性预测方法SenSel，它在放弃敏感预测决策上取得了优于常用基准方法的结果。 |
| [^179] | [A DeepParticle method for learning and generating aggregation patterns in multi-dimensional Keller-Segel chemotaxis systems.](http://arxiv.org/abs/2209.00109) | 本研究提出了一种深度粒子方法DeepParticle，用于学习和生成Keller-Segel趋化系统中的聚集模式。通过使用深度神经网络，该方法能够有效地表示并转换源分布到目标分布，从而实现物理参数变化下解的生成。 |
| [^180] | [Multi-Horizon Representations with Hierarchical Forward Models for Reinforcement Learning.](http://arxiv.org/abs/2206.11396) | 提出了一种使用层级前向模型实现多时间段表示的辅助任务HKSL，通过学习多个表示和不同步长下的评论家，能够更快地收敛到更高的或最优的回报。 |
| [^181] | [Federated Offline Reinforcement Learning.](http://arxiv.org/abs/2206.05581) | 本文提出了一种联邦离线强化学习算法，可以处理医疗机构间数据共享的隐私限制和异质性问题，同时提供了通信效率和隐私保护性。该算法的样本复杂度证明以及在现实医学数据集上的模拟实验结果表明了其有效性和效率。 |
| [^182] | [Risk Measures and Upper Probabilities: Coherence and Stratification.](http://arxiv.org/abs/2206.03183) | 该论文研究了富有决策实际意义的聚合函数类，以构建不确定性处理的数学基础，通过分析谱风险度量、Choquet积分和Lorentz范数的特性，提出了一种对风险度量进行分层的方法，并在机器学习实践中得到了验证。 |
| [^183] | [AugLoss: A Robust Augmentation-based Fine Tuning Methodology.](http://arxiv.org/abs/2206.02286) | AugLoss是一种简单而有效的方法，通过统一数据增强和稳健损失函数，实现了对训练时的噪声标注和测试时的特征分布转移的稳健性。 |
| [^184] | [MiniDisc: Minimal Distillation Schedule for Language Model Compression.](http://arxiv.org/abs/2205.14570) | 本研究提出了一个叫做MiniDisc的最小蒸馏计划，可以在最少一次尝试中调度最优的教师助手，用于实现语言模型压缩。 |
| [^185] | [Towards cost-effective and resource-aware aggregation at Edge for Federated Learning.](http://arxiv.org/abs/2204.07767) | 本研究提出了一种边缘联邦学习聚合器，通过在边缘进行自适应聚合，提高了可扩展性和时间效率，同时降低了成本。 |
| [^186] | [Hyperspectral Pixel Unmixing with Latent Dirichlet Variational Autoencoder.](http://arxiv.org/abs/2203.01327) | 本研究提出了一种使用潜在狄利克雷变分自编码器进行高光谱像素解混的方法，可以通过迁移学习范式训练模型并在真实数据上进行像素解混，取得了最新的最好结果。 |
| [^187] | [Understanding Adversarial Robustness from Feature Maps of Convolutional Layers.](http://arxiv.org/abs/2202.12435) | 通过研究卷积层的特征图，本研究发现较大的特征图可以提高神经网络对扰动的抵抗能力，这为设计鲁棒神经网络提供了新的启示，并提出了简单的修改方法来改进现有架构。 |
| [^188] | [Particle Transformer for Jet Tagging.](http://arxiv.org/abs/2202.03772) | 本研究提出了用于喷注标记的粒子变压器，并使用新的全面数据集JetClass进行训练。粒子变压器通过在注意力机制中引入粒子间的交互关系，实现了比传统变压器更高的标记性能，并在两种常用的喷注标记任务上取得了显著的性能提升。 |
| [^189] | [Computer Vision Self-supervised Learning Methods on Time Series.](http://arxiv.org/abs/2109.00783) | 该研究评估了计算机视觉自监督学习框架在时间序列上的效果，并且提出了一种改进方法，通过改进协方差项和添加迭代归一化层，加速了模型的收敛。 |
| [^190] | [Simulated Data Generation Through Algorithmic Force Coefficient Estimation for AI-Based Robotic Projectile Launch Modeling.](http://arxiv.org/abs/2105.12833) | 该论文提出了一种通过算法估计力系数来生成模拟数据的方法，以用于基于人工智能的机器人抛射建模。通过这种方法，可以解决使用物理模型进行建模时存在的未知因素和物体变形效应的问题，并且可以提高深度神经网络的性能。 |
| [^191] | [Improving Transformation-based Defenses against Adversarial Examples with First-order Perturbations.](http://arxiv.org/abs/2103.04565) | 该论文提出了一种改进基于变换的对抗示例防御方法的方法，通过对对抗示例生成一阶扰动来增加神经网络输出正确结果的概率，并在推断时应用这种方法。 |
| [^192] | [A method to integrate and classify normal distributions.](http://arxiv.org/abs/2012.14331) | 本文介绍了一种可以对任意参数维度下的任意域内正态分布进行积分的方法，提供了法向向量函数的相关概率密度和统计指标，同时还提供了可以对任意数量正态分布进行分类的方法和维度降低和可视化的技术。 |
| [^193] | [View selection in multi-view stacking: Choosing the meta-learner.](http://arxiv.org/abs/2010.16271) | 选择合适的元学习器对于多视角堆叠中的视图选择和分类准确性是非常重要的，通过对七种不同的算法进行评估，非负套索、非负自适应套索和非负弹性网络被认为是最合适的元学习器。 |
| [^194] | [An Intuitive Tutorial to Gaussian Process Regression.](http://arxiv.org/abs/2009.10862) | 《高斯过程回归的直观教程》是一篇介绍高斯过程回归的教程，旨在直观地解释GPR的基本概念、提供实现代码，并回顾最先进的高斯过程算法。适合机器学习初学者阅读，帮助他们清晰理解GPR的基本原理。 |
| [^195] | [Adversarial Attacks on Graph Neural Networks via Meta Learning.](http://arxiv.org/abs/1902.08412) | 本文通过元梯度方式对图神经网络进行训练时攻击，通过微小的图扰动导致性能下降，并证明了即使在无监督嵌入中也能产生迁移效应。这些攻击不需要任何关于目标分类器的知识或访问权限。 |

# 详细

[^1]: 新的模糊目标检测模型

    New Foggy Object Detecting Model. (arXiv:2401.15455v1 [cs.CV])

    [http://arxiv.org/abs/2401.15455](http://arxiv.org/abs/2401.15455)

    本文提出了一种新的模糊目标检测方法，并通过区域识别和目标检测两个阶段的架构实现。该方法在准确性和检测时间方面表现出显著的改进。

    

    在降低能见度条件下进行目标检测已经成为一个重要的研究领域。现有的技术在识别这种情况下的目标时不够准确。本文介绍了一种新的模糊目标检测方法，通过输入图像中的区域识别和在这些区域中检测目标的两阶段架构来实现。本文证实了所提出方法在准确性和检测时间方面相比现有技术的显著改进。

    Object detection in reduced visibility has become a prominent research area. The existing techniques are not accurate enough in recognizing objects under such circumstances. This paper introduces a new foggy object detection method through a two-staged architecture of region identification from input images and detecting objects in such regions. The paper confirms notable improvements of the proposed method's accuracy and detection time over existing techniques.
    
[^2]: 使用梯度插值和核平滑估计连续处理效果

    Continuous Treatment Effect Estimation Using Gradient Interpolation and Kernel Smoothing. (arXiv:2401.15447v1 [cs.LG])

    [http://arxiv.org/abs/2401.15447](http://arxiv.org/abs/2401.15447)

    本文提出了一种使用梯度插值和核平滑的方法，用于个性化连续处理效果估计。通过增加独立采样的处理和推断的反事实结果来处理训练数据中的混淆问题。实验证明该方法在反事实预测性能上优于其他六种最先进的方法。

    

    我们解决了个性化连续处理效果（ICTE）估计问题，通过观测数据预测任何连续值处理对个体的效果。这个估计任务的主要挑战是训练数据中处理分配与个体协变量的潜在混淆，而在推断ICTE时需要对独立采样的处理进行预测。与之前依赖于正则化器或不稳定的GAN训练的工作相反，我们主张直接方法，即通过增加独立采样的处理和推断的反事实结果来增强训练个体。 我们使用两种策略推断反事实结果：对接近观察到的处理进行梯度插值，以及基于高斯过程的核平滑，使我们能够减小推断的高方差。我们在五个基准测试上评估我们的方法，并显示我们的方法在反事实预测性能上胜过六种最先进的方法。

    We address the Individualized continuous treatment effect (ICTE) estimation problem where we predict the effect of any continuous-valued treatment on an individual using observational data. The main challenge in this estimation task is the potential confounding of treatment assignment with an individual's covariates in the training data, whereas during inference ICTE requires prediction on independently sampled treatments. In contrast to prior work that relied on regularizers or unstable GAN training, we advocate the direct approach of augmenting training individuals with independently sampled treatments and inferred counterfactual outcomes. We infer counterfactual outcomes using a two-pronged strategy: a Gradient Interpolation for close-to-observed treatments, and a Gaussian Process based Kernel Smoothing which allows us to downweigh high variance inferences. We evaluate our method on five benchmarks and show that our method outperforms six state-of-the-art methods on the counterfactu
    
[^3]: 迈向因果分类：关于图神经网络的综合研究

    Towards Causal Classification: A Comprehensive Study on Graph Neural Networks. (arXiv:2401.15444v1 [cs.LG])

    [http://arxiv.org/abs/2401.15444](http://arxiv.org/abs/2401.15444)

    这项研究通过对九种基准图分类模型的测试，揭示了因果性对图神经网络的预测能力的影响，为进一步发展和应用GNN提供了重要的见解。

    

    随着对处理图结构数据的图神经网络（GNN）的探索不断扩大，由于其通用逼近能力，尤其是它们在因果分析方面的潜力，它们被期望能够显著增强常见的基于图的任务，如分类和预测。然而，对于因果增强的GNN框架的研究仍然不够深入。为了解决这个问题，我们的研究深入探讨了九种基准图分类模型，在跨越三个不同领域的七个数据集上对它们的强度和灵活性进行了测试，以确定因果性对GNN预测能力的影响。这项研究对这些模型进行了详细评估，揭示了它们在不同数据环境中的效率和灵活性，并突出了需要进一步开发的领域。我们的发现对于深化对GNN在不同数据中心领域的理解和实际应用具有重要意义。

    The exploration of Graph Neural Networks (GNNs) for processing graph-structured data has expanded, particularly their potential for causal analysis due to their universal approximation capabilities. Anticipated to significantly enhance common graph-based tasks such as classification and prediction, the development of a causally enhanced GNN framework is yet to be thoroughly investigated. Addressing this shortfall, our study delves into nine benchmark graph classification models, testing their strength and versatility across seven datasets spanning three varied domains to discern the impact of causality on the predictive prowess of GNNs. This research offers a detailed assessment of these models, shedding light on their efficiency, and flexibility in different data environments, and highlighting areas needing advancement. Our findings are instrumental in furthering the understanding and practical application of GNNs in diverse datacentric fields
    
[^4]: 基于去中心化流言传播共学习的多参数MRI脑肿瘤分割

    Decentralized Gossip Mutual Learning (GML) for brain tumor segmentation on multi-parametric MRI. (arXiv:2401.15434v1 [eess.IV])

    [http://arxiv.org/abs/2401.15434](http://arxiv.org/abs/2401.15434)

    本论文提出了一种基于去中心化流言传播共学习的多参数MRI脑肿瘤分割框架，该框架实现了在不共享私密数据的情况下医疗中心之间的协作模型训练，并通过共学习来优化不同站点之间的数据变化。

    

    联邦学习使医疗中心之间能够在不共享私密数据的情况下进行协作模型训练。然而，传统的联邦学习在服务器故障和局部数据下性能不佳的风险方面存在问题，这是由于集中式模型聚合的特性导致的。为了解决这些问题，我们提出了去中心化的流言传播共学习（GML）框架，该框架使用流言协议进行点对点通信。此外，GML通过共学习鼓励每个站点优化其本地模型以应对不同站点之间的数据变化。在使用来自BraTS 2021数据集的四个临床站点的146个案例进行肿瘤分割的任务中，我们证明了GML胜过本地模型，并在只有25％通信开销的情况下实现了与FedAvg相似的性能。

    Federated Learning (FL) enables collaborative model training among medical centers without sharing private data. However, traditional FL risks on server failures and suboptimal performance on local data due to the nature of centralized model aggregation. To address these issues, we present Gossip Mutual Learning (GML), a decentralized framework that uses Gossip Protocol for direct peer-to-peer communication. In addition, GML encourages each site to optimize its local model through mutual learning to account for data variations among different sites. For the task of tumor segmentation using 146 cases from four clinical sites in BraTS 2021 dataset, we demonstrated GML outperformed local models and achieved similar performance as FedAvg with only 25% communication overhead.
    
[^5]: 大模型时代中的数据增强研究综述

    A Survey on Data Augmentation in Large Model Era. (arXiv:2401.15422v1 [cs.LG])

    [http://arxiv.org/abs/2401.15422](http://arxiv.org/abs/2401.15422)

    这篇论文综述了大模型驱动的数据增强方法，包括图像增强、文本增强和配对数据增强。这些方法利用大模型的能力，有效提高了数据增强的效果，是解决大模型训练中数据质量不足的重要研究方向。

    

    大模型，包括大语言和扩散模型，在近似人类级智能方面显示出卓越的潜力，引起了学术界和工业界的极大关注。然而，训练这些大模型需要大量高质量的数据，并且随着这些模型的持续更新，现有的高质量数据储备可能很快用尽。这个挑战催生了大量关于数据增强方法的研究。利用大模型，这些数据增强技术超越了传统方法。本文综合考虑，提供了大模型驱动的数据增强方法的详尽回顾。我们首先将相关研究分为图像增强、文本增强和配对数据增强三个主要类别。然后，我们深入探讨了与大模型数据增强相关的各种数据后处理技术。

    Large models, encompassing large language and diffusion models, have shown exceptional promise in approximating human-level intelligence, garnering significant interest from both academic and industrial spheres. However, the training of these large models necessitates vast quantities of high-quality data, and with continuous updates to these models, the existing reservoir of high-quality data may soon be depleted. This challenge has catalyzed a surge in research focused on data augmentation methods. Leveraging large models, these data augmentation techniques have outperformed traditional approaches. This paper offers an exhaustive review of large model-driven data augmentation methods, adopting a comprehensive perspective. We begin by establishing a classification of relevant studies into three main categories: image augmentation, text augmentation, and paired data augmentation. Following this, we delve into various data post-processing techniques pertinent to large model-based data au
    
[^6]: 利用机器学习和信号处理故障诊断感应电机

    Fault Diagnosis on Induction Motor using Machine Learning and Signal Processing. (arXiv:2401.15417v1 [cs.LG])

    [http://arxiv.org/abs/2401.15417](http://arxiv.org/abs/2401.15417)

    本研究利用机器学习和信号处理方法，研究了在Industry 4.0背景下利用MATLAB Simulink对感应电机故障进行检测和识别。生成了包含四种故障类型的数据集，通过应用快速傅里叶变换进行故障检测和识别。

    

    在Industry 4.0的背景下，利用机器学习和信号处理来检测和识别感应电机故障是避免工厂干扰和停机的有价值方法。本研究使用MATLAB Simulink对感应电机的故障检测和识别进行了研究。我们在MATLAB Simulink中开发了一个三相感应电机模型，生成了健康和故障电机数据。收集的数据包括定子电流、转子电流、输入功率、滑差、转子转速和效率。我们在感应电机中产生了四种故障：开路故障、短路故障、过载和断裂转子条。我们总共收集了15万个数据点，健康数据和故障数据比例为60:40。我们应用快速傅里叶变换(FFT)来检测和识别健康和不健康状态，并在数据中添加了一个独特的特征。生成的数据集被训练于不同的机器学习算法上。

    The detection and identification of induction motor faults using machine learning and signal processing is a valuable approach to avoiding plant disturbances and shutdowns in the context of Industry 4.0. In this work, we present a study on the detection and identification of induction motor faults using machine learning and signal processing with MATLAB Simulink. We developed a model of a three-phase induction motor in MATLAB Simulink to generate healthy and faulty motor data. The data collected included stator currents, rotor currents, input power, slip, rotor speed, and efficiency. We generated four faults in the induction motor: open circuit fault, short circuit fault, overload, and broken rotor bars. We collected a total of 150,000 data points with a 60-40% ratio of healthy to faulty motor data. We applied Fast Fourier Transform (FFT) to detect and identify healthy and unhealthy conditions and added a distinctive feature in our data. The generated dataset was trained different mach
    
[^7]: 人工神经网络验证对感应电动机声学行为的建模

    Validation of artificial neural networks to model the acoustic behaviour of induction motors. (arXiv:2401.15377v1 [cs.LG])

    [http://arxiv.org/abs/2401.15377](http://arxiv.org/abs/2401.15377)

    本研究旨在评估多任务人工神经网络作为感应电动机声学参数预测的建模技术，通过使用额定声压、响度、粗糙度和尖锐度作为输出。

    

    在过去的十年中，电动感应电机的声音质量是研究领域中的热门话题。特别是由于其广泛的应用，人们暴露在由噪音排放引起的身体和心理不适中。因此，有必要将其对人们的心理影响最小化。因此，本研究的主要目标是评估多任务人工神经网络作为感应电动机声学参数同时预测的建模技术的使用。使用了多种输入，例如电动机功率信号的电学量和极数，而不是将电动机的噪音与环境噪音分离。提出了两种不同类型的人工神经网络来评估感应电动机的声学质量，通过使用等效声压、响度、粗糙度和尖锐度作为输出。具体地说，提出了两种不同的拓扑结构来评估感应电动机的声学质量。

    In the last decade, the sound quality of electric induction motors is a hot topic in the research field. Specially, due to its high number of applications, the population is exposed to physical and psychological discomfort caused by the noise emission. Therefore, it is necessary to minimise its psychological impact on the population. In this way, the main goal of this work is to evaluate the use of multitask artificial neural networks as a modelling technique for simultaneously predicting psychoacoustic parameters of induction motors. Several inputs are used, such as, the electrical magnitudes of the motor power signal and the number of poles, instead of separating the noise of the electric motor from the environmental noise. Two different kind of artificial neural networks are proposed to evaluate the acoustic quality of induction motors, by using the equivalent sound pressure, the loudness, the roughness and the sharpness as outputs. Concretely, two different topologies have been con
    
[^8]: 基于深度学习和信息融合的长期产前电子胎儿心率监测健康监测技术研究

    Deep Learning with Information Fusion and Model Interpretation for Health Monitoring of Fetus based on Long-term Prenatal Electronic Fetal Heart Rate Monitoring Data. (arXiv:2401.15337v1 [cs.LG])

    [http://arxiv.org/abs/2401.15337](http://arxiv.org/abs/2401.15337)

    本研究通过结合深度学习和信息融合方法，开发了一个名为LARA的自动分析系统，用于长期产前电子胎儿心率监测。该系统通过卷积神经网络模型处理长期的FHR数据，提供了更全面的对胎儿状态的理解。

    

    胎儿心率（FHR）的长期监测在产前期间越来越受欢迎，其中电子FHR监测被广泛采用。与短期监测相比，这种连续监测可以收集更长时间的胎儿心率数据，从而更全面地了解胎儿的情况。然而，长期产前胎儿心率监测的解释仍处于早期阶段，缺乏相应的临床标准。此外，连续监测产生的大量数据在手动分析时对临床工作造成了重大负担。为了解决上述挑战，本研究开发了一个名为LARA（长期产前风险分析系统）的自动分析系统，结合了深度学习和信息融合方法。LARA的核心是一个成熟的卷积神经网络（CNN）模型，它将长期的FHR数据作为输入进行处理。

    Long-term fetal heart rate (FHR) monitoring during the antepartum period, increasingly popularized by electronic FHR monitoring, represents a growing approach in FHR monitoring. This kind of continuous monitoring, in contrast to the short-term one, collects an extended period of fetal heart data. This offers a more comprehensive understanding of fetus's conditions. However, the interpretation of long-term antenatal fetal heart monitoring is still in its early stages, lacking corresponding clinical standards. Furthermore, the substantial amount of data generated by continuous monitoring imposes a significant burden on clinical work when analyzed manually. To address above challenges, this study develops an automatic analysis system named LARA (Long-term Antepartum Risk Analysis system) for continuous FHR monitoring, combining deep learning and information fusion methods. LARA's core is a well-established convolutional neural network (CNN) model. It processes long-term FHR data as input 
    
[^9]: L-AutoDA: 利用大型语言模型进行自动决策型对抗攻击

    L-AutoDA: Leveraging Large Language Models for Automated Decision-based Adversarial Attacks. (arXiv:2401.15335v1 [cs.CR])

    [http://arxiv.org/abs/2401.15335](http://arxiv.org/abs/2401.15335)

    本文介绍了一种名为L-AutoDA的创新方法，利用大型语言模型自动设计决策型对抗攻击。通过与大型语言模型的迭代交互，L-AutoDA能够高效地生成竞争性的攻击算法，显示出在成功率和计算效率方面的显著改进。

    

    在快速发展的机器学习领域中，对抗攻击对模型的健壮性和安全性提出了显著挑战。决策型攻击只需要模型的决策反馈，而不需要详细的概率或分数，因此特别难以防御。本研究引入了L-AutoDA（基于大型语言模型自动生成决策型对抗攻击）的创新方法，利用大型语言模型的生成能力自动设计这些攻击。通过在进化框架中与大型语言模型进行迭代交互，L-AutoDA能够高效地自动设计出竞争性的攻击算法，减少人工工作量。我们在CIFAR-10数据集上展示了L-AutoDA的有效性，显示出在成功率和计算效率方面相比基准方法的显著改进。我们的研究结果突显了语言模型作为对抗攻击生成工具的潜力。

    In the rapidly evolving field of machine learning, adversarial attacks present a significant challenge to model robustness and security. Decision-based attacks, which only require feedback on the decision of a model rather than detailed probabilities or scores, are particularly insidious and difficult to defend against. This work introduces L-AutoDA (Large Language Model-based Automated Decision-based Adversarial Attacks), a novel approach leveraging the generative capabilities of Large Language Models (LLMs) to automate the design of these attacks. By iteratively interacting with LLMs in an evolutionary framework, L-AutoDA automatically designs competitive attack algorithms efficiently without much human effort. We demonstrate the efficacy of L-AutoDA on CIFAR-10 dataset, showing significant improvements over baseline methods in both success rate and computational efficiency. Our findings underscore the potential of language models as tools for adversarial attack generation and highli
    
[^10]: 最优稀疏生存树

    Optimal Sparse Survival Trees. (arXiv:2401.15330v1 [cs.LG])

    [http://arxiv.org/abs/2401.15330](http://arxiv.org/abs/2401.15330)

    本研究提出了一种动态规划边界法，可以在几秒钟内找到可证明最优稀疏生存树模型，对于涉及人类健康的高风险问题的分析和决策具有重要意义。

    

    在涉及人类健康的高风险问题的分析和决策中，可解释性对于医生、医院、制药公司和生物技术公司至关重要。由于其吸引人的可解释性和捕捉复杂关系的能力，基于树的方法已被广泛应用于生存分析。然而，大多数现有的生成生存树的方法依赖于启发式（或贪婪）算法，存在生成次优模型的风险。我们提出了一种动态规划边界法，可以在几秒钟内找到可证明最优稀疏生存树模型。

    Interpretability is crucial for doctors, hospitals, pharmaceutical companies and biotechnology corporations to analyze and make decisions for high stakes problems that involve human health. Tree-based methods have been widely adopted for \textit{survival analysis} due to their appealing interpretablility and their ability to capture complex relationships. However, most existing methods to produce survival trees rely on heuristic (or greedy) algorithms, which risk producing sub-optimal models. We present a dynamic-programming-with-bounds approach that finds provably-optimal sparse survival tree models, frequently in only a few seconds.
    
[^11]: 在考虑到不完整的拓扑信息的前提下定位虚假数据注入攻击在电力系统中的研究：一种时空图波卷积神经网络方法

    Localization of Dummy Data Injection Attacks in Power Systems Considering Incomplete Topological Information: A Spatio-Temporal Graph Wavelet Convolutional Neural Network Approach. (arXiv:2401.15321v1 [eess.SY])

    [http://arxiv.org/abs/2401.15321](http://arxiv.org/abs/2401.15321)

    本文研究了新型虚假数据注入攻击在电力系统中的定位问题，通过考虑电力网数据的非欧氏空间属性中的固有拓扑相关性，提出了一种基于时空图波卷积神经网络的方法，以提高攻击定位的准确性。

    

    新型虚假数据注入攻击对电力系统的安全和稳定运行构成了严重威胁。由于注入恶意数据和合法数据之间的欧氏空间距离极小，传统的基于距离的方法很难准确检测这些攻击。此外，现有研究主要关注各种机器学习技术，通常在攻击后分析时间序列数据或仅依赖于欧氏空间特征。不幸的是，这种方法往往忽视了电力网数据的非欧氏空间属性中的固有拓扑相关性，从而导致攻击定位的准确性降低。为解决这个问题，本研究采用全面的方法。首先，研究了这些新型虚假数据注入攻击对电力系统的基本原理。在这里，我们建立了虚假数据注入攻击的复杂数学模型。

    The emergence of novel the dummy data injection attack (DDIA) poses a severe threat to the secure and stable operation of power systems. These attacks are particularly perilous due to the minimal Euclidean spatial separation between the injected malicious data and legitimate data, rendering their precise detection challenging using conventional distance-based methods. Furthermore, existing research predominantly focuses on various machine learning techniques, often analyzing the temporal data sequences post-attack or relying solely on Euclidean spatial characteristics. Unfortunately, this approach tends to overlook the inherent topological correlations within the non-Euclidean spatial attributes of power grid data, consequently leading to diminished accuracy in attack localization. To address this issue, this study takes a comprehensive approach. Initially, it examines the underlying principles of these new DDIAs on power systems. Here, an intricate mathematical model of the DDIA is de
    
[^12]: 高斯喷溅：利用高斯飘落动态合成流体

    Gaussian Splashing: Dynamic Fluid Synthesis with Gaussian Splatting. (arXiv:2401.15318v1 [cs.GR])

    [http://arxiv.org/abs/2401.15318](http://arxiv.org/abs/2401.15318)

    高斯喷溅技术相结合物理基础动画和3D高斯喷溅，可以在虚拟场景中创造出无可比拟的效果，同时实现渲染、视图合成以及固体和流体的动态管理和交互。

    

    我们展示了将物理基础动画与3D高斯喷溅（3DGS）相结合的可行性，以在使用3DGS重建的虚拟场景中创建新效果。利用高斯喷溅和基于位置的动力学（PBD）在底层表示中的一致性，我们以连贯的方式管理渲染、视图合成以及固体和流体的动态。类似于高斯着色器，我们通过添加法线增强每个高斯核，将核的方向与表面法线对齐，以改进PBD模拟。这种方法有效消除了固体旋转变形产生的尖峰噪声。它还使我们能够将基于物理的渲染集成到流体的动态表面反射中。因此，我们的框架能够真实地复现动态流体上的表面亮点，并促进场景对象与流体之间的交互。

    We demonstrate the feasibility of integrating physics-based animations of solids and fluids with 3D Gaussian Splatting (3DGS) to create novel effects in virtual scenes reconstructed using 3DGS. Leveraging the coherence of the Gaussian splatting and position-based dynamics (PBD) in the underlying representation, we manage rendering, view synthesis, and the dynamics of solids and fluids in a cohesive manner. Similar to Gaussian shader, we enhance each Gaussian kernel with an added normal, aligning the kernel's orientation with the surface normal to refine the PBD simulation. This approach effectively eliminates spiky noises that arise from rotational deformation in solids. It also allows us to integrate physically based rendering to augment the dynamic surface reflections on fluids. Consequently, our framework is capable of realistically reproducing surface highlights on dynamic fluids and facilitating interactions between scene objects and fluids from new views. For more information, pl
    
[^13]: AI天气模型的实用概率基准

    A Practical Probabilistic Benchmark for AI Weather Models. (arXiv:2401.15305v1 [physics.ao-ph])

    [http://arxiv.org/abs/2401.15305](http://arxiv.org/abs/2401.15305)

    这篇论文提出了一个实用的概率基准，用于比较AI天气模型的概率技能。通过使用滞后集合，可以实现对多个模型的比较，并与操作基准进行对比。

    

    由于天气的混沌性质，天气预报旨在预测未来状态的分布而不是做出单个预测。最近，出现了多个数据驱动的天气模型，声称在技术上取得了突破。然而，这些模型大多使用确定性技能评分进行基准测试，对它们的概率技能知之甚少。不幸的是，在概率意义上公平比较AI天气模型是困难的，因为集合初始化的选择、状态定义和噪声注入方法的变化会产生混淆。此外，考虑到所涉及的数据量，即使是获得集合预报的基线也是一个巨大的工程挑战。为了解决这两个问题，我们采用了几十年前的概念 - 滞后集合，通过一个适度规模的确定性预测库构建集合。这允许对领先的AI天气模型的概率技能进行第一个无参数比较，并与操作基准进行对比。

    Since the weather is chaotic, forecasts aim to predict the distribution of future states rather than make a single prediction. Recently, multiple data driven weather models have emerged claiming breakthroughs in skill. However, these have mostly been benchmarked using deterministic skill scores, and little is known about their probabilistic skill. Unfortunately, it is hard to fairly compare AI weather models in a probabilistic sense, since variations in choice of ensemble initialization, definition of state, and noise injection methodology become confounding. Moreover, even obtaining ensemble forecast baselines is a substantial engineering challenge given the data volumes involved. We sidestep both problems by applying a decades-old idea -- lagged ensembles -- whereby an ensemble can be constructed from a moderately-sized library of deterministic forecasts. This allows the first parameter-free intercomparison of leading AI weather models' probabilistic skill against an operational base
    
[^14]: 自适应最小均方图神经网络和在线图信号估计

    Adaptive Least Mean Squares Graph Neural Networks and Online Graph Signal Estimation. (arXiv:2401.15304v1 [cs.LG])

    [http://arxiv.org/abs/2401.15304](http://arxiv.org/abs/2401.15304)

    我们提出了一种名为自适应最小均方图神经网络（LMS-GNN）的神经网络架构，用于在线估计时变图信号。实验结果表明，与基于图的方法相比，LMS-GNN实现了更准确的在线预测。

    

    在众多应用中，从有噪声的部分观测中在线预测同时存在于空间和时间中的多变量信号是一项基本任务。我们提出了一种有效的神经网络架构，用于在线估计时变图信号，称为自适应最小均方图神经网络（LMS-GNN）。LMS-GNN旨在捕捉时间变化，并在信号受到噪声和缺失值干扰的条件下建立空间时间交互。LMS-GNN是自适应图滤波器和图神经网络（GNN）的结合。在每个时间步长中，LMS-GNN的前向传播类似于自适应图滤波器，其中输出基于观测和预测之间的误差，类似于GNN。滤波器系数通过反向传播更新，类似于GNN。对真实世界温度数据的实验表明，与基于图的方法相比，我们的LMS-GNN实现了更准确的在线预测。

    The online prediction of multivariate signals, existing simultaneously in space and time, from noisy partial observations is a fundamental task in numerous applications. We propose an efficient Neural Network architecture for the online estimation of time-varying graph signals named the Adaptive Least Mean Squares Graph Neural Networks (LMS-GNN). LMS-GNN aims to capture the time variation and bridge the cross-space-time interactions under the condition that signals are corrupted by noise and missing values. The LMS-GNN is a combination of adaptive graph filters and Graph Neural Networks (GNN). At each time step, the forward propagation of LMS-GNN is similar to adaptive graph filters where the output is based on the error between the observation and the prediction similar to GNN. The filter coefficients are updated via backpropagation as in GNN. Experimenting on real-world temperature data reveals that our LMS-GNN achieves more accurate online predictions compared to graph-based methods
    
[^15]: SupplyGraph: 使用图神经网络进行供应链规划的基准数据集

    SupplyGraph: A Benchmark Dataset for Supply Chain Planning using Graph Neural Networks. (arXiv:2401.15299v1 [cs.LG])

    [http://arxiv.org/abs/2401.15299](http://arxiv.org/abs/2401.15299)

    SupplyGraph是一个基准数据集，用于使用图神经网络进行供应链规划。该数据集包含了来自孟加拉国一家领先快速消费品公司的实际数据，用于优化、预测和解决供应链问题。数据集中的时间数据作为节点特征，可用于销售预测、生产计划和故障识别。

    

    图神经网络（GNNs）在不同领域如运输、生物信息学、语言处理和计算机视觉中取得了重要进展。然而，在将GNNs应用于供应链网络方面，目前尚缺乏研究。供应链网络在结构上类似于图形，使其成为应用GNN方法的理想选择。这为优化、预测和解决供应链问题开辟了无限可能。然而，此方法的一个主要障碍在于缺乏真实世界的基准数据集以促进使用GNN来研究和解决供应链问题。为了解决这个问题，我们提供了一个来自孟加拉国一家领先的快速消费品公司的实际基准数据集，该数据集侧重于用于生产目的的供应链规划的时间任务。该数据集包括时间数据作为节点特征，以实现销售预测、生产计划和故障识别。

    Graph Neural Networks (GNNs) have gained traction across different domains such as transportation, bio-informatics, language processing, and computer vision. However, there is a noticeable absence of research on applying GNNs to supply chain networks. Supply chain networks are inherently graph-like in structure, making them prime candidates for applying GNN methodologies. This opens up a world of possibilities for optimizing, predicting, and solving even the most complex supply chain problems. A major setback in this approach lies in the absence of real-world benchmark datasets to facilitate the research and resolution of supply chain problems using GNNs. To address the issue, we present a real-world benchmark dataset for temporal tasks, obtained from one of the leading FMCG companies in Bangladesh, focusing on supply chain planning for production purposes. The dataset includes temporal data as node features to enable sales predictions, production planning, and the identification of fa
    
[^16]: 多触发后门攻击：更多触发器，更多威胁

    Multi-Trigger Backdoor Attacks: More Triggers, More Threats. (arXiv:2401.15295v1 [cs.LG])

    [http://arxiv.org/abs/2401.15295](http://arxiv.org/abs/2401.15295)

    本文主要研究了多触发后门攻击对深度神经网络的威胁。通过提出并研究了三种类型的多触发攻击，包括并行、顺序和混合攻击，文章揭示了不同触发器对同一数据集的共存、覆写和交叉激活效果。结果表明单触发攻击容易引起覆写问题。

    

    后门攻击已经成为深度神经网络（DNNs）的（预）训练和部署的主要威胁。尽管后门攻击在一些研究中已经得到了广泛的探讨，但其中大部分都集中在使用单个类型的触发器来污染数据集的单触发攻击上。可以说，在现实世界中，后门攻击可能更加复杂，例如，同一数据集可能存在多个对手，如果该数据集具有较高的价值。在这项工作中，我们研究了在多触发攻击设置下后门攻击的实际威胁，多个对手利用不同类型的触发器来污染同一数据集。通过提出和研究并行、顺序和混合攻击这三种类型的多触发攻击，我们提供了关于不同触发器对同一数据集的共存、覆写和交叉激活效果的重要认识。此外，我们还展示了单触发攻击往往容易引起覆写问题。

    Backdoor attacks have emerged as a primary threat to (pre-)training and deployment of deep neural networks (DNNs). While backdoor attacks have been extensively studied in a body of works, most of them were focused on single-trigger attacks that poison a dataset using a single type of trigger. Arguably, real-world backdoor attacks can be much more complex, e.g., the existence of multiple adversaries for the same dataset if it is of high value. In this work, we investigate the practical threat of backdoor attacks under the setting of \textbf{multi-trigger attacks} where multiple adversaries leverage different types of triggers to poison the same dataset. By proposing and investigating three types of multi-trigger attacks, including parallel, sequential, and hybrid attacks, we provide a set of important understandings of the coexisting, overwriting, and cross-activating effects between different triggers on the same dataset. Moreover, we show that single-trigger attacks tend to cause over
    
[^17]: 球面上散点数据拟合的积分算子方法

    Integral Operator Approaches for Scattered Data Fitting on Spheres. (arXiv:2401.15294v1 [math.NA])

    [http://arxiv.org/abs/2401.15294](http://arxiv.org/abs/2401.15294)

    本文提出了一种积分算子方法来解决球面上的散点数据拟合问题，通过研究加权谱滤波算法的逼近性能，成功推导出了带权重谱滤波算法的最优误差估计。这种方法可以避免一些现有方法中存在的问题，同时提供了一种优化算法的解决方案。

    

    本文着重研究了球面上的散点数据拟合问题。我们研究了一类加权谱滤波算法（包括Tikhonov正则化、Landaweber迭代、谱截断和迭代Tikhonov）在拟合可能存在的无界随机噪声的嘈杂数据时的逼近性能。为了分析这个问题，我们提出了一种积分算子方法，可以被看作是散点数据拟合领域中广泛使用的采样不等式方法和规范集方法的延伸。通过提供算子差异和数值积分规则之间的等价性，我们成功地推导出带权重谱滤波算法的Sobolev类型误差估计的最优结果。我们的误差估计不受文献中Tikhonov正则化的饱和现象、现有误差分析中的本地空间屏障和不同嵌入空间的影响。我们还提出了一种分而治之的方案，以提升加权谱滤波算法的效能。

    This paper focuses on scattered data fitting problems on spheres. We study the approximation performance of a class of weighted spectral filter algorithms, including Tikhonov regularization, Landaweber iteration, spectral cut-off, and iterated Tikhonov, in fitting noisy data with possibly unbounded random noise. For the analysis, we develop an integral operator approach that can be regarded as an extension of the widely used sampling inequality approach and norming set method in the community of scattered data fitting. After providing an equivalence between the operator differences and quadrature rules, we succeed in deriving optimal Sobolev-type error estimates of weighted spectral filter algorithms. Our derived error estimates do not suffer from the saturation phenomenon for Tikhonov regularization in the literature, native-space-barrier for existing error analysis and adapts to different embedding spaces. We also propose a divide-and-conquer scheme to equip weighted spectral filter 
    
[^18]: SkipViT: 使用令牌级跳跃连接加速Vision Transformers

    SkipViT: Speeding Up Vision Transformers with a Token-Level Skip Connection. (arXiv:2401.15293v1 [cs.CV])

    [http://arxiv.org/abs/2401.15293](http://arxiv.org/abs/2401.15293)

    SkipViT通过令牌级跳跃连接将不重要的图像令牌分离，以提高Vision Transformers的训练速度，而不影响最终模型的准确率。

    

    Vision transformers被认为比CNN模型更具计算和数据密集性。这些Transformer模型，如ViT，需要所有输入图像令牌来学习它们之间的关系。然而，许多这些令牌并不信息丰富，可能包含无关的背景或不重要的场景等无关信息。这些令牌被多头自注意力（MHSA）忽略，导致MHSA和前馈网络（FFN）中存在许多冗余和不必要的计算。在这项工作中，我们提出了一种方法，通过将这些不重要的令牌分离并通过不同的低成本计算路径发送，来优化不必要的交互量。我们的方法不会给ViT模型添加任何参数，并旨在在训练吞吐量和最终模型的Top-1准确率损失为0%之间找到最佳平衡。我们对从头开始训练ViT-small的实验结果表明，SkipViT能够有效地提高训练速度。

    Vision transformers are known to be more computationally and data-intensive than CNN models. These transformer models such as ViT, require all the input image tokens to learn the relationship among them. However, many of these tokens are not informative and may contain irrelevant information such as unrelated background or unimportant scenery. These tokens are overlooked by the multi-head self-attention (MHSA), resulting in many redundant and unnecessary computations in MHSA and the feed-forward network (FFN). In this work, we propose a method to optimize the amount of unnecessary interactions between unimportant tokens by separating and sending them through a different low-cost computational path. Our method does not add any parameters to the ViT model and aims to find the best trade-off between training throughput and achieving a 0% loss in the Top-1 accuracy of the final model. Our experimental results on training ViT-small from scratch show that SkipViT is capable of effectively dr
    
[^19]: 在任意线性变换下的自适应块稀疏正则化

    Adaptive Block sparse regularization under arbitrary linear transform. (arXiv:2401.15292v1 [cs.LG])

    [http://arxiv.org/abs/2401.15292](http://arxiv.org/abs/2401.15292)

    我们提出了一种在任意线性变换下重构具有块稀疏性的信号的方法，相比现有方法扩大了应用范围，并通过数值实验证明了其有效性。

    

    我们提出了一种用于在未知块结构下的任意线性变换下的块稀疏信号重构方法。该方法是现有方法LOP-$\ell_2$/$\ell_1$的推广，可以在非可逆变换下重构具有块稀疏性的信号，而LOP-$\ell_2$/$\ell_1$不能。我们的工作扩大了块稀疏正则化的范围，使其能够在各种信号处理领域中应用更加灵活和强大。我们推导了一个迭代算法来求解该方法，并给出了其收敛到最优解的条件。数值实验验证了该方法的有效性。

    We propose a convex signal reconstruction method for block sparsity under arbitrary linear transform with unknown block structure. The proposed method is a generalization of the existing method LOP-$\ell_2$/$\ell_1$ and can reconstruct signals with block sparsity under non-invertible transforms, unlike LOP-$\ell_2$/$\ell_1$. Our work broadens the scope of block sparse regularization, enabling more versatile and powerful applications across various signal processing domains. We derive an iterative algorithm for solving proposed method and provide conditions for its convergence to the optimal solution. Numerical experiments demonstrate the effectiveness of the proposed method.
    
[^20]: 使用非规律且稀疏的临床时间序列数据集MIMIC-IV进行基准测试

    Benchmarking with MIMIC-IV, an irregular, spare clinical time series dataset. (arXiv:2401.15290v1 [cs.LG])

    [http://arxiv.org/abs/2401.15290](http://arxiv.org/abs/2401.15290)

    这项工作旨在为最新版本的MIMIC数据集（MIMIC-IV）提供基准测试，填补该领域在深度学习和时间序列标签数据方面的研究尚未解决的问题。

    

    电子健康记录（EHR）越来越受欢迎，并且随之而来的是将机器学习解决方案应用于该领域的各种问题。这个不断增长的研究领域也引发了对EHR的可访问性的需求。MIMIC（医学信息市场）数据集是一个受欢迎的、公开的、免费的EHR数据集，以原始格式提供，并已在无数研究中使用。然而，尽管其受欢迎程度，却缺乏基准测试工作，特别是与最近在深度学习和时间序列标签数据领域的最新研究相结合。本研究的目的是通过提供最新版本的MIMIC数据集MIMIC-IV来填补这一空缺。我们还对已经完成的针对MIMIC-III的研究进行了详细的文献调查。

    Electronic health record (EHR) is more and more popular, and it comes with applying machine learning solutions to resolve various problems in the domain. This growing research area also raises the need for EHRs accessibility. Medical Information Mart for Intensive Care (MIMIC) dataset is a popular, public, and free EHR dataset in a raw format that has been used in numerous studies. However, despite of its popularity, it is lacking benchmarking work, especially with recent state of the art works in the field of deep learning with time-series tabular data. The aim of this work is to fill this lack by providing a benchmark for latest version of MIMIC dataset, MIMIC-IV. We also give a detailed literature survey about studies that has been already done for MIIMIC-III.
    
[^21]: 通过网络流量分析和机器学习技术来缓解勒索软件威胁

    Ransomware threat mitigation through network traffic analysis and machine learning techniques. (arXiv:2401.15285v1 [cs.CR])

    [http://arxiv.org/abs/2401.15285](http://arxiv.org/abs/2401.15285)

    本文介绍了一种通过分析网络流量和使用机器学习算法来识别和检测勒索软件的方法，该方法在实践中表现出高水平的精度和准确性。

    

    近年来，使用勒索软件进行网络攻击的情况明显增加。攻击者利用这种恶意软件侵入网络并损害计算机系统。这给各种组织带来了重大和长期的损害，包括政府、私营公司和普通用户。这些攻击通常导致敏感信息的丢失或泄露，正常运营的中断以及持久的漏洞。本文侧重于一种在计算机网络中识别和辨别勒索软件的方法。该方法依赖于使用机器学习算法和分析网络流量的模式。通过收集和研究这些流量，然后应用机器学习模型，我们可以准确地识别和检测勒索软件。实施这种方法的结果表明，机器学习算法可以基于网络流量准确地定位勒索软件，实现高水平的精度和准确性。

    In recent years, there has been a noticeable increase in cyberattacks using ransomware. Attackers use this malicious software to break into networks and harm computer systems. This has caused significant and lasting damage to various organizations, including government, private companies, and regular users. These attacks often lead to the loss or exposure of sensitive information, disruptions in normal operations, and persistent vulnerabilities. This paper focuses on a method for recognizing and identifying ransomware in computer networks. The approach relies on using machine learning algorithms and analyzing the patterns of network traffic. By collecting and studying this traffic, and then applying machine learning models, we can accurately identify and detect ransomware. The results of implementing this method show that machine learning algorithms can effectively pinpoint ransomware based on network traffic, achieving high levels of precision and accuracy.
    
[^22]: 有限时间分析的政策异构联邦强化学习

    Finite-Time Analysis of On-Policy Heterogeneous Federated Reinforcement Learning. (arXiv:2401.15273v1 [cs.LG])

    [http://arxiv.org/abs/2401.15273](http://arxiv.org/abs/2401.15273)

    本论文介绍了一种新颖的联邦政策强化学习方案（FedSARSA），利用线性函数逼近来解决马尔可夫取样、多个本地更新等技术挑战，从而提供了关于有限时间性能的全面分析。

    

    联邦强化学习（FRL）是一种利用不同代理的信息来降低强化学习任务样本复杂性的前景光明的范式。然而，当每个代理与一个可能不同的环境进行交互时，关于FRL算法的非渐进性能几乎没有理论上的了解。这种结果的缺乏可以归因于各种技术挑战及其复杂的相互作用：马尔可夫取样、线性函数逼近、多个本地更新以节省通信、代理的MDP的奖励函数和转移核的异质性以及连续的状态-动作空间。此外，在政策上的设置中，行为政策随时间变化，进一步使分析复杂化。针对这些挑战，我们引入了FedSARSA，一种新颖的带有线性函数逼近的联邦政策强化学习方案，以应对这些挑战并提供全面的有限时间分析。

    Federated reinforcement learning (FRL) has emerged as a promising paradigm for reducing the sample complexity of reinforcement learning tasks by exploiting information from different agents. However, when each agent interacts with a potentially different environment, little to nothing is known theoretically about the non-asymptotic performance of FRL algorithms. The lack of such results can be attributed to various technical challenges and their intricate interplay: Markovian sampling, linear function approximation, multiple local updates to save communication, heterogeneity in the reward functions and transition kernels of the agents' MDPs, and continuous state-action spaces. Moreover, in the on-policy setting, the behavior policies vary with time, further complicating the analysis. In response, we introduce FedSARSA, a novel federated on-policy reinforcement learning scheme, equipped with linear function approximation, to address these challenges and provide a comprehensive finite-ti
    
[^23]: SimFair：物理引导的公平感知学习与模拟模型

    SimFair: Physics-Guided Fairness-Aware Learning with Simulation Models. (arXiv:2401.15270v1 [cs.LG])

    [http://arxiv.org/abs/2401.15270](http://arxiv.org/abs/2401.15270)

    SimFair是一种物理引导的公平感知学习框架，通过集成模拟和逆向建模来解决纯数据驱动的公平性问题，有效保持公平性。

    

    公平感知已经成为在真实应用中负责任使用人工智能的关键基础。在许多情况下，性能不平等是由于不同区域分布的变化引起的。虽然已经开发出提高公平可迁移性的技术，但是在没有来自新区域的样本的情况下解决这个问题并不总是可行的，这对于纯数据驱动的尝试是一个瓶颈。幸运的是，基于物理机制模型已经在许多具有重大社会影响的问题上进行了研究。我们提出了SimFair，一种物理引导的公平感知学习框架，通过集成基于物理规则的模拟和逆向建模到训练设计中来弥补数据限制。以温度预测为例，我们演示了所提出的SimFair在保持公平性方面的有效性。

    Fairness-awareness has emerged as an essential building block for the responsible use of artificial intelligence in real applications. In many cases, inequity in performance is due to the change in distribution over different regions. While techniques have been developed to improve the transferability of fairness, a solution to the problem is not always feasible with no samples from the new regions, which is a bottleneck for pure data-driven attempts. Fortunately, physics-based mechanistic models have been studied for many problems with major social impacts. We propose SimFair, a physics-guided fairness-aware learning framework, which bridges the data limitation by integrating physical-rule-based simulation and inverse modeling into the training design. Using temperature prediction as an example, we demonstrate the effectiveness of the proposed SimFair in fairness preservation.
    
[^24]: 朝着稳定的利益相关方一致化机器学习偏好迈进

    Towards Stable Preferences for Stakeholder-aligned Machine Learning. (arXiv:2401.15268v1 [cs.LG])

    [http://arxiv.org/abs/2401.15268](http://arxiv.org/abs/2401.15268)

    本研究旨在通过数据驱动的方法，整合个人和团体的偏好，解决肾脏分配中的紧迫挑战，并评估偏好的稳定性。

    

    针对肾脏分配的紧迫挑战，即需求增长与利益相关方价值的结合，本研究旨在开发一个以数据驱动的解决方案，以解决这个问题。该研究的主要目标是创建一种学习个人和团体关于肾脏分配的偏好的方法。通过利用“成对肾脏患者在线调查”的数据，结合两个不同的数据集，并在个人、团体和稳定性三个层面上进行评估，我们使用机器学习分类器并通过几种度量方法进行评估。个人层面模型预测个体参与者的偏好，团体层面模型汇总参与者偏好，稳定性层面模型是团体升级的扩展，评估这些偏好随时间的稳定性。通过将利益相关方的偏好纳入肾脏分配过程，我们希望推动伦理维度的发展。

    In response to the pressing challenge of kidney allocation, characterized by growing demands for organs, this research sets out to develop a data-driven solution to this problem, which also incorporates stakeholder values. The primary objective of this study is to create a method for learning both individual and group-level preferences pertaining to kidney allocations. Drawing upon data from the 'Pairwise Kidney Patient Online Survey.' Leveraging two distinct datasets and evaluating across three levels - Individual, Group and Stability - we employ machine learning classifiers assessed through several metrics. The Individual level model predicts individual participant preferences, the Group level model aggregates preferences across participants, and the Stability level model, an extension of the Group level, evaluates the stability of these preferences over time. By incorporating stakeholder preferences into the kidney allocation process, we aspire to advance the ethical dimensions of o
    
[^25]: 在$\ell_\infty$-扰动下对抗性训练估计器的渐近行为

    Asymptotic Behavior of Adversarial Training Estimator under $\ell_\infty$-Perturbation. (arXiv:2401.15262v1 [math.ST])

    [http://arxiv.org/abs/2401.15262](http://arxiv.org/abs/2401.15262)

    本文研究了在$\ell_\infty$-扰动下的对抗性训练，证明当真实参数为0时，对抗性训练估计器在该扰动下的极限分布可能在0处有一个正概率质量，提供了稀疏性恢复能力的理论保证，并提出了一种两步过程——自适应对抗性训练，可以进一步提高性能。

    

    对抗性训练被提出来抵御机器学习和统计模型中的对抗性攻击。本文重点研究了在$\ell_\infty$-扰动下的对抗性训练，这个问题最近引起了很多研究的关注。在广义线性模型中研究了对抗性训练估计器的渐近行为。结果表明，当真实参数为0时，对抗性训练估计器在$\ell_\infty$-扰动下的极限分布可能在0处有一个正概率质量，为相关的稀疏性恢复能力提供了理论保证。此外，提出了一种两步过程——自适应对抗性训练，可以进一步提高在$\ell_\infty$-扰动下的对抗性训练的性能。具体而言，所提出的过程可以实现渐近无偏性和变量选择一致性。通过数值实验展示了稀疏性恢复的能力。

    Adversarial training has been proposed to hedge against adversarial attacks in machine learning and statistical models. This paper focuses on adversarial training under $\ell_\infty$-perturbation, which has recently attracted much research attention. The asymptotic behavior of the adversarial training estimator is investigated in the generalized linear model. The results imply that the limiting distribution of the adversarial training estimator under $\ell_\infty$-perturbation could put a positive probability mass at $0$ when the true parameter is $0$, providing a theoretical guarantee of the associated sparsity-recovery ability. Alternatively, a two-step procedure is proposed -adaptive adversarial training, which could further improve the performance of adversarial training under $\ell_\infty$-perturbation. Specifically, the proposed procedure could achieve asymptotic unbiasedness and variable-selection consistency. Numerical experiments are conducted to show the sparsity-recovery a
    
[^26]: 用任意预测器构建线性回归参数的有限样本置信区间

    Finite Sample Confidence Regions for Linear Regression Parameters Using Arbitrary Predictors. (arXiv:2401.15254v1 [stat.ML])

    [http://arxiv.org/abs/2401.15254](http://arxiv.org/abs/2401.15254)

    本文提出了一种新的方法，利用任意预测器构建线性模型参数的有限样本置信区间。该方法对噪声的要求很少，并且适用于严格线性函数偏差一定阈值的函数。这种方法能够进行鲁棒优化，并提取特定参数坐标的置信区间，也能用于假设检验。

    

    我们探索了一种新的方法，利用任意预测器构建线性模型参数的置信区间。我们的框架对噪声的要求很少，并且可以扩展到严格线性函数偏差一定阈值的函数，从而适应了广泛而实用的函数集合。得出的置信区间可以作为混合整数线性规划框架中的约束条件，从而实现线性目标的优化。这种表示方式能够进行鲁棒优化，并提取特定参数坐标的置信区间。与以前的方法不同的是，置信区间可能为空，这可以用于假设检验。最后，我们在合成数据上验证了我们方法的实证适用性。

    We explore a novel methodology for constructing confidence regions for parameters of linear models, using predictions from any arbitrary predictor. Our framework requires minimal assumptions on the noise and can be extended to functions deviating from strict linearity up to some adjustable threshold, thereby accommodating a comprehensive and pragmatically relevant set of functions. The derived confidence regions can be cast as constraints within a Mixed Integer Linear Programming framework, enabling optimisation of linear objectives. This representation enables robust optimization and the extraction of confidence intervals for specific parameter coordinates. Unlike previous methods, the confidence region can be empty, which can be used for hypothesis testing. Finally, we validate the empirical applicability of our method on synthetic data.
    
[^27]: 在预训练中通过对抗训练改进表示：从理论角度出发

    Better Representations via Adversarial Training in Pre-Training: A Theoretical Perspective. (arXiv:2401.15248v1 [cs.LG])

    [http://arxiv.org/abs/2401.15248](http://arxiv.org/abs/2401.15248)

    该论文从理论角度探讨了在预训练中通过对抗训练改进表示的思路，并证明了特征净化在预训练模型的对抗鲁棒性和下游任务之间起到重要作用。

    

    预训练被认为可以为大规模深度学习中的下游任务生成通用表示，例如大型语言模型。现有文献例如\cite{kim2020adversarial}经验性地观察到下游任务可以继承预训练模型的对抗鲁棒性。我们提供了这种鲁棒性继承现象的理论证明。我们的理论结果揭示了在两层神经网络中，特征净化在连接预训练模型的对抗鲁棒性和下游任务中起重要作用。具体来说，我们展示了(i)通过对抗训练，每个隐藏节点倾向于选择只有一个（或几个）特征；(ii)在没有对抗训练的情况下，隐藏节点可能容易受到攻击。这个观察对于监督预训练和对比学习都是有效的。通过净化节点，事实证明在下游任务中，仅仅进行干净训练就足以实现对抗鲁棒性。

    Pre-training is known to generate universal representations for downstream tasks in large-scale deep learning such as large language models. Existing literature, e.g., \cite{kim2020adversarial}, empirically observe that the downstream tasks can inherit the adversarial robustness of the pre-trained model. We provide theoretical justifications for this robustness inheritance phenomenon. Our theoretical results reveal that feature purification plays an important role in connecting the adversarial robustness of the pre-trained model and the downstream tasks in two-layer neural networks. Specifically, we show that (i) with adversarial training, each hidden node tends to pick only one (or a few) feature; (ii) without adversarial training, the hidden nodes can be vulnerable to attacks. This observation is valid for both supervised pre-training and contrastive learning. With purified nodes, it turns out that clean training is enough to achieve adversarial robustness in downstream tasks.
    
[^28]: 使用半敏感特征训练差分隐私广告预测模型

    Training Differentially Private Ad Prediction Models with Semi-Sensitive Features. (arXiv:2401.15246v1 [cs.LG])

    [http://arxiv.org/abs/2401.15246](http://arxiv.org/abs/2401.15246)

    我们介绍了一种新的算法，用于训练具有半敏感特征的差分隐私广告预测模型，并在真实广告数据集上证明了其优于传统方法的效果。

    

    我们以数字广告中出现的问题为出发点，介绍了使用半敏感特征训练差分隐私机器学习模型的任务。在这种情况下，攻击者已知一部分特征（因此无需保护），而剩余的特征以及标签对于攻击者来说是未知的，需要通过差分隐私保护。这个任务插值了使用全差分隐私（需要保护标签和所有特征）或标签差分隐私（所有特征被认为是已知的，只需保护标签）来训练模型。我们提出了一种新的算法来训练具有半敏感特征的差分隐私模型。通过对真实广告数据集的实证评估，我们证明了我们的算法在效用方面超过了（i）在所有特征上运行的差分隐私随机梯度下降（DP-SGD）基线和（ii）仅在已知特征上运行的标签差分隐私算法（而丢弃了未知的特征）。

    Motivated by problems arising in digital advertising, we introduce the task of training differentially private (DP) machine learning models with semi-sensitive features. In this setting, a subset of the features is known to the attacker (and thus need not be protected) while the remaining features as well as the label are unknown to the attacker and should be protected by the DP guarantee. This task interpolates between training the model with full DP (where the label and all features should be protected) or with label DP (where all the features are considered known, and only the label should be protected). We present a new algorithm for training DP models with semi-sensitive features. Through an empirical evaluation on real ads datasets, we demonstrate that our algorithm surpasses in utility the baselines of (i) DP stochastic gradient descent (DP-SGD) run on all features (known and unknown), and (ii) a label DP algorithm run only on the known features (while discarding the unknown one
    
[^29]: GenPluSSS：一种基于遗传算法的测量次表面散射表示的插件

    GenPluSSS: A Genetic Algorithm Based Plugin for Measured Subsurface Scattering Representation. (arXiv:2401.15245v1 [cs.GR])

    [http://arxiv.org/abs/2401.15245](http://arxiv.org/abs/2401.15245)

    本文介绍了一种基于遗传算法的插件，可以在Blender 3D建模工具上添加次表面散射的表示方法，并使用Mitsuba渲染器进行验证。实验证明该插件能够准确、紧密和高效地可视化均匀和异质次表面散射效果。

    

    本文介绍了一种在Blender 3D建模工具上添加均匀和异质、光学厚度的半透明材料表示的插件。该插件的工作原理基于遗传算法（GA）和基于奇异值分解（SVD）的次表面散射方法（GenSSS）的组合。所提出的插件使用开源渲染软件Mitsuba渲染器进行实现。该插件在测得的次表面散射数据上进行了验证。结果表明，所提出的插件能够准确、紧密和高效地可视化均匀和异质次表面散射效果。

    This paper presents a plugin that adds a representation of homogeneous and heterogeneous, optically thick, translucent materials on the Blender 3D modeling tool. The working principle of this plugin is based on a combination of Genetic Algorithm (GA) and Singular Value Decomposition (SVD)-based subsurface scattering method (GenSSS). The proposed plugin has been implemented using Mitsuba renderer, which is an open source rendering software. The proposed plugin has been validated on measured subsurface scattering data. It's shown that the proposed plugin visualizes homogeneous and heterogeneous subsurface scattering effects, accurately, compactly and computationally efficiently.
    
[^30]: 协方差均衡的近最优策略优化在一般性和的马尔可夫博弈中的应用

    Near-Optimal Policy Optimization for Correlated Equilibrium in General-Sum Markov Games. (arXiv:2401.15240v1 [cs.LG])

    [http://arxiv.org/abs/2401.15240](http://arxiv.org/abs/2401.15240)

    本文提出了一个协方差均衡的近最优策略优化算法，通过结合平滑价值更新和乐观实行者算法，以及对数障碍正则化器，实现了在一般性和的马尔可夫博弈中计算协方差均衡的近最优收敛速度$\tilde{O}(T^{-1})$。

    

    我们研究了用于计算多人一般性和的马尔可夫博弈中的协方差均衡的策略优化算法。之前的结果实现了$O(T^{-1/2})$收敛速度到协方差均衡和$O(T^{-3/4})$加速收敛速度到较弱的疏松协方差均衡。在本文中，我们通过提供一个非耦合策略优化算法，显著改进了这两个结果，使其达到计算协方差均衡的近最优$\tilde{O}(T^{-1})$收敛速度。我们的算法通过结合两个主要因素（i）平滑的价值更新和（ii）具有对数障碍正则化器的乐观实行者算法构建而成。

    We study policy optimization algorithms for computing correlated equilibria in multi-player general-sum Markov Games. Previous results achieve $O(T^{-1/2})$ convergence rate to a correlated equilibrium and an accelerated $O(T^{-3/4})$ convergence rate to the weaker notion of coarse correlated equilibrium. In this paper, we improve both results significantly by providing an uncoupled policy optimization algorithm that attains a near-optimal $\tilde{O}(T^{-1})$ convergence rate for computing a correlated equilibrium. Our algorithm is constructed by combining two main elements (i) smooth value updates and (ii) the optimistic-follow-the-regularized-leader algorithm with the log barrier regularizer.
    
[^31]: MEA-Defender:一种抵御模型提取攻击的强大水印

    MEA-Defender: A Robust Watermark against Model Extraction Attack. (arXiv:2401.15239v1 [cs.CR])

    [http://arxiv.org/abs/2401.15239](http://arxiv.org/abs/2401.15239)

    MEA-Defender是一种抵御模型提取攻击的强大水印，通过将两个源类的样本组合在一起产生水印，并确保水印的输出域与主任务样本的输出域相同，实现了对DNN模型知识产权的保护。

    

    近年来，使用深度学习算法训练了许多具有高价值的深度神经网络(DNN)。为了保护原始所有者对这些DNN模型的知识产权(IP)，基于后门的水印已经得到了广泛研究。然而，大多数这种水印在应对模型提取攻击时失败，这种攻击利用输入样本查询目标模型并获得相应的输出，从而使用这些输入-输出对来训练替代模型。在本文中，我们提出了一种新颖的水印方法，名为MEA-Defender，用于保护DNN模型的知识产权免受模型提取攻击。具体而言，我们通过将输入域中来自两个源类的样本组合在一起获得水印，并设计了一个水印损失函数，使得水印的输出域处于主任务样本的输出域之内。由于我们水印的输入域和输出域都是主任务样本不可或缺的部分，因此水印将被提取出来并应用到目标模型中。

    Recently, numerous highly-valuable Deep Neural Networks (DNNs) have been trained using deep learning algorithms. To protect the Intellectual Property (IP) of the original owners over such DNN models, backdoor-based watermarks have been extensively studied. However, most of such watermarks fail upon model extraction attack, which utilizes input samples to query the target model and obtains the corresponding outputs, thus training a substitute model using such input-output pairs. In this paper, we propose a novel watermark to protect IP of DNN models against model extraction, named MEA-Defender. In particular, we obtain the watermark by combining two samples from two source classes in the input domain and design a watermark loss function that makes the output domain of the watermark within that of the main task samples. Since both the input domain and the output domain of our watermark are indispensable parts of those of the main task samples, the watermark will be extracted into the sto
    
[^32]: 基于自监督学习的表格数据深度学习：一种自监督学习的方法

    Deep Learning with Tabular Data: A Self-supervised Approach. (arXiv:2401.15238v1 [cs.LG])

    [http://arxiv.org/abs/2401.15238](http://arxiv.org/abs/2401.15238)

    我们提出了一种使用自监督学习和TabTransformer模型进行表格数据训练的新方法，该方法能够捕捉表格数据中的复杂关系和依赖关系。相比传统的机器学习模型，我们的方法能够消除对标记数据的需求。

    

    我们描述了一种利用TabTransformer模型和自监督学习来训练表格数据的新方法。传统的表格数据机器学习模型，如GBDT，虽然被广泛使用，但我们的论文研究了专为表格数据优化的TabTransformer的有效性。TabTransformer通过利用Transformer的自注意机制捕捉表格数据中特征之间的复杂关系和依赖关系。我们在这项研究中使用了自监督学习方法，TabTransformer通过创建代理监督任务从无标签数据中学习，消除了对标记数据的需求。目标是找到最有效的TabTransformer模型来表示分类和数值特征。为了解决在Transformer中构建不同输入设置时所面临的挑战，我们还进行了比较分析。

    We have described a novel approach for training tabular data using the TabTransformer model with self-supervised learning. Traditional machine learning models for tabular data, such as GBDT are being widely used though our paper examines the effectiveness of the TabTransformer which is a Transformer based model optimised specifically for tabular data. The TabTransformer captures intricate relationships and dependencies among features in tabular data by leveraging the self-attention mechanism of Transformers. We have used a self-supervised learning approach in this study, where the TabTransformer learns from unlabelled data by creating surrogate supervised tasks, eliminating the need for the labelled data. The aim is to find the most effective TabTransformer model representation of categorical and numerical features. To address the challenges faced during the construction of various input settings into the Transformers. Furthermore, a comparative analysis is also been conducted to exami
    
[^33]: 自适应深度学习用于超低功耗纳米无人机上的高效视觉姿态估计

    Adaptive Deep Learning for Efficient Visual Pose Estimation aboard Ultra-low-power Nano-drones. (arXiv:2401.15236v1 [cs.CV])

    [http://arxiv.org/abs/2401.15236](http://arxiv.org/abs/2401.15236)

    本研究提出了一种自适应深度学习机制，用于在超低功耗纳米无人机上进行高效的视觉姿态估计。通过将两种具有不同性能和成本权衡的卷积神经网络与自适应分类模块结合使用，我们能够根据计算资源的可用性选择合适的网络以实现高效的姿态估计。

    

    小于10厘米直径的纳米无人机由于其适用于较大的飞行无人机无法到达的狭窄环境和人类附近的特点，正变得越来越受关注。然而，其微小的外形也带来了一个主要的缺点：超限的内存和处理器用于其感知流程的机载执行。因此，基于轻量级深度学习的方法越来越受欢迎，强调计算效率和节能的重要性，因为这可以决定一个完全工作的闭环系统和一个失败的闭环系统之间的区别。在本研究中，为了最大限度地利用纳米无人机上极其有限的资源，我们提出了一种新的自适应深度学习机制，用于高效执行基于视觉的人体姿态估计任务。我们结合了两种具有不同回归性能与计算成本折衷的最新卷积神经网络（CNN）。通过将这些CNN与一个自适应分类模块组合起来，我们能够根据计算资源的可用性自适应地选择合适的网络以实现高效的姿态估计。

    Sub-10cm diameter nano-drones are gaining momentum thanks to their applicability in scenarios prevented to bigger flying drones, such as in narrow environments and close to humans. However, their tiny form factor also brings their major drawback: ultra-constrained memory and processors for the onboard execution of their perception pipelines. Therefore, lightweight deep learning-based approaches are becoming increasingly popular, stressing how computational efficiency and energy-saving are paramount as they can make the difference between a fully working closed-loop system and a failing one. In this work, to maximize the exploitation of the ultra-limited resources aboard nano-drones, we present a novel adaptive deep learning-based mechanism for the efficient execution of a vision-based human pose estimation task. We leverage two State-of-the-Art (SoA) convolutional neural networks (CNNs) with different regression performance vs. computational costs trade-offs. By combining these CNNs wi
    
[^34]: CascadedGaze: 图像恢复中的全局上下文提取的高效方法

    CascadedGaze: Efficiency in Global Context Extraction for Image Restoration. (arXiv:2401.15235v1 [eess.IV])

    [http://arxiv.org/abs/2401.15235](http://arxiv.org/abs/2401.15235)

    本文提出了一种名为CascadedGaze的网络架构，使用了一种新颖而高效的全局上下文提取方法，以解决图像恢复中全局信息的问题。通过在卷积层之间引入小的卷积核，该方法可以学习到全局的依赖关系，而无需使用自注意力机制。实验结果表明，该方法在多种图像去噪任务上优于其他先进方法。

    

    传统的图像恢复任务依赖于卷积神经网络。然而，由于卷积运算符的局部性质，它们很难捕捉到全局信息。Transformer中的注意力机制的优势在于解决了这个问题，但却需要大量的计算资源。最近的一些图像恢复研究集中在通过变种Transformer解决性能和计算成本之间的平衡挑战。在本文中，我们提出了CascadedGaze网络（CGNet），它是一种编码器-解码器架构，采用了全局上下文提取器（GCE），一种新颖且高效的图像恢复全局信息的方法。GCE模块通过在卷积层之间使用小的卷积核来学习全局依赖关系，而无需自注意力机制。广泛的实验结果表明，我们的方法在去噪基准数据集上（包括真实图像去噪和）的性能优于许多最先进的方法。

    Image restoration tasks traditionally rely on convolutional neural networks. However, given the local nature of the convolutional operator, they struggle to capture global information. The promise of attention mechanisms in Transformers is to circumvent this problem, but it comes at the cost of intensive computational overhead. Many recent studies in image restoration have focused on solving the challenge of balancing performance and computational cost via Transformer variants. In this paper, we present CascadedGaze Network (CGNet), an encoder-decoder architecture that employs Global Context Extractor (GCE), a novel and efficient way to capture global information for image restoration. The GCE module leverages small kernels across convolutional layers to learn global dependencies, without requiring self-attention. Extensive experimental results show that our approach outperforms a range of state-of-the-art methods on denoising benchmark datasets including both real image denoising and 
    
[^35]: 佛兰德地区的生物评估地图：一项Sentinel-2遥感图像分析

    Biological Valuation Map of Flanders: A Sentinel-2 Imagery Analysis. (arXiv:2401.15223v1 [cs.CV])

    [http://arxiv.org/abs/2401.15223](http://arxiv.org/abs/2401.15223)

    本文介绍了一个综合的方法来填补佛兰德地区生物评估地图领域的研究差距，并提供了一张密集标记的真实地面地图。

    

    近年来，机器学习在遥感分析中变得至关重要，特别是在土地利用/土地覆盖（LULC）领域。机器学习和卫星图像分析的协同作用在这一领域展现出显著的生产力，有多个研究证明了这一点。该领域的一个显著挑战是对广阔领土上的土地使用进行语义分割映射，其中准确的土地利用数据的获取和地面真实土地利用标签的可靠性带来了重大困难。例如，在比利时的佛兰德地区，提供详细准确的像素级标记数据集尤其有价值。然而，在全球许多地区，针对此类研究缺乏规范化和形式化的数据集和工作流程。本文介绍了一个全面的方法来解决这些差距。我们提供了一个佛兰德地区配对的Sentinel-2卫星图的密集标记地面真相地图。

    In recent years, machine learning has become crucial in remote sensing analysis, particularly in the domain of Land-use/Land-cover (LULC). The synergy of machine learning and satellite imagery analysis has demonstrated significant productivity in this field, as evidenced by several studies. A notable challenge within this area is the semantic segmentation mapping of land usage over extensive territories, where the accessibility of accurate land-use data and the reliability of ground truth land-use labels pose significant difficulties. For example, providing a detailed and accurate pixel-wise labeled dataset of the Flanders region, a first-level administrative division of Belgium, can be particularly insightful. Yet there is a notable lack of regulated, formalized datasets and workflows for such studies in many regions globally. This paper introduces a comprehensive approach to addressing these gaps. We present a densely labeled ground truth map of Flanders paired with Sentinel-2 satell
    
[^36]: 在临床文本中预测实体修饰语的迁移学习：以阿片类物质使用障碍病例检测为应用

    Transfer Learning for the Prediction of Entity Modifiers in Clinical Text: Application to Opioid Use Disorder Case Detection. (arXiv:2401.15222v1 [cs.CL])

    [http://arxiv.org/abs/2401.15222](http://arxiv.org/abs/2401.15222)

    本研究提出了一种使用多任务学习和迁移学习的方法，在临床文本中预测实体修饰语。实验结果表明，该方法在公开数据集和新数据集上均取得了最新技术的结果。

    

    背景：从临床文本中提取的实体的语义可能会受到修饰语的显著改变，包括实体的否定、不确定性、条件性、严重性和主观性。现有的确定临床实体修饰语的模型涉及使用正则表达式或特征权重，这些权重是独立训练每个修饰语的。方法：我们开发并评估了一个多任务变换器架构设计，在公开可用的SemEval 2015任务14语料库和一个新的阿片类物质使用障碍（OUD）数据集上共同学习和预测修饰语，该数据集包含与SemEval共享的修饰语以及OUD特定的新修饰语。我们评估了我们的多任务学习方法与以前发表的系统的效果，并评估了仅共享部分临床修饰语时的临床实体修饰语的迁移学习的可行性。结果：我们的方法在来自SemEval 2015的ShARe语料库上取得了最新技术的结果。

    Background: The semantics of entities extracted from a clinical text can be dramatically altered by modifiers, including entity negation, uncertainty, conditionality, severity, and subject. Existing models for determining modifiers of clinical entities involve regular expression or features weights that are trained independently for each modifier.  Methods: We develop and evaluate a multi-task transformer architecture design where modifiers are learned and predicted jointly using the publicly available SemEval 2015 Task 14 corpus and a new Opioid Use Disorder (OUD) data set that contains modifiers shared with SemEval as well as novel modifiers specific for OUD. We evaluate the effectiveness of our multi-task learning approach versus previously published systems and assess the feasibility of transfer learning for clinical entity modifiers when only a portion of clinical modifiers are shared.  Results: Our approach achieved state-of-the-art results on the ShARe corpus from SemEval 2015 T
    
[^37]: HiFT:一种分层全参数微调策略

    HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy. (arXiv:2401.15207v1 [cs.LG])

    [http://arxiv.org/abs/2401.15207](http://arxiv.org/abs/2401.15207)

    HiFT是一种分层全参数微调策略，通过仅在每个训练步骤中更新参数的子集，可以显著减少GPU内存使用，并实现与参数高效微调和标准全参数微调相当的性能。

    

    随着语言模型的增长，在下游任务中微调语言模型的全参数需要占用大量GPU内存。现有方法利用零阶优化器以节省GPU内存，但这可能会影响语言模型的性能，因为非零阶优化器在大多数下游任务上更容易收敛。本文提出了一种新颖的独立于优化器的端到端分层微调策略HiFT，它仅在每个训练步骤中更新参数的子集。 HiFT可以显著减少存储在GPU内存中的梯度和优化器状态参数的量，从而减少GPU内存的使用。我们的结果表明：（1）HiFT实现了与参数高效微调和标准全参数微调相当的性能。（2）HiFT支持包括在内的各种优化器

    Full-parameter fine-tuning has become the go-to choice for adapting language models (LMs) to downstream tasks due to its excellent performance. As LMs grow in size, fine-tuning the full parameters of LMs requires a prohibitively large amount of GPU memory. Existing approaches utilize zeroth-order optimizer to conserve GPU memory, which can potentially compromise the performance of LMs as non-zero order optimizers tend to converge more readily on most downstream tasks. In this paper, we propose a novel optimizer-independent end-to-end hierarchical fine-tuning strategy, HiFT, which only updates a subset of parameters at each training step. HiFT can significantly reduce the amount of gradients and optimizer state parameters residing in GPU memory at the same time, thereby reducing GPU memory usage. Our results demonstrate that: (1) HiFT achieves comparable performance to parameter-efficient fine-tuning and standard full parameter fine-tuning. (2) HiFT supports various optimizers including
    
[^38]: FedGT: 可扩展图形变压器的联邦节点分类

    FedGT: Federated Node Classification with Scalable Graph Transformer. (arXiv:2401.15203v1 [cs.LG])

    [http://arxiv.org/abs/2401.15203](http://arxiv.org/abs/2401.15203)

    本论文提出了一种可扩展的联邦图形变压器（FedGT），用于解决子图联邦学习中缺少链接和子图异构性的挑战。

    

    图表被广泛用于建模关系数据。随着图表在现实场景中变得越来越大，存储和计算多个本地系统中的子图表成为一种趋势。最近提出的子图联邦学习方法例如通过在本地子图上进行图神经网络（GNN）的分布式训练，并通过集中服务器聚合GNN参数。然而，现有的方法存在以下限制：（1）子图联邦学习中缺少本地子图之间的链接。这可能严重影响遵循消息传递范式更新节点/边特征的GNN的性能。 （2）大多数现有方法忽视了子图异构性的问题，因为子图来自整个图的不同部分。为解决上述挑战，我们在本文中提出了一种可扩展的联邦图变压器（FedGT）。首先，我们设计了一种混合注意力方案来降低复杂性。

    Graphs are widely used to model relational data. As graphs are getting larger and larger in real-world scenarios, there is a trend to store and compute subgraphs in multiple local systems. For example, recently proposed \emph{subgraph federated learning} methods train Graph Neural Networks (GNNs) distributively on local subgraphs and aggregate GNN parameters with a central server. However, existing methods have the following limitations: (1) The links between local subgraphs are missing in subgraph federated learning. This could severely damage the performance of GNNs that follow message-passing paradigms to update node/edge features. (2) Most existing methods overlook the subgraph heterogeneity issue, brought by subgraphs being from different parts of the whole graph. To address the aforementioned challenges, we propose a scalable \textbf{Fed}erated \textbf{G}raph \textbf{T}ransformer (\textbf{FedGT}) in the paper. Firstly, we design a hybrid attention scheme to reduce the complexity 
    
[^39]: SCANIA组件X数据集：用于预测性维护的真实世界多变量时间序列数据集

    SCANIA Component X Dataset: A Real-World Multivariate Time Series Dataset for Predictive Maintenance. (arXiv:2401.15199v1 [cs.LG])

    [http://arxiv.org/abs/2401.15199](http://arxiv.org/abs/2401.15199)

    这个论文介绍了一种来自SCANIA公司的真实世界多变量时间序列数据集，该数据集适用于各种机器学习应用，尤其是预测性维护场景。它具有庞大的样本数量和多样化的特征，以及时间信息，为研究者提供了一个使用真实世界数据的标准基准。

    

    本论文介绍了一种来自SCANIA瑞典公司的卡车车队中匿名发动机部件（称为Component X）的真实世界多变量时间序列数据集。该数据集包括多种变量，捕捉了详细的操作数据、维修记录和卡车规格，同时通过匿名处理保持机密性。它非常适用于各种机器学习应用，如分类、回归、生存分析和异常检测，特别是在预测性维护场景中的应用。庞大的样本数量和以直方图和计数器形式的多样化特征，以及包含时间信息，使得这个真实世界数据集在该领域中独特。发布这个数据集的目标是让广大研究人员有可能使用来自一家国际知名公司的真实世界数据，并引入一个标准基准用于预测性维护的研究。

    This paper presents a description of a real-world, multivariate time series dataset collected from an anonymized engine component (called Component X) of a fleet of trucks from SCANIA, Sweden. This dataset includes diverse variables capturing detailed operational data, repair records, and specifications of trucks while maintaining confidentiality by anonymization. It is well-suited for a range of machine learning applications, such as classification, regression, survival analysis, and anomaly detection, particularly when applied to predictive maintenance scenarios. The large population size and variety of features in the format of histograms and numerical counters, along with the inclusion of temporal information, make this real-world dataset unique in the field. The objective of releasing this dataset is to give a broad range of researchers the possibility of working with real-world data from an internationally well-known company and introduce a standard benchmark to the predictive ma
    
[^40]: AMuSE: 自适应多模态分析用于群体对话中的说话者情感识别

    AMuSE: Adaptive Multimodal Analysis for Speaker Emotion Recognition in Group Conversations. (arXiv:2401.15164v1 [cs.SD])

    [http://arxiv.org/abs/2401.15164](http://arxiv.org/abs/2401.15164)

    本文提出了一种AMuSE模型，通过多模态注意力网络在群体对话中捕捉并分析说话者的情感。模型通过联合学习多个模态的外围和中央网络实现了不同级别的跨模态交互。

    

    在开发具有自然人机交互能力的智能代理时，分析群体对话中的个体情感是至关重要的。虽然可靠的情感识别技术依赖于不同的模态（文本、音频、视频），但这些模态之间的异质性以及受个体行为模式影响的动态跨模态交互使情感识别任务变得非常具有挑战性。在群体环境中，这种困难变得更加复杂，因为情感及其时间演变不仅受个体影响，还受到观众反应和进行中对话的背景等外部环境的影响。为了应对这个挑战，我们提出了一种多模态注意力网络，通过联合学习模态特定的外围网络和中央网络的交互集合来捕捉不同级别的空间抽象中的跨模态交互。所提出的网络通过其外围键值对注入跨模态注意力。

    Analyzing individual emotions during group conversation is crucial in developing intelligent agents capable of natural human-machine interaction. While reliable emotion recognition techniques depend on different modalities (text, audio, video), the inherent heterogeneity between these modalities and the dynamic cross-modal interactions influenced by an individual's unique behavioral patterns make the task of emotion recognition very challenging. This difficulty is compounded in group settings, where the emotion and its temporal evolution are not only influenced by the individual but also by external contexts like audience reaction and context of the ongoing conversation. To meet this challenge, we propose a Multimodal Attention Network that captures cross-modal interactions at various levels of spatial abstraction by jointly learning its interactive bunch of mode-specific Peripheral and Central networks. The proposed MAN injects cross-modal attention via its Peripheral key-value pairs 
    
[^41]: FDR控制的稀疏金融指数跟踪投资组合优化

    FDR-Controlled Portfolio Optimization for Sparse Financial Index Tracking. (arXiv:2401.15139v1 [q-fin.PM])

    [http://arxiv.org/abs/2401.15139](http://arxiv.org/abs/2401.15139)

    本论文提出了一种扩展的T-Rex框架，用于在稀疏金融指数跟踪中选择少数相关变量，并通过集成最近邻惩罚机制，可靠控制误发现率（FDR）。实验证明了该方法在过去20年内基于少量股票准确跟踪标准普尔500指数的能力。

    

    在高维数据分析中，如金融指数跟踪或生物医学应用中，关键是在保持对误发现率（FDR）的控制的同时选择少数相关变量。在这些应用中，变量之间经常存在强依赖关系（例如股票收益），这可能会削弱现有方法（如模型X knockoff方法或T-Rex选择器）的FDR控制特性。为了解决这个问题，我们扩展了T-Rex框架，以适应高度相关变量的重叠组。这是通过将最近邻惩罚机制集成到框架中实现的，该机制能够在用户定义的目标水平上可靠控制FDR。稀疏指数跟踪的实例展示了该方法在过去20年内基于少量股票准确跟踪标准普尔500指数的能力。在CRAN上提供了R包TRexSelector的开源实现。

    In high-dimensional data analysis, such as financial index tracking or biomedical applications, it is crucial to select the few relevant variables while maintaining control over the false discovery rate (FDR). In these applications, strong dependencies often exist among the variables (e.g., stock returns), which can undermine the FDR control property of existing methods like the model-X knockoff method or the T-Rex selector. To address this issue, we have expanded the T-Rex framework to accommodate overlapping groups of highly correlated variables. This is achieved by integrating a nearest neighbors penalization mechanism into the framework, which provably controls the FDR at the user-defined target level. A real-world example of sparse index tracking demonstrates the proposed method's ability to accurately track the S&P 500 index over the past 20 years based on a small number of stocks. An open-source implementation is provided within the R package TRexSelector on CRAN.
    
[^42]: 评估用于基于OSINT的网络威胁意识的LLM聊天机器人

    Evaluation of LLM Chatbots for OSINT-based Cyberthreat Awareness. (arXiv:2401.15127v1 [cs.CR])

    [http://arxiv.org/abs/2401.15127](http://arxiv.org/abs/2401.15127)

    本研究评估了LLM聊天机器人在基于OSINT的网络威胁意识中的应用能力，并发现聊天机器人在网络安全的二分类和命名实体识别任务方面表现出良好的性能。

    

    在快速发展的网络安全领域中，关于新兴威胁的知识共享至关重要，并构成了网络威胁情报的基础。在这个背景下，大型语言模型在网络安全领域越来越重要，提供了广泛的机遇。本研究探讨了ChatGPT、GPT4all、Dolly、Stanford Alpaca、Alpaca-LoRA和Falcon等聊天机器人在识别开源情报中与网络安全相关的文本方面的能力。我们评估了现有聊天机器人模型在自然语言处理任务中的能力。我们考虑了二分类和命名实体识别作为任务。本研究分析了从Twitter收集的经过充分验证的数据，该数据来源于以往的研究工作。在网络安全的二分类问题方面，商业模型Chatbot GPT-4实现了可接受的F1分数0.94，而开源模型GPT4all实现了F1分数0.90。然而，就网络安全实体识别而言，

    Knowledge sharing about emerging threats is crucial in the rapidly advancing field of cybersecurity and forms the foundation of Cyber Threat Intelligence. In this context, Large Language Models are becoming increasingly significant in the field of cybersecurity, presenting a wide range of opportunities. This study explores the capability of chatbots such as ChatGPT, GPT4all, Dolly,Stanford Alpaca, Alpaca-LoRA, and Falcon to identify cybersecurity-related text within Open Source Intelligence. We assess the capabilities of existing chatbot models for Natural Language Processing tasks. We consider binary classification and Named Entity Recognition as tasks. This study analyzes well-established data collected from Twitter, derived from previous research efforts. Regarding cybersecurity binary classification, Chatbot GPT-4 as a commercial model achieved an acceptable F1-score of 0.94, and the open-source GPT4all model achieved an F1-score of 0.90. However, concerning cybersecurity entity re
    
[^43]: 大型语言模型引导的知识蒸馏用于时间序列异常检测

    Large Language Model Guided Knowledge Distillation for Time Series Anomaly Detection. (arXiv:2401.15123v1 [cs.LG])

    [http://arxiv.org/abs/2401.15123](http://arxiv.org/abs/2401.15123)

    提出了一种基于大型语言模型引导的知识蒸馏的时间序列异常检测方法AnomalyLLM，通过训练学生网络模仿预训练的大型语言模型的特征，在测试阶段通过比较学生网络和教师网络的特征差异来检测异常。

    

    自监督方法在时间序列异常检测中变得越来越重要，因为可用标注数据的稀缺性。然而，它们通常需要大量的训练数据来获得可泛化的表示映射，这与只有少量样本的情况冲突，从而限制了它们的性能。为了克服这个限制，我们提出了一种基于知识蒸馏的时间序列异常检测方法AnomalyLLM，在这种方法中，学生网络被训练成模仿基于大规模数据集预训练的大型语言模型(LLM)的特征。在测试阶段，当学生网络与教师网络的特征差异很大时，就检测到异常。为了避免学生网络学习到教师网络对异常样本的特征，我们设计了两个关键策略。1) 将典型信号融入学生网络，以巩固正常特征提取。2) 加权教师网络和学生网络的特征，以减少异常样本的影响。

    Self-supervised methods have gained prominence in time series anomaly detection due to the scarcity of available annotations. Nevertheless, they typically demand extensive training data to acquire a generalizable representation map, which conflicts with scenarios of a few available samples, thereby limiting their performance. To overcome the limitation, we propose \textbf{AnomalyLLM}, a knowledge distillation-based time series anomaly detection approach where the student network is trained to mimic the features of the large language model (LLM)-based teacher network that is pretrained on large-scale datasets. During the testing phase, anomalies are detected when the discrepancy between the features of the teacher and student networks is large. To circumvent the student network from learning the teacher network's feature of anomalous samples, we devise two key strategies. 1) Prototypical signals are incorporated into the student network to consolidate the normal feature extraction. 2) W
    
[^44]: 一种多级对称微分方程模型用于学习蛋白质-配体结合动力学

    A Multi-Grained Symmetric Differential Equation Model for Learning Protein-Ligand Binding Dynamics. (arXiv:2401.15122v1 [cs.LG])

    [http://arxiv.org/abs/2401.15122](http://arxiv.org/abs/2401.15122)

    提出了一种能够促进数值MD模拟并有效模拟蛋白质-配体结合动力学的NeuralMD方法，采用物理信息多级对称框架，实现了准确建模多级蛋白质-配体相互作用。

    

    在药物发现中，蛋白质-配体结合的分子动力学（MD）模拟提供了一种强大的工具，用于预测结合亲和力，估计运输性能和探索口袋位点。通过改进数值方法以及最近通过机器学习（ML）方法增强MD模拟的效率已经有了很长的历史。然而，仍然存在一些挑战，例如准确建模扩展时间尺度的模拟。为了解决这个问题，我们提出了NeuralMD，这是第一个能够促进数值MD并提供准确的蛋白质-配体结合动力学模拟的ML辅助工具。我们提出了一个合理的方法，将一种新的物理信息多级对称框架纳入模型中。具体而言，我们提出了（1）一个使用向量框架满足群对称性并捕获多级蛋白质-配体相互作用的BindingNet模型，以及（2）一个增强的神经微分方程求解器，学习轨迹的演化。

    In drug discovery, molecular dynamics (MD) simulation for protein-ligand binding provides a powerful tool for predicting binding affinities, estimating transport properties, and exploring pocket sites. There has been a long history of improving the efficiency of MD simulations through better numerical methods and, more recently, by augmenting them with machine learning (ML) methods. Yet, challenges remain, such as accurate modeling of extended-timescale simulations. To address this issue, we propose NeuralMD, the first ML surrogate that can facilitate numerical MD and provide accurate simulations of protein-ligand binding dynamics. We propose a principled approach that incorporates a novel physics-informed multi-grained group symmetric framework. Specifically, we propose (1) a BindingNet model that satisfies group symmetry using vector frames and captures the multi-level protein-ligand interactions, and (2) an augmented neural differential equation solver that learns the trajectory und
    
[^45]: ReLU和Step网络在浮点运算下的表达能力

    Expressive Power of ReLU and Step Networks under Floating-Point Operations. (arXiv:2401.15121v1 [cs.LG])

    [http://arxiv.org/abs/2401.15121](http://arxiv.org/abs/2401.15121)

    该论文研究了在浮点运算下神经网络的表达能力，证明了使用二进制阈值单元或ReLU的神经网络可以记忆任何实数输入/输出对，并且可以在小误差内逼近任何连续函数。

    

    神经网络表达能力的研究调查了神经网络的基本限制。大多数现有的结果假设实数输入和参数以及在神经网络评估过程中进行精确运算。然而，神经网络通常在只能表示实数的计算机上执行，并且进行不精确的运算。在这项工作中，我们分析了在更实际的设置下神经网络的表达能力：使用浮点数和浮点运算。我们的第一组结果假设浮点运算中，浮点数的有效位数由有限位表示，但其指数可以取任何整数值。在这种设置下，我们展示了神经网络使用二进制阈值单元或ReLU可以记忆任何有限的输入/输出对，并可以在小误差内逼近任何连续函数。我们还展示了浮点运算下关于记忆和通用逼近的类似结果。

    The study of the expressive power of neural networks has investigated the fundamental limits of neural networks. Most existing results assume real-valued inputs and parameters as well as exact operations during the evaluation of neural networks. However, neural networks are typically executed on computers that can only represent a tiny subset of the reals and apply inexact operations. In this work, we analyze the expressive power of neural networks under a more realistic setup: when we use floating-point numbers and operations. Our first set of results assumes floating-point operations where the significand of a float is represented by finite bits but its exponent can take any integer value. Under this setup, we show that neural networks using a binary threshold unit or ReLU can memorize any finite input/output pairs and can approximate any continuous function within a small error. We also show similar results on memorization and universal approximation when floating-point operations u
    
[^46]: 解释时间序列Transformer模型和COVID-19感染对人口年龄组的敏感性分析

    Interpreting Time Series Transformer Models and Sensitivity Analysis of Population Age Groups to COVID-19 Infections. (arXiv:2401.15119v1 [cs.LG])

    [http://arxiv.org/abs/2401.15119](http://arxiv.org/abs/2401.15119)

    该论文研究了解释深度学习时间序列模型的重要性，并通过局部解释方法解释了最先进的Transformer模型。通过将13个输入特征与3,142个美国县的三年日案例数据相结合，最佳预测模型能够在过去两周的基础上预测接下来两周的COVID-19感染情况。此外，该研究还提出了一种创新的评估方法，用于评估感染对8个人口年龄组的敏感性。

    

    解释深度学习时间序列模型对于理解模型行为和从原始数据中学习模式以进行实时决策非常重要。然而，基于Transformer的时间序列模型的复杂性使解释个体特征对预测影响的挑战变得困难。在这项研究中，我们利用最近的局部解释方法来解释最先进的时间序列模型。为了使用真实世界的数据集，我们收集了3142个美国县的三年日案例数据。首先，我们比较了六种基于Transformer的模型，并选择了最佳的COVID-19感染预测模型。使用过去两周的13个输入特征，我们可以预测接下来两周的病例数量。其次，我们提出了一种创新的方法，评估了预测对8个人口年龄组的敏感性，以及高度动态的多变量感染数据。第三，我们将我们提出的基于扰动的解释方法与相关工作进行比较，总共包括...

    Interpreting deep learning time series models is crucial in understanding the model's behavior and learning patterns from raw data for real-time decision-making. However, the complexity inherent in transformer-based time series models poses challenges in explaining the impact of individual features on predictions. In this study, we leverage recent local interpretation methods to interpret state-of-the-art time series models. To use real-world datasets, we collected three years of daily case data for 3,142 US counties. Firstly, we compare six transformer-based models and choose the best prediction model for COVID-19 infection. Using 13 input features from the last two weeks, we can predict the cases for the next two weeks. Secondly, we present an innovative way to evaluate the prediction sensitivity to 8 population age groups over highly dynamic multivariate infection data. Thirdly, we compare our proposed perturbation-based interpretation method with related work, including a total of 
    
[^47]: 有效的在线众包与复杂注释

    Efficient Online Crowdsourcing with Complex Annotations. (arXiv:2401.15116v1 [cs.HC])

    [http://arxiv.org/abs/2401.15116](http://arxiv.org/abs/2401.15116)

    本文提出了一种有效的在线众包算法，能够在多个标注者中聚合复杂注释，通过推断标注者的准确性来改善成本-质量权衡。基于实验结果表明该算法在真实众包数据上取得了很好的效果。

    

    众包平台使用各种真实性发现算法来聚合来自多个标注者的注释。然而，在在线环境中，主要挑战是决定是否为每个项目请求更多的注释，以高效地权衡成本（即注释数量）和聚合注释的质量。本文提出了一种适用于在线众包环境的通用复杂注释（如边界框和分类路径）的新方法。我们证明了标注者的预期平均相似度与他们的准确性在"给定报告的标签"条件下是线性关系。这使得我们能够推断在各种场景下的报告标签准确度。我们在Meta的真实众包数据上进行了大量评估，并展示了我们提出的在线算法在提升成本-质量权衡方面的有效性。

    Crowdsourcing platforms use various truth discovery algorithms to aggregate annotations from multiple labelers. In an online setting, however, the main challenge is to decide whether to ask for more annotations for each item to efficiently trade off cost (i.e., the number of annotations) for quality of the aggregated annotations. In this paper, we propose a novel approach for general complex annotation (such as bounding boxes and taxonomy paths), that works in an online crowdsourcing setting. We prove that the expected average similarity of a labeler is linear in their accuracy \emph{conditional on the reported label}. This enables us to infer reported label accuracy in a broad range of scenarios. We conduct extensive evaluations on real-world crowdsourcing data from Meta and show the effectiveness of our proposed online algorithms in improving the cost-quality trade-off.
    
[^48]: 使用深度学习和开放地球观测数据实现全球冰川制图

    Towards Global Glacier Mapping with Deep Learning and Open Earth Observation Data. (arXiv:2401.15113v1 [cs.CV])

    [http://arxiv.org/abs/2401.15113](http://arxiv.org/abs/2401.15113)

    本研究提出了一种使用深度学习和开放地球观测数据进行全球冰川制图的方法，通过新的模型和策略，在多种地形和传感器上实现了较高的准确性。通过添加合成孔径雷达数据，并报告冰川范围的校准置信度，提高了预测的可靠性和可解释性。

    

    准确的全球冰川制图对于理解气候变化的影响至关重要。这个过程受到冰川多样性、难以分类的碎石和大数据处理的挑战。本文提出了Glacier-VisionTransformer-U-Net (GlaViTU)，一个卷积-Transformer深度学习模型，并提出了五种利用开放卫星影像进行多时相全球冰川制图的策略。空间、时间和跨传感器的泛化性能评估表明，我们的最佳策略在大多数情况下实现了IoU（交并比）> 0.85，并且在以冰雪为主的地区增加到了> 0.90，而在高山亚洲等碎石丰富的区域则降至> 0.75。此外，添加合成孔径雷达数据，即回波和干涉相干度，可以提高所有可用地区的准确性。报告冰川范围的校准置信度使预测更可靠和可解释。我们还发布了一个基准数据集。

    Accurate global glacier mapping is critical for understanding climate change impacts. It is challenged by glacier diversity, difficult-to-classify debris and big data processing. Here we propose Glacier-VisionTransformer-U-Net (GlaViTU), a convolutional-transformer deep learning model, and five strategies for multitemporal global-scale glacier mapping using open satellite imagery. Assessing the spatial, temporal and cross-sensor generalisation shows that our best strategy achieves intersection over union >0.85 on previously unobserved images in most cases, which drops to >0.75 for debris-rich areas such as High-Mountain Asia and increases to >0.90 for regions dominated by clean ice. Additionally, adding synthetic aperture radar data, namely, backscatter and interferometric coherence, increases the accuracy in all regions where available. The calibrated confidence for glacier extents is reported making the predictions more reliable and interpretable. We also release a benchmark dataset 
    
[^49]: 通过对比学习改善自动化胸部X射线诊断的公平性

    Improving Fairness of Automated Chest X-ray Diagnosis by Contrastive Learning. (arXiv:2401.15111v1 [eess.IV])

    [http://arxiv.org/abs/2401.15111](http://arxiv.org/abs/2401.15111)

    该论文提出了一种利用对比学习来改善胸部X射线诊断公平性的方法，通过对两个数据集进行评估，利用精心选择的正负样本生成公平的图像嵌入。

    

    目的：在放射学领域中，有限的研究探索解决和提升模型公平性的具体方法或途径。我们提出的AI模型利用监督对比学习来减少胸部X射线诊断中的偏差。材料和方法：在这项回顾性研究中，我们在两个数据集上评估了我们提出的方法：医学影像和数据资源中心（MIDRC）数据集，包括77,887张来自27,796名患者的CXR图像，收集截至2023年4月20日，用于COVID-19诊断；国立卫生研究院胸部X射线（NIH-CXR）数据集，包括112,120张来自30,805名患者在1992年至2015年间收集的CXR图像。在NIH-CXR数据集中，胸部异常包括肺不张、心脏增大、积液、浸润、肿块、结节、肺炎、气胸、实变、水肿、肺气肿、纤维化、胸膜增厚或疝气。我们的方法利用精心选择的正负样本进行监督对比学习，生成公平的图像嵌入，然后进行微调。

    Purpose: Limited studies exploring concrete methods or approaches to tackle and enhance model fairness in the radiology domain. Our proposed AI model utilizes supervised contrastive learning to minimize bias in CXR diagnosis.  Materials and Methods: In this retrospective study, we evaluated our proposed method on two datasets: the Medical Imaging and Data Resource Center (MIDRC) dataset with 77,887 CXR images from 27,796 patients collected as of April 20, 2023 for COVID-19 diagnosis, and the NIH Chest X-ray (NIH-CXR) dataset with 112,120 CXR images from 30,805 patients collected between 1992 and 2015. In the NIH-CXR dataset, thoracic abnormalities include atelectasis, cardiomegaly, effusion, infiltration, mass, nodule, pneumonia, pneumothorax, consolidation, edema, emphysema, fibrosis, pleural thickening, or hernia. Our proposed method utilizes supervised contrastive learning with carefully selected positive and negative samples to generate fair image embeddings, which are fine-tuned f
    
[^50]: 多智能体深度强化学习在竞争中为快速充电电动车中心的动态定价中的应用

    Multi-agent Deep Reinforcement Learning for Dynamic Pricing by Fast-charging Electric Vehicle Hubs in ccompetition. (arXiv:2401.15108v1 [cs.LG])

    [http://arxiv.org/abs/2401.15108](http://arxiv.org/abs/2401.15108)

    本文提出了一个多智能体深度强化学习的方法，应用于快速充电电动车中心的动态定价竞争。通过预测性购买电力需求和设定竞争性价格策略，充电站可以在竞争中进行有效定价。

    

    快速充电站将成为全球新建交通电气化基础设施的一部分。这些充电站将承载许多直流快速充电设备，仅可供电动车辆充电使用。类似于汽油加油站，同一地区的快速充电站将根据竞争调整价格以吸引同一群电动车主。这些充电站将与电力网络进行交互，通过预测性购买在前一天电力市场上的电力需求，并在实时市场上满足差额需求。充电站可能配备补充电池储能系统用于套利。本文针对充电站竞争中开发了一个两步数据驱动的动态定价方法。首先通过求解随机的前一天电力需求模型得到纳入承诺，然后通过将游戏建模为竞争来得到充电站的价格策略。

    Fast-charging hubs for electric vehicles will soon become part of the newly built infrastructure for transportation electrification across the world. These hubs are expected to host many DC fast-charging stations and will admit EVs only for charging. Like the gasoline refueling stations, fast-charging hubs in a neighborhood will dynamically vary their prices to compete for the same pool of EV owners. These hubs will interact with the electric power network by making purchase commitments for a significant part of their power needs in the day-ahead (DA) electricity market and meeting the difference from the real-time (RT) market. Hubs may have supplemental battery storage systems (BSS), which they will use for arbitrage. In this paper, we develop a two-step data-driven dynamic pricing methodology for hubs in price competition. We first obtain the DA commitment by solving a stochastic DA commitment model. Thereafter we obtain the hub pricing strategies by modeling the game as a competitiv
    
[^51]: 在李群上的神经常微分方程对SE(3)的优化潜力塑造

    Optimal Potential Shaping on SE(3) via Neural ODEs on Lie Groups. (arXiv:2401.15107v1 [math.OC])

    [http://arxiv.org/abs/2401.15107](http://arxiv.org/abs/2401.15107)

    本文提出了一种在有限维李群上优化动态系统的新方法，通过将动态系统表示为神经常微分方程，并在李群上制定优化问题。提出了一种可扩展的梯度下降算法来解决优化问题，并通过在李代数级别表示系统来降低计算成本。在一个例子中，处理了刚体控制的最优势能塑形，并通过迭代优化控制器来验证最终结果。

    

    本工作提出了一种新颖的方法，用于优化有限维李群上的动态系统。我们将动态系统重新表述为所谓的神经常微分方程(neural ODEs)，并在李群上制定优化问题。提出了一种梯度下降优化算法来解决数值优化问题。我们的算法可扩展，并适用于任何有限维李群，包括矩阵李群。通过在李代数级别表示系统，减少了梯度计算的计算成本。在一个广泛的例子中，处理了对刚体控制的最优势能塑形。将最优控制问题表述为对李群SE(3)上的神经常微分方程(ODE)的优化，并对控制器进行迭代优化。最后，在状态调节任务上验证了最终的控制器。

    This work presents a novel approach for the optimization of dynamic systems on finite-dimensional Lie groups. We rephrase dynamic systems as so-called neural ordinary differential equations (neural ODEs), and formulate the optimization problem on Lie groups. A gradient descent optimization algorithm is presented to tackle the optimization numerically. Our algorithm is scalable, and applicable to any finite dimensional Lie group, including matrix Lie groups. By representing the system at the Lie algebra level, we reduce the computational cost of the gradient computation. In an extensive example, optimal potential energy shaping for control of a rigid body is treated. The optimal control problem is phrased as an optimization of a neural ODE on the Lie group SE(3), and the controller is iteratively optimized. The final controller is validated on a state-regulation task.
    
[^52]: 基于扩散增强的超高分辨率遥感图像云去除

    Diffusion Enhancement for Cloud Removal in Ultra-Resolution Remote Sensing Imagery. (arXiv:2401.15105v1 [eess.IV])

    [http://arxiv.org/abs/2401.15105](http://arxiv.org/abs/2401.15105)

    本文提出了一种基于扩散增强的云去除方法，通过在数据和方法上进行改进，实现了对超高分辨率遥感图像中云层的准确去除和详细语义内容恢复。

    

    云层的存在严重影响了光学遥感（RS）图像的质量和效果。然而，现有的基于深度学习（DL）的云去除（CR）技术在准确重建图像的视觉真实性和详细语义内容方面遇到了困难。为了应对这一挑战，本研究提出了在数据和方法上进行改进的方案。在数据方面，建立了一个名为CUHK Cloud Removal (CUHK-CR) 的0.5m空间分辨率的超高分辨率基准并加入了丰富的细节纹理和多样化的云覆盖，为设计和评估CR模型提供了可靠的基础。从方法的角度出发，提出了一种称为Diffusion Enhancement（DE）的基于扩散的CR框架，用于进行渐进纹理细节恢复，以减轻训练难度并提高推理准确性。此外，还提出了一种权重分配（WA）网络。

    The presence of cloud layers severely compromises the quality and effectiveness of optical remote sensing (RS) images. However, existing deep-learning (DL)-based Cloud Removal (CR) techniques encounter difficulties in accurately reconstructing the original visual authenticity and detailed semantic content of the images. To tackle this challenge, this work proposes to encompass enhancements at the data and methodology fronts. On the data side, an ultra-resolution benchmark named CUHK Cloud Removal (CUHK-CR) of 0.5m spatial resolution is established. This benchmark incorporates rich detailed textures and diverse cloud coverage, serving as a robust foundation for designing and assessing CR models. From the methodology perspective, a novel diffusion-based framework for CR called Diffusion Enhancement (DE) is proposed to perform progressive texture detail recovery, which mitigates the training difficulty with improved inference accuracy. Additionally, a Weight Allocation (WA) network is dev
    
[^53]: PruneSymNet：一种用于符号回归的符号神经网络和修剪算法

    PruneSymNet: A Symbolic Neural Network and Pruning Algorithm for Symbolic Regression. (arXiv:2401.15103v1 [cs.LG])

    [http://arxiv.org/abs/2401.15103](http://arxiv.org/abs/2401.15103)

    PruneSymNet是一种用于符号回归的新颖神经网络，可以通过贪婪修剪算法提取子网络以获得所需的符号表达式。

    

    符号回归旨在从数据中导出可解释的符号表达式，以便更好地理解和解释数据。本研究提出了一种名为PruneSymNet的符号网络，用于符号回归。这是一种新颖的神经网络，其激活函数由常见的基本函数和运算符组成。整个网络是可微分的，并可以通过梯度下降方法进行训练。网络中的每个子网络对应一个表达式，我们的目标是提取这些子网络以获得所需的符号表达式。因此，提出了一种贪婪修剪算法，将网络剪成子网络，同时确保数据拟合的精度。所提出的贪婪修剪算法每次修剪都保留损失最小的边，但贪婪算法通常无法得到最优解。为了缓解这个问题，我们结合了束搜索方法。

    Symbolic regression aims to derive interpretable symbolic expressions from data in order to better understand and interpret data. %which plays an important role in knowledge discovery and interpretable machine learning.  In this study, a symbolic network called PruneSymNet is proposed for symbolic regression. This is a novel neural network whose activation function consists of common elementary functions and operators. The whole network is differentiable and can be trained by gradient descent method. Each subnetwork in the network corresponds to an expression, and our goal is to extract such subnetworks to get the desired symbolic expression.  Therefore, a greedy pruning algorithm is proposed to prune the network into a subnetwork while ensuring the accuracy of data fitting. The proposed greedy pruning algorithm preserves the edge with the least loss in each pruning, but greedy algorithm often can not get the optimal solution. In order to alleviate this problem, we combine beam search 
    
[^54]: Hi-Core: 面向连续强化学习的层次化知识迁移

    Hi-Core: Hierarchical Knowledge Transfer for Continual Reinforcement Learning. (arXiv:2401.15098v1 [cs.LG])

    [http://arxiv.org/abs/2401.15098](http://arxiv.org/abs/2401.15098)

    Hi-Core提出了一种新的框架，通过层次化的知识迁移来增强连续强化学习。该框架包括利用大型语言模型的推理能力设定目标的高层策略制定和通过强化学习按照高层目标导向的低层策略学习。在实验中，Hi-Core展现了较强的知识迁移能力。

    

    连续强化学习（Continual Reinforcement Learning, CRL）赋予强化学习智能体从一系列任务中学习的能力，保留先前的知识并利用它来促进未来的学习。然而，现有的方法往往专注于在类似任务之间传输低层次的知识，忽视了人类认知控制的层次结构，导致在各种任务之间的知识迁移不足。为了增强高层次的知识迁移，我们提出了一种名为Hi-Core (Hierarchical knowledge transfer for Continual reinforcement learning)的新框架，它由两层结构组成：1) 利用大型语言模型（Large Language Model, LLM）的强大推理能力设定目标的高层策略制定和2) 通过强化学习按照高层目标导向的低层策略学习。此外，构建了一个知识库（策略库）来存储可以用于层次化知识迁移的策略。在MiniGr实验中进行了实验。

    Continual reinforcement learning (CRL) empowers RL agents with the ability to learn from a sequence of tasks, preserving previous knowledge and leveraging it to facilitate future learning. However, existing methods often focus on transferring low-level knowledge across similar tasks, which neglects the hierarchical structure of human cognitive control, resulting in insufficient knowledge transfer across diverse tasks. To enhance high-level knowledge transfer, we propose a novel framework named Hi-Core (Hierarchical knowledge transfer for Continual reinforcement learning), which is structured in two layers: 1) the high-level policy formulation which utilizes the powerful reasoning ability of the Large Language Model (LLM) to set goals and 2) the low-level policy learning through RL which is oriented by high-level goals. Moreover, the knowledge base (policy library) is constructed to store policies that can be retrieved for hierarchical knowledge transfer. Experiments conducted in MiniGr
    
[^55]: 二进制感知机容量的研究

    A note on the capacity of the binary perceptron. (arXiv:2401.15092v1 [math.PR])

    [http://arxiv.org/abs/2401.15092](http://arxiv.org/abs/2401.15092)

    该论文研究了二进制感知机的容量问题，在确定了上界和下界后，给出了证明该容量小于0.847的条件一阶矩方法与已知结果的结合。

    

    确定二进制感知机的容量αc是一个长期存在的问题。Krauth和Mezard（1989）猜测了αc的明确值，大约等于0.833，最近Ding和Sun（2019）建立了与此预测相符的严格下界。关于上界，Kim和Roche（1998）以及Talagrand（1999）分别显示αc < 0.996，而Krauth和Mezard概述了一个可以用于显示αc < 0.847的论证。这个说明的目的是记录一个完整的证明αc < 0.847的证明。该证明是一种条件一阶矩方法与已知的球形感知机结果相结合。

    Determining the capacity $\alpha_c$ of the Binary Perceptron is a long-standing problem. Krauth and Mezard (1989) conjectured an explicit value of $\alpha_c$, approximately equal to .833, and a rigorous lower bound matching this prediction was recently established by Ding and Sun (2019). Regarding the upper bound, Kim and Roche (1998) and Talagrand (1999) independently showed that $\alpha_c$ < .996, while Krauth and Mezard outlined an argument which can be used to show that $\alpha_c$ < .847. The purpose of this expository note is to record a complete proof of the bound $\alpha_c$ < .847. The proof is a conditional first moment method combined with known results on the spherical perceptron
    
[^56]: 使用通用完全等变量加速材料属性预测

    Accelerating Material Property Prediction using Generically Complete Isometry Invariants. (arXiv:2401.15089v1 [cs.LG])

    [http://arxiv.org/abs/2401.15089](http://arxiv.org/abs/2401.15089)

    本研究提出了一种使用通用完全等变量加速材料属性预测的方法，通过采用点距离分布(PDD)作为学习算法的表示，并开发了一个修改的自注意机制的变压器模型来利用PDD。

    

    最近几年，使用机器学习进行材料或晶体属性预测变得流行起来，因为它提供了对传统模拟方法的计算上高效的替代。对于这些算法的关键第一步是周期性晶体的表示。虽然类似的分子和蛋白质等物体有有限数量的原子，并且它们的表示可以基于有限点云进行解释，但是周期性晶体的尺寸是无限的，所以它们的表示更具挑战性。在本研究中，我们采用了点距离分布(PDD)，这是一种连续且通用的完全等变量，用作我们学习算法的表示。尽管PDD在区分周期性点集的等变性上非常有效，但其没有考虑基础材料的组成。我们开发了一个具有修改的自注意机制的变压器模型，可以利用PDD和...

    Material or crystal property prediction using machine learning has grown popular in recent years as it provides a computationally efficient replacement to classical simulation methods. A crucial first step for any of these algorithms is the representation used for a periodic crystal. While similar objects like molecules and proteins have a finite number of atoms and their representation can be built based upon a finite point cloud interpretation, periodic crystals are unbounded in size, making their representation more challenging. In the present work, we adapt the Pointwise Distance Distribution (PDD), a continuous and generically complete isometry invariant for periodic point sets, as a representation for our learning algorithm. While the PDD is effective in distinguishing periodic point sets up to isometry, there is no consideration for the composition of the underlying material. We develop a transformer model with a modified self-attention mechanism that can utilize the PDD and inc
    
[^57]: 在资源有限的情况下设计和实施自动机器状态监测和维护系统

    Design & Implementation of Automatic Machine Condition Monitoring and Maintenance System in Limited Resource Situations. (arXiv:2401.15088v1 [eess.SY])

    [http://arxiv.org/abs/2401.15088](http://arxiv.org/abs/2401.15088)

    本研究设计和实施了一种在资源有限情况下的自动机器状态监测和维护系统，通过开发成本效益的数据采集系统和特征工程和数据降维方法，解决了发展中国家缺乏预测性维护和职业健康安全文化的问题。

    

    在第四次工业革命时代，自动化故障检测和诊断机械设备是至关重要的，这样可以开发一个警告系统，在发生灾难性损坏之前采取适当的行动。全球使用了一些机器健康监测系统，但它们昂贵且需要经过培训的人员操作和分析。由于发展中国家基础设施不足、缺乏熟练人员、财务危机等原因，预测性维护和职业健康安全文化不可得。从开发一种成本效益的数据采集系统开始，在本研究中探讨了有限的数据和资源对自动化过程的影响。为了解决这个问题，结合小波、微积分和信号处理的概念，开发了一种特征工程和数据降维方法。最后，为了自动化整个过程，完成了所有必要的理论和实证研究。

    In the era of the fourth industrial revolution, it is essential to automate fault detection and diagnosis of machineries so that a warning system can be developed that will help to take an appropriate action before any catastrophic damage. Some machines health monitoring systems are used globally but they are expensive and need trained personnel to operate and analyse. Predictive maintenance and occupational health and safety culture are not available due to inadequate infrastructure, lack of skilled manpower, financial crisis, and others in developing countries. Starting from developing a cost-effective DAS for collecting fault data in this study, the effect of limited data and resources has been investigated while automating the process. To solve this problem, A feature engineering and data reduction method has been developed combining the concepts from wavelets, differential calculus, and signal processing. Finally, for automating the whole process, all the necessary theoretical and
    
[^58]: 以质量守恒感知器为基础，实现可解释的物理-概念集水区尺度水文建模

    Towards Interpretable Physical-Conceptual Catchment-Scale Hydrological Modeling using the Mass-Conserving-Perceptron. (arXiv:2401.14521v1 [cs.LG])

    [http://arxiv.org/abs/2401.14521](http://arxiv.org/abs/2401.14521)

    本研究通过利用质量守恒感知器构建基于有向图结构的水文模型，实现了对集水区尺度水文过程的解释能力，在保持简洁性的同时能够准确地模拟各种流量动力学行为，并通过引入输入旁路机制进一步优化了模型的表现。

    

    本研究探讨了利用机器学习技术开发简洁可解释的集水区尺度水文模型的可行性，采用基于质量守恒感知器（MCP）的有向图结构作为基本计算单元。我们关注的是单个位置的结构复杂性（深度），而不是对大样本集水区具有普适性的广度。目标是发现一个最小的表示（单元状态数和流量路径数），用于表示能够解释给定集水区输入状态和输出行为的主要过程，特别强调模拟全范围（高、中、低）的流量动力学。我们发现，在我们的研究区域，采用类似HyMod的架构，具有3个单元状态和2个主要流动路径，能够实现这样的表示，但引入输入旁路机制可以显著改善水文图的时间和形状。

    We investigate the applicability of machine learning technologies to the development of parsimonious, interpretable, catchment-scale hydrologic models using directed-graph architectures based on the mass-conserving perceptron (MCP) as the fundamental computational unit. Here, we focus on architectural complexity (depth) at a single location, rather than universal applicability (breadth) across large samples of catchments. The goal is to discover a minimal representation (numbers of cell-states and flow paths) that represents the dominant processes that can explain the input-state-output behaviors of a given catchment, with particular emphasis given to simulating the full range (high, medium, and low) of flow dynamics. We find that a HyMod-like architecture with three cell-states and two major flow pathways achieves such a representation at our study location, but that the additional incorporation of an input-bypass mechanism significantly improves the timing and shape of the hydrograph
    
[^59]: 通过GPT引导的蒙特卡洛树搜索从数据中发现数学公式

    Discovering Mathematical Formulas from Data via GPT-guided Monte Carlo Tree Search. (arXiv:2401.14424v1 [cs.LG])

    [http://arxiv.org/abs/2401.14424](http://arxiv.org/abs/2401.14424)

    通过结合MCTS和生成式预训练模型，我们提出了一种新的符号回归算法SR-GPT，在发现数据中的数学公式方面取得了显著的改进。

    

    在科学研究和人工智能中，找到一个简洁且可解释的数学公式来准确描述数据中每个变量与预测值之间的关系是一个关键任务，也是一个重大挑战。这个问题被称为符号回归，是一个NP困难问题。去年，提出了一种基于蒙特卡洛树搜索（MCTS）的符号回归方法，并在多个数据集上获得了sota。虽然与以前的方法相比，该算法在恢复目标表达式方面显示出了相当大的改进，但是在MCTS过程中缺乏引导严重阻碍了其搜索效率。最近，一些算法在MCTS的搜索中添加了一个预训练的策略网络，但是这个预训练的策略网络的泛化能力很差。为了平衡效率和通用性，我们提出了SR-GPT，结合了AlphaZero的思想。SR-GPT是一种新的符号回归算法，将MCTS与一个通用性较好的生成式预训练模型相结合。

    Finding a concise and interpretable mathematical formula that accurately describes the relationship between each variable and the predicted value in the data is a crucial task in scientific research, as well as a significant challenge in artificial intelligence. This problem is referred to as symbolic regression, which is an NP-hard problem. Last year, a symbolic regression method based on Monte Carlo Tree Search (MCTS) was proposed and sota was obtained on multiple datasets. While this algorithm has shown considerable improvement in recovering target expressions compared to previous methods, the lack of guidance during the MCTS process severely hampers its search efficiency. Recently, some algorithms have added a pre-trained policy network to guide the search of MCTS, but the pre-trained policy network generalizes poorly. To balance efficiency and generality, we propose SR-GPT combining ideas from AlphaZero. SR-GPT is a new symbolic regression algorithm that combines MCTS with a Gener
    
[^60]: 自适应移动操作在开放环境中的可关节物体研究

    Adaptive Mobile Manipulation for Articulated Objects In the Open World. (arXiv:2401.14403v1 [cs.RO])

    [http://arxiv.org/abs/2401.14403](http://arxiv.org/abs/2401.14403)

    本文介绍了一种针对开放环境中关节物体操作的全栈方法，机器人通过自适应学习框架从少量数据中学习，并通过在线实践学习适应训练分布之外的新对象。同时，还开发了低成本的移动操作硬件平台。

    

    在开放的无结构环境中部署机器人一直是一个长期存在的问题。然而，机器人通常只在封闭的实验室环境中进行研究，之前的移动操作工作也仅限于拾取、移动、放置，这在这一领域中只是冰山一角。在本文中，我们引入了开放世界移动操作系统，采用全栈方法来解决现实世界中可关节物体的操作，例如真实世界中的门、柜子、抽屉和冰箱。机器人利用自适应学习框架，通过行为克隆先从一小组数据中学习，然后通过在线实践学习来处理训练分布之外的新对象。我们还开发了一个低成本的移动操作硬件平台，能够在无结构环境中进行安全和自主的在线适应，成本约为20,000美元。在我们的实验中，我们使用了20个可关节的物体。

    Deploying robots in open-ended unstructured environments such as homes has been a long-standing research problem. However, robots are often studied only in closed-off lab settings, and prior mobile manipulation work is restricted to pick-move-place, which is arguably just the tip of the iceberg in this area. In this paper, we introduce Open-World Mobile Manipulation System, a full-stack approach to tackle realistic articulated object operation, e.g. real-world doors, cabinets, drawers, and refrigerators in open-ended unstructured environments. The robot utilizes an adaptive learning framework to initially learns from a small set of data through behavior cloning, followed by learning from online practice on novel objects that fall outside the training distribution. We also develop a low-cost mobile manipulation hardware platform capable of safe and autonomous online adaptation in unstructured environments with a cost of around 20,000 USD. In our experiments we utilize 20 articulate obje
    
[^61]: 基于得分结构先验的部分已知高斯图模型估计

    Estimation of partially known Gaussian graphical models with score-based structural priors. (arXiv:2401.14340v1 [stat.ML])

    [http://arxiv.org/abs/2401.14340](http://arxiv.org/abs/2401.14340)

    本论文提出了一种基于得分结构先验的算法，用于估计部分已知高斯图模型。通过使用图神经网络来估计图的得分函数，我们可以在生成样本时利用退火朗格维能扩散，从而更准确地估计后验分布。数值实验表明，我们的方法具有明显的优势。

    

    我们提出了一种新的算法，用于支持估计部分已知的高斯图模型，并且结合了关于底层图的先验信息。与传统方法相比，传统方法使用点估计方法基于最大似然或最大后验准则，并使用（简单的）精度矩阵先验来提供点估计。我们考虑对图进行先验，并依赖退火朗格维能扩散从后验分布中生成样本。由于朗格维能采样器需要访问底层图先验的得分函数，因此我们使用图神经网络来有效地从图数据集（事先可用或从已知分布生成）估计得分。数值实验证明了我们方法的优势。

    We propose a novel algorithm for the support estimation of partially known Gaussian graphical models that incorporates prior information about the underlying graph. In contrast to classical approaches that provide a point estimate based on a maximum likelihood or a maximum a posteriori criterion using (simple) priors on the precision matrix, we consider a prior on the graph and rely on annealed Langevin diffusion to generate samples from the posterior distribution. Since the Langevin sampler requires access to the score function of the underlying graph prior, we use graph neural networks to effectively estimate the score from a graph dataset (either available beforehand or generated from a known distribution). Numerical experiments demonstrate the benefits of our approach.
    
[^62]: 通过自适应权重聚类和服务器端蒸馏实现高效通信的联邦学习

    Communication-Efficient Federated Learning through Adaptive Weight Clustering and Server-Side Distillation. (arXiv:2401.14211v1 [cs.LG])

    [http://arxiv.org/abs/2401.14211](http://arxiv.org/abs/2401.14211)

    本论文提出了一种名为FedCompress的新方法，通过动态权重聚类和服务器端知识蒸馏的结合，实现了高效通信的联邦学习。该方法在降低通信成本的同时，能够学习到高度可泛化的模型。

    

    联邦学习是一种有望在保护数据隐私的同时，通过多个设备共同训练深度神经网络的技术。然而，由于训练过程中重复的服务器-客户端通信导致了过多的通信成本，这给联邦学习带来了困难。为了解决这个挑战，我们应用了模型压缩技术，例如稀疏化和权重聚类，然而这些技术通常需要修改底层的模型聚合方案或者涉及繁琐的超参数调整，后者不仅调整了模型的压缩率，还限制了模型在不断增长的数据上的持续改进潜力。在本文中，我们提出了一种新颖的方法FedCompress，它结合了动态权重聚类和服务器端知识蒸馏，以降低通信成本同时学习高度可泛化的模型。通过对多个公共数据集进行全面评估，我们证明了我们的方法相比于其他方法的有效性。

    Federated Learning (FL) is a promising technique for the collaborative training of deep neural networks across multiple devices while preserving data privacy. Despite its potential benefits, FL is hindered by excessive communication costs due to repeated server-client communication during training. To address this challenge, model compression techniques, such as sparsification and weight clustering are applied, which often require modifying the underlying model aggregation schemes or involve cumbersome hyperparameter tuning, with the latter not only adjusts the model's compression rate but also limits model's potential for continuous improvement over growing data. In this paper, we propose FedCompress, a novel approach that combines dynamic weight clustering and server-side knowledge distillation to reduce communication costs while learning highly generalizable models. Through a comprehensive evaluation on diverse public datasets, we demonstrate the efficacy of our approach compared to
    
[^63]: 网络化多智能体强化学习用于点对点能源交易

    Networked Multiagent Reinforcement Learning for Peer-to-Peer Energy Trading. (arXiv:2401.13947v1 [eess.SY])

    [http://arxiv.org/abs/2401.13947](http://arxiv.org/abs/2401.13947)

    本文提出了一个利用多智能体强化学习框架来实现点对点能源交易的方法，该方法帮助自动化消费者的竞标和管理，并解决了可再生能源零边际成本和物理约束的问题。

    

    利用分布式可再生能源和能量储存资源进行点对点能源交易被长期认为是提高能源系统弹性和可持续性的解决方案。然而，消费者和自给自足者（具有能源发电资源的人）缺乏进行重复点对点交易的专业知识，并且可再生能源的零边际成本在确定公平市场价格方面存在挑战。为了解决这些问题，我们提出了多智能体强化学习（MARL）框架，以帮助自动化消费者对太阳能光伏和能量储存资源的竞标和管理，在一种利用供需比的点对点清算机制下。此外，我们展示了MARL框架如何整合物理网络约束以实现电压控制，从而确保点对点能源交易的物理可行性，并为真实世界的实施铺平了道路。

    Utilizing distributed renewable and energy storage resources in local distribution networks via peer-to-peer (P2P) energy trading has long been touted as a solution to improve energy systems' resilience and sustainability. Consumers and prosumers (those who have energy generation resources), however, do not have the expertise to engage in repeated P2P trading, and the zero-marginal costs of renewables present challenges in determining fair market prices. To address these issues, we propose multi-agent reinforcement learning (MARL) frameworks to help automate consumers' bidding and management of their solar PV and energy storage resources, under a specific P2P clearing mechanism that utilizes the so-called supply-demand ratio. In addition, we show how the MARL frameworks can integrate physical network constraints to realize voltage control, hence ensuring physical feasibility of the P2P energy trading and paving way for real-world implementations.
    
[^64]: 利用声音克隆将NVIDIA的多语言TTS系统扩展到印度语言

    Scaling NVIDIA's multi-speaker multi-lingual TTS systems with voice cloning to Indic Languages. (arXiv:2401.13851v1 [cs.SD])

    [http://arxiv.org/abs/2401.13851](http://arxiv.org/abs/2401.13851)

    本文介绍了NVIDIA开发的TTS模型，利用RAD-MMM和P-Flow实现了多语言TTS的训练，其中P-Flow在零样本TTS方面表现出色，在2024挑战中获得了第一名。

    

    在本文中，我们描述了NVIDIA为MMITS-VC（多语言、多语种印度TTS与声音克隆）2024挑战开发的TTS模型。在1和2轨道中，我们利用RAD-MMM在目标说话人数据上进行了少样本TTS的训练。在第3轨道中，我们利用P-Flow进行了零样本TTS的训练，同时使用了挑战数据集和外部数据集。我们对所有提交使用HiFi-GAN vocoders。RAD-MMM在1和2轨道上表现出竞争力，而P-Flow在第3轨道上排名第一，平均意见分(MOS)为4.4，说话人相似度分数(SMOS)为3.62。

    In this paper, we describe the TTS models developed by NVIDIA for the MMITS-VC (Multi-speaker, Multi-lingual Indic TTS with Voice Cloning) 2024 Challenge. In Tracks 1 and 2, we utilize RAD-MMM to perform few-shot TTS by training additionally on 5 minutes of target speaker data. In Track 3, we utilize P-Flow to perform zero-shot TTS by training on the challenge dataset as well as external datasets. We use HiFi-GAN vocoders for all submissions. RAD-MMM performs competitively on Tracks 1 and 2, while P-Flow ranks first on Track 3, with mean opinion score (MOS) 4.4 and speaker similarity score (SMOS) of 3.62.
    
[^65]: 研究大型语言模型在代码克隆检测方面的功效

    Investigating the Efficacy of Large Language Models for Code Clone Detection. (arXiv:2401.13802v1 [cs.SE])

    [http://arxiv.org/abs/2401.13802](http://arxiv.org/abs/2401.13802)

    这项研究探索了大型语言模型在代码克隆检测任务中的应用。

    

    大型语言模型（LLMs）在各种自然语言处理和软件工程任务中表现出了显著的成功，例如代码生成。LLMs主要在基于提示的零/少样本范式中被用于指导模型完成任务。本研究探索了LLMs在代码克隆检测（CCD）这一非生成任务中的适用性。

    Large Language Models (LLMs) have demonstrated remarkable success in various natural language processing and software engineering tasks, such as code generation. The LLMs are mainly utilized in the prompt-based zero/few-shot paradigm to guide the model in accomplishing the task. %\textbf{Goal:} GPT-based models are one of the popular ones studied for tasks such as code comment generation or test generation. These tasks are `generative' tasks. However, there is limited research on the usage of LLMs for `non-generative' tasks such as classification using the prompt-based paradigm. In this preliminary exploratory study, we investigated the applicability of LLMs for Code Clone Detection (CCD), a non-generative task. %\textbf{Method:} By building a mono-lingual and cross-lingual CCD dataset derived from CodeNet, we first investigated two different prompts using ChatGPT to detect \textcolor{black}{Type-4} code clones in Java-Java and Java-Ruby pairs in a zero-shot setting. We \textcolor{blac
    
[^66]: 相关随机向量的检测

    Detection of Correlated Random Vectors. (arXiv:2401.13429v1 [cs.IT])

    [http://arxiv.org/abs/2401.13429](http://arxiv.org/abs/2401.13429)

    本文研究了判断两个标准正态随机向量是否相关的问题，提出了一种新的方法来评估似然比的二阶矩，并发现了与整数分割函数之间的联系。

    

    在本文中，我们研究了判断两个标准正态随机向量$\mathsf{X}\in\mathbb{R}^{n}$和$\mathsf{Y}\in\mathbb{R}^{n}$是否相关的问题。这被表述为一个假设检验问题，在零假设下，这些向量是统计独立的，而在备择假设下，$\mathsf{X}$和随机均匀置换的$\mathsf{Y}$是具有相关系数$\rho$的。我们分析了信息论上不可能和可能的最优测试阈值，作为$n$和$\rho$的函数。为了得出我们的信息论下界，我们开发了一种利用正交多项式展开来评估似然比的二阶矩的新技术，该技术揭示了与整数分割函数之间的一个令人惊讶的联系。我们还研究了上述设置的多维泛化，其中我们观察到两个数据库/矩阵，而不是两个向量。

    In this paper, we investigate the problem of deciding whether two standard normal random vectors $\mathsf{X}\in\mathbb{R}^{n}$ and $\mathsf{Y}\in\mathbb{R}^{n}$ are correlated or not. This is formulated as a hypothesis testing problem, where under the null hypothesis, these vectors are statistically independent, while under the alternative, $\mathsf{X}$ and a randomly and uniformly permuted version of $\mathsf{Y}$, are correlated with correlation $\rho$. We analyze the thresholds at which optimal testing is information-theoretically impossible and possible, as a function of $n$ and $\rho$. To derive our information-theoretic lower bounds, we develop a novel technique for evaluating the second moment of the likelihood ratio using an orthogonal polynomials expansion, which among other things, reveals a surprising connection to integer partition functions. We also study a multi-dimensional generalization of the above setting, where rather than two vectors we observe two databases/matrices
    
[^67]: DittoGym:学习控制软形变机器人

    DittoGym: Learning to Control Soft Shape-Shifting Robots. (arXiv:2401.13231v1 [cs.RO] CROSS LISTED)

    [http://arxiv.org/abs/2401.13231](http://arxiv.org/abs/2401.13231)

    这篇论文介绍了一种学习控制软形变机器人的方法，并且提出了一个全面的强化学习基准系统DittoGym，该系统需要对机器人的形态进行细粒度变化来完成任务。

    

    机器人共同设计，其中机器人的形态优化与学习的策略共同解决特定任务，是一个新兴的研究领域。对于软机器人来说，这一领域具有特别的潜力，因为软机器人可以通过新颖的制造技术实现学习到的形态和执行器。受自然界和最近的新型机器人设计的启发，我们提出更进一步探索新型可重构机器人，即在其寿命内可以改变形态的机器人。我们将可重构软机器人的控制形式化为高维强化学习问题。我们在同一action空间中统一形态变化、运动和与环境的互动，并引入合适的粗到细的课程表，使我们能够发现实现对最终机器人进行细粒度控制的策略。我们还介绍了DittoGym，这是一个针对可重构软机器人的全面强化学习基准，需要对形态进行细粒度变化来完成任务。

    Robot co-design, where the morphology of a robot is optimized jointly with a learned policy to solve a specific task, is an emerging area of research. It holds particular promise for soft robots, which are amenable to novel manufacturing techniques that can realize learned morphologies and actuators. Inspired by nature and recent novel robot designs, we propose to go a step further and explore the novel reconfigurable robots, defined as robots that can change their morphology within their lifetime. We formalize control of reconfigurable soft robots as a high-dimensional reinforcement learning (RL) problem. We unify morphology change, locomotion, and environment interaction in the same action space, and introduce an appropriate, coarse-to-fine curriculum that enables us to discover policies that accomplish fine-grained control of the resulting robots. We also introduce DittoGym, a comprehensive RL benchmark for reconfigurable soft robots that require fine-grained morphology changes to a
    
[^68]: 通过重力信息驱动的深度学习框架预测非本地物种船舶交通流量和入侵风险

    Gravity-Informed Deep Learning Framework for Predicting Ship Traffic Flow and Invasion Risk of Non-Indigenous Species via Ballast Water Discharge. (arXiv:2401.13098v1 [cs.LG])

    [http://arxiv.org/abs/2401.13098](http://arxiv.org/abs/2401.13098)

    通过考虑航运通量密度、港口距离、贸易流量和交通枢纽的中心性指标等因素，本研究开发了一个受物理启发的模型来预测海事航运流量，并用于指导全球交通网络中入侵物种的风险评估和管理。

    

    水体中的入侵物种对全球环境和生物多样性构成了重大威胁。由于交通和贸易增加，非本土物种已经引入了新的环境，导致生态系统破坏，并导致农业、林业和渔业方面的经济损失。因此，迫切需要风险评估和管理技术以减轻这些入侵的影响。本研究旨在开发一种新的受物理启发的模型，用于预测海事航运交通流量，并以此指导通过全球交通网络传播的入侵物种风险评估。受国际贸易重力模型的启发，我们的模型考虑了影响船舶活动可能性和影响的各种因素，如航运通量密度、港口之间的距离、贸易流量和交通枢纽的中心性指标。此外，通过分析入侵物种的风险网络，我们为评估和管理入侵提供了全面的框架。

    Invasive species in water bodies pose a major threat to the environment and biodiversity globally. Due to increased transportation and trade, non-native species have been introduced to new environments, causing damage to ecosystems and leading to economic losses in agriculture, forestry, and fisheries. Therefore, there is a pressing need for risk assessment and management techniques to mitigate the impact of these invasions. This study aims to develop a new physics-inspired model to forecast maritime shipping traffic and thus inform risk assessment of invasive species spread through global transportation networks. Inspired by the gravity model for international trades, our model considers various factors that influence the likelihood and impact of vessel activities, such as shipping flux density, distance between ports, trade flow, and centrality measures of transportation hubs. Additionally, by analyzing the risk network of invasive species, we provide a comprehensive framework for as
    
[^69]: 在线学习世界模型的局部敏感稀疏编码

    Locality Sensitive Sparse Encoding for Learning World Models Online. (arXiv:2401.13034v1 [cs.LG])

    [http://arxiv.org/abs/2401.13034](http://arxiv.org/abs/2401.13034)

    本文提出了一种基于局部敏感稀疏编码的线性回归模型，通过非线性随机特征实现对复杂环境的拟合。这种模型能够高效地进行稀疏更新，实现了优化拟合先前经验的Follow-The-Leader（FTL）世界模型。

    

    为了解决神经网络在在线学习中遇到的数据非平稳性问题，本文提出了一种基于局部敏感稀疏编码的线性回归模型，该模型通过非线性随机特征实现了对复杂环境的拟合。通过引入局部敏感稀疏编码，我们能够进行高效的稀疏更新，在平衡模型容量和计算效率的同时实现优化拟合所有先前经验的Follow-The-Leader（FTL）世界模型。

    Acquiring an accurate world model online for model-based reinforcement learning (MBRL) is challenging due to data nonstationarity, which typically causes catastrophic forgetting for neural networks (NNs). From the online learning perspective, a Follow-The-Leader (FTL) world model is desirable, which optimally fits all previous experiences at each round. Unfortunately, NN-based models need re-training on all accumulated data at every interaction step to achieve FTL, which is computationally expensive for lifelong agents. In this paper, we revisit models that can achieve FTL with incremental updates. Specifically, our world model is a linear regression model supported by nonlinear random features. The linear part ensures efficient FTL update while the nonlinear random feature empowers the fitting of complex environments. To best trade off model capacity and computation efficiency, we introduce a locality sensitive sparse encoding, which allows us to conduct efficient sparse updates even 
    
[^70]: 用于解决一些随机最优控制问题的深度多任务神经网络

    Deep multitask neural networks for solving some stochastic optimal control problems. (arXiv:2401.12923v1 [stat.ML])

    [http://arxiv.org/abs/2401.12923](http://arxiv.org/abs/2401.12923)

    本文针对某些难以模拟底层状态变量的随机最优控制问题，引入了使用多任务神经网络的有效解决方案，并通过实验证明了该方法的优越性。

    

    大多数现有的基于神经网络的方法用于使用相关的反向动态规划原理解决随机最优控制问题，这些方法依赖于模拟底层状态变量的能力。然而，在某些问题中，这种模拟是不可行的，导致状态变量空间的离散化和需要为每个数据点训练一个神经网络。当处理大的状态变量空间时，这种方法在计算上变得低效。在本文中，我们考虑了一类这种类型的随机最优控制问题，并引入了一种使用多任务神经网络的有效解决方案。为了训练我们的多任务神经网络，我们引入了一种新的方案，在任务之间动态平衡学习。通过对真实世界的衍生品定价问题进行数值实验，我们证明了我们的方法优于最先进的方法。

    Most existing neural network-based approaches for solving stochastic optimal control problems using the associated backward dynamic programming principle rely on the ability to simulate the underlying state variables. However, in some problems, this simulation is infeasible, leading to the discretization of state variable space and the need to train one neural network for each data point. This approach becomes computationally inefficient when dealing with large state variable spaces. In this paper, we consider a class of this type of stochastic optimal control problems and introduce an effective solution employing multitask neural networks. To train our multitask neural network, we introduce a novel scheme that dynamically balances the learning across tasks. Through numerical experiments on real-world derivatives pricing problems, we prove that our method outperforms state-of-the-art approaches.
    
[^71]: 通过生成合成数据和比例类平衡技术提高小目标的物体检测性能：在工业场景中的比较研究

    Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios. (arXiv:2401.12729v1 [cs.CV])

    [http://arxiv.org/abs/2401.12729](http://arxiv.org/abs/2401.12729)

    通过生成合成数据和比例类平衡技术，提高了针对小目标的物体检测性能。这项研究解决了工业场景中收集和注释小目标数据的难题，并讨论了比例类平衡技术的效果。

    

    目标检测在提取局部类别信息方面被证明是一种重要的计算机视觉方法，并在工业中有多种应用。尽管许多最先进的目标检测模型在中等和大型目标上表现良好，但它们在小目标上表现不足。在大多数工业应用场景中，收集和注释小目标数据是困难的，因为这需要耗费时间且容易出现人为错误。此外，这些数据集往往不平衡，经常导致模型收敛效果不佳。为了解决这个挑战，本研究提出了一种新颖的方法，通过注入额外的数据点来改善目标检测模型的性能。使用合成数据生成技术，可以最小化收集和注释小目标数据点的困难，并创建一个具有平衡分布的数据集。本文讨论了一种简单的比例类平衡技术的效果，以实现模型的有效收敛。

    Object Detection (OD) has proven to be a significant computer vision method in extracting localized class information and has multiple applications in the industry. Although many of the state-of-the-art (SOTA) OD models perform well on medium and large sized objects, they seem to under perform on small objects. In most of the industrial use cases, it is difficult to collect and annotate data for small objects, as it is time-consuming and prone to human errors. Additionally, those datasets are likely to be unbalanced and often result in an inefficient model convergence. To tackle this challenge, this study presents a novel approach that injects additional data points to improve the performance of the OD models. Using synthetic data generation, the difficulties in data collection and annotations for small object data points can be minimized and to create a dataset with balanced distribution. This paper discusses the effects of a simple proportional class-balancing technique, to enable be
    
[^72]: NeuroSynt：一种用于反应合成的神经符号组合求解器

    NeuroSynt: A Neuro-symbolic Portfolio Solver for Reactive Synthesis. (arXiv:2401.12131v2 [cs.LO] UPDATED)

    [http://arxiv.org/abs/2401.12131](http://arxiv.org/abs/2401.12131)

    NeuroSynt是一个用于反应合成的神经符号组合求解器，它通过将神经和符号方法无缝集成来解决问题。NeuroSynt在处理具有挑战性的规范方面表现出色，为当前SYNTCOMP基准提供了新的解决方案。

    

    我们介绍了一种名为NeuroSynt的神经符号组合求解器框架，用于反应合成。求解器的核心是神经和符号方法在解决反应合成问题时的无缝集成。为了确保正确性，神经引擎与验证底层神经模型预测的模型检查器相结合。NeuroSynt的开源实现提供了一个集成框架，可以无缝集成新的神经和最新的符号方法进行反应合成。大量实验证明了其在处理具有挑战性的规范方面的有效性，提升了最新的反应合成求解器，并为当前SYNTCOMP基准提供了新的解决方案。

    We introduce NeuroSynt, a neuro-symbolic portfolio solver framework for reactive synthesis. At the core of the solver lies a seamless integration of neural and symbolic approaches to solving the reactive synthesis problem. To ensure soundness, the neural engine is coupled with model checkers verifying the predictions of the underlying neural models. The open-source implementation of NeuroSynt provides an integration framework for reactive synthesis in which new neural and state-of-the-art symbolic approaches can be seamlessly integrated. Extensive experiments demonstrate its efficacy in handling challenging specifications, enhancing the state-of-the-art reactive synthesis solvers, with NeuroSynt contributing novel solves in the current SYNTCOMP benchmarks.
    
[^73]: TurboSVM-FL: 通过SVM聚合为懒惰客户端增强联邦学习

    TurboSVM-FL: Boosting Federated Learning through SVM Aggregation for Lazy Clients. (arXiv:2401.12012v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.12012](http://arxiv.org/abs/2401.12012)

    TurboSVM-FL是一种新颖的联邦聚合策略，通过SVM聚合为懒惰客户端增强联邦学习。这种策略在不增加客户端计算负担的情况下解决了联邦学习中的收敛速度慢的问题。

    

    联邦学习是一种分布式协作机器学习范例，在近年来获得了强烈的推动力。在联邦学习中，中央服务器定期通过客户端协调模型，并聚合由客户端在本地训练的模型，而无需访问本地数据。尽管具有潜力，但联邦学习的实施仍然面临一些挑战，主要是由于数据异质性导致的收敛速度慢。收敛速度慢在跨设备联邦学习场景中尤为问题，其中客户端可能受到计算能力和存储空间的严重限制，因此对客户端产生额外计算或内存负担的方法，如辅助目标项和更大的训练迭代次数，可能不实际。在本文中，我们提出了一种新颖的联邦聚合策略TurboSVM-FL，它不会给客户端增加额外的计算负担

    Federated learning is a distributed collaborative machine learning paradigm that has gained strong momentum in recent years. In federated learning, a central server periodically coordinates models with clients and aggregates the models trained locally by clients without necessitating access to local data. Despite its potential, the implementation of federated learning continues to encounter several challenges, predominantly the slow convergence that is largely due to data heterogeneity. The slow convergence becomes particularly problematic in cross-device federated learning scenarios where clients may be strongly limited by computing power and storage space, and hence counteracting methods that induce additional computation or memory cost on the client side such as auxiliary objective terms and larger training iterations can be impractical. In this paper, we propose a novel federated aggregation strategy, TurboSVM-FL, that poses no additional computation burden on the client side and c
    
[^74]: Tensor视图拓扑图神经网络

    Tensor-view Topological Graph Neural Network. (arXiv:2401.12007v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.12007](http://arxiv.org/abs/2401.12007)

    提出了一种新颖的Tensor视图拓扑图神经网络（TTG-NN），该方法结合了持久同调、图卷积和张量运算，同时捕捉了局部和全局层面上的Tensor视图拓扑（TT）和Tensor视图图（TG）结构信息。

    

    图分类是一项重要的图结构数据学习任务。图神经网络（GNNs）近年来在图学习中引起了越来越多的关注，并在许多重要的图问题上显示出显著的改进。尽管现有的GNNs在性能上处于最前沿，但它们只使用了每个节点周围非常有限的邻域的局部信息，导致了多模态信息的丢失和过多计算的开销。为了解决这些问题，我们提出了一种新颖的Tensor视图拓扑图神经网络（TTG-NN），这是一种简单而有效的基于持久同调、图卷积和张量运算的拓扑深度学习方法。这种新方法同时捕捉了局部和全局层面上的Tensor视图拓扑（TT）和Tensor视图图（TG）结构信息，并在计算上充分利用了图的拓扑和结构。

    Graph classification is an important learning task for graph-structured data. Graph neural networks (GNNs) have recently gained growing attention in graph learning and have shown significant improvements in many important graph problems. Despite their state-of-the-art performances, existing GNNs only use local information from a very limited neighborhood around each node, suffering from loss of multi-modal information and overheads of excessive computation. To address these issues, we propose a novel Tensor-view Topological Graph Neural Network (TTG-NN), a class of simple yet effective topological deep learning built upon persistent homology, graph convolution, and tensor operations. This new method incorporates tensor learning to simultaneously capture Tensor-view Topological (TT), as well as Tensor-view Graph (TG) structural information on both local and global levels. Computationally, to fully exploit graph topology and structure, we propose two flexible TT and TG representation lea
    
[^75]: 空间-时间图卷积网络在交通预测上的知识蒸馏

    Knowledge Distillation on Spatial-Temporal Graph Convolutional Network for Traffic Prediction. (arXiv:2401.11798v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.11798](http://arxiv.org/abs/2401.11798)

    本论文研究了在交通预测中应用空间-时间图卷积网络和知识蒸馏的方法。知识蒸馏的思想能够实现在减少参数和保持准确性的同时提高执行效率。通过引入教师网络的空间-时间相关性，我们的方法能够使学生网络学习到复杂的交通模式。

    

    高效实时交通预测对减少交通时间至关重要。为了预测交通状况，我们采用了空间-时间图神经网络（ST-GNN）将实时交通数据建模为时间图。尽管ST-GNN具有强大的能力，但在为实际交通数据进行高效实时预测时经常面临挑战。鉴于实时数据动态性的重要性，我们采用知识蒸馏（KD）作为解决方案，以提高ST-GNN在交通预测中的执行时间。本文介绍了一个成本函数，旨在使用复杂网络（教师）的蒸馏数据来训练具有较少参数的网络（学生），同时保持其准确性接近教师的准确性。我们使用知识蒸馏，将教师网络的空间-时间相关性融入学生网络，使学生能够学习到教师感知的复杂模式。然而，面临一个挑战。

    Efficient real-time traffic prediction is crucial for reducing transportation time. To predict traffic conditions, we employ a spatio-temporal graph neural network (ST-GNN) to model our real-time traffic data as temporal graphs. Despite its capabilities, it often encounters challenges in delivering efficient real-time predictions for real-world traffic data. Recognizing the significance of timely prediction due to the dynamic nature of real-time data, we employ knowledge distillation (KD) as a solution to enhance the execution time of ST-GNNs for traffic prediction. In this paper, We introduce a cost function designed to train a network with fewer parameters (the student) using distilled data from a complex network (the teacher) while maintaining its accuracy close to that of the teacher. We use knowledge distillation, incorporating spatial-temporal correlations from the teacher network to enable the student to learn the complex patterns perceived by the teacher. However, a challenge a
    
[^76]: 安全且广义的端到端自主驾驶系统：基于强化学习和示范的研究

    Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations. (arXiv:2401.11792v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2401.11792](http://arxiv.org/abs/2401.11792)

    本文介绍了一种安全且广义的端到端自主驾驶系统 (SGADS)，使用强化学习和示范相结合的方法解决了现有方法的低安全性、泛化能力差和采样效率低的问题，同时引入了变分推理和归一化流以准确预测驾驶轨迹，并提出了鲁棒性安全约束的制定方法。

    

    一个智能驾驶系统应该能够根据当前环境和车辆状态动态制定适当的驾驶策略，同时确保系统的安全性和可靠性。然而，基于强化学习和模仿学习的现有方法存在安全性低、泛化能力差和采样效率低的问题。此外，它们无法准确预测未来的驾驶轨迹，而准确预测未来的驾驶轨迹是做出最优决策的前提。为了解决这些问题，本文引入了一种复杂而多样场景下的安全且广义的端到端自主驾驶系统 (SGADS)。我们的SGADS与变分推理和归一化流结合，使智能车辆能够准确预测未来的驾驶轨迹。此外，我们提出了鲁棒性安全约束的制定。此外，我们将强化学习与示范相结合进行增强学习。

    An intelligent driving system should be capable of dynamically formulating appropriate driving strategies based on the current environment and vehicle status, while ensuring the security and reliability of the system. However, existing methods based on reinforcement learning and imitation learning suffer from low safety, poor generalization, and inefficient sampling. Additionally, they cannot accurately predict future driving trajectories, and the accurate prediction of future driving trajectories is a precondition for making optimal decisions. To solve these problems, in this paper, we introduce a Safe and Generalized end-to-end Autonomous Driving System (SGADS) for complex and various scenarios. Our SGADS incorporates variational inference with normalizing flows, enabling the intelligent vehicle to accurately predict future driving trajectories. Moreover, we propose the formulation of robust safety constraints. Furthermore, we combine reinforcement learning with demonstrations to aug
    
[^77]: 通过具有分层正则化的医学代码中心的多模态对比EHR建模预测下次就诊诊断

    Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal Contrastive EHR Modelling with Hierarchical Regularisation. (arXiv:2401.11648v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.11648](http://arxiv.org/abs/2401.11648)

    通过医学代码中心的多模态对比EHR建模预测下次就诊诊断，并通过分层正则化提高性能。

    

    在医疗保健中，利用电子健康记录（EHR）预测下次就诊的诊断是一项必要的任务，对于制定医疗保健提供者和患者的主动未来计划至关重要。然而，之前的许多研究并没有充分解决EHR数据固有的异构和分层特征，必然导致次优的性能。为此，我们提出了NECHO，一种新颖的医学代码中心的多模态对比EHR学习框架，其中包括分层正则化。首先，我们使用定制的网络设计和一对双模态对比损失融合涵盖医学代码、人口统计数据和临床笔记的多方面信息，所有这些都围绕着医学代码表现。我们还使用医学本体中的父级信息来规范特定模态的编码器，以学习EHR数据的层次结构。对MIMIC-III数据进行的一系列实验证明了我们方法的有效性。

    Predicting next visit diagnosis using Electronic Health Records (EHR) is an essential task in healthcare, critical for devising proactive future plans for both healthcare providers and patients. Nonetheless, many preceding studies have not sufficiently addressed the heterogeneous and hierarchical characteristics inherent in EHR data, inevitably leading to sub-optimal performance. To this end, we propose NECHO, a novel medical code-centric multimodal contrastive EHR learning framework with hierarchical regularisation. First, we integrate multifaceted information encompassing medical codes, demographics, and clinical notes using a tailored network design and a pair of bimodal contrastive losses, all of which pivot around a medical code representation. We also regularise modality-specific encoders using a parental level information in medical ontology to learn hierarchical structure of EHR data. A series of experiments on MIMIC-III data demonstrates effectiveness of our approach.
    
[^78]: SPAND: 使用网络动态的睡眠预测架构

    SPAND: Sleep Prediction Architecture using Network Dynamics. (arXiv:2401.11113v1 [cs.LG])

    [http://arxiv.org/abs/2401.11113](http://arxiv.org/abs/2401.11113)

    SPAND是一个利用网络动态的睡眠预测架构，可以通过图网络和移动设备数据来预测下一天的睡眠持续时间标签。

    

    睡眠行为对健康有重大影响，对身心健康的指示也非常重要。利用普遍存在的传感器监测和预测睡眠行为，可以帮助管理睡眠并追踪相关健康状况。虽然睡眠行为取决于个体的生理状况，但也受到数字媒体使用、社交网络传染以及周围天气等外部因素的影响。在本研究中，我们提出了SPAND（Sleep Prediction Architecture using Network Dynamics），这是一个利用图网络中的社交传染来预测睡眠行为的系统，并将其与从普遍存在的移动设备和可穿戴设备中提取的生理和手机数据集成，以预测下一天的睡眠持续时间标签。我们的架构通过设计一种注意机制，克服了包含与睡眠行为无关的连接的大规模图形的局限性。广泛的实验评估突显出该系统的性能。

    Sleep behavior significantly impacts health and acts as an indicator of physical and mental well-being. Monitoring and predicting sleep behavior with ubiquitous sensors may therefore assist in both sleep management and tracking of related health conditions. While sleep behavior depends on, and is reflected in the physiology of a person, it is also impacted by external factors such as digital media usage, social network contagion, and the surrounding weather. In this work, we propose SPAND (Sleep Prediction Architecture using Network Dynamics), a system that exploits social contagion in sleep behavior through graph networks and integrates it with physiological and phone data extracted from ubiquitous mobile and wearable devices for predicting next-day sleep labels about sleep duration. Our architecture overcomes the limitations of large-scale graphs containing connections irrelevant to sleep behavior by devising an attention mechanism. The extensive experimental evaluation highlights th
    
[^79]: 使用基于机器学习的稀事件算法估计AMOC转换概率

    Estimation of AMOC transition probabilities using a machine learning based rare-event algorithm. (arXiv:2401.10800v1 [physics.ao-ph])

    [http://arxiv.org/abs/2401.10800](http://arxiv.org/abs/2401.10800)

    本研究通过结合TAMS和Next-Generation Reservoir Computing技术，利用稀事件算法估计来源于数据的确定函数，来计算大西洋经度翻转环流（AMOC）在指定时间窗口内崩溃的概率。

    

    大西洋经度翻转环流（AMOC）是全球气候的重要组成部分，被认为是一个临界因素，可以在全球变暖下崩溃。本研究的主要目标是使用一种稀事件算法（Trajectory-Adaptive Multilevel Splitting，TAMS）计算AMOC在指定时间窗口内崩溃的概率。然而，TAMS的效率和准确性取决于得分函数的选择。虽然已知最佳得分函数的定义，称为“确定函数”，但通常无法先验地计算它。在这里，我们将TAMS与下一代水库计算技术相结合，该技术可以从稀事件算法生成的数据中估计确定函数。我们在AMOC的随机盒模型中测试了这种技术，该模型存在两种转变类型，称为F（快速）转变和S（缓慢）转变。F转变的结果与那些进行了有利比较。

    The Atlantic Meridional Overturning Circulation (AMOC) is an important component of the global climate, known to be a tipping element, as it could collapse under global warming. The main objective of this study is to compute the probability that the AMOC collapses within a specified time window, using a rare-event algorithm called Trajectory-Adaptive Multilevel Splitting (TAMS). However, the efficiency and accuracy of TAMS depend on the choice of the score function. Although the definition of the optimal score function, called ``committor function" is known, it is impossible in general to compute it a priori. Here, we combine TAMS with a Next-Generation Reservoir Computing technique that estimates the committor function from the data generated by the rare-event algorithm. We test this technique in a stochastic box model of the AMOC for which two types of transition exist, the so-called F(ast)-transitions and S(low)-transitions. Results for the F-transtions compare favorably with those 
    
[^80]: BioDiffusion：用于生物医学信号合成的多功能扩散模型

    BioDiffusion: A Versatile Diffusion Model for Biomedical Signal Synthesis. (arXiv:2401.10282v1 [eess.SP])

    [http://arxiv.org/abs/2401.10282](http://arxiv.org/abs/2401.10282)

    BioDiffusion是一种用于生物医学信号合成的多功能扩散模型，能够产生高保真度、非稳态的多变量信号。通过利用这些合成的信号，可以有效解决生物医学信号机器学习任务中的数据不足、数据不平衡和标签复杂性等问题，提高准确性。

    

    生物医学信号的机器学习任务通常面临有限的数据可用性、不平衡的数据集、标签复杂性和测量噪声的干扰等问题。这些挑战经常阻碍机器学习算法的最佳训练。为了解决这些问题，我们引入了BioDiffusion，这是一种针对合成多变量生物医学信号进行优化的基于扩散的概率模型。BioDiffusion在产生高保真度、非稳态的多变量信号方面表现出色，可用于无条件、标签条件和信号条件生成等多个任务。利用这些合成的信号为上述挑战提供了一个显著的解决方案。我们的研究包括对合成数据质量进行的定性和定量评估，强调其在与生物医学信号相关的机器学习任务中提高准确性的能力。此外，与当前主流的时间系信息生成模型相比，BioDiffusion在质量和效率方面表现出更好的性能。

    Machine learning tasks involving biomedical signals frequently grapple with issues such as limited data availability, imbalanced datasets, labeling complexities, and the interference of measurement noise. These challenges often hinder the optimal training of machine learning algorithms. Addressing these concerns, we introduce BioDiffusion, a diffusion-based probabilistic model optimized for the synthesis of multivariate biomedical signals. BioDiffusion demonstrates excellence in producing high-fidelity, non-stationary, multivariate signals for a range of tasks including unconditional, label-conditional, and signal-conditional generation. Leveraging these synthesized signals offers a notable solution to the aforementioned challenges. Our research encompasses both qualitative and quantitative assessments of the synthesized data quality, underscoring its capacity to bolster accuracy in machine learning tasks tied to biomedical signals. Furthermore, when juxtaposed with current leading tim
    
[^81]: 通过对比性和局部稀疏扰动解释时间序列

    Explaining Time Series via Contrastive and Locally Sparse Perturbations. (arXiv:2401.08552v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.08552](http://arxiv.org/abs/2401.08552)

    这篇论文提出了一个局部稀疏模型ContraLSP，通过引入对立样本和对比学习来解释时间序列。实证研究表明，ContraLSP在解释时间序列数据的质量上取得了实质性的改进。

    

    解释多元时间序列是一个复合挑战，因为它需要识别时间序列中的重要位置并匹配复杂的时间模式。虽然之前基于显著性的方法解决了这些挑战，但它们的扰动可能无法减轻分布偏移问题，尤其是在异质样本中。我们提出了ContraLSP，这是一个局部稀疏模型，引入对立样本来构建无信息的扰动，但使用对比学习保持分布。此外，我们还结合了样本特定的稀疏门，生成更多二进制偏斜和平滑的遮罩，可以轻松综合时间趋势并简洁地选择显著特征。对合成数据集和真实世界数据集的实证研究表明，ContraLSP优于现有模型，在解释时间序列数据的质量上取得了实质性的改进。源代码可在\url{https://github.com/zichuan-liu/ContraL}找到。

    Explaining multivariate time series is a compound challenge, as it requires identifying important locations in the time series and matching complex temporal patterns. Although previous saliency-based methods addressed the challenges, their perturbation may not alleviate the distribution shift issue, which is inevitable especially in heterogeneous samples. We present ContraLSP, a locally sparse model that introduces counterfactual samples to build uninformative perturbations but keeps distribution using contrastive learning. Furthermore, we incorporate sample-specific sparse gates to generate more binary-skewed and smooth masks, which easily integrate temporal trends and select the salient features parsimoniously. Empirical studies on both synthetic and real-world datasets show that ContraLSP outperforms state-of-the-art models, demonstrating a substantial improvement in explanation quality for time series data. The source code is available at \url{https://github.com/zichuan-liu/ContraL
    
[^82]: 加强扩散相关光谱学中的血流评估：一种带有噪声鲁棒性分析的迁移学习方法

    Enhancing Blood Flow Assessment in Diffuse Correlation Spectroscopy: A Transfer Learning Approach with Noise Robustness Analysis. (arXiv:2401.05580v1 [cs.LG])

    [http://arxiv.org/abs/2401.05580](http://arxiv.org/abs/2401.05580)

    本研究提出了一种迁移学习方法，用于增强扩散相关光谱学中的血流评估，并通过噪声鲁棒性分析展示了其鲁棒性。

    

    扩散相关光谱学（DCS）是一种新兴的非侵入性技术，通过使用近红外相干点光源照射来检测光谱变化来测量组织血流。尽管机器学习已经显示出测量血流指数（BFi）的巨大潜力，但一个有关该方法成功与否的问题是其在涉及不同信噪比（SNR）和各种不同临床应用和设置的数据集之间的偏差方面的鲁棒性。本研究提出了一种迁移学习方法，旨在评估SNR对学习特征的泛化能力的影响，并展示迁移学习的鲁棒性。使用不同添加噪声水平的合成数据集来模拟不同的SNR。所提出的网络以1x64的自相关曲线为输入，并生成BFi和相关参数beta。所提出的模型表现出极好的性能。

    Diffuse correlation spectroscopy (DCS) is an emerging noninvasive technique that measures the tissue blood flow, by using near-infrared coherent point-source illumination to detect spectral changes. While machine learning has demonstrated significant potential for measuring blood flow index (BFi), an open question concerning the success of this approach pertains to its robustness in scenarios involving deviations between datasets with varying Signal-to-Noise Ratios (SNRs) originating from diverse clinical applications and various setups. This study proposes a transfer learning approach, aims to assess the influence of SNRs on the generalization ability of learned features, and demonstrate the robustness for transfer learning. A synthetic dataset with varying levels of added noise is utilized to simulate different SNRs. The proposed network takes a 1x64 autocorrelation curve as input and generates BFi and the correlation parameter beta. The proposed model demonstrates excellent performa
    
[^83]: 通过极限学习机快速分析脑血流

    Fast Cerebral Blood Flow Analysis via Extreme Learning Machine. (arXiv:2401.05578v1 [cs.LG])

    [http://arxiv.org/abs/2401.05578](http://arxiv.org/abs/2401.05578)

    本论文提出了一种通过极限学习机快速精确分析脑血流的方法，并通过与现有算法的综合比较验证了其优越性。它展示了强大的泛化能力，在各种噪声和光学参数下都具有更高的准确性。同时，与计算效率高的神经网络相比，该方法具有较短的训练和推理时间。这种策略适用于在线训练的边缘计算应用。

    

    我们引入一种快速精确的分析方法，利用扩散相关光谱学（DCS）和极限学习机（ELM）来分析脑血流（CBF）。我们评估了ELM和现有算法，并使用综合指标对这些算法进行了比较。我们使用合成数据集对半无穷和多层模型进行了评估。结果表明，在各种噪声水平和光学参数下，ELM始终具有更高的准确性，展示了强大的泛化能力，并优于迭代拟合算法。通过与计算效率高的神经网络进行比较，ELM获得了可比较的准确性，同时减少了训练和推理时间。值得注意的是，在ELM的训练过程中，没有反向传播过程，导致训练速度比现有的神经网络方法更快。这种提出的策略在在线训练的边缘计算应用中具有潜力。

    We introduce a rapid and precise analytical approach for analyzing cerebral blood flow (CBF) using Diffuse Correlation Spectroscopy (DCS) with the application of the Extreme Learning Machine (ELM). Our evaluation of ELM and existing algorithms involves a comprehensive set of metrics. We assess these algorithms using synthetic datasets for both semi-infinite and multi-layer models. The results demonstrate that ELM consistently achieves higher fidelity across various noise levels and optical parameters, showcasing robust generalization ability and outperforming iterative fitting algorithms. Through a comparison with a computationally efficient neural network, ELM attains comparable accuracy with reduced training and inference times. Notably, the absence of a back-propagation process in ELM during training results in significantly faster training speeds compared to existing neural network approaches. This proposed strategy holds promise for edge computing applications with online training
    
[^84]: 用于预测半自磨磨机产量的改进遗传编程

    An improved genetic programming for predicting semi autogenous grinding mill throughput. (arXiv:2401.05382v1 [cs.NE])

    [http://arxiv.org/abs/2401.05382](http://arxiv.org/abs/2401.05382)

    该论文介绍了一种改进的遗传编程方法，应用于预测半自磨磨机的产量。新的遗传编程变体可以提取多个方程式，从而精确预测不同训练数据群集的产量。

    

    半自磨磨机在矿物处理厂的磨矿回路中起着至关重要的作用。准确预测半自磨磨机的产量是一个关键性能指标。尽管以前的研究中已经开发出了经验模型来预测半自磨磨机的产量，但是应用机器学习技术来做此预测的潜力仍未得到充分发挥。与依赖昂贵且耗时的实验数据的经验建模不同，机器学习技术可以利用在正常运行期间收集的数据。遗传编程（Genetic Programming，GP）是一种机器学习技术，其优势在于能够提供一个透明的方程式来精确预测磨机的产量。本研究探讨了将GP应用于预测半自磨磨机产量，并引入了五种新的GP变种来提高预测性能。这些变种提取了多个方程式，每个方程式可以准确预测特定训练数据群集的产量。

    Semi-autogenous grinding (SAG) mills play a pivotal role in the grinding circuit of mineral processing plants. Accurate prediction of SAG mill throughput as a crucial performance metric is of utmost importance. While empirical models have been developed in previous studies for SAG mill throughput prediction, the potential of applying machine learning (ML) techniques for this purpose remains underexplored. Unlike empirical modelling, which relies on expensive and time-consuming experimental data, ML techniques can utilize data collected during regular operations. Genetic programming (GP) is one of ML techniques that offers the advantage of providing a transparent equation for precise mill throughput prediction. This study explores the application of GP to predict SAG mill throughput and introduces five new GP variants to enhance prediction performance. These variants extract multiple equations, each accurately predicting mill throughput for specific clusters of training data. These equa
    
[^85]: 通过多级域对齐实现通用的睡眠分期

    Generalizable Sleep Staging via Multi-level Domain Alignment. (arXiv:2401.05363v1 [eess.SP])

    [http://arxiv.org/abs/2401.05363](http://arxiv.org/abs/2401.05363)

    本文提出了一种通用的睡眠分期方法，通过引入域泛化概念，结合多级特征对齐的思想，提高了模型对未见过数据集的泛化能力。

    

    自动睡眠分期对于睡眠评估和疾病诊断至关重要。现有的大多数方法依赖于特定数据集，并且仅适用于相同数据集的未见过的数据集。本文引入了域泛化概念到自动睡眠分期中，并提出了通用的睡眠分期任务，旨在提高模型对未见过的数据集的泛化能力。受到现有的域泛化方法的启发，我们采用特征对齐思想，提出了一种名为SleepDG的框架来解决该问题。考虑到局部显著特征和时序特征对于睡眠分期都很重要，我们提出了一种多级特征对齐，将时代级和序列级特征对齐来学习域不变特征表示。具体而言，我们设计了一种时代级特征对齐方法，对不同睡眠时代的特征分布进行对齐。

    Automatic sleep staging is essential for sleep assessment and disorder diagnosis. Most existing methods depend on one specific dataset and are limited to be generalized to other unseen datasets, for which the training data and testing data are from the same dataset. In this paper, we introduce domain generalization into automatic sleep staging and propose the task of generalizable sleep staging which aims to improve the model generalization ability to unseen datasets. Inspired by existing domain generalization methods, we adopt the feature alignment idea and propose a framework called SleepDG to solve it. Considering both of local salient features and sequential features are important for sleep staging, we propose a Multi-level Feature Alignment combining epoch-level and sequence-level feature alignment to learn domain-invariant feature representations. Specifically, we design an Epoch-level Feature Alignment to align the feature distribution of each single sleep epoch among different 
    
[^86]: 一个好的评分并不会导致一个好的生成模型

    A Good Score Does not Lead to A Good Generative Model. (arXiv:2401.04856v1 [cs.LG])

    [http://arxiv.org/abs/2401.04856](http://arxiv.org/abs/2401.04856)

    本文通过反例证明，在某些情况下，即使评分函数学习良好，基于评分的生成模型（SGMs）仍然无法生成接近真实数据分布的样本，并且只能产生训练数据点的高斯模糊样本。

    

    基于评分的生成模型（SGMs）是生成建模中的一种主要方法，以其能够从复杂的高维数据分布中生成高质量样本而闻名。该方法在经验上取得了成功，并且有着严格的理论收敛性质的支持。特别是已经证明，如果学习到的底层评分函数良好，SGMs能够生成接近真实数据分布的样本，这表明了SGM作为生成模型的成功之处。本文提供了一个反例。通过样本复杂度的分析，我们提供了一个特定的设置，其中评分函数学习得很好。然而，在这个设置中，SGMs只能输出训练数据点的高斯模糊样本，模拟核密度估计的效果。这一发现与最近的一系列发现相一致，揭示了SGMs可能表现出强大的记忆效应并且无法生成样本的问题。

    Score-based Generative Models (SGMs) is one leading method in generative modeling, renowned for their ability to generate high-quality samples from complex, high-dimensional data distributions. The method enjoys empirical success and is supported by rigorous theoretical convergence properties. In particular, it has been shown that SGMs can generate samples from a distribution that is close to the ground-truth if the underlying score function is learned well, suggesting the success of SGM as a generative model. We provide a counter-example in this paper. Through the sample complexity argument, we provide one specific setting where the score function is learned well. Yet, SGMs in this setting can only output samples that are Gaussian blurring of training data points, mimicking the effects of kernel density estimation. The finding resonates a series of recent finding that reveal that SGMs can demonstrate strong memorization effect and fail to generate.
    
[^87]: 在训练过的神经网络上优化：进行一次轻松的漫步

    Optimization Over Trained Neural Networks: Taking a Relaxing Walk. (arXiv:2401.03451v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2401.03451](http://arxiv.org/abs/2401.03451)

    本文提出了一种更可扩展的启发式方法来优化训练过的神经网络，通过探索全局和局部线性松弛来产生更好的解决方案。

    

    除了训练之外，数学优化也被用于深度学习中，用于建模和解决训练过的神经网络的问题，如验证、压缩和带学习约束的优化。然而，随着网络规模的增长，由于弱线性松弛和密集约束矩阵，求解这些问题很快变得困难。近年来，我们通过割平面算法、重述和基于混合整数线性规划（MILP）的启发式方法取得了改进。在这项工作中，我们提出了一种基于探索全局和局部线性松弛的神经网络模型的更可扩展的启发式方法。我们的启发式方法在输入、深度和神经元数量增加时，与最先进的MILP求解器和先前的启发式方法相竞争，同时产生更好的解决方案。

    Besides training, mathematical optimization is also used in deep learning to model and solve formulations over trained neural networks for purposes such as verification, compression, and optimization with learned constraints. However, solving these formulations soon becomes difficult as the network size grows due to the weak linear relaxation and dense constraint matrix. We have seen improvements in recent years with cutting plane algorithms, reformulations, and an heuristic based on Mixed-Integer Linear Programming (MILP). In this work, we propose a more scalable heuristic based on exploring global and local linear relaxations of the neural network model. Our heuristic is competitive with a state-of-the-art MILP solver and the prior heuristic while producing better solutions with increases in input, depth, and number of neurons.
    
[^88]: 一个量子启发的用于几何建模的神经网络

    A quatum inspired neural network for geometric modeling. (arXiv:2401.01801v1 [cs.LG])

    [http://arxiv.org/abs/2401.01801](http://arxiv.org/abs/2401.01801)

    这个论文介绍了一种创新的矩阵乘积态(MPS)的消息传递策略，通过这种策略可以更好地捕捉几何图中的复杂关系。

    

    通过将物理系统构想为3D多体点云，几何图神经网络(GNN)，如SE(3)/E(3)等效GNN，展示了良好的性能。特别是，它们高效的消息传递机制使它们能够熟练地对分子和晶体材料进行建模。然而，当前的几何GNN只提供了多体系统的平均场近似，封装在两体消息传递中，因此在捕捉这些几何图中的复杂关系方面有所欠缺。为了解决这个局限性，计算物理学中广泛使用的高阶张量来处理多体系统的张量网络被引入。然而，将这些张量化网络整合到GNN的消息传递框架中面临着可扩展性和对称性保持（如置换和旋转）的挑战。作为回应，我们引入了一种创新的等变矩阵乘积态(MPS)的消息传递策略，通过实现一个

    By conceiving physical systems as 3D many-body point clouds, geometric graph neural networks (GNNs), such as SE(3)/E(3) equivalent GNNs, have showcased promising performance. In particular, their effective message-passing mechanics make them adept at modeling molecules and crystalline materials. However, current geometric GNNs only offer a mean-field approximation of the many-body system, encapsulated within two-body message passing, thus falling short in capturing intricate relationships within these geometric graphs. To address this limitation, tensor networks, widely employed by computational physics to handle manybody systems using high-order tensors, have been introduced. Nevertheless, integrating these tensorized networks into the message-passing framework of GNNs faces scalability and symmetry conservation (e.g., permutation and rotation) challenges. In response, we introduce an innovative equivariant Matrix Product State (MPS)-based message-passing strategy, through achieving a
    
[^89]: Backstepping神经操作员用于$2\times 2$双曲PDEs

    Backstepping Neural Operators for $2\times 2$ Hyperbolic PDEs. (arXiv:2312.16762v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2312.16762](http://arxiv.org/abs/2312.16762)

    本文介绍了一种用于$2\times 2$双曲PDE的Backstepping神经操作员方法。通过考虑耦合的Goursat形式PDE，并建立了从植被PDE功能系数到核PDE解的映射的连续性，证明了DeepONet逼近核PDE解的存在性

    

    深度神经网络逼近非线性操作员，通常被称为DeepONet，在单个Goursat形式的PDE控制单个反馈增益函数的PDE反向设计中已被证明具有能力。在耦合PDE的边界控制中，耦合的Goursat形式PDE控制两个或多个增益核 - 这是迄今为止DeepONet未解决的PDE结构。在本文中，我们通过考虑一个简单的逆向传播$2\times 2$耦合系统来打开超卷积$2\times 2$核PDE系统的近似主题，其控制中出现了Goursat形式的PDE系统。应用包括石油钻井、浅水波的Saint-Venant模型以及密集交通流中的Aw-Rascle-Zhang模型的停车和行驶不稳定性。在本文中，我们建立了从（总共五个）植被PDE的功能系数到核PDE解的映射的连续性，证明了DeepONet逼近核PDE解的存在性

    Deep neural network approximation of nonlinear operators, commonly referred to as DeepONet, has proven capable of approximating PDE backstepping designs in which a single Goursat-form PDE governs a single feedback gain function. In boundary control of coupled PDEs, coupled Goursat-form PDEs govern two or more gain kernels -- a PDE structure unaddressed thus far with DeepONet. In this note, we open the subject of approximating systems of gain kernel PDEs for hyperbolic PDE plants by considering a simple counter-convecting $2\times 2$ coupled system in whose control a $2\times 2$ kernel PDE systems in Goursat form arises. Applications include oil drilling, Saint-Venant model of shallow water waves, and Aw-Rascle-Zhang model of stop-and-go instability in congested traffic flow. In this paper we establish the continuity of the mapping from (a total of five) plant PDE functional coefficients to the kernel PDE solutions, prove the existence of an arbitrarily close DeepONet approximation to t
    
[^90]: 联邦学习中受效率限制的效用-隐私双目标优化的理论分析

    A Theoretical Analysis of Efficiency Constrained Utility-Privacy Bi-Objective Optimization in Federated Learning. (arXiv:2312.16554v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.16554](http://arxiv.org/abs/2312.16554)

    本文从理论上分析了联邦学习中受效率限制的效用-隐私双目标优化。先前的研究主要关注效用-隐私的权衡，忽视了训练效率和其他影响因素。该研究对差分隐私联邦学习中的关键问题进行了系统分析。

    

    联邦学习（FL）使多个客户端在不共享个体数据的情况下协同学习共享模型。FL中的效用、隐私和训练效率问题已引起重要的研究关注。差分隐私已成为FL中一种主流技术，保护个体用户数据的隐私同时影响效用和训练效率。在差分隐私联邦学习（DPFL）中，先前的研究主要关注效用-隐私的权衡，而忽视了及时完成所必需的训练效率。此外，差分隐私通过在每轮通信中对选定的客户端引入受控的随机性（噪声）来实现隐私保护。先前的工作主要研究了噪声水平（$\sigma$）和通信轮数（$T$）对隐私-效用动态的影响，但忽视了其他影响因素，如样本比例（$q$，即选定客户端的比例）。

    Federated learning (FL) enables multiple clients to collaboratively learn a shared model without sharing their individual data. Concerns about utility, privacy, and training efficiency in FL have garnered significant research attention. Differential privacy has emerged as a prevalent technique in FL, safeguarding the privacy of individual user data while impacting utility and training efficiency. Within Differential Privacy Federated Learning (DPFL), previous studies have primarily focused on the utility-privacy trade-off, neglecting training efficiency, which is crucial for timely completion. Moreover, differential privacy achieves privacy by introducing controlled randomness (noise) on selected clients in each communication round. Previous work has mainly examined the impact of noise level ($\sigma$) and communication rounds ($T$) on the privacy-utility dynamic, overlooking other influential factors like the sample ratio ($q$, the proportion of selected clients). This paper systemati
    
[^91]: 多智能体强化学习的上下文感知通信

    Context-aware Communication for Multi-agent Reinforcement Learning. (arXiv:2312.15600v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.15600](http://arxiv.org/abs/2312.15600)

    这项研究针对多智能体强化学习提出了一种上下文感知的通信方案，通过两个阶段的交流，使智能体能够发送个性化的消息，从而提高合作和团队性能。

    

    对于多智能体强化学习（MARL），有效的通信协议对于促进合作和提高团队性能至关重要。为了利用通信，许多以前的工作提出将本地信息压缩成一条消息并广播给所有可达的智能体。然而，这种简单的消息传递机制可能无法为个体智能体提供足够、关键和相关的信息，特别是在带宽严重有限的场景下。这激励我们为MARL开发上下文感知的通信方案，旨在向不同的智能体发送个性化的消息。我们的通信协议名为CACOM，由两个阶段组成。第一个阶段中，智能体以广播方式交换粗略表示，为第二个阶段提供上下文信息。紧随其后，智能体在第二个阶段中利用注意机制为接收者选择性生成个性化的消息。此外，我们还采用了学习的步长量化方法。

    Effective communication protocols in multi-agent reinforcement learning (MARL) are critical to fostering cooperation and enhancing team performance. To leverage communication, many previous works have proposed to compress local information into a single message and broadcast it to all reachable agents. This simplistic messaging mechanism, however, may fail to provide adequate, critical, and relevant information to individual agents, especially in severely bandwidth-limited scenarios. This motivates us to develop context-aware communication schemes for MARL, aiming to deliver personalized messages to different agents. Our communication protocol, named CACOM, consists of two stages. In the first stage, agents exchange coarse representations in a broadcast fashion, providing context for the second stage. Following this, agents utilize attention mechanisms in the second stage to selectively generate messages personalized for the receivers. Furthermore, we employ the learned step size quant
    
[^92]: 数据嵌入在等变量量子卷积神经网络中的作用

    The role of data embedding in equivariant quantum convolutional neural networks. (arXiv:2312.13250v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2312.13250](http://arxiv.org/abs/2312.13250)

    本论文研究了数据嵌入对等变量量子卷积神经网络的性能的影响。通过分析数据嵌入方法与对称群表示之间的关系以及不同表示对网络可表达性的影响，我们发现分类准确率与嵌入方法有明确的相关性。

    

    几何深度学习是指利用数据集的对称性来约束神经网络的参数空间，从而提高其可训练性和泛化能力。最近，这个思想已经被引入到量子机器学习领域，形成了等变量量子神经网络（EQNNs）。在本工作中，我们研究了经典到量子嵌入对等变量量子卷积神经网络（EQCNNs）在图像分类中的性能的影响。我们讨论了数据嵌入方法与对称群表示之间的联系，并分析改变表示对EQCNN可表达性的影响。我们通过数值比较EQCNN与三种不同基础排列幅度嵌入的分类准确率，与非等变量量子卷积神经网络（QCNN）的结果进行对比。我们的结果表明分类准确率与嵌入方法有明确的相关性。

    Geometric deep learning refers to the scenario in which the symmetries of a dataset are used to constrain the parameter space of a neural network and thus, improve their trainability and generalization. Recently this idea has been incorporated into the field of quantum machine learning, which has given rise to equivariant quantum neural networks (EQNNs). In this work, we investigate the role of classical-to-quantum embedding on the performance of equivariant quantum convolutional neural networks (EQCNNs) for the classification of images. We discuss the connection between the data embedding method and the resulting representation of a symmetry group and analyze how changing representation affects the expressibility of an EQCNN. We numerically compare the classification accuracy of EQCNNs with three different basis-permuted amplitude embeddings to the one obtained from a non-equivariant quantum convolutional neural network (QCNN). Our results show a clear dependence of classification acc
    
[^93]: 强化图转换器与正则化关注分数

    Stronger Graph Transformer with Regularized Attention Scores. (arXiv:2312.11730v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.11730](http://arxiv.org/abs/2312.11730)

    本论文提出了一种新颖的边缘正则化技术版本，用于缓解图神经网络在内存问题上存在的困扰。与没有位置编码的Graph Transformer相比，应用了边缘正则化技术确实可以稳定地提高性能。

    

    图神经网络以其内存消耗大而臭名昭著。最近发现，基于Transformer的GNN称为Graph Transformer在存在长程依赖性时可以获得更好的性能。然而，将图数据和Transformer架构相结合导致了记忆问题更加严重。我们提出了一种新颖的“边缘正则化技术”的版本，可以减轻对位置编码的需求，从而减轻GT的内存溢出问题。我们观察到，不清楚在位置编码的基础上是否有边缘正则化是有帮助的。然而，显然，应用我们的边缘正则化技术确实可以稳定地改善GT的性能，相比于没有位置编码的GT。

    Graph Neural Networks are notorious for its memory consumption. A recent Transformer-based GNN called Graph Transformer is shown to obtain superior performances when long range dependencies exist. However, combining graph data and Transformer architecture led to a combinationally worse memory issue. We propose a novel version of "edge regularization technique" that alleviates the need for Positional Encoding and ultimately alleviate GT's out of memory issue. We observe that it is not clear whether having an edge regularization on top of positional encoding is helpful. However, it seems evident that applying our edge regularization technique indeed stably improves GT's performance compared to GT without Positional Encoding.
    
[^94]: 时间变换器：融合本地和全局特征以实现更好的时间序列生成

    Time-Transformer: Integrating Local and Global Features for Better Time Series Generation. (arXiv:2312.11714v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.11714](http://arxiv.org/abs/2312.11714)

    本文提出了一种新的时间序列生成模型，通过时间变换器同时学习本地和全局特征，实现了对时间序列数据的更好生成能力。

    

    生成时间序列数据是解决数据不足问题的一种有前景的方法。然而，由于时间序列数据的复杂时间特性，包括本地相关性和全局依赖性，使其成为具有挑战性的任务。大多数现有的生成模型未能有效学习时间序列数据的本地和全局特性。为了解决这个问题，我们提出了一种新颖的时间序列生成模型，命名为'时间变换器AAE'，它由一个对抗性自动编码器（AAE）和一个名为'时间变换器'的新设计架构组成。时间变换器首先通过层次并行设计同时学习本地和全局特征，结合了时间卷积网络和Transformer的能力，分别提取本地特征和全局依赖性。其次，提出了一个双向交叉注意力来在两个分支之间提供互补的引导，并实现本地特征和全局特征的合适融合。

    Generating time series data is a promising approach to address data deficiency problems. However, it is also challenging due to the complex temporal properties of time series data, including local correlations as well as global dependencies. Most existing generative models have failed to effectively learn both the local and global properties of time series data. To address this open problem, we propose a novel time series generative model named 'Time-Transformer AAE', which consists of an adversarial autoencoder (AAE) and a newly designed architecture named 'Time-Transformer' within the decoder. The Time-Transformer first simultaneously learns local and global features in a layer-wise parallel design, combining the abilities of Temporal Convolutional Networks and Transformer in extracting local features and global dependencies respectively. Second, a bidirectional cross attention is proposed to provide complementary guidance across the two branches and achieve proper fusion between loc
    
[^95]: 人类反馈的迭代偏好学习：在KL约束下将理论与实践联系起来的RLHF

    Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint. (arXiv:2312.11456v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.11456](http://arxiv.org/abs/2312.11456)

    该论文研究了在KL约束下的反馈强化学习的理论框架，并提出了有效的算法和实践。实证评估表明，该框架在大型语言模型的对齐实验中表现出良好的效果。

    

    本文研究了生成模型与强化学习从人类反馈中的对齐过程的理论框架。我们考虑了一个标准的数学表达式，即反向KL正则化的上下文多臂赌博机用于RLHF。尽管它被广泛应用于实际应用，但对这个公式的严格理论分析仍然很开放。我们研究了它在离线、在线和混合三种不同场景下的行为，并提出了具有有限样本理论保证的高效算法。朝着实际应用的方向，我们的框架通过对信息理论策略改进预言的稳健近似，自然地产生了几种新颖的RLHF算法。这包括在线场景中的迭代版本的直接偏好优化(DPO)算法，以及离线情景下的多步拒绝抽样策略。我们对大型语言模型的真实对齐实验进行了实证评估。

    This paper studies the theoretical framework of the alignment process of generative models with Reinforcement Learning from Human Feedback (RLHF). We consider a standard mathematical formulation, the reverse-KL regularized contextual bandit for RLHF. Despite its widespread practical application, a rigorous theoretical analysis of this formulation remains open. We investigate its behavior in three distinct settings -- offline, online, and hybrid -- and propose efficient algorithms with finite-sample theoretical guarantees.  Moving towards practical applications, our framework, with a robust approximation of the information-theoretical policy improvement oracle, naturally gives rise to several novel RLHF algorithms. This includes an iterative version of the Direct Preference Optimization (DPO) algorithm for online settings, and a multi-step rejection sampling strategy for offline scenarios. Our empirical evaluations on real-world alignment experiment of large language model demonstrate t
    
[^96]: 揭示用于真实测试时适应的批次归一化方法

    Unraveling Batch Normalization for Realistic Test-Time Adaptation. (arXiv:2312.09486v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.09486](http://arxiv.org/abs/2312.09486)

    本文研究了测试时领域适应的问题，通过揭示批次归一化的内部机制，并介绍了测试时指数移动平均（TEMA）方法来弥补训练和测试批次之间的类别多样性差距，从而提高了准确的目标估计。

    

    尽管最近的测试时适应方法通过调整批次归一化来减小领域差异，但是当使用真实的小批量时，它们的有效性会降低，因为目标估计不准确。由于以前的尝试仅仅是引入源统计数据来缓解这个问题，因此目标估计不准确的基本问题仍然存在，使得测试时领域变化问题未解决。本文研究了小批量降级问题。通过揭示批次归一化的内部机制，我们发现不准确的目标统计主要来自于批次中类别多样性的大幅减少。根据这一发现，我们引入了一个直接的工具——测试时指数移动平均（TEMA），来弥补训练和测试批次之间类别多样性的差距。重要的是，我们的TEMA可适应地扩展了典型方法的范围，超越了当前批次的范围，以包含一个多样的类别信息集合，从而提高准确的目标估计。

    While recent test-time adaptations exhibit efficacy by adjusting batch normalization to narrow domain disparities, their effectiveness diminishes with realistic mini-batches due to inaccurate target estimation. As previous attempts merely introduce source statistics to mitigate this issue, the fundamental problem of inaccurate target estimation still persists, leaving the intrinsic test-time domain shifts unresolved. This paper delves into the problem of mini-batch degradation. By unraveling batch normalization, we discover that the inexact target statistics largely stem from the substantially reduced class diversity in batch. Drawing upon this insight, we introduce a straightforward tool, Test-time Exponential Moving Average (TEMA), to bridge the class diversity gap between training and testing batches. Importantly, our TEMA adaptively extends the scope of typical methods beyond the current batch to incorporate a diverse set of class information, which in turn boosts an accurate targe
    
[^97]: 进化沉积池用于元增强学习

    Evolving Reservoirs for Meta Reinforcement Learning. (arXiv:2312.06695v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.06695](http://arxiv.org/abs/2312.06695)

    本论文提出了一种进化沉积池的计算模型，用于研究动物适应环境的机制。这种模型基于元增强学习框架，通过演化和发展之间的相互作用，利用进化沉积池来加速和引导强化学习过程。

    

    动物在其一生中经常展示出对环境的适应能力，部分原因是由于形态和神经结构的演化。这些结构捕捉到了代际之间共享的环境特征，以加速和引导一生中的学习过程。在这项工作中，我们提出了一个计算模型来研究实现这一过程的机制。我们采用基于元增强学习的计算框架作为演化和发展之间相互作用的模型。在演化尺度上，我们演化沉积池，这是一族循环神经网络，与常规网络不同的是，我们优化的不是突触权重，而是控制结果网络架构的宏观级别属性的超参数。在发展尺度上，我们使用这些进化沉积池来促进通过强化学习来学习行为策略。在强化学习代理中，沉积池编码环境的信息以优化学习过程。

    Animals often demonstrate a remarkable ability to adapt to their environments during their lifetime. They do so partly due to the evolution of morphological and neural structures. These structures capture features of environments shared between generations to bias and speed up lifetime learning. In this work, we propose a computational model for studying a mechanism that can enable such a process. We adopt a computational framework based on meta reinforcement learning as a model of the interplay between evolution and development. At the evolutionary scale, we evolve reservoirs, a family of recurrent neural networks that differ from conventional networks in that one optimizes not the synaptic weights, but hyperparameters controlling macro-level properties of the resulting network architecture. At the developmental scale, we employ these evolved reservoirs to facilitate the learning of a behavioral policy through Reinforcement Learning (RL). Within an RL agent, a reservoir encodes the en
    
[^98]: 多分布学习的样本复杂度

    The sample complexity of multi-distribution learning. (arXiv:2312.04027v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.04027](http://arxiv.org/abs/2312.04027)

    本文解决了多分布学习的样本复杂度问题，并给出了匹配下界的样本复杂度算法。

    

    多分布学习将经典的PAC学习推广到处理来自多个分布的数据。给定一组$k$个数据分布和一个VC维度为$d$的假设类，目标是学习一个假设，使得在$k$个分布上的最大总体损失最小，误差不超过$\epsilon$。本文通过给出一个样本复杂度算法$\widetilde{O}((d+k)\epsilon^{-2}) \cdot (k/\epsilon)^{o(1)}$来解决多分布学习的样本复杂度问题。这个结果与下界相匹配，解决了Awasthi、Haghtalab和Zhao在COLT 2023中提出的开放问题 [AHZ23]。

    Multi-distribution learning generalizes the classic PAC learning to handle data coming from multiple distributions. Given a set of $k$ data distributions and a hypothesis class of VC dimension $d$, the goal is to learn a hypothesis that minimizes the maximum population loss over $k$ distributions, up to $\epsilon$ additive error. In this paper, we settle the sample complexity of multi-distribution learning by giving an algorithm of sample complexity $\widetilde{O}((d+k)\epsilon^{-2}) \cdot (k/\epsilon)^{o(1)}$. This matches the lower bound up to sub-polynomial factor and resolves the COLT 2023 open problem of Awasthi, Haghtalab and Zhao [AHZ23].
    
[^99]: 具有逻辑约束的自回归模型的伪语义损失

    A Pseudo-Semantic Loss for Autoregressive Models with Logical Constraints. (arXiv:2312.03905v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.03905](http://arxiv.org/abs/2312.03905)

    本论文提出了一种针对具有逻辑约束的自回归模型的伪语义损失方法，通过在模型输出的局部近似上优化约束的似然，提高了神经符号学习的效率和适用性。

    

    神经符号化人工智能（neuro-symbolic AI）填补了纯符号和神经学习方法之间的鸿沟。这通常需要在神经网络的输出分布方面最大化对符号约束的似然。这些输出分布通常被假设为完全因子化的。这限制了神经符号学习在更具表现力的自回归分布（例如transformers）中的适用性。在这样的分布下，甚至简单约束的概率似然的计算是#P-hard的。我们提出，不是试图将约束强加在整个输出分布上，而是在其随机的局部近似上这样做。更确切地说，我们在以模型样本为中心的基于伪似然的近似中优化约束的似然。我们的近似是因子化的，可以重用子问题的解决方案，这是高效计算神经符号损失的主要原则。此外，它是一个局部的，高保真度的似然近似。

    Neuro-symbolic AI bridges the gap between purely symbolic and neural approaches to learning. This often requires maximizing the likelihood of a symbolic constraint w.r.t the neural network's output distribution. Such output distributions are typically assumed to be fully-factorized. This limits the applicability of neuro-symbolic learning to the more expressive autoregressive distributions, e.g., transformers. Under such distributions, computing the likelihood of even simple constraints is #P-hard. Instead of attempting to enforce the constraint on the entire output distribution, we propose to do so on a random, local approximation thereof. More precisely, we optimize the likelihood of the constraint under a pseudolikelihood-based approximation centered around a model sample. Our approximation is factorized, allowing the reuse of solutions to sub-problems, a main tenet for efficiently computing neuro-symbolic losses. Moreover, it is a local, high-fidelity approximation of the likelihoo
    
[^100]: 多个任务预训练和图形提示的MultiGPrompt

    MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs. (arXiv:2312.03731v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.03731](http://arxiv.org/abs/2312.03731)

    本文提出了一种名为MultiGPrompt的多任务预训练和提示框架，用于在图形表示学习中提高鲁棒性和减少标注成本。

    

    图形可以固有地对Web上相互连接的对象进行建模，从而支持一系列Web应用，比如网络分析和内容推荐。最近，图神经网络（GNNs）已经成为图表示学习的主流技术。然而，在端到端监督框架中，它们的有效性与任务特定标签的可用性密切相关。为了减少标注成本并增强在少样本设置中的鲁棒性，基于自监督任务的预训练已经成为一种有前途的方法，而提示则被提出来进一步缩小预训练任务与下游任务之间的目标差距。虽然已经对基于提示的图形学习进行了初步的探索，但它们主要利用单个预训练任务，导致从预训练数据中可能学习的通用知识的子集受限。因此，在本文中，我们提出了一种新颖的多任务预训练和提示框架MultiGPrompt，用于进一步提高对图形的表示学习。

    Graphs can inherently model interconnected objects on the Web, thereby facilitating a series of Web applications, such as web analyzing and content recommendation. Recently, Graph Neural Networks (GNNs) have emerged as a mainstream technique for graph representation learning. However, their efficacy within an end-to-end supervised framework is significantly tied to the availabilityof task-specific labels. To mitigate labeling costs and enhance robustness in few-shot settings, pre-training on self-supervised tasks has emerged as a promising method, while prompting has been proposed to further narrow the objective gap between pretext and downstream tasks. Although there has been some initial exploration of prompt-based learning on graphs, they primarily leverage a single pretext task, resulting in a limited subset of general knowledge that could be learned from the pre-training data. Hence, in this paper, we propose MultiGPrompt, a novel multi-task pre-training and prompting framework to
    
[^101]: JAX中的自动函数微分

    Automatic Functional Differentiation in JAX. (arXiv:2311.18727v2 [cs.PL] UPDATED)

    [http://arxiv.org/abs/2311.18727](http://arxiv.org/abs/2311.18727)

    我们在JAX中扩展了自动微分功能，使其能够自动微分高阶函数，通过引入一组原始算子，我们实现了函数微分的线性化和转置规则，并展示了该工具在函数导数应用中的效果和简单性。

    

    我们扩展了JAX的功能，使其能够自动微分高阶函数（函数算子和算符）。通过将函数表示为数组的推广，我们无缝地使用JAX的现有原语系统来实现高阶函数。我们提出了一组原始算子，作为构造几种关键类型的函数算子的基本构建模块。对于每个引入的原始算子，我们推导并实现了线性化和转置规则，与JAX的前向和反向模式自动微分的内部协议保持一致。这个增强功能允许使用传统用于函数的相同语法进行函数微分。得到的函数梯度本身就是可以在python中调用的函数。我们通过一些应用展示了这个工具的效果和简单性，其中函数导数是不可或缺的。此工作的源代码已在https://github.com/sail-sg/autofd上发布。

    We extend JAX with the capability to automatically differentiate higher-order functions (functionals and operators). By representing functions as a generalization of arrays, we seamlessly use JAX's existing primitive system to implement higher-order functions. We present a set of primitive operators that serve as foundational building blocks for constructing several key types of functionals. For every introduced primitive operator, we derive and implement both linearization and transposition rules, aligning with JAX's internal protocols for forward and reverse mode automatic differentiation. This enhancement allows for functional differentiation in the same syntax traditionally use for functions. The resulting functional gradients are themselves functions ready to be invoked in python. We showcase this tool's efficacy and simplicity through applications where functional derivatives are indispensable. The source code of this work is released at https://github.com/sail-sg/autofd .
    
[^102]: 使用训练标签进行填充和通过标签填充进行分类

    Imputation using training labels and classification via label imputation. (arXiv:2311.16877v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.16877](http://arxiv.org/abs/2311.16877)

    本论文提出一种在填充缺失数据时将标签与输入堆叠的方法，能够显著提高填充效果，并同时填充标签和输入。该方法适用于各种类型的数据，且在实验证明具有有希望的准确性结果。

    

    在实际应用中，缺失数据是一个常见的问题。已经开发了各种填充方法来处理缺失数据。然而，尽管训练数据通常都有标签，但常见的填充方法通常只依赖于输入而忽略标签。在这项工作中，我们阐述了将标签堆叠到输入中可以显着提高输入的填充效果。此外，我们提出了一种分类策略，该策略将预测的测试标签初始化为缺失值，并将标签与输入堆叠在一起进行填充。这样可以同时填充标签和输入。而且，该技术能够处理具有缺失标签的训练数据，无需任何先前的填充，并且适用于连续型、分类型或混合型数据。实验证明在准确性方面取得了有希望的结果。

    Missing data is a common problem in practical settings. Various imputation methods have been developed to deal with missing data. However, even though the label is usually available in the training data, the common practice of imputation usually only relies on the input and ignores the label. In this work, we illustrate how stacking the label into the input can significantly improve the imputation of the input. In addition, we propose a classification strategy that initializes the predicted test label with missing values and stacks the label with the input for imputation. This allows imputing the label and the input at the same time. Also, the technique is capable of handling data training with missing labels without any prior imputation and is applicable to continuous, categorical, or mixed-type data. Experiments show promising results in terms of accuracy.
    
[^103]: 电力系统中动态故障特性评估

    Dynamic Fault Characteristics Evaluation in Power Grid. (arXiv:2311.16522v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.16522](http://arxiv.org/abs/2311.16522)

    该论文提出了一种在电力系统中进行故障检测的新方法，通过图神经网络识别故障节点，并利用前后时间段内节点的状态来辅助当前故障检测。实验证明该方法准确可靠，并提供了对故障节点传播的定性分析。

    

    为了增强运维的智能度，提出了一种在电力系统中进行故障检测的新方法。该方法基于图神经网络，通过专门的特征提取方法和知识图谱来识别故障节点。通过引入时间数据，该方法利用前后时间段内节点的状态来辅助当前故障检测。为了验证节点特征的有效性，还进行了每个节点输出特征的相关性分析。实验证明，该方法可以在仿真场景中准确地定位故障节点，并具有显著的准确性。此外，基于图神经网络的特征建模可以定性地考察故障如何在节点间传播，为分析故障节点提供了有价值的见解。

    To enhance the intelligence degree in operation and maintenance, a novel method for fault detection in power grids is proposed. The proposed GNN-based approach first identifies fault nodes through a specialized feature extraction method coupled with a knowledge graph. By incorporating temporal data, the method leverages the status of nodes from preceding and subsequent time periods to help current fault detection. To validate the effectiveness of the node features, a correlation analysis of the output features from each node was conducted. The results from experiments show that this method can accurately locate fault nodes in simulation scenarios with a remarkable accuracy. Additionally, the graph neural network based feature modeling allows for a qualitative examination of how faults spread across nodes, which provides valuable insights for analyzing fault nodes.
    
[^104]: 使用歌词自动确定新曲谱的节拍记号

    Automatic Time Signature Determination for New Scores Using Lyrics for Latent Rhythmic Structure. (arXiv:2311.15480v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.15480](http://arxiv.org/abs/2311.15480)

    本文提出了一种新颖的方法，通过仅使用歌词作为输入，自动生成适合歌词歌曲的节拍记号，并揭示潜在的节奏结构。

    

    最近对于人工智能生成内容(AIGC)的兴趣急剧增加。然而，尚未对音乐组成部分如节拍记号进行足够的研究，以制定新作品的算法确定方法，尤其是歌词歌曲。这可能是因为忽视了音乐细节，而音乐细节对于构建强大的框架至关重要。具体而言，节拍记号为歌曲的几乎所有方面(包括短语和音符)建立了基础的节奏结构。在本文中，我们提出了一种新颖的方法，仅使用歌词作为输入，自动生成适合歌词歌曲的节拍记号，并利用可解释的机器学习模型揭示潜在的节奏结构。具体而言，我们设计了多种与发现歌词模式和创建同时包含歌词、节奏和统计信息的新特征相关的方法。在这种方法中，

    There has recently been a sharp increase in interest in Artificial Intelligence-Generated Content (AIGC). Despite this, musical components such as time signatures have not been studied sufficiently to form an algorithmic determination approach for new compositions, especially lyrical songs. This is likely because of the neglect of musical details, which is critical for constructing a robust framework. Specifically, time signatures establish the fundamental rhythmic structure for almost all aspects of a song, including the phrases and notes. In this paper, we propose a novel approach that only uses lyrics as input to automatically generate a fitting time signature for lyrical songs and uncover the latent rhythmic structure utilizing explainable machine learning models. In particular, we devise multiple methods that are associated with discovering lyrical patterns and creating new features that simultaneously contain lyrical, rhythmic, and statistical information. In this approach, the b
    
[^105]: 通过渐进范数重新缩放实现指数级快速边界最大化

    Achieving Margin Maximization Exponentially Fast via Progressive Norm Rescaling. (arXiv:2311.14387v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.14387](http://arxiv.org/abs/2311.14387)

    通过PRGD算法，我们在分类线性可分数据时实现了指数级快速边界最大化，与现有的算法相比，取得了显著的改进。

    

    在这项工作中，我们研究了基于梯度的算法在分类线性可分数据时表现出的边界最大化偏差。我们对与（归一化的）梯度相关的速度场的特性进行了深入分析，重点关注它们在边界最大化中的作用。受到这个分析的启发，我们提出了一种名为渐进重新缩放梯度下降（PRGD）的新算法，并展示了PRGD可以以指数级快速增大边界。这与目前所有现有算法形成了鲜明对比，后者以缓慢的多项式速率最大化边界。具体而言，我们确定了数据分布的温和条件，在这些条件下，像梯度下降（GD）和归一化梯度下降（NGD）这样的现有算法在高效最大化边界时会出现失败。为了验证我们的理论发现，我们进行了合成和真实世界实验。值得注意的是，PRGD在提高泛化性能方面也表现出了潜力。

    In this work, we investigate the margin-maximization bias exhibited by gradient-based algorithms in classifying linearly separable data. We present an in-depth analysis of the specific properties of the velocity field associated with (normalized) gradients, focusing on their role in margin maximization. Inspired by this analysis, we propose a novel algorithm called Progressive Rescaling Gradient Descent (PRGD) and show that PRGD can maximize the margin at an {\em exponential rate}. This stands in stark contrast to all existing algorithms, which maximize the margin at a slow {\em polynomial rate}. Specifically, we identify mild conditions on data distribution under which existing algorithms such as gradient descent (GD) and normalized gradient descent (NGD) {\em provably fail} in maximizing the margin efficiently. To validate our theoretical findings, we present both synthetic and real-world experiments. Notably, PRGD also shows promise in enhancing the generalization performance when a
    
[^106]: 电力分配网络中的知识图谱构建

    Knowledge Graph Construction in Power Distribution Networks. (arXiv:2311.08724v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2311.08724](http://arxiv.org/abs/2311.08724)

    本文提出了一种在电力分配网络中构建知识图谱的方法，该方法利用实体特征，在分配网络的知识图谱和分配文本中进行匹配，通过实验证明了其在电力分配知识图谱构建任务中的高准确性。

    

    本文提出了一种在电力分配网络中构建知识图谱的方法。该方法利用实体特征，包括其语义、音韵和句法特征，在分配网络的知识图谱和分配文本中进行匹配。基于卷积神经网络的增强模型，用于有效地将分配文本实体与知识图谱中的实体匹配。通过在真实世界的电力分配场景中进行实验评估了该模型的有效性。结果表明，与基线方法相比，所提出的模型在链接各种实体类型方面表现出色，在电力分配知识图谱构建任务中具有很高的整体准确性。

    In this paper, we propose a method for knowledge graph construction in power distribution networks. This method leverages entity features, which involve their semantic, phonetic, and syntactic characteristics, in both the knowledge graph of distribution network and the dispatching texts. An enhanced model based on Convolutional Neural Network, is utilized for effectively matching dispatch text entities with those in the knowledge graph. The effectiveness of this model is evaluated through experiments in real-world power distribution dispatch scenarios. The results indicate that, compared with the baselines, the proposed model excels in linking a variety of entity types, demonstrating high overall accuracy in power distribution knowledge graph construction task.
    
[^107]: 使用含有半监督潜在过程的深度生成模型建模复杂疾病轨迹

    Modeling Complex Disease Trajectories using Deep Generative Models with Semi-Supervised Latent Processes. (arXiv:2311.08149v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.08149](http://arxiv.org/abs/2311.08149)

    本文提出了一种深度生成模型的时间序列方法，利用潜在时间过程来模拟复杂疾病轨迹。通过结合医学知识和半监督方法，该方法可以解释和全面分析疾病轨迹，并用于进一步的数据分析和临床假设测试。

    

    本文提出了一种使用潜在时间过程的深度生成时间序列方法，用于模拟和全面分析复杂疾病轨迹。我们旨在找到能够解释观察到的疾病轨迹的有意义的时间潜在表示，并以一种可解释和全面的方式进行分析。为了提高这些时间潜在过程的可解释性，我们开发了一种半监督方法，利用已建立的医学概念对潜在空间进行解缠。通过将生成方法与医学知识相结合，我们利用了发现疾病新方面的能力，同时将医学概念整合到模型中。我们展示了学得的时间潜在过程可以用于进一步的数据分析和临床假设测试，包括查找相似患者和将疾病聚类为新的亚型。此外，我们的方法还可以实现个性化的在线监测和预测多变量情况。

    In this paper, we propose a deep generative time series approach using latent temporal processes for modeling and holistically analyzing complex disease trajectories. We aim to find meaningful temporal latent representations of an underlying generative process that explain the observed disease trajectories in an interpretable and comprehensive way. To enhance the interpretability of these latent temporal processes, we develop a semi-supervised approach for disentangling the latent space using established medical concepts. By combining the generative approach with medical knowledge, we leverage the ability to discover novel aspects of the disease while integrating medical concepts into the model. We show that the learned temporal latent processes can be utilized for further data analysis and clinical hypothesis testing, including finding similar patients and clustering the disease into new sub-types. Moreover, our method enables personalized online monitoring and prediction of multivari
    
[^108]: 通信受限的贝叶斯主动知识蒸馏

    Communication-Constrained Bayesian Active Knowledge Distillation. (arXiv:2311.08053v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.08053](http://arxiv.org/abs/2311.08053)

    本研究提出了一种名为通信受限的贝叶斯主动知识蒸馏（CC-BAKD）的新协议，通过使用线性混合机制将贝叶斯主动学习与压缩相结合，解决了在学习者与教师之间进行通信时关于批次选择和批次编码的重要问题。

    

    传统的重传（ARQ）协议旨在确保接收方正确接收到发射方的所有分组。当发射方是一个学习者与一个教师进行通信时，这个目标与学习者的实际目标相冲突，学习者的目标是从教师那里获取最相关的标签信息。从主动学习的角度出发，本文解决以下关键协议设计问题：(i)主动批次选择：应该发送哪个批次的输入给教师以获取最有用的信息，从而减少通信轮次的数量？(ii)批次编码：是否可以组合数据点的批次以减少每个通信轮次所需的通信资源？具体而言，本研究引入了通信受限的贝叶斯主动知识蒸馏（CC-BAKD），这是一种通过线性混合机制将贝叶斯主动学习与压缩相结合的新型协议。

    Conventional retransmission (ARQ) protocols are designed with the goal of ensuring the correct reception of all the individual transmitter's packets at the receiver. When the transmitter is a learner communicating with a teacher, this goal is at odds with the actual aim of the learner, which is that of eliciting the most relevant label information from the teacher. Taking an active learning perspective, this paper addresses the following key protocol design questions: (i) Active batch selection: Which batch of inputs should be sent to the teacher to acquire the most useful information and thus reduce the number of required communication rounds? (ii) Batch encoding: Can batches of data points be combined to reduce the communication resources required at each communication round? Specifically, this work introduces Communication-Constrained Bayesian Active Knowledge Distillation (CC-BAKD), a novel protocol that integrates Bayesian active learning with compression via a linear mix-up mecha
    
[^109]: 语言模型通过代码对分类更好地检测漏洞

    Language Models are Better Bug Detector Through Code-Pair Classification. (arXiv:2311.07957v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2311.07957](http://arxiv.org/abs/2311.07957)

    本研究提出了一种代码对分类任务，通过给语言模型同时提供有错误和无错误版本的代码，实现更好的漏洞检测。研究结果表明，这个任务相比于给出代码片段并判断是否存在错误及其位置要容易得多。

    

    大型语言模型（LLMs）如GPT-3.5和CodeLlama是用于代码生成和理解的强大模型。对这些模型进行微调的计算成本高且需要一个大型标记数据集。与此相反，上下文学习技术使模型能够只使用少量示例来学习下游任务。最近，研究人员展示了上下文学习在漏洞检测和修复方面的良好表现。在本文中，我们提出了代码对分类任务，其中模型同时获取有错误和无错误版本的代码，并标识出有错误的版本。我们在真实世界的漏洞检测数据集和两个最强大的LLMs上评估了我们的任务。实验结果表明，LLM通常可以从代码的非错误版本中选择出错误版本，并且代码对分类任务相比于给出代码片段并决定是否存在错误及其位置要容易得多。

    Large language models (LLMs) such as GPT-3.5 and CodeLlama are powerful models for code generation and understanding. Fine-tuning these models comes with a high computational cost and requires a large labeled dataset. Alternatively, in-context learning techniques allow models to learn downstream tasks with only a few examples. Recently, researchers have shown how in-context learning performs well in bug detection and repair. In this paper, we propose code-pair classification task in which both the buggy and non-buggy versions are given to the model, and the model identifies the buggy ones. We evaluate our task in real-world dataset of bug detection and two most powerful LLMs. Our experiments indicate that an LLM can often pick the buggy from the non-buggy version of the code, and the code-pair classification task is much easier compared to be given a snippet and deciding if and where a bug exists.
    
[^110]: Tabdoor：基于转换器的表格数据神经网络存在后门漏洞

    Tabdoor: Backdoor Vulnerabilities in Transformer-based Neural Networks for Tabular Data. (arXiv:2311.07550v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2311.07550](http://arxiv.org/abs/2311.07550)

    这项研究全面分析了使用DNNs对表格数据进行后门攻击，揭示了基于转换器的DNNs对表格数据非常容易受到后门攻击，甚至只需最小的特征值修改。该攻击还可以推广到其他模型。

    

    深度神经网络(DNNs)在各个领域都显示出巨大的潜力。与这些发展同时，与DNN训练相关的漏洞，如后门攻击，是一个重大关切。这些攻击涉及在模型训练过程中微妙地插入触发器，从而允许操纵预测。最近，由于转换器模型的崛起，DNNs用于表格数据越来越受关注。我们的研究对使用DNNs对表格数据进行后门攻击进行了全面分析，特别关注转换器。鉴于表格数据的固有复杂性，我们探究了嵌入后门的挑战。通过对基准数据集进行系统实验，我们发现基于转换器的DNNs对表格数据非常容易受到后门攻击，即使只有最小的特征值修改。我们还验证了我们的攻击可以推广到其他模型，如XGBoost和DeepFM。我们的研究结果几乎表明后门攻击可以完美实现。

    Deep Neural Networks (DNNs) have shown great promise in various domains. Alongside these developments, vulnerabilities associated with DNN training, such as backdoor attacks, are a significant concern. These attacks involve the subtle insertion of triggers during model training, allowing for manipulated predictions.More recently, DNNs for tabular data have gained increasing attention due to the rise of transformer models.  Our research presents a comprehensive analysis of backdoor attacks on tabular data using DNNs, particularly focusing on transformers. Given the inherent complexities of tabular data, we explore the challenges of embedding backdoors. Through systematic experimentation across benchmark datasets, we uncover that transformer-based DNNs for tabular data are highly susceptible to backdoor attacks, even with minimal feature value alterations. We also verify that our attack can be generalized to other models, like XGBoost and DeepFM. Our results indicate nearly perfect attac
    
[^111]: GateLoop: 完全数据控制的线性递归用于序列建模

    GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling. (arXiv:2311.01927v1 [cs.LG])

    [http://arxiv.org/abs/2311.01927](http://arxiv.org/abs/2311.01927)

    GateLoop是一种完全数据控制的线性递归序列模型，优于现有模型，可以提供数据控制的相对位置信息给Attention。

    

    线性递归已被证明是一种有效建模长序列的强大工具。在这项工作中，我们表明现有模型未能充分利用其潜力。在这一发现的基础上，我们开发了GateLoop，这是一种基础性的序列模型，通过使用数据控制的状态转换来推广线性递归模型，如S4、S5、LRU和RetNet。利用这一理论进步，GateLoop在自回归语言建模方面在实证上优于现有模型。我们的方法具有低成本的$O(l)$递归模式和高度优化的关联扫描实现的高效$O(l \log_{2} l)$并行模式。此外，我们还推导出了一个$O(l^2)$的代理注意力模式，揭示了对Transformer和最近提出的架构的显著影响。具体而言，我们证明了我们的方法可以被解释为向Attention提供数据控制的相对位置信息。而许多现有模型仅依赖于数据无关的位置信息。

    Linear Recurrence has proven to be a powerful tool for modeling long sequences efficiently. In this work, we show that existing models fail to take full advantage of its potential. Motivated by this finding, we develop GateLoop, a foundational sequence model that generalizes linear recurrent models such as S4, S5, LRU and RetNet, by employing data-controlled state transitions. Utilizing this theoretical advance, GateLoop empirically outperforms existing models for auto-regressive language modeling. Our method comes with a low-cost $O(l)$ recurrent mode and an efficient $O(l \log_{2} l)$ parallel mode making use of highly optimized associative scan implementations. Furthermore, we derive an $O(l^2)$ surrogate attention mode, revealing remarkable implications for Transformer and recently proposed architectures. Specifically, we prove that our approach can be interpreted as providing data-controlled relative-positional information to Attention. While many existing models solely rely on da
    
[^112]: GOPlan:通过学习模型进行计划的目标条件下的离线强化学习

    GOPlan: Goal-conditioned Offline Reinforcement Learning by Planning with Learned Models. (arXiv:2310.20025v1 [cs.LG])

    [http://arxiv.org/abs/2310.20025](http://arxiv.org/abs/2310.20025)

    GOPlan是一个使用学习模型进行计划的目标条件下的离线强化学习方法，通过预训练先验策略和使用重新分析方法生成虚构轨迹，用以提高性能和处理有限数据预算和未见目标泛化的能力。

    

    离线目标条件下的强化学习（GCRL）为从多样化和多任务的离线数据集中学习通用策略提供了可行的范例。尽管近期取得了显著进展，但主导的离线GCRL方法仍然受限于无模型方法，限制了它们应对有限数据预算和未见目标泛化的能力。在这项工作中，我们提出了一种新的两阶段模型为基础的框架，Goal-conditioned Offline Planning（GOPlan），包括（1）预训练一个能够捕捉多目标数据集中多模态动作分布的先验策略；（2）利用规划的重新分析方法为微调策略生成虚构轨迹。具体而言，先验策略基于一个具有明显模式分离的带优势权重的条件生成对抗网络，以克服超出分布（OOD）动作的缺点。为进一步优化策略，重新分析方法通过规划生成高质量的虚构数据。

    Offline goal-conditioned RL (GCRL) offers a feasible paradigm to learn general-purpose policies from diverse and multi-task offline datasets. Despite notable recent progress, the predominant offline GCRL methods have been restricted to model-free approaches, constraining their capacity to tackle limited data budgets and unseen goal generalization. In this work, we propose a novel two-stage model-based framework, Goal-conditioned Offline Planning (GOPlan), including (1) pretraining a prior policy capable of capturing multi-modal action distribution within the multi-goal dataset; (2) employing the reanalysis method with planning to generate imagined trajectories for funetuning policies. Specifically, the prior policy is based on an advantage-weighted Conditioned Generative Adversarial Networks that exhibits distinct mode separation to overcome the pitfalls of out-of-distribution (OOD) actions. For further policy optimization, the reanalysis method generates high-quality imaginary data by
    
[^113]: 合并特征在联合声音分类和定位神经网络中的应用

    Feature Aggregation in Joint Sound Classification and Localization Neural Networks. (arXiv:2310.19063v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2310.19063](http://arxiv.org/abs/2310.19063)

    本研究通过在声音分类和定位网络中引入特征聚合技术，提升了模型性能和特征的鲁棒性，特别适用于区分直接和间接声音信号的SSL网络。

    

    本研究探讨了深度学习技术在联合声音信号分类和定位网络中的应用。目前最先进的声源定位深度学习网络在其架构中缺乏特征聚合。特征聚合通过使来自不同特征尺度的信息整合，提高模型性能，从而改善特征的鲁棒性和不变性。这在SSL网络中尤为重要，SSL网络必须区分直接和间接声音信号。为了弥补这一差距，我们从计算机视觉神经网络中借鉴了特征聚合技术，将其应用在信号检测神经网络中。此外，我们提出了尺度编码网络（SEN）用于特征聚合，以编码来自不同尺度的特征，压缩网络以实现更高效的聚合。为了评估特征聚合在SSL网络中的有效性，我们集成了以下计算机视觉特征聚合实验。

    This study addresses the application of deep learning techniques in joint sound signal classification and localization networks. Current state-of-the-art sound source localization deep learning networks lack feature aggregation within their architecture. Feature aggregation enhances model performance by enabling the consolidation of information from different feature scales, thereby improving feature robustness and invariance. This is particularly important in SSL networks, which must differentiate direct and indirect acoustic signals. To address this gap, we adapt feature aggregation techniques from computer vision neural networks to signal detection neural networks. Additionally, we propose the Scale Encoding Network (SEN) for feature aggregation to encode features from various scales, compressing the network for more computationally efficient aggregation. To evaluate the efficacy of feature aggregation in SSL networks, we integrated the following computer vision feature aggregation 
    
[^114]: 在不断变化的多臂赌博机中实现零样本学习

    Towards Zero Shot Learning in Restless Multi-armed Bandits. (arXiv:2310.14526v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.14526](http://arxiv.org/abs/2310.14526)

    通过开发一个基于神经网络的预训练模型，我们实现了在不断变化的多臂赌博机中的零样本学习，该模型具有泛化能力，并且能够在特定实例上进行高效微调，同时适用于多行为设置和离散或连续状态空间。

    

    近来，通过多智能体强化学习的视角研究了一类资源分配问题——不断变化的多臂赌博机（RMABs），该问题在医疗保健、在线广告和反盗猎等领域具有广泛应用。先前的RMAB研究存在一些限制，例如没有充分解决连续状态问题，并且在多个真实世界应用中，当赌博机的入选和退出不断发生时，需要从头开始重新训练，这是一个常见的挑战。为了解决这些限制，我们开发了一个基于神经网络的预训练模型（PreFeRMAB），该模型具有对之前未见过的广泛RMAB问题的零样本能力，并且可以比从头训练更加高效地对特定实例进行微调。此外，我们的模型还适用于一般的多行为设置和离散或连续状态空间。为了实现快速泛化，我们学习了一种新颖的单一策略网络模型，该模型利用特征信息并采用了一种新的训练方式。

    Restless multi-arm bandits (RMABs), a class of resource allocation problems with broad application in areas such as healthcare, online advertising, and anti-poaching, have recently been studied from a multi-agent reinforcement learning perspective. Prior RMAB research suffers from several limitations, e.g., it fails to adequately address continuous states, and requires retraining from scratch when arms opt-in and opt-out over time, a common challenge in many real world applications. We address these limitations by developing a neural network-based pre-trained model (PreFeRMAB) that has general zero-shot ability on a wide range of previously unseen RMABs, and which can be fine-tuned on specific instances in a more sample-efficient way than retraining from scratch. Our model also accommodates general multi-action settings and discrete or continuous state spaces. To enable fast generalization, we learn a novel single policy network model that utilizes feature information and employs a tra
    
[^115]: GraphMaker: 扩散模型能生成大型带属性图吗？

    GraphMaker: Can Diffusion Models Generate Large Attributed Graphs?. (arXiv:2310.13833v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.13833](http://arxiv.org/abs/2310.13833)

    GraphMaker是一种专门设计用于生成大型带属性图的新颖扩散模型。

    

    在各种实际应用中，具有节点属性的大规模图变得越来越常见。创建与真实世界示例类似的合成、富属性图对于共享图数据进行分析和开发学习模型至关重要，尤其是当原始数据限制被共享时。传统的图生成方法在处理这些复杂结构方面存在局限性。最新的扩散模型在生成没有属性和较小的分子图方面显示出潜力。然而，这些模型在生成大型带属性图方面面临着挑战，原因是复杂的属性-结构相关性和图的大规模。本文介绍了一种专门用于生成大型带属性图的新颖扩散模型：GraphMaker。我们探索了各种节点属性和图结构生成过程的组合，发现异步方法更有效地捕捉了内部。

    Large-scale graphs with node attributes are increasingly common in various real-world applications. Creating synthetic, attribute-rich graphs that mirror real-world examples is crucial, especially for sharing graph data for analysis and developing learning models when original data is restricted to be shared. Traditional graph generation methods are limited in their capacity to handle these complex structures. Recent advances in diffusion models have shown potential in generating graph structures without attributes and smaller molecular graphs. However, these models face challenges in generating large attributed graphs due to the complex attribute-structure correlations and the large size of these graphs. This paper introduces a novel diffusion model, GraphMaker, specifically designed for generating large attributed graphs. We explore various combinations of node attribute and graph structure generation processes, finding that an asynchronous approach more effectively captures the intr
    
[^116]: 利用混合模型基于后继特征强化学习跨任务传递的不确定性感知方法

    Uncertainty-aware transfer across tasks using hybrid model-based successor feature reinforcement learning. (arXiv:2310.10818v1 [cs.LG])

    [http://arxiv.org/abs/2310.10818](http://arxiv.org/abs/2310.10818)

    该论文提出了一种利用混合模型基于后继特征强化学习方法，能够在具有不同转移动力学和奖励函数的任务之间实现样本高效的不确定性感知知识传递。

    

    对于复杂和大规模的决策问题，样本效率对于开发实用的强化学习（RL）至关重要。将来自先前经验的知识转移和泛化到下游任务能够显著提高样本效率。最近的研究表明，后继特征（SF）RL算法能够在具有不同奖励但相同转移动力学的任务之间实现知识泛化。最近提出结合模型基于（MB）方法和SF算法可以缓解固定转移动力学的限制。此外，不确定性感知的探索方法被广泛认为是提高样本效率的另一种吸引人的方法。将混合模型基于后继特征（MB-SF）和不确定性的两个思想结合起来，提出了一种解决跨任务样本高效不确定性感知知识传递问题的方法。

    Sample efficiency is central to developing practical reinforcement learning (RL) for complex and large-scale decision-making problems. The ability to transfer and generalize knowledge gained from previous experiences to downstream tasks can significantly improve sample efficiency. Recent research indicates that successor feature (SF) RL algorithms enable knowledge generalization between tasks with different rewards but identical transition dynamics. It has recently been hypothesized that combining model-based (MB) methods with SF algorithms can alleviate the limitation of fixed transition dynamics. Furthermore, uncertainty-aware exploration is widely recognized as another appealing approach for improving sample efficiency. Putting together two ideas of hybrid model-based successor feature (MB-SF) and uncertainty leads to an approach to the problem of sample efficient uncertainty-aware knowledge transfer across tasks with different transition dynamics or/and reward functions. In this pa
    
[^117]: 用于识别半导体晶圆地图中缺陷模式的机器学习技术：一项调查、实证和实验评估

    Machine Learning Techniques for Identifying the Defective Patterns in Semiconductor Wafer Maps: A Survey, Empirical, and Experimental Evaluations. (arXiv:2310.10705v1 [cs.LG])

    [http://arxiv.org/abs/2310.10705](http://arxiv.org/abs/2310.10705)

    本文综述了利用机器学习技术在半导体制造中识别晶圆缺陷的方法，提出了一种创新的分类体系，为不同算法和技术之间的关系提供了深入分析，并通过严谨的评估验证了算法性能。

    

    本文综述了利用机器学习（ML）技术识别半导体制造中晶圆缺陷的方法学。尽管越来越多的研究证明了ML在晶圆缺陷识别中的有效性，但在这个主题上缺乏全面的综述。本文试图弥补这个空白，通过整合现有文献，深入分析各种ML算法在晶圆缺陷检测领域的优势、局限性和潜在应用。我们提出了一种创新的方法学分类体系，详细分类了算法，并提供了更细致的子技术划分。这个分类体系从广泛的方法学类别开始，到具体的子技术结束。它帮助研究人员理解不同算法以及它们的技术之间的复杂关系。我们采用严谨的实证和实验评估来验证算法性能。

    This survey paper offers a comprehensive review of methodologies utilizing machine learning (ML) techniques for identifying wafer defects in semiconductor manufacturing. Despite the growing body of research demonstrating the effectiveness of ML in wafer defect identification, there is a noticeable absence of comprehensive reviews on this subject. This survey attempts to fill this void by amalgamating available literature and providing an in-depth analysis of the advantages, limitations, and potential applications of various ML algorithms in the realm of wafer defect detection. An innovative taxonomy of methodologies that we present provides a detailed classification of algorithms into more refined categories and techniques. This taxonomy follows a four-tier structure, starting from broad methodology categories and ending with specific sub-techniques. It aids researchers in comprehending the complex relationships between different algorithms and their techniques. We employ a rigorous em
    
[^118]: 通过对抗性正则化对拆分学习进行袭击的被动推理攻击

    Passive Inference Attacks on Split Learning via Adversarial Regularization. (arXiv:2310.10483v4 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2310.10483](http://arxiv.org/abs/2310.10483)

    该论文介绍了一种针对拆分学习的被动推理攻击框架SDAR，通过利用辅助数据和对抗性正则化来推断客户端的私有特征和标签，在实验中取得了与主动攻击相当的攻击性能。

    

    拆分学习(SL)已成为传统联邦学习的一种实用且高效的替代方案。虽然以前攻击SL的尝试往往依赖于过于强硬的假设或者针对易受攻击的模型，但我们试图开发更加实用的攻击方法。我们引入了SDAR，这是一个针对拥有诚实但好奇的服务器的SL的新攻击框架。SDAR利用辅助数据和对抗性正则化来学习客户端私有模型的可解码模拟器，在基本SL下可以有效地推断出客户端的私有特征，并在U型SL下推断出特征和标签。我们进行了大量实验来验证我们提出的攻击方法的有效性。值得注意的是，在具有挑战性但实际的场景中，现有的被动攻击难以有效地重建客户端的私有数据时，SDAR始终实现了与主动攻击相当的攻击性能。在CIFAR-10上，在深度拆分水平为7的情况下，SDAR达到了攻击性能。

    Split Learning (SL) has emerged as a practical and efficient alternative to traditional federated learning. While previous attempts to attack SL have often relied on overly strong assumptions or targeted easily exploitable models, we seek to develop more practical attacks. We introduce SDAR, a novel attack framework against SL with an honest-but-curious server. SDAR leverages auxiliary data and adversarial regularization to learn a decodable simulator of the client's private model, which can effectively infer the client's private features under the vanilla SL, and both features and labels under the U-shaped SL. We perform extensive experiments in both configurations to validate the effectiveness of our proposed attacks. Notably, in challenging but practical scenarios where existing passive attacks struggle to reconstruct the client's private data effectively, SDAR consistently achieves attack performance comparable to active attacks. On CIFAR-10, at the deep split level of 7, SDAR achi
    
[^119]: 驯服S形瓶颈：可证明Argmaxable的稀疏多标签分类

    Taming the Sigmoid Bottleneck: Provably Argmaxable Sparse Multi-Label Classification. (arXiv:2310.10443v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.10443](http://arxiv.org/abs/2310.10443)

    本文研究了多标签分类任务中的S形瓶颈问题，并提出了通过引入离散傅立叶变换（DFT）输出层来解决这个问题的方法。实验结果表明，该方法能够有效地预测具有稀疏标签组合的输入数据。

    

    S形输出层广泛用于多标签分类（MLC）任务中，其中每个输入可以被分配多个标签。在许多实际的MLC任务中，可能的标签数量在成千上万，往往超过输入特征的数量，导致低秩输出层。在多类分类中，已知这样的低秩输出层是一个瓶颈，可能导致无法Argmax的类别：即任何输入都无法预测的类别。在本文中，我们表明对于MLC任务，类似的S形瓶颈会导致指数级数量的无法Argmax的标签组合。我们解释了如何检测这些无法Argmax的输出，并在三个广泛使用的MLC数据集中证明了它们的存在。然后，我们展示了它们可以通过引入离散傅立叶变换（DFT）输出层在实践中防止，该输出层保证所有稀疏的带有最多k个活跃标签的标签组合都是可Argmax的。我们的DFT层训练速度更快，更可扩展。

    Sigmoid output layers are widely used in multi-label classification (MLC) tasks, in which multiple labels can be assigned to any input. In many practical MLC tasks, the number of possible labels is in the thousands, often exceeding the number of input features and resulting in a low-rank output layer. In multi-class classification, it is known that such a low-rank output layer is a bottleneck that can result in unargmaxable classes: classes which cannot be predicted for any input. In this paper, we show that for MLC tasks, the analogous sigmoid bottleneck results in exponentially many unargmaxable label combinations. We explain how to detect these unargmaxable outputs and demonstrate their presence in three widely used MLC datasets. We then show that they can be prevented in practice by introducing a Discrete Fourier Transform (DFT) output layer, which guarantees that all sparse label combinations with up to $k$ active labels are argmaxable. Our DFT layer trains faster and is more para
    
[^120]: Observatory: 刻画关系表嵌入的研究

    Observatory: Characterizing Embeddings of Relational Tables. (arXiv:2310.07736v1 [cs.DB])

    [http://arxiv.org/abs/2310.07736](http://arxiv.org/abs/2310.07736)

    Observatory提出了一个正式框架来分析关系表的嵌入表示，以帮助研究人员和实践者更好地理解和选择适合特定任务的模型。

    

    最近，语言模型和专门的表嵌入模型在许多表格数据任务上展示出了强大的性能。研究人员和实践者都渴望在许多新的应用场景中利用这些模型；但是对于这些模型的优势和缺点以及它们生成的表格表示的理解有限，导致在寻找适合特定任务的模型的过程中依赖于试错。迫切需要全面了解这些模型，以减少下游使用中的低效率和失败。为了解决这个问题，我们提出了一个名为Observatory的正式框架，以系统地分析关系表的嵌入表示。在关系数据模型的不变性和关于数据分布的统计考虑的基础上，我们定义了八个原始属性，以及相应的度量来定量地刻画这些属性的表格嵌入。

    Language models and specialized table embedding models have recently demonstrated strong performance on many tasks over tabular data. Researchers and practitioners are keen to leverage these models in many new application contexts; but limited understanding of the strengths and weaknesses of these models, and the table representations they generate, makes the process of finding a suitable model for a given task reliant on trial and error. There is an urgent need to gain a comprehensive understanding of these models to minimize inefficiency and failures in downstream usage.  To address this need, we propose Observatory, a formal framework to systematically analyze embedding representations of relational tables. Motivated both by invariants of the relational data model and by statistical considerations regarding data distributions, we define eight primitive properties, and corresponding measures to quantitatively characterize table embeddings for these properties. Based on these properti
    
[^121]: BioT5：在生物学中利用化学知识和自然语言关联丰富跨模态整合

    BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations. (arXiv:2310.07276v1 [cs.CL])

    [http://arxiv.org/abs/2310.07276](http://arxiv.org/abs/2310.07276)

    BioT5是一个全面的预训练框架，在生物学中利用化学知识和自然语言关联丰富了跨模态整合，通过鲁棒的分子表示和上下文知识提取，实现了更有效的信息利用，展现出卓越的性能。

    

    最近在生物研究领域的进展利用分子、蛋白质和自然语言的整合来增强药物发现。然而，当前的模型存在一些限制，如生成无效的分子SMILES、对上下文信息的利用不足以及对结构化和非结构化知识的等量处理。为了解决这些问题，我们提出了一个全面的预训练框架BioT5，它通过化学知识和自然语言关联丰富了生物学中的跨模态整合。BioT5利用SELFIES进行100%鲁棒的分子表示，并从非结构化的生物文献中提取生物实体周围上下文的知识。此外，BioT5区分结构化和非结构化知识，从而更有效地利用信息。在微调后，BioT5在各种任务中展现出卓越的性能，表明其强大的能力。

    Recent advancements in biological research leverage the integration of molecules, proteins, and natural language to enhance drug discovery. However, current models exhibit several limitations, such as the generation of invalid molecular SMILES, underutilization of contextual information, and equal treatment of structured and unstructured knowledge. To address these issues, we propose $\mathbf{BioT5}$, a comprehensive pre-training framework that enriches cross-modal integration in biology with chemical knowledge and natural language associations. $\mathbf{BioT5}$ utilizes SELFIES for $100%$ robust molecular representations and extracts knowledge from the surrounding context of bio-entities in unstructured biological literature. Furthermore, $\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge, leading to more effective utilization of information. After fine-tuning, BioT5 shows superior performance across a wide range of tasks, demonstrating its strong capability 
    
[^122]: 通过格子过参数化范式进行格子操作器的机器学习

    The Lattice Overparametrization Paradigm for the Machine Learning of Lattice Operators. (arXiv:2310.06639v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.06639](http://arxiv.org/abs/2310.06639)

    本文介绍了一种通过过参数化格子，利用格子函数最小化算法进行学习的范式，以克服格子操作器机器学习中的三个潜在瓶颈。

    

    格子操作器的机器学习存在三个潜在瓶颈。从统计角度来看，需要设计一个基于先验信息的受限操作器类，具有低偏差和与样本大小相对较低的复杂性。从计算角度来看，应该有一种高效的算法来在该类上最小化经验误差。从理解的角度来看，需要推导出学习到的操作器的属性，以便从理论上理解其行为。统计瓶颈可以通过有关格子操作器表示的丰富文献克服，但没有通用的学习算法。在本文中，我们讨论了一种学习范式，在其中通过格子中的元素进行过参数化，将格子函数最小化算法应用于学习。我们将随机格子下降算法作为一种通用算法，用于对操作器的受限类进行学习。

    The machine learning of lattice operators has three possible bottlenecks. From a statistical standpoint, it is necessary to design a constrained class of operators based on prior information with low bias, and low complexity relative to the sample size. From a computational perspective, there should be an efficient algorithm to minimize an empirical error over the class. From an understanding point of view, the properties of the learned operator need to be derived, so its behavior can be theoretically understood. The statistical bottleneck can be overcome due to the rich literature about the representation of lattice operators, but there is no general learning algorithm for them. In this paper, we discuss a learning paradigm in which, by overparametrizing a class via elements in a lattice, an algorithm for minimizing functions in a lattice is applied to learn. We present the stochastic lattice descent algorithm as a general algorithm to learn on constrained classes of operators as long
    
[^123]: LARA：一种轻量级且抗过拟合的无监督异常检测再训练方法

    LARA: A Light and Anti-overfitting Retraining Approach for Unsupervised Anomaly Detection. (arXiv:2310.05668v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.05668](http://arxiv.org/abs/2310.05668)

    LARA是一种轻量级且抗过拟合的无监督异常检测再训练方法，它将重新训练过程形式化为一个凸问题，并设计了一个反思模块以利用历史数据，同时数学证明了在微调后可以获得更好的性能。

    

    当前大部分异常检测模型都假设正常模式始终保持不变。然而，Web服务的正常模式经常发生剧烈变化。在这种变化之后，使用旧分布数据训练的模型已经过时。每次都重新训练整个模型是昂贵的。此外，在正常模式变化开始时，新分布的观察数据不足。用有限的数据对大型神经网络模型进行重新训练容易过拟合。因此，我们提出了一种轻量级且抗过拟合的再训练方法（LARA），用于基于深度变分自编码器的时间序列异常检测方法（VAEs）。本工作旨在提出三个新颖的贡献：1）将重新训练过程形式化为一个凸问题，并能够以快速收敛以及防止过拟合；2）设计了一个反思模块，可以利用历史数据而无需储存它们；3）数学证明了在微调后可以获得更好的性能。

    Most of current anomaly detection models assume that the normal pattern remains same all the time. However, the normal patterns of Web services change dramatically and frequently. The model trained on old-distribution data is outdated after such changes. Retraining the whole model every time is expensive. Besides, at the beginning of normal pattern changes, there is not enough observation data from the new distribution. Retraining a large neural network model with limited data is vulnerable to overfitting. Thus, we propose a Light and Anti-overfitting Retraining Approach (LARA) for deep variational auto-encoder based time series anomaly detection methods (VAEs). This work aims to make three novel contributions: 1) the retraining process is formulated as a convex problem and can converge at a fast rate as well as prevent overfitting; 2) designing a ruminate block, which leverages the historical data without the need to store them; 3) mathematically proving that when fine-tuning the late
    
[^124]: 《外科健身房：基于高性能GPU的手术机器人增强学习平台》

    Surgical Gym: A high-performance GPU-based platform for reinforcement learning with surgical robots. (arXiv:2310.04676v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2310.04676](http://arxiv.org/abs/2310.04676)

    这项工作提出了一个高性能GPU平台，用于增强学习与手术机器人的协作。通过提高模拟器效率和训练数据的易获取性，为手术自动化的深度强化学习提供了可扩展的解决方案。

    

    最近在机器人辅助手术方面取得的进展使得手术越来越精确、高效和微创，开启了机器人辅助手术干预的新时代。这使得医生在与机器人协作的情况下能够通过更小的切口进行传统或微创手术，从而改善手术结果。近期的研究致力于使机器人手术更加自主，这有潜力减少手术结果的不确定性和并发症率。深度强化学习方法为手术自动化提供了可扩展的解决方案，但其有效性取决于大量的数据获取，因为在成功完成任务方面缺乏先验知识。由于模拟数据收集的密集性质，以前的工作主要致力于使现有算法更加高效。在这项工作中，我们致力于提高模拟器的效率，使训练数据比以前更易获取。

    Recent advances in robot-assisted surgery have resulted in progressively more precise, efficient, and minimally invasive procedures, sparking a new era of robotic surgical intervention. This enables doctors, in collaborative interaction with robots, to perform traditional or minimally invasive surgeries with improved outcomes through smaller incisions. Recent efforts are working toward making robotic surgery more autonomous which has the potential to reduce variability of surgical outcomes and reduce complication rates. Deep reinforcement learning methodologies offer scalable solutions for surgical automation, but their effectiveness relies on extensive data acquisition due to the absence of prior knowledge in successfully accomplishing tasks. Due to the intensive nature of simulated data collection, previous works have focused on making existing algorithms more efficient. In this work, we focus on making the simulator more efficient, making training data much more accessible than prev
    
[^125]: Cadenza ICASSP 2024大挑战

    The Cadenza ICASSP 2024 Grand Challenge. (arXiv:2310.03480v1 [eess.AS])

    [http://arxiv.org/abs/2310.03480](http://arxiv.org/abs/2310.03480)

    Cadenza项目组织了ICASSP SP Cadenza Challenge，旨在通过音乐分解/混音来提升助听器音质，处理过程考虑音乐、增益和听力损失。

    

    Cadenza项目旨在提高听力受损人群的音乐音质。作为该项目的一部分，该项目组织了ICASSP SP Cadenza Challenge：面向助听器的音乐分解/混音。该挑战可以通过将音乐分解成人声、贝斯、鼓和其他组成部分来解决。然后可以以个性化的方式智能地进行混音，以提高音频质量。另外，还可以使用端到端的方法。处理过程需要考虑音乐本身、每个组成部分的增益以及听众的听力损失。提交的作品将使用Hearing Aid Audio Quality Index（HAAQI）这一入侵式客观指标进行评估。本文概述了该挑战。

    The Cadenza project aims to enhance the audio quality of music for individuals with hearing loss. As part of this, the project is organizing the ICASSP SP Cadenza Challenge: Music Demixing/Remixing for Hearing Aids. The challenge can be tackled by decomposing the music at the hearing aid microphones into vocals, bass, drums, and other components. These can then be intelligently remixed in a personalized manner to improve audio quality. Alternatively, an end-to-end approach could be used. Processes need to consider the music itself, the gain applied to each component, and the listener's hearing loss. The submitted entries will be evaluated using the intrusive objective metric, the Hearing Aid Audio Quality Index (HAAQI). This paper outlines the challenge.
    
[^126]: 低资源语言越狱 GPT-4

    Low-Resource Languages Jailbreak GPT-4. (arXiv:2310.02446v1 [cs.CL])

    [http://arxiv.org/abs/2310.02446](http://arxiv.org/abs/2310.02446)

    通过翻译不安全的英文输入成低资源语言，我们成功绕过了GPT-4的安全机制，并展示了这种跨语言漏洞。这一方法在实验中取得了与甚至超过了最先进的越狱攻击的效果，揭示了低资源语言在AI安全性中的薄弱环节。

    

    人工智能安全培训和大型语言模型（LLM）的红队测试是减少生成不安全内容的措施。我们的工作通过将不安全的英文输入翻译成低资源语言，成功绕过GPT-4的安全机制，并揭示了这些安全机制的跨语言漏洞。在AdvBenchmark中，GPT-4针对不安全的翻译输入进行交互，并且79%的时间内提供了可行的方案，使用户实现其有害目标，这与甚至超过了最先进的越狱攻击的效果相当。其他高/中资源语言的攻击成功率显著较低，这表明跨语言漏洞主要适用于低资源语言。以前，对低资源语言的有限训练主要影响那些使用这些语言的人，造成技术差距。然而，我们的工作突出了一个关键转变：

    AI safety training and red-teaming of large language models (LLMs) are measures to mitigate the generation of unsafe content. Our work exposes the inherent cross-lingual vulnerability of these safety mechanisms, resulting from the linguistic inequality of safety training data, by successfully circumventing GPT-4's safeguard through translating unsafe English inputs into low-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafe translated inputs and provides actionable items that can get the users towards their harmful goals 79% of the time, which is on par with or even surpassing state-of-the-art jailbreaking attacks. Other high-/mid-resource languages have significantly lower attack success rate, which suggests that the cross-lingual vulnerability mainly applies to low-resource languages. Previously, limited training on low-resource languages primarily affects speakers of those languages, causing technological disparities. However, our work highlights a crucial shift:
    
[^127]: Time-LLM: 通过重新编程大型语言模型进行时间序列预测

    Time-LLM: Time Series Forecasting by Reprogramming Large Language Models. (arXiv:2310.01728v1 [cs.LG])

    [http://arxiv.org/abs/2310.01728](http://arxiv.org/abs/2310.01728)

    这项工作介绍了Time-LLM，一个重新编程的框架，通过重新利用大型语言模型，可以进行一般的时间序列预测。

    

    时间序列预测在许多实际动态系统中具有重要意义并得到了广泛研究。不同于自然语言处理（NLP）和计算机视觉（CV），在这些领域，一个单一的大型模型可以处理多个任务，而时间序列预测的模型通常是专门化的，需要为不同的任务和应用设计不同的模型。虽然在NLP和CV领域中，预训练的基础模型取得了令人瞩目的进展，但是在时间序列领域的发展受到数据稀疏性的限制。最近的研究表明，大型语言模型（LLMs）在复杂的序列标记中具有强大的模式识别和推理能力。然而，有效地将时间序列数据和自然语言的模态进行对齐以利用这些能力仍然具有挑战性。在这项工作中，我们提出了Time-LLM，这是一个重新编程的框架，可以重用LLMs来进行一般的时间序列预测，同时保持骨干语言模型的完整性。

    Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present Time-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. We begin by 
    
[^128]: 通过多变量投影进行广义激活

    Generalized Activation via Multivariate Projection. (arXiv:2309.17194v1 [cs.LG])

    [http://arxiv.org/abs/2309.17194](http://arxiv.org/abs/2309.17194)

    通过将ReLU视为从R投影到非负半线R+的操作，我们将其通过用凸锥的广义投影算子替代，扩展为具有多个输入和多个输出的多变量投影单元 (MPU)激活函数，并证明其在表达能力方面优于ReLU激活的FNN。

    

    激活函数对于引入神经网络的非线性起着至关重要的作用，Rectified Linear Unit (ReLU)常因其简单和有效而受青睐。受浅层前向神经网络 (FNN) 和单次投影梯度下降 (PGD) 算法之间结构相似性的启发，我们将ReLU视为从R投影到非负半线R+的操作。在这个解释基础上，我们通过用凸锥的广义投影算子替代ReLU，如二阶锥 (SOC) 投影，从而将其自然地扩展为多变量投影单元 (MPU)，这是具有多个输入和多个输出的激活函数。我们进一步提供了数学证明，证明了使用SOC投影激活的FNN在表达能力方面优于使用ReLU的FNN。通过对广泛采用的架构进行实验评估

    Activation functions are essential to introduce nonlinearity into neural networks, with the Rectified Linear Unit (ReLU) often favored for its simplicity and effectiveness. Motivated by the structural similarity between a shallow Feedforward Neural Network (FNN) and a single iteration of the Projected Gradient Descent (PGD) algorithm, a standard approach for solving constrained optimization problems, we consider ReLU as a projection from R onto the nonnegative half-line R+. Building on this interpretation, we extend ReLU by substituting it with a generalized projection operator onto a convex cone, such as the Second-Order Cone (SOC) projection, thereby naturally extending it to a Multivariate Projection Unit (MPU), an activation function with multiple inputs and multiple outputs. We further provide a mathematical proof establishing that FNNs activated by SOC projections outperform those utilizing ReLU in terms of expressive power. Experimental evaluations on widely-adopted architecture
    
[^129]: 早期检测2型糖尿病患者白蛋白尿风险的监督学习模型

    Supervised Learning Models for Early Detection of Albuminuria Risk in Type-2 Diabetes Mellitus Patients. (arXiv:2309.16742v1 [cs.LG])

    [http://arxiv.org/abs/2309.16742](http://arxiv.org/abs/2309.16742)

    该论文研究了2型糖尿病患者白蛋白尿的早期预测问题，并开发了一种监督学习模型。通过使用不同的监督学习算法对184条数据进行训练，得出了预测结果。

    

    糖尿病，尤其是2型糖尿病，仍然是一个重大的健康问题。与糖尿病相关的一个主要问题是其并发症的发展。糖尿病肾病是糖尿病的一种慢性并发症，不利地影响肾脏，导致肾脏损伤。诊断糖尿病肾病涉及考虑各种标准之一，其中之一是尿液中白蛋白的病理学病理学数量，称为白蛋白尿。因此，对糖尿病患者尿液中白蛋白尿的早期预测具有及时预防措施的潜力。本研究旨在开发一种监督学习模型，以预测2型糖尿病患者患有白蛋白尿的风险。所选的监督学习算法包括朴素贝叶斯，支持向量机（SVM），决策树，随机森林，AdaBoost，XGBoost和多层感知器（MLP）。我们的私有数据集包括184条糖尿病并发症风险因素的条目被用来训练算法

    Diabetes, especially T2DM, continues to be a significant health problem. One of the major concerns associated with diabetes is the development of its complications. Diabetic nephropathy, one of the chronic complication of diabetes, adversely affects the kidneys, leading to kidney damage. Diagnosing diabetic nephropathy involves considering various criteria, one of which is the presence of a pathologically significant quantity of albumin in urine, known as albuminuria. Thus, early prediction of albuminuria in diabetic patients holds the potential for timely preventive measures. This study aimed to develop a supervised learning model to predict the risk of developing albuminuria in T2DM patients. The selected supervised learning algorithms included Na\"ive Bayes, Support Vector Machine (SVM), decision tree, random forest, AdaBoost, XGBoost, and Multi-Layer Perceptron (MLP). Our private dataset, comprising 184 entries of diabetes complications risk factors, was used to train the algorithm
    
[^130]: 识别性很重要：揭示无偏学习排名中隐藏的可恢复条件

    Identifiability Matters: Revealing the Hidden Recoverable Condition in Unbiased Learning to Rank. (arXiv:2309.15560v1 [cs.IR])

    [http://arxiv.org/abs/2309.15560](http://arxiv.org/abs/2309.15560)

    研究揭示在无偏学习排名中，当点击数据不能完全拟合时，无法恢复真实相关性，导致排名性能显著降低，提出了可识别性图模型作为解决方案。

    

    无偏学习排名(Unbiased Learning to Rank, ULTR)在从有偏点击日志训练无偏排名模型的现代系统中被广泛应用。关键在于明确地建模用户行为的生成过程，并基于检验假设对点击数据进行拟合。先前的研究经验性地发现只要点击完全拟合，大多数情况下可以恢复出真实潜在相关性。然而，我们证明并非总是能够实现这一点，从而导致排名性能显著降低。在本工作中，我们旨在回答真实相关性是否能够从点击数据恢复出来的问题，这是ULTR领域的一个基本问题。我们首先将一个排名模型定义为可识别的，如果它可以恢复出真实相关性，最多只有一个缩放变换，这对于成对排名目标来说已足够。然后，我们探讨了一个等价的可识别条件，可以新颖地表达为一个图连通性测试问题：当且仅当一个图（即可识别性图）连通时，该排名模型是可识别的。

    The application of Unbiased Learning to Rank (ULTR) is widespread in modern systems for training unbiased ranking models from biased click logs. The key is to explicitly model a generation process for user behavior and fit click data based on examination hypothesis. Previous research found empirically that the true latent relevance can be recovered in most cases as long as the clicks are perfectly fitted. However, we demonstrate that this is not always achievable, resulting in a significant reduction in ranking performance. In this work, we aim to answer if or when the true relevance can be recovered from click data, which is a foundation issue for ULTR field. We first define a ranking model as identifiable if it can recover the true relevance up to a scaling transformation, which is enough for pairwise ranking objective. Then we explore an equivalent condition for identifiability that can be novely expressed as a graph connectivity test problem: if and only if a graph (namely identifi
    
[^131]: 对于神经网络的大批量训练泛化性能的LARS再审视

    Revisiting LARS for Large Batch Training Generalization of Neural Networks. (arXiv:2309.14053v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.14053](http://arxiv.org/abs/2309.14053)

    本文通过对大批量训练技术的研究，提出了一种新的算法TVLARS，该算法利用可配置的函数替代了热身阶段，以实现对于神经网络的稳健训练。实验证明，在大多数情况下，TVLARS比LARS和LAMB都有更好的性能表现，特别是在自监督学习方面。

    

    本文通过在不同场景下使用逐层自适应缩放比(LARS)来探索大批量训练技术，揭示了一些见解。具有热身阶段的LARS算法由于冗余的比例缩放导致在早期陷入尖锐的极小化器。此外，后期固定的陡峭下降限制了深度神经网络有效地遍历早期尖锐的极小化器。基于这些发现，我们提出了一种新的算法Time Varying LARS (TVLARS)，它用可配置的类似sigmoid函数替代了热身阶段，以实现在初始阶段的稳健训练。TVLARS在早期促进了梯度探索，超越了尖锐的优化器，并逐渐过渡到LARS以实现后期的稳健性。广泛的实验表明，在大多数情况下，TVLARS始终优于LARS和LAMB，分类场景中的改进达到2\%。值得注意的是，在所有自监督学习的案例中，TVLARS都胜过了LARS和LAMB，并且性能提升了

    This paper explores Large Batch Training techniques using layer-wise adaptive scaling ratio (LARS) across diverse settings, uncovering insights. LARS algorithms with warm-up tend to be trapped in sharp minimizers early on due to redundant ratio scaling. Additionally, a fixed steep decline in the latter phase restricts deep neural networks from effectively navigating early-phase sharp minimizers. Building on these findings, we propose Time Varying LARS (TVLARS), a novel algorithm that replaces warm-up with a configurable sigmoid-like function for robust training in the initial phase. TVLARS promotes gradient exploration early on, surpassing sharp optimizers and gradually transitioning to LARS for robustness in later phases. Extensive experiments demonstrate that TVLARS consistently outperforms LARS and LAMB in most cases, with up to 2\% improvement in classification scenarios. Notably, in all self-supervised learning cases, TVLARS dominates LARS and LAMB with performance improvements of
    
[^132]: 在学习者提供的问题上增强学生表现预测的SGNN-LLM协同

    Enhancing Student Performance Prediction on Learnersourced Questions with SGNN-LLM Synergy. (arXiv:2309.13500v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.13500](http://arxiv.org/abs/2309.13500)

    这项研究介绍了一种创新的策略，将有符号图神经网络（SGNNs）和大型语言模型（LLM）的潜力协同起来，用于预测学生在学习者提供的问题上的表现。该方法利用有符号二分图全面建模学生回答，并采用对比学习框架增强了噪声的鲁棒性。

    

    通过学生内容创作，学习者合作具有可扩展教育的巨大潜力。然而，预测学生在学习者提供的问题上的表现，对于个性化学习体验至关重要，由于学生生成的数据中固有的噪声，这是具有挑战性的。此外，传统的基于图的方法可以捕获学生和问题交互的复杂网络，但在冷启动条件下，其中学生对问题的有限参与导致数据稀疏，这些方法往往表现不佳。为了解决这两个挑战，我们引入了一种创新策略，将整合有符号图神经网络（SGNNs）和大型语言模型（LLM）的潜力协同起来。我们的方法利用有符号二分图全面建模学生回答，并采用对比学习框架增强了噪声的鲁棒性。此外，LLM的贡献在于生成基础问题嵌入，特别是证明了其优势。

    Learnersourcing offers great potential for scalable education through student content creation. However, predicting student performance on learnersourced questions, which is essential for personalizing the learning experience, is challenging due to the inherent noise in student-generated data. Moreover, while conventional graph-based methods can capture the complex network of student and question interactions, they often fall short under cold start conditions where limited student engagement with questions yields sparse data. To address both challenges, we introduce an innovative strategy that synergizes the potential of integrating Signed Graph Neural Networks (SGNNs) and Large Language Model (LLM) embeddings. Our methodology employs a signed bipartite graph to comprehensively model student answers, complemented by a contrastive learning framework that enhances noise resilience. Furthermore, LLM's contribution lies in generating foundational question embeddings, proving especially adv
    
[^133]: 面向黑盒文本分类器的LLM引导因果可解释性

    Towards LLM-guided Causal Explainability for Black-box Text Classifiers. (arXiv:2309.13340v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.13340](http://arxiv.org/abs/2309.13340)

    本文提出了一种利用大型语言模型（LLM）引导黑盒文本分类器的因果可解释性的方法，通过生成反事实解释来解决这一挑战。

    

    随着越来越大且更复杂的深度学习模型的出现，比如在自然语言处理（NLP）领域，像可解释性和可解释性这样的模型质量，尽管非常令人向往，但变得越来越难以解决。例如，文本分类中的最先进模型是设计为黑盒。尽管标准的解释方法可以提供一定程度的解释能力，但这些方法主要是基于相关性的，对模型的理解能力有限。因果解释能力是更理想的目标，但在NLP领域却极具挑战性，原因有很多。受到最近利用大型语言模型（LLMs）作为专家的工作的启发，本文旨在利用最新的LLMs的指导和理解能力，通过生成反事实解释来实现黑盒文本分类器的因果可解释性。为此，我们提出了一个三步骤的流程，

    With the advent of larger and more complex deep learning models, such as in Natural Language Processing (NLP), model qualities like explainability and interpretability, albeit highly desirable, are becoming harder challenges to tackle and solve. For example, state-of-the-art models in text classification are black-box by design. Although standard explanation methods provide some degree of explainability, these are mostly correlation-based methods and do not provide much insight into the model. The alternative of causal explainability is more desirable to achieve but extremely challenging in NLP due to a variety of reasons. Inspired by recent endeavors to utilize Large Language Models (LLMs) as experts, in this work, we aim to leverage the instruction-following and textual understanding capabilities of recent state-of-the-art LLMs to facilitate causal explainability via counterfactual explanation generation for black-box text classifiers. To do this, we propose a three-step pipeline via
    
[^134]: 隐私保护下的上下文学习与差分隐私弱监督生成

    Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation. (arXiv:2309.11765v1 [cs.LG])

    [http://arxiv.org/abs/2309.11765](http://arxiv.org/abs/2309.11765)

    本论文提出了一种隐私保护下的上下文学习算法，通过生成具有差分隐私保证的合成少量示范，实现了有效的ICL。实验证明该算法在强隐私级别下能够取得竞争性能，为广泛应用领域的隐私保护下ICL开辟了新的可能性。

    

    我们研究了使用大型语言模型（LLM）在私有数据集上进行上下文学习（ICL）的问题。这种情景会带来隐私风险，因为LLM可能泄漏或复述在提示中展示的私有示例。我们提出了一种新算法，可以从私有数据集中生成具有形式差分隐私保证的合成少量示范，并在实证上证明它能够实现有效的ICL。我们在标准基准测试上进行了大量实验，并将我们的算法与非私有ICL和零样本解决方案进行了比较。我们的结果表明，我们的算法可以在强隐私级别下达到竞争性能。这些结果为具有隐私保护的ICL在广泛应用领域打开了新的可能性。

    We study the problem of in-context learning (ICL) with large language models (LLMs) on private datasets. This scenario poses privacy risks, as LLMs may leak or regurgitate the private examples demonstrated in the prompt. We propose a novel algorithm that generates synthetic few-shot demonstrations from the private dataset with formal differential privacy (DP) guarantees, and show empirically that it can achieve effective ICL. We conduct extensive experiments on standard benchmarks and compare our algorithm with non-private ICL and zero-shot solutions. Our results demonstrate that our algorithm can achieve competitive performance with strong privacy levels. These results open up new possibilities for ICL with privacy protection for a broad range of applications.
    
[^135]: 可扩展的神经网络模型和千兆级数据集用于粒子流重建

    Scalable neural network models and terascale datasets for particle-flow reconstruction. (arXiv:2309.06782v1 [physics.data-an])

    [http://arxiv.org/abs/2309.06782](http://arxiv.org/abs/2309.06782)

    本研究针对高能电子-正电子碰撞中的粒子流重建，使用可扩展的机器学习模型，并通过超参数调优和硬件处理器的高度可移植性，取得了真实且具有竞争力的物理性能。

    

    本研究针对高能电子-正电子碰撞中基于高度粒度探测器模拟的完整事件重建，研究了可扩展的机器学习模型。粒子流（PF）重建可通过跟踪和量能器团簇或击中来构建监督学习任务。我们比较了图神经网络和基于内核的变换器，并证明两者都避免了二次内存分配和计算成本，同时实现了真实的粒子流重建。我们展示了在超级计算机上进行的超参数调优显著提高了模型的物理性能。我们还展示了所得模型在硬件处理器上具有高度可移植性，支持NVIDIA, AMD和英特尔 Habana卡。最后，我们证明了模型可以在由跟踪和量能器击中组成的高粒度输入上进行训练，从而获得与基准相竞争的物理性能。有关复现研究的数据集和软件已发布。

    We study scalable machine learning models for full event reconstruction in high-energy electron-positron collisions based on a highly granular detector simulation. Particle-flow (PF) reconstruction can be formulated as a supervised learning task using tracks and calorimeter clusters or hits. We compare a graph neural network and kernel-based transformer and demonstrate that both avoid quadratic memory allocation and computational cost while achieving realistic PF reconstruction. We show that hyperparameter tuning on a supercomputer significantly improves the physics performance of the models. We also demonstrate that the resulting model is highly portable across hardware processors, supporting Nvidia, AMD, and Intel Habana cards. Finally, we demonstrate that the model can be trained on highly granular inputs consisting of tracks and calorimeter hits, resulting in a competitive physics performance with the baseline. Datasets and software to reproduce the studies are published following 
    
[^136]: 表示对上下文学习的影响：对合成任务的探索

    How does representation impact in-context learning: A exploration on a synthetic task. (arXiv:2309.06054v1 [cs.LG])

    [http://arxiv.org/abs/2309.06054](http://arxiv.org/abs/2309.06054)

    本研究通过探索表示学习的角度，研究了表示对上下文学习的影响。实验结果表明，在上下文学习中，上下文内部成分对学习性能起到重要作用。

    

    上下文学习，即从上下文样本中学习，是Transformer的一项引人注目的能力。然而，驱动上下文学习的机制尚未被充分理解。本研究旨在从一个未被充分探索的表示学习角度进行调查。在上下文学习场景中，表示更加复杂，表示可以受到模型权重和上下文样本的影响。我们将上述两个概念方面的表示分别称为权重内部成分和上下文内部成分。为了研究这两个成分如何影响上下文学习能力，我们构建了一个新颖的合成任务，从而可以设计两个探针，即权重内部探针和上下文探针，分别评估这两个成分。我们证明上下文内部成分的好坏与上下文学习性能高度相关，这表明上下文学习与表示学习之间的纠缠关系。

    In-context learning, i.e., learning from in-context samples, is an impressive ability of Transformer. However, the mechanism driving the in-context learning is not yet fully understood. In this study, we aim to investigate from an underexplored perspective of representation learning. The representation is more complex for in-context learning senario, where the representation can be impacted by both model weights and in-context samples. We refer the above two conceptually aspects of representation as in-weight component and in-context component, respectively. To study how the two components affect in-context learning capabilities, we construct a novel synthetic task, making it possible to device two probes, in-weights probe and in-context probe, to evaluate the two components, respectively. We demonstrate that the goodness of in-context component is highly related to the in-context learning performance, which indicates the entanglement between in-context learning and representation lear
    
[^137]: 强化学习辅助进化算法：调查和研究机会

    Reinforcement Learning-assisted Evolutionary Algorithm: A Survey and Research Opportunities. (arXiv:2308.13420v2 [cs.NE] CROSS LISTED)

    [http://arxiv.org/abs/2308.13420](http://arxiv.org/abs/2308.13420)

    本文调查了强化学习辅助进化算法（RL-EA），该算法将强化学习与进化算法结合，以提高优化性能。对各种RL-EA的结构、操作符和搜索模式进行了分类和概述。

    

    进化算法是一类基于自然进化原理的随机搜索方法，因其在各种实际优化问题中的卓越性能而广受赞誉。尽管全球的研究人员提出了各种各样的进化算法，但仍存在一些限制，如收敛速度慢和泛化能力差。因此，许多学者积极探索改进算法结构、操作符、搜索模式等方法，以提高其优化性能。近年来，将强化学习作为进化算法框架的一个组成部分，已经展示出超越性能。本文综述了将强化学习集成到进化算法中的最新研究进展，被称为强化学习辅助进化算法（RL-EA）。我们首先介绍了强化学习和进化算法的概念。然后，我们提供了一个对RL-EA中不同结构、操作符和搜索模式的分类方法。

    Evolutionary algorithms (EA), a class of stochastic search methods based on the principles of natural evolution, have received widespread acclaim for their exceptional performance in various real-world optimization problems. While researchers worldwide have proposed a wide variety of EAs, certain limitations remain, such as slow convergence speed and poor generalization capabilities. Consequently, numerous scholars actively explore improvements to algorithmic structures, operators, search patterns, etc., to enhance their optimization performance. Reinforcement learning (RL) integrated as a component in the EA framework has demonstrated superior performance in recent years. This paper presents a comprehensive survey on integrating reinforcement learning into the evolutionary algorithm, referred to as reinforcement learning-assisted evolutionary algorithm (RL-EA). We begin with the conceptual outlines of reinforcement learning and the evolutionary algorithm. We then provide a taxonomy of
    
[^138]: 基于贝叶斯低秩适应的大型语言模型

    Bayesian low-rank adaptation for large language models. (arXiv:2308.13111v1 [cs.LG])

    [http://arxiv.org/abs/2308.13111](http://arxiv.org/abs/2308.13111)

    本研究提出了一种名为Laplace-LoRA的贝叶斯方法，通过应用拉普拉斯近似来增强经过微调的大型语言模型的校准能力。

    

    参数高效的微调（PEFT）已成为大型语言模型（LLMs）成本高效微调的新范式，其中低秩适应（LoRA）被广泛采用。然而，经过微调的LLMs往往变得过于自信，尤其是在较小数据集上进行微调时。贝叶斯方法具有估计不确定性的固有能力，可作为减轻过度自信并增强校准能力的有力工具。在这项工作中，我们引入了Laplace-LoRA，一种直观而有效的贝叶斯方法，它将拉普拉斯近似应用于LoRA参数，并显著提升了经过微调的LLMs的校准能力。

    Parameter-efficient fine-tuning (PEFT) has emerged as a new paradigm for cost-efficient fine-tuning of large language models (LLMs), with low-rank adaptation (LoRA) being a widely adopted choice. However, fine-tuned LLMs often become overconfident especially on when fine-tuned on smaller datasets. Bayesian methods, with their inherent ability to estimate uncertainty, serve as potent tools to mitigate overconfidence and enhance calibration. In this work, we introduce Laplace-LoRA, a straightforward yet effective Bayesian method, which applies the Laplace approximation to the LoRA parameters and, considerably boosts the calibration of fine-tuned LLMs.
    
[^139]: HypBO: 专家引导下的化学家参与的贝叶斯搜索新材料论文

    HypBO: Expert-Guided Chemist-in-the-Loop Bayesian Search for New Materials. (arXiv:2308.11787v1 [cs.LG])

    [http://arxiv.org/abs/2308.11787](http://arxiv.org/abs/2308.11787)

    HypBO是一种利用专家人类知识引导贝叶斯搜索的方法，通过生成改进的样本种子来更快地找到有希望的化学空间区域。

    

    机器人和自动化可以大大加速解决材料发现等难以解决的多变量科学问题，但可用的搜索空间可能非常庞大。贝叶斯优化已经成为一种受欢迎的样本高效优化引擎，在没有目标函数或属性的解析形式被知道的任务中获得了成功。在这里，我们利用专家人类知识以假设的形式，更快地将贝叶斯搜索引导到有希望的化学空间区域。先前的方法使用从现有实验测量得到的潜在分布，这对于新的未开发的科学任务是不可行的。此外，这样的分布无法捕捉精细的假设。我们提出的方法，称为HypBO，利用专家人类假设生成改进的样本种子。不太有希望的种子自动折扣，而有希望的种子用于增加代理模型数据，从而实现更具信息的采样。

    Robotics and automation offer massive accelerations for solving intractable, multivariate scientific problems such as materials discovery, but the available search spaces can be dauntingly large. Bayesian optimization (BO) has emerged as a popular sample-efficient optimization engine, thriving in tasks where no analytic form of the target function/property is known. Here we exploit expert human knowledge in the form of hypotheses to direct Bayesian searches more quickly to promising regions of chemical space. Previous methods have used underlying distributions derived from existing experimental measurements, which is unfeasible for new, unexplored scientific tasks. Also, such distributions cannot capture intricate hypotheses. Our proposed method, which we call HypBO, uses expert human hypotheses to generate an improved seed of samples. Unpromising seeds are automatically discounted, while promising seeds are used to augment the surrogate model data, thus achieving better-informed sampl
    
[^140]: 通过发现高阶抽象来学习逻辑程序

    Learning Logic Programs by Discovering Higher-Order Abstractions. (arXiv:2308.08334v1 [cs.LG])

    [http://arxiv.org/abs/2308.08334](http://arxiv.org/abs/2308.08334)

    本论文介绍了一种通过发现高阶抽象来学习逻辑程序的方法，并且在多个领域的实验结果表明，该方法能够显著提高预测精度并减少学习时间。

    

    发现新颖的抽象对于人类级别的人工智能至关重要。我们介绍了一种发现高阶抽象（例如map、filter和fold）的方法。我们专注于归纳逻辑编程，即从示例和背景知识中归纳逻辑程序。我们引入了高阶重构问题，目标是通过引入高阶抽象来压缩逻辑程序。我们将我们的方法实现在STEVIE中，它将高阶重构问题建模为约束优化问题。我们在多个领域，包括程序合成和视觉推理，的实验结果表明，与没有重构相比，STEVIE可以提高预测精度27%并将学习时间减少47%。我们还展示了STEVIE可以发现适用于不同领域的抽象。

    Discovering novel abstractions is important for human-level AI. We introduce an approach to discover higher-order abstractions, such as map, filter, and fold. We focus on inductive logic programming, which induces logic programs from examples and background knowledge. We introduce the higher-order refactoring problem, where the goal is to compress a logic program by introducing higher-order abstractions. We implement our approach in STEVIE, which formulates the higher-order refactoring problem as a constraint optimisation problem. Our experimental results on multiple domains, including program synthesis and visual reasoning, show that, compared to no refactoring, STEVIE can improve predictive accuracies by 27% and reduce learning times by 47%. We also show that STEVIE can discover abstractions that transfer to different domains
    
[^141]: 在不同环境中对单刚体角色的自适应跟踪

    Adaptive Tracking of a Single-Rigid-Body Character in Various Environments. (arXiv:2308.07491v1 [cs.RO])

    [http://arxiv.org/abs/2308.07491](http://arxiv.org/abs/2308.07491)

    本研究提出了一种基于单刚体角色仿真的深度强化学习方法，通过训练一个能够自适应各种环境变化的策略，实现在不需要额外学习的情况下完成各种任务。

    

    自从DeepMimic的引入以来，后续研究一直致力于在不同情景下扩展模拟动作的范畴。在本研究中，我们提出了一个替代方法，一种基于单刚体角色仿真的深度强化学习方法。利用质心动力学模型（CDM）将全身角色表示为单刚体（SRB），并训练一个跟踪参考动作的策略，我们可以得到一个能够适应各种未观测环境变化和控制器转换的策略，而不需要额外的学习。由于状态和动作空间的降维，学习过程具有高样本效率。最终的全身动作以物理合理的方式基于模拟SRB角色的状态进行运动生成。SRB仿真被制定为一个二次规划问题，策略输出一个动作，允许角色在不同环境中完成任务。

    Since the introduction of DeepMimic [Peng et al. 2018], subsequent research has focused on expanding the repertoire of simulated motions across various scenarios. In this study, we propose an alternative approach for this goal, a deep reinforcement learning method based on the simulation of a single-rigid-body character. Using the centroidal dynamics model (CDM) to express the full-body character as a single rigid body (SRB) and training a policy to track a reference motion, we can obtain a policy that is capable of adapting to various unobserved environmental changes and controller transitions without requiring any additional learning. Due to the reduced dimension of state and action space, the learning process is sample-efficient. The final full-body motion is kinematically generated in a physically plausible way, based on the state of the simulated SRB character. The SRB simulation is formulated as a quadratic programming (QP) problem, and the policy outputs an action that allows th
    
[^142]: 基于评分的强化学习方法

    Rating-based Reinforcement Learning. (arXiv:2307.16348v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.16348](http://arxiv.org/abs/2307.16348)

    本文提出了一种基于评分的强化学习方法，通过利用人类评分来获得人类指导，该方法不同于现有的强化学习方法，它通过对样本轨迹的评估来进行学习。研究结果表明，该方法在实验中取得了良好的效果和收益。

    

    本文提出了一种新的基于评分的强化学习方法，利用人类评分来获得人类指导强化学习。与现有的基于偏好和基于排名的强化学习范式不同，该方法基于人类对样本轨迹的评估而不是对样本对的相对比较。基于评分的强化学习方法建立在一个新的人类评分预测模型和一种新颖的多类损失函数上。我们通过合成评分和真实人类评分进行了多个实验研究，评估了新的基于评分的强化学习方法的有效性和好处。

    This paper develops a novel rating-based reinforcement learning approach that uses human ratings to obtain human guidance in reinforcement learning. Different from the existing preference-based and ranking-based reinforcement learning paradigms, based on human relative preferences over sample pairs, the proposed rating-based reinforcement learning approach is based on human evaluation of individual trajectories without relative comparisons between sample pairs. The rating-based reinforcement learning approach builds on a new prediction model for human ratings and a novel multi-class loss function. We conduct several experimental studies based on synthetic ratings and real human ratings to evaluate the effectiveness and benefits of the new rating-based reinforcement learning approach.
    
[^143]: 单层自注意力变换器使用低秩权重矩阵是否是通用逼近器？

    Are Transformers with One Layer Self-Attention Using Low-Rank Weight Matrices Universal Approximators?. (arXiv:2307.14023v1 [cs.LG])

    [http://arxiv.org/abs/2307.14023](http://arxiv.org/abs/2307.14023)

    通过澄清softmax函数与Boltzmann算符之间的关系，我们证明了单层具有低秩权重矩阵的自注意力具备完全捕获整个输入序列上下文的能力，单层Transformer对于有限样本的记忆能力，单层自注意力Transformer是紧凑域上连续函数的通用逼近器。

    

    现有的关于Transformer模型表达能力的分析要求过深的层数来实现数据的记忆，导致与实际使用的Transformer存在差异。这主要是由于将softmax函数解释为hardmax函数的逼近。通过澄清softmax函数与Boltzmann算符之间的关系，我们证明了单层具有低秩权重矩阵的自注意力具备完全捕获整个输入序列上下文的能力。因此，我们展示了单层Transformer对于有限样本的记忆能力，并且由两个前馈神经网络构成的单层自注意力Transformer是紧凑域上连续函数的通用逼近器。

    Existing analyses of the expressive capacity of Transformer models have required excessively deep layers for data memorization, leading to a discrepancy with the Transformers actually used in practice. This is primarily due to the interpretation of the softmax function as an approximation of the hardmax function. By clarifying the connection between the softmax function and the Boltzmann operator, we prove that a single layer of self-attention with low-rank weight matrices possesses the capability to perfectly capture the context of an entire input sequence. As a consequence, we show that single-layer Transformer has a memorization capacity for finite samples, and that Transformers consisting of one self-attention layer with two feed-forward neural networks are universal approximators for continuous functions on a compact domain.
    
[^144]: REX: 快速探索与利用的增强型AI代理方法

    REX: Rapid Exploration and eXploitation for AI Agents. (arXiv:2307.08962v1 [cs.AI])

    [http://arxiv.org/abs/2307.08962](http://arxiv.org/abs/2307.08962)

    本文提出了一种增强型的快速探索与利用的AI代理方法REX，它通过引入额外的奖励层和类似于UCB分数的概念，实现了更强大和高效的AI代理性能，并且具有离线行为利用和与基础模型无缝集成的优势。

    

    本文提出了一种增强型的快速探索与利用的AI代理方法，称为REX。现有的AutoGPT风格技术存在一些固有的限制，如对于决策的精确描述的过度依赖，以及缺乏类似传统强化学习(Reinforcement Learning，RL)中的尝试和失败程序的系统性方法。REX引入了额外的奖励层，并集成了类似于上限置信界限(UCB)分数的概念，从而实现更强大和高效的AI代理性能。这种方法的优势是可以利用来自日志的离线行为，并与现有的基础模型无缝集成，而不需要任何模型微调。通过与现有方法（如思维链(CoT)和规划推理(RAP)）的比较分析，基于REX的方法展示了相当的性能，并在某些情况下甚至超过了这些现有技术所取得的结果。

    In this paper, we propose an enhanced approach for Rapid Exploration and eXploitation for AI Agents called REX. Existing AutoGPT-style techniques have inherent limitations, such as a heavy reliance on precise descriptions for decision-making, and the lack of a systematic approach to leverage try-and-fail procedures akin to traditional Reinforcement Learning (RL). REX introduces an additional layer of rewards and integrates concepts similar to Upper Confidence Bound (UCB) scores, leading to more robust and efficient AI agent performance. This approach has the advantage of enabling the utilization of offline behaviors from logs and allowing seamless integration with existing foundation models while it does not require any model fine-tuning. Through comparative analysis with existing methods such as Chain-of-Thoughts(CoT) and Reasoning viA Planning(RAP), REX-based methods demonstrate comparable performance and, in certain cases, even surpass the results achieved by these existing techniqu
    
[^145]: 通过选择性同步加速分布式机器学习训练

    Accelerating Distributed ML Training via Selective Synchronization. (arXiv:2307.07950v1 [cs.DC])

    [http://arxiv.org/abs/2307.07950](http://arxiv.org/abs/2307.07950)

    本文介绍了一种名为SelSync的方法，通过选择性同步，在保持准确性的前提下减少了分布式深度神经网络训练的时间开销。该方法根据每一步的重要性动态选择是否进行通信，达到了与批量同步并行（BSP）相同或更高的准确性。

    

    在分布式训练中，深度神经网络（DNN）同时在多个工作者上启动，并使用批量同步并行（BSP）训练中的每个步骤聚合其本地更新。然而，由于聚合的通信成本较高，BSP无法线性扩展。为了减轻这种开销，FedAvg和SSP等替代方案要么降低同步频率，要么完全消除同步，通常以降低最终准确性为代价。在本文中，我们提出了一种名为SelSync的实用、低开销的DNN训练方法，它根据每一步的重要性动态选择是否进行通信。作为\texttt{SelSync}的一部分，我们提出了各种优化方法，以提高在\textit {半同步}训练环境中的收敛性。我们的系统在减少训练时间最多14％的同时，达到与BSP相同或更高的准确性。

    In distributed training, deep neural networks (DNNs) are launched over multiple workers concurrently and aggregate their local updates on each step in bulk-synchronous parallel (BSP) training. However, BSP does not linearly scale-out due to high communication cost of aggregation. To mitigate this overhead, alternatives like Federated Averaging (FedAvg) and Stale-Synchronous Parallel (SSP) either reduce synchronization frequency or eliminate it altogether, usually at the cost of lower final accuracy. In this paper, we present \texttt{SelSync}, a practical, low-overhead method for DNN training that dynamically chooses to incur or avoid communication at each step either by calling the aggregation op or applying local updates based on their significance. We propose various optimizations as part of \texttt{SelSync} to improve convergence in the context of \textit{semi-synchronous} training. Our system converges to the same or better accuracy than BSP while reducing training time by up to 14
    
[^146]: 高维线性回归的统一转移学习模型

    Unified Transfer Learning Models for High-Dimensional Linear Regression. (arXiv:2307.00238v1 [stat.ML])

    [http://arxiv.org/abs/2307.00238](http://arxiv.org/abs/2307.00238)

    UTrans是一种统一转移学习模型，它能检测可转移变量和源数据，并具有较低的估计和预测误差，同时保持可解释性。

    

    在现代数据分析中，当目标数据稀缺而源数据充足，或者源数据和目标数据的分布不同的情况下，转移学习在发挥重要作用。本文提出了一种可解释的统一转移学习模型，称为UTrans，该模型能够检测可转移变量和源数据。具体来说，我们建立了估计误差界限，并证明我们的界限低于仅有目标数据的界限。此外，我们基于假设检验提出了一种源数据检测算法，用于排除不可转移的数据。我们在多个实验中评估和比较了UTrans与现有算法。结果显示，UTrans在保持可解释性的同时，比现有方法具有更低的估计和预测误差。最后，我们将其应用于美国代际流动数据，并将我们提出的算法与经典的机器学习算法进行比较。

    Transfer learning plays a key role in modern data analysis when: (1) the target data are scarce but the source data are sufficient; (2) the distributions of the source and target data are heterogeneous. This paper develops an interpretable unified transfer learning model, termed as UTrans, which can detect both transferable variables and source data. More specifically, we establish the estimation error bounds and prove that our bounds are lower than those with target data only. Besides, we propose a source detection algorithm based on hypothesis testing to exclude the nontransferable data. We evaluate and compare UTrans to the existing algorithms in multiple experiments. It is shown that UTrans attains much lower estimation and prediction errors than the existing methods, while preserving interpretability. We finally apply it to the US intergenerational mobility data and compare our proposed algorithms to the classical machine learning algorithms.
    
[^147]: 使用语言模型的黑盒预测易出错测试修复类别

    Black-Box Prediction of Flaky Test Fix Categories Using Language Models. (arXiv:2307.00012v1 [cs.SE])

    [http://arxiv.org/abs/2307.00012](http://arxiv.org/abs/2307.00012)

    本文提出了一个使用语言模型的框架，可以自动生成易出错测试的标记数据集，并通过分析测试代码来预测测试的修复类别。实验结果表明UniXcoder优于CodeBERT。

    

    易出错测试会在相同软件版本的测试下非确定性地通过或失败，引起混乱并浪费开发者时间。尽管机器学习模型已经被用于预测易出错性及其根本原因，但在提供修复支持方面仍有较少工作。为了填补这一空白，我们提出了一个框架，通过仅分析测试代码自动生成13个修复类别的标记数据集，并训练模型来预测易出错测试的修复类别。虽然在当前阶段准确预测修复本身是不现实的，但这些类别提供了关于需要检查的测试代码部分的精确指导。我们的方法基于语言模型，即CodeBERT和UniXcoder，其输出经过前馈神经网络（FNN）或基于孪生网络的Few Shot Learning（FSL）进行了微调。我们的实验结果表明，UniXcoder在正确预测大多数修复类别方面表现优于CodeBERT。

    Flaky tests are problematic because they non-deterministically pass or fail for the same software version under test, causing confusion and wasting developer time. While machine learning models have been used to predict flakiness and its root causes, there is less work on providing support to fix the problem. To address this gap, we propose a framework that automatically generates labeled datasets for 13 fix categories and train models to predict the fix category of a flaky test by analyzing the test code only. Though it is unrealistic at this stage to accurately predict the fix itself, the categories provide precise guidance about what part of the test code to look at. Our approach is based on language models, namely CodeBERT and UniXcoder, whose output is fine-tuned with a Feed Forward Neural Network (FNN) or a Siamese Network-based Few Shot Learning (FSL). Our experimental results show that UniXcoder outperforms CodeBERT, in correctly predicting most of the categories of fixes a dev
    
[^148]: 何去何从：深度学习加速的数字硬件视角

    To Spike or Not To Spike: A Digital Hardware Perspective on Deep Learning Acceleration. (arXiv:2306.15749v1 [cs.NE])

    [http://arxiv.org/abs/2306.15749](http://arxiv.org/abs/2306.15749)

    神经形态计算旨在通过仿真脑部操作来提高深度学习模型的效率，但是在SNNs的高效硬件后端设计上仍需进一步研究。

    

    随着深度学习模型规模的增加，它们在涵盖计算机视觉到自然语言处理等领域变得越来越有竞争力；然而，这是以效率为代价的，因为它们需要越来越多的内存和计算能力。生物脑的功耗效率超过任何大规模深度学习（DL）模型；因此，神经形态计算试图模仿脑部操作，例如基于脉冲的信息处理，以提高DL模型的效率。尽管脑部有诸如高效的信息传输、密集的神经元连接和计算与存储的共同位置等优势，但可用的生物基底严重限制了生物大脑的进化。电子硬件没有相同的约束；因此，虽然建模脉冲神经网络（SNNs）可能揭示了一个谜题的一部分，但对于SNNs的高效硬件后端设计需要进一步研究。

    As deep learning models scale, they become increasingly competitive from domains spanning computer vision to natural language processing; however, this happens at the expense of efficiency since they require increasingly more memory and computing power. The power efficiency of the biological brain outperforms the one of any large-scale deep learning (DL) model; thus, neuromorphic computing tries to mimic the brain operations, such as spike-based information processing, to improve the efficiency of DL models. Despite the benefits of the brain, such as efficient information transmission, dense neuronal interconnects, and the co-location of computation and memory, the available biological substrate has severely constrained the evolution of biological brains. Electronic hardware does not have the same constraints; therefore, while modeling spiking neural networks (SNNs) might uncover one piece of the puzzle, the design of efficient hardware backends for SNNs needs further investigation, po
    
[^149]: SPRINT：通过语言指令 relabeling 实现可扩展的策略预训练

    SPRINT: Scalable Policy Pre-Training via Language Instruction Relabeling. (arXiv:2306.11886v1 [cs.RO])

    [http://arxiv.org/abs/2306.11886](http://arxiv.org/abs/2306.11886)

    SPRINT 提出了一种离线策略预训练方法，通过指令重标记及离线强化学习实现可扩展的预训练任务，大大减少了预训练所需的人力，同时使机器人能够获取更丰富的技能库，相较于之前的预训练方法，能够更快地学习新的长时间跨度任务。

    

    预训练机器人策略并赋予丰富的技能集合可以大大加速下游任务的学习。先前的研究通过自然语言指令定义预训练任务，但这需要人为地注释数十万个指令。因此，我们提出了 SPRINT，这是一种可扩展的离线策略预训练方法，可大大减少预训练多样的技能所需的人力。我们的方法使用两个核心想法来自动扩展基础预训练任务：通过大型语言模型来进行指令重标记和通过离线强化学习进行交叉轨迹技能链接。因此，SPRINT 预训练可以为机器人装备更丰富的技能库。在家庭模拟器和真实机器人厨房操作任务中的实验结果表明，SPRINT 相对于之前的预训练方法能够更快地学习新的长时间跨度任务。

    Pre-training robot policies with a rich set of skills can substantially accelerate the learning of downstream tasks. Prior works have defined pre-training tasks via natural language instructions, but doing so requires tedious human annotation of hundreds of thousands of instructions. Thus, we propose SPRINT, a scalable offline policy pre-training approach which substantially reduces the human effort needed for pre-training a diverse set of skills. Our method uses two core ideas to automatically expand a base set of pre-training tasks: instruction relabeling via large language models and cross-trajectory skill chaining through offline reinforcement learning. As a result, SPRINT pre-training equips robots with a much richer repertoire of skills. Experimental results in a household simulator and on a real robot kitchen manipulation task show that SPRINT leads to substantially faster learning of new long-horizon tasks than previous pre-training approaches. Website at https://clvrai.com/spr
    
[^150]: AdaStop：用于深度强化学习代理比较的高效可靠序列测试

    AdaStop: sequential testing for efficient and reliable comparisons of Deep RL Agents. (arXiv:2306.10882v1 [cs.LG])

    [http://arxiv.org/abs/2306.10882](http://arxiv.org/abs/2306.10882)

    AdaStop是一种基于多组序列测试的新统计测试方法，可用于比较多个深度强化学习算法来解决实验结果可复制性的问题。

    

    许多深度强化学习实验结果的可复现性受到质疑。为了解决这个可复现性危机，我们提出了一种理论上可靠的方法，用于比较多个深度强化学习算法。由于一个深度强化学习算法的一次执行性能是随机的，所以需要进行独立的多次执行来精确评估它。当比较多个强化学习算法时，一个主要问题是需要进行多少次执行，并且如何确保这样比较的结果在理论上是可靠的。深度强化学习的研究人员通常使用少于5个独立执行来比较算法：我们认为这通常是不够的。而且，当同时比较几个算法时，每个比较的误差都会累积，必须采用多重测试程序来考虑这些误差，以维持低误差保证。为了以统计学上的可靠方式解决这个问题，我们介绍了AdaStop，这是一种基于多组序列测试的新统计测试方法。

    The reproducibility of many experimental results in Deep Reinforcement Learning (RL) is under question. To solve this reproducibility crisis, we propose a theoretically sound methodology to compare multiple Deep RL algorithms. The performance of one execution of a Deep RL algorithm is random so that independent executions are needed to assess it precisely. When comparing several RL algorithms, a major question is how many executions must be made and how can we assure that the results of such a comparison is theoretically sound. Researchers in Deep RL often use less than 5 independent executions to compare algorithms: we claim that this is not enough in general. Moreover, when comparing several algorithms at once, the error of each comparison accumulates and must be taken into account with a multiple tests procedure to preserve low error guarantees. To address this problem in a statistically sound way, we introduce AdaStop, a new statistical test based on multiple group sequential tests
    
[^151]: 在通过分片密文上的同态加密数据上进行高分辨率卷积神经网络的研究

    High-Resolution Convolutional Neural Networks on Homomorphically Encrypted Data via Sharding Ciphertexts. (arXiv:2306.09189v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2306.09189](http://arxiv.org/abs/2306.09189)

    通过分片密文上的同态加密数据，我们扩展了在大尺寸和多通道图像上评估深度卷积神经网络的方法，并简化了多路复用图像格式的效率。我们还展示了如何正则化现有模型以提高效率和准确性。

    

    最近，在使用基于剩余数系统Cheon-Kim-Kim-Song (RNS-CKKS) 同态加密方案的低分辨率数据上，对包括ResNet-20在内的深度卷积神经网络(DCNNs)进行了私密评估。我们扩展了在图像上评估DCNNs的方法，使其适用于更大尺寸和更多通道的图像，超出了单个密文所能存储的范围。此外，我们简化和改进了最近引入的多路复用图像格式的效率，证明了同态评估可以使用标准的行主序矩阵打包，并导致加密推理时间加速$4.6-6.5\times$。我们还展示了如何在训练过程中对现有DCNN模型进行正则化，以进一步提高效率和准确性。应用这些技术对高分辨率的ImageNet数据集上进行同态评估，实现了$80.2\%$的top-1准确率。同时，我们还实现了对CNN进行同态评估的准确率

    Recently, Deep Convolutional Neural Networks (DCNNs) including the ResNet-20 architecture have been privately evaluated on encrypted, low-resolution data with the Residue-Number-System Cheon-Kim-Kim-Song (RNS-CKKS) homomorphic encryption scheme. We extend methods for evaluating DCNNs on images with larger dimensions and many channels, beyond what can be stored in single ciphertexts. Additionally, we simplify and improve the efficiency of the recently introduced multiplexed image format, demonstrating that homomorphic evaluation can work with standard, row-major matrix packing and results in encrypted inference time speedups by $4.6-6.5\times$. We also show how existing DCNN models can be regularized during the training process to further improve efficiency and accuracy. These techniques are applied to homomorphically evaluate a DCNN with high accuracy on the high-resolution ImageNet dataset, achieving $80.2\%$ top-1 accuracy. We also achieve an accuracy of homomorphically evaluated CNN
    
[^152]: DiffECG：ECG信号合成的一般化概率扩散模型

    DiffECG: A Generalized Probabilistic Diffusion Model for ECG Signals Synthesis. (arXiv:2306.01875v1 [cs.CV])

    [http://arxiv.org/abs/2306.01875](http://arxiv.org/abs/2306.01875)

    本文介绍了一种新颖的ECG信号合成方法——基于去噪扩散概率模型的DiffECG，能够涵盖三种情形，并且是ECG合成的第一个广义条件方法。实验证明该方法的有效性以及优于其他ECG生成模型并可提高分类器性能。

    

    近年来，深度生成模型在基于深度学习的ECG信号心脏疾病检测中作为一种有前途的数据增强解决方案备受关注。本文提出一种新颖的基于去噪扩散概率模型的ECG合成方法,覆盖了三种情形：心跳生成、部分信号完成和完整心跳预测。我们的方法是ECG合成的第一个广义条件方法，实验结果表明其对各种ECG相关任务的有效性。此外，我们还展示了我们的方法优于其他最先进的ECG生成模型并可以提高最先进的分类器的性能。

    In recent years, deep generative models have gained attention as a promising data augmentation solution for heart disease detection using deep learning approaches applied to ECG signals. In this paper, we introduce a novel approach based on denoising diffusion probabilistic models for ECG synthesis that covers three scenarios: heartbeat generation, partial signal completion, and full heartbeat forecasting. Our approach represents the first generalized conditional approach for ECG synthesis, and our experimental results demonstrate its effectiveness for various ECG-related tasks. Moreover, we show that our approach outperforms other state-of-the-art ECG generative models and can enhance the performance of state-of-the-art classifiers.
    
[^153]: 利用局部化提高图神经网络的表达能力

    Improving Expressivity of Graph Neural Networks using Localization. (arXiv:2305.19659v1 [cs.LG])

    [http://arxiv.org/abs/2305.19659](http://arxiv.org/abs/2305.19659)

    本文提出了Weisfeiler-Leman (WL)算法的局部版本，用于解决子图计数问题并提高图神经网络的表达能力，同时，也给出了一些时间和空间效率更高的$k-$WL变体和分裂技术。

    

    本文提出了Weisfeiler-Leman (WL)算法的局部版本，旨在增加表达能力并减少计算负担。我们专注于子图计数问题，并为任意$k$给出$k-$WL的局部版本。我们分析了Local $k-$WL的作用，并证明其比$k-$WL更具表现力，并且至多与$(k+1)-$WL一样具有表现力。我们给出了一些模式的特征，如果两个图是Local $k-$WL等价的，则它们的子图和诱导子图的计数是不变的。我们还介绍了$k-$WL的两个变体：层$k-$WL和递归$k-$WL。这些方法的时间和空间效率比在整个图上应用$k-$WL更高。我们还提出了一种分裂技术，使用$1-$WL即可保证所有大小不超过4的诱导子图的准确计数。相同的方法可以使用$k>1$进一步扩展到更大的模式。我们还将Local $k-$WL的表现力与其他GNN层次结构进行了比较。

    In this paper, we propose localized versions of Weisfeiler-Leman (WL) algorithms in an effort to both increase the expressivity, as well as decrease the computational overhead. We focus on the specific problem of subgraph counting and give localized versions of $k-$WL for any $k$. We analyze the power of Local $k-$WL and prove that it is more expressive than $k-$WL and at most as expressive as $(k+1)-$WL. We give a characterization of patterns whose count as a subgraph and induced subgraph are invariant if two graphs are Local $k-$WL equivalent. We also introduce two variants of $k-$WL: Layer $k-$WL and recursive $k-$WL. These methods are more time and space efficient than applying $k-$WL on the whole graph. We also propose a fragmentation technique that guarantees the exact count of all induced subgraphs of size at most 4 using just $1-$WL. The same idea can be extended further for larger patterns using $k>1$. We also compare the expressive power of Local $k-$WL with other GNN hierarc
    
[^154]: 探索长尾识别问题中的权重平衡

    Exploring Weight Balancing on Long-Tailed Recognition Problem. (arXiv:2305.16573v1 [cs.LG])

    [http://arxiv.org/abs/2305.16573](http://arxiv.org/abs/2305.16573)

    研究分析了新提出的权重平衡方法在长尾识别问题中有效的原因，发现其能够缓解神经崩溃和圆锥效应，从而提高识别性能。

    

    长尾数据中的识别问题最近变得越来越重要，因为数据集中每个类别的样本数量分布通常是指数分布，除非有意地调整样本数量。针对这些问题已经提出了各种方法。最近，提出了权重平衡方法，它结合了著名的经典正则化技术和两阶段训练。尽管其简单性，但已知其对现有各种不同方法具有高性能。然而，我们缺乏为什么这种方法对长尾数据有效的理解。在这项研究中，我们分析了该方法，并关注了神经崩溃和每个训练阶段的圆锥效应，并发现它可以分解为由权值衰减和交叉熵损失引起的特征提取器中Fisher判别比的增加以及由权重衰减和类平衡正则化引起的隐式逻辑调整。我们还证明了权重平衡方法成功缓解了神经崩溃和圆锥效应，从而提高了长尾数据的识别性能。

    Recognition problems in long-tailed data, where the sample size per class is heavily skewed, have recently gained importance because the distribution of the sample size per class in a dataset is generally exponential unless the sample size is intentionally adjusted. Various approaches have been devised to address these problems. Recently, weight balancing, which combines well-known classical regularization techniques with two-stage training, has been proposed. Despite its simplicity, it is known for its high performance against existing methods devised in various ways. However, there is a lack of understanding as to why this approach is effective for long-tailed data. In this study, we analyze the method focusing on neural collapse and cone effect at each training stage and find that it can be decomposed into the increase in Fisher's discriminant ratio of the feature extractor caused by weight decay and cross entropy loss and implicit logit adjustment caused by weight decay and class-b
    
[^155]: 具有双多模态编码器的组合图像检索候选集重排序

    Candidate Set Re-ranking for Composed Image Retrieval with Dual Multi-modal Encoder. (arXiv:2305.16304v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.16304](http://arxiv.org/abs/2305.16304)

    本论文提出了一种使用两阶段模式结合预先计算图像嵌入和参考文本-候选项三元组交互选择的方式进行组合图像检索候选集重排序的方法。

    

    组合图像检索旨在找到最匹配给定多模态用户查询(包括参考图像和文本对)的图像。现有方法通常预先计算整个语料库的图像嵌入，并在测试时将这些嵌入与经过查询文本修改的参考图像嵌入进行比较。然而，仅通过短文本描述引导修改参考图像嵌入可能很困难，特别是独立于潜在的候选项。一种替代方法是允许查询和每个可能的候选项之间的交互，即参考文本-候选项三元组，并从整个集合中选择最佳匹配。虽然这种方法更具有判别性，但对于大规模数据集，由于不能预先计算候选嵌入，因此计算成本是禁止性的。我们提出使用两阶段模式结合这两个方案的优点

    Composed image retrieval aims to find an image that best matches a given multi-modal user query consisting of a reference image and text pair. Existing methods commonly pre-compute image embeddings over the entire corpus and compare these to a reference image embedding modified by the query text at test time. Such a pipeline is very efficient at test time since fast vector distances can be used to evaluate candidates, but modifying the reference image embedding guided only by a short textual description can be difficult, especially independent of potential candidates. An alternative approach is to allow interactions between the query and every possible candidate, i.e., reference-text-candidate triplets, and pick the best from the entire set. Though this approach is more discriminative, for large-scale datasets the computational cost is prohibitive since pre-computation of candidate embeddings is no longer possible. We propose to combine the merits of both schemes using a two-stage mode
    
[^156]: 神经元元胞自动机能够对信号作出响应

    Neural Cellular Automata Can Respond to Signals. (arXiv:2305.12971v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2305.12971](http://arxiv.org/abs/2305.12971)

    神经元元胞自动机对信号作出响应的能力，为神经元元胞自动机作为人工形态发生模型的发展提供了基础，并且为将动态行为嵌入模型铺平了道路。

    

    神经元元胞自动机（NCAs）是一种形态发生模型，能够从单个种子细胞生长出二维人工生物体。本文展示了NCAs可以被训练成对信号作出响应。使用了两种类型的信号：内部（基因编码）信号和外部（环境）信号。信号被呈现给单个像素在一个时间步长内。结果显示，NCAs能够根据内部信号发展成多种不同形态，并且能够根据外部信号改变颜色。总体上，这些结果为NCAs作为人工形态发生模型的发展贡献了，并为将动态行为嵌入NCA模型奠定了基础。通过GitHub可以获得代码和目标图像：https://github.com/jstovold/ALIFE2023

    Neural Cellular Automata (NCAs) are a model of morphogenesis, capable of growing two-dimensional artificial organisms from a single seed cell. In this paper, we show that NCAs can be trained to respond to signals. Two types of signal are used: internal (genomically-coded) signals, and external (environmental) signals. Signals are presented to a single pixel for a single timestep.  Results show NCAs are able to grow into multiple distinct forms based on internal signals, and are able to change colour based on external signals. Overall these contribute to the development of NCAs as a model of artificial morphogenesis, and pave the way for future developments embedding dynamic behaviour into the NCA model.  Code and target images are available through GitHub: https://github.com/jstovold/ALIFE2023
    
[^157]: GraVAC：适应性压缩用于高效的分布式深度学习训练

    GraVAC: Adaptive Compression for Communication-Efficient Distributed DL Training. (arXiv:2305.12201v1 [cs.LG])

    [http://arxiv.org/abs/2305.12201](http://arxiv.org/abs/2305.12201)

    GraVAC提出了一个动态调整压缩因子的框架，通过评估模型进展和评估与压缩相关的梯度信息损失来进行训练。GraVAC可以在不需要任何关于模型或其超参数的先前假设的情况下，达到与先前最先进的压缩方法相同或更好的翻译准确性。在CIFAR-10和ImageNet数据集上，相对于静态压缩对应物，GraVAC可以将通信减少高达87％和75％。

    

    分布式数据并行（DDP）训练通过多个设备在数据子集上进行训练并聚合更新来提高应用程序的整体吞吐量。每次迭代的周期性同步产生了相当大的开销，受最先进的神经网络越来越大和复杂的影响更加严重。尽管许多梯度压缩技术提出了减少通信成本的方式，但最佳压缩因子导致最大速度提升或最小数据交换的问题仍然是一个开放式的问题，因为它与压缩质量、模型大小和结构、硬件、网络拓扑和带宽有关。我们提出了GraVAC，一个动态调整压缩因子的框架，通过评估模型进展和评估与压缩相关的梯度信息损失来进行训练。GraVAC以在线的黑盒方式工作，不需要任何关于模型或其超参数的先前假设，同时实现与先前最先进的压缩方法相同或更好的翻译准确性。对于CIFAR-10和ImageNet数据集，相对于其静态压缩对应物，GraVAC将通信减少了高达87％和75％，而收敛精度相同。

    Distributed data-parallel (DDP) training improves overall application throughput as multiple devices train on a subset of data and aggregate updates to produce a globally shared model. The periodic synchronization at each iteration incurs considerable overhead, exacerbated by the increasing size and complexity of state-of-the-art neural networks. Although many gradient compression techniques propose to reduce communication cost, the ideal compression factor that leads to maximum speedup or minimum data exchange remains an open-ended problem since it varies with the quality of compression, model size and structure, hardware, network topology and bandwidth. We propose GraVAC, a framework to dynamically adjust compression factor throughout training by evaluating model progress and assessing gradient information loss associated with compression. GraVAC works in an online, black-box manner without any prior assumptions about a model or its hyperparameters, while achieving the same or better
    
[^158]: 实际交通标志改变对YOLOv7-目标识别模型的影响

    Effects of Real-Life Traffic Sign Alteration on YOLOv7- an Object Recognition Model. (arXiv:2305.05499v1 [cs.CV])

    [http://arxiv.org/abs/2305.05499](http://arxiv.org/abs/2305.05499)

    本研究调查了改变后的交通标志对目标识别模型准确性和性能的影响，结果表明当暴露于不太可能的条件下修改后的交通标志时，物体检测模型的准确率显著降低。

    

    图像处理技术的进步导致了物体识别(OR)模型在各种应用中的广泛使用，如机场安全和邮件分拣。这些模型已成为AI能力的标志，并支持着国家邮政运营等重要服务。然而，OR模型的性能可能会受到现实场景的影响，例如交通标志改变。因此，本研究调查了改变后的交通标志对目标识别模型准确性和性能的影响。为此，使用公开数据集创建了不同类型的交通标志修改，包括大小、形状、颜色、可见性和角度的改变，并分析这些修改对YOLOv7 (You Only Look Once)模型的检测和分类能力的影响。结果表明，当暴露于不太可能的条件下修改后的交通标志时，物体检测模型的准确率显著降低。

    The advancement of Image Processing has led to the widespread use of Object Recognition (OR) models in various applications, such as airport security and mail sorting. These models have become essential in signifying the capabilities of AI and supporting vital services like national postal operations. However, the performance of OR models can be impeded by real-life scenarios, such as traffic sign alteration. Therefore, this research investigates the effects of altered traffic signs on the accuracy and performance of object recognition models. To this end, a publicly available dataset was used to create different types of traffic sign alterations, including changes to size, shape, color, visibility, and angles. The impact of these alterations on the YOLOv7 (You Only Look Once) model's detection and classification abilities were analyzed. It reveals that the accuracy of object detection models decreases significantly when exposed to modified traffic signs under unlikely conditions. This
    
[^159]: 通用图上的自我排斥随机游走 - 通过非线性马尔可夫链实现最小采样方差

    Self-Repellent Random Walks on General Graphs - Achieving Minimal Sampling Variance via Nonlinear Markov Chains. (arXiv:2305.05097v1 [math.PR])

    [http://arxiv.org/abs/2305.05097](http://arxiv.org/abs/2305.05097)

    本文设计了一种自我排斥随机游走模型，可实现较小的渐近采样方差，适用于网络拓扑的采样和邻域探索。

    

    我们考虑在离散状态空间上的随机游走，例如一般的无向图，其中随机游走设计成通过采样和邻域探索来逼近网络拓扑上的目标量，以马尔可夫链蒙特卡罗 (MCMC) 程序的形式进行。对于任何相应于目标概率分布的马尔可夫链，我们设计了一种自我排斥随机游走 (SRRW)，它不太可能转移到过去被高度访问的节点，而更可能转移到很少被访问的节点。对于一类由正实数 {\alpha} 参数化的 SRRW，我们证明了该过程的经验分布几乎肯定收敛于底层马尔可夫链内核的目标 (平稳) 分布。然后我们提供了一个中心极限定理，并推导出所得到的渐近协方差矩阵的精确形式，这使我们能够表明，具有更强的排斥作用 (较大的 {\alpha}) 的 SRRW 一定比具有较弱的排斥作用 (较小的 {\alpha}) 的 SRRW 实现更小的渐近采样方差。

    We consider random walks on discrete state spaces, such as general undirected graphs, where the random walkers are designed to approximate a target quantity over the network topology via sampling and neighborhood exploration in the form of Markov chain Monte Carlo (MCMC) procedures. Given any Markov chain corresponding to a target probability distribution, we design a self-repellent random walk (SRRW) which is less likely to transition to nodes that were highly visited in the past, and more likely to transition to seldom visited nodes. For a class of SRRWs parameterized by a positive real {\alpha}, we prove that the empirical distribution of the process converges almost surely to the the target (stationary) distribution of the underlying Markov chain kernel. We then provide a central limit theorem and derive the exact form of the arising asymptotic co-variance matrix, which allows us to show that the SRRW with a stronger repellence (larger {\alpha}) always achieves a smaller asymptotic
    
[^160]: 关于神经网络原像近似的研究

    On Preimage Approximation for Neural Networks. (arXiv:2305.03686v1 [cs.SE])

    [http://arxiv.org/abs/2305.03686](http://arxiv.org/abs/2305.03686)

    本文提出了一种基于线性松弛的高效实用的任意时刻算法，用于生成神经网络原像的符号下近似，以实现更快的改进和更高的压缩度。

    

    神经网络验证主要关注局部鲁棒性，然而，通常需要知道给定属性是否在整个输入域内全局成立，如果不成立，则需要知道属性成立的输入比例是多少。尽管精确的原像生成可以构建神经网络的等价表示，但在规模上是难以处理的。本文提出了一种基于线性松弛的高效实用的任意时刻算法，用于生成神经网络原像的符号下近似。我们的算法通过将输入区域划分为子区域，其中神经网络松弛边界变得更紧，迭代地最小化体积逼近误差。我们进一步采用采样和可微体积逼近来优先划分区域，并优化松弛的参数，从而实现更快的改进和更高的压缩度。

    Neural network verification mainly focuses on local robustness properties. However, often it is important to know whether a given property holds globally for the whole input domain, and if not then for what proportion of the input the property is true. While exact preimage generation can construct an equivalent representation of neural networks that can aid such (quantitative) global robustness verification, it is intractable at scale. In this work, we propose an efficient and practical anytime algorithm for generating symbolic under-approximations of the preimage of neural networks based on linear relaxation. Our algorithm iteratively minimizes the volume approximation error by partitioning the input region into subregions, where the neural network relaxation bounds become tighter. We further employ sampling and differentiable approximations to the volume in order to prioritize regions to split and optimize the parameters of the relaxation, leading to faster improvement and more compa
    
[^161]: AutoColor: 针对多色全息图的学习光功率控制

    AutoColor: Learned Light Power Control for Multi-Color Holograms. (arXiv:2305.01611v1 [cs.CV])

    [http://arxiv.org/abs/2305.01611](http://arxiv.org/abs/2305.01611)

    AutoColor 是第一个学习正确照明多色全息图所需光源功率的方法，将优化多色全息图所需的步骤数从超过1000个降至70个迭代步骤。

    

    多色全息图需要多个光源同时照射才能形成图形。这种全息图可以比传统单色全息图更好地利用光源，并提高全息显示的动态范围。我们引入了\projectname，这是一种学习方法，用于估计照明多色全息图所需的最佳光源功率。为此，我们使用合成图像和其深度信息建立了第一个多色全息图数据集。我们使用一种流行的流程结合生成模型、大语言和单眼深度估计模型来生成这些合成图像。最后，我们使用数据集训练我们的学习模型，并实验性地证明，\projectname可以将优化多色全息图所需的步骤数从超过1000个降至70个迭代步骤，而不会影响图像质量。

    Multi-color holograms rely on simultaneous illumination from multiple light sources. These multi-color holograms could utilize light sources better than conventional single-color holograms and can improve the dynamic range of holographic displays. In this letter, we introduce \projectname, the first learned method for estimating the optimal light source powers required for illuminating multi-color holograms. For this purpose, we establish the first multi-color hologram dataset using synthetic images and their depth information. We generate these synthetic images using a trending pipeline combining generative, large language, and monocular depth estimation models. Finally, we train our learned model using our dataset and experimentally demonstrate that \projectname significantly decreases the number of steps required to optimize multi-color holograms from $>1000$ to $70$ iteration steps without compromising image quality.
    
[^162]: 可行性策略迭代

    Feasible Policy Iteration. (arXiv:2304.08845v1 [cs.LG])

    [http://arxiv.org/abs/2304.08845](http://arxiv.org/abs/2304.08845)

    可行性策略迭代 (FPI) 是一个间接的安全强化学习方法，使用上一个策略的可行域来迭代地限制当前策略。可行性策略改进是其核心，它在可行域内最大化回报，在可行域外最小化约束衰减函数 (CDF).

    

    安全强化学习旨在在安全约束下解决最优控制问题。现有的 $\textit{直接}$ 安全强化学习方法会在整个学习过程中一直使用原始约束。它们或者缺乏策略迭代期间的理论保证，或者遭遇不可行性问题。为了解决这个问题，我们提出了一个叫做可行性策略迭代（FPI）的 $\textit{间接}$ 安全强化学习方法，它使用最后一个策略的可行域来迭代地限制当前策略。可行域由一个叫做约束衰减函数（CDF）的可行性函数表示。FPI 的核心是一个叫做可行性策略改进的区域性策略更新规则，它在可行域内最大化回报，在可行域外最小化 CDF。这个更新规则总是可行的，并确保可行域单调地扩展，状态值函数单调地增长。

    Safe reinforcement learning (RL) aims to solve an optimal control problem under safety constraints. Existing $\textit{direct}$ safe RL methods use the original constraint throughout the learning process. They either lack theoretical guarantees of the policy during iteration or suffer from infeasibility problems. To address this issue, we propose an $\textit{indirect}$ safe RL method called feasible policy iteration (FPI) that iteratively uses the feasible region of the last policy to constrain the current policy. The feasible region is represented by a feasibility function called constraint decay function (CDF). The core of FPI is a region-wise policy update rule called feasible policy improvement, which maximizes the return under the constraint of the CDF inside the feasible region and minimizes the CDF outside the feasible region. This update rule is always feasible and ensures that the feasible region monotonically expands and the state-value function monotonically increases inside 
    
[^163]: 模型稀疏化可以简化机器反学习

    Model sparsification can simplify machine unlearning. (arXiv:2304.04934v1 [cs.LG])

    [http://arxiv.org/abs/2304.04934](http://arxiv.org/abs/2304.04934)

    本文提出了一种基于模型稀疏化的机器反学习方案，称为prune first, then unlearn和sparsity-aware unlearning。此方案可以提高近似反学习器的多标准反学习性能，并在不同的场景中表现出一致的效果。

    

    最近的数据管制要求机器反学习（MU）：从模型中移除指定样例的影响。虽然可以通过使用剩余数据从头开始进行模型重新训练来进行精确反学习，但是其计算成本导致了近似但高效的反学习方案的开发。除了数据中心的MU解决方案，我们通过一种新颖的基于模型的视角推进MU：通过权值修剪进行稀疏化。我们的理论和实践结果表明，模型稀疏性可以提高近似反学习器的多标准反学习性能，缩小近似间隙，同时保持高效。有了这个认识，我们制定了两个新的稀疏感知反学习元方案，称为“先修剪，然后反学习”和“稀疏感知反学习”。广泛的实验表明，我们的发现和提议在各种场景下始终有益于MU，包括按类数据擦除、随机数据擦除和后门数据伪造等。

    Recent data regulations necessitate machine unlearning (MU): The removal of the effect of specific examples from the model. While exact unlearning is possible by conducting a model retraining with the remaining data from scratch, its computational cost has led to the development of approximate but efficient unlearning schemes. Beyond data-centric MU solutions, we advance MU through a novel model-based viewpoint: sparsification via weight pruning. Our results in both theory and practice indicate that model sparsity can boost the multi-criteria unlearning performance of an approximate unlearner, closing the approximation gap, while continuing to be efficient. With this insight, we develop two new sparsity-aware unlearning meta-schemes, termed `prune first, then unlearn' and `sparsity-aware unlearning'. Extensive experiments show that our findings and proposals consistently benefit MU in various scenarios, including class-wise data scrubbing, random data scrubbing, and backdoor data forge
    
[^164]: 面向不完整数据的锥束CT重建的基于立方体的3D去噪扩散概率模型

    Cube-Based 3D Denoising Diffusion Probabilistic Model for Cone Beam Computed Tomography Reconstruction with Incomplete Data. (arXiv:2303.12861v1 [eess.IV])

    [http://arxiv.org/abs/2303.12861](http://arxiv.org/abs/2303.12861)

    本文提出了一种基于立方体的3D去噪扩散概率模型（DDPM）来重建CBCT并解决了存储整个正弦图的内存问题。通过将整个CBCT volume分成多个小立方体，该模型能够实现高效的计算并在视觉和定量评价方面优于现有方法。

    

    深度学习在计算机断层摄影（CT）重建中获得了广泛的研究，特别是在稀疏视图CT重建中。然而，将DL应用于稀疏视图锥束CT（CBCT）仍然具有挑战性。本文提出了一种基于立方体的3D去噪扩散概率模型（DDPM）来重建CBCT，并解决了存储整个正弦图的内存问题。我们的方法将整个CBCT volume分成多个小立方体，以实现高效的计算。实验结果表明，该方法在视觉和定量评价方面优于现有方法。

    Deep learning (DL) has been extensively researched in the field of computed tomography (CT) reconstruction with incomplete data, particularly in sparse-view CT reconstruction. However, applying DL to sparse-view cone beam CT (CBCT) remains challenging. Many models learn the mapping from sparse-view CT images to ground truth but struggle to achieve satisfactory performance in terms of global artifact removal. Incorporating sinogram data and utilizing dual-domain information can enhance anti-artifact performance, but this requires storing the entire sinogram in memory. This presents a memory issue for high-resolution CBCT sinograms, limiting further research and application. In this paper, we propose a cube-based 3D denoising diffusion probabilistic model (DDPM) for CBCT reconstruction using down-sampled data. A DDPM network, trained on cubes extracted from paired fully sampled sinograms and down-sampled sinograms, is employed to inpaint down-sampled sinograms. Our method divides the ent
    
[^165]: Magnushammer: 一种基于Transformer的前提选择方法

    Magnushammer: A Transformer-based Approach to Premise Selection. (arXiv:2303.04488v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04488](http://arxiv.org/abs/2303.04488)

    Magnushammer是一种基于Transformer的前提选择方法，通过在PISA基准上的测试表明，它可以大幅度超越传统符号系统，并将先前最先进的证明率从57.0％提高到71.0％。

    

    前提选择是自动定理证明的一个基本问题。以往的方法常常使用复杂的符号方法，依赖领域知识，并需要大量的工程工作来解决这个任务。在这项工作中，我们展示了基于神经转换器的Magnushammer方法可以大幅度地超越传统的符号系统。通过在PISA基准上的测试，Magnushammer的证明率达到了59.5％，而最成熟和流行的基于符号的求解器Sledgehammer的证明率只有38.3％。此外，通过将Magnushammer与基于语言模型的神经形式证明器相结合，我们将先前最先进的证明率从57.0％大幅提高到71.0％。

    Premise selection is a fundamental problem of automated theorem proving. Previous works often use intricate symbolic methods, rely on domain knowledge, and require significant engineering effort to solve this task. In this work, we show that Magnushammer, a neural transformer-based approach, can outperform traditional symbolic systems by a large margin. Tested on the PISA benchmark, Magnushammer achieves $59.5\%$ proof rate compared to a $38.3\%$ proof rate of Sledgehammer, the most mature and popular symbolic-based solver. Furthermore, by combining Magnushammer with a neural formal prover based on a language model, we significantly improve the previous state-of-the-art proof rate from $57.0\%$ to $71.0\%$.
    
[^166]: 评估使用模型无关的度量标准解释机器学习预测的可解释性

    Evaluating explainability for machine learning predictions using model-agnostic metrics. (arXiv:2302.12094v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12094](http://arxiv.org/abs/2302.12094)

    本文提出了一种使用模型无关的度量标准，用于评估机器学习模型的预测结果的可解释性。这些度量标准将各个解释能力方面总结成标量，提供全面的理解并促进决策者和利益相关者之间的沟通，从而提高整体的透明度。

    

    人工智能技术的快速发展带来了管理和监管方面的众多挑战。人工智能系统正在被整合到各个行业和领域，决策者需全面细致地了解这些系统的能力和限制。这个需求的一个关键方面是能够解释机器学习模型的结果，这对于提高透明度和信任度以及帮助模型在道德上进行训练至关重要。本文提出了新颖的度量标准，用于量化AI模型预测结果是否可以通过其特征进行易于解释。我们的度量标准将解释能力的不同方面总结为标量，提供对模型预测的更全面的理解，促进决策者和利益相关者之间的沟通，从而提高整体的透明度。

    Rapid advancements in artificial intelligence (AI) technology have brought about a plethora of new challenges in terms of governance and regulation. AI systems are being integrated into various industries and sectors, creating a demand from decision-makers to possess a comprehensive and nuanced understanding of the capabilities and limitations of these systems. One critical aspect of this demand is the ability to explain the results of machine learning models, which is crucial to promoting transparency and trust in AI systems, as well as fundamental in helping machine learning models to be trained ethically. In this paper, we present novel metrics to quantify the degree of which AI model predictions can be easily explainable by its features. Our metrics summarize different aspects of explainability into scalars, providing a more comprehensive understanding of model predictions and facilitating communication between decision-makers and stakeholders, thereby increasing the overall transp
    
[^167]: 模块化深度学习

    Modular Deep Learning. (arXiv:2302.11529v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11529](http://arxiv.org/abs/2302.11529)

    模块化深度学习是一种有前景的解决方案，通过将计算与路由和局部更新模块分离，实现了积极迁移和系统化的推广。

    

    迁移学习近年来已成为机器学习的主要范式，预训练模型在下游任务中经过微调能够以更少的标记示例获得更好的性能。然而，如何开发能够专注于多个任务而不会产生负面干扰，并且能够系统地推广到非相同分布任务仍然不清楚。模块化深度学习已经成为应对这些挑战的一种有希望的解决方案。在这个框架中，计算单元通常被实现为自主参数高效的模块。信息被有条件地路由到一部分模块，然后进行汇总。这些特性通过将计算与路由和局部更新模块分离，实现了积极迁移和系统化的推广。我们提供了对模块化架构的调查，提供了在科学文献中独立演化的几个研究方向的统一视角。此外，我们还探索了各种附加...

    Transfer learning has recently become the dominant paradigm of machine learning. Pre-trained models fine-tuned for downstream tasks achieve better performance with fewer labelled examples. Nonetheless, it remains unclear how to develop models that specialise towards multiple tasks without incurring negative interference and that generalise systematically to non-identically distributed tasks. Modular deep learning has emerged as a promising solution to these challenges. In this framework, units of computation are often implemented as autonomous parameter-efficient modules. Information is conditionally routed to a subset of modules and subsequently aggregated. These properties enable positive transfer and systematic generalisation by separating computation from routing and updating modules locally. We offer a survey of modular architectures, providing a unified view over several threads of research that evolved independently in the scientific literature. Moreover, we explore various addi
    
[^168]: 用混合神经网络SBERT-CNN检测Reddit用户的抑郁症状态

    Detecting Reddit Users with Depression Using a Hybrid Neural Network SBERT-CNN. (arXiv:2302.02759v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.02759](http://arxiv.org/abs/2302.02759)

    本论文介绍了一种混合神经网络模型，结合了预训练的SBERT和CNN，用于通过分析Reddit用户的帖子自动识别抑郁症患者。

    

    抑郁症是一种广泛存在的心理健康问题，影响了全球估计3.8%的人口。它也是全球残疾的主要因素之一。最近，个人越来越喜欢使用社交媒体平台（如Reddit）来表达他们的困难和健康问题（如抑郁症），并在在线社区寻求其他用户的支持。这为通过分析数百万帖子以寻找潜在的干预机会，自动识别具有抑郁症的社交媒体用户提供了巨大的机会。深度学习方法因其易用性、高效处理能力和在许多自然语言处理（NLP）任务上的最新结果而在机器学习和自然语言处理领域开始占据主导地位。在这项工作中，我们提出了一种混合深度学习模型，将预训练的句子BERT（SBERT）和卷积神经网络（CNN）结合起来，以通过他们在Reddit上的帖子检测抑郁症患者。句子BERT用于学习句子的意思。

    Depression is a widespread mental health issue, affecting an estimated 3.8% of the global population. It is also one of the main contributors to disability worldwide. Recently it is becoming popular for individuals to use social media platforms (e.g., Reddit) to express their difficulties and health issues (e.g., depression) and seek support from other users in online communities. It opens great opportunities to automatically identify social media users with depression by parsing millions of posts for potential interventions. Deep learning methods have begun to dominate in the field of machine learning and natural language processing (NLP) because of their ease of use, efficient processing, and state-of-the-art results on many NLP tasks. In this work, we propose a hybrid deep learning model which combines a pretrained sentence BERT (SBERT) and convolutional neural network (CNN) to detect individuals with depression with their Reddit posts. The sentence BERT is used to learn the meaning
    
[^169]: 机器人与人类表征的对齐

    Aligning Robot and Human Representations. (arXiv:2302.01928v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2302.01928](http://arxiv.org/abs/2302.01928)

    本文研究了机器人与人类表征之间的对齐问题，指出了当前学习方法存在的表示不对齐的困境，并建议应将机器人表征学习方法从实现任务目标的角度转向与人类表征对齐的问题。

    

    为了在世界中行动，机器人依赖于一个凸显任务关键方面的表示：例如，为了搬运咖啡杯，机器人可能会考虑动作效率或杯子的方向。然而，如果我们希望机器人为人类而行动，它们的表示不能只是功能性的，还必须反映人类关心的事物，即它们必须对齐。我们观察到当前的学习方法存在表示不对齐的问题，即机器人学习的表示不能捕捉到人类的表示。我们认为，因为人类是机器人表现的最终评估者，所以我们必须明确地将我们的努力集中在与人类的表征对齐上，而不仅仅是学习下游任务。我们提倡从对表征对齐目标的完成程度的角度研究当前机器人表征学习方法。我们在数学上定义了这个问题，并确定了它的关键要求。

    To act in the world, robots rely on a representation of salient task aspects: for example, to carry a coffee mug, a robot may consider movement efficiency or mug orientation in its behavior. However, if we want robots to act for and with people, their representations must not be just functional but also reflective of what humans care about, i.e. they must be aligned. We observe that current learning approaches suffer from representation misalignment, where the robot's learned representation does not capture the human's representation. We suggest that because humans are the ultimate evaluator of robot performance, we must explicitly focus our efforts on aligning learned representations with humans, in addition to learning the downstream task. We advocate that current representation learning approaches in robotics should be studied from the perspective of how well they accomplish the objective of representation alignment. We mathematically define the problem, identify its key desiderata,
    
[^170]: AutoPEFT：用于参数高效微调的自动配置搜索

    AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning. (arXiv:2301.12132v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.12132](http://arxiv.org/abs/2301.12132)

    AutoPEFT是一个自动化的PEFT（参数高效微调）配置搜索方法，它能够自动地找到最佳的PEFT模块和体系结构，以优化任务的性能和参数效率。在典型的NLP任务中，AutoPEFT表现出比手动设计更好的性能。

    

    大型预训练语言模型通过专门的微调用于下游NLP任务，但这样的过程可能很昂贵。最近，参数高效微调（PEFT）方法通过更新比完整模型微调（FFT）少得多的参数，实现了强大的任务性能。然而，在PEFT配置方面做出明智的设计选择是不容易的，例如它们的体系结构、可调参数的数量，甚至是PEFT模块插入的图层。因此，目前的手动设计配置很可能在性能效率权衡方面是次优的。受神经架构搜索的进展启发，我们提出了AutoPEFT来自动选择PEFT配置：首先设计具有多个代表性PEFT模块的表达配置搜索空间。然后使用多目标贝叶斯优化进行低成本的设置，从而发现优化任务性能和参数效率的Pareto优化配置。我们在几个典型的NLP任务，包括文本分类、问答和命名实体识别上评估了AutoPEFT，并展示了其优于手动设计基线的性能。

    Large pretrained language models are widely used in downstream NLP tasks via task-specific fine-tuning, but such procedures can be costly. Recently, Parameter-Efficient Fine-Tuning (PEFT) methods have achieved strong task performance while updating a much smaller number of parameters compared to full model fine-tuning (FFT). However, it is non-trivial to make informed design choices on the PEFT configurations, such as their architecture, the number of tunable parameters, and even the layers in which the PEFT modules are inserted. Consequently, it is highly likely that the current, manually designed configurations are suboptimal in terms of their performance-efficiency trade-off. Inspired by advances in neural architecture search, we propose AutoPEFT for automatic PEFT configuration selection: we first design an expressive configuration search space with multiple representative PEFT modules as building blocks. Using multi-objective Bayesian optimisation in a low-cost setup, we then disc
    
[^171]: ScaDLES: 边缘端流式数据上的可扩展深度学习

    ScaDLES: Scalable Deep Learning over Streaming data at the Edge. (arXiv:2301.08897v1 [cs.DC] CROSS LISTED)

    [http://arxiv.org/abs/2301.08897](http://arxiv.org/abs/2301.08897)

    ScaDLES是一种用于边缘端流式数据上的可扩展深度学习方法，解决了系统和统计异质性的挑战。

    

    分布式深度学习（DDL）训练系统设计用于云和数据中心环境，假设具有均匀计算资源、高网络带宽、足够的内存和存储，以及在所有节点上独立和同分布的数据。然而，这些假设在边缘端不一定适用，特别是在在线方式下训练神经网络时。边缘计算面临系统和统计异质性的挑战。系统异质性归因于每个设备的计算资源和带宽的差异，而统计异质性则来自于边缘端的不平衡和偏斜数据。在处理流式数据时，设备之间的不同流速也可以成为异质性的另一个来源。如果流速低于训练批量大小，设备需要等待足够的样本流入后才能执行一次随机梯度下降（SGD）迭代。

    Distributed deep learning (DDL) training systems are designed for cloud and data-center environments that assumes homogeneous compute resources, high network bandwidth, sufficient memory and storage, as well as independent and identically distributed (IID) data across all nodes. However, these assumptions don't necessarily apply on the edge, especially when training neural networks on streaming data in an online manner. Computing on the edge suffers from both systems and statistical heterogeneity. Systems heterogeneity is attributed to differences in compute resources and bandwidth specific to each device, while statistical heterogeneity comes from unbalanced and skewed data on the edge. Different streaming-rates among devices can be another source of heterogeneity when dealing with streaming data. If the streaming rate is lower than training batch-size, device needs to wait until enough samples have streamed in before performing a single iteration of stochastic gradient descent (SGD).
    
[^172]: 一种在对抗机器学习中出现的非局部周长的伽玛收敛性

    Gamma-convergence of a nonlocal perimeter arising in adversarial machine learning. (arXiv:2211.15223v4 [math.AP] UPDATED)

    [http://arxiv.org/abs/2211.15223](http://arxiv.org/abs/2211.15223)

    本文研究了在对抗机器学习中出现的非局部周长的伽玛收敛性问题，证明了其与局部各向异性周长的伽玛收敛。该工作对于理解对抗性训练在二元分类中的正则化效果以及相关优化问题具有重要意义。

    

    本文证明了一种Minkowski类型的非局部周长对于一个局部各向异性周长的伽玛收敛。这种非局部模型描述了对抗性训练在二元分类中的正则化效果。能量的实质依赖于两个分布之间的相互作用，这两个分布模拟了关联类别的可能性。我们通过假设它们具有有界的BV密度，克服了对分布的典型严格正则性假设。在紧致度量空间自然拓扑上，我们证明了它与两个密度的各向异性函数确定的加权周长的伽玛收敛。尽管是局部的，这个锐利界面极限反映了对抗性扰动下的分类稳定性。我们进一步应用我们的结果推导出了关联总变差的伽玛收敛，研究了对抗性训练的渐近行为，并证明了非局部周长的图离散的伽玛收敛。

    In this paper we prove Gamma-convergence of a nonlocal perimeter of Minkowski type to a local anisotropic perimeter. The nonlocal model describes the regularizing effect of adversarial training in binary classifications. The energy essentially depends on the interaction between two distributions modelling likelihoods for the associated classes. We overcome typical strict regularity assumptions for the distributions by only assuming that they have bounded $BV$ densities. In the natural topology coming from compactness, we prove Gamma-convergence to a weighted perimeter with weight determined by an anisotropic function of the two densities. Despite being local, this sharp interface limit reflects classification stability with respect to adversarial perturbations. We further apply our results to deduce Gamma-convergence of the associated total variations, to study the asymptotics of adversarial training, and to prove Gamma-convergence of graph discretizations for the nonlocal perimeter.
    
[^173]: 对基于Transformer的自监督模型在语音处理中进行压缩

    Compressing Transformer-based self-supervised models for speech processing. (arXiv:2211.09949v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.09949](http://arxiv.org/abs/2211.09949)

    本文研究了对基于Transformer的自监督模型进行压缩的方法，包括权重修剪、头部修剪、低秩逼近和知识蒸馏。结果发现，基本的压缩技术是强大的基准，可以改善模型的压缩效果。

    

    尽管Transformer在自监督学习中取得了成功，并应用于各种下游任务，但是训练和推断的计算成本仍然是将这些模型应用于各种设备的主要挑战。目前已有一些孤立的尝试来压缩Transformer，但研究中的设置和指标各不相同。此前的工作很少涉及不同压缩率之间的权衡，这使得比较压缩技术变得困难。在这项工作中，我们旨在为这些孤立结果提供背景，研究几种常用的压缩技术，包括权重修剪、头部修剪、低秩逼近和知识蒸馏。我们报告了在不同压缩率下的权衡，包括墙钟时间、参数数量和乘加操作数量。我们的结果表明，与最近的方法相比，基本的压缩技术是强大的基准。我们进一步提出了几种压缩方法来改进模型的压缩效果。

    Despite the success of Transformers in self- supervised learning with applications to various downstream tasks, the computational cost of training and inference remains a major challenge for applying these models to a wide spectrum of devices. Several isolated attempts have been made to compress Transformers, but the settings and metrics are different across studies. Trade-off at various compression rates are also largely missing in prior work, making it difficult to compare compression techniques. In this work, we aim to provide context for the isolated results, studying several commonly used compression techniques, including weight pruning, head pruning, low-rank approximation, and knowledge distillation. We report trade- off at various compression rate, including wall-clock time, the number of parameters, and the number of multiply-accumulate operations. Our results show that compared to recent approaches, basic compression techniques are strong baselines. We further present several
    
[^174]: 离线估计控制马尔可夫链：最小化和样本复杂度

    Offline Estimation of Controlled Markov Chains: Minimaxity and Sample Complexity. (arXiv:2211.07092v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.07092](http://arxiv.org/abs/2211.07092)

    本论文研究了离线估计有限控制马尔可夫链的转移概率矩阵的非参数估计器，并通过记录策略的混合特性建立了样本复杂性界限和最小化条件。结果表明，实现特定的统计风险界限涉及到混合特性的强度和样本数量之间微妙而有趣的权衡。还使用这些样本复杂性界限建立了离线评估恒定马尔可夫控制策略的相关界限。

    

    在这项工作中，我们研究了有限控制马尔可夫链的转移概率矩阵的自然非参数估计器。我们考虑了一个固定数据集的离线设置，该数据集是使用所谓的记录策略收集的。我们为估计器开发了样本复杂性的界限，并建立了最小化的条件。我们的统计界限通过记录策略的混合特性来确定。我们表明，实现特定的统计风险界限涉及到混合特性的强度和样本数量之间微妙而有趣的权衡。我们在各种示例中验证了我们结果的有效性，包括遗传马尔可夫链，弱遗传非齐次马尔可夫链和具有非平稳马尔可夫、阶段性和贪婪控制的控制马尔可夫链。最后，我们使用这些样本复杂性界限来建立离线评估恒定马尔可夫控制策略的相关界限。

    In this work, we study a natural nonparametric estimator of the transition probability matrices of a finite controlled Markov chain. We consider an offline setting with a fixed dataset, collected using a so-called logging policy. We develop sample complexity bounds for the estimator and establish conditions for minimaxity. Our statistical bounds depend on the logging policy through its mixing properties. We show that achieving a particular statistical risk bound involves a subtle and interesting trade-off between the strength of the mixing properties and the number of samples. We demonstrate the validity of our results under various examples, such as ergodic Markov chains, weakly ergodic inhomogeneous Markov chains, and controlled Markov chains with non-stationary Markov, episodic, and greedy controls. Lastly, we use these sample complexity bounds to establish concomitant ones for offline evaluation of stationary Markov control policies.
    
[^175]: 高效的深度强化学习与预测处理近端策略优化

    Efficient Deep Reinforcement Learning with Predictive Processing Proximal Policy Optimization. (arXiv:2211.06236v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.06236](http://arxiv.org/abs/2211.06236)

    该论文介绍了一种名为预测处理近端策略优化（P4O）的深度强化学习方法，通过利用递归神经网络预测自身感觉状态来最小化惊异，从而显著提高累积奖励。

    

    强化学习的进步常常依赖于大量的计算资源，且在样本效率上仍然不高。相比之下，人类大脑能够使用有限的资源有效地学习控制策略。这引发了一个问题，即是否可以借鉴神经科学的见解来改进当前的强化学习方法。预测处理是一个流行的理论框架，它认为人类大脑主动寻求最小化惊异。我们展示了能够预测自身感觉状态的递归神经网络可以被利用来最小化惊异，从而在累积奖励上取得巨大的收益。具体而言，我们提出了预测处理近端策略优化（P4O）智能体；它是一个将预测处理应用到基于递归的PPO算法的演员批判强化学习智能体，通过将世界模型集成到其隐藏状态中。即使没有超参数调整，P4O与基线递归变体相比，也能显著提高性能。

    Advances in reinforcement learning (RL) often rely on massive compute resources and remain notoriously sample inefficient. In contrast, the human brain is able to efficiently learn effective control strategies using limited resources. This raises the question whether insights from neuroscience can be used to improve current RL methods. Predictive processing is a popular theoretical framework which maintains that the human brain is actively seeking to minimize surprise. We show that recurrent neural networks which predict their own sensory states can be leveraged to minimise surprise, yielding substantial gains in cumulative reward. Specifically, we present the Predictive Processing Proximal Policy Optimization (P4O) agent; an actor-critic reinforcement learning agent that applies predictive processing to a recurrent variant of the PPO algorithm by integrating a world model in its hidden state. Even without hyperparameter tuning, P4O significantly outperforms a baseline recurrent varian
    
[^176]: 3D点云分类的无盒子攻击

    No-Box Attacks on 3D Point Cloud Classification. (arXiv:2210.14164v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.14164](http://arxiv.org/abs/2210.14164)

    该论文介绍了一种新的方法，可以在不访问目标DNN模型的情况下预测3D点云中的对抗点，提供了无盒子攻击的新视角。

    

    对于基于深度神经网络（DNN）的各种输入信号的分析，对抗攻击构成了严重挑战。在3D点云的情况下，已经开发出了一些方法来识别在网络决策中起关键作用的点，而这些方法在生成现有的对抗攻击中变得至关重要。例如，显著性图方法是一种流行的方法，用于识别对抗攻击会显著影响网络决策的点。通常，识别对抗点的方法依赖于对目标DNN模型的访问，以确定哪些点对模型的决策至关重要。本文旨在对这个问题提供一种新的视角，在不访问目标DNN模型的情况下预测对抗点，这被称为“无盒子”攻击。为此，我们定义了14个点云特征，并使用多元线性回归来检查这些特征是否可以用于预测对抗点，以及哪些特征对预测最为重要。

    Adversarial attacks pose serious challenges for deep neural network (DNN)-based analysis of various input signals. In the case of 3D point clouds, methods have been developed to identify points that play a key role in network decision, and these become crucial in generating existing adversarial attacks. For example, a saliency map approach is a popular method for identifying adversarial drop points, whose removal would significantly impact the network decision. Generally, methods for identifying adversarial points rely on the access to the DNN model itself to determine which points are critically important for the model's decision. This paper aims to provide a novel viewpoint on this problem, where adversarial points can be predicted without access to the target DNN model, which is referred to as a ``no-box'' attack. To this end, we define 14 point cloud features and use multiple linear regression to examine whether these features can be used for adversarial point prediction, and which
    
[^177]: 学习用于最优输运回归的超度量树

    Learning Ultrametric Trees for Optimal Transport Regression. (arXiv:2210.12288v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.12288](http://arxiv.org/abs/2210.12288)

    本文提出了一种学习超度量树的方法，以逼近原始空间中的最优输运距离。通过在超度量空间上进行投影梯度下降优化，我们能够找到具有最优树结构的树-瓦瑟斯坦距离。

    

    最优输运提供了一个度量量化概率测度之间差异的方法。对于支撑在离散度量空间上的测度，找到最优输运距离的时间复杂度是与空间大小呈立方关系的。然而，在树上支撑的测度可以通过闭合形式的最优输运来计算，这样的计算复杂度是线性的。在本文中，我们旨在找到一个对于给定的离散度量空间来说具有最优树结构的树-瓦瑟斯坦距离，以逼近原始空间中的最优输运距离。我们的一个关键思想是将问题转化为超度量空间中的问题。这样做有助于我们通过投影梯度下降在超度量矩阵空间上进行优化，这是一个混合离散和连续优化问题。在优化过程中，我们通过层次最小生成树算法将参数投影到超度量空间，相当于对超度量的最接近投影。

    Optimal transport provides a metric which quantifies the dissimilarity between probability measures. For measures supported in discrete metric spaces, finding the optimal transport distance has cubic time complexity in the size of the space. However, measures supported on trees admit a closed-form optimal transport that can be computed in linear time. In this paper, we aim to find an optimal tree structure for a given discrete metric space so that the tree-Wasserstein distance approximates the optimal transport distance in the original space. One of our key ideas is to cast the problem in ultrametric spaces. This helps us optimize over the space of ultrametric trees -- a mixed-discrete and continuous optimization problem -- via projected gradient decent over the space of ultrametric matrices. During optimization, we project the parameters to the ultrametric space via a hierarchical minimum spanning tree algorithm, equivalent to the closest projection to ultrametrics under the supremum 
    
[^178]: 关于敏感性与准确性在上下文学习中的关系

    On the Relation between Sensitivity and Accuracy in In-context Learning. (arXiv:2209.07661v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.07661](http://arxiv.org/abs/2209.07661)

    在上下文学习中，我们发现ICL对多种扰动类型具有敏感性，标签偏差导致过去的研究低估了ICL的敏感性。同时，我们观察到ICL的敏感性和准确性之间呈现负相关关系。基于这些发现，我们提出了一种少样本选择性预测方法SenSel，它在放弃敏感预测决策上取得了优于常用基准方法的结果。

    

    上下文学习 (In-context learning, ICL) 在实际场景中常常受到提示的过度敏感性的影响，导致其在现实世界中不可靠。我们研究了ICL对多种扰动类型的敏感性。首先，我们发现标签偏差掩盖了真实的敏感性，因此之前的研究可能大大低估了ICL的敏感性。其次，我们观察到ICL的敏感性与准确性之间存在强烈的负相关关系：对扰动敏感的预测更不容易正确。在这些发现的基础上，我们提出了一种称为SenSel的少样本选择性预测方法，该方法避免了对敏感预测的使用。在十个分类数据集上的实验证明，SenSel在放弃预测决策上始终优于两种常用的基于置信度和基于熵的基准方法。

    In-context learning (ICL) suffers from oversensitivity to the prompt, making it unreliable in real-world scenarios. We study the sensitivity of ICL with respect to multiple perturbation types. First, we find that label bias obscures the true sensitivity, and therefore prior work may have significantly underestimated ICL sensitivity. Second, we observe a strong negative correlation between ICL sensitivity and accuracy: predictions sensitive to perturbations are less likely to be correct. Motivated by these findings, we propose \textsc{SenSel}, a few-shot selective prediction method that abstains from sensitive predictions. Experiments on ten classification datasets show that \textsc{SenSel} consistently outperforms two commonly used confidence-based and entropy-based baselines on abstention decisions.
    
[^179]: 用于学习和生成多维Keller-Segel趋化系统聚集模式的DeepParticle方法

    A DeepParticle method for learning and generating aggregation patterns in multi-dimensional Keller-Segel chemotaxis systems. (arXiv:2209.00109v2 [physics.comp-ph] UPDATED)

    [http://arxiv.org/abs/2209.00109](http://arxiv.org/abs/2209.00109)

    本研究提出了一种深度粒子方法DeepParticle，用于学习和生成Keller-Segel趋化系统中的聚集模式。通过使用深度神经网络，该方法能够有效地表示并转换源分布到目标分布，从而实现物理参数变化下解的生成。

    

    我们研究了一种正则化的相互作用粒子方法，用于计算二维和三维空间中Keller-Segel趋化系统的聚集模式和近奇异解，然后进一步发展了DeepParticle（DP）方法，以学习和生成在物理参数变化下的解。KS解被近似为自适应于解的高梯度部分的粒子的经验度量。我们利用深度神经网络（DNN）的表达能力来表示从给定初始（源）分布到有限时间T之前的目标分布的样本转换，而不假设转换的可逆性。在训练阶段，我们通过最小化输入和目标经验测度之间的离散2-Wasserstein距离来更新网络权重。为了降低计算成本，我们开发了一种迭代分而治之的算法，以在Wasserstein距离中找到最优过渡矩阵。我们展示了数值实验结果，验证了该方法的有效性。

    We study a regularized interacting particle method for computing aggregation patterns and near singular solutions of a Keller-Segal (KS) chemotaxis system in two and three space dimensions, then further develop DeepParticle (DP) method to learn and generate solutions under variations of physical parameters. The KS solutions are approximated as empirical measures of particles which self-adapt to the high gradient part of solutions. We utilize the expressiveness of deep neural networks (DNNs) to represent the transform of samples from a given initial (source) distribution to a target distribution at finite time T prior to blowup without assuming invertibility of the transforms. In the training stage, we update the network weights by minimizing a discrete 2-Wasserstein distance between the input and target empirical measures. To reduce computational cost, we develop an iterative divide-and-conquer algorithm to find the optimal transition matrix in the Wasserstein distance. We present nume
    
[^180]: 使用层级前向模型实现多时间段表示的强化学习方法

    Multi-Horizon Representations with Hierarchical Forward Models for Reinforcement Learning. (arXiv:2206.11396v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.11396](http://arxiv.org/abs/2206.11396)

    提出了一种使用层级前向模型实现多时间段表示的辅助任务HKSL，通过学习多个表示和不同步长下的评论家，能够更快地收敛到更高的或最优的回报。

    

    对于强化学习（RL）代理来说，从像素学习控制是困难的，因为表示学习和策略学习是相互交织的。先前的方法通过辅助表示学习任务来解决这个问题，但它们要么不考虑问题的时间方面，要么只考虑单步转换，如果重要的环境变化需要多个步骤才能表现出来，可能导致学习效率低下。我们提出了Hierarchical $k$-Step Latent (HKSL)，一种通过层级前向模型学习多个表示的辅助任务，这些模型学习通信以及一系列在不同步长下工作的$n$步评论家。我们在30个机器人控制任务和一个我们创建的任务中评估了HKSL的性能，包括有和无干扰物的情况。我们发现，与其他几种替代的表示学习方法相比，HKSL要么更快地收敛到更高的或最优的回报，要么更快地收敛到最优的回报。

    Learning control from pixels is difficult for reinforcement learning (RL) agents because representation learning and policy learning are intertwined. Previous approaches remedy this issue with auxiliary representation learning tasks, but they either do not consider the temporal aspect of the problem or only consider single-step transitions, which may cause learning inefficiencies if important environmental changes take many steps to manifest. We propose Hierarchical $k$-Step Latent (HKSL), an auxiliary task that learns multiple representations via a hierarchy of forward models that learn to communicate and an ensemble of $n$-step critics that all operate at varying magnitudes of step skipping. We evaluate HKSL in a suite of 30 robotic control tasks with and without distractors and a task of our creation. We find that HKSL either converges to higher or optimal episodic returns more quickly than several alternative representation learning approaches. Furthermore, we find that HKSL's repr
    
[^181]: 联邦离线强化学习

    Federated Offline Reinforcement Learning. (arXiv:2206.05581v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.05581](http://arxiv.org/abs/2206.05581)

    本文提出了一种联邦离线强化学习算法，可以处理医疗机构间数据共享的隐私限制和异质性问题，同时提供了通信效率和隐私保护性。该算法的样本复杂度证明以及在现实医学数据集上的模拟实验结果表明了其有效性和效率。

    

    基于证据或数据的动态治疗方案对于个性化医疗至关重要，可以受益于离线强化学习（RL）。虽然医疗机构间有大量健康数据可用，但由于隐私限制，它们无法共享。此外，不同站点存在异质性。因此，联邦离线RL算法是必要的且有前途，以解决这些问题。本文提出了一种多站点马尔可夫决策过程模型，允许站点之间的同质性和异质性效应。所提出的模型可以分析站点级特征。我们设计了第一个具有样本复杂度的离线RL联邦策略优化算法。所提出的算法具有通信效率和隐私保护性，仅需要通过交换摘要统计量进行一轮通信交互。我们为所提出的算法提供了理论保证，无需假设站点之间具有相同的转换动态。我们在现实医学数据集上进行了模拟，展示了所提出算法的有效性和效率。

    Evidence-based or data-driven dynamic treatment regimes are essential for personalized medicine, which can benefit from offline reinforcement learning (RL). Although massive healthcare data are available across medical institutions, they are prohibited from sharing due to privacy constraints. Besides, heterogeneity exists in different sites. As a result, federated offline RL algorithms are necessary and promising to deal with the problems. In this paper, we propose a multi-site Markov decision process model which allows both homogeneous and heterogeneous effects across sites. The proposed model makes the analysis of the site-level features possible. We design the first federated policy optimization algorithm for offline RL with sample complexity. The proposed algorithm is communication-efficient and privacy-preserving, which requires only a single round of communication interaction by exchanging summary statistics. We give a theoretical guarantee for the proposed algorithm without the 
    
[^182]: 风险度量和上概率：连贯性和分层

    Risk Measures and Upper Probabilities: Coherence and Stratification. (arXiv:2206.03183v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.03183](http://arxiv.org/abs/2206.03183)

    该论文研究了富有决策实际意义的聚合函数类，以构建不确定性处理的数学基础，通过分析谱风险度量、Choquet积分和Lorentz范数的特性，提出了一种对风险度量进行分层的方法，并在机器学习实践中得到了验证。

    

    机器学习通常假设经典概率论，这意味着聚合建立在期望之上。现在有多个理由来寻找比经典概率论更丰富的替代数学基础，用于机器学习。我们系统地研究了一种强大而丰富的替代聚合函数类，被称为谱风险度量、Choquet积分或Lorentz范数。我们提出了一系列表征结果，并展示了这个谱家族的特殊之处。通过利用可重排不变Banach空间理论的结果，我们得到了一种自然的对所有连贯风险度量进行分层的方法，用于表示它们所引导的上概率。我们从实证角度演示了这种新的不确定性处理方法如何帮助解决实际的机器学习问题。

    Machine learning typically presupposes classical probability theory which implies that aggregation is built upon expectation. There are now multiple reasons to motivate looking at richer alternatives to classical probability theory as a mathematical foundation for machine learning. We systematically examine a powerful and rich class of alternative aggregation functionals, known variously as spectral risk measures, Choquet integrals or Lorentz norms. We present a range of characterization results, and demonstrate what makes this spectral family so special. In doing so we arrive at a natural stratification of all coherent risk measures in terms of the upper probabilities that they induce by exploiting results from the theory of rearrangement invariant Banach spaces. We empirically demonstrate how this new approach to uncertainty helps tackling practical machine learning problems.
    
[^183]: AugLoss：一种稳健的基于数据增强的微调方法

    AugLoss: A Robust Augmentation-based Fine Tuning Methodology. (arXiv:2206.02286v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.02286](http://arxiv.org/abs/2206.02286)

    AugLoss是一种简单而有效的方法，通过统一数据增强和稳健损失函数，实现了对训练时的噪声标注和测试时的特征分布转移的稳健性。

    

    深度学习模型在许多领域取得了巨大的成功。然而，深度学习模型越来越面临安全性和鲁棒性方面的问题，包括训练阶段的噪声标注和测试阶段的特征分布转移。以往的研究在解决这些问题方面取得了显著进展，但主要集中在一次只解决一个问题的解决方案上。例如，最近的研究提出了使用可调的稳健损失函数来减轻标签噪声，以及使用数据增强（例如AugMix）来解决分布转移问题。为了同时解决这两个问题，我们引入了AugLoss，一种简单但有效的方法，通过统一数据增强和稳健损失函数，实现了对训练时的噪声标注和测试时的特征分布转移的稳健性。我们在各种真实数据集损坏设置下进行了全面的实验，展示了与以前方法相比AugLoss所取得的收益。

    Deep Learning (DL) models achieve great successes in many domains. However, DL models increasingly face safety and robustness concerns, including noisy labeling in the training stage and feature distribution shifts in the testing stage. Previous works made significant progress in addressing these problems, but the focus has largely been on developing solutions for only one problem at a time. For example, recent work has argued for the use of tunable robust loss functions to mitigate label noise, and data augmentation (e.g., AugMix) to combat distribution shifts. As a step towards addressing both problems simultaneously, we introduce AugLoss, a simple but effective methodology that achieves robustness against both train-time noisy labeling and test-time feature distribution shifts by unifying data augmentation and robust loss functions. We conduct comprehensive experiments in varied settings of real-world dataset corruption to showcase the gains achieved by AugLoss compared to previous 
    
[^184]: MiniDisc: 最小蒸馏计划用于语言模型压缩

    MiniDisc: Minimal Distillation Schedule for Language Model Compression. (arXiv:2205.14570v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.14570](http://arxiv.org/abs/2205.14570)

    本研究提出了一个叫做MiniDisc的最小蒸馏计划，可以在最少一次尝试中调度最优的教师助手，用于实现语言模型压缩。

    

    最近的研究发现，在教师模型和学生模型之间存在较大的容量差距时，语言模型蒸馏的效果不佳，引入了教师助手辅助蒸馏来弥补差距。然而，现有的基于教师助手的方法需要大量的尝试才能调度出最优的教师助手。为此，我们提出了一种最小蒸馏计划（MiniDisc），可以在最少一次尝试中调度最优的教师助手。MiniDisc是基于教师助手的规模-性能的权衡来度量教师助手的最优性，并可以在不对学生进行实验的情况下调度最优的教师助手。

    Recent studies have uncovered that language model distillation is less effective when facing a large capacity gap between the teacher and the student, and introduced teacher assistant-based distillation to bridge the gap. As a connection, the scale and the performance of the teacher assistant is of vital importance to bring the knowledge from the teacher to the student. However, existing teacher assistant-based methods require maximally many trials before scheduling an optimal teacher assistant. To this end, we propose a minimal distillation schedule (MiniDisc) for scheduling the optimal teacher assistant in minimally one trial. In particular, motivated by the finding that the performance of the student is positively correlated to the scale-performance tradeoff of the teacher assistant, MiniDisc is designed with a $\lambda$-tradeoff to measure the optimality of the teacher assistant without trial distillation to the student. MiniDisc then can schedule the optimal teacher assistant with
    
[^185]: 边缘资源感知的低成本边缘聚合方法在联邦学习中的应用

    Towards cost-effective and resource-aware aggregation at Edge for Federated Learning. (arXiv:2204.07767v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.07767](http://arxiv.org/abs/2204.07767)

    本研究提出了一种边缘联邦学习聚合器，通过在边缘进行自适应聚合，提高了可扩展性和时间效率，同时降低了成本。

    

    联邦学习是一种机器学习方法，通过在数据源处计算数据来解决隐私和数据传输成本问题。它在边缘和物联网应用中特别受欢迎，其中联邦学习的聚合服务器位于资源受限的边缘数据中心，以减少通信成本。现有基于云的聚合解决方案在边缘上既资源低效又昂贵，导致可扩展性低和延迟高。为了应对这些挑战，本研究比较了在物联网和边缘应用需求变化下的先前和新的聚合方法。这项工作首次提出了一种自适应的边缘联邦学习聚合器，使用户能够管理成本和效率的权衡。广泛的比较分析表明，与现有的基于云的静态方法相比，该设计提高了可扩展性最多4倍，时间效率提高了8倍，并将成本降低了超过2倍。

    Federated Learning (FL) is a machine learning approach that addresses privacy and data transfer costs by computing data at the source. It's particularly popular for Edge and IoT applications where the aggregator server of FL is in resource-capped edge data centers for reducing communication costs. Existing cloud-based aggregator solutions are resource-inefficient and expensive at the Edge, leading to low scalability and high latency. To address these challenges, this study compares prior and new aggregation methodologies under the changing demands of IoT and Edge applications. This work is the first to propose an adaptive FL aggregator at the Edge, enabling users to manage the cost and efficiency trade-off. An extensive comparative analysis demonstrates that the design improves scalability by up to 4X, time efficiency by 8X, and reduces costs by more than 2X compared to extant cloud-based static methodologies.
    
[^186]: 使用潜在狄利克雷变分自编码器进行高光谱像素解混技术

    Hyperspectral Pixel Unmixing with Latent Dirichlet Variational Autoencoder. (arXiv:2203.01327v4 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2203.01327](http://arxiv.org/abs/2203.01327)

    本研究提出了一种使用潜在狄利克雷变分自编码器进行高光谱像素解混的方法，可以通过迁移学习范式训练模型并在真实数据上进行像素解混，取得了最新的最好结果。

    

    我们提出了一种用于高光谱像素解混的方法。该方法假设(1)丰度可以编码为狄利克雷分布，(2)成分的光谱可以表示为多元正态分布。该方法在变分自编码器框架下解决了丰度估计和成分提取问题，其中狄利克雷瓶颈层建模丰度，解码器执行成分提取。该方法还可以利用迁移学习范式，模型仅在包含感兴趣的一个或多个成分的线性组合像素的合成数据上进行训练。在这种情况下，我们从美国地质调查局光谱库中检索出成分(光谱)。然后，训练好的模型可以在包含生成合成数据所使用的一部分成分的“实际数据”上进行像素解混。该模型在多个评价指标上达到了最先进的结果。

    We present a method for hyperspectral pixel {\it unmixing}. The proposed method assumes that (1) {\it abundances} can be encoded as Dirichlet distributions and (2) spectra of {\it endmembers} can be represented as multivariate Normal distributions. The method solves the problem of abundance estimation and endmember extraction within a variational autoencoder setting where a Dirichlet bottleneck layer models the abundances, and the decoder performs endmember extraction. The proposed method can also leverage transfer learning paradigm, where the model is only trained on synthetic data containing pixels that are linear combinations of one or more endmembers of interest. In this case, we retrieve endmembers (spectra) from the United States Geological Survey Spectral Library. The model thus trained can be subsequently used to perform pixel unmixing on "real data" that contains a subset of the endmembers used to generated the synthetic data. The model achieves state-of-the-art results on sev
    
[^187]: 从卷积层的特征图理解对抗鲁棒性

    Understanding Adversarial Robustness from Feature Maps of Convolutional Layers. (arXiv:2202.12435v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2202.12435](http://arxiv.org/abs/2202.12435)

    通过研究卷积层的特征图，本研究发现较大的特征图可以提高神经网络对扰动的抵抗能力，这为设计鲁棒神经网络提供了新的启示，并提出了简单的修改方法来改进现有架构。

    

    神经网络的对抗鲁棒性主要取决于两个因素：模型容量和抗扰动能力。本文从卷积层的特征图研究了网络的抗扰动能力。我们的理论分析发现，在平均池化之前较大的卷积特征图可以提高对扰动的抵抗能力，但对于最大池化并非如此。这为鲁棒神经网络的设计带来了新的启示，并促使我们将这些发现应用于改进现有架构。所提出的修改非常简单，只需要对输入进行上采样或轻微修改下采样运算符的步幅配置。我们在几个基准神经网络架构上验证了我们的方法，包括AlexNet、VGG、RestNet18和PreActResNet18。在各种攻击和防御中，都能实现自然准确度和对抗鲁棒性的非平凡的改进。

    The adversarial robustness of a neural network mainly relies on two factors: model capacity and anti-perturbation ability. In this paper, we study the anti-perturbation ability of the network from the feature maps of convolutional layers. Our theoretical analysis discovers that larger convolutional feature maps before average pooling can contribute to better resistance to perturbations, but the conclusion is not true for max pooling. It brings new inspiration to the design of robust neural networks and urges us to apply these findings to improve existing architectures. The proposed modifications are very simple and only require upsampling the inputs or slightly modifying the stride configurations of downsampling operators. We verify our approaches on several benchmark neural network architectures, including AlexNet, VGG, RestNet18, and PreActResNet18. Non-trivial improvements in terms of both natural accuracy and adversarial robustness can be achieved under various attack and defense m
    
[^188]: 粒子变压器用于喷注标记

    Particle Transformer for Jet Tagging. (arXiv:2202.03772v3 [hep-ph] UPDATED)

    [http://arxiv.org/abs/2202.03772](http://arxiv.org/abs/2202.03772)

    本研究提出了用于喷注标记的粒子变压器，并使用新的全面数据集JetClass进行训练。粒子变压器通过在注意力机制中引入粒子间的交互关系，实现了比传统变压器更高的标记性能，并在两种常用的喷注标记任务上取得了显著的性能提升。

    

    喷注标记是粒子物理学中一个关键而具有挑战性的分类任务。虽然深度学习已经改变了喷注标记，并显著提高了性能，但缺乏一个大规模的公共数据集阻碍了进一步的提升。在这项工作中，我们提出了JetClass，一个用于喷注标记的新的全面数据集。JetClass数据集包含1亿个喷注，比现有公共数据集大两个数量级。总共模拟了10种类型的喷注，包括至今为止尚未用于标记的几种类型。基于大型数据集，我们提出了一种新的基于变压器的用于喷注标记的架构，称为粒子变压器（ParT）。通过在注意力机制中引入配对粒子交互，ParT实现了比简单变压器更高的标记性能，并大幅超过了以前的最先进技术ParticleNet。预训练的ParT模型在微调后还可以大幅提升广泛应用的两种喷注标记的性能。

    Jet tagging is a critical yet challenging classification task in particle physics. While deep learning has transformed jet tagging and significantly improved performance, the lack of a large-scale public dataset impedes further enhancement. In this work, we present JetClass, a new comprehensive dataset for jet tagging. The JetClass dataset consists of 100 M jets, about two orders of magnitude larger than existing public datasets. A total of 10 types of jets are simulated, including several types unexplored for tagging so far. Based on the large dataset, we propose a new Transformer-based architecture for jet tagging, called Particle Transformer (ParT). By incorporating pairwise particle interactions in the attention mechanism, ParT achieves higher tagging performance than a plain Transformer and surpasses the previous state-of-the-art, ParticleNet, by a large margin. The pre-trained ParT models, once fine-tuned, also substantially enhance the performance on two widely adopted jet taggi
    
[^189]: 计算机视觉中基于自监督学习的时序方法

    Computer Vision Self-supervised Learning Methods on Time Series. (arXiv:2109.00783v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.00783](http://arxiv.org/abs/2109.00783)

    该研究评估了计算机视觉自监督学习框架在时间序列上的效果，并且提出了一种改进方法，通过改进协方差项和添加迭代归一化层，加速了模型的收敛。

    

    自监督学习（SSL）在计算机视觉领域取得了巨大成功。目前主流的计算机视觉自监督学习框架大多基于Siamese网络架构。这些方法通常依赖于精心设计的损失函数和训练设置，以避免特征崩溃。本研究评估了这些计算机视觉自监督学习框架在不同模态（即时间序列）上的效果。我们在UCR和UEA档案上进行了实验证明，计算机视觉自监督学习框架在时间序列上同样有效。此外，我们提出了一种改进最近提出的VICReg方法的新方法。我们改进了VICReg中提出的一个“协方差”项，同时在架构的头部增加了一个迭代归一化层，加速了模型的收敛。

    Self-supervised learning (SSL) has had great success in both computer vision. Most of the current mainstream computer vision SSL frameworks are based on Siamese network architecture. These approaches often rely on cleverly crafted loss functions and training setups to avoid feature collapse. In this study, we evaluate if those computer-vision SSL frameworks are also effective on a different modality (\textit{i.e.,} time series). The effectiveness is experimented and evaluated on the UCR and UEA archives, and we show that the computer vision SSL frameworks can be effective even for time series. In addition, we propose a new method that improves on the recently proposed VICReg method. Our method improves on a \textit{covariance} term proposed in VICReg, and in addition we augment the head of the architecture by an iterative normalization layer that accelerates the convergence of the model.
    
[^190]: 通过算法力系数估计生成模拟数据，用于基于人工智能的机器人抛射建模

    Simulated Data Generation Through Algorithmic Force Coefficient Estimation for AI-Based Robotic Projectile Launch Modeling. (arXiv:2105.12833v4 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2105.12833](http://arxiv.org/abs/2105.12833)

    该论文提出了一种通过算法估计力系数来生成模拟数据的方法，以用于基于人工智能的机器人抛射建模。通过这种方法，可以解决使用物理模型进行建模时存在的未知因素和物体变形效应的问题，并且可以提高深度神经网络的性能。

    

    非刚性物体抛射和操纵建模很复杂，考虑到影响轨迹的各种动力学因素，其中许多可能是未知的。使用物理模型可能不准确，因为它们无法考虑未知因素和物体抛射时的变形效应。此外，要想得到这些模型的力系数，就需要进行大量的实验测试。最近，数据驱动的人工智能方法的进展使得可学习的模型和系统得以出现。在机器人上训练一个预测抛射模型是可取的，因为深度神经网络可以考虑到无法测量的动力学因素。然而，无法收集大量实验数据会降低深度神经网络的性能。通过估计力系数，可以利用已接受的物理模型生成充足的补充数据，人为地增加训练集的大小，从而提高神经网络的性能。

    Modeling of non-rigid object launching and manipulation is complex considering the wide range of dynamics affecting trajectory, many of which may be unknown. Using physics models can be inaccurate because they cannot account for unknown factors and the effects of the deformation of the object as it is launched; moreover, deriving force coefficients for these models is not possible without extensive experimental testing. Recently, advancements in data-powered artificial intelligence methods have allowed learnable models and systems to emerge. It is desirable to train a model for launch prediction on a robot, as deep neural networks can account for immeasurable dynamics. However, the inability to collect large amounts of experimental data decreases performance of deep neural networks. Through estimating force coefficients, the accepted physics models can be leveraged to produce adequate supplemental data to artificially increase the size of the training set, yielding improved neural netw
    
[^191]: 提高基于变换的对抗示例防御方法的一阶扰动抵抗性

    Improving Transformation-based Defenses against Adversarial Examples with First-order Perturbations. (arXiv:2103.04565v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2103.04565](http://arxiv.org/abs/2103.04565)

    该论文提出了一种改进基于变换的对抗示例防御方法的方法，通过对对抗示例生成一阶扰动来增加神经网络输出正确结果的概率，并在推断时应用这种方法。

    

    深度神经网络在各种机器学习任务中取得了成功。然而，研究表明神经网络容易受到对抗攻击，这对基于神经网络的智能系统构成了潜在威胁。我们观察到通过对对抗示例生成针对非预测标签的小一阶扰动可以增加神经网络输出结果为正确结果的概率。基于这个观察，我们提出了一种抵抗对抗扰动以提高对抗鲁棒性的方法。在提出的方法中，我们随机选择若干类别标签，并为这些选定的标签生成小的一阶扰动。生成的扰动加在一起后被限制在指定的空间内。最终得到的扰动被添加到对抗示例中，以抵消其中的对抗扰动。所提出的方法在推断时应用。

    Deep neural networks have been successfully applied in various machine learning tasks. However, studies show that neural networks are susceptible to adversarial attacks. This exposes a potential threat to neural network-based intelligent systems. We observe that the probability of the correct result outputted by the neural network increases by applying small first-order perturbations generated for non-predicted class labels to adversarial examples. Based on this observation, we propose a method for counteracting adversarial perturbations to improve adversarial robustness. In the proposed method, we randomly select a number of class labels and generate small first-order perturbations for these selected labels. The generated perturbations are added together and then clamped onto a specified space. The obtained perturbation is finally added to the adversarial example to counteract the adversarial perturbation contained in the example. The proposed method is applied at inference time and d
    
[^192]: 一种整合和分类正态分布的方法

    A method to integrate and classify normal distributions. (arXiv:2012.14331v8 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2012.14331](http://arxiv.org/abs/2012.14331)

    本文介绍了一种可以对任意参数维度下的任意域内正态分布进行积分的方法，提供了法向向量函数的相关概率密度和统计指标，同时还提供了可以对任意数量正态分布进行分类的方法和维度降低和可视化的技术。

    

    单变量和多变量正态概率分布在模拟不确定性决策中被广泛使用。计算这些模型的性能需要在特定区域内对这些分布进行积分，这在不同的模型中可以有很大的差异。除了一些特殊情况，目前不存在通用的分析表达式、标准数值方法或软件来计算这些积分。本文提供了数学结果和开源软件，可以提供以下内容：（i）任意参数维度下任意域内法向的概率，（ii）法向向量函数的概率密度、累积分布和逆累积分布，（iii）任意数量正态分布之间的分类误差、贝叶斯最优辨别指数以及其与工作特征曲线的关系，（iv）此类问题的维度降低和可视化，以及（v）对于给定数据这些方法的可靠性测试。我们通过几个具体的例子，包括金融、生物和心理学来演示这些功能。

    Univariate and multivariate normal probability distributions are widely used when modeling decisions under uncertainty. Computing the performance of such models requires integrating these distributions over specific domains, which can vary widely across models. Besides some special cases, there exist no general analytical expressions, standard numerical methods or software for these integrals. Here we present mathematical results and open-source software that provide (i) the probability in any domain of a normal in any dimensions with any parameters, (ii) the probability density, cumulative distribution, and inverse cumulative distribution of any function of a normal vector, (iii) the classification errors among any number of normal distributions, the Bayes-optimal discriminability index and relation to the operating characteristic, (iv) dimension reduction and visualizations for such problems, and (v) tests for how reliably these methods may be used on given data. We demonstrate these
    
[^193]: 多视角堆叠中的视图选择：选择元学习器

    View selection in multi-view stacking: Choosing the meta-learner. (arXiv:2010.16271v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2010.16271](http://arxiv.org/abs/2010.16271)

    选择合适的元学习器对于多视角堆叠中的视图选择和分类准确性是非常重要的，通过对七种不同的算法进行评估，非负套索、非负自适应套索和非负弹性网络被认为是最合适的元学习器。

    

    多视角堆叠是一种将来自不同视图（即不同的特征集）描述相同对象的信息相结合的框架。在该框架中，基学习算法分别在每个视图上进行训练，它们的预测结果由元学习算法组合。在之前的研究中，堆叠的罚分逻辑回归，作为多视角堆叠的一种特殊情况，已被证明在识别对预测最重要的视图方面是有用的。在本文中，我们通过考虑七种不同的算法作为元学习器，并在模拟和两个真实的基因表达数据集上评估它们的视图选择和分类性能，扩展了这项研究。我们的结果表明，如果视图选择和分类准确性对研究都很重要，那么非负套索、非负自适应套索和非负弹性网络都是合适的元学习器。具体在这三种方法中该选择哪一种取决于...

    Multi-view stacking is a framework for combining information from different views (i.e. different feature sets) describing the same set of objects. In this framework, a base-learner algorithm is trained on each view separately, and their predictions are then combined by a meta-learner algorithm. In a previous study, stacked penalized logistic regression, a special case of multi-view stacking, has been shown to be useful in identifying which views are most important for prediction. In this article we expand this research by considering seven different algorithms to use as the meta-learner, and evaluating their view selection and classification performance in simulations and two applications on real gene-expression data sets. Our results suggest that if both view selection and classification accuracy are important to the research at hand, then the nonnegative lasso, nonnegative adaptive lasso and nonnegative elastic net are suitable meta-learners. Exactly which among these three is to be
    
[^194]: 《高斯过程回归的直观教程》

    An Intuitive Tutorial to Gaussian Process Regression. (arXiv:2009.10862v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2009.10862](http://arxiv.org/abs/2009.10862)

    《高斯过程回归的直观教程》是一篇介绍高斯过程回归的教程，旨在直观地解释GPR的基本概念、提供实现代码，并回顾最先进的高斯过程算法。适合机器学习初学者阅读，帮助他们清晰理解GPR的基本原理。

    

    本教程旨在直观介绍高斯过程回归（GPR）。由于其灵活性和对预测不确定性的固有能力，GPR模型在机器学习应用中得到广泛应用。教程从解释高斯过程构建的基本概念开始，包括多元正态分布、核函数、非参数模型以及联合概率和条件概率。然后，它提供了对GPR的简明描述和标准GPR算法的实现。此外，教程还回顾了实现最先进的高斯过程算法的软件包。本教程适用于广泛的受众，包括对机器学习不熟悉的人，以确保对GPR的基本原理有清晰的理解。

    This tutorial aims to provide an intuitive introduction to Gaussian process regression (GPR). GPR models have been widely used in machine learning applications due to their representation flexibility and inherent capability to quantify uncertainty over predictions. The tutorial starts with explaining the basic concepts that a Gaussian process is built on, including multivariate normal distribution, kernels, non-parametric models, and joint and conditional probability. It then provides a concise description of GPR and an implementation of a standard GPR algorithm. In addition, the tutorial reviews packages for implementing state-of-the-art Gaussian process algorithms. This tutorial is accessible to a broad audience, including those new to machine learning, ensuring a clear understanding of GPR fundamentals.
    
[^195]: 通过元学习对图神经网络进行对抗攻击

    Adversarial Attacks on Graph Neural Networks via Meta Learning. (arXiv:1902.08412v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1902.08412](http://arxiv.org/abs/1902.08412)

    本文通过元梯度方式对图神经网络进行训练时攻击，通过微小的图扰动导致性能下降，并证明了即使在无监督嵌入中也能产生迁移效应。这些攻击不需要任何关于目标分类器的知识或访问权限。

    

    图神经网络被广泛应用于许多任务中并取得了最新的成功，但它们的鲁棒性还知之甚少。本文通过研究对节点分类中的图神经网络进行的训练时攻击，在离散图结构上进行微小扰动。我们的核心原则是使用元梯度来解决训练时攻击背后的双层问题，本质上将图视为优化的超参数。我们的实验证明，微小的图扰动通常会导致图卷积网络性能的大幅下降，甚至传递给无监督嵌入。值得注意的是，我们的算法所创建的扰动可以误导图神经网络，使其性能比忽略所有关联信息的简单基线模型更差。我们的攻击不需要任何关于目标分类器的知识或访问权限。

    Deep learning models for graphs have advanced the state of the art on many tasks. Despite their recent success, little is known about their robustness. We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure. Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings. Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information. Our attacks do not assume any knowledge about or access to the target classifiers.
    

