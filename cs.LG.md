# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Steered Diffusion: A Generalized Framework for Plug-and-Play Conditional Image Synthesis.](http://arxiv.org/abs/2310.00224) | Steered Diffusion是一个通用的框架，利用近期基于扩散的生成模型的细粒度生成控制能力，实现了零样本条件图像生成的高质量合成。 |
| [^2] | [Beyond Random Noise: Insights on Anonymization Strategies from a Latent Bandit Study.](http://arxiv.org/abs/2310.00221) | 本文通过使用隐性强盗设置和不同的聚合策略，评估了隐私和推荐器性能之间的权衡，为定制隐私技术的需求提供了洞察。研究结果表明，对个体用户的数据记录添加拉普拉斯机制的噪声是不合适的选择，它在任何噪声水平下都会产生最大的遗憾。 |
| [^3] | [Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment.](http://arxiv.org/abs/2310.00212) | 该论文提出了一种新的强化学习框架，使用相对反馈来调整大型语言模型（LLMs）的行为，解决了现有方法在优化比较损失训练的奖励时存在的限制。同时，还提出了一种新的基于轨迹的策略梯度算法（PPPO），用于更有效地进行算法设计和函数逼近。 |
| [^4] | [Accelerating Non-IID Federated Learning via Heterogeneity-Guided Client Sampling.](http://arxiv.org/abs/2310.00198) | 本文提出了一种名为HiCS-FL的方法，通过利用客户端的网络输出层更新来估计数据的统计异质性，并利用此信息进行客户端的聚类选择，以加速非独立同分布联邦学习。 |
| [^5] | [On the Equivalence of Graph Convolution and Mixup.](http://arxiv.org/abs/2310.00183) | 这项研究发现，在两个温和的条件下，图卷积可以被视为Mixup的一种特殊形式，它在训练和测试阶段都被应用。 |
| [^6] | [MARL: Multi-scale Archetype Representation Learning for Urban Building Energy Modeling.](http://arxiv.org/abs/2310.00180) | MARL是一种利用表示学习从建筑库中提取几何特征的方法，适用于多尺度区域的城市建筑能源建模，具有自动生成、灵活适应不同尺寸建筑轮廓以及保持几何特征的优点。 |
| [^7] | [A Neural-preconditioned Poisson Solver for Mixed Dirichlet and Neumann Boundary Conditions.](http://arxiv.org/abs/2310.00177) | 我们引入了一个用于混合边界条件的泊松方程的神经预处理迭代求解器，核心是一个神经网络，能够近似逆离散结构网格拉普拉斯算子，并且在训练集之外的边界条件下仍然有效。 |
| [^8] | [Tight Bounds for Volumetric Spanners and Applications.](http://arxiv.org/abs/2310.00175) | 本文研究了体积跨度器的紧致界限和应用，并给出了对于所有 $\ell_p$ 范数的几乎最优界限。此外，我们展示了这些结果在寻找最小体积包围椭球（MVEE）问题的核心集问题上的应用。 |
| [^9] | [ADMET property prediction through combinations of molecular fingerprints.](http://arxiv.org/abs/2310.00174) | 通过使用扩展连接指纹（ECFP）、Avalon、ErG指纹和200个分子属性的组合，以及梯度提升决策树算法（尤其是CatBoost）和图神经网络指纹，我们成功地预测了ADMET属性。我们的研究结果强调了更丰富的分子表示对于准确的性质预测的重要性。 |
| [^10] | [Motif: Intrinsic Motivation from Artificial Intelligence Feedback.](http://arxiv.org/abs/2310.00166) | 本文提出了一种名为Motif的方法，通过与大型语言模型（LLM）交互来获得先验知识，并将其用于代理程序的强化学习训练。实验证明，Motif的内在奖励相比直接最大化得分的算法在挑战性游戏中获得了更高的游戏得分，并在之前没有取得进展的任务上取得了显著的进展。 |
| [^11] | [SCoRe: Submodular Combinatorial Representation Learning for Real-World Class-Imbalanced Settings.](http://arxiv.org/abs/2310.00165) | 本文介绍了SCoRe框架，用于解决真实世界中类别不平衡场景下的表示学习问题。通过使用子模组合函数，我们能够同时建模特征簇的多样性和合作性。这对于克服类别不平衡在自主导航和医学诊断等任务中的挑战具有重要意义。 |
| [^12] | [Detection-Oriented Image-Text Pretraining for Open-Vocabulary Detection.](http://arxiv.org/abs/2310.00161) | 这项研究提出了一种面向检测的图像-文本预训练方法，旨在弥合图像级预训练和开放词汇目标检测之间的差距。通过检测器架构和对比损失，该方法能够从噪声图像-文本对中学习到新出现的物体-语义线索，并提出了一种平移窗口学习方法来改进主干网络的表示。在LVIS开放词汇检测基准上，该方法取得了显著优于其他方法的40.4的掩码AP$_r$结果。 |
| [^13] | [Feedback-guided Data Synthesis for Imbalanced Classification.](http://arxiv.org/abs/2310.00158) | 本论文介绍了一种反馈引导数据合成的方法，通过从分类器到生成模型的反馈来驱动采样，将静态数据集增强为包含有用的合成样本，以提高分类器的性能。 |
| [^14] | [Primal-Dual Continual Learning: Stability and Plasticity through Lagrange Multipliers.](http://arxiv.org/abs/2310.00154) | 本文提出了原始-对偶持续学习方法，通过利用拉格朗日对偶解决受限学习问题，实现了稳定性和可塑性。作者通过分析任务层面和样本层面的约束，在基于记忆的方法中分配资源，取得了较好的效果。 |
| [^15] | [One for All: Towards Training One Graph Model for All Classification Tasks.](http://arxiv.org/abs/2310.00149) | 这项研究提出了一种名为“一刀切”的通用框架，该框架能够使用单一图模型解决不同领域的多个任务。该框架克服了图学习领域的挑战，包括不同属性和分布的图数据、不同类型的任务以及上下文学习的问题。 |
| [^16] | [Probabilistic Sampling-Enhanced Temporal-Spatial GCN: A Scalable Framework for Transaction Anomaly Detection in Ethereum Networks.](http://arxiv.org/abs/2310.00144) | 该研究提出了一种基于概率采样增强的时空图卷积网络框架，用于以太坊网络中的交易异常检测。通过将图卷积网络与时态随机游走相结合，利用时间序列的复杂性提供更精细的交易异常检测机制。实验结果表明，与传统的图卷积网络相比，该框架在检测异常和交易突发方面有显著的性能提升。这项研究强调了以太坊交易数据中时间线索的潜力，并展示了使用该框架进行交易异常检测的可行性。 |
| [^17] | [GASS: Generalizing Audio Source Separation with Large-scale Data.](http://arxiv.org/abs/2310.00140) | 本文研究了一种使用大规模数据进行音频源分离的通用方法（GASS），在有限分布范围内表现出良好的效果，并展示了其在声音事件和语音分离方面的泛化能力。然而，在分离超出分布的电影和音乐内容方面仍存在挑战。 |
| [^18] | [On the Disconnect Between Theory and Practice of Overparametrized Neural Networks.](http://arxiv.org/abs/2310.00137) | 本文研究了神经网络在无穷宽度极限下的行为，并与核方法建立了联系。虽然在合成架构中展示了一些优势，如更快的优化和可靠的不确定性量化，但实际相关的架构需要比深度大很多倍的宽度才能实现这些优势。 |
| [^19] | [Multi-Grid Tensorized Fourier Neural Operator for High-Resolution PDEs.](http://arxiv.org/abs/2310.00120) | 本论文介绍了一种称为多重网格张量傅里叶神经算子（MG-TFNO）的新型数据有效且高度并行化的算子学习方法，它通过局部和全局结构的分解来扩展至大尺度的分辨率。其创新包括多重网格的域分解、在傅里叶域中的高阶潜在子空间表示参数以及对架构的改进。 |
| [^20] | [ABScribe: Rapid Exploration of Multiple Writing Variations in Human-AI Co-Writing Tasks using Large Language Models.](http://arxiv.org/abs/2310.00117) | ABScribe是一种界面，支持在人工智能与人类共同写作任务中快速探索多种写作变化。用户可以使用大型语言模型提示快速生成多个变体，这些变体以可重用的按钮形式呈现，并且可以通过上下文工具栏进行快速的就地比较。 |
| [^21] | [Certified Robustness via Dynamic Margin Maximization and Improved Lipschitz Regularization.](http://arxiv.org/abs/2310.00116) | 本文提出了一种基于动态边界最大化和改进的Lipschitz正则化的认证鲁棒性训练算法，通过增加输出空间中的边界和正则化模型的Lipschitz常数来提高深度分类器对抗性扰动的鲁棒性。 |
| [^22] | [Learning Over Molecular Conformer Ensembles: Datasets and Benchmarks.](http://arxiv.org/abs/2310.00115) | 本研究引入了第一个MoleculAR Conformer Ensemble Learning（MARCEL）基准测试，以全面评估在构象集合上学习的潜力，并提出有前途的研究方向。 |
| [^23] | [HyperMask: Adaptive Hypernetwork-based Masks for Continual Learning.](http://arxiv.org/abs/2310.00113) | HyperMask是一种用于持续学习的方法，它使用基于超网络的掩码来训练一个单一网络，以克服人工神经网络在多任务上的灾难性遗忘问题。 |
| [^24] | [Reinforcement Learning for Node Selection in Branch-and-Bound.](http://arxiv.org/abs/2310.00112) | 本论文提出了一种在分支定界算法中使用强化学习进行节点选择的新方法。我们通过训练图神经网络来模拟整个树的状态，并使用概率分布来选择节点。尽管只在合成TSP实例上进行了训练，我们的方法在各种复杂问题集上得到了高质量的节点选择策略。 |
| [^25] | [Gradient and Uncertainty Enhanced Sequential Sampling for Global Fit.](http://arxiv.org/abs/2310.00110) | GUESS is a new sampling strategy for global fit that combines predictive posterior uncertainty and higher-order Taylor expansion values to reduce the number of samples needed for accurate surrogate modeling. - GUESS 是一种新的全局拟合采样策略，结合了预测后验不确定性和高阶泰勒展开值，可以减少准确的代理模型所需的样本数量。 |
| [^26] | [FedAIoT: A Federated Learning Benchmark for Artificial Intelligence of Things.](http://arxiv.org/abs/2310.00109) | FedAIoT是一个用于AIoT的联邦学习基准，包括八个数据集和一个统一的端到端FL框架。它填补了现有FL研究中缺乏真实物联网设备数据集的关键差距，并揭示了FL在AIoT领域的机遇和挑战。 |
| [^27] | [Practical Membership Inference Attacks Against Large-Scale Multi-Modal Models: A Pilot Study.](http://arxiv.org/abs/2310.00108) | 本文提出了一种针对大规模多模态模型的实际会员推断攻击方法，通过对目标点的文本和图像特征之间的余弦相似度进行阈值处理，并通过聚合相似度来进一步增强攻击效果。 |
| [^28] | [Latent Space Symmetry Discovery.](http://arxiv.org/abs/2310.00105) | 提出了一种新的生成模型——潜空间LieGAN（LaLiGAN），可以从数据中发现非线性对称性，并产生结构良好的潜空间，对其他下游任务非常有用。 |
| [^29] | [Federated Learning with Differential Privacy for End-to-End Speech Recognition.](http://arxiv.org/abs/2310.00098) | 本文提出了一种基于联邦学习和差分隐私的端到端语音识别方法，探索了大型Transformer模型的不同方面，并建立了基线结果。 |
| [^30] | [Towards Few-Call Model Stealing via Active Self-Paced Knowledge Distillation and Diffusion-Based Image Generation.](http://arxiv.org/abs/2310.00096) | 本研究提出了一种利用扩散模型生成合成数据集，并通过少次调用的方法窃取黑盒模型的框架，突破了访问限制条件。 |
| [^31] | [DataDAM: Efficient Dataset Distillation with Attention Matching.](http://arxiv.org/abs/2310.00093) | 本研究提出了一种基于注意力匹配的高效数据集精炼(DataDAM)方法。通过匹配空间注意力来学习合成图像，从而实现了最新技术水平的性能，同时减少了训练成本。 |
| [^32] | [Low-budget Black-box Optimization Algorithms Evaluated on BBOB and OpenAI Gym.](http://arxiv.org/abs/2310.00077) | 这项研究旨在探讨机器学习和黑盒优化之间的交叉应用潜力，并通过比较实验评估了低成本黑盒优化算法在不同领域的效果。 |
| [^33] | [EPiC-ly Fast Particle Cloud Generation with Flow-Matching and Diffusion.](http://arxiv.org/abs/2310.00049) | 本文提出了两种新方法用于高效准确地生成LHC喷注为点云，一种基于评分匹配扩散模型的等变点云架构，另一种是使用最优传输的等效连续标准化流模型。实验证明这些方法均达到了最先进的性能。 |
| [^34] | [Machine Learning Clifford invariants of ADE Coxeter elements.](http://arxiv.org/abs/2310.00041) | 本文研究了在线性变换中的Clifford几何不变量的机器学习方法，并实现了对ADE Coxeter元素的计算和数据挖掘。我们发现，不变量的输出完全取决于简单根的选择和对应反射的排列顺序，存在巨大的退化性。 |
| [^35] | [Cleanba: A Reproducible and Efficient Distributed Reinforcement Learning Platform.](http://arxiv.org/abs/2310.00036) | Cleanba是一种可复现和高效的分布式强化学习平台，它通过引入高度可复现的架构和经过高度优化的分布式变种，解决了分布式深度强化学习中的复现性问题，并在实验中展现了更短的训练时间和更具复现性的学习曲线。 |
| [^36] | [LoRA ensembles for large language model fine-tuning.](http://arxiv.org/abs/2310.00035) | 本文提出了一种使用低秩适配器（LoRA）的集成方法，用于解决大型语言模型微调中存在的不确定性量化问题，并提供了一个参数高效的微调技术。这种方法可以构建大规模的LoRA适配器集成，并具有与基础预训练模型相近的计算资源需求。 |
| [^37] | [PB-LLM: Partially Binarized Large Language Models.](http://arxiv.org/abs/2310.00034) | 本文提出的PB-LLM是一种部分二值化的大型语言模型压缩方法，可以在保持语言推理能力的同时实现极低比特量化，并通过后训练量化和量化感知训练等方法恢复量化LLMM的容量。 |
| [^38] | [Adversarial Driving Behavior Generation Incorporating Human Risk Cognition for Autonomous Vehicle Evaluation.](http://arxiv.org/abs/2310.00029) | 本论文开发了一种新型框架，结合人类风险认知来生成对手驾驶行为，用于评估自动驾驶车辆的有效性和弱点。 |
| [^39] | [Unlabeled Out-Of-Domain Data Improves Generalization.](http://arxiv.org/abs/2310.00027) | 这个论文提出了一种新的框架，可以将无标记的域外数据纳入半监督分类问题，从而改善泛化能力。该框架结合了分布鲁棒优化与自监督训练，并利用了高效的多项式时间算法。在理论上，该框架在高斯混合分类问题中得到了验证。 |
| [^40] | [De-SaTE: Denoising Self-attention Transformer Encoders for Li-ion Battery Health Prognostics.](http://arxiv.org/abs/2310.00023) | 本研究提出了De-SaTE方法，通过利用多个去噪模块以及自注意力变换编码器，准确预测锂离子电池的剩余寿命（RUL），为预防性维护和预测性分析提供关键指标估计。 |
| [^41] | [Investigation of factors regarding the effects of COVID-19 pandemic on college students' depression by quantum annealer.](http://arxiv.org/abs/2310.00018) | 通过量子退火技术，研究发现COVID-19疫情对大学生抑郁症的影响，并比较了基于量子退火和传统线性回归模型的结果。 |
| [^42] | [Artificial Empathy Classification: A Survey of Deep Learning Techniques, Datasets, and Evaluation Scales.](http://arxiv.org/abs/2310.00010) | 这篇论文综述了人工共情的分类研究，介绍了深度学习技术、数据集和评估标准的最新进展，指出训练人工共情的标准流程包括情绪识别、分析和响应动作。其中深度学习技术在虚拟代理和机器人中的应用有较高影响力。 |
| [^43] | [L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models.](http://arxiv.org/abs/2309.17446) | L2CEval是对大型语言模型的语言到代码生成能力进行系统评估的工作，分析了影响其性能的因素，并对置信度校准和人工评估进行了测量。 |
| [^44] | [Data Filtering Networks.](http://arxiv.org/abs/2309.17425) | 本文研究了学习数据过滤网络用于筛选大型未策划数据集的问题，并构建了新的数据过滤网络，从而产生最先进的图像-文本数据集。 |
| [^45] | [PlaceNav: Topological Navigation through Place Recognition.](http://arxiv.org/abs/2309.17260) | PlaceNav是一种通过地点识别进行拓扑导航的方法，将机器人无关部分分为导航特定和通用的计算机视觉组件，通过使用非机器人来源的大规模数据集增加训练数据的可用性，同时通过地点识别来提高导航性能。新模型的性能提高了76%。 |
| [^46] | [Benchmarking Collaborative Learning Methods Cost-Effectiveness for Prostate Segmentation.](http://arxiv.org/abs/2309.17097) | 本文通过比较联邦学习和基于共识的方法解决MRI前列腺分割的问题，在协作学习的情景中进行成本效益基准测试，首次使用基于共识的方法解决协作学习问题，具有显著的改进。 |
| [^47] | [On the Power of the Weisfeiler-Leman Test for Graph Motif Parameters.](http://arxiv.org/abs/2309.17053) | 这项研究探讨了图神经网络（GNN）表达能力和$k$维Weisfeiler-Leman ($k$WL)测试之间的关系，研究发现了$k$WL测试可以有效区分具有不同出现次数的模式图$P$的图形，并研究了模式图计数问题的最小维度$k$。 |
| [^48] | [Can LLMs Effectively Leverage Structural Information for Graph Learning: When and Why.](http://arxiv.org/abs/2309.16595) | 本文研究了大型语言模型（LLM）在图数据中的应用，发现LLM可以从结构信息中受益，尤其是在文本节点特征缺乏的情况下，而LLM的性能与数据泄露没有显著相关。 |
| [^49] | [Stackelberg Batch Policy Learning.](http://arxiv.org/abs/2309.16188) | Stackelberg批量策略学习是一种新颖的基于随机梯度的学习算法，采用博弈论的观点，对策略学习进行建模，并考虑了优化景观中的分层决策结构。 |
| [^50] | [ICML 2023 Topological Deep Learning Challenge : Design and Results.](http://arxiv.org/abs/2309.15188) | 本文介绍了ICML 2023拓扑深度学习挑战，该挑战要求参与者在两个月内提供开源实现的拓扑神经网络，吸引了28个合格的提交。 |
| [^51] | [SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem.](http://arxiv.org/abs/2309.15111) | 本研究通过在两层神经网络上使用小批量SGD算法，在具有二次真实函数分隔数据的情况下，通过训练数量级为$d \:\text{polylog}(d)$的样本，将网络训练到了人口误差为$o(1)$的程度。这是首次在标准神经网络上以及标准训练下，展示了在各向同性数据上高效学习XOR函数的样本复杂度为$\tilde{O}(d)$。 |
| [^52] | [Class Incremental Learning via Likelihood Ratio Based Task Prediction.](http://arxiv.org/abs/2309.15048) | 该论文提出了一种基于似然比的任务预测的类增量学习方法，利用离群检测器进行任务标识预测，解决了无任务标识符的测试样本的任务预测问题。 |
| [^53] | [Are Human-generated Demonstrations Necessary for In-context Learning?.](http://arxiv.org/abs/2309.14681) | 本文研究了上下文学习中人工生成的演示是否有必要，并提出了一种新的自反思提示策略（SEC），通过这种策略，大型语言模型（LLMs）可以自行生成演示和最终输出，避免了手动生成过程的复杂性。 |
| [^54] | [Physics of Language Models: Part 3.2, Knowledge Manipulation.](http://arxiv.org/abs/2309.14402) | 本文研究了语言模型在推理过程中操控知识的能力，发现预训练模型在知识检索方面表现出色，但在简单的分类、比较和逆向搜索任务中表现不佳。作者还提供了一个合成数据集进行实验，验证了这些内在的弱点：语言模型无法高效地操控知识。 |
| [^55] | [LinGCN: Structural Linearized Graph Convolutional Network for Homomorphically Encrypted Inference.](http://arxiv.org/abs/2309.14331) | LinGCN是一个旨在减少乘法深度和优化HE基于GCN推断性能的框架，通过结构化线性化算法和参数化的离散指示函数的联合训练，实现细粒度的节点级非线性位置选择。 |
| [^56] | [Backorder Prediction in Inventory Management: Classification Techniques and Cost Considerations.](http://arxiv.org/abs/2309.13837) | 本文介绍了一种先进的分析方法，用于预测库存管理中的缺货情况。该方法考虑了多种分类技术和成本考虑，通过提高服务水平来提高客户满意度和整体组织绩效。 |
| [^57] | [A Model-Agnostic Graph Neural Network for Integrating Local and Global Information.](http://arxiv.org/abs/2309.13459) | MaGNet是一种模型无关的图神经网络框架，能够顺序地整合不同顺序的信息，并通过识别有影响力的紧凑图结构提供有意义且可解释的结果。 |
| [^58] | [State-space Models with Layer-wise Nonlinearity are Universal Approximators with Exponential Decaying Memory.](http://arxiv.org/abs/2309.13414) | 本论文证明了堆叠具有逐层非线性激活的状态空间模型足以逼近任何连续的序列到序列关系，并且发现其加强了模型学习复杂序列模式的能力。然而，状态空间模型并不能根本解决指数衰减记忆的问题。 |
| [^59] | [Distributional Shift-Aware Off-Policy Interval Estimation: A Unified Error Quantification Framework.](http://arxiv.org/abs/2309.13278) | 本研究提出了一个对于强化学习非同策略评估问题的统一误差量化框架，并解决了分布偏移的挑战。通过在一个单一的区间内共同量化两个估计误差源，该框架揭示了之前隐藏的误差权衡，从而提高了置信区间的准确性。 |
| [^60] | [Sharpness-Aware Minimization and the Edge of Stability.](http://arxiv.org/abs/2309.12488) | 本研究通过类似的计算方法，为锐度感知最小化(SAM)，一种改进泛化性能的梯度下降变种，确定了一个稳定性边界，该边界取决于梯度的范数。 |
| [^61] | [Design of Chain-of-Thought in Math Problem Solving.](http://arxiv.org/abs/2309.11054) | 本论文研究了数学问题解决中思路链的设计方法，对比了自然语言思路链和程序思路链的效果，并发现程序思路链通常在数学问题解决中更加有效，特别是自我描述程序具有更大多样性且性能更高。此外，研究还发现Python是程序思路链的较好选择。实验结果为未来思路链设计提供了宝贵指导。 |
| [^62] | [Task Graph offloading via Deep Reinforcement Learning in Mobile Edge Computing.](http://arxiv.org/abs/2309.10569) | 本文研究了在移动边缘计算环境下通过深度强化学习实现任务图离载的问题。现有的工作往往无法适应环境变化，导致用户体验下降。我们提出了一种将任务图调度建模为马尔可夫决策过程的方法，以适应计算能力随时间变化的情况。 |
| [^63] | [Multi-Agent Deep Reinforcement Learning for Cooperative and Competitive Autonomous Vehicles using AutoDRIVE Ecosystem.](http://arxiv.org/abs/2309.10007) | 本研究提出了一个模块化和并行化的多智能体深度强化学习框架，在AutoDRIVE生态系统中培养合作与竞争行为。我们利用该生态系统开发了准确物理和逼真图形的数字孪生体，并使用它来训练和部署多智能体强化学习策略，实现了在自主车辆中的合作和竞争行为。 |
| [^64] | [Mechanic Maker 2.0: Reinforcement Learning for Evaluating Generated Rules.](http://arxiv.org/abs/2309.09476) | 本论文研究了将强化学习应用于游戏规则生成的人类游戏评估，并通过实验结果表明，强化学习生成的规则与传统基线方法有所不同，可能更适合人类使用。 |
| [^65] | [Reconstructing Existing Levels through Level Inpainting.](http://arxiv.org/abs/2309.09472) | 本论文介绍了内容增强和级别修复的方法，通过两种技术（自编码器和U-net）来重建和扩展视频游戏级别。经过综合案例研究，我们展示了这两种方法相对于基准方法的卓越性能。提供了未来研究方向的见解。 |
| [^66] | [Asymptotically Efficient Online Learning for Censored Regression Models Under Non-I.I.D Data.](http://arxiv.org/abs/2309.09454) | 这项研究提出了一种渐近高效的在线学习方法，应用于随机被审查回归模型，并在一般情况下达到了最好的性能。 |
| [^67] | [Adaptive Priority Reweighing for Generalizing Fairness Improvement.](http://arxiv.org/abs/2309.08375) | 本文提出了一种新颖的自适应重新加权方法，通过优先考虑靠近决策边界的样本并分配较高的权重，提高了公平分类器的泛化能力。 |
| [^68] | [Understanding the limitations of self-supervised learning for tabular anomaly detection.](http://arxiv.org/abs/2309.08374) | 本研究探讨了自监督学习在表格异常检测中的限制。通过多个实验发现，自监督学习得到的表征并不能提高表格异常检测的性能，这是由于神经网络引入了无关的特征。然而，使用神经网络表示的子空间可以恢复性能。 |
| [^69] | [Landscape-Sketch-Step: An AI/ML-Based Metaheuristic for Surrogate Optimization Problems.](http://arxiv.org/abs/2309.07936) | Landscape-Sketch-Step是一种基于AI/ML的元启发式方法，结合了机器学习、随机优化和强化学习技术，用于解决成本函数评估昂贵、不可访问或禁止的代理优化问题。 |
| [^70] | [Normality Learning-based Graph Anomaly Detection via Multi-Scale Contrastive Learning.](http://arxiv.org/abs/2309.06034) | 本文提出了一种基于正态学习的图异常检测框架NLGAD，通过多尺度对比学习网络来增强学习正常模式的能力，以改进异常检测性能。 |
| [^71] | [Zero-Shot Recommendations with Pre-Trained Large Language Models for Multimodal Nudging.](http://arxiv.org/abs/2309.01026) | 该论文提出了一种利用生成型AI领域的新技术进行零样本推荐的方法，通过将多模态输入转化为文本描述，并利用预训练的语言模型计算语义嵌入，实现了对非平稳内容的推荐。在合成的多模态暗示环境中进行实验证明了该方法的有效性。 |
| [^72] | [On the Implicit Bias of Adam.](http://arxiv.org/abs/2309.00079) | 本文证明了RMSProp和Adam存在隐式规范化作用，其取决于超参数和训练阶段，并讨论了这些证明事实对泛化的影响。 |
| [^73] | [BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge.](http://arxiv.org/abs/2308.16458) | BioCoder是一个用于评估预训练模型在生成生物信息学代码方面的基准，涵盖了函数代码生成中的包依赖关系、类声明和全局变量，并通过模糊测试框架进行评估。 |
| [^74] | [Noise-Free Sampling Algorithms via Regularized Wasserstein Proximals.](http://arxiv.org/abs/2308.14945) | 本文通过正则化Wasserstein Proximal方法提出了一种无噪声的抽样算法，通过给定的潜势函数确定性地进行粒子演化，并提供了优于传统方法的维度依赖性和速度收敛性能。 |
| [^75] | [Escaping the Sample Trap: Fast and Accurate Epistemic Uncertainty Estimation with Pairwise-Distance Estimators.](http://arxiv.org/abs/2308.13498) | 本文介绍了使用配对距离估计器对集成模型进行认识不确定性估计的新方法，相比于常用的深度学习方法，该方法能够更快速、更准确地在更大的空间和更高维度上估计认识不确定性。 |
| [^76] | [Compressor-Based Classification for Atrial Fibrillation Detection.](http://arxiv.org/abs/2308.13328) | 本文研究了基于压缩机的文本分类方法在房颤检测中的应用。通过对心律间隔序列进行压缩距离计算和最近邻分类器模型优化，我们实现了良好的分类性能，接近于最佳的房颤检测算法。这表明gzip分类器适用于生物医学数据和连续随机序列的分类。 |
| [^77] | [Prompt-Based Length Controlled Generation with Reinforcement Learning.](http://arxiv.org/abs/2308.12030) | 提出了一种基于提示的长度控制方法，利用强化学习和奖励模型来实现大型语言模型（LLM）的长度受控生成。该方法可以有效减少推理成本并满足不同需求。 |
| [^78] | [Mitigating Health Disparity on Biased Electronic Health Records via Deconfounder.](http://arxiv.org/abs/2308.11819) | 通过使用去混淆器模型FLMD，在电子健康记录建模中实现了公平性和准确性，解决了有偏见的EHR中的健康差异问题。 |
| [^79] | [Time Travel in LLMs: Tracing Data Contamination in Large Language Models.](http://arxiv.org/abs/2308.08493) | 该论文提出了一种用于识别大型语言模型（LLMs）中数据污染的简单而有效的方法。通过对随机样本中的单个实例进行分析，以及使用“引导指令”来评估整个数据集分区的污染程度，可以准确地识别污染的实例和分区。 |
| [^80] | [Ada-QPacknet -- adaptive pruning with bit width reduction as an efficient continual learning method without forgetting.](http://arxiv.org/abs/2308.07939) | Ada-QPacknet是一种自适应剪枝与位宽缩减的高效继续学习方法，通过剪枝和量化技术生成任务子网络，在动态和复杂环境中实现了与浮点数子网络相似的准确性。 |
| [^81] | [Fourier neural operator for real-time simulation of 3D dynamic urban microclimate.](http://arxiv.org/abs/2308.03985) | 该论文介绍了傅里叶神经运算器（FNO）用于实时模拟3D动态城市微气候。通过深度学习技术，FNO在加速解决复杂非线性相互作用和系统动力学建模方面表现出很大的潜力。 |
| [^82] | [FLIPS: Federated Learning using Intelligent Participant Selection.](http://arxiv.org/abs/2308.03901) | 本文介绍了FLIPS，这是一个用于管理联邦学习中数据和参与者异质性的中间件系统。FLIPS通过标签分布聚类和智能参与者选择，并使用可信执行环境来确保隐私保护。实证评估表明，FLIPS相比随机方法有更好的性能。 |
| [^83] | [Branched Latent Neural Operators.](http://arxiv.org/abs/2308.02599) | Branched Latent Neural Operators (BLNOs) are introduced to learn input-output maps for encoding complex physical processes. BLNOs utilize interpretable latent outputs to enhance learned dynamics and overcome the curse of dimensionality, while also showing excellent generalization properties with small training datasets and short training times. The partial connection structure reduces the number of tunable parameters. BLNOs are proven effective in a challenging biophysically detailed test case. |
| [^84] | [General Anomaly Detection of Underwater Gliders Validated by Large-scale Deployment Dataset.](http://arxiv.org/abs/2308.00180) | 本文介绍了一种用于评估海底滑翔器在不可预测海洋环境中正常操作的异常检测算法，并通过实际滑翔器部署的大规模数据集进行验证。算法能够实时提供异常警报，使驾驶员能够控制滑翔器并避免进一步损害。 |
| [^85] | [Deep Learning Meets Adaptive Filtering: A Stein's Unbiased Risk Estimator Approach.](http://arxiv.org/abs/2307.16708) | 本文通过算法展开的方法，将递归最小二乘法（RLS）和等变自适应源分离（EASI）两种算法转化为深度神经网络的层，通过利用训练过程有效地进行源信号估计。同时，通过使用基于Stein无偏风险估计（SURE）的损失函数训练，进一步提高了性能，实证评估证明了该方法的有效性。 |
| [^86] | [An Efficient Approach to Mitigate Numerical Instability in Backpropagation for 16-bit Neural Network Training.](http://arxiv.org/abs/2307.16189) | 这项研究探讨了16位计算中机器学习模型的数值不稳定性问题，并提出了一种基于Adam优化器的新方法来提高16位神经网络的学习过程的鲁棒性。 |
| [^87] | [On the Trade-off Between Efficiency and Precision of Neural Abstraction.](http://arxiv.org/abs/2307.15546) | 本研究探讨了神经抽象的效率和精确性之间的权衡问题，研究发现抽象的用途取决于具体场景，请求简单的粗略抽象同样会有其用途，而对于更复杂 |
| [^88] | [A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis.](http://arxiv.org/abs/2307.12856) | 这篇论文介绍了一种名为WebAgent的LLM驱动代理，通过自我经验学习，在真实网站上完成任务。该方法通过规划、总结和生成代码来提高在真实网站上的成功率。 |
| [^89] | [Language-based Action Concept Spaces Improve Video Self-Supervised Learning.](http://arxiv.org/abs/2307.10922) | 这项研究使用语言相关的自监督学习方法，将图像CLIP模型调整为适用于视频领域，并通过在动作概念空间中进行自蒸馏训练，提高了零样本和线性推测性能。 |
| [^90] | [Deep projection networks for learning time-homogeneous dynamical systems.](http://arxiv.org/abs/2307.09912) | 这篇论文介绍了一种利用深度投影网络学习时间齐次动力系统的有意义表示的方法。通过优化类似于经典相关分析的目标函数，避免了矩阵求逆的稳定性问题，并通过两种正则化方案进一步增强学习效果。 |
| [^91] | [Adversarial Likelihood Estimation with One-way Flows.](http://arxiv.org/abs/2307.09882) | 本文提出了一种通过单向流进行对抗性似然估计的方法，并使用重要性采样解决了Wasserstein GAN中分区函数有偏估计的问题。同时，通过最大化生成器的熵，提高了模式覆盖效果。这种方法通过计算生成样本的密度来实现对分区函数的无偏估计和生成器熵的计算。 |
| [^92] | [Tangent Model Composition for Ensembling and Continual Fine-tuning.](http://arxiv.org/abs/2307.08114) | 切线模型组合 (TMC) 是一种将独立微调的模型结合的方法，可以用于增量学习、集成和取消学习。通过标量组合方式进行组合，提高了准确率并降低了推理成本。该方法可以零成本忘记组成模型，不受顺序偏差的限制，并能在联合数据上并行执行。在任务增量、类别增量和数据增量设置中，TMC几乎在每个方案上均优于最近发表的持续微调方法。 |
| [^93] | [Memorization Through the Lens of Curvature of Loss Function Around Samples.](http://arxiv.org/abs/2307.05831) | 本研究通过对损失函数曲率进行分析，研究了神经网络在不同样本上的泛化与记忆化特性。我们发现高曲率的样本通常是具有标签错误或冲突的长尾样本，并在CIFAR100数据集上发现了一种新的失败模型。通过对部分样本进行随机标签错误，我们展示了曲率排序可以有效识别出这些样本。 |
| [^94] | [DADO -- Low-Cost Query Strategies for Deep Active Design Optimization.](http://arxiv.org/abs/2307.04536) | 这项研究将深度主动学习应用于设计优化中，通过预测性能并只模拟有前途的候选模拟，实现了大幅度节约计算资源的目标。通过提出的低成本查询策略，该方法在多目标设计优化问题中取得了显著的改进，并避免了对不确定性估计的要求。在流体动力学领域的实证评估中得到了验证。 |
| [^95] | [Score-based Conditional Generation with Fewer Labeled Data by Self-calibrating Classifier Guidance.](http://arxiv.org/abs/2307.04081) | 本论文提出了一种通过自校准分类器引导的方法改进基于评分的条件生成模型，以提高使用少量标记数据的准确性和性能。通过将分类器作为无条件生成模型的另一种视角，并利用标记和未标记数据来校准分类器，实验证实该方法的有效性。 |
| [^96] | [Building and Road Segmentation Using EffUNet and Transfer Learning Approach.](http://arxiv.org/abs/2307.03980) | 本论文提出了一种使用EffUNet和迁移学习方法进行建筑和道路分割的新架构。利用这种方法，在马萨诸塞州建筑物和道路数据集上取得了令人满意的分数。 |
| [^97] | [Steel Surface Roughness Parameter Calculations Using Lasers and Machine Learning Models.](http://arxiv.org/abs/2307.03723) | 本研究利用机器学习模型提高了钢铁表面粗糙度参数的计算准确性，通过对比不同方法，评估了在线测量转化的潜力。 |
| [^98] | [Monte Carlo Sampling without Isoperimetry: A Reverse Diffusion Approach.](http://arxiv.org/abs/2307.02037) | 本研究提出了一种无等渗性的蒙特卡洛采样方法，通过逆扩散过程实现了新颖的后验采样算法，在高维采样中表现出更优越的性能。 |
| [^99] | [Bridge the Performance Gap in Peak-hour Series Forecasting: The Seq2Peak Framework.](http://arxiv.org/abs/2307.01597) | 本文提出了Seq2Peak框架，针对高峰小时序列预测任务，该框架通过解决高度非平稳性和性能评估问题，成功缩小了在常规时间序列预测模型中观察到的性能差距。 |
| [^100] | [The Underlying Scaling Laws and Universal Statistical Structure of Complex Datasets.](http://arxiv.org/abs/2306.14975) | 本文研究了复杂数据集中的底层缩放定律和普适统计结构。通过将数据类比为物理系统，并应用统计物理学和随机矩阵理论的方法，揭示了特征-特征协方差矩阵的局部和全局特征值统计量的规律。研究发现，在无关随机数据和真实数据之间存在显著差异，并且可以通过引入长程相关性完全恢复缩放行为。同时，生成的数据和真实世界数据都属于混沌系统，并在较小的数据集大小上即可体现随机矩阵理论的统计行为。 |
| [^101] | [Scaling MLPs: A Tale of Inductive Bias.](http://arxiv.org/abs/2306.13575) | 本文研究了多层感知器（MLP）在视觉任务中的性能极限，并探讨了MLP相较于其他深度学习模型的归纳偏差，旨在推进深度学习理论和实践的结合。 |
| [^102] | [Data augmentation for recommender system: A semi-supervised approach using maximum margin matrix factorization.](http://arxiv.org/abs/2306.13050) | 本研究提出了一种基于最大边际矩阵分解的半监督方法来增广和细化协同过滤算法的评级预测。该方法利用自我训练来评估评分的置信度，并通过系统的数据增广策略来提高算法性能。 |
| [^103] | [SPRINT: Scalable Policy Pre-Training via Language Instruction Relabeling.](http://arxiv.org/abs/2306.11886) | SPRINT 提出了一种离线策略预训练方法，通过指令重标记及离线强化学习实现可扩展的预训练任务，大大减少了预训练所需的人力，同时使机器人能够获取更丰富的技能库，相较于之前的预训练方法，能够更快地学习新的长时间跨度任务。 |
| [^104] | [Textbooks Are All You Need.](http://arxiv.org/abs/2306.11644) | phi-1是一个新的大型代码语言模型，通过精心训练和优化，尽管规模相对较小，但在准确率和新的性质方面表现出了令人惊讶的结果。 |
| [^105] | [AI Driven Near Real-time Locational Marginal Pricing Method: A Feasibility and Robustness Study.](http://arxiv.org/abs/2306.10080) | 本研究评估了基于机器学习和深度学习的模型在预测定位边际定价（LMP）时的性能，并发现这些模型在多个电力网络上具有准确且稳健的预测能力。 |
| [^106] | [MUBen: Benchmarking the Uncertainty of Pre-Trained Models for Molecular Property Prediction.](http://arxiv.org/abs/2306.10060) | MUBen评估不同骨干和UQ模型组合对分子不确定性估计和属性预测的性能，以解决预训练模型微调中的过拟合校准问题。 |
| [^107] | [Towards Faster Non-Asymptotic Convergence for Diffusion-Based Generative Models.](http://arxiv.org/abs/2306.09251) | 该论文针对扩散生成模型设计了非渐进理论，提出了针对两种主流采样器的新的收敛速度，提高了总步数与收敛速度的比例。 |
| [^108] | [Noise Stability Optimization for Flat Minima with Optimal Convergence Rates.](http://arxiv.org/abs/2306.08553) | 本文提出了一个SGD-like算法，注入随机噪声并利用分布对称性来减少方差，以寻找具有低海森矩阵迹的平坦极小值，同时提供了收敛速率分析。 |
| [^109] | [MMASD: A Multimodal Dataset for Autism Intervention Analysis.](http://arxiv.org/abs/2306.08243) | 提出了一个名为MMASD的自闭症多模态数据集，收集自治疗干预。它包括从32名自闭症患儿的干预录音中分段的1,315个数据样本，每个样本包含四种隐私保护模式的数据。 |
| [^110] | [Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models.](http://arxiv.org/abs/2306.08018) | Mol-Instructions是一个专门为生物分子领域设计的综合指令数据集，可以显著提高大语言模型在生物领域中的适应能力和认知敏锐度。 |
| [^111] | [Variational Imbalanced Regression.](http://arxiv.org/abs/2306.06599) | 本文提出的变分不平衡回归（VIR）模型通过引入Probabilistic Reweighting方法，可以在不平衡回归方面表现良好，并自然产生合理的不确定性估计。 |
| [^112] | [Push: Concurrent Probabilistic Programming for Bayesian Deep Learning.](http://arxiv.org/abs/2306.06528) | Push是一个并发概率编程库，用于贝叶斯深度学习（BDL），可以在多GPU硬件上执行BDL推理算法。该库通过将神经网络表示为粒子，并允许粒子之间的异步通信和各种参数更新，简化了BDL实验和扩展粒子操作的过程。 |
| [^113] | [Learning a Neuron by a Shallow ReLU Network: Dynamics and Implicit Bias for Correlated Inputs.](http://arxiv.org/abs/2306.06479) | 这项研究证明，通过梯度流训练一个宽度任意的一层ReLU网络，可以学习一个单个神经元并收敛到零误差，同时隐式偏向于最小化网络参数的秩。这对于相关的训练数据点是有效的，与之前研究正交数据集的结果补充了彼此。 |
| [^114] | [Reconstructing Human Expressiveness in Piano Performances with a Transformer Network.](http://arxiv.org/abs/2306.06040) | 本文提出了一种利用Transformer网络来重建钢琴演奏中人类表现力的方法，使用转录乐谱来训练模型，整合钢琴家身份以控制采样过程，并成功生成了高度类人的钢琴表演。 |
| [^115] | [Enhancing Robustness of AI Offensive Code Generators via Data Augmentation.](http://arxiv.org/abs/2306.05079) | 本论文提出了一种方法，通过在代码描述中引入扰动来增强AI攻击性代码生成器的鲁棒性，并证明数据增强可有效提高代码生成器对扰动和非扰动的代码描述的性能。 |
| [^116] | [Scalable and Adaptive Log-based Anomaly Detection with Expert in the Loop.](http://arxiv.org/abs/2306.05032) | 本文提出了一种适用于大规模云系统的准确、轻量级、自适应的基于日志的异常检测框架——SeaLog。该方法利用一种基于 Trie 结构的动态增长检测代理，可以接收人类专家反馈，能够在不断变化的日志数据中实现高准确度的实时异常检测，从而减少了手动验证的工作量。 |
| [^117] | [GPT-FL: Generative Pre-trained Model-Assisted Federated Learning.](http://arxiv.org/abs/2306.02210) | GPT-FL是一种生成预训练模型辅助的联邦学习框架，通过生成多样化的合成数据并结合私有客户端数据进行训练，它在模型准确性、通信效率和客户端采样效率等方面优于最先进的方法。在FL训练中，由合成数据生成的下游模型对于控制梯度多样性的方向起着关键作用，提高了收敛速度，并显著提升了准确性。 |
| [^118] | [PAGAR: Taming Reward Misalignment in Inverse Reinforcement Learning-Based Imitation Learning with Protagonist Antagonist Guided Adversarial Reward.](http://arxiv.org/abs/2306.01731) | PAGAR是一种用于解决IRL-based IL中奖励不对齐问题的半监督奖励设计方法，在复杂IL任务和零-shot IL任务中表现优越。 |
| [^119] | [Gode -- Integrating Biochemical Knowledge Graph into Pre-training Molecule Graph Neural Network.](http://arxiv.org/abs/2306.01631) | 本研究提出了一种新的方法，在分子结构和生物医学知识图谱中集成多个领域信息，通过自我监督策略预先训练更广泛和更强大的表示，并在化学属性预测任务上展示出出色的性能。 |
| [^120] | [Meta-Learning Framework for End-to-End Imposter Identification in Unseen Speaker Recognition.](http://arxiv.org/abs/2306.00952) | 本文提出了一个元学习框架，用于未知说话人识别中的冒名者识别。通过引入强健的说话人特定阈值技术和端到端元学习，将冒名者检测从未知说话人识别中分离出来，并通过利用注册说话人的话语来学习检测冒名者，相比基线提高了10%。 |
| [^121] | [Understanding Augmentation-based Self-Supervised Representation Learning via RKHS Approximation.](http://arxiv.org/abs/2306.00788) | 本文通过RKHS逼近方法，揭示了自监督表示学习中好的数据增强的重要性，并阐述了对于任意编码器，增广函数质量的提升可以提高编码器的表示能力，同时分析了批量归一化和数据增强在自监督学习中的作用。 |
| [^122] | [A Geometric Perspective on Diffusion Models.](http://arxiv.org/abs/2305.19947) | 本文研究了扩散模型的几何结构，发现通过一个明确的准线性采样轨迹和另一个隐式的去噪轨迹平滑连接了数据分布和噪声分布，建立了基于ODE的最优采样和经典的均值漂移算法之间的理论关系。 |
| [^123] | [Stable Anisotropic Regularization.](http://arxiv.org/abs/2305.19358) | 本文提出了一种新颖的正则化方法I-STAR，可以增加模型的稳定性，提高性能，并改善自然语言处理中的组合表示问题。 |
| [^124] | [Auto-tune: PAC-Bayes Optimization over Prior and Posterior for Neural Networks.](http://arxiv.org/abs/2305.19243) | 通过提出一种PAC-Bayes训练框架，无需额外正则化和网格搜索调整超参数即可达到与传统方法相媲美的测试性能，显著提高神经网络泛化能力并具有实际应用价值。 |
| [^125] | [How to Query Human Feedback Efficiently in RL?.](http://arxiv.org/abs/2305.18505) | 该论文提出了一种针对强化学习中人类反馈查询的有效采样方法，以在最少的人类反馈下学习最佳策略，并可应用于具有线性参数化和未知过渡的偏好模型，并引入了基于行动比较反馈的RLHF。 |
| [^126] | [Statistically Optimal K-means Clustering via Nonnegative Low-rank Semidefinite Programming.](http://arxiv.org/abs/2305.18436) | 本文提出了一种与NMF算法一样简单且可扩展的K均值聚类算法，该算法通过解决非负低秩半定规划问题获得了强大的统计最优性保证，实验证明该算法在合成和实际数据集上表现优异。 |
| [^127] | [Learning Two-Layer Neural Networks, One (Giant) Step at a Time.](http://arxiv.org/abs/2305.18270) | 本文研究了浅层神经网络的训练动态及其条件，证明了动态下梯度下降可以通过有限数量的大批量梯度下降步骤来促进特征学习，并找到了多个和单一方向的最佳批量大小，有助于促进特征学习和方向的专业化。 |
| [^128] | [A Slingshot Approach to Learning in Monotone Games.](http://arxiv.org/abs/2305.16610) | 本文提出了一种新的框架, 通过正则化游戏的支付或效用和更新投石索策略，无论是否存在噪声都能够实现在单调博弈中计算均衡。 |
| [^129] | [Imitating Task and Motion Planning with Visuomotor Transformers.](http://arxiv.org/abs/2305.16309) | 本研究提出了一种名为OPTIMUS的新型模仿学习系统，通过模仿TAMP代理来训练大规模的视觉动作转换器策略。OPTIMUS引入了一个专门为模仿学习而设计的TAMP数据生成管道，可以用来训练性能优越的基于转换器的策略。 |
| [^130] | [C-MCTS: Safe Planning with Monte Carlo Tree Search.](http://arxiv.org/abs/2305.16209) | C-MCTS 提出了一种解决有约束的决策问题的方法，通过训练安全评判器进行成本估计，并在部署期间通过剪枝不安全轨迹来限制探索，实现了更高的奖励和更高效的规划步骤。 |
| [^131] | [Passive learning of active causal strategies in agents and language models.](http://arxiv.org/abs/2305.16183) | 通过被动学习，在智能体和语言模型中可以学习到一般化的主动因果策略，用于确定和使用因果关系结构。通过模仿专家数据进行训练的智能体能够在测试时推断和使用从未出现的因果链接，并将实验策略推广到从未观察到的新变量集。 |
| [^132] | [Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation.](http://arxiv.org/abs/2305.15852) | 本文对大型语言模型的自相矛盾幻觉进行了评估、检测和缓解，探究了这一幻觉形式的普遍存在性。通过设计框架有效触发自相矛盾，发现不同语言模型中这种现象都频繁出现。ChatGPT和GPT-4能够准确识别自相矛盾，而Vicuna-13B则有些困难。 |
| [^133] | [On Architectural Compression of Text-to-Image Diffusion Models.](http://arxiv.org/abs/2305.15798) | 本文研究了如何通过架构压缩方法实现文本到图像生成模型的高效化，提出了一种块删除知识提取SDMs（BK-SDMs）方法，在减少采样步骤数量和利用网络量化的同时，可以显著减少模型的参数数量、MAC和延迟，最终实现了与使用更多资源训练的模型相竞争的效果。 |
| [^134] | [Size Generalizability of Graph Neural Networks on Biological Data: Insights and Practices from the Spectral Perspective.](http://arxiv.org/abs/2305.15611) | 本文通过谱角度的方法，研究了GNNs的尺寸可泛化性问题，并在真实生物数据集上进行了实验，发现GNNs在度分布和谱分布偏移时均表现敏感，在同一数据集的大图上的性能仍然下降，揭示了 GNNs的尺寸可泛化性问题。 |
| [^135] | [Optimal Rates for Bandit Nonstochastic Control.](http://arxiv.org/abs/2305.15352) | 本研究解决了带有拟对抗扰动和时变对抗性赌博损失函数的LQR和LQG问题，并提供了一种新颖的具有记忆的赌博凸优化方案。 |
| [^136] | [An Unsupervised Method for Estimating Class Separability of Datasets with Application to LLMs Fine-Tuning.](http://arxiv.org/abs/2305.15016) | 这个论文提出了一种无监督方法，通过利用数据流形的拓扑特征估计数据的类别可分性，该方法与有监督度量具有相关性，可以用于半监督和感知学习。同时，在语言模型微调中应用该方法用于自动停止准则。 |
| [^137] | [Provable Offline Reinforcement Learning with Human Feedback.](http://arxiv.org/abs/2305.14816) | 本文提出了一种具有人类反馈的离线强化学习算法，解决了如何估计隐式奖励以及在置信集周围解决规划问题的方法。此外，作者提出了一个能够使用多项式数量的样本学习任何目标策略的新保证，同时引入了一个新的单策略集中系数来衡量目标策略的覆盖范围。 |
| [^138] | [Constant Memory Attentive Neural Processes.](http://arxiv.org/abs/2305.14567) | 提出了一种不变存储的注意力神经过程 (CMANPs) 及其注意力块 CMAB，能在常数内存下进行条件、查询和更新操作，并在元回归和少样本回归任务上获得最先进的表现。 |
| [^139] | [Amplitude-Independent Machine Learning for PPG through Visibility Graphs and Transfer Learning.](http://arxiv.org/abs/2305.14062) | 该论文提出了一种不依赖于振幅的光 plethysmography (PPG) 机器学习方法，通过可见性图和迁移学习实现了对心率和血管老化等生物特征的稳健估计和预测。 |
| [^140] | [Unsupervised ASR via Cross-Lingual Pseudo-Labeling.](http://arxiv.org/abs/2305.13330) | 本研究提出了一种基于跨语言伪标注的无监督ASR方法，能够使用其他语言中的标注数据来引导新语言的无监督AM。在Common Voice上取得了良好的效果，可以实现18% WER。而且在不同语言的数据集上都优于基线模型。 |
| [^141] | [Training Diffusion Models with Reinforcement Learning.](http://arxiv.org/abs/2305.13301) | 本文研究了利用强化学习方法直接优化扩散模型以实现下游对象的问题，并提出一种称之为去噪扩散策略优化（DDPO）的有效策略梯度算法，能够适应难以通过提示表达的图像压缩等目标，以及通过人类反馈得出的美学质量等目标。 |
| [^142] | [GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs.](http://arxiv.org/abs/2305.12788) | 本论文提出了一种名为GraphCare的框架，通过使用个性化知识图谱来改进基于电子健康记录的医疗预测，并通过在两个公共数据集上的实验证明了其有效性。 |
| [^143] | [HighLight: Efficient and Flexible DNN Acceleration with Hierarchical Structured Sparsity.](http://arxiv.org/abs/2305.12718) | HighLight是一种高效灵活的DNN加速器，通过分层结构稀疏性实现了对DNN的有效加速，优化了能耗和延迟的权衡。 |
| [^144] | [Multimodal Web Navigation with Instruction-Finetuned Foundation Models.](http://arxiv.org/abs/2305.11854) | 本文研究使用视觉语言基础模型进行数据驱动离线训练的 Web 代理，提出了一个指令跟随多模态代理WebGUM，将微调指令微调语言模型和视觉转换器，能够有效提高代理的基于视觉感知、HTML 理解和多步推理的能力。 |
| [^145] | [Structural Pruning for Diffusion Models.](http://arxiv.org/abs/2305.10924) | 本文提出了一种名为Diff-Pruning的高效压缩方法，通过一个Taylor展开过程来识别重要权重，从而从预先存在的模型中学习轻量级扩散模型，性能稳定，并在训练效率上显著提高。 |
| [^146] | [Semantically Aligned Task Decomposition in Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2305.10865) | 该论文提出了一种多智能体强化学习中的新方法SAMA，通过提前训练的语言模型和任务分解来解决ASG方法存在的样本效率问题和生成非实际任务奖励的子目标的问题。 |
| [^147] | [Evaluation Metrics for CNNs Compression.](http://arxiv.org/abs/2305.10616) | 本文提供了神经网络压缩的评估度量综述，从而为标准化神经网络压缩贡献力量。 |
| [^148] | [Grasping Extreme Aerodynamics on a Low-Dimensional Manifold.](http://arxiv.org/abs/2305.08024) | 研究探索处理极端气动学问题的基本物理机制方法。 |
| [^149] | [A Variational Perspective on Solving Inverse Problems with Diffusion Models.](http://arxiv.org/abs/2305.04391) | 该论文提出了一种通过去噪扩散过程自然地导致正则化的变分方法（RED-Diff），可用于解决不同反问题。加权机制可以衡量不同时间步长的去噪器的贡献。 |
| [^150] | [High-dimensional Bayesian Optimization via Semi-supervised Learning with Optimized Unlabeled Data Sampling.](http://arxiv.org/abs/2305.02614) | 本文提出基于半监督学习的高维贝叶斯优化方法，利用特定的未标记数据采样、参数化采样分布的优化及动态选择无标记数据等策略，解决了高维贝叶斯优化难以处理的问题。 |
| [^151] | [Differentially Private In-Context Learning.](http://arxiv.org/abs/2305.01639) | 本文提出了DP-ICL，实现了在隐私保证下对新任务的适应性。经过四个基准测试，发现其性能与非私有ICL相当。 |
| [^152] | [Towards Efficient and Comprehensive Urban Spatial-Temporal Prediction: A Unified Library and Performance Benchmark.](http://arxiv.org/abs/2304.14343) | 本研究提出了一种称为原子文件的统一空间时间数据存储格式，开发了一个名为LibCity的开源库，重新构建了65个空间时间预测模型，并收集了55个空间时间数据集。同时还提出了城市时空预测模型的性能基准，为这一领域提供了一个可靠的评估工具。 |
| [^153] | [Total-Recon: Deformable Scene Reconstruction for Embodied View Synthesis.](http://arxiv.org/abs/2304.12317) | 本论文介绍了Total-Recon，这是第一个从长的单目RGBD视频中实现光实感的可变形场景重建方法，用于合成实体视角。 |
| [^154] | [On Accelerating Diffusion-Based Sampling Process via Improved Integration Approximation.](http://arxiv.org/abs/2304.11328) | 本文提出了一种通过优化系数来加速流行的基于反向ODE解算的扩散采样过程的方法，优化方法为改进的积分逼近（IIA），在每个反向时间步长，我们建议针对某些选择的系数最小化MSE函数，给定预训练的扩散模型，只需要在一批样本上进行特定数量的神经功能评估（NFEs）一次IIA过程即可获得最佳解。 |
| [^155] | [The Isotonic Mechanism for Exponential Family Estimation.](http://arxiv.org/abs/2304.11160) | 本文利用扩展的保序机制，将其应用于指数族分布以提高同行评审的质量，并发现作者的同行评分可以较准确地在不需要知道具体分布情况下进行调整。 |
| [^156] | [Sample-efficient Model-based Reinforcement Learning for Quantum Control.](http://arxiv.org/abs/2304.09718) | 本论文提出了一种基于模型的强化学习方法，通过受到神经常微分方程进展的启发，这个方法采用自动微分的ODE表达由可学习的汉密尔顿安排参数化的模型来近似环境，在门控制和汉密尔顿参数的学习中通过系统交互解决问题。该方法在样本复杂度方面比标准基于模型自由的强化学习方法具有一个数量级的优势，适用于噪声时变门优化。 |
| [^157] | [Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling.](http://arxiv.org/abs/2304.07665) | 本文提出了一个新方法，利用贝叶斯分层建模，动态平衡探索-开发权衡，以更好地查询数据点。 |
| [^158] | [OKRidge: Scalable Optimal k-Sparse Ridge Regression for Learning Dynamical Systems.](http://arxiv.org/abs/2304.06686) | 本文提出了一种名为 OKRidge 的方法，用于确定非线性动态系统的稀疏控制方程，并通过求解稀疏岭回归问题，实现了可扩展性和快速性，和现有方法相比，有着更高的效率。 |
| [^159] | [Energy-guided Entropic Neural Optimal Transport.](http://arxiv.org/abs/2304.06094) | 本论文提出了一种新的方法，将能量基础模型和熵正则化最优输运结合起来，以解决生成建模问题。我们在2D情景和图像到图像翻译问题中验证了该方法的适用性。 |
| [^160] | [A review of ensemble learning and data augmentation models for class imbalanced problems: combination, implementation and evaluation.](http://arxiv.org/abs/2304.02858) | 本文研究了集成学习和数据增强方法的应用，针对类别不平衡问题，通过计算评估，找到了最有效的组合。 |
| [^161] | [SEENN: Towards Temporal Spiking Early-Exit Neural Networks.](http://arxiv.org/abs/2304.01230) | 本研究提出了一种名为SEENN的方法，通过对时间步数进行细粒度调整，以减少不必要的计算并提高有效性。同时，SEENN达到了多个基准数据集的最先进准确度表现。 |
| [^162] | [Maximum likelihood method revisited: Gauge symmetry in Kullback -- Leibler divergence and performance-guaranteed regularization.](http://arxiv.org/abs/2303.16721) | 本文提出了一种在最大似然方法中进行正则化的理论上保证的方法，通过关注 Kullback - Leibler 散度中的规范对称性，可以获得最优的模型。该方法不需要频繁搜索正则化的超参数。 |
| [^163] | [Conductivity Imaging from Internal Measurements with Mixed Least-Squares Deep Neural Networks.](http://arxiv.org/abs/2303.16454) | 本论文提出了一种新的通过深度神经网络从内部测量中重构椭圆问题中的导电率分布的方法，并在连续和经验损失的神经网络逼近上进行了深入的分析，展示了该方法对于数据噪声的优异稳定性以及解决高维问题的能力。 |
| [^164] | [Mask and Restore: Blind Backdoor Defense at Test Time with Masked Autoencoder.](http://arxiv.org/abs/2303.15564) | 本文提出了利用掩码自编码器的盲目防御框架（BDMAE），可以在测试时防御盲目后门攻击，不需要验证数据和模型参数，通过测试图像和 MAE 还原之间的结构相似性和标签一致性来检测后门攻击。 |
| [^165] | [Controllable Inversion of Black-Box Face-Recognition Models via Diffusion.](http://arxiv.org/abs/2303.13006) | ID3PM方法通过扩散过程的随机性质来产生黑盒人脸识别模型的高度可控的反演，能够生成逼真且多样的输出，适用于数据增强、对抗性攻击和生成个性化的训练数据等多种应用。 |
| [^166] | [Connected Superlevel Set in (Deep) Reinforcement Learning and its Application to Minimax Theorems.](http://arxiv.org/abs/2303.12981) | 本文研究了强化学习中的策略优化问题，并证明了优化函数超水平集在网络类策略和表格式下始终是连通的，并应用此结果导出了鲁棒性强化学习的极小极大定理。 |
| [^167] | [Multi-modal Variational Autoencoders for normative modelling across multiple imaging modalities.](http://arxiv.org/abs/2303.12706) | 本文提出了一种多模态规范建模框架，能够更好地检测出多种成像和生物变量中的异常性，特别适用于研究带有异质性的疾病。 |
| [^168] | [Policy Optimization for Personalized Interventions in Behavioral Health.](http://arxiv.org/abs/2303.12206) | 研究如何通过数字平台传递的行为健康介入最大化健康结果和治疗成本，提出了一个名为DecompPI的新算法，从离线数据进行预测任务，减轻了在线实验的需要，并在理论上证明了该算法的可扩展性和渐近收敛性。 |
| [^169] | [Pseudo Supervised Metrics: Evaluating Unsupervised Image to Image Translation Models In Unsupervised Cross-Domain Classification Frameworks.](http://arxiv.org/abs/2303.10310) | 本文提出了一种新方法——伪监督度量，用于评估无监督图片到图片翻译模型在无监督跨域分类框架中的性能，并在多个基准数据集上进行了实验。 |
| [^170] | [Disentangling the Link Between Image Statistics and Human Perception.](http://arxiv.org/abs/2303.09874) | 本研究直接评估自然图像的概率，并分析它如何影响人类感知。通过展示具有更丰富统计特征的自然图像被感知为具有更大的显着性，论文提供了直接支持Barlow和Attneave理论的证据，并建立了一个新的框架，用于理解图像统计与知觉之间的关系。 |
| [^171] | [Demographic Parity Inspector: Fairness Audits via the Explanation Space.](http://arxiv.org/abs/2303.08040) | 这篇论文提出了一种基于解释空间的算法方法，测量分组人口统计学平等的违规情况，并允许我们检查组间歧视的原因，提高了审计公平性的敏感度。 |
| [^172] | [Penalized Deep Partially Linear Cox Models with Application to CT Scans of Lung Cancer Patients.](http://arxiv.org/abs/2303.05341) | 通过引入罚函数，我们提出了一种创新的深度部分线性Cox模型，用于在肺癌患者的CT扫描中分析死亡风险。该模型能有效地整合已知和新兴的风险因素，解决了参数维度超出样本大小和非参数建模中维度灾难的问题。 |
| [^173] | [Co-learning Planning and Control Policies Constrained by Differentiable Logic Specifications.](http://arxiv.org/abs/2303.01346) | 本文提出了一种通过共学习规划和控制策略来解决带有复杂逻辑约束的高维度机器人导航任务的强化学习方法。相比现有算法，这种方法通过降低样本复杂性来训练出高质量的策略，并且能够高效地生成长期的机器人运动路径。实验证明了该方法的有效性。 |
| [^174] | [Efficient Explorative Key-term Selection Strategies for Conversational Contextual Bandits.](http://arxiv.org/abs/2303.00315) | 本研究提出了一种通用框架“ConLinUCB”来解决对话式情境马尔可夫决策过程中信息整合和探索性关键词选择的问题，以加速用户偏好估计的收敛速度。 |
| [^175] | [EvoPrompting: Language Models for Code-Level Neural Architecture Search.](http://arxiv.org/abs/2302.14838) | EvoPrompting利用语言模型作为自适应变异和交叉操作符来进行神经架构搜索，在MNIST-1D数据集和CLRS算法推理基准上都取得了比人类设计的架构更好的性能表现。 |
| [^176] | [HUST bearing: a practical dataset for ball bearing fault diagnosis.](http://arxiv.org/abs/2302.12533) | HUST轴承是一个实用的球轴承故障诊断数据集，其中包含90个带有6种故障类型（内部裂纹、外部裂纹、球体裂纹和它们的2种组合）的5种不同类型轴承的振动数据。研究者使用经典机器学习分类方法以及先进的非监督迁移学习算法对该数据集进行了评估，实验结果表明在分类任务上准确率可达到100%，在非监督迁移学习任务上准确率为60-80%。 |
| [^177] | [mSAM: Micro-Batch-Averaged Sharpness-Aware Minimization.](http://arxiv.org/abs/2302.09693) | mSAM是一种深度学习优化方法，通过在训练过程中聚合对抗性扰动得到的更新，从理论上证明了比传统方法更平的极小值点，实验证实了其在各种任务上的优越性能。 |
| [^178] | [Optimal Sample Complexity of Reinforcement Learning for Mixing Discounted Markov Decision Processes.](http://arxiv.org/abs/2302.07477) | 这篇论文研究了对于混合折扣马尔可夫决策过程的强化学习的最优样本复杂度理论。作者发现，在混合的情况下，最优样本复杂度依赖于总变异混合时间、折扣因子和解误差容忍度。 |
| [^179] | [AdaptSim: Task-Driven Simulation Adaptation for Sim-to-Real Transfer.](http://arxiv.org/abs/2302.04903) | 提出了AdaptSim，这是一个新的任务驱动的适应性框架，用于模拟到实际转换。它通过优化目标环境中的任务性能来解决仿真和现实之间的差距。 |
| [^180] | [Long Horizon Temperature Scaling.](http://arxiv.org/abs/2302.03686) | 提出了一种长时间尺度温度缩放（LHTS）方法，用于从温度缩放的联合分布中采样。LHTS可以优化样本的长时间尺度似然，并且在图像扩散模型和字符/语言自回归模型上展示了优势。 |
| [^181] | [The Weisfeiler-Lehman Distance: Reinterpretation and Connection with GNNs.](http://arxiv.org/abs/2302.00713) | 本文重新解释了Weisfeiler-Lehman距离，并将其与消息传递神经网络进行了联系，对于理解网络的Lipschitz性质和通用逼近结果具有重要意义。 |
| [^182] | [Limitless stability for Graph Convolutional Networks.](http://arxiv.org/abs/2301.11443) | 本研究在图卷积网络中提供了稳定性保证和可转移性界限，无需参考任何限制对象或统计分布，可处理无向图和有向图。节点级扰动的稳定性与过滤器的“充分（谱）覆盖”属性有关，边级扰动的稳定性与Lipschitz常数和新引入的过滤器半范数有关。通过数学物理工具获得了关于拓扑扰动的稳定性结果，并展示了在图粗粒化过程中图卷积网络的稳定性条件。 |
| [^183] | [Domain-Agnostic Molecular Generation with Self-feedback.](http://arxiv.org/abs/2301.11259) | MolGen是一个专注于分子生成的预训练语言模型，使用了领域无关的分子前缀调整和自我反馈的范式，实现了化学有效性、多样性、新颖性和复杂性的突破，在分子生成领域表现出了出色的性能。 |
| [^184] | [Train Hard, Fight Easy: Robust Meta Reinforcement Learning.](http://arxiv.org/abs/2301.11147) | 本文提出了一个强健的元强化学习算法，通过学习适应新任务的元策略来解决强化学习中环境和任务变化的挑战，通过识别和过采样更难的任务来提高系统的可靠性和效率。 |
| [^185] | [Quasi-optimal Reinforcement Learning with Continuous Actions.](http://arxiv.org/abs/2301.08940) | 本研究提出了一种准最优学习算法，用于解决强化学习中连续动作环境下的决策问题，特别适用于医疗应用中确定最佳剂量水平的问题。 |
| [^186] | [Short-length SSVEP data extension by a novel generative adversarial networks based framework.](http://arxiv.org/abs/2301.05599) | 本文提出了一种基于GAN的端到端信号转化网络TEGAN，可以将短SSVEP信号转换成长的人工SSVEP信号，并显著提高BCI系统的效率和准确性。 |
| [^187] | [Silent Killer: A Stealthy, Clean-Label, Black-Box Backdoor Attack.](http://arxiv.org/abs/2301.02615) | 默默杀手是一种隐蔽的、无标签的、黑盒子后门攻击，它使用了隐蔽的毒物和触发器，在无标签攻击中使用通用对抗扰动作为触发器，通过渐变对齐来提高成功率，并在MNIST、CIFAR10和ImageNet数据集上取得了最新的成果。 |
| [^188] | [Learning a Generic Value-Selection Heuristic Inside a Constraint Programming Solver.](http://arxiv.org/abs/2301.01913) | 本论文提出了一种通用学习过程，用于在约束规划求解器内获取一个值选择启发式方法，以解决当前通用值选择启发式方法较为稀缺的问题。 |
| [^189] | [Support Vector Regression: Risk Quadrangle Framework.](http://arxiv.org/abs/2212.09178) | 本文结合风险四方理论，研究了支持向量回归（SVR）。研究结果发现，SVR的两种形式对应于等效误差度量的最小化，同时加上正则化惩罚项。通过构造基本风险四方框，我们证明了SVR是对两个对称条件分位数的平均数的渐近无偏估计量。此外，我们证明了$\varepsilon$-SVR和$\nu$-SVR在一般随机环境下的等价性。 |
| [^190] | [Dataflow Analysis-Inspired Deep Learning for Efficient Vulnerability Detection.](http://arxiv.org/abs/2212.08108) | 本论文提出了一种基于数据流分析启发的深度学习方法，用于高效漏洞检测。通过设计了DeepDFA框架和嵌入技术，我们实现了对代码语义的更高效捕捉，使得深度学习在漏洞检测中更加有效和高性能。DeepDFA训练时间只需9分钟，且超过了所有非transformer基线模型75倍的性能。 |
| [^191] | [Physics-informed neural networks with unknown measurement noise.](http://arxiv.org/abs/2211.15498) | 这篇论文提出了一种解决物理信息神经网络在存在非高斯噪声情况下失效的问题的方法，即通过同时训练一个能量模型来学习正确的噪声分布。通过多个例子的实验证明了该方法的改进性能。 |
| [^192] | [ARISE: Graph Anomaly Detection on Attributed Networks via Substructure Awareness.](http://arxiv.org/abs/2211.15255) | ARISE是一个基于属性网络的图像异常检测框架，通过识别子结构来提高拓扑异常检测性能。 |
| [^193] | [Improving Multi-task Learning via Seeking Task-based Flat Regions.](http://arxiv.org/abs/2211.13723) | 通过寻找基于任务的平坦区域，可以改进多任务学习并提高模型性能，但需要正确使用正则化技术以避免次优解。 |
| [^194] | [Knowledge-Aware Federated Active Learning with Non-IID Data.](http://arxiv.org/abs/2211.13579) | 本文提出了一种知识感知的非IID数据联邦主动学习方法，通过知识专业化主动抽样和知识补偿联邦更新来解决联邦主动学习中的目标不匹配问题。 |
| [^195] | [Holistic Evaluation of Language Models.](http://arxiv.org/abs/2211.09110) | 我们提出了语言模型的整体评估（HELM），通过对潜在场景和度量进行分类并采用多度量方法，提高语言模型的透明度和可信度。 |
| [^196] | [Augmented Physics-Informed Neural Networks (APINNs): A gating network-based soft domain decomposition methodology.](http://arxiv.org/abs/2211.08939) | 本文提出了增强型物理知识编码神经网络(APINN)，采用软领域分解和参数共享，通过门控网络初始化和一般领域和函数分解来改进了扩展物理知识编码神经网络(XPINN)和基本物理知识编码神经网络(PINN)的泛化能力。 |
| [^197] | [CorruptEncoder: Data Poisoning based Backdoor Attacks to Contrastive Learning.](http://arxiv.org/abs/2211.08229) | 本文分析了现有数据污染后门攻击对对比学习的局限性，并提出了一种名为CorruptEncoder的新型攻击方法，通过理论导向的方式创建优化的污染输入，大幅提高攻击效果。实验证明，CorruptEncoder是首个仅需要少量图像和污染比例即可达到90%以上攻击成功率的攻击方法。同时，本文提出了一种名为局部裁剪的防御策略来应对数据污染后门攻击。 |
| [^198] | [Latent Multimodal Functional Graphical Model Estimation.](http://arxiv.org/abs/2210.17237) | 本研究提出了一个潜在多模态功能图模型估计的新框架，通过同时估计转换算子和潜在图来填补当前科学方法在估计多模态功能数据图模型方面的空白 |
| [^199] | [Auxo: Efficient Federated Learning via Scalable Client Clustering.](http://arxiv.org/abs/2210.16656) | Auxo 提出了一种解决联邦学习中异质性问题的方法，通过将具有相似数据分布的客户端进行分组，逐步识别大规模、低可用性和资源受限的FL人群中的这些分组，并自适应地确定如何训练特定分组的模型，以实现更好的模型性能和资源效率。 |
| [^200] | [Continuous-in-time Limit for Bayesian Bandits.](http://arxiv.org/abs/2210.07513) | 本文提出了一种适用于解决大时间长度下的贝叶斯赌博机问题的近似贝叶斯最优策略，并且其计算成本不包括依赖于时间长度的项。 |
| [^201] | [Bringing robotics taxonomies to continuous domains via GPLVM on hyperbolic manifolds.](http://arxiv.org/abs/2210.01672) | 本论文提出了一种通过在双曲流形上使用GPLVM来在连续领域中应用机器人分类法的方法，通过捕捉相关层次结构的双曲嵌入来建模分类数据，并采用图形先验和保持距离的后向约束来实现分类法结构的纳入。 |
| [^202] | [NAG-GS: Semi-Implicit, Accelerated and Robust Stochastic Optimizer.](http://arxiv.org/abs/2209.14937) | NAG-GS是一种半隐式、加速和稳健的随机优化器，通过使用加速的类Nesterov随机微分方程（SDE）和半隐式Gauss-Seidel类型离散化，该方法在二次函数最小化的情况下具有收敛性和稳定性，并在竞争性任务上表现出良好的性能。 |
| [^203] | [Predicting Swarm Equatorial Plasma Bubbles via Machine Learning and Shapley Values.](http://arxiv.org/abs/2209.13482) | 本研究提出了一种名为APE的机器学习模型，可以准确预测Swarm航天器上的等离子体泡指数。该模型在各项指标上表现良好，对赤道等离子体泡的预测具有重要意义。 |
| [^204] | [Risk of Bias in Chest Radiography Deep Learning Foundation Models.](http://arxiv.org/abs/2209.02965) | 该研究分析了一种最近发布的胸部X光基础模型中的偏倚风险，并发现在生物性别和种族之间存在亚组性能差距。 |
| [^205] | [Normalised clustering accuracy: An asymmetric external cluster validity measure.](http://arxiv.org/abs/2209.02935) | 本文提出了一种非对称的外部聚类有效度量方法，旨在区分不同任务类型上表现良好和系统性表现不佳的聚类算法。与传统的内部度量不同，该方法利用参考真实分组进行评估，并弥补了现有方法在最坏情况下的误差。 |
| [^206] | [Enhancing Heterogeneous Federated Learning with Knowledge Extraction and Multi-Model Fusion.](http://arxiv.org/abs/2208.07978) | 本文提出了一种资源感知的联邦学习方法，通过知识蒸馏将边缘模型的本地知识整合为鲁棒的全局知识，实现高效的多模型知识融合，并在保持模型异质性的同时降低通信成本和提高性能。 |
| [^207] | [Supervision Adaptation Balancing In-distribution Generalization and Out-of-distribution Detection.](http://arxiv.org/abs/2206.09380) | 该论文提出了一种监督适应性方法，通过生成自适应的监督信息来解决深度神经网络中内部分布泛化与外部分布检测之间的差异问题。 |
| [^208] | [Federated Learning with Uncertainty via Distilled Predictive Distributions.](http://arxiv.org/abs/2206.07562) | 本论文提出了一种联邦学习的不确定性框架，每个客户端在每轮中推断其参数的后验分布和后验预测分布，并将其蒸馏为单一的深度神经网络发送给服务器。这种方法可以解决现有联邦学习方法无法估计模型不确定性的问题，并在有限数据环境下取得更准确的预测。 |
| [^209] | [Superiority of GNN over NN in generalizing bandlimited functions.](http://arxiv.org/abs/2206.05904) | 本文研究了GNN在节点分类中插值带限函数的表达能力，结果表明，使用GNN结构以相同的精度插值带限函数所需的权重比使用完全连接的神经网络（NN）少得多。 |
| [^210] | [Distillation Decision Tree.](http://arxiv.org/abs/2206.04661) | 精馏决策树（DDT）是一种通过将黑盒模型中的知识精馏到决策树中来促进解释性的方法。该方法建立在知识精馏的理论基础上，并且在结构稳定性的条件下可以有效实现。 |
| [^211] | [Decoupled Self-supervised Learning for Non-Homophilous Graphs.](http://arxiv.org/abs/2206.03601) | 本文提出了一种用于非同态图的解耦自监督学习（DSSL）框架。通过模拟节点和链接的生成过程，将不同邻域之间的不同潜在语义解耦到自监督学习过程中。该框架对编码器不敏感，并且不需要预制的增强，对不同的图具有灵活性。 |
| [^212] | [Removing the fat from your posterior samples with margarine.](http://arxiv.org/abs/2205.12841) | 本文总结了一种使用掩蔽自回归流和核密度估计器的方法，可以学习对应于核心科学参数的边际后验密度。该方法在计算边际库尔巴克-勒布勒散度、边际贝叶斯模型维度、似然函数模拟和先验模拟等方面具有广泛应用。 |
| [^213] | [LDPC codes: tracking non-stationary channel noise using sequential variational Bayesian estimates.](http://arxiv.org/abs/2204.07037) | 该论文介绍了使用顺序贝叶斯学习方法跟踪非平稳信道噪声的LDPC码模型，并通过在5G行驶测试数据上的实验表明，该模型能够优于具有固定信道噪声知识的LDPC码。 |
| [^214] | [LDPC codes: comparing cluster graphs to factor graphs.](http://arxiv.org/abs/2204.06350) | 本研究比较了LDPC码的聚类图和因子图表示，结果显示聚类图表示优于传统的因子图表示。 |
| [^215] | [Output-sensitive ERM-based techniques for data-driven algorithm design.](http://arxiv.org/abs/2204.03569) | 本研究通过列举问题实例总损失函数的部分来提出了基于输出感知ERM的数据驱动算法设计技术，解决了多参数组合算法族的计算效率问题。 |
| [^216] | [An Optical Control Environment for Benchmarking Reinforcement Learning Algorithms.](http://arxiv.org/abs/2203.12114) | 本研究实现了一个光学模拟环境，用于基准测试强化学习算法。实验结果表明，相对于传统控制算法，离策略强化学习方法在复杂光学控制环境中表现更优。 |
| [^217] | [Evolving Curricula with Regret-Based Environment Design.](http://arxiv.org/abs/2203.01302) | 本文提出了一种基于遗憾的课程方法，将环境设计作为学生和教师的游戏进行，以产生学生智能体能力前沿的环境实例。相比于传统的进化方法，该方法具有广泛适用性和理论保证，并在具有挑战性的设计空间中取得了有效的关卡。 |
| [^218] | [Group-Agent Reinforcement Learning.](http://arxiv.org/abs/2202.05135) | 群体代理强化学习是一种新型的强化学习方法，其利用多个代理之间的合作来提高每个代理的学习效果。我们提出了群体代理强化学习系统的概念，并设计了一种分布式强化学习框架DDAL来支持群体代理强化学习。 |
| [^219] | [GLISp-r: A preference-based optimization algorithm with convergence guarantees.](http://arxiv.org/abs/2202.01125) | GLISp-r是一种基于偏好的优化算法，通过利用代理模型和资源勘探，迭代地提出新的样本与最佳校准进行比较。 |
| [^220] | [Multilevel Stochastic Optimization for Imputation in Massive Medical Data Records.](http://arxiv.org/abs/2110.09680) | 本文介绍了一种基于Kriging理论的多层次随机优化填补方法，能够更准确、更快速和更稳定地处理大规模医疗数据记录中的缺失数值数据。 |
| [^221] | [DEBOSH: Deep Bayesian Shape Optimization.](http://arxiv.org/abs/2109.13337) | 本论文提出了一种基于不确定性的方法，针对形状优化，在利用图神经网络预测工业设计性能时，解决了形状偏离训练集时预测不可靠的问题，并通过有效的贝叶斯优化提高了结果形状的质量。 |
| [^222] | [Sparse Plus Low Rank Matrix Decomposition: A Discrete Optimization Approach.](http://arxiv.org/abs/2109.12701) | 本文研究稀疏加低秩矩阵分解问题(SLR)，提出了一种新的离散模型和求解方法，适用于多种应用场景。 |
| [^223] | [FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Medical Imaging.](http://arxiv.org/abs/2109.09658) | 本论文介绍了一系列从经验、共识和最佳实践中提炼出的指导原则，旨在引领医学影像中值得信赖的人工智能的发展，提高信任、安全性和应用水平。 |
| [^224] | [Comfetch: Federated Learning of Large Networks on Constrained Clients via Sketching.](http://arxiv.org/abs/2109.08346) | Comfetch是一个通过使用草图的简化表示形式，允许资源受限的客户端进行大规模网络训练的算法。 |
| [^225] | [Dynamics of specialization in neural modules under resource constraints.](http://arxiv.org/abs/2106.02626) | 本研究使用人工神经网络模拟实验，发现结构模块化并不一定能够确保功能专业化，在特定环境和资源限制下，才能够出现专业化现象。 |
| [^226] | [Revisiting minimum description length complexity in overparameterized models.](http://arxiv.org/abs/2006.10189) | 本文重审了超参数模型中的最小描述长度复杂度。通过定义一个新的基于MDL的复杂度度量，我们发现复杂度不仅取决于参数数量，还与设计矩阵或核矩阵的奇异值和信噪比有关。 |
| [^227] | [Understanding the Difficulty of Training Transformers.](http://arxiv.org/abs/2004.08249) | 该论文研究了Transformer训练的困难。他们发现不平衡的梯度不是训练不稳定的根本原因，而是每一层的放大效应导致训练不稳定。他们观察到轻量级的依赖限制了模型潜力，导致表现较差的训练模型。 |
| [^228] | [A Convolutional Neural Network into graph space.](http://arxiv.org/abs/2002.09285) | 本文提出了一种将卷积神经网络转化为图空间的方法，实现了在非欧几里得定义的数据上进行卷积和池化操作，并且在实验中展示了其在简单任务上的优秀性能。 |
| [^229] | [Learning to Encode and Classify Test Executions.](http://arxiv.org/abs/2001.02444) | 本文旨在通过使用监督学习和神经网络模型，解决自动测试中的测试神谕问题。作者采用对执行轨迹进行标记和训练的方法来学习区分通过和失败执行的运行时模式，实现了通用、可扩展和准确的解决方案。 |
| [^230] | [Implicit Regularization and Momentum Algorithms in Nonlinearly Parameterized Adaptive Control and Prediction.](http://arxiv.org/abs/1912.13154) | 本论文通过利用经典自适应非线性控制技术和最近在优化和机器学习领域的进展之间的联系，展示了在自适应非线性控制和自适应动态预测的算法发展中存在相当大的潜力。通过引入受自然梯度下降和镜像下降启发的一阶自适应法则，证明了这些法则在存在多种与数据一致的动态时隐式正则化了学习的模型。 |
| [^231] | [Improving the Resolution of CNN Feature Maps Efficiently with Multisampling.](http://arxiv.org/abs/1805.10766) | 本论文提出了一种称为多样抽样的CNN子采样技术，通过子采样层显著增加特征图保留的信息量，实验证明粗糙的特征图是影响神经网络在图像分类中性能的瓶颈。 |

# 详细

[^1]: Steered Diffusion: 一种广义的插件式条件图像合成框架

    Steered Diffusion: A Generalized Framework for Plug-and-Play Conditional Image Synthesis. (arXiv:2310.00224v1 [cs.CV])

    [http://arxiv.org/abs/2310.00224](http://arxiv.org/abs/2310.00224)

    Steered Diffusion是一个通用的框架，利用近期基于扩散的生成模型的细粒度生成控制能力，实现了零样本条件图像生成的高质量合成。

    

    条件生成模型通常需要大量的注释训练集才能实现高质量的合成。因此，设计能够执行插件式合成的模型引起了很大的兴趣，即使用预定义或预训练的模型来指导生成过程（例如使用语言），而该模型并没有明确训练在生成任务上。然而，这种指导通常只对合成高级语义有用，而不是编辑图像到图像转换任务中的细粒度细节。为了解决这个问题，并借助最近基于扩散的生成模型提供的强大细粒度生成控制能力，我们引入了Steered Diffusion，这是一个通用的框架，用于使用为无条件生成而训练的扩散模型进行逼真的零样本条件图像生成。其核心思想是通过设计使用预训练的逆模型损失来在推理时指导扩散模型的图像生成。

    Conditional generative models typically demand large annotated training sets to achieve high-quality synthesis. As a result, there has been significant interest in designing models that perform plug-and-play generation, i.e., to use a predefined or pretrained model, which is not explicitly trained on the generative task, to guide the generative process (e.g., using language). However, such guidance is typically useful only towards synthesizing high-level semantics rather than editing fine-grained details as in image-to-image translation tasks. To this end, and capitalizing on the powerful fine-grained generative control offered by the recent diffusion-based generative models, we introduce Steered Diffusion, a generalized framework for photorealistic zero-shot conditional image generation using a diffusion model trained for unconditional generation. The key idea is to steer the image generation of the diffusion model at inference time via designing a loss using a pre-trained inverse mod
    
[^2]: 超越随机噪声：通过隐性强盗研究洞察匿名化策略

    Beyond Random Noise: Insights on Anonymization Strategies from a Latent Bandit Study. (arXiv:2310.00221v1 [cs.LG])

    [http://arxiv.org/abs/2310.00221](http://arxiv.org/abs/2310.00221)

    本文通过使用隐性强盗设置和不同的聚合策略，评估了隐私和推荐器性能之间的权衡，为定制隐私技术的需求提供了洞察。研究结果表明，对个体用户的数据记录添加拉普拉斯机制的噪声是不合适的选择，它在任何噪声水平下都会产生最大的遗憾。

    

    本文研究了在用户共享知识进行推荐任务的学习场景中，隐私问题。我们的研究为隐私保护机器学习领域的研究增加了贡献，并强调了需要针对特定攻击模式而非依赖一刀切解决方案的定制隐私技术。我们使用了隐性强盗设置来评估隐私和推荐器性能之间的权衡，通过采用各种聚合策略，如平均、最近邻和聚类结合噪声注入。更具体地说，我们模拟了一个利用对手收集的公开可获得的辅助信息进行链接攻击的情景。我们在三个开放的真实数据集上的结果表明，对个体用户的数据记录添加拉普拉斯机制的噪声是一个糟糕的选择。它相对于去匿名化概率和ADS度量来说，在任何噪声水平下都提供了最大的遗憾。

    This paper investigates the issue of privacy in a learning scenario where users share knowledge for a recommendation task. Our study contributes to the growing body of research on privacy-preserving machine learning and underscores the need for tailored privacy techniques that address specific attack patterns rather than relying on one-size-fits-all solutions. We use the latent bandit setting to evaluate the trade-off between privacy and recommender performance by employing various aggregation strategies, such as averaging, nearest neighbor, and clustering combined with noise injection. More specifically, we simulate a linkage attack scenario leveraging publicly available auxiliary information acquired by the adversary. Our results on three open real-world datasets reveal that adding noise using the Laplace mechanism to an individual user's data record is a poor choice. It provides the highest regret for any noise level, relative to de-anonymization probability and the ADS metric. Inst
    
[^3]: 两两邻近策略优化: 利用相对反馈进行LLM对齐

    Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment. (arXiv:2310.00212v1 [cs.LG])

    [http://arxiv.org/abs/2310.00212](http://arxiv.org/abs/2310.00212)

    该论文提出了一种新的强化学习框架，使用相对反馈来调整大型语言模型（LLMs）的行为，解决了现有方法在优化比较损失训练的奖励时存在的限制。同时，还提出了一种新的基于轨迹的策略梯度算法（PPPO），用于更有效地进行算法设计和函数逼近。

    

    大型语言模型（LLMs）通过在大型语料库上预先训练来获取广泛的世界知识。然而，由于接触到低质量数据，LLMs可能表现出与人类价值不一致的有害行为。引导LLMs朝着有益行为方向发展的主导方法涉及使用人类反馈的强化学习（RLHF），其中Proximal Policy Optimization（PPO）是默认的RL优化器。尽管其有效性，但PPO在优化基于比较损失训练的奖励时存在局限性。主要问题是，由于需要校准奖励尺度，PPO对于包含相同偏好信息的等价奖励函数不具备不变性。此外，与基于轨迹的优化相比，PPO对于基于令牌的更新的需求引入了函数逼近和算法设计方面的复杂性。本文提出了一种新的框架，基于相对反馈的强化学习，以及一种新颖的基于轨迹的策略梯度算法，Pairwise Proximal Policy Optimization（PPPO），用于解决上述问题。

    Large Language Models (LLMs) can acquire extensive world knowledge through pre-training on large corpora. However, due to exposure to low-quality data, LLMs may exhibit harmful behavior without aligning with human values. The dominant approach for steering LLMs towards beneficial behavior involves Reinforcement Learning with Human Feedback (RLHF), with Proximal Policy Optimization (PPO) serving as the default RL optimizer. Despite its effectiveness, PPO has limitations when optimizing rewards trained from comparison-based loss. Primarily, PPO is not invariant to equivalent reward functions containing identical preference information due to the need to calibrate the reward scale. Additionally, PPO's necessity for token-wise updates introduces complexity in both function approximation and algorithm design compared to trajectory-wise optimization. This paper proposes a new framework, reinforcement learning with relative feedback, and a novel trajectory-wise policy gradient algorithm, Pair
    
[^4]: 利用异构引导的客户端采样加速非独立同分布联邦学习

    Accelerating Non-IID Federated Learning via Heterogeneity-Guided Client Sampling. (arXiv:2310.00198v1 [cs.LG])

    [http://arxiv.org/abs/2310.00198](http://arxiv.org/abs/2310.00198)

    本文提出了一种名为HiCS-FL的方法，通过利用客户端的网络输出层更新来估计数据的统计异质性，并利用此信息进行客户端的聚类选择，以加速非独立同分布联邦学习。

    

    客户端设备中存在的数据的统计异质性使得在联邦学习（FL）系统中训练全局模型变得困难。尤其具有挑战性的是，在由于资源限制只有一小部分客户端能参与任何一轮FL的设置中。最近的一些方法致力于训练非独立同分布数据的FL系统中的全局模型，它们着重于开发采样更具信息更新的客户端选择方法。然而，现有的客户端选择技术要么引入了显著的计算开销，要么只在客户端具有类似异质性配置文件的情况下表现良好。本文提出了HiCS-FL（通过分层聚类采样进行联邦学习），一种新的客户端选择方法，其中服务器使用客户端网络输出层的更新来估计客户端数据的统计异质性，并依赖此信息来进行聚类。

    Statistical heterogeneity of data present at client devices in a federated learning (FL) system renders the training of a global model in such systems difficult. Particularly challenging are the settings where due to resource constraints only a small fraction of clients can participate in any given round of FL. Recent approaches to training a global model in FL systems with non-IID data have focused on developing client selection methods that aim to sample clients with more informative updates of the model. However, existing client selection techniques either introduce significant computation overhead or perform well only in the scenarios where clients have data with similar heterogeneity profiles. In this paper, we propose HiCS-FL (Federated Learning via Hierarchical Clustered Sampling), a novel client selection method in which the server estimates statistical heterogeneity of a client's data using the client's update of the network's output layer and relies on this information to clu
    
[^5]: 图卷积和Mixup之间的等价性研究

    On the Equivalence of Graph Convolution and Mixup. (arXiv:2310.00183v1 [cs.LG])

    [http://arxiv.org/abs/2310.00183](http://arxiv.org/abs/2310.00183)

    这项研究发现，在两个温和的条件下，图卷积可以被视为Mixup的一种特殊形式，它在训练和测试阶段都被应用。

    

    本文研究了图卷积和Mixup技术之间的关系。图卷积在图神经网络中是通过聚合邻居样本的特征来学习特定节点或样本的代表性特征。而Mixup是一种数据增强技术，通过对多个样本的特征和独热标签进行平均来生成新的示例。这两种技术之间的一个共同之处是它们利用了来自多个样本的信息来得出特征表示。本研究旨在探索这两种方法之间是否存在联系。我们的调查发现，在两个温和的条件下，图卷积可以被视为Mixup的一种特殊形式，它在训练和测试阶段都被应用。这两个条件是：1）\textit{同质改标} - 将目标节点的标签分配给其所有邻居，以及2）\textit{测试时Mixup} - 在测试时对特征进行Mixup。我们确定了这两个条件的数学表达，并通过实验验证了这个等价关系的有效性。

    This paper investigates the relationship between graph convolution and Mixup techniques. Graph convolution in a graph neural network involves aggregating features from neighboring samples to learn representative features for a specific node or sample. On the other hand, Mixup is a data augmentation technique that generates new examples by averaging features and one-hot labels from multiple samples. One commonality between these techniques is their utilization of information from multiple samples to derive feature representation. This study aims to explore whether a connection exists between these two approaches. Our investigation reveals that, under two mild conditions, graph convolution can be viewed as a specialized form of Mixup that is applied during both the training and testing phases. The two conditions are: 1) \textit{Homophily Relabel} - assigning the target node's label to all its neighbors, and 2) \textit{Test-Time Mixup} - Mixup the feature during the test time. We establis
    
[^6]: MARL：城市建筑能源建模的多尺度原型表示学习

    MARL: Multi-scale Archetype Representation Learning for Urban Building Energy Modeling. (arXiv:2310.00180v1 [cs.LG])

    [http://arxiv.org/abs/2310.00180](http://arxiv.org/abs/2310.00180)

    MARL是一种利用表示学习从建筑库中提取几何特征的方法，适用于多尺度区域的城市建筑能源建模，具有自动生成、灵活适应不同尺寸建筑轮廓以及保持几何特征的优点。

    

    建筑原型是建筑库的代表性模型，在城市建筑能源建模中非常关键。目前广泛采用的建筑原型是在全国范围内开发的，可能忽视了当地建筑物几何特性的影响。我们提出了一种称为多尺度原型表示学习（MARL）的方法，它利用表示学习从特定的建筑库中提取几何特征。MARL建立在VQ-AE上，将建筑物的轮廓编码，并将几何信息纯化为由多个建筑学上游任务约束的潜在向量。这些定制的表示对进一步的聚类和建筑能源建模非常有价值。我们算法的优点是它能够适应不同的建筑物轮廓尺寸，能够在多尺度区域自动生成，并能保持邻域和本地生态系统的几何特征。

    Building archetypes, representative models of building stock, are crucial for precise energy simulations in Urban Building Energy Modeling. The current widely adopted building archetypes are developed on a nationwide scale, potentially neglecting the impact of local buildings' geometric specificities. We present Multi-scale Archetype Representation Learning (MARL), an approach that leverages representation learning to extract geometric features from a specific building stock. Built upon VQ-AE, MARL encodes building footprints and purifies geometric information into latent vectors constrained by multiple architectural downstream tasks. These tailored representations are proven valuable for further clustering and building energy modeling. The advantages of our algorithm are its adaptability with respect to the different building footprint sizes, the ability for automatic generation across multi-scale regions, and the preservation of geometric features across neighborhoods and local ecolo
    
[^7]: 一个用于混合迪里切特和诺曼边界条件的神经预处理泊松求解器

    A Neural-preconditioned Poisson Solver for Mixed Dirichlet and Neumann Boundary Conditions. (arXiv:2310.00177v1 [math.NA])

    [http://arxiv.org/abs/2310.00177](http://arxiv.org/abs/2310.00177)

    我们引入了一个用于混合边界条件的泊松方程的神经预处理迭代求解器，核心是一个神经网络，能够近似逆离散结构网格拉普拉斯算子，并且在训练集之外的边界条件下仍然有效。

    

    我们引入了一个神经预处理的迭代求解器，用于具有混合边界条件的泊松方程。泊松方程在科学计算中是普遍存在的：它控制着广泛的物理现象，在许多数值算法中作为子问题出现，并且作为更广泛的椭圆PDE类的模型问题。最流行的泊松离散化方法可以产生大型稀疏线性系统。在高分辨率和对性能至关重要的应用中，迭代求解器结合强大的预处理器可以提供优势。我们求解器的核心是一个神经网络，该网络经过训练可以近似离散结构网格拉普拉斯算子的逆算子，适用于任意形状的域和混合边界条件。我们展示了该问题的结构激发了一种新颖的网络架构，即使在训练集之外的边界条件下，该架构也表现出高效的预处理器。我们展示了在具有挑战性的测试案例上的效果。

    We introduce a neural-preconditioned iterative solver for Poisson equations with mixed boundary conditions. The Poisson equation is ubiquitous in scientific computing: it governs a wide array of physical phenomena, arises as a subproblem in many numerical algorithms, and serves as a model problem for the broader class of elliptic PDEs. The most popular Poisson discretizations yield large sparse linear systems. At high resolution, and for performance-critical applications, iterative solvers can be advantageous for these -- but only when paired with powerful preconditioners. The core of our solver is a neural network trained to approximate the inverse of a discrete structured-grid Laplace operator for a domain of arbitrary shape and with mixed boundary conditions. The structure of this problem motivates a novel network architecture that we demonstrate is highly effective as a preconditioner even for boundary conditions outside the training set. We show that on challenging test cases aris
    
[^8]: 体积跨度器和应用的紧致界限

    Tight Bounds for Volumetric Spanners and Applications. (arXiv:2310.00175v1 [cs.DS])

    [http://arxiv.org/abs/2310.00175](http://arxiv.org/abs/2310.00175)

    本文研究了体积跨度器的紧致界限和应用，并给出了对于所有 $\ell_p$ 范数的几乎最优界限。此外，我们展示了这些结果在寻找最小体积包围椭球（MVEE）问题的核心集问题上的应用。

    

    给定一组感兴趣的点，体积跨度器是使用其中的一个子集可以用“小的”系数（在适当的范数中度量）表示所有点的方法。形式上，给定一组向量 $X = \{v_1, v_2, \dots, v_n\}$，目标是找到 $T \subseteq [n]$，使得每个 $v \in X$ 可以表示为 $\sum_{i\in T} \alpha_i v_i$，其中 $\|\alpha\|$ 较小。这个概念，也被称为良好条件的基，已经找到了几个应用，包括贝叶斯线性优化、行列式最大化和矩阵低秩逼近等。在本文中，我们给出了对于所有 $\ell_p$ 范数的体积跨度器大小的几乎最优界限，并展示了它们可以通过简单的局部搜索过程构建。然后，我们展示了我们结果在其他任务中的应用，特别是在寻找最小体积包围椭球（MVEE）问题的核心集问题上。

    Given a set of points of interest, a volumetric spanner is a subset of the points using which all the points can be expressed using "small" coefficients (measured in an appropriate norm). Formally, given a set of vectors $X = \{v_1, v_2, \dots, v_n\}$, the goal is to find $T \subseteq [n]$ such that every $v \in X$ can be expressed as $\sum_{i\in T} \alpha_i v_i$, with $\|\alpha\|$ being small. This notion, which has also been referred to as a well-conditioned basis, has found several applications, including bandit linear optimization, determinant maximization, and matrix low rank approximation. In this paper, we give almost optimal bounds on the size of volumetric spanners for all $\ell_p$ norms, and show that they can be constructed using a simple local search procedure. We then show the applications of our result to other tasks and in particular the problem of finding coresets for the Minimum Volume Enclosing Ellipsoid (MVEE) problem.
    
[^9]: 通过分子指纹组合进行ADMET性质预测

    ADMET property prediction through combinations of molecular fingerprints. (arXiv:2310.00174v1 [q-bio.BM])

    [http://arxiv.org/abs/2310.00174](http://arxiv.org/abs/2310.00174)

    通过使用扩展连接指纹（ECFP）、Avalon、ErG指纹和200个分子属性的组合，以及梯度提升决策树算法（尤其是CatBoost）和图神经网络指纹，我们成功地预测了ADMET属性。我们的研究结果强调了更丰富的分子表示对于准确的性质预测的重要性。

    

    在研究小分子效能预测方法时，我们发现随机森林或支持向量机与扩展连接指纹（ECFP）的组合一致优于最近开发的方法。对回归算法和分子指纹的详细调查表明，梯度提升决策树，特别是CatBoost，以及ECFP、Avalon和ErG指纹的组合，以及200个分子属性，最为有效。引入图神经网络指纹进一步提高了性能。我们成功地在22个治疗数据共享中心的ADMET基准中验证了我们的模型。我们的研究结果强调了更丰富的分子表示对于准确的性质预测的重要性。

    While investigating methods to predict small molecule potencies, we found random forests or support vector machines paired with extended-connectivity fingerprints (ECFP) consistently outperformed recently developed methods. A detailed investigation into regression algorithms and molecular fingerprints revealed gradient-boosted decision trees, particularly CatBoost, in conjunction with a combination of ECFP, Avalon, and ErG fingerprints, as well as 200 molecular properties, to be most effective. Incorporating a graph neural network fingerprint further enhanced performance. We successfully validated our model across 22 Therapeutics Data Commons ADMET benchmarks. Our findings underscore the significance of richer molecular representations for accurate property prediction.
    
[^10]: Motif: 来自人工智能反馈的内在动机

    Motif: Intrinsic Motivation from Artificial Intelligence Feedback. (arXiv:2310.00166v1 [cs.AI])

    [http://arxiv.org/abs/2310.00166](http://arxiv.org/abs/2310.00166)

    本文提出了一种名为Motif的方法，通过与大型语言模型（LLM）交互来获得先验知识，并将其用于代理程序的强化学习训练。实验证明，Motif的内在奖励相比直接最大化得分的算法在挑战性游戏中获得了更高的游戏得分，并在之前没有取得进展的任务上取得了显著的进展。

    

    在没有先验知识的情况下，探索丰富的环境并评估自己的行动是非常具有挑战性的。在本文中，我们提出了Motif，一种用大型语言模型（LLM）将先验知识与代理程序接口的通用方法。Motif的基本思想是将LLMs用于决策，而无需与环境进行交互：它通过从LLM中产生对配对标题的偏好来构建内在奖励，然后使用该奖励对代理程序进行强化学习训练。我们在具有挑战性、开放性和程序生成的NetHack游戏上评估了Motif的性能和行为。令人惊讶的是，仅通过学习最大化其内在奖励，Motif的游戏得分比直接训练以最大化得分的算法更高。当将Motif的内在奖励与环境奖励相结合时，我们的方法明显优于现有方法，并在以前从未取得进展的任务上取得进展。

    Exploring rich environments and evaluating one's actions without prior knowledge is immensely challenging. In this paper, we propose Motif, a general method to interface such prior knowledge from a Large Language Model (LLM) with an agent. Motif is based on the idea of grounding LLMs for decision-making without requiring them to interact with the environment: it elicits preferences from an LLM over pairs of captions to construct an intrinsic reward, which is then used to train agents with reinforcement learning. We evaluate Motif's performance and behavior on the challenging, open-ended and procedurally-generated NetHack game. Surprisingly, by only learning to maximize its intrinsic reward, Motif achieves a higher game score than an algorithm directly trained to maximize the score itself. When combining Motif's intrinsic reward with the environment reward, our method significantly outperforms existing approaches and makes progress on tasks where no advancements have ever been made with
    
[^11]: SCoRe：用于真实世界类别不平衡场景的子模聚合表示学习

    SCoRe: Submodular Combinatorial Representation Learning for Real-World Class-Imbalanced Settings. (arXiv:2310.00165v1 [cs.LG])

    [http://arxiv.org/abs/2310.00165](http://arxiv.org/abs/2310.00165)

    本文介绍了SCoRe框架，用于解决真实世界中类别不平衡场景下的表示学习问题。通过使用子模组合函数，我们能够同时建模特征簇的多样性和合作性。这对于克服类别不平衡在自主导航和医学诊断等任务中的挑战具有重要意义。

    

    在深度学习的演进过程中，真实世界类别不平衡场景中的表示学习已经成为一个具有挑战性的任务。对于罕见类别的视觉和结构特征缺乏多样性，限制了现代神经网络学习有区分度的特征簇。这体现为数据集中罕见对象类别之间的大型类间偏差以及丰富类别之间的高内类变化。虽然深度度量学习方法在这个领域显示出了潜力，但在自主导航和医学诊断等关键任务中，仍需要作出重大改进来克服类别不平衡带来的挑战。基于集合的组合函数，如子模信息度量，具有模拟特征簇多样性和合作性的特点。在本文中，我们引入了SCoRe（子模聚合表示学习）框架，并提出了一系列子模C家族。

    Representation Learning in real-world class-imbalanced settings has emerged as a challenging task in the evolution of deep learning. Lack of diversity in visual and structural features for rare classes restricts modern neural networks to learn discriminative feature clusters. This manifests in the form of large inter-class bias between rare object classes and elevated intra-class variance among abundant classes in the dataset. Although deep metric learning approaches have shown promise in this domain, significant improvements need to be made to overcome the challenges associated with class-imbalance in mission critical tasks like autonomous navigation and medical diagnostics. Set-based combinatorial functions like Submodular Information Measures exhibit properties that allow them to simultaneously model diversity and cooperation among feature clusters. In this paper, we introduce the SCoRe (Submodular Combinatorial Representation Learning) framework and propose a family of Submodular C
    
[^12]: 面向检测的图像-文本预训练方法用于开放词汇检测

    Detection-Oriented Image-Text Pretraining for Open-Vocabulary Detection. (arXiv:2310.00161v1 [cs.CV])

    [http://arxiv.org/abs/2310.00161](http://arxiv.org/abs/2310.00161)

    这项研究提出了一种面向检测的图像-文本预训练方法，旨在弥合图像级预训练和开放词汇目标检测之间的差距。通过检测器架构和对比损失，该方法能够从噪声图像-文本对中学习到新出现的物体-语义线索，并提出了一种平移窗口学习方法来改进主干网络的表示。在LVIS开放词汇检测基准上，该方法取得了显著优于其他方法的40.4的掩码AP$_r$结果。

    

    我们提出了一种基于面向检测的图像-文本预训练的新的开放词汇检测方法，以填补图像级预训练和开放词汇目标检测之间的差距。在预训练阶段，我们用检测器架构替代常用的分类架构，通过使检测器头部能够从噪声图像-文本对中学习，更好地满足检测的区域级识别需求。我们的方法只使用标准的对比损失而不使用伪标签，是对对比学习方法的简单而有效的扩展，可以学习到新出现的物体-语义线索。此外，我们提出了一种基于窗口注意力的平移窗口学习方法，使主干网络的表示更加鲁棒、平移不变，并且不受窗口模式的偏差影响。在流行的LVIS开放词汇检测基准上，我们的方法使用常见的ViT-L主干网络取得了40.4的掩码AP$_r$新的最优结果，明显优于其他方法。

    We present a new open-vocabulary detection approach based on detection-oriented image-text pretraining to bridge the gap between image-level pretraining and open-vocabulary object detection. At the pretraining phase, we replace the commonly used classification architecture with the detector architecture, which better serves the region-level recognition needs of detection by enabling the detector heads to learn from noisy image-text pairs. Using only standard contrastive loss and no pseudo-labeling, our approach is a simple yet effective extension of the contrastive learning method to learn emergent object-semantic cues. In addition, we propose a shifted-window learning approach upon window attention to make the backbone representation more robust, translation-invariant, and less biased by the window pattern. On the popular LVIS open-vocabulary detection benchmark, our approach sets a new state of the art of 40.4 mask AP$_r$ using the common ViT-L backbone, significantly outperforming t
    
[^13]: 不平衡分类中的反馈引导数据合成

    Feedback-guided Data Synthesis for Imbalanced Classification. (arXiv:2310.00158v1 [cs.CV])

    [http://arxiv.org/abs/2310.00158](http://arxiv.org/abs/2310.00158)

    本论文介绍了一种反馈引导数据合成的方法，通过从分类器到生成模型的反馈来驱动采样，将静态数据集增强为包含有用的合成样本，以提高分类器的性能。

    

    当前机器学习中的现状是使用来自长尾分布的真实图像的静态数据集进行训练。最近生成模型的进展使研究人员开始用合成数据增强这些静态数据集，并在分类任务上报告了适度的性能改进。我们假设这些性能提升受到从分类器到生成模型的反馈不足的限制，这将促进生成样本的有用性以提高分类器的性能。在这项工作中，我们介绍了一种用有用的合成样本增强静态数据集的框架，该框架利用从分类器到生成模型的一次性反馈来驱动采样。为了使该框架有效，我们发现样本必须接近手头任务的真实数据支持，并且具有足够的多样性。我们在一个长尾数据集（ImageNe...上验证了三个反馈标准。

    Current status quo in machine learning is to use static datasets of real images for training, which often come from long-tailed distributions. With the recent advances in generative models, researchers have started augmenting these static datasets with synthetic data, reporting moderate performance improvements on classification tasks. We hypothesize that these performance gains are limited by the lack of feedback from the classifier to the generative model, which would promote the usefulness of the generated samples to improve the classifier's performance. In this work, we introduce a framework for augmenting static datasets with useful synthetic samples, which leverages one-shot feedback from the classifier to drive the sampling of the generative model. In order for the framework to be effective, we find that the samples must be close to the support of the real data of the task at hand, and be sufficiently diverse. We validate three feedback criteria on a long-tailed dataset (ImageNe
    
[^14]: 原始-对偶持续学习：通过拉格朗日乘子实现稳定性和可塑性

    Primal-Dual Continual Learning: Stability and Plasticity through Lagrange Multipliers. (arXiv:2310.00154v1 [cs.LG])

    [http://arxiv.org/abs/2310.00154](http://arxiv.org/abs/2310.00154)

    本文提出了原始-对偶持续学习方法，通过利用拉格朗日对偶解决受限学习问题，实现了稳定性和可塑性。作者通过分析任务层面和样本层面的约束，在基于记忆的方法中分配资源，取得了较好的效果。

    

    持续学习固有地是一个受限学习问题。目标是在“无遗忘”要求下学习一个预测器。尽管之前有几项研究将其形式化为这样一个问题，但它们没有明确解决这个受限问题。在这项工作中，我们展示了直接解决这个受限优化问题是可行且有益的。为此，我们利用了最近在限制性学习中的拉格朗日对偶的结果。我们聚焦于基于记忆的方法，其中可以将先前任务中的一小部分样本存储在回放缓冲区中。在这个设置中，我们分析了持续学习问题的两个版本：一个在任务层面上有约束的粗糙方法和一个在样本层面上有约束的精细方法。我们展示了对偶变量指示了最优值对于约束扰动的敏感性。然后，我们利用这个结果在粗糙方法中对缓冲区进行了划分，将更多资源分配给更难的任务。

    Continual learning is inherently a constrained learning problem. The goal is to learn a predictor under a \emph{no-forgetting} requirement. Although several prior studies formulate it as such, they do not solve the constrained problem explicitly. In this work, we show that it is both possible and beneficial to undertake the constrained optimization problem directly. To do this, we leverage recent results in constrained learning through Lagrangian duality. We focus on memory-based methods, where a small subset of samples from previous tasks can be stored in a replay buffer. In this setting, we analyze two versions of the continual learning problem: a coarse approach with constraints at the task level and a fine approach with constraints at the sample level. We show that dual variables indicate the sensitivity of the optimal value with respect to constraint perturbations. We then leverage this result to partition the buffer in the coarse approach, allocating more resources to harder task
    
[^15]: 一刀切：向能够训练多个分类任务的单一图模型迈进

    One for All: Towards Training One Graph Model for All Classification Tasks. (arXiv:2310.00149v1 [cs.LG])

    [http://arxiv.org/abs/2310.00149](http://arxiv.org/abs/2310.00149)

    这项研究提出了一种名为“一刀切”的通用框架，该框架能够使用单一图模型解决不同领域的多个任务。该框架克服了图学习领域的挑战，包括不同属性和分布的图数据、不同类型的任务以及上下文学习的问题。

    

    设计一个能够解决多个任务的单一模型一直是人工智能领域的长期目标。最近，大型语言模型展示了在语言领域内整合和解决不同任务的异常能力。然而，在图学习领域，针对各种任务的统一模型仍然未被充分探索，主要是由于图学习领域独特的挑战。首先，来自不同领域的图数据具有不同的属性和遵循不同的分布。这种差异使得很难将图表示在一个统一的表示空间中。其次，图上的任务分化为节点、链接和图任务，需要不同的嵌入策略。最后，关于上下文学习的适当图提示范式尚不清楚。为了应对上述挑战，我们提出了"一刀切"（OFA），这是第一个能够使用单一图模型来解决上述挑战的通用框架。

    Designing a single model that addresses multiple tasks has been a long-standing objective in artificial intelligence. Recently, large language models have demonstrated exceptional capability in integrating and solving different tasks within the language domain. However, a unified model for various tasks on graphs remains underexplored, primarily due to the challenges unique to the graph learning domain. First, graph data from different areas carry distinct attributes and follow different distributions. Such discrepancy makes it hard to represent graphs in a single representation space. Second, tasks on graphs diversify into node, link, and graph tasks, requiring distinct embedding strategies. Finally, an appropriate graph prompting paradigm for in-context learning is unclear. Striving to handle all the aforementioned challenges, we propose One for All (OFA), the first general framework that can use a single graph model to address the above challenges. Specifically, OFA proposes text-at
    
[^16]: 基于概率采样增强的时空图卷积网络：一种可扩展的以太坊网络交易异常检测框架

    Probabilistic Sampling-Enhanced Temporal-Spatial GCN: A Scalable Framework for Transaction Anomaly Detection in Ethereum Networks. (arXiv:2310.00144v1 [cs.LG])

    [http://arxiv.org/abs/2310.00144](http://arxiv.org/abs/2310.00144)

    该研究提出了一种基于概率采样增强的时空图卷积网络框架，用于以太坊网络中的交易异常检测。通过将图卷积网络与时态随机游走相结合，利用时间序列的复杂性提供更精细的交易异常检测机制。实验结果表明，与传统的图卷积网络相比，该框架在检测异常和交易突发方面有显著的性能提升。这项研究强调了以太坊交易数据中时间线索的潜力，并展示了使用该框架进行交易异常检测的可行性。

    

    以太坊网络的快速演进需要先进的技术来确保其对潜在威胁的鲁棒性并保持透明度。虽然图神经网络（GNN）在此类平台的异常检测方面取得了先导性成果，但捕捉空间和时间事务模式的复杂性仍然是一个挑战。本研究提出了一种将图卷积网络（GCNs）与使用概率采样增强的时态随机游走（TRW）相结合的方法，以弥合这一差距。与传统的GCNs不同，我们的方法利用TRW的优势来识别以太坊交易中复杂的时间序列，从而提供更细致入微的交易异常检测机制。初步评估表明，我们的TRW-GCN框架在检测异常和交易突发的性能指标上显著提高了传统GCNs的表现。这项研究不仅强调了以太坊交易数据中时间线索的潜力，同时揭示了使用概率采样增强的时空GCNs进行交易异常检测的可行性。

    The rapid evolution of the Ethereum network necessitates sophisticated techniques to ensure its robustness against potential threats and to maintain transparency. While Graph Neural Networks (GNNs) have pioneered anomaly detection in such platforms, capturing the intricacies of both spatial and temporal transactional patterns has remained a challenge. This study presents a fusion of Graph Convolutional Networks (GCNs) with Temporal Random Walks (TRW) enhanced by probabilistic sampling to bridge this gap. Our approach, unlike traditional GCNs, leverages the strengths of TRW to discern complex temporal sequences in Ethereum transactions, thereby providing a more nuanced transaction anomaly detection mechanism. Preliminary evaluations demonstrate that our TRW-GCN framework substantially advances the performance metrics over conventional GCNs in detecting anomalies and transaction bursts. This research not only underscores the potential of temporal cues in Ethereum transactional data but a
    
[^17]: GASS：使用大规模数据进行音频源分离的泛化方法

    GASS: Generalizing Audio Source Separation with Large-scale Data. (arXiv:2310.00140v1 [cs.SD])

    [http://arxiv.org/abs/2310.00140](http://arxiv.org/abs/2310.00140)

    本文研究了一种使用大规模数据进行音频源分离的通用方法（GASS），在有限分布范围内表现出良好的效果，并展示了其在声音事件和语音分离方面的泛化能力。然而，在分离超出分布的电影和音乐内容方面仍存在挑战。

    

    通用源分离的目标是分离任意混合音频中的音频源，消除仅操作于特定领域（如语音或音乐）的约束。然而，通用源分离的潜力受限于大多数现有作品主要关注具有主要声音事件的混合以及小型训练数据集对于监督学习的潜力限制。本文研究了使用大规模数据集以有监督的方式训练的单一通用音频源分离（GASS）模型，该模型能够分离语音、音乐和声音事件。我们对GASS模型进行了多样化任务的评估。我们的强有力分布结果显示了GASS模型的可行性，而在声音事件和语音分离方面的竞争性超出分布性能则显示了其泛化能力。然而，GASS模型在分离超出分布的电影和音乐内容方面具有挑战性。我们还对每个数据集对GASS模型进行了微调，并始终表现优于其他模型。

    Universal source separation targets at separating the audio sources of an arbitrary mix, removing the constraint to operate on a specific domain like speech or music. Yet, the potential of universal source separation is limited because most existing works focus on mixes with predominantly sound events, and small training datasets also limit its potential for supervised learning. Here, we study a single general audio source separation (GASS) model trained to separate speech, music, and sound events in a supervised fashion with a large-scale dataset. We assess GASS models on a diverse set of tasks. Our strong in-distribution results show the feasibility of GASS models, and the competitive out-of-distribution performance in sound event and speech separation shows its generalization abilities. Yet, it is challenging for GASS models to generalize for separating out-of-distribution cinematic and music content. We also fine-tune GASS models on each dataset and consistently outperform the ones
    
[^18]: 关于过参数化神经网络理论与实践的脱节

    On the Disconnect Between Theory and Practice of Overparametrized Neural Networks. (arXiv:2310.00137v1 [cs.LG])

    [http://arxiv.org/abs/2310.00137](http://arxiv.org/abs/2310.00137)

    本文研究了神经网络在无穷宽度极限下的行为，并与核方法建立了联系。虽然在合成架构中展示了一些优势，如更快的优化和可靠的不确定性量化，但实际相关的架构需要比深度大很多倍的宽度才能实现这些优势。

    

    神经网络（NNs）的无穷宽度极限作为分析大规模、过参数化网络行为的理论框架已经引起了重要关注。通过接近无限宽度，NNs可以有效地收敛到一个具有由神经切线核(NTK)特征化的线性模型。这建立了NNs和核方法之间的联系，后者是被充分理解的。基于这种联系，已经假设并在合成架构中从理论上和算法上验证了一些优势。这些优势包括更快的优化、可靠的不确定性量化和改进的持续学习能力。然而，目前量化向核心领域收敛速度的结果表明，利用这些优势需要比深度大几个数量级的架构。这个假设引发了对实际相关架构是否表现如预测的担忧。

    The infinite-width limit of neural networks (NNs) has garnered significant attention as a theoretical framework for analyzing the behavior of large-scale, overparametrized networks. By approaching infinite width, NNs effectively converge to a linear model with features characterized by the neural tangent kernel (NTK). This establishes a connection between NNs and kernel methods, the latter of which are well understood. Based on this link, theoretical benefits and algorithmic improvements have been hypothesized and empirically demonstrated in synthetic architectures. These advantages include faster optimization, reliable uncertainty quantification and improved continual learning. However, current results quantifying the rate of convergence to the kernel regime suggest that exploiting these benefits requires architectures that are orders of magnitude wider than they are deep. This assumption raises concerns that practically relevant architectures do not exhibit behavior as predicted via 
    
[^19]: 高分辨率偏微分方程的多重网格张量傅里叶神经算子

    Multi-Grid Tensorized Fourier Neural Operator for High-Resolution PDEs. (arXiv:2310.00120v1 [cs.LG])

    [http://arxiv.org/abs/2310.00120](http://arxiv.org/abs/2310.00120)

    本论文介绍了一种称为多重网格张量傅里叶神经算子（MG-TFNO）的新型数据有效且高度并行化的算子学习方法，它通过局部和全局结构的分解来扩展至大尺度的分辨率。其创新包括多重网格的域分解、在傅里叶域中的高阶潜在子空间表示参数以及对架构的改进。

    

    到目前为止，内存复杂性和数据稀缺性阻碍了学习高分辨率偏微分方程（PDE）的解算子。我们通过引入一种新的数据有效且高度并行化的算子学习方法，降低了内存需求并改进了泛化性能，称为多重网格张量化神经算子（MG-TFNO）。MG-TFNO通过利用完整世界现象的局部和全局结构，通过输入域和算子参数空间的分解来扩展到大尺度的分辨率。我们的贡献有三个方面：i）我们通过一种新颖的基于多重网格的域分解实现了对输入样本的并行化，ii）我们通过傅里叶域中的高阶潜在子空间将模型参数表示，通过全局张量分解，大大减少参数数量并提高泛化性能，iii）我们提出了对b的架构改进。

    Memory complexity and data scarcity have so far prohibited learning solution operators of partial differential equations (PDEs) at high resolutions. We address these limitations by introducing a new data efficient and highly parallelizable operator learning approach with reduced memory requirement and better generalization, called multi-grid tensorized neural operator (MG-TFNO). MG-TFNO scales to large resolutions by leveraging local and global structures of full-scale, real-world phenomena, through a decomposition of both the input domain and the operator's parameter space. Our contributions are threefold: i) we enable parallelization over input samples with a novel multi-grid-based domain decomposition, ii) we represent the parameters of the model in a high-order latent subspace of the Fourier domain, through a global tensor factorization, resulting in an extreme reduction in the number of parameters and improved generalization, and iii) we propose architectural improvements to the b
    
[^20]: ABScribe: 使用大型语言模型在人工智能与人类共同写作任务中快速探索多种写作变化

    ABScribe: Rapid Exploration of Multiple Writing Variations in Human-AI Co-Writing Tasks using Large Language Models. (arXiv:2310.00117v1 [cs.HC])

    [http://arxiv.org/abs/2310.00117](http://arxiv.org/abs/2310.00117)

    ABScribe是一种界面，支持在人工智能与人类共同写作任务中快速探索多种写作变化。用户可以使用大型语言模型提示快速生成多个变体，这些变体以可重用的按钮形式呈现，并且可以通过上下文工具栏进行快速的就地比较。

    

    通过重新书写文本来探索替代想法是写作过程的关键。最先进的大型语言模型（LLM）可以简化写作变化生成的过程。然而，当前的界面存在同时考虑多种变化的挑战：在不覆盖文本的情况下创建新的版本可能很困难，而按顺序粘贴它们可能会使文档变得杂乱，增加工作量，并打断作者的流程。为了解决这个问题，我们提出了ABScribe，一种支持在人工智能与人类共同写作任务中快速且结构化地探索写作变化的界面。通过ABScribe，用户可以使用LLM提示快速产生多个变体，这些变体会自动转换成可重用的按钮形式。变体在文本段落中被存储在相邻位置，通过在上下文工具栏上的鼠标悬停交互进行快速的就地比较。我们对12名撰写人员进行的用户研究表明，ABScribe能显著减轻任务负荷（d = 1.20, p < 0.001），提高用户的认知程度。

    Exploring alternative ideas by rewriting text is integral to the writing process. State-of-the-art large language models (LLMs) can simplify writing variation generation. However, current interfaces pose challenges for simultaneous consideration of multiple variations: creating new versions without overwriting text can be difficult, and pasting them sequentially can clutter documents, increasing workload and disrupting writers' flow. To tackle this, we present ABScribe, an interface that supports rapid, yet visually structured, exploration of writing variations in human-AI co-writing tasks. With ABScribe, users can swiftly produce multiple variations using LLM prompts, which are auto-converted into reusable buttons. Variations are stored adjacently within text segments for rapid in-place comparisons using mouse-over interactions on a context toolbar. Our user study with 12 writers shows that ABScribe significantly reduces task workload (d = 1.20, p < 0.001), enhances user perceptions o
    
[^21]: 动态边界最大化和改进的Lipschitz正则化的认证鲁棒性

    Certified Robustness via Dynamic Margin Maximization and Improved Lipschitz Regularization. (arXiv:2310.00116v1 [cs.LG])

    [http://arxiv.org/abs/2310.00116](http://arxiv.org/abs/2310.00116)

    本文提出了一种基于动态边界最大化和改进的Lipschitz正则化的认证鲁棒性训练算法，通过增加输出空间中的边界和正则化模型的Lipschitz常数来提高深度分类器对抗性扰动的鲁棒性。

    

    为了提高深度分类器对抗性扰动的鲁棒性，已经提出了许多方法，例如设计具有更好鲁棒性性质的新架构（例如，Lipschitz-capped网络）或修改训练过程本身（例如，最小-最大优化，约束学习或正则化）。然而，这些方法对于增加输入（特征）空间中的边界可能并不有效。因此，越来越多的人开始对开发能够直接操纵输入空间中的决策边界的训练过程感兴趣。在本文中，我们在该类别的最新发展基础上，开发了一种鲁棒训练算法，其目标是在输出（logit）空间中增加边界，并沿着脆弱方向正则化模型的Lipschitz常数。我们证明这两个目标可以直接促进输入空间中更大的边界。为此，我们开发了一种可扩展的方法来计算...

    To improve the robustness of deep classifiers against adversarial perturbations, many approaches have been proposed, such as designing new architectures with better robustness properties (e.g., Lipschitz-capped networks), or modifying the training process itself (e.g., min-max optimization, constrained learning, or regularization). These approaches, however, might not be effective at increasing the margin in the input (feature) space. As a result, there has been an increasing interest in developing training procedures that can directly manipulate the decision boundary in the input space. In this paper, we build upon recent developments in this category by developing a robust training algorithm whose objective is to increase the margin in the output (logit) space while regularizing the Lipschitz constant of the model along vulnerable directions. We show that these two objectives can directly promote larger margins in the input space. To this end, we develop a scalable method for calcula
    
[^22]: 学习分子构象集合：数据集和基准测试

    Learning Over Molecular Conformer Ensembles: Datasets and Benchmarks. (arXiv:2310.00115v1 [cs.LG])

    [http://arxiv.org/abs/2310.00115](http://arxiv.org/abs/2310.00115)

    本研究引入了第一个MoleculAR Conformer Ensemble Learning（MARCEL）基准测试，以全面评估在构象集合上学习的潜力，并提出有前途的研究方向。

    

    分子表示学习在药物发现和酶设计等众多生物化学应用中已经证明具有重要影响力。然而，现有的工作往往忽略了分子的灵活性，分子通过化学键旋转和微小振动扰动不断在构象之间相互转化。为了更好地考虑分子的灵活性，最近的一些工作将分子表示学习定义为一个集合学习问题，专注于从一组构象结构中明确学习。然而，大多数研究在数据集、任务和模型方面存在局限性。在本研究中，我们引入了第一个MoleculAR Conformer Ensemble Learning（MARCEL）基准测试，以全面评估在构象集合上学习的潜力，并提出有前途的研究方向。MARCEL包括四个数据集，涵盖了多样的分子和反应水平信息。

    Molecular Representation Learning (MRL) has proven impactful in numerous biochemical applications such as drug discovery and enzyme design. While Graph Neural Networks (GNNs) are effective at learning molecular representations from a 2D molecular graph or a single 3D structure, existing works often overlook the flexible nature of molecules, which continuously interconvert across conformations via chemical bond rotations and minor vibrational perturbations. To better account for molecular flexibility, some recent works formulate MRL as an ensemble learning problem, focusing on explicitly learning from a set of conformer structures. However, most of these studies have limited datasets, tasks, and models. In this work, we introduce the first MoleculAR Conformer Ensemble Learning (MARCEL) benchmark to thoroughly evaluate the potential of learning on conformer ensembles and suggest promising research directions. MARCEL includes four datasets covering diverse molecule- and reaction-level pro
    
[^23]: HyperMask: 自适应的基于超网络的掩码用于持续学习

    HyperMask: Adaptive Hypernetwork-based Masks for Continual Learning. (arXiv:2310.00113v1 [cs.LG])

    [http://arxiv.org/abs/2310.00113](http://arxiv.org/abs/2310.00113)

    HyperMask是一种用于持续学习的方法，它使用基于超网络的掩码来训练一个单一网络，以克服人工神经网络在多任务上的灾难性遗忘问题。

    

    当人工神经网络在多个任务上顺序训练时，往往会出现灾难性遗忘的问题。为了克服这个问题，已经存在许多持续学习策略，其中最有效的之一是基于超网络的方法。超网络根据任务的特征生成目标模型的权重。然而，该模型的主要限制是超网络对于每个任务可以产生完全不同的网络结构，因此每个任务都是单独解决的。模型在学习后续任务时不使用之前任务所关联的网络信息，并实际上产生了新的网络架构。为了解决这个问题，我们使用了彩票票证假设，该假设认为存在稀疏的子网络（即中奖票），可以保持完整网络的性能。在本文中，我们提出了一种名为HyperMask的方法，该方法为所有任务训练一个单一网络。超网络产生半二进制掩码，以获取目标子网络。

    Artificial neural networks suffer from catastrophic forgetting when they are sequentially trained on multiple tasks. To overcome this problem, there exist many continual learning strategies. One of the most effective is the hypernetwork-based approach. The hypernetwork generates the weights of a target model based on the task's identity. The model's main limitation is that hypernetwork can produce completely different nests for each task. Consequently, each task is solved separately. The model does not use information from the network dedicated to previous tasks and practically produces new architectures when it learns the subsequent tasks. To solve such a problem, we use the lottery ticket hypothesis, which postulates the existence of sparse subnetworks, named winning tickets, that preserve the performance of a full network.  In the paper, we propose a method called HyperMask, which trains a single network for all tasks. Hypernetwork produces semi-binary masks to obtain target subnetw
    
[^24]: 基于强化学习的分支定界算法中的节点选择

    Reinforcement Learning for Node Selection in Branch-and-Bound. (arXiv:2310.00112v1 [cs.LG])

    [http://arxiv.org/abs/2310.00112](http://arxiv.org/abs/2310.00112)

    本论文提出了一种在分支定界算法中使用强化学习进行节点选择的新方法。我们通过训练图神经网络来模拟整个树的状态，并使用概率分布来选择节点。尽管只在合成TSP实例上进行了训练，我们的方法在各种复杂问题集上得到了高质量的节点选择策略。

    

    分支定界算法中的一个重要挑战是从搜索树中确定最优节点。当前最先进的选择器要么使用手工制作的集合，自动切换为天真的子节点选择器，要么使用依赖于个别节点数据的学习节点选择器。我们提出了一种新颖的双模拟技术，利用强化学习（RL）考虑整个树状态，而不仅仅是孤立的节点。为了实现这一点，我们训练了一个图神经网络，它根据模型从根节点到“待选择”叶子节点的路径产生一个概率分布。将节点选择建模为概率分布使我们能够使用最先进的强化学习技术来训练模型，捕捉内在节点质量和节点评估成本。尽管只是在专门设计的合成TSP实例上进行训练，我们的方法在一组多样且复杂的问题集上引出了高质量的节点选择策略。

    A big challenge in branch and bound lies in identifying the optimal node within the search tree from which to proceed. Current state-of-the-art selectors utilize either hand-crafted ensembles that automatically switch between naive sub-node selectors, or learned node selectors that rely on individual node data. We propose a novel bi-simulation technique that uses reinforcement learning (RL) while considering the entire tree state, rather than just isolated nodes. To achieve this, we train a graph neural network that produces a probability distribution based on the path from the model's root to its ``to-be-selected'' leaves. Modelling node-selection as a probability distribution allows us to train the model using state-of-the-art RL techniques that capture both intrinsic node-quality and node-evaluation costs. Our method induces a high quality node selection policy on a set of varied and complex problem sets, despite only being trained on specially designed, synthetic TSP instances. Exp
    
[^25]: Gradient and Uncertainty Enhanced Sequential Sampling for Global Fit. (arXiv:2310.00110v1 [stat.ML]) - 全局拟合中的梯度和不确定性增强的顺序采样

    Gradient and Uncertainty Enhanced Sequential Sampling for Global Fit. (arXiv:2310.00110v1 [stat.ML])

    [http://arxiv.org/abs/2310.00110](http://arxiv.org/abs/2310.00110)

    GUESS is a new sampling strategy for global fit that combines predictive posterior uncertainty and higher-order Taylor expansion values to reduce the number of samples needed for accurate surrogate modeling. - GUESS 是一种新的全局拟合采样策略，结合了预测后验不确定性和高阶泰勒展开值，可以减少准确的代理模型所需的样本数量。

    

    基于机器学习方法的代理模型已成为现代工程的重要组成部分，用以取代昂贵的计算机模拟。创建代理模型所使用的数据对于模型的准确性至关重要，但往往受到成本和时间限制的限制。自适应采样策略已被证明可以减少创建准确模型所需的样本数量。本文提出了一种新的全局拟合采样策略，称为Gradient and Uncertainty Enhanced Sequential Sampling (GUESS)。采用两个术语的收购功能：用于探索未见区域的代理模型的预测后验不确定性及用于开发的二阶及更高阶泰勒展开值的加权逼近值。尽管迄今为止已提出了各种采样策略，但选择合适的方法并不容易。因此，我们将我们提出的策略与基于26个不同统计学和机器学习方法的9种自适应采样策略进行了比较。

    Surrogate models based on machine learning methods have become an important part of modern engineering to replace costly computer simulations. The data used for creating a surrogate model are essential for the model accuracy and often restricted due to cost and time constraints. Adaptive sampling strategies have been shown to reduce the number of samples needed to create an accurate model. This paper proposes a new sampling strategy for global fit called Gradient and Uncertainty Enhanced Sequential Sampling (GUESS). The acquisition function uses two terms: the predictive posterior uncertainty of the surrogate model for exploration of unseen regions and a weighted approximation of the second and higher-order Taylor expansion values for exploitation. Although various sampling strategies have been proposed so far, the selection of a suitable method is not trivial. Therefore, we compared our proposed strategy to 9 adaptive sampling strategies for global surrogate modeling, based on 26 diff
    
[^26]: FedAIoT: 一种用于物联网人工智能的联邦学习基准

    FedAIoT: A Federated Learning Benchmark for Artificial Intelligence of Things. (arXiv:2310.00109v1 [cs.LG])

    [http://arxiv.org/abs/2310.00109](http://arxiv.org/abs/2310.00109)

    FedAIoT是一个用于AIoT的联邦学习基准，包括八个数据集和一个统一的端到端FL框架。它填补了现有FL研究中缺乏真实物联网设备数据集的关键差距，并揭示了FL在AIoT领域的机遇和挑战。

    

    在物联网人工智能领域，联邦学习（FL）具有重要的相关性。然而，大多数现有的FL研究并不是基于从真实物联网设备收集的数据集，这些数据集捕捉了物联网数据的独特模式和固有挑战。在这项工作中，我们引入了FedAIoT，一种用于AIoT的FL基准，以填补这个关键的差距。FedAIoT包括从各种物联网设备收集的八个数据集。这些数据集涵盖了物联网的独特模式，并针对AIoT的典型应用。FedAIoT还包括一种用于AIoT的统一的端到端FL框架，简化了数据集性能的基准测试。我们的基准测试结果揭示了FL在AIoT领域的机遇和挑战。我们希望FedAIoT能成为在FL for AIoT这一重要领域推动进展的珍贵资源。FedAIoT的代码仓库位于https://github.com/AIoT-MLSys-Lab/FedAIoT。

    There is a significant relevance of federated learning (FL) in the realm of Artificial Intelligence of Things (AIoT). However, most existing FL works are not conducted on datasets collected from authentic IoT devices that capture unique modalities and inherent challenges of IoT data. In this work, we introduce FedAIoT, an FL benchmark for AIoT to fill this critical gap. FedAIoT includes eight datatsets collected from a wide range of IoT devices. These datasets cover unique IoT modalities and target representative applications of AIoT. FedAIoT also includes a unified end-to-end FL framework for AIoT that simplifies benchmarking the performance of the datasets. Our benchmark results shed light on the opportunities and challenges of FL for AIoT. We hope FedAIoT could serve as an invaluable resource to foster advancements in the important field of FL for AIoT. The repository of FedAIoT is maintained at https://github.com/AIoT-MLSys-Lab/FedAIoT.
    
[^27]: 大规模多模态模型的实际会员推断攻击：一项初步研究

    Practical Membership Inference Attacks Against Large-Scale Multi-Modal Models: A Pilot Study. (arXiv:2310.00108v1 [cs.LG])

    [http://arxiv.org/abs/2310.00108](http://arxiv.org/abs/2310.00108)

    本文提出了一种针对大规模多模态模型的实际会员推断攻击方法，通过对目标点的文本和图像特征之间的余弦相似度进行阈值处理，并通过聚合相似度来进一步增强攻击效果。

    

    会员推断攻击（MIAs）旨在推断数据点是否用于训练机器学习模型。这些攻击可用于识别潜在的隐私漏洞和检测个人数据的未经授权使用。虽然传统上研究的是针对简单分类模型的MIAs，但多模态预训练（如CLIP）的最新发展已经展示了在各种计算机视觉任务上的非常好的零样本性能。然而，大规模数据和模型的规模给执行这些攻击带来了重大的计算挑战。本文首次尝试开发针对大规模多模态模型的实际MIAs。我们通过对目标点的文本和图像特征之间的余弦相似度进行阈值处理，引入了一个简单的基准策略，并提出通过对目标的变换进行余弦相似度聚合来进一步增强基准策略。我们还提出了一种新的弱监督攻击方法，对模型进行泛化攻击。

    Membership inference attacks (MIAs) aim to infer whether a data point has been used to train a machine learning model. These attacks can be employed to identify potential privacy vulnerabilities and detect unauthorized use of personal data. While MIAs have been traditionally studied for simple classification models, recent advancements in multi-modal pre-training, such as CLIP, have demonstrated remarkable zero-shot performance across a range of computer vision tasks. However, the sheer scale of data and models presents significant computational challenges for performing the attacks.  This paper takes a first step towards developing practical MIAs against large-scale multi-modal models. We introduce a simple baseline strategy by thresholding the cosine similarity between text and image features of a target point and propose further enhancing the baseline by aggregating cosine similarity across transformations of the target. We also present a new weakly supervised attack method that lev
    
[^28]: 潜空间的对称性发现

    Latent Space Symmetry Discovery. (arXiv:2310.00105v1 [cs.LG])

    [http://arxiv.org/abs/2310.00105](http://arxiv.org/abs/2310.00105)

    提出了一种新的生成模型——潜空间LieGAN（LaLiGAN），可以从数据中发现非线性对称性，并产生结构良好的潜空间，对其他下游任务非常有用。

    

    等变神经网络需要明确知道对称群。自动对称性发现方法旨在放宽这个约束，并从数据中学习不变性和等变性。然而，现有的对称性发现方法在搜索空间中仅限于线性对称性，无法处理实际世界中的对称性复杂性，尤其是高维数据。我们提出了一种新颖的生成模型，潜空间LieGAN（LaLiGAN），可以从数据中发现非线性对称性。它学习了一种从数据到潜空间的映射，在其中对称性变得线性，并同时发现潜空间中的对称性。理论上，我们证明了在一定条件下我们的方法可以表示任何非线性对称性。实验上，我们的方法可以捕捉高维观测中的内在对称性，从而产生一个结构良好的潜空间，对其他下游任务非常有用。我们演示了LaLiGAN在改进方程发现方面的应用案例。

    Equivariant neural networks require explicit knowledge of the symmetry group. Automatic symmetry discovery methods aim to relax this constraint and learn invariance and equivariance from data. However, existing symmetry discovery methods are limited to linear symmetries in their search space and cannot handle the complexity of symmetries in real-world, often high-dimensional data. We propose a novel generative model, Latent LieGAN (LaLiGAN), which can discover nonlinear symmetries from data. It learns a mapping from data to a latent space where the symmetries become linear and simultaneously discovers symmetries in the latent space. Theoretically, we show that our method can express any nonlinear symmetry under certain conditions. Experimentally, our method can capture the intrinsic symmetry in high-dimensional observations, which results in a well-structured latent space that is useful for other downstream tasks. We demonstrate the use cases for LaLiGAN in improving equation discovery
    
[^29]: 使用差分隐私的联邦学习进行端到端语音识别

    Federated Learning with Differential Privacy for End-to-End Speech Recognition. (arXiv:2310.00098v1 [cs.LG])

    [http://arxiv.org/abs/2310.00098](http://arxiv.org/abs/2310.00098)

    本文提出了一种基于联邦学习和差分隐私的端到端语音识别方法，探索了大型Transformer模型的不同方面，并建立了基线结果。

    

    联邦学习是一种有前景的训练机器学习模型的方法，但在自动语音识别领域仅限于初步探索。此外，联邦学习不能本质上保证用户隐私，并需要差分隐私来提供稳健的隐私保证。然而，我们还不清楚在自动语音识别中应用差分隐私的先前工作。本文旨在通过为联邦学习提供差分隐私的自动语音识别基准，并建立第一个基线来填补这一研究空白。我们扩展了现有的联邦学习自动语音识别研究，探索了最新的大型端到端Transformer模型的不同方面：架构设计，种子模型，数据异质性，领域转移，以及cohort大小的影响。通过合理的中央聚合数量，我们能够训练出即使在异构数据、来自另一个领域的种子模型或无预先训练的情况下仍然接近最优的联邦学习模型。

    While federated learning (FL) has recently emerged as a promising approach to train machine learning models, it is limited to only preliminary explorations in the domain of automatic speech recognition (ASR). Moreover, FL does not inherently guarantee user privacy and requires the use of differential privacy (DP) for robust privacy guarantees. However, we are not aware of prior work on applying DP to FL for ASR. In this paper, we aim to bridge this research gap by formulating an ASR benchmark for FL with DP and establishing the first baselines. First, we extend the existing research on FL for ASR by exploring different aspects of recent $\textit{large end-to-end transformer models}$: architecture design, seed models, data heterogeneity, domain shift, and impact of cohort size. With a $\textit{practical}$ number of central aggregations we are able to train $\textbf{FL models}$ that are \textbf{nearly optimal} even with heterogeneous data, a seed model from another domain, or no pre-trai
    
[^30]: 近似黑盒模型窃取的少次调用方法：活跃自适应知识蒸馏和基于扩散的图像生成

    Towards Few-Call Model Stealing via Active Self-Paced Knowledge Distillation and Diffusion-Based Image Generation. (arXiv:2310.00096v1 [cs.CV])

    [http://arxiv.org/abs/2310.00096](http://arxiv.org/abs/2310.00096)

    本研究提出了一种利用扩散模型生成合成数据集，并通过少次调用的方法窃取黑盒模型的框架，突破了访问限制条件。

    

    扩散模型在图像合成方面展示了强大的能力，并在许多计算机视觉任务中取得了巨大成功。为此，我们提出了一个新的用例，即在没有访问原始训练数据、架构和模型权重的情况下复制黑盒分类模型，即只能通过推理API使用模型。具体来说，我们只能观察到一些图像样本作为输入传递给模型时的（软性或硬性）标签。此外，我们考虑到限制模型调用次数的额外约束，主要关注于少次调用的模型窃取。为了在应用限制条件的情况下解决模型提取任务，我们提出了以下框架。作为训练数据，我们利用扩散模型生成逼真且多样化的图像创建了一个合成数据集（称为代理数据集）。给定允许的最大API调用次数，我们传递相应数量的样本进行训练。

    Diffusion models showcased strong capabilities in image synthesis, being used in many computer vision tasks with great success. To this end, we propose to explore a new use case, namely to copy black-box classification models without having access to the original training data, the architecture, and the weights of the model, \ie~the model is only exposed through an inference API. More specifically, we can only observe the (soft or hard) labels for some image samples passed as input to the model. Furthermore, we consider an additional constraint limiting the number of model calls, mostly focusing our research on few-call model stealing. In order to solve the model extraction task given the applied restrictions, we propose the following framework. As training data, we create a synthetic data set (called proxy data set) by leveraging the ability of diffusion models to generate realistic and diverse images. Given a maximum number of allowed API calls, we pass the respective number of sampl
    
[^31]: DataDAM: 基于注意力匹配的高效数据集精炼

    DataDAM: Efficient Dataset Distillation with Attention Matching. (arXiv:2310.00093v1 [cs.CV])

    [http://arxiv.org/abs/2310.00093](http://arxiv.org/abs/2310.00093)

    本研究提出了一种基于注意力匹配的高效数据集精炼(DataDAM)方法。通过匹配空间注意力来学习合成图像，从而实现了最新技术水平的性能，同时减少了训练成本。

    

    研究人员长期以来一直在尽量减少深度学习的训练成本，同时保持在多样化数据集上的强大泛化能力。最近的数据集精炼研究旨在通过创建一个包含更大真实数据集信息的小型合成数据集来减少训练成本，并最终实现与整个数据集训练的模型相当的测试准确性。然而，之前方法生成的合成数据并不能像原始训练数据那样分布和区分，而且会带来显著的计算成本。尽管取得了令人期待的结果，但精炼合成数据集上训练的模型与整个数据集上训练的模型之间仍然存在明显的性能差距。在本文中，我们通过使用基于注意力匹配的高效数据集精炼(DataDAM)来应对这些挑战，实现了最新技术水平的性能，同时减少了训练成本。具体而言，我们通过匹配空间注意力来学习合成图像。

    Researchers have long tried to minimize training costs in deep learning while maintaining strong generalization across diverse datasets. Emerging research on dataset distillation aims to reduce training costs by creating a small synthetic set that contains the information of a larger real dataset and ultimately achieves test accuracy equivalent to a model trained on the whole dataset. Unfortunately, the synthetic data generated by previous methods are not guaranteed to distribute and discriminate as well as the original training data, and they incur significant computational costs. Despite promising results, there still exists a significant performance gap between models trained on condensed synthetic sets and those trained on the whole dataset. In this paper, we address these challenges using efficient Dataset Distillation with Attention Matching (DataDAM), achieving state-of-the-art performance while reducing training costs. Specifically, we learn synthetic images by matching the spa
    
[^32]: 低成本黑盒优化算法在BBOB和OpenAI Gym上的评估

    Low-budget Black-box Optimization Algorithms Evaluated on BBOB and OpenAI Gym. (arXiv:2310.00077v1 [cs.LG])

    [http://arxiv.org/abs/2310.00077](http://arxiv.org/abs/2310.00077)

    这项研究旨在探讨机器学习和黑盒优化之间的交叉应用潜力，并通过比较实验评估了低成本黑盒优化算法在不同领域的效果。

    

    机器学习在计算机科学的各个领域中的广泛应用，包括黑盒优化（BBO）。近期的研究特别关注贝叶斯优化（BO）。基于BO的算法在机器学习社区中非常受欢迎，因为它们用于超参数优化和算法配置等。然而，随着问题维度和评估预算的增加，它们的效率会降低。与此同时，无导数优化方法在优化社区中独立发展。因此，我们迫切需要了解是否可以在机器学习和BBO之间进行交叉受精，即机器学习中广泛使用的算法在BBO中是否同样有效，反之亦然。比较实验通常涉及相对较小的基准和实验设置中的可见问题，如基线初始化不良、过度拟合等。

    The growing ubiquity of machine learning (ML) has led it to enter various areas of computer science, including black-box optimization (BBO). Recent research is particularly concerned with Bayesian optimization (BO). BO-based algorithms are popular in the ML community, as they are used for hyperparameter optimization and more generally for algorithm configuration. However, their efficiency decreases as the dimensionality of the problem and the budget of evaluations increase. Meanwhile, derivative-free optimization methods have evolved independently in the optimization community. Therefore, we urge to understand whether cross-fertilization is possible between the two communities, ML and BBO, i.e., whether algorithms that are heavily used in ML also work well in BBO and vice versa. Comparative experiments often involve rather small benchmarks and show visible problems in the experimental setup, such as poor initialization of baselines, overfitting due to problem-specific setting of hyperp
    
[^33]: EPiC-ly快速生成带有流匹配和扩散的粒子云

    EPiC-ly Fast Particle Cloud Generation with Flow-Matching and Diffusion. (arXiv:2310.00049v1 [hep-ph])

    [http://arxiv.org/abs/2310.00049](http://arxiv.org/abs/2310.00049)

    本文提出了两种新方法用于高效准确地生成LHC喷注为点云，一种基于评分匹配扩散模型的等变点云架构，另一种是使用最优传输的等效连续标准化流模型。实验证明这些方法均达到了最先进的性能。

    

    LHC中的喷注通常由大量高度相关的粒子组成，是深度生成建模的一个有趣实验室。在本文中，我们提出了两种新方法，以高效准确地生成LHC喷注为点云。我们引入了基于深度集合框架的等变点云（EPiC）架构的\epcjedi，该架构将评分匹配扩散模型与之结合。该模型提供了一个比之前基于转换器的扩散模型更快的选择，而不会降低生成喷注的质量。此外，我们引入了用于粒子云生成的首个等效连续标准化流（CNF）\epcfm。该模型使用基于最优传输的可扩展易训练的流匹配目标，直接回归连接高斯噪声先验和数据分布的矢量场。我们的实验证明，\epcjedi和\epcfm均达到了最先进的性能。

    Jets at the LHC, typically consisting of a large number of highly correlated particles, are a fascinating laboratory for deep generative modeling. In this paper, we present two novel methods that generate LHC jets as point clouds efficiently and accurately. We introduce \epcjedi, which combines score-matching diffusion models with the Equivariant Point Cloud (EPiC) architecture based on the deep sets framework. This model offers a much faster alternative to previous transformer-based diffusion models without reducing the quality of the generated jets. In addition, we introduce \epcfm, the first permutation equivariant continuous normalizing flow (CNF) for particle cloud generation. This model is trained with {\it flow-matching}, a scalable and easy-to-train objective based on optimal transport that directly regresses the vector fields connecting the Gaussian noise prior to the data distribution. Our experiments demonstrate that \epcjedi and \epcfm both achieve state-of-the-art performa
    
[^34]: 机器学习ADE Coxeter元素的Clifford不变量

    Machine Learning Clifford invariants of ADE Coxeter elements. (arXiv:2310.00041v1 [cs.LG])

    [http://arxiv.org/abs/2310.00041](http://arxiv.org/abs/2310.00041)

    本文研究了在线性变换中的Clifford几何不变量的机器学习方法，并实现了对ADE Coxeter元素的计算和数据挖掘。我们发现，不变量的输出完全取决于简单根的选择和对应反射的排列顺序，存在巨大的退化性。

    

    最近在线性变换的Clifford几何不变量上引起了广泛关注。这引发了对一类在根系、反射群、李群和李代数的背景中感兴趣的几何变换的不变量的研究: Coxeter变换。我们对$A_8$、$D_8$和$E_8$的所有Coxeter变换进行了详尽的计算，并使用高性能计算机计算了它们的不变量。这种计算代数的范式生成了一个数据集，可以利用数据科学的技术，如监督和无监督机器学习来挖掘。本文重点研究神经网络分类和主成分分析。由于输出 - 不变量 - 完全由简单根的选择和Coxeter元素中对应反射的排列顺序决定，我们期望在映射中存在巨大的退化性。

    There has been recent interest in novel Clifford geometric invariants of linear transformations. This motivates the investigation of such invariants for a certain type of geometric transformation of interest in the context of root systems, reflection groups, Lie groups and Lie algebras: the Coxeter transformations. We perform exhaustive calculations of all Coxeter transformations for $A_8$, $D_8$ and $E_8$ for a choice of basis of simple roots and compute their invariants, using high-performance computing. This computational algebra paradigm generates a dataset that can then be mined using techniques from data science such as supervised and unsupervised machine learning. In this paper we focus on neural network classification and principal component analysis. Since the output -- the invariants -- is fully determined by the choice of simple roots and the permutation order of the corresponding reflections in the Coxeter element, we expect huge degeneracy in the mapping. This provides the
    
[^35]: Cleanba: 一种可复现和高效的分布式强化学习平台

    Cleanba: A Reproducible and Efficient Distributed Reinforcement Learning Platform. (arXiv:2310.00036v1 [cs.LG])

    [http://arxiv.org/abs/2310.00036](http://arxiv.org/abs/2310.00036)

    Cleanba是一种可复现和高效的分布式强化学习平台，它通过引入高度可复现的架构和经过高度优化的分布式变种，解决了分布式深度强化学习中的复现性问题，并在实验中展现了更短的训练时间和更具复现性的学习曲线。

    

    分布式深度强化学习（DRL）旨在利用更多的计算资源，以在较短的训练时间内训练自主智能体。尽管该领域最近取得了进展，但复现性问题尚未得到充分探索。本文首先展示了即使控制了超参数，典型的actor-learner框架也可能存在复现性问题。然后，我们介绍了Cleanba，一个新的开源的分布式DRL平台，提出了一个高度可复现的架构。Cleanba实现了经过高度优化的PPO和IMPALA的分布式变种。我们在Atari实验中发现，这些变种在moolib和torchbeast的强IMPALA基线以及CleanRL的PPO基线中可以获得相等或更高的得分。然而，Cleanba的变体在不同硬件设置下呈现出1）更短的训练时间和2）更具复现性的学习曲线。Cleanba的源代码可在\url{https://github.com/vwxyzjn/cleanba}上获取。

    Distributed Deep Reinforcement Learning (DRL) aims to leverage more computational resources to train autonomous agents with less training time. Despite recent progress in the field, reproducibility issues have not been sufficiently explored. This paper first shows that the typical actor-learner framework can have reproducibility issues even if hyperparameters are controlled. We then introduce Cleanba, a new open-source platform for distributed DRL that proposes a highly reproducible architecture. Cleanba implements highly optimized distributed variants of PPO and IMPALA. Our Atari experiments show that these variants can obtain equivalent or higher scores than strong IMPALA baselines in moolib and torchbeast and PPO baseline in CleanRL. However, Cleanba variants present 1) shorter training time and 2) more reproducible learning curves in different hardware settings. Cleanba's source code is available at \url{https://github.com/vwxyzjn/cleanba}
    
[^36]: 大型语言模型微调中的LoRA集成

    LoRA ensembles for large language model fine-tuning. (arXiv:2310.00035v1 [cs.LG])

    [http://arxiv.org/abs/2310.00035](http://arxiv.org/abs/2310.00035)

    本文提出了一种使用低秩适配器（LoRA）的集成方法，用于解决大型语言模型微调中存在的不确定性量化问题，并提供了一个参数高效的微调技术。这种方法可以构建大规模的LoRA适配器集成，并具有与基础预训练模型相近的计算资源需求。

    

    细调的语言模型往往表现出较差的不确定性量化，表现为过于自信、校准不佳以及对测试数据或超出分布的样本的预测结果不可靠。为了缓解这个问题，本文提出了一种使用低秩适配器（LoRA）的集成方法，该方法是一种参数高效的微调技术。这些低秩适配器表示的参数数量非常小，比基础预训练模型小几个数量级。因此，可以构建大规模的LoRA适配器集成，几乎具有相同的计算资源需求。

    Finetuned LLMs often exhibit poor uncertainty quantification, manifesting as overconfidence, poor calibration, and unreliable prediction results on test data or out-of-distribution samples. One approach commonly used in vision for alleviating this issue is a deep ensemble, which constructs an ensemble by training the same model multiple times using different random initializations. However, there is a huge challenge to ensembling LLMs: the most effective LLMs are very, very large. Keeping a single LLM in memory is already challenging enough: keeping an ensemble of e.g. 5 LLMs in memory is impossible in many settings. To address these issues, we propose an ensemble approach using Low-Rank Adapters (LoRA), a parameter-efficient fine-tuning technique. Critically, these low-rank adapters represent a very small number of parameters, orders of magnitude less than the underlying pre-trained model. Thus, it is possible to construct large ensembles of LoRA adapters with almost the same computat
    
[^37]: PB-LLM: 部分二值化大型语言模型

    PB-LLM: Partially Binarized Large Language Models. (arXiv:2310.00034v1 [cs.LG])

    [http://arxiv.org/abs/2310.00034](http://arxiv.org/abs/2310.00034)

    本文提出的PB-LLM是一种部分二值化的大型语言模型压缩方法，可以在保持语言推理能力的同时实现极低比特量化，并通过后训练量化和量化感知训练等方法恢复量化LLMM的容量。

    

    本文探讨了网络二值化，一种压缩模型权重为单个比特的量化的激进形式，专门应用于大型语言模型（LLMs）的压缩。由于之前的二值化方法会导致LLMs崩溃，我们提出了一种新颖的方法，部分二值化LLM（PB-LLM），可以实现极低比特量化，并同时保持量化LLMs的语言推理能力。具体而言，我们的研究首先揭示了现有二值化算法的原生应用的无效性，并强调了显著权重在实现低位量化中的重要作用。因此，PB-LLM在二进制化过程中过滤了一小部分显著权重，将它们分配到高位存储中，即部分二值化。PB-LLM在后训练量化（PTQ）和量化感知训练（QAT）的角度分析后，扩展了恢复量化LLMM容量的能力。在PTQ下，结合了GPTQ的概念，我们重构了...

    This paper explores network binarization, a radical form of quantization, compressing model weights to a single bit, specifically for Large Language Models (LLMs) compression. Due to previous binarization methods collapsing LLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which can achieve extreme low-bit quantization while maintaining the linguistic reasoning capacity of quantized LLMs. Specifically, our exploration first uncovers the ineffectiveness of naive applications of existing binarization algorithms and highlights the imperative role of salient weights in achieving low-bit quantization. Thus, PB-LLM filters a small ratio of salient weights during binarization, allocating them to higher-bit storage, i.e., partially-binarization. PB-LLM is extended to recover the capacities of quantized LMMs, by analyzing from the perspective of post-training quantization (PTQ) and quantization-aware training (QAT). Under PTQ, combining the concepts from GPTQ, we reconstruct 
    
[^38]: 融入人类风险认知的对抗驾驶行为生成技术用于自动驾驶车辆评估

    Adversarial Driving Behavior Generation Incorporating Human Risk Cognition for Autonomous Vehicle Evaluation. (arXiv:2310.00029v1 [cs.AI])

    [http://arxiv.org/abs/2310.00029](http://arxiv.org/abs/2310.00029)

    本论文开发了一种新型框架，结合人类风险认知来生成对手驾驶行为，用于评估自动驾驶车辆的有效性和弱点。

    

    这篇论文关注于开发一种用于生成对手驾驶行为的新型框架，以暴露出自动驾驶车辆面对的有效和合理的风险事件。具体而言，采用强化学习与累积前景理论相结合的方法来学习对手行为，累积前景理论能够表示人类的风险认知。然后，提出了扩展版本的深度确定性策略梯度技术，用于训练对手策略，同时保证了训练的稳定性。在高保真的硬件在环（HiL）平台上进行了基于并线情景的对比案例研究，结果证明了对手的有效性，可以推断出被测试自动驾驶车辆的弱点。

    Autonomous vehicle (AV) evaluation has been the subject of increased interest in recent years both in industry and in academia. This paper focuses on the development of a novel framework for generating adversarial driving behavior of background vehicle interfering against the AV to expose effective and rational risky events. Specifically, the adversarial behavior is learned by a reinforcement learning (RL) approach incorporated with the cumulative prospect theory (CPT) which allows representation of human risk cognition. Then, the extended version of deep deterministic policy gradient (DDPG) technique is proposed for training the adversarial policy while ensuring training stability as the CPT action-value function is leveraged. A comparative case study regarding the cut-in scenario is conducted on a high fidelity Hardware-in-the-Loop (HiL) platform and the results demonstrate the adversarial effectiveness to infer the weakness of the tested AV.
    
[^39]: 无标记的域外数据改善了泛化能力

    Unlabeled Out-Of-Domain Data Improves Generalization. (arXiv:2310.00027v1 [stat.ML])

    [http://arxiv.org/abs/2310.00027](http://arxiv.org/abs/2310.00027)

    这个论文提出了一种新的框架，可以将无标记的域外数据纳入半监督分类问题，从而改善泛化能力。该框架结合了分布鲁棒优化与自监督训练，并利用了高效的多项式时间算法。在理论上，该框架在高斯混合分类问题中得到了验证。

    

    我们提出了一种将无标记数据纳入半监督分类问题的新框架，其中考虑了最小化鲁棒性损失函数或非鲁棒性损失函数的情景。值得注意的是，我们允许无标记样本在总变差意义上略微偏离域内分布。我们的框架的核心思想是将分布鲁棒优化（DRO）与自监督训练相结合。因此，我们还利用了训练阶段的高效多项式时间算法。从理论上讲，我们将我们的框架应用于在$\mathbb{R}^d$中的两个高斯混合分类问题，除了来自真实分布的$m$个独立标记样本之外，还给出了一组$n$个（通常$n\gg m$）域外和无标记样本。已知仅使用标记数据，泛化误差可以通过$\propto\left(d/m\right)$进行界定。

    We propose a novel framework for incorporating unlabeled data into semi-supervised classification problems, where scenarios involving the minimization of either i) adversarially robust or ii) non-robust loss functions have been considered. Notably, we allow the unlabeled samples to deviate slightly (in total variation sense) from the in-domain distribution. The core idea behind our framework is to combine Distributionally Robust Optimization (DRO) with self-supervised training. As a result, we also leverage efficient polynomial-time algorithms for the training stage. From a theoretical standpoint, we apply our framework on the classification problem of a mixture of two Gaussians in $\mathbb{R}^d$, where in addition to the $m$ independent and labeled samples from the true distribution, a set of $n$ (usually with $n\gg m$) out of domain and unlabeled samples are gievn as well. Using only the labeled data, it is known that the generalization error can be bounded by $\propto\left(d/m\right
    
[^40]: De-SaTE：用于锂离子电池健康预测的去噪自注意力变换编码器

    De-SaTE: Denoising Self-attention Transformer Encoders for Li-ion Battery Health Prognostics. (arXiv:2310.00023v1 [cs.LG])

    [http://arxiv.org/abs/2310.00023](http://arxiv.org/abs/2310.00023)

    本研究提出了De-SaTE方法，通过利用多个去噪模块以及自注意力变换编码器，准确预测锂离子电池的剩余寿命（RUL），为预防性维护和预测性分析提供关键指标估计。

    

    锂离子电池在各个行业中得到了广泛应用，从为便携式电子设备供电到推动电动汽车和支持能源存储系统。有效管理锂离子电池的一个核心挑战是准确预测其剩余寿命（RUL），这是预防性维护和预测性分析的关键指标。本研究提出了一种新的方法，利用多个去噪模块的能量，每个模块都经过训练来处理电池数据中常见的噪声类型。具体而言，我们使用去噪自动编码器和小波去噪器来生成编码/分解表示，然后将其通过专用自注意力变换编码器进行处理。在NASA和CALCE数据集上进行了大量实验后，我们能够表征多种噪声模式下的广泛健康指标估计。我们发现我们报告的误差

    Lithium Ion (Li-ion) batteries have gained widespread popularity across various industries, from powering portable electronic devices to propelling electric vehicles and supporting energy storage systems. A central challenge in managing Li-ion batteries effectively is accurately predicting their Remaining Useful Life (RUL), which is a critical measure for proactive maintenance and predictive analytics. This study presents a novel approach that harnesses the power of multiple denoising modules, each trained to address specific types of noise commonly encountered in battery data. Specifically we use a denoising auto-encoder and a wavelet denoiser to generate encoded/decomposed representations, which are subsequently processed through dedicated self-attention transformer encoders. After extensive experimentation on the NASA and CALCE datasets, we are able to characterize a broad spectrum of health indicator estimations under a set of diverse noise patterns. We find that our reported error
    
[^41]: 通过量子退火机器人研究COVID-19疫情对大学生抑郁症影响的因素

    Investigation of factors regarding the effects of COVID-19 pandemic on college students' depression by quantum annealer. (arXiv:2310.00018v1 [quant-ph])

    [http://arxiv.org/abs/2310.00018](http://arxiv.org/abs/2310.00018)

    通过量子退火技术，研究发现COVID-19疫情对大学生抑郁症的影响，并比较了基于量子退火和传统线性回归模型的结果。

    

    先前的研究报告了COVID-19疫情对心理健康的影响以及相关因素。由于大学生易受到疫情的影响，他们经常被选为目标人群。本研究收集了751名大学生的多变量数据集，基于各种心理健康因素之间的复杂关系。我们使用商业D-Wave量子计算机执行的基于量子退火的特征选择算法，以确定疫情前后相关因素的相对重要性变化。还应用了多变量线性回归（MLR）和XGBoost模型来验证基于量子退火的算法。基于实验结果，我们确认基于量子退火的算法具有与先前广泛使用的MLR模型在因素分析研究中相当的能力。

    Diverse cases regarding the impact, with its related factors, of the COVID-19 pandemic on mental health have been reported in previous studies. College student groups have been frequently selected as the target population in previous studies because they are easily affected by pandemics. In this study, multivariable datasets were collected from 751 college students based on the complex relationships between various mental health factors. We utilized quantum annealing (QA)-based feature selection algorithms that were executed by commercial D-Wave quantum computers to determine the changes in the relative importance of the associated factors before and after the pandemic. Multivariable linear regression (MLR) and XGBoost models were also applied to validate the QA-based algorithms. Based on the experimental results, we confirm that QA-based algorithms have comparable capabilities in factor analysis research to the MLR models that have been widely used in previous studies. Furthermore, th
    
[^42]: 人工共情分类：深度学习技术、数据集和评估标准的综述

    Artificial Empathy Classification: A Survey of Deep Learning Techniques, Datasets, and Evaluation Scales. (arXiv:2310.00010v1 [cs.RO])

    [http://arxiv.org/abs/2310.00010](http://arxiv.org/abs/2310.00010)

    这篇论文综述了人工共情的分类研究，介绍了深度学习技术、数据集和评估标准的最新进展，指出训练人工共情的标准流程包括情绪识别、分析和响应动作。其中深度学习技术在虚拟代理和机器人中的应用有较高影响力。

    

    近十年来，机器学习（ML）和辅助发展机器人学（ADR）领域的研究人员对人工共情（AE）作为可能的未来人机交互（HRI）范式产生了兴趣。人类从出生开始就学会共情，因此在机器人和智能机器中灌输这种感觉是具有挑战性的。然而，通过对大量数据和时间进行训练，在某种程度上可以使机器人模仿共情。人工共情的标准工作流程包括三个阶段：1）使用从视频或文本数据中提取的特征进行情绪识别（ER），2）分析感知的情绪或共情程度以选择最佳行动方案，3）执行响应动作。最近的研究显示，使用虚拟代理或机器人的AE常常涉及深度学习（DL）技术。

    From the last decade, researchers in the field of machine learning (ML) and assistive developmental robotics (ADR) have taken an interest in artificial empathy (AE) as a possible future paradigm for human-robot interaction (HRI). Humans learn empathy since birth, therefore, it is challenging to instill this sense in robots and intelligent machines. Nevertheless, by training over a vast amount of data and time, imitating empathy, to a certain extent, can be possible for robots. Training techniques for AE, along with findings from the field of empathetic AI research, are ever-evolving. The standard workflow for artificial empathy consists of three stages: 1) Emotion Recognition (ER) using the retrieved features from video or textual data, 2) analyzing the perceived emotion or degree of empathy to choose the best course of action, and 3) carrying out a response action. Recent studies that show AE being used with virtual agents or robots often include Deep Learning (DL) techniques. For ins
    
[^43]: L2CEval:评估大型语言模型的语言到代码生成能力

    L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models. (arXiv:2309.17446v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.17446](http://arxiv.org/abs/2309.17446)

    L2CEval是对大型语言模型的语言到代码生成能力进行系统评估的工作，分析了影响其性能的因素，并对置信度校准和人工评估进行了测量。

    

    最近，特别是那些在代码上预训练的大型语言模型（LLM）已经展示出了在几次训练甚至零次训练的情况下，从自然语言输入生成程序的强大能力。尽管有着有希望的结果，但对于这些模型的语言到代码生成能力缺乏全面的评估。现有的研究往往集中在特定任务、模型架构或学习范式上，导致对整体情况的理解零散。在这项工作中，我们提出了L2CEval，对LLM在语义解析、数学推理和Python编程的7个任务上的语言到代码生成能力进行系统评估，并分析可能影响其性能的因素，如模型大小、预训练数据、指令调整和不同的提示方法。除了评估模型性能，我们还对模型进行了置信度校准的测量和人工评估。

    Recently, large language models (LLMs), especially those that are pretrained on code, have demonstrated strong capabilities in generating programs from natural language inputs in a few-shot or even zero-shot manner. Despite promising results, there is a notable lack of a comprehensive evaluation of these models language-to-code generation capabilities. Existing studies often focus on specific tasks, model architectures, or learning paradigms, leading to a fragmented understanding of the overall landscape. In this work, we present L2CEval, a systematic evaluation of the language-to-code generation capabilities of LLMs on 7 tasks across the domain spectrum of semantic parsing, math reasoning and Python programming, analyzing the factors that potentially affect their performance, such as model size, pretraining data, instruction tuning, and different prompting methods. In addition to assessing model performance, we measure confidence calibration for the models and conduct human evaluation
    
[^44]: 数据过滤网络

    Data Filtering Networks. (arXiv:2309.17425v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2309.17425](http://arxiv.org/abs/2309.17425)

    本文研究了学习数据过滤网络用于筛选大型未策划数据集的问题，并构建了新的数据过滤网络，从而产生最先进的图像-文本数据集。

    

    大型训练集已成为机器学习的基石，并为语言建模和多模态学习的最新进展奠定了基础。虽然对于预训练的数据采集仍然是一种常见的范式，但数据策划往往仍然是临时的。一种常见的方法是首先从网络上收集大量数据，然后通过各种启发式方法将此候选池筛选到实际的训练集中。在这项工作中，我们研究了学习数据过滤网络（DFN）用于筛选大型未策划数据集的问题。我们的主要发现是，用于筛选的网络的质量与其在下游任务上的表现是不同的：例如，一个在ImageNet上表现良好的模型可能会产生比一个在ImageNet上准确率较低但在一小部分高质量数据上进行训练的模型更差的训练集。基于我们的洞察力，我们构建了新的数据过滤网络，从而产生了最先进的图像-文本数据集。具体而言，我们表现最佳的数据集DFN-5B使我们能够进行训练。

    Large training sets have become a cornerstone of machine learning and are the foundation for recent advances in language modeling and multimodal learning. While data curation for pre-training is often still ad-hoc, one common paradigm is to first collect a massive pool of data from the Web and then filter this candidate pool down to an actual training set via various heuristics. In this work, we study the problem of learning a data filtering network (DFN) for this second step of filtering a large uncurated dataset. Our key finding is that the quality of a network for filtering is distinct from its performance on downstream tasks: for instance, a model that performs well on ImageNet can yield worse training sets than a model with low ImageNet accuracy that is trained on a small amount of high-quality data. Based on our insights, we construct new data filtering networks that induce state-of-the-art image-text datasets. Specifically, our best performing dataset DFN-5B enables us to train 
    
[^45]: PlaceNav: 通过地点识别进行拓扑导航

    PlaceNav: Topological Navigation through Place Recognition. (arXiv:2309.17260v1 [cs.RO])

    [http://arxiv.org/abs/2309.17260](http://arxiv.org/abs/2309.17260)

    PlaceNav是一种通过地点识别进行拓扑导航的方法，将机器人无关部分分为导航特定和通用的计算机视觉组件，通过使用非机器人来源的大规模数据集增加训练数据的可用性，同时通过地点识别来提高导航性能。新模型的性能提高了76%。

    

    最近的研究结果表明，将拓扑导航分为机器人无关和机器人特定的组件可以提高导航性能，通过使用不同类型机器人收集的数据来训练机器人无关部分。然而，导航方法仍受到适合训练数据的稀缺性和计算缩放性差的限制。在本文中，我们提出了一个名为PlaceNav的方法，将机器人无关部分分为导航特定和通用的计算机视觉组件。我们利用视觉地点识别来选择拓扑导航流程中的子目标。这使得子目标选择更高效，并能够利用非机器人来源的大规模数据集增加训练数据的可用性。地点识别使得贝叶斯滤波成为可能，进一步通过增加子目标的时间一致性来提高导航性能。我们的实验结果验证了这一设计，并且新模型的性能提高了76%。

    Recent results suggest that splitting topological navigation into robot-independent and robot-specific components improves navigation performance by enabling the robot-independent part to be trained with data collected by different robot types. However, the navigation methods are still limited by the scarcity of suitable training data and suffer from poor computational scaling. In this work, we present~\methodname, subdividing the robot-independent part into navigation-specific and generic computer vision components. We utilize visual place recognition for the subgoal selection of the topological navigation pipeline. This makes subgoal selection more efficient and enables leveraging large-scale datasets from non-robotics sources, increasing training data availability. Bayes filtering, enabled by place recognition, further improves navigation performance by increasing the temporal consistency of subgoals. Our experimental results verify the design and the new model obtains a 76% higher 
    
[^46]: Collaborative Learning方法在前列腺分割的成本效益的基准测试

    Benchmarking Collaborative Learning Methods Cost-Effectiveness for Prostate Segmentation. (arXiv:2309.17097v1 [cs.LG])

    [http://arxiv.org/abs/2309.17097](http://arxiv.org/abs/2309.17097)

    本文通过比较联邦学习和基于共识的方法解决MRI前列腺分割的问题，在协作学习的情景中进行成本效益基准测试，首次使用基于共识的方法解决协作学习问题，具有显著的改进。

    

    医疗数据通常被分割成多个医院的中小型集合，并且由于隐私规定的限制，访问这些数据变得困难。这给使用这些数据来开发机器学习和深度学习模型带来了困难，而这些模型以数据为基础。克服这个限制的方法之一是使用协作学习（CL）方法，这允许医院在不需要显式共享本地数据的情况下共同解决一个任务。在本文中，我们通过比较两种不同的方法来解决协作场景下的MRI前列腺分割问题：联邦学习（FL）和基于共识的方法（CBM）。据我们所知，这是第一次使用包括标签融合技术在内的CBM解决协作学习问题。在这个设置中，CBM将来自本地训练模型的预测进行组合，以获得具有理想增强鲁棒性和预测方差的联邦强学习模型。

    Healthcare data is often split into medium/small-sized collections across multiple hospitals and access to it is encumbered by privacy regulations. This brings difficulties to use them for the development of machine learning and deep learning models, which are known to be data-hungry. One way to overcome this limitation is to use collaborative learning (CL) methods, which allow hospitals to work collaboratively to solve a task, without the need to explicitly share local data.  In this paper, we address a prostate segmentation problem from MRI in a collaborative scenario by comparing two different approaches: federated learning (FL) and consensus-based methods (CBM).  To the best of our knowledge, this is the first work in which CBM, such as label fusion techniques, are used to solve a problem of collaborative learning. In this setting, CBM combine predictions from locally trained models to obtain a federated strong learner with ideally improved robustness and predictive variance proper
    
[^47]: 关于Weisfeiler-Leman测试在图形模式参数中的能力

    On the Power of the Weisfeiler-Leman Test for Graph Motif Parameters. (arXiv:2309.17053v1 [cs.LG])

    [http://arxiv.org/abs/2309.17053](http://arxiv.org/abs/2309.17053)

    这项研究探讨了图神经网络（GNN）表达能力和$k$维Weisfeiler-Leman ($k$WL)测试之间的关系，研究发现了$k$WL测试可以有效区分具有不同出现次数的模式图$P$的图形，并研究了模式图计数问题的最小维度$k$。

    

    图神经网络（GNN）领域的重要研究揭示了图神经网络的表达能力与$k$维Weisfeiler-Leman（$k$WL）测试之间的直接对应关系，$k$WL测试是一种广为认可的用于验证图同构的方法。这个连接重新引发了人们对$k$WL测试能够有效区分的特定图属性的兴趣。这个领域的研究的中心是确定最小维度$k$，使得$k$WL可以区分具有不同出现次数的模式图$P$的图形。我们将这个最小$k$称为这个模式计数问题的WL维度。这个调查传统上探讨与图案相关的两个不同的计数问题：子图计数和诱导子图计数。有趣的是，尽管它们最初看起来是具有看似不同方法的独立挑战，但这两个问题都是一个更全面问题的相互关联部分。

    Seminal research in the field of graph neural networks (GNNs) has revealed a direct correspondence between the expressive capabilities of GNNs and the $k$-dimensional Weisfeiler-Leman ($k$WL) test, a widely-recognized method for verifying graph isomorphism. This connection has reignited interest in comprehending the specific graph properties effectively distinguishable by the $k$WL test. A central focus of research in this field revolves around determining the least dimensionality $k$, for which $k$WL can discern graphs with different number of occurrences of a pattern graph $P$. We refer to such a least $k$ as the WL-dimension of this pattern counting problem. This inquiry traditionally delves into two distinct counting problems related to patterns: subgraph counting and induced subgraph counting. Intriguingly, despite their initial appearance as separate challenges with seemingly divergent approaches, both of these problems are interconnected components of a more comprehensive proble
    
[^48]: LLM能否有效利用结构信息进行图学习：何时何地。

    Can LLMs Effectively Leverage Structural Information for Graph Learning: When and Why. (arXiv:2309.16595v1 [cs.LG])

    [http://arxiv.org/abs/2309.16595](http://arxiv.org/abs/2309.16595)

    本文研究了大型语言模型（LLM）在图数据中的应用，发现LLM可以从结构信息中受益，尤其是在文本节点特征缺乏的情况下，而LLM的性能与数据泄露没有显著相关。

    

    本文研究了大型语言模型（LLM）在结构化数据（特别是图数据）上的应用，这是LLM文献中尚未充分探索的重要数据形态。我们旨在了解在节点分类任务中，何时何地引入图数据中的结构信息可以提高LLM的预测性能。为了解决“何时”问题，我们研究了多种编码结构信息的提示方法，设置中文本节点特征丰富或稀缺。对于“为什么”问题，我们探讨了LLM性能的两个潜在因素：数据泄露和同质性。我们的研究结果表明：（i）LLM可以从结构信息中受益，尤其是在文本节点特征缺乏的情况下；（ii）没有实质性的证据表明LLM性能与数据泄露有显著相关；（iii）LLM在目标节点上的性能与正向相关。

    This paper studies Large Language Models (LLMs) for structured data--particularly graphs--a crucial data modality that remains underexplored in the LLM literature. We aim to understand when and why the incorporation of structural information inherent in graph data can improve the prediction performance of LLMs on node classification tasks. To address the ``when'' question, we examine a variety of prompting methods for encoding structural information, in settings where textual node features are either rich or scarce. For the ``why'' questions, we probe into two potential contributing factors to the LLM performance: data leakage and homophily. Our exploration of these questions reveals that (i) LLMs can benefit from structural information, especially when textual node features are scarce; (ii) there is no substantial evidence indicating that the performance of LLMs is significantly attributed to data leakage; and (iii) the performance of LLMs on a target node is strongly positively relat
    
[^49]: Stackelberg批量策略学习

    Stackelberg Batch Policy Learning. (arXiv:2309.16188v1 [stat.ML])

    [http://arxiv.org/abs/2309.16188](http://arxiv.org/abs/2309.16188)

    Stackelberg批量策略学习是一种新颖的基于随机梯度的学习算法，采用博弈论的观点，对策略学习进行建模，并考虑了优化景观中的分层决策结构。

    

    批量强化学习定义了从固定的数据批次中进行学习，缺乏详尽的探索。最坏情况下的最优算法使用经验数据来校准价值函数模型，并在学习模型下执行某种悲观评估，已经成为批量强化学习中一种有前景的范式。然而，对于这个流派的现代研究通常忽视了优化景观中隐藏的分层决策结构。在本文中，我们采用博弈论的观点，将策略学习图表建模为具有领导者-跟随者结构的两人零和博弈。我们提出了一种新颖的基于随机梯度的学习算法：StackelbergLearner，领导者根据其目标的全导数进行更新，而不是通常的个体梯度，而跟随者进行个体更新并确保过渡一致的悲观推理。推导出的学习动力

    Batch reinforcement learning (RL) defines the task of learning from a fixed batch of data lacking exhaustive exploration. Worst-case optimality algorithms, which calibrate a value-function model class from logged experience and perform some type of pessimistic evaluation under the learned model, have emerged as a promising paradigm for batch RL. However, contemporary works on this stream have commonly overlooked the hierarchical decision-making structure hidden in the optimization landscape. In this paper, we adopt a game-theoretical viewpoint and model the policy learning diagram as a two-player general-sum game with a leader-follower structure. We propose a novel stochastic gradient-based learning algorithm: StackelbergLearner, in which the leader player updates according to the total derivative of its objective instead of the usual individual gradient, and the follower player makes individual updates and ensures transition-consistent pessimistic reasoning. The derived learning dynam
    
[^50]: ICML 2023拓扑深度学习挑战：设计与结果

    ICML 2023 Topological Deep Learning Challenge : Design and Results. (arXiv:2309.15188v1 [cs.LG])

    [http://arxiv.org/abs/2309.15188](http://arxiv.org/abs/2309.15188)

    本文介绍了ICML 2023拓扑深度学习挑战，该挑战要求参与者在两个月内提供开源实现的拓扑神经网络，吸引了28个合格的提交。

    

    本文介绍了ICML 2023拓扑与几何机器学习研讨会中举办的拓扑深度学习计算挑战。该比赛要求参与者通过贡献于python包TopoNetX（数据处理）和TopoModelX（深度学习）的开源实现来提供文献中的拓扑神经网络。该挑战在两个月的时间内吸引了28个合格的提交。本文描述了挑战的设计并总结了其主要发现。

    This paper presents the computational challenge on topological deep learning that was hosted within the ICML 2023 Workshop on Topology and Geometry in Machine Learning. The competition asked participants to provide open-source implementations of topological neural networks from the literature by contributing to the python packages TopoNetX (data processing) and TopoModelX (deep learning). The challenge attracted twenty-eight qualifying submissions in its two-month duration. This paper describes the design of the challenge and summarizes its main findings.
    
[^51]: SGD在具有接近最优样本复杂度的双层神经网络中寻找并调整特征：以XOR问题为案例研究

    SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem. (arXiv:2309.15111v1 [cs.LG])

    [http://arxiv.org/abs/2309.15111](http://arxiv.org/abs/2309.15111)

    本研究通过在两层神经网络上使用小批量SGD算法，在具有二次真实函数分隔数据的情况下，通过训练数量级为$d \:\text{polylog}(d)$的样本，将网络训练到了人口误差为$o(1)$的程度。这是首次在标准神经网络上以及标准训练下，展示了在各向同性数据上高效学习XOR函数的样本复杂度为$\tilde{O}(d)$。

    

    本文研究了小批量随机梯度下降（SGD）在具有二次真实函数分隔数据的双层神经网络上的优化过程。我们证明，对于从$d$维布尔超立方体中由二次“XOR”函数$y = -x_ix_j$标记的数据，可以通过标准小批量SGD在逻辑损失上同时训练两层ReLU激活的双层神经网络，用$d \:\text{polylog}(d)$个样本将其训练到人口误差为$o(1)$的程度。据我们所知，这是首次给出了在标准神经网络上以及标准训练下，对于在各向同性数据上高效学习XOR函数的样本复杂度为$\tilde{O}(d)$。我们的主要技术是展示网络演化有两个阶段：一个”信号发现“阶段，在此网络规模较小且许多神经元独立演化以寻找特征，以及一个”信号密集“阶段，其中许多神经元相互作用以优化预测。

    In this work, we consider the optimization process of minibatch stochastic gradient descent (SGD) on a 2-layer neural network with data separated by a quadratic ground truth function. We prove that with data drawn from the $d$-dimensional Boolean hypercube labeled by the quadratic ``XOR'' function $y = -x_ix_j$, it is possible to train to a population error $o(1)$ with $d \:\text{polylog}(d)$ samples. Our result considers simultaneously training both layers of the two-layer-neural network with ReLU activations via standard minibatch SGD on the logistic loss. To our knowledge, this work is the first to give a sample complexity of $\tilde{O}(d)$ for efficiently learning the XOR function on isotropic data on a standard neural network with standard training. Our main technique is showing that the network evolves in two phases: a $\textit{signal-finding}$ phase where the network is small and many of the neurons evolve independently to find features, and a $\textit{signal-heavy}$ phase, wher
    
[^52]: 基于似然比的任务预测的类增量学习

    Class Incremental Learning via Likelihood Ratio Based Task Prediction. (arXiv:2309.15048v1 [cs.LG])

    [http://arxiv.org/abs/2309.15048](http://arxiv.org/abs/2309.15048)

    该论文提出了一种基于似然比的任务预测的类增量学习方法，利用离群检测器进行任务标识预测，解决了无任务标识符的测试样本的任务预测问题。

    

    类增量学习是一种具有挑战性的不断学习的设置，通过顺序学习一系列任务。每个任务由一组唯一的类组成。类增量学习的关键特点是，在测试时不提供每个测试样本的任务标识符（或任务ID）。为每个测试样本预测任务ID是一个具有挑战性的问题。一种新兴的理论上合理且有效的方法是根据任务增量学习的方法，在共享网络中为所有任务训练每个任务的任务特定模型，以处理遗忘。该方法中每个任务的模型是一个非常规分类器而不是传统分类器的离群检测器。离群检测器可以对任务内（分布内（IND））的类进行预测和识别离群数据。在推断期间，离群检测能力是每个测试样本的任务ID预测的关键。然而，本文认为使用传统的离群检测器进行任务ID预测是次优的。

    Class incremental learning (CIL) is a challenging setting of continual learning, which learns a series of tasks sequentially. Each task consists of a set of unique classes. The key feature of CIL is that no task identifier (or task-id) is provided at test time for each test sample. Predicting the task-id for each test sample is a challenging problem. An emerging theoretically justified and effective approach is to train a task-specific model for each task in a shared network for all tasks based on a task-incremental learning (TIL) method to deal with forgetting. The model for each task in this approach is an out-of-distribution (OOD) detector rather than a conventional classifier. The OOD detector can perform both within-task (in-distribution (IND)) class prediction and OOD detection. The OOD detection capability is the key for task-id prediction during inference for each test sample. However, this paper argues that using a traditional OOD detector for task-id prediction is sub-optimal
    
[^53]: 人工生成的演示是否对于上下文学习有必要？

    Are Human-generated Demonstrations Necessary for In-context Learning?. (arXiv:2309.14681v1 [cs.LG])

    [http://arxiv.org/abs/2309.14681](http://arxiv.org/abs/2309.14681)

    本文研究了上下文学习中人工生成的演示是否有必要，并提出了一种新的自反思提示策略（SEC），通过这种策略，大型语言模型（LLMs）可以自行生成演示和最终输出，避免了手动生成过程的复杂性。

    

    尽管大型语言模型（LLMs）具备良好的少样本能力，但在上下文学习（ICL）的标准范式中存在以下弊端：易受选定演示的影响，生成这些演示的复杂性。本文提出了对于ICL，人工生成的演示是否有必要的基本问题，并提出了自反思提示策略（SEC），这是一种不依赖人工演示的范例。SEC的关键点在于，不使用手工制作的示例作为ICL中的演示，而是要求LLMs首先自行创建演示，然后生成最终输出。SEC是一种灵活的框架，可适应原始ICL和“思维链”（CoT），并且更加便捷：因为可以节省示例和理由的手动生成过程。在算术推理、常识推理和多任务语言理解方面进行了大量实验。

    Despite the promising few-shot ability of large language models (LLMs), the standard paradigm of In-context Learning (ICL) suffers the disadvantages of susceptibility to selected demonstrations and the intricacy to generate these demonstrations. In this paper, we raise the fundamental question that whether human-generated demonstrations are necessary for ICL. To answer this question, we propose self-contemplation prompting strategy (SEC), a paradigm free from human-crafted demonstrations. The key point of SEC is that, instead of using hand-crafted examples as demonstrations in ICL, SEC asks LLMs to first create demonstrations on their own, based on which the final output is generated. SEC is a flexible framework and can be adapted to both the vanilla ICL and the chain-of-thought (CoT), but with greater ease: as the manual-generation process of both examples and rationale can be saved. Extensive experiments in arithmetic reasoning, commonsense reasoning, multi-task language understandin
    
[^54]: 语言模型的物理学：第3.2部分，知识操控

    Physics of Language Models: Part 3.2, Knowledge Manipulation. (arXiv:2309.14402v1 [cs.CL])

    [http://arxiv.org/abs/2309.14402](http://arxiv.org/abs/2309.14402)

    本文研究了语言模型在推理过程中操控知识的能力，发现预训练模型在知识检索方面表现出色，但在简单的分类、比较和逆向搜索任务中表现不佳。作者还提供了一个合成数据集进行实验，验证了这些内在的弱点：语言模型无法高效地操控知识。

    

    语言模型可以存储大量事实知识，但它们在使用这些知识进行逻辑推理方面的能力仍然存在问题。本文探讨了语言模型在推理过程中操控其存储知识的能力。我们重点研究了四种操控类型：检索（例如，“A的属性X是什么”）、分类（例如，“A的属性X是奇数还是偶数”）、比较（例如，“在属性X中A是否大于B”）和逆向搜索（例如，“哪个人的属性X等于T”）。我们观察到，像GPT2/3/4这样的预训练语言模型在知识检索方面表现出色，但在简单的分类或比较任务中很难胜任，除非在训练和推理过程中采用了Chain of Thoughts（CoTs）。无论提示是什么，它们在逆向知识搜索中表现都很差。我们的主要贡献是一个为控制实验而设计的合成数据集，证实了这些内在的弱点：语言模型无法高效地操控知识。

    Language models can store vast amounts of factual knowledge, but their ability to use this knowledge for logical reasoning remains questionable. This paper explores a language model's ability to manipulate its stored knowledge during inference. We focus on four manipulation types: retrieval (e.g., "What is person A's attribute X"), classification (e.g., "Is A's attribute X even or odd?"), comparison (e.g., "Is A greater than B in attribute X?") and inverse search (e.g., "Which person's attribute X equals T?")  We observe that pre-trained language models like GPT2/3/4 excel in knowledge retrieval but struggle with simple classification or comparison tasks unless Chain of Thoughts (CoTs) are employed during both training and inference. They also perform poorly in inverse knowledge search, irrespective of the prompts. Our primary contribution is a synthetic dataset for a controlled experiment that confirms these inherent weaknesses: a language model cannot efficiently manipulate knowledge
    
[^55]: LinGCN: 结构化的线性化图卷积网络用于同态加密推断

    LinGCN: Structural Linearized Graph Convolutional Network for Homomorphically Encrypted Inference. (arXiv:2309.14331v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.14331](http://arxiv.org/abs/2309.14331)

    LinGCN是一个旨在减少乘法深度和优化HE基于GCN推断性能的框架，通过结构化线性化算法和参数化的离散指示函数的联合训练，实现细粒度的节点级非线性位置选择。

    

    图卷积网络（GCN）模型的规模增长已经在个人医疗和金融系统等多个应用领域取得了超越人类表现的革命性进展。然而，在云端部署GCN引发了对客户数据可能受到对抗性攻击的隐私问题。为了解决安全问题，采用同态加密（HE）的隐私保护机器学习（PPML）可以确保敏感客户数据的安全。然而，在实际应用中，这引入了相当大的计算开销。为了解决这些挑战，我们提出了LinGCN，这是一个旨在减少乘法深度并优化HE基于GCN推断性能的框架。LinGCN围绕三个关键要素展开：（1）可微的结构化线性化算法，搭配参数化的离散指示函数，通过与模型权重一起进行联合训练以满足优化目标。这种策略促进了细粒度的节点级非线性位置选择，从而实现了

    The growth of Graph Convolution Network (GCN) model sizes has revolutionized numerous applications, surpassing human performance in areas such as personal healthcare and financial systems. The deployment of GCNs in the cloud raises privacy concerns due to potential adversarial attacks on client data. To address security concerns, Privacy-Preserving Machine Learning (PPML) using Homomorphic Encryption (HE) secures sensitive client data. However, it introduces substantial computational overhead in practical applications. To tackle those challenges, we present LinGCN, a framework designed to reduce multiplication depth and optimize the performance of HE based GCN inference. LinGCN is structured around three key elements: (1) A differentiable structural linearization algorithm, complemented by a parameterized discrete indicator function, co-trained with model weights to meet the optimization goal. This strategy promotes fine-grained node-level non-linear location selection, resulting in a 
    
[^56]: 库存管理中的缺货预测: 分类技术和成本考虑

    Backorder Prediction in Inventory Management: Classification Techniques and Cost Considerations. (arXiv:2309.13837v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.13837](http://arxiv.org/abs/2309.13837)

    本文介绍了一种先进的分析方法，用于预测库存管理中的缺货情况。该方法考虑了多种分类技术和成本考虑，通过提高服务水平来提高客户满意度和整体组织绩效。

    

    本文介绍了一种先进的分析方法，用于预测库存管理中的缺货情况。缺货是指由于库存耗尽而无法立即满足的订单。本文使用多种分类技术，包括平衡装袋分类器、模糊逻辑、变分自编码器-生成对抗网络和多层感知器分类器，使用ROC-AUC和PR-AUC等性能评估指标对其进行评估。此外，本文还考虑了利润函数和错分成本，考虑了与库存管理和缺货处理相关的财务影响和成本。结果表明，预测模型能够提高库存系统的服务水平，从而提高客户满意度和整体组织绩效。在商业应用中考虑可解释性是使用人工智能的一个重要方面，本研究还应用了排列重要性方法来选择模型。

    This article introduces an advanced analytical approach for predicting backorders in inventory management. Backorder refers to an order that cannot be immediately fulfilled due to stock depletion. Multiple classification techniques, including Balanced Bagging Classifiers, Fuzzy Logic, Variational Autoencoder - Generative Adversarial Networks, and Multi-layer Perceptron classifiers, are assessed in this work using performance evaluation metrics such as ROC-AUC and PR-AUC. Moreover, this work incorporates a profit function and misclassification costs, considering the financial implications and costs associated with inventory management and backorder handling. The results demonstrate the effectiveness of the predictive model in enhancing inventory system service levels, which leads to customer satisfaction and overall organizational performance. Considering interpretability is a significant aspect of using AI in commercial applications, permutation importance is applied to the selected mo
    
[^57]: 模型无关的图神经网络用于整合局部和全局信息的研究

    A Model-Agnostic Graph Neural Network for Integrating Local and Global Information. (arXiv:2309.13459v1 [stat.ML])

    [http://arxiv.org/abs/2309.13459](http://arxiv.org/abs/2309.13459)

    MaGNet是一种模型无关的图神经网络框架，能够顺序地整合不同顺序的信息，并通过识别有影响力的紧凑图结构提供有意义且可解释的结果。

    

    图神经网络（GNNs）在各种以图为重点的任务中取得了令人满意的性能。尽管取得了成功，但现有的GNN存在两个重要限制：由于黑盒特性，结果缺乏可解释性；无法学习不同顺序的表示。为了解决这些问题，我们提出了一种新的模型无关的图神经网络（MaGNet）框架，能够顺序地整合不同顺序的信息，从高阶邻居中提取知识，并通过识别有影响力的紧凑图结构提供有意义且可解释的结果。特别地，MaGNet由两个组件组成：图拓扑下复杂关系的潜在表示的估计模型和识别有影响力的节点、边和重要节点特征的解释模型。从理论上，我们通过经验Rademacher复杂度建立了MaGNet的泛化误差界，并展示了其强大的能力。

    Graph Neural Networks (GNNs) have achieved promising performance in a variety of graph-focused tasks. Despite their success, existing GNNs suffer from two significant limitations: a lack of interpretability in results due to their black-box nature, and an inability to learn representations of varying orders. To tackle these issues, we propose a novel Model-agnostic Graph Neural Network (MaGNet) framework, which is able to sequentially integrate information of various orders, extract knowledge from high-order neighbors, and provide meaningful and interpretable results by identifying influential compact graph structures. In particular, MaGNet consists of two components: an estimation model for the latent representation of complex relationships under graph topology, and an interpretation model that identifies influential nodes, edges, and important node features. Theoretically, we establish the generalization error bound for MaGNet via empirical Rademacher complexity, and showcase its pow
    
[^58]: 具有逐层非线性的状态空间模型是带有指数衰减记忆的全能逼近器

    State-space Models with Layer-wise Nonlinearity are Universal Approximators with Exponential Decaying Memory. (arXiv:2309.13414v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.13414](http://arxiv.org/abs/2309.13414)

    本论文证明了堆叠具有逐层非线性激活的状态空间模型足以逼近任何连续的序列到序列关系，并且发现其加强了模型学习复杂序列模式的能力。然而，状态空间模型并不能根本解决指数衰减记忆的问题。

    

    由于其简单有效的网络结构，状态空间模型在序列建模中变得越来越受欢迎。然而，沿时间方向缺乏非线性激活限制了模型的容量。本文证明了堆叠具有逐层非线性激活的状态空间模型足以逼近任何连续的序列到序列关系。我们的研究结果表明，逐层非线性激活的添加提高了模型学习复杂序列模式的能力。与此同时，可以从理论和实证上看到，状态空间模型并不根本解决指数衰减记忆的问题。理论结果经过了数值验证。

    State-space models have gained popularity in sequence modelling due to their simple and efficient network structures. However, the absence of nonlinear activation along the temporal direction limits the model's capacity. In this paper, we prove that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship. Our findings demonstrate that the addition of layer-wise nonlinear activation enhances the model's capacity to learn complex sequence patterns. Meanwhile, it can be seen both theoretically and empirically that the state-space models do not fundamentally resolve the exponential decaying memory issue. Theoretical results are justified by numerical verifications.
    
[^59]: 分布偏移感知的强化学习非同策略区间估计方法：一个统一的误差量化框架

    Distributional Shift-Aware Off-Policy Interval Estimation: A Unified Error Quantification Framework. (arXiv:2309.13278v1 [stat.ML])

    [http://arxiv.org/abs/2309.13278](http://arxiv.org/abs/2309.13278)

    本研究提出了一个对于强化学习非同策略评估问题的统一误差量化框架，并解决了分布偏移的挑战。通过在一个单一的区间内共同量化两个估计误差源，该框架揭示了之前隐藏的误差权衡，从而提高了置信区间的准确性。

    

    本文研究了在无限时间马尔可夫决策过程中的高置信度非同策略评估问题，其目标是仅利用从未知行为策略预先收集的离线数据为目标策略的值建立一个置信区间（CI）。该任务面临两个主要挑战：在CI估计中提供全面且严格的误差量化，并解决由目标策略产生的分布偏移问题，该分布与离线数据生成过程之间存在差异。受到创新的统一误差分析的启发，我们在一个单一的区间内共同量化两个估计误差来源：在建模边际化重要性权重时的规范不准确误差和抽样导致的统计不确定性。这一统一的框架揭示了误差之间以前隐藏的权衡，从而削弱了CI的紧密性。通过依靠精心设计的判别函数，提出了一种新的解决方案来克服分布偏移问题。

    We study high-confidence off-policy evaluation in the context of infinite-horizon Markov decision processes, where the objective is to establish a confidence interval (CI) for the target policy value using only offline data pre-collected from unknown behavior policies. This task faces two primary challenges: providing a comprehensive and rigorous error quantification in CI estimation, and addressing the distributional shift that results from discrepancies between the distribution induced by the target policy and the offline data-generating process. Motivated by an innovative unified error analysis, we jointly quantify the two sources of estimation errors: the misspecification error on modeling marginalized importance weights and the statistical uncertainty due to sampling, within a single interval. This unified framework reveals a previously hidden tradeoff between the errors, which undermines the tightness of the CI. Relying on a carefully designed discriminator function, the proposed
    
[^60]: 锐度感知最小化和稳定性边界。

    Sharpness-Aware Minimization and the Edge of Stability. (arXiv:2309.12488v1 [cs.LG])

    [http://arxiv.org/abs/2309.12488](http://arxiv.org/abs/2309.12488)

    本研究通过类似的计算方法，为锐度感知最小化(SAM)，一种改进泛化性能的梯度下降变种，确定了一个稳定性边界，该边界取决于梯度的范数。

    

    最近的实验表明，当使用梯度下降(GD)训练神经网络时，损失函数的Hessian矩阵的操作符范数会增长，直到接近$2/\eta$，之后会在该值周围波动。根据对损失函数的局部二次逼近，$2/\eta$被称为“稳定性边界”。我们使用类似的计算方法，为锐度感知最小化(SAM)确定了一个“稳定性边界”，SAM是一种改进泛化性能的GD变种。与GD不同，SAM的稳定性边界取决于梯度的范数。通过三个深度学习任务的实证，我们观察到SAM在这个分析中确定的稳定性边界上运行。

    Recent experiments have shown that, often, when training a neural network with gradient descent (GD) with a step size $\eta$, the operator norm of the Hessian of the loss grows until it approximately reaches $2/\eta$, after which it fluctuates around this value.  The quantity $2/\eta$ has been called the "edge of stability" based on consideration of a local quadratic approximation of the loss. We perform a similar calculation to arrive at an "edge of stability" for Sharpness-Aware Minimization (SAM), a variant of GD which has been shown to improve its generalization. Unlike the case for GD, the resulting SAM-edge depends on the norm of the gradient. Using three deep learning training tasks, we see empirically that SAM operates on the edge of stability identified by this analysis.
    
[^61]: 数学问题解决中的思路链设计

    Design of Chain-of-Thought in Math Problem Solving. (arXiv:2309.11054v1 [cs.CL])

    [http://arxiv.org/abs/2309.11054](http://arxiv.org/abs/2309.11054)

    本论文研究了数学问题解决中思路链的设计方法，对比了自然语言思路链和程序思路链的效果，并发现程序思路链通常在数学问题解决中更加有效，特别是自我描述程序具有更大多样性且性能更高。此外，研究还发现Python是程序思路链的较好选择。实验结果为未来思路链设计提供了宝贵指导。

    

    思路链在数学问题解决中扮演着至关重要的角色。我们对设计思路链的方法进行了全面的考察，比较了传统自然语言思路链和各种程序思路链，包括自我描述程序、注释描述程序和非描述程序。此外，我们还研究了编程语言对程序思路链的影响，比较了Python和Wolfram语言。通过对GSM8K、MATHQA和SVAMP进行广泛实验，我们发现程序思路链在数学问题解决中通常具有更好的效果。值得注意的是，具有30B参数的最佳组合明显超过了GPT-3.5-turbo。结果表明，自我描述程序提供了更大的多样性，因此通常可以实现更高的性能。我们还发现，Python是程序思路链的更好选择比Wolfram语言。实验结果为未来考虑因素提供了宝贵的指导。

    Chain-of-Thought (CoT) plays a crucial role in reasoning for math problem solving. We conduct a comprehensive examination of methods for designing CoT, comparing conventional natural language CoT with various program CoTs, including the self-describing program, the comment-describing program, and the non-describing program. Furthermore, we investigate the impact of programming language on program CoTs, comparing Python and Wolfram Language. Through extensive experiments on GSM8K, MATHQA, and SVAMP, we find that program CoTs often have superior effectiveness in math problem solving. Notably, the best performing combination with 30B parameters beats GPT-3.5-turbo by a significant margin. The results show that self-describing program offers greater diversity and thus can generally achieve higher performance. We also find that Python is a better choice of language than Wolfram for program CoTs. The experimental results provide a valuable guideline for future CoT designs that take into acco
    
[^62]: 移动边缘计算中通过深度强化学习进行任务图离载

    Task Graph offloading via Deep Reinforcement Learning in Mobile Edge Computing. (arXiv:2309.10569v1 [cs.DC])

    [http://arxiv.org/abs/2309.10569](http://arxiv.org/abs/2309.10569)

    本文研究了在移动边缘计算环境下通过深度强化学习实现任务图离载的问题。现有的工作往往无法适应环境变化，导致用户体验下降。我们提出了一种将任务图调度建模为马尔可夫决策过程的方法，以适应计算能力随时间变化的情况。

    

    随着移动应用程序越来越复杂，其中包含的依赖任务越来越流行，这些应用程序通常具有低延迟要求，从而导致对计算资源的需求急剧增加。随着移动边缘计算（MEC）的出现，将应用程序任务卸载到部署在移动网络边缘的小型设备上以获得高质量用户体验成为最重要的问题。然而，由于MEC环境是动态的，大多数依赖专家知识或准确的分析模型的现有工作在任务图离载方面无法完全适应这种环境变化，导致用户体验降低。本文研究了MEC中的任务图离载，考虑到边缘计算设备的计算能力随时间变化。为了适应环境变化，我们将计算离载的任务图调度建模为一个Markov决策过程（Markov Decision Process）。

    Various mobile applications that comprise dependent tasks are gaining widespread popularity and are increasingly complex. These applications often have low-latency requirements, resulting in a significant surge in demand for computing resources. With the emergence of mobile edge computing (MEC), it becomes the most significant issue to offload the application tasks onto small-scale devices deployed at the edge of the mobile network for obtaining a high-quality user experience. However, since the environment of MEC is dynamic, most existing works focusing on task graph offloading, which rely heavily on expert knowledge or accurate analytical models, fail to fully adapt to such environmental changes, resulting in the reduction of user experience. This paper investigates the task graph offloading in MEC, considering the time-varying computation capabilities of edge computing devices. To adapt to environmental changes, we model the task graph scheduling for computation offloading as a Mark
    
[^63]: 自主车辆间的多智能体深度强化学习在AutoDRIVE生态系统中的合作与竞争

    Multi-Agent Deep Reinforcement Learning for Cooperative and Competitive Autonomous Vehicles using AutoDRIVE Ecosystem. (arXiv:2309.10007v1 [cs.RO])

    [http://arxiv.org/abs/2309.10007](http://arxiv.org/abs/2309.10007)

    本研究提出了一个模块化和并行化的多智能体深度强化学习框架，在AutoDRIVE生态系统中培养合作与竞争行为。我们利用该生态系统开发了准确物理和逼真图形的数字孪生体，并使用它来训练和部署多智能体强化学习策略，实现了在自主车辆中的合作和竞争行为。

    

    本研究提出了一个模块化和可并行化的多智能体深度强化学习框架，用于在自主车辆中培养合作和竞争行为。我们引入了AutoDRIVE生态系统作为一个工具，开发出与真实的Nigel和F1TENTH两种比例自主车辆平台具有独特特性和能力的准确物理和逼真图形的数字孪生体，并利用这个生态系统来训练和部署多智能体强化学习策略。我们首先研究了一个交叉路口穿越问题，使用一组合作车辆（Nigel）在单个或多个智能体学习环境中共享有限状态信息，采用一种公共策略方法。然后我们研究了一个对抗性的头对头自主赛车问题，使用另一组车辆（F1TENTH）在多个智能体学习环境中采用个体策略方法。在任何一组实验中，都采用了分散学习架构。

    This work presents a modular and parallelizable multi-agent deep reinforcement learning framework for imbibing cooperative as well as competitive behaviors within autonomous vehicles. We introduce AutoDRIVE Ecosystem as an enabler to develop physically accurate and graphically realistic digital twins of Nigel and F1TENTH, two scaled autonomous vehicle platforms with unique qualities and capabilities, and leverage this ecosystem to train and deploy multi-agent reinforcement learning policies. We first investigate an intersection traversal problem using a set of cooperative vehicles (Nigel) that share limited state information with each other in single as well as multi-agent learning settings using a common policy approach. We then investigate an adversarial head-to-head autonomous racing problem using a different set of vehicles (F1TENTH) in a multi-agent learning setting using an individual policy approach. In either set of experiments, a decentralized learning architecture was adopted
    
[^64]: 机械化生成器2.0: 强化学习用于评估生成的游戏规则

    Mechanic Maker 2.0: Reinforcement Learning for Evaluating Generated Rules. (arXiv:2309.09476v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2309.09476](http://arxiv.org/abs/2309.09476)

    本论文研究了将强化学习应用于游戏规则生成的人类游戏评估，并通过实验结果表明，强化学习生成的规则与传统基线方法有所不同，可能更适合人类使用。

    

    自动游戏设计（AGD）是研究自动生成游戏规则的技术游戏研究的一个长期课题。 AGD方法通常依赖于对人类玩家游戏的近似，可以是客观函数或AI代理。尽管如此，大部分这些近似器是静态的，也就是说，它们不能反映人类玩家在游戏中的学习和提高能力。本文中，我们研究了将强化学习（RL）应用于生成规则的人类游戏评估中。我们在Unity中重新创建了经典的AGD环境Mechanic Maker作为一个全新的开源生成规则框架。我们的结果表明，RL与A*代理基线产生了不同的规则集，这些规则可能更适合人类使用。

    Automated game design (AGD), the study of automatically generating game rules, has a long history in technical games research. AGD approaches generally rely on approximations of human play, either objective functions or AI agents. Despite this, the majority of these approximators are static, meaning they do not reflect human player's ability to learn and improve in a game. In this paper, we investigate the application of Reinforcement Learning (RL) as an approximator for human play for rule generation. We recreate the classic AGD environment Mechanic Maker in Unity as a new, open-source rule generation framework. Our results demonstrate that RL produces distinct sets of rules from an A* agent baseline, which may be more usable by humans.
    
[^65]: 通过级别修复来重构现有级别

    Reconstructing Existing Levels through Level Inpainting. (arXiv:2309.09472v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.09472](http://arxiv.org/abs/2309.09472)

    本论文介绍了内容增强和级别修复的方法，通过两种技术（自编码器和U-net）来重建和扩展视频游戏级别。经过综合案例研究，我们展示了这两种方法相对于基准方法的卓越性能。提供了未来研究方向的见解。

    

    在先前的工作中，已经使用了过程化内容生成（PCG）和通过机器学习进行的过程化内容生成（PCGML）来生成各种游戏中的级别。本文介绍了内容增强，并将重点放在级别修复的子问题上，该问题涉及重建和扩展视频游戏级别。受图像修复的启发，我们从这一领域中调整了两种技术以解决我们的特定用例。我们提出了两种级别修复的方法：自编码器和U-net。通过一个全面的案例研究，我们展示了它们相对于基准方法的卓越性能，并讨论了它们的相对优势。此外，我们为级别修复任务提供了两种方法的实际演示，并提供了对未来研究方向的见解。

    Procedural Content Generation (PCG) and Procedural Content Generation via Machine Learning (PCGML) have been used in prior work for generating levels in various games. This paper introduces Content Augmentation and focuses on the subproblem of level inpainting, which involves reconstructing and extending video game levels. Drawing inspiration from image inpainting, we adapt two techniques from this domain to address our specific use case. We present two approaches for level inpainting: an Autoencoder and a U-net. Through a comprehensive case study, we demonstrate their superior performance compared to a baseline method and discuss their relative merits. Furthermore, we provide a practical demonstration of both approaches for the level inpainting task and offer insights into potential directions for future research.
    
[^66]: 非独立同分布数据条件下渐近高效的在线学习方法在被审查回归模型中的应用

    Asymptotically Efficient Online Learning for Censored Regression Models Under Non-I.I.D Data. (arXiv:2309.09454v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.09454](http://arxiv.org/abs/2309.09454)

    这项研究提出了一种渐近高效的在线学习方法，应用于随机被审查回归模型，并在一般情况下达到了最好的性能。

    

    本文研究了渐近高效的在线学习方法在随机被审查回归模型中的应用，该模型涉及到学习和统计学的各个领域，但迄今为止仍缺乏关于学习算法效率的全面理论研究。为此，我们提出了一种两步在线算法，第一步专注于实现算法收敛性，第二步用于改善估计性能。在数据的一般激励条件下，我们通过应用随机李雅普诺夫函数方法和对鞅的极限理论，证明了我们的算法是强一致的和渐近正态的。此外，我们还证明了估计值的协方差在渐近上可以达到克拉美洛界，这意味着所提出的算法的性能在一般情况下是可以期望的最好的。与大多数现有的研究不同，我们的结果是不依赖传统方法而得出的。

    The asymptotically efficient online learning problem is investigated for stochastic censored regression models, which arise from various fields of learning and statistics but up to now still lacks comprehensive theoretical studies on the efficiency of the learning algorithms. For this, we propose a two-step online algorithm, where the first step focuses on achieving algorithm convergence, and the second step is dedicated to improving the estimation performance. Under a general excitation condition on the data, we show that our algorithm is strongly consistent and asymptotically normal by employing the stochastic Lyapunov function method and limit theories for martingales. Moreover, we show that the covariances of the estimates can achieve the Cramer-Rao (C-R) bound asymptotically, indicating that the performance of the proposed algorithm is the best possible that one can expect in general. Unlike most of the existing works, our results are obtained without resorting to the traditionall
    
[^67]: 自适应优先级重新加权以提高公平性泛化能力

    Adaptive Priority Reweighing for Generalizing Fairness Improvement. (arXiv:2309.08375v1 [cs.LG])

    [http://arxiv.org/abs/2309.08375](http://arxiv.org/abs/2309.08375)

    本文提出了一种新颖的自适应重新加权方法，通过优先考虑靠近决策边界的样本并分配较高的权重，提高了公平分类器的泛化能力。

    

    随着机器学习应用在关键决策领域的普及，对算法公平性的呼声越来越大。尽管已经通过学习公平约束来改善算法的公平性的各种方式，但它们在测试集上的性能并不能很好地推广。需要一种性能有前景且具有更好泛化能力的公平算法。本文提出了一种新颖的自适应重新加权方法，以消除训练数据和测试数据之间分布偏移对模型泛化能力的影响。大多数先前的重新加权方法提议为每个（子）组分配一个统一的权重。相反，我们的方法细粒度地建模了样本预测与决策边界的距离。我们的自适应重新加权方法优先考虑靠近决策边界的样本，并分配较高的权重来提高公平分类器的泛化能力。进行了大量实验验证了其泛化能力。

    With the increasing penetration of machine learning applications in critical decision-making areas, calls for algorithmic fairness are more prominent. Although there have been various modalities to improve algorithmic fairness through learning with fairness constraints, their performance does not generalize well in the test set. A performance-promising fair algorithm with better generalizability is needed. This paper proposes a novel adaptive reweighing method to eliminate the impact of the distribution shifts between training and test data on model generalizability. Most previous reweighing methods propose to assign a unified weight for each (sub)group. Rather, our method granularly models the distance from the sample predictions to the decision boundary. Our adaptive reweighing method prioritizes samples closer to the decision boundary and assigns a higher weight to improve the generalizability of fair classifiers. Extensive experiments are performed to validate the generalizability 
    
[^68]: 理解自监督学习在表格异常检测中的限制

    Understanding the limitations of self-supervised learning for tabular anomaly detection. (arXiv:2309.08374v1 [cs.LG])

    [http://arxiv.org/abs/2309.08374](http://arxiv.org/abs/2309.08374)

    本研究探讨了自监督学习在表格异常检测中的限制。通过多个实验发现，自监督学习得到的表征并不能提高表格异常检测的性能，这是由于神经网络引入了无关的特征。然而，使用神经网络表示的子空间可以恢复性能。

    

    尽管自监督学习已经改进了计算机视觉和自然语言处理中的异常检测，但表格数据是否可以从中受益尚不清楚。本文探讨了自监督学习在表格异常检测中的限制。我们在26个基准数据集上进行了多个实验，涉及各种预训练任务，以了解这种情况的原因。我们的结果证实，与使用原始数据表示相比，通过自监督学习得到的表征并不能提高表格异常检测的性能。我们展示了这是由于神经网络引入了无关的特征，从而降低了异常检测器的有效性。然而，我们证明了使用神经网络表示的子空间可以恢复性能。

    While self-supervised learning has improved anomaly detection in computer vision and natural language processing, it is unclear whether tabular data can benefit from it. This paper explores the limitations of self-supervision for tabular anomaly detection. We conduct several experiments spanning various pretext tasks on 26 benchmark datasets to understand why this is the case. Our results confirm representations derived from self-supervision do not improve tabular anomaly detection performance compared to using the raw representations of the data. We show this is due to neural networks introducing irrelevant features, which reduces the effectiveness of anomaly detectors. However, we demonstrate that using a subspace of the neural network's representation can recover performance.
    
[^69]: Landscape-Sketch-Step: 一种基于AI/ML的元启发式方法解决代理优化问题

    Landscape-Sketch-Step: An AI/ML-Based Metaheuristic for Surrogate Optimization Problems. (arXiv:2309.07936v1 [cs.LG])

    [http://arxiv.org/abs/2309.07936](http://arxiv.org/abs/2309.07936)

    Landscape-Sketch-Step是一种基于AI/ML的元启发式方法，结合了机器学习、随机优化和强化学习技术，用于解决成本函数评估昂贵、不可访问或禁止的代理优化问题。

    

    本文介绍了一种新的全局优化启发式方法，用于在成本函数的评估非常昂贵、不可访问或甚至禁止的场景下进行优化。该方法称为Landscape-Sketch-Step（LSS），结合了机器学习、随机优化和强化学习技术，依赖于先前采样点的历史信息，以明智地选择应评估成本函数的参数值。与复制交换蒙特卡洛方法相比，该方法所需的成本函数评估次数与模拟退火方法相当，这在高通量计算或高性能计算任务等环境中尤为重要，因为评估要么计算成本高昂，要么需要很长时间才能完成。该方法与标准的代理优化技术也不同，因为它不构建代理模型。

    In this paper, we introduce a new heuristics for global optimization in scenarios where extensive evaluations of the cost function are expensive, inaccessible, or even prohibitive. The method, which we call Landscape-Sketch-and-Step (LSS), combines Machine Learning, Stochastic Optimization, and Reinforcement Learning techniques, relying on historical information from previously sampled points to make judicious choices of parameter values where the cost function should be evaluated at. Unlike optimization by Replica Exchange Monte Carlo methods, the number of evaluations of the cost function required in this approach is comparable to that used by Simulated Annealing, quality that is especially important in contexts like high-throughput computing or high-performance computing tasks, where evaluations are either computationally expensive or take a long time to be performed. The method also differs from standard Surrogate Optimization techniques, for it does not construct a surrogate model
    
[^70]: 基于正态学习的多尺度对比学习的图异常检测

    Normality Learning-based Graph Anomaly Detection via Multi-Scale Contrastive Learning. (arXiv:2309.06034v1 [cs.LG])

    [http://arxiv.org/abs/2309.06034](http://arxiv.org/abs/2309.06034)

    本文提出了一种基于正态学习的图异常检测框架NLGAD，通过多尺度对比学习网络来增强学习正常模式的能力，以改进异常检测性能。

    

    图异常检测（GAD）在机器学习和数据挖掘领域引起了越来越多的关注。最近的研究主要集中在如何捕捉更丰富的信息，以提高GAD中节点嵌入的质量。尽管它们在检测性能方面取得了显著进展，但对于任务的特性仍然相对不足。GAD旨在识别与大多数节点有所偏离的异常。然而，模型很容易学习到组成大多数样本的正常样本的模式。与此同时，当异常行为与正常性不同的时候，异常可以很容易被检测到。因此，通过增强学习正常模式的能力，性能可以进一步提高。为此，我们提出了一种基于正态学习的GAD框架，使用多尺度对比学习网络（简称NLGAD）。具体而言，我们首先使用不同尺度的对比网络初始化模型，以提供充足可靠的正常节点样本。

    Graph anomaly detection (GAD) has attracted increasing attention in machine learning and data mining. Recent works have mainly focused on how to capture richer information to improve the quality of node embeddings for GAD. Despite their significant advances in detection performance, there is still a relative dearth of research on the properties of the task. GAD aims to discern the anomalies that deviate from most nodes. However, the model is prone to learn the pattern of normal samples which make up the majority of samples. Meanwhile, anomalies can be easily detected when their behaviors differ from normality. Therefore, the performance can be further improved by enhancing the ability to learn the normal pattern. To this end, we propose a normality learning-based GAD framework via multi-scale contrastive learning networks (NLGAD for abbreviation). Specifically, we first initialize the model with the contrastive networks on different scales. To provide sufficient and reliable normal nod
    
[^71]: 使用预训练的大型语言模型进行多模态暗示的零样本推荐

    Zero-Shot Recommendations with Pre-Trained Large Language Models for Multimodal Nudging. (arXiv:2309.01026v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2309.01026](http://arxiv.org/abs/2309.01026)

    该论文提出了一种利用生成型AI领域的新技术进行零样本推荐的方法，通过将多模态输入转化为文本描述，并利用预训练的语言模型计算语义嵌入，实现了对非平稳内容的推荐。在合成的多模态暗示环境中进行实验证明了该方法的有效性。

    

    我们提出了一种利用生成型人工智能领域最新进展的方法，用于零样本推荐多模态非平稳内容。我们建议将不同模态的输入渲染为文本描述，并利用预训练的LLM计算语义嵌入获取它们的数值表示。一旦获得所有内容项的统一表示，可以通过计算适当的相似度度量来进行推荐，而无需进行额外的学习。我们在一个合成的多模态暗示环境中演示了我们的方法，其中输入包括表格、文本和视觉数据。

    We present a method for zero-shot recommendation of multimodal non-stationary content that leverages recent advancements in the field of generative AI. We propose rendering inputs of different modalities as textual descriptions and to utilize pre-trained LLMs to obtain their numerical representations by computing semantic embeddings. Once unified representations of all content items are obtained, the recommendation can be performed by computing an appropriate similarity metric between them without any additional learning. We demonstrate our approach on a synthetic multimodal nudging environment, where the inputs consist of tabular, textual, and visual data.
    
[^72]: 关于Adam的隐式偏差

    On the Implicit Bias of Adam. (arXiv:2309.00079v1 [cs.LG])

    [http://arxiv.org/abs/2309.00079](http://arxiv.org/abs/2309.00079)

    本文证明了RMSProp和Adam存在隐式规范化作用，其取决于超参数和训练阶段，并讨论了这些证明事实对泛化的影响。

    

    在以前的文献中，后向误差分析被用来找到近似梯度下降轨迹的常微分方程（ODEs）。发现有限步长会隐式地规范化解决方案，因为出现在ODE中的项会惩罚损失梯度的二范数。我们证明了RMSProp和Adam中是否存在类似的隐式规范化取决于它们的超参数和训练阶段，但涉及的“范数”不同：对应的ODE项要么惩罚（扰动的）损失梯度的一范数，要么相反地阻止其减小（后一种情况是典型的）。我们还进行了数值实验，并讨论了这些证明事实如何影响泛化。

    In previous literature, backward error analysis was used to find ordinary differential equations (ODEs) approximating the gradient descent trajectory. It was found that finite step sizes implicitly regularize solutions because terms appearing in the ODEs penalize the two-norm of the loss gradients. We prove that the existence of similar implicit regularization in RMSProp and Adam depends on their hyperparameters and the training stage, but with a different "norm" involved: the corresponding ODE terms either penalize the (perturbed) one-norm of the loss gradients or, on the contrary, hinder its decrease (the latter case being typical). We also conduct numerical experiments and discuss how the proven facts can influence generalization.
    
[^73]: BioCoder: 一种带有上下文语用知识的生物信息学代码生成基准

    BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge. (arXiv:2308.16458v1 [cs.LG])

    [http://arxiv.org/abs/2308.16458](http://arxiv.org/abs/2308.16458)

    BioCoder是一个用于评估预训练模型在生成生物信息学代码方面的基准，涵盖了函数代码生成中的包依赖关系、类声明和全局变量，并通过模糊测试框架进行评估。

    

    预训练的语言模型（如ChatGPT）显著改进了代码生成。随着这些模型的扩大，需要输出来处理更复杂的任务的需求也越来越多。此外，在生物信息学中，生成功能程序由于领域知识量大、需要复杂的数据操作和复杂的功能依赖关系而面临额外的挑战。在这里，我们介绍了BioCoder，这是一个用于评估现有预训练模型在生成生物信息学代码方面的基准。与函数代码生成有关，BioCoder涵盖了可能的包依赖关系、类声明和全局变量。它包括来自GitHub的1026个Python和Java函数和1243个方法，以及来自Rosalind项目的253个示例。BioCoder还结合了一个用于评估的模糊测试框架，我们已经应用它来评估许多模型，包括InCoder、CodeGen、CodeGen2、SantaCoder、StarCoder、StarCoder+、InstructCodeT。

    Pre-trained language models like ChatGPT have significantly improved code generation. As these models scale up, there is an increasing need for the output to handle more intricate tasks. Moreover, in bioinformatics, generating functional programs poses additional notable challenges due to the amount of domain knowledge, the need for complicated data operations, and intricate functional dependencies between the operations. Here, we present BioCoder, a benchmark developed to evaluate existing pre-trained models in generating bioinformatics code. In relation to function-code generation, BioCoder covers potential package dependencies, class declarations, and global variables. It incorporates 1026 functions and 1243 methods in Python and Java from GitHub and 253 examples from the Rosalind Project. BioCoder incorporates a fuzz-testing framework for evaluation, and we have applied it to evaluate many models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT
    
[^74]: 通过正则化Wasserstein Proximals实现无噪声的抽样算法

    Noise-Free Sampling Algorithms via Regularized Wasserstein Proximals. (arXiv:2308.14945v1 [stat.ML])

    [http://arxiv.org/abs/2308.14945](http://arxiv.org/abs/2308.14945)

    本文通过正则化Wasserstein Proximal方法提出了一种无噪声的抽样算法，通过给定的潜势函数确定性地进行粒子演化，并提供了优于传统方法的维度依赖性和速度收敛性能。

    

    本文考虑由潜势函数控制的分布抽样问题。本文提出了一种显式的基于评分的确定性马尔科夫链蒙特卡洛方法，使得粒子的演化变为确定性的，而不是随机微分方程的演化。评分项由正则化的Wasserstein proximal以闭合形式给出，使用采样来近似核卷积。我们在不同问题上展示了快速收敛，并且与未调整Langevin算法和Metropolis调整Langevin算法相比，显示了高斯分布的混合时间边界的改善维度依赖性。我们还推导了二次潜势函数每次迭代的分布的闭合形式表达式，表征了方差降低。实证结果表明，粒子的行为是有组织的，位于潜势的等值线上。此外，后验均值估计结果显示了该方法的有效性。

    We consider the problem of sampling from a distribution governed by a potential function. This work proposes an explicit score-based MCMC method that is deterministic, resulting in a deterministic evolution for particles rather than a stochastic differential equation evolution. The score term is given in closed form by a regularized Wasserstein proximal, using a kernel convolution that is approximated by sampling. We demonstrate fast convergence on various problems and show improved dimensional dependence of mixing time bounds for the case of Gaussian distributions compared to the unadjusted Langevin algorithm (ULA) and the Metropolis-adjusted Langevin algorithm (MALA). We additionally derive closed form expressions for the distributions at each iterate for quadratic potential functions, characterizing the variance reduction. Empirical results demonstrate that the particles behave in an organized manner, lying on level set contours of the potential. Moreover, the posterior mean estimat
    
[^75]: 逃离样本陷阱：使用配对距离估计器快速准确地估计认识不确定性

    Escaping the Sample Trap: Fast and Accurate Epistemic Uncertainty Estimation with Pairwise-Distance Estimators. (arXiv:2308.13498v1 [cs.LG])

    [http://arxiv.org/abs/2308.13498](http://arxiv.org/abs/2308.13498)

    本文介绍了使用配对距离估计器对集成模型进行认识不确定性估计的新方法，相比于常用的深度学习方法，该方法能够更快速、更准确地在更大的空间和更高维度上估计认识不确定性。

    

    本文介绍了一种使用配对距离估计器（PaiDEs）对集成模型进行认识不确定性估计的新方法。这些估计器利用模型组件之间的配对距离来建立熵的边界，并将这些边界作为基于信息准则的估计值。与最近基于样本的蒙特卡洛估计器用于认识不确定性估计的深度学习方法不同，PaiDEs能够在更大的空间（最多100倍）上以更快的速度（最多100倍）估计认识不确定性，并在更高维度上具有更准确的性能。为了验证我们的方法，我们进行了一系列用于评估认识不确定性估计的实验：一维正弦数据，摆动物体（Pendulum-v0），跳跃机器人（Hopper-v2），蚂蚁机器人（Ant-v2）和人形机器人（Humanoid-v2）。对于每个实验设置，我们应用了主动学习框架来展示PaiDEs在认识不确定性估计中的优势。

    This work introduces a novel approach for epistemic uncertainty estimation for ensemble models using pairwise-distance estimators (PaiDEs). These estimators utilize the pairwise-distance between model components to establish bounds on entropy and uses said bounds as estimates for information-based criterion. Unlike recent deep learning methods for epistemic uncertainty estimation, which rely on sample-based Monte Carlo estimators, PaiDEs are able to estimate epistemic uncertainty up to 100$\times$ faster, over a larger space (up to 100$\times$) and perform more accurately in higher dimensions. To validate our approach, we conducted a series of experiments commonly used to evaluate epistemic uncertainty estimation: 1D sinusoidal data, Pendulum-v0, Hopper-v2, Ant-v2 and Humanoid-v2. For each experimental setting, an Active Learning framework was applied to demonstrate the advantages of PaiDEs for epistemic uncertainty estimation.
    
[^76]: 基于压缩机的房颤检测分类器

    Compressor-Based Classification for Atrial Fibrillation Detection. (arXiv:2308.13328v1 [eess.SP])

    [http://arxiv.org/abs/2308.13328](http://arxiv.org/abs/2308.13328)

    本文研究了基于压缩机的文本分类方法在房颤检测中的应用。通过对心律间隔序列进行压缩距离计算和最近邻分类器模型优化，我们实现了良好的分类性能，接近于最佳的房颤检测算法。这表明gzip分类器适用于生物医学数据和连续随机序列的分类。

    

    房颤是最常见的心律失常之一，对公共健康有重大影响。自动检测房颤发作是生物医学工程中最重要的任务之一。本文将最近引入的基于压缩机的文本分类方法应用于房颤检测任务（心律之间的二分类）。我们研究了对ΔRR和RR间期序列应用归一化压缩距离，k-最近邻分类器的配置，以及最佳窗口长度。我们实现了良好的分类结果（平均敏感度=97.1%，平均特异度=91.7%，最佳敏感度为99.8%，最佳特异度为97.6% 5折交叉验证）。所获得的性能接近于最佳的专门的房颤检测算法。我们的结果表明，gzip分类器，最初用于文本，也适用于生物医学数据和连续随机序列。

    Atrial fibrillation (AF) is one of the most common arrhythmias with challenging public health implications. Automatic detection of AF episodes is therefore one of the most important tasks in biomedical engineering. In this paper, we apply the recently introduced method of compressor-based text classification to the task of AF detection (binary classification between heart rhythms). We investigate the normalised compression distance applied to $\Delta$RR and RR-interval sequences, the configuration of the k-Nearest Neighbour classifier, and an optimal window length. We achieve good classification results (avg. sensitivity = 97.1%, avg. specificity = 91.7%, best sensitivity of 99.8%, best specificity of 97.6% with 5-fold cross-validation). Obtained performance is close to the best specialised AF detection algorithms. Our results suggest that gzip classification, originally proposed for texts, is suitable for biomedical data and continuous stochastic sequences in general.
    
[^77]: 基于提示的长度受控生成与强化学习

    Prompt-Based Length Controlled Generation with Reinforcement Learning. (arXiv:2308.12030v1 [cs.CL])

    [http://arxiv.org/abs/2308.12030](http://arxiv.org/abs/2308.12030)

    提出了一种基于提示的长度控制方法，利用强化学习和奖励模型来实现大型语言模型（LLM）的长度受控生成。该方法可以有效减少推理成本并满足不同需求。

    

    最近，大型语言模型（LLM）如ChatGPT和GPT-4因其惊人的改进和性能而受到广泛关注。长度受控生成成为LLM中的一个重要话题，它还使用户能够充分利用LLM的能力在更多实际场景中生成所需长度的合适答案或文章。此外，LLM中的自回归生成非常耗时，而控制生成长度的能力可以通过限制长度任意降低推理成本，从而满足不同需求。因此，我们旨在提出一种基于提示的长度控制方法来实现长度受控生成，这种方法也可以广泛应用于类似GPT的LLM中。具体而言，我们采用强化学习，使用可训练或基于规则的奖励模型提供奖励信号，进一步通过对预定义目标长度进行奖励来影响LLM的生成。实验证明...

    Recently, large language models (LLMs) like ChatGPT and GPT-4 have attracted great attention given their surprising improvement and performance. Length controlled generation of LLMs emerges as an important topic, which also enables users to fully leverage the capability of LLMs in more real-world scenarios like generating a proper answer or essay of a desired length. In addition, the autoregressive generation in LLMs is extremely time-consuming, while the ability of controlling this generated length can arbitrarily reduce the inference cost by limiting the length, and thus satisfy different needs. Therefore, we aim to propose a prompt-based length control method to achieve this length controlled generation, which can also be widely applied in GPT-style LLMs. In particular, we adopt reinforcement learning with the reward signal given by either trainable or rule-based reward model, which further affects the generation of LLMs via rewarding a pre-defined target length. Experiments show th
    
[^78]: 通过去混淆器解决有偏见的电子健康记录中的健康差异问题

    Mitigating Health Disparity on Biased Electronic Health Records via Deconfounder. (arXiv:2308.11819v1 [cs.LG])

    [http://arxiv.org/abs/2308.11819](http://arxiv.org/abs/2308.11819)

    通过使用去混淆器模型FLMD，在电子健康记录建模中实现了公平性和准确性，解决了有偏见的EHR中的健康差异问题。

    

    临床数据建模的公平性问题，尤其是在电子健康记录（EHRs）上，由于EHR的复杂潜在结构和潜在选择偏差问题而至关重要。在实践中，通常需要在保持模型整体准确性的同时减少健康差异。然而，传统方法往往在准确性和公平性之间存在权衡，因为它们无法捕捉到观察数据之外的潜在因素。为了解决这个挑战，我们提出了一个名为公平纵向医疗去混淆器（FLMD）的新模型，旨在实现纵向电子健康记录（EHR）的公平性和准确性建模。受到去混淆器理论的启发，FLMD采用了一个两阶段的训练过程。在第一阶段，FLMD捕捉了每次接触的未观察到的混淆因子，这有效地表示了超出观察到的EHR之外的潜在医疗因素，如患者基因型和生活习惯。这个未观察到的混淆因子是至关重要的。

    The fairness issue of clinical data modeling, especially on Electronic Health Records (EHRs), is of utmost importance due to EHR's complex latent structure and potential selection bias. It is frequently necessary to mitigate health disparity while keeping the model's overall accuracy in practice. However, traditional methods often encounter the trade-off between accuracy and fairness, as they fail to capture the underlying factors beyond observed data. To tackle this challenge, we propose a novel model called Fair Longitudinal Medical Deconfounder (FLMD) that aims to achieve both fairness and accuracy in longitudinal Electronic Health Records (EHR) modeling. Drawing inspiration from the deconfounder theory, FLMD employs a two-stage training process. In the first stage, FLMD captures unobserved confounders for each encounter, which effectively represents underlying medical factors beyond observed EHR, such as patient genotypes and lifestyle habits. This unobserved confounder is crucial 
    
[^79]: LLM中的时间旅行：追踪大型语言模型中的数据污染

    Time Travel in LLMs: Tracing Data Contamination in Large Language Models. (arXiv:2308.08493v1 [cs.CL])

    [http://arxiv.org/abs/2308.08493](http://arxiv.org/abs/2308.08493)

    该论文提出了一种用于识别大型语言模型（LLMs）中数据污染的简单而有效的方法。通过对随机样本中的单个实例进行分析，以及使用“引导指令”来评估整个数据集分区的污染程度，可以准确地识别污染的实例和分区。

    

    数据污染是指大型语言模型（LLMs）的训练数据中存在来自下游任务的测试数据，这可能是理解LLMs在其他任务上有效性的一个重要问题。我们提出了一种简单而有效的方法来识别LLMs中的数据污染。我们的方法核心是通过识别从小的随机样本中抽取的单个实例中的潜在污染，然后评估整个数据集分区是否受到污染。为了估计单个实例的污染程度，我们使用了“引导指令”：即一个由数据集名称、分区类型和参考实例的初始部分组成的提示，要求LLM完成它。如果LLM的输出与参考实例的后一部分完全或接近匹配，那么该实例被标记为受到污染。为了了解整个分区是否受到污染，我们提出了两个想法。第一个想法是标记一个数据集的分区，该分区中的实例大多数都被判断为受到污染。

    Data contamination, i.e., the presence of test data from downstream tasks in the training data of large language models (LLMs), is a potential major issue in understanding LLMs' effectiveness on other tasks. We propose a straightforward yet effective method for identifying data contamination within LLMs. At its core, our approach starts by identifying potential contamination in individual instances that are drawn from a small random sample; using this information, our approach then assesses if an entire dataset partition is contaminated. To estimate contamination of individual instances, we employ "guided instruction:" a prompt consisting of the dataset name, partition type, and the initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM's output either exactly or closely matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset
    
[^80]: Ada-QPacknet -- 自适应剪枝与位宽缩减作为一种高效的继续学习方法，不会遗忘的算法

    Ada-QPacknet -- adaptive pruning with bit width reduction as an efficient continual learning method without forgetting. (arXiv:2308.07939v1 [cs.LG])

    [http://arxiv.org/abs/2308.07939](http://arxiv.org/abs/2308.07939)

    Ada-QPacknet是一种自适应剪枝与位宽缩减的高效继续学习方法，通过剪枝和量化技术生成任务子网络，在动态和复杂环境中实现了与浮点数子网络相似的准确性。

    

    继续学习（CL）是一个过程，其中人类和深度学习模型之间的效率仍存在巨大差距。最近设计了许多CL算法，大部分都存在在动态和复杂环境中学习的问题。本文描述了一种基于新架构的方法Ada-QPacknet。它通过剪枝提取每个任务的子网络。基于架构的CL方法的关键是容量。在提出的方法中，通过高效的线性和非线性量化方法减小了模型的规模。该方法减小了权重格式的位宽。实验结果显示，混合8位和4位量化在著名的CL场景上实现了与浮点数子网络相似的准确性。据我们所知，这是第一个将剪枝和量化这两种压缩技术应用于生成任务子网络的CL策略。该算法在著名的情节组合上进行了测试。

    Continual Learning (CL) is a process in which there is still huge gap between human and deep learning model efficiency. Recently, many CL algorithms were designed. Most of them have many problems with learning in dynamic and complex environments. In this work new architecture based approach Ada-QPacknet is described. It incorporates the pruning for extracting the sub-network for each task. The crucial aspect in architecture based CL methods is theirs capacity. In presented method the size of the model is reduced by efficient linear and nonlinear quantisation approach. The method reduces the bit-width of the weights format. The presented results shows that hybrid 8 and 4-bit quantisation achieves similar accuracy as floating-point sub-network on a well-know CL scenarios. To our knowledge it is the first CL strategy which incorporates both compression techniques pruning and quantisation for generating task sub-networks. The presented algorithm was tested on well-known episode combination
    
[^81]: 傅里叶神经运算器用于实时模拟3D动态城市微气候

    Fourier neural operator for real-time simulation of 3D dynamic urban microclimate. (arXiv:2308.03985v1 [cs.LG])

    [http://arxiv.org/abs/2308.03985](http://arxiv.org/abs/2308.03985)

    该论文介绍了傅里叶神经运算器（FNO）用于实时模拟3D动态城市微气候。通过深度学习技术，FNO在加速解决复杂非线性相互作用和系统动力学建模方面表现出很大的潜力。

    

    全球城市化凸显了城市微气候对人类舒适度、健康和建筑/城市能效的重要性。它们对建筑设计和城市规划产生重大的环境影响。理解局部微气候对于城市应对气候变化和有效实施弹性措施至关重要。然而，分析城市微气候需要在计算域内考虑复杂的室外参数，涵盖较长时期，并涵盖城市规模。因此，数值方法如计算流体力学（CFD）在评估城市微气候的影响时变得计算复杂。深度学习技术的兴起为加速复杂非线性相互作用和系统动力学建模提供了新机会。最近，傅里叶神经运算器（FNO）在加速解决偏微分方程方面表现出了非常有希望的结果。

    Global urbanization has underscored the significance of urban microclimates for human comfort, health, and building/urban energy efficiency. They profoundly influence building design and urban planning as major environmental impacts. Understanding local microclimates is essential for cities to prepare for climate change and effectively implement resilience measures. However, analyzing urban microclimates requires considering a complex array of outdoor parameters within computational domains at the city scale over a longer period than indoors. As a result, numerical methods like Computational Fluid Dynamics (CFD) become computationally expensive when evaluating the impact of urban microclimates. The rise of deep learning techniques has opened new opportunities for accelerating the modeling of complex non-linear interactions and system dynamics. Recently, the Fourier Neural Operator (FNO) has been shown to be very promising in accelerating solving the Partial Differential Equations (PDEs
    
[^82]: FLIPS: 使用智能参与者选择的联邦学习

    FLIPS: Federated Learning using Intelligent Participant Selection. (arXiv:2308.03901v1 [cs.LG])

    [http://arxiv.org/abs/2308.03901](http://arxiv.org/abs/2308.03901)

    本文介绍了FLIPS，这是一个用于管理联邦学习中数据和参与者异质性的中间件系统。FLIPS通过标签分布聚类和智能参与者选择，并使用可信执行环境来确保隐私保护。实证评估表明，FLIPS相比随机方法有更好的性能。

    

    本文介绍了FLIPS的设计和实现，这是一个用于管理联邦学习中数据和参与者异质性的中间件系统。特别地，我们研究了标签分布聚类在联邦学习中参与者选择中的好处。FLIPS根据数据的标签分布预先对参与FL训练作业的各方进行聚类，并在FL训练期间确保每个聚类在被选中的参与者中公平地表示。FLIPS可以支持最常见的FL算法，包括FedAvg，FedProx，FedDyn，FedOpt和FedYogi。为了管理平台的异构性和动态资源可用性，FLIPS还结合了一种处理分布式智能社区应用中容量变化的拖累管理机制。标签分布、聚类和参与者选择的隐私通过可信执行环境(TEE)来确保。我们全面的实证评估将FLIPS与随机方法进行了比较。

    This paper presents the design and implementation of FLIPS, a middleware system to manage data and participant heterogeneity in federated learning (FL) training workloads. In particular, we examine the benefits of label distribution clustering on participant selection in federated learning. FLIPS clusters parties involved in an FL training job based on the label distribution of their data apriori, and during FL training, ensures that each cluster is equitably represented in the participants selected. FLIPS can support the most common FL algorithms, including FedAvg, FedProx, FedDyn, FedOpt and FedYogi. To manage platform heterogeneity and dynamic resource availability, FLIPS incorporates a straggler management mechanism to handle changing capacities in distributed, smart community applications. Privacy of label distributions, clustering and participant selection is ensured through a trusted execution environment (TEE). Our comprehensive empirical evaluation compares FLIPS with random p
    
[^83]: 分支潜在神经算子

    Branched Latent Neural Operators. (arXiv:2308.02599v1 [cs.LG])

    [http://arxiv.org/abs/2308.02599](http://arxiv.org/abs/2308.02599)

    Branched Latent Neural Operators (BLNOs) are introduced to learn input-output maps for encoding complex physical processes. BLNOs utilize interpretable latent outputs to enhance learned dynamics and overcome the curse of dimensionality, while also showing excellent generalization properties with small training datasets and short training times. The partial connection structure reduces the number of tunable parameters. BLNOs are proven effective in a challenging biophysically detailed test case.

    

    我们介绍了分支潜在神经算子（BLNOs）来学习编码复杂物理过程的输入-输出映射。BLNO由一个简单紧凑的前馈部分连接神经网络定义，该网络在结构上将不同固有角色的输入进行解离，例如将微分方程的时间变量与模型参数分离，并将它们转化为感兴趣的通用领域。BLNO利用可解释的潜在输出增强了学习到的动态，并通过在单个处理器上使用小的训练数据集和短的训练时间展示了出色的泛化性能。实际上，它们的泛化误差在测试阶段采用的离散化方式相同的情况下保持可比性。此外，部分连接在全连接结构的基础上显著减少了可调参数的数量。我们展示了BLNO在涉及生物物理细节的具有挑战性的测试案例中的能力。

    We introduce Branched Latent Neural Operators (BLNOs) to learn input-output maps encoding complex physical processes. A BLNO is defined by a simple and compact feedforward partially-connected neural network that structurally disentangles inputs with different intrinsic roles, such as the time variable from model parameters of a differential equation, while transferring them into a generic field of interest. BLNOs leverage interpretable latent outputs to enhance the learned dynamics and break the curse of dimensionality by showing excellent generalization properties with small training datasets and short training times on a single processor. Indeed, their generalization error remains comparable regardless of the adopted discretization during the testing phase. Moreover, the partial connections, in place of a fully-connected structure, significantly reduce the number of tunable parameters. We show the capabilities of BLNOs in a challenging test case involving biophysically detailed elect
    
[^84]: 海底滑翔器的通用异常检测验证方法——大规模部署数据集的应用

    General Anomaly Detection of Underwater Gliders Validated by Large-scale Deployment Dataset. (arXiv:2308.00180v1 [cs.RO])

    [http://arxiv.org/abs/2308.00180](http://arxiv.org/abs/2308.00180)

    本文介绍了一种用于评估海底滑翔器在不可预测海洋环境中正常操作的异常检测算法，并通过实际滑翔器部署的大规模数据集进行验证。算法能够实时提供异常警报，使驾驶员能够控制滑翔器并避免进一步损害。

    

    本文利用一种异常检测算法评估在不可预测的海洋环境中海底滑翔器的正常操作。一旦检测到任何异常，可以向滑翔器驾驶员提供实时警报，使其能够接管滑翔器并防止进一步的损害。该检测算法应用于由Skidaway海洋研究所（SkIO）和南佛罗里达大学（USF）领导的实际滑翔器部署中收集的大量数据集。就泛化性而言，实验评估包括离线和在线检测模式。离线检测利用完整的回收后数据集，具有高分辨率的信息，对异常进行详细分析并与驾驶员日志进行比较。在线检测专注于从滑翔器传输的实时数据子集。虽然实时数据可能不包含与回收后数据一样丰富的信息，但在线检测是实时的。

    This paper employs an anomaly detection algorithm to assess the normal operation of underwater gliders in unpredictable ocean environments. Real-time alerts can be provided to glider pilots upon detecting any anomalies, enabling them to assume control of the glider and prevent further harm. The detection algorithm is applied to abundant data sets collected in real glider deployments led by the Skidaway Institute of Oceanography (SkIO) and the University of South Florida (USF). Regarding generality, the experimental evaluation is composed of both offline and online detection modes. The offline detection utilizes full post-recovery data sets, which carries high-resolution information, to present detailed analysis of the anomaly and compare it with pilot logs. The online detection focuses on the real-time subsets of data transmitted from the glider at the surfacing events. While the real-time data may not contain as much rich information as the post-recovery data, the online detection is 
    
[^85]: 深度学习与自适应滤波：Stein无偏风险估计方法的应用

    Deep Learning Meets Adaptive Filtering: A Stein's Unbiased Risk Estimator Approach. (arXiv:2307.16708v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2307.16708](http://arxiv.org/abs/2307.16708)

    本文通过算法展开的方法，将递归最小二乘法（RLS）和等变自适应源分离（EASI）两种算法转化为深度神经网络的层，通过利用训练过程有效地进行源信号估计。同时，通过使用基于Stein无偏风险估计（SURE）的损失函数训练，进一步提高了性能，实证评估证明了该方法的有效性。

    

    本文通过算法展开的视角重新审视了两种著名的自适应滤波算法，即递归最小二乘法（RLS）和等变自适应源分离（EASI），在源估计和分离的环境中。在展开方法的基础上，我们引入了新的基于任务的深度学习框架，称为Deep RLS和Deep EASI。这些架构将原始算法的迭代变换为深度神经网络的层，从而通过利用训练过程有效地进行源信号估计。为了进一步提高性能，我们提出使用基于Stein无偏风险估计（SURE）的损失函数对这些深度展开网络进行训练。我们的实证评估证明了这种基于SURE的方法对于增强源信号估计的有效性。

    This paper revisits two prominent adaptive filtering algorithms through the lens of algorithm unrolling, namely recursive least squares (RLS) and equivariant adaptive source separation (EASI), in the context of source estimation and separation. Building upon the unrolling methodology, we introduce novel task-based deep learning frameworks, denoted as Deep RLS and Deep EASI. These architectures transform the iterations of the original algorithms into layers of a deep neural network, thereby enabling efficient source signal estimation by taking advantage of a training process. To further enhance performance, we propose training these deep unrolled networks utilizing a loss function grounded on a Stein's unbiased risk estimator (SURE). Our empirical evaluations demonstrate the efficacy of this SURE-based approach for enhanced source signal estimation.
    
[^86]: 用于16位神经网络训练中数值不稳定性的高效方法

    An Efficient Approach to Mitigate Numerical Instability in Backpropagation for 16-bit Neural Network Training. (arXiv:2307.16189v1 [cs.LG])

    [http://arxiv.org/abs/2307.16189](http://arxiv.org/abs/2307.16189)

    这项研究探讨了16位计算中机器学习模型的数值不稳定性问题，并提出了一种基于Adam优化器的新方法来提高16位神经网络的学习过程的鲁棒性。

    

    在这项研究中，我们深入探讨了在16位计算中使用流行的优化算法（如RMSProp和Adam）时观察到的数值不稳定性的复杂性。这种不稳定性通常在深度神经网络的训练阶段中出现，导致学习过程受到干扰，从而妨碍了这些模型的有效部署。我们确定了单一超参数epsilon是这种数值不稳定性的主要原因。对16位计算中这些优化器中epsilon的作用进行了深入探索，发现微调其值可以恢复RMSProp和Adam的功能，从而实现有效利用16位神经网络。我们提出了一种新的方法来减轻被发现的数值不稳定性问题。该方法利用Adam优化器的更新，并显著改善了16位神经网络的学习过程的鲁棒性。

    In this research, we delve into the intricacies of the numerical instability observed in 16-bit computations of machine learning models, particularly when employing popular optimization algorithms such as RMSProp and Adam. This instability is commonly experienced during the training phase of deep neural networks, leading to disrupted learning processes and hindering the effective deployment of such models. We identify the single hyperparameter, epsilon, as the main culprit behind this numerical instability. An in-depth exploration of the role of epsilon in these optimizers within 16-bit computations reveals that a minor adjustment of its value can restore the functionality of RMSProp and Adam, consequently enabling the effective utilization of 16-bit neural networks. We propose a novel method to mitigate the identified numerical instability issues. This method capitalizes on the updates from the Adam optimizer and significantly improves the robustness of the learning process in 16-bit 
    
[^87]: 关于神经抽象的效率和精确性之间的权衡

    On the Trade-off Between Efficiency and Precision of Neural Abstraction. (arXiv:2307.15546v1 [cs.LO])

    [http://arxiv.org/abs/2307.15546](http://arxiv.org/abs/2307.15546)

    本研究探讨了神经抽象的效率和精确性之间的权衡问题，研究发现抽象的用途取决于具体场景，请求简单的粗略抽象同样会有其用途，而对于更复杂

    

    神经抽象最近被引入作为复杂非线性动力模型的形式近似。它们包括一个神经ODE和抽象神经网络与具体动力模型之间误差的证明上界。到目前为止，神经抽象仅以全$ReLU$激活函数组成的神经网络形式得到，导致具有分段仿射动力学的神经ODE模型，可以等效地解释为线性混合自动机。在这项工作中，我们观察到抽象的效用取决于它的使用：某些情况可能需要容易分析的粗略抽象，而其他情况可能需要更复杂的精细抽象。因此，我们考虑替代形状的神经抽象，即分段常数或非线性非多项式（具体来说，通过sigmoidal激活函数获得）。我们采用正式的归纳综合程序来生成神经抽象。

    Neural abstractions have been recently introduced as formal approximations of complex, nonlinear dynamical models. They comprise a neural ODE and a certified upper bound on the error between the abstract neural network and the concrete dynamical model. So far neural abstractions have exclusively been obtained as neural networks consisting entirely of $ReLU$ activation functions, resulting in neural ODE models that have piecewise affine dynamics, and which can be equivalently interpreted as linear hybrid automata. In this work, we observe that the utility of an abstraction depends on its use: some scenarios might require coarse abstractions that are easier to analyse, whereas others might require more complex, refined abstractions. We therefore consider neural abstractions of alternative shapes, namely either piecewise constant or nonlinear non-polynomial (specifically, obtained via sigmoidal activations). We employ formal inductive synthesis procedures to generate neural abstractions t
    
[^88]: 一种具有规划、长期上下文理解和程序合成能力的现实世界WebAgent

    A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis. (arXiv:2307.12856v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.12856](http://arxiv.org/abs/2307.12856)

    这篇论文介绍了一种名为WebAgent的LLM驱动代理，通过自我经验学习，在真实网站上完成任务。该方法通过规划、总结和生成代码来提高在真实网站上的成功率。

    

    最近，预训练的大型语言模型（LLMs）在自主Web自动化方面取得了更好的泛化性能和样本效率。然而，在真实世界的网站上，性能仍然受到三个方面的限制：开放领域性、有限的上下文长度和对HTML的归纳偏差的缺乏。我们介绍了一种名为WebAgent的LLM驱动代理，它通过自我经验学习，在遵循自然语言指令的前提下，在真实网站上完成任务。WebAgent通过将指令分解为规范的子指令，将长HTML文档总结为与任务相关的片段，并通过从中生成的Python程序对网站进行操作来提前进行规划。我们使用Flan-U-PaLM设计了WebAgent，用于生成有根代码，并使用HTML-T5进行预训练LLMs，利用局部和全局注意机制以及混合长跨度去噪目标来进行规划和总结。我们通过实验证明，我们的模块化方法提高了在真实网站上的成功率。

    Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation. However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML. We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions. WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those. We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization. We empirically demonstrate that our modular recipe improves the success on real websites by ov
    
[^89]: 基于语言的动作概念空间改进视频自监督学习

    Language-based Action Concept Spaces Improve Video Self-Supervised Learning. (arXiv:2307.10922v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.10922](http://arxiv.org/abs/2307.10922)

    这项研究使用语言相关的自监督学习方法，将图像CLIP模型调整为适用于视频领域，并通过在动作概念空间中进行自蒸馏训练，提高了零样本和线性推测性能。

    

    最近的对比语言图像预训练方法已经实现了学习可传递和鲁棒的图像表示。然而，如何在少量监督的情况下将这些模型应用于视频领域仍然是一个未解决的问题。我们探索了朝着这个方向的简单步骤，使用与语言相关的自我监督学习，将图像CLIP模型调整为视频领域。我们修改了适用于时间建模的骨干网络，通过在动作概念空间中使用训练目标进行自蒸馏训练。使用相关文本提示从语言编码器提取的各个动作概念的特征向量构成了这个空间。我们提出了两个训练目标，概念蒸馏和概念对齐，既保留了原始表示的广泛性，又强化了动作和其属性之间的关系。我们的方法改进了三个动作识别基准上的零样本和线性推测性能。

    Recent contrastive language image pre-training has led to learning highly transferable and robust image representations. However, adapting these models to video domains with minimal supervision remains an open problem. We explore a simple step in that direction, using language tied self-supervised learning to adapt an image CLIP model to the video domain. A backbone modified for temporal modeling is trained under self-distillation settings with train objectives operating in an action concept space. Feature vectors of various action concepts extracted from a language encoder using relevant textual prompts construct this space. We introduce two train objectives, concept distillation and concept alignment, that retain generality of original representations while enforcing relations between actions and their attributes. Our approach improves zero-shot and linear probing performance on three action recognition benchmarks.
    
[^90]: 用于学习时间齐次动力系统的深度投影网络

    Deep projection networks for learning time-homogeneous dynamical systems. (arXiv:2307.09912v1 [cs.LG])

    [http://arxiv.org/abs/2307.09912](http://arxiv.org/abs/2307.09912)

    这篇论文介绍了一种利用深度投影网络学习时间齐次动力系统的有意义表示的方法。通过优化类似于经典相关分析的目标函数，避免了矩阵求逆的稳定性问题，并通过两种正则化方案进一步增强学习效果。

    

    我们考虑了一般的时间齐次动力系统，包括离散和连续的，并研究了从观测数据中学习状态的有意义表示的问题。这对于学习系统的前向传输算子至关重要，该算子可以用于预测未来的状态或可观测量。表示通常通过神经网络参数化，与投影算子相关联，并通过优化类似于经典相关分析（CCA）的目标函数来学习。然而，与CCA不同，我们的目标函数避免了矩阵求逆，因此通常更稳定且适用于具有挑战性的场景。我们的目标函数是CCA的一个紧松弛，我们进一步通过提出两种正则化方案来增强它，一种鼓励表示的分量正交，而另一种利用了 Chapman-Kolmogorov 方程。我们将我们的方法应用于具有挑战性的离散动力系统

    We consider the general class of time-homogeneous dynamical systems, both discrete and continuous, and study the problem of learning a meaningful representation of the state from observed data. This is instrumental for the task of learning a forward transfer operator of the system, that in turn can be used for forecasting future states or observables. The representation, typically parametrized via a neural network, is associated with a projection operator and is learned by optimizing an objective function akin to that of canonical correlation analysis (CCA). However, unlike CCA, our objective avoids matrix inversions and therefore is generally more stable and applicable to challenging scenarios. Our objective is a tight relaxation of CCA and we further enhance it by proposing two regularization schemes, one encouraging the orthogonality of the components of the representation while the other exploiting Chapman-Kolmogorov's equation. We apply our method to challenging discrete dynamical
    
[^91]: 通过单向流进行对抗性似然估计

    Adversarial Likelihood Estimation with One-way Flows. (arXiv:2307.09882v1 [cs.LG])

    [http://arxiv.org/abs/2307.09882](http://arxiv.org/abs/2307.09882)

    本文提出了一种通过单向流进行对抗性似然估计的方法，并使用重要性采样解决了Wasserstein GAN中分区函数有偏估计的问题。同时，通过最大化生成器的熵，提高了模式覆盖效果。这种方法通过计算生成样本的密度来实现对分区函数的无偏估计和生成器熵的计算。

    

    生成对抗网络（GAN）能够产生高质量的样本，但无法提供样本周围的概率密度估计。然而，已经注意到在能量模型的设置中，最大化对数似然可以导致判别器提供非归一化的密度（通常称为能量）的对抗性框架。我们进一步发展了这一观点，结合重要性采样，并展示了以下内容：1）Wasserstein GAN对分区函数进行了有偏估计，我们提出使用无偏估计方法；2）在最优化似然时，必须最大化生成器的熵。这被假设会提供更好的模式覆盖。与以前的工作不同，我们明确计算了生成样本的密度。这是设计无偏估计分区函数以及计算生成器熵的关键因素。生成密度是通过一种新型的流网络来获得的，称为单向流网络。

    Generative Adversarial Networks (GANs) can produce high-quality samples, but do not provide an estimate of the probability density around the samples. However, it has been noted that maximizing the log-likelihood within an energy-based setting can lead to an adversarial framework where the discriminator provides unnormalized density (often called energy). We further develop this perspective, incorporate importance sampling, and show that 1) Wasserstein GAN performs a biased estimate of the partition function, and we propose instead to use an unbiased estimator; 2) when optimizing for likelihood, one must maximize generator entropy. This is hypothesized to provide a better mode coverage. Different from previous works, we explicitly compute the density of the generated samples. This is the key enabler to designing an unbiased estimator of the partition function and computation of the generator entropy term. The generator density is obtained via a new type of flow network, called one-way 
    
[^92]: 切线模型组合用于集成和持续微调

    Tangent Model Composition for Ensembling and Continual Fine-tuning. (arXiv:2307.08114v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.08114](http://arxiv.org/abs/2307.08114)

    切线模型组合 (TMC) 是一种将独立微调的模型结合的方法，可以用于增量学习、集成和取消学习。通过标量组合方式进行组合，提高了准确率并降低了推理成本。该方法可以零成本忘记组成模型，不受顺序偏差的限制，并能在联合数据上并行执行。在任务增量、类别增量和数据增量设置中，TMC几乎在每个方案上均优于最近发表的持续微调方法。

    

    切线模型组合 (TMC) 是一种将独立微调的组成模型结合在预训练点周围的方法。组成模型是与预训练模型相关的切线向量，可以通过加法、缩放或减法来支持增量学习、集成或取消学习。在推理时，组成模型通过标量组合的方式进行组合，将集成的成本降低到单个模型的水平。与非线性微调模型进行集成相比，TMC提高了4.2%的准确率，并将推理成本降低了2.5倍至10倍，与组成模型的数量呈线性增长。每个组成模型可以以零成本忘记，对推理结果没有剩余影响。当用于持续微调时，TMC不受顺序偏差的限制，并且可以在联合数据上并行执行。在任务增量、类别增量和数据增量设置中，TMC几乎在每个方案上均优于最近发表的持续微调方法。

    Tangent Model Composition (TMC) is a method to combine component models independently fine-tuned around a pre-trained point. Component models are tangent vectors to the pre-trained model that can be added, scaled, or subtracted to support incremental learning, ensembling, or unlearning. Component models are composed at inference time via scalar combination, reducing the cost of ensembling to that of a single model. TMC improves accuracy by 4.2% compared to ensembling non-linearly fine-tuned models at a 2.5x to 10x reduction of inference cost, growing linearly with the number of component models. Each component model can be forgotten at zero cost, with no residual effect on the resulting inference. When used for continual fine-tuning, TMC is not constrained by sequential bias and can be executed in parallel on federated data. TMC outperforms recently published continual fine-tuning methods almost uniformly on each setting -- task-incremental, class-incremental, and data-incremental -- o
    
[^93]: 通过损失函数曲率视角揭示记忆化过程

    Memorization Through the Lens of Curvature of Loss Function Around Samples. (arXiv:2307.05831v1 [cs.LG])

    [http://arxiv.org/abs/2307.05831](http://arxiv.org/abs/2307.05831)

    本研究通过对损失函数曲率进行分析，研究了神经网络在不同样本上的泛化与记忆化特性。我们发现高曲率的样本通常是具有标签错误或冲突的长尾样本，并在CIFAR100数据集上发现了一种新的失败模型。通过对部分样本进行随机标签错误，我们展示了曲率排序可以有效识别出这些样本。

    

    神经网络参数过多，很容易过拟合训练数据。极端情况下，它们可以完全记忆训练集，即使标签是随机的。我们提议使用训练样本周围的损失函数曲率作为记忆化程度的度量，对所有训练轮次进行平均。我们利用这个度量来研究常见图像数据集中不同样本的泛化与记忆化特性。我们可视化具有最高损失曲率的样本，发现它们通常是长尾样本、标签错误或冲突样本。这种分析帮助我们在CIFAR100数据集上发现了一种新的失败模型，即具有不同标签的重复图像。我们还通过随机错误化少量样本的标签来人为地给数据集引入标签错误，并展示了按曲率排序可以高效地识别出标签错误样本的高AUROC值。

    Neural networks are overparametrized and easily overfit the datasets they train on. In the extreme case, it is shown that they can memorize a training set with fully randomized labels. We propose using the curvature of loss function around the training sample as a measure of its memorization, averaged over all training epochs. We use this to study the generalization versus memorization properties of different samples in popular image datasets. We visualize samples with the highest curvature of loss around them, and show that these visually correspond to long-tailed, mislabeled or conflicting samples. This analysis helps us find a, to the best of our knowledge, novel failure model on the CIFAR100 dataset, that of duplicated images with different labels. We also synthetically mislabel a proportion of the dataset by randomly corrupting the labels of a few samples, and show that sorting by curvature yields high AUROC values for identifying the mislabeled samples.
    
[^94]: DADO -- 用于深度主动设计优化的低成本查询策略

    DADO -- Low-Cost Query Strategies for Deep Active Design Optimization. (arXiv:2307.04536v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.04536](http://arxiv.org/abs/2307.04536)

    这项研究将深度主动学习应用于设计优化中，通过预测性能并只模拟有前途的候选模拟，实现了大幅度节约计算资源的目标。通过提出的低成本查询策略，该方法在多目标设计优化问题中取得了显著的改进，并避免了对不确定性估计的要求。在流体动力学领域的实证评估中得到了验证。

    

    在这篇经验报告中，我们将深度主动学习应用于设计优化领域，以减少计算昂贵的数值模拟的数量。我们致力于优化结构组件的设计，其中形状由一组参数描述。如果我们可以基于这些参数预测性能，并仅考虑有前途的候选模拟，就有巨大的节省计算能力的潜力。我们提出了两种自我优化的选择策略，以减少多目标设计优化问题中的计算成本。我们提出的方法提供了一种直观的方法，易于应用，在随机抽样方面具有显著的改进，并且规避了对不确定性估计的需求。我们在流体动力学领域的大型数据集上评估了我们的策略，并引入了两个新的评估指标来确定模型的性能。评估结果突出了我们的策略的有效性

    In this experience report, we apply deep active learning to the field of design optimization to reduce the number of computationally expensive numerical simulations. We are interested in optimizing the design of structural components, where the shape is described by a set of parameters. If we can predict the performance based on these parameters and consider only the promising candidates for simulation, there is an enormous potential for saving computing power. We present two selection strategies for self-optimization to reduce the computational cost in multi-objective design optimization problems. Our proposed methodology provides an intuitive approach that is easy to apply, offers significant improvements over random sampling, and circumvents the need for uncertainty estimation. We evaluate our strategies on a large dataset from the domain of fluid dynamics and introduce two new evaluation metrics to determine the model's performance. Findings from our evaluation highlights the effec
    
[^95]: 自校准分类器引导下，少量标记数据的基于评分的条件生成

    Score-based Conditional Generation with Fewer Labeled Data by Self-calibrating Classifier Guidance. (arXiv:2307.04081v1 [cs.CV])

    [http://arxiv.org/abs/2307.04081](http://arxiv.org/abs/2307.04081)

    本论文提出了一种通过自校准分类器引导的方法改进基于评分的条件生成模型，以提高使用少量标记数据的准确性和性能。通过将分类器作为无条件生成模型的另一种视角，并利用标记和未标记数据来校准分类器，实验证实该方法的有效性。

    

    基于评分的生成模型（SGMs）是一种流行的深度生成模型家族，能够达到领先的图像生成质量。早期的研究已经将SGMs扩展到处理类条件生成，通过将无条件的SGM与经过训练的分类器的引导相结合。然而，这种分类器引导的SGMs在训练数量较少的标记数据时并不总能实现准确的条件生成。我们认为问题根源在于分类器的不可靠梯度和无法充分利用未标记数据。因此，我们提出通过让分类器自校准来改进分类器引导的SGMs。我们的关键思想是使用能量模型的原则将分类器转化为无条件SGM的另一种视角。然后，可以采用现有的无条件SGM损失函数来使用标记和未标记数据来校准分类器。实证结果验证了所提出方法显著改善了条件生成的性能。

    Score-based Generative Models (SGMs) are a popular family of deep generative models that achieves leading image generation quality. Earlier studies have extended SGMs to tackle class-conditional generation by coupling an unconditional SGM with the guidance of a trained classifier. Nevertheless, such classifier-guided SGMs do not always achieve accurate conditional generation, especially when trained with fewer labeled data. We argue that the issue is rooted in unreliable gradients of the classifier and the inability to fully utilize unlabeled data during training. We then propose to improve classifier-guided SGMs by letting the classifier calibrate itself. Our key idea is to use principles from energy-based models to convert the classifier as another view of the unconditional SGM. Then, existing loss for the unconditional SGM can be adopted to calibrate the classifier using both labeled and unlabeled data. Empirical results validate that the proposed approach significantly improves the
    
[^96]: 使用EffUNet和迁移学习方法进行建筑和道路分割

    Building and Road Segmentation Using EffUNet and Transfer Learning Approach. (arXiv:2307.03980v1 [cs.CV])

    [http://arxiv.org/abs/2307.03980](http://arxiv.org/abs/2307.03980)

    本论文提出了一种使用EffUNet和迁移学习方法进行建筑和道路分割的新架构。利用这种方法，在马萨诸塞州建筑物和道路数据集上取得了令人满意的分数。

    

    在城市中，了解城市对象（如供水、铁路线、电力线路、建筑物、道路等）的信息对城市规划是必要的。特别是，政策制定者需要了解这些对象的分布、位置和容量，以做出有影响力的决策。本论文旨在通过卫星和无人机拍摄的航空图像对建筑物和道路进行分割。许多不同的架构已经被提出用于语义分割任务，其中UNet是其中之一。在本论文中，我们提出了一种基于谷歌新提出的EfficientNetV2作为特征提取的编码器和UNet解码器构建分割图的新架构。使用这种方法，我们在马萨诸塞州建筑物和道路数据集上实现了基准分数，分别为0.8365和0.9153的mIOU。

    In city, information about urban objects such as water supply, railway lines, power lines, buildings, roads, etc., is necessary for city planning. In particular, information about the spread of these objects, locations and capacity is needed for the policymakers to make impactful decisions. This thesis aims to segment the building and roads from the aerial image captured by the satellites and UAVs. Many different architectures have been proposed for the semantic segmentation task and UNet being one of them. In this thesis, we propose a novel architecture based on Google's newly proposed EfficientNetV2 as an encoder for feature extraction with UNet decoder for constructing the segmentation map. Using this approach we achieved a benchmark score for the Massachusetts Building and Road dataset with an mIOU of 0.8365 and 0.9153 respectively.
    
[^97]: 使用激光和机器学习模型进行钢铁表面粗糙度参数计算

    Steel Surface Roughness Parameter Calculations Using Lasers and Machine Learning Models. (arXiv:2307.03723v1 [cs.LG])

    [http://arxiv.org/abs/2307.03723](http://arxiv.org/abs/2307.03723)

    本研究利用机器学习模型提高了钢铁表面粗糙度参数的计算准确性，通过对比不同方法，评估了在线测量转化的潜力。

    

    控制带钢表面纹理对于满足镀锌和轧制工艺中客户要求至关重要。传统方法依赖于生产后的测针测量，而在线技术则提供了对整个带钢进行非接触和实时测量的能力。然而，确保准确测量对于其在制造流程中的有效利用至关重要。此外，准确的在线测量使得在生产过程中能够实时调整制造加工参数，确保产品质量的一致性和轧机的闭环控制的可能性。本研究利用最先进的机器学习模型改进在线测量转化为更准确的Ra表面粗糙度指标。通过比较包括深度学习和非深度学习方法在内的一系列数据驱动方法与闭合形式转化，我们评估了它们提高表面粗糙度参数计算的潜力。

    Control of surface texture in strip steel is essential to meet customer requirements during galvanizing and temper rolling processes. Traditional methods rely on post-production stylus measurements, while on-line techniques offer non-contact and real-time measurements of the entire strip. However, ensuring accurate measurement is imperative for their effective utilization in the manufacturing pipeline. Moreover, accurate on-line measurements enable real-time adjustments of manufacturing processing parameters during production, ensuring consistent quality and the possibility of closed-loop control of the temper mill. In this study, we leverage state-of-the-art machine learning models to enhance the transformation of on-line measurements into significantly a more accurate Ra surface roughness metric. By comparing a selection of data-driven approaches, including both deep learning and non-deep learning methods, to the close-form transformation, we evaluate their potential for improving su
    
[^98]: 无等渗性的蒙特卡洛采样：一种逆扩散方法

    Monte Carlo Sampling without Isoperimetry: A Reverse Diffusion Approach. (arXiv:2307.02037v1 [stat.ML])

    [http://arxiv.org/abs/2307.02037](http://arxiv.org/abs/2307.02037)

    本研究提出了一种无等渗性的蒙特卡洛采样方法，通过逆扩散过程实现了新颖的后验采样算法，在高维采样中表现出更优越的性能。

    

    现代生成模型的有效性通常取决于扩散路径上得分估计的精度，重点关注扩散模型及其生成高质量数据样本的能力。本研究深入探讨了通过逆扩散进行后验采样的潜力。通过对采样文献进行研究，发现可以通过转移核的分解将得分估计转化为均值估计问题。通过估计辅助分布的均值，逆扩散过程可以产生一种新颖的后验采样算法，该算法与传统的基于梯度的马尔科夫链蒙特卡洛（MCMC）方法不同。我们提供了总变差距离下的收敛分析，并证明了所提算法的等渗性依赖性相对较低，比传统的MCMC技术表现出更高的高维采样性能。

    The efficacy of modern generative models is commonly contingent upon the precision of score estimation along the diffusion path, with a focus on diffusion models and their ability to generate high-quality data samples. This study delves into the potentialities of posterior sampling through reverse diffusion. An examination of the sampling literature reveals that score estimation can be transformed into a mean estimation problem via the decomposition of the transition kernel. By estimating the mean of the auxiliary distribution, the reverse diffusion process can give rise to a novel posterior sampling algorithm, which diverges from traditional gradient-based Markov Chain Monte Carlo (MCMC) methods. We provide the convergence analysis in total variation distance and demonstrate that the isoperimetric dependency of the proposed algorithm is comparatively lower than that observed in conventional MCMC techniques, which justifies the superior performance for high dimensional sampling with er
    
[^99]: 在高峰小时序列预测中缩小性能差距: Seq2Peak框架

    Bridge the Performance Gap in Peak-hour Series Forecasting: The Seq2Peak Framework. (arXiv:2307.01597v1 [cs.LG])

    [http://arxiv.org/abs/2307.01597](http://arxiv.org/abs/2307.01597)

    本文提出了Seq2Peak框架，针对高峰小时序列预测任务，该框架通过解决高度非平稳性和性能评估问题，成功缩小了在常规时间序列预测模型中观察到的性能差距。

    

    高峰小时序列预测（PHSF）是各个领域中一个重要但未被充分探索的任务。虽然最先进的深度学习模型在常规时间序列预测（TSF）中表现出色，但在PHSF中却难以达到可比较的结果。这可能归因于高峰小时序列中高度非平稳性的挑战，使得直接预测比标准的TSF更加困难。此外，手动从常规预测结果中提取最大值会导致性能不佳，因为模型会最小化平均差。为了解决这些问题，本文提出了Seq2Peak，一个专为PHSF任务而设计的新颖框架，以弥合在TSF模型中观察到的性能差距。Seq2Peak具有两个关键组件：CyclicNorm流程来减轻非平稳性问题，以及一个简单而有效的可训练参数自由峰值小时解码器，采用混合损失函数来利用原始序列和高峰小时序列作为监督信号。

    Peak-Hour Series Forecasting (PHSF) is a crucial yet underexplored task in various domains. While state-of-the-art deep learning models excel in regular Time Series Forecasting (TSF), they struggle to achieve comparable results in PHSF. This can be attributed to the challenges posed by the high degree of non-stationarity in peak-hour series, which makes direct forecasting more difficult than standard TSF. Additionally, manually extracting the maximum value from regular forecasting results leads to suboptimal performance due to models minimizing the mean deficit. To address these issues, this paper presents Seq2Peak, a novel framework designed specifically for PHSF tasks, bridging the performance gap observed in TSF models. Seq2Peak offers two key components: the CyclicNorm pipeline to mitigate the non-stationarity issue, and a simple yet effective trainable-parameter-free peak-hour decoder with a hybrid loss function that utilizes both the original series and peak-hour series as superv
    
[^100]: 复杂数据集的底层缩放定律和普适统计结构

    The Underlying Scaling Laws and Universal Statistical Structure of Complex Datasets. (arXiv:2306.14975v1 [cs.LG])

    [http://arxiv.org/abs/2306.14975](http://arxiv.org/abs/2306.14975)

    本文研究了复杂数据集中的底层缩放定律和普适统计结构。通过将数据类比为物理系统，并应用统计物理学和随机矩阵理论的方法，揭示了特征-特征协方差矩阵的局部和全局特征值统计量的规律。研究发现，在无关随机数据和真实数据之间存在显著差异，并且可以通过引入长程相关性完全恢复缩放行为。同时，生成的数据和真实世界数据都属于混沌系统，并在较小的数据集大小上即可体现随机矩阵理论的统计行为。

    

    我们研究了在真实世界的复杂数据集和人工生成的数据集中都出现的普遍特征。我们将数据类比为物理系统，并利用统计物理学和随机矩阵理论的工具揭示其底层结构。我们重点分析了特征-特征协方差矩阵，分析了其局部和全局特征值统计量。我们的主要观察结果是：(i) 大部分特征值呈现的幂律缩放在无相关随机数据和真实数据之间存在显著差异，(ii) 通过简单地引入长程相关性，可以完全恢复这种缩放行为到合成数据中，(iii) 从随机矩阵理论的角度看，生成的数据集和真实世界数据集属于同一个普适性类别，都是混沌系统而非可积系统，(iv) 预期的随机矩阵理论统计行为在相对较小的数据集大小上就已经在经验协方差矩阵中得到体现。

    We study universal traits which emerge both in real-world complex datasets, as well as in artificially generated ones. Our approach is to analogize data to a physical system and employ tools from statistical physics and Random Matrix Theory (RMT) to reveal their underlying structure. We focus on the feature-feature covariance matrix, analyzing both its local and global eigenvalue statistics. Our main observations are: (i) The power-law scalings that the bulk of its eigenvalues exhibit are vastly different for uncorrelated random data compared to real-world data, (ii) this scaling behavior can be completely recovered by introducing long range correlations in a simple way to the synthetic data, (iii) both generated and real-world datasets lie in the same universality class from the RMT perspective, as chaotic rather than integrable systems, (iv) the expected RMT statistical behavior already manifests for empirical covariance matrices at dataset sizes significantly smaller than those conv
    
[^101]: MLP的规模化：归纳偏差的故事

    Scaling MLPs: A Tale of Inductive Bias. (arXiv:2306.13575v1 [cs.LG])

    [http://arxiv.org/abs/2306.13575](http://arxiv.org/abs/2306.13575)

    本文研究了多层感知器（MLP）在视觉任务中的性能极限，并探讨了MLP相较于其他深度学习模型的归纳偏差，旨在推进深度学习理论和实践的结合。

    

    在此工作中，我们重新审视了深度学习中最基本的构建块——多层感知器（MLP），并研究了它在视觉任务中的性能极限。MLP的实验性洞见在多个方面都非常重要。

    In this work we revisit the most fundamental building block in deep learning, the multi-layer perceptron (MLP), and study the limits of its performance on vision tasks. Empirical insights into MLPs are important for multiple reasons. (1) Given the recent narrative "less inductive bias is better", popularized due to transformers eclipsing convolutional models, it is natural to explore the limits of this hypothesis. To that end, MLPs offer an ideal test bed, being completely free of any inductive bias. (2) MLPs have almost exclusively been the main protagonist in the deep learning theory literature due to their mathematical simplicity, serving as a proxy to explain empirical phenomena observed for more complex architectures. Surprisingly, experimental datapoints for MLPs are very difficult to find in the literature, especially when coupled with large pre-training protocols. This discrepancy between practice and theory is worrying: Do MLPs reflect the empirical advances exhibited by pract
    
[^102]: 推荐系统的数据增广：一种基于最大边际矩阵分解的半监督方法

    Data augmentation for recommender system: A semi-supervised approach using maximum margin matrix factorization. (arXiv:2306.13050v1 [cs.IR])

    [http://arxiv.org/abs/2306.13050](http://arxiv.org/abs/2306.13050)

    本研究提出了一种基于最大边际矩阵分解的半监督方法来增广和细化协同过滤算法的评级预测。该方法利用自我训练来评估评分的置信度，并通过系统的数据增广策略来提高算法性能。

    

    协同过滤已成为推荐系统开发的常用方法，其中，根据用户的过去喜好和其他用户的可用偏好信息预测其对新物品的评分。尽管CF方法很受欢迎，但其性能通常受观察到的条目的稀疏性的极大限制。本研究探讨最大边际矩阵分解（MMMF）的数据增广和细化方面，该方法是广泛接受的用于评级预测的CF技术，之前尚未进行研究。我们利用CF算法的固有特性来评估单个评分的置信度，并提出了一种基于自我训练的半监督评级增强方法。我们假设任何CF算法的预测低置信度是由于训练数据的某些不足，因此，通过采用系统的数据增广策略，可以提高算法的性能。

    Collaborative filtering (CF) has become a popular method for developing recommender systems (RS) where ratings of a user for new items is predicted based on her past preferences and available preference information of other users. Despite the popularity of CF-based methods, their performance is often greatly limited by the sparsity of observed entries. In this study, we explore the data augmentation and refinement aspects of Maximum Margin Matrix Factorization (MMMF), a widely accepted CF technique for the rating predictions, which have not been investigated before. We exploit the inherent characteristics of CF algorithms to assess the confidence level of individual ratings and propose a semi-supervised approach for rating augmentation based on self-training. We hypothesize that any CF algorithm's predictions with low confidence are due to some deficiency in the training data and hence, the performance of the algorithm can be improved by adopting a systematic data augmentation strategy
    
[^103]: SPRINT：通过语言指令 relabeling 实现可扩展的策略预训练

    SPRINT: Scalable Policy Pre-Training via Language Instruction Relabeling. (arXiv:2306.11886v1 [cs.RO])

    [http://arxiv.org/abs/2306.11886](http://arxiv.org/abs/2306.11886)

    SPRINT 提出了一种离线策略预训练方法，通过指令重标记及离线强化学习实现可扩展的预训练任务，大大减少了预训练所需的人力，同时使机器人能够获取更丰富的技能库，相较于之前的预训练方法，能够更快地学习新的长时间跨度任务。

    

    预训练机器人策略并赋予丰富的技能集合可以大大加速下游任务的学习。先前的研究通过自然语言指令定义预训练任务，但这需要人为地注释数十万个指令。因此，我们提出了 SPRINT，这是一种可扩展的离线策略预训练方法，可大大减少预训练多样的技能所需的人力。我们的方法使用两个核心想法来自动扩展基础预训练任务：通过大型语言模型来进行指令重标记和通过离线强化学习进行交叉轨迹技能链接。因此，SPRINT 预训练可以为机器人装备更丰富的技能库。在家庭模拟器和真实机器人厨房操作任务中的实验结果表明，SPRINT 相对于之前的预训练方法能够更快地学习新的长时间跨度任务。

    Pre-training robot policies with a rich set of skills can substantially accelerate the learning of downstream tasks. Prior works have defined pre-training tasks via natural language instructions, but doing so requires tedious human annotation of hundreds of thousands of instructions. Thus, we propose SPRINT, a scalable offline policy pre-training approach which substantially reduces the human effort needed for pre-training a diverse set of skills. Our method uses two core ideas to automatically expand a base set of pre-training tasks: instruction relabeling via large language models and cross-trajectory skill chaining through offline reinforcement learning. As a result, SPRINT pre-training equips robots with a much richer repertoire of skills. Experimental results in a household simulator and on a real robot kitchen manipulation task show that SPRINT leads to substantially faster learning of new long-horizon tasks than previous pre-training approaches. Website at https://clvrai.com/spr
    
[^104]: 教科书是你需要的全部。 (arXiv:2306.11644v2 [cs.CL] UPDATED)

    Textbooks Are All You Need. (arXiv:2306.11644v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.11644](http://arxiv.org/abs/2306.11644)

    phi-1是一个新的大型代码语言模型，通过精心训练和优化，尽管规模相对较小，但在准确率和新的性质方面表现出了令人惊讶的结果。

    

    我们介绍了一个新的大型代码语言模型phi-1，其体积明显小于竞争模型：phi-1是一个基于Transformer的模型，拥有13亿个参数，在8个A100上进行了4天的训练，使用了来自网络的“教科书质量”数据（60亿个标记）和使用GPT-3.5合成生成的教科书和练习（10亿个标记）。尽管规模小，phi-1在HumanEval上的pass@1准确率为50.6％，在MBPP上为55.5％。与我们在编码练习数据集上进行微调之前的模型 phi-1-base 和具有相同流程的350M参数的较小模型 phi-1-small 相比，它还展现了令人惊讶的新的性质，phi-1-small 在 HumanEval 上仍达到45％的准确率。

    We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.
    
[^105]: 基于人工智能的准实时定位边际定价方法：可行性和稳健性研究

    AI Driven Near Real-time Locational Marginal Pricing Method: A Feasibility and Robustness Study. (arXiv:2306.10080v2 [cs.CE] UPDATED)

    [http://arxiv.org/abs/2306.10080](http://arxiv.org/abs/2306.10080)

    本研究评估了基于机器学习和深度学习的模型在预测定位边际定价（LMP）时的性能，并发现这些模型在多个电力网络上具有准确且稳健的预测能力。

    

    准确的价格预测对于市场参与者来说至关重要，以优化他们的操作计划和竞标策略，特别是在当前情况下，使用经典方法预测电力价格变得更加波动和不可预测。定位边际定价（LMP）定价机制在许多现代电力市场中使用，其中传统方法利用最优潮流（OPF）求解器。然而，对于大型电力网络，这个过程变得耗时且计算密集。基于机器学习（ML）的预测可以为LMP预测提供高效工具，特别是在具有可再生能源等间歇性来源的能源市场中。本研究评估了流行的机器学习和深度学习模型在多个电力网络上预测LMP的性能。考虑多种情景评估了这些模型在预测LMP方面的准确性和稳健性。结果表明，ML模型可以提供准确且稳健的LMP预测。

    Accurate price predictions are essential for market participants in order to optimize their operational schedules and bidding strategies, especially in the current context where electricity prices become more volatile and less predictable using classical approaches. The Locational Marginal Pricing (LMP) pricing mechanism is used in many modern power markets, where the traditional approach utilizes optimal power flow (OPF) solvers. However, for large electricity grids this process becomes prohibitively time-consuming and computationally intensive. Machine learning (ML) based predictions could provide an efficient tool for LMP prediction, especially in energy markets with intermittent sources like renewable energy. This study evaluates the performance of popular machine learning and deep learning models in predicting LMP on multiple electricity grids. The accuracy and robustness of these models in predicting LMP is assessed considering multiple scenarios. The results show that ML models 
    
[^106]: MUBen：评估分子性质预测预训练模型的不确定性基准

    MUBen: Benchmarking the Uncertainty of Pre-Trained Models for Molecular Property Prediction. (arXiv:2306.10060v1 [physics.chem-ph])

    [http://arxiv.org/abs/2306.10060](http://arxiv.org/abs/2306.10060)

    MUBen评估不同骨干和UQ模型组合对分子不确定性估计和属性预测的性能，以解决预训练模型微调中的过拟合校准问题。

    

    预训练于大规模无标签分子数据的大型Transformer模型在预测分子性质方面取得了巨大成功。然而，在微调期间，这些模型可能容易出现过拟合，导致对测试数据的过度自信预测落在了训练分布之外。为了解决这个问题，可以使用不确定性量化（UQ）方法来改善模型的预测校准。虽然存在许多UQ方法，但并不是所有方法都会导致性能改善。虽然一些研究使用UQ来改善分子预训练模型，但选择适合的骨干和UQ方法以可靠地估计分子不确定性的过程仍然是未经探索的。为了解决这个差距，我们提出了MUBen，评估不同的骨干和UQ模型组合，以量化它们在属性预测和不确定性估计方面的性能。通过微调使用不同分子描述符的各种骨干分子表示模型。

    Large Transformer models pre-trained on massive unlabeled molecular data have shown great success in predicting molecular properties. However, these models can be prone to overfitting during fine-tuning, resulting in over-confident predictions on test data that fall outside of the training distribution. To address this issue, uncertainty quantification (UQ) methods can be used to improve the models' calibration of predictions. Although many UQ approaches exist, not all of them lead to improved performance. While some studies have used UQ to improve molecular pre-trained models, the process of selecting suitable backbone and UQ methods for reliable molecular uncertainty estimation remains underexplored. To address this gap, we present MUBen, which evaluates different combinations of backbone and UQ models to quantify their performance for both property prediction and uncertainty estimation. By fine-tuning various backbone molecular representation models using different molecular descrip
    
[^107]: 面向扩散式生成模型的非渐进快速收敛方法

    Towards Faster Non-Asymptotic Convergence for Diffusion-Based Generative Models. (arXiv:2306.09251v1 [stat.ML])

    [http://arxiv.org/abs/2306.09251](http://arxiv.org/abs/2306.09251)

    该论文针对扩散生成模型设计了非渐进理论，提出了针对两种主流采样器的新的收敛速度，提高了总步数与收敛速度的比例。

    

    扩散模型通过学习反转马尔可夫扩散过程将噪音转化为新数据实例，在当代生成建模领域中已成为基石。虽然它们的实用性现在已被广泛认可，但其理论基础仍然不够成熟。在这项工作中，我们开发了一套非渐进理论，以理解离散时间下扩散模型的数据生成过程，假设可以获得（Stein）得分函数的可靠估计。针对一种流行的确定性采样器（基于概率流ODE），我们建立了一个与 $T$（总步数）成比例的收敛速度，改进了过去的结果；对于另一种主流的随机采样器（即一种去噪扩散概率模型（DDPM）），我们导出了一个与 $1/\sqrt{T}$ 成比例的收敛速度，与最先进的理论相匹配。我们的理论对目标数据分布只作出最小的假设（例如，没有平滑）。

    Diffusion models, which convert noise into new data instances by learning to reverse a Markov diffusion process, have become a cornerstone in contemporary generative modeling. While their practical power has now been widely recognized, the theoretical underpinnings remain far from mature. In this work, we develop a suite of non-asymptotic theory towards understanding the data generation process of diffusion models in discrete time, assuming access to reliable estimates of the (Stein) score functions. For a popular deterministic sampler (based on the probability flow ODE), we establish a convergence rate proportional to $1/T$ (with $T$ the total number of steps), improving upon past results; for another mainstream stochastic sampler (i.e., a type of the denoising diffusion probabilistic model (DDPM)), we derive a convergence rate proportional to $1/\sqrt{T}$, matching the state-of-the-art theory. Our theory imposes only minimal assumptions on the target data distribution (e.g., no smoot
    
[^108]: 噪声稳定优化对于具有最优收敛率的平坦极小值的影响

    Noise Stability Optimization for Flat Minima with Optimal Convergence Rates. (arXiv:2306.08553v1 [cs.LG])

    [http://arxiv.org/abs/2306.08553](http://arxiv.org/abs/2306.08553)

    本文提出了一个SGD-like算法，注入随机噪声并利用分布对称性来减少方差，以寻找具有低海森矩阵迹的平坦极小值，同时提供了收敛速率分析。

    

    本文研究通过加入加权扰动来找到平坦的极小值。给定一个非凸函数$f:\mathbb{R}^d\rightarrow \mathbb{R}$和一个$d$维分布$\mathcal{P}$，我们扰动$f$的权重，并定义$F(W)=\mathbb{E}[f({W+U})]$，其中$U$是一个从$\mathcal{P}$中随机抽取的样本。这个过程通过$f$的海森矩阵的迹来诱导正则化，以适应于小的、各向同性的高斯扰动。因此，加权扰动的函数偏向于带有低海森矩阵迹的极小值。本文提出了一种类似于SGD的算法，在计算梯度之前注入随机噪声，同时利用$\mathcal{P}$的对称性来减少方差。我们还提供了严格的分析，证明了...

    We consider finding flat, local minimizers by adding average weight perturbations. Given a nonconvex function $f: \mathbb{R}^d \rightarrow \mathbb{R}$ and a $d$-dimensional distribution $\mathcal{P}$ which is symmetric at zero, we perturb the weight of $f$ and define $F(W) = \mathbb{E}[f({W + U})]$, where $U$ is a random sample from $\mathcal{P}$. This injection induces regularization through the Hessian trace of $f$ for small, isotropic Gaussian perturbations. Thus, the weight-perturbed function biases to minimizers with low Hessian trace. Several prior works have studied settings related to this weight-perturbed function by designing algorithms to improve generalization. Still, convergence rates are not known for finding minima under the average perturbations of the function $F$. This paper considers an SGD-like algorithm that injects random noise before computing gradients while leveraging the symmetry of $\mathcal{P}$ to reduce variance. We then provide a rigorous analysis, showing
    
[^109]: 一种用于自闭症干预分析的多模态数据集MMASD。

    MMASD: A Multimodal Dataset for Autism Intervention Analysis. (arXiv:2306.08243v1 [cs.CV])

    [http://arxiv.org/abs/2306.08243](http://arxiv.org/abs/2306.08243)

    提出了一个名为MMASD的自闭症多模态数据集，收集自治疗干预。它包括从32名自闭症患儿的干预录音中分段的1,315个数据样本，每个样本包含四种隐私保护模式的数据。

    

    自闭症谱系障碍（ASD）是一种发育性疾病，其特征是重大的社交沟通障碍和困难的知觉和表达沟通提示。机器学习技术已广泛应用于促进自闭症研究和评估。然而，计算模型主要集中于特定分析，并在自闭症社区的私有数据集上进行验证，这限制了由于数据共享复杂性而跨模型的比较。本研究提出了一种新的隐私保护开源数据集MMASD作为多模式ASD基准数据集，收集自患有自闭症儿童的游戏治疗干预。MMASD包括32名患有ASD的儿童的数据，以及从100多小时的干预录音中分段的1,315个数据样本。为促进公共访问，每个数据样本包含四种隐私保护模式的数据：（1）光流，（2）2D骨架，（3）3D骨架和（4）临床医生ASD评估。

    Autism spectrum disorder (ASD) is a developmental disorder characterized by significant social communication impairments and difficulties perceiving and presenting communication cues. Machine learning techniques have been broadly adopted to facilitate autism studies and assessments. However, computational models are primarily concentrated on specific analysis and validated on private datasets in the autism community, which limits comparisons across models due to privacy-preserving data sharing complications. This work presents a novel privacy-preserving open-source dataset, MMASD as a MultiModal ASD benchmark dataset, collected from play therapy interventions of children with Autism. MMASD includes data from 32 children with ASD, and 1,315 data samples segmented from over 100 hours of intervention recordings. To promote public access, each data sample consists of four privacy-preserving modalities of data: (1) optical flow, (2) 2D skeleton, (3) 3D skeleton, and (4) clinician ASD evalua
    
[^110]: Mol-Instructions: 一个大规模生物分子指令数据集，为大语言模型提供支持

    Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models. (arXiv:2306.08018v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.08018](http://arxiv.org/abs/2306.08018)

    Mol-Instructions是一个专门为生物分子领域设计的综合指令数据集，可以显著提高大语言模型在生物领域中的适应能力和认知敏锐度。

    

    大语言模型（LLM）以其卓越的任务处理能力和创新的输出，在许多领域推动了重大进展。然而，它们在生物分子研究等专业领域的熟练应用还受到限制。为了解决这个挑战，我们介绍了Mol-Instructions，这是一个经过精心策划、专门针对生物分子领域设计的综合指令数据集。Mol-Instructions由三个关键组成部分组成：分子导向指令、蛋白质导向指令和生物分子文本指令，每个部分都被策划用于增强LLM对生物分子特性和行为的理解和预测能力。通过对代表性LLM的广泛指令调整实验，我们强调了Mol-Instructions在增强大模型在生物分子研究复杂领域内的适应能力和认知敏锐度方面的潜力，从而促进生物分子领域的进一步发展。

    Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a meticulously curated, comprehensive instruction dataset expressly designed for the biomolecular realm. Mol-Instructions is composed of three pivotal components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions, each curated to enhance the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on the representative LLM, we underscore the potency of Mol-Instructions to enhance the adaptability and cognitive acuity of large models within the complex sphere of biomolecular studies, thereby promoting advancements in the biomol
    
[^111]: 变分不平衡回归(Variational Imbalanced Regression)

    Variational Imbalanced Regression. (arXiv:2306.06599v1 [cs.LG])

    [http://arxiv.org/abs/2306.06599](http://arxiv.org/abs/2306.06599)

    本文提出的变分不平衡回归（VIR）模型通过引入Probabilistic Reweighting方法，可以在不平衡回归方面表现良好，并自然产生合理的不确定性估计。

    

    当标签分布不平衡时，现有的回归模型往往在准确性和不确定性估计方面表现不佳。本文提出了一种概率深度学习模型——变分不平衡回归（VIR），它不仅在不平衡回归方面表现出色，而且自然地产生合理的不确定性估计。与典型的变分自编码器假设I.I.D.表示（数据点的表示不直接受其他数据点的影响）不同，我们的VIR借用具有类似回归标签的数据来计算潜在表示的变分分布；此外，不同于产生点估计的确定性回归模型， VIR预测整个正态反-伽玛分布并调节相关联的共轭分布，对不平衡数据施加概率重新加权，从而提供更好的不确定性估计。在几个真实世界的数据集上进行了实验。

    Existing regression models tend to fall short in both accuracy and uncertainty estimation when the label distribution is imbalanced. In this paper, we propose a probabilistic deep learning model, dubbed variational imbalanced regression (VIR), which not only performs well in imbalanced regression but naturally produces reasonable uncertainty estimation as a byproduct. Different from typical variational autoencoders assuming I.I.D. representations (a data point's representation is not directly affected by other data points), our VIR borrows data with similar regression labels to compute the latent representation's variational distribution; furthermore, different from deterministic regression models producing point estimates, VIR predicts the entire normal-inverse-gamma distributions and modulates the associated conjugate distributions to impose probabilistic reweighting on the imbalanced data, thereby providing better uncertainty estimation. Experiments in several real-world datasets sh
    
[^112]: Push: 并发概率编程用于贝叶斯深度学习

    Push: Concurrent Probabilistic Programming for Bayesian Deep Learning. (arXiv:2306.06528v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.06528](http://arxiv.org/abs/2306.06528)

    Push是一个并发概率编程库，用于贝叶斯深度学习（BDL），可以在多GPU硬件上执行BDL推理算法。该库通过将神经网络表示为粒子，并允许粒子之间的异步通信和各种参数更新，简化了BDL实验和扩展粒子操作的过程。

    

    我们介绍了一个名为Push的库，采用概率编程的方法来进行贝叶斯深度学习（BDL）。该库可在多GPU硬件上并发执行BDL推理算法，用于神经网络（NN）模型。为实现这一目标，Push引入了一种抽象，将输入NN表示为一个粒子。Push使得创建粒子变得容易，以便于复制和粒子之间的异步通信，以实现各种参数更新，包括常见的BDL算法。我们希望通过Push降低进行BDL实验的门槛，通过简化在多GPU上扩展粒子的操作。我们评估了利用单节点多GPU设备进行视觉和科学机器学习（SciML）任务时的粒子扩展行为。

    We introduce a library called Push that takes a probabilistic programming approach to Bayesian deep learning (BDL). This library enables concurrent execution of BDL inference algorithms on multi-GPU hardware for neural network (NN) models. To accomplish this, Push introduces an abstraction that represents an input NN as a particle. Push enables easy creation of particles so that an input NN can be replicated and particles can communicate asynchronously so that a variety of parameter updates can be expressed, including common BDL algorithms. Our hope is that Push lowers the barrier to experimenting with BDL by streamlining the scaling of particles across GPUs. We evaluate the scaling behavior of particles on single-node multi-GPU devices on vision and scientific machine learning (SciML) tasks.
    
[^113]: 通过浅层ReLU网络学习神经元：对相关输入的动态和隐式偏差的探索

    Learning a Neuron by a Shallow ReLU Network: Dynamics and Implicit Bias for Correlated Inputs. (arXiv:2306.06479v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.06479](http://arxiv.org/abs/2306.06479)

    这项研究证明，通过梯度流训练一个宽度任意的一层ReLU网络，可以学习一个单个神经元并收敛到零误差，同时隐式偏向于最小化网络参数的秩。这对于相关的训练数据点是有效的，与之前研究正交数据集的结果补充了彼此。

    

    我们证明了对于学习单个神经元的基本回归任务，通过从小的初始值梯度流训练任意宽度的一层隐藏层ReLU网络会收敛到零误差，并且在隐式上偏向于最小化网络参数的秩。假设训练点与教师神经元相关，我们补充了先前考虑正交数据集的研究结果。我们的结果基于对训练过程中每个隐藏神经元动态的详细非渐进性分析。我们还展示并表征了这种情况下最小秩内插网络与最小欧几里德范数的令人惊讶的区别。最后，我们进行了一系列数值实验，以验证我们的理论发现。

    We prove that, for the fundamental regression task of learning a single neuron, training a one-hidden layer ReLU network of any width by gradient flow from a small initialisation converges to zero loss and is implicitly biased to minimise the rank of network parameters. By assuming that the training points are correlated with the teacher neuron, we complement previous work that considered orthogonal datasets. Our results are based on a detailed non-asymptotic analysis of the dynamics of each hidden neuron throughout the training. We also show and characterise a surprising distinction in this setting between interpolator networks of minimal rank and those of minimal Euclidean norm. Finally we perform a range of numerical experiments, which corroborate our theoretical findings.
    
[^114]: 利用Transformer网络重建钢琴演奏中人类表现力

    Reconstructing Human Expressiveness in Piano Performances with a Transformer Network. (arXiv:2306.06040v1 [cs.SD])

    [http://arxiv.org/abs/2306.06040](http://arxiv.org/abs/2306.06040)

    本文提出了一种利用Transformer网络来重建钢琴演奏中人类表现力的方法，使用转录乐谱来训练模型，整合钢琴家身份以控制采样过程，并成功生成了高度类人的钢琴表演。

    

    利用计算方法捕捉人类音乐演奏中复杂微妙的表现力变化是一项具有挑战性的任务。本文提出了一种新颖的方法，利用多层双向Transformer编码器重建钢琴演奏中的人类表现力。为了解决训练神经网络需要大量准确捕捉和得分对齐的演奏数据的需求，我们使用从现有转录模型获取的转录乐谱来训练我们的模型。我们整合了钢琴家身份以控制采样过程，并探讨了我们的系统模拟不同钢琴家表现力变化的能力。通过生成的表现力演奏的统计分析和听力测试对系统进行评估。总体而言，结果表明我们的方法在从转录的乐谱中生成类人钢琴演奏方面达到了最先进的水平，同时充分和一致地重建了人类表现力。

    Capturing intricate and subtle variations in human expressiveness in music performance using computational approaches is challenging. In this paper, we propose a novel approach for reconstructing human expressiveness in piano performance with a multi-layer bi-directional Transformer encoder. To address the needs for large amounts of accurately captured and score-aligned performance data in training neural networks, we use transcribed scores obtained from an existing transcription model to train our model. We integrate pianist identities to control the sampling process and explore the ability of our system to model variations in expressiveness for different pianists. The system is evaluated through statistical analysis of generated expressive performances and a listening test. Overall, the results suggest that our method achieves state-of-the-art in generating human-like piano performances from transcribed scores, while fully and consistently reconstructing human expressiveness poses fu
    
[^115]: 通过数据增强提升AI攻击性代码生成器的鲁棒性

    Enhancing Robustness of AI Offensive Code Generators via Data Augmentation. (arXiv:2306.05079v1 [cs.LG])

    [http://arxiv.org/abs/2306.05079](http://arxiv.org/abs/2306.05079)

    本论文提出了一种方法，通过在代码描述中引入扰动来增强AI攻击性代码生成器的鲁棒性，并证明数据增强可有效提高代码生成器对扰动和非扰动的代码描述的性能。

    

    本研究提出了一种将扰动添加到安全性代码上下文中的代码描述中的方法，即来自善意开发者的自然语言输入（NL），并分析了扰动如何以及在什么程度上影响AI攻击性代码生成器的性能。我们的实验表明，NL描述中的扰动高度影响代码生成器的性能。为了增强代码生成器的鲁棒性，我们使用该方法执行数据增强，即增加训练数据的变异性和多样性，并证明其对扰动和非扰动的代码描述的有效性。

    In this work, we present a method to add perturbations to the code descriptions, i.e., new inputs in natural language (NL) from well-intentioned developers, in the context of security-oriented code, and analyze how and to what extent perturbations affect the performance of AI offensive code generators. Our experiments show that the performance of the code generators is highly affected by perturbations in the NL descriptions. To enhance the robustness of the code generators, we use the method to perform data augmentation, i.e., to increase the variability and diversity of the training data, proving its effectiveness against both perturbed and non-perturbed code descriptions.
    
[^116]: 可扩展自适应的基于日志的异常检测，并带有专家反馈

    Scalable and Adaptive Log-based Anomaly Detection with Expert in the Loop. (arXiv:2306.05032v1 [cs.SE])

    [http://arxiv.org/abs/2306.05032](http://arxiv.org/abs/2306.05032)

    本文提出了一种适用于大规模云系统的准确、轻量级、自适应的基于日志的异常检测框架——SeaLog。该方法利用一种基于 Trie 结构的动态增长检测代理，可以接收人类专家反馈，能够在不断变化的日志数据中实现高准确度的实时异常检测，从而减少了手动验证的工作量。

    

    系统日志在维护软件系统可靠性方面起着关键作用。已有研究探索了自动基于日志的异常检测，并在基准数据集上取得了显著的准确性。然而，当应用于大规模云系统时，这些解决方案由于高资源消耗和缺乏对不断变化的日志数据的适应性而面临限制。因此，本文提出了一种准确，轻量级和自适应的基于日志的异常检测框架——SeaLog。我们的方法引入了一种基于 Trie 的检测代理 (TDA)，它利用一种轻量级、动态增长的 Trie 结构进行实时异常检测。为了增强 TDA 对不断变化的日志数据的准确性，我们使其能够从专家获得反馈。有趣的是，我们的研究发现，现代的大语言模型，如 ChatGPT，可以提供与人类专家相当一致性的反馈，这可能有助于减少手动验证的工作量。我们对该方法进行了全面的实验评估，并证明了其在大规模环境下的可扩展性和高效性。

    System logs play a critical role in maintaining the reliability of software systems. Fruitful studies have explored automatic log-based anomaly detection and achieved notable accuracy on benchmark datasets. However, when applied to large-scale cloud systems, these solutions face limitations due to high resource consumption and lack of adaptability to evolving logs. In this paper, we present an accurate, lightweight, and adaptive log-based anomaly detection framework, referred to as SeaLog. Our method introduces a Trie-based Detection Agent (TDA) that employs a lightweight, dynamically-growing trie structure for real-time anomaly detection. To enhance TDA's accuracy in response to evolving log data, we enable it to receive feedback from experts. Interestingly, our findings suggest that contemporary large language models, such as ChatGPT, can provide feedback with a level of consistency comparable to human experts, which can potentially reduce manual verification efforts. We extensively 
    
[^117]: GPT-FL: 生成预训练模型辅助的联邦学习

    GPT-FL: Generative Pre-trained Model-Assisted Federated Learning. (arXiv:2306.02210v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02210](http://arxiv.org/abs/2306.02210)

    GPT-FL是一种生成预训练模型辅助的联邦学习框架，通过生成多样化的合成数据并结合私有客户端数据进行训练，它在模型准确性、通信效率和客户端采样效率等方面优于最先进的方法。在FL训练中，由合成数据生成的下游模型对于控制梯度多样性的方向起着关键作用，提高了收敛速度，并显著提升了准确性。

    

    在这项工作中，我们提出了GPT-FL，一种生成预训练模型辅助的联邦学习（FL）框架。GPT-FL利用生成预训练模型生成多样化的合成数据。这些生成的数据用于在服务器上训练下游模型，然后在标准FL框架下使用私有客户端数据进行微调。我们展示了GPT-FL在模型测试准确性、通信效率和客户端采样效率方面始终优于最先进的FL方法。通过全面的消融分析，我们发现在FL训练过程中，由合成数据生成的下游模型对于控制梯度多样性的方向起着关键作用，这提高了收敛速度，并对观察到的GPT-FL的显著准确性提升做出了贡献。此外，无论目标数据是否在预训练生成模型的领域内或外，GPT-FL始终实现了显著的性能提升。

    In this work, we propose GPT-FL, a generative pre-trained model-assisted federated learning (FL) framework. At its core, GPT-FL leverages generative pre-trained models to generate diversified synthetic data. These generated data are used to train a downstream model on the server, which is then fine-tuned with private client data under the standard FL framework. We show that GPT-FL consistently outperforms state-of-the-art FL methods in terms of model test accuracy, communication efficiency, and client sampling efficiency. Through comprehensive ablation analysis, we discover that the downstream model generated by synthetic data plays a crucial role in controlling the direction of gradient diversity during FL training, which enhances convergence speed and contributes to the notable accuracy boost observed with GPT-FL. Also, regardless of whether the target data falls within or outside the domain of the pre-trained generative model, GPT-FL consistently achieves significant performance gai
    
[^118]: PAGAR: 用主角-反派引导的对抗性奖励驯服逆强化学习在基于模仿学习中的奖励不对齐问题

    PAGAR: Taming Reward Misalignment in Inverse Reinforcement Learning-Based Imitation Learning with Protagonist Antagonist Guided Adversarial Reward. (arXiv:2306.01731v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.01731](http://arxiv.org/abs/2306.01731)

    PAGAR是一种用于解决IRL-based IL中奖励不对齐问题的半监督奖励设计方法，在复杂IL任务和零-shot IL任务中表现优越。

    

    许多模仿学习(imitation learning, IL)算法使用逆强化学习(inverse reinforcement learning, IRL)来推断专家以隐式方式优化的潜在奖励函数，基于其展示的行为。然而，推断的奖励与真实任务目标之间的不对齐可能导致任务失败。在本文中，我们引入了主角-反派引导的对抗性奖励(PAGAR)，这是一种半监督奖励设计范式，用于解决IRL-based IL中的奖励不对齐问题。我们确定了候选奖励函数满足的条件，PAGAR能够保证产生一个在底层任务中成功的策略。此外，我们提出了一种实用的在策略和离策略方法来在IRL-based IL中实施PAGAR。实验结果表明，我们的算法在复杂的IL任务和有限演示的迁移环境的零-shot IL任务上优于竞争的基线模型。

    Many imitation learning (IL) algorithms employ inverse reinforcement learning (IRL) to infer the underlying reward function that an expert is implicitly optimizing for, based on their demonstrated behaviors. However, a misalignment between the inferred reward and the true task objective can result in task failures. In this paper, we introduce Protagonist Antagonist Guided Adversarial Reward (PAGAR), a semi-supervised reward design paradigm to tackle this reward misalignment problem in IRL-based IL. We identify the conditions on the candidate reward functions under which PAGAR can guarantee to induce a policy that succeeds in the underlying task. Furthermore, we present a practical on-and-off policy approach to implement PAGAR in IRL-based IL. Experimental results show that our algorithm outperforms competitive baselines on complex IL tasks and zero-shot IL tasks in transfer environments with limited demonstrations.
    
[^119]: Gode -- 将生物化学知识图谱集成到分子图神经网络的预训练中

    Gode -- Integrating Biochemical Knowledge Graph into Pre-training Molecule Graph Neural Network. (arXiv:2306.01631v1 [cs.LG])

    [http://arxiv.org/abs/2306.01631](http://arxiv.org/abs/2306.01631)

    本研究提出了一种新的方法，在分子结构和生物医学知识图谱中集成多个领域信息，通过自我监督策略预先训练更广泛和更强大的表示，并在化学属性预测任务上展示出出色的性能。

    

    分子属性的准确预测对于促进创新治疗方法的发展和理解化学物质和生物系统之间复杂的相互作用至关重要。本研究提出了一种新的方法，将单个分子结构的图表示与生物医学知识图谱 (KG) 的多个领域信息进行集成。通过集成两个级别的信息，我们可以使用自我监督策略预先训练更广泛和更强大的表示，用于分子级和 KG 级预测任务。在性能评估方面，我们在 11 个具有挑战性的化学属性预测任务上微调我们预先训练的模型。我们的框架的结果表明，我们微调的模型优于现有的最先进的模型。

    The precise prediction of molecular properties holds paramount importance in facilitating the development of innovative treatments and comprehending the intricate interplay between chemicals and biological systems. In this study, we propose a novel approach that integrates graph representations of individual molecular structures with multi-domain information from biomedical knowledge graphs (KGs). Integrating information from both levels, we can pre-train a more extensive and robust representation for both molecule-level and KG-level prediction tasks with our novel self-supervision strategy. For performance evaluation, we fine-tune our pre-trained model on 11 challenging chemical property prediction tasks. Results from our framework demonstrate our fine-tuned models outperform existing state-of-the-art models.
    
[^120]: 元学习框架用于未知说话人识别中的端到端冒名者识别

    Meta-Learning Framework for End-to-End Imposter Identification in Unseen Speaker Recognition. (arXiv:2306.00952v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2306.00952](http://arxiv.org/abs/2306.00952)

    本文提出了一个元学习框架，用于未知说话人识别中的冒名者识别。通过引入强健的说话人特定阈值技术和端到端元学习，将冒名者检测从未知说话人识别中分离出来，并通过利用注册说话人的话语来学习检测冒名者，相比基线提高了10%。

    

    说话人识别系统被部署在多样的环境中，往往与它们所训练和测试的实验室条件不同。本文首先展示了使用固定阈值（使用EER度量计算）进行未知说话人识别中的冒名者识别时的泛化问题，并引入了一种强健的说话人特定阈值技术以提高性能。其次，受最近在说话人验证中使用元学习技术的启发，我们提出了一种用于冒名者检测的端到端元学习框架，将冒名者检测问题从未知说话人识别中分离出来。因此，与大多数先前的研究使用一些启发式方法来检测冒名者不同，所提出的网络通过利用注册说话人的话语来学习检测冒名者。此外，我们在VoxCeleb1、VCTK和FFSVC 2022数据集上展示了所提出技术的有效性，相比基线提高了10%。

    Speaker identification systems are deployed in diverse environments, often different from the lab conditions on which they are trained and tested. In this paper, first, we show the problem of generalization using fixed thresholds (computed using EER metric) for imposter identification in unseen speaker recognition and then introduce a robust speaker-specific thresholding technique for better performance. Secondly, inspired by the recent use of meta-learning techniques in speaker verification, we propose an end-to-end meta-learning framework for imposter detection which decouples the problem of imposter detection from unseen speaker identification. Thus, unlike most prior works that use some heuristics to detect imposters, the proposed network learns to detect imposters by leveraging the utterances of the enrolled speakers. Furthermore, we show the efficacy of the proposed techniques on VoxCeleb1, VCTK and the FFSVC 2022 datasets, beating the baselines by up to 10%.
    
[^121]: 通过RKHS逼近理解基于增广的自监督表示学习

    Understanding Augmentation-based Self-Supervised Representation Learning via RKHS Approximation. (arXiv:2306.00788v1 [cs.LG])

    [http://arxiv.org/abs/2306.00788](http://arxiv.org/abs/2306.00788)

    本文通过RKHS逼近方法，揭示了自监督表示学习中好的数据增强的重要性，并阐述了对于任意编码器，增广函数质量的提升可以提高编码器的表示能力，同时分析了批量归一化和数据增强在自监督学习中的作用。

    

    好的数据增强是自监督表示学习（如对比学习和掩码语言建模）实现经验成功的关键因素之一，但其在学习好的表示方面的理论理解仍然有限。最近的工作建立了自监督学习和逼近图拉普拉斯算子的顶部特征空间之间的联系。在这项工作中，我们利用这一洞察力对基于增广的预训练进行统计分析。我们从保持等距的属性出发，这是由增强给出的目标函数的关键几何特征。我们的第一主要定理为任意编码器提供了接近紧密的上限，用于估计通过在编码器之上拟合线性探测器而产生的估计误差和编码器学习的RKHS的逼近误差。我们的第二个主要定理表明，在温和条件下，可以通过RKHS函数任意精确地逼近增广函数。这个结果意味着，随着增广函数质量的提高，学习的编码器的表示能力也会提高。我们的分析还揭示了自监督学习中批量归一化和数据增强的作用。

    Good data augmentation is one of the key factors that lead to the empirical success of self-supervised representation learning such as contrastive learning and masked language modeling, yet theoretical understanding of its role in learning good representations remains limited. Recent work has built the connection between self-supervised learning and approximating the top eigenspace of a graph Laplacian operator. Learning a linear probe on top of such features can naturally be connected to RKHS regression. In this work, we use this insight to perform a statistical analysis of augmentation-based pretraining. We start from the isometry property, a key geometric characterization of the target function given by the augmentation. Our first main theorem provides, for an arbitrary encoder, near tight bounds for both the estimation error incurred by fitting the linear probe on top of the encoder, and the approximation error entailed by the fitness of the RKHS the encoder learns. Our second main
    
[^122]: 扩散模型的几何视角

    A Geometric Perspective on Diffusion Models. (arXiv:2305.19947v1 [cs.CV])

    [http://arxiv.org/abs/2305.19947](http://arxiv.org/abs/2305.19947)

    本文研究了扩散模型的几何结构，发现通过一个明确的准线性采样轨迹和另一个隐式的去噪轨迹平滑连接了数据分布和噪声分布，建立了基于ODE的最优采样和经典的均值漂移算法之间的理论关系。

    

    近年来，针对扩散模型的高效训练和快速采样方法取得了显著进展。最近的一个重要进展是使用随机微分方程（SDE）来描述数据扰动和生成建模，以实现统一的数学框架。本文揭示了扩散模型的几个有趣的几何结构，并为其采样动力学提供了简单而强大的解释。通过仔细检查一种流行的方差爆炸SDE及其保持边际的普通微分方程（ODE）用于采样，我们发现数据分布和噪声分布通过一个明确的准线性采样轨迹和另一个隐式的去噪轨迹平滑连接，即使在视觉质量方面也收敛更快。我们还建立起基于ODE的最优采样和经典的均值漂移（寻找模式）算法之间的理论关系。

    Recent years have witnessed significant progress in developing efficient training and fast sampling approaches for diffusion models. A recent remarkable advancement is the use of stochastic differential equations (SDEs) to describe data perturbation and generative modeling in a unified mathematical framework. In this paper, we reveal several intriguing geometric structures of diffusion models and contribute a simple yet powerful interpretation to their sampling dynamics. Through carefully inspecting a popular variance-exploding SDE and its marginal-preserving ordinary differential equation (ODE) for sampling, we discover that the data distribution and the noise distribution are smoothly connected with an explicit, quasi-linear sampling trajectory, and another implicit denoising trajectory, which even converges faster in terms of visual quality. We also establish a theoretical relationship between the optimal ODE-based sampling and the classic mean-shift (mode-seeking) algorithm, with w
    
[^123]: 稳健的各向异性正则化

    Stable Anisotropic Regularization. (arXiv:2305.19358v1 [cs.CL])

    [http://arxiv.org/abs/2305.19358](http://arxiv.org/abs/2305.19358)

    本文提出了一种新颖的正则化方法I-STAR，可以增加模型的稳定性，提高性能，并改善自然语言处理中的组合表示问题。

    

    鉴于大型语言模型（LLMs）的成功，研究模型激活的属性已引起了相当大的兴趣。文献普遍认为LLMs表示由少数具有极高方差和幅度的“异常维度”主导。自然语言处理（NLP）中的几项研究试图减轻这些异常维度的影响，并迫使LLMs成为各向同性（即在嵌入空间中所有维度具有均匀方差）的。各向同性被认为是LLMs的一种理想属性，可以提高模型性能并更加贴近人类直觉的文本表示。然而，关于NLP中各向同性的许多观点都是基于嵌入的平均余弦相似度，最近已经表明这是一种有缺陷的各向同性度量。在本文中，我们提出了I-STAR：基于IsoScore$^{\star}$的稳定各向异性正则化，这是一种新颖的正则化方法，可以用于增加模型的稳定性并提高性能。

    Given the success of Large Language Models (LLMs), there has been considerable interest in studying the properties of model activations. The literature overwhelmingly agrees that LLM representations are dominated by a few ``outlier dimensions'' with exceedingly high variance and magnitude. Several studies in Natural Language Processing (NLP) have sought to mitigate the impact of such outlier dimensions and force LLMs to be isotropic (i.e., have uniform variance across all dimensions in embedding space). Isotropy is thought to be a desirable property for LLMs that improves model performance and more closely aligns textual representations with human intuition. However, many of the claims regarding isotropy in NLP have been based on the average cosine similarity of embeddings, which has recently been shown to be a flawed measure of isotropy. In this paper, we propose I-STAR: IsoScore$^{\star}$-based STable Anisotropic Regularization, a novel regularization method that can be used to incre
    
[^124]: Auto-tune: 神经网络的先验与后验PAC-Bayes优化

    Auto-tune: PAC-Bayes Optimization over Prior and Posterior for Neural Networks. (arXiv:2305.19243v1 [stat.ML])

    [http://arxiv.org/abs/2305.19243](http://arxiv.org/abs/2305.19243)

    通过提出一种PAC-Bayes训练框架，无需额外正则化和网格搜索调整超参数即可达到与传统方法相媲美的测试性能，显著提高神经网络泛化能力并具有实际应用价值。

    

    通过精心设计训练过程，可以显著提高神经网络的泛化能力。目前最先进的训练方法涉及使用随机梯度下降或Adam优化算法，以及额外的正则化技术，如权重衰减、Dropout或噪声注入。通过网格搜索调整数量众多的超参数才能达到最优泛化，这可能耗时，并需要额外的验证数据集。为解决这个问题，我们引入了一个切实可行的PAC-Bayes训练框架，几乎是无需调整，也不需要额外的正则化，而在完成网格搜索和加入额外正则化后，达到了与SGD/Adam可比较的测试性能。我们提出的算法展示了PAC训练在深度神经网络上实现最先进性能的显著潜力。

    It is widely recognized that the generalization ability of neural networks can be greatly enhanced through carefully designing the training procedure. The current state-of-the-art training approach involves utilizing stochastic gradient descent (SGD) or Adam optimization algorithms along with a combination of additional regularization techniques such as weight decay, dropout, or noise injection. Optimal generalization can only be achieved by tuning a multitude of hyperparameters through grid search, which can be time-consuming and necessitates additional validation datasets. To address this issue, we introduce a practical PAC-Bayes training framework that is nearly tuning-free and requires no additional regularization while achieving comparable testing performance to that of SGD/Adam after a complete grid search and with extra regularizations. Our proposed algorithm demonstrates the remarkable potential of PAC training to achieve state-of-the-art performance on deep neural networks wit
    
[^125]: 如何有效地在强化学习中进行人类反馈查询？

    How to Query Human Feedback Efficiently in RL?. (arXiv:2305.18505v1 [cs.LG])

    [http://arxiv.org/abs/2305.18505](http://arxiv.org/abs/2305.18505)

    该论文提出了一种针对强化学习中人类反馈查询的有效采样方法，以在最少的人类反馈下学习最佳策略，并可应用于具有线性参数化和未知过渡的偏好模型，并引入了基于行动比较反馈的RLHF。

    

    人类反馈强化学习（RLHF）是一种范例，在此范例下，RL代理学习使用对轨迹的成对优先级反馈来最优化任务，而不是使用明确的奖励信号。尽管RLHF在微调语言模型方面已经取得了实用成功，但现有的实证研究并未解决如何高效采样轨迹对以查询人类反馈的挑战。在本研究中，我们提出了一种有效的采样方法，用于获取探索性轨迹，在收集任何人类反馈之前，使学习隐藏的奖励函数更加准确。理论分析表明，与现有文献相比，我们的算法在线性参数化和未知过渡的基于偏好模型下学习最优策略所需的人类反馈更少。具体而言，我们的框架可以纳入线性和低秩MDPs。此外，我们研究了使用基于行动比较的反馈的RLHF，并介绍了一种高效的采样方法，以在优化具有有限反馈的任务时获得探索性轨迹。

    Reinforcement Learning with Human Feedback (RLHF) is a paradigm in which an RL agent learns to optimize a task using pair-wise preference-based feedback over trajectories, rather than explicit reward signals. While RLHF has demonstrated practical success in fine-tuning language models, existing empirical work does not address the challenge of how to efficiently sample trajectory pairs for querying human feedback. In this study, we propose an efficient sampling approach to acquiring exploratory trajectories that enable accurate learning of hidden reward functions before collecting any human feedback. Theoretical analysis demonstrates that our algorithm requires less human feedback for learning the optimal policy under preference-based models with linear parameterization and unknown transitions, compared to the existing literature. Specifically, our framework can incorporate linear and low-rank MDPs. Additionally, we investigate RLHF with action-based comparison feedback and introduce an
    
[^126]: 通过非负低秩半定规划实现最优K均值聚类

    Statistically Optimal K-means Clustering via Nonnegative Low-rank Semidefinite Programming. (arXiv:2305.18436v1 [stat.ML])

    [http://arxiv.org/abs/2305.18436](http://arxiv.org/abs/2305.18436)

    本文提出了一种与NMF算法一样简单且可扩展的K均值聚类算法，该算法通过解决非负低秩半定规划问题获得了强大的统计最优性保证，实验证明该算法在合成和实际数据集上表现优异。

    

    K均值聚类是一种广泛应用于大数据集中发现模式的机器学习方法。半定规划（SDP）松弛最近被提出用于解决K均值优化问题，具有很强的统计最优性保证。但实现SDP求解器的巨大成本使得这些保证无法应用于实际数据集。相比之下，非负矩阵分解（NMF）是一种简单的聚类算法，被机器学习从业者广泛使用，但缺乏坚实的统计基础或严格的保证。在本文中，我们描述了一种类似于NMF的算法，通过使用非凸Burer-Monteiro分解方法解决半定规划松弛的K均值公式的非负低秩限制。所得到的算法与最先进的NMF算法一样简单和可扩展，同时也享有与SDP相同的强大的统计最优性保证。在我们的实验中，我们证明了我们的算法优于现有的NMF算法，并在合成和实际数据集上表现与最先进的SDP求解器相当。

    $K$-means clustering is a widely used machine learning method for identifying patterns in large datasets. Semidefinite programming (SDP) relaxations have recently been proposed for solving the $K$-means optimization problem that enjoy strong statistical optimality guarantees, but the prohibitive cost of implementing an SDP solver renders these guarantees inaccessible to practical datasets. By contrast, nonnegative matrix factorization (NMF) is a simple clustering algorithm that is widely used by machine learning practitioners, but without a solid statistical underpinning nor rigorous guarantees. In this paper, we describe an NMF-like algorithm that works by solving a nonnegative low-rank restriction of the SDP relaxed $K$-means formulation using a nonconvex Burer--Monteiro factorization approach. The resulting algorithm is just as simple and scalable as state-of-the-art NMF algorithms, while also enjoying the same strong statistical optimality guarantees as the SDP. In our experiments,
    
[^127]: 学习两层神经网络：一次(巨大)的步骤。

    Learning Two-Layer Neural Networks, One (Giant) Step at a Time. (arXiv:2305.18270v1 [stat.ML])

    [http://arxiv.org/abs/2305.18270](http://arxiv.org/abs/2305.18270)

    本文研究了浅层神经网络的训练动态及其条件，证明了动态下梯度下降可以通过有限数量的大批量梯度下降步骤来促进特征学习，并找到了多个和单一方向的最佳批量大小，有助于促进特征学习和方向的专业化。

    

    我们研究了浅层神经网络的训练动态，研究了有限数量的大批量梯度下降步骤有助于在核心范围之外促进特征学习的条件。我们比较了批量大小和多个(但有限的)步骤的影响。我们分析了单步骤过程，发现批量大小为$n=O(d)$可以促进特征学习，但只适合学习单一方向或单索引模型。相比之下，$n=O(d^2)$对于学习多个方向和专业化至关重要。此外，我们证明“硬”方向缺乏前$\ell$个Hermite系数，仍未被发现，并且需要批量大小为$n=O(d^\ell)$才能被梯度下降捕获。经过几次迭代，情况发生变化：批量大小为$n=O(d)$足以学习新的目标方向，这些方向在Hermite基础上线性连接到之前学习的方向所涵盖的子空间。

    We study the training dynamics of shallow neural networks, investigating the conditions under which a limited number of large batch gradient descent steps can facilitate feature learning beyond the kernel regime. We compare the influence of batch size and that of multiple (but finitely many) steps. Our analysis of a single-step process reveals that while a batch size of $n = O(d)$ enables feature learning, it is only adequate for learning a single direction, or a single-index model. In contrast, $n = O(d^2)$ is essential for learning multiple directions and specialization. Moreover, we demonstrate that ``hard'' directions, which lack the first $\ell$ Hermite coefficients, remain unobserved and require a batch size of $n = O(d^\ell)$ for being captured by gradient descent. Upon iterating a few steps, the scenario changes: a batch-size of $n = O(d)$ is enough to learn new target directions spanning the subspace linearly connected in the Hermite basis to the previously learned directions,
    
[^128]: 学习单调博弈的投石索方法

    A Slingshot Approach to Learning in Monotone Games. (arXiv:2305.16610v1 [cs.GT])

    [http://arxiv.org/abs/2305.16610](http://arxiv.org/abs/2305.16610)

    本文提出了一种新的框架, 通过正则化游戏的支付或效用和更新投石索策略，无论是否存在噪声都能够实现在单调博弈中计算均衡。

    

    本文解决了在单调博弈中计算均衡的问题。传统的遵循正则化领导者算法即使在双人零和游戏中也无法收敛到均衡。虽然已经提出了这些算法的乐观版本并具有最后迭代的收敛保证，但它们需要无噪声的梯度反馈。为了克服这个限制，我们提出了一个新的框架，即使在存在噪声的情况下也能实现最后一次迭代的收敛。我们的关键思想是扰动或正则化游戏的支付或效用。这种扰动有助于将当前策略拉向一个锚定策略，我们称之为“投石索”策略。首先，我们建立了框架的收敛速度，从而获得靠近均衡点的稳定点，无论是否存在噪声。接下来，我们介绍了一种方法，定期更新投石索策略和当前策略。我们将这种方法解释为近端p

    In this paper, we address the problem of computing equilibria in monotone games. The traditional Follow the Regularized Leader algorithms fail to converge to an equilibrium even in two-player zero-sum games. Although optimistic versions of these algorithms have been proposed with last-iterate convergence guarantees, they require noiseless gradient feedback. To overcome this limitation, we present a novel framework that achieves last-iterate convergence even in the presence of noise. Our key idea involves perturbing or regularizing the payoffs or utilities of the games. This perturbation serves to pull the current strategy to an anchored strategy, which we refer to as a {\it slingshot} strategy. First, we establish the convergence rates of our framework to a stationary point near an equilibrium, regardless of the presence or absence of noise. Next, we introduce an approach to periodically update the slingshot strategy with the current strategy. We interpret this approach as a proximal p
    
[^129]: 用视觉动作转换器模拟任务和动作规划

    Imitating Task and Motion Planning with Visuomotor Transformers. (arXiv:2305.16309v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2305.16309](http://arxiv.org/abs/2305.16309)

    本研究提出了一种名为OPTIMUS的新型模仿学习系统，通过模仿TAMP代理来训练大规模的视觉动作转换器策略。OPTIMUS引入了一个专门为模仿学习而设计的TAMP数据生成管道，可以用来训练性能优越的基于转换器的策略。

    

    模仿学习是训练机器人操作策略的强大工具，使其能够从专家演示中学习而无需手动编程或试错。然而，常见的数据收集方法，如人工监督，因为耗时和劳动密集而难以扩展。相反，任务和动作规划（TAMP）可以自主地生成大规模的多样化演示数据集。在这项工作中，我们展示了由TAMP监督员生成的大规模数据集与灵活的Transformer模型相结合是机器人操作的强大范例。为此，我们提出了一种名为OPTIMUS的新型模仿学习系统，通过模仿TAMP代理来训练大规模的视觉动作转换器策略。OPTIMUS引入了一个专门为模仿学习而设计的TAMP数据生成管道，可以用来训练性能优越的基于转换器的策略。在本文中，我们对设计进行了全面的研究

    Imitation learning is a powerful tool for training robot manipulation policies, allowing them to learn from expert demonstrations without manual programming or trial-and-error. However, common methods of data collection, such as human supervision, scale poorly, as they are time-consuming and labor-intensive. In contrast, Task and Motion Planning (TAMP) can autonomously generate large-scale datasets of diverse demonstrations. In this work, we show that the combination of large-scale datasets generated by TAMP supervisors and flexible Transformer models to fit them is a powerful paradigm for robot manipulation. To that end, we present a novel imitation learning system called OPTIMUS that trains large-scale visuomotor Transformer policies by imitating a TAMP agent. OPTIMUS introduces a pipeline for generating TAMP data that is specifically curated for imitation learning and can be used to train performant transformer-based policies. In this paper, we present a thorough study of the design
    
[^130]: C-MCTS: 安全规划与蒙特卡洛树搜索

    C-MCTS: Safe Planning with Monte Carlo Tree Search. (arXiv:2305.16209v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.16209](http://arxiv.org/abs/2305.16209)

    C-MCTS 提出了一种解决有约束的决策问题的方法，通过训练安全评判器进行成本估计，并在部署期间通过剪枝不安全轨迹来限制探索，实现了更高的奖励和更高效的规划步骤。

    

    有约束的马尔可夫决策过程（CMDP）可以解决受约束的安全决策问题。尽管CMDP在强化学习的文献中得到了广泛研究，但对于使用MCTS等基于采样的规划算法来解决CMDP的研究却很少。以往的方法在成本方面保守行事，通过使用蒙特卡洛成本估计来避免违反约束，但这种估计存在高方差。我们提出了约束MCTS（C-MCTS），它使用先前在代理部署之前通过时间差分学习训练的安全评判器来估计成本。在部署期间，评判器通过剪枝不安全轨迹来限制探索。C-MCTS满足成本约束，但操作接近约束边界，比以往的工作获得更高的奖励。作为一个很好的副产品，这个规划器在规划步骤方面更加高效。最重要的是，在模型下，

    The Constrained Markov Decision Process (CMDP) formulation allows to solve safety-critical decision making tasks that are subject to constraints. While CMDPs have been extensively studied in the Reinforcement Learning literature, little attention has been given to sampling-based planning algorithms such as MCTS for solving them. Previous approaches perform conservatively with respect to costs as they avoid constraint violations by using Monte Carlo cost estimates that suffer from high variance. We propose Constrained MCTS (C-MCTS), which estimates cost using a safety critic that is trained with Temporal Difference learning in an offline phase prior to agent deployment. The critic limits exploration by pruning unsafe trajectories within MCTS during deployment. C-MCTS satisfies cost constraints but operates closer to the constraint boundary, achieving higher rewards than previous work. As a nice byproduct, the planner is more efficient w.r.t. planning steps. Most importantly, under model
    
[^131]: 在智能体和语言模型中被动学习主动因果策略

    Passive learning of active causal strategies in agents and language models. (arXiv:2305.16183v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.16183](http://arxiv.org/abs/2305.16183)

    通过被动学习，在智能体和语言模型中可以学习到一般化的主动因果策略，用于确定和使用因果关系结构。通过模仿专家数据进行训练的智能体能够在测试时推断和使用从未出现的因果链接，并将实验策略推广到从未观察到的新变量集。

    

    通过被动数据，我们能够学习到关于因果关系和实验的什么信息？鉴于被动训练的语言模型在工具使用等交互领域的最新成功，这个问题变得很重要。被动学习本质上是有限的。然而，我们展示了纯粹的被动学习实际上能够让智能体学习到一般化的策略，用于确定和使用因果关系结构，只要智能体能够在测试时干预。我们在形式上说明了首先进行实验，然后寻求目标的策略能够原则上使被动学习实现一般化。然后，我们从经验上展示了通过模仿专家数据进行训练的智能体在测试时能够推断和使用训练数据中从未出现的因果链接；这些智能体还能够将实验策略推广到从未在训练中观察到的新变量集。然后，我们展示了从被动数据中一般化因果干预和利用策略。

    What can be learned about causality and experimentation from passive data? This question is salient given recent successes of passively-trained language models in interactive domains such as tool use. Passive learning is inherently limited. However, we show that purely passive learning can in fact allow an agent to learn generalizable strategies for determining and using causal structures, as long as the agent can intervene at test time. We formally illustrate that learning a strategy of first experimenting, then seeking goals, can allow generalization from passive learning in principle. We then show empirically that agents trained via imitation on expert data can indeed generalize at test time to infer and use causal links which are never present in the training data; these agents can also generalize experimentation strategies to novel variable sets never observed in training. We then show that strategies for causal intervention and exploitation can be generalized from passive data ev
    
[^132]: 大型语言模型的自相矛盾幻觉：评估、检测和缓解

    Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation. (arXiv:2305.15852v1 [cs.CL])

    [http://arxiv.org/abs/2305.15852](http://arxiv.org/abs/2305.15852)

    本文对大型语言模型的自相矛盾幻觉进行了评估、检测和缓解，探究了这一幻觉形式的普遍存在性。通过设计框架有效触发自相矛盾，发现不同语言模型中这种现象都频繁出现。ChatGPT和GPT-4能够准确识别自相矛盾，而Vicuna-13B则有些困难。

    

    大型语言模型容易产生幻想的文本。自相矛盾是一种重要的幻觉形式，指的是语言模型在同一语境中生成两个矛盾的句子。本文针对最先进、经过指导的语言模型，对自相矛盾进行了全面的分析、评估、检测和缓解。我们设计了一个框架来有效地触发自相矛盾，评估结果表明，无论是对于著名的还是不太出名的话题，不同的语言模型中自相矛盾都经常发生。

    Large language models (large LMs) are susceptible to producing text with hallucinated content. Self-contradiction, where the LM generates two contradictory sentences within the same context, is an important form of hallucination. In this work, we present a comprehensive analysis on self-contradiction for state-of-the-art, instruction-tuned LMs, including evaluation, detection, and mitigation. To effectively trigger self-contradictions, we design a framework that constrains LMs to generate appropriate sentence pairs. Our evaluation on these sentence pairs reveals that self-contradictions occur frequently across different LMs for both famous and lesser-known topics. Next, we prompt the LMs to detect self-contradictions. Our results indicate that ChatGPT and GPT-4 are able to accurately identify self-contradictions, while Vicuna-13B struggles to do so. For example, with our best prompting method, ChatGPT achieves 91.0% precision and 80.5% recall on the sentence pairs generated by itself. 
    
[^133]: 关于文本到图像扩散模型的架构压缩问题研究

    On Architectural Compression of Text-to-Image Diffusion Models. (arXiv:2305.15798v1 [cs.LG])

    [http://arxiv.org/abs/2305.15798](http://arxiv.org/abs/2305.15798)

    本文研究了如何通过架构压缩方法实现文本到图像生成模型的高效化，提出了一种块删除知识提取SDMs（BK-SDMs）方法，在减少采样步骤数量和利用网络量化的同时，可以显著减少模型的参数数量、MAC和延迟，最终实现了与使用更多资源训练的模型相竞争的效果。

    

    稳定扩散模型（SDMs）中出色的文本到图像（T2I）生成结果需要大量计算资源。为了解决这个问题，近期关于高效SDMs的研究将重点放在减少采样步骤的数量和利用网络量化上。与这些方向相反，本研究通过引入块删除知识提取SDMs（BK-SDMs），强调了经典架构压缩在通用T2I合成中的作用。我们从SDMs的U-Net中删除了几个残差和注意力块，使参数数量、每个采样步骤的MAC和延迟减少了超过30％。我们在单个A100 GPU上仅使用0.22M LAION对进行蒸馏预训练（少于全体训练对的0.1％）。尽管使用有限的资源进行训练，我们的紧凑型模型可以通过传递的知识模仿原始SDM，并在对抗较大的多十亿参数模型的情况下实现具有竞争力的结果。

    Exceptional text-to-image (T2I) generation results of Stable Diffusion models (SDMs) come with substantial computational demands. To resolve this issue, recent research on efficient SDMs has prioritized reducing the number of sampling steps and utilizing network quantization. Orthogonal to these directions, this study highlights the power of classical architectural compression for general-purpose T2I synthesis by introducing block-removed knowledge-distilled SDMs (BK-SDMs). We eliminate several residual and attention blocks from the U-Net of SDMs, obtaining over a 30% reduction in the number of parameters, MACs per sampling step, and latency. We conduct distillation-based pretraining with only 0.22M LAION pairs (fewer than 0.1% of the full training pairs) on a single A100 GPU. Despite being trained with limited resources, our compact models can imitate the original SDM by benefiting from transferred knowledge and achieve competitive results against larger multi-billion parameter models
    
[^134]: 基于谱角度剖析生物数据中图神经网络的尺寸可泛化性：观点和实践

    Size Generalizability of Graph Neural Networks on Biological Data: Insights and Practices from the Spectral Perspective. (arXiv:2305.15611v1 [cs.LG])

    [http://arxiv.org/abs/2305.15611](http://arxiv.org/abs/2305.15611)

    本文通过谱角度的方法，研究了GNNs的尺寸可泛化性问题，并在真实生物数据集上进行了实验，发现GNNs在度分布和谱分布偏移时均表现敏感，在同一数据集的大图上的性能仍然下降，揭示了 GNNs的尺寸可泛化性问题。

    

    本文探讨了图神经网络 (GNNs) 是否具有从小图中学习的知识可推广到同一领域的大图中。之前的研究表明，不同大小的图之间的分布偏移，尤其是度分布，可能会导致图分类任务的性能下降。然而，在生物数据集中，度数是有界的，因此度分布的偏移很小。即使度分布偏移很小，我们观察到GNNs在同一数据集的大图上的性能仍然下降，暗示有其他原因。事实上，以往对于真实数据集中各种图尺寸引起的分布偏移类型和属性的探索不足。此外，以前的尺寸可泛化性分析大多集中在空间领域。为填补这些空白，我们采用谱角度去研究GNNs在生物图数据上的尺寸可泛化性。我们首先提出一个新框架来模拟各种类型的度分布偏移，并利用它来测试GNNs 在真实生物数据集上的尺寸可泛化性。我们的实验表明，除了度分布偏移外，GNNs 还对图大小变化引起的谱分布偏移很敏感。我们进一步分析了不同的GNN模型的影响，并表明，一些模型比其他模型更具有尺寸泛化性。本文展示了关于GNNs尺寸可泛化性问题的新观点和实践，并为该领域的未来研究提供了有益的洞察和建议。

    We investigate the question of whether the knowledge learned by graph neural networks (GNNs) from small graphs is generalizable to large graphs in the same domain. Prior works suggest that the distribution shift, particularly in the degree distribution, between graphs of different sizes can lead to performance degradation in the graph classification task. However, this may not be the case for biological datasets where the degrees are bounded and the distribution shift of degrees is small. Even with little degree distribution shift, our observations show that GNNs' performance on larger graphs from the same datasets still degrades, suggesting other causes. In fact, there has been a lack of exploration in real datasets to understand the types and properties of distribution shifts caused by various graph sizes. Furthermore, previous analyses of size generalizability mostly focus on the spatial domain.  To fill these gaps, we take the spectral perspective and study the size generalizabilit
    
[^135]: 带有拟对抗扰动和时变对抗性赌博损失函数的最优率问题

    Optimal Rates for Bandit Nonstochastic Control. (arXiv:2305.15352v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.15352](http://arxiv.org/abs/2305.15352)

    本研究解决了带有拟对抗扰动和时变对抗性赌博损失函数的LQR和LQG问题，并提供了一种新颖的具有记忆的赌博凸优化方案。

    

    线性二次调节器（LQR）和线性二次高斯（LQG）控制是最优控制中基础且广泛研究的问题。我们研究了具有拟对抗扰动和时变对抗性赌博损失函数的LQR和LQG问题。已知的最佳亚线性遗憾算法~\cite{gradu2020non}在时间横跨度上具有$T^{\frac{3}{4}}$的依赖关系，其作者提出了一个关于是否可以达到$\sqrt{T}$的紧致率的开放问题。我们回答肯定地，提供了一种对于已知和未知系统都能达到最优（除对数因子外）遗憾的赌博LQR和LQG算法。我们方法的一个核心组成部分是一种具有记忆的赌博凸优化新方案，这具有独立的意义。

    Linear Quadratic Regulator (LQR) and Linear Quadratic Gaussian (LQG) control are foundational and extensively researched problems in optimal control. We investigate LQR and LQG problems with semi-adversarial perturbations and time-varying adversarial bandit loss functions. The best-known sublinear regret algorithm of~\cite{gradu2020non} has a $T^{\frac{3}{4}}$ time horizon dependence, and its authors posed an open question about whether a tight rate of $\sqrt{T}$ could be achieved. We answer in the affirmative, giving an algorithm for bandit LQR and LQG which attains optimal regret (up to logarithmic factors) for both known and unknown systems. A central component of our method is a new scheme for bandit convex optimization with memory, which is of independent interest.
    
[^136]: 一种无监督方法用于估计数据集的类别可分性，并应用到LLMs的微调

    An Unsupervised Method for Estimating Class Separability of Datasets with Application to LLMs Fine-Tuning. (arXiv:2305.15016v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.15016](http://arxiv.org/abs/2305.15016)

    这个论文提出了一种无监督方法，通过利用数据流形的拓扑特征估计数据的类别可分性，该方法与有监督度量具有相关性，可以用于半监督和感知学习。同时，在语言模型微调中应用该方法用于自动停止准则。

    

    本论文提出了一种无监督方法，利用数据流形的拓扑特征来估计数据的类别可分性，而不需要标签信息。在本论文中，对几个数据集进行的实验表明，所提出的方法估计的类别可分性与需要标签信息的监督度量（如Fisher判别比率（FDR）和分类器的交叉验证）之间存在清晰的相关性和一致性。这可以实现从有标签和无标签数据中学习的学习范式，如半监督学习和感知学习。当我们有限的有标签数据和相对较大的无标签数据集可以用来增强学习过程时，这将特别有用。该方法在无监督的设置下，通过监测嵌入空间流形的类别可分性，实现了语言模型微调和自动停止准则的应用。

    This paper proposes an unsupervised method that leverages topological characteristics of data manifolds to estimate class separability of the data without requiring labels. Experiments conducted in this paper on several datasets demonstrate a clear correlation and consistency between the class separability estimated by the proposed method with supervised metrics like Fisher Discriminant Ratio~(FDR) and cross-validation of a classifier, which both require labels. This can enable implementing learning paradigms aimed at learning from both labeled and unlabeled data, like semi-supervised and transductive learning. This would be particularly useful when we have limited labeled data and a relatively large unlabeled dataset that can be used to enhance the learning process. The proposed method is implemented for language model fine-tuning with automated stopping criterion by monitoring class separability of the embedding-space manifold in an unsupervised setting. The proposed methodology has 
    
[^137]: 具有人类反馈的可证明的离线强化学习

    Provable Offline Reinforcement Learning with Human Feedback. (arXiv:2305.14816v1 [cs.LG])

    [http://arxiv.org/abs/2305.14816](http://arxiv.org/abs/2305.14816)

    本文提出了一种具有人类反馈的离线强化学习算法，解决了如何估计隐式奖励以及在置信集周围解决规划问题的方法。此外，作者提出了一个能够使用多项式数量的样本学习任何目标策略的新保证，同时引入了一个新的单策略集中系数来衡量目标策略的覆盖范围。

    

    本文研究了离线强化学习的问题，其中反馈是以轨迹对之间的偏好形式提供的。我们提出的算法包括两个主要步骤：（1）使用通用函数逼近从离线数据估计隐式奖励，和（2）在MLE周围的置信集上解决分布鲁棒的规划问题。我们考虑了通用的奖励设置，其中奖励可以在整个轨迹上定义，并提供了一个新的保证，只要目标策略被离线数据覆盖，我们就可以使用多项式数量的样本来学习任何目标策略。为了衡量目标策略的覆盖范围，我们引入了一个新的单策略集中系数，可以通过每个轨迹的集中系数上界来上界化。

    In this paper, we investigate the problem of offline reinforcement learning with human feedback where feedback is available in the form of preference between trajectory pairs rather than explicit rewards. Our proposed algorithm consists of two main steps: (1) estimate the implicit reward using Maximum Likelihood Estimation (MLE) with general function approximation from offline data and (2) solve a distributionally robust planning problem over a confidence set around the MLE. We consider the general reward setting where the reward can be defined over the whole trajectory and provide a novel guarantee that allows us to learn any target policy with a polynomial number of samples, as long as the target policy is covered by the offline data. This guarantee is the first of its kind with general function approximation. To measure the coverage of the target policy, we introduce a new single-policy concentrability coefficient, which can be upper bounded by the per-trajectory concentrability coe
    
[^138]: 不变存储的注意力神经过程

    Constant Memory Attentive Neural Processes. (arXiv:2305.14567v1 [cs.LG])

    [http://arxiv.org/abs/2305.14567](http://arxiv.org/abs/2305.14567)

    提出了一种不变存储的注意力神经过程 (CMANPs) 及其注意力块 CMAB，能在常数内存下进行条件、查询和更新操作，并在元回归和少样本回归任务上获得最先进的表现。

    

    神经过程 (Neural Processes, NPs) 是估计预测不确定性的高效方法。NPs 包含一个编码条件数据集的条件阶段、一个使用编码预测的查询阶段和一个使用新数据点更新编码的更新阶段。然而，最先进的方法需要额外的内存，这个内存随数据集的大小呈线性或二次函数增长，限制了它们在低资源环境下的应用。在这项工作中，我们提出了不变存储的注意力神经过程 (Constant Memory Attentive Neural Processes, CMANPs)，它的条件、查询和更新阶段均只需要常数内存。在构建 CMANPs 时，我们提出了一种新型的通用注意力块，称为 Constant Memory Attention Block (CMAB)，它可以在常数内存中计算输出并进行常数的更新计算。实验结果表明，CMANPs 在元回归和少样本回归任务上实现了最先进的表现，同时保持常数内存复杂度。

    Neural Processes (NPs) are efficient methods for estimating predictive uncertainties. NPs comprise of a conditioning phase where a context dataset is encoded, a querying phase where the model makes predictions using the context dataset encoding, and an updating phase where the model updates its encoding with newly received datapoints. However, state-of-the-art methods require additional memory which scales linearly or quadratically with the size of the dataset, limiting their applications, particularly in low-resource settings. In this work, we propose Constant Memory Attentive Neural Processes (CMANPs), an NP variant which only requires constant memory for the conditioning, querying, and updating phases. In building CMANPs, we propose Constant Memory Attention Block (CMAB), a novel general-purpose attention block that can compute its output in constant memory and perform updates in constant computation. Empirically, we show CMANPs achieve state-of-the-art results on meta-regression an
    
[^139]: 不依赖于振幅的光 plethysmography (PPG) 机器学习方法：通过可见性图和迁移学习

    Amplitude-Independent Machine Learning for PPG through Visibility Graphs and Transfer Learning. (arXiv:2305.14062v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2305.14062](http://arxiv.org/abs/2305.14062)

    该论文提出了一种不依赖于振幅的光 plethysmography (PPG) 机器学习方法，通过可见性图和迁移学习实现了对心率和血管老化等生物特征的稳健估计和预测。

    

    光体积描记法 (PPG) 是使用光测量血液体积的变化的一种方法，是大多数可穿戴设备的特征。PPG信号能够提供对人体循环系统的洞察，并可用于提取各种生物特征，例如心率和血管老化。尽管已经提出了几种算法，但许多算法存在限制，包括过多地依赖人工校准、高信号质量要求和缺乏泛化能力。在本文中，我们引入了一种结合了图论和计算机视觉算法的PPG信号处理框架，该框架对振幅无关并且对仿射变换不变。它还需要最少的预处理，通过RGB通道融合信息，并在任务和数据集上展现了稳健的泛化能力。所提出的VGTL-net在血管老化预测方面实现了最先进的性能，并展示了稳健的估计能力。

    Photoplethysmography (PPG) refers to the measurement of variations in blood volume using light and is a feature of most wearable devices. The PPG signals provide insight into the body's circulatory system and can be employed to extract various bio-features, such as heart rate and vascular ageing. Although several algorithms have been proposed for this purpose, many exhibit limitations, including heavy reliance on human calibration, high signal quality requirements, and a lack of generalisation. In this paper, we introduce a PPG signal processing framework that integrates graph theory and computer vision algorithms, to provide an analysis framework which is amplitude-independent and invariant to affine transformations. It also requires minimal preprocessing, fuses information through RGB channels and exhibits robust generalisation across tasks and datasets. The proposed VGTL-net achieves state-of-the-art performance in the prediction of vascular ageing and demonstrates robust estimation
    
[^140]: 基于跨语言伪标注的无监督自动语音识别

    Unsupervised ASR via Cross-Lingual Pseudo-Labeling. (arXiv:2305.13330v1 [eess.AS])

    [http://arxiv.org/abs/2305.13330](http://arxiv.org/abs/2305.13330)

    本研究提出了一种基于跨语言伪标注的无监督ASR方法，能够使用其他语言中的标注数据来引导新语言的无监督AM。在Common Voice上取得了良好的效果，可以实现18% WER。而且在不同语言的数据集上都优于基线模型。

    

    最近的研究表明，可以仅使用非配对的音频和文本来训练无监督自动语音识别（ASR）系统。现有的无监督ASR方法假定不能使用任何标注数据进行训练。本文认为，即使没有给定语言的任何标注音频，也始终可以使用其他语言中的标注数据。本文展示了如何使用其他语言的字符级声学模型（AM），来引导新语言的无监督AM。 这里，“无监督”意味着没有可用于目标语言的标注音频。本文的方法基于两个关键因素：（i）使用其他语言AM生成“目标”语言的伪标签（PLs）；（ii）使用“目标语言模型”限制这些PLs。我们的方法在Common Voice上非常有效：例如，将英语AM传递到斯瓦希里语可以实现18％的WER。 它还在不同语言的多个数据集上优于基于字符的基线模型。

    Recent work has shown that it is possible to train an $\textit{unsupervised}$ automatic speech recognition (ASR) system using only unpaired audio and text. Existing unsupervised ASR methods assume that no labeled data can be used for training. We argue that even if one does not have any labeled audio for a given language, there is $\textit{always}$ labeled data available for other languages. We show that it is possible to use character-level acoustic models (AMs) from other languages to bootstrap an $\textit{unsupervised}$ AM in a new language. Here, "unsupervised" means no labeled audio is available for the $\textit{target}$ language. Our approach is based on two key ingredients: (i) generating pseudo-labels (PLs) of the $\textit{target}$ language using some $\textit{other}$ language AM and (ii) constraining these PLs with a $\textit{target language model}$. Our approach is effective on Common Voice: e.g. transfer of English AM to Swahili achieves 18% WER. It also outperforms characte
    
[^141]: 利用强化学习训练扩散模型

    Training Diffusion Models with Reinforcement Learning. (arXiv:2305.13301v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.13301](http://arxiv.org/abs/2305.13301)

    本文研究了利用强化学习方法直接优化扩散模型以实现下游对象的问题，并提出一种称之为去噪扩散策略优化（DDPO）的有效策略梯度算法，能够适应难以通过提示表达的图像压缩等目标，以及通过人类反馈得出的美学质量等目标。

    

    扩散模型是一类灵活的生成模型，采用对数似然目标的近似训练。然而，大多数扩散模型的使用案例并不关注似然，而是关注人类感知的图像质量或药物效力等下游目标。本文研究利用强化学习方法直接优化扩散模型以实现此类目标。我们描述了将去噪视为多步决策问题的方法，并提出称之为去噪扩散策略优化（DDPO）的一类策略梯度算法，相对于替代的奖励加权似然方法更为有效。在实证研究中，DDPO能够适应难以通过提示表达的图像压缩等目标，以及通过人类反馈得出的美学质量等目标。最后，我们展示DDPO可以利用来自反馈的提示-图像对齐方式来进行优化。

    Diffusion models are a class of flexible generative models trained with an approximation to the log-likelihood objective. However, most use cases of diffusion models are not concerned with likelihoods, but instead with downstream objectives such as human-perceived image quality or drug effectiveness. In this paper, we investigate reinforcement learning methods for directly optimizing diffusion models for such objectives. We describe how posing denoising as a multi-step decision-making problem enables a class of policy gradient algorithms, which we refer to as denoising diffusion policy optimization (DDPO), that are more effective than alternative reward-weighted likelihood approaches. Empirically, DDPO is able to adapt text-to-image diffusion models to objectives that are difficult to express via prompting, such as image compressibility, and those derived from human feedback, such as aesthetic quality. Finally, we show that DDPO can improve prompt-image alignment using feedback from a 
    
[^142]: GraphCare: 使用个性化知识图谱提升医疗预测能力

    GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs. (arXiv:2305.12788v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2305.12788](http://arxiv.org/abs/2305.12788)

    本论文提出了一种名为GraphCare的框架，通过使用个性化知识图谱来改进基于电子健康记录的医疗预测，并通过在两个公共数据集上的实验证明了其有效性。

    

    临床预测模型通常依赖于患者的电子健康记录(EHR)，但将医学知识整合到预测和决策中以提高效果具有挑战性。这是因为个性化预测需要个性化的知识图谱(KG)，而从患者EHR数据中生成个性化知识图谱很困难。为了解决这个问题，我们提出了一个名为\textsc{GraphCare}的开放式框架，它使用外部知识图谱来改进基于EHR的预测。我们的方法从大规模语言模型(LLM)和外部生物医学知识图谱中提取知识，构建个体化的患者知识图谱，然后使用我们提出的Bi-attention AugmenTed (BAT)图神经网络(GNN)进行医疗预测训练。在两个公共数据集MIMIC-III和MIMIC-IV上，\textsc{GraphCare}在四个关键的医疗预测任务上均超过了基准线：死亡率、再入院率、住院天数和药物推荐。在MIMIC-III上，它将AUROC提高了17.6%和6.6%，将F1得分提高了7.9%。

    Clinical predictive models often rely on patients' electronic health records (EHR), but integrating medical knowledge to enhance predictions and decision-making is challenging. This is because personalized predictions require personalized knowledge graphs (KGs), which are difficult to generate from patient EHR data. To address this, we propose \textsc{GraphCare}, an open-world framework that uses external KGs to improve EHR-based predictions. Our method extracts knowledge from large language models (LLMs) and external biomedical KGs to build patient-specific KGs, which are then used to train our proposed Bi-attention AugmenTed (BAT) graph neural network (GNN) for healthcare predictions. On two public datasets, MIMIC-III and MIMIC-IV, \textsc{GraphCare} surpasses baselines in four vital healthcare prediction tasks: mortality, readmission, length of stay (LOS), and drug recommendation. On MIMIC-III, it boosts AUROC by 17.6\% and 6.6\% for mortality and readmission, and F1-score by 7.9\% 
    
[^143]: HighLight: 高效灵活的深度神经网络加速器与分层结构稀疏性

    HighLight: Efficient and Flexible DNN Acceleration with Hierarchical Structured Sparsity. (arXiv:2305.12718v2 [cs.AR] UPDATED)

    [http://arxiv.org/abs/2305.12718](http://arxiv.org/abs/2305.12718)

    HighLight是一种高效灵活的DNN加速器，通过分层结构稀疏性实现了对DNN的有效加速，优化了能耗和延迟的权衡。

    

    由于各种深度神经网络（DNN）优化技术之间的复杂相互作用，现代DNN的权重和激活可以是稠密的或稀疏的，其稀疏程度各不相同。为了在准确性和硬件性能之间找到一个良好的权衡，理想的DNN加速器应具有高度灵活性，能够有效地将DNN的稀疏性转化为能量和/或延迟的降低而不会带来显著的复杂性开销。本文介绍了分层结构稀疏性（HSS）, 通过将多个简单稀疏模式层级组合来系统地表示不同的稀疏程度。因此，HSS简化了底层硬件，因为它只需要支持简单稀疏模式，这显著减少了稀疏加速的开销，提高了效率。在这种机会的推动下，我们提出了一种同时高效灵活的加速器，命名为HighLight，用于加速DNN。

    Due to complex interactions among various deep neural network (DNN) optimization techniques, modern DNNs can have weights and activations that are dense or sparse with diverse sparsity degrees. To offer a good trade-off between accuracy and hardware performance, an ideal DNN accelerator should have high flexibility to efficiently translate DNN sparsity into reductions in energy and/or latency without incurring significant complexity overhead.  This paper introduces hierarchical structured sparsity (HSS), with the key insight that we can systematically represent diverse sparsity degrees by having them hierarchically composed from multiple simple sparsity patterns. As a result, HSS simplifies the underlying hardware since it only needs to support simple sparsity patterns; this significantly reduces the sparsity acceleration overhead, which improves efficiency. Motivated by such opportunities, we propose a simultaneously efficient and flexible accelerator, named HighLight, to accelerate D
    
[^144]: 使用指令微调基础模型的多模态 Web 导航。

    Multimodal Web Navigation with Instruction-Finetuned Foundation Models. (arXiv:2305.11854v1 [cs.LG])

    [http://arxiv.org/abs/2305.11854](http://arxiv.org/abs/2305.11854)

    本文研究使用视觉语言基础模型进行数据驱动离线训练的 Web 代理，提出了一个指令跟随多模态代理WebGUM，将微调指令微调语言模型和视觉转换器，能够有效提高代理的基于视觉感知、HTML 理解和多步推理的能力。

    

    自主 Web 导航的进展受到了依赖数十亿次在线强化学习的探索性交互和具有领域特定模型设计的影响，这使得难以利用来自丰富领域外数据的泛化。在本工作中，我们研究了基于数据驱动的脱机训练，用于使用视觉语言基础模型的 Web 代理。我们提出了一个指令跟随多模态代理， WebGUM，它观察了网页截图和 HTML 页面，并输出 Web 导航操作，如单击和输入。WebGUM 是通过联合微调指令微调语言模型和视觉转换器在大量的演示语料库上训练的。我们凭经验证明，这种方法可以提高代理的基于视觉感知、HTML 理解和多步推理的能力，明显优于之前的工作。在 MiniWoB 基准测试中，我们超过之前最佳脱机方法 31.9% 以上，接近实现在线交互的表现。

    The progress of autonomous web navigation has been hindered by the dependence on billions of exploratory interactions via online reinforcement learning, and domain-specific model designs that make it difficult to leverage generalization from rich out-of-domain data. In this work, we study data-driven offline training for web agents with vision-language foundation models. We propose an instruction-following multimodal agent, WebGUM, that observes both webpage screenshots and HTML pages and outputs web navigation actions, such as click and type. WebGUM is trained by jointly finetuning an instruction-finetuned language model and a vision transformer on a large corpus of demonstrations. We empirically demonstrate this recipe improves the agent's ability of grounded visual perception, HTML comprehension and multi-step reasoning, outperforming prior works by a significant margin. On the MiniWoB benchmark, we improve over the previous best offline methods by more than 31.9%, being close to re
    
[^145]: 扩散模型的结构剪枝

    Structural Pruning for Diffusion Models. (arXiv:2305.10924v1 [cs.LG])

    [http://arxiv.org/abs/2305.10924](http://arxiv.org/abs/2305.10924)

    本文提出了一种名为Diff-Pruning的高效压缩方法，通过一个Taylor展开过程来识别重要权重，从而从预先存在的模型中学习轻量级扩散模型，性能稳定，并在训练效率上显著提高。

    

    生成建模最近取得了显著的进展，主要是因为扩散概率模型（DPM）的转型意义。然而，这些模型的令人印象深刻的能力通常涉及到显著的计算开销，在训练和推理期间都是如此。为了应对这一挑战，我们提出了Diff-Pruning，一种专为从预先存在的模型中学习轻量级扩散模型而设计的高效压缩方法，无需进行大量的重新训练。Diff-Pruning的本质是通过剪枝时间步长的Taylor展开，在过滤掉无贡献扩散步骤和整合有信息的梯度来识别重要权重的过程。我们在四个不同数据集上进行的实证评估突出了我们所提出方法的两个主要优点：1）效率：它可以以原始训练投入的仅10％到20％的代价实现约50％的FLOPs减少; 2）一致性: 剪枝后的扩散模型产生的效果与原始模型相当，不会影响生成建模的质量。

    Generative modeling has recently undergone remarkable advancements, primarily propelled by the transformative implications of Diffusion Probabilistic Models (DPMs). The impressive capability of these models, however, often entails significant computational overhead during both training and inference. To tackle this challenge, we present Diff-Pruning, an efficient compression method tailored for learning lightweight diffusion models from pre-existing ones, without the need for extensive re-training. The essence of Diff-Pruning is encapsulated in a Taylor expansion over pruned timesteps, a process that disregards non-contributory diffusion steps and ensembles informative gradients to identify important weights. Our empirical assessment, undertaken across four diverse datasets highlights two primary benefits of our proposed method: 1) Efficiency: it enables approximately a 50% reduction in FLOPs at a mere 10% to 20% of the original training expenditure; 2) Consistency: the pruned diffusio
    
[^146]: 多智能体强化学习中的语义对齐任务分解

    Semantically Aligned Task Decomposition in Multi-Agent Reinforcement Learning. (arXiv:2305.10865v1 [cs.LG])

    [http://arxiv.org/abs/2305.10865](http://arxiv.org/abs/2305.10865)

    该论文提出了一种多智能体强化学习中的新方法SAMA，通过提前训练的语言模型和任务分解来解决ASG方法存在的样本效率问题和生成非实际任务奖励的子目标的问题。

    

    合作型MARL中的奖励稀疏问题着重于适当的信用分配。自动子目标生成（ASG）是最近出现的一种可行的MARL方法，其灵感来自于在内在驱动的增强学习中利用子目标。然而，从稀疏奖励中进行复杂任务规划的端到端学习无疑需要大量的培训样本。为了解决这个问题，我们提出了一种新的"解耦"决策方法，即在MARL中的语义对齐任务分解（SAMA），受到解耦表示学习的启发。

    The difficulty of appropriately assigning credit is particularly heightened in cooperative MARL with sparse reward, due to the concurrent time and structural scales involved. Automatic subgoal generation (ASG) has recently emerged as a viable MARL approach inspired by utilizing subgoals in intrinsically motivated reinforcement learning. However, end-to-end learning of complex task planning from sparse rewards without prior knowledge, undoubtedly requires massive training samples. Moreover, the diversity-promoting nature of existing ASG methods can lead to the "over-representation" of subgoals, generating numerous spurious subgoals of limited relevance to the actual task reward and thus decreasing the sample efficiency of the algorithm. To address this problem and inspired by the disentangled representation learning, we propose a novel "disentangled" decision-making method, Semantically Aligned task decomposition in MARL (SAMA), that prompts pretrained language models with chain-of-thou
    
[^147]: CNN 压缩的评估度量

    Evaluation Metrics for CNNs Compression. (arXiv:2305.10616v1 [cs.LG])

    [http://arxiv.org/abs/2305.10616](http://arxiv.org/abs/2305.10616)

    本文提供了神经网络压缩的评估度量综述，从而为标准化神经网络压缩贡献力量。

    

    研究人员致力于开发不同的神经网络压缩技术，但社区似乎缺乏标准化的评估和比较不同压缩技术的方法，这是识别不同应用程序的最合适的压缩技术的关键。本文通过提出评估度量的综述来为神经网络压缩的标准化贡献。这些度量已被实现到NetZIP，一个标准化的神经网络压缩基准之中。我们通过三个案例研究展示一些被审查的度量，分别聚焦于目标分类、目标检测和边缘设备。

    There is a lot of research effort devoted by researcher into developing different techniques for neural networks compression, yet the community seems to lack standardised ways of evaluating and comparing between different compression techniques, which is key to identifying the most suitable compression technique for different applications. In this paper we contribute towards standardisation of neural network compression by providing a review of evaluation metrics. These metrics have been implemented into NetZIP, a standardised neural network compression bench. We showcase some of the metrics reviewed using three case studies focusing on object classification, object detection, and edge devices.
    
[^148]: 降维处理极端气动学问题

    Grasping Extreme Aerodynamics on a Low-Dimensional Manifold. (arXiv:2305.08024v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2305.08024](http://arxiv.org/abs/2305.08024)

    研究探索处理极端气动学问题的基本物理机制方法。

    

    现代航空器执行广泛的任务，包括运输、国防、监视和救援。这些飞机可以在平静的条件下飞行，但避免在窄谷，山区和船舶尾迹等多风区操作。其中，小型飞机尤其容易受到这些气流干扰。随着全球变暖使极端天气越来越频繁，预计飞机，尤其是那些较小的飞机，会遇到大规模的大气干扰，但仍应执行稳定的飞行。然而，目前几乎没有理论来描述强烈的气流对飞行器的影响。更为复杂的是，翼面遇到气流的参数空间非常大，而不同参数组合之间的涡旋气流和翼的相互作用似乎具有不同的特点。本研究展示了一种处理这一问题的基本物理机制的方法。

    Modern air vehicles perform a wide range of operations, including transportation, defense, surveillance, and rescue. These aircraft can fly in calm conditions but avoid operations in gusty environments, which are seen in urban canyons, over mountainous terrains, and in ship wakes. Smaller aircraft are especially prone to such gust disturbances. With extreme weather becoming ever more frequent due to global warming, it is anticipated that aircraft, especially those that are smaller in size, encounter large-scale atmospheric disturbances and still be expected to manage stable flight. However, there exists virtually no foundation to describe the influence of extreme vortical gusts on flying bodies. To compound on this difficult problem, there is an enormous parameter space for gusty conditions wings encounter. While the interaction between the vortical gusts and wings is seemingly complex and different for each combination of gust parameters, we show in this study that the fundamental phy
    
[^149]: 用扩散模型解决反问题的变分视角

    A Variational Perspective on Solving Inverse Problems with Diffusion Models. (arXiv:2305.04391v1 [cs.LG])

    [http://arxiv.org/abs/2305.04391](http://arxiv.org/abs/2305.04391)

    该论文提出了一种通过去噪扩散过程自然地导致正则化的变分方法（RED-Diff），可用于解决不同反问题。加权机制可以衡量不同时间步长的去噪器的贡献。

    

    扩散模型已成为视觉领域基础模型的关键支柱之一。其中一个关键应用是通过单个扩散先验普遍解决不同的反问题，而无需为每个任务重新训练。然而，由于扩散过程的非线性和迭代性质使得后验难以处理，因此我们提出了一种变分方法，旨在近似真实后验分布。我们展示了我们的方法通过去噪扩散过程（RED-Diff）自然地导致正则化，其中来自不同时间步长的去噪器同时对图像施加不同的结构约束。为了衡量不同时间步长的去噪器的贡献，我们提出了一种基于信号-t的加权机制。

    Diffusion models have emerged as a key pillar of foundation models in visual domains. One of their critical applications is to universally solve different downstream inverse tasks via a single diffusion prior without re-training for each task. Most inverse tasks can be formulated as inferring a posterior distribution over data (e.g., a full image) given a measurement (e.g., a masked image). This is however challenging in diffusion models since the nonlinear and iterative nature of the diffusion process renders the posterior intractable. To cope with this challenge, we propose a variational approach that by design seeks to approximate the true posterior distribution. We show that our approach naturally leads to regularization by denoising diffusion process (RED-Diff) where denoisers at different timesteps concurrently impose different structural constraints over the image. To gauge the contribution of denoisers from different timesteps, we propose a weighting mechanism based on signal-t
    
[^150]: 基于半监督学习的高维贝叶斯优化及优化无标签数据采样

    High-dimensional Bayesian Optimization via Semi-supervised Learning with Optimized Unlabeled Data Sampling. (arXiv:2305.02614v1 [cs.LG])

    [http://arxiv.org/abs/2305.02614](http://arxiv.org/abs/2305.02614)

    本文提出基于半监督学习的高维贝叶斯优化方法，利用特定的未标记数据采样、参数化采样分布的优化及动态选择无标记数据等策略，解决了高维贝叶斯优化难以处理的问题。

    

    贝叶斯优化（BO）是一种寻找黑箱函数全局最优解的强大工具。虽然黑箱函数的评估成本往往很高，但减少昂贵标记数据的使用是理想的。本文首次提出了一种教师-学生模型，利用半监督学习在BO环境下利用大量未标记的数据。其中，关键在于选择验证和未标记数据以提高BO的表现。为了优化无标签数据的采样，我们采用黑箱参数化采样分布，将其优化为所采用双层优化框架的一部分。更进一步，通过从动态适应的极值分布中选择未标签数据，我们证明了BO的性能可以进一步提高。我们的BO方法在学习后的低维潜在空间中运行，使其可扩展到高维问题。

    Bayesian optimization (BO) is a powerful tool for seeking the global optimum of black-box functions. While evaluations of the black-box functions can be highly costly, it is desirable to reduce the use of expensive labeled data. For the first time, we introduce a teacher-student model to exploit semi-supervised learning that can make use of large amounts of unlabelled data under the context of BO. Importantly, we show that the selection of the validation and unlabeled data is key to the performance of BO. To optimize the sampling of unlabeled data, we employ a black-box parameterized sampling distribution optimized as part of the employed bi-level optimization framework. Taking one step further, we demonstrate that the performance of BO can be further improved by selecting unlabeled data from a dynamically fitted extreme value distribution. Our BO method operates in a learned latent space with reduced dimensionality, making it scalable to high-dimensional problems. The proposed approac
    
[^151]: 差分隐私下的上下文学习

    Differentially Private In-Context Learning. (arXiv:2305.01639v1 [cs.LG])

    [http://arxiv.org/abs/2305.01639](http://arxiv.org/abs/2305.01639)

    本文提出了DP-ICL，实现了在隐私保证下对新任务的适应性。经过四个基准测试，发现其性能与非私有ICL相当。

    

    在部署大型语言模型（LLM）时，一个重要的问题是如何使用私有数据增强LLM。我们提出了"DP-ICL"来实现对新任务的适应性，同时保持隐私保证。DP-ICL通过使用"report-noisy-max"机制在示例集合上建立嘈杂一致性来进行私有推断。我们在四个基准测试上评估了DP-ICL，发现其与非私有ICL相比具有可比性的性能(<2%降级)。

    An important question in deploying large language models (LLMs) is how to augment LLMs with private data. We propose Differentially Private In-context Learning (DP-ICL) to enable LLMs to adapt to new tasks while maintaining privacy guarantees. DP-ICL performs private inference by establishing noisy consensus over an ensemble of exemplars using the Report-Noisy-Max mechanism. We evaluate DP-ICL on four benchmarks and find that it achieves comparable performance (<2\% degradation) with non-private ICL.
    
[^152]: 实现高效和全面的城市时空预测：一个统一的库和性能基准

    Towards Efficient and Comprehensive Urban Spatial-Temporal Prediction: A Unified Library and Performance Benchmark. (arXiv:2304.14343v1 [cs.LG])

    [http://arxiv.org/abs/2304.14343](http://arxiv.org/abs/2304.14343)

    本研究提出了一种称为原子文件的统一空间时间数据存储格式，开发了一个名为LibCity的开源库，重新构建了65个空间时间预测模型，并收集了55个空间时间数据集。同时还提出了城市时空预测模型的性能基准，为这一领域提供了一个可靠的评估工具。

    

    随着深度学习技术的不断推进和城市时空数据的积累，越来越多的深度学习模型被提出来解决城市时空预测问题。然而，现有领域存在许多限制，包括开放数据以各种格式存在，使用困难，极少数论文公开其代码和数据，以及开源模型经常使用不同的框架和平台，使得比较具有挑战性。迫切需要一个统一的框架来实施和评估这些方法。为解决这些问题，我们提供了一个城市时空预测的综合评估，并提出了一种称为原子文件的统一空间时间数据存储格式。我们还提出了一个名为LibCity的开源库，为研究人员提供了一个可靠的实验工具和一个方便的开发框架。在这个库中，我们已经重新构建了65个空间时间预测模型，并收集了55个空间时间数据集。此外，我们还引入了一个城市时空预测模型性能基准，包括效率和有效性度量，以进行公平比较。在这个基准上的实验结果证明了我们提出的统一库和基准的有用性和有效性。

    As deep learning technology advances and more urban spatial-temporal data accumulates, an increasing number of deep learning models are being proposed to solve urban spatial-temporal prediction problems. However, there are limitations in the existing field, including open-source data being in various formats and difficult to use, few papers making their code and data openly available, and open-source models often using different frameworks and platforms, making comparisons challenging. A standardized framework is urgently needed to implement and evaluate these methods. To address these issues, we provide a comprehensive review of urban spatial-temporal prediction and propose a unified storage format for spatial-temporal data called atomic files. We also propose LibCity, an open-source library that offers researchers a credible experimental tool and a convenient development framework. In this library, we have reproduced 65 spatial-temporal prediction models and collected 55 spatial-temp
    
[^153]: Total-Recon: 可变形场景重建用于实体视角合成

    Total-Recon: Deformable Scene Reconstruction for Embodied View Synthesis. (arXiv:2304.12317v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2304.12317](http://arxiv.org/abs/2304.12317)

    本论文介绍了Total-Recon，这是第一个从长的单目RGBD视频中实现光实感的可变形场景重建方法，用于合成实体视角。

    

    我们研究了从可变形场景的单目视频中合成实体视角的任务。给定一个长达一分钟的RGBD视频，拍摄了人与他们宠物互动的场景，我们从场景中的运动派生出新的相机轨迹，渲染出该场景：(1)模拟目标演员视角的自我相机和(2)跟随演员的第三人称相机。构建这样的系统需要重建每个演员的根-身体和关节运动，以及支持自由视点合成的场景表示。长视频更有可能从不同的视角捕捉到场景（有助于重建），但也更有可能包含较大的运动（使重建变得复杂）。为了解决这些挑战，我们提出了Total-Recon，这是第一个从长的单目RGBD视频中实现光实感的可变形场景重建方法。关键是，为了适应长视频，我们的方法将场景分层分解为背景和前景，并使用不同的深度估计技术进行重建。

    We explore the task of embodied view synthesis from monocular videos of deformable scenes. Given a minute-long RGBD video of people interacting with their pets, we render the scene from novel camera trajectories derived from the in-scene motion of actors: (1) egocentric cameras that simulate the point of view of a target actor and (2) 3rd-person cameras that follow the actor. Building such a system requires reconstructing the root-body and articulated motion of every actor, as well as a scene representation that supports free-viewpoint synthesis. Longer videos are more likely to capture the scene from diverse viewpoints (which helps reconstruction) but are also more likely to contain larger motions (which complicates reconstruction). To address these challenges, we present Total-Recon, the first method to photorealistically reconstruct deformable scenes from long monocular RGBD videos. Crucially, to scale to long videos, our method hierarchically decomposes the scene into the backgroun
    
[^154]: 通过改进的积分逼近加速扩散采样过程

    On Accelerating Diffusion-Based Sampling Process via Improved Integration Approximation. (arXiv:2304.11328v1 [cs.LG])

    [http://arxiv.org/abs/2304.11328](http://arxiv.org/abs/2304.11328)

    本文提出了一种通过优化系数来加速流行的基于反向ODE解算的扩散采样过程的方法，优化方法为改进的积分逼近（IIA），在每个反向时间步长，我们建议针对某些选择的系数最小化MSE函数，给定预训练的扩散模型，只需要在一批样本上进行特定数量的神经功能评估（NFEs）一次IIA过程即可获得最佳解。

    

    一种流行的基于扩散的采样策略尝试有效地解决反向常微分方程（ODEs）。所得ODE求解器的系数由ODE公式，反向离散的时间步长和使用的ODE方法预先确定。本文考虑通过优化某些系数来加速几种流行的基于ODE的采样过程，优化方法为改进的积分逼近（IIA）。在每个反向时间步长，我们建议针对某些选择的系数最小化均方误差（MSE）函数。通过应用一组细粒度时间步长的原始ODE求解器构造MSE，从原理上提供了更精确的积分逼近，以预测下一个扩散隐藏状态，给定预训练的扩散模型，只需要在一批样本上进行特定数量的神经功能评估（NFEs）一次IIA过程即可获得最佳解

    One popular diffusion-based sampling strategy attempts to solve the reverse ordinary differential equations (ODEs) effectively. The coefficients of the obtained ODE solvers are pre-determined by the ODE formulation, the reverse discrete timesteps, and the employed ODE methods. In this paper, we consider accelerating several popular ODE-based sampling processes by optimizing certain coefficients via improved integration approximation (IIA). At each reverse timestep, we propose to minimize a mean squared error (MSE) function with respect to certain selected coefficients. The MSE is constructed by applying the original ODE solver for a set of fine-grained timesteps which in principle provides a more accurate integration approximation in predicting the next diffusion hidden state. Given a pre-trained diffusion model, the procedure for IIA for a particular number of neural functional evaluations (NFEs) only needs to be conducted once over a batch of samples. The obtained optimal solutions f
    
[^155]: 利用保序机制提高机器学习和人工智能会议的同行评审

    The Isotonic Mechanism for Exponential Family Estimation. (arXiv:2304.11160v1 [math.ST])

    [http://arxiv.org/abs/2304.11160](http://arxiv.org/abs/2304.11160)

    本文利用扩展的保序机制，将其应用于指数族分布以提高同行评审的质量，并发现作者的同行评分可以较准确地在不需要知道具体分布情况下进行调整。

    

    本文致力于扩展保序机制，将其应用于指数族分布以提高同行评审的质量。该机制可生成与原始评分接近的调整分数，并符合作者指定的排名要求，得到广泛的指数族分布应用，而且不需要知道具体的分布形式。研究表明，在一定的指数族分布下，如果作者的效用函数采用简单的凸可加函数，则激励作者提供准确的排名建议。

    In 2023, the International Conference on Machine Learning (ICML) required authors with multiple submissions to rank their submissions based on perceived quality. In this paper, we aim to employ these author-specified rankings to enhance peer review in machine learning and artificial intelligence conferences by extending the Isotonic Mechanism (Su, 2021, 2022) to exponential family distributions. This mechanism generates adjusted scores closely align with the original scores while adhering to author-specified rankings. Despite its applicability to a broad spectrum of exponential family distributions, this mechanism's implementation does not necessitate knowledge of the specific distribution form. We demonstrate that an author is incentivized to provide accurate rankings when her utility takes the form of a convex additive function of the adjusted review scores. For a certain subclass of exponential family distributions, we prove that the author reports truthfully only if the question in
    
[^156]: 基于样本效率的模型驱动量子控制强化学习

    Sample-efficient Model-based Reinforcement Learning for Quantum Control. (arXiv:2304.09718v1 [quant-ph])

    [http://arxiv.org/abs/2304.09718](http://arxiv.org/abs/2304.09718)

    本论文提出了一种基于模型的强化学习方法，通过受到神经常微分方程进展的启发，这个方法采用自动微分的ODE表达由可学习的汉密尔顿安排参数化的模型来近似环境，在门控制和汉密尔顿参数的学习中通过系统交互解决问题。该方法在样本复杂度方面比标准基于模型自由的强化学习方法具有一个数量级的优势，适用于噪声时变门优化。

    

    我们提出了一种基于模型的强化学习方法，用于噪声时变门优化，其样本复杂度优于基于模型自由的强化学习。样本复杂度是控制器与物理系统交互的次数。借助一个归纳偏置，受最近神经常微分方程的进展启发，我们使用可微的ODE，其由可学习的汉密尔顿安排参数化，以表示模型近似环境，其时变部分（包括控制）完全已知。控制器和连续时域独立参数的汉密尔顿学习是通过与系统的交互来解决的。在真实数值实验中，我们展示了使用我们方法在准备一些标准单量子门的闭合和开放系统动态时，在样本复杂度方面与标准模型自由强化学习相比，具有一个数量级的优势，这包括单次测量、任意希尔伯特空间截断和不确定性等。

    We propose a model-based reinforcement learning (RL) approach for noisy time-dependent gate optimization with improved sample complexity over model-free RL. Sample complexity is the number of controller interactions with the physical system. Leveraging an inductive bias, inspired by recent advances in neural ordinary differential equations (ODEs), we use an auto-differentiable ODE parametrised by a learnable Hamiltonian ansatz to represent the model approximating the environment whose time-dependent part, including the control, is fully known. Control alongside Hamiltonian learning of continuous time-independent parameters is addressed through interactions with the system. We demonstrate an order of magnitude advantage in the sample complexity of our method over standard model-free RL in preparing some standard unitary gates with closed and open system dynamics, in realistic numerical experiments incorporating single shot measurements, arbitrary Hilbert space truncations and uncertaint
    
[^157]: 贝叶斯分层建模中主动学习回归中的动态探索-开发权衡

    Dynamic Exploration-Exploitation Trade-Off in Active Learning Regression with Bayesian Hierarchical Modeling. (arXiv:2304.07665v1 [cs.LG])

    [http://arxiv.org/abs/2304.07665](http://arxiv.org/abs/2304.07665)

    本文提出了一个新方法，利用贝叶斯分层建模，动态平衡探索-开发权衡，以更好地查询数据点。

    

    主动学习提供了一种自适应采样最具信息的实验以学习未知的黑盒函数的框架。本文提出了一种贝叶斯分层方法来动态平衡探索-开发权衡，以更好地查询数据点。

    Active learning provides a framework to adaptively sample the most informative experiments towards learning an unknown black-box function. Various approaches of active learning have been proposed in the literature, however, they either focus on exploration or exploitation in the design space. Methods that do consider exploration-exploitation simultaneously employ fixed or ad-hoc measures to control the trade-off that may not be optimal. In this paper, we develop a Bayesian hierarchical approach to dynamically balance the exploration-exploitation trade-off as more data points are queried. We subsequently formulate an approximate Bayesian computation approach based on the linear dependence of data samples in the feature space to sample from the posterior distribution of the trade-off parameter obtained from the Bayesian hierarchical model. Simulated and real-world examples show the proposed approach achieves at least 6% and 11% average improvement when compared to pure exploration and ex
    
[^158]: OKRidge: 用于学习动态系统的可扩展 k 稀疏岭回归

    OKRidge: Scalable Optimal k-Sparse Ridge Regression for Learning Dynamical Systems. (arXiv:2304.06686v1 [cs.LG])

    [http://arxiv.org/abs/2304.06686](http://arxiv.org/abs/2304.06686)

    本文提出了一种名为 OKRidge 的方法，用于确定非线性动态系统的稀疏控制方程，并通过求解稀疏岭回归问题，实现了可扩展性和快速性，和现有方法相比，有着更高的效率。

    

    本文解决了科学发现中的一个重要问题，即，确定非线性动态系统的稀疏控制方程，通过求解稀疏岭回归问题可以证明最优性，以确定驱动基础动态的项。我们提出了一种称为 OKRidge 的快速算法，用一种新颖的下界计算方法，涉及鞍点公式，然后使用线性系统或基于 ADMM 的方法来解决，其中可以通过解决另一个线性系统和单调回归问题来有效地计算近端算子。我们还提出了一种启动我们求解器的方法，利用了波束搜索。在实验中，我们的方法达到可证明的最优性，并且运行时间比商业求解器 Gurobi 解决的现有 MIP公式运行时间快几个数量级。

    We consider an important problem in scientific discovery, identifying sparse governing equations for nonlinear dynamical systems. This involves solving sparse ridge regression problems to provable optimality in order to determine which terms drive the underlying dynamics. We propose a fast algorithm, OKRidge, for sparse ridge regression, using a novel lower bound calculation involving, first, a saddle point formulation, and from there, either solving (i) a linear system or (ii) using an ADMM-based approach, where the proximal operators can be efficiently evaluated by solving another linear system and an isotonic regression problem. We also propose a method to warm-start our solver, which leverages a beam search. Experimentally, our methods attain provable optimality with run times that are orders of magnitude faster than those of the existing MIP formulations solved by the commercial solver Gurobi.
    
[^159]: 能量引导的熵神经最优输运

    Energy-guided Entropic Neural Optimal Transport. (arXiv:2304.06094v1 [cs.LG])

    [http://arxiv.org/abs/2304.06094](http://arxiv.org/abs/2304.06094)

    本论文提出了一种新的方法，将能量基础模型和熵正则化最优输运结合起来，以解决生成建模问题。我们在2D情景和图像到图像翻译问题中验证了该方法的适用性。

    

    能量基础模型（EBMs）在机器学习社区已经有数十年的历史。自两千年代起，一直有很多高效的方法通过能量势（非归一化的似然函数）来解决生成建模问题。相比之下，最优输运（OT）领域，尤其是神经OT求解器，受到的探索要少得多，仅有一些近期的研究（不包括利用OT作为损失函数来解决问题的WGAN方法）。在本研究中，我们弥合了EBMs和熵正则化OT之间的差距，提出了一种新的方法，允许利用前者的最新发展和技术改进来丰富后者。我们在2D情景和标准的图像到图像翻译问题中验证了我们方法的适用性。为简单起见，我们选择了简短和长跑的EBMs。

    Energy-Based Models (EBMs) are known in the Machine Learning community for the decades. Since the seminal works devoted to EBMs dating back to the noughties there have been appearing a lot of efficient methods which solve the generative modelling problem by means of energy potentials (unnormalized likelihood functions). In contrast, the realm of Optimal Transport (OT) and, in particular, neural OT solvers is much less explored and limited by few recent works (excluding WGAN based approaches which utilize OT as a loss function and do not model OT maps themselves). In our work, we bridge the gap between EBMs and Entropy-regularized OT. We present the novel methodology which allows utilizing the recent developments and technical improvements of the former in order to enrich the latter. We validate the applicability of our method on toy 2D scenarios as well as standard unpaired image-to-image translation problems. For the sake of simplicity, we choose simple short- and long- run EBMs as a 
    
[^160]: 面向类别不均问题的集成学习和数据增强模型综述：组合、实现和评估

    A review of ensemble learning and data augmentation models for class imbalanced problems: combination, implementation and evaluation. (arXiv:2304.02858v1 [cs.LG])

    [http://arxiv.org/abs/2304.02858](http://arxiv.org/abs/2304.02858)

    本文研究了集成学习和数据增强方法的应用，针对类别不平衡问题，通过计算评估，找到了最有效的组合。

    

    分类问题中的类别不平衡（CI）是指属于一个类的观测值数量低于其他类的数量。集成学习结合数据增强方法已被广泛应用于解决类别不平衡问题。在过去的十年里，一些策略已经被应用于增强集成学习和数据增强方法，同时还开发了一些新方法，如生成对抗网络（GAN）。本文对用于解决基准CI问题的数据增强和集成学习方法进行计算评估。我们提出了一个评估CI问题的10个数据增强方法和10个集成学习方法的通用框架。我们的目标是识别提高分类效果最有效的组合。

    Class imbalance (CI) in classification problems arises when the number of observations belonging to one class is lower than the other classes. Ensemble learning that combines multiple models to obtain a robust model has been prominently used with data augmentation methods to address class imbalance problems. In the last decade, a number of strategies have been added to enhance ensemble learning and data augmentation methods, along with new methods such as generative adversarial networks (GANs). A combination of these has been applied in many studies, but the true rank of different combinations would require a computational review. In this paper, we present a computational review to evaluate data augmentation and ensemble learning methods used to address prominent benchmark CI problems. We propose a general framework that evaluates 10 data augmentation and 10 ensemble learning methods for CI problems. Our objective was to identify the most effective combination for improving classificat
    
[^161]: SEENN: 实现时间编码早期退出神经网络的研究

    SEENN: Towards Temporal Spiking Early-Exit Neural Networks. (arXiv:2304.01230v1 [cs.NE])

    [http://arxiv.org/abs/2304.01230](http://arxiv.org/abs/2304.01230)

    本研究提出了一种名为SEENN的方法，通过对时间步数进行细粒度调整，以减少不必要的计算并提高有效性。同时，SEENN达到了多个基准数据集的最先进准确度表现。

    

    脉冲神经网络（SNNs）因其生物学特性成为传统人工神经网络（ANNs）的替代品，最近变得越来越流行。SNNs既费用效益又易于部署，因为它们可以用二进制脉冲以空间和时间方式处理输入。然而，我们观察到SNNs中的信息容量受到时间步骤数量的影响，导致准确性和效率的权衡。在本研究中，我们研究了SNNs中时间步骤数量的细粒度调整。具体地，我们将时间步数视为一个变量，针对不同的输入样本来减少冗余时间步骤。我们称这种方法为早期退出脉冲神经网络（SEENN）。为了确定适当的时间步数，我们提出了SEENN-I，它使用置信度阈值来过滤不确定的预测，以及SEENN-II，它通过强化学习确定时间步骤的数量。此外，我们证明SEENN比传统SNNs更有效，并在几个基准数据集上实现了最先进的准确性。

    Spiking Neural Networks (SNNs) have recently become more popular as a biologically plausible substitute for traditional Artificial Neural Networks (ANNs). SNNs are cost-efficient and deployment-friendly because they process input in both spatial and temporal manners using binary spikes. However, we observe that the information capacity in SNNs is affected by the number of timesteps, leading to an accuracy-efficiency tradeoff. In this work, we study a fine-grained adjustment of the number of timesteps in SNNs. Specifically, we treat the number of timesteps as a variable conditioned on different input samples to reduce redundant timesteps for certain data. We call our method Spiking Early-Exit Neural Networks (SEENNs). To determine the appropriate number of timesteps, we propose SEENN-I which uses a confidence score thresholding to filter out the uncertain predictions, and SEENN-II which determines the number of timesteps by reinforcement learning. Moreover, we demonstrate that SEENN is 
    
[^162]: 最大似然方法再探：Kullback - Leibler 散度中的规范对称性和性能保证的正则化

    Maximum likelihood method revisited: Gauge symmetry in Kullback -- Leibler divergence and performance-guaranteed regularization. (arXiv:2303.16721v1 [stat.ML])

    [http://arxiv.org/abs/2303.16721](http://arxiv.org/abs/2303.16721)

    本文提出了一种在最大似然方法中进行正则化的理论上保证的方法，通过关注 Kullback - Leibler 散度中的规范对称性，可以获得最优的模型。该方法不需要频繁搜索正则化的超参数。

    

    最大似然方法是估计数据背后概率的最知名方法。然而，传统方法获得与经验分布最接近的概率模型，导致过度拟合。然后，正则化方法可以防止模型过度接近错误的概率，但是对它们的性能知之甚少。正则化的思想类似于纠错代码，通过将次优解与错误接收到的代码混合，获得最优解码。纠错代码中的最优解码是基于规范对称性实现的。我们通过关注 Kullback - Leibler 散度中的规范对称性，提出了最大似然方法中的理论上保证的正则化。在我们的方法中，我们可以获得最优的模型，而无需频繁搜索正则化中经常出现的超参数。

    The maximum likelihood method is the best-known method for estimating the probabilities behind the data. However, the conventional method obtains the probability model closest to the empirical distribution, resulting in overfitting. Then regularization methods prevent the model from being excessively close to the wrong probability, but little is known systematically about their performance. The idea of regularization is similar to error-correcting codes, which obtain optimal decoding by mixing suboptimal solutions with an incorrectly received code. The optimal decoding in error-correcting codes is achieved based on gauge symmetry. We propose a theoretically guaranteed regularization in the maximum likelihood method by focusing on a gauge symmetry in Kullback -- Leibler divergence. In our approach, we obtain the optimal model without the need to search for hyperparameters frequently appearing in regularization.
    
[^163]: 混合最小二乘深度神经网络在内部测量下的导电率成像

    Conductivity Imaging from Internal Measurements with Mixed Least-Squares Deep Neural Networks. (arXiv:2303.16454v1 [math.NA])

    [http://arxiv.org/abs/2303.16454](http://arxiv.org/abs/2303.16454)

    本论文提出了一种新的通过深度神经网络从内部测量中重构椭圆问题中的导电率分布的方法，并在连续和经验损失的神经网络逼近上进行了深入的分析，展示了该方法对于数据噪声的优异稳定性以及解决高维问题的能力。

    

    在这项工作中，我们使用深度神经网络开发了一种新的方法，从一个内部测量中重构椭圆问题中的导电率分布。该方法基于控制方程的混合改造，并利用标准的最小二乘目标函数同时近似导电率和通量，以深度神经网络作为试探函数。我们对连续和经验损失的神经网络逼近进行了深入的分析，包括通过噪声水平、各种惩罚参数和神经网络结构参数（深度、宽度和参数边界）显式地估计误差的严格估计。我们还进行了广泛的数值实验，以展示该方法的不同特点，例如对于数据噪声的优异稳定性以及解决高维问题的能力。

    In this work we develop a novel approach using deep neural networks to reconstruct the conductivity distribution in elliptic problems from one internal measurement. The approach is based on a mixed reformulation of the governing equation and utilizes the standard least-squares objective to approximate the conductivity and flux simultaneously, with deep neural networks as ansatz functions. We provide a thorough analysis of the neural network approximations for both continuous and empirical losses, including rigorous error estimates that are explicit in terms of the noise level, various penalty parameters and neural network architectural parameters (depth, width and parameter bound). We also provide extensive numerical experiments in two- and multi-dimensions to illustrate distinct features of the approach, e.g., excellent stability with respect to data noise and capability of solving high-dimensional problems.
    
[^164]: 掩码还原技术：利用掩码自编码器在测试时防御盲目后门攻击

    Mask and Restore: Blind Backdoor Defense at Test Time with Masked Autoencoder. (arXiv:2303.15564v1 [cs.LG])

    [http://arxiv.org/abs/2303.15564](http://arxiv.org/abs/2303.15564)

    本文提出了利用掩码自编码器的盲目防御框架（BDMAE），可以在测试时防御盲目后门攻击，不需要验证数据和模型参数，通过测试图像和 MAE 还原之间的结构相似性和标签一致性来检测后门攻击。

    

    深度神经网络容易受到恶意攻击，攻击者会通过在图像上叠加特殊的触发器来恶意操纵模型行为，这称为后门攻击。现有的后门防御方法通常需要访问一些验证数据和模型参数，这在许多实际应用中是不切实际的，例如当模型作为云服务提供时。为了解决这个问题，本文致力于测试时的盲目后门防御实践，特别是针对黑盒模型。每个测试图像的真实标签需要从可疑模型的硬标签预测中恢复。然而，在图像空间中启发式触发器搜索不适用于复杂触发器或高分辨率的图片。我们通过利用通用图像生成模型，提出了一种利用掩码自编码器的盲目防御框架（BDMAE），通过测试图像和 MAE 还原之间的结构相似性和标签一致性来检测后门攻击。

    Deep neural networks are vulnerable to backdoor attacks, where an adversary maliciously manipulates the model behavior through overlaying images with special triggers. Existing backdoor defense methods often require accessing a few validation data and model parameters, which are impractical in many real-world applications, e.g., when the model is provided as a cloud service. In this paper, we address the practical task of blind backdoor defense at test time, in particular for black-box models. The true label of every test image needs to be recovered on the fly from the hard label predictions of a suspicious model. The heuristic trigger search in image space, however, is not scalable to complex triggers or high image resolution. We circumvent such barrier by leveraging generic image generation models, and propose a framework of Blind Defense with Masked AutoEncoder (BDMAE). It uses the image structural similarity and label consistency between the test image and MAE restorations to detec
    
[^165]: 通过扩散可控地反向黑盒人脸识别模型状态

    Controllable Inversion of Black-Box Face-Recognition Models via Diffusion. (arXiv:2303.13006v1 [cs.CV])

    [http://arxiv.org/abs/2303.13006](http://arxiv.org/abs/2303.13006)

    ID3PM方法通过扩散过程的随机性质来产生黑盒人脸识别模型的高度可控的反演，能够生成逼真且多样的输出，适用于数据增强、对抗性攻击和生成个性化的训练数据等多种应用。

    

    人脸识别模型将人脸图像嵌入低维身份向量中，包含身份特征的抽象编码，这些特征允许区分个体。我们面临着在没有完全模型访问的情况下（即黑盒设置下）反向预训练人脸识别模型的潜在空间的挑战。已经有许多方法提出解决这个问题，但它们存在严重的缺陷，如缺乏现实输出、推理时间长以及对数据集和人脸识别模型的可访问性有强烈的要求。通过对黑盒反演问题的分析，我们展示了条件性扩散模型漏洞的自然涌现，并且即使没有身份特征的损失，我们也可以有效地从反向分布中进行采样。我们的方法名为身份去噪扩散概率模型（ID3PM），利用去噪扩散过程的随机性质来产生黑盒人脸识别模型的高度可控的反演。该方法能够生成逼真且多样的输出，使其适用于数据增强、对抗性攻击和生成个性化的训练数据等多种应用。

    Face recognition models embed a face image into a low-dimensional identity vector containing abstract encodings of identity-specific facial features that allow individuals to be distinguished from one another. We tackle the challenging task of inverting the latent space of pre-trained face recognition models without full model access (i.e. black-box setting). A variety of methods have been proposed in literature for this task, but they have serious shortcomings such as a lack of realistic outputs, long inference times, and strong requirements for the data set and accessibility of the face recognition model. Through an analysis of the black-box inversion problem, we show that the conditional diffusion model loss naturally emerges and that we can effectively sample from the inverse distribution even without an identity-specific loss. Our method, named identity denoising diffusion probabilistic model (ID3PM), leverages the stochastic nature of the denoising diffusion process to produce hi
    
[^166]: (深度)强化学习中的连接超水平集及其在极小极大定理中的应用

    Connected Superlevel Set in (Deep) Reinforcement Learning and its Application to Minimax Theorems. (arXiv:2303.12981v1 [cs.LG])

    [http://arxiv.org/abs/2303.12981](http://arxiv.org/abs/2303.12981)

    本文研究了强化学习中的策略优化问题，并证明了优化函数超水平集在网络类策略和表格式下始终是连通的，并应用此结果导出了鲁棒性强化学习的极小极大定理。

    

    本文的目的是改善强化学习中策略优化问题的优化函数图景理解。具体而言，我们证明了策略参数的优化目标函数的超水平集，在网络类策略和表格式下始终是连通的。同时，我们证明了奖励作为超水平集的函数满足更强的“等连通”性质。此外，我们还应用这些超水平集的连通性结果，导出了鲁棒性强化学习的极小极大定理。我们发现，任何一个在一侧为凸的，另一侧为等连通的极小极大优化问题都有纳什均衡。这些结论是新颖而且之前未知的。

    The aim of this paper is to improve the understanding of the optimization landscape for policy optimization problems in reinforcement learning. Specifically, we show that the superlevel set of the objective function with respect to the policy parameter is always a connected set both in the tabular setting and under policies represented by a class of neural networks. In addition, we show that the optimization objective as a function of the policy parameter and reward satisfies a stronger "equiconnectedness" property. To our best knowledge, these are novel and previously unknown discoveries.  We present an application of the connectedness of these superlevel sets to the derivation of minimax theorems for robust reinforcement learning. We show that any minimax optimization program which is convex on one side and is equiconnected on the other side observes the minimax equality (i.e. has a Nash equilibrium). We find that this exact structure is exhibited by an interesting robust reinforceme
    
[^167]: 多模态变分自编码器用于跨多种成像模态进行规范建模

    Multi-modal Variational Autoencoders for normative modelling across multiple imaging modalities. (arXiv:2303.12706v1 [cs.CV])

    [http://arxiv.org/abs/2303.12706](http://arxiv.org/abs/2303.12706)

    本文提出了一种多模态规范建模框架，能够更好地检测出多种成像和生物变量中的异常性，特别适用于研究带有异质性的疾病。

    

    研究常见神经疾病的挑战之一是疾病异质性，包括病因、神经成像特征、合并症或基因变异的差异。规范建模已成为研究这种人群的流行方法，其中对生理系统的“正常”行为进行建模，并可以用于个体层面上检测与疾病病理相关的偏差。对于许多异质性疾病，我们预计会观察到多种神经成像和生物变量的异常。然而，到目前为止，规范模型主要是为了研究单一成像模态而开发的。我们旨在开发一种多模态规范建模框架，在多个模态的变量中聚合异常性，并且比单模式基线更能检测到偏差。我们提出了两种用于检测T1和DTI数据中的个体层面偏差的多模态VAE规范模型。与单模式基线相比，我们提出的模型能够更好地检测到轻度认知受损的受试者中的偏差，证明了多模态规范建模用于疾病异质性的潜力。

    One of the challenges of studying common neurological disorders is disease heterogeneity including differences in causes, neuroimaging characteristics, comorbidities, or genetic variation. Normative modelling has become a popular method for studying such cohorts where the 'normal' behaviour of a physiological system is modelled and can be used at subject level to detect deviations relating to disease pathology. For many heterogeneous diseases, we expect to observe abnormalities across a range of neuroimaging and biological variables. However, thus far, normative models have largely been developed for studying a single imaging modality. We aim to develop a multi-modal normative modelling framework where abnormality is aggregated across variables of multiple modalities and is better able to detect deviations than uni-modal baselines. We propose two multi-modal VAE normative models to detect subject level deviations across T1 and DTI data. Our proposed models were better able to detect di
    
[^168]: 行为健康个性化介入的政策优化

    Policy Optimization for Personalized Interventions in Behavioral Health. (arXiv:2303.12206v1 [cs.LG])

    [http://arxiv.org/abs/2303.12206](http://arxiv.org/abs/2303.12206)

    研究如何通过数字平台传递的行为健康介入最大化健康结果和治疗成本，提出了一个名为DecompPI的新算法，从离线数据进行预测任务，减轻了在线实验的需要，并在理论上证明了该算法的可扩展性和渐近收敛性。

    

    问题定义：通过数字平台传递的行为健康介入，通过教育，激励，提醒和外展，有望显着改善健康结果。我们研究了在介入具有成本和能力限制的情况下，优化患者个性化介入以最大化某种长期结果的问题。方法/结果：本文提供了一种无模型方法来解决这个问题。我们发现，来自增强学习文献的通用无模型方法对于医疗应用来说过于数据密集，而更简单的赌臂问题方法取得了进展，但忽略了长期患者动态。我们提出了一种新算法，称为DecompPI，它近似于一步政策迭代。实现DecompPI只需从离线数据进行预测任务，减轻了在线实验的需要。在理论上，我们展示了在一种自然的结构假设下，DecompPI可以获得算法复杂度的渐近收敛性，同时保持一个可扩展的模型.

    Problem definition: Behavioral health interventions, delivered through digital platforms, have the potential to significantly improve health outcomes, through education, motivation, reminders, and outreach. We study the problem of optimizing personalized interventions for patients to maximize some long-term outcome, in a setting where interventions are costly and capacity-constrained.  Methodology/results: This paper provides a model-free approach to solving this problem. We find that generic model-free approaches from the reinforcement learning literature are too data intensive for healthcare applications, while simpler bandit approaches make progress at the expense of ignoring long-term patient dynamics. We present a new algorithm we dub DecompPI that approximates one step of policy iteration. Implementing DecompPI simply consists of a prediction task from offline data, alleviating the need for online experimentation. Theoretically, we show that under a natural set of structural assu
    
[^169]: 伪监督度量：在无监督跨域分类框架中评估无监督图像翻译模型

    Pseudo Supervised Metrics: Evaluating Unsupervised Image to Image Translation Models In Unsupervised Cross-Domain Classification Frameworks. (arXiv:2303.10310v1 [cs.CV])

    [http://arxiv.org/abs/2303.10310](http://arxiv.org/abs/2303.10310)

    本文提出了一种新方法——伪监督度量，用于评估无监督图片到图片翻译模型在无监督跨域分类框架中的性能，并在多个基准数据集上进行了实验。

    

    图像分类的准确性和高效性取决于访问大型标记数据集并在模型训练的相同领域上测试数据。当处理来自不同领域的新数据时，分类变得更加具有挑战性，因为收集大型标记数据集并从头训练新分类器耗时、昂贵，有时是不可行或不可能的。跨域分类框架通过利用无监督图像对图像 (UI2I) 翻译模型将输入图像从未标记的域转换为标记域来处理这个数据域漂移问题。这些无监督模型的问题在于它们是无监督的。由于缺少注释，无法使用传统的监督度量来评估这些翻译模型以选择最佳的检查点模型。在本文中，我们介绍了一种新的方法，称为伪监督度量，专门用于评估无监督跨域分类框架中 UI2I 翻译模型的性能。我们通过对几个基准数据集的实验证明了我们的方法的有效性。

    The ability to classify images accurately and efficiently is dependent on having access to large labeled datasets and testing on data from the same domain that the model is trained on. Classification becomes more challenging when dealing with new data from a different domain, where collecting a large labeled dataset and training a new classifier from scratch is time-consuming, expensive, and sometimes infeasible or impossible. Cross-domain classification frameworks were developed to handle this data domain shift problem by utilizing unsupervised image-to-image (UI2I) translation models to translate an input image from the unlabeled domain to the labeled domain. The problem with these unsupervised models lies in their unsupervised nature. For lack of annotations, it is not possible to use the traditional supervised metrics to evaluate these translation models to pick the best-saved checkpoint model. In this paper, we introduce a new method called Pseudo Supervised Metrics that was desig
    
[^170]: 图像统计与人类感知之间的关联关系分离

    Disentangling the Link Between Image Statistics and Human Perception. (arXiv:2303.09874v1 [cs.CV])

    [http://arxiv.org/abs/2303.09874](http://arxiv.org/abs/2303.09874)

    本研究直接评估自然图像的概率，并分析它如何影响人类感知。通过展示具有更丰富统计特征的自然图像被感知为具有更大的显着性，论文提供了直接支持Barlow和Attneave理论的证据，并建立了一个新的框架，用于理解图像统计与知觉之间的关系。

    

    在20世纪50年代，霍勒斯巴洛和弗雷德阿特纳夫提出了感官系统和它们如何适应环境之间的关系：早期视觉的进化是为了最大限度地传递关于输入信号的信息。按照香农的定义，这些信息是通过自然场景中拍摄的图像的概率来描述的。由于计算能力的限制，以前无法直接准确地预测图像的概率。尽管这种想法的探索是间接的，主要基于图像密度的过度简化模型或系统设计方法，但这些方法在重现各种生理和心理物理现象方面取得了成功。在本文中，我们直接评估自然图像的概率，并分析它如何确定知觉灵敏度。我们使用与人类意见相关性很高的图像质量指标作为人类视觉的代理，以及一个先进的生成模型来直接估计自然图像的概率密度函数。我们的结果表明，根据Barlow和Attneave理论预测的图像统计与人类知觉之间存在系统性的关联。我们通过展示具有更丰富统计特征的自然图像被感知为具有更大的显着性来说明这一发现，这是通过视觉搜索实验测量的。我们的工作提供了直接支持Barlow和Attneave理论的证据，并建立了一个新的框架，用于理解图像统计与知觉之间的关系。

    In the 1950s Horace Barlow and Fred Attneave suggested a connection between sensory systems and how they are adapted to the environment: early vision evolved to maximise the information it conveys about incoming signals. Following Shannon's definition, this information was described using the probability of the images taken from natural scenes. Previously, direct accurate predictions of image probabilities were not possible due to computational limitations. Despite the exploration of this idea being indirect, mainly based on oversimplified models of the image density or on system design methods, these methods had success in reproducing a wide range of physiological and psychophysical phenomena. In this paper, we directly evaluate the probability of natural images and analyse how it may determine perceptual sensitivity. We employ image quality metrics that correlate well with human opinion as a surrogate of human vision, and an advanced generative model to directly estimate the probabil
    
[^171]: 《人口统计平等检查员：通过解释空间进行公平审核》

    Demographic Parity Inspector: Fairness Audits via the Explanation Space. (arXiv:2303.08040v1 [cs.LG])

    [http://arxiv.org/abs/2303.08040](http://arxiv.org/abs/2303.08040)

    这篇论文提出了一种基于解释空间的算法方法，测量分组人口统计学平等的违规情况，并允许我们检查组间歧视的原因，提高了审计公平性的敏感度。

    

    即使具有最好的意图，机器学习方法也可能延续、放大甚至创造社会偏见。衡量机器学习模型的歧视性（非歧视性）的方法已被提出。然而，导致歧视效果的受保护属性的代理仍然是一个具有挑战性的问题。我们提出了一种新的算法方法，它可以测量分组人口统计学平等的违规情况，并允许我们检查组间歧视的原因。我们的方法依赖于一种新颖的思想，即基于解释空间对模型对受保护属性的依赖度进行测量，解释空间是一种提供比输入数据或预测分布的原始空间更敏感审计的信息空间，从而允许断言理论上的人口统计审核保证。我们提供了数学分析、合成样例和实际数据的实验评估。我们还发布了一个开源的Pytorch实现和一个易于使用的Web应用程序。

    Even if deployed with the best intentions, machine learning methods can perpetuate, amplify or even create social biases. Measures of (un-)fairness have been proposed as a way to gauge the (non-)discriminatory nature of machine learning models. However, proxies of protected attributes causing discriminatory effects remain challenging to address. In this work, we propose a new algorithmic approach that measures group-wise demographic parity violations and allows us to inspect the causes of inter-group discrimination. Our method relies on the novel idea of measuring the dependence of a model on the protected attribute based on the explanation space, an informative space that allows for more sensitive audits than the primary space of input data or prediction distributions, and allowing for the assertion of theoretical demographic parity auditing guarantees. We provide a mathematical analysis, synthetic examples, and experimental evaluation of real-world data. We release an open-source Pyt
    
[^172]: 利用罚函数的深度部分线性Cox模型及其在肺癌患者CT扫描中的应用

    Penalized Deep Partially Linear Cox Models with Application to CT Scans of Lung Cancer Patients. (arXiv:2303.05341v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.05341](http://arxiv.org/abs/2303.05341)

    通过引入罚函数，我们提出了一种创新的深度部分线性Cox模型，用于在肺癌患者的CT扫描中分析死亡风险。该模型能有效地整合已知和新兴的风险因素，解决了参数维度超出样本大小和非参数建模中维度灾难的问题。

    

    肺癌是全球癌症死亡率的主要原因，突出了理解其死亡风险对设计有效的以患者为中心的治疗的重要性。国家肺部筛查试验（NLST）采用了计算机断层扫描纹理分析，提供了CT扫描上纹理模式的客观测量，用于量化肺癌患者的死亡风险。部分线性Cox模型通过将风险函数分解为参数和非参数分量，成为生存分析中备受青睐的方法，可以有效地将已知风险因素（如年龄和临床变量）和新兴风险因素（如图像特征）整合在一个统一的框架中。然而，当参数分量的维度超过样本大小时，模型拟合变得困难，而非参数建模则面临维度灾难的问题。我们提出了一种新颖的罚函数深度部分线性Cox模型（Penali

    Lung cancer is a leading cause of cancer mortality globally, highlighting the importance of understanding its mortality risks to design effective patient-centered therapies. The National Lung Screening Trial (NLST) employed computed tomography texture analysis, which provides objective measurements of texture patterns on CT scans, to quantify the mortality risks of lung cancer patients. Partially linear Cox models have gained popularity for survival analysis by dissecting the hazard function into parametric and nonparametric components, allowing for the effective incorporation of both well-established risk factors (such as age and clinical variables) and emerging risk factors (e.g., image features) within a unified framework. However, when the dimension of parametric components exceeds the sample size, the task of model fitting becomes formidable, while nonparametric modeling grapples with the curse of dimensionality. We propose a novel Penalized Deep Partially Linear Cox Model (Penali
    
[^173]: 受可微分逻辑约束的共学习规划与控制策略

    Co-learning Planning and Control Policies Constrained by Differentiable Logic Specifications. (arXiv:2303.01346v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2303.01346](http://arxiv.org/abs/2303.01346)

    本文提出了一种通过共学习规划和控制策略来解决带有复杂逻辑约束的高维度机器人导航任务的强化学习方法。相比现有算法，这种方法通过降低样本复杂性来训练出高质量的策略，并且能够高效地生成长期的机器人运动路径。实验证明了该方法的有效性。

    

    在机器人技术中综合规划与控制策略是一项基本任务，但复杂的逻辑约束和高维度的机器人动力学使其变得更加复杂。本文提出了一种新颖的强化学习方法，通过共学习规划和控制策略来解决带有复杂逻辑约束的高维度机器人导航任务。值得注意的是，这种方法显著降低了训练的样本复杂性，相比现有的强化学习算法，我们可以用更少的样本训练出高质量的策略。此外，我们的方法简化了从地图图像中提取复杂规范并能够高效生成不同地图布局的长期机器人运动路径。此外，我们的方法还展示了在高维度控制和避免次优策略方面的能力。通过模拟高维机器人导航任务的实验验证了我们方法的有效性。

    Synthesizing planning and control policies in robotics is a fundamental task, further complicated by factors such as complex logic specifications and high-dimensional robot dynamics. This paper presents a novel reinforcement learning approach to solving high-dimensional robot navigation tasks with complex logic specifications by co-learning planning and control policies. Notably, this approach significantly reduces the sample complexity in training, allowing us to train high-quality policies with much fewer samples compared to existing reinforcement learning algorithms. In addition, our methodology streamlines complex specification extraction from map images and enables the efficient generation of long-horizon robot motion paths across different map layouts. Moreover, our approach also demonstrates capabilities for high-dimensional control and avoiding suboptimal policies via policy alignment. The efficacy of our approach is demonstrated through experiments involving simulated high-dim
    
[^174]: 对话式情境马尔可夫决策过程的高效探索性关键词选择策略

    Efficient Explorative Key-term Selection Strategies for Conversational Contextual Bandits. (arXiv:2303.00315v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00315](http://arxiv.org/abs/2303.00315)

    本研究提出了一种通用框架“ConLinUCB”来解决对话式情境马尔可夫决策过程中信息整合和探索性关键词选择的问题，以加速用户偏好估计的收敛速度。

    

    对话式情境马尔可夫决策过程通过偶尔询问显式反馈中的关键词来加快学习过程。然而，现有方法存在一些局限性。首先，关于关键词层面的对话和臂级推荐的信息没有被妥善地结合起来以加速学习。其次，重要的是问一些探索性的关键词，以迅速了解用户在各个领域的潜在兴趣，从而加速用户偏好估计的收敛速度，而这在现有研究中从未被考虑过。为了解决这些问题，我们首先提出了“ConLinUCB”这一对话式决策过程的通用框架，它能够更好地将关键词层面和臂级反馈结合起来，在每个时间步骤上一步估计用户偏好。基于这个框架，我们进一步设计了两种带有探索性关键词选择策略的决策过程算法，即ConLinUCB-BS和ConLinUCB-MCR。

    Conversational contextual bandits elicit user preferences by occasionally querying for explicit feedback on key-terms to accelerate learning. However, there are aspects of existing approaches which limit their performance. First, information gained from key-term-level conversations and arm-level recommendations is not appropriately incorporated to speed up learning. Second, it is important to ask explorative key-terms to quickly elicit the user's potential interests in various domains to accelerate the convergence of user preference estimation, which has never been considered in existing works. To tackle these issues, we first propose ``ConLinUCB", a general framework for conversational bandits with better information incorporation, combining arm-level and key-term-level feedback to estimate user preference in one step at each time. Based on this framework, we further design two bandit algorithms with explorative key-term selection strategies, ConLinUCB-BS and ConLinUCB-MCR. We prove t
    
[^175]: EvoPrompting: 适用于代码级神经架构搜索的语言模型

    EvoPrompting: Language Models for Code-Level Neural Architecture Search. (arXiv:2302.14838v1 [cs.NE] CROSS LISTED)

    [http://arxiv.org/abs/2302.14838](http://arxiv.org/abs/2302.14838)

    EvoPrompting利用语言模型作为自适应变异和交叉操作符来进行神经架构搜索，在MNIST-1D数据集和CLRS算法推理基准上都取得了比人类设计的架构更好的性能表现。

    

    鉴于语言模型（LM）在代码生成方面的最新成就，我们探索将LM作为进化神经架构搜索（NAS）算法的自适应变异和交叉操作符的使用。尽管NAS仍然过于困难，以至于仅仅通过提示就难以成功，但我们发现进化提示工程与软提示调整的组合，一种我们称之为EvoPrompting的方法，始终可以发现多样化且性能高的模型。我们首先证明EvoPrompting在MNIST-1D数据集上是有效的，其中EvoPrompting产生的卷积架构变体在准确率和模型大小方面均优于人类专家设计的架构和天真的少数先导提示。然后，我们将我们的方法应用于在CLRS算法推理基准上搜索图神经网络，其中EvoPrompting能够设计出比当前最先进的模型更好的新颖结构。

    Given the recent impressive accomplishments of language models (LMs) for code generation, we explore the use of LMs as adaptive mutation and crossover operators for an evolutionary neural architecture search (NAS) algorithm. While NAS still proves too difficult a task for LMs to succeed at solely through prompting, we find that the combination of evolutionary prompt engineering with soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse and high performing models. We first demonstrate that EvoPrompting is effective on the computationally efficient MNIST-1D dataset, where EvoPrompting produces convolutional architecture variants that outperform both those designed by human experts and naive few-shot prompting in terms of accuracy and model size. We then apply our method to searching for graph neural networks on the CLRS Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel architectures that outperform current state-of-the-art models on 21 ou
    
[^176]: HUST轴承：一个实用的球轴承故障诊断数据集

    HUST bearing: a practical dataset for ball bearing fault diagnosis. (arXiv:2302.12533v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12533](http://arxiv.org/abs/2302.12533)

    HUST轴承是一个实用的球轴承故障诊断数据集，其中包含90个带有6种故障类型（内部裂纹、外部裂纹、球体裂纹和它们的2种组合）的5种不同类型轴承的振动数据。研究者使用经典机器学习分类方法以及先进的非监督迁移学习算法对该数据集进行了评估，实验结果表明在分类任务上准确率可达到100%，在非监督迁移学习任务上准确率为60-80%。

    

    在这项工作中，我们介绍了一个名为HUST轴承的实用数据集，该数据集提供了一组不同球轴承的振动数据。该数据集包括5种类型轴承的90个原始振动数据，其中包括内部裂纹、外部裂纹、球体裂纹以及它们的2种组合在内的6种缺陷类型，以及3个工作条件下的采样率为51,200次/秒。我们在引入的数据集上建立了包络分析和阶次跟踪分析，以进行数据的初步评估。使用不同域中的特征，我们使用了多种经典的机器学习分类方法来识别数据集中的轴承故障。同时，我们还使用了典型的先进非监督迁移学习算法，以观察数据集各部分之间的知识可迁移性。在数据集上经过实验的方法在分类任务上获得了达到100%的不同准确率，并在非监督迁移学习任务上获得了60-80%的准确率。

    In this work, we introduce a practical dataset named HUST bearing, that provides a large set of vibration data on different ball bearings. This dataset contains 90 raw vibration data of 6 types of defects (inner crack, outer crack, ball crack, and their 2-combinations) on 5 types of bearing at 3 working conditions with the sample rate of 51,200 samples per second. We established the envelope analysis and order tracking analysis on the introduced dataset to allow an initial evaluation of the data. A number of classical machine learning classification methods are used to identify bearing faults of the dataset using features in different domains. The typical advanced unsupervised transfer learning algorithms also perform to observe the transferability of knowledge among parts of the dataset. The experimental results of examined methods on the dataset gain divergent accuracy up to 100% on classification task and 60-80% on unsupervised transfer learning task.
    
[^177]: mSAM: 微批量平均锐度感知最小化

    mSAM: Micro-Batch-Averaged Sharpness-Aware Minimization. (arXiv:2302.09693v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.09693](http://arxiv.org/abs/2302.09693)

    mSAM是一种深度学习优化方法，通过在训练过程中聚合对抗性扰动得到的更新，从理论上证明了比传统方法更平的极小值点，实验证实了其在各种任务上的优越性能。

    

    现代深度学习模型是过参数化的，不同的极值可能导致广泛变化的泛化性能。锐度感知最小化（Sharpness-Aware Minimization，SAM）技术修改了基本的损失函数，使随机梯度下降方法朝着更平的极小值点前进，这被认为能够展现出增强的泛化能力。我们的研究深入探讨了一种特定的SAM变体，即微批量SAM（mSAM）。这种变体在训练过程中通过对来自多个分片（微批量）的对抗性扰动得到的更新进行聚合。我们将最近开发和研究的用于平坦性分析的通用框架扩展到理论上证明SAM实现了比随机梯度下降更平的极小值点，而mSAM比SAM实现了更加平坦的极小值点。我们对各种图像分类和自然语言处理任务进行了彻底的实证评估以验证这一理论进展。我们还表明，与以前的工作相反，mSAM可以被实现。

    Modern deep learning models are over-parameterized, where different optima can result in widely varying generalization performance. The Sharpness-Aware Minimization (SAM) technique modifies the fundamental loss function that steers gradient descent methods toward flatter minima, which are believed to exhibit enhanced generalization prowess. Our study delves into a specific variant of SAM known as micro-batch SAM (mSAM). This variation involves aggregating updates derived from adversarial perturbations across multiple shards (micro-batches) of a mini-batch during training. We extend a recently developed and well-studied general framework for flatness analysis to theoretically show that SAM achieves flatter minima than SGD, and mSAM achieves even flatter minima than SAM. We provide a thorough empirical evaluation of various image classification and natural language processing tasks to substantiate this theoretical advancement. We also show that contrary to previous work, mSAM can be impl
    
[^178]: 对于混合折扣马尔可夫决策过程的强化学习的最优样本复杂度研究

    Optimal Sample Complexity of Reinforcement Learning for Mixing Discounted Markov Decision Processes. (arXiv:2302.07477v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07477](http://arxiv.org/abs/2302.07477)

    这篇论文研究了对于混合折扣马尔可夫决策过程的强化学习的最优样本复杂度理论。作者发现，在混合的情况下，最优样本复杂度依赖于总变异混合时间、折扣因子和解误差容忍度。

    

    我们考虑了表格型强化学习（RL）对于在马尔可夫决策过程（MDP）中最大化无穷时间折扣奖励的最优样本复杂度理论。在这种设定下，已经为表格型问题开发了最优最坏情况复杂度结果，导致样本复杂度依赖于折扣系数$\gamma$和解误差容忍度$\epsilon$的形式为$\tilde \Theta((1-\gamma)^{-3}\epsilon^{-2})$，其中$\gamma$表示折扣因子，$\epsilon$为解误差容忍度。然而，在许多感兴趣的应用中，最优策略（或所有策略）会产生混合。我们确定，在这种情况下，最优样本复杂度的依赖关系为$\tilde \Theta(t_{\text{mix}}(1-\gamma)^{-2}\epsilon^{-2})$，其中$t_{\text{mix}}$是总变异混合时间。我们的分析基于再生型思想，我们认为这些思想对于研究一般状态空间MDPs的RL问题具有独立的兴趣。

    We consider the optimal sample complexity theory of tabular reinforcement learning (RL) for maximizing the infinite horizon discounted reward in a Markov decision process (MDP). Optimal worst-case complexity results have been developed for tabular RL problems in this setting, leading to a sample complexity dependence on $\gamma$ and $\epsilon$ of the form $\tilde \Theta((1-\gamma)^{-3}\epsilon^{-2})$, where $\gamma$ denotes the discount factor and $\epsilon$ is the solution error tolerance. However, in many applications of interest, the optimal policy (or all policies) induces mixing. We establish that in such settings, the optimal sample complexity dependence is $\tilde \Theta(t_{\text{mix}}(1-\gamma)^{-2}\epsilon^{-2})$, where $t_{\text{mix}}$ is the total variation mixing time. Our analysis is grounded in regeneration-type ideas, which we believe are of independent interest, as they can be used to study RL problems for general state space MDPs.
    
[^179]: AdaptSim: 用于模拟到实际转换的任务驱动仿真适应

    AdaptSim: Task-Driven Simulation Adaptation for Sim-to-Real Transfer. (arXiv:2302.04903v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2302.04903](http://arxiv.org/abs/2302.04903)

    提出了AdaptSim，这是一个新的任务驱动的适应性框架，用于模拟到实际转换。它通过优化目标环境中的任务性能来解决仿真和现实之间的差距。

    

    仿真参数设置（如接触模型和物体几何近似）对于训练能够从仿真转换到真实环境部署的稳健机器人策略至关重要。以往的方法通常手工制作参数分布（域随机化），或者识别最能匹配真实环境动力学的参数（系统识别）。然而，仿真与现实之间往往存在着无法消除的差距：试图在所有状态和任务下匹配仿真和现实的动力学可能是不可行的，也不一定能够在特定任务的现实环境中表现良好。针对这个问题，我们提出了AdaptSim，这是一个新的任务驱动的适应性框架，用于模拟到实际转换，其目标是优化目标（真实）环境中的任务性能，而不是匹配仿真和现实之间的动力学。首先，我们使用强化学习在仿真中元学习一个适应策略。

    Simulation parameter settings such as contact models and object geometry approximations are critical to training robust robotic policies capable of transferring from simulation to real-world deployment. Previous approaches typically handcraft distributions over such parameters (domain randomization), or identify parameters that best match the dynamics of the real environment (system identification). However, there is often an irreducible gap between simulation and reality: attempting to match the dynamics between simulation and reality across all states and tasks may be infeasible and may not lead to policies that perform well in reality for a specific task. Addressing this issue, we propose AdaptSim, a new task-driven adaptation framework for sim-to-real transfer that aims to optimize task performance in target (real) environments -- instead of matching dynamics between simulation and reality. First, we meta-learn an adaptation policy in simulation using reinforcement learning for adj
    
[^180]: 长时间尺度温度缩放

    Long Horizon Temperature Scaling. (arXiv:2302.03686v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03686](http://arxiv.org/abs/2302.03686)

    提出了一种长时间尺度温度缩放（LHTS）方法，用于从温度缩放的联合分布中采样。LHTS可以优化样本的长时间尺度似然，并且在图像扩散模型和字符/语言自回归模型上展示了优势。

    

    温度缩放是一种调节模型分布锐度的常用技术。它广泛应用于采样可能的生成物和校准模型不确定性，甚至在许多大型语言模型的部署中作为可控参数。然而，自回归模型依赖于贪婪地优化下一个标记的短视温度缩放。为了解决这个问题，我们提出了一种新颖的方法，即长时间尺度温度缩放（LHTS），用于从温度缩放的联合分布中采样。LHTS与所有基于似然的模型兼容，并优化样本的长时间尺度似然。我们推导了一个温度相关的LHTS目标，并展示了在一系列温度上微调模型可以产生一个具有可控长时间尺度温度参数的单一模型。我们在图像扩散模型和字符/语言自回归模型上进行了LHTS实验，证明了相比于短视温度缩放的优势。

    Temperature scaling is a popular technique for tuning the sharpness of a model distribution. It is used extensively for sampling likely generations and calibrating model uncertainty, and even features as a controllable parameter to many large language models in deployment. However, autoregressive models rely on myopic temperature scaling that greedily optimizes the next token. To address this, we propose Long Horizon Temperature Scaling (LHTS), a novel approach for sampling from temperature-scaled joint distributions. LHTS is compatible with all likelihood-based models, and optimizes for the long horizon likelihood of samples. We derive a temperature-dependent LHTS objective, and show that finetuning a model on a range of temperatures produces a single model capable of generation with a controllable long horizon temperature parameter. We experiment with LHTS on image diffusion models and character/language autoregressive models, demonstrating advantages over myopic temperature scaling 
    
[^181]: Weisfeiler-Lehman距离：重新解释和与GNN的连接

    The Weisfeiler-Lehman Distance: Reinterpretation and Connection with GNNs. (arXiv:2302.00713v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00713](http://arxiv.org/abs/2302.00713)

    本文重新解释了Weisfeiler-Lehman距离，并将其与消息传递神经网络进行了联系，对于理解网络的Lipschitz性质和通用逼近结果具有重要意义。

    

    本文通过使用随机过程的概念，对Chen等人（2022年）提出的所谓的Weisfeiler-Lehman（WL）距离进行了新颖的解释。WL距离旨在比较具有节点特征的图形，具有与经典的Weisfeiler-Lehman图同构测试相同的判别能力，并与Gromov-Wasserstein距离有着深入的联系。这种新的解释将WL距离与用于随机过程距离的文献联系起来，从而使得对距离的解释更加简洁易懂。我们进一步探讨了WL距离与某些消息传递神经网络之间的联系，并讨论了WL距离对于理解这些网络的Lipschitz性质和通用逼近结果的影响。

    In this paper, we present a novel interpretation of the so-called Weisfeiler-Lehman (WL) distance, introduced by Chen et al. (2022), using concepts from stochastic processes. The WL distance aims at comparing graphs with node features, has the same discriminative power as the classic Weisfeiler-Lehman graph isomorphism test and has deep connections to the Gromov-Wasserstein distance. This new interpretation connects the WL distance to the literature on distances for stochastic processes, which also makes the interpretation of the distance more accessible and intuitive. We further explore the connections between the WL distance and certain Message Passing Neural Networks, and discuss the implications of the WL distance for understanding the Lipschitz property and the universal approximation results for these networks.
    
[^182]: 无限稳定性的图卷积网络

    Limitless stability for Graph Convolutional Networks. (arXiv:2301.11443v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11443](http://arxiv.org/abs/2301.11443)

    本研究在图卷积网络中提供了稳定性保证和可转移性界限，无需参考任何限制对象或统计分布，可处理无向图和有向图。节点级扰动的稳定性与过滤器的“充分（谱）覆盖”属性有关，边级扰动的稳定性与Lipschitz常数和新引入的过滤器半范数有关。通过数学物理工具获得了关于拓扑扰动的稳定性结果，并展示了在图粗粒化过程中图卷积网络的稳定性条件。

    

    本研究为图卷积网络提供了严格、新颖且具有广泛适用性的稳定性保证和可转移性界限 - 不需要对任何潜在限制对象或统计分布进行参考。关键是，所使用的图移位运算符（GSO）不一定是正规的，可以处理无向图和有向图。节点级扰动的稳定性与每层过滤器中的“充分（谱）覆盖”属性有关。边级扰动的稳定性与Lipschitz常数和新引入的过滤器半范数有关。通过最近开发的基于数学物理工具获得了关于拓扑扰动的稳定性结果。作为一个重要而新颖的例子，展示了只有在GSO是t时，图卷积网络在图粗粒化过程中（通过用单个节点替换强连通子图）是稳定的。

    This work establishes rigorous, novel and widely applicable stability guarantees and transferability bounds for graph convolutional networks -without reference to any underlying limit object or statistical distribution. Crucially, utilized graph-shift operators (GSOs) are not necessarily assumed to be normal, allowing for the treatment of networks on both undirected- and for the first time also directed graphs. Stability to node-level perturbations is related to an 'adequate (spectral) covering' property of the filters in each layer. Stability to edge-level perturbations is related to Lipschitz constants and newly introduced semi-norms of filters. Results on stability to topological perturbations are obtained through recently developed mathematical-physics based tools. As an important and novel example, it is showcased that graph convolutional networks are stable under graph-coarse-graining procedures (replacing strongly-connected sub-graphs by single nodes) precisely if the GSO is t
    
[^183]: 领域无关的分子生成与自我反馈

    Domain-Agnostic Molecular Generation with Self-feedback. (arXiv:2301.11259v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11259](http://arxiv.org/abs/2301.11259)

    MolGen是一个专注于分子生成的预训练语言模型，使用了领域无关的分子前缀调整和自我反馈的范式，实现了化学有效性、多样性、新颖性和复杂性的突破，在分子生成领域表现出了出色的性能。

    

    分子的生成已经受到极大的关注，其革新了科学家设计分子结构的方式，并为化学和药物设计提供了宝贵的支持。然而，尽管在分子生成中使用语言模型具有潜力，但它们面临着许多挑战，比如生成语法或化学存在缺陷的分子，狭窄的领域专注以及由于缺乏注释数据或外部分子数据库而限制了生成多样性和可行性。因此，我们引入了MolGen，它是一个专门用于分子生成的预训练分子语言模型。MolGen通过重构一亿多个分子SELFIES获得了固有的结构和语法概念，并通过领域无关的分子前缀调整促进了不同领域之间的知识传递。此外，我们提出了一种自我反馈范式，启发预训练模型与最终下游目标对齐，有助于更稳健和高效的分子生成。我们在基准数据集上的实验表明，MolGen在化学有效性，多样性，新颖性和复杂性方面优于现有技术。

    The generation of molecules with desired properties has gained tremendous popularity, revolutionizing the way scientists design molecular structures and providing valuable support for chemical and drug design. However, despite the potential of language models in molecule generation, they face numerous challenges such as the generation of syntactically or chemically flawed molecules, narrow domain focus, and limitations in creating diverse and directionally feasible molecules due to a dearth of annotated data or external molecular databases. To this end, we introduce MolGen, a pre-trained molecular language model tailored specifically for molecule generation. MolGen acquires intrinsic structural and grammatical insights by reconstructing over 100 million molecular SELFIES, while facilitating knowledge transfer between different domains through domain-agnostic molecular prefix tuning. Moreover, we present a self-feedback paradigm that inspires the pre-trained model to align with the ulti
    
[^184]: 刻苦训练，轻松战斗：强健的元强化学习

    Train Hard, Fight Easy: Robust Meta Reinforcement Learning. (arXiv:2301.11147v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11147](http://arxiv.org/abs/2301.11147)

    本文提出了一个强健的元强化学习算法，通过学习适应新任务的元策略来解决强化学习中环境和任务变化的挑战，通过识别和过采样更难的任务来提高系统的可靠性和效率。

    

    强化学习在实际应用中面临着环境、任务或客户之间的差异，元强化学习(MRL)通过学习适应新任务的元策略来解决这个问题。标准的MRL方法优化任务的平均回报，但在高风险或高难度的任务中往往结果不佳。由于事先不知道测试任务，这限制了系统的可靠性。在这项工作中，我们定义了一个具有可控鲁棒性水平的强健MRL目标。我们证明了在我们提出的MRL框架中，梯度偏差消失了。通过新颖的Robust Meta RL算法（RoML），我们解决了数据效率的问题。RoML是一个元算法，通过在训练过程中识别和过采样更难的任务来生成任何给定MRL算法的强健版本。我们证明了RoML在多个任务上实现了强健回报。

    A major challenge of reinforcement learning (RL) in real-world applications is the variation between environments, tasks or clients. Meta-RL (MRL) addresses this issue by learning a meta-policy that adapts to new tasks. Standard MRL methods optimize the average return over tasks, but often suffer from poor results in tasks of high risk or difficulty. This limits system reliability since test tasks are not known in advance. In this work, we define a robust MRL objective with a controlled robustness level. Optimization of analogous robust objectives in RL is known to lead to both *biased gradients* and *data inefficiency*. We prove that the gradient bias disappears in our proposed MRL framework. The data inefficiency is addressed via the novel Robust Meta RL algorithm (RoML). RoML is a meta-algorithm that generates a robust version of any given MRL algorithm, by identifying and over-sampling harder tasks throughout training. We demonstrate that RoML achieves robust returns on multiple na
    
[^185]: 具有连续动作的准最优强化学习

    Quasi-optimal Reinforcement Learning with Continuous Actions. (arXiv:2301.08940v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.08940](http://arxiv.org/abs/2301.08940)

    本研究提出了一种准最优学习算法，用于解决强化学习中连续动作环境下的决策问题，特别适用于医疗应用中确定最佳剂量水平的问题。

    

    强化学习在许多现实应用中需要在连续动作环境中做出决策。在医疗治疗方案的开发中，确定最佳剂量水平起着至关重要的作用。然而，将现有的强化学习算法应用于医疗应用中的一个挑战是，流行的无穷支持随机策略（例如高斯策略）可能会分配过高的剂量，严重危害患者。因此，引导一个支持仅包含近似最优动作的策略类别，并缩小效果和可靠性的动作搜索区域是很重要的。为了实现这一点，我们开发了一种新的“准最优学习算法”，该算法在离线策略设置下可以轻松优化，并在一般函数逼近下保证收敛。在理论上，我们分析了所提出算法的一致性、样本复杂度、适应性和收敛性。我们通过全面的模拟实验评估了我们的算法。

    Many real-world applications of reinforcement learning (RL) require making decisions in continuous action environments. In particular, determining the optimal dose level plays a vital role in developing medical treatment regimes. One challenge in adapting existing RL algorithms to medical applications, however, is that the popular infinite support stochastic policies, e.g., Gaussian policy, may assign riskily high dosages and harm patients seriously. Hence, it is important to induce a policy class whose support only contains near-optimal actions, and shrink the action-searching area for effectiveness and reliability. To achieve this, we develop a novel \emph{quasi-optimal learning algorithm}, which can be easily optimized in off-policy settings with guaranteed convergence under general function approximations. Theoretically, we analyze the consistency, sample complexity, adaptability, and convergence of the proposed algorithm. We evaluate our algorithm with comprehensive simulated expe
    
[^186]: 基于对抗生成网络的短SSVEP数据扩展框架

    Short-length SSVEP data extension by a novel generative adversarial networks based framework. (arXiv:2301.05599v3 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2301.05599](http://arxiv.org/abs/2301.05599)

    本文提出了一种基于GAN的端到端信号转化网络TEGAN，可以将短SSVEP信号转换成长的人工SSVEP信号，并显著提高BCI系统的效率和准确性。

    

    基于SSVEP的脑机接口因其高信息传输速率和目标数量可用性而受到广泛关注。然而，频率识别方法的性能在很大程度上取决于用户校准数据的数量和数据长度，这限制了它在实际应用中的部署。最近，基于生成对抗网络（GANs）的数据生成方法已被广泛采用来创建合成的脑电数据，有望解决这些问题。本文提出了一种基于GANs的端到端信号转化网络TEGAN，用于数据长度扩展。TEGAN可以将短SSVEP信号转换成长的人工SSVEP信号。通过将一个新颖的U型生成器架构和一个辅助分类器加入到网络结构中，TEGAN可以在合成数据中产生有条件的特征。此外，我们实现并比较了两种最先进的频率识别方法，以评估TEGAN生成数据的有效性。实验结果表明，所提出的TEGAN方法优于传统的线性插值方法和最先进的基于深度学习的方法。所提出的TEGAN方法可以显著提高BCI系统的效率，减少所需的校准时间并改善分类的准确性。

    Steady-state visual evoked potentials (SSVEPs) based brain-computer interface (BCI) has received considerable attention due to its high information transfer rate (ITR) and available quantity of targets. However, the performance of frequency identification methods heavily hinges on the amount of user calibration data and data length, which hinders the deployment in real-world applications. Recently, generative adversarial networks (GANs)-based data generation methods have been widely adopted to create synthetic electroencephalography (EEG) data, holds promise to address these issues. In this paper, we proposed a GAN-based end-to-end signal transformation network for data length extension, termed as TEGAN. TEGAN transforms short-length SSVEP signals into long-length artificial SSVEP signals. By incorporating a novel U-Net generator architecture and an auxiliary classifier into the network architecture, the TEGAN could produce conditioned features in the synthetic data. Additionally, we i
    
[^187]: 默默杀手: 一种隐蔽的、无标签的、黑盒子后门攻击

    Silent Killer: A Stealthy, Clean-Label, Black-Box Backdoor Attack. (arXiv:2301.02615v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2301.02615](http://arxiv.org/abs/2301.02615)

    默默杀手是一种隐蔽的、无标签的、黑盒子后门攻击，它使用了隐蔽的毒物和触发器，在无标签攻击中使用通用对抗扰动作为触发器，通过渐变对齐来提高成功率，并在MNIST、CIFAR10和ImageNet数据集上取得了最新的成果。

    

    后门污染攻击对神经网络构成了众所周知的风险。然而，大多数研究都集中在宽松的威胁模型上。我们引入了一种名为默默杀手的新型攻击，在无标签的黑盒子环境中运行，使用隐蔽的毒物和触发器，并且胜过现有的方法。我们研究了在无标签攻击中使用通用对抗扰动作为触发器的方法，在毒标签设置下的成功案例之后。我们分析了一个天真的适应方法的成功情况，并发现需要渐变对齐以确保高成功率。我们对MNIST、CIFAR10和一个缩小版的ImageNet进行了彻底的实验，并取得了最新的成果。

    Backdoor poisoning attacks pose a well-known risk to neural networks. However, most studies have focused on lenient threat models. We introduce Silent Killer, a novel attack that operates in clean-label, black-box settings, uses a stealthy poison and trigger and outperforms existing methods. We investigate the use of universal adversarial perturbations as triggers in clean-label attacks, following the success of such approaches under poison-label settings. We analyze the success of a naive adaptation and find that gradient alignment for crafting the poison is required to ensure high success rates. We conduct thorough experiments on MNIST, CIFAR10, and a reduced version of ImageNet and achieve state-of-the-art results.
    
[^188]: 在约束规划求解器内学习通用的值选择启发式

    Learning a Generic Value-Selection Heuristic Inside a Constraint Programming Solver. (arXiv:2301.01913v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.01913](http://arxiv.org/abs/2301.01913)

    本论文提出了一种通用学习过程，用于在约束规划求解器内获取一个值选择启发式方法，以解决当前通用值选择启发式方法较为稀缺的问题。

    

    约束规划被认为是解决组合问题的一种高效方法。求解器中的重要设计选择是分支启发式，它们旨在在最短的时间内寻找最佳解决方案。然而，开发这些启发式需要耗费大量时间，并需要问题特定的专业知识。这一观察结果激发了许多使用机器学习自动学习高效启发式的努力，而无需专家干预。据我们所知，这仍然是一个开放的研究问题。尽管文献中有几种通用的变量选择启发式方法，但对于通用的值选择启发式方法的选择却较少。在本文中，我们提出通过引入一种通用的学习过程来解决这个问题，该过程可以用于在约束规划求解器内获得一个值选择启发式方法。这得益于深度Q学习算法和一个...

    Constraint programming is known for being an efficient approach for solving combinatorial problems. Important design choices in a solver are the branching heuristics, which are designed to lead the search to the best solutions in a minimum amount of time. However, developing these heuristics is a time-consuming process that requires problem-specific expertise. This observation has motivated many efforts to use machine learning to automatically learn efficient heuristics without expert intervention. To the best of our knowledge, it is still an open research question. Although several generic variable-selection heuristics are available in the literature, the options for a generic value-selection heuristic are more scarce. In this paper, we propose to tackle this issue by introducing a generic learning procedure that can be used to obtain a value-selection heuristic inside a constraint programming solver. This has been achieved thanks to the combination of a deep Q-learning algorithm, a t
    
[^189]: 支持向量回归: 风险四方框架

    Support Vector Regression: Risk Quadrangle Framework. (arXiv:2212.09178v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2212.09178](http://arxiv.org/abs/2212.09178)

    本文结合风险四方理论，研究了支持向量回归（SVR）。研究结果发现，SVR的两种形式对应于等效误差度量的最小化，同时加上正则化惩罚项。通过构造基本风险四方框，我们证明了SVR是对两个对称条件分位数的平均数的渐近无偏估计量。此外，我们证明了$\varepsilon$-SVR和$\nu$-SVR在一般随机环境下的等价性。

    

    本文在基本的风险四方理论的背景下研究了支持向量回归（SVR），该理论将优化、风险管理和统计估计联系起来。研究结果表明，SVR的两种形式，$\varepsilon$-SVR和$\nu$-SVR，都对应于等效误差度量（分别为Vapnik误差和CVaR范数）的最小化，同时加上正则化惩罚项。这些误差度量又定义了相应的风险四方框。通过构造与SVR对应的基本风险四方框，我们证明了SVR是两个对称条件分位数的平均数的渐近无偏估计量。此外，我们在一般随机环境中证明了$\varepsilon$-SVR和$\nu$-SVR的等价性。此外，SVR被表述为带有正则化惩罚项的正则偏离最小化问题。最后，推导了在风险四方框架中的SVR的对偶形式。

    This paper investigates Support Vector Regression (SVR) in the context of the fundamental risk quadrangle theory, which links optimization, risk management, and statistical estimation. It is shown that both formulations of SVR, $\varepsilon$-SVR and $\nu$-SVR, correspond to the minimization of equivalent error measures (Vapnik error and CVaR norm, respectively) with a regularization penalty. These error measures, in turn, define the corresponding risk quadrangles. By constructing the fundamental risk quadrangle, which corresponds to SVR, we show that SVR is the asymptotically unbiased estimator of the average of two symmetric conditional quantiles. Further, we prove the equivalence of the $\varepsilon$-SVR and $\nu$-SVR in a general stochastic setting. Additionally, SVR is formulated as a regular deviation minimization problem with a regularization penalty. Finally, the dual formulation of SVR in the risk quadrangle framework is derived.
    
[^190]: 基于数据流分析启发的深度学习用于高效漏洞检测

    Dataflow Analysis-Inspired Deep Learning for Efficient Vulnerability Detection. (arXiv:2212.08108v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2212.08108](http://arxiv.org/abs/2212.08108)

    本论文提出了一种基于数据流分析启发的深度学习方法，用于高效漏洞检测。通过设计了DeepDFA框架和嵌入技术，我们实现了对代码语义的更高效捕捉，使得深度学习在漏洞检测中更加有效和高性能。DeepDFA训练时间只需9分钟，且超过了所有非transformer基线模型75倍的性能。

    

    基于深度学习的漏洞检测已经展示出了很好的性能，并且在一些研究中超过了静态分析工具。然而，最高性能的方法使用基于token的transformer模型，这不是捕捉漏洞检测所需的代码语义最高效的方法。传统的程序分析技术，如数据流分析，可以根据其根本原因检测出许多类型的错误。在本文中，我们提出将此类基于因果关系的漏洞检测算法与深度学习相结合，以实现更高效和有效的漏洞检测。具体而言，我们设计了DeepDFA，这是一个基于数据流分析启发的图学习框架和一种嵌入技术，可以使图学习模拟数据流计算。我们展示了DeepDFA既具有性能又具有效率。DeepDFA超过了所有非transformer基线模型。它的训练时间只需9分钟，比具有最高性能的基线模型快75倍。

    Deep learning-based vulnerability detection has shown great performance and, in some studies, outperformed static analysis tools. However, the highest-performing approaches use token-based transformer models, which are not the most efficient to capture code semantics required for vulnerability detection. Classical program analysis techniques such as dataflow analysis can detect many types of bugs based on their root causes. In this paper, we propose to combine such causal-based vulnerability detection algorithms with deep learning, aiming to achieve more efficient and effective vulnerability detection. Specifically, we designed DeepDFA, a dataflow analysis-inspired graph learning framework and an embedding technique that enables graph learning to simulate dataflow computation. We show that DeepDFA is both performant and efficient. DeepDFA outperformed all non-transformer baselines. It was trained in 9 minutes, 75x faster than the highest-performing baseline model. When using only 50+ v
    
[^191]: 具有未知测量噪声的物理信息神经网络

    Physics-informed neural networks with unknown measurement noise. (arXiv:2211.15498v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.15498](http://arxiv.org/abs/2211.15498)

    这篇论文提出了一种解决物理信息神经网络在存在非高斯噪声情况下失效的问题的方法，即通过同时训练一个能量模型来学习正确的噪声分布。通过多个例子的实验证明了该方法的改进性能。

    

    物理信息神经网络(PINNs)是一种既能找到解决方案又能识别偏微分方程参数的灵活方法。大多数相关的研究都假设数据是无噪声的，或者是受弱高斯噪声污染的。我们展示了标准PINN框架在非高斯噪声情况下失效的问题，并提出了一种解决这个根本性问题的方法，即同时训练一个能量模型(Energy-Based Model, EBM)来学习正确的噪声分布。我们通过多个例子展示了我们方法的改进性能。

    Physics-informed neural networks (PINNs) constitute a flexible approach to both finding solutions and identifying parameters of partial differential equations. Most works on the topic assume noiseless data, or data contaminated by weak Gaussian noise. We show that the standard PINN framework breaks down in case of non-Gaussian noise. We give a way of resolving this fundamental issue and we propose to jointly train an energy-based model (EBM) to learn the correct noise distribution. We illustrate the improved performance of our approach using multiple examples.
    
[^192]: ARISE：通过子结构认知在属性网络上进行图像异常检测

    ARISE: Graph Anomaly Detection on Attributed Networks via Substructure Awareness. (arXiv:2211.15255v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15255](http://arxiv.org/abs/2211.15255)

    ARISE是一个基于属性网络的图像异常检测框架，通过识别子结构来提高拓扑异常检测性能。

    

    最近，属性网络上的图像异常检测引起了数据挖掘和机器学习社区的广泛关注。除了属性异常外，图像异常检测还旨在检测表现出集体异常行为的具有可疑拓扑异常的节点。紧密连接的不相关节点组在网络中形成异常密集的子结构。然而，现有方法忽视了通过识别这种集体模式，可以提高拓扑异常检测性能的事实。为此，我们提出了一种基于属性网络的图像异常检测框架，通过子结构认知（简称ARISE）。与以往的算法不同，我们的重点是识别图中的子结构以辨别异常。具体而言，我们建立了一个区域提案模块，以发现网络中的高密度子结构作为可疑区域。节点对相似度的平均值可被视为子结构内节点的拓扑异常程度。

    Recently, graph anomaly detection on attributed networks has attracted growing attention in data mining and machine learning communities. Apart from attribute anomalies, graph anomaly detection also aims at suspicious topological-abnormal nodes that exhibit collective anomalous behavior. Closely connected uncorrelated node groups form uncommonly dense substructures in the network. However, existing methods overlook that the topology anomaly detection performance can be improved by recognizing such a collective pattern. To this end, we propose a new graph anomaly detection framework on attributed networks via substructure awareness (ARISE for abbreviation). Unlike previous algorithms, we focus on the substructures in the graph to discern abnormalities. Specifically, we establish a region proposal module to discover high-density substructures in the network as suspicious regions. The average node-pair similarity can be regarded as the topology anomaly degree of nodes within substructures
    
[^193]: 通过寻找基于任务的平坦区域来改进多任务学习

    Improving Multi-task Learning via Seeking Task-based Flat Regions. (arXiv:2211.13723v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.13723](http://arxiv.org/abs/2211.13723)

    通过寻找基于任务的平坦区域，可以改进多任务学习并提高模型性能，但需要正确使用正则化技术以避免次优解。

    

    多任务学习（MTL）是一种广泛使用且强大的学习范式，用于训练深度神经网络，可以通过单个骨干学习多个目标。与单独训练任务相比，MTL显着降低了计算成本，提高了数据效率，并通过利用任务之间的知识来潜在地提高模型性能。因此，它已经被应用于各种应用领域，从计算机视觉到自然语言处理和语音识别。其中，MTL的一个新兴研究方向集中在操纵任务梯度以推导出对所有任务有益的最终梯度下降方向。尽管在许多基准测试上取得了令人印象深刻的结果，但是在实际问题上直接应用这些方法而不使用适当的正则化技术可能会导致次优解。特别是，标准训练在训练数据上最小化经验损失，很容易遭受过拟合问题。

    Multi-Task Learning (MTL) is a widely-used and powerful learning paradigm for training deep neural networks that allows learning more than one objective by a single backbone. Compared to training tasks separately, MTL significantly reduces computational costs, improves data efficiency, and potentially enhances model performance by leveraging knowledge across tasks. Hence, it has been adopted in a variety of applications, ranging from computer vision to natural language processing and speech recognition. Among them, there is an emerging line of work in MTL that focuses on manipulating the task gradient to derive an ultimate gradient descent direction to benefit all tasks. Despite achieving impressive results on many benchmarks, directly applying these approaches without using appropriate regularization techniques might lead to suboptimal solutions on real-world problems. In particular, standard training that minimizes the empirical loss on the training data can easily suffer from overfi
    
[^194]: 知识感知的非IID数据联邦主动学习

    Knowledge-Aware Federated Active Learning with Non-IID Data. (arXiv:2211.13579v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.13579](http://arxiv.org/abs/2211.13579)

    本文提出了一种知识感知的非IID数据联邦主动学习方法，通过知识专业化主动抽样和知识补偿联邦更新来解决联邦主动学习中的目标不匹配问题。

    

    联邦学习使得多个分散的客户端能够在不共享本地训练数据的情况下进行协作学习。然而，获取本地数据标签的昂贵注释成本仍然是利用本地数据的障碍。在本文中，我们提出了一种联邦主动学习范式，以在有限注释预算的同时保护数据隐私，以分散化学习的方式高效地学习全局模型。联邦主动学习面临的主要挑战是服务器端全局模型的主动抽样目标与异步本地客户端的目标不匹配。当数据在本地客户端之间分布非IID时，这一挑战变得更加重要。为了解决上述挑战，我们提出了一种知识感知的联邦主动学习(KAFAL)方法，它包括知识专业化主动抽样(KSAS)和知识补偿联邦更新(KCFU)。

    Federated learning enables multiple decentralized clients to learn collaboratively without sharing the local training data. However, the expensive annotation cost to acquire data labels on local clients remains an obstacle in utilizing local data. In this paper, we propose a federated active learning paradigm to efficiently learn a global model with limited annotation budget while protecting data privacy in a decentralized learning way. The main challenge faced by federated active learning is the mismatch between the active sampling goal of the global model on the server and that of the asynchronous local clients. This becomes even more significant when data is distributed non-IID across local clients. To address the aforementioned challenge, we propose Knowledge-Aware Federated Active Learning (KAFAL), which consists of Knowledge-Specialized Active Sampling (KSAS) and Knowledge-Compensatory Federated Update (KCFU). KSAS is a novel active sampling method tailored for the federated acti
    
[^195]: 语言模型的整体评估

    Holistic Evaluation of Language Models. (arXiv:2211.09110v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.09110](http://arxiv.org/abs/2211.09110)

    我们提出了语言模型的整体评估（HELM），通过对潜在场景和度量进行分类并采用多度量方法，提高语言模型的透明度和可信度。

    

    语言模型（LMs）正在成为几乎所有主要语言技术的基础，但它们的能力、限制和风险并不被很好地理解。我们提出了语言模型的整体评估（HELM），以提高语言模型的透明度。首先，我们对感兴趣的潜在场景（即用例）和度量（即期望）的广阔空间进行分类。然后，我们选择了一个宽泛的子集，基于覆盖范围和可行性，注意到了缺失或未充分代表的内容（例如，为被忽视的英语方言进行问答，用于可信度的度量）。其次，我们采用多度量方法：我们分别针对每个核心场景测量了准确度、校准度、鲁棒性、公平性、偏见、有毒性和效率这7个度量指标（在87.5%的时间内）。这确保了准确度以外的度量不会被忽视，并且权衡清晰。我们还进行了7个针对性评估，基于26个针对性场景，以分析特定场景下的性能。

    Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze speci
    
[^196]: 增强型物理知识编码神经网络 (APINNs)：基于门控网络的软领域分解方法

    Augmented Physics-Informed Neural Networks (APINNs): A gating network-based soft domain decomposition methodology. (arXiv:2211.08939v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.08939](http://arxiv.org/abs/2211.08939)

    本文提出了增强型物理知识编码神经网络(APINN)，采用软领域分解和参数共享，通过门控网络初始化和一般领域和函数分解来改进了扩展物理知识编码神经网络(XPINN)和基本物理知识编码神经网络(PINN)的泛化能力。

    

    本文提出了增强型物理知识编码神经网络 (APINN)，采用软可训练的领域分解和灵活的参数共享以进一步改进扩展物理知识编码神经网络 (XPINN) 和基本物理知识编码神经网络 (PINN) 方法。具体而言，使用可训练的门控网络来模拟 XPINN 的硬分解，可以灵活地微调以发现更好的分区。APINN的输出是几个子网络的权重平均值。APINN不需要复杂的界面条件，并且其子网络可以利用所有训练样本，而不仅仅是其子域中的一部分训练数据。最后，每个子网络共享一部分共同参数，以捕捉每个分解函数中的相似组件。此外，根据胡等人[2021]的PINN泛化理论，我们展示了APINN可以通过适当的门控网络初始化和一般领域和函数分解来改善泛化能力。大量实验证明了APINN方法的有效性和优越性。

    In this paper, we propose the augmented physics-informed neural network (APINN), which adopts soft and trainable domain decomposition and flexible parameter sharing to further improve the extended PINN (XPINN) as well as the vanilla PINN methods. In particular, a trainable gate network is employed to mimic the hard decomposition of XPINN, which can be flexibly fine-tuned for discovering a potentially better partition. It weight-averages several sub-nets as the output of APINN. APINN does not require complex interface conditions, and its sub-nets can take advantage of all training samples rather than just part of the training data in their subdomains. Lastly, each sub-net shares part of the common parameters to capture the similar components in each decomposed function. Furthermore, following the PINN generalization theory in Hu et al. [2021], we show that APINN can improve generalization by proper gate network initialization and general domain & function decomposition. Extensive experi
    
[^197]: CorruptEncoder：基于数据污染的对比学习后门攻击

    CorruptEncoder: Data Poisoning based Backdoor Attacks to Contrastive Learning. (arXiv:2211.08229v4 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2211.08229](http://arxiv.org/abs/2211.08229)

    本文分析了现有数据污染后门攻击对对比学习的局限性，并提出了一种名为CorruptEncoder的新型攻击方法，通过理论导向的方式创建优化的污染输入，大幅提高攻击效果。实验证明，CorruptEncoder是首个仅需要少量图像和污染比例即可达到90%以上攻击成功率的攻击方法。同时，本文提出了一种名为局部裁剪的防御策略来应对数据污染后门攻击。

    

    对比学习使用无标签的预训练数据集对通用编码器进行预训练，包括图像或图像-文本对。对比学习容易受到基于数据污染的后门攻击（DPBA）的攻击，攻击者通过向预训练数据集中注入被污染的输入来后门化编码器。然而，现有的DPBA的效果有限。本文首先分析现有攻击的局限性，并提出了一种名为CorruptEncoder的新型DPBA来对抗对比学习。CorruptEncoder使用理论导向的方法创建最优的污染输入以最大限度地提高攻击效果。实验结果表明，CorruptEncoder在攻击效果上明显优于现有的DPBA。尤其是，CorruptEncoder是首个仅需要少量（3个）参考图像和小规模污染比例（0.5%）即可达到90%以上的攻击成功率的DPBA。此外，本文还提出了一种名为局部裁剪的防御策略来抵御DPBA。实验结果表明，我们的防御策略能有效抵御DPBA。

    Contrastive learning (CL) pre-trains general-purpose encoders using an unlabeled pre-training dataset, which consists of images or image-text pairs. CL is vulnerable to data poisoning based backdoor attacks (DPBAs), in which an attacker injects poisoned inputs into the pre-training dataset so the encoder is backdoored. However, existing DPBAs achieve limited effectiveness. In this work, we take the first step to analyze the limitations of existing attacks and propose new DPBAs called CorruptEncoder to CL. CorruptEncoder uses a theory-guided method to create optimal poisoned inputs to maximize attack effectiveness. Our experiments show that CorruptEncoder substantially outperforms existing DPBAs. In particular, CorruptEncoder is the first DPBA that achieves more than 90% attack success rates with only a few (3) reference images and a small poisoning ratio (0.5%). Moreover, we also propose a defense, called localized cropping, to defend against DPBAs. Our results show that our defense ca
    
[^198]: 潜在多模态功能图模型估计

    Latent Multimodal Functional Graphical Model Estimation. (arXiv:2210.17237v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2210.17237](http://arxiv.org/abs/2210.17237)

    本研究提出了一个潜在多模态功能图模型估计的新框架，通过同时估计转换算子和潜在图来填补当前科学方法在估计多模态功能数据图模型方面的空白

    

    共同多模态功能数据采集是一种现代的方法，通过最近在神经学和生物科学中的工程突破，可以同时从同一主体中测量来自多种模式的功能数据。获取这样的数据的一个重要动机是通过结合多模态信号来发现潜在的连接性。尽管存在科学兴趣，但在估计多模态功能数据下的图模型方面仍存在差距。为此，我们提出了一个新的综合框架，对数据生成过程进行建模，并识别从观测空间到潜在空间的算子映射。然后，我们开发了一个估计器，可以同时估计转换算子和潜在图。这个估计器基于偏相关算子，我们从多元到功能设置中严格推广了它。我们的程序是pr封闭的

    Joint multimodal functional data acquisition, where functional data from multiple modes are measured simultaneously from the same subject, has emerged as an exciting modern approach enabled by recent engineering breakthroughs in the neurological and biological sciences. One prominent motivation to acquire such data is to enable new discoveries of the underlying connectivity by combining multimodal signals. Despite the scientific interest, there remains a gap in principled statistical methods for estimating the graph underlying multimodal functional data. To this end, we propose a new integrative framework that models the data generation process and identifies operators mapping from the observation space to the latent space. We then develop an estimator that simultaneously estimates the transformation operators and the latent graph. This estimator is based on the partial correlation operator, which we rigorously extend from the multivariate to the functional setting. Our procedure is pr
    
[^199]: Auxo: 高效的联邦学习通过可扩展的客户端聚类

    Auxo: Efficient Federated Learning via Scalable Client Clustering. (arXiv:2210.16656v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.16656](http://arxiv.org/abs/2210.16656)

    Auxo 提出了一种解决联邦学习中异质性问题的方法，通过将具有相似数据分布的客户端进行分组，逐步识别大规模、低可用性和资源受限的FL人群中的这些分组，并自适应地确定如何训练特定分组的模型，以实现更好的模型性能和资源效率。

    

    联邦学习(FL)是一种新兴的机器学习(ML)范式，它使得异构边缘设备能够在不向一个逻辑上集中的服务器透露原始数据的情况下共同训练ML模型。然而，除了异构设备容量外，FL参与者往往在其数据分布方面存在差异，这些差异不是独立同分布的(Non-IID)。许多现有的工作针对由于客户端异质性引起的收敛速度慢、最终准确度低和偏差等问题提出了一些点解决方案。在本文中，我们通过将有统计相似数据分布的客户端进行分组来解决这种异质性问题。我们提出了Auxo来逐步识别大规模、低可用性和资源受限的FL人群中的这些分组。然后，Auxo自适应地确定如何训练特定分组的模型，以实现更好的模型性能并确保资源的有效利用。

    Federated learning (FL) is an emerging machine learning (ML) paradigm that enables heterogeneous edge devices to collaboratively train ML models without revealing their raw data to a logically centralized server. However, beyond the heterogeneous device capacity, FL participants often exhibit differences in their data distributions, which are not independent and identically distributed (Non-IID). Many existing works present point solutions to address issues like slow convergence, low final accuracy, and bias in FL, all stemming from client heterogeneity. In this paper, we explore an additional layer of complexity to mitigate such heterogeneity by grouping clients with statistically similar data distributions (cohorts). We propose Auxo to gradually identify such cohorts in large-scale, low-availability, and resource-constrained FL populations. Auxo then adaptively determines how to train cohort-specific models in order to achieve better model performance and ensure resource efficiency. 
    
[^200]: 贝叶斯赌博机问题的连续时间极限

    Continuous-in-time Limit for Bayesian Bandits. (arXiv:2210.07513v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2210.07513](http://arxiv.org/abs/2210.07513)

    本文提出了一种适用于解决大时间长度下的贝叶斯赌博机问题的近似贝叶斯最优策略，并且其计算成本不包括依赖于时间长度的项。

    

    本文重新审视了贝叶斯设置下的赌博机问题。贝叶斯方法将赌博机问题制定为一个优化问题，旨在寻找最优策略以最小化贝叶斯遗憾。面对的主要挑战之一是，当问题的时间长度或臂数较大时，计算最优策略通常是不可行的。我们首先展示了在适当的重缩放下，贝叶斯赌博机问题收敛于一个连续的哈密尔顿 - 雅各比 - 贝尔曼（HJB）方程。对于常见的一些赌博机问题，可以明确获得极限HJB方程的最优策略，并且在无法明确解决方案的情况下，我们提供了解决HJB方程的数字方法。基于这些结果，我们提出了一种适用于解决大时间长度下的贝叶斯赌博机问题的近似贝叶斯最优策略。我们的方法的计算成本不包括依赖于时间长度的项，这与现有方法不同。数值模拟表明了我们方法的有效性。

    This paper revisits the bandit problem in the Bayesian setting. The Bayesian approach formulates the bandit problem as an optimization problem, and the goal is to find the optimal policy which minimizes the Bayesian regret. One of the main challenges facing the Bayesian approach is that computation of the optimal policy is often intractable, especially when the length of the problem horizon or the number of arms is large. In this paper, we first show that under a suitable rescaling, the Bayesian bandit problem converges toward a continuous Hamilton-Jacobi-Bellman (HJB) equation. The optimal policy for the limiting HJB equation can be explicitly obtained for several common bandit problems, and we give numerical methods to solve the HJB equation when an explicit solution is not available. Based on these results, we propose an approximate Bayes-optimal policy for solving Bayesian bandit problems with large horizons. Our method has the added benefit that its computational cost does not inc
    
[^201]: 通过在双曲流形上使用GPLVM将机器人分类带入连续领域

    Bringing robotics taxonomies to continuous domains via GPLVM on hyperbolic manifolds. (arXiv:2210.01672v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.01672](http://arxiv.org/abs/2210.01672)

    本论文提出了一种通过在双曲流形上使用GPLVM来在连续领域中应用机器人分类法的方法，通过捕捉相关层次结构的双曲嵌入来建模分类数据，并采用图形先验和保持距离的后向约束来实现分类法结构的纳入。

    

    机器人分类被用作将人类的移动和与环境互动的方式进行高层次的分层抽象。它们已被证明对于分析抓取、操纵技能和全身支撑姿势非常有用。尽管我们在设计层次结构和基础类别方面做出了大量努力，但它们在应用领域的使用仍然有限。这可能是因为缺乏填补分类层级结构和与其类别相关联的高维异构数据之间差距的计算模型。为了解决这个问题，我们建议通过捕捉相关层次结构的双曲嵌入来建模分类数据。我们通过构建一个新颖的高斯过程双曲潜变量模型来实现这一点，该模型通过图形先验和保持距离的后向约束将分类法结构纳入潜在空间中。我们在三个不同的机器人分类法上验证了我们的模型。

    Robotic taxonomies serve as high-level hierarchical abstractions that classify how humans move and interact with their environment. They have proven useful to analyse grasps, manipulation skills, and whole-body support poses. Despite substantial efforts devoted to design their hierarchy and underlying categories, their use in application fields remains limited. This may be attributed to the lack of computational models that fill the gap between the discrete hierarchical structure of the taxonomy and the high-dimensional heterogeneous data associated to its categories. To overcome this problem, we propose to model taxonomy data via hyperbolic embeddings that capture the associated hierarchical structure. We achieve this by formulating a novel Gaussian process hyperbolic latent variable model that incorporates the taxonomy structure through graph-based priors on the latent space and distance-preserving back constraints. We validate our model on three different robotics taxonomies to lear
    
[^202]: NAG-GS: 半隐式、加速和稳健的随机优化器

    NAG-GS: Semi-Implicit, Accelerated and Robust Stochastic Optimizer. (arXiv:2209.14937v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2209.14937](http://arxiv.org/abs/2209.14937)

    NAG-GS是一种半隐式、加速和稳健的随机优化器，通过使用加速的类Nesterov随机微分方程（SDE）和半隐式Gauss-Seidel类型离散化，该方法在二次函数最小化的情况下具有收敛性和稳定性，并在竞争性任务上表现出良好的性能。

    

    传统的机器学习模型，如深度神经网络，通常通过使用基于随机梯度下降（SGD）算法进行训练。经典的SGD可以解释为随机梯度流的离散化。本文提出了一种新颖、稳健且加速的随机优化器，它依赖于两个关键因素：（1）加速的类Nesterov随机微分方程（SDE）和（2）其半隐式Gauss-Seidel类型离散化。首先，在二次函数最小化的情况下广泛研究了所得方法的收敛性和稳定性。通过仔细分析我们方法的所有超参数相对于迭代矩阵和稳态下的协方差矩阵的谱半径，我们可以得到一个优化的学习率，以保证NAG-GS的收敛速度和稳定性。此外，我们还展示了NAG-GS在竞争性任务上的性能。

    Classical machine learning models such as deep neural networks are usually trained by using Stochastic Gradient Descent-based (SGD) algorithms. The classical SGD can be interpreted as a discretization of the stochastic gradient flow. In this paper we propose a novel, robust and accelerated stochastic optimizer that relies on two key elements: (1) an accelerated Nesterov-like Stochastic Differential Equation (SDE) and (2) its semi-implicit Gauss-Seidel type discretization. The convergence and stability of the obtained method, referred to as NAG-GS, are first studied extensively in the case of the minimization of a quadratic function. This analysis allows us to come up with an optimal learning rate in terms of the convergence rate while ensuring the stability of NAG-GS. This is achieved by the careful analysis of the spectral radius of the iteration matrix and the covariance matrix at stationarity with respect to all hyperparameters of our method. Further, we show that NAG- GS is competi
    
[^203]: 使用机器学习和Shapley值预测群集赤道等离子体泡的论文

    Predicting Swarm Equatorial Plasma Bubbles via Machine Learning and Shapley Values. (arXiv:2209.13482v2 [physics.space-ph] UPDATED)

    [http://arxiv.org/abs/2209.13482](http://arxiv.org/abs/2209.13482)

    本研究提出了一种名为APE的机器学习模型，可以准确预测Swarm航天器上的等离子体泡指数。该模型在各项指标上表现良好，对赤道等离子体泡的预测具有重要意义。

    

    本研究中，我们提出了一种名为APE的机器学习模型，可以准确预测Swarm航天器上的电离层泡指数（IBI）。IBI是等离子体密度扰动与磁场之间的相关性（$R^2$），其来源可以是赤道等离子体泡（EPB）。EPB已经研究了多年，但其天天变化使得对其进行预测成为一项相当大的挑战。我们构建了一个集成机器学习模型来预测IBI。我们使用2014-22年的数据，分辨率为1秒，并将其转换为一个具有相应EPB $R^2$（0-1）标签的6维空间。APE在所有指标上表现良好，展示了分数、关联和均方根误差分别为0.96、0.98和0.08。该模型在日落后、美洲/大西洋地区、春秋分和太阳活动较高时表现最佳。这是一个有希望的结果，因为EPB是最重要的等离子体物理现象之一。

    In this study we present AI Prediction of Equatorial Plasma Bubbles (APE), a machine learning model that can accurately predict the Ionospheric Bubble Index (IBI) on the Swarm spacecraft. IBI is a correlation ($R^2$) between perturbations in plasma density and the magnetic field, whose source can be Equatorial Plasma Bubbles (EPBs). EPBs have been studied for a number of years, but their day-to-day variability has made predicting them a considerable challenge. We build an ensemble machine learning model to predict IBI. We use data from 2014-22 at a resolution of 1sec, and transform it from a time-series into a 6-dimensional space with a corresponding EPB $R^2$ (0-1) acting as the label. APE performs well across all metrics, exhibiting a skill, association and root mean squared error score of 0.96, 0.98 and 0.08 respectively. The model performs best post-sunset, in the American/Atlantic sector, around the equinoxes, and when solar activity is high. This is promising because EPBs are mos
    
[^204]: 胸部X光深度学习基础模型中的偏倚风险

    Risk of Bias in Chest Radiography Deep Learning Foundation Models. (arXiv:2209.02965v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.02965](http://arxiv.org/abs/2209.02965)

    该研究分析了一种最近发布的胸部X光基础模型中的偏倚风险，并发现在生物性别和种族之间存在亚组性能差距。

    

    目的：分析最近发布的胸部X光基础模型是否存在偏倚，可能导致在生物性别和种族之间存在亚组性能差距。材料和方法：本回顾性研究使用CheXpert数据集中自2002年10月至2017年7月期间收集的42,884名患者（年龄平均为63岁，标准偏差为17岁；男性23,623人，女性19,261人）的127,118张胸部X光。使用降维方法和两样本Kolmogorov-Smirnov检验检测胸部X光基础模型和基础深度学习模型生成的特征中的分布偏差，以确定是否存在偏倚。然后进行全面的疾病检测性能分析，将特征中的任何偏倚与患者亚组的分类性能差异相关联。

    Purpose: To analyze a recently published chest radiography foundation model for the presence of biases that could lead to subgroup performance disparities across biological sex and race.  Materials and Methods: This retrospective study used 127,118 chest radiographs from 42,884 patients (mean age, 63 [SD] 17 years; 23,623 male, 19,261 female) from the CheXpert dataset collected between October 2002 and July 2017. To determine the presence of bias in features generated by a chest radiography foundation model and baseline deep learning model, dimensionality reduction methods together with two-sample Kolmogorov-Smirnov tests were used to detect distribution shifts across sex and race. A comprehensive disease detection performance analysis was then performed to associate any biases in the features to specific disparities in classification performance across patient subgroups.  Results: Ten out of twelve pairwise comparisons across biological sex and race showed statistically significant di
    
[^205]: 规范化聚类准确度：一种非对称的外部聚类有效度量

    Normalised clustering accuracy: An asymmetric external cluster validity measure. (arXiv:2209.02935v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.02935](http://arxiv.org/abs/2209.02935)

    本文提出了一种非对称的外部聚类有效度量方法，旨在区分不同任务类型上表现良好和系统性表现不佳的聚类算法。与传统的内部度量不同，该方法利用参考真实分组进行评估，并弥补了现有方法在最坏情况下的误差。

    

    没有一个最好的聚类算法，我们仍然希望能够区分出在某些任务类型上表现良好和系统性表现不佳的方法。传统上，聚类算法使用内部或外部有效度量进行评估。内部度量量化所得分区的不同方面，例如，簇紧密度的平均程度或点的可分离性。然而，它们的有效性是有问题的，因为它们促使的聚类有时可能是无意义的。另一方面，外部度量将算法的输出与由专家提供的参考真实分组进行比较。在本文中，我们认为常用的经典分区相似性评分，例如规范化互信息、Fowlkes-Mallows或调整兰德指数，缺少一些可取的属性，例如，它们不能正确识别最坏情况，也不易解释。

    There is no, nor will there ever be, single best clustering algorithm, but we would still like to be able to distinguish between methods which work well on certain task types and those that systematically underperform. Clustering algorithms are traditionally evaluated using either internal or external validity measures. Internal measures quantify different aspects of the obtained partitions, e.g., the average degree of cluster compactness or point separability. Yet, their validity is questionable, because the clusterings they promote can sometimes be meaningless. External measures, on the other hand, compare the algorithms' outputs to the reference, ground truth groupings that are provided by experts. In this paper, we argue that the commonly-used classical partition similarity scores, such as the normalised mutual information, Fowlkes-Mallows, or adjusted Rand index, miss some desirable properties, e.g., they do not identify worst-case scenarios correctly or are not easily interpretab
    
[^206]: 提升异构联邦学习的知识提取和多模型融合

    Enhancing Heterogeneous Federated Learning with Knowledge Extraction and Multi-Model Fusion. (arXiv:2208.07978v2 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2208.07978](http://arxiv.org/abs/2208.07978)

    本文提出了一种资源感知的联邦学习方法，通过知识蒸馏将边缘模型的本地知识整合为鲁棒的全局知识，实现高效的多模型知识融合，并在保持模型异质性的同时降低通信成本和提高性能。

    

    本文提出了一种新的联邦学习方法，旨在解决用户数据隐私问题，该方法可以在边缘设备上训练机器学习模型而不访问敏感数据。传统的联邦学习方法虽然具有隐私保护性，但由于依赖于聚合方法，无法管理模型异质性，并造成高通信成本。为了解决这个问题，我们提出了一种资源感知的联邦学习方法，该方法通过知识蒸馏，将来自边缘模型的本地知识整合为鲁棒的全局知识。这种方法可以实现高效的多模型知识融合，并在保持模型异质性的同时部署资源感知的模型。与现有的联邦学习算法相比，我们的方法在异构数据和模型中改善了通信成本和性能。值得注意的是，我们的方法将ResNet-32的通信成本降低了最多50\％，将VGG-11的通信成本降低了最多10倍，同时提供了更优越的性能。

    Concerned with user data privacy, this paper presents a new federated learning (FL) method that trains machine learning models on edge devices without accessing sensitive data. Traditional FL methods, although privacy-protective, fail to manage model heterogeneity and incur high communication costs due to their reliance on aggregation methods. To address this limitation, we propose a resource-aware FL method that aggregates local knowledge from edge models and distills it into robust global knowledge through knowledge distillation. This method allows efficient multi-model knowledge fusion and the deployment of resource-aware models while preserving model heterogeneity. Our method improves communication cost and performance in heterogeneous data and models compared to existing FL algorithms. Notably, it reduces the communication cost of ResNet-32 by up to 50\% and VGG-11 by up to 10$\times$ while delivering superior performance.
    
[^207]: 监督适应性平衡内部分布泛化与外部分布检测

    Supervision Adaptation Balancing In-distribution Generalization and Out-of-distribution Detection. (arXiv:2206.09380v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.09380](http://arxiv.org/abs/2206.09380)

    该论文提出了一种监督适应性方法，通过生成自适应的监督信息来解决深度神经网络中内部分布泛化与外部分布检测之间的差异问题。

    

    深度神经网络中内部分布（ID）和外部分布（OOD）样本之间的差异可能导致网络的分布脆弱性，进而导致对OOD样本的高置信度预测。这主要是由于训练过程中缺少OOD样本，无法充分约束网络。为了解决这个问题，一些最先进的方法包括在训练中添加额外的OOD样本，并为其分配手动定义的标签。然而，这种做法可能导致不可靠的标注，对ID分类产生负面影响。分布脆弱性对于非IID深度学习提出了一个关键挑战，即通过平衡ID泛化和OOD检测来实现对OOD容忍的ID分类。本文提出了一种新颖的“监督适应性”方法，用于为OOD样本生成自适应的监督信息，使其更兼容ID样本。

    The discrepancy between in-distribution (ID) and out-of-distribution (OOD) samples can lead to \textit{distributional vulnerability} in deep neural networks, which can subsequently lead to high-confidence predictions for OOD samples. This is mainly due to the absence of OOD samples during training, which fails to constrain the network properly. To tackle this issue, several state-of-the-art methods include adding extra OOD samples to training and assign them with manually-defined labels. However, this practice can introduce unreliable labeling, negatively affecting ID classification. The distributional vulnerability presents a critical challenge for non-IID deep learning, which aims for OOD-tolerant ID classification by balancing ID generalization and OOD detection. In this paper, we introduce a novel \textit{supervision adaptation} approach to generate adaptive supervision information for OOD samples, making them more compatible with ID samples. Firstly, we measure the dependency betw
    
[^208]: 通过蒸馏预测分布的不确定性进行联邦学习

    Federated Learning with Uncertainty via Distilled Predictive Distributions. (arXiv:2206.07562v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.07562](http://arxiv.org/abs/2206.07562)

    本论文提出了一种联邦学习的不确定性框架，每个客户端在每轮中推断其参数的后验分布和后验预测分布，并将其蒸馏为单一的深度神经网络发送给服务器。这种方法可以解决现有联邦学习方法无法估计模型不确定性的问题，并在有限数据环境下取得更准确的预测。

    

    大多数现有的联邦学习方法无法估计模型/预测的不确定性，因为客户端模型使用标准损失函数最小化方法进行训练，忽略了这种不确定性。然而，在许多情况下，特别是在有限数据环境中，考虑每个客户端模型参数的不确定性是有益的，因为它可以导致更准确的预测，并且可靠的不确定性估计可以用于诸如分布外（OOD）检测和序贯决策任务（如主动学习）等任务。我们提出了一个带有不确定性的联邦学习框架，在每一轮中，每个客户端推断其参数的后验分布和后验预测分布（PPD），将PPD蒸馏成一个单一的深度神经网络，并将该网络发送到服务器。与最近一些贝叶斯方法不同，我们的方法不要求发送所有原始数据至服务器，保护了客户隐私。

    Most existing federated learning methods are unable to estimate model/predictive uncertainty since the client models are trained using the standard loss function minimization approach which ignores such uncertainties. In many situations, however, especially in limited data settings, it is beneficial to take into account the uncertainty in the model parameters at each client as it leads to more accurate predictions and also because reliable estimates of uncertainty can be used for tasks, such as out-of-distribution (OOD) detection, and sequential decision-making tasks, such as active learning. We present a framework for federated learning with uncertainty where, in each round, each client infers the posterior distribution over its parameters as well as the posterior predictive distribution (PPD), distills the PPD into a single deep neural network, and sends this network to the server. Unlike some of the recent Bayesian approaches to federated learning, our approach does not require send
    
[^209]: GNN在推广带限函数方面的优越性比NN更加明显

    Superiority of GNN over NN in generalizing bandlimited functions. (arXiv:2206.05904v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.05904](http://arxiv.org/abs/2206.05904)

    本文研究了GNN在节点分类中插值带限函数的表达能力，结果表明，使用GNN结构以相同的精度插值带限函数所需的权重比使用完全连接的神经网络（NN）少得多。

    

    图神经网络（GNN）以其整合图形信息的能力被广泛用于数据分析。然而，GNN的表达能力仅针对图级任务进行了研究，而不是针对节点级任务，例如节点分类，其中试图从观察到的节点标签中插值出缺失的标签信息。本文研究了GNN在所述分类任务中的表达能力，它实质上是一个函数插值问题。具体而言，我们导出了GNN插值$\mathbb{R}^d$中带限函数所需的权重和层数。我们的结果显示，使用GNN架构以$\epsilon$-近似离散带限信号仅需要$O((\log \epsilon^{-1})^{d})$个权重，这比使用完全连接的神经网络（NN）得到的最佳结果的所需权重少得多 - 特别地，使用使用$O((\log \epsilon^{-1})^{d})$个样本来训练GNN以$\epsilon$-逼近带限函数。

    Graph Neural Network (GNN) with its ability to integrate graph information has been widely used for data analyses. However, the expressive power of GNN has only been studied for graph-level tasks but not for node-level tasks, such as node classification, where one tries to interpolate missing nodal labels from the observed ones. In this paper, we study the expressive power of GNN for the said classification task, which is in essence a function interpolation problem. Explicitly, we derive the number of weights and layers needed for a GNN to interpolate a band-limited function in $\mathbb{R}^d$. Our result shows that, the number of weights needed to $\epsilon$-approximate a bandlimited function using the GNN architecture is much fewer than the best known one using a fully connected neural network (NN) - in particular, one only needs $O((\log \epsilon^{-1})^{d})$ weights using a GNN trained by $O((\log \epsilon^{-1})^{d})$ samples to $\epsilon$-approximate a discretized bandlimited signal
    
[^210]: 精馏决策树

    Distillation Decision Tree. (arXiv:2206.04661v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2206.04661](http://arxiv.org/abs/2206.04661)

    精馏决策树（DDT）是一种通过将黑盒模型中的知识精馏到决策树中来促进解释性的方法。该方法建立在知识精馏的理论基础上，并且在结构稳定性的条件下可以有效实现。

    

    机器学习模型，特别是黑盒模型，因其出色的预测能力而受到广泛青睐。然而，由于缺乏可解释性，它们经常面临批评和挑战。矛盾的是，它们强大的预测能力表明对底层数据有深入的理解，从而意味着重要的解释潜力。借助知识精馏的新概念，我们引入了精馏决策树（DDT）的方法。该方法将关于数据的知识从黑盒模型精馏到决策树中，从而促进了对黑盒模型的解释。通过知识精馏过程构建的DDT的可解释性在很大程度上依赖于其结构的稳定性。我们为DDT的结构稳定性建立了理论基础，证明其在一些假设下可以实现结构稳定性。此外，我们还开发了算法用于...

    Machine learning models, particularly the black-box models, are widely favored for their outstanding predictive capabilities. However, they often face scrutiny and criticism due to the lack of interpretability. Paradoxically, their strong predictive capabilities suggest a deep understanding about the underlying data, implying significant potential for interpretation. Leveraging the emerging concept of knowledge distillation, we introduced the method of distillation decision tree (DDT). This method enables the distillation of knowledge about the data from a black-box model into a decision tree, thereby facilitating the interpretation of the black-box model. Constructed through the knowledge distillation process, the interpretability of DDT relies significantly on the stability of its structure. We establish the theoretical foundations for the structural stability of DDT, demonstrating that its structure can achieve stability under mild assumptions. Furthermore, we develop algorithms for
    
[^211]: 非同态图的解耦自监督学习

    Decoupled Self-supervised Learning for Non-Homophilous Graphs. (arXiv:2206.03601v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.03601](http://arxiv.org/abs/2206.03601)

    本文提出了一种用于非同态图的解耦自监督学习（DSSL）框架。通过模拟节点和链接的生成过程，将不同邻域之间的不同潜在语义解耦到自监督学习过程中。该框架对编码器不敏感，并且不需要预制的增强，对不同的图具有灵活性。

    

    本文研究了在图上进行节点表示学习的自监督学习问题。大部分现有的自监督学习方法都假设图是同质的，即连接的节点通常属于同一类或具有相似的特征。然而，在现实世界的图中，这种同质性的假设并不总是成立。为了解决这个问题，我们提出了一种用于图神经网络的解耦自监督学习（DSSL）框架。DSSL通过从语义结构的潜变量建模中模拟节点和链接的生成过程，将不同邻域之间的不同潜在语义解耦到自监督学习过程中。我们的DSSL框架对编码器不敏感，并且不需要预制的增强，因此适用于不同的图。为了有效优化该框架，我们推导了自监督目标的证据下界，并开发了一个具有变分特性的可扩展训练算法。

    This paper studies the problem of conducting self-supervised learning for node representation learning on graphs. Most existing self-supervised learning methods assume the graph is homophilous, where linked nodes often belong to the same class or have similar features. However, such assumptions of homophily do not always hold in real-world graphs. We address this problem by developing a decoupled self-supervised learning (DSSL) framework for graph neural networks. DSSL imitates a generative process of nodes and links from latent variable modeling of the semantic structure, which decouples different underlying semantics between different neighborhoods into the self-supervised learning process. Our DSSL framework is agnostic to the encoders and does not need prefabricated augmentations, thus is flexible to different graphs. To effectively optimize the framework, we derive the evidence lower bound of the self-supervised objective and develop a scalable training algorithm with variational 
    
[^212]: 用人造黄油从后验样本中去除脂肪

    Removing the fat from your posterior samples with margarine. (arXiv:2205.12841v3 [astro-ph.IM] UPDATED)

    [http://arxiv.org/abs/2205.12841](http://arxiv.org/abs/2205.12841)

    本文总结了一种使用掩蔽自回归流和核密度估计器的方法，可以学习对应于核心科学参数的边际后验密度。该方法在计算边际库尔巴克-勒布勒散度、边际贝叶斯模型维度、似然函数模拟和先验模拟等方面具有广泛应用。

    

    贝叶斯分析已经成为许多宇宙学领域的不可或缺工具，包括引力波研究、宇宙微波背景和宇宙黎明时期的21厘米信号等现象。该方法提供了一种将复杂模型与描述关键宇宙学和天体物理信号以及各种污染信号和仪器效应的'干扰参数'拟合到数据的方式。在本文中，我们总结了一种使用掩蔽自回归流和核密度估计器来学习对应于核心科学参数的边际后验密度的方法。我们发现，边际或“无干扰”的后验分布及其相关的似然函数具有许多应用，包括计算以前难以处理的边际库尔巴克-勒布勒散度和边际贝叶斯模型维度，似然函数模拟和先验模拟。我们使用玩具例子和实际案例分别展示了每个应用。

    Bayesian analysis has become an indispensable tool across many different cosmological fields including the study of gravitational waves, the Cosmic Microwave Background and the 21-cm signal from the Cosmic Dawn among other phenomena. The method provides a way to fit complex models to data describing key cosmological and astrophysical signals and a whole host of contaminating signals and instrumental effects modelled with 'nuisance parameters'. In this paper, we summarise a method that uses Masked Autoregressive Flows and Kernel Density Estimators to learn marginal posterior densities corresponding to core science parameters. We find that the marginal or 'nuisance-free' posteriors and the associated likelihoods have an abundance of applications including; the calculation of previously intractable marginal Kullback-Leibler divergences and marginal Bayesian Model Dimensionalities, likelihood emulation and prior emulation. We demonstrate each application using toy examples, examples from t
    
[^213]: LDPC码：使用顺序变分贝叶斯估计跟踪非平稳信道噪声

    LDPC codes: tracking non-stationary channel noise using sequential variational Bayesian estimates. (arXiv:2204.07037v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2204.07037](http://arxiv.org/abs/2204.07037)

    该论文介绍了使用顺序贝叶斯学习方法跟踪非平稳信道噪声的LDPC码模型，并通过在5G行驶测试数据上的实验表明，该模型能够优于具有固定信道噪声知识的LDPC码。

    

    我们提出了一种顺序贝叶斯学习方法，用于使用概率图模型跟踪LDPC码中非平稳信噪比。我们使用一种通用的聚簇图构造算法——分层树运行交集属性（LTRIP）算法将LDPC码表示为聚簇图。信道噪声估计器是一个全局Gamma聚团，我们将其扩展以允许贝叶斯跟踪非平稳噪声变化。我们在真实的5G行驶测试数据上评估了我们提出的模型。我们的结果表明，我们的模型能够跟踪非平稳信道噪声，优于具有固定实际平均信道噪声知识的LDPC码。

    We present a sequential Bayesian learning method for tracking non-stationary signal-to-noise ratios in LDPC codes using probabilistic graphical models. We represent the LDPC code as a cluster graph using a general purpose cluster graph construction algorithm called the layered trees running intersection property (LTRIP) algorithm. The channel noise estimator is a global Gamma cluster, which we extend to allow for Bayesian tracking of non-stationary noise variation. We evaluate our proposed model on real-world 5G drive test data. Our results show that our model is capable of tracking non-stationary channel noise, which outperforms an LDPC code with a fixed knowledge of the actual average channel noise.
    
[^214]: LDPC码：比较聚类图和因子图

    LDPC codes: comparing cluster graphs to factor graphs. (arXiv:2204.06350v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2204.06350](http://arxiv.org/abs/2204.06350)

    本研究比较了LDPC码的聚类图和因子图表示，结果显示聚类图表示优于传统的因子图表示。

    

    我们对LDPC码的聚类图和因子图表示进行了比较研究。在概率图模型中，聚类图保留了推理过程中随机变量之间的有用依赖关系，这在计算成本、收敛速度和边际概率准确性方面具有优势。这项研究探讨了这些优势在LDPC码的背景下的应用，并表明聚类图表示优于传统的因子图表示。

    We present a comparison study between a cluster and factor graph representation of LDPC codes. In probabilistic graphical models, cluster graphs retain useful dependence between random variables during inference, which are advantageous in terms of computational cost, convergence speed, and accuracy of marginal probabilities. This study investigates these benefits in the context of LDPC codes and shows that a cluster graph representation outperforms the traditional factor graph representation.
    
[^215]: 基于输出感知ERM的数据驱动算法设计技术

    Output-sensitive ERM-based techniques for data-driven algorithm design. (arXiv:2204.03569v2 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2204.03569](http://arxiv.org/abs/2204.03569)

    本研究通过列举问题实例总损失函数的部分来提出了基于输出感知ERM的数据驱动算法设计技术，解决了多参数组合算法族的计算效率问题。

    

    数据驱动算法设计是一种有潜力的基于学习的方法，用于超出最坏情况分析具有可调参数的算法。一个重要的开放问题是为具有多个参数的组合算法族设计计算效率高的数据驱动算法。当固定问题实例并变化参数时，"对偶"损失函数通常具有分段可分解的结构，即除了某些尖锐的转换边界外都表现良好。在本工作中，我们通过列举一组问题实例的总损失函数的部分来开展技术研究，以开发用于数据驱动算法设计的高效ERM学习算法。我们的方法的运行时间与实际出现的部分数目成比例，而不是基于部分数目的最坏情况上界。我们的方法涉及两个新颖的要素 - 一种用于枚举由一组超平面诱导的多面体的输出感知算法。

    Data-driven algorithm design is a promising, learning-based approach for beyond worst-case analysis of algorithms with tunable parameters. An important open problem is the design of computationally efficient data-driven algorithms for combinatorial algorithm families with multiple parameters. As one fixes the problem instance and varies the parameters, the "dual" loss function typically has a piecewise-decomposable structure, i.e. is well-behaved except at certain sharp transition boundaries. In this work we initiate the study of techniques to develop efficient ERM learning algorithms for data-driven algorithm design by enumerating the pieces of the sum dual loss functions for a collection of problem instances. The running time of our approach scales with the actual number of pieces that appear as opposed to worst case upper bounds on the number of pieces. Our approach involves two novel ingredients -- an output-sensitive algorithm for enumerating polytopes induced by a set of hyperpla
    
[^216]: 用于基准测试强化学习算法的光学控制环境

    An Optical Control Environment for Benchmarking Reinforcement Learning Algorithms. (arXiv:2203.12114v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.12114](http://arxiv.org/abs/2203.12114)

    本研究实现了一个光学模拟环境，用于基准测试强化学习算法。实验结果表明，相对于传统控制算法，离策略强化学习方法在复杂光学控制环境中表现更优。

    

    深度强化学习有潜力解决各种科学问题。本文实现了一个用于强化学习基于控制器的光学模拟环境。该环境捕捉了光学系统固有的非凸性、非线性和时变噪声的本质，提供了一个更真实的设置。随后，我们提供了几种强化学习算法在提出的模拟环境中的基准结果。实验结果表明，离策略强化学习方法在导航复杂光学控制环境中的复杂性方面优于传统控制算法。论文代码可在https://github.com/Walleclipse/Reinforcement-Learning-Pulse-Stacking获取。

    Deep reinforcement learning has the potential to address various scientific problems. In this paper, we implement an optics simulation environment for reinforcement learning based controllers. The environment captures the essence of nonconvexity, nonlinearity, and time-dependent noise inherent in optical systems, offering a more realistic setting. Subsequently, we provide the benchmark results of several reinforcement learning algorithms on the proposed simulation environment. The experimental findings demonstrate the superiority of off-policy reinforcement learning approaches over traditional control algorithms in navigating the intricacies of complex optical control environments. The code of the paper is available at https://github.com/Walleclipse/Reinforcement-Learning-Pulse-Stacking.
    
[^217]: 用遗憾基于环境设计来演进课程

    Evolving Curricula with Regret-Based Environment Design. (arXiv:2203.01302v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.01302](http://arxiv.org/abs/2203.01302)

    本文提出了一种基于遗憾的课程方法，将环境设计作为学生和教师的游戏进行，以产生学生智能体能力前沿的环境实例。相比于传统的进化方法，该方法具有广泛适用性和理论保证，并在具有挑战性的设计空间中取得了有效的关卡。

    

    在强化学习中，训练具有普遍能力的智能体仍然是一个重大挑战。改进强化学习智能体鲁棒性的一个有希望的途径是使用课程。一类方法将环境设计视为学生和教师之间的一个游戏，利用基于遗憾的目标来产生学生智能体能力前沿的环境实例（或关卡）。这些方法具有广泛适用性，并在平衡状态下具有理论保证，但在具有挑战性的设计空间中经常难以找到有效的关卡。相比之下，进化方法致力于逐步改变环境复杂性，从而实现潜在的无限学习，但通常依赖于特定领域的启发式方法和大量的计算资源。在本文中，我们提出了一种基于遗憾的课程方法，以合理、有原则地利用进化的力量。我们的方法称为通过编辑关卡逐渐复合复杂性。

    It remains a significant challenge to train generally capable agents with reinforcement learning (RL). A promising avenue for improving the robustness of RL agents is through the use of curricula. One such class of methods frames environment design as a game between a student and a teacher, using regret-based objectives to produce environment instantiations (or levels) at the frontier of the student agent's capabilities. These methods benefit from their generality, with theoretical guarantees at equilibrium, yet they often struggle to find effective levels in challenging design spaces. By contrast, evolutionary approaches seek to incrementally alter environment complexity, resulting in potentially open-ended learning, but often rely on domain-specific heuristics and vast amounts of computational resources. In this paper we propose to harness the power of evolution in a principled, regret-based curriculum. Our approach, which we call Adversarially Compounding Complexity by Editing Level
    
[^218]: 群体代理强化学习

    Group-Agent Reinforcement Learning. (arXiv:2202.05135v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.05135](http://arxiv.org/abs/2202.05135)

    群体代理强化学习是一种新型的强化学习方法，其利用多个代理之间的合作来提高每个代理的学习效果。我们提出了群体代理强化学习系统的概念，并设计了一种分布式强化学习框架DDAL来支持群体代理强化学习。

    

    如果多个地理分布的代理进行合作性的个别强化学习任务，可以为每个代理的强化学习过程带来很大的好处。与多智能体强化学习（MARL）不同，MARL中多个代理共同存在于一个环境中，并且需要学习如何合作或竞争。在群体代理强化学习中，每个代理都有自己的环境，并且只与其他代理进行通信以分享知识，没有合作或竞争行为作为学习结果。事实上，这种情景在现实生活中普遍存在，其概念可以应用于许多应用领域，但尚未很好理解和表述。作为首次尝试，我们提出了群体代理强化学习系统作为对单个代理和多个代理系统的第三类强化学习系统的表述。然后，我们提出了一种分布式强化学习框架DDAL（分散式分布式异步学习），专为群体代理强化学习而设计。

    It can largely benefit the reinforcement learning (RL) process of each agent if multiple geographically distributed agents perform their separate RL tasks cooperatively. Different from multi-agent reinforcement learning (MARL) where multiple agents are in a common environment and should learn to cooperate or compete with each other, in this case each agent has its separate environment and only communicates with others to share knowledge without any cooperative or competitive behaviour as a learning outcome. In fact, this scenario exists widely in real life whose concept can be utilised in many applications, but is not well understood yet and not well formulated. As the first effort, we propose group-agent system for RL as a formulation of this scenario and the third type of RL system with respect to single-agent and multi-agent systems. We then propose a distributed RL framework called DDAL (Decentralised Distributed Asynchronous Learning) designed for group-agent reinforcement learnin
    
[^219]: GLISp-r：一种具有收敛保证的基于偏好的优化算法

    GLISp-r: A preference-based optimization algorithm with convergence guarantees. (arXiv:2202.01125v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2202.01125](http://arxiv.org/abs/2202.01125)

    GLISp-r是一种基于偏好的优化算法，通过利用代理模型和资源勘探，迭代地提出新的样本与最佳校准进行比较。

    

    基于偏好的优化算法是一种迭代过程，仅基于不同调谐之间的比较，寻求决策向量的最优校准。在每次迭代中，人为决策者表达对两个校准（样本）之间的偏好，强调哪个校准（如果有）优于另一个。优化过程必须使用观察到的偏好来找到决策者最喜欢的决策向量调谐，同时还要最小化比较的次数。在这项工作中，我们从效用理论的角度来阐述基于偏好的优化问题。然后，我们提出了GLISp-r，这是最近一种基于偏好的优化过程GLISp的扩展。后者使用径向基函数替代模型来描述决策者的喜好。GLISp-r通过在利用代理模型开发和资源勘探之间权衡，迭代地提出新的样本与最佳校准进行比较。

    Preference-based optimization algorithms are iterative procedures that seek the optimal calibration of a decision vector based only on comparisons between couples of different tunings. At each iteration, a human decision-maker expresses a preference between two calibrations (samples), highlighting which one, if any, is better than the other. The optimization procedure must use the observed preferences to find the tuning of the decision vector that is most preferred by the decision-maker, while also minimizing the number of comparisons. In this work, we formulate the preference-based optimization problem from a utility theory perspective. Then, we propose GLISp-r, an extension of a recent preference-based optimization procedure called GLISp. The latter uses a Radial Basis Function surrogate to describe the tastes of the decision-maker. Iteratively, GLISp proposes new samples to compare with the best calibration available by trading off exploitation of the surrogate model and exploration
    
[^220]: 大规模医疗数据记录中的多层次随机优化填补方法

    Multilevel Stochastic Optimization for Imputation in Massive Medical Data Records. (arXiv:2110.09680v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2110.09680](http://arxiv.org/abs/2110.09680)

    本文介绍了一种基于Kriging理论的多层次随机优化填补方法，能够更准确、更快速和更稳定地处理大规模医疗数据记录中的缺失数值数据。

    

    探索和分析大规模数据集最近在研究和发展社区中引起了越来越多的关注。长期以来，人们一直认识到许多数据集中包含大量缺失的数值数据。我们引入了一种基于Kriging理论的数学原则随机优化填补方法，该方法被证明是一种强大的填补方法。然而，其计算成本和潜在的数值不稳定性会导致昂贵和/或不可靠的预测，可能限制其在大规模数据集上的使用。在本文中，我们将最近开发的多层次随机优化方法应用于大规模医疗记录中的填补问题。该方法基于计算应用数学技术，并具有高精度。特别地，对于最佳线性无偏预测器（BLUP），该多层次形式化是精确的，而且计算速度更快，数值稳定性更高。

    Exploration and analysis of massive datasets has recently generated increasing interest in the research and development communities. It has long been a recognized problem that many datasets contain significant levels of missing numerical data. We introduce a mathematically principled stochastic optimization imputation method based on the theory of Kriging. This is shown to be a powerful method for imputation. However, its computational effort and potential numerical instabilities produce costly and/or unreliable predictions, potentially limiting its use on large scale datasets. In this paper, we apply a recently developed multi-level stochastic optimization approach to the problem of imputation in massive medical records. The approach is based on computational applied mathematics techniques and is highly accurate. In particular, for the Best Linear Unbiased Predictor (BLUP) this multi-level formulation is exact, and is also significantly faster and more numerically stable. This permits
    
[^221]: DEBOSH: 深度贝叶斯形状优化

    DEBOSH: Deep Bayesian Shape Optimization. (arXiv:2109.13337v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.13337](http://arxiv.org/abs/2109.13337)

    本论文提出了一种基于不确定性的方法，针对形状优化，在利用图神经网络预测工业设计性能时，解决了形状偏离训练集时预测不可靠的问题，并通过有效的贝叶斯优化提高了结果形状的质量。

    

    图神经网络（GNNs）可以快速准确地预测工业设计的性能，并用于有效优化其形状。然而，为了充分探索形状空间，通常需要考虑与训练集明显偏离的形状。对于这些情况，GNN的预测变得不可靠，但这通常被忽视。针对依赖高斯过程的优化技术，贝叶斯优化（BO）通过利用其评估自身精度的能力来解决这个问题。然而，当使用神经网络时，估计其不确定性的标准方法往往会导致计算量大和模型准确性降低。因此，我们提出了一种针对形状优化的新颖基于不确定性的方法。它实现了有效的BO，并提高了结果形状的质量，超过了最先进的方法。

    Graph Neural Networks (GNNs) can predict the performance of an industrial design quickly and accurately and be used to optimize its shape effectively. However, to fully explore the shape space, one must often consider shapes deviating significantly from the training set. For these, GNN predictions become unreliable, something that is often ignored. For optimization techniques relying on Gaussian Processes, Bayesian Optimization (BO) addresses this issue by exploiting their ability to assess their own accuracy. Unfortunately, this is harder to do when using neural networks because standard approaches to estimating their uncertainty can entail high computational loads and reduced model accuracy. Hence, we propose a novel uncertainty-based method tailored to shape optimization. It enables effective BO and increases the quality of the resulting shapes beyond that of state-of-the-art approaches.
    
[^222]: 稀疏加低秩矩阵分解: 一种离散优化方法

    Sparse Plus Low Rank Matrix Decomposition: A Discrete Optimization Approach. (arXiv:2109.12701v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2109.12701](http://arxiv.org/abs/2109.12701)

    本文研究稀疏加低秩矩阵分解问题(SLR)，提出了一种新的离散模型和求解方法，适用于多种应用场景。

    

    本文研究稀疏加低秩分解问题(SLR)，即将损坏的数据矩阵分解为包含基本真值的低秩矩阵和包含扰动的稀疏矩阵。 SLR是运筹学和机器学习领域的基础问题，在数据压缩、潜在语义索引、协同过滤和医学成像等各种应用中出现。我们提出了一种新的离散模型，并设计了交替最小化启发式算法以及新的半定松弛算法来解决这个问题。此外，我们还开发了一个自定义分支定界算法，利用我们的启发式算法和凸松弛来解决小规模的SLR问题。我们的启发式算法可以解决 $n=10000$ 的问题规模。

    We study the Sparse Plus Low-Rank decomposition problem (SLR), which is the problem of decomposing a corrupted data matrix into a sparse matrix of perturbations plus a low-rank matrix containing the ground truth. SLR is a fundamental problem in Operations Research and Machine Learning which arises in various applications, including data compression, latent semantic indexing, collaborative filtering, and medical imaging. We introduce a novel formulation for SLR that directly models its underlying discreteness. For this formulation, we develop an alternating minimization heuristic that computes high-quality solutions and a novel semidefinite relaxation that provides meaningful bounds for the solutions returned by our heuristic. We also develop a custom branch-and-bound algorithm that leverages our heuristic and convex relaxations to solve small instances of SLR to certifiable (near) optimality. Given an input $n$-by-$n$ matrix, our heuristic scales to solve instances where $n=10000$ in m
    
[^223]: FUTURE-AI:医学影像中值得信赖的人工智能的指导原则和共识建议

    FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Medical Imaging. (arXiv:2109.09658v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2109.09658](http://arxiv.org/abs/2109.09658)

    本论文介绍了一系列从经验、共识和最佳实践中提炼出的指导原则，旨在引领医学影像中值得信赖的人工智能的发展，提高信任、安全性和应用水平。

    

    近年来，人工智能和临床系统生成的大量数据的结合，推动了医学影像领域整个价值链上的成像人工智能解决方案的发展，包括图像重建、医学图像分割、基于图像的诊断和治疗规划。尽管医学影像中的人工智能取得了成功并有着巨大的潜力，但许多利益相关者担心成像人工智能解决方案的潜在风险和伦理问题，认为其复杂、不透明、难以理解、难以应用和难以在关键临床应用中建立信任。尽管存在这些担忧和风险，但目前尚没有具体的指导原则和最佳实践来引导未来医学影像中人工智能的发展以增加信任、安全性和采用。为了弥补这一空白，本文提出了从积累的经验、共识和最佳实践中精选出的指导原则。

    The recent advancements in artificial intelligence (AI) combined with the extensive amount of data generated by today's clinical systems, has led to the development of imaging AI solutions across the whole value chain of medical imaging, including image reconstruction, medical image segmentation, image-based diagnosis and treatment planning. Notwithstanding the successes and future potential of AI in medical imaging, many stakeholders are concerned of the potential risks and ethical implications of imaging AI solutions, which are perceived as complex, opaque, and difficult to comprehend, utilise, and trust in critical clinical applications. Despite these concerns and risks, there are currently no concrete guidelines and best practices for guiding future AI developments in medical imaging towards increased trust, safety and adoption. To bridge this gap, this paper introduces a careful selection of guiding principles drawn from the accumulated experiences, consensus, and best practices f
    
[^224]: Comfetch: 通过草图在受限客户端上开展大规模网络的联邦学习

    Comfetch: Federated Learning of Large Networks on Constrained Clients via Sketching. (arXiv:2109.08346v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.08346](http://arxiv.org/abs/2109.08346)

    Comfetch是一个通过使用草图的简化表示形式，允许资源受限的客户端进行大规模网络训练的算法。

    

    联邦学习（FL）是边缘上进行私密和协作模型训练的流行范式。在集中式FL中，全局架构的参数（如深度神经网络）由中央服务器/控制器维护和分发给客户端，后者根据本地优化向服务器传输模型更新（梯度）。尽管许多工作都致力于减少梯度传输的通信复杂性，但绝大多数基于压缩的算法都假设每个参与的客户端能够下载和训练当前和完整的参数集，这可能不是一个实际的假设，因为较小的客户端（如移动设备）可能具有资源限制。在这项工作中，我们提出了一种简单而有效的新算法Comfetch，它允许客户端使用全局架构的简化表示来训练大型网络，通过计数草图减少了本地计算和内存成本以及双向的数据传输。

    Federated learning (FL) is a popular paradigm for private and collaborative model training on the edge. In centralized FL, the parameters of a global architecture (such as a deep neural network) are maintained and distributed by a central server/controller to clients who transmit model updates (gradients) back to the server based on local optimization. While many efforts have focused on reducing the communication complexity of gradient transmission, the vast majority of compression-based algorithms assume that each participating client is able to download and train the current and full set of parameters, which may not be a practical assumption depending on the resource constraints of smaller clients such as mobile devices. In this work, we propose a simple yet effective novel algorithm, Comfetch, which allows clients to train large networks using reduced representations of the global architecture via the count sketch, which reduces local computational and memory costs along with bi-dir
    
[^225]: 约束资源下神经模块专业化的动力学研究

    Dynamics of specialization in neural modules under resource constraints. (arXiv:2106.02626v2 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2106.02626](http://arxiv.org/abs/2106.02626)

    本研究使用人工神经网络模拟实验，发现结构模块化并不一定能够确保功能专业化，在特定环境和资源限制下，才能够出现专业化现象。

    

    长期以来，人们一直认为大脑在结构和功能上高度模块化，但最近的证据使一些人对两种模块化的程度产生了怀疑。我们使用人工神经网络来测试结构模块化是否足以保证功能专业化，并发现一般情况下，并不一定成立，除非在极端水平上。然后，我们系统地测试了环境和网络的哪些特征会导致专业化的出现。我们使用了一个简单的玩具环境、任务和网络，以精确控制条件，并表明在这个设置中，几个不同的专业化度量指标给出了类似的结果。我们进一步发现，（1）专业化只能在环境中那些可以明确分离的特征存在的情况下出现，（2）专业化更容易在网络资源受到强烈限制的情况下出现，（3）这些发现在 qualitatively 上相似。

    It has long been believed that the brain is highly modular both in terms of structure and function, although recent evidence has led some to question the extent of both types of modularity. We used artificial neural networks to test the hypothesis that structural modularity is sufficient to guarantee functional specialization, and find that in general, this doesn't necessarily hold except at extreme levels. We then systematically tested which features of the environment and network do lead to the emergence of specialization. We used a simple toy environment, task and network, allowing us precise control, and show that in this setup, several distinct measures of specialization give qualitatively similar results. We further find that (1) specialization can only emerge in environments where features of that environment are meaningfully separable, (2) specialization preferentially emerges when the network is strongly resource-constrained, and (3) these findings are qualitatively similar ac
    
[^226]: 重审超参数模型中的最小描述长度复杂度

    Revisiting minimum description length complexity in overparameterized models. (arXiv:2006.10189v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2006.10189](http://arxiv.org/abs/2006.10189)

    本文重审了超参数模型中的最小描述长度复杂度。通过定义一个新的基于MDL的复杂度度量，我们发现复杂度不仅取决于参数数量，还与设计矩阵或核矩阵的奇异值和信噪比有关。

    

    复杂度是统计学习理论中的一个基本概念，旨在提供有关泛化性能的信息。在低维度情况下，参数数量在一定程度上是成功的，但在超参数模型中，当参数数量超过训练样本数量时，其合理性不足。我们重新审视了基于Rissanen最小描述长度（MDL）原理的复杂度度量，并定义了一种新的适用于超参数模型的基于MDL的复杂度（MDL-COMP）。MDL-COMP通过对一个良好的Ridge估计类所引起的编码而定义出来的最优性准则。我们对线性模型和核方法的MDL-COMP进行了广泛的理论刻画，并表明它不仅是参数数量的函数，而是设计或核矩阵的奇异值和信噪比的函数。对于具有n个观测值，d个参数和独立同分布的高斯预测因子的线性模型，MDL-COMP的尺度是线性的。

    Complexity is a fundamental concept underlying statistical learning theory that aims to inform generalization performance. Parameter count, while successful in low-dimensional settings, is not well-justified for overparameterized settings when the number of parameters is more than the number of training samples. We revisit complexity measures based on Rissanen's principle of minimum description length (MDL) and define a novel MDL-based complexity (MDL-COMP) that remains valid for overparameterized models. MDL-COMP is defined via an optimality criterion over the encodings induced by a good Ridge estimator class. We provide an extensive theoretical characterization of MDL-COMP for linear models and kernel methods and show that it is not just a function of parameter count, but rather a function of the singular values of the design or the kernel matrix and the signal-to-noise ratio. For a linear model with $n$ observations, $d$ parameters, and i.i.d. Gaussian predictors, MDL-COMP scales li
    
[^227]: 理解Transformer训练的困难

    Understanding the Difficulty of Training Transformers. (arXiv:2004.08249v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2004.08249](http://arxiv.org/abs/2004.08249)

    该论文研究了Transformer训练的困难。他们发现不平衡的梯度不是训练不稳定的根本原因，而是每一层的放大效应导致训练不稳定。他们观察到轻量级的依赖限制了模型潜力，导致表现较差的训练模型。

    

    Transformer在许多自然语言处理任务中被证明是有效的。然而，它们的训练需要设计先进的优化器和学习率调度器的非平凡工作（例如，传统的SGD无法有效训练Transformer）。我们的目标是从经验和理论的角度理解$\textit{什么使得Transformer的训练变得困难}$。我们的分析表明，不平衡的梯度并不是训练不稳定的根本原因。相反，我们确定了一种影响训练的放大效应--对于多层Transformer模型中的每一层，它对其残差分支的依赖程度较高，导致训练不稳定，因为它放大了小的参数扰动（例如参数更新），并导致模型输出中的显著扰动。然而，我们观察到轻量级的依赖限制了模型的潜力，并导致表现较差的训练模型。在我们的分析启发下，我们提出了Admin（$\textbf{Ad}$aptive 重述部分

    Transformers have proved effective in many NLP tasks. However, their training requires non-trivial efforts regarding designing cutting-edge optimizers and learning rate schedulers carefully (e.g., conventional SGD fails to train Transformers effectively). Our objective here is to understand $\textit{what complicates Transformer training}$ from both empirical and theoretical perspectives. Our analysis reveals that unbalanced gradients are not the root cause of the instability of training. Instead, we identify an amplification effect that influences training substantially -- for each layer in a multi-layer Transformer model, heavy dependency on its residual branch makes training unstable, since it amplifies small parameter perturbations (e.g., parameter updates) and results in significant disturbances in the model output. Yet we observe that a light dependency limits the model potential and leads to inferior trained models. Inspired by our analysis, we propose Admin ($\textbf{Ad}$aptive 
    
[^228]: 一个将卷积神经网络转化为图空间的方法

    A Convolutional Neural Network into graph space. (arXiv:2002.09285v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2002.09285](http://arxiv.org/abs/2002.09285)

    本文提出了一种将卷积神经网络转化为图空间的方法，实现了在非欧几里得定义的数据上进行卷积和池化操作，并且在实验中展示了其在简单任务上的优秀性能。

    

    卷积神经网络(CNNs)在几十年的发展中，在分类任务中超越了现有的技术方法。然而，CNN在构建时被限制在欧几里得空间中进行操作。因为卷积是针对欧几里得空间定义的信号操作。这限制了深度学习主要用于欧几里得定义的数据，如声音或图像。而实际上，许多计算机应用领域(包括网络分析、计算社会科学、化学信息学或计算机图形学)涉及非欧几里得定义的数据，如图、网络或流形。本文提出了一种新的卷积神经网络架构，直接在图空间中定义。卷积和池化操作在图域中定义。我们展示了其在反向传播环境下使用的可行性。实验结果表明，我们的模型在简单任务上的性能达到了最先进水平，并且在图域变化方面表现稳健。

    Convolutional neural networks (CNNs), in a few decades, have outperformed the existing state of the art methods in classification context. However, in the way they were formalised, CNNs are bound to operate on euclidean spaces. Indeed, convolution is a signal operation that are defined on euclidean spaces. This has restricted deep learning main use to euclidean-defined data such as sound or image. And yet, numerous computer application fields (among which network analysis, computational social science, chemo-informatics or computer graphics) induce non-euclideanly defined data such as graphs, networks or manifolds. In this paper we propose a new convolution neural network architecture, defined directly into graph space. Convolution and pooling operators are defined in graph domain. We show its usability in a back-propagation context. Experimental results show that our model performance is at state of the art level on simple tasks. It shows robustness with respect to graph domain change
    
[^229]: 学习编码和分类测试执行

    Learning to Encode and Classify Test Executions. (arXiv:2001.02444v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2001.02444](http://arxiv.org/abs/2001.02444)

    本文旨在通过使用监督学习和神经网络模型，解决自动测试中的测试神谕问题。作者采用对执行轨迹进行标记和训练的方法来学习区分通过和失败执行的运行时模式，实现了通用、可扩展和准确的解决方案。

    

    自动确定测试执行的正确性被称为测试神谕问题，是自动化测试中的一个关键问题。本文的目标是以一种通用、可扩展和准确的方式解决测试神谕问题。为了实现这一目标，我们使用监督学习来学习测试执行的轨迹。我们对执行轨迹的少部分进行标记，以其通过与失败的结论进行标记。我们使用这些标记的轨迹来训练一个神经网络模型，以学习区分给定程序的通过与失败执行的运行时模式。我们构建这个神经网络模型的方法涉及以下步骤，1. 为程序安装记录执行轨迹，作为方法调用和全局状态的序列，2. 对执行轨迹的少部分进行标记，3. 设计一个神经网络组件，用于将执行轨迹中的信息嵌入到固定长度的向量中，4. 设计一个使用轨迹信息的神经网络模型。

    The challenge of automatically determining the correctness of test executions is referred to as the test oracle problem and is one of the key remaining issues for automated testing. The goal in this paper is to solve the test oracle problem in a way that is general, scalable and accurate. To achieve this, we use supervised learning over test execution traces. We label a small fraction of the execution traces with their verdict of pass or fail. We use the labelled traces to train a neural network (NN) model to learn to distinguish runtime patterns for passing versus failing executions for a given program. Our approach for building this NN model involves the following steps, 1. Instrument the program to record execution traces as sequences of method invocations and global state, 2. Label a small fraction of the execution traces with their verdicts, 3. Designing a NN component that embeds information in execution traces to fixed length vectors, 4. Design a NN model that uses the trace inf
    
[^230]: 非线性参数自适应控制和预测中的隐式正则化和动量算法

    Implicit Regularization and Momentum Algorithms in Nonlinearly Parameterized Adaptive Control and Prediction. (arXiv:1912.13154v7 [math.OC] UPDATED)

    [http://arxiv.org/abs/1912.13154](http://arxiv.org/abs/1912.13154)

    本论文通过利用经典自适应非线性控制技术和最近在优化和机器学习领域的进展之间的联系，展示了在自适应非线性控制和自适应动态预测的算法发展中存在相当大的潜力。通过引入受自然梯度下降和镜像下降启发的一阶自适应法则，证明了这些法则在存在多种与数据一致的动态时隐式正则化了学习的模型。

    

    稳定的并发学习和控制动态系统是自适应控制的主题。尽管自适应控制是一个已经建立起来具有许多实际应用和丰富理论的领域，但非线性系统的自适应控制和自适应动态预测的发展主要围绕着一些关键算法。通过利用经典自适应非线性控制技术和最近在优化和机器学习领域的进展之间的密切联系，我们展示了在自适应非线性控制和自适应动态预测的算法发展中存在着相当大的潜力。我们首先介绍了受自然梯度下降和镜像下降启发的一阶自适应法则。我们证明了当存在多种与数据一致的动态时，这些非欧几里德自适应法则隐式正则化了学习的模型。因此，在学习过程中施加的局部几何性质可以用来选择参数向量 - 在可能实现完美跟踪或预测的许多参数向量中选择一个。

    Stable concurrent learning and control of dynamical systems is the subject of adaptive control. Despite being an established field with many practical applications and a rich theory, much of the development in adaptive control for nonlinear systems revolves around a few key algorithms. By exploiting strong connections between classical adaptive nonlinear control techniques and recent progress in optimization and machine learning, we show that there exists considerable untapped potential in algorithm development for both adaptive nonlinear control and adaptive dynamics prediction. We begin by introducing first-order adaptation laws inspired by natural gradient descent and mirror descent. We prove that when there are multiple dynamics consistent with the data, these non-Euclidean adaptation laws implicitly regularize the learned model. Local geometry imposed during learning thus may be used to select parameter vectors -- out of the many that will achieve perfect tracking or prediction --
    
[^231]: 用多样抽样有效地提高CNN特征图的分辨率

    Improving the Resolution of CNN Feature Maps Efficiently with Multisampling. (arXiv:1805.10766v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/1805.10766](http://arxiv.org/abs/1805.10766)

    本论文提出了一种称为多样抽样的CNN子采样技术，通过子采样层显著增加特征图保留的信息量，实验证明粗糙的特征图是影响神经网络在图像分类中性能的瓶颈。

    

    我们描述了一种新的CNN子采样技术类别，称为多样抽样，通过子采样层大大增加了特征图保留的信息量。我们的其中一个方法称为方格子采样，能够显著提高现有架构（如DenseNet和ResNet）的准确性，而无需任何额外参数，并且非常显著地提高了某些预先训练好的ImageNet模型的准确性，且无需训练或微调。我们对数据增强的性质有了一些可能的观察，并通过实验证明，粗糙的特征图是影响神经网络在图像分类中性能的瓶颈。

    We describe a new class of subsampling techniques for CNNs, termed multisampling, that significantly increases the amount of information kept by feature maps through subsampling layers. One version of our method, which we call checkered subsampling, significantly improves the accuracy of state-of-the-art architectures such as DenseNet and ResNet without any additional parameters and, remarkably, improves the accuracy of certain pretrained ImageNet models without any training or fine-tuning. We glean possible insight into the nature of data augmentations and demonstrate experimentally that coarse feature maps are bottlenecking the performance of neural networks in image classification.
    

