# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [A Note on Randomized Kaczmarz Algorithm for Solving Doubly-Noisy Linear Systems.](http://arxiv.org/abs/2308.16904) | 本文分析了当系数矩阵和向量都存在加性和乘性噪声时，随机Kaczmarz算法在解决噪声线性系统中的收敛性。分析表明，RK的收敛性受到𝜏的大小影响，其中𝜏表示带有噪声的系数矩阵A的乘子范数的平方与Frobenius范数的平方的乘积。 |
| [^2] | [Learning to Taste: A Multimodal Wine Dataset.](http://arxiv.org/abs/2308.16900) | 这个论文介绍了一个大型多模态葡萄酒数据集，用于研究视觉感知、语言和口感之间的关系，并提出了低维概念嵌入算法，将人类经验与自动机器相似度核相结合，改进了口味分类，并与人类口味知觉相一致。 |
| [^3] | [Transformers as Support Vector Machines.](http://arxiv.org/abs/2308.16898) | 这项工作建立了自注意力和硬间隔支持向量机问题之间的正式等价关系，通过转换器架构的优化几何来解决自然语言处理问题，同时揭示了梯度下降优化的转换器的隐式偏差。 |
| [^4] | [PointOcc: Cylindrical Tri-Perspective View for Point-based 3D Semantic Occupancy Prediction.](http://arxiv.org/abs/2308.16896) | 本文提出了一个新的三透视圆柱视图表示方法，在自动驾驶中进行点云三维语义占据预测。通过构建柱坐标系下的三透视圆柱视图，实现了对点云的精细建模，同时利用空间组池化和二维主干网络高效处理数据。结果表明，该方法可以有效地预测点云的语义占据情况。 |
| [^5] | [Language-Conditioned Path Planning.](http://arxiv.org/abs/2308.16893) | 本研究提出了一种语言条件路径规划的方法，通过学习碰撞函数，预测机器人与环境之间的碰撞，实现灵活、有条件的路径规划，无需手动标注或者真实物体模型。 |
| [^6] | [GNFactor: Multi-Task Real Robot Learning with Generalizable Neural Feature Fields.](http://arxiv.org/abs/2308.16891) | GNFactor是一个用于多任务机器人操作的代理方法，它利用可泛化神经特征场和Perceiver Transformer模块，以及深度三维体素表示来实现对真实世界环境中的操作任务的执行。它通过将视觉和语义信息纳入三维表示来提高场景的理解能力，并在多个任务上进行了验证。 |
| [^7] | [Federated Learning in UAV-Enhanced Networks: Joint Coverage and Convergence Time Optimization.](http://arxiv.org/abs/2308.16889) | 本研究针对无人机增强网络中的联邦学习问题，提出了一种能够同时优化覆盖范围和减少收敛时间的解决方案。通过分析无人机增强的无线传感器网络的统计特征，以及基于多目标多臂赌博机理论，将FL和无人机部署方法相结合，能够有效地减少FL延迟并提高网络性能。 |
| [^8] | [Prediction of Diblock Copolymer Morphology via Machine Learning.](http://arxiv.org/abs/2308.16886) | 通过利用机器学习方法，该论文提出了一种加速计算嵌段聚合物形态演化的策略，利用粒子模拟学习驱动缺陷消除过程，采用UNet架构实现了不同边界条件下的预测，并应用可解释的人工智能方法可视化了形态演化过程。 |
| [^9] | [The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants.](http://arxiv.org/abs/2308.16884) | Belebele是一个包含122种语言变体的多选机器阅读理解数据集，可用于评估文本模型在高、中和低资源语言中的性能。尽管英语为中心的大型语言模型在跨语言转移方面表现良好，但小型多语言遮蔽语言模型在其他语言上表现更佳。 |
| [^10] | [Information Theoretically Optimal Sample Complexity of Learning Dynamical Directed Acyclic Graphs.](http://arxiv.org/abs/2308.16859) | 本文研究了学习动态有向无环图（DDAG）的信息理论最优样本复杂度，提出了一种基于观测时间序列的功率谱密度矩阵的度量和算法来重建DDAG。 |
| [^11] | [Majorization-Minimization for sparse SVMs.](http://arxiv.org/abs/2308.16858) | 该论文提出了一种通过平滑的稀疏促进正则化的方法来训练支持向量机（SVM），并通过主要化-最小化方法实现快速训练。该方法可处理稀疏保留正则化器，提高性能，并通过在多个数据集上的测试验证其优越性能。 |
| [^12] | [Natural Quantum Monte Carlo Computation of Excited States.](http://arxiv.org/abs/2308.16848) | 该论文提出了一种变分蒙特卡洛算法，用于估计量子系统中的激发态，通过转化问题使其成为寻找扩展系统的基态的问题。这种方法特别适用于多电子系统，并且可以准确地计算各种可观测量的期望值，包括非对角线期望值和跃迁偶极矩，并在苯等大分子上得到了良好的结果。 |
| [^13] | [Diffusion Models for Interferometric Satellite Aperture Radar.](http://arxiv.org/abs/2308.16847) | 该论文使用概率扩散模型（PDMs）生成了基于雷达的卫星图像数据集，展示了PDMs成功生成了复杂和逼真结构的图像，但采样时间仍然是一个问题。他们还提供了一个简单而多功能的开源工具，可以在单个GPU上训练、采样和评估PDMs使用任何数据集。 |
| [^14] | [FedDD: Toward Communication-efficient Federated Learning with Differential Parameter Dropout.](http://arxiv.org/abs/2308.16835) | 本文提出了一种通过差分参数丢弃实现通信高效的联邦学习方案（FedDD）。这种方案避免了频繁交换模型参数的通信延迟问题，并通过模型参数丢弃而不是客户端选择来优化全局模型泛化能力。 |
| [^15] | [Latent Variable Multi-output Gaussian Processes for Hierarchical Datasets.](http://arxiv.org/abs/2308.16822) | 本文提出了一种针对层级数据集的扩展多输出高斯过程 (MOGPs)，通过引入潜在变量和特定核函数，可以更好地捕捉不同输出之间的关系和相关性。这种方法预计能够在任务数量增加时提高可扩展性。 |
| [^16] | [Irregular Traffic Time Series Forecasting Based on Asynchronous Spatio-Temporal Graph Convolutional Network.](http://arxiv.org/abs/2308.16818) | 该论文提出了一种基于异步时空图卷积网络的不规则交通时间序列预测方法，用于解决智能交叉口产生的异步空间依赖、不规则时间依赖和可变长度序列预测等挑战。 |
| [^17] | [Rank Collapse Causes Over-Smoothing and Over-Correlation in Graph Neural Networks.](http://arxiv.org/abs/2308.16800) | 本文研究了图神经网络中的平滑过度和特征关联过高现象，发现固定不变的子空间导致了节点表示的等级崩塌。在该子空间中平滑向量的存在导致过度平滑，即使避免过度平滑也会导致过高的关联。为了解决这个问题，我们提出了一种克罗内克积之和作为一种有效方法。 |
| [^18] | [Joint Semantic-Native Communication and Inference via Minimal Simplicial Structures.](http://arxiv.org/abs/2308.16789) | 通过使用最小单纯结构，这项研究实现了学生代理和教师代理之间的语义通信和推断，提高了推断查询的准确性。 |
| [^19] | [StratMed: Relevance Stratification for Low-resource Medication Recommendation.](http://arxiv.org/abs/2308.16781) | StratMed是一种面向低资源药物推荐的模型，通过相关性分层机制来解决医疗数据长尾分布不平衡的问题，平衡了药物组合的安全性和准确性。 |
| [^20] | [Efficacy of Neural Prediction-Based NAS for Zero-Shot NAS Paradigm.](http://arxiv.org/abs/2308.16775) | 这项研究提出了一种新的方法，通过深度学习进行零样本架构搜索，通过使用可学习的傅里叶正弦和求和编码来构建计算的前馈图，从而解决了基于预测的神经架构搜索中性能指标泛化的限制。 |
| [^21] | [Constructing Indoor Region-based Radio Map without Location Labels.](http://arxiv.org/abs/2308.16759) | 本文提出了一种无需位置标签的基于区域的无线电地图构建方法，该方法利用接收信号强度（RSS）测量数据，并通过一个综合的分割和聚类算法实现了全局最优解的匹配。 |
| [^22] | [Training Neural Networks Using Reproducing Kernel Space Interpolation and Model Reduction.](http://arxiv.org/abs/2308.16754) | 这篇论文研究了使用再生核空间插值和模型简化训练神经网络的理论，并推导了一个多维的亚当扬-阿罗夫-克雷因定理，提出了一种新型的神经网络架构（PNN），在噪声环境中表现出比传统方法更优越的性能。 |
| [^23] | [Moreau Envelope ADMM for Decentralized Weakly Convex Optimization.](http://arxiv.org/abs/2308.16752) | 本文提出了一种分布式弱凸优化问题的近似变种ADMM算法，通过分析Moreau包络函数，证明了该算法在一些条件下能收敛到稳定点，同时实验结果表明该方法比常用方法更快、更稳健。 |
| [^24] | [US-SFNet: A Spatial-Frequency Domain-based Multi-branch Network for Cervical Lymph Node Lesions Diagnoses in Ultrasound Images.](http://arxiv.org/abs/2308.16738) | 本论文提出了一种基于空间频域的多分支网络US-SFNet，用于超声图像中颈部淋巴结病变的诊断。通过使用Conv-FFT块来建模图像，实现更准确的诊断结果。 |
| [^25] | [Robust Networked Federated Learning for Localization.](http://arxiv.org/abs/2308.16737) | 本文提出了一种鲁棒的网络化联邦学习方法，通过采用$L_1$-范数鲁棒性和分布式次梯度框架，解决了在分布式环境中定位问题中的异常数据干扰和算法收敛挑战。 |
| [^26] | [Robust Representation Learning for Unreliable Partial Label Learning.](http://arxiv.org/abs/2308.16718) | 本文介绍了一种解决不可靠部分标签学习问题的鲁棒表示学习方法，通过利用鲁棒对比学习和标签质量改进策略，提高模型性能并抵抗不准确部分标签的影响。 |
| [^27] | [Everyone Can Attack: Repurpose Lossy Compression as a Natural Backdoor Attack.](http://arxiv.org/abs/2308.16684) | 本文发现了一种更严重的后门攻击威胁，即任何人都可以利用易获取的有损压缩算法进行自然后门攻击，无需设计特定触发器或进行繁琐调试。 |
| [^28] | [Everything, Everywhere All in One Evaluation: Using Multiverse Analysis to Evaluate the Influence of Model Design Decisions on Algorithmic Fairness.](http://arxiv.org/abs/2308.16681) | 通过多元宇宙分析评估模型设计决策对算法公平性的影响，可以揭示算法决策系统中设计决策的关键作用。 |
| [^29] | [Branches of a Tree: Taking Derivatives of Programs with Discrete and Branching Randomness in High Energy Physics.](http://arxiv.org/abs/2308.16680) | 该论文提出了应用多种梯度估计技术实现对具有离散和分支随机性的程序进行求导的方法，并发展了首个完全可微分的分支程序，这项工作的贡献在于为高能物理中的梯度优化提供了新的可能性。 |
| [^30] | [Dynamic nsNet2: Efficient Deep Noise Suppression with Early Exiting.](http://arxiv.org/abs/2308.16678) | 本文提出了一个提前退出模型，通过适应原始架构和分割信息流，实现了在资源受限的设备上高效的深度噪声抑制，并展示了性能和计算复杂性之间的权衡。 |
| [^31] | [Communication-Efficient Decentralized Federated Learning via One-Bit Compressive Sensing.](http://arxiv.org/abs/2308.16671) | 本文提出了基于一位压缩感知的通信高效的分散式联邦学习算法，通过在邻居节点之间传输一位信息并减少通信回合的数量，实现了对具有稀疏约束的共享模型的高效训练。 |
| [^32] | [What can we learn from quantum convolutional neural networks?.](http://arxiv.org/abs/2308.16664) | 通过分析量子卷积神经网络（QCNNs），我们发现它们通过隐藏特征映射嵌入物理系统参数，并且利用量子临界性生成适合的基函数集，池化层选择能够形成高性能决策边界的基函数，而模型的泛化性能依赖于嵌入类型。 |
| [^33] | [Autoencoder-based Online Data Quality Monitoring for the CMS Electromagnetic Calorimeter.](http://arxiv.org/abs/2308.16659) | 本研究基于自编码器开发了一种实时的异常检测系统，可有效检测到CMS电磁量能器(ECAL)中过去未见的异常，具有较低的错误发现率。 |
| [^34] | [Generate Your Own Scotland: Satellite Image Generation Conditioned on Maps.](http://arxiv.org/abs/2308.16648) | 本论文提出了一种基于地图的卫星图像生成方法，使用预训练的扩散模型，可以生成逼真的卫星图像，实验结果表明图像质量和地图保真度都很好。这些方法在遥感领域有很大的应用前景和挑战。 |
| [^35] | [Towards Long-Tailed Recognition for Graph Classification via Collaborative Experts.](http://arxiv.org/abs/2308.16609) | 本文提出了一种新颖的方法，通过合作专家实现了长尾图分类，解决了现有方法在处理图数据上的不足。 |
| [^36] | [A Causal Discovery Approach To Learn How Urban Form Shapes Sustainable Mobility Across Continents.](http://arxiv.org/abs/2308.16599) | 通过因果关系发现方法，研究者们利用高分辨率的出行数据，发现城市形态变量对于城市内出行方式产生了间接影响，填补了当前研究的不足，为实现全球可持续交通系统提供了指导。 |
| [^37] | [Towards Optimal Patch Size in Vision Transformers for Tumor Segmentation.](http://arxiv.org/abs/2308.16598) | 本论文研究了在肿瘤分割中，视觉变换器中最佳补丁尺寸的选择。目前基于完全卷积神经网络的深度学习模型已成为主流，但由于卷积层的局限性，它们不能有效捕捉长距离依赖和全局上下文。为解决这个问题，引入了视觉变换器，但由于对输入补丁尺寸敏感，其在不同肿瘤大小上的性能下降。因此，本文提出了一种技术来选择视觉变换器的最佳输入多分辨率。 |
| [^38] | [Towards Spontaneous Style Modeling with Semi-supervised Pre-training for Conversational Text-to-Speech Synthesis.](http://arxiv.org/abs/2308.16593) | 本文提出了一种半监督预训练方法，通过增加自发风格语音和自发行为标签的数量，以实现自发风格建模用于对话文本到语音合成。实验结果显示，该方法能够在自发风格的语音中建模自发行为，并从文本中预测合理的自发行为。 |
| [^39] | [Development and validation of an interpretable machine learning-based calculator for predicting 5-year weight trajectories after bariatric surgery: a multinational retrospective cohort SOPHIA study.](http://arxiv.org/abs/2308.16585) | 开发和验证了一种机器学习算法，可以预测减重手术后5年的体重变化轨迹，为个体术前提供准确预测。 |
| [^40] | [CL-MAE: Curriculum-Learned Masked Autoencoders.](http://arxiv.org/abs/2308.16572) | 本文提出了一种课程学习的遮罩自编码器（CL-MAE）。我们引入了一种可学习的遮罩模块，通过更新遮罩策略来增加自监督重构任务的复杂性。通过逐渐增加任务复杂性，模型可以学习更复杂和可迁移的表示。 |
| [^41] | [Document Layout Analysis on BaDLAD Dataset: A Comprehensive MViTv2 Based Approach.](http://arxiv.org/abs/2308.16571) | 我们使用MViTv2模型在BaDLAD数据集上进行文档布局分析，实现了文本框、段落、图片和表格的自动提取。我们还探索了增强方法，并发现了一些有效的性能改进。 |
| [^42] | [MONDEO: Multistage Botnet Detection.](http://arxiv.org/abs/2308.16570) | MONDEO是一种多阶段机制，用于检测基于DNS的僵尸网络恶意软件，它轻巧且可以在手机设备中部署，通过处理数据包流进行高效的攻击识别。 |
| [^43] | [Forecasting Emergency Department Crowding with Advanced Machine Learning Models and Multivariable Input.](http://arxiv.org/abs/2308.16544) | 本研究使用高级机器学习模型和多变量输入来预测急诊室拥挤，发现N-BEATS和LightGBM相较于基准模型分别提供了11%和9%的性能改进。 |
| [^44] | [Scalable Incomplete Multi-View Clustering with Structure Alignment.](http://arxiv.org/abs/2308.16541) | 该论文提出了一种可扩展的不完整多视图聚类方法，通过解决锚点不对齐问题和视图间的差异，实现了精确的聚类性能提升。 |
| [^45] | [On a Connection between Differential Games, Optimal Control, and Energy-based Models for Multi-Agent Interactions.](http://arxiv.org/abs/2308.16539) | 本论文研究了微分博弈、最优控制和基于能量的模型之间的联系，并提出了基于能量的潜在博弈的新的端到端学习应用，通过神经网络和可微分的博弈论优化层的组合来提高预测性能。 |
| [^46] | [Conditioning Score-Based Generative Models by Neuro-Symbolic Constraints.](http://arxiv.org/abs/2308.16534) | 本文提出了一种方法，通过神经符号约束来调节基于评分的生成模型，实现了在非条件生成模型下强制执行任意的逻辑约束，从而获得了一个有效的、无需额外训练的条件采样算法。 |
| [^47] | [SA6D: Self-Adaptive Few-Shot 6D Pose Estimator for Novel and Occluded Objects.](http://arxiv.org/abs/2308.16528) | SA6D是一种面向新颖和遮挡物体的自适应少样本6D姿态估计器，通过使用自适应分割模块和少量参考图像构建目标对象的点云模型，无需额外的对象信息，能在具有遮挡的复杂场景中表现出更好的性能。 |
| [^48] | [Curvature-based Pooling within Graph Neural Networks.](http://arxiv.org/abs/2308.16516) | 本研究提出了一种名为CurvPool的汇聚方法，通过利用图的曲率概念来解决过度平滑和过度压缩的问题。它能够根据曲率自适应地识别负责这两种现象的结构，并构建具有更合适结构的图，从而实现更深层模型和远距离信息的结合。 |
| [^49] | [In-class Data Analysis Replications: Teaching Students while Testing Science.](http://arxiv.org/abs/2308.16491) | 这项研究揭示了课堂数据分析复制的可行性，以及这种方法对学生、教育者和科学家的成本与收益。同时，学生对数据的预期与实际情况存在差异。 |
| [^50] | [Latent Painter.](http://arxiv.org/abs/2308.16490) | 这个论文介绍了一种名为潜在画家的技术，它利用潜在作为画布和扩散器的预测作为计划来生成绘画动画，同时还可以在不同的检查点集中转换图像。 |
| [^51] | [Test-Time Adaptation for Point Cloud Upsampling Using Meta-Learning.](http://arxiv.org/abs/2308.16484) | 本文提出了一种使用元学习进行测试时间适应的方法来增强点云上采样模型的普适性，解决了测试数据分布与训练数据不同导致性能下降的问题。 |
| [^52] | [Echocardiographic View Classification with Integrated Out-of-Distribution Detection for Enhanced Automatic Echocardiographic Analysis.](http://arxiv.org/abs/2308.16483) | ECHO-VICODE是一个深度学习框架，通过训练来分类超声心动图的31个视图类别，并具有集成的离群检测功能，可以显著降低超声心动图中的错误可能性。 |
| [^53] | [Point-TTA: Test-Time Adaptation for Point Cloud Registration Using Multitask Meta-Auxiliary Learning.](http://arxiv.org/abs/2308.16481) | Point-TTA是一种通过多任务元辅助学习实现的点云配准测试时自适应框架，能够提高配准模型的泛化性能。 |
| [^54] | [A Policy Adaptation Method for Implicit Multitask Reinforcement Learning Problems.](http://arxiv.org/abs/2308.16471) | 本研究提出了一种适用于动态运动生成任务的多任务强化学习算法，可用于适应单个运动类别中的隐式变化，并在头球任务中取得良好的适应效果。 |
| [^55] | [Domain-adaptive Message Passing Graph Neural Network.](http://arxiv.org/abs/2308.16470) | 本论文提出了一个基于领域自适应的消息传递图神经网络(DM-GNN)，用于解决跨网络节点分类问题。该方法通过结合图神经网络和条件对抗领域自适应，能够学习可传递的节点分类信息。具体而言，通过双特征提取器构建GNN编码器，同时利用标签传播节点分类器和标签感知的传播方案来提高节点分类的准确性和泛化性能。 |
| [^56] | [Computing excited states of molecules using normalizing flows.](http://arxiv.org/abs/2308.16468) | 使用规范化流计算分子的激发态，通过逼近波函数并优化基函数的线性空间内的近似。该方法在计算量子系统中取得了准确和有效的结果，并在能量预测准确性和基组收敛速度方面进行了显著改善。 |
| [^57] | [BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge.](http://arxiv.org/abs/2308.16458) | BioCoder是一个用于评估预训练模型在生成生物信息学代码方面的基准，涵盖了函数代码生成中的包依赖关系、类声明和全局变量，并通过模糊测试框架进行评估。 |
| [^58] | [Least Squares Maximum and Weighted Generalization-Memorization Machines.](http://arxiv.org/abs/2308.16456) | 本文提出了一种记忆影响机制用于最小二乘支持向量机，能够准确划分训练集并避免过拟合，提出的最大记忆影响模型和加权记忆影响模型在泛化性能和时间成本方面具有优势。 |
| [^59] | [Adversarial Finetuning with Latent Representation Constraint to Mitigate Accuracy-Robustness Tradeoff.](http://arxiv.org/abs/2308.16454) | 本文提出了一种名为ARREST的新型对抗训练方法，通过对抗微调、基于表示的知识蒸馏和嘈杂重播来减少标准准确性和对抗鲁棒性之间的权衡问题。 |
| [^60] | [Listen to Minority: Encrypted Traffic Classification for Class Imbalance with Contrastive Pre-Training.](http://arxiv.org/abs/2308.16453) | 本文提出了一种基于对比预训练的不平衡类别加密流量分类框架PASS，通过重新采样训练数据集并进行对比性预训练，避免了标签偏见和流量均匀性等问题，为加密流量分类提供了稳健的特征表示。 |
| [^61] | [AntM$^{2}$C: A Large Scale Dataset For Multi-Scenario Multi-Modal CTR Prediction.](http://arxiv.org/abs/2308.16437) | 本研究提出了一个名为AntM$^{2}$C的大规模数据集，用于多场景多模态点击率预测。该数据集弥补了现有数据集的限制，包括多个场景中不同类型项目的建模以及多模态特征的缺乏。它将为模型的可靠评估提供更全面的性能差异。 |
| [^62] | [On the Equivalence between Implicit and Explicit Neural Networks: A High-dimensional Viewpoint.](http://arxiv.org/abs/2308.16425) | 本文研究了高维隐式神经网络，提供了共轭核和神经切向核的高维等价表达，并在高维空间建立了隐式网络和显式网络的等价性。 |
| [^63] | [DECODE: DilatEd COnvolutional neural network for Detecting Extreme-mass-ratio inspirals.](http://arxiv.org/abs/2308.16422) | 该论文介绍了一种名为DECODE的扩张卷积神经网络模型，用于检测极端质量比激发的信号。该模型通过在频域进行序列建模，并考虑时间延迟干涉仪以处理多通道TDI数据。 |
| [^64] | [CktGNN: Circuit Graph Neural Network for Electronic Design Automation.](http://arxiv.org/abs/2308.16406) | 本文提出了一种名为CktGNN的电路图神经网络，通过识别电路的图形特性，同时自动化电路拓扑生成和器件尺寸调整。它使用两级GNN框架对电路图进行编码，并在设计效率上取得了显著的提升。 |
| [^65] | [Balancing between the Local and Global Structures (LGS) in Graph Embedding.](http://arxiv.org/abs/2308.16403) | 本论文提出了一种在图嵌入中平衡局部和全局结构的方法，通过可调参数进行调节。我们的结果表明，该方法在合成和真实世界的数据集上与最先进的方法相竞争，并引入了一种新颖的质量指标，聚类距离保持，用于评估嵌入的质量。 |
| [^66] | [Improving Robustness and Accuracy of Ponzi Scheme Detection on Ethereum Using Time-Dependent Features.](http://arxiv.org/abs/2308.16391) | 这篇论文提出了一种基于交易的方法来提高以太坊上庞氏骗局的检测鲁棒性和准确性。现有的方法主要基于智能合约源代码或操作码进行检测，但缺乏鲁棒性。通过分析交易数据，可以更有效地识别庞氏骗局，因为交易更难伪装。 |
| [^67] | [BenchTemp: A General Benchmark for Evaluating Temporal Graph Neural Networks.](http://arxiv.org/abs/2308.16385) | BenchTemp是一个通用基准，用于评估时间图神经网络（TGNN）模型在不同工作负载上的表现。BenchTemp提供一组基准数据集和一个标准流程，用于公平比较不同的TGNN模型。通过BenchTemp，我们对不同任务和设置下的代表性TGNN模型进行了广泛比较。 |
| [^68] | [Multi-Objective Decision Transformers for Offline Reinforcement Learning.](http://arxiv.org/abs/2308.16379) | 这项研究将离线强化学习问题视为多目标优化问题，通过扩展预测到状态和回报，解决了传统单任务学习方法中 Transformer 模型注意力机制的局限性，并指出了轨迹表示中可能存在的缺陷。 |
| [^69] | [A Survey on Privacy in Graph Neural Networks: Attacks, Preservation, and Applications.](http://arxiv.org/abs/2308.16375) | 这篇综述调查了图神经网络中的隐私问题，包括攻击、保护方法以及应用领域。研究人员着重总结了攻击类型、隐私保护技术分类以及可用于分析和解决GNNs中隐私问题的数据集和应用，同时提出了未来研究的方向，以构建更好的隐私保护GNNs。 |
| [^70] | [SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills.](http://arxiv.org/abs/2308.16369) | SARATHI通过使用分块预填充和解码最大批处理的方式，高效地处理LLM推断中的预填充和解码阶段的不平衡问题，提高推断效率。 |
| [^71] | [A Unified Analysis for the Subgradient Methods Minimizing Composite Nonconvex, Nonsmooth and Non-Lipschitz Functions.](http://arxiv.org/abs/2308.16362) | 本文提出了一种解决非凸非光滑非Lipschitz函数优化问题的近端次梯度方法，并建立了统一的递归关系用于 Moreau包络的建立。同时，提出了一些新的随机次梯度上界条件，并分析了随机次梯度方法的收敛性和迭代复杂度。 |
| [^72] | [Emoji Promotes Developer Participation and Issue Resolution on GitHub.](http://arxiv.org/abs/2308.16360) | 本研究探讨了表情符号在虚拟工作空间中的使用对开发者参与和问题解决的影响。研究发现，表情符号可以显著缩短问题的解决时间，并吸引更多用户参与。不同类型问题对表情符号的效果也存在差异。 |
| [^73] | [ToddlerBERTa: Exploiting BabyBERTa for Grammar Learning and Language Understanding.](http://arxiv.org/abs/2308.16336) | ToddlerBERTa是一个类似于BabyBERTa的语言模型，尽管在较小的数据集上进行训练，但它展示了令人称赞的性能，并具有强大的语言理解能力，与最先进的RoBERTa-base相媲美。 |
| [^74] | [Symmetry Preservation in Hamiltonian Systems: Simulation and Learning.](http://arxiv.org/abs/2308.16331) | 本论文提出了一个通用的几何框架，用于模拟和学习在Lie群变换下不变的哈密顿系统的动力学。通过构造$G$-不变的拉格朗日子流形，我们的方法能够保持与原始系统相同的保守量，提供了更忠实和准确的预测能力。 |
| [^75] | [Ten Years of Generative Adversarial Nets (GANs): A survey of the state-of-the-art.](http://arxiv.org/abs/2308.16316) | 生成对抗网络（GANs）自2014年诞生以来快速发展，它是一种在计算机视觉和其他领域生成逼真且多样化数据的强大工具。本文综述了GANs的最新技术发展和应用领域，并深入研究了GAN与统计学、信息论和优化方法之间的关系。 |
| [^76] | [Classification of Anomalies in Telecommunication Network KPI Time Series.](http://arxiv.org/abs/2308.16279) | 本文提出了一个模块化的电信网络KPI时间序列异常分类框架，其中包括时间序列模拟器、异常检测模型和分类模型。这个框架可以对网络KPI中检测到的异常进行准确分类和识别。 |
| [^77] | [Learning Diverse Features in Vision Transformers for Improved Generalization.](http://arxiv.org/abs/2308.16274) | 本文中，我们首先研究了视觉Transformer并发现其具有提取稳健和虚假特征的特点。通过剪枝虚假特征对应的注意头，我们证明了在验证数据上使用"oracle选择"可以显著提高其在分布变化下的性能。其次，我们提出了一种方法来增加学习特征的多样性和互补性，通过鼓励注意头输入梯度的正交性。我们观察到，这种增强特征多样性和剪枝不良注意头的方法在诊断基准测试中取得了改进的超出分布性能。 |
| [^78] | [A numerical approach for the fractional Laplacian via deep neural networks.](http://arxiv.org/abs/2308.16272) | 本文通过深度神经网络的数值方法解决了具有分数Laplacian的椭圆问题，并提供了四个数值示例来验证算法的效率。 |
| [^79] | [Emergence of Segmentation with Minimalistic White-Box Transformers.](http://arxiv.org/abs/2308.16271) | 本研究表明，在使用CRATE架构的最简监督训练方法下，Transformer模型可以实现分割特性的出现，无需复杂的自监督学习机制。 |
| [^80] | [Materials Informatics Transformer: A Language Model for Interpretable Materials Properties Prediction.](http://arxiv.org/abs/2308.16259) | 本研究提出了一种名为材料信息学变压器（MatInFormer）的语言模型，通过学习晶体学语法和引入MOFs数据，实现了对材料性能的准确预测，并通过注意力可视化揭示了模型的关键特征。 |
| [^81] | [Calibrated Explanations for Regression.](http://arxiv.org/abs/2308.16245) | 本文介绍了一种针对回归问题的特征重要性解释方法的扩展，可以量化特征重要性的不确定性。 |
| [^82] | [Deep Video Codec Control.](http://arxiv.org/abs/2308.16215) | 本文提出了第一个端到端可学习的深度视频编码控制方法，同时考虑了带宽限制和下游视觉性能，并在不破坏现有标准化的情况下实现了保护深度视觉模型的目标。 |
| [^83] | [RetroBridge: Modeling Retrosynthesis with Markov Bridges.](http://arxiv.org/abs/2308.16212) | 本研究提出了一种基于马尔可夫桥模型的反合成建模方法，通过对两个难以处理的离散分布之间的依赖关系进行近似，直接生成可能的前体分子，为反合成规划提供了准确的预测和置信度估计。 |
| [^84] | [Deep Inductive Logic Programming meets Reinforcement Learning.](http://arxiv.org/abs/2308.16210) | 本研究将深层归纳逻辑编程与强化学习相结合，提出了一种在动态连续环境中应用可微分神经逻辑网络的方法，以改进当前关系强化学习中的ILP方法。该方法能够学习一阶逻辑规则并处理连续环境中的问题。 |
| [^85] | [MASA-TCN: Multi-anchor Space-aware Temporal Convolutional Neural Networks for Continuous and Discrete EEG Emotion Recognition.](http://arxiv.org/abs/2308.16207) | MASA-TCN是一种用于连续和离散EEG情绪识别的多锚点空间感知时间卷积神经网络模型。该模型通过引入空间感知时间层来提取EEG空间模式，并能在情绪回归和分类任务中取得更好的性能。 |
| [^86] | [Learning Collaborative Information Dissemination with Graph-based Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2308.16198) | 本论文介绍了一种使用多智能体强化学习的方法来实现协作信息传播。通过提出分布式POMDP形式，在消息转发上实现了每个智能体的独立决策，相比传统的基于多点中继选择的启发式方法具有重大创新和贡献。同时，该方法利用图卷积强化学习和动态注意力机制捕捉关键网络特征，并提出了不同信息交换方式的两种方法进行评估。 |
| [^87] | [MedShapeNet -- A Large-Scale Dataset of 3D Medical Shapes for Computer Vision.](http://arxiv.org/abs/2308.16139) | MedShapeNet是一个大规模的三维医学形状数据集，作为一种对于常用形状基准的替代品，为计算机视觉研究提供了新的选择。 |
| [^88] | [CongNaMul: A Dataset for Advanced Image Processing of Soybean Sprouts.](http://arxiv.org/abs/2308.15690) | 提出了一个用于大豆芽图像处理的名为CongNaMul的数据集，旨在支持图像分类、语义分割、分解和测量等任务。提供了质量分类、语义分割和图像分解的标记，以及5个芽的物理特征供测量使用。 |
| [^89] | [Multi-Response Heteroscedastic Gaussian Process Models and Their Inference.](http://arxiv.org/abs/2308.15370) | 本文介绍了多响应异方差高斯过程模型，将其应用于回归、分类和状态空间模型，并提出了一种利用变分推断来近似后验的方法，解决了高斯过程模型在捕捉函数平滑性的突变和适应异方差错误方面的局限性。 |
| [^90] | [Biclustering Methods via Sparse Penalty.](http://arxiv.org/abs/2308.14388) | 本文提出了一种基于稀疏惩罚的双聚类方法，主要关注了SSVD方法，并尝试了一种新的稀疏惩罚方法。模拟研究结果表明混合的Prenet惩罚对于非重叠数据非常有效。 |
| [^91] | [Hypergraph Structure Inference From Data Under Smoothness Prior.](http://arxiv.org/abs/2308.14172) | 本文提出了一种光滑性先验方法，用于从节点特征中推断超图的结构，并捕捉数据内在的关系。该方法不需要标记数据作为监督，能够推断出每个潜在超边的概率。 |
| [^92] | [Stochastic Configuration Machines for Industrial Artificial Intelligence.](http://arxiv.org/abs/2308.13570) | 本文提出了一种新颖的随机学习器模型，称为随机配置机（SCMs），其基于随机配置网络（SCNs），旨在强调工业人工智能中的有效建模和节约数据大小。SCMs通过压缩模型存储，并保持有利的预测性能，具有在工业应用中很大的潜力。 |
| [^93] | [Quantization-based Optimization with Perspective of Quantum Mechanics.](http://arxiv.org/abs/2308.11594) | 基于量子力学视角的量化优化方法在全局优化中利用薛定谔方程推导的隧道效应，从而能够避免局部最小值。 |
| [^94] | [xxMD: Benchmarking Neural Force Fields Using Extended Dynamics beyond Equilibrium.](http://arxiv.org/abs/2308.11155) | 在神经力场模型中，常用的MD17数据集对于表示经历化学反应的系统不足。为了解决这一问题，我们引入了xxMD数据集，该数据集采样自扩展激发态分子动力学，包含了能量和力的信息。 |
| [^95] | [RBA-GCN: Relational Bilevel Aggregation Graph Convolutional Network for Emotion Recognition.](http://arxiv.org/abs/2308.11029) | 提出了RBA-GCN模型用于情感识别。该模型通过引入关系双层聚合和图生成模块，解决了GCN模型中的节点信息冗余和远距离上下文信息捕获问题。 |
| [^96] | [Adaptive Uncertainty-Guided Model Selection for Data-Driven PDE Discovery.](http://arxiv.org/abs/2308.10283) | 我们提出了一种自适应的不确定性引导模型选择方法，通过优先考虑对观测数据进行充分约束的简约偏微分方程，并结合物理学翻译神经网络学习进行验证。数值结果证实了该方法在识别真实主导PDE方面的成功应用。 |
| [^97] | [Symmetry-Preserving Program Representations for Learning Code Semantics.](http://arxiv.org/abs/2308.03312) | 本文提出了一种在代码中保持对称性的程序表示方法，通过引入循环层来提高在程序分析和建模中的效果。 |
| [^98] | [Speeding up Fourier Neural Operators via Mixed Precision.](http://arxiv.org/abs/2307.15034) | 通过混合精度训练，加速了傅里叶神经算子（FNO）的运行时间和内存使用，提高了训练效率。 |
| [^99] | [Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media.](http://arxiv.org/abs/2307.09312) | 多模态讨论变换器 (mDT) 是一个用于检测在线社交网络中仇恨言论的新颖模型。与传统的仅使用文本的方法不同，mDT通过整体分析文本和图像，结合图变换器捕捉评论周围整个讨论的上下文关系，并通过交织融合层将文本和图像嵌入进行组合。研究发现，捕捉对话的整体视图可以极大地提高检测反社会行为的准确性。 |
| [^100] | [Towards Understanding Adversarial Transferability From Surrogate Training.](http://arxiv.org/abs/2307.07873) | 本论文探索了对抗性可转移性的理解，特别关注替代训练。通过研究模型的平滑性和梯度相似性之间的权衡，发现对抗训练可以提高模型的替代能力。研究结果对数据分布的转变提出了新的推测。 |
| [^101] | [Online Distributed Learning with Quantized Finite-Time Coordination.](http://arxiv.org/abs/2307.06620) | 本文研究了一种在线分布式学习问题，提出了一种依靠量化、有限时间协作协议的分布式算法来聚合本地训练的模型，并允许使用随机梯度来提高效率和可扩展性。 |
| [^102] | [DNAGPT: A Generalized Pretrained Tool for Multiple DNA Sequence Analysis Tasks.](http://arxiv.org/abs/2307.05628) | DNAGPT是一个通用的基础模型，通过预训练模型和独特的标记设计，可以适用于任何DNA序列分析任务。它在多个任务上进行了评估，并展示出了良好的性能。 |
| [^103] | [Diffusion Policies for Out-of-Distribution Generalization in Offline Reinforcement Learning.](http://arxiv.org/abs/2307.04726) | 该论文介绍了一种名为状态重构扩散策略 (SRDP) 的新方法，该方法在最新的扩散策略类中引入了状态重构特征学习，以解决脱机强化学习中的分布偏移和有效表示策略的问题。 |
| [^104] | [Data-driven Predictive Latency for 5G: A Theoretical and Experimental Analysis Using Network Measurements.](http://arxiv.org/abs/2307.02329) | 本研究利用实际网络数据对5G网络中的预测性延迟进行了全面分析，并提出了以Hypoexponential分布为基础的用户面延迟的分析表达式。通过机器学习领域的贝叶斯学习和图机器学习技术，我们进行了概率回归、异常检测和预测性预测的实验。测试结果表明，该预测框架适用于不同情景下的移动性、城市交通和社交聚会。 |
| [^105] | [Learning Delays in Spiking Neural Networks using Dilated Convolutions with Learnable Spacings.](http://arxiv.org/abs/2306.17670) | 本文提出了一种新的离散时间算法，通过反向传播学习尖峰神经网络(SNNs)中的延迟，提高了SNNs在节能信息处理系统中的表达能力。 |
| [^106] | [Neural Shape Diameter Function for Efficient Mesh Segmentation.](http://arxiv.org/abs/2306.11737) | 该论文提出了一种利用深度学习在网格分割之前编码映射函数的数据驱动方法，可以用于多种应用，不受分辨率影响。 |
| [^107] | [Neural Mixed Effects for Nonlinear Personalized Predictions.](http://arxiv.org/abs/2306.08149) | 本文提出了神经混合效应（NME）模型，用于个性化预测，并通过结合个人通用和个人特定参数来考虑线性和非线性趋势。 |
| [^108] | [Improving the Validity of Decision Trees as Explanations.](http://arxiv.org/abs/2306.06777) | 该论文介绍了一个新的决策树模型，利用挂起的树的方式提高了其解释性和统计性能，达到了无限深度决策树的水平，并可与XGBoost等最先进的方法相媲美。 |
| [^109] | [The Role of Diverse Replay for Generalisation in Reinforcement Learning.](http://arxiv.org/abs/2306.05727) | 本文研究了在多任务强化学习中，增加重放缓存中数据过渡的多样性可以提高零-shot泛化性能，并且可能通过提高潜在表示的泛化性能来实现这种改善。 |
| [^110] | [Kernel Metric Learning for Clustering Mixed-type Data.](http://arxiv.org/abs/2306.01890) | 提出了一种使用混合核测量不相似性的度量方法，并通过交叉验证找到最佳核带宽。该方法可为现有的基于距离的聚类算法提高聚类准确度，适用于包含混合类型数据的模拟和实际数据集。 |
| [^111] | [Knowledge Graph Embeddings in the Biomedical Domain: Are They Useful? A Look at Link Prediction, Rule Learning, and Downstream Polypharmacy Tasks.](http://arxiv.org/abs/2305.19979) | 该论文研究了在生物医学领域中知识图谱嵌入的有效性和限制，通过在生物医学知识图谱上的实验，证明了最先进的模型在性能和下游用途方面的优势，同时提供了可解释的预测结果。 |
| [^112] | [Dynamic Data Augmentation via MCTS for Prostate MRI Segmentation.](http://arxiv.org/abs/2305.15777) | 提出了一种动态数据增强（DDAug）的方法，该方法使用高效的蒙特卡罗树搜索算法来学习不同数据集的有利增强策略，有效且计算代价可忽略不计。 |
| [^113] | [Generative Sliced MMD Flows with Riesz Kernels.](http://arxiv.org/abs/2305.11463) | 本文使用Riesz核展示了生成式分割MMD流的高效计算方法，实现了在大规模应用中通过神经网络训练生成模型。 |
| [^114] | [pTSE: A Multi-model Ensemble Method for Probabilistic Time Series Forecasting.](http://arxiv.org/abs/2305.11304) | 提出了pTSE，一种基于隐马尔可夫模型的概率预测的多模型分布集成方法，实现了对时间序列的鲁棒性和准确性的提高。 |
| [^115] | [Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models.](http://arxiv.org/abs/2305.10474) | 本论文介绍了一种新的视频噪声先验，用于微调图像扩散模型，以实现更高质量的视频合成。经过广泛的实验验证，该模型已经取得了UCF-101和MSR-VTT基准测试的最佳结果。 |
| [^116] | [MaxViT-UNet: Multi-Axis Attention for Medical Image Segmentation.](http://arxiv.org/abs/2305.08396) | MaxViT-UNet是基于编码器-解码器混合视觉变压器的医学图像分割模型，多轴自我关注机制在每个解码器阶段有助于更有效地区分对象和背景区域。 |
| [^117] | [When Deep Learning Meets Polyhedral Theory: A Survey.](http://arxiv.org/abs/2305.00241) | 本文综述了深度学习与多面体理论的交叉领域。修正线性单元（ReLU）等函数使得一些神经网络结构能够通过多面体理论进行分析，应用线性和混合整数线性规划来实现网络修剪、鲁棒性分析和神经网络验证等任务。 |
| [^118] | [A Stable and Scalable Method for Solving Initial Value PDEs with Neural Networks.](http://arxiv.org/abs/2304.14994) | 本文提出了一种用神经网络求解初值偏微分方程的稳定可扩展方法，解决了在全局最小化神经网络参数中的 PDE 残差中遇到的灾难性遗忘和 ODE 方法中随着数量呈立方级别扩展等问题。 |
| [^119] | [Transformer-based interpretable multi-modal data fusion for skin lesion classification.](http://arxiv.org/abs/2304.14505) | 本文提出了一种基于Transformer的可解释多模态数据融合算法，用于帮助皮肤疾病的诊断。 |
| [^120] | [From Chaos Comes Order: Ordering Event Representations for Object Detection.](http://arxiv.org/abs/2304.13455) | 本文提出了一种基于Gromov-Wasserstein Discrepancy选择最佳事件表示的方法，这种方法可以在多个表示、网络骨干和数据集上保持任务性能排名的一致性。利用这一方法，本文对大型事件表示法家族进行超参数搜索，选择最适合物体检测的表示法，取得了优于最先进的基于事件的对象检测方法的成果。 |
| [^121] | [Expressive Text-to-Image Generation with Rich Text.](http://arxiv.org/abs/2304.06720) | 本文提出了一种使用富文本编辑器生成表达性文本图像的方法，可以通过局部样式控制、明确的标记重新加权、精确的颜色渲染和详细的区域合成，生成高质量且多样化的图像。 |
| [^122] | [Knowledge Enhanced Graph Neural Networks.](http://arxiv.org/abs/2303.15487) | KeGNN是一个神经符号框架，可以结合先前的知识来优化图数据上的节点分类和链接预测任务。 |
| [^123] | [BACKpropagation through BACK substitution with a BACKslash.](http://arxiv.org/abs/2303.15449) | 本文提出了一种通过在三角方程组上使用通用的“backslash”或高斯消元来计算梯度的反向传播的线性代数公式，可以实现优化和实现便利性。 |
| [^124] | [DR.CPO: Diversified and Realistic 3D Augmentation via Iterative Construction, Random Placement, and HPR Occlusion.](http://arxiv.org/abs/2303.12743) | 该论文提出了一种多样化和逼真的增强方法，可以创建整体对象并灵活地定位和旋转对象，并相应地应用自遮挡和外遮挡。通过迭代构建多个对象来提高整体对象构造的多样性，构造的对象可以在训练帧中随机放置和旋转。 |
| [^125] | [Inferring Traffic Models in Terminal Airspace from Flight Tracks and Procedures.](http://arxiv.org/abs/2303.09981) | 本文提出了一种通过收集飞行轨迹和程序数据学习飞行器行为变异性的概率模型，并且可以生成涉及任意数量飞行器的交通模型。 |
| [^126] | [Sensitivity-Aware Visual Parameter-Efficient Tuning.](http://arxiv.org/abs/2303.08566) | 本文提出了敏感度感知的视觉参数低效调整（SPT）方案，可以自适应地将可训练参数分配到任务特定的重要位置，以提高表示能力，适应预训练视觉模型到下游任务。 |
| [^127] | [StyleDiff: Attribute Comparison Between Unlabeled Datasets in Latent Disentangled Space.](http://arxiv.org/abs/2303.05102) | StyleDiff是一种在潜在解缠空间中比较未标记数据集属性差异的方法，可以帮助开发人员了解两个数据集的差异，并以易于理解的方式提供分析。方法具有高效性能和准确性。 |
| [^128] | [Collage Diffusion.](http://arxiv.org/abs/2303.00262) | Collage Diffusion通过图层建模和协调技术实现了对扩散图像生成的精确控制，用户可以在每个对象上调整图像协调程度，并可以在保持其他对象固定的情况下编辑单个对象。 |
| [^129] | [Fair Attribute Completion on Graph with Missing Attributes.](http://arxiv.org/abs/2302.12977) | 本文提出一种公平属性补全方法FairAC，用于处理具有缺失属性的图数据中的不公平性问题。FairAC采用注意机制处理属性缺失问题，并减轻属性和补全导致的两种不公平性，即属性不公平和拓扑不公平。 |
| [^130] | [Flexible Phase Dynamics for Bio-Plausible Contrastive Learning.](http://arxiv.org/abs/2302.12431) | 本研究展示了生物合理对比学习可以在时间上具有局部性，并且即使放松了标准训练过程的许多动力学要求，仍然可以正常运行。 |
| [^131] | [Metropolitan Segment Traffic Speeds from Massive Floating Car Data in 10 Cities.](http://arxiv.org/abs/2302.08761) | 本研究基于10个城市的大规模浮动车数据，提供了15分钟分辨率的交通速度信息，涵盖了从主干道到当地街道的所有街道级别，为城市交通运营和规划提供了重要数据。 |
| [^132] | [On-Demand Communication for Asynchronous Multi-Agent Bandits.](http://arxiv.org/abs/2302.07446) | 本文研究了一种协作多智能体多臂赌博问题，提出了一种按需通信协议ODC，可以根据智能体的经验拉动时间调整每对智能体间的通信，同时将ODC集成到UCB和AAE算法的自然扩展中，提出了两种通信效率高的协作算法，分析表明这两个算法在遗憾方面都接近最优。 |
| [^133] | [System identification of neural systems: If we got it right, would we know?.](http://arxiv.org/abs/2302.06677) | 该论文研究了神经系统的系统辨识问题，通过对比人工神经网络与生物神经元的记录来验证模型的有效性。然而，系统辨识的性能很大程度上取决于刺激图像等因素，并且对识别更高级别架构图案方面存在局限性。 |
| [^134] | [Transformers Meet Directed Graphs.](http://arxiv.org/abs/2302.00049) | 这项工作提出了两种有向图的方向和结构感知的位置编码，通过应用于排序网络的正确性测试和源代码理解等任务中，该模型相对于之前的最新技术提升了14.7%。 |
| [^135] | [Point Cloud-based Proactive Link Quality Prediction for Millimeter-wave Communications.](http://arxiv.org/abs/2301.00752) | 本研究提出了一种基于点云的毫米波通信主动链路质量预测方法，相比于基于图像的方法，其适用性更广且不涉及敏感信息。 |
| [^136] | [Invertible normalizing flow neural networks by JKO scheme.](http://arxiv.org/abs/2212.14424) | 本文提出了一种基于JKO方案的可逆归一化流神经网络，通过按块进行残差块的训练，减少了内存负载和深度流网络训练的难度。并且通过自适应时间重新参数化的流网络，在概率空间中逐步细化轨迹，从而提高了模型的训练效率和准确性。 |
| [^137] | [StyleGAN as a Utility-Preserving Face De-identification Method.](http://arxiv.org/abs/2212.02611) | 本文研究了使用StyleGAN生成去识别人脸的方法，通过风格混合，StyleGAN能够在保护用户隐私的同时保持图像的实用性，与其他两种最先进的方法相比表现出色。 |
| [^138] | [Sequential Informed Federated Unlearning: Efficient and Provable Client Unlearning in Federated Optimization.](http://arxiv.org/abs/2211.11656) | 本文提出一种名为知情联合消除（IFU）的新颖联邦优化方法，可实现有效且可量化的客户端消除请求，实验结果表明其效率较基本方法和最先进的FU方法更高。 |
| [^139] | [Federated Adaptive Prompt Tuning for Multi-domain Collaborative Learning.](http://arxiv.org/abs/2211.07864) | 本文提出了一种面向多领域协作学习的联邦自适应提示调优算法 FedAPT，利用强大的预训练模型实现更高的性能。其核心思想是为每个测试样本提供个性化提示，通过自适应地释放特定领域的知识来实现。通过设计一个自适应提示调优模块，服务器生成关键信息并分配给客户端，从而实现协同训练全局的自适应网络和元提示。 |
| [^140] | [Learning Melanocytic Cell Masks from Adjacent Stained Tissue.](http://arxiv.org/abs/2211.00646) | 本文提出了一种从相邻染色组织学片中训练深度神经网络进行黑色素细胞分割的方法，实现了0.64的平均IOU，尽管存在不完美的标签。 |
| [^141] | [Principled Pruning of Bayesian Neural Networks through Variational Free Energy Minimization.](http://arxiv.org/abs/2210.09134) | 本文提出了一种基于变分自由能最小化的贝叶斯模型简化方法，用于对贝叶斯神经网络进行原则性剪枝。通过引入迭代剪枝算法，解决了直接应用贝叶斯模型简化的近似误差问题，并在实验证明了该方法的有效性和优势。 |
| [^142] | [Pre-Training Representations of Binary Code Using Contrastive Learning.](http://arxiv.org/abs/2210.05102) | 提出了一种使用对比学习预训练二进制代码表示的方法，可以将源代码和注释信息纳入二进制代码的表示学习中，对于反向工程和计算机安全任务有重要意义。 |
| [^143] | [Hypernetwork approach to Bayesian MAML.](http://arxiv.org/abs/2210.02796) | 该论文提出了一种名为贝叶斯HMAML的新框架，利用超网络进行权重更新，以解决MAML的过拟合和不确定性问题。 |
| [^144] | [Dynamical systems' based neural networks.](http://arxiv.org/abs/2210.02373) | 本文通过基于动力系统的方法设计神经网络，以更好地理解网络的行为，并实现一些特定的属性。主要关注包含非1-Lipschitz层的1-Lipschitz架构。 |
| [^145] | [Seeking Interpretability and Explainability in Binary Activated Neural Networks.](http://arxiv.org/abs/2209.03450) | 本论文研究了在表格数据回归任务中使用二进制激活神经网络作为可解释和可解释的预测器的方法。我们提供了对其表达能力的保证，并提出了一种基于SHAP值的方法来量化特征、隐藏神经元和权重的相对重要性。同时，我们提出了一种贪婪算法来构建紧凑的网络，以实现解释性。 |
| [^146] | [GRASP: A Goodness-of-Fit Test for Classification Learning.](http://arxiv.org/abs/2209.02064) | 本文提出了一种适合度检验方法GRASP，用于评估通用二分类器对给定特征向量的标签的条件概率分布的拟合程度。 |
| [^147] | [Visual correspondence-based explanations improve AI robustness and human-AI team accuracy.](http://arxiv.org/abs/2208.00780) | 该论文提出了基于视觉对应的解释方法，用于改善AI的鲁棒性和人机团队的准确性。在大规模的人类研究中，该方法被发现比kNN解释更有用，帮助用户更准确地拒绝AI的错误决策。同时，该方法在超出分布数据集上改进了性能，并实现了互补人机团队准确性的可能性。 |
| [^148] | [Extending regionalization algorithms to explore spatial process heterogeneity.](http://arxiv.org/abs/2206.09429) | 提出两种新的空间区域划分算法，应用于合成和真实数据集，并取得了较好的成果。其中，两阶段K模型算法表现最佳，具有较好的模型拟合、区域重构和系数估计能力。 |
| [^149] | [0/1 Deep Neural Networks via Block Coordinate Descent.](http://arxiv.org/abs/2206.09379) | 本文介绍了一种用于训练0/1 DNNs的块坐标下降算法，该算法能够有效解决步函数作为激活函数所带来的困难，并获得了具有鲁棒性和最佳预测准确性的DNNs模型。 |
| [^150] | [Edge Inference with Fully Differentiable Quantized Mixed Precision Neural Networks.](http://arxiv.org/abs/2206.07741) | 本论文提出了一种新的边缘推理混合精度神经网络的量化方法，通过硬件感知的异构可微分量化和目标梯度修改，实现了在较小的内存占用下达到最优的模型准确性 |
| [^151] | [Neuronal diversity can improve machine learning for physics and beyond.](http://arxiv.org/abs/2204.04348) | 本文展示了使用多样化到神经元来改进机器学习，构建出能够通过学习自身激活函数快速多样化的神经网络，并且胜过传统的同构神经元网络，在图像分类和非线性回归任务中表现更优，这种学习到的多样性为动态系统选择多样性而非均匀性提供了例子，并阐明了多样性在自然和人工系统中的作用。 |
| [^152] | [MGNN: Graph Neural Networks Inspired by Distance Geometry Problem.](http://arxiv.org/abs/2201.12994) | 本文提出了一种受距离几何问题启发的图神经网络（MGNN），通过改进基函数设计来提高GNN模型的近似能力，并从几何和物理的角度对空间GNNs的普适性进行了综合分析。 |
| [^153] | [Leveraging Image-based Generative Adversarial Networks for Time Series Generation.](http://arxiv.org/abs/2112.08060) | 本文提出了一种基于图像的生成对抗网络用于时间序列生成的方法，通过引入拓展的时间序列间隔回报图（XIRP）作为二维图像表示，能够以尺度不变和可逆的方式捕捉时间序列的动态特性，从而在降低训练时间和提高样本质量方面取得显著优势。通过与其他图像表示方法和模型的比较，验证了该方法在预测能力方面的优越性。同时，引入了改进的随机反演方法以改善时间序列的重建效果。 |
| [^154] | [Combining Inductive and Deductive Reasoning for Query Answering over Incomplete Knowledge Graphs.](http://arxiv.org/abs/2106.14052) | 该论文研究了将归纳推理和演绎推理相结合应用于不完整知识图谱上的查询回答。通过将本体论纳入基于嵌入的查询回答模型，采用不同的集成策略和损失函数调整，取得了20%到的性能提升。 |
| [^155] | [Simulation-Based Optimization of User Interfaces for Quality-Assuring Machine Learning Model Predictions.](http://arxiv.org/abs/2104.01129) | 本论文介绍了一种基于模拟的优化方法，用于改进机器学习质量保证界面，通过模拟人类智能和机器智能的组合效应，以任务完成时间作为度量标准，来评估和优化QA4ML界面的效率 |
| [^156] | [Learning Optimal Strategies for Temporal Tasks in Stochastic Games.](http://arxiv.org/abs/2102.04307) | 本论文提出了一种无模型强化学习方法，用于从给定的LTL规范中学习最优控制策略，即使环境完全未知。该方法通过将问题建模为控制器和对抗环境之间的随机博弈，最大化满足LTL规范的概率，抵抗最坏情况下的环境行为。 |

# 详细

[^1]: 关于解决双向噪声线性系统的随机Kaczmarz算法的注释

    A Note on Randomized Kaczmarz Algorithm for Solving Doubly-Noisy Linear Systems. (arXiv:2308.16904v1 [math.NA])

    [http://arxiv.org/abs/2308.16904](http://arxiv.org/abs/2308.16904)

    本文分析了当系数矩阵和向量都存在加性和乘性噪声时，随机Kaczmarz算法在解决噪声线性系统中的收敛性。分析表明，RK的收敛性受到𝜏的大小影响，其中𝜏表示带有噪声的系数矩阵A的乘子范数的平方与Frobenius范数的平方的乘积。

    

    大规模线性系统Ax=b在实践中经常出现，需要有效的迭代求解器。通常，由于操作误差或错误的数据收集过程，这些系统会出现噪声。在过去的十年中，随机Kaczmarz（RK）算法已被广泛研究作为这些系统的高效迭代求解器。然而，现有对RK在噪声情况下的收敛性研究有限，只考虑右侧向量b中的测量噪声。不幸的是，在实践中，并不总是这样；系数矩阵A也可能是有噪声的。在本文中，我们分析了当系数矩阵A以及向量b都受有加性和乘性噪声影响时，RK的收敛性。在我们的分析中，变量 𝜏=∥ 𝜏 𝐴 ∗ ∥2^2 ∥ 𝐜𝐷𝐻∗𝑐 − 𝐛 ∥_𝐹^2🈶 𝑜 𝑅 的大小会影响RK的收敛性，其中 𝜏𝐴 表示A的带有噪声的版本。我们声称我们的分析是健壮且逼近实际的.

    Large-scale linear systems, $Ax=b$, frequently arise in practice and demand effective iterative solvers. Often, these systems are noisy due to operational errors or faulty data-collection processes. In the past decade, the randomized Kaczmarz (RK) algorithm has been studied extensively as an efficient iterative solver for such systems. However, the convergence study of RK in the noisy regime is limited and considers measurement noise in the right-hand side vector, $b$. Unfortunately, in practice, that is not always the case; the coefficient matrix $A$ can also be noisy. In this paper, we analyze the convergence of RK for noisy linear systems when the coefficient matrix, $A$, is corrupted with both additive and multiplicative noise, along with the noisy vector, $b$. In our analyses, the quantity $\tilde R=\| \tilde A^{\dagger} \|_2^2 \|\tilde A \|_F^2$ influences the convergence of RK, where $\tilde A$ represents a noisy version of $A$. We claim that our analysis is robust and realistic
    
[^2]: 学习品味：一个多模态葡萄酒数据集

    Learning to Taste: A Multimodal Wine Dataset. (arXiv:2308.16900v1 [cs.LG])

    [http://arxiv.org/abs/2308.16900](http://arxiv.org/abs/2308.16900)

    这个论文介绍了一个大型多模态葡萄酒数据集，用于研究视觉感知、语言和口感之间的关系，并提出了低维概念嵌入算法，将人类经验与自动机器相似度核相结合，改进了口味分类，并与人类口味知觉相一致。

    

    我们提出了一个大型的多模态葡萄酒数据集WineSensed，用于研究视觉感知、语言和口感之间的关系。该数据集包含89.7万张葡萄酒标签图片和82.4万条来自Vivino平台的葡萄酒评论。该数据集具有超过35万个独特的年份，附带了年份、产地、评分、酒精含量、价格和葡萄组成的注释。我们通过一项品酒实验对部分数据进行了细粒度的口味注释，共有256名参与者被要求根据口味的相似性对葡萄酒进行排序，得到了超过5千个配对的口味距离。我们提出了一种低维概念嵌入算法，将人类经验与自动机器相似度核相结合。我们证明，这个共享的概念嵌入空间在粗粒度口味分类（酒精含量，国家，葡萄，价格，评分）上改进，并且与复杂的人类口味知觉相一致。

    We present WineSensed, a large multimodal wine dataset for studying the relations between visual perception, language, and flavor. The dataset encompasses 897k images of wine labels and 824k reviews of wines curated from the Vivino platform. It has over 350k unique vintages, annotated with year, region, rating, alcohol percentage, price, and grape composition. We obtained fine-grained flavor annotations on a subset by conducting a wine-tasting experiment with 256 participants who were asked to rank wines based on their similarity in flavor, resulting in more than 5k pairwise flavor distances. We propose a low-dimensional concept embedding algorithm that combines human experience with automatic machine similarity kernels. We demonstrate that this shared concept embedding space improves upon separate embedding spaces for coarse flavor classification (alcohol percentage, country, grape, price, rating) and aligns with the intricate human perception of flavor.
    
[^3]: Transformers作为支持向量机

    Transformers as Support Vector Machines. (arXiv:2308.16898v1 [cs.LG])

    [http://arxiv.org/abs/2308.16898](http://arxiv.org/abs/2308.16898)

    这项工作建立了自注意力和硬间隔支持向量机问题之间的正式等价关系，通过转换器架构的优化几何来解决自然语言处理问题，同时揭示了梯度下降优化的转换器的隐式偏差。

    

    自从"Attention Is All You Need"中引入转换器架构以来，它在自然语言处理领域取得了革命性的进展。转换器中的注意力层接受输入令牌序列$X$并通过计算softmax$(XQK^\top X^\top)$的成对相似性使它们相互作用，其中$(K,Q)$是可训练的键-查询参数。在这项工作中，我们建立了自注意力优化几何和一个硬间隔支持向量机问题之间的正式等价关系，通过对令牌对的外积施加线性约束，将最佳输入令牌与非最佳令牌分离。这个形式主义使我们能够表征梯度下降优化的单层转换器的隐式偏差：(1)优化注意力层，使用可变正则化参数$(K,Q)$，收敛的方向是一个最小化综合参数$W=KQ^\top$的核范数的支持向量机解决方案。而直接使用$W$进行参数化则最小化一个Frobenius范数目标。

    Since its inception in "Attention Is All You Need", transformer architecture has led to revolutionary advancements in NLP. The attention layer within the transformer admits a sequence of input tokens $X$ and makes them interact through pairwise similarities computed as softmax$(XQK^\top X^\top)$, where $(K,Q)$ are the trainable key-query parameters. In this work, we establish a formal equivalence between the optimization geometry of self-attention and a hard-margin SVM problem that separates optimal input tokens from non-optimal tokens using linear constraints on the outer-products of token pairs. This formalism allows us to characterize the implicit bias of 1-layer transformers optimized with gradient descent: (1) Optimizing the attention layer with vanishing regularization, parameterized by $(K,Q)$, converges in direction to an SVM solution minimizing the nuclear norm of the combined parameter $W=KQ^\top$. Instead, directly parameterizing by $W$ minimizes a Frobenius norm objective. 
    
[^4]: PointOcc: 基于点云的三透视圆柱视图用于点云三维语义占据预测

    PointOcc: Cylindrical Tri-Perspective View for Point-based 3D Semantic Occupancy Prediction. (arXiv:2308.16896v1 [cs.CV])

    [http://arxiv.org/abs/2308.16896](http://arxiv.org/abs/2308.16896)

    本文提出了一个新的三透视圆柱视图表示方法，在自动驾驶中进行点云三维语义占据预测。通过构建柱坐标系下的三透视圆柱视图，实现了对点云的精细建模，同时利用空间组池化和二维主干网络高效处理数据。结果表明，该方法可以有效地预测点云的语义占据情况。

    

    自动驾驶中的语义分割正从稀疏点分割发展到密集体素分割，目标是预测所关注的三维空间中每个体素的语义占据情况。现有的基于二维投影的方法（如鸟瞰图、距离视图等）因为预测空间的密集性而无效，因为它们只能描述三维场景的子空间。为了解决这个问题，我们提出了一个三透视圆柱视图来有效而全面地表示点云，并提出了一个点云模型 PointOcc 来高效地处理它们。考虑到激光雷达点云的距离分布，我们利用柱坐标系构建了三透视圆柱视图，以更细粒度地对近区域进行建模。我们采用空间组池化来保留投影过程中的结构细节，并采用二维主干网路高效处理每个透视面。最后，我们通过聚合的方式获得每个点的特征。

    Semantic segmentation in autonomous driving has been undergoing an evolution from sparse point segmentation to dense voxel segmentation, where the objective is to predict the semantic occupancy of each voxel in the concerned 3D space. The dense nature of the prediction space has rendered existing efficient 2D-projection-based methods (e.g., bird's eye view, range view, etc.) ineffective, as they can only describe a subspace of the 3D scene. To address this, we propose a cylindrical tri-perspective view to represent point clouds effectively and comprehensively and a PointOcc model to process them efficiently. Considering the distance distribution of LiDAR point clouds, we construct the tri-perspective view in the cylindrical coordinate system for more fine-grained modeling of nearer areas. We employ spatial group pooling to maintain structural details during projection and adopt 2D backbones to efficiently process each TPV plane. Finally, we obtain the features of each point by aggregat
    
[^5]: 语言条件路径规划

    Language-Conditioned Path Planning. (arXiv:2308.16893v1 [cs.RO])

    [http://arxiv.org/abs/2308.16893](http://arxiv.org/abs/2308.16893)

    本研究提出了一种语言条件路径规划的方法，通过学习碰撞函数，预测机器人与环境之间的碰撞，实现灵活、有条件的路径规划，无需手动标注或者真实物体模型。

    

    接触是机器人操作的核心。有时候，我们希望使用接触（例如操纵和抓取），而有时候，接触是有害的（例如避免障碍物）。然而，传统的路径规划算法只关注于无碰撞路径，限制了它们在接触丰富任务中的适用性。为了解决这个问题，我们提出了语言条件路径规划的领域，将接触感知性融入到路径规划问题中。作为该领域的第一步，我们提出了语言条件碰撞函数（LACO），这是一种新颖的方法，只使用单视图图像、语言提示和机器人配置来学习碰撞函数。LACO可以预测机器人和环境之间的碰撞，从而实现灵活、有条件的路径规划，无需手动对象标注、点云数据或真实物体模型。在仿真和实际环境中，我们证明LACO可以实现复杂而细致的路径规划。

    Contact is at the core of robotic manipulation. At times, it is desired (e.g. manipulation and grasping), and at times, it is harmful (e.g. when avoiding obstacles). However, traditional path planning algorithms focus solely on collision-free paths, limiting their applicability in contact-rich tasks. To address this limitation, we propose the domain of Language-Conditioned Path Planning, where contact-awareness is incorporated into the path planning problem. As a first step in this domain, we propose Language-Conditioned Collision Functions (LACO) a novel approach that learns a collision function using only a single-view image, language prompt, and robot configuration. LACO predicts collisions between the robot and the environment, enabling flexible, conditional path planning without the need for manual object annotations, point cloud data, or ground-truth object meshes. In both simulation and the real world, we demonstrate that LACO can facilitate complex, nuanced path plans that allo
    
[^6]: GNFactor：具有可泛化神经特征场的多任务真实机器人学习

    GNFactor: Multi-Task Real Robot Learning with Generalizable Neural Feature Fields. (arXiv:2308.16891v1 [cs.RO])

    [http://arxiv.org/abs/2308.16891](http://arxiv.org/abs/2308.16891)

    GNFactor是一个用于多任务机器人操作的代理方法，它利用可泛化神经特征场和Perceiver Transformer模块，以及深度三维体素表示来实现对真实世界环境中的操作任务的执行。它通过将视觉和语义信息纳入三维表示来提高场景的理解能力，并在多个任务上进行了验证。

    

    在无结构的现实世界环境中，从视觉观察中开发能够执行多样化操作任务的代理机器人一直是机器人学中的一个长期问题。为了实现这个目标，机器人需要全面理解场景的三维结构和语义。在这项工作中，我们提出了GNFactor，一种用于多任务机器人操作的可视行为克隆代理，它利用可泛化神经特征场（GNF）作为重建模块，Perceiver Transformer作为决策模块，共享深度三维体素表示。为了将语义纳入三维表示，重建模块利用视觉语言基础模型（例如，稳定扩散）将丰富的语义信息提取到深度三维体素中。我们在3个真实机器人任务上评估了GNFactor，并对10个RLBench任务进行了详细的消融实验，只使用了有限数量的数据。

    It is a long-standing problem in robotics to develop agents capable of executing diverse manipulation tasks from visual observations in unstructured real-world environments. To achieve this goal, the robot needs to have a comprehensive understanding of the 3D structure and semantics of the scene. In this work, we present $\textbf{GNFactor}$, a visual behavior cloning agent for multi-task robotic manipulation with $\textbf{G}$eneralizable $\textbf{N}$eural feature $\textbf{F}$ields. GNFactor jointly optimizes a generalizable neural field (GNF) as a reconstruction module and a Perceiver Transformer as a decision-making module, leveraging a shared deep 3D voxel representation. To incorporate semantics in 3D, the reconstruction module utilizes a vision-language foundation model ($\textit{e.g.}$, Stable Diffusion) to distill rich semantic information into the deep 3D voxel. We evaluate GNFactor on 3 real robot tasks and perform detailed ablations on 10 RLBench tasks with a limited number of
    
[^7]: 无人机增强网络中的联邦学习：联合覆盖和收敛时间优化

    Federated Learning in UAV-Enhanced Networks: Joint Coverage and Convergence Time Optimization. (arXiv:2308.16889v1 [cs.LG])

    [http://arxiv.org/abs/2308.16889](http://arxiv.org/abs/2308.16889)

    本研究针对无人机增强网络中的联邦学习问题，提出了一种能够同时优化覆盖范围和减少收敛时间的解决方案。通过分析无人机增强的无线传感器网络的统计特征，以及基于多目标多臂赌博机理论，将FL和无人机部署方法相结合，能够有效地减少FL延迟并提高网络性能。

    

    联邦学习（FL）涉及多个设备共同训练一个共享模型，而无需传输他们的本地数据。FL减少了通信开销，使其成为无人机增强无线网络中一种有前景的学习方法，因为该网络能源稀缺。尽管有潜力，但在无人机增强网络中实现FL是具有挑战性的，传统的最大化覆盖范围的无人机部署方法会显著增加FL的延迟。此外，关键变量（如信道质量）的不确定性和缺乏先验信息使问题加剧。本文首先分析了带能量收集的无人机增强无线传感器网络（WSN）的统计特征。然后，我们基于多目标多臂赌博机理论开发了一个模型和解决方案，以最大化网络覆盖范围同时最小化FL延迟。此外，我们提出了另一种特别适用于具有大动作集和严格能量约束的UAV-WSN系统的解决方案。

    Federated learning (FL) involves several devices that collaboratively train a shared model without transferring their local data. FL reduces the communication overhead, making it a promising learning method in UAV-enhanced wireless networks with scarce energy resources. Despite the potential, implementing FL in UAV-enhanced networks is challenging, as conventional UAV placement methods that maximize coverage increase the FL delay significantly. Moreover, the uncertainty and lack of a priori information about crucial variables, such as channel quality, exacerbate the problem. In this paper, we first analyze the statistical characteristics of a UAV-enhanced wireless sensor network (WSN) with energy harvesting. We then develop a model and solution based on the multi-objective multi-armed bandit theory to maximize the network coverage while minimizing the FL delay. Besides, we propose another solution that is particularly useful with large action sets and strict energy constraints at the U
    
[^8]: 通过机器学习预测二嵌段共聚物的形态学

    Prediction of Diblock Copolymer Morphology via Machine Learning. (arXiv:2308.16886v1 [physics.chem-ph])

    [http://arxiv.org/abs/2308.16886](http://arxiv.org/abs/2308.16886)

    通过利用机器学习方法，该论文提出了一种加速计算嵌段聚合物形态演化的策略，利用粒子模拟学习驱动缺陷消除过程，采用UNet架构实现了不同边界条件下的预测，并应用可解释的人工智能方法可视化了形态演化过程。

    

    提出了一种机器学习方法，用于加速计算大领域长时间尺度下嵌段聚合物形态演化。该策略利用了粗粒度粒子在单体尺度上的演化和介观尺度上缓慢的形态演化之间的特征时间的分离。与经验性连续模型不同，所提出的方法直接从基于粒子的模拟中学习随机驱动的缺陷消除过程。采用了一种支持不同边界条件的UNet架构，从而允许任意形状的周期性和固定基底边界条件。通过损失函数引入了物理概念，并通过数据增强加入了对称性。用三个不同的应用案例验证了该模型。应用可解释的人工智能方法可视化了时间上的形态演化。该方法实现了大规模系统和长尺度情况下的形态学演化预测。

    A machine learning approach is presented to accelerate the computation of block polymer morphology evolution for large domains over long timescales. The strategy exploits the separation of characteristic times between coarse-grained particle evolution on the monomer scale and slow morphological evolution over mesoscopic scales. In contrast to empirical continuum models, the proposed approach learns stochastically driven defect annihilation processes directly from particle-based simulations. A UNet architecture that respects different boundary conditions is adopted, thereby allowing periodic and fixed substrate boundary conditions of arbitrary shape. Physical concepts are also introduced via the loss function and symmetries are incorporated via data augmentation. The model is validated using three different use cases. Explainable artificial intelligence methods are applied to visualize the morphology evolution over time. This approach enables the generation of large system sizes and lon
    
[^9]: Belebele基准数据集：122种语言变体的并行阅读理解数据集

    The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants. (arXiv:2308.16884v1 [cs.CL])

    [http://arxiv.org/abs/2308.16884](http://arxiv.org/abs/2308.16884)

    Belebele是一个包含122种语言变体的多选机器阅读理解数据集，可用于评估文本模型在高、中和低资源语言中的性能。尽管英语为中心的大型语言模型在跨语言转移方面表现良好，但小型多语言遮蔽语言模型在其他语言上表现更佳。

    

    我们提出了Belebele，一个包含122种语言变体的多选机器阅读理解（MRC）数据集。该数据集极大地扩展了自然语言理解（NLU）基准的语言覆盖范围，使得可以评估文本模型在高、中和低资源语言中的性能。每个问题都基于Flores-200数据集中的一个短篇文章，并提供了四个多选答案。问题经过精心策划，以区分具有不同通用语言理解水平的模型。单独的英语数据集已经足够困难，可以挑战最先进的语言模型。由于完全并行，该数据集可以直接比较所有语言的模型性能。我们使用该数据集评估多语言遮蔽语言模型（MLMs）和大型语言模型（LLMs）的能力。我们展示了广泛的结果，并发现尽管英语为中心的LLMs之间存在显著的跨语言转移，但小型MLMs在其他语言上的表现相对较好。

    We present Belebele, a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. Significantly expanding the language coverage of natural language understanding (NLU) benchmarks, this dataset enables the evaluation of text models in high-, medium-, and low-resource languages. Each question is based on a short passage from the Flores-200 dataset and has four multiple-choice answers. The questions were carefully curated to discriminate between models with different levels of general language comprehension. The English dataset on its own proves difficult enough to challenge state-of-the-art language models. Being fully parallel, this dataset enables direct comparison of model performance across all languages. We use this dataset to evaluate the capabilities of multilingual masked language models (MLMs) and large language models (LLMs). We present extensive results and find that despite significant cross-lingual transfer in English-centric LLMs, much small
    
[^10]: 学习动态有向无环图的信息理论最优样本复杂度

    Information Theoretically Optimal Sample Complexity of Learning Dynamical Directed Acyclic Graphs. (arXiv:2308.16859v1 [stat.ML])

    [http://arxiv.org/abs/2308.16859](http://arxiv.org/abs/2308.16859)

    本文研究了学习动态有向无环图（DDAG）的信息理论最优样本复杂度，提出了一种基于观测时间序列的功率谱密度矩阵的度量和算法来重建DDAG。

    

    本文研究了学习线性动态系统（LDS）在有向无环图（DAG）上的底层相互作用/依赖关系的最优样本复杂度。学习DAG结构的样本复杂度在静态系统中已经得到了很好的研究，其中节点状态的样本是独立同分布的（i.i.d.）。然而，在具有动态系统的DAG中，这样的研究较少。我们将这样的DAG称为\emph{动态}DAG（DDAG）。具体来说，我们考虑了一个DDAG，其中节点动力学由未观测的外生噪声源驱动，这些噪声源在时间上是宽幅平稳的（WSS），但彼此之间是不相关的，并且具有相同的功率谱密度（PSD）。受静态设置的启发，我们提出了一种基于观测时间序列的PSD矩阵的度量和算法来重建DDAG。噪声PSD相等的假设可以放宽，以使其可识别。

    In this article, the optimal sample complexity of learning the underlying interaction/dependencies of a Linear Dynamical System (LDS) over a Directed Acyclic Graph (DAG) is studied. The sample complexity of learning a DAG's structure is well-studied for static systems, where the samples of nodal states are independent and identically distributed (i.i.d.). However, such a study is less explored for DAGs with dynamical systems, where the nodal states are temporally correlated. We call such a DAG underlying an LDS as \emph{dynamical} DAG (DDAG). In particular, we consider a DDAG where the nodal dynamics are driven by unobserved exogenous noise sources that are wide-sense stationary (WSS) in time but are mutually uncorrelated, and have the same {power spectral density (PSD)}. Inspired by the static settings, a metric and an algorithm based on the PSD matrix of the observed time series are proposed to reconstruct the DDAG. The equal noise PSD assumption can be relaxed such that identifiabil
    
[^11]: 稀疏支持向量机的主要化-最小化方法

    Majorization-Minimization for sparse SVMs. (arXiv:2308.16858v1 [cs.LG])

    [http://arxiv.org/abs/2308.16858](http://arxiv.org/abs/2308.16858)

    该论文提出了一种通过平滑的稀疏促进正则化的方法来训练支持向量机（SVM），并通过主要化-最小化方法实现快速训练。该方法可处理稀疏保留正则化器，提高性能，并通过在多个数据集上的测试验证其优越性能。

    

    几十年前，支持向量机（SVM）在监督框架下被引入用于执行二进制分类任务。如今，它们通常优于其他有监督方法，并且仍然是机器学习领域最流行的方法之一。在这项工作中，我们通过平滑的稀疏促进正则化的二次铰链损失最小化来研究SVM的训练。这个选择为基于主要化-最小化方法的快速训练方法的应用铺平了道路，从而受益于损失函数的Lipschitz可微性。此外，所提出的方法允许我们处理促进选择最重要特征的稀疏保留正则化器，从而提高性能。在三个不同的数据集上进行的数值测试和比较表明，所提出的方法在定性度量（准确率、精确率、召回率和F 1 得分）以及计算方面表现良好。

    Several decades ago, Support Vector Machines (SVMs) were introduced for performing binary classification tasks, under a supervised framework. Nowadays, they often outperform other supervised methods and remain one of the most popular approaches in the machine learning arena. In this work, we investigate the training of SVMs through a smooth sparse-promoting-regularized squared hinge loss minimization. This choice paves the way to the application of quick training methods built on majorization-minimization approaches, benefiting from the Lipschitz differentiabililty of the loss function. Moreover, the proposed approach allows us to handle sparsity-preserving regularizers promoting the selection of the most significant features, so enhancing the performance. Numerical tests and comparisons conducted on three different datasets demonstrate the good performance of the proposed methodology in terms of qualitative metrics (accuracy, precision, recall, and F 1 score) as well as computational 
    
[^12]: 量子系统中激发态的自然量子蒙特卡洛计算

    Natural Quantum Monte Carlo Computation of Excited States. (arXiv:2308.16848v1 [physics.comp-ph])

    [http://arxiv.org/abs/2308.16848](http://arxiv.org/abs/2308.16848)

    该论文提出了一种变分蒙特卡洛算法，用于估计量子系统中的激发态，通过转化问题使其成为寻找扩展系统的基态的问题。这种方法特别适用于多电子系统，并且可以准确地计算各种可观测量的期望值，包括非对角线期望值和跃迁偶极矩，并在苯等大分子上得到了良好的结果。

    

    我们提出了一种变分蒙特卡洛算法，用于估计量子系统的最低激发态，这是对寻找基态的估计的自然推广。该方法没有自由参数，并且不需要显式正交化不同的态，而是将寻找给定系统的激发态的问题转化为寻找扩展系统的基态的问题。可以计算任意可观测量的期望值，包括不同态之间的非对角线期望值，如跃迁偶极矩。尽管该方法完全通用，但与最近关于使用神经网络作为多电子系统变分参数的工作结合使用效果特别好，我们展示了通过将该方法与FermiNet和Psiformer变分参数结合使用，可以准确地恢复苯等大分子的垂直激发能和振子强度。除了在分子上的示例之外，我们还...

    We present a variational Monte Carlo algorithm for estimating the lowest excited states of a quantum system which is a natural generalization of the estimation of ground states. The method has no free parameters and requires no explicit orthogonalization of the different states, instead transforming the problem of finding excited states of a given system into that of finding the ground state of an expanded system. Expected values of arbitrary observables can be calculated, including off-diagonal expectations between different states such as the transition dipole moment. Although the method is entirely general, it works particularly well in conjunction with recent work on using neural networks as variational Ansatze for many-electron systems, and we show that by combining this method with the FermiNet and Psiformer Ansatze we can accurately recover vertical excitation energies and oscillator strengths on molecules as large as benzene. Beyond the examples on molecules presented here, we 
    
[^13]: 干涉合成孔径雷达的扩散模型

    Diffusion Models for Interferometric Satellite Aperture Radar. (arXiv:2308.16847v1 [cs.CV])

    [http://arxiv.org/abs/2308.16847](http://arxiv.org/abs/2308.16847)

    该论文使用概率扩散模型（PDMs）生成了基于雷达的卫星图像数据集，展示了PDMs成功生成了复杂和逼真结构的图像，但采样时间仍然是一个问题。他们还提供了一个简单而多功能的开源工具，可以在单个GPU上训练、采样和评估PDMs使用任何数据集。

    

    概率扩散模型（PDMs）最近被证明是一类非常有前景的生成模型，能够在自然图像生成方面取得高性能。然而，与非自然图像（如基于雷达的卫星数据）相比，它们在性能上还是大部分未知的。生成大量的合成（尤其是标记的）卫星数据对于实现深度学习方法来处理和分析（干涉）卫星孔径雷达数据至关重要。在这里，我们利用PDMs生成了几个基于雷达的卫星图像数据集。我们展示了PDMs成功生成了具有复杂和逼真结构的图像，但采样时间仍然是一个问题。事实上，加速采样策略在简单的图像数据集（如MNIST）上效果良好，但在我们的雷达数据集上失败了。我们提供了一个简单而多功能的开源工具https://github.com/thomaskerdreux/PDM_SAR_InSAR_generation，可以在单个GPU上训练、采样和评估PDMs使用任何数据集。

    Probabilistic Diffusion Models (PDMs) have recently emerged as a very promising class of generative models, achieving high performance in natural image generation. However, their performance relative to non-natural images, like radar-based satellite data, remains largely unknown. Generating large amounts of synthetic (and especially labelled) satellite data is crucial to implement deep-learning approaches for the processing and analysis of (interferometric) satellite aperture radar data. Here, we leverage PDMs to generate several radar-based satellite image datasets. We show that PDMs succeed in generating images with complex and realistic structures, but that sampling time remains an issue. Indeed, accelerated sampling strategies, which work well on simple image datasets like MNIST, fail on our radar datasets. We provide a simple and versatile open-source https://github.com/thomaskerdreux/PDM_SAR_InSAR_generation to train, sample and evaluate PDMs using any dataset on a single GPU.
    
[^14]: FedDD: 通过差分参数丢弃实现通信高效的联邦学习

    FedDD: Toward Communication-efficient Federated Learning with Differential Parameter Dropout. (arXiv:2308.16835v1 [cs.LG])

    [http://arxiv.org/abs/2308.16835](http://arxiv.org/abs/2308.16835)

    本文提出了一种通过差分参数丢弃实现通信高效的联邦学习方案（FedDD）。这种方案避免了频繁交换模型参数的通信延迟问题，并通过模型参数丢弃而不是客户端选择来优化全局模型泛化能力。

    

    联邦学习（FL）需要频繁交换模型参数，这导致了长时间的通信延迟，尤其是当客户端的网络环境差异很大时。此外，参数服务器需要等待最慢的客户端（即straggler，可能具有最大的模型大小、最低的计算能力或最差的网络条件）上传参数，这可能会严重影响通信效率。常用的客户端选择方法，如部分客户端选择，会导致计算资源的浪费并削弱全局模型的泛化能力。为了解决这个问题，在本文中，我们提出了一种不同的方法，即使用模型参数丢弃而不是客户端选择，并据此提出了一种新的差分参数丢弃的联邦学习方案（FedDD）框架。FedDD包括两个关键模块：丢弃率分配和上传参数选择，将优化模型参数。

    Federated Learning (FL) requires frequent exchange of model parameters, which leads to long communication delay, especially when the network environments of clients vary greatly. Moreover, the parameter server needs to wait for the slowest client (i.e., straggler, which may have the largest model size, lowest computing capability or worst network condition) to upload parameters, which may significantly degrade the communication efficiency. Commonly-used client selection methods such as partial client selection would lead to the waste of computing resources and weaken the generalization of the global model. To tackle this problem, along a different line, in this paper, we advocate the approach of model parameter dropout instead of client selection, and accordingly propose a novel framework of Federated learning scheme with Differential parameter Dropout (FedDD). FedDD consists of two key modules: dropout rate allocation and uploaded parameter selection, which will optimize the model par
    
[^15]: 针对层级数据集的潜在变量多输出高斯过程

    Latent Variable Multi-output Gaussian Processes for Hierarchical Datasets. (arXiv:2308.16822v1 [cs.LG])

    [http://arxiv.org/abs/2308.16822](http://arxiv.org/abs/2308.16822)

    本文提出了一种针对层级数据集的扩展多输出高斯过程 (MOGPs)，通过引入潜在变量和特定核函数，可以更好地捕捉不同输出之间的关系和相关性。这种方法预计能够在任务数量增加时提高可扩展性。

    

    多输出高斯过程（MOGPs）已经被引入，通过利用不同输出之间的相关性来处理多个任务。通常，MOGPs模型假设输出之间存在平坦的相关结构。然而，这种公式并不能考虑更复杂的关系，例如，如果每个输出都有多个重复观察值（这是生物实验中的典型设置）。本文提出了针对层级数据集的MOGPs扩展（即可以在树状结构中表示观测之间的关系的数据集）。我们的模型定义了一个定制的核函数，考虑了数据中的层级结构，以捕捉不同层次的相关性，同时引入潜在变量来通过专用核函数表示输出之间的潜在依赖关系。预计这个特性能够在任务数量增加时显著提高可扩展性。

    Multi-output Gaussian processes (MOGPs) have been introduced to deal with multiple tasks by exploiting the correlations between different outputs. Generally, MOGPs models assume a flat correlation structure between the outputs. However, such a formulation does not account for more elaborate relationships, for instance, if several replicates were observed for each output (which is a typical setting in biological experiments). This paper proposes an extension of MOGPs for hierarchical datasets (i.e. datasets for which the relationships between observations can be represented within a tree structure). Our model defines a tailored kernel function accounting for hierarchical structures in the data to capture different levels of correlations while leveraging the introduction of latent variables to express the underlying dependencies between outputs through a dedicated kernel. This latter feature is expected to significantly improve scalability as the number of tasks increases. An extensive e
    
[^16]: 基于异步时空图卷积网络的不规则交通时间序列预测

    Irregular Traffic Time Series Forecasting Based on Asynchronous Spatio-Temporal Graph Convolutional Network. (arXiv:2308.16818v1 [cs.LG])

    [http://arxiv.org/abs/2308.16818](http://arxiv.org/abs/2308.16818)

    该论文提出了一种基于异步时空图卷积网络的不规则交通时间序列预测方法，用于解决智能交叉口产生的异步空间依赖、不规则时间依赖和可变长度序列预测等挑战。

    

    准确预测智能交通信号控制系统中受智能交叉口控制的交叉口的交通流量对于提升交通出行效率至关重要。然而，由于智能交叉口产生的交通时间序列不规则，交通流量预测任务变得更加困难，并且面临三个主要挑战：1）异步的空间依赖性，2）交通数据的不规则时间依赖性，3) 需要预测的可变长度序列，严重影响了当前交通流量预测方法的性能。为此，我们提出了一种异步时空图卷积网络(ASeer)来预测智能交叉口进入车道的交通状态。具体而言，通过在交通扩散图上连接车道，我们首先提出了一种异步图扩散网络来模拟车道的异步空间依赖性。

    Accurate traffic forecasting at intersections governed by intelligent traffic signals is critical for the advancement of an effective intelligent traffic signal control system. However, due to the irregular traffic time series produced by intelligent intersections, the traffic forecasting task becomes much more intractable and imposes three major new challenges: 1) asynchronous spatial dependency, 2) irregular temporal dependency among traffic data, and 3) variable-length sequence to be predicted, which severely impede the performance of current traffic forecasting methods. To this end, we propose an Asynchronous Spatio-tEmporal graph convolutional nEtwoRk (ASeer) to predict the traffic states of the lanes entering intelligent intersections in a future time window. Specifically, by linking lanes via a traffic diffusion graph, we first propose an Asynchronous Graph Diffusion Network to model the asynchronous spatial dependency between the time-misaligned traffic state measurements of la
    
[^17]: 图神经网络中的等级崩塌导致平滑过度和关联过高

    Rank Collapse Causes Over-Smoothing and Over-Correlation in Graph Neural Networks. (arXiv:2308.16800v1 [cs.LG])

    [http://arxiv.org/abs/2308.16800](http://arxiv.org/abs/2308.16800)

    本文研究了图神经网络中的平滑过度和特征关联过高现象，发现固定不变的子空间导致了节点表示的等级崩塌。在该子空间中平滑向量的存在导致过度平滑，即使避免过度平滑也会导致过高的关联。为了解决这个问题，我们提出了一种克罗内克积之和作为一种有效方法。

    

    我们的研究揭示了深度图神经网络中平滑过度和特征关联过高的新理论见解。我们展示了固定不变子空间的普遍存在，它表现出一种相对的行为，不受特征转换的影响。我们的工作阐明了与收敛到常数状态和节点状态的过分分离相关的最新观察结果，因为子空间的放大只取决于聚合函数的频谱。在线性场景中，这导致节点表示由低维子空间主导，并且具有与特征转换无关的渐近收敛速率。当平滑向量跨越这个子空间时，这会导致节点表示的等级崩塌，从而导致过度平滑，即使避免过度平滑也会导致过高的关联。在我们的理论指导下，我们提出了一种克罗内克积之和作为一种有益特性，可以可靠地防止过度平滑、过高关联和等级崩塌。

    Our study reveals new theoretical insights into over-smoothing and feature over-correlation in deep graph neural networks. We show the prevalence of invariant subspaces, demonstrating a fixed relative behavior that is unaffected by feature transformations. Our work clarifies recent observations related to convergence to a constant state and a potential over-separation of node states, as the amplification of subspaces only depends on the spectrum of the aggregation function. In linear scenarios, this leads to node representations being dominated by a low-dimensional subspace with an asymptotic convergence rate independent of the feature transformations. This causes a rank collapse of the node representations, resulting in over-smoothing when smooth vectors span this subspace, and over-correlation even when over-smoothing is avoided. Guided by our theory, we propose a sum of Kronecker products as a beneficial property that can provably prevent over-smoothing, over-correlation, and rank c
    
[^18]: 通过最小单纯结构实现语义本地化和推断的联合通信

    Joint Semantic-Native Communication and Inference via Minimal Simplicial Structures. (arXiv:2308.16789v1 [eess.SP])

    [http://arxiv.org/abs/2308.16789](http://arxiv.org/abs/2308.16789)

    通过使用最小单纯结构，这项研究实现了学生代理和教师代理之间的语义通信和推断，提高了推断查询的准确性。

    

    本研究探讨了语义通信和推断问题，其中学生代理（即移动设备）向教师代理（即云服务器）查询生成生活在一个单纯复合体中的高阶数据语义。具体而言，教师先将其数据映射到k阶单纯复合体中，并学习其高阶相关性。为了实现有效的通信和推断，教师通过Hodge拉普拉斯运算选择性地移除单纯形，寻找最小充分和不变的语义结构，在不影响推断查询准确性的前提下传达信息。随后，学生基于掩码单纯卷积自编码器（SCAE）本地运行自己的一组查询，利用本地和远程教师的知识。数值结果验证了所提出方法在不同信道条件下提高推断查询准确性的有效性。

    In this work, we study the problem of semantic communication and inference, in which a student agent (i.e. mobile device) queries a teacher agent (i.e. cloud sever) to generate higher-order data semantics living in a simplicial complex. Specifically, the teacher first maps its data into a k-order simplicial complex and learns its high-order correlations. For effective communication and inference, the teacher seeks minimally sufficient and invariant semantic structures prior to conveying information. These minimal simplicial structures are found via judiciously removing simplices selected by the Hodge Laplacians without compromising the inference query accuracy. Subsequently, the student locally runs its own set of queries based on a masked simplicial convolutional autoencoder (SCAE) leveraging both local and remote teacher's knowledge. Numerical results corroborate the effectiveness of the proposed approach in terms of improving inference query accuracy under different channel conditio
    
[^19]: StratMed：面向低资源药物推荐的相关性分层方法

    StratMed: Relevance Stratification for Low-resource Medication Recommendation. (arXiv:2308.16781v1 [cs.AI])

    [http://arxiv.org/abs/2308.16781](http://arxiv.org/abs/2308.16781)

    StratMed是一种面向低资源药物推荐的模型，通过相关性分层机制来解决医疗数据长尾分布不平衡的问题，平衡了药物组合的安全性和准确性。

    

    随着有限医疗资源与日益增长的需求之间的失衡，基于人工智能的临床任务变得至关重要。作为一个子领域，药物推荐旨在将患者的纵向历史与医学知识相结合，帮助医生更安全、更准确地开具药物组合处方。现有方法忽视了医疗数据中固有的长尾分布，缺乏头尾数据之间的平衡表示，导致模型性能次优。为了解决这个挑战，我们引入了StratMed，这是一个结合了创新的相关性分层机制的模型。它通过协调数据长尾分布中的差异，并在药物组合的安全性和准确性之间取得平衡。具体而言，我们首先使用深度学习网络构建预训练方法来获取实体表示。然后，我们设计了一个类似金字塔的数据分层方法，以获得更通用的实体表示。

    With the growing imbalance between limited medical resources and escalating demands, AI-based clinical tasks have become paramount. Medication recommendation, as a sub-domain, aims to amalgamate longitudinal patient history with medical knowledge, assisting physicians in prescribing safer and more accurate medication combinations. Existing methods overlook the inherent long-tail distribution in medical data, lacking balanced representation between head and tail data, which leads to sub-optimal model performance. To address this challenge, we introduce StratMed, a model that incorporates an innovative relevance stratification mechanism. It harmonizes discrepancies in data long-tail distribution and strikes a balance between the safety and accuracy of medication combinations. Specifically, we first construct a pre-training method using deep learning networks to obtain entity representation. After that, we design a pyramid-like data stratification method to obtain more generalized entity 
    
[^20]: 基于神经预测的零样本NAS范式的有效性

    Efficacy of Neural Prediction-Based NAS for Zero-Shot NAS Paradigm. (arXiv:2308.16775v1 [cs.LG])

    [http://arxiv.org/abs/2308.16775](http://arxiv.org/abs/2308.16775)

    这项研究提出了一种新的方法，通过深度学习进行零样本架构搜索，通过使用可学习的傅里叶正弦和求和编码来构建计算的前馈图，从而解决了基于预测的神经架构搜索中性能指标泛化的限制。

    

    在基于预测的神经架构搜索（NAS）中，通过图卷积网络得到的性能指标取得了显著的成功。然而，通过one-hot编码将前馈结构表示为组件图的这些指标面临一个限制：无法在不同的搜索空间中评估架构的性能。相反，手工性能指标（零样本NAS）可以在多个搜索空间中泛化，因为它们使用相同的架构和随机初始化。为了解决这个限制，我们提出了一种新的深度学习方法，用于零样本NAS。我们的方法采用傅里叶正弦和求和编码来进行卷积核的编码，从而构建了一个计算的前馈图，其结构类似于正在评估的架构。这些编码是可学习的，并提供了架构拓扑信息的全面视图。然后，伴随的多层感知器（MLP）对架构进行排序。

    In prediction-based Neural Architecture Search (NAS), performance indicators derived from graph convolutional networks have shown significant success. These indicators, achieved by representing feed-forward structures as component graphs through one-hot encoding, face a limitation: their inability to evaluate architecture performance across varying search spaces. In contrast, handcrafted performance indicators (zero-shot NAS), which use the same architecture with random initialization, can generalize across multiple search spaces. Addressing this limitation, we propose a novel approach for zero-shot NAS using deep learning. Our method employs Fourier sum of sines encoding for convolutional kernels, enabling the construction of a computational feed-forward graph with a structure similar to the architecture under evaluation. These encodings are learnable and offer a comprehensive view of the architecture's topological information. An accompanying multi-layer perceptron (MLP) then ranks t
    
[^21]: 无需位置标签构建室内基于区域的无线电地图

    Constructing Indoor Region-based Radio Map without Location Labels. (arXiv:2308.16759v1 [cs.LG])

    [http://arxiv.org/abs/2308.16759](http://arxiv.org/abs/2308.16759)

    本文提出了一种无需位置标签的基于区域的无线电地图构建方法，该方法利用接收信号强度（RSS）测量数据，并通过一个综合的分割和聚类算法实现了全局最优解的匹配。

    

    无线电地图的构建需要大量带有位置标签的无线电测量数据，这给部署成本带来了很高的压力。本文提出了一种基于区域的无线电地图构建方法，它利用接收信号强度（RSS）测量数据而无需位置标签。构建过程基于从一个设备上盲目收集到的RSS测量数据，该设备在室内区域中的各个区域中恰好访问一次，但没有记录脚印和时间戳。主要挑战是将RSS数据聚类，并将聚类与物理区域进行匹配。由于多径和噪声的存在，传统的聚类算法无法有效处理RSS数据，因为RSS数据自然而然地呈现为非聚类的形式。本文构建了一个带有顺序先验的信号子空间模型用于处理RSS数据，并开发了一种综合分割和聚类算法，在特殊情况下证明能够找到全局最优解。此外，使用基于图的算法将聚类数据与物理区域进行匹配。

    Radio map construction requires a large amount of radio measurement data with location labels, which imposes a high deployment cost. This paper develops a region-based radio map from received signal strength (RSS) measurements without location labels. The construction is based on a set of blindly collected RSS measurement data from a device that visits each region in an indoor area exactly once, where the footprints and timestamps are not recorded. The main challenge is to cluster the RSS data and match clusters with the physical regions. Classical clustering algorithms fail to work as the RSS data naturally appears as non-clustered due to multipaths and noise. In this paper, a signal subspace model with a sequential prior is constructed for the RSS data, and an integrated segmentation and clustering algorithm is developed, which is shown to find the globally optimal solution in a special case. Furthermore, the clustered data is matched with the physical regions using a graph-based app
    
[^22]: 使用再生核空间插值和模型简化训练神经网络的理论研究

    Training Neural Networks Using Reproducing Kernel Space Interpolation and Model Reduction. (arXiv:2308.16754v1 [math.FA])

    [http://arxiv.org/abs/2308.16754](http://arxiv.org/abs/2308.16754)

    这篇论文研究了使用再生核空间插值和模型简化训练神经网络的理论，并推导了一个多维的亚当扬-阿罗夫-克雷因定理，提出了一种新型的神经网络架构（PNN），在噪声环境中表现出比传统方法更优越的性能。

    

    我们引入并研究了使用再生核希尔伯特空间理论中的插值技术训练神经网络的理论。我们将该方法推广到克莱因空间，并证明了广泛使用的神经网络架构是再生核克莱因空间（RKKS）的子集。我们研究了与RKKS关联的希尔伯特空间的概念，并开发了改进各种激活函数表达能力的技术。接下来，我们使用几个复变函数的理论概念证明了一个可计算的、多维的亚当扬-阿罗夫-克雷因（AAK）定理的推广。该定理产生了一类新型神经网络，称为延拓神经网络（PNN）。我们证明，通过将多维AAK定理用于获得PNN，可以在噪声环境中获得优于我们的插值方法和当前最先进方法的性能。我们在实例中提供了我们方法的有用说明。

    We introduce and study the theory of training neural networks using interpolation techniques from reproducing kernel Hilbert space theory. We generalize the method to Krein spaces, and show that widely-used neural network architectures are subsets of reproducing kernel Krein spaces (RKKS). We study the concept of "associated Hilbert spaces" of RKKS and develop techniques to improve upon the expressivity of various activation functions. Next, using concepts from the theory of functions of several complex variables, we prove a computationally applicable, multidimensional generalization of the celebrated Adamjan- Arov-Krein (AAK) theorem. The theorem yields a novel class of neural networks, called Prolongation Neural Networks (PNN). We demonstrate that, by applying the multidimensional AAK theorem to gain a PNN, one can gain performance superior to both our interpolatory methods and current state-of-the-art methods in noisy environments. We provide useful illustrations of our methods in p
    
[^23]: Moreau包络ADMM算法用于分散弱凸优化

    Moreau Envelope ADMM for Decentralized Weakly Convex Optimization. (arXiv:2308.16752v1 [math.OC])

    [http://arxiv.org/abs/2308.16752](http://arxiv.org/abs/2308.16752)

    本文提出了一种分布式弱凸优化问题的近似变种ADMM算法，通过分析Moreau包络函数，证明了该算法在一些条件下能收敛到稳定点，同时实验结果表明该方法比常用方法更快、更稳健。

    

    本文提出了一种分布式优化问题的局部方向乘法（ADMM）的近似变种。尽管目前的ADMM算法在许多凸和非凸优化问题中都提供了接近最优解的有希望的数值结果，但对于弱凸和局部非光滑函数是否能收敛到稳定点仍然不清楚。通过使用Moreau包络函数进行分析，我们证明了MADM在一些温和条件下确实能收敛到稳定点。我们的分析还包括通过将Moreau包络函数的梯度与近心函数相关联来计算对偶变量更新步骤中的改变界限。此外，我们的数值实验结果表明，我们的方法比广泛使用的方法更快、更稳健。

    This paper proposes a proximal variant of the alternating direction method of multipliers (ADMM) for distributed optimization. Although the current versions of ADMM algorithm provide promising numerical results in producing solutions that are close to optimal for many convex and non-convex optimization problems, it remains unclear if they can converge to a stationary point for weakly convex and locally non-smooth functions. Through our analysis using the Moreau envelope function, we demonstrate that MADM can indeed converge to a stationary point under mild conditions. Our analysis also includes computing the bounds on the amount of change in the dual variable update step by relating the gradient of the Moreau envelope function to the proximal function. Furthermore, the results of our numerical experiments indicate that our method is faster and more robust than widely-used approaches.
    
[^24]: US-SFNet:基于空间频域的多分支网络用于超声图像中的颈部淋巴结病灶诊断

    US-SFNet: A Spatial-Frequency Domain-based Multi-branch Network for Cervical Lymph Node Lesions Diagnoses in Ultrasound Images. (arXiv:2308.16738v1 [eess.IV])

    [http://arxiv.org/abs/2308.16738](http://arxiv.org/abs/2308.16738)

    本论文提出了一种基于空间频域的多分支网络US-SFNet，用于超声图像中颈部淋巴结病变的诊断。通过使用Conv-FFT块来建模图像，实现更准确的诊断结果。

    

    超声成像是诊断颈部淋巴结病灶的关键工具。然而，这些图像的诊断主要依赖于医务人员的专业知识，使得这一过程容易出现误诊。尽管迅速发展的深度学习在改进各种超声图像的诊断方面取得了实质性的进展，但在颈部淋巴结方面仍存在明显的研究空白。我们的目标是通过利用深度学习模型准确诊断颈部淋巴结病灶。为此，我们首先收集了3392张正常淋巴结、良性淋巴结病灶、恶性原发淋巴结病灶和恶性转移淋巴结病灶的图像。鉴于超声图像是由声波在不同的身体组织中反射和散射产生的，我们提出了Conv-FFT块。它将卷积操作与快速傅里叶变换相结合，更准确地建模图像。在此基础上构建了...

    Ultrasound imaging serves as a pivotal tool for diagnosing cervical lymph node lesions. However, the diagnoses of these images largely hinge on the expertise of medical practitioners, rendering the process susceptible to misdiagnoses. Although rapidly developing deep learning has substantially improved the diagnoses of diverse ultrasound images, there remains a conspicuous research gap concerning cervical lymph nodes. The objective of our work is to accurately diagnose cervical lymph node lesions by leveraging a deep learning model. To this end, we first collected 3392 images containing normal lymph nodes, benign lymph node lesions, malignant primary lymph node lesions, and malignant metastatic lymph node lesions. Given that ultrasound images are generated by the reflection and scattering of sound waves across varied bodily tissues, we proposed the Conv-FFT Block. It integrates convolutional operations with the fast Fourier transform to more astutely model the images. Building upon thi
    
[^25]: 鲁棒的网络化联邦学习在定位中的应用

    Robust Networked Federated Learning for Localization. (arXiv:2308.16737v1 [cs.LG])

    [http://arxiv.org/abs/2308.16737](http://arxiv.org/abs/2308.16737)

    本文提出了一种鲁棒的网络化联邦学习方法，通过采用$L_1$-范数鲁棒性和分布式次梯度框架，解决了在分布式环境中定位问题中的异常数据干扰和算法收敛挑战。

    

    本文解决了在数据分布在多设备上的联邦环境中，本质上是非凸非光滑的定位问题。由于联邦环境的分散性质，分布式学习成为可伸缩性和适应性的关键。此外，这些环境经常受到异常数据的干扰，使得传统方法在维护估计精度和确保算法收敛方面面临重大挑战。为了解决这些挑战，我们提出了一种采用分布式次梯度框架中$L_1$-范数鲁棒性的方法，专门设计用于处理这些障碍。我们的方法以原始形式解决问题，而不是采用迭代简化或近似方法，从而提高计算效率和估计精度。我们证明了我们的方法收敛到一个稳定点，突出了其有效性。

    This paper addresses the problem of localization, which is inherently non-convex and non-smooth in a federated setting where the data is distributed across a multitude of devices. Due to the decentralized nature of federated environments, distributed learning becomes essential for scalability and adaptability. Moreover, these environments are often plagued by outlier data, which presents substantial challenges to conventional methods, particularly in maintaining estimation accuracy and ensuring algorithm convergence. To mitigate these challenges, we propose a method that adopts an $L_1$-norm robust formulation within a distributed sub-gradient framework, explicitly designed to handle these obstacles. Our approach addresses the problem in its original form, without resorting to iterative simplifications or approximations, resulting in enhanced computational efficiency and improved estimation accuracy. We demonstrate that our method converges to a stationary point, highlighting its effec
    
[^26]: 可靠性部分标签学习的鲁棒表示学习

    Robust Representation Learning for Unreliable Partial Label Learning. (arXiv:2308.16718v1 [cs.LG])

    [http://arxiv.org/abs/2308.16718](http://arxiv.org/abs/2308.16718)

    本文介绍了一种解决不可靠部分标签学习问题的鲁棒表示学习方法，通过利用鲁棒对比学习和标签质量改进策略，提高模型性能并抵抗不准确部分标签的影响。

    

    部分标签学习是一种弱监督学习，其中每个训练实例被分配一个候选标签集，但只有一个标签是真实的。然而，由于潜在的注释不准确性，这个理想化的假设并不总是成立，意味着真实标签可能不在候选标签集中。这被称为不可靠的部分标签学习，在现有方法中由于部分标签的不可靠性和不确定性而导致性能亚优。为了解决这个挑战，我们提出了一种名为可靠性鲁棒表示学习框架（URRL）的方法，它利用鲁棒对比学习来帮助模型有效地抵御不可靠的部分标签。同时，我们提出了一种双重策略，结合基于KNN的候选标签集校正和基于一致性正则化的标签消除来改进标签质量和提高性能。

    Partial Label Learning (PLL) is a type of weakly supervised learning where each training instance is assigned a set of candidate labels, but only one label is the ground-truth. However, this idealistic assumption may not always hold due to potential annotation inaccuracies, meaning the ground-truth may not be present in the candidate label set. This is known as Unreliable Partial Label Learning (UPLL) that introduces an additional complexity due to the inherent unreliability and ambiguity of partial labels, often resulting in a sub-optimal performance with existing methods. To address this challenge, we propose the Unreliability-Robust Representation Learning framework (URRL) that leverages unreliability-robust contrastive learning to help the model fortify against unreliable partial labels effectively. Concurrently, we propose a dual strategy that combines KNN-based candidate label set correction and consistency-regularization-based label disambiguation to refine label quality and enh
    
[^27]: 任何人都可以攻击：将有损压缩重新用作自然后门攻击

    Everyone Can Attack: Repurpose Lossy Compression as a Natural Backdoor Attack. (arXiv:2308.16684v1 [cs.CR])

    [http://arxiv.org/abs/2308.16684](http://arxiv.org/abs/2308.16684)

    本文发现了一种更严重的后门攻击威胁，即任何人都可以利用易获取的有损压缩算法进行自然后门攻击，无需设计特定触发器或进行繁琐调试。

    

    最近，后门攻击对实际应用中的机器学习模型的可信度构成了威胁。传统智慧认为，并不是每个人都可以成为攻击者，因为设计触发器生成算法的过程通常需要大量的努力和广泛的实验来确保攻击的隐秘性和有效性。然而，本文指出存在一种更为严重的后门威胁：任何人都可以利用易获取的算法进行隐悄后门攻击。具体来说，攻击者可以利用各种压缩工具中广泛使用的有损图片压缩技术，无需留下任何明显的痕迹就能轻松地将触发器模式注入到图像中，即生成的触发器是自然的图像伪影。使用有损图片压缩工具时，人们并不需要广泛知识，只需点击“转换”或“另存为”按钮即可。通过这种攻击，攻击者无需设计一个专门的触发器或进行繁琐的调试。

    The vulnerabilities to backdoor attacks have recently threatened the trustworthiness of machine learning models in practical applications. Conventional wisdom suggests that not everyone can be an attacker since the process of designing the trigger generation algorithm often involves significant effort and extensive experimentation to ensure the attack's stealthiness and effectiveness. Alternatively, this paper shows that there exists a more severe backdoor threat: anyone can exploit an easily-accessible algorithm for silent backdoor attacks. Specifically, this attacker can employ the widely-used lossy image compression from a plethora of compression tools to effortlessly inject a trigger pattern into an image without leaving any noticeable trace; i.e., the generated triggers are natural artifacts. One does not require extensive knowledge to click on the "convert" or "save as" button while using tools for lossy image compression. Via this attack, the adversary does not need to design a 
    
[^28]: 通过多元宇宙分析评估模型设计决策对算法公平性的影响：一切，无处不在，全方位评估

    Everything, Everywhere All in One Evaluation: Using Multiverse Analysis to Evaluate the Influence of Model Design Decisions on Algorithmic Fairness. (arXiv:2308.16681v1 [stat.ML])

    [http://arxiv.org/abs/2308.16681](http://arxiv.org/abs/2308.16681)

    通过多元宇宙分析评估模型设计决策对算法公平性的影响，可以揭示算法决策系统中设计决策的关键作用。

    

    全球范围内的许多系统都利用算法决策来（部分）自动化以前由人类进行的决策。当设计良好时，这些系统承诺更客观的决策，同时节省大量资源，节约人力。然而，当算法决策系统设计不良时，可能会导致对社会群体进行歧视的不公平决策。算法决策系统的下游效应在很大程度上取决于系统设计和实施过程中的决策，因为数据中的偏见可能会在建模过程中缓解或加强。许多这些设计决策是隐含进行的，不知道它们确切地如何影响最终系统。因此，明确算法决策系统设计中的决策并了解这些决策如何影响结果系统的公平性非常重要。为了研究这个问题，我们借鉴了心理学领域的见解，并引入了多元宇宙分析方法。

    A vast number of systems across the world use algorithmic decision making (ADM) to (partially) automate decisions that have previously been made by humans. When designed well, these systems promise more objective decisions while saving large amounts of resources and freeing up human time. However, when ADM systems are not designed well, they can lead to unfair decisions which discriminate against societal groups. The downstream effects of ADMs critically depend on the decisions made during the systems' design and implementation, as biases in data can be mitigated or reinforced along the modeling pipeline. Many of these design decisions are made implicitly, without knowing exactly how they will influence the final system. It is therefore important to make explicit the decisions made during the design of ADM systems and understand how these decisions affect the fairness of the resulting system.  To study this issue, we draw on insights from the field of psychology and introduce the metho
    
[^29]: 树的分支：在高能物理中对具有离散和分支随机性的程序进行求导

    Branches of a Tree: Taking Derivatives of Programs with Discrete and Branching Randomness in High Energy Physics. (arXiv:2308.16680v1 [stat.ML])

    [http://arxiv.org/abs/2308.16680](http://arxiv.org/abs/2308.16680)

    该论文提出了应用多种梯度估计技术实现对具有离散和分支随机性的程序进行求导的方法，并发展了首个完全可微分的分支程序，这项工作的贡献在于为高能物理中的梯度优化提供了新的可能性。

    

    我们提出应用多种梯度估计技术来实现对在高能物理中具有离散随机性的程序进行求导。由于存在分支过程和基于聚类的分析，此类程序在高能物理中很常见。因此，对这类程序进行求导可以为梯度优化在探测器设计优化、模拟器调整或数据分析和重构优化等方面开辟道路。我们讨论了几种可能的梯度估计策略，包括最近的随机自动微分（Stochastic AD）方法，并在简化的探测器设计实验中进行了比较。在这样做的过程中，我们开发了迄今为止首个完全可微分的分支程序。

    We propose to apply several gradient estimation techniques to enable the differentiation of programs with discrete randomness in High Energy Physics. Such programs are common in High Energy Physics due to the presence of branching processes and clustering-based analysis. Thus differentiating such programs can open the way for gradient based optimization in the context of detector design optimization, simulator tuning, or data analysis and reconstruction optimization. We discuss several possible gradient estimation strategies, including the recent Stochastic AD method, and compare them in simplified detector design experiments. In doing so we develop, to the best of our knowledge, the first fully differentiable branching program.
    
[^30]: 动态nsNet2: 高效的提前退出深度噪声抑制模型

    Dynamic nsNet2: Efficient Deep Noise Suppression with Early Exiting. (arXiv:2308.16678v1 [cs.SD])

    [http://arxiv.org/abs/2308.16678](http://arxiv.org/abs/2308.16678)

    本文提出了一个提前退出模型，通过适应原始架构和分割信息流，实现了在资源受限的设备上高效的深度噪声抑制，并展示了性能和计算复杂性之间的权衡。

    

    尽管深度学习在深度噪声抑制领域取得了进展，但在资源受限的设备上利用深度架构仍然具有挑战性。因此，我们提出了一个基于nsNet2的提前退出模型，通过在不同阶段停止计算来提供多个精度级别和资源节省。此外，我们通过分割信息流来适应原始架构，以考虑注入的动态性。我们根据既定的指标展示性能和计算复杂性之间的权衡。

    Although deep learning has made strides in the field of deep noise suppression, leveraging deep architectures on resource-constrained devices still proved challenging. Therefore, we present an early-exiting model based on nsNet2 that provides several levels of accuracy and resource savings by halting computations at different stages. Moreover, we adapt the original architecture by splitting the information flow to take into account the injected dynamism. We show the trade-offs between performance and computational complexity based on established metrics.
    
[^31]: 基于一位压缩感知的通信高效的分散式联邦学习

    Communication-Efficient Decentralized Federated Learning via One-Bit Compressive Sensing. (arXiv:2308.16671v1 [cs.LG])

    [http://arxiv.org/abs/2308.16671](http://arxiv.org/abs/2308.16671)

    本文提出了基于一位压缩感知的通信高效的分散式联邦学习算法，通过在邻居节点之间传输一位信息并减少通信回合的数量，实现了对具有稀疏约束的共享模型的高效训练。

    

    分散式联邦学习（DFL）因其在各种应用中的实用性而变得流行。与集中式版本相比，在DFL中在大量节点之间训练共享模型更具挑战性，因为没有中央服务器来协调训练过程。尤其是当分布式节点在通信或计算资源方面存在限制时，DFL的训练将变得非常低效和不稳定。鉴于这些挑战，本文基于不精确交替方向方法（iADM）框架提出了一种新算法。一方面，我们的目标是训练具有稀疏约束的共享模型。该约束使我们能够利用一位压缩感知（1BCS），允许邻居节点之间传输一位信息。另一方面，邻居节点之间的通信仅在某些步骤中发生，减少了通信回合的数量。因此，该算法展现了高效的特点。

    Decentralized federated learning (DFL) has gained popularity due to its practicality across various applications. Compared to the centralized version, training a shared model among a large number of nodes in DFL is more challenging, as there is no central server to coordinate the training process. Especially when distributed nodes suffer from limitations in communication or computational resources, DFL will experience extremely inefficient and unstable training. Motivated by these challenges, in this paper, we develop a novel algorithm based on the framework of the inexact alternating direction method (iADM). On one hand, our goal is to train a shared model with a sparsity constraint. This constraint enables us to leverage one-bit compressive sensing (1BCS), allowing transmission of one-bit information among neighbour nodes. On the other hand, communication between neighbour nodes occurs only at certain steps, reducing the number of communication rounds. Therefore, the algorithm exhibi
    
[^32]: 我们可以从量子卷积神经网络中学到什么？

    What can we learn from quantum convolutional neural networks?. (arXiv:2308.16664v1 [quant-ph])

    [http://arxiv.org/abs/2308.16664](http://arxiv.org/abs/2308.16664)

    通过分析量子卷积神经网络（QCNNs），我们发现它们通过隐藏特征映射嵌入物理系统参数，并且利用量子临界性生成适合的基函数集，池化层选择能够形成高性能决策边界的基函数，而模型的泛化性能依赖于嵌入类型。

    

    通过分析量子卷积神经网络（QCNNs），我们可以得出以下结论：1）通过隐藏特征映射，工作于量子数据可以被视为嵌入物理系统参数；2）对于量子相位识别，其高性能可以归因于在基态嵌入期间生成非常适合的基函数集，其中自旋模型的量子临界性导致具有快速变化特征的基函数；3）QCNN的池化层负责选择那些能够有助于形成高性能决策边界的基函数，学习过程对应于适应性测量，使得少量量子比特算符映射到整个寄存器可观测量；4）QCNN模型的泛化强烈依赖于嵌入类型，基于傅里叶基的旋转特征映射需要仔细的特征工程；5）基于有限数量的测量次数的读出的QCNN的准确性和泛化能力倾向于地面态。

    We can learn from analyzing quantum convolutional neural networks (QCNNs) that: 1) working with quantum data can be perceived as embedding physical system parameters through a hidden feature map; 2) their high performance for quantum phase recognition can be attributed to generation of a very suitable basis set during the ground state embedding, where quantum criticality of spin models leads to basis functions with rapidly changing features; 3) pooling layers of QCNNs are responsible for picking those basis functions that can contribute to forming a high-performing decision boundary, and the learning process corresponds to adapting the measurement such that few-qubit operators are mapped to full-register observables; 4) generalization of QCNN models strongly depends on the embedding type, and that rotation-based feature maps with the Fourier basis require careful feature engineering; 5) accuracy and generalization of QCNNs with readout based on a limited number of shots favor the groun
    
[^33]: 基于自编码器的CMS电磁量能器在线数据质量监测

    Autoencoder-based Online Data Quality Monitoring for the CMS Electromagnetic Calorimeter. (arXiv:2308.16659v1 [physics.ins-det])

    [http://arxiv.org/abs/2308.16659](http://arxiv.org/abs/2308.16659)

    本研究基于自编码器开发了一种实时的异常检测系统，可有效检测到CMS电磁量能器(ECAL)中过去未见的异常，具有较低的错误发现率。

    

    CMS电磁量能器（ECAL）的在线数据质量监测系统（DQM）是一个关键的操作工具，它使ECAL专家能够快速识别、定位和诊断各种可能影响物理数据质量的探测器问题。尽管现有的ECAL DQM系统已不断更新以应对新问题，但仍比新问题慢一步。利用无监督深度学习，开发了一种实时的基于自编码器的异常检测系统，能够检测到过去数据中未见的ECAL异常。考虑到ECAL响应的空间变化和异常的时间演变，新系统能够高效地检测到异常，同时保持估计的错误发现率在$10^{-2}$到$10^{-4}$之间，比现有基准提高了两个数量级。通过在2018年和2022年发现的异常进行验证，证实了系统的现实性能。

    The online Data Quality Monitoring system (DQM) of the CMS electromagnetic calorimeter (ECAL) is a crucial operational tool that allows ECAL experts to quickly identify, localize, and diagnose a broad range of detector issues that would otherwise hinder physics-quality data taking. Although the existing ECAL DQM system has been continuously updated to respond to new problems, it remains one step behind newer and unforeseen issues. Using unsupervised deep learning, a real-time autoencoder-based anomaly detection system is developed that is able to detect ECAL anomalies unseen in past data. After accounting for spatial variations in the response of the ECAL and the temporal evolution of anomalies, the new system is able to efficiently detect anomalies while maintaining an estimated false discovery rate between $10^{-2}$ to $10^{-4}$, beating existing benchmarks by about two orders of magnitude. The real-world performance of the system is validated using anomalies found in 2018 and 2022 L
    
[^34]: 根据地图生成真实卫星影像：一种基于地图的卫星图像生成方法

    Generate Your Own Scotland: Satellite Image Generation Conditioned on Maps. (arXiv:2308.16648v1 [cs.CV])

    [http://arxiv.org/abs/2308.16648](http://arxiv.org/abs/2308.16648)

    本论文提出了一种基于地图的卫星图像生成方法，使用预训练的扩散模型，可以生成逼真的卫星图像，实验结果表明图像质量和地图保真度都很好。这些方法在遥感领域有很大的应用前景和挑战。

    

    尽管图像生成领域取得了一些进展，但在地球观测领域中，扩散模型仍然被大多数人忽视。本文展示了最先进的预训练扩散模型可以通过地理数据来生成逼真的卫星图像。我们提供了两个大型数据集，其中包含了OpenStreetMap图像和苏格兰中央带地区的卫星图像。我们训练了一个ControlNet模型，并对结果进行了定性评估，证明了图像质量和地图保真度都是可行的。最后，我们对将这些模型应用于遥感领域的机遇和挑战进行了一些探讨。我们公开了创建数据集的模型权重和代码，网址为https://github.com/miquel-espinosa/map-sat。

    Despite recent advancements in image generation, diffusion models still remain largely underexplored in Earth Observation. In this paper we show that state-of-the-art pretrained diffusion models can be conditioned on cartographic data to generate realistic satellite images. We provide two large datasets of paired OpenStreetMap images and satellite views over the region of Mainland Scotland and the Central Belt. We train a ControlNet model and qualitatively evaluate the results, demonstrating that both image quality and map fidelity are possible. Finally, we provide some insights on the opportunities and challenges of applying these models for remote sensing. Our model weights and code for creating the dataset are publicly available at https://github.com/miquel-espinosa/map-sat.
    
[^35]: 通过合作专家实现长尾图分类的研究

    Towards Long-Tailed Recognition for Graph Classification via Collaborative Experts. (arXiv:2308.16609v1 [cs.LG])

    [http://arxiv.org/abs/2308.16609](http://arxiv.org/abs/2308.16609)

    本文提出了一种新颖的方法，通过合作专家实现了长尾图分类，解决了现有方法在处理图数据上的不足。

    

    图分类旨在学习用于有效类别分配的图级表示，在平衡的类别分布的高质量数据集的支持下取得了杰出成果。事实上，大多数现实世界的图数据自然呈现长尾形式，其中头部类别的样本数量远超过尾部类别，因此在长尾数据上研究图级分类是至关重要的，但仍然较少探索。然而，现有的视觉中的长尾学习方法大多无法同时优化表示学习和分类器训练，并且忽略了难以分类的类别的挖掘。直接将现有方法应用于图可能导致次优性能，因为在图上训练的模型由于复杂的拓扑特征会更加敏感于长尾分布。因此，在本文中，我们提出了一种新颖的对长尾图级分类的方法

    Graph classification, aiming at learning the graph-level representations for effective class assignments, has received outstanding achievements, which heavily relies on high-quality datasets that have balanced class distribution. In fact, most real-world graph data naturally presents a long-tailed form, where the head classes occupy much more samples than the tail classes, it thus is essential to study the graph-level classification over long-tailed data while still remaining largely unexplored. However, most existing long-tailed learning methods in visions fail to jointly optimize the representation learning and classifier training, as well as neglect the mining of the hard-to-classify classes. Directly applying existing methods to graphs may lead to sub-optimal performance, since the model trained on graphs would be more sensitive to the long-tailed distribution due to the complex topological characteristics. Hence, in this paper, we propose a novel long-tailed graph-level classifica
    
[^36]: 通过因果关系发现方法学习城市形态如何影响不同大洲的可持续出行方式

    A Causal Discovery Approach To Learn How Urban Form Shapes Sustainable Mobility Across Continents. (arXiv:2308.16599v1 [cs.LG])

    [http://arxiv.org/abs/2308.16599](http://arxiv.org/abs/2308.16599)

    通过因果关系发现方法，研究者们利用高分辨率的出行数据，发现城市形态变量对于城市内出行方式产生了间接影响，填补了当前研究的不足，为实现全球可持续交通系统提供了指导。

    

    全球可持续发展需要低碳城市交通系统，这需要适当的基础设施，低碳交通方式的推广和出行行为的改变。为了正确实施基础设施变革，了解建筑环境对出行的影响的地理特定的因果关系机制至关重要。然而，目前的研究在表示6D城市形态变量和出行之间的因果关系、在不同地区之间进行泛化以及在高空间分辨率下建模城市形态效应方面存在不足。在本研究中，我们利用因果发现和可解释的机器学习框架，根据三个大洲六个城市的高分辨率出行数据，检测城市形态对城市内出行的影响，并填补了这三个差距。我们发现，距离市中心、人口统计数据和密度间接影响其他城市形态特征。通过考虑这些因果关系，我们发现地理位置对不同城市之间的交通方式选择产生了影响。

    Global sustainability requires low-carbon urban transport systems, shaped by adequate infrastructure, deployment of low-carbon transport modes and shifts in travel behavior. To adequately implement alterations in infrastructure, it's essential to grasp the location-specific cause-and-effect mechanisms that the constructed environment has on travel. Yet, current research falls short in representing causal relationships between the 6D urban form variables and travel, generalizing across different regions, and modeling urban form effects at high spatial resolution. Here, we address all three gaps by utilizing a causal discovery and an explainable machine learning framework to detect urban form effects on intra-city travel based on high-resolution mobility data of six cities across three continents. We show that both distance to city center, demographics and density indirectly affect other urban form features. By considering the causal relationships, we find that location-specific influenc
    
[^37]: 关于肿瘤分割中视觉变换器中最佳补丁尺寸的研究

    Towards Optimal Patch Size in Vision Transformers for Tumor Segmentation. (arXiv:2308.16598v1 [eess.IV])

    [http://arxiv.org/abs/2308.16598](http://arxiv.org/abs/2308.16598)

    本论文研究了在肿瘤分割中，视觉变换器中最佳补丁尺寸的选择。目前基于完全卷积神经网络的深度学习模型已成为主流，但由于卷积层的局限性，它们不能有效捕捉长距离依赖和全局上下文。为解决这个问题，引入了视觉变换器，但由于对输入补丁尺寸敏感，其在不同肿瘤大小上的性能下降。因此，本文提出了一种技术来选择视觉变换器的最佳输入多分辨率。

    

    在肝癌的早期诊断和治疗中，转移性结直肠癌（mCRC）中的肿瘤检测起着重要作用。以完全卷积神经网络（FCNN）为主干的深度学习模型已经成为分割3D计算机断层扫描（CT）的主要模型。然而，由于卷积层的局限性，它们不能捕捉到远距离的依赖和全局语境。为了解决这个限制，引入了视觉变换器来解决FCNN的接受域的局部性。尽管变换器可以捕捉到远距离的特征，但由于对输入补丁大小的敏感性，其分割性能在各种肿瘤大小上都有所下降。虽然找到最佳补丁尺寸可以提高基于视觉变换器的模型在分割任务上的性能，但这是一个耗时且具有挑战性的过程。本文提出了一种选择视觉变换器最佳输入多分辨率的技术

    Detection of tumors in metastatic colorectal cancer (mCRC) plays an essential role in the early diagnosis and treatment of liver cancer. Deep learning models backboned by fully convolutional neural networks (FCNNs) have become the dominant model for segmenting 3D computerized tomography (CT) scans. However, since their convolution layers suffer from limited kernel size, they are not able to capture long-range dependencies and global context. To tackle this restriction, vision transformers have been introduced to solve FCNN's locality of receptive fields. Although transformers can capture long-range features, their segmentation performance decreases with various tumor sizes due to the model sensitivity to the input patch size. While finding an optimal patch size improves the performance of vision transformer-based models on segmentation tasks, it is a time-consuming and challenging procedure. This paper proposes a technique to select the vision transformer's optimal input multi-resoluti
    
[^38]: 通过半监督预训练实现自发风格建模用于对话文本到语音合成

    Towards Spontaneous Style Modeling with Semi-supervised Pre-training for Conversational Text-to-Speech Synthesis. (arXiv:2308.16593v1 [cs.SD])

    [http://arxiv.org/abs/2308.16593](http://arxiv.org/abs/2308.16593)

    本文提出了一种半监督预训练方法，通过增加自发风格语音和自发行为标签的数量，以实现自发风格建模用于对话文本到语音合成。实验结果显示，该方法能够在自发风格的语音中建模自发行为，并从文本中预测合理的自发行为。

    

    对话中经常发生的自发行为使得语音听起来更加像人类，而不是像朗读。然而，合成自发风格的语音具有挑战性，因为缺乏高质量的自发数据集，并且标记自发行为的成本较高。在本文中，我们提出了一种半监督预训练方法，以增加自发风格语音和自发行为标签的数量。在半监督学习的过程中，考虑了文本和语音信息，以便在语音中检测自发行为标签。此外，使用了一个语言感知编码器来建模对话中每个句子之间的关系。实验结果表明，我们提出的方法在表达性语音合成性能方面具有优越性，能够在自发风格的语音中建模自发行为，并从文本中预测合理的自发行为。

    The spontaneous behavior that often occurs in conversations makes speech more human-like compared to reading-style. However, synthesizing spontaneous-style speech is challenging due to the lack of high-quality spontaneous datasets and the high cost of labeling spontaneous behavior. In this paper, we propose a semi-supervised pre-training method to increase the amount of spontaneous-style speech and spontaneous behavioral labels. In the process of semi-supervised learning, both text and speech information are considered for detecting spontaneous behaviors labels in speech. Moreover, a linguistic-aware encoder is used to model the relationship between each sentence in the conversation. Experimental results indicate that our proposed method achieves superior expressive speech synthesis performance with the ability to model spontaneous behavior in spontaneous-style speech and predict reasonable spontaneous behavior from text.
    
[^39]: 开发和验证可解释的基于机器学习的算法，用于预测减重手术后5年的体重变化轨迹：一项跨国回顾性队列研究

    Development and validation of an interpretable machine learning-based calculator for predicting 5-year weight trajectories after bariatric surgery: a multinational retrospective cohort SOPHIA study. (arXiv:2308.16585v1 [cs.LG])

    [http://arxiv.org/abs/2308.16585](http://arxiv.org/abs/2308.16585)

    开发和验证了一种机器学习算法，可以预测减重手术后5年的体重变化轨迹，为个体术前提供准确预测。

    

    背景：减重手术后的体重变化轨迹因人而异，预测手术前的减重情况仍然具有挑战性。本研究旨在使用机器学习模型，为手术后5年的体重变化轨迹提供个体化的术前预测。

    Background Weight loss trajectories after bariatric surgery vary widely between individuals, and predicting weight loss before the operation remains challenging. We aimed to develop a model using machine learning to provide individual preoperative prediction of 5-year weight loss trajectories after surgery. Methods In this multinational retrospective observational study we enrolled adult participants (aged $\ge$18 years) from ten prospective cohorts (including ABOS [NCT01129297], BAREVAL [NCT02310178], the Swedish Obese Subjects study, and a large cohort from the Dutch Obesity Clinic [Nederlandse Obesitas Kliniek]) and two randomised trials (SleevePass [NCT00793143] and SM-BOSS [NCT00356213]) in Europe, the Americas, and Asia, with a 5 year followup after Roux-en-Y gastric bypass, sleeve gastrectomy, or gastric band. Patients with a previous history of bariatric surgery or large delays between scheduled and actual visits were excluded. The training cohort comprised patients from two ce
    
[^40]: CL-MAE: 课程学习的遮罩自编码器

    CL-MAE: Curriculum-Learned Masked Autoencoders. (arXiv:2308.16572v1 [cs.CV])

    [http://arxiv.org/abs/2308.16572](http://arxiv.org/abs/2308.16572)

    本文提出了一种课程学习的遮罩自编码器（CL-MAE）。我们引入了一种可学习的遮罩模块，通过更新遮罩策略来增加自监督重构任务的复杂性。通过逐渐增加任务复杂性，模型可以学习更复杂和可迁移的表示。

    

    遮罩图像建模已被证明是一种强大的预文本任务，用于生成能够有效泛化到多个下游任务的鲁棒表示。通常，这种方法涉及在输入图像中随机遮罩补丁（标记），并且遮罩策略在训练过程中保持不变。本文提出了一种课程学习方法，通过更新遮罩策略以持续增加自监督重构任务的复杂性。我们推测，通过逐渐增加任务复杂性，模型可以学习更复杂和可迁移的表示。为了实现这一点，我们引入了一种新颖的可学习遮罩模块，具有生成不同复杂度遮罩的能力，并将该模块与遮罩自编码器（MAE）集成。我们的模块与MAE一同训练，同时调整其行为，在训练过程中从MAE的参与者过渡到MAE（优化相同的重构目标）。

    Masked image modeling has been demonstrated as a powerful pretext task for generating robust representations that can be effectively generalized across multiple downstream tasks. Typically, this approach involves randomly masking patches (tokens) in input images, with the masking strategy remaining unchanged during training. In this paper, we propose a curriculum learning approach that updates the masking strategy to continually increase the complexity of the self-supervised reconstruction task. We conjecture that, by gradually increasing the task complexity, the model can learn more sophisticated and transferable representations. To facilitate this, we introduce a novel learnable masking module that possesses the capability to generate masks of different complexities, and integrate the proposed module into masked autoencoders (MAE). Our module is jointly trained with the MAE, while adjusting its behavior during training, transitioning from a partner to the MAE (optimizing the same rec
    
[^41]: 在BaDLAD数据集上的文档布局分析：一种基于MViTv2的综合方法

    Document Layout Analysis on BaDLAD Dataset: A Comprehensive MViTv2 Based Approach. (arXiv:2308.16571v1 [cs.CV])

    [http://arxiv.org/abs/2308.16571](http://arxiv.org/abs/2308.16571)

    我们使用MViTv2模型在BaDLAD数据集上进行文档布局分析，实现了文本框、段落、图片和表格的自动提取。我们还探索了增强方法，并发现了一些有效的性能改进。

    

    在迅速发展的数字时代，文档布局分析在自动化信息提取和解释中起到关键作用。在我们的工作中，我们使用级联掩码R-CNN训练了MViTv2 transformer模型架构，从文档中提取文本框、段落、图片和表格。在一个3阶段循环中，我们在20365个文档图像上进行了36个周期的训练，实现了0.2125的训练损失和0.19的掩码损失。我们的工作不仅限于训练，还深入探讨了潜在的增强途径。我们调查了旋转和翻转增强的影响，切片输入图像预推论的有效性，变化的变换器骨干分辨率的影响，以及采用双通推论来发现漏掉的文本框的潜力。通过这些探索，我们观察到了一系列结果，一些修改导致了有形的性能改进，而其他修改则提供了独特的见解。

    In the rapidly evolving digital era, the analysis of document layouts plays a pivotal role in automated information extraction and interpretation. In our work, we have trained MViTv2 transformer model architecture with cascaded mask R-CNN on BaDLAD dataset to extract text box, paragraphs, images and tables from a document. After training on 20365 document images for 36 epochs in a 3 phase cycle, we achieved a training loss of 0.2125 and a mask loss of 0.19. Our work extends beyond training, delving into the exploration of potential enhancement avenues. We investigate the impact of rotation and flip augmentation, the effectiveness of slicing input images pre-inference, the implications of varying the resolution of the transformer backbone, and the potential of employing a dual-pass inference to uncover missed text-boxes. Through these explorations, we observe a spectrum of outcomes, where some modifications result in tangible performance improvements, while others offer unique insights 
    
[^42]: MONDEO: 多阶段僵尸网络检测

    MONDEO: Multistage Botnet Detection. (arXiv:2308.16570v1 [cs.CR])

    [http://arxiv.org/abs/2308.16570](http://arxiv.org/abs/2308.16570)

    MONDEO是一种多阶段机制，用于检测基于DNS的僵尸网络恶意软件，它轻巧且可以在手机设备中部署，通过处理数据包流进行高效的攻击识别。

    

    移动设备已广泛成为最常用的技术设备。由于其特性，它们成为僵尸网络恶意软件的主要目标。FluBot是一种感染移动设备的候选僵尸网恶意软件的例子。特别是，FluBot是一种基于DNS的僵尸网络，使用域名生成算法(DGA)与命令和控制服务器(C2)进行通信。MONDEO是一种多阶段机制，具有灵活的设计，用于检测基于DNS的僵尸网络恶意软件。MONDEO轻巧，并且可以在手机设备中部署而不需要部署软件、代理或配置，方便地集成到核心网络中。MONDEO包括四个检测阶段：黑名单/白名单，查询率分析， DGA分析和机器学习评估。它被设计用来处理数据包流以高效地识别在不同阶段中的攻击。MONDEO通过对多个数据集进行测试以衡量其效率和性能。

    Mobile devices have widespread to become the most used piece of technology. Due to their characteristics, they have become major targets for botnet-related malware. FluBot is one example of botnet malware that infects mobile devices. In particular, FluBot is a DNS-based botnet that uses Domain Generation Algorithms (DGA) to establish communication with the Command and Control Server (C2). MONDEO is a multistage mechanism with a flexible design to detect DNS-based botnet malware. MONDEO is lightweight and can be deployed without requiring the deployment of software, agents, or configuration in mobile devices, allowing easy integration in core networks. MONDEO comprises four detection stages: Blacklisting/Whitelisting, Query rate analysis, DGA analysis, and Machine learning evaluation. It was created with the goal of processing streams of packets to identify attacks with high efficiency, in the distinct phases. MONDEO was tested against several datasets to measure its efficiency and perf
    
[^43]: 使用高级机器学习模型和多变量输入预测急诊室拥挤

    Forecasting Emergency Department Crowding with Advanced Machine Learning Models and Multivariable Input. (arXiv:2308.16544v1 [cs.LG])

    [http://arxiv.org/abs/2308.16544](http://arxiv.org/abs/2308.16544)

    本研究使用高级机器学习模型和多变量输入来预测急诊室拥挤，发现N-BEATS和LightGBM相较于基准模型分别提供了11%和9%的性能改进。

    

    急诊室拥挤对患者的安全构成重大威胁，并且与增加的死亡率有关。预测未来的服务需求有潜在的患者结果。尽管对这个主题进行了积极的研究，但仍存在几个差距：1）由于快速增加的高级机器学习模型（ML），所提出的预测模型变得过时，2）多变量输入数据的量有限，3）很少报告具体的性能指标。在这项研究中，我们记录了一组先进的ML模型在预测24小时前的急诊室占用率方面的性能。我们使用一个大型综合急诊室的电子健康记录数据和一系列解释变量，包括救治区域医院的床位可用性，来自当地观测站的交通数据，天气变量等。我们展示了N-BEATS和LightGBM在11％和9％的改进中超越了基准，并且DeepAR可以预测第二天的人员状况。

    Emergency department (ED) crowding is a significant threat to patient safety and it has been repeatedly associated with increased mortality. Forecasting future service demand has the potential patient outcomes. Despite active research on the subject, several gaps remain: 1) proposed forecasting models have become outdated due to quick influx of advanced machine learning models (ML), 2) amount of multivariable input data has been limited and 3) discrete performance metrics have been rarely reported. In this study, we document the performance of a set of advanced ML models in forecasting ED occupancy 24 hours ahead. We use electronic health record data from a large, combined ED with an extensive set of explanatory variables, including the availability of beds in catchment area hospitals, traffic data from local observation stations, weather variables, etc. We show that N-BEATS and LightGBM outpeform benchmarks with 11 % and 9 % respective improvements and that DeepAR predicts next day cr
    
[^44]: 可扩展的不完整多视图聚类方法与结构对齐

    Scalable Incomplete Multi-View Clustering with Structure Alignment. (arXiv:2308.16541v1 [cs.LG])

    [http://arxiv.org/abs/2308.16541](http://arxiv.org/abs/2308.16541)

    该论文提出了一种可扩展的不完整多视图聚类方法，通过解决锚点不对齐问题和视图间的差异，实现了精确的聚类性能提升。

    

    现有的多视图聚类方法假设所有视图都是完整的，但实际上由于数据损坏或传感器故障，样本通常是部分可用的，这引发了对不完整多视图聚类的研究。虽然已经提出了一些基于锚点的不完整多视图聚类方法来处理大规模的不完整数据，但它们仍然存在以下缺点：i）大多数现有方法忽视了视图间的差异，强制要求跨视图表示一致，这会破坏模型的表示能力；ii）由于不同视图之间样本的差异，学习到的锚点可能不对齐，我们称之为不完整数据下的锚点不对齐问题（AUP-ID）。这种AUP-ID会导致图融合不准确，降低聚类性能。为了解决这些问题，我们提出了一种新颖的不完整锚点图学习框架，称为可扩展的不完整多视图聚类方法与结构对齐。

    The success of existing multi-view clustering (MVC) relies on the assumption that all views are complete. However, samples are usually partially available due to data corruption or sensor malfunction, which raises the research of incomplete multi-view clustering (IMVC). Although several anchor-based IMVC methods have been proposed to process the large-scale incomplete data, they still suffer from the following drawbacks: i) Most existing approaches neglect the inter-view discrepancy and enforce cross-view representation to be consistent, which would corrupt the representation capability of the model; ii) Due to the samples disparity between different views, the learned anchor might be misaligned, which we referred as the Anchor-Unaligned Problem for Incomplete data (AUP-ID). Such the AUP-ID would cause inaccurate graph fusion and degrades clustering performance. To tackle these issues, we propose a novel incomplete anchor graph learning framework termed Scalable Incomplete Multi-View C
    
[^45]: 关于多智能体相互作用的微分博弈、最优控制和基于能量的模型之间的联系

    On a Connection between Differential Games, Optimal Control, and Energy-based Models for Multi-Agent Interactions. (arXiv:2308.16539v1 [cs.RO])

    [http://arxiv.org/abs/2308.16539](http://arxiv.org/abs/2308.16539)

    本论文研究了微分博弈、最优控制和基于能量的模型之间的联系，并提出了基于能量的潜在博弈的新的端到端学习应用，通过神经网络和可微分的博弈论优化层的组合来提高预测性能。

    

    博弈论提供了一个可解释的数学框架，用于建模多智能体的相互作用。然而，在现实世界中的机器人应用中，博弈论的适用性受到多个挑战的阻碍，比如未知的智能体偏好和目标。为了解决这些挑战，我们展示了微分博弈、最优控制和基于能量的模型之间的联系，并展示了如何将现有方法统一到我们提出的基于能量的潜在博弈的形式化中。在这个形式化的基础上，本文引入了一种新的端到端学习应用，将神经网络用于博弈参数推断，并通过可微分的博弈论优化层作为归纳偏好。使用模拟的移动机器人行人相互作用和真实世界中的自动驾驶数据的实验证据表明，博弈论层改善了各种神经网络主干的预测性能。

    Game theory offers an interpretable mathematical framework for modeling multi-agent interactions. However, its applicability in real-world robotics applications is hindered by several challenges, such as unknown agents' preferences and goals. To address these challenges, we show a connection between differential games, optimal control, and energy-based models and demonstrate how existing approaches can be unified under our proposed Energy-based Potential Game formulation. Building upon this formulation, this work introduces a new end-to-end learning application that combines neural networks for game-parameter inference with a differentiable game-theoretic optimization layer, acting as an inductive bias. The experiments using simulated mobile robot pedestrian interactions and real-world automated driving data provide empirical evidence that the game-theoretic layer improves the predictive performance of various neural network backbones.
    
[^46]: 通过神经符号约束来调节基于评分的生成模型

    Conditioning Score-Based Generative Models by Neuro-Symbolic Constraints. (arXiv:2308.16534v1 [cs.LG])

    [http://arxiv.org/abs/2308.16534](http://arxiv.org/abs/2308.16534)

    本文提出了一种方法，通过神经符号约束来调节基于评分的生成模型，实现了在非条件生成模型下强制执行任意的逻辑约束，从而获得了一个有效的、无需额外训练的条件采样算法。

    

    基于评分和扩散模型已经成为一种有效的条件和非条件生成方法。然而，条件生成基于特定训练的条件模型或分类器指导，这需要训练一个噪声依赖的分类器，即使对于未损坏数据的分类器已经给出。我们提出了一种方法，可以从非条件评分生成模型中采样，可以强制执行任意的逻辑约束，而无需进行额外的训练。首先，我们展示了如何操纵学习得到的评分，以便在用户定义的约束条件下从非归一化分布中采样。然后，我们定义了一个灵活而数值稳定的神经符号框架，用于编码软逻辑约束。将这两个组成部分结合起来，我们获得了一个一般的但是近似的条件采样算法。我们进一步开发了有效的启发式方法来改进近似。最后，我们展示了我们方法的有效性。

    Score-based and diffusion models have emerged as effective approaches for both conditional and unconditional generation. Still conditional generation is based on either a specific training of a conditional model or classifier guidance, which requires training a noise-dependent classifier, even when the classifier for uncorrupted data is given. We propose an approach to sample from unconditional score-based generative models enforcing arbitrary logical constraints, without any additional training. Firstly, we show how to manipulate the learned score in order to sample from an un-normalized distribution conditional on a user-defined constraint. Then, we define a flexible and numerically stable neuro-symbolic framework for encoding soft logical constraints. Combining these two ingredients we obtain a general, but approximate, conditional sampling algorithm. We further developed effective heuristics aimed at improving the approximation. Finally, we show the effectiveness of our approach fo
    
[^47]: SA6D: 面向新颖和遮挡物体的自适应少样本6D姿态估计器

    SA6D: Self-Adaptive Few-Shot 6D Pose Estimator for Novel and Occluded Objects. (arXiv:2308.16528v1 [cs.CV])

    [http://arxiv.org/abs/2308.16528](http://arxiv.org/abs/2308.16528)

    SA6D是一种面向新颖和遮挡物体的自适应少样本6D姿态估计器，通过使用自适应分割模块和少量参考图像构建目标对象的点云模型，无需额外的对象信息，能在具有遮挡的复杂场景中表现出更好的性能。

    

    为了在真实环境中实现对物体的有意义的机器人操作，6D姿态估计是一个关键的方面。大多数现有方法在将预测扩展到不断引入新物体实例的场景，特别是重度遮挡的情况下存在困难。在这项工作中，我们提出了一种称为SA6D的少样本姿态估计（FSPE）方法，它使用自适应分割模块识别新颖目标对象，并使用仅有少量混杂参考图像构建目标对象的点云模型。与现有方法不同，SA6D不需要面向对象的参考图像或任何其他对象信息，使其成为一个更具通用性和可扩展性的解决方案。

    To enable meaningful robotic manipulation of objects in the real-world, 6D pose estimation is one of the critical aspects. Most existing approaches have difficulties to extend predictions to scenarios where novel object instances are continuously introduced, especially with heavy occlusions. In this work, we propose a few-shot pose estimation (FSPE) approach called SA6D, which uses a self-adaptive segmentation module to identify the novel target object and construct a point cloud model of the target object using only a small number of cluttered reference images. Unlike existing methods, SA6D does not require object-centric reference images or any additional object information, making it a more generalizable and scalable solution across categories. We evaluate SA6D on real-world tabletop object datasets and demonstrate that SA6D outperforms existing FSPE methods, particularly in cluttered scenes with occlusions, while requiring fewer reference images.
    
[^48]: 基于曲率的图神经网络中的汇聚方法研究

    Curvature-based Pooling within Graph Neural Networks. (arXiv:2308.16516v1 [cs.LG])

    [http://arxiv.org/abs/2308.16516](http://arxiv.org/abs/2308.16516)

    本研究提出了一种名为CurvPool的汇聚方法，通过利用图的曲率概念来解决过度平滑和过度压缩的问题。它能够根据曲率自适应地识别负责这两种现象的结构，并构建具有更合适结构的图，从而实现更深层模型和远距离信息的结合。

    

    过度压缩和过度平滑是限制图神经网络（GNN）能力的两个关键问题。过度平滑消除了节点之间的差异，使它们难以区分，而过度压缩指的是GNN无法在较长的距离上传播信息，因为指数级的节点状态被压缩成固定大小的表示。这两种现象具有类似的原因，都在很大程度上是由图拓扑引起的。为了解决这些问题，在图分类任务中，我们提出了一种新颖的汇聚方法CurvPool。CurvPool利用图的曲率概念自适应地识别负责过度平滑和过度压缩的结构。通过基于平衡Forman曲率对节点进行聚类，CurvPool构建了一个具有更合适结构的图，允许深层模型和远距离信息的结合。我们将其与其他最先进的汇聚方法进行比较，并确定其竞争力。

    Over-squashing and over-smoothing are two critical issues, that limit the capabilities of graph neural networks (GNNs). While over-smoothing eliminates the differences between nodes making them indistinguishable, over-squashing refers to the inability of GNNs to propagate information over long distances, as exponentially many node states are squashed into fixed-size representations. Both phenomena share similar causes, as both are largely induced by the graph topology. To mitigate these problems in graph classification tasks, we propose CurvPool, a novel pooling method. CurvPool exploits the notion of curvature of a graph to adaptively identify structures responsible for both over-smoothing and over-squashing. By clustering nodes based on the Balanced Forman curvature, CurvPool constructs a graph with a more suitable structure, allowing deeper models and the combination of distant information. We compare it to other state-of-the-art pooling approaches and establish its competitiveness 
    
[^49]: 课堂数据分析复制：教学生，同时测试科学

    In-class Data Analysis Replications: Teaching Students while Testing Science. (arXiv:2308.16491v1 [cs.CY])

    [http://arxiv.org/abs/2308.16491](http://arxiv.org/abs/2308.16491)

    这项研究揭示了课堂数据分析复制的可行性，以及这种方法对学生、教育者和科学家的成本与收益。同时，学生对数据的预期与实际情况存在差异。

    

    科学正面临可重复性危机。先前的工作提出将数据分析复制纳入课堂作为潜在解决方案。然而，尽管潜在的好处，目前尚不清楚这一方法是否可行，如果可行，涉及的利益相关者-学生、教育者和科学家-应该期望什么。学生能够在课堂上进行数据分析复制吗？教育者的成本与收益如何？这个解决方案如何帮助评估和改进科学的现状？本研究在EPFL教授的应用数据分析课程（CS-401）的项目部分中纳入了数据分析复制（N=354名学生）。在此报告中，我们基于课程期间进行的调查提前进行注册的发现。首先，我们证明学生可以复制先前发表的科学论文，大部分是定性的，有些是完全一样的。我们发现学生对数据的预期与实际情况存在差异

    Science is facing a reproducibility crisis. Previous work has proposed incorporating data analysis replications into classrooms as a potential solution. However, despite the potential benefits, it is unclear whether this approach is feasible, and if so, what the involved stakeholders-students, educators, and scientists-should expect from it. Can students perform a data analysis replication over the course of a class? What are the costs and benefits for educators? And how can this solution help benchmark and improve the state of science?  In the present study, we incorporated data analysis replications in the project component of the Applied Data Analysis course (CS-401) taught at EPFL (N=354 students). Here we report pre-registered findings based on surveys administered throughout the course. First, we demonstrate that students can replicate previously published scientific papers, most of them qualitatively and some exactly. We find discrepancies between what students expect of data an
    
[^50]: 潜在画家

    Latent Painter. (arXiv:2308.16490v1 [cs.CV])

    [http://arxiv.org/abs/2308.16490](http://arxiv.org/abs/2308.16490)

    这个论文介绍了一种名为潜在画家的技术，它利用潜在作为画布和扩散器的预测作为计划来生成绘画动画，同时还可以在不同的检查点集中转换图像。

    

    潜在扩散器在生成AI领域引起了革命，并激发了创造性艺术。在去噪潜在时，每个步骤预测的原始图像共同形成了动画。然而，动画受到扩散器去噪特性的限制，只呈现了一个锐化过程。本文介绍了潜在画家，它以潜在作为画布，以扩散器的预测作为计划，生成绘画动画。潜在画家还可以将一个生成的图像转换为另一个图像，这可以发生在两个不同检查点集中的图像之间。

    Latent diffusers revolutionized the generative AI and inspired creative art. When denoising the latent, the predicted original image at each step collectively animates the formation. However, the animation is limited by the denoising nature of the diffuser, and only renders a sharpening process. This work presents Latent Painter, which uses the latent as the canvas, and the diffuser predictions as the plan, to generate painting animation. Latent Painter also transits one generated image to another, which can happen between images from two different sets of checkpoints.
    
[^51]: 使用元学习进行点云上采样的测试时间适应

    Test-Time Adaptation for Point Cloud Upsampling Using Meta-Learning. (arXiv:2308.16484v1 [cs.CV])

    [http://arxiv.org/abs/2308.16484](http://arxiv.org/abs/2308.16484)

    本文提出了一种使用元学习进行测试时间适应的方法来增强点云上采样模型的普适性，解决了测试数据分布与训练数据不同导致性能下降的问题。

    

    廉价的3D扫描仪经常产生稀疏和非均匀的点云，这对机器人系统中的下游应用产生负面影响。虽然现有的点云上采样架构在标准基准数据上展示了有希望的结果，但当测试数据与训练数据具有不同分布时，它们往往会出现显著的性能下降。为了解决这个问题，本文提出了一种测试时间适应方法来增强点云上采样模型的普适性。所提出的方法利用元学习来显式地学习测试时间适应的网络参数。我们的方法不需要任何关于测试数据的先验信息。在元训练过程中，模型参数是从训练数据的稀疏-密集点云对的集合中学习的。在元测试过程中，经过少量梯度更新的训练模型可以产生一组唯一的网络参数。

    Affordable 3D scanners often produce sparse and non-uniform point clouds that negatively impact downstream applications in robotic systems. While existing point cloud upsampling architectures have demonstrated promising results on standard benchmarks, they tend to experience significant performance drops when the test data have different distributions from the training data. To address this issue, this paper proposes a test-time adaption approach to enhance model generality of point cloud upsampling. The proposed approach leverages meta-learning to explicitly learn network parameters for test-time adaption. Our method does not require any prior information about the test data. During meta-training, the model parameters are learned from a collection of instance-level tasks, each of which consists of a sparse-dense pair of point clouds from the training data. During meta-testing, the trained model is fine-tuned with a few gradient updates to produce a unique set of network parameters for
    
[^52]: 具有集成离群检测的超声心动图视图分类，以增强自动超声心动图分析

    Echocardiographic View Classification with Integrated Out-of-Distribution Detection for Enhanced Automatic Echocardiographic Analysis. (arXiv:2308.16483v1 [eess.SP])

    [http://arxiv.org/abs/2308.16483](http://arxiv.org/abs/2308.16483)

    ECHO-VICODE是一个深度学习框架，通过训练来分类超声心动图的31个视图类别，并具有集成的离群检测功能，可以显著降低超声心动图中的错误可能性。

    

    在快速发展的自动超声心动图分析和解释领域中，自动视图分类是一个关键但具有挑战性的任务，这是由于超声心动图数据的固有复杂性和可变性。本研究提出了ECHO-VICODE（超声心动图视图分类与离群检测），这是一个基于深度学习的新型框架，通过训练来分类31个类别，超过了先前的研究，并展示了其处理多种超声心动图视图的能力。此外，ECHO-VICODE还加入了一个集成的离群检测功能，利用相对马氏距离有效识别常见的“接近离群”实例。通过大量实验，我们展示了ECHO-VICODE在视图分类和离群检测方面的出色性能，显著降低了超声心动图中潜在错误的可能性。

    In the rapidly evolving field of automatic echocardiographic analysis and interpretation, automatic view classification is a critical yet challenging task, owing to the inherent complexity and variability of echocardiographic data. This study presents ECHOcardiography VIew Classification with Out-of-Distribution dEtection (ECHO-VICODE), a novel deep learning-based framework that effectively addresses this challenge by training to classify 31 classes, surpassing previous studies and demonstrating its capacity to handle a wide range of echocardiographic views. Furthermore, ECHO-VICODE incorporates an integrated out-of-distribution (OOD) detection function, leveraging the relative Mahalanobis distance to effectively identify 'near-OOD' instances commonly encountered in echocardiographic data. Through extensive experimentation, we demonstrated the outstanding performance of ECHO-VICODE in terms of view classification and OOD detection, significantly reducing the potential for errors in ech
    
[^53]: Point-TTA: 使用多任务元辅助学习的点云配准测试时间自适应

    Point-TTA: Test-Time Adaptation for Point Cloud Registration Using Multitask Meta-Auxiliary Learning. (arXiv:2308.16481v1 [cs.CV])

    [http://arxiv.org/abs/2308.16481](http://arxiv.org/abs/2308.16481)

    Point-TTA是一种通过多任务元辅助学习实现的点云配准测试时自适应框架，能够提高配准模型的泛化性能。

    

    我们提出了Point-TTA，这是一种新颖的点云配准测试时间自适应框架，可以提高配准模型的泛化性能。虽然基于学习的方法取得了令人印象深刻的进展，但面对未知的测试环境的泛化仍然是一个重大挑战，原因是3D扫描的变化较大。现有方法通常训练一个通用模型，并在每个实例上应用相同的训练模型。这可能是次优的，因为同一模型很难处理测试期间的所有变化。在本文中，我们提出了一种用于点云配准的测试时间自适应方法。我们的模型可以适应测试时未知的分布，无需任何关于测试数据的先验知识。具体地，我们设计了三个自监督辅助任务，这些任务与主要的配准任务一起进行优化。给定一个测试实例，我们使用这些辅助任务来调整我们的模型，并使用更新后的模型进行推断。

    We present Point-TTA, a novel test-time adaptation framework for point cloud registration (PCR) that improves the generalization and the performance of registration models. While learning-based approaches have achieved impressive progress, generalization to unknown testing environments remains a major challenge due to the variations in 3D scans. Existing methods typically train a generic model and the same trained model is applied on each instance during testing. This could be sub-optimal since it is difficult for the same model to handle all the variations during testing. In this paper, we propose a test-time adaptation approach for PCR. Our model can adapt to unseen distributions at test-time without requiring any prior knowledge of the test data. Concretely, we design three self-supervised auxiliary tasks that are optimized jointly with the primary PCR task. Given a test instance, we adapt our model using these auxiliary tasks and the updated model is used to perform the inference. 
    
[^54]: 一种适用于隐式多任务强化学习问题的策略适应方法

    A Policy Adaptation Method for Implicit Multitask Reinforcement Learning Problems. (arXiv:2308.16471v1 [cs.RO])

    [http://arxiv.org/abs/2308.16471](http://arxiv.org/abs/2308.16471)

    本研究提出了一种适用于动态运动生成任务的多任务强化学习算法，可用于适应单个运动类别中的隐式变化，并在头球任务中取得良好的适应效果。

    

    在动态运动生成任务中，包括接触和碰撞，策略参数的小改变可能导致极其不同的回报。例如，在足球中，通过稍微改变踢球位置或施加球的力或者球的摩擦力发生变化，球可以以完全不同的方向飞行。然而，很难想象在不同的方向上头球需要完全不同的技能。在本研究中，我们提出了一种多任务强化学习算法，用于在单个运动类别中适应目标或环境的隐式变化，包括不同的奖励函数或环境的物理参数。我们利用单脚机器人模型对所提出的方法进行了评估，在头球任务中取得了良好的适应效果。结果表明，所提出的方法可以适应目标位置的隐式变化或球的恢复系数的变化，而标准的领域随机化方法则不能。

    In dynamic motion generation tasks, including contact and collisions, small changes in policy parameters can lead to extremely different returns. For example, in soccer, the ball can fly in completely different directions with a similar heading motion by slightly changing the hitting position or the force applied to the ball or when the friction of the ball varies. However, it is difficult to imagine that completely different skills are needed for heading a ball in different directions. In this study, we proposed a multitask reinforcement learning algorithm for adapting a policy to implicit changes in goals or environments in a single motion category with different reward functions or physical parameters of the environment. We evaluated the proposed method on the ball heading task using a monopod robot model. The results showed that the proposed method can adapt to implicit changes in the goal positions or the coefficients of restitution of the ball, whereas the standard domain randomi
    
[^55]: 基于领域自适应的消息传递图神经网络

    Domain-adaptive Message Passing Graph Neural Network. (arXiv:2308.16470v1 [cs.LG])

    [http://arxiv.org/abs/2308.16470](http://arxiv.org/abs/2308.16470)

    本论文提出了一个基于领域自适应的消息传递图神经网络(DM-GNN)，用于解决跨网络节点分类问题。该方法通过结合图神经网络和条件对抗领域自适应，能够学习可传递的节点分类信息。具体而言，通过双特征提取器构建GNN编码器，同时利用标签传播节点分类器和标签感知的传播方案来提高节点分类的准确性和泛化性能。

    

    近来，跨网络节点分类(CNNC)吸引了越来越多的关注，旨在通过从具有丰富标签的源网络中转移知识，对标签不充分的目标网络中的节点进行分类。为了解决CNNC问题，我们提出了一种结合了图神经网络(GNN)和条件对抗领域自适应的领域自适应消息传递图神经网络(DM-GNN)，能够学习可传递的节点分类信息。首先，通过双特征提取器构建了GNN编码器，将自我嵌入学习与邻居嵌入学习分离，以共同捕捉连接节点之间的共性和差异。其次，提出了一种标签传播节点分类器，通过将节点自身预测和邻居预测相结合来细化每个节点的标签预测。此外，针对有标签源网络设计了标签感知的传播方案，以促进标签的传播。

    Cross-network node classification (CNNC), which aims to classify nodes in a label-deficient target network by transferring the knowledge from a source network with abundant labels, draws increasing attention recently. To address CNNC, we propose a domain-adaptive message passing graph neural network (DM-GNN), which integrates graph neural network (GNN) with conditional adversarial domain adaptation. DM-GNN is capable of learning informative representations for node classification that are also transferrable across networks. Firstly, a GNN encoder is constructed by dual feature extractors to separate ego-embedding learning from neighbor-embedding learning so as to jointly capture commonality and discrimination between connected nodes. Secondly, a label propagation node classifier is proposed to refine each node's label prediction by combining its own prediction and its neighbors' prediction. In addition, a label-aware propagation scheme is devised for the labeled source network to promo
    
[^56]: 使用规范化流计算分子的激发态

    Computing excited states of molecules using normalizing flows. (arXiv:2308.16468v1 [physics.chem-ph])

    [http://arxiv.org/abs/2308.16468](http://arxiv.org/abs/2308.16468)

    使用规范化流计算分子的激发态，通过逼近波函数并优化基函数的线性空间内的近似。该方法在计算量子系统中取得了准确和有效的结果，并在能量预测准确性和基组收敛速度方面进行了显著改善。

    

    我们提出了一种新的非线性变分框架，可以同时计算量子系统的基态和激发态。我们的方法基于通过与规范化流的组合来逼近波函数，这些波函数位于基函数的线性空间中，并对其进行优化。我们通过计算三原子H$_2$S分子的大量振动态以及典型的单电子系统（包括氢原子、分子氢离子和碳原子在单激发电子近似下的基态和多个激发态）来证明我们方法的准确性和效率。结果表明，即使使用参数较少的规范化流，能量预测的准确性和基组收敛速度也有显著改善。该方法也可以被看作是对最佳捕捉底层物理的一组内禀坐标进行优化的过程。

    We present a new nonlinear variational framework for simultaneously computing ground and excited states of quantum systems. Our approach is based on approximating wavefunctions in the linear span of basis functions that are augmented and optimized \emph{via} composition with normalizing flows. The accuracy and efficiency of our approach are demonstrated in the calculations of a large number of vibrational states of the triatomic H$_2$S molecule as well as ground and several excited electronic states of prototypical one-electron systems including the hydrogen atom, the molecular hydrogen ion, and a carbon atom in a single-active-electron approximation. The results demonstrate significant improvements in the accuracy of energy predictions and accelerated basis-set convergence even when using normalizing flows with a small number of parameters. The present approach can be also seen as the optimization of a set of intrinsic coordinates that best capture the underlying physics within the gi
    
[^57]: BioCoder: 一种带有上下文语用知识的生物信息学代码生成基准

    BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge. (arXiv:2308.16458v1 [cs.LG])

    [http://arxiv.org/abs/2308.16458](http://arxiv.org/abs/2308.16458)

    BioCoder是一个用于评估预训练模型在生成生物信息学代码方面的基准，涵盖了函数代码生成中的包依赖关系、类声明和全局变量，并通过模糊测试框架进行评估。

    

    预训练的语言模型（如ChatGPT）显著改进了代码生成。随着这些模型的扩大，需要输出来处理更复杂的任务的需求也越来越多。此外，在生物信息学中，生成功能程序由于领域知识量大、需要复杂的数据操作和复杂的功能依赖关系而面临额外的挑战。在这里，我们介绍了BioCoder，这是一个用于评估现有预训练模型在生成生物信息学代码方面的基准。与函数代码生成有关，BioCoder涵盖了可能的包依赖关系、类声明和全局变量。它包括来自GitHub的1026个Python和Java函数和1243个方法，以及来自Rosalind项目的253个示例。BioCoder还结合了一个用于评估的模糊测试框架，我们已经应用它来评估许多模型，包括InCoder、CodeGen、CodeGen2、SantaCoder、StarCoder、StarCoder+、InstructCodeT。

    Pre-trained language models like ChatGPT have significantly improved code generation. As these models scale up, there is an increasing need for the output to handle more intricate tasks. Moreover, in bioinformatics, generating functional programs poses additional notable challenges due to the amount of domain knowledge, the need for complicated data operations, and intricate functional dependencies between the operations. Here, we present BioCoder, a benchmark developed to evaluate existing pre-trained models in generating bioinformatics code. In relation to function-code generation, BioCoder covers potential package dependencies, class declarations, and global variables. It incorporates 1026 functions and 1243 methods in Python and Java from GitHub and 253 examples from the Rosalind Project. BioCoder incorporates a fuzz-testing framework for evaluation, and we have applied it to evaluate many models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT
    
[^58]: 最小二乘法最大化和加权泛化记忆机

    Least Squares Maximum and Weighted Generalization-Memorization Machines. (arXiv:2308.16456v1 [stat.ML])

    [http://arxiv.org/abs/2308.16456](http://arxiv.org/abs/2308.16456)

    本文提出了一种记忆影响机制用于最小二乘支持向量机，能够准确划分训练集并避免过拟合，提出的最大记忆影响模型和加权记忆影响模型在泛化性能和时间成本方面具有优势。

    

    本文提出了一种用于最小二乘支持向量机（LSSVM）的记忆影响机制，实现记忆的新方法。在不改变原始LSSVM方程约束的情况下，该机制能够准确地对训练集进行划分，避免过拟合。然后提出了最大记忆影响模型（MIMM）和加权记忆影响模型（WIMM）。实验证明这些模型可以退化为LSSVM。此外，我们还为MIMM和WIMM提出了一些不同的记忆影响函数。实验结果表明，相比于LSSVM，我们的MIMM和WIMM在泛化性能上有更好的表现，并且在时间成本上比其他记忆模型具有明显优势。

    In this paper, we propose a new way of remembering by introducing a memory influence mechanism for the least squares support vector machine (LSSVM). Without changing the equation constraints of the original LSSVM, this mechanism, allows an accurate partitioning of the training set without overfitting. The maximum memory impact model (MIMM) and the weighted impact memory model (WIMM) are then proposed. It is demonstrated that these models can be degraded to the LSSVM. Furthermore, we propose some different memory impact functions for the MIMM and WIMM. The experimental results show that that our MIMM and WIMM have better generalization performance compared to the LSSVM and significant advantage in time cost compared to other memory models.
    
[^59]: 通过隐含表示约束的对抗微调来减少准确性与鲁棒性的权衡

    Adversarial Finetuning with Latent Representation Constraint to Mitigate Accuracy-Robustness Tradeoff. (arXiv:2308.16454v1 [cs.CV])

    [http://arxiv.org/abs/2308.16454](http://arxiv.org/abs/2308.16454)

    本文提出了一种名为ARREST的新型对抗训练方法，通过对抗微调、基于表示的知识蒸馏和嘈杂重播来减少标准准确性和对抗鲁棒性之间的权衡问题。

    

    本文针对深度神经网络中干净样本的标准准确性和对抗样本的鲁棒性之间的权衡问题提出了一个解决方案。尽管对抗训练可以提高网络的鲁棒性，但会降低其标准准确性，从而产生权衡问题。为了减轻这种权衡，我们提出了一种新的对抗训练方法ARREST，包括三个组成部分：（i）对抗微调（AFT），（ii）基于表示的知识蒸馏（RGKD），和（iii）嘈杂重播（NR）。AFT通过将参数初始化为在干净样本上标准预训练的DNN，对DNN进行对抗样本的训练。RGKD和NR分别利用正则化项和算法在AFT期间保留干净样本的隐含表示。RGKD惩罚标准预训练和AFT DNN之间的表示距离。当AFT期间表示发生显著变化时，NR将输入对抗样本切换为非对抗样本。通过组合这些组件，我们实现了同时提高标准准确性和对抗鲁棒性的效果。

    This paper addresses the tradeoff between standard accuracy on clean examples and robustness against adversarial examples in deep neural networks (DNNs). Although adversarial training (AT) improves robustness, it degrades the standard accuracy, thus yielding the tradeoff. To mitigate this tradeoff, we propose a novel AT method called ARREST, which comprises three components: (i) adversarial finetuning (AFT), (ii) representation-guided knowledge distillation (RGKD), and (iii) noisy replay (NR). AFT trains a DNN on adversarial examples by initializing its parameters with a DNN that is standardly pretrained on clean examples. RGKD and NR respectively entail a regularization term and an algorithm to preserve latent representations of clean examples during AFT. RGKD penalizes the distance between the representations of the standardly pretrained and AFT DNNs. NR switches input adversarial examples to nonadversarial ones when the representation changes significantly during AFT. By combining t
    
[^60]: 听取少数群体的声音：基于对比预训练的不平衡类别加密流量分类

    Listen to Minority: Encrypted Traffic Classification for Class Imbalance with Contrastive Pre-Training. (arXiv:2308.16453v1 [cs.CR])

    [http://arxiv.org/abs/2308.16453](http://arxiv.org/abs/2308.16453)

    本文提出了一种基于对比预训练的不平衡类别加密流量分类框架PASS，通过重新采样训练数据集并进行对比性预训练，避免了标签偏见和流量均匀性等问题，为加密流量分类提供了稳健的特征表示。

    

    移动互联网在各个方面深刻地改变了现代生活方式。加密流量分类在管理移动互联网中起到了至关重要的作用，特别是在移动应用程序使用加密通信的爆炸性增长的情况下。尽管一些现有的基于学习的加密流量分类方法显示出了有希望的结果，但在真实网络环境中仍然存在三个限制：1）由流量类别不平衡引起的标签偏见，2）由组件共享引起的流量均匀性，以及3）依赖充足标记流量进行训练。没有任何现有的加密流量分类方法能够解决所有这些限制。本文提出了一种新颖的基于预训练的半监督加密流量分类框架，称为PASS。我们的关键见解是重新采样原始的训练数据集，并进行对比性预训练，而不直接使用个体应用程序标签，以避免由于类别不平衡引起的标签偏见问题，同时获得稳健的特征表示来区分重叠的流量。

    Mobile Internet has profoundly reshaped modern lifestyles in various aspects. Encrypted Traffic Classification (ETC) naturally plays a crucial role in managing mobile Internet, especially with the explosive growth of mobile apps using encrypted communication. Despite some existing learning-based ETC methods showing promising results, three-fold limitations still remain in real-world network environments, 1) label bias caused by traffic class imbalance, 2) traffic homogeneity caused by component sharing, and 3) training with reliance on sufficient labeled traffic. None of the existing ETC methods can address all these limitations. In this paper, we propose a novel Pre-trAining Semi-Supervised ETC framework, dubbed PASS. Our key insight is to resample the original train dataset and perform contrastive pre-training without using individual app labels directly to avoid label bias issues caused by class imbalance, while obtaining a robust feature representation to differentiate overlapping 
    
[^61]: AntM$^{2}$C：一个用于多场景多模态点击率预测的大规模数据集

    AntM$^{2}$C: A Large Scale Dataset For Multi-Scenario Multi-Modal CTR Prediction. (arXiv:2308.16437v1 [cs.IR])

    [http://arxiv.org/abs/2308.16437](http://arxiv.org/abs/2308.16437)

    本研究提出了一个名为AntM$^{2}$C的大规模数据集，用于多场景多模态点击率预测。该数据集弥补了现有数据集的限制，包括多个场景中不同类型项目的建模以及多模态特征的缺乏。它将为模型的可靠评估提供更全面的性能差异。

    

    点击率（CTR）预测在推荐系统中是一个关键问题。出现了各种公开的CTR数据集。然而，现有数据集主要存在以下限制。首先，用户通常会从多个场景中点击不同类型的项目，从多个场景建模可以更全面地了解用户。现有数据集只包括来自单个场景的相同类型项目的数据。其次，多模态特征在多场景预测中是必不可少的，因为它们解决了不同场景之间不一致的ID编码问题。现有数据集基于ID特征，缺乏多模态特征。第三，大规模数据集可以提供更可靠的模型评估，充分反映模型之间的性能差异。现有数据集的规模约为1亿，与现实世界的CTR预测相比相对较小。为了解决这些限制

    Click-through rate (CTR) prediction is a crucial issue in recommendation systems. There has been an emergence of various public CTR datasets. However, existing datasets primarily suffer from the following limitations. Firstly, users generally click different types of items from multiple scenarios, and modeling from multiple scenarios can provide a more comprehensive understanding of users. Existing datasets only include data for the same type of items from a single scenario. Secondly, multi-modal features are essential in multi-scenario prediction as they address the issue of inconsistent ID encoding between different scenarios. The existing datasets are based on ID features and lack multi-modal features. Third, a large-scale dataset can provide a more reliable evaluation of models, fully reflecting the performance differences between models. The scale of existing datasets is around 100 million, which is relatively small compared to the real-world CTR prediction. To address these limit
    
[^62]: 隐式神经网络与显式神经网络的等价性：高维视角

    On the Equivalence between Implicit and Explicit Neural Networks: A High-dimensional Viewpoint. (arXiv:2308.16425v1 [cs.LG])

    [http://arxiv.org/abs/2308.16425](http://arxiv.org/abs/2308.16425)

    本文研究了高维隐式神经网络，提供了共轭核和神经切向核的高维等价表达，并在高维空间建立了隐式网络和显式网络的等价性。

    

    隐式神经网络在各种任务中取得了显著的成功。然而，对于隐式网络和显式网络之间的连接和差异缺乏理论分析。本文研究了高维隐式神经网络，并为对应的共轭核和神经切向核提供了高维等价表达。基于此，我们在高维空间建立了隐式网络和显式网络的等价性。

    Implicit neural networks have demonstrated remarkable success in various tasks. However, there is a lack of theoretical analysis of the connections and differences between implicit and explicit networks. In this paper, we study high-dimensional implicit neural networks and provide the high dimensional equivalents for the corresponding conjugate kernels and neural tangent kernels. Built upon this, we establish the equivalence between implicit and explicit networks in high dimensions.
    
[^63]: DECODE: 用于检测极端质量比激发的扩张卷积神经网络

    DECODE: DilatEd COnvolutional neural network for Detecting Extreme-mass-ratio inspirals. (arXiv:2308.16422v1 [astro-ph.IM])

    [http://arxiv.org/abs/2308.16422](http://arxiv.org/abs/2308.16422)

    该论文介绍了一种名为DECODE的扩张卷积神经网络模型，用于检测极端质量比激发的信号。该模型通过在频域进行序列建模，并考虑时间延迟干涉仪以处理多通道TDI数据。

    

    由于其复杂的波形、持久的持续时间和低信噪比，极端质量比激发(EMRI)的检测是复杂的，这使得它们与紧凑的二进制融合相比更难被识别。虽然基于匹配滤波的技术以其计算要求而闻名，但现有的基于深度学习的方法主要处理时域数据，并且通常受到数据持续时间和信噪比的限制。此外，大多数现有工作忽略了时间延迟干涉仪(TDI)并在探测器响应计算中应用了长波近似，从而限制了其处理激光频率噪声的能力。在这项研究中，我们介绍了DECODE，这是一个以频域序列建模为重点的端到端模型，用于EMRI信号检测。DECODE围绕着一个以扩张因果卷积神经网络为中心，使用考虑到TDI-1.5探测器响应的合成数据进行训练，可以高效地处理一年的多通道TDI数据。

    The detection of Extreme Mass Ratio Inspirals (EMRIs) is intricate due to their complex waveforms, extended duration, and low signal-to-noise ratio (SNR), making them more challenging to be identified compared to compact binary coalescences. While matched filtering-based techniques are known for their computational demands, existing deep learning-based methods primarily handle time-domain data and are often constrained by data duration and SNR. In addition, most existing work ignores time-delay interferometry (TDI) and applies the long-wavelength approximation in detector response calculations, thus limiting their ability to handle laser frequency noise. In this study, we introduce DECODE, an end-to-end model focusing on EMRI signal detection by sequence modeling in the frequency domain. Centered around a dilated causal convolutional neural network, trained on synthetic data considering TDI-1.5 detector response, DECODE can efficiently process a year's worth of multichannel TDI data wi
    
[^64]: CktGNN：用于电子设计自动化的电路图神经网络

    CktGNN: Circuit Graph Neural Network for Electronic Design Automation. (arXiv:2308.16406v1 [cs.LG])

    [http://arxiv.org/abs/2308.16406](http://arxiv.org/abs/2308.16406)

    本文提出了一种名为CktGNN的电路图神经网络，通过识别电路的图形特性，同时自动化电路拓扑生成和器件尺寸调整。它使用两级GNN框架对电路图进行编码，并在设计效率上取得了显著的提升。

    

    由于巨大的设计空间和复杂的设计权衡，模拟电路的电子设计自动化一直是集成电路领域的一个长期挑战。在过去的几十年中，人们大多数关注于在给定电路拓扑的情况下自动调整晶体管尺寸。通过识别电路的图形特性，本文提出了一种电路图神经网络(CktGNN)，它基于编码器依赖的优化子程序，同时自动化电路拓扑生成和器件尺寸调整。特别是，CktGNN使用两级GNN框架（嵌套GNN）对电路图进行编码，其中电路表示为已知子图基础上的子图组合。通过这种方式，它通过减少子图数量来显著提高设计效率以执行消息传递。

    The electronic design automation of analog circuits has been a longstanding challenge in the integrated circuit field due to the huge design space and complex design trade-offs among circuit specifications. In the past decades, intensive research efforts have mostly been paid to automate the transistor sizing with a given circuit topology. By recognizing the graph nature of circuits, this paper presents a Circuit Graph Neural Network (CktGNN) that simultaneously automates the circuit topology generation and device sizing based on the encoder-dependent optimization subroutines. Particularly, CktGNN encodes circuit graphs using a two-level GNN framework (of nested GNN) where circuits are represented as combinations of subgraphs in a known subgraph basis. In this way, it significantly improves design efficiency by reducing the number of subgraphs to perform message passing. Nonetheless, another critical roadblock to advancing learning-assisted circuit design automation is a lack of public
    
[^65]: 在图嵌入中平衡局部和全局结构（LGS）的方法

    Balancing between the Local and Global Structures (LGS) in Graph Embedding. (arXiv:2308.16403v1 [cs.HC])

    [http://arxiv.org/abs/2308.16403](http://arxiv.org/abs/2308.16403)

    本论文提出了一种在图嵌入中平衡局部和全局结构的方法，通过可调参数进行调节。我们的结果表明，该方法在合成和真实世界的数据集上与最先进的方法相竞争，并引入了一种新颖的质量指标，聚类距离保持，用于评估嵌入的质量。

    

    我们提出了一种通过可调参数在图嵌入中平衡局部和全局结构（LGS）的方法。一些嵌入方法旨在捕捉全局结构，而其他方法则试图保留局部邻域。很少有方法尝试同时做到这两点，并且在二维空间中很难很好地捕捉到局部和全局信息，而大部分图的绘制都是在这个空间中进行的。根据任务和底层数据的结构选择使用局部还是全局嵌入来进行可视化不仅取决于任务本身，还取决于底层数据的结构，而这是事先不知道的。对于给定的图，LGS旨在找到一个良好的平衡，以保留局部和全局结构。我们用合成和真实世界的数据集评估了LGS的性能，并且我们的结果表明，它在使用了诸如压力和邻域保持等已建立的质量指标时与最先进的方法相竞争。我们引入了一种新颖的质量指标，聚类距离保持，用于评估嵌入的质量。

    We present a method for balancing between the Local and Global Structures (LGS) in graph embedding, via a tunable parameter. Some embedding methods aim to capture global structures, while others attempt to preserve local neighborhoods. Few methods attempt to do both, and it is not always possible to capture well both local and global information in two dimensions, which is where most graph drawing live. The choice of using a local or a global embedding for visualization depends not only on the task but also on the structure of the underlying data, which may not be known in advance. For a given graph, LGS aims to find a good balance between the local and global structure to preserve. We evaluate the performance of LGS with synthetic and real-world datasets and our results indicate that it is competitive with the state-of-the-art methods, using established quality metrics such as stress and neighborhood preservation. We introduce a novel quality metric, cluster distance preservation, to 
    
[^66]: 提高以太坊上庞氏骗局检测的鲁棒性和准确性的方法

    Improving Robustness and Accuracy of Ponzi Scheme Detection on Ethereum Using Time-Dependent Features. (arXiv:2308.16391v1 [cs.CR])

    [http://arxiv.org/abs/2308.16391](http://arxiv.org/abs/2308.16391)

    这篇论文提出了一种基于交易的方法来提高以太坊上庞氏骗局的检测鲁棒性和准确性。现有的方法主要基于智能合约源代码或操作码进行检测，但缺乏鲁棒性。通过分析交易数据，可以更有效地识别庞氏骗局，因为交易更难伪装。

    

    区块链的快速发展导致越来越多的资金涌入加密货币市场，也吸引了近年来网络犯罪分子的兴趣。庞氏骗局作为一种老式的欺诈行为，现在也流行于区块链上，给许多加密货币投资者造成了巨大的财务损失。现有文献中已经提出了一些庞氏骗局检测方法，其中大多数是基于智能合约的源代码或操作码进行检测的。虽然基于合约代码的方法在准确性方面表现出色，但它缺乏鲁棒性：首先，大部分以太坊上的合约源代码并不公开可用；其次，庞氏骗局开发者可以通过混淆操作码或者创造新的分配逻辑来欺骗基于合约代码的检测模型（因为这些模型仅在现有的庞氏逻辑上进行训练）。基于交易的方法可以提高检测的鲁棒性，因为与智能合约不同，交易更加难以伪装。

    The rapid development of blockchain has led to more and more funding pouring into the cryptocurrency market, which also attracted cybercriminals' interest in recent years. The Ponzi scheme, an old-fashioned fraud, is now popular on the blockchain, causing considerable financial losses to many crypto-investors. A few Ponzi detection methods have been proposed in the literature, most of which detect a Ponzi scheme based on its smart contract source code or opcode. The contract-code-based approach, while achieving very high accuracy, is not robust: first, the source codes of a majority of contracts on Ethereum are not available, and second, a Ponzi developer can fool a contract-code-based detection model by obfuscating the opcode or inventing a new profit distribution logic that cannot be detected (since these models were trained on existing Ponzi logics only). A transaction-based approach could improve the robustness of detection because transactions, unlike smart contracts, are harder t
    
[^67]: BenchTemp: 用于评估时间图神经网络的通用基准

    BenchTemp: A General Benchmark for Evaluating Temporal Graph Neural Networks. (arXiv:2308.16385v1 [cs.LG])

    [http://arxiv.org/abs/2308.16385](http://arxiv.org/abs/2308.16385)

    BenchTemp是一个通用基准，用于评估时间图神经网络（TGNN）模型在不同工作负载上的表现。BenchTemp提供一组基准数据集和一个标准流程，用于公平比较不同的TGNN模型。通过BenchTemp，我们对不同任务和设置下的代表性TGNN模型进行了广泛比较。

    

    为了处理特征或连接在演化的图中的时间，已经提出了一系列的时间图神经网络（TGNNs）。尽管这些TGNN取得了成功，但以往的TGNN评估揭示了四个关键问题上的几个限制：1）数据集不一致，2）评估流程不一致，3）缺乏工作负载多样性，4）缺乏有效的比较。总的来说，缺乏一个将TGNN模型放在同一起跑线上并全面比较它们的实证研究。为此，我们提出了BenchTemp，一个用于在各种工作负载上评估TGNN模型的通用基准。BenchTemp提供一组基准数据集，以便可以公平地比较不同的TGNN模型。此外，BenchTemp还设计了一个标准流程，统一了TGNN的评估。借助BenchTemp，我们广泛比较了不同任务（例如链接预测和节点分类）和设置（传递和归纳）下的代表性TGNN模型。

    To handle graphs in which features or connectivities are evolving over time, a series of temporal graph neural networks (TGNNs) have been proposed. Despite the success of these TGNNs, the previous TGNN evaluations reveal several limitations regarding four critical issues: 1) inconsistent datasets, 2) inconsistent evaluation pipelines, 3) lacking workload diversity, and 4) lacking efficient comparison. Overall, there lacks an empirical study that puts TGNN models onto the same ground and compares them comprehensively. To this end, we propose BenchTemp, a general benchmark for evaluating TGNN models on various workloads. BenchTemp provides a set of benchmark datasets so that different TGNN models can be fairly compared. Further, BenchTemp engineers a standard pipeline that unifies the TGNN evaluation. With BenchTemp, we extensively compare the representative TGNN models on different tasks (e.g., link prediction and node classification) and settings (transductive and inductive), w.r.t. bo
    
[^68]: 离线强化学习的多目标决策Transformer

    Multi-Objective Decision Transformers for Offline Reinforcement Learning. (arXiv:2308.16379v1 [cs.LG])

    [http://arxiv.org/abs/2308.16379](http://arxiv.org/abs/2308.16379)

    这项研究将离线强化学习问题视为多目标优化问题，通过扩展预测到状态和回报，解决了传统单任务学习方法中 Transformer 模型注意力机制的局限性，并指出了轨迹表示中可能存在的缺陷。

    

    离线强化学习 (RL) 结构化地从静态轨迹数据中提取策略，而无需实时环境交互。最近的研究表明，将离线 RL 架构化为序列建模任务是可行的，其中唯一的目标是使用 Transformer 架构基于先前上下文预测动作。然而，这种单任务学习方法的局限性在于可能削弱 Transformer 模型的注意力机制，这理想情况下应该在输入上下文中为最佳预测分配不同的注意力权重。为了解决这个问题，我们将离线 RL 重新构造为多目标优化问题，其中预测扩展到状态和回报。我们还强调了用于序列建模的轨迹表示中可能存在的一个缺陷，这可能在建模状态和回报分布时产生不准确性。这是由于操作分布的不平滑性造成的。

    Offline Reinforcement Learning (RL) is structured to derive policies from static trajectory data without requiring real-time environment interactions. Recent studies have shown the feasibility of framing offline RL as a sequence modeling task, where the sole aim is to predict actions based on prior context using the transformer architecture. However, the limitation of this single task learning approach is its potential to undermine the transformer model's attention mechanism, which should ideally allocate varying attention weights across different tokens in the input context for optimal prediction. To address this, we reformulate offline RL as a multi-objective optimization problem, where the prediction is extended to states and returns. We also highlight a potential flaw in the trajectory representation used for sequence modeling, which could generate inaccuracies when modeling the state and return distributions. This is due to the non-smoothness of the action distribution within the 
    
[^69]: 图神经网络中的隐私调查：攻击、保护和应用

    A Survey on Privacy in Graph Neural Networks: Attacks, Preservation, and Applications. (arXiv:2308.16375v1 [cs.LG])

    [http://arxiv.org/abs/2308.16375](http://arxiv.org/abs/2308.16375)

    这篇综述调查了图神经网络中的隐私问题，包括攻击、保护方法以及应用领域。研究人员着重总结了攻击类型、隐私保护技术分类以及可用于分析和解决GNNs中隐私问题的数据集和应用，同时提出了未来研究的方向，以构建更好的隐私保护GNNs。

    

    随着处理图结构数据的能力和实际应用的改善，图神经网络（GNNs）引起了人们的极大关注。然而，许多这些模型优先考虑高效能表现，如准确性，而缺乏隐私考虑，这是现代社会隐私攻击盛行的重要问题。为了解决这个问题，研究人员开始开发保护隐私的GNNs。尽管取得了进展，但在图领域缺乏对攻击和隐私保护技术的综合概述。在本调查中，我们旨在通过总结针对图数据的攻击、对GNNs中的隐私保护技术进行分类以及审查可用于分析/解决GNNs中隐私问题的数据集和应用程序，填补这一空白。我们还概述了未来研究的潜在方向，以建立更好的隐私保护GNNs。

    Graph Neural Networks (GNNs) have gained significant attention owing to their ability to handle graph-structured data and the improvement in practical applications. However, many of these models prioritize high utility performance, such as accuracy, with a lack of privacy consideration, which is a major concern in modern society where privacy attacks are rampant. To address this issue, researchers have started to develop privacy-preserving GNNs. Despite this progress, there is a lack of a comprehensive overview of the attacks and the techniques for preserving privacy in the graph domain. In this survey, we aim to address this gap by summarizing the attacks on graph data according to the targeted information, categorizing the privacy preservation techniques in GNNs, and reviewing the datasets and applications that could be used for analyzing/solving privacy issues in GNNs. We also outline potential directions for future research in order to build better privacy-preserving GNNs.
    
[^70]: SARATHI：通过与分块预填充解码相结合的方式实现高效的LLM推断

    SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills. (arXiv:2308.16369v1 [cs.LG])

    [http://arxiv.org/abs/2308.16369](http://arxiv.org/abs/2308.16369)

    SARATHI通过使用分块预填充和解码最大批处理的方式，高效地处理LLM推断中的预填充和解码阶段的不平衡问题，提高推断效率。

    

    大型语言模型（LLM）推断包括两个不同的阶段：预填充阶段处理输入提示，解码阶段按自回归方式生成输出令牌。虽然预填充阶段在小批量大小下有效利用了GPU计算能力，但解码阶段每次请求只生成一个令牌，导致计算利用率较低。使用管道并行时，预填充和解码时间的变化还会导致微批量之间的不平衡，进一步增加了效率上的损失。我们提出了SARATHI来应对这些挑战。SARATHI使用分块预填充，将预填充请求分成相等大小的块，并使用解码最大批处理将一个预填充块构造成一个批次，并用解码填充其余的插槽。在推断过程中，预填充块可充分利用GPU计算能力，而解码请求则“搭便车”，成本比仅解码的批次低一个数量级。分块预填充使得约束适应了GPU和CPU之间的差异，并提高了推断的效率。

    Large Language Model (LLM) inference consists of two distinct phases prefill phase which processes the input prompt and decode phase which generates output tokens autoregressively. While the prefill phase effectively saturates GPU compute at small batch sizes, the decode phase results in low compute utilization as it generates one token at a time per request. The varying prefill and decode times also lead to imbalance across micro-batches when using pipeline parallelism, resulting in further inefficiency due to bubbles.  We present SARATHI to address these challenges. SARATHI employs chunked-prefills, which splits a prefill request into equal sized chunks, and decode-maximal batching, which constructs a batch using a single prefill chunk and populates the remaining slots with decodes. During inference, the prefill chunk saturates GPU compute, while the decode requests 'piggyback' and cost up to an order of magnitude less compared to a decode-only batch. Chunked-prefills allows constr
    
[^71]: 解决复合非凸非光滑非Lipschitz函数的次梯度方法的统一分析

    A Unified Analysis for the Subgradient Methods Minimizing Composite Nonconvex, Nonsmooth and Non-Lipschitz Functions. (arXiv:2308.16362v1 [math.OC])

    [http://arxiv.org/abs/2308.16362](http://arxiv.org/abs/2308.16362)

    本文提出了一种解决非凸非光滑非Lipschitz函数优化问题的近端次梯度方法，并建立了统一的递归关系用于 Moreau包络的建立。同时，提出了一些新的随机次梯度上界条件，并分析了随机次梯度方法的收敛性和迭代复杂度。

    

    本文提出了一种用于解决非凸非光滑优化问题的近端次梯度方法(Prox-SubGrad)，而无需假设Lipschitz连续性条件。我们提出了一些次梯度上界及其关系，并通过这些上界条件建立了弱凸优化Moreau包络的一些统一递归关系。这种统一方案简化并统一了建立Prox-SubGrad收敛速率的证明方案，而无需假设Lipschitz连续性。我们在这个背景下提出了一种新的收敛性分析。此外，我们还提出了一些新的随机次梯度上界条件，并为随机次梯度方法(Sto-SubGrad)解决非Lipschitz非光滑随机优化问题建立了收敛性和迭代复杂度的性质。特别地，对于无需Lipschitz连续性的弱凸优化问题，我们还讨论了确定性和随机次梯度方法的收敛性。

    In this paper we propose a proximal subgradient method (Prox-SubGrad) for solving nonconvex and nonsmooth optimization problems without assuming Lipschitz continuity conditions. A number of subgradient upper bounds and their relationships are presented. By means of these upper bounding conditions, we establish some uniform recursive relations for the Moreau envelopes for weakly convex optimization. This uniform scheme simplifies and unifies the proof schemes to establish rate of convergence for Prox-SubGrad without assuming Lipschitz continuity. We present a novel convergence analysis in this context. Furthermore, we propose some new stochastic subgradient upper bounding conditions and establish convergence and iteration complexity rates for the stochastic subgradient method (Sto-SubGrad) to solve non-Lipschitz and nonsmooth stochastic optimization problems. In particular, for both deterministic and stochastic subgradient methods on weakly convex optimization problems without Lipschitz
    
[^72]: 表情符号促进了GitHub上开发者的参与和问题解决

    Emoji Promotes Developer Participation and Issue Resolution on GitHub. (arXiv:2308.16360v1 [cs.CY])

    [http://arxiv.org/abs/2308.16360](http://arxiv.org/abs/2308.16360)

    本研究探讨了表情符号在虚拟工作空间中的使用对开发者参与和问题解决的影响。研究发现，表情符号可以显著缩短问题的解决时间，并吸引更多用户参与。不同类型问题对表情符号的效果也存在差异。

    

    尽管在疫情期间远程工作越来越普遍，但许多人对远程工作的低效率表示担忧。在基于文本的沟通中缺乏面部表情和肢体语言等非语言线索，这妨碍了有效的沟通，并对工作结果产生负面影响。作为替代的非语言线索，在社交媒体平台上广泛使用的表情符号在虚拟工作空间中也越来越受欢迎。本文研究了表情符号的使用如何影响虚拟工作空间中开发者的参与和问题解决。为此，我们收集了GitHub的一个一年周期内的问题，并应用因果推断技术来衡量表情符号对问题结果的因果效应，控制问题内容、仓库和作者信息等混淆因素。我们发现表情符号可以显著缩短问题的解决时间，并吸引更多用户参与。我们还比较了不同类型问题的异质效应。

    Although remote working is increasingly adopted during the pandemic, many are concerned by the low-efficiency in the remote working. Missing in text-based communication are non-verbal cues such as facial expressions and body language, which hinders the effective communication and negatively impacts the work outcomes. Prevalent on social media platforms, emojis, as alternative non-verbal cues, are gaining popularity in the virtual workspaces well. In this paper, we study how emoji usage influences developer participation and issue resolution in virtual workspaces. To this end, we collect GitHub issues for a one-year period and apply causal inference techniques to measure the causal effect of emojis on the outcome of issues, controlling for confounders such as issue content, repository, and author information. We find that emojis can significantly reduce the resolution time of issues and attract more user participation. We also compare the heterogeneous effect on different types of issue
    
[^73]: ToddlerBERTa: 利用BabyBERTa进行语法学习和语言理解

    ToddlerBERTa: Exploiting BabyBERTa for Grammar Learning and Language Understanding. (arXiv:2308.16336v1 [cs.CL])

    [http://arxiv.org/abs/2308.16336](http://arxiv.org/abs/2308.16336)

    ToddlerBERTa是一个类似于BabyBERTa的语言模型，尽管在较小的数据集上进行训练，但它展示了令人称赞的性能，并具有强大的语言理解能力，与最先进的RoBERTa-base相媲美。

    

    我们提出了ToddlerBERTa，这是一个类似于BabyBERTa的语言模型，并通过五种不同的具有不同超参数的模型来探索其能力。在BLiMP，SuperGLUE，MSGS和BabyLM挑战中进行评估，我们发现较小的模型在特定任务上表现出色，而较大的模型在大量数据方面表现良好。尽管在较小的数据集上训练，ToddlerBERTa展示了令人称赞的性能，与最先进的RoBERTa-base相媲美。该模型展示了强大的语言理解能力，即使是在单句预训练的情况下，也能与利用更广泛上下文信息的基线竞争。我们的工作为超参数选择和数据利用提供了洞察，并为语言模型的发展做出了贡献。

    We present ToddlerBERTa, a BabyBERTa-like language model, exploring its capabilities through five different models with varied hyperparameters. Evaluating on BLiMP, SuperGLUE, MSGS, and a Supplement benchmark from the BabyLM challenge, we find that smaller models can excel in specific tasks, while larger models perform well with substantial data. Despite training on a smaller dataset, ToddlerBERTa demonstrates commendable performance, rivalling the state-of-the-art RoBERTa-base. The model showcases robust language understanding, even with single-sentence pretraining, and competes with baselines that leverage broader contextual information. Our work provides insights into hyperparameter choices, and data utilization, contributing to the advancement of language models.
    
[^74]: 哈密顿系统中的对称性保持：模拟与学习

    Symmetry Preservation in Hamiltonian Systems: Simulation and Learning. (arXiv:2308.16331v1 [math-ph])

    [http://arxiv.org/abs/2308.16331](http://arxiv.org/abs/2308.16331)

    本论文提出了一个通用的几何框架，用于模拟和学习在Lie群变换下不变的哈密顿系统的动力学。通过构造$G$-不变的拉格朗日子流形，我们的方法能够保持与原始系统相同的保守量，提供了更忠实和准确的预测能力。

    

    本文提出了一个通用的几何框架，用于模拟和学习在Lie群变换下不变的哈密顿系统的动力学。这意味着已知一组对称性沿着系统作用，尊重其动力学，并且根据Noether定理，保守量会被观察到。我们提议通过构造$G$-不变的拉格朗日子流形来模拟和学习感兴趣的映射，这在辛几何学中是关键的对象。我们的构造的一个显著特点是，模拟/学习的动力学也保持与原始系统相同的保守量，这使得它比非对称意识方法更忠实地模拟原始动力学，并且更准确地预测非观察到的轨迹。此外，我们的设置能够模拟/学习不仅哈密顿流，还包括任何Lie群等变辛变换。我们的设计利用了关键的技术

    This work presents a general geometric framework for simulating and learning the dynamics of Hamiltonian systems that are invariant under a Lie group of transformations. This means that a group of symmetries is known to act on the system respecting its dynamics and, as a consequence, Noether's Theorem, conserved quantities are observed. We propose to simulate and learn the mappings of interest through the construction of $G$-invariant Lagrangian submanifolds, which are pivotal objects in symplectic geometry. A notable property of our constructions is that the simulated/learned dynamics also preserves the same conserved quantities as the original system, resulting in a more faithful surrogate of the original dynamics than non-symmetry aware methods, and in a more accurate predictor of non-observed trajectories. Furthermore, our setting is able to simulate/learn not only Hamiltonian flows, but any Lie group-equivariant symplectic transformation. Our designs leverage pivotal techniques an
    
[^75]: 十年生成对抗网络（GANs）的综述：最新技术的调查

    Ten Years of Generative Adversarial Nets (GANs): A survey of the state-of-the-art. (arXiv:2308.16316v1 [cs.LG])

    [http://arxiv.org/abs/2308.16316](http://arxiv.org/abs/2308.16316)

    生成对抗网络（GANs）自2014年诞生以来快速发展，它是一种在计算机视觉和其他领域生成逼真且多样化数据的强大工具。本文综述了GANs的最新技术发展和应用领域，并深入研究了GAN与统计学、信息论和优化方法之间的关系。

    

    自2014年诞生以来，生成对抗网络（GANs）迅速成为在计算机视觉和其他应用领域生成逼真且多样化数据的强大工具。GANs由一个判别网络和一个生成网络组成，它们在一个极小极大博弈中相互对抗，彻底改变了生成建模领域。在2018年2月，GAN被马萨诸塞州科技评论发布的“全球十大突破性技术列表”中排名第一。多年来，人们提出了许多先进技术，导致了各种GAN变种的丰富。这篇综述旨在提供GANs的概述，总结了广泛认可的变种的潜在架构、验证指标和应用领域。我们还深入研究了最近的理论发展，探索了对抗网络与统计学、信息论和优化方法之间的深刻联系。

    Since their inception in 2014, Generative Adversarial Networks (GANs) have rapidly emerged as powerful tools for generating realistic and diverse data across various domains, including computer vision and other applied areas. Consisting of a discriminative network and a generative network engaged in a Minimax game, GANs have revolutionized the field of generative modeling. In February 2018, GAN secured the leading spot on the ``Top Ten Global Breakthrough Technologies List'' issued by the Massachusetts Science and Technology Review. Over the years, numerous advancements have been proposed, leading to a rich array of GAN variants, such as conditional GAN, Wasserstein GAN, CycleGAN, and StyleGAN, among many others. This survey aims to provide a general overview of GANs, summarizing the latent architecture, validation metrics, and application areas of the most widely recognized variants. We also delve into recent theoretical developments, exploring the profound connection between the adve
    
[^76]: 电信网络KPI时间序列中异常的分类

    Classification of Anomalies in Telecommunication Network KPI Time Series. (arXiv:2308.16279v1 [cs.LG])

    [http://arxiv.org/abs/2308.16279](http://arxiv.org/abs/2308.16279)

    本文提出了一个模块化的电信网络KPI时间序列异常分类框架，其中包括时间序列模拟器、异常检测模型和分类模型。这个框架可以对网络KPI中检测到的异常进行准确分类和识别。

    

    电信网络的复杂性和规模的增加导致了对自动异常检测系统的日益关注。然而，对于在网络关键性能指标（KPI）上检测到的异常的分类却受到较少关注，导致缺乏关于异常特征和分类过程的信息。为了解决这个问题，本文提出了一个模块化的异常分类框架。该框架假设异常分类器和检测器作为独立的实体，允许对时间序列上的异常检测和分类任务进行独立处理。本研究的目标是：（1）开发一个生成类似于实际网络KPI行为的合成时间序列的时间序列模拟器；（2）构建一个检测模型，识别时间序列中的异常；（3）构建准确将检测到的异常分类到预定义类别的分类模型；（4）评估分类框架的性能。

    The increasing complexity and scale of telecommunication networks have led to a growing interest in automated anomaly detection systems. However, the classification of anomalies detected on network Key Performance Indicators (KPI) has received less attention, resulting in a lack of information about anomaly characteristics and classification processes. To address this gap, this paper proposes a modular anomaly classification framework. The framework assumes separate entities for the anomaly classifier and the detector, allowing for a distinct treatment of anomaly detection and classification tasks on time series. The objectives of this study are (1) to develop a time series simulator that generates synthetic time series resembling real-world network KPI behavior, (2) to build a detection model to identify anomalies in the time series, (3) to build classification models that accurately categorize detected anomalies into predefined classes (4) to evaluate the classification framework per
    
[^77]: 学习多样化特征以改善视觉Transformer的泛化性能

    Learning Diverse Features in Vision Transformers for Improved Generalization. (arXiv:2308.16274v1 [cs.CV])

    [http://arxiv.org/abs/2308.16274](http://arxiv.org/abs/2308.16274)

    本文中，我们首先研究了视觉Transformer并发现其具有提取稳健和虚假特征的特点。通过剪枝虚假特征对应的注意头，我们证明了在验证数据上使用"oracle选择"可以显著提高其在分布变化下的性能。其次，我们提出了一种方法来增加学习特征的多样性和互补性，通过鼓励注意头输入梯度的正交性。我们观察到，这种增强特征多样性和剪枝不良注意头的方法在诊断基准测试中取得了改进的超出分布性能。

    

    深度学习模型通常仅依赖于少量特征，即使训练数据中存在丰富的预测信号。这使得模型脆弱且对分布变化敏感。本研究首先研究了视觉Transformer(ViTs)，发现它们倾向于提取具有不同注意头的健壮和虚假特征。通过剪枝对应于虚假特征的注意头，在验证数据上使用"oracle选择"证明它们在分布变化下的性能可以得到显著改善。其次，我们提出了一种方法，通过鼓励注意头输入梯度的正交性，进一步增强学习特征的多样性和互补性。通过增强特征的多样性和剪枝不良注意头，我们观察到在诊断基准测试中（MNIST-CIFAR，水鸟）的改进的超出分布性能。

    Deep learning models often rely only on a small set of features even when there is a rich set of predictive signals in the training data. This makes models brittle and sensitive to distribution shifts. In this work, we first examine vision transformers (ViTs) and find that they tend to extract robust and spurious features with distinct attention heads. As a result of this modularity, their performance under distribution shifts can be significantly improved at test time by pruning heads corresponding to spurious features, which we demonstrate using an "oracle selection" on validation data. Second, we propose a method to further enhance the diversity and complementarity of the learned features by encouraging orthogonality of the attention heads' input gradients. We observe improved out-of-distribution performance on diagnostic benchmarks (MNIST-CIFAR, Waterbirds) as a consequence of the enhanced diversity of features and the pruning of undesirable heads.
    
[^78]: 通过深度神经网络的数值方法解决分数Laplacian问题

    A numerical approach for the fractional Laplacian via deep neural networks. (arXiv:2308.16272v1 [math.AP])

    [http://arxiv.org/abs/2308.16272](http://arxiv.org/abs/2308.16272)

    本文通过深度神经网络的数值方法解决了具有分数Laplacian的椭圆问题，并提供了四个数值示例来验证算法的效率。

    

    本文考虑在$\mathbb{R}^d$的有界凸域$D$上具有Dirichlet边界条件的分数椭圆问题，其中$d \geq 2$。我们采用随机梯度下降算法通过深度神经网络逼近分数问题的解。此外，我们提供了四个数值示例来测试算法的效率，并研究了每个示例中$\alpha \in (1,2)$和$d \geq 2$的多个值。

    We consider the fractional elliptic problem with Dirichlet boundary conditions on a bounded and convex domain $D$ of $\mathbb{R}^d$, with $d \geq 2$. In this paper, we perform a stochastic gradient descent algorithm that approximates the solution of the fractional problem via Deep Neural Networks. Additionally, we provide four numerical examples to test the efficiency of the algorithm, and each example will be studied for many values of $\alpha \in (1,2)$ and $d \geq 2$.
    
[^79]: 最简白盒Transformer中的分割出现现象

    Emergence of Segmentation with Minimalistic White-Box Transformers. (arXiv:2308.16271v1 [cs.CV])

    [http://arxiv.org/abs/2308.16271](http://arxiv.org/abs/2308.16271)

    本研究表明，在使用CRATE架构的最简监督训练方法下，Transformer模型可以实现分割特性的出现，无需复杂的自监督学习机制。

    

    最近，用于视觉任务的Transformer模型已被证明在诸多下游应用（如分割和检测）中非常有效。先前的研究表明，通过使用DINO等自监督方法训练的视觉Transformer（ViTs）出现了分割特性，但通过在监督分类任务上训练的模型却没有出现这种特性。本研究探究了分割是否仅通过复杂的自监督学习机制在基于Transformer的模型中出现，或者通过适当设计模型架构可以在更广泛的条件下实现相同的出现。通过广泛的实验结果，我们证明了当采用CRATE这种被称为白盒Transformer的架构时，该架构明确地对数据分布中的低维结构进行建模，并通过最简化的监督训练方法已经实现了整体和局部层面的分割特性的出现。

    Transformer-like models for vision tasks have recently proven effective for a wide range of downstream applications such as segmentation and detection. Previous works have shown that segmentation properties emerge in vision transformers (ViTs) trained using self-supervised methods such as DINO, but not in those trained on supervised classification tasks. In this study, we probe whether segmentation emerges in transformer-based models solely as a result of intricate self-supervised learning mechanisms, or if the same emergence can be achieved under much broader conditions through proper design of the model architecture. Through extensive experimental results, we demonstrate that when employing a white-box transformer-like architecture known as CRATE, whose design explicitly models and pursues low-dimensional structures in the data distribution, segmentation properties, at both the whole and parts levels, already emerge with a minimalistic supervised training recipe. Layer-wise finer-gra
    
[^80]: 材料信息学变压器：一种用于可解释材料性能预测的语言模型

    Materials Informatics Transformer: A Language Model for Interpretable Materials Properties Prediction. (arXiv:2308.16259v1 [cs.LG])

    [http://arxiv.org/abs/2308.16259](http://arxiv.org/abs/2308.16259)

    本研究提出了一种名为材料信息学变压器（MatInFormer）的语言模型，通过学习晶体学语法和引入MOFs数据，实现了对材料性能的准确预测，并通过注意力可视化揭示了模型的关键特征。

    

    最近，大型语言模型(LLMs)在自然语言处理、计算机视觉和分子建模等多个研究领域展示了显著的能力。我们通过引入我们的模型材料信息学变压器(MatInFormer)，将LLMs的这种范式扩展到材料性能预测领域。具体而言，我们引入了一种新颖的方法，通过对相关的空间群信息进行标记化，学习了晶体学的语法。我们进一步展示了MatInFormer的适应性，通过 incorporating task-specific data pertaining to Metal-Organic Frameworks (MOFs)。通过注意力可视化，我们揭示了模型在属性预测过程中优先考虑的关键特征。我们的提议模型在14个不同的数据集中经过实证验证，从而强调了其在通过准确的材料属性预测进行高通量筛选方面的潜力。

    Recently, the remarkable capabilities of large language models (LLMs) have been illustrated across a variety of research domains such as natural language processing, computer vision, and molecular modeling. We extend this paradigm by utilizing LLMs for material property prediction by introducing our model Materials Informatics Transformer (MatInFormer). Specifically, we introduce a novel approach that involves learning the grammar of crystallography through the tokenization of pertinent space group information. We further illustrate the adaptability of MatInFormer by incorporating task-specific data pertaining to Metal-Organic Frameworks (MOFs). Through attention visualization, we uncover the key features that the model prioritizes during property prediction. The effectiveness of our proposed model is empirically validated across 14 distinct datasets, hereby underscoring its potential for high throughput screening through accurate material property prediction.
    
[^81]: 回归问题的校准解释

    Calibrated Explanations for Regression. (arXiv:2308.16245v1 [cs.LG])

    [http://arxiv.org/abs/2308.16245](http://arxiv.org/abs/2308.16245)

    本文介绍了一种针对回归问题的特征重要性解释方法的扩展，可以量化特征重要性的不确定性。

    

    人工智能（AI）通常是现代决策支持系统（DSS）的一部分。在基于AI的DSS中使用的最佳预测模型缺乏透明度。可解释的人工智能（XAI）旨在创建能够向人类用户解释其理由的AI系统。XAI中的局部解释可以提供关于个别预测的原因的信息，即特征重要性。然而，现有局部解释方法的一个关键缺点是无法量化与特征重要性相关的不确定性。本文介绍了特征重要性解释方法Calibrated Explanations（CE）的扩展，之前只支持分类，现在支持标准回归和概率回归，即目标超过任意阈值的概率。回归问题的扩展保留了CE的所有优点，例如将底层模型的预测与置信度校准。

    Artificial Intelligence (AI) is often an integral part of modern decision support systems (DSSs). The best-performing predictive models used in AI-based DSSs lack transparency. Explainable Artificial Intelligence (XAI) aims to create AI systems that can explain their rationale to human users. Local explanations in XAI can provide information about the causes of individual predictions in terms of feature importance. However, a critical drawback of existing local explanation methods is their inability to quantify the uncertainty associated with a feature's importance. This paper introduces an extension of a feature importance explanation method, Calibrated Explanations (CE), previously only supporting classification, with support for standard regression and probabilistic regression, i.e., the probability that the target is above an arbitrary threshold. The extension for regression keeps all the benefits of CE, such as calibration of the prediction from the underlying model with confidenc
    
[^82]: 深度视频编码控制

    Deep Video Codec Control. (arXiv:2308.16215v1 [eess.IV])

    [http://arxiv.org/abs/2308.16215](http://arxiv.org/abs/2308.16215)

    本文提出了第一个端到端可学习的深度视频编码控制方法，同时考虑了带宽限制和下游视觉性能，并在不破坏现有标准化的情况下实现了保护深度视觉模型的目标。

    

    丢失率视频压缩通常用于传输和存储视频数据。尽管存在进阶（神经）压缩方法，但统一视频编码器（如H.264或H.265）仍然是事实上的标准。在面对动态网络带宽条件的视频传输中，视频编码器需要适应非常不同的压缩强度。速率控制模块增强编解码器的压缩能力，以满足带宽限制并尽量减少视频失真。然而，标准视频编码器及其速率控制模块是为了最小化人类质量评估而开发的，却没有考虑保护深度视觉模型的下游性能。在本文中，我们提出了第一个端到端可学习的深度视频编码控制方法，考虑了带宽限制和下游视觉性能，并不破坏现有的标准化。我们针对两个常见的视觉任务（语义分割...

    Lossy video compression is commonly used when transmitting and storing video data. Unified video codecs (e.g., H.264 or H.265) remain the \emph{de facto} standard, despite the availability of advanced (neural) compression approaches. Transmitting videos in the face of dynamic network bandwidth conditions requires video codecs to adapt to vastly different compression strengths. Rate control modules augment the codec's compression such that bandwidth constraints are satisfied and video distortion is minimized. While, both standard video codes and their rate control modules are developed to minimize video distortion w.r.t. human quality assessment, preserving the downstream performance of deep vision models is not considered. In this paper, we present the first end-to-end learnable deep video codec control considering both bandwidth constraints and downstream vision performance, while not breaking existing standardization. We demonstrate for two common vision tasks (semantic segmentation 
    
[^83]: RetroBridge: 使用马尔可夫桥模型进行反合成建模

    RetroBridge: Modeling Retrosynthesis with Markov Bridges. (arXiv:2308.16212v1 [q-bio.QM])

    [http://arxiv.org/abs/2308.16212](http://arxiv.org/abs/2308.16212)

    本研究提出了一种基于马尔可夫桥模型的反合成建模方法，通过对两个难以处理的离散分布之间的依赖关系进行近似，直接生成可能的前体分子，为反合成规划提供了准确的预测和置信度估计。

    

    反合成规划是化学中的一项基本挑战，旨在从市售起始材料设计反应路径到目标分子。多步反合成规划中的每一步都需要准确预测给定目标分子的可能前体分子，并给出置信度估计以指导启发式搜索算法。我们将单步反合成规划建模为离散状态空间中的分布学习问题。首先，我们引入了马尔可夫桥模型，一种生成框架，旨在近似两个难以处理的离散分布之间的依赖关系，通过有限样本的耦合数据点进行访问。我们的框架基于马尔可夫桥的概念，即以其端点为初始点固定的马尔可夫过程。与基于扩散的方法不同，我们的马尔可夫桥模型不需要作为采样代理的可追踪噪声分布，并直接使用输入产物分子作为来自难以处理的先验分布的样本进行操作。

    Retrosynthesis planning is a fundamental challenge in chemistry which aims at designing reaction pathways from commercially available starting materials to a target molecule. Each step in multi-step retrosynthesis planning requires accurate prediction of possible precursor molecules given the target molecule and confidence estimates to guide heuristic search algorithms. We model single-step retrosynthesis planning as a distribution learning problem in a discrete state space. First, we introduce the Markov Bridge Model, a generative framework aimed to approximate the dependency between two intractable discrete distributions accessible via a finite sample of coupled data points. Our framework is based on the concept of a Markov bridge, a Markov process pinned at its endpoints. Unlike diffusion-based methods, our Markov Bridge Model does not need a tractable noise distribution as a sampling proxy and directly operates on the input product molecules as samples from the intractable prior di
    
[^84]: 深层归纳逻辑编程与强化学习的结合

    Deep Inductive Logic Programming meets Reinforcement Learning. (arXiv:2308.16210v1 [cs.LG])

    [http://arxiv.org/abs/2308.16210](http://arxiv.org/abs/2308.16210)

    本研究将深层归纳逻辑编程与强化学习相结合，提出了一种在动态连续环境中应用可微分神经逻辑网络的方法，以改进当前关系强化学习中的ILP方法。该方法能够学习一阶逻辑规则并处理连续环境中的问题。

    

    解释机器学习模型中的层级理解水平的一种方法是归纳逻辑编程（ILP）的符号方法，它在数据效率和学习能力方面均具备，可以学习能够推导数据行为的一阶逻辑规则。可微扩展的ILP，即可微分的神经逻辑（dNL）网络，能够学习布尔函数，因为它的神经结构包括符号推理。我们提出了将dNL应用于关系强化学习（RRL）领域的方法，以解决动态连续环境问题。这是在RRL设置中应用基于dNL的ILP的扩展，因为我们的模型更新了架构，使其能够在连续RL环境中解决问题。本研究的目标是改进用于RRL的当前ILP方法，通过引入非线性连续谓词，使RRL代理能够在动态连续环境中进行推理和决策。

    One approach to explaining the hierarchical levels of understanding within a machine learning model is the symbolic method of inductive logic programming (ILP), which is data efficient and capable of learning first-order logic rules that can entail data behaviour. A differentiable extension to ILP, so-called differentiable Neural Logic (dNL) networks, are able to learn Boolean functions as their neural architecture includes symbolic reasoning. We propose an application of dNL in the field of Relational Reinforcement Learning (RRL) to address dynamic continuous environments. This represents an extension of previous work in applying dNL-based ILP in RRL settings, as our proposed model updates the architecture to enable it to solve problems in continuous RL environments. The goal of this research is to improve upon current ILP methods for use in RRL by incorporating non-linear continuous predicates, allowing RRL agents to reason and make decisions in dynamic and continuous environments.
    
[^85]: MASA-TCN: 多锚点空间感知时间卷积神经网络用于连续和离散EEG情绪识别

    MASA-TCN: Multi-anchor Space-aware Temporal Convolutional Neural Networks for Continuous and Discrete EEG Emotion Recognition. (arXiv:2308.16207v1 [cs.LG])

    [http://arxiv.org/abs/2308.16207](http://arxiv.org/abs/2308.16207)

    MASA-TCN是一种用于连续和离散EEG情绪识别的多锚点空间感知时间卷积神经网络模型。该模型通过引入空间感知时间层来提取EEG空间模式，并能在情绪回归和分类任务中取得更好的性能。

    

    使用脑电图（EEG）进行情绪识别主要有两种情况：离散标签的分类和连续标记的回归。尽管已经提出了许多用于分类任务的算法，但对于回归任务只有少数方法。对于情绪回归，标签在时间上是连续的。一个自然的方法是学习时态动态模式。在先前的研究中，长短期记忆网络（LSTM）和时间卷积神经网络（TCN）被用来学习EEG特征向量的时间背景信息。然而，EEG的空间模式没有被有效提取出来。为了使TCN在回归和分类性能上具有更好的空间学习能力，我们提出了一种新的统一模型，名为MASA-TCN，用于EEG情绪回归和分类任务。空间感知时间层使得TCN能够从EEG电极之间的空间关系中进行额外的学习。

    Emotion recognition using electroencephalogram (EEG) mainly has two scenarios: classification of the discrete labels and regression of the continuously tagged labels. Although many algorithms were proposed for classification tasks, there are only a few methods for regression tasks. For emotion regression, the label is continuous in time. A natural method is to learn the temporal dynamic patterns. In previous studies, long short-term memory (LSTM) and temporal convolutional neural networks (TCN) were utilized to learn the temporal contextual information from feature vectors of EEG. However, the spatial patterns of EEG were not effectively extracted. To enable the spatial learning ability of TCN towards better regression and classification performances, we propose a novel unified model, named MASA-TCN, for EEG emotion regression and classification tasks. The space-aware temporal layer enables TCN to additionally learn from spatial relations among EEG electrodes. Besides, a novel multi-an
    
[^86]: 使用基于图的多智能体强化学习学习协作信息传播

    Learning Collaborative Information Dissemination with Graph-based Multi-Agent Reinforcement Learning. (arXiv:2308.16198v1 [cs.LG])

    [http://arxiv.org/abs/2308.16198](http://arxiv.org/abs/2308.16198)

    本论文介绍了一种使用多智能体强化学习的方法来实现协作信息传播。通过提出分布式POMDP形式，在消息转发上实现了每个智能体的独立决策，相比传统的基于多点中继选择的启发式方法具有重大创新和贡献。同时，该方法利用图卷积强化学习和动态注意力机制捕捉关键网络特征，并提出了不同信息交换方式的两种方法进行评估。

    

    在现代通信系统中，高效可靠的信息传播对支持关键操作至关重要，如灾难响应、自动驾驶车辆和传感器网络。本文介绍了一种多智能体强化学习（MARL）方法，作为实现更为分散、高效和协作解决方案的重要进展。我们提出了一种用于信息传播的分布式POMDP（Decentralized-POMDP）形式，使得每个智能体可以独立决定消息的转发。这构成了一种从传统基于多点中继（MPR）选择的启发式方法的重大范式转移。我们的方法利用图卷积强化学习，采用具有动态注意力的图注意力网络（GAT）来捕捉关键网络特征。我们提出了两种方法，L-DGN和HL-DGN，它们在智能体之间交换的信息上有所不同。通过将我们的分散方法与基于MPR的方法进行比较，我们评估了其性能。

    In modern communication systems, efficient and reliable information dissemination is crucial for supporting critical operations across domains like disaster response, autonomous vehicles, and sensor networks. This paper introduces a Multi-Agent Reinforcement Learning (MARL) approach as a significant step forward in achieving more decentralized, efficient, and collaborative solutions. We propose a Decentralized-POMDP formulation for information dissemination, empowering each agent to independently decide on message forwarding. This constitutes a significant paradigm shift from traditional heuristics based on Multi-Point Relay (MPR) selection. Our approach harnesses Graph Convolutional Reinforcement Learning, employing Graph Attention Networks (GAT) with dynamic attention to capture essential network features. We propose two approaches, L-DGN and HL-DGN, which differ in the information that is exchanged among agents. We evaluate the performance of our decentralized approaches, by compari
    
[^87]: MedShapeNet - 一个用于计算机视觉的大规模三维医学形状数据集

    MedShapeNet -- A Large-Scale Dataset of 3D Medical Shapes for Computer Vision. (arXiv:2308.16139v1 [cs.CV])

    [http://arxiv.org/abs/2308.16139](http://arxiv.org/abs/2308.16139)

    MedShapeNet是一个大规模的三维医学形状数据集，作为一种对于常用形状基准的替代品，为计算机视觉研究提供了新的选择。

    

    我们提出了MedShapeNet，一个包含了解剖形状（如骨骼、器官、血管）和三维手术器械模型的大型数据集。在深度学习时代之前，统计形状模型在医学图像分析中的广泛应用证明了形状常被用来描述医学数据。然而，当前医学图像领域的最先进深度学习算法主要是基于体素的。相反，在计算机视觉领域，形状（包括体素占据网格、网格、点云和隐式表面模型）是三维数据的首选表示方法，这一点可以从大量关于形状的文章及在顶级计算机视觉会议（如IEEE/CVF计算机视觉与模式识别会议（CVPR））中见到，同时ShapeNet（约51300个模型）和普林斯顿ModelNet（127,915个模型）的流行度也在不断增加。MedShapeNet的创建是为了作为这些常用形状基准的替代品。

    We present MedShapeNet, a large collection of anatomical shapes (e.g., bones, organs, vessels) and 3D surgical instrument models. Prior to the deep learning era, the broad application of statistical shape models (SSMs) in medical image analysis is evidence that shapes have been commonly used to describe medical data. Nowadays, however, state-of-the-art (SOTA) deep learning algorithms in medical imaging are predominantly voxel-based. In computer vision, on the contrary, shapes (including, voxel occupancy grids, meshes, point clouds and implicit surface models) are preferred data representations in 3D, as seen from the numerous shape-related publications in premier vision conferences, such as the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), as well as the increasing popularity of ShapeNet (about 51,300 models) and Princeton ModelNet (127,915 models) in computer vision research. MedShapeNet is created as an alternative to these commonly used shape benchmarks to f
    
[^88]: CongNaMul: 一种用于大豆芽图像处理的数据集

    CongNaMul: A Dataset for Advanced Image Processing of Soybean Sprouts. (arXiv:2308.15690v1 [cs.CV])

    [http://arxiv.org/abs/2308.15690](http://arxiv.org/abs/2308.15690)

    提出了一个用于大豆芽图像处理的名为CongNaMul的数据集，旨在支持图像分类、语义分割、分解和测量等任务。提供了质量分类、语义分割和图像分解的标记，以及5个芽的物理特征供测量使用。

    

    我们提出了“CongNaMul”，这是一个为大豆芽图像分析的各种任务而设计的综合数据集。CongNaMul数据集旨在促进图像分类、语义分割、分解以及长度和重量的测量等任务。分类任务提供了四个类别来确定大豆芽的质量：正常、断裂、斑点和断裂和斑点，以开发基于人工智能辅助的自动质量检测技术。对于语义分割，数据集包括了具有不同复杂度的图像，从单个芽图像到具有多个芽的图像，以及人工标记的掩膜图像。标签包括4个不同的类别：背景、头部、身体和尾部。数据集还为图像分解任务提供了图像和掩膜，包括两个分离的芽图像和它们的组合形式。最后，还提供了芽的5个物理特征（头部长度、身体长度、身体厚度、尾部长度、重量）供基于图像的测量使用。

    We present 'CongNaMul', a comprehensive dataset designed for various tasks in soybean sprouts image analysis. The CongNaMul dataset is curated to facilitate tasks such as image classification, semantic segmentation, decomposition, and measurement of length and weight. The classification task provides four classes to determine the quality of soybean sprouts: normal, broken, spotted, and broken and spotted, for the development of AI-aided automatic quality inspection technology. For semantic segmentation, images with varying complexity, from single sprout images to images with multiple sprouts, along with human-labelled mask images, are included. The label has 4 different classes: background, head, body, tail. The dataset also provides images and masks for the image decomposition task, including two separate sprout images and their combined form. Lastly, 5 physical features of sprouts (head length, body length, body thickness, tail length, weight) are provided for image-based measurement
    
[^89]: 多响应异方差高斯过程模型及其推断

    Multi-Response Heteroscedastic Gaussian Process Models and Their Inference. (arXiv:2308.15370v1 [stat.ML])

    [http://arxiv.org/abs/2308.15370](http://arxiv.org/abs/2308.15370)

    本文介绍了多响应异方差高斯过程模型，将其应用于回归、分类和状态空间模型，并提出了一种利用变分推断来近似后验的方法，解决了高斯过程模型在捕捉函数平滑性的突变和适应异方差错误方面的局限性。

    

    尽管高斯过程模型被广泛用于灵活的非参数建模，但它们在有效捕捉函数平滑性的突变和适应异方差错误方面存在局限性。为了解决这些问题，异方差高斯过程（HeGP）回归旨在通过承认回归模型中协变量间残差方差的可变性来引入灵活性。本文将HeGP概念扩展到分类和状态空间模型，并提出了一种新的框架，将高斯过程与协变量诱导的精度矩阵过程相结合，采用混合形式。这种方法使得可以对协变量之间的异方差协方差函数进行建模。为了解决采样带来的计算挑战，我们采用变分推断来近似后验并便利计算。

    Despite the widespread utilization of Gaussian process models for versatile nonparametric modeling, they exhibit limitations in effectively capturing abrupt changes in function smoothness and accommodating relationships with heteroscedastic errors. Addressing these shortcomings, the heteroscedastic Gaussian process (HeGP) regression seeks to introduce flexibility by acknowledging the variability of residual variances across covariates in the regression model. In this work, we extend the HeGP concept, expanding its scope beyond regression tasks to encompass classification and state-space models. To achieve this, we propose a novel framework where the Gaussian process is coupled with a covariate-induced precision matrix process, adopting a mixture formulation. This approach enables the modeling of heteroscedastic covariance functions across covariates. To mitigate the computational challenges posed by sampling, we employ variational inference to approximate the posterior and facilitate p
    
[^90]: 通过稀疏惩罚进行的双聚类方法

    Biclustering Methods via Sparse Penalty. (arXiv:2308.14388v1 [stat.ML])

    [http://arxiv.org/abs/2308.14388](http://arxiv.org/abs/2308.14388)

    本文提出了一种基于稀疏惩罚的双聚类方法，主要关注了SSVD方法，并尝试了一种新的稀疏惩罚方法。模拟研究结果表明混合的Prenet惩罚对于非重叠数据非常有效。

    

    本文首先回顾了几种用于识别基因表达数据中最显著聚类的双聚类方法。我们主要关注了SSVD（稀疏SVD）方法，并尝试了一种仅用于因子分析的新的稀疏惩罚方法，称为"Prenet惩罚"。然后在模拟研究中，我们尝试了不同类型的生成数据集（具有不同的稀疏性和维度），并尝试了一层逼近和k层逼近，结果表明混合的Prenet惩罚对于非重叠数据非常有效。最后，我们使用了一些真实的基因表达数据来展示我们方法的行为。

    In this paper, we first reviewed several biclustering methods that are used to identify the most significant clusters in gene expression data. Here we mainly focused on the SSVD(sparse SVD) method and tried a new sparse penalty named "Prenet penalty" which has been used only in factor analysis to gain sparsity. Then in the simulation study, we tried different types of generated datasets (with different sparsity and dimension) and tried 1-layer approximation then for k-layers which shows the mixed Prenet penalty is very effective for non-overlapped data. Finally, we used some real gene expression data to show the behavior of our methods.
    
[^91]: 从数据中基于光滑性先验推断超图结构

    Hypergraph Structure Inference From Data Under Smoothness Prior. (arXiv:2308.14172v1 [cs.LG])

    [http://arxiv.org/abs/2308.14172](http://arxiv.org/abs/2308.14172)

    本文提出了一种光滑性先验方法，用于从节点特征中推断超图的结构，并捕捉数据内在的关系。该方法不需要标记数据作为监督，能够推断出每个潜在超边的概率。

    

    超图在处理涉及多个实体的高阶关系数据中非常重要。在没有明确超图可用的情况下，希望能够从节点特征中推断出有意义的超图结构，以捕捉数据内在的关系。然而，现有的方法要么采用简单预定义的规则，不能精确捕捉潜在超图结构的分布，要么学习超图结构和节点特征之间的映射，但需要大量标记数据（即预先存在的超图结构）进行训练。这两种方法都局限于实际情景中的应用。为了填补这一空白，我们提出了一种新的光滑性先验，使我们能够设计一种方法，在没有标记数据作为监督的情况下推断出每个潜在超边的概率。所提出的先验表示超边中的节点特征与包含该超边的超边的特征高度相关。

    Hypergraphs are important for processing data with higher-order relationships involving more than two entities. In scenarios where explicit hypergraphs are not readily available, it is desirable to infer a meaningful hypergraph structure from the node features to capture the intrinsic relations within the data. However, existing methods either adopt simple pre-defined rules that fail to precisely capture the distribution of the potential hypergraph structure, or learn a mapping between hypergraph structures and node features but require a large amount of labelled data, i.e., pre-existing hypergraph structures, for training. Both restrict their applications in practical scenarios. To fill this gap, we propose a novel smoothness prior that enables us to design a method to infer the probability for each potential hyperedge without labelled data as supervision. The proposed prior indicates features of nodes in a hyperedge are highly correlated by the features of the hyperedge containing th
    
[^92]: 工业人工智能中的随机配置机

    Stochastic Configuration Machines for Industrial Artificial Intelligence. (arXiv:2308.13570v1 [cs.LG])

    [http://arxiv.org/abs/2308.13570](http://arxiv.org/abs/2308.13570)

    本文提出了一种新颖的随机学习器模型，称为随机配置机（SCMs），其基于随机配置网络（SCNs），旨在强调工业人工智能中的有效建模和节约数据大小。SCMs通过压缩模型存储，并保持有利的预测性能，具有在工业应用中很大的潜力。

    

    在工业人工智能（IAI）中，需要实时、准确的预测建模，神经网络在其中起到关键作用。工业人工智能中的神经网络需要强大的高性能计算设备来处理大量的浮点数据。本文基于随机配置网络（SCNs），提出了一种新的随机学习器模型，称为随机配置机（SCMs），以强调对于工业应用非常有用和有价值的有效建模和节约数据大小。与具有二值化实现的随机向量功能链接（RVFL）网络相比，SCMs的模型存储可以显著压缩，同时保持有利的预测性能。除了SCM学习器模型的架构和学习算法，作为本文的重要部分，我们还通过分析模型的复杂性提供了SCMs的学习能力的理论基础。实验研究也进行了。

    Real-time predictive modelling with desired accuracy is highly expected in industrial artificial intelligence (IAI), where neural networks play a key role. Neural networks in IAI require powerful, high-performance computing devices to operate a large number of floating point data. Based on stochastic configuration networks (SCNs), this paper proposes a new randomized learner model, termed stochastic configuration machines (SCMs), to stress effective modelling and data size saving that are useful and valuable for industrial applications. Compared to SCNs and random vector functional-link (RVFL) nets with binarized implementation, the model storage of SCMs can be significantly compressed while retaining favourable prediction performance. Besides the architecture of the SCM learner model and its learning algorithm, as an important part of this contribution, we also provide a theoretical basis on the learning capacity of SCMs by analysing the model's complexity. Experimental studies are ca
    
[^93]: 基于量子力学视角的量化优化方法

    Quantization-based Optimization with Perspective of Quantum Mechanics. (arXiv:2308.11594v1 [quant-ph])

    [http://arxiv.org/abs/2308.11594](http://arxiv.org/abs/2308.11594)

    基于量子力学视角的量化优化方法在全局优化中利用薛定谔方程推导的隧道效应，从而能够避免局部最小值。

    

    基于热力学的统计和随机分析一直是随机全局优化的主要分析框架。最近，出现了用于全局优化的量子退火或量子隧道算法，我们需要一个新的研究框架来进行全局优化算法。在本文中，我们提供了基于薛定谔方程的量化优化的分析，以揭示量子力学中的哪些属性使全局优化成为可能。我们提出，薛定谔方程推导出的隧道效应使得量化优化能够逃离局部最小值。此外，我们确认这种隧道效应是包含在基于量子力学的全局优化中的相同属性。对标准多模态基准函数进行的实验表明了所提出的分析的有效性。

    Statistical and stochastic analysis based on thermodynamics has been the main analysis framework for stochastic global optimization. Recently, appearing quantum annealing or quantum tunneling algorithm for global optimization, we require a new researching framework for global optimization algorithms. In this paper, we provide the analysis for quantization-based optimization based on the Schr\"odinger equation to reveal what property in quantum mechanics enables global optimization. We present that the tunneling effect derived by the Schr\"odinger equation in quantization-based optimization enables to escape of a local minimum. Additionally, we confirm that this tunneling effect is the same property included in quantum mechanics-based global optimization. Experiments with standard multi-modal benchmark functions represent that the proposed analysis is valid.
    
[^94]: 通过超出平衡状态的扩展动力学性能评估神经力场

    xxMD: Benchmarking Neural Force Fields Using Extended Dynamics beyond Equilibrium. (arXiv:2308.11155v1 [cs.LG])

    [http://arxiv.org/abs/2308.11155](http://arxiv.org/abs/2308.11155)

    在神经力场模型中，常用的MD17数据集对于表示经历化学反应的系统不足。为了解决这一问题，我们引入了xxMD数据集，该数据集采样自扩展激发态分子动力学，包含了能量和力的信息。

    

    神经力场已成为计算化学中的重要模型，取代了从头算的分子动力学中的量子化学计算。目前对神经力场的主要评估基准是MD17数据集及其后续扩展。这些数据集主要包含来自基态势能面平衡区域的几何结构，采样自直接绝热动力学。然而，许多化学反应涉及到较大的分子变形，特别是键断裂。我们展示了MD17数据集中内坐标和能量的约束分布，凸显了其在表示经历化学反应的系统方面的不足。为了解决这种采样限制，我们引入了xxMD（扩展激发态分子动力学）数据集，从非绝热动力学中派生。该数据集包含了从多参考波函数理论和密度泛函中确定的能量和力。

    Neural force fields (NFFs) have gained prominence in computational chemistry as surrogate models, superseding quantum-chemistry calculations in ab initio molecular dynamics. The prevalent benchmark for NFFs has been the MD17 dataset and its subsequent extension. These datasets predominantly comprise geometries from the equilibrium region of the ground electronic state potential energy surface, sampling from direct adiabatic dynamics. However, many chemical reactions entail significant molecular deformations, notably bond breaking. We demonstrate the constrained distribution of internal coordinates and energies in the MD17 datasets, underscoring their inadequacy for representing systems undergoing chemical reactions. Addressing this sampling limitation, we introduce the xxMD (Extended Excited-state Molecular Dynamics) dataset, derived from non-adiabatic dynamics. This dataset encompasses energies and forces ascertained from both multireference wave function theory and density functional
    
[^95]: RBA-GCN: 关系双层聚合图卷积网络用于情感识别

    RBA-GCN: Relational Bilevel Aggregation Graph Convolutional Network for Emotion Recognition. (arXiv:2308.11029v1 [cs.AI])

    [http://arxiv.org/abs/2308.11029](http://arxiv.org/abs/2308.11029)

    提出了RBA-GCN模型用于情感识别。该模型通过引入关系双层聚合和图生成模块，解决了GCN模型中的节点信息冗余和远距离上下文信息捕获问题。

    

    情感识别在对话中的应用受到了研究人员的关注，由于它具有广泛的应用。由于对话具有自然的图结构，很多基于图卷积网络（GCNs）的ERC模型方法取得了显著的结果。然而，传统GCNs的聚合方法存在节点信息冗余问题，导致节点辨别信息的丢失。此外，单层GCNs缺乏从图中捕获远距离上下文信息的能力。此外，大多数方法都是基于文本模态或将不同模态拼接在一起，导致捕捉模态间交互能力弱。为了解决这些问题，我们提出了关系双层聚合图卷积网络（RBA-GCN），它由三个模块组成：图生成模块（GGM）、基于相似性的簇构建模块（SCBM）和双层聚合模块。

    Emotion recognition in conversation (ERC) has received increasing attention from researchers due to its wide range of applications. As conversation has a natural graph structure, numerous approaches used to model ERC based on graph convolutional networks (GCNs) have yielded significant results. However, the aggregation approach of traditional GCNs suffers from the node information redundancy problem, leading to node discriminant information loss. Additionally, single-layer GCNs lack the capacity to capture long-range contextual information from the graph. Furthermore, the majority of approaches are based on textual modality or stitching together different modalities, resulting in a weak ability to capture interactions between modalities. To address these problems, we present the relational bilevel aggregation graph convolutional network (RBA-GCN), which consists of three modules: the graph generation module (GGM), similarity-based cluster building module (SCBM) and bilevel aggregation 
    
[^96]: 自适应的不确定性引导模型选择用于数据驱动的PDE发现

    Adaptive Uncertainty-Guided Model Selection for Data-Driven PDE Discovery. (arXiv:2308.10283v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.10283](http://arxiv.org/abs/2308.10283)

    我们提出了一种自适应的不确定性引导模型选择方法，通过优先考虑对观测数据进行充分约束的简约偏微分方程，并结合物理学翻译神经网络学习进行验证。数值结果证实了该方法在识别真实主导PDE方面的成功应用。

    

    我们提出了一种新的参数自适应的不确定性惩罚贝叶斯信息准则（UBIC），以优先考虑对噪音空时观测数据进行充分约束的简约偏微分方程（PDE）。由于直接使用BIC进行模型选择已知会导致不理想的过度拟合PDE，UBIC在概率视图中通过模型支持的变异系数来量化不确定性，并对找到的PDE进行惩罚。我们还引入了基于物理的神经网络学习作为一种基于模拟的方法，以进一步灵活地验证所选定的PDE与其他发现的PDE。数值结果证实了UBIC在识别真实的主导PDE上的成功应用。此外，我们揭示了去噪观测数据对改善BIC得分和模型复杂度之间的权衡有一个有趣的效果。

    We propose a new parameter-adaptive uncertainty-penalized Bayesian information criterion (UBIC) to prioritize the parsimonious partial differential equation (PDE) that sufficiently governs noisy spatial-temporal observed data with few reliable terms. Since the naive use of the BIC for model selection has been known to yield an undesirable overfitted PDE, the UBIC penalizes the found PDE not only by its complexity but also the quantified uncertainty, derived from the model supports' coefficient of variation in a probabilistic view. We also introduce physics-informed neural network learning as a simulation-based approach to further validate the selected PDE flexibly against the other discovered PDE. Numerical results affirm the successful application of the UBIC in identifying the true governing PDE. Additionally, we reveal an interesting effect of denoising the observed data on improving the trade-off between the BIC score and model complexity. Code is available at https://github.com/Po
    
[^97]: 保持对称性的程序表示法用于学习代码语义

    Symmetry-Preserving Program Representations for Learning Code Semantics. (arXiv:2308.03312v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.03312](http://arxiv.org/abs/2308.03312)

    本文提出了一种在代码中保持对称性的程序表示方法，通过引入循环层来提高在程序分析和建模中的效果。

    

    大型语言模型(LLMs)在自动程序推理方面显示出潜力，这是许多安全任务的关键方面。然而，现有的用于代码的LLM架构通常从其他领域（如自然语言处理）借用，引发对其泛化能力和对未知代码的健壮性的担忧。一个关键的泛化挑战是将代码语义的知识，包括控制和数据流，纳入LLM架构中。受到利用平移对称性的卷积层的启发，我们探索了代码对称性如何增强程序分析和建模的LLM架构。我们提供了一个严格的群论框架，形式化地定义了代码对称性作为保持语义的变换，并提供了在LLM架构中精确推理对称性保持的技术。利用这个框架，我们引入了一种保持程序对称性的新型自注意力变体，并展示了其有效性。

    Large Language Models (LLMs) have shown promise in automated program reasoning, a crucial aspect of many security tasks. However, existing LLM architectures for code are often borrowed from other domains like natural language processing, raising concerns about their generalization and robustness to unseen code. A key generalization challenge is to incorporate the knowledge of code semantics, including control and data flow, into the LLM architectures.  Drawing inspiration from examples of convolution layers exploiting translation symmetry, we explore how code symmetries can enhance LLM architectures for program analysis and modeling. We present a rigorous group-theoretic framework that formally defines code symmetries as semantics-preserving transformations and provides techniques for precisely reasoning about symmetry preservation within LLM architectures. Using this framework, we introduce a novel variant of self-attention that preserves program symmetries, demonstrating its effectiv
    
[^98]: 通过混合精度加速傅里叶神经算子

    Speeding up Fourier Neural Operators via Mixed Precision. (arXiv:2307.15034v1 [cs.LG])

    [http://arxiv.org/abs/2307.15034](http://arxiv.org/abs/2307.15034)

    通过混合精度训练，加速了傅里叶神经算子（FNO）的运行时间和内存使用，提高了训练效率。

    

    傅里叶神经算子（FNO）是一种强大的技术，用于学习偏微分方程（PDE）解算器的代理映射。对于许多现实世界的应用，通常需要高分辨率的数据点，训练时间和内存使用量都是重要瓶颈。虽然对于标准神经网络有混合精度训练技术，但那些只适用于有限维度上的实值数据类型，因此不能直接应用于在复值（傅里叶）域和函数空间中重要操作的FNO。另一方面，由于傅里叶变换本身就是一次近似（由于离散化误差的存在），我们不需要以完全精度执行操作。在这项工作中，我们（i）对使用全精度和混合精度训练的FNO进行内存和运行时间剖析，（ii）对混合精度训练的数值稳定性进行研究，以及（iii）设计了一种训练过程，大大缩短了训练时间和内存使用率。

    The Fourier neural operator (FNO) is a powerful technique for learning surrogate maps for partial differential equation (PDE) solution operators. For many real-world applications, which often require high-resolution data points, training time and memory usage are significant bottlenecks. While there are mixed-precision training techniques for standard neural networks, those work for real-valued datatypes on finite dimensions and therefore cannot be directly applied to FNO, which crucially operates in the (complex-valued) Fourier domain and in function spaces. On the other hand, since the Fourier transform is already an approximation (due to discretization error), we do not need to perform the operation at full precision. In this work, we (i) profile memory and runtime for FNO with full and mixed-precision training, (ii) conduct a study on the numerical stability of mixed-precision training of FNO, and (iii) devise a training routine which substantially decreases training time and memor
    
[^99]: 多模态讨论变换器：整合文本、图像和图变换器以检测社交媒体上的仇恨言论。

    Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media. (arXiv:2307.09312v1 [cs.CL])

    [http://arxiv.org/abs/2307.09312](http://arxiv.org/abs/2307.09312)

    多模态讨论变换器 (mDT) 是一个用于检测在线社交网络中仇恨言论的新颖模型。与传统的仅使用文本的方法不同，mDT通过整体分析文本和图像，结合图变换器捕捉评论周围整个讨论的上下文关系，并通过交织融合层将文本和图像嵌入进行组合。研究发现，捕捉对话的整体视图可以极大地提高检测反社会行为的准确性。

    

    我们提出了一种新颖的多模态基于图的变换器模型，名为多模态讨论变换器（mDT），用于检测在线社交网络中的仇恨言论。与传统的仅使用文本的方法不同，我们将标记评论为仇恨言论的方法围绕文本和图像的整体分析展开。这是通过利用图变换器来捕捉评论周围整个讨论中的上下文关系，并采用交织融合层来组合文本和图像嵌入，而不是单独处理不同的模态。我们将模型的性能与仅处理文本的基线进行比较，还进行了广泛的消融研究。最后，我们展望了多模态解决方案在在线环境中提供社会价值的未来工作，并认为捕捉对话的整体视图极大地推进了检测反社会行为的努力。

    We present the Multi-Modal Discussion Transformer (mDT), a novel multi-modal graph-based transformer model for detecting hate speech in online social networks. In contrast to traditional text-only methods, our approach to labelling a comment as hate speech centers around the holistic analysis of text and images. This is done by leveraging graph transformers to capture the contextual relationships in the entire discussion that surrounds a comment, with interwoven fusion layers to combine text and image embeddings instead of processing different modalities separately. We compare the performance of our model to baselines that only process text; we also conduct extensive ablation studies. We conclude with future work for multimodal solutions to deliver social value in online contexts, arguing that capturing a holistic view of a conversation greatly advances the effort to detect anti-social behavior.
    
[^100]: 探索从替代训练中理解对抗性可转移性

    Towards Understanding Adversarial Transferability From Surrogate Training. (arXiv:2307.07873v1 [cs.LG])

    [http://arxiv.org/abs/2307.07873](http://arxiv.org/abs/2307.07873)

    本论文探索了对抗性可转移性的理解，特别关注替代训练。通过研究模型的平滑性和梯度相似性之间的权衡，发现对抗训练可以提高模型的替代能力。研究结果对数据分布的转变提出了新的推测。

    

    对DNNs的对抗样本(AEs)已经表明是可转移的：成功欺骗白盒子替代模型的AEs也可以欺骗具有不同架构的其他黑盒模型。虽然许多经验研究提供了生成高度可转移AE的指导，但这些研究缺乏解释甚至导致不一致的建议。本文在理解对抗性可转移性方面迈出了一步，特别关注替代方面。从着名的小健壮性现象开始，通过以轻微扰动的对抗性样本对模型进行对抗训练可以得到更好的替代模型，我们将其归因于两个主要因素之间的权衡：模型的平滑性和梯度相似性。我们的研究集中在它们的共同效果上，而不是它们与可转移性的单独相关性。通过一系列理论和实证分析，我们推测数据分布的转变。

    Adversarial examples (AEs) for DNNs have been shown to be transferable: AEs that successfully fool white-box surrogate models can also deceive other black-box models with different architectures. Although a bunch of empirical studies have provided guidance on generating highly transferable AEs, many of these findings lack explanations and even lead to inconsistent advice. In this paper, we take a further step towards understanding adversarial transferability, with a particular focus on surrogate aspects. Starting from the intriguing little robustness phenomenon, where models adversarially trained with mildly perturbed adversarial samples can serve as better surrogates, we attribute it to a trade-off between two predominant factors: model smoothness and gradient similarity. Our investigations focus on their joint effects, rather than their separate correlations with transferability. Through a series of theoretical and empirical analyses, we conjecture that the data distribution shift in
    
[^101]: 在线分布式学习与量化有限时间协作

    Online Distributed Learning with Quantized Finite-Time Coordination. (arXiv:2307.06620v1 [cs.LG])

    [http://arxiv.org/abs/2307.06620](http://arxiv.org/abs/2307.06620)

    本文研究了一种在线分布式学习问题，提出了一种依靠量化、有限时间协作协议的分布式算法来聚合本地训练的模型，并允许使用随机梯度来提高效率和可扩展性。

    

    本文研究在线分布式学习问题。在线分布式学习是指在分布式数据源上训练学习模型的过程。在我们的设置中，一组代理需要合作地从流数据中训练学习模型。与联邦学习不同，所提出的方法不依赖于中央服务器，而仅依靠代理之间的点对点通信。该方法经常用于数据由于隐私、安全或成本原因不能移动到集中位置的场景。为了克服缺少中央服务器的问题，我们提出了一个分布式算法，该算法依赖于一个量化的、有限时间的协作协议来聚合本地训练的模型。此外，我们的算法允许在本地训练过程中使用随机梯度。随机梯度是使用本地训练数据的随机抽样子集计算的，这使得所提出的算法更加高效和可扩展。

    In this paper we consider online distributed learning problems. Online distributed learning refers to the process of training learning models on distributed data sources. In our setting a set of agents need to cooperatively train a learning model from streaming data. Differently from federated learning, the proposed approach does not rely on a central server but only on peer-to-peer communications among the agents. This approach is often used in scenarios where data cannot be moved to a centralized location due to privacy, security, or cost reasons. In order to overcome the absence of a central server, we propose a distributed algorithm that relies on a quantized, finite-time coordination protocol to aggregate the locally trained models. Furthermore, our algorithm allows for the use of stochastic gradients during local training. Stochastic gradients are computed using a randomly sampled subset of the local training data, which makes the proposed algorithm more efficient and scalable th
    
[^102]: DNAGPT：用于多个DNA序列分析任务的通用预训练工具

    DNAGPT: A Generalized Pretrained Tool for Multiple DNA Sequence Analysis Tasks. (arXiv:2307.05628v1 [q-bio.GN])

    [http://arxiv.org/abs/2307.05628](http://arxiv.org/abs/2307.05628)

    DNAGPT是一个通用的基础模型，通过预训练模型和独特的标记设计，可以适用于任何DNA序列分析任务。它在多个任务上进行了评估，并展示出了良好的性能。

    

    GPT系列的成功证明了GPT可以从序列中提取一般性信息，从而使得所有下游任务受益。这激发了我们使用预训练模型来探索DNA序列中的隐藏信息。然而，DNA序列分析中的数据和任务需求非常复杂和多样化，因为DNA相关数据包含不同类型的信息，如序列、表达水平等，目前还没有专门针对这些特点设计的模型。因此，我们提出了DNAGPT，这是一个通用的基础模型，它在9个物种的超过100亿个碱基对上进行了预训练，并可以针对任何DNA序列分析任务进行微调。我们的模型可以同时处理或输出DNA序列和数值。此外，我们独特的标记设计允许用户根据自己的任务需求来设计提示，使其适用于任何类型的任务。我们在分类、回归和生成任务上对模型进行了评估。

    The success of the GPT series proves that GPT can extract general information from sequences, thereby benefiting all downstream tasks. This motivates us to use pre-trained models to explore the hidden information in DNA sequences. However, data and task requirements in DNA sequence analysis are complexity and diversity as DNA relevant data includes different types of information, such as sequences, expression levels, etc, while there is currently no model specifically designed for these characteristics. Hereby, we present DNAGPT, a generalized foundation model pre-trained on over 10 billion base pairs from 9 species which can be fine-tuned for any DNA sequence analysis task. Our model can simultaneously process or output DNA sequences and numbers. In addition, our unique token design allows users to design prompts according to their own task requirements, making it applicable to any type of task. We have evaluated our model on classification, regression, and generation tasks. We demons
    
[^103]: 脱机强化学习中的离散策略的扩散策略

    Diffusion Policies for Out-of-Distribution Generalization in Offline Reinforcement Learning. (arXiv:2307.04726v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.04726](http://arxiv.org/abs/2307.04726)

    该论文介绍了一种名为状态重构扩散策略 (SRDP) 的新方法，该方法在最新的扩散策略类中引入了状态重构特征学习，以解决脱机强化学习中的分布偏移和有效表示策略的问题。

    

    脱机强化学习 (RL) 方法利用以前的经验来学习比用于数据收集的行为策略更好的策略。与行为克隆相反，行为克隆假设数据是从专家演示中收集的，而脱机 RL 可以使用非专家数据和多模态行为策略。然而，脱机 RL 算法在处理分布偏移和有效表示策略方面面临挑战，因为训练过程中缺乏在线交互。先前关于脱机 RL 的工作使用条件扩散模型来表示数据集中的多模态行为。然而，这些方法并没有针对缓解脱机分布状态泛化而制定。我们介绍了一种新的方法，名为状态重构扩散策略 (SRDP)，将状态重构特征学习纳入到最新的扩散策略类中，以解决脱机分布通用化问题。状态重构损失促进了更详细的描述。

    Offline Reinforcement Learning (RL) methods leverage previous experiences to learn better policies than the behavior policy used for data collection. In contrast to behavior cloning, which assumes the data is collected from expert demonstrations, offline RL can work with non-expert data and multimodal behavior policies. However, offline RL algorithms face challenges in handling distribution shifts and effectively representing policies due to the lack of online interaction during training. Prior work on offline RL uses conditional diffusion models to represent multimodal behavior in the dataset. Nevertheless, these methods are not tailored toward alleviating the out-of-distribution state generalization. We introduce a novel method, named State Reconstruction for Diffusion Policies (SRDP), incorporating state reconstruction feature learning in the recent class of diffusion policies to address the out-of-distribution generalization problem. State reconstruction loss promotes more descript
    
[^104]: 基于数据驱动的预测性5G延迟：利用网络测量进行理论和实验分析

    Data-driven Predictive Latency for 5G: A Theoretical and Experimental Analysis Using Network Measurements. (arXiv:2307.02329v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2307.02329](http://arxiv.org/abs/2307.02329)

    本研究利用实际网络数据对5G网络中的预测性延迟进行了全面分析，并提出了以Hypoexponential分布为基础的用户面延迟的分析表达式。通过机器学习领域的贝叶斯学习和图机器学习技术，我们进行了概率回归、异常检测和预测性预测的实验。测试结果表明，该预测框架适用于不同情景下的移动性、城市交通和社交聚会。

    

    随着5G服务和应用的出现，具有绑定延迟要求和保证服务质量（QoS）的需求迫使网络管理程序纳入自主和主动决策。我们研究的目标是利用移动网络运营商（MNOs）可以访问的真实网络数据，对5G网络中的预测性延迟进行全面分析。具体而言，（i）我们提出了作为Hypoexponential分布的用户面延迟的分析表达式，并通过与实证测量的比较分析进行验证，（ii）我们进行了概率回归、异常检测和预测性预测的实验结果，利用了机器学习领域中的新兴领域，如贝叶斯学习（BL）和图机器学习（GML）。我们使用从车辆移动、密集城区交通和社交聚会场景中收集的数据测试我们的预测框架。

    The advent of novel 5G services and applications with binding latency requirements and guaranteed Quality of Service (QoS) hastened the need to incorporate autonomous and proactive decision-making in network management procedures. The objective of our study is to provide a thorough analysis of predictive latency within 5G networks by utilizing real-world network data that is accessible to mobile network operators (MNOs). In particular, (i) we present an analytical formulation of the user-plane latency as a Hypoexponential distribution, which is validated by means of a comparative analysis with empirical measurements, and (ii) we conduct experimental results of probabilistic regression, anomaly detection, and predictive forecasting leveraging on emerging domains in Machine Learning (ML), such as Bayesian Learning (BL) and Machine Learning on Graphs (GML). We test our predictive framework using data gathered from scenarios of vehicular mobility, dense-urban traffic, and social gathering 
    
[^105]: 使用可学习间距的膨胀卷积学习尖峰神经网络中的延迟

    Learning Delays in Spiking Neural Networks using Dilated Convolutions with Learnable Spacings. (arXiv:2306.17670v1 [cs.NE])

    [http://arxiv.org/abs/2306.17670](http://arxiv.org/abs/2306.17670)

    本文提出了一种新的离散时间算法，通过反向传播学习尖峰神经网络(SNNs)中的延迟，提高了SNNs在节能信息处理系统中的表达能力。

    

    尖峰神经网络(SNNs)是构建节能信息处理系统的一种有前途的研究方向，特别适用于如语音识别等时间任务。在SNNs中，延迟指的是从一个神经元到另一个神经元传播需要的时间。这些延迟很重要，因为它们影响脉冲到达时间，已知尖峰神经元对于重叠的输入脉冲有更强的响应。更正式地说，理论上已经证明可塑性延迟极大增加了SNNs的表达能力。然而，目前缺乏有效的算法来学习这些延迟。在这里，我们提出了一种新的离线离散时间算法，用于通过反向传播在深度前馈SNNs中解决这个问题。为了模拟连续层之间的延迟，我们使用了沿时间轴的一维卷积。卷积核仅包含少数非零权重 - 每个突触一个 - 它们的位置对应于延迟。这些位置与权重一起被学习。

    Spiking Neural Networks (SNNs) are a promising research direction for building power-efficient information processing systems, especially for temporal tasks such as speech recognition. In SNNs, delays refer to the time needed for one spike to travel from one neuron to another. These delays matter because they influence the spike arrival times, and it is well-known that spiking neurons respond more strongly to coincident input spikes. More formally, it has been shown theoretically that plastic delays greatly increase the expressivity in SNNs. Yet, efficient algorithms to learn these delays have been lacking. Here, we propose a new discrete-time algorithm that addresses this issue in deep feedforward SNNs using backpropagation, in an offline manner. To simulate delays between consecutive layers, we use 1D convolutions across time. The kernels contain only a few non-zero weights - one per synapse - whose positions correspond to the delays. These positions are learned together with the wei
    
[^106]: 用于高效网格分割的神经形状直径函数

    Neural Shape Diameter Function for Efficient Mesh Segmentation. (arXiv:2306.11737v1 [cs.GR])

    [http://arxiv.org/abs/2306.11737](http://arxiv.org/abs/2306.11737)

    该论文提出了一种利用深度学习在网格分割之前编码映射函数的数据驱动方法，可以用于多种应用，不受分辨率影响。

    

    将多边形网格分割为有意义的部分可能具有挑战性。许多应用程序需要将这些结构分解以进行计算机图形学中的进一步处理。在过去的十年中，提出了几种方法来解决这个问题，但代价是计算时间的大量消耗。最近，机器学习已被证明对于对3D结构的分割任务是有效的。然而，这些最先进的方法往往难以推广，并需要将学习的模型分成几个特定的对象类别，以避免过度拟合。我们提出了一种数据驱动的方法，利用深度学习在网格分割之前编码映射函数，以用于多种应用。我们的网络使用顶点邻域之间的相似性复现邻域图，利用我们对形状直径函数（SDF）方法的了解。我们的方法不受分辨率影响，因为我们对输入网格进行下采样，并仅针对邻居查询完整分辨率结构。

    Partitioning a polygonal mesh into meaningful parts can be challenging. Many applications require decomposing such structures for further processing in computer graphics. In the last decade, several methods were proposed to tackle this problem, at the cost of intensive computational times. Recently, machine learning has proven to be effective for the segmentation task on 3D structures. Nevertheless, these state-of-the-art methods are often hardly generalizable and require dividing the learned model into several specific classes of objects to avoid overfitting. We present a data-driven approach leveraging deep learning to encode a mapping function prior to mesh segmentation for multiple applications. Our network reproduces a neighborhood map using our knowledge of the \textsl{Shape Diameter Function} (SDF) method using similarities among vertex neighborhoods. Our approach is resolution-agnostic as we downsample the input meshes and query the full-resolution structure solely for neighbor
    
[^107]: 非线性个性化预测的神经混合效应

    Neural Mixed Effects for Nonlinear Personalized Predictions. (arXiv:2306.08149v1 [cs.LG])

    [http://arxiv.org/abs/2306.08149](http://arxiv.org/abs/2306.08149)

    本文提出了神经混合效应（NME）模型，用于个性化预测，并通过结合个人通用和个人特定参数来考虑线性和非线性趋势。

    

    个性化预测是一种机器学习方法，根据过去标记观测预测一个人未来的观测值，通常用于连续任务，例如预测日常情绪评分。在进行个性化预测时，模型可以结合两种趋势：（a）跨人共享的趋势，即个人通用趋势，例如周末更开心，和（b）每个人独特的趋势，即个人特定的趋势，例如每周有一次压力大的会议。混合效应模型是一种流行的统计模型，用于通过组合个人通用和个人特定参数来研究这两种趋势。尽管现在线性混合效应模型通过将其与神经网络整合而变得越来越流行，但这种整合目前仅限于线性个人特定参数：排除非线性个人特定趋势。在本文中，我们提出了神经混合效应（NME）模型，以优化非线性个人特定参数。

    Personalized prediction is a machine learning approach that predicts a person's future observations based on their past labeled observations and is typically used for sequential tasks, e.g., to predict daily mood ratings. When making personalized predictions, a model can combine two types of trends: (a) trends shared across people, i.e., person-generic trends, such as being happier on weekends, and (b) unique trends for each person, i.e., person-specific trends, such as a stressful weekly meeting. Mixed effect models are popular statistical models to study both trends by combining person-generic and person-specific parameters. Though linear mixed effect models are gaining popularity in machine learning by integrating them with neural networks, these integrations are currently limited to linear person-specific parameters: ruling out nonlinear person-specific trends. In this paper, we propose Neural Mixed Effect (NME) models to optimize nonlinear person-specific parameters anywhere in a 
    
[^108]: 提高决策树解释性的有效性

    Improving the Validity of Decision Trees as Explanations. (arXiv:2306.06777v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.06777](http://arxiv.org/abs/2306.06777)

    该论文介绍了一个新的决策树模型，利用挂起的树的方式提高了其解释性和统计性能，达到了无限深度决策树的水平，并可与XGBoost等最先进的方法相媲美。

    

    在基于表格数据的分类和预测中，人们经常使用基于树的模型。这可以在表格数据上与深度神经网络竞争[参见Grinsztajn等人，NeurIPS 2022，arXiv：2207.08815]，并且在某些条件下是可解释的。可解释性取决于树的深度和每个叶节点的准确性。在这里，我们训练了一个低深度的树，其目标是最小化每个叶节点上的最大错误分类，并从低深度树的每个叶节点“挂起”进一步的基于树的模型（例如无限深度的树）。低深度树易于解释，而综合低深度和挂起的基于树的模型的整体统计性能优于使用经典方法（例如CART）训练的无限深度决策树，并且与最先进的方法（例如优化的XGBoost）相当。

    In classification and forecasting with tabular data, one often utilizes tree-based models. This can be competitive with deep neural networks on tabular data [cf. Grinsztajn et al., NeurIPS 2022, arXiv:2207.08815] and, under some conditions, explainable. The explainability depends on the depth of the tree and the accuracy in each leaf of the tree. Here, we train a low-depth tree with the objective of minimising the maximum misclassification error across each leaf node, and then ``suspend'' further tree-based models (e.g., trees of unlimited depth) from each leaf of the low-depth tree. The low-depth tree is easily explainable, while the overall statistical performance of the combined low-depth and suspended tree-based models improves upon decision trees of unlimited depth trained using classical methods (e.g., CART) and is comparable to state-of-the-art methods (e.g., well-tuned XGBoost).
    
[^109]: 强化学习中多样性重放在泛化中的作用

    The Role of Diverse Replay for Generalisation in Reinforcement Learning. (arXiv:2306.05727v1 [cs.LG])

    [http://arxiv.org/abs/2306.05727](http://arxiv.org/abs/2306.05727)

    本文研究了在多任务强化学习中，增加重放缓存中数据过渡的多样性可以提高零-shot泛化性能，并且可能通过提高潜在表示的泛化性能来实现这种改善。

    

    在强化学习中，探索策略和重放缓存是其许多算法的关键组成部分。这些策略规定了要收集和训练哪些环境数据，并在强化学习文献中得到了广泛研究。本文旨在研究这些组件在多任务强化学习中泛化时的影响。我们研究了一个假设：从训练环境中收集和训练更多样化的数据将提高到新环境/任务的零-shot泛化性能。我们通过数学推导和实证研究证明，增加重放缓存中过渡的多样性可以提高到训练过程中“可达到”的状态的泛化性能。此外，我们还证明了这种策略对于类似但“不可达”状态的泛化性能也有所提高，并且可能是由于潜在表示的泛化性能得到了改善。

    In reinforcement learning (RL), key components of many algorithms are the exploration strategy and replay buffer. These strategies regulate what environment data is collected and trained on and have been extensively studied in the RL literature. In this paper, we investigate the impact of these components in the context of generalisation in multi-task RL. We investigate the hypothesis that collecting and training on more diverse data from the training environment will improve zero-shot generalisation to new environments/tasks. We motivate mathematically and show empirically that generalisation to states that are "reachable" during training is improved by increasing the diversity of transitions in the replay buffer. Furthermore, we show empirically that this same strategy also shows improvement for generalisation to similar but "unreachable" states and could be due to improved generalisation of latent representations.
    
[^110]: 混合类型数据的核度量学习

    Kernel Metric Learning for Clustering Mixed-type Data. (arXiv:2306.01890v1 [cs.LG])

    [http://arxiv.org/abs/2306.01890](http://arxiv.org/abs/2306.01890)

    提出了一种使用混合核测量不相似性的度量方法，并通过交叉验证找到最佳核带宽。该方法可为现有的基于距离的聚类算法提高聚类准确度，适用于包含混合类型数据的模拟和实际数据集。

    

    基于距离的聚类和分类广泛应用于各个领域，以将混合数值和分类数据分组。预定义的距离测量用于根据它们的不相似性来聚类数据点。虽然存在许多适用于具有纯数字属性和几个有序和无序分类指标的数据的基于距离的度量方法，但混合型数据的最佳距离是一个尚未解决的问题。许多度量将数字属性转换为分类属性或反之亦然。他们将数据点处理为单个属性类型，或者分别计算每个属性之间的距离并将它们相加。我们提出了一种度量方法，使用混合核测量不相似性，并进行交叉验证来寻找最佳核带宽。我们的方法对包含纯连续，分类和混合类型数据的模拟和实际数据集应用于现有的基于距离的聚类算法时，提高了聚类准确度。

    Distance-based clustering and classification are widely used in various fields to group mixed numeric and categorical data. A predefined distance measurement is used to cluster data points based on their dissimilarity. While there exist numerous distance-based measures for data with pure numerical attributes and several ordered and unordered categorical metrics, an optimal distance for mixed-type data is an open problem. Many metrics convert numerical attributes to categorical ones or vice versa. They handle the data points as a single attribute type or calculate a distance between each attribute separately and add them up. We propose a metric that uses mixed kernels to measure dissimilarity, with cross-validated optimal kernel bandwidths. Our approach improves clustering accuracy when utilized for existing distance-based clustering algorithms on simulated and real-world datasets containing pure continuous, categorical, and mixed-type data.
    
[^111]: 在生物医学领域中的知识图谱嵌入：它们有用吗？对连接预测、规则学习和下游多药物任务的探究

    Knowledge Graph Embeddings in the Biomedical Domain: Are They Useful? A Look at Link Prediction, Rule Learning, and Downstream Polypharmacy Tasks. (arXiv:2305.19979v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19979](http://arxiv.org/abs/2305.19979)

    该论文研究了在生物医学领域中知识图谱嵌入的有效性和限制，通过在生物医学知识图谱上的实验，证明了最先进的模型在性能和下游用途方面的优势，同时提供了可解释的预测结果。

    

    知识图谱是表示和组织复杂生物医学数据的强大工具。已经提出了多种知识图谱嵌入算法来学习和完善知识图谱。然而，最近的一项研究表明，当应用于生物医学知识图谱时，这些嵌入算法的有效性有限，引发了对知识图谱嵌入在生物医学环境中是否存在限制的疑问。本研究旨在将最先进的知识图谱嵌入模型应用于最近的生物医学知识图谱BioKG，评估其性能和潜在的下游用途。在相同的生物医学知识图谱上，我们在HITS@10得分方面的性能改进了三倍。此外，我们通过基于规则的方法提供可解释的预测。通过对四个任务上表现最佳的模型进行评估，我们证明了知识图谱嵌入模型在实践中是可应用的。

    Knowledge graphs are powerful tools for representing and organising complex biomedical data. Several knowledge graph embedding algorithms have been proposed to learn from and complete knowledge graphs. However, a recent study demonstrates the limited efficacy of these embedding algorithms when applied to biomedical knowledge graphs, raising the question of whether knowledge graph embeddings have limitations in biomedical settings. This study aims to apply state-of-the-art knowledge graph embedding models in the context of a recent biomedical knowledge graph, BioKG, and evaluate their performance and potential downstream uses. We achieve a three-fold improvement in terms of performance based on the HITS@10 score over previous work on the same biomedical knowledge graph. Additionally, we provide interpretable predictions through a rule-based method. We demonstrate that knowledge graph embedding models are applicable in practice by evaluating the best-performing model on four tasks that r
    
[^112]: 通过MCTS进行前列腺MRI分割的动态数据增强

    Dynamic Data Augmentation via MCTS for Prostate MRI Segmentation. (arXiv:2305.15777v1 [eess.IV])

    [http://arxiv.org/abs/2305.15777](http://arxiv.org/abs/2305.15777)

    提出了一种动态数据增强（DDAug）的方法，该方法使用高效的蒙特卡罗树搜索算法来学习不同数据集的有利增强策略，有效且计算代价可忽略不计。

    

    医学图像数据通常由于昂贵的获取和注释过程而受到限制。因此，只使用原始数据训练深度学习模型很容易导致过拟合。解决这个问题的一种方法是使用各种转换来增强原始数据，提高模型对新数据的推广能力。然而，由于不一致的获取方法和数据分布，为不同数据集手动配置通用增强组合和参数并不容易，因此提出了自动数据增强来学习不同数据集的有利增强策略，但会产生大量GPU开销。为此，我们提出了一种新颖的方法，称为动态数据增强（DDAug），该方法高效且计算代价可忽略不计。我们的DDAug开发了一种分层树结构来表示各种增强，并利用高效的蒙特卡罗树搜索算法来更新、修剪和抽样树。因此，

    Medical image data are often limited due to the expensive acquisition and annotation process. Hence, training a deep-learning model with only raw data can easily lead to overfitting. One solution to this problem is to augment the raw data with various transformations, improving the model's ability to generalize to new data. However, manually configuring a generic augmentation combination and parameters for different datasets is non-trivial due to inconsistent acquisition approaches and data distributions. Therefore, automatic data augmentation is proposed to learn favorable augmentation strategies for different datasets while incurring large GPU overhead. To this end, we present a novel method, called Dynamic Data Augmentation (DDAug), which is efficient and has negligible computation cost. Our DDAug develops a hierarchical tree structure to represent various augmentations and utilizes an efficient Monte-Carlo tree searching algorithm to update, prune, and sample the tree. As a result,
    
[^113]: 利用Riesz核的生成式分割MMD流

    Generative Sliced MMD Flows with Riesz Kernels. (arXiv:2305.11463v1 [cs.LG])

    [http://arxiv.org/abs/2305.11463](http://arxiv.org/abs/2305.11463)

    本文使用Riesz核展示了生成式分割MMD流的高效计算方法，实现了在大规模应用中通过神经网络训练生成模型。

    

    在大规模计算中，最大平均差异度(MMD)流的计算成本很高。在本文中，我们展示了使用Riesz核$K(x,y)=-\|x-y\|^r$，$r \in (0,2)$的MMD流具有杰出的性质，可允许其进行高效计算。首先，Riesz核的MMD与其分割版本的MMD重合。因此，可以在一维设置中进行MMD梯度的计算。在此处，对于$r=1$，可以应用简单的排序算法将两个经验度量的复杂度从$O(MN+N^2)$降低到$O((M+N)\log(M+N))$，其中$M$和$N$是支持点。对于实现，我们通过仅使用有限数量的$P$个切片来近似分割MMD的梯度。我们展示了由此产生的误差具有$O(\sqrt{d/P})$的复杂度，其中$d$是数据维度。这些结果使我们能够通过神经网络近似MMD梯度流来训练生成模型，甚至用于大规模应用。

    Maximum mean discrepancy (MMD) flows suffer from high computational costs in large scale computations. In this paper, we show that MMD flows with Riesz kernels $K(x,y) = - \|x-y\|^r$, $r \in (0,2)$ have exceptional properties which allow for their efficient computation. First, the MMD of Riesz kernels coincides with the MMD of their sliced version. As a consequence, the computation of gradients of MMDs can be performed in the one-dimensional setting. Here, for $r=1$, a simple sorting algorithm can be applied to reduce the complexity from $O(MN+N^2)$ to $O((M+N)\log(M+N))$ for two empirical measures with $M$ and $N$ support points. For the implementations we approximate the gradient of the sliced MMD by using only a finite number $P$ of slices. We show that the resulting error has complexity $O(\sqrt{d/P})$, where $d$ is the data dimension. These results enable us to train generative models by approximating MMD gradient flows by neural networks even for large scale applications. We demo
    
[^114]: pTSE:一种用于概率时间序列预测的多模型集成方法

    pTSE: A Multi-model Ensemble Method for Probabilistic Time Series Forecasting. (arXiv:2305.11304v1 [cs.LG])

    [http://arxiv.org/abs/2305.11304](http://arxiv.org/abs/2305.11304)

    提出了pTSE，一种基于隐马尔可夫模型的概率预测的多模型分布集成方法，实现了对时间序列的鲁棒性和准确性的提高。

    

    出现了各种概率时间序列预测模型并表现出了卓越的性能。然而，模型的选择高度依赖于输入时间序列的特征和模型基于的固定分布。由于概率分布不能直接平均不同模型，目前的时间序列模型集成方法不能直接用于提高预测的鲁棒性和准确性。为了解决这个问题，我们提出了pTSE，一种基于隐马尔可夫模型的概率预测的多模型分布集成方法。pTSE只需从成员模型获取现成输出，不需要进一步了解每个模型的信息。此外，我们对pTSE进行了完整的理论分析，证明了时间序列经HMM处理后的经验分布几乎一定收敛于稳态分布。基准实验显示，与单个模型和其他最新集成方法相比，pTSE在准确性和鲁棒性方面具有优越性。

    Various probabilistic time series forecasting models have sprung up and shown remarkably good performance. However, the choice of model highly relies on the characteristics of the input time series and the fixed distribution that the model is based on. Due to the fact that the probability distributions cannot be averaged over different models straightforwardly, the current time series model ensemble methods cannot be directly applied to improve the robustness and accuracy of forecasting. To address this issue, we propose pTSE, a multi-model distribution ensemble method for probabilistic forecasting based on Hidden Markov Model (HMM). pTSE only takes off-the-shelf outputs from member models without requiring further information about each model. Besides, we provide a complete theoretical analysis of pTSE to prove that the empirical distribution of time series subject to an HMM will converge to the stationary distribution almost surely. Experiments on benchmarks show the superiority of p
    
[^115]: 保留你自己的相关性：用于视频扩散模型的噪声先验

    Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models. (arXiv:2305.10474v1 [cs.CV])

    [http://arxiv.org/abs/2305.10474](http://arxiv.org/abs/2305.10474)

    本论文介绍了一种新的视频噪声先验，用于微调图像扩散模型，以实现更高质量的视频合成。经过广泛的实验验证，该模型已经取得了UCF-101和MSR-VTT基准测试的最佳结果。

    

    尽管扩散模型在生成高质量图像方面取得了巨大进展，但合成连续的动画帧，既具有光真实感，又具有时间相关性仍处于起步阶段。在可以使用成亿级图像数据集的同时，收集相似规模的视频数据仍然具有挑战性。此外，与其图像对应的模型相比，训练视频扩散模型的计算代价更高。在本文中，我们探讨了使用视频数据微调预训练的图像扩散模型作为视频合成任务的实用解决方案。我们发现，在视频扩散中天真地将图像噪声先验扩展为视频噪声先验会导致次优的性能。我们设计了一种精心设计的视频噪声先验，其在视频扩散中具有显著的更好性能。广泛的实验验证表明，我们的模型 Preserve Your Own Correlation (PYoCo) 在 UCF-101 和 MSR-VTT 基准测试中获得了零样本文本对视频的最佳结果。

    Despite tremendous progress in generating high-quality images using diffusion models, synthesizing a sequence of animated frames that are both photorealistic and temporally coherent is still in its infancy. While off-the-shelf billion-scale datasets for image generation are available, collecting similar video data of the same scale is still challenging. Also, training a video diffusion model is computationally much more expensive than its image counterpart. In this work, we explore finetuning a pretrained image diffusion model with video data as a practical solution for the video synthesis task. We find that naively extending the image noise prior to video noise prior in video diffusion leads to sub-optimal performance. Our carefully designed video noise prior leads to substantially better performance. Extensive experimental validation shows that our model, Preserve Your Own Correlation (PYoCo), attains SOTA zero-shot text-to-video results on the UCF-101 and MSR-VTT benchmarks. It also
    
[^116]: MaxViT-UNet: 多轴注意力用于医学图像分割

    MaxViT-UNet: Multi-Axis Attention for Medical Image Segmentation. (arXiv:2305.08396v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2305.08396](http://arxiv.org/abs/2305.08396)

    MaxViT-UNet是基于编码器-解码器混合视觉变压器的医学图像分割模型，多轴自我关注机制在每个解码器阶段有助于更有效地区分对象和背景区域。

    

    近年来，卷积神经网络在医学图像分析方面取得了重大进展。然而，卷积算子的局部性质抑制了CNNs捕捉全局和长程交互。最近，Transformer在计算机视觉社区和医学图像分割中变得流行。但是，自我注意机制的可扩展性问题和缺乏CNN类归纳偏差限制了它们的应用。在本文中，我们提出了MaxViT-UNet，一种基于编码器-解码器混合视觉变压器的医学图像分割模型。提出的混合解码器，还基于MaxViT-block，旨在在每个解码阶段最小化计算负担下利用卷积和自我注意机制的力量。每个解码器阶段的多轴自我关注有助于更有效地区分对象和背景区域。混合解码器块最初通过上采样传输低层特征。

    Convolutional neural networks have made significant strides in medical image analysis in recent years. However, the local nature of the convolution operator inhibits the CNNs from capturing global and long-range interactions. Recently, Transformers have gained popularity in the computer vision community and also medical image segmentation. But scalability issues of self-attention mechanism and lack of the CNN like inductive bias have limited their adoption. In this work, we present MaxViT-UNet, an Encoder-Decoder based hybrid vision transformer for medical image segmentation. The proposed hybrid decoder, also based on MaxViT-block, is designed to harness the power of convolution and self-attention mechanism at each decoding stage with minimal computational burden. The multi-axis self-attention in each decoder stage helps in differentiating between the object and background regions much more efficiently. The hybrid decoder block initially fuses the lower level features upsampled via tra
    
[^117]: 当深度学习遇见多面体理论：一项综述

    When Deep Learning Meets Polyhedral Theory: A Survey. (arXiv:2305.00241v1 [math.OC])

    [http://arxiv.org/abs/2305.00241](http://arxiv.org/abs/2305.00241)

    本文综述了深度学习与多面体理论的交叉领域。修正线性单元（ReLU）等函数使得一些神经网络结构能够通过多面体理论进行分析，应用线性和混合整数线性规划来实现网络修剪、鲁棒性分析和神经网络验证等任务。

    

    在过去的十年中，深度学习成为了预测建模的主要方法，得益于深度神经网络在计算机视觉和自然语言处理等任务中的显著准确性。与此同时，神经网络的结构回归到了基于分段常数和分段线性函数的简单表示，例如修正线性单元（ReLU），这种激活函数成为神经网络中最常用的类型。这使得某些类型的网络结构，如典型的全连接前馈神经网络，能够通过多面体理论进行分析，并应用线性规划（LP）和混合整数线性规划（MILP）等方法用于各种目的。本文综述了这个快速发展领域涌现的主要主题，为更详细地了解神经网络以及应用数学提供了新的视角。我们介绍了多面体理论的基础知识以及它与深度学习的关系，并回顾了该主题的最新进展，包括在网络修剪、鲁棒性分析和神经网络验证等任务中使用LP和MILP。最后，我们讨论了当前挑战和未来研究方向。

    In the past decade, deep learning became the prevalent methodology for predictive modeling thanks to the remarkable accuracy of deep neural networks in tasks such as computer vision and natural language processing. Meanwhile, the structure of neural networks converged back to simpler representations based on piecewise constant and piecewise linear functions such as the Rectified Linear Unit (ReLU), which became the most commonly used type of activation function in neural networks. That made certain types of network structure $\unicode{x2014}$such as the typical fully-connected feedforward neural network$\unicode{x2014}$ amenable to analysis through polyhedral theory and to the application of methodologies such as Linear Programming (LP) and Mixed-Integer Linear Programming (MILP) for a variety of purposes. In this paper, we survey the main topics emerging from this fast-paced area of work, which bring a fresh perspective to understanding neural networks in more detail as well as to app
    
[^118]: 用神经网络求解初值偏微分方程的稳定可扩展方法

    A Stable and Scalable Method for Solving Initial Value PDEs with Neural Networks. (arXiv:2304.14994v1 [cs.LG])

    [http://arxiv.org/abs/2304.14994](http://arxiv.org/abs/2304.14994)

    本文提出了一种用神经网络求解初值偏微分方程的稳定可扩展方法，解决了在全局最小化神经网络参数中的 PDE 残差中遇到的灾难性遗忘和 ODE 方法中随着数量呈立方级别扩展等问题。

    

    与传统的网格和基于网格的方法不同，神经网络有可能打破维数灾难，在使用经典求解器困难或不可能的问题中提供近似解。全局最小化神经网络参数中的 PDE 残差对于边界值问题效果良好，但是灾难性忘却损害了这种方法对于初值问题的适用性。在替代的局部时间方法中，可以将优化问题转化为网络参数上的常微分方程（ODE），并将解向前传播。然而，我们证明了目前基于这种方法的方法存在两个关键问题。首先，遵循 ODE 会导致问题条件增长无法控制，最终导致不可接受的大数值误差。其次，随着 ODE 方法随着 m 的数量呈立方级别扩展。

    Unlike conventional grid and mesh based methods for solving partial differential equations (PDEs), neural networks have the potential to break the curse of dimensionality, providing approximate solutions to problems where using classical solvers is difficult or impossible. While global minimization of the PDE residual over the network parameters works well for boundary value problems, catastrophic forgetting impairs the applicability of this approach to initial value problems (IVPs). In an alternative local-in-time approach, the optimization problem can be converted into an ordinary differential equation (ODE) on the network parameters and the solution propagated forward in time; however, we demonstrate that current methods based on this approach suffer from two key issues. First, following the ODE produces an uncontrolled growth in the conditioning of the problem, ultimately leading to unacceptably large numerical errors. Second, as the ODE methods scale cubically with the number of m
    
[^119]: 基于Transformer的可解释多模态数据融合用于皮肤病分类

    Transformer-based interpretable multi-modal data fusion for skin lesion classification. (arXiv:2304.14505v1 [eess.IV])

    [http://arxiv.org/abs/2304.14505](http://arxiv.org/abs/2304.14505)

    本文提出了一种基于Transformer的可解释多模态数据融合算法，用于帮助皮肤疾病的诊断。

    

    当今许多深度学习（DL）研究主要集中在提高定量指标方面，而忽略了其他因素。在人类中心的应用领域，如皮肤病分类在皮肤科中，仍处于其初级阶段的DL驱动的临床决策支持系统，由于其决策过程的透明度有限。此外，缺乏能够解释训练的DL算法行为的程序几乎没有得到临床医师的信任。为诊断皮肤病变，皮肤科医生依靠疾病的视觉评估和患者病史收集的数据。处理多模态数据的数据驱动算法受限于卷积结构所需的特征级和决策级融合程序的分离。为解决这个问题，我们通过基于Transformer的架构的注意机制实现单阶段多模态数据融合，以帮助诊断皮肤疾病。

    A lot of deep learning (DL) research these days is mainly focused on improving on quantitative metrics regardless of other factors. In human centered applications, like skin lesion classification in dermatology, DL-driven clinical decision support systems are still in their infancy due to the limited transparency of their decision-making process. Moreover, the lack of procedures that can explain the behavior of trained DL algorithms leads to almost no trust from the clinical physicians. To diagnose skin lesions, dermatologists rely on both visual assessment of the disease and the data gathered from the anamnesis of the patient. Data-driven algorithms dealing with multi-modal data are limited by the separation of feature-level and decision-level fusion procedures required by convolutional architectures. To address this issue, we enable single-stage multi-modal data fusion via the attention mechanism of transformer-based architectures to aid in the diagnosis of skin diseases. Our method 
    
[^120]: 从混沌中迸发出秩序：为物体检测排序事件表示法

    From Chaos Comes Order: Ordering Event Representations for Object Detection. (arXiv:2304.13455v1 [cs.CV])

    [http://arxiv.org/abs/2304.13455](http://arxiv.org/abs/2304.13455)

    本文提出了一种基于Gromov-Wasserstein Discrepancy选择最佳事件表示的方法，这种方法可以在多个表示、网络骨干和数据集上保持任务性能排名的一致性。利用这一方法，本文对大型事件表示法家族进行超参数搜索，选择最适合物体检测的表示法，取得了优于最先进的基于事件的对象检测方法的成果。

    

    如今，处理事件的顶尖深度神经网络在使用现成网络之前，首先将其转换为稠密的网格状输入表示。然而，传统上为任务选择适当的表示需要针对每个表示训练一个神经网络，并根据验证分数选择最佳表示，这非常耗时。在这项工作中，我们通过基于原始事件及其表示之间的Gromov-Wasserstein Discrepancy (GWD)选择最佳表示来消除这个瓶颈。它的计算速度大约比训练神经网络快200倍，同时在多个表示、网络骨干和数据集上保持事件表示法任务性能排名的一致性。这意味着找到具有高任务分数的表示相当于找到具有低GWD的表示。我们利用这一观察结果，首次对大型事件表示法家族进行超参数搜索，选择最适合物体检测的表示。我们的方法在Moving MNIST和N-Caltech101数据集上都优于最先进的基于事件的对象检测方法，在后者达到了83.0%的1%误报率下的mAP新的最高水平。

    Today, state-of-the-art deep neural networks that process events first convert them into dense, grid-like input representations before using an off-the-shelf network. However, selecting the appropriate representation for the task traditionally requires training a neural network for each representation and selecting the best one based on the validation score, which is very time-consuming. In this work, we eliminate this bottleneck by selecting the best representation based on the Gromov-Wasserstein Discrepancy (GWD) between the raw events and their representation. It is approximately 200 times faster to compute than training a neural network and preserves the task performance ranking of event representations across multiple representations, network backbones, and datasets. This means that finding a representation with a high task score is equivalent to finding a representation with a low GWD. We use this insight to, for the first time, perform a hyperparameter search on a large family o
    
[^121]: 富文本生成表达性文本图像

    Expressive Text-to-Image Generation with Rich Text. (arXiv:2304.06720v1 [cs.CV])

    [http://arxiv.org/abs/2304.06720](http://arxiv.org/abs/2304.06720)

    本文提出了一种使用富文本编辑器生成表达性文本图像的方法，可以通过局部样式控制、明确的标记重新加权、精确的颜色渲染和详细的区域合成，生成高质量且多样化的图像。

    

    纯文本已经成为文字到图像合成的流行界面。但是，它的定制选项有限，阻碍了用户精确描述所需的输出。为了解决这些挑战，我们提出使用支持字体样式、大小、颜色和脚注等格式的富文本编辑器。我们从富文本中提取每个字的属性，以启用局部样式控制、明确的标记重新加权、精确的颜色渲染和详细的区域合成。我们通过基于区域的扩散过程实现了这些功能。我们的实验表明，我们的方法可以比现有的文本到图像方法更好地生成高质量和多样化的图像。

    Plain text has become a prevalent interface for text-to-image synthesis. However, its limited customization options hinder users from accurately describing desired outputs. For example, plain text makes it hard to specify continuous quantities, such as the precise RGB color value or importance of each word. Furthermore, creating detailed text prompts for complex scenes is tedious for humans to write and challenging for text encoders to interpret. To address these challenges, we propose using a rich-text editor supporting formats such as font style, size, color, and footnote. We extract each word's attributes from rich text to enable local style control, explicit token reweighting, precise color rendering, and detailed region synthesis. We achieve these capabilities through a region-based diffusion process. We first obtain each word's region based on cross-attention maps of a vanilla diffusion process using plain text. For each region, we enforce its text attributes by creating region-s
    
[^122]: 知识增强的图神经网络

    Knowledge Enhanced Graph Neural Networks. (arXiv:2303.15487v1 [cs.AI])

    [http://arxiv.org/abs/2303.15487](http://arxiv.org/abs/2303.15487)

    KeGNN是一个神经符号框架，可以结合先前的知识来优化图数据上的节点分类和链接预测任务。

    

    图数据是无处不在的，并且具有各种应用，例如自然科学、社交网络或语义网。尽管富含信息，但图形通常噪声和不完整。因此，图补全任务，如节点分类或链接预测，已经受到关注。一方面，神经方法（如图神经网络）已经被证明是处理噪声图的稳健工具。另一方面，符号方法可以对图进行精确推理。我们提出了KeGNN，这是一个用于在图数据上学习的神经符号框架，结合了两种范例，并允许将先前的知识集成到图神经网络模型中。从本质上讲，KeGNN由一个图神经网络组成，其中基于目标将知识增强层堆叠在其上，以使针对先前知识的预测得到优化。我们将KeGNN与两个标准图神经网络：图卷积网络和图注意力网络一起实例化。实验结果表明，将先前的知识集成到图神经网络模型中可以提高节点分类和链接预测任务的准确性。

    Graph data is omnipresent and has a large variety of applications such as natural science, social networks or semantic web. Though rich in information, graphs are often noisy and incomplete. Therefore, graph completion tasks such as node classification or link prediction have gained attention. On the one hand, neural methods such as graph neural networks have proven to be robust tools for learning rich representations of noisy graphs. On the other hand, symbolic methods enable exact reasoning on graphs. We propose KeGNN, a neuro-symbolic framework for learning on graph data that combines both paradigms and allows for the integration of prior knowledge into a graph neural network model. In essence, KeGNN consists of a graph neural network as a base on which knowledge enhancement layers are stacked with the objective of refining predictions with respect to prior knowledge. We instantiate KeGNN in conjunction with two standard graph neural networks: Graph Convolutional Networks and Graph 
    
[^123]: 通过BACKslash实现BACK substitution的BACKpropagation线性代数公式

    BACKpropagation through BACK substitution with a BACKslash. (arXiv:2303.15449v1 [math.NA])

    [http://arxiv.org/abs/2303.15449](http://arxiv.org/abs/2303.15449)

    本文提出了一种通过在三角方程组上使用通用的“backslash”或高斯消元来计算梯度的反向传播的线性代数公式，可以实现优化和实现便利性。

    

    我们提出了一种通过在三角方程组上使用通用的“backslash”或高斯消元来计算梯度的反向传播的线性代数公式。通常，矩阵元素是算子。这篇论文有三个贡献：1.用左作用的算子理论和基于图形的方法取代传统的自动微分处理具有知识价值。2.可以将算子放置在矩阵中作为实现选项的软件中，如Julia。3.我们引入了一种新的符号，“transpose dot”操作符“$\{\}^{T_\bullet}$”，允许反转算子。我们展示了算子方法在适合的编程语言中（如Julia）的优雅性，并证明了这种抽象可以在代码中实现。我们的实施展示了通用线性代数如何实现传统特殊形式的操作，例如backpropagation，以矩阵操作的形式书写，这打开了优化和实施便利性的可能性。

    We present a linear algebra formulation of backpropagation which allows the calculation of gradients by using a generically written ``backslash'' or Gaussian elimination on triangular systems of equations. Generally the matrix elements are operators. This paper has three contributions:  1. It is of intellectual value to replace traditional treatments of automatic differentiation with a (left acting) operator theoretic, graph-based approach.  2. Operators can be readily placed in matrices in software in programming languages such as Ju lia as an implementation option.  3. We introduce a novel notation, ``transpose dot'' operator ``$\{\}^{T_\bullet}$'' that allows the reversal of operators.  We demonstrate the elegance of the operators approach in a suitable programming language consisting of generic linear algebra operators such as Julia \cite{bezanson2017julia}, and that it is possible to realize this abstraction in code. Our implementation shows how generic linear algebra can allow op
    
[^124]: DR.CPO：通过迭代构建、随机放置和 HPR 遮蔽实现的多样化和逼真的三维增强

    DR.CPO: Diversified and Realistic 3D Augmentation via Iterative Construction, Random Placement, and HPR Occlusion. (arXiv:2303.12743v1 [cs.CV])

    [http://arxiv.org/abs/2303.12743](http://arxiv.org/abs/2303.12743)

    该论文提出了一种多样化和逼真的增强方法，可以创建整体对象并灵活地定位和旋转对象，并相应地应用自遮挡和外遮挡。通过迭代构建多个对象来提高整体对象构造的多样性，构造的对象可以在训练帧中随机放置和旋转。

    

    在自动驾驶中，数据增强常用于改进三维物体检测。最基本的方法包括插入复制对象和旋转和缩放整个训练帧。也已经开发了许多变体。然而，现有方法与现实世界的可能性相比相当有限。在这项工作中，我们开发了一种多样化和逼真增强方法，可以灵活地构造整体对象，自由地定位和旋转对象，并相应地应用自遮挡和外遮挡。为了提高整体对象构造的多样性，我们开发了一种迭代方法，将从现实世界观察到的多个对象随机组合成单个对象。与现有增强方法不同的是，构造的对象可以随机放置和旋转在训练帧中，因为适当的遮挡可以反映在最终整体对象中。最后，为了防止过度增强导致过拟合，我们介绍了一种分层遮挡概率设置，通过对象的位置和大小调整遮挡强度。

    In autonomous driving, data augmentation is commonly used for improving 3D object detection. The most basic methods include insertion of copied objects and rotation and scaling of the entire training frame. Numerous variants have been developed as well. The existing methods, however, are considerably limited when compared to the variety of the real world possibilities. In this work, we develop a diversified and realistic augmentation method that can flexibly construct a whole-body object, freely locate and rotate the object, and apply self-occlusion and external-occlusion accordingly. To improve the diversity of the whole-body object construction, we develop an iterative method that stochastically combines multiple objects observed from the real world into a single object. Unlike the existing augmentation methods, the constructed objects can be randomly located and rotated in the training frame because proper occlusions can be reflected to the whole-body objects in the final step. Fina
    
[^125]: 从飞行轨迹和程序中推断终端空域交通模型

    Inferring Traffic Models in Terminal Airspace from Flight Tracks and Procedures. (arXiv:2303.09981v1 [cs.LG])

    [http://arxiv.org/abs/2303.09981](http://arxiv.org/abs/2303.09981)

    本文提出了一种通过收集飞行轨迹和程序数据学习飞行器行为变异性的概率模型，并且可以生成涉及任意数量飞行器的交通模型。

    

    真实的航空器轨迹模型对于空中交通管理（ATM）系统设计和验证很有用。仪表飞行规则（IFR）下操作的飞行器模型需要捕捉飞行器按照标准飞行程序的固有变异性。飞行器行为的变异性在不同的飞行阶段之间各不相同。本文提出了一种概率模型，可以从程序数据和从雷达监视数据收集的飞行轨迹中学习变异性。 对于每个段落，使用高斯混合模型学习飞行器轨迹与其程序之间的偏差。给定新的程序，我们可以通过从经过训练的高斯分布中抽样一系列偏差，并使用偏差和程序重构飞行器轨迹来生成合成轨迹。我们将这种方法扩展到捕捉飞行器之间的成对相关性，并展示如何使用成对模型来生成涉及任意数量飞行器的交通模型。

    Realistic aircraft trajectory models are useful in the design and validation of air traffic management (ATM) systems. Models of aircraft operated under instrument flight rules (IFR) require capturing the variability inherent in how aircraft follow standard flight procedures. The variability in aircraft behavior varies among flight stages. In this paper, we propose a probabilistic model that can learn the variability from the procedural data and flight tracks collected from radar surveillance data. For each segment, a Gaussian mixture model is used to learn the deviations of aircraft trajectories from their procedures. Given new procedures, we can generate synthetic trajectories by sampling a series of deviations from the trained Gaussian distributions and reconstructing the aircraft trajectory using the deviations and the procedures. We extend this method to capture pairwise correlations between aircraft and show how a pairwise model can be used to generate traffic involving an arbitra
    
[^126]: 敏感度感知的视觉参数低效调整

    Sensitivity-Aware Visual Parameter-Efficient Tuning. (arXiv:2303.08566v1 [cs.CV])

    [http://arxiv.org/abs/2303.08566](http://arxiv.org/abs/2303.08566)

    本文提出了敏感度感知的视觉参数低效调整（SPT）方案，可以自适应地将可训练参数分配到任务特定的重要位置，以提高表示能力，适应预训练视觉模型到下游任务。

    

    视觉参数低效调整（VPET）已成为自适应预训练视觉模型到下游任务的强劲替代方法。现有VPET方法根据人工启发式方法将可训练参数引入不同任务的相同位置，忽略领域差异。本文提出了一种新颖的敏感度感知的视觉参数低效调整（SPT）方案，以自适应的方式分配可训练参数到任务特定的重要位置，给定所需的可调参数预算。本文首先依据数据的相关性快速识别特定任务所需调整的敏感参数，然后提升表示能力，增大重要的权重矩阵数量。

    Visual Parameter-Efficient Tuning (VPET) has become a powerful alternative for full fine-tuning so as to adapt pre-trained vision models to downstream tasks, which only tunes a small number of parameters while freezing the vast majority ones to ease storage burden and optimization difficulty. However, existing VPET methods introduce trainable parameters to the same positions across different tasks depending solely on human heuristics and neglect the domain gaps. To this end, we study where to introduce and how to allocate trainable parameters by proposing a novel Sensitivity-aware visual Parameter-efficient Tuning (SPT) scheme, which adaptively allocates trainable parameters to task-specific important positions given a desired tunable parameter budget. Specifically, our SPT first quickly identifies the sensitive parameters that require tuning for a given task in a data-dependent way. Next, our SPT further boosts the representational capability for the weight matrices whose number of se
    
[^127]: StyleDiff: 在潜在解缠空间中比较未标记数据集的属性差异

    StyleDiff: Attribute Comparison Between Unlabeled Datasets in Latent Disentangled Space. (arXiv:2303.05102v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.05102](http://arxiv.org/abs/2303.05102)

    StyleDiff是一种在潜在解缠空间中比较未标记数据集属性差异的方法，可以帮助开发人员了解两个数据集的差异，并以易于理解的方式提供分析。方法具有高效性能和准确性。

    

    机器学习应用中的一个主要挑战是解决开发中使用的数据集与实际应用中获取的数据集之间的不匹配。这些不匹配可能导致预测不准确和错误，进而影响产品质量和系统的可靠性。本研究提出了StyleDiff，以便开发人员了解两个数据集之间的差异，以实现机器学习系统的稳定发展。使用最近提出的生成模型获得的解缠图像空间，StyleDiff通过关注图像中的属性来比较两个数据集，并提供易于理解的差异分析。所提出的StyleDiff的性能为$O(dN\log N)$，其中$N$是数据集的大小，$d$是属性的数量，可以应用于大型数据集。我们证明StyleDiff能准确检测数据集之间的差异，并以易于理解的格式呈现。

    One major challenge in machine learning applications is coping with mismatches between the datasets used in the development and those obtained in real-world applications. These mismatches may lead to inaccurate predictions and errors, resulting in poor product quality and unreliable systems. In this study, we propose StyleDiff to inform developers of the differences between the two datasets for the steady development of machine learning systems. Using disentangled image spaces obtained from recently proposed generative models, StyleDiff compares the two datasets by focusing on attributes in the images and provides an easy-to-understand analysis of the differences between the datasets. The proposed StyleDiff performs in $O (d N\log N)$, where $N$ is the size of the datasets and $d$ is the number of attributes, enabling the application to large datasets. We demonstrate that StyleDiff accurately detects differences between datasets and presents them in an understandable format using, for 
    
[^128]: 蒙太奇扩散

    Collage Diffusion. (arXiv:2303.00262v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.00262](http://arxiv.org/abs/2303.00262)

    Collage Diffusion通过图层建模和协调技术实现了对扩散图像生成的精确控制，用户可以在每个对象上调整图像协调程度，并可以在保持其他对象固定的情况下编辑单个对象。

    

    我们通过将复杂场景建模为图层序列，为用户提供对基于扩散的图像生成具有精确控制的能力，这些图层定义了场景中对象的期望空间布置和视觉属性。蒙太奇扩散使输入图层协调一致，使对象互相适应 - 关键挑战在于在协调过程中最小化输入图层的位置和主要视觉属性的变化，同时允许其他属性发生变化。我们通过修改文本-图像交叉注意力与图层的alpha掩模来确保对象在正确位置生成。通过学习每个图层的专门文本表示，并扩展ControlNet以操作图层，我们可以保留输入图层的关键视觉属性。图层输入允许用户在每个对象上基于对象控制图像协调的程度，并且用户甚至可以在保持其他对象固定的情况下迭代地编辑生成的图像中的单个对象。通过利用丰富的信息

    We seek to give users precise control over diffusion-based image generation by modeling complex scenes as sequences of layers, which define the desired spatial arrangement and visual attributes of objects in the scene. Collage Diffusion harmonizes the input layers to make objects fit together -- the key challenge involves minimizing changes in the positions and key visual attributes of the input layers while allowing other attributes to change in the harmonization process. We ensure that objects are generated in the correct locations by modifying text-image cross-attention with the layers' alpha masks. We preserve key visual attributes of input layers by learning specialized text representations per layer and by extending ControlNet to operate on layers. Layer input allows users to control the extent of image harmonization on a per-object basis, and users can even iteratively edit individual objects in generated images while keeping other objects fixed. By leveraging the rich informati
    
[^129]: 在具有缺失属性的图中进行公平属性补全

    Fair Attribute Completion on Graph with Missing Attributes. (arXiv:2302.12977v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12977](http://arxiv.org/abs/2302.12977)

    本文提出一种公平属性补全方法FairAC，用于处理具有缺失属性的图数据中的不公平性问题。FairAC采用注意机制处理属性缺失问题，并减轻属性和补全导致的两种不公平性，即属性不公平和拓扑不公平。

    

    解决图学习模型中的不公平性是一项具有挑战性的任务，因为图中的不公平问题涉及属性和拓扑结构。现有的公平图学习工作假设训练模型时所有节点的属性都是可用的，然后进行公平预测。然而，在实践中，由于数据缺失或隐私问题，一些节点的属性可能无法访问，这使得公平图学习更加具有挑战性。本文提出了一种公平属性补全方法FairAC，用于补全缺失信息并学习具有公平性的图节点嵌入。FairAC采用注意机制处理属性缺失问题，并减轻属性和补全导致的两种不公平性，即属性不公平和拓扑不公平。FairAC可以适应不同类型的同质图并为它们生成公平的嵌入，因此可以应用于大多数图数据场景。

    Tackling unfairness in graph learning models is a challenging task, as the unfairness issues on graphs involve both attributes and topological structures. Existing work on fair graph learning simply assumes that attributes of all nodes are available for model training and then makes fair predictions. In practice, however, the attributes of some nodes might not be accessible due to missing data or privacy concerns, which makes fair graph learning even more challenging. In this paper, we propose FairAC, a fair attribute completion method, to complement missing information and learn fair node embeddings for graphs with missing attributes. FairAC adopts an attention mechanism to deal with the attribute missing problem and meanwhile, it mitigates two types of unfairness, i.e., feature unfairness from attributes and topological unfairness due to attribute completion. FairAC can work on various types of homogeneous graphs and generate fair embeddings for them and thus can be applied to most d
    
[^130]: 生物合理对比学习的灵活相位动力学

    Flexible Phase Dynamics for Bio-Plausible Contrastive Learning. (arXiv:2302.12431v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12431](http://arxiv.org/abs/2302.12431)

    本研究展示了生物合理对比学习可以在时间上具有局部性，并且即使放松了标准训练过程的许多动力学要求，仍然可以正常运行。

    

    许多作为神经科学规范模型或用于神经形态芯片学习的候选方法的学习算法通过将一组网络状态与另一组进行对比来进行学习。这些对比学习（CL）算法通常采用刚性、时间上不连续和周期性的学习动力学实现，这可能限制可以利用CL的物理系统范围。本研究建立在最近的工作基础上，探索了CL如何在生物或神经形态系统中实现，并展示了这种学习形式可以在时间上具有局部性，并且即使放松了标准训练过程的许多动力学要求，仍然可以正常运行。通过一组在几个CL模型上进行的数值实验支持的一般定理，我们的结果为生物和神经形态神经网络的CL方法的研究和发展提供了理论基础。

    Many learning algorithms used as normative models in neuroscience or as candidate approaches for learning on neuromorphic chips learn by contrasting one set of network states with another. These Contrastive Learning (CL) algorithms are traditionally implemented with rigid, temporally non-local, and periodic learning dynamics that could limit the range of physical systems capable of harnessing CL. In this study, we build on recent work exploring how CL might be implemented by biological or neurmorphic systems and show that this form of learning can be made temporally local, and can still function even if many of the dynamical requirements of standard training procedures are relaxed. Thanks to a set of general theorems corroborated by numerical experiments across several CL models, our results provide theoretical foundations for the study and development of CL methods for biological and neuromorphic neural networks.
    
[^131]: 基于大规模浮动车数据的10个城市都市路段交通速度数据

    Metropolitan Segment Traffic Speeds from Massive Floating Car Data in 10 Cities. (arXiv:2302.08761v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08761](http://arxiv.org/abs/2302.08761)

    本研究基于10个城市的大规模浮动车数据，提供了15分钟分辨率的交通速度信息，涵盖了从主干道到当地街道的所有街道级别，为城市交通运营和规划提供了重要数据。

    

    交通分析对城市运营和规划至关重要，但超出环路检测器范围的城市交通密集数据仍然很少。我们提供了一个大规模浮动车辆数据集，即“10个城市的都市路段交通速度数据集”，可用于全球10个城市，并具有每个都市区域1500多平方公里的15分钟分辨率的收集周期，收集时间为2019-2021年，覆盖从主干道到当地街道的所有街道级别的交通信息。该数据集利用工业规模的浮动车辆Traffic4cast数据，通过隐私保护的时空聚合提供了速度和车辆计数。我们详细介绍了高效的匹配方法，将数据映射到OpenStreetMap路网图中。

    Traffic analysis is crucial for urban operations and planning, while the availability of dense urban traffic data beyond loop detectors is still scarce. We present a large-scale floating vehicle dataset of per-street segment traffic information, Metropolitan Segment Traffic Speeds from Massive Floating Car Data in 10 Cities (MeTS-10), available for 10 global cities with a 15-minute resolution for collection periods ranging between 108 and 361 days in 2019-2021 and covering more than 1500 square kilometers per metropolitan area. MeTS-10 features traffic speed information at all street levels from main arterials to local streets for Antwerp, Bangkok, Barcelona, Berlin, Chicago, Istanbul, London, Madrid, Melbourne and Moscow. The dataset leverages the industrial-scale floating vehicle Traffic4cast data with speeds and vehicle counts provided in a privacy-preserving spatio-temporal aggregation. We detail the efficient matching approach mapping the data to the OpenStreetMap road graph. We e
    
[^132]: 异步多智能体赌博机的按需通信

    On-Demand Communication for Asynchronous Multi-Agent Bandits. (arXiv:2302.07446v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07446](http://arxiv.org/abs/2302.07446)

    本文研究了一种协作多智能体多臂赌博问题，提出了一种按需通信协议ODC，可以根据智能体的经验拉动时间调整每对智能体间的通信，同时将ODC集成到UCB和AAE算法的自然扩展中，提出了两种通信效率高的协作算法，分析表明这两个算法在遗憾方面都接近最优。

    

    本文研究了一种协作多智能体多臂赌博问题，其中智能体的操作是异步的 - 智能体的拉动时间和速率是未知的、不规则的和异构的 - 并且面对相同的K臂赌博问题的实例。智能体可以共享奖励信息以加快学习过程，但需要额外的通信成本。我们提出了一种按需通信协议ODC，根据智能体的经验拉动时间调整每对智能体间的通信。当智能体的拉动时间高度不均匀时，ODC具有高效性，并且其通信复杂性取决于智能体的经验拉动时间。ODC是一个通用的协议，可以集成到大多数协作赌博算法中而不降低其性能。然后，我们将ODC集成到UCB和AAE算法的自然扩展中，并提出了两种通信效率高的协作算法。我们的分析表明，这两个算法在遗憾方面都接近最优。

    This paper studies a cooperative multi-agent multi-armed stochastic bandit problem where agents operate asynchronously -- agent pull times and rates are unknown, irregular, and heterogeneous -- and face the same instance of a K-armed bandit problem. Agents can share reward information to speed up the learning process at additional communication costs. We propose ODC, an on-demand communication protocol that tailors the communication of each pair of agents based on their empirical pull times. ODC is efficient when the pull times of agents are highly heterogeneous, and its communication complexity depends on the empirical pull times of agents. ODC is a generic protocol that can be integrated into most cooperative bandit algorithms without degrading their performance. We then incorporate ODC into the natural extensions of UCB and AAE algorithms and propose two communication-efficient cooperative algorithms. Our analysis shows that both algorithms are near-optimal in regret.
    
[^133]: 神经系统的系统辨识：如果我们理解正确，我们会知道吗？

    System identification of neural systems: If we got it right, would we know?. (arXiv:2302.06677v2 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2302.06677](http://arxiv.org/abs/2302.06677)

    该论文研究了神经系统的系统辨识问题，通过对比人工神经网络与生物神经元的记录来验证模型的有效性。然而，系统辨识的性能很大程度上取决于刺激图像等因素，并且对识别更高级别架构图案方面存在局限性。

    

    人工神经网络被提议作为大脑的部分模型。将这些网络与生物神经元的记录进行比较，并认为在重现神经反应方面的良好性能支持模型的有效性。一个关键问题是，这种系统辨识方法对我们了解脑部计算有多大帮助。它是否能验证某种模型架构优于另一种？我们评估了最常用的比较技术，如线性编码模型和中心核对齐，通过用已知的真实模型替换脑部记录来正确识别模型。系统辨识的性能相当不稳定，它还显著依赖于独立于真实模型架构的因素，如刺激图像。此外，我们展示了使用功能相似性评分在识别更高级别架构图案方面的局限性。

    Artificial neural networks are being proposed as models of parts of the brain. The networks are compared to recordings of biological neurons, and good performance in reproducing neural responses is considered to support the model's validity. A key question is how much this system identification approach tells us about brain computation. Does it validate one model architecture over another? We evaluate the most commonly used comparison techniques, such as a linear encoding model and centered kernel alignment, to correctly identify a model by replacing brain recordings with known ground truth models. System identification performance is quite variable; it also depends significantly on factors independent of the ground truth architecture, such as stimuli images. In addition, we show the limitations of using functional similarity scores in identifying higher-level architectural motifs.
    
[^134]: Transformers遇见有向图

    Transformers Meet Directed Graphs. (arXiv:2302.00049v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00049](http://arxiv.org/abs/2302.00049)

    这项工作提出了两种有向图的方向和结构感知的位置编码，通过应用于排序网络的正确性测试和源代码理解等任务中，该模型相对于之前的最新技术提升了14.7%。

    

    Transformers最初被提出作为文本的序列到序列模型，但现在已广泛应用于包括图像、音频、视频和无向图等多种模态。然而，有向图的transformers却是一个意外未被充分开发的主题，尽管它们在包括源代码和逻辑电路在内的普遍领域中具有适用性。在这项工作中，我们提出了两种有向图的方向和结构感知的位置编码：（1）磁场拉普拉斯算子的特征向量 - 是组合拉普拉斯算子的方向感知推广；（2）方向随机游走编码。在实证上，我们展示了附加的方向信息在包括排序网络的正确性测试和源代码理解等各种下游任务中的有效性。结合数据流为中心的图构建，我们的模型在Open Graph Benchmark Code2上相对于之前的最新技术提升了14.7%。

    Transformers were originally proposed as a sequence-to-sequence model for text but have become vital for a wide range of modalities, including images, audio, video, and undirected graphs. However, transformers for directed graphs are a surprisingly underexplored topic, despite their applicability to ubiquitous domains, including source code and logic circuits. In this work, we propose two direction- and structure-aware positional encodings for directed graphs: (1) the eigenvectors of the Magnetic Laplacian - a direction-aware generalization of the combinatorial Laplacian; (2) directional random walk encodings. Empirically, we show that the extra directionality information is useful in various downstream tasks, including correctness testing of sorting networks and source code understanding. Together with a data-flow-centric graph construction, our model outperforms the prior state of the art on the Open Graph Benchmark Code2 relatively by 14.7%.
    
[^135]: 基于点云的毫米波通信主动链路质量预测

    Point Cloud-based Proactive Link Quality Prediction for Millimeter-wave Communications. (arXiv:2301.00752v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2301.00752](http://arxiv.org/abs/2301.00752)

    本研究提出了一种基于点云的毫米波通信主动链路质量预测方法，相比于基于图像的方法，其适用性更广且不涉及敏感信息。

    

    本研究展示了基于点云的毫米波（mmWave）通信的主动链路质量预测的可行性。以往的研究提出了基于机器学习的方法，利用深度图像的时间序列来预测未来时间段的接收信号强度，以缓解行人阻挡因素对mmWave通信的影响。但是，由于隐私问题，这些基于图像的方法的适用性有限，因为摄像头图像可能包含敏感信息。本研究提出了一种基于点云的mmWave链路质量预测方法，并通过实验证明其可行性。点云将三维空间表示为点集，其空间性质更加稀疏，不太可能包含敏感信息，并且还提供了3D位置和运动信息，这对了解涉及行人的无线电传播环境是必要的。

    This study demonstrates the feasibility of point cloud-based proactive link quality prediction for millimeter-wave (mmWave) communications. Previous studies have proposed machine learning-based methods to predict received signal strength for future time periods using time series of depth images to mitigate the line-of-sight (LOS) path blockage by pedestrians in mmWave communication. However, these image-based methods have limited applicability due to privacy concerns as camera images may contain sensitive information. This study proposes a point cloud-based method for mmWave link quality prediction and demonstrates its feasibility through experiments. Point clouds represent three-dimensional (3D) spaces as a set of points and are sparser and less likely to contain sensitive information than camera images. Additionally, point clouds provide 3D position and motion information, which is necessary for understanding the radio propagation environment involving pedestrians. This study designs
    
[^136]: 基于JKO方案的可逆归一化流神经网络

    Invertible normalizing flow neural networks by JKO scheme. (arXiv:2212.14424v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2212.14424](http://arxiv.org/abs/2212.14424)

    本文提出了一种基于JKO方案的可逆归一化流神经网络，通过按块进行残差块的训练，减少了内存负载和深度流网络训练的难度。并且通过自适应时间重新参数化的流网络，在概率空间中逐步细化轨迹，从而提高了模型的训练效率和准确性。

    

    归一化流是一类用于高效采样和密度估计的深度生成模型。实际中，流通常表示为一系列可逆的神经网络模块链; 为了便于训练，现有的工作对流轨迹进行了正则化，并设计了特殊的网络架构。本文提出了受Jordan-Kinderleherer-Otto (JKO)方案启发的神经ODE流网络，它允许有效地按块进行残差块的训练，无需采样SDE轨迹或分数匹配或变分学习的内循环。由于JKO方案展开了梯度流的动态，所提出的模型自然地逐个堆叠残差网络块，降低了内存负载和进行端到端深度流网络训练的难度。我们还开发了自适应时间重新参数化的流网络，通过在概率空间中逐步细化轨迹，提高了模型的训练效率和准确性。

    Normalizing flow is a class of deep generative models for efficient sampling and density estimation. In practice, the flow often appears as a chain of invertible neural network blocks; to facilitate training, existing works have regularized flow trajectories and designed special network architectures. The current paper develops a neural ODE flow network inspired by the Jordan-Kinderleherer-Otto (JKO) scheme, which allows efficient block-wise training of the residual blocks without sampling SDE trajectories or inner loops of score matching or variational learning. As the JKO scheme unfolds the dynamic of gradient flow, the proposed model naturally stacks residual network blocks one by one, reducing the memory load and difficulty in performing end-to-end deep flow network training. We also develop adaptive time reparameterization of the flow network with a progressive refinement of the trajectory in probability space, which improves the model training efficiency and accuracy in practice.
    
[^137]: StyleGAN作为一种保持实用性的人脸去识别方法

    StyleGAN as a Utility-Preserving Face De-identification Method. (arXiv:2212.02611v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.02611](http://arxiv.org/abs/2212.02611)

    本文研究了使用StyleGAN生成去识别人脸的方法，通过风格混合，StyleGAN能够在保护用户隐私的同时保持图像的实用性，与其他两种最先进的方法相比表现出色。

    

    人脸去识别方法被提出来通过模糊人脸来保护用户的隐私。然而，这些方法会降低照片的质量，并且通常不能保持人脸的实用性，即年龄、性别、姿势和表情。最近，提出了一种名为StyleGAN的GAN模型，它可以生成逼真高质量的虚拟人脸。本文通过风格混合来研究StyleGAN在生成去识别人脸方面的应用。我们通过实施多种人脸检测、验证和识别攻击，并进行用户研究来评估这种去识别方法的实用性和隐私保护效果。我们广泛的实验、人工评估和与CIAGAN和DeepPrivacy两种最先进方法的比较结果显示，StyleGAN在保护用户隐私和图片实用性方面表现出色甚至更好。特别是，基于机器学习的实验结果显示……

    Face de-identification methods have been proposed to preserve users' privacy by obscuring their faces. These methods, however, can degrade the quality of photos, and they usually do not preserve the utility of faces, i.e., their age, gender, pose, and facial expression. Recently, GANs, such as StyleGAN, have been proposed, which generate realistic, high-quality imaginary faces. In this paper, we investigate the use of StyleGAN in generating de-identified faces through style mixing. We examined this de-identification method for preserving utility and privacy by implementing several face detection, verification, and identification attacks and conducting a user study. The results from our extensive experiments, human evaluation, and comparison with two state-of-the-art methods, i.e., CIAGAN and DeepPrivacy, show that StyleGAN performs on par or better than these methods, preserving users' privacy and images' utility. In particular, the results of the machine learning-based experiments sho
    
[^138]: 顺序知情联合消除：联邦优化中高效且可证明的客户端消除

    Sequential Informed Federated Unlearning: Efficient and Provable Client Unlearning in Federated Optimization. (arXiv:2211.11656v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.11656](http://arxiv.org/abs/2211.11656)

    本文提出一种名为知情联合消除（IFU）的新颖联邦优化方法，可实现有效且可量化的客户端消除请求，实验结果表明其效率较基本方法和最先进的FU方法更高。

    

    机器消除（MU）旨在提供有关从训练过程中删除给定数据点的贡献的理论保证。联邦消除（FU）是将MU扩展到从联合训练过程中消除给定客户端的贡献。当前的FU方法通常不具有可扩展性，并且没有对消除效果的有效性进行合理的理论量化。在本文中，我们提出了知情联合消除(IFU)，这是一种新颖的高效且可量化的FU方法。在接收到给定客户端的消除请求后，IFU通过随机扰动机制确定了重新初始化FL所需的最佳FL迭代，可以获得消除保证。IFU的理论也可以扩展以解决顺序消除请求。不同任务和数据集上的实验结果表明，与基本重新训练和最先进的FU方法相比，IFU可以实现更高效的消除过程。

    The aim of Machine Unlearning (MU) is to provide theoretical guarantees on the removal of the contribution of a given data point from a training procedure. Federated Unlearning (FU) consists in extending MU to unlearn a given client's contribution from a federated training routine. Current FU approaches are generally not scalable, and do not come with sound theoretical quantification of the effectiveness of unlearning. In this work we present Informed Federated Unlearning (IFU), a novel efficient and quantifiable FU approach. Upon unlearning request from a given client, IFU identifies the optimal FL iteration from which FL has to be reinitialized, with unlearning guarantees obtained through a randomized perturbation mechanism. The theory of IFU is also extended to account for sequential unlearning requests. Experimental results on different tasks and dataset show that IFU leads to more efficient unlearning procedures as compared to basic re-training and state-of-the-art FU approaches.
    
[^139]: 面向多领域协作学习的联邦自适应提示调优

    Federated Adaptive Prompt Tuning for Multi-domain Collaborative Learning. (arXiv:2211.07864v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.07864](http://arxiv.org/abs/2211.07864)

    本文提出了一种面向多领域协作学习的联邦自适应提示调优算法 FedAPT，利用强大的预训练模型实现更高的性能。其核心思想是为每个测试样本提供个性化提示，通过自适应地释放特定领域的知识来实现。通过设计一个自适应提示调优模块，服务器生成关键信息并分配给客户端，从而实现协同训练全局的自适应网络和元提示。

    

    联邦学习使得多个客户端能够在不泄露数据的情况下协同训练全局模型。以往的研究通常需要训练完整的模型参数。然而，强大的预训练模型的出现使得在联邦学习中使用更少的可训练参数能够实现更高的性能。本文提出了一种面向多领域协作图像分类的联邦自适应提示调优算法 FedAPT，利用类似 CLIP 的强大基础模型。与直接联邦提示调优相比，我们的核心思想是针对每个测试样本自适应地释放特定领域的知识，为其提供个性化提示。为了实现这个想法，我们设计了一个自适应提示调优模块，它包括元提示，自适应网络和一些关键信息。服务器随机生成一组关键信息，并将每个客户端分配一个唯一的关键信息。然后，所有客户端协同训练全局自适应网络和元提示。

    Federated learning (FL) enables multiple clients to collaboratively train a global model without disclosing their data. Previous researches often require training the complete model parameters. However, the emergence of powerful pre-trained models makes it possible to achieve higher performance with fewer learnable parameters in FL. In this paper, we propose a federated adaptive prompt tuning algorithm, FedAPT, for multi-domain collaborative image classification with powerful foundation models, like CLIP. Compared with direct federated prompt tuning, our core idea is to adaptively unlock specific domain knowledge for each test sample in order to provide them with personalized prompts. To implement this idea, we design an adaptive prompt tuning module, which consists of a meta prompt, an adaptive network, and some keys. The server randomly generates a set of keys and assigns a unique key to each client. Then all clients cooperatively train the global adaptive network and meta prompt wit
    
[^140]: 从相邻的染色组织学片中学习黑色素细胞掩膜

    Learning Melanocytic Cell Masks from Adjacent Stained Tissue. (arXiv:2211.00646v2 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2211.00646](http://arxiv.org/abs/2211.00646)

    本文提出了一种从相邻染色组织学片中训练深度神经网络进行黑色素细胞分割的方法，实现了0.64的平均IOU，尽管存在不完美的标签。

    

    黑色素瘤是最具侵袭性的皮肤癌之一，导致大部分皮肤癌死亡。然而，病理学家对黑色素瘤的诊断可靠性较低。由于黑色素瘤是黑色素细胞的肿瘤，需要开发一种与病理学家的差异无关并能自动进行像素级注释的黑色素细胞分割工具。然而，大规模病理学家标注是不现实的。在本文中，我们提出了一种方法，使用邻近组织切片上的偶联免疫组织化学（IHC）染色片，训练深度神经网络进行黑色素细胞分割，虽然很难有完美的标签，但达到了0.64的平均IOU。

    Melanoma is one of the most aggressive forms of skin cancer, causing a large proportion of skin cancer deaths. However, melanoma diagnoses by pathologists shows low interrater reliability. As melanoma is a cancer of the melanocyte, there is a clear need to develop a melanocytic cell segmentation tool that is agnostic to pathologist variability and automates pixel-level annotation. Gigapixel-level pathologist labeling, however, is impractical. Herein, we propose a means to train deep neural networks for melanocytic cell segmentation from hematoxylin and eosin (H&E) stained slides using paired immunohistochemical (IHC) slides of adjacent tissue sections, achieving a mean IOU of 0.64 despite imperfect ground-truth labels.
    
[^141]: 通过变分自由能最小化对贝叶斯神经网络进行原则性剪枝

    Principled Pruning of Bayesian Neural Networks through Variational Free Energy Minimization. (arXiv:2210.09134v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.09134](http://arxiv.org/abs/2210.09134)

    本文提出了一种基于变分自由能最小化的贝叶斯模型简化方法，用于对贝叶斯神经网络进行原则性剪枝。通过引入迭代剪枝算法，解决了直接应用贝叶斯模型简化的近似误差问题，并在实验证明了该方法的有效性和优势。

    

    贝叶斯模型简化提供了一种有效的方法，用于比较模型的所有嵌套子模型的性能，而无需重新评估这些子模型。迄今为止，贝叶斯模型简化主要应用于计算神经科学社区的简单模型。本文提出并应用了基于变分自由能最小化的贝叶斯模型简化方法，用于对贝叶斯神经网络进行原则性剪枝。然而，直接应用贝叶斯模型简化会产生近似误差。因此，本文提出了一种新颖的迭代剪枝算法，以缓解直接应用贝叶斯模型简化所引起的问题，并在公开可用的UCI数据集上通过实验证明其效果对不同推理算法。这种新颖的参数剪枝方案解决了信号处理社区使用的当前最先进的剪枝方法的缺点。所提出的方法具有明确的停止准则和m.

    Bayesian model reduction provides an efficient approach for comparing the performance of all nested sub-models of a model, without re-evaluating any of these sub-models. Until now, Bayesian model reduction has been applied mainly in the computational neuroscience community on simple models. In this paper, we formulate and apply Bayesian model reduction to perform principled pruning of Bayesian neural networks, based on variational free energy minimization. Direct application of Bayesian model reduction, however, gives rise to approximation errors. Therefore, a novel iterative pruning algorithm is presented to alleviate the problems arising with naive Bayesian model reduction, as supported experimentally on the publicly available UCI datasets for different inference algorithms. This novel parameter pruning scheme solves the shortcomings of current state-of-the-art pruning methods that are used by the signal processing community. The proposed approach has a clear stopping criterion and m
    
[^142]: 使用对比学习预训练二进制代码表示

    Pre-Training Representations of Binary Code Using Contrastive Learning. (arXiv:2210.05102v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2210.05102](http://arxiv.org/abs/2210.05102)

    提出了一种使用对比学习预训练二进制代码表示的方法，可以将源代码和注释信息纳入二进制代码的表示学习中，对于反向工程和计算机安全任务有重要意义。

    

    编译后的软件以可执行的二进制代码形式交付。开发人员编写源代码来表达软件的语义，但编译器将其转换为CPU可以直接执行的二进制格式。因此，二进制代码分析对于反向工程和计算机安全任务等没有源代码的应用程序至关重要。然而，与包含丰富语义信息的源代码和自然语言不同，二进制代码通常难以理解和分析。虽然现有的工作使用AI模型辅助源代码分析，但很少有研究考虑二进制代码。在本文中，我们提出了一种将源代码和注释信息纳入二进制代码进行表示学习的对比学习模型，称为COMBO。具体而言，我们在COMBO中提出了三个组件：（1）用于冷启动预训练的主要对比学习方法，（2）用于将源代码和注释信息插入到二进制代码中的单纯插值方法。

    Compiled software is delivered as executable binary code. Developers write source code to express the software semantics, but the compiler converts it to a binary format that the CPU can directly execute. Therefore, binary code analysis is critical to applications in reverse engineering and computer security tasks where source code is not available. However, unlike source code and natural language that contain rich semantic information, binary code is typically difficult for human engineers to understand and analyze. While existing work uses AI models to assist source code analysis, few studies have considered binary code. In this paper, we propose a COntrastive learning Model for Binary cOde Analysis, or COMBO, that incorporates source code and comment information into binary code during representation learning. Specifically, we present three components in COMBO: (1) a primary contrastive learning method for cold-start pre-training, (2) a simplex interpolation method to incorporate so
    
[^143]: Bayesian MAML的超网络方法

    Hypernetwork approach to Bayesian MAML. (arXiv:2210.02796v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.02796](http://arxiv.org/abs/2210.02796)

    该论文提出了一种名为贝叶斯HMAML的新框架，利用超网络进行权重更新，以解决MAML的过拟合和不确定性问题。

    

    少样本学习算法的主要目标是能够从少量数据中进行学习。其中一种最流行且优雅的少样本学习方法是模型无关元学习（MAML）。该方法的主要思想是学习元模型的共享通用权重，然后将其适应于特定任务。然而，该方法存在过拟合问题，因为数据量有限，很难准确量化不确定性。贝叶斯方法可以通过学习权重分布而不是点值权重来缓解这些问题。不幸的是，先前修改的MAML方法由于高斯分布的简单性、基于梯度的类MAML权重更新或强制执行相同结构的通用权重和适应权重，受到一定的限制。在本文中，我们提出了一种新的贝叶斯MAML框架，称为贝叶斯HMAML，它利用超网络进行权重更新。

    The main goal of Few-Shot learning algorithms is to enable learning from small amounts of data. One of the most popular and elegant Few-Shot learning approaches is Model-Agnostic Meta-Learning (MAML). The main idea behind this method is to learn the shared universal weights of a meta-model, which are then adapted for specific tasks. However, the method suffers from over-fitting and poorly quantifies uncertainty due to limited data size. Bayesian approaches could, in principle, alleviate these shortcomings by learning weight distributions in place of point-wise weights. Unfortunately, previous modifications of MAML are limited due to the simplicity of Gaussian posteriors, MAML-like gradient-based weight updates, or by the same structure enforced for universal and adapted weights.  In this paper, we propose a novel framework for Bayesian MAML called BayesianHMAML, which employs Hypernetworks for weight updates. It learns the universal weights point-wise, but a probabilistic structure is 
    
[^144]: 基于动力系统的神经网络

    Dynamical systems' based neural networks. (arXiv:2210.02373v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.02373](http://arxiv.org/abs/2210.02373)

    本文通过基于动力系统的方法设计神经网络，以更好地理解网络的行为，并实现一些特定的属性。主要关注包含非1-Lipschitz层的1-Lipschitz架构。

    

    神经网络在许多应用中表现出了高效性，但是它们的数学属性通常不太清楚。如果数据或近似函数中存在某种潜在的几何结构，那么在设计神经网络时就往往希望考虑这一点。在本工作中，我们从一个非自治的常微分方程开始，使用适当的、保持结构的数值时间离散化方法构建神经网络。然后，神经网络的结构由常微分方程向量场的性质推导而来。除了在网络架构中注入更多结构之外，这种建模方法还可以更好地理解神经网络的行为。我们提出了两个普适逼近的结果，并演示了如何在神经网络上实施一些特定的属性。特别关注的是包含非1-Lipschitz层的1-Lipschitz架构。这些网络的表达方式更+

    Neural networks have gained much interest because of their effectiveness in many applications. However, their mathematical properties are generally not well understood. If there is some underlying geometric structure inherent to the data or to the function to approximate, it is often desirable to take this into account in the design of the neural network. In this work, we start with a non-autonomous ODE and build neural networks using a suitable, structure-preserving, numerical time-discretisation. The structure of the neural network is then inferred from the properties of the ODE vector field. Besides injecting more structure into the network architectures, this modelling procedure allows a better theoretical understanding of their behaviour. We present two universal approximation results and demonstrate how to impose some particular properties on the neural networks. A particular focus is on 1-Lipschitz architectures including layers that are not 1-Lipschitz. These networks are expre
    
[^145]: 在二进制激活神经网络中寻求解释性和可解释性

    Seeking Interpretability and Explainability in Binary Activated Neural Networks. (arXiv:2209.03450v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.03450](http://arxiv.org/abs/2209.03450)

    本论文研究了在表格数据回归任务中使用二进制激活神经网络作为可解释和可解释的预测器的方法。我们提供了对其表达能力的保证，并提出了一种基于SHAP值的方法来量化特征、隐藏神经元和权重的相对重要性。同时，我们提出了一种贪婪算法来构建紧凑的网络，以实现解释性。

    

    我们研究了在表格数据的回归任务中，将二进制激活神经网络作为可解释和可解释的预测器的使用。具体而言，我们对它们的表达能力提供了保证，并提出了一种基于有效计算SHAP值的方法，用于量化特征、隐藏神经元甚至权重的相对重要性。由于模型的简单性在实现解释性方面起着关键作用，我们提出了一种贪婪算法，用于构建紧凑的二进制激活网络。这种方法不需要预先设定网络的架构：它逐层、逐个神经元地构建，使得对于给定任务，预测器不会过于复杂。

    We study the use of binary activated neural networks as interpretable and explainable predictors in the context of regression tasks on tabular data; more specifically, we provide guarantees on their expressiveness, present an approach based on the efficient computation of SHAP values for quantifying the relative importance of the features, hidden neurons and even weights. As the model's simplicity is instrumental in achieving interpretability, we propose a greedy algorithm for building compact binary activated networks. This approach doesn't need to fix an architecture for the network in advance: it is built one layer at a time, one neuron at a time, leading to predictors that aren't needlessly complex for a given task.
    
[^146]: GRASP: 一种用于分类学习的适合度检验方法

    GRASP: A Goodness-of-Fit Test for Classification Learning. (arXiv:2209.02064v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2209.02064](http://arxiv.org/abs/2209.02064)

    本文提出了一种适合度检验方法GRASP，用于评估通用二分类器对给定特征向量的标签的条件概率分布的拟合程度。

    

    分类器的性能通常以测试数据的平均准确率衡量。尽管平均准确率是一种标准的衡量方法，但它在描述模型对给定特征向量的标签的条件概率分布的拟合程度方面存在缺陷，例如模型错误规范化、过拟合和高维度等。在本文中，我们考虑了评估通用二分类器拟合程度的基本问题。我们的框架不对条件概率分布$Y|X$进行任何参数假设，并将其视为黑盒子模型，只能通过查询访问。我们将适合度评估问题表述为容忍度假设检验的形式\[ H_0: \mathbb{E}\Big[D_f\Big({\sf Bern}(\eta(X))\|{\sf Bern}(\hat{\eta}(X))\Big)\Big]\leq \tau\,, \]其中$D_f$表示一个$f$-散度函数，$\eta(x)$和$\hat{\eta}(x)$分别表示特征向量$x$的真实和估计的似然度。

    Performance of classifiers is often measured in terms of average accuracy on test data. Despite being a standard measure, average accuracy fails in characterizing the fit of the model to the underlying conditional law of labels given the features vector ($Y|X$), e.g. due to model misspecification, over fitting, and high-dimensionality. In this paper, we consider the fundamental problem of assessing the goodness-of-fit for a general binary classifier. Our framework does not make any parametric assumption on the conditional law $Y|X$, and treats that as a black box oracle model which can be accessed only through queries. We formulate the goodness-of-fit assessment problem as a tolerance hypothesis testing of the form \[ H_0: \mathbb{E}\Big[D_f\Big({\sf Bern}(\eta(X))\|{\sf Bern}(\hat{\eta}(X))\Big)\Big]\leq \tau\,, \] where $D_f$ represents an $f$-divergence function, and $\eta(x)$, $\hat{\eta}(x)$ respectively denote the true and an estimate likelihood for a feature vector $x$ admitting
    
[^147]: 基于视觉对应的解释提高了AI的鲁棒性和人机团队准确性

    Visual correspondence-based explanations improve AI robustness and human-AI team accuracy. (arXiv:2208.00780v5 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2208.00780](http://arxiv.org/abs/2208.00780)

    该论文提出了基于视觉对应的解释方法，用于改善AI的鲁棒性和人机团队的准确性。在大规模的人类研究中，该方法被发现比kNN解释更有用，帮助用户更准确地拒绝AI的错误决策。同时，该方法在超出分布数据集上改进了性能，并实现了互补人机团队准确性的可能性。

    

    在许多高风险应用中，解释人工智能（AI）的预测变得越来越重要，甚至是必不可少的，因为人类是最终的决策者。在这项工作中，我们提出了两种新颖的自解释图像分类器架构，它们首先通过利用查询图像和示例之间的视觉对应关系进行解释，然后进行预测（与事后解释相对）。我们的模型在超出分布（OOD）数据集上一致改进（1到4个点），而在分布测试上表现略次于ResNet-50和一个k最近邻分类器（kNN）（下降1到2个点）。通过对ImageNet和CUB进行的大规模人类研究，我们发现基于对应关系的解释比kNN解释对用户更有用。我们的解释帮助用户更准确地拒绝AI的错误决策，胜过所有其他测试方法。有趣的是，我们首次展示了实现互补人机团队准确性的可能性。

    Explaining artificial intelligence (AI) predictions is increasingly important and even imperative in many high-stakes applications where humans are the ultimate decision-makers. In this work, we propose two novel architectures of self-interpretable image classifiers that first explain, and then predict (as opposed to post-hoc explanations) by harnessing the visual correspondences between a query image and exemplars. Our models consistently improve (by 1 to 4 points) on out-of-distribution (OOD) datasets while performing marginally worse (by 1 to 2 points) on in-distribution tests than ResNet-50 and a $k$-nearest neighbor classifier (kNN). Via a large-scale, human study on ImageNet and CUB, our correspondence-based explanations are found to be more useful to users than kNN explanations. Our explanations help users more accurately reject AI's wrong decisions than all other tested methods. Interestingly, for the first time, we show that it is possible to achieve complementary human-AI tea
    
[^148]: 将区域化算法扩展到探索空间进程异质性

    Extending regionalization algorithms to explore spatial process heterogeneity. (arXiv:2206.09429v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2206.09429](http://arxiv.org/abs/2206.09429)

    提出两种新的空间区域划分算法，应用于合成和真实数据集，并取得了较好的成果。其中，两阶段K模型算法表现最佳，具有较好的模型拟合、区域重构和系数估计能力。

    

    在空间回归模型中，空间异质性可以通过连续或离散规范进行考虑。后者涉及到确定具有同质变量之间的均质关系的连续空间区域（空间区域）。尽管在空间分析领域中提出并研究了各种区域化算法，但优化空间区域的方法基本未被探索。本文提出了两种新的空间区域划分算法，即两阶段K模型和区域K模型。我们还将经典的自动分区程序扩展到空间回归背景下。这些算法被应用于一系列合成数据集和两个真实数据集。结果表明，所有三种算法在模型拟合、区域重构和系数估计方面均达到了超越或可比较的性能，而两阶段K模型算法在模型拟合、区域重构和系数估计方面明显优于现有方法。

    In spatial regression models, spatial heterogeneity may be considered with either continuous or discrete specifications. The latter is related to delineation of spatially connected regions with homogeneous relationships between variables (spatial regimes). Although various regionalization algorithms have been proposed and studied in the field of spatial analytics, methods to optimize spatial regimes have been largely unexplored. In this paper, we propose two new algorithms for spatial regime delineation, two-stage K-Models and Regional-K-Models. We also extend the classic Automatic Zoning Procedure to spatial regression context. The proposed algorithms are applied to a series of synthetic datasets and two real-world datasets. Results indicate that all three algorithms achieve superior or comparable performance to existing approaches, while the two-stage K-Models algorithm largely outperforms existing approaches on model fitting, region reconstruction, and coefficient estimation. Our wo
    
[^149]: 0/1深度神经网络的块坐标下降算法

    0/1 Deep Neural Networks via Block Coordinate Descent. (arXiv:2206.09379v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.09379](http://arxiv.org/abs/2206.09379)

    本文介绍了一种用于训练0/1 DNNs的块坐标下降算法，该算法能够有效解决步函数作为激活函数所带来的困难，并获得了具有鲁棒性和最佳预测准确性的DNNs模型。

    

    步函数是深度神经网络（DNNs）中最简单且最自然的激活函数之一。由于它对于正变量计数为1，对于其他变量计数为0，其固有特性（如不连续性和无有效的次梯度信息）阻碍了其几十年的发展。尽管有许多用连续激活函数设计的DNNs，可以视为步函数的替代品，但步函数仍具有一些优势特性，如对异常值的完全鲁棒性和具备最佳学习理论担保的预测准确性。因此，本文旨在训练使用步函数作为激活函数的DNNs（称为0/1 DNNs）。我们首先将0/1 DNNs重新表述为无约束优化问题，然后通过块坐标下降（BCD）方法对其进行求解。此外，我们还获得了BCD子问题的闭式解以及其收敛性。

    The step function is one of the simplest and most natural activation functions for deep neural networks (DNNs). As it counts 1 for positive variables and 0 for others, its intrinsic characteristics (e.g., discontinuity and no viable information of subgradients) impede its development for several decades. Even if there is an impressive body of work on designing DNNs with continuous activation functions that can be deemed as surrogates of the step function, it is still in the possession of some advantageous properties, such as complete robustness to outliers and being capable of attaining the best learning-theoretic guarantee of predictive accuracy. Hence, in this paper, we aim to train DNNs with the step function used as an activation function (dubbed as 0/1 DNNs). We first reformulate 0/1 DNNs as an unconstrained optimization problem and then solve it by a block coordinate descend (BCD) method. Moreover, we acquire closed-form solutions for sub-problems of BCD as well as its convergenc
    
[^150]: 具有完全可微分量化的混合精度神经网络的边缘推理

    Edge Inference with Fully Differentiable Quantized Mixed Precision Neural Networks. (arXiv:2206.07741v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.07741](http://arxiv.org/abs/2206.07741)

    本论文提出了一种新的边缘推理混合精度神经网络的量化方法，通过硬件感知的异构可微分量化和目标梯度修改，实现了在较小的内存占用下达到最优的模型准确性

    

    深度神经网络(DNNs)的大量计算和存储成本常常限制了其在资源受限设备上的使用。将参数和操作量化为低位精度可以大幅节省神经网络推理的内存和能量，便于在边缘计算平台上使用DNNs。最近的量化DNNs的努力采用了一系列的技术，包括渐进式量化、步长适应和梯度缩放。本文提出了一种针对边缘计算的混合精度卷积神经网络(CNNs)的新的量化方法。我们的方法在模型准确性和内存占用方面建立了一个新的帕累托前沿，展示了一系列量化模型，在不到4.3 MB的权重和激活下实现了最佳的准确性。我们的主要贡献包括：(i) 基于硬件感知的异构可微分量化，使用张量分块学习的精度，(ii) 针对权重的有针对性的梯度修改

    The large computing and memory cost of deep neural networks (DNNs) often precludes their use in resource-constrained devices. Quantizing the parameters and operations to lower bit-precision offers substantial memory and energy savings for neural network inference, facilitating the use of DNNs on edge computing platforms. Recent efforts at quantizing DNNs have employed a range of techniques encompassing progressive quantization, step-size adaptation, and gradient scaling. This paper proposes a new quantization approach for mixed precision convolutional neural networks (CNNs) targeting edge-computing. Our method establishes a new pareto frontier in model accuracy and memory footprint demonstrating a range of quantized models, delivering best-in-class accuracy below 4.3 MB of weights (wgts.) and activations (acts.). Our main contributions are: (i) hardware-aware heterogeneous differentiable quantization with tensor-sliced learned precision, (ii) targeted gradient modification for wgts. an
    
[^151]: 神经元多样性能够提高物理学及其他领域的机器学习

    Neuronal diversity can improve machine learning for physics and beyond. (arXiv:2204.04348v2 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2204.04348](http://arxiv.org/abs/2204.04348)

    本文展示了使用多样化到神经元来改进机器学习，构建出能够通过学习自身激活函数快速多样化的神经网络，并且胜过传统的同构神经元网络，在图像分类和非线性回归任务中表现更优，这种学习到的多样性为动态系统选择多样性而非均匀性提供了例子，并阐明了多样性在自然和人工系统中的作用。

    

    自然界表现出多样性的优点，但是人工神经网络的层通常是由同构神经元构成的。本文中我们建立起能够学习自身激活函数、快速多样化并且在图像分类和非线性回归任务中胜过同构神经元的神经网络。子网络实例化了神经元，而元学习尤其高效的非线性响应集合。例子包括传统的神经网络分类数字和预测一个 van der Pol 振荡器和一种物理学驱动的 Hamiltonian 神经网络学习 Hénond-Heiles 轨道。这种学习到的多样性为动态系统选择多样性而非均匀性提供了例子，并阐明了多样性在自然和人工系统中的作用。

    Diversity conveys advantages in nature, yet homogeneous neurons typically comprise the layers of artificial neural networks. Here we construct neural networks from neurons that learn their own activation functions, quickly diversify, and subsequently outperform their homogeneous counterparts on image classification and nonlinear regression tasks. Sub-networks instantiate the neurons, which meta-learn especially efficient sets of nonlinear responses. Examples include conventional neural networks classifying digits and forecasting a van der Pol oscillator and a physics-informed Hamiltonian neural network learning H\'enon-Heiles orbits. Such learned diversity provides examples of dynamical systems selecting diversity over uniformity and elucidates the role of diversity in natural and artificial systems.
    
[^152]: MGNN: 受距离几何问题启发的图神经网络

    MGNN: Graph Neural Networks Inspired by Distance Geometry Problem. (arXiv:2201.12994v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.12994](http://arxiv.org/abs/2201.12994)

    本文提出了一种受距离几何问题启发的图神经网络（MGNN），通过改进基函数设计来提高GNN模型的近似能力，并从几何和物理的角度对空间GNNs的普适性进行了综合分析。

    

    图神经网络（GNNs）已成为机器学习领域的重要研究课题。现有的GNN模型通常分为两种类型：基于多项式图滤波器设计的频谱GNNs和以消息传递方案作为模型基础的空间GNNs。对于频谱GNNs的表达能力和普适性，自然的方法是改进基函数的设计，以提高近似能力。至于空间GNNs，像图同构网络（GIN）这样的模型通过分析其表达能力来进行图同构测试。最近，有人尝试建立空间GNNs与几何概念如曲率和细胞束以及物理现象如振荡器之间的联系。然而，尽管最近取得了一些进展，但在几何和物理的角度上，关于空间GNNs普适性的综合分析仍然缺乏。在本文中，我们提出了一种受距离几何问题启发的图神经网络（MGNN）。

    Graph Neural Networks (GNNs) have emerged as a prominent research topic in the field of machine learning. Existing GNN models are commonly categorized into two types: spectral GNNs, which are designed based on polynomial graph filters, and spatial GNNs, which utilize a message-passing scheme as the foundation of the model. For the expressive power and universality of spectral GNNs, a natural approach is to improve the design of basis functions for better approximation ability. As for spatial GNNs, models like Graph Isomorphism Networks (GIN) analyze their expressive power based on Graph Isomorphism Tests. Recently, there have been attempts to establish connections between spatial GNNs and geometric concepts like curvature and cellular sheaves, as well as physical phenomena like oscillators. However, despite the recent progress, there is still a lack of comprehensive analysis regarding the universality of spatial GNNs from the perspectives of geometry and physics. In this paper, we prop
    
[^153]: 利用基于图像的生成对抗网络进行时间序列生成

    Leveraging Image-based Generative Adversarial Networks for Time Series Generation. (arXiv:2112.08060v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.08060](http://arxiv.org/abs/2112.08060)

    本文提出了一种基于图像的生成对抗网络用于时间序列生成的方法，通过引入拓展的时间序列间隔回报图（XIRP）作为二维图像表示，能够以尺度不变和可逆的方式捕捉时间序列的动态特性，从而在降低训练时间和提高样本质量方面取得显著优势。通过与其他图像表示方法和模型的比较，验证了该方法在预测能力方面的优越性。同时，引入了改进的随机反演方法以改善时间序列的重建效果。

    

    由于生成模型能够从复杂的数据分布中生成逼真的样本，因此在计算机视觉和自然语言处理领域中，基于图像的生成模型引起了广泛关注。为了利用基于图像的生成模型在时间序列领域的进展，我们提出了一种二维图像表示方法，即拓展的时间序列间隔回报图（Extended Intertemporal Return Plot，XIRP）。我们的方法以尺度不变和可逆的方式捕捉了时间序列的动态特性，降低了训练时间并提高了样本质量。我们使用带有梯度惩罚的Wasserstein生成对抗网络（WGAN-GP）对合成的XIRP进行了基准测试，与其他图像表示方法和模型进行了相似性和预测能力指标的比较。我们的创新性、经过验证的时间序列图像表示方法在预测能力方面始终显著优于最先进的基于RNN的生成模型。此外，我们引入了改进的随机反演方法使得重建时间序列的效果更好。

    Generative models for images have gained significant attention in computer vision and natural language processing due to their ability to generate realistic samples from complex data distributions. To leverage the advances of image-based generative models for the time series domain, we propose a two-dimensional image representation for time series, the Extended Intertemporal Return Plot (XIRP). Our approach captures the intertemporal time series dynamics in a scale-invariant and invertible way, reducing training time and improving sample quality. We benchmark synthetic XIRPs obtained by an off-the-shelf Wasserstein GAN with gradient penalty (WGAN-GP) to other image representations and models regarding similarity and predictive ability metrics. Our novel, validated image representation for time series consistently and significantly outperforms a state-of-the-art RNN-based generative model regarding predictive ability. Further, we introduce an improved stochastic inversion to substantial
    
[^154]: 将归纳推理和演绎推理相结合进行不完整知识图谱上的查询回答

    Combining Inductive and Deductive Reasoning for Query Answering over Incomplete Knowledge Graphs. (arXiv:2106.14052v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2106.14052](http://arxiv.org/abs/2106.14052)

    该论文研究了将归纳推理和演绎推理相结合应用于不完整知识图谱上的查询回答。通过将本体论纳入基于嵌入的查询回答模型，采用不同的集成策略和损失函数调整，取得了20%到的性能提升。

    

    当前用于不完整知识图谱的基于嵌入的查询回答方法只集中在归纳推理上，即通过从数据中学习模式来预测答案，并缺乏进行演绎推理的能力，演绎推理需要应用领域知识来推断更多信息。为了解决这个缺点，我们研究了将本体论纳入基于嵌入的查询回答模型的问题，定义了基于嵌入的本体中介查询回答任务。我们提出了各种集成策略，包括（1）不同的本体驱动数据增强技术和（2）适应本体公理的损失函数的调整。我们设计了基于LUBM和NELL知识图谱的新颖基准，并在其上评估了我们的方法。在需要归纳和演绎推理的设置中，我们取得了从20%到的提升。

    Current methods for embedding-based query answering over incomplete Knowledge Graphs (KGs) only focus on inductive reasoning, i.e., predicting answers by learning patterns from the data, and lack the complementary ability to do deductive reasoning, which requires the application of domain knowledge to infer further information. To address this shortcoming, we investigate the problem of incorporating ontologies into embedding-based query answering models by defining the task of embedding-based ontology-mediated query answering. We propose various integration strategies into prominent representatives of embedding models that involve (1) different ontology-driven data augmentation techniques and (2) adaptation of the loss function to enforce the ontology axioms. We design novel benchmarks for the considered task based on the LUBM and the NELL KGs and evaluate our methods on them. The achieved improvements in the setting that requires both inductive and deductive reasoning are from 20% to 
    
[^155]: 基于模拟的用户界面优化用于保证机器学习模型预测质量

    Simulation-Based Optimization of User Interfaces for Quality-Assuring Machine Learning Model Predictions. (arXiv:2104.01129v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2104.01129](http://arxiv.org/abs/2104.01129)

    本论文介绍了一种基于模拟的优化方法，用于改进机器学习质量保证界面，通过模拟人类智能和机器智能的组合效应，以任务完成时间作为度量标准，来评估和优化QA4ML界面的效率

    

    机器学习质量敏感应用需要在模型预测被部署之前由人类进行质量保证。机器学习的质量保证接口需要用户查看大量数据并进行多次交互以纠正模型的错误。优化的用户界面可以显著降低交互成本。虽然可以通过用户研究评估设计选项来优化用户界面，但这种方法不可扩展，因为通常有许多微小的变化会影响QA4ML界面的效率。因此，我们提出使用模拟来评估和辅助优化QA4ML界面。特别地，我们关注在模拟中人类智能发起适当的交互命令和机器智能提供算法辅助加速QA4ML过程的综合效果。由于QA4ML通常需要大量劳动力，我们以模拟的任务完成时间作为度量标准

    Quality-sensitive applications of machine learning (ML) require quality assurance (QA) by humans before the predictions of an ML model can be deployed. QA for ML (QA4ML) interfaces require users to view a large amount of data and perform many interactions to correct errors made by the ML model. An optimized user interface (UI) can significantly reduce interaction costs. While UI optimization can be informed by user studies evaluating design options, this approach is not scalable because there are typically numerous small variations that can affect the efficiency of a QA4ML interface. Hence, we propose using simulation to evaluate and aid the optimization of QA4ML interfaces. In particular, we focus on simulating the combined effects of human intelligence in initiating appropriate interaction commands and machine intelligence in providing algorithmic assistance for accelerating QA4ML processes. As QA4ML is usually labor-intensive, we use the simulated task completion time as the metric 
    
[^156]: 在随机博弈中学习时态任务的最优策略

    Learning Optimal Strategies for Temporal Tasks in Stochastic Games. (arXiv:2102.04307v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2102.04307](http://arxiv.org/abs/2102.04307)

    本论文提出了一种无模型强化学习方法，用于从给定的LTL规范中学习最优控制策略，即使环境完全未知。该方法通过将问题建模为控制器和对抗环境之间的随机博弈，最大化满足LTL规范的概率，抵抗最坏情况下的环境行为。

    

    从线性时态逻辑（LTL）规范中进行合成可以为在随机和潜在对抗环境中运行的系统提供保证的控制器。然而，自动合成工具需要一个环境模型来构建控制器。在这项工作中，我们引入了一种无模型强化学习（RL）方法，用于从给定的LTL规范中推导控制器，即使环境完全未知。我们将问题建模为控制器和对抗环境之间的随机博弈（SG），然后学习最优控制策略，以最大化满足LTL规范的概率，抵抗最坏情况下的环境行为。我们首先使用从给定LTL规范翻译的确定性奇偶自动机（DPA）构建一个乘积博弈。通过从DPA接受条件导出不同的奖励和折扣因子，我们将最大化最坏情况下满足LTL规范的概率的问题简化为一个强化学习问题。

    Synthesis from linear temporal logic (LTL) specifications provides assured controllers for systems operating in stochastic and potentially adversarial environments. Automatic synthesis tools, however, require a model of the environment to construct controllers. In this work, we introduce a model-free reinforcement learning (RL) approach to derive controllers from given LTL specifications even when the environment is completely unknown. We model the problem as a stochastic game (SG) between the controller and the adversarial environment; we then learn optimal control strategies that maximize the probability of satisfying the LTL specifications against the worst-case environment behavior. We first construct a product game using the deterministic parity automaton (DPA) translated from the given LTL specification. By deriving distinct rewards and discount factors from the acceptance condition of the DPA, we reduce the maximization of the worst-case probability of satisfying the LTL specifi
    

