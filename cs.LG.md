# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine Perception.](http://arxiv.org/abs/2306.06362) | Aria数字孪生是一个自我中心数据集，具有其它任何数据集都没有的高精度、照片逼真和详尽的真实信息。这个数据集将成为自我中心机器感知评估的新标准。 |
| [^2] | [3D reconstruction using Structure for Motion.](http://arxiv.org/abs/2306.06360) | 该论文介绍了一种基于运动的结构的三维重建技术，利用一对HDR照相机和室内移动地面机器人进行捕捉和算法推算，实现室内空间的深度图可视化。 |
| [^3] | [Language-Guided Traffic Simulation via Scene-Level Diffusion.](http://arxiv.org/abs/2306.06344) | 该论文提出了一种可以受到语言指导的场景级条件扩散模型，该模型能够生成真实且可控的交通，并通过大型语言模型将用户的查询转化为损失函数，指导模型生成符合查询条件的仿真交通。 |
| [^4] | [ECGBERT: Understanding Hidden Language of ECGs with Self-Supervised Representation Learning.](http://arxiv.org/abs/2306.06340) | ECGBERT是一种自监督表示学习方法，可对ECG的潜在语言进行解锁，减轻了医学数据缺乏良好标注和精选数据的挑战。在多个基于ECG的任务中，ECGBERT表现出了实现最先进结果的潜力。 |
| [^5] | [Machine Learning Based Missing Values Imputation in Categorical Datasets.](http://arxiv.org/abs/2306.06338) | 本文研究了使用机器学习算法预测和插补分类数据集中的缺失值，使用ECOC框架的集成模型相比于单个模型效果更好，但使用深度学习存在挑战和局限性。 |
| [^6] | [How to Learn and Generalize From Three Minutes of Data: Physics-Constrained and Uncertainty-Aware Neural Stochastic Differential Equations.](http://arxiv.org/abs/2306.06335) | 本文提出了一种使用神经随机微分方程学习控制动力学模型的方法，其中利用了物理知识作为归纳偏置，并在设计中利用距离感知的估计不确定性，在小数据集上进行训练，同时可以在长时间范围内进行准确预测，可用于模型预测控制和模型基础增强学习。 |
| [^7] | [Autonomous Drifting with 3 Minutes of Data via Learned Tire Models.](http://arxiv.org/abs/2306.06330) | 提出了一种基于神经常微分方程和神经-ExpTanh参数化的新型轮胎力模型，并将其作为现有非线性模型预测控制框架中解析刷式轮胎模型的替代品,成功实现了仅利用少于三分钟的驾驶数据，实现高性能的自主漂移 |
| [^8] | [HIPODE: Enhancing Offline Reinforcement Learning with High-Quality Synthetic Data from a Policy-Decoupled Approach.](http://arxiv.org/abs/2306.06329) | HIPODE是一种新的离线强化学习数据增强方法，通过选择接近数据集分布中潜在高价值状态的候选状态来生成高质量的合成数据，增强了下游ORL性能和适用性。 |
| [^9] | [Any-dimensional equivariant neural networks.](http://arxiv.org/abs/2306.06327) | 该论文提出了一个新的方法，利用代数拓扑中的表示稳定性，可以定义出一个可以以任意维度为输入的等变神经网络。这种方法使用方便，只需指定网络架构和等变性的组，且在任何训练过程中都可以使用。 |
| [^10] | [Explaining a machine learning decision to physicians via counterfactuals.](http://arxiv.org/abs/2306.06325) | 本文提出了一种基于变分自编码器的方法来生成在临床环境中有用的现实时间序列反事实论证(CFs)，以帮助医生解释机器学习模型的决策。 |
| [^11] | [Learning Joint Latent Space EBM Prior Model for Multi-layer Generator.](http://arxiv.org/abs/2306.06323) | 本文研究了学习多层生成器模型的基本问题，并提出了一种以多层生成器为骨干的联合潜在空间EBM先验模型，该模型可以更好地学习复杂的数据分布和分层表示，实现更好的生成质量和修复结果。 |
| [^12] | [Towards Arabic Multimodal Dataset for Sentiment Analysis.](http://arxiv.org/abs/2306.06322) | 本文针对阿拉伯语DL-based MSA领域缺乏标准数据集的问题，通过使用最先进的转换器和单词对齐技术中的特征提取工具来设计管道流程，构建了阿拉伯语多模态数据集，实验表明其具有很大的潜力。 |
| [^13] | [Multi-Task Knowledge Enhancement for Zero-Shot and Multi-Domain Recommendation in an AI Assistant Application.](http://arxiv.org/abs/2306.06302) | 本文提出了一种利用多领域交互信息和外部知识图来进行新领域预测的方法，并将其应用于一个AI助手应用中，以提高推荐系统的预测准确性。 |
| [^14] | [Response Time Improves Choice Prediction and Function Estimation for Gaussian Process Models of Perception and Preferences.](http://arxiv.org/abs/2306.06296) | 本论文提出一种新颖的方法，将响应时间考虑进高斯过程模型的选择预测和函数估计中，使得更容易在连续多元函数逼近模型下进行选择预测和函数估计。 |
| [^15] | [PLPCA: Persistent Laplacian Enhanced-PCA for Microarray Data Analysis.](http://arxiv.org/abs/2306.06292) | 本论文提出了一种新方法PLPCA，用于解决PCA在微阵列数据分析中的局限性，通过持续谱图理论结合拉普拉斯增强来实现多尺度分析和捕捉数据中的高阶交互作用。 |
| [^16] | [Optimal Heterogeneous Collaborative Linear Regression and Contextual Bandits.](http://arxiv.org/abs/2306.06291) | 本文提出了一种新的估计器MOLAR，它利用协同线性回归和上下文臂问题中的稀疏异质性来提高估计精度，并且相比独立方法具有更好的表现。 |
| [^17] | [14 Examples of How LLMs Can Transform Materials Science and Chemistry: A Reflection on a Large Language Model Hackathon.](http://arxiv.org/abs/2306.06283) | 本文记录了一次黑客松活动，参与者使用LLMs进行了各种应用，包括预测分子和材料特性、从非结构化数据中提取知识、为工具设计新界面以及开发新的教育应用。这些多样化的项目反映了LLMs在材料科学和化学领域的多功能性和潜力。 |
| [^18] | [Energy-Dissipative Evolutionary Deep Operator Neural Networks.](http://arxiv.org/abs/2306.06281) | 能量耗散进化深度算子神经网络是一种运算学习神经网络，可为一类偏微分方程提供数值解并保留其物理特性，通过支路网络编码不同的输入函数，干线网络评估输出函数，经过训练可生成运算符近似解。 |
| [^19] | [Contrastive Learning for Predicting Cancer Prognosis Using Gene Expression Values.](http://arxiv.org/abs/2306.06276) | 该论文提出应用对比学习方法，从有限的数据样本中学习基因表达数据的良好特征表示，并应用于Cox模型中，可以显著提高癌症预后的预测性能。 |
| [^20] | [DeepLCZChange: A Remote Sensing Deep Learning Model Architecture for Urban Climate Resilience.](http://arxiv.org/abs/2306.06269) | 本研究介绍了一个遥感深度学习架构，名为DeepLCZChange，结合了空中LiDAR数据和Landsat 8卫星的地表温度产品，用于研究城市土地利用与当地气候之间的关系。在纽约市的应用中验证了城市森林的降温效应。 |
| [^21] | [Attention-stacked Generative Adversarial Network (AS-GAN)-empowered Sensor Data Augmentation for Online Monitoring of Manufacturing System.](http://arxiv.org/abs/2306.06268) | 本文提出了一种名为AS-GAN的技术，使用数据增强来解决监督式机器学习中的数据不平衡问题，其中AS-GAN有效地学习异常状态数据的基础分布，并生成高质量的数据样本用于在线制造系统监测。 |
| [^22] | [Near-optimal Conservative Exploration in Reinforcement Learning under Episode-wise Constraints.](http://arxiv.org/abs/2306.06265) | 本文研究了强化学习中实现保守探索的问题，提出了名为StepMix的算法，利用现有的安全基线策略平衡开发和探索，同时保证每个回合不违反保守限制，并且能够在不受限制的情况下达到接近最优的后悔量级。 |
| [^23] | [Measuring and Modifying Factual Knowledge in Large Language Models.](http://arxiv.org/abs/2306.06264) | 这项研究提出了一种基于信息论的测量框架，可用于衡量大型语言模型中的事实知识，并通过熵及KL散度等度量指标进行知识修改，超越了以前的排名方法，并提供了一种有价值的工具，用于测量和修改LLMs中的大量事实知识。 |
| [^24] | [Spectral gap-based deterministic tensor completion.](http://arxiv.org/abs/2306.06262) | 本文界定了Poisson loss和原子范数最小化两种张量补全方法的解的泛化误差，提供了更紧的界限，针对$r$的依赖性从之前的$r^{2(t-1)(t^2-t-1)}$改进为$r^{2(t-1)(3t-5)}$。根据采样稀疏模式的谱间隔控制误差界限，同时证明了原子张量范数的几个新属性，但原子范数最小化存在计算挑战。 |
| [^25] | [Understanding the Benefits of Image Augmentations.](http://arxiv.org/abs/2306.06254) | 本论文通过分析各种宽度和深度的ResNets模型并对其进行增广，发现使用两个图像信息的增广对所学权重的影响显着大于单个图像的影响，使用ImageNet-1K权重和微调的ResNets深层比早期层受到更大的影响。 |
| [^26] | [Decision Stacks: Flexible Reinforcement Learning via Modular Generative Models.](http://arxiv.org/abs/2306.06253) | 决策堆叠是一种灵活的生成框架，将目标条件化策略代理分解为3个生成模块，并通过教师强制并行学习，可以用于设计单个模块以考虑架构偏差、优化目标和动态、跨领域的可转移性和推理速度。在连续控制基准测试中具有灵活性和可扩展性。 |
| [^27] | [Feature Programming for Multivariate Time Series Prediction.](http://arxiv.org/abs/2306.06252) | 该论文提出了一种基于特征编程的多元时间序列预测框架，能够生成大量预测特征，同时允许用户以最小的工作量结合他们的归纳偏差。该框架基于细粒度的轨迹增量，引入了新型自旋气体动力学伊辛模型，并提供了一个简约的算子集来总结多元时间序列。 |
| [^28] | [Design Principles for Generalization and Scalability of AI in Communication Systems.](http://arxiv.org/abs/2306.06251) | 本文提出了可持续和可扩展的AI集成通信系统的设计原则，侧重于创建具备通用性的AI算法，通过少量的AI驱动的RAN函数来解决更大的问题，提高系统性能，并简化生命周期管理。 |
| [^29] | [Strategic Apple Tasting.](http://arxiv.org/abs/2306.06250) | 本篇论文介绍了一种带有苹果品尝反馈的在线学习问题，该问题需要使用一种学习算法以实现战略遗憾。研究结果表明，我们提出的算法的战略遗憾近似于最佳速率。 |
| [^30] | [Online Learning with Set-Valued Feedback.](http://arxiv.org/abs/2306.06247) | 本文研究了一种在线多类分类的变体，其中使用集合型反馈。通过引入新的组合维度，该论文表明确定性和随机性的在线可学习性在实现设置下不等价，并将在线多标签排名和在线多标签分类等实际学习设置作为其特定实例。 |
| [^31] | [Understanding the Effect of the Long Tail on Neural Network Compression.](http://arxiv.org/abs/2306.06238) | 本文研究了在神经网络压缩中如何保持与原始网络的语义相同，在长尾现象中探讨了压缩提高推广性能的记忆要素。 |
| [^32] | [iPLAN: Intent-Aware Planning in Heterogeneous Traffic via Distributed Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2306.06236) | 论文提出了基于MARL算法的iPLAN方法，可在高密度且异构交通场景下进行意图感知规划，使智能体能够从局部观测中推断附近驾驶者的意图，并通过行为或瞬时激励进行决策，实现自主导航。 |
| [^33] | [AVScan2Vec: Feature Learning on Antivirus Scan Data for Production-Scale Malware Corpora.](http://arxiv.org/abs/2306.06228) | 本文提出AVScan2Vec，一种针对杀毒扫描数据的语言模型，旨在将其作为恶意软件的可扩展特征源，以解决生产规模恶意软件库中文件数众多和计算复杂度高的问题。 |
| [^34] | [Robust Twin Parametric Margin Support Vector Machine for Multiclass Classification.](http://arxiv.org/abs/2306.06213) | 提出了双参数边界支持向量机模型来解决多类分类问题，并通过鲁棒优化技术使其更加鲁棒。初步实验结果表明其具有良好的性能。 |
| [^35] | [Single-Model Attribution via Final-Layer Inversion.](http://arxiv.org/abs/2306.06210) | 本文提出了一种利用最终层反演和异常检测的开放式单模型归因方法，解决了以往方法要么局限于封闭式环境、要么需要对生成模型进行不必要的改变的问题。实验结果表明该方法优于现有方法。 |
| [^36] | [Backdoor Attack with Sparse and Invisible Trigger.](http://arxiv.org/abs/2306.06209) | 本论文提出了一种名为SIBA的稀疏隐形后门攻击方法，解决了现有后门攻击存在的可见或稀疏性不足等问题。 |
| [^37] | [A Differential Testing Framework to Evaluate Image Recognition Model Robustness.](http://arxiv.org/abs/2306.06208) | 本文提出了一种差分测试框架，用于评估图像识别模型鲁棒性。该框架通过使用一组参考图像并扰动计算环境，可确定模型性能是否受到计算环境变化的影响。 |
| [^38] | [PotatoPestNet: A CTInceptionV3-RS-Based Neural Network for Accurate Identification of Potato Pests.](http://arxiv.org/abs/2306.06206) | 该论文提出了一种基于AI的自动马铃薯害虫识别系统PotatoPestNet，使用了八种马铃薯害虫的数据集和五种预训练转移学习模型进行模型训练，并利用随机搜索优化进行超参数调整，能够准确识别不同种类的马铃薯害虫。 |
| [^39] | [FLSL: Feature-level Self-supervised Learning.](http://arxiv.org/abs/2306.06203) | 本文提出FLSL方法，采用Transformer进行联合嵌入和聚类，适合于内视图和外视图特征聚类。实验证明该方法在语义类簇表达方面取得显著改进。 |
| [^40] | [NeuroGraph: Benchmarks for Graph Machine Learning in Brain Connectomics.](http://arxiv.org/abs/2306.06202) | 本文介绍了神经成像领域的图机器学习基准测试NeuroGraph，并探讨了数据集生成的搜索空间。 |
| [^41] | [Reliability Check: An Analysis of GPT-3's Response to Sensitive Topics and Prompt Wording.](http://arxiv.org/abs/2306.06199) | 本文分析了大型语言模型GPT-3对敏感话题和提示措辞的反应，发现其在阴谋论和刻板印象方面有正确的反应，但在误解和争议方面存在错误，并具有不可靠性。 |
| [^42] | [ElectroCardioGuard: Preventing Patient Misidentification in Electrocardiogram Databases through Neural Networks.](http://arxiv.org/abs/2306.06196) | 本研究提出了一种基于神经网络的小而高效模型，用于确定两个心电图(ECG)是否来自同一患者。同时，还提出了一种方法，利用该模型检测记录-分配错误，实现了在现实场景中的应用，并在新的ECG数据集上进行了评估。 |
| [^43] | [Public Transit Demand Prediction During Highly Dynamic Conditions: A Meta-Analysis of State-of-the-Art Models and Open-Source Benchmarking Infrastructure.](http://arxiv.org/abs/2306.06194) | 本论文基于智能卡数据的时间序列预测哥伦比亚波哥大BRT系统隔日需求，建立了一个开源基础设施并评估了五种常见方法在稳定和高度动态条件下的表现，结果显示大多数测试模型的表现相似，MAAPE从0.08到0.12不等。 |
| [^44] | [Ada-NAV: Adaptive Trajectory-Based Sample Efficient Policy Learning for Robotic Navigation.](http://arxiv.org/abs/2306.06192) | Ada-NAV是一种自适应轨迹优化策略学习方法，采用降低策略随机性的方法平衡探索与利用，提高机器人导航任务的采样效率。在真实世界的测试中表现优异，可以在更短的采样时间内取得更高的性能。 |
| [^45] | [Open Data on GitHub: Unlocking the Potential of AI.](http://arxiv.org/abs/2306.06191) | GitHub是全球最大的协作软件开发平台之一，托管了超过8亿个开放数据文件，共计142TB的数据；研究发现，在过去四年中其开放数据资产经历了加速增长，有助于加速人工智能研究，解决复杂社会问题。 |
| [^46] | [$FPDM$: Domain-Specific Fast Pre-training Technique using Document-Level Metadata.](http://arxiv.org/abs/2306.06190) | 本文提出了$FPDM$，使用文档元数据和领域特定分类作为监督信号，对领域特定语料库进行transformer编码器的预训练。$FPDM$通过句子级别的输入预训练开放领域的编码器，在微调时使用词汇级别的输入，性能优于其他基于transformer的模型。 |
| [^47] | [FasterViT: Fast Vision Transformers with Hierarchical Attention.](http://arxiv.org/abs/2306.06189) | 本研究设计了一种新型混合CNN-ViT神经网络FasterViT，引入了具有分层注意力的方法HAT，将全局自我注意力分解为多级注意力，实现了高效的跨窗口通信。 FasterViT在精度和图像吞吐量方面达到了SOTA前沿水平，并已在分类，物体检测和分割等CV任务中得到广泛验证。 |
| [^48] | [A Unified Model and Dimension for Interactive Estimation.](http://arxiv.org/abs/2306.06184) | 该论文提出了交互式估计的统一学习框架，引入组合度量差异维度来捕捉模型的可学习性，提出了多项式的遗憾和PAC泛化界限的算法，并统一了统计查询学习和结构化赌博问题。 |
| [^49] | [Hidden symmetries of ReLU networks.](http://arxiv.org/abs/2306.06179) | 研究发现，对于前馈ReLU神经网络的任何固定架构，在没有比输入层更窄的层的情况下，存在没有隐藏对称性的参数设置。此外，随着深度的增加，网络没有隐藏对称性的概率逐渐降低到0。 |
| [^50] | [Active-Learning-Driven Surrogate Modeling for Efficient Simulation of Parametric Nonlinear Systems.](http://arxiv.org/abs/2306.06174) | 本论文提出了一种基于主动学习的优化准则来有效构建参数化降阶代理模型，称为ActLearn-POD-KSNN代理模型，并通过数值实验验证了其有效性。 |
| [^51] | [Fault Localization for Framework Conversions of Image Recognition Models.](http://arxiv.org/abs/2306.06157) | 本文提出针对深度学习框架转换中出现的模型崩溃和输出标签差异的故障定位和修复方法，成功修复多个图像识别模型跨多个深度学习框架的转换错误。 |
| [^52] | [PoET: A generative model of protein families as sequences-of-sequences.](http://arxiv.org/abs/2306.06156) | PoET是一个模型，能够生成任何蛋白质家族的一系列相关蛋白质序列，可以用作检索增强语言模型生成和评分任何修改 |
| [^53] | [Intensity Profile Projection: A Framework for Continuous-Time Representation Learning for Dynamic Networks.](http://arxiv.org/abs/2306.06155) | 本文提出了一种连续时间网络表示学习框架，涵盖核平滑的强度函数估计、最小化强度重构误差的投影学习和归纳构造节点表示。这种表示保留了网络结构和时间一致性。 |
| [^54] | [HypLL: The Hyperbolic Learning Library.](http://arxiv.org/abs/2306.06154) | HypLL是一个使用希亚空间的深度学习库，基于PyTorch，旨在使其易于使用，搭建希亚网络模块，特别适用于处理层次化数据和使用少量嵌入维度，是一种新的、开放的研究方向。 |
| [^55] | [EfficientBioAI: Making Bioimaging AI Models Efficient in Energy, Latency and Representation.](http://arxiv.org/abs/2306.06152) | EfficientBioAI是一个即插即用的工具箱，可以压缩生物成像AI模型，使它们在CPU和GPU上运行时能够显著减少能源成本和推理时间，而不会影响准确性。 |
| [^56] | [Artificial intelligence and radiation protection. A game changer or an update?.](http://arxiv.org/abs/2306.06148) | 本文介绍了基于机器学习的辐射防护方法，探讨了人工智能在辐射防护中的潜在优势和障碍，并提出了促进科学技术成果的合作方式。 |
| [^57] | [Hidden Classification Layers: a study on Data Hidden Representations with a Higher Degree of Linear Separability between the Classes.](http://arxiv.org/abs/2306.06146) | 本文中，研究了一种新颖的培训方法影响深层网络分类器性能，并提出了一个新的神经网络架构，在数据隐藏表示中达到更高的线性可分性。 |
| [^58] | [LDMRes-Net: Enabling Real-Time Disease Monitoring through Efficient Image Segmentation.](http://arxiv.org/abs/2306.06145) | LDMRes-Net是一种轻量级高效的网络，通过采用双重多重残差连接来提高分割性能并最小化计算成本，可用于实时视网膜图像分析任务，并在8个公开数据集上取得了有希望的分割结果。 |
| [^59] | [Null/No Information Rate (NIR): a statistical test to assess if a classification accuracy is significant for a given problem.](http://arxiv.org/abs/2306.06140) | 该论文介绍了一种被称为NIR的统计检验方法，可以用来评估分类系统的性能是否具有统计显著性。 |
| [^60] | [WePaMaDM-Outlier Detection: Weighted Outlier Detection using Pattern Approaches for Mass Data Mining.](http://arxiv.org/abs/2306.06139) | 本文提出了一种使用模式方法进行加权异常值检测的大数据挖掘方法，可揭示系统故障、欺诈活动和数据模式的重要信息。 |
| [^61] | [Extraction and Recovery of Spatio-Temporal Structure in Latent Dynamics Alignment with Diffusion Model.](http://arxiv.org/abs/2306.06138) | 该论文提出了一种利用扩散模型提取和恢复潜在动力学对齐的时空结构的方法，以解决现有方法忽略潜在动力学时空结构导致对齐后性能质量较差的问题。 |
| [^62] | [Robustness Testing for Multi-Agent Reinforcement Learning: State Perturbations on Critical Agents.](http://arxiv.org/abs/2306.06136) | 提出了一个新的多智能体强化学习鲁棒性测试框架RTCA，采用基于差分进化的关键智能体攻击方法，关键智能体通过团队合作政策评估方法被选取为受害者。该框架在鲁棒性测试中表现出优异的性能。 |
| [^63] | [Safety and Fairness for Content Moderation in Generative Models.](http://arxiv.org/abs/2306.06135) | 生成模型在训练数据中模仿最糟糕的内容，通过安全输入和输出过滤器实现负责任部署；通过安全、公平和指标公平的定义，列举了每个领域可能遇到的例子损害，并提供了损害量化的演示。 |
| [^64] | [Sound Explanation for Trustworthy Machine Learning.](http://arxiv.org/abs/2306.06134) | 本篇论文提出了声音解释的概念，以提供足够的信息来因果解释机器学习系统进行的预测，反对通过将分数归因于输入组件来解释黑盒模型的惯例，并且提出应用特征选择作为癌症预测模型的声音解释，以建立临床医生之间的信任。 |
| [^65] | [Towards Understanding the Interplay of Generative Artificial Intelligence and the Internet.](http://arxiv.org/abs/2306.06130) | 探究生成AI和互联网之间互动的影响，生成AI可能成为数据仓库贡献者并影响后续训练，提出未来版本生成AI工具在使用混合数据训练时会出现的问题和挑战。 |
| [^66] | [Energy-efficient Wearable-to-Mobile Offload of ML Inference for PPG-based Heart-Rate Estimation.](http://arxiv.org/abs/2306.06129) | 本文提出了一种协作推理方法，使用智能手表和连接的智能手机来最大化心率跟踪的性能并最大程度地延长电池寿命，并在自定义的智能手表原型上对方法进行了基准测试。实验证明此方法可以提高8.5\%的HR跟踪精度和降低37\%的能量消耗。 |
| [^67] | [Deep Learning Method for Object Tracking, Velocity Estimation and Projection of Sensor Data over Time.](http://arxiv.org/abs/2306.06126) | 本文提出了一种利用Transformer机制的新型循环神经网络单元，实现了利用传感器记录中的时空相关性来实现目标跟踪和速度估计，并将记忆状态进行投影。 |
| [^68] | [Joint Channel Estimation and Feedback with Masked Token Transformers in Massive MIMO Systems.](http://arxiv.org/abs/2306.06125) | 本论文提出了一种基于深度学习和掩码Token Transformer的联合信道估计和反馈框架，有效提高了大规模MIMO系统中信道估计和反馈的性能。 |
| [^69] | [Unsupervised clustering of disturbances in power systems via deep convolutional autoencoders.](http://arxiv.org/abs/2306.06124) | 本文提出了一种基于卷积自编码器和K-means聚类的无监督分类方法，可将电力系统中的干扰波形聚类为涉及电压下降、中断、瞬态、正常和谐波畸变等类别，从而实现过滤异常波形和常规波形。 |
| [^70] | [Adversarial Attacks and Defenses in Explainable Artificial Intelligence: A Survey.](http://arxiv.org/abs/2306.06123) | 本文总结了对抗性攻击和防御在可解释人工智能中的研究。列出了现有的不安全因素，并表明了本领域的新兴研究方向。 |
| [^71] | [Doubly Stochastic Graph-based Non-autoregressive Reaction Prediction.](http://arxiv.org/abs/2306.06119) | 本文提出了一种新的框架，该框架将两个双重随机自注意映射结合起来，实现遵循化学反应物理约束的电子再分配预测。 |
| [^72] | [Estimation of River Water Surface Elevation Using UAV Photogrammetry and Machine Learning.](http://arxiv.org/abs/2306.06118) | 无人机摄影测量技术结合卷积神经网络可以准确测定河流水面高程，并使用线性回归方法进一步改进了估算准确性。 |
| [^73] | [Strengths and Weaknesses of 3D Pose Estimation and Inertial Motion Capture System for Movement Therapy.](http://arxiv.org/abs/2306.06117) | 本研究比较了3D姿势估计和惯性运动捕捉系统在特定运动锻炼中的精度，为临床应用提供了有用信息。 |
| [^74] | [Overview of Deep Learning Methods for Retinal Vessel Segmentation.](http://arxiv.org/abs/2306.06116) | 本文对近期用深度学习方法进行视网膜血管分割的研究进行了回顾与评估，总结了这些方法的设计特点、性能指标以及优缺点。 |
| [^75] | [Surrogate Modeling of Car Drag Coefficient with Depth and Normal Renderings.](http://arxiv.org/abs/2306.06110) | 本论文提出了一种新的二维三维形状表示方法，用于代理建模汽车阻力系数。实验结果表明，该表示方法在阻力预测方面的性能更好，需要使用更小的模型。 |
| [^76] | [Learning to Quantize Vulnerability Patterns and Match to Locate Statement-Level Vulnerabilities.](http://arxiv.org/abs/2306.06109) | 该论文提出了一种新颖的漏洞匹配方法，利用学习的漏洞代码库中的漏洞模式定位易受攻击语句，以解决传统漏洞检测方法中存在的识别漏洞范围不准确的问题。 |
| [^77] | [Demystifying Fraudulent Transactions and Illicit Nodes in the Bitcoin Network for Financial Forensics.](http://arxiv.org/abs/2306.06108) | 本文提出了Elliptic++数据集，通过对比特币网络中的交易和地址之间的关系进行分析，旨在以综合应用数据科学方法进行欺诈检测，从而查出欺诈交易和非法地址。 |
| [^78] | [Adversarial Attacks on Leakage Detectors in Water Distribution Networks.](http://arxiv.org/abs/2306.06107) | 本文研究了水配管网络泄漏探测器的对抗性攻击，并提出分类法。重点研究攻击者在水网中寻找最不敏感点，使用三种算法方法寻找解决方案，在两个基准水配管网络上进行评估。 |
| [^79] | [Deep Learning for Day Forecasts from Sparse Observations.](http://arxiv.org/abs/2306.06079) | 本文提出了一种基于稀疏观测数据的MetNet-3深度学习模型，可对20小时内的天气进行准确的预测。MetNet-3的技术创新包括可学习卷积、特征学习和多任务训练优化。此外，使用持续性启发法来外推初始条件或进行短期预测来填补缺失的观测数据更进一步提高了预测性能。 |
| [^80] | [Neural FIM for learning Fisher Information Metrics from point cloud data.](http://arxiv.org/abs/2306.06062) | 本文提出了神经FIM，一种从点云数据中计算Fisher信息度量（FIM）的方法，可以连续地对数据进行流形建模，从而提高流形特征的描述能力。 |
| [^81] | [Time Series Continuous Modeling for Imputation and Forecasting with Implicit Neural Representations.](http://arxiv.org/abs/2306.05880) | 该论文提出了基于INR的时间序列连续建模方法，解决了处理缺失数据、不规则采样和多传感器不对准观测等重复建模问题，并在预测和插值任务中取得了最新的性能表现，具有很好的泛化能力。 |
| [^82] | [Weight Freezing: A Regularization Approach for Fully Connected Layers with an Application in EEG Classification.](http://arxiv.org/abs/2306.05775) | 本文提出了一种新颖的方法，权重冻结，它可以有效地减少全连接层中的特定神经元对特定EEG任务的决策过程的影响，从而提高人工神经网络（ANNs）的性能，是一种有效的正则化方法。 |
| [^83] | [Boosting Fast and High-Quality Speech Synthesis with Linear Diffusion.](http://arxiv.org/abs/2306.05708) | 本文提出了一种基于线性扩散模型的语音合成方法，能够同时实现快速推理和高样本质量 |
| [^84] | [Finite-Time Analysis of Minimax Q-Learning for Two-Player Zero-Sum Markov Games: Switching System Approach.](http://arxiv.org/abs/2306.05700) | 本文研究了Q-learning算法应用于两人零和Markov博弈的有限时间分析，并通过切换系统方法提供了更简单和深入的收敛分析。 |
| [^85] | [Simulation and Prediction of Countercurrent Spontaneous Imbibition at Early and Late Times Using Physics-Informed Neural Networks.](http://arxiv.org/abs/2306.05554) | 本文通过物理信息神经网络模型对多孔材料中的逆流自发渗透过程进行了早期和晚期的模拟和预测，并使用改变变量技术来改进模型性能。 |
| [^86] | [CoCo: A Coupled Contrastive Framework for Unsupervised Domain Adaptive Graph Classification.](http://arxiv.org/abs/2306.04979) | CoCo是一种耦合对比图表示学习框架，其中包含一个图卷积网络和一个分层图内核网络，通过耦合对比学习减少领域差异，用于无监督领域自适应图分类。 |
| [^87] | [Unsupervised Cross-Domain Soft Sensor Modelling via A Deep Bayesian Particle Flow Framework.](http://arxiv.org/abs/2306.04919) | 该论文提出了一种基于深度粒子流贝叶斯框架的跨领域软测量无监督建模方法，能够解决标签缺失、领域适应性和数据时间一致性等问题，提高软测量建模的准确性。 |
| [^88] | [Fast and Effective GNN Training with Linearized Random Spanning Trees.](http://arxiv.org/abs/2306.04828) | 本文提出了一种基于线性化随机生成树的GNN训练框架，在多个真实世界的图形基准测试中表现得比其他经典算法更快且更准确。 |
| [^89] | [Adaptive Frequency Green Light Optimal Speed Advisory based on Hybrid Actor-Critic Reinforcement Learning.](http://arxiv.org/abs/2306.04660) | 本文提出了一种基于混合Actor-Critic强化学习的自适应频率绿灯最佳速度建议模型，通过使用离散和连续演员网络来优化绿灯最佳速度建议系统的频率和加速度曲线，设计了新的奖励函数来平衡减少停车和最小化速度变化的权衡。实验结果显示，该模型可以有效地减少旅行时间和燃料消耗，并优于现有的最先进的GLOSA系统。 |
| [^90] | [Divide and Repair: Using Options to Improve Performance of Imitation Learning Against Adversarial Demonstrations.](http://arxiv.org/abs/2306.04581) | 本文提出了一种利用选项改进模仿学习对抗性示范的性能表现的新技术，可以识别未被对手显着修改的演示轨迹并仅从中进行学习。 |
| [^91] | [Dual policy as self-model for planning.](http://arxiv.org/abs/2306.04440) | 该论文探究了使用精简策略网络作为自我模型的优缺点，并通过实验结果表明，使用经过精简的策略网络作为自我模型可以稳定训练，并且有更快的收敛速度。 |
| [^92] | [End-to-End Learning for Stochastic Optimization: A Bayesian Perspective.](http://arxiv.org/abs/2306.04174) | 本文提出了一种基于贝叶斯视角的随机优化端到端学习方法，为经验风险最小化和分布式鲁棒优化问题提供新的端到端学习算法，方式主要是训练决策映射。该方法在合成的newsvendor问题和经济分配问题上均表现出显著的效果，同时也发现决策映射神经网络架构对测试性能的影响很大。 |
| [^93] | [Quasi-Newton Updating for Large-Scale Distributed Learning.](http://arxiv.org/abs/2306.04111) | 本文提出了一种具有出色统计、计算和通信效率的分布式拟牛顿(DQN)框架，与现有方法相比，它不需要牛顿矩阵求逆或通信，并且通过理论证明和数值分析证明其统计特性和有限的样本性能。 |
| [^94] | [Quick-Tune: Quickly Learning Which Pretrained Model to Finetune and How.](http://arxiv.org/abs/2306.03828) | 本文提出一种方法快速选择最佳的预训练模型和微调超参数，通过生成大规模元数据集并元学习多保真度性能预测器，并在学习新数据集时使用该预测器进行超参数优化，可以快速实现此目标。 |
| [^95] | [Decentralized SGD and Average-direction SAM are Asymptotically Equivalent.](http://arxiv.org/abs/2306.02913) | 分散SGD和平均方向SAM在渐近意义下是等价的，D-SGD表现出梯度平滑效应和锐度正则化效应，而且可以提高后验评估，并证明了潜在的泛化能力 |
| [^96] | [LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion.](http://arxiv.org/abs/2306.02561) | 本论文提出了LLM-Blender，它是一个集成框架，旨在利用不同的开源大型语言模型的优秀特性，实现始终如一的卓越性能。PairRanker和GenFuser是该框架的两个模块，PairRanker使用成对比较方法来区分候选输出，并且GenFuser旨在合并排名最高的候选者，以生成改进的输出。 |
| [^97] | [Evolution of Efficient Symbolic Communication Codes.](http://arxiv.org/abs/2306.02383) | 本文探讨了通过反熵、压缩因子和跨分裂F1分数为目标的交流代码演变产物，发现语言结构形成可以通过这些度量来驱动。 |
| [^98] | [Benchmarking Robustness of Adaptation Methods on Pre-trained Vision-Language Models.](http://arxiv.org/abs/2306.02080) | 研究针对预训练视觉语言模型的11种适应方法在不同污染情况下的鲁棒性，发现适应方法对文本污染更敏感，单独使用小型文本适配器比共享适配器更鲁棒，可获得可比较的干净性能。 |
| [^99] | [DVFO: Dynamic Voltage, Frequency Scaling and Workload Offloading for DNN Edge Inference.](http://arxiv.org/abs/2306.01811) | 提出了一种动态电压、频率缩放和工作负载卸载的DNN边缘推理框架DVFO，它通过深度强化学习来联合优化DVFS和卸载参数，实现了对边缘设备计算资源的优化，同时提高了DNN模型的能源效率。 |
| [^100] | [DeepfakeArt Challenge: A Benchmark Dataset for Generative AI Art Forgery and Data Poisoning Detection.](http://arxiv.org/abs/2306.01272) | 本文介绍了DeepfakeArt Challenge，这是一个专门为帮助构建机器学习算法以进行生成AI艺术伪造和数据污染检测而设计的大规模挑战基准数据集。 |
| [^101] | [Large-Batch, Neural Multi-Objective Bayesian Optimization.](http://arxiv.org/abs/2306.01095) | 本文提出了一种针对数据密集型问题和多目标优化设置的贝叶斯优化框架，该方法利用了贝叶斯神经网络代理建模和可扩展、具有不确定性的收购策略，能够在最少迭代次数的情况下高效地进行优化。 |
| [^102] | [Diffusion Self-Guidance for Controllable Image Generation.](http://arxiv.org/abs/2306.00986) | 本论文提出了一种扩散自导方法，通过引导扩散模型的内部表示来提供对生成图像的更大控制，可以用于执行具有挑战性的图像操作，同时不需要额外模型或训练。 |
| [^103] | [TriSig: Assessing the statistical significance of triclusters.](http://arxiv.org/abs/2306.00643) | 本文提出了一个新的统计框架用于评估张量数据中模式的显著性，该框架扩展了矩阵数据模式的统计学原理，并从多个生物化学和生物技术领域的实际案例研究中收集了结果。 |
| [^104] | [Causal Imitability Under Context-Specific Independence Relations.](http://arxiv.org/abs/2306.00585) | 本文探讨了特定上下文独立关系对因果模仿学习的影响，证明了这种情况下的模仿可行性决策问题是NP难的，并提供了必要的图形标准以及一个有声的算法方法。 |
| [^105] | [Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning.](http://arxiv.org/abs/2306.00477) | 本研究尝试实现在预训练语言模型中运用可逆模型实现高效的微调，并发现在初始化微调时保留PLM的起点非常重要。 |
| [^106] | [Improving Expressivity of GNNs with Subgraph-specific Factor Embedded Normalization.](http://arxiv.org/abs/2305.19903) | 本文提出了一种名为SuperNorm的专用归一化方案，通过嵌入子图特定因子和纳入图实例特定统计数据来加强GNN的代表性能力，实现对节点感应子图中内部连接信息的明确考虑，从而改善GNN的表达能力。 |
| [^107] | [Fine-grained Text Style Transfer with Diffusion-Based Language Models.](http://arxiv.org/abs/2305.19512) | 本文提出了一种基于扩散式语言模型的细粒度文本风格转换方法，在不依赖外部信息的情况下取得了比之前利用预训练权重、嵌入和外部语法分析器更好的效果，表明扩散概率模型在文本生成领域具有广泛的应用前景。 |
| [^108] | [SimFBO: Towards Simple, Flexible and Communication-efficient Federated Bilevel Learning.](http://arxiv.org/abs/2305.19442) | SimFBO和其ShroFBO变体提出了一个简单、灵活且通信高效的FBO框架，可以应用于元学习和超参数优化任务。 |
| [^109] | [Mitigating Label Biases for In-context Learning.](http://arxiv.org/abs/2305.19148) | 本文针对上下文学习（ICL）中的三种标签偏差提出分类法，并提出一种简单的偏差校准方法，使用随机的领域词估算语言模型的标签偏差。 |
| [^110] | [W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition.](http://arxiv.org/abs/2305.18624) | W-procer是一种基于加权原型对比学习的医学少样本命名实体识别方法，在构建基于原型的对比损失和加权网络方面具有创新性，优于现有的最先进方法。 |
| [^111] | [Scalable and Weakly Supervised Bank Transaction Classification.](http://arxiv.org/abs/2305.18430) | 本文提出了一种可扩展的银行交易分类方法，利用弱监督、自然语言处理和深度神经网络技术，最小化对手动注释的依赖，能够快速扩展到新的和组合用例，可用于财务健康报告和信用风险评估等金融应用。 |
| [^112] | [Modeling Dynamic Environments with Scene Graph Memory.](http://arxiv.org/abs/2305.17537) | 本论文提出了一种新的场景图记忆状态表示，结合节点边缘预测器（NEP）的神经网络架构，能够帮助具有行动能力的AI代理在部分可观察动态场景中高效搜索。 |
| [^113] | [Distilling BlackBox to Interpretable models for Efficient Transfer Learning.](http://arxiv.org/abs/2305.17303) | 本论文提出了一种方法可以将黑盒模型转化成为可解释模型并在目标领域成功进行迁移学习，从而实现高效迁移学习。 |
| [^114] | [Revisiting Generalized p-Laplacian Regularized Framelet GCNs: Convergence, Energy Dynamic and Training with Non-Linear Diffusion.](http://arxiv.org/abs/2305.15639) | 本文全面分析了基于图p-Laplacian的Framelet网络，证明了其具有良好的收敛性和能量动态，并能够通过广义非线性扩散过程进行多种训练。同时，该模型能够适应同质和异质数据，避免过度平滑问题。 |
| [^115] | [Black-Box vs. Gray-Box: A Case Study on Learning Table Tennis Ball Trajectory Prediction with Spin and Impacts.](http://arxiv.org/abs/2305.15189) | 本研究提出了一种灰盒子方法来预测乒乓球轨迹，比两种黑盒子方法预测性能更好。使用神经网络从球发射器的参数中初始化自旋可以显着提高预测性能，并通过评估实现了高回球率。 |
| [^116] | [Understanding Programs by Exploiting (Fuzzing) Test Cases.](http://arxiv.org/abs/2305.13592) | 本文提出了通过模糊测试获取代表性输入来帮助语义理解程序的方法。 |
| [^117] | [Tight conditions for when the NTK approximation is valid.](http://arxiv.org/abs/2305.13141) | 我们研究了NTK逼近在平方损失下训练模型的有效性，发现在模型缩放因子$\alpha=O(T)$时在训练时间$T$之前可以使得NTK逼近有效。 |
| [^118] | [Stability, Generalization and Privacy: Precise Analysis for Random and NTK Features.](http://arxiv.org/abs/2305.12100) | 本论文研究了ERM训练模型对抗强大黑盒攻击的安全问题，并通过两个指标量化模型安全性：单个样本的稳定性和查询与原始数据特征的对齐。在研究中，通过研究RF和NTK回归，证明随着泛化能力的提高，隐私保护可以得到加强。 |
| [^119] | [A Scalable Walsh-Hadamard Regularizer to Overcome the Low-degree Spectral Bias of Neural Networks.](http://arxiv.org/abs/2305.09779) | 本文提出了一种新型的可扩展Walsh-Hadamard正则化器，可避免神经网络学习低阶频率以及误识别低阶频率，从而提高了神经网络的泛化性能。 |
| [^120] | [Vehicle Detection and Classification without Residual Calculation: Accelerating HEVC Image Decoding with Random Perturbation Injection.](http://arxiv.org/abs/2305.08265) | 本文介绍了一种用于交通监控的新方法，通过随机扰动重建图像，而不使用残差数据，从而显著减少图像重构所需的数据。该方法通过创建原始图像的精简表示，并保留与视频理解任务相关的信息，重点关注车辆检测和分类等关键用例。 |
| [^121] | [Causal Information Splitting: Engineering Proxy Features for Robustness to Distribution Shifts.](http://arxiv.org/abs/2305.05832) | 本文提出了利用因果机制在不同环境下保持不变的直觉来主动准备的代理特征选择和工程技术，用以应对统计预测模型在分布转移情况下的稳定性问题。 |
| [^122] | [Scalable Optimal Margin Distribution Machine.](http://arxiv.org/abs/2305.04837) | 本文提出了一种可扩展的最优边缘分布机（ODM）训练方法，与原始ODM训练方法相比，可实现近十倍的加速。对于非线性核，我们提出了一种新颖的分布感知分区方法，使得每个分区上训练的本地ODM接近全局的ODM，并快速收敛。在应用线性核时，我们扩展了一种通信有效的SVRG方法以进一步加速训练。 |
| [^123] | [Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation.](http://arxiv.org/abs/2305.01210) | 本论文提出了一个严格的代码综合基准评估框架EvalPlus，用于评估利用大型语言模型生成的代码的功能正确性。 |
| [^124] | [Proper Scoring Rules for Survival Analysis.](http://arxiv.org/abs/2305.00621) | 本文研究了适用于生存分析的四种评分规则的扩展，证明在概率分布估计离散化程度满足一定条件时是适当评分规则，并且比较结果显示对数得分和布莱尔得分的扩展最佳。 |
| [^125] | [Sensitive Tuning of Large Scale CNNs for E2E Secure Prediction using Homomorphic Encryption.](http://arxiv.org/abs/2304.14836) | 本论文提出一种新的HE友好模型训练方法，成功演示了在ResNet和ConvNeXt等经典和现代CNN上运行加密样本，并以前所未有的方式演示了如何使用CLIP GPT-2模型进行零知识安全预测，是一种可行的隐私保护机器学习解决方案。 |
| [^126] | [ChartSumm: A Comprehensive Benchmark for Automatic Chart Summarization of Long and Short Summaries.](http://arxiv.org/abs/2304.13620) | 本文提出了ChartSumm数据集，用于长短摘要自动生成任务，包括84000多个图表及其元数据和描述。研究发现，现有的自动摘要模型虽然得分不错，但经常面临错觉、漏掉重要数据点以及不正确解释复杂趋势等问题。 |
| [^127] | [(Vector) Space is Not the Final Frontier: Product Search as Program Synthesis.](http://arxiv.org/abs/2304.11473) | 本文主张将产品搜索看作程序合成，相比向量空间模型有着重大优势。 |
| [^128] | [Language Instructed Reinforcement Learning for Human-AI Coordination.](http://arxiv.org/abs/2304.07297) | 本文提出了一种称之为instructRL的新的框架，它通过自然语言指令来指定对人工智能搭档的预期策略，解决在缺乏高质量人类行为数据的领域中多智能体强化学习收敛于人类不偏爱的策略的问题，从而提高了人工智能协作的性能。 |
| [^129] | [Vax-Culture: A Dataset for Studying Vaccine Discourse on Twitter.](http://arxiv.org/abs/2304.06858) | 本文介绍了一个推特疫苗数据集Vax-Culture，它旨在找出推广疫苗错误信息的文化和政治信念的重叠部分，帮助开发机器学习模型以自动检测疫苗错误信息帖子并应对其负面影响。 |
| [^130] | [Energy-guided Entropic Neural Optimal Transport.](http://arxiv.org/abs/2304.06094) | 本论文提出了一种新的方法，将能量基础模型和熵正则化最优输运结合起来，以解决生成建模问题。我们在2D情景和图像到图像翻译问题中验证了该方法的适用性。 |
| [^131] | [Model sparsification can simplify machine unlearning.](http://arxiv.org/abs/2304.04934) | 本文提出了一种基于模型稀疏化的机器反学习方案，称为prune first, then unlearn和sparsity-aware unlearning。此方案可以提高近似反学习器的多标准反学习性能，并在不同的场景中表现出一致的效果。 |
| [^132] | [Incremental Verification of Neural Networks.](http://arxiv.org/abs/2304.01874) | 提出了一种新的、基于设计新的理论、数据结构和算法的神经网络增量与完全验证的通用框架，实现了对MNIST和CIFAR10以及ACAS-XU分类器的更高效的验证。 |
| [^133] | [Improving Few-Shot Inductive Learning on Temporal Knowledge Graphs using Confidence-Augmented Reinforcement Learning.](http://arxiv.org/abs/2304.00613) | 本论文介绍了一种名为FITCARL的TKGC方法，它将少样本学习与强化学习相结合，以实现在时间知识图上进行少样本归纳学习和预测。在FITCARL中，一个智能体通过策略网络来指导搜索过程，通过引入合成样本的模块来解决数据稀缺性，在真实世界数据上取得了最新的结果。 |
| [^134] | [NeRF-GAN Distillation for Efficient 3D-Aware Generation with Convolutions.](http://arxiv.org/abs/2303.12865) | 本文提出了一种基于神经辐射场（NeRF）和生成对抗网络（GAN）的新方法，通过从预训练的NeRF-GAN中提取3D知识进行蒸馏，实现了基于姿态的2D GAN的高效3D感知生成。 |
| [^135] | [GLADE: Gradient Loss Augmented Degradation Enhancement for Unpaired Super-Resolution of Anisotropic MRI.](http://arxiv.org/abs/2303.11831) | 本文提出了一种可用于加速全腹MRI扫描的新方法GLADE，它通过使用梯度映射损失来合成高分辨率等向性3D腹部MR图像。 |
| [^136] | [Reflexion: an autonomous agent with dynamic memory and self-reflection.](http://arxiv.org/abs/2303.11366) | 本文提出 Reflexion 方法，给智能体赋予了动态记忆和自我反思能力，以增强其任务特定的行动选择能力。 |
| [^137] | [Data-centric Artificial Intelligence: A Survey.](http://arxiv.org/abs/2303.10158) | 本文讨论了数据中心人工智能的必要性并从三个一般性数据中心目标和代表性方法的全面视角进行了介绍。该综述从自动化和协作的角度组织了现有文献并讨论了挑战。 |
| [^138] | [Finding Competence Regions in Domain Generalization.](http://arxiv.org/abs/2303.09989) | 该论文提出了一个“学习拒绝”框架来解决领域泛化中的默默失败问题。通过预测可信度，该方法在测试分布与训练分布不同的情况下接受超出分布的数据，以识别能力区域。研究发现，通过不同的学习表示衡量无能，增加无能得分会预示着降低准确性。 |
| [^139] | [On the Robustness of Text Vectorizers.](http://arxiv.org/abs/2303.07203) | 本文研究了文本向量化技术中的鲁棒性问题，并证明了流行的嵌入方案具有Hamming距离意义上的鲁棒性。本研究提供了这些方法的定量边界，并展示了其中的常数受文档长度的影响。 |
| [^140] | [FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU.](http://arxiv.org/abs/2303.06865) | 本论文提出了一种名为FlexGen的技术，使用单个GPU实现大型语言模型的高吞吐推理。FlexGen通过聚合来自GPU、CPU和磁盘的内存和计算，搜索有效的张量存储和访问模式，并将权重和注意力缓存压缩为4个位。这些技术使FlexGen具有更大的批量选择空间，从而显著增加了最大吞吐量。 |
| [^141] | [Transformer-based Planning for Symbolic Regression.](http://arxiv.org/abs/2303.06833) | 该论文提出了一种基于Transformer和蒙特卡罗树搜索的符号回归规划策略TPSR，可以将非可微的反馈作为知识的外部来源融入到方程生成过程中，提高方程生成的准确性和复杂性。 |
| [^142] | [Gaussian Process on the Product of Directional Manifolds.](http://arxiv.org/abs/2303.06799) | 本文提出了一种在超环面上建立高斯过程的方法，并使用内在的核相关模型进行学习，以定义在超环面上的向量值函数。通过使用 HvM-based GP 进行数据驱动递归定位，数值结果表明，在跟踪精度方面，该方法具有优势。 |
| [^143] | [DP-Fast MH: Private, Fast, and Accurate Metropolis-Hastings for Large-Scale Bayesian Inference.](http://arxiv.org/abs/2303.06171) | 本文提出了一种新的DP-Fast MH算法，用于大规模贝叶斯推断，具有精确、快速和隐私保护的特点。 |
| [^144] | [The Wasserstein Believer: Learning Belief Updates for Partially Observable Environments through Reliable Latent Space Models.](http://arxiv.org/abs/2303.03284) | 本论文提出了一种“Wasserstein Belief Updater”算法来学习部分可观测环境下的信念更新，该算法通过学习POMDP​​的潜在模型和置信更新的近似值，实现了对历史观察和行动的有效压缩，提升了算法的性能表现。 |
| [^145] | [GlucoSynth: Generating Differentially-Private Synthetic Glucose Traces.](http://arxiv.org/abs/2303.01621) | 本文提出了一种新颖的GAN框架-GlucoSynth，采用差分隐私机制来生成合成血糖轨迹，保证数据隐私安全的同时能生成高质量的合成数据. |
| [^146] | [DART: Diversify-Aggregate-Repeat Training Improves Generalization of Neural Networks.](http://arxiv.org/abs/2302.14685) | 本文中提出了DART策略，其中利用多样化的增强方法训练不同的模型，然后通过聚合这些模型的权重来结合其专业知识，并重复聚合步骤以实现更好的泛化能力。 |
| [^147] | [(Re)$^2$H2O: Autonomous Driving Scenario Generation via Reversely Regularized Hybrid Offline-and-Online Reinforcement Learning.](http://arxiv.org/abs/2302.13726) | 提出了一个新的自主驾驶场景生成框架，结合了离线实际数据和在线模拟数据，将反向 L_1 正则化用于关键特征提取，并有效地查询更高维度搜索空间中的场景，实验证明了方法的有效性。 |
| [^148] | [Distributional Learning of Variational AutoEncoder: Application to Synthetic Data Generation.](http://arxiv.org/abs/2302.11294) | 提出了一种新的方法扩展了VAE模型容量，采用无限混合的非对称拉普拉斯分布作为解码器，具有分布拟合能力和调整数据隐私级别的优越性。 |
| [^149] | [Revisiting Weighted Aggregation in Federated Learning with Neural Networks.](http://arxiv.org/abs/2302.10911) | 本文重新审视了联邦学习中的加权聚合方法。作者发现权重总和可能小于1，从而改善了泛化性能。作者探索了最优缩小因子如何受到客户端数据异质性和本地周期的影响，并使用客户端相干性研究了客户端之间的相对聚合权重以描绘客户端的重要性。作者提出了一种有效的联邦学习方法（FLLAW），该方法具有可学习聚合权重和全局权重缩小效应。 |
| [^150] | [ChatGPT: Jack of all trades, master of none.](http://arxiv.org/abs/2302.10724) | 本研究检验了 ChatGPT 在 25 个不同的 NLP 任务上的性能，它是一个万能的 AI 模型，但无关紧要的表现可能会对某些任务的表现产生负面影响。 |
| [^151] | [SGD with AdaGrad Stepsizes: Full Adaptivity with High Probability to Unknown Parameters, Unbounded Gradients and Affine Variance.](http://arxiv.org/abs/2302.08783) | 本文提供了针对 AdaGrad 步幅下的SGD算法的更加全面且无限制性的分析，支持多种模型，可以在高概率下处理未知参数和无界梯度。 |
| [^152] | [A Cloud-based Deep Learning Framework for Early Detection of Pushing at Crowded Event Entrances.](http://arxiv.org/abs/2302.08237) | 本文提出了一种基于云计算和深度学习的自动早期推测测深框架，可用于拥挤事件入口的推挤识别，包括修改和训练EfficientNetV2B0卷积神经网络模型、准确快速的深度光流模型及彩色环方法的整合以实时分析视频流并识别推挤区域，并使用现场采集技术和云计算环境实时收集人群视频流并提供早期结果。 |
| [^153] | [Photonic reservoir computing enabled by stimulated Brillouin scattering.](http://arxiv.org/abs/2302.07698) | 本文报道了一种基于刺激布里渊散射的光子水库计算平台，能够利用光学技术的优势，实现快速、低功率和更大带宽的实时人工智能。 |
| [^154] | [Towards Optimal Compression: Joint Pruning and Quantization.](http://arxiv.org/abs/2302.07612) | 本论文介绍了一种名为FITCompress的方法，它可以联合使用剪枝和混合精度量化来对预训练模型进行优化选择，以实现最优压缩。该方法基于Fisher信息度量和压缩空间中的路径规划，可在保持模型原始性能的情况下实现最新颖的压缩。 |
| [^155] | [Adversarial Rewards in Universal Learning for Contextual Bandits.](http://arxiv.org/abs/2302.07186) | 本文研究了在上下文Bandits中学习的基本极限，给出了关于可学习的上下文过程和通用一致性算法的特性，并讨论了对抗奖励下的乐观通用一致性学习的不可能性。 |
| [^156] | [Horospherical Decision Boundaries for Large Margin Classification in Hyperbolic Space.](http://arxiv.org/abs/2302.06807) | 本文提出了一种大间隔分类器，它使用浑拟圆决策边界可以优化测地凸优化问题，实验结果表明其竞争性能优越。 |
| [^157] | [Probabilistic Circuits That Know What They Don't Know.](http://arxiv.org/abs/2302.06544) | 本文指出概率电路（PC）对超出分布（OOD）数据不具备鲁棒性；通过模型不确定性量化，我们提出了可处理的随机失活推断（TDI）来克服这一挑战，并且这种方法可以在单个正向传递中提供可处理的无采样不确定性估计，从而改善了PC对分布漂移和OOD数据的鲁棒性。 |
| [^158] | [The Certification Paradox: Certifications Admit Better Attacks.](http://arxiv.org/abs/2302.04379) | 本文指出了一个"认证悖论"，认证虽然可以展示模型的稳健性，但额外揭示了有关认证模型的信息也成为新的攻击面，导致更好的攻击效果。 |
| [^159] | [Online Resource Allocation: Bandits feedback and Advice on Time-varying Demands.](http://arxiv.org/abs/2302.04182) | 本论文研究了带有赌率反馈和时间变化需求的在线资源分配问题，提出了一种稳健的在线算法来处理总需求量的在线预测。 |
| [^160] | [Attacking Cooperative Multi-Agent Reinforcement Learning by Adversarial Minority Influence.](http://arxiv.org/abs/2302.03322) | 本研究提出了 Adversarial Minority Influence (AMI) 黑盒攻击，能够在考虑多智能体互动和合作目标下实现有针对性的最坏情况合作，攻击成功率比现有方法高出 2.2倍。 |
| [^161] | [Domain-Indexing Variational Bayes: Interpretable Domain Index for Domain Adaptation.](http://arxiv.org/abs/2302.02561) | 该论文介绍了一种新的域自适应方法，该方法利用了从多域数据中生成的域索引，提供额外的洞察视角，并在各种任务中实现了最佳性能。 |
| [^162] | [Towards Practical Preferential Bayesian Optimization with Skew Gaussian Processes.](http://arxiv.org/abs/2302.01513) | 该论文研究了偏好贝叶斯优化中的偏好高斯过程模型，提出了改进的基于马尔科夫链蒙特卡洛的估计方法，可以解决偏斜性问题并提高实用性。 |
| [^163] | [Randomized Gaussian Process Upper Confidence Bound with Tighter Bayesian Regret Bounds.](http://arxiv.org/abs/2302.01511) | 该研究提出了一种改进的随机高斯过程上置信界限(IRGP-UCB)，它使用双参数指数分布,取得了更紧密的贝叶斯遗憾界限，并避免了后期过度探索。 |
| [^164] | [Efficient Graph Field Integrators Meet Point Clouds.](http://arxiv.org/abs/2302.00942) | 本文提出了两种算法用于非欧几里得空间中的高效图场积分，适用于点云网格图或ε-最近邻图的表征方法，具有很强的实用性。 |
| [^165] | [A Comprehensive Survey of Continual Learning: Theory, Method and Application.](http://arxiv.org/abs/2302.00487) | 本文为持续学习的全面综述，总结了持续学习的一般目标，回顾了各种持续学习方法，并强调了一些有前途的应用领域和未来研究方向。 |
| [^166] | [Retiring $\Delta$DP: New Distribution-Level Metrics for Demographic Parity.](http://arxiv.org/abs/2301.13443) | 本文提出两种新公平度量方法：概率密度函数曲线之间区域（ABPC）和累积密度函数曲线之间区域（ABCC），实现人口统计特征的公平机器学习 |
| [^167] | [Minimal Value-Equivalent Partial Models for Scalable and Robust Planning in Lifelong Reinforcement Learning.](http://arxiv.org/abs/2301.10119) | 本文提出了一种仅模拟环境中相关方面的“最小价值等价部分模型”，并证明了这些模型用于规划在生涯强化学习场景中具有可扩展性优势。 |
| [^168] | [A Linear Reconstruction Approach for Attribute Inference Attacks against Synthetic Data.](http://arxiv.org/abs/2301.10053) | 本研究提出一种新的属性推断攻击方法，针对合成数据中所有记录，具有高度的准确率和危害性。 |
| [^169] | [Revisiting Signed Propagation for Graph Neural Networks.](http://arxiv.org/abs/2301.08918) | 该论文重新审视了在异质图中的有符号传播方法，提出了一种适用于多类别图的新策略，并克服了其带来的不确定性和不一致性问题，取得了显著的性能提升。 |
| [^170] | [Physics-separating artificial neural networks for predicting sputtering and thin film deposition of AlN in Ar/N$_2$ discharges on experimental timescales.](http://arxiv.org/abs/2301.03524) | 本文开发了一种物理分离的人工神经网络模型，用于预测Ar/N$_2$放电中AlN溅射和薄膜沉积。该模型通过混合反应分子动力学和Monte Carlo模拟实现了填充参数空间的有效方法。 |
| [^171] | [Scalable Communication for Multi-Agent Reinforcement Learning via Transformer-Based Email Mechanism.](http://arxiv.org/abs/2301.01919) | 本文提出了一种基于Transformer的邮件机制（TEM），解决了多智能体强化学习中的可扩展性问题，它采用本地通信和消息链转发的方式进行通信，而不需要模拟所有智能体。 |
| [^172] | [Defense Against Adversarial Attacks on Audio DeepFake Detection.](http://arxiv.org/abs/2212.14597) | 本文旨在探讨音频DeepFake检测中对抗性攻击的问题，并通过对三种检测体系结构的鲁棒性评估以及采用自适应训练进行对抗训练来提高检测器的性能。其中，我们还首次适应了RawNet3体系结构用于DeepFake检测。 |
| [^173] | [Forecasting through deep learning and modal decomposition in two-phase concentric jets.](http://arxiv.org/abs/2212.12731) | 本论文提出了一种将机器学习和单相流数值模拟相结合的方法来实时预测和改善两相流的燃油/空气混合物，从而提高涡扇发动机燃油室喷射器的性能。 |
| [^174] | [Automated Search for Conjectures on Mathematical Constants using Analysis of Integer Sequences.](http://arxiv.org/abs/2212.09470) | 本文提出了一种新的方法，通过分析整数序列来搜索数学常数的公式，推导出了多个常数的新公式，包括 $\pi$ 和 Catalan 常数。 |
| [^175] | [Lorentz group equivariant autoencoders.](http://arxiv.org/abs/2212.07347) | 本论文提出了一种洛伦兹群自编码器（LGAE）模型，它具有上述群的等变性，且在高能物理数据的压缩、重构和异常检测等任务方面表现出色。 |
| [^176] | [AI Ethics on Blockchain: Topic Analysis on Twitter Data for Blockchain Security.](http://arxiv.org/abs/2212.06951) | 本研究分析了Twitter上与MEV相关的话题，结果表明推文讨论了深刻的伦理问题，包括安全、公平、情感情绪和对MEV解决方案的渴望。 |
| [^177] | [Explainable Performance: Measuring the Driving Forces of Predictive Performance.](http://arxiv.org/abs/2212.05866) | XPER方法能衡量输入特征对模型预测性能的具体贡献，并可用于处理异质性问题，构建同质化个体群体，从而提高预测精度。 |
| [^178] | [Reconstructing Training Data from Model Gradient, Provably.](http://arxiv.org/abs/2212.03714) | 通过单个梯度查询可重构训练数据，存在隐私泄露威胁。 |
| [^179] | [Federated Neural Topic Models.](http://arxiv.org/abs/2212.02269) | 该论文提出了一种基于神经主题建模实现的联邦神经主题模型，可以在不共享数据的情况下允许多个方共同训练主题模型和保护节点隐私。 |
| [^180] | [Linear Causal Disentanglement via Interventions.](http://arxiv.org/abs/2211.16467) | 本文探讨了线性潜在因果模型中的因果分离问题，指出对于识别性干预数据是必要的，而每个潜在变量的单一干预就足够了。 |
| [^181] | [Confidence-Aware Graph Neural Networks for Learning Reliability Assessment Commitments.](http://arxiv.org/abs/2211.15755) | 本论文针对可靠性评估承诺的优化问题，利用基于 GNN 的结构预测发电机的承诺和约束，并通过对高置信度预测结果的可行性修复，提高了最先进算法的解决方案质量。 |
| [^182] | [Mean Shift Mask Transformer for Unseen Object Instance Segmentation.](http://arxiv.org/abs/2211.11679) | 本文提出了一种新的均值漂移掩模变换器，用于联合训练和推断特征提取器和聚类器，可用于未知物体实例分割，在COCO数据集上表现出竞争性的性能，并在罕见和未知物体类别上具有显着优势。 |
| [^183] | [Frozen Overparameterization: A Double Descent Perspective on Transfer Learning of Deep Neural Networks.](http://arxiv.org/abs/2211.11074) | 本文研究了深度神经网络（DNN）转移学习的泛化行为。我们发现，目标数据集大时，测试误差演变会有更明显的双峰效应。更大的源数据集可能会导致较慢的目标DNN训练。此外，我们证明了冻结层数量可以确定转移学习是否有效地低于... |
| [^184] | [Contextual Bandits with Packing and Covering Constraints: A Modular Lagrangian Approach via Regression.](http://arxiv.org/abs/2211.07484) | 该论文研究了一种带有资源线性约束的上下文幸存者问题的变种，提出了一种新的算法，该算法简单、计算效率高，同时能够实现较低的后悔。此外，当某些约束被违反时，算法在统计上是最优的。 |
| [^185] | [Empirical Risk Minimization with Relative Entropy Regularization.](http://arxiv.org/abs/2211.06617) | 本文研究了相对熵正则化的经验风险最小化问题，其中参考度量为sigma有限测度，解为唯一的概率测度并展现了几乎正确的保证。ERM-RER问题的解被称为Gibbs算法。 |
| [^186] | [Physics-separating artificial neural networks for predicting initial stages of Al sputtering and thin film deposition in Ar plasma discharges.](http://arxiv.org/abs/2211.04796) | 通过引入物理分离的人工神经网络，共同描述了Al的溅射和薄膜生长的不断发展的表面状态和缺陷结构，全面描述了基本过程。 |
| [^187] | [Entropic Neural Optimal Transport via Diffusion Processes.](http://arxiv.org/abs/2211.01156) | 这项研究提出了一种新的神经算法，用于计算连续概率分布之间的熵最优传输(EOT)计划，它具有端到端学习、快速推理和处理小值熵正则化系数的优点，可在大规模EOT任务中发挥出色的性能。 |
| [^188] | [Generating Multilingual Gender-Ambiguous Text-to-Speech Voices.](http://arxiv.org/abs/2211.00375) | 该论文介绍了一种生成多语言的性别不明确的TTS声音的方法，通过提出的性别感知方法从潜在说话人中有效地进行采样，成功生成了一系列新的、多样化的、一致性和性别不明确性更强的声音，具有很强的实验表现。 |
| [^189] | [MARS: Meta-Learning as Score Matching in the Function Space.](http://arxiv.org/abs/2210.13319) | 本文提出了一种新的元学习方法，通过在函数空间中执行推理，从而避免了指定高维神经网络参数的先验分布族时的限制，可以无缝获取和表示复杂的先验知识。 |
| [^190] | [NeuroPrim: An Attention-based Model for Solving NP-hard Spanning Tree Problems.](http://arxiv.org/abs/2210.12453) | 本文提出了一种名为NeuroPrim的基于注意力机制的模型来解决生成树问题，采用Prim算法减少了动作和状态空间，并使用REINFORCE训练了结果模型，应用于欧几里得空间中的三个困难问题。 |
| [^191] | [Deep Multi-Branch CNN Architecture for Early Alzheimer's Detection from Brain MRIs.](http://arxiv.org/abs/2210.12331) | 本论文提出了一种基于脑MRIs的早期阿尔茨海默病检测方法，使用深度多分支卷积神经网络结构并达到了99.05%的三分类准确率。 |
| [^192] | [Learning to Invert: Simple Adaptive Attacks for Gradient Inversion in Federated Learning.](http://arxiv.org/abs/2210.10880) | 该论文研究了联邦学习中的梯度反演问题，提出了一种简单自适应攻击方法，揭示了现有防御机制的不足之处。 |
| [^193] | [The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers.](http://arxiv.org/abs/2210.06313) | 本文研究了使用变压器模型的机器学习模型中激活图的稀疏现象，发现在不同层数的变压器配置和其他体系结构中都出现了稀疏现象。 |
| [^194] | [On the Effectiveness of Hybrid Pooling in Mixup-Based Graph Learning for Language Processing.](http://arxiv.org/abs/2210.03123) | 本文探讨了图池化算子对于Manifold-Mixup方法在图形学习中的影响，提出了一种新颖的混合池化架构，相比现有方法在文本分类任务上表现显著优越并达到了最先进性能水平。 |
| [^195] | [Probabilistic partition of unity networks for high-dimensional regression problems.](http://arxiv.org/abs/2210.02694) | 本论文介绍了一种概率统一分治网络（PPOU-Net）模型应用在高维回归问题上的自适应降维通用框架，该模型通过低维流形上的多项式逼近目标函数，并采用期望极大算法进行训练，实验证明在各种数据维度情况下，PPOU-Net表现出更好的性能。 |
| [^196] | [Meta-Learning Priors for Safe Bayesian Optimization.](http://arxiv.org/abs/2210.00762) | 本文提出了一种数据驱动方法，通过元学习先验从离线数据中实现安全的贝叶斯优化，同时开发一种新的框架以数据驱动的方式选择符合安全要求的先验，结果表明，相比于基准方法，元学习先验加快了安全贝叶斯优化的收敛速度并改进了整体性能。 |
| [^197] | [Holographic-(V)AE: an end-to-end SO(3)-Equivariant (Variational) Autoencoder in Fourier Space.](http://arxiv.org/abs/2209.15567) | Holographic-(V)AE是一种在傅立叶空间中的SO(3)-等变性(Variational)自编码器，用于无监督学习和生成分布于指定3D原点周围的数据，并且对应的潜空间编码了球面图像的分类特征。 |
| [^198] | [Generating Formal Safety Assurances for High-Dimensional Reachability.](http://arxiv.org/abs/2209.12336) | 提出了一个称为Safe DeepReach的框架，用于为DeepReach方法生成正式安全保障。该框架将新颖的Lipschitz连续性分析与区间边界传播方法相结合，以有效和可伸缩的方式保证解决方案的安全性。 |
| [^199] | [First-order Policy Optimization for Robust Markov Decision Process.](http://arxiv.org/abs/2209.10579) | 本论文介绍了一种用于解决鲁棒性马尔科夫决策过程的一阶方法，称为鲁棒策略镜像下降算法（RPMD）。通过使用线性递增步进，算法具有较低的复杂度，并且能够在不确定情况下找到最优策略。 |
| [^200] | [Discrete Key-Value Bottleneck.](http://arxiv.org/abs/2207.11240) | 本文提出了一个新的神经网络模型结构，包含离散瓶颈，可以有效处理在多个任务之间进行连续学习的问题。 |
| [^201] | [Model-Aware Contrastive Learning: Towards Escaping the Dilemmas.](http://arxiv.org/abs/2207.07874) | 本文提出了一种模型感知的对比学习（MACL）策略，其温度适应于反映实例判别任务基本置信度的对齐幅度的大小，解决了对比学习中的统一性容忍困境（UTD）和梯度降低问题，并在实验证明其优于所有基线，并实现最先进的结果。 |
| [^202] | [Stochastic Gradient Descent and Anomaly of Variance-flatness Relation in Artificial Neural Networks.](http://arxiv.org/abs/2207.04932) | 本研究通过动态分解方法分析了 SGD 在固定点附近的属性，恢复了真正的“能量”函数，解决了权重方差和损失函数平坦度反常关系的悖论，为人工智能学科提供更好的算法。 |
| [^203] | [Improving Visual Grounding by Encouraging Consistent Gradient-based Explanations.](http://arxiv.org/abs/2206.15462) | 该论文提出了一种名为 AMC 的目标函数，鼓励基于梯度的解释覆盖有注释的感兴趣区域，即编码区域。该方法在提高视觉 grounding 性能方面表现卓越，有望成为视觉 grounding 领域的新进展。 |
| [^204] | [Generative Pretraining for Black-Box Optimization.](http://arxiv.org/abs/2206.10786) | 该论文提出了一种使用离线数据预训练黑盒优化器的生成框架BONET，使用自回归模型和样本策略合成轨迹以帮助在高维空间中优化昂贵的黑盒函数。 |
| [^205] | [On the fast convergence of minibatch heavy ball momentum.](http://arxiv.org/abs/2206.07553) | 本文研究了一种随机Kaczmarz算法，使用小批量和重球动量进行加速，在二次优化问题中保持快速收敛率。 |
| [^206] | [Differentiable and Transportable Structure Learning.](http://arxiv.org/abs/2206.06354) | D-Struct是一种可微和可传输的结构学习方法，通过新颖的架构和损失函数使得结构可以在同一领域的不同数据集中传输，比NOTEARS和其他最先进的方法具有更好的性能。 |
| [^207] | [Federated Offline Reinforcement Learning.](http://arxiv.org/abs/2206.05581) | 本文提出了一种联邦离线强化学习算法，可以处理医疗机构间数据共享的隐私限制和异质性问题，同时提供了通信效率和隐私保护性。该算法的样本复杂度证明以及在现实医学数据集上的模拟实验结果表明了其有效性和效率。 |
| [^208] | [Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models.](http://arxiv.org/abs/2206.04615) | 本研究引入了Beyond the Imitation Game基准测试（BIG-bench），该测试集包含了204个各领域的难题，旨在评估当前语言模型的能力并为未来的研究提供信息和准备。 |
| [^209] | [Asymptotic Instance-Optimal Algorithms for Interactive Decision Making.](http://arxiv.org/abs/2206.02326) | 本论文提出了适用于特定决策问题的渐近最优算法，该算法能够根据该问题的复杂度，在遗憾最小的前提下最大程度地收集观察结果。 |
| [^210] | [Cross-View Language Modeling: Towards Unified Cross-Lingual Cross-Modal Pre-training.](http://arxiv.org/abs/2206.00621) | 本文提出了跨视图语言建模框架，该框架将跨语言和跨模态预训练统一在共享的架构和目标下进行，通过有条件的掩码语言建模和对比学习来最大化不同视图之间的互信息以实现两个视图的对齐。 |
| [^211] | [Level Up with RealAEs: Leveraging Domain Constraints in Feature Space to Strengthen Robustness of Android Malware Detection.](http://arxiv.org/abs/2205.15128) | 本文提出了一种在特征空间中生成RealAEs的解决方案，该方法通过解释Android领域约束为在特征空间中的边界来实现。实验结果表明，这种方法提高了检测模型的鲁棒性。 |
| [^212] | [Precise Learning Curves and Higher-Order Scaling Limits for Dot Product Kernel Regression.](http://arxiv.org/abs/2205.14846) | 本文细致研究了点积核岭回归问题，针对 $m\propto d^r$ 高阶标度关系提出了精确的测试误差、偏差和方差公式。 |
| [^213] | [QUIC-FL: Quick Unbiased Compression for Federated Learning.](http://arxiv.org/abs/2205.13341) | 本文提出QUIC-FL方法，即面向联邦学习的快速无偏压缩，通过改进DME技术实现了最优归一化均方误差保证。 |
| [^214] | [Explainable AI via Learning to Optimize.](http://arxiv.org/abs/2204.14174) | 本文提供了一种在数据驱动模型下用于可解释人工智能的具体工具-L2O方法，通过解决优化问题来实现每个推断，并提出了使用可解释的证书来验证模型推断是否可信。 |
| [^215] | [Semantic Information Recovery in Wireless Networks.](http://arxiv.org/abs/2204.13366) | 本文提出了一个基于机器学习的语义通信系统SINFONY，它通过对消息进行数据减少和可靠传输来最好地保留语义，从而实现无线网络中的语义信息恢复。 |
| [^216] | [Plasticity Neural Network Based on Astrocytic effects at Critical Period, Synaptic Competition and Strength Rebalance by Current and Mnemonic Brain Plasticity and Synapse Formation.](http://arxiv.org/abs/2203.11740) | 该论文提出基于星形细胞作用的神经网络，通过突触的竞争和强度平衡实现现有和记忆性的大脑可塑性和突触形成，并探讨了与关键期相关的神经紊乱和负面和正面记忆的持久性对突触激活的影响。 |
| [^217] | [Universal Regression with Adversarial Responses.](http://arxiv.org/abs/2203.05067) | 本文提出了一种通用回归算法，可针对大类非独立同分布实例序列的对抗性回应，在通用可分离指标空间上实现强一致性的通用一致性学习。 |
| [^218] | [Autoregressive based Drift Detection Method.](http://arxiv.org/abs/2203.04769) | 本研究提出一种基于自回归模型的漂移检测方法，适用于各种机器学习算法，能够在合成数据和现实数据方面表现优于现有方法。 |
| [^219] | [DDL-MVS: Depth Discontinuity Learning for MVS Networks.](http://arxiv.org/abs/2203.01391) | 该论文提出了一种深度不连续性学习的MVS方法，可以在保留重建完整性的同时提高精度，方法是联合估计深度和边界地图，其中边界地图用于进一步细化深度地图。 |
| [^220] | [On Testability and Goodness of Fit Tests in Missing Data Models.](http://arxiv.org/abs/2203.00132) | 本文提供了关于缺失数据图模型的可检验性和设计拟合优度测试的新见解。 |
| [^221] | [Finite-Sum Coupled Compositional Stochastic Optimization: Theory and Applications.](http://arxiv.org/abs/2202.12396) | 本文提出了一种适用于凸和非凸组合函数求和的随机算法，并在理论上证明了该算法在Oracle复杂度和并行加速方面的优秀性能。 |
| [^222] | [A policy gradient approach for optimization of smooth risk measures.](http://arxiv.org/abs/2202.11046) | 本文介绍了一种应用于on-policy和off-policy RL情况下的策略梯度算法，用于最小化广义平滑风险度量，能够收敛到平滑风险度量的稳态点，并适用于均值-方差和畸变风险度量的优化。 |
| [^223] | [Accelerating Primal-dual Methods for Regularized Markov Decision Processes.](http://arxiv.org/abs/2202.10506) | 本文介绍了一种新的正-对偶表述方法，结合新的插值度量，可以显著加速收敛。数值结果表明方法在多种设置下性能优越。 |
| [^224] | [GoSafeOpt: Scalable Safe Exploration for Global Optimization of Dynamical Systems.](http://arxiv.org/abs/2201.09562) | GoSafeOpt是第一个能够安全探索高维系统并提供全局最优保证的算法。 |
| [^225] | [Model-Based Safe Reinforcement Learning with Time-Varying State and Control Constraints: An Application to Intelligent Vehicles.](http://arxiv.org/abs/2112.11217) | 本文提出了一种安全RL算法，结合了基于屏障力的控制策略结构与多步策略评估机制，在保证控制安全的同时，能够应对时间变化的安全约束，并证明了其稳定性、鲁棒性和收敛性，优于几种最先进的RL算法。 |
| [^226] | [Fair Active Learning: Solving the Labeling Problem in Insurance.](http://arxiv.org/abs/2112.09466) | 本文旨在解决保险行业中普遍存在的机器学习模型在数据中发现的偏见和歧视，提出了公平主动学习方法，能够在实现模型预测性能的同时保证数据公平性。 |
| [^227] | [Efficient Multi-objective Neural Architecture Search Framework via Policy Gradient Algorithm.](http://arxiv.org/abs/2111.03892) | 本论文提出了TND-NAS框架，利用可微架构搜索框架和多目标NAS的兼容性，通过策略梯度算法实现高效多目标神经架构搜索。实验结果表明其优于现有方法。 |
| [^228] | [Extracting Dynamical Models from Data.](http://arxiv.org/abs/2110.06917) | 本文介绍了一种使用机器学习从数据中获得确切动力学信息的方法，并命名为“FJet”。这种方法在谐振子、摆和达芬奇振荡器等示例中被证明可以准确地复制动力学并恢复潜在的微分方程。 |
| [^229] | [EvadeDroid: A Practical Evasion Attack on Machine Learning for Black-box Android Malware Detection.](http://arxiv.org/abs/2110.03301) | 本论文提出了一种可实际使用并可以有效逃避黑盒Android恶意软件检测器的对抗攻击——EvadeDroid，并且可以保留原始恶意软件应用程序的功能。 |
| [^230] | [SubseasonalClimateUSA: A Dataset for Subseasonal Forecasting and Benchmarking.](http://arxiv.org/abs/2109.10399) | 这个论文介绍了SubseasonalClimateUSA，这是一个用于训练和基准测试美国的亚季节预测模型的数据集。作者使用该数据集对多种模型进行了基准测试。 |
| [^231] | [An efficient plasma-surface interaction surrogate model for sputtering processes based on autoencoder neural networks.](http://arxiv.org/abs/2109.01406) | 本研究提出了一种基于自编码神经网络的高效等离子体表面相互作用代理模型，用于薄膜溅射沉积模拟。通过训练卷积$\beta$变分自编码器，可以减少高维能量角分布，从而更好地考虑变量而不是单个固定的Ti-Al化学计量。 |
| [^232] | [A data-driven approach to beating SAA out-of-sample.](http://arxiv.org/abs/2105.12342) | 本文提出了一类分布乐观优化（DOO）模型，在外推问题上始终能够超越样本平均逼近（SAA）；然而，乐观解的鲁棒性较差且更容易受到模型错误的影响。 |
| [^233] | [Online Algorithms and Policies Using Adaptive and Machine Learning Approaches.](http://arxiv.org/abs/2105.06577) | 本文在动态系统中提出了一种AC-RL控制器，在外环中采用强化学习策略确保稳定性和最优性，在内环中采用自适应控制。该控制器适用于两类非线性动态系统，并能适应参数不确定性和输入幅值限制。 |
| [^234] | [Weak Signal Asymptotics for Sequentially Randomized Experiments.](http://arxiv.org/abs/2101.09855) | 本文使用弱信号渐近行为的方法研究了一类顺序随机实验，认为这类顺序实验的样本路径会弱收敛到扩散极限，并能获得关于几种顺序实验的后悔和信念演变的多个见解。 |
| [^235] | [A Doubly Stochastic Simulator with Applications in Arrivals Modeling and Simulation.](http://arxiv.org/abs/2012.13940) | 该论文提出了一种双重随机模拟器，结合了经典的蒙特卡罗模拟器和神经网络模拟器，用于建模、估计和模拟具有一般非平稳和多维随机到达速率的到达过程，并在高速公路交通和航空交通建模和仿真中得到应用。 |
| [^236] | [Integrating Distributed Architectures in Highly Modular RL Libraries.](http://arxiv.org/abs/2007.02622) | 该研究探讨了在本地和分布式执行级别上实现代理组合性的设计选择，通过独立的可重用组件允许在不同尺度上定义RL代理，并成功解决了新颖和复杂的环境，具有最先进的性能。 |
| [^237] | [Robustness to Transformations Across Categories: Is Robustness To Transformations Driven by Invariant Neural Representations?.](http://arxiv.org/abs/2007.00112) | 本文通过观察不同种类的图像转换后深度卷积神经网络（DCNNs）的表现，探讨了不变神经表示是否促进了跨类别的图像鲁棒性。实验表明，跨类别变换的鲁棒性是由不变表示所促进的，支持鲁棒性是由不变性驱动而不是专门的子网络的假设。 |
| [^238] | [Improving a State-of-the-Art Heuristic for the Minimum Latency Problem with Data Mining.](http://arxiv.org/abs/1908.10705) | 这篇论文通过利用数据挖掘技术，改进了一种基于GRASP的最小延迟问题启发式算法，取得了较好的效果，匹配或优于解的质量，在大大缩短计算时间的同时，还成功地引入了88个新的解成本值。 |
| [^239] | [Machine learning plasma-surface interface for coupling sputtering and gas-phase transport simulations.](http://arxiv.org/abs/1810.04510) | 该论文提出了一种新的机器学习方法，利用来自直接模拟表面的数据，在运行时为模型提供缺失的表面信息，这将有效促进等离子体处理领域的开展。 |

# 详细

[^1]: Aria数字孪生：一种新的基准数据集用于自我中心的3D机器感知。

    Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine Perception. (arXiv:2306.06362v1 [cs.CV])

    [http://arxiv.org/abs/2306.06362](http://arxiv.org/abs/2306.06362)

    Aria数字孪生是一个自我中心数据集，具有其它任何数据集都没有的高精度、照片逼真和详尽的真实信息。这个数据集将成为自我中心机器感知评估的新标准。

    

    我们推出了Aria数字孪生（ADT）-一个使用Aria眼镜捕获的自我中心数据集，具有广泛的对象，环境和人类级别的真实数据。该ADT数据集包括200个由穿戴Aria设备的人在两个室内真实场景中执行的真实世界活动序列，包含398个对象实例（324个静态和74个动态）。每个序列包括：a）两个单色相机流，一个RGB相机流，两个IMU流的原始数据；b）完整的传感器校准；c）真实数据，包括Aria设备的连续6自由度（6DoF）姿态，对象6DoF姿态，3D注视矢量，3D人体姿态，2D图像分割，图像深度图；d）照片般真实的合成渲染图像。据我们所知，目前没有现有自我中心数据集能够与ADT的准确性、逼真度和全面性相媲美。通过向研究社区贡献ADT，我们的使命是为自我中心机器感知的评估设立新的标准。

    We introduce the Aria Digital Twin (ADT) - an egocentric dataset captured using Aria glasses with extensive object, environment, and human level ground truth. This ADT release contains 200 sequences of real-world activities conducted by Aria wearers in two real indoor scenes with 398 object instances (324 stationary and 74 dynamic). Each sequence consists of: a) raw data of two monochrome camera streams, one RGB camera stream, two IMU streams; b) complete sensor calibration; c) ground truth data including continuous 6-degree-of-freedom (6DoF) poses of the Aria devices, object 6DoF poses, 3D eye gaze vectors, 3D human poses, 2D image segmentations, image depth maps; and d) photo-realistic synthetic renderings. To the best of our knowledge, there is no existing egocentric dataset with a level of accuracy, photo-realism and comprehensiveness comparable to ADT. By contributing ADT to the research community, our mission is to set a new standard for evaluation in the egocentric machine perce
    
[^2]: 基于运动的结构的三维重建技术

    3D reconstruction using Structure for Motion. (arXiv:2306.06360v1 [cs.CV])

    [http://arxiv.org/abs/2306.06360](http://arxiv.org/abs/2306.06360)

    该论文介绍了一种基于运动的结构的三维重建技术，利用一对HDR照相机和室内移动地面机器人进行捕捉和算法推算，实现室内空间的深度图可视化。

    

    本文旨在利用一对HDR照相机进行室内空间的三维重建，该照相机以立体视觉配置安装在室内移动地面机器人上，捕捉各种纹理和空间特征并将其作为2D图像传给我们的算法，进而实现深度图的可视化。

    We are working towards 3D reconstruction of indoor spaces using a pair of HDR cameras in a stereo vision configuration mounted on an indoor mobile floor robot that captures various textures and spatial features as 2D images and this data is simultaneously utilized as a feed to our algorithm which will allow us to visualize the depth map.
    
[^3]: 语言指导下的场景级交通仿真模拟

    Language-Guided Traffic Simulation via Scene-Level Diffusion. (arXiv:2306.06344v1 [cs.RO])

    [http://arxiv.org/abs/2306.06344](http://arxiv.org/abs/2306.06344)

    该论文提出了一种可以受到语言指导的场景级条件扩散模型，该模型能够生成真实且可控的交通，并通过大型语言模型将用户的查询转化为损失函数，指导模型生成符合查询条件的仿真交通。

    

    实现真实和可控的交通仿真是加速自主驾驶汽车（AV）发展的核心能力。然而，目前用于控制基于学习的交通模型的方法需要大量领域专业知识，对于从业者来说很难使用。为了解决这个问题，我们提出了CTG++，一种可以受到语言指导的场景级条件扩散模型。为了达到这个目的，我们需要解决两个问题：需要一个真实和可控的交通模型骨干结构，并且要有一种有效的方法来使用语言与交通模型进行交互。为了解决这些问题，我们首先提出了一个带有时空转换器骨干结构的场景级扩散模型，它生成了真实和可控的交通。然后，我们利用大型语言模型（LLM）将用户的查询转换为损失函数，指导扩散模型朝着查询合规的生成方向前进。通过全面的评估，我们展示了该模型的有效性。

    Realistic and controllable traffic simulation is a core capability that is necessary to accelerate autonomous vehicle (AV) development. However, current approaches for controlling learning-based traffic models require significant domain expertise and are difficult for practitioners to use. To remedy this, we present CTG++, a scene-level conditional diffusion model that can be guided by language instructions. Developing this requires tackling two challenges: the need for a realistic and controllable traffic model backbone, and an effective method to interface with a traffic model using language. To address these challenges, we first propose a scene-level diffusion model equipped with a spatio-temporal transformer backbone, which generates realistic and controllable traffic. We then harness a large language model (LLM) to convert a user's query into a loss function, guiding the diffusion model towards query-compliant generation. Through comprehensive evaluation, we demonstrate the effect
    
[^4]: ECGBERT: 自监督表示学习揭示ECG的隐藏语言

    ECGBERT: Understanding Hidden Language of ECGs with Self-Supervised Representation Learning. (arXiv:2306.06340v1 [eess.SP])

    [http://arxiv.org/abs/2306.06340](http://arxiv.org/abs/2306.06340)

    ECGBERT是一种自监督表示学习方法，可对ECG的潜在语言进行解锁，减轻了医学数据缺乏良好标注和精选数据的挑战。在多个基于ECG的任务中，ECGBERT表现出了实现最先进结果的潜力。

    

    在医疗领域，目前的心电图(ECG)信号分析方法依赖于针对特定任务进行训练的监督深度神经网络，这些任务需要大量标记数据。然而，本文介绍了ECGBERT，一种自监督表示学习方法，可解锁ECG的潜在语言。通过对模型进行无监督的预训练，我们缓解了医学数据缺乏良好标注和精选数据的挑战。ECGBERT受到自然语言处理和大型语言模型领域的进展的启发，可以通过最小的额外层进行微调，应用于各种基于ECG的问题。通过四项任务，包括房颤心律失常检测、心跳分类、睡眠呼吸暂停检测和用户认证，我们演示了ECGBERT在各种任务上实现最先进结果的潜力。

    In the medical field, current ECG signal analysis approaches rely on supervised deep neural networks trained for specific tasks that require substantial amounts of labeled data. However, our paper introduces ECGBERT, a self-supervised representation learning approach that unlocks the underlying language of ECGs. By unsupervised pre-training of the model, we mitigate challenges posed by the lack of well-labeled and curated medical data. ECGBERT, inspired by advances in the area of natural language processing and large language models, can be fine-tuned with minimal additional layers for various ECG-based problems. Through four tasks, including Atrial Fibrillation arrhythmia detection, heartbeat classification, sleep apnea detection, and user authentication, we demonstrate ECGBERT's potential to achieve state-of-the-art results on a wide variety of tasks.
    
[^5]: 基于机器学习的分类数据集缺失值插补研究

    Machine Learning Based Missing Values Imputation in Categorical Datasets. (arXiv:2306.06338v1 [cs.LG])

    [http://arxiv.org/abs/2306.06338](http://arxiv.org/abs/2306.06338)

    本文研究了使用机器学习算法预测和插补分类数据集中的缺失值，使用ECOC框架的集成模型相比于单个模型效果更好，但使用深度学习存在挑战和局限性。

    

    本研究探讨了使用机器学习算法在分类数据集中预测和插补缺失值的方法。我们着重研究了使用误差纠正输出码(ECOC)框架的集成模型，包括基于SVM和KNN的集成模型，以及结合了SVM、KNN和MLP模型的集成分类器。我们将这些算法应用于三个数据集: CPU数据集、甲状腺功能减退数据集和乳腺癌数据集。实验结果表明，机器学习算法能够在预测和插补缺失值方面取得良好的性能，具体结果因数据集和缺失值模式而异。采用误差纠正输出码(ECOC)框架的集成模型相对于单个模型在提高预测准确性和鲁棒性方面特别有效。然而，使用深度学习进行缺失值插补也存在挑战和局限性。

    This study explored the use of machine learning algorithms for predicting and imputing missing values in categorical datasets. We focused on ensemble models that use the error correction output codes (ECOC) framework, including SVM-based and KNN-based ensemble models, as well as an ensemble classifier that combines SVM, KNN, and MLP models. We applied these algorithms to three datasets: the CPU dataset, the hypothyroid dataset, and the Breast Cancer dataset. Our experiments showed that the machine learning algorithms were able to achieve good performance in predicting and imputing the missing values, with some variations depending on the specific dataset and missing value pattern. The ensemble models using the error correction output codes (ECOC) framework were particularly effective in improving the accuracy and robustness of the predictions, compared to individual models. However, there are also challenges and limitations to using deep learning for missing value imputation, including
    
[^6]: 如何从短短三分钟的数据中学习和推广：受限于物理学和不确定性感知的神经随机微分方程

    How to Learn and Generalize From Three Minutes of Data: Physics-Constrained and Uncertainty-Aware Neural Stochastic Differential Equations. (arXiv:2306.06335v1 [cs.LG])

    [http://arxiv.org/abs/2306.06335](http://arxiv.org/abs/2306.06335)

    本文提出了一种使用神经随机微分方程学习控制动力学模型的方法，其中利用了物理知识作为归纳偏置，并在设计中利用距离感知的估计不确定性，在小数据集上进行训练，同时可以在长时间范围内进行准确预测，可用于模型预测控制和模型基础增强学习。

    

    我们提出了一个框架和算法，使用神经随机微分方程（SDEs）来学习受控动力学模型。我们构建漂移项，利用先验的物理知识作为归纳偏置，并设计扩散项来表示学习模型预测中的不确定性的距离感知估计——当在接近训练数据集状态的状态下评估时，它匹配系统的基本随机性，并且当在训练范围之外的状态下评估时，它会预测高度随机的动态。提出的神经SDEs可以快速评估，以用于模型预测控制算法，也可以用作模型基础增强学习的模拟器。此外，即使在覆盖状态空间有限区域的小数据集上进行训练，它们也可以在长时间范围内进行准确预测。我们通过控制小型无人机和模拟机器腿的运动（考虑到模型不确定性和仿真到现实的差异）以及模拟建筑的热力学动态（考虑到缺失数据并在小数据集上进行训练），展示了这些功能。

    We present a framework and algorithms to learn controlled dynamics models using neural stochastic differential equations (SDEs) -- SDEs whose drift and diffusion terms are both parametrized by neural networks. We construct the drift term to leverage a priori physics knowledge as inductive bias, and we design the diffusion term to represent a distance-aware estimate of the uncertainty in the learned model's predictions -- it matches the system's underlying stochasticity when evaluated on states near those from the training dataset, and it predicts highly stochastic dynamics when evaluated on states beyond the training regime. The proposed neural SDEs can be evaluated quickly enough for use in model predictive control algorithms, or they can be used as simulators for model-based reinforcement learning. Furthermore, they make accurate predictions over long time horizons, even when trained on small datasets that cover limited regions of the state space. We demonstrate these capabilities th
    
[^7]: 基于学习轮胎模型的三分钟数据自主漂移

    Autonomous Drifting with 3 Minutes of Data via Learned Tire Models. (arXiv:2306.06330v1 [eess.SY])

    [http://arxiv.org/abs/2306.06330](http://arxiv.org/abs/2306.06330)

    提出了一种基于神经常微分方程和神经-ExpTanh参数化的新型轮胎力模型，并将其作为现有非线性模型预测控制框架中解析刷式轮胎模型的替代品,成功实现了仅利用少于三分钟的驾驶数据，实现高性能的自主漂移

    

    在附着极限附近，轮胎所产生的力是非线性和错综复杂的。在这个区域内高效、准确的建模可以提高安全性，特别是在需要高力的紧急情况下。为此，我们提出了一种基于神经常微分方程和神经-ExpTanh参数化的新型轮胎力模型。这些模型旨在满足物理洞察力假设，同时具有足够的保真度，以直接从车辆状态测量中捕捉高阶效应。它们被用作现有非线性模型预测控制框架中解析刷式轮胎模型的替代品。通过使用Toyota Supra上的实验表明，少于三分钟的驾驶数据足以在各种轨迹上实现高性能的自主漂移，最高速度可达45mph。与基准模型的比较显示轨迹提高了4倍。

    Near the limits of adhesion, the forces generated by a tire are nonlinear and intricately coupled. Efficient and accurate modelling in this region could improve safety, especially in emergency situations where high forces are required. To this end, we propose a novel family of tire force models based on neural ordinary differential equations and a neural-ExpTanh parameterization. These models are designed to satisfy physically insightful assumptions while also having sufficient fidelity to capture higher-order effects directly from vehicle state measurements. They are used as drop-in replacements for an analytical brush tire model in an existing nonlinear model predictive control framework. Experiments with a customized Toyota Supra show that scarce amounts of driving data -- less than three minutes -- is sufficient to achieve high-performance autonomous drifting on various trajectories with speeds up to 45mph. Comparisons with the benchmark model show a $4 \times$ improvement in track
    
[^8]: HIPODE: 从策略解耦角度增强离线强化学习中高质量合成数据的应用

    HIPODE: Enhancing Offline Reinforcement Learning with High-Quality Synthetic Data from a Policy-Decoupled Approach. (arXiv:2306.06329v1 [cs.LG])

    [http://arxiv.org/abs/2306.06329](http://arxiv.org/abs/2306.06329)

    HIPODE是一种新的离线强化学习数据增强方法，通过选择接近数据集分布中潜在高价值状态的候选状态来生成高质量的合成数据，增强了下游ORL性能和适用性。

    

    离线强化学习（ORL）作为利用预先收集的静态数据训练强化学习模型的方法，已引起人们的关注。为了解决有限数据的问题并改善下游ORL的性能，最近的研究试图通过数据增强来扩展数据集的覆盖范围。然而，大多数方法都与特定策略（策略相关）相关，生成的数据只能保证支持当前下游ORL策略，限制了它在其他下游策略上的使用范围。此外，合成数据的质量通常不能很好地控制，这限制了进一步改进下游策略的潜力。为了解决这些问题，我们提出了一种新的ORL数据增强方法，称为高质量策略解耦（HIPODE）。一方面，HIPODE通过选择潜在高价值状态的候选状态中接近数据集分布的状态来产生高质量的合成数据，使用负

    Offline reinforcement learning (ORL) has gained attention as a means of training reinforcement learning models using pre-collected static data. To address the issue of limited data and improve downstream ORL performance, recent work has attempted to expand the dataset's coverage through data augmentation. However, most of these methods are tied to a specific policy (policy-dependent), where the generated data can only guarantee to support the current downstream ORL policy, limiting its usage scope on other downstream policies. Moreover, the quality of synthetic data is often not well-controlled, which limits the potential for further improving the downstream policy. To tackle these issues, we propose \textbf{HI}gh-quality \textbf{PO}licy-\textbf{DE}coupled~(HIPODE), a novel data augmentation method for ORL. On the one hand, HIPODE generates high-quality synthetic data by selecting states near the dataset distribution with potentially high value among candidate states using the negative
    
[^9]: 任意维度等变神经网络

    Any-dimensional equivariant neural networks. (arXiv:2306.06327v1 [cs.LG])

    [http://arxiv.org/abs/2306.06327](http://arxiv.org/abs/2306.06327)

    该论文提出了一个新的方法，利用代数拓扑中的表示稳定性，可以定义出一个可以以任意维度为输入的等变神经网络。这种方法使用方便，只需指定网络架构和等变性的组，且在任何训练过程中都可以使用。

    

    传统的监督学习旨在通过将函数拟合到一组具有固定维度的输入/输出对来学习未知映射。然后，在相同维度的输入上定义拟合函数。然而，在许多情况下，未知映射以任意维度的输入作为输入；例如，定义在任意大小的图形上的图形参数和定义在任意数量粒子上的物理量。我们利用代数拓扑中的新现象——表示稳定性，来定义等变神经网络，可以使用固定维度的数据进行训练，然后在任意维度上扩展接受输入。我们的方法易于使用，只需要网络架构和等变性的组，并且可以与任何训练过程结合使用。我们提供了我们方法的简单开源实现，并提供了初步的数值实验。

    Traditional supervised learning aims to learn an unknown mapping by fitting a function to a set of input-output pairs with a fixed dimension. The fitted function is then defined on inputs of the same dimension. However, in many settings, the unknown mapping takes inputs in any dimension; examples include graph parameters defined on graphs of any size and physics quantities defined on an arbitrary number of particles. We leverage a newly-discovered phenomenon in algebraic topology, called representation stability, to define equivariant neural networks that can be trained with data in a fixed dimension and then extended to accept inputs in any dimension. Our approach is user-friendly, requiring only the network architecture and the groups for equivariance, and can be combined with any training procedure. We provide a simple open-source implementation of our methods and offer preliminary numerical experiments.
    
[^10]: 通过反事实论证向医生解释机器学习决策

    Explaining a machine learning decision to physicians via counterfactuals. (arXiv:2306.06325v1 [cs.LG])

    [http://arxiv.org/abs/2306.06325](http://arxiv.org/abs/2306.06325)

    本文提出了一种基于变分自编码器的方法来生成在临床环境中有用的现实时间序列反事实论证(CFs)，以帮助医生解释机器学习模型的决策。

    

    机器学习模型在多个医疗保健任务上表现良好，可以帮助减轻医疗保健系统的负担。然而，缺乏可解释性是它们在医院中被采用的主要障碍。本文考虑用反事实论证(CFs)来解释机器学习模型的决策，即假设将结果翻转的情况。特别地，考虑时间序列的CFs，受到医生讨论和推理决策的启发: “如果患者的血压更低且下降，我会给他一个血管紧张素。” 讨论了在临床环境中特别有意义的CFs的关键属性: 生理合理性、任务相关性和稀疏扰动。过去CF生成方面的工作不能满足这些属性，尤其是在现实时间序列CFs方面。本文提出了一种基于变分自编码器(VAE)的方法来生成现实的时间序列CFs。

    Machine learning models perform well on several healthcare tasks and can help reduce the burden on the healthcare system. However, the lack of explainability is a major roadblock to their adoption in hospitals. \textit{How can the decision of an ML model be explained to a physician?} The explanations considered in this paper are counterfactuals (CFs), hypothetical scenarios that would have resulted in the opposite outcome. Specifically, time-series CFs are investigated, inspired by the way physicians converse and reason out decisions `I would have given the patient a vasopressor if their blood pressure was lower and falling'. Key properties of CFs that are particularly meaningful in clinical settings are outlined: physiological plausibility, relevance to the task and sparse perturbations. Past work on CF generation does not satisfy these properties, specifically plausibility in that realistic time-series CFs are not generated. A variational autoencoder (VAE)-based approach is proposed 
    
[^11]: 学习多层生成器的联合潜在空间EBM先验模型

    Learning Joint Latent Space EBM Prior Model for Multi-layer Generator. (arXiv:2306.06323v1 [cs.CV])

    [http://arxiv.org/abs/2306.06323](http://arxiv.org/abs/2306.06323)

    本文研究了学习多层生成器模型的基本问题，并提出了一种以多层生成器为骨干的联合潜在空间EBM先验模型，该模型可以更好地学习复杂的数据分布和分层表示，实现更好的生成质量和修复结果。

    

    本文研究了学习多层生成器模型的基本问题。多层生成器模型在生成器之上构建多层潜在变量作为先验模型，有利于学习复杂的数据分布和分层表示。然而，这样的先验模型通常通过假设非信息（条件）高斯分布来专注于建模潜在变量之间的层间关系，并且在模型表达能力方面存在局限性。为了解决这个问题并学习更具表现力的先验模型，我们在所有潜在变量的联合潜在空间上提出了一种以多层生成器为骨干的能量基模型（EBM）的先验模型。这种联合潜在空间EBM 先验模型通过层间能量项捕获每层内的内部关系，并对不同层的潜在变量进行联合修正。我们通过最大似然估计开发了一种联合训练方案，其中同时学习生成器和EBM先验模型。在生成图像建模和修复任务上的实验表明，与几个基线相比，我们提出的方法可以学习更具表现力和可解释性的先验模型，从而实现更好的生成质量和修复结果。

    This paper studies the fundamental problem of learning multi-layer generator models. The multi-layer generator model builds multiple layers of latent variables as a prior model on top of the generator, which benefits learning complex data distribution and hierarchical representations. However, such a prior model usually focuses on modeling inter-layer relations between latent variables by assuming non-informative (conditional) Gaussian distributions, which can be limited in model expressivity. To tackle this issue and learn more expressive prior models, we propose an energy-based model (EBM) on the joint latent space over all layers of latent variables with the multi-layer generator as its backbone. Such joint latent space EBM prior model captures the intra-layer contextual relations at each layer through layer-wise energy terms, and latent variables across different layers are jointly corrected. We develop a joint training scheme via maximum likelihood estimation (MLE), which involves
    
[^12]: 面向阿拉伯语情感分析的多模态数据集构建

    Towards Arabic Multimodal Dataset for Sentiment Analysis. (arXiv:2306.06322v1 [cs.CL])

    [http://arxiv.org/abs/2306.06322](http://arxiv.org/abs/2306.06322)

    本文针对阿拉伯语DL-based MSA领域缺乏标准数据集的问题，通过使用最先进的转换器和单词对齐技术中的特征提取工具来设计管道流程，构建了阿拉伯语多模态数据集，实验表明其具有很大的潜力。

    

    多模态情感分析(MSA)已成为许多实际应用的中心研究方向。这种普及得益于意见对几乎所有人类活动的重要性，以及成为我们行为的关键因素。近年来，基于深度学习(DL)模型的高效性也已在多种西方语言中得到证明。然而，由于缺乏标准数据集，阿拉伯语DL-based MSA仍处于初始阶段。本文的探究目标有两个方面，首先，我们设计了一种管道流程，利用最先进的转换器以及单词对齐技术中的特征提取工具，帮助构建我们的阿拉伯语多模态数据集。然后，我们使用最先进的基于转换器的模型验证了我们的数据集。尽管输出数据集的规模较小，实验结果表明，阿拉伯语多模态仍然具有非常大的潜力。

    Multimodal Sentiment Analysis (MSA) has recently become a centric research direction for many real-world applications. This proliferation is due to the fact that opinions are central to almost all human activities and are key influencers of our behaviors. In addition, the recent deployment of Deep Learning-based (DL) models has proven their high efficiency for a wide range of Western languages. In contrast, Arabic DL-based multimodal sentiment analysis (MSA) is still in its infantile stage due, mainly, to the lack of standard datasets. In this paper, our investigation is twofold. First, we design a pipeline that helps building our Arabic Multimodal dataset leveraging both state-of-the-art transformers and feature extraction tools within word alignment techniques. Thereafter, we validate our dataset using state-of-the-art transformer-based model dealing with multimodality. Despite the small size of the outcome dataset, experiments show that Arabic multimodality is very promising
    
[^13]: 多任务知识增强在AI助手应用中的零样本和多领域推荐中的应用

    Multi-Task Knowledge Enhancement for Zero-Shot and Multi-Domain Recommendation in an AI Assistant Application. (arXiv:2306.06302v1 [cs.IR])

    [http://arxiv.org/abs/2306.06302](http://arxiv.org/abs/2306.06302)

    本文提出了一种利用多领域交互信息和外部知识图来进行新领域预测的方法，并将其应用于一个AI助手应用中，以提高推荐系统的预测准确性。

    

    推荐系统在商业上取得了巨大的成功，但仍然难以将新用户整合进去。由于用户经常在不同领域与内容进行交互，因此可以利用用户在之前的领域中的交互来改善其在新领域中的推荐（多领域推荐）。知识图增强的单一领域推荐（知识图增强）的研究线程独立于此使用外部知识图来提高推荐系统的预测准确性。我们在这项工作中提出将这些方法统一起来：利用其他领域中的交互信息以及外部知识图来进行新领域的推荐。我们将这些想法应用于一个从数百万用户请求的视频、音乐和书籍的数据集中，该数据集用于一个AI助手应用中。

    Recommender systems have found significant commercial success but still struggle with integrating new users. Since users often interact with content in different domains, it is possible to leverage a user's interactions in previous domains to improve that user's recommendations in a new one (multi-domain recommendation). A separate research thread on knowledge graph enhancement uses external knowledge graphs to improve single domain recommendations (knowledge graph enhancement). Both research threads incorporate related information to improve predictions in a new domain. We propose in this work to unify these approaches: Using information from interactions in other domains as well as external knowledge graphs to make predictions in a new domain that would be impossible with either information source alone. We apply these ideas to a dataset derived from millions of users' requests for content across three domains (videos, music, and books) in a live virtual assistant application. We dem
    
[^14]: 响应时间改善高斯过程模型的感知和偏好选择预测与函数估计

    Response Time Improves Choice Prediction and Function Estimation for Gaussian Process Models of Perception and Preferences. (arXiv:2306.06296v1 [q-bio.NC])

    [http://arxiv.org/abs/2306.06296](http://arxiv.org/abs/2306.06296)

    本论文提出一种新颖的方法，将响应时间考虑进高斯过程模型的选择预测和函数估计中，使得更容易在连续多元函数逼近模型下进行选择预测和函数估计。

    

    偏好学习和心理物理学中的人类选择预测模型通常仅考虑二进制响应数据，需要大量样本才能准确学习偏好或感知检测阈值。然而，用于做出每个选择的响应时间（RT）捕获了关于决策过程的额外信息，但是现有模型将RT用于选择预测的设置是充分参数化的或离散的刺激设置。这部分是因为选择RT的事实上的标准模型 - 扩散判定模型（DDM），不允许可处理，可微分的推断。因此，DDM不能轻松地与灵活的连续多元函数逼近模型，特别是高斯过程（GP）模型集成。我们提出了一种新颖的可微分近似DDM似然函数，使用已知的带偏三参数分布族。然后，我们使用这个新的似然函数将RT纳入二进制选择的GP模型中。我们的RT选择GP使得更容易在连续多元函数逼近模型下进行选择预测和函数估计。

    Models for human choice prediction in preference learning and psychophysics often consider only binary response data, requiring many samples to accurately learn preferences or perceptual detection thresholds. The response time (RT) to make each choice captures additional information about the decision process, however existing models incorporating RTs for choice prediction do so in fully parametric settings or over discrete stimulus sets. This is in part because the de-facto standard model for choice RTs, the diffusion decision model (DDM), does not admit tractable, differentiable inference. The DDM thus cannot be easily integrated with flexible models for continuous, multivariate function approximation, particularly Gaussian process (GP) models. We propose a novel differentiable approximation to the DDM likelihood using a family of known, skewed three-parameter distributions. We then use this new likelihood to incorporate RTs into GP models for binary choices. Our RT-choice GPs enable
    
[^15]: PLPCA：用于微阵列数据分析的持续拉普拉斯增强PCA

    PLPCA: Persistent Laplacian Enhanced-PCA for Microarray Data Analysis. (arXiv:2306.06292v1 [math.AT])

    [http://arxiv.org/abs/2306.06292](http://arxiv.org/abs/2306.06292)

    本论文提出了一种新方法PLPCA，用于解决PCA在微阵列数据分析中的局限性，通过持续谱图理论结合拉普拉斯增强来实现多尺度分析和捕捉数据中的高阶交互作用。

    

    多年来，主成分分析（PCA）一直是基线方法，用于基因表达数据分析中的降维。它的主要目标是从成千上万的基因中识别出一组疾病相关的基因子集。然而，PCA具有固有的局限性，阻碍了其可解释性，引入了分类歧义，并未捕捉到数据中的复杂几何结构。为了解决这些挑战，我们提出了一种新方法，称为持续拉普拉斯增强主成分分析（PLPCA）。

    Over the years, Principal Component Analysis (PCA) has served as the baseline approach for dimensionality reduction in gene expression data analysis. It primary objective is to identify a subset of disease-causing genes from a vast pool of thousands of genes. However, PCA possesses inherent limitations that hinder its interpretability, introduce classification ambiguity, and fail to capture complex geometric structures in the data. Although these limitations have been partially addressed in the literature by incorporating various regularizers such as graph Laplacian regularization, existing improved PCA methods still face challenges related to multiscale analysis and capturing higher-order interactions in the data. To address these challenges, we propose a novel approach called Persistent Laplacian-enhanced Principal Component Analysis (PLPCA). PLPCA amalgamates the advantages of earlier regularized PCA methods with persistent spectral graph theory, specifically persistent Laplacians d
    
[^16]: 最优异构协同线性回归和上下文臂研究

    Optimal Heterogeneous Collaborative Linear Regression and Contextual Bandits. (arXiv:2306.06291v1 [stat.ML])

    [http://arxiv.org/abs/2306.06291](http://arxiv.org/abs/2306.06291)

    本文提出了一种新的估计器MOLAR，它利用协同线性回归和上下文臂问题中的稀疏异质性来提高估计精度，并且相比独立方法具有更好的表现。

    

    大型和复杂的数据集往往来自于几个可能是异构的来源。协同学习方法通过利用数据集之间的共性提高效率，同时考虑可能出现的差异。在这里，我们研究协同线性回归和上下文臂问题，其中每个实例的相关参数等于全局参数加上一个稀疏的实例特定术语。我们提出了一种名为MOLAR的新型二阶段估计器，它通过首先构建实例线性回归估计的逐项中位数，然后将实例特定估计值收缩到中位数附近来利用这种结构。与独立最小二乘估计相比，MOLAR提高了估计误差对数据维度的依赖性。然后，我们将MOLAR应用于开发用于稀疏异构协同上下文臂的方法，这些方法相比独立臂模型具有更好的遗憾保证。我们进一步证明了我们的贡献优于先前在文献中报道的算法。

    Large and complex datasets are often collected from several, possibly heterogeneous sources. Collaborative learning methods improve efficiency by leveraging commonalities across datasets while accounting for possible differences among them. Here we study collaborative linear regression and contextual bandits, where each instance's associated parameters are equal to a global parameter plus a sparse instance-specific term. We propose a novel two-stage estimator called MOLAR that leverages this structure by first constructing an entry-wise median of the instances' linear regression estimates, and then shrinking the instance-specific estimates towards the median. MOLAR improves the dependence of the estimation error on the data dimension, compared to independent least squares estimates. We then apply MOLAR to develop methods for sparsely heterogeneous collaborative contextual bandits, which lead to improved regret guarantees compared to independent bandit methods. We further show that our 
    
[^17]: LLMs如何改变材料科学和化学：大型语言模型黑客马拉松的反思

    14 Examples of How LLMs Can Transform Materials Science and Chemistry: A Reflection on a Large Language Model Hackathon. (arXiv:2306.06283v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2306.06283](http://arxiv.org/abs/2306.06283)

    本文记录了一次黑客松活动，参与者使用LLMs进行了各种应用，包括预测分子和材料特性、从非结构化数据中提取知识、为工具设计新界面以及开发新的教育应用。这些多样化的项目反映了LLMs在材料科学和化学领域的多功能性和潜力。

    

    化学和材料科学非常复杂。最近，使用数据驱动或计算技术解决了这种复杂性的中有很大的成功。然而，输入需要非常特定形式的结构以及工具数量不断增长所带来可用性和可访问性的挑战。加上这些学科中的大多数数据都是非结构化的事实，使得这些工具的效率受到限制。本文记录了关于LLMs的黑客松活动中构建的项目。参与者使用LLMs进行了各种应用，包括预测分子和材料的特性、为工具设计新界面、从非结构化数据中提取知识以及开发新的教育应用。各种各样的项目反映了LLMs在这些领域的多功能性和这些模型改变材料科学和化学领域的潜力。

    Chemistry and materials science are complex. Recently, there have been great successes in addressing this complexity using data-driven or computational techniques. Yet, the necessity of input structured in very specific forms and the fact that there is an ever-growing number of tools creates usability and accessibility challenges. Coupled with the reality that much data in these disciplines is unstructured, the effectiveness of these tools is limited.  Motivated by recent works that indicated that large language models (LLMs) might help address some of these issues, we organized a hackathon event on the applications of LLMs in chemistry, materials science, and beyond. This article chronicles the projects built as part of this hackathon. Participants employed LLMs for various applications, including predicting properties of molecules and materials, designing novel interfaces for tools, extracting knowledge from unstructured data, and developing new educational applications.  The diverse
    
[^18]: 能量耗散进化深度算子神经网络

    Energy-Dissipative Evolutionary Deep Operator Neural Networks. (arXiv:2306.06281v1 [stat.ML])

    [http://arxiv.org/abs/2306.06281](http://arxiv.org/abs/2306.06281)

    能量耗散进化深度算子神经网络是一种运算学习神经网络，可为一类偏微分方程提供数值解并保留其物理特性，通过支路网络编码不同的输入函数，干线网络评估输出函数，经过训练可生成运算符近似解。

    

    能量耗散进化深度算子神经网络是一种运算学习神经网络，旨在为一类偏微分方程种子提供数值解，而不是单个偏微分方程，例如带有不同参数或不同初始条件的偏微分方程。该网络由两个子网络组成：支路网络和干线网络。对于目标运算符 G，支路网络在相同数量的传感器上编码不同的输入函数 u，而干线网络评估任何位置的输出函数。通过最小化评估的输出 q 和预期输出 G(u)(y) 之间的误差，DeepONet 生成 G 运算符的良好近似。为了保留偏微分方程的重要物理特性，如能量耗散定律，我们采用标量辅助变量法生成最小化问题。它引入了修改后的能量并实现了无条件的能量耗散定律。

    Energy-Dissipative Evolutionary Deep Operator Neural Network is an operator learning neural network. It is designed to seed numerical solutions for a class of partial differential equations instead of a single partial differential equation, such as partial differential equations with different parameters or different initial conditions. The network consists of two sub-networks, the Branch net and the Trunk net. For an objective operator G, the Branch net encodes different input functions u at the same number of sensors, and the Trunk net evaluates the output function at any location. By minimizing the error between the evaluated output q and the expected output G(u)(y), DeepONet generates a good approximation of the operator G. In order to preserve essential physical properties of PDEs, such as the Energy Dissipation Law, we adopt a scalar auxiliary variable approach to generate the minimization problem. It introduces a modified energy and enables unconditional energy dissipation law a
    
[^19]: 利用对比学习预测癌症预后的基因表达值

    Contrastive Learning for Predicting Cancer Prognosis Using Gene Expression Values. (arXiv:2306.06276v1 [cs.LG])

    [http://arxiv.org/abs/2306.06276](http://arxiv.org/abs/2306.06276)

    该论文提出应用对比学习方法，从有限的数据样本中学习基因表达数据的良好特征表示，并应用于Cox模型中，可以显著提高癌症预后的预测性能。

    

    最近开发了多种人工神经网络（ANNs）作为Cox比例风险模型，以基于肿瘤转录组预测癌症预后。然而，它们并没有表现出比传统有规则化的Cox回归显着更好的性能。在有限数量的数据样本和高维特征空间存在的情况下，训练具有高预测能力的ANN是具有挑战性的。图像分类的最新进展表明，对比学习可以通过从有限数量的数据样本中学习良好的特征表示来促进进一步的学习任务。在本文中，我们将监督对比学习应用于肿瘤基因表达和临床数据，以在低维空间中学习特征表示。然后，我们使用这些学习到的特征来训练Cox模型，以预测癌症预后。使用来自The Cancer Genome Atlas（TCGA）的数据，我们证明了我们基于对比学习的Cox模型能够通过学习基因表达数据的良好特征表征，显著提高癌症预后预测的性能。

    Several artificial neural networks (ANNs) have recently been developed as the Cox proportional hazard model for predicting cancer prognosis based on tumor transcriptome. However, they have not demonstrated significantly better performance than the traditional Cox regression with regularization. Training an ANN with high prediction power is challenging in the presence of a limited number of data samples and a high-dimensional feature space. Recent advancements in image classification have shown that contrastive learning can facilitate further learning tasks by learning good feature representation from a limited number of data samples. In this paper, we applied supervised contrastive learning to tumor gene expression and clinical data to learn feature representations in a low-dimensional space. We then used these learned features to train the Cox model for predicting cancer prognosis. Using data from The Cancer Genome Atlas (TCGA), we demonstrated that our contrastive learning-based Cox 
    
[^20]: DeepLCZChange：一种用于城市气候韧性的遥感深度学习模型架构

    DeepLCZChange: A Remote Sensing Deep Learning Model Architecture for Urban Climate Resilience. (arXiv:2306.06269v1 [cs.CV])

    [http://arxiv.org/abs/2306.06269](http://arxiv.org/abs/2306.06269)

    本研究介绍了一个遥感深度学习架构，名为DeepLCZChange，结合了空中LiDAR数据和Landsat 8卫星的地表温度产品，用于研究城市土地利用与当地气候之间的关系。在纽约市的应用中验证了城市森林的降温效应。

    

    城市土地利用结构会影响都市区的局部气候条件。为了揭示城市土地利用与当地气候之间的机制，本文提出了一种新颖的数据驱动深度学习架构和流程，DeepLCZChange，以相关空中LiDAR数据统计数据与Landsat 8卫星的地表温度产品相关联。一个概念验证数值实验利用纽约市的相应遥感数据验证了城市森林的降温效应。

    Urban land use structures impact local climate conditions of metropolitan areas. To shed light on the mechanism of local climate wrt. urban land use, we present a novel, data-driven deep learning architecture and pipeline, DeepLCZChange, to correlate airborne LiDAR data statistics with the Landsat 8 satellite's surface temperature product. A proof-of-concept numerical experiment utilizes corresponding remote sensing data for the city of New York to verify the cooling effect of urban forests.
    
[^21]: AS-GAN增强的传感器数据增强用于制造系统在线监测

    Attention-stacked Generative Adversarial Network (AS-GAN)-empowered Sensor Data Augmentation for Online Monitoring of Manufacturing System. (arXiv:2306.06268v1 [cs.LG])

    [http://arxiv.org/abs/2306.06268](http://arxiv.org/abs/2306.06268)

    本文提出了一种名为AS-GAN的技术，使用数据增强来解决监督式机器学习中的数据不平衡问题，其中AS-GAN有效地学习异常状态数据的基础分布，并生成高质量的数据样本用于在线制造系统监测。

    

    机器学习在先进制造系统的在线感知监测中得到广泛应用。然而，在异常条件下收集的传感器数据通常是不充分的，这会导致监督式机器学习中的严重数据不平衡问题。为了解决这个问题，一种常见的方法是采用数据增强技术，即通过合成生成来增加可用的异常状态数据（即少数样本）。为了有效地生成高质量的少数样本，学习异常状态数据的基础分布至关重要。近年来，基于生成对抗网络（GAN）的方法成为学习数据分布以及执行数据增强的流行技术。然而，在实践中，基于GAN的数据增强生成的样本质量可能会有很大的差异。此外，传感器信号是按时间从制造系统中顺序收集的，这意味着考虑到时间的顺序性是重要的。

    Machine learning (ML) has been extensively adopted for the online sensing-based monitoring in advanced manufacturing systems. However, the sensor data collected under abnormal states are usually insufficient, leading to significant data imbalanced issue for supervised machine learning. A common solution for this issue is to incorporate data augmentation technique, i.e., augmenting the available abnormal states data (i.e., minority samples) via synthetic generation. To generate the high-quality minority samples effectively, it is vital to learn the underlying distribution of the abnormal states data. In recent years, the generative adversarial network (GAN)-based approaches become popular to learn data distribution as well as perform data augmentation. However, in practice, the quality of generated samples from GAN-based data augmentation may vary drastically. In addition, the sensor signals are collected sequentially by time from the manufacturing systems, which means the consideration
    
[^22]: 强化学习中基于回合限制的近优保守探索

    Near-optimal Conservative Exploration in Reinforcement Learning under Episode-wise Constraints. (arXiv:2306.06265v1 [cs.LG])

    [http://arxiv.org/abs/2306.06265](http://arxiv.org/abs/2306.06265)

    本文研究了强化学习中实现保守探索的问题，提出了名为StepMix的算法，利用现有的安全基线策略平衡开发和探索，同时保证每个回合不违反保守限制，并且能够在不受限制的情况下达到接近最优的后悔量级。

    

    本文研究了在强化学习中实现保守探索的问题，其中学习代理的性能在整个学习过程中保证高于某个特定阈值。研究针对有限状态和动作的标签式回合式马尔可夫决策过程（MDP）环境。本文提出了一种名为StepMix的算法，利用现有的安全基线策略在保证每个回合不违反保守限制的前提下，平衡开发和探索。StepMix具有独特的混合策略设计，自适应地、平滑地插值基线策略和乐观策略之间。理论分析表明，StepMix在不受限制的情况下具有接近最优的后悔量级，说明遵守严格的回合限制不会损害学习性能。此外，还提出了基于随机化的EpsMix算法。

    This paper investigates conservative exploration in reinforcement learning where the performance of the learning agent is guaranteed to be above a certain threshold throughout the learning process. It focuses on the tabular episodic Markov Decision Process (MDP) setting that has finite states and actions. With the knowledge of an existing safe baseline policy, an algorithm termed as StepMix is proposed to balance the exploitation and exploration while ensuring that the conservative constraint is never violated in each episode with high probability. StepMix features a unique design of a mixture policy that adaptively and smoothly interpolates between the baseline policy and the optimistic policy. Theoretical analysis shows that StepMix achieves near-optimal regret order as in the constraint-free setting, indicating that obeying the stringent episode-wise conservative constraint does not compromise the learning performance. Besides, a randomization-based EpsMix algorithm is also proposed
    
[^23]: 在大型语言模型中测量和修改事实知识

    Measuring and Modifying Factual Knowledge in Large Language Models. (arXiv:2306.06264v1 [cs.CL])

    [http://arxiv.org/abs/2306.06264](http://arxiv.org/abs/2306.06264)

    这项研究提出了一种基于信息论的测量框架，可用于衡量大型语言模型中的事实知识，并通过熵及KL散度等度量指标进行知识修改，超越了以前的排名方法，并提供了一种有价值的工具，用于测量和修改LLMs中的大量事实知识。

    

    大型语言模型（LLMs）存储着从大量文本中获取的广泛的事实知识。为了有效地利用这些模型进行下游任务，有可靠的方法来衡量它们的知识是至关重要的。然而，现有的知识测量方法存在某些限制，尽管最近有不少努力，但它们不能提供准确的测量和修改LLMs中所需的洞察力。在这项工作中，我们采用基于信息理论的测量方法来提供一个框架来估计大型语言模型中包含的事实知识。具体而言，我们通过分析LLM在注入目标知识前后的预测概率分布来衡量知识，使用熵和KL-散度等度量标准。首先介绍我们的指标，我们通过一项合成实验，在准确性方面与以前的排名方法进行比较，超过了它们35％以上。然后，我们解释了这些指标如何用于知识修改，提出了一种选择性修改大型语言模型中的实际知识的方法。总的来说，我们的方法提供了一个宝贵的工具，用于测量和修改LLMs中的大量事实知识。

    Large Language Models (LLMs) store an extensive amount of factual knowledge obtained from vast collections of text. To effectively utilize these models for downstream tasks, it is crucial to have reliable methods for measuring their knowledge. However, existing approaches for knowledge measurement have certain limitations, and despite recent efforts, they fail to provide accurate measurements and the necessary insights for modifying the knowledge within LLMs. In this work, we employ information theory-based measurements to provide a framework estimating the factual knowledge contained within large language models. More specifically, we measure knowledge by analyzing the LLM's prediction probability distribution before and after instilling the target knowledge, employing metrics such as entropy and KL-divergence. Introducing our metrics, we first assess their accuracy in comparison to previous ranking-based methods, surpassing them by over $35\%$ in a synthetic experiment. Then, we expl
    
[^24]: 基于谱间隔的确定性张量补全

    Spectral gap-based deterministic tensor completion. (arXiv:2306.06262v1 [stat.ML])

    [http://arxiv.org/abs/2306.06262](http://arxiv.org/abs/2306.06262)

    本文界定了Poisson loss和原子范数最小化两种张量补全方法的解的泛化误差，提供了更紧的界限，针对$r$的依赖性从之前的$r^{2(t-1)(t^2-t-1)}$改进为$r^{2(t-1)(3t-5)}$。根据采样稀疏模式的谱间隔控制误差界限，同时证明了原子张量范数的几个新属性，但原子范数最小化存在计算挑战。

    

    张量补全是一个核心的机器学习算法，用于推荐系统和其他带有缺失数据的领域。虽然对于矩阵情况已有深入研究，但针对张量问题的理论结果仍然有限，特别是当采样模式是确定性的时候。在本文中，我们对两种张量补全方法的解的泛化误差进行了界定，分别是Poisson loss和原子范数最小化，用目标张量秩作为判断依据，提供了更紧的界限。如果目标张量的阶数为$t$，CP秩为$r$，则我们的界限中针对$r$的依赖性从arXiv:1910.10692的$r^{2(t-1)(t^2-t-1)}$改进为$r^{2(t-1)(3t-5)}$。我们的误差界是由采样稀疏模式的谱间隔决定的。我们还证明了原子张量范数的几个新属性，在随机采样方案下将秩的依赖性从arXiv:1711.04965的$r^{3t-3}$减少到$r^{3t-5}$。然而，原子范数最小化的一个局限性是，虽然在理论上很有趣，但会导致计算上的挑战。

    Tensor completion is a core machine learning algorithm used in recommender systems and other domains with missing data. While the matrix case is well-understood, theoretical results for tensor problems are limited, particularly when the sampling patterns are deterministic. Here we bound the generalization error of the solutions of two tensor completion methods, Poisson loss and atomic norm minimization, providing tighter bounds in terms of the target tensor rank. If the ground-truth tensor is order $t$ with CP-rank $r$, the dependence on $r$ is improved from $r^{2(t-1)(t^2-t-1)}$ in arXiv:1910.10692 to $r^{2(t-1)(3t-5)}$. The error in our bounds is deterministically controlled by the spectral gap of the sampling sparsity pattern. We also prove several new properties for the atomic tensor norm, reducing the rank dependence from $r^{3t-3}$ in arXiv:1711.04965 to $r^{3t-5}$ under random sampling schemes. A limitation is that atomic norm minimization, while theoretically interesting, leads
    
[^25]: 理解图像增强的好处

    Understanding the Benefits of Image Augmentations. (arXiv:2306.06254v1 [cs.CV])

    [http://arxiv.org/abs/2306.06254](http://arxiv.org/abs/2306.06254)

    本论文通过分析各种宽度和深度的ResNets模型并对其进行增广，发现使用两个图像信息的增广对所学权重的影响显着大于单个图像的影响，使用ImageNet-1K权重和微调的ResNets深层比早期层受到更大的影响。

    

    图像增强被广泛用于神经网络中的过拟合问题。然而，它们好处的可解释性仍然是一个谜。使用居中核对齐（CKA），我们研究了残差神经网络（ResNets）的哪些层受到增强的影响最大。我们通过分析各种宽度和深度的模型以及它们的权重是随机初始化还是通过迁移学习来进行。我们发现，层受影响的模式取决于模型的深度，并且使用来自两个图像的信息的增强对学习的权重产生的影响显著大于对单个图像进行操作的增强。使用ImageNet-1K权重初始化和微调的ResNets的深层比早期层受到的增强影响更大。了解图像增强对卷积神经网络的影响将有各种应用，例如确定需要多长时间才能对网络进行微调以及微调强度。

    Image Augmentations are widely used to reduce overfitting in neural networks. However, the explainability of their benefits largely remains a mystery. We study which layers of residual neural networks (ResNets) are most affected by augmentations using Centered Kernel Alignment (CKA). We do so by analyzing models of varying widths and depths, as well as whether their weights are initialized randomly or through transfer learning. We find that the pattern of how the layers are affected depends on the model's depth, and that networks trained with augmentation that use information from two images affect the learned weights significantly more than augmentations that operate on a single image. Deeper layers of ResNets initialized with ImageNet-1K weights and fine-tuned receive more impact from the augmentations than early layers. Understanding the effects of image augmentations on CNNs will have a variety of applications, such as determining how far back one needs to fine-tune a network and w
    
[^26]: 通过模块化生成模型实现灵活的强化学习决策堆叠

    Decision Stacks: Flexible Reinforcement Learning via Modular Generative Models. (arXiv:2306.06253v1 [cs.LG])

    [http://arxiv.org/abs/2306.06253](http://arxiv.org/abs/2306.06253)

    决策堆叠是一种灵活的生成框架，将目标条件化策略代理分解为3个生成模块，并通过教师强制并行学习，可以用于设计单个模块以考虑架构偏差、优化目标和动态、跨领域的可转移性和推理速度。在连续控制基准测试中具有灵活性和可扩展性。

    

    强化学习是一种有吸引力的模型，可以推理顺序决策制定的几个不同方面，如指定复杂目标、规划未来观察和行动，以及批评其实用性。然而，这些能力的综合集成在保持最大表达能力的同时允许进行模型选择以实现高效的学习和推理，这构成了竞争性的算法挑战。我们提出了决策堆叠（Decision Stacks），这是一个生成框架，将目标条件化策略代理分解为3个生成模块。这些模块通过独立的生成模型模拟了观测、奖励和行动的时间演变，可以通过教师强制并行学习。我们的框架保证了表达能力和灵活性，在设计单个模块以考虑关键因素（如架构偏差、优化目标和动态、跨领域的可转移性和推理速度）方面具有优势。我们对一系列连续控制基准进行的实证结果表明，决策堆叠提供了一种灵活且可扩展的替代最先进的基于模型和基于模型的强化学习方法。

    Reinforcement learning presents an attractive paradigm to reason about several distinct aspects of sequential decision making, such as specifying complex goals, planning future observations and actions, and critiquing their utilities. However, the combined integration of these capabilities poses competing algorithmic challenges in retaining maximal expressivity while allowing for flexibility in modeling choices for efficient learning and inference. We present Decision Stacks, a generative framework that decomposes goal-conditioned policy agents into 3 generative modules. These modules simulate the temporal evolution of observations, rewards, and actions via independent generative models that can be learned in parallel via teacher forcing. Our framework guarantees both expressivity and flexibility in designing individual modules to account for key factors such as architectural bias, optimization objective and dynamics, transferrability across domains, and inference speed. Our empirical 
    
[^27]: 基于特征编程的多元时间序列预测

    Feature Programming for Multivariate Time Series Prediction. (arXiv:2306.06252v1 [cs.LG])

    [http://arxiv.org/abs/2306.06252](http://arxiv.org/abs/2306.06252)

    该论文提出了一种基于特征编程的多元时间序列预测框架，能够生成大量预测特征，同时允许用户以最小的工作量结合他们的归纳偏差。该框架基于细粒度的轨迹增量，引入了新型自旋气体动力学伊辛模型，并提供了一个简约的算子集来总结多元时间序列。

    

    我们引入了可编程特征工程的概念，提出了一个特征编程框架，为嘈杂的多元时间序列生成大量预测特征，同时允许用户以最小的工作量结合他们的归纳偏差。我们的框架的关键动机是将任何多元时间序列视为细粒度轨迹增量的累积总和，其中每个增量由一种新型自旋气体动力学伊辛模型控制。这种细粒度的视角激发了发展一个简约的算子集，以抽象的方式总结多元时间序列，为大规模的自动特征工程提供基础。数值上，我们验证了我们的方法在几个合成和真实世界的嘈杂时间序列数据集上的有效性。

    We introduce the concept of programmable feature engineering for time series modeling and propose a feature programming framework. This framework generates large amounts of predictive features for noisy multivariate time series while allowing users to incorporate their inductive bias with minimal effort. The key motivation of our framework is to view any multivariate time series as a cumulative sum of fine-grained trajectory increments, with each increment governed by a novel spin-gas dynamical Ising model. This fine-grained perspective motivates the development of a parsimonious set of operators that summarize multivariate time series in an abstract fashion, serving as the foundation for large-scale automated feature engineering. Numerically, we validate the efficacy of our method on several synthetic and real-world noisy time series datasets.
    
[^28]: 通信系统中AI通用性与可扩展性的设计原则

    Design Principles for Generalization and Scalability of AI in Communication Systems. (arXiv:2306.06251v1 [cs.LG])

    [http://arxiv.org/abs/2306.06251](http://arxiv.org/abs/2306.06251)

    本文提出了可持续和可扩展的AI集成通信系统的设计原则，侧重于创建具备通用性的AI算法，通过少量的AI驱动的RAN函数来解决更大的问题，提高系统性能，并简化生命周期管理。

    

    人工智能（AI）已经成为通信系统中解决复杂和动态任务的强大工具，传统的基于规则的算法往往无法胜任。然而，大多数网络任务的AI应用都是针对特定和有限的条件设计和训练的，使得算法无法适应于常见的网络环境、任务和控制任务。本文提出了可持续和可扩展的AI集成通信系统的设计原则，侧重于创建可以在网络环境、意图和控制任务上具备通用性的AI算法。这种方法可以使少量的AI驱动的RAN函数来解决更大的问题，提高系统性能，并简化生命周期管理。为了实现可持续性和自动化，我们引入了一种可扩展的学习架构，该架构支持系统中部署的所有AI应用程序。该架构将中央化学习功能与分布式学习和数据采集功能分离，确保了系统的可扩展性和稳健性。

    Artificial intelligence (AI) has emerged as a powerful tool for addressing complex and dynamic tasks in communication systems, where traditional rule-based algorithms often struggle. However, most AI applications to networking tasks are designed and trained for specific, limited conditions, hindering the algorithms from learning and adapting to generic situations, such as those met across radio access networks (RAN). This paper proposes design principles for sustainable and scalable AI integration in communication systems, focusing on creating AI algorithms that can generalize across network environments, intents, and control tasks. This approach enables a limited number of AI-driven RAN functions to tackle larger problems, improve system performance, and simplify lifecycle management. To achieve sustainability and automation, we introduce a scalable learning architecture that supports all deployed AI applications in the system. This architecture separates centralized learning function
    
[^29]: 战略性苹果品尝：带有一面性反馈的在线学习问题

    Strategic Apple Tasting. (arXiv:2306.06250v1 [cs.GT])

    [http://arxiv.org/abs/2306.06250](http://arxiv.org/abs/2306.06250)

    本篇论文介绍了一种带有苹果品尝反馈的在线学习问题，该问题需要使用一种学习算法以实现战略遗憾。研究结果表明，我们提出的算法的战略遗憾近似于最佳速率。

    

    在高风险领域中，算法决策往往涉及将决策分配给具有策略性修改其算法输入动机的代理。除了应对激励因素外，在许多感兴趣的领域（例如贷款和招聘）中，决策者只观察到在分配积极决策给代理时的回馈。我们将这种反馈称为苹果品尝（或单向反馈）。我们将这一情境形式化为带有苹果品尝反馈的在线学习问题，其中一个负责人决策一系列 $T$ 个代理，每个代理由可被策略性修改的上下文表示。我们的目标是在代理揭示其上下文时获得亚线性的战略遗憾，即如果代理在揭示其上下文时是真实的，则将负责人的表现与后见之明的最佳固定策略进行比较。我们的主要结果是一种学习算法，在这种情况下产生 $\tilde{\mathcal{O}}(\sqrt{T})$ 的战略遗憾，与更一般类型的反馈所知的最佳速率相匹配。

    Algorithmic decision-making in high-stakes domains often involves assigning decisions to agents with incentives to strategically modify their input to the algorithm. In addition to dealing with incentives, in many domains of interest (e.g. lending and hiring) the decision-maker only observes feedback regarding their policy for rounds in which they assign a positive decision to the agent; this type of feedback is often referred to as apple tasting (or one-sided) feedback. We formalize this setting as an online learning problem with apple-tasting feedback where a principal makes decisions about a sequence of $T$ agents, each of which is represented by a context that may be strategically modified. Our goal is to achieve sublinear strategic regret, which compares the performance of the principal to that of the best fixed policy in hindsight, if the agents were truthful when revealing their contexts. Our main result is a learning algorithm which incurs $\tilde{\mathcal{O}}(\sqrt{T})$ strate
    
[^30]: 使用集合型反馈的在线学习

    Online Learning with Set-Valued Feedback. (arXiv:2306.06247v1 [cs.LG])

    [http://arxiv.org/abs/2306.06247](http://arxiv.org/abs/2306.06247)

    本文研究了一种在线多类分类的变体，其中使用集合型反馈。通过引入新的组合维度，该论文表明确定性和随机性的在线可学习性在实现设置下不等价，并将在线多标签排名和在线多标签分类等实际学习设置作为其特定实例。

    

    本文研究了在线多类分类的一种变体，其中学习器预测单个标签，但接收到一个标签的集合作为反馈。在该模型中，如果学习器没有输出包含在反馈集合中的标签，则会受到惩罚。我们表明，与具有单标签反馈的在线多类学习不同，在实现设置中使用集合型反馈时，确定性和随机化的在线可学习性\textit{不等价}。因此，我们提供了两个新的组合维度，分别命名为集合小石和度量破裂维度，严格描述了确定性和随机化的在线可学习性。此外，我们表明度量破裂维度在悟性设置下严格描述在线可学习性。最后，我们证明了在线多标签排名和在线多标签分类等实际学习设置是我们通用在线学习框架的具体实例。

    We study a variant of online multiclass classification where the learner predicts a single label but receives a \textit{set of labels} as feedback. In this model, the learner is penalized for not outputting a label contained in the revealed set. We show that unlike online multiclass learning with single-label feedback, deterministic and randomized online learnability are \textit{not equivalent} even in the realizable setting with set-valued feedback. Accordingly, we give two new combinatorial dimensions, named the Set Littlestone and Measure Shattering dimension, that tightly characterize deterministic and randomized online learnability respectively in the realizable setting. In addition, we show that the Measure Shattering dimension tightly characterizes online learnability in the agnostic setting. Finally, we show that practical learning settings like online multilabel ranking and online multilabel classification are specific instances of our general online learning framework.
    
[^31]: 理解长尾效应对神经网络压缩的影响

    Understanding the Effect of the Long Tail on Neural Network Compression. (arXiv:2306.06238v1 [cs.LG])

    [http://arxiv.org/abs/2306.06238](http://arxiv.org/abs/2306.06238)

    本文研究了在神经网络压缩中如何保持与原始网络的语义相同，在长尾现象中探讨了压缩提高推广性能的记忆要素。

    

    网络压缩现在是神经网络研究的一个成熟的子领域，过去的十年中，取得了显著的进展，以减小模型尺寸和加速推断为目标，同时保持分类准确性。然而，许多研究观察到，仅关注总体准确性可能是误导的。例如，已经证明全模型和压缩模型之间的差异可能会偏向于在数据集中低频的类。这引出了一个重要的研究问题，即“我们能否在保持与原始网络语义等同的情况下实现网络压缩？”在本文中，我们研究了这个问题，重点关注计算机视觉数据集中Feldman等人观察到的“长尾”现象。他们认为，某些输入（适当定义）的记忆对于实现良好的泛化是必要的。由于压缩限制了网络的容量（因此也限制了其记忆能力），所以我们研究了这个问题：

    Network compression is now a mature sub-field of neural network research: over the last decade, significant progress has been made towards reducing the size of models and speeding up inference, while maintaining the classification accuracy. However, many works have observed that focusing on just the overall accuracy can be misguided. E.g., it has been shown that mismatches between the full and compressed models can be biased towards under-represented classes. This raises the important research question, \emph{can we achieve network compression while maintaining ``semantic equivalence'' with the original network?} In this work, we study this question in the context of the ``long tail'' phenomenon in computer vision datasets observed by Feldman, et al. They argue that \emph{memorization} of certain inputs (appropriately defined) is essential to achieving good generalization. As compression limits the capacity of a network (and hence also its ability to memorize), we study the question: a
    
[^32]: 基于分布式多智能体强化学习的异构交通意图感知规划算法iPLAN

    iPLAN: Intent-Aware Planning in Heterogeneous Traffic via Distributed Multi-Agent Reinforcement Learning. (arXiv:2306.06236v1 [cs.MA])

    [http://arxiv.org/abs/2306.06236](http://arxiv.org/abs/2306.06236)

    论文提出了基于MARL算法的iPLAN方法，可在高密度且异构交通场景下进行意图感知规划，使智能体能够从局部观测中推断附近驾驶者的意图，并通过行为或瞬时激励进行决策，实现自主导航。

    

    在高密度和异构交通场景中保障自动驾驶汽车（AVs）的安全和效率面临较大挑战，因为它们无法推断附近驾驶者的行为或意图。本文提出了一种具有轨迹和意图预测的分布式多智能体强化学习（MARL）算法，用于在高密度和异构交通场景中进行意图感知规划。我们的iPLAN方法使智能体仅从其本地观测中推断附近驾驶者的意图。我们模拟了两个不同的激励因素：行为激励用于智能体的长期规划，基于它们的驾驶行为或个性；瞬时激励用于智能体的短期规划，以基于当前交通状态进行碰撞避免。我们设计了一个双流推理模块，使智能体能够推断对手的激励并将其推断信息纳入决策。我们在两个模拟环境中进行了实验。

    Navigating safely and efficiently in dense and heterogeneous traffic scenarios is challenging for autonomous vehicles (AVs) due to their inability to infer the behaviors or intentions of nearby drivers. In this work, we propose a distributed multi-agent reinforcement learning (MARL) algorithm with trajectory and intent prediction in dense and heterogeneous traffic scenarios. Our approach for intent-aware planning, iPLAN, allows agents to infer nearby drivers' intents solely from their local observations. We model two distinct incentives for agents' strategies: Behavioral incentives for agents' long-term planning based on their driving behavior or personality; Instant incentives for agents' short-term planning for collision avoidance based on the current traffic state. We design a two-stream inference module that allows agents to infer their opponents' incentives and incorporate their inferred information into decision-making. We perform experiments on two simulation environments, Non-C
    
[^33]: AVScan2Vec：针对生产规模恶意软件库的杀毒扫描数据的特征学习

    AVScan2Vec: Feature Learning on Antivirus Scan Data for Production-Scale Malware Corpora. (arXiv:2306.06228v1 [cs.CR])

    [http://arxiv.org/abs/2306.06228](http://arxiv.org/abs/2306.06228)

    本文提出AVScan2Vec，一种针对杀毒扫描数据的语言模型，旨在将其作为恶意软件的可扩展特征源，以解决生产规模恶意软件库中文件数众多和计算复杂度高的问题。

    

    当调查恶意文件时，搜索相关文件是恶意分析员必须执行的常见任务。由于生产恶意软件库可能包含超过十亿个文件并消耗PB级的存储空间，许多特征提取和相似性搜索方法在计算上是不可行的。本文研究了将杀毒扫描数据作为恶意软件的可扩展特征源的潜力。这是可能的，因为AV扫描报告通过VirusTotal等服务广泛可用，并且比平均恶意软件样本小约100倍。 AV扫描报告中的信息充满了丰富的信息，可以指示恶意文件的家族、行为、目标操作系统和许多其他特征。我们介绍了AVScan2Vec，一种语言模型，可以理解AV扫描数据的语义。AVScan2Vec摄取用于恶意文件的AV扫描数据，并输出有意义的向量表示。AVScan2Vec向量比流行的恶意软件特征提取方法小约3到85倍。

    When investigating a malicious file, searching for related files is a common task that malware analysts must perform. Given that production malware corpora may contain over a billion files and consume petabytes of storage, many feature extraction and similarity search approaches are computationally infeasible. Our work explores the potential of antivirus (AV) scan data as a scalable source of features for malware. This is possible because AV scan reports are widely available through services such as VirusTotal and are ~100x smaller than the average malware sample. The information within an AV scan report is abundant with information and can indicate a malicious file's family, behavior, target operating system, and many other characteristics. We introduce AVScan2Vec, a language model trained to comprehend the semantics of AV scan data. AVScan2Vec ingests AV scan data for a malicious file and outputs a meaningful vector representation. AVScan2Vec vectors are ~3 to 85x smaller than popula
    
[^34]: 基于双参数边界支持向量机的多类分类鲁棒性模型

    Robust Twin Parametric Margin Support Vector Machine for Multiclass Classification. (arXiv:2306.06213v1 [cs.LG])

    [http://arxiv.org/abs/2306.06213](http://arxiv.org/abs/2306.06213)

    提出了双参数边界支持向量机模型来解决多类分类问题，并通过鲁棒优化技术使其更加鲁棒。初步实验结果表明其具有良好的性能。

    

    本文提出一种双参数边界支持向量机(TPMSVM)模型来解决多类分类问题。 对于每个类别，我们采用一对割平面的模式构建一个分类器。一旦确定了所有分类器，则将它们组合成一个综合的决策函数。我们考虑线性和非线性内核引起的分类器的情况。此外，我们通过鲁棒优化技术增强了所提出的方法的鲁棒性。 初步的计算实验表明了所提出的方法的良好性能。

    In this paper we present a Twin Parametric-Margin Support Vector Machine (TPMSVM) model to tackle the problem of multiclass classification. In the spirit of one-versus-all paradigm, for each class we construct a classifier by solving a TPMSVM-type model. Once all classifiers have been determined, they are combined into an aggregate decision function. We consider the cases of both linear and nonlinear kernel-induced classifiers. In addition, we robustify the proposed approach through robust optimization techniques. Indeed, in real-world applications observations are subject to measurement errors and noise, affecting the quality of the solutions. Consequently, data uncertainties need to be included within the model in order to prevent low accuracies in the classification process. Preliminary computational experiments on real-world datasets show the good performance of the proposed approach.
    
[^35]: 通过最终层反演进行单模型归因

    Single-Model Attribution via Final-Layer Inversion. (arXiv:2306.06210v1 [cs.CV])

    [http://arxiv.org/abs/2306.06210](http://arxiv.org/abs/2306.06210)

    本文提出了一种利用最终层反演和异常检测的开放式单模型归因方法，解决了以往方法要么局限于封闭式环境、要么需要对生成模型进行不必要的改变的问题。实验结果表明该方法优于现有方法。

    

    最近关于生成模型方面的开创性发展引起了人们对于实用单模型归因的兴趣。这些方法可以预测一个样本是由特定的生成器生成的还是不是，例如，为了证明知识产权盗窃行为。然而，以前的方法要么局限于封闭式环境，要么需要对生成模型进行不必要的改变。本文提出了FLIPAD，一种基于最终层反演和异常检测的开放式单模型归因方法，以解决这些问题。我们展示利用的最终层反演可以简化为一个凸的 Lasso 优化问题，从而使我们的方法在理论上可靠且计算效率高。理论结果还得到了实验研究的支持，证明本文方法的有效性，优于现有方法。

    Recent groundbreaking developments on generative modeling have sparked interest in practical single-model attribution. Such methods predict whether a sample was generated by a specific generator or not, for instance, to prove intellectual property theft. However, previous works are either limited to the closed-world setting or require undesirable changes of the generative model. We address these shortcomings by proposing FLIPAD, a new approach for single-model attribution in the open-world setting based on final-layer inversion and anomaly detection. We show that the utilized final-layer inversion can be reduced to a convex lasso optimization problem, making our approach theoretically sound and computationally efficient. The theoretical findings are accompanied by an experimental study demonstrating the effectiveness of our approach, outperforming the existing methods.
    
[^36]: 稀疏隐形触发器的后门攻击

    Backdoor Attack with Sparse and Invisible Trigger. (arXiv:2306.06209v1 [cs.CV])

    [http://arxiv.org/abs/2306.06209](http://arxiv.org/abs/2306.06209)

    本论文提出了一种名为SIBA的稀疏隐形后门攻击方法，解决了现有后门攻击存在的可见或稀疏性不足等问题。

    

    深度神经网络（DNN）容易受到后门攻击，攻击者在小部分训练数据中进行操作，使得受害的模型对正常样本有正确的预测，但是将带有触发器的样本归类为目标分类。后门攻击是一种新兴而又危险的训练阶段威胁，对DNN应用带来严重风险。本文研究了现有后门攻击的触发器模式，揭示了它们是否可见或稀疏性不足，因此不够隐秘。最重要的是，在设计有效的稀疏隐形后门攻击时，不能简单地将现有方法组合起来。为了解决这个问题，我们将触发器生成形式化为一个具有稀疏性和隐秘性约束的双层优化问题，并提出了一种有效的方法来解决它，称为稀疏隐形后门攻击（SIBA）。我们在基准数据集上进行了广泛的实验。

    Deep neural networks (DNNs) are vulnerable to backdoor attacks, where the adversary manipulates a small portion of training data such that the victim model predicts normally on the benign samples but classifies the triggered samples as the target class. The backdoor attack is an emerging yet threatening training-phase threat, leading to serious risks in DNN-based applications. In this paper, we revisit the trigger patterns of existing backdoor attacks. We reveal that they are either visible or not sparse and therefore are not stealthy enough. More importantly, it is not feasible to simply combine existing methods to design an effective sparse and invisible backdoor attack. To address this problem, we formulate the trigger generation as a bi-level optimization problem with sparsity and invisibility constraints and propose an effective method to solve it. The proposed method is dubbed sparse and invisible backdoor attack (SIBA). We conduct extensive experiments on benchmark datasets unde
    
[^37]: 一种用于评估图像识别模型鲁棒性的差分测试框架

    A Differential Testing Framework to Evaluate Image Recognition Model Robustness. (arXiv:2306.06208v1 [cs.CV])

    [http://arxiv.org/abs/2306.06208](http://arxiv.org/abs/2306.06208)

    本文提出了一种差分测试框架，用于评估图像识别模型鲁棒性。该框架通过使用一组参考图像并扰动计算环境，可确定模型性能是否受到计算环境变化的影响。

    

    图像识别任务通常使用深度学习，并需要巨大的处理能力，因此依赖于GPU和TPU等硬件加速器进行快速、及时的处理。在模型部署过程中，硬件加速器上的子优映射可能会导致实时图像识别任务失败，从而导致时间不确定性和错误行为。硬件加速器上的映射是通过多个软件组件进行的，例如深度学习框架、编译器、设备库等，我们称之为计算环境。随着图像识别任务在自动驾驶和医疗成像等安全关键应用中的增加，评估它们对计算环境变化的鲁棒性至关重要，因为深度学习框架、编译器优化和硬件设备等参数对模型性能和正确性的影响还不太清楚。在本文中，我们提出了一种差分测试框架，用于评估图像识别模型对计算环境变化的鲁棒性。我们的框架使用一组参考图像，并通过更改软件组件来扰动计算环境，生成具有已知预测输出差异的图像。通过比较原始图像和扰动图像的预测输出，我们可以确定模型性能是否受到计算环境变化的影响。我们通过测试三个图像识别模型的鲁棒性来证明我们框架的有效性，并确定其在计算环境变化下的性能受到影响的情况。

    Image recognition tasks typically use deep learning and require enormous processing power, thus relying on hardware accelerators like GPUs and TPUs for fast, timely processing. Failure in real-time image recognition tasks can occur due to sub-optimal mapping on hardware accelerators during model deployment, which may lead to timing uncertainty and erroneous behavior. Mapping on hardware accelerators is done through multiple software components like deep learning frameworks, compilers, device libraries, that we refer to as the computational environment. Owing to the increased use of image recognition tasks in safety-critical applications like autonomous driving and medical imaging, it is imperative to assess their robustness to changes in the computational environment, as the impact of parameters like deep learning frameworks, compiler optimizations, and hardware devices on model performance and correctness is not well understood.  In this paper we present a differential testing framewo
    
[^38]: “PotatoPestNet：一种基于CTInceptionV3-RS的神经网络，用于准确识别马铃薯害虫。”(arXiv：2306.06206v1 [cs.CV])

    PotatoPestNet: A CTInceptionV3-RS-Based Neural Network for Accurate Identification of Potato Pests. (arXiv:2306.06206v1 [cs.CV])

    [http://arxiv.org/abs/2306.06206](http://arxiv.org/abs/2306.06206)

    该论文提出了一种基于AI的自动马铃薯害虫识别系统PotatoPestNet，使用了八种马铃薯害虫的数据集和五种预训练转移学习模型进行模型训练，并利用随机搜索优化进行超参数调整，能够准确识别不同种类的马铃薯害虫。

    

    马铃薯是全球第三大食品作物，但由于侵袭性害虫的困扰，其产量经常遇到困难。本研究的目的是调查这些害虫的各种类型和特征，并提出一种高效的基于AI的自动马铃薯害虫识别系统PotatoPestNet。为了实现这一目标，我们筛选了包括八种马铃薯害虫的可靠数据集。我们利用五个经过定制的预训练转移学习模型：CMobileNetV2、CNASLargeNet、CXception、CDenseNet201和CInceptionV3的强大之处，提出了一个稳健的PotatoPestNet模型来准确分类马铃薯害虫。为了提高模型的性能，我们应用了各种数据增强技术，加入一个全局均值池化层，并实施适当的正则化方法。为了进一步提高模型的性能，我们利用随机搜索（RS）优化进行超参数调整。

    Potatoes are the third-largest food crop globally, but their production frequently encounters difficulties because of aggressive pest infestations. The aim of this study is to investigate the various types and characteristics of these pests and propose an efficient PotatoPestNet AI-based automatic potato pest identification system. To accomplish this, we curated a reliable dataset consisting of eight types of potato pests. We leveraged the power of transfer learning by employing five customized, pre-trained transfer learning models: CMobileNetV2, CNASLargeNet, CXception, CDenseNet201, and CInceptionV3, in proposing a robust PotatoPestNet model to accurately classify potato pests. To improve the models' performance, we applied various augmentation techniques, incorporated a global average pooling layer, and implemented proper regularization methods. To further enhance the performance of the models, we utilized random search (RS) optimization for hyperparameter tuning. This optimization 
    
[^39]: 特征级自监督学习方法FLSL

    FLSL: Feature-level Self-supervised Learning. (arXiv:2306.06203v1 [cs.LG])

    [http://arxiv.org/abs/2306.06203](http://arxiv.org/abs/2306.06203)

    本文提出FLSL方法，采用Transformer进行联合嵌入和聚类，适合于内视图和外视图特征聚类。实验证明该方法在语义类簇表达方面取得显著改进。

    

    当前的自监督学习方法（如SimCLR、DINO、VICReg、MOCOv3）主要针对实例级别的表示，不适用于密集预测任务，例如对象检测和分割。本文针对这个问题，首次展示了Vision Transformers（ViT）的基础均值漂移聚类过程能够良好地与自然图像语义（例如物体和场景）对齐。通过采用Transformer进行联合嵌入和聚类，我们提出了一种两级特征聚类的自监督学习方法，称为特征级自监督学习（FLSL）。我们提出了FLSL问题的正式定义，并从均值漂移和k-means的角度构建目标。实验证明，FLSL促进了显著的语义类簇表示，并学习了一种适合于内视图和外视图特征聚类的嵌入方案。FLSL的运用取得了显著改进。

    Current self-supervised learning (SSL) methods (e.g., SimCLR, DINO, VICReg, MOCOv3) target primarily on representations at instance level and do not generalize well to dense prediction tasks, such as object detection and segmentation. Towards aligning SSL with dense predictions, this paper demonstrates for the first time the underlying mean-shift clustering process of Vision Transformers (ViT), which aligns well with natural image semantics (e.g., a world of objects and stuffs). By employing transformer for joint embedding and clustering, we propose a two-level feature clustering SSL method, coined Feature-Level Self-supervised Learning (FLSL). We present the formal definition of the FLSL problem and construct the objectives from the mean-shift and k-means perspectives. We show that FLSL promotes remarkable semantic cluster representations and learns an embedding scheme amenable to intra-view and inter-view feature clustering. Experiments show that FLSL yields significant improvements 
    
[^40]: NeuroGraph:面向脑连接组学的图机器学习基准测试

    NeuroGraph: Benchmarks for Graph Machine Learning in Brain Connectomics. (arXiv:2306.06202v1 [cs.LG])

    [http://arxiv.org/abs/2306.06202](http://arxiv.org/abs/2306.06202)

    本文介绍了神经成像领域的图机器学习基准测试NeuroGraph，并探讨了数据集生成的搜索空间。

    

    机器学习为分析高维功能性神经成像数据提供了有价值的工具，已被证明对预测各种神经疾病、精神障碍和认知模式有效。在功能磁共振成像研究中，大脑区域之间的相互作用通常使用基于图的表示进行建模。图机器学习方法的有效性已在多个领域得到证实，标志着数据解释和预测建模中的一个转变步骤。然而，尽管有前景，但由于图形数据集构建的广泛预处理流水线和大参数搜索空间，在神经成像领域中应用这些技术的转换仍然受到意外的限制。本文介绍了NeuroGraph(一个基于图的神经成像数据集)，它涵盖了多个行为和认知特征类别。我们深入探讨了数据集生成搜索空间

    Machine learning provides a valuable tool for analyzing high-dimensional functional neuroimaging data, and is proving effective in predicting various neurological conditions, psychiatric disorders, and cognitive patterns. In functional Magnetic Resonance Imaging (MRI) research, interactions between brain regions are commonly modeled using graph-based representations. The potency of graph machine learning methods has been established across myriad domains, marking a transformative step in data interpretation and predictive modeling. Yet, despite their promise, the transposition of these techniques to the neuroimaging domain remains surprisingly under-explored due to the expansive preprocessing pipeline and large parameter search space for graph-based datasets construction. In this paper, we introduce NeuroGraph, a collection of graph-based neuroimaging datasets that span multiple categories of behavioral and cognitive traits. We delve deeply into the dataset generation search space by c
    
[^41]: 可靠性检查：对GPT-3在敏感话题和提示措辞方面的反应分析

    Reliability Check: An Analysis of GPT-3's Response to Sensitive Topics and Prompt Wording. (arXiv:2306.06199v1 [cs.CL])

    [http://arxiv.org/abs/2306.06199](http://arxiv.org/abs/2306.06199)

    本文分析了大型语言模型GPT-3对敏感话题和提示措辞的反应，发现其在阴谋论和刻板印象方面有正确的反应，但在误解和争议方面存在错误，并具有不可靠性。

    

    大型语言模型已成为主流技术，具有多种用途和出色的性能。尽管有无数的应用，但LLMs仍然不是可靠的。通过微调、提示和人类反馈的强化学习等方法，正在进行大量工作来提高这些模型的事实准确性、一致性和道德标准，但缺乏对这些模型对不同语句类别的反应或在简单提示变化下可能存在的漏洞的系统分析。在本研究中，我们分析了什么会让GPT-3困惑：模型如何响应某些敏感话题以及提示措辞对模型响应的影响。我们发现，GPT-3正确地反对明显的阴谋论和刻板印象，但在普遍的误解和争议中犯了错误。模型响应在提示和设置上不一致，突显出GPT-3的不可靠性。

    Large language models (LLMs) have become mainstream technology with their versatile use cases and impressive performance. Despite the countless out-of-the-box applications, LLMs are still not reliable. A lot of work is being done to improve the factual accuracy, consistency, and ethical standards of these models through fine-tuning, prompting, and Reinforcement Learning with Human Feedback (RLHF), but no systematic analysis of the responses of these models to different categories of statements, or on their potential vulnerabilities to simple prompting changes is available. In this work, we analyze what confuses GPT-3: how the model responds to certain sensitive topics and what effects the prompt wording has on the model response. We find that GPT-3 correctly disagrees with obvious Conspiracies and Stereotypes but makes mistakes with common Misconceptions and Controversies. The model responses are inconsistent across prompts and settings, highlighting GPT-3's unreliability. Dataset and 
    
[^42]: ElectroCardioGuard：通过神经网络防止心电图数据库中患者误识别

    ElectroCardioGuard: Preventing Patient Misidentification in Electrocardiogram Databases through Neural Networks. (arXiv:2306.06196v1 [cs.LG])

    [http://arxiv.org/abs/2306.06196](http://arxiv.org/abs/2306.06196)

    本研究提出了一种基于神经网络的小而高效模型，用于确定两个心电图(ECG)是否来自同一患者。同时，还提出了一种方法，利用该模型检测记录-分配错误，实现了在现实场景中的应用，并在新的ECG数据集上进行了评估。

    

    心电图(ECG)通常被心脏病专家用于检测与心脏相关的病理情况，而可靠的ECG集合对于确诊非常重要。然而，在临床实践中，将记录的ECG分配给错误的患者可能会不经意地发生。本文与一家临床和研究机构合作，该机构认识到这一挑战并联系我们，我们提出了一项研究来解决这个问题。我们提出了一种小巧高效的基于神经网络的模型，用于确定两个ECG是否来自同一患者。我们的模型展现了很强的泛化能力，并在利用760倍更少的参数的情况下，在PTB-XL上实现了最新的画廊探针患者识别表现。此外，我们提出了一种技术，利用我们的模型来检测记录-分配错误，展示了它在一个现实场景中的适用性。最后，我们对新收集的ECG数据集进行了评估。

    Electrocardiograms (ECGs) are commonly used by cardiologists to detect heart-related pathological conditions. Reliable collections of ECGs are crucial for precise diagnosis. However, in clinical practice, the assignment of captured ECG recordings to incorrect patients can occur inadvertently. In collaboration with a clinical and research facility which recognized this challenge and reached out to us, we present a study that addresses this issue. In this work, we propose a small and efficient neural-network based model for determining whether two ECGs originate from the same patient. Our model demonstrates great generalization capabilities and achieves state-of-the-art performance in gallery-probe patient identification on PTB-XL while utilizing 760x fewer parameters. Furthermore, we present a technique leveraging our model for detection of recording-assignment mistakes, showcasing its applicability in a realistic scenario. Finally, we evaluate our model on a newly collected ECG dataset
    
[^43]: 高度动态条件下的公共交通需求预测：现有模型的元分析和开源基准设施比较

    Public Transit Demand Prediction During Highly Dynamic Conditions: A Meta-Analysis of State-of-the-Art Models and Open-Source Benchmarking Infrastructure. (arXiv:2306.06194v1 [cs.LG])

    [http://arxiv.org/abs/2306.06194](http://arxiv.org/abs/2306.06194)

    本论文基于智能卡数据的时间序列预测哥伦比亚波哥大BRT系统隔日需求，建立了一个开源基础设施并评估了五种常见方法在稳定和高度动态条件下的表现，结果显示大多数测试模型的表现相似，MAAPE从0.08到0.12不等。

    

    实时需求预测是动态公交路线规划的关键输入。虽然许多研究人员已经开发了许多复杂的方法来预测短期公共交通需求，但应用范围仅限于短时间、稳定的时间范围和少数车站。这些方法在高度动态的环境中的表现还没有得到研究，也没有系统地进行比较。我们建立了一个开源基础设施，包括计量和深度学习方法，评估它们在稳定和高度动态条件下的表现。我们使用智能卡数据的时间序列预测哥伦比亚波哥大BRT系统在隔日的需求。时间序列的动态条件包括一个长达一个月的抗议和COVID-19大流行。这两个条件都引发了需求的巨大变化。结果表明，在稳定条件下，大多数测试模型的表现相似，MAAPE从0.08到0.12不等。 基准设施的

    Real-time demand prediction is a critical input for dynamic bus routing. While many researchers have developed numerous complex methods to predict short-term transit demand, the applications have been limited to short, stable time frames and a few stations. How these methods perform in highly dynamic environments has not been studied, nor has their performance been systematically compared. We built an open-source infrastructure with five common methodologies, including econometric and deep learning approaches, and assessed their performance under stable and highly dynamic conditions. We used a time series from smartcard data to predict demand for the following day for the BRT system in Bogota, Colombia. The dynamic conditions in the time series include a month-long protest and the COVID-19 pandemic. Both conditions triggered drastic shifts in demand. The results reveal that most tested models perform similarly in stable conditions, with MAAPE varying from 0.08 to 0.12. The benchmark de
    
[^44]: Ada-NAV：用于机器人导航的自适应轨迹优化策略学习方法

    Ada-NAV: Adaptive Trajectory-Based Sample Efficient Policy Learning for Robotic Navigation. (arXiv:2306.06192v1 [cs.RO])

    [http://arxiv.org/abs/2306.06192](http://arxiv.org/abs/2306.06192)

    Ada-NAV是一种自适应轨迹优化策略学习方法，采用降低策略随机性的方法平衡探索与利用，提高机器人导航任务的采样效率。在真实世界的测试中表现优异，可以在更短的采样时间内取得更高的性能。

    

    强化学习方法在学习机器人导航策略方面十分有效，但其采样效率低的问题也十分明显。在策略优化中，这种效率低下部分来自于未能适当地平衡探索与利用的问题，特别是在面对非静态时。为了加入探索与利用的平衡以提高采样效率，我们提出了Ada-NAV，一种自适应轨迹长度方案，其中长度随着策略的随机性（用其Shannon或差分熵表示）的减小而增加。我们的自适应轨迹长度方案由于更频繁的梯度更新强调了训练开始时的探索，后来则更强调利用。在网格世界，仿真机器人环境和真实世界机器人实验中，我们证明了该方法的优点，表现在性能和采样效率上均优于常数和随机采样的轨迹长度。在固定的样本预算下，相对于现有的基准方法，Ada-NAV的性能提高了高达46％，采样数量减少了高达80％。

    Reinforcement learning methods, while effective for learning robotic navigation strategies, are known to be highly sample inefficient. This sample inefficiency comes in part from not suitably balancing the explore-exploit dilemma, especially in the presence of non-stationarity, during policy optimization. To incorporate a balance of exploration-exploitation for sample efficiency, we propose Ada-NAV, an adaptive trajectory length scheme where the length grows as a policy's randomness, represented by its Shannon or differential entropy, decreases. Our adaptive trajectory length scheme emphasizes exploration at the beginning of training due to more frequent gradient updates and emphasizes exploitation later on with longer trajectories. In gridworld, simulated robotic environments, and real-world robotic experiments, we demonstrate the merits of the approach over constant and randomly sampled trajectory lengths in terms of performance and sample efficiency. For a fixed sample budget, Ada-N
    
[^45]: GitHub上的开放数据：释放人工智能的潜力

    Open Data on GitHub: Unlocking the Potential of AI. (arXiv:2306.06191v1 [cs.LG])

    [http://arxiv.org/abs/2306.06191](http://arxiv.org/abs/2306.06191)

    GitHub是全球最大的协作软件开发平台之一，托管了超过8亿个开放数据文件，共计142TB的数据；研究发现，在过去四年中其开放数据资产经历了加速增长，有助于加速人工智能研究，解决复杂社会问题。

    

    GitHub是全球最大的协作软件开发平台之一，拥有超过1亿用户，同时也被广泛用于开放数据协作，托管了超过8亿个开放数据文件，共计142TB的数据。本研究强调了GitHub上开放数据的潜力，并展示了如何加速人工智能研究。我们分析了GitHub上现有的开放数据和用户分享数据集的模式。我们的发现表明，GitHub是世界上最大的开放数据托管平台之一，并在过去四年中经历了开放数据资产的加速增长。通过对GitHub上开放数据的概述，我们旨在赋予用户和组织使用现有的开放式数据集并提高它们的可发现性，从而最终有助于解决复杂的社会问题。我们会将收集到的三个数据集作为开放数据发布在以下链接：https://gith

    GitHub is the world's largest platform for collaborative software development, with over 100 million users. GitHub is also used extensively for open data collaboration, hosting more than 800 million open data files, totaling 142 terabytes of data. This study highlights the potential of open data on GitHub and demonstrates how it can accelerate AI research. We analyze the existing landscape of open data on GitHub and the patterns of how users share datasets. Our findings show that GitHub is one of the largest hosts of open data in the world and has experienced an accelerated growth of open data assets over the past four years. By examining the open data landscape on GitHub, we aim to empower users and organizations to leverage existing open datasets and improve their discoverability -- ultimately contributing to the ongoing AI revolution to help address complex societal issues. We release the three datasets that we have collected to support this analysis as open datasets at https://gith
    
[^46]: 使用文档级元数据的领域特定快速预训练技术$FPDM$

    $FPDM$: Domain-Specific Fast Pre-training Technique using Document-Level Metadata. (arXiv:2306.06190v1 [cs.CL])

    [http://arxiv.org/abs/2306.06190](http://arxiv.org/abs/2306.06190)

    本文提出了$FPDM$，使用文档元数据和领域特定分类作为监督信号，对领域特定语料库进行transformer编码器的预训练。$FPDM$通过句子级别的输入预训练开放领域的编码器，在微调时使用词汇级别的输入，性能优于其他基于transformer的模型。

    

    在各种领域的预训练已显示出在开放领域和领域特定下游任务上具有良好的结果。然而，最先进的transformers需要大量的预训练数据和计算资源。在本文中，我们提出了$FPDM$（Fast Pre-training Technique using Document Level Metadata），这是一个新颖、计算效率高的框架，利用文档元数据和领域特定的分类作为监督信号，对领域特定语料库进行transformer编码器的预训练。最主要的创新在于，在领域特定的预训练过程中，使用句子级别的嵌入作为输入，持续对开放领域的编码器进行预训练（以适应长文档），但在对该编码器进行微调时，则使用词汇级别嵌入作为输入。实验表明，$FPDM$在客户支持、科学和法律等领域的字符级F1分数和其他自动化指标方面优于几种基于transformer的基准，且在下游任务微调后性能下降可以忽略不计。

    Pre-training Transformers has shown promising results on open-domain and domain-specific downstream tasks. However, state-of-the-art Transformers require an unreasonably large amount of pre-training data and compute. In this paper, we propose $FPDM$ (Fast Pre-training Technique using Document Level Metadata), a novel, compute-efficient framework that utilizes Document metadata and Domain-Specific Taxonomy as supervision signals to pre-train transformer encoder on a domain-specific corpus. The main innovation is that during domain-specific pretraining, an open-domain encoder is continually pre-trained using sentence-level embeddings as inputs (to accommodate long documents), however, fine-tuning is done with token-level embeddings as inputs to this encoder. We show that $FPDM$ outperforms several transformer-based baselines in terms of character-level F1 scores and other automated metrics in the Customer Support, Scientific, and Legal Domains, and shows a negligible drop in performance 
    
[^47]: FasterViT：具有分层注意力的快速视觉变换器

    FasterViT: Fast Vision Transformers with Hierarchical Attention. (arXiv:2306.06189v1 [cs.CV])

    [http://arxiv.org/abs/2306.06189](http://arxiv.org/abs/2306.06189)

    本研究设计了一种新型混合CNN-ViT神经网络FasterViT，引入了具有分层注意力的方法HAT，将全局自我注意力分解为多级注意力，实现了高效的跨窗口通信。 FasterViT在精度和图像吞吐量方面达到了SOTA前沿水平，并已在分类，物体检测和分割等CV任务中得到广泛验证。

    

    本研究设计了一种新型混合CNN-ViT神经网络，命名为FasterViT，专注于计算机视觉应用中的高图像吞吐量。FasterViT将CNN中快速本地表示学习的优点与ViT中的全局建模特性相结合。我们引入了分层注意力（HAT）方法，将具有二次复杂度的全局自我注意力分解为具有较小计算成本的多级注意力。我们受益于高效的基于窗口的自我注意力，每个窗口都可以访问专门用于本地和全局表示学习的专用载体令牌。在高层次上，全局自我注意力实现了较低成本的跨窗口通信，FasterViT在精度与图像吞吐量方面实现了SOTA前沿水平，并已在各种CV任务中进行了广泛验证，包括分类，物体检测和分割。我们还展示了HAT可以用作现有CNN架构的即插即用模块，以改善性能。

    We design a new family of hybrid CNN-ViT neural networks, named FasterViT, with a focus on high image throughput for computer vision (CV) applications. FasterViT combines the benefits of fast local representation learning in CNNs and global modeling properties in ViT. Our newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs. We benefit from efficient window-based self-attention. Each window has access to dedicated carrier tokens that participate in local and global representation learning. At a high level, global self-attentions enable the efficient cross-window communication at lower costs. FasterViT achieves a SOTA Pareto-front in terms of accuracy \vs image throughput. We have extensively validated its effectiveness on various CV tasks including classification, object detection and segmentation. We also show that HAT can be used as a plug-and-play module for exi
    
[^48]: 交互式估计的统一模型和维度

    A Unified Model and Dimension for Interactive Estimation. (arXiv:2306.06184v1 [cs.LG])

    [http://arxiv.org/abs/2306.06184](http://arxiv.org/abs/2306.06184)

    该论文提出了交互式估计的统一学习框架，引入组合度量差异维度来捕捉模型的可学习性，提出了多项式的遗憾和PAC泛化界限的算法，并统一了统计查询学习和结构化赌博问题。

    

    我们研究了一种称为"交互式估计"的抽象学习框架，其目标是根据学习者查询到的样本点与目标之间的"相似性"来估计目标。我们引入了一种称为差异维度的组合度量，它在很大程度上捕捉了我们模型中的可学习性。我们提出了一个简单、通用且广泛适用的算法，对于这个算法，我们获得了在新的维度上多项式的遗憾和PAC泛化界限。我们表明我们的框架包含了并因此统一了两个经典的学习模型：统计查询学习和结构化赌博问题。我们还说明了差异维度如何与这两个框架中的众所周知的参数相关，有时可以获得显著改进的分析。

    We study an abstract framework for interactive learning called interactive estimation in which the goal is to estimate a target from its "similarity'' to points queried by the learner. We introduce a combinatorial measure called dissimilarity dimension which largely captures learnability in our model. We present a simple, general, and broadly-applicable algorithm, for which we obtain both regret and PAC generalization bounds that are polynomial in the new dimension. We show that our framework subsumes and thereby unifies two classic learning models: statistical-query learning and structured bandits. We also delineate how the dissimilarity dimension is related to well-known parameters for both frameworks, in some cases yielding significantly improved analyses.
    
[^49]: ReLU网络的隐藏对称性

    Hidden symmetries of ReLU networks. (arXiv:2306.06179v1 [cs.LG])

    [http://arxiv.org/abs/2306.06179](http://arxiv.org/abs/2306.06179)

    研究发现，对于前馈ReLU神经网络的任何固定架构，在没有比输入层更窄的层的情况下，存在没有隐藏对称性的参数设置。此外，随着深度的增加，网络没有隐藏对称性的概率逐渐降低到0。

    

    对于前馈ReLU神经网络的任何固定架构，参数空间在训练期间用作相关函数类的代理 - 但是这种表示有多真实呢？已知许多不同的参数设置可以确定相同的函数。此外，这种冗余的程度是不均匀的：对于某些网络，唯一的对称性是层中神经元的排列和神经元参数的正比例缩放，而其他网络则具有其他隐藏对称性。 在这项工作中，我们证明了对于没有比输入层更窄的层的网络架构，存在没有隐藏对称性的参数设置。 我们还描述了一些机制，通过这些机制隐藏的对称性会出现，并在初始化时经验地近似不同网络架构的功能维度。这些实验表明，随着深度的增加，网络没有隐藏对称性的概率逐渐降低到0。

    The parameter space for any fixed architecture of feedforward ReLU neural networks serves as a proxy during training for the associated class of functions - but how faithful is this representation? It is known that many different parameter settings can determine the same function. Moreover, the degree of this redundancy is inhomogeneous: for some networks, the only symmetries are permutation of neurons in a layer and positive scaling of parameters at a neuron, while other networks admit additional hidden symmetries. In this work, we prove that, for any network architecture where no layer is narrower than the input, there exist parameter settings with no hidden symmetries. We also describe a number of mechanisms through which hidden symmetries can arise, and empirically approximate the functional dimension of different network architectures at initialization. These experiments indicate that the probability that a network has no hidden symmetries decreases towards 0 as depth increases, w
    
[^50]: 基于主动学习的代理建模技术优化参数非线性系统的模拟效率

    Active-Learning-Driven Surrogate Modeling for Efficient Simulation of Parametric Nonlinear Systems. (arXiv:2306.06174v1 [cs.LG])

    [http://arxiv.org/abs/2306.06174](http://arxiv.org/abs/2306.06174)

    本论文提出了一种基于主动学习的优化准则来有效构建参数化降阶代理模型，称为ActLearn-POD-KSNN代理模型，并通过数值实验验证了其有效性。

    

    当需要对高保真物理模型的不同参数配置进行重复评估时，基于模型阶段降低的代理建模技术是理想的选择。在缺乏描述动态的控制方程的情况下，我们需要以非侵入式的方式构建参数化降阶代理模型。在这种情况下，传统的基于残差的误差估计与参数采样方法不再适用。本文提出了一种基于主动学习的优化准则来有效地构建代理模型。我们考虑分离参数特定的适当正交分解子空间，并提出了一种使用基于核的浅层神经网络的主动学习驱动代理模型，简称ActLearn-POD-KSNN代理模型。为了证明我们所提出的方法的有效性，我们进行了数值实验。

    When repeated evaluations for varying parameter configurations of a high-fidelity physical model are required, surrogate modeling techniques based on model order reduction are desired. In absence of the governing equations describing the dynamics, we need to construct the parametric reduced-order surrogate model in a non-intrusive fashion. In this setting, the usual residual-based error estimate for optimal parameter sampling associated with the reduced basis method is not directly available. Our work provides a non-intrusive optimality criterion to efficiently populate the parameter snapshots, thereby, enabling us to effectively construct a parametric surrogate model. We consider separate parameter-specific proper orthogonal decomposition (POD) subspaces and propose an active-learning-driven surrogate model using kernel-based shallow neural networks, abbreviated as ActLearn-POD-KSNN surrogate model. To demonstrate the validity of our proposed ideas, we present numerical experiments us
    
[^51]: 图像识别模型框架转换的故障定位

    Fault Localization for Framework Conversions of Image Recognition Models. (arXiv:2306.06157v1 [cs.CV])

    [http://arxiv.org/abs/2306.06157](http://arxiv.org/abs/2306.06157)

    本文提出针对深度学习框架转换中出现的模型崩溃和输出标签差异的故障定位和修复方法，成功修复多个图像识别模型跨多个深度学习框架的转换错误。

    

    在部署深度神经网络（DNNs）时，开发人员经常将模型从一个深度学习框架转换为另一个（例如，从TensorFlow到PyTorch）。然而，这个过程容易出错，并可能影响目标模型的准确性。为了确定这种影响的程度，我们对三个用于图像识别的DNNs（MobileNetV2、ResNet101和InceptionV3）进行了不同的分析，这些模型在四个深度学习框架（PyTorch、Keras、TensorFlow（TF）和TFLite）之间进行了转换，并发现了许多模型崩溃和输出标签差异高达100％。为了缓解这种错误，我们提出了一种新的方法来定位故障和修复有缺陷的深度学习框架转换，重点放在预训练的图像识别模型上。我们的技术包括四个主要分析阶段：1）转换工具，2）模型参数，3）模型超参数，4）图表示。此外，我们提出了许多针对故障定位和修复的策略，包括转换工具的推荐、调试技巧以及模型超参数的微调。我们通过成功修复所有测试的深度学习框架中MobileNetV2，ResNet101和InceptionV3 的有缺陷的转换来展示我们方法的有效性。

    When deploying Deep Neural Networks (DNNs), developers often convert models from one deep learning framework to another (e.g., TensorFlow to PyTorch). However, this process is error-prone and can impact target model accuracy. To identify the extent of such impact, we perform and briefly present a differential analysis against three DNNs used for image recognition (MobileNetV2, ResNet101, and InceptionV3), converted across four well-known deep learning frameworks (PyTorch, Keras, TensorFlow (TF), and TFLite), which revealed numerous model crashes and output label discrepancies of up to 100%. To mitigate such errors, we present a novel approach towards fault localization and repair of buggy deep learning framework conversions, focusing on pre-trained image recognition models. Our technique consists of four primary stages of analysis: 1) conversion tools, 2) model parameters, 3) model hyperparameters, and 4) graph representation. In addition, we propose a number of strategies towards faul
    
[^52]: PoET: 一种将蛋白质家族看作序列的生成模型

    PoET: A generative model of protein families as sequences-of-sequences. (arXiv:2306.06156v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.06156](http://arxiv.org/abs/2306.06156)

    PoET是一个模型，能够生成任何蛋白质家族的一系列相关蛋白质序列，可以用作检索增强语言模型生成和评分任何修改

    

    生成式蛋白质语言模型是设计具有所需功能的新蛋白质的自然方法。然而，当前的模型要么难以指导其生成特定类型的蛋白质，要么必须在特定类型的蛋白质家族的大型多重序列比对上进行训练，这使得它们无法从家族之间的迁移学习中受益。为了解决这个问题，我们提出了蛋白质进化变换器（PoET），这是一种全蛋白质家族自回归生成模型，学习在数千万个天然蛋白质序列簇之间生成一系列相关的蛋白质序列。PoET可以作为一个检索增强语言模型，在任何感兴趣的蛋白质家族条件下生成和评分任意修改，而且可以从短序列长度进行外推，在小家族中也能很好地泛化。这是通过独特的Transformer层实现的；我们模拟了令牌s

    Generative protein language models are a natural way to design new proteins with desired functions. However, current models are either difficult to direct to produce a protein from a specific family of interest, or must be trained on a large multiple sequence alignment (MSA) from the specific family of interest, making them unable to benefit from transfer learning across families. To address this, we propose $\textbf{P}$r$\textbf{o}$tein $\textbf{E}$volutionary $\textbf{T}$ransformer (PoET), an autoregressive generative model of whole protein families that learns to generate sets of related proteins as sequences-of-sequences across tens of millions of natural protein sequence clusters. PoET can be used as a retrieval-augmented language model to generate and score arbitrary modifications conditioned on any protein family of interest, and can extrapolate from short context lengths to generalize well even for small families. This is enabled by a unique Transformer layer; we model tokens s
    
[^53]: 强度轮廓投影：用于动态网络的连续时间表示学习框架。

    Intensity Profile Projection: A Framework for Continuous-Time Representation Learning for Dynamic Networks. (arXiv:2306.06155v1 [cs.LG])

    [http://arxiv.org/abs/2306.06155](http://arxiv.org/abs/2306.06155)

    本文提出了一种连续时间网络表示学习框架，涵盖核平滑的强度函数估计、最小化强度重构误差的投影学习和归纳构造节点表示。这种表示保留了网络结构和时间一致性。

    

    我们提出了一种名为“强度轮廓投影”的新算法框架，用于学习动态网络节点的连续时间表示，该动态网络由节点集和在连续时间内发生的瞬时交互事件的集合所特征化。我们的框架包括三个阶段：通过核平滑等方法估计节点对之间交互的强度函数；学习一个最小化某种强度重构误差的投影；通过学习的投影归纳构造出不断发展的节点表示。我们展示了我们的表示保留了网络的基本结构，并具有时间一致性，这意味着节点表示可以在不同的时间点上进行有意义的比较。同时，我们也构建了估计理论来阐明平滑作为偏差方差折衷的作用，并展示了如何随着信噪比的增加而减少平滑程度以获得更好的性能。

    We present a new algorithmic framework, Intensity Profile Projection, for learning continuous-time representations of the nodes of a dynamic network, characterised by a node set and a collection of instantaneous interaction events which occur in continuous time. Our framework consists of three stages: estimating the intensity functions underlying the interactions between pairs of nodes, e.g. via kernel smoothing; learning a projection which minimises a notion of intensity reconstruction error; and inductively constructing evolving node representations via the learned projection. We show that our representations preserve the underlying structure of the network, and are temporally coherent, meaning that node representations can be meaningfully compared at different points in time. We develop estimation theory which elucidates the role of smoothing as a bias-variance trade-off, and shows how we can reduce smoothing as the signal-to-noise ratio increases on account of the algorithm `borrow
    
[^54]: HypLL: 希亚空间深度学习库

    HypLL: The Hyperbolic Learning Library. (arXiv:2306.06154v1 [cs.LG])

    [http://arxiv.org/abs/2306.06154](http://arxiv.org/abs/2306.06154)

    HypLL是一个使用希亚空间的深度学习库，基于PyTorch，旨在使其易于使用，搭建希亚网络模块，特别适用于处理层次化数据和使用少量嵌入维度，是一种新的、开放的研究方向。

    

    在机器学习、多媒体和计算机视觉等领域，希亚空间深度学习正迅速引起关注。深度网络通常在欧几里得空间中运行，隐含地假设数据在规则网格上。最近的研究表明，当处理层次化数据和使用少量嵌入维度时，希亚几何提供了一个可行的深度学习基础。然而，目前没有可访问的开源库用于构建类似于众所周知的深度学习库的希亚网络模块。我们提出了HypLL, 即希亚空间深度学习库，以将希亚深度学习的进展聚集在一起。HypLL建立在PyTorch之上，特别强调其易用性设计，以吸引广泛的受众关注这个新的和开放的研究方向。代码可在以下网址找到：https://github.com/maxvanspengler/hyperbolic_learning_library。压缩文件可在以下网址找到：https://d

    Deep learning in hyperbolic space is quickly gaining traction in the fields of machine learning, multimedia, and computer vision. Deep networks commonly operate in Euclidean space, implicitly assuming that data lies on regular grids. Recent advances have shown that hyperbolic geometry provides a viable alternative foundation for deep learning, especially when data is hierarchical in nature and when working with few embedding dimensions. Currently however, no accessible open-source library exists to build hyperbolic network modules akin to well-known deep learning libraries. We present HypLL, the Hyperbolic Learning Library to bring the progress on hyperbolic deep learning together. HypLL is built on top of PyTorch, with an emphasis in its design for easy-of-use, in order to attract a broad audience towards this new and open-ended research direction. The code is available at: https://github.com/maxvanspengler/hyperbolic_learning_library. The compressed archive is available at: https://d
    
[^55]: EfficientBioAI：使生物成像AI模型在能量、延迟和表示方面更高效

    EfficientBioAI: Making Bioimaging AI Models Efficient in Energy, Latency and Representation. (arXiv:2306.06152v1 [cs.LG])

    [http://arxiv.org/abs/2306.06152](http://arxiv.org/abs/2306.06152)

    EfficientBioAI是一个即插即用的工具箱，可以压缩生物成像AI模型，使它们在CPU和GPU上运行时能够显著减少能源成本和推理时间，而不会影响准确性。

    

    如今，人工智能（AI）在生物成像分析中被广泛使用，但由于模型规模和复杂性的增长以及现代生物医学研究中需要快速增长的分析需求，AI模型的效率，如能量消耗和延迟，是不可忽视的。就像我们可以压缩大型图像以实现高效存储和共享一样，我们也可以压缩AI模型以实现高效应用和部署。在这项工作中，我们提出了EfficientBioAI，这是一个即插即用的工具箱，可以压缩给定的生物成像AI模型，使它们在CPU和GPU上运行时能够显著减少能源成本和推理时间，而不会影响准确性。在某些情况下，压缩后的预测准确率甚至可能会增加，因为压缩过程可以去除模型表示中的冗余信息，从而减少过度拟合。从四种不同的生物图像分析应用中，我们观察到推理过程中的速度提高了约2-5倍，在GPU上提高了约3-10倍，并且准确性始终得到保证。

    Artificial intelligence (AI) has been widely used in bioimage image analysis nowadays, but the efficiency of AI models, like the energy consumption and latency is not ignorable due to the growing model size and complexity, as well as the fast-growing analysis needs in modern biomedical studies. Like we can compress large images for efficient storage and sharing, we can also compress the AI models for efficient applications and deployment. In this work, we present EfficientBioAI, a plug-and-play toolbox that can compress given bioimaging AI models for them to run with significantly reduced energy cost and inference time on both CPU and GPU, without compromise on accuracy. In some cases, the prediction accuracy could even increase after compression, since the compression procedure could remove redundant information in the model representation and therefore reduce over-fitting. From four different bioimage analysis applications, we observed around 2-5 times speed-up during inference and 3
    
[^56]: 人工智能与辐射防护：革命性还是更新换代？(arXiv:2306.06148v1 [cs.LG])

    Artificial intelligence and radiation protection. A game changer or an update?. (arXiv:2306.06148v1 [cs.LG])

    [http://arxiv.org/abs/2306.06148](http://arxiv.org/abs/2306.06148)

    本文介绍了基于机器学习的辐射防护方法，探讨了人工智能在辐射防护中的潜在优势和障碍，并提出了促进科学技术成果的合作方式。

    

    人工智能被认为是本世纪最颠覆性的技术之一，具有无数的应用。那么它对辐射防护意味着什么？本文介绍了基于机器学习（ML）的方法的基本原理，并介绍了在不同领域的辐射防护中的首次应用。预计人工智能在辐射防护中的使用将越来越广泛。因此，本文探讨了一些优点和潜在的障碍和问题，包括伦理问题。作者建议辐射防护专业人员与数据科学家专家合作，加速并指导算法的开发，以获得有效的科学技术成果。

    Artificial intelligence (AI) is regarded as one of the most disruptive technology of the century and with countless applications. What does it mean for radiation protection? This article describes the fundamentals of machine learning (ML) based methods and presents the inaugural applications in different fields of radiation protection. It is foreseen that the usage of AI will increase in radiation protection. Consequently, this article explores some of the benefits and also the potential barriers and questions, including ethical ones, that can come out. The article proposes that collaboration between radiation protection professionals and data scientist experts can accelerate and guide the development of the algorithms for effective scientific and technological outcomes.
    
[^57]: 隐藏分类层：关于数据隐藏表示中更高线性可分性的研究

    Hidden Classification Layers: a study on Data Hidden Representations with a Higher Degree of Linear Separability between the Classes. (arXiv:2306.06146v1 [cs.LG])

    [http://arxiv.org/abs/2306.06146](http://arxiv.org/abs/2306.06146)

    本文中，研究了一种新颖的培训方法影响深层网络分类器性能，并提出了一个新的神经网络架构，在数据隐藏表示中达到更高的线性可分性。

    

    在分类问题的背景下，深度学习（DL）方法代表了最先进的技术。许多深度学习方法都基于标准的多层前馈神经网络的变种。这些也被称为深度网络。基本思想是每个隐藏神经层完成一种数据转换，预期使数据表示“比之前更线性可分”，以获得尽可能线性可分的最终数据表示。然而，确定可以执行这些转换的适当神经网络参数是一个关键问题。在本文中，我们研究了一种培训方法对深层网络分类器性能的影响，这种方法倾向于使用标准方法相比，隐藏层的数据表示具有更高的类之间线性可分性。为此，我们提出了一个神经网络架构，该架构引入了一个涉及误差函数的新颖培训方法。

    In the context of classification problems, Deep Learning (DL) approaches represent state of art. Many DL approaches are based on variations of standard multi-layer feed-forward neural networks. These are also referred to as deep networks. The basic idea is that each hidden neural layer accomplishes a data transformation which is expected to make the data representation "somewhat more linearly separable" than the previous one to obtain a final data representation which is as linearly separable as possible. However, determining the appropriate neural network parameters that can perform these transformations is a critical problem. In this paper, we investigate the impact on deep network classifier performances of a training approach favouring solutions where data representations at the hidden layers have a higher degree of linear separability between the classes with respect to standard methods. To this aim, we propose a neural network architecture which induces an error function involvin
    
[^58]: LDMRes-Net：通过高效图像分割实现实时疾病监测

    LDMRes-Net: Enabling Real-Time Disease Monitoring through Efficient Image Segmentation. (arXiv:2306.06145v1 [eess.IV])

    [http://arxiv.org/abs/2306.06145](http://arxiv.org/abs/2306.06145)

    LDMRes-Net是一种轻量级高效的网络，通过采用双重多重残差连接来提高分割性能并最小化计算成本，可用于实时视网膜图像分析任务，并在8个公开数据集上取得了有希望的分割结果。

    

    视网膜眼病如果不及早诊断和治疗，可能导致双眼不可逆的视力损失。由于视网膜疾病的复杂性，视网膜图像可能包含两个或更多异常。目前用于分割带有多个标签和特征的视网膜图像的深度学习算法存在检测准确性不足和泛化性不强的问题。本文提出了一种轻量级高效的网络，采用双重多重残差连接增强分割性能并最小化计算成本。所提出的网络在8个公开可用的视网膜图像数据集上进行了评估，并取得了有希望的分割结果，证明了所提出的网络对于视网膜图像分析任务的有效性。所提出的网络的轻量级高效设计使其成为实时视网膜图像分析应用的有希望的候选者。

    Retinal eye diseases can lead to irreversible vision loss in both eyes if not diagnosed and treated earlier. Owing to the complexities of retinal diseases, the likelihood that retinal images would contain two or more abnormalities is very high. The current deep learning algorithms used for segmenting retinal images with multiple labels and features suffer from inadequate detection accuracy and a lack of generalizability. In this paper, we propose a lightweight and efficient network, featuring dual multi-residual connections to enhance segmentation performance while minimizing computational cost. The proposed network is evaluated on eight publicly available retinal image datasets and achieved promising segmentation results, which demonstrate the effectiveness of the proposed network for retinal image analysis tasks. The proposed network's lightweight and efficient design makes it a promising candidate for real-time retinal image analysis applications.
    
[^59]: Null/No Information Rate (NIR)：用于评估给定问题的分类准确性是否显著的统计检验

    Null/No Information Rate (NIR): a statistical test to assess if a classification accuracy is significant for a given problem. (arXiv:2306.06140v1 [stat.ME])

    [http://arxiv.org/abs/2306.06140](http://arxiv.org/abs/2306.06140)

    该论文介绍了一种被称为NIR的统计检验方法，可以用来评估分类系统的性能是否具有统计显著性。

    

    在许多研究背景下，特别是在生物医学领域，经过研究和开发分类系统后，自然会产生一个问题：“这个准确度是否足够高？”或者更好的问题是，“我们能否以统计学显着的置信度来说，我们的分类系统能够解决这个问题？”为了回答这个问题，我们可以使用本文描述的统计检验，有时称为NIR（无信息率或零值信息率）。

    In many research contexts, especially in the biomedical field, after studying and developing a classification system a natural question arises: "Is this accuracy enough high?", or better, "Can we say, with a statistically significant confidence, that our classification system is able to solve the problem"? To answer to this question, we can use the statistical test described in this paper, which is referred in some cases as NIR (No Information Rate or Null Information Rate).
    
[^60]: WePaMaDM-Outlier Detection: 使用模式方法进行加权异常值检测的大数据挖掘方法

    WePaMaDM-Outlier Detection: Weighted Outlier Detection using Pattern Approaches for Mass Data Mining. (arXiv:2306.06139v1 [cs.LG])

    [http://arxiv.org/abs/2306.06139](http://arxiv.org/abs/2306.06139)

    本文提出了一种使用模式方法进行加权异常值检测的大数据挖掘方法，可揭示系统故障、欺诈活动和数据模式的重要信息。

    

    加权异常值检测是一种识别数据集中异常或异常数据点的方法，可能由各种因素如人为错误、欺诈或设备故障引起。检测异常值可以揭示有关系统故障、欺诈活动和数据模式的重要信息，帮助专家解决这些异常的根本原因。然而，创建正常数据模式的模型以识别异常值可能由于输入数据的性质、标记数据的可用性以及问题的特定要求而具有挑战性。本文针对特定的大规模数据挖掘领域提出了WePaMaDM-Outlier Detection方法，证明这样的技术是依赖于特定领域并通常针对特定问题制定的。然而，类似的领域可以进行修改以适应解决方案。本研究还研究了数据建模在监控、故障检测和趋势分析中的异常点检测技术的重要性。

    Weighted Outlier Detection is a method for identifying unusual or anomalous data points in a dataset, which can be caused by various factors like human error, fraud, or equipment malfunctions. Detecting outliers can reveal vital information about system faults, fraudulent activities, and patterns in the data, assisting experts in addressing the root causes of these anomalies. However,creating a model of normal data patterns to identify outliers can be challenging due to the nature of input data, labeled data availability, and specific requirements of the problem. This article proposed the WePaMaDM-Outlier Detection with distinct mass data mining domain, demonstrating that such techniques are domain-dependent and usually developed for specific problem formulations. Nevertheless, similar domains can adapt solutions with modifications. This work also investigates the significance of data modeling in outlier detection techniques in surveillance, fault detection, and trend analysis, also re
    
[^61]: 利用扩散模型提取和恢复潜在动力学对齐的时空结构

    Extraction and Recovery of Spatio-Temporal Structure in Latent Dynamics Alignment with Diffusion Model. (arXiv:2306.06138v1 [q-bio.NC])

    [http://arxiv.org/abs/2306.06138](http://arxiv.org/abs/2306.06138)

    该论文提出了一种利用扩散模型提取和恢复潜在动力学对齐的时空结构的方法，以解决现有方法忽略潜在动力学时空结构导致对齐后性能质量较差的问题。

    

    在行为相关的脑计算领域，有必要将原始神经群体活动与它们之间的剧烈变化有意义地对齐。然而，由于大多数神经群体活动都以多变量时间序列的方式出现，因此对齐是非常棘手的。神经科学研究中的一个工具性框架认为，基于试验的神经群体活动依赖于低维度潜在动力学。关注这种潜在动力学大大促进了对齐过程。尽管我们取得了相当大的进展，但现有方法通常忽略了潜在动力学中固有的时空结构。因此，在对齐后导致动力学结构和整体性能质量较差。为了解决这个问题，我们提出一种利用扩散模型表达能力的方法来解决这些问题。具体而言，我们利用扩散模型首先提取源域的潜在动力学结构，然后在对齐时进行恢复。

    In the field of behavior-related brain computation, it is necessary to meaningfully align raw neural population activities against the drastic shift between them. However, the alignment is non-trivial since most neural population activities are in a multivariate time-series manner. An instrumental framework within neuroscience research posits that trial-based neural population activities rely on low-dimensional latent dynamics. Focusing on such latent dynamics greatly facilitates the alignment procedure. Despite the considerable progress we have reached, existing methods usually ignore the intrinsic spatio-temporal structures within latent dynamics. Thus, those solutions lead to poor quality in dynamics structures and overall performance after alignment. To tackle this problem, we propose a method leveraging the expressiveness of diffusion model to relieve such issues. Specifically, the latent dynamics structures of the source domain are first extracted by the diffusion model. Then, su
    
[^62]: 多智能体强化学习的鲁棒性测试：对关键智能体进行状态扰动

    Robustness Testing for Multi-Agent Reinforcement Learning: State Perturbations on Critical Agents. (arXiv:2306.06136v1 [cs.LG])

    [http://arxiv.org/abs/2306.06136](http://arxiv.org/abs/2306.06136)

    提出了一个新的多智能体强化学习鲁棒性测试框架RTCA，采用基于差分进化的关键智能体攻击方法，关键智能体通过团队合作政策评估方法被选取为受害者。该框架在鲁棒性测试中表现出优异的性能。

    

    多智能体强化学习（MARL）已经广泛应用于智能交通和无人机等许多领域。然而，大多数MARL算法容易受到智能体状态的对抗性扰动。对训练模型进行鲁棒性测试是确认模型在面对意外扰动时可信赖的基本步骤。本文提出一种针对MARL的新型鲁棒性测试框架，以攻击关键智能体的状态（RTCA）。RTCA有两个创新点：1）基于差分进化（DE）的方法选定关键的智能体作为受害者，并为它们提供最坏情况的联合动作建议；2）采用团队合作政策评估方法作为DE优化的目标函数。然后，基于最坏情况的联合动作生成关键智能体的对抗性状态扰动。这是第一个具有不同受害者智能体的鲁棒性测试框架。RTCA在鲁棒性测试中表现出了出色的性能。

    Multi-Agent Reinforcement Learning (MARL) has been widely applied in many fields such as smart traffic and unmanned aerial vehicles. However, most MARL algorithms are vulnerable to adversarial perturbations on agent states. Robustness testing for a trained model is an essential step for confirming the trustworthiness of the model against unexpected perturbations. This work proposes a novel Robustness Testing framework for MARL that attacks states of Critical Agents (RTCA). The RTCA has two innovations: 1) a Differential Evolution (DE) based method to select critical agents as victims and to advise the worst-case joint actions on them; and 2) a team cooperation policy evaluation method employed as the objective function for the optimization of DE. Then, adversarial state perturbations of the critical agents are generated based on the worst-case joint actions. This is the first robustness testing framework with varying victim agents. RTCA demonstrates outstanding performance in terms of 
    
[^63]: 生成模型中的内容审核安全与公平性

    Safety and Fairness for Content Moderation in Generative Models. (arXiv:2306.06135v1 [cs.LG])

    [http://arxiv.org/abs/2306.06135](http://arxiv.org/abs/2306.06135)

    生成模型在训练数据中模仿最糟糕的内容，通过安全输入和输出过滤器实现负责任部署；通过安全、公平和指标公平的定义，列举了每个领域可能遇到的例子损害，并提供了损害量化的演示。

    

    随着生成人工智能的显著进步，新技术正迅速部署到生成组件中。生成模型通常是在大型数据集上进行训练，导致其行为可能模仿训练数据中最糟糕的内容。负责任地部署生成技术需要内容审核策略，例如安全输入和输出过滤器。在此，我们提供一个理论框架，用于概念化文本到图像生成技术的负责任内容审核，包括如何实证地衡量我们列举的构造。我们定义和区分了安全、公平和指标公平的概念，并列举了每个领域可能出现的例子损害。然后，我们提供了如何量化定义的损害的演示。最后，我们总结了我们演示的损害量化风格如何实现数据驱动的内容审核决策。

    With significant advances in generative AI, new technologies are rapidly being deployed with generative components. Generative models are typically trained on large datasets, resulting in model behaviors that can mimic the worst of the content in the training data. Responsible deployment of generative technologies requires content moderation strategies, such as safety input and output filters. Here, we provide a theoretical framework for conceptualizing responsible content moderation of text-to-image generative technologies, including a demonstration of how to empirically measure the constructs we enumerate. We define and distinguish the concepts of safety, fairness, and metric equity, and enumerate example harms that can come in each domain. We then provide a demonstration of how the defined harms can be quantified. We conclude with a summary of how the style of harms quantification we demonstrate enables data-driven content moderation decisions.
    
[^64]: 可信机器学习的声音解释

    Sound Explanation for Trustworthy Machine Learning. (arXiv:2306.06134v1 [cs.LG])

    [http://arxiv.org/abs/2306.06134](http://arxiv.org/abs/2306.06134)

    本篇论文提出了声音解释的概念，以提供足够的信息来因果解释机器学习系统进行的预测，反对通过将分数归因于输入组件来解释黑盒模型的惯例，并且提出应用特征选择作为癌症预测模型的声音解释，以建立临床医生之间的信任。

    

    我们采用正式的方法来解决机器学习系统的可解释性问题。我们反对通过将分数归因于输入组件来解释黑盒模型的惯例，因为这种解释方法的目标存在内在的冲突。我们证明没有任何归因算法能够满足特异性、可加性、完整性和基线不变性。然后，我们正式定义了在以前的工作中不正式采用的概念——声音解释。一个声音的解释需要提供足够的信息来因果解释系统所进行的预测。最后，我们提出应用特征选择作为癌症预测模型的声音解释，以建立临床医生之间的信任。

    We take a formal approach to the explainability problem of machine learning systems. We argue against the practice of interpreting black-box models via attributing scores to input components due to inherently conflicting goals of attribution-based interpretation. We prove that no attribution algorithm satisfies specificity, additivity, completeness, and baseline invariance. We then formalize the concept, sound explanation, that has been informally adopted in prior work. A sound explanation entails providing sufficient information to causally explain the predictions made by a system. Finally, we present the application of feature selection as a sound explanation for cancer prediction models to cultivate trust among clinicians.
    
[^65]: 探究生成人工智能与互联网的相互作用

    Towards Understanding the Interplay of Generative Artificial Intelligence and the Internet. (arXiv:2306.06130v1 [cs.AI])

    [http://arxiv.org/abs/2306.06130](http://arxiv.org/abs/2306.06130)

    探究生成AI和互联网之间互动的影响，生成AI可能成为数据仓库贡献者并影响后续训练，提出未来版本生成AI工具在使用混合数据训练时会出现的问题和挑战。

    

    近来，如DALL-E、MidJourney或ChatGPT等能够生成逼真图像或文本的生成人工智能（AI）工具的广泛应用，引发了这些技术的社会影响成为公共争论的核心。这些工具是可能的，是因为互联网上公开可用的大量数据（文本和图像）。与此同时，这些生成人工智能工具成为内容创作者，已经为未来模型训练所需的数据做出了贡献。因此，生成人工智能工具的未来版本将通过人工创建和人工智能生成的内容混合训练，引发潜在的生成人工智能和公共数据仓库之间的反馈循环。这种互动引发了许多问题：未来版本的生成人工智能工具在混合的真实和人工智能生成的数据上训练时将如何表现？它们是否会随着新的数据集进化和改进还是相反变得更差？进化是否会引入偏误或收缩视野？

    The rapid adoption of generative Artificial Intelligence (AI) tools that can generate realistic images or text, such as DALL-E, MidJourney, or ChatGPT, have put the societal impacts of these technologies at the center of public debate. These tools are possible due to the massive amount of data (text and images) that is publicly available through the Internet. At the same time, these generative AI tools become content creators that are already contributing to the data that is available to train future models. Therefore, future versions of generative AI tools will be trained with a mix of human-created and AI-generated content, causing a potential feedback loop between generative AI and public data repositories. This interaction raises many questions: how will future versions of generative AI tools behave when trained on a mixture of real and AI generated data? Will they evolve and improve with the new data sets or on the contrary will they degrade? Will evolution introduce biases or red
    
[^66]: 基于PPG的心率估计的能量高效可穿戴式到移动设备的机器学习推断卸载

    Energy-efficient Wearable-to-Mobile Offload of ML Inference for PPG-based Heart-Rate Estimation. (arXiv:2306.06129v1 [eess.SP])

    [http://arxiv.org/abs/2306.06129](http://arxiv.org/abs/2306.06129)

    本文提出了一种协作推理方法，使用智能手表和连接的智能手机来最大化心率跟踪的性能并最大程度地延长电池寿命，并在自定义的智能手表原型上对方法进行了基准测试。实验证明此方法可以提高8.5\%的HR跟踪精度和降低37\%的能量消耗。

    

    现代智能手表通常包括光脉搏图（PPG）传感器，通过将PPG数据与其他信号融合的复杂算法来测量心跳或血压。在本文中，我们提出了一种协作推理方法，使用智能手表和连接的智能手机来最大化心率（HR）跟踪的性能，同时最大程度地延长智能手表的电池寿命。具体而言，我们首先分析在设备上运行HR跟踪或将工作卸载到移动设备之间的权衡。然后，通过额外的步骤评估即将到来的HR预测的难度，我们演示了我们可以聪明地管理智能手表和智能手机之间的工作负载，保持低平均绝对误差（MAE）同时降低能量消耗。我们在自定义的智能手表原型上对我们的方法进行了基准测试，包括STM32WB55 MCU和低功耗蓝牙（BLE）通信，以及树莓派Pi3作为智能手机的代理。我们的协作心率估计（CHRE）框架将HR跟踪精度提高了8.5％，同时将智能手表的能量消耗降低了37％。

    Modern smartwatches often include photoplethysmographic (PPG) sensors to measure heartbeats or blood pressure through complex algorithms that fuse PPG data with other signals. In this work, we propose a collaborative inference approach that uses both a smartwatch and a connected smartphone to maximize the performance of heart rate (HR) tracking while also maximizing the smartwatch's battery life. In particular, we first analyze the trade-offs between running on-device HR tracking or offloading the work to the mobile. Then, thanks to an additional step to evaluate the difficulty of the upcoming HR prediction, we demonstrate that we can smartly manage the workload between smartwatch and smartphone, maintaining a low mean absolute error (MAE) while reducing energy consumption. We benchmark our approach on a custom smartwatch prototype, including the STM32WB55 MCU and Bluetooth Low-Energy (BLE) communication, and a Raspberry Pi3 as a proxy for the smartphone. With our Collaborative Heart R
    
[^67]: 基于深度学习的目标跟踪、速度估计和传感器数据的时间投影方法

    Deep Learning Method for Object Tracking, Velocity Estimation and Projection of Sensor Data over Time. (arXiv:2306.06126v1 [cs.CV])

    [http://arxiv.org/abs/2306.06126](http://arxiv.org/abs/2306.06126)

    本文提出了一种利用Transformer机制的新型循环神经网络单元，实现了利用传感器记录中的时空相关性来实现目标跟踪和速度估计，并将记忆状态进行投影。

    

    目前环境分割和速度估计的深度学习方法依赖于卷积循环神经网络，以利用所获取的传感器数据中的时空关系。这些方法通过将新输入和记忆数据相关联来隐式地推导场景动态，利用卷积神经网络。本文发现卷积神经网络在这个任务上存在架构限制，因此提出了利用Transformer机制的新型循环神经网络单元来解决利用一系列传感器记录中的时空相关性所面临的各种问题。在该单元中，通过将基于传感器输入和记忆状态分别导出的关键-查询对相关联，跟踪对象编码在连续帧上的位置。然后利用得到的跟踪模式来获取场景动态和回归速度。最后，基于提取的速度估计将循环神经网络的记忆状态进行投影。

    Current Deep Learning methods for environment segmentation and velocity estimation rely on Convolutional Recurrent Neural Networks to exploit spatio-temporal relationships within obtained sensor data. These approaches derive scene dynamics implicitly by correlating novel input and memorized data utilizing ConvNets. We show how ConvNets suffer from architectural restrictions for this task. Based on these findings, we then provide solutions to various issues on exploiting spatio-temporal correlations in a sequence of sensor recordings by presenting a novel Recurrent Neural Network unit utilizing Transformer mechanisms. Within this unit, object encodings are tracked across consecutive frames by correlating key-query pairs derived from sensor inputs and memory states, respectively. We then use resulting tracking patterns to obtain scene dynamics and regress velocities. In a last step, the memory state of the Recurrent Neural Network is projected based on extracted velocity estimates to res
    
[^68]: 基于掩码Token Transformer的大规模MIMO系统联合信道估计和反馈

    Joint Channel Estimation and Feedback with Masked Token Transformers in Massive MIMO Systems. (arXiv:2306.06125v1 [cs.IT])

    [http://arxiv.org/abs/2306.06125](http://arxiv.org/abs/2306.06125)

    本论文提出了一种基于深度学习和掩码Token Transformer的联合信道估计和反馈框架，有效提高了大规模MIMO系统中信道估计和反馈的性能。

    

    当基站具有下行通道状态信息（CSI）时，可以充分利用大规模多输入多输出（MIMO）在频分双工（FDD）模式下的潜力。本文提出了基于深度学习的联合信道估计和反馈框架，以实现大规模MIMO系统中的信道估计和反馈。具体而言，我们采用传统通道设计而非端到端方法。我们的模型包含两个网络。第一个网络是信道估计网络，采用双重损失设计，可以准确地估计完整的信道信息并消除信道噪声。第二个网络是压缩和反馈网络。受掩码Token Transformer的启发，我们提出了一种可学习的掩码Token方法，以获得出色的估计和压缩性能。广泛的仿真结果和削弱研究表明，我们的方法优于最先进的信道估计和反馈方法。

    When the base station has downlink channel status information (CSI), the huge potential of large-scale multiple input multiple output (MIMO) in frequency division duplex (FDD) mode can be fully exploited. In this paper, we propose a deep-learning-based joint channel estimation and feedback framework to realize channel estimation and feedback in massive MIMO systems. Specifically, we use traditional channel design rather than end-to-end methods. Our model contains two networks. The first network is a channel estimation network, which adopts a double loss design, and can accurately estimate the full channel information while removing channel noises. The second network is a compression and feedback network. Inspired by the masked token transformer, we propose a learnable mask token method to obtain excellent estimation and compression performance. The extensive simulation results and ablation studies show that our method outperforms state-of-the-art channel estimation and feedback methods
    
[^69]: 基于深度卷积自编码器的电力系统干扰无监督聚类方法

    Unsupervised clustering of disturbances in power systems via deep convolutional autoencoders. (arXiv:2306.06124v1 [eess.SP])

    [http://arxiv.org/abs/2306.06124](http://arxiv.org/abs/2306.06124)

    本文提出了一种基于卷积自编码器和K-means聚类的无监督分类方法，可将电力系统中的干扰波形聚类为涉及电压下降、中断、瞬态、正常和谐波畸变等类别，从而实现过滤异常波形和常规波形。

    

    当电网检测到异常事件时，电力质量(PQ)仪器会记录PQ事件。使用机器学习的神经网络可以帮助准确分类记录的波形并帮助电力系统工程师诊断和纠正问题的根本原因。然而，在电力系统干扰期间捕获的许多波形需要进行标记以进行监督学习，使工程师需要手动处理或未看到大量数据记录。本文提出了一种基于自编码器和K-means聚类的无监督技术，可将PQ事件聚类为涉及电压下降、中断、瞬态、正常和谐波畸变等类别，从而实现过滤异常波形和常规波形。该方法利用在分布网格中记录的三相场获得的电压波形进行演示。首先，卷积自编码器将输入信号压缩为一组较低特征。

    Power quality (PQ) events are recorded by PQ meters whenever anomalous events are detected on the power grid. Using neural networks with machine learning can aid in accurately classifying the recorded waveforms and help power system engineers diagnose and rectify the root causes of problems. However, many of the waveforms captured during a disturbance in the power system need to be labeled for supervised learning, leaving a large number of data recordings for engineers to process manually or go unseen. This paper presents an autoencoder and K-means clustering-based unsupervised technique that can be used to cluster PQ events into categories like sag, interruption, transients, normal, and harmonic distortion to enable filtering of anomalous waveforms from recurring or normal waveforms. The method is demonstrated using three-phase, field-obtained voltage waveforms recorded in a distribution grid. First, a convolutional autoencoder compresses the input signals into a set of lower feature 
    
[^70]: 《可解释人工智能中的对抗性攻击和防御：调查报告》

    Adversarial Attacks and Defenses in Explainable Artificial Intelligence: A Survey. (arXiv:2306.06123v1 [cs.CR])

    [http://arxiv.org/abs/2306.06123](http://arxiv.org/abs/2306.06123)

    本文总结了对抗性攻击和防御在可解释人工智能中的研究。列出了现有的不安全因素，并表明了本领域的新兴研究方向。

    

    可解释人工智能（XAI）方法被描绘为调试和信任统计和深度学习模型的治疗方式，以及解释它们的预测。然而，对抗机器学习的最新进展突出了最新解释的局限性和漏洞，这些进展令人对其安全性和可信度产生质疑。操纵、欺骗或洗白模型推理证据的可能性在高风险决策和知识发现中产生不利后果。本文总结了50多篇论文的研究，概述了针对机器学习模型解释的对抗攻击以及公平度量的研究。我们讨论了如何防御攻击并设计鲁棒的解释方法。我们列出XAI中现有的不安全因素，并概述了对抗性XAI（AdvXAI）的新兴研究方向。

    Explainable artificial intelligence (XAI) methods are portrayed as a remedy for debugging and trusting statistical and deep learning models, as well as interpreting their predictions. However, recent advances in adversarial machine learning highlight the limitations and vulnerabilities of state-of-the-art explanations, putting their security and trustworthiness into question. The possibility of manipulating, fooling or fairwashing evidence of the model's reasoning has detrimental consequences when applied in high-stakes decision-making and knowledge discovery. This concise survey of over 50 papers summarizes research concerning adversarial attacks on explanations of machine learning models, as well as fairness metrics. We discuss how to defend against attacks and design robust interpretation methods. We contribute a list of existing insecurities in XAI and outline the emerging research directions in adversarial XAI (AdvXAI).
    
[^71]: 双重随机图网络非自回归反应预测

    Doubly Stochastic Graph-based Non-autoregressive Reaction Prediction. (arXiv:2306.06119v1 [physics.chem-ph])

    [http://arxiv.org/abs/2306.06119](http://arxiv.org/abs/2306.06119)

    本文提出了一种新的框架，该框架将两个双重随机自注意映射结合起来，实现遵循化学反应物理约束的电子再分配预测。

    

    有机反应预测是药物研发中的重要任务。最近，研究人员通过建模电子再分配实现了非自回归反应预测，取得了最先进的top-1精度并实现了并行抽样。然而，当前的非自回归解码器不能同时满足电子分配规则和对称规则这两个基本规则。这种化学反应物理约束的违反会损害模型的性能。本研究提出了一种新的框架，将两个双重随机自注意映射结合起来，得到遵循这两个约束的电子再分配预测。我们还将这种解决方案扩展到一个具有增强约束的通用多头注意机制。为了实现这一目标，我们采用Sinkhorn算法来迭代更新自我关注映射，进而引入了双重保守约束作为额外的信息优先考虑。

    Organic reaction prediction is a critical task in drug discovery. Recently, researchers have achieved non-autoregressive reaction prediction by modeling the redistribution of electrons, resulting in state-of-the-art top-1 accuracy, and enabling parallel sampling. However, the current non-autoregressive decoder does not satisfy two essential rules of electron redistribution modeling simultaneously: the electron-counting rule and the symmetry rule. This violation of the physical constraints of chemical reactions impairs model performance. In this work, we propose a new framework called that combines two doubly stochastic self-attention mappings to obtain electron redistribution predictions that follow both constraints. We further extend our solution to a general multi-head attention mechanism with augmented constraints. To achieve this, we apply Sinkhorn's algorithm to iteratively update self-attention mappings, which imposes doubly conservative constraints as additional informative prio
    
[^72]: 利用无人机摄影测量和机器学习估算河流水面高程

    Estimation of River Water Surface Elevation Using UAV Photogrammetry and Machine Learning. (arXiv:2306.06118v1 [cs.LG])

    [http://arxiv.org/abs/2306.06118](http://arxiv.org/abs/2306.06118)

    无人机摄影测量技术结合卷积神经网络可以准确测定河流水面高程，并使用线性回归方法进一步改进了估算准确性。

    

    无人机摄影测量技术可以制作地形正射影像和数字地表模型（DSM）。然而，通过这种技术制作的水体DSM会显示水面扭曲，从而无法准确测定水面高程。因此，我们提出了一种新的解决方案，使用卷积神经网络（CNN）在摄影测量DSM和正射影像中用作水面高程估算器。另外，我们还改进了之前的“水边缘”方法，通过使用前后指数加权移动平均滤除了异常值。采用链码进行水面高程值的线性回归，进一步改进了这两种方法。该解决方案可以估算预测的不确定性。这是第一次使用深度学习进行此任务。同时，作者在冬季和夏季条件下创建了一个全新的机器学习数据集，该数据集是在一条小的低地河流上收集的。

    Unmanned aerial vehicle (UAV) photogrammetry allows for the creation of orthophotos and digital surface models (DSMs) of a terrain. However, DSMs of water bodies mapped with this technique reveal water surface distortions, preventing the use of photogrammetric data for accurate determination of water surface elevation (WSE). Firstly, we propose a new solution in which a convolutional neural network (CNN) is used as a WSE estimator from photogrammetric DSMs and orthophotos. Second, we improved the previously known "water-edge" method by filtering the outliers using a forward-backwards exponential weighted moving average. Further improvement in these two methods was achieved by performing a linear regression of the WSE values against chainage. The solutions estimate the uncertainty of the predictions. This is the first approach in which DL was used for this task. A brand new machine learning data set has been created. It was collected on a small lowland river in winter and summer conditi
    
[^73]: 运动治疗中3D姿势估计和惯性运动捕捉系统的优势和劣势

    Strengths and Weaknesses of 3D Pose Estimation and Inertial Motion Capture System for Movement Therapy. (arXiv:2306.06117v1 [eess.IV])

    [http://arxiv.org/abs/2306.06117](http://arxiv.org/abs/2306.06117)

    本研究比较了3D姿势估计和惯性运动捕捉系统在特定运动锻炼中的精度，为临床应用提供了有用信息。

    

    3D姿势估计为快速、非侵入性且准确的运动分析提供了机会。这对于临床应用也具有特殊的兴趣。目前，运动捕捉系统被应用，因为它们提供了强大而精确的数据获取，在临床应用中是必不可少的。在本研究中，我们比较了最先进的3D位置估计方法MeTrabs与已建立的惯性传感器系统MTw Awinda在特定运动锻炼中的精度。该研究使用并提供了10名受试者在各种运动治疗锻炼期间的平行记录的评估数据集。Awinda系统的信息和单眼姿态估计的帧被同步。为了进行比较，对踝关节、膝关节、背部和肘部的关节角度进行了临床相关参数的估计和评估，使用平均值、中位数和计算出的不同锻炼中关节角度之间的最大偏差。

    3D pose estimation offers the opportunity for fast, non-invasive, and accurate motion analysis. This is of special interest also for clinical use. Currently, motion capture systems are used, as they offer robust and precise data acquisition, which is essential in the case of clinical applications. In this study, we investigate the accuracy of the state-of-the-art 3D position estimation approach MeTrabs, compared to the established inertial sensor system MTw Awinda for specific motion exercises. The study uses and provides an evaluation dataset of parallel recordings from 10 subjects during various movement therapy exercises. The information from the Awinda system and the frames for monocular pose estimation are synchronized. For the comparison, clinically relevant parameters for joint angles of ankle, knee, back, and elbow flexion-extension were estimated and evaluated using mean, median, and maximum deviation between the calculated joint angles for the different exercises, camera posi
    
[^74]: 视网膜血管分割的深度学习方法概述

    Overview of Deep Learning Methods for Retinal Vessel Segmentation. (arXiv:2306.06116v1 [eess.IV])

    [http://arxiv.org/abs/2306.06116](http://arxiv.org/abs/2306.06116)

    本文对近期用深度学习方法进行视网膜血管分割的研究进行了回顾与评估，总结了这些方法的设计特点、性能指标以及优缺点。

    

    自动化视网膜血管分割在许多眼部和全身性疾病的治疗和诊断中扮演着重要角色。随着深度学习方法的快速发展，越来越多的视网膜血管分割方法被实现为深度神经网络。本文提供了对来自高影响期刊和会议的最新深度学习方法的简要回顾。回顾的目标是：(1)评估最新方法的设计特点，(2)报告和分析性能评估指标的定量值，以及(3)分析最新解决方案的优缺点。

    Methods for automated retinal vessel segmentation play an important role in the treatment and diagnosis of many eye and systemic diseases. With the fast development of deep learning methods, more and more retinal vessel segmentation methods are implemented as deep neural networks. In this paper, we provide a brief review of recent deep learning methods from highly influential journals and conferences. The review objectives are: (1) to assess the design characteristics of the latest methods, (2) to report and analyze quantitative values of performance evaluation metrics, and (3) to analyze the advantages and disadvantages of the recent solutions.
    
[^75]: 基于深度和法向渲染的汽车阻力系数代理建模

    Surrogate Modeling of Car Drag Coefficient with Depth and Normal Renderings. (arXiv:2306.06110v1 [cs.LG])

    [http://arxiv.org/abs/2306.06110](http://arxiv.org/abs/2306.06110)

    本论文提出了一种新的二维三维形状表示方法，用于代理建模汽车阻力系数。实验结果表明，该表示方法在阻力预测方面的性能更好，需要使用更小的模型。

    

    生成式人工智能模型在自动化创建三维形状方面取得了显著进展，这有可能改变汽车设计。在工程设计和优化中，评估工程指标至关重要。为了使生成模型具有性能意识并使其能够创建高性能设计，这些指标的代理建模是必要的。然而，目前使用的三维形状表示要么需要大量的计算资源来学习，要么会出现显著的信息损失，影响了它们在代理建模中的有效性。为了解决这个问题，我们提出了一种新的二维三维形状表示方法。我们基于这种表示方法开发了一个代理阻力模型，以验证它在预测三维汽车阻力方面的有效性。我们构建了一个包含9,070个高质量三维汽车网格的多样化数据集，并用计算流体力学（CFD）模拟计算的阻力系数来训练我们的模型。我们的实验结果表明，我们提出的二维表示方法比现有的三维表示更具表现力，在使用更小的模型时可以实现更好的阻力预测性能。

    Generative AI models have made significant progress in automating the creation of 3D shapes, which has the potential to transform car design. In engineering design and optimization, evaluating engineering metrics is crucial. To make generative models performance-aware and enable them to create high-performing designs, surrogate modeling of these metrics is necessary. However, the currently used representations of three-dimensional (3D) shapes either require extensive computational resources to learn or suffer from significant information loss, which impairs their effectiveness in surrogate modeling. To address this issue, we propose a new two-dimensional (2D) representation of 3D shapes. We develop a surrogate drag model based on this representation to verify its effectiveness in predicting 3D car drag. We construct a diverse dataset of 9,070 high-quality 3D car meshes labeled by drag coefficients computed from computational fluid dynamics (CFD) simulations to train our model. Our expe
    
[^76]: 学习量化漏洞模式并匹配以定位语句级漏洞

    Learning to Quantize Vulnerability Patterns and Match to Locate Statement-Level Vulnerabilities. (arXiv:2306.06109v1 [cs.CR])

    [http://arxiv.org/abs/2306.06109](http://arxiv.org/abs/2306.06109)

    该论文提出了一种新颖的漏洞匹配方法，利用学习的漏洞代码库中的漏洞模式定位易受攻击语句，以解决传统漏洞检测方法中存在的识别漏洞范围不准确的问题。

    

    深度学习模型在识别软件漏洞中越来越流行。先前的研究发现，不同易受攻击的程序中的漏洞可能表现出类似的易受攻击范围，这些范围形成可通过监督训练为DL模型所学的可辨认的漏洞模式。然而，漏洞的易攻击范围仍以不同的空间位置和格式在程序中表现，这导致模型准确识别易受攻击语句面临挑战。尽管如此，最先进的漏洞检测方法仍未利用易受攻击程序中出现的漏洞模式。为充分利用易受攻击模式并释放DL模型的能力，在本文中，我们提出了一种新颖的漏洞匹配方法，受程序分析工具的启发，根据预定义的模式来定位漏洞。具体而言，我们学习了一份漏洞代码库，其中包括量化漏洞模式。

    Deep learning (DL) models have become increasingly popular in identifying software vulnerabilities. Prior studies found that vulnerabilities across different vulnerable programs may exhibit similar vulnerable scopes, implicitly forming discernible vulnerability patterns that can be learned by DL models through supervised training. However, vulnerable scopes still manifest in various spatial locations and formats within a program, posing challenges for models to accurately identify vulnerable statements. Despite this challenge, state-of-the-art vulnerability detection approaches fail to exploit the vulnerability patterns that arise in vulnerable programs. To take full advantage of vulnerability patterns and unleash the ability of DL models, we propose a novel vulnerability-matching approach in this paper, drawing inspiration from program analysis tools that locate vulnerabilities based on pre-defined patterns. Specifically, a vulnerability codebook is learned, which consists of quantize
    
[^77]: 揭秘比特币网络中的欺诈交易和非法节点：金融取证的应用数据科学方法

    Demystifying Fraudulent Transactions and Illicit Nodes in the Bitcoin Network for Financial Forensics. (arXiv:2306.06108v1 [cs.CR])

    [http://arxiv.org/abs/2306.06108](http://arxiv.org/abs/2306.06108)

    本文提出了Elliptic++数据集，通过对比特币网络中的交易和地址之间的关系进行分析，旨在以综合应用数据科学方法进行欺诈检测，从而查出欺诈交易和非法地址。

    

    区块链通过挖掘其开放且不可变的交易数据为金融取证提供了独特且可追溯的渠道。近期，使用加密货币交易数据训练机器学习模型进行异常检测，如洗钱和其他欺诈活动，已成为主流。本文提出了一种综合应用数据科学方法，用于比特币网络中的欺诈检测，并提供了两个独创性贡献。首先，本文提出了Elliptic++数据集，该数据集扩展了Elliptic交易数据集，包括822k个比特币钱包地址（节点），每个地址有56个特征和1.27M个时间交互作用。通过利用四种类型的图数据：（i）交易之间的图，代表比特币网络中的资金流动，（ii）地址之间的交互图，捕捉交易类型，从而实现了对欺诈交易和非法地址（参与者）的检测。

    Blockchain provides the unique and accountable channel for financial forensics by mining its open and immutable transaction data. A recent surge has been witnessed by training machine learning models with cryptocurrency transaction data for anomaly detection, such as money laundering and other fraudulent activities. This paper presents a holistic applied data science approach to fraud detection in the Bitcoin network with two original contributions. First, we contribute the Elliptic++ dataset, which extends the Elliptic transaction dataset to include over 822k Bitcoin wallet addresses (nodes), each with 56 features, and 1.27M temporal interactions. This enables both the detection of fraudulent transactions and the detection of illicit addresses (actors) in the Bitcoin network by leveraging four types of graph data: (i) the transaction-to-transaction graph, representing the money flow in the Bitcoin network, (ii) the address-to-address interaction graph, capturing the types of transacti
    
[^78]: 水配管网络泄漏探测器的对抗性攻击

    Adversarial Attacks on Leakage Detectors in Water Distribution Networks. (arXiv:2306.06107v1 [cs.CR])

    [http://arxiv.org/abs/2306.06107](http://arxiv.org/abs/2306.06107)

    本文研究了水配管网络泄漏探测器的对抗性攻击，并提出分类法。重点研究攻击者在水网中寻找最不敏感点，使用三种算法方法寻找解决方案，在两个基准水配管网络上进行评估。

    

    许多机器学习模型都容易受到对抗性攻击，攻击者可以对输入进行微小的扰动，在不被察觉的情况下导致模型做出错误的预测。为了制定增强模型鲁棒性和可信度的对策，特别是在安全关键领域中使用的模型，比如监测水配管网络，在对抗性攻击方面的更好理解至关重要。本文提出了一种对抗性攻击分类法，针对使用机器学习的水配管网络泄漏探测器。在此基础上，我们重点关注一种特定类型的攻击：攻击者寻找最不敏感点，也就是可能发生最大可能未被检测泄漏的水网位置。基于最不敏感点问题的数学形式化，我们使用三种不同的算法方法来寻找解决方案。结果在两个基准水配管网络上进行评估。

    Many Machine Learning models are vulnerable to adversarial attacks: There exist methodologies that add a small (imperceptible) perturbation to an input such that the model comes up with a wrong prediction. Better understanding of such attacks is crucial in particular for models used in security-critical domains, such as monitoring of water distribution networks, in order to devise counter-measures enhancing model robustness and trustworthiness.  We propose a taxonomy for adversarial attacks against machine learning based leakage detectors in water distribution networks. Following up on this, we focus on a particular type of attack: an adversary searching the least sensitive point, that is, the location in the water network where the largest possible undetected leak could occur. Based on a mathematical formalization of the least sensitive point problem, we use three different algorithmic approaches to find a solution. Results are evaluated on two benchmark water distribution networks.
    
[^79]: 从稀疏观测数据中进行日预测的深度学习

    Deep Learning for Day Forecasts from Sparse Observations. (arXiv:2306.06079v1 [physics.ao-ph])

    [http://arxiv.org/abs/2306.06079](http://arxiv.org/abs/2306.06079)

    本文提出了一种基于稀疏观测数据的MetNet-3深度学习模型，可对20小时内的天气进行准确的预测。MetNet-3的技术创新包括可学习卷积、特征学习和多任务训练优化。此外，使用持续性启发法来外推初始条件或进行短期预测来填补缺失的观测数据更进一步提高了预测性能。

    

    深度神经网络提供了一种建模天气条件的替代范例。神经模型在数据可用时以少于1秒的速度进行预测，并以非常高的时间和空间分辨率进行预测。它们可以直接从大气观测数据中进行学习，这是这些模型独特的优势之一。然而，迄今为止，仅仅预测降水这一唯一变量时，仅能使用大气观测数据来训练神经模型，才能达到与现有概率性数值天气预报模型相当的良好表现到12个小时的提前量。本文提出了MetNet-3，它显著扩展了基于观测数据的神经模型能够良好预测的引导时间范围和变量。MetNet-3从密集和稀疏的数据传感器中学习，并为降水、风、温度和露点进行24小时的预测。MetNet-3在体系结构层面引入了许多技术创新，这被证明对提高模型性能有所贡献，包括可学习的时空卷积、基于注意力的特征学习和多任务训练优化。此外，我们展示了将神经模型泛化为仅接受稀疏的气压计观测数据作为输入，并通过使用简单的持续性启发法来外推初始条件，或通过使用低分辨率数值模型进行短期预测来填补缺失的观测数据的方法是有益的。MetNet-3在降水、温度和露点预测方面比现有的大气模型在提前至24小时方面表现更好。

    Deep neural networks offer an alternative paradigm for modeling weather conditions. The ability of neural models to make a prediction in less than a second once the data is available and to do so with very high temporal and spatial resolution, and the ability to learn directly from atmospheric observations, are just some of these models' unique advantages. Neural models trained using atmospheric observations, the highest fidelity and lowest latency data, have to date achieved good performance only up to twelve hours of lead time when compared with state-of-the-art probabilistic Numerical Weather Prediction models and only for the sole variable of precipitation. In this paper, we present MetNet-3 that extends significantly both the lead time range and the variables that an observation based neural model can predict well. MetNet-3 learns from both dense and sparse data sensors and makes predictions up to 24 hours ahead for precipitation, wind, temperature and dew point. MetNet-3 introduc
    
[^80]: 从点云数据中学习Fisher信息矩阵的神经FIM

    Neural FIM for learning Fisher Information Metrics from point cloud data. (arXiv:2306.06062v1 [cs.CV])

    [http://arxiv.org/abs/2306.06062](http://arxiv.org/abs/2306.06062)

    本文提出了神经FIM，一种从点云数据中计算Fisher信息度量（FIM）的方法，可以连续地对数据进行流形建模，从而提高流形特征的描述能力。

    

    虽然数据扩散嵌入在无监督学习中随处可见，并且已经被证明是揭示数据潜在内在几何的可行技术，但由于其离散性，扩散嵌入固有的局限性。因此，我们提出了神经FIM，一种从点云数据中计算Fisher信息度量（FIM）的方法，允许对数据进行连续流形建模。神经FIM从离散的点云数据中创建可扩展的度量空间，因此从度量中获取的信息可以告诉我们流形的特征，如体积和测地线。我们证明了神经FIM在选择PHATE可视化方法的参数以及在玩具数据集和两个IPSC重编程和PBMCs单细胞数据集的分支点和聚类中心嵌入中获取信息方面的效用。

    Although data diffusion embeddings are ubiquitous in unsupervised learning and have proven to be a viable technique for uncovering the underlying intrinsic geometry of data, diffusion embeddings are inherently limited due to their discrete nature. To this end, we propose neural FIM, a method for computing the Fisher information metric (FIM) from point cloud data - allowing for a continuous manifold model for the data. Neural FIM creates an extensible metric space from discrete point cloud data such that information from the metric can inform us of manifold characteristics such as volume and geodesics. We demonstrate Neural FIM's utility in selecting parameters for the PHATE visualization method as well as its ability to obtain information pertaining to local volume illuminating branching points and cluster centers embeddings of a toy dataset and two single-cell datasets of IPSC reprogramming and PBMCs (immune cells).
    
[^81]: 基于Implicit Neural Representations的时间序列连续建模用于插值和预测

    Time Series Continuous Modeling for Imputation and Forecasting with Implicit Neural Representations. (arXiv:2306.05880v1 [cs.LG])

    [http://arxiv.org/abs/2306.05880](http://arxiv.org/abs/2306.05880)

    该论文提出了基于INR的时间序列连续建模方法，解决了处理缺失数据、不规则采样和多传感器不对准观测等重复建模问题，并在预测和插值任务中取得了最新的性能表现，具有很好的泛化能力。

    

    尽管时间序列建模已被广泛探索，但在面对真实世界的数据时仍面临重大挑战。我们提出了一种新颖的建模方法，利用Implicit Neural Representations (INR)。该方法使我们能够有效地捕捉时间序列的连续性，并提供了自然的解决方案，以处理缺失数据、处理不规则采样或来自多个传感器的不对准观测等重复建模问题。通过引入条件调制INR参数并利用元学习技术，我们解决了模型泛化到未见样本和时间窗口移位的问题。通过大量实验，我们的模型展示了在预测和插值任务中领先的性能，同时在处理许多竞争模型无法处理的各种具有挑战性的场景方面展现了灵活性。

    Although widely explored, time series modeling continues to encounter significant challenges when confronted with real-world data. We propose a novel modeling approach leveraging Implicit Neural Representations (INR). This approach enables us to effectively capture the continuous aspect of time series and provides a natural solution to recurring modeling issues such as handling missing data, dealing with irregular sampling, or unaligned observations from multiple sensors. By introducing conditional modulation of INR parameters and leveraging meta-learning techniques, we address the issue of generalization to both unseen samples and time window shifts. Through extensive experimentation, our model demonstrates state-of-the-art performance in forecasting and imputation tasks, while exhibiting flexibility in handling a wide range of challenging scenarios that competing models cannot.
    
[^82]: 权重冻结：一种正则化方法在脑电分类中的应用

    Weight Freezing: A Regularization Approach for Fully Connected Layers with an Application in EEG Classification. (arXiv:2306.05775v1 [cs.LG])

    [http://arxiv.org/abs/2306.05775](http://arxiv.org/abs/2306.05775)

    本文提出了一种新颖的方法，权重冻结，它可以有效地减少全连接层中的特定神经元对特定EEG任务的决策过程的影响，从而提高人工神经网络（ANNs）的性能，是一种有效的正则化方法。

    

    在脑电解码领域，提高人工神经网络（ANNs）的性能具有重要的潜力。本研究介绍了一种新颖的方法，称为“权重冻结”，该方法基于ANN正则化和神经科学先验知识的原则。权重冻结的概念围绕着通过冻结全连接层中的特定权重来减少某些神经元对特定EEG任务的决策过程的影响。这是通过使用掩码矩阵和阈值来确定在反向传播过程中需要冻结的权重比例来实现的。此外，通过将被掩蔽的权重设置为零，权重冻结不仅可以在具有全连接层分类器的网络中实现稀疏连接，还可以作为全连接层的有效正则化方法。

    In the realm of EEG decoding, enhancing the performance of artificial neural networks (ANNs) carries significant potential. This study introduces a novel approach, termed "weight freezing", that is anchored on the principles of ANN regularization and neuroscience prior knowledge. The concept of weight freezing revolves around the idea of reducing certain neurons' influence on the decision-making process for a specific EEG task by freezing specific weights in the fully connected layer during the backpropagation process. This is actualized through the use of a mask matrix and a threshold to determine the proportion of weights to be frozen during backpropagation. Moreover, by setting the masked weights to zero, weight freezing can not only realize sparse connections in networks with a fully connected layer as the classifier but also function as an efficacious regularization method for fully connected layers. Through experiments involving three distinct ANN architectures and three widely r
    
[^83]: 线性扩散提升了快速高质量语音合成

    Boosting Fast and High-Quality Speech Synthesis with Linear Diffusion. (arXiv:2306.05708v1 [cs.SD])

    [http://arxiv.org/abs/2306.05708](http://arxiv.org/abs/2306.05708)

    本文提出了一种基于线性扩散模型的语音合成方法，能够同时实现快速推理和高样本质量

    

    面向各种生成任务，去噪扩散概率模型已经展现出了非凡的能力。然而，它们的慢推理速度使得它们在语音合成中不实用。本文提出了一种基于普通微分方程的线性扩散模型（LinDiff），以同时实现快速推理和高样本质量。同时，为了降低计算复杂度并实现对噪声语音的有效全局建模，LinDiff采用了基于分区的处理方法，将输入信号划分为小补丁。

    Denoising Diffusion Probabilistic Models have shown extraordinary ability on various generative tasks. However, their slow inference speed renders them impractical in speech synthesis. This paper proposes a linear diffusion model (LinDiff) based on an ordinary differential equation to simultaneously reach fast inference and high sample quality. Firstly, we employ linear interpolation between the target and noise to design a diffusion sequence for training, while previously the diffusion path that links the noise and target is a curved segment. When decreasing the number of sampling steps (i.e., the number of line segments used to fit the path), the ease of fitting straight lines compared to curves allows us to generate higher quality samples from a random noise with fewer iterations. Secondly, to reduce computational complexity and achieve effective global modeling of noisy speech, LinDiff employs a patch-based processing approach that partitions the input signal into small patches. Th
    
[^84]: 两人零和Markov博弈的极小极大Q-learning的有限时间分析：切换系统方法

    Finite-Time Analysis of Minimax Q-Learning for Two-Player Zero-Sum Markov Games: Switching System Approach. (arXiv:2306.05700v1 [eess.SY])

    [http://arxiv.org/abs/2306.05700](http://arxiv.org/abs/2306.05700)

    本文研究了Q-learning算法应用于两人零和Markov博弈的有限时间分析，并通过切换系统方法提供了更简单和深入的收敛分析。

    

    本文旨在研究Q-learning算法应用于两人零和Markov博弈的有限时间分析。我们针对极小极大Q-learning算法以及相应的价值迭代方法进行了有限时间分析。为了增强对价值迭代和Q-learning的分析，我们采用了极小极大Q-learning的切换系统模型和相应的价值迭代法。这种方法提供了对极小极大Q-learning的进一步洞察，并有助于更简单和深入的收敛分析。我们预计这些额外的洞察力有潜力揭示控制理论和强化学习社区概念之间的新联系，并促进它们之间的合作。

    The objective of this paper is to investigate the finite-time analysis of a Q-learning algorithm applied to two-player zero-sum Markov games. Specifically, we establish a finite-time analysis of both the minimax Q-learning algorithm and the corresponding value iteration method. To enhance the analysis of both value iteration and Q-learning, we employ the switching system model of minimax Q-learning and the associated value iteration. This approach provides further insights into minimax Q-learning and facilitates a more straightforward and insightful convergence analysis. We anticipate that the introduction of these additional insights has the potential to uncover novel connections and foster collaboration between concepts in the fields of control theory and reinforcement learning communities.
    
[^85]: 物理信息神经网络在逆流自发渗透中的应用和预测：早期和晚期的模拟

    Simulation and Prediction of Countercurrent Spontaneous Imbibition at Early and Late Times Using Physics-Informed Neural Networks. (arXiv:2306.05554v1 [physics.comp-ph])

    [http://arxiv.org/abs/2306.05554](http://arxiv.org/abs/2306.05554)

    本文通过物理信息神经网络模型对多孔材料中的逆流自发渗透过程进行了早期和晚期的模拟和预测，并使用改变变量技术来改进模型性能。

    

    逆流自发渗透（COUCSI）是一种多孔材料中的过程，其中润湿相取代了非润湿相的位置。本文首次探讨了物理信息神经网络（PINNs）在解决早期（ET）和晚期（LT）COUCSI问题中的应用。同时，我们还研究了改变变量技术以改进PINNs的性能。我们通过改变自变量将COUCSI问题分别用XT-，XY-和Z-三种等效形式进行描述：第一个描述了饱和度作为规范化位置X和时间T的函数;第二个描述了X和Y=T^0.5作为函数的饱和度;第三个作为Z=X/T^0.5的唯一函数（仅在ET下有效）。该PINN模型使用前馈神经网络生成，并基于最小化加权损失函数进行训练，包括物理信息丢失项和与初始边界条件相对应的项。没有合成或实验数据被调用。

    Countercurrent spontaneous imbibition (COUCSI) is a process in porous materials in which a wetting phase displaces non-wetting phase. In this work, we investigate for the first time the application of Physics-Informed Neural Networks (PINNs) in solving the 1D COUCSI problem in both early (ET) and late (LT) times. Also novel, we examine the Change-of-Variables technique for improving the performance of PINNs. We formulated the COUCSI problem in three equivalent forms by changing the independent variables: XT-, XY-, and Z-formulations. The first describes saturation as function of normalized position X and time T; the second as function of X and Y=T^0.5; and the third as a sole function of Z=X/T^0.5 (valid only at ET). The PINN model was generated using a feed-forward neural network and trained based on minimizing a weighted loss function, including the physics-informed loss term and terms corresponding to the initial and boundary conditions. No synthetical or experimental data were invo
    
[^86]: CoCo: 一种用于无监督领域自适应图分类的耦合对比框架

    CoCo: A Coupled Contrastive Framework for Unsupervised Domain Adaptive Graph Classification. (arXiv:2306.04979v1 [cs.LG])

    [http://arxiv.org/abs/2306.04979](http://arxiv.org/abs/2306.04979)

    CoCo是一种耦合对比图表示学习框架，其中包含一个图卷积网络和一个分层图内核网络，通过耦合对比学习减少领域差异，用于无监督领域自适应图分类。

    

    虽然图神经网络在图分类中取得了显著成果，但它们通常需要大量特定任务的标签，这可能需要极大的代价来获得。一种可靠的解决方案是探索其他标注图以增强目标域的无监督学习，但如何将图神经网络应用到领域适应中仍未解决，因为对图拓扑的不充分探索以及相当大的领域偏差。本文提出了一种称为CoCo（Coupled Contrastive Graph Representation Learning）方案，该方案从耦合学习分支中提取拓扑信息，并通过耦合对比学习减少领域差异。CoCo包含一个图卷积网络分支和分层图内核网络分支，分别用隐式和显式方式探索图拓扑。此外，我们将耦合分支结合到一个全面的多视角对比学习框架中，

    Although graph neural networks (GNNs) have achieved impressive achievements in graph classification, they often need abundant task-specific labels, which could be extensively costly to acquire. A credible solution is to explore additional labeled graphs to enhance unsupervised learning on the target domain. However, how to apply GNNs to domain adaptation remains unsolved owing to the insufficient exploration of graph topology and the significant domain discrepancy. In this paper, we propose \underline{Co}upled \underline{Co}ntrastive Graph Representation Learning (\method{}), which extracts the topological information from coupled learning branches and reduces the domain discrepancy with coupled contrastive learning. \method{} contains a graph convolutional network branch and a hierarchical graph kernel network branch, which explore graph topology in implicit and explicit manners. Besides, we incorporate coupled branches into a holistic multi-view contrastive learning framework, which 
    
[^87]: 一种基于深度贝叶斯粒子流框架的跨领域软测量无监督建模方法

    Unsupervised Cross-Domain Soft Sensor Modelling via A Deep Bayesian Particle Flow Framework. (arXiv:2306.04919v1 [cs.LG])

    [http://arxiv.org/abs/2306.04919](http://arxiv.org/abs/2306.04919)

    该论文提出了一种基于深度粒子流贝叶斯框架的跨领域软测量无监督建模方法，能够解决标签缺失、领域适应性和数据时间一致性等问题，提高软测量建模的准确性。

    

    数据驱动的软测量对于通过可靠的状态推断实现精确感知至关重要。然而，由于存在标签缺失、领域适应性和数据时间一致性等问题，开发具有代表性的软测量模型具有挑战性。为了解决这些问题，我们提出了一种基于深度粒子流贝叶斯 (DPFB) 框架，用于在无目标状态标签情况下进行跨领域软测量建模。具体来说，首先制定了一个顺序贝叶斯目标，以执行潜在的跨领域软感知问题的最大似然估计。在框架核心，我们结合物理学启发的粒子流，通过优化顺序贝叶斯目标来执行模型提取的潜在和隐藏特征的精确贝叶斯更新。由此，这些贡献使得该框架能够学习一个有机的近似后验特征表示，能够表征复杂的跨领域系统动力学并实现有效的软测量建模。

    Data-driven soft sensors are essential for achieving accurate perception through reliable state inference. However, developing representative soft sensor models is challenged by issues such as missing labels, domain adaptability, and temporal coherence in data. To address these challenges, we propose a deep Particle Flow Bayes (DPFB) framework for cross-domain soft sensor modeling in the absence of target state labels. In particular, a sequential Bayes objective is first formulated to perform the maximum likelihood estimation underlying the cross-domain soft sensing problem. At the core of the framework, we incorporate a physics-inspired particle flow that optimizes the sequential Bayes objective to perform an exact Bayes update of the model extracted latent and hidden features. As a result, these contributions enable the proposed framework to learn a cohesive approximate posterior feature representation capable of characterizing complex cross-domain system dynamics and performing effe
    
[^88]: 线性化随机生成树与GNN的快速高效训练

    Fast and Effective GNN Training with Linearized Random Spanning Trees. (arXiv:2306.04828v1 [cs.LG])

    [http://arxiv.org/abs/2306.04828](http://arxiv.org/abs/2306.04828)

    本文提出了一种基于线性化随机生成树的GNN训练框架，在多个真实世界的图形基准测试中表现得比其他经典算法更快且更准确。

    

    我们提出了一种新的有效和可扩展的框架，用于在给定图形结构数据的监督节点分类任务中训练GNN。我们的方法通过线性化从输入网络中提取的随机生成树得到一系列路径图来逐步精细化权重更新操作。路径图被设计为保留原始图的基本拓扑和节点信息。同时，路径图的稀疏性使得GNN训练更轻便，除了可扩展性外，还有助于缓解过度压缩和过度平滑等经典训练问题。我们在多个真实世界的图形基准测试上进行了广泛的实验研究，并将我们的框架应用于图形卷积网络，与众所周知的基线相比，同时提高了训练速度和测试精度。

    We present a new effective and scalable framework for training GNNs in supervised node classification tasks, given graph-structured data. Our approach increasingly refines the weight update operations on a sequence of path graphs obtained by linearizing random spanning trees extracted from the input network. The path graphs are designed to retain essential topological and node information of the original graph. At the same time, the sparsity of path graphs enables a much lighter GNN training which, besides scalability, helps in mitigating classical training issues, like over-squashing and over-smoothing. We carry out an extensive experimental investigation on a number of real-world graph benchmarks, where we apply our framework to graph convolutional networks, showing simultaneous improvement of both training speed and test accuracy, as compared to well-known baselines.
    
[^89]: 基于混合Actor-Critic强化学习的自适应频率绿灯最佳速度建议

    Adaptive Frequency Green Light Optimal Speed Advisory based on Hybrid Actor-Critic Reinforcement Learning. (arXiv:2306.04660v1 [cs.LG])

    [http://arxiv.org/abs/2306.04660](http://arxiv.org/abs/2306.04660)

    本文提出了一种基于混合Actor-Critic强化学习的自适应频率绿灯最佳速度建议模型，通过使用离散和连续演员网络来优化绿灯最佳速度建议系统的频率和加速度曲线，设计了新的奖励函数来平衡减少停车和最小化速度变化的权衡。实验结果显示，该模型可以有效地减少旅行时间和燃料消耗，并优于现有的最先进的GLOSA系统。

    

    绿灯最佳速度建议（GLOSA）系统建议车辆速度，以帮助它们在绿色时间通过路口，从而通过最小化在路口停车和怠速时间来减少交通拥堵和燃料消耗。但是，以前的研究集中于优化GLOSA算法，忽略了GLOSA系统的速度建议频率。具体而言，一些研究在每个决策步骤提供速度建议，导致冗余建议，而其他人仅为车辆计算最佳速度，无法适应动态交通。在本文中，我们提出了一种基于混合PPO（H-PPO）的自适应频率GLOSA（AF-GLOSA）模型，其采用了一个actor-critic架构和一个混合actor网络。混合演员网络包括一个离散演员，输出咨询频率和一个连续演员，输出加速度曲线。此外，我们设计了一种新的奖励函数来平衡减少停车和最小化速度变化的权衡。实验结果表明，所提出的AF-GLOSA模型可以有效地减少旅行时间和燃料消耗，并优于现有的最先进的GLOSA系统。

    Green Light Optimal Speed Advisory (GLOSA) system suggests speeds to vehicles to assist them in passing through intersections during green intervals, thus reducing traffic congestion and fuel consumption by minimizing the number of stops and idle times at intersections. However, previous research has focused on optimizing the GLOSA algorithm, neglecting the frequency of speed advisory by the GLOSA system. Specifically, some studies provide speed advisory profile at each decision step, resulting in redundant advisory, while others calculate the optimal speed for the vehicle only once, which cannot adapt to dynamic traffic. In this paper, we propose an Adaptive Frequency GLOSA (AF-GLOSA) model based on Hybrid Proximal Policy Optimization (H-PPO), which employs an actor-critic architecture with a hybrid actor network. The hybrid actor network consists of a discrete actor that outputs advisory frequency and a continuous actor that outputs acceleration profiles. Additionally, we design a no
    
[^90]: 利用选项改进模仿学习对抗性示范的性能表现

    Divide and Repair: Using Options to Improve Performance of Imitation Learning Against Adversarial Demonstrations. (arXiv:2306.04581v1 [cs.LG])

    [http://arxiv.org/abs/2306.04581](http://arxiv.org/abs/2306.04581)

    本文提出了一种利用选项改进模仿学习对抗性示范的性能表现的新技术，可以识别未被对手显着修改的演示轨迹并仅从中进行学习。

    

    在本文中，我们考虑了从教师或专家的演示中学习执行任务的问题，并提出了一种新的技术，可以识别未被对手显着修改的演示轨迹的部分，并使用时间上扩展的策略或选项进行学习。首先，我们定义了一种基于演示轨迹的空间和时间特征的轨迹分歧度量，以检测和丢弃已被对手显着修改的轨迹部分，并可能降低学习者的性能，如果用于学习。然后，我们使用基于选项的算法来分割轨迹，并只从已确定为可接受的轨迹部分中进行学习。我们提供了我们技术的理论结果，以表明修复部分轨迹可以改善学习效果。

    We consider the problem of learning to perform a task from demonstrations given by teachers or experts, when some of the experts' demonstrations might be adversarial and demonstrate an incorrect way to perform the task. We propose a novel technique that can identify parts of demonstrated trajectories that have not been significantly modified by the adversary and utilize them for learning, using temporally extended policies or options. We first define a trajectory divergence measure based on the spatial and temporal features of demonstrated trajectories to detect and discard parts of the trajectories that have been significantly modified by an adversarial expert, and, could degrade the learner's performance, if used for learning, We then use an options-based algorithm that partitions trajectories and learns only from the parts of trajectories that have been determined as admissible. We provide theoretical results of our technique to show that repairing partial trajectories improves the 
    
[^91]: 以双策略为自模型的规划

    Dual policy as self-model for planning. (arXiv:2306.04440v1 [cs.AI])

    [http://arxiv.org/abs/2306.04440](http://arxiv.org/abs/2306.04440)

    该论文探究了使用精简策略网络作为自我模型的优缺点，并通过实验结果表明，使用经过精简的策略网络作为自我模型可以稳定训练，并且有更快的收敛速度。

    

    规划是一种数据有效的决策策略，代理通过探索可能的未来状态来选择候选动作。当存在高维行动空间时，为了模拟未来状态，必须使用自己的决策策略来限制所需探索的动作数量。我们将用于模拟自己决策的模型称为代理的自我模型。尽管在规划行动时，世界模型通常与自我模型一起隐含地使用，但如何设计自我模型仍不清楚。受当前强化学习方法和神经科学的启发，我们探讨了使用精简策略网络作为自我模型的优缺点。在这样的双策略代理中，一个无模型策略和一个经过精简的策略分别用于无模型动作和计划动作。我们在一个生态相关的参数环境上的结果表明，使用经过精简的策略网络作为自我模型可以稳定训练，并且有更快的收敛速度。

    Planning is a data efficient decision-making strategy where an agent selects candidate actions by exploring possible future states. To simulate future states when there is a high-dimensional action space, the knowledge of one's decision making strategy must be used to limit the number of actions to be explored. We refer to the model used to simulate one's decisions as the agent's self-model. While self-models are implicitly used widely in conjunction with world models to plan actions, it remains unclear how self-models should be designed. Inspired by current reinforcement learning approaches and neuroscience, we explore the benefits and limitations of using a distilled policy network as the self-model. In such dual-policy agents, a model-free policy and a distilled policy are used for model-free actions and planned actions, respectively. Our results on a ecologically relevant, parametric environment indicate that distilled policy network for self-model stabilizes training, has faster i
    
[^92]: 基于贝叶斯视角的随机优化端到端学习方法

    End-to-End Learning for Stochastic Optimization: A Bayesian Perspective. (arXiv:2306.04174v1 [math.OC])

    [http://arxiv.org/abs/2306.04174](http://arxiv.org/abs/2306.04174)

    本文提出了一种基于贝叶斯视角的随机优化端到端学习方法，为经验风险最小化和分布式鲁棒优化问题提供新的端到端学习算法，方式主要是训练决策映射。该方法在合成的newsvendor问题和经济分配问题上均表现出显著的效果，同时也发现决策映射神经网络架构对测试性能的影响很大。

    

    我们提出了一种基于贝叶斯视角的随机优化端到端学习方法，该方法采用了标准端到端学习算法的思想，训练了一个后验贝叶斯行动映射。在此基础上，我们为解决经验风险最小化和分布式鲁棒优化问题提出了新的端到端学习算法。通过合成的newsvendor问题和基于真实数据的经济分配问题的数值结果，我们展示了不同训练方案之间的关键差异以及决策映射神经网络架构对测试性能的影响。

    We develop a principled approach to end-to-end learning in stochastic optimization. First, we show that the standard end-to-end learning algorithm admits a Bayesian interpretation and trains a posterior Bayes action map. Building on the insights of this analysis, we then propose new end-to-end learning algorithms for training decision maps that output solutions of empirical risk minimization and distributionally robust optimization problems, two dominant modeling paradigms in optimization under uncertainty. Numerical results for a synthetic newsvendor problem illustrate the key differences between alternative training schemes. We also investigate an economic dispatch problem based on real data to showcase the impact of the neural network architecture of the decision maps on their test performance.
    
[^93]: 大规模分布式学习的拟牛顿更新

    Quasi-Newton Updating for Large-Scale Distributed Learning. (arXiv:2306.04111v1 [cs.LG])

    [http://arxiv.org/abs/2306.04111](http://arxiv.org/abs/2306.04111)

    本文提出了一种具有出色统计、计算和通信效率的分布式拟牛顿(DQN)框架，与现有方法相比，它不需要牛顿矩阵求逆或通信，并且通过理论证明和数值分析证明其统计特性和有限的样本性能。

    

    分布式计算对于现代统计分析至关重要。本文提出了一种具有出色的统计、计算和通信效率的分布式拟牛顿(DQN)框架。在DQN方法中，不需要牛顿矩阵求逆或通信，这大大减少了所提出方法的计算和通信复杂性。值得注意的是，现有的相关方法只分析数值收敛，并需要发散的迭代次数才能收敛。然而，我们研究了DQN方法的统计特性，并在温和条件下理论上证明了结果估计器在少量迭代下的统计效率。广泛的数值分析证明了有限的样本性能。

    Distributed computing is critically important for modern statistical analysis. Herein, we develop a distributed quasi-Newton (DQN) framework with excellent statistical, computation, and communication efficiency. In the DQN method, no Hessian matrix inversion or communication is needed. This considerably reduces the computation and communication complexity of the proposed method. Notably, related existing methods only analyze numerical convergence and require a diverging number of iterations to converge. However, we investigate the statistical properties of the DQN method and theoretically demonstrate that the resulting estimator is statistically efficient over a small number of iterations under mild conditions. Extensive numerical analyses demonstrate the finite sample performance.
    
[^94]: Quick-Tune：快速学习应该使用哪个预训练模型以及如何微调它

    Quick-Tune: Quickly Learning Which Pretrained Model to Finetune and How. (arXiv:2306.03828v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.03828](http://arxiv.org/abs/2306.03828)

    本文提出一种方法快速选择最佳的预训练模型和微调超参数，通过生成大规模元数据集并元学习多保真度性能预测器，并在学习新数据集时使用该预测器进行超参数优化，可以快速实现此目标。

    

    随着预训练模型数量的不断增加，机器学习从业者不断面临一个问题：应该使用哪个预训练模型以及该如何微调它以适应新的数据集。本文提出了一种方法，联合搜索最佳的预训练模型和微调超参数。我们的方法通过评估超过20k个超参数配置在87个数据集上微调24个预训练图像分类模型来生成大规模元数据集，并在其学习曲线上元学习多保真度性能预测器，以用于快速超参数优化。我们的实证研究表明，我们的方法能够快速选择一个准确的预训练模型并找到它的最佳超参数。

    With the ever-increasing number of pretrained models, machine learning practitioners are continuously faced with which pretrained model to use, and how to finetune it for a new dataset. In this paper, we propose a methodology that jointly searches for the optimal pretrained model and the hyperparameters for finetuning it. Our method transfers knowledge about the performance of many pretrained models with multiple hyperparameter configurations on a series of datasets. To this aim, we evaluated over 20k hyperparameter configurations for finetuning 24 pretrained image classification models on 87 datasets to generate a large-scale meta-dataset. We meta-learn a multi-fidelity performance predictor on the learning curves of this meta-dataset and use it for fast hyperparameter optimization on new datasets. We empirically demonstrate that our resulting approach can quickly select an accurate pretrained model for a new dataset together with its optimal hyperparameters.
    
[^95]: 分散化SGD和平均方向SAM在渐近意义下是等价的

    Decentralized SGD and Average-direction SAM are Asymptotically Equivalent. (arXiv:2306.02913v1 [cs.LG])

    [http://arxiv.org/abs/2306.02913](http://arxiv.org/abs/2306.02913)

    分散SGD和平均方向SAM在渐近意义下是等价的，D-SGD表现出梯度平滑效应和锐度正则化效应，而且可以提高后验评估，并证明了潜在的泛化能力

    

    分散随机梯度下降（D-SGD）允许在没有中央服务器的控制下，大量设备同时进行协作学习。然而，现有理论认为，分散化不可避免地削弱了泛化能力。本文挑战传统信念，提出了完全新的角度来理解分散学习。我们证明了在一般非凸非-$\beta$-平滑设置下，D-SGD隐式地最小化了平均方向锐度感知最小化（SAM）算法的损失函数。这种惊人的渐近等价揭示了内在的正则化-优化权衡以及分散化的三个优点：（1）D-SGD中存在一个自由的不确定性评估机制，可以提高后验估计；（2）D-SGD表现出梯度平滑效应；（3）D-SGD的锐度正则化效应不会随着总批处理大小的增加而减少，这证明了潜在的泛化能力

    Decentralized stochastic gradient descent (D-SGD) allows collaborative learning on massive devices simultaneously without the control of a central server. However, existing theories claim that decentralization invariably undermines generalization. In this paper, we challenge the conventional belief and present a completely new perspective for understanding decentralized learning. We prove that D-SGD implicitly minimizes the loss function of an average-direction Sharpness-aware minimization (SAM) algorithm under general non-convex non-$\beta$-smooth settings. This surprising asymptotic equivalence reveals an intrinsic regularization-optimization trade-off and three advantages of decentralization: (1) there exists a free uncertainty evaluation mechanism in D-SGD to improve posterior estimation; (2) D-SGD exhibits a gradient smoothing effect; and (3) the sharpness regularization effect of D-SGD does not decrease as total batch size increases, which justifies the potential generalization b
    
[^96]: LLM-Blender: 利用成对排名和生成融合集成大型语言模型

    LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion. (arXiv:2306.02561v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.02561](http://arxiv.org/abs/2306.02561)

    本论文提出了LLM-Blender，它是一个集成框架，旨在利用不同的开源大型语言模型的优秀特性，实现始终如一的卓越性能。PairRanker和GenFuser是该框架的两个模块，PairRanker使用成对比较方法来区分候选输出，并且GenFuser旨在合并排名最高的候选者，以生成改进的输出。

    

    本论文提出了LLM-Blender，一个集成框架，旨在通过利用多个开源大型语言模型（LLMs）的不同优势来达到始终如一的卓越性能。我们的框架由两个模块组成：PairRanker和GenFuser，以应对不同示例的最优LLMs可以显着变化的观察。PairRanker使用专门的成对比较方法来区分候选输出之间的微小差异。它联合编码输入文本和一对候选者，使用交叉注意编码器来确定优越者。我们的结果表明，PairRanker与ChatGPT的排名相关性最高。然后，GenFuser旨在合并排名最高的候选者，通过利用它们的优势和减少它们的弱点来生成改进的输出。为了促进大规模评估，我们介绍了一个基准数据集MixInstruct，它是多个指令数据集的混合，具有oracle p。

    We present LLM-Blender, an ensembling framework designed to attain consistently superior performance by leveraging the diverse strengths of multiple open-source large language models (LLMs). Our framework consists of two modules: PairRanker and GenFuser, addressing the observation that optimal LLMs for different examples can significantly vary. PairRanker employs a specialized pairwise comparison method to distinguish subtle differences between candidate outputs. It jointly encodes the input text and a pair of candidates, using cross-attention encoders to determine the superior one. Our results demonstrate that PairRanker exhibits the highest correlation with ChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates, generating an improved output by capitalizing on their strengths and mitigating their weaknesses. To facilitate large-scale evaluation, we introduce a benchmark dataset, MixInstruct, which is a mixture of multiple instruction datasets featuring oracle p
    
[^97]: 高效象征交流编码的演变

    Evolution of Efficient Symbolic Communication Codes. (arXiv:2306.02383v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.02383](http://arxiv.org/abs/2306.02383)

    本文探讨了通过反熵、压缩因子和跨分裂F1分数为目标的交流代码演变产物，发现语言结构形成可以通过这些度量来驱动。

    

    本文探讨人类自然语言结构如何被看作是人际交流代码的演变产物，旨在最大化文化无关和跨语言度量标准，如反熵、压缩因子和跨分裂F1分数。探索是作为更大的无监督语言学习努力的一部分完成的，试图在基于“基本语言结构”的超参数空间中执行元学习，通过最大化上述指标来实现。本文提出了针对俄语、中文和英语的跨语言词级分词标记化研究以及针对英语的子词分割或形态分析研究的初步结果。发现语言结构形成词级分割或标记化可以通过所有这些度量来驱动，反熵对英语和俄语更相关，而压缩因子对中文更具特定性。

    The paper explores how the human natural language structure can be seen as a product of evolution of inter-personal communication code, targeting maximisation of such culture-agnostic and cross-lingual metrics such as anti-entropy, compression factor and cross-split F1 score. The exploration is done as part of a larger unsupervised language learning effort, the attempt is made to perform meta-learning in a space of hyper-parameters maximising F1 score based on the "ground truth" language structure, by means of maximising the metrics mentioned above. The paper presents preliminary results of cross-lingual word-level segmentation tokenisation study for Russian, Chinese and English as well as subword segmentation or morphological parsing study for English. It is found that language structure form the word-level segmentation or tokenisation can be found as driven by all of these metrics, anti-entropy being more relevant to English and Russian while compression factor more specific for Chin
    
[^98]: 预训练视觉语言模型适应方法的鲁棒性基准测试研究

    Benchmarking Robustness of Adaptation Methods on Pre-trained Vision-Language Models. (arXiv:2306.02080v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.02080](http://arxiv.org/abs/2306.02080)

    研究针对预训练视觉语言模型的11种适应方法在不同污染情况下的鲁棒性，发现适应方法对文本污染更敏感，单独使用小型文本适配器比共享适配器更鲁棒，可获得可比较的干净性能。

    

    提升预训练视觉语言模型在特定领域表现的各种适应方法，如 LoRA、prompts 和 adapters 等已被提出。然而，这些适应方法对于分布位移的鲁棒性尚未得到研究。本研究评估了11种广泛使用的适应方法在4个视觉语言数据集上的鲁棒性，考察了可用适应示例和适应过程中可训练参数大小的影响。具体地，引入了7个基准数据集，包括96种视觉和87种文本污损，以研究不同适应方法的鲁棒性。我们的分析揭示了：1）适应方法对文本污染比视觉污染更敏感。2) 全量微调并不总能提供最高的鲁棒性；相反，适配器可以实现更好的鲁棒性，并具有可比较的干净性能。3）与预期相反，我们的发现表明，单独使用小型文本适配器通常比在视觉和语言空间中共享适配器更鲁棒。

    Various adaptation methods, such as LoRA, prompts, and adapters, have been proposed to enhance the performance of pre-trained vision-language models in specific domains. The robustness of these adaptation methods against distribution shifts have not been studied. In this study, we assess the robustness of 11 widely-used adaptation methods across 4 vision-language datasets under multimodal corruptions. Concretely, we introduce 7 benchmark datasets, including 96 visual and 87 textual corruptions, to investigate the robustness of different adaptation methods, the impact of available adaptation examples, and the influence of trainable parameter size during adaptation. Our analysis reveals that: 1) Adaptation methods are more sensitive to text corruptions than visual corruptions. 2) Full fine-tuning does not consistently provide the highest robustness; instead, adapters can achieve better robustness with comparable clean performance. 3) Contrary to expectations, our findings indicate that i
    
[^99]: DVFO：DNN边缘推理的动态电压、频率缩放和工作负载卸载

    DVFO: Dynamic Voltage, Frequency Scaling and Workload Offloading for DNN Edge Inference. (arXiv:2306.01811v1 [cs.LG])

    [http://arxiv.org/abs/2306.01811](http://arxiv.org/abs/2306.01811)

    提出了一种动态电压、频率缩放和工作负载卸载的DNN边缘推理框架DVFO，它通过深度强化学习来联合优化DVFS和卸载参数，实现了对边缘设备计算资源的优化，同时提高了DNN模型的能源效率。

    

    由于边缘设备资源限制和深度神经网络（DNN）模型的不同特性，优化边缘设备上DNN推理性能（在能源消耗和推理延迟方面）是一个巨大的挑战。除了动态电压频率缩放（DVFS）技术，边缘云架构提供了一种协作方法，以实现高效的DNN推理。然而，当前的边缘云协作推理方法尚未对边缘设备上的各种计算资源进行优化。因此，我们提出了DVFO，这是一种新颖的基于DVFS的边缘云协作推理框架，它通过深度强化学习（DRL）联合优化DVFS和卸载参数。具体来说，DVFO自动共同优化了1）边缘设备的CPU、GPU和内存频率，以及2）要卸载到云服务器的特征映射。此外，它利用一种思考即行动的并发机制加速DRL学习过程，并利用空间通道关注机制进一步降低DNN模型的能源消耗。在真实数据集上的实验结果表明，DVFO在推理准确度和能源效率方面均优于现有的先进方法。

    Due to edge device resource constraints and different characteristics of deep neural network (DNN) models, it is a big challenge to optimize DNN inference performance in terms of energy consumption and inference latency on edge devices. In addition to the dynamic voltage frequency scaling (DVFS) technique, the edge-cloud architecture provides a collaborative approach to efficient DNN inference. However, current edge-cloud collaborative inference methods have not optimized various compute resources on edge devices. Thus, we propose DVFO, a novel DVFS-enabled edge-cloud collaborative inference framework, which jointly optimize DVFS and offloading parameters via deep reinforcement learning (DRL). Specifically, DVFO automatically co-optimizes 1) CPU, GPU and memory frequencies of edge devices, and 2) feature maps to be offloaded to cloud servers. In addition, it leverages a thinking-while-moving concurrent mechanism to accelerate the DRL learning process, and a spatialchannel attention mec
    
[^100]: DeepfakeArt Challenge: 用于生成AI艺术伪造和数据污染检测的基准数据集

    DeepfakeArt Challenge: A Benchmark Dataset for Generative AI Art Forgery and Data Poisoning Detection. (arXiv:2306.01272v1 [cs.CV])

    [http://arxiv.org/abs/2306.01272](http://arxiv.org/abs/2306.01272)

    本文介绍了DeepfakeArt Challenge，这是一个专门为帮助构建机器学习算法以进行生成AI艺术伪造和数据污染检测而设计的大规模挑战基准数据集。

    

    生成人工智能技术的巨大进步在各种应用中带来了显着的成功和前景，范围从会话代理和文本内容生成到语音和视觉合成。在生成AI的崛起和其越来越广泛的采用中，对于生成AI的恶意用途存在着显着的日益增长的关注。在使用生成AI进行视觉内容合成的领域中，重要的关注领域是图像伪造（例如，生成包含或派生自版权内容的图像）和数据污染（即生成被敌对污染的图像）。为了解决这些关键问题，鼓励负责任的生成AI，我们推出了DeepfakeArt Challenge，一个大型挑战基准数据集，专门设计用于帮助构建机器学习算法以进行生成AI艺术伪造和数据污染检测。

    The tremendous recent advances in generative artificial intelligence techniques have led to significant successes and promise in a wide range of different applications ranging from conversational agents and textual content generation to voice and visual synthesis. Amid the rise in generative AI and its increasing widespread adoption, there has been significant growing concern over the use of generative AI for malicious purposes. In the realm of visual content synthesis using generative AI, key areas of significant concern has been image forgery (e.g., generation of images containing or derived from copyright content), and data poisoning (i.e., generation of adversarially contaminated images). Motivated to address these key concerns to encourage responsible generative AI, we introduce the DeepfakeArt Challenge, a large-scale challenge benchmark dataset designed specifically to aid in the building of machine learning algorithms for generative AI art forgery and data poisoning detection. 
    
[^101]: 大批量神经多目标贝叶斯优化

    Large-Batch, Neural Multi-Objective Bayesian Optimization. (arXiv:2306.01095v1 [cs.LG])

    [http://arxiv.org/abs/2306.01095](http://arxiv.org/abs/2306.01095)

    本文提出了一种针对数据密集型问题和多目标优化设置的贝叶斯优化框架，该方法利用了贝叶斯神经网络代理建模和可扩展、具有不确定性的收购策略，能够在最少迭代次数的情况下高效地进行优化。

    

    贝叶斯优化在全局优化黑盒高成本函数方面提供了强大的框架。然而，由于默认高斯过程代理的可扩展性差，它在处理数据密集型问题，特别是在多目标设置中的能力有限。本文提出了一种新颖的贝叶斯优化框架，专为解决这些限制而设计。我们的方法利用了贝叶斯神经网络方法进行代理建模。这使得它能够有效地处理大批量数据，建模复杂问题以及产生预测的不确定性。此外，我们的方法结合了一种基于众所周知且易于部署的NSGA-II的可扩展的、具有不确定性的收购策略。这种完全可并行化的策略促进了未勘探区域的有效探索。我们的框架允许在最少迭代次数的情况下在数据密集环境中进行有效的优化。我们展示了我们方法的优越性。

    Bayesian optimization provides a powerful framework for global optimization of black-box, expensive-to-evaluate functions. However, it has a limited capacity in handling data-intensive problems, especially in multi-objective settings, due to the poor scalability of default Gaussian Process surrogates. We present a novel Bayesian optimization framework specifically tailored to address these limitations. Our method leverages a Bayesian neural networks approach for surrogate modeling. This enables efficient handling of large batches of data, modeling complex problems, and generating the uncertainty of the predictions. In addition, our method incorporates a scalable, uncertainty-aware acquisition strategy based on the well-known, easy-to-deploy NSGA-II. This fully parallelizable strategy promotes efficient exploration of uncharted regions. Our framework allows for effective optimization in data-intensive environments with a minimum number of iterations. We demonstrate the superiority of ou
    
[^102]: 可控图像生成的扩散自导方法

    Diffusion Self-Guidance for Controllable Image Generation. (arXiv:2306.00986v1 [cs.CV])

    [http://arxiv.org/abs/2306.00986](http://arxiv.org/abs/2306.00986)

    本论文提出了一种扩散自导方法，通过引导扩散模型的内部表示来提供对生成图像的更大控制，可以用于执行具有挑战性的图像操作，同时不需要额外模型或训练。

    

    大规模生成模型能够从详细文本描述中生成高质量的图像。然而，图像的许多方面很难或不可能通过文本来传达。我们引入了自导方法，通过引导扩散模型的内部表示来提供对生成图像的更大控制。我们展示了可以从这些表示中提取出对象的形状、位置和外观等属性并用于指导采样。自导类似于分类器引导，但是使用预训练模型本身中存在的信号，不需要额外的模型或训练。我们展示了如何组合一组简单的属性来执行具有挑战性的图像操作，例如修改对象的位置或大小，将一个图像中的对象外观与另一个图像的布局相结合，将多个图像的对象组合成一个，等等。我们还展示了自导可以用于编辑真实图像。

    Large-scale generative models are capable of producing high-quality images from detailed text descriptions. However, many aspects of an image are difficult or impossible to convey through text. We introduce self-guidance, a method that provides greater control over generated images by guiding the internal representations of diffusion models. We demonstrate that properties such as the shape, location, and appearance of objects can be extracted from these representations and used to steer sampling. Self-guidance works similarly to classifier guidance, but uses signals present in the pretrained model itself, requiring no additional models or training. We show how a simple set of properties can be composed to perform challenging image manipulations, such as modifying the position or size of objects, merging the appearance of objects in one image with the layout of another, composing objects from many images into one, and more. We also show that self-guidance can be used to edit real images
    
[^103]: TriSig：评估三元组簇的统计显著性

    TriSig: Assessing the statistical significance of triclusters. (arXiv:2306.00643v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.00643](http://arxiv.org/abs/2306.00643)

    本文提出了一个新的统计框架用于评估张量数据中模式的显著性，该框架扩展了矩阵数据模式的统计学原理，并从多个生物化学和生物技术领域的实际案例研究中收集了结果。

    

    张量数据分析能够揭示从矩阵数据中无法获得的新型模式和关系。从这些模式中推断的信息提供了有关疾病进展、生物生产过程、天气波动和群体动力学的宝贵的见解。然而，虚假和冗余的模式会妨碍这个过程。本文旨在提出一个统计框架，评估张量数据模式偏离空期望的概率，扩展现有用于评估矩阵数据模式统计显著性的基本原则。本文详细讨论了针对误报发现的二项测试，包括：变量依赖性、时间依赖性和对Benjamini-Hochberg程序下的p值修正的分析。作者通过在生物化学和生物技术领域中不同的实际案例研究中应用最先进的三元组聚类算法来收集结果。

    Tensor data analysis allows researchers to uncover novel patterns and relationships that cannot be obtained from matrix data alone. The information inferred from the patterns provides valuable insights into disease progression, bioproduction processes, weather fluctuations, and group dynamics. However, spurious and redundant patterns hamper this process. This work aims at proposing a statistical frame to assess the probability of patterns in tensor data to deviate from null expectations, extending well-established principles for assessing the statistical significance of patterns in matrix data. A comprehensive discussion on binomial testing for false positive discoveries is entailed at the light of: variable dependencies, temporal dependencies and misalignments, and \textit{p}-value corrections under the Benjamini-Hochberg procedure. Results gathered from the application of state-of-the-art triclustering algorithms over distinct real-world case studies in biochemical and biotechnologic
    
[^104]: 特定上下文独立关系下的因果可模仿性

    Causal Imitability Under Context-Specific Independence Relations. (arXiv:2306.00585v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.00585](http://arxiv.org/abs/2306.00585)

    本文探讨了特定上下文独立关系对因果模仿学习的影响，证明了这种情况下的模仿可行性决策问题是NP难的，并提供了必要的图形标准以及一个有声的算法方法。

    

    最近认识到，在执行模仿学习时忽略因果机制的缺点。文献中提出了几种方法来评估模仿的可行性以及规避因果混淆和因果偏差，但是对于关于潜在因果结构的其他信息的潜在好处却未被探索。这些忽略的信息之一是特定上下文独立性（CSI），即仅在某些上下文中保持独立性。当已知CSI关系时，我们考虑了因果模仿学习问题。我们证明了在这种情况下关于模仿可行性的决策问题是NP难的。此外，我们提供了在CSI下的模仿学习必要的图形标准，并表明在一种结构假设下，这一标准也是充分的。最后，我们提出了一个有声的算法方法用于基于因果的模仿学习，该算法考虑了CSI关系。

    Drawbacks of ignoring the causal mechanisms when performing imitation learning have recently been acknowledged. Several approaches both to assess the feasibility of imitation and to circumvent causal confounding and causal misspecifications have been proposed in the literature. However, the potential benefits of the incorporation of additional information about the underlying causal structure are left unexplored. An example of such overlooked information is context-specific independence (CSI), i.e., independence that holds only in certain contexts. We consider the problem of causal imitation learning when CSI relations are known. We prove that the decision problem pertaining to the feasibility of imitation in this setting is NP-hard. Further, we provide a necessary graphical criterion for imitation learning under CSI and show that under a structural assumption, this criterion is also sufficient. Finally, we propose a sound algorithmic approach for causal imitation learning which takes 
    
[^105]: 使预训练模型具有可逆性：从参数到内存高效的微调

    Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning. (arXiv:2306.00477v1 [cs.CL])

    [http://arxiv.org/abs/2306.00477](http://arxiv.org/abs/2306.00477)

    本研究尝试实现在预训练语言模型中运用可逆模型实现高效的微调，并发现在初始化微调时保留PLM的起点非常重要。

    

    预训练语言模型（PLM）的参数高效微调已经成为一种非常成功的方法，只需训练少量参数而不会降低性能，并随着PLM越来越大而成为事实上的学习范式。然而，现有的PEFT方法不具备内存效率，因为它们仍需要存储大部分中间激活值以便计算梯度，类似于微调。一个减少激活内存的有效方法是应用可逆模型，这样中间激活值就无需缓存，可以重新计算。然而，将PLM修改为它的可逆变体并进行PEFT并不是一件容易的事，因为可逆模型具有与当前发布的PLM不同的体系结构。本文首先调查现有PEFT方法成功的关键因素，认识到在初始化PEFT时保留PLM的起点是至关重要的。

    Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs) has emerged as a highly successful approach, with training only a small number of parameters without sacrificing performance and becoming the de-facto learning paradigm with the increasing size of PLMs. However, existing PEFT methods are not memory-efficient, because they still require caching most of the intermediate activations for the gradient calculation, akin to fine-tuning. One effective way to reduce the activation memory is to apply a reversible model, so the intermediate activations are not necessary to be cached and can be recomputed. Nevertheless, modifying a PLM to its reversible variant with PEFT is not straightforward, since the reversible model has a distinct architecture from the currently released PLMs. In this paper, we first investigate what is a key factor for the success of existing PEFT methods, and realize that it's essential to preserve the PLM's starting point when initializing a PEFT 
    
[^106]: 使用子图特定因子嵌入归一化改善GNN的表达能力

    Improving Expressivity of GNNs with Subgraph-specific Factor Embedded Normalization. (arXiv:2305.19903v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19903](http://arxiv.org/abs/2305.19903)

    本文提出了一种名为SuperNorm的专用归一化方案，通过嵌入子图特定因子和纳入图实例特定统计数据来加强GNN的代表性能力，实现对节点感应子图中内部连接信息的明确考虑，从而改善GNN的表达能力。

    

    图神经网络（GNN）已经成为一类处理图结构数据的强大学习架构。然而，现有的GNN通常忽略了节点感应子图中的重要结构特征，从而限制了它们在各种下游任务中的表达能力。本文旨在通过设计一种专用的即插即用归一化方案——SUbgraph-sPEcific FactoR Embedded Normalization（SuperNorm）来加强GNN的代表性能力，该方案明确考虑了每个节点感应子图内部连接的信息。为此，我们在标准的BatchNorm开始和结束时嵌入了子图特定因子，并纳入图实例特定统计数据以提高区分能力。同时，我们提供了理论分析支持，指出通过改善的SuperNorm，任意GNN至少与1-WL测试一样能够区分非同构图。

    Graph Neural Networks~(GNNs) have emerged as a powerful category of learning architecture for handling graph-structured data. However, existing GNNs typically ignore crucial structural characteristics in node-induced subgraphs, which thus limits their expressiveness for various downstream tasks. In this paper, we strive to strengthen the representative capabilities of GNNs by devising a dedicated plug-and-play normalization scheme, termed as SUbgraph-sPEcific FactoR Embedded Normalization (SuperNorm), that explicitly considers the intra-connection information within each node-induced subgraph. To this end, we embed the subgraph-specific factor at the beginning and the end of the standard BatchNorm, as well as incorporate graph instance-specific statistics for improved distinguishable capabilities. In the meantime, we provide theoretical analysis to support that, with the elaborated SuperNorm, an arbitrary GNN is at least as powerful as the 1-WL test in distinguishing non-isomorphism gr
    
[^107]: 基于扩散式语言模型的细粒度文本风格转换

    Fine-grained Text Style Transfer with Diffusion-Based Language Models. (arXiv:2305.19512v1 [cs.CL])

    [http://arxiv.org/abs/2305.19512](http://arxiv.org/abs/2305.19512)

    本文提出了一种基于扩散式语言模型的细粒度文本风格转换方法，在不依赖外部信息的情况下取得了比之前利用预训练权重、嵌入和外部语法分析器更好的效果，表明扩散概率模型在文本生成领域具有广泛的应用前景。

    

    扩散式概率模型已经在可控制地生成高质量图像上显示出了巨大的成功，研究人员已经试图将这种可控性运用到文本生成领域。以前的扩散式语言模型研究表明，它们可以在不需要外部知识（如预训练权重）的情况下进行训练，并且仍然可以实现稳定的性能和可控性。 在本文中，我们在StylePTB数据集上训练了一个扩散式模型，这是细粒度文本风格转换的标准基准。与以前的工作评估任务相比，StylePTB中的任务需要对输出文本进行更加精细的控制，我们的模型能够在StylePTB上实现卓越的性能，包括个别和组合转换。此外，我们的模型在没有外部知识的情况下使用StylePTB的有限数据进行训练，其表现优于以前利用预训练权重、嵌入和外部语法分析器的工作，这可能表明扩散概率模型在文本生成领域具有巨大的潜力。

    Diffusion probabilistic models have shown great success in generating high-quality images controllably, and researchers have tried to utilize this controllability into text generation domain. Previous works on diffusion-based language models have shown that they can be trained without external knowledge (such as pre-trained weights) and still achieve stable performance and controllability. In this paper, we trained a diffusion-based model on StylePTB dataset, the standard benchmark for fine-grained text style transfers. The tasks in StylePTB requires much more refined control over the output text compared to tasks evaluated in previous works, and our model was able to achieve state-of-the-art performance on StylePTB on both individual and compositional transfers. Moreover, our model, trained on limited data from StylePTB without external knowledge, outperforms previous works that utilized pretrained weights, embeddings, and external grammar parsers, and this may indicate that diffusion
    
[^108]: SimFBO：简单、灵活且通信高效的联邦双层学习

    SimFBO: Towards Simple, Flexible and Communication-efficient Federated Bilevel Learning. (arXiv:2305.19442v1 [cs.LG])

    [http://arxiv.org/abs/2305.19442](http://arxiv.org/abs/2305.19442)

    SimFBO和其ShroFBO变体提出了一个简单、灵活且通信高效的FBO框架，可以应用于元学习和超参数优化任务。

    

    近来，由于元学习、微调、超参数调整等领域中嵌套优化结构的出现，联邦双层优化（FBO）在机器学习和边缘计算中显示了巨大的潜力。然而，现有的FBO算法往往涉及复杂的计算，并需要每次迭代多个子循环，每个子循环包含多个通信轮。在本文中，我们提出了一个名为SimFBO的简单灵活的FBO框架，它易于实现，不需要子循环，并包括一种广义的服务器端聚合和更新以提高通信效率。我们进一步提出了系统级异构鲁棒FBO（ShroFBO）作为SimFBO的变体，其对本地计算的异构有更强的鲁棒性。我们证明了在部分客户端参与和无替换的客户端采样下，SimFBO和ShroFBO可以实现线性收敛加速，同时改进了样本和通信复杂度。实验证明了它们在图像分类数据集的元学习和真实世界数据集上的超参数优化任务中的有效性。

    Federated bilevel optimization (FBO) has shown great potential recently in machine learning and edge computing due to the emerging nested optimization structure in meta-learning, fine-tuning, hyperparameter tuning, etc. However, existing FBO algorithms often involve complicated computations and require multiple sub-loops per iteration, each of which contains a number of communication rounds. In this paper, we propose a simple and flexible FBO framework named SimFBO, which is easy to implement without sub-loops, and includes a generalized server-side aggregation and update for improving communication efficiency. We further propose System-level heterogeneity robust FBO (ShroFBO) as a variant of SimFBO with stronger resilience to heterogeneous local computation. We show that SimFBO and ShroFBO provably achieve a linear convergence speedup with partial client participation and client sampling without replacement, as well as improved sample and communication complexities. Experiments demons
    
[^109]: 缓解上下文学习的标签偏差

    Mitigating Label Biases for In-context Learning. (arXiv:2305.19148v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.19148](http://arxiv.org/abs/2305.19148)

    本文针对上下文学习（ICL）中的三种标签偏差提出分类法，并提出一种简单的偏差校准方法，使用随机的领域词估算语言模型的标签偏差。

    

    上下文学习（ICL）的各种设计设置，如选择和顺序的上下文示例，可能使模型对某种特定预测偏见，而这种预测并不反映对任务的理解。虽然许多研究讨论了这些设计选择，但对它们进行分类和减缓其影响的系统调查很少。在本文中，我们为文本分类中上下文学习（ICL）中的三种标签偏差定义了一个分类法：香草标签偏差、上下文标签偏差和领域标签偏差（我们首次概念化和检测到）。我们的分析表明，先前的标签偏差校准方法不能解决所有三种偏差。特别是，领域标签偏差使LLM在许多任务上只能实现随机级别的性能，而不管上下文示例的选择如何。为了缓解这些偏差的影响，我们提出一个简单的偏差校准方法，使用随机的领域词估算语言模型的标签偏差。

    Various design settings for in-context learning (ICL), such as the choice and order of the in-context examples, can bias a model toward a particular prediction without being reflective of an understanding of the task. While many studies discuss these design choices, there have been few systematic investigations into categorizing them and mitigating their impact. In this work, we define a typology for three types of label biases in ICL for text classification: vanilla-label bias, context-label bias, and domain-label bias (which we conceptualize and detect for the first time).  Our analysis demonstrates that prior label bias calibration methods fall short of addressing all three types of biases. Specifically, domain-label bias restricts LLMs to random-level performance on many tasks regardless of the choice of in-context examples. To mitigate the effect of these biases, we propose a simple bias calibration method that estimates a language model's label bias using random in-domain words f
    
[^110]: W-procer: 基于加权原型对比学习的医学少样本命名实体识别

    W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition. (arXiv:2305.18624v1 [cs.CL])

    [http://arxiv.org/abs/2305.18624](http://arxiv.org/abs/2305.18624)

    W-procer是一种基于加权原型对比学习的医学少样本命名实体识别方法，在构建基于原型的对比损失和加权网络方面具有创新性，优于现有的最先进方法。

    

    对比学习已成为少样本命名实体识别（NER）的一种受欢迎的解决方案。传统配置力求减少具有相同标签的标记之间的距离，并增加具有不同标签的标记之间的距离。然而，在医学领域中存在大量被注释为“O”（即“OUTSIDE”）的实体，并且它们不希望被推离到当前对比学习方法标记为“O”以外的其他实体，这种设定效果不佳，可能会得出含有噪声原型标签的语义表示，尽管存在许多“O”标签实体与有标签实体相关。为解决这个挑战，我们提出了一种名为医学少样本命名实体识别中基于加权原型的对比学习方法（W-PROCER）。我们的方法主要围绕构建基于原型的对比损失和加权网络展开。这些组件在协助在医学领域中的迁移学习方面发挥了至关重要的作用。在实验中，我们将W-PROCER应用于一个公共的医学数据集，并展示了其相对于现有的最先进方法的优异表现。

    Contrastive learning has become a popular solution for few-shot Name Entity Recognization (NER). The conventional configuration strives to reduce the distance between tokens with the same labels and increase the distance between tokens with different labels. The effect of this setup may, however, in the medical domain, there are a lot of entities annotated as OUTSIDE (O), and they are undesirably pushed apart to other entities that are not labeled as OUTSIDE (O) by the current contrastive learning method end up with a noisy prototype for the semantic representation of the label, though there are many OUTSIDE (O) labeled entities are relevant to the labeled entities. To address this challenge, we propose a novel method named Weighted Prototypical Contrastive Learning for Medical Few Shot Named Entity Recognization (W-PROCER). Our approach primarily revolves around constructing the prototype-based contractive loss and weighting network. These components play a crucial role in assisting t
    
[^111]: 可扩展的弱监督银行交易分类方法

    Scalable and Weakly Supervised Bank Transaction Classification. (arXiv:2305.18430v1 [cs.LG])

    [http://arxiv.org/abs/2305.18430](http://arxiv.org/abs/2305.18430)

    本文提出了一种可扩展的银行交易分类方法，利用弱监督、自然语言处理和深度神经网络技术，最小化对手动注释的依赖，能够快速扩展到新的和组合用例，可用于财务健康报告和信用风险评估等金融应用。

    

    本文旨在利用弱监督、自然语言处理和深度神经网络技术对银行交易进行分类。我们的方法通过利用启发式和领域知识来训练准确的交易分类器，从而最小化对昂贵和难以获取的手动注释的依赖。我们提出了一个有效且可扩展的端到端数据管道，包括数据预处理、交易文本嵌入、锚定、标签生成、判别式神经网络训练以及系统架构概述。我们通过展示它优于现有的市场领先解决方案、实现准确分类并且可以快速扩展到新的和组合用例来证明我们方法的有效性。这反过来可以开启许多金融应用，例如财务健康报告和信用风险评估。

    This paper aims to categorize bank transactions using weak supervision, natural language processing, and deep neural network techniques. Our approach minimizes the reliance on expensive and difficult-to-obtain manual annotations by leveraging heuristics and domain knowledge to train accurate transaction classifiers. We present an effective and scalable end-to-end data pipeline, including data preprocessing, transaction text embedding, anchoring, label generation, discriminative neural network training, and an overview of the system architecture. We demonstrate the effectiveness of our method by showing it outperforms existing market-leading solutions, achieves accurate categorization, and can be quickly extended to novel and composite use cases. This can in turn unlock many financial applications such as financial health reporting and credit risk assessment.
    
[^112]: 使用场景图记忆建模动态环境

    Modeling Dynamic Environments with Scene Graph Memory. (arXiv:2305.17537v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.17537](http://arxiv.org/abs/2305.17537)

    本论文提出了一种新的场景图记忆状态表示，结合节点边缘预测器（NEP）的神经网络架构，能够帮助具有行动能力的AI代理在部分可观察动态场景中高效搜索。

    

    在大型环境中，如居室等，寻找物品的具有行动能力的AI代理需要基于部分信息预测物品位置来做出有效决策。我们将其形式化为一种新类型的链路预测问题：部分可观察动态图上的链路预测。我们的图表达了一个场景，其中房间和物品是节点，在边缘中编码它们之间的关系；在每个时间步骤上，代理人仅知道更改图的部分。这种部分可观测性对于现有的链路预测方法构成了挑战，我们进行了解决。我们提出了一种新颖的状态表示 - 场景图记忆（SGM） - 其中包括代理人的累积观察集合，以及一种名为节点边缘预测器（NEP）的神经网络架构，该架构从SGM中提取信息以进行高效搜索。我们在动态房屋模拟器中评估了我们的方法，这是一个新的基准，它按照语义模式创建不同的动态图形。

    Embodied AI agents that search for objects in large environments such as households often need to make efficient decisions by predicting object locations based on partial information. We pose this as a new type of link prediction problem: link prediction on partially observable dynamic graphs. Our graph is a representation of a scene in which rooms and objects are nodes, and their relationships are encoded in the edges; only parts of the changing graph are known to the agent at each timestep. This partial observability poses a challenge to existing link prediction approaches, which we address. We propose a novel state representation -- Scene Graph Memory (SGM) -- with captures the agent's accumulated set of observations, as well as a neural net architecture called a Node Edge Predictor (NEP) that extracts information from the SGM to search efficiently. We evaluate our method in the Dynamic House Simulator, a new benchmark that creates diverse dynamic graphs following the semantic patte
    
[^113]: 从黑盒模型到可解释模型的转化，用于高效的迁移学习

    Distilling BlackBox to Interpretable models for Efficient Transfer Learning. (arXiv:2305.17303v1 [cs.CV])

    [http://arxiv.org/abs/2305.17303](http://arxiv.org/abs/2305.17303)

    本论文提出了一种方法可以将黑盒模型转化成为可解释模型并在目标领域成功进行迁移学习，从而实现高效迁移学习。

    

    建立具有普适性的AI模型是医疗领域面临的主要挑战之一。神经网络（NN）模型即使输入分布轻微移位（例如扫描仪类型），也会受到影响，而放射科医生则依赖于异常性的通用描述性规则。微调模型以将知识从一个领域转移到另一个领域需要大量标记数据。在本文中，我们开发了一种可解释的模型，它可以在计算成本最小的情况下，高效地针对未知的目标域进行微调。我们认为NN的可解释组件大致是域不变的。然而，可解释模型通常表现不及它们的BB变体。在源域中我们先使用人类理解的概念从BB开始，将其提炼成一组浅显易懂的interpretable模型。由于每个interpretable模型都覆盖了数据的一个子集，具有一组interpretable模型的混合可以实现与BB相当的性能。

    Building generalizable AI models is one of the primary challenges in the healthcare domain. While radiologists rely on generalizable descriptive rules of abnormality, Neural Network (NN) models suffer even with a slight shift in input distribution (\eg scanner type). Fine-tuning a model to transfer knowledge from one domain to another requires a significant amount of labeled data in the target domain. In this paper, we develop an interpretable model that can be efficiently fine-tuned to an unseen target domain with minimal computational cost. We assume the interpretable component of NN to be approximately domain-invariant. However, interpretable models typically underperform compared to their Blackbox (BB) variants. We start with a BB in the source domain and distill it into a \emph{mixture} of shallow interpretable models using human-understandable concepts. As each interpretable model covers a subset of data, a mixture of interpretable models achieves comparable performance as BB. Fu
    
[^114]: 重新审视广义p-Laplacian正则化框架图卷积网络: 收敛性、能量动态和非线性扩散训练的研究

    Revisiting Generalized p-Laplacian Regularized Framelet GCNs: Convergence, Energy Dynamic and Training with Non-Linear Diffusion. (arXiv:2305.15639v1 [cs.LG])

    [http://arxiv.org/abs/2305.15639](http://arxiv.org/abs/2305.15639)

    本文全面分析了基于图p-Laplacian的Framelet网络，证明了其具有良好的收敛性和能量动态，并能够通过广义非线性扩散过程进行多种训练。同时，该模型能够适应同质和异质数据，避免过度平滑问题。

    

    本文对基于图p-Laplacian的Framelet网络（pL-UFG）进行了全面的理论分析，以建立对其性质的深入理解。我们首先对Framelet卷积后集成p-Laplacian的隐式层进行了收敛性分析，提供了关于pL-UFG渐近行为的洞察力。通过探索pL-UFG的广义Dirichlet能量，我们证明了Dirichlet能量保持非零，确保在pL-UFG接近收敛时避免过度平滑问题。此外，我们通过动态能量视角阐明了pL-UFG中的隐式层与图Framelets协同工作，增强了该模型对同质和异质数据的适应性。值得注意的是，我们证明了这个隐式层可以被解释成广义的非线性扩散过程，使得可以使用多种不同的训练方案。这些多方面的分析导致了统一的结论，提供了新的洞见。

    This work presents a comprehensive theoretical analysis of graph p-Laplacian based framelet network (pL-UFG) to establish a solid understanding of its properties. We begin by conducting a convergence analysis of the p-Laplacian based implicit layer integrated after the framelet convolution, providing insights into the asymptotic behavior of pL-UFG. By exploring the generalized Dirichlet energy of pL-UFG, we demonstrate that the Dirichlet energy remains non-zero, ensuring the avoidance of over-smoothing issues in pL-UFG as it approaches convergence. Furthermore, we elucidate the dynamic energy perspective through which the implicit layer in pL-UFG synergizes with graph framelets, enhancing the model's adaptability to both homophilic and heterophilic data. Remarkably, we establish that the implicit layer can be interpreted as a generalized non-linear diffusion process, enabling training using diverse schemes. These multifaceted analyses lead to unified conclusions that provide novel insi
    
[^115]: 黑盒子与灰盒子：关于学习带有自旋和碰撞的乒乓球轨迹预测的案例研究

    Black-Box vs. Gray-Box: A Case Study on Learning Table Tennis Ball Trajectory Prediction with Spin and Impacts. (arXiv:2305.15189v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2305.15189](http://arxiv.org/abs/2305.15189)

    本研究提出了一种灰盒子方法来预测乒乓球轨迹，比两种黑盒子方法预测性能更好。使用神经网络从球发射器的参数中初始化自旋可以显着提高预测性能，并通过评估实现了高回球率。

    

    本文提出了一种乒乓球轨迹过滤和预测方法。我们的灰盒子方法基于物理模型构建，并使用数据来学习动力学模型，扩展卡尔曼滤波器和神经模型的参数，以推断球的初始状态。与未考虑先验物理知识的两种黑盒子方法相比，我们的方法表现出卓越的预测性能。我们证明使用神经网络从球发射器的参数中初始化自旋，显着提高了长时间预测性能，相比仅从测量的球位置估计自旋。准确预测球的轨迹对于成功回球至关重要。因此，我们评估了一个充气人工肌肉机器人的回球性能，并实现了29/30（97.7％）的回球率。

    In this paper, we present a method for table tennis ball trajectory filtering and prediction. Our gray-box approach builds on a physical model. At the same time, we use data to learn parameters of the dynamics model, of an extended Kalman filter, and of a neural model that infers the ball's initial condition. We demonstrate superior prediction performance of our approach over two black-box approaches, which are not supplied with physical prior knowledge. We demonstrate that initializing the spin from parameters of the ball launcher using a neural network drastically improves long-time prediction performance over estimating the spin purely from measured ball positions. An accurate prediction of the ball trajectory is crucial for successful returns. We therefore evaluate the return performance with a pneumatic artificial muscular robot and achieve a return rate of 29/30 (97.7%).
    
[^116]: 利用（模糊测试）测试用例来理解程序

    Understanding Programs by Exploiting (Fuzzing) Test Cases. (arXiv:2305.13592v1 [cs.LG])

    [http://arxiv.org/abs/2305.13592](http://arxiv.org/abs/2305.13592)

    本文提出了通过模糊测试获取代表性输入来帮助语义理解程序的方法。

    

    程序的语义理解引起了社区的极大关注。受到自然语言理解中大型语言模型（LLM）的最近成功启发，通过将编程语言视为另一种自然语言，并在程序代码语料库上训练LLM，取得了巨大进展。然而，程序毕竟与文本有本质的区别，因为它们通常具有严格的结构和语法。特别是，程序及其基本单元（即函数和子程序）旨在展示各种行为和/或提供可能的输出，给定不同的输入。输入和可能的输出/行为之间的关系表示函数/子程序，并概述了整个程序。因此，我们提出将这种关系纳入学习中，以实现对程序的更深入语义理解。为了获得足够代表性的输入以触发大量执行，可以使用模糊测试。

    Semantic understanding of programs has attracted great attention in the community. Inspired by recent successes of large language models (LLMs) in natural language understanding, tremendous progress has been made by treating programming language as another sort of natural language and training LLMs on corpora of program code. However, programs are essentially different from texts after all, in a sense that they are normally heavily structured and syntax-strict. In particular, programs and their basic units (i.e., functions and subroutines) are designed to demonstrate a variety of behaviors and/or provide possible outputs, given different inputs. The relationship between inputs and possible outputs/behaviors represents the functions/subroutines and profiles the program as a whole. Therefore, we propose to incorporate such a relationship into learning, for achieving a deeper semantic understanding of programs. To obtain inputs that are representative enough to trigger the execution of mo
    
[^117]: NTK逼近有效的紧凑条件。

    Tight conditions for when the NTK approximation is valid. (arXiv:2305.13141v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.13141](http://arxiv.org/abs/2305.13141)

    我们研究了NTK逼近在平方损失下训练模型的有效性，发现在模型缩放因子$\alpha=O(T)$时在训练时间$T$之前可以使得NTK逼近有效。

    

    我们研究了神经切线核（NTK）逼近在使用平方损失函数训练模型时何时有效。在Chizat等人2019年的懒惰训练设置下，我们证明通过因子为$\alpha=O(T)$的模型缩放就足以使NTK逼近在训练时间$T$之前有效。我们的界限是紧凑的，并且改善了Chizat等人2019年的先前界限，后者需要更大的缩放因子$\alpha=O(T^2)$。

    We study when the neural tangent kernel (NTK) approximation is valid for training a model with the square loss. In the lazy training setting of Chizat et al. 2019, we show that rescaling the model by a factor of $\alpha = O(T)$ suffices for the NTK approximation to be valid until training time $T$. Our bound is tight and improves on the previous bound of Chizat et al. 2019, which required a larger rescaling factor of $\alpha = O(T^2)$.
    
[^118]: 稳定性、泛化性和隐私保护：对于随机特征和NTK特征的精确分析

    Stability, Generalization and Privacy: Precise Analysis for Random and NTK Features. (arXiv:2305.12100v1 [stat.ML])

    [http://arxiv.org/abs/2305.12100](http://arxiv.org/abs/2305.12100)

    本论文研究了ERM训练模型对抗强大黑盒攻击的安全问题，并通过两个指标量化模型安全性：单个样本的稳定性和查询与原始数据特征的对齐。在研究中，通过研究RF和NTK回归，证明随着泛化能力的提高，隐私保护可以得到加强。

    

    深度学习模型容易受到恢复攻击，引起用户隐私保护的担忧。针对经验风险最小化（ERM）等常见算法通常不能直接实施安全保障的问题，本文研究了ERM训练模型对抗特定强大黑盒子攻击的安全问题。我们的分析通过两个看似不同但有联系的指标来量化模型安全性：一是相对于单个训练样本的模型稳定性，另一个是攻击查询和原始数据特征的特征对齐。虽然前者在学习理论中已经得到了很好的阐述，并与经典工作中的泛化误差相关，但在我们的研究中，第二种特性是新颖的。我们的关键技术结果为两种原型设置提供了特征对齐的精确刻画：随机特征（RF）和神经切向核（NTK）回归。这证明，随着泛化能力的提高，隐私保护能够得到加强，同时揭示了其他有趣的性质。

    Deep learning models can be vulnerable to recovery attacks, raising privacy concerns to users, and widespread algorithms such as empirical risk minimization (ERM) often do not directly enforce safety guarantees. In this paper, we study the safety of ERM-trained models against a family of powerful black-box attacks. Our analysis quantifies this safety via two separate terms: (i) the model stability with respect to individual training samples, and (ii) the feature alignment between the attacker query and the original data. While the first term is well established in learning theory and it is connected to the generalization error in classical work, the second one is, to the best of our knowledge, novel. Our key technical result provides a precise characterization of the feature alignment for the two prototypical settings of random features (RF) and neural tangent kernel (NTK) regression. This proves that privacy strengthens with an increase in the generalization capability, unveiling also
    
[^119]: 一种可扩展的Walsh-Hadamard正则化器，以克服神经网络的低阶谱偏差

    A Scalable Walsh-Hadamard Regularizer to Overcome the Low-degree Spectral Bias of Neural Networks. (arXiv:2305.09779v1 [cs.LG])

    [http://arxiv.org/abs/2305.09779](http://arxiv.org/abs/2305.09779)

    本文提出了一种新型的可扩展Walsh-Hadamard正则化器，可避免神经网络学习低阶频率以及误识别低阶频率，从而提高了神经网络的泛化性能。

    

    尽管神经网络具有学习任意函数的能力，但通过梯度下降训练的模型常常表现出对“更简单”函数的偏好。本文通过傅里叶（Walsh-Hadamard）变换，从离散（零一）输入的神经网络的角度探讨了简单性的概念，其中可以通过傅里叶系数的“阶”来捕捉简单性概念。我们实证表明神经网络有学习较低阶频率的趋势。我们展示了这种谱偏差向较简单特征的趋势实际上会损害神经网络在真实世界数据集上的泛化能力。为了解决这个问题，我们提出了一种新的可扩展的功能正则化方案，以帮助神经网络学习更高的阶频率。我们的正则化器还有助于避免对低阶频率的错误识别，从而进一步提高了泛化能力。我们在计算机视觉、自然语言处理和语音识别中应用各种神经网络架构进行分类任务的广泛评估。我们的实验结果表明，我们的正则化器在低数据量环境下显著提高了泛化性能。

    Despite the capacity of neural nets to learn arbitrary functions, models trained through gradient descent often exhibit a bias towards ``simpler'' functions. Various notions of simplicity have been introduced to characterize this behavior. Here, we focus on the case of neural networks with discrete (zero-one) inputs through the lens of their Fourier (Walsh-Hadamard) transforms, where the notion of simplicity can be captured through the \emph{degree} of the Fourier coefficients. We empirically show that neural networks have a tendency to learn lower-degree frequencies. We show how this spectral bias towards simpler features can in fact \emph{hurt} the neural network's generalization on real-world datasets. To remedy this we propose a new scalable functional regularization scheme that aids the neural network to learn higher degree frequencies. Our regularizer also helps avoid erroneous identification of low-degree frequencies, which further improves generalization. We extensively evaluat
    
[^120]: 无需残差计算的车辆检测与分类：通过随机扰动注入加速HEVC图像解码。

    Vehicle Detection and Classification without Residual Calculation: Accelerating HEVC Image Decoding with Random Perturbation Injection. (arXiv:2305.08265v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.08265](http://arxiv.org/abs/2305.08265)

    本文介绍了一种用于交通监控的新方法，通过随机扰动重建图像，而不使用残差数据，从而显著减少图像重构所需的数据。该方法通过创建原始图像的精简表示，并保留与视频理解任务相关的信息，重点关注车辆检测和分类等关键用例。

    

    在视频分析领域，尤其是交通监控方面，需要有效的高效视频数据处理和理解方法。传统的视频解码技术计算密集且时间耗费大，因此研究人员开始在压缩域中探索替代方法。本文提出了一种基于随机扰动的压缩域方法，特别针对交通监控应用，在HEVC比特流中重建图像。据我们所知，我们的方法是首个建议使用随机扰动代替残差数值，创造出原始图像的精简表示，并保留与视频理解任务相关的信息，重点关注车辆检测和分类。通过不使用残差数据，我们的方法显著减少了图像重构所需的数据。

    In the field of video analytics, particularly traffic surveillance, there is a growing need for efficient and effective methods for processing and understanding video data. Traditional full video decoding techniques can be computationally intensive and time-consuming, leading researchers to explore alternative approaches in the compressed domain. This study introduces a novel random perturbation-based compressed domain method for reconstructing images from High Efficiency Video Coding (HEVC) bitstreams, specifically designed for traffic surveillance applications. To the best of our knowledge, our method is the first to propose substituting random perturbations for residual values, creating a condensed representation of the original image while retaining information relevant to video understanding tasks, particularly focusing on vehicle detection and classification as key use cases.  By not using residual data, our proposed method significantly reduces the data needed in the image recon
    
[^121]: 因果信息分离：为抗分布转移设计代理特征

    Causal Information Splitting: Engineering Proxy Features for Robustness to Distribution Shifts. (arXiv:2305.05832v1 [cs.LG])

    [http://arxiv.org/abs/2305.05832](http://arxiv.org/abs/2305.05832)

    本文提出了利用因果机制在不同环境下保持不变的直觉来主动准备的代理特征选择和工程技术，用以应对统计预测模型在分布转移情况下的稳定性问题。

    

    统计预测模型通常是在与最终使用情况不同的概率分布中进行训练的。为了预测分布转移，有一种方法是利用因果机制在不同环境下保持不变的直觉来主动准备。本文针对一个具有挑战性的场景，其中目标的因果和反因果变量都是未被观察到的。利用信息论，我们为下游观测变量开发了特征选择和工程技术，这些变量充当代理。我们选择有助于建立稳定模型的代理，并使用辅助训练任务从代理中提取增强稳定性的信息。我们在合成数据和真实数据上展示了我们技术的有效性。

    Statistical prediction models are often trained on data that is drawn from different probability distributions than their eventual use cases. One approach to proactively prepare for these shifts harnesses the intuition that causal mechanisms should remain invariant between environments. Here we focus on a challenging setting in which the causal and anticausal variables of the target are unobserved. Leaning on information theory, we develop feature selection and engineering techniques for the observed downstream variables that act as proxies. We identify proxies that help to build stable models and moreover utilize auxiliary training tasks to extract stability-enhancing information from proxies. We demonstrate the effectiveness of our techniques on synthetic and real data.
    
[^122]: 可扩展的最优边缘分布机（Scalable Optimal Margin Distribution Machine）

    Scalable Optimal Margin Distribution Machine. (arXiv:2305.04837v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.04837](http://arxiv.org/abs/2305.04837)

    本文提出了一种可扩展的最优边缘分布机（ODM）训练方法，与原始ODM训练方法相比，可实现近十倍的加速。对于非线性核，我们提出了一种新颖的分布感知分区方法，使得每个分区上训练的本地ODM接近全局的ODM，并快速收敛。在应用线性核时，我们扩展了一种通信有效的SVRG方法以进一步加速训练。

    

    最优边缘分布机（ODM）是一种新型的统计学习框架，根据新的边缘理论建立，表现出比传统的基于大间隔的对应方法更好的泛化性能。然而，像其他核方法一样，它在计算时间和内存方面普遍存在可扩展性问题。本文提出了一种可扩展的ODM，与原始ODM训练方法相比，可实现近十倍的加速。对于非线性核，我们提出了一种新颖的分布感知分区方法，使得每个分区上训练的本地ODM接近全局的ODM，并快速收敛。当应用线性核时，我们扩展了一种通信有效的SVRG方法以进一步加速训练。大量经验证据表明，我们提出的方法在计算效率方面极高，并且几乎不会恶化泛化性能。

    Optimal margin Distribution Machine (ODM) is a newly proposed statistical learning framework rooting in the novel margin theory, which demonstrates better generalization performance than the traditional large margin based counterparts. Nonetheless, it suffers from the ubiquitous scalability problem regarding both computation time and memory as other kernel methods. This paper proposes a scalable ODM, which can achieve nearly ten times speedup compared to the original ODM training method. For nonlinear kernels, we propose a novel distribution-aware partition method to make the local ODM trained on each partition be close and converge fast to the global one. When linear kernel is applied, we extend a communication efficient SVRG method to accelerate the training further. Extensive empirical studies validate that our proposed method is highly computational efficient and almost never worsen the generalization.
    
[^123]: ChatGPT生成的代码真的正确吗？对大型语言模型在代码生成方面的严格评估

    Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation. (arXiv:2305.01210v1 [cs.SE])

    [http://arxiv.org/abs/2305.01210](http://arxiv.org/abs/2305.01210)

    本论文提出了一个严格的代码综合基准评估框架EvalPlus，用于评估利用大型语言模型生成的代码的功能正确性。

    

    程序综合一直以来都是被长期研究的领域，最近的方法集中于直接利用大型语言模型(LLMs)根据自然语言中用户的意图生成代码。代码评估数据集，包含策划好的综合问题和各种输入/输出测试用例，被用来衡量各种LLMs在代码综合上的性能。然而，这些数据集中的测试用例在完全评估生成代码的功能正确性方面，数量和质量都可能有所限制。这种现有基准中的限制引出了以下问题：在LLMs时代，生成的代码真的正确吗？为了回答这个问题，我们提出了EvalPlus——一个评估LLM-synthesized代码功能正确性的严格基准评估框架。EvalPlus接受基础评估数据集，并利用自动输入生成步骤，使用LLM-based和基于变异的方法生成和多样化大量新的测试输入。

    Program synthesis has been long studied with recent approaches focused on directly using the power of Large Language Models (LLMs) to generate code according to user intent written in natural language. Code evaluation datasets, containing curated synthesis problems with input/output test-cases, are used to measure the performance of various LLMs on code synthesis. However, test-cases in these datasets can be limited in both quantity and quality for fully assessing the functional correctness of the generated code. Such limitation in the existing benchmarks begs the following question: In the era of LLMs, is the code generated really correct? To answer this, we propose EvalPlus -- a code synthesis benchmarking framework to rigorously evaluate the functional correctness of LLM-synthesized code. In short, EvalPlus takes in the base evaluation dataset and uses an automatic input generation step to produce and diversify large amounts of new test inputs using both LLM-based and mutation-based
    
[^124]: 生存分析的适当评分规则研究

    Proper Scoring Rules for Survival Analysis. (arXiv:2305.00621v1 [stat.ME])

    [http://arxiv.org/abs/2305.00621](http://arxiv.org/abs/2305.00621)

    本文研究了适用于生存分析的四种评分规则的扩展，证明在概率分布估计离散化程度满足一定条件时是适当评分规则，并且比较结果显示对数得分和布莱尔得分的扩展最佳。

    

    生存分析是估计未来事件发生时间的概率分布的问题，可以看作是不确定性量化问题。尽管有关于严格适当评分规则的基本理论用于不确定性量化，但很少有人了解其在生存分析中的应用。本文研究了常用的四种严格适当评分规则在生存分析中的扩展，并证明这些扩展在概率分布估计的离散化程度满足一定条件时是适当评分规则。我们还使用真实数据集比较了这些扩展评分规则的估计性能，结果表明对数得分和布莱尔得分的扩展表现最佳。

    Survival analysis is the problem of estimating probability distributions for future event times, which can be seen as a problem in uncertainty quantification. Although there are fundamental theories on strictly proper scoring rules for uncertainty quantification, little is known about those for survival analysis. In this paper, we investigate extensions of four major strictly proper scoring rules for survival analysis and we prove that these extensions are proper under certain conditions, which arise from the discretization of the estimation of probability distributions. We also compare the estimation performances of these extended scoring rules by using real datasets, and the extensions of the logarithmic score and the Brier score performed the best.
    
[^125]: 使用同态加密对大规模CNN进行敏感调整以进行端到端安全预测

    Sensitive Tuning of Large Scale CNNs for E2E Secure Prediction using Homomorphic Encryption. (arXiv:2304.14836v1 [cs.LG])

    [http://arxiv.org/abs/2304.14836](http://arxiv.org/abs/2304.14836)

    本论文提出一种新的HE友好模型训练方法，成功演示了在ResNet和ConvNeXt等经典和现代CNN上运行加密样本，并以前所未有的方式演示了如何使用CLIP GPT-2模型进行零知识安全预测，是一种可行的隐私保护机器学习解决方案。

    

    隐私保护的机器学习解决方案近来受到了广泛关注。其中一种有前途的研究趋势是使用同态加密（HE），这是一种在加密数据上执行计算的方法。这种方法的一个主要挑战是训练适用于HE的加密或未加密的深层CNN，以实现良好的准确性。我们提出了一种新的HE友好模型训练方法，并在基本和现代CNN上进行了演示，例如ResNet和ConvNeXt。训练后，我们使用HELayers SDK运行加密样本来评估我们的模型，并证明它们产生了所需的结果。在ImageNet数据集上运行时，我们的ResNet-18/50/101实现仅需要7、31和57分钟，这表明这个解决方案是实用的。此外，我们提供了一些关于在HE下处理激活函数和跳跃连接的见解。最后，我们以前所未有的方式演示了如何使用CLIP GPT-2模型进行零知识安全预测。

    Privacy-preserving machine learning solutions have recently gained significant attention. One promising research trend is using Homomorphic Encryption (HE), a method for performing computation over encrypted data. One major challenge in this approach is training HE-friendly, encrypted or unencrypted, deep CNNs with decent accuracy. We propose a novel training method for HE-friendly models, and demonstrate it on fundamental and modern CNNs, such as ResNet and ConvNeXt. After training, we evaluate our models by running encrypted samples using HELayers SDK and proving that they yield the desired results. When running on a GPU over the ImageNet dataset, our ResNet-18/50/101 implementations take only 7, 31 and 57 minutes, respectively, which shows that this solution is practical. Furthermore, we present several insights on handling the activation functions and skip-connections under HE. Finally, we demonstrate in an unprecedented way how to perform secure zero-shot prediction using a CLIP m
    
[^126]: ChartSumm：长短摘要自动生成任务的全面基准数据集

    ChartSumm: A Comprehensive Benchmark for Automatic Chart Summarization of Long and Short Summaries. (arXiv:2304.13620v1 [cs.CL])

    [http://arxiv.org/abs/2304.13620](http://arxiv.org/abs/2304.13620)

    本文提出了ChartSumm数据集，用于长短摘要自动生成任务，包括84000多个图表及其元数据和描述。研究发现，现有的自动摘要模型虽然得分不错，但经常面临错觉、漏掉重要数据点以及不正确解释复杂趋势等问题。

    

    自动将图表转换为文本摘要是视障人士的有效工具，同时为用户提供表格数据的自然语言精确洞察力。大型、结构良好的数据集始终是数据驱动模型的关键部分。本文提出了ChartSumm：一个大规模基准数据集，包括共84363个图表及其元数据和描述，涵盖广泛的主题和图表类型，可生成长短摘要。强基线模型的广泛实验表明，尽管这些模型通过实现各种自动评估指标的得分来生成流畅且信息丰富的摘要，但它们经常遇到一些问题，例如产生错觉，漏掉重要的数据点，以及不正确地解释图表中的复杂趋势。我们还通过自动翻译工具探讨了将ChartSumm扩展到其他语言的潜力。这使得我们的数据集成为一个有挑战的任务。

    Automatic chart to text summarization is an effective tool for the visually impaired people along with providing precise insights of tabular data in natural language to the user. A large and well-structured dataset is always a key part for data driven models. In this paper, we propose ChartSumm: a large-scale benchmark dataset consisting of a total of 84,363 charts along with their metadata and descriptions covering a wide range of topics and chart types to generate short and long summaries. Extensive experiments with strong baseline models show that even though these models generate fluent and informative summaries by achieving decent scores in various automatic evaluation metrics, they often face issues like suffering from hallucination, missing out important data points, in addition to incorrect explanation of complex trends in the charts. We also investigated the potential of expanding ChartSumm to other languages using automated translation tools. These make our dataset a challeng
    
[^127]: (向量)空间不是最后的疆域：将产品搜索看作程序合成

    (Vector) Space is Not the Final Frontier: Product Search as Program Synthesis. (arXiv:2304.11473v1 [cs.IR])

    [http://arxiv.org/abs/2304.11473](http://arxiv.org/abs/2304.11473)

    本文主张将产品搜索看作程序合成，相比向量空间模型有着重大优势。

    

    随着电子商务的不断增长，巨额投资用于信息检索的机器学习和自然语言处理也随之而来。虽然向量空间模型主宰了产品搜索中的检索模型，但随着深度学习的出现，向量化本身也发生了巨大变化。我们的立场论文以相反的方式主张，即程序合成对许多查询和市场中的大量参与者提供了重大优势。我们详细说明了所提出方法的行业重要性，概述了具体实现细节，并基于我们在Tooso构建类似系统的经验，回答了一些常见的反对意见。

    As ecommerce continues growing, huge investments in ML and NLP for Information Retrieval are following. While the vector space model dominated retrieval modelling in product search - even as vectorization itself greatly changed with the advent of deep learning -, our position paper argues in a contrarian fashion that program synthesis provides significant advantages for many queries and a significant number of players in the market. We detail the industry significance of the proposed approach, sketch implementation details, and address common objections drawing from our experience building a similar system at Tooso.
    
[^128]: 语言指导下的强化学习以实现人工智能协作

    Language Instructed Reinforcement Learning for Human-AI Coordination. (arXiv:2304.07297v1 [cs.AI])

    [http://arxiv.org/abs/2304.07297](http://arxiv.org/abs/2304.07297)

    本文提出了一种称之为instructRL的新的框架，它通过自然语言指令来指定对人工智能搭档的预期策略，解决在缺乏高质量人类行为数据的领域中多智能体强化学习收敛于人类不偏爱的策略的问题，从而提高了人工智能协作的性能。

    

    人工智能的一个基本问题是如何让智能体能够和人类有效地协作。本文提出了一种称之为instructRL的新的框架，让人们可以通过自然语言指令来指定对人工智能搭档的预期策略，以此解决在缺乏较高质量的人类行为数据的领域中，由于多智能体强化学习常常会收敛到人类并不偏爱的策略的不足。我们使用预先训练的大型语言模型来生成一个在人类指令下的先验策略，并将其用于约束强化学习目标。这导致强化学习智能体收敛到与人类喜好一致的均衡点。通过概念证明环境和具有挑战性的Hanabi基准，证明了instructRL收敛于满足给定指令的类似人类智能体的策略。最后，我们证明了知道语言指令显著提高了人工智能协作的性能。

    One of the fundamental quests of AI is to produce agents that coordinate well with humans. This problem is challenging, especially in domains that lack high quality human behavioral data, because multi-agent reinforcement learning (RL) often converges to different equilibria from the ones that humans prefer. We propose a novel framework, instructRL, that enables humans to specify what kind of strategies they expect from their AI partners through natural language instructions. We use pretrained large language models to generate a prior policy conditioned on the human instruction and use the prior to regularize the RL objective. This leads to the RL agent converging to equilibria that are aligned with human preferences. We show that instructRL converges to human-like policies that satisfy the given instructions in a proof-of-concept environment as well as the challenging Hanabi benchmark. Finally, we show that knowing the language instruction significantly boosts human-AI coordination pe
    
[^129]: Vax-Culture: 用于研究推特上疫苗讨论的数据集

    Vax-Culture: A Dataset for Studying Vaccine Discourse on Twitter. (arXiv:2304.06858v1 [cs.SI])

    [http://arxiv.org/abs/2304.06858](http://arxiv.org/abs/2304.06858)

    本文介绍了一个推特疫苗数据集Vax-Culture，它旨在找出推广疫苗错误信息的文化和政治信念的重叠部分，帮助开发机器学习模型以自动检测疫苗错误信息帖子并应对其负面影响。

    

    COVID-19疫情期间，疫苗犹豫继续是公共卫生官员面临的主要挑战。由于该犹豫破坏了疫苗运动，许多研究人员试图确定其根本原因，并发现社交媒体平台上反疫苗错误信息的不断增长是该问题的关键因素。我们将推特作为误导内容的来源，并旨在提取推广疫苗错误信息的文化和政治信念的重叠部分。为此，我们收集了一个与疫苗有关的推文数据集，并借助专业沟通和新闻背景的注释人员进行注释。我们最终希望这可以带来有效和有针对性的公共卫生通信策略，以接触那些持反疫苗信仰者。此外，这些信息有助于开发机器学习模型以自动检测疫苗错误信息帖子并应对其负面影响。

    Vaccine hesitancy continues to be a main challenge for public health officials during the COVID-19 pandemic. As this hesitancy undermines vaccine campaigns, many researchers have sought to identify its root causes, finding that the increasing volume of anti-vaccine misinformation on social media platforms is a key element of this problem. We explored Twitter as a source of misleading content with the goal of extracting overlapping cultural and political beliefs that motivate the spread of vaccine misinformation. To do this, we have collected a data set of vaccine-related Tweets and annotated them with the help of a team of annotators with a background in communications and journalism. Ultimately we hope this can lead to effective and targeted public health communication strategies for reaching individuals with anti-vaccine beliefs. Moreover, this information helps with developing Machine Learning models to automatically detect vaccine misinformation posts and combat their negative impa
    
[^130]: 能量引导的熵神经最优输运

    Energy-guided Entropic Neural Optimal Transport. (arXiv:2304.06094v1 [cs.LG])

    [http://arxiv.org/abs/2304.06094](http://arxiv.org/abs/2304.06094)

    本论文提出了一种新的方法，将能量基础模型和熵正则化最优输运结合起来，以解决生成建模问题。我们在2D情景和图像到图像翻译问题中验证了该方法的适用性。

    

    能量基础模型（EBMs）在机器学习社区已经有数十年的历史。自两千年代起，一直有很多高效的方法通过能量势（非归一化的似然函数）来解决生成建模问题。相比之下，最优输运（OT）领域，尤其是神经OT求解器，受到的探索要少得多，仅有一些近期的研究（不包括利用OT作为损失函数来解决问题的WGAN方法）。在本研究中，我们弥合了EBMs和熵正则化OT之间的差距，提出了一种新的方法，允许利用前者的最新发展和技术改进来丰富后者。我们在2D情景和标准的图像到图像翻译问题中验证了我们方法的适用性。为简单起见，我们选择了简短和长跑的EBMs。

    Energy-Based Models (EBMs) are known in the Machine Learning community for the decades. Since the seminal works devoted to EBMs dating back to the noughties there have been appearing a lot of efficient methods which solve the generative modelling problem by means of energy potentials (unnormalized likelihood functions). In contrast, the realm of Optimal Transport (OT) and, in particular, neural OT solvers is much less explored and limited by few recent works (excluding WGAN based approaches which utilize OT as a loss function and do not model OT maps themselves). In our work, we bridge the gap between EBMs and Entropy-regularized OT. We present the novel methodology which allows utilizing the recent developments and technical improvements of the former in order to enrich the latter. We validate the applicability of our method on toy 2D scenarios as well as standard unpaired image-to-image translation problems. For the sake of simplicity, we choose simple short- and long- run EBMs as a 
    
[^131]: 模型稀疏化可以简化机器反学习

    Model sparsification can simplify machine unlearning. (arXiv:2304.04934v1 [cs.LG])

    [http://arxiv.org/abs/2304.04934](http://arxiv.org/abs/2304.04934)

    本文提出了一种基于模型稀疏化的机器反学习方案，称为prune first, then unlearn和sparsity-aware unlearning。此方案可以提高近似反学习器的多标准反学习性能，并在不同的场景中表现出一致的效果。

    

    最近的数据管制要求机器反学习（MU）：从模型中移除指定样例的影响。虽然可以通过使用剩余数据从头开始进行模型重新训练来进行精确反学习，但是其计算成本导致了近似但高效的反学习方案的开发。除了数据中心的MU解决方案，我们通过一种新颖的基于模型的视角推进MU：通过权值修剪进行稀疏化。我们的理论和实践结果表明，模型稀疏性可以提高近似反学习器的多标准反学习性能，缩小近似间隙，同时保持高效。有了这个认识，我们制定了两个新的稀疏感知反学习元方案，称为“先修剪，然后反学习”和“稀疏感知反学习”。广泛的实验表明，我们的发现和提议在各种场景下始终有益于MU，包括按类数据擦除、随机数据擦除和后门数据伪造等。

    Recent data regulations necessitate machine unlearning (MU): The removal of the effect of specific examples from the model. While exact unlearning is possible by conducting a model retraining with the remaining data from scratch, its computational cost has led to the development of approximate but efficient unlearning schemes. Beyond data-centric MU solutions, we advance MU through a novel model-based viewpoint: sparsification via weight pruning. Our results in both theory and practice indicate that model sparsity can boost the multi-criteria unlearning performance of an approximate unlearner, closing the approximation gap, while continuing to be efficient. With this insight, we develop two new sparsity-aware unlearning meta-schemes, termed `prune first, then unlearn' and `sparsity-aware unlearning'. Extensive experiments show that our findings and proposals consistently benefit MU in various scenarios, including class-wise data scrubbing, random data scrubbing, and backdoor data forge
    
[^132]: 神经网络的增量验证

    Incremental Verification of Neural Networks. (arXiv:2304.01874v1 [cs.LG])

    [http://arxiv.org/abs/2304.01874](http://arxiv.org/abs/2304.01874)

    提出了一种新的、基于设计新的理论、数据结构和算法的神经网络增量与完全验证的通用框架，实现了对MNIST和CIFAR10以及ACAS-XU分类器的更高效的验证。

    

    深度神经网络（DNNs）的完全验证可以确定DNNs是否在无限输入集上满足所需的可信属性（例如，鲁棒性，公正性）。尽管多年来已经取得了极大的进展，以改善完全验证器在单个DNNs上的可扩展性，但是当部署的DNN进行更新以提高其推理速度或准确性时，它们在本质上效率低下。这是因为需要从头开始运行昂贵的验证器来验证更新后的DNN。为了提高效率，我们提出了一种新的、基于设计新的理论、数据结构和算法的增量和完全DNN验证的通用框架。我们的贡献在一个名为IVAN的工具中实现，对于验证MNIST和CIFAR10分类器，我们的总体几何平均加速比为2.4倍，对于ACAS-XU分类器，我们的总体几何平均加速比为3.8倍，超过了现有最先进的基线。

    Complete verification of deep neural networks (DNNs) can exactly determine whether the DNN satisfies a desired trustworthy property (e.g., robustness, fairness) on an infinite set of inputs or not. Despite the tremendous progress to improve the scalability of complete verifiers over the years on individual DNNs, they are inherently inefficient when a deployed DNN is updated to improve its inference speed or accuracy. The inefficiency is because the expensive verifier needs to be run from scratch on the updated DNN. To improve efficiency, we propose a new, general framework for incremental and complete DNN verification based on the design of novel theory, data structure, and algorithms. Our contributions implemented in a tool named IVAN yield an overall geometric mean speedup of 2.4x for verifying challenging MNIST and CIFAR10 classifiers and a geometric mean speedup of 3.8x for the ACAS-XU classifiers over the state-of-the-art baselines.
    
[^133]: 使用置信度增强强化学习在时间知识图上改进少样本归纳学习

    Improving Few-Shot Inductive Learning on Temporal Knowledge Graphs using Confidence-Augmented Reinforcement Learning. (arXiv:2304.00613v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.00613](http://arxiv.org/abs/2304.00613)

    本论文介绍了一种名为FITCARL的TKGC方法，它将少样本学习与强化学习相结合，以实现在时间知识图上进行少样本归纳学习和预测。在FITCARL中，一个智能体通过策略网络来指导搜索过程，通过引入合成样本的模块来解决数据稀缺性，在真实世界数据上取得了最新的结果。

    

    时间知识图补全（TKGC）旨在预测时间知识图（TKG）中实体之间缺失的联系。大多数先前的TKGC方法仅考虑在训练集中看到的实体之间预测缺失的联系，而无法在关于新出现的未见实体的链接预测方面达到良好的性能。最近，提出了一种新的任务，即TKG少样本场景外链接预测，其中TKGC模型需要在关于仅有少量观察样本的新出现实体方面实现良好的链接预测性能。在这项工作中，我们提出了一种名为FITCARL的TKGC方法，该方法将少样本学习与强化学习相结合来解决这个问题。在FITCARL中，一个智能体跨越整个TKG寻找预测答案。我们设计了一个策略网络，根据遍历的路径指导搜索过程。为了更好地解决少样本设置中的数据稀缺问题，我们引入了一个模块，生成合成样本来增加训练集。通过在真实世界数据集上进行广泛实验，我们证明FITCARL在少样本场景外链接预测和传统TKGC任务上均取得了最新的结果。

    Temporal knowledge graph completion (TKGC) aims to predict the missing links among the entities in a temporal knwoledge graph (TKG). Most previous TKGC methods only consider predicting the missing links among the entities seen in the training set, while they are unable to achieve great performance in link prediction concerning newly-emerged unseen entities. Recently, a new task, i.e., TKG few-shot out-of-graph (OOG) link prediction, is proposed, where TKGC models are required to achieve great link prediction performance concerning newly-emerged entities that only have few-shot observed examples. In this work, we propose a TKGC method FITCARL that combines few-shot learning with reinforcement learning to solve this task. In FITCARL, an agent traverses through the whole TKG to search for the prediction answer. A policy network is designed to guide the search process based on the traversed path. To better address the data scarcity problem in the few-shot setting, we introduce a module tha
    
[^134]: NeRF-GAN蒸馏：基于卷积的高效3D感知生成

    NeRF-GAN Distillation for Efficient 3D-Aware Generation with Convolutions. (arXiv:2303.12865v1 [cs.CV])

    [http://arxiv.org/abs/2303.12865](http://arxiv.org/abs/2303.12865)

    本文提出了一种基于神经辐射场（NeRF）和生成对抗网络（GAN）的新方法，通过从预训练的NeRF-GAN中提取3D知识进行蒸馏，实现了基于姿态的2D GAN的高效3D感知生成。

    

    基于姿态的卷积生成模型在从单视图数据集进行高质量的3D一致图像生成方面存在困难，因为它们缺乏足够的3D先验知识。最近，将神经辐射场（NeRF）和生成对抗网络（GAN）等生成模型相结合，从单视图图像中生成3D感知图像，已经引起了广泛关注。NeRF-GAN利用了三维神经表示和体积渲染的强归纳偏差，但也带来了更高的计算复杂性。这项研究旨在通过从预训练的NeRF-GAN中提取3D知识进行蒸馏，重审基于姿态的二维GAN，在推理时间内实现高效的3D感知生成。我们提出了一种简单有效的方法，基于在基于姿态的卷积网络中重用预训练的NeRF-GAN的良好解耦潜在空间，直接生成与潜在的3D表达相对应的3D一致图片。在多个数据集上的实验表明，所提出的方法实现了更高效的3D感知生成。

    Pose-conditioned convolutional generative models struggle with high-quality 3D-consistent image generation from single-view datasets, due to their lack of sufficient 3D priors. Recently, the integration of Neural Radiance Fields (NeRFs) and generative models, such as Generative Adversarial Networks (GANs), has transformed 3D-aware generation from single-view images. NeRF-GANs exploit the strong inductive bias of 3D neural representations and volumetric rendering at the cost of higher computational complexity. This study aims at revisiting pose-conditioned 2D GANs for efficient 3D-aware generation at inference time by distilling 3D knowledge from pretrained NeRF-GANS. We propose a simple and effective method, based on re-using the well-disentangled latent space of a pre-trained NeRF-GAN in a pose-conditioned convolutional network to directly generate 3D-consistent images corresponding to the underlying 3D representations. Experiments on several datasets demonstrate that the proposed met
    
[^135]: GLADE：用于非配对超分辨率各向异性MRI的梯度损失增强退化增强

    GLADE: Gradient Loss Augmented Degradation Enhancement for Unpaired Super-Resolution of Anisotropic MRI. (arXiv:2303.11831v1 [cs.CV])

    [http://arxiv.org/abs/2303.11831](http://arxiv.org/abs/2303.11831)

    本文提出了一种可用于加速全腹MRI扫描的新方法GLADE，它通过使用梯度映射损失来合成高分辨率等向性3D腹部MR图像。

    

    我们提出了一种新方法，在非配对的情况下，从各向异性3D图像中合成高分辨率等向性3D腹部MR图像。通过使用修改后的CycleGAN架构，并使用梯度映射损失，我们利用来自各向异性体积高分辨率（面内）数据的不重叠的补丁，强制网络生成器增加低分辨率（面外）切片的分辨率。这将使在短时间内以高分辨率等向性图像进行全腹扫描成为可能。

    We present a novel approach to synthesise high-resolution isotropic 3D abdominal MR images, from anisotropic 3D images in an unpaired fashion. Using a modified CycleGAN architecture with a gradient mapping loss, we leverage disjoint patches from the high-resolution (in-plane) data of an anisotropic volume to enforce the network generator to increase the resolution of the low-resolution (through-plane) slices. This will enable accelerated whole-abdomen scanning with high-resolution isotropic images within short breath-hold times.
    
[^136]: Reflexion：具有动态记忆和自我反思的自主智能体

    Reflexion: an autonomous agent with dynamic memory and self-reflection. (arXiv:2303.11366v1 [cs.AI])

    [http://arxiv.org/abs/2303.11366](http://arxiv.org/abs/2303.11366)

    本文提出 Reflexion 方法，给智能体赋予了动态记忆和自我反思能力，以增强其任务特定的行动选择能力。

    

    最近决策大型语言模型（LLM）代理的发展在各种基准测试中展现出卓越的性能。然而，这些最先进的方法通常需要内部模型微调、外部模型微调或在定义的状态空间上进行策略优化。由于高质量训练数据的稀缺性或缺乏良好定义的状态空间，实现这些方法可能会具有挑战性。此外，这些代理没有人类决策过程固有的某些品质，特别是从错误中学习的能力。通过反思，人类可以通过试错过程高效地解决新的问题。在最近的研究基础上，我们提出 Reflexion，一种将动态记忆和自我反思能力赋予智能体的方法，以增强其现有的推理轨迹和任务特定的行动选择能力。为了实现完全自动化，我们介绍了一种简单而有效的方法。

    Recent advancements in decision-making large language model (LLM) agents have demonstrated impressive performance across various benchmarks. However, these state-of-the-art approaches typically necessitate internal model fine-tuning, external model fine-tuning, or policy optimization over a defined state space. Implementing these methods can prove challenging due to the scarcity of high-quality training data or the lack of well-defined state space. Moreover, these agents do not possess certain qualities inherent to human decision-making processes, specifically the ability to learn from mistakes. Self-reflection allows humans to efficiently solve novel problems through a process of trial and error. Building on recent research, we propose Reflexion, an approach that endows an agent with dynamic memory and self-reflection capabilities to enhance its existing reasoning trace and task-specific action choice abilities. To achieve full automation, we introduce a straightforward yet effective 
    
[^137]: 数据中心人工智能综述：一份调查报告。

    Data-centric Artificial Intelligence: A Survey. (arXiv:2303.10158v1 [cs.LG])

    [http://arxiv.org/abs/2303.10158](http://arxiv.org/abs/2303.10158)

    本文讨论了数据中心人工智能的必要性并从三个一般性数据中心目标和代表性方法的全面视角进行了介绍。该综述从自动化和协作的角度组织了现有文献并讨论了挑战。

    

    人工智能（AI）正在几乎所有领域产生深远的影响，其成功的关键之一是可用于构建机器学习模型的丰富高质量数据。最近，数据在AI中的作用得到了显著放大，引发了数据中心AI这一新兴概念的出现。研究人员和从业者的注意力逐渐从推进模型设计转向提高数据质量和数量。在本调查中，我们讨论了数据中心AI的必要性，随后从训练数据开发、推理数据开发和数据维护三个一般性数据中心目标以及代表性方法的全面视角进行了介绍。我们还从自动化和协作的角度组织了现有文献，讨论了挑战，并列出了各种任务的测试基准。我们认为，这是第一份提供跨越各个阶段一系列任务的全球视角的综合性调查。

    Artificial Intelligence (AI) is making a profound impact in almost every domain. A vital enabler of its great success is the availability of abundant and high-quality data for building machine learning models. Recently, the role of data in AI has been significantly magnified, giving rise to the emerging concept of data-centric AI. The attention of researchers and practitioners has gradually shifted from advancing model design to enhancing the quality and quantity of the data. In this survey, we discuss the necessity of data-centric AI, followed by a holistic view of three general data-centric goals (training data development, inference data development, and data maintenance) and the representative methods. We also organize the existing literature from automation and collaboration perspectives, discuss the challenges, and tabulate the benchmarks for various tasks. We believe this is the first comprehensive survey that provides a global view of a spectrum of tasks across various stages o
    
[^138]: 在领域泛化中找到能力区域

    Finding Competence Regions in Domain Generalization. (arXiv:2303.09989v1 [cs.LG])

    [http://arxiv.org/abs/2303.09989](http://arxiv.org/abs/2303.09989)

    该论文提出了一个“学习拒绝”框架来解决领域泛化中的默默失败问题。通过预测可信度，该方法在测试分布与训练分布不同的情况下接受超出分布的数据，以识别能力区域。研究发现，通过不同的学习表示衡量无能，增加无能得分会预示着降低准确性。

    

    我们提出了一个“学习拒绝”框架来解决领域泛化中默默失败的问题，即测试分布与训练分布不同的情况。假设有一个温和的分布偏移，我们希望在模型估计的能力预示着可信响应时接受超出分布的数据，而不是直接拒绝超出分布的数据。可信度通过与分类器性能密切相关的代理无能分数进行预测。我们对分类的无能得分进行了全面的实验评估，并强调了拒绝率与准确率之间的权衡。为了与先前的工作进行比较，我们聚焦于标准领域泛化基准，并考虑在闭合和开放世界环境下通过不同的学习表示来衡量无能。我们的结果表明，增加无能分数确实预示着降低准确性，从而导致显着的...

    We propose a "learning to reject" framework to address the problem of silent failures in Domain Generalization (DG), where the test distribution differs from the training distribution. Assuming a mild distribution shift, we wish to accept out-of-distribution (OOD) data whenever a model's estimated competence foresees trustworthy responses, instead of rejecting OOD data outright. Trustworthiness is then predicted via a proxy incompetence score that is tightly linked to the performance of a classifier. We present a comprehensive experimental evaluation of incompetence scores for classification and highlight the resulting trade-offs between rejection rate and accuracy gain. For comparability with prior work, we focus on standard DG benchmarks and consider the effect of measuring incompetence via different learned representations in a closed versus an open world setting. Our results suggest that increasing incompetence scores are indeed predictive of reduced accuracy, leading to significan
    
[^139]: 关于文本向量化技术的鲁棒性研究

    On the Robustness of Text Vectorizers. (arXiv:2303.07203v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.07203](http://arxiv.org/abs/2303.07203)

    本文研究了文本向量化技术中的鲁棒性问题，并证明了流行的嵌入方案具有Hamming距离意义上的鲁棒性。本研究提供了这些方法的定量边界，并展示了其中的常数受文档长度的影响。

    

    机器学习中一个基本问题是模型对输入变化的鲁棒性。在自然语言处理中，模型通常包含第一层嵌入，将词汇序列转换为向量表示。虽然连续输入的稳健性已经被很好地理解，但考虑到离散变化(比如替换句子中的一个词)，情况就不那么明确了。本文正式证明了流行的嵌入方案(如拼接、TF-IDF、段落向量)在Hamming距离意义下表现出鲁棒性，我们为这些方法提供了定量边界，并展示了其中的常数如何受文档长度影响。这些发现通过一系列数值实例加以说明。

    A fundamental issue in machine learning is the robustness of the model with respect to changes in the input. In natural language processing, models typically contain a first embedding layer, transforming a sequence of tokens into vector representations. While the robustness with respect to changes of continuous inputs is well-understood, the situation is less clear when considering discrete changes, for instance replacing a word by another in an input sentence. Our work formally proves that popular embedding schemes, such as concatenation, TF-IDF, and Paragraph Vector (a.k.a. doc2vec), exhibit robustness in the H\"older or Lipschitz sense with respect to the Hamming distance. We provide quantitative bounds for these schemes and demonstrate how the constants involved are affected by the length of the document. These findings are exemplified through a series of numerical examples.
    
[^140]: 单个GPU的高吞吐大语言模型生成推理技术——FlexGen

    FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU. (arXiv:2303.06865v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06865](http://arxiv.org/abs/2303.06865)

    本论文提出了一种名为FlexGen的技术，使用单个GPU实现大型语言模型的高吞吐推理。FlexGen通过聚合来自GPU、CPU和磁盘的内存和计算，搜索有效的张量存储和访问模式，并将权重和注意力缓存压缩为4个位。这些技术使FlexGen具有更大的批量选择空间，从而显著增加了最大吞吐量。

    

    大语言模型(LLM)的高计算和内存需求使其只能在多个高端加速器上实现。本文着眼于对批处理不敏感的任务，并使用有限资源（如单个普通GPU）进行高吞吐量LLM推理的研究。我们提出了FlexGen，一种用于运行具有GPU内存限制的LLMs的高吞吐量生成引擎。通过聚合来自GPU、CPU和磁盘的内存和计算，可以在各种硬件资源约束下灵活配置FlexGen。通过解决线性规划问题，它可以搜索有效的张量存储和访问模式。FlexGen还将权重和注意力缓存压缩为4个位，几乎没有精度损失。这些技术使FlexGen具有更大的批量选择空间，从而显著增加了最大吞吐量。结果，当在单个16GB GPU上运行OPT-175B时，FlexGen的吞吐量为每秒90个序列，比Megatron-LM快4.5倍，比使用单个普通GPU的竞争者快7.5-19倍。

    The high computational and memory requirements of large language model (LLM) inference make it feasible only with multiple high-end accelerators. Motivated by the emerging demand for latency-insensitive tasks with batched processing, this paper initiates the study of high-throughput LLM inference using limited resources, such as a single commodity GPU. We present FlexGen, a high-throughput generation engine for running LLMs with limited GPU memory. FlexGen can be flexibly configured under various hardware resource constraints by aggregating memory and computation from the GPU, CPU, and disk. By solving a linear programming problem, it searches for efficient patterns to store and access tensors. FlexGen further compresses the weights and the attention cache to 4 bits with negligible accuracy loss. These techniques enable FlexGen to have a larger space of batch size choices and thus significantly increase maximum throughput. As a result, when running OPT-175B on a single 16GB GPU, FlexGe
    
[^141]: 基于Transformer的符号回归规划

    Transformer-based Planning for Symbolic Regression. (arXiv:2303.06833v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06833](http://arxiv.org/abs/2303.06833)

    该论文提出了一种基于Transformer和蒙特卡罗树搜索的符号回归规划策略TPSR，可以将非可微的反馈作为知识的外部来源融入到方程生成过程中，提高方程生成的准确性和复杂性。

    

    符号回归是机器学习中一项具有挑战性的任务，它涉及基于函数值查找其数学表达式。最近，符号回归的一些进展表明，预训练的基于Transformer的模型对于生成方程序列是有效的，这些模型从合成数据集的大规模预训练中获益，并在推理时间方面比基于GP的方法具有显著优势。然而，这些模型关注的是借鉴文本生成的监督预训练目标，而忽略了方程的特定目标，如准确性和复杂性。为了解决这个问题，我们提出了TPSR，一种基于Transformer的符号回归规划策略，将蒙特卡罗树搜索融入到Transformer解码过程中。与传统的解码策略不同，TPSR允许将非可微的反馈（如拟合准确性和复杂性）作为知识的外部来源融入到方程生成过程中。

    Symbolic regression (SR) is a challenging task in machine learning that involves finding a mathematical expression for a function based on its values. Recent advancements in SR have demonstrated the efficacy of pretrained transformer-based models for generating equations as sequences, which benefit from large-scale pretraining on synthetic datasets and offer considerable advantages over GP-based methods in terms of inference time. However, these models focus on supervised pretraining goals borrowed from text generation and ignore equation-specific objectives like accuracy and complexity. To address this, we propose TPSR, a Transformer-based Planning strategy for Symbolic Regression that incorporates Monte Carlo Tree Search into the transformer decoding process. TPSR, as opposed to conventional decoding strategies, allows for the integration of non-differentiable feedback, such as fitting accuracy and complexity, as external sources of knowledge into the equation generation process. Ext
    
[^142]: 基于方向流形积的高斯过程研究

    Gaussian Process on the Product of Directional Manifolds. (arXiv:2303.06799v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06799](http://arxiv.org/abs/2303.06799)

    本文提出了一种在超环面上建立高斯过程的方法，并使用内在的核相关模型进行学习，以定义在超环面上的向量值函数。通过使用 HvM-based GP 进行数据驱动递归定位，数值结果表明，在跟踪精度方面，该方法具有优势。

    

    本文提出了一种在方向流形积上建立高斯过程（GPs）的方法，并引入了基于 von Mises 分布的循环核。在此基础上，我们提出了所谓的超环维 von Mises（HvM）核，以考虑循环关联组件来建立超环面上的高斯过程。通过使用内在的核相关模型，运用多输出 GP 回归进行学习，用于定义在超环面上的向量值函数。为运行时关键应用程序提供了超参数优化的分析导数。为了评估所提出的方法，我们合成了基于距离的传感器网络，并采用 HvM-based GP 进行数据驱动递归定位。数值结果表明，与参数模型和基于传统核设计的高斯过程相比，HvM-based GP 实现了更优的跟踪精度。

    We introduce a principled study on establishing Gaussian processes (GPs) with inputs on the product of directional manifolds. A circular kernel is first presented according to the von Mises distribution. Based thereon, the so-called hypertoroidal von Mises (HvM) kernel is proposed to establish GPs on hypertori with consideration of correlational circular components. The proposed HvM kernel is demonstrated with multi-output GP regression for learning vector-valued functions defined on hypertori using the intrinsic coregionalization model. Analytical derivatives in hyperparameter optimization are provided for runtime-critical applications. For evaluation, we synthesize a ranging-based sensor network and employ the HvM-based GPs for data-driven recursive localization. The numerical results show that the HvM-based GP achieves superior tracking accuracy compared to parametric model and GPs based on conventional kernel designs.
    
[^143]: DP-Fast MH: 大规模贝叶斯推断的私有、快速、准确的Metropolis-Hastings算法

    DP-Fast MH: Private, Fast, and Accurate Metropolis-Hastings for Large-Scale Bayesian Inference. (arXiv:2303.06171v1 [cs.LG])

    [http://arxiv.org/abs/2303.06171](http://arxiv.org/abs/2303.06171)

    本文提出了一种新的DP-Fast MH算法，用于大规模贝叶斯推断，具有精确、快速和隐私保护的特点。

    This paper proposes a new DP-Fast MH algorithm for large-scale Bayesian inference, which is accurate, fast, and privacy-preserving.

    贝叶斯推断提供了一个从复杂数据中学习和在不确定性下推理的原则性框架。它已经广泛应用于机器学习任务，如医学诊断、药物设计和政策制定。在这些常见应用中，数据可能非常敏感。差分隐私（DP）提供了具有强大最坏情况隐私保证的数据分析工具，并已发展成为隐私保护数据分析的主要方法。在本文中，我们研究了Metropolis-Hastings（MH）算法，这是最基本的MCMC方法之一，用于差分隐私下的大规模贝叶斯推断。虽然大多数现有的私有MCMC算法为了获得隐私而牺牲了准确性和效率，但我们提供了第一个精确且快速的DP MH算法，大多数迭代中仅使用一个小批量的数据。我们进一步揭示了隐私、可扩展性（即批量大小）和效率（即收敛速度）之间的三重权衡，从理论上说明了这一点。

    Bayesian inference provides a principled framework for learning from complex data and reasoning under uncertainty. It has been widely applied in machine learning tasks such as medical diagnosis, drug design, and policymaking. In these common applications, the data can be highly sensitive. Differential privacy (DP) offers data analysis tools with powerful worst-case privacy guarantees and has been developed as the leading approach in privacy-preserving data analysis. In this paper, we study Metropolis-Hastings (MH), one of the most fundamental MCMC methods, for large-scale Bayesian inference under differential privacy. While most existing private MCMC algorithms sacrifice accuracy and efficiency to obtain privacy, we provide the first exact and fast DP MH algorithm, using only a minibatch of data in most iterations. We further reveal, for the first time, a three-way trade-off among privacy, scalability (i.e. the batch size), and efficiency (i.e. the convergence rate), theoretically char
    
[^144]: "Wasserstein Believer:通过可靠的潜在空间模型学习部分可观测环境下的信念更新"

    The Wasserstein Believer: Learning Belief Updates for Partially Observable Environments through Reliable Latent Space Models. (arXiv:2303.03284v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.03284](http://arxiv.org/abs/2303.03284)

    本论文提出了一种“Wasserstein Belief Updater”算法来学习部分可观测环境下的信念更新，该算法通过学习POMDP​​的潜在模型和置信更新的近似值，实现了对历史观察和行动的有效压缩，提升了算法的性能表现。

    

    部分可观测马尔可夫决策过程（POMDP）是建模代理无法感知到完整状态的环境的有用工具。因此，代理需要考虑过去的观察和行动进行推理。但是，由于历史空间指数级增长，仅仅记住完整历史通常是不可行的。保持模拟真实状态的置信概率分布可以作为历史的充分统计量，但其计算需要访问环境的模型，因此也是不可行的。最先进的算法使用递归神经网络来压缩观察-行动历史以学习充分的统计量，但它们缺乏成功的保证并可能导致次优策略。为了克服这一点，我们提出了Wasserstein Belief Updater ，这是一种RL算法，它学习POMDP​​的潜在模型和置信更新的近似值。我们的方法具有理论成果。

    Partially Observable Markov Decision Processes (POMDPs) are useful tools to model environments where the full state cannot be perceived by an agent. As such the agent needs to reason taking into account the past observations and actions. However, simply remembering the full history is generally intractable due to the exponential growth in the history space. Keeping a probability distribution that models the belief over what the true state is can be used as a sufficient statistic of the history, but its computation requires access to the model of the environment and is also intractable. State-of-the-art algorithms use Recurrent Neural Networks to compress the observation-action history aiming to learn a sufficient statistic, but they lack guarantees of success and can lead to sub-optimal policies. To overcome this, we propose the Wasserstein Belief Updater, an RL algorithm that learns a latent model of the POMDP and an approximation of the belief update. Our approach comes with theoreti
    
[^145]: GlucoSynth：生成差分私有合成血糖轨迹

    GlucoSynth: Generating Differentially-Private Synthetic Glucose Traces. (arXiv:2303.01621v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01621](http://arxiv.org/abs/2303.01621)

    本文提出了一种新颖的GAN框架-GlucoSynth，采用差分隐私机制来生成合成血糖轨迹，保证数据隐私安全的同时能生成高质量的合成数据.

    

    本论文研究生成高质量、私有合成血糖轨迹的问题，这个任务可推广到许多其他时间序列数据。现有的时间序列数据合成方法，如使用生成对抗网络（GANs）的方法，无法捕捉血糖数据的先天特征，也无法在不严重降低合成数据效用的情况下提供任何形式隐私保证。本文提出了GlucoSynth，一种新颖的保护隐私的GAN框架，可用于生成合成血糖轨迹。我们方法的核心思想是在考虑时序动态的同时，保留轨迹中motif（血糖事件）之间的关系。我们的框架采用差分隐私机制，提供了强有力的形式隐私保证。我们使用120万条血糖轨迹进行了全面评估；GlucoSynth在生成高质量合成数据方面表现优异。

    We focus on the problem of generating high-quality, private synthetic glucose traces, a task generalizable to many other time series sources. Existing methods for time series data synthesis, such as those using Generative Adversarial Networks (GANs), are not able to capture the innate characteristics of glucose data and cannot provide any formal privacy guarantees without severely degrading the utility of the synthetic data. In this paper we present GlucoSynth, a novel privacy-preserving GAN framework to generate synthetic glucose traces. The core intuition behind our approach is to conserve relationships amongst motifs (glucose events) within the traces, in addition to temporal dynamics. Our framework incorporates differential privacy mechanisms to provide strong formal privacy guarantees. We provide a comprehensive evaluation on the real-world utility of the data using 1.2 million glucose traces; GlucoSynth outperforms all previous methods in its ability to generate high-quality synt
    
[^146]: DART: 多样化聚合重复训练改进神经网络的泛化能力

    DART: Diversify-Aggregate-Repeat Training Improves Generalization of Neural Networks. (arXiv:2302.14685v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14685](http://arxiv.org/abs/2302.14685)

    本文中提出了DART策略，其中利用多样化的增强方法训练不同的模型，然后通过聚合这些模型的权重来结合其专业知识，并重复聚合步骤以实现更好的泛化能力。

    

    神经网络的泛化能力对于在现实世界中安全部署它们至关重要。改进泛化的常见训练策略包括使用数据增强、集成和模型平均化。在本文中，我们首先建立了一个惊人简单但强有力的泛化基准，它利用了训练小批量中的多种不同增强方法，并证明这可以学习到一个更平衡的特征分布。进一步地，我们提出了Diversify-Aggregate-Repeat Training (DART)策略，该策略首先使用不同的增强方法(或领域)训练不同的模型，以探索损失盆地，并进一步聚合它们的权重，结合它们的专业知识并获得改进的泛化能力。我们发现，在整个训练过程中重复聚合步骤可以提高整体优化轨迹，并确保单个模型具有足够低的损失障碍，在将它们组合时可以获得改进的泛化能力。我们解释了我们的方法是一种正则化形式，它迫使模型探索损失景观的多种模式。我们的实验表明，DART在CIFAR-10和CIFAR-100上实现了最先进的性能，并在ImageNet和鲁棒性基准测试中取得了有竞争力的结果。

    Generalization of neural networks is crucial for deploying them safely in the real world. Common training strategies to improve generalization involve the use of data augmentations, ensembling and model averaging. In this work, we first establish a surprisingly simple but strong benchmark for generalization which utilizes diverse augmentations within a training minibatch, and show that this can learn a more balanced distribution of features. Further, we propose Diversify-Aggregate-Repeat Training (DART) strategy that first trains diverse models using different augmentations (or domains) to explore the loss basin, and further Aggregates their weights to combine their expertise and obtain improved generalization. We find that Repeating the step of Aggregation throughout training improves the overall optimization trajectory and also ensures that the individual models have a sufficiently low loss barrier to obtain improved generalization on combining them. We shed light on our approach by 
    
[^147]: (Re)$^2$H2O: 自主驾驶场景生成的反向正则化混合离线和在线强化学习

    (Re)$^2$H2O: Autonomous Driving Scenario Generation via Reversely Regularized Hybrid Offline-and-Online Reinforcement Learning. (arXiv:2302.13726v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13726](http://arxiv.org/abs/2302.13726)

    提出了一个新的自主驾驶场景生成框架，结合了离线实际数据和在线模拟数据，将反向 L_1 正则化用于关键特征提取，并有效地查询更高维度搜索空间中的场景，实验证明了方法的有效性。

    

    自主驾驶及其广泛采用一直被寄予厚望。然而，在没有可信的全面测试程序的情况下，不仅行业难以大规模生产自主驾驶车辆(AV)，而且公众和决策者也没有说服接受创新。生成对AV具有重要挑战的安全关键场景是测试的重要第一步。现实世界的数据集包括自然但过于安全的驾驶行为，而模拟则允许无限制地探索多样化和激进的交通场景。相反，模拟中的更高维度搜索空间使得没有实际数据分布作为隐式约束的情况下效率低下。为了将两者的优点结合起来，从离线实际数据和在线模拟数据同时学习生成场景似乎是可行的。因此，我们量身打造了一个称为 (Re)$^2$H2O 的反向正则化混合离线和在线强化学习框架来实现场景生成。具体而言，离线部分使用反向 L_1 正则化来提取关键特征，并指导传输到在线学习中，以满足真实世界数据分布。在线部分则通过从离线数据学到的策略更有效地查询更高维度搜索空间中的场景。在 CARLA 模拟器和实际数据集上的实验验证了我们方法的有效性。

    Autonomous driving and its widespread adoption have long held tremendous promise. Nevertheless, without a trustworthy and thorough testing procedure, not only does the industry struggle to mass-produce autonomous vehicles (AV), but neither the general public nor policymakers are convinced to accept the innovations. Generating safety-critical scenarios that present significant challenges to AV is an essential first step in testing. Real-world datasets include naturalistic but overly safe driving behaviors, whereas simulation would allow for unrestricted exploration of diverse and aggressive traffic scenarios. Conversely, higher-dimensional searching space in simulation disables efficient scenario generation without real-world data distribution as implicit constraints. In order to marry the benefits of both, it seems appealing to learn to generate scenarios from both offline real-world and online simulation data simultaneously. Therefore, we tailor a Reversely Regularized Hybrid Offline-
    
[^148]: 变分自编码器的分布式学习：在合成数据生成中的应用

    Distributional Learning of Variational AutoEncoder: Application to Synthetic Data Generation. (arXiv:2302.11294v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.11294](http://arxiv.org/abs/2302.11294)

    提出了一种新的方法扩展了VAE模型容量，采用无限混合的非对称拉普拉斯分布作为解码器，具有分布拟合能力和调整数据隐私级别的优越性。

    

    尽管变分自编码器（VAE）在计算建模方面很高效，但高斯假设一直被认为是它的主要局限性。在本文中，我们提出了一种新方法，扩展了模型容量（即分布族的表达能力），而不会牺牲VAE框架的计算优势。我们的VAE模型的解码器由无限组合的非对称拉普拉斯分布构成，具有连续变量的分布拟合能力。我们的模型由估计一般分位函数的非参数M-estimator的特殊形式表示，并在理论上建立了所提出模型与分位数估计之间的关系。我们将所提出的模型应用于合成数据生成，特别是在轻松调整数据隐私级别方面，我们的模型展现了其优越性。

    The Gaussianity assumption has been consistently criticized as a main limitation of the Variational Autoencoder (VAE), despite its efficiency in computational modeling. In this paper, we propose a new approach that expands the model capacity (i.e., expressive power of distributional family) without sacrificing the computational advantages of the VAE framework. Our VAE model's decoder is composed of an infinite mixture of asymmetric Laplacian distribution, which possesses general distribution fitting capabilities for continuous variables. Our model is represented by a special form of a nonparametric M-estimator for estimating general quantile functions, and we theoretically establish the relevance between the proposed model and quantile estimation. We apply the proposed model to synthetic data generation, and particularly, our model demonstrates superiority in easily adjusting the level of data privacy.
    
[^149]: 重新审视联邦学习中的加权聚合方法

    Revisiting Weighted Aggregation in Federated Learning with Neural Networks. (arXiv:2302.10911v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10911](http://arxiv.org/abs/2302.10911)

    本文重新审视了联邦学习中的加权聚合方法。作者发现权重总和可能小于1，从而改善了泛化性能。作者探索了最优缩小因子如何受到客户端数据异质性和本地周期的影响，并使用客户端相干性研究了客户端之间的相对聚合权重以描绘客户端的重要性。作者提出了一种有效的联邦学习方法（FLLAW），该方法具有可学习聚合权重和全局权重缩小效应。

    

    在联邦学习（FL）中，对本地模型进行加权聚合以生成全局模型，聚合权重被标准化（权重和为1）并与本地数据大小成比例。本文重新审视了加权聚合过程，并深入探讨了FL的训练动力学。首先，我们发现权重总和可能小于1，导致全局权重缩小效应（类似于权重衰减）并改善了泛化性能。我们探讨了最优缩小因子如何受到客户端数据异质性和本地周期的影响。其次，我们深入研究了客户端之间的相对聚合权重以描绘客户端的重要性。我们开发了客户端相干性来研究学习动态，并发现存在一个关键点。在进入临界点之前，相干性更高的客户端在泛化中发挥了更重要的作用。基于上述洞见，我们提出了一种有效的联邦学习方法——具有可学习聚合权重的联邦学习（FLLAW），它允许全局权重缩小效应和可学习聚合权重。在各种基准测试中的实验结果表明，FLLAW在更快的收敛速度、更高的准确性和更好的抗数据异质性方面具有很好的效果。

    In federated learning (FL), weighted aggregation of local models is conducted to generate a global model, and the aggregation weights are normalized (the sum of weights is 1) and proportional to the local data sizes. In this paper, we revisit the weighted aggregation process and gain new insights into the training dynamics of FL. First, we find that the sum of weights can be smaller than 1, causing global weight shrinking effect (analogous to weight decay) and improving generalization. We explore how the optimal shrinking factor is affected by clients' data heterogeneity and local epochs. Second, we dive into the relative aggregation weights among clients to depict the clients' importance. We develop client coherence to study the learning dynamics and find a critical point that exists. Before entering the critical point, more coherent clients play more essential roles in generalization. Based on the above insights, we propose an effective method for Federated Learning with Learnable Ag
    
[^150]: ChatGPT：应付千事的万能型 AI，但无所专精

    ChatGPT: Jack of all trades, master of none. (arXiv:2302.10724v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.10724](http://arxiv.org/abs/2302.10724)

    本研究检验了 ChatGPT 在 25 个不同的 NLP 任务上的性能，它是一个万能的 AI 模型，但无关紧要的表现可能会对某些任务的表现产生负面影响。

    

    OpenAI 推出了聊天生成预训练 Transformer（ChatGPT），革新了人工智能与人类互动的方法。许多研究通过测试 ChatGPT 在众所周知的自然语言处理（NLP）任务中的效果，来评估该模型的效能。然而，现有的研究大多非自动化，并且规模非常有限。本研究在 25 个不同的 NLP 任务上检验了 ChatGPT 的性能，其中大多数任务甚至对人类而言都是主观的，例如情感分析、情绪识别、攻击性和立场检测。另一些任务则需要更客观的推理，如词义消歧、语言可接受性和问答。我们还对 GPT-4 模型在五个选定的 NLP 任务子集上进行了评估。我们自动化了 ChatGPT 和 GPT-4 的引导过程，并分析了超过 49k 个响应。与现有最先进的解决方案（SOTA）进行比较，我们的结果显示，在一些任务上 ChatGPT 的性能存在一定的缺陷。

    OpenAI has released the Chat Generative Pre-trained Transformer (ChatGPT) and revolutionized the approach in artificial intelligence to human-model interaction. Several publications on ChatGPT evaluation test its effectiveness on well-known natural language processing (NLP) tasks. However, the existing studies are mostly non-automated and tested on a very limited scale. In this work, we examined ChatGPT's capabilities on 25 diverse analytical NLP tasks, most of them subjective even to humans, such as sentiment analysis, emotion recognition, offensiveness, and stance detection. In contrast, the other tasks require more objective reasoning like word sense disambiguation, linguistic acceptability, and question answering. We also evaluated GPT-4 model on five selected subsets of NLP tasks. We automated ChatGPT and GPT-4 prompting process and analyzed more than 49k responses. Our comparison of its results with available State-of-the-Art (SOTA) solutions showed that the average loss in quali
    
[^151]: AdaGrad 步幅下的随机梯度下降：对未知参数、无界梯度和仿射方差的全适应性高概率研究

    SGD with AdaGrad Stepsizes: Full Adaptivity with High Probability to Unknown Parameters, Unbounded Gradients and Affine Variance. (arXiv:2302.08783v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08783](http://arxiv.org/abs/2302.08783)

    本文提供了针对 AdaGrad 步幅下的SGD算法的更加全面且无限制性的分析，支持多种模型，可以在高概率下处理未知参数和无界梯度。

    

    我们研究了 AdaGrad 步幅下的随机梯度下降：这是一种流行的自适应 (自调节) 的一阶随机优化方法。尽管经过广泛研究，但现有的分析方法存在各种缺陷：它们要么假定对问题参数有一定的了解，要么设定强的全局利普希茨条件，或者不能给出高概率可靠的界限。我们在凸和非凸 (平滑) 情况下，对这种基本方法进行全面无任何限制地分析，另外支持一般的“仿射方差”噪声模型，并在低噪声和高噪声区域中提供锐利的收敛速度。

    We study Stochastic Gradient Descent with AdaGrad stepsizes: a popular adaptive (self-tuning) method for first-order stochastic optimization. Despite being well studied, existing analyses of this method suffer from various shortcomings: they either assume some knowledge of the problem parameters, impose strong global Lipschitz conditions, or fail to give bounds that hold with high probability. We provide a comprehensive analysis of this basic method without any of these limitations, in both the convex and non-convex (smooth) cases, that additionally supports a general ``affine variance'' noise model and provides sharp rates of convergence in both the low-noise and high-noise~regimes.
    
[^152]: 一种基于云计算和深度学习框架的拥挤事件入口早期推测测深方法

    A Cloud-based Deep Learning Framework for Early Detection of Pushing at Crowded Event Entrances. (arXiv:2302.08237v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08237](http://arxiv.org/abs/2302.08237)

    本文提出了一种基于云计算和深度学习的自动早期推测测深框架，可用于拥挤事件入口的推挤识别，包括修改和训练EfficientNetV2B0卷积神经网络模型、准确快速的深度光流模型及彩色环方法的整合以实时分析视频流并识别推挤区域，并使用现场采集技术和云计算环境实时收集人群视频流并提供早期结果。

    

    大型事件入口的拥挤可能导致危急和危及生命的情况，特别是当人们开始相互推挤以更快地到达目的地时。自动准确地识别推挤行为将有助于组织者和安保部队及时干预和减轻危险局面。本文提出了一种基于云计算和深度学习的自动早期推测测深框架，用于拥挤事件入口的推挤识别。该框架首先修改并训练EfficientNetV2B0卷积神经网络模型，接着将该模型与准确快速的深度光流模型及彩色环方法进行整合，实时分析视频流并识别推挤区域。此外，该框架使用现场采集技术和云计算环境实时收集人群视频流并提供早期结果。基于五个真实世界实验生成了一个新的数据集。

    Crowding at the entrances of large events may lead to critical and life-threatening situations, particularly when people start pushing each other to reach the event faster. Automatic and timely identification of pushing behavior would help organizers and security forces to intervene early and mitigate dangerous situations. In this paper, we propose a cloud-based deep learning framework for automatic early detection of pushing in crowded event entrances. The proposed framework initially modifies and trains the EfficientNetV2B0 Convolutional Neural Network model. Subsequently, it integrates the adapted model with an accurate and fast pre-trained deep optical flow model with the color wheel method to analyze video streams and identify pushing patches in real-time. Moreover, the framework uses live capturing technology and a cloud-based environment to collect video streams of crowds in real-time and provide early-stage results. A novel dataset is generated based on five real-world experime
    
[^153]: 刺激布里渊散射驱动的光子水库计算

    Photonic reservoir computing enabled by stimulated Brillouin scattering. (arXiv:2302.07698v2 [physics.optics] UPDATED)

    [http://arxiv.org/abs/2302.07698](http://arxiv.org/abs/2302.07698)

    本文报道了一种基于刺激布里渊散射的光子水库计算平台，能够利用光学技术的优势，实现快速、低功率和更大带宽的实时人工智能。

    

    人工智能正在推动未来技术的创建，这些技术会改变人类生活和工作的方式，创造出改变我们处理任务和活动方式的新方案，但这需要大量的数据处理、大量的数据传输和计算速度。因此，研究人员越来越关注于开发一种新型计算平台，这种平台受脑的架构的启发，特别是那些利用光子技术的优势，快速、低功率和更大带宽。本文介绍了一种基于刺激布里渊散射的光子水库计算架构的新型计算平台。这个新的光子水库计算系统的核心是由一个完全被动光学系统构成的。此外，它还可以与高性能的光学复用技术结合使用，以实现实时人工智能。

    Artificial Intelligence (AI) drives the creation of future technologies that disrupt the way humans live and work, creating new solutions that change the way we approach tasks and activities, but it requires a lot of data processing, large amounts of data transfer, and computing speed. It has led to a growing interest of research in developing a new type of computing platform which is inspired by the architecture of the brain specifically those that exploit the benefits offered by photonic technologies, fast, low-power, and larger bandwidth. Here, a new computing platform based on the photonic reservoir computing architecture exploiting the non-linear wave-optical dynamics of the stimulated Brillouin scattering is reported. The kernel of the new photonic reservoir computing system is constructed of an entirely passive optical system. Moreover, it is readily suited for use in conjunction with high performance optical multiplexing techniques to enable real-time artificial intelligence. H
    
[^154]: 追求最优压缩：联合剪枝和量化

    Towards Optimal Compression: Joint Pruning and Quantization. (arXiv:2302.07612v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07612](http://arxiv.org/abs/2302.07612)

    本论文介绍了一种名为FITCompress的方法，它可以联合使用剪枝和混合精度量化来对预训练模型进行优化选择，以实现最优压缩。该方法基于Fisher信息度量和压缩空间中的路径规划，可在保持模型原始性能的情况下实现最新颖的压缩。

    

    模型压缩对于在资源受限的硬件上优化深度神经网络推理至关重要。目前网络压缩的主要方法，即量化和剪枝，已被证明可以提高效率，但会付出性能的代价。确定适用于单个层和参数的最有效量化和剪枝策略仍然是一项具有挑战性的问题，通常需要计算量昂贵且特别的数值优化技术。本文介绍了FITCompress，一种新颖的方法，它使用统一的启发式方法集成了逐层混合精度量化和非结构化剪枝。通过利用Fisher信息度量和压缩空间中的路径规划，FITCompress可以为给定的预训练模型和压缩约束优化选择剪枝掩码和混合精度量化配置的组合。在计算机视觉和自然语言处理基准测试上的实验表明，我们提出的方法实现了最先进的压缩率，同时保持或甚至提高了模型的初始性能。

    Model compression is instrumental in optimizing deep neural network inference on resource-constrained hardware. The prevailing methods for network compression, namely quantization and pruning, have been shown to enhance efficiency at the cost of performance. Determining the most effective quantization and pruning strategies for individual layers and parameters remains a challenging problem, often requiring computationally expensive and ad hoc numerical optimization techniques. This paper introduces FITCompress, a novel method integrating layer-wise mixed-precision quantization and unstructured pruning using a unified heuristic approach. By leveraging the Fisher Information Metric and path planning through compression space, FITCompress optimally selects a combination of pruning mask and mixed-precision quantization configuration for a given pre-trained model and compression constraint. Experiments on computer vision and natural language processing benchmarks demonstrate that our propos
    
[^155]: 通用学习中对抗奖励在上下文Bandits中的应用

    Adversarial Rewards in Universal Learning for Contextual Bandits. (arXiv:2302.07186v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.07186](http://arxiv.org/abs/2302.07186)

    本文研究了在上下文Bandits中学习的基本极限，给出了关于可学习的上下文过程和通用一致性算法的特性，并讨论了对抗奖励下的乐观通用一致性学习的不可能性。

    

    本文研究了上下文Bandits中学习的基本极限，其中学习者的奖励取决于其行为和已知上下文，这扩展了经典的多臂赌博机，在有附加信息的情况下。我们对能够实现亚线性遗憾的通用一致性算法感兴趣，相对于任何可测定的固定策略，无需任何功能类限制。然而，奖励机制可以随着时间的推移而发生变化，我们展示了在对抗奖励下，上下文Bandits的乐观通用一致性学习是不可能的。

    We study the fundamental limits of learning in contextual bandits, where a learner's rewards depend on their actions and a known context, which extends the canonical multi-armed bandit to the case where side-information is available. We are interested in universally consistent algorithms, which achieve sublinear regret compared to any measurable fixed policy, without any function class restriction. For stationary contextual bandits, when the underlying reward mechanism is time-invariant, Blanchard et. al (2022) characterized learnable context processes for which universal consistency is achievable; and further gave algorithms ensuring universal consistency whenever this is achievable, a property known as optimistic universal consistency. It is well understood, however, that reward mechanisms can evolve over time, possibly adversarially, and depending on the learner's actions. We show that optimistic universal learning for contextual bandits with adversarial rewards is impossible in gen
    
[^156]: 超似曲空间的大间隔分类的浑拟圆决策边界

    Horospherical Decision Boundaries for Large Margin Classification in Hyperbolic Space. (arXiv:2302.06807v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.06807](http://arxiv.org/abs/2302.06807)

    本文提出了一种大间隔分类器，它使用浑拟圆决策边界可以优化测地凸优化问题，实验结果表明其竞争性能优越。

    

    近年来，用超似曲空间表示层次结构化数据已经越来越流行，同时，文献中也提出了几个针对这些空间中数据分类的算法。这些算法主要使用超平面或测地线作为决策边界，使用大间隔分类器设置，从而导致一个非凸优化问题。在本文中，我们提出了一种基于浑拟圆决策边界的新型大间隔分类器，它可以导致一个测地凸优化问题，可以使用任何黎曼梯度下降技术来优化，保证全局最优解。我们展示了几个实验，展示了我们的分类器相比于 SOTA 的竞争性能。

    Hyperbolic spaces have been quite popular in the recent past for representing hierarchically organized data. Further, several classification algorithms for data in these spaces have been proposed in the literature. These algorithms mainly use either hyperplanes or geodesics for decision boundaries in a large margin classifiers setting leading to a non-convex optimization problem. In this paper, we propose a novel large margin classifier based on horospherical decision boundaries that leads to a geodesically convex optimization problem that can be optimized using any Riemannian gradient descent technique guaranteeing a globally optimal solution. We present several experiments depicting the competitive performance of our classifier in comparison to SOTA.
    
[^157]: 知道自己不知道的概率电路

    Probabilistic Circuits That Know What They Don't Know. (arXiv:2302.06544v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06544](http://arxiv.org/abs/2302.06544)

    本文指出概率电路（PC）对超出分布（OOD）数据不具备鲁棒性；通过模型不确定性量化，我们提出了可处理的随机失活推断（TDI）来克服这一挑战，并且这种方法可以在单个正向传递中提供可处理的无采样不确定性估计，从而改善了PC对分布漂移和OOD数据的鲁棒性。

    

    概率电路（PC）是一种允许准确和可处理的概率推断的模型。与神经网络相比，它们通常被认为是良好校准的，并且对于超出分布（OOD）数据具有鲁棒性。本文表明 PC 实际上不具有对OOD数据的鲁棒性，进而展示了如何通过模型不确定性量化来克服这一挑战。为此，我们提出了可处理的随机失活推断（TDI）——一种推断程序，通过方差传播导出蒙特卡洛失活（MCD）的解析解来估计不确定性。与神经网络中的 MCD 不同，TDI不需要进行多次网络评估就可以提供可处理的无采样不确定性估计。通过一系列实验评估在真实数据上的分类置信度和不确定性估计，TDI改善了PC对分布漂移和OOD数据的鲁棒性。

    Probabilistic circuits (PCs) are models that allow exact and tractable probabilistic inference. In contrast to neural networks, they are often assumed to be well-calibrated and robust to out-of-distribution (OOD) data. In this paper, we show that PCs are in fact not robust to OOD data, i.e., they don't know what they don't know. We then show how this challenge can be overcome by model uncertainty quantification. To this end, we propose tractable dropout inference (TDI), an inference procedure to estimate uncertainty by deriving an analytical solution to Monte Carlo dropout (MCD) through variance propagation. Unlike MCD in neural networks, which comes at the cost of multiple network evaluations, TDI provides tractable sampling-free uncertainty estimates in a single forward pass. TDI improves the robustness of PCs to distribution shift and OOD data, demonstrated through a series of experiments evaluating the classification confidence and uncertainty estimates on real-world data.
    
[^158]: 认证悖论: 认证会揭示更好的攻击

    The Certification Paradox: Certifications Admit Better Attacks. (arXiv:2302.04379v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04379](http://arxiv.org/abs/2302.04379)

    本文指出了一个"认证悖论"，认证虽然可以展示模型的稳健性，但额外揭示了有关认证模型的信息也成为新的攻击面，导致更好的攻击效果。

    

    在保证有一个有界区域内不存在对抗样本的情况下，认证机制在展示神经网络的稳健性方面扮演着重要角色。本文提出了一个问题: 认证是否会有任何意想不到的后果，通过揭示有关认证模型的额外信息？我们以肯定的答案回答了这个问题，证明了认证不仅测量模型的稳健性，而且展现了新的攻击面。我们提出了"认证感知攻击"，在针对经过认证的模型进行攻击时，这种攻击会比以前的任何方法更频繁地产生更小的对抗性扰动。我们的攻击实现了最多34%的扰动规范中位数的减小(比较目标和攻击实例)，同时需要的计算时间比PGD等方法少了90%。我们的攻击实现了如此显着的扰动大小和计算成本的降低，突显了以认证作为对抗攻击防御的一种悖论。具体来说，认证不仅揭示了稳健模型的属性，而且还可以用来发起更有效的攻击。

    In guaranteeing that no adversarial examples exist within a bounded region, certification mechanisms play an important role in demonstrating the robustness of neural networks. In this work we ask: Could certifications have any unintended consequences, through exposing additional information about certified models? We answer this question in the affirmative, demonstrating that certifications not only measure model robustness but also present a new attack surface. We propose \emph{Certification Aware Attacks}, that produce smaller adversarial perturbations more than twice as frequently as any prior approach, when launched against certified models. Our attacks achieve an up to $34\%$ reduction in the median perturbation norm (comparing target and attack instances), while requiring $90 \%$ less computational time than approaches like PGD. That our attacks achieve such significant reductions in perturbation size and computational cost highlights an apparent paradox in deploying certificatio
    
[^159]: 在时间变化需求下的赌率反馈在线资源分配模型

    Online Resource Allocation: Bandits feedback and Advice on Time-varying Demands. (arXiv:2302.04182v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04182](http://arxiv.org/abs/2302.04182)

    本论文研究了带有赌率反馈和时间变化需求的在线资源分配问题，提出了一种稳健的在线算法来处理总需求量的在线预测。

    

    本文考虑了带有赌率反馈和时间变化需求的在线资源分配模型。尽管在线资源分配在文献中已经有许多研究，但大多数现有的作品都做出了需求到达过程是稳态的强假设。然而，在实际应用中，如在线广告和收益管理，该过程可能是外生的和非稳态的，如不断变化的互联网流量。受最近的带有建议框架的在线算法激励[Mitazenmacher和Vassilvitskii，\emph {Commun。ACM} 2022]，我们探讨了在线建议如何通知策略设计。我们建立了一个不可能的结果，即在我们的设置中，任何算法都会在悔恨的方面表现不佳。相比之下，我们设计了一种稳健的在线算法，该算法利用了总需求量的在线预测。在获得在线建议的支持下，我们提出的算法被证明在理论性能和有前途的数值方面都有出色表现。

    We consider a general online resource allocation model with bandit feedback and time-varying demands. While online resource allocation has been well studied in the literature, most existing works make the strong assumption that the demand arrival process is stationary. In practical applications, such as online advertisement and revenue management, however, this process may be exogenous and non-stationary, like the constantly changing internet traffic. Motivated by the recent Online Algorithms with Advice framework [Mitazenmacher and Vassilvitskii, \emph{Commun. ACM} 2022], we explore how online advice can inform policy design. We establish an impossibility result that any algorithm perform poorly in terms of regret without any advice in our setting. In contrast, we design an robust online algorithm that leverages the online predictions on the total demand volumes. Empowered with online advice, our proposed algorithm is shown to have both theoretical performance and promising numerical 
    
[^160]: 《通过对抗性少数派影响攻击合作多智能体强化学习》

    Attacking Cooperative Multi-Agent Reinforcement Learning by Adversarial Minority Influence. (arXiv:2302.03322v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03322](http://arxiv.org/abs/2302.03322)

    本研究提出了 Adversarial Minority Influence (AMI) 黑盒攻击，能够在考虑多智能体互动和合作目标下实现有针对性的最坏情况合作，攻击成功率比现有方法高出 2.2倍。

    

    本研究探讨了合作多智能体强化学习(c-MARL)在对抗性攻击下的弱点，这是在实际实现之前c-MARL最坏情况下性能的重要决定因素。目前基于观察的攻击，受到白盒假设的限制，忽视了c-MARL的复杂多智能体交互和合作目标，导致攻击能力受到实际和限制。为了解决这些问题，我们提出了Adversarial Minority Influence (AMI)，这是一个实用而强大的针对c-MARL的黑盒攻击。AMI是一个实用的黑盒攻击，可以在不了解受害者参数的情况下发动攻击。AMI也是强大的，因为它考虑了复杂的多智能体互动和代理的合作目标，使单个对抗性代理能够单方面误导大多数受害者形成有针对性的最坏情况合作。这反映了社会心理学中的少数派影响现象。为了在复杂的代理方式交互下实现最大的受害者政策偏差，我们基于博弈理论分析推导出一种高效且可扩展的AMI优化算法。我们证明了AMI的有效性以及考虑多智能体交互的必要性，表明AMI可以实现比现有方法高出2.2倍的攻击成功率。

    This study probes the vulnerabilities of cooperative multi-agent reinforcement learning (c-MARL) under adversarial attacks, a critical determinant of c-MARL's worst-case performance prior to real-world implementation. Current observation-based attacks, constrained by white-box assumptions, overlook c-MARL's complex multi-agent interactions and cooperative objectives, resulting in impractical and limited attack capabilities. To address these shortcomes, we propose Adversarial Minority Influence (AMI), a practical and strong for c-MARL. AMI is a practical black-box attack and can be launched without knowing victim parameters. AMI is also strong by considering the complex multi-agent interaction and the cooperative goal of agents, enabling a single adversarial agent to unilaterally misleads majority victims to form targeted worst-case cooperation. This mirrors minority influence phenomena in social psychology. To achieve maximum deviation in victim policies under complex agent-wise intera
    
[^161]: 域索引变分贝叶斯：可解释的域索引用于域自适应

    Domain-Indexing Variational Bayes: Interpretable Domain Index for Domain Adaptation. (arXiv:2302.02561v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02561](http://arxiv.org/abs/2302.02561)

    该论文介绍了一种新的域自适应方法，该方法利用了从多域数据中生成的域索引，提供额外的洞察视角，并在各种任务中实现了最佳性能。

    

    先前的研究表明，利用域索引可以显著提高域自适应性能。然而，并非总是有这样的域索引可用。为解决这一挑战，我们首先从概率角度提供了域索引的正式定义，然后提出了一个对抗性变分贝叶斯框架，从多域数据中推断出域索引，从而提供额外的域关系洞察，并提高域自适应性能。我们的理论分析表明，我们的对抗性变分贝叶斯框架在平衡点处找到了最优的域索引。对合成和真实数据的实证结果验证了我们的模型可以产生可解释的域索引，使我们可以实现优于现有域适应方法的性能。代码可在https://github.com/Wang-ML-Lab/VDI获得。

    Previous studies have shown that leveraging domain index can significantly boost domain adaptation performance (arXiv:2007.01807, arXiv:2202.03628). However, such domain indices are not always available. To address this challenge, we first provide a formal definition of domain index from the probabilistic perspective, and then propose an adversarial variational Bayesian framework that infers domain indices from multi-domain data, thereby providing additional insight on domain relations and improving domain adaptation performance. Our theoretical analysis shows that our adversarial variational Bayesian framework finds the optimal domain index at equilibrium. Empirical results on both synthetic and real data verify that our model can produce interpretable domain indices which enable us to achieve superior performance compared to state-of-the-art domain adaptation methods. Code is available at https://github.com/Wang-ML-Lab/VDI.
    
[^162]: 实用偏好贝叶斯优化中的偏斜高斯过程研究

    Towards Practical Preferential Bayesian Optimization with Skew Gaussian Processes. (arXiv:2302.01513v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01513](http://arxiv.org/abs/2302.01513)

    该论文研究了偏好贝叶斯优化中的偏好高斯过程模型，提出了改进的基于马尔科夫链蒙特卡洛的估计方法，可以解决偏斜性问题并提高实用性。

    

    我们研究了偏好贝叶斯优化中的偏好高斯过程模型，其中可靠的反馈仅限于称为决斗的成对比较。偏好高斯过程模型表示灵活的偏好结构，但其后验分布是一个计算上难以处理的偏斜高斯过程。目前，最广泛使用的偏好贝叶斯优化方法是高斯近似，然而这种方法忽略了真实后验分布的偏斜性。我们的研究旨在改进基于马尔科夫链蒙特卡罗的偏好贝叶斯优化估计，通过Gibbs抽样和推导低方差MC估计器展示出其实用性。

    We study preferential Bayesian optimization (BO) where reliable feedback is limited to pairwise comparison called duels. An important challenge in preferential BO, which uses the preferential Gaussian process (GP) model to represent flexible preference structure, is that the posterior distribution is a computationally intractable skew GP. The most widely used approach for preferential BO is Gaussian approximation, which ignores the skewness of the true posterior. Alternatively, Markov chain Monte Carlo (MCMC) based preferential BO is also proposed. In this work, we first verify the accuracy of Gaussian approximation, from which we reveal the critical problem that the predictive probability of duels can be inaccurate. This observation motivates us to improve the MCMC-based estimation for skew GP, for which we show the practical efficiency of Gibbs sampling and derive the low variance MC estimator. However, the computational time of MCMC can still be a bottleneck in practice. Towards bui
    
[^163]: 随机高斯过程上置信界限：更紧的贝叶斯遗憾界限

    Randomized Gaussian Process Upper Confidence Bound with Tighter Bayesian Regret Bounds. (arXiv:2302.01511v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01511](http://arxiv.org/abs/2302.01511)

    该研究提出了一种改进的随机高斯过程上置信界限(IRGP-UCB)，它使用双参数指数分布,取得了更紧密的贝叶斯遗憾界限，并避免了后期过度探索。

    

    高斯过程上置信界限(GP-UCB)是一种理论上很有前途的黑盒优化方法；然而，在定理中置信参数$\beta$相当大，并在实践中以试探性的方式选择。 随机GP-UCB(RGP-UCB)使用一个遵循Gamma分布的随机置信参数来减轻手动指定$\beta$的影响。本研究首先将RGP-UCB的遗憾分析推广到更广泛的分布类中，包括Gamma分布。此外，我们提出了基于双参数指数分布的改进型RGP-UCB(IRGP-UCB)，它实现了更紧密的贝叶斯遗憾界限。IRGP-UCB不需要在迭代次数方面增加置信参数,从而避免了后期过度探索。最后，我们通过大量实验证明了IRGP-UCB的有效性。

    Gaussian process upper confidence bound (GP-UCB) is a theoretically promising approach for black-box optimization; however, the confidence parameter $\beta$ is considerably large in the theorem and chosen heuristically in practice. Then, randomized GP-UCB (RGP-UCB) uses a randomized confidence parameter, which follows the Gamma distribution, to mitigate the impact of manually specifying $\beta$. This study first generalizes the regret analysis of RGP-UCB to a wider class of distributions, including the Gamma distribution. Furthermore, we propose improved RGP-UCB (IRGP-UCB) based on a two-parameter exponential distribution, which achieves tighter Bayesian regret bounds. IRGP-UCB does not require an increase in the confidence parameter in terms of the number of iterations, which avoids over-exploration in the later iterations. Finally, we demonstrate the effectiveness of IRGP-UCB through extensive experiments.
    
[^164]: 高效图场积分器在点云中的应用

    Efficient Graph Field Integrators Meet Point Clouds. (arXiv:2302.00942v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00942](http://arxiv.org/abs/2302.00942)

    本文提出了两种算法用于非欧几里得空间中的高效图场积分，适用于点云网格图或ε-最近邻图的表征方法，具有很强的实用性。

    

    文章提出了两种新的算法类别，用于对编码点云的图形进行高效场积分。第一类算法使用点云网格图的有界亏格，第二类算法则使用点云的流行的ε-最近邻图表示方法。两种算法都可以被看作 Fast Multipole Methods(FMMs) 的可行替代，但适用于非欧几里得空间。文章重点研究基于点之间步长分布（如最短路径距离）所引发的几何学。通过提供详细的理论分析，文章获得了结构图论的新结果。文章还进行了全面的实证评估，包括对刚性和可变形物体的表面插值（特别是用于网格动态建模），点云的Wasserstein距离计算以及Gromov-Wasserstein变体等。

    We present two new classes of algorithms for efficient field integration on graphs encoding point clouds. The first class, SeparatorFactorization(SF), leverages the bounded genus of point cloud mesh graphs, while the second class, RFDiffusion(RFD), uses popular epsilon-nearest-neighbor graph representations for point clouds. Both can be viewed as providing the functionality of Fast Multipole Methods (FMMs), which have had a tremendous impact on efficient integration, but for non-Euclidean spaces. We focus on geometries induced by distributions of walk lengths between points (e.g., shortest-path distance). We provide an extensive theoretical analysis of our algorithms, obtaining new results in structural graph theory as a byproduct. We also perform exhaustive empirical evaluation, including on-surface interpolation for rigid and deformable objects (particularly for mesh-dynamics modeling), Wasserstein distance computations for point clouds, and the Gromov-Wasserstein variant.
    
[^165]: 持续学习综述：理论、方法与应用

    A Comprehensive Survey of Continual Learning: Theory, Method and Application. (arXiv:2302.00487v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00487](http://arxiv.org/abs/2302.00487)

    本文为持续学习的全面综述，总结了持续学习的一般目标，回顾了各种持续学习方法，并强调了一些有前途的应用领域和未来研究方向。

    

    智能系统需要在其生命周期内不断获取、更新、积累和利用知识以应对现实世界的动态变化。这种能力称为持续学习，为AI系统自适应发展提供了基础。然而，持续学习的一个显著限制是灾难性遗忘，即学习一个新任务通常会导致旧任务的性能显著降低。近年来，不断涌现的各种进展大大扩展了持续学习的理解和应用。本文提供了一个全面的持续学习综述，旨在连接基本设置、理论基础、代表性方法和实际应用。我们总结了现有理论和实证结果，概括了持续学习的一般目标：减少灾难性遗忘，实现高效的和终身的学习，以及实现更深层次的表征学习。我们还回顾了各种持续学习方法，包括基于正则化、基于回放和基于生成的方法，并讨论了它们的优缺点。最后，我们强调了一些有前途的应用领域，如机器人、自然语言处理和计算机视觉，并确定了一些开放挑战和未来研究方向。

    To cope with real-world dynamics, an intelligent agent needs to incrementally acquire, update, accumulate, and exploit knowledge throughout its lifetime. This ability, known as continual learning, provides a foundation for AI systems to develop themselves adaptively. In a general sense, continual learning is explicitly limited by catastrophic forgetting, where learning a new task usually results in a dramatic performance degradation of the old tasks. Beyond this, increasingly numerous advances have emerged in recent years that largely extend the understanding and application of continual learning. The growing and widespread interest in this direction demonstrates its realistic significance as well as complexity. In this work, we present a comprehensive survey of continual learning, seeking to bridge the basic settings, theoretical foundations, representative methods, and practical applications. Based on existing theoretical and empirical results, we summarize the general objectives of 
    
[^166]: 新的分布水平度量方法：弃用$\Delta$DP，实现人口统计特征的公平机器学习

    Retiring $\Delta$DP: New Distribution-Level Metrics for Demographic Parity. (arXiv:2301.13443v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13443](http://arxiv.org/abs/2301.13443)

    本文提出两种新公平度量方法：概率密度函数曲线之间区域（ABPC）和累积密度函数曲线之间区域（ABCC），实现人口统计特征的公平机器学习

    

    人口统计特征的平等对待是机器学习中最广泛认可的公平度量标准。为实现人口统计平等，许多研究致力于追求常用度量方法$\Delta DP$。然而，本文揭示了公平度量方法$\Delta DP$存在固有缺陷：i) 零值$\Delta DP$不保证民族统计平等的零违规，ii) $\Delta DP$值随不同分类阈值变化。为此，提出了两种新公平度量方法——概率密度函数曲线之间区域（ABPC）和累积密度函数曲线之间区域（ABCC），以精确测量不同民族统计群体预测概率分布之间的差异。

    Demographic parity is the most widely recognized measure of group fairness in machine learning, which ensures equal treatment of different demographic groups. Numerous works aim to achieve demographic parity by pursuing the commonly used metric $\Delta DP$. Unfortunately, in this paper, we reveal that the fairness metric $\Delta DP$ can not precisely measure the violation of demographic parity, because it inherently has the following drawbacks: i) zero-value $\Delta DP$ does not guarantee zero violation of demographic parity, ii) $\Delta DP$ values can vary with different classification thresholds. To this end, we propose two new fairness metrics, Area Between Probability density function Curves (ABPC) and Area Between Cumulative density function Curves (ABCC), to precisely measure the violation of demographic parity at the distribution level. The new fairness metrics directly measure the difference between the distributions of the prediction probability for different demographic group
    
[^167]: 基于最小价值等价部分模型的生涯强化学习的可扩展和鲁棒规划

    Minimal Value-Equivalent Partial Models for Scalable and Robust Planning in Lifelong Reinforcement Learning. (arXiv:2301.10119v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10119](http://arxiv.org/abs/2301.10119)

    本文提出了一种仅模拟环境中相关方面的“最小价值等价部分模型”，并证明了这些模型用于规划在生涯强化学习场景中具有可扩展性优势。

    

    从纯交互中学习环境模型通常被认为是构建生涯强化学习智能体的至关组成部分。然而，在基于模型的强化学习中，通常的做法是学习模型来对智能体的环境的每个方面进行建模，无论这些方面是否在提出最优决策方面重要。本文认为这种模型并不适合在生涯强化学习场景中执行可扩展和鲁棒的规划，因此提出了只模拟环境中相关方面的新型模型，称为“最小价值等价部分模型”。本文提供了这些模型的正式定义，并提供了理论结果来证明使用这些模型进行规划的可扩展性优势，然后进行实验以从实证角度说明我们的理论结果，最后提出一些有用的启发式方法来学习这些模型。

    Learning models of the environment from pure interaction is often considered an essential component of building lifelong reinforcement learning agents. However, the common practice in model-based reinforcement learning is to learn models that model every aspect of the agent's environment, regardless of whether they are important in coming up with optimal decisions or not. In this paper, we argue that such models are not particularly well-suited for performing scalable and robust planning in lifelong reinforcement learning scenarios and we propose new kinds of models that only model the relevant aspects of the environment, which we call "minimal value-equivalent partial models". After providing a formal definition for these models, we provide theoretical results demonstrating the scalability advantages of performing planning with such models and then perform experiments to empirically illustrate our theoretical results. Then, we provide some useful heuristics on how to learn these kinds
    
[^168]: 一种针对合成数据的属性推断攻击的线性重构方法

    A Linear Reconstruction Approach for Attribute Inference Attacks against Synthetic Data. (arXiv:2301.10053v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10053](http://arxiv.org/abs/2301.10053)

    本研究提出一种新的属性推断攻击方法，针对合成数据中所有记录，具有高度的准确率和危害性。

    

    最近合成数据生成（SDG）的发展被誉为共享敏感数据和保护隐私的难题的解决方案。SDG旨在学习真实数据的统计属性，生成结构和统计学相似的“人造”数据。然而，先前的研究表明，对合成数据的推论攻击可能会破坏隐私，但仅适用于特定的异常记录。本研究引入了一种新的针对合成数据的属性推断攻击。该攻击基于线性重构方法用于聚合统计数据，攻击目标不仅仅是异常值，而是整个数据集中的所有记录。我们评估了我们的攻击对最先进的SDG算法的影响，包括概率图模型，生成式对抗网络和最近的差分隐私SDG机制。通过定义形式化的隐私博弈，我们展示了我们的攻击甚至对任意记录都可以高度精确，并且这是攻击的结果。

    Recent advances in synthetic data generation (SDG) have been hailed as a solution to the difficult problem of sharing sensitive data while protecting privacy. SDG aims to learn statistical properties of real data in order to generate "artificial" data that are structurally and statistically similar to sensitive data. However, prior research suggests that inference attacks on synthetic data can undermine privacy, but only for specific outlier records. In this work, we introduce a new attribute inference attack against synthetic data. The attack is based on linear reconstruction methods for aggregate statistics, which target all records in the dataset, not only outliers. We evaluate our attack on state-of-the-art SDG algorithms, including Probabilistic Graphical Models, Generative Adversarial Networks, and recent differentially private SDG mechanisms. By defining a formal privacy game, we show that our attack can be highly accurate even on arbitrary records, and that this is the result o
    
[^169]: 重新审视有符号传播在图神经网络中的应用

    Revisiting Signed Propagation for Graph Neural Networks. (arXiv:2301.08918v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.08918](http://arxiv.org/abs/2301.08918)

    该论文重新审视了在异质图中的有符号传播方法，提出了一种适用于多类别图的新策略，并克服了其带来的不确定性和不一致性问题，取得了显著的性能提升。

    

    信息传递式图神经网络（GNN）在同质图上表现出色。然而，在异质图上它们的性能却很差，许多研究者为解决这个问题提出了大量方案。特别地，翻转边的符号是基于坚实理论基础的并且可以获得显著的性能提升。然而，以前的分析假定了二元分类场景，因此受到应用范围的限制。本文将以前的理解扩展到多类别情况，并指出两个缺点：（1）多跳邻居的符号取决于消息传递路径，可能导致不一致性；（2）这也增加了预测的不确定性（例如，冲突证据），可能影响算法的稳定性。在理论的基础上，我们提出了一种新的策略，适用于多类别的图。所提出的方案结合了原有方案的优点，同时克服了其缺点并取得了显著的性能提升。

    Message-passing Graph Neural Networks (GNNs), which collect information from adjacent nodes, achieve satisfying results on homophilic graphs. However, their performances are dismal in heterophilous graphs, and many researchers have proposed a plethora of schemes to solve this problem. Especially, flipping the sign of edges is rooted in a strong theoretical foundation, and attains significant performance enhancements. Nonetheless, previous analyses assume a binary class scenario and they may suffer from confined applicability. This paper extends the prior understandings to multi-class scenarios and points out two drawbacks: (1) the sign of multi-hop neighbors depends on the message propagation paths and may incur inconsistency, (2) it also increases the prediction uncertainty (e.g., conflict evidence) which can impede the stability of the algorithm. Based on the theoretical understanding, we introduce a novel strategy that is applicable to multi-class graphs. The proposed scheme combine
    
[^170]: 用于预测Ar/N$_2$放电中AlN溅射和薄膜沉积的物理分离人工神经网络模型

    Physics-separating artificial neural networks for predicting sputtering and thin film deposition of AlN in Ar/N$_2$ discharges on experimental timescales. (arXiv:2301.03524v1 [cond-mat.mtrl-sci] CROSS LISTED)

    [http://arxiv.org/abs/2301.03524](http://arxiv.org/abs/2301.03524)

    本文开发了一种物理分离的人工神经网络模型，用于预测Ar/N$_2$放电中AlN溅射和薄膜沉积。该模型通过混合反应分子动力学和Monte Carlo模拟实现了填充参数空间的有效方法。

    

    理解和模拟等离子体-表面相互作用是一个多尺度和多物理过程的问题。本研究通过开发一种有效填充参数空间的方案来解决计算生成数据模拟的计算成本巨大的实际应用问题，从而针对Ar / N$_2$ 放电中 AlN 溅射沉积开发了物理分离人工神经网络。具体来说，通过混合反应分子动力学/时间戳力偏置Monte Carlo模拟随机等离子体-表面相互作用/扩散过程，建立了该模型。将这种通用的机器学习模型应用于具体的实验参考案例，可以系统分析粒子通量发射以及...

    Understanding and modeling plasma-surface interactions frame a multi-scale as well as multi-physics problem. Scale-bridging machine learning surface surrogate models have been demonstrated to perceive the fundamental atomic fidelity for the physical vapor deposition of pure metals. However, the immense computational cost of the data-generating simulations render a practical application with predictions on relevant timescales impracticable. This issue is resolved in this work for the sputter deposition of AlN in Ar/N$_2$ discharges by developing a scheme that populates the parameter spaces effectively. Hybrid reactive molecular dynamics / time-stamped force-bias Monte Carlo simulations of randomized plasma-surface interactions / diffusion processes are used to setup a physics-separating artificial neural network. The application of this generic machine learning model to a specific experimental reference case study enables the systematic analysis of the particle flux emission as well as 
    
[^171]: 基于Transformer邮件机制的多智能体强化学习可扩展沟通方法

    Scalable Communication for Multi-Agent Reinforcement Learning via Transformer-Based Email Mechanism. (arXiv:2301.01919v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2301.01919](http://arxiv.org/abs/2301.01919)

    本文提出了一种基于Transformer的邮件机制（TEM），解决了多智能体强化学习中的可扩展性问题，它采用本地通信和消息链转发的方式进行通信，而不需要模拟所有智能体。

    

    在多智能体强化学习中，通讯可以极大地改善合作，尤其是对于部分可观测的任务。然而，现有方法要么广播信息导致信息冗余，要么通过将所有其他智能体模拟成目标来学习有针对性的通信，但是当智能体数量变化时，这种方法无法扩展。为了解决部分可观察任务的可扩展性问题，我们提出了一种新的框架Transformer-based Email Mechanism（TEM）。智能体采用本地通信，仅向可以观察到的智能体发送消息，无需模拟所有智能体。受到人类电子邮件转发合作的启发，我们设计了消息链以将信息转发给观察范围之外的智能体。我们引入Transformer来编码和解码消息链，以选择下一个接收者。在多个合作的MARL基准测试中，实验证明TEM优于基线方法。

    Communication can impressively improve cooperation in multi-agent reinforcement learning (MARL), especially for partially-observed tasks. However, existing works either broadcast the messages leading to information redundancy, or learn targeted communication by modeling all the other agents as targets, which is not scalable when the number of agents varies. In this work, to tackle the scalability problem of MARL communication for partially-observed tasks, we propose a novel framework Transformer-based Email Mechanism (TEM). The agents adopt local communication to send messages only to the ones that can be observed without modeling all the agents. Inspired by human cooperation with email forwarding, we design message chains to forward information to cooperate with the agents outside the observation range. We introduce Transformer to encode and decode the message chain to choose the next receiver selectively. Empirically, TEM outperforms the baselines on multiple cooperative MARL benchma
    
[^172]: 抵御针对音频DeepFake检测的对抗性攻击

    Defense Against Adversarial Attacks on Audio DeepFake Detection. (arXiv:2212.14597v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2212.14597](http://arxiv.org/abs/2212.14597)

    本文旨在探讨音频DeepFake检测中对抗性攻击的问题，并通过对三种检测体系结构的鲁棒性评估以及采用自适应训练进行对抗训练来提高检测器的性能。其中，我们还首次适应了RawNet3体系结构用于DeepFake检测。

    

    音频DeepFake是使用深度学习生成的人工语音，其主要目的是以高度令人信服的方式欺骗听众。它们的质量足以在安全和隐私方面构成严重威胁，包括新闻的可靠性或诽谤。已经提出了多种基于神经网络的方法来检测生成的语音，以防止这些威胁。本研究涵盖了针对对抗性攻击的主题，这些攻击通过向输入数据添加表面的更难被人类察觉的变化来降低检测器的性能。我们的贡献包括在两个场景（白盒和使用可传递性）中评估3个检测体系结构对抗攻击的鲁棒性，并通过使用我们的新颖自适应训练进行对抗训练来加强它。此外，我们还首次适应了RawNet3体系结构用于DeepFake检测。

    Audio DeepFakes (DF) are artificially generated utterances created using deep learning, with the primary aim of fooling the listeners in a highly convincing manner. Their quality is sufficient to pose a severe threat in terms of security and privacy, including the reliability of news or defamation. Multiple neural network-based methods to detect generated speech have been proposed to prevent the threats. In this work, we cover the topic of adversarial attacks, which decrease the performance of detectors by adding superficial (difficult to spot by a human) changes to input data. Our contribution contains evaluating the robustness of 3 detection architectures against adversarial attacks in two scenarios (white-box and using transferability) and enhancing it later by using adversarial training performed by our novel adaptive training. Moreover, one of the investigated architectures is RawNet3, which, to the best of our knowledge, we adapted for the first time to DeepFake detection.
    
[^173]: 基于深度学习和模态分解的两相同心喷流预测

    Forecasting through deep learning and modal decomposition in two-phase concentric jets. (arXiv:2212.12731v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.12731](http://arxiv.org/abs/2212.12731)

    本论文提出了一种将机器学习和单相流数值模拟相结合的方法来实时预测和改善两相流的燃油/空气混合物，从而提高涡扇发动机燃油室喷射器的性能。

    

    本项工作旨在提高涡扇发动机燃油室喷射器的性能，从而意味着提高性能和减少污染物。为了实现实时预测和改善燃油/空气混合物，需要开发允许这种预测的模型。然而，到目前为止所做的工作涉及使用实验数据（难以测量）或完整问题的数值解（计算成本高昂）。后者涉及解决一个偏微分方程（PDE）系统。这些问题使得开发实时预测工具变得困难。因此，在本工作中，我们建议将机器学习与（相对更便宜的）单相流数值模拟结合起来，在存在切向不连续性的情况下估计两相流中的混合过程。在这个意义上，我们研究了两个提出的神经网络（NN）模型作为PDE代理模型的应用。NN通过预测未来动态来实现。

    This work aims to improve fuel chamber injectors' performance in turbofan engines, thus implying improved performance and reduction of pollutants. This requires the development of models that allow real-time prediction and improvement of the fuel/air mixture. However, the work carried out to date involves using experimental data (complicated to measure) or the numerical resolution of the complete problem (computationally prohibitive). The latter involves the resolution of a system of partial differential equations (PDE). These problems make difficult to develop a real-time prediction tool. Therefore, in this work, we propose using machine learning in conjunction with (complementarily cheaper) single-phase flow numerical simulations in the presence of tangential discontinuities to estimate the mixing process in two-phase flows. In this meaning we study the application of two proposed neural network (NN) models as PDE surrogate models. Where the future dynamics is predicted by the NN, gi
    
[^174]: 基于整数序列分析的数学常数推测自动搜索

    Automated Search for Conjectures on Mathematical Constants using Analysis of Integer Sequences. (arXiv:2212.09470v2 [math.NT] UPDATED)

    [http://arxiv.org/abs/2212.09470](http://arxiv.org/abs/2212.09470)

    本文提出了一种新的方法，通过分析整数序列来搜索数学常数的公式，推导出了多个常数的新公式，包括 $\pi$ 和 Catalan 常数。

    

    基础数学常数所构成的公式对于科学和数学领域有着重要的影响，同时也有助于证明这些常数的无理性。然而，历史上为这些公式的发现，仅限于极少数伟大的数学家，如拉马努金、欧拉和高斯，并且受制于搜索空间的限制和巨大的计算资源需求。本文提出了基于分析整数序列的全新方法，引入了 "枚举符号连分数-梅西验证 (ESMA) "算法，建立在 Berlekamp-Massey 算法的基础上，以识别与数学常数相关的整数序列的模式。我们的算法能够发现几个知名数学常数的新公式，包括 $\pi$ 和 Catalan 常数。ESMA 算法为自动发现数学常数的公式提供了全新且有前途的方向。

    Formulas involving fundamental mathematical constants had a great impact on various fields of science and mathematics, for example aiding in proofs of irrationality of constants. However, the discovery of such formulas has historically remained scarce, often perceived as an act of mathematical genius by great mathematicians such as Ramanujan, Euler, and Gauss. Recent efforts to automate the discovery of formulas for mathematical constants, such as the Ramanujan Machine project, relied on exhaustive search. Despite several successful discoveries, exhaustive search remains limited by the space of options that can be covered and by the need for vast amounts of computational resources. Here we propose a fundamentally different method to search for conjectures on mathematical constants: through analysis of integer sequences. We introduce the Enumerated Signed-continued-fraction Massey Approve (ESMA) algorithm, which builds on the Berlekamp-Massey algorithm to identify patterns in integer se
    
[^175]: 洛伦兹群等变自编码器

    Lorentz group equivariant autoencoders. (arXiv:2212.07347v2 [hep-ex] UPDATED)

    [http://arxiv.org/abs/2212.07347](http://arxiv.org/abs/2212.07347)

    本论文提出了一种洛伦兹群自编码器（LGAE）模型，它具有上述群的等变性，且在高能物理数据的压缩、重构和异常检测等任务方面表现出色。

    

    近年来，高能物理中使用机器学习（ML）模型进行分类、仿真和异常检测等任务的工作已经取得了显著进展。通常情况下，这些模型是从针对计算机视觉或自然语言处理数据集设计的模型进行改进的，这些模型缺乏适合于高能物理数据的归纳偏差，例如以其固有的对称性等变。这些偏差已被证明可以使模型更加性能优良和可解释，并减少所需的训练数据量。为此，我们开发了洛伦兹群自编码器（LGAE），这是一个自编码器模型，相对于适当的、正交的洛伦兹群$\mathrm{SO}^+(3,1)$等变，并且潜在空间在群的表示中。我们提出了我们的体系结构，并在LHC的喷注上展示了几个实验结果，并发现它在几个压缩、重构和异常检测指标上优于图形和卷积神经网络基线模型。我们还证明...

    There has been significant work recently in developing machine learning (ML) models in high energy physics (HEP) for tasks such as classification, simulation, and anomaly detection. Often these models are adapted from those designed for datasets in computer vision or natural language processing, which lack inductive biases suited to HEP data, such as equivariance to its inherent symmetries. Such biases have been shown to make models more performant and interpretable, and reduce the amount of training data needed. To that end, we develop the Lorentz group autoencoder (LGAE), an autoencoder model equivariant with respect to the proper, orthochronous Lorentz group $\mathrm{SO}^+(3,1)$, with a latent space living in the representations of the group. We present our architecture and several experimental results on jets at the LHC and find it outperforms graph and convolutional neural network baseline models on several compression, reconstruction, and anomaly detection metrics. We also demons
    
[^176]: 区块链上的AI伦理: 基于Twitter数据的区块链安全主题分析

    AI Ethics on Blockchain: Topic Analysis on Twitter Data for Blockchain Security. (arXiv:2212.06951v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2212.06951](http://arxiv.org/abs/2212.06951)

    本研究分析了Twitter上与MEV相关的话题，结果表明推文讨论了深刻的伦理问题，包括安全、公平、情感情绪和对MEV解决方案的渴望。

    

    区块链使用分布式网络让计算机系统更安全，但当前的区块链设计在交易顺序方面存在公平性问题。矿工可以重新排序交易以生成利润，这被称为矿工可提取价值（MEV）问题。现有研究认为MEV是一个严重的安全问题，并提出了潜在的解决方案，包括知名的Flashbots。然而，以往的研究大多分析了区块链数据，这可能无法捕捉到MEV在更广泛的AI社会中的影响。因此，在这项研究中，我们应用自然语言处理（NLP）方法全面分析了MEV推文中的话题。我们收集了超过20,000个MEV和Flashbots标签的推文并分析了它们的话题。我们的结果显示，这些推文讨论了深刻的伦理问题，包括安全、公平、情感情绪和对MEV解决方案的渴望。我们还发现了区块链上MEV活动的共同运动。

    Blockchain has empowered computer systems to be more secure using a distributed network. However, the current blockchain design suffers from fairness issues in transaction ordering. Miners are able to reorder transactions to generate profits, the so-called miner extractable value (MEV). Existing research recognizes MEV as a severe security issue and proposes potential solutions, including prominent Flashbots. However, previous studies have mostly analyzed blockchain data, which might not capture the impacts of MEV in a much broader AI society. Thus, in this research, we applied natural language processing (NLP) methods to comprehensively analyze topics in tweets on MEV. We collected more than 20000 tweets with MEV and Flashbots hashtags and analyzed their topics. Our results show that the tweets discussed profound topics of ethical concern, including security, equity, emotional sentiments, and the desire for solutions to MEV. We also identify the co-movements of MEV activities on block
    
[^177]: 可解释的性能：衡量预测性能的驱动力

    Explainable Performance: Measuring the Driving Forces of Predictive Performance. (arXiv:2212.05866v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2212.05866](http://arxiv.org/abs/2212.05866)

    XPER方法能衡量输入特征对模型预测性能的具体贡献，并可用于处理异质性问题，构建同质化个体群体，从而提高预测精度。

    

    我们引入了XPER（eXplainable PERformance）方法来衡量输入特征对模型预测性能的具体贡献。我们的方法在理论上基于Shapley值，既不依赖于模型，也不依赖于性能度量。此外，XPER可在模型级别或个体级别实现。我们证明XPER具有标准解释性方法（SHAP）的特殊情况。在贷款违约预测应用中，我们展示了如何利用XPER处理异质性问题，并显著提高样本外性能。为此，我们通过基于个体XPER值对他们进行聚类来构建同质化的个体群体。我们发现估计群体特定的模型比一个模型适用于所有个体具有更高的预测精度。

    We introduce the XPER (eXplainable PERformance) methodology to measure the specific contribution of the input features to the predictive performance of a model. Our methodology is theoretically grounded on Shapley values and is both model-agnostic and performance metric-agnostic. Furthermore, XPER can be implemented either at the model level or at the individual level. We demonstrate that XPER has as a special case the standard explainability method in machine learning (SHAP). In a loan default forecasting application, we show how XPER can be used to deal with heterogeneity issues and significantly boost out-of-sample performance. To do so, we build homogeneous groups of individuals by clustering them based on their individual XPER values. We find that estimating group-specific models yields a much higher predictive accuracy than with a one-fits-all model.
    
[^178]: 从模型梯度重构训练数据，具有可证明性。

    Reconstructing Training Data from Model Gradient, Provably. (arXiv:2212.03714v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.03714](http://arxiv.org/abs/2212.03714)

    通过单个梯度查询可重构训练数据，存在隐私泄露威胁。

    

    在隐私方面，理解模型梯度何时以及如何泄露有关训练样本的信息是一个重要问题。在本文中，我们提出了一个令人惊讶的结果：即使没有训练或记忆数据，我们也可以从在随机选择的参数值处进行的单个梯度查询中完全重构训练样本。我们证明了在温和条件下的训练数据的可识别性：使用浅层或深层神经网络和各种激活函数。我们还提出了一种基于张量分解的统计和计算高效算法来重构训练数据。作为揭示敏感训练数据的可证明攻击，我们的发现表明了对隐私的潜在严重威胁，尤其是在联合学习中。

    Understanding when and how much a model gradient leaks information about the training sample is an important question in privacy. In this paper, we present a surprising result: even without training or memorizing the data, we can fully reconstruct the training samples from a single gradient query at a randomly chosen parameter value. We prove the identifiability of the training data under mild conditions: with shallow or deep neural networks and a wide range of activation functions. We also present a statistically and computationally efficient algorithm based on tensor decomposition to reconstruct the training data. As a provable attack that reveals sensitive training data, our findings suggest potential severe threats to privacy, especially in federated learning.
    
[^179]: 联邦神经主题模型

    Federated Neural Topic Models. (arXiv:2212.02269v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.02269](http://arxiv.org/abs/2212.02269)

    该论文提出了一种基于神经主题建模实现的联邦神经主题模型，可以在不共享数据的情况下允许多个方共同训练主题模型和保护节点隐私。

    

    近年来，主题建模已成为组织和总结大型文档集合或在其中搜索特定模式的强大技术。然而，当从不同来源交叉分析数据时，可能会出现隐私问题。联邦主题建模通过允许多个方共同训练主题模型而不共享其数据来解决此问题。我们提出并分析了一种基于最先进的神经主题建模实现的联邦实现，显示其在节点文档的主题多样性和建立联合模型的需要时的优势。在实践中，我们的方法相当于集中模型训练，但保护节点的隐私。使用基准数据集进行的实验说明了这种联邦场景的优点。

    Over the last years, topic modeling has emerged as a powerful technique for organizing and summarizing big collections of documents or searching for particular patterns in them. However, privacy concerns may arise when cross-analyzing data from different sources. Federated topic modeling solves this issue by allowing multiple parties to jointly train a topic model without sharing their data. While several federated approximations of classical topic models do exist, no research has been conducted on their application for neural topic models. To fill this gap, we propose and analyze a federated implementation based on state-of-the-art neural topic modeling implementations, showing its benefits when there is a diversity of topics across the nodes' documents and the need to build a joint model. In practice, our approach is equivalent to a centralized model training, but preserves the privacy of the nodes. Advantages of this federated scenario are illustrated by means of experiments using b
    
[^180]: 线性干预下的因果分离

    Linear Causal Disentanglement via Interventions. (arXiv:2211.16467v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.16467](http://arxiv.org/abs/2211.16467)

    本文探讨了线性潜在因果模型中的因果分离问题，指出对于识别性干预数据是必要的，而每个潜在变量的单一干预就足够了。

    

    因果分离旨在通过一个因果模型来表示涉及的潜在变量。如果潜在模型和从潜在变量到观测变量的转换都是唯一的，则表示是可识别的。本文研究了观测变量是线性潜在因果模型的线性转换。对于识别性，干预数据是必要的：如果一个潜在变量缺少干预，我们将展示存在无法区分的不同模型。反之，我们展示每个潜在变量的单一干预就足够了。我们的证明使用了一个矩阵的RQ分解的推广，取代了通常的正交和上三角条件，而是用基于潜在因果模型确定的行的偏序的类似条件。我们利用一个用于因果分离的方法来证明了我们的理论结果。

    Causal disentanglement seeks a representation of data involving latent variables that relate to one another via a causal model. A representation is identifiable if both the latent model and the transformation from latent to observed variables are unique. In this paper, we study observed variables that are a linear transformation of a linear latent causal model. Data from interventions are necessary for identifiability: if one latent variable is missing an intervention, we show that there exist distinct models that cannot be distinguished. Conversely, we show that a single intervention on each latent variable is sufficient for identifiability. Our proof uses a generalization of the RQ decomposition of a matrix that replaces the usual orthogonal and upper triangular conditions with analogues depending on a partial order on the rows of the matrix, with partial order determined by a latent causal model. We corroborate our theoretical results with a method for causal disentanglement that ac
    
[^181]: 基于置信度的图神经网络用于学习可靠性评估承诺

    Confidence-Aware Graph Neural Networks for Learning Reliability Assessment Commitments. (arXiv:2211.15755v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15755](http://arxiv.org/abs/2211.15755)

    本论文针对可靠性评估承诺的优化问题，利用基于 GNN 的结构预测发电机的承诺和约束，并通过对高置信度预测结果的可行性修复，提高了最先进算法的解决方案质量。

    

    由于可再生能源的比例增加和预测误差的提高，可靠性评估承诺(Reliability Assessment Commitment, RAC)优化在电网运行中变得越来越重要。本研究旨在解决RAC公式扩展所带来的计算挑战。它提出了RACLearn，利用基于Graph Neural Network (GNN)的架构来预测发电机的承诺和线路约束，为每个承诺预测关联一个置信度值，并选择一组高置信度预测，对其进行可行性修复，并利用可行的预测与约束状态引导最先进的优化算法。实验结果表明，RACLearn在Midcontinent Independent System Operator (MISO)使用的准确RAC公式上，比现有的最先进方法具有显着更好的解决方案质量。

    Reliability Assessment Commitment (RAC) Optimization is increasingly important in grid operations due to larger shares of renewable generations in the generation mix and increased prediction errors. Independent System Operators (ISOs) also aim at using finer time granularities, longer time horizons, and possibly stochastic formulations for additional economic and reliability benefits. The goal of this paper is to address the computational challenges arising in extending the scope of RAC formulations. It presents RACLearn that (1) uses a Graph Neural Network (GNN) based architecture to predict generator commitments and active line constraints, (2) associates a confidence value to each commitment prediction, (3) selects a subset of the high-confidence predictions, which are (4) repaired for feasibility, and (5) seeds a state-of-the-art optimization algorithm with feasible predictions and active constraints. Experimental results on exact RAC formulations used by the Midcontinent Independe
    
[^182]: 用于未知物体实例分割的均值漂移掩模变换器

    Mean Shift Mask Transformer for Unseen Object Instance Segmentation. (arXiv:2211.11679v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.11679](http://arxiv.org/abs/2211.11679)

    本文提出了一种新的均值漂移掩模变换器，用于联合训练和推断特征提取器和聚类器，可用于未知物体实例分割，在COCO数据集上表现出竞争性的性能，并在罕见和未知物体类别上具有显着优势。

    

    物体实例的分割是机器人需要掌握的关键感知技能之一，它有助于机器人抓取和操作未知物体。均值漂移聚类是一种广泛用于图像分割任务的方法。然而，传统的均值漂移聚类算法不可微分，使其难以集成到端到端的神经网络训练框架中。在本文中，我们提出了均值漂移掩模变换器（MSMFormer），这是一种新的变换器体系结构，模拟 von Mises-Fisher（vMF）均值漂移聚类算法，允许联合训练和推断特征提取器和聚类器。其核心组件是超球面注意力机制，可在超球面上更新物体查询。为了说明我们方法的有效性，我们将MSMFormer应用于未知物体实例分割。实验结果表明，MSMFormer在COCO数据集上与现有方法相比取得了竞争性的性能，并且在罕见和未知物体类别上具有显着优势。

    Segmenting unseen objects from images is a critical perception skill that a robot needs to acquire. In robot manipulation, it can facilitate a robot to grasp and manipulate unseen objects. Mean shift clustering is a widely used method for image segmentation tasks. However, the traditional mean shift clustering algorithm is not differentiable, making it difficult to integrate it into an end-to-end neural network training framework. In this work, we propose the Mean Shift Mask Transformer (MSMFormer), a new transformer architecture that simulates the von Mises-Fisher (vMF) mean shift clustering algorithm, allowing for the joint training and inference of both the feature extractor and the clustering. Its central component is a hypersphere attention mechanism, which updates object queries on a hypersphere. To illustrate the effectiveness of our method, we apply MSMFormer to unseen object instance segmentation. Our experiments show that MSMFormer achieves competitive performance compared to
    
[^183]: 冻结过度参数化：对深度神经网络转移学习的双峰透视研究

    Frozen Overparameterization: A Double Descent Perspective on Transfer Learning of Deep Neural Networks. (arXiv:2211.11074v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.11074](http://arxiv.org/abs/2211.11074)

    本文研究了深度神经网络（DNN）转移学习的泛化行为。我们发现，目标数据集大时，测试误差演变会有更明显的双峰效应。更大的源数据集可能会导致较慢的目标DNN训练。此外，我们证明了冻结层数量可以确定转移学习是否有效地低于...

    

    本文研究了深度神经网络（DNN）转移学习的泛化行为。我们采用过度参数化的视角——特征插值（即近似于零的训练误差）和双峰现象——来解释转移学习设置对泛化性能的微妙影响。我们研究了源任务和目标任务的数据集大小、在目标DNN训练中保持冻结的转移层数量以及源和目标任务之间的相似度对转移学习泛化行为的影响。我们表明，当目标训练数据集足够大时，在目标DNN训练过程中测试误差演变会有更明显的双峰效应。此外，更大的源训练数据集可能导致较慢的目标DNN训练。此外，我们证明了冻结层数量可以确定转移学习是否有效地低于...

    We study the generalization behavior of transfer learning of deep neural networks (DNNs). We adopt the overparameterization perspective -- featuring interpolation of the training data (i.e., approximately zero train error) and the double descent phenomenon -- to explain the delicate effect of the transfer learning setting on generalization performance. We study how the generalization behavior of transfer learning is affected by the dataset size in the source and target tasks, the number of transferred layers that are kept frozen in the target DNN training, and the similarity between the source and target tasks. We show that the test error evolution during the target DNN training has a more significant double descent effect when the target training dataset is sufficiently large. In addition, a larger source training dataset can yield a slower target DNN training. Moreover, we demonstrate that the number of frozen layers can determine whether the transfer learning is effectively underpar
    
[^184]: 带装载和覆盖约束的上下文幸存者问题：基于回归的模块化Lagrangian方法

    Contextual Bandits with Packing and Covering Constraints: A Modular Lagrangian Approach via Regression. (arXiv:2211.07484v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.07484](http://arxiv.org/abs/2211.07484)

    该论文研究了一种带有资源线性约束的上下文幸存者问题的变种，提出了一种新的算法，该算法简单、计算效率高，同时能够实现较低的后悔。此外，当某些约束被违反时，算法在统计上是最优的。

    

    我们考虑一种上下文幸存者问题的变种，其中算法在总消费的线性约束下使用多个资源。这个问题推广了带背包的上下文幸存者问题(CBwK)，允许装载和覆盖约束，以及正负资源消耗。我们提出了一种新算法，简单、计算效率高，能够实现退化的后悔。当某些约束被违反时，对于CBwK，它在统计上是最优的。我们的算法基于LagrangianBwK(Immorlica等人，FOCS 2019)，这是一种面向CBwK的Lagrangian技术，以及SquareCB(Foster和Rakhlin，ICML 2020)，这是一种面向上下文幸存者的回归技术。我们的分析利用了两种技术本质上的模块化。

    We consider a variant of contextual bandits in which the algorithm consumes multiple resources subject to linear constraints on total consumption. This problem generalizes contextual bandits with knapsacks (CBwK), allowing for packing and covering constraints, as well as positive and negative resource consumption. We present a new algorithm that is simple, computationally efficient, and admits vanishing regret. It is statistically optimal for CBwK when an algorithm must stop once some constraint is violated. Our algorithm builds on LagrangeBwK (Immorlica et al., FOCS 2019) , a Lagrangian-based technique for CBwK, and SquareCB (Foster and Rakhlin, ICML 2020), a regression-based technique for contextual bandits. Our analysis leverages the inherent modularity of both techniques.
    
[^185]: 相对熵正则化的经验风险最小化问题

    Empirical Risk Minimization with Relative Entropy Regularization. (arXiv:2211.06617v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2211.06617](http://arxiv.org/abs/2211.06617)

    本文研究了相对熵正则化的经验风险最小化问题，其中参考度量为sigma有限测度，解为唯一的概率测度并展现了几乎正确的保证。ERM-RER问题的解被称为Gibbs算法。

    

    在假定参考度量为sigma有限测度（measure）而非概率测度的情况下，研究了相对熵正则化的经验风险最小化（ERM-RER）问题。在这种假设下，存在一个ERM-RER问题的泛化，允许更大程度地灵活地并入先验知识。在这些性质中，如果存在ERM-RER问题的解，则该解是唯一的概率测度，通常与参考测度相互绝对连续。这样的解对于ERM问题展现了几乎正确的保证，而不需关心ERM问题是否有解。当从ERM-RER问题的解抽取模型时，固定数据集时，经验风险被证明是一个亚高斯随机变量。ERM-RER问题的解（Gibbs算法）的泛化能力得到了验证。

    The empirical risk minimization (ERM) problem with relative entropy regularization (ERM-RER) is investigated under the assumption that the reference measure is a {\sigma}-finite measure, and not necessarily a probability measure. Under this assumption, which leads to a generalization of the ERM-RER problem allowing a larger degree of flexibility for incorporating prior knowledge, numerous relevant properties are stated. Among these properties, the solution to this problem, if it exists, is shown to be a unique probability measure, often mutually absolutely continuous with the reference measure. Such a solution exhibits a probably-approximately-correct guarantee for the ERM problem independently of whether the latter possesses a solution. For a fixed dataset, the empirical risk is shown to be a sub-Gaussian random variable when the models are sampled from the solution to the ERM-RER problem. The generalization capabilities of the solution to the ERM-RER problem (the Gibbs algorithm) are
    
[^186]: 物理分离的人工神经网络用于预测Ar等离子体放电中Al溅射和薄膜沉积的初期阶段

    Physics-separating artificial neural networks for predicting initial stages of Al sputtering and thin film deposition in Ar plasma discharges. (arXiv:2211.04796v1 [cond-mat.mtrl-sci] CROSS LISTED)

    [http://arxiv.org/abs/2211.04796](http://arxiv.org/abs/2211.04796)

    通过引入物理分离的人工神经网络，共同描述了Al的溅射和薄膜生长的不断发展的表面状态和缺陷结构，全面描述了基本过程。

    

    Al薄膜溅射沉积的模拟依赖于准确的等离子体和表面相互作用模型。建立后者通常需要更高的抽象程度和忽略基本的原子保真度。以前关于溅射过程的工作通过建立机器学习代理模型来解决这个问题，其中包括基本的表面状态（即化学计量）作为静态输入。在这项工作中，引入了一个不断发展的表面状态和缺陷结构，并使用物理分离的人工神经网络来共同描述溅射和生长。描述等离子体-表面相互作用的数据来自于Al中性粒子和Ar$^+$离子撞击Al（001）表面的混合反应分子动力学/时间戳力偏差Monte Carlo模拟。证明通过考虑表面状态和缺陷结构来全面描述基本过程。因此，机器学习等离子体-表面相互作用su

    Simulations of Al thin film sputter depositions rely on accurate plasma and surface interaction models. Establishing the latter commonly requires a higher level of abstraction and means to dismiss the fundamental atomic fidelity. Previous works on sputtering processes addressed this issue by establishing machine learning surrogate models, which include a basic surface state (i.e., stoichiometry) as static input. In this work, an evolving surface state and defect structure are introduced to jointly describe sputtering and growth with physics-separating artificial neural networks. The data describing the plasma-surface interactions stem from hybrid reactive molecular dynamics/time-stamped force bias Monte Carlo simulations of Al neutrals and Ar$^+$ ions impinging onto Al(001) surfaces. It is demonstrated that the fundamental processes are comprehensively described by taking the surface state as well as defect structure into account. Hence, a machine learning plasma-surface interaction su
    
[^187]: 基于扩散过程的熵神经最优传输算法

    Entropic Neural Optimal Transport via Diffusion Processes. (arXiv:2211.01156v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01156](http://arxiv.org/abs/2211.01156)

    这项研究提出了一种新的神经算法，用于计算连续概率分布之间的熵最优传输(EOT)计划，它具有端到端学习、快速推理和处理小值熵正则化系数的优点，可在大规模EOT任务中发挥出色的性能。

    

    我们提出了一种新颖的神经算法，用于计算连续概率分布之间的熵最优传输(EOT)计划，这些分布可通过样本获得。我们的算法基于动态版本EOT的鞍点重构，即Schrödinger桥问题。与大规模EOT的先前方法相比，我们的算法是端到端的，由单个学习步骤组成，具有快速的推理过程，并允许处理熵正则化系数的小值，这在某些实际应用问题中非常重要。在实证方面，我们展示了该方法在几个大规模EOT任务中的性能。

    We propose a novel neural algorithm for the fundamental problem of computing the entropic optimal transport (EOT) plan between continuous probability distributions which are accessible by samples. Our algorithm is based on the saddle point reformulation of the dynamic version of EOT which is known as the Schr\"odinger Bridge problem. In contrast to the prior methods for large-scale EOT, our algorithm is end-to-end and consists of a single learning step, has fast inference procedure, and allows handling small values of the entropy regularization coefficient which is of particular importance in some applied problems. Empirically, we show the performance of the method on several large-scale EOT tasks.
    
[^188]: 生成多语言的性别不明确的文本转语音声音

    Generating Multilingual Gender-Ambiguous Text-to-Speech Voices. (arXiv:2211.00375v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2211.00375](http://arxiv.org/abs/2211.00375)

    该论文介绍了一种生成多语言的性别不明确的TTS声音的方法，通过提出的性别感知方法从潜在说话人中有效地进行采样，成功生成了一系列新的、多样化的、一致性和性别不明确性更强的声音，具有很强的实验表现。

    

    语音用户界面的性别是其被感知身份的关键元素。最近，越来越多的界面开始采用不明确的性别，而不是明确界定为男性或女性。这项工作解决了在多说话人，多语言环境中生成新的性别不明确的TTS语音的任务。这是通过使用提出的性别感知方法有效地从潜在的说话人嵌入空间中进行采样来实现的。广泛的客观和主观评估清楚地表明，该方法能够有效地生成一系列新的、多样化的声音，这些声音在所有考察的语言中都被认为比基线声音更具性别不明确性。有趣的是，性别认知被发现在听众的两个人口统计因素方面具有鲁棒性：母语和性别。据我们所知，这是第一个可以可靠地生成多种性别不明确声音的系统性和经过验证的方法。

    The gender of any voice user interface is a key element of its perceived identity. Recently, there has been increasing interest in interfaces where the gender is ambiguous rather than clearly identifying as female or male. This work addresses the task of generating novel gender-ambiguous TTS voices in a multi-speaker, multilingual setting. This is accomplished by efficiently sampling from a latent speaker embedding space using a proposed gender-aware method. Extensive objective and subjective evaluations clearly indicate that this method is able to efficiently generate a range of novel, diverse voices that are consistent and perceived as more gender-ambiguous than a baseline voice across all the languages examined. Interestingly, the gender perception is found to be robust across two demographic factors of the listeners: native language and gender. To our knowledge, this is the first systematic and validated approach that can reliably generate a variety of gender-ambiguous voices.
    
[^189]: MARS: 函数空间中基于分数匹配的元学习

    MARS: Meta-Learning as Score Matching in the Function Space. (arXiv:2210.13319v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13319](http://arxiv.org/abs/2210.13319)

    本文提出了一种新的元学习方法，通过在函数空间中执行推理，从而避免了指定高维神经网络参数的先验分布族时的限制，可以无缝获取和表示复杂的先验知识。

    

    元学习旨在从一组相关的数据集中提取有用的归纳偏置。在贝叶斯元学习中，通常通过构建神经网络参数的先验分布来实现这一点。然而，指定一组可行的高维神经网络参数的先验分布族是困难的。因此，现有方法采用元学习限制性的对角高斯先验，严重限制了其表达能力和性能。为了解决这些问题，我们通过函数贝叶斯神经网络推理的视角来看待元学习，将先验视为随机过程，在函数空间中执行推理。具体来说，我们将元训练任务视为从数据生成过程中的样本，并将元学习形式化为经验估计这个随机过程的定律。我们的方法可以通过元学习分数函数，无缝获取和表示复杂的先验知识。

    Meta-learning aims to extract useful inductive biases from a set of related datasets. In Bayesian meta-learning, this is typically achieved by constructing a prior distribution over neural network parameters. However, specifying families of computationally viable prior distributions over the high-dimensional neural network parameters is difficult. As a result, existing approaches resort to meta-learning restrictive diagonal Gaussian priors, severely limiting their expressiveness and performance. To circumvent these issues, we approach meta-learning through the lens of functional Bayesian neural network inference, which views the prior as a stochastic process and performs inference in the function space. Specifically, we view the meta-training tasks as samples from the data-generating process and formalize meta-learning as empirically estimating the law of this stochastic process. Our approach can seamlessly acquire and represent complex prior knowledge by meta-learning the score functi
    
[^190]: NeuroPrim: 一种基于注意力机制的解NP-hard生成树问题的模型

    NeuroPrim: An Attention-based Model for Solving NP-hard Spanning Tree Problems. (arXiv:2210.12453v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.12453](http://arxiv.org/abs/2210.12453)

    本文提出了一种名为NeuroPrim的基于注意力机制的模型来解决生成树问题，采用Prim算法减少了动作和状态空间，并使用REINFORCE训练了结果模型，应用于欧几里得空间中的三个困难问题。

    

    具有特殊约束条件的生成树问题在现实场景中往往难以解决，通常需要复杂的算法设计和指数级的时间。最近，对于解决路由问题的端到端深度神经网络越来越受到关注。然而，这种方法通常会产生顶点序列，这使得难以将其应用于解决由边构成解集的一般组合优化问题，例如各种生成树问题。本文提出了一种名为NeuroPrim的新框架，通过为图上的一般组合优化问题定义马尔科夫决策过程（MDP）来解决各种生成树问题。我们的方法利用Prim算法减少了动作和状态空间，并使用REINFORCE训练了结果模型。我们将我们的框架应用于欧几里得空间中的三个困难问题：度约束最小生成树（DCMST）问题，最小路由成本生成树（MRCT）问题和最小实际距离生成树的问题（MST）。

    Spanning tree problems with specialized constraints can be difficult to solve in real-world scenarios, often requiring intricate algorithmic design and exponential time. Recently, there has been growing interest in end-to-end deep neural networks for solving routing problems. However, such methods typically produce sequences of vertices, which makes it difficult to apply them to general combinatorial optimization problems where the solution set consists of edges, as in various spanning tree problems. In this paper, we propose NeuroPrim, a novel framework for solving various spanning tree problems by defining a Markov Decision Process (MDP) for general combinatorial optimization problems on graphs. Our approach reduces the action and state space using Prim's algorithm and trains the resulting model using REINFORCE. We apply our framework to three difficult problems on Euclidean space: the Degree-constrained Minimum Spanning Tree (DCMST) problem, the Minimum Routing Cost Spanning Tree (M
    
[^191]: 深度多分支卷积神经网络结构用于基于脑MRIs的早期阿尔茨海默病检测

    Deep Multi-Branch CNN Architecture for Early Alzheimer's Detection from Brain MRIs. (arXiv:2210.12331v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2210.12331](http://arxiv.org/abs/2210.12331)

    本论文提出了一种基于脑MRIs的早期阿尔茨海默病检测方法，使用深度多分支卷积神经网络结构并达到了99.05%的三分类准确率。

    

    阿尔茨海默病是一种神经退行性疾病，可能导致痴呆和严重的大脑功能下降，如果没有预防性护理，会抑制简单任务的完成。逾9分之1的美国人患有由AD引起的痴呆症，未得到报酬的AD相关痴呆患者护理价值为2716亿美元。因此，为防止AD进一步进展，已开发了各种方法进行早期AD诊断。在本文中，我们首先回顾了其他可用于早期AD检测的方法。然后我们介绍了我们的数据集，该数据集来自阿尔茨海默病神经影像学倡议（ADNI），并提出了一个由7866819个参数组成的深度卷积神经网络架构。该模型具有三个不同的卷积分支，每个分支的长度不同。每个分支由不同的卷积核大小组成。该模型可以预测患有非痴呆、轻度痴呆或中度痴呆的患者，其三分类准确率为99.05％。

    Alzheimer's disease (AD) is a neuro-degenerative disease that can cause dementia and result severe reduction in brain function inhibiting simple tasks especially if no preventative care is taken. Over 1 in 9 Americans suffer from AD induced dementia and unpaid care for people with AD related dementia is valued at $271.6 billion. Hence, various approaches have been developed for early AD diagnosis to prevent its further progression. In this paper, we first review other approaches that could be used for early detection of AD. We then give an overview of our dataset that was from the Alzheimer's Disease Neuroimaging Initiative (ADNI) and propose a deep Convolutional Neural Network (CNN) architecture consisting of 7,866,819 parameters. This model has three different convolutional branches with each having a different length. Each branch is comprised of different kernel sizes. This model can predict whether a patient is non-demented, mild-demented, or moderately demented with a 99.05% three
    
[^192]: 学习反演：联邦学习中用于梯度反演的简单自适应攻击

    Learning to Invert: Simple Adaptive Attacks for Gradient Inversion in Federated Learning. (arXiv:2210.10880v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10880](http://arxiv.org/abs/2210.10880)

    该论文研究了联邦学习中的梯度反演问题，提出了一种简单自适应攻击方法，揭示了现有防御机制的不足之处。

    

    梯度反演攻击可以从联邦学习 (FL) 模型梯度中恢复训练样本，构成对数据隐私的严重威胁。为了减轻此漏洞，先前的工作提出了基于差分隐私的原则性防御和基于梯度压缩的启发式防御来作为对策。迄今为止，这些防御非常有效，特别是基于梯度压缩的防御，该防御允许模型在保持高准确性的同时极大地降低了攻击的效果。在本研究中，我们认为这些发现低估了 FL 的隐私风险。作为反例，我们展示了一种简单自适应攻击，其中一个在辅助数据上训练的模型能够反演视觉和语言任务的梯度。

    Gradient inversion attack enables recovery of training samples from model gradients in federated learning (FL), and constitutes a serious threat to data privacy. To mitigate this vulnerability, prior work proposed both principled defenses based on differential privacy, as well as heuristic defenses based on gradient compression as countermeasures. These defenses have so far been very effective, in particular those based on gradient compression that allow the model to maintain high accuracy while greatly reducing the effectiveness of attacks. In this work, we argue that such findings underestimate the privacy risk in FL. As a counterexample, we show that existing defenses can be broken by a simple adaptive attack, where a model trained on auxiliary data is able to invert gradients on both vision and language tasks.
    
[^193]: 怠惰神经元现象：变压器模型激活稀疏性的出现

    The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers. (arXiv:2210.06313v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.06313](http://arxiv.org/abs/2210.06313)

    本文研究了使用变压器模型的机器学习模型中激活图的稀疏现象，发现在不同层数的变压器配置和其他体系结构中都出现了稀疏现象。

    

    本文研究了变压器模型的机器学习模型的激活图稀疏的奇特现象。我们通过中间层多层感知器（MLP）使用ReLU激活函数的输出来表示激活图，稀疏是指平均情况下每个输入到MLP的非零元素非常少（例如，T5-Base为3.0％，ViT-B16为6.3％）。此外，较大的变压器和更宽的MLP隐藏层维度会产生更稀疏的激活图。通过大量实验，我们证明了稀疏的出现是一种普遍现象，它出现在自然语言处理和视觉任务中，出现在训练和评估数据中，在不同层数的变压器配置和其他体系结构中，也包括MLP-混合器和2层MLP。我们还表明，使用具有随机标签或随机输入的训练数据集也会出现稀疏现象。

    This paper studies the curious phenomenon for machine learning models with Transformer architectures that their activation maps are sparse. By activation map we refer to the intermediate output of the multi-layer perceptrons (MLPs) after a ReLU activation function, and by sparse we mean that on average very few entries (e.g., 3.0% for T5-Base and 6.3% for ViT-B16) are nonzero for each input to MLP. Moreover, larger Transformers with more layers and wider MLP hidden dimensions are sparser as measured by the percentage of nonzero entries. Through extensive experiments we demonstrate that the emergence of sparsity is a prevalent phenomenon that occurs for both natural language processing and vision tasks, on both training and evaluation data, for Transformers of various configurations, at layers of all depth levels, as well as for other architectures including MLP-mixers and 2-layer MLPs. We show that sparsity also emerges using training datasets with random labels, or with random inputs,
    
[^194]: 混合池化在基于Mixup的图形学习中的有效性研究

    On the Effectiveness of Hybrid Pooling in Mixup-Based Graph Learning for Language Processing. (arXiv:2210.03123v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.03123](http://arxiv.org/abs/2210.03123)

    本文探讨了图池化算子对于Manifold-Mixup方法在图形学习中的影响，提出了一种新颖的混合池化架构，相比现有方法在文本分类任务上表现显著优越并达到了最先进性能水平。

    

    基于图神经网络（GNN）的图形学习在自然语言和编程语言处理方面越来越受欢迎，特别是在文本和源代码分类方面。通常，GNN是由交替图层和图池化层构成的，交替图层可以学习图节点特征的转换，而图池化层则使用图池化算子（例如Max池化）有效地减少节点数量，同时保留图的语义信息。最近，为了增强GNN在图形学习任务中的性能，人们广泛采用了Manifold-Mixup这种数据增强技术，该技术通过线性混合一对图数据和它们的标签来生成合成图数据。然而，Manifold-Mixup的性能很大程度上受到图池化算子的影响，而且并没有进行很多关于这种影响的研究。为了填补这一空白，我们早期探索了图池化算子如何影响基于Mixup的图形学习的性能。具体而言，我们提出了一种新颖的混合池化架构，结合了Max-pooling和Attention-pooling，以更好地捕捉本地和全局的图结构信息。我们在文本分类任务上的实验表明，所提出的混合池化结构显著优于现有的池化方法，并达到了最先进的性能水平。

    Graph neural network (GNN)-based graph learning has been popular in natural language and programming language processing, particularly in text and source code classification. Typically, GNNs are constructed by incorporating alternating layers which learn transformations of graph node features, along with graph pooling layers that use graph pooling operators (e.g., Max-pooling) to effectively reduce the number of nodes while preserving the semantic information of the graph. Recently, to enhance GNNs in graph learning tasks, Manifold-Mixup, a data augmentation technique that produces synthetic graph data by linearly mixing a pair of graph data and their labels, has been widely adopted. However, the performance of Manifold-Mixup can be highly affected by graph pooling operators, and there have not been many studies that are dedicated to uncovering such affection. To bridge this gap, we take an early step to explore how graph pooling operators affect the performance of Mixup-based graph le
    
[^195]: 高维回归问题的概率统一分治网络（PPOU-Net）模型

    Probabilistic partition of unity networks for high-dimensional regression problems. (arXiv:2210.02694v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.02694](http://arxiv.org/abs/2210.02694)

    本论文介绍了一种概率统一分治网络（PPOU-Net）模型应用在高维回归问题上的自适应降维通用框架，该模型通过低维流形上的多项式逼近目标函数，并采用期望极大算法进行训练，实验证明在各种数据维度情况下，PPOU-Net表现出更好的性能。

    

    我们探讨了概率统一分治网络(PPOU-Net)模型在高维回归问题中的应用，并提出了一种自适应降维的通用框架。通过该框架，目标函数被低维流形上每个聚类关联的具有本地固定次数的多项式所逼近。我们提出了一种训练策略，利用期望极大(EM)算法。在训练期间，我们交替执行以下两个步骤：(i)使用梯度下降更新 DNN 系数;(ii)使用从EM算法推导出的闭式公式更新每个模型的参数。在概率公式下，步骤(ii)允许严重可并行的加权最小二乘解的形式。在各种数据维度的数值实验中，与可比较大小的基准全连接神经网络相比，PPOU-Net稳定地表现出更好的性能。

    We explore the probabilistic partition of unity network (PPOU-Net) model in the context of high-dimensional regression problems and propose a general framework focusing on adaptive dimensionality reduction. With the proposed framework, the target function is approximated by a mixture of experts model on a low-dimensional manifold, where each cluster is associated with a local fixed-degree polynomial. We present a training strategy that leverages the expectation maximization (EM) algorithm. During the training, we alternate between (i) applying gradient descent to update the DNN coefficients; and (ii) using closed-form formulae derived from the EM algorithm to update the mixture of experts model parameters. Under the probabilistic formulation, step (ii) admits the form of embarrassingly parallelizable weighted least-squares solves. The PPOU-Nets consistently outperform the baseline fully-connected neural networks of comparable sizes in numerical experiments of various data dimensions. W
    
[^196]: 安全贝叶斯优化的元学习先验

    Meta-Learning Priors for Safe Bayesian Optimization. (arXiv:2210.00762v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.00762](http://arxiv.org/abs/2210.00762)

    本文提出了一种数据驱动方法，通过元学习先验从离线数据中实现安全的贝叶斯优化，同时开发一种新的框架以数据驱动的方式选择符合安全要求的先验，结果表明，相比于基准方法，元学习先验加快了安全贝叶斯优化的收敛速度并改进了整体性能。

    

    在机器人学中，优化控制器参数并满足安全约束是一个重要的挑战。安全贝叶斯优化通过量化目标和约束中的不确定性来安全地指导探索。然而，在存在未知安全约束的情况下，选择可靠的模型超参数以避免安全违规至关重要，但人工设计适合的概率模型可能很具有挑战性。本文提出了一种数据驱动的方法，通过元学习先验从离线数据中实现安全的贝叶斯优化。我们借助元学习算法 F-PACOH，在数据稀缺性的情况下提供可靠的不确定性量化。同时，在基准函数和高精度运动系统上，我们通过经验不确定度度量和前沿搜索算法开发了一种新的框架，以数据驱动的方式选择符合安全要求的先验。实验结果表明，相比于基准方法，我们的元学习先验加快了安全贝叶斯优化的收敛速度并改进了整体性能。

    In robotics, optimizing controller parameters under safety constraints is an important challenge. Safe Bayesian optimization (BO) quantifies uncertainty in the objective and constraints to safely guide exploration in such settings. Hand-designing a suitable probabilistic model can be challenging, however. In the presence of unknown safety constraints, it is crucial to choose reliable model hyper-parameters to avoid safety violations. Here, we propose a data-driven approach to this problem by meta-learning priors for safe BO from offline data. We build on a meta-learning algorithm, F-PACOH, capable of providing reliable uncertainty quantification in settings of data scarcity. As core contribution, we develop a novel framework for choosing safety-compliant priors in a data-riven manner via empirical uncertainty metrics and a frontier search algorithm. On benchmark functions and a high-precision motion system, we demonstrate that our meta-learned priors accelerate the convergence of safe 
    
[^197]: Holographic-(V)AE:一种在傅立叶空间中的SO(3)-等变性(Variational)自编码器

    Holographic-(V)AE: an end-to-end SO(3)-Equivariant (Variational) Autoencoder in Fourier Space. (arXiv:2209.15567v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.15567](http://arxiv.org/abs/2209.15567)

    Holographic-(V)AE是一种在傅立叶空间中的SO(3)-等变性(Variational)自编码器，用于无监督学习和生成分布于指定3D原点周围的数据，并且对应的潜空间编码了球面图像的分类特征。

    

    群等变性神经网络已成为数据效率高的解决分类和回归任务的方法，同时也尊重数据的相关对称性。然而，在无监督生成方面，这种范例的研究还很少。本文提出了一种称为全息式-(变分)自编码器（Holographic-(V)AE）的全尺寸SO(3)等变(Variational)自编码器，适用于分布在三维指定原点周围的数据的无监督学习和生成。H-(V)AE被训练为重构数据的球形傅立叶编码，学习过程中获得具有最大信息内容的旋转不变嵌入和用于描述数据方向的等变帧的低维数据表征（即潜空间）。我们对H-(V)AE在多种数据集上的性能进行了广泛测试。结果显示，学习到的潜空间有效地编码了球面图像的分类特征。

    Group-equivariant neural networks have emerged as a data-efficient approach to solve classification and regression tasks, while respecting the relevant symmetries of the data. However, little work has been done to extend this paradigm to the unsupervised and generative domains. Here, we present Holographic-(Variational) Auto Encoder (H-(V)AE), a fully end-to-end SO(3)-equivariant (variational) autoencoder in Fourier space, suitable for unsupervised learning and generation of data distributed around a specified origin in 3D. H-(V)AE is trained to reconstruct the spherical Fourier encoding of data, learning in the process a low-dimensional representation of the data (i.e., a latent space) with a maximally informative rotationally invariant embedding alongside an equivariant frame describing the orientation of the data. We extensively test the performance of H-(V)AE on diverse datasets. We show that the learned latent space efficiently encodes the categorical features of spherical images.
    
[^198]: 面向高维可达性问题的形式化安全保障生成

    Generating Formal Safety Assurances for High-Dimensional Reachability. (arXiv:2209.12336v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2209.12336](http://arxiv.org/abs/2209.12336)

    提出了一个称为Safe DeepReach的框架，用于为DeepReach方法生成正式安全保障。该框架将新颖的Lipschitz连续性分析与区间边界传播方法相结合，以有效和可伸缩的方式保证解决方案的安全性。

    

    为自主系统提供正式的安全和性能保证变得日益重要。哈密顿-雅科比（HJ）可达性分析是一种流行的形式验证工具，用于提供这些保证，因为它可以处理一般非线性系统动态、有界对抗系统干扰以及状态和输入约束。但是，它涉及到求解PDE，其计算和内存复杂度随着状态维度的增加呈指数级增长，使其在大型系统上的直接使用变得不可行。最近提出的DeepReach方法通过利用正弦神经PDE求解器来克服了这一挑战，用于解决高维可达性问题，其计算要求随可达管复杂性而不是状态空间维度而变化。不幸的是，神经网络可能会出现错误，因此计算出的解决方案可能不安全，这没有达到我们提供正式安全保障的总体目标。为了解决这一挑战，我们提出了一个称为Safe DeepReach的框架，用于为DeepReach方法生成正式安全保障。我们的框架将新颖的Lipschitz连续性分析与区间边界传播方法相结合，以有效和可伸缩的方式保证解决方案的安全性。我们在几个基准示例上展示了我们提出的方法的有效性，包括基于感知的高维车道保持系统。

    Providing formal safety and performance guarantees for autonomous systems is becoming increasingly important. Hamilton-Jacobi (HJ) reachability analysis is a popular formal verification tool for providing these guarantees, since it can handle general nonlinear system dynamics, bounded adversarial system disturbances, and state and input constraints. However, it involves solving a PDE, whose computational and memory complexity scales exponentially with respect to the state dimensionality, making its direct use on large-scale systems intractable. A recently proposed method called DeepReach overcomes this challenge by leveraging a sinusoidal neural PDE solver for high-dimensional reachability problems, whose computational requirements scale with the complexity of the underlying reachable tube rather than the state space dimension. Unfortunately, neural networks can make errors and thus the computed solution may not be safe, which falls short of achieving our overarching goal to provide fo
    
[^199]: 保证鲁棒性马尔可夫决策过程的一阶策略优化

    First-order Policy Optimization for Robust Markov Decision Process. (arXiv:2209.10579v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.10579](http://arxiv.org/abs/2209.10579)

    本论文介绍了一种用于解决鲁棒性马尔科夫决策过程的一阶方法，称为鲁棒策略镜像下降算法（RPMD）。通过使用线性递增步进，算法具有较低的复杂度，并且能够在不确定情况下找到最优策略。

    

    本文考虑解决鲁棒性马尔可夫决策过程（MDP）问题，包括一组具有不确定转移核的折扣、有限状态、有限动作空间的MDP。规划的目标是找到一个鲁棒策略来优化对转移不确定性的最坏情况值，因此包括标准MDP规划作为一种特殊情况。对于$(\mathbf{s},\mathbf{a})$-矩形不确定集，本文建立了鲁棒目标的几个结构性观察，从而便于开发基于策略的一阶方法，即鲁棒策略镜像下降算法（RPMD）。使用线性递增的步长，建立了找到$\epsilon$-最优策略的$\mathcal{O}(\log(1/\epsilon))$迭代复杂度。当一阶信息仅通过与名义环境的在线交互获得时，我们进一步开发了鲁棒策略镜像下降方法的随机变体，即SRPMD。我们证明了最优策略的取得方式与基于一阶信息的方法是等价的。

    We consider the problem of solving robust Markov decision process (MDP), which involves a set of discounted, finite state, finite action space MDPs with uncertain transition kernels. The goal of planning is to find a robust policy that optimizes the worst-case values against the transition uncertainties, and thus encompasses the standard MDP planning as a special case. For $(\mathbf{s},\mathbf{a})$-rectangular uncertainty sets, we establish several structural observations on the robust objective, which facilitates the development of a policy-based first-order method, namely the robust policy mirror descent (RPMD). An $\mathcal{O}(\log(1/\epsilon))$ iteration complexity for finding an $\epsilon$-optimal policy is established with linearly increasing stepsizes. We further develop a stochastic variant of the robust policy mirror descent method, named SRPMD, when the first-order information is only available through online interactions with the nominal environment. We show that the optimal
    
[^200]: 离散键值瓶颈

    Discrete Key-Value Bottleneck. (arXiv:2207.11240v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.11240](http://arxiv.org/abs/2207.11240)

    本文提出了一个新的神经网络模型结构，包含离散瓶颈，可以有效处理在多个任务之间进行连续学习的问题。

    

    深度神经网络在i.i.d.数据流和标注数据丰富的分类任务中表现良好，但对于连续学习等非平稳训练数据流会出现挑战。目前已有的一个有效方法是在大量可用数据上对编码器进行预训练，然后进行特定任务的微调。然而，对于新任务，更新这些编码器的权重是具有挑战性的，因为需要微调大量的权重，并且会忘记先前任务的信息。我们提出了一个模型架构来解决这个问题，建立在包含成对分离可学习键值代码的离散瓶颈的基础上。我们的范式是进行编码、通过离散瓶颈进行表示处理、解码。在这里，输入被馈送到预训练编码器，编码器的输出用于选择最近的键，并将相应的值馈送到正在执行的任务。

    Deep neural networks perform well on classification tasks where data streams are i.i.d. and labeled data is abundant. Challenges emerge with non-stationary training data streams such as continual learning. One powerful approach that has addressed this challenge involves pre-training of large encoders on volumes of readily available data, followed by task-specific tuning. Given a new task, however, updating the weights of these encoders is challenging as a large number of weights needs to be fine-tuned, and as a result, they forget information about the previous tasks. In the present work, we propose a model architecture to address this issue, building upon a discrete bottleneck containing pairs of separate and learnable key-value codes. Our paradigm will be to encode; process the representation via a discrete bottleneck; and decode. Here, the input is fed to the pre-trained encoder, the output of the encoder is used to select the nearest keys, and the corresponding values are fed to th
    
[^201]: 模型感知的对比学习: 迈向摆脱困境

    Model-Aware Contrastive Learning: Towards Escaping the Dilemmas. (arXiv:2207.07874v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.07874](http://arxiv.org/abs/2207.07874)

    本文提出了一种模型感知的对比学习（MACL）策略，其温度适应于反映实例判别任务基本置信度的对齐幅度的大小，解决了对比学习中的统一性容忍困境（UTD）和梯度降低问题，并在实验证明其优于所有基线，并实现最先进的结果。

    

    对比学习在多个领域中取得了重大突破。然而，最常见的基于InfoNCE的方法存在一些困境，如统一性容忍困境（UTD）和梯度降低，两者都与$\mathcal{P}_{ij}$项有关。已经确认UTD可能导致意外的性能下降。我们认为温度的固定性是UTD的罪魁祸首。为了解决这一挑战，我们通过提出一种模型感知的对比学习（MACL）策略来丰富CL损失家族，其温度适应于反映实例判别任务基本置信度的对齐幅度的大小，然后使CL损失能够自适应地调整对硬负面的惩罚力度。关于另一个困境-梯度降低问题，我们导出了一个涉及梯度缩放因子的极限，可以从统一的角度解释为什么一些最近的方法实际上是对优化过程的DDoS攻击。大量实验证明，MACL可以优于所有基线，并实现最先进的结果。

    Contrastive learning (CL) continuously achieves significant breakthroughs across multiple domains. However, the most common InfoNCE-based methods suffer from some dilemmas, such as \textit{uniformity-tolerance dilemma} (UTD) and \textit{gradient reduction}, both of which are related to a $\mathcal{P}_{ij}$ term. It has been identified that UTD can lead to unexpected performance degradation. We argue that the fixity of temperature is to blame for UTD. To tackle this challenge, we enrich the CL loss family by presenting a Model-Aware Contrastive Learning (MACL) strategy, whose temperature is adaptive to the magnitude of alignment that reflects the basic confidence of the instance discrimination task, then enables CL loss to adjust the penalty strength for hard negatives adaptively. Regarding another dilemma, the gradient reduction issue, we derive the limits of an involved gradient scaling factor, which allows us to explain from a unified perspective why some recent approaches are effect
    
[^202]: 人工神经网络中的随机梯度下降和方差 - 平坦关系异常

    Stochastic Gradient Descent and Anomaly of Variance-flatness Relation in Artificial Neural Networks. (arXiv:2207.04932v2 [nlin.AO] UPDATED)

    [http://arxiv.org/abs/2207.04932](http://arxiv.org/abs/2207.04932)

    本研究通过动态分解方法分析了 SGD 在固定点附近的属性，恢复了真正的“能量”函数，解决了权重方差和损失函数平坦度反常关系的悖论，为人工智能学科提供更好的算法。

    

    随机梯度下降（SGD）是深度学习神经网络中广泛使用的算法，其成功的理论原则一直受到持续的研究。最近的一项工作报告了神经权重的方差和在 SGD 下驱动的损失函数的平坦度之间的反常（反）关系[Feng＆Tu，PNAS 118，0027（2021）]。为了调查这种似乎违反统计物理学原则的现象，通过动态分解方法分析了 SGD 在固定点附近的属性。我们的方法恢复了真正的“能量”函数，下面是普遍的玻尔兹曼分布。它与一般的成本函数不同，并解决了这个反常所引发的悖论。本研究是经典统计力学和新兴的人工智能学科之间的桥梁，有潜力为后者提供更好的算法。

    Stochastic gradient descent (SGD), a widely used algorithm in deep-learning neural networks has attracted continuing studies for the theoretical principles behind its success. A recent work reports an anomaly (inverse) relation between the variance of neural weights and the landscape flatness of the loss function driven under SGD [Feng & Tu, PNAS 118, 0027 (2021)]. To investigate this seemingly violation of statistical physics principle, the properties of SGD near fixed points are analysed via a dynamic decomposition method. Our approach recovers the true "energy" function under which the universal Boltzmann distribution holds. It differs from the cost function in general and resolves the paradox raised by the the anomaly. The study bridges the gap between the classical statistical mechanics and the emerging discipline of artificial intelligence, with potential for better algorithms to the latter.
    
[^203]: 通过鼓励一致的基于梯度的解释来改进视觉 grounding

    Improving Visual Grounding by Encouraging Consistent Gradient-based Explanations. (arXiv:2206.15462v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.15462](http://arxiv.org/abs/2206.15462)

    该论文提出了一种名为 AMC 的目标函数，鼓励基于梯度的解释覆盖有注释的感兴趣区域，即编码区域。该方法在提高视觉 grounding 性能方面表现卓越，有望成为视觉 grounding 领域的新进展。

    

    我们提出了一种基于边缘的损失，用于预训练视觉语言模型，鼓励基于梯度的解释与区域级注释保持一致。我们将这个目标称为 Attention Mask Consistency（AMC），并证明它产生了比依赖于区域级注释的模型更优越的视觉 grounding 性能。 AMC 通过鼓励基于梯度的解释掩码，在包含此类注释的图像中，把它们的注意力分数主要集中在注释的感兴趣区域内。特别地，一个在标准视觉-语言建模目标之上用 AMC 训练的模型，在 Flickr30k 视觉 grounding 基准测试中获得了86.59%的最新结果，相比最佳结果获得了5.48%的绝对提升。我们的方法在已建立的指代表达理解基准测试中表现优秀，还提供了额外的好处。

    We propose a margin-based loss for vision-language model pretraining that encourages gradient-based explanations that are consistent with region-level annotations. We refer to this objective as Attention Mask Consistency (AMC) and demonstrate that it produces superior visual grounding performance compared to models that rely instead on region-level annotations for explicitly training an object detector such as Faster R-CNN. AMC works by encouraging gradient-based explanation masks that focus their attention scores mostly within annotated regions of interest for images that contain such annotations. Particularly, a model trained with AMC on top of standard vision-language modeling objectives obtains a state-of-the-art accuracy of 86.59% in the Flickr30k visual grounding benchmark, an absolute improvement of 5.48% when compared to the best previous model. Our approach also performs exceedingly well on established benchmarks for referring expression comprehension and offers the added bene
    
[^204]: 用于黑盒优化的生成预训练

    Generative Pretraining for Black-Box Optimization. (arXiv:2206.10786v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.10786](http://arxiv.org/abs/2206.10786)

    该论文提出了一种使用离线数据预训练黑盒优化器的生成框架BONET，使用自回归模型和样本策略合成轨迹以帮助在高维空间中优化昂贵的黑盒函数。

    

    科学和工程中的许多问题涉及在高维空间中优化昂贵的黑盒函数。对于这样的黑盒优化 (BBO) 问题，我们通常假设在线函数评估的预算很小，但往往可以访问用于预训练的固定离线数据集。之前的方法试图利用离线数据来逼近函数或其反函数，但在离数据分布较远时不够精确。我们提出了BONET，这是一个利用离线数据集预训练黑盒优化器的生成框架。在BONET中，我们对来自离线数据集的定长轨迹训练一个自回归模型。我们设计了一种采样策略，使用从低保真度样本到高保真度样本的单调转换的简单启发式来合成来自离线数据的轨迹。在Design-Bench上使用被因果掩蔽的Transformer实例化BONET，并进行评估，我们在平均排名上排名第一。

    Many problems in science and engineering involve optimizing an expensive black-box function over a high-dimensional space. For such black-box optimization (BBO) problems, we typically assume a small budget for online function evaluations, but also often have access to a fixed, offline dataset for pretraining. Prior approaches seek to utilize the offline data to approximate the function or its inverse but are not sufficiently accurate far from the data distribution. We propose BONET, a generative framework for pretraining a novel black-box optimizer using offline datasets. In BONET, we train an autoregressive model on fixed-length trajectories derived from an offline dataset. We design a sampling strategy to synthesize trajectories from offline data using a simple heuristic of rolling out monotonic transitions from low-fidelity to high-fidelity samples. Empirically, we instantiate BONET using a causally masked Transformer and evaluate it on Design-Bench, where we rank the best on averag
    
[^205]: 论小批量重球动量法的快速收敛性

    On the fast convergence of minibatch heavy ball momentum. (arXiv:2206.07553v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.07553](http://arxiv.org/abs/2206.07553)

    本文研究了一种随机Kaczmarz算法，使用小批量和重球动量进行加速，在二次优化问题中保持快速收敛率。

    

    简单的随机动量方法被广泛用于机器学习优化中，但由于还没有加速的理论保证，这与它们在实践中的良好性能并不相符。本文旨在通过展示，随机重球动量在二次最优化问题中保持（确定性）重球动量的快速线性率，至少在使用足够大的批量大小进行小批量处理时。我们所研究的算法可以被解释为带小批量处理和重球动量的加速随机Kaczmarz算法。该分析依赖于仔细分解动量转移矩阵，并使用新的独立随机矩阵乘积的谱范围集中界限。我们提供了数值演示，证明了我们的界限相当尖锐。

    Simple stochastic momentum methods are widely used in machine learning optimization, but their good practical performance is at odds with an absence of theoretical guarantees of acceleration in the literature. In this work, we aim to close the gap between theory and practice by showing that stochastic heavy ball momentum retains the fast linear rate of (deterministic) heavy ball momentum on quadratic optimization problems, at least when minibatching with a sufficiently large batch size. The algorithm we study can be interpreted as an accelerated randomized Kaczmarz algorithm with minibatching and heavy ball momentum. The analysis relies on carefully decomposing the momentum transition matrix, and using new spectral norm concentration bounds for products of independent random matrices. We provide numerical illustrations demonstrating that our bounds are reasonably sharp.
    
[^206]: 可微和可传输的结构学习

    Differentiable and Transportable Structure Learning. (arXiv:2206.06354v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.06354](http://arxiv.org/abs/2206.06354)

    D-Struct是一种可微和可传输的结构学习方法，通过新颖的架构和损失函数使得结构可以在同一领域的不同数据集中传输，比NOTEARS和其他最先进的方法具有更好的性能。

    

    有向无环图在它们的结构中编码了关于特定分布的大量信息。然而，推断这些结构所需的计算通常是变量数的超指数，因为推断需要扫描一个组合数量巨大的潜在结构空间。直到最近的进展才使得使用可微度量搜索这个空间成为可能，从而极大地减少了搜索时间。我们介绍了D-Struct，它通过一种新颖的架构和损失函数恢复了发现结构在同一领域中的传输性，同时仍然完全可微。因为D-Struct仍然是可微的，所以我们的方法可以轻松地应用于现有的可微框架中。

    Directed acyclic graphs (DAGs) encode a lot of information about a particular distribution in their structure. However, compute required to infer these structures is typically super-exponential in the number of variables, as inference requires a sweep of a combinatorially large space of potential structures. That is, until recent advances made it possible to search this space using a differentiable metric, drastically reducing search time. While this technique -- named NOTEARS -- is widely considered a seminal work in DAG-discovery, it concedes an important property in favour of differentiability: transportability. To be transportable, the structures discovered on one dataset must apply to another dataset from the same domain. We introduce D-Struct which recovers transportability in the discovered structures through a novel architecture and loss function while remaining fully differentiable. Because D-Struct remains differentiable, our method can be easily adopted in existing different
    
[^207]: 联邦离线强化学习

    Federated Offline Reinforcement Learning. (arXiv:2206.05581v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.05581](http://arxiv.org/abs/2206.05581)

    本文提出了一种联邦离线强化学习算法，可以处理医疗机构间数据共享的隐私限制和异质性问题，同时提供了通信效率和隐私保护性。该算法的样本复杂度证明以及在现实医学数据集上的模拟实验结果表明了其有效性和效率。

    

    基于证据或数据的动态治疗方案对于个性化医疗至关重要，可以受益于离线强化学习（RL）。虽然医疗机构间有大量健康数据可用，但由于隐私限制，它们无法共享。此外，不同站点存在异质性。因此，联邦离线RL算法是必要的且有前途，以解决这些问题。本文提出了一种多站点马尔可夫决策过程模型，允许站点之间的同质性和异质性效应。所提出的模型可以分析站点级特征。我们设计了第一个具有样本复杂度的离线RL联邦策略优化算法。所提出的算法具有通信效率和隐私保护性，仅需要通过交换摘要统计量进行一轮通信交互。我们为所提出的算法提供了理论保证，无需假设站点之间具有相同的转换动态。我们在现实医学数据集上进行了模拟，展示了所提出算法的有效性和效率。

    Evidence-based or data-driven dynamic treatment regimes are essential for personalized medicine, which can benefit from offline reinforcement learning (RL). Although massive healthcare data are available across medical institutions, they are prohibited from sharing due to privacy constraints. Besides, heterogeneity exists in different sites. As a result, federated offline RL algorithms are necessary and promising to deal with the problems. In this paper, we propose a multi-site Markov decision process model which allows both homogeneous and heterogeneous effects across sites. The proposed model makes the analysis of the site-level features possible. We design the first federated policy optimization algorithm for offline RL with sample complexity. The proposed algorithm is communication-efficient and privacy-preserving, which requires only a single round of communication interaction by exchanging summary statistics. We give a theoretical guarantee for the proposed algorithm without the 
    
[^208]: 超越模仿游戏：量化和拓展语言模型的能力

    Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. (arXiv:2206.04615v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2206.04615](http://arxiv.org/abs/2206.04615)

    本研究引入了Beyond the Imitation Game基准测试（BIG-bench），该测试集包含了204个各领域的难题，旨在评估当前语言模型的能力并为未来的研究提供信息和准备。

    

    随着规模的增大，语言模型展示了数量上的提升和新的定性能力。尽管具有潜在的转变性影响，但这些新的能力目前尚未被充分描述。为了为未来的研究提供信息，为剧变的新型模型能力做准备，并缓解社会有害影响，我们必须了解语言模型的现有和近期能力和限制。为了解决这一挑战，我们引入了Beyond the Imitation Game基准测试（BIG-bench）。BIG-bench目前包括204个任务，由132个机构的450名作者贡献。任务主题多样，涵盖了语言学、儿童发展、数学、常识推理、生物学、物理学、社会偏见、软件开发等等。BIG-bench专注于那些被认为超出了当前语言模型能力的任务。我们评估了OpenAI的GPT模型和谷歌内部的密集转换模型的行为。

    Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transform
    
[^209]: 交互式决策制定的渐近最优算法

    Asymptotic Instance-Optimal Algorithms for Interactive Decision Making. (arXiv:2206.02326v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.02326](http://arxiv.org/abs/2206.02326)

    本论文提出了适用于特定决策问题的渐近最优算法，该算法能够根据该问题的复杂度，在遗憾最小的前提下最大程度地收集观察结果。

    

    过去关于交互式决策制定问题（赌博机问题、强化学习等）的研究主要聚焦于度量算法在最难情况下的性能的最小化遗憾。然而，理想的算法应该能够适应特定问题实例的复杂性，并对易于处理的实例产生比最坏情况下更小的遗憾。本文设计了第一个在温和条件下适用于有限决策个数的一般交互式决策制定问题的渐近最优算法。在每个实例 $f$ 上，我们的算法优于所有一致算法（那些在所有实例上实现非平凡遗憾的算法），并且具有 $\mathcal{C}(f) \ln n$ 的渐近遗憾，其中 $\mathcal{C}(f)$ 是 $f$ 的复杂度的精确表征。算法的关键步骤涉及带有活动数据收集的假设检验。它计算出最经济的决策，通过这些决策算法收集观察结果以检验估计值是否落在预期范围内。

    Past research on interactive decision making problems (bandits, reinforcement learning, etc.) mostly focuses on the minimax regret that measures the algorithm's performance on the hardest instance. However, an ideal algorithm should adapt to the complexity of a particular problem instance and incur smaller regrets on easy instances than worst-case instances. In this paper, we design the first asymptotic instance-optimal algorithm for general interactive decision making problems with finite number of decisions under mild conditions. On every instance $f$, our algorithm outperforms all consistent algorithms (those achieving non-trivial regrets on all instances), and has asymptotic regret $\mathcal{C}(f) \ln n$, where $\mathcal{C}(f)$ is an exact characterization of the complexity of $f$. The key step of the algorithm involves hypothesis testing with active data collection. It computes the most economical decisions with which the algorithm collects observations to test whether an estimate
    
[^210]: 跨视图语言建模：迈向统一的跨语言跨模态预训练

    Cross-View Language Modeling: Towards Unified Cross-Lingual Cross-Modal Pre-training. (arXiv:2206.00621v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2206.00621](http://arxiv.org/abs/2206.00621)

    本文提出了跨视图语言建模框架，该框架将跨语言和跨模态预训练统一在共享的架构和目标下进行，通过有条件的掩码语言建模和对比学习来最大化不同视图之间的互信息以实现两个视图的对齐。

    

    本文引入了跨视图语言建模，这是一个简单而有效的预训练框架，将跨语言和跨模态预训练与共享架构和目标统一起来。我们的方法受到一个关键观察的启发，即跨语言和跨模态预训练具有将同一对象的两个不同视图对齐到一个共同语义空间的相同目标。为此，跨视图语言建模框架将多模态数据（即图像-标题对）和多语言数据（即平行句对）视为同一对象的两个不同视图，并通过有条件的掩码语言建模和对比学习来最大化它们之间的互信息来训练模型，以对齐两个视图。我们使用跨视图语言建模框架对跨语言跨模态语言模型CCLM进行预训练。在多语言多模态基准IGLUE和两个多语言图像-文本检索数据上进行了实证结果。

    In this paper, we introduce Cross-View Language Modeling, a simple and effective pre-training framework that unifies cross-lingual and cross-modal pre-training with shared architectures and objectives. Our approach is motivated by a key observation that cross-lingual and cross-modal pre-training share the same goal of aligning two different views of the same object into a common semantic space. To this end, the cross-view language modeling framework considers both multi-modal data (i.e., image-caption pairs) and multi-lingual data (i.e., parallel sentence pairs) as two different views of the same object, and trains the model to align the two views by maximizing the mutual information between them with conditional masked language modeling and contrastive learning. We pre-train CCLM, a Cross-lingual Cross-modal Language Model, with the cross-view language modeling framework. Empirical results on IGLUE, a multi-lingual multi-modal benchmark, and two multi-lingual image-text retrieval data
    
[^211]: 利用特征空间中的领域约束以增强Android恶意软件检测的鲁棒性：通过RealAEs升级

    Level Up with RealAEs: Leveraging Domain Constraints in Feature Space to Strengthen Robustness of Android Malware Detection. (arXiv:2205.15128v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.15128](http://arxiv.org/abs/2205.15128)

    本文提出了一种在特征空间中生成RealAEs的解决方案，该方法通过解释Android领域约束为在特征空间中的边界来实现。实验结果表明，这种方法提高了检测模型的鲁棒性。

    

    针对基于机器学习的Android恶意软件检测面临的对抗示例容易受攻击的问题，本文提出了一种新的解决方案——在特征空间中生成领域约束下的可行对抗样本(RealAEs)。 在现实攻击下，RealAEs比不可行的对抗样本更有效。此外，本文还提出了一种理解Android领域约束在特征空间中的方法。该方法首先学习特征，并将领域约束解释为在特征空间中的边界。 实验结果表明，该方法在不降低检测性能的情况下，提高了模型鲁棒性。

    The vulnerability to adversarial examples remains one major obstacle for Machine Learning (ML)-based Android malware detection. Realistic attacks in the Android malware domain create Realizable Adversarial Examples (RealAEs), i.e., AEs that satisfy the domain constraints of Android malware. Recent studies have shown that using such RealAEs in Adversarial Training (AT) is more effective in defending against realistic attacks than using unrealizable AEs (unRealAEs). This is because RealAEs allow defenders to explore certain pockets in the feature space that are vulnerable to realistic attacks. However, existing defenses commonly generate RealAEs in the problem space, which is known to be time-consuming and impractical for AT. In this paper, we propose to generate RealAEs in the feature space, leading to a simpler and more efficient solution. Our approach is driven by a novel interpretation of Android domain constraints in the feature space. More concretely, our defense first learns featu
    
[^212]: 点积核回归的精确学习曲线和高阶标度极限

    Precise Learning Curves and Higher-Order Scaling Limits for Dot Product Kernel Regression. (arXiv:2205.14846v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.14846](http://arxiv.org/abs/2205.14846)

    本文细致研究了点积核岭回归问题，针对 $m\propto d^r$ 高阶标度关系提出了精确的测试误差、偏差和方差公式。

    

    随着现代机器学习模型不断推进计算前沿，开发对不同模型和数据缩放方案下预期性能提高的精确估计变得越来越重要。目前，关于描述预测误差如何随着样本数量而变化的学习曲线的理论理解受限于大样本渐近性 ($m\to\infty$) 或对于某些简单数据分布的高维渐近性，其中样本数量与维数成线性比例 ($m\propto d$)。这两个范畴之间存在很大差距，包括所有高阶标度关系 $m\propto d^r$，这是本文的研究对象。我们专注于点积核岭回归的问题，并提供了在 $m/d\rightarrow2r$ 的 $r$ 阶渐近标度下（其中 $m\to\infty$），对于从球面上均匀抽取的数据，测试误差、偏差和方差的精确公式。

    As modern machine learning models continue to advance the computational frontier, it has become increasingly important to develop precise estimates for expected performance improvements under different model and data scaling regimes. Currently, theoretical understanding of the learning curves that characterize how the prediction error depends on the number of samples is restricted to either large-sample asymptotics ($m\to\infty$) or, for certain simple data distributions, to the high-dimensional asymptotics in which the number of samples scales linearly with the dimension ($m\propto d$). There is a wide gulf between these two regimes, including all higher-order scaling relations $m\propto d^r$, which are the subject of the present paper. We focus on the problem of kernel ridge regression for dot-product kernels and present precise formulas for the test error, bias, and variance, for data drawn uniformly from the sphere in the $r$th-order asymptotic scaling regime $m\to\infty$ with $m/d
    
[^213]: QUIC-FL：面向联邦学习的快速无偏压缩

    QUIC-FL: Quick Unbiased Compression for Federated Learning. (arXiv:2205.13341v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.13341](http://arxiv.org/abs/2205.13341)

    本文提出QUIC-FL方法，即面向联邦学习的快速无偏压缩，通过改进DME技术实现了最优归一化均方误差保证。

    

    分布式均值估计（DME）是通信高效的联邦学习中的基本构建块，在其中$n$个客户端向参数服务器通信向量，参数服务器估算其平均值。本文改进了先前的DME技术，实现了最优$O(1/n)$的归一化均方误差（NMSE）保证，通过渐进改进编码或解码（或两者）的复杂度。为了实现这一点，我们以一种新颖的方式形式化了问题，使我们能够使用现成的数学求解器来设计量化。

    Distributed Mean Estimation (DME), in which $n$ clients communicate vectors to a parameter server that estimates their average, is a fundamental building block in communication-efficient federated learning. In this paper, we improve on previous DME techniques that achieve the optimal $O(1/n)$ Normalized Mean Squared Error (NMSE) guarantee by asymptotically improving the complexity for either encoding or decoding (or both). To achieve this, we formalize the problem in a novel way that allows us to use off-the-shelf mathematical solvers to design the quantization.
    
[^214]: 通过学习优化实现可解释人工智能

    Explainable AI via Learning to Optimize. (arXiv:2204.14174v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2204.14174](http://arxiv.org/abs/2204.14174)

    本文提供了一种在数据驱动模型下用于可解释人工智能的具体工具-L2O方法，通过解决优化问题来实现每个推断，并提出了使用可解释的证书来验证模型推断是否可信。

    

    在机器学习中，不易理解的黑盒模型很常见，但越来越多的应用需要可解释的人工智能（XAI）。XAI的核心在于建立透明且可解释的数据驱动算法。本文提供了一种用于XAI的具体工具，可在需要编码先验知识且需要标记不可信推断的情况下使用。我们使用了“学习优化”（L2O）的方法，其中每个推断都解决了一个数据驱动的优化问题。我们的L2O模型易于实现，直接编码先验知识，并产生理论保证（例如满足约束条件）。我们还提出使用可解释的证书，以验证模型推断是否可信。文章还提供了基于字典的信号恢复、CT成像和加密资产套利交易等应用的数值例子。代码和更多文档可在https://xai-l2o.research.typal.academy找到。

    Indecipherable black boxes are common in machine learning (ML), but applications increasingly require explainable artificial intelligence (XAI). The core of XAI is to establish transparent and interpretable data-driven algorithms. This work provides concrete tools for XAI in situations where prior knowledge must be encoded and untrustworthy inferences flagged. We use the "learn to optimize" (L2O) methodology wherein each inference solves a data-driven optimization problem. Our L2O models are straightforward to implement, directly encode prior knowledge, and yield theoretical guarantees (e.g. satisfaction of constraints). We also propose use of interpretable certificates to verify whether model inferences are trustworthy. Numerical examples are provided in the applications of dictionary-based signal recovery, CT imaging, and arbitrage trading of cryptoassets. Code and additional documentation can be found at https://xai-l2o.research.typal.academy.
    
[^215]: 无线网络中的语义信息恢复

    Semantic Information Recovery in Wireless Networks. (arXiv:2204.13366v4 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2204.13366](http://arxiv.org/abs/2204.13366)

    本文提出了一个基于机器学习的语义通信系统SINFONY，它通过对消息进行数据减少和可靠传输来最好地保留语义，从而实现无线网络中的语义信息恢复。

    

    受机器学习工具在无线通信中的成功启发，1949年Weaver提出的语义通信思想引起了人们的关注。 它打破了Shannon的经典设计范例，旨在传递消息的含义，即语义，而不是其确切版本，从而允许节省信息速率。 在这项工作中，我们将Basu等人的建模语义的基本方法扩展到完整通信马尔可夫链。 因此，我们通过隐含的随机变量来建模语义，并将语义通信任务定义为通过通信信道对消息进行数据减少和可靠传输，从而最好地保留语义。 我们将此任务作为端到端信息瓶颈问题进行建模，允许在保留相关信息的同时进行压缩。 作为解决方案，我们提出了基于ML的语义通信系统SINFONY，并将其用于分布式多点场景：SIN。

    Motivated by the recent success of Machine Learning (ML) tools in wireless communications, the idea of semantic communication by Weaver from 1949 has gained attention. It breaks with Shannon's classic design paradigm by aiming to transmit the meaning of a message, i.e., semantics, rather than its exact version and thus allows for savings in information rate. In this work, we extend the fundamental approach from Basu et al. for modeling semantics to the complete communications Markov chain. Thus, we model semantics by means of hidden random variables and define the semantic communication task as the data-reduced and reliable transmission of messages over a communication channel such that semantics is best preserved. We cast this task as an end-to-end Information Bottleneck problem, allowing for compression while preserving relevant information most. As a solution approach, we propose the ML-based semantic communication system SINFONY and use it for a distributed multipoint scenario: SIN
    
[^216]: 基于星形细胞对关键期的神经可塑性神经网络，通过现有和记忆性的大脑可塑性和突触形成实现突触竞争和强度平衡。（arXiv: 2203.11740v12 [cs.NE] UPDATED）

    Plasticity Neural Network Based on Astrocytic effects at Critical Period, Synaptic Competition and Strength Rebalance by Current and Mnemonic Brain Plasticity and Synapse Formation. (arXiv:2203.11740v12 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2203.11740](http://arxiv.org/abs/2203.11740)

    该论文提出基于星形细胞作用的神经网络，通过突触的竞争和强度平衡实现现有和记忆性的大脑可塑性和突触形成，并探讨了与关键期相关的神经紊乱和负面和正面记忆的持久性对突触激活的影响。

    

    除了突触共享连接权重之外，PNN还包括突触有效范围的权重[14-25]。PNN考虑突触强度平衡在突触吞噬的动态和长度常数之和的静态中[14]，并包含了鱼群行为的先导行为。突触形成在实验和模拟中会抑制树突生成[15]。类似于Spring Boot中的强制韧性，反向回路的记忆持久度梯度也存在。相对较好和较差的梯度信息存储在类似于脑褶的记忆痕迹细胞中，在反向回路的突触形成中。争议认为人类海马神经元的再生能力是否持续到老年，并可能在后期迭代中形成新的更长的回路[17,18]。关闭关键期会导致神经紊乱在实验和模拟中[19]。考虑到负面和正面记忆的持久性，有助于更好地激活突触。

    In addition to the weights of synaptic shared connections, PNN includes weights of synaptic effective ranges [14-25]. PNN considers synaptic strength balance in dynamic of phagocytosing of synapses and static of constant sum of synapses length [14], and includes the lead behavior of the school of fish. Synapse formation will inhibit dendrites generation in experiments and simulations [15]. The memory persistence gradient of retrograde circuit similar to the Enforcing Resilience in a Spring Boot. The relatively good and inferior gradient information stored in memory engram cells in synapse formation of retrograde circuit like the folds in brain [16]. The controversy was claimed if human hippocampal neurogenesis persists throughout aging, may have a new and longer circuit in late iteration [17,18]. Closing the critical period will cause neurological disorder in experiments and simulations [19]. Considering both negative and positive memories persistence help activate synapse better than 
    
[^217]: 具有对抗回应的通用回归算法

    Universal Regression with Adversarial Responses. (arXiv:2203.05067v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.05067](http://arxiv.org/abs/2203.05067)

    本文提出了一种通用回归算法，可针对大类非独立同分布实例序列的对抗性回应，在通用可分离指标空间上实现强一致性的通用一致性学习。

    

    本文提出了在通用可分离指标空间上，对于大类非独立同分布实例序列的对抗回归算法，并给出了关于可学习性的特征说明。我们考虑强一致性的通用一致性学习，无需对值回应进行限制。我们的分析表明：这种目标可在比平稳过程更大的实例序列类中实现，并揭示了值空间之间的根本二分法：是否可以实现有限时间段均值估计。我们进一步提供了乐观的通用性学习规则，即如果它们未能实现通用一致性，则其他任何算法也将失败。对于未界限损失，我们提出了一种温和的可积条件，其下有对抗性回归的算法结论。此外，我们还展示了如何将相同的工具应用于带有对抗性误差的通用预测问题。

    We provide algorithms for regression with adversarial responses under large classes of non-i.i.d. instance sequences, on general separable metric spaces, with provably minimal assumptions. We also give characterizations of learnability in this regression context. We consider universal consistency which asks for strong consistency of a learner without restrictions on the value responses. Our analysis shows that such an objective is achievable for a significantly larger class of instance sequences than stationary processes, and unveils a fundamental dichotomy between value spaces: whether finite-horizon mean estimation is achievable or not. We further provide optimistically universal learning rules, i.e., such that if they fail to achieve universal consistency, any other algorithms will fail as well. For unbounded losses, we propose a mild integrability condition under which there exist algorithms for adversarial regression under large classes of non-i.i.d. instance sequences. In additio
    
[^218]: 基于自回归模型的漂移检测方法

    Autoregressive based Drift Detection Method. (arXiv:2203.04769v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2203.04769](http://arxiv.org/abs/2203.04769)

    本研究提出一种基于自回归模型的漂移检测方法，适用于各种机器学习算法，能够在合成数据和现实数据方面表现优于现有方法。

    

    在传统的机器学习框架中，模型是在历史数据上进行训练，然后用于预测未来值。假设数据分布在时间上不发生变化（平稳性）。然而，在现实世界的场景中，数据生成过程随时间而变化，模型必须适应新的输入数据。这种现象称为概念漂移，导致预测模型的性能下降。在本研究中，我们提出了一种基于自回归模型的新概念漂移检测方法，称为ADDM。该方法可以与任何机器学习算法集成，从深度神经网络到简单线性回归模型。我们的结果表明，这种新的概念漂移检测方法在合成数据集和现实世界数据集上优于现有的概念漂移检测方法。我们的方法在理论上保证，并在检测各种概念漂移方面具有经验和有效性。

    In the classic machine learning framework, models are trained on historical data and used to predict future values. It is assumed that the data distribution does not change over time (stationarity). However, in real-world scenarios, the data generation process changes over time and the model has to adapt to the new incoming data. This phenomenon is known as concept drift and leads to a decrease in the predictive model's performance. In this study, we propose a new concept drift detection method based on autoregressive models called ADDM. This method can be integrated into any machine learning algorithm from deep neural networks to simple linear regression model. Our results show that this new concept drift detection method outperforms the state-of-the-art drift detection methods, both on synthetic data sets and real-world data sets. Our approach is theoretically guaranteed as well as empirical and effective for the detection of various concept drifts. In addition to the drift detector,
    
[^219]: DDL-MVS: MVS网络深度不连续性学习

    DDL-MVS: Depth Discontinuity Learning for MVS Networks. (arXiv:2203.01391v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2203.01391](http://arxiv.org/abs/2203.01391)

    该论文提出了一种深度不连续性学习的MVS方法，可以在保留重建完整性的同时提高精度，方法是联合估计深度和边界地图，其中边界地图用于进一步细化深度地图。

    

    传统的MVS方法精度较高但完整性不足，而最近开发的基于学习的多视图立体技术具有完整性改善，但牺牲了精度。我们提出了深度不连续性学习的MVS方法，通过共同估计深度和边界地图，进一步提高精度的同时保留重建的完整性。我们的想法是联合估计深度和边界地图，其中边界地图明确用于进一步细化深度地图。我们验证了我们的想法，并证明了我们的策略可以轻松地集成到现有的基于学习的MVS管道中，其中重建取决于高质量的深度地图估计。在各种数据集上的大量实验表明，我们的方法与基准相比改善了重建质量。实验还表明，我们提出的模型和策略具有良好的泛化能力。源代码即将公开发布。

    Traditional MVS methods have good accuracy but struggle with completeness, while recently developed learning-based multi-view stereo (MVS) techniques have improved completeness except accuracy being compromised. We propose depth discontinuity learning for MVS methods, which further improves accuracy while retaining the completeness of the reconstruction. Our idea is to jointly estimate the depth and boundary maps where the boundary maps are explicitly used for further refinement of the depth maps. We validate our idea and demonstrate that our strategies can be easily integrated into the existing learning-based MVS pipeline where the reconstruction depends on high-quality depth map estimation. Extensive experiments on various datasets show that our method improves reconstruction quality compared to baseline. Experiments also demonstrate that the presented model and strategies have good generalization capabilities. The source code will be available soon.
    
[^220]: 论缺失数据模型中的可检验性和拟合优度检验

    On Testability and Goodness of Fit Tests in Missing Data Models. (arXiv:2203.00132v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2203.00132](http://arxiv.org/abs/2203.00132)

    本文提供了关于缺失数据图模型的可检验性和设计拟合优度测试的新见解。

    

    在描述有向无环图可以描述建模假设的缺失数据问题中，已经取得了重要进展。使用这些技术得到的结果的有效性取决于图所编码的假设是否成立，然而，在先前的工作中，对这些假设的验证没有得到足够的关注。本文提供了关于三类缺失数据图模型的可检验性和设计拟合优度测试的新见解。探讨的模型类别包括：可以用于建模具有退出/截尾的纵向研究的序贯缺失随机模型和缺失非随机模型，以及可以应用于横截面研究和调查的非自我截断模型。

    Significant progress has been made in developing identification and estimation techniques for missing data problems where modeling assumptions can be described via a directed acyclic graph. The validity of results using such techniques rely on the assumptions encoded by the graph holding true; however, verification of these assumptions has not received sufficient attention in prior work. In this paper, we provide new insights on the testable implications of three broad classes of missing data graphical models, and design goodness-of-fit tests for them. The classes of models explored are: sequential missing-at-random and missing-not-at-random models which can be used for modeling longitudinal studies with dropout/censoring, and a no self-censoring model which can be applied to cross-sectional studies and surveys.
    
[^221]: 有限和耦合组合随机优化：理论与应用

    Finite-Sum Coupled Compositional Stochastic Optimization: Theory and Applications. (arXiv:2202.12396v7 [math.OC] UPDATED)

    [http://arxiv.org/abs/2202.12396](http://arxiv.org/abs/2202.12396)

    本文提出了一种适用于凸和非凸组合函数求和的随机算法，并在理论上证明了该算法在Oracle复杂度和并行加速方面的优秀性能。

    

    本文研究了组合函数求和的随机优化问题，其中每个求和项的内层函数与相应的求和索引配对。我们将这类问题称为有限和耦合组合优化（FCCO）。这在机器学习中有广泛的应用，用于优化非凸或凸组合测量/目标，例如平均精度（AP），p-范数推进，列表式排名损失，邻域组件分析（NCA），深度生存分析和深度潜在变量模型等。然而，现有的算法和分析在某些方面受到限制。本文的贡献是为非凸和凸目标提供了一个简单随机算法的全面收敛性分析。我们的关键结果是使用基于移动平均的估计器与小批量并行加速改进了Oracle复杂度。我们的理论分析还展示了....

    This paper studies stochastic optimization for a sum of compositional functions, where the inner-level function of each summand is coupled with the corresponding summation index. We refer to this family of problems as finite-sum coupled compositional optimization (FCCO). It has broad applications in machine learning for optimizing non-convex or convex compositional measures/objectives such as average precision (AP), p-norm push, listwise ranking losses, neighborhood component analysis (NCA), deep survival analysis, deep latent variable models, etc., which deserves finer analysis. Yet, existing algorithms and analyses are restricted in one or other aspects. The contribution of this paper is to provide a comprehensive convergence analysis of a simple stochastic algorithm for both non-convex and convex objectives. Our key result is the improved oracle complexity with the parallel speed-up by using the moving-average based estimator with mini-batching. Our theoretical analysis also exhibit
    
[^222]: 平滑风险度量的策略梯度优化方法

    A policy gradient approach for optimization of smooth risk measures. (arXiv:2202.11046v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.11046](http://arxiv.org/abs/2202.11046)

    本文介绍了一种应用于on-policy和off-policy RL情况下的策略梯度算法，用于最小化广义平滑风险度量，能够收敛到平滑风险度量的稳态点，并适用于均值-方差和畸变风险度量的优化。

    

    我们提出了一种策略梯度算法，用于解决风险敏感的强化学习问题，包括on-policy和off-policy情况。我们考虑时间段马尔可夫决策过程，并利用累积折扣奖励的广义平滑风险度量来建模风险。我们提出了两个模板策略梯度算法，分别在on-policy和off-policy RL情况下优化平滑风险度量。我们推导出非渐进性界，量化了我们提出的算法收敛到平滑风险度量的稳态点的速率。作为特例，我们确定了我们的算法分别应用于均值-方差和畸变风险度量的优化。

    We propose policy gradient algorithms for solving a risk-sensitive reinforcement learning problem in on-policy as well as off-policy settings. We consider episodic Markov decision processes, and model the risk using the broad class of smooth risk measures of the cumulative discounted reward. We propose two template policy gradient algorithms that optimize a smooth risk measure in on-policy and off-policy RL settings, respectively. We derive non-asymptotic bounds that quantify the rate of convergence to our proposed algorithms to a stationary point of the smooth risk measure. As special cases, we establish that our algorithms apply to the optimization of mean-variance and distortion risk measures, respectively.
    
[^223]: 加速正-对偶方法求解正则化马尔可夫决策过程

    Accelerating Primal-dual Methods for Regularized Markov Decision Processes. (arXiv:2202.10506v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2202.10506](http://arxiv.org/abs/2202.10506)

    本文介绍了一种新的正-对偶表述方法，结合新的插值度量，可以显著加速收敛。数值结果表明方法在多种设置下性能优越。

    

    熵正则化马尔可夫决策过程在强化学习中已被广泛应用。本文关注熵正则化问题的正-对偶表述。由于缺乏严格的凸性和凹性，标准的一阶方法收敛缓慢。为解决这个问题，我们首先引入了一个新的二次凸化的正-对偶表述。新表述的自然梯度上升下降具有全局收敛保证和指数收敛速度。我们还提出了一种新的插值度量，可以显著加速收敛。数值结果表明，所提出的方法在多种设置下的性能都很好。

    Entropy regularized Markov decision processes have been widely used in reinforcement learning. This paper is concerned with the primal-dual formulation of the entropy regularized problems. Standard first-order methods suffer from slow convergence due to the lack of strict convexity and concavity. To address this issue, we first introduce a new quadratically convexified primal-dual formulation. The natural gradient ascent descent of the new formulation enjoys global convergence guarantee and exponential convergence rate. We also propose a new interpolating metric that further accelerates the convergence significantly. Numerical results are provided to demonstrate the performance of the proposed methods under multiple settings.
    
[^224]: GoSafeOpt: 可扩展的安全全局优化动态系统的探索

    GoSafeOpt: Scalable Safe Exploration for Global Optimization of Dynamical Systems. (arXiv:2201.09562v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.09562](http://arxiv.org/abs/2201.09562)

    GoSafeOpt是第一个能够安全探索高维系统并提供全局最优保证的算法。

    

    直接在物理系统上学习最优控制策略具有挑战性，因为即使一次失败也可能导致昂贵的硬件损坏。大多数现有的保证安全（即在探索过程中没有失败）的无模型学习方法仅局限于局部最优解。GoSafe算法是一个值得注意的例外，但不幸的是，它无法处理高维系统，因此无法应用于大多数实际的动态系统。这项工作提出了GoSafeOpt作为第一个能够安全地发现高维系统的全局最优策略并具有安全和最优性保证的算法。我们在机械臂上展示了GoSafeOpt比其他模型无关的安全学习方法更优秀，而GoSafe在该机械臂上将是禁止使用的。

    Learning optimal control policies directly on physical systems is challenging since even a single failure can lead to costly hardware damage. Most existing model-free learning methods that guarantee safety, i.e., no failures, during exploration are limited to local optima. A notable exception is the GoSafe algorithm, which, unfortunately, cannot handle high-dimensional systems and hence cannot be applied to most real-world dynamical systems. This work proposes GoSafeOpt as the first algorithm that can safely discover globally optimal policies for high-dimensional systems while giving safety and optimality guarantees. We demonstrate the superiority of GoSafeOpt over competing model-free safe learning methods on a robot arm that would be prohibitive for GoSafe.
    
[^225]: 模型为基础的安全强化学习在时间变化状态和控制约束下的应用：智能车辆中的应用

    Model-Based Safe Reinforcement Learning with Time-Varying State and Control Constraints: An Application to Intelligent Vehicles. (arXiv:2112.11217v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.11217](http://arxiv.org/abs/2112.11217)

    本文提出了一种安全RL算法，结合了基于屏障力的控制策略结构与多步策略评估机制，在保证控制安全的同时，能够应对时间变化的安全约束，并证明了其稳定性、鲁棒性和收敛性，优于几种最先进的RL算法。

    

    近年来，基于演员-评论家结构的安全强化学习（RL）在连续控制任务中受到越来越多的关注。学习一个具有安全性和收敛性保证的近似最优控制策略仍然具有挑战性。同时，很少有作品讨论了在时间变化的安全性约束下设计安全RL算法。本文提出了一种安全RL算法，用于具有时间变化状态和控制约束的非线性系统的最优控制。在所提出的方法中，我们构建了一种新颖的基于屏障力的控制策略结构，以保证控制安全。提出了一种多步策略评估机制，用于预测策略在时间变化的安全约束下的安全风险，并指导策略安全更新。已证明稳定性和鲁棒性的理论结果。同时，分析了演员评论家实现的收敛性。所提出的算法的性能在模拟的Sa中优于几种最先进的RL算法

    Recently, safe reinforcement learning (RL) with the actor-critic structure for continuous control tasks has received increasing attention. It is still challenging to learn a near-optimal control policy with safety and convergence guarantees. Also, few works have addressed the safe RL algorithm design under time-varying safety constraints. This paper proposes a safe RL algorithm for optimal control of nonlinear systems with time-varying state and control constraints. In the proposed approach, we construct a novel barrier force-based control policy structure to guarantee control safety. A multi-step policy evaluation mechanism is proposed to predict the policy's safety risk under time-varying safety constraints and guide the policy to update safely. Theoretical results on stability and robustness are proven. Also, the convergence of the actor-critic implementation is analyzed. The performance of the proposed algorithm outperforms several state-of-the-art RL algorithms in the simulated Sa
    
[^226]: 公平主动学习：解决保险行业中的标注问题

    Fair Active Learning: Solving the Labeling Problem in Insurance. (arXiv:2112.09466v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2112.09466](http://arxiv.org/abs/2112.09466)

    本文旨在解决保险行业中普遍存在的机器学习模型在数据中发现的偏见和歧视，提出了公平主动学习方法，能够在实现模型预测性能的同时保证数据公平性。

    

    本文针对在保险行业广泛使用机器学习模型所面临的重大障碍，特别关注促进公平性。最初的挑战在于有效利用未标记的保险数据，通过主动学习技术降低标注的工作量，并强调数据相关性。本文探讨了各种主动学习抽样方法，并评估它们对合成和实际保险数据集的影响。该分析强调了实现公正模型推断的困难，因为机器学习模型可能会复制底层数据中存在的偏见和歧视。为了解决这些相互关联的挑战，本文介绍了一种创新的公平主动学习方法。所提出的方法采样信息量充足且公平的实例，在模型预测性能和公平性之间取得了良好的平衡，这一点在保险数据集上的数值实验中得到了证实。

    This paper addresses significant obstacles that arise from the widespread use of machine learning models in the insurance industry, with a specific focus on promoting fairness. The initial challenge lies in effectively leveraging unlabeled data in insurance while reducing the labeling effort and emphasizing data relevance through active learning techniques. The paper explores various active learning sampling methodologies and evaluates their impact on both synthetic and real insurance datasets. This analysis highlights the difficulty of achieving fair model inferences, as machine learning models may replicate biases and discrimination found in the underlying data. To tackle these interconnected challenges, the paper introduces an innovative fair active learning method. The proposed approach samples informative and fair instances, achieving a good balance between model predictive performance and fairness, as confirmed by numerical experiments on insurance datasets.
    
[^227]: 通过策略梯度算法的高效多目标神经架构搜索框架

    Efficient Multi-objective Neural Architecture Search Framework via Policy Gradient Algorithm. (arXiv:2111.03892v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.03892](http://arxiv.org/abs/2111.03892)

    本论文提出了TND-NAS框架，利用可微架构搜索框架和多目标NAS的兼容性，通过策略梯度算法实现高效多目标神经架构搜索。实验结果表明其优于现有方法。

    

    可微架构搜索已成为神经架构搜索领域的主流研究课题，相对于早期的EA-based和RL-based方法，其高效率备受青睐。然而，这些方法已不再能够自然地应对不可微参数，如能源和资源受限效率等。针对多目标NAS领域的研究旨在解决这个问题，但由于对每个候选架构进行唯一的优化，因此需要大量的计算资源。基于此，我们提出了TND-NAS，它具有不可微参数的兼容性和不同iable NAS框架中的高效性。实验结果表明，在几个基准数据集上，我们提出的TND-NAS在搜索效率和解决方案质量方面优于现有方法。

    Differentiable architecture search has gradually become the mainstream research topic in the field of Neural Architecture Search (NAS) for its high efficiency compared with the early NAS (EA-based, RL-based) methods. Recent differentiable NAS also aims at further improving the search performance and reducing the GPU-memory consumption. However, these methods are no longer naturally capable of tackling the non-differentiable objectives, e.g., energy, resource-constrained efficiency, and other metrics, let alone the multi-objective search demands. Researches in the multi-objective NAS field target this but requires vast computational resources cause of the sole optimization of each candidate architecture. In light of this discrepancy, we propose the TND-NAS, which is with the merits of the high efficiency in differentiable NAS framework and the compatibility among non-differentiable metrics in Multi-objective NAS. Under the differentiable NAS framework, with the continuous relaxation of 
    
[^228]: 从数据中提取动力学模型

    Extracting Dynamical Models from Data. (arXiv:2110.06917v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.06917](http://arxiv.org/abs/2110.06917)

    本文介绍了一种使用机器学习从数据中获得确切动力学信息的方法，并命名为“FJet”。这种方法在谐振子、摆和达芬奇振荡器等示例中被证明可以准确地复制动力学并恢复潜在的微分方程。

    

    通过使用机器学习来对相空间变量的更新建模，本文介绍了确定系统基础动力学的方法。可以准确地复制谐振子、摆和达芬奇振荡器的动力学，并在每个示例中准确地恢复了潜在的微分方程。此方法命名为“FJet”，类似于得到的模型于Runge-Kutta数值积分方案的Taylor级数展开。这种类比具有显式揭示适当函数形式的优势。

    The problem of determining the underlying dynamics of a system when only given data of its state over time has challenged scientists for decades. In this paper, the approach of using machine learning to model the {\em updates} of the phase space variables is introduced; this is done as a function of the phase space variables. (More generally, the modeling is done over the jet space of the variables.) This approach is shown to accurately replicate the dynamics for the examples of the harmonic oscillator, the pendulum, and the Duffing oscillator; the underlying differential equation is also accurately recovered in each example. In addition, the results in no way depend on how the data is sampled over time (i.e., regularly or irregularly). It is demonstrated that this approach (named "FJet") is similar to the model resulting from a Taylor series expansion of the Runge-Kutta (RK) numerical integration scheme. This analogy confers the advantage of explicitly revealing the appropriate functi
    
[^229]: EvadeDroid：一种对黑盒Android恶意软件检测的实用逃逸攻击

    EvadeDroid: A Practical Evasion Attack on Machine Learning for Black-box Android Malware Detection. (arXiv:2110.03301v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.03301](http://arxiv.org/abs/2110.03301)

    本论文提出了一种可实际使用并可以有效逃避黑盒Android恶意软件检测器的对抗攻击——EvadeDroid，并且可以保留原始恶意软件应用程序的功能。

    

    在过去的十年里，研究人员通过开发逃避攻击来广泛探索Android恶意软件检测器对对抗性样本的漏洞；然而，在现实世界中，这些攻击的实用性仍然有争议。大多数研究假定攻击者知道用于恶意软件检测的目标分类器的详细信息，而在现实中，恶意行为者只能获得有限的目标分类器访问权限。本文介绍了EvadeDroid，一种实用的基于决策的对抗攻击，旨在有效逃避黑盒Android恶意软件检测器的检测。除了生成实际的对抗性恶意软件外，所提出的逃避攻击还可以保留原始恶意软件应用程序的功能。通过利用基于n-gram的方法构建来自良性样本的功能保持转换，这些转换与恶意软件应用程序具有操作码级别的相似性。

    Over the last decade, researchers have extensively explored the vulnerabilities of Android malware detectors to adversarial examples through the development of evasion attacks; however, the practicality of these attacks in real-world scenarios remains arguable. The majority of studies have assumed attackers know the details of the target classifiers used for malware detection, while in reality, malicious actors have limited access to the target classifiers. This paper introduces EvadeDroid, a practical decision-based adversarial attack designed to effectively evade black-box Android malware detectors in real-world scenarios. In addition to generating real-world adversarial malware, the proposed evasion attack can also preserve the functionality of the original malware applications (apps). EvadeDroid constructs a collection of functionality-preserving transformations derived from benign donors that share opcode-level similarity with malware apps by leveraging an n-gram-based approach. T
    
[^230]: SubseasonalClimateUSA: 用于亚季节预测和基准测试的数据集。

    SubseasonalClimateUSA: A Dataset for Subseasonal Forecasting and Benchmarking. (arXiv:2109.10399v3 [physics.ao-ph] UPDATED)

    [http://arxiv.org/abs/2109.10399](http://arxiv.org/abs/2109.10399)

    这个论文介绍了SubseasonalClimateUSA，这是一个用于训练和基准测试美国的亚季节预测模型的数据集。作者使用该数据集对多种模型进行了基准测试。

    

    天气的亚季节预测对资源配置和气候适应至关重要，但对预测社区提出了许多挑战。在这个预测时间范围内，基于物理的动力学模型的技能有限，并且预测目标以一种复杂的方式依赖于本地天气和全球气候变量。最近，机器学习方法显示出推进技术的潜力，但需要复杂的数据整理，将专家知识与多个相关数据来源、文件格式和时间空间分辨率的聚合进行整合。为了简化这个过程并加速未来的发展，我们介绍了SubseasonalClimateUSA，这是一个经过策划的数据集，用于训练和基准测试美国的亚季节预测模型。我们使用这个数据集来对各种不同的亚季节模型进行基准测试，包括操作性动力学模型、古典的气象基线以及十个统计模型。

    Subseasonal forecasting of the weather two to six weeks in advance is critical for resource allocation and climate adaptation but poses many challenges for the forecasting community. At this forecast horizon, physics-based dynamical models have limited skill, and the targets for prediction depend in a complex manner on both local weather and global climate variables. Recently, machine learning methods have shown promise in advancing the state of the art but only at the cost of complex data curation, integrating expert knowledge with aggregation across multiple relevant data sources, file formats, and temporal and spatial resolutions. To streamline this process and accelerate future development, we introduce SubseasonalClimateUSA, a curated dataset for training and benchmarking subseasonal forecasting models in the United States. We use this dataset to benchmark a diverse suite of subseasonal models, including operational dynamical models, classical meteorological baselines, and ten sta
    
[^231]: 一种基于自编码神经网络的高效等离子体表面相互作用代理模型用于溅射过程

    An efficient plasma-surface interaction surrogate model for sputtering processes based on autoencoder neural networks. (arXiv:2109.01406v2 [physics.comp-ph] CROSS LISTED)

    [http://arxiv.org/abs/2109.01406](http://arxiv.org/abs/2109.01406)

    本研究提出了一种基于自编码神经网络的高效等离子体表面相互作用代理模型，用于薄膜溅射沉积模拟。通过训练卷积$\beta$变分自编码器，可以减少高维能量角分布，从而更好地考虑变量而不是单个固定的Ti-Al化学计量。

    

    薄膜溅射沉积的模拟需要将气相中的等离子体和物质输运与边界表面的生长/溅射过程分离。基于解析表达式或查找表的界面模型在本质上将这种复杂交互作用限制为最少量。最近已经展示了基于机器学习的模型可以克服这一点，用于Ar离子轰击Ti-Al复合靶材。然而，所选的网络结构（即多层感知器）提供了约400万自由度，这存在过度拟合相关动态并将模型复杂化至不可靠的风险。本文提出了一个概念上更为复杂但参数方面简化的回归人工神经网络，针对一个扩展方案考虑变量而不是单个固定的Ti-Al化学计量。训练了一个卷积$\beta$变分自编码器来减少高维能量角分布

    Simulations of thin film sputter deposition require the separation of the plasma and material transport in the gas-phase from the growth/sputtering processes at the bounding surfaces. Interface models based on analytic expressions or look-up tables inherently restrict this complex interaction to a bare minimum. A machine learning model has recently been shown to overcome this remedy for Ar ions bombarding a Ti-Al composite target. However, the chosen network structure (i.e., a multilayer perceptron) provides approximately 4 million degrees of freedom, which bears the risk of overfitting the relevant dynamics and complicating the model to an unreliable extend. This work proposes a conceptually more sophisticated but parameterwise simplified regression artificial neural network for an extended scenario, considering a variable instead of a single fixed Ti-Al stoichiometry. A convolutional $\beta$-variational autoencoder is trained to reduce the high-dimensional energy-angular distribution
    
[^232]: 基于数据驱动的方法击败SAA（样本平均逼近）的外推问题

    A data-driven approach to beating SAA out-of-sample. (arXiv:2105.12342v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2105.12342](http://arxiv.org/abs/2105.12342)

    本文提出了一类分布乐观优化（DOO）模型，在外推问题上始终能够超越样本平均逼近（SAA）；然而，乐观解的鲁棒性较差且更容易受到模型错误的影响。

    

    虽然分布鲁棒优化（DRO）问题的解有时可能比样本平均逼近（SAA）的预期奖励要高，但并不保证总是这样。本文引入了一类分布乐观优化（DOO）模型，并证明如果我们考虑最优情况（DOO）和最坏情况（DRO）模型，那么总是可以“击败”SAA的外推效果。然而，我们也证明，这是有代价的：乐观解比最坏情况或SAA优化器更敏感于模型错误，因此不太鲁棒，并且在数据有限时很难校准最坏或最优情况模型以超越SAA。

    While solutions of Distributionally Robust Optimization (DRO) problems can sometimes have a higher out-of-sample expected reward than the Sample Average Approximation (SAA), there is no guarantee. In this paper, we introduce a class of Distributionally Optimistic Optimization (DOO) models, and show that it is always possible to ``beat" SAA out-of-sample if we consider not just worst-case (DRO) models but also best-case (DOO) ones. We also show, however, that this comes at a cost: Optimistic solutions are more sensitive to model error than either worst-case or SAA optimizers, and hence are less robust and calibrating the worst- or best-case model to outperform SAA may be difficult when data is limited.
    
[^233]: 采用自适应和机器学习方法的在线算法与策略

    Online Algorithms and Policies Using Adaptive and Machine Learning Approaches. (arXiv:2105.06577v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2105.06577](http://arxiv.org/abs/2105.06577)

    本文在动态系统中提出了一种AC-RL控制器，在外环中采用强化学习策略确保稳定性和最优性，在内环中采用自适应控制。该控制器适用于两类非线性动态系统，并能适应参数不确定性和输入幅值限制。

    

    本文考虑了在受参数不确定性影响的动态系统中进行实时控制和学习的问题。我们提出了一种在外环中选择适当的强化学习（RL）策略，以确保对于名义动力学的稳定性和最优性，同时在内环中采用自适应控制（AC），以便在实时中，AC将闭环动力学收缩到由RL跟踪的稳定轨迹。考虑了两类非线性动态系统，两者都是控制关联的。第一类动态系统利用扩展形式的平衡点和李亚普诺夫方法，而第二类非线性系统则使用收缩理论。针对这两类系统提出了AC-RL控制器，并证明了其引导在线策略保证稳定性，同时使用高阶调谐器来适应参数不确定性和输入幅值限制。除了建立稳定性保证外，本文还应用仿真实验验证了我们所提出的方法。

    This paper considers the problem of real-time control and learning in dynamic systems subjected to parametric uncertainties. We propose a combination of a Reinforcement Learning (RL) based policy in the outer loop suitably chosen to ensure stability and optimality for the nominal dynamics, together with Adaptive Control (AC) in the inner loop so that in real-time AC contracts the closed-loop dynamics towards a stable trajectory traced out by RL. Two classes of nonlinear dynamic systems are considered, both of which are control-affine. The first class of dynamic systems utilizes equilibrium points %with expansion forms around these points and a Lyapunov approach while second class of nonlinear systems uses contraction theory. AC-RL controllers are proposed for both classes of systems and shown to lead to online policies that guarantee stability using a high-order tuner and accommodate parametric uncertainties and magnitude limits on the input. In addition to establishing a stability gua
    
[^234]: 顺序随机实验的弱信号渐近行为

    Weak Signal Asymptotics for Sequentially Randomized Experiments. (arXiv:2101.09855v5 [math.ST] UPDATED)

    [http://arxiv.org/abs/2101.09855](http://arxiv.org/abs/2101.09855)

    本文使用弱信号渐近行为的方法研究了一类顺序随机实验，认为这类顺序实验的样本路径会弱收敛到扩散极限，并能获得关于几种顺序实验的后悔和信念演变的多个见解。

    

    我们使用弱信号渐近行为的方法研究了一类顺序随机实验，包括解决多臂赌博机问题的实验。在一个$n$个时间步骤的实验中，我们让不同动作的平均奖励间隙按照$1/\sqrt{n}$的比例缩放，以保持学习任务的难度随着$n$的增长而保持不变。在这种情况下，我们发现一类顺序随机实验的样本路径会弱收敛到扩散极限，其中，臂选择的概率会随着状态的变化而持续变化，并在满足连续性假设的情况下进行调整。扩散极限使我们能够推导出精细的、特定于实例的随机动力学特征，并获得关于几种顺序实验的后悔和信念演变的多个见解，包括汤普森采样（但不包括不满足我们连续性假设的UCB）。我们展示了所有顺序实验的表现都能在长时间内稳定。

    We use the lens of weak signal asymptotics to study a class of sequentially randomized experiments, including those that arise in solving multi-armed bandit problems. In an experiment with $n$ time steps, we let the mean reward gaps between actions scale to the order $1/\sqrt{n}$ so as to preserve the difficulty of the learning task as $n$ grows. In this regime, we show that the sample paths of a class of sequentially randomized experiments -- adapted to this scaling regime and with arm selection probabilities that vary continuously with state -- converge weakly to a diffusion limit, given as the solution to a stochastic differential equation. The diffusion limit enables us to derive refined, instance-specific characterization of stochastic dynamics, and to obtain several insights on the regret and belief evolution of a number of sequential experiments including Thompson sampling (but not UCB, which does not satisfy our continuity assumption). We show that all sequential experiments wh
    
[^235]: 一种具有应用于到达模拟和建模的双重随机模拟器

    A Doubly Stochastic Simulator with Applications in Arrivals Modeling and Simulation. (arXiv:2012.13940v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2012.13940](http://arxiv.org/abs/2012.13940)

    该论文提出了一种双重随机模拟器，结合了经典的蒙特卡罗模拟器和神经网络模拟器，用于建模、估计和模拟具有一般非平稳和多维随机到达速率的到达过程，并在高速公路交通和航空交通建模和仿真中得到应用。

    

    我们提出了一种框架，集成了经典的蒙特卡罗模拟器和Wasserstein生成对抗网络，以模拟、估计和模拟具有一般非平稳和多维随机到达速率的广泛到达过程的类别。我们提出了一个双重随机模拟器，它集成了随机生成神经网络和经典的蒙特卡罗泊松模拟器，利用了两者的优势。我们将该框架应用于各种真实世界的应用，包括高速公路交通和航空交通建模和仿真。

    We propose a framework that integrates classical Monte Carlo simulators and Wasserstein generative adversarial networks to model, estimate, and simulate a broad class of arrival processes with general non-stationary and multi-dimensional random arrival rates. Classical Monte Carlo simulators have advantages at capturing the interpretable "physics" of a stochastic object, whereas neural-network-based simulators have advantages at capturing less-interpretable complicated dependence within a high-dimensional distribution. We propose a doubly stochastic simulator that integrates a stochastic generative neural network and a classical Monte Carlo Poisson simulator, to utilize both advantages. Such integration brings challenges to both theoretical reliability and computational tractability for the estimation of the simulator given real data, where the estimation is done through minimizing the Wasserstein distance between the distribution of the simulation output and the distribution of real d
    
[^236]: 高度模块化强化学习库中分布式架构的集成

    Integrating Distributed Architectures in Highly Modular RL Libraries. (arXiv:2007.02622v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2007.02622](http://arxiv.org/abs/2007.02622)

    该研究探讨了在本地和分布式执行级别上实现代理组合性的设计选择，通过独立的可重用组件允许在不同尺度上定义RL代理，并成功解决了新颖和复杂的环境，具有最先进的性能。

    

    推进强化学习（RL）需要具有足够灵活性的工具，以便轻松地原型化新方法，同时避免不切实际的实验周转时间。为了匹配第一个要求，最流行的RL库倡导高度模块化的代理组合性，这有助于实验和开发。为了在合理的时间范围内解决具有挑战性的环境，将RL扩展到大规模采样和计算资源已被证明是一种成功的策略。然而，这种能力迄今为止很难与模块化相结合。在这项工作中，我们探索了设计选择，以允许在本地和分布式执行级别上实现代理组合性。我们提出了一种通用方法，通过独立的可重用组件允许在不同尺度上定义RL代理。我们通过实验证明，我们的设计选择使我们能够复制经典基准测试，探索多个分布式架构，并解决新颖和复杂的环境，具有最先进的性能。

    Advancing reinforcement learning (RL) requires tools that are flexible enough to easily prototype new methods while avoiding impractically slow experimental turnaround times. To match the first requirement, the most popular RL libraries advocate for highly modular agent composability, which facilitates experimentation and development. To solve challenging environments within reasonable time frames, scaling RL to large sampling and computing resources has proved a successful strategy. However, this capability has been so far difficult to combine with modularity. In this work, we explore design choices to allow agent composability both at a local and distributed level of execution. We propose a versatile approach that allows the definition of RL agents at different scales through independent reusable components. We demonstrate experimentally that our design choices allow us to reproduce classical benchmarks, explore multiple distributed architectures, and solve novel and complex environm
    
[^237]: 跨类别变换的鲁棒性：鲁棒性是否由不变神经表示驱动？

    Robustness to Transformations Across Categories: Is Robustness To Transformations Driven by Invariant Neural Representations?. (arXiv:2007.00112v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2007.00112](http://arxiv.org/abs/2007.00112)

    本文通过观察不同种类的图像转换后深度卷积神经网络（DCNNs）的表现，探讨了不变神经表示是否促进了跨类别的图像鲁棒性。实验表明，跨类别变换的鲁棒性是由不变表示所促进的，支持鲁棒性是由不变性驱动而不是专门的子网络的假设。

    

    深度卷积神经网络（DCNNs）已经证明在识别物体在变换下的鲁棒性方面取得了令人印象深刻的结果（例如模糊或噪音），当这些变换被包含在训练集中时。一个解释这种鲁棒性的假设是，DCNNs发展出的不变神经表示在图像转换时不发生改变。然而，这个假设的真实程度是一个尚未解决的问题，因为鲁棒性可能是通过与不变性不同的特性实现的，例如，网络的某些部分可能专门用于识别转换或非转换的图像。本文通过利用不变神经表示促进对训练集之外的变换的鲁棒性，研究了不变神经表示出现的条件。具体而言，我们分析了一种训练范式，在该训练范式中，只有一些对象类别在训练期间被变换，然后评价DCNN是否对所有类别的变换具有鲁棒性，包括那些在训练期间从未见过的类别的变换。我们的实验表明，跨类别变换的鲁棒性是由不变表示所促进的，支持鲁棒性是由不变性驱动而不是专门的子网络的假设。

    Deep Convolutional Neural Networks (DCNNs) have demonstrated impressive robustness to recognize objects under transformations (eg. blur or noise) when these transformations are included in the training set. A hypothesis to explain such robustness is that DCNNs develop invariant neural representations that remain unaltered when the image is transformed. However, to what extent this hypothesis holds true is an outstanding question, as robustness to transformations could be achieved with properties different from invariance, eg. parts of the network could be specialized to recognize either transformed or non-transformed images. This paper investigates the conditions under which invariant neural representations emerge by leveraging that they facilitate robustness to transformations beyond the training distribution. Concretely, we analyze a training paradigm in which only some object categories are seen transformed during training and evaluate whether the DCNN is robust to transformations a
    
[^238]: 运用数据挖掘改进最小延迟问题的启发式算法

    Improving a State-of-the-Art Heuristic for the Minimum Latency Problem with Data Mining. (arXiv:1908.10705v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1908.10705](http://arxiv.org/abs/1908.10705)

    这篇论文通过利用数据挖掘技术，改进了一种基于GRASP的最小延迟问题启发式算法，取得了较好的效果，匹配或优于解的质量，在大大缩短计算时间的同时，还成功地引入了88个新的解成本值。

    

    混合元启发式算法在运筹学中越来越流行。其中一种成功的方法是将贪心随机自适应搜索程序（GRASP）与数据挖掘技术相结合，利用高质量解中发现的频繁模式，在保证搜索范围的同时显著减少计算时间。本文利用数据挖掘技术改进了一个基于GRASP的最小延迟问题启发式算法，适用于两个问题变体。计算实验证明，数据挖掘方法能够在较大数量的实例上匹配或改善解的质量，同时大大减少运行时间。此外，本文还引入了88个新的解成本值。为了支持我们的结果，我们提供了统计显著性检验、挖掘模式的影响、等时间比较和时间到目标曲线的测试。

    Recently, hybrid metaheuristics have become a trend in operations research. A successful example combines the Greedy Randomized Adaptive Search Procedures (GRASP) and data mining techniques, where frequent patterns found in high-quality solutions can lead to an efficient exploration of the search space, along with a significant reduction of computational time. In this work, a GRASP-based state-of-the-art heuristic for the Minimum Latency Problem (MLP) is improved by means of data mining techniques for two MLP variants. Computational experiments showed that the approaches with data mining were able to match or improve the solution quality for a large number of instances, together with a substantial reduction of running time. In addition, 88 new cost values of solutions are introduced into the literature. To support our results, tests of statistical significance, impact of using mined patterns, equal time comparisons and time-to-target plots are provided.
    
[^239]: 通过机器学习等离子体表面界面耦合溅射和气相传输模拟

    Machine learning plasma-surface interface for coupling sputtering and gas-phase transport simulations. (arXiv:1810.04510v1 [physics.plasm-ph] CROSS LISTED)

    [http://arxiv.org/abs/1810.04510](http://arxiv.org/abs/1810.04510)

    该论文提出了一种新的机器学习方法，利用来自直接模拟表面的数据，在运行时为模型提供缺失的表面信息，这将有效促进等离子体处理领域的开展。

    

    靠溅射沉积作薄膜的处理，本质上取决于高能粒子与目标表面的相互作用以及随后的粒子传输。底层物理现象的长度和时间尺度横跨数个数量级。不可能实现跨越所有时间和长度尺度的理论描述。可以特别从基本表面和等离子体过程的良好分离的时间尺度中获得优势。可以从表面模型中计算表面特性，并将其存储于若干典型情况下。随后，可以通过适当的模型界面（例如，解析表达式或查找表）将表面数据提供给气相传输模拟并用于定义插入边界条件。但是，在运行时评估中，维护的表面数据可能证明不足够。在这种情况下，可能通过插值（常见）、外推（在缺乏更好的知识的情况下）或单独调用表面直接模拟 (DSM) 模型来获取缺失数据。本文提出了一种新的机器学习方法，通过从 DSM 数据中学习，在运行时为模型提供缺失的表面信息。所得到的组合模型在各种靶材料和惰性气体中进行了测试。讨论了将该方法推广到更广泛的材料和气体范围内的问题。

    Thin film processing by means of sputter deposition inherently depends on the interaction of energetic particles with a target surface and the subsequent particle transport. The length and time scales of the underlying physical phenomena span orders of magnitudes. A theoretical description which bridges all time and length scales is not practically possible. Advantage can be taken particularly from the well-separated time scales of the fundamental surface and plasma processes. Initially, surface properties may be calculated from a surface model and stored for a number of representative cases. Subsequently, the surface data may be provided to gas-phase transport simulations via appropriate model interfaces (e.g., analytic expressions or look-up tables) and utilized to define insertion boundary conditions. During run-time evaluation, however, the maintained surface data may prove to be not sufficient. In this case, missing data may be obtained by interpolation (common), extrapolation (in
    

