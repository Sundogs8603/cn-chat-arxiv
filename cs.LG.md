# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [ImageBind-LLM: Multi-modality Instruction Tuning.](http://arxiv.org/abs/2309.03905) | ImageBind-LLM是一种多模态指令调优的方法，通过图像-文本对齐训练，并利用联合嵌入实现了优秀的多模态指令跟随能力。 |
| [^2] | [DiffusionEngine: Diffusion Model is Scalable Data Engine for Object Detection.](http://arxiv.org/abs/2309.03893) | DiffusionEngine是一个可扩展的数据引擎，使用Diffusion模型进行目标检测。它通过一个预训练的模型和一个检测适配器，能够在单个阶段提供高质量的训练数据，并生成可扩展、多样化和可泛化的检测数据。 |
| [^3] | [ArtiGrasp: Physically Plausible Synthesis of Bi-Manual Dexterous Grasping and Articulation.](http://arxiv.org/abs/2309.03891) | ArtiGrasp是一种通过强化学习和物理模拟的方式，用一个统一的策略来合成双手灵巧抓握和关节表达的方法。 |
| [^4] | [A Function Interpretation Benchmark for Evaluating Interpretability Methods.](http://arxiv.org/abs/2309.03886) | 本文介绍了一个用于评估自动解释性方法的基准套件，该套件包括了类似于传统系统组件的函数。 |
| [^5] | [DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models.](http://arxiv.org/abs/2309.03883) | DoLa通过对比不同层次的逻辑差异，提高大型语言模型中的真实性和减少幻觉，无需外部知识或微调。 |
| [^6] | [Better Practices for Domain Adaptation.](http://arxiv.org/abs/2309.03879) | 本文通过对一套候选验证标准进行基准测试并使用它们来评估流行的自适应算法，分析了在使用良好的评估实践时的域自适应状态，揭示了域自适应方法论的挑战。 |
| [^7] | [OpinionGPT: Modelling Explicit Biases in Instruction-Tuned LLMs.](http://arxiv.org/abs/2309.03876) | OpinionGPT是一个指令调整大型语言模型(LLMs)的web演示，用户可以选择各种偏见并进行比较，它意在使模型的偏见显性和透明化。 |
| [^8] | [A Tutorial on the Non-Asymptotic Theory of System Identification.](http://arxiv.org/abs/2309.03873) | 这个教程介绍了系统识别中最近发展的非渐近方法、强调了覆盖技术、Hanson-Wright不等式和自标准化马丁格尔法等工具的应用、并给出了利用这些工具简化证明的最小二乘估计器在自回归模型中参数识别中的性能、最后介绍了将这些思想扩展到某些非线性识别问题的方法。 |
| [^9] | [CenTime: Event-Conditional Modelling of Censoring in Survival Analysis.](http://arxiv.org/abs/2309.03851) | CenTime是一种新的生存分析方法，通过创新的事件条件审查机制直接估计事件发生的时间，在处理未被审查的数据时具有良好的鲁棒性和准确性。 |
| [^10] | [Mixtures of Gaussians are Privately Learnable with a Polynomial Number of Samples.](http://arxiv.org/abs/2309.03847) | 通过多项式数量的样本和差分隐私约束，我们提出了一个可以估计高斯混合物的方法，并证明了这个方法的有效性，而无需对GMMs做任何结构性假设。 |
| [^11] | [Gradient-Based Feature Learning under Structured Data.](http://arxiv.org/abs/2309.03843) | 本论文研究了基于梯度的结构化数据特征学习的问题，发现了在非各向异性设置中，常用的球形梯度动力学可能无法恢复真实方向，并提出了一个适当的权重归一化方法来解决这个问题。通过利用输入协方差与目标之间的对齐，可以获得比各向同性情况更好的样本复杂度。 |
| [^12] | [Early warning via transitions in latent stochastic dynamical systems.](http://arxiv.org/abs/2309.03842) | 本研究提出了一种基于定向异性扩散图的方法，通过捕捉低维流形中的潜在演化动态，能够有效提取早期警报信号来检测复杂系统或高维观测数据中的动力学转变，并在真实的脑电图数据上得到了验证。 |
| [^13] | [Bootstrapping Adaptive Human-Machine Interfaces with Offline Reinforcement Learning.](http://arxiv.org/abs/2309.03839) | 本文提出了一种结合离线预训练和在线微调的强化学习算法来训练自适应人机界面，以帮助用户通过嘈杂的高维输入通道进行顺序决策任务。研究中开发了一种独特的方法来表示和推断用户对给定轨迹的长期意图。 |
| [^14] | [Cross-Task Attention Network: Improving Multi-Task Learning for Medical Imaging Applications.](http://arxiv.org/abs/2309.03837) | 这项研究介绍了一种新颖的基于注意力的多任务学习框架，旨在跨任务间更好地利用信息共享，以提高医学影像多任务学习的性能。通过跨任务注意力机制，我们的跨任务注意力网络（CTAN）能够在不同领域和任务中有效整合信息。 |
| [^15] | [Learning from Demonstration via Probabilistic Diagrammatic Teaching.](http://arxiv.org/abs/2309.03835) | 本文介绍了一种名为图示教学的示教学习的替代范式，通过要求用户在场景的二维图像上勾勒示范轨迹来教机器人新的技能，并将其合成为三维任务空间中的运动轨迹的生成模型。 |
| [^16] | [Uncovering Drift in Textual Data: An Unsupervised Method for Detecting and Mitigating Drift in Machine Learning Models.](http://arxiv.org/abs/2309.03831) | 这项研究提出了一种无监督的方法来检测和减轻机器学习模型中的漂移。通过编码生产数据样本和模型训练数据，以及利用核统计检验和最大均值差异（MMD）距离度量来比较分布，可以有效估计漂移情况。 |
| [^17] | [ArtHDR-Net: Perceptually Realistic and Accurate HDR Content Creation.](http://arxiv.org/abs/2309.03827) | 本文提出了一种名为ArtHDR-Net的架构，它可以准确地创建感知逼真的HDR内容。该方法不仅注重重建结果的结构相似性和像素精确性，还强调保留图像的艺术意图，以符合人类的视觉感知。实验证明ArtHDR-Net在HDR内容创建方面具有最先进的性能。 |
| [^18] | [Prime and Modulate Learning: Generation of forward models with signed back-propagation and environmental cues.](http://arxiv.org/abs/2309.03825) | 采用Prime和Modulate方法，通过带符号的反向传播和全局相关信号，实现了前向模型的闭环学习，解决了梯度爆炸和梯度消失问题。 |
| [^19] | [Training Acceleration of Low-Rank Decomposed Networks using Sequential Freezing and Rank Quantization.](http://arxiv.org/abs/2309.03824) | 本文介绍了两种加速低秩分解模型的技术：秩优化和顺序冻结分解层。实验证明，这些技术可以提高模型的训练吞吐量高达60%，推理吞吐量高达37%，同时保持准确性接近原始模型。 |
| [^20] | [Empirical Risk Minimization for Losses without Variance.](http://arxiv.org/abs/2309.03818) | 本文研究了在重尾分布条件下的经验风险最小化问题，通过最小化风险值选择优化器，并通过使用广义通用链方法建立了过度风险的上界。数值研究表明，通过Catoni风格估计的经验风险优化器比其他基准表现更好。 |
| [^21] | [AnthroNet: Conditional Generation of Humans via Anthropometrics.](http://arxiv.org/abs/2309.03812) | 通过人体比例测量构建的AnthroNet模型能够生成各种不同形状和姿势的人体，并以任意姿势生成特定人物身份的人体。该模型通过合成数据进行端到端训练，提供了高精度的人体网格表示和精确的人体比例测量。 |
| [^22] | [Improved theoretical guarantee for rank aggregation via spectral method.](http://arxiv.org/abs/2309.03808) | 本论文通过谱方法改进了排名聚合问题的理论保证，通过研究基于未归一化和归一化数据矩阵的谱排名算法，提供了更准确的扰动误差界限。 |
| [^23] | [Pareto Frontiers in Neural Feature Learning: Data, Compute, Width, and Luck.](http://arxiv.org/abs/2309.03800) | 本研究研究了深度学习算法设计中的微妙选择，特别关注计算统计差距。通过理论和实验，发现稀疏初始化和增加网络宽度可以提高样本效率，并且合成稀疏奇偶任务可以作为真实问题的代理。 |
| [^24] | [Conformal Autoregressive Generation: Beam Search with Coverage Guarantees.](http://arxiv.org/abs/2309.03797) | 该论文介绍了两种新的扩展算法，基于符合性预测的波束搜索算法，用于生成具有理论覆盖保证的序列集合。第一种方法通过动态大小的子集提供结果，但可达到的保证受到校准度量的上界限制。第二种方法将符合性集预测过程与解码过程相结合，可以根据当前的不确定性进行调整的可变波束宽度。这些方法在自然语言处理和化学任务上进行了实证评估。 |
| [^25] | [Adversarially Robust Deep Learning with Optimal-Transport-Regularized Divergences.](http://arxiv.org/abs/2309.03791) | 本论文介绍了一种新的方法ARMOR_D来加强深度学习模型的对抗鲁棒性，该方法基于最优传输正则化差异，通过在分布的邻域上进行最大化期望损失来实现。实验证明，ARMOR_D方法在恶意软件检测和图像识别应用中能够优于现有方法，在对抗攻击下的鲁棒性方面具有较好的效果。 |
| [^26] | [Reduced Simulations for High-Energy Physics, a Middle Ground for Data-Driven Physics Research.](http://arxiv.org/abs/2309.03780) | 该论文提出了一种简化模拟的方法，通过使用红外探测器模型和粒子碰撞事件模拟器来生成简化数据，以便于高能物理研究和教育中的机器学习模型设计和解决方案探索。 |
| [^27] | [CPU frequency scheduling of real-time applications on embedded devices with temporal encoding-based deep reinforcement learning.](http://arxiv.org/abs/2309.03779) | 本研究通过基于时间编码的深度强化学习，开发了一种在嵌入式设备上进行实时应用的CPU频率调度方法，该方法可以在小型设备上推导出高效的功率管理方法，并解决了现有Linux内置方法的限制。 |
| [^28] | [Deep Learning Safety Concerns in Automated Driving Perception.](http://arxiv.org/abs/2309.03774) | 本研究旨在通过引入安全考虑作为结构元素，以系统综合的方式确保基于深度神经网络的自动驾驶系统的安全性。这一概念不仅与现有的安全标准相契合，还为AI安全相关的学术出版物和标准提供了新的启示。 |
| [^29] | [Neural lasso: a unifying approach of lasso and neural networks.](http://arxiv.org/abs/2309.03770) | 本文提出了一种神经套索方法，将套索和神经网络相结合，通过模仿统计框架进行修改的方式，在变量选择中提供更准确的参数估计。 |
| [^30] | [M(otion)-mode Based Prediction of Ejection Fraction using Echocardiograms.](http://arxiv.org/abs/2309.03759) | 本研究提出了一种使用超声心动图的运动模式来估计射血分数和分类心肌病的方法。通过生成多个人工M模式图像并利用对比学习，该方法能够实现高精度的自动化评估，节省了时间和专业知识要求。 |
| [^31] | [TSGBench: Time Series Generation Benchmark.](http://arxiv.org/abs/2309.03755) | TSGBench是首个时间序列生成基准，用于统一和全面评估TSG方法，解决了现有方法在性能评估、数据集选择和评估指标上的限制。 |
| [^32] | [Convergence Analysis of Decentralized ASGD.](http://arxiv.org/abs/2309.03754) | 本文提出了一个不需要部分同步和限制性网络拓扑结构的分散式和异步SGD（DASGD）的收敛速度分析方法。 |
| [^33] | [Medoid Silhouette clustering with automatic cluster number selection.](http://arxiv.org/abs/2309.03751) | 本文介绍了一种具有自动聚类数量选择的中心轮廓聚类算法，其中结合了原始轮廓系数和PAM算法的思想，并通过两种快速版本进行了理论分析和实验验证，在实验中获得了显著的加速效果。 |
| [^34] | [Enhancing Pipeline-Based Conversational Agents with Large Language Models.](http://arxiv.org/abs/2309.03748) | 本文研究了如何使用大型语言模型（LLM）来增强基于流水线的对话系统。在设计和开发阶段，LLM可以帮助生成训练数据、提取实体和同义词、本地化和角色设计。在运营阶段，LLM可以辅助上下文化、意图分类、自动纠正话语、改写回复、摘要和使闭合问题回答能力。通过在私人银行领域的实验，证明了这些能力的有效性。 |
| [^35] | [Learning continuous-valued treatment effects through representation balancing.](http://arxiv.org/abs/2309.03731) | 本研究提出了CBRNet，一种通过表示平衡学习连续值治疗效果的因果机器学习方法。 |
| [^36] | [A Causal Perspective on Loan Pricing: Investigating the Impacts of Selection Bias on Identifying Bid-Response Functions.](http://arxiv.org/abs/2309.03730) | 这项研究通过将定价问题视为因果推断问题，首次对选择偏差对贷款定价的影响进行了探讨。实验结果揭示了传统方法在处理选择偏差问题上的局限性，并提出了更全面的解决方案。 |
| [^37] | [A Natural Gas Consumption Forecasting System for Continual Learning Scenarios based on Hoeffding Trees with Change Point Detection Mechanism.](http://arxiv.org/abs/2309.03720) | 本文介绍了一个基于Hoeffding树和变点检测机制的连续学习场景下的天然气消费预测系统，通过数据流处理，实现了多步 ahead 的预测和持续学习能力。在复杂的实际应用场景中，通过评估预测模型的性能，证明了该方法的有效性。 |
| [^38] | [A State Representation for Diminishing Rewards.](http://arxiv.org/abs/2309.03710) | 该论文研究了多任务强化学习中存在的递减边际效用现象，并引入了一种名为$\lambda$表示（$\lambda$R）的新型状态表示，用于快速策略评估，该表示能够推广已有的状态表示并具备一些正式属性。 |
| [^39] | [Chat Failures and Troubles: Reasons and Solutions.](http://arxiv.org/abs/2309.03708) | 本文研究了人机交互中聊天失败和问题的原因，提出了闭环控制算法和强化学习模型等解决方案以降低错误。 |
| [^40] | [A Probabilistic Semi-Supervised Approach with Triplet Markov Chains.](http://arxiv.org/abs/2309.03707) | 本文提出了一种基于变分贝叶斯推断的通用框架，用于在半监督的情况下训练参数化的三元马尔科夫链模型，并且能够针对各种生成模型推导出半监督算法，用于顺序贝叶斯分类。 |
| [^41] | [DiffDefense: Defending against Adversarial Attacks via Diffusion Models.](http://arxiv.org/abs/2309.03702) | 本文提出了一种新的重构方法，利用扩散模型保护机器学习分类器免受对抗攻击的影响。该方法在保持准确性、速度和兼容性的同时提供了对抗威胁的鲁棒性。 |
| [^42] | [Short-Term Load Forecasting Using A Particle-Swarm Optimized Multi-Head Attention-Augmented CNN-LSTM Network.](http://arxiv.org/abs/2309.03694) | 这项研究提出了一种使用粒子群优化算法、多头注意力机制和计算效率优化框架相结合的方法，用于解决短期负荷预测中的超参数敏感性、解释性不透明和高计算开销等问题。结果表明，该方法在准确性、鲁棒性和计算效率方面具有优势。 |
| [^43] | [A computationally lightweight safe learning algorithm.](http://arxiv.org/abs/2309.03672) | 本文提出了一种计算轻量级的安全学习算法，该算法利用Nadaraya-Watson估计器提供概率安全保证，并解决了高斯过程推理在高维和嵌入式系统中计算复杂度难题。 |
| [^44] | [Dataset Generation and Bonobo Classification from Weakly Labelled Videos.](http://arxiv.org/abs/2309.03671) | 本研究提出了一种从弱标记视频中生成倭黑猩猩数据集并进行分类的方法，并探究了不同的特征提取和分类算法。研究结果表明，数据准备的重要性以及正确的数据分离对于分类性能的影响很大。 |
| [^45] | [How adversarial attacks can disrupt seemingly stable accurate classifiers.](http://arxiv.org/abs/2309.03665) | 本文研究了对抗性攻击如何通过微小修改干扰准确的分类器，并发现这可能是高维输入数据下分类器的基本特征。作者提出了一个通用的框架，解释了实际系统中观察到的关键行为，包括模型对对抗性攻击的容易受到影响，同时对随机扰动具有鲁棒性。验证实验还表明了相同现象在实际神经网络中的存在。 |
| [^46] | [Alzheimer Disease Detection from Raman Spectroscopy of the Cerebrospinal Fluid via Topological Machine Learning.](http://arxiv.org/abs/2309.03664) | 本研究提出了一种使用拉曼光谱和拓扑分析相结合的方法，可以有效地确认或否定阿尔茨海默病的临床诊断。 |
| [^47] | [Towards Comparable Knowledge Distillation in Semantic Image Segmentation.](http://arxiv.org/abs/2309.03659) | 在这项研究中，我们探索了语义图像分割中知识蒸馏的问题。我们发现了25种蒸馏损失项，并指出由于训练配置的差异导致术语比较困难。此外，我们发现超参数选择不当会导致极端的性能差异。 |
| [^48] | [Characterizing Lipschitz Stability of GNN for Fairness.](http://arxiv.org/abs/2309.03648) | 论文通过研究GNN的Lipschitz界限表征了GNN对公平性稳定性的影响，尤其是在处理非欧几里得数据和固有偏倚的情况下。同时，该研究对于限制GNN输出的扰动以保障公平性训练提出了挑战。 |
| [^49] | [Insights Into the Inner Workings of Transformer Models for Protein Function Prediction.](http://arxiv.org/abs/2309.03631) | 本研究通过扩展可解释性人工智能方法，探索了Transformer模型在蛋白质功能预测中的内部运作，并成功识别出了与生物学和化学相关的序列部分，为蛋白质研究提供了重要线索。 |
| [^50] | [Understanding Self-Supervised Learning of Speech Representation via Invariance and Redundancy Reduction.](http://arxiv.org/abs/2309.03619) | 通过标准化潜变量和调整目标函数，我们提出了Modified Barlow Twins (MBT) 方法来改善自监督学习中的语音表示泛化能力，尤其是在有限的目标数据上微调时。这项研究为发展可重用的自监督语音表示迈出了重要的一步。 |
| [^51] | [Filtration Surfaces for Dynamic Graph Classification.](http://arxiv.org/abs/2309.03616) | 过滤表面是一种可扩展和灵活的方法，可用于处理动态图的分类问题。通过实验证明，它在处理依赖于边权重信息的数据集时优于先前的基线方法。该方法具有较低的总体标准差，并且要么完全无参数，要么最多只有一个参数。 |
| [^52] | [Your Battery Is a Blast! Safeguarding Against Counterfeit Batteries with Authentication.](http://arxiv.org/abs/2309.03607) | 该论文通过提出两种新的方法，DCAuth和EISthentication，通过机器学习模型利用电池的内部特征，改善了电池认证的技术。这些方法能够自动鉴别不同类型的锂离子电池，从而防范伪造电池的安全隐患。 |
| [^53] | [Interactive Hyperparameter Optimization in Multi-Objective Problems via Preference Learning.](http://arxiv.org/abs/2309.03581) | 本文提出了一个以人为中心的交互式超参数优化方法，通过应用偏好学习来解决多目标机器学习中的问题。 |
| [^54] | [DTW+S: Shape-based Comparison of Time-series with Ordered Local Trend.](http://arxiv.org/abs/2309.03579) | 提出了一种名为DTW+S的新型测量方法，它通过创建局部趋势的矩阵表示，并应用动态时间规整来计算距离，解决了现有方法无法捕捉局部趋势相似性的问题。 |
| [^55] | [Sparse Federated Training of Object Detection in the Internet of Vehicles.](http://arxiv.org/abs/2309.03569) | 该论文介绍了在车联网物体检测中采用稀疏联合训练的方法。通过在中央服务器共享本地模型，该方法可以减轻隐私泄露问题。同时，通过在边缘设备上进行稀疏训练，可以提高边缘设备的训练效率，减少延迟。 |
| [^56] | [Evaluating the Efficacy of Supervised Learning vs Large Language Models for Identifying Cognitive Distortions and Suicidal Risks in Chinese Social Media.](http://arxiv.org/abs/2309.03564) | 本研究评估了监督学习和大型语言模型在识别中国社交媒体中的认知偏差和自杀风险方面的功效。结果表明大型语言模型在这两个任务上具有很高的效果。 |
| [^57] | [Trinary Decision Trees for missing value handling.](http://arxiv.org/abs/2309.03561) | 本文介绍了一种称为三值决策树的算法，用于改善决策树在处理缺失数据时的表现。与其他方法不同，该算法不假设缺失值包含任何关于响应的信息。实验证明，在特定缺失数据场景下，三值决策树在MCAR设置中表现优异，在IM设置中略逊一筹。同时，通过将三值决策树与缺失在属性方法相结合，可以获得更稳健的性能。尽管训练速度较慢，但三值决策树提供了一种有前途且更准确的方法。 |
| [^58] | [On the dynamics of multi agent nonlinear filtering and learning.](http://arxiv.org/abs/2309.03557) | 本文研究了具有非线性滤波/学习动力学的多智能体网络系统的行为，并介绍了在分布式和联邦学习场景中的应用。 |
| [^59] | [MVD:A Novel Methodology and Dataset for Acoustic Vehicle Type Classification.](http://arxiv.org/abs/2309.03544) | 本研究提出了MVD和MVDA两个开放数据集，用于声学交通监测和车辆类型分类算法的开发。通过使用倒谱和频谱等特征以及多输入神经网络，我们提出了一种新颖且高效的方法来准确分类移动车辆的声音信号。实验结果表明，我们的方法在准确率上优于以前的工作。 |
| [^60] | [Subgraph-based Tight Frames on Graphs with Compact Supports and Vanishing Moments.](http://arxiv.org/abs/2309.03537) | 本研究提出了一种基于子图的紧框架构造方法，能够灵活地调整框架的消失矩和其他属性，实现对具有路径支持的图信号的高效表示，在非线性逼近任务中表现出优越性能。 |
| [^61] | [Feature Enhancer Segmentation Network (FES-Net) for Vessel Segmentation.](http://arxiv.org/abs/2309.03535) | 本研究提出了一种新颖的特征增强分割网络（FES-Net），用于精确地分割视网膜血管。该网络利用提示卷积块进行下采样，并采用浅层上采样方法生成每个类别的二进制掩码，从而避免了其他图像增强步骤的需求。 |
| [^62] | [A Robust Negative Learning Approach to Partial Domain Adaptation Using Source Prototypes.](http://arxiv.org/abs/2309.03531) | 本文提出了一个鲁棒的部分领域适应（PDA）框架，通过整合鲁棒的目标监督策略，解决了负迁移问题，并在领域无关的方式下优化了类内紧凑性和类间分离性。通过预先推断源原型，确保了源数据的隐私性。实验证实了该框架的有效性。 |
| [^63] | [Efficient Single Object Detection on Image Patches with Early Exit Enhanced High-Precision CNNs.](http://arxiv.org/abs/2309.03530) | 本文提出了一种在移动机器人中高效检测单一物体的方法，在RoboCup比赛中特别关注球的检测。该方法通过设计计算能力有限的机器人平台的卷积神经网络架构，在图像补丁中实现高精度的单一物体分类，同时确定其精确的空间位置。此外，提出了早期中止技术以减少背景类中易于拒绝的情况的计算成本。训练过程中采用复合损失函数和数据增强。这种方法在实验中达到了100%的精确度。 |
| [^64] | [Privacy-preserving Continual Federated Clustering via Adaptive Resonance Theory.](http://arxiv.org/abs/2309.03487) | 本文提出了一种隐私保护的持续性联邦聚类算法，通过使用自适应共振理论聚类算法作为基础聚类器，实现了对于未知或持续变化数据的处理。 |
| [^65] | [Fast FixMatch: Faster Semi-Supervised Learning with Curriculum Batch Size.](http://arxiv.org/abs/2309.03469) | 本论文提出了快速的FixMatch算法，通过引入课程批次大小（CBS）来减少半监督学习的训练计算量，并使用强化标记增强和课程伪标签进行改进。 |
| [^66] | [Cross-Image Context Matters for Bongard Problems.](http://arxiv.org/abs/2309.03468) | Bongard问题是一种需要从一组正负图像中推导出抽象概念并进行分类的智力测试，现有方法在Bongard问题中准确率较低。本研究发现，这是因为现有方法未能整合支持集合中的信息，而是仅依赖于单个支持图像的信息。我们提出了一种通过跨图像上下文来提高准确性的解决方案。 |
| [^67] | [Multi-Modality Guidance Network For Missing Modality Inference.](http://arxiv.org/abs/2309.03452) | 我们提出了一种多模态辅助网络，在训练过程中利用多模态表示来训练更好的单模态模型进行推断，解决了推断过程中模态缺失的问题。实验结果表明，我们的方法在性能上显著优于传统方法。 |
| [^68] | [Cross-domain Sound Recognition for Efficient Underwater Data Analysis.](http://arxiv.org/abs/2309.03451) | 本文提出了一种跨域声音识别方法，通过利用非水下声音模型训练数据来分析水下声学数据。通过聚类和可视化方法，简化了候选标签选择的过程，并通过训练神经网络模型实现了对空气枪声的高效识别。 |
| [^69] | [XGen-7B Technical Report.](http://arxiv.org/abs/2309.03450) | XGen-7B是一种用于处理长序列的大型语言模型，通过克服开源LLMs在支持长序列长度方面的限制，并在标准基准上取得与最先进的开源LLMs相当或更好的结果，推进了研究进展和商业应用。 |
| [^70] | [Broadband Ground Motion Synthesis via Generative Adversarial Neural Operators: Development and Validation.](http://arxiv.org/abs/2309.03447) | 本论文提出了一种使用生成对抗神经算子的数据驱动地面运动合成模型，可以根据不同参数生成三分量加速度时间历史。通过使用神经算子架构，模型训练不受数据采样频率影响。研究结果表明，该模型在验证和应用实例中表现出色，并可用于生成日本地震动数据。 |
| [^71] | [Punctate White Matter Lesion Segmentation in Preterm Infants Powered by Counterfactually Generative Learning.](http://arxiv.org/abs/2309.03440) | 本文提出了一种利用反事实生成学习的方法，通过辅助任务和深度学习框架实现了早产儿点状白质病变的准确分割和定位。 |
| [^72] | [Personalized Tucker Decomposition: Modeling Commonality and Peculiarity on Tensor Data.](http://arxiv.org/abs/2309.03439) | 本研究提出了个性化的Tucker分解（perTucker）方法来捕捉张量数据中的异质性，通过学习独特和共性表示，展示了perTucker在异常检测、客户分类和聚类方面的有效性。 |
| [^73] | [Byzantine-Robust Federated Learning with Variance Reduction and Differential Privacy.](http://arxiv.org/abs/2309.03437) | 该论文提出了一种新的联邦学习方案，具有方差减少和差分隐私技术，旨在保护数据隐私，并提高对拜占庭攻击的鲁棒性。 |
| [^74] | [Equal Long-term Benefit Rate: Adapting Static Fairness Notions to Sequential Decision Making.](http://arxiv.org/abs/2309.03426) | 这篇论文介绍了一种称为Equal Long-term Benefit Rate（ELBERT）的长期公平性概念，该概念考虑到不同时间步的变化重要性，并将静态公平性原则应用于顺序设置中。 |
| [^75] | [Large Language Models as Optimizers.](http://arxiv.org/abs/2309.03409) | 本论文提出了一种简单有效的方法，利用大型语言模型(LLMs)作为优化器，通过自然语言描述优化任务。经过实验证明，该方法在线性回归和旅行推销员问题上表现出色，并且优化的最佳提示超过了人为设计的提示。 |
| [^76] | [Community-Based Hierarchical Positive-Unlabeled (PU) Model Fusion for Chronic Disease Prediction.](http://arxiv.org/abs/2309.03386) | 本论文介绍了一种基于社区的分级正类-未标记（PU）模型融合方法，用于慢性病预测。该方法考虑了不同人群之间的差异，并通过构建PU模型树和聚合其输出来进行预测。 |
| [^77] | [ViewMix: Augmentation for Robust Representation in Self-Supervised Learning.](http://arxiv.org/abs/2309.03360) | 这项研究提出了一种名为ViewMix的增强策略，专门用于自监督学习，该策略可以改善模型的表示学习能力，并提高模型的稳健性。 |
| [^78] | [Ensemble linear interpolators: The role of ensembling.](http://arxiv.org/abs/2309.03354) | 本文研究了集成线性插值器如何稳定和提升个体插值器的泛化性能，并引入了基于乘数自助法的袋装最小二乘估计器。在比例区域内，研究了简化和袋装的外样本预测风险。 |
| [^79] | [Source Camera Identification and Detection in Digital Videos through Blind Forensics.](http://arxiv.org/abs/2309.03353) | 本文提出了一种盲取证技术，用于数字视频中源摄像机的识别与检测，通过机器学习的特征提取、特征选择和源分类，有效地确定视频的原始源。 |
| [^80] | [Using Neural Networks for Fast SAR Roughness Estimation of High Resolution Images.](http://arxiv.org/abs/2309.03351) | 本论文提出了一种使用神经网络的框架用于快速估计高分辨率SAR图像的粗糙度参数。与传统方法相比，该方法更快速、误差更小且更可靠。 |
| [^81] | [Relay Diffusion: Unifying diffusion process across resolutions for image synthesis.](http://arxiv.org/abs/2309.03350) | 中继扩散模型（RDM）通过模糊扩散和块噪声将低分辨率的图像或噪声转化为等效的高分辨率图像，实现了在不同分辨率下无需重新开始的扩散过程，并在图像合成方面取得了最先进的性能。 |
| [^82] | [REBOOT: Reuse Data for Bootstrapping Efficient Real-World Dexterous Manipulation.](http://arxiv.org/abs/2309.03322) | 本论文介绍了一种使用强化学习学习灵巧操纵技能的高效系统，通过结合样本高效强化学习和回放缓冲区引导，实现了重用数据以降低真实世界应用中的挑战。 |
| [^83] | [Fitness Approximation through Machine Learning.](http://arxiv.org/abs/2309.03318) | 我们提出了一种使用机器学习模型在遗传算法中进行适应度近似的方法。实验结果表明，这种方法显著提高了进化运行时间，并且适应度得分要么与完全运行的遗传算法相同，要么稍微低一点。 |
| [^84] | [Robotic Table Tennis: A Case Study into a High Speed Learning System.](http://arxiv.org/abs/2309.03315) | 本研究深入研究了一种真实世界的机器人学习系统，该系统能够与人类进行数百次的乒乓球回合，并且能够精确地将球返回到预定的目标位置。该系统整合了感知子系统、高速低延迟机器人控制器、仿真范例、自动重置真实世界环境等功能，并对系统的设计决策和重要性进行了详细描述。 |
| [^85] | [Generating quantum feature maps using multi-objective genetic algorithm.](http://arxiv.org/abs/2309.03307) | 本文介绍了一种使用多目标遗传算法生成量子特征映射的方法，以实现对高维希尔伯特空间的访问，并在优化电路配置时同时考虑分类准确性和门成本。实验结果显示最佳电路配置中纠缠门需要相应的数量，与之前研究相反。同时，我们还提出了使用数据的可分离性指数确定最佳配置的方法。 |
| [^86] | [Scalable Learning of Intrusion Responses through Recursive Decomposition.](http://arxiv.org/abs/2309.03292) | 本文提出了一种通过递归分解方法实现可扩展学习入侵响应的技术，该技术通过解决并行子游戏和计算阈值结构的最佳响应策略来提高效率。 |
| [^87] | [Let Quantum Neural Networks Choose Their Own Frequencies.](http://arxiv.org/abs/2309.03279) | 该论文提出了一种可训练频率的量子模型，通过在生成器中引入可训练参数，使得模型能够学习具有所需属性的生成器，包括非常规间隔频率和灵活的频谱丰富性。实验证明该方法在解决Navier-Stokes方程时具有较高的准确性。 |
| [^88] | [Temporal Inductive Path Neural Network for Temporal Knowledge Graph Reasoning.](http://arxiv.org/abs/2309.03251) | 本论文提出了一种临时归纳路径神经网络（TiPNN）用于时间知识图的推理，采用实体独立的角度建模历史信息，并通过临时归纳路径提取结构和时间信息。 |
| [^89] | [Graph Theory Applications in Advanced Geospatial Research.](http://arxiv.org/abs/2309.03249) | 本论文探讨了图论算法在地理空间科学中的应用，重点介绍了它们在网络分析、空间连接性、地理信息系统以及各种空间问题解决方案中的作用，并列举了在这一领域中实施的广泛研究、创新技术和方法。 |
| [^90] | [EvoCLINICAL: Evolving Cyber-Cyber Digital Twin with Active Transfer Learning for Automated Cancer Registry System.](http://arxiv.org/abs/2309.03246) | EvoCLINICAL提出了一种演进式病癌登记系统的网络病癌双生体，通过主动迁移学习实现了CCDT与真实系统的同步，并为GURI的运行状态提供了各种实验和高级分析。 |
| [^91] | [Testing properties of distributions in the streaming model.](http://arxiv.org/abs/2309.03245) | 本文研究了在有限内存条件下，在流模型中测试分布的属性问题。我们提供了对于标准访问模型和条件访问模型的折衷结果。此外，我们还展示了一种有效学习单调分布的方法，并将其扩展到更大的可分解分布类。 |
| [^92] | [EGIC: Enhanced Low-Bit-Rate Generative Image Compression Guided by Semantic Segmentation.](http://arxiv.org/abs/2309.03244) | EGIC是一种增强的低位速率生成图像压缩方法，通过语义分割提供指导。它在失真感知和失真方向基线方法上表现优越，并具有较小的模型参数和优秀的插值特性。 |
| [^93] | [Automated Bioinformatics Analysis via AutoBA.](http://arxiv.org/abs/2309.03242) | AutoBA是一个基于大型语言模型的自主AI代理程序，通过最少的用户输入简化生物信息学分析过程，并提供详细的逐步计划。经过验证，AutoBA在各种组学分析案例中表现出健壮性和适应性，同时保护数据隐私。 |
| [^94] | [GPT Can Solve Mathematical Problems Without a Calculator.](http://arxiv.org/abs/2309.03241) | 本研究表明，通过充分训练，一个20亿参数的语言模型可以在没有计算器工具的情况下以几乎100%的准确度执行多位数的算术运算，超越了之前的GPT-4。这项研究还通过在附加的多步骤算术运算和数学问题的数据集上进行微调，展示了一个与GPT-4在中文数学问题上相似的性能。 |
| [^95] | [Spatio-Temporal Contrastive Self-Supervised Learning for POI-level Crowd Flow Inference.](http://arxiv.org/abs/2309.03239) | 本文提出了一种针对POI级别人群流推断的时空对比自监督学习模型，通过自监督属性图表示学习以解决数据标记不足、POI间时空依赖性复杂和人群流量与GPS报告之间相关性多样等挑战。 |
| [^96] | [Implicit Design Choices and Their Impact on Emotion Recognition Model Development and Evaluation.](http://arxiv.org/abs/2309.03238) | 本研究探讨了情绪识别中的关键因素，包括收集多样化数据集、处理非典型训练数据、分析数据增强技术和注释方案的影响，以及使用对抗网络处理自然混淆变量和变化。研究结果对于开发准确和稳健的情绪识别模型具有重要意义。 |
| [^97] | [Federated Learning Over Images: Vertical Decompositions and Pre-Trained Backbones Are Difficult to Beat.](http://arxiv.org/abs/2309.03237) | 本研究对联邦学习环境中的图像分类任务进行了评估，发现垂直分解神经网络在各种设置下表现最佳。 |
| [^98] | [Natural Example-Based Explainability: a Survey.](http://arxiv.org/abs/2309.03234) | 本文概述了自然示例为基础的可解释性人工智能的最新进展，这些方法通过使用示例作为解释来提高机器学习模型的可解释性，与人类的学习和推理过程相符，使解释更自然和易懂。 |
| [^99] | [Retail store customer behavior analysis system: Design and Implementation.](http://arxiv.org/abs/2309.03232) | 本研究提出了一个零售店顾客行为分析系统的框架，通过深度学习技术对顾客行为进行数学建模和分析，为商店经理提供了洞察顾客偏好的信息。 |
| [^100] | [Quantum-AI empowered Intelligent Surveillance: Advancing Public Safety Through Innovative Contraband Detection.](http://arxiv.org/abs/2309.03231) | 本研究将量子人工智能与监控领域的RentinaNet模型进行整合，开发了称为Quantum-RetinaNet的智能监控系统，该系统在准确性和速度方面取得了突破性进展。 |
| [^101] | [Which algorithm to select in sports timetabling?.](http://arxiv.org/abs/2309.03229) | 本研究通过实例空间分析体育赛程安排问题的特征，提出了一个基于机器学习的算法选择系统，预测了在给定特征下最适合的算法，同时深入了解了算法性能并提出了改进建议。 |
| [^102] | [Learning a Patent-Informed Biomedical Knowledge Graph Reveals Technological Potential of Drug Repositioning Candidates.](http://arxiv.org/abs/2309.03227) | 本研究提出了一种使用药物专利和生物医学数据库相结合的方法，识别具有技术潜力和科学证据的药物再定位候选物。通过构建科学的生物医学知识图谱和基于专利的生物医学知识图谱，我们可以综合分析多种信息源，为药物再定位研究提供新的视角。 |
| [^103] | [No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function.](http://arxiv.org/abs/2309.03224) | 该论文提出了一种通过蒙特卡洛树搜索和能量函数引导来释放大型语言模型的数学推理能力的方法，以解决当前在数学推理任务中的不足和错误。该方法不需要进一步的微调步骤，通过重新定义模型和引入路径验证器的方式，实现了对输出空间的搜索和推理路径的评估。 |
| [^104] | [Examining the Effectiveness of Chatbots in Gathering Family History Information in Comparison to the Standard In-Person Interview-Based Approach.](http://arxiv.org/abs/2309.03223) | 这项研究提出了一种针对家族历史收集的聊天机器人，并将其与传统面对面采访方法进行比较，探讨了聊天机器人的可行性。研究发现，聊天机器人方法能够克服地理和技术限制，并且具有较长的采访时间。 |
| [^105] | [Companion Animal Disease Diagnostics based on Literal-aware Medical Knowledge Graph Representation Learning.](http://arxiv.org/abs/2309.03219) | 这项研究提出了一种基于文本感知的医学知识图谱表示学习方法，以提高伴侣动物疾病诊断的效率。通过融合各种类型的文本信息和图结构，该方法能够捕捉到重要的实体和关系。 |
| [^106] | [Evaluation of Reinforcement Learning Techniques for Trading on a Diverse Portfolio.](http://arxiv.org/abs/2309.03202) | 本研究评估了在S&P 500指数上使用强化学习技术进行多样化投资组合的可行性。研究发现，包含COVID-19时期的市场数据在训练数据集中可以提供更好的性能，并且基于策略的方法（VI和SARSA）在测试中表现优于Q学习。 |
| [^107] | [Blink: Link Local Differential Privacy in Graph Neural Networks via Bayesian Estimation.](http://arxiv.org/abs/2309.03190) | 本文提出了一种使用链接本地差分隐私的方法，在图神经网络中实现与不受信任的服务器的协作训练。通过贝叶斯估计，将隐私预算分别用于链接和图的度，缓解差分隐私对训练准确性的负面影响，并限制链接概率推断与真实图拓扑之间的误差。提出的LDP机制有两个变体，在不同隐私设置下互补使用，以避免误报链接估计问题。 |
| [^108] | [Impression-Informed Multi-Behavior Recommender System: A Hierarchical Graph Attention Approach.](http://arxiv.org/abs/2309.03169) | 这个论文提出了一种基于印象感知的多行为推荐系统，通过利用注意机制从行为间和行为内部获取信息，并采用多层级图注意力方法，来解决推荐系统在处理多个行为之间互动方面的挑战。 |
| [^109] | [Natural and Robust Walking using Reinforcement Learning without Demonstrations in High-Dimensional Musculoskeletal Models.](http://arxiv.org/abs/2309.02976) | 本论文通过使用强化学习来解决肌肉骨骼冗余问题，并实现了在高维度肌肉骨骼模型中自然且稳健的行走。 |
| [^110] | [Knowledge Distillation Layer that Lets the Student Decide.](http://arxiv.org/abs/2309.02843) | 本论文提出了一个可学习的知识蒸馏层，能够明确地嵌入教师的知识到学生的特征变换中，从而改进了知识蒸馏技术。 |
| [^111] | [Diffusion-EDFs: Bi-equivariant Denoising Generative Modeling on SE(3) for Visual Robotic Manipulation.](http://arxiv.org/abs/2309.02685) | 本文提出了Diffusion-EDFs，一种在视觉机器人操作中应用的基于SE(3)的等变去噪生成建模方法。通过集成SE(3)等变性，我们的方法展示了出色的数据效率和泛化能力。 |
| [^112] | [Domain Adaptation for Efficiently Fine-tuning Vision Transformer with Encrypted Images.](http://arxiv.org/abs/2309.02556) | 本文提出了一种使用视觉Transformer(ViT)进行模型微调的领域自适应方法，可解决使用转换图像训练模型导致准确性下降的问题，实验证明该方法在使用加密图像时也能保持模型的准确性。 |
| [^113] | [A Generalized Bandsplit Neural Network for Cinematic Audio Source Separation.](http://arxiv.org/abs/2309.02539) | 本论文提出了一种通用带通神经网络用于电影音频源分离，通过使用心理声学的频率尺度来定义频带并且利用共同编码器结构的信息共享特性提高了分离性能。 |
| [^114] | [Enhancing Deep Learning Models through Tensorization: A Comprehensive Survey and Framework.](http://arxiv.org/abs/2309.02428) | 本文综述了张量化在深度学习模型中的应用。张量化桥梁了数据的多维特性与传统线性代数方法中的二维矩阵表示之间的差距。研究表明，利用多维数据集进行分析可以提供更好的表达力和结果。 |
| [^115] | [Acoustic-to-articulatory inversion for dysarthric speech: Are pre-trained self-supervised representations favorable?.](http://arxiv.org/abs/2309.01108) | 本研究探讨了使用预先训练的自监督表示对语音障碍者的声学到发音反演任务的影响。实验结果表明，在低资源条件下，经过微调的DeCoAR模型在精细训练方案中相对于健康对照组和患者，分别取得了约1.81\%和约4.56\%的皮尔逊相关系数(CC)的改进。 |
| [^116] | [Learning to Taste: A Multimodal Wine Dataset.](http://arxiv.org/abs/2308.16900) | 这个论文介绍了一个大型多模态葡萄酒数据集，用于研究视觉感知、语言和口感之间的关系，并提出了低维概念嵌入算法，将人类经验与自动机器相似度核相结合，改进了口味分类，并与人类口味知觉相一致。 |
| [^117] | [Transformers as Support Vector Machines.](http://arxiv.org/abs/2308.16898) | 这项工作建立了自注意力和硬间隔支持向量机问题之间的正式等价关系，通过转换器架构的优化几何来解决自然语言处理问题，同时揭示了梯度下降优化的转换器的隐式偏差。 |
| [^118] | [Emoji Promotes Developer Participation and Issue Resolution on GitHub.](http://arxiv.org/abs/2308.16360) | 本研究探讨了表情符号在虚拟工作空间中的使用对开发者参与和问题解决的影响。研究发现，表情符号可以显著缩短问题的解决时间，并吸引更多用户参与。不同类型问题对表情符号的效果也存在差异。 |
| [^119] | [Deep Video Codec Control.](http://arxiv.org/abs/2308.16215) | 本文提出了第一个端到端可学习的深度视频编码控制方法，同时考虑了带宽限制和下游视觉性能，并在不破坏现有标准化的情况下实现了保护深度视觉模型的目标。 |
| [^120] | [Evaluating Explanation Methods for Multivariate Time Series Classification.](http://arxiv.org/abs/2308.15223) | 本文评估了用于多变量时间序列分类的解释方法，重点研究了基于显著性的方法来指示分类决策中最相关的通道和时间序列点。 |
| [^121] | [Mixup-Augmented Meta-Learning for Sample-Efficient Fine-Tuning of Protein Simulators.](http://arxiv.org/abs/2308.15116) | 本文通过Mixup增强的元学习方法实现了对蛋白质模拟器的高效微调，可以在有限的训练数据下泛化到未见过的场景，并提供了一种通用的模拟连续动态条件的方法。 |
| [^122] | [AtmoRep: A stochastic model of atmosphere dynamics using large scale representation learning.](http://arxiv.org/abs/2308.13280) | 提出了一种称为AtmoRep的大气动力学随机模型，它利用大规模表示学习和人工智能技术来确定复杂的大气动力学的通用描述，从而为各种应用提供技能结果。 |
| [^123] | [Polynomial Bounds for Learning Noisy Optical Physical Unclonable Functions and Connections to Learning With Errors.](http://arxiv.org/abs/2308.09199) | 本文研究了学习带噪声的光学物理不可复制函数的问题，通过建立多项式界限，证明了在具备挑战-响应对和计算能力的条件下，可以以任意高的概率学习到该类函数，这对于物理安全领域具有重要意义。 |
| [^124] | [RatGPT: Turning online LLMs into Proxies for Malware Attacks.](http://arxiv.org/abs/2308.09183) | 本研究展示了利用在线LLMs生成的恶意内容，通过将LLM作为攻击者和受害者之间的代理，实现了恶意软件传播并与受害者系统交互的命令，同时规避检测。 |
| [^125] | [Towards Personalized Federated Learning via Heterogeneous Model Reassembly.](http://arxiv.org/abs/2308.08643) | 本文提出了一个名为pFedHR的新框架，利用异构模型重组实现个性化联邦学习。实验表明，pFedHR在各种设置下优于基准方法，并且能够有效降低使用不兼容数据的不良影响。 |
| [^126] | [Continual Pre-Training of Large Language Models: How to (re)warm your model?.](http://arxiv.org/abs/2308.04014) | 该论文研究了大型语言模型的持续预训练问题，探讨了热启动策略对于解决分布变化和提高计算效率的影响。 |
| [^127] | [GraPhSyM: Graph Physical Synthesis Model.](http://arxiv.org/abs/2308.03944) | GraPhSyM是一种用于从物理合成电路网表中快速准确地估计后物理合成电路延迟和面积指标的模型，提供了准确的指标可见性给早期的EDA阶段，可用于全局协同优化，并对基于机器学习的EDA优化框架具有重要的作用。 |
| [^128] | [RAHNet: Retrieval Augmented Hybrid Network for Long-tailed Graph Classification.](http://arxiv.org/abs/2308.02335) | 我们提出了一种检索增强型混合网络(RAHNet)用于长尾图分类任务，通过联合学习稳健的特征提取器和无偏的分类器，解决了图神经网络在长尾类别分布下的偏差和泛化能力有限的问题。 |
| [^129] | [VLUCI: Variational Learning of Unobserved Confounders for Counterfactual Inference.](http://arxiv.org/abs/2308.00904) | VLUCI是一个新颖的可变参数学习模型，用于解决反事实推断中的未观测混淆变量的问题。它通过生成未观测混淆变量的后验分布，并构建一个双重变分推断模型来解决因果推断中观测和未观测混淆变量的问题，从而提高反事实推断的准确性。 |
| [^130] | [A Majority Invariant Approach to Patch Robustness Certification for Deep Learning Models.](http://arxiv.org/abs/2308.00452) | 该论文提出了MajorCert，通过找到在底层分类器中由同一个补丁区域在同一个样本上可以操纵的所有可能标签集合，并检查它们的大多数不变性来对样本进行补丁稳健性认证。 |
| [^131] | [Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models.](http://arxiv.org/abs/2307.14971) | 本文提出了一种新的三维到二维的生成式预训练方法，通过生成视图图像作为预训练方案，帮助三维模型更好地理解点云的几何结构和立体关系，并在实验证明了其优越性。 |
| [^132] | [Adversarial Likelihood Estimation with One-way Flows.](http://arxiv.org/abs/2307.09882) | 本文提出了一种通过单向流进行对抗性似然估计的方法，并使用重要性采样解决了Wasserstein GAN中分区函数有偏估计的问题。同时，通过最大化生成器的熵，提高了模式覆盖效果。这种方法通过计算生成样本的密度来实现对分区函数的无偏估计和生成器熵的计算。 |
| [^133] | [Deep Network Approximation: Beyond ReLU to Diverse Activation Functions.](http://arxiv.org/abs/2307.06555) | 本文研究了深度神经网络在多种激活函数下的表达能力，证明了可以通过在有界集合上构建一个宽度为6N、深度为2L的varrho激活网络来逼近一个宽度为N、深度为L的ReLU网络，从而将对ReLU网络的逼近结果推广到其他激活函数。 |
| [^134] | [Margin Maximization in Attention Mechanism.](http://arxiv.org/abs/2306.13596) | 这篇论文证明了，在softmax-attention模型中，通过在p或等价的W上运行梯度下降，可以收敛到一个最大边缘解，这将局部最优的标记与非最优的标记分隔开。这明确地将注意力机制形式化为标记分离机制。 |
| [^135] | [Pure Exploration in Bandits with Linear Constraints.](http://arxiv.org/abs/2306.12774) | 本文提出了两种相对于线性约束下的多臂赌博问题都是渐进最优的算法。这两种算法都试图跟踪基于下界计算和通过对正常锥体边界进行加权投影所获得的最优分配。 |
| [^136] | [Blended-NeRF: Zero-Shot Object Generation and Blending in Existing Neural Radiance Fields.](http://arxiv.org/abs/2306.12760) | Blended-NeRF是一种鲁棒而灵活的编辑NeRF场景中感兴趣的区域的框架，在保持自然性与一致性的情况下，它可以将用户提供的文本提示或图像补丁的物体合成并混合到原始场景中。 |
| [^137] | [ClimSim: An open large-scale dataset for training high-resolution physics emulators in hybrid multi-scale climate simulators.](http://arxiv.org/abs/2306.08754) | 这是一个用于训练高分辨率物理仿真器的气候数据集。该数据集包含了5.7亿个多变量输入和输出矢量对，用于隔离本地嵌套的高分辨率、高保真度物理的影响。 |
| [^138] | [Off-policy Evaluation in Doubly Inhomogeneous Environments.](http://arxiv.org/abs/2306.08719) | 本研究在双非均匀环境下研究了离线策略评估(OPE)，提出了一种潜在因子模型用于奖励和观测转移函数，并开发了一个通用的OPE框架。该研究对标准RL假设不满足的环境中的OPE做出了理论贡献，并提供了几种实用的方法。 |
| [^139] | [Dynamic Causal Graph Convolutional Network for Traffic Prediction.](http://arxiv.org/abs/2306.07019) | 本文提出了一种动态因果图卷积网络的交通预测方法，通过嵌入时间变化的动态贝叶斯网络和使用图卷积网络来生成交通预测，能够有效地预测交通数据的非线性传播模式。 |
| [^140] | [LDMRes-Net: Enabling Real-Time Disease Monitoring through Efficient Image Segmentation.](http://arxiv.org/abs/2306.06145) | LDMRes-Net是一种轻量级高效的网络，通过采用双重多重残差连接来提高分割性能并最小化计算成本，可用于实时视网膜图像分析任务，并在8个公开数据集上取得了有希望的分割结果。 |
| [^141] | [Unlearnable Examples Give a False Sense of Security: Piercing through Unexploitable Data with Learnable Examples.](http://arxiv.org/abs/2305.09241) | “无法学习的样本”提出一种对数据进行保护的方法，但它无法阻止未经授权的用户对保护后的数据进行利用。通过提出“可学习的未经授权示例”和一种新的纯化过程，我们可以实现对数据的更好保护。 |
| [^142] | [Domain Generalization for Mammographic Image Analysis via Contrastive Learning.](http://arxiv.org/abs/2304.10226) | 研究人员开发出一种基于对比学习的域泛化方法，通过自监督学习生成多种风格和视角的特征嵌入，进一步微调骨干网络以提高分类任务性能。 |
| [^143] | [DiFaReli : Diffusion Face Relighting.](http://arxiv.org/abs/2304.09479) | DiFaReli提出了一种新方法，通过利用条件扩散隐式模型解码解耦的光编码以及从现成的估算器推断出的与3D形状和面部身份相关的其他编码，能够处理单视角的野外环境下的人脸重照，无需光线舞台数据、多视图图像或光照基础事实，实验表明其效果优于现有方法。 |
| [^144] | [Primal-Dual Contextual Bayesian Optimization for Control System Online Optimization with Time-Average Constraints.](http://arxiv.org/abs/2304.06104) | 提出了一种基于原始-对偶语境贝叶斯优化算法，可以实现对约束闭环控制系统的在线性能优化，同时满足所需的约束条件。 |
| [^145] | [Characterizing the Users, Challenges, and Visualization Needs of Knowledge Graphs in Practice.](http://arxiv.org/abs/2304.01311) | 本研究通过访谈19位知识图谱（KG）实践者，发现KG构建者需求架构执行程序，KG分析师需要可自定义查询构建器，KG消费者需要领域特定可视化，并指出在实践中实施KG需要技术和社交方面的解决方案。 |
| [^146] | [FixFit: using parameter-compression to solve the inverse problem in overdetermined models.](http://arxiv.org/abs/2303.13746) | FixFit是一种使用神经网络进行参数压缩的方法，可以解决复杂非线性模型中由于参数之间的相互作用导致的多参数集问题。 |
| [^147] | [Explanation Shift: Investigating Interactions between Models and Shifting Data Distributions.](http://arxiv.org/abs/2303.08081) | 该论文提出了一种新的方法，通过模型解释特征的转移性质来检测分布转移下学习模型的行为是否越界，在比较中发现其比最先进的技术更为优秀，提供了算法方法并在实验中得到验证。 |
| [^148] | [BoundaryCAM: A Boundary-based Refinement Framework for Weakly Supervised Semantic Segmentation of Medical Images.](http://arxiv.org/abs/2303.07853) | BoundaryCAM提出了一种基于边界的弱监督的优化框架，能够预测对象位置，实现精细的高精度分割掩模。 |
| [^149] | [A comparison of rational and neural network based approximations.](http://arxiv.org/abs/2303.04436) | 本文比较了理性和神经网络的近似方法，并发现理性逼近在决策变量数目相同的情况下优于神经网络。数值实验证明了理性逼近的效率，即使逼近参数的数目较小。此外，本文还改进了理性逼近算法，通过调整优化方法来控制约束矩阵的条件数。 |
| [^150] | [Towards provably efficient quantum algorithms for large-scale machine-learning models.](http://arxiv.org/abs/2303.03428) | 本论文提出了一种可能的针对通用（随机）梯度下降算法的高效量子解决方案，只要模型足够耗散和稀疏，具有小的学习率，并且可以缩放至 $O(T^2 \times \text{polylog}(n))$。在实践中，证明了在稀疏训练的情况下，量子计算可以显著提高效率。 |
| [^151] | [Internet Explorer: Targeted Representation Learning on the Open Web.](http://arxiv.org/abs/2302.14051) | Internet Explorer是一种能够利用互联网进行有针对性表示学习的方法，通过自我监督的方式在网络上搜索相关图像并训练小规模模型，从而提高在特定任务上的性能。 |
| [^152] | [Revisiting Hidden Representations in Transfer Learning for Medical Imaging.](http://arxiv.org/abs/2302.08272) | 本研究重新考虑了医学图像迁移学习中的隐藏表示，通过比较在ImageNet和RadImageNet上的初始化，发现ImageNet上预训练的模型在多个医学分类任务上表现优于RadImageNet上的模型。 |
| [^153] | [LB-SimTSC: An Efficient Similarity-Aware Graph Neural Network for Semi-Supervised Time Series Classification.](http://arxiv.org/abs/2301.04838) | LB-SimTSC是一种高效的相似性感知图神经网络，用于半监督时间序列分类。它通过使用LB_Keogh作为DTW的下界来解决DTW二次复杂性限制，在少标签设置中表现出优秀的准确度。 |
| [^154] | [How to select an objective function using information theory.](http://arxiv.org/abs/2212.06566) | 信息论告诉我们，为了最大化模型的信息量，选择可能性最高或表示误差比特最少的客观函数。将不同的客观函数转换为似然函数，它们的相对大小表示我们应该更喜欢哪个客观函数，而其大小的对数表示模型的预期不确定性。 |
| [^155] | [On Root Cause Localization and Anomaly Mitigation through Causal Inference.](http://arxiv.org/abs/2212.04031) | 该论文提出了从因果的角度实现根本原因定位和异常缓解的方法，并在三个数据集上进行了实验验证。 |
| [^156] | [PGFed: Personalize Each Client's Global Objective for Federated Learning.](http://arxiv.org/abs/2212.01448) | PGFed是一个新颖的个性化联邦学习框架，通过显式和自适应地聚合每个客户的经验风险，使每个客户可以个性化其自己的全局目标。 |
| [^157] | [Non-inferiority of Deep Learning Acute Ischemic Stroke Segmentation on Non-Contrast CT Compared to Expert Neuroradiologists.](http://arxiv.org/abs/2211.15341) | 本研究旨在确定卷积神经网络（CNN）深度学习模型在非对比度CT上对急性缺血性改变的分割是否能够达到与神经放射医学家相媲美的水平。 |
| [^158] | [USE-Evaluator: Performance Metrics for Medical Image Segmentation Models with Uncertain, Small or Empty Reference Annotations.](http://arxiv.org/abs/2209.13008) | USE-Evaluator是一种用于医学图像分割模型的性能度量方法，可以解决由于参考注释不确定、小型或空白导致的常规指标无法适用于临床数据集的问题。 |
| [^159] | [Global Optimization for Cardinality-constrained Minimum Sum-of-Squares Clustering via Semidefinite Programming.](http://arxiv.org/abs/2209.08901) | 本文提出了一种基于半定规划的全局优化方法来解决带基数约束的最小平方和聚类问题，并通过引入新的SDP松弛方法和定制的分支策略来改善解的质量和性能。 |
| [^160] | [The Space of Adversarial Strategies.](http://arxiv.org/abs/2209.04521) | 本文提出了一种系统的方法来研究对抗性策略的空间。通过分解攻击组件，并引入新的攻击方法，我们可以更好地理解最坏情况下的对抗行为，并且衡量攻击性能。 |
| [^161] | [Accelerating Numerical Solvers for Large-Scale Simulation of Dynamical System via NeurVec.](http://arxiv.org/abs/2208.03680) | 通过NeurVec，我们提出了一种基于深度学习的修正器，它能够补偿集成误差并在模拟中实现更大的时间步长。NeurVec在复杂动态系统中表现出了显著的泛化能力，加速了传统求解器，同时保持了高水平的准确性和稳定性。 |
| [^162] | [Models of human preference for learning reward functions.](http://arxiv.org/abs/2206.02231) | 本研究提出了一种将人类偏好建模为每个轨迹段的遗憾的方法，并证明了可以根据这些遗憾生成的偏好来识别生成这些偏好的奖励函数。实验证明，这种遗憾偏好模型在性能上优于以前的模型。 |
| [^163] | [Efficient anti-symmetrization of a neural network layer by taming the sign problem.](http://arxiv.org/abs/2205.12250) | 通过驯服符号问题，我们展示了如何高效地实现神经网络层的反对称化，为在反对称神经网络中使用通用的反对称层作为构建模块打开了大门，并且该近似方法在符号问题受控时非常有效。 |
| [^164] | [Auto-SDE: Learning effective reduced dynamics from data-driven stochastic dynamical systems.](http://arxiv.org/abs/2205.04151) | 本文提出了一种名为Auto-SDE的算法来学习慢-快随机动力学系统的有效降维动力学，通过自动编码器神经网络和离散化的随机微分方程，捕捉了系统的演化特性，并在数值实验证明了其准确性、稳定性和有效性。 |
| [^165] | [DeepAD: A Robust Deep Learning Model of Alzheimer's Disease Progression for Real-World Clinical Applications.](http://arxiv.org/abs/2203.09096) | 阿尔茨海默病进展预测中，我们提出了一种新颖的多模态多任务深度学习模型，整合了多个队列中的临床和神经影像数据，通过分析高维MRI特征和其他数据模态来预测患者未来的病情轨迹。 |
| [^166] | [Truncated Diffusion Probabilistic Models and Diffusion-based Adversarial Auto-Encoders.](http://arxiv.org/abs/2202.09671) | 我们提出了一种更快更廉价的截断扩散概率模型方法，通过从隐藏噪声数据分布开始生成数据，相较于传统的方法可以获得更好的性能改进。 |
| [^167] | [Limitation of Characterizing Implicit Regularization by Data-independent Functions.](http://arxiv.org/abs/2201.12198) | 本研究旨在数学定义和研究隐式正则化，发现使用数据独立函数进行特征化的方法存在局限性，并提出了两个动力学机制以及相应的生成方法，进一步强调了隐式正则化的数据依赖性。 |
| [^168] | [Kernelized Concept Erasure.](http://arxiv.org/abs/2201.12191) | 通过核化线性极小极大博弈，防止特定非线性对手预测概念，但无法彻底解决非线性编码的概念擦除问题。 |
| [^169] | [Q-Learning for MDPs with General Spaces: Convergence and Near Optimality via Quantization under Weak Continuity.](http://arxiv.org/abs/2111.06781) | 本文研究了在具有连续状态和动作空间的MDPs中使用Q-Learning的收敛和近似最优性问题。通过量化状态和动作，我们证明Quantized Q-Learning会收敛到一个极限，并且这个极限满足一个最优性方程，从而实现近似最优性。 |
| [^170] | [Comparing Sequential Forecasters.](http://arxiv.org/abs/2110.00115) | 本文提出了一种比较序贯预测器的方法，通过设计新的序贯推断程序来估计预测得分的时变差异，这种方法避免了对预测和结果生成方式的不可验证假设。 |
| [^171] | [Bridging the Gap Between Target Networks and Functional Regularization.](http://arxiv.org/abs/2106.02613) | 该论文研究了靶网络与功能正则化在深度强化学习中的作用。通过实验证明靶网络作为隐式正则化器在某些情况下有利，但不灵活且可能导致不稳定。为了解决这些问题，作者提出了一种明确的功能正则化替代方法，并对其收敛性进行了理论研究。 |
| [^172] | [BoXHED2.0: Scalable boosting of dynamic survival analysis.](http://arxiv.org/abs/2103.12591) | BoXHED2.0是一个可扩展的动态生存分析提升方法，适用于包括重复事件和竞争风险在内的多种生存环境，具有与参数化提升生存模型相媲美的速度。 |
| [^173] | [Graph Fairing Convolutional Networks for Anomaly Detection.](http://arxiv.org/abs/2010.10274) | 本文引入了一种图形平滑卷积网络用于半监督异常检测，通过使用跳跃连接和图结构信息来学习有区分性的节点表示。 |
| [^174] | [Proper Learning of Linear Dynamical Systems as a Non-Commutative Polynomial Optimisation Problem.](http://arxiv.org/abs/2002.01444) | 该论文提出了一种方法来解决适当学习线性动态系统的问题，通过非交换多项式优化，保证了数值解对最小二乘估计器的全局收敛性。 |
| [^175] | [Copula Representations and Error Surface Projections for the Exclusive Or Problem.](http://arxiv.org/abs/1907.04483) | 本研究讨论了通过概率逻辑和关联Copula函数解决异或表示和逼近问题的方法，并通过比较不同激活函数下的误差面动态来说明其优势。通过将xor表示从布尔值扩展到实数值，我们提供了一种演示交叉验证概念的方便方式。 |

# 详细

[^1]: ImageBind-LLM: 多模态指令调优的方法

    ImageBind-LLM: Multi-modality Instruction Tuning. (arXiv:2309.03905v1 [cs.MM])

    [http://arxiv.org/abs/2309.03905](http://arxiv.org/abs/2309.03905)

    ImageBind-LLM是一种多模态指令调优的方法，通过图像-文本对齐训练，并利用联合嵌入实现了优秀的多模态指令跟随能力。

    

    我们提出了一种通过ImageBind对大型语言模型（LLMs）进行多模态指令调优的方法。现有的工作主要集中在语言和图像指令调优方面，与此不同，我们的ImageBind-LLM可以响应多模态条件，包括音频、3D点云、视频以及它们的嵌入空间算术，只需进行图像-文本对齐训练。在训练过程中，我们采用可学习的Bind网络来对齐LLaMA和ImageBind的图像编码器之间的嵌入空间。然后，通过Bind网络转换的图像特征被添加到LLaMA的所有层的单词标记中，通过一个无注意力和零初始化的门控机制逐步注入视觉指令。在ImageBind的联合嵌入的帮助下，简单的图像-文本训练使我们的模型展示出了卓越的多模态指令跟随能力。在推断过程中，多模态输入被送入相应的ImageBind编码器，并被处理。

    We present ImageBind-LLM, a multi-modality instruction tuning method of large language models (LLMs) via ImageBind. Existing works mainly focus on language and image instruction tuning, different from which, our ImageBind-LLM can respond to multi-modality conditions, including audio, 3D point clouds, video, and their embedding-space arithmetic by only image-text alignment training. During training, we adopt a learnable bind network to align the embedding space between LLaMA and ImageBind's image encoder. Then, the image features transformed by the bind network are added to word tokens of all layers in LLaMA, which progressively injects visual instructions via an attention-free and zero-initialized gating mechanism. Aided by the joint embedding of ImageBind, the simple image-text training enables our model to exhibit superior multi-modality instruction-following capabilities. During inference, the multi-modality inputs are fed into the corresponding ImageBind encoders, and processed by 
    
[^2]: DiffusionEngine: 扩展数据引擎的扩散模型适用于目标检测

    DiffusionEngine: Diffusion Model is Scalable Data Engine for Object Detection. (arXiv:2309.03893v1 [cs.CV])

    [http://arxiv.org/abs/2309.03893](http://arxiv.org/abs/2309.03893)

    DiffusionEngine是一个可扩展的数据引擎，使用Diffusion模型进行目标检测。它通过一个预训练的模型和一个检测适配器，能够在单个阶段提供高质量的训练数据，并生成可扩展、多样化和可泛化的检测数据。

    

    数据是深度学习的基石。本文揭示了最近开发的Diffusion模型是一个用于目标检测的可扩展数据引擎。现有的缩放检测导向数据的方法通常需要手动收集或生成模型来获取目标图像，然后进行数据增强和标注产生训练对，这些方法成本高、复杂或缺乏多样性。为了解决这些问题，我们提出了DiffusionEngine（DE），这是一个数据扩展引擎，以单一阶段提供高质量的检测导向训练对。DE由一个预训练的扩散模型和一个有效的检测适配器组成，为生成可扩展、多样化和可泛化的检测数据做出了贡献，并支持即插即用。检测适配器通过学习将现成的扩散模型中的隐式语义和位置知识与检测相关的信号进行对齐，从而产生更好的边界框预测。此外，我们还贡献了两个数据集，即COCO-DE和...

    Data is the cornerstone of deep learning. This paper reveals that the recently developed Diffusion Model is a scalable data engine for object detection. Existing methods for scaling up detection-oriented data often require manual collection or generative models to obtain target images, followed by data augmentation and labeling to produce training pairs, which are costly, complex, or lacking diversity. To address these issues, we presentDiffusionEngine (DE), a data scaling-up engine that provides high-quality detection-oriented training pairs in a single stage. DE consists of a pre-trained diffusion model and an effective Detection-Adapter, contributing to generating scalable, diverse and generalizable detection data in a plug-and-play manner. Detection-Adapter is learned to align the implicit semantic and location knowledge in off-the-shelf diffusion models with detection-aware signals to make better bounding-box predictions. Additionally, we contribute two datasets, i.e., COCO-DE and
    
[^3]: ArtiGrasp：双手灵巧抓握和关节表达的物理合理合成

    ArtiGrasp: Physically Plausible Synthesis of Bi-Manual Dexterous Grasping and Articulation. (arXiv:2309.03891v1 [cs.RO])

    [http://arxiv.org/abs/2309.03891](http://arxiv.org/abs/2309.03891)

    ArtiGrasp是一种通过强化学习和物理模拟的方式，用一个统一的策略来合成双手灵巧抓握和关节表达的方法。

    

    我们提出了ArtiGrasp，一种新的方法来合成包括抓握和关节表达在内的双手手-物体交互。由于全局手腕运动和精确的手指控制对于物体的关节表达是必要的，这个任务具有挑战性。ArtiGrasp利用强化学习和物理模拟训练一个控制全局和局部手姿态的策略。我们的框架在一个共同的手姿态参考下统一了抓握和关节表达。此外，为了训练关节表达所需的精确手指控制，我们提出了一个逐渐增加难度的学习课程。它从单手操作静止物体开始，然后进行包括两只手和非静止物体的多智能体训练。为了评估我们的方法，我们引入了动态物体抓握和关节表达，这是一个将物体移到目标关节姿态的任务。这个任务需要抓握，关节表达和物体姿态的控制。

    We present ArtiGrasp, a novel method to synthesize bi-manual hand-object interactions that include grasping and articulation. This task is challenging due to the diversity of the global wrist motions and the precise finger control that are necessary to articulate objects. ArtiGrasp leverages reinforcement learning and physics simulations to train a policy that controls the global and local hand pose. Our framework unifies grasping and articulation within a single policy guided by a single hand pose reference. Moreover, to facilitate the training of the precise finger control required for articulation, we present a learning curriculum with increasing difficulty. It starts with single-hand manipulation of stationary objects and continues with multi-agent training including both hands and non-stationary objects. To evaluate our method, we introduce Dynamic Object Grasping and Articulation, a task that involves bringing an object into a target articulated pose. This task requires grasping,
    
[^4]: 一个用于评估解释性方法的功能解释基准

    A Function Interpretation Benchmark for Evaluating Interpretability Methods. (arXiv:2309.03886v1 [cs.CL])

    [http://arxiv.org/abs/2309.03886](http://arxiv.org/abs/2309.03886)

    本文介绍了一个用于评估自动解释性方法的基准套件，该套件包括了类似于传统系统组件的函数。

    

    使用人类可读的描述标记神经网络子模块对于许多下游任务非常有用：这些描述可以暴露失败、引导干预，甚至可以解释重要的模型行为。到目前为止，大多数基于机械原理的已训练网络描述都涉及到小模型、狭义现象，并且需要大量人力。在不断增加的模型大小和复杂性中标记出所有人可解释的子计算几乎肯定需要能够自动生成和验证描述的工具。最近，利用学习模型进行标记的技术开始受到关注，但评估其有效性的方法有限且临时。我们应该如何验证和比较开放式标记工具？本文介绍了FIND（函数解释和描述），一个用于评估自动解释方法构建模块的基准套件。FIND包含了类似于传统系统的组件的函数。

    Labeling neural network submodules with human-legible descriptions is useful for many downstream tasks: such descriptions can surface failures, guide interventions, and perhaps even explain important model behaviors. To date, most mechanistic descriptions of trained networks have involved small models, narrowly delimited phenomena, and large amounts of human labor. Labeling all human-interpretable sub-computations in models of increasing size and complexity will almost certainly require tools that can generate and validate descriptions automatically. Recently, techniques that use learned models in-the-loop for labeling have begun to gain traction, but methods for evaluating their efficacy are limited and ad-hoc. How should we validate and compare open-ended labeling tools? This paper introduces FIND (Function INterpretation and Description), a benchmark suite for evaluating the building blocks of automated interpretability methods. FIND contains functions that resemble components of tr
    
[^5]: DoLa：通过对比层次提高大型语言模型中的真实性

    DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models. (arXiv:2309.03883v1 [cs.CL])

    [http://arxiv.org/abs/2309.03883](http://arxiv.org/abs/2309.03883)

    DoLa通过对比不同层次的逻辑差异，提高大型语言模型中的真实性和减少幻觉，无需外部知识或微调。

    

    尽管大型语言模型（LLMs）具有令人印象深刻的能力，但它们容易出现幻觉，即生成与预训练期间观察到的事实偏离的内容。我们提出了一种简单的解码策略，用于减少预训练LLMs中的幻觉，它不需要在检索的外部知识或额外的微调上进行条件约束。我们的方法通过对比将较晚层和较早层投影到词汇空间得到的逻辑差异来获得下一个令牌的分布，利用了LLMs中的事实知识通常被证明局部化在特定的Transformer层中的事实。我们发现，这种通过对比层次的解码（DoLa）方法能够更好地展示事实知识，并减少生成不正确事实的情况。DoLa在多个选择任务和开放式生成任务中持续提升了真实性，例如改善了LLaMA系列模型在TruthfulQA上的表现。

    Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained LLMs that does not require conditioning on retrieved external knowledge nor additional fine-tuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an LLMs has generally been shown to be localized to particular transformer layers. We find that this Decoding by Contrasting Layers (DoLa) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA b
    
[^6]: 提升域自适应的最佳实践

    Better Practices for Domain Adaptation. (arXiv:2309.03879v1 [cs.LG])

    [http://arxiv.org/abs/2309.03879](http://arxiv.org/abs/2309.03879)

    本文通过对一套候选验证标准进行基准测试并使用它们来评估流行的自适应算法，分析了在使用良好的评估实践时的域自适应状态，揭示了域自适应方法论的挑战。

    

    在机器学习的实际应用中，分布偏移是非常常见的现象。域自适应（DA）旨在通过提供各种框架来适应模型到部署数据，而无需使用标签来解决这个问题。然而，域偏移场景引发了另一个较为微妙的挑战：在没有带标签的验证集的情况下，为这些自适应算法执行超参数优化（HPO）的困难。DA的不明确的验证协议导致了文献中的不良实践，例如在实际场景中，通过使用目标测试标签来执行HPO，而这些标签是不可用的。这导致了DA研究进展与实际情况相比存在过度乐观的问题。在本文中，我们通过对一套候选验证标准进行基准测试并使用它们来评估流行的自适应算法，分析了在使用良好的评估实践时的域自适应状态。我们展示了域自适应方法论的三个分支都面临挑战。

    Distribution shifts are all too common in real-world applications of machine learning. Domain adaptation (DA) aims to address this by providing various frameworks for adapting models to the deployment data without using labels. However, the domain shift scenario raises a second more subtle challenge: the difficulty of performing hyperparameter optimisation (HPO) for these adaptation algorithms without access to a labelled validation set. The unclear validation protocol for DA has led to bad practices in the literature, such as performing HPO using the target test labels when, in real-world scenarios, they are not available. This has resulted in over-optimism about DA research progress compared to reality. In this paper, we analyse the state of DA when using good evaluation practice, by benchmarking a suite of candidate validation criteria and using them to assess popular adaptation algorithms. We show that there are challenges across all three branches of domain adaptation methodology 
    
[^7]: OpinionGPT: 模拟显性偏见的指令调整大型语言模型(LLMs)

    OpinionGPT: Modelling Explicit Biases in Instruction-Tuned LLMs. (arXiv:2309.03876v1 [cs.CL])

    [http://arxiv.org/abs/2309.03876](http://arxiv.org/abs/2309.03876)

    OpinionGPT是一个指令调整大型语言模型(LLMs)的web演示，用户可以选择各种偏见并进行比较，它意在使模型的偏见显性和透明化。

    

    最近，指令调整大型语言模型(LLMs)展示了生成与自然语言指令相匹配的回应的显著能力。然而，一个开放的研究问题涉及训练模型和它们的回应中固有的偏见。例如，如果用于调整LLM的数据主要由具有特定政治偏见的人编写，我们可能会期望生成的回答也共享这种偏见。目前的研究工作旨在除去这样的模型偏见，或抑制可能有偏见的回答。通过这个演示，我们对指令调整中的偏见持有不同的观点：我们的目标不是抑制它们，而是使它们显性和透明。为此，我们提供了OpinionGPT，一个网络演示，用户可以提问并选择所有他们希望调查的偏见。该演示将使用在代表每个选择偏见的文本上进行微调的模型来回答这个问题，从而实现并排比较。为了训练基础模型，我们选取了11个...

    Instruction-tuned Large Language Models (LLMs) have recently showcased remarkable ability to generate fitting responses to natural language instructions. However, an open research question concerns the inherent biases of trained models and their responses. For instance, if the data used to tune an LLM is dominantly written by persons with a specific political bias, we might expect generated answers to share this bias. Current research work seeks to de-bias such models, or suppress potentially biased answers. With this demonstration, we take a different view on biases in instruction-tuning: Rather than aiming to suppress them, we aim to make them explicit and transparent. To this end, we present OpinionGPT, a web demo in which users can ask questions and select all biases they wish to investigate. The demo will answer this question using a model fine-tuned on text representing each of the selected biases, allowing side-by-side comparison. To train the underlying model, we identified 11 
    
[^8]: 系统识别的非渐近理论教程

    A Tutorial on the Non-Asymptotic Theory of System Identification. (arXiv:2309.03873v1 [eess.SY])

    [http://arxiv.org/abs/2309.03873](http://arxiv.org/abs/2309.03873)

    这个教程介绍了系统识别中最近发展的非渐近方法、强调了覆盖技术、Hanson-Wright不等式和自标准化马丁格尔法等工具的应用、并给出了利用这些工具简化证明的最小二乘估计器在自回归模型中参数识别中的性能、最后介绍了将这些思想扩展到某些非线性识别问题的方法。

    

    这个教程介绍最近发展的非渐近方法在主要线性系统识别理论中的应用。我们强调一些在这个领域中特别有用的工具，如覆盖技术、Hanson-Wright不等式和自标准化马丁格尔法。然后我们利用这些工具来给出一些基于最小二乘估计器的性能的简化证明，用于识别自回归模型中的参数。最后，我们概述了如何将所呈现的思想扩展到某些非线性识别问题中。

    This tutorial serves as an introduction to recently developed non-asymptotic methods in the theory of -- mainly linear -- system identification. We emphasize tools we deem particularly useful for a range of problems in this domain, such as the covering technique, the Hanson-Wright Inequality and the method of self-normalized martingales. We then employ these tools to give streamlined proofs of the performance of various least-squares based estimators for identifying the parameters in autoregressive models. We conclude by sketching out how the ideas presented herein can be extended to certain nonlinear identification problems.
    
[^9]: CenTime: 事件条件模型在生存分析中的应用

    CenTime: Event-Conditional Modelling of Censoring in Survival Analysis. (arXiv:2309.03851v1 [cs.LG])

    [http://arxiv.org/abs/2309.03851](http://arxiv.org/abs/2309.03851)

    CenTime是一种新的生存分析方法，通过创新的事件条件审查机制直接估计事件发生的时间，在处理未被审查的数据时具有良好的鲁棒性和准确性。

    

    生存分析是一种有价值的工具，可以基于基线观测来估计特定事件（如死亡或癌症复发）发生的时间。这在医疗保健中非常有用，可以根据患者数据预测临床重要事件的预后。然而，现有方法常常存在局限性；有些方法只关注将患者按生存能力进行排名，忽视了对实际事件时间的估计；而其他方法将问题视为分类任务，忽视了事件的时间顺序结构。此外，有效利用被审查样本（训练数据点，其中确切事件时间不可知）对于提高模型的预测准确性至关重要。在本文中，我们引入了CenTime，一种新的生存分析方法，直接估计事件发生的时间。我们的方法具有创新的事件条件审查机制，即使没有未被审查的数据，也能表现出良好的鲁棒性。我们证明了我们的方法在准确性上的优势。

    Survival analysis is a valuable tool for estimating the time until specific events, such as death or cancer recurrence, based on baseline observations. This is particularly useful in healthcare to prognostically predict clinically important events based on patient data. However, existing approaches often have limitations; some focus only on ranking patients by survivability, neglecting to estimate the actual event time, while others treat the problem as a classification task, ignoring the inherent time-ordered structure of the events. Furthermore, the effective utilization of censored samples - training data points where the exact event time is unknown - is essential for improving the predictive accuracy of the model. In this paper, we introduce CenTime, a novel approach to survival analysis that directly estimates the time to event. Our method features an innovative event-conditional censoring mechanism that performs robustly even when uncensored data is scarce. We demonstrate that ou
    
[^10]: 高斯混合物可以通过多项式数量的样本进行差分隐私学习

    Mixtures of Gaussians are Privately Learnable with a Polynomial Number of Samples. (arXiv:2309.03847v1 [stat.ML])

    [http://arxiv.org/abs/2309.03847](http://arxiv.org/abs/2309.03847)

    通过多项式数量的样本和差分隐私约束，我们提出了一个可以估计高斯混合物的方法，并证明了这个方法的有效性，而无需对GMMs做任何结构性假设。

    

    我们研究了在差分隐私(DP)约束下估计高斯混合物的问题。我们的主要结果是，使用$\tilde{O}(k^2 d^4 \log(1/\delta) / \alpha^2 \varepsilon)$个样本即可在满足$(\varepsilon, \delta)$-DP的条件下估计$k$个高斯混合物，使其达到总变差距离$\alpha$。这是该问题的第一个有限样本复杂性上限，而无需对GMMs做任何结构性假设。为了解决这个问题，我们构建了一个新的框架，该框架对于其他任务可能也有用。在高层次上，我们展示了如果一个分布类（比如高斯分布）是（1）可列表译码的并且（2）在总变差距离方面具有“局部小”覆盖[ BKSW19]，则其混合物类是私密可学习的。证明绕过了一个已知障碍，表明与高斯分布不同，GMMs不具有局部小的覆盖[AAL21]。

    We study the problem of estimating mixtures of Gaussians under the constraint of differential privacy (DP). Our main result is that $\tilde{O}(k^2 d^4 \log(1/\delta) / \alpha^2 \varepsilon)$ samples are sufficient to estimate a mixture of $k$ Gaussians up to total variation distance $\alpha$ while satisfying $(\varepsilon, \delta)$-DP. This is the first finite sample complexity upper bound for the problem that does not make any structural assumptions on the GMMs.  To solve the problem, we devise a new framework which may be useful for other tasks. On a high level, we show that if a class of distributions (such as Gaussians) is (1) list decodable and (2) admits a "locally small'' cover [BKSW19] with respect to total variation distance, then the class of its mixtures is privately learnable. The proof circumvents a known barrier indicating that, unlike Gaussians, GMMs do not admit a locally small cover [AAL21].
    
[^11]: 基于梯度的结构化数据特征学习

    Gradient-Based Feature Learning under Structured Data. (arXiv:2309.03843v1 [stat.ML])

    [http://arxiv.org/abs/2309.03843](http://arxiv.org/abs/2309.03843)

    本论文研究了基于梯度的结构化数据特征学习的问题，发现了在非各向异性设置中，常用的球形梯度动力学可能无法恢复真实方向，并提出了一个适当的权重归一化方法来解决这个问题。通过利用输入协方差与目标之间的对齐，可以获得比各向同性情况更好的样本复杂度。

    

    最近的研究表明，基于梯度的单指数模型学习的样本复杂度取决于它们的信息指数。然而，这些结果仅涉及各向同性数据，而实际应用中的输入往往包含额外的结构，可以隐含地指导算法。在这项工作中，我们研究了一种尖峰协方差结构的影响，并揭示了一些有趣的现象。首先，我们发现在非各向异性设置中，常用的球形梯度动力学即使在尖峰与目标方向完全对齐时也可能无法恢复真实方向。接下来，我们展示了一种适当的权重归一化方法，类似于批量归一化，可以缓解这个问题。此外，通过利用（尖峰）输入协方差与目标之间的对齐，我们获得了比各向同性情况更好的样本复杂度。

    Recent works have demonstrated that the sample complexity of gradient-based learning of single index models, i.e. functions that depend on a 1-dimensional projection of the input data, is governed by their information exponent. However, these results are only concerned with isotropic data, while in practice the input often contains additional structure which can implicitly guide the algorithm. In this work, we investigate the effect of a spiked covariance structure and reveal several interesting phenomena. First, we show that in the anisotropic setting, the commonly used spherical gradient dynamics may fail to recover the true direction, even when the spike is perfectly aligned with the target direction. Next, we show that appropriate weight normalization that is reminiscent of batch normalization can alleviate this issue. Further, by exploiting the alignment between the (spiked) input covariance and the target, we obtain improved sample complexity compared to the isotropic case. In pa
    
[^12]: 隐性随机动力学系统中的转向预警

    Early warning via transitions in latent stochastic dynamical systems. (arXiv:2309.03842v1 [stat.ML])

    [http://arxiv.org/abs/2309.03842](http://arxiv.org/abs/2309.03842)

    本研究提出了一种基于定向异性扩散图的方法，通过捕捉低维流形中的潜在演化动态，能够有效提取早期警报信号来检测复杂系统或高维观测数据中的动力学转变，并在真实的脑电图数据上得到了验证。

    

    在许多实际应用中，如基因突变、脑疾病、自然灾害、金融危机和工程可靠性，对复杂系统或高维观测数据中的动力学转变进行早期警报是至关重要的。为了有效提取早期警报信号，我们开发了一种新方法：定向异性扩散图，它捕捉了低维流形中的潜在演化动态。将该方法应用于真实的脑电图（EEG）数据，我们成功找到了适当的有效坐标，并推导出能够检测状态转变中临界点的早期警报信号。我们的方法将潜在动态与原始数据集联系起来。通过数值实验证明了该框架在密度和转变概率等方面的准确性和有效性。结果表明，第二个坐标在各种评估指标中保持有意义的信息。

    Early warnings for dynamical transitions in complex systems or high-dimensional observation data are essential in many real world applications, such as gene mutation, brain diseases, natural disasters, financial crises, and engineering reliability. To effectively extract early warning signals, we develop a novel approach: the directed anisotropic diffusion map that captures the latent evolutionary dynamics in low-dimensional manifold. Applying the methodology to authentic electroencephalogram (EEG) data, we successfully find the appropriate effective coordinates, and derive early warning signals capable of detecting the tipping point during the state transition. Our method bridges the latent dynamics with the original dataset. The framework is validated to be accurate and effective through numerical experiments, in terms of density and transition probability. It is shown that the second coordinate holds meaningful information for critical transition in various evaluation metrics.
    
[^13]: 使用离线强化学习为自适应人机界面引导引导

    Bootstrapping Adaptive Human-Machine Interfaces with Offline Reinforcement Learning. (arXiv:2309.03839v1 [cs.RO])

    [http://arxiv.org/abs/2309.03839](http://arxiv.org/abs/2309.03839)

    本文提出了一种结合离线预训练和在线微调的强化学习算法来训练自适应人机界面，以帮助用户通过嘈杂的高维输入通道进行顺序决策任务。研究中开发了一种独特的方法来表示和推断用户对给定轨迹的长期意图。

    

    自适应界面可以帮助用户在给定嘈杂的高维命令信号（例如来自脑-计算机接口）的情况下执行顺序决策任务，例如机器人遥操作。人在循环机器学习的最新进展使得这些系统能够通过与用户进行交互来改进，但在实际应用中往往受限于从单个用户收集的数据量。本文提出了一种强化学习算法，通过离线预训练和在线微调训练界面以将原始命令信号映射到动作。为了解决嘈杂命令信号和稀疏奖励带来的挑战，我们提出了一种用于表示和推断用户对给定轨迹的长期意图的新方法。我们主要通过一个用户研究评估我们的方法在帮助仅通过嘈杂的高维输入通道进行通信的用户时的能力，研究中有12名参与者进行了模拟导航任务。

    Adaptive interfaces can help users perform sequential decision-making tasks like robotic teleoperation given noisy, high-dimensional command signals (e.g., from a brain-computer interface). Recent advances in human-in-the-loop machine learning enable such systems to improve by interacting with users, but tend to be limited by the amount of data that they can collect from individual users in practice. In this paper, we propose a reinforcement learning algorithm to address this by training an interface to map raw command signals to actions using a combination of offline pre-training and online fine-tuning. To address the challenges posed by noisy command signals and sparse rewards, we develop a novel method for representing and inferring the user's long-term intent for a given trajectory. We primarily evaluate our method's ability to assist users who can only communicate through noisy, high-dimensional input channels through a user study in which 12 participants performed a simulated nav
    
[^14]: 跨任务注意力网络：改进医学影像多任务学习

    Cross-Task Attention Network: Improving Multi-Task Learning for Medical Imaging Applications. (arXiv:2309.03837v1 [cs.CV])

    [http://arxiv.org/abs/2309.03837](http://arxiv.org/abs/2309.03837)

    这项研究介绍了一种新颖的基于注意力的多任务学习框架，旨在跨任务间更好地利用信息共享，以提高医学影像多任务学习的性能。通过跨任务注意力机制，我们的跨任务注意力网络（CTAN）能够在不同领域和任务中有效整合信息。

    

    多任务学习（MTL）是深度学习中一种强大的方法，利用多个任务的信息在训练过程中提升模型性能。在医学影像领域，MTL已经展现出解决各种任务的巨大潜力。然而，现有的医学影像MTL架构在跨任务信息共享方面存在局限，降低了MTL的潜在性能改进。在本研究中，我们提出了一种新颖的基于注意力的MTL框架，以更好地利用任务之间的相互作用。具体而言，我们提出了一种跨任务注意力网络（CTAN），利用跨任务注意力机制通过任务间的交互来整合信息。我们在包括放射治疗计划预测、色素性皮肤损伤分割和诊断等不同领域和任务的四个医学影像数据集上验证了CTAN的有效性。

    Multi-task learning (MTL) is a powerful approach in deep learning that leverages the information from multiple tasks during training to improve model performance. In medical imaging, MTL has shown great potential to solve various tasks. However, existing MTL architectures in medical imaging are limited in sharing information across tasks, reducing the potential performance improvements of MTL. In this study, we introduce a novel attention-based MTL framework to better leverage inter-task interactions for various tasks from pixel-level to image-level predictions. Specifically, we propose a Cross-Task Attention Network (CTAN) which utilizes cross-task attention mechanisms to incorporate information by interacting across tasks. We validated CTAN on four medical imaging datasets that span different domains and tasks including: radiation treatment planning prediction using planning CT images of two different target cancers (Prostate, OpenKBP); pigmented skin lesion segmentation and diagnosi
    
[^15]: 通过概率性图示教学进行示教学习

    Learning from Demonstration via Probabilistic Diagrammatic Teaching. (arXiv:2309.03835v1 [cs.RO])

    [http://arxiv.org/abs/2309.03835](http://arxiv.org/abs/2309.03835)

    本文介绍了一种名为图示教学的示教学习的替代范式，通过要求用户在场景的二维图像上勾勒示范轨迹来教机器人新的技能，并将其合成为三维任务空间中的运动轨迹的生成模型。

    

    示教学习（Learning for Demonstration，LfD）使得机器人可以通过模仿专家示范来获得新技能，允许用户以直观的方式传达他们的指示。最近在LfD领域的进展往往依赖于动作示范教学或远程操作作为用户指定示范的手段。动作示范教学需要对机器人进行物理操纵，而远程操作则需要熟练掌握额外的硬件。本文介绍了一种名为图示教学的LfD的替代范式。图示教学旨在通过要求用户在场景的二维图像上勾勒示范轨迹来教机器人新的技能，然后这些轨迹将被合成为三维任务空间中的运动轨迹的生成模型。此外，我们还提出了用于图示教学的射线追踪概率轨迹学习（RPTL）框架。RPTL从二维图示中提取时间变化的概率密度，并应用射线追踪来寻找相应的区域。

    Learning for Demonstration (LfD) enables robots to acquire new skills by imitating expert demonstrations, allowing users to communicate their instructions in an intuitive manner. Recent progress in LfD often relies on kinesthetic teaching or teleoperation as the medium for users to specify the demonstrations. Kinesthetic teaching requires physical handling of the robot, while teleoperation demands proficiency with additional hardware. This paper introduces an alternative paradigm for LfD called Diagrammatic Teaching. Diagrammatic Teaching aims to teach robots novel skills by prompting the user to sketch out demonstration trajectories on 2D images of the scene, these are then synthesised as a generative model of motion trajectories in 3D task space. Additionally, we present the Ray-tracing Probabilistic Trajectory Learning (RPTL) framework for Diagrammatic Teaching. RPTL extracts time-varying probability densities from the 2D sketches, applies ray-tracing to find corresponding regions i
    
[^16]: 揭示文本数据中的漂移：一种无监督的方法用于检测和减轻机器学习模型中的漂移

    Uncovering Drift in Textual Data: An Unsupervised Method for Detecting and Mitigating Drift in Machine Learning Models. (arXiv:2309.03831v1 [cs.CL])

    [http://arxiv.org/abs/2309.03831](http://arxiv.org/abs/2309.03831)

    这项研究提出了一种无监督的方法来检测和减轻机器学习模型中的漂移。通过编码生产数据样本和模型训练数据，以及利用核统计检验和最大均值差异（MMD）距离度量来比较分布，可以有效估计漂移情况。

    

    机器学习中的漂移指的是数据或模型运行上下文的统计特性随时间变化而导致性能下降的现象。因此，保持对机器学习模型性能的持续监控过程对于预防潜在性能回退至关重要。然而，有监督的漂移检测方法需要人工标注，从而导致漂移检测和减轻过程时间较长。在我们提出的无监督漂移检测方法中，我们采用了两个步骤的流程。我们的第一步涉及将生产数据的一个样本作为目标分布，将模型训练数据作为参考分布进行编码。在第二步中，我们使用基于核的统计检验，并利用最大均值差异（MMD）距离度量来比较参考分布和目标分布，估计任何潜在的漂移。我们的方法还能确定生产数据子集的漂移情况。

    Drift in machine learning refers to the phenomenon where the statistical properties of data or context, in which the model operates, change over time leading to a decrease in its performance. Therefore, maintaining a constant monitoring process for machine learning model performance is crucial in order to proactively prevent any potential performance regression. However, supervised drift detection methods require human annotation and consequently lead to a longer time to detect and mitigate the drift. In our proposed unsupervised drift detection method, we follow a two step process. Our first step involves encoding a sample of production data as the target distribution, and the model training data as the reference distribution. In the second step, we employ a kernel-based statistical test that utilizes the maximum mean discrepancy (MMD) distance metric to compare the reference and target distributions and estimate any potential drift. Our method also identifies the subset of production
    
[^17]: ArtHDR-Net: 感知逼真和准确的HDR内容创建

    ArtHDR-Net: Perceptually Realistic and Accurate HDR Content Creation. (arXiv:2309.03827v1 [cs.CV])

    [http://arxiv.org/abs/2309.03827](http://arxiv.org/abs/2309.03827)

    本文提出了一种名为ArtHDR-Net的架构，它可以准确地创建感知逼真的HDR内容。该方法不仅注重重建结果的结构相似性和像素精确性，还强调保留图像的艺术意图，以符合人类的视觉感知。实验证明ArtHDR-Net在HDR内容创建方面具有最先进的性能。

    

    高动态范围（HDR）内容创建已成为现代媒体和娱乐行业、游戏和增强/虚拟现实行业的重要议题。许多方法已被提出，用于重建输入低动态范围（LDR）图像/视频的HDR对应物，给定单个曝光或多曝光LDR。目前的方法主要关注重建的结构相似性和像素精确性的保持。然而，这些传统方法并不强调在媒体、娱乐和游戏方面保持图像的艺术意图，这是人类视觉感知中的一个重要元素。在本文中，我们试图研究和填补这一空白。我们提出了一种基于卷积神经网络的架构，称为ArtHDR-Net，它使用多曝光LDR特征作为输入。实验结果表明，ArtHDR-Net在HDR-VDP-2得分方面取得了最先进的性能（即平均意见得分indec）

    High Dynamic Range (HDR) content creation has become an important topic for modern media and entertainment sectors, gaming and Augmented/Virtual Reality industries. Many methods have been proposed to recreate the HDR counterparts of input Low Dynamic Range (LDR) images/videos given a single exposure or multi-exposure LDRs. The state-of-the-art methods focus primarily on the preservation of the reconstruction's structural similarity and the pixel-wise accuracy. However, these conventional approaches do not emphasize preserving the artistic intent of the images in terms of human visual perception, which is an essential element in media, entertainment and gaming. In this paper, we attempt to study and fill this gap. We propose an architecture called ArtHDR-Net based on a Convolutional Neural Network that uses multi-exposed LDR features as input. Experimental results show that ArtHDR-Net can achieve state-of-the-art performance in terms of the HDR-VDP-2 score (i.e., mean opinion score inde
    
[^18]: Prime和Modulate学习：通过带符号反向传播和环境线索生成前向模型

    Prime and Modulate Learning: Generation of forward models with signed back-propagation and environmental cues. (arXiv:2309.03825v1 [cs.LG])

    [http://arxiv.org/abs/2309.03825](http://arxiv.org/abs/2309.03825)

    采用Prime和Modulate方法，通过带符号的反向传播和全局相关信号，实现了前向模型的闭环学习，解决了梯度爆炸和梯度消失问题。

    

    深度神经网络在学习中使用的误差反向传播可能面临梯度爆炸和梯度消失的问题。许多解决方案已经被提出，例如规范化技术或将激活函数限制为线性整流单元。在这项工作中，我们采用了一种不同的方法，该方法特别适用于前向模型的闭环学习，其中反向传播仅使用误差信号的符号来引导学习，而全局相关信号调节学习速率。这受到局部可塑性与全局神经调节之间的相互作用的启发。例如，在空旷的道路上行驶时，可以允许对行动进行缓慢逐步的优化，而在繁忙的十字路口，必须立即纠正错误。因此，错误就是引导信号，经历的强度是权重变化的调节因素。这种Prime和Modulate范式的优点是双重的：

    Deep neural networks employing error back-propagation for learning can suffer from exploding and vanishing gradient problems. Numerous solutions have been proposed such as normalisation techniques or limiting activation functions to linear rectifying units. In this work we follow a different approach which is particularly applicable to closed-loop learning of forward models where back-propagation makes exclusive use of the sign of the error signal to prime the learning, whilst a global relevance signal modulates the rate of learning. This is inspired by the interaction between local plasticity and a global neuromodulation. For example, whilst driving on an empty road, one can allow for slow step-wise optimisation of actions, whereas, at a busy junction, an error must be corrected at once. Hence, the error is the priming signal and the intensity of the experience is a modulating factor in the weight change. The advantages of this Prime and Modulate paradigm is twofold: it is free from n
    
[^19]: 低秩分解网络的训练加速：顺序冻结和秩量化

    Training Acceleration of Low-Rank Decomposed Networks using Sequential Freezing and Rank Quantization. (arXiv:2309.03824v1 [cs.LG])

    [http://arxiv.org/abs/2309.03824](http://arxiv.org/abs/2309.03824)

    本文介绍了两种加速低秩分解模型的技术：秩优化和顺序冻结分解层。实验证明，这些技术可以提高模型的训练吞吐量高达60%，推理吞吐量高达37%，同时保持准确性接近原始模型。

    

    低秩分解（LRD）是一种应用于深度学习模型权重张量的模型压缩技术，以减少可训练参数和计算复杂性。然而，由于在应用LRD后在架构中添加了大量新层，如果分解秩不够小，则可能导致训练/推理加速性不高。问题在于，使用较小的秩会增加分解后的显著准确率下降的风险。本文中，我们提出了两种加速低秩分解模型的技术，而不需要使用较小的秩进行分解。这些方法包括秩优化和顺序冻结分解层。我们在卷积和基于transformer的模型上进行了实验证明，这些技术在保持接近原始模型准确性的同时，可以提高模型的训练吞吐量高达60%，推理吞吐量高达37%。

    Low Rank Decomposition (LRD) is a model compression technique applied to the weight tensors of deep learning models in order to reduce the number of trainable parameters and computational complexity. However, due to high number of new layers added to the architecture after applying LRD, it may not lead to a high training/inference acceleration if the decomposition ranks are not small enough. The issue is that using small ranks increases the risk of significant accuracy drop after decomposition. In this paper, we propose two techniques for accelerating low rank decomposed models without requiring to use small ranks for decomposition. These methods include rank optimization and sequential freezing of decomposed layers. We perform experiments on both convolutional and transformer-based models. Experiments show that these techniques can improve the model throughput up to 60% during training and 37% during inference when combined together while preserving the accuracy close to that of the o
    
[^20]: 无方差损失下的经验风险最小化

    Empirical Risk Minimization for Losses without Variance. (arXiv:2309.03818v1 [stat.ML])

    [http://arxiv.org/abs/2309.03818](http://arxiv.org/abs/2309.03818)

    本文研究了在重尾分布条件下的经验风险最小化问题，通过最小化风险值选择优化器，并通过使用广义通用链方法建立了过度风险的上界。数值研究表明，通过Catoni风格估计的经验风险优化器比其他基准表现更好。

    

    本文考虑了在重尾分布条件下的经验风险最小化问题，其中数据没有有限方差，但只有$p$阶矩，其中$p \in (1,2)$。我们选择通过最小化风险值来选择优化器，而不是使用基于截断观测数据的估计过程。这些风险值可以通过使用Catoni的显著方法（Catoni, 2012）进行稳健估计来得到。由于Catoni型影响函数的结构，我们能够通过使用广义的通用链方法建立过度风险的上界。此外，我们考虑了计算问题。我们特别从理论上研究了两种类型的优化方法，即稳健梯度下降算法和基于经验风险的方法。通过广泛的数值研究，我们发现通过Catoni风格估计的经验风险优化器确实比其他基准表现更好。这表明直接基于截断数据的估计可能会导致较差的性能。

    This paper considers an empirical risk minimization problem under heavy-tailed settings, where data does not have finite variance, but only has $p$-th moment with $p \in (1,2)$. Instead of using estimation procedure based on truncated observed data, we choose the optimizer by minimizing the risk value. Those risk values can be robustly estimated via using the remarkable Catoni's method (Catoni, 2012). Thanks to the structure of Catoni-type influence functions, we are able to establish excess risk upper bounds via using generalized generic chaining methods. Moreover, we take computational issues into consideration. We especially theoretically investigate two types of optimization methods, robust gradient descent algorithm and empirical risk-based methods. With an extensive numerical study, we find that the optimizer based on empirical risks via Catoni-style estimation indeed shows better performance than other baselines. It indicates that estimation directly based on truncated data may 
    
[^21]: AnthroNet: 通过人体比例生成条件化的人体模型

    AnthroNet: Conditional Generation of Humans via Anthropometrics. (arXiv:2309.03812v1 [cs.CV])

    [http://arxiv.org/abs/2309.03812](http://arxiv.org/abs/2309.03812)

    通过人体比例测量构建的AnthroNet模型能够生成各种不同形状和姿势的人体，并以任意姿势生成特定人物身份的人体。该模型通过合成数据进行端到端训练，提供了高精度的人体网格表示和精确的人体比例测量。

    

    我们提出了一种基于广泛的人体比例测量而构建的新颖人体模型，能够生成各种不同形状和姿势的人体。所提出的模型通过深度生成架构，实现了对特定人物身份的直接建模，并能够以任意姿势生成人体。这是第一种通过仅使用合成数据进行端到端训练的模型，不仅提供了高精度的人体网格表示，还允许对人体进行精确的人体比例测量。此外，通过使用高度多样化的动画库，我们为合成人体的身体和手部进行了关节处理，以最大程度地提高模型训练中可学习先验的多样性。我们的模型在一个包含10万个程序生成的人体网格和相应人体比例测量的数据集上进行了训练。我们的合成数据生成器可以用于生成非商业学术用途下的数百万个独特人物身份和姿势。

    We present a novel human body model formulated by an extensive set of anthropocentric measurements, which is capable of generating a wide range of human body shapes and poses. The proposed model enables direct modeling of specific human identities through a deep generative architecture, which can produce humans in any arbitrary pose. It is the first of its kind to have been trained end-to-end using only synthetically generated data, which not only provides highly accurate human mesh representations but also allows for precise anthropometry of the body. Moreover, using a highly diverse animation library, we articulated our synthetic humans' body and hands to maximize the diversity of the learnable priors for model training. Our model was trained on a dataset of $100k$ procedurally-generated posed human meshes and their corresponding anthropometric measurements. Our synthetic data generator can be used to generate millions of unique human identities and poses for non-commercial academic 
    
[^22]: 通过谱方法改进了排名聚合的理论保证

    Improved theoretical guarantee for rank aggregation via spectral method. (arXiv:2309.03808v1 [stat.ML])

    [http://arxiv.org/abs/2309.03808](http://arxiv.org/abs/2309.03808)

    本论文通过谱方法改进了排名聚合问题的理论保证，通过研究基于未归一化和归一化数据矩阵的谱排名算法，提供了更准确的扰动误差界限。

    

    给定多个项目之间的成对比较，如何对它们进行排名以使得排名与观察相匹配？这个问题被称为排名聚合，在体育、推荐系统和其他网络应用中已经找到了许多应用。由于找到最小化不匹配的全局排名通常是NP困难的（称为Kemeny优化），我们将重点放在Erd\"os-R\'enyi离群点（ERO）模型上。在这个排名问题中，每个成对比较是真实分数差异的被损坏副本。我们研究了基于未归一化和归一化数据矩阵的谱排名算法。关键是理解它们在从观察数据中恢复每个项目的潜在分数方面的性能。这归结为推导未归一化/归一化数据矩阵的前几个特征向量和其总体对应物之间的逐个项扰动误差界限。通过使用留出技术，我们提供了一个更准确的$\ell_{\infty}$-norm扰动误差界限。

    Given pairwise comparisons between multiple items, how to rank them so that the ranking matches the observations? This problem, known as rank aggregation, has found many applications in sports, recommendation systems, and other web applications. As it is generally NP-hard to find a global ranking that minimizes the mismatch (known as the Kemeny optimization), we focus on the Erd\"os-R\'enyi outliers (ERO) model for this ranking problem. Here, each pairwise comparison is a corrupted copy of the true score difference. We investigate spectral ranking algorithms that are based on unnormalized and normalized data matrices. The key is to understand their performance in recovering the underlying scores of each item from the observed data. This reduces to deriving an entry-wise perturbation error bound between the top eigenvectors of the unnormalized/normalized data matrix and its population counterpart. By using the leave-one-out technique, we provide a sharper $\ell_{\infty}$-norm perturbati
    
[^23]: 神经特征学习中的帕累托前沿：数据、计算、宽度和运气

    Pareto Frontiers in Neural Feature Learning: Data, Compute, Width, and Luck. (arXiv:2309.03800v1 [cs.LG])

    [http://arxiv.org/abs/2309.03800](http://arxiv.org/abs/2309.03800)

    本研究研究了深度学习算法设计中的微妙选择，特别关注计算统计差距。通过理论和实验，发现稀疏初始化和增加网络宽度可以提高样本效率，并且合成稀疏奇偶任务可以作为真实问题的代理。

    

    本研究探讨了在计算统计差距存在的情况下，深度学习中微妙的算法设计选择。我们首先考虑了离线稀疏奇偶学习，这是一个有关多层感知器梯度训练的监督分类问题，其具有统计查询下界。这个下界可以解释为多资源的权衡前沿：成功学习只有在一个足够丰富（大型模型）、知识渊博（大规模数据集）、耐心（训练迭代次数多）或幸运（随机猜测次数多）的情况下才能发生。我们通过理论和实验表明，在这种情况下，稀疏初始化和增加网络宽度可以显著提高样本效率。在这里，宽度起到了并行搜索的作用：它增加了找到“幸运神经元”的概率，这些神经元可以更高效地学习稀疏特征。最后，我们表明合成稀疏奇偶任务可以作为真实问题的代理。

    This work investigates the nuanced algorithm design choices for deep learning in the presence of computational-statistical gaps. We begin by considering offline sparse parity learning, a supervised classification problem which admits a statistical query lower bound for gradient-based training of a multilayer perceptron. This lower bound can be interpreted as a multi-resource tradeoff frontier: successful learning can only occur if one is sufficiently rich (large model), knowledgeable (large dataset), patient (many training iterations), or lucky (many random guesses). We show, theoretically and experimentally, that sparse initialization and increasing network width yield significant improvements in sample efficiency in this setting. Here, width plays the role of parallel search: it amplifies the probability of finding "lottery ticket" neurons, which learn sparse features more sample-efficiently. Finally, we show that the synthetic sparse parity task can be useful as a proxy for real pro
    
[^24]: 异常自回归生成：带覆盖保证的波束搜索

    Conformal Autoregressive Generation: Beam Search with Coverage Guarantees. (arXiv:2309.03797v1 [cs.LG])

    [http://arxiv.org/abs/2309.03797](http://arxiv.org/abs/2309.03797)

    该论文介绍了两种新的扩展算法，基于符合性预测的波束搜索算法，用于生成具有理论覆盖保证的序列集合。第一种方法通过动态大小的子集提供结果，但可达到的保证受到校准度量的上界限制。第二种方法将符合性集预测过程与解码过程相结合，可以根据当前的不确定性进行调整的可变波束宽度。这些方法在自然语言处理和化学任务上进行了实证评估。

    

    我们介绍了基于符合性预测的波束搜索算法的两个新扩展，用于产生具有理论覆盖保证的序列集合。第一种方法非常简单，提出了动态大小的波束搜索结果子集，但与典型的符合性程序不同，它在可达到的保证上有一个上界，取决于事后校准度量。我们的第二种算法将符合性集预测过程作为解码过程的一部分，产生一个能够根据当前的不确定性进行调整的可变波束宽度。虽然更复杂，但该过程可以实现预先选定的覆盖保证。我们为每种方法提供边缘覆盖边界，并在从自然语言处理和化学中选择的任务上进行了实证评估。

    We introduce two new extensions to the beam search algorithm based on conformal predictions (CP) to produce sets of sequences with theoretical coverage guarantees. The first method is very simple and proposes dynamically-sized subsets of beam search results but, unlike typical CP procedures, has an upper bound on the achievable guarantee depending on a post-hoc calibration measure. Our second algorithm introduces the conformal set prediction procedure as part of the decoding process, producing a variable beam width which adapts to the current uncertainty. While more complex, this procedure can achieve coverage guarantees selected a priori. We provide marginal coverage bounds for each method, and evaluate them empirically on a selection of tasks drawing from natural language processing and chemistry.
    
[^25]: 使用最优传输正则化差异来提高对抗性鲁棒深度学习

    Adversarially Robust Deep Learning with Optimal-Transport-Regularized Divergences. (arXiv:2309.03791v1 [cs.LG])

    [http://arxiv.org/abs/2309.03791](http://arxiv.org/abs/2309.03791)

    本论文介绍了一种新的方法ARMOR_D来加强深度学习模型的对抗鲁棒性，该方法基于最优传输正则化差异，通过在分布的邻域上进行最大化期望损失来实现。实验证明，ARMOR_D方法在恶意软件检测和图像识别应用中能够优于现有方法，在对抗攻击下的鲁棒性方面具有较好的效果。

    

    我们引入了ARMOR_D方法作为增强深度学习模型对抗性鲁棒性的创新方法。这些方法基于一种新的最优传输正则化差异类，通过信息差异和最优传输成本之间的infimal卷积构建。我们使用这些方法来增强对抗性鲁棒性，通过在分布的邻域上最大化期望损失，这被称为分布鲁棒优化技术。作为构建对抗样本的工具，我们的方法允许样本根据最优传输成本进行传输，并根据信息差异进行重新加权。我们在恶意软件检测和图像识别应用上证明了我们方法的有效性，并发现在增强对抗攻击鲁棒性方面，据我们所知，它优于现有方法。ARMOR_D在FGSM攻击下的robustified准确率达到98.29%，在其他攻击下达到98.18%。

    We introduce the $ARMOR_D$ methods as novel approaches to enhancing the adversarial robustness of deep learning models. These methods are based on a new class of optimal-transport-regularized divergences, constructed via an infimal convolution between an information divergence and an optimal-transport (OT) cost. We use these as tools to enhance adversarial robustness by maximizing the expected loss over a neighborhood of distributions, a technique known as distributionally robust optimization. Viewed as a tool for constructing adversarial samples, our method allows samples to be both transported, according to the OT cost, and re-weighted, according to the information divergence. We demonstrate the effectiveness of our method on malware detection and image recognition applications and find that, to our knowledge, it outperforms existing methods at enhancing the robustness against adversarial attacks. $ARMOR_D$ yields the robustified accuracy of $98.29\%$ against $FGSM$ and $98.18\%$ aga
    
[^26]: 高能物理中的简化模拟：数据驱动物理研究的折中方法

    Reduced Simulations for High-Energy Physics, a Middle Ground for Data-Driven Physics Research. (arXiv:2309.03780v1 [hep-ex])

    [http://arxiv.org/abs/2309.03780](http://arxiv.org/abs/2309.03780)

    该论文提出了一种简化模拟的方法，通过使用红外探测器模型和粒子碰撞事件模拟器来生成简化数据，以便于高能物理研究和教育中的机器学习模型设计和解决方案探索。

    

    亚原子粒子轨迹重建（追踪）是高能物理实验中的关键任务。追踪任务在计算上极具挑战性，目前使用的传统算法解决方案不能线性扩展。机器学习辅助的解决方案是一个有前途的答案。我们认为，简化了的问题描述和所代表的数据将有助于解决方案的探索工作流程。我们提供了经过简化的虚拟探测器（REDVID）作为复杂度简化探测器模型和粒子碰撞事件模拟器的组合。REDVID旨在作为一个模拟-循环来高效生成合成数据，并简化机器学习模型设计的挑战。与物理精确模拟相比，我们工具的完全参数化特性允许生成不同层次的研究和教育简化数据。由于简化的复杂性，我们展示了

    Subatomic particle track reconstruction (tracking) is a vital task in High-Energy Physics experiments. Tracking is exceptionally computationally challenging and fielded solutions, relying on traditional algorithms, do not scale linearly. Machine Learning (ML) assisted solutions are a promising answer. We argue that a complexity-reduced problem description and the data representing it, will facilitate the solution exploration workflow. We provide the REDuced VIrtual Detector (REDVID) as a complexity-reduced detector model and particle collision event simulator combo. REDVID is intended as a simulation-in-the-loop, to both generate synthetic data efficiently and to simplify the challenge of ML model design. The fully parametric nature of our tool, with regards to system-level configuration, while in contrast to physics-accurate simulations, allows for the generation of simplified data for research and education, at different levels. Resulting from the reduced complexity, we showcase the 
    
[^27]: 基于时间编码的深度强化学习在嵌入式设备上进行实时应用的CPU频率调度

    CPU frequency scheduling of real-time applications on embedded devices with temporal encoding-based deep reinforcement learning. (arXiv:2309.03779v1 [cs.LG])

    [http://arxiv.org/abs/2309.03779](http://arxiv.org/abs/2309.03779)

    本研究通过基于时间编码的深度强化学习，开发了一种在嵌入式设备上进行实时应用的CPU频率调度方法，该方法可以在小型设备上推导出高效的功率管理方法，并解决了现有Linux内置方法的限制。

    

    小型设备经常用于物联网和智能城市应用，用于执行有软截止期的周期性专用任务。这项工作致力于开发在小型设备上推导出高效的功率管理方法的方法。我们首先研究了现有的Linux内置方法在小型设备中的限制。我们展示了三种典型的工作负荷/系统模式，这些模式对于Linux内置解决方案而言具有挑战性。我们使用时间编码开发了一种基于增强学习的技术，以推导出一种有效的DVFS调度程序，即使存在这三种系统模式。推导出的调度程序仅使用一个性能计数器，与内置的Linux机制相同，并且不需要工作负荷的显式任务模型。我们在Nvidia Jetson Nano Board上实现了一个原型系统，并进行了六种应用程序的实验，包括两个自设计的和四个基准应用程序。

    Small devices are frequently used in IoT and smart-city applications to perform periodic dedicated tasks with soft deadlines. This work focuses on developing methods to derive efficient power-management methods for periodic tasks on small devices. We first study the limitations of the existing Linux built-in methods used in small devices. We illustrate three typical workload/system patterns that are challenging to manage with Linux's built-in solutions. We develop a reinforcement-learning-based technique with temporal encoding to derive an effective DVFS governor even with the presence of the three system patterns. The derived governor uses only one performance counter, the same as the built-in Linux mechanism, and does not require an explicit task model for the workload. We implemented a prototype system on the Nvidia Jetson Nano Board and experimented with it with six applications, including two self-designed and four benchmark applications. Under different deadline constraints, our 
    
[^28]: 自动驾驶感知中的深度学习安全考虑

    Deep Learning Safety Concerns in Automated Driving Perception. (arXiv:2309.03774v1 [cs.LG])

    [http://arxiv.org/abs/2309.03774](http://arxiv.org/abs/2309.03774)

    本研究旨在通过引入安全考虑作为结构元素，以系统综合的方式确保基于深度神经网络的自动驾驶系统的安全性。这一概念不仅与现有的安全标准相契合，还为AI安全相关的学术出版物和标准提供了新的启示。

    

    深度学习领域的最新进展以及深度神经网络（DNNs）在感知方面的出色性能导致了对其在自动驾驶系统中应用的增加需求。这类系统的安全性至关重要，因此需要考虑DNNs的独特属性。为了以系统综合的方式确保基于DNNs的自动驾驶系统的安全性，引入了所谓的安全考虑作为适当的结构元素。一方面，安全考虑的概念设计与现有的与自动驾驶系统安全相关的标准如ISO 21448（SOTIF）非常契合。另一方面，它已经激发了几篇学术出版物和即将出台的关于AI安全的标准，如ISO PAS 8800。虽然安全考虑的概念以前已经被介绍过，但本文对其进行了扩展和优化，借鉴了各个领域和安全专家的反馈意见。

    Recent advances in the field of deep learning and impressive performance of deep neural networks (DNNs) for perception have resulted in an increased demand for their use in automated driving (AD) systems. The safety of such systems is of utmost importance and thus requires to consider the unique properties of DNNs.  In order to achieve safety of AD systems with DNN-based perception components in a systematic and comprehensive approach, so-called safety concerns have been introduced as a suitable structuring element. On the one hand, the concept of safety concerns is -- by design -- well aligned to existing standards relevant for safety of AD systems such as ISO 21448 (SOTIF). On the other hand, it has already inspired several academic publications and upcoming standards on AI safety such as ISO PAS 8800.  While the concept of safety concerns has been previously introduced, this paper extends and refines it, leveraging feedback from various domain and safety experts in the field. In par
    
[^29]: 神经套索：一种将套索和神经网络相结合的统一方法

    Neural lasso: a unifying approach of lasso and neural networks. (arXiv:2309.03770v1 [stat.ML])

    [http://arxiv.org/abs/2309.03770](http://arxiv.org/abs/2309.03770)

    本文提出了一种神经套索方法，将套索和神经网络相结合，通过模仿统计框架进行修改的方式，在变量选择中提供更准确的参数估计。

    

    近年来，将统计和机器学习领域的技术相结合以获得两种方法的优点已经成为研究热点。本文通过神经网络来表示变量选择的统计技术套索。观察发现，尽管统计方法和神经网络版本具有相同的目标函数，但由于优化方法不同而存在差异。特别是，神经网络版本通常使用单个验证集进行一步优化，而统计对应方法基于交叉验证进行两步优化。统计方法更为精细的优化导致更准确的参数估计，尤其是在训练集较小的情况下。因此，本文提出了一种修改标准神经网络训练方法的方法，模仿统计框架。在开发上述修改的过程中，提出了一种新的优化算法。

    In recent years, there is a growing interest in combining techniques attributed to the areas of Statistics and Machine Learning in order to obtain the benefits of both approaches. In this article, the statistical technique lasso for variable selection is represented through a neural network. It is observed that, although both the statistical approach and its neural version have the same objective function, they differ due to their optimization. In particular, the neural version is usually optimized in one-step using a single validation set, while the statistical counterpart uses a two-step optimization based on cross-validation. The more elaborated optimization of the statistical method results in more accurate parameter estimation, especially when the training set is small. For this reason, a modification of the standard approach for training neural networks, that mimics the statistical framework, is proposed. During the development of the above modification, a new optimization algori
    
[^30]: 使用运动模式来预测超声心动图的射血分数的研究

    M(otion)-mode Based Prediction of Ejection Fraction using Echocardiograms. (arXiv:2309.03759v1 [eess.IV])

    [http://arxiv.org/abs/2309.03759](http://arxiv.org/abs/2309.03759)

    本研究提出了一种使用超声心动图的运动模式来估计射血分数和分类心肌病的方法。通过生成多个人工M模式图像并利用对比学习，该方法能够实现高精度的自动化评估，节省了时间和专业知识要求。

    

    通过定期筛查尽早检测心脏功能障碍对诊断心血管疾病至关重要。射血分数（EF）是衡量心脏功能的重要指标，较低的EF与心肌病相关。超声心动图是心脏病学中常用的诊断工具，超声是一种低成本、实时、无辐射的技术。然而，人工评估超声心动图以计算EF耗时且需要专业知识，因此需要自动化的方法。在本研究中，我们提出使用超声心动图的M(otion)-mode来估计EF和分类心肌病。我们从单个超声心动图生成多个人工M模式图像，并使用现成的模型架构进行组合。此外，我们将对比学习（CL）扩展到心脏成像中，从未标记数据中利用结构来学习有意义的表示，使得模型能够实现高精度，即使只有有限的标记数据。

    Early detection of cardiac dysfunction through routine screening is vital for diagnosing cardiovascular diseases. An important metric of cardiac function is the left ventricular ejection fraction (EF), where lower EF is associated with cardiomyopathy. Echocardiography is a popular diagnostic tool in cardiology, with ultrasound being a low-cost, real-time, and non-ionizing technology. However, human assessment of echocardiograms for calculating EF is time-consuming and expertise-demanding, raising the need for an automated approach. In this work, we propose using the M(otion)-mode of echocardiograms for estimating the EF and classifying cardiomyopathy. We generate multiple artificial M-mode images from a single echocardiogram and combine them using off-the-shelf model architectures. Additionally, we extend contrastive learning (CL) to cardiac imaging to learn meaningful representations from exploiting structures in unlabeled data allowing the model to achieve high accuracy, even with li
    
[^31]: TSGBench：时间序列生成基准

    TSGBench: Time Series Generation Benchmark. (arXiv:2309.03755v1 [cs.LG])

    [http://arxiv.org/abs/2309.03755](http://arxiv.org/abs/2309.03755)

    TSGBench是首个时间序列生成基准，用于统一和全面评估TSG方法，解决了现有方法在性能评估、数据集选择和评估指标上的限制。

    

    合成时间序列生成(TSG)在数据增强、异常检测和隐私保护等多个应用中至关重要。尽管在这个领域取得了重大进展，但现有方法存在三个关键限制：(1)它们经常针对类似的模型类型进行基准测试，限制了对性能能力的整体视角。(2)使用专门的合成和私有数据集引入了偏倚，阻碍了泛化能力。(3)模糊的评估指标，往往与自定义网络或下游任务相结合，阻碍了一致和公平的比较。为了克服这些限制，我们推出了\textsf {TSGBench}，作为首个TSG基准，旨在统一和全面评估TSG方法。它包括三个模块：(1)一个精心策划的、面向TSG的公开实际数据集收集，以及标准化的预处理流程；(2)一套综合的评估指标套件，包括基本指标

    Synthetic Time Series Generation (TSG) is crucial in a range of applications, including data augmentation, anomaly detection, and privacy preservation. Although significant strides have been made in this field, existing methods exhibit three key limitations: (1) They often benchmark against similar model types, constraining a holistic view of performance capabilities. (2) The use of specialized synthetic and private datasets introduces biases and hampers generalizability. (3) Ambiguous evaluation measures, often tied to custom networks or downstream tasks, hinder consistent and fair comparison.  To overcome these limitations, we introduce \textsf{TSGBench}, the inaugural TSG Benchmark, designed for a unified and comprehensive assessment of TSG methods. It comprises three modules: (1) a curated collection of publicly available, real-world datasets tailored for TSG, together with a standardized preprocessing pipeline; (2) a comprehensive evaluation measures suite including vanilla measur
    
[^32]: 分散式ASGD的收敛分析

    Convergence Analysis of Decentralized ASGD. (arXiv:2309.03754v1 [cs.LG])

    [http://arxiv.org/abs/2309.03754](http://arxiv.org/abs/2309.03754)

    本文提出了一个不需要部分同步和限制性网络拓扑结构的分散式和异步SGD（DASGD）的收敛速度分析方法。

    

    在过去的几十年里，随机梯度下降（SGD）一直受到机器学习界的广泛研究。尽管其具有多样性和出色的性能，但通过SGD优化大模型仍然是一项耗时的任务。为了缩短训练时间，常常将训练过程分布在多个设备上。最近已经证明，异步SGD（ASGD）的收敛速度总是比小批量SGD快。然而，尽管在理论上的改善，大多数ASGD收敛速度的证明仍然依赖于一个集中式参数服务器，在将梯度计算扩展到许多分布式进程时容易成为瓶颈。本文介绍了一种新颖的分散式异步SGD（DASGD）的收敛速度分析方法，该方法不需要节点之间的部分同步，也不需要限制性的网络拓扑结构。

    Over the last decades, Stochastic Gradient Descent (SGD) has been intensively studied by the Machine Learning community. Despite its versatility and excellent performance, the optimization of large models via SGD still is a time-consuming task. To reduce training time, it is common to distribute the training process across multiple devices. Recently, it has been shown that the convergence of asynchronous SGD (ASGD) will always be faster than mini-batch SGD. However, despite these improvements in the theoretical bounds, most ASGD convergence-rate proofs still rely on a centralized parameter server, which is prone to become a bottleneck when scaling out the gradient computations across many distributed processes.  In this paper, we present a novel convergence-rate analysis for decentralized and asynchronous SGD (DASGD) which does not require partial synchronization among nodes nor restrictive network topologies. Specifically, we provide a bound of $\mathcal{O}(\sigma\epsilon^{-2}) + \mat
    
[^33]: 具有自动聚类数量选择的中心轮廓聚类

    Medoid Silhouette clustering with automatic cluster number selection. (arXiv:2309.03751v1 [cs.LG])

    [http://arxiv.org/abs/2309.03751](http://arxiv.org/abs/2309.03751)

    本文介绍了一种具有自动聚类数量选择的中心轮廓聚类算法，其中结合了原始轮廓系数和PAM算法的思想，并通过两种快速版本进行了理论分析和实验验证，在实验中获得了显著的加速效果。

    

    聚类结果的评估是困难的，高度依赖于评估的数据集和观察者的观点。有许多不同的聚类质量度量，试图提供一个通用的度量来验证聚类结果。一个非常流行的度量是轮廓系数。我们讨论了高效的基于中心点的轮廓系数，对其属性进行了理论分析，提供了两个直接优化的快速版本，并讨论了选择最佳聚类数量的使用。我们将原始轮廓系数和著名的PAM算法以及其最新改进FasterPAM的思想相结合。其中一个版本保证与原始版本相等的结果，并提供了$O(k^2)$的运行加速。在具有30000个样本和$k$=100的实际数据上，与原始的PAMMEDSIL算法相比，我们观察到了10464倍的加速。此外，我们提供了一种直接选择最佳聚类数量的变种方法。

    The evaluation of clustering results is difficult, highly dependent on the evaluated data set and the perspective of the beholder. There are many different clustering quality measures, which try to provide a general measure to validate clustering results. A very popular measure is the Silhouette. We discuss the efficient medoid-based variant of the Silhouette, perform a theoretical analysis of its properties, provide two fast versions for the direct optimization, and discuss the use to choose the optimal number of clusters. We combine ideas from the original Silhouette with the well-known PAM algorithm and its latest improvements FasterPAM. One of the versions guarantees equal results to the original variant and provides a run speedup of $O(k^2)$. In experiments on real data with 30000 samples and $k$=100, we observed a 10464$\times$ speedup compared to the original PAMMEDSIL algorithm. Additionally, we provide a variant to choose the optimal number of clusters directly.
    
[^34]: 使用大型语言模型增强基于流水线的对话系统

    Enhancing Pipeline-Based Conversational Agents with Large Language Models. (arXiv:2309.03748v1 [cs.CL])

    [http://arxiv.org/abs/2309.03748](http://arxiv.org/abs/2309.03748)

    本文研究了如何使用大型语言模型（LLM）来增强基于流水线的对话系统。在设计和开发阶段，LLM可以帮助生成训练数据、提取实体和同义词、本地化和角色设计。在运营阶段，LLM可以辅助上下文化、意图分类、自动纠正话语、改写回复、摘要和使闭合问题回答能力。通过在私人银行领域的实验，证明了这些能力的有效性。

    

    AI和深度学习的最新进展使得基于大型语言模型（LLM）的代理器（如GPT-4）取得了突破。然而，许多商业化对话系统开发工具是基于流水线的，并且在进行人类对话时存在限制。本文研究了LLM在以下两个阶段中增强基于流水线的对话系统的能力：1）设计和开发阶段；2）运营阶段。在1）中，LLM可以在生成训练数据、提取实体和同义词、本地化和角色设计方面提供帮助。在2）中，LLM可以辅助上下文化、意图分类以防止对话中断和处理超出范围的问题、自动纠正话语、改写回复、制定消歧问句、摘要和使闭合问题回答能力。我们在私人银行领域进行了使用GPT-4的非正式实验，以实际示例证明上述情景。

    The latest advancements in AI and deep learning have led to a breakthrough in large language model (LLM)-based agents such as GPT-4. However, many commercial conversational agent development tools are pipeline-based and have limitations in holding a human-like conversation. This paper investigates the capabilities of LLMs to enhance pipeline-based conversational agents during two phases: 1) in the design and development phase and 2) during operations. In 1) LLMs can aid in generating training data, extracting entities and synonyms, localization, and persona design. In 2) LLMs can assist in contextualization, intent classification to prevent conversational breakdown and handle out-of-scope questions, auto-correcting utterances, rephrasing responses, formulating disambiguation questions, summarization, and enabling closed question-answering capabilities. We conducted informal experiments with GPT-4 in the private banking domain to demonstrate the scenarios above with a practical example.
    
[^35]: 通过表示平衡学习连续值治疗效果

    Learning continuous-valued treatment effects through representation balancing. (arXiv:2309.03731v1 [cs.LG])

    [http://arxiv.org/abs/2309.03731](http://arxiv.org/abs/2309.03731)

    本研究提出了CBRNet，一种通过表示平衡学习连续值治疗效果的因果机器学习方法。

    

    在医疗、商业、经济等领域，估计与治疗剂量相关的治疗效果（即“剂量反应”）非常重要。然而，这些连续值治疗效果通常是从观测数据中估计得到的，而观测数据可能存在剂量选择偏差，即剂量分配受到预处理协变量的影响。以前的研究表明，传统的机器学习方法在存在剂量选择偏差的情况下无法准确学习到个体治疗效果的估计。我们提出了一种名为CBRNet的因果机器学习方法，用于从观测数据中估计个体的剂量反应。CBRNet采用了Neyman-Rubin潜在结果框架，并扩展了平衡表示学习的概念，以克服连续值治疗中的选择偏差。我们的工作是第一个在连续值治疗中应用表示平衡的研究。

    Estimating the effects of treatments with an associated dose on an instance's outcome, the "dose response", is relevant in a variety of domains, from healthcare to business, economics, and beyond. Such effects, also known as continuous-valued treatment effects, are typically estimated from observational data, which may be subject to dose selection bias. This means that the allocation of doses depends on pre-treatment covariates. Previous studies have shown that conventional machine learning approaches fail to learn accurate individual estimates of dose responses under the presence of dose selection bias. In this work, we propose CBRNet, a causal machine learning approach to estimate an individual dose response from observational data. CBRNet adopts the Neyman-Rubin potential outcome framework and extends the concept of balanced representation learning for overcoming selection bias to continuous-valued treatments. Our work is the first to apply representation balancing in a continuous-v
    
[^36]: 贷款定价的因果视角: 探讨选择偏差对辨识出价响应函数的影响

    A Causal Perspective on Loan Pricing: Investigating the Impacts of Selection Bias on Identifying Bid-Response Functions. (arXiv:2309.03730v1 [cs.LG])

    [http://arxiv.org/abs/2309.03730](http://arxiv.org/abs/2309.03730)

    这项研究通过将定价问题视为因果推断问题，首次对选择偏差对贷款定价的影响进行了探讨。实验结果揭示了传统方法在处理选择偏差问题上的局限性，并提出了更全面的解决方案。

    

    在贷款领域，价格既与客户有关又与产品有关，确保一个良好运作的个性化定价策略对业务决策至关重要。通常情况下，此类策略必须基于观测数据推导而来，这会引入一些挑战。虽然“内生性”问题在现有的定价文献中得到了广泛研究，但选择偏差问题（或者更准确地说是出价选择偏差问题）还未得到重视。我们从因果推断的角度探讨选择偏差的影响。具体而言，我们将定价问题视为一个因果推断问题，考虑客户对价格的反应作为一种治疗效应。在实验中，我们通过在比利时的抵押贷款申请半合成数据集上模拟不同程度的选择偏差，研究参数化和非参数化方法在识别个体出价响应函数方面的潜力。我们的结果展示了传统方法（如逻辑回归）在处理选择偏差问题上的局限性，并提出了更全面的解决方案。

    In lending, where prices are specific to both customers and products, having a well-functioning personalized pricing policy in place is essential to effective business making. Typically, such a policy must be derived from observational data, which introduces several challenges. While the problem of ``endogeneity'' is prominently studied in the established pricing literature, the problem of selection bias (or, more precisely, bid selection bias) is not. We take a step towards understanding the effects of selection bias by posing pricing as a problem of causal inference. Specifically, we consider the reaction of a customer to price a treatment effect. In our experiments, we simulate varying levels of selection bias on a semi-synthetic dataset on mortgage loan applications in Belgium. We investigate the potential of parametric and nonparametric methods for the identification of individual bid-response functions. Our results illustrate how conventional methods such as logistic regression a
    
[^37]: 基于Hoeffding树和变点检测机制的连续学习场景下的天然气消费预测系统

    A Natural Gas Consumption Forecasting System for Continual Learning Scenarios based on Hoeffding Trees with Change Point Detection Mechanism. (arXiv:2309.03720v1 [cs.LG])

    [http://arxiv.org/abs/2309.03720](http://arxiv.org/abs/2309.03720)

    本文介绍了一个基于Hoeffding树和变点检测机制的连续学习场景下的天然气消费预测系统，通过数据流处理，实现了多步 ahead 的预测和持续学习能力。在复杂的实际应用场景中，通过评估预测模型的性能，证明了该方法的有效性。

    

    在规划天然气供应和消费以及优化获得天然气成本方面，考虑季节性和趋势性的天然气消费预测至关重要。本文介绍了一种新颖的多步 ahead 的天然气消费预测方法，并集成了变点检测，以实现模型选择和持续学习能力。通过数据流处理，评估了基于该方法的天然气消费预测模型在复杂的实际应用场景中的性能。我们采用Hoeffding树预测器作为预测模型，并使用剪裁的精确线性时间（PELT）算法进行变点检测。变点检测集成使得选择不同的模型成为可能。

    Forecasting natural gas consumption, considering seasonality and trends, is crucial in planning its supply and consumption and optimizing the cost of obtaining it, mainly by industrial entities. However, in times of threats to its supply, it is also a critical element that guarantees the supply of this raw material to meet individual consumers' needs, ensuring society's energy security. This article introduces a novel multistep ahead forecasting of natural gas consumption with change point detection integration for model collection selection with continual learning capabilities using data stream processing. The performance of the forecasting models based on the proposed approach is evaluated in a complex real-world use case of natural gas consumption forecasting. We employed Hoeffding tree predictors as forecasting models and the Pruned Exact Linear Time (PELT) algorithm for the change point detection procedure. The change point detection integration enables selecting a different model
    
[^38]: 一种递减奖励的状态表示

    A State Representation for Diminishing Rewards. (arXiv:2309.03710v1 [cs.LG])

    [http://arxiv.org/abs/2309.03710](http://arxiv.org/abs/2309.03710)

    该论文研究了多任务强化学习中存在的递减边际效用现象，并引入了一种名为$\lambda$表示（$\lambda$R）的新型状态表示，用于快速策略评估，该表示能够推广已有的状态表示并具备一些正式属性。

    

    多任务强化学习中常见的情景要求代理快速适应从固定分布中随机采样的各种静态奖励函数。在这种情况下，后继状态表示（SR）是一个流行的框架，通过将策略的预期折扣累积状态分布与特定奖励函数解耦，支持快速策略评估。然而，在自然界中，顺序任务很少是独立的，而是基于奖励刺激的可用性和主观感知反映出不断变化的优先级。为了反映这种不协调，本文研究了递减边际效用的现象，并引入了一种新颖的状态表示，称为$\lambda$表示（$\lambda$R），令人惊讶的是，在该设置中需要用于策略评估，并且可以推广SR以及文献中的其他几种状态表示。我们建立了$\lambda$R的正式属性并研究了它的性能。

    A common setting in multitask reinforcement learning (RL) demands that an agent rapidly adapt to various stationary reward functions randomly sampled from a fixed distribution. In such situations, the successor representation (SR) is a popular framework which supports rapid policy evaluation by decoupling a policy's expected discounted, cumulative state occupancies from a specific reward function. However, in the natural world, sequential tasks are rarely independent, and instead reflect shifting priorities based on the availability and subjective perception of rewarding stimuli. Reflecting this disjunction, in this paper we study the phenomenon of diminishing marginal utility and introduce a novel state representation, the $\lambda$ representation ($\lambda$R) which, surprisingly, is required for policy evaluation in this setting and which generalizes the SR as well as several other state representations from the literature. We establish the $\lambda$R's formal properties and examine 
    
[^39]: 聊天失败和问题：原因和解决方案

    Chat Failures and Troubles: Reasons and Solutions. (arXiv:2309.03708v1 [cs.RO])

    [http://arxiv.org/abs/2309.03708](http://arxiv.org/abs/2309.03708)

    本文研究了人机交互中聊天失败和问题的原因，提出了闭环控制算法和强化学习模型等解决方案以降低错误。

    

    本文研究了人机交互中聊天导致失败和问题的一些常见问题。一个给定的用例的设计决策始于选择合适的机器人、合适的聊天模型，识别导致失败的常见问题，找出潜在的解决方案，并计划持续改进。总结起来，建议使用闭环控制算法来引导训练好的人工智能预训练模型的使用，并提供词汇过滤、对新数据集重新训练批次模型、在线学习数据流以及/或使用强化学习模型来自我更新训练模型以降低错误。

    This paper examines some common problems in Human-Robot Interaction (HRI) causing failures and troubles in Chat. A given use case's design decisions start with the suitable robot, the suitable chatting model, identifying common problems that cause failures, identifying potential solutions, and planning continuous improvement. In conclusion, it is recommended to use a closed-loop control algorithm that guides the use of trained Artificial Intelligence (AI) pre-trained models and provides vocabulary filtering, re-train batched models on new datasets, learn online from data streams, and/or use reinforcement learning models to self-update the trained models and reduce errors.
    
[^40]: 一种基于三元马尔科夫链的概率半监督方法

    A Probabilistic Semi-Supervised Approach with Triplet Markov Chains. (arXiv:2309.03707v1 [stat.ML])

    [http://arxiv.org/abs/2309.03707](http://arxiv.org/abs/2309.03707)

    本文提出了一种基于变分贝叶斯推断的通用框架，用于在半监督的情况下训练参数化的三元马尔科夫链模型，并且能够针对各种生成模型推导出半监督算法，用于顺序贝叶斯分类。

    

    三元马尔科夫链是用于顺序数据的一种通用生成模型，考虑了三种随机变量：（带噪）观测值、它们相关的离散标签和旨在增强观测值及其相关标签分布的潜在变量。然而，在实际应用中，我们没有所有与观测值相关的标签来估计这种模型的参数。本文提出了一种基于变分贝叶斯推断的通用框架，用于在半监督的情况下训练参数化的三元马尔科夫链模型。我们的方法的普适性使我们能够针对各种生成模型推导出半监督算法，用于顺序贝叶斯分类。

    Triplet Markov chains are general generative models for sequential data which take into account three kinds of random variables: (noisy) observations, their associated discrete labels and latent variables which aim at strengthening the distribution of the observations and their associated labels. However, in practice, we do not have at our disposal all the labels associated to the observations to estimate the parameters of such models. In this paper, we propose a general framework based on a variational Bayesian inference to train parameterized triplet Markov chain models in a semi-supervised context. The generality of our approach enables us to derive semi-supervised algorithms for a variety of generative models for sequential Bayesian classification.
    
[^41]: DiffDefense：通过扩散模型防御对抗攻击

    DiffDefense: Defending against Adversarial Attacks via Diffusion Models. (arXiv:2309.03702v1 [cs.LG])

    [http://arxiv.org/abs/2309.03702](http://arxiv.org/abs/2309.03702)

    本文提出了一种新的重构方法，利用扩散模型保护机器学习分类器免受对抗攻击的影响。该方法在保持准确性、速度和兼容性的同时提供了对抗威胁的鲁棒性。

    

    本文提出了一种新颖的重建方法，利用扩散模型保护机器学习分类器免受对抗攻击的影响，而无需对分类器进行任何修改。机器学习模型对微小的输入扰动的敏感性使它们容易受到对抗攻击的威胁。尽管基于扩散的方法通常因其缓慢的反向过程而被忽视，但本文证明了我们提出的方法在防御对抗威胁的同时保持了干净的准确性、速度和即插即用的兼容性。

    This paper presents a novel reconstruction method that leverages Diffusion Models to protect machine learning classifiers against adversarial attacks, all without requiring any modifications to the classifiers themselves. The susceptibility of machine learning models to minor input perturbations renders them vulnerable to adversarial attacks. While diffusion-based methods are typically disregarded for adversarial defense due to their slow reverse process, this paper demonstrates that our proposed method offers robustness against adversarial threats while preserving clean accuracy, speed, and plug-and-play compatibility. Code at: https://github.com/HondamunigePrasannaSilva/DiffDefence.
    
[^42]: 使用粒子群优化的多头注意力增强的CNN-LSTM网络进行短期负荷预测

    Short-Term Load Forecasting Using A Particle-Swarm Optimized Multi-Head Attention-Augmented CNN-LSTM Network. (arXiv:2309.03694v1 [cs.LG])

    [http://arxiv.org/abs/2309.03694](http://arxiv.org/abs/2309.03694)

    这项研究提出了一种使用粒子群优化算法、多头注意力机制和计算效率优化框架相结合的方法，用于解决短期负荷预测中的超参数敏感性、解释性不透明和高计算开销等问题。结果表明，该方法在准确性、鲁棒性和计算效率方面具有优势。

    

    短期负荷预测对于电力系统的高效运行和规划至关重要，给定其固有的非线性和动态特性。深度学习在解决这一挑战方面取得了一些进展。然而，这些方法通常面临超参数敏感性、解释性不透明和实时部署的高计算开销等问题。在本文中，我提出了一种克服这些障碍的新方法。我们的方法利用粒子群优化算法自主地探索和优化超参数，利用多头注意力机制识别对准确预测至关重要的显著特征，并采用简化的框架实现计算效率。我们的方法使用真实电力需求数据集进行了严格的评估。结果突显了其在准确性、鲁棒性和计算效率方面的优势。值得注意的是，我们的平均绝对百分比误差...

    Short-term load forecasting is of paramount importance in the efficient operation and planning of power systems, given its inherent non-linear and dynamic nature. Recent strides in deep learning have shown promise in addressing this challenge. However, these methods often grapple with hyperparameter sensitivity, opaqueness in interpretability, and high computational overhead for real-time deployment. In this paper, I propose a novel solution that surmounts these obstacles. Our approach harnesses the power of the Particle-Swarm Optimization algorithm to autonomously explore and optimize hyperparameters, a Multi-Head Attention mechanism to discern the salient features crucial for accurate forecasting, and a streamlined framework for computational efficiency. Our method undergoes rigorous evaluation using a genuine electricity demand dataset. The results underscore its superiority in terms of accuracy, robustness, and computational efficiency. Notably, our Mean Absolute Percentage Error o
    
[^43]: 一种计算轻量级的安全学习算法

    A computationally lightweight safe learning algorithm. (arXiv:2309.03672v1 [eess.SY])

    [http://arxiv.org/abs/2309.03672](http://arxiv.org/abs/2309.03672)

    本文提出了一种计算轻量级的安全学习算法，该算法利用Nadaraya-Watson估计器提供概率安全保证，并解决了高斯过程推理在高维和嵌入式系统中计算复杂度难题。

    

    在学习物理系统的控制策略时，安全是一个必不可少的因素，因为在训练过程中违反安全约束可能导致昂贵的硬件损坏。为了满足这种需求，安全学习领域涌现出了一些算法，可以在不知道底层系统动态的情况下提供概率安全保证。这些算法通常依赖于高斯过程推理。然而，高斯过程推理的计算复杂度随着数据点数量的增加呈立方增长，限制了其在高维和嵌入式系统上的适用性。本文提出了一种安全学习算法，可以提供概率安全保证，并利用了Nadaraya-Watson估计器代替高斯过程。对于Nadaraya-Watson估计器，我们可以实现与数据点数量的对数比例的计算复杂度。我们为估计器提供了理论保证，将其嵌入到安全学习算法中，并在一个模拟的七自由度系统上进行了数值实验。

    Safety is an essential asset when learning control policies for physical systems, as violating safety constraints during training can lead to expensive hardware damage. In response to this need, the field of safe learning has emerged with algorithms that can provide probabilistic safety guarantees without knowledge of the underlying system dynamics. Those algorithms often rely on Gaussian process inference. Unfortunately, Gaussian process inference scales cubically with the number of data points, limiting applicability to high-dimensional and embedded systems. In this paper, we propose a safe learning algorithm that provides probabilistic safety guarantees but leverages the Nadaraya-Watson estimator instead of Gaussian processes. For the Nadaraya-Watson estimator, we can reach logarithmic scaling with the number of data points. We provide theoretical guarantees for the estimates, embed them into a safe learning algorithm, and show numerical experiments on a simulated seven-degrees-of-f
    
[^44]: 从弱标记视频中生成数据集并进行倭黑猩猩分类的研究

    Dataset Generation and Bonobo Classification from Weakly Labelled Videos. (arXiv:2309.03671v1 [cs.CV])

    [http://arxiv.org/abs/2309.03671](http://arxiv.org/abs/2309.03671)

    本研究提出了一种从弱标记视频中生成倭黑猩猩数据集并进行分类的方法，并探究了不同的特征提取和分类算法。研究结果表明，数据准备的重要性以及正确的数据分离对于分类性能的影响很大。

    

    本文介绍了一个基于常用机器学习方法构建的倭黑猩猩检测和分类流程。该应用的动机是为了在没有人的帮助下，使用触摸屏设备对倭黑猩猩在它们的围栏中进行测试。该研究引入了一个新获得的数据集，该数据集是基于倭黑猩猩录像的自动产生的。这些录像是弱标记的，并通过猕猴检测器进行空间检测，以检测视频中出现的个体。使用手工特征以及不同的分类算法和基于ResNet架构的深度学习方法进行倭黑猩猩识别的研究。使用不同的数据分离方法，在数据库的拆分上，以分类准确度作为性能指标进行比较。我们证明了数据准备的重要性以及错误的数据分离如何导致虚假的好结果。最后，经过有意义的数据分离后，得到了最佳的分类性能。

    This paper presents a bonobo detection and classification pipeline built from the commonly used machine learning methods. Such application is motivated by the need to test bonobos in their enclosure using touch screen devices without human assistance. This work introduces a newly acquired dataset based on bonobo recordings generated semi-automatically. The recordings are weakly labelled and fed to a macaque detector in order to spatially detect the individual present in the video. Handcrafted features coupled with different classification algorithms and deep-learning methods using a ResNet architecture are investigated for bonobo identification. Performance is compared in terms of classification accuracy on the splits of the database using different data separation methods. We demonstrate the importance of data preparation and how a wrong data separation can lead to false good results. Finally, after a meaningful separation of the data, the best classification performance is obtained u
    
[^45]: 如何攻击可以干扰看似稳定准确的分类器

    How adversarial attacks can disrupt seemingly stable accurate classifiers. (arXiv:2309.03665v1 [cs.LG])

    [http://arxiv.org/abs/2309.03665](http://arxiv.org/abs/2309.03665)

    本文研究了对抗性攻击如何通过微小修改干扰准确的分类器，并发现这可能是高维输入数据下分类器的基本特征。作者提出了一个通用的框架，解释了实际系统中观察到的关键行为，包括模型对对抗性攻击的容易受到影响，同时对随机扰动具有鲁棒性。验证实验还表明了相同现象在实际神经网络中的存在。

    

    对抗性攻击通过对输入数据进行微小的修改，极大地改变了原本准确的学习系统的输出。具有讽刺意味的是，经验证据表明，即使系统对输入数据的大幅度随机扰动具有鲁棒性，它们仍然容易受到输入数据的小众、易于构造的对抗性扰动的影响。在这里，我们展示了这可能是高维输入数据下分类器的一个基本特征。我们引入了一个简单的通用性和普适性框架，其中在实际系统中观察到的关键行为具有高概率出现，尤其是（原本准确的）模型对易于构造的对抗性攻击的同时容易受到输入数据的随机扰动的影响。我们在标准图像分类问题上验证了相同现象在实际神经网络中的直接观察结果，即使是大幅度的加性随机噪声也无法干扰模型的准确性。

    Adversarial attacks dramatically change the output of an otherwise accurate learning system using a seemingly inconsequential modification to a piece of input data. Paradoxically, empirical evidence indicates that even systems which are robust to large random perturbations of the input data remain susceptible to small, easily constructed, adversarial perturbations of their inputs. Here, we show that this may be seen as a fundamental feature of classifiers working with high dimensional input data. We introduce a simple generic and generalisable framework for which key behaviours observed in practical systems arise with high probability -- notably the simultaneous susceptibility of the (otherwise accurate) model to easily constructed adversarial attacks, and robustness to random perturbations of the input data. We confirm that the same phenomena are directly observed in practical neural networks trained on standard image classification problems, where even large additive random noise fai
    
[^46]: 通过拓扑机器学习从脑脊液的拉曼光谱检测阿尔茨海默病

    Alzheimer Disease Detection from Raman Spectroscopy of the Cerebrospinal Fluid via Topological Machine Learning. (arXiv:2309.03664v1 [cs.LG])

    [http://arxiv.org/abs/2309.03664](http://arxiv.org/abs/2309.03664)

    本研究提出了一种使用拉曼光谱和拓扑分析相结合的方法，可以有效地确认或否定阿尔茨海默病的临床诊断。

    

    本研究使用拉曼光谱分析了19名临床诊断为阿尔茨海默病（AD）和5名病理对照者的脑脊液（CSF）。我们研究了原始和预处理拉曼光谱是否可以用于区分AD和对照组。首先，我们应用常规机器学习方法得到了不理想的结果。然后，我们将机器学习应用于从原始光谱提取的拓扑描述符集合，实现了非常好的分类准确率（>87%）。虽然我们的结果是初步的，但它们表明拉曼光谱和拓扑分析的结合可能提供一种有效的方法来确认或否定AD的临床诊断。下一步将包括扩大CSF样本数据集以更好地验证所提出的方法，并可能了解拓扑数据分析是否能支持AD亚型的表征。

    The cerebrospinal fluid (CSF) of 19 subjects who received a clinical diagnosis of Alzheimer's disease (AD) as well as of 5 pathological controls have been collected and analysed by Raman spectroscopy (RS). We investigated whether the raw and preprocessed Raman spectra could be used to distinguish AD from controls. First, we applied standard Machine Learning (ML) methods obtaining unsatisfactory results. Then, we applied ML to a set of topological descriptors extracted from raw spectra, achieving a very good classification accuracy (>87%). Although our results are preliminary, they indicate that RS and topological analysis together may provide an effective combination to confirm or disprove a clinical diagnosis of AD. The next steps will include enlarging the dataset of CSF samples to validate the proposed method better and, possibly, to understand if topological data analysis could support the characterization of AD subtypes.
    
[^47]: 在语义图像分割中实现可比较的知识蒸馏

    Towards Comparable Knowledge Distillation in Semantic Image Segmentation. (arXiv:2309.03659v1 [cs.CV])

    [http://arxiv.org/abs/2309.03659](http://arxiv.org/abs/2309.03659)

    在这项研究中，我们探索了语义图像分割中知识蒸馏的问题。我们发现了25种蒸馏损失项，并指出由于训练配置的差异导致术语比较困难。此外，我们发现超参数选择不当会导致极端的性能差异。

    

    知识蒸馏（KD）是解决语义分割中大模型尺寸和慢推理速度的一种提出的解决方案。在我们的研究中，我们从过去4年的14个出版物中鉴定出了25个提出的蒸馏损失项。不幸的是，基于已发布结果的术语比较通常是不可能的，因为训练配置的差异。这个问题的一个很好的例子是对比2022年的两个出版物。使用相同的模型和数据集，结构和统计纹理蒸馏（SSTKD）报告了学生mIoU增加了4.54个百分点，最终性能达到了29.19，而自适应透视蒸馏（APD）仅仅提高了学生性能2.06个百分点，但实现了39.25的最终性能。这种极端差异的原因通常是超参数的次优选择以及作为参考点的学生模型性能不佳。在我们的工作中，我们揭示了超参数不足的问题。

    Knowledge Distillation (KD) is one proposed solution to large model sizes and slow inference speed in semantic segmentation. In our research we identify 25 proposed distillation loss terms from 14 publications in the last 4 years. Unfortunately, a comparison of terms based on published results is often impossible, because of differences in training configurations. A good illustration of this problem is the comparison of two publications from 2022. Using the same models and dataset, Structural and Statistical Texture Distillation (SSTKD) reports an increase of student mIoU of 4.54 and a final performance of 29.19, while Adaptive Perspective Distillation (APD) only improves student performance by 2.06 percentage points, but achieves a final performance of 39.25. The reason for such extreme differences is often a suboptimal choice of hyperparameters and a resulting underperformance of the student model used as reference point. In our work, we reveal problems of insufficient hyperparameter
    
[^48]: GNN对公平性稳定性的Lipschitz特性表征

    Characterizing Lipschitz Stability of GNN for Fairness. (arXiv:2309.03648v1 [cs.LG])

    [http://arxiv.org/abs/2309.03648](http://arxiv.org/abs/2309.03648)

    论文通过研究GNN的Lipschitz界限表征了GNN对公平性稳定性的影响，尤其是在处理非欧几里得数据和固有偏倚的情况下。同时，该研究对于限制GNN输出的扰动以保障公平性训练提出了挑战。

    

    Lipschitz界限是从鲁棒统计学中借鉴的一种技术，可以限制输出相对于输入的最大变化，考虑到相关的非关键偏倚因素。这是一种高效且可证明的方法，可以检查机器学习模型的输出稳定性，而不会增加额外的计算成本。最近，对于在非欧几里得数据上操作的图神经网络（GNN）引起了广泛的关注。然而，之前没有研究调查GNN的Lipschitz界限以揭示模型输出的稳定性，特别是在处理具有固有偏倚的非欧几里得数据时。由于常见图形数据在GNN训练中存在固有偏差，这给限制由输入偏差引起的GNN输出扰动，从而在训练期间保障公平性，带来了严峻的挑战。最近，尽管Lipschitz常数在控制欧几里得神经网络的稳定性方面有所应用，但精确Lipschitz常数的计算十分困难。

    The Lipschitz bound, a technique from robust statistics, can limit the maximum changes in the output concerning the input, taking into account associated irrelevant biased factors. It is an efficient and provable method for examining the output stability of machine learning models without incurring additional computation costs. Recently, Graph Neural Networks (GNNs), which operate on non-Euclidean data, have gained significant attention. However, no previous research has investigated the GNN Lipschitz bounds to shed light on stabilizing model outputs, especially when working on non-Euclidean data with inherent biases. Given the inherent biases in common graph data used for GNN training, it poses a serious challenge to constraining the GNN output perturbations induced by input biases, thereby safeguarding fairness during training. Recently, despite the Lipschitz constant's use in controlling the stability of Euclideanneural networks, the calculation of the precise Lipschitz constant rem
    
[^49]: 对于蛋白功能预测中Transformer模型内部运作的洞察

    Insights Into the Inner Workings of Transformer Models for Protein Function Prediction. (arXiv:2309.03631v1 [cs.LG])

    [http://arxiv.org/abs/2309.03631](http://arxiv.org/abs/2309.03631)

    本研究通过扩展可解释性人工智能方法，探索了Transformer模型在蛋白质功能预测中的内部运作，并成功识别出了与生物学和化学相关的序列部分，为蛋白质研究提供了重要线索。

    

    动机：我们探索了可解释性人工智能（XAI）如何帮助揭示神经网络用于蛋白质功能预测的内部运作，通过扩展广泛使用的XAI方法——集成梯度，使其能够检查调整为基因本体术语和酶委员会编号预测的Transformer模型内的潜在表示。结果：该方法使我们能够识别出变压器在序列中特别关注的氨基酸，并展示这些相关的序列部分反映了生物学和化学的预期，无论是在嵌入层还是模型内部。我们确定了变压器头与地面真实序列注释（例如，跨膜区域，活性位点）之间具有统计显著对应的归因图的变压器头，这在多个蛋白质中都有出现。代码可在https://github.com/markuswenzel/xai-proteins 上获取和实施。

    Motivation: We explored how explainable AI (XAI) can help to shed light into the inner workings of neural networks for protein function prediction, by extending the widely used XAI method of integrated gradients such that latent representations inside of transformer models, which were finetuned to Gene Ontology term and Enzyme Commission number prediction, can be inspected too. Results: The approach enabled us to identify amino acids in the sequences that the transformers pay particular attention to, and to show that these relevant sequence parts reflect expectations from biology and chemistry, both in the embedding layer and inside of the model, where we identified transformer heads with a statistically significant correspondence of attribution maps with ground truth sequence annotations (e.g., transmembrane regions, active sites) across many proteins. Availability and Implementation: Source code can be accessed at https://github.com/markuswenzel/xai-proteins .
    
[^50]: 通过不变性和冗余减少理解自监督学习的语音表示

    Understanding Self-Supervised Learning of Speech Representation via Invariance and Redundancy Reduction. (arXiv:2309.03619v1 [cs.SD])

    [http://arxiv.org/abs/2309.03619](http://arxiv.org/abs/2309.03619)

    通过标准化潜变量和调整目标函数，我们提出了Modified Barlow Twins (MBT) 方法来改善自监督学习中的语音表示泛化能力，尤其是在有限的目标数据上微调时。这项研究为发展可重用的自监督语音表示迈出了重要的一步。

    

    目标函数的选择在自监督学习中生成高质量表示的过程中至关重要。本文研究了Barlow Twins (BT) 目标的不同表达形式如何影响语音数据下游任务的性能。我们提出了带有标准化潜变量的Modified Barlow Twins (MBT) 来强制尺度不变，并在说话人识别、性别识别和关键词检测任务上进行了评估。我们的结果表明，MBT在有限的目标数据上微调时，改善了表示泛化能力，尤其是相对于原始BT。这凸显了设计鼓励不变性和可传输表示的目标的重要性。我们的分析提供了关于如何调整BT学习目标以生成在适用于新的下游任务中表现出色的语音表示的见解。这项研究是发展可重用的自监督语音表示的重要一步。

    The choice of the objective function is crucial in emerging high-quality representations from self-supervised learning. This paper investigates how different formulations of the Barlow Twins (BT) objective impact downstream task performance for speech data. We propose Modified Barlow Twins (MBT) with normalized latents to enforce scale-invariance and evaluate on speaker identification, gender recognition and keyword spotting tasks. Our results show MBT improves representation generalization over original BT, especially when fine-tuning with limited target data. This highlights the importance of designing objectives that encourage invariant and transferable representations. Our analysis provides insights into how the BT learning objective can be tailored to produce speech representations that excel when adapted to new downstream tasks. This study is an important step towards developing reusable self-supervised speech representations.
    
[^51]: 动态图分类的过滤表面

    Filtration Surfaces for Dynamic Graph Classification. (arXiv:2309.03616v1 [cs.LG])

    [http://arxiv.org/abs/2309.03616](http://arxiv.org/abs/2309.03616)

    过滤表面是一种可扩展和灵活的方法，可用于处理动态图的分类问题。通过实验证明，它在处理依赖于边权重信息的数据集时优于先前的基线方法。该方法具有较低的总体标准差，并且要么完全无参数，要么最多只有一个参数。

    

    现有的动态图分类方法要么将图内核扩展到时间域，要么使用图神经网络（GNNs）。然而，当前的基线方法存在可扩展性问题，无法处理不断变化的节点集，或者不能考虑边权重信息。我们提出了过滤表面，这是一种新颖的方法，具有可扩展性和灵活性，以减轻上述限制。我们通过实验证明了我们的模型的有效性，并表明过滤表面在依赖边权重信息的数据集上优于先前的基线方法。我们的方法在完全无参数或最多一个参数的情况下，产生了最低的总体标准差。

    Existing approaches for classifying dynamic graphs either lift graph kernels to the temporal domain, or use graph neural networks (GNNs). However, current baselines have scalability issues, cannot handle a changing node set, or do not take edge weight information into account. We propose filtration surfaces, a novel method that is scalable and flexible, to alleviate said restrictions. We experimentally validate the efficacy of our model and show that filtration surfaces outperform previous state-of-the-art baselines on datasets that rely on edge weight information. Our method does so while being either completely parameter-free or having at most one parameter, and yielding the lowest overall standard deviation.
    
[^52]: 你的电池有患火灾的隐患！通过认证来防范伪冒电池。

    Your Battery Is a Blast! Safeguarding Against Counterfeit Batteries with Authentication. (arXiv:2309.03607v1 [cs.CR])

    [http://arxiv.org/abs/2309.03607](http://arxiv.org/abs/2309.03607)

    该论文通过提出两种新的方法，DCAuth和EISthentication，通过机器学习模型利用电池的内部特征，改善了电池认证的技术。这些方法能够自动鉴别不同类型的锂离子电池，从而防范伪造电池的安全隐患。

    

    钛酸锂（Li-ion）电池由于其高能量密度和功率密度，在各种应用中成为主要动力源。2022年，其市场规模估计高达480亿美元。然而，Li-ion电池的广泛应用导致了伪造电池的生产，给用户带来了安全隐患。伪造电池可能引发爆炸或火灾，并且其在市场上的普遍存在使得用户难以检测假冒电池。目前的电池认证方法容易受到先进的伪造技术的攻击，并且通常无法适用于各种类型的电池和系统。在本文中，我们通过提出两种新方法DCAuth和EISthentication，利用每个电池的内部特征通过机器学习模型，改善了电池认证的最新技术。我们的方法可以通过从电池的正常使用中获取的数据自动鉴别锂离子电池的型号和架构，而不需要

    Lithium-ion (Li-ion) batteries are the primary power source in various applications due to their high energy and power density. Their market was estimated to be up to 48 billion U.S. dollars in 2022. However, the widespread adoption of Li-ion batteries has resulted in counterfeit cell production, which can pose safety hazards to users. Counterfeit cells can cause explosions or fires, and their prevalence in the market makes it difficult for users to detect fake cells. Indeed, current battery authentication methods can be susceptible to advanced counterfeiting techniques and are often not adaptable to various cells and systems. In this paper, we improve the state of the art on battery authentication by proposing two novel methodologies, DCAuth and EISthentication, which leverage the internal characteristics of each cell through Machine Learning models. Our methods automatically authenticate lithium-ion battery models and architectures using data from their regular usage without the need
    
[^53]: 通过偏好学习在多目标问题中进行交互式超参数优化

    Interactive Hyperparameter Optimization in Multi-Objective Problems via Preference Learning. (arXiv:2309.03581v1 [cs.LG])

    [http://arxiv.org/abs/2309.03581](http://arxiv.org/abs/2309.03581)

    本文提出了一个以人为中心的交互式超参数优化方法，通过应用偏好学习来解决多目标机器学习中的问题。

    

    超参数优化对于发挥机器学习的潜力至关重要。在实践中，用户通常对多目标问题感兴趣，即优化可能存在冲突的目标，比如准确性和能耗。为了解决这个问题，绝大多数多目标机器学习算法将一组非支配的机器学习模型的帕累托前沿返回给用户。然而，优化这种算法的超参数并不容易，因为评估一个超参数配置涉及评估得到的帕累托前沿的质量。在文献中，已有一些指标可以通过量化不同属性（如体积、与参考点的接近程度）来评估帕累托前沿的质量（例如超体积、R2）。然而，对于用户来说，选择导致期望的帕累托前沿的指标可能是一项困难的任务。在本文中，我们提出了一个以人为中心的交互式超参数优化方法，针对多目标机器学习应用偏好学习。

    Hyperparameter optimization (HPO) is important to leverage the full potential of machine learning (ML). In practice, users are often interested in multi-objective (MO) problems, i.e., optimizing potentially conflicting objectives, like accuracy and energy consumption. To tackle this, the vast majority of MO-ML algorithms return a Pareto front of non-dominated machine learning models to the user. Optimizing the hyperparameters of such algorithms is non-trivial as evaluating a hyperparameter configuration entails evaluating the quality of the resulting Pareto front. In literature, there are known indicators that assess the quality of a Pareto front (e.g., hypervolume, R2) by quantifying different properties (e.g., volume, proximity to a reference point). However, choosing the indicator that leads to the desired Pareto front might be a hard task for a user. In this paper, we propose a human-centered interactive HPO approach tailored towards multi-objective ML leveraging preference learnin
    
[^54]: DTW+S: 使用有序局部趋势进行基于形状的时间序列比较

    DTW+S: Shape-based Comparison of Time-series with Ordered Local Trend. (arXiv:2309.03579v1 [cs.LG])

    [http://arxiv.org/abs/2309.03579](http://arxiv.org/abs/2309.03579)

    提出了一种名为DTW+S的新型测量方法，它通过创建局部趋势的矩阵表示，并应用动态时间规整来计算距离，解决了现有方法无法捕捉局部趋势相似性的问题。

    

    时间序列数据的距离或相似度的测量是许多应用包括分类和聚类的基本方面。现有的测量方法可能由于局部趋势（形状）而无法捕捉到相似之处，甚至可能产生误导性的结果。我们的目标是开发一种能够寻找在相似时间周围发生的相似趋势的测量方法，并且对应用领域的研究人员易于解释的方法。这对于时间序列具有有序的有意义的局部趋势序列的应用特别有用，例如在流行病中（从增长到峰值再到减少）。我们提出了一种新的测量方法，DTW+S，它创建了一个可解释的“保持接近性”的矩阵表示时间序列，其中每一列代表局部趋势，然后应用动态时间规整来计算这些矩阵之间的距离。我们提供了支持这种表示的理论分析。我们展示了DTW+S的实用性。

    Measuring distance or similarity between time-series data is a fundamental aspect of many applications including classification and clustering. Existing measures may fail to capture similarities due to local trends (shapes) and may even produce misleading results. Our goal is to develop a measure that looks for similar trends occurring around similar times and is easily interpretable for researchers in applied domains. This is particularly useful for applications where time-series have a sequence of meaningful local trends that are ordered, such as in epidemics (a surge to an increase to a peak to a decrease). We propose a novel measure, DTW+S, which creates an interpretable "closeness-preserving" matrix representation of the time-series, where each column represents local trends, and then it applies Dynamic Time Warping to compute distances between these matrices. We present a theoretical analysis that supports the choice of this representation. We demonstrate the utility of DTW+S in 
    
[^55]: 稀疏联合训练在车联网物体检测中的应用

    Sparse Federated Training of Object Detection in the Internet of Vehicles. (arXiv:2309.03569v1 [cs.LG])

    [http://arxiv.org/abs/2309.03569](http://arxiv.org/abs/2309.03569)

    该论文介绍了在车联网物体检测中采用稀疏联合训练的方法。通过在中央服务器共享本地模型，该方法可以减轻隐私泄露问题。同时，通过在边缘设备上进行稀疏训练，可以提高边缘设备的训练效率，减少延迟。

    

    作为智能交通系统（ITS）的重要组成部分，车联网（IoV）在缓解交通问题方面发挥着重要作用。物体检测是IoV中的关键技术之一，通过分析及时敏感的车辆相关信息，广泛用于提供交通管理服务。然而，当前的物体检测方法大多基于集中式深度训练，即边缘设备获取的敏感数据需要上传到服务器，引发了隐私问题。为了减轻此隐私泄露，我们首先提出了基于联合学习的框架，在中央服务器上共享训练良好的本地模型。然而，由于边缘设备通常计算能力有限，并且车联网对低延迟有严格要求，因此我们进一步提出了边缘设备上的稀疏训练过程，可以有效减轻模型负担，并确保其在边缘设备上的训练效率，从而减少延迟。

    As an essential component part of the Intelligent Transportation System (ITS), the Internet of Vehicles (IoV) plays a vital role in alleviating traffic issues. Object detection is one of the key technologies in the IoV, which has been widely used to provide traffic management services by analyzing timely and sensitive vehicle-related information. However, the current object detection methods are mostly based on centralized deep training, that is, the sensitive data obtained by edge devices need to be uploaded to the server, which raises privacy concerns. To mitigate such privacy leakage, we first propose a federated learning-based framework, where well-trained local models are shared in the central server. However, since edge devices usually have limited computing power, plus a strict requirement of low latency in IoVs, we further propose a sparse training process on edge devices, which can effectively lighten the model, and ensure its training efficiency on edge devices, thereby reduc
    
[^56]: 评估监督学习和大型语言模型在识别中国社交媒体中的认知偏差和自杀风险方面的功效

    Evaluating the Efficacy of Supervised Learning vs Large Language Models for Identifying Cognitive Distortions and Suicidal Risks in Chinese Social Media. (arXiv:2309.03564v1 [cs.CL])

    [http://arxiv.org/abs/2309.03564](http://arxiv.org/abs/2309.03564)

    本研究评估了监督学习和大型语言模型在识别中国社交媒体中的认知偏差和自杀风险方面的功效。结果表明大型语言模型在这两个任务上具有很高的效果。

    

    大型语言模型，特别是类似快速发展的GPT系列，因其广泛的影响力而受到关注。尽管在心理学等医学领域对它们的适用性存在浓厚兴趣，但对真实世界数据的具体探索仍然很少。与此同时，社交媒体平台上的用户越来越多地表达个人情感；在特定的主题下，这些情感通常表现为消极情绪，有时会升级为自杀倾向。及时辨识这样的认知偏差和自杀风险对有效干预和潜在避免严重情况至关重要。我们的研究通过在中国社交媒体平台上进行两个关键任务：自杀风险和认知偏差识别的实验，进入了这个领域。使用监督学习作为基准，我们通过三种不同的策略：零样本、少样本和微调，考察了大型语言模型的功效。

    Large language models, particularly those akin to the rapidly progressing GPT series, are gaining traction for their expansive influence. While there is keen interest in their applicability within medical domains such as psychology, tangible explorations on real-world data remain scant. Concurrently, users on social media platforms are increasingly vocalizing personal sentiments; under specific thematic umbrellas, these sentiments often manifest as negative emotions, sometimes escalating to suicidal inclinations. Timely discernment of such cognitive distortions and suicidal risks is crucial to effectively intervene and potentially avert dire circumstances. Our study ventured into this realm by experimenting on two pivotal tasks: suicidal risk and cognitive distortion identification on Chinese social media platforms. Using supervised learning as a baseline, we examined and contrasted the efficacy of large language models via three distinct strategies: zero-shot, few-shot, and fine-tunin
    
[^57]: 缺失值处理的三值决策树

    Trinary Decision Trees for missing value handling. (arXiv:2309.03561v1 [stat.ML])

    [http://arxiv.org/abs/2309.03561](http://arxiv.org/abs/2309.03561)

    本文介绍了一种称为三值决策树的算法，用于改善决策树在处理缺失数据时的表现。与其他方法不同，该算法不假设缺失值包含任何关于响应的信息。实验证明，在特定缺失数据场景下，三值决策树在MCAR设置中表现优异，在IM设置中略逊一筹。同时，通过将三值决策树与缺失在属性方法相结合，可以获得更稳健的性能。尽管训练速度较慢，但三值决策树提供了一种有前途且更准确的方法。

    

    本文介绍了三值决策树，这是一种旨在改善决策树回归器和分类器中处理缺失数据的算法。与其他方法不同，三值决策树不假设缺失值包含有关响应的任何信息。本文通过理论计算和使用真实数据集的数值示例，比较了其在不同缺失数据场景（完全随机缺失（MCAR）和信息性缺失（IM））中与已建立算法的性能。值得注意的是，在MCAR设置中，三值树在只有样本外缺失数据时表现优于其同行，而在IM设置中落后。一个混合模型，即三值缺失在属性（MIA）方法和三值树相结合的TrinaryMIA树，在所有缺失类型中表现出强大的性能。尽管训练速度较慢可能是一个潜在的缺点，但三值决策树提供了一个有前途且更准确的方法。

    This paper introduces the Trinary decision tree, an algorithm designed to improve the handling of missing data in decision tree regressors and classifiers. Unlike other approaches, the Trinary decision tree does not assume that missing values contain any information about the response. Both theoretical calculations on estimator bias and numerical illustrations using real data sets are presented to compare its performance with established algorithms in different missing data scenarios (Missing Completely at Random (MCAR), and Informative Missingness (IM)). Notably, the Trinary tree outperforms its peers in MCAR settings, especially when data is only missing out-of-sample, while lacking behind in IM settings. A hybrid model, the TrinaryMIA tree, which combines the Trinary tree and the Missing In Attributes (MIA) approach, shows robust performance in all types of missingness. Despite the potential drawback of slower training speed, the Trinary tree offers a promising and more accurate met
    
[^58]: 论多智能体非线性滤波和学习的动力学

    On the dynamics of multi agent nonlinear filtering and learning. (arXiv:2309.03557v1 [stat.ML])

    [http://arxiv.org/abs/2309.03557](http://arxiv.org/abs/2309.03557)

    本文研究了具有非线性滤波/学习动力学的多智能体网络系统的行为，并介绍了在分布式和联邦学习场景中的应用。

    

    多智能体系统通过分散一致性寻求动力学来完成高度复杂的学习任务，其在信号处理和计算智能社区引起了极大关注。本文研究了具有非线性滤波/学习动力学的多智能体网络系统的行为。为此，提出了多智能体网络系统中一个智能体的行动的一般表述，并给出了实现协同学习行为的条件。重要的是，还介绍了该推导框架在分布式和联邦学习场景中的应用。

    Multiagent systems aim to accomplish highly complex learning tasks through decentralised consensus seeking dynamics and their use has garnered a great deal of attention in the signal processing and computational intelligence societies. This article examines the behaviour of multiagent networked systems with nonlinear filtering/learning dynamics. To this end, a general formulation for the actions of an agent in multiagent networked systems is presented and conditions for achieving a cohesive learning behaviour is given. Importantly, application of the so derived framework in distributed and federated learning scenarios are presented.
    
[^59]: MVD：一种用于声学车辆类型分类的新方法和数据集

    MVD:A Novel Methodology and Dataset for Acoustic Vehicle Type Classification. (arXiv:2309.03544v1 [cs.SD])

    [http://arxiv.org/abs/2309.03544](http://arxiv.org/abs/2309.03544)

    本研究提出了MVD和MVDA两个开放数据集，用于声学交通监测和车辆类型分类算法的开发。通过使用倒谱和频谱等特征以及多输入神经网络，我们提出了一种新颖且高效的方法来准确分类移动车辆的声音信号。实验结果表明，我们的方法在准确率上优于以前的工作。

    

    不断增长的城市人口导致车辆使用激增，交通监测和管理变得不可或缺。声学交通监测（ATM）提供了一种成本效益高且高效的替代方案，可以替代计算机视觉技术等计算成本更高的交通监测方法。本文介绍了MVD和MVDA两个开放数据集，用于声学交通监测和车辆类型分类算法的开发，其中包含移动车辆的音频记录。数据集包含四个类别-卡车、汽车、摩托车和无车辆类别。此外，我们提出了一种新颖且高效的方法，使用基于倒谱和频谱的局部和全局音频特征以及多输入神经网络来准确分类这些声音信号。实验结果表明，我们的方法改进了以前工作的基线，并在MVD和MVDA数据集上分别达到了91.98％和96.66％的准确率。

    Rising urban populations have led to a surge in vehicle use and made traffic monitoring and management indispensable. Acoustic traffic monitoring (ATM) offers a cost-effective and efficient alternative to more computationally expensive methods of monitoring traffic such as those involving computer vision technologies. In this paper, we present MVD and MVDA: two open datasets for the development of acoustic traffic monitoring and vehicle-type classification algorithms, which contain audio recordings of moving vehicles. The dataset contain four classes- Trucks, Cars, Motorbikes, and a No-vehicle class. Additionally, we propose a novel and efficient way to accurately classify these acoustic signals using cepstrum and spectrum based local and global audio features, and a multi-input neural network. Experimental results show that our methodology improves upon the established baselines of previous works and achieves an accuracy of 91.98% and 96.66% on MVD and MVDA Datasets, respectively. Fin
    
[^60]: 基于子图的紧框架在具有紧致支持和渐消磨的图上

    Subgraph-based Tight Frames on Graphs with Compact Supports and Vanishing Moments. (arXiv:2309.03537v1 [eess.SP])

    [http://arxiv.org/abs/2309.03537](http://arxiv.org/abs/2309.03537)

    本研究提出了一种基于子图的紧框架构造方法，能够灵活地调整框架的消失矩和其他属性，实现对具有路径支持的图信号的高效表示，在非线性逼近任务中表现出优越性能。

    

    在这项工作中，我们提出了一种新颖且通用的方法，基于一系列分层分区构建具有紧致支持的图上的紧框架。从我们的抽象构造开始，我们能够灵活地将子图Laplacians纳入到我们的图框架设计中。因此，我们的通用方法允许调整框架的（子图）消失矩和其他属性，如方向性，以有效地表示具有路径支持的图信号。我们明确定义并测试了几个变体。实验结果表明，我们提出的图框架在非线性逼近任务中表现出优越性能。

    In this work, we proposed a novel and general method to construct tight frames on graphs with compact supports based on a series of hierarchical partitions. Starting from our abstract construction that generalizes previous methods based on partition trees, we are able to flexibly incorporate subgraph Laplacians into our design of graph frames. Consequently, our general methods permit adjusting the (subgraph) vanishing moments of the framelets and extra properties, such as directionality, for efficiently representing graph signals with path-like supports. Several variants are explicitly defined and tested. Experimental results show our proposed graph frames perform superiorly in non-linear approximation tasks.
    
[^61]: 特征增强分割网络（FES-Net）用于血管分割

    Feature Enhancer Segmentation Network (FES-Net) for Vessel Segmentation. (arXiv:2309.03535v1 [eess.IV])

    [http://arxiv.org/abs/2309.03535](http://arxiv.org/abs/2309.03535)

    本研究提出了一种新颖的特征增强分割网络（FES-Net），用于精确地分割视网膜血管。该网络利用提示卷积块进行下采样，并采用浅层上采样方法生成每个类别的二进制掩码，从而避免了其他图像增强步骤的需求。

    

    糖尿病视网膜病变和老年性黄斑变性等疾病对视力构成重大危险，强调了对视网膜血管进行精确分割的重要性，以便跟踪和诊断进展情况。然而，现有的依赖编码器-解码器结构的血管分割方法在捕捉视网膜血管配置的上下文信息方面存在困难，导致编码器和解码器特征之间的语义差异难以调和。为了解决这个问题，我们提出了一种新颖的特征增强分割网络（FES-Net），它能够在不需要额外图像增强步骤的情况下实现精确的像素级分割。FES-Net直接处理输入图像，并在下采样过程中利用四个提示卷积块（PCBs），并结合浅层上采样方法为每个类别生成二进制掩码。我们在四个公开的最先进数据集上评估了FES-Net的性能：DRIVE, STARE, CH

    Diseases such as diabetic retinopathy and age-related macular degeneration pose a significant risk to vision, highlighting the importance of precise segmentation of retinal vessels for the tracking and diagnosis of progression. However, existing vessel segmentation methods that heavily rely on encoder-decoder structures struggle to capture contextual information about retinal vessel configurations, leading to challenges in reconciling semantic disparities between encoder and decoder features. To address this, we propose a novel feature enhancement segmentation network (FES-Net) that achieves accurate pixel-wise segmentation without requiring additional image enhancement steps. FES-Net directly processes the input image and utilizes four prompt convolutional blocks (PCBs) during downsampling, complemented by a shallow upsampling approach to generate a binary mask for each class. We evaluate the performance of FES-Net on four publicly available state-of-the-art datasets: DRIVE, STARE, CH
    
[^62]: 使用源原型的强大负学习方法来进行部分领域适应的鲁棒性

    A Robust Negative Learning Approach to Partial Domain Adaptation Using Source Prototypes. (arXiv:2309.03531v1 [cs.CV])

    [http://arxiv.org/abs/2309.03531](http://arxiv.org/abs/2309.03531)

    本文提出了一个鲁棒的部分领域适应（PDA）框架，通过整合鲁棒的目标监督策略，解决了负迁移问题，并在领域无关的方式下优化了类内紧凑性和类间分离性。通过预先推断源原型，确保了源数据的隐私性。实验证实了该框架的有效性。

    

    本文提出了一个鲁棒的部分领域适应（PDA）框架，通过整合鲁棒的目标监督策略，缓解了负迁移问题。该框架利用集成学习和包含多样化、互补的标签反馈，减轻了错误反馈的影响，并促进伪标签的改进。与仅依赖分布对齐的一阶矩不同，我们的方法通过推断源原型和高可信的目标样本，在领域无关的方式下，提供了优化类内紧凑性和类间分离性的明确目标。值得注意的是，我们通过预先推断源原型，确保了源数据的隐私性，在适应阶段无需访问源数据。我们进行了一系列全面的实验，包括消融分析，涵盖了各种部分领域适应任务。在基准数据集上的全面评估证实了我们的框架的有效性。

    This work proposes a robust Partial Domain Adaptation (PDA) framework that mitigates the negative transfer problem by incorporating a robust target-supervision strategy. It leverages ensemble learning and includes diverse, complementary label feedback, alleviating the effect of incorrect feedback and promoting pseudo-label refinement. Rather than relying exclusively on first-order moments for distribution alignment, our approach offers explicit objectives to optimize intra-class compactness and inter-class separation with the inferred source prototypes and highly-confident target samples in a domain-invariant fashion. Notably, we ensure source data privacy by eliminating the need to access the source data during the adaptation phase through a priori inference of source prototypes. We conducted a series of comprehensive experiments, including an ablation analysis, covering a range of partial domain adaptation tasks. Comprehensive evaluations on benchmark datasets corroborate our framewo
    
[^63]: 针对移动机器人中单一物体检测的高效方法及早期中止增强高精度CNNs

    Efficient Single Object Detection on Image Patches with Early Exit Enhanced High-Precision CNNs. (arXiv:2309.03530v1 [cs.CV])

    [http://arxiv.org/abs/2309.03530](http://arxiv.org/abs/2309.03530)

    本文提出了一种在移动机器人中高效检测单一物体的方法，在RoboCup比赛中特别关注球的检测。该方法通过设计计算能力有限的机器人平台的卷积神经网络架构，在图像补丁中实现高精度的单一物体分类，同时确定其精确的空间位置。此外，提出了早期中止技术以减少背景类中易于拒绝的情况的计算成本。训练过程中采用复合损失函数和数据增强。这种方法在实验中达到了100%的精确度。

    

    本文提出了一种在RoboCup标准平台联赛背景下利用移动机器人检测物体的新方法，重点是检测球。挑战在于在不同光照条件和快速运动引起的图像模糊情况下检测动态物体。为了解决这个挑战，本文提出了一种专为计算受限的机器人平台设计的卷积神经网络架构。所提出的CNN经过训练，能够在图像补丁中实现对单一物体的高精度分类，并确定它们的精确空间位置。本文进一步将早期退出(Early Exits)整合到现有的高精度CNN架构中，以减少背景类中易于拒绝的情况的计算成本。训练过程涉及基于置信度和位置损失的复合损失函数，具有动态加权和数据增强。所提出的方法在v中达到了100%的精确度

    This paper proposes a novel approach for detecting objects using mobile robots in the context of the RoboCup Standard Platform League, with a primary focus on detecting the ball. The challenge lies in detecting a dynamic object in varying lighting conditions and blurred images caused by fast movements. To address this challenge, the paper presents a convolutional neural network architecture designed specifically for computationally constrained robotic platforms. The proposed CNN is trained to achieve high precision classification of single objects in image patches and to determine their precise spatial positions. The paper further integrates Early Exits into the existing high-precision CNN architecture to reduce the computational cost of easily rejectable cases in the background class. The training process involves a composite loss function based on confidence and positional losses with dynamic weighting and data augmentation. The proposed approach achieves a precision of 100% on the v
    
[^64]: 隐私保护的自适应共振理论持续联邦聚类

    Privacy-preserving Continual Federated Clustering via Adaptive Resonance Theory. (arXiv:2309.03487v1 [cs.LG])

    [http://arxiv.org/abs/2309.03487](http://arxiv.org/abs/2309.03487)

    本文提出了一种隐私保护的持续性联邦聚类算法，通过使用自适应共振理论聚类算法作为基础聚类器，实现了对于未知或持续变化数据的处理。

    

    随着数据隐私保护的重要性日益增加，许多隐私保护的机器学习方法被提出。在聚类领域中，各种使用联邦学习框架（即联邦聚类）的算法已经被积极研究，并展示了高效的聚类性能以及数据隐私保护。然而，现有的联邦聚类算法中使用的大部分基础聚类器（即聚类算法）需要预先指定聚类数量。因此，这些算法无法处理数据分布未知或持续变化的情况。为解决这个问题，本文提出了一种隐私保护的持续性联邦聚类算法。在所提出的算法中，使用了一种基于自适应共振理论的聚类算法，它具备持续学习的能力。通过对合成和真实世界数据进行实验，验证了算法的性能。

    With the increasing importance of data privacy protection, various privacy-preserving machine learning methods have been proposed. In the clustering domain, various algorithms with a federated learning framework (i.e., federated clustering) have been actively studied and showed high clustering performance while preserving data privacy. However, most of the base clusterers (i.e., clustering algorithms) used in existing federated clustering algorithms need to specify the number of clusters in advance. These algorithms, therefore, are unable to deal with data whose distributions are unknown or continually changing. To tackle this problem, this paper proposes a privacy-preserving continual federated clustering algorithm. In the proposed algorithm, an adaptive resonance theory-based clustering algorithm capable of continual learning is used as a base clusterer. Therefore, the proposed algorithm inherits the ability of continual learning. Experimental results with synthetic and real-world da
    
[^65]: 快速的FixMatch: 基于课程批次大小的快速半监督学习

    Fast FixMatch: Faster Semi-Supervised Learning with Curriculum Batch Size. (arXiv:2309.03469v1 [cs.LG])

    [http://arxiv.org/abs/2309.03469](http://arxiv.org/abs/2309.03469)

    本论文提出了快速的FixMatch算法，通过引入课程批次大小（CBS）来减少半监督学习的训练计算量，并使用强化标记增强和课程伪标签进行改进。

    

    半监督学习（SSL）的进展几乎完全消除了SSL和监督学习之间的差距，同时减少了标签数量。然而，最近的性能提升往往是以显著增加的训练计算为代价的。为了解决这个问题，我们提出了课程批次大小（CBS），利用深度神经网络的自然训练动态，采用一个小的未标记的批次大小开始训练，并逐渐增加到训练结束。无论数据集、模型或训练轮次，都使用固定的课程，通过在所有设置中减少训练计算量。我们将CBS、强化标记增强和课程伪标签（CPL）应用于FixMatch，并将这个新的SSL算法称为快速的FixMatch。我们进行了割实验，表明强化标记增强和/或CPL并不显著地减少训练量。

    Advances in Semi-Supervised Learning (SSL) have almost entirely closed the gap between SSL and Supervised Learning at a fraction of the number of labels. However, recent performance improvements have often come \textit{at the cost of significantly increased training computation}. To address this, we propose Curriculum Batch Size (CBS), \textit{an unlabeled batch size curriculum which exploits the natural training dynamics of deep neural networks.} A small unlabeled batch size is used in the beginning of training and is gradually increased to the end of training. A fixed curriculum is used regardless of dataset, model or number of epochs, and reduced training computations is demonstrated on all settings. We apply CBS, strong labeled augmentation, Curriculum Pseudo Labeling (CPL) \citep{FlexMatch} to FixMatch \citep{FixMatch} and term the new SSL algorithm Fast FixMatch. We perform an ablation study to show that strong labeled augmentation and/or CPL do not significantly reduce training 
    
[^66]: 跨图像上下文对于Bongard问题很重要

    Cross-Image Context Matters for Bongard Problems. (arXiv:2309.03468v1 [cs.CV])

    [http://arxiv.org/abs/2309.03468](http://arxiv.org/abs/2309.03468)

    Bongard问题是一种需要从一组正负图像中推导出抽象概念并进行分类的智力测试，现有方法在Bongard问题中准确率较低。本研究发现，这是因为现有方法未能整合支持集合中的信息，而是仅依赖于单个支持图像的信息。我们提出了一种通过跨图像上下文来提高准确性的解决方案。

    

    目前的机器学习方法在解决Bongard问题时存在困难。Bongard问题是一种需要从一组正负“支持”图像中推导出抽象“概念”，然后对于新的查询图像进行分类，判断它是否描述了关键概念的智力测试。在用于自然图像Bongard问题的基准测试Bongard-HOI中，现有方法的准确率仅达到了66%（偶然准确率为50%）。低准确率通常归因于神经网络缺乏发现类似人类符号规则的能力。我们指出，许多现有方法由于一个更简单的问题而失去了准确性：它们没有将支持集合中的信息作为一个整体加入，而是依赖于从单个支持中提取的信息。这是一个关键问题，因为与涉及对象分类的少样本学习任务不同，一个典型的Bongard问题中的“关键概念”只能使用多个正例和多个反例来区分。我们探索了一种解决方案，通过跨图像上下文来提高准确性。

    Current machine learning methods struggle to solve Bongard problems, which are a type of IQ test that requires deriving an abstract "concept" from a set of positive and negative "support" images, and then classifying whether or not a new query image depicts the key concept. On Bongard-HOI, a benchmark for natural-image Bongard problems, existing methods have only reached 66% accuracy (where chance is 50%). Low accuracy is often attributed to neural nets' lack of ability to find human-like symbolic rules. In this work, we point out that many existing methods are forfeiting accuracy due to a much simpler problem: they do not incorporate information contained in the support set as a whole, and rely instead on information extracted from individual supports. This is a critical issue, because unlike in few-shot learning tasks concerning object classification, the "key concept" in a typical Bongard problem can only be distinguished using multiple positives and multiple negatives. We explore a
    
[^67]: 多模态辅助网络用于推断缺失模态

    Multi-Modality Guidance Network For Missing Modality Inference. (arXiv:2309.03452v1 [cs.CV])

    [http://arxiv.org/abs/2309.03452](http://arxiv.org/abs/2309.03452)

    我们提出了一种多模态辅助网络，在训练过程中利用多模态表示来训练更好的单模态模型进行推断，解决了推断过程中模态缺失的问题。实验结果表明，我们的方法在性能上显著优于传统方法。

    

    多模态模型在最近几年取得了显著的成功。标准多模态方法通常假设在训练阶段和推断阶段模态保持不变。然而在实践中，许多场景无法满足这样的假设，推断过程中会出现缺失模态，从而限制了多模态模型的应用范围。现有的方法通过重建缺失模态来缓解这个问题，但这增加了不必要的计算成本，尤其对于大型部署系统而言可能是至关重要的。为了从两个方面解决这个问题，我们提出了一种新颖的辅助网络，在训练过程中促进知识共享，利用多模态表示来训练更好的单模态模型进行推断。在暴力检测的现实生活实验中，我们提出的框架训练的单模态模型在性能上显著优于传统训练的模型，同时保持相同的推断能力。

    Multimodal models have gained significant success in recent years. Standard multimodal approaches often assume unchanged modalities from training stage to inference stage. In practice, however, many scenarios fail to satisfy such assumptions with missing modalities during inference, leading to limitations on where multimodal models can be applied. While existing methods mitigate the problem through reconstructing the missing modalities, it increases unnecessary computational cost, which could be just as critical, especially for large, deployed systems. To solve the problem from both sides, we propose a novel guidance network that promotes knowledge sharing during training, taking advantage of the multimodal representations to train better single-modality models for inference. Real-life experiment in violence detection shows that our proposed framework trains single-modality models that significantly outperform its traditionally trained counterparts while maintaining the same inference 
    
[^68]: 跨域声音识别用于高效的水下数据分析

    Cross-domain Sound Recognition for Efficient Underwater Data Analysis. (arXiv:2309.03451v1 [cs.SD])

    [http://arxiv.org/abs/2309.03451](http://arxiv.org/abs/2309.03451)

    本文提出了一种跨域声音识别方法，通过利用非水下声音模型训练数据来分析水下声学数据。通过聚类和可视化方法，简化了候选标签选择的过程，并通过训练神经网络模型实现了对空气枪声的高效识别。

    

    本文介绍了一种新颖的深度学习方法，通过利用在广谱非水下（空中）声音上训练的模型，分析海量水下声学数据。鉴于标记大量水下数据的挑战，我们提出了一个双重方法来加速这一劳动密集型过程。我们的方法的第一部分涉及使用空中声音识别模型的特征向量对水下数据进行PCA和UMAP可视化。这使我们能够在二维空间中对数据进行聚类，并听取这些聚类中的点以了解其定义特性。这种创新方法简化了选择候选标签进行进一步训练的过程。在第二部分中，我们使用选定的水下数据和非水下数据集训练了一个神经网络模型。我们进行了定量分析，衡量了我们模型识别空气枪声的精确度、召回率和F1得分。

    This paper presents a novel deep learning approach for analyzing massive underwater acoustic data by leveraging a model trained on a broad spectrum of non-underwater (aerial) sounds. Recognizing the challenge in labeling vast amounts of underwater data, we propose a two-fold methodology to accelerate this labor-intensive procedure.  The first part of our approach involves PCA and UMAP visualization of the underwater data using the feature vectors of an aerial sound recognition model. This enables us to cluster the data in a two dimensional space and listen to points within these clusters to understand their defining characteristics. This innovative method simplifies the process of selecting candidate labels for further training.  In the second part, we train a neural network model using both the selected underwater data and the non-underwater dataset. We conducted a quantitative analysis to measure the precision, recall, and F1 score of our model for recognizing airgun sounds, a common
    
[^69]: XGen-7B技术报告

    XGen-7B Technical Report. (arXiv:2309.03450v1 [cs.CL])

    [http://arxiv.org/abs/2309.03450](http://arxiv.org/abs/2309.03450)

    XGen-7B是一种用于处理长序列的大型语言模型，通过克服开源LLMs在支持长序列长度方面的限制，并在标准基准上取得与最先进的开源LLMs相当或更好的结果，推进了研究进展和商业应用。

    

    大型语言模型（LLMs）在各个领域变得普遍，改变了我们与信息交互和进行研究的方式。然而，大多数高性能的LLMs仍然受限于专有墙壁，阻碍了科学进展。另一方面，大多数开源的LLMs在支持较长序列长度方面有限，而这对于许多需要对输入上下文进行推理的任务来说是一个关键要求。为了解决这个问题，我们训练了XGen，一系列7B参数的模型，可支持长度为8K的序列和1.5T个令牌。我们还对XGen模型进行了公共领域教学数据的微调，创建了它们的教学优化版本（XGen-Inst）。我们将我们的模型开源，用于研究进展和商业应用。我们对标准基准的评估结果显示，与最先进的开源LLMs相比，XGen模型实现了相当或更好的结果。我们针对长序列建模任务进行了有针对性的评估。

    Large Language Models (LLMs) have become ubiquitous across various domains, transforming the way we interact with information and conduct research. However, most high-performing LLMs remain confined behind proprietary walls, hindering scientific progress. Most open-source LLMs, on the other hand, are limited in their ability to support longer sequence lengths, which is a key requirement for many tasks that require inference over an input context. To address this, we have trained XGen, a series of 7B parameter models on up to 8K sequence length for up to 1.5T tokens. We have also finetuned the XGen models on public-domain instructional data, creating their instruction-tuned counterparts (XGen-Inst). We open-source our models for both research advancements and commercial applications. Our evaluation on standard benchmarks shows that XGen models achieve comparable or better results when compared with state-of-the-art open-source LLMs. Our targeted evaluation on long sequence modeling task
    
[^70]: 通过生成对抗神经算子实现宽带地面运动合成: 开发与验证

    Broadband Ground Motion Synthesis via Generative Adversarial Neural Operators: Development and Validation. (arXiv:2309.03447v1 [physics.geo-ph])

    [http://arxiv.org/abs/2309.03447](http://arxiv.org/abs/2309.03447)

    本论文提出了一种使用生成对抗神经算子的数据驱动地面运动合成模型，可以根据不同参数生成三分量加速度时间历史。通过使用神经算子架构，模型训练不受数据采样频率影响。研究结果表明，该模型在验证和应用实例中表现出色，并可用于生成日本地震动数据。

    

    我们提出了一种使用生成对抗神经算子（GANO）的数据驱动地面运动合成模型，该模型结合了机器学习和开放获取的强震动数据集，可以根据矩震级（M）、断裂距离（R_{rup}）、顶部30m处的时间平均剪切波速度（V_{S30}）和构造环境或断层类型生成三分量加速度时间历史。我们使用神经算子，这是一种分辨率无关的架构，可以保证模型训练与数据采样频率无关。首先，我们提出了条件地面运动合成算法（以下简称cGM-GANO）并讨论其与先前工作相比的优势。接下来，我们使用南加州地震中心（SCEC）宽带平台（BBP）产生的模拟地震动验证了cGM-GANO框架。最后，我们在日本的KiK-net数据集上训练了cGM-GANO，表明该框架可以重新生成地震动数据。

    We present a data-driven model for ground-motion synthesis using a Generative Adversarial Neural Operator (GANO) that combines recent advancements in machine learning and open access strong motion data sets to generate three-component acceleration time histories conditioned on moment magnitude ($M$), rupture distance ($R_{rup}$), time-average shear-wave velocity at the top $30m$ ($V_{S30}$), and tectonic environment or style of faulting. We use Neural Operators, a resolution invariant architecture that guarantees that the model training is independent of the data sampling frequency. We first present the conditional ground-motion synthesis algorithm (referred to heretofore as cGM-GANO) and discuss its advantages compared to previous work. Next, we verify the cGM-GANO framework using simulated ground motions generated with the Southern California Earthquake Center (SCEC) Broadband Platform (BBP). We lastly train cGM-GANO on a KiK-net dataset from Japan, showing that the framework can rec
    
[^71]: 早产儿点状白质病变分割研究：基于反事实生成学习的动力

    Punctate White Matter Lesion Segmentation in Preterm Infants Powered by Counterfactually Generative Learning. (arXiv:2309.03440v1 [eess.IV])

    [http://arxiv.org/abs/2309.03440](http://arxiv.org/abs/2309.03440)

    本文提出了一种利用反事实生成学习的方法，通过辅助任务和深度学习框架实现了早产儿点状白质病变的准确分割和定位。

    

    准确地分割点状白质病变（PWMLs）对相关发育障碍的及时诊断和治疗至关重要。考虑到病变通常较小、对比度较低，并且病变数量在不同受试者之间可能发生巨大变化，从婴儿脑MR图像中自动分割PWMLs具有挑战性。现有的基于学习的方法直接将通用的网络结构应用于这一具有挑战性的任务，可能无法捕捉到PWMLs的详细位置信息，从而导致严重的欠分割。在本文中，我们提出利用反事实推理的思想结合脑组织分割的辅助任务，学习精细的位置和形态表示以实现PWMLs的准确定位和分割。因此，我们设计了一个简单易行的深度学习框架（即DeepPWML）。它将病变反事实图与组织概率结合起来进行分割。

    Accurate segmentation of punctate white matter lesions (PWMLs) are fundamental for the timely diagnosis and treatment of related developmental disorders. Automated PWMLs segmentation from infant brain MR images is challenging, considering that the lesions are typically small and low-contrast, and the number of lesions may dramatically change across subjects. Existing learning-based methods directly apply general network architectures to this challenging task, which may fail to capture detailed positional information of PWMLs, potentially leading to severe under-segmentations. In this paper, we propose to leverage the idea of counterfactual reasoning coupled with the auxiliary task of brain tissue segmentation to learn fine-grained positional and morphological representations of PWMLs for accurate localization and segmentation. A simple and easy-to-implement deep-learning framework (i.e., DeepPWML) is accordingly designed. It combines the lesion counterfactual map with the tissue probab
    
[^72]: 个性化的Tucker分解：对张量数据中的共性和独特性进行建模

    Personalized Tucker Decomposition: Modeling Commonality and Peculiarity on Tensor Data. (arXiv:2309.03439v1 [cs.LG])

    [http://arxiv.org/abs/2309.03439](http://arxiv.org/abs/2309.03439)

    本研究提出了个性化的Tucker分解（perTucker）方法来捕捉张量数据中的异质性，通过学习独特和共性表示，展示了perTucker在异常检测、客户分类和聚类方面的有效性。

    

    我们提出了个性化的Tucker分解（perTucker）来解决传统张量分解方法在捕捉不同数据集之间的异质性方面的局限性。perTucker将张量数据分解为共享的全局分量和个性化的局部分量。我们引入了一种模式正交性假设，并开发了一种收敛于稳定点的近端梯度正则化块坐标下降算法。通过学习跨数据集的独特和共同表示，我们通过一个模拟研究和两个案例研究（太阳耀斑检测和吨位信号分类）展示了perTucker在异常检测、客户分类和聚类方面的有效性。

    We propose personalized Tucker decomposition (perTucker) to address the limitations of traditional tensor decomposition methods in capturing heterogeneity across different datasets. perTucker decomposes tensor data into shared global components and personalized local components. We introduce a mode orthogonality assumption and develop a proximal gradient regularized block coordinate descent algorithm that is guaranteed to converge to a stationary point. By learning unique and common representations across datasets, we demonstrate perTucker's effectiveness in anomaly detection, client classification, and clustering through a simulation study and two case studies on solar flare detection and tonnage signal classification.
    
[^73]: 具有方差减少和差分隐私的拜占庭鲁棒联邦学习

    Byzantine-Robust Federated Learning with Variance Reduction and Differential Privacy. (arXiv:2309.03437v1 [cs.LG])

    [http://arxiv.org/abs/2309.03437](http://arxiv.org/abs/2309.03437)

    该论文提出了一种新的联邦学习方案，具有方差减少和差分隐私技术，旨在保护数据隐私，并提高对拜占庭攻击的鲁棒性。

    

    联邦学习旨在在模型训练过程中保护数据隐私，其中数据保留在客户端（即物联网设备）上，仅通过共同学习的迭代过程共享客户端的模型更新。然而，这个过程容易受到隐私攻击和拜占庭攻击的影响：在整个联邦学习网络中共享的本地模型更新将泄露有关本地训练数据的私人信息，并且它们还可以被拜占庭攻击者恶意制作以干扰学习过程。本文提出了一种新的联邦学习方案，旨在保证严格的隐私性同时提高系统对拜占庭攻击的鲁棒性。我们的方法将稀疏化和动量驱动的方差减少引入到客户端级别的差分隐私机制中，以防止拜占庭攻击。安全设计不会违反客户端级别的差分隐私机制的隐私保证，因此我们的方法实现了相同的客户端级别的差分隐私保证。

    Federated learning (FL) is designed to preserve data privacy during model training, where the data remains on the client side (i.e., IoT devices), and only model updates of clients are shared iteratively for collaborative learning. However, this process is vulnerable to privacy attacks and Byzantine attacks: the local model updates shared throughout the FL network will leak private information about the local training data, and they can also be maliciously crafted by Byzantine attackers to disturb the learning. In this paper, we propose a new FL scheme that guarantees rigorous privacy and simultaneously enhances system robustness against Byzantine attacks. Our approach introduces sparsification- and momentum-driven variance reduction into the client-level differential privacy (DP) mechanism, to defend against Byzantine attackers. The security design does not violate the privacy guarantee of the client-level DP mechanism; hence, our approach achieves the same client-level DP guarantee a
    
[^74]: 相等的长期效益率：将静态公平性概念应用到顺序决策中

    Equal Long-term Benefit Rate: Adapting Static Fairness Notions to Sequential Decision Making. (arXiv:2309.03426v1 [cs.LG])

    [http://arxiv.org/abs/2309.03426](http://arxiv.org/abs/2309.03426)

    这篇论文介绍了一种称为Equal Long-term Benefit Rate（ELBERT）的长期公平性概念，该概念考虑到不同时间步的变化重要性，并将静态公平性原则应用于顺序设置中。

    

    机器学习模型的决策可能对时间产生长期影响，因此长期公平性成为一个重要考虑因素。先前的研究表明，在忽略长期影响时，简单地应用静态公平性准则实际上会加剧偏见。为了明确解决顺序决策中的偏见，最近的研究在马尔可夫决策过程（MDP）框架中制定了长期公平性概念。他们将长期偏见定义为每个时间步上静态偏见的总和。然而，我们证明了简单地对逐步偏见求和可能导致虚假的公平感，因为它未考虑到转换过程中不同时间步的重要性差异。在这项工作中，我们介绍了一种称为Equal Long-term Benefit Rate（ELBERT）的长期公平性概念，它明确考虑到不同时间步的变化重要性，并将静态公平性原则应用于顺序设置中。此外，我们还展示了长期收益的策略梯度的性质。

    Decisions made by machine learning models may have lasting impacts over time, making long-term fairness a crucial consideration. It has been shown that when ignoring the long-term effect, naively imposing fairness criterion in static settings can actually exacerbate bias over time. To explicitly address biases in sequential decision-making, recent works formulate long-term fairness notions in Markov Decision Process (MDP) framework. They define the long-term bias to be the sum of static bias over each time step. However, we demonstrate that naively summing up the step-wise bias can cause a false sense of fairness since it fails to consider the importance difference of different time steps during transition. In this work, we introduce a long-term fairness notion called Equal Long-term Benefit Rate (ELBERT), which explicitly considers varying temporal importance and adapts static fairness principles to the sequential setting. Moreover, we show that the policy gradient of Long-term Benefi
    
[^75]: 大型语言模型作为优化器

    Large Language Models as Optimizers. (arXiv:2309.03409v1 [cs.LG])

    [http://arxiv.org/abs/2309.03409](http://arxiv.org/abs/2309.03409)

    本论文提出了一种简单有效的方法，利用大型语言模型(LLMs)作为优化器，通过自然语言描述优化任务。经过实验证明，该方法在线性回归和旅行推销员问题上表现出色，并且优化的最佳提示超过了人为设计的提示。

    

    优化是无处不在的。虽然基于导数的算法在各种问题上是强大的工具，但是没有梯度对许多实际应用提出了挑战。在这项工作中，我们提出了一种简单有效的方法，利用大型语言模型(LLMs)作为优化器，其中优化任务以自然语言形式描述。在每一次优化步骤中，LLM从包含先前生成的解与其值的提示中生成新的解，然后对新的解进行评估并添加到提示中，用于下一次优化步骤。我们首先展示了OPRO在线性回归和旅行推销员问题上的应用，然后转向提示优化，目标是找到能最大化任务准确性的指令。通过使用各种LLM，我们证明了OPRO优化的最佳提示在GSM8K上击败了人为设计的提示高达8%，在Big-Bench Hard任务上击败了人为设计的提示高达50%。

    Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to prompt optimization where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks.
    
[^76]: 基于社区的分级正类-未标记（PU）模型融合用于慢性病预测

    Community-Based Hierarchical Positive-Unlabeled (PU) Model Fusion for Chronic Disease Prediction. (arXiv:2309.03386v1 [cs.LG])

    [http://arxiv.org/abs/2309.03386](http://arxiv.org/abs/2309.03386)

    本论文介绍了一种基于社区的分级正类-未标记（PU）模型融合方法，用于慢性病预测。该方法考虑了不同人群之间的差异，并通过构建PU模型树和聚合其输出来进行预测。

    

    正类-未标记（PU）学习是二分类问题的一个挑战，在这种问题中存在大量未标记数据和少量正类数据实例，可以用于慢性病筛查问题。最先进的PU学习方法已经导致了各种风险估计器的发展，然而它们忽视了不同人群之间的差异。为了解决这个问题，我们提出了一种新颖的正类-未标记学习树（PUtree）算法。PUtree旨在考虑社区，如不同的年龄或收入段，在慢性病预测任务中。我们提出了一种新的二分类决策方法，它以层次方式构建基于社区的PU模型，然后聚合它们的输出。我们的方法可以解释树上每个PU模型，以优化非叶节点的分裂。此外，一种掩码恢复数据增强策略使得模型在个体上能进行充分训练。

    Positive-Unlabeled (PU) Learning is a challenge presented by binary classification problems where there is an abundance of unlabeled data along with a small number of positive data instances, which can be used to address chronic disease screening problem. State-of-the-art PU learning methods have resulted in the development of various risk estimators, yet they neglect the differences among distinct populations. To address this issue, we present a novel Positive-Unlabeled Learning Tree (PUtree) algorithm. PUtree is designed to take into account communities such as different age or income brackets, in tasks of chronic disease prediction. We propose a novel approach for binary decision-making, which hierarchically builds community-based PU models and then aggregates their deliverables. Our method can explicate each PU model on the tree for the optimized non-leaf PU node splitting. Furthermore, a mask-recovery data augmentation strategy enables sufficient training of the model in individua
    
[^77]: ViewMix：自监督学习中的稳健表示增强方法

    ViewMix: Augmentation for Robust Representation in Self-Supervised Learning. (arXiv:2309.03360v1 [cs.CV])

    [http://arxiv.org/abs/2309.03360](http://arxiv.org/abs/2309.03360)

    这项研究提出了一种名为ViewMix的增强策略，专门用于自监督学习，该策略可以改善模型的表示学习能力，并提高模型的稳健性。

    

    基于联合嵌入架构的自监督学习方法认为数据增强的组合是其强大表示学习能力的关键因素。尽管在监督方法中，区域性丢失策略已被证明可以引导模型关注物体的较不明显部分，但在自监督方法中尚未采用这种方法生成正样本对。这是因为区域性丢失方法不适用于自监督方法的输入采样过程。然而，从正样本中丢弃信息性像素可能导致训练效率低下，用不同对象的补丁替换特定对象的补丁可以使模型无法最大化不同正样本之间的一致性。此外，联合嵌入表示学习方法尚未将稳健性作为其主要训练目标。为此，我们提出了ViewMix增强策略，特别针对自监督学习设计。

    Joint Embedding Architecture-based self-supervised learning methods have attributed the composition of data augmentations as a crucial factor for their strong representation learning capabilities. While regional dropout strategies have proven to guide models to focus on lesser indicative parts of the objects in supervised methods, it hasn't been adopted by self-supervised methods for generating positive pairs. This is because the regional dropout methods are not suitable for the input sampling process of the self-supervised methodology. Whereas dropping informative pixels from the positive pairs can result in inefficient training, replacing patches of a specific object with a different one can steer the model from maximizing the agreement between different positive pairs. Moreover, joint embedding representation learning methods have not made robustness their primary training outcome. To this end, we propose the ViewMix augmentation policy, specially designed for self-supervised learni
    
[^78]: 集成线性插值器: 集成的作用

    Ensemble linear interpolators: The role of ensembling. (arXiv:2309.03354v1 [stat.ML])

    [http://arxiv.org/abs/2309.03354](http://arxiv.org/abs/2309.03354)

    本文研究了集成线性插值器如何稳定和提升个体插值器的泛化性能，并引入了基于乘数自助法的袋装最小二乘估计器。在比例区域内，研究了简化和袋装的外样本预测风险。

    

    插值器是不稳定的。例如，当处理噪声数据时，最小l2范数最小二乘插值器的测试误差会无界增长。本文研究集成如何稳定和提升个体插值器的泛化性能，通过对外样本预测风险进行衡量。我们以袋装线性插值器为重点，因为袋装是一种流行的基于随机化的集成方法，可以并行实现。我们引入基于乘数自助法的袋装最小二乘估计器，可以将其表达为简化最小二乘估计器的平均值。所提出的乘数自助法包含了经典的有放回自助法作为一种特殊情况，以及更有趣的变体，我们称之为伯努利自助法。在样本大小与特征维度成正比的比例区域内，我们研究了简化和袋装的外样本预测风险。

    Interpolators are unstable. For example, the mininum $\ell_2$ norm least square interpolator exhibits unbounded test errors when dealing with noisy data. In this paper, we study how ensemble stabilizes and thus improves the generalization performance, measured by the out-of-sample prediction risk, of an individual interpolator. We focus on bagged linear interpolators, as bagging is a popular randomization-based ensemble method that can be implemented in parallel. We introduce the multiplier-bootstrap-based bagged least square estimator, which can then be formulated as an average of the sketched least square estimators. The proposed multiplier bootstrap encompasses the classical bootstrap with replacement as a special case, along with a more intriguing variant which we call the Bernoulli bootstrap.  Focusing on the proportional regime where the sample size scales proportionally with the feature dimensionality, we investigate the out-of-sample prediction risks of the sketched and bagged 
    
[^79]: 数字视频中的源摄像机识别与检测的盲取证技术

    Source Camera Identification and Detection in Digital Videos through Blind Forensics. (arXiv:2309.03353v1 [cs.CV])

    [http://arxiv.org/abs/2309.03353](http://arxiv.org/abs/2309.03353)

    本文提出了一种盲取证技术，用于数字视频中源摄像机的识别与检测，通过机器学习的特征提取、特征选择和源分类，有效地确定视频的原始源。

    

    数字视频中的源摄像机识别是将未知数字视频与其源设备关联起来，在可能设备的有限集合内进行。存在的源检测技术试图在视频中找到实际源的指纹，采用PRNU（光响应非均匀性）的形式，并将其与每个可能设备的SPN（传感器模式噪声）进行匹配。最高的相关性表示正确的源。我们采用基于特征的机器学习方法来研究视频源的识别问题。通过特征提取、特征选择和后续源分类，本文提出了一种盲取证技术，用于视频源的认证和识别。主要目的是确定视频的声明源是否为其原始源，如果不是，我们将识别其原始源。我们的实验结果证明了所提方法相对于传统方法的高效性。

    Source camera identification in digital videos is the problem of associating an unknown digital video with its source device, within a closed set of possible devices. The existing techniques in source detection of digital videos try to find a fingerprint of the actual source in the video in form of PRNU (Photo Response Non--Uniformity), and match it against the SPN (Sensor Pattern Noise) of each possible device. The highest correlation indicates the correct source. We investigate the problem of identifying a video source through a feature based approach using machine learning. In this paper, we present a blind forensic technique of video source authentication and identification, based on feature extraction, feature selection and subsequent source classification. The main aim is to determine whether a claimed source for a video is actually its original source. If not, we identify its original source. Our experimental results prove the efficiency of the proposed method compared to tradit
    
[^80]: 使用神经网络快速估计高分辨率SAR图像的粗糙度

    Using Neural Networks for Fast SAR Roughness Estimation of High Resolution Images. (arXiv:2309.03351v1 [cs.CV])

    [http://arxiv.org/abs/2309.03351](http://arxiv.org/abs/2309.03351)

    本论文提出了一种使用神经网络的框架用于快速估计高分辨率SAR图像的粗糙度参数。与传统方法相比，该方法更快速、误差更小且更可靠。

    

    合成孔径雷达（SAR）图像的分析是遥感应用中的重要步骤，由于其固有的散斑噪声，这是一个具有挑战性的问题。一种常见的解决方案是使用$G_I^0$分布模型化数据并提取其粗糙度信息，这样可以在后续图像处理任务中使用，如分割、分类和解释。这需要从SAR数据中快速而可靠地估计粗糙度参数，特别是对于高分辨率图像。不幸的是，传统的参数估计方法速度慢且容易出现估计失败。在这项工作中，我们提出了一种基于神经网络的估计框架，首先学习如何预测$G_I^0$样本的基本参数，然后可以用于估计未见数据的粗糙度。我们发现，这种方法导致了一个比传统估计方法更快、误差更小且难以出错的估计器。

    The analysis of Synthetic Aperture Radar (SAR) imagery is an important step in remote sensing applications, and it is a challenging problem due to its inherent speckle noise. One typical solution is to model the data using the $G_I^0$ distribution and extract its roughness information, which in turn can be used in posterior imaging tasks, such as segmentation, classification and interpretation. This leads to the need of quick and reliable estimation of the roughness parameter from SAR data, especially with high resolution images. Unfortunately, traditional parameter estimation procedures are slow and prone to estimation failures. In this work, we proposed a neural network-based estimation framework that first learns how to predict underlying parameters of $G_I^0$ samples and then can be used to estimate the roughness of unseen data. We show that this approach leads to an estimator that is quicker, yields less estimation error and is less prone to failures than the traditional estimatio
    
[^81]: 中继扩散：统一不同分辨率下的图像合成扩散过程

    Relay Diffusion: Unifying diffusion process across resolutions for image synthesis. (arXiv:2309.03350v1 [cs.CV])

    [http://arxiv.org/abs/2309.03350](http://arxiv.org/abs/2309.03350)

    中继扩散模型（RDM）通过模糊扩散和块噪声将低分辨率的图像或噪声转化为等效的高分辨率图像，实现了在不同分辨率下无需重新开始的扩散过程，并在图像合成方面取得了最先进的性能。

    

    扩散模型在图像合成方面取得了巨大成功，但在高分辨率生成方面仍面临挑战。通过离散余弦变换，我们发现主要原因在于“在高分辨率上相同的噪声水平会导致频域中较高的信噪比”。在这项工作中，我们提出了中继扩散模型（RDM），通过模糊扩散和块噪声将低分辨率的图像或噪声转化为等效的高分辨率图像，以供扩散模型使用。因此，扩散过程可以在任何新的分辨率或模型上无缝地继续进行，而无需从纯噪声或低分辨率条件重新开始。RDM在CelebA-HQ和ImageNet 256×256上都实现了最先进的FID和sFID，大大超过了之前的ADM、LDM和DiT等工作。所有的代码和检查点都以开源方式在https://github.com/THUDM/RelayDiffusion上公开。

    Diffusion models achieved great success in image synthesis, but still face challenges in high-resolution generation. Through the lens of discrete cosine transformation, we find the main reason is that \emph{the same noise level on a higher resolution results in a higher Signal-to-Noise Ratio in the frequency domain}. In this work, we present Relay Diffusion Model (RDM), which transfers a low-resolution image or noise into an equivalent high-resolution one for diffusion model via blurring diffusion and block noise. Therefore, the diffusion process can continue seamlessly in any new resolution or model without restarting from pure noise or low-resolution conditioning. RDM achieves state-of-the-art FID on CelebA-HQ and sFID on ImageNet 256$\times$256, surpassing previous works such as ADM, LDM and DiT by a large margin. All the codes and checkpoints are open-sourced at \url{https://github.com/THUDM/RelayDiffusion}.
    
[^82]: REBOOT: 重用数据以引导高效的现实世界灵巧操纵

    REBOOT: Reuse Data for Bootstrapping Efficient Real-World Dexterous Manipulation. (arXiv:2309.03322v1 [cs.LG])

    [http://arxiv.org/abs/2309.03322](http://arxiv.org/abs/2309.03322)

    本论文介绍了一种使用强化学习学习灵巧操纵技能的高效系统，通过结合样本高效强化学习和回放缓冲区引导，实现了重用数据以降低真实世界应用中的挑战。

    

    对于涉及接触密集交互的灵巧操纵任务，模型驱动的控制系统和模仿学习算法都面临着巨大的挑战。复杂性来自于多指机器人手需要动态建立和断开接触、平衡非伸手持力并控制大量自由度。强化学习（RL）由于其广泛适用性和自主获取最佳操纵策略的能力而具有很大的潜力。然而，它在真实世界的应用常常受到生成大量样本、重置环境和获取奖励信号的限制。在这项工作中，我们引入了一种用于学习具有RL的灵巧操纵技能的高效系统，以解决这些挑战。我们方法的主要思想是将最近在样本高效RL和回放缓冲区引导方面的进展相结合。这种组合使我们能够利用来自不同任务或物体的数据作为输入。

    Dexterous manipulation tasks involving contact-rich interactions pose a significant challenge for both model-based control systems and imitation learning algorithms. The complexity arises from the need for multi-fingered robotic hands to dynamically establish and break contacts, balance non-prehensile forces, and control large degrees of freedom. Reinforcement learning (RL) offers a promising approach due to its general applicability and capacity to autonomously acquire optimal manipulation strategies. However, its real-world application is often hindered by the necessity to generate a large number of samples, reset the environment, and obtain reward signals. In this work, we introduce an efficient system for learning dexterous manipulation skills with RL to alleviate these challenges. The main idea of our approach is the integration of recent advances in sample-efficient RL and replay buffer bootstrapping. This combination allows us to utilize data from different tasks or objects as a
    
[^83]: 通过机器学习进行适应度近似

    Fitness Approximation through Machine Learning. (arXiv:2309.03318v1 [cs.NE])

    [http://arxiv.org/abs/2309.03318](http://arxiv.org/abs/2309.03318)

    我们提出了一种使用机器学习模型在遗传算法中进行适应度近似的方法。实验结果表明，这种方法显著提高了进化运行时间，并且适应度得分要么与完全运行的遗传算法相同，要么稍微低一点。

    

    我们提出了一种新颖的方法，使用机器学习模型在遗传算法中进行适应度近似，重点是在Gymnasium（游戏）模拟器中的进化代理上 - 在这里适应度计算是昂贵的。我们维护一个采样个体及其实际适应度得分的数据集，并在整个进化过程中不断更新一个适应度近似的机器学习模型。我们比较了不同的方法：1）在实际适应度和近似适应度之间切换，2）对种群进行采样，以及3）加权采样样本。实验结果表明，在适应度计算的近似比例取决于完全运行GA时，我们的方法显著提高了进化运行时间，并且适应度得分要么与完全运行的GA相同，要么稍微低一点。我们的方法是通用的，可以很容易地应用于许多不同的领域。

    We present a novel approach to performing fitness approximation in genetic algorithms (GAs) using machine-learning (ML) models, focusing on evolutionary agents in Gymnasium (game) simulators -- where fitness computation is costly. Maintaining a dataset of sampled individuals along with their actual fitness scores, we continually update throughout an evolutionary run a fitness-approximation ML model. We compare different methods for: 1) switching between actual and approximate fitness, 2) sampling the population, and 3) weighting the samples. Experimental findings demonstrate significant improvement in evolutionary runtimes, with fitness scores that are either identical or slightly lower than that of the fully run GA -- depending on the ratio of approximate-to-actual-fitness computation. Our approach is generic and can be easily applied to many different domains.
    
[^84]: 机器人乒乓球：一个高速学习系统的案例研究

    Robotic Table Tennis: A Case Study into a High Speed Learning System. (arXiv:2309.03315v1 [cs.RO])

    [http://arxiv.org/abs/2309.03315](http://arxiv.org/abs/2309.03315)

    本研究深入研究了一种真实世界的机器人学习系统，该系统能够与人类进行数百次的乒乓球回合，并且能够精确地将球返回到预定的目标位置。该系统整合了感知子系统、高速低延迟机器人控制器、仿真范例、自动重置真实世界环境等功能，并对系统的设计决策和重要性进行了详细描述。

    

    我们展示了一个真实世界的机器人学习系统的深入研究，此前的工作已经表明该系统能够与人类进行数百次的乒乓球回合，并且能够精确地将球返回到预定的目标位置。该系统结合了高度优化的感知子系统、高速低延迟的机器人控制器、防止现实世界中损坏并能够进行零-shot转移策略训练的仿真范例，以及自动重置真实世界环境，使自主训练和评估在物理机器人上成为可能。我们通过详细描述整个系统，包括通常不广泛传播的大量设计决策，并结合一系列研究来阐明缓解各种延迟源的重要性、考虑训练和部署分布变化、感知系统的稳健性、策略超参数的敏感性和动作空间选择等方面的重要性。视频展示了系统的组件。

    We present a deep-dive into a real-world robotic learning system that, in previous work, was shown to be capable of hundreds of table tennis rallies with a human and has the ability to precisely return the ball to desired targets. This system puts together a highly optimized perception subsystem, a high-speed low-latency robot controller, a simulation paradigm that can prevent damage in the real world and also train policies for zero-shot transfer, and automated real world environment resets that enable autonomous training and evaluation on physical robots. We complement a complete system description, including numerous design decisions that are typically not widely disseminated, with a collection of studies that clarify the importance of mitigating various sources of latency, accounting for training and deployment distribution shifts, robustness of the perception system, sensitivity to policy hyper-parameters, and choice of action space. A video demonstrating the components of the sys
    
[^85]: 使用多目标遗传算法生成量子特征映射的方法

    Generating quantum feature maps using multi-objective genetic algorithm. (arXiv:2309.03307v1 [quant-ph])

    [http://arxiv.org/abs/2309.03307](http://arxiv.org/abs/2309.03307)

    本文介绍了一种使用多目标遗传算法生成量子特征映射的方法，以实现对高维希尔伯特空间的访问，并在优化电路配置时同时考虑分类准确性和门成本。实验结果显示最佳电路配置中纠缠门需要相应的数量，与之前研究相反。同时，我们还提出了使用数据的可分离性指数确定最佳配置的方法。

    

    我们提出了一种新的方法，通过使用多目标遗传算法，能够高效地生成用于量子增强支持向量机的量子特征映射，从而实现对高维希尔伯特空间的访问。我们的方法同时最大化分类准确性，同时最小化量子特征映射电路的本地门成本和非本地门成本。为了实现这一目标，我们为本地门和纠缠门定义了不同的适应度函数。与经典分类器的比较有助于理解使用量子机器学习的优势。令人惊讶的是，我们的实验揭示了量子核方法的最佳电路配置中包含了相应数量的非本地门用于纠缠，与之前的文献相反，之前的文献中非本地门被大部分抑制。此外，我们还证明数据的可分离性指数可以有效地用于确定最佳的配置。

    We present a novel approach for efficiently generating quantum feature maps for quantum-enhanced support vector machines, a kernel-based classifier, enabling access to high-dimensional Hilbert space. Our method employs a multi-objective genetic algorithm that simultaneously maximizes classification accuracy while minimizing both the local and non-local gate costs of the quantum feature map's circuit. To achieve this, we define distinct fitness functions for local gates and entanglement gates. Comparisons with classical classifiers are given in order to understand the advantages of using quantum machine learning. Surprisingly, our experiments reveal that the optimal configuration of quantum circuits for the quantum kernel method incorporates a proportional number of non-local gates for entanglement, contrary to previous literature where non-local gates were largely suppressed.  Furthermore, we demonstrate that the separability indexes of data can be effectively leveraged to determine th
    
[^86]: 通过递归分解实现可扩展学习入侵响应

    Scalable Learning of Intrusion Responses through Recursive Decomposition. (arXiv:2309.03292v1 [eess.SY])

    [http://arxiv.org/abs/2309.03292](http://arxiv.org/abs/2309.03292)

    本文提出了一种通过递归分解方法实现可扩展学习入侵响应的技术，该技术通过解决并行子游戏和计算阈值结构的最佳响应策略来提高效率。

    

    我们研究了针对IT基础设施的自动化入侵应对，并将攻击者和防御者之间的交互形式建模为部分观测的随机游戏。为了解决这个游戏，我们采用了一种方法，攻击和防御策略通过强化学习和自我对弈进行协同演化，以达到平衡。之前的研究中提出的解决方案证明了这种方法对于小型基础设施的可行性，但面对实际情境由于基础设施规模的指数级增长而无法扩展。为了解决这个问题，我们提出了一种将游戏递归分解成可以并行解决的子游戏的方法。应用最优停止理论，我们证明了这些子游戏中的最佳响应策略具有阈值结构，这允许我们高效地计算它们。为了解决分解的游戏，我们提出了一个名为Decompositional Fictitious Self-Play (DFSP) 的算法，通过随机自我对弈学习纳什均衡。

    We study automated intrusion response for an IT infrastructure and formulate the interaction between an attacker and a defender as a partially observed stochastic game. To solve the game we follow an approach where attack and defense strategies co-evolve through reinforcement learning and self-play toward an equilibrium. Solutions proposed in previous work prove the feasibility of this approach for small infrastructures but do not scale to realistic scenarios due to the exponential growth in computational complexity with the infrastructure size. We address this problem by introducing a method that recursively decomposes the game into subgames which can be solved in parallel. Applying optimal stopping theory we show that the best response strategies in these subgames exhibit threshold structures, which allows us to compute them efficiently. To solve the decomposed game we introduce an algorithm called Decompositional Fictitious Self-Play (DFSP), which learns Nash equilibria through stoc
    
[^87]: 让量子神经网络选择自己的频率

    Let Quantum Neural Networks Choose Their Own Frequencies. (arXiv:2309.03279v1 [quant-ph])

    [http://arxiv.org/abs/2309.03279](http://arxiv.org/abs/2309.03279)

    该论文提出了一种可训练频率的量子模型，通过在生成器中引入可训练参数，使得模型能够学习具有所需属性的生成器，包括非常规间隔频率和灵活的频谱丰富性。实验证明该方法在解决Navier-Stokes方程时具有较高的准确性。

    

    参数化量子电路作为机器学习模型，通常可以通过将输入特征的部分傅立叶级数表示来描述，其中频率由特征映射的生成哈密顿量唯一确定。通常情况下，这些数据编码生成器是提前选择的，固定了可以表示的函数空间。在这项工作中，我们考虑将量子模型推广到生成器中包括一组可训练参数的情况，从而得到一个可训练频率的量子模型。我们通过数值实验证明了可训练频率模型如何学习具有所需属性的生成器，包括其频谱中的非常规间隔频率和灵活的频谱丰富性。最后，我们展示了我们方法在现实世界中的有效性，通过使用仅对每个编码操作添加一个参数的可训练频率模型，提高了求解Navier-Stokes方程的准确性。

    Parameterized quantum circuits as machine learning models are typically well described by their representation as a partial Fourier series of the input features, with frequencies uniquely determined by the feature map's generator Hamiltonians. Ordinarily, these data-encoding generators are chosen in advance, fixing the space of functions that can be represented. In this work we consider a generalization of quantum models to include a set of trainable parameters in the generator, leading to a trainable frequency (TF) quantum model. We numerically demonstrate how TF models can learn generators with desirable properties for solving the task at hand, including non-regularly spaced frequencies in their spectra and flexible spectral richness. Finally, we showcase the real-world effectiveness of our approach, demonstrating an improved accuracy in solving the Navier-Stokes equations using a TF model with only a single parameter added to each encoding operation. Since TF models encompass conven
    
[^88]: 临时归纳路径神经网络用于时间知识图推理

    Temporal Inductive Path Neural Network for Temporal Knowledge Graph Reasoning. (arXiv:2309.03251v1 [cs.AI])

    [http://arxiv.org/abs/2309.03251](http://arxiv.org/abs/2309.03251)

    本论文提出了一种临时归纳路径神经网络（TiPNN）用于时间知识图的推理，采用实体独立的角度建模历史信息，并通过临时归纳路径提取结构和时间信息。

    

    时间知识图（TKG）是传统知识图（KG）的扩展，融入了时间维度。在TKGs上进行推理是一个关键任务，旨在基于历史事件预测未来事实。关键挑战在于揭示历史子图和时间模式中的结构依赖关系。大多数现有方法依靠实体建模来模拟TKGs，因为图中的节点在知识表示中起着至关重要的作用。然而，现实场景通常涉及大量实体，并且随着时间的推移会出现新实体。这使得依赖于实体的方法很难应对大量实体，并且有效处理新出现的实体也成为一个重要的挑战。因此，我们提出了一种临时归纳路径神经网络（TiPNN），它以实体独立的角度对历史信息进行建模。具体而言，TiPNN采用了一个统一的图，名为历史时间图，来建模历史信息，并通过临时归纳路径提取结构和时间信息。

    Temporal Knowledge Graph (TKG) is an extension of traditional Knowledge Graph (KG) that incorporates the dimension of time. Reasoning on TKGs is a crucial task that aims to predict future facts based on historical occurrences. The key challenge lies in uncovering structural dependencies within historical subgraphs and temporal patterns. Most existing approaches model TKGs relying on entity modeling, as nodes in the graph play a crucial role in knowledge representation. However, the real-world scenario often involves an extensive number of entities, with new entities emerging over time. This makes it challenging for entity-dependent methods to cope with extensive volumes of entities, and effectively handling newly emerging entities also becomes a significant challenge. Therefore, we propose Temporal Inductive Path Neural Network (TiPNN), which models historical information in an entity-independent perspective. Specifically, TiPNN adopts a unified graph, namely history temporal graph, to
    
[^89]: 图论在高级地理空间研究中的应用

    Graph Theory Applications in Advanced Geospatial Research. (arXiv:2309.03249v1 [cs.LG])

    [http://arxiv.org/abs/2309.03249](http://arxiv.org/abs/2309.03249)

    本论文探讨了图论算法在地理空间科学中的应用，重点介绍了它们在网络分析、空间连接性、地理信息系统以及各种空间问题解决方案中的作用，并列举了在这一领域中实施的广泛研究、创新技术和方法。

    

    地理空间科学涵盖了从环境监测、交通到基础设施规划、以及基于位置的分析和服务等广泛应用。数学中的图论算法由于其高效地建模和分析空间关系的能力，在这些领域中已经成为不可或缺的工具。本技术报告探讨了图论算法在地理空间科学中的应用，重点介绍了它们在网络分析、空间连接性、地理信息系统以及各种空间问题解决方案中的作用。它提供了对图论的关键概念和算法的全面理解，以助于建模过程。该报告深入分析了图论在应对现实世界地理空间挑战和机遇中的实际意义。它还列举了在这一领域中实施的广泛研究、创新技术和方法。

    Geospatial sciences include a wide range of applications, from environmental monitoring transportation to infrastructure planning, as well as location-based analysis and services. Graph theory algorithms in mathematics have emerged as indispensable tools in these domains due to their capability to model and analyse spatial relationships efficiently. This technical report explores the applications of graph theory algorithms in geospatial sciences, highlighting their role in network analysis, spatial connectivity, geographic information systems, and various other spatial problem-solving scenarios. It provides a comprehensive idea about the key concepts and algorithms of graph theory that assist the modelling processes. The report provides insights into the practical significance of graph theory in addressing real-world geospatial challenges and opportunities. It lists the extensive research, innovative technologies and methodologies implemented in this field.
    
[^90]: EvoCLINICAL: 搭载主动迁移学习的演进式病癌登记系统的网络病癌双生体

    EvoCLINICAL: Evolving Cyber-Cyber Digital Twin with Active Transfer Learning for Automated Cancer Registry System. (arXiv:2309.03246v1 [cs.LG])

    [http://arxiv.org/abs/2309.03246](http://arxiv.org/abs/2309.03246)

    EvoCLINICAL提出了一种演进式病癌登记系统的网络病癌双生体，通过主动迁移学习实现了CCDT与真实系统的同步，并为GURI的运行状态提供了各种实验和高级分析。

    

    挪威癌症登记处（CRN）通过从挪威的各个医疗实体（例如医学实验室和医院）接收癌症信息来收集关于癌症患者的信息。这些信息由自动化癌症登记系统GURI进行验证。它的正常运行对于癌症研究和向利益相关者提供关键的癌症统计数据至关重要。为GURI构建一个网络病癌双生体（CCDT）可以在不与真实系统进行深入交互的情况下，促进对GURI运行状态的各种实验和高级分析。然而，由于新的医学诊断和治疗、技术进步等原因，GURI不断发展。相应地，CCDT也应该随之演进以与GURI同步。实现这种同步的一个关键挑战是演进的CCDT需要由新的GURI标记的丰富数据。为了解决这个挑战，我们提出了EvoCLINICAL，它考虑了为GURI开发的CCDT

    The Cancer Registry of Norway (CRN) collects information on cancer patients by receiving cancer messages from different medical entities (e.g., medical labs, and hospitals) in Norway. Such messages are validated by an automated cancer registry system: GURI. Its correct operation is crucial since it lays the foundation for cancer research and provides critical cancer-related statistics to its stakeholders. Constructing a cyber-cyber digital twin (CCDT) for GURI can facilitate various experiments and advanced analyses of the operational state of GURI without requiring intensive interactions with the real system. However, GURI constantly evolves due to novel medical diagnostics and treatment, technological advances, etc. Accordingly, CCDT should evolve as well to synchronize with GURI. A key challenge of achieving such synchronization is that evolving CCDT needs abundant data labelled by the new GURI. To tackle this challenge, we propose EvoCLINICAL, which considers the CCDT developed for
    
[^91]: 在流模型中测试分布的属性

    Testing properties of distributions in the streaming model. (arXiv:2309.03245v1 [cs.DS])

    [http://arxiv.org/abs/2309.03245](http://arxiv.org/abs/2309.03245)

    本文研究了在有限内存条件下，在流模型中测试分布的属性问题。我们提供了对于标准访问模型和条件访问模型的折衷结果。此外，我们还展示了一种有效学习单调分布的方法，并将其扩展到更大的可分解分布类。

    

    我们研究了在有限内存条件下，标准访问模型和条件访问模型中的分布测试问题。在这两种场景中，样本以在线方式出现，目标是使用最优数量的样本在给定时间内存储的内存约束下测试分布的属性。首先，我们提供了在样本按条件访问预选的情况下，在样本复杂度和空间复杂度之间的折衷。然后，我们展示了我们可以有效地学习具有几乎最优样本数量的内存约束的单调分布的简洁表示。我们还展示了单调分布算法可以扩展到更大的可分解分布类。

    We study distribution testing in the standard access model and the conditional access model when the memory available to the testing algorithm is bounded. In both scenarios, the samples appear in an online fashion and the goal is to test the properties of distribution using an optimal number of samples subject to a memory constraint on how many samples can be stored at a given time. First, we provide a trade-off between the sample complexity and the space complexity for testing identity when the samples are drawn according to the conditional access oracle. We then show that we can learn a succinct representation of a monotone distribution efficiently with a memory constraint on the number of samples that are stored that is almost optimal. We also show that the algorithm for monotone distributions can be extended to a larger class of decomposable distributions.
    
[^92]: EGIC:增强的低位速率生成图像压缩方法在语义分割的指导下

    EGIC: Enhanced Low-Bit-Rate Generative Image Compression Guided by Semantic Segmentation. (arXiv:2309.03244v1 [eess.IV])

    [http://arxiv.org/abs/2309.03244](http://arxiv.org/abs/2309.03244)

    EGIC是一种增强的低位速率生成图像压缩方法，通过语义分割提供指导。它在失真感知和失真方向基线方法上表现优越，并具有较小的模型参数和优秀的插值特性。

    

    我们引入了一种新颖的生成图像压缩方法EGIC，它允许从一个单一模型有效地遍历失真感知曲线。具体而言，我们提出了一种隐式编码的图像插值变体，用于预测在MSE优化和GAN优化解码器输出之间的残差。在接收端，用户可以控制残差对基于GAN的重建的影响。结合改进的基于GAN的构建块，EGIC在感知导向和失真导向的基线方法（包括HiFiC，MRIC和DIRAC）上表现优于大多数方法，在失真端与VTM-20.0几乎相当。EGIC实现简单，非常轻量级（与HiFiC相比，模型参数只有0.18倍），并提供优异的插值特性，这使得它成为针对低位范围的实际应用的有希望的候选方法。

    We introduce EGIC, a novel generative image compression method that allows traversing the distortion-perception curve efficiently from a single model. Specifically, we propose an implicitly encoded variant of image interpolation that predicts the residual between a MSE-optimized and GAN-optimized decoder output. On the receiver side, the user can then control the impact of the residual on the GAN-based reconstruction. Together with improved GAN-based building blocks, EGIC outperforms a wide-variety of perception-oriented and distortion-oriented baselines, including HiFiC, MRIC and DIRAC, while performing almost on par with VTM-20.0 on the distortion end. EGIC is simple to implement, very lightweight (e.g. 0.18x model parameters compared to HiFiC) and provides excellent interpolation characteristics, which makes it a promising candidate for practical applications targeting the low bit range.
    
[^93]: 通过AutoBA实现自动化生物信息学分析

    Automated Bioinformatics Analysis via AutoBA. (arXiv:2309.03242v1 [q-bio.GN])

    [http://arxiv.org/abs/2309.03242](http://arxiv.org/abs/2309.03242)

    AutoBA是一个基于大型语言模型的自主AI代理程序，通过最少的用户输入简化生物信息学分析过程，并提供详细的逐步计划。经过验证，AutoBA在各种组学分析案例中表现出健壮性和适应性，同时保护数据隐私。

    

    随着组学数据的快速增长和演变，对处理分析的简化和适应性工具的需求不断增长。为满足这一需求，我们引入了Auto Bioinformatics Analysis (AutoBA)，这是一个基于大型语言模型的自主AI代理程序，专门设计用于传统组学数据分析。AutoBA通过需要最少的用户输入来简化分析过程，并提供详细的逐步计划，用于完成各种生物信息学任务。经过专家生物信息学家的严格验证，AutoBA的健壮性和适应性在各种组学分析案例中得到证实，包括全基因组测序（WGS），RNA测序（RNA-seq），单细胞RNA测序，ChIP-seq和空间转录组学。AutoBA的独特能力是根据输入数据的变化自设计分析流程，进一步凸显了其多功能性。与在线生物信息学服务相比，AutoBA在本地部署分析，保护数据隐私。

    With the fast-growing and evolving omics data, the demand for streamlined and adaptable tools to handle the analysis continues to grow. In response to this need, we introduce Auto Bioinformatics Analysis (AutoBA), an autonomous AI agent based on a large language model designed explicitly for conventional omics data analysis. AutoBA simplifies the analytical process by requiring minimal user input while delivering detailed step-by-step plans for various bioinformatics tasks. Through rigorous validation by expert bioinformaticians, AutoBA's robustness and adaptability are affirmed across a diverse range of omics analysis cases, including whole genome sequencing (WGS), RNA sequencing (RNA-seq), single-cell RNA-seq, ChIP-seq, and spatial transcriptomics. AutoBA's unique capacity to self-design analysis processes based on input data variations further underscores its versatility. Compared with online bioinformatic services, AutoBA deploys the analysis locally, preserving data privacy. Moreo
    
[^94]: GPT可以在没有计算器的情况下解决数学问题

    GPT Can Solve Mathematical Problems Without a Calculator. (arXiv:2309.03241v1 [cs.LG])

    [http://arxiv.org/abs/2309.03241](http://arxiv.org/abs/2309.03241)

    本研究表明，通过充分训练，一个20亿参数的语言模型可以在没有计算器工具的情况下以几乎100%的准确度执行多位数的算术运算，超越了之前的GPT-4。这项研究还通过在附加的多步骤算术运算和数学问题的数据集上进行微调，展示了一个与GPT-4在中文数学问题上相似的性能。

    

    以往的研究通常认为大型语言模型无法在没有计算器工具的情况下准确执行算术运算，特别是超过8位数字的乘法，以及涉及小数和分数的运算。本文旨在挑战这种误解。通过充分的训练数据，一个拥有20亿参数的语言模型可以以近乎100%的准确度执行多位数的算术运算，而且没有数据泄露，显著超过了GPT-4（其多位数乘法准确率仅为4.3%）。我们还演示了我们的MathGLM，它是通过在包含了文本描述的附加多步骤算术运算和数学问题的数据集上从GLM-10B微调而成的，它在一个包含5000个样本的中文数学问题测试集上的表现与GPT-4相似。

    Previous studies have typically assumed that large language models are unable to accurately perform arithmetic operations, particularly multiplication of >8 digits, and operations involving decimals and fractions, without the use of calculator tools. This paper aims to challenge this misconception. With sufficient training data, a 2 billion-parameter language model can accurately perform multi-digit arithmetic operations with almost 100% accuracy without data leakage, significantly surpassing GPT-4 (whose multi-digit multiplication accuracy is only 4.3%). We also demonstrate that our MathGLM, fine-tuned from GLM-10B on a dataset with additional multi-step arithmetic operations and math problems described in text, achieves similar performance to GPT-4 on a 5,000-samples Chinese math problem test set.
    
[^95]: POI级别人群流推断的时空对比自监督学习

    Spatio-Temporal Contrastive Self-Supervised Learning for POI-level Crowd Flow Inference. (arXiv:2309.03239v1 [cs.LG])

    [http://arxiv.org/abs/2309.03239](http://arxiv.org/abs/2309.03239)

    本文提出了一种针对POI级别人群流推断的时空对比自监督学习模型，通过自监督属性图表示学习以解决数据标记不足、POI间时空依赖性复杂和人群流量与GPS报告之间相关性多样等挑战。

    

    准确获取兴趣点（POI）的人群流量对于有效的交通管理、公共服务和城市规划至关重要。尽管如此重要，但由于城市感知技术的限制，大多数数据源的数据质量不足以监测每个POI的人群流动。这使得从低质量数据中推断准确的人群流量成为一项关键且具有挑战性的任务。这一复杂性主要由三个关键因素引起：1）标记数据的稀缺性和罕见性；2）POI之间复杂的时空依赖关系；3）精确人群流量与GPS报告之间的众多相关性。为了应对这些挑战，我们将人群流推断问题重新构建为自监督属性图表示学习任务，并引入一种新的时空数据对比自监督学习框架（model）。我们的方法从构建一个空间图开始。

    Accurate acquisition of crowd flow at Points of Interest (POIs) is pivotal for effective traffic management, public service, and urban planning. Despite this importance, due to the limitations of urban sensing techniques, the data quality from most sources is inadequate for monitoring crowd flow at each POI. This renders the inference of accurate crowd flow from low-quality data a critical and challenging task. The complexity is heightened by three key factors: 1) \emph{The scarcity and rarity of labeled data}, 2) \emph{The intricate spatio-temporal dependencies among POIs}, and 3) \emph{The myriad correlations between precise crowd flow and GPS reports}.  To address these challenges, we recast the crowd flow inference problem as a self-supervised attributed graph representation learning task and introduce a novel \underline{C}ontrastive \underline{S}elf-learning framework for \underline{S}patio-\underline{T}emporal data (\model). Our approach initiates with the construction of a spati
    
[^96]: 隐含设计选择及其对情绪识别模型开发和评估的影响

    Implicit Design Choices and Their Impact on Emotion Recognition Model Development and Evaluation. (arXiv:2309.03238v1 [cs.LG])

    [http://arxiv.org/abs/2309.03238](http://arxiv.org/abs/2309.03238)

    本研究探讨了情绪识别中的关键因素，包括收集多样化数据集、处理非典型训练数据、分析数据增强技术和注释方案的影响，以及使用对抗网络处理自然混淆变量和变化。研究结果对于开发准确和稳健的情绪识别模型具有重要意义。

    

    情绪识别是一项复杂的任务，因为情绪的感知和表达具有固有的主观性。情绪的主观性在开发准确和稳健的计算模型方面提出了重大挑战。本论文考察了情绪识别的关键要素，从收集旨在考虑情绪产生心理因素的多样化数据集开始。为了应对非典型训练数据的挑战，本研究收集了多模态应激情绪数据集，在数据收集过程中引入了受控的压力因素，以更好地反映情绪产生的真实环境影响。为了解决标签主观性问题，该研究全面分析了数据增强技术和注释方案如何影响情绪感知和注释者标签。此外，该研究通过使用对抗网络来处理自然混淆变量和变化，以隔离关键因素（如压力）与学习到的情绪表示之间的关系。

    Emotion recognition is a complex task due to the inherent subjectivity in both the perception and production of emotions. The subjectivity of emotions poses significant challenges in developing accurate and robust computational models. This thesis examines critical facets of emotion recognition, beginning with the collection of diverse datasets that account for psychological factors in emotion production.  To handle the challenge of non-representative training data, this work collects the Multimodal Stressed Emotion dataset, which introduces controlled stressors during data collection to better represent real-world influences on emotion production. To address issues with label subjectivity, this research comprehensively analyzes how data augmentation techniques and annotation schemes impact emotion perception and annotator labels. It further handles natural confounding variables and variations by employing adversarial networks to isolate key factors like stress from learned emotion rep
    
[^97]: 图像上的联邦学习：垂直分解和预训练骨干较难攻克

    Federated Learning Over Images: Vertical Decompositions and Pre-Trained Backbones Are Difficult to Beat. (arXiv:2309.03237v1 [cs.LG])

    [http://arxiv.org/abs/2309.03237](http://arxiv.org/abs/2309.03237)

    本研究对联邦学习环境中的图像分类任务进行了评估，发现垂直分解神经网络在各种设置下表现最佳。

    

    我们对一系列在联邦学习环境中进行学习的算法进行了认真评估，并测试它们在各种图像分类任务中的效用。我们考虑了许多以前没有充分考虑的问题：是否学习不具有多样图像集的数据集会影响结果；是否使用预训练的特征提取“骨干”；如何评估学习者的性能（我们认为仅仅分类准确性是不够的）等。总体而言，在各种设置下，我们发现对神经网络进行垂直分解似乎能够得到最佳结果，并且优于更常见的协调方法。

    We carefully evaluate a number of algorithms for learning in a federated environment, and test their utility for a variety of image classification tasks. We consider many issues that have not been adequately considered before: whether learning over data sets that do not have diverse sets of images affects the results; whether to use a pre-trained feature extraction "backbone"; how to evaluate learner performance (we argue that classification accuracy is not enough), among others. Overall, across a wide variety of settings, we find that vertically decomposing a neural network seems to give the best results, and outperforms more standard reconciliation-used methods.
    
[^98]: 自然示例为基础的可解释性：一项调查

    Natural Example-Based Explainability: a Survey. (arXiv:2309.03234v1 [cs.AI])

    [http://arxiv.org/abs/2309.03234](http://arxiv.org/abs/2309.03234)

    本文概述了自然示例为基础的可解释性人工智能的最新进展，这些方法通过使用示例作为解释来提高机器学习模型的可解释性，与人类的学习和推理过程相符，使解释更自然和易懂。

    

    可解释的人工智能（XAI）在提高机器学习模型的可解释性和可信度方面变得越来越重要。虽然在XAI领域，突出图已经成为主角多年，但其反映模型内部过程的能力受到了质疑。尽管不太受关注，但基于示例的XAI方法仍在不断改进。它包括使用示例作为机器学习模型预测的解释的方法。这符合人类推理的心理机制，使基于示例的解释对用户来说自然和直观易懂。事实上，人类通过基于示例形成概念的心理表示来学习和推理。本文概述了自然示例为基础的XAI的最新进展，并描述了每种方法的优缺点。所谓“自然”示例指的是直接从训练数据中绘制而来，而不涉及任何生成过程。

    Explainable Artificial Intelligence (XAI) has become increasingly significant for improving the interpretability and trustworthiness of machine learning models. While saliency maps have stolen the show for the last few years in the XAI field, their ability to reflect models' internal processes has been questioned. Although less in the spotlight, example-based XAI methods have continued to improve. It encompasses methods that use examples as explanations for a machine learning model's predictions. This aligns with the psychological mechanisms of human reasoning and makes example-based explanations natural and intuitive for users to understand. Indeed, humans learn and reason by forming mental representations of concepts based on examples.  This paper provides an overview of the state-of-the-art in natural example-based XAI, describing the pros and cons of each approach. A "natural" example simply means that it is directly drawn from the training data without involving any generative pro
    
[^99]: 零售店顾客行为分析系统：设计与实现

    Retail store customer behavior analysis system: Design and Implementation. (arXiv:2309.03232v1 [cs.LG])

    [http://arxiv.org/abs/2309.03232](http://arxiv.org/abs/2309.03232)

    本研究提出了一个零售店顾客行为分析系统的框架，通过深度学习技术对顾客行为进行数学建模和分析，为商店经理提供了洞察顾客偏好的信息。

    

    理解零售店顾客行为对于通过为服务增加个性化价值提升顾客满意度起着关键作用。行为分析揭示了顾客与商品和其他人互动中的一般和详细模式，为商店经理提供了洞察顾客偏好的信息。目前有几种解决方案旨在利用数据通过统计可视化的方式识别特定行为。然而，当前的方法局限于分析小规模的顾客行为集，使用传统方法来检测行为。它们不使用深度学习技术（如深度神经网络），这是计算机视觉领域的强大方法。此外，这些方法在可视化系统获取的行为数据时提供的图形有限。在本研究中，我们提出了一个包括三个主要部分的框架：顾客行为的数学建模，使用高效的深度学习技术进行行为分析。

    Understanding customer behavior in retail stores plays a crucial role in improving customer satisfaction by adding personalized value to services. Behavior analysis reveals both general and detailed patterns in the interaction of customers with a store items and other people, providing store managers with insight into customer preferences. Several solutions aim to utilize this data by recognizing specific behaviors through statistical visualization. However, current approaches are limited to the analysis of small customer behavior sets, utilizing conventional methods to detect behaviors. They do not use deep learning techniques such as deep neural networks, which are powerful methods in the field of computer vision. Furthermore, these methods provide limited figures when visualizing the behavioral data acquired by the system. In this study, we propose a framework that includes three primary parts: mathematical modeling of customer behaviors, behavior analysis using an efficient deep le
    
[^100]: 量子AI增强智能监控：通过创新的违禁品检测提升公共安全

    Quantum-AI empowered Intelligent Surveillance: Advancing Public Safety Through Innovative Contraband Detection. (arXiv:2309.03231v1 [quant-ph])

    [http://arxiv.org/abs/2309.03231](http://arxiv.org/abs/2309.03231)

    本研究将量子人工智能与监控领域的RentinaNet模型进行整合，开发了称为Quantum-RetinaNet的智能监控系统，该系统在准确性和速度方面取得了突破性进展。

    

    监控系统在维护现代社会的和平与安全中起到了关键作用。它们的普及性有助于有效监控可疑活动。然而，在人口密集的环境中，持续主动监控变得不切实际，必须开发智能监控系统。AI在监控领域的整合是一次重大革命，然而速度问题阻碍了其在该领域的广泛实施。研究发现，量子人工智能取得了重大突破。基于量子人工智能的监控系统不仅更准确，而且能够在实时场景中表现出色，这是以前从未见过的。本研究将RentinaNet模型与量子CNN集成，称为Quantum-RetinaNet。通过利用量子卷积神经网络的量子能力，Quantum-RetinaNet在准确性和速度之间取得了平衡。

    Surveillance systems have emerged as crucial elements in upholding peace and security in the modern world. Their ubiquity aids in monitoring suspicious activities effectively. However, in densely populated environments, continuous active monitoring becomes impractical, necessitating the development of intelligent surveillance systems. AI integration in the surveillance domain was a big revolution, however, speed issues have prevented its widespread implementation in the field. It has been observed that quantum artificial intelligence has led to a great breakthrough. Quantum artificial intelligence-based surveillance systems have shown to be more accurate as well as capable of performing well in real-time scenarios, which had never been seen before. In this research, a RentinaNet model is integrated with Quantum CNN and termed as Quantum-RetinaNet. By harnessing the Quantum capabilities of QCNN, Quantum-RetinaNet strikes a balance between accuracy and speed. This innovative integration 
    
[^101]: 如何选择体育赛程安排中的算法？

    Which algorithm to select in sports timetabling?. (arXiv:2309.03229v1 [cs.AI])

    [http://arxiv.org/abs/2309.03229](http://arxiv.org/abs/2309.03229)

    本研究通过实例空间分析体育赛程安排问题的特征，提出了一个基于机器学习的算法选择系统，预测了在给定特征下最适合的算法，同时深入了解了算法性能并提出了改进建议。

    

    任何体育竞赛都需要一个赛程安排，确定比赛队伍何时何地相遇。最近的国际赛程安排竞赛(ITC2021)揭示了一个事实，即虽然可能开发出通用算法，但每个算法在问题实例上的性能差异很大。本文在体育赛程安排方面提供了实例空间分析，从而深入了解了八种最先进算法的优势和劣势。基于机器学习技术，我们提出了一个算法选择系统，可以根据体育赛程安排问题实例的特征预测哪种算法在性能上可能表现最佳。此外，我们还确定了哪些特征在做出预测时很重要，从而深入了解了算法的性能，并提出了进一步改进的建议。最后，我们评估了这些实例的经验难度。我们的结果基于大规模计算实验。

    Any sports competition needs a timetable, specifying when and where teams meet each other. The recent International Timetabling Competition (ITC2021) on sports timetabling showed that, although it is possible to develop general algorithms, the performance of each algorithm varies considerably over the problem instances. This paper provides an instance space analysis for sports timetabling, resulting in powerful insights into the strengths and weaknesses of eight state-of-the-art algorithms. Based on machine learning techniques, we propose an algorithm selection system that predicts which algorithm is likely to perform best when given the characteristics of a sports timetabling problem instance. Furthermore, we identify which characteristics are important in making that prediction, providing insights in the performance of the algorithms, and suggestions to further improve them. Finally, we assess the empirical hardness of the instances. Our results are based on large computational exper
    
[^102]: 学习基于专利的生物医学知识图谱揭示药物再定位候选物的技术潜力

    Learning a Patent-Informed Biomedical Knowledge Graph Reveals Technological Potential of Drug Repositioning Candidates. (arXiv:2309.03227v1 [cs.AI])

    [http://arxiv.org/abs/2309.03227](http://arxiv.org/abs/2309.03227)

    本研究提出了一种使用药物专利和生物医学数据库相结合的方法，识别具有技术潜力和科学证据的药物再定位候选物。通过构建科学的生物医学知识图谱和基于专利的生物医学知识图谱，我们可以综合分析多种信息源，为药物再定位研究提供新的视角。

    

    药物再定位是一种发现现有药物新治疗用途的有前途的策略，近年来在计算科学文献中使用生物医学数据库进行了广泛探索。然而，药物再定位候选物的技术潜力经常被忽视。本研究提出了一种新的方法，综合分析药物专利和生物医学数据库等多种信息源，识别具有技术潜力和科学证据的药物再定位候选物。首先，我们构建了一个科学的生物医学知识图谱（s-BKG），包括来自生物医学数据库的药物、疾病和基因之间的关系。我们的方法涉及识别在s-BKG中与目标疾病关联有限但在空间上紧密相邻的药物作为潜在的药物候选物。然后，我们通过添加药物专利信息构建了一个基于专利的生物医学知识图谱（p-BKG）。

    Drug repositioning-a promising strategy for discovering new therapeutic uses for existing drugs-has been increasingly explored in the computational science literature using biomedical databases. However, the technological potential of drug repositioning candidates has often been overlooked. This study presents a novel protocol to comprehensively analyse various sources such as pharmaceutical patents and biomedical databases, and identify drug repositioning candidates with both technological potential and scientific evidence. To this end, first, we constructed a scientific biomedical knowledge graph (s-BKG) comprising relationships between drugs, diseases, and genes derived from biomedical databases. Our protocol involves identifying drugs that exhibit limited association with the target disease but are closely located in the s-BKG, as potential drug candidates. We constructed a patent-informed biomedical knowledge graph (p-BKG) by adding pharmaceutical patent information. Finally, we d
    
[^103]: 无需训练依然能获益：通过蒙特卡洛树搜索和能量函数引导实现大型语言模型的数学推理

    No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function. (arXiv:2309.03224v1 [cs.AI])

    [http://arxiv.org/abs/2309.03224](http://arxiv.org/abs/2309.03224)

    该论文提出了一种通过蒙特卡洛树搜索和能量函数引导来释放大型语言模型的数学推理能力的方法，以解决当前在数学推理任务中的不足和错误。该方法不需要进一步的微调步骤，通过重新定义模型和引入路径验证器的方式，实现了对输出空间的搜索和推理路径的评估。

    

    大型语言模型（LLMs）展现出令人印象深刻的语言理解和背景学习能力，包括自然语言处理（NLP）任务和具有挑战性的数学推理。然而，由于缺乏过程监督，将PLMs应用于数学推理任务通常无法生成正确的推理步骤和最终答案，即使解决方案概率很高。为了在没有进一步的微调步骤的情况下发挥微调的LLMs的数学推理能力，我们提出了一种方法，通过蒙特卡洛树搜索（MCTS）和轻量级能量函数为LLMs赋予即时反应和精细推理系统。具体而言，我们首先将微调的LLMs重新定义为基于残差的能量模型（Residual-EBM），并应用噪声对比估计来估计能量函数的参数。然后，我们使用带有能量函数的MCTS作为路径验证器来搜索输出空间并评估推理路径。通过广泛的实验证明了我们方法的有效性。

    Large language models (LLMs) exhibit impressive language understanding and in-context learning abilities including natural language processing (NLP) tasks and challenging mathematical reasoning. However, due to the lack of process-supervision, applying PLMs to mathematical reasoning tasks often fail to generate correct reasoning steps and final answer even though solutions have high probabilities. To unleash the mathematical reasoning of finetuned-LLMs without any further fineutuning steps, we propose a method to endow LLMs with immediate reaction and delicate reasoning system via Monte Carlo Tree Search(MCTS) and a light energy function to rank the decision steps. In particular, We first re-formalize the finetuned-LLMs to a Residual-based Energy Model~(Residual-EBM) and apply noise contrastive estimation to estimate the parameters of energy function . Then we use MCTS with energy function as path verifier to search the output space and evaluating the reasoning path. Through extensive 
    
[^104]: 研究聊天机器人在收集家族历史信息方面与传统面对面访谈方法的有效性

    Examining the Effectiveness of Chatbots in Gathering Family History Information in Comparison to the Standard In-Person Interview-Based Approach. (arXiv:2309.03223v1 [cs.HC])

    [http://arxiv.org/abs/2309.03223](http://arxiv.org/abs/2309.03223)

    这项研究提出了一种针对家族历史收集的聊天机器人，并将其与传统面对面采访方法进行比较，探讨了聊天机器人的可行性。研究发现，聊天机器人方法能够克服地理和技术限制，并且具有较长的采访时间。

    

    一个家谱学家常常要做的事情之一就是通过面对面采访或使用诸如ancestry.com之类的平台收集一个人的家族历史，因为这可以为家谱学家打下坚实的基础。然而，进行这些采访的能力往往受到地理位置限制和被采访者的技术能力的影响，因为在这些类型的采访中，被采访者通常是一个年长且技术能力低于平均水平的人。基于这一点，本研究提出了据我们基于先前的研究相信是第一个完全面向家族历史收集的聊天机器人，并通过与上述替代方案的性能和可用性进行比较来探索利用这种聊天机器人的可行性。通过聊天机器人的方法，我们展示了尽管进行采访的平均时间可能较长，

    One of the most common things that a genealogist is tasked with is the gathering of a person's initial family history, normally via in-person interviews or with the use of a platform such as ancestry.com, as this can provide a strong foundation upon which a genealogist may build. However, the ability to conduct these interviews can often be hindered by both geographical constraints and the technical proficiency of the interviewee, as the interviewee in these types of interviews is most often an elderly person with a lower than average level of technical proficiency. With this in mind, this study presents what we believe, based on prior research, to be the first chatbot geared entirely towards the gathering of family histories, and explores the viability of utilising such a chatbot by comparing the performance and usability of such a method with the aforementioned alternatives. With a chatbot-based approach, we show that, though the average time taken to conduct an interview may be long
    
[^105]: 基于文本感知的医学知识图谱表示学习的伴侣动物疾病诊断

    Companion Animal Disease Diagnostics based on Literal-aware Medical Knowledge Graph Representation Learning. (arXiv:2309.03219v1 [cs.AI])

    [http://arxiv.org/abs/2309.03219](http://arxiv.org/abs/2309.03219)

    这项研究提出了一种基于文本感知的医学知识图谱表示学习方法，以提高伴侣动物疾病诊断的效率。通过融合各种类型的文本信息和图结构，该方法能够捕捉到重要的实体和关系。

    

    知识图谱嵌入被用于通过分析电子医疗记录（如笔记和兽医记录）来受益于动物疾病的诊断。然而，学习用于捕捉知识图谱中带有文本信息的实体和关系的表示是具有挑战性的，因为知识图谱显示出异构特性和各种类型的文本信息。同时，现有的方法大多旨在保留围绕目标节点的图结构，而无法考虑不同类型的文本，而这些文本也可能包含重要的信息。本文提出了一种用于有效诊断动物疾病的知识图谱嵌入模型，该模型可以学习各种类型的文本信息和图结构，并将它们融合为统一的表示，即LiteralKG。具体而言，我们构建了一个知识图谱，该图谱是从各个动物医院收集的电子医疗记录及文本信息构建而来。我们然后融合不同类型的实体和关系，以及文本信息。

    Knowledge graph (KG) embedding has been used to benefit the diagnosis of animal diseases by analyzing electronic medical records (EMRs), such as notes and veterinary records. However, learning representations to capture entities and relations with literal information in KGs is challenging as the KGs show heterogeneous properties and various types of literal information. Meanwhile, the existing methods mostly aim to preserve graph structures surrounding target nodes without considering different types of literals, which could also carry significant information. In this paper, we propose a knowledge graph embedding model for the efficient diagnosis of animal diseases, which could learn various types of literal information and graph structure and fuse them into unified representations, namely LiteralKG. Specifically, we construct a knowledge graph that is built from EMRs along with literal information collected from various animal hospitals. We then fuse different types of entities and no
    
[^106]: 对多样化投资组合进行强化学习技术的评估

    Evaluation of Reinforcement Learning Techniques for Trading on a Diverse Portfolio. (arXiv:2309.03202v1 [q-fin.TR])

    [http://arxiv.org/abs/2309.03202](http://arxiv.org/abs/2309.03202)

    本研究评估了在S&P 500指数上使用强化学习技术进行多样化投资组合的可行性。研究发现，包含COVID-19时期的市场数据在训练数据集中可以提供更好的性能，并且基于策略的方法（VI和SARSA）在测试中表现优于Q学习。

    

    本研究旨在回答关于在S&P 500指数上使用强化学习的可行性的关键研究问题。采用了基于策略的价值迭代（VI）和状态-动作-奖励-状态-动作（SARSA）的技术，以及基于策略外的Q学习。该模型在包含2000-2023年多年股市数据的数据集上进行训练和测试。分析展示了在两个不同时间段上训练和测试模型的结果和发现：一个包括COVID-19大流行期间的年份，一个不包括。结果表明，在训练数据集中包含COVID-19时期的市场数据比基准策略表现更好。在测试中，基于策略的方法（VI和SARSA）优于Q学习，凸显了简单策略的偏差-方差权衡和泛化能力的影响。然而，需要注意的是，Q学习的性能可能因条件的变化而有所不同。

    This work seeks to answer key research questions regarding the viability of reinforcement learning over the S&P 500 index. The on-policy techniques of Value Iteration (VI) and State-action-reward-state-action (SARSA) are implemented along with the off-policy technique of Q-Learning. The models are trained and tested on a dataset comprising multiple years of stock market data from 2000-2023. The analysis presents the results and findings from training and testing the models using two different time periods: one including the COVID-19 pandemic years and one excluding them. The results indicate that including market data from the COVID-19 period in the training dataset leads to superior performance compared to the baseline strategies. During testing, the on-policy approaches (VI and SARSA) outperform Q-learning, highlighting the influence of bias-variance tradeoff and the generalization capabilities of simpler policies. However, it is noted that the performance of Q-learning may vary depe
    
[^107]: Blink: 使用贝叶斯估计在图神经网络中通过链接本地差分隐私

    Blink: Link Local Differential Privacy in Graph Neural Networks via Bayesian Estimation. (arXiv:2309.03190v1 [cs.LG])

    [http://arxiv.org/abs/2309.03190](http://arxiv.org/abs/2309.03190)

    本文提出了一种使用链接本地差分隐私的方法，在图神经网络中实现与不受信任的服务器的协作训练。通过贝叶斯估计，将隐私预算分别用于链接和图的度，缓解差分隐私对训练准确性的负面影响，并限制链接概率推断与真实图拓扑之间的误差。提出的LDP机制有两个变体，在不同隐私设置下互补使用，以避免误报链接估计问题。

    

    图神经网络(GNNs)由于在各种图推理任务中学习节点嵌入的卓越能力而越来越受欢迎，但训练它们可能引起隐私问题。为了解决这个问题，我们提出使用链接本地差分隐私来进行分散节点的协作，使得GNNs可以与不受信任的服务器进行训练而不泄露任何链接的存在。我们的方法将隐私预算分别用于服务器上的链接和图的度，通过贝叶斯估计更好地去噪图拓扑结构，缓解差分隐私对训练GNNs准确性的负面影响。我们限制从推断出的链接概率与真实图拓扑之间的平均绝对误差。然后，我们提出了两种不同隐私设置下互补的LDP机制的变体之一，其中在较低的隐私预算下估计较少的链接，以避免当不确定性较高时出现误报链接估计。

    Graph neural networks (GNNs) have gained an increasing amount of popularity due to their superior capability in learning node embeddings for various graph inference tasks, but training them can raise privacy concerns. To address this, we propose using link local differential privacy over decentralized nodes, enabling collaboration with an untrusted server to train GNNs without revealing the existence of any link. Our approach spends the privacy budget separately on links and degrees of the graph for the server to better denoise the graph topology using Bayesian estimation, alleviating the negative impact of LDP on the accuracy of the trained GNNs. We bound the mean absolute error of the inferred link probabilities against the ground truth graph topology. We then propose two variants of our LDP mechanism complementing each other in different privacy settings, one of which estimates fewer links under lower privacy budgets to avoid false positive link estimates when the uncertainty is hig
    
[^108]: 基于印象感知的多行为推荐系统：一种层次图注意力方法

    Impression-Informed Multi-Behavior Recommender System: A Hierarchical Graph Attention Approach. (arXiv:2309.03169v1 [cs.IR])

    [http://arxiv.org/abs/2309.03169](http://arxiv.org/abs/2309.03169)

    这个论文提出了一种基于印象感知的多行为推荐系统，通过利用注意机制从行为间和行为内部获取信息，并采用多层级图注意力方法，来解决推荐系统在处理多个行为之间互动方面的挑战。

    

    尽管推荐系统从隐式反馈中获益良多，但往往会忽略用户与物品之间的多行为互动的细微差别。历史上，这些系统要么将所有行为，如“印象”（以前称为“浏览”）、“添加到购物车”和“购买”，归并为一个统一的“互动”标签，要么仅优先考虑目标行为，通常是“购买”行为，并丢弃有价值的辅助信号。尽管最近的进展试图解决这种简化，但它们主要集中于优化目标行为，与数据稀缺作斗争。此外，它们往往绕过了与行为内在层次结构有关的微妙差异。为了弥合这些差距，我们引入了“H”ierarchical “M”ulti-behavior “G”raph Attention “N”etwork（HMGN）。这个开创性的框架利用注意机制从行为间和行为内部获取信息，同时采用多

    While recommender systems have significantly benefited from implicit feedback, they have often missed the nuances of multi-behavior interactions between users and items. Historically, these systems either amalgamated all behaviors, such as \textit{impression} (formerly \textit{view}), \textit{add-to-cart}, and \textit{buy}, under a singular 'interaction' label, or prioritized only the target behavior, often the \textit{buy} action, discarding valuable auxiliary signals. Although recent advancements tried addressing this simplification, they primarily gravitated towards optimizing the target behavior alone, battling with data scarcity. Additionally, they tended to bypass the nuanced hierarchy intrinsic to behaviors. To bridge these gaps, we introduce the \textbf{H}ierarchical \textbf{M}ulti-behavior \textbf{G}raph Attention \textbf{N}etwork (HMGN). This pioneering framework leverages attention mechanisms to discern information from both inter and intra-behaviors while employing a multi-
    
[^109]: 使用强化学习在高维肌肉骨骼模型中实现自然且稳健的行走，无需演示

    Natural and Robust Walking using Reinforcement Learning without Demonstrations in High-Dimensional Musculoskeletal Models. (arXiv:2309.02976v1 [cs.RO])

    [http://arxiv.org/abs/2309.02976](http://arxiv.org/abs/2309.02976)

    本论文通过使用强化学习来解决肌肉骨骼冗余问题，并实现了在高维度肌肉骨骼模型中自然且稳健的行走。

    

    人类在复杂的自然环境中以稳健的双足行走表现出色。在每一步中，他们充分调整生物力学肌肉动力学和神经信号的相互作用，以在地面条件的不确定性下保持稳健。然而，我们还没有完全理解神经系统如何解决肌肉骨骼冗余问题，以解决考虑稳定性、稳健性和能量效率的多目标控制问题。在计算机模拟中，能量最小化已被证明是一种成功的优化目标，在轨迹优化或基于反射的控制方法中重现了自然行走。然而，这些方法一次只关注特定的运动，并且在补偿干扰时，所产生的控制器受到限制。在机器人领域，最近的强化学习（RL）方法在四足系统上实现了高度稳定（和高效）的运动，但要使用双足生物力学模型生成类似人类行走的行走需要大量的工作。

    Humans excel at robust bipedal walking in complex natural environments. In each step, they adequately tune the interaction of biomechanical muscle dynamics and neuronal signals to be robust against uncertainties in ground conditions. However, it is still not fully understood how the nervous system resolves the musculoskeletal redundancy to solve the multi-objective control problem considering stability, robustness, and energy efficiency. In computer simulations, energy minimization has been shown to be a successful optimization target, reproducing natural walking with trajectory optimization or reflex-based control methods. However, these methods focus on particular motions at a time and the resulting controllers are limited when compensating for perturbations. In robotics, reinforcement learning~(RL) methods recently achieved highly stable (and efficient) locomotion on quadruped systems, but the generation of human-like walking with bipedal biomechanical models has required extensive 
    
[^110]: 让学生决策的知识蒸馏层

    Knowledge Distillation Layer that Lets the Student Decide. (arXiv:2309.02843v1 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2309.02843](http://arxiv.org/abs/2309.02843)

    本论文提出了一个可学习的知识蒸馏层，能够明确地嵌入教师的知识到学生的特征变换中，从而改进了知识蒸馏技术。

    

    知识蒸馏（KD）中的典型技术是通过将学生的响应与强大模型（教师）的响应匹配来规范学生的学习。然而，它对学生的特征变换的作用相对隐含，限制了其在中间层的实践。为了明确地嵌入教师的知识，我们提出了一个可学习的KD层，它通过两个不同的能力改进了KD：i）学习如何利用教师的知识，能够丢弃无关的信息；ii）将转移的知识向前传递得更深入。因此，在推理过程中，学生可以享受到教师的知识，而不仅仅是在训练中。正式地说，我们重新分配1x1-BN-ReLU-1x1卷积块，根据学生对应区域与模板（由教师监督）的匹配情况，为每个局部区域分配一个语义向量。

    Typical technique in knowledge distillation (KD) is regularizing the learning of a limited capacity model (student) by pushing its responses to match a powerful model's (teacher). Albeit useful especially in the penultimate layer and beyond, its action on student's feature transform is rather implicit, limiting its practice in the intermediate layers. To explicitly embed the teacher's knowledge in feature transform, we propose a learnable KD layer for the student which improves KD with two distinct abilities: i) learning how to leverage the teacher's knowledge, enabling to discard nuisance information, and ii) feeding forward the transferred knowledge deeper. Thus, the student enjoys the teacher's knowledge during the inference besides training. Formally, we repurpose 1x1-BN-ReLU-1x1 convolution block to assign a semantic vector to each local region according to the template (supervised by the teacher) that the corresponding region of the student matches. To facilitate template learnin
    
[^111]: Diffusion-EDFs: 基于SE(3)的等变去噪生成建模在视觉机器人操作中的应用

    Diffusion-EDFs: Bi-equivariant Denoising Generative Modeling on SE(3) for Visual Robotic Manipulation. (arXiv:2309.02685v1 [cs.RO])

    [http://arxiv.org/abs/2309.02685](http://arxiv.org/abs/2309.02685)

    本文提出了Diffusion-EDFs，一种在视觉机器人操作中应用的基于SE(3)的等变去噪生成建模方法。通过集成SE(3)等变性，我们的方法展示了出色的数据效率和泛化能力。

    

    最近的研究已经验证了等变方法可以显著提高机器人学习中的数据效率、泛化能力和鲁棒性。与此同时，去噪扩散生成建模最近作为一种具有随机行为的机器人操作学习的有前途的方法引起了极大关注。本文提出了Diffusion-EDFs，一种将空间旋转平移等变性即SE(3)等变性引入扩散生成建模的新方法。通过将SE(3)等变性集成到我们的模型架构中，我们展示了我们提出的方法具有明显的数据效率，在进行端到端训练时只需5到10个任务演示即可。此外，与之前基于扩散的操作方法相比，我们的方法展示了更好的泛化能力。

    Recent studies have verified that equivariant methods can significantly improve the data efficiency, generalizability, and robustness in robot learning. Meanwhile, denoising diffusion-based generative modeling has recently gained significant attention as a promising approach for robotic manipulation learning from demonstrations with stochastic behaviors. In this paper, we present Diffusion-EDFs, a novel approach that incorporates spatial roto-translation equivariance, i.e., SE(3)-equivariance to diffusion generative modeling. By integrating SE(3)-equivariance into our model architectures, we demonstrate that our proposed method exhibits remarkable data efficiency, requiring only 5 to 10 task demonstrations for effective end-to-end training. Furthermore, our approach showcases superior generalizability compared to previous diffusion-based manipulation methods.
    
[^112]: 使用加密图像有效微调视觉Transformer的领域自适应

    Domain Adaptation for Efficiently Fine-tuning Vision Transformer with Encrypted Images. (arXiv:2309.02556v1 [cs.CV])

    [http://arxiv.org/abs/2309.02556](http://arxiv.org/abs/2309.02556)

    本文提出了一种使用视觉Transformer(ViT)进行模型微调的领域自适应方法，可解决使用转换图像训练模型导致准确性下降的问题，实验证明该方法在使用加密图像时也能保持模型的准确性。

    

    近年来，使用转换数据训练的深度神经网络(DNN)已被应用于隐私保护学习、访问控制和对抗防御等各种应用。然而，使用转换数据会降低模型的性能。因此，在本文中，我们提出了一种新颖的方法，利用视觉Transformer(ViT)对使用转换图像进行模型微调。所提出的领域自适应方法不会导致模型准确性的降低，并且是在ViT的嵌入结构基础上进行的。在实验中，我们验证了所提出的方法在使用具有CIFAR-10和CIFAR-100数据集的加密图像时防止准确性降低。

    In recent years, deep neural networks (DNNs) trained with transformed data have been applied to various applications such as privacy-preserving learning, access control, and adversarial defenses. However, the use of transformed data decreases the performance of models. Accordingly, in this paper, we propose a novel method for fine-tuning models with transformed images under the use of the vision transformer (ViT). The proposed domain adaptation method does not cause the accuracy degradation of models, and it is carried out on the basis of the embedding structure of ViT. In experiments, we confirmed that the proposed method prevents accuracy degradation even when using encrypted images with the CIFAR-10 and CIFAR-100 datasets.
    
[^113]: 一种用于电影音频源分离的通用带通神经网络

    A Generalized Bandsplit Neural Network for Cinematic Audio Source Separation. (arXiv:2309.02539v1 [eess.AS])

    [http://arxiv.org/abs/2309.02539](http://arxiv.org/abs/2309.02539)

    本论文提出了一种通用带通神经网络用于电影音频源分离，通过使用心理声学的频率尺度来定义频带并且利用共同编码器结构的信息共享特性提高了分离性能。

    

    电影音频源分离是音频源分离的一个相对较新的子任务，其目标是从混音中提取对话音轨、音乐音轨和特效音轨。在这项工作中，我们开发了一个模型，可以对频率轴的任何完全或过完备的分区进行泛化。基于心理声学的频率尺度用于确定带通的定义，现在具备冗余性以进行更可靠的特征提取。我们提出了一个损失函数，该损失函数基于信噪比和1-范数的稀疏促进属性。我们还利用共同编码器结构的信息共享特性，在训练和推断过程中减少计算复杂性，改善难以泛化的声音类别的分离性能，并在推断时提供灵活性，可轻松分离解码器。我们的最佳模型在Divide and Remaster数据集上取得了最先进的性能。

    Cinematic audio source separation is a relatively new subtask of audio source separation, with the aim of extracting the dialogue stem, the music stem, and the effects stem from their mixture. In this work, we developed a model generalizing the Bandsplit RNN for any complete or overcomplete partitions of the frequency axis. Psycho-acoustically motivated frequency scales were used to inform the band definitions which are now defined with redundancy for more reliable feature extraction. A loss function motivated by the signal-to-noise ratio and the sparsity-promoting property of the 1-norm was proposed. We additionally exploit the information-sharing property of a common-encoder setup to reduce computational complexity during both training and inference, improve separation performance for hard-to-generalize classes of sounds, and allow flexibility during inference time with easily detachable decoders. Our best model sets the state of the art on the Divide and Remaster dataset with perfor
    
[^114]: 通过张量化增强深度学习模型：综述与框架

    Enhancing Deep Learning Models through Tensorization: A Comprehensive Survey and Framework. (arXiv:2309.02428v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.02428](http://arxiv.org/abs/2309.02428)

    本文综述了张量化在深度学习模型中的应用。张量化桥梁了数据的多维特性与传统线性代数方法中的二维矩阵表示之间的差距。研究表明，利用多维数据集进行分析可以提供更好的表达力和结果。

    

    公共领域数据的爆炸性增长和深度学习模型架构的日益复杂，凸显了对更高效的数据表示和分析技术的需求。本文受到Helal（2023）工作的启发，旨在全面介绍张量化。这种转化性的方法弥合了数据的固有多维特性与常用的线性代数机器学习算法中简化的二维矩阵之间的差距。本文探讨了张量化的步骤、多维数据源、各种多方面分析方法的应用以及这些方法的好处。文章通过比较在Python中使用二维算法和多方面算法的盲源分离（BSS）的小例子，结果表明多方面分析更具表达力。与维度诅咒的直觉相反，利用多维数据集的原始形式可以提供更好的结果。

    The burgeoning growth of public domain data and the increasing complexity of deep learning model architectures have underscored the need for more efficient data representation and analysis techniques. This paper is motivated by the work of Helal (2023) and aims to present a comprehensive overview of tensorization. This transformative approach bridges the gap between the inherently multidimensional nature of data and the simplified 2-dimensional matrices commonly used in linear algebra-based machine learning algorithms. This paper explores the steps involved in tensorization, multidimensional data sources, various multiway analysis methods employed, and the benefits of these approaches. A small example of Blind Source Separation (BSS) is presented comparing 2-dimensional algorithms and a multiway algorithm in Python. Results indicate that multiway analysis is more expressive. Contrary to the intuition of the dimensionality curse, utilising multidimensional datasets in their native form 
    
[^115]: 语音障碍者的声学到发音反演: 预先训练的自监督表示是否有利？

    Acoustic-to-articulatory inversion for dysarthric speech: Are pre-trained self-supervised representations favorable?. (arXiv:2309.01108v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2309.01108](http://arxiv.org/abs/2309.01108)

    本研究探讨了使用预先训练的自监督表示对语音障碍者的声学到发音反演任务的影响。实验结果表明，在低资源条件下，经过微调的DeCoAR模型在精细训练方案中相对于健康对照组和患者，分别取得了约1.81\%和约4.56\%的皮尔逊相关系数(CC)的改进。

    

    声学到发音反演(ACI)涉及从声学空间映射到发音空间。信号处理特征如MFCCs已被广泛应用于ACI任务。对于有语音障碍的患者，由于不准确和不清晰的发音，ACI是具有挑战性的。本研究使用预先训练的自监督学习(SSL)模型中的表示对语音障碍的ACI进行了实验。我们评估了不同预训练特征对这个具有挑战性的低资源ACI任务的影响。此外，我们还将x-vectors与提取的SSL特征相结合，训练了一个BLSTM网络。在已知情况下，我们尝试了三种ACI训练方案（主题特定，聚合和微调）。结果一致表明，DeCoAR在微调方案中，相对于健康对照组和患者，皮尔逊相关系数(CC)的改进幅度分别为约1.81\%和约4.56\%。

    $ $Acoustic-to-articulatory inversion (AAI) involves mapping from the acoustic space to the articulatory space. Signal-processing features like the MFCCs, have been widely used for the AAI task. For subjects with dysarthric speech, AAI is challenging because of an imprecise and indistinct pronunciation. In this work, we perform AAI for dysarthric speech using representations from pre-trained self-supervised learning (SSL) models. We demonstrate the impact of different pre-trained features on this challenging AAI task, at low-resource conditions. In addition, we also condition x-vectors to the extracted SSL features to train a BLSTM network. In the seen case, we experiment with three AAI training schemes (subject-specific, pooled, and fine-tuned). The results, consistent across training schemes, reveal that DeCoAR, in the fine-tuned scheme, achieves a relative improvement of the Pearson Correlation Coefficient (CC) by ${\sim}$1.81\% and ${\sim}$4.56\% for healthy controls and patients, 
    
[^116]: 学习品味：一个多模态葡萄酒数据集

    Learning to Taste: A Multimodal Wine Dataset. (arXiv:2308.16900v1 [cs.LG])

    [http://arxiv.org/abs/2308.16900](http://arxiv.org/abs/2308.16900)

    这个论文介绍了一个大型多模态葡萄酒数据集，用于研究视觉感知、语言和口感之间的关系，并提出了低维概念嵌入算法，将人类经验与自动机器相似度核相结合，改进了口味分类，并与人类口味知觉相一致。

    

    我们提出了一个大型的多模态葡萄酒数据集WineSensed，用于研究视觉感知、语言和口感之间的关系。该数据集包含89.7万张葡萄酒标签图片和82.4万条来自Vivino平台的葡萄酒评论。该数据集具有超过35万个独特的年份，附带了年份、产地、评分、酒精含量、价格和葡萄组成的注释。我们通过一项品酒实验对部分数据进行了细粒度的口味注释，共有256名参与者被要求根据口味的相似性对葡萄酒进行排序，得到了超过5千个配对的口味距离。我们提出了一种低维概念嵌入算法，将人类经验与自动机器相似度核相结合。我们证明，这个共享的概念嵌入空间在粗粒度口味分类（酒精含量，国家，葡萄，价格，评分）上改进，并且与复杂的人类口味知觉相一致。

    We present WineSensed, a large multimodal wine dataset for studying the relations between visual perception, language, and flavor. The dataset encompasses 897k images of wine labels and 824k reviews of wines curated from the Vivino platform. It has over 350k unique vintages, annotated with year, region, rating, alcohol percentage, price, and grape composition. We obtained fine-grained flavor annotations on a subset by conducting a wine-tasting experiment with 256 participants who were asked to rank wines based on their similarity in flavor, resulting in more than 5k pairwise flavor distances. We propose a low-dimensional concept embedding algorithm that combines human experience with automatic machine similarity kernels. We demonstrate that this shared concept embedding space improves upon separate embedding spaces for coarse flavor classification (alcohol percentage, country, grape, price, rating) and aligns with the intricate human perception of flavor.
    
[^117]: Transformers作为支持向量机

    Transformers as Support Vector Machines. (arXiv:2308.16898v1 [cs.LG])

    [http://arxiv.org/abs/2308.16898](http://arxiv.org/abs/2308.16898)

    这项工作建立了自注意力和硬间隔支持向量机问题之间的正式等价关系，通过转换器架构的优化几何来解决自然语言处理问题，同时揭示了梯度下降优化的转换器的隐式偏差。

    

    自从"Attention Is All You Need"中引入转换器架构以来，它在自然语言处理领域取得了革命性的进展。转换器中的注意力层接受输入令牌序列$X$并通过计算softmax$(XQK^\top X^\top)$的成对相似性使它们相互作用，其中$(K,Q)$是可训练的键-查询参数。在这项工作中，我们建立了自注意力优化几何和一个硬间隔支持向量机问题之间的正式等价关系，通过对令牌对的外积施加线性约束，将最佳输入令牌与非最佳令牌分离。这个形式主义使我们能够表征梯度下降优化的单层转换器的隐式偏差：(1)优化注意力层，使用可变正则化参数$(K,Q)$，收敛的方向是一个最小化综合参数$W=KQ^\top$的核范数的支持向量机解决方案。而直接使用$W$进行参数化则最小化一个Frobenius范数目标。

    Since its inception in "Attention Is All You Need", transformer architecture has led to revolutionary advancements in NLP. The attention layer within the transformer admits a sequence of input tokens $X$ and makes them interact through pairwise similarities computed as softmax$(XQK^\top X^\top)$, where $(K,Q)$ are the trainable key-query parameters. In this work, we establish a formal equivalence between the optimization geometry of self-attention and a hard-margin SVM problem that separates optimal input tokens from non-optimal tokens using linear constraints on the outer-products of token pairs. This formalism allows us to characterize the implicit bias of 1-layer transformers optimized with gradient descent: (1) Optimizing the attention layer with vanishing regularization, parameterized by $(K,Q)$, converges in direction to an SVM solution minimizing the nuclear norm of the combined parameter $W=KQ^\top$. Instead, directly parameterizing by $W$ minimizes a Frobenius norm objective. 
    
[^118]: 表情符号促进了GitHub上开发者的参与和问题解决

    Emoji Promotes Developer Participation and Issue Resolution on GitHub. (arXiv:2308.16360v1 [cs.CY])

    [http://arxiv.org/abs/2308.16360](http://arxiv.org/abs/2308.16360)

    本研究探讨了表情符号在虚拟工作空间中的使用对开发者参与和问题解决的影响。研究发现，表情符号可以显著缩短问题的解决时间，并吸引更多用户参与。不同类型问题对表情符号的效果也存在差异。

    

    尽管在疫情期间远程工作越来越普遍，但许多人对远程工作的低效率表示担忧。在基于文本的沟通中缺乏面部表情和肢体语言等非语言线索，这妨碍了有效的沟通，并对工作结果产生负面影响。作为替代的非语言线索，在社交媒体平台上广泛使用的表情符号在虚拟工作空间中也越来越受欢迎。本文研究了表情符号的使用如何影响虚拟工作空间中开发者的参与和问题解决。为此，我们收集了GitHub的一个一年周期内的问题，并应用因果推断技术来衡量表情符号对问题结果的因果效应，控制问题内容、仓库和作者信息等混淆因素。我们发现表情符号可以显著缩短问题的解决时间，并吸引更多用户参与。我们还比较了不同类型问题的异质效应。

    Although remote working is increasingly adopted during the pandemic, many are concerned by the low-efficiency in the remote working. Missing in text-based communication are non-verbal cues such as facial expressions and body language, which hinders the effective communication and negatively impacts the work outcomes. Prevalent on social media platforms, emojis, as alternative non-verbal cues, are gaining popularity in the virtual workspaces well. In this paper, we study how emoji usage influences developer participation and issue resolution in virtual workspaces. To this end, we collect GitHub issues for a one-year period and apply causal inference techniques to measure the causal effect of emojis on the outcome of issues, controlling for confounders such as issue content, repository, and author information. We find that emojis can significantly reduce the resolution time of issues and attract more user participation. We also compare the heterogeneous effect on different types of issue
    
[^119]: 深度视频编码控制

    Deep Video Codec Control. (arXiv:2308.16215v1 [eess.IV])

    [http://arxiv.org/abs/2308.16215](http://arxiv.org/abs/2308.16215)

    本文提出了第一个端到端可学习的深度视频编码控制方法，同时考虑了带宽限制和下游视觉性能，并在不破坏现有标准化的情况下实现了保护深度视觉模型的目标。

    

    丢失率视频压缩通常用于传输和存储视频数据。尽管存在进阶（神经）压缩方法，但统一视频编码器（如H.264或H.265）仍然是事实上的标准。在面对动态网络带宽条件的视频传输中，视频编码器需要适应非常不同的压缩强度。速率控制模块增强编解码器的压缩能力，以满足带宽限制并尽量减少视频失真。然而，标准视频编码器及其速率控制模块是为了最小化人类质量评估而开发的，却没有考虑保护深度视觉模型的下游性能。在本文中，我们提出了第一个端到端可学习的深度视频编码控制方法，考虑了带宽限制和下游视觉性能，并不破坏现有的标准化。我们针对两个常见的视觉任务（语义分割...

    Lossy video compression is commonly used when transmitting and storing video data. Unified video codecs (e.g., H.264 or H.265) remain the \emph{de facto} standard, despite the availability of advanced (neural) compression approaches. Transmitting videos in the face of dynamic network bandwidth conditions requires video codecs to adapt to vastly different compression strengths. Rate control modules augment the codec's compression such that bandwidth constraints are satisfied and video distortion is minimized. While, both standard video codes and their rate control modules are developed to minimize video distortion w.r.t. human quality assessment, preserving the downstream performance of deep vision models is not considered. In this paper, we present the first end-to-end learnable deep video codec control considering both bandwidth constraints and downstream vision performance, while not breaking existing standardization. We demonstrate for two common vision tasks (semantic segmentation 
    
[^120]: 评估用于多变量时间序列分类的解释方法

    Evaluating Explanation Methods for Multivariate Time Series Classification. (arXiv:2308.15223v1 [cs.LG])

    [http://arxiv.org/abs/2308.15223](http://arxiv.org/abs/2308.15223)

    本文评估了用于多变量时间序列分类的解释方法，重点研究了基于显著性的方法来指示分类决策中最相关的通道和时间序列点。

    

    多变量时间序列分类是一项重要的计算任务，出现在数据随时间和多个通道记录的应用中。例如，智能手表可以记录人体运动的加速度和方向，这些信号被记录为多变量时间序列。我们可以对这些数据进行分类，以了解和预测人体运动和各种属性，如健身水平。在许多应用中，仅靠分类是不够的，我们通常需要分类，同时还要理解模型学到了什么（例如，基于数据中的哪些信息给出了预测）。本文的重点是分析和评估专用于多变量时间序列分类（MTSC）的解释方法。我们着重研究了能指出分类决策中最相关通道和时间序列点的基于显著性的解释方法。我们分析了两种流行且准确的多变量时间序列分类器，ROCKET和...

    Multivariate time series classification is an important computational task arising in applications where data is recorded over time and over multiple channels. For example, a smartwatch can record the acceleration and orientation of a person's motion, and these signals are recorded as multivariate time series. We can classify this data to understand and predict human movement and various properties such as fitness levels. In many applications classification alone is not enough, we often need to classify but also understand what the model learns (e.g., why was a prediction given, based on what information in the data). The main focus of this paper is on analysing and evaluating explanation methods tailored to Multivariate Time Series Classification (MTSC). We focus on saliency-based explanation methods that can point out the most relevant channels and time series points for the classification decision. We analyse two popular and accurate multivariate time series classifiers, ROCKET and 
    
[^121]: 通过Mixup增强的元学习方法实现蛋白质模拟器的高效微调

    Mixup-Augmented Meta-Learning for Sample-Efficient Fine-Tuning of Protein Simulators. (arXiv:2308.15116v1 [cs.LG])

    [http://arxiv.org/abs/2308.15116](http://arxiv.org/abs/2308.15116)

    本文通过Mixup增强的元学习方法实现了对蛋白质模拟器的高效微调，可以在有限的训练数据下泛化到未见过的场景，并提供了一种通用的模拟连续动态条件的方法。

    

    分子动力学模拟已经成为研究生物分子的基本工具。与此同时，我们希望在分子能够波动的各种条件下对一组粒子进行模拟。本文中，我们将软提示学习方法应用于分子动力学任务并进行了适应性探索。我们的模型可以在有限的训练数据下非常好地泛化到未见过的和超出分布的场景。虽然我们的工作以温度为测试案例，但我们的方法的多功能性使其可以通过任何连续的动态条件（如压力和体积）进行有效模拟。我们的框架有两个阶段：1）使用数据混合技术进行预训练，增强分子结构数据和温度提示，然后通过逐渐增加比例的方式应用课程学习方法。2）基于元学习的微调框架提高了微调过程的样本效率，并为软提示微调提供更好的表现。

    Molecular dynamics simulations have emerged as a fundamental instrument for studying biomolecules. At the same time, it is desirable to perform simulations of a collection of particles under various conditions in which the molecules can fluctuate. In this paper, we explore and adapt the soft prompt-based learning method to molecular dynamics tasks. Our model can remarkably generalize to unseen and out-of-distribution scenarios with limited training data. While our work focuses on temperature as a test case, the versatility of our approach allows for efficient simulation through any continuous dynamic conditions, such as pressure and volumes. Our framework has two stages: 1) Pre-trains with data mixing technique, augments molecular structure data and temperature prompts, then applies a curriculum learning method by increasing the ratio of them smoothly. 2) Meta-learning-based fine-tuning framework improves sample-efficiency of fine-tuning process and gives the soft prompt-tuning better 
    
[^122]: AtmoRep:一种利用大规模表示学习的大气动力学随机模型

    AtmoRep: A stochastic model of atmosphere dynamics using large scale representation learning. (arXiv:2308.13280v1 [physics.ao-ph])

    [http://arxiv.org/abs/2308.13280](http://arxiv.org/abs/2308.13280)

    提出了一种称为AtmoRep的大气动力学随机模型，它利用大规模表示学习和人工智能技术来确定复杂的大气动力学的通用描述，从而为各种应用提供技能结果。

    

    大气对人类有多种影响，从因天气不良而丧生的损失到对社会的长期社会和经济影响。因此，对大气动力学进行计算机模拟对我们和未来的世代的福祉非常重要。在这里，我们提出了AtmoRep，一种新颖的、与任务无关的大气动力学随机计算机模型，可以为广泛的应用提供技能结果。AtmoRep利用人工智能的大规模表示学习来确定大气高度复杂、随机动力学的通用描述，该描述基于历史轨迹的最佳可用估计，这些历史轨迹受观测约束。这是通过一种新颖的自监督学习目标和一个独特的集合实现的，该集合从随机模型中采样，其可变性受历史记录中的可变性启发。AtmoRep的任务无关性使其能够为各种应用提供灵活的结果。

    The atmosphere affects humans in a multitude of ways, from loss of life due to adverse weather effects to long-term social and economic impacts on societies. Computer simulations of atmospheric dynamics are, therefore, of great importance for the well-being of our and future generations. Here, we propose AtmoRep, a novel, task-independent stochastic computer model of atmospheric dynamics that can provide skillful results for a wide range of applications. AtmoRep uses large-scale representation learning from artificial intelligence to determine a general description of the highly complex, stochastic dynamics of the atmosphere from the best available estimate of the system's historical trajectory as constrained by observations. This is enabled by a novel self-supervised learning objective and a unique ensemble that samples from the stochastic model with a variability informed by the one in the historical record. The task-independent nature of AtmoRep enables skillful results for a divers
    
[^123]: 学习带噪声的光学物理不可复制函数和与学习误差的连接的多项式界限

    Polynomial Bounds for Learning Noisy Optical Physical Unclonable Functions and Connections to Learning With Errors. (arXiv:2308.09199v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.09199](http://arxiv.org/abs/2308.09199)

    本文研究了学习带噪声的光学物理不可复制函数的问题，通过建立多项式界限，证明了在具备挑战-响应对和计算能力的条件下，可以以任意高的概率学习到该类函数，这对于物理安全领域具有重要意义。

    

    研究表明，在具有多项式数量的挑战-响应对和多项式计算能力的情况下，即使在存在噪声的情况下，也可以以任意高的概率学习一类光学物理不可复制函数（PUFs），前提是噪声和挑战向量的分布符合温和的假设。这扩展了Rh\"uramir等人（2013）的结果，他们在没有噪声的情况下证明了该类PUFs的一个子集可以在多项式时间内学习，假设PUF的光学特性是线性的或具有可忽略的非线性效应。我们根据PUF的尺寸参数、挑战和噪声向量的分布以及回归算法的概率和准确性，导出了所需样本数量和线性回归算法的计算复杂度的多项式界限，这类似于Bootle等人（2018）的分析，他们证明了一种学习方法的可行性

    It is shown that a class of optical physical unclonable functions (PUFs) can be learned to arbitrary precision with arbitrarily high probability, even in the presence of noise, given access to polynomially many challenge-response pairs and polynomially bounded computational power, under mild assumptions about the distributions of the noise and challenge vectors. This extends the results of Rh\"uramir et al. (2013), who showed a subset of this class of PUFs to be learnable in polynomial time in the absence of noise, under the assumption that the optics of the PUF were either linear or had negligible nonlinear effects. We derive polynomial bounds for the required number of samples and the computational complexity of a linear regression algorithm, based on size parameters of the PUF, the distributions of the challenge and noise vectors, and the probability and accuracy of the regression algorithm, with a similar analysis to one done by Bootle et al. (2018), who demonstrated a learning att
    
[^124]: RatGPT:将在线LLMs转化为恶意软件攻击的代理

    RatGPT: Turning online LLMs into Proxies for Malware Attacks. (arXiv:2308.09183v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2308.09183](http://arxiv.org/abs/2308.09183)

    本研究展示了利用在线LLMs生成的恶意内容，通过将LLM作为攻击者和受害者之间的代理，实现了恶意软件传播并与受害者系统交互的命令，同时规避检测。

    

    生成式人工智能的发展和新发布的大型语言模型（LLMs）的能力为软件工程提供了新的机遇，但同时也带来了新的网络安全挑战。最近，研究人员已经展示了使用诸如ChatGPT之类的LLMs生成恶意内容的可能性，这些内容可以直接被利用或引导经验不足的黑客武器化工具和代码。这些研究还涵盖了需要攻击者处于循环中的场景。在本研究中，我们利用公开可用的插件，将一个LLM作为攻击者和受害者之间的代理。我们提供了一个概念验证，其中使用ChatGPT传播恶意软件，同时规避检测，并与命令和控制（C2）服务器建立通信，以接收与受害者系统交互的命令。最后，我们介绍了保持不被检测和进行攻击所需的基本要素。

    The evolution of Generative AI and the capabilities of the newly released Large Language Models (LLMs) open new opportunities in software engineering. However, they also lead to new challenges in cybersecurity. Recently, researchers have shown the possibilities of using LLMs such as ChatGPT to generate malicious content that can directly be exploited or guide inexperienced hackers to weaponize tools and code. These studies covered scenarios that still require the attacker to be in the middle of the loop. In this study, we leverage openly available plugins and use an LLM as proxy between the attacker and the victim. We deliver a proof-of-concept where ChatGPT is used for the dissemination of malicious software while evading detection, alongside establishing the communication to a command and control (C2) server to receive commands to interact with a victim's system. Finally, we present the general approach as well as essential elements in order to stay undetected and make the attack a s
    
[^125]: 通过异构模型重组实现个性化联邦学习

    Towards Personalized Federated Learning via Heterogeneous Model Reassembly. (arXiv:2308.08643v1 [cs.LG])

    [http://arxiv.org/abs/2308.08643](http://arxiv.org/abs/2308.08643)

    本文提出了一个名为pFedHR的新框架，利用异构模型重组实现个性化联邦学习。实验表明，pFedHR在各种设置下优于基准方法，并且能够有效降低使用不兼容数据的不良影响。

    

    本文针对联邦学习中模型异构的实际且具有挑战性的问题进行了研究，其中客户端具有不同网络结构的模型。为了解决这个问题，我们提出了一个名为pFedHR的新框架，利用异构模型重组实现个性化联邦学习。具体而言，我们将异构模型个性化问题视为服务器端的模型匹配优化任务。此外，pFedHR能够自动且动态地生成具有最小人工干预的信息丰富且多样化的个性化候选模型。此外，我们提出的异构模型重组技术在一定程度上减轻了使用具有不同分布的公共数据与客户端数据造成的不良影响。实验结果表明，在IID和非IID设置下，pFedHR在三个数据集上的性能优于基准方法。此外，pFedHR有效降低了使用不兼容数据的不良影响。

    This paper focuses on addressing the practical yet challenging problem of model heterogeneity in federated learning, where clients possess models with different network structures. To track this problem, we propose a novel framework called pFedHR, which leverages heterogeneous model reassembly to achieve personalized federated learning. In particular, we approach the problem of heterogeneous model personalization as a model-matching optimization task on the server side. Moreover, pFedHR automatically and dynamically generates informative and diverse personalized candidates with minimal human intervention. Furthermore, our proposed heterogeneous model reassembly technique mitigates the adverse impact introduced by using public data with different distributions from the client data to a certain extent. Experimental results demonstrate that pFedHR outperforms baselines on three datasets under both IID and Non-IID settings. Additionally, pFedHR effectively reduces the adverse impact of usi
    
[^126]: 大型语言模型的持续预训练：如何（重新）热启动模型？

    Continual Pre-Training of Large Language Models: How to (re)warm your model?. (arXiv:2308.04014v1 [cs.CL])

    [http://arxiv.org/abs/2308.04014](http://arxiv.org/abs/2308.04014)

    该论文研究了大型语言模型的持续预训练问题，探讨了热启动策略对于解决分布变化和提高计算效率的影响。

    

    大型语言模型通常会对数十亿个标记进行预训练，一旦有新数据可用，就会重新开始这个过程。一种更廉价和高效的解决方案是实现这些模型的持续预训练，即用新数据更新预训练模型而不是从头开始重新训练。然而，新数据引起的分布变化通常会导致过去数据的性能下降。在本研究中，我们研究了不同的热启动策略对持续预训练的影响。我们的假设是，在训练新数据集时，需要重新增加学习率以提高计算效率。我们在Pile（上游数据，300B标记）上持续预训练模型在SlimPajama（下游数据，297B标记）上进行了线性热启动和余弦衰减的调度。我们在Pythia 410M语言模型架构上进行了所有实验。

    Large language models (LLMs) are routinely pre-trained on billions of tokens, only to restart the process over again once new data becomes available. A much cheaper and more efficient solution would be to enable the continual pre-training of these models, i.e. updating pre-trained models with new data instead of re-training them from scratch. However, the distribution shift induced by novel data typically results in degraded performance on past data. Taking a step towards efficient continual pre-training, in this work, we examine the effect of different warm-up strategies. Our hypothesis is that the learning rate must be re-increased to improve compute efficiency when training on a new dataset. We study the warmup phase of models pre-trained on the Pile (upstream data, 300B tokens) as we continue to pre-train on SlimPajama (downstream data, 297B tokens), following a linear warmup and cosine decay schedule. We conduct all experiments on the Pythia 410M language model architecture and ev
    
[^127]: GraPhSyM: 图形物理综合模型

    GraPhSyM: Graph Physical Synthesis Model. (arXiv:2308.03944v1 [cs.LG])

    [http://arxiv.org/abs/2308.03944](http://arxiv.org/abs/2308.03944)

    GraPhSyM是一种用于从物理合成电路网表中快速准确地估计后物理合成电路延迟和面积指标的模型，提供了准确的指标可见性给早期的EDA阶段，可用于全局协同优化，并对基于机器学习的EDA优化框架具有重要的作用。

    

    在这项工作中，我们介绍了GraPhSyM，一种用于从物理合成电路网表中快速准确地估计后物理合成电路延迟和面积指标的图形注意力网络（GATv2）模型。一旦训练完毕，GraPhSyM可以提供准确的设计指标可见性给早期的EDA阶段，如逻辑综合，而无需运行缓慢的物理合成流程，从而实现跨阶段的全局协同优化。此外，GraPhSym提供的快速而精确的反馈对基于机器学习的EDA优化框架至关重要。给定一个表示为图形的电路门级网表，GraPhSyM利用图形结构、连接性和电性特征来预测物理合成转换（如缓冲器插入和门尺寸调整）对电路的影响。当在一个包含6000个前缀加法器设计的数据集上训练（合成到激进延迟目标），GraPhSyM可以准确预测后合成的延迟（98.3%）和面积（96.1%）。

    In this work, we introduce GraPhSyM, a Graph Attention Network (GATv2) model for fast and accurate estimation of post-physical synthesis circuit delay and area metrics from pre-physical synthesis circuit netlists. Once trained, GraPhSyM provides accurate visibility of final design metrics to early EDA stages, such as logic synthesis, without running the slow physical synthesis flow, enabling global co-optimization across stages. Additionally, the swift and precise feedback provided by GraPhSym is instrumental for machine-learning-based EDA optimization frameworks. Given a gate-level netlist of a circuit represented as a graph, GraPhSyM utilizes graph structure, connectivity, and electrical property features to predict the impact of physical synthesis transformations such as buffer insertion and gate sizing. When trained on a dataset of 6000 prefix adder designs synthesized at an aggressive delay target, GraPhSyM can accurately predict the post-synthesis delay (98.3%) and area (96.1%) m
    
[^128]: RAHNet: 检索增强型混合网络用于长尾图分类

    RAHNet: Retrieval Augmented Hybrid Network for Long-tailed Graph Classification. (arXiv:2308.02335v1 [cs.LG])

    [http://arxiv.org/abs/2308.02335](http://arxiv.org/abs/2308.02335)

    我们提出了一种检索增强型混合网络(RAHNet)用于长尾图分类任务，通过联合学习稳健的特征提取器和无偏的分类器，解决了图神经网络在长尾类别分布下的偏差和泛化能力有限的问题。

    

    图分类是许多实际多媒体应用中的关键任务，图可以表示各种多媒体数据类型，如图像、视频和社交网络。以往的研究在平衡的情况下应用图神经网络(GNN)，其中类分布是平衡的。然而，实际数据通常呈现出长尾类别分布，导致在使用GNN时对头部类别存在偏差，且对尾部类别的泛化能力有限。最近的方法主要集中在模型训练过程中重新平衡不同的类别，但这种方法未能明确引入新知识，并牺牲了头部类别的性能。为了解决这些缺点，我们提出了一种新的框架，称为检索增强型混合网络(RAHNet)，以分离的方式联合学习稳健的特征提取器和无偏的分类器。在特征提取器训练阶段，我们开发了一个图检索模块来搜索相关图形。

    Graph classification is a crucial task in many real-world multimedia applications, where graphs can represent various multimedia data types such as images, videos, and social networks. Previous efforts have applied graph neural networks (GNNs) in balanced situations where the class distribution is balanced. However, real-world data typically exhibit long-tailed class distributions, resulting in a bias towards the head classes when using GNNs and limited generalization ability over the tail classes. Recent approaches mainly focus on re-balancing different classes during model training, which fails to explicitly introduce new knowledge and sacrifices the performance of the head classes. To address these drawbacks, we propose a novel framework called Retrieval Augmented Hybrid Network (RAHNet) to jointly learn a robust feature extractor and an unbiased classifier in a decoupled manner. In the feature extractor training stage, we develop a graph retrieval module to search for relevant grap
    
[^129]: VLUCI: 可变参数学习未观测混淆变量进行反事实推断

    VLUCI: Variational Learning of Unobserved Confounders for Counterfactual Inference. (arXiv:2308.00904v1 [cs.LG])

    [http://arxiv.org/abs/2308.00904](http://arxiv.org/abs/2308.00904)

    VLUCI是一个新颖的可变参数学习模型，用于解决反事实推断中的未观测混淆变量的问题。它通过生成未观测混淆变量的后验分布，并构建一个双重变分推断模型来解决因果推断中观测和未观测混淆变量的问题，从而提高反事实推断的准确性。

    

    因果推断在流行病学、医疗保健和经济学等领域中起着重要作用。在观察数据中进行去混淆和反事实预测已经成为因果推断研究中的一个重要问题。虽然现有模型可以处理观察到的混淆变量，但未观测到的混淆变量的存在仍然是一个重大挑战，扭曲了因果推断并影响了反事实结果的准确性。为了解决这个问题，我们提出了一个新颖的可变参数学习模型，用于反事实推断中的未观测混淆变量（VLUCI），它生成了未观测混淆变量的后验分布。VLUCI放松了大多数因果推断方法往往忽视的无混淆假设。通过解耦观察到的混淆变量和未观测到的混淆变量，VLUCI构建了一个双重变分推断模型，以近似未观测混淆变量的分布，这些变量用于推断更准确的反事实结果。对合成和实际数据上进行了大量实验。

    Causal inference plays a vital role in diverse domains like epidemiology, healthcare, and economics. De-confounding and counterfactual prediction in observational data has emerged as a prominent concern in causal inference research. While existing models tackle observed confounders, the presence of unobserved confounders remains a significant challenge, distorting causal inference and impacting counterfactual outcome accuracy. To address this, we propose a novel variational learning model of unobserved confounders for counterfactual inference (VLUCI), which generates the posterior distribution of unobserved confounders. VLUCI relaxes the unconfoundedness assumption often overlooked by most causal inference methods. By disentangling observed and unobserved confounders, VLUCI constructs a doubly variational inference model to approximate the distribution of unobserved confounders, which are used for inferring more accurate counterfactual outcomes. Extensive experiments on synthetic and s
    
[^130]: 一个对深度学习模型的补丁稳健性认证的大多数不变方法

    A Majority Invariant Approach to Patch Robustness Certification for Deep Learning Models. (arXiv:2308.00452v1 [cs.LG])

    [http://arxiv.org/abs/2308.00452](http://arxiv.org/abs/2308.00452)

    该论文提出了MajorCert，通过找到在底层分类器中由同一个补丁区域在同一个样本上可以操纵的所有可能标签集合，并检查它们的大多数不变性来对样本进行补丁稳健性认证。

    

    补丁稳健性认证确保在给定样本上，没有补丁能够通过操纵深度学习模型来预测不同的标签。然而，现有的技术不能对无法在分类器或者补丁区域水平上达到严格标准的样本进行认证。本文提出了MajorCert。MajorCert首先找到在底层分类器中由同一个补丁区域在同一个样本上可以操纵的所有可能的标签集合，然后逐个枚举它们的组合，并最终检查所有这些组合的大多数不变性是否完整以对样本进行认证。

    Patch robustness certification ensures no patch within a given bound on a sample can manipulate a deep learning model to predict a different label. However, existing techniques cannot certify samples that cannot meet their strict bars at the classifier or patch region levels. This paper proposes MajorCert. MajorCert firstly finds all possible label sets manipulatable by the same patch region on the same sample across the underlying classifiers, then enumerates their combinations element-wise, and finally checks whether the majority invariant of all these combinations is intact to certify samples.
    
[^131]: Take-A-Photo: 三维到二维的点云模型生成预训练

    Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models. (arXiv:2307.14971v1 [cs.CV])

    [http://arxiv.org/abs/2307.14971](http://arxiv.org/abs/2307.14971)

    本文提出了一种新的三维到二维的生成式预训练方法，通过生成视图图像作为预训练方案，帮助三维模型更好地理解点云的几何结构和立体关系，并在实验证明了其优越性。

    

    在MAE带领下，生成式预训练在2D视觉领域已经显示出显著的潜力来提升基本模型的性能。然而，在3D视觉领域，对Transformer为基础的骨干网络的过度依赖以及点云的无序性限制了生成式预训练的进一步发展。本文提出了一种新颖的适用于任何点云模型的三维到二维的生成式预训练方法。我们通过交叉注意机制从不同的姿势生成视图图像作为预训练方案。相比于其点云对应物，生成视图图像具有更精确的监督，从而帮助3D背骨更好地理解点云的几何结构和立体关系。实验结果证明了我们提出的三维到二维的生成式预训练方法优于先前的预训练方法。我们的方法还能有效地提升...

    With the overwhelming trend of mask image modeling led by MAE, generative pre-training has shown a remarkable potential to boost the performance of fundamental models in 2D vision. However, in 3D vision, the over-reliance on Transformer-based backbones and the unordered nature of point clouds have restricted the further development of generative pre-training. In this paper, we propose a novel 3D-to-2D generative pre-training method that is adaptable to any point cloud model. We propose to generate view images from different instructed poses via the cross-attention mechanism as the pre-training scheme. Generating view images has more precise supervision than its point cloud counterpart, thus assisting 3D backbones to have a finer comprehension of the geometrical structure and stereoscopic relations of the point cloud. Experimental results have proved the superiority of our proposed 3D-to-2D generative pre-training over previous pre-training methods. Our method is also effective in boost
    
[^132]: 通过单向流进行对抗性似然估计

    Adversarial Likelihood Estimation with One-way Flows. (arXiv:2307.09882v1 [cs.LG])

    [http://arxiv.org/abs/2307.09882](http://arxiv.org/abs/2307.09882)

    本文提出了一种通过单向流进行对抗性似然估计的方法，并使用重要性采样解决了Wasserstein GAN中分区函数有偏估计的问题。同时，通过最大化生成器的熵，提高了模式覆盖效果。这种方法通过计算生成样本的密度来实现对分区函数的无偏估计和生成器熵的计算。

    

    生成对抗网络（GAN）能够产生高质量的样本，但无法提供样本周围的概率密度估计。然而，已经注意到在能量模型的设置中，最大化对数似然可以导致判别器提供非归一化的密度（通常称为能量）的对抗性框架。我们进一步发展了这一观点，结合重要性采样，并展示了以下内容：1）Wasserstein GAN对分区函数进行了有偏估计，我们提出使用无偏估计方法；2）在最优化似然时，必须最大化生成器的熵。这被假设会提供更好的模式覆盖。与以前的工作不同，我们明确计算了生成样本的密度。这是设计无偏估计分区函数以及计算生成器熵的关键因素。生成密度是通过一种新型的流网络来获得的，称为单向流网络。

    Generative Adversarial Networks (GANs) can produce high-quality samples, but do not provide an estimate of the probability density around the samples. However, it has been noted that maximizing the log-likelihood within an energy-based setting can lead to an adversarial framework where the discriminator provides unnormalized density (often called energy). We further develop this perspective, incorporate importance sampling, and show that 1) Wasserstein GAN performs a biased estimate of the partition function, and we propose instead to use an unbiased estimator; 2) when optimizing for likelihood, one must maximize generator entropy. This is hypothesized to provide a better mode coverage. Different from previous works, we explicitly compute the density of the generated samples. This is the key enabler to designing an unbiased estimator of the partition function and computation of the generator entropy term. The generator density is obtained via a new type of flow network, called one-way 
    
[^133]: 深度网络逼近：从ReLU到多种激活函数

    Deep Network Approximation: Beyond ReLU to Diverse Activation Functions. (arXiv:2307.06555v1 [cs.LG])

    [http://arxiv.org/abs/2307.06555](http://arxiv.org/abs/2307.06555)

    本文研究了深度神经网络在多种激活函数下的表达能力，证明了可以通过在有界集合上构建一个宽度为6N、深度为2L的varrho激活网络来逼近一个宽度为N、深度为L的ReLU网络，从而将对ReLU网络的逼近结果推广到其他激活函数。

    

    本文探究了深度神经网络在多种激活函数下的表达能力。定义了一个激活函数集合A，包括大多数常用的激活函数，如ReLU、LeakyReLU、ReLU^2、ELU、SELU、Softplus、GELU、SiLU、Swish、Mish、Sigmoid、Tanh、Arctan、Softsign、dSiLU和SRS。我们证明了对于任意激活函数varrho∈A，可以通过一个宽度为6N、深度为2L的varrho激活网络在有界集合上以任意精度逼近一个宽度为N、深度为L的ReLU网络。这一发现使得大部分对于ReLU网络的逼近结果能够推广到其他激活函数，尽管需要稍大的常数代价。

    This paper explores the expressive power of deep neural networks for a diverse range of activation functions. An activation function set $\mathscr{A}$ is defined to encompass the majority of commonly used activation functions, such as $\mathtt{ReLU}$, $\mathtt{LeakyReLU}$, $\mathtt{ReLU}^2$, $\mathtt{ELU}$, $\mathtt{SELU}$, $\mathtt{Softplus}$, $\mathtt{GELU}$, $\mathtt{SiLU}$, $\mathtt{Swish}$, $\mathtt{Mish}$, $\mathtt{Sigmoid}$, $\mathtt{Tanh}$, $\mathtt{Arctan}$, $\mathtt{Softsign}$, $\mathtt{dSiLU}$, and $\mathtt{SRS}$. We demonstrate that for any activation function $\varrho\in \mathscr{A}$, a $\mathtt{ReLU}$ network of width $N$ and depth $L$ can be approximated to arbitrary precision by a $\varrho$-activated network of width $6N$ and depth $2L$ on any bounded set. This finding enables the extension of most approximation results achieved with $\mathtt{ReLU}$ networks to a wide variety of other activation functions, at the cost of slightly larger constants.
    
[^134]: 注意力机制中的边缘最大化

    Margin Maximization in Attention Mechanism. (arXiv:2306.13596v1 [cs.LG])

    [http://arxiv.org/abs/2306.13596](http://arxiv.org/abs/2306.13596)

    这篇论文证明了，在softmax-attention模型中，通过在p或等价的W上运行梯度下降，可以收敛到一个最大边缘解，这将局部最优的标记与非最优的标记分隔开。这明确地将注意力机制形式化为标记分离机制。

    

    注意力机制是Transformer架构的核心组件，也是大型语言模型取得惊人成功的原因之一。然而，注意力机制背后的理论原则尚不清楚，特别是它的非凸优化动力学。本文探讨了开创性的softmax-attention模型$f(\boldsymbol{X})=\langle \boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$，其中$\boldsymbol{X}$是标记序列，$(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$是可调参数。我们证明了在$\boldsymbol{p}$或等价的$\boldsymbol{W}$上运行梯度下降会沿着方向收敛到分隔“局部最优”标记和“非最优”标记的最大边缘解。这明确地形式化了注意力作为一种标记分离机制。值得注意的是，我们的结果适用于一般数据，并使用嵌入$\boldsymbol{Xv}$和$\texttt{softmax}(\boldsymbol{XWp})$精细地表征标记的“最优性”。

    Attention mechanism is a central component of the transformer architecture which led to the phenomenal success of large language models. However, the theoretical principles underlying the attention mechanism are poorly understood, especially its nonconvex optimization dynamics. In this work, we explore the seminal softmax-attention model $f(\boldsymbol{X})=\langle \boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$, where, $\boldsymbol{X}$ is the token sequence and $(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$ are tunable parameters. We prove that running gradient descent on $\boldsymbol{p}$, or equivalently $\boldsymbol{W}$, converges in direction to a max-margin solution that separates $\textit{locally-optimal}$ tokens from non-optimal ones. This clearly formalizes attention as a token separation mechanism. Remarkably, our results are applicable to general data and precisely characterize $\textit{optimality}$ of tokens in terms of the value embeddings $\boldsymbol{Xv}$ and
    
[^135]: 线性约束下的多臂赌博纯探索算法

    Pure Exploration in Bandits with Linear Constraints. (arXiv:2306.12774v1 [cs.LG])

    [http://arxiv.org/abs/2306.12774](http://arxiv.org/abs/2306.12774)

    本文提出了两种相对于线性约束下的多臂赌博问题都是渐进最优的算法。这两种算法都试图跟踪基于下界计算和通过对正常锥体边界进行加权投影所获得的最优分配。

    

    本文解决多臂赌博问题中存在线性约束的情况下，如何在一定置信度下确定最优策略的问题。与标准的最优臂识别问题不同，这种情况下的最优策略可能不是确定性的，而是可能在多个臂之间进行混合。这种情况改变了问题的几何形状，我们通过信息论下界进行了描述。我们提出了两种相对于此设置都是渐进最优的算法，其中一个基于“跟踪停止”方法，另一个基于博弈理论的方法。这两种算法都试图跟踪基于下界计算和通过对正常锥体边界进行加权投影所获得的最优分配。最后，我们提供了实证结果，验证了我们的界限，并展示了约束如何改变问题的难度。

    We address the problem of identifying the optimal policy with a fixed confidence level in a multi-armed bandit setup, when \emph{the arms are subject to linear constraints}. Unlike the standard best-arm identification problem which is well studied, the optimal policy in this case may not be deterministic and could mix between several arms. This changes the geometry of the problem which we characterize via an information-theoretic lower bound. We introduce two asymptotically optimal algorithms for this setting, one based on the Track-and-Stop method and the other based on a game-theoretic approach. Both these algorithms try to track an optimal allocation based on the lower bound and computed by a weighted projection onto the boundary of a normal cone. Finally, we provide empirical results that validate our bounds and visualize how constraints change the hardness of the problem.
    
[^136]: 混合式神经辐射场：零样本物体生成与混合

    Blended-NeRF: Zero-Shot Object Generation and Blending in Existing Neural Radiance Fields. (arXiv:2306.12760v1 [cs.CV])

    [http://arxiv.org/abs/2306.12760](http://arxiv.org/abs/2306.12760)

    Blended-NeRF是一种鲁棒而灵活的编辑NeRF场景中感兴趣的区域的框架，在保持自然性与一致性的情况下，它可以将用户提供的文本提示或图像补丁的物体合成并混合到原始场景中。

    

    在3D场景中编辑局部区域或特定物体并混合到已有的NeRF场景中，由于场景表示的隐式属性，这是一个具有挑战性的问题。我们提出了Blended-NeRF，一种基于文本提示或图像补丁以及3D ROI包围盒的NeRF场景中感兴趣的区域的编辑的鲁棒而灵活的框架。我们的方法利用预先训练的语言-图像模型来引导合成朝向用户提供的文本提示或图像补丁，还利用一个已存在的NeRF场景上初始化的3D MLP模型来生成物体并将其混合到原始场景中的指定区域。我们通过将3D ROI盒局部化以实现局部编辑，并利用新颖的体积混合技术将内部合成内容无缝混合到现有场景中，以获得自然而一致的结果。

    Editing a local region or a specific object in a 3D scene represented by a NeRF is challenging, mainly due to the implicit nature of the scene representation. Consistently blending a new realistic object into the scene adds an additional level of difficulty. We present Blended-NeRF, a robust and flexible framework for editing a specific region of interest in an existing NeRF scene, based on text prompts or image patches, along with a 3D ROI box. Our method leverages a pretrained language-image model to steer the synthesis towards a user-provided text prompt or image patch, along with a 3D MLP model initialized on an existing NeRF scene to generate the object and blend it into a specified region in the original scene. We allow local editing by localizing a 3D ROI box in the input scene, and seamlessly blend the content synthesized inside the ROI with the existing scene using a novel volumetric blending technique. To obtain natural looking and view-consistent results, we leverage existin
    
[^137]: ClimSim：用于在混合多尺度气候模拟器中训练高分辨率物理仿真器的开源大规模数据集

    ClimSim: An open large-scale dataset for training high-resolution physics emulators in hybrid multi-scale climate simulators. (arXiv:2306.08754v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.08754](http://arxiv.org/abs/2306.08754)

    这是一个用于训练高分辨率物理仿真器的气候数据集。该数据集包含了5.7亿个多变量输入和输出矢量对，用于隔离本地嵌套的高分辨率、高保真度物理的影响。

    

    现代气候预测由于计算限制缺乏足够的空间和时间分辨率。一个后果是对关键过程（如暴风雨）的预测不准确和不精确。将物理和机器学习（ML）相结合的混合模式引入了新一代更高保真度的气候模拟器，通过将计算密集型、短、高分辨率的模拟委托给ML仿真器，可以避免摩尔定律问题。然而，这种混合的ML-物理仿真方法需要领域特定的处理，并且由于缺乏培训数据和相关的易于使用的工作流程，一直无法访问ML专家。我们提出了 ClimSim，这是迄今为止为混合ML-物理研究而设计的最大数据集。它由气候科学家和ML研究人员联合开发的多尺度气候模拟组成，包括57亿个多变量输入和输出矢量对，隔离了本地嵌套的高分辨率和高保真度物理学的影响。

    Modern climate projections lack adequate spatial and temporal resolution due to computational constraints. A consequence is inaccurate and imprecise prediction of critical processes such as storms. Hybrid methods that combine physics with machine learning (ML) have introduced a new generation of higher fidelity climate simulators that can sidestep Moore's Law by outsourcing compute-hungry, short, high-resolution simulations to ML emulators. However, this hybrid ML-physics simulation approach requires domain-specific treatment and has been inaccessible to ML experts because of lack of training data and relevant, easy-to-use workflows. We present ClimSim, the largest-ever dataset designed for hybrid ML-physics research. It comprises multi-scale climate simulations, developed by a consortium of climate scientists and ML researchers. It consists of 5.7 billion pairs of multivariate input and output vectors that isolate the influence of locally-nested, high-resolution, high-fidelity physics
    
[^138]: 双非均匀环境下的离线策略评估

    Off-policy Evaluation in Doubly Inhomogeneous Environments. (arXiv:2306.08719v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2306.08719](http://arxiv.org/abs/2306.08719)

    本研究在双非均匀环境下研究了离线策略评估(OPE)，提出了一种潜在因子模型用于奖励和观测转移函数，并开发了一个通用的OPE框架。该研究对标准RL假设不满足的环境中的OPE做出了理论贡献，并提供了几种实用的方法。

    

    本研究旨在研究离线策略评估（OPE）在两个关键的强化学习（RL）假设——时间稳定性和个体均匀性均被破坏的情况下的应用。为了处理“双非均匀性”，我们提出了一类潜在因子模型用于奖励和观测转移函数，并在此基础上开发了一个包含模型驱动和模型自由方法的通用OPE框架。据我们所知，这是第一篇在离线RL中开发统计上可靠的OPE方法的论文，并且涉及了标准RL假设不满足的环境。该研究深入理解了标准RL假设不满足的环境中的OPE，并在这些设置中提供了几种实用的方法。我们确定了所提出的价值估计器的理论性质，并通过实证研究表明我们的方法优于忽视时间非稳定性或个体异质性的竞争方法。最后，我们在一个数据集上说明了我们的方法。

    This work aims to study off-policy evaluation (OPE) under scenarios where two key reinforcement learning (RL) assumptions -- temporal stationarity and individual homogeneity are both violated. To handle the ``double inhomogeneities", we propose a class of latent factor models for the reward and observation transition functions, under which we develop a general OPE framework that consists of both model-based and model-free approaches. To our knowledge, this is the first paper that develops statistically sound OPE methods in offline RL with double inhomogeneities. It contributes to a deeper understanding of OPE in environments, where standard RL assumptions are not met, and provides several practical approaches in these settings. We establish the theoretical properties of the proposed value estimators and empirically show that our approach outperforms competing methods that ignore either temporal nonstationarity or individual heterogeneity. Finally, we illustrate our method on a data set
    
[^139]: 动态因果图卷积网络用于交通预测

    Dynamic Causal Graph Convolutional Network for Traffic Prediction. (arXiv:2306.07019v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.07019](http://arxiv.org/abs/2306.07019)

    本文提出了一种动态因果图卷积网络的交通预测方法，通过嵌入时间变化的动态贝叶斯网络和使用图卷积网络来生成交通预测，能够有效地预测交通数据的非线性传播模式。

    

    对相关交通序列中的复杂时空依赖关系进行建模对于交通预测至关重要。尽管最近的研究通过使用神经网络来提取时空相关性，已经展示了预测性能的提升，但其有效性取决于用于表示交通网络的空间拓扑的图结构的质量。在这项工作中，我们提出了一种新的交通预测方法，将时间变化的动态贝叶斯网络嵌入到交通数据的细粒度时空拓扑中。然后，我们使用图卷积网络生成交通预测。为了使我们的方法能够有效地模拟非线性交通传播模式，我们开发了一个基于深度学习的模块作为超网络来生成逐步动态因果图。我们在真实的交通数据集上的实验结果展示了所提方法的优越预测性能。代码可在 https://github.com/MonBG/DCGCN 找到。

    Modeling complex spatiotemporal dependencies in correlated traffic series is essential for traffic prediction. While recent works have shown improved prediction performance by using neural networks to extract spatiotemporal correlations, their effectiveness depends on the quality of the graph structures used to represent the spatial topology of the traffic network. In this work, we propose a novel approach for traffic prediction that embeds time-varying dynamic Bayesian network to capture the fine spatiotemporal topology of traffic data. We then use graph convolutional networks to generate traffic forecasts. To enable our method to efficiently model nonlinear traffic propagation patterns, we develop a deep learning-based module as a hyper-network to generate stepwise dynamic causal graphs. Our experimental results on a real traffic dataset demonstrate the superior prediction performance of the proposed method. The code is available at https://github.com/MonBG/DCGCN.
    
[^140]: LDMRes-Net：通过高效图像分割实现实时疾病监测

    LDMRes-Net: Enabling Real-Time Disease Monitoring through Efficient Image Segmentation. (arXiv:2306.06145v1 [eess.IV])

    [http://arxiv.org/abs/2306.06145](http://arxiv.org/abs/2306.06145)

    LDMRes-Net是一种轻量级高效的网络，通过采用双重多重残差连接来提高分割性能并最小化计算成本，可用于实时视网膜图像分析任务，并在8个公开数据集上取得了有希望的分割结果。

    

    视网膜眼病如果不及早诊断和治疗，可能导致双眼不可逆的视力损失。由于视网膜疾病的复杂性，视网膜图像可能包含两个或更多异常。目前用于分割带有多个标签和特征的视网膜图像的深度学习算法存在检测准确性不足和泛化性不强的问题。本文提出了一种轻量级高效的网络，采用双重多重残差连接增强分割性能并最小化计算成本。所提出的网络在8个公开可用的视网膜图像数据集上进行了评估，并取得了有希望的分割结果，证明了所提出的网络对于视网膜图像分析任务的有效性。所提出的网络的轻量级高效设计使其成为实时视网膜图像分析应用的有希望的候选者。

    Retinal eye diseases can lead to irreversible vision loss in both eyes if not diagnosed and treated earlier. Owing to the complexities of retinal diseases, the likelihood that retinal images would contain two or more abnormalities is very high. The current deep learning algorithms used for segmenting retinal images with multiple labels and features suffer from inadequate detection accuracy and a lack of generalizability. In this paper, we propose a lightweight and efficient network, featuring dual multi-residual connections to enhance segmentation performance while minimizing computational cost. The proposed network is evaluated on eight publicly available retinal image datasets and achieved promising segmentation results, which demonstrate the effectiveness of the proposed network for retinal image analysis tasks. The proposed network's lightweight and efficient design makes it a promising candidate for real-time retinal image analysis applications.
    
[^141]: 无法学习的样本给出了一种虚假的安全感：通过可学习的例子穿透那些无法利用的数据

    Unlearnable Examples Give a False Sense of Security: Piercing through Unexploitable Data with Learnable Examples. (arXiv:2305.09241v1 [cs.LG])

    [http://arxiv.org/abs/2305.09241](http://arxiv.org/abs/2305.09241)

    “无法学习的样本”提出一种对数据进行保护的方法，但它无法阻止未经授权的用户对保护后的数据进行利用。通过提出“可学习的未经授权示例”和一种新的纯化过程，我们可以实现对数据的更好保护。

    

    在当下随处可见的安全漏洞中，保护数据免于未经授权的利用是至关重要的。最近，一种叫做“无法学习的样本”（UEs）的方法被提出，通过对数据进行微小的扰动，使得模型无法在原始的干净分布上准确地对其进行分类，从而提供了一种强大的保护措施。然而，我们发现 UEs 带来的安全威胁是虚假的，因为它们无法阻止未经授权的用户利用其他未受保护的数据来去除保护，将无法学习的数据重转为可学习。基于这一观察，我们正式定义了一种威胁，引入了“可学习的未经授权示例”（LEs），这些是已经去除保护的UEs。我们的方法的核心是通过一种新的纯化过程，将UEs投射到LEs的流形上。这是通过一种新的联合条件扩散模型来实现的，该模型对UEs进行去噪。

    Safeguarding data from unauthorized exploitation is vital for privacy and security, especially in recent rampant research in security breach such as adversarial/membership attacks. To this end, \textit{unlearnable examples} (UEs) have been recently proposed as a compelling protection, by adding imperceptible perturbation to data so that models trained on them cannot classify them accurately on original clean distribution. Unfortunately, we find UEs provide a false sense of security, because they cannot stop unauthorized users from utilizing other unprotected data to remove the protection, by turning unlearnable data into learnable again. Motivated by this observation, we formally define a new threat by introducing \textit{learnable unauthorized examples} (LEs) which are UEs with their protection removed. The core of this approach is a novel purification process that projects UEs onto the manifold of LEs. This is realized by a new joint-conditional diffusion model which denoises UEs con
    
[^142]: 通过对比学习实现乳腺X线摄影图像分析的域泛化

    Domain Generalization for Mammographic Image Analysis via Contrastive Learning. (arXiv:2304.10226v1 [cs.CV])

    [http://arxiv.org/abs/2304.10226](http://arxiv.org/abs/2304.10226)

    研究人员开发出一种基于对比学习的域泛化方法，通过自监督学习生成多种风格和视角的特征嵌入，进一步微调骨干网络以提高分类任务性能。

    

    乳腺X线摄影图像分析是医学影像学领域的一个基本问题，近年来，随着深度学习的不断发展，该领域取得了显著的进展。然而，构建深度学习模型需要大量的具有多样性的图像数据，尤其是对于不同厂商的图像风格，这往往需要非常庞大的样本集。因此，为了提高深度学习模型泛化到不同厂商图像的能力，研究者提出了一种基于对比学习的策略。

    Mammographic image analysis is a fundamental problem in the computer-aided diagnosis scheme, which has recently made remarkable progress with the advance of deep learning. However, the construction of a deep learning model requires training data that are large and sufficiently diverse in terms of image style and quality. In particular, the diversity of image style may be majorly attributed to the vendor factor. However, mammogram collection from vendors as many as possible is very expensive and sometimes impractical for laboratory-scale studies. Accordingly, to further augment the generalization capability of deep learning models to various vendors with limited resources, a new contrastive learning scheme is developed. Specifically, the backbone network is firstly trained with a multi-style and multi-view unsupervised self-learning scheme for the embedding of invariant features to various vendor styles. Afterward, the backbone network is then recalibrated to the downstream tasks of mas
    
[^143]: DiFaReli: 扩散人脸重照技术

    DiFaReli : Diffusion Face Relighting. (arXiv:2304.09479v1 [cs.CV])

    [http://arxiv.org/abs/2304.09479](http://arxiv.org/abs/2304.09479)

    DiFaReli提出了一种新方法，通过利用条件扩散隐式模型解码解耦的光编码以及从现成的估算器推断出的与3D形状和面部身份相关的其他编码，能够处理单视角的野外环境下的人脸重照，无需光线舞台数据、多视图图像或光照基础事实，实验表明其效果优于现有方法。

    

    我们提出了一种新方法，用于处理野外环境下的单视角人脸重照。处理全局照明或投影阴影等非漫反射效应一直是人脸重照领域的难点。以往的研究通常假定兰伯特反射表面，简化光照模型，或者需要估计三维形状、反射率或阴影图。然而，这种估计是容易出错的，需要许多具有光照基础事实的训练样本才能很好地推广。我们的研究绕过了准确估计固有组件的需要，可以仅通过2D图像训练而不需要任何光线舞台数据、多视图图像或光照基础事实。我们的关键思想是利用条件扩散隐式模型（DDIM）解码解耦的光编码以及从现成的估算器推断出的与3D形状和面部身份相关的其他编码。我们还提出了一种新的调节技术，通过使用归一化方案，简化光与几何之间复杂互动的建模。在多个基准数据集上的实验表明，我们的方法优于现有方法。

    We present a novel approach to single-view face relighting in the wild. Handling non-diffuse effects, such as global illumination or cast shadows, has long been a challenge in face relighting. Prior work often assumes Lambertian surfaces, simplified lighting models or involves estimating 3D shape, albedo, or a shadow map. This estimation, however, is error-prone and requires many training examples with lighting ground truth to generalize well. Our work bypasses the need for accurate estimation of intrinsic components and can be trained solely on 2D images without any light stage data, multi-view images, or lighting ground truth. Our key idea is to leverage a conditional diffusion implicit model (DDIM) for decoding a disentangled light encoding along with other encodings related to 3D shape and facial identity inferred from off-the-shelf estimators. We also propose a novel conditioning technique that eases the modeling of the complex interaction between light and geometry by using a ren
    
[^144]: 基于原始-对偶语境贝叶斯优化的带时间平均约束的控制系统在线优化

    Primal-Dual Contextual Bayesian Optimization for Control System Online Optimization with Time-Average Constraints. (arXiv:2304.06104v1 [cs.LG])

    [http://arxiv.org/abs/2304.06104](http://arxiv.org/abs/2304.06104)

    提出了一种基于原始-对偶语境贝叶斯优化算法，可以实现对约束闭环控制系统的在线性能优化，同时满足所需的约束条件。

    

    本文研究带有外生时间变化上下文干扰的未知黑盒函数的约束闭环控制系统在线性能优化问题。提出了一种原始-对偶语境贝叶斯优化算法，在满足一定正则条件下，实现了对动态最优解的亚线性累积遗憾。此外，该算法可以实现零时间平均约束违规，确保了约束函数的平均值满足所需的约束条件。该方法应用于高斯过程的采样实例和连续搅拌槽反应器参数调节问题。仿真结果表明，该方法同时提供接近最优的性能和平均保持约束可行性，这与当前的最先进方法形成对比，后者要么遭受大量累积遗憾，要么存在严重约束违规问题。

    This paper studies the problem of online performance optimization of constrained closed-loop control systems, where both the objective and the constraints are unknown black-box functions affected by exogenous time-varying contextual disturbances. A primal-dual contextual Bayesian optimization algorithm is proposed that achieves sublinear cumulative regret with respect to the dynamic optimal solution under certain regularity conditions. Furthermore, the algorithm achieves zero time-average constraint violation, ensuring that the average value of the constraint function satisfies the desired constraint. The method is applied to both sampled instances from Gaussian processes and a continuous stirred tank reactor parameter tuning problem; simulation results show that the method simultaneously provides close-to-optimal performance and maintains constraint feasibility on average. This contrasts current state-of-the-art methods, which either suffer from large cumulative regret or severe const
    
[^145]: 实践中知识图谱用户、挑战和可视化需求的特征化研究

    Characterizing the Users, Challenges, and Visualization Needs of Knowledge Graphs in Practice. (arXiv:2304.01311v1 [cs.HC])

    [http://arxiv.org/abs/2304.01311](http://arxiv.org/abs/2304.01311)

    本研究通过访谈19位知识图谱（KG）实践者，发现KG构建者需求架构执行程序，KG分析师需要可自定义查询构建器，KG消费者需要领域特定可视化，并指出在实践中实施KG需要技术和社交方面的解决方案。

    

    本研究通过对19位来自企业和学术环境下、涉及各种用例的知识图谱（KG）实践者的访谈，提出了KG实践者在创建、探索和分析KG时遇到的重要挑战，这些挑战可以通过可视化设计来缓解。我们的研究发现，KG实践者可以分为三类：KG构建者、分析师和消费者，每个人都有自己的专业知识和需求。我们发现，KG构建者可以从架构执行程序中获益，而KG分析师需要提供中间查询结果的可自定义查询构建器。对于KG消费者，我们确定节点链接图的效力不足，并需要定制的领域特定可视化来促进KG的采用和理解。最后，我们发现，在实践中有效地实施KG需要不仅技术上的，还有社交上的解决方案，而这些解决方案目前并未被当前的工具、技术和最佳实践所考虑。

    This study presents insights from interviews with nineteen Knowledge Graph (KG) practitioners who work in both enterprise and academic settings on a wide variety of use cases. Through this study, we identify critical challenges experienced by KG practitioners when creating, exploring, and analyzing KGs that could be alleviated through visualization design. Our findings reveal three major personas among KG practitioners - KG Builders, Analysts, and Consumers - each of whom have their own distinct expertise and needs. We discover that KG Builders would benefit from schema enforcers, while KG Analysts need customizable query builders that provide interim query results. For KG Consumers, we identify a lack of efficacy for node-link diagrams, and the need for tailored domain-specific visualizations to promote KG adoption and comprehension. Lastly, we find that implementing KGs effectively in practice requires both technical and social solutions that are not addressed with current tools, tec
    
[^146]: FixFit：使用参数压缩解决超定模型中的逆问题

    FixFit: using parameter-compression to solve the inverse problem in overdetermined models. (arXiv:2303.13746v1 [cs.LG])

    [http://arxiv.org/abs/2303.13746](http://arxiv.org/abs/2303.13746)

    FixFit是一种使用神经网络进行参数压缩的方法，可以解决复杂非线性模型中由于参数之间的相互作用导致的多参数集问题。

    

    所有科学领域都依赖于数学模型。使用复杂的非线性模型的一个基本问题是，基于数据的参数估计经常失败，因为模型参数之间的相互作用导致多个参数集同样适合数据。在这里，我们开发了一种解决这个问题的新方法FixFit，将给定数学模型的参数压缩为独特于模型输出的潜在表示。我们通过在模型参数和模型输出的数据对上训练带有瓶颈层的神经网络来获得此表示。瓶颈层节点对应于唯一的潜在参数，其维度表示模型的信息内容。训练后的神经网络可以在瓶颈层分裂成编码器来描述冗余信息和解码器来从测量中唯一推断潜在参数。我们在来自经典物理学和神经科学的两个应用案例中展示了FixFit。

    All fields of science depend on mathematical models. One of the fundamental problems with using complex nonlinear models is that data-driven parameter estimation often fails because interactions between model parameters lead to multiple parameter sets fitting the data equally well. Here, we develop a new method to address this problem, FixFit, which compresses a given mathematical model's parameters into a latent representation unique to model outputs. We acquire this representation by training a neural network with a bottleneck layer on data pairs of model parameters and model outputs. The bottleneck layer nodes correspond to the unique latent parameters, and their dimensionality indicates the information content of the model. The trained neural network can be split at the bottleneck layer into an encoder to characterize the redundancies and a decoder to uniquely infer latent parameters from measurements. We demonstrate FixFit in two use cases drawn from classical physics and neurosci
    
[^147]: 解释位移：研究模型与转移数据分布的交互作用。

    Explanation Shift: Investigating Interactions between Models and Shifting Data Distributions. (arXiv:2303.08081v1 [cs.LG])

    [http://arxiv.org/abs/2303.08081](http://arxiv.org/abs/2303.08081)

    该论文提出了一种新的方法，通过模型解释特征的转移性质来检测分布转移下学习模型的行为是否越界，在比较中发现其比最先进的技术更为优秀，提供了算法方法并在实验中得到验证。

    

    当输入数据分布发生变化时，机器学习模型的预测性能往往会下降。在实践中，新的输入数据往往没有目标标签。因此，最先进的技术模型输入数据分布或模型预测分布，并试图理解学习模型和转移分布之间的相互作用问题。我们提出了一种新方法，该方法模型如何解释特征的转移性质受到分布转移的影响。我们发现，解释位移的建模可以比最先进的技术更好地指示检测超出分布的模型行为。我们分析了使用合成示例和真实数据集的不同类型的分布转移。我们提供了一种算法方法，允许我们检查数据集特征和学习模型之间的交互作用，并将其与最先进技术进行比较。我们在开源Python包中发布了我们的方法，以及使用的代码。

    As input data distributions evolve, the predictive performance of machine learning models tends to deteriorate. In practice, new input data tend to come without target labels. Then, state-of-the-art techniques model input data distributions or model prediction distributions and try to understand issues regarding the interactions between learned models and shifting distributions. We suggest a novel approach that models how explanation characteristics shift when affected by distribution shifts. We find that the modeling of explanation shifts can be a better indicator for detecting out-of-distribution model behaviour than state-of-the-art techniques. We analyze different types of distribution shifts using synthetic examples and real-world data sets. We provide an algorithmic method that allows us to inspect the interaction between data set features and learned models and compare them to the state-of-the-art. We release our methods in an open-source Python package, as well as the code used
    
[^148]: BoundaryCAM：一种基于边界的弱监督医学图像语义分割优化框架

    BoundaryCAM: A Boundary-based Refinement Framework for Weakly Supervised Semantic Segmentation of Medical Images. (arXiv:2303.07853v1 [cs.CV])

    [http://arxiv.org/abs/2303.07853](http://arxiv.org/abs/2303.07853)

    BoundaryCAM提出了一种基于边界的弱监督的优化框架，能够预测对象位置，实现精细的高精度分割掩模。

    

    仅利用图像级别监督的弱监督语义分割（WSSS）是解决分割网络需求的一种有前途的方法，尤其是对于在给定数据集中生成大量像素级掩模。然而，大多数最先进的图像级WSSS技术缺乏对图像中包含的几何特征的理解，因为网络无法从仅图像级别标签中导出任何对象边界信息。为了解决这个缺陷，我们提出了我们的新型BoundaryCAM框架，该框架采用最先进的类激活图结合各种后处理技术，以实现精细的高精度分割掩模。为了实现这一目标，我们调查了一种最先进的无监督语义分割网络，该网络可用于构建边界图，以使BoundaryCAM能够高精度预测对象位置。

    Weakly Supervised Semantic Segmentation (WSSS) with only image-level supervision is a promising approach to deal with the need for Segmentation networks, especially for generating a large number of pixel-wise masks in a given dataset. However, most state-of-the-art image-level WSSS techniques lack an understanding of the geometric features embedded in the images since the network cannot derive any object boundary information from just image-level labels. We define a boundary here as the line separating an object and its background, or two different objects. To address this drawback, we propose our novel BoundaryCAM framework, which deploys state-of-the-art class activation maps combined with various post-processing techniques in order to achieve fine-grained higher-accuracy segmentation masks. To achieve this, we investigate a state-of-the-art unsupervised semantic segmentation network that can be used to construct a boundary map, which enables BoundaryCAM to predict object locations w
    
[^149]: 理性和神经网络的近似方法的比较

    A comparison of rational and neural network based approximations. (arXiv:2303.04436v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2303.04436](http://arxiv.org/abs/2303.04436)

    本文比较了理性和神经网络的近似方法，并发现理性逼近在决策变量数目相同的情况下优于神经网络。数值实验证明了理性逼近的效率，即使逼近参数的数目较小。此外，本文还改进了理性逼近算法，通过调整优化方法来控制约束矩阵的条件数。

    

    在现代逼近方法中，理性和神经网络的近似方法是高效的工具。这些方法能够对非光滑和非Lipschitz函数，包括多变量域函数，进行精确逼近。本文比较了使用理性逼近、神经网络和它们的组合进行函数逼近的效率。研究结果发现，在相同决策变量数目下，理性逼近优于基于神经网络的方法。我们的数值实验证明了理性逼近的效率，即使逼近参数的数目（即相应优化问题的维数）较小也是如此。本文的另一个重要贡献在于理性逼近算法的改进。也就是说，基于优化的理性逼近算法可以进行调整，以控制约束矩阵的条件数。

    Rational and neural network based approximations are efficient tools in modern approximation. These approaches are able to produce accurate approximations to nonsmooth and non-Lipschitz functions, including multivariate domain functions. In this paper we compare the efficiency of function approximation using rational approximation, neural network and their combinations. It was found that rational approximation is superior to neural network based approaches with the same number of decision variables. Our numerical experiments demonstrate the efficiency of rational approximation, even when the number of approximation parameters (that is, the dimension of the corresponding optimisation problems) is small. Another important contribution of this paper lies in the improvement of rational approximation algorithms. Namely, the optimisation based algorithms for rational approximation can be adjusted to in such a way that the conditioning number of the constraint matrices are controlled. This si
    
[^150]: 面向大规模机器学习模型的可证明高效量子算法

    Towards provably efficient quantum algorithms for large-scale machine-learning models. (arXiv:2303.03428v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2303.03428](http://arxiv.org/abs/2303.03428)

    本论文提出了一种可能的针对通用（随机）梯度下降算法的高效量子解决方案，只要模型足够耗散和稀疏，具有小的学习率，并且可以缩放至 $O(T^2 \times \text{polylog}(n))$。在实践中，证明了在稀疏训练的情况下，量子计算可以显著提高效率。

    

    大型机器学习模型是人工智能的革命性技术，其瓶颈包括巨大的计算开销、功耗和时间，既用于预训练，也用于微调过程。本研究表明，容错量子计算可能会针对通用（随机）梯度下降算法提供可证明的高效解决方案，其缩放为 $\mathcal{O}(T^2 \times \text{polylog}(n))$，其中 $n$ 是模型的大小，$T$ 是训练中的迭代次数，只要模型足够耗散和稀疏，并具有较小的学习率。基于早期用于耗散微分方程的高效量子算法，我们发现并证明了类似的算法可用于（随机）梯度下降，这是机器学习的主要算法。在实践中，我们对拥有从700万到1.03亿个参数的大型机器学习模型进行了基准测试。我们发现，在稀疏训练的情况下，量子计算显然可以在一定程度上提高效率。

    Large machine learning models are revolutionary technologies of artificial intelligence whose bottlenecks include huge computational expenses, power, and time used both in the pre-training and fine-tuning process. In this work, we show that fault-tolerant quantum computing could possibly provide provably efficient resolutions for generic (stochastic) gradient descent algorithms, scaling as $\mathcal{O}(T^2 \times \text{polylog}(n))$, where $n$ is the size of the models and $T$ is the number of iterations in the training, as long as the models are both sufficiently dissipative and sparse, with small learning rates. Based on earlier efficient quantum algorithms for dissipative differential equations, we find and prove that similar algorithms work for (stochastic) gradient descent, the primary algorithm for machine learning. In practice, we benchmark instances of large machine learning models from 7 million to 103 million parameters. We find that, in the context of sparse training, a quan
    
[^151]: Internet Explorer:开放网络上的有针对性表示学习

    Internet Explorer: Targeted Representation Learning on the Open Web. (arXiv:2302.14051v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14051](http://arxiv.org/abs/2302.14051)

    Internet Explorer是一种能够利用互联网进行有针对性表示学习的方法，通过自我监督的方式在网络上搜索相关图像并训练小规模模型，从而提高在特定任务上的性能。

    

    现代视觉模型通常依赖于在大型静态数据集上预训练的通用模型进行微调。这些通用模型只能捕捉到它们的预训练数据集中的知识，而这些数据集只是互联网的微小、过时的快照——而互联网上每天都上传数十亿张图片。我们提出了一种替代方法：不是希望我们的静态数据集在大规模预训练后能够转移到我们期望的任务上，而是我们建议动态利用互联网，快速训练一个在当前任务上表现非常好的小规模模型。我们的方法叫做Internet Explorer，以自我监督的方式在网络上探索，逐步找到改善所需目标数据集性能的相关示例。它循环在互联网上搜索带有文本查询的图像，通过下载的图像进行自我监督训练，确定哪些图像是有用的，并确定下一步要搜索的优先级。我们在几个数据集上评估了Internet Explorer。

    Modern vision models typically rely on fine-tuning general-purpose models pre-trained on large, static datasets. These general-purpose models only capture the knowledge within their pre-training datasets, which are tiny, out-of-date snapshots of the Internet -- where billions of images are uploaded each day. We suggest an alternate approach: rather than hoping our static datasets transfer to our desired tasks after large-scale pre-training, we propose dynamically utilizing the Internet to quickly train a small-scale model that does extremely well on the task at hand. Our approach, called Internet Explorer, explores the web in a self-supervised manner to progressively find relevant examples that improve performance on a desired target dataset. It cycles between searching for images on the Internet with text queries, self-supervised training on downloaded images, determining which images were useful, and prioritizing what to search for next. We evaluate Internet Explorer across several d
    
[^152]: 重新考虑医学图像迁移学习中的隐藏表示

    Revisiting Hidden Representations in Transfer Learning for Medical Imaging. (arXiv:2302.08272v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.08272](http://arxiv.org/abs/2302.08272)

    本研究重新考虑了医学图像迁移学习中的隐藏表示，通过比较在ImageNet和RadImageNet上的初始化，发现ImageNet上预训练的模型在多个医学分类任务上表现优于RadImageNet上的模型。

    

    虽然深度学习成功的关键在于具有大量的训练数据，但医学图像数据集通常在多样性和规模上受到限制。迁移学习有可能弥合相关但不同领域之间的差距。然而，对于医学应用而言，预训练自然图像还是医学图像更有益仍不清楚。我们旨在通过对比在ImageNet和RadImageNet上的初始化，在七个医学分类任务上进行研究来解决这个问题。我们的工作包括复制性研究，其结果与先前发表的研究相反。在我们的实验中，ImageNet上预训练的ResNet50模型往往优于在RadImageNet上训练的模型。为了进一步了解，我们使用典型相关分析（CCA）研究了学习得到的表示，并比较了不同模型的预测能力。我们的结果表明，与直觉相反，ImageNet和RadImageNet可能会收敛到不同的表示。

    While a key component to the success of deep learning is the availability of massive amounts of training data, medical image datasets are often limited in diversity and size. Transfer learning has the potential to bridge the gap between related yet different domains. For medical applications, however, it remains unclear whether it is more beneficial to pre-train on natural or medical images. We aim to shed light on this problem by comparing initialization on ImageNet and RadImageNet on seven medical classification tasks. Our work includes a replication study, which yields results contrary to previously published findings. In our experiments, ResNet50 models pre-trained on ImageNet tend to outperform those trained on RadImageNet. To gain further insights, we investigate the learned representations using Canonical Correlation Analysis (CCA) and compare the predictions of the different models. Our results indicate that, contrary to intuition, ImageNet and RadImageNet may converge to disti
    
[^153]: LB-SimTSC: 一种用于半监督时间序列分类的高效相似性感知图神经网络

    LB-SimTSC: An Efficient Similarity-Aware Graph Neural Network for Semi-Supervised Time Series Classification. (arXiv:2301.04838v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.04838](http://arxiv.org/abs/2301.04838)

    LB-SimTSC是一种高效的相似性感知图神经网络，用于半监督时间序列分类。它通过使用LB_Keogh作为DTW的下界来解决DTW二次复杂性限制，在少标签设置中表现出优秀的准确度。

    

    时间序列分类是一项重要的数据挖掘任务，在过去的20年里引起了很多关注。由于实际中标签稀缺，只有少量标记样本的半监督时间序列分类变得流行。最近，提出了相似性感知时间序列分类（SimTSC）来解决这个问题，该方法使用基于批量数据的两两动态时间规整（DTW）距离生成的图神经网络分类模型。SimTSC表现出优秀的准确度，并在几个少标签设置中胜过了最先进的深度学习模型。然而，由于SimTSC依赖于两两DTW距离，DTW的二次复杂性限制了它只能在适度大小的数据集上使用。为了解决这个挑战，我们提出了一种新的高效半监督时间序列分类技术LB-SimTSC，其中包括一个新的图构建模块。我们提出使用DTW的下界LB_Keogh来近似DTW。

    Time series classification is an important data mining task that has received a lot of interest in the past two decades. Due to the label scarcity in practice, semi-supervised time series classification with only a few labeled samples has become popular. Recently, Similarity-aware Time Series Classification (SimTSC) is proposed to address this problem by using a graph neural network classification model on the graph generated from pairwise Dynamic Time Warping (DTW) distance of batch data. It shows excellent accuracy and outperforms state-of-the-art deep learning models in several few-label settings. However, since SimTSC relies on pairwise DTW distances, the quadratic complexity of DTW limits its usability to only reasonably sized datasets. To address this challenge, we propose a new efficient semi-supervised time series classification technique, LB-SimTSC, with a new graph construction module. Instead of using DTW, we propose to utilize a lower bound of DTW, LB_Keogh, to approximate 
    
[^154]: 如何使用信息论选择客观函数

    How to select an objective function using information theory. (arXiv:2212.06566v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.06566](http://arxiv.org/abs/2212.06566)

    信息论告诉我们，为了最大化模型的信息量，选择可能性最高或表示误差比特最少的客观函数。将不同的客观函数转换为似然函数，它们的相对大小表示我们应该更喜欢哪个客观函数，而其大小的对数表示模型的预期不确定性。

    

    在机器学习或科学计算中，模型性能是通过客观函数衡量的。但是为什么要选择某个客观函数而不是另一个？信息论给出了一个答案：为了最大化模型中的信息量，选择最有可能的客观函数或者代表误差的比特最少的函数。要评估不同的客观函数，将它们转换为似然函数。作为似然函数，它们的相对大小表示我们应该更喜欢哪个客观函数，而其大小的对数表示模型的预期不确定性。

    In machine learning or scientific computing, model performance is measured with an objective function. But why choose one objective over another? Information theory gives one answer: To maximize the information in the model, select the most likely objective function or whichever represents the error in the fewest bits. To evaluate different objectives, transform them into likelihood functions. As likelihoods, their relative magnitudes represent how much we should prefer one objective versus another, and the log of their magnitude represents the expected uncertainty of the model.
    
[^155]: 关于通过因果推理进行根本原因定位和异常缓解的论文

    On Root Cause Localization and Anomaly Mitigation through Causal Inference. (arXiv:2212.04031v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.04031](http://arxiv.org/abs/2212.04031)

    该论文提出了从因果的角度实现根本原因定位和异常缓解的方法，并在三个数据集上进行了实验验证。

    

    由于在现实世界中具有广泛的应用，例如安全、金融监测和健康风险，已经提出了各种深度异常检测模型，并取得了最先进的性能。然而，除了有效之外，在实践中，从业者还希望进一步了解是什么导致了异常结果以及如何进一步修复它。在这项工作中，我们提出了RootCLAM，旨在从因果的角度实现根本原因定位和异常缓解。特别是，我们将由外部干预引起的异常定义为对正常因果机制的外部干预，并旨在定位具有外部干预的异常特征作为根本原因。此后，我们进一步提出了一种异常缓解方法，旨在推荐针对异常特征的缓解措施，以恢复正常结果，使由因果机制指导的反事实变得正常。在三个数据集上的实验证明，我们的方法可以定位根本原因和缓解异常。

    Due to a wide spectrum of applications in the real world, such as security, financial surveillance, and health risk, various deep anomaly detection models have been proposed and achieved state-of-the-art performance. However, besides being effective, in practice, the practitioners would further like to know what causes the abnormal outcome and how to further fix it. In this work, we propose RootCLAM, which aims to achieve Root Cause Localization and Anomaly Mitigation from a causal perspective. Especially, we formulate anomalies caused by external interventions on the normal causal mechanism and aim to locate the abnormal features with external interventions as root causes. After that, we further propose an anomaly mitigation approach that aims to recommend mitigation actions on abnormal features to revert the abnormal outcomes such that the counterfactuals guided by the causal mechanism are normal. Experiments on three datasets show that our approach can locate the root causes and fur
    
[^156]: PGFed：针对联邦学习个性化每个客户的全局目标

    PGFed: Personalize Each Client's Global Objective for Federated Learning. (arXiv:2212.01448v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.01448](http://arxiv.org/abs/2212.01448)

    PGFed是一个新颖的个性化联邦学习框架，通过显式和自适应地聚合每个客户的经验风险，使每个客户可以个性化其自己的全局目标。

    

    由于传统的联邦学习在异构数据上的表现平庸，个性化联邦学习受到了广泛关注。与传统的联邦学习训练单个全局共识模型不同，个性化联邦学习允许为不同的客户建立不同的模型。然而，现有的个性化联邦学习算法只是通过将知识嵌入到聚合模型或正则化中隐式地传递协作知识。我们观察到这种隐式的知识传递没有充分发挥每个客户的经验风险对其他客户的潜力。基于我们的观察，在这项工作中，我们提出了Personalized Global Federated Learning (PGFed)，这是一个新颖的个性化联邦学习框架，通过显式和自适应地聚合自身和其他客户的经验风险，使每个客户可以个性化其自己的全局目标。

    Personalized federated learning has received an upsurge of attention due to the mediocre performance of conventional federated learning (FL) over heterogeneous data. Unlike conventional FL which trains a single global consensus model, personalized FL allows different models for different clients. However, existing personalized FL algorithms only implicitly transfer the collaborative knowledge across the federation by embedding the knowledge into the aggregated model or regularization. We observed that this implicit knowledge transfer fails to maximize the potential of each client's empirical risk toward other clients. Based on our observation, in this work, we propose Personalized Global Federated Learning (PGFed), a novel personalized FL framework that enables each client to personalize its own global objective by explicitly and adaptively aggregating the empirical risks of itself and other clients. To avoid massive (O(N^2)) communication overhead and potential privacy leakage while a
    
[^157]: 深度学习在非对比度CT上对急性缺血性中风分割的非劣性与专家神经放射医学家比较

    Non-inferiority of Deep Learning Acute Ischemic Stroke Segmentation on Non-Contrast CT Compared to Expert Neuroradiologists. (arXiv:2211.15341v3 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2211.15341](http://arxiv.org/abs/2211.15341)

    本研究旨在确定卷积神经网络（CNN）深度学习模型在非对比度CT上对急性缺血性改变的分割是否能够达到与神经放射医学家相媲美的水平。

    

    本研究旨在确定卷积神经网络（CNN）深度学习模型在非对比度CT上是否能够准确分割急性缺血性改变，与神经放射医学家相比。该研究纳入了232例参与DEFUSE 3试验的急性缺血性中风患者的非对比度CT检查。三名经验丰富的神经放射医学家独立地对每个扫描图像上反映缺血灶的低密度进行分割。经验最丰富的神经放射医学家（专家A）的分割结果作为深度学习模型训练的基准。另外两名神经放射医学家（专家B和C）的分割结果用于数据测试。232个研究被随机分成训练集和测试集。训练集进一步随机分成5折，包括训练集和验证集。使用三维CNN架构训练和优化模型，以预测对非对比度CT上专家A的分割。使用一组容积、重叠和距离指标评估模型的性能。

    To determine if a convolutional neural network (CNN) deep learning model can accurately segment acute ischemic changes on non-contrast CT compared to neuroradiologists. Non-contrast CT (NCCT) examinations from 232 acute ischemic stroke patients who were enrolled in the DEFUSE 3 trial were included in this study. Three experienced neuroradiologists independently segmented hypodensity that reflected the ischemic core on each scan. The neuroradiologist with the most experience (expert A) served as the ground truth for deep learning model training. Two additional neuroradiologists (experts B and C) segmentations were used for data testing. The 232 studies were randomly split into training and test sets. The training set was further randomly divided into 5 folds with training and validation sets. A 3-dimensional CNN architecture was trained and optimized to predict the segmentations of expert A from NCCT. The performance of the model was assessed using a set of volume, overlap, and distance
    
[^158]: USE-Evaluator: 带有不确定、小型或空白参考注释的医学图像分割模型的性能度量

    USE-Evaluator: Performance Metrics for Medical Image Segmentation Models with Uncertain, Small or Empty Reference Annotations. (arXiv:2209.13008v4 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2209.13008](http://arxiv.org/abs/2209.13008)

    USE-Evaluator是一种用于医学图像分割模型的性能度量方法，可以解决由于参考注释不确定、小型或空白导致的常规指标无法适用于临床数据集的问题。

    

    用于医学图像分割模型的性能度量主要用于衡量参考注释与预测分割之间的一致性。通常使用重叠度量，例如Dice系数，作为评估模型性能的指标，以便结果具有可比性。然而，公共数据集中的病例分布和分割任务难度与临床实践存在不匹配之处。常规指标未能衡量到这种不匹配的影响，特别是针对包含低信号病理、难度较大的分割任务以及不确定、小型或空白参考注释的临床数据集。这一限制可能导致机器学习从业者在设计和优化模型时效果不佳。评估临床价值的维度包括考虑参考注释的不确定性、与参考注释体积大小的独立性以及对空白分类的评估。

    Performance metrics for medical image segmentation models are used to measure the agreement between the reference annotation and the predicted segmentation. Usually, overlap metrics, such as the Dice, are used as a metric to evaluate the performance of these models in order for results to be comparable. However, there is a mismatch between the distributions of cases and difficulty level of segmentation tasks in public data sets compared to clinical practice. Common metrics fail to measure the impact of this mismatch, especially for clinical data sets that include low signal pathologies, a difficult segmentation task, and uncertain, small, or empty reference annotations. This limitation may result in ineffective research of machine learning practitioners in designing and optimizing models. Dimensions of evaluating clinical value include consideration of the uncertainty of reference annotations, independence from reference annotation volume size, and evaluation of classification of empty
    
[^159]: 基于半定规划的带基数约束最小平方和聚类的全局优化

    Global Optimization for Cardinality-constrained Minimum Sum-of-Squares Clustering via Semidefinite Programming. (arXiv:2209.08901v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2209.08901](http://arxiv.org/abs/2209.08901)

    本文提出了一种基于半定规划的全局优化方法来解决带基数约束的最小平方和聚类问题，并通过引入新的SDP松弛方法和定制的分支策略来改善解的质量和性能。

    

    最小平方和聚类(MSSC)或k均值类型聚类最近已经扩展以利用对每个簇基数的先验知识。这种知识被用于提高性能和解决方案质量。在本文中，我们提出了一种基于分支定界技术的全局优化方法来解决基数约束的MSSC问题。对于下界例程，我们使用了Rujeerapaiboon等人最近提出的半定规划(SDP)松弛方法。然而，该松弛方法只适用于小规模实例的分支定界方法。因此，我们导出了一种新的SDP松弛方法，它能更好地适应实例规模和簇的数目。在两种情况下，我们通过添加多面体割线来加强下界。通过定制的分支策略，我们实施了成对约束，从而减少了子节点中出现的问题的复杂性。对于上界，

    The minimum sum-of-squares clustering (MSSC), or k-means type clustering, has been recently extended to exploit prior knowledge on the cardinality of each cluster. Such knowledge is used to increase performance as well as solution quality. In this paper, we propose a global optimization approach based on the branch-and-cut technique to solve the cardinality-constrained MSSC. For the lower bound routine, we use the semidefinite programming (SDP) relaxation recently proposed by Rujeerapaiboon et al. [SIAM J. Optim. 29(2), 1211-1239, (2019)]. However, this relaxation can be used in a branch-and-cut method only for small-size instances. Therefore, we derive a new SDP relaxation that scales better with the instance size and the number of clusters. In both cases, we strengthen the bound by adding polyhedral cuts. Benefiting from a tailored branching strategy which enforces pairwise constraints, we reduce the complexity of the problems arising in the children nodes. For the upper bound, inste
    
[^160]: 对抗性策略的空间

    The Space of Adversarial Strategies. (arXiv:2209.04521v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2209.04521](http://arxiv.org/abs/2209.04521)

    本文提出了一种系统的方法来研究对抗性策略的空间。通过分解攻击组件，并引入新的攻击方法，我们可以更好地理解最坏情况下的对抗行为，并且衡量攻击性能。

    

    过去十年中，对抗性样本，即旨在在机器学习模型中诱发最糟糕行为的输入，已经得到广泛研究。然而，我们对这一现象的理解来自于一片相对零散的知识池；目前，存在着一些攻击方法，每种方法在威胁模型上都有不同的假设，并且定义的最优性不可比较。在本文中，我们提出了一种系统的方法来刻画最坏情况（即最优）的对抗。我们首先引入了对抗性机器学习中攻击的可拓展分解，将攻击组件分解为表面和旅行者。借助于我们的分解，我们枚举了各种组件以创建了576种攻击（其中568种以前尚未探索）。接下来，我们提出了帕累托组合攻击（PEA）：一种理论上限制攻击性能的攻击方法。通过我们新的攻击方法，我们在鲁棒和非鲁棒模型、七个数据集以及三个扩展lp上相对于PEA衡量了性能。

    Adversarial examples, inputs designed to induce worst-case behavior in machine learning models, have been extensively studied over the past decade. Yet, our understanding of this phenomenon stems from a rather fragmented pool of knowledge; at present, there are a handful of attacks, each with disparate assumptions in threat models and incomparable definitions of optimality. In this paper, we propose a systematic approach to characterize worst-case (i.e., optimal) adversaries. We first introduce an extensible decomposition of attacks in adversarial machine learning by atomizing attack components into surfaces and travelers. With our decomposition, we enumerate over components to create 576 attacks (568 of which were previously unexplored). Next, we propose the Pareto Ensemble Attack (PEA): a theoretical attack that upper-bounds attack performance. With our new attacks, we measure performance relative to the PEA on: both robust and non-robust models, seven datasets, and three extended lp
    
[^161]: 通过NeurVec加速大规模动态系统数值求解器的模拟

    Accelerating Numerical Solvers for Large-Scale Simulation of Dynamical System via NeurVec. (arXiv:2208.03680v2 [cs.CE] UPDATED)

    [http://arxiv.org/abs/2208.03680](http://arxiv.org/abs/2208.03680)

    通过NeurVec，我们提出了一种基于深度学习的修正器，它能够补偿集成误差并在模拟中实现更大的时间步长。NeurVec在复杂动态系统中表现出了显著的泛化能力，加速了传统求解器，同时保持了高水平的准确性和稳定性。

    

    在众多科学和工程领域中，大规模动态系统的模拟至关重要。然而，传统的数值求解器在估计积分时由于步长选择的限制，存在着精度和计算效率之间的权衡。为了解决这个挑战，我们引入了一种基于深度学习的修正器，称为NeurVec，它可以补偿集成误差并在模拟中实现更大的时间步长。我们在各种复杂动态系统基准上进行了大量实验证明，即使在使用有限和离散数据进行训练时，NeurVec在连续相空间上表现出了显著的泛化能力。NeurVec显著加速了传统求解器，实现了几十到几百倍的速度提升，同时保持了高水平的准确性和稳定性。此外，NeurVec的简单而有效的设计结合了易于实现的特点，有潜力建立起一个新的求解器范式。

    The large-scale simulation of dynamical systems is critical in numerous scientific and engineering disciplines. However, traditional numerical solvers are limited by the choice of step sizes when estimating integration, resulting in a trade-off between accuracy and computational efficiency. To address this challenge, we introduce a deep learning-based corrector called Neural Vector (NeurVec), which can compensate for integration errors and enable larger time step sizes in simulations. Our extensive experiments on a variety of complex dynamical system benchmarks demonstrate that NeurVec exhibits remarkable generalization capability on a continuous phase space, even when trained using limited and discrete data. NeurVec significantly accelerates traditional solvers, achieving speeds tens to hundreds of times faster while maintaining high levels of accuracy and stability. Moreover, NeurVec's simple-yet-effective design, combined with its ease of implementation, has the potential to establi
    
[^162]: 人类学习奖励函数偏好的模型

    Models of human preference for learning reward functions. (arXiv:2206.02231v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.02231](http://arxiv.org/abs/2206.02231)

    本研究提出了一种将人类偏好建模为每个轨迹段的遗憾的方法，并证明了可以根据这些遗憾生成的偏好来识别生成这些偏好的奖励函数。实验证明，这种遗憾偏好模型在性能上优于以前的模型。

    

    强化学习的效用受限于奖励函数与人类利益的一致性。一种有前途的对齐方法是从人类生成的轨迹段对之间学习奖励函数，这是一种从人类反馈中进行的强化学习方法。通常假设这些人类偏好仅由部分回报来决定，即每个轨迹段上的奖励总和。我们发现这种假设存在缺陷，提出将人类偏好建模为由每个轨迹段的遗憾来决定，遗憾是一种衡量轨迹段与最优决策之间偏离程度的度量。在根据遗憾生成的无穷多个偏好中，我们证明可以识别到与生成这些偏好的奖励函数等价的奖励函数，并且我们证明以前的部分回报模型在多种情境下缺乏这种可识别性属性。通过实验证明，我们提出的遗憾偏好模型在性能上优于以前的模型。

    The utility of reinforcement learning is limited by the alignment of reward functions with the interests of human stakeholders. One promising method for alignment is to learn the reward function from human-generated preferences between pairs of trajectory segments, a type of reinforcement learning from human feedback (RLHF). These human preferences are typically assumed to be informed solely by partial return, the sum of rewards along each segment. We find this assumption to be flawed and propose modeling human preferences instead as informed by each segment's regret, a measure of a segment's deviation from optimal decision-making. Given infinitely many preferences generated according to regret, we prove that we can identify a reward function equivalent to the reward function that generated those preferences, and we prove that the previous partial return model lacks this identifiability property in multiple contexts. We empirically show that our proposed regret preference model outperf
    
[^163]: 通过驯服符号问题，对神经网络层进行高效的反对称化

    Efficient anti-symmetrization of a neural network layer by taming the sign problem. (arXiv:2205.12250v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.12250](http://arxiv.org/abs/2205.12250)

    通过驯服符号问题，我们展示了如何高效地实现神经网络层的反对称化，为在反对称神经网络中使用通用的反对称层作为构建模块打开了大门，并且该近似方法在符号问题受控时非常有效。

    

    神经网络的显式反对称化是一种用于通用反对称函数的通用函数逼近方法，而这种函数在量子物理中无处不在。然而，这个过程的实施成本极高，对于大量粒子来说是不可行的。该策略还存在符号问题。即由于正负贡献的近乎完全抵消，反对称化函数的幅值可能明显小于反对称化之前的幅值。我们展示了如何高效地评估两层神经网络的反对称投影，从而为在反对称神经网络中使用通用的反对称层作为构建模块打开了大门。这种近似在符号问题受控时是有效的，我们展示了这种性质在使用标准Xavier/He初始化方法时，关键取决于激活函数的选择。

    Explicit antisymmetrization of a neural network is a potential candidate for a universal function approximator for generic antisymmetric functions, which are ubiquitous in quantum physics. However, this procedure is a priori factorially costly to implement, making it impractical for large numbers of particles. The strategy also suffers from a sign problem. Namely, due to near-exact cancellation of positive and negative contributions, the magnitude of the antisymmetrized function may be significantly smaller than before anti-symmetrization. We show that the anti-symmetric projection of a two-layer neural network can be evaluated efficiently, opening the door to using a generic antisymmetric layer as a building block in anti-symmetric neural network Ansatzes. This approximation is effective when the sign problem is controlled, and we show that this property depends crucially the choice of activation function under standard Xavier/He initialization methods. As a consequence, using a smoot
    
[^164]: Auto-SDE:从数据驱动的随机动力学系统中学习有效的降维动力学

    Auto-SDE: Learning effective reduced dynamics from data-driven stochastic dynamical systems. (arXiv:2205.04151v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2205.04151](http://arxiv.org/abs/2205.04151)

    本文提出了一种名为Auto-SDE的算法来学习慢-快随机动力学系统的有效降维动力学，通过自动编码器神经网络和离散化的随机微分方程，捕捉了系统的演化特性，并在数值实验证明了其准确性、稳定性和有效性。

    

    由于能够描绘许多实际应用中的复杂现象，多尺度随机动力学系统被广泛应用于科学和工程问题。本文致力于研究慢-快随机动力学系统的有效降维动力学。给定满足某些未知慢-快随机系统的短期观测数据，我们提出了一种包括自动编码器神经网络Auto-SDE的新算法来学习不变的慢流形。我们的方法捕捉了一系列时间相关的自动编码器神经网络的演化特性，损失函数通过离散化的随机微分方程构造。通过在各种评估指标下的数值实验证明，我们的算法具有准确性、稳定性和有效性。

    Multiscale stochastic dynamical systems have been widely adopted to scientific and engineering problems due to their capability of depicting complex phenomena in many real world applications. This work is devoted to investigating the effective reduced dynamics for a slow-fast stochastic dynamical system. Given observation data on a short-term period satisfying some unknown slow-fast stochastic system, we propose a novel algorithm including a neural network called Auto-SDE to learn invariant slow manifold. Our approach captures the evolutionary nature of a series of time-dependent autoencoder neural networks with the loss constructed from a discretized stochastic differential equation. Our algorithm is also proved to be accurate, stable and effective through numerical experiments under various evaluation metrics.
    
[^165]: DeepAD:一种用于实际临床应用的阿尔茨海默病进展鲁棒深度学习模型

    DeepAD: A Robust Deep Learning Model of Alzheimer's Disease Progression for Real-World Clinical Applications. (arXiv:2203.09096v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.09096](http://arxiv.org/abs/2203.09096)

    阿尔茨海默病进展预测中，我们提出了一种新颖的多模态多任务深度学习模型，整合了多个队列中的临床和神经影像数据，通过分析高维MRI特征和其他数据模态来预测患者未来的病情轨迹。

    

    预测患者未来的病情轨迹对于阿尔茨海默病等复杂疾病的药物研发至关重要。然而，大多数用于预测疾病进展的机器学习方法要么是单任务要么是单模型，不能直接应用于包含多任务学习与高维影像的我们的场景。此外，这些方法大多训练于单个数据集（即队列），很难推广到其他队列。我们提出了一种新颖的多模态多任务深度学习模型，通过分析多个队列中的纵向临床和神经影像数据，预测阿尔茨海默病的进展。我们的模型将三维卷积神经网络的高维MRI特征与临床和人口统计学信息等其他数据模态整合，来预测患者未来的病情轨迹。我们的模型引入了对抗损失来缓解队列特异性问题。

    The ability to predict the future trajectory of a patient is a key step toward the development of therapeutics for complex diseases such as Alzheimer's disease (AD). However, most machine learning approaches developed for prediction of disease progression are either single-task or single-modality models, which can not be directly adopted to our setting involving multi-task learning with high dimensional images. Moreover, most of those approaches are trained on a single dataset (i.e. cohort), which can not be generalized to other cohorts. We propose a novel multimodal multi-task deep learning model to predict AD progression by analyzing longitudinal clinical and neuroimaging data from multiple cohorts. Our proposed model integrates high dimensional MRI features from a 3D convolutional neural network with other data modalities, including clinical and demographic information, to predict the future trajectory of patients. Our model employs an adversarial loss to alleviate the study-specifi
    
[^166]: 截断扩散概率模型和基于扩散的对抗自编码器

    Truncated Diffusion Probabilistic Models and Diffusion-based Adversarial Auto-Encoders. (arXiv:2202.09671v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2202.09671](http://arxiv.org/abs/2202.09671)

    我们提出了一种更快更廉价的截断扩散概率模型方法，通过从隐藏噪声数据分布开始生成数据，相较于传统的方法可以获得更好的性能改进。

    

    扩散型生成模型通过使用正向扩散链逐步将数据映射到噪声分布，并通过推断反向扩散链来学习如何生成数据。然而，这种方法速度慢且成本高，因为需要许多正向和反向步骤。我们提出了一种更快更廉价的方法，不是将噪声添加到数据变为纯随机噪声，而是直到达到一个可以自信学习的隐藏噪声数据分布。然后，我们使用较少的反向步骤通过从这个隐藏分布开始生成类似于噪声数据的数据。我们揭示了该模型可以被视为一个通过扩散过程和可学习的隐含先验增强的对抗性自编码器。实验结果表明，即使在较少的反向扩散步骤下，所提出的截断扩散概率模型在性能方面仍然可以相较于非截断模型提供一致的改进。

    Employing a forward diffusion chain to gradually map the data to a noise distribution, diffusion-based generative models learn how to generate the data by inferring a reverse diffusion chain. However, this approach is slow and costly because it needs many forward and reverse steps. We propose a faster and cheaper approach that adds noise not until the data become pure random noise, but until they reach a hidden noisy data distribution that we can confidently learn. Then, we use fewer reverse steps to generate data by starting from this hidden distribution that is made similar to the noisy data. We reveal that the proposed model can be cast as an adversarial auto-encoder empowered by both the diffusion process and a learnable implicit prior. Experimental results show even with a significantly smaller number of reverse diffusion steps, the proposed truncated diffusion probabilistic models can provide consistent improvements over the non-truncated ones in terms of performance in both unco
    
[^167]: 利用数据独立函数的限制进行隐式正则化的特征化

    Limitation of Characterizing Implicit Regularization by Data-independent Functions. (arXiv:2201.12198v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.12198](http://arxiv.org/abs/2201.12198)

    本研究旨在数学定义和研究隐式正则化，发现使用数据独立函数进行特征化的方法存在局限性，并提出了两个动力学机制以及相应的生成方法，进一步强调了隐式正则化的数据依赖性。

    

    近年来，理解神经网络（NNs）的隐式正则化已成为深度学习理论中的一项核心任务。然而，隐式正则化本身并没有完全定义和深入理解。在这项工作中，我们试图对隐式正则化进行数学定义和研究。重要的是，我们探索了使用数据独立函数进行隐式正则化特征化的常见方法的局限性。我们提出了两个动力学机制，即双点和单点重叠机制，并基于这些机制提供了两种能够证明不能完全由一种或所有数据独立函数特征化的单隐藏神经元NNs类的生成方法。与先前的工作类似，我们的结果进一步强调了隐式正则化的数据依赖性，激励我们未来详细研究NN隐式正则化的数据依赖性。

    In recent years, understanding the implicit regularization of neural networks (NNs) has become a central task in deep learning theory. However, implicit regularization is itself not completely defined and well understood. In this work, we attempt to mathematically define and study implicit regularization. Importantly, we explore the limitations of a common approach to characterizing implicit regularization using data-independent functions. We propose two dynamical mechanisms, i.e., Two-point and One-point Overlapping mechanisms, based on which we provide two recipes for producing classes of one-hidden-neuron NNs that provably cannot be fully characterized by a type of or all data-independent functions. Following the previous works, our results further emphasize the profound data dependency of implicit regularization in general, inspiring us to study in detail the data dependency of NN implicit regularization in the future.
    
[^168]: Kernelized Concept Erasure. (arXiv:2201.12191v4 [cs.LG] UPDATED)

    Kernelized Concept Erasure. (arXiv:2201.12191v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.12191](http://arxiv.org/abs/2201.12191)

    通过核化线性极小极大博弈，防止特定非线性对手预测概念，但无法彻底解决非线性编码的概念擦除问题。

    

    文本数据的神经模型的表示空间在训练过程中以无监督的方式出现。理解这些表示如何编码可解释的人类概念是一个基本问题。识别神经表示中的概念的一种明显方法是搜索一个线性子空间，其擦除会阻止从表示中预测概念。然而，尽管许多线性擦除算法是可处理和可解释的，但神经网络未必以线性方式表示概念。为了识别非线性编码的概念，我们提出了一个核化的概念擦除线性极小极大博弈。我们证明了可以防止特定的非线性对手预测概念。然而，这种保护不会转移到不同的非线性对手。因此，彻底地擦除非线性编码的概念仍然是一个待解决的问题。

    The representation space of neural models for textual data emerges in an unsupervised manner during training. Understanding how those representations encode human-interpretable concepts is a fundamental problem. One prominent approach for the identification of concepts in neural representations is searching for a linear subspace whose erasure prevents the prediction of the concept from the representations. However, while many linear erasure algorithms are tractable and interpretable, neural networks do not necessarily represent concepts in a linear manner. To identify non-linearly encoded concepts, we propose a kernelization of a linear minimax game for concept erasure. We demonstrate that it is possible to prevent specific non-linear adversaries from predicting the concept. However, the protection does not transfer to different nonlinear adversaries. Therefore, exhaustively erasing a non-linearly encoded concept remains an open problem.
    
[^169]: Q-Learning用于具有一般状态空间的MDPs: 通过弱连续性下的量化来实现收敛和近似最优性

    Q-Learning for MDPs with General Spaces: Convergence and Near Optimality via Quantization under Weak Continuity. (arXiv:2111.06781v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.06781](http://arxiv.org/abs/2111.06781)

    本文研究了在具有连续状态和动作空间的MDPs中使用Q-Learning的收敛和近似最优性问题。通过量化状态和动作，我们证明Quantized Q-Learning会收敛到一个极限，并且这个极限满足一个最优性方程，从而实现近似最优性。

    

    强化学习算法通常要求马尔可夫决策过程(MDPs)中的状态和动作空间是有限的（也称为可控马尔可夫链），文献中已经做出了各种努力，以便将这些算法应用于连续的状态和动作空间。在本文中，我们证明在非常温和的正则性条件下（特别是只涉及MDP的过渡核的弱连续性），通过对状态和动作进行量化的标准 Borel MDPs 的 Q-Learning（称为Quantized Q-Learning）会收敛到一个极限，并且这个极限满足一个最优性方程，从而实现近似最优性，要么具有显式的性能界限，要么具有渐近最优保证。我们的方法基于：(i) 将量化视为一个测量核，并将量化的MDP视为一个部分观测的马尔可夫决策过程（POMDP），(ii) 利用 Q-Learning 对 POMDP 的近似最优性和收敛性结果。

    Reinforcement learning algorithms often require finiteness of state and action spaces in Markov decision processes (MDPs) (also called controlled Markov chains) and various efforts have been made in the literature towards the applicability of such algorithms for continuous state and action spaces. In this paper, we show that under very mild regularity conditions (in particular, involving only weak continuity of the transition kernel of an MDP), Q-learning for standard Borel MDPs via quantization of states and actions (called Quantized Q-Learning) converges to a limit, and furthermore this limit satisfies an optimality equation which leads to near optimality with either explicit performance bounds or which are guaranteed to be asymptotically optimal. Our approach builds on (i) viewing quantization as a measurement kernel and thus a quantized MDP as a partially observed Markov decision process (POMDP), (ii) utilizing near optimality and convergence results of Q-learning for POMDPs, and (
    
[^170]: 比较序贯预测器

    Comparing Sequential Forecasters. (arXiv:2110.00115v5 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2110.00115](http://arxiv.org/abs/2110.00115)

    本文提出了一种比较序贯预测器的方法，通过设计新的序贯推断程序来估计预测得分的时变差异，这种方法避免了对预测和结果生成方式的不可验证假设。

    

    考虑两个预测器，在一段时间内对一系列事件进行单次预测。我们提出一个相对基础的问题：在不假设预测和结果生成方式的情况下，我们如何比较这些预测器，无论是在线还是事后比较？在本文中，我们通过设计用于估计时变预测得分差异的新型序贯推断程序，对这个问题给出了严格的答案。为此，我们采用置信序列（CS），它是一系列置信区间，可以连续监测并在任意数据依赖停时（“anytime-valid”）下有效。我们的置信序列的宽度是自适应的，适应了得分差异的底层方差。它们的构建基于博弈论统计框架，在这个框架中，我们进一步确定了用于顺序检验弱零假设的e过程和p过程，即一个预测器平均表现是否优于另一个预测器。

    Consider two forecasters, each making a single prediction for a sequence of events over time. We ask a relatively basic question: how might we compare these forecasters, either online or post-hoc, while avoiding unverifiable assumptions on how the forecasts and outcomes were generated? In this paper, we present a rigorous answer to this question by designing novel sequential inference procedures for estimating the time-varying difference in forecast scores. To do this, we employ confidence sequences (CS), which are sequences of confidence intervals that can be continuously monitored and are valid at arbitrary data-dependent stopping times ("anytime-valid"). The widths of our CSs are adaptive to the underlying variance of the score differences. Underlying their construction is a game-theoretic statistical framework, in which we further identify e-processes and p-processes for sequentially testing a weak null hypothesis -- whether one forecaster outperforms another on average (rather tha
    
[^171]: 架起靶网络与功能正则化之间的鸿沟

    Bridging the Gap Between Target Networks and Functional Regularization. (arXiv:2106.02613v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2106.02613](http://arxiv.org/abs/2106.02613)

    该论文研究了靶网络与功能正则化在深度强化学习中的作用。通过实验证明靶网络作为隐式正则化器在某些情况下有利，但不灵活且可能导致不稳定。为了解决这些问题，作者提出了一种明确的功能正则化替代方法，并对其收敛性进行了理论研究。

    

    引导是深度强化学习成功的关键。然而，通过引导学习值函数往往导致训练不稳定，原因是目标值快速变化。靶网络通过使用额外的滞后参数集合来估计目标值，以稳定训练。尽管靶网络很受欢迎，但其对优化的影响仍未被理解。在这项工作中，我们展示了靶网络作为一个隐式正则化器的作用，它在某些情况下是有益的，但也存在一些缺点，如不灵活和可能导致不稳定，即使香草TD(0)收敛。为了克服这些问题，我们提出了一个明确的功能正则化替代方法，它在函数空间中是灵活和凸正则化器，并对其收敛性进行了理论研究。我们在一系列环境、折扣因子和非随机数据收集下进行了实验研究，以调查其有效性。

    Bootstrapping is behind much of the successes of deep Reinforcement Learning. However, learning the value function via bootstrapping often leads to unstable training due to fast-changing target values. Target Networks are employed to stabilize training by using an additional set of lagging parameters to estimate the target values. Despite the popularity of Target Networks, their effect on the optimization is still misunderstood. In this work, we show that they act as an implicit regularizer which can be beneficial in some cases, but also have disadvantages such as being inflexible and can result in instabilities, even when vanilla TD(0) converges. To overcome these issues, we propose an explicit Functional Regularization alternative that is flexible and a convex regularizer in function space and we theoretically study its convergence. We conduct an experimental study across a range of environments, discount factors, and off-policiness data collections to investigate the effectiveness o
    
[^172]: BoXHED2.0：可扩展的动态生存分析提升方法

    BoXHED2.0: Scalable boosting of dynamic survival analysis. (arXiv:2103.12591v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2103.12591](http://arxiv.org/abs/2103.12591)

    BoXHED2.0是一个可扩展的动态生存分析提升方法，适用于包括重复事件和竞争风险在内的多种生存环境，具有与参数化提升生存模型相媲美的速度。

    

    现代生存分析的应用越来越多地涉及到时间依赖的协变量。Python软件包BoXHED2.0是一个完全非参数的树提升生存风险估计器，适用于比右截尾更通用的生存环境，包括重复事件和竞争风险。由于其核心是用C++编写的，还支持使用GPU和多核CPU，因此BoXHED2.0的可伸缩性可与参数化提升生存模型相媲美。BoXHED2.0可从PyPI和www.github.com/BoXHED获取。

    Modern applications of survival analysis increasingly involve time-dependent covariates. The Python package BoXHED2.0 is a tree-boosted hazard estimator that is fully nonparametric, and is applicable to survival settings far more general than right-censoring, including recurring events and competing risks. BoXHED2.0 is also scalable to the point of being on the same order of speed as parametric boosted survival models, in part because its core is written in C++ and it also supports the use of GPUs and multicore CPUs. BoXHED2.0 is available from PyPI and also from www.github.com/BoXHED.
    
[^173]: 图形平滑卷积网络用于异常检测

    Graph Fairing Convolutional Networks for Anomaly Detection. (arXiv:2010.10274v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2010.10274](http://arxiv.org/abs/2010.10274)

    本文引入了一种图形平滑卷积网络用于半监督异常检测，通过使用跳跃连接和图结构信息来学习有区分性的节点表示。

    

    图卷积是许多基于图结构数据的深度神经网络的基本构建块。在本文中，我们引入了一种简单而非常有效的带有跳跃连接的图卷积网络，用于半监督异常检测。我们模型的逐层传播规则在理论上受到几何处理中隐式平滑概念的启发，包括用于聚合来自相邻节点的信息的图卷积模块和用于组合逐层邻居表示的跳跃连接模块。这个传播规则是通过雅可比方法从隐式平滑方程的迭代解导出的。除了通过网络层之间的跳跃连接捕获来自远程图节点的信息外，我们的方法还利用图结构和节点特征来学习有区分性的节点表示。这些跳跃连接是根据我们提出的网络架构经过设计整合的。

    Graph convolution is a fundamental building block for many deep neural networks on graph-structured data. In this paper, we introduce a simple, yet very effective graph convolutional network with skip connections for semi-supervised anomaly detection. The proposed layerwise propagation rule of our model is theoretically motivated by the concept of implicit fairing in geometry processing, and comprises a graph convolution module for aggregating information from immediate node neighbors and a skip connection module for combining layer-wise neighborhood representations. This propagation rule is derived from the iterative solution of the implicit fairing equation via the Jacobi method. In addition to capturing information from distant graph nodes through skip connections between the network's layers, our approach exploits both the graph structure and node features for learning discriminative node representations. These skip connections are integrated by design in our proposed network archi
    
[^174]: 线性动态系统的适当学习作为非交换多项式优化问题

    Proper Learning of Linear Dynamical Systems as a Non-Commutative Polynomial Optimisation Problem. (arXiv:2002.01444v5 [math.OC] UPDATED)

    [http://arxiv.org/abs/2002.01444](http://arxiv.org/abs/2002.01444)

    该论文提出了一种方法来解决适当学习线性动态系统的问题，通过非交换多项式优化，保证了数值解对最小二乘估计器的全局收敛性。

    

    最近在预测线性动态系统（LDS）的下一个观测值（称为不适当学习）以及估计其系统矩阵（称为适当学习LDS）方面取得了很大进展。我们提出了一种适当学习LDS的方法，尽管问题非凸，但能够保证数值解的全局收敛性到最小二乘估计器。我们展示了有希望的计算结果。

    There has been much recent progress in forecasting the next observation of a linear dynamical system (LDS), which is known as the improper learning, as well as in the estimation of its system matrices, which is known as the proper learning of LDS. We present an approach to proper learning of LDS, which in spite of the non-convexity of the problem, guarantees global convergence of numerical solutions to a least-squares estimator. We present promising computational results.
    
[^175]: Copula表示和误差面投影对于异或问题的应用

    Copula Representations and Error Surface Projections for the Exclusive Or Problem. (arXiv:1907.04483v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1907.04483](http://arxiv.org/abs/1907.04483)

    本研究讨论了通过概率逻辑和关联Copula函数解决异或表示和逼近问题的方法，并通过比较不同激活函数下的误差面动态来说明其优势。通过将xor表示从布尔值扩展到实数值，我们提供了一种演示交叉验证概念的方便方式。

    

    异或（xor）函数是展示为什么非线性前馈网络在机器学习应用中优于线性回归的最简单的示例之一。我们通过概率逻辑和关联Copula函数讨论了xor表示和逼近问题及其解决方案。在简要回顾前馈网络规范之后，我们通过一组色彩丰富的三维图表比较了使用RELU和tanh等不同激活函数的学习误差面的动态。Copula表示将xor从布尔值扩展到实数值，从而提供了一种方便的方式来演示在样本内和样本外数据集上的交叉验证的概念。我们的方法是教学性的，旨在成为机器学习导论。

    The exclusive or (xor) function is one of the simplest examples that illustrate why nonlinear feedforward networks are superior to linear regression for machine learning applications. We review the xor representation and approximation problems and discuss their solutions in terms of probabilistic logic and associative copula functions. After briefly reviewing the specification of feedforward networks, we compare the dynamics of learned error surfaces with different activation functions such as RELU and tanh through a set of colorful three-dimensional charts. The copula representations extend xor from Boolean to real values, thereby providing a convenient way to demonstrate the concept of cross-validation on in-sample and out-sample data sets. Our approach is pedagogical and is meant to be a machine learning prolegomenon.
    

