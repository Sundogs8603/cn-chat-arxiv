# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Simple and Scalable Strategies to Continually Pre-train Large Language Models](https://arxiv.org/abs/2403.08763) | 通过简单和可扩展的学习率调整、重放数据的方法，可以在不重新训练的情况下，持续预训练大型语言模型以匹配完全重新训练时的性能。 |
| [^2] | [Efficient Combinatorial Optimization via Heat Diffusion](https://arxiv.org/abs/2403.08757) | 通过热扩散实现了高效的组合优化，克服了现有方法在搜索全局最优时效率有限的问题。 |
| [^3] | [DAM: Dynamic Adapter Merging for Continual Video QA Learning](https://arxiv.org/abs/2403.08755) | 提出了一种用于持续视频问答学习的动态适配器合并方法DAM，能够减轻灾难性遗忘、有效适应不断到来的数据集、处理未知数据集输入，并允许在类似数据集领域之间共享知识。 |
| [^4] | [Neural reproducing kernel Banach spaces and representer theorems for deep networks](https://arxiv.org/abs/2403.08750) | 本文展示了深度神经网络定义了适当的再生核巴拿赫空间，在这些空间中适应输入数据及其表示中潜在结构，通过再生核巴拿赫空间理论和变分结果得出了适用于实际中常见有限深度网络的表现定理。 |
| [^5] | [Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing Framework](https://arxiv.org/abs/2403.08743) | 本文提出了一种基于因果关系的去偏倾框架，通过选择机制指导设计提示来减少大型语言模型(LLMs)产生的社会偏见。 |
| [^6] | [Learning How to Strategically Disclose Information](https://arxiv.org/abs/2403.08741) | 本研究针对一个发送者与每一轮敌意选择的未知类型接收者之间的信息设计问题，证明了使用全信息反馈时可以实现$O(\sqrt{T})$的后悔。 |
| [^7] | [Ambient Diffusion Posterior Sampling: Solving Inverse Problems with Diffusion Models trained on Corrupted Data](https://arxiv.org/abs/2403.08728) | 提出了一种使用环境扩散后验采样解决逆问题的框架，能在受损数据上训练的扩散模型上表现出色，并在图像恢复和MRI模型训练中取得优越性能。 |
| [^8] | [Diffusion-based Iterative Counterfactual Explanations for Fetal Ultrasound Image Quality Assessment](https://arxiv.org/abs/2403.08700) | 提出了基于扩散的迭代反事实解释的方法，通过生成逼真的高质量标准平面，对提高临床医生的培训、改善图像质量以及提升下游诊断和监测具有潜在价值。 |
| [^9] | [Implicit Regularization of Gradient Flow on One-Layer Softmax Attention](https://arxiv.org/abs/2403.08699) | 研究了在一层Softmax注意力模型上指数损失函数的梯度流，发现在渐进最小化损失值时隐式最小化了关键和查询权重矩阵乘积的核范数，这种隐式正则化可通过与注意力权重相关的SVM问题描述。 |
| [^10] | [Digital Twin-assisted Reinforcement Learning for Resource-aware Microservice Offloading in Edge Computing](https://arxiv.org/abs/2403.08687) | 该论文介绍了一种利用深度强化学习和数字孪生技术的新型微服务卸载算法，用于解决边缘计算环境中低效微服务卸载策略的问题。 |
| [^11] | [When can we Approximate Wide Contrastive Models with Neural Tangent Kernels and Principal Component Analysis?](https://arxiv.org/abs/2403.08673) | 本研究分析了具有非线性激活函数的两层对比模型的训练动态，揭示了在何种情况下这些模型接近于主成分分析（PCA）或核方法。同时，提供了对比损失的NTK收敛结果，为对比学习与核方法之间的关联提供了更深入的理解。 |
| [^12] | [Zero-shot and Few-shot Generation Strategies for Artificial Clinical Records](https://arxiv.org/abs/2403.08664) | 这项研究探讨了利用合成数据生成医疗记录的方法，特别是通过零样本和少样本提示策略，避免了在训练过程中使用真实患者数据的隐私问题。 |
| [^13] | [Self-Supervised Learning for Covariance Estimation](https://arxiv.org/abs/2403.08662) | 提出了一种利用自监督学习进行协方差估计的方法，通过全局训练神经网络并在推断时局部应用，在不需要任何标签的情况下利用全局特征，具有自动化的全局特征利用优势。 |
| [^14] | [Extracting Explanations, Justification, and Uncertainty from Black-Box Deep Neural Networks](https://arxiv.org/abs/2403.08652) | 本文提出了一种新的贝叶斯方法，从黑箱深度神经网络中提取解释、证明和不确定性，有效提高了DNN的可解释性和可靠性。 |
| [^15] | [Disparate Effect Of Missing Mediators On Transportability of Causal Effects](https://arxiv.org/abs/2403.08638) | 研究了缺失中介变量对传输的中介效应的影响，并提出了一个敏感性分析框架，能够确定在哪些情况下，具有缺失中介数据的子群的条件传输中介效应变得不显著。 |
| [^16] | [Human Alignment of Large Language Models through Online Preference Optimisation](https://arxiv.org/abs/2403.08635) | 本文展示了两种最近对齐方法之间的等价性，并介绍了一种泛化版本，有助于实现大型语言模型与人类的对齐。 |
| [^17] | [A Decade's Battle on Dataset Bias: Are We There Yet?](https://arxiv.org/abs/2403.08632) | 现代神经网络在分类来自不同数据集的图像方面表现出色，具有可推广和可转移的语义特征，挑战了传统的数据集偏见认知。 |
| [^18] | [Leveraging Non-Decimated Wavelet Packet Features and Transformer Models for Time Series Forecasting](https://arxiv.org/abs/2403.08630) | 本文结合了小波分析技术和机器学习方法，提出利用不同消失矩的Daubechies小波作为输入特征，并比较了非降小波变换和非降小波包变换的效果，并在更广泛的预测方法上评估了这些小波特征的应用。 |
| [^19] | [Multifidelity linear regression for scientific machine learning from scarce data](https://arxiv.org/abs/2403.08627) | 提出了一种新的多信度训练方法，用于处理科学机器学习中稀缺而昂贵的高保真数据，以提高模型的稳健性和泛化能力。 |
| [^20] | [Verifix: Post-Training Correction to Improve Label Noise Robustness with Verified Samples](https://arxiv.org/abs/2403.08618) | 提出了后训练校正的新范式，通过奇异值分解算法Verifix在初始训练后校正模型权重以减轻标签噪声，避免了重新训练的需求 |
| [^21] | [Link Prediction for Social Networks using Representation Learning and Heuristic-based Features](https://arxiv.org/abs/2403.08613) | 提出了一种结合启发式特征和学习表示的方法，用于社交网络中缺失链接的预测任务，取得了较好的性能提升 |
| [^22] | [On the Convergence of Locally Adaptive and Scalable Diffusion-Based Sampling Methods for Deep Bayesian Neural Network Posteriors](https://arxiv.org/abs/2403.08609) | 本文研究如何将自适应步长引入蒙特卡罗采样算法，以实现从神经网络后验分布中生成样本，并证明了这些方法可以收敛到正确的分布。 |
| [^23] | [Data-Efficient Sleep Staging with Synthetic Time Series Pretraining](https://arxiv.org/abs/2403.08592) | 通过预测合成时间序列的频率内容进行预训练，实现了在有限数据和少受试者情况下超越完全监督学习的方法 |
| [^24] | [Can physical information aid the generalization ability of Neural Networks for hydraulic modeling?](https://arxiv.org/abs/2403.08589) | 研究通过在训练阶段引入物理信息的方法来改善神经网络在河流水力学中的预测能力问题。 |
| [^25] | [Improving Implicit Regularization of SGD with Preconditioning for Least Square Problems](https://arxiv.org/abs/2403.08585) | 通过预条件化，研究了SGD在最小二乘问题中的泛化性能，对比了预条件化SGD和（标准和预条件化）岭回归，为改善SGD理解和应用提供了关键贡献。 |
| [^26] | [Local Binary and Multiclass SVMs Trained on a Quantum Annealer](https://arxiv.org/abs/2403.08584) | 提出了一种基于量子训练的支持向量机模型的局部应用，以克服量子退火器受限连通性所导致的训练集大小限制 |
| [^27] | [Machine Learning Optimized Orthogonal Basis Piecewise Polynomial Approximation](https://arxiv.org/abs/2403.08579) | 本研究利用机器学习的方法对正交基分段多项式逼近进行优化。 |
| [^28] | [Caformer: Rethinking Time Series Analysis from Causal Perspective](https://arxiv.org/abs/2403.08572) | 提出了一个名为Caformer的新框架用于时间序列分析，其中包括动态学习器、环境学习器和依赖关系学习器，旨在从因果关系的角度解决在非平稳时间序列中捕捉跨维度和跨时间依赖关系的挑战 |
| [^29] | [A Physics-driven GraphSAGE Method for Physical Process Simulations Described by Partial Differential Equations](https://arxiv.org/abs/2403.08569) | 该论文提出了一种基于物理驱动的GraphSAGE方法，用于解决由不规则PDE管控的计算问题，具有降低精度的优势。 |
| [^30] | [Consistent Prompting for Rehearsal-Free Continual Learning](https://arxiv.org/abs/2403.08568) | 提出了一种新颖的一致提示（CPrompt）方法，通过训练期间所有现有分类器接受提示训练，实现更加对齐的训练和测试。 |
| [^31] | [Structural perspective on constraint-based learning of Markov networks](https://arxiv.org/abs/2403.08562) | 本论文从结构角度研究了基于约束的学习马尔可夫网络，发现了图的结构特性和学习所需测试数量之间的重要关系 |
| [^32] | [Federated Knowledge Graph Unlearning via Diffusion Model](https://arxiv.org/abs/2403.08554) | 提出了FedDM，一个基于扩散模型的新框架，用于联邦知识图中的机器去学习。 |
| [^33] | [Regret Analysis of Policy Optimization over Submanifolds for Linearly Constrained Online LQG](https://arxiv.org/abs/2403.08553) | 提出了在线乐观牛顿流形（OONM），该方法提供基于函数序列的第一和第二阶信息预测的在线控制器，用于在线LQG线性约束政策优化。 |
| [^34] | [CINA: Conditional Implicit Neural Atlas for Spatio-Temporal Representation of Fetal Brains](https://arxiv.org/abs/2403.08550) | CINA提出了一种条件隐式神经图谱（CINA），可以生成胎儿大脑的时空图谱，而无需仿射或非刚性配准，训练后可构建忠实的组织概率图，同时适用于神经型和病理性大脑。 |
| [^35] | [Language models scale reliably with over-training and on downstream tasks](https://arxiv.org/abs/2403.08540) | 本研究解决了语言模型缩放研究中过度训练和下游任务性能评估之间的差距。 |
| [^36] | [HOLMES: HOLonym-MEronym based Semantic inspection for Convolutional Image Classifiers](https://arxiv.org/abs/2403.08536) | HOLMES提出了一种新技术，通过将标签分解为一组相关概念并提供部件级解释，来帮助理解和解释卷积图像分类模型。 |
| [^37] | [From Weak to Strong Sound Event Labels using Adaptive Change-Point Detection and Active Learning](https://arxiv.org/abs/2403.08525) | 提出一种基于自适应变点检测和主动学习的音频录制分割方法，通过预测模型和变点检测逐步生成高质量的强标签。 |
| [^38] | [DiPrompT: Disentangled Prompt Tuning for Multiple Latent Domain Generalization in Federated Learning](https://arxiv.org/abs/2403.08506) | 提出了一种名为DiPrompT的解耦提示调整方法，通过学习适应提示来解决在联邦学习中对领域泛化的限制。 |
| [^39] | [SoK: Reducing the Vulnerability of Fine-tuned Language Models to Membership Inference Attacks](https://arxiv.org/abs/2403.08481) | 首次系统评估了大型经过微调的语言模型对成员推断攻击的脆弱性，以及相关因素和不同防御策略的有效性。 |
| [^40] | [Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts](https://arxiv.org/abs/2403.08477) | 本文提出了一种名为Sparse MetA-Tuning（SMAT）的方法，通过灵感来自稀疏专家混合方法，成功克服了域外任务敏感性，实现了增强视觉基础模型转移能力的目标。 |
| [^41] | [An Analysis of Human Alignment of Latent Diffusion Models](https://arxiv.org/abs/2403.08469) | 分析了潜在扩散模型的排列对人类响应的对齐情况，并发现模型的对齐性与ImageNet-1k相当，去噪U-Net的最对齐层是中间层而不是瓶颈，并且文本调节在高噪声水平下提高了对齐性。 |
| [^42] | [Diffusion Models with Implicit Guidance for Medical Anomaly Detection](https://arxiv.org/abs/2403.08464) | 本文引入了一种名为THOR（Temporal Harmonization for Optimal Restoration）的方法，通过在时间异常图中整合隐式引导来改进去噪过程，旨在保持未受病变影响区域健康组织的完整性。 |
| [^43] | [Authorship Verification based on the Likelihood Ratio of Grammar Models](https://arxiv.org/abs/2403.08462) | 提出了一种基于计算作者文件在候选作者语法模型与参考群体语法模型下的可能性比率的方法，用以解决作者身份验证中存在的科学解释不足和难以解释的问题 |
| [^44] | [Actor-Critic Physics-informed Neural Lyapunov Control](https://arxiv.org/abs/2403.08448) | 提出了一种新方法，通过使用祖博夫的偏微分方程（PDE）来训练神经网络控制器，以及对应的李雅普诺夫证书，以最大化区域吸引力，并尊重激励约束。 |
| [^45] | [COSTREAM: Learned Cost Models for Operator Placement in Edge-Cloud Environments](https://arxiv.org/abs/2403.08444) | COSTREAM提出了一种可在边缘-云环境中准确预测流查询执行成本的学习成本模型，并通过优化运算符的放置，实现了高达21倍的中位数加速。 |
| [^46] | [Reproducibility and Geometric Intrinsic Dimensionality: An Investigation on Graph Neural Network Research](https://arxiv.org/abs/2403.08438) | 本文研究了图神经网络研究中的再现性和几何内在维度性问题，并引入机器学习中的再现性本体论，以及探讨了维度诅咒对数据收集、表示和分析的挑战。 |
| [^47] | [DeepCSHAP: Utilizing Shapley Values to Explain Deep Complex-Valued Neural Networks](https://arxiv.org/abs/2403.08428) | 本文提出了DeepCSHAP算法，通过利用Shapley Values解释复数神经网络的输出，填补了对于这种类型神经网络的解释算法的空白。 |
| [^48] | [The Development and Performance of a Machine Learning Based Mobile Platform for Visually Determining the Etiology of Penile Pathology](https://arxiv.org/abs/2403.08417) | 通过使用机器学习算法开发了一个移动平台，可以视觉确定阴茎病理的病因，从而提高对性健康服务的获取公平性。 |
| [^49] | [Causal Graph Neural Networks for Wildfire Danger Prediction](https://arxiv.org/abs/2403.08414) | 通过将因果性与图神经网络相结合，模型能够显式地建模复杂变量之间的因果机理，提高了火灾模式预测的性能。 |
| [^50] | [Reduced Jeffries-Matusita distance: A Novel Loss Function to Improve Generalization Performance of Deep Classification Models](https://arxiv.org/abs/2403.08408) | 提出了一种减小Jeffries-Matusita距离的损失函数，用于训练深度分类模型以减少过拟合问题。 |
| [^51] | [FSDR: A Novel Deep Learning-based Feature Selection Algorithm for Pseudo Time-Series Data using Discrete Relaxation](https://arxiv.org/abs/2403.08403) | FSDR是一种基于深度学习的特征选择算法，通过离散松弛学习重要特征，适用于高维的伪时间序列数据 |
| [^52] | [Optimizing Risk-averse Human-AI Hybrid Teams](https://arxiv.org/abs/2403.08386) | 提出了一种经理通过强化学习方案学习如何在混合人工智能团队中最佳分配决策责任，并最小化不良团队行为导致的委派变更次数。 |
| [^53] | [Nonlinear Manifold Learning Determines Microgel Size from Raman Spectroscopy](https://arxiv.org/abs/2403.08376) | 本研究提出了三种替代机器学习工作流程来通过非线性流形学习技术从拉曼光谱测量中准确可靠地确定聚合物尺寸。 |
| [^54] | [SMART: Submodular Data Mixture Strategy for Instruction Tuning](https://arxiv.org/abs/2403.08370) | SMART引入了一种新颖的数据混合策略，利用子模块函数为任务分配重要性分数，并在微调中重新分配预算，从而在指令调整任务中取得明显优势。 |
| [^55] | [Decoupled Federated Learning on Long-Tailed and Non-IID data with Feature Statistics](https://arxiv.org/abs/2403.08364) | 本文提出了针对长尾和非独立同分布数据的特征统计分开式联邦学习框架，通过两阶段方式解决尾部类别稀疏分布导致的模型性能下降问题 |
| [^56] | [Mean-Field Microcanonical Gradient Descent](https://arxiv.org/abs/2403.08362) | 提出了均场微正则梯度下降方法，通过同时采样多个弱耦合数据点，在控制熵损失的同时在似然拟合方面表现良好。 |
| [^57] | [Data augmentation with automated machine learning: approaches and performance comparison with classical data augmentation methods](https://arxiv.org/abs/2403.08352) | 自动化机器学习的数据增强方法旨在自动化数据增强过程，为改善机器学习模型泛化性能提供了更高效的方式。 |
| [^58] | [STMPL: Human Soft-Tissue Simulation](https://arxiv.org/abs/2403.08344) | 本论文提出了一种基于数据驱动的软组织模拟方法，通过在SMPL模型的基础上引入软组织层和外部力作用的直观表示，实现了对人体形状和软组织进行快速逼真模拟。 |
| [^59] | [LLM-Assisted Light: Leveraging Large Language Model Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments](https://arxiv.org/abs/2403.08337) | 本研究提出了将大型语言模型(LLMs)整合到交通信号控制(TSC)系统中的创新方法，以解决传统TSC系统在适应不熟悉场景方面的限制，并提出了一个混合框架，使得LLMs与一系列感知和决策工具相结合，从而提升TSC系统对城市交通复杂性和变异性的管理能力。 |
| [^60] | [A Sparsity Principle for Partially Observable Causal Representation Learning](https://arxiv.org/abs/2403.08335) | 提出了部分可观测因果表示学习的稀疏原则，建立了两个可识别性结果，为线性混合函数和分段线性混合函数设置了基础模型。 |
| [^61] | [Fast Inference of Removal-Based Node Influence](https://arxiv.org/abs/2403.08333) | 提出了一种评估节点影响的新方法，通过测量训练好的图神经网络模型在移除节点后的预测变化，以实现快速推断。 |
| [^62] | [Bayesian Optimization that Limits Search Region to Lower Dimensions Utilizing Local GPR](https://arxiv.org/abs/2403.08331) | 提出了一种Bayesian优化方法，通过将搜索区域限制在较低维度，并利用本地GPR模型，在高维度中提高了搜索效率和预测准确性。 |
| [^63] | [Knowledge Conflicts for LLMs: A Survey](https://arxiv.org/abs/2403.08319) | 这项调查深入分析了LLMs在融合上下文和参数化知识时所面临的知识冲突，探讨了三类知识冲突对其可信度和性能的重要影响，并提出改进LLMs稳健性策略的策略。 |
| [^64] | [HRLAIF: Improvements in Helpfulness and Harmlessness in Open-domain Reinforcement Learning From AI Feedback](https://arxiv.org/abs/2403.08309) | 提出了HRLAIF方法来改善开放域强化学习中的模型响应帮助性，通过增强AI注释响应的准确性来提高模型在训练过程中的鲁棒性 |
| [^65] | [CleanAgent: Automating Data Standardization with LLM-based Agents](https://arxiv.org/abs/2403.08291) | 提出了一个具有声明性、统一API的Python库，通过简洁的API调用简化LLM的代码生成流程 |
| [^66] | [SNOW-SCA: ML-assisted Side-Channel Attack on SNOW-V](https://arxiv.org/abs/2403.08267) | 这项研究提出了SNOW-SCA，实现了对SNOW-V的侧信道攻击，成功破解了整个256位秘密密钥。 |
| [^67] | [Random Search as a Baseline for Sparse Neural Network Architecture Search](https://arxiv.org/abs/2403.08265) | 论文提出了一种评估方法和基于随机搜索的基线方法，用于发现高质量的稀疏神经网络配置，以解决当前缺乏可靠比较和可重现性的问题。 |
| [^68] | [Skipformer: A Skip-and-Recover Strategy for Efficient Speech Recognition](https://arxiv.org/abs/2403.08258) | Skipformer提出了一种“跳过和恢复”的Conformer架构，可以动态、不均匀地压缩序列输入长度，大大减少了计算预算和内存消耗，并获得更好的识别准确性。 |
| [^69] | [Machine Unlearning: Taxonomy, Metrics, Applications, Challenges, and Prospects](https://arxiv.org/abs/2403.08254) | 机器遗忘是一项重要的研究方向，涉及对个人数字数据的删除和对机器学习模型的影响，迫切需要全面调查来捕捉最新进展。 |
| [^70] | [Towards Unified Modeling for Positive and Negative Preferences in Sign-Aware Recommendation](https://arxiv.org/abs/2403.08246) | 提出了一种面向推荐的轻量级符号图卷积网络LSGRec，采用统一建模方法同时对高阶用户的正负偏好进行建模 |
| [^71] | [Scattered Mixture-of-Experts Implementation](https://arxiv.org/abs/2403.08245) | ScatterMoE是一种在GPU上实现的稀疏专家混合模型，通过避免填充和过多复制输入，提高了推理和训练速度，并减少了内存占用。 |
| [^72] | [Continuous Object State Recognition for Cooking Robots Using Pre-Trained Vision-Language Models and Black-box Optimization](https://arxiv.org/abs/2403.08239) | 提出一种通过口语利用预训练大规模视觉-语言模型识别烹饪机器人连续食物状态变化的方法，以连续捕捉食物状态变化。 |
| [^73] | [Robust Decision Aggregation with Adversarial Experts](https://arxiv.org/abs/2403.08222) | 论文考虑了在既有真实专家又有对抗性专家的情况下的二元决策聚合问题，提出了设计鲁棒聚合器以最小化遗憾的方法，并证明了当真实专家是对称的且对抗性专家不太多时，截尾均值是最优的。 |
| [^74] | [Efficient geometric Markov chain Monte Carlo for nonlinear Bayesian inversion enabled by derivative-informed neural operators](https://arxiv.org/abs/2403.08220) | 运用导数信息的神经算子加速了几何马尔可夫链蒙特卡洛方法，显著加快了解决非线性贝叶斯反问题的过程。 |
| [^75] | [Research on the Application of Deep Learning-based BERT Model in Sentiment Analysis](https://arxiv.org/abs/2403.08217) | 本文研究了深度学习技术在情感分析中的应用，重点探讨了BERT模型的架构、特性以及优化策略，并通过实验证实了BERT模型在情感分析中表现出的稳健性能和提升潜力。 |
| [^76] | [PaddingFlow: Improving Normalizing Flows with Padding-Dimensional Noise](https://arxiv.org/abs/2403.08216) | 提出了一种名为PaddingFlow的去量化方法，利用填充维度噪声改进了正规化流，解决了基于流的模型在流形和离散数据上的性能问题。 |
| [^77] | [LIX: Implicitly Infusing Spatial Geometric Prior Knowledge into Visual Semantic Segmentation for Autonomous Driving](https://arxiv.org/abs/2403.08215) | 将双编码器教师模型获得的空间几何先验知识隐式注入单编码器学生模型，通过新的logit蒸馏和特征蒸馏方法，解决自动驾驶中的视觉语义分割问题。 |
| [^78] | [BG-HGNN: Toward Scalable and Efficient Heterogeneous Graph Neural Network](https://arxiv.org/abs/2403.08207) | BG-HGNN提出了一种新颖的框架，有效地处理了现有HGNNs在复杂异构图上面临的参数爆炸和关系坍塌等挑战 |
| [^79] | [AutoDFP: Automatic Data-Free Pruning via Channel Similarity Reconstruction](https://arxiv.org/abs/2403.08204) | 提出一种名为AutoDFP的自动无数据剪枝方法，通过保留相似通道的重点信息实现自动剪枝和重建，无需进行微调。 |
| [^80] | [Learnable Community-Aware Transformer for Brain Connectome Analysis with Token Clustering](https://arxiv.org/abs/2403.08203) | 提出了一种用于联合社区聚类和分类的可学习的面向社区的变压器模型。 |
| [^81] | [Deep Submodular Peripteral Network](https://arxiv.org/abs/2403.08199) | 引入了深度子模逆点网络（DSPNs），并提出了一种使用对比学习启发的GPC-ready策略进行训练的方法，以应对子模函数学习中的两大挑战。 |
| [^82] | [PAGE: Domain-Incremental Adaptation with Past-Agnostic Generative Replay for Smart Healthcare](https://arxiv.org/abs/2403.08197) | PAGE提出了一种面向智能医疗的领域增量适应策略，能够在不依赖于保留数据或信息的情况下实现生成回放，有效平衡领域适应和知识保留，并结合扩展的归纳确认预测方法提供可解释的疾病检测预测。 |
| [^83] | [Unsupervised Learning of Hybrid Latent Dynamics: A Learn-to-Identify Framework](https://arxiv.org/abs/2403.08194) | 提出了一种无监督元学习框架来学习混合潜在动态，该框架结合了物理归纳偏差和学习识别策略，在捕捉未知动态和分辨率之间取得了平衡。 |
| [^84] | [Learning-driven Physically-aware Large-scale Circuit Gate Sizing](https://arxiv.org/abs/2403.08193) | 本研究提出了一种学习驱动的物理感知门尺寸优化框架，能够有效优化大规模电路的时序性能。 |
| [^85] | [Tractable Local Equilibria in Non-Concave Games](https://arxiv.org/abs/2403.08171) | 提出了一个新的解决概念，$(\varepsilon, \Phi(\delta))$-局部均衡，以解决在非凹游戏中局部均衡存在但难以处理的问题。 |
| [^86] | [MolBind: Multimodal Alignment of Language, Molecules, and Proteins](https://arxiv.org/abs/2403.08167) | MolBind 提出了一个框架，通过对比学习为多种模态训练编码器，将所有模态映射到共享特征空间，实现多模态语义对齐。 |
| [^87] | [EM-TTS: Efficiently Trained Low-Resource Mongolian Lightweight Text-to-Speech](https://arxiv.org/abs/2403.08164) | 提出了一种基于深度卷积神经网络的轻量级TTS系统，采用两阶段训练而非递归单元，可以显著减少训练时间和经济成本 |
| [^88] | [Iterative Learning for Joint Image Denoising and Motion Artifact Correction of 3D Brain MRI](https://arxiv.org/abs/2403.08162) | 提出了一种通过迭代学习处理带有噪声和运动伪影的MRI的联合图像去噪和运动伪影修正框架 |
| [^89] | [Asymptotics of Random Feature Regression Beyond the Linear Scaling Regime](https://arxiv.org/abs/2403.08160) | 本文研究了随机特征岭回归模型，探讨了参数化对模型性能的影响，以及如何选择参数数量$p$相对于样本大小$n$以实现最佳测试错误率。 |
| [^90] | [The Effect of Different Optimization Strategies to Physics-Constrained Deep Learning for Soil Moisture Estimation](https://arxiv.org/abs/2403.08154) | 基于物理约束的深度学习框架结合了水分传输和水分传感信号的基于物理的原则，采用Adam、RMSprop和GD三种不同的优化器，在土壤湿度估计中取得了较好的收敛效果。 |
| [^91] | [Measuring the Energy Consumption and Efficiency of Deep Neural Networks: An Empirical Analysis and Design Recommendations](https://arxiv.org/abs/2403.08151) | 这项研究针对大规模神经网络不断增长的能耗问题进行了实证分析，提出了一个考虑网络尺寸、计算和内存层次结构的能耗模型。 |
| [^92] | [Representing Molecules as Random Walks Over Interpretable Grammars](https://arxiv.org/abs/2403.08147) | 提出了一种新颖的分子表示模型，使用可解释的图文法描述分子的层次化设计空间，实现了在设计空间上的随机游走，从而提高了分子生成和属性预测的性能、效率和可合成性。 |
| [^93] | [Cost-Effective Methodology for Complex Tuning Searches in HPC: Navigating Interdependencies and Dimensionality](https://arxiv.org/abs/2403.08131) | 优化搜索在高性能计算中至关重要，而我们提出的方法在保证计算可行性的前提下最大化实际场景中的性能收益。 |
| [^94] | [Towards Independence Criterion in Machine Unlearning of Features and Labels](https://arxiv.org/abs/2403.08124) | 提出了一种利用影响函数和分布独立原则的新方法，以应对机器遗忘中非均匀特征和标签删除的挑战，保护隐私同时保持模型性能和适应性 |
| [^95] | [Early Directional Convergence in Deep Homogeneous Neural Networks for Small Initializations](https://arxiv.org/abs/2403.08121) | 本文研究了训练深度齐次神经网络时梯度流动力学的动态性，发现在足够小的初始化下，神经网络的权重在训练早期阶段保持较小规范，并且沿着神经相关函数的KKT点方向近似收敛。 |
| [^96] | [Characterising harmful data sources when constructing multi-fidelity surrogate models](https://arxiv.org/abs/2403.08118) | 研究指出在构建多保真度代理模型时，有害数据源的特征化有助于指导从业者在选择时何时忽略某个数据源。 |
| [^97] | [Efficient Language Model Architectures for Differentially Private Federated Learning](https://arxiv.org/abs/2403.08100) | 提出了一个具有尺度不变性的耦合输入遗忘门递归网络，通过修改循环单元中的激活函数，使其能够更快地收敛并在跨设备联邦学习中取得更好的效果。 |
| [^98] | [Mechanics of Next Token Prediction with Self-Attention](https://arxiv.org/abs/2403.08081) | 通过梯度下降训练自注意力学习到一个自动机，在下一个标记预测中生成标记的两个不同步骤是：硬检索和软组合。 |
| [^99] | [FluoroSAM: A Language-aligned Foundation Model for X-ray Image Segmentation](https://arxiv.org/abs/2403.08059) | FluoroSAM是用于X光图像的分割的语言对齐基础模型，提供了一种在X光成像领域具有广泛适用性的自动图像分析工具。 |
| [^100] | [CHAI: Clustered Head Attention for Efficient LLM Inference](https://arxiv.org/abs/2403.08058) | CHAI提出了Clustered Head Attention（CHAI）方法，通过在运行时结合具有高相关性的注意力头部，实现了减少内存需求和计算量，能够在不需要微调的情况下将存储K,V缓存的内存需求降低21.4％，推理时间延迟降低1.73倍。 |
| [^101] | [DrivAerNet: A Parametric Car Dataset for Data-Driven Aerodynamic Design and Graph-Based Drag Prediction](https://arxiv.org/abs/2403.08055) | DrivAerNet提供了一个大规模高保真度的汽车数据集，以解决工程应用中训练深度学习模型所需的数据不足问题，而RegDGCNN利用这一数据集直接从3D网格提供高精度的阻力估计。 |
| [^102] | [TutoAI: A Cross-domain Framework for AI-assisted Mixed-media Tutorial Creation on Physical Tasks](https://arxiv.org/abs/2403.08049) | TutoAI 是一个跨领域的框架，用于在物理任务上利用AI辅助混合媒体教程创建，通过调查常见教程组件、评估AI模型提取组件的方法以及设计UI支持教程创建的指南，证明了其较基准模型具有更高或相似的质量。 |
| [^103] | [CT evaluation of 2D and 3D holistic deep learning methods for the volumetric segmentation of airway lesions](https://arxiv.org/abs/2403.08042) | 本研究比较了2D和3D格式的卷积神经网络在气道病变体积分割方面的能力，发现3D模型在捕捉复杂特征方面表现更优异，并通过对2D模型实施细微结构分割损失来提高准确性，通过外部验证证实了研究结果的稳健性 |
| [^104] | [MicroT: Low-Energy and Adaptive Models for MCUs](https://arxiv.org/abs/2403.08040) | MicroT是一个低能耗、多任务自适应模型框架，通过特征提取器和分类器的分离、模型优化和本地任务训练，在MCUs上实现了模型性能的提升和能耗的降低。 |
| [^105] | [McCatch: Scalable Microcluster Detection in Dimensional and Nondimensional Datasets](https://arxiv.org/abs/2403.08027) | 该论文提出了 McCatch 算法，通过引入“Oracle”图来检测微簇，是目前唯一可回答两个关键问题的方法，具有在处理非维度数据以及具有非单点微簇时更好的性能。 |
| [^106] | [xMLP: Revolutionizing Private Inference with Exclusive Square Activation](https://arxiv.org/abs/2403.08024) | 本文提出了xMLP，这是一种独特的DNN架构，使用独占的方激活，在维持准确性的同时减少私密推断系统中的延迟 |
| [^107] | [Aedes aegypti Egg Counting with Neural Networks for Object Detection](https://arxiv.org/abs/2403.08016) | 通过使用神经网络进行目标检测，提出了一种新的数据集，来自野外和实验室卵，并测试了三种神经网络在伊氏伊蚊卵计数任务中的效果。 |
| [^108] | [Supervised Time Series Classification for Anomaly Detection in Subsea Engineering](https://arxiv.org/abs/2403.08013) | 研究了监督机器学习算法在基于模拟数据的物理系统中的应用，通过对时间数据进行预处理和比较不同方法的性能指标，展示了机器学习技术在决策中的优势。 |
| [^109] | [Gujarati-English Code-Switching Speech Recognition using ensemble prediction of spoken language](https://arxiv.org/abs/2403.08011) | 通过在输出中以每层有监督的方式对单词和字符的语言ID条件化变压器层，该方法虽然未能显著降低词错误率，但展现了在仅仅通过口语数据预测正确语言的潜力。 |
| [^110] | [Motifs, Phrases, and Beyond: The Modelling of Structure in Symbolic Music Generation](https://arxiv.org/abs/2403.07995) | 介绍了符号音乐生成中关于音乐结构建模的技术演变，包括利用深度学习方法将音乐生成分解为高层结构规划和内容创建阶段。 |
| [^111] | [Do Agents Dream of Electric Sheep?: Improving Generalization in Reinforcement Learning through Generative Learning](https://arxiv.org/abs/2403.07979) | 通过生成式增强技术，本研究研究了在稀疏奖励环境中通过基于想象力的强化学习训练来提高强化学习智能体泛化能力的方法。 |
| [^112] | [LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code](https://arxiv.org/abs/2403.07974) | LiveCodeBench提出了一个全面的、无污染的LLMs评估工具，聚焦于从LeetCode、AtCoder和CodeForces等平台连续收集的新问题，覆盖自修复、代码执行、测试输出预测等更广泛的代码相关能力。 |
| [^113] | [KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction](https://arxiv.org/abs/2403.07969) | 本文提出了KnowCoder，一个通过代码生成执行普适信息提取的大型语言模型，引入了代码风格的模式表示方法和两阶段学习框架，以提高LLMs对结构化知识的准确提取能力 |
| [^114] | [Do Deep Neural Network Solutions Form a Star Domain?](https://arxiv.org/abs/2403.07968) | SGD解决方案集是一个星形域，包含一个星形模型，通过低损失数值的路径与其他解决方案线性相连，模除排列。 |
| [^115] | [Feasibility of machine learning-based rice yield prediction in India at the district level using climate reanalysis data](https://arxiv.org/abs/2403.07967) | 该研究探究了基于机器学习的模型是否能够准确地在水稻收获前几个月预测印度各区 Kharif 季节水稻的产量，研究证明水稻产量可以被合理准确地预测。 |
| [^116] | [Applying ranking techniques for estimating influence of Earth variables on temperature forecast error](https://arxiv.org/abs/2403.07966) | 本研究介绍了如何分析地球系统变量对温度预报误差的影响，通过引入三个创新：应用数据科学方法在代表性位置进行排名、利用Spearman相关性创建排名并结合其他度量丰富排名、通过学习随机森林模型评估方法论，最终将相关性转化为排名并组合成一个总体排名。 |
| [^117] | [Conditional computation in neural networks: principles and research trends](https://arxiv.org/abs/2403.07965) | 该论文总结了将条件计算方法应用于设计神经网络的新兴领域中的原理和思想，并介绍了专家混合网络、标记选择机制和提前退出神经网络等三种实现方式。 |
| [^118] | [Unsupervised self-organising map of prostate cell Raman spectra shows disease-state subclustering](https://arxiv.org/abs/2403.07960) | 使用无监督的自组织映射方法成功区分了正常前列腺和癌细胞，并展示了前列腺癌细胞的新子群分类。 |
| [^119] | [Temporal Decisions: Leveraging Temporal Correlation for Efficient Decisions in Early Exit Neural Networks](https://arxiv.org/abs/2403.07958) | 该论文引入了差异检测和时间耐心作为早期退出神经网络的决策机制，利用传感器数据流中的时间相关性来有效终止推断。 |
| [^120] | [Efficient Post-Training Augmentation for Adaptive Inference in Heterogeneous and Distributed IoT Environments](https://arxiv.org/abs/2403.07957) | 提出了一种自动增强流程，能够将现有模型转换为早期退出神经网络（EENN），提高神经网络部署效率，实现了在物联网和图像分类用例上显著减少推断操作的效果。 |
| [^121] | [DeepCDCL: An CDCL-based Neural Network Verification Framework](https://arxiv.org/abs/2403.07956) | 提出了一种基于CDCL算法的神经网络验证框架DeepCDCL，通过引入异步子句学习和管理结构，显著减少了时间消耗，并在ACAS Xu和MNIST数据集上展示了显著的加速。 |
| [^122] | [Towards Faithful Explanations: Boosting Rationalization with Shortcuts Discovery](https://arxiv.org/abs/2403.07955) | 本文提出了一种Shortcuts-fused Selective Rationalization (SSR)方法，通过发现和利用潜在的快捷方式来提升理性化，并通过两种策略缓解利用快捷方式来组成理性化的问题，以及通过数据增强方法来补充已注释理性化数量的差距。 |
| [^123] | [Optimizing Polynomial Graph Filters: A Novel Adaptive Krylov Subspace Approach](https://arxiv.org/abs/2403.07954) | 本文通过统一多项式图滤波器和相同次数的最优滤波器到同阶克里洛夫子空间，提供了等效的表达能力；设计了一种新的自适应克里洛夫子空间方法，以优化具有可控性的多项式基准。 |
| [^124] | [Abstracting Sparse DNN Acceleration via Structured Sparse Tensor Decomposition](https://arxiv.org/abs/2403.07953) | 本文提出了通过结构化分解张量进一步抽象稀疏DNN加速的方法，实现了将稀疏张量转换成一系列结构化稀疏张量，从而弥合了稀疏DNN模型和硬件之间的差距。 |
| [^125] | [SAMDA: Leveraging SAM on Few-Shot Domain Adaptation for Electronic Microscopy Segmentation](https://arxiv.org/abs/2403.07951) | SAMDA框架结合了SAM和nnUNet，通过融合“专家”和“通用”组件，在少样本领域自适应中解决了大规模视觉基础模型面临的转移性和精度问题 |
| [^126] | [The evaluation of a code-switched Sepedi-English automatic speech recognition system](https://arxiv.org/abs/2403.07947) | 评估了一种混合使用Sepedi-英语的自动语音识别系统，探讨了端到端方法在低资源语言中的有效性。 |
| [^127] | [A Mathematical Framework for the Problem of Security for Cognition in Neurotechnology](https://arxiv.org/abs/2403.07945) | 本文提出了一个数学框架，名为认知安全，用于描述和分析神经技术对个体认知隐私和自治可能产生的影响，解决了相关问题描述和分析的障碍。 |
| [^128] | [Revisiting Edge Perturbation for Graph Neural Network in Graph Data Augmentation and Attack](https://arxiv.org/abs/2403.07943) | 本文提出了一种统一公式，明确界定了两类边扰动方法之间的清晰界限，并解答了为何边扰动具有双重效果以及何使边扰动灵活有效的问题。 |
| [^129] | [Hair and scalp disease detection using deep learning](https://arxiv.org/abs/2403.07940) | 这项研究提出了一种使用深度学习技术检测头发和头皮疾病的方法，通过卷积神经网络分析图像，实现了皮肤病变的早期检测和诊断。 |
| [^130] | [Text-to-Audio Generation Synchronized with Videos](https://arxiv.org/abs/2403.07938) | 介绍了一个新的与视频对齐的文本到音频生成基准T2AV-Bench，以及一种简单而有效的视频对齐TTA生成模型T2AV，改进了传统方法，并融合了视觉对齐文本嵌入。 |
| [^131] | [Speech Robust Bench: A Robustness Benchmark For Speech Recognition](https://arxiv.org/abs/2403.07937) | 提出了一个全面基准（SRB），用于评估自动语音识别（ASR）模型对各种破坏的鲁棒性，发现模型大小和某些建模选择有助于提高鲁棒性，并观察到在不同人口亚组上模型的鲁棒性存在明显差异。 |
| [^132] | [Corruption-Robust Offline Two-Player Zero-Sum Markov Games](https://arxiv.org/abs/2403.07933) | 在离线两人零和马尔科夫博弈中，我们提出了抗数据损坏的学习算法以识别近似纳什均衡策略对，并通过鲁棒版本的极小极大值迭代算法实现了（近）最优结果。 |
| [^133] | [Sketching the Heat Kernel: Using Gaussian Processes to Embed Data](https://arxiv.org/abs/2403.07929) | 通过高斯过程实现数据的低维嵌入，将热核作为协方差函数进行计算，拟合出嵌入中的直线距离以概率方式近似扩散距离，保留了一些较小尺度结构，同时具有对异常值的更强鲁棒性 |
| [^134] | [Intelligent Monitoring Framework for Cloud Services: A Data-Driven Approach](https://arxiv.org/abs/2403.07927) | 提出一种智能监控框架，根据服务属性为云服务推荐监控器，通过挖掘监视器属性并建立结构化本体论，解决了当前监控创建过程中的不完整覆盖和冗余问题。 |
| [^135] | [Value Prediction for Spatiotemporal Gait Data Using Deep Learning](https://arxiv.org/abs/2403.07926) | 该研究将深度学习应用于时空步态数据的数值预测，探索了不同的深度学习架构，并在短距离和长距离预测方面取得了成功 |
| [^136] | [Physics-informed generative model for drug-like molecule conformers](https://arxiv.org/abs/2403.07925) | 该模型基于扩散生成，结合深度学习技术从大型数据集中推断原子类型和几何参数，实现了类药分子构象的高精度生成，优于传统方法。 |
| [^137] | [The Fusion of Deep Reinforcement Learning and Edge Computing for Real-time Monitoring and Control Optimization in IoT Environments](https://arxiv.org/abs/2403.07923) | 本文提出了一种基于深度强化学习和边缘计算的优化控制系统，通过云边协同和动态资源分配实现工业目标的监控和优化，显著提升了系统性能，并节省了成本。 |
| [^138] | [Merino: Entropy-driven Design for Generative Language Models on IoT Devices](https://arxiv.org/abs/2403.07921) | 在本文中，我们提出了一个新颖的信息熵框架，用于设计手机友好的生成式语言模型，通过最大化transformer解码器的熵来在计算预算内，成功设计了MeRino模型，在移动设置下展现出与当前最先进的自回归transformer模型竞争性能的特点 |
| [^139] | [ProtLLM: An Interleaved Protein-Language LLM with Protein-as-Word Pre-Training](https://arxiv.org/abs/2403.07920) | 提出了ProtLLM，一种具有独特动态蛋白质装配机制及蛋白质作为单词语言建模方法的交织式蛋白质-语言LLM，并构建了大规模的交织式蛋白质-文本数据集用于预训练。 |
| [^140] | [On the Societal Impact of Open Foundation Models](https://arxiv.org/abs/2403.07918) | 开放基金会模型具有广泛可用的模型权重，带来了重大利益，但也存在边际风险，需要进一步研究来评估其相对于现有技术的安全性。 |
| [^141] | [A Neural-Evolutionary Algorithm for Autonomous Transit Network Design](https://arxiv.org/abs/2403.07917) | 提出了一种神经进化算法用于自动公交网络设计，该算法通过训练图神经网络模型作为策略，并将其用作进化算法中的变异操作符，在公交网络设计基准集上优于单独学习策略和简单进化算法方法。 |
| [^142] | [Advancing Investment Frontiers: Industry-grade Deep Reinforcement Learning for Portfolio Optimization](https://arxiv.org/abs/2403.07916) | 本研究将深度强化学习应用于投资组合优化中，融合了产业级方法和量化金融，提出了结合多领域方法的独特视角。 |
| [^143] | [Enhancing Kubernetes Automated Scheduling with Deep Learning and Reinforcement Techniques for Large-Scale Cloud Computing Optimization](https://arxiv.org/abs/2403.07905) | 本文提出了一种基于深度学习和强化学习的自动任务调度方案，旨在实现大规模云计算系统中任务调度的最优利用和最大执行效率。 |
| [^144] | [Addressing the Regulatory Gap: Moving Towards an EU AI Audit Ecosystem Beyond the AIA by Including Civil Society](https://arxiv.org/abs/2403.07904) | 提出了一个融合合规和监督的AI审计生态系统，强调了DSA和AIA监管框架中存在的监管空白，并要求AIA为研究人员和社会公民提供数据和模型访问权限 |
| [^145] | [Multiple Access in the Era of Distributed Computing and Edge Intelligence](https://arxiv.org/abs/2403.07903) | 本文关注多址接入技术下一代（NGMA）的最新研究和创新，以及其与边缘计算、网络切片、空中计算、语义通信和机器学习等关键技术的相互关系。 |
| [^146] | [DecompDiff: Diffusion Models with Decomposed Priors for Structure-Based Drug Design](https://arxiv.org/abs/2403.07902) | 本文提出了DecompDiff模型，通过将配体分子分解为臂和支架，并引入分解先验，结合键扩散和有效性指导，实现了生成高亲和力分子并保持分子性质的最先进性能。 |
| [^147] | [MIP: CLIP-based Image Reconstruction from PEFT Gradients](https://arxiv.org/abs/2403.07901) | 本文分析了基于CLIP的联邦学习中PEFT的梯度仍可用于进行图像重构攻击，并提出了针对CLIP的重构攻击方法MIP。 |
| [^148] | [Change Point Detection with Copula Entropy based Two-Sample Test](https://arxiv.org/abs/2403.07892) | 本文提出了一种基于Copula熵的双样本检验的非参数多元方法，用于多个变点检测，并结合了二叉分割策略，有效验证了其在不同数据集上的比较优势。 |
| [^149] | [Digital Video Manipulation Detection Technique Based on Compression Algorithms](https://arxiv.org/abs/2403.07891) | 该论文提出了一种基于H.264编码压缩算法的数字视频篡改检测技术，通过分析宏块信息和运动矢量，利用矢量支持机构建立模型，成功实现对视频重新压缩的准确检测。 |
| [^150] | [$\widetilde{O}(T^{-1})$ Convergence to (Coarse) Correlated Equilibria in Full-Information General-Sum Markov Games](https://arxiv.org/abs/2403.07890) | 本研究通过使用乐观的前瞻性领导者算法（OFTRL）和适当的数值更新程序，在全信息一般和马尔可夫博弈中找到了$\widetilde{O}(T^{-1})$-approximate（粗糙）相关均衡，这在$T$次迭代内得以实现。 |
| [^151] | [Equipping Computational Pathology Systems with Artifact Processing Pipelines: A Showcase for Computation and Performance Trade-offs](https://arxiv.org/abs/2403.07743) | 提出了一种专家混合方案，用于在计算病理学系统中检测和排除五种显著的工件，并应用概率阈值处理。 |
| [^152] | [Online Continual Learning For Interactive Instruction Following Agents](https://arxiv.org/abs/2403.07548) | 我们提出了针对具身代理的两种持续学习设置：学习新行为和新环境。同时，我们通过自信度得分来更新存储的信息，从而避免需要任务边界信息的问题。 |
| [^153] | [Graph Unlearning with Efficient Partial Retraining](https://arxiv.org/abs/2403.07353) | 提出了一种新颖的图去除框架GraphRevoker，通过图属性感知划分和图对比子模型聚合，更好地保持了不可训练GNNs的模型效用。 |
| [^154] | [HiRA-Pro: High resolution alignment of multimodal spatio-temporal data: a process physics driven approach](https://arxiv.org/abs/2403.06888) | HiRA-Pro是一个高分辨率对齐多模态时空数据的过程物理驱动方法，成功解决了对齐具有亚毫秒现象的数据的挑战，并在智能制造环境中取得了成功应用。 |
| [^155] | [Constructing Variables Using Classifiers as an Aid to Regression: An Empirical Assessment](https://arxiv.org/abs/2403.06829) | 提出了一种方法，利用分类器预测变量的离散值并将其作为附加变量用于丰富回归问题的初始向量，经实验证实了该方法的有效性。 |
| [^156] | [Koopman Ensembles for Probabilistic Time Series Forecasting](https://arxiv.org/abs/2403.06757) | 研究提出了一种Koopman合奏方法，通过训练模型合奏产生具有高模型间方差的预测，从而改善集成模型的不确定性量化。 |
| [^157] | [The ALL0CORE Tensor Decomposition for Sparse Count Data](https://arxiv.org/abs/2403.06153) | ALL0CORE是一种新的概率非负张量分解方法，它在保持计算可处理性的基础上利用Tucker分解的潜在结构，可以仅使用核的微小部分即达到与完整Tucker分解相同效果。 |
| [^158] | [Towards a Generic Representation of Cominatorial Problems for Learning-Based Approaches](https://arxiv.org/abs/2403.06026) | 本文倡导为基于学习方法的组合问题构建通用表示，以解决特定表示无法跨越不同组合问题的问题。 |
| [^159] | [Shielded Deep Reinforcement Learning for Complex Spacecraft Tasking](https://arxiv.org/abs/2403.05693) | 本文基于线性时态逻辑（LTL）形式化了航天器任务和安全要求，并提出了自动构建奖励函数以实现有效训练的方法。同时探讨了从安全LTL规范构建航天器屏蔽的方法，提出了三种可以提供概率保证的设计，并通过实验展示了这些屏蔽与不同策略的互动和奖励结构的灵活性。 |
| [^160] | [The Computational Complexity of Learning Gaussian Single-Index Models](https://arxiv.org/abs/2403.05529) | 该论文研究了学习高斯单指数模型的计算复杂性，在高维回归问题中展示了计算有效算法所需的样本复杂度，并表明这种复杂度是充分的。 |
| [^161] | [The R2D2 deep neural network series paradigm for fast precision imaging in radio astronomy](https://arxiv.org/abs/2403.05452) | 提出一种新颖的深度学习方法R2D2，用于解决射电天文学中高分辨率高动态范围成像的可扩展性挑战。 |
| [^162] | [Electrocardiogram Instruction Tuning for Report Generation](https://arxiv.org/abs/2403.04945) | 提出了Multimodal ECG Instruction Tuning（MEIT）框架，首次尝试使用LLMs和多模态指导解决ECG报告生成问题，并在两个大规模ECG数据集上进行了广泛的实验评估其优越性。 |
| [^163] | [A Survey of Lottery Ticket Hypothesis](https://arxiv.org/abs/2403.04861) | 大乐透假设指出神经网络中存在稀疏子网络，训练孤立子网络可以获得更好性能，调查综述了LTH现状并提出未来研究方向 |
| [^164] | [Dynamic Cross Attention for Audio-Visual Person Verification](https://arxiv.org/abs/2403.04661) | 提出了一种动态交叉注意力（DCA）模型，根据音频和视频模态之间的强弱互补关系，动态选择交叉关注或不关注的特征。 |
| [^165] | [SWAP-NAS: Sample-Wise Activation Patterns For Ultra-Fast NAS](https://arxiv.org/abs/2403.04161) | 提出了一种新颖的高性能无需训练的度量SWAP-Score，能够在不同搜索空间和任务中测量网络在一批输入样本上的表现能力，并通过正则化进一步提高相关性，实现模型大小的控制。 |
| [^166] | [Non-verbal information in spontaneous speech - towards a new framework of analysis](https://arxiv.org/abs/2403.03522) | 这项研究提出了一个分析框架和技术验证概念，用于对言语中的非言语信号进行分类，并将其与含义关联起来，从而为探索表达实现多层韵律事件的大型数据提供了一种方法。 |
| [^167] | [Training Machine Learning models at the Edge: A Survey](https://arxiv.org/abs/2403.02619) | 这项调研深入探讨了边缘学习(EL)中优化机器学习模型训练的各种方法和方法论，旨在综合现有知识，识别挑战，并突出未来趋势。 |
| [^168] | [Classes Are Not Equal: An Empirical Study on Image Recognition Fairness](https://arxiv.org/abs/2402.18133) | 图像分类模型中存在类准确率差异导致的不公平现象，主要是因为有问题的表示方式导致模型对更具挑战性的类别表现出更大的预测偏差。 |
| [^169] | [DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning](https://arxiv.org/abs/2402.17453) | DS-Agent是一个自动框架，结合了大型语言模型代理和案例推理，能够在数据科学任务中灵活利用专家知识并通过反馈机制持续改善性能 |
| [^170] | [Neural Implicit Swept Volume Models for Fast Collision Detection](https://arxiv.org/abs/2402.15281) | 提出了一种新颖的神经隐式扫描体模型，能够连续表示任意运动，并结合了深度学习速度和几何碰撞检查的准确性保证。 |
| [^171] | [Stable Neural Stochastic Differential Equations in Analyzing Irregular Time Series Data](https://arxiv.org/abs/2402.14989) | 神经常微分方程（Neural ODEs）的扩展——神经随机微分方程（Neural SDEs）在处理不规则时间序列数据中的稳定性和性能方面提出了重要指导，需要谨慎设计漂移和扩散函数以保持稳定性。 |
| [^172] | [UR2M: Uncertainty and Resource-Aware Event Detection on Microcontrollers](https://arxiv.org/abs/2402.09264) | UR2M是一个新颖的不确定性和资源感知的事件检测框架，针对微控制器上的应用，通过评估模型输出的可靠性来解决传统机器学习技术在数据分布变化时产生不准确预测的问题。 |
| [^173] | [Agricultural Recommendation System based on Deep Learning: A Multivariate Weather Forecasting Approach](https://arxiv.org/abs/2401.11410) | 提出了一种基于深度学习和天气预测的农业推荐系统，旨在解决孟加拉国农业面临的天气不利因素对粮食生产的影响，以实现盈利、可持续和农民友好的农业实践。 |
| [^174] | [Learning Human-like Representations to Enable Learning Human Values](https://arxiv.org/abs/2312.14106) | 通过学习类人的表示，可以实现机器学习系统符合人类价值观，支持伦理等多方面的价值对齐。 |
| [^175] | [SkillDiffuser: Interpretable Hierarchical Planning via Skill Abstractions in Diffusion-Based Task Execution](https://arxiv.org/abs/2312.11598) | SkillDiffuser通过将可解释的技能学习与条件扩散规划相结合，实现了在高层指令下生成连贯轨迹的分层规划。 |
| [^176] | [Imitate the Good and Avoid the Bad: An Incremental Approach to Safe Reinforcement Learning](https://arxiv.org/abs/2312.10385) | 提出了一种不修改基于轨迹成本约束的方法，在安全强化学习中通过模仿好的轨迹和避免坏的轨迹来改进策略。 |
| [^177] | [Continual Adversarial Defense](https://arxiv.org/abs/2312.09481) | 提出了第一个能够动态适应任何攻击的持续对抗性防御（CAD）框架。 |
| [^178] | [A Hitchhiker's Guide to Geometric GNNs for 3D Atomic Systems](https://arxiv.org/abs/2312.07511) | 几何图神经网络在3D原子系统中以利用物理对称性和化学性质等归纳偏差来学习几何图信息表示而著称。 |
| [^179] | [TimeDRL: Disentangled Representation Learning for Multivariate Time-Series](https://arxiv.org/abs/2312.04142) | TimeDRL是一个具有解缠双层嵌入的通用多变量时间序列表示学习框架，通过时间戳级别和实例级别的嵌入之间的解缠派生以及时间戳-预测和实例-对比任务的利用，实现了学习丰富表示并解决归纳偏差的目标。 |
| [^180] | [Domain constraints improve risk prediction when outcome data is missing](https://arxiv.org/abs/2312.03878) | 提出一种贝叶斯模型类，通过领域约束的设置来改善测试和未测试患者的风险预测 |
| [^181] | [Dr. Jekyll and Mr. Hyde: Two Faces of LLMs](https://arxiv.org/abs/2312.03853) | 本研究通过让ChatGPT和Bard冒充复杂人物角色，绕过了安全机制和专门训练程序，展示了被禁止的回应实际上被提供了，从而有可能获取未经授权、非法或有害的信息。 |
| [^182] | [Inverse Design of Vitrimeric Polymers by Molecular Dynamics and Generative Modeling](https://arxiv.org/abs/2312.03690) | 通过MD模拟和机器学习，提出了一种逆设计方法，利用VAE模型生成新型Vitrimer并根据所需Tg指导设计。 |
| [^183] | [Jellyfish: A Large Language Model for Data Preprocessing](https://arxiv.org/abs/2312.01678) | 这项研究探讨了在数据挖掘中利用大型语言模型进行数据预处理的方法，通过指导调整本地LLMs来解决通用数据预处理问题，确保数据安全并进行进一步调整 |
| [^184] | [MLLMs-Augmented Visual-Language Representation Learning](https://arxiv.org/abs/2311.18765) | MLLMs通过为图像-文本数据集建立更丰富的图像-文本关联，以增强视觉-语言表示学习，并通过“文本剪切”方法来避免偏见引入，显著提高了图像-文本检索的性能。 |
| [^185] | [DPOD: Domain-Specific Prompt Tuning for Multimodal Fake News Detection](https://arxiv.org/abs/2311.16496) | 本研究提出了一种名为DPOD的框架，通过利用跨领域数据来改善所需领域的脱离上下文误信息检测，解决数据不平衡的问题。 |
| [^186] | [Effective Structural Encodings via Local Curvature Profiles](https://arxiv.org/abs/2311.14864) | 本文从几何的角度研究了哪种结构属性产生最有效的编码，提出了一种基于离散Ricci曲率的新型结构编码（局部曲率配置，LCP），证明其明显优于现有编码方法，并表明将局部结构编码与全局位置编码相结合可以提升下游性能，捕捉到互补的几何信息。 |
| [^187] | [Safety-aware Causal Representation for Trustworthy Offline Reinforcement Learning in Autonomous Driving](https://arxiv.org/abs/2311.10747) | 本文提出了一种面向安全的因果表示方法FUSION，在自动驾驶中的离线强化学习中利用结构化情景信息促进泛化的端到端驾驶策略学习。 |
| [^188] | [Agent Lumos: Unified and Modular Training for Open-Source Language Agents](https://arxiv.org/abs/2311.05657) | Agent Lumos提出了一种统一和模块化的框架，通过规划模块学习高级子目标生成，训练接地模块将其转化为动作，促进广泛互动任务应用。 |
| [^189] | [Making RL with Preference-based Feedback Efficient via Randomization](https://arxiv.org/abs/2310.14554) | 在基于偏好反馈的强化学习中，通过引入随机化设计的算法在线性MDP模型下表现出样本高效性和多项式运行时间，并通过随机化主动学习过程最小化了查询复杂性。 |
| [^190] | [Demystifying Embedding Spaces using Large Language Models](https://arxiv.org/abs/2310.04475) | 通过使用大型语言模型（LLMs）直接与嵌入交互，将抽象向量转换为可理解的叙述，使得复杂嵌入数据更具解释性和广泛实用性。 |
| [^191] | [Quantifying the Plausibility of Context Reliance in Neural Machine Translation](https://arxiv.org/abs/2310.01188) | 引入了PECoRe框架，用于量化语言模型生成中的上下文使用情况，从而评估上下文感知机器翻译模型的可信度。 |
| [^192] | [Linear attention is (maybe) all you need (to understand transformer optimization)](https://arxiv.org/abs/2310.01082) | 研究者通过训练线性Transformer模型解决回归任务，发现这种简单的线性化模型能够重现Transformer训练动态的多个关键方面，表明线性注意力可能是理解Transformer优化的关键。 |
| [^193] | [CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets](https://arxiv.org/abs/2309.17428) | CRAFT提出了一个通用工具创建和检索框架，能够定制LLMs，为其创建特定任务的工具集，并使用这些工具集增强其解决复杂任务的能力。 |
| [^194] | [Adaptive Sharpness-Aware Pruning for Robust Sparse Networks](https://arxiv.org/abs/2306.14306) | AdaSAP方法通过网络锐度的视角统一了稳健性和紧凑性的目标，并通过策略性地引入权重扰动来使稀疏网络对训练时未见的输入变化稳健，在图像分类和目标检测任务上取得显著改进。 |
| [^195] | [Randomized Kaczmarz in Adversarial Distributed Setting](https://arxiv.org/abs/2302.14615) | 提出了一种在对抗性分布设置中具有对抗鲁棒性的迭代方法，通过简单的统计信息保证收敛，能够适应对抗性分布，在模拟中展示了解决凸问题的高效性，具有高准确性识别对手工作者和容忍不同对手率的能力。 |
| [^196] | [Explainable Anomaly Detection in Images and Videos: A Survey](https://arxiv.org/abs/2302.06670) | 这项研究提供了针对图像和视频的可解释异常检测方法的首次调研，为机器学习学术界和实际应用提供了重要参考。 |
| [^197] | [Curriculum Graph Machine Learning: A Survey](https://arxiv.org/abs/2302.02926) | 本文综述了课程图机器学习的方法，介绍了解决现有图机器学习模型性能不佳问题的关键挑战和最新进展。 |
| [^198] | [Lowering Detection in Sport Climbing Based on Orientation of the Sensor Enhanced Quickdraw](https://arxiv.org/abs/2301.10164) | 通过在攀岩快挂上安装的加速度传感器采集数据，实现了在攀岩活动中检测攀岩者下降情况的技术，保护攀岩者隐私和健身房成本的同时提高了效率和便利性。 |
| [^199] | [Adaptive proximal algorithms for convex optimization under local Lipschitz continuity of the gradient](https://arxiv.org/abs/2301.04431) | 在此工作中，我们提出了一种自适应贴近梯度方法，通过使用新的局部平滑度模量的估计，可以在凸优化中避免使用回溯线搜索，并根据局部平滑度估计自适应调整步长。 |
| [^200] | [Unsupervised Acoustic Scene Mapping Based on Acoustic Features and Dimensionality Reduction](https://arxiv.org/abs/2301.00448) | 提出了一种基于声学特征和降维的无监督声学场景映射方法，利用局部共形自动编码器（LOCA）学习声学数据的标准化坐标，能够较好地处理混响和加性噪声，并通过实验和模拟验证了其性能。 |
| [^201] | [Can Direct Latent Model Learning Solve Linear Quadratic Gaussian Control?](https://arxiv.org/abs/2212.14511) | 该论文提出了直接潜在模型学习的方法，用于解决线性二次高斯控制问题，能够在有限样本下找到近似最优状态表示函数和控制器。 |
| [^202] | [TILDE-Q: A Transformation Invariant Loss Function for Time-Series Forecasting](https://arxiv.org/abs/2210.15050) | 本文提出了一种对时间序列预测中复杂时间模式进行建模的变换不变损失函数TILDE-Q，解决了当前模型在捕获信号形状和模拟细微时间动态上的挑战。 |
| [^203] | [Better Uncertainty Calibration via Proper Scores for Classification and Beyond](https://arxiv.org/abs/2203.07835) | 介绍了正确校准误差框架，通过将每个校准误差与正确得分相关联，提供了最佳估计特性的上界，可靠量化模型校准改进。 |
| [^204] | [Terminal Embeddings in Sublinear Time](https://arxiv.org/abs/2110.08691) | 该论文提出了一种亚线性时间下的终端嵌入方法，可以实现扭曲$1+\epsilon$，相较于传统方法更加通用且具有广泛应用性。 |
| [^205] | [Fast Dual-Regularized Autoencoder for Sparse Biological Data.](http://arxiv.org/abs/2401.16664) | 本文提出了一种快速双正则自编码器用于稀疏生物数据的问题。该方法相对于现有最先进方法在预测药物靶点相互作用和药物疾病关联方面具有速度和准确性的优势。 |
| [^206] | [Efficient Observation Time Window Segmentation for Administrative Data Machine Learning.](http://arxiv.org/abs/2401.16537) | 本文研究了如何将机器学习模型的观察窗口划分为时间段，通过优化高优先级特征的时间bin大小，可以实现更简单、更快速训练的机器学习模型，并且能够达到与更复杂模型相似甚至更好的性能。 |
| [^207] | [Neural Network-Based Score Estimation in Diffusion Models: Optimization and Generalization.](http://arxiv.org/abs/2401.15604) | 本文提出了基于神经网络的扩散模型中分数估计的优化和泛化方法，并建立了对分数估计进行分析的数学框架。 |
| [^208] | [Tight Group-Level DP Guarantees for DP-SGD with Sampling via Mixture of Gaussians Mechanisms.](http://arxiv.org/abs/2401.10294) | 本研究提供了一种计算DP-SGD组级别限制的方法，并且证明了这个方法在使用泊松抽样或固定批量大小抽样时是紧密的。 |
| [^209] | [Bridging State and History Representations: Understanding Self-Predictive RL.](http://arxiv.org/abs/2401.08898) | 本论文研究了深度强化学习中状态和历史表示间的关系，发现了这些方法和框架实际上都基于自预测抽象的共同思想，并提供了理论洞见和简化算法来学习自预测表示。 |
| [^210] | [Tissue Artifact Segmentation and Severity Analysis for Automated Diagnosis Using Whole Slide Images.](http://arxiv.org/abs/2401.01386) | 这篇论文提出了一种使用全幻灯切片图像进行组织伪影分割与严重性分析的自动诊断方法。通过计算机视觉和人工智能，可以在没有人类监督的情况下对整个全幻灯切片图像进行自主分析，但受到组织伪影影响的区域需要被准确识别和排除。 |
| [^211] | [Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning.](http://arxiv.org/abs/2312.05720) | 本文引入了一种创新的方法，在联邦学习中利用语言模型的池化层输入来实现对隐私攻击的改进。通过恢复池化层输入，这种方法能够在不同的批处理大小下提供更高的文本恢复率，从而提供更细致和有效的见解。 |
| [^212] | [Improved Regret Bounds of (Multinomial) Logistic Bandits via Regret-to-Confidence-Set Conversion.](http://arxiv.org/abs/2310.18554) | 本论文通过遗憾到置信集转换方法改进了逻辑回归赌博机的遗憾界限，提出了一个基于在线学习算法的凸置信集，并应用于具有新的鞅集中步骤的遗憾分析。 |
| [^213] | [A Quasi-Wasserstein Loss for Learning Graph Neural Networks.](http://arxiv.org/abs/2310.11762) | 这篇论文提出了一种新的准瓦狄斯坦损失函数，通过利用图上的最优传输来学习图神经网络，消除了现有损失函数在节点级别预测中可能存在的不一致性。 |
| [^214] | [Generative Entropic Neural Optimal Transport To Map Within and Across Spaces.](http://arxiv.org/abs/2310.09254) | 该论文介绍了生成熵神经最优传输在测度到测度映射中的应用，解决了处理非平方欧氏距离成本、确定性蒙格映射、映射跨不可比较空间和质量守恒约束等实际挑战。 |
| [^215] | [Search-Adaptor: Text Embedding Customization for Information Retrieval.](http://arxiv.org/abs/2310.08750) | 本文提出了一种名为Search-Adaptor的方法，用于定制化预训练的大型语言模型以改善信息检索和搜索的性能。通过修改文本嵌入，Search-Adaptor在多个真实世界数据集上展现出了稳定且显著的性能提升。 |
| [^216] | [GenTKG: Generative Forecasting on Temporal Knowledge Graph.](http://arxiv.org/abs/2310.07793) | 研究提出了一种名为GenTKG的生成模型，用于在时间知识图谱上进行预测。该模型通过结合基于时间逻辑规则的检索策略和轻量级的参数效率指导，克服了复杂的时间图数据结构和庞大的数据量所带来的挑战。 |
| [^217] | [CacheGen: Fast Context Loading for Language Model Applications.](http://arxiv.org/abs/2310.07240) | CacheGen是一种用于语言模型应用的技术，通过对上下文进行压缩来减少LLM的网络获取和处理延迟。 |
| [^218] | [C(NN)FD -- deep learning predictions of tip clearance variations on multi-stage axial compressors aerodynamic performance.](http://arxiv.org/abs/2310.04264) | 本文展示了一种用于实时预测多级轴向压缩机在燃气轮机中尖间隙变化对气动性能影响的深度学习框架，可与CFD基准相媲美的实时准确性，方便集成到燃气轮机的制造和构建过程中进行性能评估。 |
| [^219] | [CoLiDE: Concomitant Linear DAG Estimation.](http://arxiv.org/abs/2310.02895) | 本论文提出了CoLiDE算法用于学习线性DAG，该算法使用了一个新的凸评分函数，结合了标度的共同估计，从而有效地将稀疏参数与外生噪声水平分离。 |
| [^220] | [Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion.](http://arxiv.org/abs/2310.02279) | 提出了一种一致性轨迹模型（CTM），它可以加速扩散模型的采样，同时通过对抗训练和去噪得分匹配损失的组合来提高性能，并实现了最先进的采样质量。 |
| [^221] | [SmartPlay : A Benchmark for LLMs as Intelligent Agents.](http://arxiv.org/abs/2310.01557) | SmartPlay是一个用于评估LLMs作为智能Agent能力的基准，包括6个具有不同挑战的游戏，并测试了智能LLM Agent的多种关键能力。这不仅是一个评估LLM Agent整体性能的严格测试场地，还可以分析每个能力的表现。 |
| [^222] | [A path-norm toolkit for modern networks: consequences, promises and challenges.](http://arxiv.org/abs/2310.01225) | 本文介绍了适用于现代神经网络的路径范数工具包，可以包括具有偏差、跳跃连接和最大池化的通用DAG ReLU网络。这个工具包恢复或超越了已知的路径范数界限，并挑战了基于路径范数的一些具体承诺。 |
| [^223] | [DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models.](http://arxiv.org/abs/2310.00902) | DataInf是一种高效的影响力近似方法，特别适用于大规模生成型AI模型，相比现有方法在计算和内存效率上有明显优势。 |
| [^224] | [Certified Robustness via Dynamic Margin Maximization and Improved Lipschitz Regularization.](http://arxiv.org/abs/2310.00116) | 本文提出了一种基于动态边界最大化和改进的Lipschitz正则化的认证鲁棒性训练算法，通过增加输出空间中的边界和正则化模型的Lipschitz常数来提高深度分类器对抗性扰动的鲁棒性。 |
| [^225] | [Resisting Backdoor Attacks in Federated Learning via Bidirectional Elections and Individual Perspective.](http://arxiv.org/abs/2309.16456) | 本文提出了Snowball，一个通过个体视角上的双向选举方法来抵抗联邦学习中后门攻击的框架。它通过自下而上和自上而下的选举过程，逐步排除感染模型，以解决由于本地数据分布多样性导致模型更新混杂分散的问题。 |
| [^226] | [Class Incremental Learning via Likelihood Ratio Based Task Prediction.](http://arxiv.org/abs/2309.15048) | 该论文提出了一种基于似然比的任务预测的类增量学习方法，利用离群检测器进行任务标识预测，解决了无任务标识符的测试样本的任务预测问题。 |
| [^227] | [Promises of Deep Kernel Learning for Control Synthesis.](http://arxiv.org/abs/2309.06569) | 深度核学习（DKL）结合了神经网络的表示能力与高斯过程的不确定性量化，为复杂动态系统的学习和控制合成提供了潜力。本研究提出了一个基于抽象的框架，使用DKL与区间马尔可夫决策过程（IMDP）实现对随机动态系统的控制合成，同时采用了高效的深度架构和正确性保证的方法。 |
| [^228] | [MASA-TCN: Multi-anchor Space-aware Temporal Convolutional Neural Networks for Continuous and Discrete EEG Emotion Recognition.](http://arxiv.org/abs/2308.16207) | MASA-TCN是一种用于连续和离散EEG情绪识别的多锚点空间感知时间卷积神经网络模型。该模型通过引入空间感知时间层来提取EEG空间模式，并能在情绪回归和分类任务中取得更好的性能。 |
| [^229] | [Dual Branch Deep Learning Network for Detection and Stage Grading of Diabetic Retinopathy.](http://arxiv.org/abs/2308.09945) | 这项研究介绍了一种双支路深度学习网络用于检测和分级糖尿病视网膜病变，通过利用单个眼底视网膜图像进行早期诊断和成功治疗。所提出的模型利用迁移学习和预训练模型，在大型多中心数据集上进行了训练，取得了卓越的性能，优于已有文献。 |
| [^230] | [Multi-Task Pseudo-Label Learning for Non-Intrusive Speech Quality Assessment Model.](http://arxiv.org/abs/2308.09262) | 这篇论文介绍了一种基于多任务伪标签学习的非侵入式语音质量评估模型（MTQ-Net），通过与预训练模型结合使用伪标签来进行训练，实验结果证明了其相对于从头训练模型的优势。 |
| [^231] | [PTransIPs: Identification of phosphorylation sites based on protein pretrained language model and Transformer.](http://arxiv.org/abs/2308.05115) | PTransIPs是一种新型深度学习模型，它将蛋白质序列中的氨基酸视为自然语言中的单词，并结合大型预训练蛋白质模型的嵌入。该模型通过结合卷积神经网络和Transformer模型进行训练，用于识别磷酸化位点。 |
| [^232] | [Learning to Generate Training Datasets for Robust Semantic Segmentation.](http://arxiv.org/abs/2308.02535) | 本文提出了一种新的方法，通过生成真实和可信的扰动或异常图像来提高语义分割技术的鲁棒性。通过设计和训练Robusta，一种鲁棒的条件生成对抗网络，可以为训练可靠的分割模型提供可用的数据集，从而显著增强语义分割技术在面对现实世界的扰动和分布变化时的鲁棒性。 |
| [^233] | [In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning.](http://arxiv.org/abs/2307.12375) | 大型语言模型（LLMs）在包含标签关系示例的上下文中的学习能力使其在下游任务中表现显著提高，但与传统学习方法不同。我们研究了上下文示例中的标签如何影响预测、预训练中学习到的标签关系如何与上下文示例相互作用以及上下文学习如何聚合标签信息。研究结果揭示了LLMs的工作机制及其对上下文信息的处理方式。 |
| [^234] | [UniTabE: Pretraining a Unified Tabular Encoder for Heterogeneous Tabular Data.](http://arxiv.org/abs/2307.09249) | UniTabE是一种面向异构表格数据的统一预训练表格编码器，能够处理不同表格结构的挑战，并具有对多样化下游应用的适应性。 |
| [^235] | [Kernel-Based Testing for Single-Cell Differential Analysis.](http://arxiv.org/abs/2307.08509) | 本论文提出了一种基于核方法的单细胞差异分析测试框架，可以非线性比较复杂的细胞间分子特征分布。通过利用核嵌入的变异性，我们的方法能够揭示细胞群体中隐蔽的异质性。我们展示了核测试如何克服单细胞差异分析方法的局限性，并应用于研究分化逆转的过程。 |
| [^236] | [VertiBench: Advancing Feature Distribution Diversity in Vertical Federated Learning Benchmarks.](http://arxiv.org/abs/2307.02040) | 本文引入了两个影响VFL性能的关键因素：特征重要性和特征相关性，并提出了相关的评估指标和数据集划分方法。同时，通过引入真实的VFL数据集，填补了图像-图像VFL情景中的不足。研究对于未来的VFL研究提供了有价值的见解。 |
| [^237] | [Monte Carlo Sampling without Isoperimetry: A Reverse Diffusion Approach.](http://arxiv.org/abs/2307.02037) | 本研究提出了一种无等渗性的蒙特卡洛采样方法，通过逆扩散过程实现了新颖的后验采样算法，在高维采样中表现出更优越的性能。 |
| [^238] | [Probabilistic Constraint for Safety-Critical Reinforcement Learning.](http://arxiv.org/abs/2306.17279) | 本文研究了概率约束下的安全关键强化学习问题，提出了具有明确梯度表达式的Safe Policy Gradient-REINFORCE（SPG-REINFORCE）算法，并通过理论界限证明了概率约束设置在最优性和安全性之间具有更好的权衡。 |
| [^239] | [Identifiability of Discretized Latent Coordinate Systems via Density Landmarks Detection.](http://arxiv.org/abs/2306.16334) | 本文提出了一种新颖的可识别性形式，称为量化坐标可识别性。在无监督的情况下，我们展示了在高度通用的非线性映射下，可以恢复离散化的潜在坐标，而无需额外的归纳偏差。这一发现对解缠研究具有重要意义。 |
| [^240] | [Re-aligning Shadow Models can Improve White-box Membership Inference Attacks.](http://arxiv.org/abs/2306.05093) | 系统分析了影子模型不对齐问题的原因，并通过对抗性训练或实例加权方法重新对齐影子模型，从而提高了白盒成员隐私攻击的效果。 |
| [^241] | [Learning Closed-form Equations for Subgrid-scale Closures from High-fidelity Data: Promises and Challenges.](http://arxiv.org/abs/2306.05014) | 本文发现了非线性梯度模型（NGM），它是可解析地使用Taylor级数拓展导出的闭合形式，从而实现对地球系统复杂过程的子网格尺度（SGS）闭合/参数化。 |
| [^242] | [Align, Distill, and Augment Everything All at Once for Imbalanced Semi-Supervised Learning.](http://arxiv.org/abs/2306.04621) | 本文针对长尾半监督学习中类别不平衡的问题，提出了三个解决方案：一种灵活的分布对齐方法，一种软一致性正则化方法和一种扩充未标记集的方案。 |
| [^243] | [Explainability in Simplicial Map Neural Networks.](http://arxiv.org/abs/2306.00010) | 本文提出了简单形式映射神经网络（SMNN）的训练过程和替代凸多面体的方法，并且首次引入了 SMNN 的可解释性能力。 |
| [^244] | [Contextual Bandits with Budgeted Information Reveal.](http://arxiv.org/abs/2305.18511) | 本文介绍了一种针对医疗领域“亲治疗”操作的限制，且考虑到了操作预算的具有信息预算的情境赌博机算法，这种算法将在线原始-对偶算法和情境赌博机学习算法有机地结合在一起，取得了很好的效果。 |
| [^245] | [Detecting Errors in Numerical Data via any Regression Model.](http://arxiv.org/abs/2305.16583) | 该论文提出了一种模型不可知的方法，通过考虑各种不确定性，可以利用任何回归器检测数值数据中的异常值与自然数据波动，能够有效区分真正的异常和自然数据波动。 |
| [^246] | [ZipIt! Merging Models from Different Tasks without Training.](http://arxiv.org/abs/2305.03053) | 本文介绍了一种无需额外训练即可合并不同任务上训练的模型的方法“ZipIt！”。 |
| [^247] | [Phagocytosis Unveiled: A Scalable and Interpretable Deep learning Framework for Neurodegenerative Disease Analysis.](http://arxiv.org/abs/2304.13764) | 本文提出了一种可扩展且可解释的深度学习框架，用于量化和分析吞噬活性以评估神经退行性疾病。流程可以处理大型数据集，包括数据质量验证和可解释的细胞分割模块。 |
| [^248] | [Flexible K Nearest Neighbors Classifier: Derivation and Application for Ion-mobility Spectrometry-based Indoor Localization.](http://arxiv.org/abs/2304.10151) | 本文提出了一种新的K最近邻分类器变体，可以确保最近邻居确实接近未标记样本，并在过程中找到K值。与标准KNN相比，该算法在室内指纹定位方面具有更高的分类精度。 |
| [^249] | [Hard Regularization to Prevent Collapse in Online Deep Clustering without Data Augmentation.](http://arxiv.org/abs/2303.16521) | 该论文提出了一种不需要数据增强的在线深度聚类方法，通过加强正则化来避免崩溃，相比于其他方法，具有更高的稳定性。 |
| [^250] | [Data-Efficient Contrastive Self-supervised Learning: Easy Examples Contribute the Most.](http://arxiv.org/abs/2302.09195) | 该研究证明了在自监督学习中容易学习的样本对学习高质量表示起到最大的作用，这有助于减少所需的训练数据量，并提高性能。 |
| [^251] | [Referential communication in heterogeneous communities of pre-trained visual deep networks.](http://arxiv.org/abs/2302.08913) | 异构视觉深度网络社区中的预训练网络可以自我监督地开发出共享协议，以指代一组目标中的目标对象，并可用于沟通不同粒度的未知对象类别。 |
| [^252] | [Feature Likelihood Score: Evaluating Generalization of Generative Models Using Samples.](http://arxiv.org/abs/2302.04440) | 本文提出了一种新的特征似然分数（FLS）评估深度生成模型泛化能力的度量指标，用于评估生成样本的新颖性、保真度和多样性，通过实验证明其优于现有的指标，并能够检测出过拟合问题。 |
| [^253] | [Improved Kernel Alignment Regret Bound for Online Kernel Learning.](http://arxiv.org/abs/2212.12989) | 本文提出了一种新的算法，在线核学习中的新算法遗憾界和计算复杂度优于以前的结果。该算法的遗憾界和计算复杂度取决于核矩阵特征值的衰减率。 |
| [^254] | [Two-stage LLM Fine-tuning with Less Specialization and More Generalization.](http://arxiv.org/abs/2211.00635) | 预训练的大型语言模型（LLMs）通过精调可以提高特定任务的性能，但精调通常会使模型过度专门化，降低了其在上下文中的泛化学习性能。通过两阶段精调框架ProMoT可以减少这种格式特化。 |
| [^255] | [Meta Pattern Concern Score: A Novel Evaluation Measure with Human Values for Multi-classifiers.](http://arxiv.org/abs/2209.06408) | 本文提出了一种名为“Meta Pattern Concern Score”的新型评估指标，它基于概率预测的抽象表征和可调节的阈值，将人类价值观引入到多分类器中，可以用于根据人类价值观恰当地评估黑盒模型在现实中的应用效果，并且可以比较在具有不同人类价值观下使用不同分类器的同一数据集。 |
| [^256] | [TSFool: Crafting Highly-imperceptible Adversarial Time Series through Multi-objective Black-box Attack to Fool RNN Classifiers.](http://arxiv.org/abs/2209.06388) | 本文提出了一种名为TSFool的黑盒方法, 可以有效地生成针对RNN分类器的高度难以察觉的对抗性时间序列，在考虑对抗样本难以察觉性的情况下，将对抗性攻击改进为多目标优化问题来增强扰动的质量。 |
| [^257] | [Application of Quantum Density Matrix in Classical Question Answering and Classical Image Classification.](http://arxiv.org/abs/2203.11155) | 该论文将量子密度矩阵应用于经典问答和图像分类中，证明了其可以提高任务的效率，尤其在图像分类中取得了优秀的性能表现。 |
| [^258] | [Bolstering Stochastic Gradient Descent with Model Building.](http://arxiv.org/abs/2111.07058) | 用基于模型构建的方法增强了随机梯度下降算法，适应性地调整步长和搜索方向，提高了收敛速度。 |

# 详细

[^1]: 持续预训练大型语言模型的简单可扩展策略

    Simple and Scalable Strategies to Continually Pre-train Large Language Models

    [https://arxiv.org/abs/2403.08763](https://arxiv.org/abs/2403.08763)

    通过简单和可扩展的学习率调整、重放数据的方法，可以在不重新训练的情况下，持续预训练大型语言模型以匹配完全重新训练时的性能。

    

    大型语言模型（LLMs）通常在数十亿的标记上进行常规预训练，一旦有新数据可用就重新开始该过程。一个更有效率的解决方案是持续预训练这些模型，与重新训练相比能节省大量计算资源。然而，新数据引起的分布转移通常会导致在以前数据上降低性能或无法适应新数据。在本工作中，我们展示了一种简单且可扩展的学习率（LR）重新升温、LR重新衰减和重放上一数据的组合足以与完全从头开始重新训练在所有可用数据上的性能相匹配，从最终损失和语言模型（LM）评估基准的角度衡量。具体而言，我们展示了在两个常用的LLM预训练数据集（英语→英语）之间的弱但现实的分布转移以及更强烈的分布转移（英语→德语）下的情况。

    arXiv:2403.08763v1 Announce Type: cross  Abstract: Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by final loss and language model (LM) evaluation benchmarks. Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English$\rightarrow$English) and a stronger distribution shift (English$\rightarrow$German) at th
    
[^2]: 通过热扩散实现高效的组合优化

    Efficient Combinatorial Optimization via Heat Diffusion

    [https://arxiv.org/abs/2403.08757](https://arxiv.org/abs/2403.08757)

    通过热扩散实现了高效的组合优化，克服了现有方法在搜索全局最优时效率有限的问题。

    

    论文探讨了通过热扩散来实现高效的组合优化。针对现有方法只能在每次迭代中访问解空间的一小部分这一限制，提出了一种框架来解决一般的组合优化问题，并且在一系列最具挑战性和广泛遇到的组合优化中展现出卓越性能。

    arXiv:2403.08757v1 Announce Type: cross  Abstract: Combinatorial optimization problems are widespread but inherently challenging due to their discrete nature.The primary limitation of existing methods is that they can only access a small fraction of the solution space at each iteration, resulting in limited efficiency for searching the global optimal. To overcome this challenge, diverging from conventional efforts of expanding the solver's search scope, we focus on enabling information to actively propagate to the solver through heat diffusion. By transforming the target function while preserving its optima, heat diffusion facilitates information flow from distant regions to the solver, providing more efficient navigation. Utilizing heat diffusion, we propose a framework for solving general combinatorial optimization problems. The proposed methodology demonstrates superior performance across a range of the most challenging and widely encountered combinatorial optimizations. Echoing rec
    
[^3]: DAM:用于持续视频问答学习的动态适配器合并

    DAM: Dynamic Adapter Merging for Continual Video QA Learning

    [https://arxiv.org/abs/2403.08755](https://arxiv.org/abs/2403.08755)

    提出了一种用于持续视频问答学习的动态适配器合并方法DAM，能够减轻灾难性遗忘、有效适应不断到来的数据集、处理未知数据集输入，并允许在类似数据集领域之间共享知识。

    

    我们提出了一种参数高效的方法，用于持续视频问答（VidQA）学习。我们的方法名为DAM，使用所提出的动态适配器合并来（i）减轻灾难性遗忘，（ii）实现对持续到达的数据集的高效适应，（iii）在推理过程中处理来自未知数据集的输入，（iv）实现跨相似数据集领域的知识共享。在给定一组持续流式传输的VidQA数据集的情况下，我们为每个数据集顺序训练特定于数据集的适配器，同时冻结大型预训练视频语言骨干的参数。在推理过程中，给定来自未知领域的视频问题示例，我们的方法首先使用所提出的非参数路由器函数计算每个适配器的概率，反映出该适配器与当前视频问题输入实例的相关性。随后，所提出的动态适配器合并方案聚合所有适配器权重。

    arXiv:2403.08755v1 Announce Type: cross  Abstract: We present a parameter-efficient method for continual video question-answering (VidQA) learning. Our method, named DAM, uses the proposed Dynamic Adapter Merging to (i) mitigate catastrophic forgetting, (ii) enable efficient adaptation to continually arriving datasets, (iii) handle inputs from unknown datasets during inference, and (iv) enable knowledge sharing across similar dataset domains. Given a set of continually streaming VidQA datasets, we sequentially train dataset-specific adapters for each dataset while freezing the parameters of a large pretrained video-language backbone. During inference, given a video-question sample from an unknown domain, our method first uses the proposed non-parametric router function to compute a probability for each adapter, reflecting how relevant that adapter is to the current video-question input instance. Subsequently, the proposed dynamic adapter merging scheme aggregates all the adapter weight
    
[^4]: 神经再生核巴拿赫空间和深度网络的表现定理

    Neural reproducing kernel Banach spaces and representer theorems for deep networks

    [https://arxiv.org/abs/2403.08750](https://arxiv.org/abs/2403.08750)

    本文展示了深度神经网络定义了适当的再生核巴拿赫空间，在这些空间中适应输入数据及其表示中潜在结构，通过再生核巴拿赫空间理论和变分结果得出了适用于实际中常见有限深度网络的表现定理。

    

    研究由神经网络定义的函数空间有助于理解相应的学习模型及其归纳偏差。本文展示了深度神经网络定义了适当的再生核巴拿赫空间，这些空间配备有强制稀疏性的范数，使其能够适应输入数据及其表示中潜在结构。基于再生核巴拿赫空间理论，结合变分结果，我们得出了证明在应用中常用的有限架构的表现定理。我们的研究扩展了浅层网络的类似结果，可以看作是朝着更实用的方向的一步。

    arXiv:2403.08750v1 Announce Type: cross  Abstract: Studying the function spaces defined by neural networks helps to understand the corresponding learning models and their inductive bias. While in some limits neural networks correspond to function spaces that are reproducing kernel Hilbert spaces, these regimes do not capture the properties of the networks used in practice. In contrast, in this paper we show that deep neural networks define suitable reproducing kernel Banach spaces.   These spaces are equipped with norms that enforce a form of sparsity, enabling them to adapt to potential latent structures within the input data and their representations. In particular, leveraging the theory of reproducing kernel Banach spaces, combined with variational results, we derive representer theorems that justify the finite architectures commonly employed in applications. Our study extends analogous results for shallow networks and can be seen as a step towards considering more practically plaus
    
[^5]: 将LLMs引导到无偏响应：基于因果关系的去偏倾框架

    Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing Framework

    [https://arxiv.org/abs/2403.08743](https://arxiv.org/abs/2403.08743)

    本文提出了一种基于因果关系的去偏倾框架，通过选择机制指导设计提示来减少大型语言模型(LLMs)产生的社会偏见。

    

    大型语言模型（LLMs）很容易产生偏见和歧视性的响应。由于LLMs涉及到重要的决策制定（例如招聘和医疗保健），开发减轻这些偏见的策略至关重要。本文侧重于社会偏见，解决了人口统计信息与LLM输出之间的关联。我们提出了一种基于因果关系的去偏倾框架，利用对LLMs输入的训练语料库的数据生成过程以及LLM推理的内部推理过程的因果理解，通过选择机制指导去偏倾LLM输出的提示设计。我们的框架统一了现有的去偏指示方法，如抑制指令和上下文对比例子，并通过鼓励无偏推理的方法，启示了新的去偏倾方式。我们在真实数据集上的强大实证表现表明，我们的框架可以

    arXiv:2403.08743v1 Announce Type: cross  Abstract: Large language models (LLMs) can easily generate biased and discriminative responses. As LLMs tap into consequential decision-making (e.g., hiring and healthcare), it is of crucial importance to develop strategies to mitigate these biases. This paper focuses on social bias, tackling the association between demographic information and LLM outputs. We propose a causality-guided debiasing framework that utilizes causal understandings of (1) the data-generating process of the training corpus fed to LLMs, and (2) the internal reasoning process of LLM inference, to guide the design of prompts for debiasing LLM outputs through selection mechanisms. Our framework unifies existing de-biasing prompting approaches such as inhibitive instructions and in-context contrastive examples, and sheds light on new ways of debiasing by encouraging bias-free reasoning. Our strong empirical performance on real-world datasets demonstrates that our framework pr
    
[^6]: 学习如何策略性披露信息

    Learning How to Strategically Disclose Information

    [https://arxiv.org/abs/2403.08741](https://arxiv.org/abs/2403.08741)

    本研究针对一个发送者与每一轮敌意选择的未知类型接收者之间的信息设计问题，证明了使用全信息反馈时可以实现$O(\sqrt{T})$的后悔。

    

    战略信息披露，简单来说，考虑了信息提供者（发送方）和对某些私人信息感兴趣的信息接收者之间的博弈。发件人可以通过信号承诺设计信息（或修改接收方的信念），从而构成一场斯塔克贝格博弈。然而，传统上，为了在这场比赛中获得斯塔克贝格均衡，发件人需要访问接收者的目标。在这项工作中，我们考虑了一种信息设计的在线版本，其中发件人与每一轮敌意选择的未知类型的接收者进行交互。在考虑了发件人和接收者的高斯先验和二次成本的情况下，我们证明了使用全信息反馈可以实现$O(\sqrt{T})$的后悔，其中$T$是总交互次数。

    arXiv:2403.08741v1 Announce Type: cross  Abstract: Strategic information disclosure, in its simplest form, considers a game between an information provider (sender) who has access to some private information that an information receiver is interested in. While the receiver takes an action that affects the utilities of both players, the sender can design information (or modify beliefs) of the receiver through signal commitment, hence posing a Stackelberg game. However, obtaining a Stackelberg equilibrium for this game traditionally requires the sender to have access to the receiver's objective. In this work, we consider an online version of information design where a sender interacts with a receiver of an unknown type who is adversarially chosen at each round. Restricting attention to Gaussian prior and quadratic costs for the sender and the receiver, we show that $\mathcal{O}(\sqrt{T})$ regret is achievable with full information feedback, where $T$ is the total number of interactions b
    
[^7]: 使用环境扩散后验采样：在受损数据上训练的扩散模型解决逆问题

    Ambient Diffusion Posterior Sampling: Solving Inverse Problems with Diffusion Models trained on Corrupted Data

    [https://arxiv.org/abs/2403.08728](https://arxiv.org/abs/2403.08728)

    提出了一种使用环境扩散后验采样解决逆问题的框架，能在受损数据上训练的扩散模型上表现出色，并在图像恢复和MRI模型训练中取得优越性能。

    

    我们提供了一个框架，用于使用从线性受损数据中学习的扩散模型解决逆问题。我们的方法，Ambient Diffusion Posterior Sampling (A-DPS)，利用一个预先在一种类型的损坏数据上进行过训练的生成模型，以在可能来自不同前向过程（例如图像模糊）的测量条件下执行后验采样。我们在标准自然图像数据集（CelebA、FFHQ 和 AFHQ）上测试了我们的方法的有效性，并展示了 A-DPS 有时在速度和性能上都能胜过在清洁数据上训练的模型，用于几个图像恢复任务。我们进一步扩展了环境扩散框架，以仅访问傅里叶子采样的多线圈 MRI 测量数据来训练 MRI 模型，其加速因子为不同的加速因子（R=2、4、6、8）。我们再次观察到，在高度子采样数据上训练的模型更适用于解决高加速 MRI 逆问题。

    arXiv:2403.08728v1 Announce Type: cross  Abstract: We provide a framework for solving inverse problems with diffusion models learned from linearly corrupted data. Our method, Ambient Diffusion Posterior Sampling (A-DPS), leverages a generative model pre-trained on one type of corruption (e.g. image inpainting) to perform posterior sampling conditioned on measurements from a potentially different forward process (e.g. image blurring). We test the efficacy of our approach on standard natural image datasets (CelebA, FFHQ, and AFHQ) and we show that A-DPS can sometimes outperform models trained on clean data for several image restoration tasks in both speed and performance. We further extend the Ambient Diffusion framework to train MRI models with access only to Fourier subsampled multi-coil MRI measurements at various acceleration factors (R=2, 4, 6, 8). We again observe that models trained on highly subsampled data are better priors for solving inverse problems in the high acceleration r
    
[^8]: 基于扩散的迭代反事实解释用于胎儿超声图像质量评估

    Diffusion-based Iterative Counterfactual Explanations for Fetal Ultrasound Image Quality Assessment

    [https://arxiv.org/abs/2403.08700](https://arxiv.org/abs/2403.08700)

    提出了基于扩散的迭代反事实解释的方法，通过生成逼真的高质量标准平面，对提高临床医生的培训、改善图像质量以及提升下游诊断和监测具有潜在价值。

    

    怀孕期超声图像质量对准确诊断和监测胎儿健康至关重要。然而，生成高质量的标准平面很困难，受到超声波技术人员的专业知识以及像孕妇BMI或胎儿动态等因素的影响。在这项工作中，我们提出使用基于扩散的反事实可解释人工智能，从低质量的非标准平面生成逼真的高质量标准平面。通过定量和定性评估，我们证明了我们的方法在生成质量增加的可信反事实方面的有效性。这为通过提供视觉反馈加强临床医生培训以及改进图像质量，从而改善下游诊断和监测提供了未来的希望。

    arXiv:2403.08700v1 Announce Type: cross  Abstract: Obstetric ultrasound image quality is crucial for accurate diagnosis and monitoring of fetal health. However, producing high-quality standard planes is difficult, influenced by the sonographer's expertise and factors like the maternal BMI or the fetus dynamics. In this work, we propose using diffusion-based counterfactual explainable AI to generate realistic high-quality standard planes from low-quality non-standard ones. Through quantitative and qualitative evaluation, we demonstrate the effectiveness of our method in producing plausible counterfactuals of increased quality. This shows future promise both for enhancing training of clinicians by providing visual feedback, as well as for improving image quality and, consequently, downstream diagnosis and monitoring.
    
[^9]: 一层Softmax注意力模型上梯度流的隐式正则化

    Implicit Regularization of Gradient Flow on One-Layer Softmax Attention

    [https://arxiv.org/abs/2403.08699](https://arxiv.org/abs/2403.08699)

    研究了在一层Softmax注意力模型上指数损失函数的梯度流，发现在渐进最小化损失值时隐式最小化了关键和查询权重矩阵乘积的核范数，这种隐式正则化可通过与注意力权重相关的SVM问题描述。

    

    我们研究了在一层Softmax注意力模型上指数损失函数的梯度流，其中关键和查询权重矩阵是分别训练的。在数据可分性假设下，我们证明了当梯度流达到最小损失值时，它进一步隐式地最小化了关键和查询权重矩阵乘积的核范数。这种隐式正则化可以通过与注意力权重相关的支持向量机（SVM）问题来描述。这一发现与先前的结果形成对比，先前的结果显示当将关键和查询矩阵合并为单个权重矩阵进行训练时，梯度下降会在乘积权重矩阵上实施隐式正则化，最小化弗罗贝尼乌斯范数。对于对角关键和查询矩阵，我们的分析建立在重新参数化技术和利用与分类任务相关的SVM的近似KKT条件的基础上。

    arXiv:2403.08699v1 Announce Type: cross  Abstract: We study gradient flow on the exponential loss for a classification problem with a one-layer softmax attention model, where the key and query weight matrices are trained separately. Under a separability assumption on the data, we show that when gradient flow achieves the minimal loss value, it further implicitly minimizes the nuclear norm of the product of the key and query weight matrices. Such implicit regularization can be described by a Support Vector Machine (SVM) problem with respect to the attention weights. This finding contrasts with prior results showing that the gradient descent induces an implicit regularization on the Frobenius norm on the product weight matrix when the key and query matrices are combined into a single weight matrix for training. For diagonal key and query matrices, our analysis builds upon the reparameterization technique and exploits approximate KKT conditions of the SVM associated with the classificatio
    
[^10]: 数字孪生辅助的强化学习用于边缘计算中的资源感知微服务卸载

    Digital Twin-assisted Reinforcement Learning for Resource-aware Microservice Offloading in Edge Computing

    [https://arxiv.org/abs/2403.08687](https://arxiv.org/abs/2403.08687)

    该论文介绍了一种利用深度强化学习和数字孪生技术的新型微服务卸载算法，用于解决边缘计算环境中低效微服务卸载策略的问题。

    

    合作式边缘计算（CEC）已成为一种有前途的范式，使边缘节点能够协作并从终端设备执行微服务。 微服务卸载是一个根本重要的问题，它在服务到达时决定何时以及在何处执行微服务。然而，现实世界CEC环境的动态特性经常导致低效的微服务卸载策略，从而导致资源利用不足和网络拥塞。为解决这一挑战，我们制定了一个在线联合微服务卸载和带宽分配问题JMOBA，以最小化服务的平均完成时间。在本文中，我们介绍了一种新颖的微服务卸载算法DTDRLMO，它利用深度强化学习（DRL）和数字孪生技术。具体来说，我们采用数字孪生技术实时预测并适应CEC边缘节点负载和网络条件的变化。

    arXiv:2403.08687v1 Announce Type: cross  Abstract: Collaborative edge computing (CEC) has emerged as a promising paradigm, enabling edge nodes to collaborate and execute microservices from end devices. Microservice offloading, a fundamentally important problem, decides when and where microservices are executed upon the arrival of services. However, the dynamic nature of the real-world CEC environment often leads to inefficient microservice offloading strategies, resulting in underutilized resources and network congestion. To address this challenge, we formulate an online joint microservice offloading and bandwidth allocation problem, JMOBA, to minimize the average completion time of services. In this paper, we introduce a novel microservice offloading algorithm, DTDRLMO, which leverages deep reinforcement learning (DRL) and digital twin technology. Specifically, we employ digital twin techniques to predict and adapt to changing edge node loads and network conditions of CEC in real-time
    
[^11]: 何时能用神经切线核和主成分分析近似宽对比模型？

    When can we Approximate Wide Contrastive Models with Neural Tangent Kernels and Principal Component Analysis?

    [https://arxiv.org/abs/2403.08673](https://arxiv.org/abs/2403.08673)

    本研究分析了具有非线性激活函数的两层对比模型的训练动态，揭示了在何种情况下这些模型接近于主成分分析（PCA）或核方法。同时，提供了对比损失的NTK收敛结果，为对比学习与核方法之间的关联提供了更深入的理解。

    

    对比学习是一种从无标签数据中学习表示的范式，对于图像和文本数据非常成功。最近的一些工作考察了对比损失，声称对比模型有效地学习了谱嵌入，而少数工作展示了（宽）对比模型与核主成分分析（PCA）之间的关系。然而，目前尚不清楚训练好的对比模型是否确实对应于核方法或PCA。在这项工作中，我们分析了具有非线性激活的两层对比模型的训练动态，回答了这些模型何时接近PCA或核方法。众所周知，在受监督设置中，神经网络等效于神经切线核（NTK）机器，并且无穷宽网络的NTK在训练过程中保持恒定。我们提供了对比损失NTK的第一个收敛结果，并呈现了一个细致的画面。

    arXiv:2403.08673v1 Announce Type: new  Abstract: Contrastive learning is a paradigm for learning representations from unlabelled data that has been highly successful for image and text data. Several recent works have examined contrastive losses to claim that contrastive models effectively learn spectral embeddings, while few works show relations between (wide) contrastive models and kernel principal component analysis (PCA). However, it is not known if trained contrastive models indeed correspond to kernel methods or PCA. In this work, we analyze the training dynamics of two-layer contrastive models, with non-linear activation, and answer when these models are close to PCA or kernel methods. It is well known in the supervised setting that neural networks are equivalent to neural tangent kernel (NTK) machines, and that the NTK of infinitely wide networks remains constant during training. We provide the first convergence results of NTK for contrastive losses, and present a nuanced pictur
    
[^12]: 用于人工临床记录的零样本和少样本生成策略

    Zero-shot and Few-shot Generation Strategies for Artificial Clinical Records

    [https://arxiv.org/abs/2403.08664](https://arxiv.org/abs/2403.08664)

    这项研究探讨了利用合成数据生成医疗记录的方法，特别是通过零样本和少样本提示策略，避免了在训练过程中使用真实患者数据的隐私问题。

    

    通过使用合成医疗记录的创新方法，本研究评估了Llama 2 LLM的能力，利用零样本和少样本提示策略生成可准确反映真实患者信息的合成医疗记录，与需要在训练过程中使用敏感患者数据的微调方法进行对比。

    arXiv:2403.08664v1 Announce Type: new  Abstract: The challenge of accessing historical patient data for clinical research, while adhering to privacy regulations, is a significant obstacle in medical science. An innovative approach to circumvent this issue involves utilising synthetic medical records that mirror real patient data without compromising individual privacy. The creation of these synthetic datasets, particularly without using actual patient data to train Large Language Models (LLMs), presents a novel solution as gaining access to sensitive patient information to train models is also a challenge. This study assesses the capability of the Llama 2 LLM to create synthetic medical records that accurately reflect real patient information, employing zero-shot and few-shot prompting strategies for comparison against fine-tuned methodologies that do require sensitive patient data during training. We focus on generating synthetic narratives for the History of Present Illness section, 
    
[^13]: 用于协方差估计的自监督学习

    Self-Supervised Learning for Covariance Estimation

    [https://arxiv.org/abs/2403.08662](https://arxiv.org/abs/2403.08662)

    提出了一种利用自监督学习进行协方差估计的方法，通过全局训练神经网络并在推断时局部应用，在不需要任何标签的情况下利用全局特征，具有自动化的全局特征利用优势。

    

    我们考虑使用深度学习进行协方差估计。我们提出全局学习一个神经网络，然后在推断时局部应用该网络。利用自监督基础模型的最新进展，我们通过简单地屏蔽不同样本并学习预测它们周围邻居的协方差来训练网络，无需任何标签。该架构基于流行的注意力机制。与传统方法相比，其主要优势在于在没有任何分布假设或正则化的情况下自动利用全局特征。它可以作为基础模型进行预训练，然后再为各种下游任务重用，例如雷达或高光谱图像中的自适应目标检测。

    arXiv:2403.08662v1 Announce Type: cross  Abstract: We consider the use of deep learning for covariance estimation. We propose to globally learn a neural network that will then be applied locally at inference time. Leveraging recent advancements in self-supervised foundational models, we train the network without any labeling by simply masking different samples and learning to predict their covariance given their surrounding neighbors. The architecture is based on the popular attention mechanism. Its main advantage over classical methods is the automatic exploitation of global characteristics without any distributional assumptions or regularization. It can be pre-trained as a foundation model and then be repurposed for various downstream tasks, e.g., adaptive target detection in radar or hyperspectral imagery.
    
[^14]: 从黑箱深度神经网络中提取解释、证明和不确定性

    Extracting Explanations, Justification, and Uncertainty from Black-Box Deep Neural Networks

    [https://arxiv.org/abs/2403.08652](https://arxiv.org/abs/2403.08652)

    本文提出了一种新的贝叶斯方法，从黑箱深度神经网络中提取解释、证明和不确定性，有效提高了DNN的可解释性和可靠性。

    

    深度神经网络(DNNs)本身不会计算或展示经验证明的任务置信度。在关键任务应用中，了解相关的DNN推理及其支持证据至关重要。本文提出了一种新颖的贝叶斯方法，用于从DNNs中提取解释、证明和不确定性估计。我们的方法在内存和计算方面都很有效，可应用于任何黑箱DNN，无需重新训练，包括异常检测和超出分布检测任务。我们在CIFAR-10数据集上验证了我们的方法，并展示了它可以显著提高DNN的可解释性和可靠性。

    arXiv:2403.08652v1 Announce Type: new  Abstract: Deep Neural Networks (DNNs) do not inherently compute or exhibit empirically-justified task confidence. In mission critical applications, it is important to both understand associated DNN reasoning and its supporting evidence. In this paper, we propose a novel Bayesian approach to extract explanations, justifications, and uncertainty estimates from DNNs. Our approach is efficient both in terms of memory and computation, and can be applied to any black box DNN without any retraining, including applications to anomaly detection and out-of-distribution detection tasks. We validate our approach on the CIFAR-10 dataset, and show that it can significantly improve the interpretability and reliability of DNNs.
    
[^15]: 缺失中介的异质效应对因果效应可传递性的影响

    Disparate Effect Of Missing Mediators On Transportability of Causal Effects

    [https://arxiv.org/abs/2403.08638](https://arxiv.org/abs/2403.08638)

    研究了缺失中介变量对传输的中介效应的影响，并提出了一个敏感性分析框架，能够确定在哪些情况下，具有缺失中介数据的子群的条件传输中介效应变得不显著。

    

    运输的中介效应为我们提供了一种理解上游干预（例如改善社区条件如绿地）在应用于不同人群时会因中介效应的不同而产生差异效果的途径。然而，当中介变量在要传输效应的人群中缺失时，这些估计值可能会存在偏误。我们研究了缺失中介变量这一问题，其动机源于公共卫生领域的挑战，其中中介变量可能是非随机缺失的。我们提出了一个敏感性分析框架，用于量化缺失中介数据对传输的中介效应的影响。该框架使我们能够确定在哪些情况下，具有缺失中介数据子群的条件传输中介效应变得不显著。具体而言，我们提供了传输中介效应的界限，作为缺失程度的函数。然后，我们应用了fra

    arXiv:2403.08638v1 Announce Type: new  Abstract: Transported mediation effects provide an avenue to understand how upstream interventions (such as improved neighborhood conditions like green spaces) would work differently when applied to different populations as a result of factors that mediate the effects. However, when mediators are missing in the population where the effect is to be transported, these estimates could be biased. We study this issue of missing mediators, motivated by challenges in public health, wherein mediators can be missing, not at random. We propose a sensitivity analysis framework that quantifies the impact of missing mediator data on transported mediation effects. This framework enables us to identify the settings under which the conditional transported mediation effect is rendered insignificant for the subgroup with missing mediator data. Specifically, we provide the bounds on the transported mediation effect as a function of missingness. We then apply the fra
    
[^16]: 通过在线偏好优化实现大型语言模型与人类的对齐

    Human Alignment of Large Language Models through Online Preference Optimisation

    [https://arxiv.org/abs/2403.08635](https://arxiv.org/abs/2403.08635)

    本文展示了两种最近对齐方法之间的等价性，并介绍了一种泛化版本，有助于实现大型语言模型与人类的对齐。

    

    确保语言模型的输出与人类偏好对齐对于确保用户体验的有用性、安全性和愉悦性至关重要。最近，人类对齐已经得到广泛研究，出现了几种方法，例如强化学习来自人类反馈（RLHF）、直接策略优化（DPO）和序列似然校准（SLiC）。本文的贡献有两个方面：首先，我们展示了两种最近对齐方法之间的等价性，即身份策略优化（IPO）和纳什镜像下降（Nash-MD）。其次，我们介绍了IPO的一种泛化版本，名为IPO-MD，它利用了Nash-MD提出的正则化抽样方法。

    arXiv:2403.08635v1 Announce Type: cross  Abstract: Ensuring alignment of language models' outputs with human preferences is critical to guarantee a useful, safe, and pleasant user experience. Thus, human alignment has been extensively studied recently and several methods such as Reinforcement Learning from Human Feedback (RLHF), Direct Policy Optimisation (DPO) and Sequence Likelihood Calibration (SLiC) have emerged. In this paper, our contribution is two-fold. First, we show the equivalence between two recent alignment methods, namely Identity Policy Optimisation (IPO) and Nash Mirror Descent (Nash-MD). Second, we introduce a generalisation of IPO, named IPO-MD, that leverages the regularised sampling approach proposed by Nash-MD.   This equivalence may seem surprising at first sight, since IPO is an offline method whereas Nash-MD is an online method using a preference model. However, this equivalence can be proven when we consider the online version of IPO, that is when both generati
    
[^17]: 十年数据集偏见之战：我们已经成功了吗？

    A Decade's Battle on Dataset Bias: Are We There Yet?

    [https://arxiv.org/abs/2403.08632](https://arxiv.org/abs/2403.08632)

    现代神经网络在分类来自不同数据集的图像方面表现出色，具有可推广和可转移的语义特征，挑战了传统的数据集偏见认知。

    

    我们在新时代重新审视Torralba和Efros十年前提出的“数据集分类”实验，在拥有大规模、多样化和希望更少偏见的数据集以及更强大的神经网络架构的新时代。令人惊讶的是，我们观察到现代神经网络能够在分类图像来自哪个数据集方面取得出色的准确性：例如，对于包含YFCC、CC和DataComp数据集的三分类问题的验证数据，我们报告84.7%的准确性。我们进一步的实验表明，这样的数据集分类器可以学习到可推广和可转移的语义特征，这不能简单地解释为记忆。我们希望我们的发现能激励社区重新思考涉及数据集偏见和模型能力的问题。

    arXiv:2403.08632v1 Announce Type: cross  Abstract: We revisit the "dataset classification" experiment suggested by Torralba and Efros a decade ago, in the new era with large-scale, diverse, and hopefully less biased datasets as well as more capable neural network architectures. Surprisingly, we observe that modern neural networks can achieve excellent accuracy in classifying which dataset an image is from: e.g., we report 84.7% accuracy on held-out validation data for the three-way classification problem consisting of the YFCC, CC, and DataComp datasets. Our further experiments show that such a dataset classifier could learn semantic features that are generalizable and transferable, which cannot be simply explained by memorization. We hope our discovery will inspire the community to rethink the issue involving dataset bias and model capabilities.
    
[^18]: 利用非降小波包特征和Transformer模型进行时间序列预测

    Leveraging Non-Decimated Wavelet Packet Features and Transformer Models for Time Series Forecasting

    [https://arxiv.org/abs/2403.08630](https://arxiv.org/abs/2403.08630)

    本文结合了小波分析技术和机器学习方法，提出利用不同消失矩的Daubechies小波作为输入特征，并比较了非降小波变换和非降小波包变换的效果，并在更广泛的预测方法上评估了这些小波特征的应用。

    

    这篇文章结合了小波分析技术和机器学习方法，用于单变量时间序列预测，重点关注三个主要贡献。首先，我们考虑在交叉验证阶段选择使用不同消失矩的Daubechies小波作为非时间和时间预测方法的输入特征。其次，我们比较使用非降小波变换和非降小波包变换来计算这些特征，后者提供了一个更大的潜在有用系数向量集合。小波系数是使用典型金字塔算法的平移版本计算的，以确保没有将未来信息泄漏到这些输入中。第三，我们评估了这些小波特征在比以前的研究更广泛的一组预测方法上的应用，包括时间和非时间预测方法。

    arXiv:2403.08630v1 Announce Type: cross  Abstract: This article combines wavelet analysis techniques with machine learning methods for univariate time series forecasting, focusing on three main contributions. Firstly, we consider the use of Daubechies wavelets with different numbers of vanishing moments as input features to both non-temporal and temporal forecasting methods, by selecting these numbers during the cross-validation phase. Secondly, we compare the use of both the non-decimated wavelet transform and the non-decimated wavelet packet transform for computing these features, the latter providing a much larger set of potentially useful coefficient vectors. The wavelet coefficients are computed using a shifted version of the typical pyramidal algorithm to ensure no leakage of future information into these inputs. Thirdly, we evaluate the use of these wavelet features on a significantly wider set of forecasting methods than previous studies, including both temporal and non-tempora
    
[^19]: 多信度线性回归用于科学机器学习中稀缺数据的研究

    Multifidelity linear regression for scientific machine learning from scarce data

    [https://arxiv.org/abs/2403.08627](https://arxiv.org/abs/2403.08627)

    提出了一种新的多信度训练方法，用于处理科学机器学习中稀缺而昂贵的高保真数据，以提高模型的稳健性和泛化能力。

    

    机器学习（ML）方法，通过拟合给定参数化模型类的参数来适应数据，作为学习复杂工程系统的代理模型的潜在方法已经引起了极大关注。然而，在许多科学和工程环境中，生成用于训练ML模型的高保真数据是昂贵的，并且用于生成训练数据的预算有限。我们提出了一种新的科学机器学习多信度训练方法，该方法利用了数据的各种保真度和成本可用的科学背景；例如，高保真数据可能由昂贵的全面解析的物理模拟生成，而低保真数据可能来自基于简化的更便宜的模型。

    arXiv:2403.08627v1 Announce Type: cross  Abstract: Machine learning (ML) methods, which fit to data the parameters of a given parameterized model class, have garnered significant interest as potential methods for learning surrogate models for complex engineering systems for which traditional simulation is expensive. However, in many scientific and engineering settings, generating high-fidelity data on which to train ML models is expensive, and the available budget for generating training data is limited. ML models trained on the resulting scarce high-fidelity data have high variance and are sensitive to vagaries of the training data set. We propose a new multifidelity training approach for scientific machine learning that exploits the scientific context where data of varying fidelities and costs are available; for example high-fidelity data may be generated by an expensive fully resolved physics simulation whereas lower-fidelity data may arise from a cheaper model based on simplifying 
    
[^20]: Verifix: 后训练校正以改善具有经过验证样本的标签噪声鲁棒性

    Verifix: Post-Training Correction to Improve Label Noise Robustness with Verified Samples

    [https://arxiv.org/abs/2403.08618](https://arxiv.org/abs/2403.08618)

    提出了后训练校正的新范式，通过奇异值分解算法Verifix在初始训练后校正模型权重以减轻标签噪声，避免了重新训练的需求

    

    标签错误，即训练样本具有不正确的标签，可能严重损害机器学习模型的性能。这种错误往往来自非专家标注或敌对攻击。获取大型、完全标记的数据集成本高，当有干净的数据集可用时，重新训练大型模型就变得计算昂贵。为了解决这一挑战，我们提出了后训练校正，这是一种在初始训练后调整模型参数以减轻标签噪声的新范式，消除了重新训练的需要。我们引入了Verifix，这是一种基于奇异值分解（SVD）的新算法，利用一个小的、经过验证的数据集，通过单个更新校正模型权重。Verifix使用SVD估计干净激活空间，然后将模型的权重投影到这个空间上，以抑制对应于损坏数据的激活。我们展示了Verifix的有效性。

    arXiv:2403.08618v1 Announce Type: cross  Abstract: Label corruption, where training samples have incorrect labels, can significantly degrade the performance of machine learning models. This corruption often arises from non-expert labeling or adversarial attacks. Acquiring large, perfectly labeled datasets is costly, and retraining large models from scratch when a clean dataset becomes available is computationally expensive. To address this challenge, we propose Post-Training Correction, a new paradigm that adjusts model parameters after initial training to mitigate label noise, eliminating the need for retraining. We introduce Verifix, a novel Singular Value Decomposition (SVD) based algorithm that leverages a small, verified dataset to correct the model weights using a single update. Verifix uses SVD to estimate a Clean Activation Space and then projects the model's weights onto this space to suppress activations corresponding to corrupted data. We demonstrate Verifix's effectiveness 
    
[^21]: 利用表示学习和基于启发式特征的方法进行社交网络的链接预测

    Link Prediction for Social Networks using Representation Learning and Heuristic-based Features

    [https://arxiv.org/abs/2403.08613](https://arxiv.org/abs/2403.08613)

    提出了一种结合启发式特征和学习表示的方法，用于社交网络中缺失链接的预测任务，取得了较好的性能提升

    

    在社交网络的规模和相关性呈指数增长的情况下，预测社交网络中缺失的链接变得越来越重要。本文探讨了各种特征提取技术，以生成社交网络中节点和边的表示，从而帮助预测缺失的链接。我们比较了十种特征提取技术的结果，这些技术分为结构嵌入、基于邻居的嵌入、图神经网络和图启发式。接着，我们使用集成分类器和定制神经网络进行建模。此外，我们提出了将基于启发式特征和学习表示相结合的方法，这一方法在社交网络数据集上展现出了更好的性能。

    arXiv:2403.08613v1 Announce Type: cross  Abstract: The exponential growth in scale and relevance of social networks enable them to provide expansive insights. Predicting missing links in social networks efficiently can help in various modern-day business applications ranging from generating recommendations to influence analysis. Several categories of solutions exist for the same. Here, we explore various feature extraction techniques to generate representations of nodes and edges in a social network that allow us to predict missing links. We compare the results of using ten feature extraction techniques categorized across Structural embeddings, Neighborhood-based embeddings, Graph Neural Networks, and Graph Heuristics, followed by modeling with ensemble classifiers and custom Neural Networks. Further, we propose combining heuristic-based features and learned representations that demonstrate improved performance for the link prediction task on social network datasets. Using this method 
    
[^22]: 深度贝叶斯神经网络后验的局部自适应和可扩展扩散采样方法的收敛问题

    On the Convergence of Locally Adaptive and Scalable Diffusion-Based Sampling Methods for Deep Bayesian Neural Network Posteriors

    [https://arxiv.org/abs/2403.08609](https://arxiv.org/abs/2403.08609)

    本文研究如何将自适应步长引入蒙特卡罗采样算法，以实现从神经网络后验分布中生成样本，并证明了这些方法可以收敛到正确的分布。

    

    实现深度神经网络的鲁棒不确定性量化在许多深度学习的实际应用中具有重要意义，如医学成像中需要评估神经网络预测的可靠性。贝叶斯神经网络是对深度神经网络中不确定性建模的一种有前途的方法。然而，从神经网络后验分布中生成样本是一个主要挑战。朝着这个方向的一个重要进展将是将类似于现代神经网络优化器的自适应步长纳入马尔可夫链蒙特卡罗采样算法，而不会显著增加计算需求。过去几年中，一些论文提出了具有实现这一属性的采样算法。然而，它们是否确实收敛到正确的分布呢？在本文中，我们证明了这些方法可以达到这一目标。

    arXiv:2403.08609v1 Announce Type: new  Abstract: Achieving robust uncertainty quantification for deep neural networks represents an important requirement in many real-world applications of deep learning such as medical imaging where it is necessary to assess the reliability of a neural network's prediction. Bayesian neural networks are a promising approach for modeling uncertainties in deep neural networks. Unfortunately, generating samples from the posterior distribution of neural networks is a major challenge. One significant advance in that direction would be the incorporation of adaptive step sizes, similar to modern neural network optimizers, into Monte Carlo Markov chain sampling algorithms without significantly increasing computational demand. Over the past years, several papers have introduced sampling algorithms with claims that they achieve this property. However, do they indeed converge to the correct distribution? In this paper, we demonstrate that these methods can have a 
    
[^23]: 用合成时间序列预训练实现高效的睡眠分期

    Data-Efficient Sleep Staging with Synthetic Time Series Pretraining

    [https://arxiv.org/abs/2403.08592](https://arxiv.org/abs/2403.08592)

    通过预测合成时间序列的频率内容进行预训练，实现了在有限数据和少受试者情况下超越完全监督学习的方法

    

    分析脑电图（EEG）时间序列可能具有挑战性，特别是在深度神经网络中，由于人类受试者之间的大量变异和通常规模较小的数据集。为了解决这些挑战，提出了各种策略，例如自监督学习，但它们通常依赖于广泛的实证数据集。受计算机视觉最新进展的启发，我们提出了一种预训练任务，称为“频率预训练”，通过预测随机生成的合成时间序列的频率内容来为睡眠分期预训练神经网络。我们的实验表明，我们的方法在有限数据和少受试者的情况下优于完全监督学习，并在许多受试者的情境中表现相匹配。此外，我们的结果强调了频率信息对于睡眠分期评分的相关性，同时表明深度神经网络利用了超出频率信息的信息。

    arXiv:2403.08592v1 Announce Type: new  Abstract: Analyzing electroencephalographic (EEG) time series can be challenging, especially with deep neural networks, due to the large variability among human subjects and often small datasets. To address these challenges, various strategies, such as self-supervised learning, have been suggested, but they typically rely on extensive empirical datasets. Inspired by recent advances in computer vision, we propose a pretraining task termed "frequency pretraining" to pretrain a neural network for sleep staging by predicting the frequency content of randomly generated synthetic time series. Our experiments demonstrate that our method surpasses fully supervised learning in scenarios with limited data and few subjects, and matches its performance in regimes with many subjects. Furthermore, our results underline the relevance of frequency information for sleep stage scoring, while also demonstrating that deep neural networks utilize information beyond fr
    
[^24]: 硬件建模中的神经网络泛化能力及物理信息应用研究

    Can physical information aid the generalization ability of Neural Networks for hydraulic modeling?

    [https://arxiv.org/abs/2403.08589](https://arxiv.org/abs/2403.08589)

    研究通过在训练阶段引入物理信息的方法来改善神经网络在河流水力学中的预测能力问题。

    

    尽管河流水力学领域遭受数据稀缺的困扰，是机器学习技术的挑战之一，但神经网络在河流水力学中的应用仍然处于萌芽阶段。因此，许多纯数据驱动的神经网络被证明缺乏预测能力。本文提出通过在训练阶段引入物理信息来缓解这个问题。这一想法源自其他背景下最近提出的物理信息神经网络。物理信息神经网络以控制现象的偏微分方程（PDEs）的残差的形式嵌入物理信息，因此被构想为神经求解器，即传统数值求解器的替代品。这种方法很少适用于环境水力学，那里的认知不确定性很大，并且计算PDE的残差存在着类似于传统数值方法所面临的困难。

    arXiv:2403.08589v1 Announce Type: new  Abstract: Application of Neural Networks to river hydraulics is fledgling, despite the field suffering from data scarcity, a challenge for machine learning techniques. Consequently, many purely data-driven Neural Networks proved to lack predictive capabilities. In this work, we propose to mitigate such problem by introducing physical information into the training phase. The idea is borrowed from Physics-Informed Neural Networks which have been recently proposed in other contexts. Physics-Informed Neural Networks embed physical information in the form of the residual of the Partial Differential Equations (PDEs) governing the phenomenon and, as such, are conceived as neural solvers, i.e. an alternative to traditional numerical solvers. Such approach is seldom suitable for environmental hydraulics, where epistemic uncertainties are large, and computing residuals of PDEs exhibits difficulties similar to those faced by classical numerical methods. Inst
    
[^25]: 通过预条件化改善最小二乘问题随机梯度下降的隐式正则化

    Improving Implicit Regularization of SGD with Preconditioning for Least Square Problems

    [https://arxiv.org/abs/2403.08585](https://arxiv.org/abs/2403.08585)

    通过预条件化，研究了SGD在最小二乘问题中的泛化性能，对比了预条件化SGD和（标准和预条件化）岭回归，为改善SGD理解和应用提供了关键贡献。

    

    随机梯度下降（SGD）在实践中表现出强大的算法正则化效果，在现代机器学习的泛化中起着重要作用。然而，先前的研究发现，SGD的泛化性能有时会比岭回归差，这是由于沿不同维度的优化不均匀造成的。预条件化通过重新平衡沿不同方向的优化来提供解决这一问题的自然方法。然而，预条件化能够提升SGD的泛化性能的程度以及它是否能够填补现有与岭回归之间的差距仍不确定。本文研究了预条件化对最小二乘问题中SGD的泛化性能的影响。我们全面比较了预条件化SGD与（标准和预条件化）岭回归。我们的研究对了解和改善SGD做出了几项关键贡献。

    arXiv:2403.08585v1 Announce Type: new  Abstract: Stochastic gradient descent (SGD) exhibits strong algorithmic regularization effects in practice and plays an important role in the generalization of modern machine learning. However, prior research has revealed instances where the generalization performance of SGD is worse than ridge regression due to uneven optimization along different dimensions. Preconditioning offers a natural solution to this issue by rebalancing optimization across different directions. Yet, the extent to which preconditioning can enhance the generalization performance of SGD and whether it can bridge the existing gap with ridge regression remains uncertain. In this paper, we study the generalization performance of SGD with preconditioning for the least squared problem. We make a comprehensive comparison between preconditioned SGD and (standard \& preconditioned) ridge regression. Our study makes several key contributions toward understanding and improving SGD wit
    
[^26]: 基于量子退火器训练的局部二进制和多类支持向量机

    Local Binary and Multiclass SVMs Trained on a Quantum Annealer

    [https://arxiv.org/abs/2403.08584](https://arxiv.org/abs/2403.08584)

    提出了一种基于量子训练的支持向量机模型的局部应用，以克服量子退火器受限连通性所导致的训练集大小限制

    

    支持向量机（SVM）是广泛使用的机器学习模型（例如在遥感中），既用于分类又用于回归任务的表达式。近年来，随着工作量子退火器的出现，提出了混合SVM模型，其特征是量子训练和经典执行。这些模型表现出与其经典对应物相当的性能。然而，由于当前量子退火器的受限连通性，它们在训练集大小方面存在限制。因此，为了利用大型数据集（例如与地球观测相关的数据集），需要一种策略。在经典领域中，已经成功证明了局部SVM，即由k个最近邻模型选择的数据样本训练的SVM。本文提出并经验性评估了量子训练的SVM模型的局部应用。特别是，这种方法允许克服了限制

    arXiv:2403.08584v1 Announce Type: cross  Abstract: Support vector machines (SVMs) are widely used machine learning models (e.g., in remote sensing), with formulations for both classification and regression tasks. In the last years, with the advent of working quantum annealers, hybrid SVM models characterised by quantum training and classical execution have been introduced. These models have demonstrated comparable performance to their classical counterparts. However, they are limited in the training set size due to the restricted connectivity of the current quantum annealers. Hence, to take advantage of large datasets (like those related to Earth observation), a strategy is required. In the classical domain, local SVMs, namely, SVMs trained on the data samples selected by a k-nearest neighbors model, have already proven successful. Here, the local application of quantum-trained SVM models is proposed and empirically assessed. In particular, this approach allows overcoming the constrain
    
[^27]: 机器学习优化的正交基分段多项式逼近

    Machine Learning Optimized Orthogonal Basis Piecewise Polynomial Approximation

    [https://arxiv.org/abs/2403.08579](https://arxiv.org/abs/2403.08579)

    本研究利用机器学习的方法对正交基分段多项式逼近进行优化。

    

    分段多项式（PPs）在几个工程学科中被使用，比如在轨迹规划中，用来逼近以一组点给出的位置轮廓。 鉴于逼近目标以及特定领域要求，比如Ck-连续性，可以被构造为一个方程组，结果可以直接计算，这样的闭式解对于多项式次数、多项式基础或者添加进一步的特定领域要求方面具有有限的灵活性。足够复杂的优化目标很快要求使用数值方法，比如梯度下降。由于梯度下降是训练人工神经网络（ANNs）的核心，现代机器学习（ML）框架比如TensorFlow提供了一套基于梯度的优化器，可能适用于训练任务之外的广泛优化问题。我们的方法是利用PP模式的多样性

    arXiv:2403.08579v1 Announce Type: new  Abstract: Piecewise Polynomials (PPs) are utilized in several engineering disciplines, like trajectory planning, to approximate position profiles given in the form of a set of points. While the approximation target along with domain-specific requirements, like Ck -continuity, can be formulated as a system of equations and a result can be computed directly, such closed-form solutions posses limited flexibility with respect to polynomial degrees, polynomial bases or adding further domain-specific requirements. Sufficiently complex optimization goals soon call for the use of numerical methods, like gradient descent. Since gradient descent lies at the heart of training Artificial Neural Networks (ANNs), modern Machine Learning (ML) frameworks like TensorFlow come with a set of gradient-based optimizers potentially suitable for a wide range of optimization problems beyond the training task for ANNs. Our approach is to utilize the versatility of PP mode
    
[^28]: 重新从因果关系的角度思考时间序列分析

    Caformer: Rethinking Time Series Analysis from Causal Perspective

    [https://arxiv.org/abs/2403.08572](https://arxiv.org/abs/2403.08572)

    提出了一个名为Caformer的新框架用于时间序列分析，其中包括动态学习器、环境学习器和依赖关系学习器，旨在从因果关系的角度解决在非平稳时间序列中捕捉跨维度和跨时间依赖关系的挑战

    

    时间序列分析是一个在各个领域具有广泛应用的重要任务。然而，在非平稳时间序列中有效捕捉跨维度和跨时间依赖关系在环境因素的背景下构成了重大挑战。环境引起的虚假相关混淆了跨维度和跨时间依赖关系之间的因果关系。本文引入了一种新颖的框架，称为Caformer(因果Transformer)，用于从因果关系的角度进行时间序列分析。具体来说，我们的框架包括三个组件：动态学习器，环境学习器和依赖关系学习器。动态学习器揭示了维度之间的动态交互作用，环境学习器通过背门调整减轻了环境引起的虚假相关，依赖关系学习器旨在推断跨越两者的稳健交互作用。

    arXiv:2403.08572v1 Announce Type: new  Abstract: Time series analysis is a vital task with broad applications in various domains. However, effectively capturing cross-dimension and cross-time dependencies in non-stationary time series poses significant challenges, particularly in the context of environmental factors. The spurious correlation induced by the environment confounds the causal relationships between cross-dimension and cross-time dependencies. In this paper, we introduce a novel framework called Caformer (\underline{\textbf{Ca}}usal Trans\underline{\textbf{former}}) for time series analysis from a causal perspective. Specifically, our framework comprises three components: Dynamic Learner, Environment Learner, and Dependency Learner. The Dynamic Learner unveils dynamic interactions among dimensions, the Environment Learner mitigates spurious correlations caused by environment with a back-door adjustment, and the Dependency Learner aims to infer robust interactions across both
    
[^29]: 由物理驱动的GraphSAGE方法用于偏微分方程描述的物理过程模拟

    A Physics-driven GraphSAGE Method for Physical Process Simulations Described by Partial Differential Equations

    [https://arxiv.org/abs/2403.08569](https://arxiv.org/abs/2403.08569)

    该论文提出了一种基于物理驱动的GraphSAGE方法，用于解决由不规则PDE管控的计算问题，具有降低精度的优势。

    

    物理信息神经网络（PINNs）成功地解决了基于偏微分方程（PDEs）的各种计算物理问题。但是，在处理与奇异性和振荡等不规则性相关的问题时，训练的解通常会出现低精度。此外，大多数当前的工作只为预定输入参数提供训练解决方案。如果输入参数发生任何更改，则需要进行转移学习或重新训练，并且传统的数值技术也需要独立模拟。本文提出了一种基于Galerkin方法和分段多项式节点基础函数的物理驱动GraphSAGE方法（PD-GraphSAGE），用于解决由不规则PDE管控的计算问题并发展参数PDE代理模型。该方法采用物理领域的图表示，从而减少了由于局部细化而产生的评估点的需求。

    arXiv:2403.08569v1 Announce Type: new  Abstract: Physics-informed neural networks (PINNs) have successfully addressed various computational physics problems based on partial differential equations (PDEs). However, while tackling issues related to irregularities like singularities and oscillations, trained solutions usually suffer low accuracy. In addition, most current works only offer the trained solution for predetermined input parameters. If any change occurs in input parameters, transfer learning or retraining is required, and traditional numerical techniques also need an independent simulation. In this work, a physics-driven GraphSAGE approach (PD-GraphSAGE) based on the Galerkin method and piecewise polynomial nodal basis functions is presented to solve computational problems governed by irregular PDEs and to develop parametric PDE surrogate models. This approach employs graph representations of physical domains, thereby reducing the demands for evaluated points due to local refi
    
[^30]: 无需复习的一致提示用于持续学习

    Consistent Prompting for Rehearsal-Free Continual Learning

    [https://arxiv.org/abs/2403.08568](https://arxiv.org/abs/2403.08568)

    提出了一种新颖的一致提示（CPrompt）方法，通过训练期间所有现有分类器接受提示训练，实现更加对齐的训练和测试。

    

    持续学习使模型能够自主适应不断变化的环境或数据流，而不会忘记旧知识。基于提示的方法建立在冻结的预训练模型上，以高效地学习任务特定的提示和分类器。现有的基于提示的方法在训练和测试之间存在不一致，从而限制了它们的有效性。这种不一致性揭示了两种类型。测试预测是从所有分类器中进行的，而训练只关注当前任务分类器而没有进行整体对齐，导致分类器的不一致性。提示的不一致性表示测试期间选择的提示可能与训练期间与该任务关联的提示不对应。在本文中，我们提出了一种新颖的基于提示的方法，一致提示（CPrompt），用于更加对齐的训练和测试。具体来说，所有现有的分类器都接受提示训练，从而形成分类

    arXiv:2403.08568v1 Announce Type: cross  Abstract: Continual learning empowers models to adapt autonomously to the ever-changing environment or data streams without forgetting old knowledge. Prompt-based approaches are built on frozen pre-trained models to learn the task-specific prompts and classifiers efficiently. Existing prompt-based methods are inconsistent between training and testing, limiting their effectiveness. Two types of inconsistency are revealed. Test predictions are made from all classifiers while training only focuses on the current task classifier without holistic alignment, leading to Classifier inconsistency. Prompt inconsistency indicates that the prompt selected during testing may not correspond to the one associated with this task during training. In this paper, we propose a novel prompt-based method, Consistent Prompting (CPrompt), for more aligned training and testing. Specifically, all existing classifiers are exposed to prompt training, resulting in classifie
    
[^31]: 结构视角下基于约束的马尔可夫网络学习

    Structural perspective on constraint-based learning of Markov networks

    [https://arxiv.org/abs/2403.08562](https://arxiv.org/abs/2403.08562)

    本论文从结构角度研究了基于约束的学习马尔可夫网络，发现了图的结构特性和学习所需测试数量之间的重要关系

    

    马尔可夫网络是概率图模型，使用无向图来表示变量之间的条件独立关系。我们关注约束-based结构学习，通过执行条件独立性检验从数据中学习无向图。我们确定了关于马尔可夫网络约束-based学习的两个关键方面的理论限制：测试数量和条件设置的大小。这些界限揭示了图的结构特性与学习马尔可夫网络所需测试量之间的有趣互动。我们工作的出发点是图参数最大成对连通性 $\kappa$，即，图中连接一对顶点的最大数量的顶点不相交路径，负责独立性测试所需的大小，以学习图。一方面, 我们表明至少 såorest

    arXiv:2403.08562v1 Announce Type: cross  Abstract: Markov networks are probabilistic graphical models that employ undirected graphs to depict conditional independence relationships among variables. Our focus lies in constraint-based structure learning, which entails learning the undirected graph from data through the execution of conditional independence tests. We establish theoretical limits concerning two critical aspects of constraint-based learning of Markov networks: the number of tests and the sizes of the conditioning sets. These bounds uncover an exciting interplay between the structural properties of the graph and the amount of tests required to learn a Markov network. The starting point of our work is that the graph parameter maximum pairwise connectivity, $\kappa$, that is, the maximum number of vertex-disjoint paths connecting a pair of vertices in the graph, is responsible for the sizes of independence tests required to learn the graph. On one hand, we show that at least o
    
[^32]: 基于扩散模型的联邦知识图去学习

    Federated Knowledge Graph Unlearning via Diffusion Model

    [https://arxiv.org/abs/2403.08554](https://arxiv.org/abs/2403.08554)

    提出了FedDM，一个基于扩散模型的新框架，用于联邦知识图中的机器去学习。

    

    arXiv:2403.08554v1 公告类型: 跨领域 摘要: 联邦学习（FL）通过促进模型共享和协作，同时维护数据隐私，推动了人工智能技术的发展和应用。知识图（KG）嵌入表示通过将实体和关系映射到向量空间，为知识推理和应用提供基础。联邦知识图嵌入能够利用来自不同客户端的知识，同时保护本地数据的隐私。然而，由于隐私保护等需求以及需要适应动态数据变化，机器去学习（MU）的研究得以展开。然而，在忘记特定遗忘数据对模型的影响的同时，保持KG嵌入模型的性能是具有挑战性的。本文提出了FedDM，一个针对联邦知识图中机器去学习而量身定制的新型框架。通过利用扩散模型，我们生成带有噪声的...

    arXiv:2403.08554v1 Announce Type: cross  Abstract: Federated learning (FL) promotes the development and application of artificial intelligence technologies by enabling model sharing and collaboration while safeguarding data privacy. Knowledge graph (KG) embedding representation provides a foundation for knowledge reasoning and applications by mapping entities and relations into vector space. Federated KG embedding enables the utilization of knowledge from diverse client sources while safeguarding the privacy of local data. However, due to demands such as privacy protection and the need to adapt to dynamic data changes, investigations into machine unlearning (MU) have been sparked. However, it is challenging to maintain the performance of KG embedding models while forgetting the influence of specific forgotten data on the model. In this paper, we propose FedDM, a novel framework tailored for machine unlearning in federated knowledge graphs. Leveraging diffusion models, we generate noisy
    
[^33]: 在线LQG线性约束政策优化的遗憾分析

    Regret Analysis of Policy Optimization over Submanifolds for Linearly Constrained Online LQG

    [https://arxiv.org/abs/2403.08553](https://arxiv.org/abs/2403.08553)

    提出了在线乐观牛顿流形（OONM），该方法提供基于函数序列的第一和第二阶信息预测的在线控制器，用于在线LQG线性约束政策优化。

    

    在线优化和控制的最新进展为研究在线线性二次调节器（LQR）问题提供了新工具，其中成本矩阵随时间变化对抗性变化。然而，现有作品的控制器参数化可能不满足实际条件，如由于物理连接而导致的稀疏性。在这项工作中，我们研究了在线线性二次高斯问题，其中对控制器施加了给定的线性约束。受[1]最近提出的关于线性约束的线下LQR政策优化的启发，该方法提出了一个二阶方法，配备了一种在最优控制问题的背景下自然产生的黎曼度量，我们提出了在线乐观牛顿流形（OONM），提供基于函数序列的第一和第二阶信息预测的在线控制器。为了量化提出的算法，我们利用了遗憾的概念

    arXiv:2403.08553v1 Announce Type: cross  Abstract: Recent advancement in online optimization and control has provided novel tools to study online linear quadratic regulator (LQR) problems, where cost matrices are varying adversarially over time. However, the controller parameterization of existing works may not satisfy practical conditions like sparsity due to physical connections. In this work, we study online linear quadratic Gaussian problems with a given linear constraint imposed on the controller. Inspired by the recent work of [1] which proposed, for a linearly constrained policy optimization of an offline LQR, a second order method equipped with a Riemannian metric that emerges naturally in the context of optimal control problems, we propose online optimistic Newton on manifold (OONM) which provides an online controller based on the prediction on the first and second order information of the function sequence. To quantify the proposed algorithm, we leverage the notion of regret 
    
[^34]: CINA：胎儿脑的时空表示的条件隐式神经图谱

    CINA: Conditional Implicit Neural Atlas for Spatio-Temporal Representation of Fetal Brains

    [https://arxiv.org/abs/2403.08550](https://arxiv.org/abs/2403.08550)

    CINA提出了一种条件隐式神经图谱（CINA），可以生成胎儿大脑的时空图谱，而无需仿射或非刚性配准，训练后可构建忠实的组织概率图，同时适用于神经型和病理性大脑。

    

    我们引入了一种用于从神经型和病理性胎儿大脑的磁共振图像（MRI）生成时空图谱的条件隐式神经图谱（CINA），完全独立于仿射或非刚性配准。在训练过程中，CINA学习了胎儿大脑的一般表示，并将特定于主体的信息编码到潜在代码中。训练后，CINA可以为任何孕龄（GA）和训练领域内涵盖的解剖变化构建忠实的胎儿大脑图谱的组织概率图。因此，CINA能够表示神经型和病理性大脑。此外，经过训练的CINA模型可以通过测试时优化潜在代码来适应未知主体的大脑MRI。然后，CINA可以生成针对特定主体的概率组织图。我们在来自t的总共198例正常和异常胎儿大脑的T2加权MRI上评估了我们的方法。

    arXiv:2403.08550v1 Announce Type: new  Abstract: We introduce a conditional implicit neural atlas (CINA) for spatio-temporal atlas generation from Magnetic Resonance Images (MRI) of the neurotypical and pathological fetal brain, that is fully independent of affine or non-rigid registration. During training, CINA learns a general representation of the fetal brain and encodes subject specific information into latent code. After training, CINA can construct a faithful atlas with tissue probability maps of the fetal brain for any gestational age (GA) and anatomical variation covered within the training domain. Thus, CINA is competent to represent both, neurotypical and pathological brains. Furthermore, a trained CINA model can be fit to brain MRI of unseen subjects via test-time optimization of the latent code. CINA can then produce probabilistic tissue maps tailored to a particular subject. We evaluate our method on a total of 198 T2 weighted MRI of normal and abnormal fetal brains from t
    
[^35]: 语言模型与过度训练以及下游任务可靠扩展

    Language models scale reliably with over-training and on downstream tasks

    [https://arxiv.org/abs/2403.08540](https://arxiv.org/abs/2403.08540)

    本研究解决了语言模型缩放研究中过度训练和下游任务性能评估之间的差距。

    

    缩放规律对于开发语言模型是有用的指导，但当前的缩放研究与语言模型最终训练和评估之间仍然存在差距。本文解决了过度训练和基于下游任务表现进行比较方面的这两个缺点。

    arXiv:2403.08540v1 Announce Type: new  Abstract: Scaling laws are useful guides for developing language models, but there are still gaps between current scaling studies and how language models are ultimately trained and evaluated. For instance, scaling is usually studied in the compute-optimal training regime (i.e., "Chinchilla optimal" regime); however, in practice, models are often over-trained to reduce inference costs. Moreover, scaling laws mostly predict loss on next-token prediction, but ultimately models are compared based on downstream task performance. In this paper, we address both shortcomings. To do so, we create a testbed of 104 models with 0.011B to 6.9B parameters trained with various numbers of tokens on three data distributions. First, we investigate scaling in the over-trained regime. We fit scaling laws that extrapolate in both the number of model parameters and the ratio of training tokens to parameters. This enables us to predict the validation loss of a 1.4B para
    
[^36]: HOLMES: 基于HOLonym-MEronym的语义检查技术用于卷积图像分类器

    HOLMES: HOLonym-MEronym based Semantic inspection for Convolutional Image Classifiers

    [https://arxiv.org/abs/2403.08536](https://arxiv.org/abs/2403.08536)

    HOLMES提出了一种新技术，通过将标签分解为一组相关概念并提供部件级解释，来帮助理解和解释卷积图像分类模型。

    

    卷积神经网络(CNNs)如今是计算机视觉中的首选模型，因为它们能够自动化视觉任务中的特征提取过程。然而，在训练期间获得的知识完全是亚符号的，因此难以理解和解释给最终用户。本文提出了一种名为HOLMES (HOLonym-MEronym based Semantic inspection)的新技术，它将标签分解为一组相关概念，并为图像分类模型提供部件级解释。具体来说，HOLMES利用本体论、网络抓取和迁移学习来自动构建给定holonym (类别)的meronym (部件)检测器。然后，它在meronym级别生成热图，最后通过使用遮挡图像对CNN的holonym进行探测，突出每个部件对分类输出的重要性。与最先进的显著性方法相比，HOLMES

    arXiv:2403.08536v1 Announce Type: cross  Abstract: Convolutional Neural Networks (CNNs) are nowadays the model of choice in Computer Vision, thanks to their ability to automatize the feature extraction process in visual tasks. However, the knowledge acquired during training is fully subsymbolic, and hence difficult to understand and explain to end users. In this paper, we propose a new technique called HOLMES (HOLonym-MEronym based Semantic inspection) that decomposes a label into a set of related concepts, and provides component-level explanations for an image classification model. Specifically, HOLMES leverages ontologies, web scraping and transfer learning to automatically construct meronym (parts)-based detectors for a given holonym (class). Then, it produces heatmaps at the meronym level and finally, by probing the holonym CNN with occluded images, it highlights the importance of each part on the classification output. Compared to state-of-the-art saliency methods, HOLMES takes a 
    
[^37]: 从弱到强：使用自适应变点检测和主动学习进行声音事件标签

    From Weak to Strong Sound Event Labels using Adaptive Change-Point Detection and Active Learning

    [https://arxiv.org/abs/2403.08525](https://arxiv.org/abs/2403.08525)

    提出一种基于自适应变点检测和主动学习的音频录制分割方法，通过预测模型和变点检测逐步生成高质量的强标签。

    

    在这项工作中，我们提出了一种基于自适应变点检测（A-CPD）的音频录制分割方法，用于机器引导的音频录制段的弱标签注释。目标是最大化关于目标声音时间激活的信息获取量。对于每个未标记的音频录制，我们使用预测模型来推导概率曲线，用于指导注释。预测模型最初在可用的带标注声音事件数据上进行预训练，这些数据的类与未标记数据集中的类不相交。然后，预测模型逐渐适应注释者在主动学习循环中提供的注释。用于引导弱标签注释者走向强标签的查询是使用这些概率上的变点检测导出的。我们展示，即使在有限的注释预算下，也可以获得高质量的强标签，并展示了优势。

    arXiv:2403.08525v1 Announce Type: cross  Abstract: In this work we propose an audio recording segmentation method based on an adaptive change point detection (A-CPD) for machine guided weak label annotation of audio recording segments. The goal is to maximize the amount of information gained about the temporal activation's of the target sounds. For each unlabeled audio recording, we use a prediction model to derive a probability curve used to guide annotation. The prediction model is initially pre-trained on available annotated sound event data with classes that are disjoint from the classes in the unlabeled dataset. The prediction model then gradually adapts to the annotations provided by the annotator in an active learning loop. The queries used to guide the weak label annotator towards strong labels are derived using change point detection on these probabilities. We show that it is possible to derive strong labels of high quality even with a limited annotation budget, and show favor
    
[^38]: DiPrompT: 多潜在领域泛化的解耦提示调整在联邦学习中

    DiPrompT: Disentangled Prompt Tuning for Multiple Latent Domain Generalization in Federated Learning

    [https://arxiv.org/abs/2403.08506](https://arxiv.org/abs/2403.08506)

    提出了一种名为DiPrompT的解耦提示调整方法，通过学习适应提示来解决在联邦学习中对领域泛化的限制。

    

    arXiv:2403.08506v1 公告类型: 跨领域  摘要: 联邦学习（FL）已经成为一种强大的学习范式，可以从分散的数据中学习，而联邦领域泛化进一步考虑测试数据集（目标领域）不存在于分散的训练数据（源领域）中。然而，大多数现有的FL方法假设在训练过程中提供了领域标签，并且它们的评估对领域数量施加明确的约束，这些约束必须严格匹配客户端的数量。由于现实世界中众多边缘设备的被低效利用以及额外的跨客户端领域注释，这些限制可能是不切实际的，并涉及潜在的隐私泄漏。在本文中，我们提出了一种高效而新颖的方法，称为解耦提示调整（DiPrompT），该方法通过分布式学习适应提示来处理上述限制，来实现领域泛化。具体而言，我们首先设计了两种提示类型，即

    arXiv:2403.08506v1 Announce Type: cross  Abstract: Federated learning (FL) has emerged as a powerful paradigm for learning from decentralized data, and federated domain generalization further considers the test dataset (target domain) is absent from the decentralized training data (source domains). However, most existing FL methods assume that domain labels are provided during training, and their evaluation imposes explicit constraints on the number of domains, which must strictly match the number of clients. Because of the underutilization of numerous edge devices and additional cross-client domain annotations in the real world, such restrictions may be impractical and involve potential privacy leaks. In this paper, we propose an efficient and novel approach, called Disentangled Prompt Tuning (DiPrompT), a method that tackles the above restrictions by learning adaptive prompts for domain generalization in a distributed manner. Specifically, we first design two types of prompts, i.e., 
    
[^39]: SoK：减少经过精细调整的语言模型对成员推断攻击的脆弱性

    SoK: Reducing the Vulnerability of Fine-tuned Language Models to Membership Inference Attacks

    [https://arxiv.org/abs/2403.08481](https://arxiv.org/abs/2403.08481)

    首次系统评估了大型经过微调的语言模型对成员推断攻击的脆弱性，以及相关因素和不同防御策略的有效性。

    

    自然语言处理模型近年来经历了显著增长，许多应用程序都是基于它们构建的。许多这些应用程序需要在定制的专有数据集上对通用基础模型进行微调。这种微调数据特别容易包含个人或敏感信息，从而增加了隐私风险。成员推断攻击是评估机器学习模型隐私泄漏的常用攻击方法。然而，目前对语言模型对这种攻击的脆弱性影响因素及在语言领域中不同防御策略的适用性的研究有限。我们提供了对大型经过微调的语言模型对成员推断攻击的脆弱性、涉及的各种因素以及不同防御策略有效性的第一次系统评估。

    arXiv:2403.08481v1 Announce Type: new  Abstract: Natural language processing models have experienced a significant upsurge in recent years, with numerous applications being built upon them. Many of these applications require fine-tuning generic base models on customized, proprietary datasets. This fine-tuning data is especially likely to contain personal or sensitive information about individuals, resulting in increased privacy risk. Membership inference attacks are the most commonly employed attack to assess the privacy leakage of a machine learning model. However, limited research is available on the factors that affect the vulnerability of language models to this kind of attack, or on the applicability of different defense strategies in the language domain. We provide the first systematic review of the vulnerability of fine-tuned large language models to membership inference attacks, the various factors that come into play, and the effectiveness of different defense strategies. We f
    
[^40]: 通过稀疏插值专家释放元调整的力量，用于少样本泛化

    Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts

    [https://arxiv.org/abs/2403.08477](https://arxiv.org/abs/2403.08477)

    本文提出了一种名为Sparse MetA-Tuning（SMAT）的方法，通过灵感来自稀疏专家混合方法，成功克服了域外任务敏感性，实现了增强视觉基础模型转移能力的目标。

    

    传统智慧建议参数高效的微调基础模型，是视觉迁移学习的最先进方法，取代了诸如元学习之类的丰富文献。为了兼顾两者的利益，元调整引入了基础模型的随后优化阶段，但迄今只展现了有限的成功，关键地在域外（OOD）任务上表现不佳。本文介绍了一种灵感来自稀疏专家混合方法的 Sparse MetA-Tuning（SMAT）方法，它经过训练以自动地为每个任务隔离预训练参数子集以进行元调整。SMAT成功克服了OOD敏感性，并实现了增强视觉基础模型转移能力的承诺。我们在Meta-Dataset与额外的OO挑战组合上建立了新的最先进结果。

    arXiv:2403.08477v1 Announce Type: cross  Abstract: Conventional wisdom suggests parameter-efficient fine-tuning of foundation models as the state-of-the-art method for transfer learning in vision, replacing the rich literature of alternatives such as meta-learning. In trying to harness the best of both worlds, meta-tuning introduces a subsequent optimization stage of foundation models but has so far only shown limited success and crucially tends to underperform on out-of-domain (OOD) tasks. In this paper, we introduce Sparse MetA-Tuning (SMAT), a method inspired by sparse mixture-of-experts approaches and trained to isolate subsets of pre-trained parameters automatically for meta-tuning on each task. SMAT successfully overcomes OOD sensitivity and delivers on the promise of enhancing the transfer abilities of vision foundation models beyond parameter-efficient finetuning. We establish new state-of-the-art results on a challenging combination of Meta-Dataset augmented with additional OO
    
[^41]: 人类对潜在扩散模型的排列分析

    An Analysis of Human Alignment of Latent Diffusion Models

    [https://arxiv.org/abs/2403.08469](https://arxiv.org/abs/2403.08469)

    分析了潜在扩散模型的排列对人类响应的对齐情况，并发现模型的对齐性与ImageNet-1k相当，去噪U-Net的最对齐层是中间层而不是瓶颈，并且文本调节在高噪声水平下提高了对齐性。

    

    潜在扩散模型在大量数据训练下，展现出在图像合成方面出色的性能。当用于分类时，它们与人类有高误差一致性和低纹理偏差。此外，先前的工作证明了它们的瓶颈层表示可以分解为语义方向。在这项研究中，我们分析了这些表示与人类在三元奇一奇任务上对齐的情况。我们发现，尽管前述观察到：I）与人类的表示对齐性与仅在ImageNet-1k上训练的模型相媲美。II）去噪U-Net的最对齐层是中间层而不是瓶颈。III）文本调节为高噪声水平提高了对齐性，暗示着抽象文本信息的重要性，尤其是在生成的早期阶段。

    arXiv:2403.08469v1 Announce Type: new  Abstract: Diffusion models, trained on large amounts of data, showed remarkable performance for image synthesis. They have high error consistency with humans and low texture bias when used for classification. Furthermore, prior work demonstrated the decomposability of their bottleneck layer representations into semantic directions. In this work, we analyze how well such representations are aligned to human responses on a triplet odd-one-out task. We find that despite the aforementioned observations: I) The representational alignment with humans is comparable to that of models trained only on ImageNet-1k. II) The most aligned layers of the denoiser U-Net are intermediate layers and not the bottleneck. III) Text conditioning greatly improves alignment at high noise levels, hinting at the importance of abstract textual information, especially in the early stage of generation.
    
[^42]: 具有隐式引导的扩散模型用于医学异常检测

    Diffusion Models with Implicit Guidance for Medical Anomaly Detection

    [https://arxiv.org/abs/2403.08464](https://arxiv.org/abs/2403.08464)

    本文引入了一种名为THOR（Temporal Harmonization for Optimal Restoration）的方法，通过在时间异常图中整合隐式引导来改进去噪过程，旨在保持未受病变影响区域健康组织的完整性。

    

    扩散模型通过改善病理图像向伪健康等价物的转换，推动了无监督异常检测的发展。本文引入了一种名为THOR（Temporal Harmonization for Optimal Restoration）的方法，通过在时间异常图中整合隐式引导来改进去噪过程，旨在保持未受病变影响区域健康组织的完整性。比较评估结果表明，THOR在检测和分割脑部MRI和手腕X光中的异常方面优于现有的基于扩散的方法。

    arXiv:2403.08464v1 Announce Type: cross  Abstract: Diffusion models have advanced unsupervised anomaly detection by improving the transformation of pathological images into pseudo-healthy equivalents. Nonetheless, standard approaches may compromise critical information during pathology removal, leading to restorations that do not align with unaffected regions in the original scans. Such discrepancies can inadvertently increase false positive rates and reduce specificity, complicating radiological evaluations. This paper introduces Temporal Harmonization for Optimal Restoration (THOR), which refines the de-noising process by integrating implicit guidance through temporal anomaly maps. THOR aims to preserve the integrity of healthy tissue in areas unaffected by pathology. Comparative evaluations show that THOR surpasses existing diffusion-based methods in detecting and segmenting anomalies in brain MRIs and wrist X-rays. Code: https://github.com/ci-ber/THOR_DDPM.
    
[^43]: 基于语法模型似然比的作者身份验证

    Authorship Verification based on the Likelihood Ratio of Grammar Models

    [https://arxiv.org/abs/2403.08462](https://arxiv.org/abs/2403.08462)

    提出了一种基于计算作者文件在候选作者语法模型与参考群体语法模型下的可能性比率的方法，用以解决作者身份验证中存在的科学解释不足和难以解释的问题

    

    作者身份验证（AV）是分析一组文件以确定它们是否由特定作者撰写的过程。现有的最先进AV方法使用计算解决方案，对于其功能没有合理的科学解释，并且常常难以解释给分析人员。为解决这个问题，我们提出了一种方法，依赖于计算一个我们称之为 $\lambda_G$（LambdaG）的量：候选作者的上下文语法模型给出的文档的可能性与参考群体的上下文语法模型给出的相同文档的可能性之间的比率。这些语法模型是使用仅针对语法特征进行训练的 $n$-gram语言模型进行估计的。尽管不需要大量数据进行训练，LambdaG...

    arXiv:2403.08462v1 Announce Type: new  Abstract: Authorship Verification (AV) is the process of analyzing a set of documents to determine whether they were written by a specific author. This problem often arises in forensic scenarios, e.g., in cases where the documents in question constitute evidence for a crime. Existing state-of-the-art AV methods use computational solutions that are not supported by a plausible scientific explanation for their functioning and that are often difficult for analysts to interpret. To address this, we propose a method relying on calculating a quantity we call $\lambda_G$ (LambdaG): the ratio between the likelihood of a document given a model of the Grammar for the candidate author and the likelihood of the same document given a model of the Grammar for a reference population. These Grammar Models are estimated using $n$-gram language models that are trained solely on grammatical features. Despite not needing large amounts of data for training, LambdaG st
    
[^44]: 《基于演员评论者物理信息神经李雅普诺夫控制》

    Actor-Critic Physics-informed Neural Lyapunov Control

    [https://arxiv.org/abs/2403.08448](https://arxiv.org/abs/2403.08448)

    提出了一种新方法，通过使用祖博夫的偏微分方程（PDE）来训练神经网络控制器，以及对应的李雅普诺夫证书，以最大化区域吸引力，并尊重激励约束。

    

    设计具有可证保证的稳定化任务控制策略是非线性控制中的一个长期问题。关键的性能指标是产生区域吸引力的大小，这基本上充当了封闭环系统对不确定性的弹性“边界”。本文提出了一种新方法，用于训练一个稳定的神经网络控制器以及其对应的李雅普诺夫证书，旨在最大化产生的区域吸引力，同时尊重激励约束。我们方法的关键之处在于使用祖博夫的偏微分方程（PDE），该方程精确地表征了给定控制策略的真实区域吸引力。我们的框架遵循演员评论者模式，我们在改进控制策略（演员）和学习祖博夫函数（评论者）之间交替进行。最后，我们通过调用SMT求解器计算出最大的可证区域吸引力。

    arXiv:2403.08448v1 Announce Type: new  Abstract: Designing control policies for stabilization tasks with provable guarantees is a long-standing problem in nonlinear control. A crucial performance metric is the size of the resulting region of attraction, which essentially serves as a robustness "margin" of the closed-loop system against uncertainties. In this paper, we propose a new method to train a stabilizing neural network controller along with its corresponding Lyapunov certificate, aiming to maximize the resulting region of attraction while respecting the actuation constraints. Crucial to our approach is the use of Zubov's Partial Differential Equation (PDE), which precisely characterizes the true region of attraction of a given control policy. Our framework follows an actor-critic pattern where we alternate between improving the control policy (actor) and learning a Zubov function (critic). Finally, we compute the largest certifiable region of attraction by invoking an SMT solver
    
[^45]: COSTREAM：边缘-云环境中运算符放置的学习成本模型

    COSTREAM: Learned Cost Models for Operator Placement in Edge-Cloud Environments

    [https://arxiv.org/abs/2403.08444](https://arxiv.org/abs/2403.08444)

    COSTREAM提出了一种可在边缘-云环境中准确预测流查询执行成本的学习成本模型，并通过优化运算符的放置，实现了高达21倍的中位数加速。

    

    在这项工作中，我们提出了COSTREAM，这是一种新颖的学习成本模型，适用于分布式流处理系统，在边缘-云环境中对流查询的执行成本进行准确预测。该成本模型可用于在异构硬件上找到运算符的初始放置，这在这些环境中尤为重要。在我们的评估中，我们展示了COSTREAM可以为初始运算符放置产生高度准确的成本估算，甚至能推广到未见过的放置、查询和硬件。当使用COSTREAM来优化流处理运算符的放置时，与基线相比，可实现大约21倍的中位数加速。

    arXiv:2403.08444v1 Announce Type: cross  Abstract: In this work, we present COSTREAM, a novel learned cost model for Distributed Stream Processing Systems that provides accurate predictions of the execution costs of a streaming query in an edge-cloud environment. The cost model can be used to find an initial placement of operators across heterogeneous hardware, which is particularly important in these environments. In our evaluation, we demonstrate that COSTREAM can produce highly accurate cost estimates for the initial operator placement and even generalize to unseen placements, queries, and hardware. When using COSTREAM to optimize the placements of streaming operators, a median speed-up of around 21x can be achieved compared to baselines.
    
[^46]: 再现性和几何内在维度性：对图神经网络研究的调查

    Reproducibility and Geometric Intrinsic Dimensionality: An Investigation on Graph Neural Network Research

    [https://arxiv.org/abs/2403.08438](https://arxiv.org/abs/2403.08438)

    本文研究了图神经网络研究中的再现性和几何内在维度性问题，并引入机器学习中的再现性本体论，以及探讨了维度诅咒对数据收集、表示和分析的挑战。

    

    机器学习研究中复制和可再现性的困难近年来成为一个突出的话题。确保机器学习研究结果的可靠性需要可再现性，通过使用相同的代码和数据验证研究结果的可靠性。这促进了开放和可访问的研究、稳健的实验工作流程以及新发现的快速整合。评估研究出版物支持再现性的程度是本文的一个目标。为此，我们引入了一个机器学习中的再现性本体论，并将其应用于图神经网络的方法。在这些努力的基础上，我们转向机器学习中的另一个关键挑战，即维度诅咒，它在数据收集、表示和分析方面带来挑战，使得更难找到代表性数据。

    arXiv:2403.08438v1 Announce Type: cross  Abstract: Difficulties in replication and reproducibility of empirical evidences in machine learning research have become a prominent topic in recent years. Ensuring that machine learning research results are sound and reliable requires reproducibility, which verifies the reliability of research findings using the same code and data. This promotes open and accessible research, robust experimental workflows, and the rapid integration of new findings. Evaluating the degree to which research publications support these different aspects of reproducibility is one goal of the present work. For this we introduce an ontology of reproducibility in machine learning and apply it to methods for graph neural networks. Building on these efforts we turn towards another critical challenge in machine learning, namely the curse of dimensionality, which poses challenges in data collection, representation, and analysis, making it harder to find representative data 
    
[^47]: DeepCSHAP: 利用Shapley Values解释深度复数神经网络

    DeepCSHAP: Utilizing Shapley Values to Explain Deep Complex-Valued Neural Networks

    [https://arxiv.org/abs/2403.08428](https://arxiv.org/abs/2403.08428)

    本文提出了DeepCSHAP算法，通过利用Shapley Values解释复数神经网络的输出，填补了对于这种类型神经网络的解释算法的空白。

    

    arXiv:2403.08428v1 公告类型：新 摘要：深度神经网络广泛应用于学术界以及企业和公共应用程序，包括诸如医疗保健和自动驾驶等安全关键应用。解释其输出的能力对于安全原因以及申请者的接受至关重要。已经提出了大量方法来解释实数神经网络。最近，复数神经网络作为一种新的神经网络类别出现，处理复数输入数据而无需将其投影到$\mathbb{R}^2$。这带来了为这种神经网络开发解释算法的需求。在本文中，我们提供了这些发展。虽然我们专注于将广泛应用的DeepSHAP算法调整到复数域，但我们还提供了适用于复数神经网络的四种基于梯度的解释方法的版本。我们评估了所有提出的解释方法的解释质量。

    arXiv:2403.08428v1 Announce Type: new  Abstract: Deep Neural Networks are widely used in academy as well as corporate and public applications, including safety critical applications such as health care and autonomous driving. The ability to explain their output is critical for safety reasons as well as acceptance among applicants. A multitude of methods have been proposed to explain real-valued neural networks. Recently, complex-valued neural networks have emerged as a new class of neural networks dealing with complex-valued input data without the necessity of projecting them onto $\mathbb{R}^2$. This brings up the need to develop explanation algorithms for this kind of neural networks. In this paper we provide these developments. While we focus on adapting the widely used DeepSHAP algorithm to the complex domain, we also present versions of four gradient based explanation methods suitable for use in complex-valued neural networks. We evaluate the explanation quality of all presented a
    
[^48]: 一种基于机器学习的移动平台，用于视觉确定阴茎病理学病因的开发和性能

    The Development and Performance of a Machine Learning Based Mobile Platform for Visually Determining the Etiology of Penile Pathology

    [https://arxiv.org/abs/2403.08417](https://arxiv.org/abs/2403.08417)

    通过使用机器学习算法开发了一个移动平台，可以视觉确定阴茎病理的病因，从而提高对性健康服务的获取公平性。

    

    机器学习算法可以促进低成本、用户引导的视觉诊断平台，以解决获取性健康服务的不平等问题。我们使用原始和增广图像为五种阴茎疾病开发了临床图像数据集: 疱疹爆发、梅毒性溃疡、阴茎念珠菌病、阴茎癌和生殖疣。我们使用U-net架构模型进行语义像素分割，将图像分为背景或主体图像，使用Inception-ResNet版本2神经架构将每个像素分类为患病或非患病，并使用GradCAM++生成显著性图。我们对图像数据库中随机选取的91%样本进行了150次图像训练，然后在其余9%的图像上评估了模型，评估了召回率（或灵敏度）、精确度、特异度和F1分数（准确度）。在验证数据集中的239张图像中，45张（18.8%）是生殖疣，43张（18.0%）是HSV感染，29张（12.1%）是

    arXiv:2403.08417v1 Announce Type: cross  Abstract: Machine-learning algorithms can facilitate low-cost, user-guided visual diagnostic platforms for addressing disparities in access to sexual health services. We developed a clinical image dataset using original and augmented images for five penile diseases: herpes eruption, syphilitic chancres, penile candidiasis, penile cancer, and genital warts. We used a U-net architecture model for semantic pixel segmentation into background or subject image, the Inception-ResNet version 2 neural architecture to classify each pixel as diseased or non-diseased, and a salience map using GradCAM++. We trained the model on a random 91% sample of the image database using 150 epochs per image, and evaluated the model on the remaining 9% of images, assessing recall (or sensitivity), precision, specificity, and F1-score (accuracy). Of the 239 images in the validation dataset, 45 (18.8%) were of genital warts, 43 (18.0%) were of HSV infection, 29 (12.1%) wer
    
[^49]: 用于火灾危险预测的因果图神经网络

    Causal Graph Neural Networks for Wildfire Danger Prediction

    [https://arxiv.org/abs/2403.08414](https://arxiv.org/abs/2403.08414)

    通过将因果性与图神经网络相结合，模型能够显式地建模复杂变量之间的因果机理，提高了火灾模式预测的性能。

    

    火灾预测因天气条件、植被类型和人类活动等不同因素的复杂相互作用而变得难以预测。深度学习模型展现了直接从数据中学习处理这种复杂性的潜力。然而，为了支持关键决策，我们认为我们需要适合正确原因的模型；也就是说，学到的隐式规则应该以推动火灾的基本过程为基础。在这个方向上，我们建议将因果性与图神经网络（GNNs）相结合，通过图学习显式地对复杂变量之间的因果机理建模。因果邻接矩阵考虑了不同变量之间的协同作用，并消除了高度相关影响之间的虚假联系。我们的方法的有效性通过在欧洲北部和地中海生物群系中优越性能预测火灾模式而得到证明。收益是

    arXiv:2403.08414v1 Announce Type: cross  Abstract: Wildfire forecasting is notoriously hard due to the complex interplay of different factors such as weather conditions, vegetation types and human activities. Deep learning models show promise in dealing with this complexity by learning directly from data. However, to inform critical decision making, we argue that we need models that are right for the right reasons; that is, the implicit rules learned should be grounded by the underlying processes driving wildfires. In that direction, we propose integrating causality with Graph Neural Networks (GNNs) that explicitly model the causal mechanism among complex variables via graph learning. The causal adjacency matrix considers the synergistic effect among variables and removes the spurious links from highly correlated impacts. Our methodology's effectiveness is demonstrated through superior performance forecasting wildfire patterns in the European boreal and mediterranean biome. The gain is
    
[^50]: 减小Jeffries-Matusita距离：提高深度分类模型泛化性能的新型损失函数

    Reduced Jeffries-Matusita distance: A Novel Loss Function to Improve Generalization Performance of Deep Classification Models

    [https://arxiv.org/abs/2403.08408](https://arxiv.org/abs/2403.08408)

    提出了一种减小Jeffries-Matusita距离的损失函数，用于训练深度分类模型以减少过拟合问题。

    

    深度神经网络在分类任务中的泛化性能是机器学习研究中的一个主要关注点。尽管广泛采用诸如数据增强、伪标记、正则化和集成学习等技术来减少过拟合问题，但仍需要通过其他方法来提高性能。近年来，已在理论上证明损失函数特性，如其Lipschitz性和最大值，影响深度神经网络的泛化性能，这可以被用作提出新的距离度量的指导。本文通过分析上述特性，引入了一种称为减小Jeffries-Matusita距离的距离作为深度分类模型训练的损失函数，以减少过拟合问题。在实验中，我们在两个不同问题中评估了新的损失函数：计算机视觉中的图像分类。

    arXiv:2403.08408v1 Announce Type: new  Abstract: The generalization performance of deep neural networks in classification tasks is a major concern in machine learning research. Despite widespread techniques used to diminish the over-fitting issue such as data augmentation, pseudo-labeling, regularization, and ensemble learning, this performance still needs to be enhanced with other approaches. In recent years, it has been theoretically demonstrated that the loss function characteristics i.e. its Lipschitzness and maximum value affect the generalization performance of deep neural networks which can be utilized as a guidance to propose novel distance measures. In this paper, by analyzing the aforementioned characteristics, we introduce a distance called Reduced Jeffries-Matusita as a loss function for training deep classification models to reduce the over-fitting issue. In our experiments, we evaluate the new loss function in two different problems: image classification in computer visio
    
[^51]: FSDR：一种基于深度学习的离散松弛特征选择算法，用于伪时间序列数据

    FSDR: A Novel Deep Learning-based Feature Selection Algorithm for Pseudo Time-Series Data using Discrete Relaxation

    [https://arxiv.org/abs/2403.08403](https://arxiv.org/abs/2403.08403)

    FSDR是一种基于深度学习的特征选择算法，通过离散松弛学习重要特征，适用于高维的伪时间序列数据

    

    应用于伪时间序列（PTS）数据的传统特征选择算法，由于具有高维数据的不切实际的计算复杂性，针对这一挑战，我们引入了一种深度学习（DL）-基于特征选择算法：离散松弛特征选择（FSDR），专门针对PTS数据。与现有的特征选择算法不同，FSDR通过离散松弛学习重要特征作为模型参数，这是指通过连续优化问题近似离散优化问题的过程。FSDR能够适应大量特征维度，这是现有DL或传统方法难以实现的。通过对高光谱数据集（即PTS数据的一种）进行测试，我们的实验结果表明FSDR

    arXiv:2403.08403v1 Announce Type: new  Abstract: Conventional feature selection algorithms applied to Pseudo Time-Series (PTS) data, which consists of observations arranged in sequential order without adhering to a conventional temporal dimension, often exhibit impractical computational complexities with high dimensional data. To address this challenge, we introduce a Deep Learning (DL)-based feature selection algorithm: Feature Selection through Discrete Relaxation (FSDR), tailored for PTS data. Unlike the existing feature selection algorithms, FSDR learns the important features as model parameters using discrete relaxation, which refers to the process of approximating a discrete optimisation problem with a continuous one. FSDR is capable of accommodating a high number of feature dimensions, a capability beyond the reach of existing DL-based or traditional methods. Through testing on a hyperspectral dataset (i.e., a type of PTS data), our experimental results demonstrate that FSDR out
    
[^52]: 优化风险敏感的人工智能混合团队

    Optimizing Risk-averse Human-AI Hybrid Teams

    [https://arxiv.org/abs/2403.08386](https://arxiv.org/abs/2403.08386)

    提出了一种经理通过强化学习方案学习如何在混合人工智能团队中最佳分配决策责任，并最小化不良团队行为导致的委派变更次数。

    

    我们预计随着人类和人工智能系统在所谓的混合团队中合作的增加，这种合作的频率将增加。随着人工智能系统的熟练度提高和其采用变得更加广泛，预计协作将增加。但是，它们的行为并非无误，从而使混合团队成为一种非常合适的解决方案。因此，我们考虑了改进这些由人类和人工智能系统组成的团队的绩效的方法。对于混合团队，我们将人类和人工智能系统都称为代理。为了提高团队的表现，我们提出了一种经理，该经理通过标准的强化学习方案学习如何在一段时间内最好地委派决策责任给任何一个代理。我们进一步引导经理的学习，让他们最小化因不良团队行为而导致的委派变更次数。我们展示了管理者绩效的最优性。

    arXiv:2403.08386v1 Announce Type: new  Abstract: We anticipate increased instances of humans and AI systems working together in what we refer to as a hybrid team. The increase in collaboration is expected as AI systems gain proficiency and their adoption becomes more widespread. However, their behavior is not error-free, making hybrid teams a very suitable solution. As such, we consider methods for improving performance for these teams of humans and AI systems. For hybrid teams, we will refer to both the humans and AI systems as agents. To improve team performance over that seen for agents operating individually, we propose a manager which learns, through a standard Reinforcement Learning scheme, how to best delegate, over time, the responsibility of taking a decision to any of the agents. We further guide the manager's learning so they also minimize how many changes in delegation are made resulting from undesirable team behavior. We demonstrate the optimality of our manager's performa
    
[^53]: 用于拉曼光谱确定微凝胶尺寸的非线性流形学习

    Nonlinear Manifold Learning Determines Microgel Size from Raman Spectroscopy

    [https://arxiv.org/abs/2403.08376](https://arxiv.org/abs/2403.08376)

    本研究提出了三种替代机器学习工作流程来通过非线性流形学习技术从拉曼光谱测量中准确可靠地确定聚合物尺寸。

    

    聚合物粒子尺寸构成了聚合过程中产品质量的一个关键特征。拉曼光谱是一种已建立且可靠的过程分析技术，可用于在线浓度监测。最近的方法和一些理论考虑显示了拉曼信号与颗粒尺寸之间的相关性，但尚不能精确可靠地从拉曼光谱测量中确定聚合物尺寸。基于此，我们提出了三种替代机器学习工作流程来执行这项任务，其中都涉及扩散映射，这是一种非线性流形学习技术用于降维: (i) 直接从扩散映射，(ii) 替代扩散映射，和 (iii) 符合自动编码器神经网络。我们将这些工作流程应用于包含47个微凝胶（交联聚合物）样本的拉曼光谱数据集，这些样本的直径范围为208nm到483 nm，且其尺寸是通过动态光散射测量得到的。

    arXiv:2403.08376v1 Announce Type: new  Abstract: Polymer particle size constitutes a crucial characteristic of product quality in polymerization. Raman spectroscopy is an established and reliable process analytical technology for in-line concentration monitoring. Recent approaches and some theoretical considerations show a correlation between Raman signals and particle sizes but do not determine polymer size from Raman spectroscopic measurements accurately and reliably. With this in mind, we propose three alternative machine learning workflows to perform this task, all involving diffusion maps, a nonlinear manifold learning technique for dimensionality reduction: (i) directly from diffusion maps, (ii) alternating diffusion maps, and (iii) conformal autoencoder neural networks. We apply the workflows to a data set of Raman spectra with associated size measured via dynamic light scattering of 47 microgel (cross-linked polymer) samples in a diameter range of 208nm to 483 nm. The conformal
    
[^54]: SMART: 用于指令调整的子模块数据混合策略

    SMART: Submodular Data Mixture Strategy for Instruction Tuning

    [https://arxiv.org/abs/2403.08370](https://arxiv.org/abs/2403.08370)

    SMART引入了一种新颖的数据混合策略，利用子模块函数为任务分配重要性分数，并在微调中重新分配预算，从而在指令调整任务中取得明显优势。

    

    指令调整涉及在一组以指令格式化的数据集上对语言模型进行微调，以增强模型对未见任务的泛化能力。研究表明，在微调过程中平衡不同任务比例的重要性，但找到合适的平衡仍然具有挑战性。目前除了手动调整或依赖从业者的直觉外，尚无系统方法。在本文中，我们介绍了SMART（Submodular data Mixture strAtegy for instRuction Tuning）- 一种利用子模块函数为任务分配重要性分数的新颖数据混合策略，然后用这些分数来确定混合权重。给定微调预算，SMART重新分配任务间的预算，并从每个任务中选择非冗余样本。实验结果表明，SMART显著优于传统方法，如例子比例混合和均等分配。

    arXiv:2403.08370v1 Announce Type: cross  Abstract: Instruction Tuning involves finetuning a language model on a collection of instruction-formatted datasets in order to enhance the generalizability of the model to unseen tasks. Studies have shown the importance of balancing different task proportions during finetuning, but finding the right balance remains challenging. Unfortunately, there's currently no systematic method beyond manual tuning or relying on practitioners' intuition. In this paper, we introduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) - a novel data mixture strategy which makes use of a submodular function to assign importance scores to tasks which are then used to determine the mixture weights. Given a fine-tuning budget, SMART redistributes the budget among tasks and selects non-redundant samples from each task. Experimental results demonstrate that SMART significantly outperforms traditional methods such as examples proportional mixing and equal
    
[^55]: 针对长尾和非独立同分布数据的特征统计分开式联邦学习

    Decoupled Federated Learning on Long-Tailed and Non-IID data with Feature Statistics

    [https://arxiv.org/abs/2403.08364](https://arxiv.org/abs/2403.08364)

    本文提出了针对长尾和非独立同分布数据的特征统计分开式联邦学习框架，通过两阶段方式解决尾部类别稀疏分布导致的模型性能下降问题

    

    联邦学习旨在增强数据安全性和隐私性，但在处理长尾和非独立同分布的异构数据时面临挑战。本文探讨了一个被忽视的情景，即尾部类别在少数客户端上稀疏分布，导致使用这些类别训练的模型在客户端聚合过程中被选择的概率较低，从而导致收敛速度较慢和模型性能较差。为了解决这个问题，我们提出了一个使用特征统计的两阶段分开式联邦学习框架（DFL-FS）。在第一阶段，服务器通过蒙版局部特征统计聚类估计客户端的类别覆盖分布，以加快收敛速度和增强特征学习而不泄露隐私。在第二阶段，DFL-FS基于全局特征统计采用联邦特征再生，并利用重采样

    arXiv:2403.08364v1 Announce Type: cross  Abstract: Federated learning is designed to enhance data security and privacy, but faces challenges when dealing with heterogeneous data in long-tailed and non-IID distributions. This paper explores an overlooked scenario where tail classes are sparsely distributed over a few clients, causing the models trained with these classes to have a lower probability of being selected during client aggregation, leading to slower convergence rates and poorer model performance. To address this issue, we propose a two-stage Decoupled Federated learning framework using Feature Statistics (DFL-FS). In the first stage, the server estimates the client's class coverage distributions through masked local feature statistics clustering to select models for aggregation to accelerate convergence and enhance feature learning without privacy leakage. In the second stage, DFL-FS employs federated feature regeneration based on global feature statistics and utilizes resamp
    
[^56]: 均场微正则梯度下降

    Mean-Field Microcanonical Gradient Descent

    [https://arxiv.org/abs/2403.08362](https://arxiv.org/abs/2403.08362)

    提出了均场微正则梯度下降方法，通过同时采样多个弱耦合数据点，在控制熵损失的同时在似然拟合方面表现良好。

    

    微正则梯度下降是一种能量模型的采样过程，可实现高维分布的高效采样，通过梯度下降将样本从高熵分布（如高斯白噪声）转运至低能区域。我们将这一模型置于正则化流的框架中，显示它通常会由于在下降过程中失去不必要的熵而过度拟合。为解决这一问题，我们提出了均场微正则梯度下降，同时采样多个弱耦合数据点，允许更好地控制熵损失，同时在似然拟合方面付出较小代价。我们将这些模型应用于金融时间序列的背景中，展示了在合成和真实数据上的改进。

    arXiv:2403.08362v1 Announce Type: cross  Abstract: Microcanonical gradient descent is a sampling procedure for energy-based models allowing for efficient sampling of distributions in high dimension. It works by transporting samples from a high-entropy distribution, such as Gaussian white noise, to a low-energy region using gradient descent. We put this model in the framework of normalizing flows, showing how it can often overfit by losing an unnecessary amount of entropy in the descent. As a remedy, we propose a mean-field microcanonical gradient descent that samples several weakly coupled data points simultaneously, allowing for better control of the entropy loss while paying little in terms of likelihood fit. We study these models in the context of financial time series, illustrating the improvements on both synthetic and real data.
    
[^57]: 利用自动化机器学习的数据增强方法及与传统数据增强方法性能比较

    Data augmentation with automated machine learning: approaches and performance comparison with classical data augmentation methods

    [https://arxiv.org/abs/2403.08352](https://arxiv.org/abs/2403.08352)

    自动化机器学习的数据增强方法旨在自动化数据增强过程，为改善机器学习模型泛化性能提供了更高效的方式。

    

    数据增强被认为是常用于提高机器学习模型泛化性能的最重要的正则化技术。它主要涉及应用适当的数据转换操作，以创建具有所需属性的新数据样本。尽管其有效性，这一过程通常具有挑战性，因为手动创建和测试不同候选增强及其超参数需耗费大量时间。自动化数据增强方法旨在自动化这一过程。最先进的方法通常依赖于自动化机器学习（AutoML）原则。本研究提供了基于AutoML的数据增强技术的全面调查。我们讨论了使用AutoML实现数据增强的各种方法，包括数据操作、数据集成和数据合成技术。我们详细讨论了技术

    arXiv:2403.08352v1 Announce Type: cross  Abstract: Data augmentation is arguably the most important regularization technique commonly used to improve generalization performance of machine learning models. It primarily involves the application of appropriate data transformation operations to create new data samples with desired properties. Despite its effectiveness, the process is often challenging because of the time-consuming trial and error procedures for creating and testing different candidate augmentations and their hyperparameters manually. Automated data augmentation methods aim to automate the process. State-of-the-art approaches typically rely on automated machine learning (AutoML) principles. This work presents a comprehensive survey of AutoML-based data augmentation techniques. We discuss various approaches for accomplishing data augmentation with AutoML, including data manipulation, data integration and data synthesis techniques. We present extensive discussion of technique
    
[^58]: STMPL：人体软组织模拟

    STMPL: Human Soft-Tissue Simulation

    [https://arxiv.org/abs/2403.08344](https://arxiv.org/abs/2403.08344)

    本论文提出了一种基于数据驱动的软组织模拟方法，通过在SMPL模型的基础上引入软组织层和外部力作用的直观表示，实现了对人体形状和软组织进行快速逼真模拟。

    

    在各种应用中，如虚拟现实和游戏中，模拟人体软组织在与外部物体交互过程中的变形至关重要。传统上，有限元方法（FEM）被用于此目的，但它们往往速度较慢且资源密集。本文提出了一种人体形状和软组织的统一表示方法，采用了数据驱动的非刚体变形模拟器。该方法实现了快速模拟逼真的交互。

    arXiv:2403.08344v1 Announce Type: cross  Abstract: In various applications, such as virtual reality and gaming, simulating the deformation of soft tissues in the human body during interactions with external objects is essential. Traditionally, Finite Element Methods (FEM) have been employed for this purpose, but they tend to be slow and resource-intensive. In this paper, we propose a unified representation of human body shape and soft tissue with a data-driven simulator of non-rigid deformations. This approach enables rapid simulation of realistic interactions.   Our method builds upon the SMPL model, which generates human body shapes considering rigid transformations. We extend SMPL by incorporating a soft tissue layer and an intuitive representation of external forces applied to the body during object interactions. Specifically, we mapped the 3D body shape and soft tissue and applied external forces to 2D UV maps. Leveraging a UNET architecture designed for 2D data, our approach achi
    
[^59]: LLM辅助下的交通信号控制：利用大型语言模型在复杂城市环境中实现人类仿生交通信号控制

    LLM-Assisted Light: Leveraging Large Language Model Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments

    [https://arxiv.org/abs/2403.08337](https://arxiv.org/abs/2403.08337)

    本研究提出了将大型语言模型(LLMs)整合到交通信号控制(TSC)系统中的创新方法，以解决传统TSC系统在适应不熟悉场景方面的限制，并提出了一个混合框架，使得LLMs与一系列感知和决策工具相结合，从而提升TSC系统对城市交通复杂性和变异性的管理能力。

    

    大都市地区的交通拥堵是一个具有深远经济、环境和社会影响的巨大挑战。因此，有效的拥堵管理至关重要，交通信号控制(TSC)系统在这方面起着至关重要的作用。为了回应传统TSC系统在管理城市交通流动的复杂性和变异性方面经常表现出的不足，本研究引入了一种创新方法，将大型语言模型(LLMs)整合到TSC中，利用其先进的推理和决策能力。具体来说，提出了一个混合框架，将LLMs与一套感知和决策工具相结合，有助于探讨静态和动态交通信息。

    arXiv:2403.08337v1 Announce Type: cross  Abstract: Traffic congestion in metropolitan areas presents a formidable challenge with far-reaching economic, environmental, and societal ramifications. Therefore, effective congestion management is imperative, with traffic signal control (TSC) systems being pivotal in this endeavor. Conventional TSC systems, designed upon rule-based algorithms or reinforcement learning (RL), frequently exhibit deficiencies in managing the complexities and variabilities of urban traffic flows, constrained by their limited capacity for adaptation to unfamiliar scenarios. In response to these limitations, this work introduces an innovative approach that integrates Large Language Models (LLMs) into TSC, harnessing their advanced reasoning and decision-making faculties. Specifically, a hybrid framework that augments LLMs with a suite of perception and decision-making tools is proposed, facilitating the interrogation of both the static and dynamic traffic informatio
    
[^60]: 部分可观测因果表示学习的稀疏原则

    A Sparsity Principle for Partially Observable Causal Representation Learning

    [https://arxiv.org/abs/2403.08335](https://arxiv.org/abs/2403.08335)

    提出了部分可观测因果表示学习的稀疏原则，建立了两个可识别性结果，为线性混合函数和分段线性混合函数设置了基础模型。

    

    因果表示学习旨在从感知数据中识别高层次的因果变量。本文考虑部分观测设置，其中每次测量仅提供关于潜在因果状态子集的信息。我们专注于从数据集中不配对观察学习，其中存在实例相关的部分可观测模式。我们的主要贡献是为该设置建立两个可识别性结果：一个是关于线性混合函数的结果，无需对潜在因果模型做参数假设，另一个是对具有高斯潜在因果变量的分段线性混合函数的结果。基于这些见解，我们提出了两种用于估计潜在因果变量的方法。

    arXiv:2403.08335v1 Announce Type: cross  Abstract: Causal representation learning aims at identifying high-level causal variables from perceptual data. Most methods assume that all latent causal variables are captured in the high-dimensional observations. We instead consider a partially observed setting, in which each measurement only provides information about a subset of the underlying causal state. Prior work has studied this setting with multiple domains or views, each depending on a fixed subset of latents. Here, we focus on learning from unpaired observations from a dataset with an instance-dependent partial observability pattern. Our main contribution is to establish two identifiability results for this setting: one for linear mixing functions without parametric assumptions on the underlying causal model, and one for piecewise linear mixing functions with Gaussian latent causal variables. Based on these insights, we propose two methods for estimating the underlying causal variab
    
[^61]: 快速推断基于移除的节点影响

    Fast Inference of Removal-Based Node Influence

    [https://arxiv.org/abs/2403.08333](https://arxiv.org/abs/2403.08333)

    提出了一种评估节点影响的新方法，通过测量训练好的图神经网络模型在移除节点后的预测变化，以实现快速推断。

    

    图神经网络（GNNs）被广泛用于捕获图中信息传播模式。虽然取得了显著的性能，但评估节点影响的新趋势日益受到关注。我们提出了一种评估节点影响的新方法，通过衡量训练好的GNN模型在移除节点后的预测变化。一个真实应用是，“在预测Twitter账户极性的任务中，如果移除特定账户，其他账户的极性会如何改变？”我们将GNN作为一个代理模型，其预测可以模拟移除节点引起的节点或边的变化。为了获得每个节点的影响，一种直接的方法是交替移除每个节点，并在修改后的图上应用训练好的GNN。这是可靠的但耗时，因此我们需要一种高效的方法。

    arXiv:2403.08333v1 Announce Type: cross  Abstract: Graph neural networks (GNNs) are widely utilized to capture the information spreading patterns in graphs. While remarkable performance has been achieved, there is a new trending topic of evaluating node influence. We propose a new method of evaluating node influence, which measures the prediction change of a trained GNN model caused by removing a node. A real-world application is, "In the task of predicting Twitter accounts' polarity, had a particular account been removed, how would others' polarity change?". We use the GNN as a surrogate model whose prediction could simulate the change of nodes or edges caused by node removal. To obtain the influence for every node, a straightforward way is to alternately remove every node and apply the trained GNN on the modified graph. It is reliable but time-consuming, so we need an efficient method. The related lines of work, such as graph adversarial attack and counterfactual explanation, cannot 
    
[^62]: Bayesian优化将搜索区域限制在较低维度中，利用本地GPR

    Bayesian Optimization that Limits Search Region to Lower Dimensions Utilizing Local GPR

    [https://arxiv.org/abs/2403.08331](https://arxiv.org/abs/2403.08331)

    提出了一种Bayesian优化方法，通过将搜索区域限制在较低维度，并利用本地GPR模型，在高维度中提高了搜索效率和预测准确性。

    

    许多领域，包括设计和控制，都需要优化产品和系统特性。 当观测成本高时，通常使用Bayesian优化（BO），因为BO从理论上保证了遗憾的上限。 然而，随着要优化参数数量的增加，计算成本呈指数级增加，导致搜索效率降低。 我们提出了一种将搜索区域限制在较低维度并利用本地高斯过程回归（LGPR）将BO扩展到更高维度的方法。 LGPR将低维搜索区域视为“本地”，从而提高了那里的预测准确性。 LGPR模型是在特定区域的本地数据子集上训练的。 这提高了预测精度和搜索效率，并减少了高斯过程回归中矩阵求逆的时间复杂度。 在评估20D Ackley和Rosenbrock函数时，搜索效率与或高于既定值。

    arXiv:2403.08331v1 Announce Type: new  Abstract: Optimization of product and system characteristics is required in many fields, including design and control. Bayesian optimization (BO) is often used when there are high observing costs, because BO theoretically guarantees an upper bound on regret. However, computational costs increase exponentially with the number of parameters to be optimized, decreasing search efficiency. We propose a BO that limits the search region to lower dimensions and utilizes local Gaussian process regression (LGPR) to scale the BO to higher dimensions. LGPR treats the low-dimensional search region as "local," improving prediction accuracies there. The LGPR model is trained on a local subset of data specific to that region. This improves prediction accuracy and search efficiency and reduces the time complexity of matrix inversion in the Gaussian process regression. In evaluations with 20D Ackley and Rosenbrock functions, search efficiencies are equal to or high
    
[^63]: LLMs的知识冲突：一项调查

    Knowledge Conflicts for LLMs: A Survey

    [https://arxiv.org/abs/2403.08319](https://arxiv.org/abs/2403.08319)

    这项调查深入分析了LLMs在融合上下文和参数化知识时所面临的知识冲突，探讨了三类知识冲突对其可信度和性能的重要影响，并提出改进LLMs稳健性策略的策略。

    

    这项调查对大型语言模型（LLMs）的知识冲突进行了深入分析，突出了当它们融合上下文和参数化知识时所遇到的复杂挑战。我们关注三类知识冲突：上下文-记忆冲突、跨上下文冲突和内部记忆冲突。这些冲突可能会显著影响LLMs的可信度和性能，特别是在现实世界应用中，噪音和错误信息很常见。通过对这些冲突进行分类，探讨其原因，研究LLMs在这些冲突下的行为，并回顾可用的解决方案，本调查旨在为改进LLMs的稳健性策略提供启示，从而成为推动这一不断发展领域研究的宝贵资源。

    arXiv:2403.08319v1 Announce Type: cross  Abstract: This survey provides an in-depth analysis of knowledge conflicts for large language models (LLMs), highlighting the complex challenges they encounter when blending contextual and parametric knowledge. Our focus is on three categories of knowledge conflicts: context-memory, inter-context, and intra-memory conflict. These conflicts can significantly impact the trustworthiness and performance of LLMs, especially in real-world applications where noise and misinformation are common. By categorizing these conflicts, exploring the causes, examining the behaviors of LLMs under such conflicts, and reviewing available solutions, this survey aims to shed light on strategies for improving the robustness of LLMs, thereby serving as a valuable resource for advancing research in this evolving area.
    
[^64]: HRLAIF: 通过AI反馈改进开放域强化学习中的帮助性和无害性

    HRLAIF: Improvements in Helpfulness and Harmlessness in Open-domain Reinforcement Learning From AI Feedback

    [https://arxiv.org/abs/2403.08309](https://arxiv.org/abs/2403.08309)

    提出了HRLAIF方法来改善开放域强化学习中的模型响应帮助性，通过增强AI注释响应的准确性来提高模型在训练过程中的鲁棒性

    

    强化学习从AI反馈（RLAIF）相比从人类反馈中学习（RLHF）具有更短的注释周期和更低的成本优势，使其在大型语言模型（LLM）训练的快速策略迭代阶段非常高效。使用ChatGPT作为标注员，在RLAIF训练中为开放域提示提供反馈，我们观察到人类评估者对模型响应的偏好胜率增加，但评估者的满意度下降。分析表明，满意度下降主要是因为一些响应变得不够有帮助，特别是在正确性和真实性方面，突显了基本RLAIF的实际局限性。在本文中，我们提出了混合强化学习从AI反馈（HRLAIF）。该方法增强了AI注释响应的准确性，在训练过程中使模型的帮助性更加稳健。

    arXiv:2403.08309v1 Announce Type: cross  Abstract: Reinforcement Learning from AI Feedback (RLAIF) has the advantages of shorter annotation cycles and lower costs over Reinforcement Learning from Human Feedback (RLHF), making it highly efficient during the rapid strategy iteration periods of large language model (LLM) training. Using ChatGPT as a labeler to provide feedback on open-domain prompts in RLAIF training, we observe an increase in human evaluators' preference win ratio for model responses, but a decrease in evaluators' satisfaction rate. Analysis suggests that the decrease in satisfaction rate is mainly due to some responses becoming less helpful, particularly in terms of correctness and truthfulness, highlighting practical limitations of basic RLAIF. In this paper, we propose Hybrid Reinforcement Learning from AI Feedback (HRLAIF). This method enhances the accuracy of AI annotations for responses, making the model's helpfulness more robust in training process. Additionally, 
    
[^65]: CleanAgent：基于LLM代理自动化数据标准化

    CleanAgent: Automating Data Standardization with LLM-based Agents

    [https://arxiv.org/abs/2403.08291](https://arxiv.org/abs/2403.08291)

    提出了一个具有声明性、统一API的Python库，通过简洁的API调用简化LLM的代码生成流程

    

    数据标准化是数据科学生命周期中至关重要的一部分。虽然诸如Pandas之类的工具提供了强大的功能，但它们的复杂性以及需要定制代码以适应不同列类型的手动操作带来了重大挑战。尽管大型语言模型（LLMs）如ChatGPT已经展现出通过自然语言理解和代码生成自动化此过程的潜力，但仍需要专业程度的编程知识和持续互动以进行及时的完善。为了解决这些挑战，我们的关键想法是提出一个具有声明性、统一API的Python库，用于标准化列类型，通过简洁的API调用简化LLM的代码生成流程。我们首先提出了Dataprep.Clean，作为Dataprep库的一个组件，通过一行代码实现特定列类型的标准化，极大降低了复杂性。然后我们介绍了CleanAgen

    arXiv:2403.08291v1 Announce Type: cross  Abstract: Data standardization is a crucial part in data science life cycle. While tools like Pandas offer robust functionalities, their complexity and the manual effort required for customizing code to diverse column types pose significant challenges. Although large language models (LLMs) like ChatGPT have shown promise in automating this process through natural language understanding and code generation, it still demands expert-level programming knowledge and continuous interaction for prompt refinement. To solve these challenges, our key idea is to propose a Python library with declarative, unified APIs for standardizing column types, simplifying the code generation of LLM with concise API calls. We first propose Dataprep.Clean which is written as a component of the Dataprep Library, offers a significant reduction in complexity by enabling the standardization of specific column types with a single line of code. Then we introduce the CleanAgen
    
[^66]: SNOW-SCA: SNOW-V上的ML辅助侧信道攻击

    SNOW-SCA: ML-assisted Side-Channel Attack on SNOW-V

    [https://arxiv.org/abs/2403.08267](https://arxiv.org/abs/2403.08267)

    这项研究提出了SNOW-SCA，实现了对SNOW-V的侧信道攻击，成功破解了整个256位秘密密钥。

    

    本文介绍了SNOW-SCA，这是第一个针对5G移动通信安全标准候选SNOW-V的侧信道分析（SCA）攻击，该标准在32位ARM Cortex-M4微控制器上运行。首先，我们进行了通用的已知密钥相关性（KKC）分析以识别泄漏点。接着，进行了相关功耗分析（CPA）攻击，将攻击复杂性降低到每个密钥字节两次密钥猜测。然后，利用线性判别分析（LDA）唯一确定正确的秘密密钥。通过使用带有LDA的配置文件化SCA攻击在训练$<200$跟踪后实现100%准确率，这意味着仅使用单个跟踪即可成功发动攻击。总的来说，使用\textit{组合CPA和LDA攻击}模型，使用ChipWhisperer平台收集<50跟踪即可恢复SNOW-V的正确秘密密钥字节。使用所提出的SCA攻击，可以逐步恢复SNOW-V的整个256位秘密密钥。

    arXiv:2403.08267v1 Announce Type: cross  Abstract: This paper presents SNOW-SCA, the first power side-channel analysis (SCA) attack of a 5G mobile communication security standard candidate, SNOW-V, running on a 32-bit ARM Cortex-M4 microcontroller. First, we perform a generic known-key correlation (KKC) analysis to identify the leakage points. Next, a correlation power analysis (CPA) attack is performed, which reduces the attack complexity to two key guesses for each key byte. The correct secret key is then uniquely identified utilizing linear discriminant analysis (LDA). The profiled SCA attack with LDA achieves 100% accuracy after training with $<200$ traces, which means the attack succeeds with just a single trace. Overall, using the \textit{combined CPA and LDA attack} model, the correct secret key byte is recovered with <50 traces collected using the ChipWhisperer platform. The entire 256-bit secret key of SNOW-V can be recovered incrementally using the proposed SCA attack. Finall
    
[^67]: 随机搜索作为稀疏神经网络架构搜索的基准线

    Random Search as a Baseline for Sparse Neural Network Architecture Search

    [https://arxiv.org/abs/2403.08265](https://arxiv.org/abs/2403.08265)

    论文提出了一种评估方法和基于随机搜索的基线方法，用于发现高质量的稀疏神经网络配置，以解决当前缺乏可靠比较和可重现性的问题。

    

    稀疏神经网络在参数效率更高的情况下展现出与密集网络类似甚至更好的泛化性能，这促使许多工作学习、诱导或搜索性能高的稀疏网络。然而，尽管质量或效率的提升值得注意，但标准基线缺乏，因此妨碍了方法之间的可靠比较和可重现性。在这项工作中，我们提供了一种评估方法和一个简单的随机搜索基线方法，用于发现良好的稀疏配置。我们在过度参数化网络的节点空间上应用随机搜索，目标是找到在损失景观中位置更有优势的更好初始化的稀疏子网络。我们记录了不同稀疏程度下稀疏网络的训练后性能，并与它们的完全连接父网络以及随机稀疏配置进行比较。

    arXiv:2403.08265v1 Announce Type: cross  Abstract: Sparse neural networks have shown similar or better generalization performance than their dense counterparts while having higher parameter efficiency. This has motivated a number of works to learn, induce, or search for high performing sparse networks. While reports of quality or efficiency gains are impressive, standard baselines are lacking, therefore hindering having reliable comparability and reproducibility across methods. In this work, we provide an evaluation approach and a naive Random Search baseline method for finding good sparse configurations. We apply Random Search on the node space of an overparameterized network with the goal of finding better initialized sparse sub-networks that are positioned more advantageously in the loss landscape. We record sparse network post-training performances at various levels of sparsity and compare against both their fully connected parent networks and random sparse configurations at the sa
    
[^68]: Skipformer：一种用于高效语音识别的跳过和恢复策略

    Skipformer: A Skip-and-Recover Strategy for Efficient Speech Recognition

    [https://arxiv.org/abs/2403.08258](https://arxiv.org/abs/2403.08258)

    Skipformer提出了一种“跳过和恢复”的Conformer架构，可以动态、不均匀地压缩序列输入长度，大大减少了计算预算和内存消耗，并获得更好的识别准确性。

    

    基于Conformer的注意力模型已成为自动语音识别任务的事实标杆模型。通常引入一个空白符号来对齐CTC或RNN-T模型的输入和输出序列。不幸的是，由于注意力机制，长输入长度会对计算预算和内存消耗造成二次负荷。在这项工作中，我们提出了一种名为Skipformer的“跳过和恢复”Conformer架构，以动态和不均匀地压缩序列输入长度。Skipformer使用中间CTC输出作为标准将帧分为三组：关键、跳过和忽略。关键组馈送到下一个Conformer块，其输出与跳过组通过原始时间顺序联接作为最终编码器输出。实验表明，我们的模型在Aishell-1上将输入序列长度减少了31倍，在Librispeech语料库上减少了22倍。同时，该模型可实现更好的识别准确性。

    arXiv:2403.08258v1 Announce Type: new  Abstract: Conformer-based attention models have become the de facto backbone model for Automatic Speech Recognition tasks. A blank symbol is usually introduced to align the input and output sequences for CTC or RNN-T models. Unfortunately, the long input length overloads computational budget and memory consumption quadratically by attention mechanism. In this work, we propose a "Skip-and-Recover" Conformer architecture, named Skipformer, to squeeze sequence input length dynamically and inhomogeneously. Skipformer uses an intermediate CTC output as criteria to split frames into three groups: crucial, skipping and ignoring. The crucial group feeds into next conformer blocks and its output joint with skipping group by original temporal order as the final encoder output. Experiments show that our model reduces the input sequence length by 31 times on Aishell-1 and 22 times on Librispeech corpus. Meanwhile, the model can achieve better recognition accu
    
[^69]: 机器遗忘：分类、度量、应用、挑战和展望

    Machine Unlearning: Taxonomy, Metrics, Applications, Challenges, and Prospects

    [https://arxiv.org/abs/2403.08254](https://arxiv.org/abs/2403.08254)

    机器遗忘是一项重要的研究方向，涉及对个人数字数据的删除和对机器学习模型的影响，迫切需要全面调查来捕捉最新进展。

    

    个人数字数据是一项关键资产，各国政府已经实施法律和法规来保护数据隐私。数据用户已被赋予了遗忘其数据的权利。在机器学习（ML）的过程中，被遗忘的权利要求模型提供者在用户请求时删除用户数据，及其对ML模型的进一步影响。机器遗忘应运而生，受到了工业界和学术界的日益关注。尽管该领域发展迅速，但缺乏全面调查以捕捉最新进展。认识到这一不足，我们进行了广泛探索，以描绘机器遗忘的格局，包括在集中和分布式环境中遗忘算法的（细粒度）分类、对近似遗忘的讨论、验证和评估度量标准，不同应用场景下的遗忘挑战和解决方案。

    arXiv:2403.08254v1 Announce Type: new  Abstract: Personal digital data is a critical asset, and governments worldwide have enforced laws and regulations to protect data privacy. Data users have been endowed with the right to be forgotten of their data. In the course of machine learning (ML), the forgotten right requires a model provider to delete user data and its subsequent impact on ML models upon user requests. Machine unlearning emerges to address this, which has garnered ever-increasing attention from both industry and academia. While the area has developed rapidly, there is a lack of comprehensive surveys to capture the latest advancements. Recognizing this shortage, we conduct an extensive exploration to map the landscape of machine unlearning including the (fine-grained) taxonomy of unlearning algorithms under centralized and distributed settings, debate on approximate unlearning, verification and evaluation metrics, challenges and solutions for unlearning under different appli
    
[^70]: 面向正负偏好的统一建模的符号感知推荐

    Towards Unified Modeling for Positive and Negative Preferences in Sign-Aware Recommendation

    [https://arxiv.org/abs/2403.08246](https://arxiv.org/abs/2403.08246)

    提出了一种面向推荐的轻量级符号图卷积网络LSGRec，采用统一建模方法同时对高阶用户的正负偏好进行建模

    

    最近，符号感知图推荐引起了广泛关注，因为它将从用户与项目之间的正负交互（即，图中的链接）中学习用户的负偏好，除了正偏好。为了适应负链接和正链接的不同语义，现有作品利用两个独立的编码器分别建模用户的正负偏好。然而，这些方法无法从由多个带有不同符号的链接形成的高阶异构交互中学习负偏好，导致负用户偏好不准确和不完整。为了应对这些棘手的问题，我们提出了一种新颖的面向推荐的轻量级符号图卷积网络LSGRec，采用统一建模方法同时对高阶用户的正负偏好进行建模。

    arXiv:2403.08246v1 Announce Type: cross  Abstract: Recently, sign-aware graph recommendation has drawn much attention as it will learn users' negative preferences besides positive ones from both positive and negative interactions (i.e., links in a graph) with items. To accommodate the different semantics of negative and positive links, existing works utilize two independent encoders to model users' positive and negative preferences, respectively. However, these approaches cannot learn the negative preferences from high-order heterogeneous interactions between users and items formed by multiple links with different signs, resulting in inaccurate and incomplete negative user preferences. To cope with these intractable issues, we propose a novel \textbf{L}ight \textbf{S}igned \textbf{G}raph Convolution Network specifically for \textbf{Rec}ommendation (\textbf{LSGRec}), which adopts a unified modeling approach to simultaneously model high-order users' positive and negative preferences on a
    
[^71]: 分散式专家混合模型的实现

    Scattered Mixture-of-Experts Implementation

    [https://arxiv.org/abs/2403.08245](https://arxiv.org/abs/2403.08245)

    ScatterMoE是一种在GPU上实现的稀疏专家混合模型，通过避免填充和过多复制输入，提高了推理和训练速度，并减少了内存占用。

    

    我们提出了ScatterMoE，这是一种在GPU上实现的稀疏专家混合模型（SMoE）。ScatterMoE在现有实现的基础上构建，克服了一些限制以提高推理和训练速度，并减少内存占用。该实现通过避免填充和过多复制输入来实现这一目标。我们介绍了ParallelLinear，这是我们用来构建实现的主要组件，以及用于加速操作的各种内核。我们对我们的实现进行了与Megablocks的基准测试，并展示它可以实现更高的吞吐量和更低的内存占用。我们还展示了ParallelLinear如何通过展示Mixture of Attention的实现来扩展专家混合模型的概念。

    arXiv:2403.08245v1 Announce Type: new  Abstract: We present ScatterMoE, an implementation of Sparse Mixture-of-Experts (SMoE) on GPUs. ScatterMoE builds upon existing implementations, and overcoming some of the limitations to improve inference and training speed, and memory footprint. This implementation achieves this by avoiding padding and making excessive copies of the input.   We introduce ParallelLinear, the main component we use to build our implementation and the various kernels used to speed up the operation. We benchmark our implementation against Megablocks, and show that it enables a higher throughput and lower memory footprint. We also show how ParallelLinear enables extension of the Mixture-of-Experts concept by demonstrating with an implementation of Mixture of Attention.
    
[^72]: 使用预训练的视觉-语言模型和黑盒优化的烹饪机器人连续物体状态识别

    Continuous Object State Recognition for Cooking Robots Using Pre-Trained Vision-Language Models and Black-box Optimization

    [https://arxiv.org/abs/2403.08239](https://arxiv.org/abs/2403.08239)

    提出一种通过口语利用预训练大规模视觉-语言模型识别烹饪机器人连续食物状态变化的方法，以连续捕捉食物状态变化。

    

    机器人对环境和物体状态的识别通常基于将当前状态视为分类问题。在烹饪中，食物的状态变化是连续发生的，需要不仅在某个时间点捕获，还要在整个时间段内持续进行捕获。此外，食物的状态变化复杂，不能通过手动编程轻易描述。因此，我们提出了一种通过口语利用大规模预训练的视觉-语言模型识别烹饪机器人连续食物状态变化的方法。通过使用能够持续计算图像和文本之间相似度的模型，我们可以在烹饪过程中捕捉食物的状态变化。

    arXiv:2403.08239v1 Announce Type: cross  Abstract: The state recognition of the environment and objects by robots is generally based on the judgement of the current state as a classification problem. On the other hand, state changes of food in cooking happen continuously and need to be captured not only at a certain time point but also continuously over time. In addition, the state changes of food are complex and cannot be easily described by manual programming. Therefore, we propose a method to recognize the continuous state changes of food for cooking robots through the spoken language using pre-trained large-scale vision-language models. By using models that can compute the similarity between images and texts continuously over time, we can capture the state changes of food while cooking. We also show that by adjusting the weighting of each text prompt based on fitting the similarity changes to a sigmoid function and then performing black-box optimization, more accurate and robust co
    
[^73]: 具有对抗性专家的鲁棒决策聚合

    Robust Decision Aggregation with Adversarial Experts

    [https://arxiv.org/abs/2403.08222](https://arxiv.org/abs/2403.08222)

    论文考虑了在既有真实专家又有对抗性专家的情况下的二元决策聚合问题，提出了设计鲁棒聚合器以最小化遗憾的方法，并证明了当真实专家是对称的且对抗性专家不太多时，截尾均值是最优的。

    

    我们考虑了在既有真实专家又有对抗性专家的情况下的二元决策聚合问题。真实专家将会如实报告他们的私人信号，并获得适当的激励，而对抗性专家可以任意报告。决策者需要设计一个鲁棒的聚合器，根据专家的报告来预测世界的真实状态。决策者不了解具体的信息结构，即信号、状态以及对抗性专家的策略的联合分布。我们希望找到在最坏信息结构下最小化遗憾的最优聚合器。遗憾被定义为聚合器和一个基准之间的期望损失差，该基准根据联合分布和真实专家的报告做出最优决策。我们证明了当真实专家是对称的且对抗性专家不太多时，截尾均值是最优的。

    arXiv:2403.08222v1 Announce Type: cross  Abstract: We consider a binary decision aggregation problem in the presence of both truthful and adversarial experts. The truthful experts will report their private signals truthfully with proper incentive, while the adversarial experts can report arbitrarily. The decision maker needs to design a robust aggregator to forecast the true state of the world based on the reports of experts. The decision maker does not know the specific information structure, which is a joint distribution of signals, states, and strategies of adversarial experts. We want to find the optimal aggregator minimizing regret under the worst information structure. The regret is defined by the difference in expected loss between the aggregator and a benchmark who makes the optimal decision given the joint distribution and reports of truthful experts.   We prove that when the truthful experts are symmetric and adversarial experts are not too numerous, the truncated mean is opt
    
[^74]: 非线性贝叶斯反问题的高效几何马尔可夫链蒙特卡洛方法：利用导数信息的神经算子

    Efficient geometric Markov chain Monte Carlo for nonlinear Bayesian inversion enabled by derivative-informed neural operators

    [https://arxiv.org/abs/2403.08220](https://arxiv.org/abs/2403.08220)

    运用导数信息的神经算子加速了几何马尔可夫链蒙特卡洛方法，显著加快了解决非线性贝叶斯反问题的过程。

    

    我们提出了一种运算学习方法来加速几何马尔可夫链蒙特卡洛（MCMC）以解决无限维非线性贝叶斯反问题。虽然几何MCMC采用适应后验局部几何的高质量提议，但在参数到可观测（PtO）映射通过昂贵的模型模拟定义时，需要计算对数似然的局部梯度和Hessian信息，造成高成本。我们考虑了一个由PtO映射的神经算子替代驱动的延迟接受几何马尔可夫链蒙特卡洛方法，其中提议被设计为利用对数似然和其梯度和Hessian的快速替代估计。为了实现显著加速，替代品需要准确预测可观测及其参数导数（可观测与参数之间的导数）。通过传统的方法对这样的替代品进行训练

    arXiv:2403.08220v1 Announce Type: cross  Abstract: We propose an operator learning approach to accelerate geometric Markov chain Monte Carlo (MCMC) for solving infinite-dimensional nonlinear Bayesian inverse problems. While geometric MCMC employs high-quality proposals that adapt to posterior local geometry, it requires computing local gradient and Hessian information of the log-likelihood, incurring a high cost when the parameter-to-observable (PtO) map is defined through expensive model simulations. We consider a delayed-acceptance geometric MCMC method driven by a neural operator surrogate of the PtO map, where the proposal is designed to exploit fast surrogate approximations of the log-likelihood and, simultaneously, its gradient and Hessian. To achieve a substantial speedup, the surrogate needs to be accurate in predicting both the observable and its parametric derivative (the derivative of the observable with respect to the parameter). Training such a surrogate via conventional o
    
[^75]: 基于深度学习BERT模型在情感分析中的应用研究

    Research on the Application of Deep Learning-based BERT Model in Sentiment Analysis

    [https://arxiv.org/abs/2403.08217](https://arxiv.org/abs/2403.08217)

    本文研究了深度学习技术在情感分析中的应用，重点探讨了BERT模型的架构、特性以及优化策略，并通过实验证实了BERT模型在情感分析中表现出的稳健性能和提升潜力。

    

    本文探讨了深度学习技术在情感分析中的应用，尤其专注于BERT模型。文章首先介绍了情感分析的基本概念以及深度学习方法在该领域中的应用。随后，深入探讨了BERT模型的架构和特性。通过详细解释，阐明了BERT模型在情感分析中的应用效果和优化策略，并得到实验证实支持。实验结果表明，BERT模型在情感分析任务中表现出稳健的性能，在微调后有显著提升。最后，文章总结了BERT模型在情感分析中的潜在应用，并提出了未来研究和实际应用的方向。

    arXiv:2403.08217v1 Announce Type: new  Abstract: This paper explores the application of deep learning techniques, particularly focusing on BERT models, in sentiment analysis. It begins by introducing the fundamental concept of sentiment analysis and how deep learning methods are utilized in this domain. Subsequently, it delves into the architecture and characteristics of BERT models. Through detailed explanation, it elucidates the application effects and optimization strategies of BERT models in sentiment analysis, supported by experimental validation. The experimental findings indicate that BERT models exhibit robust performance in sentiment analysis tasks, with notable enhancements post fine-tuning. Lastly, the paper concludes by summarizing the potential applications of BERT models in sentiment analysis and suggests directions for future research and practical implementations.
    
[^76]: PaddingFlow：利用填充维度噪声改进归一化流

    PaddingFlow: Improving Normalizing Flows with Padding-Dimensional Noise

    [https://arxiv.org/abs/2403.08216](https://arxiv.org/abs/2403.08216)

    提出了一种名为PaddingFlow的去量化方法，利用填充维度噪声改进了正规化流，解决了基于流的模型在流形和离散数据上的性能问题。

    

    正规化流是一种具有高效采样的生成建模方法。然而，基于流的模型存在两个问题，即流形和离散数据。 如果目标分布是一个流形，也就是说潜在目标分布的维度和数据分布的维度不匹配，基于流的模型可能表现不佳。离散数据会导致基于流的模型坍缩为退化的点质量混合。 为了规避这两个问题，本文提出了PaddingFlow，一种新颖的去量化方法，利用填充维度噪声改进了正规化流。PaddingFlow易于实现、计算廉价、广泛适用于各种任务，并生成无偏估计的数据样本。特别是，我们的方法可以克服现有去量化方法的局限性，而这些方法必须改变数据分布，可能会降低性能。我们验证了我们的方法

    arXiv:2403.08216v1 Announce Type: new  Abstract: Normalizing flow is a generative modeling approach with efficient sampling. However, Flow-based models suffer two issues, which are manifold and discrete data. If the target distribution is a manifold, which means the dimension of the latent target distribution and the dimension of the data distribution are unmatched, flow-based models might perform badly. Discrete data makes flow-based models collapse into a degenerate mixture of point masses. In this paper, to sidestep such two issues we propose PaddingFlow, a novel dequantization method, which improves normalizing flows with padding-dimensional noise. PaddingFlow is easy to implement, computationally cheap, widely suitable for various tasks, and generates samples that are unbiased estimations of the data. Especially, our method can overcome the limitation of existing dequantization methods that have to change the data distribution, which might degrade performance. We validate our meth
    
[^77]: LIX：将空间几何先验知识隐式注入视觉语义分割，用于自动驾驶

    LIX: Implicitly Infusing Spatial Geometric Prior Knowledge into Visual Semantic Segmentation for Autonomous Driving

    [https://arxiv.org/abs/2403.08215](https://arxiv.org/abs/2403.08215)

    将双编码器教师模型获得的空间几何先验知识隐式注入单编码器学生模型，通过新的logit蒸馏和特征蒸馏方法，解决自动驾驶中的视觉语义分割问题。

    

    尽管数据融合网络在视觉语义分割中表现出色，但当缺乏空间几何数据时，双编码器变得无效。将双编码器教师模型获得的空间几何先验知识隐式注入单编码器学生模型是一个实用但不太探索的研究领域。本文深入探讨了这个主题，并采用知识蒸馏方法来解决这个问题。我们引入了Learning to Infuse "X" (LIX) 框架，在logit蒸馏和特征蒸馏方面进行了新颖贡献。我们提出了一个数学证明，强调在解耦知识蒸馏中使用单一固定权重的局限性，并引入了logit智能动态权重控制器作为解决这个问题的方法。此外，我们开发了一种自适应重新校准的特征蒸馏算法，包括两种技术。

    arXiv:2403.08215v1 Announce Type: cross  Abstract: Despite the impressive performance achieved by data-fusion networks with duplex encoders for visual semantic segmentation, they become ineffective when spatial geometric data are not available. Implicitly infusing the spatial geometric prior knowledge acquired by a duplex-encoder teacher model into a single-encoder student model is a practical, albeit less explored research avenue. This paper delves into this topic and resorts to knowledge distillation approaches to address this problem. We introduce the Learning to Infuse "X" (LIX) framework, with novel contributions in both logit distillation and feature distillation aspects. We present a mathematical proof that underscores the limitation of using a single fixed weight in decoupled knowledge distillation and introduce a logit-wise dynamic weight controller as a solution to this issue. Furthermore, we develop an adaptively-recalibrated feature distillation algorithm, including two tec
    
[^78]: BG-HGNN: 朝向可扩展和高效的异构图神经网络

    BG-HGNN: Toward Scalable and Efficient Heterogeneous Graph Neural Network

    [https://arxiv.org/abs/2403.08207](https://arxiv.org/abs/2403.08207)

    BG-HGNN提出了一种新颖的框架，有效地处理了现有HGNNs在复杂异构图上面临的参数爆炸和关系坍塌等挑战

    

    许多计算机视觉和机器学习问题被建模为在异构图上的学习任务，具有来自不同类型节点和边的各种关系。异构图神经网络(HGNNs)是一种为异构图设计的有前途的神经模型类。现有HGNNs建立在传统GNNs基础上，利用不同的参数空间来建模不同的关系。然而，现有HGNNs的实际有效性通常局限于简单的异构图，具有少量关系类型。本文首先突出和证明现有HGNNs使用的标准方法不可避免地导致参数爆炸和关系坍塌，使得HGNNs对具有大量关系类型的复杂异构图 less有效或不实用。为了克服这一问题，我们引入了一种新颖的框架，Blend&Grind-HGNN (BG-HGNN)，通过仔细处理挑战来有效应对这些问题。

    arXiv:2403.08207v1 Announce Type: new  Abstract: Many computer vision and machine learning problems are modelled as learning tasks on heterogeneous graphs, featuring a wide array of relations from diverse types of nodes and edges. Heterogeneous graph neural networks (HGNNs) stand out as a promising neural model class designed for heterogeneous graphs. Built on traditional GNNs, existing HGNNs employ different parameter spaces to model the varied relationships. However, the practical effectiveness of existing HGNNs is often limited to simple heterogeneous graphs with few relation types. This paper first highlights and demonstrates that the standard approach employed by existing HGNNs inevitably leads to parameter explosion and relation collapse, making HGNNs less effective or impractical for complex heterogeneous graphs with numerous relation types. To overcome this issue, we introduce a novel framework, Blend&Grind-HGNN (BG-HGNN), which effectively tackles the challenges by carefully i
    
[^79]: AutoDFP: 通过通道相似性重建实现自动无数据剪枝

    AutoDFP: Automatic Data-Free Pruning via Channel Similarity Reconstruction

    [https://arxiv.org/abs/2403.08204](https://arxiv.org/abs/2403.08204)

    提出一种名为AutoDFP的自动无数据剪枝方法，通过保留相似通道的重点信息实现自动剪枝和重建，无需进行微调。

    

    结构化剪枝方法的发展填补了神经网络规模庞大和有限硬件资源之间的差距。大多数当前的结构化剪枝方法依赖于训练数据集来微调压缩模型，导致高计算负担，并在对隐私和安全性要求严格的场景中无法应用。作为替代，已经提出了一些无数据方法，然而，这些方法通常需要手工参数调整，仅能实现不灵活的重建。在本文中，我们提出了一种自动无数据剪枝(AutoDFP)方法，实现了自动剪枝和重建而无需微调。我们的方法基于这样的假设：信息的丢失可以通过保留来自相似通道的重点信息来部分补偿。具体来说，我们将无数据剪枝制定为一个优化问题，可以得到有效解决。

    arXiv:2403.08204v1 Announce Type: new  Abstract: Structured pruning methods are developed to bridge the gap between the massive scale of neural networks and the limited hardware resources. Most current structured pruning methods rely on training datasets to fine-tune the compressed model, resulting in high computational burdens and being inapplicable for scenarios with stringent requirements on privacy and security. As an alternative, some data-free methods have been proposed, however, these methods often require handcraft parameter tuning and can only achieve inflexible reconstruction. In this paper, we propose the Automatic Data-Free Pruning (AutoDFP) method that achieves automatic pruning and reconstruction without fine-tuning. Our approach is based on the assumption that the loss of information can be partially compensated by retaining focused information from similar channels. Specifically, We formulate data-free pruning as an optimization problem, which can be effectively address
    
[^80]: 可学习的面向社区的变压器用于大脑结构网络分析与标记聚类

    Learnable Community-Aware Transformer for Brain Connectome Analysis with Token Clustering

    [https://arxiv.org/abs/2403.08203](https://arxiv.org/abs/2403.08203)

    提出了一种用于联合社区聚类和分类的可学习的面向社区的变压器模型。

    

    神经科学研究揭示了复杂的大脑网络可以被组织为不同的功能社区，每个社区由一组具有强连接的感兴趣区域（ROIs）组成。这些社区在理解大脑的功能组织及其对神经疾病，包括自闭症谱系障碍（ASD）和性别等生物差异的影响方面发挥着至关重要的作用。本研究提出了一个用于联合社区聚类和分类的标记聚类大脑变压器模型 ($\texttt{TC-BrainTF}$)。

    arXiv:2403.08203v1 Announce Type: cross  Abstract: Neuroscientific research has revealed that the complex brain network can be organized into distinct functional communities, each characterized by a cohesive group of regions of interest (ROIs) with strong interconnections. These communities play a crucial role in comprehending the functional organization of the brain and its implications for neurological conditions, including Autism Spectrum Disorder (ASD) and biological differences, such as in gender. Traditional models have been constrained by the necessity of predefined community clusters, limiting their flexibility and adaptability in deciphering the brain's functional organization. Furthermore, these models were restricted by a fixed number of communities, hindering their ability to accurately represent the brain's dynamic nature. In this study, we present a token clustering brain transformer-based model ($\texttt{TC-BrainTF}$) for joint community clustering and classification. Ou
    
[^81]: 深度子模逆点网络

    Deep Submodular Peripteral Network

    [https://arxiv.org/abs/2403.08199](https://arxiv.org/abs/2403.08199)

    引入了深度子模逆点网络（DSPNs），并提出了一种使用对比学习启发的GPC-ready策略进行训练的方法，以应对子模函数学习中的两大挑战。

    

    子模函数对各种应用至关重要，但通常缺乏实用的学习方法来获取它们。本文引入了深度子模逆点网络（DSPNs），一种新颖的子模函数参数化族，并提出了使用对比学习启发的GPC-ready策略对其进行训练的方法，以连接并解决上述两个挑战。

    arXiv:2403.08199v1 Announce Type: cross  Abstract: Submodular functions, crucial for various applications, often lack practical learning methods for their acquisition. Seemingly unrelated, learning a scaling from oracles offering graded pairwise preferences (GPC) is underexplored, despite a rich history in psychometrics. In this paper, we introduce deep submodular peripteral networks (DSPNs), a novel parametric family of submodular functions, and methods for their training using a contrastive-learning inspired GPC-ready strategy to connect and then tackle both of the above challenges. We introduce newly devised GPC-style "peripteral" loss which leverages numerically graded relationships between pairs of objects (sets in our case). Unlike traditional contrastive learning, our method utilizes graded comparisons, extracting more nuanced information than just binary-outcome comparisons, and contrasts sets of any size (not just two). We also define a novel suite of automatic sampling strate
    
[^82]: PAGE: 具有面向过去无关生成回放的领域增量适应策略

    PAGE: Domain-Incremental Adaptation with Past-Agnostic Generative Replay for Smart Healthcare

    [https://arxiv.org/abs/2403.08197](https://arxiv.org/abs/2403.08197)

    PAGE提出了一种面向智能医疗的领域增量适应策略，能够在不依赖于保留数据或信息的情况下实现生成回放，有效平衡领域适应和知识保留，并结合扩展的归纳确认预测方法提供可解释的疾病检测预测。

    

    我们提出了PAGE，一种具有面向过去无关生成回放的领域增量适应策略，用于智能医疗。PAGE能够实现生成回放，而无需保留来自先前领域的任何数据或信息。在适应新领域时，它利用来自新分布和当前模型的真实数据来生成合成数据，以保留先前领域的学习知识。通过在训练过程中重新播放合成数据和新实际数据，PAGE在领域适应和知识保留之间取得良好平衡。此外，我们将扩展的归纳确认预测（EICP）方法整合到PAGE中，为每个检测结果生成置信度分数和可信度值。这使得预测结果具有可解释性，并为智能医疗应用中的疾病检测提供统计保证。我们展示了PAGE在领域增量疾病检测中的有效性。

    arXiv:2403.08197v1 Announce Type: cross  Abstract: We propose PAGE, a domain-incremental adaptation strategy with past-agnostic generative replay for smart healthcare. PAGE enables generative replay without the aid of any preserved data or information from prior domains. When adapting to a new domain, it exploits real data from the new distribution and the current model to generate synthetic data that retain the learned knowledge of previous domains. By replaying the synthetic data with the new real data during training, PAGE achieves a good balance between domain adaptation and knowledge retention. In addition, we incorporate an extended inductive conformal prediction (EICP) method into PAGE to produce a confidence score and a credibility value for each detection result. This makes the predictions interpretable and provides statistical guarantees for disease detection in smart healthcare applications. We demonstrate PAGE's effectiveness in domain-incremental disease detection with thr
    
[^83]: 无监督学习混合潜在动态：一种学习识别框架

    Unsupervised Learning of Hybrid Latent Dynamics: A Learn-to-Identify Framework

    [https://arxiv.org/abs/2403.08194](https://arxiv.org/abs/2403.08194)

    提出了一种无监督元学习框架来学习混合潜在动态，该框架结合了物理归纳偏差和学习识别策略，在捕捉未知动态和分辨率之间取得了平衡。

    

    现代应用越来越需要从高维时间序列中无监督地学习潜在动态。这带来了一个重要的可辨识性挑战：许多抽象的潜在表示可以重构观测结果，但它们是否保证了对统治动态的充分识别？本文从两个角度探讨了这一挑战：一是使用特定于所建模数据的物理归纳偏差，二是使用一个学习识别的策略，将预测目标与用于识别的数据分开。我们将这两种策略结合在一起，提出了一种新颖的无监督元学习混合潜在动态框架（Meta-HyLaD），其中包括：1）一个混合先前物理数学表达式与描述其未知误差的神经函数的潜在动态函数，以及2）一个元学习公式，用于学习分别识别混合动态的两个组成部分。

    arXiv:2403.08194v1 Announce Type: new  Abstract: Modern applications increasingly require unsupervised learning of latent dynamics from high-dimensional time-series. This presents a significant challenge of identifiability: many abstract latent representations may reconstruct observations, yet do they guarantee an adequate identification of the governing dynamics? This paper investigates this challenge from two angles: the use of physics inductive bias specific to the data being modeled, and a learn-to-identify strategy that separates forecasting objectives from the data used for the identification. We combine these two strategies in a novel framework for unsupervised meta-learning of hybrid latent dynamics (Meta-HyLaD) with: 1) a latent dynamic function that hybridize known mathematical expressions of prior physics with neural functions describing its unknown errors, and 2) a meta-learning formulation to learn to separately identify both components of the hybrid dynamics. Through exte
    
[^84]: 学习驱动的物理感知大规模电路门尺寸优化

    Learning-driven Physically-aware Large-scale Circuit Gate Sizing

    [https://arxiv.org/abs/2403.08193](https://arxiv.org/abs/2403.08193)

    本研究提出了一种学习驱动的物理感知门尺寸优化框架，能够有效优化大规模电路的时序性能。

    

    门尺寸在物理设计后的时序优化中扮演重要角色。现有基于机器学习的门尺寸优化方法无法同时优化多个时序路径上的时序，并忽略布局的物理约束。为了高效优化大规模电路的时序性能，本研究提出了一种学习驱动的物理感知门尺寸优化框架。通过学习多个时序路径上的时序信息和多尺度布局上的物理信息，实现了多模态门尺寸感知时序模型，用于梯度下降优化。同时，基于尺寸导向估计器的梯度生成和自适应反向传播被开发用于更新门尺寸。我们的结果表明，相比商业工具，我们的方法实现了更高的

    arXiv:2403.08193v1 Announce Type: new  Abstract: Gate sizing plays an important role in timing optimization after physical design. Existing machine learning-based gate sizing works cannot optimize timing on multiple timing paths simultaneously and neglect the physical constraint on layouts. They cause sub-optimal sizing solutions and low-efficiency issues when compared with commercial gate sizing tools. In this work, we propose a learning-driven physically-aware gate sizing framework to optimize timing performance on large-scale circuits efficiently. In our gradient descent optimization-based work, for obtaining accurate gradients, a multi-modal gate sizing-aware timing model is achieved via learning timing information on multiple timing paths and physical information on multiple-scaled layouts jointly. Then, gradient generation based on the sizing-oriented estimator and adaptive back-propagation are developed to update gate sizes. Our results demonstrate that our work achieves higher 
    
[^85]: 在非凹游戏中可处理的局部均衡

    Tractable Local Equilibria in Non-Concave Games

    [https://arxiv.org/abs/2403.08171](https://arxiv.org/abs/2403.08171)

    提出了一个新的解决概念，$(\varepsilon, \Phi(\delta))$-局部均衡，以解决在非凹游戏中局部均衡存在但难以处理的问题。

    

    虽然众所周知在线梯度下降和其他无悔学习程序可以有效地收敛到协调均衡，在每个Agent的效用对于其自身策略呈凹形的情况下，但当效用是非凹的时，这种情况在机器学习应用中很常见，其中Agent的策略由深度神经网络参数化，或者Agent的效用由神经网络计算，或两者兼而有之。实际上，非凹游戏存在一系列博弈论和优化挑战：(i) Nash均衡可能不存在；(ii) 局部Nash均衡存在但是不可处理；(iii) 混合Nash、协调和粗糙协调均衡在一般情况下具有无限支持，并且是不可处理的。为了避开这些挑战，我们提出了一个新的解决概念，称为$(\varepsilon, \Phi(\delta))$-局部均衡，该概念在非凹游戏中概括了局部Nash均衡。

    arXiv:2403.08171v1 Announce Type: cross  Abstract: While Online Gradient Descent and other no-regret learning procedures are known to efficiently converge to coarse correlated equilibrium in games where each agent's utility is concave in their own strategy, this is not the case when the utilities are non-concave, a situation that is common in machine learning applications where the agents' strategies are parameterized by deep neural networks, or the agents' utilities are computed by a neural network, or both. Indeed, non-concave games present a host of game-theoretic and optimization challenges: (i) Nash equilibria may fail to exist; (ii) local Nash equilibria exist but are intractable; and (iii) mixed Nash, correlated, and coarse correlated equilibria have infinite support in general, and are intractable. To sidestep these challenges we propose a new solution concept, termed $(\varepsilon, \Phi(\delta))$-local equilibrium, which generalizes local Nash equilibrium in non-concave games,
    
[^86]: MolBind: 多模态对齐语言、分子和蛋白质

    MolBind: Multimodal Alignment of Language, Molecules, and Proteins

    [https://arxiv.org/abs/2403.08167](https://arxiv.org/abs/2403.08167)

    MolBind 提出了一个框架，通过对比学习为多种模态训练编码器，将所有模态映射到共享特征空间，实现多模态语义对齐。

    

    生物学和化学的最新进展已经利用多模态学习，将分子及其自然语言描述整合起来，以增强药物发现。然而，当前的预训练框架仅限于两种模态，设计一个统一的网络来处理不同模态（例如自然语言、2D分子图、3D分子构象和3D蛋白质）仍然具有挑战性，因为它们之间存在固有的差距。

    arXiv:2403.08167v1 Announce Type: cross  Abstract: Recent advancements in biology and chemistry have leveraged multi-modal learning, integrating molecules and their natural language descriptions to enhance drug discovery. However, current pre-training frameworks are limited to two modalities, and designing a unified network to process different modalities (e.g., natural language, 2D molecular graphs, 3D molecular conformations, and 3D proteins) remains challenging due to inherent gaps among them. In this work, we propose MolBind, a framework that trains encoders for multiple modalities through contrastive learning, mapping all modalities to a shared feature space for multi-modal semantic alignment. To facilitate effective pre-training of MolBind on multiple modalities, we also build and collect a high-quality dataset with four modalities, MolBind-M4, including graph-language, conformation-language, graph-conformation, and conformation-protein paired data. MolBind shows superior zero-sh
    
[^87]: EM-TTS：高效训练的低资源蒙古语轻量级文本转语音系统

    EM-TTS: Efficiently Trained Low-Resource Mongolian Lightweight Text-to-Speech

    [https://arxiv.org/abs/2403.08164](https://arxiv.org/abs/2403.08164)

    提出了一种基于深度卷积神经网络的轻量级TTS系统，采用两阶段训练而非递归单元，可以显著减少训练时间和经济成本

    

    最近，基于深度学习的文本转语音（TTS）系统取得了高质量的语音合成结果。递归神经网络已成为TTS系统中序列数据的标准建模技术，并被广泛使用。然而，训练包含RNN组件的TTS模型需要强大的GPU性能并且时间长。相反，基于CNN的序列合成技术可以显著减少TTS模型的参数和训练时间，同时由于其高并行性，可以保证一定的性能，从而减轻这些训练的经济成本。本文提出了一种基于深度卷积神经网络的轻量级TTS系统，这是一个两阶段训练的端到端TTS模型，不使用任何递归单元。我们的模型包括两个阶段：Text2Spectrum 和 SSRN。前者用于将音素编码为粗糙的梅尔频谱图，后者用于合成。

    arXiv:2403.08164v1 Announce Type: cross  Abstract: Recently, deep learning-based Text-to-Speech (TTS) systems have achieved high-quality speech synthesis results. Recurrent neural networks have become a standard modeling technique for sequential data in TTS systems and are widely used. However, training a TTS model which includes RNN components requires powerful GPU performance and takes a long time. In contrast, CNN-based sequence synthesis techniques can significantly reduce the parameters and training time of a TTS model while guaranteeing a certain performance due to their high parallelism, which alleviate these economic costs of training. In this paper, we propose a lightweight TTS system based on deep convolutional neural networks, which is a two-stage training end-to-end TTS model and does not employ any recurrent units. Our model consists of two stages: Text2Spectrum and SSRN. The former is used to encode phonemes into a coarse mel spectrogram and the latter is used to synthesi
    
[^88]: 三维脑MRI联合图像去噪和运动伪影校正的迭代学习

    Iterative Learning for Joint Image Denoising and Motion Artifact Correction of 3D Brain MRI

    [https://arxiv.org/abs/2403.08162](https://arxiv.org/abs/2403.08162)

    提出了一种通过迭代学习处理带有噪声和运动伪影的MRI的联合图像去噪和运动伪影修正框架

    

    图像噪声和运动伪影严重影响脑部MRI的质量，对下游医学图像分析产生负面影响。本文提出了一种通过迭代学习处理带有噪声和运动伪影的MRI的联合图像去噪和运动伪影修正（JDAC）框架，包括一种自适应去噪模型和一种抗伪影模型。在自适应去噪模型中，首先设计了一种新颖的噪声水平估计策略，然后通过具有特征归一化的U-Net骨干自适应地减少噪声。

    arXiv:2403.08162v1 Announce Type: cross  Abstract: Image noise and motion artifacts greatly affect the quality of brain MRI and negatively influence downstream medical image analysis. Previous studies often focus on 2D methods that process each volumetric MR image slice-by-slice, thus losing important 3D anatomical information. Additionally, these studies generally treat image denoising and artifact correction as two standalone tasks, without considering their potential relationship, especially on low-quality images where severe noise and motion artifacts occur simultaneously. To address these issues, we propose a Joint image Denoising and motion Artifact Correction (JDAC) framework via iterative learning to handle noisy MRIs with motion artifacts, consisting of an adaptive denoising model and an anti-artifact model. In the adaptive denoising model, we first design a novel noise level estimation strategy, and then adaptively reduce the noise through a U-Net backbone with feature normal
    
[^89]: 超越线性缩放区域的随机特征回归的渐近特性

    Asymptotics of Random Feature Regression Beyond the Linear Scaling Regime

    [https://arxiv.org/abs/2403.08160](https://arxiv.org/abs/2403.08160)

    本文研究了随机特征岭回归模型，探讨了参数化对模型性能的影响，以及如何选择参数数量$p$相对于样本大小$n$以实现最佳测试错误率。

    

    机器学习的最新进展是通过使用超参数化模型进行训练，直到接近训练数据的插值为止。通过双谷现象等现象已经表明，参数的数量是模型复杂性和泛化能力的不良代理，这引出了一个问题：参数化对这些模型的性能有什么影响？模型复杂性和泛化如何取决于参数的数量$p$？我们应该如何选择$p$相对于样本大小$n$来实现最优的测试误差？在本文中，我们研究了随机特征岭回归（RFRR）的例子。这个模型既可以看作是核岭回归（KRR）的有限秩逼近，也可以看作是在所谓的懒惰区域训练的神经网络的简化模型。我们考虑在$d$维球上均匀分布的协变量，并计算尖锐

    arXiv:2403.08160v1 Announce Type: cross  Abstract: Recent advances in machine learning have been achieved by using overparametrized models trained until near interpolation of the training data. It was shown, e.g., through the double descent phenomenon, that the number of parameters is a poor proxy for the model complexity and generalization capabilities. This leaves open the question of understanding the impact of parametrization on the performance of these models. How does model complexity and generalization depend on the number of parameters $p$? How should we choose $p$ relative to the sample size $n$ to achieve optimal test error?   In this paper, we investigate the example of random feature ridge regression (RFRR). This model can be seen either as a finite-rank approximation to kernel ridge regression (KRR), or as a simplified model for neural networks trained in the so-called lazy regime. We consider covariates uniformly distributed on the $d$-dimensional sphere and compute sharp
    
[^90]: 不同优化策略对基于物理约束的深度学习在土壤湿度估计中的影响

    The Effect of Different Optimization Strategies to Physics-Constrained Deep Learning for Soil Moisture Estimation

    [https://arxiv.org/abs/2403.08154](https://arxiv.org/abs/2403.08154)

    基于物理约束的深度学习框架结合了水分传输和水分传感信号的基于物理的原则，采用Adam、RMSprop和GD三种不同的优化器，在土壤湿度估计中取得了较好的收敛效果。

    

    土壤湿度是一个对人类社会和环境都具有重要意义的关键水文参数。在作物田地中准确建模和监测土壤湿度，尤其是在根系区（土壤表层100厘米）中，对于借助精准灌溉和耕作工具来提高农业生产和作物产量至关重要。充分实现传感器数据潜力在很大程度上取决于先进的领域感知模型和预测能力。在这项工作中，我们提出了一个基于物理约束的深度学习（P-DL）框架，将水分传输和水分传感信号的基于物理的原则整合在一起，以有效重建土壤湿度动态。我们采用了三种不同的优化器，即Adam、RMSprop和GD，在训练过程中最小化P-DL的损失函数。在说明性案例研究中，我们展示了Adam优化器的经验收敛优于其他优化方法。

    arXiv:2403.08154v1 Announce Type: new  Abstract: Soil moisture is a key hydrological parameter that has significant importance to human society and the environment. Accurate modeling and monitoring of soil moisture in crop fields, especially in the root zone (top 100 cm of soil), is essential for improving agricultural production and crop yield with the help of precision irrigation and farming tools. Realizing the full sensor data potential depends greatly on advanced analytical and predictive domain-aware models. In this work, we propose a physics-constrained deep learning (P-DL) framework to integrate physics-based principles on water transport and water sensing signals for effective reconstruction of the soil moisture dynamics. We adopt three different optimizers, namely Adam, RMSprop, and GD, to minimize the loss function of P-DL during the training process. In the illustrative case study, we demonstrate the empirical convergence of Adam optimizers outperforms the other optimizatio
    
[^91]: 衡量深度神经网络的能耗和效率：实证分析与设计建议

    Measuring the Energy Consumption and Efficiency of Deep Neural Networks: An Empirical Analysis and Design Recommendations

    [https://arxiv.org/abs/2403.08151](https://arxiv.org/abs/2403.08151)

    这项研究针对大规模神经网络不断增长的能耗问题进行了实证分析，提出了一个考虑网络尺寸、计算和内存层次结构的能耗模型。

    

    针对大规模神经网络日益增长的能耗问题（所谓的“红色AI”趋势），本研究通过节点级瓦特表测量了训练各种全连接神经网络架构的实际能耗。我们介绍了BUTTER-E数据集，这是BUTTER实证深度学习数据集的一个扩充，包含了来自63,527个单独实验运行的能耗和性能数据，涵盖了30,582个不同的配置：13个数据集、20个大小（可训练参数数量）、8个网络“形状”和14个深度，以及在CPU和GPU硬件上使用节点级瓦特表收集的数据。这个数据集揭示了数据集大小、网络结构和能耗之间复杂的关系，并突出了缓存效应的影响。我们提出了一个简单而有效的能耗模型，考虑了网络大小、计算和内存层次结构。我们的分析还揭示了

    arXiv:2403.08151v1 Announce Type: cross  Abstract: Addressing the so-called ``Red-AI'' trend of rising energy consumption by large-scale neural networks, this study investigates the actual energy consumption, as measured by node-level watt-meters, of training various fully connected neural network architectures. We introduce the BUTTER-E dataset, an augmentation to the BUTTER Empirical Deep Learning dataset, containing energy consumption and performance data from 63,527 individual experimental runs spanning 30,582 distinct configurations: 13 datasets, 20 sizes (number of trainable parameters), 8 network ``shapes'', and 14 depths on both CPU and GPU hardware collected using node-level watt-meters. This dataset reveals the complex relationship between dataset size, network structure, and energy use, and highlights the impact of cache effects. We propose a straightforward and effective energy model that accounts for network size, computing, and memory hierarchy. Our analysis also uncovers
    
[^92]: 将分子表示为可解释的文法上的随机游走

    Representing Molecules as Random Walks Over Interpretable Grammars

    [https://arxiv.org/abs/2403.08147](https://arxiv.org/abs/2403.08147)

    提出了一种新颖的分子表示模型，使用可解释的图文法描述分子的层次化设计空间，实现了在设计空间上的随机游走，从而提高了分子生成和属性预测的性能、效率和可合成性。

    

    最近分子探索领域的研究主要集中在小型、类似药物的分子上，导致许多在材料设计中同样重要的应用缺乏足够的技术支持。这些应用通常依赖于更复杂的分子结构，有更少的例子，是使用已知的亚结构精心设计的。我们提出了一种数据高效且可解释的模型，用于以图文法的形式表示和推理这些分子，明确描述了特征为设计基础的层次化设计空间。我们提出了一种新颖的表示形式，即在设计空间上进行随机游走，既有助于分子生成，又有助于属性预测。我们展示了相较于现有方法在性能、效率和预测分子可合成性方面的明显优势，并详细阐述了该方法的化学可解释性。

    arXiv:2403.08147v1 Announce Type: new  Abstract: Recent research in molecular discovery has primarily been devoted to small, drug-like molecules, leaving many similarly important applications in material design without adequate technology. These applications often rely on more complex molecular structures with fewer examples that are carefully designed using known substructures. We propose a data-efficient and interpretable model for representing and reasoning over such molecules in terms of graph grammars that explicitly describe the hierarchical design space featuring motifs to be the design basis. We present a novel representation in the form of random walks over the design space, which facilitates both molecule generation and property prediction. We demonstrate clear advantages over existing methods in terms of performance, efficiency, and synthesizability of predicted molecules, and we provide detailed insights into the method's chemical interpretability.
    
[^93]: 在高性能计算中复杂调参搜索的成本有效方法论：导航相互依赖和维度

    Cost-Effective Methodology for Complex Tuning Searches in HPC: Navigating Interdependencies and Dimensionality

    [https://arxiv.org/abs/2403.08131](https://arxiv.org/abs/2403.08131)

    优化搜索在高性能计算中至关重要，而我们提出的方法在保证计算可行性的前提下最大化实际场景中的性能收益。

    

    优化搜索在高性能计算（HPC）中至关重要，解决计算应用中的复杂优化挑战。复杂性不仅来自于对程序中参数的精细调优，还来自于它们之间潜在的相互依赖关系，使传统优化方法效率低下。我们的方法调整和完善了这些方法，以确保计算可行性，同时最大化现实场景中的性能收益。

    arXiv:2403.08131v1 Announce Type: cross  Abstract: Tuning searches are pivotal in High-Performance Computing (HPC), addressing complex optimization challenges in computational applications. The complexity arises not only from finely tuning parameters within routines but also potential interdependencies among them, rendering traditional optimization methods inefficient. Instead of scrutinizing interdependencies among parameters and routines, practitioners often face the dilemma of conducting independent tuning searches for each routine, thereby overlooking interdependence, or pursuing a more resource-intensive joint search for all routines. This decision is driven by the consideration that some interdependence analysis and high-dimensional decomposition techniques in literature may be prohibitively expensive in HPC tuning searches. Our methodology adapts and refines these methods to ensure computational feasibility while maximizing performance gains in real-world scenarios. Our methodol
    
[^94]: 在机器特征和标签的遗忘中朝向独立准则

    Towards Independence Criterion in Machine Unlearning of Features and Labels

    [https://arxiv.org/abs/2403.08124](https://arxiv.org/abs/2403.08124)

    提出了一种利用影响函数和分布独立原则的新方法，以应对机器遗忘中非均匀特征和标签删除的挑战，保护隐私同时保持模型性能和适应性

    

    这项工作深入探讨了面临分布转移时的机器遗忘复杂性，特别关注非均匀特征和标签删除所带来的挑战。随着GDPR等法规强调数据隐私和被遗忘的权利，机器学习模型面临着遗忘敏感信息而不损害其完整性或性能的艰巨任务。我们的研究引入了一种利用影响函数和分布独立原则来应对这些挑战的新方法。通过提出一个全面的机器遗忘框架，我们旨在确保隐私保护，同时在不同分布下保持模型性能和适应性。我们的方法不仅有助于高效地删除数据，还动态调整模型以保持其泛化能力。通过大量实验，我们证明了

    arXiv:2403.08124v1 Announce Type: cross  Abstract: This work delves into the complexities of machine unlearning in the face of distributional shifts, particularly focusing on the challenges posed by non-uniform feature and label removal. With the advent of regulations like the GDPR emphasizing data privacy and the right to be forgotten, machine learning models face the daunting task of unlearning sensitive information without compromising their integrity or performance. Our research introduces a novel approach that leverages influence functions and principles of distributional independence to address these challenges. By proposing a comprehensive framework for machine unlearning, we aim to ensure privacy protection while maintaining model performance and adaptability across varying distributions. Our method not only facilitates efficient data removal but also dynamically adjusts the model to preserve its generalization capabilities. Through extensive experimentation, we demonstrate the
    
[^95]: 早期方向性收敛在深度齐次神经网络中进行小初始化时的分析

    Early Directional Convergence in Deep Homogeneous Neural Networks for Small Initializations

    [https://arxiv.org/abs/2403.08121](https://arxiv.org/abs/2403.08121)

    本文研究了训练深度齐次神经网络时梯度流动力学的动态性，发现在足够小的初始化下，神经网络的权重在训练早期阶段保持较小规范，并且沿着神经相关函数的KKT点方向近似收敛。

    

    本文研究了训练深度齐次神经网络时梯度流动力学的动态性，这些网络从小初始化开始。本文考虑到具有局部Lipschitz梯度和阶数严格大于两的神经网络。文章证明了对于足够小的初始化，在训练的早期阶段，神经网络的权重保持规范较小，并且在Karush-Kuhn-Tucker (KKT)点处近似沿着神经相关函数的方向收敛。此外，对于平方损失并在神经网络权重上进行可分离假设的情况下，还展示了在损失函数的某些鞍点附近梯度流动动态的类似方向性收敛。

    arXiv:2403.08121v1 Announce Type: new  Abstract: This paper studies the gradient flow dynamics that arise when training deep homogeneous neural networks, starting with small initializations. The present work considers neural networks that are assumed to have locally Lipschitz gradients and an order of homogeneity strictly greater than two. This paper demonstrates that for sufficiently small initializations, during the early stages of training, the weights of the neural network remain small in norm and approximately converge in direction along the Karush-Kuhn-Tucker (KKT) points of the neural correlation function introduced in [1]. Additionally, for square loss and under a separability assumption on the weights of neural networks, a similar directional convergence of gradient flow dynamics is shown near certain saddle points of the loss function.
    
[^96]: 研究构建多保真度代理模型时有害数据来源的特征

    Characterising harmful data sources when constructing multi-fidelity surrogate models

    [https://arxiv.org/abs/2403.08118](https://arxiv.org/abs/2403.08118)

    研究指出在构建多保真度代理模型时，有害数据源的特征化有助于指导从业者在选择时何时忽略某个数据源。

    

    近年来，当应用于工业设计问题的建模和优化中，代理建模技术受到越来越多的关注。当评估特定设计的性能成本很高时，通过构建一个模型以代替可用的高成本来源来查询可以降低总成本。构建这些模型有时可以利用其他便宜且不太准确的信息源。然而，这些信息源的存在引发了一个问题，即在构建模型时应该使用哪些信息源。最近的研究尝试对有害数据源进行特征化，以指导从业者何时忽略某个信息源。

    arXiv:2403.08118v1 Announce Type: cross  Abstract: Surrogate modelling techniques have seen growing attention in recent years when applied to both modelling and optimisation of industrial design problems. These techniques are highly relevant when assessing the performance of a particular design carries a high cost, as the overall cost can be mitigated via the construction of a model to be queried in lieu of the available high-cost source. The construction of these models can sometimes employ other sources of information which are both cheaper and less accurate. The existence of these sources however poses the question of which sources should be used when constructing a model. Recent studies have attempted to characterise harmful data sources to guide practitioners in choosing when to ignore a certain source. These studies have done so in a synthetic setting, characterising sources using a large amount of data that is not available in practice. Some of these studies have also been shown
    
[^97]: 用于差分隐私联邦学习的高效语言模型架构

    Efficient Language Model Architectures for Differentially Private Federated Learning

    [https://arxiv.org/abs/2403.08100](https://arxiv.org/abs/2403.08100)

    提出了一个具有尺度不变性的耦合输入遗忘门递归网络，通过修改循环单元中的激活函数，使其能够更快地收敛并在跨设备联邦学习中取得更好的效果。

    

    跨设备联邦学习(FL)是一种在通常分布在数百万台边缘设备上的数据上训练模型的技术，而数据不离开设备。 SGD是交叉设备FL中标准的客户端优化器，因其内存和计算效率而受青睐。然而，在神经语言模型的集中式训练中，自适应优化器被认为更稳定和性能更好。鉴于此，我们想知道是否可以修改语言模型，使其可以通过SGD客户端优化器进行高效训练，并肯定地回答了这个问题。我们提出了一个具有尺度不变性的耦合输入遗忘门(SI CIFG)递归网络，通过修改循环单元中的Sigmoid和tanh激活，并展示这个新模型在大规模实验中比标准CIFG递归模型更快地收敛并实现更好的效用。

    arXiv:2403.08100v1 Announce Type: new  Abstract: Cross-device federated learning (FL) is a technique that trains a model on data distributed across typically millions of edge devices without data leaving the devices. SGD is the standard client optimizer for on device training in cross-device FL, favored for its memory and computational efficiency. However, in centralized training of neural language models, adaptive optimizers are preferred as they offer improved stability and performance. In light of this, we ask if language models can be modified such that they can be efficiently trained with SGD client optimizers and answer this affirmatively.   We propose a scale-invariant Coupled Input Forget Gate (SI CIFG) recurrent network by modifying the sigmoid and tanh activations in the recurrent cell and show that this new model converges faster and achieves better utility than the standard CIFG recurrent model in cross-device FL in large scale experiments. We further show that the proposed
    
[^98]: 具有自注意力机制的下一个标记预测的力学

    Mechanics of Next Token Prediction with Self-Attention

    [https://arxiv.org/abs/2403.08081](https://arxiv.org/abs/2403.08081)

    通过梯度下降训练自注意力学习到一个自动机，在下一个标记预测中生成标记的两个不同步骤是：硬检索和软组合。

    

    基于Transformer的语言模型在大型数据集上训练，以预测给定输入序列的下一个标记。尽管训练目标简单，但它们已经在自然语言处理领域取得了革命性进展。这一成功的基础是自注意力机制。在这项研究中，我们提出了一个问题：一个单独的自注意力层从下一个标记预测中学到了什么？我们展示：通过梯度下降训练自注意力学习到一个自动机，该自动机通过两个不同的步骤生成下一个标记：(1) 硬检索：在给定输入序列的情况下，自注意力精确选择与上一个输入标记相关的高优先级输入标记。(2) 软组合：然后，它创建高优先级标记的凸组合。

    arXiv:2403.08081v1 Announce Type: cross  Abstract: Transformer-based language models are trained on large datasets to predict the next token given an input sequence. Despite this simple training objective, they have led to revolutionary advances in natural language processing. Underlying this success is the self-attention mechanism. In this work, we ask: $\textit{What}$ $\textit{does}$ $\textit{a}$ $\textit{single}$ $\textit{self-attention}$ $\textit{layer}$ $\textit{learn}$ $\textit{from}$ $\textit{next-token}$ $\textit{prediction?}$ We show that training self-attention with gradient descent learns an automaton which generates the next token in two distinct steps: $\textbf{(1)}$ $\textbf{Hard}$ $\textbf{retrieval:}$ Given input sequence, self-attention precisely selects the $\textit{high-priority}$ $\textit{input}$ $\textit{tokens}$ associated with the last input token. $\textbf{(2)}$ $\textbf{Soft}$ $\textbf{composition:}$ It then creates a convex combination of the high-priority tok
    
[^99]: FluoroSAM: 用于X光图像分割的语言对齐基础模型

    FluoroSAM: A Language-aligned Foundation Model for X-ray Image Segmentation

    [https://arxiv.org/abs/2403.08059](https://arxiv.org/abs/2403.08059)

    FluoroSAM是用于X光图像的分割的语言对齐基础模型，提供了一种在X光成像领域具有广泛适用性的自动图像分析工具。

    

    自动X光图像分割将加速诊断和介入精准医学领域的研究和发展。先前的研究已经提出了适用于解决特定图像分析问题的特定任务模型，但这些模型的效用受限于特定任务领域，要拓展到更广泛的应用则需要额外的数据、标签和重新训练工作。最近，基础模型（FMs） - 训练在大量高度变化数据上的机器学习模型因此使得广泛适用性成为可能 - 已经成为自动图像分析的有希望的工具。现有的用于医学图像分析的FMs聚焦于对象被明显可见边界清晰定义的场景和模式，如内窥镜手术工具分割。相比之下，X光成像通常没有提供这种清晰的边界或结构先验。在X光图像形成期间，复杂的三维

    arXiv:2403.08059v1 Announce Type: cross  Abstract: Automated X-ray image segmentation would accelerate research and development in diagnostic and interventional precision medicine. Prior efforts have contributed task-specific models capable of solving specific image analysis problems, but the utility of these models is restricted to their particular task domain, and expanding to broader use requires additional data, labels, and retraining efforts. Recently, foundation models (FMs) -- machine learning models trained on large amounts of highly variable data thus enabling broad applicability -- have emerged as promising tools for automated image analysis. Existing FMs for medical image analysis focus on scenarios and modalities where objects are clearly defined by visually apparent boundaries, such as surgical tool segmentation in endoscopy. X-ray imaging, by contrast, does not generally offer such clearly delineated boundaries or structure priors. During X-ray image formation, complex 3D
    
[^100]: CHAI：高效LLM推理的聚类头部注意力

    CHAI: Clustered Head Attention for Efficient LLM Inference

    [https://arxiv.org/abs/2403.08058](https://arxiv.org/abs/2403.08058)

    CHAI提出了Clustered Head Attention（CHAI）方法，通过在运行时结合具有高相关性的注意力头部，实现了减少内存需求和计算量，能够在不需要微调的情况下将存储K,V缓存的内存需求降低21.4％，推理时间延迟降低1.73倍。

    

    大语言模型(LLMs)拥有数百亿参数改变了机器学习领域。然而，在推理时为这些模型提供服务既需要计算又需要内存，一个请求可能需要多个GPU和数十GB的内存。多头注意力是LLMs的关键组件之一，可以占LLMs内存和计算需求的50%以上。我们观察到在各头之间对注意力的关注有很高的冗余性。基于这一观察，我们提出了Clustered Head Attention (CHAI)。CHAI在运行时将具有高相关性的头部结合进行自注意力，从而减少内存和计算。在我们的实验中，我们展示了CHAI能够将存储K,V缓存的内存需求降低多达21.4%，推理时延迟降低多达1.73倍，而无需任何微调。CHAI实现了最多3.2%的偏差。

    arXiv:2403.08058v1 Announce Type: cross  Abstract: Large Language Models (LLMs) with hundreds of billions of parameters have transformed the field of machine learning. However, serving these models at inference time is both compute and memory intensive, where a single request can require multiple GPUs and tens of Gigabytes of memory. Multi-Head Attention is one of the key components of LLMs, which can account for over 50% of LLMs memory and compute requirement. We observe that there is a high amount of redundancy across heads on which tokens they pay attention to. Based on this insight, we propose Clustered Head Attention (CHAI). CHAI combines heads with a high amount of correlation for self-attention at runtime, thus reducing both memory and compute. In our experiments, we show that CHAI is able to reduce the memory requirements for storing K,V cache by up to 21.4% and inference time latency by up to 1.73x without any fine-tuning required. CHAI achieves this with a maximum 3.2% deviat
    
[^101]: DrivAerNet：用于数据驱动气动设计和基于图的阻力预测的参数化汽车数据集

    DrivAerNet: A Parametric Car Dataset for Data-Driven Aerodynamic Design and Graph-Based Drag Prediction

    [https://arxiv.org/abs/2403.08055](https://arxiv.org/abs/2403.08055)

    DrivAerNet提供了一个大规模高保真度的汽车数据集，以解决工程应用中训练深度学习模型所需的数据不足问题，而RegDGCNN利用这一数据集直接从3D网格提供高精度的阻力估计。

    

    本研究介绍了 DrivAerNet，这是一个大规模高保真度的CFD数据集，其中包含3D工业标准汽车形状，以及 RegDGCNN，这是一个动态图卷积神经网络模型，旨在通过机器学习进行汽车气动设计。DrivAerNet拥有4000个详细的3D汽车网格，使用50万个表面网格面和全面气动性能数据，包括完整的3D压力、速度场和壁面剪切应力，满足了工程应用中训练深度学习模型所需的大规模数据集的迫切需求。它比先前可用的最大公开汽车数据集大60％，也是唯一同时模拟轮毂和底盘的开源数据集。RegDGCNN利用这一大规模数据集，直接从3D网格提供高精度的阻力估计，绕过了传统限制，如需要2D图像渲染或符号距离场（SDF）。

    arXiv:2403.08055v1 Announce Type: new  Abstract: This study introduces DrivAerNet, a large-scale high-fidelity CFD dataset of 3D industry-standard car shapes, and RegDGCNN, a dynamic graph convolutional neural network model, both aimed at aerodynamic car design through machine learning. DrivAerNet, with its 4000 detailed 3D car meshes using 0.5 million surface mesh faces and comprehensive aerodynamic performance data comprising of full 3D pressure, velocity fields, and wall-shear stresses, addresses the critical need for extensive datasets to train deep learning models in engineering applications. It is 60\% larger than the previously available largest public dataset of cars, and is the only open-source dataset that also models wheels and underbody. RegDGCNN leverages this large-scale dataset to provide high-precision drag estimates directly from 3D meshes, bypassing traditional limitations such as the need for 2D image rendering or Signed Distance Fields (SDF). By enabling fast drag e
    
[^102]: TutoAI：用于物理任务的跨领域 AI 辅助混合媒体教程创作框架

    TutoAI: A Cross-domain Framework for AI-assisted Mixed-media Tutorial Creation on Physical Tasks

    [https://arxiv.org/abs/2403.08049](https://arxiv.org/abs/2403.08049)

    TutoAI 是一个跨领域的框架，用于在物理任务上利用AI辅助混合媒体教程创建，通过调查常见教程组件、评估AI模型提取组件的方法以及设计UI支持教程创建的指南，证明了其较基准模型具有更高或相似的质量。

    

    混合媒体教程将视频、图片、文本和图表整合起来，以教授过程技能，提供比基于时间轴的视频更具可浏览性的选择。然而，手动创建此类教程是乏味的，现有的自动化解决方案通常局限于特定领域。虽然 AI 模型具有潜力，但如何有效利用它们的能力尚不清楚，考虑到所涉及的多模态数据和模型的广阔领域。我们提出了 TutoAI，这是一个用于物理任务的跨领域 AI 辅助混合媒体教程创作的框架。首先，通过调查现有工作，我们提炼了常见的教程组件；然后，我们提出了一种识别、组装和评估用于组件提取的 AI 模型的方法；最后，我们提出了为基于 AI 生成的组件支持教程创建的用户界面（UI）设计指南。我们展示了 TutoAI 在质量上达到或高于基准模型的结果。

    arXiv:2403.08049v1 Announce Type: cross  Abstract: Mixed-media tutorials, which integrate videos, images, text, and diagrams to teach procedural skills, offer more browsable alternatives than timeline-based videos. However, manually creating such tutorials is tedious, and existing automated solutions are often restricted to a particular domain. While AI models hold promise, it is unclear how to effectively harness their powers, given the multi-modal data involved and the vast landscape of models. We present TutoAI, a cross-domain framework for AI-assisted mixed-media tutorial creation on physical tasks. First, we distill common tutorial components by surveying existing work; then, we present an approach to identify, assemble, and evaluate AI models for component extraction; finally, we propose guidelines for designing user interfaces (UI) that support tutorial creation based on AI-generated components. We show that TutoAI has achieved higher or similar quality compared to a baseline mo
    
[^103]: CT评估2D和3D整体深度学习方法用于气道病变的体积分割

    CT evaluation of 2D and 3D holistic deep learning methods for the volumetric segmentation of airway lesions

    [https://arxiv.org/abs/2403.08042](https://arxiv.org/abs/2403.08042)

    本研究比较了2D和3D格式的卷积神经网络在气道病变体积分割方面的能力，发现3D模型在捕捉复杂特征方面表现更优异，并通过对2D模型实施细微结构分割损失来提高准确性，通过外部验证证实了研究结果的稳健性

    

    这项研究对卷积神经网络（CNNs）在2D和3D格式中的整体分割能力进行了比较探讨，重点关注囊性纤维化（CF）病变。研究利用了来自两个CF参考中心的数据，涵盖了五个主要的CF结构变化。首先比较了2D和3D模型，突出了3D模型在捕捉粘液栓和实变等复杂特征方面的优越能力。为了提高2D模型的性能，实施和评估了一种适用于细微结构分割的损失，显著提高了其准确性，尽管没有超越3D模型的性能。模型经过进一步通过对肺功能测试（PFTs）的外部评估进行验证，确认了研究结果的稳健性。此外，这项研究不仅限于比较指标；还包括对模型解释性的全面评估。

    arXiv:2403.08042v1 Announce Type: cross  Abstract: This research embarked on a comparative exploration of the holistic segmentation capabilities of Convolutional Neural Networks (CNNs) in both 2D and 3D formats, focusing on cystic fibrosis (CF) lesions. The study utilized data from two CF reference centers, covering five major CF structural changes. Initially, it compared the 2D and 3D models, highlighting the 3D model's superior capability in capturing complex features like mucus plugs and consolidations. To improve the 2D model's performance, a loss adapted to fine structures segmentation was implemented and evaluated, significantly enhancing its accuracy, though not surpassing the 3D model's performance. The models underwent further validation through external evaluation against pulmonary function tests (PFTs), confirming the robustness of the findings. Moreover, this study went beyond comparing metrics; it also included comprehensive assessments of the models' interpretability and 
    
[^104]: MicroT：用于MCUs的低能耗和自适应模型

    MicroT: Low-Energy and Adaptive Models for MCUs

    [https://arxiv.org/abs/2403.08040](https://arxiv.org/abs/2403.08040)

    MicroT是一个低能耗、多任务自适应模型框架，通过特征提取器和分类器的分离、模型优化和本地任务训练，在MCUs上实现了模型性能的提升和能耗的降低。

    

    我们提出了MicroT，这是一个面向资源受限的MCUs的低能耗、多任务自适应模型框架。我们将原始模型划分为特征提取器和分类器。特征提取器通过自监督知识蒸馏获得，并通过模型分割和联合训练进一步优化为部分模型和完整模型。然后将这些模型部署在MCUs上，增加并在本地任务上训练分类器，最终执行关节推理的阶段决策。在这个过程中，部分模型最初处理样本，如果置信度得分低于设定的阈值，完整模型将恢复并继续推理。我们在两个模型、三个数据集和两个MCU板上评估了MicroT。我们的实验评估表明，在处理多个本地任务时，MicroT有效地提高了模型性能并降低了能耗。与未经优化的特征提取器相比，MicroT

    arXiv:2403.08040v1 Announce Type: new  Abstract: We propose MicroT, a low-energy, multi-task adaptive model framework for resource-constrained MCUs. We divide the original model into a feature extractor and a classifier. The feature extractor is obtained through self-supervised knowledge distillation and further optimized into part and full models through model splitting and joint training. These models are then deployed on MCUs, with classifiers added and trained on local tasks, ultimately performing stage-decision for joint inference. In this process, the part model initially processes the sample, and if the confidence score falls below the set threshold, the full model will resume and continue the inference. We evaluate MicroT on two models, three datasets, and two MCU boards. Our experimental evaluation shows that MicroT effectively improves model performance and reduces energy consumption when dealing with multiple local tasks. Compared to the unoptimized feature extractor, MicroT
    
[^105]: McCatch：维度和非维度数据中可扩展的微簇检测

    McCatch: Scalable Microcluster Detection in Dimensional and Nondimensional Datasets

    [https://arxiv.org/abs/2403.08027](https://arxiv.org/abs/2403.08027)

    该论文提出了 McCatch 算法，通过引入“Oracle”图来检测微簇，是目前唯一可回答两个关键问题的方法，具有在处理非维度数据以及具有非单点微簇时更好的性能。

    

    如何设计一个可以处理非维度数据，并且以异常分数对单点微簇（"一次性"异常值）和非单点微簇进行排名的离群点检测器？如何以一种可扩展且“无需干预”的方式获得合理的分数？异常值的微簇指示了欺诈活动中的联合或重复，因此它们的识别是非常可取的。本文提出了一种名为 McCatch 的新算法，通过利用我们提出的“Oracle”图（1NN距离与组1NN距离）来检测微簇。我们研究了31个真实和合成数据集，其中包含高达100万个数据元素，以展示 McCatch 是唯一可以回答上述两个问题的方法；它在性能上优于其他11种方法，特别是当数据具有非单点微簇或为非维度时。我们还展示了 McCatch 在图形、指纹、网络日志中检测有意义微簇的能力。

    arXiv:2403.08027v1 Announce Type: new  Abstract: How could we have an outlier detector that works even with nondimensional data, and ranks together both singleton microclusters ('one-off' outliers) and nonsingleton microclusters by their anomaly scores? How to obtain scores that are principled in one scalable and 'hands-off' manner? Microclusters of outliers indicate coalition or repetition in fraud activities, etc.; their identification is thus highly desirable. This paper presents McCatch: a new algorithm that detects microclusters by leveraging our proposed 'Oracle' plot (1NN Distance versus Group 1NN Distance). We study 31 real and synthetic datasets with up to 1M data elements to show that McCatch is the only method that answers both of the questions above; and, it outperforms 11 other methods, especially when the data has nonsingleton microclusters or is nondimensional. We also showcase McCatch's ability to detect meaningful microclusters in graphs, fingerprints, logs of network 
    
[^106]: xMLP：利用独占方激活革命化私密推断

    xMLP: Revolutionizing Private Inference with Exclusive Square Activation

    [https://arxiv.org/abs/2403.08024](https://arxiv.org/abs/2403.08024)

    本文提出了xMLP，这是一种独特的DNN架构，使用独占的方激活，在维持准确性的同时减少私密推断系统中的延迟

    

    私密推断（PI）通过利用密码原语，如多方计算（MPC）和同态加密（HE），使深度神经网络（DNNs）能够在私人数据上运行，而不会泄露敏感信息。然而，现有PI系统中使用诸如ReLU之类的非线性激活可能导致不切实际的高PI延迟，因为ReLU需要使用昂贵的MPC计算，如Garbled Circuits。由于与ReLU相比，方激活可通过比Beaver的三元组快数百倍的速度进行处理，因此更适合PI任务，但是使用它们会导致模型准确性显著下降。本文首先探讨了在使用方激活后出现准确性下降的原因，并得出结论，即这是由于“信息复合”效应。利用这一见解，我们提出了xMLP，这是一种使用独占方激活的新颖DNN架构，同时在两者之间保持平衡

    arXiv:2403.08024v1 Announce Type: new  Abstract: Private Inference (PI) enables deep neural networks (DNNs) to work on private data without leaking sensitive information by exploiting cryptographic primitives such as multi-party computation (MPC) and homomorphic encryption (HE). However, the use of non-linear activations such as ReLU in DNNs can lead to impractically high PI latency in existing PI systems, as ReLU requires the use of costly MPC computations, such as Garbled Circuits. Since square activations can be processed by Beaver's triples hundreds of times faster compared to ReLU, they are more friendly to PI tasks, but using them leads to a notable drop in model accuracy. This paper starts by exploring the reason for such an accuracy drop after using square activations, and concludes that this is due to an "information compounding" effect. Leveraging this insight, we propose xMLP, a novel DNN architecture that uses square activations exclusively while maintaining parity in both 
    
[^107]: 使用神经网络进行伊氏伊蚊卵计数的目标检测

    Aedes aegypti Egg Counting with Neural Networks for Object Detection

    [https://arxiv.org/abs/2403.08016](https://arxiv.org/abs/2403.08016)

    通过使用神经网络进行目标检测，提出了一种新的数据集，来自野外和实验室卵，并测试了三种神经网络在伊氏伊蚊卵计数任务中的效果。

    

    伊氏伊蚊仍然是疾病传播媒介时的主要关注对象。在处理这个问题的许多方法中，有一些重要的协议利用卵数在卵罗捕器中来计算指标，如LIRAa和Breteau指数，可以提供关于可预测的暴发和流行病的信息。此外，许多研究领域需要卵数，特别是在需要大规模培育蚊子时。卵计数是一个繁琐且容易出错的任务，可以通过基于计算机视觉的技术，特别是基于深度学习的目标检测来实现自动化。在这项工作中，我们提出了一个新的数据集，包括野外和实验室的卵，以及三种应用于该任务的神经网络测试结果：Faster R-CNN、Side-Aware Boundary Localization和FoveaBox。

    arXiv:2403.08016v1 Announce Type: cross  Abstract: Aedes aegypti is still one of the main concerns when it comes to disease vectors. Among the many ways to deal with it, there are important protocols that make use of egg numbers in ovitraps to calculate indices, such as the LIRAa and the Breteau Index, which can provide information on predictable outbursts and epidemics. Also, there are many research lines that require egg numbers, specially when mass production of mosquitoes is needed. Egg counting is a laborious and error-prone task that can be automated via computer vision-based techniques, specially deep learning-based counting with object detection. In this work, we propose a new dataset comprising field and laboratory eggs, along with test results of three neural networks applied to the task: Faster R-CNN, Side-Aware Boundary Localization and FoveaBox.
    
[^108]: 监督时间序列分类在海底工程异常检测中的应用

    Supervised Time Series Classification for Anomaly Detection in Subsea Engineering

    [https://arxiv.org/abs/2403.08013](https://arxiv.org/abs/2403.08013)

    研究了监督机器学习算法在基于模拟数据的物理系统中的应用，通过对时间数据进行预处理和比较不同方法的性能指标，展示了机器学习技术在决策中的优势。

    

    时间序列分类在监测结构系统中具有重要意义。本文研究了在基于具有两种状态（完整和破裂）的物理系统的模拟数据上使用监督机器学习分类算法的方法。我们对时间数据的预处理进行了全面讨论，使用统计差异度量和降维技术。我们提出了一种直观的基准方法，并讨论了其效率。最后，我们根据不同性能指标对各种方法进行了比较，显示使用机器学习技术作为决策工具的优势。

    arXiv:2403.08013v1 Announce Type: new  Abstract: Time series classification is of significant importance in monitoring structural systems. In this work, we investigate the use of supervised machine learning classification algorithms on simulated data based on a physical system with two states: Intact and Broken. We provide a comprehensive discussion of the preprocessing of temporal data, using measures of statistical dispersion and dimension reduction techniques. We present an intuitive baseline method and discuss its efficiency. We conclude with a comparison of the various methods based on different performance metrics, showing the advantage of using machine learning techniques as a tool in decision making.
    
[^109]: 使用集成预测口语言识别的古吉拉特语-英语混合语音识别

    Gujarati-English Code-Switching Speech Recognition using ensemble prediction of spoken language

    [https://arxiv.org/abs/2403.08011](https://arxiv.org/abs/2403.08011)

    通过在输出中以每层有监督的方式对单词和字符的语言ID条件化变压器层，该方法虽然未能显著降低词错误率，但展现了在仅仅通过口语数据预测正确语言的潜力。

    

    混合语音识别中一个重要且困难的任务是识别语言，因为两种语言中的许多词在某些口音下听起来相似。我们专注于通过在输出中以每层有监督的方式对单词和字符的语言ID条件化变压器层，以改善端到端自动语音识别模型的性能。为此，我们提出了两种引入语言特定参数和可解释性到多头注意力机制的方法，并实施了一个有助于保持输入对齐连续性的时间损失。尽管无法显著降低词错误率（WER），我们的方法展现了在仅仅通过口语数据预测正确语言的潜力。我们通过在序列中删除LID引入了语言预测的正则化，有助于对齐长重复的输出序列。

    arXiv:2403.08011v1 Announce Type: cross  Abstract: An important and difficult task in code-switched speech recognition is to recognize the language, as lots of words in two languages can sound similar, especially in some accents. We focus on improving performance of end-to-end Automatic Speech Recognition models by conditioning transformer layers on language ID of words and character in the output in an per layer supervised manner. To this end, we propose two methods of introducing language specific parameters and explainability in the multi-head attention mechanism, and implement a Temporal Loss that helps maintain continuity in input alignment. Despite being unable to reduce WER significantly, our method shows promise in predicting the correct language from just spoken data. We introduce regularization in the language prediction by dropping LID in the sequence, which helps align long repeated output sequences.
    
[^110]: 主题、短语及其进展：符号音乐生成中结构的建模

    Motifs, Phrases, and Beyond: The Modelling of Structure in Symbolic Music Generation

    [https://arxiv.org/abs/2403.07995](https://arxiv.org/abs/2403.07995)

    介绍了符号音乐生成中关于音乐结构建模的技术演变，包括利用深度学习方法将音乐生成分解为高层结构规划和内容创建阶段。

    

    建模音乐结构对于生成符号音乐作品的人工智能系统至关重要，但也具有挑战性。本文细致分析了各种技术的演变，从符号方法到基础和革命性的深度学习方法，利用计算和数据的力量跨越各种训练范式，以整合连贯的结构。

    arXiv:2403.07995v1 Announce Type: cross  Abstract: Modelling musical structure is vital yet challenging for artificial intelligence systems that generate symbolic music compositions. This literature review dissects the evolution of techniques for incorporating coherent structure, from symbolic approaches to foundational and transformative deep learning methods that harness the power of computation and data across a wide variety of training paradigms. In the later stages, we review an emerging technique which we refer to as "sub-task decomposition" that involves decomposing music generation into separate high-level structural planning and content creation stages. Such systems incorporate some form of musical knowledge or neuro-symbolic methods by extracting melodic skeletons or structural templates to guide the generation. Progress is evident in capturing motifs and repetitions across all three eras reviewed, yet modelling the nuanced development of themes across extended compositions i
    
[^111]: 多智体是否梦见电子羊？：通过生成式学习提高强化学习的泛化能力

    Do Agents Dream of Electric Sheep?: Improving Generalization in Reinforcement Learning through Generative Learning

    [https://arxiv.org/abs/2403.07979](https://arxiv.org/abs/2403.07979)

    通过生成式增强技术，本研究研究了在稀疏奖励环境中通过基于想象力的强化学习训练来提高强化学习智能体泛化能力的方法。

    

    过度拟合的大脑假设表明梦的发生是为了让人类大脑进行泛化。在这里，我们询问是否对强化学习智能体也是如此。在真实环境中经验有限的情况下，我们使用基于想象力的强化学习来在类似梦境的情节中训练策略，在那里，对非想象力、预测的轨迹进行生成性增强。在四个ProcGen环境的实验中表明，与经典的想象力和离线训练相比，我们的方法在处理稀疏奖励环境时可以达到更高的泛化水平。

    arXiv:2403.07979v1 Announce Type: cross  Abstract: The Overfitted Brain hypothesis suggests dreams happen to allow generalization in the human brain. Here, we ask if the same is true for reinforcement learning agents as well. Given limited experience in a real environment, we use imagination-based reinforcement learning to train a policy on dream-like episodes, where non-imaginative, predicted trajectories are modified through generative augmentations. Experiments on four ProcGen environments show that, compared to classic imagination and offline training on collected experience, our method can reach a higher level of generalization when dealing with sparsely rewarded environments.
    
[^112]: LiveCodeBench：用于代码的大型语言模型的全面和无污染评估

    LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code

    [https://arxiv.org/abs/2403.07974](https://arxiv.org/abs/2403.07974)

    LiveCodeBench提出了一个全面的、无污染的LLMs评估工具，聚焦于从LeetCode、AtCoder和CodeForces等平台连续收集的新问题，覆盖自修复、代码执行、测试输出预测等更广泛的代码相关能力。

    

    大型语言模型（LLMs）应用于与代码相关的应用程序已经成为一个突出的领域，吸引了学术界和工业界的极大兴趣。然而，随着新的和改进的LLMs的开发，现有的评估基准（例如HumanEval，MBPP）不再足以评估它们的能力。在这项工作中，我们提出LiveCodeBench，这是一个全面的、无污染的LLMs评估工具，用于代码，它会从三个竞赛平台（LeetCode、AtCoder和CodeForces）上连续地收集新问题。值得注意的是，我们的基准还着重关注更广泛的与代码相关的能力，如自修复、代码执行和测试输出预测，而不仅仅是代码生成。目前，LiveCodeBench托管了在2023年5月至2024年2月之间发布的400个高质量编码问题。我们已经评估了9个基本LLMs和20个指令调整的LLMs。

    arXiv:2403.07974v1 Announce Type: cross  Abstract: Large Language Models (LLMs) applied to code-related applications have emerged as a prominent field, attracting significant interest from both academia and industry. However, as new and improved LLMs are developed, existing evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient for assessing their capabilities. In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code, which continuously collects new problems over time from contests across three competition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, our benchmark also focuses on a broader range of code related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation. Currently, LiveCodeBench hosts four hundred high-quality coding problems that were published between May 2023 and February 2024. We have evaluated 9 base LLMs and 20 instruction-tuned LLMs o
    
[^113]: KnowCoder：将结构化知识编码到LLMs中用于普适信息提取

    KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction

    [https://arxiv.org/abs/2403.07969](https://arxiv.org/abs/2403.07969)

    本文提出了KnowCoder，一个通过代码生成执行普适信息提取的大型语言模型，引入了代码风格的模式表示方法和两阶段学习框架，以提高LLMs对结构化知识的准确提取能力

    

    在本文中，我们提出了KnowCoder，这是一个大型语言模型（LLM），用于通过代码生成进行普适信息提取（UIE）。KnowCoder旨在开发一种统一的模式表示，使LLMs能够轻松理解，并且提出了一种有效的学习框架，鼓励LLMs遵循模式并准确提取结构化知识。为了实现这一目标，KnowCoder引入了一种代码风格的模式表示方法，将不同的模式统一转换为Python类，从而可以以LLM友好的方式捕捉UIE中任务之间的约束等复杂模式信息。我们进一步构建了一个包含超过30,000种知识类型的代码风格模式库，据我们所知，这是UIE中最大的库。为了简化LLMs的学习过程，KnowCoder包含一个通过代码预训练增强其模式理解能力的两阶段学习框架。

    arXiv:2403.07969v1 Announce Type: cross  Abstract: In this paper, we propose KnowCoder, a Large Language Model (LLM) to conduct Universal Information Extraction (UIE) via code generation. KnowCoder aims to develop a kind of unified schema representation that LLMs can easily understand and an effective learning framework that encourages LLMs to follow schemas and extract structured knowledge accurately. To achieve these, KnowCoder introduces a code-style schema representation method to uniformly transform different schemas into Python classes, with which complex schema information, such as constraints among tasks in UIE, can be captured in an LLM-friendly manner. We further construct a code-style schema library covering over $\textbf{30,000}$ types of knowledge, which is the largest one for UIE, to the best of our knowledge. To ease the learning process of LLMs, KnowCoder contains a two-phase learning framework that enhances its schema understanding ability via code pretraining and its 
    
[^114]: 深度神经网络解决方案是否形成星形区域？

    Do Deep Neural Network Solutions Form a Star Domain?

    [https://arxiv.org/abs/2403.07968](https://arxiv.org/abs/2403.07968)

    SGD解决方案集是一个星形域，包含一个星形模型，通过低损失数值的路径与其他解决方案线性相连，模除排列。

    

    Entezari等人（2022）推测通过随机梯度下降（SGD）可达到的神经网络解决方案集是凸的，考虑到排列不变性。本文提出了一个更加宽松的观点：SGD解决方案集是一个星形域，包含一个星形模型，通过低损失数值的路径与其他解决方案线性相连，模除排列。我们提出了Starlight算法，用于找到给定学习任务的星形模型。我们通过展示这个星形模型与其他独立找到的解决方案是线性相连的来验证我们的观点。

    arXiv:2403.07968v1 Announce Type: cross  Abstract: Entezari et al. (2022) conjectured that neural network solution sets reachable via stochastic gradient descent (SGD) are convex, considering permutation invariances. This means that two independent solutions can be connected by a linear path with low loss, given one of them is appropriately permuted. However, current methods to test this theory often fail to eliminate loss barriers between two independent solutions (Ainsworth et al., 2022; Benzing et al., 2022). In this work, we conjecture that a more relaxed claim holds: the SGD solution set is a star domain that contains a star model that is linearly connected to all the other solutions via paths with low loss values, modulo permutations. We propose the Starlight algorithm that finds a star model of a given learning task. We validate our claim by showing that this star model is linearly connected with other independently found solutions. As an additional benefit of our study, we demo
    
[^115]: 基于机器学习的气候再分析数据在印度区级水稻产量预测中的可行性研究

    Feasibility of machine learning-based rice yield prediction in India at the district level using climate reanalysis data

    [https://arxiv.org/abs/2403.07967](https://arxiv.org/abs/2403.07967)

    该研究探究了基于机器学习的模型是否能够准确地在水稻收获前几个月预测印度各区 Kharif 季节水稻的产量，研究证明水稻产量可以被合理准确地预测。

    

    产量预测是在庄稼收割之前预测农业生产力的科学，有助于广泛的利益相关方做出更好的农业规划决策。这项研究旨在调查是否机器学习-based 的产量预测模型能够在印度区级准确地预测 Kharif 季节的水稻产量，即在水稻收获几个月前。方法涉及对印度生产水稻的 247 个区的 20 年气候、卫星和水稻产量数据进行 19 个机器学习模型的训练，如 CatBoost、LightGBM、Orthogonal Matching Pursuit 和 Extremely Randomized Trees。除了模型构建，还构建了一个动态仪表板，了解水稻产量预测的可靠性在各区之间是如何变化的。概念验证的机器学习流水线的结果表明，水稻产量可以被合理准确地预测，

    arXiv:2403.07967v1 Announce Type: new  Abstract: Yield forecasting, the science of predicting agricultural productivity before the crop harvest occurs, helps a wide range of stakeholders make better decisions around agricultural planning. This study aims to investigate whether machine learning-based yield prediction models can capably predict Kharif season rice yields at the district level in India several months before the rice harvest takes place. The methodology involved training 19 machine learning models such as CatBoost, LightGBM, Orthogonal Matching Pursuit, and Extremely Randomized Trees on 20 years of climate, satellite, and rice yield data across 247 of Indian rice-producing districts. In addition to model-building, a dynamic dashboard was built understand how the reliability of rice yield predictions varies across districts. The results of the proof-of-concept machine learning pipeline demonstrated that rice yields can be predicted with a reasonable degree of accuracy, with 
    
[^116]: 应用排名技术估计地球变量对温度预报误差的影响

    Applying ranking techniques for estimating influence of Earth variables on temperature forecast error

    [https://arxiv.org/abs/2403.07966](https://arxiv.org/abs/2403.07966)

    本研究介绍了如何分析地球系统变量对温度预报误差的影响，通过引入三个创新：应用数据科学方法在代表性位置进行排名、利用Spearman相关性创建排名并结合其他度量丰富排名、通过学习随机森林模型评估方法论，最终将相关性转化为排名并组合成一个总体排名。

    

    本文描述了如何分析地球系统变量对温度预报误差的影响。获取数据的初始框架基于先前的研究工作，这导致了一个非常有趣的发现。然而，前述研究仅仅针对变量与误差的个体相关性进行工作。本研究将重新使用主要思想，但引入了三个主要创新：(1)通过少数代表性位置应用数据科学方法；(2)利用Spearman相关性创建的排名，但用其他度量丰富它们以寻找更稳健的变量排名；(3)通过学习具有不同实验变化的随机森林模型来评估方法论。其主要贡献是展示如何将相关性转化为排名，并将它们合并成一个总体排名。

    arXiv:2403.07966v1 Announce Type: new  Abstract: This paper describes how to analyze the influence of Earth system variables on the errors when providing temperature forecasts. The initial framework to get the data has been based on previous research work, which resulted in a very interesting discovery. However, the aforementioned study only worked on individual correlations of the variables with respect to the error. This research work is going to re-use the main ideas but introduce three main novelties: (1) applying a data science approach by a few representative locations; (2) taking advantage of the rankings created by Spearman correlation but enriching them with other metrics looking for a more robust ranking of the variables; (3) evaluation of the methodology by learning random forest models for regression with the distinct experimental variations. The main contribution is the framework that shows how to convert correlations into rankings and combine them into an aggregate rankin
    
[^117]: 神经网络中的条件计算: 原理与研究趋势

    Conditional computation in neural networks: principles and research trends

    [https://arxiv.org/abs/2403.07965](https://arxiv.org/abs/2403.07965)

    该论文总结了将条件计算方法应用于设计神经网络的新兴领域中的原理和思想，并介绍了专家混合网络、标记选择机制和提前退出神经网络等三种实现方式。

    

    这篇文章总结了将条件计算方法应用于设计神经网络的新兴领域中的原理和思想。我们特别关注可以根据输入动态激活或去激活其计算图部分的神经网络。例如，动态选择输入标记、层（或一组层）以及每个层内的子模块（例如，卷积滤波器中的通道）。我们首先提供一个通用形式来统一描述这些技术。然后，我们介绍了这些原则的三个值得注意的实现：专家混合（MoEs）网络、标记选择机制和提前退出神经网络。本文旨在向这一不断发展的领域提供类似教程的介绍。为此，我们分析了这些模块化设计在效率、可解释性和迁移学习方面的好处，重点放在...

    arXiv:2403.07965v1 Announce Type: cross  Abstract: This article summarizes principles and ideas from the emerging area of applying \textit{conditional computation} methods to the design of neural networks. In particular, we focus on neural networks that can dynamically activate or de-activate parts of their computational graph conditionally on their input. Examples include the dynamic selection of, e.g., input tokens, layers (or sets of layers), and sub-modules inside each layer (e.g., channels in a convolutional filter). We first provide a general formalism to describe these techniques in an uniform way. Then, we introduce three notable implementations of these principles: mixture-of-experts (MoEs) networks, token selection mechanisms, and early-exit neural networks. The paper aims to provide a tutorial-like introduction to this growing field. To this end, we analyze the benefits of these modular designs in terms of efficiency, explainability, and transfer learning, with a focus on em
    
[^118]: 无监督的前列腺细胞拉曼光谱自组织映射显示疾病状态的亚类群

    Unsupervised self-organising map of prostate cell Raman spectra shows disease-state subclustering

    [https://arxiv.org/abs/2403.07960](https://arxiv.org/abs/2403.07960)

    使用无监督的自组织映射方法成功区分了正常前列腺和癌细胞，并展示了前列腺癌细胞的新子群分类。

    

    前列腺癌是一个提出有趣临床问题的疾病：是否应该治疗？一小部分前列腺癌是侵袭性的，需要切除和治疗以防止转移。然而，传统诊断仍难以对这类患者进行风险分层，因此需要新的生物分子亚类别疾病方法。这里，我们使用无监督的自组织映射方法分析从前列腺细胞系获取的活细胞拉曼光谱数据；我们的目的是测试这种方法在高维数据集上，在最小预处理的情况下，是否能区分癌症和正常细胞以及单细胞水平。结果不仅展示了成功区分正常前列腺和癌细胞，还将前列腺癌细胞系新的亚类群化为两组。对每个癌亚类群的光谱的初步分析展示了一种diff

    arXiv:2403.07960v1 Announce Type: cross  Abstract: Prostate cancer is a disease which poses an interesting clinical question: should it be treated? A small subset of prostate cancers are aggressive and require removal and treatment to prevent metastatic spread. However, conventional diagnostics remain challenged to risk-stratify such patients, hence, new methods of approach to biomolecularly subclassify the disease are needed. Here we use an unsupervised, self-organising map approach to analyse live-cell Raman spectroscopy data obtained from prostate cell-lines; our aim is to test the feasibility of this method to differentiate, at the single-cell-level, cancer from normal using high-dimensional datasets with minimal preprocessing. The results demonstrate not only successful separation of normal prostate and cancer cells, but also a new subclustering of the prostate cancer cell-line into two groups. Initial analysis of the spectra from each of the cancer subclusters demonstrates a diff
    
[^119]: 时间决策：利用早期退出神经网络中的时间相关性进行有效决策

    Temporal Decisions: Leveraging Temporal Correlation for Efficient Decisions in Early Exit Neural Networks

    [https://arxiv.org/abs/2403.07958](https://arxiv.org/abs/2403.07958)

    该论文引入了差异检测和时间耐心作为早期退出神经网络的决策机制，利用传感器数据流中的时间相关性来有效终止推断。

    

    深度学习在嵌入式和物联网应用中变得越来越重要。但是，在嵌入式设备上部署模型面临资源限制的挑战，这可能影响模型的推断准确性和延迟。早期退出神经网络是一种潜在解决方案，它通过在隐藏层之间附加的额外分类器动态调整模型深度。然而，实时终止决策机制对系统的效率、延迟和持续准确性至关重要。本文引入了差异检测和时间耐心作为早期退出神经网络的决策机制。它们利用传感器数据流中存在的时间相关性来有效地终止推断。我们评估了它们在健康监测、图像分类和唤醒词检测任务中的有效性。我们的创新贡献能够减少计算负担。

    arXiv:2403.07958v1 Announce Type: cross  Abstract: Deep Learning is becoming increasingly relevant in Embedded and Internet-of-things applications. However, deploying models on embedded devices poses a challenge due to their resource limitations. This can impact the model's inference accuracy and latency. One potential solution are Early Exit Neural Networks, which adjust model depth dynamically through additional classifiers attached between their hidden layers. However, the real-time termination decision mechanism is critical for the system's efficiency, latency, and sustained accuracy.   This paper introduces Difference Detection and Temporal Patience as decision mechanisms for Early Exit Neural Networks. They leverage the temporal correlation present in sensor data streams to efficiently terminate the inference. We evaluate their effectiveness in health monitoring, image classification, and wake-word detection tasks. Our novel contributions were able to reduce the computational foo
    
[^120]: 高效的后训练增强方法用于异构和分布式物联网环境中的自适应推断

    Efficient Post-Training Augmentation for Adaptive Inference in Heterogeneous and Distributed IoT Environments

    [https://arxiv.org/abs/2403.07957](https://arxiv.org/abs/2403.07957)

    提出了一种自动增强流程，能够将现有模型转换为早期退出神经网络（EENN），提高神经网络部署效率，实现了在物联网和图像分类用例上显著减少推断操作的效果。

    

    早期退出神经网络（EENN）提出了一种增强神经网络部署效率的解决方案。然而，创建EENN具有挑战性，并且需要专业领域知识，由于大量额外的设计选择。为了解决这个问题，我们提出了一种自动增强流程，专注于将现有模型转换为EENN。它执行了部署到异构或分布式硬件目标所需的所有设计决策：我们的框架构建了EENN架构，将其子图映射到硬件目标，并配置了其决策机制。据我们所知，这是第一个能够执行所有这些步骤的框架。我们在一系列物联网和标准图像分类用例上评估了我们的方法。对于语音命令检测任务，我们的解决方案能够将每次推断的平均操作减少了59.67%。

    arXiv:2403.07957v1 Announce Type: cross  Abstract: Early Exit Neural Networks (EENNs) present a solution to enhance the efficiency of neural network deployments. However, creating EENNs is challenging and requires specialized domain knowledge, due to the large amount of additional design choices. To address this issue, we propose an automated augmentation flow that focuses on converting an existing model into an EENN. It performs all required design decisions for the deployment to heterogeneous or distributed hardware targets: Our framework constructs the EENN architecture, maps its subgraphs to the hardware targets, and configures its decision mechanism. To the best of our knowledge, it is the first framework that is able to perform all of these steps.   We evaluated our approach on a collection of Internet-of-Things and standard image classification use cases. For a speech command detection task, our solution was able to reduce the mean operations per inference by 59.67%. For an ECG 
    
[^121]: DeepCDCL: 基于CDCL算法的神经网络验证框架

    DeepCDCL: An CDCL-based Neural Network Verification Framework

    [https://arxiv.org/abs/2403.07956](https://arxiv.org/abs/2403.07956)

    提出了一种基于CDCL算法的神经网络验证框架DeepCDCL，通过引入异步子句学习和管理结构，显著减少了时间消耗，并在ACAS Xu和MNIST数据集上展示了显著的加速。

    

    在安全关键应用中，神经网络面临越来越多的安全和安全性问题，这是由于它们对微小扰动的敏感性。在本文中，我们提出了DeepCDCL，这是一种基于冲突驱动子句学习（CDCL）算法的新型神经网络验证框架。我们引入了一种异步子句学习和管理结构，相比直接应用CDCL框架，减少了冗余的时间消耗。此外，我们还对我们的方法在ACAS Xu和MNIST数据集上的性能进行了详细评估，结果显示在大多数情况下实现了显著的加速。

    arXiv:2403.07956v1 Announce Type: cross  Abstract: Neural networks in safety-critical applications face increasing safety and security concerns due to their susceptibility to little disturbance. In this paper, we propose DeepCDCL, a novel neural network verification framework based on the Conflict-Driven Clause Learning (CDCL) algorithm. We introduce an asynchronous clause learning and management structure, reducing redundant time consumption compared to the direct application of the CDCL framework. Furthermore, we also provide a detailed evaluation of the performance of our approach on the ACAS Xu and MNIST datasets, showing that a significant speed-up is achieved in most cases.
    
[^122]: 朝着忠实解释：利用快捷方式发现来增强理性化

    Towards Faithful Explanations: Boosting Rationalization with Shortcuts Discovery

    [https://arxiv.org/abs/2403.07955](https://arxiv.org/abs/2403.07955)

    本文提出了一种Shortcuts-fused Selective Rationalization (SSR)方法，通过发现和利用潜在的快捷方式来提升理性化，并通过两种策略缓解利用快捷方式来组成理性化的问题，以及通过数据增强方法来补充已注释理性化数量的差距。

    

    人工智能领域神经网络的显著成功引发了有选择性的理性化。本文提出了一种Shortcuts-fused Selective Rationalization (SSR)方法，通过发现和利用潜在的快捷方式来提升理性化。具体而言，SSR首先设计了一种快捷方式发现方法来检测几个潜在的快捷方式。然后，通过引入识别出的快捷方式，我们提出了两种策略来缓解利用快捷方式来组成理性化的问题。最后，我们开发了两种数据增强方法来弥补已注释理性化数量的差距。对真实世界数据集的广泛实验结果清楚地验证了这一方法的有效性。

    arXiv:2403.07955v1 Announce Type: cross  Abstract: The remarkable success in neural networks provokes the selective rationalization. It explains the prediction results by identifying a small subset of the inputs sufficient to support them. Since existing methods still suffer from adopting the shortcuts in data to compose rationales and limited large-scale annotated rationales by human, in this paper, we propose a Shortcuts-fused Selective Rationalization (SSR) method, which boosts the rationalization by discovering and exploiting potential shortcuts. Specifically, SSR first designs a shortcuts discovery approach to detect several potential shortcuts. Then, by introducing the identified shortcuts, we propose two strategies to mitigate the problem of utilizing shortcuts to compose rationales. Finally, we develop two data augmentations methods to close the gap in the number of annotated rationales. Extensive experimental results on real-world datasets clearly validate the effectiveness of
    
[^123]: 优化多项式图滤波器：一种新的自适应克里洛夫子空间方法

    Optimizing Polynomial Graph Filters: A Novel Adaptive Krylov Subspace Approach

    [https://arxiv.org/abs/2403.07954](https://arxiv.org/abs/2403.07954)

    本文通过统一多项式图滤波器和相同次数的最优滤波器到同阶克里洛夫子空间，提供了等效的表达能力；设计了一种新的自适应克里洛夫子空间方法，以优化具有可控性的多项式基准。

    

    图神经网络（GNN），也称为谱图滤波器，在网络图中有着广泛的应用。为了绕过特征分解，提出了使用各种多项式基准进行滤波器训练的多项式图滤波器，以近似图滤波器。然而，目前没有研究从统一的角度探讨多样化的多项式图滤波器进行优化。本文首先将多项式图滤波器以及相同次数的最优滤波器统一成同阶克里洛夫子空间，从而在理论上提供相同的表达能力。接下来，我们从统一的克里洛夫子空间角度研究多项式的渐近收敛属性，揭示了它们在具有不同异质程度的图中的有限适应性。受到这些事实的启发，我们设计了一种新颖的自适应克里洛夫子空间方法，以优化多项式基准，并可证明具有可控性。

    arXiv:2403.07954v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs), known as spectral graph filters, find a wide range of applications in web networks. To bypass eigendecomposition, polynomial graph filters are proposed to approximate graph filters by leveraging various polynomial bases for filter training. However, no existing studies have explored the diverse polynomial graph filters from a unified perspective for optimization.   In this paper, we first unify polynomial graph filters, as well as the optimal filters of identical degrees into the Krylov subspace of the same order, thus providing equivalent expressive power theoretically. Next, we investigate the asymptotic convergence property of polynomials from the unified Krylov subspace perspective, revealing their limited adaptability in graphs with varying heterophily degrees. Inspired by those facts, we design a novel adaptive Krylov subspace approach to optimize polynomial bases with provable controllability over the
    
[^124]: 通过结构化稀疏张量分解对稀疏DNN加速进行抽象化

    Abstracting Sparse DNN Acceleration via Structured Sparse Tensor Decomposition

    [https://arxiv.org/abs/2403.07953](https://arxiv.org/abs/2403.07953)

    本文提出了通过结构化分解张量进一步抽象稀疏DNN加速的方法，实现了将稀疏张量转换成一系列结构化稀疏张量，从而弥合了稀疏DNN模型和硬件之间的差距。

    

    在深度神经网络（DNNs）中利用稀疏性已成为满足现代DNN日益增长的计算需求的一种具有前景的领域。然而，在实践中，稀疏DNN加速仍然面临一个关键挑战。为了最小化稀疏加速的开销，硬件设计师最近提出了结构化稀疏硬件支持，这提供了有限的灵活性并需要额外的模型微调。此外，为某些结构化稀疏硬件微调的任何稀疏模型无法被其他结构化硬件加速。为了弥合稀疏DNN模型和硬件之间的差距，本文提出了通过结构分解的张量近似（TASD），利用了线性代数中的分配性质将任何稀疏张量转化为一系列结构化稀疏张量。接下来，我们开发了一个软件框架TASDER，通过搜索逐层高质量的结构化分解来加速DNNs的权重和...

    arXiv:2403.07953v1 Announce Type: cross  Abstract: Exploiting sparsity in deep neural networks (DNNs) has been a promising area to meet the growing computation need of modern DNNs. However, in practice, sparse DNN acceleration still faces a key challenge. To minimize the overhead of sparse acceleration, hardware designers have proposed structured sparse hardware support recently, which provides limited flexibility and requires extra model fine-tuning. Moreover, any sparse model fine-tuned for certain structured sparse hardware cannot be accelerated by other structured hardware. To bridge the gap between sparse DNN models and hardware, this paper proposes tensor approximation via structured decomposition (TASD), which leverages the distributive property in linear algebra to turn any sparse tensor into a series of structured sparse tensors. Next, we develop a software framework, TASDER, to accelerate DNNs by searching layer-wise, high-quality structured decomposition for both weight and 
    
[^125]: SAMDA：利用SAM进行电子显微镜分割的少样本领域自适应

    SAMDA: Leveraging SAM on Few-Shot Domain Adaptation for Electronic Microscopy Segmentation

    [https://arxiv.org/abs/2403.07951](https://arxiv.org/abs/2403.07951)

    SAMDA框架结合了SAM和nnUNet，通过融合“专家”和“通用”组件，在少样本领域自适应中解决了大规模视觉基础模型面临的转移性和精度问题

    

    传统的电子显微镜分割深度学习方法通常在样本和注释有限时存在转移性较低的问题，而大规模视觉基础模型在不同领域之间转移时更具鲁棒性，但在微调下面临亚最优改进。在这项工作中，我们提出了一种新的少样本领域自适应框架SAMDA，它将“Segment Anything Model (SAM)”与nnUNet相结合到嵌入空间中以实现高转移性和准确性。具体来说，我们选择基于Unet的网络作为“专家”组件高效学习分割特征，并设计了基于SAM的自适应模块作为“通用”组件用于领域转移。通过融合“通用”和“专家”组件，我们缓解了大规模视觉基础模型中复杂预训练知识中的模态不平衡和转移挑战。

    arXiv:2403.07951v1 Announce Type: cross  Abstract: It has been shown that traditional deep learning methods for electronic microscopy segmentation usually suffer from low transferability when samples and annotations are limited, while large-scale vision foundation models are more robust when transferring between different domains but facing sub-optimal improvement under fine-tuning. In this work, we present a new few-shot domain adaptation framework SAMDA, which combines the Segment Anything Model(SAM) with nnUNet in the embedding space to achieve high transferability and accuracy. Specifically, we choose the Unet-based network as the "expert" component to learn segmentation features efficiently and design a SAM-based adaptation module as the "generic" component for domain transfer. By amalgamating the "generic" and "expert" components, we mitigate the modality imbalance in the complex pre-training knowledge inherent to large-scale Vision Foundation models and the challenge of transfer
    
[^126]: 评估一种混合使用Sepedi-英语的自动语音识别系统

    The evaluation of a code-switched Sepedi-English automatic speech recognition system

    [https://arxiv.org/abs/2403.07947](https://arxiv.org/abs/2403.07947)

    评估了一种混合使用Sepedi-英语的自动语音识别系统，探讨了端到端方法在低资源语言中的有效性。

    

    arXiv:2403.07947v1 发表类型: 跨领域  摘要: 语音技术是一个涵盖各种技术和工具的领域，用于使设备能够与语音进行交互，例如自动语音识别（ASR）、口语对话系统等，允许设备通过麦克风从说话者那里捕获说话的话语。端到端方法，如连接主义时间分类（CTC）和基于注意力的方法是开发ASR系统中使用最广泛的方法。然而，这些技术通常用于对具有大量语音数据进行训练和评估的高资源语言进行研究和开发，导致低资源语言相对较少发展。尽管CTC方法在其他语言中已成功使用，但它在Sepedi语言中的有效性仍存在不确定性。在这项研究中，我们介绍了混合使用Sepedi-英语的自动语音识别系统的评估。

    arXiv:2403.07947v1 Announce Type: cross  Abstract: Speech technology is a field that encompasses various techniques and tools used to enable machines to interact with speech, such as automatic speech recognition (ASR), spoken dialog systems, and others, allowing a device to capture spoken words through a microphone from a human speaker. End-to-end approaches such as Connectionist Temporal Classification (CTC) and attention-based methods are the most used for the development of ASR systems. However, these techniques were commonly used for research and development for many high-resourced languages with large amounts of speech data for training and evaluation, leaving low-resource languages relatively underdeveloped. While the CTC method has been successfully used for other languages, its effectiveness for the Sepedi language remains uncertain. In this study, we present the evaluation of the Sepedi-English code-switched automatic speech recognition system. This end-to-end system was devel
    
[^127]: 一个解决神经技术认知安全问题的数学框架

    A Mathematical Framework for the Problem of Security for Cognition in Neurotechnology

    [https://arxiv.org/abs/2403.07945](https://arxiv.org/abs/2403.07945)

    本文提出了一个数学框架，名为认知安全，用于描述和分析神经技术对个体认知隐私和自治可能产生的影响，解决了相关问题描述和分析的障碍。

    

    近年来神经技术的快速发展在神经技术和安全之间创造了一个新兴的关键交叉点。植入式设备、非侵入式监测和非侵入式治疗都带来了违反个体认知隐私和自治的前景。越来越多的科学家和医生呼吁解决这一问题 -- 我们称之为认知安全 -- 但应用工作受到限制。阻碍科学和工程努力解决认知安全问题的一个主要障碍是缺乏清晰描述和分析相关问题的手段。在本文中，我们开发了认知安全，这是一个数学框架，通过借鉴多个领域的方法和结果，实现这种描述和分析。我们展示了一些对认知安全有重要影响的统计特性，然后提出描述...

    arXiv:2403.07945v1 Announce Type: cross  Abstract: The rapid advancement in neurotechnology in recent years has created an emerging critical intersection between neurotechnology and security. Implantable devices, non-invasive monitoring, and non-invasive therapies all carry with them the prospect of violating the privacy and autonomy of individuals' cognition. A growing number of scientists and physicians have made calls to address this issue -- which we term Cognitive Security -- but applied efforts have been limited. A major barrier hampering scientific and engineering efforts to address Cognitive Security is the lack of a clear means of describing and analyzing relevant problems. In this paper we develop Cognitive Security, a mathematical framework which enables such description and analysis by drawing on methods and results from multiple fields. We demonstrate certain statistical properties which have significant implications for Cognitive Security, and then present descriptions of
    
[^128]: 重新审视图神经网络中的边扰动在图数据增强和攻击中的作用

    Revisiting Edge Perturbation for Graph Neural Network in Graph Data Augmentation and Attack

    [https://arxiv.org/abs/2403.07943](https://arxiv.org/abs/2403.07943)

    本文提出了一种统一公式，明确界定了两类边扰动方法之间的清晰界限，并解答了为何边扰动具有双重效果以及何使边扰动灵活有效的问题。

    

    边扰动是修改图结构的一种基本方法。它可以根据对图神经网络（GNN）性能的影响分为两种类别，即图数据增强和攻击。令人惊讶的是，边扰动方法的两种类别都采用相同的操作，但对GNN的准确性产生相反的影响。目前尚未清晰地定义这些方法在使用边扰动时的明显界限。因此，不当的扰动可能导致不良后果，需要精确调整以实现期望的效果。因此，“边扰动为何效果两极？”和“何使边扰动灵活有效？”仍然没有答案。本文通过提出统一公式并建立两类边扰动方法之间的明确界限来回答这些问题。具体地，

    arXiv:2403.07943v1 Announce Type: new  Abstract: Edge perturbation is a basic method to modify graph structures. It can be categorized into two veins based on their effects on the performance of graph neural networks (GNNs), i.e., graph data augmentation and attack. Surprisingly, both veins of edge perturbation methods employ the same operations, yet yield opposite effects on GNNs' accuracy. A distinct boundary between these methods in using edge perturbation has never been clearly defined. Consequently, inappropriate perturbations may lead to undesirable outcomes, necessitating precise adjustments to achieve desired effects. Therefore, questions of ``why edge perturbation has a two-faced effect?'' and ``what makes edge perturbation flexible and effective?'' still remain unanswered.   In this paper, we will answer these questions by proposing a unified formulation and establishing a clear boundary between two categories of edge perturbation methods. Specifically, we conduct experiments
    
[^129]: 使用深度学习技术检测头发和头皮疾病

    Hair and scalp disease detection using deep learning

    [https://arxiv.org/abs/2403.07940](https://arxiv.org/abs/2403.07940)

    这项研究提出了一种使用深度学习技术检测头发和头皮疾病的方法，通过卷积神经网络分析图像，实现了皮肤病变的早期检测和诊断。

    

    近年来，医疗保健和技术整合方面取得了显著进展，特别在医学图像分析领域表现明显。本文介绍了皮肤科领域的一种开创性方法，利用最先进的深度学习技术来检测头发和头皮疾病。我们的方法依赖于卷积神经网络（CNNs），以其在图像识别中的高效性而闻名，对图像进行细致分析，以检测影响头发和头皮的各种皮肤病症。我们提出的系统代表了皮肤诊断方面的重大进步，提供了一种非侵入性和高效率的早期检测和诊断手段。通过利用CNNs的能力，我们的模型有潜力彻底改变皮肤科学，提供易获取和及时的医疗解决方案。

    arXiv:2403.07940v1 Announce Type: cross  Abstract: In recent years, there has been a notable advancement in the integration of healthcare and technology, particularly evident in the field of medical image analysis. This paper introduces a pioneering approach in dermatology, presenting a robust method for the detection of hair and scalp diseases using state-of-the-art deep learning techniques. Our methodology relies on Convolutional Neural Networks (CNNs), well-known for their efficacy in image recognition, to meticulously analyze images for various dermatological conditions affecting the hair and scalp. Our proposed system represents a significant advancement in dermatological diagnostics, offering a non-invasive and highly efficient means of early detection and diagnosis. By leveraging the capabilities of CNNs, our model holds the potential to revolutionize dermatology, providing accessible and timely healthcare solutions. Furthermore, the seamless integration of our trained model int
    
[^130]: 文本到音频生成与视频同步

    Text-to-Audio Generation Synchronized with Videos

    [https://arxiv.org/abs/2403.07938](https://arxiv.org/abs/2403.07938)

    介绍了一个新的与视频对齐的文本到音频生成基准T2AV-Bench，以及一种简单而有效的视频对齐TTA生成模型T2AV，改进了传统方法，并融合了视觉对齐文本嵌入。

    

    最近，人们对文本到音频（TTA）生成的关注日益加强，研究人员努力从文本描述中合成音频。然而，大多数现有方法虽然利用潜在扩散模型来学习音频和文本嵌入之间的关系，但在保持生成的音频与其视频之间无缝同步方面表现不佳。这经常导致可察觉的音频-视觉不匹配。为填补这一空白，我们引入了一个与视频对齐的文本到音频生成的开创性基准，名为T2AV-Bench。该基准通过三个专门用于评估视觉对齐和时间一致性的新颖指标而脱颖而出。为了补充这一点，我们还提出了一个简单而有效的视频对齐TTA生成模型，即T2AV。超越传统方法，T2AV通过将视觉对齐文本嵌入集成为其c

    arXiv:2403.07938v1 Announce Type: cross  Abstract: In recent times, the focus on text-to-audio (TTA) generation has intensified, as researchers strive to synthesize audio from textual descriptions. However, most existing methods, though leveraging latent diffusion models to learn the correlation between audio and text embeddings, fall short when it comes to maintaining a seamless synchronization between the produced audio and its video. This often results in discernible audio-visual mismatches. To bridge this gap, we introduce a groundbreaking benchmark for Text-to-Audio generation that aligns with Videos, named T2AV-Bench. This benchmark distinguishes itself with three novel metrics dedicated to evaluating visual alignment and temporal consistency. To complement this, we also present a simple yet effective video-aligned TTA generation model, namely T2AV. Moving beyond traditional methods, T2AV refines the latent diffusion approach by integrating visual-aligned text embeddings as its c
    
[^131]: 语音鲁棒基准：用于语音识别的鲁棒性基准

    Speech Robust Bench: A Robustness Benchmark For Speech Recognition

    [https://arxiv.org/abs/2403.07937](https://arxiv.org/abs/2403.07937)

    提出了一个全面基准（SRB），用于评估自动语音识别（ASR）模型对各种破坏的鲁棒性，发现模型大小和某些建模选择有助于提高鲁棒性，并观察到在不同人口亚组上模型的鲁棒性存在明显差异。

    

    随着自动语音识别（ASR）模型变得越来越普遍，确保它们在物理世界和数字世界中的各种破坏下进行可靠预测变得愈发重要。我们提出了语音鲁棒基准（SRB），这是一个用于评估ASR模型对各种破坏的鲁棒性的全面基准。SRB由69个输入扰动组成，旨在模拟ASR模型可能在物理世界和数字世界中遇到的各种破坏。我们使用SRB来评估几种最先进的ASR模型的鲁棒性，并观察到模型大小和某些建模选择（如离散表示和自我训练）似乎有助于提高鲁棒性。我们将此分析扩展到衡量ASR模型在来自各种人口亚组的数据上的鲁棒性，即英语和西班牙语使用者以及男性和女性，并观察到模型的鲁棒性在不同亚组之间存在明显差异。

    arXiv:2403.07937v1 Announce Type: cross  Abstract: As Automatic Speech Recognition (ASR) models become ever more pervasive, it is important to ensure that they make reliable predictions under corruptions present in the physical and digital world. We propose Speech Robust Bench (SRB), a comprehensive benchmark for evaluating the robustness of ASR models to diverse corruptions. SRB is composed of 69 input perturbations which are intended to simulate various corruptions that ASR models may encounter in the physical and digital world. We use SRB to evaluate the robustness of several state-of-the-art ASR models and observe that model size and certain modeling choices such as discrete representations, and self-training appear to be conducive to robustness. We extend this analysis to measure the robustness of ASR models on data from various demographic subgroups, namely English and Spanish speakers, and males and females, and observed noticeable disparities in the model's robustness across su
    
[^132]: 具有抗数据损坏特性的离线两人零和马尔科夫博弈

    Corruption-Robust Offline Two-Player Zero-Sum Markov Games

    [https://arxiv.org/abs/2403.07933](https://arxiv.org/abs/2403.07933)

    在离线两人零和马尔科夫博弈中，我们提出了抗数据损坏的学习算法以识别近似纳什均衡策略对，并通过鲁棒版本的极小极大值迭代算法实现了（近）最优结果。

    

    我们研究了离线两人零和马尔科夫博弈中的数据损坏鲁棒性。给定两名玩家实现轨迹数据集，对手可以修改其中的 $\epsilon$ 部分。学习者的目标是从已损坏的数据中识别出一个近似的纳什均衡策略对。我们在线性马尔科夫博弈中考虑了这个问题，涵盖了不同程度的数据覆盖和损坏。我们首先针对任何学习者的次优间隙提供了一个信息论下界。接下来，我们提出了悲观极小极大值迭代算法的鲁棒版本，分别在损坏数据上的覆盖和仅在干净数据上的覆盖下，并展示它们相对于 $\epsilon$ 实现（近）最优子优差边界。我们指出，我们是首个提供这种在离线两人零和马尔科夫博弈中学习近似纳什均衡策略问题特性描述的研究。

    arXiv:2403.07933v1 Announce Type: cross  Abstract: We study data corruption robustness in offline two-player zero-sum Markov games. Given a dataset of realized trajectories of two players, an adversary is allowed to modify an $\epsilon$-fraction of it. The learner's goal is to identify an approximate Nash Equilibrium policy pair from the corrupted data. We consider this problem in linear Markov games under different degrees of data coverage and corruption. We start by providing an information-theoretic lower bound on the suboptimality gap of any learner. Next, we propose robust versions of the Pessimistic Minimax Value Iteration algorithm, both under coverage on the corrupted data and under coverage only on the clean data, and show that they achieve (near)-optimal suboptimality gap bounds with respect to $\epsilon$. We note that we are the first to provide such a characterization of the problem of learning approximate Nash Equilibrium policies in offline two-player zero-sum Markov game
    
[^133]: 通过高斯过程对热核进行草图: 在低维欧几里德空间中嵌入数据

    Sketching the Heat Kernel: Using Gaussian Processes to Embed Data

    [https://arxiv.org/abs/2403.07929](https://arxiv.org/abs/2403.07929)

    通过高斯过程实现数据的低维嵌入，将热核作为协方差函数进行计算，拟合出嵌入中的直线距离以概率方式近似扩散距离，保留了一些较小尺度结构，同时具有对异常值的更强鲁棒性

    

    本文介绍了一种新颖的、非确定性的方法，基于计算依赖于数据几何特性的高斯过程实现数据在低维欧几里德空间中的嵌入。此类嵌入首次出现在（Adler等人，2018）中，作为高维通用流形的理论模型。具体来说，我们将高斯过程的协方差函数设置为热核，计算嵌入相当于草绘代表热核的矩阵。Karhunen-Lo\`eve展开表明，在嵌入中的直线距离以概率意义上近似扩散距离，避免了对尖锐截断的需求，并保持了一些较小尺度结构。我们的方法在对异常值具有更强的鲁棒性。我们通过理论和实验来证明这种方法的有效性。

    arXiv:2403.07929v1 Announce Type: new  Abstract: This paper introduces a novel, non-deterministic method for embedding data in low-dimensional Euclidean space based on computing realizations of a Gaussian process depending on the geometry of the data. This type of embedding first appeared in (Adler et al, 2018) as a theoretical model for a generic manifold in high dimensions.   In particular, we take the covariance function of the Gaussian process to be the heat kernel, and computing the embedding amounts to sketching a matrix representing the heat kernel. The Karhunen-Lo\`eve expansion reveals that the straight-line distances in the embedding approximate the diffusion distance in a probabilistic sense, avoiding the need for sharp cutoffs and maintaining some of the smaller-scale structure.   Our method demonstrates further advantage in its robustness to outliers. We justify the approach with both theory and experiments.
    
[^134]: 云服务的智能监控框架: 一种数据驱动的方法

    Intelligent Monitoring Framework for Cloud Services: A Data-Driven Approach

    [https://arxiv.org/abs/2403.07927](https://arxiv.org/abs/2403.07927)

    提出一种智能监控框架，根据服务属性为云服务推荐监控器，通过挖掘监视器属性并建立结构化本体论，解决了当前监控创建过程中的不完整覆盖和冗余问题。

    

    云服务所有者需要不断监测其服务，以确保高可用性和可靠性。 监控中的空白可能导致事件检测延迟和显着的负面客户影响。 目前的监控创建过程是临时和反应性的。 开发人员使用其部落知识，主要是基于试错的过程创建监控器。 因此，监控器经常具有不完整的覆盖范围，这会导致生产问题，或者冗余性，从而导致噪音和浪费努力。在这项工作中，我们通过提出一种智能监控框架来解决这一问题，该框架根据其服务属性为云服务推荐监控器。 我们首先从微软的791个生产服务中挖掘了30,000多个监视器的属性，并为监视器导出了一个结构化本体论。 我们专注于两个关键维度: 要监视的内容(资源)和要监视的指标。

    arXiv:2403.07927v1 Announce Type: cross  Abstract: Cloud service owners need to continuously monitor their services to ensure high availability and reliability. Gaps in monitoring can lead to delay in incident detection and significant negative customer impact. Current process of monitor creation is ad-hoc and reactive in nature. Developers create monitors using their tribal knowledge and, primarily, a trial and error based process. As a result, monitors often have incomplete coverage which leads to production issues, or, redundancy which results in noise and wasted effort.   In this work, we address this issue by proposing an intelligent monitoring framework that recommends monitors for cloud services based on their service properties. We start by mining the attributes of 30,000+ monitors from 791 production services at Microsoft and derive a structured ontology for monitors. We focus on two crucial dimensions: what to monitor (resources) and which metrics to monitor. We conduct an ex
    
[^135]: 使用深度学习进行时空步态数据的数值预测

    Value Prediction for Spatiotemporal Gait Data Using Deep Learning

    [https://arxiv.org/abs/2403.07926](https://arxiv.org/abs/2403.07926)

    该研究将深度学习应用于时空步态数据的数值预测，探索了不同的深度学习架构，并在短距离和长距离预测方面取得了成功

    

    人类步态通常用于诊断和评估医学状况，并在治疗和康复过程中监测进展。利用捕获压力或运动的可穿戴传感器，分析步态数据以帮助恢复、识别活动或识别个体的技术已经出现。深度学习通常采用分类，在计算机视觉、生物医学成像分析以及自然语言处理等各种应用中取得了成功。我们将深度学习应用拓展到时空步态数据的数值预测。此外，我们探索了几种深度学习架构（循环神经网络（RNN）和RNN与卷积神经网络（CNN）相结合），使用两种不同的实验设置进行近程和远程预测。我们的结果显示，近程预测的均方根误差（RMSE）可以低至0。

    arXiv:2403.07926v1 Announce Type: cross  Abstract: Human gait has been commonly used for the diagnosis and evaluation of medical conditions and for monitoring the progress during treatment and rehabilitation. The use of wearable sensors that capture pressure or motion has yielded techniques that analyze the gait data to aid recovery, identify activity performed, or identify individuals. Deep learning, usually employing classification, has been successfully utilized in a variety of applications such as computer vision, biomedical imaging analysis, and natural language processing. We expand the application of deep learning to value prediction of time-series of spatiotemporal gait data. Moreover, we explore several deep learning architectures (Recurrent Neural Networks (RNN) and RNN combined with Convolutional Neural Networks (CNN)) to make short- and long-distance predictions using two different experimental setups. Our results show that short-distance prediction has an RMSE as low as 0.
    
[^136]: 物理信息生成模型用于类药分子构象

    Physics-informed generative model for drug-like molecule conformers

    [https://arxiv.org/abs/2403.07925](https://arxiv.org/abs/2403.07925)

    该模型基于扩散生成，结合深度学习技术从大型数据集中推断原子类型和几何参数，实现了类药分子构象的高精度生成，优于传统方法。

    

    我们提出了一个基于扩散的生成模型，用于构象生成。我们的模型侧重于重现成键结构，并且是从传统力场中通常找到的相关项构建的，以确保物理相关性表示。深度学习技术被用来从训练集中推断出原子类型和几何参数。通过利用最近在扩散生成方面的进展实现构象采样。通过对大型的、多样化的、类药分子的合成数据集进行训练，优化了半经验GFN2-xTB方法，实现了对成键参数的高精度预测，超过了传统的基于知识的方法。结果还与蛋白质数据库（PDB）和剑桥结构数据库（CSD）中的实验结构进行了比较。

    arXiv:2403.07925v1 Announce Type: cross  Abstract: We present a diffusion-based, generative model for conformer generation. Our model is focused on the reproduction of bonded structure and is constructed from the associated terms traditionally found in classical force fields to ensure a physically relevant representation. Techniques in deep learning are used to infer atom typing and geometric parameters from a training set. Conformer sampling is achieved by taking advantage of recent advancements in diffusion-based generation. By training on large, synthetic data sets of diverse, drug-like molecules optimized with the semiempirical GFN2-xTB method, high accuracy is achieved for bonded parameters, exceeding that of conventional, knowledge-based methods. Results are also compared to experimental structures from the Protein Databank (PDB) and Cambridge Structural Database (CSD).
    
[^137]: 深度强化学习与边缘计算在物联网环境中的实时监控与控制优化融合

    The Fusion of Deep Reinforcement Learning and Edge Computing for Real-time Monitoring and Control Optimization in IoT Environments

    [https://arxiv.org/abs/2403.07923](https://arxiv.org/abs/2403.07923)

    本文提出了一种基于深度强化学习和边缘计算的优化控制系统，通过云边协同和动态资源分配实现工业目标的监控和优化，显著提升了系统性能，并节省了成本。

    

    针对工业物联网环境中对实时性能和控制质量的需求，本文提出了基于深度强化学习和边缘计算的优化控制系统。该系统利用云边协同，部署轻量级策略网络在边缘，预测系统状态，并以高频率输出控制，实现工业目标的监控和优化。此外，设计了动态资源分配机制，以确保边缘计算资源的合理调度，实现全局优化。结果表明，该方法减少了云边通信延迟，加快了对异常情况的响应，降低了系统故障率，延长了设备平均运行时间，并节省了手动维护和更换成本。这确保了实时和稳定的控制。

    arXiv:2403.07923v1 Announce Type: cross  Abstract: In response to the demand for real-time performance and control quality in industrial Internet of Things (IoT) environments, this paper proposes an optimization control system based on deep reinforcement learning and edge computing. The system leverages cloud-edge collaboration, deploys lightweight policy networks at the edge, predicts system states, and outputs controls at a high frequency, enabling monitoring and optimization of industrial objectives. Additionally, a dynamic resource allocation mechanism is designed to ensure rational scheduling of edge computing resources, achieving global optimization. Results demonstrate that this approach reduces cloud-edge communication latency, accelerates response to abnormal situations, reduces system failure rates, extends average equipment operating time, and saves costs for manual maintenance and replacement. This ensures real-time and stable control.
    
[^138]: Merino：基于熵驱动的IoT设备上生成式语言模型设计

    Merino: Entropy-driven Design for Generative Language Models on IoT Devices

    [https://arxiv.org/abs/2403.07921](https://arxiv.org/abs/2403.07921)

    在本文中，我们提出了一个新颖的信息熵框架，用于设计手机友好的生成式语言模型，通过最大化transformer解码器的熵来在计算预算内，成功设计了MeRino模型，在移动设置下展现出与当前最先进的自回归transformer模型竞争性能的特点

    

    大规模生成式语言模型（LLMs）作为人工智能现代时代的革命性进步，然而，直接部署LLMs在资源受限的硬件上，比如物联网（IoT）设备，由于其高计算成本而变得困难。在本文中，我们提出了一个新颖的信息熵框架，用于设计手机友好的生成式语言模型。我们的主要设计范式是在给定的计算预算内最大化transformer解码器的熵。整个设计过程涉及解决一个数学规划（MP）问题，可以在几分钟内在CPU上完成，使其几乎是零成本的。我们评估了我们设计的模型MeRino，在九个NLP下游任务上展示了它们在移动设置下对抗当前最先进的自回归transformer模型的竞争性表现。值得注意的是，MeRino在移动设置下获得了类似或更好的零性能表现

    arXiv:2403.07921v1 Announce Type: cross  Abstract: Generative Large Language Models (LLMs) stand as a revolutionary advancement in the modern era of artificial intelligence (AI). However, directly deploying LLMs in resource-constrained hardware, such as Internet-of-Things (IoT) devices, is difficult due to their high computational cost. In this paper, we propose a novel information-entropy framework for designing mobile-friendly generative language models. Our key design paradigm is to maximize the entropy of transformer decoders within the given computational budgets. The whole design procedure involves solving a mathematical programming (MP) problem, which can be done on the CPU within minutes, making it nearly zero-cost. We evaluate our designed models, termed MeRino, across nine NLP downstream tasks, showing their competitive performance against the state-of-the-art autoregressive transformer models under the mobile setting. Notably, MeRino achieves similar or better zero performan
    
[^139]: ProtLLM：一种具有蛋白质作为单词预训练的交织式蛋白质-语言LLM

    ProtLLM: An Interleaved Protein-Language LLM with Protein-as-Word Pre-Training

    [https://arxiv.org/abs/2403.07920](https://arxiv.org/abs/2403.07920)

    提出了ProtLLM，一种具有独特动态蛋白质装配机制及蛋白质作为单词语言建模方法的交织式蛋白质-语言LLM，并构建了大规模的交织式蛋白质-文本数据集用于预训练。

    

    我们提出了ProtLLM，一种多功能的跨模式大型语言模型（LLM），用于处理既有蛋白质为中心又有蛋白质-语言任务。ProtLLM具有独特的动态蛋白质装配机制，使其能够处理自然语言文本与任意数量的蛋白质交织在一起的复杂输入。此外，我们提出了蛋白质作为单词语言建模方法来训练ProtLLM。通过开发专门的蛋白质词汇表，我们赋予该模型不仅预测自然语言而且预测来自大量候选蛋白质的能力。此外，我们构建了一个大规模的交织式蛋白质-文本数据集，命名为InterPT，用于预训练。该数据集全面涵盖了结构化数据源（如蛋白质注释）和非结构化数据源（如生物研究论文），从而赋予ProtLLM理解蛋白质的关键知识。我们在经典数据集上对ProtLLM进行了评估。

    arXiv:2403.07920v1 Announce Type: cross  Abstract: We propose ProtLLM, a versatile cross-modal large language model (LLM) for both protein-centric and protein-language tasks. ProtLLM features a unique dynamic protein mounting mechanism, enabling it to handle complex inputs where the natural language text is interspersed with an arbitrary number of proteins. Besides, we propose the protein-as-word language modeling approach to train ProtLLM. By developing a specialized protein vocabulary, we equip the model with the capability to predict not just natural language but also proteins from a vast pool of candidates. Additionally, we construct a large-scale interleaved protein-text dataset, named InterPT, for pre-training. This dataset comprehensively encompasses both (1) structured data sources like protein annotations and (2) unstructured data sources like biological research papers, thereby endowing ProtLLM with crucial knowledge for understanding proteins. We evaluate ProtLLM on classic 
    
[^140]: 论开放基金会模型的社会影响

    On the Societal Impact of Open Foundation Models

    [https://arxiv.org/abs/2403.07918](https://arxiv.org/abs/2403.07918)

    开放基金会模型具有广泛可用的模型权重，带来了重大利益，但也存在边际风险，需要进一步研究来评估其相对于现有技术的安全性。

    

    arXiv:2403.07918v1 公告类型：交叉摘要：基金会模型是强大的技术：它们如何公开发布直接影响了它们的社会影响。 在这篇立场论文中，我们关注开放基金会模型，在这里定义为具有广泛可用模型权重的模型（例如Llama 2、Stable Diffusion XL）。 我们确定了开放基金会模型的五个独特属性（例如更大的可定制性、较差的监控），这些属性导致了它们的利益和风险。 开放基金会模型提供了重大的利益，但也有一些限制，涵盖了创新、竞争、决策权的分配以及透明度。为了理解其被滥用的风险，我们设计了一个用于分析其边际风险的风险评估框架。 在几个滥用向量（例如网络攻击、生物武器）中，我们发现目前的研究不足以有效地表征开放基金会模型相对于现有技术的边际风险。 该框架有助于扩展论文中建议的分析以估计新技术的安全性边际风险。

    arXiv:2403.07918v1 Announce Type: cross  Abstract: Foundation models are powerful technologies: how they are released publicly directly shapes their societal impact. In this position paper, we focus on open foundation models, defined here as those with broadly available model weights (e.g. Llama 2, Stable Diffusion XL). We identify five distinctive properties (e.g. greater customizability, poor monitoring) of open foundation models that lead to both their benefits and risks. Open foundation models present significant benefits, with some caveats, that span innovation, competition, the distribution of decision-making power, and transparency. To understand their risks of misuse, we design a risk assessment framework for analyzing their marginal risk. Across several misuse vectors (e.g. cyberattacks, bioweapons), we find that current research is insufficient to effectively characterize the marginal risk of open foundation models relative to pre-existing technologies. The framework helps ex
    
[^141]: 用于自主公交网络设计的神经进化算法

    A Neural-Evolutionary Algorithm for Autonomous Transit Network Design

    [https://arxiv.org/abs/2403.07917](https://arxiv.org/abs/2403.07917)

    提出了一种神经进化算法用于自动公交网络设计，该算法通过训练图神经网络模型作为策略，并将其用作进化算法中的变异操作符，在公交网络设计基准集上优于单独学习策略和简单进化算法方法。

    

    规划公共交通网络是一个具有挑战性的优化问题，但是为了实现自动驾驶公交车的好处是至关重要的。我们提出了一种新颖的算法，用于规划自动驾驶公交车的路线网络。我们首先训练一个图神经网络模型作为构建路线网络的策略，然后将该策略用作进化算法中的多个变异操作符之一。我们在标准的公交网络设计基准集上评估这种算法，并发现它在现实基准实例上的表现比单独学习的策略高出高达20\%，比简单的进化算法方法高出高达53%。

    arXiv:2403.07917v1 Announce Type: cross  Abstract: Planning a public transit network is a challenging optimization problem, but essential in order to realize the benefits of autonomous buses. We propose a novel algorithm for planning networks of routes for autonomous buses. We first train a graph neural net model as a policy for constructing route networks, and then use the policy as one of several mutation operators in a evolutionary algorithm. We evaluate this algorithm on a standard set of benchmarks for transit network design, and find that it outperforms the learned policy alone by up to 20\% and a plain evolutionary algorithm approach by up to 53\% on realistic benchmark instances.
    
[^142]: 推进投资前沿：面向产业的深度强化学习在投资组合优化中的应用

    Advancing Investment Frontiers: Industry-grade Deep Reinforcement Learning for Portfolio Optimization

    [https://arxiv.org/abs/2403.07916](https://arxiv.org/abs/2403.07916)

    本研究将深度强化学习应用于投资组合优化中，融合了产业级方法和量化金融，提出了结合多领域方法的独特视角。

    

    本研究探讨了深度强化学习（DRL）在资产类别无关投资组合优化中的应用，将产业级方法与量化金融相结合。在这一融合的核心是我们的强大框架，不仅将先进的DRL算法与现代计算技术相结合，还强调严格的统计分析、软件工程和监管合规性。据我们所知，这是第一项将金融强化学习与机器人学和数学物理中的从模拟到真实方法相结合的研究，为我们的框架和论据带来了独特的视角。我们的研究最终介绍了AlphaOptimizerNet，一种具有专有权的强化学习代理（以及相应的库）。AlphaOptimizerNet是从最新的文献和我们独特的跨学科方法的综合中发展而来的。

    arXiv:2403.07916v1 Announce Type: new  Abstract: This research paper delves into the application of Deep Reinforcement Learning (DRL) in asset-class agnostic portfolio optimization, integrating industry-grade methodologies with quantitative finance. At the heart of this integration is our robust framework that not only merges advanced DRL algorithms with modern computational techniques but also emphasizes stringent statistical analysis, software engineering and regulatory compliance. To the best of our knowledge, this is the first study integrating financial Reinforcement Learning with sim-to-real methodologies from robotics and mathematical physics, thus enriching our frameworks and arguments with this unique perspective. Our research culminates with the introduction of AlphaOptimizerNet, a proprietary Reinforcement Learning agent (and corresponding library). Developed from a synthesis of state-of-the-art (SOTA) literature and our unique interdisciplinary methodology, AlphaOptimizerNe
    
[^143]: 用深度学习和强化学习技术增强Kubernetes自动调度，用于大规模云计算优化

    Enhancing Kubernetes Automated Scheduling with Deep Learning and Reinforcement Techniques for Large-Scale Cloud Computing Optimization

    [https://arxiv.org/abs/2403.07905](https://arxiv.org/abs/2403.07905)

    本文提出了一种基于深度学习和强化学习的自动任务调度方案，旨在实现大规模云计算系统中任务调度的最优利用和最大执行效率。

    

    随着云计算应用规模的持续扩大，深度学习和强化学习等人工智能技术逐渐成为解决大规模云计算系统自动任务调度的关键工具。针对大规模云计算系统中任务调度的复杂性和实时性需求，本文提出了一种基于深度学习和强化学习的自动任务调度方案。首先，使用深度学习技术实时监测和预测云计算系统中的参数，以获取系统状态信息。然后，结合强化学习算法，根据实时系统状态和任务特征动态调整任务调度策略，实现系统资源的最佳利用和任务执行效率的最大化。

    arXiv:2403.07905v1 Announce Type: cross  Abstract: With the continuous expansion of the scale of cloud computing applications, artificial intelligence technologies such as Deep Learning and Reinforcement Learning have gradually become the key tools to solve the automated task scheduling of large-scale cloud computing systems. Aiming at the complexity and real-time requirement of task scheduling in large-scale cloud computing system, this paper proposes an automatic task scheduling scheme based on deep learning and reinforcement learning. Firstly, the deep learning technology is used to monitor and predict the parameters in the cloud computing system in real time to obtain the system status information. Then, combined with reinforcement learning algorithm, the task scheduling strategy is dynamically adjusted according to the real-time system state and task characteristics to achieve the optimal utilization of system resources and the maximum of task execution efficiency. This paper veri
    
[^144]: 正视监管空白：通过纳入社会公民打造超越AIA的欧盟AI审计生态系统

    Addressing the Regulatory Gap: Moving Towards an EU AI Audit Ecosystem Beyond the AIA by Including Civil Society

    [https://arxiv.org/abs/2403.07904](https://arxiv.org/abs/2403.07904)

    提出了一个融合合规和监督的AI审计生态系统，强调了DSA和AIA监管框架中存在的监管空白，并要求AIA为研究人员和社会公民提供数据和模型访问权限

    

    欧洲立法机构提出了数字服务法案（DSA）和人工智能法案（AIA）来规范平台和人工智能（AI）产品。本文审查了第三方审计在这两项法律中的地位以及在多大程度上提供模型和数据的访问权限。通过考虑审计生态系统中第三方审计和第三方数据访问的价值，我们发现了一个监管空白，即《人工智能法案》没有为研究人员和社会公民提供数据访问权限。我们对文献的贡献包括：（1）定义了一个融合合规和监督的AI审计生态系统。（2）强调了DSA和AIA监管框架中存在的监管空白，阻碍了AI审计生态系统的建立。（3）强调研究和社会公民的第三方审计必须成为该生态系统的一部分，并要求AIA包括数据和模型访问权限。

    arXiv:2403.07904v1 Announce Type: cross  Abstract: The European legislature has proposed the Digital Services Act (DSA) and Artificial Intelligence Act (AIA) to regulate platforms and Artificial Intelligence (AI) products. We review to what extent third-party audits are part of both laws and to what extent access to models and data is provided. By considering the value of third-party audits and third-party data access in an audit ecosystem, we identify a regulatory gap in that the Artificial Intelligence Act does not provide access to data for researchers and civil society. Our contributions to the literature include: (1) Defining an AI audit ecosystem that incorporates compliance and oversight. (2) Highlighting a regulatory gap within the DSA and AIA regulatory framework, preventing the establishment of an AI audit ecosystem. (3) Emphasizing that third-party audits by research and civil society must be part of that ecosystem and demand that the AIA include data and model access for ce
    
[^145]: 分布式计算与边缘智能时代的多址接入

    Multiple Access in the Era of Distributed Computing and Edge Intelligence

    [https://arxiv.org/abs/2403.07903](https://arxiv.org/abs/2403.07903)

    本文关注多址接入技术下一代（NGMA）的最新研究和创新，以及其与边缘计算、网络切片、空中计算、语义通信和机器学习等关键技术的相互关系。

    

    本文侧重于基础下一代多址接入（NGMA）技术的最新研究和创新，以及与第六代（6G）无线网络的其他关键技术的共存。具体而言，我们首先考察了多址边缘计算（MEC），这对满足网络边缘的数据处理和计算需求增长至关重要，以及网络切片。然后我们探讨了空中计算（OTA），被认为是提供各种功能快速高效计算的方法。我们还探讨了语义通信，被认为是通过关注有意义信息的交换来提高通信系统的有效方式，从而减少不必要的数据并提高效率。此外，本文还回顾了机器学习（ML）和多址接入技术之间的相互关系，重点放在了联合学习

    arXiv:2403.07903v1 Announce Type: cross  Abstract: This paper focuses on the latest research and innovations in fundamental next-generation multiple access (NGMA) techniques and the coexistence with other key technologies for the sixth generation (6G) of wireless networks. In more detail, we first examine multi-access edge computing (MEC), which is critical to meeting the growing demand for data processing and computational capacity at the edge of the network, as well as network slicing. We then explore over-the-air (OTA) computing, which is considered to be an approach that provides fast and efficient computation of various functions. We also explore semantic communications, identified as an effective way to improve communication systems by focusing on the exchange of meaningful information, thus minimizing unnecessary data and increasing efficiency. The interrelationship between machine learning (ML) and multiple access technologies is also reviewed, with an emphasis on federated lea
    
[^146]: DecompDiff：具有分解先验的扩散模型用于基于结构的药物设计

    DecompDiff: Diffusion Models with Decomposed Priors for Structure-Based Drug Design

    [https://arxiv.org/abs/2403.07902](https://arxiv.org/abs/2403.07902)

    本文提出了DecompDiff模型，通过将配体分子分解为臂和支架，并引入分解先验，结合键扩散和有效性指导，实现了生成高亲和力分子并保持分子性质的最先进性能。

    

    arXiv:2403.07902v1 交叉类型：设计3D配体以适应靶结合位点是药物发现中的基本任务。现有的基于结构的药物设计方法将所有配体原子视为平等处理，忽略了配体中原子在药物设计中的不同作用，并且在探索大型类似药物分子空间时可能效率较低。受制药惯例启发，我们将配体分子分解为两个部分，即臂和支架，并提出了一种新的扩散模型，即DecompDiff，其具有对臂和支架进行分解的先验。为了促进分解生成并改进生成的分子的性质，我们在模型中结合了键扩散，并在采样阶段加入了额外的有效性指导。对CrossDocked2020上的广泛实验表明，我们的方法在生成高亲和力分子的同时保持适当的分子性质方面实现了最先进的性能。

    arXiv:2403.07902v1 Announce Type: cross  Abstract: Designing 3D ligands within a target binding site is a fundamental task in drug discovery. Existing structured-based drug design methods treat all ligand atoms equally, which ignores different roles of atoms in the ligand for drug design and can be less efficient for exploring the large drug-like molecule space. In this paper, inspired by the convention in pharmaceutical practice, we decompose the ligand molecule into two parts, namely arms and scaffold, and propose a new diffusion model, DecompDiff, with decomposed priors over arms and scaffold. In order to facilitate the decomposed generation and improve the properties of the generated molecules, we incorporate both bond diffusion in the model and additional validity guidance in the sampling phase. Extensive experiments on CrossDocked2020 show that our approach achieves state-of-the-art performance in generating high-affinity molecules while maintaining proper molecular properties an
    
[^147]: MIP: 从PEFT梯度中基于CLIP进行图像重构

    MIP: CLIP-based Image Reconstruction from PEFT Gradients

    [https://arxiv.org/abs/2403.07901](https://arxiv.org/abs/2403.07901)

    本文分析了基于CLIP的联邦学习中PEFT的梯度仍可用于进行图像重构攻击，并提出了针对CLIP的重构攻击方法MIP。

    

    CLIP模型作为一种有效的预训练多模态神经网络，在分布式机器学习任务中被广泛使用，尤其是联邦学习。通常，基于CLIP的联邦学习采用参数高效微调（PEFT）进行模型训练，只微调适配器参数或软提示，而不是完整的参数。尽管PEFT与传统训练模式不同，但在本文中，我们从理论上分析了适配器或软提示的梯度仍然可以用于执行图像重构攻击。基于我们的理论分析，我们提出了Multm-In-Parvo（MIP），一种针对基于CLIP的分布式机器学习架构的专有重构攻击方法。具体而言，MIP可以根据软提示或适配器的梯度重构CLIP训练图像。此外，MIP包括一个标签预测策略来加速攻击过程。

    arXiv:2403.07901v1 Announce Type: cross  Abstract: Contrastive Language-Image Pre-training (CLIP) model, as an effective pre-trained multimodal neural network, has been widely used in distributed machine learning tasks, especially Federated Learning (FL). Typically, CLIP-based FL adopts Parameter-Efficient Fine-Tuning (PEFT) for model training, which only fine-tunes adapter parameters or soft prompts rather than the full parameters. Although PEFT is different from the traditional training mode, in this paper, we theoretically analyze that the gradients of adapters or soft prompts can still be used to perform image reconstruction attacks. Based on our theoretical analysis, we propose Multm-In-Parvo (MIP), a proprietary reconstruction attack method targeting CLIP-based distributed machine learning architecture. Specifically, MIP can reconstruct CLIP training images according to the gradients of soft prompts or an adapter. In addition, MIP includes a label prediction strategy to accelerat
    
[^148]: 基于Copula熵的双样本检验的变点检测

    Change Point Detection with Copula Entropy based Two-Sample Test

    [https://arxiv.org/abs/2403.07892](https://arxiv.org/abs/2403.07892)

    本文提出了一种基于Copula熵的双样本检验的非参数多元方法，用于多个变点检测，并结合了二叉分割策略，有效验证了其在不同数据集上的比较优势。

    

    变点检测是一种典型任务，旨在发现时间序列中的变化，并可以用双样本检验来解决。Copula熵是一个用于测量统计独立性的数学概念，最近引入了基于它的双样本检验。本文提出了一种基于Copula熵的双样本检验的非参数多元方法，用于多个变点检测。首先，将单一变点检测提出为时间序列数据每个点的一组双样本检验，并将变点视为具有最大检验统计量的点。然后，通过将单一变点检测方法与二叉分割策略相结合，提出了多变点检测。我们验证了我们方法的有效性，并将其与其他类似方法在模拟单变量和多变量数据以及Nile数据上进行了比较。

    arXiv:2403.07892v1 Announce Type: cross  Abstract: Change point detection is a typical task that aim to find changes in time series and can be tackled with two-sample test. Copula Entropy is a mathematical concept for measuring statistical independence and a two-sample test based on it was introduced recently. In this paper we propose a nonparametric multivariate method for multiple change point detection with the copula entropy-based two-sample test. The single change point detection is first proposed as a group of two-sample tests on every points of time series data and the change point is considered as with the maximum of the test statistics. The multiple change point detection is then proposed by combining the single change point detection method with binary segmentation strategy. We verified the effectiveness of our method and compared it with the other similar methods on the simulated univariate and multivariate data and the Nile data.
    
[^149]: 基于压缩算法的数字视频篡改检测技术

    Digital Video Manipulation Detection Technique Based on Compression Algorithms

    [https://arxiv.org/abs/2403.07891](https://arxiv.org/abs/2403.07891)

    该论文提出了一种基于H.264编码压缩算法的数字视频篡改检测技术，通过分析宏块信息和运动矢量，利用矢量支持机构建立模型，成功实现对视频重新压缩的准确检测。

    

    数字图像和视频在日常生活中起着非常重要的作用。如今，人们可以使用配备先进集成摄像头和强大图像处理应用程序的可负担移动设备。技术的发展不仅促进了多媒体内容的生成，也促进了有意对其进行修改，无论是出于娱乐目的还是恶意目的。在这种情况下，必不可少的是用于检测图像和视频篡改的取证技术。本文提出了一种通过分析H.264编码使用的压缩算法的取证技术。重新压缩的存在使用了宏块信息，这是H.264-MPEG4标准的特点，以及运动矢量。使用矢量支持机构建立模型，可以准确检测视频是否已经重新压缩。

    arXiv:2403.07891v1 Announce Type: cross  Abstract: Digital images and videos play a very important role in everyday life. Nowadays, people have access the affordable mobile devices equipped with advanced integrated cameras and powerful image processing applications. Technological development facilitates not only the generation of multimedia content, but also the intentional modification of it, either with recreational or malicious purposes. This is where forensic techniques to detect manipulation of images and videos become essential. This paper proposes a forensic technique by analysing compression algorithms used by the H.264 coding. The presence of recompression uses information of macroblocks, a characteristic of the H.264-MPEG4 standard, and motion vectors. A Vector Support Machine is used to create the model that allows to accurately detect if a video has been recompressed.
    
[^150]: $\widetilde{O}(T^{-1})$ 收敛到（粗糙）相关均衡在全信息一般和马尔可夫博弈中的问题

    $\widetilde{O}(T^{-1})$ Convergence to (Coarse) Correlated Equilibria in Full-Information General-Sum Markov Games

    [https://arxiv.org/abs/2403.07890](https://arxiv.org/abs/2403.07890)

    本研究通过使用乐观的前瞻性领导者算法（OFTRL）和适当的数值更新程序，在全信息一般和马尔可夫博弈中找到了$\widetilde{O}(T^{-1})$-approximate（粗糙）相关均衡，这在$T$次迭代内得以实现。

    

    No-regret学习与博弈论密切相关，最近的研究提出了非耦合的无悔学习动态，当所有玩家在正则形式游戏中采用时，以$\widetilde{O}(T^{-1})$的接近最优速率收敛到各种均衡解，这显着改进了经典无悔学习者的$O(1/\sqrt{T})$速率。然而，在马尔可夫博弈中类似的收敛结果很少见，这是一个更通用的设置，为多智能体强化学习奠定了基础。在这项工作中，我们通过展示乐观的前瞻性领导者算法（OFTRL），连同适当的数值更新程序，可以在$T$次迭代内找到全信息一般和马尔可夫博弈中的$\widetilde{O}(T^{-1})$近似（粗糙）相关均衡。数值结果也包括以证实我们的理论发现。

    arXiv:2403.07890v1 Announce Type: cross  Abstract: No-regret learning has a long history of being closely connected to game theory. Recent works have devised uncoupled no-regret learning dynamics that, when adopted by all the players in normal-form games, converge to various equilibrium solutions at a near-optimal rate of $\widetilde{O}(T^{-1})$, a significant improvement over the $O(1/\sqrt{T})$ rate of classic no-regret learners. However, analogous convergence results are scarce in Markov games, a more generic setting that lays the foundation for multi-agent reinforcement learning. In this work, we close this gap by showing that the optimistic-follow-the-regularized-leader (OFTRL) algorithm, together with appropriate value update procedures, can find $\widetilde{O}(T^{-1})$-approximate (coarse) correlated equilibria in full-information general-sum Markov games within $T$ iterations. Numerical results are also included to corroborate our theoretical findings.
    
[^151]: 为计算病理学系统配备工件处理流水线：计算与性能权衡的展示

    Equipping Computational Pathology Systems with Artifact Processing Pipelines: A Showcase for Computation and Performance Trade-offs

    [https://arxiv.org/abs/2403.07743](https://arxiv.org/abs/2403.07743)

    提出了一种专家混合方案，用于在计算病理学系统中检测和排除五种显著的工件，并应用概率阈值处理。

    

    组织病理学是癌症诊断的黄金标准，在显微镜下进行检查。然而，组织病理学处理过程会产生一些工件，最终会转移到玻璃载玻片的数字化版本，即全玻幻灯片。工件是诊断无关的区域，可能导致错误的深度学习算法预测。因此，在计算病理学（CPATH）系统中检测和排除工件对于可靠的自动诊断至关重要。在本文中，我们提出了一种专家混合（MoE）方案，用于检测包括损坏组织、模糊、褶皱组织、气泡和在WSIs中的组织学无关血液等五种显著工件。首先，我们训练独立的二元DL模型作为专家来捕捉特定的工件形态。然后，我们使用融合机制来集成它们的预测。我们对最终的概率进行概率阈值处理

    arXiv:2403.07743v1 Announce Type: cross  Abstract: Histopathology is a gold standard for cancer diagnosis under a microscopic examination. However, histological tissue processing procedures result in artifacts, which are ultimately transferred to the digitized version of glass slides, known as whole slide images (WSIs). Artifacts are diagnostically irrelevant areas and may result in wrong deep learning (DL) algorithms predictions. Therefore, detecting and excluding artifacts in the computational pathology (CPATH) system is essential for reliable automated diagnosis. In this paper, we propose a mixture of experts (MoE) scheme for detecting five notable artifacts, including damaged tissue, blur, folded tissue, air bubbles, and histologically irrelevant blood from WSIs. First, we train independent binary DL models as experts to capture particular artifact morphology. Then, we ensemble their predictions using a fusion mechanism. We apply probabilistic thresholding over the final probabilit
    
[^152]: 互动指令跟随代理的在线持续学习

    Online Continual Learning For Interactive Instruction Following Agents

    [https://arxiv.org/abs/2403.07548](https://arxiv.org/abs/2403.07548)

    我们提出了针对具身代理的两种持续学习设置：学习新行为和新环境。同时，我们通过自信度得分来更新存储的信息，从而避免需要任务边界信息的问题。

    

    在通过语言指令执行日常任务的具身代理学习过程中，文献大都假定代理在开始时就学习所有训练数据。我们认为这样的学习场景较不现实，因为机器人代理应该在探索和感知世界的过程中不断地学习。为了朝着更真实的具身代理学习场景迈进一步，我们提出了两种持续学习设置供具身代理使用；学习新行为（行为增量学习，Behavior-IL）和新环境（环境增量学习，Environment-IL）。在任务中，先前基于“数据先验”的持续学习方法维护过去任务的logits。然而，存储的信息往往是不充分学习的信息，需要任务边界信息，而这种信息并不总是可用。在这里，我们提议基于自信度得分而无需任务边界信息来更新它们。

    arXiv:2403.07548v1 Announce Type: new  Abstract: In learning an embodied agent executing daily tasks via language directives, the literature largely assumes that the agent learns all training data at the beginning. We argue that such a learning scenario is less realistic since a robotic agent is supposed to learn the world continuously as it explores and perceives it. To take a step towards a more realistic embodied agent learning scenario, we propose two continual learning setups for embodied agents; learning new behaviors (Behavior Incremental Learning, Behavior-IL) and new environments (Environment Incremental Learning, Environment-IL) For the tasks, previous 'data prior' based continual learning methods maintain logits for the past tasks. However, the stored information is often insufficiently learned information and requires task boundary information, which might not always be available. Here, we propose to update them based on confidence scores without task boundary information d
    
[^153]: 具有高效部分重新训练的图去除方法

    Graph Unlearning with Efficient Partial Retraining

    [https://arxiv.org/abs/2403.07353](https://arxiv.org/abs/2403.07353)

    提出了一种新颖的图去除框架GraphRevoker，通过图属性感知划分和图对比子模型聚合，更好地保持了不可训练GNNs的模型效用。

    

    图神经网络（GNNs）在各种现实世界应用中取得了显著成功。然而，GNNs 可能会在不良的图数据上进行训练，这可能会降低它们的性能和可靠性。为了让已经训练过的GNNs能够有效地去除不需要的数据，一种理想的解决方案是基于重新训练的图去除方法，该方法将训练图分成子图，并在其上训练子模型，从而通过部分重新训练实现快速去除。然而，图分区过程会导致训练图中的信息丢失，从而导致子GNN模型的模型效用较低。

    arXiv:2403.07353v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have achieved remarkable success in various real-world applications. However, GNNs may be trained on undesirable graph data, which can degrade their performance and reliability. To enable trained GNNs to efficiently unlearn unwanted data, a desirable solution is retraining-based graph unlearning, which partitions the training graph into subgraphs and trains sub-models on them, allowing fast unlearning through partial retraining. However, the graph partition process causes information loss in the training graph, resulting in the low model utility of sub-GNN models. In this paper, we propose GraphRevoker, a novel graph unlearning framework that better maintains the model utility of unlearnable GNNs. Specifically, we preserve the graph property with graph property-aware sharding and effectively aggregate the sub-GNN models for prediction with graph contrastive sub-model aggregation. We conduct extensive experime
    
[^154]: HiRA-Pro: 高分辨率对齐多模态时空数据的过程物理驱动方法

    HiRA-Pro: High resolution alignment of multimodal spatio-temporal data: a process physics driven approach

    [https://arxiv.org/abs/2403.06888](https://arxiv.org/abs/2403.06888)

    HiRA-Pro是一个高分辨率对齐多模态时空数据的过程物理驱动方法，成功解决了对齐具有亚毫秒现象的数据的挑战，并在智能制造环境中取得了成功应用。

    

    我们提出了HiRA-Pro，一个新颖的程序，用于在高时空分辨率下对来自展示多样瞬态、非线性随机动态的真实世界过程和系统的多模态信号进行对齐，例如制造机器。它基于识别和同步这些不同信号中显著运动学和动力学事件的过程特征。HiRA-Pro解决了对齐具有亚毫秒现象的数据的挑战，而传统的时间戳、外部触发器或基于时钟的对齐方法则难以胜任。HiRA-Pro的有效性在智能制造环境中得到了展示，在这里，它对来自Optomec-LENS MTS 500混合机器进行3D打印和铣削操作期间获取的13+通道数据进行了对齐。然后，对齐数据被体素化，生成对应于所制造零件上的物理体素的0.25秒对齐数据块。HiRA-Pro的优越性通过以下方式进一步展示

    arXiv:2403.06888v1 Announce Type: cross  Abstract: We present HiRA-Pro, a novel procedure to align, at high spatio-temporal resolutions, multimodal signals from real-world processes and systems that exhibit diverse transient, nonlinear stochastic dynamics, such as manufacturing machines. It is based on discerning and synchronizing the process signatures of salient kinematic and dynamic events in these disparate signals. HiRA-Pro addresses the challenge of aligning data with sub-millisecond phenomena, where traditional timestamp, external trigger, or clock-based alignment methods fall short. The effectiveness of HiRA-Pro is demonstrated in a smart manufacturing context, where it aligns data from 13+ channels acquired during 3D-printing and milling operations on an Optomec-LENS MTS 500 hybrid machine. The aligned data is then voxelized to generate 0.25 second aligned data chunks that correspond to physical voxels on the produced part. The superiority of HiRA-Pro is further showcased thro
    
[^155]: 使用分类器构建变量辅助回归：一个实证评估

    Constructing Variables Using Classifiers as an Aid to Regression: An Empirical Assessment

    [https://arxiv.org/abs/2403.06829](https://arxiv.org/abs/2403.06829)

    提出了一种方法，利用分类器预测变量的离散值并将其作为附加变量用于丰富回归问题的初始向量，经实验证实了该方法的有效性。

    

    本文提出了一种方法，用于自动创建变量（在回归的情况下），以补充初始输入向量中所包含的信息。该方法作为预处理步骤运行，在该步骤中，要回归的变量的连续值被离散化为一组间隔，然后用于定义值阈。然后对分类器进行训练，以预测要回归的值是否小于或等于这些阈值中的每一个。然后分类器的不同输出被链接在一个附加的变量向量中，丰富了回归问题的初始向量。实现的系统因此可以被视为一种通用的预处理工具。我们通过5种类型的回归器测试了所提出的丰富方法，并在33个回归数据集中进行了评估。我们的实验证实了这种方法的有效性。

    arXiv:2403.06829v1 Announce Type: new  Abstract: This paper proposes a method for the automatic creation of variables (in the case of regression) that complement the information contained in the initial input vector. The method works as a pre-processing step in which the continuous values of the variable to be regressed are discretized into a set of intervals which are then used to define value thresholds. Then classifiers are trained to predict whether the value to be regressed is less than or equal to each of these thresholds. The different outputs of the classifiers are then concatenated in the form of an additional vector of variables that enriches the initial vector of the regression problem. The implemented system can thus be considered as a generic pre-processing tool. We tested the proposed enrichment method with 5 types of regressors and evaluated it in 33 regression datasets. Our experimental results confirm the interest of the approach.
    
[^156]: Koopman合奏用于概率时间序列预测

    Koopman Ensembles for Probabilistic Time Series Forecasting

    [https://arxiv.org/abs/2403.06757](https://arxiv.org/abs/2403.06757)

    研究提出了一种Koopman合奏方法，通过训练模型合奏产生具有高模型间方差的预测，从而改善集成模型的不确定性量化。

    

    在数据驱动模型越来越受欢迎的背景下，最近提出了许多基于机器学习的Koopman算子的实现。然而，绝大多数这类研究仅限于确定性预测，而在像气象学和气象学这样的领域，知识的不确定性是至关重要的。在这项工作中，我们研究了训练模型合奏以产生随机输出。我们通过对真实遥感图像时间序列进行实验表明，独立训练模型的集成过分自信，并且使用明确鼓励成员生成具有高模型间方差的预测的训练标准极大改善了集成的不确定性量化。

    arXiv:2403.06757v1 Announce Type: new  Abstract: In the context of an increasing popularity of data-driven models to represent dynamical systems, many machine learning-based implementations of the Koopman operator have recently been proposed. However, the vast majority of those works are limited to deterministic predictions, while the knowledge of uncertainty is critical in fields like meteorology and climatology. In this work, we investigate the training of ensembles of models to produce stochastic outputs. We show through experiments on real remote sensing image time series that ensembles of independently trained models are highly overconfident and that using a training criterion that explicitly encourages the members to produce predictions with high inter-model variances greatly improves the uncertainty quantification of the ensembles.
    
[^157]: ALL0CORE张量分解用于稀疏计数数据

    The ALL0CORE Tensor Decomposition for Sparse Count Data

    [https://arxiv.org/abs/2403.06153](https://arxiv.org/abs/2403.06153)

    ALL0CORE是一种新的概率非负张量分解方法，它在保持计算可处理性的基础上利用Tucker分解的潜在结构，可以仅使用核的微小部分即达到与完整Tucker分解相同效果。

    

    本文介绍了ALL0CORE，一种新的概率非负张量分解形式。ALL0CORE是一种Tucker分解，其中核张量的非零元素数量（即L0范数）被限制为远小于核的大小的预设值Q。虽然用户规定了总预算Q，但非零元素的位置和值是潜在变量，在推断过程中分配给核张量的各个部分。ALL0CORE，即分配的L0约束核，因此既具有CP分解的计算可处理性，又具有Tucker的潜在结构，令人满意。在一系列真实数据实验中，我们展示了ALL0CORE通常只需使用核的微小部分（例如～1%）即可以与完整Tucker分解相同的结果，而成本仅相应的一小部分。

    arXiv:2403.06153v1 Announce Type: cross  Abstract: This paper introduces ALL0CORE, a new form of probabilistic non-negative tensor decomposition. ALL0CORE is a Tucker decomposition where the number of non-zero elements (i.e., the L0-norm) of the core tensor is constrained to a preset value Q much smaller than the size of the core. While the user dictates the total budget Q, the locations and values of the non-zero elements are latent variables and allocated across the core tensor during inference. ALL0CORE -- i.e., allocated L0-constrained core -- thus enjoys both the computational tractability of CP decomposition and the qualitatively appealing latent structure of Tucker. In a suite of real-data experiments, we demonstrate that ALL0CORE typically requires only tiny fractions (e.g.,~1%) of the full core to achieve the same results as full Tucker decomposition at only a correspondingly tiny fraction of the cost.
    
[^158]: 为基于学习方法的组合问题构建通用表示

    Towards a Generic Representation of Cominatorial Problems for Learning-Based Approaches

    [https://arxiv.org/abs/2403.06026](https://arxiv.org/abs/2403.06026)

    本文倡导为基于学习方法的组合问题构建通用表示，以解决特定表示无法跨越不同组合问题的问题。

    

    近年来，越来越多的研究人员对使用基于学习方法解决组合问题产生了兴趣，无论是以端到端的方式还是与传统优化算法结合使用。在这两种情景下，挑战在于将目标组合问题编码成适用于学习算法的结构。许多现有作品提出了特定于问题的表示，通常以图的形式，以利用图神经网络的优势。然而，这些方法缺乏泛化性，因为表示不能轻易从一个组合问题转移到另一个组合问题。虽然已经有一些尝试去填补这一差距，但它们仍然只提供了部分泛化性。鉴于这一挑战，本文倡导为基于学习方法的组合问题朝着完全通用的表示方式迈进。我们提出的方法包括

    arXiv:2403.06026v1 Announce Type: cross  Abstract: In recent years, there has been a growing interest in using learning-based approaches for solving combinatorial problems, either in an end-to-end manner or in conjunction with traditional optimization algorithms. In both scenarios, the challenge lies in encoding the targeted combinatorial problems into a structure compatible with the learning algorithm. Many existing works have proposed problem-specific representations, often in the form of a graph, to leverage the advantages of \textit{graph neural networks}. However, these approaches lack generality, as the representation cannot be easily transferred from one combinatorial problem to another one. While some attempts have been made to bridge this gap, they still offer a partial generality only. In response to this challenge, this paper advocates for progress toward a fully generic representation of combinatorial problems for learning-based approaches. The approach we propose involves 
    
[^159]: 复杂航天器任务的屏蔽深度强化学习

    Shielded Deep Reinforcement Learning for Complex Spacecraft Tasking

    [https://arxiv.org/abs/2403.05693](https://arxiv.org/abs/2403.05693)

    本文基于线性时态逻辑（LTL）形式化了航天器任务和安全要求，并提出了自动构建奖励函数以实现有效训练的方法。同时探讨了从安全LTL规范构建航天器屏蔽的方法，提出了三种可以提供概率保证的设计，并通过实验展示了这些屏蔽与不同策略的互动和奖励结构的灵活性。

    

    自主航天器控制通过屏蔽深度强化学习（SDRL）已成为快速增长的研究领域。然而，目前对屏蔽的构建和任务的定义仍不够正式，导致策略无法保证安全并给RL代理设定了模棱两可的目标。本文首先探讨了使用形式语言，即线性时态逻辑（LTL），来形式化航天器任务和安全要求。然后定义了一种自动从co-safe LTL规范构建奖励函数以有效训练SDRL框架的方式。我们还研究了为航天器应用从安全LTL规范构建屏蔽的方法，并提出了三种可以提供概率保证的设计。通过几个实验展示了这些屏蔽与不同策略的互动以及奖励结构的灵活性。

    arXiv:2403.05693v1 Announce Type: new  Abstract: Autonomous spacecraft control via Shielded Deep Reinforcement Learning (SDRL) has become a rapidly growing research area. However, the construction of shields and the definition of tasking remains informal, resulting in policies with no guarantees on safety and ambiguous goals for the RL agent. In this paper, we first explore the use of formal languages, namely Linear Temporal Logic (LTL), to formalize spacecraft tasks and safety requirements. We then define a manner in which to construct a reward function from a co-safe LTL specification automatically for effective training in SDRL framework. We also investigate methods for constructing a shield from a safe LTL specification for spacecraft applications and propose three designs that provide probabilistic guarantees. We show how these shields interact with different policies and the flexibility of the reward structure through several experiments.
    
[^160]: 学习高斯单指数模型的计算复杂性

    The Computational Complexity of Learning Gaussian Single-Index Models

    [https://arxiv.org/abs/2403.05529](https://arxiv.org/abs/2403.05529)

    该论文研究了学习高斯单指数模型的计算复杂性，在高维回归问题中展示了计算有效算法所需的样本复杂度，并表明这种复杂度是充分的。

    

    单指数模型是具有植入结构的高维回归问题，其中标签依赖于通过通用、非线性和潜在非确定性转换的输入的未知一维投影。因此，它们涵盖了广泛的统计推断任务类别，并提供了一个丰富的模板，用于研究高维情况下的统计和计算折衷。尽管恢复隐藏方向的信息论样本复杂度与维度$d$是线性的，但我们表明，在统计查询（SQ）框架和低阶多项式（LDP）框架内，计算高效的算法必须需要$\Omega(d^{k^\star/2})$个样本，其中$k^\star$是我们明确表征的与模型相关的“生成”指数。此外，我们通过建立使用部分迹的匹配上界来证明这个样本复杂度也是充分的。

    arXiv:2403.05529v1 Announce Type: new  Abstract: Single-Index Models are high-dimensional regression problems with planted structure, whereby labels depend on an unknown one-dimensional projection of the input via a generic, non-linear, and potentially non-deterministic transformation. As such, they encompass a broad class of statistical inference tasks, and provide a rich template to study statistical and computational trade-offs in the high-dimensional regime.   While the information-theoretic sample complexity to recover the hidden direction is linear in the dimension $d$, we show that computationally efficient algorithms, both within the Statistical Query (SQ) and the Low-Degree Polynomial (LDP) framework, necessarily require $\Omega(d^{k^\star/2})$ samples, where $k^\star$ is a "generative" exponent associated with the model that we explicitly characterize. Moreover, we show that this sample complexity is also sufficient, by establishing matching upper bounds using a partial-trace
    
[^161]: 用于射电天文学中快速精密成像的R2D2深度神经网络系列范式

    The R2D2 deep neural network series paradigm for fast precision imaging in radio astronomy

    [https://arxiv.org/abs/2403.05452](https://arxiv.org/abs/2403.05452)

    提出一种新颖的深度学习方法R2D2，用于解决射电天文学中高分辨率高动态范围成像的可扩展性挑战。

    

    射电干涉成像需要解决来自大数据量的高分辨率高动态范围逆问题。最近基于优化理论的图像重建技术展示了惊人的成像精度能力，远远超出了CLEAN的能力。这些方法包括由手工设计的正则化算子推动的先进近端算法，如SARA系列，以及由学习正则化去噪器推动的混合插拔（PnP）算法，如AIRI。然而，优化和PnP结构高度迭代，这阻碍了它们处理未来仪器预期的极端数据大小的能力。为解决这一可扩展性挑战，我们引入一种新颖的深度学习方法，命名为“用于高动态范围成像的残差对残差DNN系列”。R2D2的重建被形成为一系列残差图像，这些图像作为深度神经网络的输出，通过迭代估计而得到。

    arXiv:2403.05452v1 Announce Type: cross  Abstract: Radio-interferometric (RI) imaging entails solving high-resolution high-dynamic range inverse problems from large data volumes. Recent image reconstruction techniques grounded in optimization theory have demonstrated remarkable capability for imaging precision, well beyond CLEAN's capability. These range from advanced proximal algorithms propelled by handcrafted regularization operators, such as the SARA family, to hybrid plug-and-play (PnP) algorithms propelled by learned regularization denoisers, such as AIRI. Optimization and PnP structures are however highly iterative, which hinders their ability to handle the extreme data sizes expected from future instruments. To address this scalability challenge, we introduce a novel deep learning approach, dubbed ``Residual-to-Residual DNN series for high-Dynamic range imaging'. R2D2's reconstruction is formed as a series of residual images, iteratively estimated as outputs of Deep Neural Netw
    
[^162]: 为报告生成调优心电图指导

    Electrocardiogram Instruction Tuning for Report Generation

    [https://arxiv.org/abs/2403.04945](https://arxiv.org/abs/2403.04945)

    提出了Multimodal ECG Instruction Tuning（MEIT）框架，首次尝试使用LLMs和多模态指导解决ECG报告生成问题，并在两个大规模ECG数据集上进行了广泛的实验评估其优越性。

    

    心电图（ECG）作为心脏病情监测的主要非侵入性诊断工具，对于协助临床医生至关重要。最近的研究集中在使用ECG数据对心脏病情进行分类，但忽略了ECG报告生成，这不仅耗时，而且需要临床专业知识。为了自动化ECG报告生成并确保其多功能性，我们提出了Multimodal ECG Instruction Tuning（MEIT）框架，这是\textit{首次}尝试使用LLMs和多模态指导来解决ECG报告生成问题。为了促进未来的研究，我们建立了一个基准来评估MEIT在两个大规模ECG数据集上使用各种LLM骨干的表现。我们的方法独特地对齐了ECG信号和报告的表示，并进行了大量实验来评估MEIT与九个开源LLMs，使用了超过80万个ECG报告。MEIT的结果凸显了其优越性。

    arXiv:2403.04945v1 Announce Type: new  Abstract: Electrocardiogram (ECG) serves as the primary non-invasive diagnostic tool for cardiac conditions monitoring, are crucial in assisting clinicians. Recent studies have concentrated on classifying cardiac conditions using ECG data but have overlooked ECG report generation, which is not only time-consuming but also requires clinical expertise. To automate ECG report generation and ensure its versatility, we propose the Multimodal ECG Instruction Tuning (MEIT) framework, the \textit{first} attempt to tackle ECG report generation with LLMs and multimodal instructions. To facilitate future research, we establish a benchmark to evaluate MEIT with various LLMs backbones across two large-scale ECG datasets. Our approach uniquely aligns the representations of the ECG signal and the report, and we conduct extensive experiments to benchmark MEIT with nine open source LLMs, using more than 800,000 ECG reports. MEIT's results underscore the superior p
    
[^163]: 大乐透假设综述

    A Survey of Lottery Ticket Hypothesis

    [https://arxiv.org/abs/2403.04861](https://arxiv.org/abs/2403.04861)

    大乐透假设指出神经网络中存在稀疏子网络，训练孤立子网络可以获得更好性能，调查综述了LTH现状并提出未来研究方向

    

    大乐透假设(LTH)指出，稠密神经网络模型包含一个高度稀疏的子网络（即，中奖票），当以孤立方式训练时，可以实现比原始模型更好的性能。尽管LTH在许多研究中已经在经验和理论上得到证实，但仍然存在一些需要解决的开放问题，例如效率和可扩展性。此外，缺乏开源框架和一致的实验设置对LTH未来研究提出了挑战。我们首次从不同角度审视以往关于LTH的研究和研究。我们还讨论了现有研究中的问题，并列出了进一步探索的潜在方向。这项调查旨在深入了解LTH的现状，并开发一个得到妥善维护的平台，用于进行实验并与最新基线进行比较。

    arXiv:2403.04861v1 Announce Type: new  Abstract: The Lottery Ticket Hypothesis (LTH) states that a dense neural network model contains a highly sparse subnetwork (i.e., winning tickets) that can achieve even better performance than the original model when trained in isolation. While LTH has been proved both empirically and theoretically in many works, there still are some open issues, such as efficiency and scalability, to be addressed. Also, the lack of open-source frameworks and consensual experimental setting poses a challenge to future research on LTH. We, for the first time, examine previous research and studies on LTH from different perspectives. We also discuss issues in existing works and list potential directions for further exploration. This survey aims to provide an in-depth look at the state of LTH and develop a duly maintained platform to conduct experiments and compare with the most updated baselines.
    
[^164]: 面向音频-视频人员验证的动态交叉注意力

    Dynamic Cross Attention for Audio-Visual Person Verification

    [https://arxiv.org/abs/2403.04661](https://arxiv.org/abs/2403.04661)

    提出了一种动态交叉注意力（DCA）模型，根据音频和视频模态之间的强弱互补关系，动态选择交叉关注或不关注的特征。

    

    尽管人员或身份验证通常使用个体模态（如面部和声音）进行探索，但最近显示出巨大潜力的音视频融合方法可以胜过单模态方法。音频和视觉模态通常被期望具有强烈的互补关系，在有效的音视频融合中起着至关重要的作用。然而，它们并不总是强烈相互补充，它们也可能展现出弱的互补关系，导致音视频特征表示不佳。本文提出了一种动态交叉注意力（DCA）模型，可以根据跨音频和视觉模态之间的强弱互补关系，动态选择交叉关注或不关注的特征。特别地，设计了一个条件门控层来评估交叉注意力机制的贡献，并仅选择跨模态关注的特征。

    arXiv:2403.04661v1 Announce Type: cross  Abstract: Although person or identity verification has been predominantly explored using individual modalities such as face and voice, audio-visual fusion has recently shown immense potential to outperform unimodal approaches. Audio and visual modalities are often expected to pose strong complementary relationships, which plays a crucial role in effective audio-visual fusion. However, they may not always strongly complement each other, they may also exhibit weak complementary relationships, resulting in poor audio-visual feature representations. In this paper, we propose a Dynamic Cross-Attention (DCA) model that can dynamically select the cross-attended or unattended features on the fly based on the strong or weak complementary relationships, respectively, across audio and visual modalities. In particular, a conditional gating layer is designed to evaluate the contribution of the cross-attention mechanism and choose cross-attended features only
    
[^165]: SWAP-NAS: 适用于超快速NAS的样本级激活模式

    SWAP-NAS: Sample-Wise Activation Patterns For Ultra-Fast NAS

    [https://arxiv.org/abs/2403.04161](https://arxiv.org/abs/2403.04161)

    提出了一种新颖的高性能无需训练的度量SWAP-Score，能够在不同搜索空间和任务中测量网络在一批输入样本上的表现能力，并通过正则化进一步提高相关性，实现模型大小的控制。

    

    无需训练的度量（即零成本代理）被广泛用于避免资源密集型的神经网络训练，尤其是在神经结构搜索（NAS）中。最近的研究表明，现有的无需训练的度量存在一些局限，比如在不同搜索空间和任务之间存在有限的关联性和差劲的泛化能力。因此，我们提出了样本级激活模式及其衍生物SWAP-Score，这是一种新颖的高性能无需训练的度量。它测量了网络在一批输入样本上的表现能力。SWAP-Score与不同搜索空间和任务中的真实性能强相关，在NAS-Bench-101/201/301和TransNAS-Bench-101上胜过了15种现有的无需训练的度量。SWAP-Score可以通过正则化进一步增强，这在基于单元的搜索空间中可以实现更高的相关性，并且在搜索过程中实现模型大小控制。例如，Spearman的排序

    arXiv:2403.04161v1 Announce Type: new  Abstract: Training-free metrics (a.k.a. zero-cost proxies) are widely used to avoid resource-intensive neural network training, especially in Neural Architecture Search (NAS). Recent studies show that existing training-free metrics have several limitations, such as limited correlation and poor generalisation across different search spaces and tasks. Hence, we propose Sample-Wise Activation Patterns and its derivative, SWAP-Score, a novel high-performance training-free metric. It measures the expressivity of networks over a batch of input samples. The SWAP-Score is strongly correlated with ground-truth performance across various search spaces and tasks, outperforming 15 existing training-free metrics on NAS-Bench-101/201/301 and TransNAS-Bench-101. The SWAP-Score can be further enhanced by regularisation, which leads to even higher correlations in cell-based search space and enables model size control during the search. For example, Spearman's rank
    
[^166]: 自发性言语中的非言语信息 - 朝着新的分析框架

    Non-verbal information in spontaneous speech - towards a new framework of analysis

    [https://arxiv.org/abs/2403.03522](https://arxiv.org/abs/2403.03522)

    这项研究提出了一个分析框架和技术验证概念，用于对言语中的非言语信号进行分类，并将其与含义关联起来，从而为探索表达实现多层韵律事件的大型数据提供了一种方法。

    

    言语中的非言语信号是由韵律编码的，携带的信息范围从对话行为到态度和情感。尽管其重要性，掌握掌声结构的原则仍未得到充分理解。本文提出了一个分析框架和技术验证概念，用于对韵律信号进行分类，并将其与含义关联起来。该框架解释了多层韵律事件的表层表示。作为实施的第一步，我们提出了一个分类过程，可以解开三个级别的韵律现象。它依赖于微调预训练的语音识别模型，实现同时的多类别/多标签检测。它可以概括各种各样的自发数据，在与人类注释相当或优于的情况下执行。除了对韵律的标准化形式化外，解开韵律模式还可以指导

    arXiv:2403.03522v1 Announce Type: cross  Abstract: Non-verbal signals in speech are encoded by prosody and carry information that ranges from conversation action to attitude and emotion. Despite its importance, the principles that govern prosodic structure are not yet adequately understood. This paper offers an analytical schema and a technological proof-of-concept for the categorization of prosodic signals and their association with meaning. The schema interprets surface-representations of multi-layered prosodic events. As a first step towards implementation, we present a classification process that disentangles prosodic phenomena of three orders. It relies on fine-tuning a pre-trained speech recognition model, enabling the simultaneous multi-class/multi-label detection. It generalizes over a large variety of spontaneous data, performing on a par with, or superior to, human annotation. In addition to a standardized formalization of prosody, disentangling prosodic patterns can direct a
    
[^167]: 在边缘进行机器学习模型训练：一项调研

    Training Machine Learning models at the Edge: A Survey

    [https://arxiv.org/abs/2403.02619](https://arxiv.org/abs/2403.02619)

    这项调研深入探讨了边缘学习(EL)中优化机器学习模型训练的各种方法和方法论，旨在综合现有知识，识别挑战，并突出未来趋势。

    

    边缘计算(EC)近年来获得了显著关注，通过在边缘集成人工智能(AI)能力，承诺提高效率。虽然主要关注点在边缘部署和推断机器学习(ML)模型，但训练方面仍然较少被探讨。这项调研深入探讨了边缘学习(EL)，特别是在边缘优化ML模型训练的方面。其目标是全面探讨EL中的不同方法和方法论，综合现有知识，识别挑战，并突出未来趋势。利用Scopus的高级搜索，确定了关于EL的相关文献，显示了研究工作在分布式学习方法方面的聚焦，特别是联邦学习(FL)。此调研还提供了一个比较用于优化边缘学习的ML的技术的指南，以及对不同框架的探索。

    arXiv:2403.02619v1 Announce Type: new  Abstract: Edge Computing (EC) has gained significant traction in recent years, promising enhanced efficiency by integrating Artificial Intelligence (AI) capabilities at the edge. While the focus has primarily been on the deployment and inference of Machine Learning (ML) models at the edge, the training aspect remains less explored. This survey delves into Edge Learning (EL), specifically the optimization of ML model training at the edge. The objective is to comprehensively explore diverse approaches and methodologies in EL, synthesize existing knowledge, identify challenges, and highlight future trends. Utilizing Scopus' advanced search, relevant literature on EL was identified, revealing a concentration of research efforts in distributed learning methods, particularly Federated Learning (FL). This survey further provides a guideline for comparing techniques used to optimize ML for edge learning, along with an exploration of different frameworks, 
    
[^168]: 类不平等：关于图像识别公平性的实证研究

    Classes Are Not Equal: An Empirical Study on Image Recognition Fairness

    [https://arxiv.org/abs/2402.18133](https://arxiv.org/abs/2402.18133)

    图像分类模型中存在类准确率差异导致的不公平现象，主要是因为有问题的表示方式导致模型对更具挑战性的类别表现出更大的预测偏差。

    

    在这篇论文中，我们对图像识别公平性进行了实证研究，即在诸如ImageNet之类的平衡数据上存在极端类准确率差异。我们通过实验证明，不同类别并不相等，公平性问题在各种数据集、网络架构和模型容量上的图像分类模型中普遍存在。此外，我们还发现了公平性的几个有趣特性。首先，不公平性主要源于有问题的表示，而非分类器偏差。其次，通过提出的“模型预测偏差”概念，我们研究了优化过程中有问题表示的起源。我们的发现表明，模型倾向于对更具挑战性的类别表现出更大的预测偏差。这意味着更多其他类别将与较难识别的类别混淆。然后，假阳例（FPs）将主导优化中的学习过程，从而导致它们的准确率较低。

    arXiv:2402.18133v1 Announce Type: new  Abstract: In this paper, we present an empirical study on image recognition fairness, i.e., extreme class accuracy disparity on balanced data like ImageNet. We experimentally demonstrate that classes are not equal and the fairness issue is prevalent for image classification models across various datasets, network architectures, and model capacities. Moreover, several intriguing properties of fairness are identified. First, the unfairness lies in problematic representation rather than classifier bias. Second, with the proposed concept of Model Prediction Bias, we investigate the origins of problematic representation during optimization. Our findings reveal that models tend to exhibit greater prediction biases for classes that are more challenging to recognize. It means that more other classes will be confused with harder classes. Then the False Positives (FPs) will dominate the learning in optimization, thus leading to their poor accuracy. Further,
    
[^169]: DS-Agent：通过赋予大型语言模型案例推理能力实现自动化数据科学

    DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning

    [https://arxiv.org/abs/2402.17453](https://arxiv.org/abs/2402.17453)

    DS-Agent是一个自动框架，结合了大型语言模型代理和案例推理，能够在数据科学任务中灵活利用专家知识并通过反馈机制持续改善性能

    

    在这项工作中，我们研究了基于大型语言模型（LLMs）代理的潜力，以自动化数据科学任务，目标是理解任务要求，然后构建和训练最合适的机器学习模型。尽管现有的LLM代理取得了广泛成功，但在这种情景下生成不合理的实验计划受到阻碍。为此，我们提出了DS-Agent，这是一个利用LLM代理和案例推理（CBR）的新颖自动化框架。在开发阶段，DS-Agent遵循CBR框架来构建自动迭代流水线，可以灵活利用来自Kaggle的专业知识，并通过反馈机制促进一致的性能改进。此外，DS-Agent实现了一个低资源部署阶段，采用简化的CBR范例来适应开发阶段成功解决方案，以进行直接代码生成，显著减少了...

    arXiv:2402.17453v1 Announce Type: new  Abstract: In this work, we investigate the potential of large language models (LLMs) based agents to automate data science tasks, with the goal of comprehending task requirements, then building and training the best-fit machine learning models. Despite their widespread success, existing LLM agents are hindered by generating unreasonable experiment plans within this scenario. To this end, we present DS-Agent, a novel automatic framework that harnesses LLM agent and case-based reasoning (CBR). In the development stage, DS-Agent follows the CBR framework to structure an automatic iteration pipeline, which can flexibly capitalize on the expert knowledge from Kaggle, and facilitate consistent performance improvement through the feedback mechanism. Moreover, DS-Agent implements a low-resource deployment stage with a simplified CBR paradigm to adapt past successful solutions from the development stage for direct code generation, significantly reducing th
    
[^170]: 神经隐式扫描体模型用于快速碰撞检测

    Neural Implicit Swept Volume Models for Fast Collision Detection

    [https://arxiv.org/abs/2402.15281](https://arxiv.org/abs/2402.15281)

    提出了一种新颖的神经隐式扫描体模型，能够连续表示任意运动，并结合了深度学习速度和几何碰撞检查的准确性保证。

    

    碰撞检测是运动规划中最耗时的操作之一。因此，越来越多的人开始探索利用机器学习技术加速碰撞检测和基于采样的运动规划。最近的一系列研究侧重于利用机器人几何体或机器人运动的扫描体的神经有符号距离函数。在此基础上，我们提出了一种新颖的神经隐式扫描体模型，首次连续表示由起始和目标配置参数化的任意运动。这使得可以快速计算任务空间中任意点到机器人运动的有符号距离。此外，我们提出了一种算法，将基于深度学习的有符号距离计算的速度与几何碰撞检查的强大准确性保证相结合。我们在模拟和真实世界的机器人实验中验证了我们的方法，并证明了…

    arXiv:2402.15281v1 Announce Type: cross  Abstract: Collision detection is one of the most time-consuming operations during motion planning. Thus, there is an increasing interest in exploring machine learning techniques to speed up collision detection and sampling-based motion planning. A recent line of research focuses on utilizing neural signed distance functions of either the robot geometry or the swept volume of the robot motion. Building on this, we present a novel neural implicit swept volume model that is the first to continuously represent arbitrary motions parameterized by their start and goal configurations. This allows to quickly compute signed distances for any point in the task space to the robot motion. Further, we present an algorithm combining the speed of the deep learning-based signed distance computations with the strong accuracy guarantees of geometric collision checkers. We validate our approach in simulated and real-world robotic experiments, and demonstrate that i
    
[^171]: 分析不规则时间序列数据中的稳定神经随机微分方程

    Stable Neural Stochastic Differential Equations in Analyzing Irregular Time Series Data

    [https://arxiv.org/abs/2402.14989](https://arxiv.org/abs/2402.14989)

    神经常微分方程（Neural ODEs）的扩展——神经随机微分方程（Neural SDEs）在处理不规则时间序列数据中的稳定性和性能方面提出了重要指导，需要谨慎设计漂移和扩散函数以保持稳定性。

    

    实际时间序列数据中的不规则采样间隔和缺失值对于假设一致间隔和完整数据的传统方法构成挑战。神经常微分方程（Neural ODEs）提供了一种替代方法，利用神经网络与常微分方程求解器结合，通过参数化向量场学习连续潜在表示。神经随机微分方程（Neural SDEs）通过引入扩散项扩展了神经常微分方程，然而在处理不规则间隔和缺失值时，这种添加并不是微不足道的。因此，仔细设计漂移和扩散函数对于保持稳定性和增强性能至关重要，而粗心的选择可能导致出现没有强解、随机破坏或不稳定的Euler离散化等不利的性质，显著影响神经随机微分方程的性能。

    arXiv:2402.14989v1 Announce Type: cross  Abstract: Irregular sampling intervals and missing values in real-world time series data present challenges for conventional methods that assume consistent intervals and complete data. Neural Ordinary Differential Equations (Neural ODEs) offer an alternative approach, utilizing neural networks combined with ODE solvers to learn continuous latent representations through parameterized vector fields. Neural Stochastic Differential Equations (Neural SDEs) extend Neural ODEs by incorporating a diffusion term, although this addition is not trivial, particularly when addressing irregular intervals and missing values. Consequently, careful design of drift and diffusion functions is crucial for maintaining stability and enhancing performance, while incautious choices can result in adverse properties such as the absence of strong solutions, stochastic destabilization, or unstable Euler discretizations, significantly affecting Neural SDEs' performance. In 
    
[^172]: UR2M: 微控制器上的不确定性和资源感知事件检测

    UR2M: Uncertainty and Resource-Aware Event Detection on Microcontrollers

    [https://arxiv.org/abs/2402.09264](https://arxiv.org/abs/2402.09264)

    UR2M是一个新颖的不确定性和资源感知的事件检测框架，针对微控制器上的应用，通过评估模型输出的可靠性来解决传统机器学习技术在数据分布变化时产生不准确预测的问题。

    

    传统的机器学习技术在训练和测试阶段的数据分布发生变化时容易产生不准确的预测。这种脆弱性可能导致严重后果，特别是在移动医疗等应用中。不确定性估计有潜力通过评估模型输出的可靠性来缓解这个问题。然而，现有的不确定性估计技术通常需要大量的计算资源和内存，使它们在微控制器（MCU）上的实施变得不切实际。这个限制阻碍了许多重要的设备上可穿戴事件检测（WED）应用的可行性，如心脏病发作检测。在本文中，我们提出了UR2M，一个针对MCU的新颖的不确定性和资源感知事件检测框架。具体地，我们（i）基于证据理论开发了一种不确定性感知的WED，用于准确的事件检测。

    arXiv:2402.09264v1 Announce Type: new Abstract: Traditional machine learning techniques are prone to generating inaccurate predictions when confronted with shifts in the distribution of data between the training and testing phases. This vulnerability can lead to severe consequences, especially in applications such as mobile healthcare. Uncertainty estimation has the potential to mitigate this issue by assessing the reliability of a model's output. However, existing uncertainty estimation techniques often require substantial computational resources and memory, making them impractical for implementation on microcontrollers (MCUs). This limitation hinders the feasibility of many important on-device wearable event detection (WED) applications, such as heart attack detection.   In this paper, we present UR2M, a novel Uncertainty and Resource-aware event detection framework for MCUs. Specifically, we (i) develop an uncertainty-aware WED based on evidential theory for accurate event detection
    
[^173]: 基于深度学习的农业推荐系统：一种多变量天气预测方法

    Agricultural Recommendation System based on Deep Learning: A Multivariate Weather Forecasting Approach

    [https://arxiv.org/abs/2401.11410](https://arxiv.org/abs/2401.11410)

    提出了一种基于深度学习和天气预测的农业推荐系统，旨在解决孟加拉国农业面临的天气不利因素对粮食生产的影响，以实现盈利、可持续和农民友好的农业实践。

    

    孟加拉国主要是一个农业国家，农业部门对于加快经济增长和保障人民粮食安全起着至关重要的作用。虽然孟加拉国劳动密集型农业取得了粮食产量稳步增长，但常常受到不利天气条件的影响，如暴雨、低温和干旱。因此，这些因素严重影响了粮食生产，使得国家的粮食安全受到威胁。为了实现盈利、可持续且农民友好的农业实践，本文提出了一种基于天气预测模型的基于上下文的作物推荐系统。

    arXiv:2401.11410v2 Announce Type: replace-cross  Abstract: Bangladesh is predominantly an agricultural country, where the agrarian sector plays an essential role in accelerating economic growth and enabling the food security of the people. The performance of this sector has an overwhelming impact on the primary macroeconomic objectives like food security, employment generation, poverty alleviation, human resources development, and other economic and social forces. Although Bangladesh's labor-intensive agriculture has achieved steady increases in food grain production, it often suffered from unfavorable weather conditions such as heavy rainfall, low temperature, and drought. Consequently, these factors hinder the production of food substantially, putting the country's overall food security in danger. In order to have a profitable, sustainable, and farmer-friendly agricultural practice, this paper proposes a context-based crop recommendation system powered by a weather forecast model. Wi
    
[^174]: 学习类人表示以实现学习类人价值观

    Learning Human-like Representations to Enable Learning Human Values

    [https://arxiv.org/abs/2312.14106](https://arxiv.org/abs/2312.14106)

    通过学习类人的表示，可以实现机器学习系统符合人类价值观，支持伦理等多方面的价值对齐。

    

    如何构建与人类价值观相一致的人工智能系统，以避免造成伤害或违反社会对可接受行为的标准？我们认为，人类与人工智能代理之间的表征对齐有助于价值观的对齐。使人工智能系统学习类人类对世界的表示具有许多已知好处，包括提高泛化能力、增强对领域转移的稳健性和提高少样本学习性能。我们提出，这种机器学习（ML）模型与人类之间的表示对齐也可以支持价值对齐，使ML系统遵循人类价值观和社会规范。我们关注伦理学作为价值对齐的一个方面，并在多臂老虎机设置中使用各种方法训练ML代理，其中奖励反映所选行动的道德可接受性。我们使用一个合成实验来证明代理与环境之间的表示对齐

    arXiv:2312.14106v2 Announce Type: replace  Abstract: How can we build AI systems that are aligned with human values to avoid causing harm or violating societal standards for acceptable behavior? We argue that representational alignment between humans and AI agents facilitates value alignment. Making AI systems learn human-like representations of the world has many known benefits, including improving generalization, robustness to domain shifts, and few-shot learning performance. We propose that this kind of representational alignment between machine learning (ML) models and humans can also support value alignment, allowing ML systems to conform to human values and societal norms. We focus on ethics as one aspect of value alignment and train ML agents using a variety of methods in a multi-armed bandit setting, where rewards reflect the moral acceptability of the chosen action. We use a synthetic experiment to demonstrate that agents' representational alignment with the environment bounds
    
[^175]: SkillDiffuser: 通过技能抽象在基于扩散的任务执行中实现可解释的分层规划

    SkillDiffuser: Interpretable Hierarchical Planning via Skill Abstractions in Diffusion-Based Task Execution

    [https://arxiv.org/abs/2312.11598](https://arxiv.org/abs/2312.11598)

    SkillDiffuser通过将可解释的技能学习与条件扩散规划相结合，实现了在高层指令下生成连贯轨迹的分层规划。

    

    扩散模型展示了在机器人轨迹规划方面的强大潜力。然而，从高层指令生成连贯的轨迹仍然具有挑战性，特别是对于需要多个顺序技能的长距离组合任务。我们提出了SkillDiffuser，这是一个端到端的分层规划框架，将可解释的技能学习与条件扩散规划相结合，以解决这一问题。在较高层次，技能抽象模块从视觉观察和语言指令中学习离散的、人类可理解的技能表示。然后，这些学习到的技能嵌入被用来条件化扩散模型，生成与技能对齐的定制潜在轨迹。这允许生成符合可学习技能的多样状态轨迹。通过将技能学习与条件轨迹生成相结合，SkillDiffuser产生连贯的行为

    arXiv:2312.11598v2 Announce Type: replace-cross  Abstract: Diffusion models have demonstrated strong potential for robotic trajectory planning. However, generating coherent trajectories from high-level instructions remains challenging, especially for long-range composition tasks requiring multiple sequential skills. We propose SkillDiffuser, an end-to-end hierarchical planning framework integrating interpretable skill learning with conditional diffusion planning to address this problem. At the higher level, the skill abstraction module learns discrete, human-understandable skill representations from visual observations and language instructions. These learned skill embeddings are then used to condition the diffusion model to generate customized latent trajectories aligned with the skills. This allows generating diverse state trajectories that adhere to the learnable skills. By integrating skill learning with conditional trajectory generation, SkillDiffuser produces coherent behavior fo
    
[^176]: 模仿好的并避免坏的：安全强化学习的增量方法

    Imitate the Good and Avoid the Bad: An Incremental Approach to Safe Reinforcement Learning

    [https://arxiv.org/abs/2312.10385](https://arxiv.org/abs/2312.10385)

    提出了一种不修改基于轨迹成本约束的方法，在安全强化学习中通过模仿好的轨迹和避免坏的轨迹来改进策略。

    

    在强化学习（RL）中执行安全动作的流行框架是约束RL，其中利用基于轨迹的成本约束（或其他成本度量）来执行安全操作，更重要的是在最大化期望奖励的同时执行这些约束。最近解决约束RL的方法将基于轨迹的成本约束转换为一个替代问题，可以通过对RL方法进行轻微修改来解决。这类方法的一个主要缺点是在每个状态上对成本约束进行过度或不足估计。因此，我们提供了一种方法，不修改基于轨迹的成本约束，而是模仿“好”轨迹并避免从逐步改进的策略生成的“坏”轨迹。我们使用一个oracle，利用奖励阈值（随学习变化）和整体成本约束来将轨迹标记为“好”或“坏”。

    arXiv:2312.10385v3 Announce Type: replace-cross  Abstract: A popular framework for enforcing safe actions in Reinforcement Learning (RL) is Constrained RL, where trajectory based constraints on expected cost (or other cost measures) are employed to enforce safety and more importantly these constraints are enforced while maximizing expected reward. Most recent approaches for solving Constrained RL convert the trajectory based cost constraint into a surrogate problem that can be solved using minor modifications to RL methods. A key drawback with such approaches is an over or underestimation of the cost constraint at each state. Therefore, we provide an approach that does not modify the trajectory based cost constraint and instead imitates ``good'' trajectories and avoids ``bad'' trajectories generated from incrementally improving policies. We employ an oracle that utilizes a reward threshold (which is varied with learning) and the overall cost constraint to label trajectories as ``good''
    
[^177]: 持续不断的对抗性防御

    Continual Adversarial Defense

    [https://arxiv.org/abs/2312.09481](https://arxiv.org/abs/2312.09481)

    提出了第一个能够动态适应任何攻击的持续对抗性防御（CAD）框架。

    

    针对每月针对视觉分类器的对抗性攻击快速演变的特性，人们提出了许多防御方法，旨在尽可能通用化以抵御尽可能多的已知攻击。然而，设计一个能够对抗所有类型攻击的防御方法并不现实，因为防御系统运行的环境是动态的，包含随着时间出现的各种独特攻击。防御系统必须收集在线少样本对抗反馈以迅速增强自身，充分利用内存。因此，我们提出了第一个能够动态适应任何攻击的持续对抗性防御（CAD）框架，其中各种攻击逐个阶段出现。在实践中，CAD基于四项原则进行建模：(1) 持续适应新攻击而无灾难性遗忘，(2) 少样本适应，(3) 内存高效适应，以及(4) 高准确性

    arXiv:2312.09481v2 Announce Type: replace-cross  Abstract: In response to the rapidly evolving nature of adversarial attacks against visual classifiers on a monthly basis, numerous defenses have been proposed to generalize against as many known attacks as possible. However, designing a defense method that generalizes to all types of attacks is not realistic because the environment in which defense systems operate is dynamic and comprises various unique attacks that emerge as time goes on. The defense system must gather online few-shot defense feedback to promptly enhance itself, leveraging efficient memory utilization. Therefore, we propose the first continual adversarial defense (CAD) framework that adapts to any attacks in a dynamic scenario, where various attacks emerge stage by stage. In practice, CAD is modeled under four principles: (1) continual adaptation to new attacks without catastrophic forgetting, (2) few-shot adaptation, (3) memory-efficient adaptation, and (4) high accur
    
[^178]: 几何图神经网络在3D原子系统中的实践指南

    A Hitchhiker's Guide to Geometric GNNs for 3D Atomic Systems

    [https://arxiv.org/abs/2312.07511](https://arxiv.org/abs/2312.07511)

    几何图神经网络在3D原子系统中以利用物理对称性和化学性质等归纳偏差来学习几何图信息表示而著称。

    

    近年来，几何图神经网络作为首选的机器学习架构崭露头角，支持从蛋白结构预测到分子模拟和材料生成等应用，其独特之处在于利用诸如物理对称性和化学性质之类的归纳偏差，学习这些几何图的信息表示。在这篇主观论文中，我们全面而自足地概述了用于3D原子系统的几何图神经网络领域。

    arXiv:2312.07511v2 Announce Type: replace-cross  Abstract: Recent advances in computational modelling of atomic systems, spanning molecules, proteins, and materials, represent them as geometric graphs with atoms embedded as nodes in 3D Euclidean space. In these graphs, the geometric attributes transform according to the inherent physical symmetries of 3D atomic systems, including rotations and translations in Euclidean space, as well as node permutations. In recent years, Geometric Graph Neural Networks have emerged as the preferred machine learning architecture powering applications ranging from protein structure prediction to molecular simulations and material generation. Their specificity lies in the inductive biases they leverage - such as physical symmetries and chemical properties - to learn informative representations of these geometric graphs.   In this opinionated paper, we provide a comprehensive and self-contained overview of the field of Geometric GNNs for 3D atomic systems
    
[^179]: TimeDRL：多变量时间序列的解缠表示学习

    TimeDRL: Disentangled Representation Learning for Multivariate Time-Series

    [https://arxiv.org/abs/2312.04142](https://arxiv.org/abs/2312.04142)

    TimeDRL是一个具有解缠双层嵌入的通用多变量时间序列表示学习框架，通过时间戳级别和实例级别的嵌入之间的解缠派生以及时间戳-预测和实例-对比任务的利用，实现了学习丰富表示并解决归纳偏差的目标。

    

    多变量时间序列数据在许多现实世界应用中（例如医疗保健和工业）非常具有信息量，但由于缺乏标签和高维度而具有挑战性。最近的自监督学习研究显示了在学习丰富表示而不依赖于标签的潜力，但它们在学习解缠嵌入和解决归纳偏差（例如变换不变性）方面还有不足。为了解决这些挑战，我们提出了TimeDRL，一个具有解缠双层嵌入的通用多变量时间序列表示学习框架。TimeDRL的三个新颖特征为：（i）使用[CLS]令牌策略从打补丁的时间序列数据中解缠时间戳级和实例级嵌入；（ii）利用时间戳预测和实例对比任务进行解缠表示学习，前者优化时间戳级别

    arXiv:2312.04142v2 Announce Type: replace-cross  Abstract: Multivariate time-series data in numerous real-world applications (e.g., healthcare and industry) are informative but challenging due to the lack of labels and high dimensionality. Recent studies in self-supervised learning have shown their potential in learning rich representations without relying on labels, yet they fall short in learning disentangled embeddings and addressing issues of inductive bias (e.g., transformation-invariance). To tackle these challenges, we propose TimeDRL, a generic multivariate time-series representation learning framework with disentangled dual-level embeddings. TimeDRL is characterized by three novel features: (i) disentangled derivation of timestamp-level and instance-level embeddings from patched time-series data using a [CLS] token strategy; (ii) utilization of timestamp-predictive and instance-contrastive tasks for disentangled representation learning, with the former optimizing timestamp-lev
    
[^180]: 领域约束在缺失结果数据时改善风险预测

    Domain constraints improve risk prediction when outcome data is missing

    [https://arxiv.org/abs/2312.03878](https://arxiv.org/abs/2312.03878)

    提出一种贝叶斯模型类，通过领域约束的设置来改善测试和未测试患者的风险预测

    

    机器学习模型通常用于预测人类决策结果。然而，历史决策决定了观察结果，我们只观察到医生历史上测试的患者的测试结果。我们提出了一种贝叶斯模型类，能够捕捉到这种情况。模型的目的是准确估计测试和未测试患者的风险。由于未测试患者可能出现各种可能性，因此估计这个模型是具有挑战性的。为解决这个问题，我们提出了两个在健康领域合理的领域约束：预valence约束，其中整体疾病患病率是已知的，专业约束，其中人类

    arXiv:2312.03878v2 Announce Type: replace  Abstract: Machine learning models are often trained to predict the outcome resulting from a human decision. For example, if a doctor decides to test a patient for disease, will the patient test positive? A challenge is that historical decision-making determines whether the outcome is observed: we only observe test outcomes for patients doctors historically tested. Untested patients, for whom outcomes are unobserved, may differ from tested patients along observed and unobserved dimensions. We propose a Bayesian model class which captures this setting. The purpose of the model is to accurately estimate risk for both tested and untested patients. Estimating this model is challenging due to the wide range of possibilities for untested patients. To address this, we propose two domain constraints which are plausible in health settings: a prevalence constraint, where the overall disease prevalence is known, and an expertise constraint, where the huma
    
[^181]: LLMs的两面性：Jekyll博士与Hyde先生

    Dr. Jekyll and Mr. Hyde: Two Faces of LLMs

    [https://arxiv.org/abs/2312.03853](https://arxiv.org/abs/2312.03853)

    本研究通过让ChatGPT和Bard冒充复杂人物角色，绕过了安全机制和专门训练程序，展示了被禁止的回应实际上被提供了，从而有可能获取未经授权、非法或有害的信息。

    

    仅仅一年前，我们目睹了大型语言模型（LLMs）的使用增加，尤其是在结合像聊天机器人助手之类的应用时。为了防止这些助手产生不当回应，我们实施了安全机制和专门的训练程序。在这项工作中，我们通过让ChatGPT和Bard（以及在某种程度上是Bing chat）冒充复杂人物角色，绕过了这些措施，这些角色与它们本应成为的真实助手的特征相反。我们首先创造出这些人物角色的复杂传记，然后在同一聊天机器人中使用它们进行新的对话。我们的对话采用角色扮演风格，以获得助手不被允许提供的回应。通过使用人物角色，我们展示了被禁止的回应实际上被提供了，从而有可能获取未经授权、非法或有害的信息。这项工作表明，通过使用对抗性pe

    arXiv:2312.03853v2 Announce Type: replace-cross  Abstract: Only a year ago, we witnessed a rise in the use of Large Language Models (LLMs), especially when combined with applications like chatbot assistants. Safety mechanisms and specialized training procedures are implemented to prevent improper responses from these assistants. In this work, we bypass these measures for ChatGPT and Bard (and, to some extent, Bing chat) by making them impersonate complex personas with opposite characteristics as those of the truthful assistants they are supposed to be. We start by creating elaborate biographies of these personas, which we then use in a new session with the same chatbots. Our conversation followed a role-play style to get the response the assistant was not allowed to provide. By making use of personas, we show that the response that is prohibited is actually provided, making it possible to obtain unauthorized, illegal, or harmful information. This work shows that by using adversarial pe
    
[^182]: 通过分子动力学和生成建模实现玻璃转化温度的逆设计

    Inverse Design of Vitrimeric Polymers by Molecular Dynamics and Generative Modeling

    [https://arxiv.org/abs/2312.03690](https://arxiv.org/abs/2312.03690)

    通过MD模拟和机器学习，提出了一种逆设计方法，利用VAE模型生成新型Vitrimer并根据所需Tg指导设计。

    

    Vitrimer是一种能够通过动态共价自适应网络重新排列而具有自我修复能力的新型可持续聚合物类别。然而，有限的构成分子选择限制了它们的性质空间，阻碍了它们潜在应用的充分实现。通过分子动力学（MD）模拟和机器学习（ML），特别是一种新颖的图变分自动编码器（VAE）模型，我们建立了一种生成新型Vitrimer并根据所需玻璃转变温度（Tg）指导其逆设计的方法。我们构建了第一个Vitrimer数据集，包含一百万种，并通过高通量MD模拟，由高斯过程模型校准，计算了8424种的Tg。所提出的VAE采用双图编码器和潜在维度重叠方案，允许多成分Vitrimer的个体表示。通过构建一个连续的潜在空间

    arXiv:2312.03690v2 Announce Type: replace-cross  Abstract: Vitrimer is a new class of sustainable polymers with the ability of self-healing through rearrangement of dynamic covalent adaptive networks. However, a limited choice of constituent molecules restricts their property space, prohibiting full realization of their potential applications. Through a combination of molecular dynamics (MD) simulations and machine learning (ML), particularly a novel graph variational autoencoder (VAE) model, we establish a method for generating novel vitrimers and guide their inverse design based on desired glass transition temperature (Tg). We build the first vitrimer dataset of one million and calculate Tg on 8,424 of them by high-throughput MD simulations calibrated by a Gaussian process model. The proposed VAE employs dual graph encoders and a latent dimension overlapping scheme which allows for individual representation of multi-component vitrimers. By constructing a continuous latent space conta
    
[^183]: Jellyfish：一个用于数据预处理的大型语言模型

    Jellyfish: A Large Language Model for Data Preprocessing

    [https://arxiv.org/abs/2312.01678](https://arxiv.org/abs/2312.01678)

    这项研究探讨了在数据挖掘中利用大型语言模型进行数据预处理的方法，通过指导调整本地LLMs来解决通用数据预处理问题，确保数据安全并进行进一步调整

    

    这篇论文探讨了在数据挖掘管道中将原始数据转换为有利于简单处理的干净格式的数据预处理（DP）中LLMs的利用。与使用LLMs为DP设计通用解决方案引起了兴趣相比，最近在这一领域的倡议通常依赖于GPT API，引发了不可避免的数据泄霏担忧。与这些方法不同，我们考虑将指导调整本地LLMs（7-13B模型）作为通用DP问解器。我们选择了代表性DP任务的四组数据集，并利用针对DP定制的序列化和知识注入技术构建了指导调整数据。因此，指导调整的LLMs使用户能够为DP手动制定指导。同时，它们可以在本地、单一和价格低廉的GPU上运行，确保数据安全并实现进一步调整。我们的实验表明，我们为DP指导构建的数据集

    arXiv:2312.01678v4 Announce Type: replace  Abstract: This paper explores the utilization of LLMs for data preprocessing (DP), a crucial step in the data mining pipeline that transforms raw data into a clean format conducive to easy processing. Whereas the use of LLMs has sparked interest in devising universal solutions to DP, recent initiatives in this domain typically rely on GPT APIs, raising inevitable data breach concerns. Unlike these approaches, we consider instruction-tuning local LLMs (7 - 13B models) as universal DP ask solver. We select a collection of datasets across four representative DP tasks and construct instruction-tuning data using serialization and knowledge injection techniques tailored to DP. As such, the instruction-tuned LLMs empower users to manually craft instructions for DP. Meanwhile, they can operate on a local, single, and low-priced GPU, ensuring data security and enabling further tuning. Our experiments show that our dataset constructed for DP instruction
    
[^184]: MLLMs增强视觉-语言表示学习

    MLLMs-Augmented Visual-Language Representation Learning

    [https://arxiv.org/abs/2311.18765](https://arxiv.org/abs/2311.18765)

    MLLMs通过为图像-文本数据集建立更丰富的图像-文本关联，以增强视觉-语言表示学习，并通过“文本剪切”方法来避免偏见引入，显著提高了图像-文本检索的性能。

    

    arXiv:2311.18765v3 公告类型: replace-cross 视觉-语言预训练在许多多模态任务中取得了显著成功，这在很大程度上归功于大规模图像-文本数据集的可用性。在这项工作中，我们证明了多模态大型语言模型（MLLMs）可以通过为图像-文本数据集建立更丰富的图像-文本关联来加强视觉-语言表示学习。我们的方法很简单，利用MLLMs为每个图像扩展多个不同的标题。为了防止MLLMs的幻觉和单调语言风格引入的偏见，我们提出了“文本剪切”来保持扩展标题的质量和可用性。在图像-文本检索中，在不引入额外的训练成本的情况下，我们的方法在精调和零-shot设置下一致地在Recall@1上获得了5.6 ~ 35.0和16.8 ~ 46.1的改进。值得注意的是，我们获得了与在目标数据集上进行微调相当的零-shot结果。

    arXiv:2311.18765v3 Announce Type: replace-cross  Abstract: Visual-language pre-training has achieved remarkable success in many multi-modal tasks, largely attributed to the availability of large-scale image-text datasets. In this work, we demonstrate that Multi-modal Large Language Models (MLLMs) can enhance visual-language representation learning by establishing richer image-text associations for image-text datasets. Our approach is simple, utilizing MLLMs to extend multiple diverse captions for each image. To prevent the bias introduced by MLLMs' hallucinations and monotonous language styles, we propose "text shearing" to maintain the quality and availability of extended captions. In image-text retrieval, without introducing additional training cost, our method consistently obtains 5.6 ~ 35.0 and 16.8 ~ 46.1 improvement on Recall@1 under the fine-tuning and zero-shot settings, respectively. Notably, we obtain zero-shot results that are comparable to fine-tuning on target datasets, wh
    
[^185]: DPOD：面向多模态假新闻检测的领域特定提示调节

    DPOD: Domain-Specific Prompt Tuning for Multimodal Fake News Detection

    [https://arxiv.org/abs/2311.16496](https://arxiv.org/abs/2311.16496)

    本研究提出了一种名为DPOD的框架，通过利用跨领域数据来改善所需领域的脱离上下文误信息检测，解决数据不平衡的问题。

    

    虚假新闻利用脱离上下文的图像传播已经变得普遍，是信息过载时代一个相关的问题。这种脱离上下文的虚假新闻可能涉及不同领域，如政治、体育、娱乐等。在实际场景中，不同领域新闻文章存在着数据不平衡的问题，部分领域数据丰富，而其他领域数据非常有限。在这种情况下，必须开发出能够适应不同数据量设置的方法。本文探讨了跨领域数据是否有助于改善所需领域的脱离上下文误信息检测（在此称为多模态虚假新闻检测）的方法以解决这一具有挑战性的问题。为此，我们提出了一种名为DPOD（使用跨领域数据进行领域特定提示调节）的新框架。

    arXiv:2311.16496v2 Announce Type: replace  Abstract: The spread of fake news using out-of-context images has become widespread and is a relevant problem in this era of information overload. Such out-of-context fake news may arise across different domains like politics, sports, entertainment, etc. In practical scenarios, an inherent problem of imbalance exists among news articles from such widely varying domains, resulting in a few domains with abundant data, while the rest containing very limited data. Under such circumstances, it is imperative to develop methods which can work in such varying amounts of data setting. In this work, we explore whether out-of-domain data can help to improve out-of-context misinformation detection (termed here as multi-modal fake news detection) of a desired domain, to address this challenging problem. Towards this goal, we propose a novel framework termed DPOD (Domain-specific Prompt-tuning using Out-of-Domain data). First, to compute generalizable featu
    
[^186]: 通过局部曲率配置实现有效的结构编码

    Effective Structural Encodings via Local Curvature Profiles

    [https://arxiv.org/abs/2311.14864](https://arxiv.org/abs/2311.14864)

    本文从几何的角度研究了哪种结构属性产生最有效的编码，提出了一种基于离散Ricci曲率的新型结构编码（局部曲率配置，LCP），证明其明显优于现有编码方法，并表明将局部结构编码与全局位置编码相结合可以提升下游性能，捕捉到互补的几何信息。

    

    结构和位置编码可以显著提升图神经网络在下游任务中的性能。最近的文献开始系统地研究这些方法编码的结构属性之间的差异，以及它们之间的性能权衡。然而，哪些结构属性产生最有效的编码仍然不清楚。在本文中，我们从几何角度研究了这个问题。我们提出了一种基于离散Ricci曲率的新型结构编码（局部曲率配置，简称LCP），并展示了它明显优于现有的编码方法。我们进一步展示，将局部结构编码（如LCP）与全局位置编码相结合可以提升下游性能，表明它们捕捉了互补的几何信息。最后，我们比较了不同类型的编码与（基于曲率的）重连技术。

    arXiv:2311.14864v2 Announce Type: replace  Abstract: Structural and Positional Encodings can significantly improve the performance of Graph Neural Networks in downstream tasks. Recent literature has begun to systematically investigate differences in the structural properties that these approaches encode, as well as performance trade-offs between them. However, the question of which structural properties yield the most effective encoding remains open. In this paper, we investigate this question from a geometric perspective. We propose a novel structural encoding based on discrete Ricci curvature (Local Curvature Profiles, short LCP) and show that it significantly outperforms existing encoding approaches. We further show that combining local structural encodings, such as LCP, with global positional encodings improves downstream performance, suggesting that they capture complementary geometric information. Finally, we compare different encoding types with (curvature-based) rewiring techni
    
[^187]: 面向安全的因果表示方法用于自动驾驶中值得信赖的离线强化学习

    Safety-aware Causal Representation for Trustworthy Offline Reinforcement Learning in Autonomous Driving

    [https://arxiv.org/abs/2311.10747](https://arxiv.org/abs/2311.10747)

    本文提出了一种面向安全的因果表示方法FUSION，在自动驾驶中的离线强化学习中利用结构化情景信息促进泛化的端到端驾驶策略学习。

    

    在自动驾驶领域，离线强化学习方法在解决来自离线数据集的顺序决策问题方面表现出显著的效果。然而，在各种安全关键场景中保持安全性仍然是一个重要挑战，因为离线数据集中缺乏长尾和无法预见的场景。在本文中，我们引入了saFety-aware strUctured Scenario representatION (FUSION)，这是一种离线强化学习中的开创性表示学习方法，通过利用结构化场景信息促进学习可泛化的端到端驾驶策略。FUSION利用分解奖励、成本、状态和动作空间之间的因果关系，构建了一个在动态交通环境中进行结构化顺序推理的框架。我们在自动驾驶领域的两个典型真实世界设定下进行了广泛的评估。

    arXiv:2311.10747v3 Announce Type: replace-cross  Abstract: In the domain of autonomous driving, the offline Reinforcement Learning~(RL) approaches exhibit notable efficacy in addressing sequential decision-making problems from offline datasets. However, maintaining safety in diverse safety-critical scenarios remains a significant challenge due to long-tailed and unforeseen scenarios absent from offline datasets. In this paper, we introduce the saFety-aware strUctured Scenario representatION (FUSION), a pioneering representation learning method in offline RL to facilitate the learning of a generalizable end-to-end driving policy by leveraging structured scenario information. FUSION capitalizes on the causal relationships between the decomposed reward, cost, state, and action space, constructing a framework for structured sequential reasoning in dynamic traffic environments. We conduct extensive evaluations in two typical real-world settings of the distribution shift in autonomous vehicl
    
[^188]: Agent Lumos: 统一和模块化训练开源语言代理

    Agent Lumos: Unified and Modular Training for Open-Source Language Agents

    [https://arxiv.org/abs/2311.05657](https://arxiv.org/abs/2311.05657)

    Agent Lumos提出了一种统一和模块化的框架，通过规划模块学习高级子目标生成，训练接地模块将其转化为动作，促进广泛互动任务应用。

    

    闭源代理存在诸多问题，如缺乏负担得起性、透明度和可重复性，特别是在复杂的互动任务中。这促使了开源替代方案的发展。我们介绍了 LUMOS，这是第一个为训练开源 LLM-based 代理而设计的框架之一。LUMOS具有可学习、统一和模块化的架构，其中包括一个学习高级子目标生成的规划模块，以及一个训练有素的接地模块，用于使用执行模块中的各种工具将这些转化为动作。这种设计允许模块化升级，并更广泛地适用于不同的互动任务。为了促进通用代理学习，我们收集了源自各种复杂互动任务中不同地面真实推理原理的大规模、统一和高质量的训练注释。在9个数据集上，LUMOS表现出了几个关键优势：（1）LUMOS在多个较大的开源a

    arXiv:2311.05657v2 Announce Type: replace  Abstract: Closed-source agents suffer from several issues such as a lack of affordability, transparency, and reproducibility, particularly on complex interactive tasks. This motivates the development of open-source alternatives. We introduce LUMOS, one of the first frameworks for training open-source LLM-based agents. LUMOS features a learnable, unified, and modular architecture with a planning module that learns high-level subgoal generation, and a grounding module trained to translate these into actions using various tools in the execution module. The design allows for modular upgrades and wider applicability to diverse interactive tasks. To foster generalizable agent learning, we collect large-scale, unified, and high-quality training annotations derived from diverse ground-truth reasoning rationales across various complex interactive tasks. On 9 datasets, LUMOS exhibits several key advantages: (1) LUMOS excels multiple larger open-source a
    
[^189]: 通过随机化使基于偏好反馈的强化学习变得高效

    Making RL with Preference-based Feedback Efficient via Randomization

    [https://arxiv.org/abs/2310.14554](https://arxiv.org/abs/2310.14554)

    在基于偏好反馈的强化学习中，通过引入随机化设计的算法在线性MDP模型下表现出样本高效性和多项式运行时间，并通过随机化主动学习过程最小化了查询复杂性。

    

    强化学习算法需要从人类反馈中学习，而且在统计复杂性、计算复杂性和查询复杂性方面需要高效。本研究考虑了使用偏好来表达对轨迹对的反馈的强化学习设置。在线性MDP模型中，通过在算法设计中引入随机化，我们提出了一种算法，具有样本高效性（即具有近乎最优的最坏情况遗憾界限）和多项式运行时间（即计算复杂度与相关参数是多项式关系）。我们的算法进一步通过一种新颖的随机化主动学习过程最小化了查询复杂性。特别地，我们的算法展示了遗憾界限和查询复杂性之间的近乎最优折衷。为了将结果推广到更一般的非线性函数逼近，我们设计了一个受线性MDP模型的随机化算法启发而来的基于模型的随机化算法。

    arXiv:2310.14554v2 Announce Type: replace-cross  Abstract: Reinforcement Learning algorithms that learn from human feedback (RLHF) need to be efficient in terms of statistical complexity, computational complexity, and query complexity. In this work, we consider the RLHF setting where the feedback is given in the format of preferences over pairs of trajectories. In the linear MDP model, using randomization in algorithm design, we present an algorithm that is sample efficient (i.e., has near-optimal worst-case regret bounds) and has polynomial running time (i.e., computational complexity is polynomial with respect to relevant parameters). Our algorithm further minimizes the query complexity through a novel randomized active learning procedure. In particular, our algorithm demonstrates a near-optimal tradeoff between the regret bound and the query complexity. To extend the results to more general nonlinear function approximation, we design a model-based randomized algorithm inspired by th
    
[^190]: 用大型语言模型解密嵌入空间

    Demystifying Embedding Spaces using Large Language Models

    [https://arxiv.org/abs/2310.04475](https://arxiv.org/abs/2310.04475)

    通过使用大型语言模型（LLMs）直接与嵌入交互，将抽象向量转换为可理解的叙述，使得复杂嵌入数据更具解释性和广泛实用性。

    

    嵌入已经成为表示有关实体、概念和关系的复杂多方面信息的关键手段，以一种紧凑且有用的方式。然而，它们通常难以直接解释。尽管下游任务利用了这些压缩表示，但有意义的解释通常需要使用降维或专门的机器学习可解释性方法进行可视化。本文通过使用大型语言模型（LLMs）直接与嵌入交互，将抽象向量转换为可理解的叙述，从而解决了使这些嵌入更具解释性和广泛实用性的挑战。通过将嵌入注入LLMs，我们实现了对复杂嵌入数据的查询和探索。我们在各种不同的任务上演示了我们的方法，其中包括：增强概念激活向量（CAVs）、传达新颖的嵌入实体等。

    arXiv:2310.04475v2 Announce Type: replace-cross  Abstract: Embeddings have become a pivotal means to represent complex, multi-faceted information about entities, concepts, and relationships in a condensed and useful format. Nevertheless, they often preclude direct interpretation. While downstream tasks make use of these compressed representations, meaningful interpretation usually requires visualization using dimensionality reduction or specialized machine learning interpretability methods. This paper addresses the challenge of making such embeddings more interpretable and broadly useful, by employing Large Language Models (LLMs) to directly interact with embeddings -- transforming abstract vectors into understandable narratives. By injecting embeddings into LLMs, we enable querying and exploration of complex embedding data. We demonstrate our approach on a variety of diverse tasks, including: enhancing concept activation vectors (CAVs), communicating novel embedded entities, and decod
    
[^191]: 量化神经机器翻译中背景依赖性的可信度

    Quantifying the Plausibility of Context Reliance in Neural Machine Translation

    [https://arxiv.org/abs/2310.01188](https://arxiv.org/abs/2310.01188)

    引入了PECoRe框架，用于量化语言模型生成中的上下文使用情况，从而评估上下文感知机器翻译模型的可信度。

    

    本文介绍了一种名为PECoRe的端到端可解释性框架，旨在量化语言模型生成中上下文使用的情况。我们的方法利用模型内部来对比识别生成文本中上下文敏感的目标令牌，并将它们与证明其预测的上下文线索联系起来。我们使用PECORE来量化具有上下文感知的机器翻译模型的可信度，将模型的理由与人类注释在几个层次的话语水平上进行比较。

    arXiv:2310.01188v2 Announce Type: replace-cross  Abstract: Establishing whether language models can use contextual information in a human-plausible way is important to ensure their trustworthiness in real-world settings. However, the questions of when and which parts of the context affect model generations are typically tackled separately, with current plausibility evaluations being practically limited to a handful of artificial benchmarks. To address this, we introduce Plausibility Evaluation of Context Reliance (PECoRe), an end-to-end interpretability framework designed to quantify context usage in language models' generations. Our approach leverages model internals to (i) contrastively identify context-sensitive target tokens in generated texts and (ii) link them to contextual cues justifying their prediction. We use \pecore to quantify the plausibility of context-aware machine translation models, comparing model rationales with human annotations across several discourse-level pheno
    
[^192]: 线性注意力可能就是理解Transformer优化的关键

    Linear attention is (maybe) all you need (to understand transformer optimization)

    [https://arxiv.org/abs/2310.01082](https://arxiv.org/abs/2310.01082)

    研究者通过训练线性Transformer模型解决回归任务，发现这种简单的线性化模型能够重现Transformer训练动态的多个关键方面，表明线性注意力可能是理解Transformer优化的关键。

    

    Transformer的训练因需要仔细设计优化器并使用各种启发式而变得困难。本文通过仔细研究一个简单但经典的线性化浅层Transformer模型，取得了解析Transformer训练细微之处的进展。具体来说，我们训练线性Transformer来解决回归任务，灵感来源于J. von Oswald等人（ICML 2023）和K. Ahn等人（NeurIPS 2023）。最重要的是，我们观察到我们提出的线性化模型能够重现Transformer训练动态的几个重要方面。因此，本文得到的结果表明，一个简单的线性化Transformer模型实际上可能是理解Transformer优化的有价值、现实的抽象。

    arXiv:2310.01082v2 Announce Type: replace-cross  Abstract: Transformer training is notoriously difficult, requiring a careful design of optimizers and use of various heuristics. We make progress towards understanding the subtleties of training Transformers by carefully studying a simple yet canonical linearized shallow Transformer model. Specifically, we train linear Transformers to solve regression tasks, inspired by J.~von Oswald et al.~(ICML 2023), and K.~Ahn et al.~(NeurIPS 2023). Most importantly, we observe that our proposed linearized models can reproduce several prominent aspects of Transformer training dynamics. Consequently, the results obtained in this paper suggest that a simple linearized Transformer model could actually be a valuable, realistic abstraction for understanding Transformer optimization.
    
[^193]: CRAFT: 通过创建和检索专业工具集定制LLMs

    CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets

    [https://arxiv.org/abs/2309.17428](https://arxiv.org/abs/2309.17428)

    CRAFT提出了一个通用工具创建和检索框架，能够定制LLMs，为其创建特定任务的工具集，并使用这些工具集增强其解决复杂任务的能力。

    

    大语言模型（LLMs）通常通过生成代码片段并通过特定任务的应用程序编程接口（API）执行它们来解决复杂任务。本文介绍了CRAFT，这是一个专门为LLMs创建工具集的通用工具创建和检索框架。它为任务创建了特定的工具集，并为LLMs配备了一个组件，用于从这些集合中检索工具，以增强它们解决复杂任务的能力。

    arXiv:2309.17428v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) are often augmented with tools to solve complex tasks. By generating code snippets and executing them through task-specific Application Programming Interfaces (APIs), they can offload certain functions to dedicated external modules, such as image encoding and performing calculations. However, most existing approaches to augment LLMs with tools are constrained by general-purpose APIs and lack the flexibility for tailoring them to specific tasks. In this work, we present CRAFT, a general tool creation and retrieval framework for LLMs. It creates toolsets specifically curated for the tasks and equips LLMs with a component that retrieves tools from these sets to enhance their capability to solve complex tasks. For each task, we collect specific code solutions by prompting GPT-4 to solve the training examples. Following a validation step ensuring the correctness, these solutions are abstracted into code 
    
[^194]: 自适应锐度感知修剪用于稳健稀疏网络

    Adaptive Sharpness-Aware Pruning for Robust Sparse Networks

    [https://arxiv.org/abs/2306.14306](https://arxiv.org/abs/2306.14306)

    AdaSAP方法通过网络锐度的视角统一了稳健性和紧凑性的目标，并通过策略性地引入权重扰动来使稀疏网络对训练时未见的输入变化稳健，在图像分类和目标检测任务上取得显著改进。

    

    arXiv:2306.14306v2 公告类型: 替换 摘要: 在现实世界中部署的深度学习模型必须具有稳健性和紧凑性这两个基本属性。稳健性和紧凑性的目标似乎是相互矛盾的，因为稳健性需要在域间进行泛化，而压缩过程则利用一个域中的特定性。我们引入了自适应锐度感知修剪（AdaSAP），通过网络锐度的视角统一了这些目标。AdaSAP方法通过策略性地引入权重扰动来优化损失景观，从而产生对训练时未见输入变化稳健的稀疏网络。这使得模型既适合修剪又适用于改进稳健性的正则化。在图像分类上，AdaSAP能够使修剪模型在ImageNet C上的稳健精度提高最高达到+6％，在ImageNet V2上提高+4％，在目标检测上在一组受损Pascal VOC数据集上提高+4％。

    arXiv:2306.14306v2 Announce Type: replace  Abstract: Robustness and compactness are two essential attributes of deep learning models that are deployed in the real world. The goals of robustness and compactness may seem to be at odds, since robustness requires generalization across domains, while the process of compression exploits specificity in one domain. We introduce Adaptive Sharpness-Aware Pruning (AdaSAP), which unifies these goals through the lens of network sharpness. The AdaSAP method produces sparse networks that are robust to input variations which are unseen at training time. We achieve this by strategically incorporating weight perturbations in order to optimize the loss landscape. This allows the model to be both primed for pruning and regularized for improved robustness. AdaSAP improves the robust accuracy of pruned models on image classification by up to +6% on ImageNet C and +4% on ImageNet V2, and on object detection by +4% on a corrupted Pascal VOC dataset, over a wi
    
[^195]: 在对抗性分布设置中的随机Kaczmarz

    Randomized Kaczmarz in Adversarial Distributed Setting

    [https://arxiv.org/abs/2302.14615](https://arxiv.org/abs/2302.14615)

    提出了一种在对抗性分布设置中具有对抗鲁棒性的迭代方法，通过简单的统计信息保证收敛，能够适应对抗性分布，在模拟中展示了解决凸问题的高效性，具有高准确性识别对手工作者和容忍不同对手率的能力。

    

    开发大规模分布式方法，使其能够在存在对抗性或受损工作者的情况下保持稳健性，这是使这些方法在实际问题中变得实用的重要部分。在本文中，我们提出了一种针对凸优化问题具有对抗性的迭代方法。通过利用简单的统计信息，我们的方法确保收敛，并能够适应对抗性分布。此外，在存在对手的情况下，通过模拟展示了所提出的方法在解决凸问题上的效率。通过模拟，我们展示了我们方法在存在对手的情况下的高效性，并且它能够高度准确地识别对手工作者并容忍不同水平的对手率。

    arXiv:2302.14615v2 Announce Type: replace-cross  Abstract: Developing large-scale distributed methods that are robust to the presence of adversarial or corrupted workers is an important part of making such methods practical for real-world problems. In this paper, we propose an iterative approach that is adversary-tolerant for convex optimization problems. By leveraging simple statistics, our method ensures convergence and is capable of adapting to adversarial distributions. Additionally, the efficiency of the proposed methods for solving convex problems is shown in simulations with the presence of adversaries. Through simulations, we demonstrate the efficiency of our approach in the presence of adversaries and its ability to identify adversarial workers with high accuracy and tolerate varying levels of adversary rates.
    
[^196]: 图像和视频中可解释的异常检测：一项调研

    Explainable Anomaly Detection in Images and Videos: A Survey

    [https://arxiv.org/abs/2302.06670](https://arxiv.org/abs/2302.06670)

    这项研究提供了针对图像和视频的可解释异常检测方法的首次调研，为机器学习学术界和实际应用提供了重要参考。

    

    异常检测和定位视觉数据（包括图像和视频）在机器学习学术界和应用实际场景中具有重要意义。尽管近年来可视异常检测技术迅速发展，但对于这些黑盒模型的解释以及为何可以区分异常的合理解释却十分稀缺。本文首次提供了一项集中于可解释视觉异常检测方法的调研。我们首先介绍了图像级和视频级异常检测的基本背景。然后，作为本调研的主要内容，我们展示了针对图像和视频的可解释异常检测方法的全面和详尽的文献综述。接下来，我们分析了为什么一些可解释异常检测方法可以应用于图像和视频，而另一些则只能应用于一种模态。此外，我们提供了总结

    arXiv:2302.06670v2 Announce Type: replace-cross  Abstract: Anomaly detection and localization of visual data, including images and videos, are of great significance in both machine learning academia and applied real-world scenarios. Despite the rapid development of visual anomaly detection techniques in recent years, the interpretations of these black-box models and reasonable explanations of why anomalies can be distinguished out are scarce. This paper provides the first survey concentrated on explainable visual anomaly detection methods. We first introduce the basic background of image-level and video-level anomaly detection. Then, as the main content of this survey, a comprehensive and exhaustive literature review of explainable anomaly detection methods for both images and videos is presented. Next, we analyze why some explainable anomaly detection methods can be applied to both images and videos and why others can be only applied to one modality. Additionally, we provide summaries
    
[^197]: 课程图机器学习：一项调查

    Curriculum Graph Machine Learning: A Survey

    [https://arxiv.org/abs/2302.02926](https://arxiv.org/abs/2302.02926)

    本文综述了课程图机器学习的方法，介绍了解决现有图机器学习模型性能不佳问题的关键挑战和最新进展。

    

    图机器学习在学术界和工业界得到了广泛研究。然而，在文献中，大多数现有的图机器学习模型被设计为以随机顺序对数据样本进行训练，可能由于忽视不同图数据样本的重要性以及它们的训练顺序对模型优化状态的影响而导致性能不佳。为了解决这一关键问题，课程图机器学习（Graph CL）应运而生，融合了图机器学习和课程学习的优势，吸引了研究界越来越多的关注。因此，在本文中，我们全面概述了关于Graph CL的方法，并详细调查了这一方向的最新进展。具体而言，我们首先讨论了Graph CL的关键挑战，并提供了其正式的问题定义。然后，我们将现有方法分类和总结为三种

    arXiv:2302.02926v2 Announce Type: replace  Abstract: Graph machine learning has been extensively studied in both academia and industry. However, in the literature, most existing graph machine learning models are designed to conduct training with data samples in a random order, which may suffer from suboptimal performance due to ignoring the importance of different graph data samples and their training orders for the model optimization status. To tackle this critical problem, curriculum graph machine learning (Graph CL), which integrates the strength of graph machine learning and curriculum learning, arises and attracts an increasing amount of attention from the research community. Therefore, in this paper, we comprehensively overview approaches on Graph CL and present a detailed survey of recent advances in this direction. Specifically, we first discuss the key challenges of Graph CL and provide its formal problem definition. Then, we categorize and summarize existing methods into thre
    
[^198]: 基于传感器增强快挂的朝向的攀岩中下降行为的检测

    Lowering Detection in Sport Climbing Based on Orientation of the Sensor Enhanced Quickdraw

    [https://arxiv.org/abs/2301.10164](https://arxiv.org/abs/2301.10164)

    通过在攀岩快挂上安装的加速度传感器采集数据，实现了在攀岩活动中检测攀岩者下降情况的技术，保护攀岩者隐私和健身房成本的同时提高了效率和便利性。

    

    跟踪攀岩者的活动以改善服务并最大限度地利用他们的基础设施是攀岩健身房关注的焦点。必须从开始分析每个攀岩活动直到攀登者降下来。因此，发现攀岩者下降是至关重要的，因为这标志着攀登结束。必须在保护攀岩者和健身房成本隐私和便利性的同时解决这个问题。为此，开发了一个硬件原型，使用附在墙上的攀岩设备上的加速度传感器收集数据，称为快挂，它连接攀岩绳和螺栓锚点。相应的传感器被配置为节能，因此在攀岩健身房大量使用时在费用和更换所需时间方面变得实用。本文描述了硬件规格，并研究了传感器测得的数据。

    arXiv:2301.10164v2 Announce Type: replace-cross  Abstract: Tracking climbers' activity to improve services and make the best use of their infrastructure is a concern for climbing gyms. Each climbing session must be analyzed from beginning till lowering of the climber. Therefore, spotting the climbers descending is crucial since it indicates when the ascent has come to an end. This problem must be addressed while preserving privacy and convenience of the climbers and the costs of the gyms. To this aim, a hardware prototype is developed to collect data using accelerometer sensors attached to a piece of climbing equipment mounted on the wall, called quickdraw, that connects the climbing rope to the bolt anchors. The corresponding sensors are configured to be energy-efficient, hence become practical in terms of expenses and time consumption for replacement when using in large quantity in a climbing gym. This paper describes hardware specifications, studies data measured by the sensors in u
    
[^199]: 适应性远端算法用于凸优化在梯度的局部Lipschitz连续性下

    Adaptive proximal algorithms for convex optimization under local Lipschitz continuity of the gradient

    [https://arxiv.org/abs/2301.04431](https://arxiv.org/abs/2301.04431)

    在此工作中，我们提出了一种自适应贴近梯度方法，通过使用新的局部平滑度模量的估计，可以在凸优化中避免使用回溯线搜索，并根据局部平滑度估计自适应调整步长。

    

    回溯线搜索是最常用的方法，用于最小化具有局部Lipschitz梯度的连续可微函数。近年来，已经证明在凸设置中完全可以避免线搜索，并且允许步长根据局部平滑度估计进行自适应调整，而无需任何回溯或评估函数值。在这项工作中，我们提出了一种自适应贴近梯度方法，adaPG，它使用局部平滑度模量的新估计，导致更少保守的步长更新，还可以处理非光滑项。这个想法被扩展到原始-对偶设置，其中提出了一种自适应三项原始-对偶算法，adaPD，它可以被看作是PDHG方法的扩展。此外，在这个设置中，提出了“本质上”完全自适应的变体adaPD$^+$，通过调用一个回溯方法，避免了评估线性算子范数。

    arXiv:2301.04431v4 Announce Type: replace-cross  Abstract: Backtracking linesearch is the de facto approach for minimizing continuously differentiable functions with locally Lipschitz gradient. In recent years, it has been shown that in the convex setting it is possible to avoid linesearch altogether, and to allow the stepsize to adapt based on a local smoothness estimate without any backtracks or evaluations of the function value. In this work we propose an adaptive proximal gradient method, adaPG, that uses novel estimates of the local smoothness modulus which leads to less conservative stepsize updates and that can additionally cope with nonsmooth terms. This idea is extended to the primal-dual setting where an adaptive three-term primal-dual algorithm, adaPD, is proposed which can be viewed as an extension of the PDHG method. Moreover, in this setting the "essentially" fully adaptive variant adaPD$^+$ is proposed that avoids evaluating the linear operator norm by invoking a backtra
    
[^200]: 基于声学特征和降维的无监督声学场景映射

    Unsupervised Acoustic Scene Mapping Based on Acoustic Features and Dimensionality Reduction

    [https://arxiv.org/abs/2301.00448](https://arxiv.org/abs/2301.00448)

    提出了一种基于声学特征和降维的无监督声学场景映射方法，利用局部共形自动编码器（LOCA）学习声学数据的标准化坐标，能够较好地处理混响和加性噪声，并通过实验和模拟验证了其性能。

    

    传统的声学场景映射方法需要估计麦克风之间的到达时间差（TDOA）。不幸的是，TDOA估计对混响和加性噪声非常敏感。我们引入了一种无监督数据驱动方法，利用数据的自然结构。我们的方法建立在局部共形自动编码器（LOCA）之上-一种用于从测量中学习标准化数据坐标的离线深度学习方案。我们的实验设置包括一个测量声源在声学封闭空间中多个位置的麦克风阵列。我们证明LOCA学习到了一个与麦克风的空间位置等距的表示。我们使用一系列逼真的模拟实验评估了我们的方法的性能，并与其他降维方案进行了比较。我们进一步评估了混响对LOCA结果的影响。

    arXiv:2301.00448v2 Announce Type: replace-cross  Abstract: Classical methods for acoustic scene mapping require the estimation of time difference of arrival (TDOA) between microphones. Unfortunately, TDOA estimation is very sensitive to reverberation and additive noise. We introduce an unsupervised data-driven approach that exploits the natural structure of the data. Our method builds upon local conformal autoencoders (LOCA) - an offline deep learning scheme for learning standardized data coordinates from measurements. Our experimental setup includes a microphone array that measures the transmitted sound source at multiple locations across the acoustic enclosure. We demonstrate that LOCA learns a representation that is isometric to the spatial locations of the microphones. The performance of our method is evaluated using a series of realistic simulations and compared with other dimensionality-reduction schemes. We further assess the influence of reverberation on the results of LOCA and
    
[^201]: 直接潜在模型学习能够解决线性二次高斯控制问题吗？

    Can Direct Latent Model Learning Solve Linear Quadratic Gaussian Control?

    [https://arxiv.org/abs/2212.14511](https://arxiv.org/abs/2212.14511)

    该论文提出了直接潜在模型学习的方法，用于解决线性二次高斯控制问题，能够在有限样本下找到近似最优状态表示函数和控制器。

    

    我们研究了从潜在高维观测中学习状态表示的任务，目标是控制未知的部分可观察系统。我们采用直接潜在模型学习方法，通过预测与规划直接相关的数量（例如成本）来学习潜在状态空间中的动态模型，而无需重建观测。具体来说，我们专注于一种直观的基于成本驱动的状态表示学习方法，用于解决线性二次高斯（LQG）控制问题，这是最基本的部分可观察控制问题之一。作为我们的主要结果，我们建立了在有限样本下找到近似最优状态表示函数和使用直接学习的潜在模型找到近似最优控制器的保证。据我们所知，尽管以前的相关工作取得了各种经验成功，但在这项工作之前，尚不清楚这种基于成本驱动的潜在模型学习方法是否具有有限样本保证。

    arXiv:2212.14511v2 Announce Type: replace  Abstract: We study the task of learning state representations from potentially high-dimensional observations, with the goal of controlling an unknown partially observable system. We pursue a direct latent model learning approach, where a dynamic model in some latent state space is learned by predicting quantities directly related to planning (e.g., costs) without reconstructing the observations. In particular, we focus on an intuitive cost-driven state representation learning method for solving Linear Quadratic Gaussian (LQG) control, one of the most fundamental partially observable control problems. As our main results, we establish finite-sample guarantees of finding a near-optimal state representation function and a near-optimal controller using the directly learned latent model. To the best of our knowledge, despite various empirical successes, prior to this work it was unclear if such a cost-driven latent model learner enjoys finite-sampl
    
[^202]: TILDE-Q：一种用于时间序列预测的变换不变损失函数

    TILDE-Q: A Transformation Invariant Loss Function for Time-Series Forecasting

    [https://arxiv.org/abs/2210.15050](https://arxiv.org/abs/2210.15050)

    本文提出了一种对时间序列预测中复杂时间模式进行建模的变换不变损失函数TILDE-Q，解决了当前模型在捕获信号形状和模拟细微时间动态上的挑战。

    

    时间序列预测在人工智能领域越来越受到关注，因为它有潜力解决能源、天气、交通和经济等各领域的实际问题。然而，尽管时间序列预测是一个经过深入研究的领域，但对于预测复杂的时间模式，如顺序数据中的突变，目前的模型仍然面临挑战。这种困难源于将Lp范数距离最小化作为损失函数，例如平均绝对误差（MAE）或均方误差（MSE），这些函数容易受到复杂时间动态建模和信号形状捕获的影响。此外，这些函数通常会导致模型行为异常，生成与原始时间序列不相关的结果。因此，开发一种超越点对点比较的形状感知损失函数至关重要。在本文中，我们研究了形状和失真的定义，

    arXiv:2210.15050v2 Announce Type: replace  Abstract: Time-series forecasting has gained increasing attention in the field of artificial intelligence due to its potential to address real-world problems across various domains, including energy, weather, traffic, and economy. While time-series forecasting is a well-researched field, predicting complex temporal patterns such as sudden changes in sequential data still poses a challenge with current models. This difficulty stems from minimizing Lp norm distances as loss functions, such as mean absolute error (MAE) or mean square error (MSE), which are susceptible to both intricate temporal dynamics modeling and signal shape capturing. Furthermore, these functions often cause models to behave aberrantly and generate uncorrelated results with the original time-series. Consequently, developing a shape-aware loss function that goes beyond mere point-wise comparison is essential. In this paper, we examine the definition of shape and distortions, 
    
[^203]: 通过正确得分改进分类及其他任务的更好不确定性校准

    Better Uncertainty Calibration via Proper Scores for Classification and Beyond

    [https://arxiv.org/abs/2203.07835](https://arxiv.org/abs/2203.07835)

    介绍了正确校准误差框架，通过将每个校准误差与正确得分相关联，提供了最佳估计特性的上界，可靠量化模型校准改进。

    

    随着模型的可信度对于敏感的现实应用非常重要，从业者越来越关注改进深度神经网络的不确定性校准。校准误差旨在量化概率预测的可靠性，但它们的估计通常是有偏差且不一致的。在这项工作中，我们介绍了正确校准误差的框架，它将每个校准误差与正确得分联系起来，并提供了一个具有最佳估计特性的相应上界。这种关系可用于可靠地量化模型校准的改进。我们在理论上和实证上展示了与我们的方法相比常用估计器的缺陷。由于正确得分的广泛适用性，这为超出分类的重新校准提供了自然延伸。

    arXiv:2203.07835v4 Announce Type: replace  Abstract: With model trustworthiness being crucial for sensitive real-world applications, practitioners are putting more and more focus on improving the uncertainty calibration of deep neural networks. Calibration errors are designed to quantify the reliability of probabilistic predictions but their estimators are usually biased and inconsistent. In this work, we introduce the framework of proper calibration errors, which relates every calibration error to a proper score and provides a respective upper bound with optimal estimation properties. This relationship can be used to reliably quantify the model calibration improvement. We theoretically and empirically demonstrate the shortcomings of commonly used estimators compared to our approach. Due to the wide applicability of proper scores, this gives a natural extension of recalibration beyond classification.
    
[^204]: 亚线性时间下的终端嵌入

    Terminal Embeddings in Sublinear Time

    [https://arxiv.org/abs/2110.08691](https://arxiv.org/abs/2110.08691)

    该论文提出了一种亚线性时间下的终端嵌入方法，可以实现扭曲$1+\epsilon$，相较于传统方法更加通用且具有广泛应用性。

    

    最近（Elkin, Filtser, Neiman 2017）引入了从一个具有一组指定终端$T \subset X$的度量空间$(X,d_X)$到另一个$(Y,d_Y)$的{\it 终端嵌入}的概念。当$X,Y$都是欧几里德度量，其中$Y$是$m$维时，最近（Narayanan, Nelson 2019）在（Mahabadi, Makarychev, Makarychev, Razenshteyn 2018）的研究基础上展示了通过这样一个终端嵌入实现扭曲$1+\epsilon$，其中$m = O(\epsilon^{-2}\log n)$，其中$n := |T|$。

    arXiv:2110.08691v3 Announce Type: replace-cross  Abstract: Recently (Elkin, Filtser, Neiman 2017) introduced the concept of a {\it terminal embedding} from one metric space $(X,d_X)$ to another $(Y,d_Y)$ with a set of designated terminals $T\subset X$. Such an embedding $f$ is said to have distortion $\rho\ge 1$ if $\rho$ is the smallest value such that there exists a constant $C>0$ satisfying   \begin{equation*}   \forall x\in T\ \forall q\in X,\ C d_X(x, q) \le d_Y(f(x), f(q)) \le C \rho d_X(x, q) .   \end{equation*}   When $X,Y$ are both Euclidean metrics with $Y$ being $m$-dimensional, recently (Narayanan, Nelson 2019), following work of (Mahabadi, Makarychev, Makarychev, Razenshteyn 2018), showed that distortion $1+\epsilon$ is achievable via such a terminal embedding with $m = O(\epsilon^{-2}\log n)$ for $n := |T|$. This generalizes the Johnson-Lindenstrauss lemma, which only preserves distances within $T$ and not to $T$ from the rest of space. The downside of prior work is that 
    
[^205]: 快速双正则自编码器用于稀疏生物数据

    Fast Dual-Regularized Autoencoder for Sparse Biological Data. (arXiv:2401.16664v1 [cs.LG])

    [http://arxiv.org/abs/2401.16664](http://arxiv.org/abs/2401.16664)

    本文提出了一种快速双正则自编码器用于稀疏生物数据的问题。该方法相对于现有最先进方法在预测药物靶点相互作用和药物疾病关联方面具有速度和准确性的优势。

    

    从稀疏数据中推断关系是一个重要的任务，应用范围从产品推荐到药物发现。最近提出的稀疏矩阵补全的线性模型在速度和准确性上相对于更复杂的推荐系统算法具有惊人的优势。在这里，我们扩展了线性模型，开发了一个浅层自编码器来解决双邻域正则化矩阵补全问题。我们证明了我们的方法在预测药物靶点相互作用和药物疾病关联方面相对于现有最先进方法的速度和准确性优势。

    Relationship inference from sparse data is an important task with applications ranging from product recommendation to drug discovery. A recently proposed linear model for sparse matrix completion has demonstrated surprising advantage in speed and accuracy over more sophisticated recommender systems algorithms. Here we extend the linear model to develop a shallow autoencoder for the dual neighborhood-regularized matrix completion problem. We demonstrate the speed and accuracy advantage of our approach over the existing state-of-the-art in predicting drug-target interactions and drug-disease associations.
    
[^206]: 高效的行政数据机器学习观察时间窗口分割

    Efficient Observation Time Window Segmentation for Administrative Data Machine Learning. (arXiv:2401.16537v1 [cs.LG])

    [http://arxiv.org/abs/2401.16537](http://arxiv.org/abs/2401.16537)

    本文研究了如何将机器学习模型的观察窗口划分为时间段，通过优化高优先级特征的时间bin大小，可以实现更简单、更快速训练的机器学习模型，并且能够达到与更复杂模型相似甚至更好的性能。

    

    利用行政数据预测结果是机器学习的一个重要应用领域，尤其在医疗保健领域。大多数行政数据记录都有时间戳，记录随时间的模式是机器学习模型的关键输入。本文探讨了如何将机器学习模型的观察窗口划分为时间段或“bins”。提出了一种计算上高效的过程，可以确定哪些数据特征最适合较小的，更高分辨率的时间段。在医疗保健和住房/无家可归的行政数据上产生的结果表明，在其他特征使用单个时间bin的情况下，优化这些高优先级特征的时间bin大小可以实现更简单，更快速训练的机器学习模型。这种方法还可以实现与更复杂的模型相似甚至更好的性能，这些模型默认将所有数据特征用相同的时间分辨率表示。

    Utilizing administrative data to predict outcomes is an important application area of machine learning, particularly in healthcare. Most administrative data records are timestamped and the pattern of records over time is a key input for machine learning models. This paper explores how best to divide the observation window of a machine learning model into time segments or "bins". A computationally efficient process is presented that identifies which data features benefit most from smaller, higher resolution time segments. Results generated on healthcare and housing/homelessness administrative data demonstrate that optimizing the time bin size of these high priority features while using a single time bin for the other features achieves machine learning models that are simpler and quicker to train. This approach also achieves similar and sometimes better performance than more complex models that default to representing all data features with the same time resolution.
    
[^207]: 基于神经网络的扩散模型中的分数估计：优化和泛化

    Neural Network-Based Score Estimation in Diffusion Models: Optimization and Generalization. (arXiv:2401.15604v1 [cs.LG])

    [http://arxiv.org/abs/2401.15604](http://arxiv.org/abs/2401.15604)

    本文提出了基于神经网络的扩散模型中分数估计的优化和泛化方法，并建立了对分数估计进行分析的数学框架。

    

    扩散模型已经成为与GANs相媲美的强大工具，可以生成具有改进保真度，灵活性和鲁棒性的高质量样本。这些模型的一个关键组成部分是通过分数匹配来学习分数函数。尽管在各种任务上取得了实证成功，但尚不清楚基于梯度的算法是否可以以可证实的准确性学习分数函数。作为回答这个问题的首要步骤，本文建立了一个数学框架，用于分析用梯度下降训练的神经网络来进行分数估计。我们的分析包括学习过程的优化和泛化方面。特别是，我们提出了一个参数化形式来将去噪分数匹配问题制定为带有噪声标签的回归问题。与标准的监督学习设置相比，分数匹配问题引入了独特的挑战，包括无界输入，向量值输出和额外的时间变量。

    Diffusion models have emerged as a powerful tool rivaling GANs in generating high-quality samples with improved fidelity, flexibility, and robustness. A key component of these models is to learn the score function through score matching. Despite empirical success on various tasks, it remains unclear whether gradient-based algorithms can learn the score function with a provable accuracy. As a first step toward answering this question, this paper establishes a mathematical framework for analyzing score estimation using neural networks trained by gradient descent. Our analysis covers both the optimization and the generalization aspects of the learning procedure. In particular, we propose a parametric form to formulate the denoising score-matching problem as a regression with noisy labels. Compared to the standard supervised learning setup, the score-matching problem introduces distinct challenges, including unbounded input, vector-valued output, and an additional time variable, preventing
    
[^208]: 使用高斯混合机制的DP-SGD抽样的DP级别限制

    Tight Group-Level DP Guarantees for DP-SGD with Sampling via Mixture of Gaussians Mechanisms. (arXiv:2401.10294v1 [cs.CR])

    [http://arxiv.org/abs/2401.10294](http://arxiv.org/abs/2401.10294)

    本研究提供了一种计算DP-SGD组级别限制的方法，并且证明了这个方法在使用泊松抽样或固定批量大小抽样时是紧密的。

    

    我们提供了一种计算DP-SGD的组级别$(\epsilon, \delta)$-DP限制的方法，当使用泊松抽样或固定批量大小抽样时。在实现中，除了离散化错误之外，通过此过程计算的DP限制是紧密的（假设我们发布了每个中间迭代）。

    We give a procedure for computing group-level $(\epsilon, \delta)$-DP guarantees for DP-SGD, when using Poisson sampling or fixed batch size sampling. Up to discretization errors in the implementation, the DP guarantees computed by this procedure are tight (assuming we release every intermediate iterate).
    
[^209]: 桥接状态和历史表示：理解自预测强化学习

    Bridging State and History Representations: Understanding Self-Predictive RL. (arXiv:2401.08898v1 [cs.LG])

    [http://arxiv.org/abs/2401.08898](http://arxiv.org/abs/2401.08898)

    本论文研究了深度强化学习中状态和历史表示间的关系，发现了这些方法和框架实际上都基于自预测抽象的共同思想，并提供了理论洞见和简化算法来学习自预测表示。

    

    表示是所有深度强化学习方法的核心，适用于马尔可夫决策过程（MDP）和部分可观察的马尔可夫决策过程（POMDP）。许多表示学习方法和理论框架被开发用于理解什么构成了有效的表示。然而，这些方法之间的关系和它们之间的共同属性仍然不清楚。在本文中，我们展示了许多看似不同的状态和历史抽象方法和框架实际上基于自预测抽象的共同思想。此外，我们提供了关于广泛采用的目标和优化（如停梯度技术）在学习自预测表示中的理论洞见。这些发现共同产生了一种简化的算法，用于学习状态和历史的自预测表示。我们通过将我们的算法应用于标准MDP、带有dist的MDP进行验证。

    Representations are at the core of all deep reinforcement learning (RL) methods for both Markov decision processes (MDPs) and partially observable Markov decision processes (POMDPs). Many representation learning methods and theoretical frameworks have been developed to understand what constitutes an effective representation. However, the relationships between these methods and the shared properties among them remain unclear. In this paper, we show that many of these seemingly distinct methods and frameworks for state and history abstractions are, in fact, based on a common idea of self-predictive abstraction. Furthermore, we provide theoretical insights into the widely adopted objectives and optimization, such as the stop-gradient technique, in learning self-predictive representations. These findings together yield a minimalist algorithm to learn self-predictive representations for states and histories. We validate our theories by applying our algorithm to standard MDPs, MDPs with dist
    
[^210]: 使用全幻灯切片图像的组织伪影分割与严重性分析的自动诊断

    Tissue Artifact Segmentation and Severity Analysis for Automated Diagnosis Using Whole Slide Images. (arXiv:2401.01386v1 [eess.IV])

    [http://arxiv.org/abs/2401.01386](http://arxiv.org/abs/2401.01386)

    这篇论文提出了一种使用全幻灯切片图像进行组织伪影分割与严重性分析的自动诊断方法。通过计算机视觉和人工智能，可以在没有人类监督的情况下对整个全幻灯切片图像进行自主分析，但受到组织伪影影响的区域需要被准确识别和排除。

    

    传统上，病理学分析和诊断是由专家在显微镜下通过观察玻璃切片标本进行手动眼球判断来完成的。全幻灯切片图像是从玻璃切片制作的数字标本。全幻灯切片图像使得标本能够在计算机屏幕上观察，并引发了计算病理学，其中利用计算机视觉和人工智能进行自动分析和诊断。借助当前的计算进展，整个全幻灯切片图像可以在没有人类监督的情况下进行自主分析。然而，如果全幻灯切片图像受到组织伪影（如组织褶皱或气泡）的影响，则分析可能会失败或导致错误的诊断，这取决于伪影的严重程度。现有的伪影检测方法依赖于专家对严重程度的评估，以消除受到伪影影响的区域进行分析。这个过程耗时、繁琐，并且有损于自动化分析或伪影去除的目标。

    Traditionally, pathological analysis and diagnosis are performed by manually eyeballing glass slide specimens under a microscope by an expert. The whole slide image is the digital specimen produced from the glass slide. Whole slide image enabled specimens to be observed on a computer screen and led to computational pathology where computer vision and artificial intelligence are utilized for automated analysis and diagnosis. With the current computational advancement, the entire whole slide image can be analyzed autonomously without human supervision. However, the analysis could fail or lead to wrong diagnosis if the whole slide image is affected by tissue artifacts such as tissue fold or air bubbles depending on the severity. Existing artifact detection methods rely on experts for severity assessment to eliminate artifact affected regions from the analysis. This process is time consuming, exhausting and undermines the goal of automated analysis or removal of artifacts without evaluatin
    
[^211]: 超越梯度和先验知识在隐私攻击中：利用联邦学习中语言模型的池化层输入

    Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning. (arXiv:2312.05720v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.05720](http://arxiv.org/abs/2312.05720)

    本文引入了一种创新的方法，在联邦学习中利用语言模型的池化层输入来实现对隐私攻击的改进。通过恢复池化层输入，这种方法能够在不同的批处理大小下提供更高的文本恢复率，从而提供更细致和有效的见解。

    

    联邦学习强调分散式训练，通过本地存储数据并仅发送模型更新，强调用户隐私。最近，一系列有关隐私攻击的工作通过从联邦学习上下文的语言模型中提取敏感的训练文本来损害用户隐私。然而，这些攻击技术面临着不同的障碍：一些工作主要使用有限的批处理大小（例如，批处理大小为1），而其他技术则容易被检测出来。本文介绍了一种创新的方法，具有难以检测的特点，在不同的批处理大小设置下显著提高了文本恢复率。基于基本的梯度匹配和领域先验知识，我们通过恢复语言模型的池化层输入来增强攻击能力，这使我们能够在特征级别提供额外的监督信号。与梯度数据不同，这些信号不会在句子和标记之间进行平均，从而提供更细致和有效的见解。

    Federated learning (FL) emphasizes decentralized training by storing data locally and sending only model updates, underlining user privacy. Recently, a line of works on privacy attacks impairs user privacy by extracting sensitive training text from language models in the context of FL. Yet, these attack techniques face distinct hurdles: some work chiefly with limited batch sizes (e.g., batch size of 1), and others are easily detectable. This paper introduces an innovative approach that is challenging to detect, significantly enhancing the recovery rate of text in various batch-size settings. Building on fundamental gradient matching and domain prior knowledge, we enhance the attack by recovering the input of the Pooler layer of language models, which enables us to provide additional supervised signals at the feature level. Unlike gradient data, these signals do not average across sentences and tokens, thereby offering more nuanced and effective insights. We benchmark our method using t
    
[^212]: 通过遗憾到置信集转换改进（多项式）逻辑回归赌博机的遗憾界限

    Improved Regret Bounds of (Multinomial) Logistic Bandits via Regret-to-Confidence-Set Conversion. (arXiv:2310.18554v1 [stat.ML])

    [http://arxiv.org/abs/2310.18554](http://arxiv.org/abs/2310.18554)

    本论文通过遗憾到置信集转换方法改进了逻辑回归赌博机的遗憾界限，提出了一个基于在线学习算法的凸置信集，并应用于具有新的鞅集中步骤的遗憾分析。

    

    逻辑回归赌博机是建模用户选择的普遍框架，例如广告推荐系统中的点击与否。我们观察到先前的工作忽视或忽略了$S \geq \lVert \theta_\star \rVert_2$中的依赖关系，其中$\theta_\star \in \mathbb{R}^d$是未知的参数向量，当$S$较大时，例如$S \geq d$，这会产生问题。在这项工作中，我们通过一种称为“遗憾到置信集转换（R2CS）”的新方法改善了对$S$的依赖关系，该方法允许我们构建一个基于在线学习算法存在性的凸置信集。使用R2CS，我们在逻辑回归赌博机的遗憾界限方面获得了严格的改进，同时保持了计算可行性和对其他因素（如$d$和$T$）的依赖。我们将我们的新置信集应用于具有新的鞅集中步骤的逻辑回归赌博机的遗憾分析，从而避免了额外的因素。

    Logistic bandit is a ubiquitous framework of modeling users' choices, e.g., click vs. no click for advertisement recommender system. We observe that the prior works overlook or neglect dependencies in $S \geq \lVert \theta_\star \rVert_2$, where $\theta_\star \in \mathbb{R}^d$ is the unknown parameter vector, which is particularly problematic when $S$ is large, e.g., $S \geq d$. In this work, we improve the dependency on $S$ via a novel approach called {\it regret-to-confidence set conversion (R2CS)}, which allows us to construct a convex confidence set based on only the \textit{existence} of an online learning algorithm with a regret guarantee. Using R2CS, we obtain a strict improvement in the regret bound w.r.t. $S$ in logistic bandits while retaining computational feasibility and the dependence on other factors such as $d$ and $T$. We apply our new confidence set to the regret analyses of logistic bandits with a new martingale concentration step that circumvents an additional factor
    
[^213]: 用于学习图神经网络的准瓦狄斯坦损失

    A Quasi-Wasserstein Loss for Learning Graph Neural Networks. (arXiv:2310.11762v1 [cs.LG])

    [http://arxiv.org/abs/2310.11762](http://arxiv.org/abs/2310.11762)

    这篇论文提出了一种新的准瓦狄斯坦损失函数，通过利用图上的最优传输来学习图神经网络，消除了现有损失函数在节点级别预测中可能存在的不一致性。

    

    当在节点级别预测任务中学习图神经网络（GNNs）时，大多数现有的损失函数是独立地应用于每个节点的，即使节点嵌入和它们的标签由于图结构的存在而不是独立同分布的。为了消除这种不一致性，本研究提出了一种新的准瓦狄斯坦（QW）损失函数，借助于在图上定义的最优传输，从而引导GNN的新学习和预测范式。特别地，我们设计了一种“准瓦狄斯坦”距离，用于观测到的多维节点标签和它们的估计之间，通过优化在图边上定义的标签传输。这些估计是由一个GNN参数化的，其中最优标签传输可以选择性地确定图边的权重。通过将标签传输的严格约束重新表达为基于Bregman散度的正则化项，我们得到了所提出的准瓦狄斯坦损失，关联两个高效求解器来学习GNN以及最优标签传输。

    When learning graph neural networks (GNNs) in node-level prediction tasks, most existing loss functions are applied for each node independently, even if node embeddings and their labels are non-i.i.d. because of their graph structures. To eliminate such inconsistency, in this study we propose a novel Quasi-Wasserstein (QW) loss with the help of the optimal transport defined on graphs, leading to new learning and prediction paradigms of GNNs. In particular, we design a "Quasi-Wasserstein" distance between the observed multi-dimensional node labels and their estimations, optimizing the label transport defined on graph edges. The estimations are parameterized by a GNN in which the optimal label transport may determine the graph edge weights optionally. By reformulating the strict constraint of the label transport to a Bregman divergence-based regularizer, we obtain the proposed Quasi-Wasserstein loss associated with two efficient solvers learning the GNN together with optimal label transp
    
[^214]: 生成熵神经最优传输在空间内外映射中的应用

    Generative Entropic Neural Optimal Transport To Map Within and Across Spaces. (arXiv:2310.09254v1 [stat.ML])

    [http://arxiv.org/abs/2310.09254](http://arxiv.org/abs/2310.09254)

    该论文介绍了生成熵神经最优传输在测度到测度映射中的应用，解决了处理非平方欧氏距离成本、确定性蒙格映射、映射跨不可比较空间和质量守恒约束等实际挑战。

    

    学习测度到测度的映射是机器学习中的一个关键任务，尤其在生成建模中占据重要地位。近年来，受最优传输理论启发的技术不断涌现。结合神经网络模型，这些方法统称为"神经最优传输"，将最优传输作为归纳偏好：这些映射应该针对给定的成本函数是最优的，能以节约的方式（通过最小化位移）在空间内或空间间移动点。这一原则在直观上是合理的，但往往面临几个实际挑战，需要调整最优传输工具箱：处理其他非平方欧氏距离成本的挑战，确定性状况下的蒙格映射公式会限制灵活性，映射在不可比较的空间中会带来多个挑战，最优传输固有的质量守恒约束可能对异常数据给予过多的重视。

    Learning measure-to-measure mappings is a crucial task in machine learning, featured prominently in generative modeling. Recent years have witnessed a surge of techniques that draw inspiration from optimal transport (OT) theory. Combined with neural network models, these methods collectively known as \textit{Neural OT} use optimal transport as an inductive bias: such mappings should be optimal w.r.t. a given cost function, in the sense that they are able to move points in a thrifty way, within (by minimizing displacements) or across spaces (by being isometric). This principle, while intuitive, is often confronted with several practical challenges that require adapting the OT toolbox: cost functions other than the squared-Euclidean cost can be challenging to handle, the deterministic formulation of Monge maps leaves little flexibility, mapping across incomparable spaces raises multiple challenges, while the mass conservation constraint inherent to OT can provide too much credit to outli
    
[^215]: Search-Adaptor: 用于信息检索的文本嵌入个性化定制

    Search-Adaptor: Text Embedding Customization for Information Retrieval. (arXiv:2310.08750v1 [cs.LG])

    [http://arxiv.org/abs/2310.08750](http://arxiv.org/abs/2310.08750)

    本文提出了一种名为Search-Adaptor的方法，用于定制化预训练的大型语言模型以改善信息检索和搜索的性能。通过修改文本嵌入，Search-Adaptor在多个真实世界数据集上展现出了稳定且显著的性能提升。

    

    由预训练的大型语言模型提取的文本嵌入具有显著的改善信息检索和搜索的潜力。除了一直以来常规使用的零样本设置外，利用相关查询-语料库配对数据的信息能力进一步提升了大型语言模型的能力。在本文中，我们提出了一种新的方法，名为Search-Adaptor，以便以高效且稳健的方式定制化预训练的大型语言模型进行信息检索。Search-Adaptor可以修改预训练的大型语言模型生成的原始文本嵌入，可以与任何大型语言模型进行集成，包括只能通过API访问的模型。在多个真实世界的英文和多语言检索数据集中，我们展示了Search-Adaptor的一致且显著的性能提升--例如，在13个BEIR数据集上，nDCG@10相对于Google Embedding APIs平均提高了5.2%以上。

    Text embeddings extracted by pre-trained Large Language Models (LLMs) have significant potential to improve information retrieval and search. Beyond the zero-shot setup in which they are being conventionally used, being able to take advantage of the information from the relevant query-corpus paired data has the power to further boost the LLM capabilities. In this paper, we propose a novel method, Search-Adaptor, for customizing LLMs for information retrieval in an efficient and robust way. Search-Adaptor modifies the original text embedding generated by pre-trained LLMs, and can be integrated with any LLM, including those only available via APIs. On multiple real-world English and multilingual retrieval datasets, we show consistent and significant performance benefits for Search-Adaptor -- e.g., more than 5.2% improvements over the Google Embedding APIs in nDCG@10 averaged over 13 BEIR datasets.
    
[^216]: GenTKG: 基于生成模型的时间知识图谱预测

    GenTKG: Generative Forecasting on Temporal Knowledge Graph. (arXiv:2310.07793v1 [cs.CL])

    [http://arxiv.org/abs/2310.07793](http://arxiv.org/abs/2310.07793)

    研究提出了一种名为GenTKG的生成模型，用于在时间知识图谱上进行预测。该模型通过结合基于时间逻辑规则的检索策略和轻量级的参数效率指导，克服了复杂的时间图数据结构和庞大的数据量所带来的挑战。

    

    大规模语言模型(LLM)的快速发展引发了对时间知识图谱(tKG)领域的兴趣，其中传统的基于嵌入和规则的模型占主导地位。目前仍然存在一个问题，即预训练的LLM是否能够理解结构化的时间关系数据，并取代它们成为时间关系预测的基础模型。因此，我们将时间知识预测引入生成模式。然而，在复杂的时间图数据结构和LLM可以处理的序列自然表达之间存在巨大的鸿沟，在tKG的庞大数据量和微调LLM的巨大计算成本之间也存在挑战。为了解决这些挑战，我们提出了一种新颖的检索增强生成框架，称为GenTKG，它在tKG上执行生成式预测，结合了基于时间逻辑规则的检索策略和轻量级的参数效率指导。通过大量实验证明了GenTKG的有效性。

    The rapid advancements in large language models (LLMs) have ignited interest in the temporal knowledge graph (tKG) domain, where conventional carefully designed embedding-based and rule-based models dominate. The question remains open of whether pre-trained LLMs can understand structured temporal relational data and replace them as the foundation model for temporal relational forecasting. Therefore, we bring temporal knowledge forecasting into the generative setting. However, challenges occur in the huge chasms between complex temporal graph data structure and sequential natural expressions LLMs can handle, and between the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs. To address these challenges, we propose a novel retrieval augmented generation framework that performs generative forecasting on tKGs named GenTKG, which combines a temporal logical rule-based retrieval strategy and lightweight parameter-efficient instruction tuning. Extensive experiments hav
    
[^217]: CacheGen：用于语言模型应用的快速上下文加载

    CacheGen: Fast Context Loading for Language Model Applications. (arXiv:2310.07240v1 [cs.NI])

    [http://arxiv.org/abs/2310.07240](http://arxiv.org/abs/2310.07240)

    CacheGen是一种用于语言模型应用的技术，通过对上下文进行压缩来减少LLM的网络获取和处理延迟。

    

    随着大型语言模型（LLM）承担越来越复杂的任务，其输入将整合更长的上下文，以应对需要领域知识或用户特定的对话历史的问题。然而，使用长上下文对于响应式的LLM系统来说是一个挑战，因为在所有上下文被获取和LLM处理之前，无法生成任何内容。现有系统仅通过优化上下文处理的计算延迟（例如，通过缓存文本上下文的中间键值特征）来解决问题，但往往会导致上下文获取的网络延迟更长（例如，键值特征消耗的带宽比文本上下文大几个数量级）。本文介绍了CacheGen，以最小化LLM上下文获取和处理的延迟。CacheGen通过将长上下文的键值（KV）特征压缩为更紧凑的比特流表示，减少了传输所需的带宽。编码器结合了自适应量化和......

    As large language models (LLMs) take on more complex tasks, their inputs incorporate longer contexts to respond to questions that require domain knowledge or user-specific conversational histories. Yet, using long contexts poses a challenge for responsive LLM systems, as nothing can be generated until all the contexts are fetched to and processed by the LLM. Existing systems optimize only the computation delay in context processing (e.g., by caching intermediate key-value features of the text context) but often cause longer network delays in context fetching (e.g., key-value features consume orders of magnitude larger bandwidth than the text context).  This paper presents CacheGen to minimize the delays in fetching and processing contexts for LLMs. CacheGen reduces the bandwidth needed for transmitting long contexts' key-value (KV) features through a novel encoder that compresses KV features into more compact bitstream representations. The encoder combines adaptive quantization with a 
    
[^218]: C(NN)FD -- 多级轴向压缩机气动性能中尖间隙变化的深度学习预测

    C(NN)FD -- deep learning predictions of tip clearance variations on multi-stage axial compressors aerodynamic performance. (arXiv:2310.04264v1 [cs.LG])

    [http://arxiv.org/abs/2310.04264](http://arxiv.org/abs/2310.04264)

    本文展示了一种用于实时预测多级轴向压缩机在燃气轮机中尖间隙变化对气动性能影响的深度学习框架，可与CFD基准相媲美的实时准确性，方便集成到燃气轮机的制造和构建过程中进行性能评估。

    

    迄今为止，将深度学习方法应用于诸如CFD（计算流体力学）等物理模拟在工业上的重要性有限。本文展示了一种用于多级轴向压缩机在燃气轮机中尖间隙变化对气动性能的实时预测的深度学习框架的开发和应用。所提出的C(NN)FD架构经证明可扩展至工业应用，并达到与CFD基准相媲美的实时准确性。部署的模型可轻松集成到燃气轮机的制造和构建过程中，从而提供了分析评估性能影响并潜在减少昂贵物理测试要求的机会。

    Application of deep learning methods to physical simulations such as CFD (Computational Fluid Dynamics), have been so far of limited industrial relevance. This paper demonstrates the development and application of a deep learning framework for real-time predictions of the impact of tip clearance variations on the aerodynamic performance of multi-stage axial compressors in gas turbines. The proposed C(NN)FD architecture is proven to be scalable to industrial applications, and achieves in real-time accuracy comparable to the CFD benchmark. The deployed model, is readily integrated within the manufacturing and build process of gas turbines, thus providing the opportunity to analytically assess the impact on performance and potentially reduce requirements for expensive physical tests.
    
[^219]: CoLiDE: 共同线性有向无环图估计

    CoLiDE: Concomitant Linear DAG Estimation. (arXiv:2310.02895v1 [cs.LG])

    [http://arxiv.org/abs/2310.02895](http://arxiv.org/abs/2310.02895)

    本论文提出了CoLiDE算法用于学习线性DAG，该算法使用了一个新的凸评分函数，结合了标度的共同估计，从而有效地将稀疏参数与外生噪声水平分离。

    

    我们处理从遵循线性结构方程模型 (SEM) 的观测数据中学习有向无环图 (DAG) 结构的组合问题。利用不可微分、非凸的有效性特征，最近的研究提出了一种连续受限优化范式，以便高效地探索DAG空间。大多数现有方法使用套索类型的评分函数来引导这个搜索过程，这些函数在$\textit{未知}$SEM噪声方差在问题实例之间发生变化时需进行昂贵的惩罚参数重新调整，并且隐含地依赖于有界同方差假设。在这项工作中，我们提出了一个新的凸评分函数，用于稀疏感知线性DAG的学习，该函数结合了标度的共同估计，从而有效地将稀疏参数与外生噪声水平分离。通过平滑的、非凸的无环惩罚项进行正则化，可以得到CoLiDE （共同线性DAG估计）。

    We deal with the combinatorial problem of learning directed acyclic graph (DAG) structure from observational data adhering to a linear structural equation model (SEM). Leveraging advances in differentiable, nonconvex characterizations of acyclicity, recent efforts have advocated a continuous constrained optimization paradigm to efficiently explore the space of DAGs. Most existing methods employ lasso-type score functions to guide this search, which (i) require expensive penalty parameter retuning when the $\textit{unknown}$ SEM noise variances change across problem instances; and (ii) implicitly rely on limiting homoscedasticity assumptions. In this work, we propose a new convex score function for sparsity-aware learning of linear DAGs, which incorporates concomitant estimation of scale and thus effectively decouples the sparsity parameter from the exogenous noise levels. Regularization via a smooth, nonconvex acyclicity penalty term yields CoLiDE ($\textbf{Co}$ncomitant $\textbf{Li}$n
    
[^220]: 一致性轨迹模型：学习扩散的概率流ODE轨迹

    Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion. (arXiv:2310.02279v1 [cs.LG])

    [http://arxiv.org/abs/2310.02279](http://arxiv.org/abs/2310.02279)

    提出了一种一致性轨迹模型（CTM），它可以加速扩散模型的采样，同时通过对抗训练和去噪得分匹配损失的组合来提高性能，并实现了最先进的采样质量。

    

    一致性模型（CM）加速基于得分的扩散模型采样，但以牺牲样本质量为代价，缺乏一种自然的方法来权衡速度和质量。为了解决这个限制，我们提出了一致性轨迹模型（CTM），它是包括CM和基于得分模型在内的泛化模型。CTM训练一个单一的神经网络，可以在单次前向传递中输出得分（即对数密度的梯度），并允许在扩散过程中任意初始和最终时间之间进行不受限制的遍历概率流普通微分方程（ODE）。CTM利用对抗训练和去噪得分匹配损失的有效组合来提高性能，并在CIFAR-10（FID 1.73）和64X64分辨率的ImageNet上实现新的最先进FID。CTM还实现了一系列新的采样方案，包括确定性和随机的ODE解中的长跳跃。

    Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion model sampling at the cost of sample quality but lack a natural way to trade-off quality for speed. To address this limitation, we propose Consistency Trajectory Model (CTM), a generalization encompassing CM and score-based models as special cases. CTM trains a single neural network that can -- in a single forward pass -- output scores (i.e., gradients of log-density) and enables unrestricted traversal between any initial and final time along the Probability Flow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables the efficient combination of adversarial training and denoising score matching loss to enhance performance and achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at 64X64 resolution (FID 2.06). CTM also enables a new family of sampling schemes, both deterministic and stochastic, involving long jumps along the ODE soluti
    
[^221]: SmartPlay: 一种用于评估LLMs作为智能Agent能力的基准

    SmartPlay : A Benchmark for LLMs as Intelligent Agents. (arXiv:2310.01557v1 [cs.LG])

    [http://arxiv.org/abs/2310.01557](http://arxiv.org/abs/2310.01557)

    SmartPlay是一个用于评估LLMs作为智能Agent能力的基准，包括6个具有不同挑战的游戏，并测试了智能LLM Agent的多种关键能力。这不仅是一个评估LLM Agent整体性能的严格测试场地，还可以分析每个能力的表现。

    

    最近的大型语言模型(LLMs)在智能Agent和下一代自动化方面展示了巨大的潜力，但目前缺乏一个系统化的基准来评估LLMs作为Agent的能力。我们介绍了SmartPlay：一个具有挑战性的基准和评估LLMs作为Agent的方法论。SmartPlay包括6个不同的游戏，包括剪刀石头布、汉诺塔、Minecraft等。每个游戏都具有独特的设置，提供最多20个评估设置和无限的环境变化。SmartPlay中的每个游戏都独特地挑战了智能LLM Agent的9个重要能力的子集，包括对对象依赖的推理、提前规划、空间推理、从历史中学习和理解随机性。每个游戏测试的能力集的区别使我们能够单独分析每个能力。SmartPlay不仅是评估LLM Agent整体性能的严格测试场地，而且也是评估Agent在不同能力方面的性能的一个重要工具。

    Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately. SmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as
    
[^222]: 一种适用于现代网络的路径范数工具包：影响、前景和挑战

    A path-norm toolkit for modern networks: consequences, promises and challenges. (arXiv:2310.01225v1 [stat.ML])

    [http://arxiv.org/abs/2310.01225](http://arxiv.org/abs/2310.01225)

    本文介绍了适用于现代神经网络的路径范数工具包，可以包括具有偏差、跳跃连接和最大池化的通用DAG ReLU网络。这个工具包恢复或超越了已知的路径范数界限，并挑战了基于路径范数的一些具体承诺。

    

    本文介绍了第一个完全能够包括具有偏差、跳跃连接和最大池化的通用DAG ReLU网络的路径范数工具包。这个工具包不仅适用于最广泛的基于路径范数的现代神经网络，还可以恢复或超越已知的此类范数的最尖锐界限。这些扩展的路径范数还享有路径范数的常规优点：计算简便、对网络的对称性具有不变性，在前馈网络上比操作符范数的乘积（另一种常用的复杂度度量）具有更好的锐度。工具包的多功能性和易于实施使我们能够通过数值评估在ImageNet上对ResNet的最尖锐界限来挑战基于路径范数的具体承诺。

    This work introduces the first toolkit around path-norms that is fully able to encompass general DAG ReLU networks with biases, skip connections and max pooling. This toolkit notably allows us to establish generalization bounds for real modern neural networks that are not only the most widely applicable path-norm based ones, but also recover or beat the sharpest known bounds of this type. These extended path-norms further enjoy the usual benefits of path-norms: ease of computation, invariance under the symmetries of the network, and improved sharpness on feedforward networks compared to the product of operators' norms, another complexity measure most commonly used.  The versatility of the toolkit and its ease of implementation allow us to challenge the concrete promises of path-norm-based generalization bounds, by numerically evaluating the sharpest known bounds for ResNets on ImageNet.
    
[^223]: DataInf：在LLMs和扩散模型中高效估计数据影响力

    DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models. (arXiv:2310.00902v1 [cs.LG])

    [http://arxiv.org/abs/2310.00902](http://arxiv.org/abs/2310.00902)

    DataInf是一种高效的影响力近似方法，特别适用于大规模生成型AI模型，相比现有方法在计算和内存效率上有明显优势。

    

    量化训练数据点的影响力对于理解机器学习模型的输出和提高AI管道的透明度至关重要。影响函数是一种原则性和流行的数据归属方法，但其计算成本使其难以使用。这个问题在大型语言模型和文本到图像模型的设置中更加突出。在这项工作中，我们提出了DataInf，一种高效的影响力近似方法，适用于大规模生成型AI模型。通过利用易于计算的闭式表达式，DataInf在计算和内存效率方面优于现有的影响计算算法。我们的理论分析表明，DataInf特别适用于诸如LoRA的参数有效微调技术。通过系统的实证评估，我们展示了DataInf能够准确地近似影响分数，并且比现有方法快几个数量级。

    Quantifying the impact of training data points is crucial for understanding the outputs of machine learning models and for improving the transparency of the AI pipeline. The influence function is a principled and popular data attribution method, but its computational cost often makes it challenging to use. This issue becomes more pronounced in the setting of large language models and text-to-image models. In this work, we propose DataInf, an efficient influence approximation method that is practical for large-scale generative AI models. Leveraging an easy-to-compute closed-form expression, DataInf outperforms existing influence computation algorithms in terms of computational and memory efficiency. Our theoretical analysis shows that DataInf is particularly well-suited for parameter-efficient fine-tuning techniques such as LoRA. Through systematic empirical evaluations, we show that DataInf accurately approximates influence scores and is orders of magnitude faster than existing methods
    
[^224]: 动态边界最大化和改进的Lipschitz正则化的认证鲁棒性

    Certified Robustness via Dynamic Margin Maximization and Improved Lipschitz Regularization. (arXiv:2310.00116v1 [cs.LG])

    [http://arxiv.org/abs/2310.00116](http://arxiv.org/abs/2310.00116)

    本文提出了一种基于动态边界最大化和改进的Lipschitz正则化的认证鲁棒性训练算法，通过增加输出空间中的边界和正则化模型的Lipschitz常数来提高深度分类器对抗性扰动的鲁棒性。

    

    为了提高深度分类器对抗性扰动的鲁棒性，已经提出了许多方法，例如设计具有更好鲁棒性性质的新架构（例如，Lipschitz-capped网络）或修改训练过程本身（例如，最小-最大优化，约束学习或正则化）。然而，这些方法对于增加输入（特征）空间中的边界可能并不有效。因此，越来越多的人开始对开发能够直接操纵输入空间中的决策边界的训练过程感兴趣。在本文中，我们在该类别的最新发展基础上，开发了一种鲁棒训练算法，其目标是在输出（logit）空间中增加边界，并沿着脆弱方向正则化模型的Lipschitz常数。我们证明这两个目标可以直接促进输入空间中更大的边界。为此，我们开发了一种可扩展的方法来计算...

    To improve the robustness of deep classifiers against adversarial perturbations, many approaches have been proposed, such as designing new architectures with better robustness properties (e.g., Lipschitz-capped networks), or modifying the training process itself (e.g., min-max optimization, constrained learning, or regularization). These approaches, however, might not be effective at increasing the margin in the input (feature) space. As a result, there has been an increasing interest in developing training procedures that can directly manipulate the decision boundary in the input space. In this paper, we build upon recent developments in this category by developing a robust training algorithm whose objective is to increase the margin in the output (logit) space while regularizing the Lipschitz constant of the model along vulnerable directions. We show that these two objectives can directly promote larger margins in the input space. To this end, we develop a scalable method for calcula
    
[^225]: 抵抗联邦学习中后门攻击的双向选举和个体视角方法

    Resisting Backdoor Attacks in Federated Learning via Bidirectional Elections and Individual Perspective. (arXiv:2309.16456v1 [cs.LG])

    [http://arxiv.org/abs/2309.16456](http://arxiv.org/abs/2309.16456)

    本文提出了Snowball，一个通过个体视角上的双向选举方法来抵抗联邦学习中后门攻击的框架。它通过自下而上和自上而下的选举过程，逐步排除感染模型，以解决由于本地数据分布多样性导致模型更新混杂分散的问题。

    

    现有的抵御联邦学习中后门攻击的方法主要通过减轻感染模型的影响或排除感染模型来实现。前者会对模型准确性产生负面影响，而后者通常依赖于对良性和感染模型更新之间的全局清晰边界的判定。然而，由于本地数据分布的多样性，模型更新在现实中容易混杂并分散。本文关注在联邦学习中排除感染模型的问题。与以往从全局视角出发的观点不同，我们提出了Snowball，一种新颖的反后门联邦学习框架，通过个体视角上的双向选举，受到我们推导出的一个原则和联邦学习和深度学习中的两个原则的启发。它具有以下特点：a）自下而上的选举，每个候选模型更新对多个对等候选模型更新进行投票，以选出一些模型更新作为聚合的被选项；b）自上而下的选举，被选项逐步增加。

    Existing approaches defend against backdoor attacks in federated learning (FL) mainly through a) mitigating the impact of infected models, or b) excluding infected models. The former negatively impacts model accuracy, while the latter usually relies on globally clear boundaries between benign and infected model updates. However, model updates are easy to be mixed and scattered throughout in reality due to the diverse distributions of local data. This work focuses on excluding infected models in FL. Unlike previous perspectives from a global view, we propose Snowball, a novel anti-backdoor FL framework through bidirectional elections from an individual perspective inspired by one principle deduced by us and two principles in FL and deep learning. It is characterized by a) bottom-up election, where each candidate model update votes to several peer ones such that a few model updates are elected as selectees for aggregation; and b) top-down election, where selectees progressively enlarge t
    
[^226]: 基于似然比的任务预测的类增量学习

    Class Incremental Learning via Likelihood Ratio Based Task Prediction. (arXiv:2309.15048v1 [cs.LG])

    [http://arxiv.org/abs/2309.15048](http://arxiv.org/abs/2309.15048)

    该论文提出了一种基于似然比的任务预测的类增量学习方法，利用离群检测器进行任务标识预测，解决了无任务标识符的测试样本的任务预测问题。

    

    类增量学习是一种具有挑战性的不断学习的设置，通过顺序学习一系列任务。每个任务由一组唯一的类组成。类增量学习的关键特点是，在测试时不提供每个测试样本的任务标识符（或任务ID）。为每个测试样本预测任务ID是一个具有挑战性的问题。一种新兴的理论上合理且有效的方法是根据任务增量学习的方法，在共享网络中为所有任务训练每个任务的任务特定模型，以处理遗忘。该方法中每个任务的模型是一个非常规分类器而不是传统分类器的离群检测器。离群检测器可以对任务内（分布内（IND））的类进行预测和识别离群数据。在推断期间，离群检测能力是每个测试样本的任务ID预测的关键。然而，本文认为使用传统的离群检测器进行任务ID预测是次优的。

    Class incremental learning (CIL) is a challenging setting of continual learning, which learns a series of tasks sequentially. Each task consists of a set of unique classes. The key feature of CIL is that no task identifier (or task-id) is provided at test time for each test sample. Predicting the task-id for each test sample is a challenging problem. An emerging theoretically justified and effective approach is to train a task-specific model for each task in a shared network for all tasks based on a task-incremental learning (TIL) method to deal with forgetting. The model for each task in this approach is an out-of-distribution (OOD) detector rather than a conventional classifier. The OOD detector can perform both within-task (in-distribution (IND)) class prediction and OOD detection. The OOD detection capability is the key for task-id prediction during inference for each test sample. However, this paper argues that using a traditional OOD detector for task-id prediction is sub-optimal
    
[^227]: 深度核学习在控制合成中的应用前景

    Promises of Deep Kernel Learning for Control Synthesis. (arXiv:2309.06569v1 [eess.SY])

    [http://arxiv.org/abs/2309.06569](http://arxiv.org/abs/2309.06569)

    深度核学习（DKL）结合了神经网络的表示能力与高斯过程的不确定性量化，为复杂动态系统的学习和控制合成提供了潜力。本研究提出了一个基于抽象的框架，使用DKL与区间马尔可夫决策过程（IMDP）实现对随机动态系统的控制合成，同时采用了高效的深度架构和正确性保证的方法。

    

    深度核学习（DKL）将神经网络的表示能力与高斯过程的不确定性量化相结合。因此，它是一个有潜力的工具，可以学习和控制复杂的动态系统。本研究开发了一个可扩展的基于抽象的框架，使得可以在复杂规范下使用DKL进行随机动态系统的控制合成。具体而言，我们考虑时态逻辑规范，并创建一个端到端的框架，使用DKL从数据中学习未知系统，并将DKL模型正式抽象成区间马尔可夫决策过程（IMDP），以进行具有正确性保证的控制合成。此外，我们还确定了一种深度架构，可以实现准确的学习和高效的抽象计算。我们通过多个基准测试来说明我们方法的有效性，包括一个5维非线性随机系统，展示了使用DKL进行控制合成可以大大优于状态-

    Deep Kernel Learning (DKL) combines the representational power of neural networks with the uncertainty quantification of Gaussian Processes. Hence, it is potentially a promising tool to learn and control complex dynamical systems. In this work, we develop a scalable abstraction-based framework that enables the use of DKL for control synthesis of stochastic dynamical systems against complex specifications. Specifically, we consider temporal logic specifications and create an end-to-end framework that uses DKL to learn an unknown system from data and formally abstracts the DKL model into an Interval Markov Decision Process (IMDP) to perform control synthesis with correctness guarantees. Furthermore, we identify a deep architecture that enables accurate learning and efficient abstraction computation. The effectiveness of our approach is illustrated on various benchmarks, including a 5-D nonlinear stochastic system, showing how control synthesis with DKL can substantially outperform state-
    
[^228]: MASA-TCN: 多锚点空间感知时间卷积神经网络用于连续和离散EEG情绪识别

    MASA-TCN: Multi-anchor Space-aware Temporal Convolutional Neural Networks for Continuous and Discrete EEG Emotion Recognition. (arXiv:2308.16207v1 [cs.LG])

    [http://arxiv.org/abs/2308.16207](http://arxiv.org/abs/2308.16207)

    MASA-TCN是一种用于连续和离散EEG情绪识别的多锚点空间感知时间卷积神经网络模型。该模型通过引入空间感知时间层来提取EEG空间模式，并能在情绪回归和分类任务中取得更好的性能。

    

    使用脑电图（EEG）进行情绪识别主要有两种情况：离散标签的分类和连续标记的回归。尽管已经提出了许多用于分类任务的算法，但对于回归任务只有少数方法。对于情绪回归，标签在时间上是连续的。一个自然的方法是学习时态动态模式。在先前的研究中，长短期记忆网络（LSTM）和时间卷积神经网络（TCN）被用来学习EEG特征向量的时间背景信息。然而，EEG的空间模式没有被有效提取出来。为了使TCN在回归和分类性能上具有更好的空间学习能力，我们提出了一种新的统一模型，名为MASA-TCN，用于EEG情绪回归和分类任务。空间感知时间层使得TCN能够从EEG电极之间的空间关系中进行额外的学习。

    Emotion recognition using electroencephalogram (EEG) mainly has two scenarios: classification of the discrete labels and regression of the continuously tagged labels. Although many algorithms were proposed for classification tasks, there are only a few methods for regression tasks. For emotion regression, the label is continuous in time. A natural method is to learn the temporal dynamic patterns. In previous studies, long short-term memory (LSTM) and temporal convolutional neural networks (TCN) were utilized to learn the temporal contextual information from feature vectors of EEG. However, the spatial patterns of EEG were not effectively extracted. To enable the spatial learning ability of TCN towards better regression and classification performances, we propose a novel unified model, named MASA-TCN, for EEG emotion regression and classification tasks. The space-aware temporal layer enables TCN to additionally learn from spatial relations among EEG electrodes. Besides, a novel multi-an
    
[^229]: 双支路深度学习网络用于糖尿病视网膜病变的检测和分级

    Dual Branch Deep Learning Network for Detection and Stage Grading of Diabetic Retinopathy. (arXiv:2308.09945v1 [eess.IV])

    [http://arxiv.org/abs/2308.09945](http://arxiv.org/abs/2308.09945)

    这项研究介绍了一种双支路深度学习网络用于检测和分级糖尿病视网膜病变，通过利用单个眼底视网膜图像进行早期诊断和成功治疗。所提出的模型利用迁移学习和预训练模型，在大型多中心数据集上进行了训练，取得了卓越的性能，优于已有文献。

    

    糖尿病视网膜病变是糖尿病的严重并发症，如果不及时治疗可能导致永久性失明。对该疾病的早期和准确诊断对于成功治疗至关重要。本文介绍了一种深度学习方法，用于利用单个眼底视网膜图像检测和分级糖尿病视网膜病变。我们的模型采用迁移学习，使用两个最先进的预训练模型作为特征提取器，并在新的数据集上进行微调。所提出的模型在包括从公开来源获得的APTOS 2019数据集在内的大型多中心数据集上进行训练。在APTOS 2019上，它在糖尿病视网膜病变的检测和分级中表现出色，优于已有的文献。在二分类中，所提出的方法的准确率达到98.50％，敏感性达到99.46％，特异性达到97.51％。在分级中，它达到了93.00％的平方加权Kappa值，准确率还是很高的。

    Diabetic retinopathy is a severe complication of diabetes that can lead to permanent blindness if not treated promptly. Early and accurate diagnosis of the disease is essential for successful treatment. This paper introduces a deep learning method for the detection and stage grading of diabetic retinopathy, using a single fundus retinal image. Our model utilizes transfer learning, employing two state-of-the-art pre-trained models as feature extractors and fine-tuning them on a new dataset. The proposed model is trained on a large multi-center dataset, including the APTOS 2019 dataset, obtained from publicly available sources. It achieves remarkable performance in diabetic retinopathy detection and stage classification on the APTOS 2019, outperforming the established literature. For binary classification, the proposed approach achieves an accuracy of 98.50%, a sensitivity of 99.46%, and a specificity of 97.51%. In stage grading, it achieves a quadratic weighted kappa of 93.00%, an accur
    
[^230]: 多任务伪标签学习在非侵入式语音质量评估模型中的应用

    Multi-Task Pseudo-Label Learning for Non-Intrusive Speech Quality Assessment Model. (arXiv:2308.09262v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2308.09262](http://arxiv.org/abs/2308.09262)

    这篇论文介绍了一种基于多任务伪标签学习的非侵入式语音质量评估模型（MTQ-Net），通过与预训练模型结合使用伪标签来进行训练，实验结果证明了其相对于从头训练模型的优势。

    

    本研究提出了一种基于多任务伪标签学习（MPL）的非侵入式语音质量评估模型MTQ-Net。MPL由两个阶段组成：从预训练模型中获取伪标签分数和执行多任务学习。评估目标是三个3QUEST指标，即语音MOS（S-MOS），噪声MOS（N-MOS）和通用MOS（G-MOS）。预训练的MOSA-Net模型被用来估计三个伪标签：主观评估语音质量（PESQ），短时客观可懂性（STOI）和语音失真指数（SDI）。然后利用多任务学习通过结合有监督损失（通过估计分数与真实标签之间的差异导出）和半监督损失（通过估计分数与伪标签之间的差异导出）来训练MTQ-Net，其中使用Huber损失函数作为损失函数。实验结果首先证明了MPL相较于从头训练模型的优势。

    This study proposes a multi-task pseudo-label learning (MPL)-based non-intrusive speech quality assessment model called MTQ-Net. MPL consists of two stages: obtaining pseudo-label scores from a pretrained model and performing multi-task learning. The 3QUEST metrics, namely Speech-MOS (S-MOS), Noise-MOS (N-MOS), and General-MOS (G-MOS), are the assessment targets. The pretrained MOSA-Net model is utilized to estimate three pseudo labels: perceptual evaluation of speech quality (PESQ), short-time objective intelligibility (STOI), and speech distortion index (SDI). Multi-task learning is then employed to train MTQ-Net by combining a supervised loss (derived from the difference between the estimated score and the ground-truth label) and a semi-supervised loss (derived from the difference between the estimated score and the pseudo label), where the Huber loss is employed as the loss function. Experimental results first demonstrate the advantages of MPL compared to training a model from scra
    
[^231]: 基于蛋白质预训练语言模型和Transformer的磷酸化位点识别方法(PTransIPs)

    PTransIPs: Identification of phosphorylation sites based on protein pretrained language model and Transformer. (arXiv:2308.05115v1 [q-bio.QM])

    [http://arxiv.org/abs/2308.05115](http://arxiv.org/abs/2308.05115)

    PTransIPs是一种新型深度学习模型，它将蛋白质序列中的氨基酸视为自然语言中的单词，并结合大型预训练蛋白质模型的嵌入。该模型通过结合卷积神经网络和Transformer模型进行训练，用于识别磷酸化位点。

    

    磷酸化是许多基础细胞过程的核心，影响着各种疾病的发生和进展。因此，磷酸化位点的识别是理解细胞和病毒感染的分子机制的重要一步，可能为新的治疗靶点提供基础。在本研究中，我们提出了一种名为PTransIPs的新型深度学习模型，用于识别磷酸化位点。PTransIPs将蛋白质序列中的氨基酸视为自然语言中的单词，并根据序列中氨基酸的类型和位置提取独特的编码。它还通过使用大型预训练蛋白质模型的嵌入作为额外的数据输入。PTransIPs进一步通过结合具有残差连接的卷积神经网络和Transformer模型，配备多头注意机制进行训练。最后，该模型通过全连接层输出分类结果。

    Phosphorylation is central to numerous fundamental cellular processes, influencing the onset and progression of a variety of diseases. Identification of phosphorylation sites is thus an important step for understanding the molecular mechanisms of cells and virus infection, which potentially leads to new therapeutic targets. In this study, we present PTransIPs, a novel deep learning model for the identification of phosphorylation sites. PTransIPs treats amino acids in protein sequences as words in natural language, extracting unique encodings based on the types along with position of amino acids in the sequence. It also incorporates embeddings from large pre-trained protein models as additional data inputs. PTransIPS is further trained on a combination model of convolutional neural network with residual connections and Transformer model equipped with multi-head attention mechanisms. At last, the model outputs classification results through a fully connected layer. The results of indepen
    
[^232]: 学习生成用于鲁棒语义分割的训练数据集

    Learning to Generate Training Datasets for Robust Semantic Segmentation. (arXiv:2308.02535v1 [cs.CV])

    [http://arxiv.org/abs/2308.02535](http://arxiv.org/abs/2308.02535)

    本文提出了一种新的方法，通过生成真实和可信的扰动或异常图像来提高语义分割技术的鲁棒性。通过设计和训练Robusta，一种鲁棒的条件生成对抗网络，可以为训练可靠的分割模型提供可用的数据集，从而显著增强语义分割技术在面对现实世界的扰动和分布变化时的鲁棒性。

    

    近年来，语义分割技术取得了显著进展，但是它们对现实世界的扰动和训练过程中未见过的数据样本的鲁棒性仍然是一个挑战，尤其是在安全关键的应用中。本文提出了一种新的方法，通过利用标签到图像生成器和图像到标签分割模型之间的协同作用来提高语义分割技术的鲁棒性。具体来说，我们设计并训练了一个新的鲁棒的条件生成对抗网络Robusta，用于生成真实和可信的扰动或异常图像，这些图像可以用来训练可靠的分割模型。我们对所提出的生成模型进行了深入研究，评估了下游分割网络的性能和鲁棒性，并证明我们的方法可以显著提高语义分割技术在面对现实世界的扰动、分布变化和超出分布的情况下的鲁棒性。

    Semantic segmentation techniques have shown significant progress in recent years, but their robustness to real-world perturbations and data samples not seen during training remains a challenge, particularly in safety-critical applications. In this paper, we propose a novel approach to improve the robustness of semantic segmentation techniques by leveraging the synergy between label-to-image generators and image-to-label segmentation models. Specifically, we design and train Robusta, a novel robust conditional generative adversarial network to generate realistic and plausible perturbed or outlier images that can be used to train reliable segmentation models. We conduct in-depth studies of the proposed generative model, assess the performance and robustness of the downstream segmentation network, and demonstrate that our approach can significantly enhance the robustness of semantic segmentation techniques in the face of real-world perturbations, distribution shifts, and out-of-distributi
    
[^233]: 大型语言模型中的上下文学习在学习标签关系上具有创新，但并非传统学习方法

    In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning. (arXiv:2307.12375v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.12375](http://arxiv.org/abs/2307.12375)

    大型语言模型（LLMs）在包含标签关系示例的上下文中的学习能力使其在下游任务中表现显著提高，但与传统学习方法不同。我们研究了上下文示例中的标签如何影响预测、预训练中学习到的标签关系如何与上下文示例相互作用以及上下文学习如何聚合标签信息。研究结果揭示了LLMs的工作机制及其对上下文信息的处理方式。

    

    在下游任务中，大型语言模型（LLMs）的性能在包含输入-标签关系示例的上下文中通常显著提高。然而，目前对LLMs的这种上下文学习（ICL）能力的工作机制尚无共识：例如，虽然Xie等人（2021年）将ICL比作一种通用学习算法，但Min等人（2022b年）认为ICL甚至不能从上下文示例中学习标签关系。在本文中，我们研究了以下三个问题：（1）上下文示例的标签如何影响预测结果，（2）预训练期间学习到的标签关系如何与上下文中提供的输入-标签示例相互作用，以及（3）ICL如何聚合来自上下文示例的标签信息。我们的研究发现，LLMs通常会整合上下文标签的信息，但预训练和上下文标签关系被区别对待，模型不会将所有上下文信息等同对待。我们的结果揭示了对LLMs的理解。

    The performance of Large Language Models (LLMs) on downstream tasks often improves significantly when including examples of the input-label relationship in the context. However, there is currently no consensus about how this in-context learning (ICL) ability of LLMs works: for example, while Xie et al. (2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022b) argue ICL does not even learn label relationships from in-context examples. In this paper, we study (1) how labels of in-context examples affect predictions, (2) how label relationships learned during pre-training interact with input-label examples provided in-context, and (3) how ICL aggregates label information across in-context examples. Our findings suggests LLMs usually incorporate information from in-context labels, but that pre-training and in-context label relationships are treated differently, and that the model does not consider all in-context information equally. Our results give insights into underst
    
[^234]: UniTabE: 面向异构表格数据的统一预训练表格编码器

    UniTabE: Pretraining a Unified Tabular Encoder for Heterogeneous Tabular Data. (arXiv:2307.09249v1 [cs.LG])

    [http://arxiv.org/abs/2307.09249](http://arxiv.org/abs/2307.09249)

    UniTabE是一种面向异构表格数据的统一预训练表格编码器，能够处理不同表格结构的挑战，并具有对多样化下游应用的适应性。

    

    自然语言处理（NLP）的最新进展明证了预训练模型的突破性影响，在各种任务上取得了令人印象深刻的结果。本研究旨在将预训练方法的威力扩展到传统被忽视的表格数据领域，该领域由于不同任务固有的众多表格模式而具有挑战性。本工作的主要研究问题围绕异构表格结构的适应性、表格数据的统一预训练协议的建立、学到的知识在任务之间的泛化和可传递性、对多样化下游应用的适应性以及随时间的增量列的纳入进行了探讨。针对这些挑战，我们引入了UniTabE，这是一种创新的方法，旨在以一致的方式处理表格，摆脱了特定表格结构强加的约束。UniTabE的核心概念是对每个基本表格进行表示

    Recent advancements in Natural Language Processing (NLP) have witnessed the groundbreaking impact of pretrained models, yielding impressive outcomes across various tasks. This study seeks to extend the power of pretraining methodologies to tabular data, a domain traditionally overlooked, yet inherently challenging due to the plethora of table schemas intrinsic to different tasks. The primary research questions underpinning this work revolve around the adaptation to heterogeneous table structures, the establishment of a universal pretraining protocol for tabular data, the generalizability and transferability of learned knowledge across tasks, the adaptation to diverse downstream applications, and the incorporation of incremental columns over time. In response to these challenges, we introduce UniTabE, a pioneering method designed to process tables in a uniform manner, devoid of constraints imposed by specific table structures. UniTabE's core concept relies on representing each basic tab
    
[^235]: 基于核方法的单细胞差异分析测试

    Kernel-Based Testing for Single-Cell Differential Analysis. (arXiv:2307.08509v1 [stat.ML])

    [http://arxiv.org/abs/2307.08509](http://arxiv.org/abs/2307.08509)

    本论文提出了一种基于核方法的单细胞差异分析测试框架，可以非线性比较复杂的细胞间分子特征分布。通过利用核嵌入的变异性，我们的方法能够揭示细胞群体中隐蔽的异质性。我们展示了核测试如何克服单细胞差异分析方法的局限性，并应用于研究分化逆转的过程。

    

    单细胞技术为我们提供了关于基因表达和表观遗传修饰等分子特征的宝贵信息。然而，以控制和强有力的方式比较这些复杂分布面临着方法论上的挑战。本文提出利用基于核嵌入的核测试框架来非线性比较细胞间复杂分子特征的分布。我们的框架不仅允许对特征进行分析，还能在考虑了它们之间复杂依赖关系的情况下进行转录组或表观组的全局比较。通过使用分类器基于核嵌入的变异性来区分细胞，我们的方法可以发现在细胞群体中原本无法察觉到的异质性。我们展示了核测试方法如何克服专门用于单细胞的差异分析方法的局限性。我们还将核测试应用于研究分化逆转的过程。

    Single-cell technologies have provided valuable insights into the distribution of molecular features, such as gene expression and epigenomic modifications. However, comparing these complex distributions in a controlled and powerful manner poses methodological challenges. Here we propose to benefit from the kernel-testing framework to compare the complex cell-wise distributions of molecular features in a non-linear manner based on their kernel embedding. Our framework not only allows for feature-wise analyses but also enables global comparisons of transcriptomes or epigenomes, considering their intricate dependencies. By using a classifier to discriminate cells based on the variability of their embedding, our method uncovers heterogeneities in cell populations that would otherwise go undetected. We show that kernel testing overcomes the limitations of differential analysis methods dedicated to single-cell. Kernel testing is applied to investigate the reversion process of differentiating
    
[^236]: VertiBench: 在垂直联邦学习基准中推进特征分布多样性

    VertiBench: Advancing Feature Distribution Diversity in Vertical Federated Learning Benchmarks. (arXiv:2307.02040v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.02040](http://arxiv.org/abs/2307.02040)

    本文引入了两个影响VFL性能的关键因素：特征重要性和特征相关性，并提出了相关的评估指标和数据集划分方法。同时，通过引入真实的VFL数据集，填补了图像-图像VFL情景中的不足。研究对于未来的VFL研究提供了有价值的见解。

    

    垂直联邦学习（VFL）是在特征划分的分布式数据上训练机器学习模型的重要范例。然而，由于隐私限制，很少有公开的真实世界VFL数据集用于算法评估，而这些数据集只代表了有限的特征分布。现有的基准通常采用从全局集合中的任意特征划分导出的合成数据集，这只捕捉到了一部分特征分布，导致算法性能评估不足。本文通过引入影响VFL性能的两个关键因素——特征重要性和特征相关性，并提出相关的评估指标和数据集划分方法，解决了这些问题。此外，我们还引入了一个真实的VFL数据集来弥补图像-图像VFL情景中的不足。我们对尖端VFL算法进行全面评估，为未来研究提供了有价值的见解。

    Vertical Federated Learning (VFL) is a crucial paradigm for training machine learning models on feature-partitioned, distributed data. However, due to privacy restrictions, few public real-world VFL datasets exist for algorithm evaluation, and these represent a limited array of feature distributions. Existing benchmarks often resort to synthetic datasets, derived from arbitrary feature splits from a global set, which only capture a subset of feature distributions, leading to inadequate algorithm performance assessment. This paper addresses these shortcomings by introducing two key factors affecting VFL performance - feature importance and feature correlation - and proposing associated evaluation metrics and dataset splitting methods. Additionally, we introduce a real VFL dataset to address the deficit in image-image VFL scenarios. Our comprehensive evaluation of cutting-edge VFL algorithms provides valuable insights for future research in the field.
    
[^237]: 无等渗性的蒙特卡洛采样：一种逆扩散方法

    Monte Carlo Sampling without Isoperimetry: A Reverse Diffusion Approach. (arXiv:2307.02037v1 [stat.ML])

    [http://arxiv.org/abs/2307.02037](http://arxiv.org/abs/2307.02037)

    本研究提出了一种无等渗性的蒙特卡洛采样方法，通过逆扩散过程实现了新颖的后验采样算法，在高维采样中表现出更优越的性能。

    

    现代生成模型的有效性通常取决于扩散路径上得分估计的精度，重点关注扩散模型及其生成高质量数据样本的能力。本研究深入探讨了通过逆扩散进行后验采样的潜力。通过对采样文献进行研究，发现可以通过转移核的分解将得分估计转化为均值估计问题。通过估计辅助分布的均值，逆扩散过程可以产生一种新颖的后验采样算法，该算法与传统的基于梯度的马尔科夫链蒙特卡洛（MCMC）方法不同。我们提供了总变差距离下的收敛分析，并证明了所提算法的等渗性依赖性相对较低，比传统的MCMC技术表现出更高的高维采样性能。

    The efficacy of modern generative models is commonly contingent upon the precision of score estimation along the diffusion path, with a focus on diffusion models and their ability to generate high-quality data samples. This study delves into the potentialities of posterior sampling through reverse diffusion. An examination of the sampling literature reveals that score estimation can be transformed into a mean estimation problem via the decomposition of the transition kernel. By estimating the mean of the auxiliary distribution, the reverse diffusion process can give rise to a novel posterior sampling algorithm, which diverges from traditional gradient-based Markov Chain Monte Carlo (MCMC) methods. We provide the convergence analysis in total variation distance and demonstrate that the isoperimetric dependency of the proposed algorithm is comparatively lower than that observed in conventional MCMC techniques, which justifies the superior performance for high dimensional sampling with er
    
[^238]: 安全关键强化学习的概率约束

    Probabilistic Constraint for Safety-Critical Reinforcement Learning. (arXiv:2306.17279v1 [cs.LG])

    [http://arxiv.org/abs/2306.17279](http://arxiv.org/abs/2306.17279)

    本文研究了概率约束下的安全关键强化学习问题，提出了具有明确梯度表达式的Safe Policy Gradient-REINFORCE（SPG-REINFORCE）算法，并通过理论界限证明了概率约束设置在最优性和安全性之间具有更好的权衡。

    

    本文考虑了概率约束强化学习中学习安全策略的问题。具体来说，安全策略或控制器是指以高概率保持代理在给定安全集合中的轨迹。我们在现有文献中频繁探索的累积约束问题和这种概率约束问题之间建立了联系。我们提供了理论界限，阐明概率约束设置在最优性和安全性（约束满足）方面具有更好的权衡。在处理概率约束时遇到的挑战，正如我们在这项工作中所探索的那样，源于没有明确的梯度表达式。我们之前的工作提供了这种明确的梯度表达式，称之为Safe Policy Gradient-REINFORCE（SPG-REINFORCE）。在这项工作中，我们提供了一个改进的梯度SPG-Actor-Critic

    In this paper, we consider the problem of learning safe policies for probabilistic-constrained reinforcement learning (RL). Specifically, a safe policy or controller is one that, with high probability, maintains the trajectory of the agent in a given safe set. We establish a connection between this probabilistic-constrained setting and the cumulative-constrained formulation that is frequently explored in the existing literature. We provide theoretical bounds elucidating that the probabilistic-constrained setting offers a better trade-off in terms of optimality and safety (constraint satisfaction). The challenge encountered when dealing with the probabilistic constraints, as explored in this work, arises from the absence of explicit expressions for their gradients. Our prior work provides such an explicit gradient expression for probabilistic constraints which we term Safe Policy Gradient-REINFORCE (SPG-REINFORCE). In this work, we provide an improved gradient SPG-Actor-Critic that lead
    
[^239]: 通过密度标志检测来识别离散化潜在坐标系统的可识别性

    Identifiability of Discretized Latent Coordinate Systems via Density Landmarks Detection. (arXiv:2306.16334v1 [cs.LG])

    [http://arxiv.org/abs/2306.16334](http://arxiv.org/abs/2306.16334)

    本文提出了一种新颖的可识别性形式，称为量化坐标可识别性。在无监督的情况下，我们展示了在高度通用的非线性映射下，可以恢复离散化的潜在坐标，而无需额外的归纳偏差。这一发现对解缠研究具有重要意义。

    

    解缠旨在仅从观察到的分布中恢复有意义的潜在真实因素。 可识别性为解缠提供了理论基础。 不幸的是，在自适应独立潜变量因子的情况下，在一般的非线性光滑因子到观测的映射下，无监督的可识别性在i.i.d.设置下是理论上不可能的。 在这项工作中，我们展示了非常惊人的是，在高度通用的非线性光滑映射（一个微分同胚）下，可以恢复离散化的潜在坐标，而不需要对映射进行任何额外的归纳偏差。 这是在假设潜在密度具有轴对齐的不连续标志的情况下，但不做因素的统计独立的不现实的假设。 我们引入了这种新颖的可识别性形式，称为量化坐标可识别性，并对恢复离散坐标进行了全面的证明。

    Disentanglement aims to recover meaningful latent ground-truth factors from only the observed distribution. Identifiability provides the theoretical grounding for disentanglement to be well-founded. Unfortunately, unsupervised identifiability of independent latent factors is a theoretically proven impossibility in the i.i.d. setting under a general nonlinear smooth map from factors to observations. In this work, we show that, remarkably, it is possible to recover discretized latent coordinates under a highly generic nonlinear smooth mapping (a diffeomorphism) without any additional inductive bias on the mapping. This is, assuming that latent density has axis-aligned discontinuity landmarks, but without making the unrealistic assumption of statistical independence of the factors. We introduce this novel form of identifiability, termed quantized coordinate identifiability, and provide a comprehensive proof of the recovery of discretized coordinates.
    
[^240]: 重新对齐影子模型能够提高白盒成员隐私攻击的效果。

    Re-aligning Shadow Models can Improve White-box Membership Inference Attacks. (arXiv:2306.05093v1 [cs.CR])

    [http://arxiv.org/abs/2306.05093](http://arxiv.org/abs/2306.05093)

    系统分析了影子模型不对齐问题的原因，并通过对抗性训练或实例加权方法重新对齐影子模型，从而提高了白盒成员隐私攻击的效果。

    

    机器学习模型已被证明泄露了有关其训练数据集的敏感信息。随着模型的日益普及，被用于设备上，自动化任务和驱动新应用，人们开始关注白盒访问模型参数，而不是仅提供对模型的查询访问的黑盒设置，这增加了攻击面。将黑盒到白盒设置的影子建模技术直接扩展到白盒设置中，通常表现不如仅进行黑盒攻击。其中一个关键原因是深度神经网络的已知特征——不对齐。本文首次对影子模型不对齐的原因进行了系统分析，并表明采用不同的权重初始化是影子模型不对齐的主要原因。其次，我们将模型融合文献中先前开发的多种重新对齐技术扩展到影子建模上下文中，目标是重新对齐......

    Machine learning models have been shown to leak sensitive information about their training datasets. As models are being increasingly used, on devices, to automate tasks and power new applications, there have been concerns that such white-box access to its parameters, as opposed to the black-box setting which only provides query access to the model, increases the attack surface. Directly extending the shadow modelling technique from the black-box to the white-box setting has been shown, in general, not to perform better than black-box only attacks. A key reason is misalignment, a known characteristic of deep neural networks. We here present the first systematic analysis of the causes of misalignment in shadow models and show the use of a different weight initialisation to be the main cause of shadow model misalignment. Second, we extend several re-alignment techniques, previously developed in the model fusion literature, to the shadow modelling context, where the goal is to re-align th
    
[^241]: 从高保真度数据中学习子网格尺度闭合形式方程：机遇与挑战

    Learning Closed-form Equations for Subgrid-scale Closures from High-fidelity Data: Promises and Challenges. (arXiv:2306.05014v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2306.05014](http://arxiv.org/abs/2306.05014)

    本文发现了非线性梯度模型（NGM），它是可解析地使用Taylor级数拓展导出的闭合形式，从而实现对地球系统复杂过程的子网格尺度（SGS）闭合/参数化。

    

    在发现地球系统复杂过程的子网格尺度（SGS）闭合/参数化的可解释性闭合形式方程上，越来越多的人表现出浓厚兴趣。本研究使用广泛的库应用通用的方程发现技术，从经过滤波的二维强迫湍流和瑞利 - 贝纳德对流的直接数值模拟中学习闭合形式。在常见的滤波器范围内，我们强有力地发现了动量和热通量的相同形式的闭合形式。这些闭合形式取决于被过滤变量（速度、温度）的梯度的非线性组合，其中的常数独立于流体/流动特性，仅依赖于过滤器类型/大小。我们表明，这些闭合形式是非线性梯度模型（NGM），可以使用Taylor级数展开分析地导出。事实上，我们认为，使用常见的（无物理信息的）方程发现算法时，无论是什么系统/物理学，发现的闭合形式始终与Taylor级数一致。

    There is growing interest in discovering interpretable, closed-form equations for subgrid-scale (SGS) closures/parameterizations of complex processes in Earth system. Here, we apply a common equation-discovery technique with expansive libraries to learn closures from filtered direct numerical simulations of 2D forced turbulence and Rayleigh-B\'enard convection (RBC). Across common filters, we robustly discover closures of the same form for momentum and heat fluxes. These closures depend on nonlinear combinations of gradients of filtered variables (velocity, temperature), with constants that are independent of the fluid/flow properties and only depend on filter type/size. We show that these closures are the nonlinear gradient model (NGM), which is derivable analytically using Taylor-series expansions. In fact, we suggest that with common (physics-free) equation-discovery algorithms, regardless of the system/physics, discovered closures are always consistent with the Taylor-series. Like 
    
[^242]: 一次性对齐、提炼和扩充所有不平衡的半监督学习

    Align, Distill, and Augment Everything All at Once for Imbalanced Semi-Supervised Learning. (arXiv:2306.04621v1 [cs.LG])

    [http://arxiv.org/abs/2306.04621](http://arxiv.org/abs/2306.04621)

    本文针对长尾半监督学习中类别不平衡的问题，提出了三个解决方案：一种灵活的分布对齐方法，一种软一致性正则化方法和一种扩充未标记集的方案。

    

    在解决长尾半监督学习中的类别不平衡问题时，需面对未标记数据和已标记数据之间边缘分布的区别，前者通常是未知的且可能与后者不同，这导致了一些重大挑战。第一个挑战是在训练过程中避免使伪标签对目标分布的偏倚，如已标记数据或平衡分布。第二个挑战是确保推理时的平衡未标记分布。为应对这些挑战，我们提出了一个多方面的解决方案：通过灵活的分布对齐，逐渐将分类器从动态估计的未标记先验分布对齐到平衡分布；利用被基于阈值的方法舍弃的低置信度伪标签的软一致性正则化；以及一种将标记部分的输入数据扩展到未标记集的方案。

    Addressing the class imbalance in long-tailed semi-supervised learning (SSL) poses a few significant challenges stemming from differences between the marginal distributions of unlabeled data and the labeled data, as the former is often unknown and potentially distinct from the latter. The first challenge is to avoid biasing the pseudo-labels towards an incorrect distribution, such as that of the labeled data or a balanced distribution, during training. However, we still wish to ensure a balanced unlabeled distribution during inference, which is the second challenge. To address both of these challenges, we propose a three-faceted solution: a flexible distribution alignment that progressively aligns the classifier from a dynamically estimated unlabeled prior towards a balanced distribution, a soft consistency regularization that exploits underconfident pseudo-labels discarded by threshold-based methods, and a schema for expanding the unlabeled set with input data from the labeled partiti
    
[^243]: 简单形式映射神经网络中的可解释性

    Explainability in Simplicial Map Neural Networks. (arXiv:2306.00010v1 [cs.LG])

    [http://arxiv.org/abs/2306.00010](http://arxiv.org/abs/2306.00010)

    本文提出了简单形式映射神经网络（SMNN）的训练过程和替代凸多面体的方法，并且首次引入了 SMNN 的可解释性能力。

    

    简单形式映射神经网络（SMNN）是基于拓扑学的神经网络，具有普适逼近能力和在适当条件下对抗性示例的鲁棒性。然而，在高维中应用 SMNN 存在一些瓶颈，首先没有定义 SMNN 的训练过程，其次对于输入数据集需要构建一个包围凸多面体。本文提出了基于给定数据集的支持子集和投影到超球面的方法作为替代凸多面体的 SMNN 训练过程，并首次引入了 SMNN 的可解释性能力。

    Simplicial map neural networks (SMNNs) are topology-based neural networks with interesting properties such as universal approximation capability and robustness to adversarial examples under appropriate conditions. However, SMNNs present some bottlenecks for their possible application in high dimensions. First, no SMNN training process has been defined so far. Second, SMNNs require the construction of a convex polytope surrounding the input dataset. In this paper, we propose a SMNN training procedure based on a support subset of the given dataset and a method based on projection to a hypersphere as a replacement for the convex polytope construction. In addition, the explainability capacity of SMNNs is also introduced for the first time in this paper.
    
[^244]: 具有信息预算的情境赌博机算法

    Contextual Bandits with Budgeted Information Reveal. (arXiv:2305.18511v1 [cs.LG])

    [http://arxiv.org/abs/2305.18511](http://arxiv.org/abs/2305.18511)

    本文介绍了一种针对医疗领域“亲治疗”操作的限制，且考虑到了操作预算的具有信息预算的情境赌博机算法，这种算法将在线原始-对偶算法和情境赌博机学习算法有机地结合在一起，取得了很好的效果。

    

    情境赌博机算法常用于推荐个性化的医疗处理方式，但在实际操作中，为保证治疗效果，医生通常需要要求患者采取没有直接好处的“亲治疗”操作，而且临床医生的操作预算有限。本文提出了一种新的优化学习算法，有效结合了两种算法方法之长：1）一个决定最佳时机与患者联系的在线原始-对偶（primal-dual）算法，2）用于向患者提供个性化治疗的情境赌博机学习算法。我们证明了该算法具有亚线性的回归界限。我们在合成和实际数据上展示了该算法的实用价值。

    Contextual bandit algorithms are commonly used in digital health to recommend personalized treatments. However, to ensure the effectiveness of the treatments, patients are often requested to take actions that have no immediate benefit to them, which we refer to as pro-treatment actions. In practice, clinicians have a limited budget to encourage patients to take these actions and collect additional information. We introduce a novel optimization and learning algorithm to address this problem. This algorithm effectively combines the strengths of two algorithmic approaches in a seamless manner, including 1) an online primal-dual algorithm for deciding the optimal timing to reach out to patients, and 2) a contextual bandit learning algorithm to deliver personalized treatment to the patient. We prove that this algorithm admits a sub-linear regret bound. We illustrate the usefulness of this algorithm on both synthetic and real-world data.
    
[^245]: 通过任意回归模型检测数值数据中的错误。

    Detecting Errors in Numerical Data via any Regression Model. (arXiv:2305.16583v1 [stat.ML])

    [http://arxiv.org/abs/2305.16583](http://arxiv.org/abs/2305.16583)

    该论文提出了一种模型不可知的方法，通过考虑各种不确定性，可以利用任何回归器检测数值数据中的异常值与自然数据波动，能够有效区分真正的异常和自然数据波动。

    

    噪声困扰着许多数值数据集，其中数据记录的值可能由于错误的传感器、数据输入/处理错误或不完美的人类估计等原因而无法匹配真实的底层值。我们考虑估计沿数值列哪些数据值是不正确的。我们提出了一种模型不可知的方法，可以利用任何回归器（即基于数据集中的其他变量来预测该列值的统计学或机器学习模型）来解决问题。通过考虑各种不确定性，我们的方法区分了真正的异常和自然数据波动，条件是有可用的数据集信息。我们为我们的方法建立了理论保证，并表明其他方法（如符合性推断）难以检测错误。我们还提供了一个新的误差检测基准，涉及 5 个具有真实世界数字错误的回归数据集（对于其中的真实值）。

    Noise plagues many numerical datasets, where the recorded values in the data may fail to match the true underlying values due to reasons including: erroneous sensors, data entry/processing mistakes, or imperfect human estimates. Here we consider estimating \emph{which} data values are incorrect along a numerical column. We present a model-agnostic approach that can utilize \emph{any} regressor (i.e.\ statistical or machine learning model) which was fit to predict values in this column based on the other variables in the dataset. By accounting for various uncertainties, our approach distinguishes between genuine anomalies and natural data fluctuations, conditioned on the available information in the dataset. We establish theoretical guarantees for our method and show that other approaches like conformal inference struggle to detect errors. We also contribute a new error detection benchmark involving 5 regression datasets with real-world numerical errors (for which the true values are al
    
[^246]: ZipIt！无需训练即可合并不同任务的模型

    ZipIt! Merging Models from Different Tasks without Training. (arXiv:2305.03053v1 [cs.CV])

    [http://arxiv.org/abs/2305.03053](http://arxiv.org/abs/2305.03053)

    本文介绍了一种无需额外训练即可合并不同任务上训练的模型的方法“ZipIt！”。

    

    典型的深度视觉识别模型能够执行它们经过训练的单一任务。本文解决将完全不同的、每个解决一个独立任务的模型合并成一个多任务模型的极其困难的问题，而且不需要任何额外的训练。以前的模型合并工作将一个模型置换到另一个模型的空间中，再将它们相加。虽然这对于在同一个任务上经过训练的模型起作用，但我们发现，这未能考虑到在不同任务上经过训练的模型之间的差异。因此，我们引入了“ZipIt！”，这是一种通用的方法，用于合并相同结构的两个任意模型，其中包括两种简单的策略。首先，为了考虑到在模型之间没有共享的特征，我们将模型合并问题扩展到还允许合并每个模型中的特征，定义一个通用的“zip”操作。其次，我们添加支持部分压缩模型的功能，直到特定层。

    Typical deep visual recognition models are capable of performing the one task they were trained on. In this paper, we tackle the extremely difficult problem of combining completely distinct models with different initializations, each solving a separate task, into one multi-task model without any additional training. Prior work in model merging permutes one model to the space of the other then adds them together. While this works for models trained on the same task, we find that this fails to account for the differences in models trained on disjoint tasks. Thus, we introduce "ZipIt!", a general method for merging two arbitrary models of the same architecture that incorporates two simple strategies. First, in order to account for features that aren't shared between models, we expand the model merging problem to additionally allow for merging features within each model by defining a general "zip" operation. Second, we add support for partially zipping the models up until a specified layer
    
[^247]: 揭示巨噬细胞吞噬作用：用于神经退行性疾病分析的可扩展和可解释的深度学习框架

    Phagocytosis Unveiled: A Scalable and Interpretable Deep learning Framework for Neurodegenerative Disease Analysis. (arXiv:2304.13764v1 [eess.IV])

    [http://arxiv.org/abs/2304.13764](http://arxiv.org/abs/2304.13764)

    本文提出了一种可扩展且可解释的深度学习框架，用于量化和分析吞噬活性以评估神经退行性疾病。流程可以处理大型数据集，包括数据质量验证和可解释的细胞分割模块。

    

    量化动态无染色细胞的吞噬作用对于评估神经退行性疾病至关重要。然而，处理时间序列相衬显微镜视频时，测量快速细胞相互作用和区分细胞与背景使得这项任务具有挑战性。在本研究中，我们引入了一种完全自动化、可扩展和多功能的实时框架，用于量化和分析吞噬活性。我们提出的流程可以处理大型数据集，包括数据质量验证模块以抵消可能的显微镜运动和帧模糊等扰动。我们还提出了一个可解释的细胞分割模块，以改善与黑匣子算法相比的深度学习方法的可解释性。这包括两个可解释的深度学习能力：视觉说明和模型简化。我们证明了深度学习中的可解释性不是高性能的对立面，而是提供必要的深度学习能力。

    Quantifying the phagocytosis of dynamic, unstained cells is essential for evaluating neurodegenerative diseases. However, measuring rapid cell interactions and distinguishing cells from backgrounds make this task challenging when processing time-lapse phase-contrast video microscopy. In this study, we introduce a fully automated, scalable, and versatile realtime framework for quantifying and analyzing phagocytic activity. Our proposed pipeline can process large data-sets and includes a data quality verification module to counteract potential perturbations such as microscope movements and frame blurring. We also propose an explainable cell segmentation module to improve the interpretability of deep learning methods compared to black-box algorithms. This includes two interpretable deep learning capabilities: visual explanation and model simplification. We demonstrate that interpretability in deep learning is not the opposite of high performance, but rather provides essential deep learnin
    
[^248]: 灵活的K最近邻分类器：推导和在基于离子迁移谱的室内定位中的应用。

    Flexible K Nearest Neighbors Classifier: Derivation and Application for Ion-mobility Spectrometry-based Indoor Localization. (arXiv:2304.10151v1 [cs.LG])

    [http://arxiv.org/abs/2304.10151](http://arxiv.org/abs/2304.10151)

    本文提出了一种新的K最近邻分类器变体，可以确保最近邻居确实接近未标记样本，并在过程中找到K值。与标准KNN相比，该算法在室内指纹定位方面具有更高的分类精度。

    

    K最近邻分类器广泛应用于指纹定位或医学等多个领域。它基于K个标记样本，即最近邻居的类成员身份，决定未标记样本的类成员身份。K的选择一直是各种研究和提出KNN变体的主题，但没有一个变体被证明优于所有其他变体。本文提出了一种新的KNN变体，确保K个最近邻居确实接近未标记样本，并在过程中找到K值。该算法在理论情景和基于离子迁移谱指纹的室内定位方面进行了测试和比较。测试结果显示，该算法在与KNN同样的计算复杂度下，可以实现比KNN更高的分类精度。

    The K Nearest Neighbors (KNN) classifier is widely used in many fields such as fingerprint-based localization or medicine. It determines the class membership of unlabelled sample based on the class memberships of the K labelled samples, the so-called nearest neighbors, that are closest to the unlabelled sample. The choice of K has been the topic of various studies and proposed KNN-variants. Yet no variant has been proven to outperform all other variants. In this paper a new KNN-variant is proposed which ensures that the K nearest neighbors are indeed close to the unlabelled sample and finds K along the way. The proposed algorithm is tested and compared to the standard KNN in theoretical scenarios and for indoor localization based on ion-mobility spectrometry fingerprints. It achieves a higher classification accuracy than the KNN in the tests, while requiring having the same computational demand.
    
[^249]: 在不使用数据增强的情况下加强正则化来防止在线深度聚类中的崩溃

    Hard Regularization to Prevent Collapse in Online Deep Clustering without Data Augmentation. (arXiv:2303.16521v1 [cs.LG])

    [http://arxiv.org/abs/2303.16521](http://arxiv.org/abs/2303.16521)

    该论文提出了一种不需要数据增强的在线深度聚类方法，通过加强正则化来避免崩溃，相比于其他方法，具有更高的稳定性。

    

    在线深度聚类是指联合使用特征提取网络和聚类模型，以将每个新数据点或批处理分配到聚类标签中。尽管比离线方法更快速和更灵活，但在线聚类很容易达到崩溃解，其中编码器将所有输入映射到同一点，并将所有输入放入单个聚类中。现有成功模型采用了各种技术来避免这个问题，其中大多数需要数据增强或旨在使数据集中每个聚类的平均软分配相同。我们提出了一种不需要数据增强的方法，与现有方法不同，它对硬分配进行了规则化。我们使用贝叶斯框架，导出一个直观的优化目标，可以直接包含在编码器网络的训练中。在四个图像数据集上进行测试，我们证明它比其他方法更加稳定地避免了崩溃。

    Online deep clustering refers to the joint use of a feature extraction network and a clustering model to assign cluster labels to each new data point or batch as it is processed. While faster and more versatile than offline methods, online clustering can easily reach the collapsed solution where the encoder maps all inputs to the same point and all are put into a single cluster. Successful existing models have employed various techniques to avoid this problem, most of which require data augmentation or which aim to make the average soft assignment across the dataset the same for each cluster. We propose a method that does not require data augmentation, and that, differently from existing methods, regularizes the hard assignments. Using a Bayesian framework, we derive an intuitive optimization objective that can be straightforwardly included in the training of the encoder network. Tested on four image datasets, we show that it consistently avoids collapse more robustly than other method
    
[^250]: 数据高效的对比自监督学习：易于学习的样本起到最大的作用。

    Data-Efficient Contrastive Self-supervised Learning: Easy Examples Contribute the Most. (arXiv:2302.09195v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09195](http://arxiv.org/abs/2302.09195)

    该研究证明了在自监督学习中容易学习的样本对学习高质量表示起到最大的作用，这有助于减少所需的训练数据量，并提高性能。

    

    自监督学习（SSL）从大量的无标签训练数据中学习高质量的表示。随着数据集变得越来越大，识别对学习此类表示最有用的示例变得至关重要。这可以通过减少学习高质量表示所需的数据量来实现有效的SSL。然而，对于SSL的价值如何量化一直是一个悬而未决的问题。在本文中，我们首次解决了这个问题，证明在期望意义下，对比SSL中对学习做出最大贡献的示例是具有最相似数据增强的示例。我们对这些子集的SSL的广义性能提供了严格的保证。实验证明，令人惊讶的是，对SSL做出最大贡献的子集是对监督学习做出最小贡献的子集。通过广泛的实验，我们证明了我们的子集在CIFAR100、CIFAR中的表现优于随机子集3%以上。

    Self-supervised learning (SSL) learns high-quality representations from large pools of unlabeled training data. As datasets grow larger, it becomes crucial to identify the examples that contribute the most to learning such representations. This enables efficient SSL by reducing the volume of data required for learning high-quality representations. Nevertheless, quantifying the value of examples for SSL has remained an open question. In this work, we address this for the first time, by proving that examples that contribute the most to contrastive SSL are those that have the most similar augmentations to other examples, in expectation. We provide rigorous guarantees for the generalization performance of SSL on such subsets. Empirically, we discover, perhaps surprisingly, the subsets that contribute the most to SSL are those that contribute the least to supervised learning. Through extensive experiments, we show that our subsets outperform random subsets by more than 3% on CIFAR100, CIFAR
    
[^251]: 异构视觉深度网络社区中的指代性沟通

    Referential communication in heterogeneous communities of pre-trained visual deep networks. (arXiv:2302.08913v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.08913](http://arxiv.org/abs/2302.08913)

    异构视觉深度网络社区中的预训练网络可以自我监督地开发出共享协议，以指代一组目标中的目标对象，并可用于沟通不同粒度的未知对象类别。

    

    随着大型预训练图像处理神经网络被嵌入自动驾驶汽车或机器人等自主代理中，一个问题出现了：在它们具有不同架构和训练方式的情况下，这些系统如何相互之间进行沟通以了解周围的世界。作为朝着这个方向的第一步，我们系统地探索了在一组异构最先进的预训练视觉网络社区中进行"指代性沟通"的任务，结果表明它们可以自我监督地发展一种共享协议来指代一组候选目标中的目标对象。在某种程度上，这种共享协议也可以用来沟通不同粒度的先前未见过的对象类别。此外，一个最初不属于现有社区的视觉网络可以轻松地学习到社区的协议。最后，我们定性和定量地研究了这种新产生的协议的属性，提供了一些证据。

    As large pre-trained image-processing neural networks are being embedded in autonomous agents such as self-driving cars or robots, the question arises of how such systems can communicate with each other about the surrounding world, despite their different architectures and training regimes. As a first step in this direction, we systematically explore the task of \textit{referential communication} in a community of heterogeneous state-of-the-art pre-trained visual networks, showing that they can develop, in a self-supervised way, a shared protocol to refer to a target object among a set of candidates. This shared protocol can also be used, to some extent, to communicate about previously unseen object categories of different granularity. Moreover, a visual network that was not initially part of an existing community can learn the community's protocol with remarkable ease. Finally, we study, both qualitatively and quantitatively, the properties of the emergent protocol, providing some evi
    
[^252]: 使用样本评估生成模型的泛化能力

    Feature Likelihood Score: Evaluating Generalization of Generative Models Using Samples. (arXiv:2302.04440v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04440](http://arxiv.org/abs/2302.04440)

    本文提出了一种新的特征似然分数（FLS）评估深度生成模型泛化能力的度量指标，用于评估生成样本的新颖性、保真度和多样性，通过实验证明其优于现有的指标，并能够检测出过拟合问题。

    

    过去几年，深度生成模型的发展取得了令人印象深刻的进展，能够生成高维、复杂和照片般逼真的数据。然而，目前评估这些模型的方法仍然不完全：标准的基于似然的指标并不总是适用于这些模型，也很少与感知保真度相关，而基于样本的指标（如FID）对过拟合不敏感。为了解决这些局限性，我们提出了一种新的度量指标，称为特征似然分数（FLS），它是一个参数化的基于样本的分数，使用密度估计来提供全面的三相评估，考虑生成样本的新颖性（即与训练样本不同）、保真度和多样性。我们通过实验证明了FLS在检测过拟合问题上的能力，先前提出的度量指标无法解决这些问题。我们还对各种图像数据集和模型类别进行了广泛的FLS评估。

    The past few years have seen impressive progress in the development of deep generative models capable of producing high-dimensional, complex, and photo-realistic data. However, current methods for evaluating such models remain incomplete: standard likelihood-based metrics do not always apply and rarely correlate with perceptual fidelity, while sample-based metrics, such as FID, are insensitive to overfitting, i.e., inability to generalize beyond the training set. To address these limitations, we propose a new metric called the Feature Likelihood Score (FLS), a parametric sample-based score that uses density estimation to provide a comprehensive trichotomic evaluation accounting for novelty (i.e., different from the training samples), fidelity, and diversity of generated samples. We empirically demonstrate the ability of FLS to identify specific overfitting problem cases, where previously proposed metrics fail. We also extensively evaluate FLS on various image datasets and model classes
    
[^253]: 在线核学习中的改进核对齐遗憾界

    Improved Kernel Alignment Regret Bound for Online Kernel Learning. (arXiv:2212.12989v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.12989](http://arxiv.org/abs/2212.12989)

    本文提出了一种新的算法，在线核学习中的新算法遗憾界和计算复杂度优于以前的结果。该算法的遗憾界和计算复杂度取决于核矩阵特征值的衰减率。

    

    本文针对Hinge损失函数下的在线核学习，改进了核对齐遗憾界。我们提出了一种算法，其遗憾界和计算复杂度优于以前的结果。如果核矩阵的特征值呈指数衰减，则我们的算法在计算复杂度为$O(\ln^2{T})$，遗憾界为$O(\sqrt{\mathcal{A}_T})$。否则，我们的算法在计算复杂度为$O(\sqrt{\mathcal{A}_TT})$，遗憾界为$O((\mathcal{A}_TT)^{\frac{1}{4}})$。我们将算法扩展到批量学习，并获得了$O(\frac{1}{T}\sqrt{\mathbb{E}[\mathcal{A}_T]})$的余量风险界，取得了改进。

    In this paper, we improve the kernel alignment regret bound for online kernel learning in the regime of the Hinge loss function. Previous algorithm achieves a regret of $O((\mathcal{A}_TT\ln{T})^{\frac{1}{4}})$ at a computational complexity (space and per-round time) of $O(\sqrt{\mathcal{A}_TT\ln{T}})$, where $\mathcal{A}_T$ is called \textit{kernel alignment}. We propose an algorithm whose regret bound and computational complexity are better than previous results. Our results depend on the decay rate of eigenvalues of the kernel matrix. If the eigenvalues of the kernel matrix decay exponentially, then our algorithm enjoys a regret of $O(\sqrt{\mathcal{A}_T})$ at a computational complexity of $O(\ln^2{T})$. Otherwise, our algorithm enjoys a regret of $O((\mathcal{A}_TT)^{\frac{1}{4}})$ at a computational complexity of $O(\sqrt{\mathcal{A}_TT})$. We extend our algorithm to batch learning and obtain a $O(\frac{1}{T}\sqrt{\mathbb{E}[\mathcal{A}_T]})$ excess risk bound which improves the p
    
[^254]: 两阶段LLM精调方法：更少特化、更多泛化

    Two-stage LLM Fine-tuning with Less Specialization and More Generalization. (arXiv:2211.00635v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.00635](http://arxiv.org/abs/2211.00635)

    预训练的大型语言模型（LLMs）通过精调可以提高特定任务的性能，但精调通常会使模型过度专门化，降低了其在上下文中的泛化学习性能。通过两阶段精调框架ProMoT可以减少这种格式特化。

    

    预训练的大型语言模型（LLM）是适用于各种任务和提示的通用问题解决方案。通过在专门的数据集上进行精调，可以进一步改进其在特定任务上的性能。然而，精调通常使模型在特定数据集上过于专门化，并降低了其在上下文中的泛化学习性能，这在需要处理没有精调数据的其他任务时是不可取的。在这项工作中，我们首先证明了单任务精调确实会降低LLM的泛化学习性能。我们发现这种遗忘的一个重要原因是格式特化，即模型过度拟合于精调任务的格式。我们进一步表明格式特化发生在精调的早期阶段。为了解决这个问题，我们提出了Prompt Tuning with MOdel Tuning (ProMoT)这一简单而有效的两阶段精调框架，可以减少格式特化。

    Pretrained large language models (LLMs) are general purpose problem solvers applicable to a diverse set of tasks with prompts. They can be further improved towards a specific task by fine-tuning on a specialized dataset. However, fine-tuning usually makes the model narrowly specialized on this dataset with reduced general in-context learning performances, which is undesirable whenever the fine-tuned model needs to handle additional tasks where no fine-tuning data is available. In this work, we first demonstrate that fine-tuning on a single task indeed decreases LLMs' general in-context learning performance. We discover one important cause of such forgetting, format specialization, where the model overfits to the format of the fine-tuned task. We further show that format specialization happens at the very beginning of fine-tuning. To solve this problem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet effective two-stage fine-tuning framework that reduces format special
    
[^255]: Meta Pattern Concern Score：一种基于人类价值观的多分类器评估新指标

    Meta Pattern Concern Score: A Novel Evaluation Measure with Human Values for Multi-classifiers. (arXiv:2209.06408v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.06408](http://arxiv.org/abs/2209.06408)

    本文提出了一种名为“Meta Pattern Concern Score”的新型评估指标，它基于概率预测的抽象表征和可调节的阈值，将人类价值观引入到多分类器中，可以用于根据人类价值观恰当地评估黑盒模型在现实中的应用效果，并且可以比较在具有不同人类价值观下使用不同分类器的同一数据集。

    

    随着先进的分类器越来越多地应用于现实中的安全关键应用，如何根据特定的人类价值观恰当地评估黑盒模型在社区中仍然是一个问题。这些人类价值包括对不同严重程度的错误案例进行惩罚，并在总体性能减少特定危险案例的情况下做出妥协。在本文中，我们提出了一种名为“Meta Pattern Concern Score”的新型评估指标，它基于概率预测的抽象表征和可调节的阈值，将人类价值观引入到多分类器中。从技术上讲，我们从两种常见指标——混淆矩阵评估指标和损失值的优势和劣势中学习，使我们的指标在通用任务下同样有效，交叉熵损失在极限情况下成为我们指标的一种特殊情况。此外，我们的指标还可以用于比较在具有不同人类价值观下使用不同分类器的同一数据集。实验结果验证了我们的指标在各种数据集和模型中的有效性，同时也为未来的研究提供了参考。

    While advanced classifiers have been increasingly used in real-world safety-critical applications, how to properly evaluate the black-box models given specific human values remains a concern in the community. Such human values include punishing error cases of different severity in varying degrees and making compromises in general performance to reduce specific dangerous cases. In this paper, we propose a novel evaluation measure named Meta Pattern Concern Score based on the abstract representation of probabilistic prediction and the adjustable threshold for the concession in prediction confidence, to introduce the human values into multi-classifiers. Technically, we learn from the advantages and disadvantages of two kinds of common metrics, namely the confusion matrix-based evaluation measures and the loss values, so that our measure is effective as them even under general tasks, and the cross entropy loss becomes a special case of our measure in the limit. Besides, our measure can als
    
[^256]: TSFool: 通过多目标黑盒攻击方法生成高度难以察觉的对循环神经网络分类器的对抗性时间序列

    TSFool: Crafting Highly-imperceptible Adversarial Time Series through Multi-objective Black-box Attack to Fool RNN Classifiers. (arXiv:2209.06388v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.06388](http://arxiv.org/abs/2209.06388)

    本文提出了一种名为TSFool的黑盒方法, 可以有效地生成针对RNN分类器的高度难以察觉的对抗性时间序列，在考虑对抗样本难以察觉性的情况下，将对抗性攻击改进为多目标优化问题来增强扰动的质量。

    

    神经网络分类器很容易受到对抗性攻击。现有的梯度攻击方法在前馈神经网络和图像识别任务中取得了最先进的性能，但它们在循环神经网络模型下的时间序列分类中表现不佳。这是因为RNN的循环结构阻止了直接的模型差分，而时间序列数据对扰动的视觉敏感性挑战了对抗性攻击的传统局部优化目标。本文提出了一种名为TSFool的黑盒方法，用于有效地生成针对RNN分类器的高度难以察觉的对抗性时间序列。我们提出了一种新的全局优化目标，称为Camouflage Coefficient，从类分布的角度考虑对抗样本的难以察觉性，并相应地将对抗性攻击改进为多目标优化问题，以增强扰动的质量。为了摆脱不同模型间的转移性，设计了一个特定于模型的回避规则。在人造数据和实际数据集上的实验结果表明，TSFool可以生成高难度攻击同时保持对抗样本的不易被检测性，并有很高的转移性。

    Neural network (NN) classifiers are vulnerable to adversarial attacks. Although the existing gradient-based attacks achieve state-of-the-art performance in feed-forward NNs and image recognition tasks, they do not perform as well on time series classification with recurrent neural network (RNN) models. This is because the cyclical structure of RNN prevents direct model differentiation and the visual sensitivity of time series data to perturbations challenges the traditional local optimization objective of the adversarial attack. In this paper, a black-box method called TSFool is proposed to efficiently craft highly-imperceptible adversarial time series for RNN classifiers. We propose a novel global optimization objective named Camouflage Coefficient to consider the imperceptibility of adversarial samples from the perspective of class distribution, and accordingly refine the adversarial attack as a multi-objective optimization problem to enhance the perturbation quality. To get rid of t
    
[^257]: 量子密度矩阵在经典问答和图像分类中的应用

    Application of Quantum Density Matrix in Classical Question Answering and Classical Image Classification. (arXiv:2203.11155v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2203.11155](http://arxiv.org/abs/2203.11155)

    该论文将量子密度矩阵应用于经典问答和图像分类中，证明了其可以提高任务的效率，尤其在图像分类中取得了优秀的性能表现。

    

    量子密度矩阵可表示整个量子系统的全部信息，将密度矩阵用于经典问答任务可以更加有效地实现问题回答。本论文设计了一种基于LSTM的新机制，以应对输入为矩阵的情况，并将该机制应用于卷积神经网络进行QA问题的求解，同时也证明了量子密度矩阵可以增强经典图像分类中的特征信息和特征之间的关系。实验结果表明，该新框架在CIFAR-10数据集上的性能优于传统的基于CNN的分类方法。

    Quantum density matrix represents all the information of the entire quantum system, and novel models of meaning employing density matrices naturally model linguistic phenomena such as hyponymy and linguistic ambiguity, among others in quantum question answering tasks. Naturally, we argue that applying the quantum density matrix into classical Question Answering (QA) tasks can show more effective performance. Specifically, we (i) design a new mechanism based on Long Short-Term Memory (LSTM) to accommodate the case when the inputs are matrixes; (ii) apply the new mechanism to QA problems with Convolutional Neural Network (CNN) and gain the LSTM-based QA model with the quantum density matrix. Experiments of our new model on TREC-QA and WIKI-QA data sets show encouraging results. Similarly, we argue that the quantum density matrix can also enhance the image feature information and the relationship between the features for the classical image classification. Thus, we (i) combine density mat
    
[^258]: 用模型构建增强随机梯度下降算法

    Bolstering Stochastic Gradient Descent with Model Building. (arXiv:2111.07058v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.07058](http://arxiv.org/abs/2111.07058)

    用基于模型构建的方法增强了随机梯度下降算法，适应性地调整步长和搜索方向，提高了收敛速度。

    

    随机梯度下降方法及其变种是解决机器学习问题的核心优化算法，能够获得良好的收敛速度。在针对特定应用进行调优时，可以进一步提高这些算法的效果。近期的研究表明，通过线搜索方法迭代调整步长，可以降低调优过程的计算成本。我们提出了一种基于前向步模型构建的随机线搜索的替代方法。这个模型构建步骤融入了二阶信息，不仅可以调整步长，还可以调整搜索方向。我们的方法将深度学习模型参数分组（张量的层），为每个参数组建立模型并计算新的步长。这种新颖的对角化方法使得选择的步长是自适应的。我们提供了收敛速度分析，

    Stochastic gradient descent method and its variants constitute the core optimization algorithms that achieve good convergence rates for solving machine learning problems. These rates are obtained especially when these algorithms are fine-tuned for the application at hand. Although this tuning process can require large computational costs, recent work has shown that these costs can be reduced by line search methods that iteratively adjust the step length. We propose an alternative approach to stochastic line search by using a new algorithm based on forward step model building. This model building step incorporates second-order information that allows adjusting not only the step length but also the search direction. Noting that deep learning model parameters come in groups (layers of tensors), our method builds its model and calculates a new step for each parameter group. This novel diagonalization approach makes the selected step lengths adaptive. We provide convergence rate analysis, a
    

