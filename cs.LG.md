# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models.](http://arxiv.org/abs/2307.06949) | HyperDreamBooth是一个超网络，可以从一个人的单张图片中快速生成个性化权重，从而实现在多种背景和风格下合成一个人的面部，保持高保真度并同时保留对多样化风格和语义修改的关键知识。 |
| [^2] | [In-context Autoencoder for Context Compression in a Large Language Model.](http://arxiv.org/abs/2307.06945) | 在大型语言模型中，我们提出了一种称为In-context Autoencoder (ICAE)的上下文自编码器，它通过将长上下文压缩为有限数量的内存槽，实现了$4\times$的上下文压缩，并能够根据内存槽进行条件处理以响应各种提示。 |
| [^3] | [On the Connection between Game-Theoretic Feature Attributions and Counterfactual Explanations.](http://arxiv.org/abs/2307.06941) | 本研究通过建立理论联系，将博弈论特征归因和反事实解释相结合。通过变换和证明，在一定条件下它们是等价的。研究结果还指出了仅仅使用反事实解释来提供特征重要性的局限性。 |
| [^4] | [FDAPT: Federated Domain-adaptive Pre-training for Language Models.](http://arxiv.org/abs/2307.06933) | FDAPT是一种联邦领域自适应预训练的方法，在保护数据隐私的同时，能够通过利用敏感和分布式数据来增强模型适应能力。对于IID和非IID情况下的下游任务，FDAPT能够维持与中央基线相竞争的性能。提出的FFDAPT算法进一步提高了计算效率，并展现出与标准FDAPT类似的下游任务性能。此外，我们也确定了这个新研究领域的有希望的未来研究方向。 |
| [^5] | [Domain-Agnostic Tuning-Encoder for Fast Personalization of Text-To-Image Models.](http://arxiv.org/abs/2307.06925) | 提出了一种面向领域通用的方法，在文本到图像个性化中有效地使用编码器技术，同时避免了对特定数据集或先前信息的依赖。引入了对比度正则化技术，以保持高保真度和可编辑性，并通过将预测的标记推向最近的现有标记来实现这一目标。 |
| [^6] | [DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual Language Grounding.](http://arxiv.org/abs/2307.06924) | DRAGON是一种基于对话的导航机器人，能够理解用户的指令并通过语言与用户沟通，为视力受损者提供导航和环境描述的帮助。 |
| [^7] | [Weighted Averaged Stochastic Gradient Descent: Asymptotic Normality and Optimality.](http://arxiv.org/abs/2307.06915) | 本文探索了一种加权平均随机梯度下降（SGD）方案，并建立了渐近正态性，提供了渐近有效的在线推理方法。此外，我们提出了一种自适应平均方案，具有最优的统计速度和有利的非渐近收敛性。 |
| [^8] | [Uncovering Unique Concept Vectors through Latent Space Decomposition.](http://arxiv.org/abs/2307.06913) | 通过潜在空间分解和无监督聚类，我们提出了一种自动揭示深度学习模型学习到的概念向量的方法，这些概念向量与模型预测相关且具有语义的独特概念，并且在实验中表明这些概念对人类来说易于理解和与任务相关。 |
| [^9] | [Provable Multi-Task Representation Learning by Two-Layer ReLU Neural Networks.](http://arxiv.org/abs/2307.06887) | 通过双层ReLU神经网络，本论文提出了一种可证明的多任务表示学习方法，用于解决神经网络在实践中同时训练多个任务时遇到的问题。 |
| [^10] | [Min-Max Optimization under Delays.](http://arxiv.org/abs/2307.06886) | 在大规模机器学习中，研究了min-max优化在延迟下的性能。对于简单实例，即使是小的延迟也可能导致Extra-gradient算法发散，因此需要对延迟版本的min-max优化算法进行仔细分析。为此，我们证明了在适当的技术假设下，梯度下降-上升算法在延迟情况下的收敛性和性能。 |
| [^11] | [The complexity of non-stationary reinforcement learning.](http://arxiv.org/abs/2307.06877) | 强化学习中的非平稳学习是一个重要挑战，我们证明了在修改概率或奖励时需要花费大量的时间来保持值函数的最新状态，并且这个挑战与状态数目密切相关。 |
| [^12] | [Identifying Early Help Referrals For Local Authorities With Machine Learning And Bias Analysis.](http://arxiv.org/abs/2307.06871) | 本文研究了利用机器学习和偏见分析来帮助地方政府识别有可能需要早期援助的家庭。在构建模型时，虽然模型展示了识别年轻人需要干预的能力，但在不平衡数据下也产生了很多错误的正例。 |
| [^13] | [Embodied Lifelong Learning for Task and Motion Planning.](http://arxiv.org/abs/2307.06870) | 这篇论文研究了在家中长期部署的机器人所面临的具身化终身学习问题，在任务和运动规划的背景下采用了新颖的问题建模方法。利用TAMP系统的模块化特性，提出了一个生成混合模型来产生规划器的候选参数。通过学习共享和非共享的模型，并根据代理任务来在线选择使用的模型，该方法在模拟的2D领域和BEHAVIOR基准测试中取得了显著的规划成功改进。 |
| [^14] | [AnuraSet: A dataset for benchmarking Neotropical anuran calls identification in passive acoustic monitoring.](http://arxiv.org/abs/2307.06860) | AnuraSet是一个用于对新热带蛙类叫声进行识别的基准测试数据集，该数据集包括27小时的专家注释的记录，提供了基准模型以及用于解决蛙类叫声识别问题的挑战。 |
| [^15] | [Self-consistency for open-ended generations.](http://arxiv.org/abs/2307.06857) | 本论文提出了一种改进大规模预训练语言模型生成输出质量和一致性的新方法，通过扩展自洽性框架的适用性，实现了从一个候选集中恢复最优或接近最优的生成结果，并提出了一种轻量级无参数相似性函数来改进代码生成、自动形式化和摘要任务的效果。 |
| [^16] | [Data Augmentation in Training CNNs: Injecting Noise to Images.](http://arxiv.org/abs/2307.06855) | 这项研究分析了将不同噪声模型以不同幅度添加到CNN架构中的效果，并提出了一些噪声注入的新启发和建议，有助于优化图像分类的学习过程。 |
| [^17] | [Tensor Completion via Leverage Sampling and Tensor QR Decomposition for Network Latency Estimation.](http://arxiv.org/abs/2307.06848) | 本文提出了一种通过改进张量采样策略和引入张量QR分解来加快网络延迟估计问题的张量完成方法。 |
| [^18] | [Federated Multi-Agent Deep Reinforcement Learning for Dynamic and Flexible 3D Operation of 5G Multi-MAP Networks.](http://arxiv.org/abs/2307.06842) | 本文提出了一个联邦多智能体深度强化学习的架构，用于动态和灵活地操作5G多MAP网络。在高层决策中，通过共识确定MAPs的数量并考虑网络自管理的相互依赖关系。在低层，使用一个双注意力的DRL模型来管理MAPs的位置，同时提出了一个联邦机制来训练和共享每个MAP的位置模型。最后，通过多目标奖励函数共同优化MAPs的位置和回传连接。 |
| [^19] | [Ensemble learning for blending gridded satellite and gauge-measured precipitation data.](http://arxiv.org/abs/2307.06840) | 本研究填补论文领域中的空白，提出了11个新的集成学习算法并对其进行了广泛的比较，旨在改进卫星降水产品的准确性。 |
| [^20] | [PC-Droid: Faster diffusion and improved quality for particle cloud generation.](http://arxiv.org/abs/2307.06836) | PC-Droid是一个新的扩散模型，通过利用新的扩散公式和研究更近期的积分求解器，同时对所有类型的喷注进行训练，实现了最先进的性能。它不仅能提供更快的生成速度，而且在所有评估指标上都具有卓越的性能。 |
| [^21] | [Enhancing Reliability in Federated mmWave Networks: A Practical and Scalable Solution using Radar-Aided Dynamic Blockage Recognition.](http://arxiv.org/abs/2307.06834) | 本文提出了一种利用雷达辅助动态阻挡识别的方法来提高动态户外环境中毫米波和太赫兹网络服务的可靠性。通过联合学习和神经网络模型，可以同时预测阻挡状态和时间，并通过主动切换或波束切换减少延迟，以确保高质量的用户体验。 |
| [^22] | [A Novel Bayes' Theorem for Upper Probabilities.](http://arxiv.org/abs/2307.06831) | 本文推广了瓦塞尔曼和卡代纳的结果，给出了一种新的贝叶斯定理用于处理与似然函数相关的不确定性。该结果对于工程、机器学习和人工智能领域具有潜在应用价值。 |
| [^23] | [A Causal Framework to Unify Common Domain Generalization Approaches.](http://arxiv.org/abs/2307.06825) | 本文提出了一个因果框架用于统一常见领域泛化方法的理解，通过回答关键思想、理论基础和方法关系等问题，帮助研究者更好地理解和发展领域泛化方法。 |
| [^24] | [CLAIMED -- the open source framework for building coarse-grained operators for accelerated discovery in science.](http://arxiv.org/abs/2307.06824) | CLAIMED是一个开放源代码框架，用于在现代数据驱动的科学中构建可重用的运算符和可扩展的科学工作流程，从而解决了重复性和可重用性问题。 |
| [^25] | [TinyMetaFed: Efficient Federated Meta-Learning for TinyML.](http://arxiv.org/abs/2307.06822) | TinyMetaFed是一个适用于TinyML的高效联邦元学习框架，通过协同训练神经网络初始化，在小型设备上能够快速微调，同时实现通信节省和隐私保护。 |
| [^26] | [Equalization in Dispersion-Managed Systems Using Learned Digital Back-Propagation.](http://arxiv.org/abs/2307.06821) | 本文研究了使用学习数字后向传播（LDBP）来均衡双极化光纤传输中的色散管理（DM）链路。实验结果表明，在单信道和WDM传输中，LDBP相比于线性均衡和DBP分别实现了显著的信噪比改善和Q因子增益。此外，频域实现LDBP和DBP在复杂性上更具优势。 |
| [^27] | [Data-driven Nonlinear Parametric Model Order Reduction Framework using Deep Hierarchical Variational Autoencoder.](http://arxiv.org/abs/2307.06816) | 提出了一种使用深度分层变分自编码器的数据驱动非线性参数模型降阶方法，能够提高准确性和稳定性，并在实际系统上进行了验证和评估。 |
| [^28] | [Fast and Functional Structured Data Generators Rooted in Out-of-Equilibrium Physics.](http://arxiv.org/abs/2307.06797) | 这项研究提出了一种基于非平衡物理学的训练算法，用于解决使用能量模型生成高质量结构化数据的挑战。该方法通过改善模型的分类能力和生成速度，在多个领域取得了成功应用。 |
| [^29] | [Robotic surface exploration with vision and tactile sensing for cracks detection and characterisation.](http://arxiv.org/abs/2307.06784) | 本文介绍了一种利用光纤的视觉和触觉传感进行裂缝定位和检测的算法，并提出了一个用于裂缝探测的机器人运动规划器和触觉数据分类方法。 |
| [^30] | [A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media.](http://arxiv.org/abs/2307.06775) | 本研究创建了一个多模态深度学习模型，将文本和视觉数据相结合，能够准确识别社交媒体上的促进饮食紊乱的内容。最有效的模型是RoBERTa自然语言处理模型和MaxViT图像分类模型的融合模型，准确率和F1分数分别达到95.9%和0.959。 |
| [^31] | [Generalizing Supervised Deep Learning MRI Reconstruction to Multiple and Unseen Contrasts using Meta-Learning Hypernetworks.](http://arxiv.org/abs/2307.06771) | 本文提出了一种名为KM-MAML的多模态元学习模型，通过演化能力和超网络生成模态特定的权重，以用于多种和未知对比度的图像重建。 |
| [^32] | [Privacy-Utility Trade-offs in Neural Networks for Medical Population Graphs: Insights from Differential Privacy and Graph Structure.](http://arxiv.org/abs/2307.06760) | 本文通过对医疗领域的人口图进行差分隐私图神经网络的实证研究，调查了隐私和效用之间的权衡，并通过成员推理攻击进行审核。研究结果表明这一特定差分隐私应用领域具有潜力和挑战，并发现图的同质性程度与训练模型的准确性之间存在相关性。 |
| [^33] | [Cramer Type Distances for Learning Gaussian Mixture Models by Gradient Descent.](http://arxiv.org/abs/2307.06753) | 本文提出了一种用于学习高斯混合模型的Cramer距离，该距离函数在多元情况下具有闭式表达式，并且易于计算和实现，并在梯度下降算法中有效。 |
| [^34] | [Vehicle Dispatching and Routing of On-Demand Intercity Ride-Pooling Services: A Multi-Agent Hierarchical Reinforcement Learning Approach.](http://arxiv.org/abs/2307.06742) | 本研究提出了一个基于多代理层次强化学习的框架，用于即时城际拼车服务的车辆调度和路径规划。数值研究证明该框架有效缓解了供给不足问题。 |
| [^35] | [MPR-Net:Multi-Scale Pattern Reproduction Guided Universality Time Series Interpretable Forecasting.](http://arxiv.org/abs/2307.06736) | 本文提出了一种名为MPR-Net的预测模型，通过自适应地分解多尺度历史序列模式，并基于模式重现的先验知识构建模式扩展预测方法，最后使用反卷积运算将未来模式重建为未来序列。MPR-Net能够有效地捕捉时间序列中的重要模式并实现可解释的预测。 |
| [^36] | [Breaking 3-Factor Approximation for Correlation Clustering in Polylogarithmic Rounds.](http://arxiv.org/abs/2307.06723) | 本文提出了第一个具有更好逼近比的对数级别深度并行算法来解决关联聚类问题。 |
| [^37] | [Why Guided Dialog Policy Learning performs well? Understanding the role of adversarial learning and its alternative.](http://arxiv.org/abs/2307.06721) | 本论文通过分析对话策略和奖励估计器的目标函数，解释了对抗学习在对话策略学习中的作用，并提出了一种替代方法。 |
| [^38] | [Unsupervised Calibration through Prior Adaptation for Text Classification using Large Language Models.](http://arxiv.org/abs/2307.06713) | 本文提出了一种使用大型语言模型进行文本分类的无监督校准方法，通过调整先验类别分布，实现在没有标记样本和仅少量领域内样本查询的情况下执行任务。 |
| [^39] | [GRAN is superior to GraphRNN: node orderings, kernel- and graph embeddings-based metrics for graph generators.](http://arxiv.org/abs/2307.06709) | 本论文通过比较基于节点排序、核函数和图嵌入的度量，展示了GRAN优于GraphRNN的优势，并为小型图提供了有效的GraphRNN改进方法。此外，论文还提供了关于数据集选择和节点特征初始化的最佳实践指南。 |
| [^40] | [S-HR-VQVAE: Sequential Hierarchical Residual Learning Vector Quantized Variational Autoencoder for Video Prediction.](http://arxiv.org/abs/2307.06701) | S-HR-VQVAE是一种序列分层残差学习向量量化变分自编码器，通过结合分层残差向量量化变分自编码器（HR-VQVAE）和时空PixelCNN（ST-PixelCNN）的能力，解决了视频预测中的主要挑战，并在KTH人体动作和Moving-MNIST任务上取得了较好的实验结果。 |
| [^41] | [IntelliGraphs: Datasets for Benchmarking Knowledge Graph Generation.](http://arxiv.org/abs/2307.06698) | IntelliGraphs是一组新的知识图谱数据集，用于评估知识图谱生成。其中包含具有逻辑规则表达的语义的子图，用于评估子图推断的模型。 |
| [^42] | [Ageing Analysis of Embedded SRAM on a Large-Scale Testbed Using Machine Learning.](http://arxiv.org/abs/2307.06693) | 使用机器学习在大规模测试平台上进行嵌入式SRAM老化分析，发现通过SRAM初始化偏差和各种特征提取指标可以准确估计嵌入式设备的使用时间。 |
| [^43] | [Aeolus Ocean -- A simulation environment for the autonomous COLREG-compliant navigation of Unmanned Surface Vehicles using Deep Reinforcement Learning and Maritime Object Detection.](http://arxiv.org/abs/2307.06688) | 本文提出了Aeolus Ocean，这是一个用于无人水面航行器自主COLREG合规导航的仿真环境。通过在虚拟环境中复制真实的操作条件，可以为DRL和CV算法的开发提供基础。 |
| [^44] | [Multivariate Time Series characterization and forecasting of VoIP traffic in real mobile networks.](http://arxiv.org/abs/2307.06645) | 本论文旨在通过多变量时间序列分析，预测实时移动网络中VoIP流量的关键QoS/QoE描述符的行为，以帮助运营商优化网络规划和资源分配。 |
| [^45] | [An Improved Uniform Convergence Bound with Fat-Shattering Dimension.](http://arxiv.org/abs/2307.06644) | 本文提出了一个改进的均匀收敛界限，填补了现有状态-of-the-art上界与下界之间的空缺。 |
| [^46] | [Discovering How Agents Learn Using Few Data.](http://arxiv.org/abs/2307.06640) | 本研究提出了一个在少量数据上识别智能体学习动力学的算法框架。通过使用多项式回归和引入捕捉智能体行为的侧信息约束，该方法可以在使用单个轨迹的短暂运行中仅仅5个样本的情况下准确恢复真实动力学。 |
| [^47] | [Frameless Graph Knowledge Distillation.](http://arxiv.org/abs/2307.06631) | 这里是针对图知识蒸馏的无框架KD框架，通过充分利用图框架分解提供的多尺度图知识，学生模型能够适应不同类型的图，并具有缓解的潜力。 |
| [^48] | [Quantum Autoencoders for Learning Quantum Channel Codes.](http://arxiv.org/abs/2307.06622) | 本研究使用量子机器学习技术研究了经典和量子通信中的量子通道编码，并展示了其在不同通道模型下的强大性能，为推进量子通信系统研究提供了潜力，能够更好地理解通信容量界限。 |
| [^49] | [Online Distributed Learning with Quantized Finite-Time Coordination.](http://arxiv.org/abs/2307.06620) | 本文研究了一种在线分布式学习问题，提出了一种依靠量化、有限时间协作协议的分布式算法来聚合本地训练的模型，并允许使用随机梯度来提高效率和可扩展性。 |
| [^50] | [Learning IMM Filter Parameters from Measurements using Gradient Descent.](http://arxiv.org/abs/2307.06618) | 本文提出了一种使用梯度下降从测量中学习IMM滤波器参数的方法，通过仅使用测量数据即可优化滤波器的参数。在模拟数据上的实验结果表明，该方法能够达到使用真值参数化的滤波器的性能水平。 |
| [^51] | [Introducing Foundation Models as Surrogate Models: Advancing Towards More Practical Adversarial Attacks.](http://arxiv.org/abs/2307.06608) | 本文将对抗攻击重新设定为下游任务，通过生成图像噪声来满足新兴趋势，并将基础模型引入作为代理模型。虽然基础模型的表现不佳，但通过在特征空间中进行分析，我们发现缺乏对应的特征。 |
| [^52] | [Deep Neural Networks for Semiparametric Frailty Models via H-likelihood.](http://arxiv.org/abs/2307.06581) | 本文提出了一种新的基于深度神经网络的脆弱性模型，并使用H-似然法进行训练和预测。实验结果表明该方法提高了预测性能，特别是在包含个体特定脆弱性的情况下。 |
| [^53] | [Efficient SGD Neural Network Training via Sublinear Activated Neuron Identification.](http://arxiv.org/abs/2307.06565) | 通过亚线性激活神经元识别实现高效的SGD神经网络训练，算法收敛时间复杂度为$O(M^2/\epsilon^2)$。 |
| [^54] | [Prescriptive Process Monitoring Under Resource Constraints: A Reinforcement Learning Approach.](http://arxiv.org/abs/2307.06564) | 本论文提出了一种在资源限制下进行处方过程监控的强化学习方法。通过考虑对干预需求、及时性或效果预测的不确定性和资源利用水平，来触发干预，从而优化业务过程的性能。 |
| [^55] | [Metal Oxide-based Gas Sensor Array for the VOCs Analysis in Complex Mixtures using Machine Learning.](http://arxiv.org/abs/2307.06556) | 本研究提出了一种基于金属氧化物传感器阵列和机器学习的方法，可以在复杂混合物中识别出不同的挥发性有机化合物（VOCs），且取得了很高的准确率和回归分析结果。 |
| [^56] | [Deep Network Approximation: Beyond ReLU to Diverse Activation Functions.](http://arxiv.org/abs/2307.06555) | 本文研究了深度神经网络在多种激活函数下的表达能力，证明了可以通过在有界集合上构建一个宽度为6N、深度为2L的varrho激活网络来逼近一个宽度为N、深度为L的ReLU网络，从而将对ReLU网络的逼近结果推广到其他激活函数。 |
| [^57] | [Full-resolution Lung Nodule Segmentation from Chest X-ray Images using Residual Encoder-Decoder Networks.](http://arxiv.org/abs/2307.06547) | 本研究使用高效的编码-解码神经网络处理全分辨率图像，实现从胸部X射线图像中定位肺结节的目标，避免了降采样导致的信号丢失，并通过自动化框架对其进行了评估。 |
| [^58] | [On the Effective Horizon of Inverse Reinforcement Learning.](http://arxiv.org/abs/2307.06541) | 本研究分析了逆强化学习中时间视野的重要性，发现短于实际值的有效时间视野可以更快且更准确地估计奖励函数，减轻过拟合问题。此外，研究还呼吁在IRL中同时学习奖励和有效时间视野。 |
| [^59] | [Convolutional Neural Networks for Sentiment Analysis on Weibo Data: A Natural Language Processing Approach.](http://arxiv.org/abs/2307.06540) | 本研究提出了一种基于卷积神经网络的方法，利用微博数据进行情感分析，取得了约0.73的宏平均F1分数，结果表明了CNN在情感分析任务中的有效性，对社交媒体分析、市场研究和政策研究等领域有重要意义。 |
| [^60] | [Tensor Decompositions Meet Control Theory: Learning General Mixtures of Linear Dynamical Systems.](http://arxiv.org/abs/2307.06538) | 本论文以张量分解方法为基础，提出了学习线性动力系统混合模型的新方法。算法成功地应用于没有组件分离条件的情况，并可以与贝叶斯最优聚类竞争。此外，算法可以在部分观测设置下工作。 |
| [^61] | [DSV: An Alignment Validation Loss for Self-supervised Outlier Model Selection.](http://arxiv.org/abs/2307.06534) | DSV提出了一种无监督的验证损失函数用于选择有效的自监督离群点模型，通过捕捉数据增强函数与异常生成机制之间的对齐性，提高了检测准确性。 |
| [^62] | [Artificial Intelligence for Drug Discovery: Are We There Yet?.](http://arxiv.org/abs/2307.06521) | 本综述讨论了人工智能在药物发现中的应用，重点关注小分子药物。通过使用生成化学、机器学习和多属性优化等人工智能技术，已有多种化合物进入了临床试验。科学界必须仔细审查已知信息，以解决可重复性危机。只有具有足够的真实基准和适当的训练数据，才能实现人工智能在药物发现中的全部潜力。 |
| [^63] | [Machine Learning practices and infrastructures.](http://arxiv.org/abs/2307.06518) | 本文研究了机器学习实践中从业者与工具的互动，以及这些互动对于机器学习实践和系统开发的影响。通过实证研究，发现交互式计算平台在学习和协调实践中起到了重要的基础设施作用。 |
| [^64] | [Leveraging Contextual Counterfactuals Toward Belief Calibration.](http://arxiv.org/abs/2307.06513) | 本文讨论了通过引入上下文反事实推理来准确校准AI系统中的信念的问题。研究发现，在高后悔情况下，上下文反事实和补救成本对于更新决策者的信念以及所持信念的强度至关重要。通过将信念的多样性分成两类:主观性和认识不确定性，可以更好地理解和处理信念的校准问题。 |
| [^65] | [Hybrid Control Policy for Artificial Pancreas via Ensemble Deep Reinforcement Learning.](http://arxiv.org/abs/2307.06501) | 本研究提出了一种名为HyCPAP的混合控制策略，通过结合模型预测控制和集成深度强化学习，并充分利用它们各自的优势，以解决人工胰腺的复杂生理过程、延迟胰岛素反应和不准确血糖测量等挑战。 |
| [^66] | [Microbial Genetic Algorithm-based Black-box Attack against Interpretable Deep Learning Systems.](http://arxiv.org/abs/2307.06496) | 这项工作提出了一种基于微生物遗传算法的黑箱攻击方法，QuScore，对抗对解释模型进行耦合的可解释深度学习系统(IDLSes)。该方法能够在不了解目标模型的情况下，通过转移和评分方法减少查询次数，实现成功的攻击。 |
| [^67] | [Misclassification in Automated Content Analysis Causes Bias in Regression. Can We Fix It? Yes We Can!.](http://arxiv.org/abs/2307.06483) | 传播学领域中的自动化内容分析常忽视了错误分类的偏差，我们介绍并测试了统计方法来纠正这种偏差，并设计了一种新方法来修复之。 |
| [^68] | [Tackling Combinatorial Distribution Shift: A Matrix Completion Perspective.](http://arxiv.org/abs/2307.06457) | 该论文研究了组合分布偏移的问题，提出了基于矩阵补全的解决方法。通过在特殊情况下的双线性嵌入，实现对训练中未涵盖的测试分布进行外推。这个设置将缺失非随机数据的矩阵补全问题广义化。 |
| [^69] | [Stochastic Delay Differential Games: Financial Modeling and Machine Learning Algorithms.](http://arxiv.org/abs/2307.06450) | 本文提出了一种使用深度学习方法找到随机时滞微分博弈闭环纳什均衡的数值方法，通过循环神经网络对每个玩家的控制进行参数化并使用修改后的布朗虚拟博弈结合深度学习技术进行训练。 |
| [^70] | [On Collaboration in Distributed Parameter Estimation with Resource Constraints.](http://arxiv.org/abs/2307.06442) | 在资源约束下的分布参数估计中，我们研究了传感器/代理数据收集和协作策略，通过最大化费舍尔信息或最小化Cramer-Rao界来解决传感器/代理的数据收集和协作策略设计问题。 |
| [^71] | [No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models.](http://arxiv.org/abs/2307.06440) | 本论文重新审视了基于Transformer的语言模型的高效训练算法，包括动态架构，批量选择和高效优化器。然而，在使用这些算法预训练时，相对于基线方法，它们的训练、验证和下游收益消失了。同时，论文提出了一个评估协议来进行计算，并释放了代码来促进高效训练的研究。 |
| [^72] | [Improved selective background Monte Carlo simulation at Belle II with graph attention networks and weighted events.](http://arxiv.org/abs/2307.06434) | 这项工作通过引入图注意力网络和统计方法，改进了选择性背景蒙特卡洛模拟，并用于Belle II测量稀有过程中节约资源并处理过滤引入的偏差。 |
| [^73] | [Energy Discrepancies: A Score-Independent Loss for Energy-Based Models.](http://arxiv.org/abs/2307.06431) | 我们提出了一种新的能量模型损失函数，能够在不依赖分数计算或昂贵的蒙特卡罗方法的情况下，近似实现显式分数匹配和负对数似然损失，并在学习低维数据分布时具有更好的性能。 |
| [^74] | [Differentially Private Decoupled Graph Convolutions for Multigranular Topology Protection.](http://arxiv.org/abs/2307.06422) | 本论文提出了一种差分私有的解耦图卷积方法，用于多粒度拓扑保护。引入了图差分隐私框架，可以确保模型参数和预测的私密性。 |
| [^75] | [Testing Sparsity Assumptions in Bayesian Networks.](http://arxiv.org/abs/2307.06406) | 本论文研究了在贝叶斯网络中测试稀疏性假设的问题，并提出了一种基于样本特征值的假设检验方法，可以帮助选择适当的结构发现算法。 |
| [^76] | [Trainability, Expressivity and Interpretability in Gated Neural ODEs.](http://arxiv.org/abs/2307.06398) | 本研究扩展了神经常微分方程（nODEs）模型，将其赋予自适应时间尺度，并称之为门控神经ODEs（gnODEs）。通过在连续记忆任务中展示，我们发现gnODEs具有学习（近似）连续吸引子的归纳偏差学习能力。此外，我们还展示了降维后的gnODEs在提高可解释性的同时保持了建模能力，并可以对学习到的吸引子结构进行显性可视化。 |
| [^77] | [Temporal Label-Refinement for Weakly-Supervised Audio-Visual Event Localization.](http://arxiv.org/abs/2307.06385) | 本文解决了弱监督条件下的音视频事件定位问题，通过使用基础模型在训练数据上以更细的时间分辨率估计标签，并提出辅助目标来处理合成视频的分布外特性。 |
| [^78] | [Personalized Anomaly Detection in PPG Data using Representation Learning and Biometric Identification.](http://arxiv.org/abs/2307.06380) | 本研究提出了一个两阶段框架，使用表示学习和个性化来改善PPG数据中的异常检测性能。通过表示学习将原始PPG信号转换为更具区分性和紧凑性的表示，然后结合三种不同的无监督异常检测方法实现运动检测和生物特征识别。实验结果证明，表示学习显著提高了异常检测的性能。 |
| [^79] | [Spectral-Bias and Kernel-Task Alignment in Physically Informed Neural Networks.](http://arxiv.org/abs/2307.06362) | 本文提出了一个综合的理论框架，解决了物理信息神经网络（PINN）设计和训练协议的选择问题。通过将超参数化神经网络和高斯过程回归等价起来，推导出了一种在大数据集限制下决定PINN预测的积分微分方程，以及通过原始微分方程中源项的谱分解来量化网络引入的隐含偏差。 |
| [^80] | [Sequential Experimental Design for X-Ray CT Using Deep Reinforcement Learning.](http://arxiv.org/abs/2307.06343) | 本论文提出了一种使用深度强化学习的顺序实验设计方法，该方法可以在X射线CT中减少扫描角度的数量同时保持重建质量，从而适用于在线质量控制。 |
| [^81] | [Assessment of the suitability of degradation models for the planning of CCTV inspections of sewer pipes.](http://arxiv.org/abs/2307.06341) | 该论文评估了降解模型在排污管道CCTV检查规划中的适用性。结果表明，虽然集成模型具有最高的准确度，但无法推测长期降解；与此相反，逻辑回归模型准确度稍低，但能够产生可解释性强且一致的降解曲线。 |
| [^82] | [Incomplete Utterance Rewriting as Sequential Greedy Tagging.](http://arxiv.org/abs/2307.06337) | 本论文提出了一种基于序列标注的模型，能够更好地从对话上下文中提取信息，并引入了区分说话者的嵌入来建模说话者的变化。实验结果表明，该模型在恢复得分方面达到了最优，并在推理速度上胜过大多数之前的模型。 |
| [^83] | [Provably Faster Gradient Descent via Long Steps.](http://arxiv.org/abs/2307.06324) | 本研究通过计算机辅助分析技术，证明了非常数步长策略下的梯度下降方法经过长距离步骤可以实现更快的收敛速度。 |
| [^84] | [Filling time-series gaps using image techniques: Multidimensional context autoencoder approach for building energy data imputation.](http://arxiv.org/abs/2307.05926) | 本论文提出了一种使用多维环境自编码器的方法来填补能源数据中的缺失间隙。这个方法可以解决能源系统准确预测和管理的问题，并提高数据在决策和研究中的可用性。 |
| [^85] | [Efficient Task Offloading Algorithm for Digital Twin in Edge/Cloud Computing Environment.](http://arxiv.org/abs/2307.05888) | 本论文提出了一种新的数字孪生系统模型，考虑了异构MEC/MCC环境，并基于分布式深度学习提出了一种新的任务卸载方案，实现了高效的实时反馈。 |
| [^86] | [PIGEON: Predicting Image Geolocations.](http://arxiv.org/abs/2307.05845) | PIGEON是一个用于全球规模图像地理定位的多任务端到端系统，通过语义地理单元的创建和精化，以及无监督聚类和ProtoNets的应用，实现了最先进的性能，并提供了预训练的CLIP转换器模型StreetCLIP。 |
| [^87] | [Large Language Models for Supply Chain Optimization.](http://arxiv.org/abs/2307.03875) | 这项研究研究了利用大型语言模型（LLMs）来帮助解释和解读供应链优化结果的方法。他们设计了一个框架，可以接受普通文本查询作为输入，并输出关于底层优化结果的洞察。通过定量回答假设情况，该框架在不放弃最先进的组合优化技术的情况下帮助企业运营者更好地理解和信任优化结果。 |
| [^88] | [inTformer: A Time-Embedded Attention-Based Transformer for Crash Likelihood Prediction at Intersections Using Connected Vehicle Data.](http://arxiv.org/abs/2307.03854) | inTformer是一种基于时间嵌入的关注机制Transformer，用于使用连接车辆数据预测路口事故可能性。与其他深度学习模型相比，inTformer具有处理长期依赖性、并行处理数据序列以及解决梯度消失问题的优势。 |
| [^89] | [Understanding Uncertainty Sampling.](http://arxiv.org/abs/2307.02719) | 本研究通过系统研究流式和池式主动学习下的不确定性采样算法，提出了一个等效损失的概念，并证明不确定性采样算法实质上是针对该等效损失进行优化。 |
| [^90] | [Accelerated stochastic approximation with state-dependent noise.](http://arxiv.org/abs/2307.01497) | 该论文研究了一类具有状态相关噪声的随机平滑凸优化问题。通过引入两种非欧几里得加速随机逼近算法，实现了在精度、问题参数和小批量大小方面的最优性。 |
| [^91] | [Towards Safe Autonomous Driving Policies using a Neuro-Symbolic Deep Reinforcement Learning Approach.](http://arxiv.org/abs/2307.01316) | 本文介绍了一种名为DRL with Symbolic Logics (DRLSL)的新颖神经符号无模型深度强化学习方法，旨在实现在真实环境中安全学习自主驾驶策略。该方法结合了深度强化学习和符号逻辑驱动的推理，允许通过与物理环境的实时交互来学习自主驾驶策略并确保安全性。 |
| [^92] | [Beyond the Snapshot: Brain Tokenized Graph Transformer for Longitudinal Brain Functional Connectome Embedding.](http://arxiv.org/abs/2307.00858) | 本论文提出了一种新的解释性框架，用于将脑功能连接的轨迹嵌入神经退行性疾病的诊断和预后中，并且提出了脑连接图嵌入翻译器（Brain TokenGT）方法。该方法能够揭示脑功能连接随疾病进展的演变方式，对于制定有效的疾病干预策略具有重要意义。 |
| [^93] | [Hiding in Plain Sight: Differential Privacy Noise Exploitation for Evasion-resilient Localized Poisoning Attacks in Multiagent Reinforcement Learning.](http://arxiv.org/abs/2307.00268) | 本文介绍了一种利用差分隐私噪音的本地中毒攻击方法（PeLPA）以绕过异常检测系统，并针对合作多智能体强化学习（CMARL）中的私有知识共享过程中的中毒威胁。研究结果表明，在不同环境下，PeLPA攻击能够显著增加平均步数。 |
| [^94] | [TrustGuard: GNN-based Robust and Explainable Trust Evaluation with Dynamicity Support.](http://arxiv.org/abs/2306.13339) | TrustGuard是一种基于GNN的信任评估模型，支持信任动态性，抗击鲁棒并提供解释能力，它的实验结果在准确性、鲁棒性和可解释性方面都优于其他方法。 |
| [^95] | [A survey on deep learning approaches for data integration in autonomous driving system.](http://arxiv.org/abs/2306.11740) | 本文综述了自主驾驶系统感知模块的最新深度学习集成技术。其提出了一个新的集成分类系统，总结了集成操作及其优缺点，提供了新的见解，阐明了“理想”数据集成方法的特性，可减轻现有方法的局限性。本文总结了优化数据集成方法的关键特点。 |
| [^96] | [Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction.](http://arxiv.org/abs/2306.10045) | 本研究提出了一种高效逼近晶体材料的完整相互作用势的方法，能够覆盖所有原子对之间的势，克服了目前方法中只考虑附近原子间势和无法捕捉无限重复模式的限制。 |
| [^97] | [14 Examples of How LLMs Can Transform Materials Science and Chemistry: A Reflection on a Large Language Model Hackathon.](http://arxiv.org/abs/2306.06283) | 本文记录了一次黑客松活动，参与者使用LLMs进行了各种应用，包括预测分子和材料特性、从非结构化数据中提取知识、为工具设计新界面以及开发新的教育应用。这些多样化的项目反映了LLMs在材料科学和化学领域的多功能性和潜力。 |
| [^98] | [Personalization Disentanglement for Federated Learning.](http://arxiv.org/abs/2306.03570) | 本文通过使用联邦双变分自编码器（FedDVA）明确分解潜在表示，捕捉共享知识和客户特定个性化，从而导致更可靠和有效的个性化联邦学习，并在广泛实验中验证了该方法的优越性。 |
| [^99] | [Revisiting Generalized p-Laplacian Regularized Framelet GCNs: Convergence, Energy Dynamic and Training with Non-Linear Diffusion.](http://arxiv.org/abs/2305.15639) | 本文全面分析了基于图p-Laplacian的Framelet网络，证明了其具有良好的收敛性和能量动态，并能够通过广义非线性扩散过程进行多种训练。同时，该模型能够适应同质和异质数据，避免过度平滑问题。 |
| [^100] | [Dr. LLaMA: Improving Small Language Models in Domain-Specific QA via Generative Data Augmentation.](http://arxiv.org/abs/2305.07804) | 本论文介绍了一种名为Dr. LLaMA的方法，通过使用大型语言模型进行生成式数据增强，以改善小语言模型的性能，特别是在医学问答任务中。这种方法在微调后使模型性能提高，并提出了在特定领域问答任务中使用LLM所面临的挑战和潜在的研究方向。 |
| [^101] | [Parallel bootstrap-based on-policy deep reinforcement learning for continuous flow control applications.](http://arxiv.org/abs/2304.12330) | 本文提出了一种基于并行bootstrap的on-policy深度强化学习方法，通过部分轨迹缓冲区和返回bootstrapping步骤来实现灵活使用并行环境，同时保持更新的on-policy性，该方法在连续流控制问题上有很好的应用前景。 |
| [^102] | [Differentially Private Synthetic Data Generation via Lipschitz-Regularised Variational Autoencoders.](http://arxiv.org/abs/2304.11336) | 本文提出了通过利普希茨正则化变分自编码器生成差分隐私合成数据的新方法。该方法可以生成符合差分隐私保证的高质量合成数据。 |
| [^103] | [Convergence of Message Passing Graph Neural Networks with Generic Aggregation On Large Random Graphs.](http://arxiv.org/abs/2304.11140) | 本文研究了消息传递图神经网络在随机图模型上的收敛性，将收敛结论从只适用于度规范化平均聚合函数扩展到所有传统聚合函数，并考虑了聚合函数采用逐个坐标最大值时的情况。 |
| [^104] | [EfficientNet Algorithm for Classification of Different Types of Cancer.](http://arxiv.org/abs/2304.08715) | 本文用EfficientNet算法分类不同类型的癌症，实验结果表明该算法在公共数据集上表现优异，具有在临床实践中提高癌症诊断准确性和效率的潜力。 |
| [^105] | [Learning Graph ARMA Processes from Time-Vertex Spectra.](http://arxiv.org/abs/2302.06887) | 本研究提出了一种基于学习过程谱密度的算法，用于推断缺失的信号值和进行信号插值，实验结果显示其在时间-顶点信号估计问题中具有高准确性。 |
| [^106] | [Improving and generalizing flow-based generative models with minibatch optimal transport.](http://arxiv.org/abs/2302.00482) | 这篇论文提出了一种称为广义条件流匹配（CFM）的技术，在连续正则化流（CNFs）的生成模型中无需模拟训练，极大提高了效率和稳定性。此外，论文还引入了最优传输CFM（OT-CFM）的变体，可以以无模拟方式计算动态OT，加速了推断过程。 |
| [^107] | [Robust online active learning.](http://arxiv.org/abs/2302.00422) | 本文提出了一种自适应方法，用于鲁棒的在线主动学习，并在受污染的数据流中证明了其性能表现优异，同时确保了稳定性并减少异常值的负面影响。 |
| [^108] | [SAN: Inducing Metrizability of GAN with Discriminative Normalized Linear Layer.](http://arxiv.org/abs/2301.12811) | 本文提出了一种新的GAN训练方案，切片对抗网络（SAN），通过优化生成器和判别器的最小最大目标函数，使生成器分布接近目标分布。实验证实了SAN相对于普通GAN的有效性，并在StyleGAN-XL上取得了最先进的FID评分。 |
| [^109] | [A Deep Learning Method for Comparing Bayesian Hierarchical Models.](http://arxiv.org/abs/2301.11873) | 这个论文提出了一种深度学习方法，用于比较贝叶斯层次模型。该方法通过支持分摊推断，能够高效地进行模型比较和性能验证。同时，作者还对四个层次证据积累模型进行了比较。 |
| [^110] | [EPiC-GAN: Equivariant Point Cloud Generation for Particle Jets.](http://arxiv.org/abs/2301.08128) | 本文介绍了一种名为EPiC-GAN的等变点云生成对抗网络，可以灵活地生成不同多重性的点云，该网络基于深度集合，并能够快速模拟粒子喷流。与其他方法相比，EPiC-GAN具有更高的计算效率和良好的扩展性。 |
| [^111] | [A Survey on Transformers in Reinforcement Learning.](http://arxiv.org/abs/2301.03044) | 这篇论文是一项调查研究，总结了在强化学习领域使用Transformers的动机、进展和未来前景。 |
| [^112] | [The Effectiveness of World Models for Continual Reinforcement Learning.](http://arxiv.org/abs/2211.15944) | 本论文展示了世界模型在连续学习中的应用，通过研究选择性经验回放方法的影响，提出了Continual-Dreamer模型，该模型在Minigrid和Minihack基准测试中表现优于最先进的任务不可知连续强化学习方法。 |
| [^113] | [Control Transformer: Robot Navigation in Unknown Environments through PRM-Guided Return-Conditioned Sequence Modeling.](http://arxiv.org/abs/2211.06407) | Control Transformer是一种通过采样规划引导的低层策略建模返回条件序列的方法，可以在未知环境中成功解决长时间范围的导航任务。 |
| [^114] | [Human Biophysics as Network Weights: Conditional Generative Models for Dynamic Simulation.](http://arxiv.org/abs/2211.01856) | 提出了一种使用条件生成模型插值的架构，可以在动态模拟中降低建模时间并保持高的生成精度。 |
| [^115] | [Adversarial Policies Beat Superhuman Go AIs.](http://arxiv.org/abs/2211.00241) | 通过对抗性策略攻击，我们成功战胜了超级人类级围棋AI KataGo，揭示了其核心弱点，并展示了即使是超级AI系统也可能存在意想不到的失败模式。 |
| [^116] | [Generalized Laplacian Regularized Framelet Graph Neural Networks.](http://arxiv.org/abs/2210.15092) | 本文提出了一种广义的Laplacian正则化框架图神经网络，该网络利用了p-Laplacian的性质和图信号的多分辨率分解的表达能力。实验结果表明，该网络在节点分类和信号去噪等图学习任务中具有出色的性能。 |
| [^117] | [RulE: Neural-Symbolic Knowledge Graph Reasoning with Rule Embedding.](http://arxiv.org/abs/2210.14905) | RulE是一个框架，通过将实体、关系和逻辑规则统一表示在一个嵌入空间中，有效利用逻辑规则提升知识图推理。同时，RulE注入先前的逻辑规则信息，改进了实体/关系嵌入，使得知识图嵌入方法也表现更好。 |
| [^118] | [A kernel Stein test of goodness of fit for sequential models.](http://arxiv.org/abs/2210.10741) | 我们提出了一种基于核斯坦检验的适配性度量方法，可以适用于具有不同维度观测的概率密度模型，包括文本文档或可变长度序列。这种方法扩展了核斯坦差异(KSD)到可变维度设置，并提出了一种新颖的KSD适配性检验方法，无需密度归一化，并在离散顺序数据基准上表现良好。 |
| [^119] | [Revisiting Discrete Soft Actor-Critic.](http://arxiv.org/abs/2209.10081) | 本研究重新审视了将连续动作空间的Soft Actor-Critic方法调整为离散动作空间的问题，并提出了解决Q值低估和性能不稳定的方法，验证了其在Atari游戏和大规模MOBA游戏中的有效性。 |
| [^120] | [Joint User and Data Detection in Grant-Free NOMA with Attention-based BiLSTM Network.](http://arxiv.org/abs/2209.06392) | 本文提出了一种基于注意力的双向LSTM网络用于解决免授权NOMA中的多用户检测问题，通过利用时间相关性和注意力机制，实现了对活动设备的准确识别和数据解码。 |
| [^121] | [Local Intrinsic Dimensionality Measures for Graphs, with Applications to Graph Embeddings.](http://arxiv.org/abs/2208.11986) | 本文介绍了一种新的与LID相关的度量方法NC-LID，用于量化图结构化数据的本地特性，同时利用NC-LID设计了LID感知的图嵌入算法。 |
| [^122] | [Data Augmentation is a Hyperparameter: Cherry-picked Self-Supervision for Unsupervised Anomaly Detection is Creating the Illusion of Success.](http://arxiv.org/abs/2208.07734) | 这项研究通过广泛的实验，证明数据增强与异常生成机制之间的对齐是自监督学习在无监督异常检测中取得成功的关键，并且在缺乏对齐时，自监督学习甚至可能降低准确性。 |
| [^123] | [TRUST-LAPSE: An Explainable and Actionable Mistrust Scoring Framework for Model Monitoring.](http://arxiv.org/abs/2207.11290) | TRUST-LAPSE是一个可解释和可操作的连续模型监控框架，通过使用潜空间嵌入评估每个输入样本的模型预测的可信度，并利用距离和相似度度量以及顺序相关性偏差来实现对模型的连续监控。 |
| [^124] | [Classification and Generation of real-world data with an Associative Memory Model.](http://arxiv.org/abs/2207.04827) | 本文提出了一种基于联想记忆模型的多模态框架，可以以容错的方式存储和检索大量真实世界数据，并且可以用于推断缺失的模态。 |
| [^125] | [Multiple Testing Framework for Out-of-Distribution Detection.](http://arxiv.org/abs/2206.09522) | 本研究提出了一个多重检验框架用于离群分布检测的问题，包括了定义OOD概念和提供强有力保证的方法，与之前的基于阈值的测试相比，在不同类型的OOD实例中表现更一致。 |
| [^126] | [Energy-efficient Deployment of Deep Learning Applications on Cortex-M based Microcontrollers using Deep Compression.](http://arxiv.org/abs/2205.10369) | 本研究通过深度压缩技术实现了在资源受限的微控制器上高效部署深度学习模型，分析了准确性、内存消耗、执行时间和功耗之间的权衡关系。 |
| [^127] | [Efficient Bayesian Policy Reuse with a Scalable Observation Model in Deep Reinforcement Learning.](http://arxiv.org/abs/2204.07729) | 本研究提出了一种改进的贝叶斯策略复用方法，在深度强化学习中实现更高效的策略转移。该方法使用即时且信息丰富的状态转换样本作为观测信号，并提出了一个可扩展的观测模型来加速任务推断和策略复用。 |
| [^128] | [Adapting to Mixing Time in Stochastic Optimization with Markovian Data.](http://arxiv.org/abs/2202.04428) | 本文提出了一种适用于马尔可夫数据的随机优化问题的方法，不需要对混合时间有任何了解，但在凸问题中可以获得最优收敛速度。这种方法还可以应用于非凸优化以及时差学习，并且完全无视混合时间。方法的关键是多层蒙特卡洛梯度估计与自适应学习方法的组合。 |
| [^129] | [Prospective Learning: Principled Extrapolation to the Future.](http://arxiv.org/abs/2201.07372) | 这项研究提出了一种基于未来信息的学习方法，将学习问题重新定义为关于动态未来的概念，并认为前瞻性学习更准确地描述了现实世界中的许多问题。 |
| [^130] | [Bregman Deviations of Generic Exponential Families.](http://arxiv.org/abs/2201.07306) | 我们通过结合Bregman差异和超马丁格尔混合方法，建立了一种通用边界，控制指数族参数与参数有限样本估计之间的Bregman差异，该边界是时间均匀的，并引入了Bregman信息增益。我们将此边界应用于多个经典指数族，并得到了置信区间和Bregman信息增益的明确形式。 |
| [^131] | [Climate-Invariant Machine Learning.](http://arxiv.org/abs/2112.08440) | 本研究提出了一种新的框架——"气候不变"的机器学习，通过将气候过程的知识纳入机器学习算法中，可以在广泛的气候和地理条件下保持高准确性。 |
| [^132] | [Emergent Neural Network Mechanisms for Generalization to Objects in Novel Orientations.](http://arxiv.org/abs/2109.13445) | 本研究提供了证据表明，深度神经网络具有通过传播方向不变性来泛化到新颖方向上的对象的能力。这种能力受到训练中使用的熟悉对象数量的影响，但仅限于涉及2D旋转的熟悉方向。 |
| [^133] | [Balanced Coarsening for Multilevel Hypergraph Partitioning via Wasserstein Discrepancy.](http://arxiv.org/abs/2106.07501) | 本研究提出了一种平衡粗化方案，用于多层超图划分。通过Wasserstein差异度协调粗化过程中的最优传输，并使用三点算法找到在平衡约束下的最佳划分。 |
| [^134] | [A New Formalism, Method and Open Issues for Zero-Shot Coordination.](http://arxiv.org/abs/2106.06613) | 本研究提出了零知识协调问题的正式定义，并证明了之前的解决方法不是最优解。针对这一问题，引入了带有打破平局的其他对局算法作为最优解。 |
| [^135] | [Learning low-rank latent mesoscale structures in networks.](http://arxiv.org/abs/2102.06984) | 这项研究提出了一种学习网络中低秩潜在中尺度结构的新方法，并通过合成网络模型和实际网络验证了其有效性。通过利用少量的“潜在模式”，可以成功地近似网络的大多数子图。这项研究对于理解复杂系统的行为具有重要意义。 |
| [^136] | [Declarative Mechanism Design.](http://arxiv.org/abs/1912.13122) | 本文介绍了声明性机制设计的研究，提出了机构神经网络作为一种受管制的人工神经网络，引起人们对人工教学的关注，并提供了初步的答案。 |
| [^137] | [Towards Learning to Imitate from a Single Video Demonstration.](http://arxiv.org/abs/1901.07186) | 本研究提出了一种从单一视频演示中学习模仿的方法，通过使用对比训练和Siamese循环神经网络，我们能够学习到智能体的行为与演示之间的奖励函数，并通过 RL 策略的训练最小化这个距离。实验表明，引入多任务数据和额外的图像编码损失可以改善学习到的奖励的时间一致性，并显著提高策略学习的效果。我们在不同维度的仿真智能体上验证了我们的方法的优越性。 |

# 详细

[^1]: HyperDreamBooth：用于快速个性化文本到图像模型的超网络

    HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models. (arXiv:2307.06949v1 [cs.CV])

    [http://arxiv.org/abs/2307.06949](http://arxiv.org/abs/2307.06949)

    HyperDreamBooth是一个超网络，可以从一个人的单张图片中快速生成个性化权重，从而实现在多种背景和风格下合成一个人的面部，保持高保真度并同时保留对多样化风格和语义修改的关键知识。

    

    个性化已经成为生成式人工智能领域中的一个重要方面，使得在不同背景和风格下合成个体成为可能，同时保持高保真度。然而，个性化过程在时间和内存需求方面存在困难。每个个性化模型的微调需要大量的GPU时间投入，为每个主题存储一个个性化模型会对存储容量提出要求。为了克服这些挑战，我们提出了HyperDreamBooth-一种能够从一个人的单张图片有效生成一组个性化权重的超网络。通过将这些权重组合到扩散模型中，并搭配快速微调，HyperDreamBooth能够以多种背景和风格生成一个人的面部，保持高主题细节同时也保持模型对多样化风格和语义修改的关键知识。我们的方法在大约50倍体现了面部个性化。

    Personalization has emerged as a prominent aspect within the field of generative AI, enabling the synthesis of individuals in diverse contexts and styles, while retaining high-fidelity to their identities. However, the process of personalization presents inherent challenges in terms of time and memory requirements. Fine-tuning each personalized model needs considerable GPU time investment, and storing a personalized model per subject can be demanding in terms of storage capacity. To overcome these challenges, we propose HyperDreamBooth-a hypernetwork capable of efficiently generating a small set of personalized weights from a single image of a person. By composing these weights into the diffusion model, coupled with fast finetuning, HyperDreamBooth can generate a person's face in various contexts and styles, with high subject details while also preserving the model's crucial knowledge of diverse styles and semantic modifications. Our method achieves personalization on faces in roughly 
    
[^2]: 在大型语言模型中的上下文压缩的上下文自编码器

    In-context Autoencoder for Context Compression in a Large Language Model. (arXiv:2307.06945v1 [cs.CL])

    [http://arxiv.org/abs/2307.06945](http://arxiv.org/abs/2307.06945)

    在大型语言模型中，我们提出了一种称为In-context Autoencoder (ICAE)的上下文自编码器，它通过将长上下文压缩为有限数量的内存槽，实现了$4\times$的上下文压缩，并能够根据内存槽进行条件处理以响应各种提示。

    

    我们提出了一种用于大型语言模型中上下文压缩的上下文自编码器（ICAE）。 ICAE有两个模块：一个可学习的编码器，通过从LLM中采用LoRA方式将长上下文压缩为有限数量的内存槽，以及一个固定的解码器，作为目标LLM，可以根据内存槽来进行各种目的的条件处理。我们首先使用自编码和语言建模目标在大规模文本数据上预训练ICAE，使其能够生成准确和全面表示原始上下文的内存槽。然后，我们使用少量指导数据对预训练的ICAE进行微调，以增强其与各种提示的交互，从而产生理想的响应。我们的实验结果表明，使用我们提出的预训练和微调范式学习的ICAE可以有效地产生$4\times$上下文压缩的内存槽，目标LLM可以很好地对其进行条件处理，以响应各种提示。

    We propose the In-context Autoencoder (ICAE) for context compression in a large language model (LLM). The ICAE has two modules: a learnable encoder adapted with LoRA from an LLM for compressing a long context into a limited number of memory slots, and a fixed decoder which is the target LLM that can condition on the memory slots for various purposes. We first pretrain the ICAE using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context. Then, we fine-tune the pretrained ICAE on a small amount of instruct data to enhance its interaction with various prompts for producing desirable responses. Our experimental results demonstrate that the ICAE learned with our proposed pretraining and fine-tuning paradigm can effectively produce memory slots with $4\times$ context compression, which can be well conditioned on by the target LLM to respond to various prompts. The promis
    
[^3]: 关于博弈论特征归因和反事实解释之间的联系

    On the Connection between Game-Theoretic Feature Attributions and Counterfactual Explanations. (arXiv:2307.06941v1 [cs.AI])

    [http://arxiv.org/abs/2307.06941](http://arxiv.org/abs/2307.06941)

    本研究通过建立理论联系，将博弈论特征归因和反事实解释相结合。通过变换和证明，在一定条件下它们是等价的。研究结果还指出了仅仅使用反事实解释来提供特征重要性的局限性。

    

    最近，可解释的人工智能（XAI）引起了广泛关注，其中两种最受欢迎的解释类型是特征归因和反事实解释。这两类方法一直在独立地进行研究，而对它们的调和只有少数试图是经验性的。本研究建立了博弈论特征归因（主要关注但不限于SHAP）和反事实解释之间的明确理论联系。通过对基于Shapley值的特征归因和反事实解释进行有效变换，并在一定条件下证明它们实际上是等价的。然后，我们将等价结果扩展到了Shapley值以外的博弈论解概念。此外，通过对等价条件的分析，我们揭示了只简单地使用反事实解释来提供特征重要性的局限性。在三个数据集上进行了定量实验。

    Explainable Artificial Intelligence (XAI) has received widespread interest in recent years, and two of the most popular types of explanations are feature attributions, and counterfactual explanations. These classes of approaches have been largely studied independently and the few attempts at reconciling them have been primarily empirical. This work establishes a clear theoretical connection between game-theoretic feature attributions, focusing on but not limited to SHAP, and counterfactuals explanations. After motivating operative changes to Shapley values based feature attributions and counterfactual explanations, we prove that, under conditions, they are in fact equivalent. We then extend the equivalency result to game-theoretic solution concepts beyond Shapley values. Moreover, through the analysis of the conditions of such equivalence, we shed light on the limitations of naively using counterfactual explanations to provide feature importances. Experiments on three datasets quantita
    
[^4]: FDAPT: 面向语言模型的联邦领域自适应预训练

    FDAPT: Federated Domain-adaptive Pre-training for Language Models. (arXiv:2307.06933v1 [cs.LG])

    [http://arxiv.org/abs/2307.06933](http://arxiv.org/abs/2307.06933)

    FDAPT是一种联邦领域自适应预训练的方法，在保护数据隐私的同时，能够通过利用敏感和分布式数据来增强模型适应能力。对于IID和非IID情况下的下游任务，FDAPT能够维持与中央基线相竞争的性能。提出的FFDAPT算法进一步提高了计算效率，并展现出与标准FDAPT类似的下游任务性能。此外，我们也确定了这个新研究领域的有希望的未来研究方向。

    

    将领域自适应预训练（DAPT）与联邦学习（FL）相结合可以通过利用更敏感和分布式数据来增强模型适应能力，同时保护数据隐私。然而，目前关于这种方法的研究还很少。因此，我们进行了第一次全面的实证研究，以评估联邦领域自适应预训练（FDAPT）的性能。我们证明了FDAPT在IID和非IID情况下都能维持与中央基线相竞争的下游任务性能。此外，我们提出了一种新算法，冻结的联邦领域自适应预训练（FFDAPT）。FFDAPT平均提高了12.1%的计算效率，并且在标准FDAPT的情况下展现出类似的下游任务性能，一般性能波动保持在1%以下。最后，通过对我们的工作进行批判性评估，我们确定了这个新的研究领域的有希望的未来研究方向。

    Combining Domain-adaptive Pre-training (DAPT) with Federated Learning (FL) can enhance model adaptation by leveraging more sensitive and distributed data while preserving data privacy. However, few studies have focused on this method. Therefore, we conduct the first comprehensive empirical study to evaluate the performance of Federated Domain-adaptive Pre-training (FDAPT). We demonstrate that FDAPT can maintain competitive downstream task performance to the centralized baseline in both IID and non-IID situations. Furthermore, we propose a novel algorithm, Frozen Federated Domain-adaptive Pre-training (FFDAPT). FFDAPT improves the computational efficiency by 12.1% on average and exhibits similar downstream task performance to standard FDAPT, with general performance fluctuations remaining less than 1%. Finally, through a critical evaluation of our work, we identify promising future research directions for this new research area.
    
[^5]: 面向领域通用的调优编码器用于快速个性化文本到图像模型

    Domain-Agnostic Tuning-Encoder for Fast Personalization of Text-To-Image Models. (arXiv:2307.06925v1 [cs.CV])

    [http://arxiv.org/abs/2307.06925](http://arxiv.org/abs/2307.06925)

    提出了一种面向领域通用的方法，在文本到图像个性化中有效地使用编码器技术，同时避免了对特定数据集或先前信息的依赖。引入了对比度正则化技术，以保持高保真度和可编辑性，并通过将预测的标记推向最近的现有标记来实现这一目标。

    

    文本到图像（T2I）个性化允许用户通过将自己的视觉概念与自然语言提示相结合来指导创造性图像生成过程。最近，基于编码器的技术已经成为T2I个性化的一种新的有效方法，减少了对多个图像和长时间训练的需求。然而，大多数现有的编码器都局限于单一领域，这限制了它们处理多样化概念的能力。在这项工作中，我们提出了一种领域通用的方法，不需要任何专门的数据集或关于个性化概念的先前信息。我们引入了一种新颖的对比度正则化技术，以保持对目标概念特征的高保真度，同时使预测的嵌入保持接近潜在空间的可编辑区域，通过将预测的标记推向其最近的现有CLIP标记。我们的实验结果证明了我们方法的有效性，并展示了学习到的标记如何

    Text-to-image (T2I) personalization allows users to guide the creative image generation process by combining their own visual concepts in natural language prompts. Recently, encoder-based techniques have emerged as a new effective approach for T2I personalization, reducing the need for multiple images and long training times. However, most existing encoders are limited to a single-class domain, which hinders their ability to handle diverse concepts. In this work, we propose a domain-agnostic method that does not require any specialized dataset or prior information about the personalized concepts. We introduce a novel contrastive-based regularization technique to maintain high fidelity to the target concept characteristics while keeping the predicted embeddings close to editable regions of the latent space, by pushing the predicted tokens toward their nearest existing CLIP tokens. Our experimental results demonstrate the effectiveness of our approach and show how the learned tokens are 
    
[^6]: DRAGON: 一种基于对话的带有视觉语言关联的辅助导航机器人

    DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual Language Grounding. (arXiv:2307.06924v1 [cs.RO])

    [http://arxiv.org/abs/2307.06924](http://arxiv.org/abs/2307.06924)

    DRAGON是一种基于对话的导航机器人，能够理解用户的指令并通过语言与用户沟通，为视力受损者提供导航和环境描述的帮助。

    

    视力受损者在理解和导航周围空间方面存在困难。目前的导航技术要么只关注导航，要么提供有限的关于环境的沟通。受到最近在视觉语言关联和语义导航方面的进展的启发，我们提出了DRAGON，一种由对话系统驱动的导航机器人，并具有将环境与自然语言关联的能力。通过理解用户的指令，DRAGON能够引导用户到地图上的目标地标，描述环境，并通过视觉观察回答问题。通过有效利用对话，机器人可以将用户的自由形式描述与环境中的地标关联起来，并通过口语提供语义信息给用户。我们在日常室内环境中进行了盲目参与者的用户研究。我们的结果表明，DRAGON能够与用户顺畅地沟通，

    Persons with visual impairments (PwVI) have difficulties understanding and navigating spaces around them. Current wayfinding technologies either focus solely on navigation or provide limited communication about the environment. Motivated by recent advances in visual-language grounding and semantic navigation, we propose DRAGON, a guiding robot powered by a dialogue system and the ability to associate the environment with natural language. By understanding the commands from the user, DRAGON is able to guide the user to the desired landmarks on the map, describe the environment, and answer questions from visual observations. Through effective utilization of dialogue, the robot can ground the user's free-form descriptions to landmarks in the environment, and give the user semantic information through spoken language. We conduct a user study with blindfolded participants in an everyday indoor environment. Our results demonstrate that DRAGON is able to communicate with the user smoothly, pr
    
[^7]: 加权平均随机梯度下降: 渐近正态性和最优性

    Weighted Averaged Stochastic Gradient Descent: Asymptotic Normality and Optimality. (arXiv:2307.06915v1 [stat.ML])

    [http://arxiv.org/abs/2307.06915](http://arxiv.org/abs/2307.06915)

    本文探索了一种加权平均随机梯度下降（SGD）方案，并建立了渐近正态性，提供了渐近有效的在线推理方法。此外，我们提出了一种自适应平均方案，具有最优的统计速度和有利的非渐近收敛性。

    

    随机梯度下降（SGD）是现代统计和机器学习中最简单和最流行的算法之一，由于其计算和内存效率而受到青睐。在不同的情境下，已经提出了各种平均方案来加速SGD的收敛。在本文中，我们探讨了一种用于SGD的通用平均方案。具体而言，我们建立了一类加权平均SGD解的渐近正态性，并提供了渐近有效的在线推理方法。此外，我们提出了一种自适应平均方案，展现出最优的统计速度和有利的非渐近收敛性，借鉴了线性模型的非渐近均方误差（MSE）的最优权重的见解。

    Stochastic Gradient Descent (SGD) is one of the simplest and most popular algorithms in modern statistical and machine learning due to its computational and memory efficiency. Various averaging schemes have been proposed to accelerate the convergence of SGD in different settings. In this paper, we explore a general averaging scheme for SGD. Specifically, we establish the asymptotic normality of a broad range of weighted averaged SGD solutions and provide asymptotically valid online inference approaches. Furthermore, we propose an adaptive averaging scheme that exhibits both optimal statistical rate and favorable non-asymptotic convergence, drawing insights from the optimal weight for the linear model in terms of non-asymptotic mean squared error (MSE).
    
[^8]: 通过潜在空间分解揭示独特的概念向量

    Uncovering Unique Concept Vectors through Latent Space Decomposition. (arXiv:2307.06913v1 [cs.LG])

    [http://arxiv.org/abs/2307.06913](http://arxiv.org/abs/2307.06913)

    通过潜在空间分解和无监督聚类，我们提出了一种自动揭示深度学习模型学习到的概念向量的方法，这些概念向量与模型预测相关且具有语义的独特概念，并且在实验中表明这些概念对人类来说易于理解和与任务相关。

    

    解释深度学习模型的内部工作对于建立信任和确保模型安全至关重要。基于概念的解释已经成为一种更易解释的方法，比如像素显著性等特征归因估计。然而，定义解释分析的概念会受到用户对概念期望的偏差影响。为了解决这个问题，我们提出了一种新的事后无监督方法，可以自动揭示深度模型在训练期间学习到的概念。通过分解一个层的潜在空间成奇异向量，并通过无监督聚类对其进行精炼，我们揭示了与模型预测相关的高方差方向上的概念向量，并指向语义上独特的概念。我们广泛的实验结果显示，我们的大部分概念对人类来说是易于理解的，具有一致性，并与所需任务相关。此外，我们还展示了...

    Interpreting the inner workings of deep learning models is crucial for establishing trust and ensuring model safety. Concept-based explanations have emerged as a superior approach that is more interpretable than feature attribution estimates such as pixel saliency. However, defining the concepts for the interpretability analysis biases the explanations by the user's expectations on the concepts. To address this, we propose a novel post-hoc unsupervised method that automatically uncovers the concepts learned by deep models during training. By decomposing the latent space of a layer in singular vectors and refining them by unsupervised clustering, we uncover concept vectors aligned with directions of high variance that are relevant to the model prediction, and that point to semantically distinct concepts. Our extensive experiments reveal that the majority of our concepts are readily understandable to humans, exhibit coherency, and bear relevance to the task at hand. Moreover, we showcase
    
[^9]: 通过双层ReLU神经网络实现可证明的多任务表示学习

    Provable Multi-Task Representation Learning by Two-Layer ReLU Neural Networks. (arXiv:2307.06887v1 [cs.LG])

    [http://arxiv.org/abs/2307.06887](http://arxiv.org/abs/2307.06887)

    通过双层ReLU神经网络，本论文提出了一种可证明的多任务表示学习方法，用于解决神经网络在实践中同时训练多个任务时遇到的问题。

    

    特征学习是神经网络实际成功的关键，然而如何以及为何发生特征学习仍然难以解释。最近的理论研究表明，在用梯度下降方法优化的浅层神经网络上可以学习有意义的特征，扩展了我们对于神经切向核或随机特征范例中微不足道的特征学习的了解。然而，在实践中，神经网络越来越经常地同时训练多个具有不同损失函数的任务，并且这些先前的分析并不适用于这种情况。在多任务学习设置中，各种研究已经表明简单线性模型可以有效地进行特征学习。然而，通过非线性模型进行多任务学习，这在实践中是最常见的学习范式，仍然存在许多未知。在这项工作中，我们首次提出了一种可证明的多任务表示学习方法，通过双层ReLU神经网络实现。

    Feature learning, i.e. extracting meaningful representations of data, is quintessential to the practical success of neural networks trained with gradient descent, yet it is notoriously difficult to explain how and why it occurs. Recent theoretical studies have shown that shallow neural networks optimized on a single task with gradient-based methods can learn meaningful features, extending our understanding beyond the neural tangent kernel or random feature regime in which negligible feature learning occurs. But in practice, neural networks are increasingly often trained on {\em many} tasks simultaneously with differing loss functions, and these prior analyses do not generalize to such settings. In the multi-task learning setting, a variety of studies have shown effective feature learning by simple linear models. However, multi-task learning via {\em nonlinear} models, arguably the most common learning paradigm in practice, remains largely mysterious. In this work, we present the first 
    
[^10]: Min-Max优化在延迟下的研究

    Min-Max Optimization under Delays. (arXiv:2307.06886v1 [cs.LG])

    [http://arxiv.org/abs/2307.06886](http://arxiv.org/abs/2307.06886)

    在大规模机器学习中，研究了min-max优化在延迟下的性能。对于简单实例，即使是小的延迟也可能导致Extra-gradient算法发散，因此需要对延迟版本的min-max优化算法进行仔细分析。为此，我们证明了在适当的技术假设下，梯度下降-上升算法在延迟情况下的收敛性和性能。

    

    在通信起重要作用的大规模机器学习问题中，延迟和异步是不可避免的。因此，一些研究团队广泛分析了具有延迟梯度的随机优化问题。但据我们所知，尚无类似的理论可用于min-max优化，这个话题由于在对抗鲁棒性、博弈论和强化学习中的应用而越来越受关注。针对这一差距，我们对带有延迟梯度更新的标准min-max优化算法的性能进行了研究。首先，我们（经验性地）展示了即使是小的延迟也可能导致像Extra-gradient (EG) 这样的杰出算法在简单实例上发散，而在没有延迟的情况下EG可以保证收敛。因此，我们的经验研究表明有必要对延迟版本的min-max优化算法进行仔细分析。相应地，在适当的技术假设下，我们证明了梯度下降 - 上升 (GDA)算法在延迟情况下的收敛性和性能。

    Delays and asynchrony are inevitable in large-scale machine-learning problems where communication plays a key role. As such, several works have extensively analyzed stochastic optimization with delayed gradients. However, as far as we are aware, no analogous theory is available for min-max optimization, a topic that has gained recent popularity due to applications in adversarial robustness, game theory, and reinforcement learning. Motivated by this gap, we examine the performance of standard min-max optimization algorithms with delayed gradient updates. First, we show (empirically) that even small delays can cause prominent algorithms like Extra-gradient (\texttt{EG}) to diverge on simple instances for which \texttt{EG} guarantees convergence in the absence of delays. Our empirical study thus suggests the need for a careful analysis of delayed versions of min-max optimization algorithms. Accordingly, under suitable technical assumptions, we prove that Gradient Descent-Ascent (\texttt{G
    
[^11]: 非平稳强化学习的复杂性

    The complexity of non-stationary reinforcement learning. (arXiv:2307.06877v1 [cs.LG])

    [http://arxiv.org/abs/2307.06877](http://arxiv.org/abs/2307.06877)

    强化学习中的非平稳学习是一个重要挑战，我们证明了在修改概率或奖励时需要花费大量的时间来保持值函数的最新状态，并且这个挑战与状态数目密切相关。

    

    非平稳强化学习的问题被认为是强化学习应用中的一个重要挑战。我们证明了最坏情况下的复杂性结果，我们认为这恰好捕捉到了这个挑战：修改强化学习问题中一个状态-动作对的概率或奖励，需要花费几乎与状态数目一样多的时间来及时更新值函数，除非强指数时间假设(SETH)是错误的；SETH是P≠NP猜想的广泛接受的加强版。需要注意的是，目前强化学习应用中的状态数目通常是天文数字级别的。相反，我们还展示了仅仅"添加"一个新的状态-动作对要容易得多。

    The problem of continual learning in the domain of reinforcement learning, often called non-stationary reinforcement learning, has been identified as an important challenge to the application of reinforcement learning. We prove a worst-case complexity result, which we believe captures this challenge: Modifying the probabilities or the reward of a single state-action pair in a reinforcement learning problem requires an amount of time almost as large as the number of states in order to keep the value function up to date, unless the strong exponential time hypothesis (SETH) is false; SETH is a widely accepted strengthening of the P $\neq$ NP conjecture. Recall that the number of states in current applications of reinforcement learning is typically astronomical. In contrast, we show that just $\textit{adding}$ a new state-action pair is considerably easier to implement.
    
[^12]: 利用机器学习和偏见分析来识别地方政府的早期援助转介

    Identifying Early Help Referrals For Local Authorities With Machine Learning And Bias Analysis. (arXiv:2307.06871v1 [cs.LG])

    [http://arxiv.org/abs/2307.06871](http://arxiv.org/abs/2307.06871)

    本文研究了利用机器学习和偏见分析来帮助地方政府识别有可能需要早期援助的家庭。在构建模型时，虽然模型展示了识别年轻人需要干预的能力，但在不平衡数据下也产生了很多错误的正例。

    

    英格兰的地方政府（如莱斯特郡委员会）提供早期援助服务，可在年轻人面临无法仅靠普遍服务（如学校）支持的困难时提供援助。本文研究了利用机器学习（ML）来帮助专家识别可能需要进行早期援助评估和支持的家庭。莱斯特郡委员会提供了一个包含14360条18岁以下年轻人记录的匿名化数据集。对数据集进行预处理，构建了机器学习模型，并进行了实验以验证和测试模型的性能。应用了偏见缓解技术来提高这些模型的公平性。在测试过程中，尽管模型展示了识别需要干预或早期援助的年轻人的能力，但也产生了大量错误的正例，特别是当使用不平衡的数据构建时。

    Local authorities in England, such as Leicestershire County Council (LCC), provide Early Help services that can be offered at any point in a young person's life when they experience difficulties that cannot be supported by universal services alone, such as schools. This paper investigates the utilisation of machine learning (ML) to assist experts in identifying families that may need to be referred for Early Help assessment and support. LCC provided an anonymised dataset comprising 14360 records of young people under the age of 18. The dataset was pre-processed, machine learning models were build, and experiments were conducted to validate and test the performance of the models. Bias mitigation techniques were applied to improve the fairness of these models. During testing, while the models demonstrated the capability to identify young people requiring intervention or early help, they also produced a significant number of false positives, especially when constructed with imbalanced dat
    
[^13]: 任务和运动规划的具身化终身学习

    Embodied Lifelong Learning for Task and Motion Planning. (arXiv:2307.06870v1 [cs.RO])

    [http://arxiv.org/abs/2307.06870](http://arxiv.org/abs/2307.06870)

    这篇论文研究了在家中长期部署的机器人所面临的具身化终身学习问题，在任务和运动规划的背景下采用了新颖的问题建模方法。利用TAMP系统的模块化特性，提出了一个生成混合模型来产生规划器的候选参数。通过学习共享和非共享的模型，并根据代理任务来在线选择使用的模型，该方法在模拟的2D领域和BEHAVIOR基准测试中取得了显著的规划成功改进。

    

    在一个长时间内部署在家中的机器人面临着真正的终身学习问题。作为机器人寻求为用户提供帮助，它应该利用任何积累的经验来改进自己的知识，成为一个更熟练的助手。我们在任务和运动规划（TAMP）学习的背景下，对这种情况进行了新颖的终身学习问题建模。利用TAMP系统的模块化特性，我们开发了一个生成混合模型，为规划器生成候选连续参数。与大多数现有的终身学习方法预先确定数据如何在任务模型之间共享不同，我们的方法学习共享和非共享模型，并根据代理任务来决定在规划过程中在线使用哪个模型，这些任务作为每个模型对状态的理解的代理。我们的方法在模拟的2D领域和BEHAVIOR基准测试中的多个问题上展示了显著的规划成功的改进。

    A robot deployed in a home over long stretches of time faces a true lifelong learning problem. As it seeks to provide assistance to its users, the robot should leverage any accumulated experience to improve its own knowledge to become a more proficient assistant. We formalize this setting with a novel lifelong learning problem formulation in the context of learning for task and motion planning (TAMP). Exploiting the modularity of TAMP systems, we develop a generative mixture model that produces candidate continuous parameters for a planner. Whereas most existing lifelong learning approaches determine a priori how data is shared across task models, our approach learns shared and non-shared models and determines which to use online during planning based on auxiliary tasks that serve as a proxy for each model's understanding of a state. Our method exhibits substantial improvements in planning success on simulated 2D domains and on several problems from the BEHAVIOR benchmark.
    
[^14]: AnuraSet：一份用于对被动声学监测中新热带蛙类叫声识别进行基准测试的数据集

    AnuraSet: A dataset for benchmarking Neotropical anuran calls identification in passive acoustic monitoring. (arXiv:2307.06860v1 [cs.SD])

    [http://arxiv.org/abs/2307.06860](http://arxiv.org/abs/2307.06860)

    AnuraSet是一个用于对新热带蛙类叫声进行识别的基准测试数据集，该数据集包括27小时的专家注释的记录，提供了基准模型以及用于解决蛙类叫声识别问题的挑战。

    

    全球变化预计将引发蛙类声学行为的转变，可以通过被动声学监测进行研究。了解叫声行为的变化需要识别蛙类物种，由于新热带音景的特殊特点，这是一项具有挑战性的任务。在本文中，我们介绍了一份大规模的多物种蛙类叫声数据集，该数据集由被动声学监测记录的42个不同物种的专家注释组成，总计27小时。我们向公众提供了数据集的开放访问，包括原始记录、实验设置代码以及一项细粒度分类问题的基准模型。此外，我们强调了数据集的挑战，鼓励机器学习研究者解决蛙类叫声识别问题以推进保护政策。我们的所有实验和资源都可以在我们的GitHub存储库https://github.com/soundclim/anuraset 上找到。

    Global change is predicted to induce shifts in anuran acoustic behavior, which can be studied through passive acoustic monitoring (PAM). Understanding changes in calling behavior requires the identification of anuran species, which is challenging due to the particular characteristics of neotropical soundscapes. In this paper, we introduce a large-scale multi-species dataset of anuran amphibians calls recorded by PAM, that comprises 27 hours of expert annotations for 42 different species from two Brazilian biomes. We provide open access to the dataset, including the raw recordings, experimental setup code, and a benchmark with a baseline model of the fine-grained categorization problem. Additionally, we highlight the challenges of the dataset to encourage machine learning researchers to solve the problem of anuran call identification towards conservation policy. All our experiments and resources can be found on our GitHub repository https://github.com/soundclim/anuraset.
    
[^15]: 自洽性方法用于无限生成问题的改进

    Self-consistency for open-ended generations. (arXiv:2307.06857v1 [cs.AI])

    [http://arxiv.org/abs/2307.06857](http://arxiv.org/abs/2307.06857)

    本论文提出了一种改进大规模预训练语言模型生成输出质量和一致性的新方法，通过扩展自洽性框架的适用性，实现了从一个候选集中恢复最优或接近最优的生成结果，并提出了一种轻量级无参数相似性函数来改进代码生成、自动形式化和摘要任务的效果。

    

    在这篇论文中，我们提出了一种改进大规模预训练语言模型生成输出的质量和一致性的新方法。自洽性已经被证明是一种有效的方法，对于具有固定答案的提示，选择得票最多的答案。我们引入了一个推广的自洽性框架，扩展了其适用性，超越了固定答案问题的范围。通过大量的模拟实验，我们证明了我们的方法能够从候选集中恢复最优或接近最优的生成结果。我们还提出了一种轻量级无参数相似性函数，即使没有访问到标记的概率，也能在代码生成、自动形式化和摘要任务中显著和一致地改进效果。我们的方法几乎没有计算开销，不需要额外的再排序模型或对现有模型的修改。

    In this paper, we present a novel approach for improving the quality and consistency of generated outputs from large-scale pre-trained language models (LLMs). Self-consistency has emerged as an effective approach for prompts with fixed answers, selecting the answer with the highest number of votes. In this paper, we introduce a generalized framework for self-consistency that extends its applicability beyond problems that have fixed-answer answers. Through extensive simulations, we demonstrate that our approach consistently recovers the optimal or near-optimal generation from a set of candidates. We also propose lightweight parameter-free similarity functions that show significant and consistent improvements across code generation, autoformalization, and summarization tasks, even without access to token log probabilities. Our method incurs minimal computational overhead, requiring no auxiliary reranker models or modifications to the existing model.
    
[^16]: 训练CNN的数据增强技术：给图像注入噪声

    Data Augmentation in Training CNNs: Injecting Noise to Images. (arXiv:2307.06855v1 [cs.CV])

    [http://arxiv.org/abs/2307.06855](http://arxiv.org/abs/2307.06855)

    这项研究分析了将不同噪声模型以不同幅度添加到CNN架构中的效果，并提出了一些噪声注入的新启发和建议，有助于优化图像分类的学习过程。

    

    噪声注入是数据增强的一个基本工具，然而目前还没有广泛接受的程序将其与学习框架相结合。本研究分析了将不同噪声模型以不同的幅度添加到卷积神经网络（CNN）架构上的效果。为了进行比较，使用结构相似度（SSIM）度量方法给出具有不同密度函数的噪声模型相同的幅度水平。基本结果一致地符合机器学习中的大部分常识，同时还介绍了一些关于噪声注入的新启发和建议。这些新方法将有助于更好地理解图像分类的最佳学习过程。

    Noise injection is a fundamental tool for data augmentation, and yet there is no widely accepted procedure to incorporate it with learning frameworks. This study analyzes the effects of adding or applying different noise models of varying magnitudes to Convolutional Neural Network (CNN) architectures. Noise models that are distributed with different density functions are given common magnitude levels via Structural Similarity (SSIM) metric in order to create an appropriate ground for comparison. The basic results are conforming with the most of the common notions in machine learning, and also introduce some novel heuristics and recommendations on noise injection. The new approaches will provide better understanding on optimal learning procedures for image classification.
    
[^17]: 通过使用Leverage Sampling和Tensor QR分解进行网络延迟估计的张量完成

    Tensor Completion via Leverage Sampling and Tensor QR Decomposition for Network Latency Estimation. (arXiv:2307.06848v1 [cs.NI])

    [http://arxiv.org/abs/2307.06848](http://arxiv.org/abs/2307.06848)

    本文提出了一种通过改进张量采样策略和引入张量QR分解来加快网络延迟估计问题的张量完成方法。

    

    本文主要研究网络延迟估计问题，提出了一种更快速且保持高准确性的方法。方法利用网络节点的数据结构形成矩阵，通过引入时间维度构建了张量模型，将整个问题归结为张量完成问题。我们改进了张量Leverage Sampling策略，引入了Tensor QR分解来加快张量完成的过程。通过将张量奇异值分解（t-SVD）替换为张量CSVD-QR，实现了更快速的张量Levarage Sampling。为了加快不完全张量的完成速度，我们使用了张量的$L_{2,1}$-norm，而不是传统的张量核范数。此外，我们还将Tensor QR分解引入到交替方向乘子法（ADMM）中。

    In this paper, we consider the network latency estimation, which has been an important metric for network performance. However, a large scale of network latency estimation requires a lot of computing time. Therefore, we propose a new method that is much faster and maintains high accuracy. The data structure of network nodes can form a matrix, and the tensor model can be formed by introducing the time dimension. Thus, the entire problem can be be summarized as a tensor completion problem. The main idea of our method is improving the tensor leverage sampling strategy and introduce tensor QR decomposition into tensor completion. To achieve faster tensor leverage sampling, we replace tensor singular decomposition (t-SVD) with tensor CSVD-QR to appoximate t-SVD. To achieve faster completion for incomplete tensor, we use the tensor $L_{2,1}$-norm rather than traditional tensor nuclear norm. Furthermore, we introduce tensor QR decomposition into alternating direction method of multipliers (AD
    
[^18]: 面向动态和灵活的5G多MAP网络操作的联邦多智能体深度强化学习

    Federated Multi-Agent Deep Reinforcement Learning for Dynamic and Flexible 3D Operation of 5G Multi-MAP Networks. (arXiv:2307.06842v1 [cs.NI])

    [http://arxiv.org/abs/2307.06842](http://arxiv.org/abs/2307.06842)

    本文提出了一个联邦多智能体深度强化学习的架构，用于动态和灵活地操作5G多MAP网络。在高层决策中，通过共识确定MAPs的数量并考虑网络自管理的相互依赖关系。在低层，使用一个双注意力的DRL模型来管理MAPs的位置，同时提出了一个联邦机制来训练和共享每个MAP的位置模型。最后，通过多目标奖励函数共同优化MAPs的位置和回传连接。

    

    本文研究了在5G网络中高效管理移动接入点(MAPs)，MAPs是无人机(UAV)。我们提出了一个两级分层架构，通过考虑综合接入-回传(IAB)约束来动态重配置网络。高层决策过程通过共识确定MAPs的数量，我们开发了一个联合优化过程来考虑网络自管理中的相互依赖关系。在低层，MAPs使用基于双注意力的深度强化学习(DRL)模型进行位置管理，鼓励合作而无需重新训练。为了提高泛化能力和降低复杂性，我们提出了一种联邦机制，用于在低层训练和共享每个MAP的一个位置模型。此外，我们使用多目标奖励函数共同优化MAPs的位置和回传连接，考虑到不同MAP位置对无线回传的影响。

    This paper addresses the efficient management of Mobile Access Points (MAPs), which are Unmanned Aerial Vehicles (UAV), in 5G networks. We propose a two-level hierarchical architecture, which dynamically reconfigures the network while considering Integrated Access-Backhaul (IAB) constraints. The high-layer decision process determines the number of MAPs through consensus, and we develop a joint optimization process to account for co-dependence in network self-management. In the low-layer, MAPs manage their placement using a double-attention based Deep Reinforcement Learning (DRL) model that encourages cooperation without retraining. To improve generalization and reduce complexity, we propose a federated mechanism for training and sharing one placement model for every MAP in the low-layer. Additionally, we jointly optimize the placement and backhaul connectivity of MAPs using a multi-objective reward function, considering the impact of varying MAP placement on wireless backhaul connectiv
    
[^19]: 集成学习用于混合格网卫星和测量降水数据

    Ensemble learning for blending gridded satellite and gauge-measured precipitation data. (arXiv:2307.06840v1 [cs.LG])

    [http://arxiv.org/abs/2307.06840](http://arxiv.org/abs/2307.06840)

    本研究填补论文领域中的空白，提出了11个新的集成学习算法并对其进行了广泛的比较，旨在改进卫星降水产品的准确性。

    

    在改善卫星降水产品准确性方面，常常使用回归算法。在这种情况下，基于地面的测量是因变量，卫星数据是预测变量，还有地形因素。与此同时，各个领域越来越认识到通过集成学习将多个算法结合起来可以显著提高预测性能。然而，目前文献中缺乏足够数量的集成学习算法以提高卫星降水产品的准确性，并对它们进行大规模比较。本文通过在整个美国和15年时间段内进行广泛比较，填补了这个特定的空白，提出了11个新的集成学习算法。我们使用PERSIANN（使用人工神经网络的遥感信息估计降水）和IMERG（多星联合反演估计降水）的月度数据。

    Regression algorithms are regularly used for improving the accuracy of satellite precipitation products. In this context, ground-based measurements are the dependent variable and the satellite data are the predictor variables, together with topography factors. Alongside this, it is increasingly recognised in many fields that combinations of algorithms through ensemble learning can lead to substantial predictive performance improvements. Still, a sufficient number of ensemble learners for improving the accuracy of satellite precipitation products and their large-scale comparison are currently missing from the literature. In this work, we fill this specific gap by proposing 11 new ensemble learners in the field and by extensively comparing them for the entire contiguous United States and for a 15-year period. We use monthly data from the PERSIANN (Precipitation Estimation from Remotely Sensed Information using Artificial Neural Networks) and IMERG (Integrated Multi-satellitE Retrievals f
    
[^20]: PC-Droid: 更快的扩散速度和改进的粒子云生成质量

    PC-Droid: Faster diffusion and improved quality for particle cloud generation. (arXiv:2307.06836v1 [hep-ex])

    [http://arxiv.org/abs/2307.06836](http://arxiv.org/abs/2307.06836)

    PC-Droid是一个新的扩散模型，通过利用新的扩散公式和研究更近期的积分求解器，同时对所有类型的喷注进行训练，实现了最先进的性能。它不仅能提供更快的生成速度，而且在所有评估指标上都具有卓越的性能。

    

    在PC-JeDi的基础上，我们引入了PC-Droid，这是一个大幅改进的扩散模型，用于生成喷注粒子云。通过利用新的扩散公式、研究更近期的积分求解器，并同时对所有类型的喷注进行训练，我们能够在所有评估指标上实现最先进的性能。我们通过比较基于注意力的两种体系结构和一致性蒸馏的潜力来研究生成速度和质量之间的权衡。更快的体系结构和一致性模型都表现出超越许多竞争模型的性能，生成时间比PC-JeDi快上两个数量级。

    Building on the success of PC-JeDi we introduce PC-Droid, a substantially improved diffusion model for the generation of jet particle clouds. By leveraging a new diffusion formulation, studying more recent integration solvers, and training on all jet types simultaneously, we are able to achieve state-of-the-art performance for all types of jets across all evaluation metrics. We study the trade-off between generation speed and quality by comparing two attention based architectures, as well as the potential of consistency distillation to reduce the number of diffusion steps. Both the faster architecture and consistency models demonstrate performance surpassing many competing models, with generation time up to two orders of magnitude faster than PC-JeDi.
    
[^21]: 在联合mmWave网络中提高可靠性：一种使用雷达辅助动态阻挡识别的实用可扩展解决方案

    Enhancing Reliability in Federated mmWave Networks: A Practical and Scalable Solution using Radar-Aided Dynamic Blockage Recognition. (arXiv:2307.06834v1 [cs.NI])

    [http://arxiv.org/abs/2307.06834](http://arxiv.org/abs/2307.06834)

    本文提出了一种利用雷达辅助动态阻挡识别的方法来提高动态户外环境中毫米波和太赫兹网络服务的可靠性。通过联合学习和神经网络模型，可以同时预测阻挡状态和时间，并通过主动切换或波束切换减少延迟，以确保高质量的用户体验。

    

    本文介绍了一种在动态户外环境中提高毫米波（mmWave）和太赫兹（THz）网络服务可靠性的新方法。在这些环境中，视距（LoS）连接很容易被移动的障碍物（如人和车辆）打断。所提出的方法被称为雷达辅助动态阻塞识别（RaDaR），利用雷达测量和联合学习（FL）来训练一个双输出神经网络（NN）模型，能够同时预测阻塞状态和时间。这可以确定主动切换（PHO）或波束切换的最佳点，从而减少5G新无线电程序引入的延迟，并确保高质量的体验（QoE）。该框架利用雷达传感器来监控和跟踪物体的运动，生成有用于场景分析和预测的距离-角度和距离-速度图。

    This article introduces a new method to improve the dependability of millimeter-wave (mmWave) and terahertz (THz) network services in dynamic outdoor environments. In these settings, line-of-sight (LoS) connections are easily interrupted by moving obstacles like humans and vehicles. The proposed approach, coined as Radar-aided Dynamic blockage Recognition (RaDaR), leverages radar measurements and federated learning (FL) to train a dual-output neural network (NN) model capable of simultaneously predicting blockage status and time. This enables determining the optimal point for proactive handover (PHO) or beam switching, thereby reducing the latency introduced by 5G new radio procedures and ensuring high quality of experience (QoE). The framework employs radar sensors to monitor and track objects movement, generating range-angle and range-velocity maps that are useful for scene analysis and predictions. Moreover, FL provides additional benefits such as privacy protection, scalability, an
    
[^22]: 一种新的贝叶斯定理用于上概率

    A Novel Bayes' Theorem for Upper Probabilities. (arXiv:2307.06831v1 [stat.ML])

    [http://arxiv.org/abs/2307.06831](http://arxiv.org/abs/2307.06831)

    本文推广了瓦塞尔曼和卡代纳的结果，给出了一种新的贝叶斯定理用于处理与似然函数相关的不确定性。该结果对于工程、机器学习和人工智能领域具有潜在应用价值。

    

    在他们1990年的开创性论文中，瓦塞尔曼和卡代纳建立了在先验概率位于概率类别P，且似然函数是精确函数时，可测集A的贝叶斯后验概率的上限。他们还给出了这种上限成立时的充分条件。本文中，我们通过额外处理与似然函数相关的不确定性来引入他们结果的推广。我们给出了当先验概率和似然函数都属于一组概率时的后验概率上限，并且给出了这种上限成为等式的充分条件。这个结果本身很有趣，并且有可能应用于工程领域（例如模型预测控制）、机器学习和人工智能。

    In their seminal 1990 paper, Wasserman and Kadane establish an upper bound for the Bayes' posterior probability of a measurable set $A$, when the prior lies in a class of probability measures $\mathcal{P}$ and the likelihood is precise. They also give a sufficient condition for such upper bound to hold with equality. In this paper, we introduce a generalization of their result by additionally addressing uncertainty related to the likelihood. We give an upper bound for the posterior probability when both the prior and the likelihood belong to a set of probabilities. Furthermore, we give a sufficient condition for this upper bound to become an equality. This result is interesting on its own, and has the potential of being applied to various fields of engineering (e.g. model predictive control), machine learning, and artificial intelligence.
    
[^23]: 一个统一常见领域泛化方法的因果框架

    A Causal Framework to Unify Common Domain Generalization Approaches. (arXiv:2307.06825v1 [cs.LG])

    [http://arxiv.org/abs/2307.06825](http://arxiv.org/abs/2307.06825)

    本文提出了一个因果框架用于统一常见领域泛化方法的理解，通过回答关键思想、理论基础和方法关系等问题，帮助研究者更好地理解和发展领域泛化方法。

    

    领域泛化(DG)是指学习能够很好地推广到与训练领域相关但不同的新领域的模型。它是机器学习中的一个基本问题，并在最近几年引起了很多关注。已提出了大量不同的方法。不同的方法从不同的角度出发，使得很难对这个领域有一个整体的理解。在本文中，我们提出了一个用于领域泛化的因果框架，并在框架中对常见的DG方法进行了理解。我们的工作为以下问题提供了新的观点：(1)每个DG方法背后的关键思想是什么？(2)理论上为什么期望它改善对新领域的推广能力？(3)不同的DG方法之间如何相关，有什么相对优势和局限性？通过提供对DG的统一视角，我们希望能帮助研究者更好地理解其中的原则并开发出更有效的方法。

    Domain generalization (DG) is about learning models that generalize well to new domains that are related to, but different from, the training domain(s). It is a fundamental problem in machine learning and has attracted much attention in recent years. A large number of approaches have been proposed. Different approaches are motivated from different perspectives, making it difficult to gain an overall understanding of the area. In this paper, we propose a causal framework for domain generalization and present an understanding of common DG approaches in the framework. Our work sheds new lights on the following questions: (1) What are the key ideas behind each DG method? (2) Why is it expected to improve generalization to new domains theoretically? (3) How are different DG methods related to each other and what are relative advantages and limitations? By providing a unified perspective on DG, we hope to help researchers better understand the underlying principles and develop more effective
    
[^24]: CLAIMED -- 在科学加速发现中构建粗粒度运算符的开放源代码框架

    CLAIMED -- the open source framework for building coarse-grained operators for accelerated discovery in science. (arXiv:2307.06824v1 [cs.AI])

    [http://arxiv.org/abs/2307.06824](http://arxiv.org/abs/2307.06824)

    CLAIMED是一个开放源代码框架，用于在现代数据驱动的科学中构建可重用的运算符和可扩展的科学工作流程，从而解决了重复性和可重用性问题。

    

    在现代数据驱动的科学中，重复性和重用性是关键挑战。科学家在从数据到出版的过程中有着丰富的经验。尽管一些出版渠道要求源代码和数据可获得，但重新运行和验证实验通常很困难，原因是缺乏标准。因此，重用现有的科学数据处理代码来自最新研究是困难的。这就是为什么我们引入CLAIMED，它在现代数据驱动的科学中解决了重复性和可重用性问题的科学研究实践。CLAIMED是一个框架，通过支持科学家从现有的粗粒度科学运算符库中重新组合工作流程，来构建可重用的运算符和可扩展的科学工作流程。尽管有各种实现，但CLAIMED是编程语言、科学库和执行环境无关的。

    In modern data-driven science, reproducibility and reusability are key challenges. Scientists are well skilled in the process from data to publication. Although some publication channels require source code and data to be made accessible, rerunning and verifying experiments is usually hard due to a lack of standards. Therefore, reusing existing scientific data processing code from state-of-the-art research is hard as well. This is why we introduce CLAIMED, which has a proven track record in scientific research for addressing the repeatability and reusability issues in modern data-driven science. CLAIMED is a framework to build reusable operators and scalable scientific workflows by supporting the scientist to draw from previous work by re-composing workflows from existing libraries of coarse-grained scientific operators. Although various implementations exist, CLAIMED is programming language, scientific library, and execution environment agnostic.
    
[^25]: TinyMetaFed: 高效的用于TinyML的联邦元学习

    TinyMetaFed: Efficient Federated Meta-Learning for TinyML. (arXiv:2307.06822v1 [cs.LG])

    [http://arxiv.org/abs/2307.06822](http://arxiv.org/abs/2307.06822)

    TinyMetaFed是一个适用于TinyML的高效联邦元学习框架，通过协同训练神经网络初始化，在小型设备上能够快速微调，同时实现通信节省和隐私保护。

    

    Tiny Machine Learning (TinyML)领域在使得机器学习在低功耗设备（如微控制器）上实现方面取得了重大进展。这些微型设备的普及引发了一个问题，即聚合它们的知识是否能够使TinyML应用受益。联邦元学习是这个问题的一个有前景的答案，因为它解决了现实世界中标记数据的稀缺性和设备之间的异构数据分布。然而，部署TinyML硬件面临着独特的资源限制，现有方法由于能源、隐私和通信限制而不实用。我们引入了TinyMetaFed，一个适用于TinyML的模型无关的元学习框架。TinyMetaFed促进了神经网络初始化的协同训练，可以在新设备上快速微调。它通过部分本地重构和Top-P%选择性通信提供通信节省和隐私保护，具有计算效果好。

    The field of Tiny Machine Learning (TinyML) has made substantial advancements in democratizing machine learning on low-footprint devices, such as microcontrollers. The prevalence of these miniature devices raises the question of whether aggregating their knowledge can benefit TinyML applications. Federated meta-learning is a promising answer to this question, as it addresses the scarcity of labeled data and heterogeneous data distribution across devices in the real world. However, deploying TinyML hardware faces unique resource constraints, making existing methods impractical due to energy, privacy, and communication limitations. We introduce TinyMetaFed, a model-agnostic meta-learning framework suitable for TinyML. TinyMetaFed facilitates collaborative training of a neural network initialization that can be quickly fine-tuned on new devices. It offers communication savings and privacy protection through partial local reconstruction and Top-P% selective communication, computational eff
    
[^26]: 使用学习数字后向传播在色散管理系统中进行均衡

    Equalization in Dispersion-Managed Systems Using Learned Digital Back-Propagation. (arXiv:2307.06821v1 [cs.NI])

    [http://arxiv.org/abs/2307.06821](http://arxiv.org/abs/2307.06821)

    本文研究了使用学习数字后向传播（LDBP）来均衡双极化光纤传输中的色散管理（DM）链路。实验结果表明，在单信道和WDM传输中，LDBP相比于线性均衡和DBP分别实现了显著的信噪比改善和Q因子增益。此外，频域实现LDBP和DBP在复杂性上更具优势。

    

    本文研究了使用学习数字后向传播（LDBP）来均衡双极化光纤传输中的色散管理（DM）链路。 LDBP是一个深度神经网络，使用随机梯度下降优化DBP的参数。我们在一个模拟的WDM双极化光纤传输系统中评估了DBP和LDBP，该系统以每个通道256 Gbit / s的比特率运行，具有为2016 km链路设计的15％剩余色散的色散映射。我们的结果显示，在单信道传输中，LDBP分别比线性均衡和DBP实现了6.3 dB和2.5 dB的有效信噪比改善。在WDM传输中，相应的$Q$因子增益分别为1.1 dB和0.4 dB。此外，我们进行了复杂性分析，发现频域实现LDBP和DBP在复杂性方面比时域实现更有优势。这些发现证明了LDBP在均衡色散管理系统中的有效性和可行性。

    In this paper, we investigate the use of the learned digital back-propagation (LDBP) for equalizing dual-polarization fiber-optic transmission in dispersion-managed (DM) links. LDBP is a deep neural network that optimizes the parameters of DBP using the stochastic gradient descent. We evaluate DBP and LDBP in a simulated WDM dual-polarization fiber transmission system operating at the bitrate of 256 Gbit/s per channel, with a dispersion map designed for a 2016 km link with 15% residual dispersion. Our results show that in single-channel transmission, LDBP achieves an effective signal-to-noise ratio improvement of 6.3 dB and 2.5 dB, respectively, over linear equalization and DBP. In WDM transmission, the corresponding $Q$-factor gains are 1.1 dB and 0.4 dB, respectively. Additionally, we conduct a complexity analysis, which reveals that a frequency-domain implementation of LDBP and DBP is more favorable in terms of complexity than the time-domain implementation. These findings demonstra
    
[^27]: 基于深层分层变分自编码器的数据驱动非线性参数模型降阶方法

    Data-driven Nonlinear Parametric Model Order Reduction Framework using Deep Hierarchical Variational Autoencoder. (arXiv:2307.06816v1 [cs.LG])

    [http://arxiv.org/abs/2307.06816](http://arxiv.org/abs/2307.06816)

    提出了一种使用深度分层变分自编码器的数据驱动非线性参数模型降阶方法，能够提高准确性和稳定性，并在实际系统上进行了验证和评估。

    

    提出了一种使用深度人工神经网络的数据驱动参数模型降阶（MOR）方法。该方法采用最小二乘分层变分自编码器（LSH-VAE），能够针对具有大量自由度的非线性动态系统进行参数插值的非线性MOR。LSH-VAE通过两个主要改变提高了现有网络的准确性和稳定性：分层深度结构和混合加权概率损失函数。与传统的非线性MOR方法、自编码器和变分自编码器相比，这些改进极大地提高了精度和稳定性。基于LSH-VAE，提出了一个基于潜在流形球线性插值的参数MOR框架。这个框架在三个非线性和多物理动态系统上进行了验证和评估。

    A data-driven parametric model order reduction (MOR) method using a deep artificial neural network is proposed. The present network, which is the least-squares hierarchical variational autoencoder (LSH-VAE), is capable of performing nonlinear MOR for the parametric interpolation of a nonlinear dynamic system with a significant number of degrees of freedom. LSH-VAE exploits two major changes to the existing networks: a hierarchical deep structure and a hybrid weighted, probabilistic loss function. The enhancements result in a significantly improved accuracy and stability compared against the conventional nonlinear MOR methods, autoencoder, and variational autoencoder. Upon LSH-VAE, a parametric MOR framework is presented based on the spherically linear interpolation of the latent manifold. The present framework is validated and evaluated on three nonlinear and multiphysics dynamic systems. First, the present framework is evaluated on the fluid-structure interaction benchmark problem to 
    
[^28]: 基于非平衡物理学的快速且功能性结构化数据生成器

    Fast and Functional Structured Data Generators Rooted in Out-of-Equilibrium Physics. (arXiv:2307.06797v1 [cs.LG])

    [http://arxiv.org/abs/2307.06797](http://arxiv.org/abs/2307.06797)

    这项研究提出了一种基于非平衡物理学的训练算法，用于解决使用能量模型生成高质量结构化数据的挑战。该方法通过改善模型的分类能力和生成速度，在多个领域取得了成功应用。

    

    在这项研究中，我们解决了使用基于能量的模型在复杂结构化数据集（如人口基因组学、RNA或蛋白质序列数据）中生成高质量、标签特定数据的挑战。传统的训练方法由于马尔可夫链蒙特卡洛混合效率低下而遇到困难，这影响了合成数据的多样性并增加了生成时间。为了解决这些问题，我们使用了一种利用非平衡效应的新型训练算法。这种方法应用于受限玻尔兹曼机，提高了模型对样本的正确分类能力，并只需少数几个采样步骤即可生成高质量的合成数据。该方法的有效性通过其成功应用于四种不同类型的数据得到证明：手写数字，按大陆起源分类的人类基因组突变，酶蛋白家族的功能序列，以及特定分类法的同源RNA序列。

    In this study, we address the challenge of using energy-based models to produce high-quality, label-specific data in complex structured datasets, such as population genetics, RNA or protein sequences data. Traditional training methods encounter difficulties due to inefficient Markov chain Monte Carlo mixing, which affects the diversity of synthetic data and increases generation times. To address these issues, we use a novel training algorithm that exploits non-equilibrium effects. This approach, applied on the Restricted Boltzmann Machine, improves the model's ability to correctly classify samples and generate high-quality synthetic data in only a few sampling steps. The effectiveness of this method is demonstrated by its successful application to four different types of data: handwritten digits, mutations of human genomes classified by continental origin, functionally characterized sequences of an enzyme protein family, and homologous RNA sequences from specific taxonomies.
    
[^29]: 机器人使用视觉和触觉传感进行表面探测以实现裂缝的检测和特征化

    Robotic surface exploration with vision and tactile sensing for cracks detection and characterisation. (arXiv:2307.06784v1 [cs.RO])

    [http://arxiv.org/abs/2307.06784](http://arxiv.org/abs/2307.06784)

    本文介绍了一种利用光纤的视觉和触觉传感进行裂缝定位和检测的算法，并提出了一个用于裂缝探测的机器人运动规划器和触觉数据分类方法。

    

    本文介绍了一种基于光纤的视觉和触觉分析的裂缝定位和检测算法。利用光纤制成的指状传感器用于数据采集，以收集分析和实验所需的数据。为了检测裂缝的可能位置，使用摄像头对环境进行扫描，并运行对象检测算法。一旦检测到裂缝，从裂缝的骨架化版本创建一个全连接图。然后采用最小生成树计算探测裂缝的最短路径，用于为机器人操作已开发的运动规划器。运动规划器将裂缝划分成多个节点，然后逐个探测。然后，机械臂开始探测并执行触觉数据分类，以确认该位置是否真的存在裂缝，或者只是视觉算法的误报。如果检测到裂缝，则还进行裂缝定量特征化的分析。

    This paper presents a novel algorithm for crack localisation and detection based on visual and tactile analysis via fibre-optics. A finger-shaped sensor based on fibre-optics is employed for the data acquisition to collect data for the analysis and the experiments. To detect the possible locations of cracks a camera is used to scan an environment while running an object detection algorithm. Once the crack is detected, a fully-connected graph is created from a skeletonised version of the crack. A minimum spanning tree is then employed for calculating the shortest path to explore the crack which is then used to develop the motion planner for the robotic manipulator. The motion planner divides the crack into multiple nodes which are then explored individually. Then, the manipulator starts the exploration and performs the tactile data classification to confirm if there is indeed a crack in that location or just a false positive from the vision algorithm. If a crack is detected, also the le
    
[^30]: 一种新型的与平台无关的多模态深度学习模型，用于识别社交媒体上的促进饮食紊乱内容

    A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media. (arXiv:2307.06775v1 [cs.LG])

    [http://arxiv.org/abs/2307.06775](http://arxiv.org/abs/2307.06775)

    本研究创建了一个多模态深度学习模型，将文本和视觉数据相结合，能够准确识别社交媒体上的促进饮食紊乱的内容。最有效的模型是RoBERTa自然语言处理模型和MaxViT图像分类模型的融合模型，准确率和F1分数分别达到95.9%和0.959。

    

    在过去的十年中，饮食紊乱的诊断和与之相关的死亡数量大幅增加，尤其是在新冠疫情期间。这种巨大增长部分来源于疫情的压力，但也与社交媒体的暴露增加有关，社交媒体上充斥着促进饮食紊乱的内容。这些内容可以诱发观看者的饮食紊乱。本研究旨在创建一个多模态深度学习模型，能够基于视觉和文本数据的组合判断给定的社交媒体帖子是否促进饮食紊乱。从Twitter收集了一个带有标签的推文数据集，对其进行了十二个深度学习模型的训练和测试。根据模型的性能，最有效的深度学习模型是RoBERTa自然语言处理模型和MaxViT图像分类模型的多模态融合模型，准确率和F1分数分别达到95.9%和0.959。RoBERTa和MaxViT融合模型可以有效地识别社交媒体上的促进饮食紊乱的内容。

    Over the last decade, there has been a vast increase in eating disorder diagnoses and eating disorder-attributed deaths, reaching their zenith during the Covid-19 pandemic. This immense growth derived in part from the stressors of the pandemic but also from increased exposure to social media, which is rife with content that promotes eating disorders. Such content can induce eating disorders in viewers. This study aimed to create a multimodal deep learning model capable of determining whether a given social media post promotes eating disorders based on a combination of visual and textual data. A labeled dataset of Tweets was collected from Twitter, upon which twelve deep learning models were trained and tested. Based on model performance, the most effective deep learning model was the multimodal fusion of the RoBERTa natural language processing model and the MaxViT image classification model, attaining accuracy and F1 scores of 95.9% and 0.959 respectively. The RoBERTa and MaxViT fusion
    
[^31]: 将监督深度学习MRI重建推广到多种和未知对比度的元学习超网络

    Generalizing Supervised Deep Learning MRI Reconstruction to Multiple and Unseen Contrasts using Meta-Learning Hypernetworks. (arXiv:2307.06771v1 [eess.IV])

    [http://arxiv.org/abs/2307.06771](http://arxiv.org/abs/2307.06771)

    本文提出了一种名为KM-MAML的多模态元学习模型，通过演化能力和超网络生成模态特定的权重，以用于多种和未知对比度的图像重建。

    

    元学习是一种最近兴起的数据高效学习技术，可用于各种医学图像操作，并有助于推进当代深度学习模型的发展。此外，元学习通过学习用于各种图像任务的共享和判别权重来增强图像任务的知识推广能力。然而，现有的元学习模型试图学习单个神经网络的权重初始化集合，这可能对于多模态数据来说是有限制的。本文旨在开发一种多模态元学习模型，用于图像重建，并通过演化能力来包括多样化的多模态数据采集设置。我们提出的模型称为KM-MAML（基于核调制的多模态元学习），具有进化的超网络，用于生成模态特定的权重。这些权重通过重新校准基网络的每个核，为多个模式提供模式特定的归纳偏见。

    Meta-learning has recently been an emerging data-efficient learning technique for various medical imaging operations and has helped advance contemporary deep learning models. Furthermore, meta-learning enhances the knowledge generalization of the imaging tasks by learning both shared and discriminative weights for various configurations of imaging tasks. However, existing meta-learning models attempt to learn a single set of weight initializations of a neural network that might be restrictive for multimodal data. This work aims to develop a multimodal meta-learning model for image reconstruction, which augments meta-learning with evolutionary capabilities to encompass diverse acquisition settings of multimodal data. Our proposed model called KM-MAML (Kernel Modulation-based Multimodal Meta-Learning), has hypernetworks that evolve to generate mode-specific weights. These weights provide the mode-specific inductive bias for multiple modes by re-calibrating each kernel of the base network
    
[^32]: 医疗人口图中神经网络的隐私与效用权衡：来自差分隐私和图结构的观点

    Privacy-Utility Trade-offs in Neural Networks for Medical Population Graphs: Insights from Differential Privacy and Graph Structure. (arXiv:2307.06760v1 [cs.LG])

    [http://arxiv.org/abs/2307.06760](http://arxiv.org/abs/2307.06760)

    本文通过对医疗领域的人口图进行差分隐私图神经网络的实证研究，调查了隐私和效用之间的权衡，并通过成员推理攻击进行审核。研究结果表明这一特定差分隐私应用领域具有潜力和挑战，并发现图的同质性程度与训练模型的准确性之间存在相关性。

    

    本文通过对医疗领域的人口图进行差分隐私图神经网络的实证研究，调查了在不同隐私水平下的隐私与效用的权衡，并通过成员推理攻击进行审核。我们的研究结果突出了这一特定差分隐私应用领域的潜力和挑战。此外，我们发现底层图结构与训练模型的准确性之间存在相关性，表明图的同质性程度与训练模型的准确性之间存在潜在因素，这可能导致更大的性能差距。

    We initiate an empirical investigation into differentially private graph neural networks on population graphs from the medical domain by examining privacy-utility trade-offs at different privacy levels on both real-world and synthetic datasets and performing auditing through membership inference attacks. Our findings highlight the potential and the challenges of this specific DP application area. Moreover, we find evidence that the underlying graph structure constitutes a potential factor for larger performance gaps by showing a correlation between the degree of graph homophily and the accuracy of the trained model.
    
[^33]: 用梯度下降学习高斯混合模型的Cramer距离

    Cramer Type Distances for Learning Gaussian Mixture Models by Gradient Descent. (arXiv:2307.06753v1 [cs.LG])

    [http://arxiv.org/abs/2307.06753](http://arxiv.org/abs/2307.06753)

    本文提出了一种用于学习高斯混合模型的Cramer距离，该距离函数在多元情况下具有闭式表达式，并且易于计算和实现，并在梯度下降算法中有效。

    

    高斯混合模型的学习在机器学习中起着重要作用。高斯混合模型以其表达力和可解释性而闻名，广泛应用于统计学、计算机视觉和分布式强化学习等领域。然而，目前为止，很少有已知算法可以拟合或学习这些模型，其中一些包括期望最大化算法和分割Wasserstein距离。与梯度下降相兼容的算法更少，这是神经网络的常见学习过程。在本文中，我们推导了一维情况下两个高斯混合模型的闭式公式，然后提出了一种称为Sliced Cramer 2距离的距离函数，用于学习一般的多元高斯混合模型。我们的方法比许多先前方法具有几个优点。首先，在一维情况下具有闭式表达式，并且可以使用常见的机器学习库（例如PyTorch）进行易于计算和实现的操作。

    The learning of Gaussian Mixture Models (also referred to simply as GMMs) plays an important role in machine learning. Known for their expressiveness and interpretability, Gaussian mixture models have a wide range of applications, from statistics, computer vision to distributional reinforcement learning. However, as of today, few known algorithms can fit or learn these models, some of which include Expectation-Maximization algorithms and Sliced Wasserstein Distance. Even fewer algorithms are compatible with gradient descent, the common learning process for neural networks.  In this paper, we derive a closed formula of two GMMs in the univariate, one-dimensional case, then propose a distance function called Sliced Cram\'er 2-distance for learning general multivariate GMMs. Our approach has several advantages over many previous methods. First, it has a closed-form expression for the univariate case and is easy to compute and implement using common machine learning libraries (e.g., PyTorc
    
[^34]: 基于多代理层次强化学习方法的即时城际拼车服务的车辆调度和路径规划

    Vehicle Dispatching and Routing of On-Demand Intercity Ride-Pooling Services: A Multi-Agent Hierarchical Reinforcement Learning Approach. (arXiv:2307.06742v1 [eess.SY])

    [http://arxiv.org/abs/2307.06742](http://arxiv.org/abs/2307.06742)

    本研究提出了一个基于多代理层次强化学习的框架，用于即时城际拼车服务的车辆调度和路径规划。数值研究证明该框架有效缓解了供给不足问题。

    

    城市群一体化的发展导致了对城际旅行的不断增长需求。城际拼车服务通过实施需求响应性增强措施，有望升级传统的城际客车服务。然而，其在线操作受到车辆资源分配和拼车车辆路径规划之间耦合性的固有复杂性的影响。为了解决这些挑战，本研究提出了一个两层框架，旨在促进在线车队管理。具体而言，在框架的上层，提出了一种新颖的多代理封建强化学习模型，用于协同分配闲置车辆到不同的城际线路，而在下层，则使用自适应大邻域搜索启发式算法更新车辆的路线。基于中国厦门及其周边城市的真实数据集进行的数值研究表明，所提出的框架有效地缓解了供给不足问题。

    The integrated development of city clusters has given rise to an increasing demand for intercity travel. Intercity ride-pooling service exhibits considerable potential in upgrading traditional intercity bus services by implementing demand-responsive enhancements. Nevertheless, its online operations suffer the inherent complexities due to the coupling of vehicle resource allocation among cities and pooled-ride vehicle routing. To tackle these challenges, this study proposes a two-level framework designed to facilitate online fleet management. Specifically, a novel multi-agent feudal reinforcement learning model is proposed at the upper level of the framework to cooperatively assign idle vehicles to different intercity lines, while the lower level updates the routes of vehicles using an adaptive large neighborhood search heuristic. Numerical studies based on the realistic dataset of Xiamen and its surrounding cities in China show that the proposed framework effectively mitigates the supp
    
[^35]: MPR-Net: 多尺度模式重现引导的通用时间序列可解释性预测

    MPR-Net:Multi-Scale Pattern Reproduction Guided Universality Time Series Interpretable Forecasting. (arXiv:2307.06736v1 [cs.LG])

    [http://arxiv.org/abs/2307.06736](http://arxiv.org/abs/2307.06736)

    本文提出了一种名为MPR-Net的预测模型，通过自适应地分解多尺度历史序列模式，并基于模式重现的先验知识构建模式扩展预测方法，最后使用反卷积运算将未来模式重建为未来序列。MPR-Net能够有效地捕捉时间序列中的重要模式并实现可解释的预测。

    

    时间序列预测由于其广泛的应用和固有的挑战性而受到了广泛的关注。研究的挑战在于识别历史序列中的有效模式，并将其应用于未来的预测。基于点对点连接的多层感知机和Transformer架构的先进模型具有强大的拟合能力，但其辅助计算复杂度限制了实用性。此外，这些结构固有地破坏了时间顺序，降低了信息利用率，并使预测过程解释性降低。为了解决这些问题，本文提出了一种预测模型MPR-Net。它首先使用卷积运算自适应地分解多尺度历史序列模式，然后基于模式重现的先验知识构建一种模式扩展预测方法，最后使用反卷积运算将未来模式重建为未来序列。通过利用时间依赖性，MPR-Net能够有效地捕捉时间序列中的重要模式并实现可解释的预测。

    Time series forecasting has received wide interest from existing research due to its broad applications and inherent challenging. The research challenge lies in identifying effective patterns in historical series and applying them to future forecasting. Advanced models based on point-wise connected MLP and Transformer architectures have strong fitting power, but their secondary computational complexity limits practicality. Additionally, those structures inherently disrupt the temporal order, reducing the information utilization and making the forecasting process uninterpretable. To solve these problems, this paper proposes a forecasting model, MPR-Net. It first adaptively decomposes multi-scale historical series patterns using convolution operation, then constructs a pattern extension forecasting method based on the prior knowledge of pattern reproduction, and finally reconstructs future patterns into future series using deconvolution operation. By leveraging the temporal dependencies 
    
[^36]: 在对数级别的轮数中打破关联聚类的3因子逼近

    Breaking 3-Factor Approximation for Correlation Clustering in Polylogarithmic Rounds. (arXiv:2307.06723v1 [cs.DS])

    [http://arxiv.org/abs/2307.06723](http://arxiv.org/abs/2307.06723)

    本文提出了第一个具有更好逼近比的对数级别深度并行算法来解决关联聚类问题。

    

    本文研究了关联聚类问题的并行算法，其中每对两个不同的实体被标记为相似或不相似。目标是将实体分成聚类，以最小化与标签的不一致数量。目前，所有高效的并行算法的逼近比至少为3。与多项式时间顺序算法[CLN22]获得的$1.994+\epsilon$比率相比，存在显著差距。我们提出了第一个具有更好逼近比的对数级别深度并行算法，具体地，我们的算法计算出一个$(2.4+\epsilon)$-近似解，并使用$\tilde{O}(m^{1.5})$的工作。此外，它可以转化为一个$\tilde{O}(m^{1.5})$时间顺序算法和一个具有$\tilde{O}(m^{1.5})$总内存的对数级别轮数亚线性内存MPC算法。我们的方法受到了Awerbuch，Khandekar和Rao的[AKR12]受长度约束的m的启发。

    In this paper, we study parallel algorithms for the correlation clustering problem, where every pair of two different entities is labeled with similar or dissimilar. The goal is to partition the entities into clusters to minimize the number of disagreements with the labels. Currently, all efficient parallel algorithms have an approximation ratio of at least 3. In comparison with the $1.994+\epsilon$ ratio achieved by polynomial-time sequential algorithms [CLN22], a significant gap exists.  We propose the first poly-logarithmic depth parallel algorithm that achieves a better approximation ratio than 3. Specifically, our algorithm computes a $(2.4+\epsilon)$-approximate solution and uses $\tilde{O}(m^{1.5})$ work. Additionally, it can be translated into a $\tilde{O}(m^{1.5})$-time sequential algorithm and a poly-logarithmic rounds sublinear-memory MPC algorithm with $\tilde{O}(m^{1.5})$ total memory.  Our approach is inspired by Awerbuch, Khandekar, and Rao's [AKR12] length-constrained m
    
[^37]: 为什么导向式对话策略学习表现优秀？理解对抗学习及其替代方法的作用。

    Why Guided Dialog Policy Learning performs well? Understanding the role of adversarial learning and its alternative. (arXiv:2307.06721v1 [cs.CL])

    [http://arxiv.org/abs/2307.06721](http://arxiv.org/abs/2307.06721)

    本论文通过分析对话策略和奖励估计器的目标函数，解释了对抗学习在对话策略学习中的作用，并提出了一种替代方法。

    

    对话策略是根据每个对话轮次的当前状态确定系统动作的关键。近年来，强化学习 (RL) 已成为对话策略学习 (DPL) 的一种有前途的选择。在基于 RL 的 DPL 中，根据奖励更新对话策略。对于具有大量状态动作对组合的多领域任务导向型对话场景，精细构建像状态-动作相关的奖励来有效指导对话策略是具有挑战性的。一种从收集到的数据中估计奖励的方式是使用对抗学习 (AL) 同时训练奖励估计器和对话策略。尽管这种方法在实验中表现出优越的性能，但其固有的对抗学习问题（例如模式坍缩）也十分棘手。本文通过对对话策略和奖励估计器的目标函数进行详细分析，首先确定了 AL 在 DPL 中的作用。接下来，基于此，该论文提出了一种替代方法。

    Dialog policies, which determine a system's action based on the current state at each dialog turn, are crucial to the success of the dialog. In recent years, reinforcement learning (RL) has emerged as a promising option for dialog policy learning (DPL). In RL-based DPL, dialog policies are updated according to rewards. The manual construction of fine-grained rewards, such as state-action-based ones, to effectively guide the dialog policy is challenging in multi-domain task-oriented dialog scenarios with numerous state-action pair combinations. One way to estimate rewards from collected data is to train the reward estimator and dialog policy simultaneously using adversarial learning (AL). Although this method has demonstrated superior performance experimentally, it is fraught with the inherent problems of AL, such as mode collapse. This paper first identifies the role of AL in DPL through detailed analyses of the objective functions of dialog policy and reward estimator. Next, based on 
    
[^38]: 使用大型语言模型实现无监督校准的文本分类方法的先验适应

    Unsupervised Calibration through Prior Adaptation for Text Classification using Large Language Models. (arXiv:2307.06713v1 [cs.CL])

    [http://arxiv.org/abs/2307.06713](http://arxiv.org/abs/2307.06713)

    本文提出了一种使用大型语言模型进行文本分类的无监督校准方法，通过调整先验类别分布，实现在没有标记样本和仅少量领域内样本查询的情况下执行任务。

    

    当前有许多自然语言任务正在使用大规模语言模型（LLM）进行研究。这些模型通常通过大量无监督文本数据进行训练，并通过微调、校准或上下文学习等方法进行适应以执行下游自然语言任务。在本研究中，我们提出了一种方法，通过调整先验类别分布，实现在没有标记样本和仅少量领域内样本查询的情况下执行文本分类任务。该方法将LLM视为黑盒，在模型屏障中添加了一个阶段，用于校准模型后验以完成任务。结果表明，这些方法在不同数量的提示训练样本和无适应数据下的校准方法中优于未适应的模型。

    A wide variety of natural language tasks are currently being addressed with large-scale language models (LLMs). These models are usually trained with a very large amount of unsupervised text data and adapted to perform a downstream natural language task using methods like fine-tuning, calibration or in-context learning. In this work, we propose an approach to adapt the prior class distribution to perform text classification tasks without the need for labelled samples and only few in-domain sample queries. The proposed approach treats the LLM as a black box, adding a stage where the model posteriors are calibrated to the task. Results show that these methods outperform the un-adapted model for different number of training shots in the prompt and a previous approach were calibration is performed without using any adaptation data.
    
[^39]: GRAN优于GraphRNN：基于节点排序、核函数和图嵌入的度量用于图生成器

    GRAN is superior to GraphRNN: node orderings, kernel- and graph embeddings-based metrics for graph generators. (arXiv:2307.06709v1 [cs.LG])

    [http://arxiv.org/abs/2307.06709](http://arxiv.org/abs/2307.06709)

    本论文通过比较基于节点排序、核函数和图嵌入的度量，展示了GRAN优于GraphRNN的优势，并为小型图提供了有效的GraphRNN改进方法。此外，论文还提供了关于数据集选择和节点特征初始化的最佳实践指南。

    

    针对图的生成模型已经被广泛提出，并在药物发现、道路网络、神经架构搜索和程序综合等领域得到应用。然而，生成图面临着理论上的挑战，如同构表示，评估生成模型的性能很困难。在应用领域中，如何选择合适的模型？我们对分布图不变量的核函数度量进行了广泛研究，并在图嵌入空间中研究了基于流形和基于核函数的度量。基于流形的度量在嵌入空间中表现出色。我们使用这些度量来比较两个著名的图生成模型GraphRNN和GRAN，并揭示了节点排序的影响。结果显示GRAN优于GraphRNN，而且我们提出的使用深度优先搜索排序的GraphRNN适用于小型图。我们提供了关于数据集选择和节点特征初始化的最佳实践指南。

    A wide variety of generative models for graphs have been proposed. They are used in drug discovery, road networks, neural architecture search, and program synthesis. Generating graphs has theoretical challenges, such as isomorphic representations -- evaluating how well a generative model performs is difficult. Which model to choose depending on the application domain?  We extensively study kernel-based metrics on distributions of graph invariants and manifold-based and kernel-based metrics in graph embedding space. Manifold-based metrics outperform kernel-based metrics in embedding space. We use these metrics to compare GraphRNN and GRAN, two well-known generative models for graphs, and unveil the influence of node orderings. It shows the superiority of GRAN over GraphRNN - further, our proposed adaptation of GraphRNN with a depth-first search ordering is effective for small-sized graphs.  A guideline on good practices regarding dataset selection and node feature initialization is prov
    
[^40]: S-HR-VQVAE: 序列分层残差学习向量量化变分自编码器用于视频预测

    S-HR-VQVAE: Sequential Hierarchical Residual Learning Vector Quantized Variational Autoencoder for Video Prediction. (arXiv:2307.06701v1 [cs.CV])

    [http://arxiv.org/abs/2307.06701](http://arxiv.org/abs/2307.06701)

    S-HR-VQVAE是一种序列分层残差学习向量量化变分自编码器，通过结合分层残差向量量化变分自编码器（HR-VQVAE）和时空PixelCNN（ST-PixelCNN）的能力，解决了视频预测中的主要挑战，并在KTH人体动作和Moving-MNIST任务上取得了较好的实验结果。

    

    我们提出了一种新的模型，将我们最近提出的分层残差向量量化变分自编码器（HR-VQVAE）与一种新颖的时空PixelCNN（ST-PixelCNN）相结合，用来解决视频预测任务。我们将这种方法称为序列分层残差学习向量量化变分自编码器（S-HR-VQVAE）。通过利用HR-VQVAE在对静止图像进行建模时的内在能力和紧凑表示，以及ST-PixelCNN处理时空信息的能力， S-HR-VQVAE能够更好地应对视频预测中的主要挑战，包括学习时空信息、处理高维数据、消除模糊预测和隐式建模物理特性。对KTH人体动作和Moving-MNIST任务的大量实验证明，我们的模型在定量和定性评估方面与顶级视频预测技术相比具有优势。

    We address the video prediction task by putting forth a novel model that combines (i) our recently proposed hierarchical residual vector quantized variational autoencoder (HR-VQVAE), and (ii) a novel spatiotemporal PixelCNN (ST-PixelCNN). We refer to this approach as a sequential hierarchical residual learning vector quantized variational autoencoder (S-HR-VQVAE). By leveraging the intrinsic capabilities of HR-VQVAE at modeling still images with a parsimonious representation, combined with the ST-PixelCNN's ability at handling spatiotemporal information, S-HR-VQVAE can better deal with chief challenges in video prediction. These include learning spatiotemporal information, handling high dimensional data, combating blurry prediction, and implicit modeling of physical characteristics. Extensive experimental results on the KTH Human Action and Moving-MNIST tasks demonstrate that our model compares favorably against top video prediction techniques both in quantitative and qualitative evalu
    
[^41]: IntelliGraphs: 用于评估知识图谱生成的数据集

    IntelliGraphs: Datasets for Benchmarking Knowledge Graph Generation. (arXiv:2307.06698v1 [cs.AI])

    [http://arxiv.org/abs/2307.06698](http://arxiv.org/abs/2307.06698)

    IntelliGraphs是一组新的知识图谱数据集，用于评估知识图谱生成。其中包含具有逻辑规则表达的语义的子图，用于评估子图推断的模型。

    

    知识图谱嵌入（KGE）模型用于学习实体和关系的连续表示。文献中一个关键的任务是预测实体之间的缺失链接。然而，知识图谱不仅仅是链接的集合，还具有其结构中的语义。语义在多个下游任务中至关重要，例如查询回答或推理。我们引入了子图推断任务，其中一个模型必须生成可能的并且语义上有效的子图。我们提出了IntelliGraphs，一个包含五个新的知识图谱数据集的集合。IntelliGraphs数据集包含具有逻辑规则表达的语义的子图，用于评估子图推断。我们还设计了产生合成数据集的数据集生成器。我们设计了四个新的基准模型，其中包括基于传统KGE的三个模型。我们评估了它们的表达能力，并展示了这些模型无法捕捉到语义。我们相信这一基准将促进该领域的发展。

    Knowledge Graph Embedding (KGE) models are used to learn continuous representations of entities and relations. A key task in the literature is predicting missing links between entities. However, Knowledge Graphs are not just sets of links but also have semantics underlying their structure. Semantics is crucial in several downstream tasks, such as query answering or reasoning. We introduce the subgraph inference task, where a model has to generate likely and semantically valid subgraphs. We propose IntelliGraphs, a set of five new Knowledge Graph datasets. The IntelliGraphs datasets contain subgraphs with semantics expressed in logical rules for evaluating subgraph inference. We also present the dataset generator that produced the synthetic datasets. We designed four novel baseline models, which include three models based on traditional KGEs. We evaluate their expressiveness and show that these models cannot capture the semantics. We believe this benchmark will encourage the development
    
[^42]: 使用机器学习在大规模测试平台上进行嵌入式SRAM老化分析

    Ageing Analysis of Embedded SRAM on a Large-Scale Testbed Using Machine Learning. (arXiv:2307.06693v1 [cs.AR])

    [http://arxiv.org/abs/2307.06693](http://arxiv.org/abs/2307.06693)

    使用机器学习在大规模测试平台上进行嵌入式SRAM老化分析，发现通过SRAM初始化偏差和各种特征提取指标可以准确估计嵌入式设备的使用时间。

    

    老化检测和故障预测在许多物联网（IoT）部署中至关重要，这些部署在野外无人看管地运行多年的嵌入式设备数量巨大。本文针对一般用途测试平台上的154个板进行了大规模的SRAM老化实证分析。从每个节点在启动时容易收集到的SRAM初始化偏差出发，我们应用各种特征提取指标，并尝试使用常见的机器学习方法来预测该节点的运行时间。我们的研究结果表明，尽管老化影响是微妙的，但我们的指标可以很好地估计使用时间，回归器的R2分数为0.77，平均误差为24%，应用六个月分辨率的分类器的F1分数超过0.6。

    Ageing detection and failure prediction are essential in many Internet of Things (IoT) deployments, which operate huge quantities of embedded devices unattended in the field for years. In this paper, we present a large-scale empirical analysis of natural SRAM wear-out using 154 boards from a general-purpose testbed. Starting from SRAM initialization bias, which each node can easily collect at startup, we apply various metrics for feature extraction and experiment with common machine learning methods to predict the age of operation for this node. Our findings indicate that even though ageing impacts are subtle, our indicators can well estimate usage times with an $R^2$ score of 0.77 and a mean error of 24% using regressors, and with an F1 score above 0.6 for classifiers applying a six-months resolution.
    
[^43]: Aeolus Ocean -- 使用深度强化学习和海洋物体检测的无人水面航行器自主COLREG合规导航的仿真环境

    Aeolus Ocean -- A simulation environment for the autonomous COLREG-compliant navigation of Unmanned Surface Vehicles using Deep Reinforcement Learning and Maritime Object Detection. (arXiv:2307.06688v1 [cs.RO])

    [http://arxiv.org/abs/2307.06688](http://arxiv.org/abs/2307.06688)

    本文提出了Aeolus Ocean，这是一个用于无人水面航行器自主COLREG合规导航的仿真环境。通过在虚拟环境中复制真实的操作条件，可以为DRL和CV算法的开发提供基础。

    

    在海运领域，朝着无人水面航行器（USV）的航行自主化发展，不仅可以有效提高安全性并减少运营成本，还可以为海洋研究、探索和监测提供一系列令人兴奋的新能力。然而，实现这一目标具有挑战性。 USV 控制系统必须能够安全可靠地遵守海上避碰国际规定（COLREGs），在与其他船只相遇时按给定航点航行，同时受到白天或夜间的真实天气条件的影响。为了应对可能的多种场景，必须有一个能够复制USVs将遇到的真实操作条件的虚拟环境，在实际应用之前进行模拟。这种“数字孪生”是将深度强化学习（DRL）和计算机视觉（CV）算法用于开发的基础。

    Heading towards navigational autonomy in unmanned surface vehicles (USVs) in the maritime sector can fundamentally lead towards safer waters as well as reduced operating costs, while also providing a range of exciting new capabilities for oceanic research, exploration and monitoring. However, achieving such a goal is challenging. USV control systems must, safely and reliably, be able to adhere to the international regulations for preventing collisions at sea (COLREGs) in encounters with other vessels as they navigate to a given waypoint while being affected by realistic weather conditions, either during the day or at night. To deal with the multitude of possible scenarios, it is critical to have a virtual environment that is able to replicate the realistic operating conditions USVs will encounter, before they can be implemented in the real world. Such "digital twins" form the foundations upon which Deep Reinforcement Learning (DRL) and Computer Vision (CV) algorithms can be used to dev
    
[^44]: 实时移动网络中VoIP流量的多变量时间序列特征和预测

    Multivariate Time Series characterization and forecasting of VoIP traffic in real mobile networks. (arXiv:2307.06645v1 [cs.NI])

    [http://arxiv.org/abs/2307.06645](http://arxiv.org/abs/2307.06645)

    本论文旨在通过多变量时间序列分析，预测实时移动网络中VoIP流量的关键QoS/QoE描述符的行为，以帮助运营商优化网络规划和资源分配。

    

    预测移动网络中实时流量（例如VoIP）的行为，可以帮助运营商更好地规划网络基础设施并优化资源分配。因此，本研究提出了对VoIP流量在实际移动环境中关键QoS/QoE描述符进行预测分析（其中一些在技术文献中被忽略）。该问题以多变量时间序列分析的形式进行建模，这种形式化可以发现和建模各种描述符之间的时间关系，并预测它们在未来时期的行为。通过将多变量时间序列问题转化为监督学习问题，采用向量自回归模型和机器学习（基于深度学习和基于树的方法）进行性能和时间复杂度的比较。此外，还进行了一系列辅助分析（平稳性，正交脉冲响应等）可以发现VoIP流量的特性。

    Predicting the behavior of real-time traffic (e.g., VoIP) in mobility scenarios could help the operators to better plan their network infrastructures and to optimize the allocation of resources. Accordingly, in this work the authors propose a forecasting analysis of crucial QoS/QoE descriptors (some of which neglected in the technical literature) of VoIP traffic in a real mobile environment. The problem is formulated in terms of a multivariate time series analysis. Such a formalization allows to discover and model the temporal relationships among various descriptors and to forecast their behaviors for future periods. Techniques such as Vector Autoregressive models and machine learning (deep-based and tree-based) approaches are employed and compared in terms of performance and time complexity, by reframing the multivariate time series problem into a supervised learning one. Moreover, a series of auxiliary analyses (stationarity, orthogonal impulse responses, etc.) are performed to disco
    
[^45]: 一个具有“fat-shattering”维度的改进的均匀收敛界限

    An Improved Uniform Convergence Bound with Fat-Shattering Dimension. (arXiv:2307.06644v1 [cs.LG])

    [http://arxiv.org/abs/2307.06644](http://arxiv.org/abs/2307.06644)

    本文提出了一个改进的均匀收敛界限，填补了现有状态-of-the-art上界与下界之间的空缺。

    

    “fat-shattering”维度刻画了实值函数的均匀收敛特性。现有的状态-of-the-art上界在样本复杂度上存在一个乘法平方对数因子，与已有的下界之间存在一个空缺。本文提供了一个改进的均匀收敛界限来填补这个空缺。

    The fat-shattering dimension characterizes the uniform convergence property of real-valued functions. The state-of-the-art upper bounds feature a multiplicative squared logarithmic factor on the sample complexity, leaving an open gap with the existing lower bound. We provide an improved uniform convergence bound that closes this gap.
    
[^46]: 发现智能体在少量数据上的学习方式

    Discovering How Agents Learn Using Few Data. (arXiv:2307.06640v1 [cs.GT])

    [http://arxiv.org/abs/2307.06640](http://arxiv.org/abs/2307.06640)

    本研究提出了一个在少量数据上识别智能体学习动力学的算法框架。通过使用多项式回归和引入捕捉智能体行为的侧信息约束，该方法可以在使用单个轨迹的短暂运行中仅仅5个样本的情况下准确恢复真实动力学。

    

    分散式学习算法是设计多智能体系统的重要工具，它们使得智能体能够从经验和过去的交互中自主学习。本研究提出了一个理论和算法框架，用于实时识别规定智能体行为的学习动力学，只需使用单个系统轨迹的短暂突发。我们的方法通过多项式回归识别智能体动力学，在有限的数据上通过引入捕捉智能体行为的基本假设或期望的侧信息约束来补偿，并且通过使用和优化约束的平方和计算来实施这些约束，从而得到越来越准确的智能体动力学近似层次。广泛的实验证明，我们的方法只使用单个轨迹的短暂运行中的5个样本，就可以准确恢复各种基准测试中的真实动力学，包括均衡选择和预测。

    Decentralized learning algorithms are an essential tool for designing multi-agent systems, as they enable agents to autonomously learn from their experience and past interactions. In this work, we propose a theoretical and algorithmic framework for real-time identification of the learning dynamics that govern agent behavior using a short burst of a single system trajectory. Our method identifies agent dynamics through polynomial regression, where we compensate for limited data by incorporating side-information constraints that capture fundamental assumptions or expectations about agent behavior. These constraints are enforced computationally using sum-of-squares optimization, leading to a hierarchy of increasingly better approximations of the true agent dynamics. Extensive experiments demonstrated that our approach, using only 5 samples from a short run of a single trajectory, accurately recovers the true dynamics across various benchmarks, including equilibrium selection and predictio
    
[^47]: 无框图知识蒸馏

    Frameless Graph Knowledge Distillation. (arXiv:2307.06631v1 [cs.LG])

    [http://arxiv.org/abs/2307.06631](http://arxiv.org/abs/2307.06631)

    这里是针对图知识蒸馏的无框架KD框架，通过充分利用图框架分解提供的多尺度图知识，学生模型能够适应不同类型的图，并具有缓解的潜力。

    

    知识蒸馏（Knowledge Distillation，KD）已显示出在将复杂的教师模型的知识传输到简单的学生模型中具有巨大潜力，从而可以高效地完成繁重的学习任务而不失去太多的预测准确性。最近，许多研究尝试将KD机制应用于图表示学习模型，如图神经网络（Graph Neural Networks，GNNs），以通过学生模型加速模型的推断速度。然而，许多现有的基于KD的GNNs在学生模型中利用MLP作为通用逼近器来模仿教师模型的流程，而不考虑教师模型的图知识。在这项工作中，我们提供了一个基于多尺度GNNs的KD框架，即图框架（graph framelet），并证明通过充分利用图框架分解提供的多尺度图知识，学生模型能够适应同质和异质图，并具有缓解的潜力。

    Knowledge distillation (KD) has shown great potential for transferring knowledge from a complex teacher model to a simple student model in which the heavy learning task can be accomplished efficiently and without losing too much prediction accuracy. Recently, many attempts have been made by applying the KD mechanism to the graph representation learning models such as graph neural networks (GNNs) to accelerate the model's inference speed via student models. However, many existing KD-based GNNs utilize MLP as a universal approximator in the student model to imitate the teacher model's process without considering the graph knowledge from the teacher model. In this work, we provide a KD-based framework on multi-scaled GNNs, known as graph framelet, and prove that by adequately utilizing the graph knowledge in a multi-scaled manner provided by graph framelet decomposition, the student model is capable of adapting both homophilic and heterophilic graphs and has the potential of alleviating t
    
[^48]: 用于学习量子通道编码的量子自编码器

    Quantum Autoencoders for Learning Quantum Channel Codes. (arXiv:2307.06622v1 [quant-ph])

    [http://arxiv.org/abs/2307.06622](http://arxiv.org/abs/2307.06622)

    本研究使用量子机器学习技术研究了经典和量子通信中的量子通道编码，并展示了其在不同通道模型下的强大性能，为推进量子通信系统研究提供了潜力，能够更好地理解通信容量界限。

    

    本研究探讨了量子机器学习技术在经典和量子通信中的应用，涉及不同量子比特通道模型。通过使用参数化量子电路和灵活的通道噪声模型，我们开发了一个机器学习框架来生成量子通道编码并评估其有效性。我们在该框架内探索了经典、辅助纠缠和量子通信场景。通过将其应用于各种量子通道模型作为概念验证，我们在每种情况下展示了强大的性能。我们的结果突显了量子机器学习在推进量子通信系统研究方面的潜力，能够更好地理解在调制约束、各种通信设置和不同通道模型下的容量界限。

    This work investigates the application of quantum machine learning techniques for classical and quantum communication across different qubit channel models. By employing parameterized quantum circuits and a flexible channel noise model, we develop a machine learning framework to generate quantum channel codes and evaluate their effectiveness. We explore classical, entanglement-assisted, and quantum communication scenarios within our framework. Applying it to various quantum channel models as proof of concept, we demonstrate strong performance in each case. Our results highlight the potential of quantum machine learning in advancing research on quantum communication systems, enabling a better understanding of capacity bounds under modulation constraints, various communication settings, and diverse channel models.
    
[^49]: 在线分布式学习与量化有限时间协作

    Online Distributed Learning with Quantized Finite-Time Coordination. (arXiv:2307.06620v1 [cs.LG])

    [http://arxiv.org/abs/2307.06620](http://arxiv.org/abs/2307.06620)

    本文研究了一种在线分布式学习问题，提出了一种依靠量化、有限时间协作协议的分布式算法来聚合本地训练的模型，并允许使用随机梯度来提高效率和可扩展性。

    

    本文研究在线分布式学习问题。在线分布式学习是指在分布式数据源上训练学习模型的过程。在我们的设置中，一组代理需要合作地从流数据中训练学习模型。与联邦学习不同，所提出的方法不依赖于中央服务器，而仅依靠代理之间的点对点通信。该方法经常用于数据由于隐私、安全或成本原因不能移动到集中位置的场景。为了克服缺少中央服务器的问题，我们提出了一个分布式算法，该算法依赖于一个量化的、有限时间的协作协议来聚合本地训练的模型。此外，我们的算法允许在本地训练过程中使用随机梯度。随机梯度是使用本地训练数据的随机抽样子集计算的，这使得所提出的算法更加高效和可扩展。

    In this paper we consider online distributed learning problems. Online distributed learning refers to the process of training learning models on distributed data sources. In our setting a set of agents need to cooperatively train a learning model from streaming data. Differently from federated learning, the proposed approach does not rely on a central server but only on peer-to-peer communications among the agents. This approach is often used in scenarios where data cannot be moved to a centralized location due to privacy, security, or cost reasons. In order to overcome the absence of a central server, we propose a distributed algorithm that relies on a quantized, finite-time coordination protocol to aggregate the locally trained models. Furthermore, our algorithm allows for the use of stochastic gradients during local training. Stochastic gradients are computed using a randomly sampled subset of the local training data, which makes the proposed algorithm more efficient and scalable th
    
[^50]: 使用梯度下降从测量中学习IMM滤波器参数

    Learning IMM Filter Parameters from Measurements using Gradient Descent. (arXiv:2307.06618v1 [cs.LG])

    [http://arxiv.org/abs/2307.06618](http://arxiv.org/abs/2307.06618)

    本文提出了一种使用梯度下降从测量中学习IMM滤波器参数的方法，通过仅使用测量数据即可优化滤波器的参数。在模拟数据上的实验结果表明，该方法能够达到使用真值参数化的滤波器的性能水平。

    

    数据融合和跟踪算法的性能通常依赖于既描述传感器系统又可以是任务特定的参数。尽管调整这些变量对于传感器系统来说是耗时的，并且通常需要专家知识，但在跟踪目标的内在参数在系统部署之前甚至可能完全不可观测。随着最先进的传感器系统越来越复杂，参数的数量自然增加，需要自动优化模型变量。本文通过仅使用测量来优化交互多模型（IMM）滤波器的参数，因此无需任何基础数据。通过对模拟数据进行消融研究评估了结果方法，结果方法成功匹配了使用基础真值参数化的滤波器的性能。

    The performance of data fusion and tracking algorithms often depends on parameters that not only describe the sensor system, but can also be task-specific. While for the sensor system tuning these variables is time-consuming and mostly requires expert knowledge, intrinsic parameters of targets under track can even be completely unobservable until the system is deployed. With state-of-the-art sensor systems growing more and more complex, the number of parameters naturally increases, necessitating the automatic optimization of the model variables. In this paper, the parameters of an interacting multiple model (IMM) filter are optimized solely using measurements, thus without necessity for any ground-truth data. The resulting method is evaluated through an ablation study on simulated data, where the trained model manages to match the performance of a filter parametrized with ground-truth values.
    
[^51]: 将基础模型作为代理模型引入：朝着更实用的对抗攻击迈进

    Introducing Foundation Models as Surrogate Models: Advancing Towards More Practical Adversarial Attacks. (arXiv:2307.06608v1 [cs.LG])

    [http://arxiv.org/abs/2307.06608](http://arxiv.org/abs/2307.06608)

    本文将对抗攻击重新设定为下游任务，通过生成图像噪声来满足新兴趋势，并将基础模型引入作为代理模型。虽然基础模型的表现不佳，但通过在特征空间中进行分析，我们发现缺乏对应的特征。

    

    最近，无盒对抗攻击成为了最实用且具有挑战性的攻击方式，攻击者无法访问模型的架构、权重和训练数据。然而，在无盒设置中，对于代理模型选择过程的潜力和灵活性缺乏认识。受到利用基础模型解决下游任务的兴趣的启发，本文采用了1）将对抗攻击重新设定为下游任务，具体而言，是生成图像噪声以满足新兴趋势；2）将基础模型引入作为代理模型的创新思想。通过利用非鲁棒特征的概念，我们阐述了选择代理模型的两个指导原则，以解释为什么基础模型是这一角色的最佳选择。然而，矛盾地的是，我们观察到这些基础模型表现不佳。通过在特征空间中分析这种意外行为，我们归因于缺乏上述指导原则所需的特征。

    Recently, the no-box adversarial attack, in which the attacker lacks access to the model's architecture, weights, and training data, become the most practical and challenging attack setup. However, there is an unawareness of the potential and flexibility inherent in the surrogate model selection process on no-box setting. Inspired by the burgeoning interest in utilizing foundational models to address downstream tasks, this paper adopts an innovative idea that 1) recasting adversarial attack as a downstream task. Specifically, image noise generation to meet the emerging trend and 2) introducing foundational models as surrogate models. Harnessing the concept of non-robust features, we elaborate on two guiding principles for surrogate model selection to explain why the foundational model is an optimal choice for this role. However, paradoxically, we observe that these foundational models underperform. Analyzing this unexpected behavior within the feature space, we attribute the lackluster
    
[^52]: 基于深度神经网络的半参数脆弱性模型及其H-似然法

    Deep Neural Networks for Semiparametric Frailty Models via H-likelihood. (arXiv:2307.06581v1 [stat.ML])

    [http://arxiv.org/abs/2307.06581](http://arxiv.org/abs/2307.06581)

    本文提出了一种新的基于深度神经网络的脆弱性模型，并使用H-似然法进行训练和预测。实验结果表明该方法提高了预测性能，特别是在包含个体特定脆弱性的情况下。

    

    为了预测群集化的时间至事件数据，我们提出了一种新的基于深度神经网络的伽马脆弱性模型（DNN-FM）。该模型的一个优势是通过新的H-似然函数的联合最大化，为固定参数提供了最大似然估计器，并为随机脆性提供了最佳无偏预测器。因此，所提出的DNN-FM通过使用负面剖析的H-似然函数作为损失函数进行训练，通过剖析非参数基线风险来构造。实验研究表明，所提出的方法提高了现有方法的预测性能。实际数据分析表明，包含个体特定脆弱性有助于改善基于DNN的Cox模型（DNN-Cox）的预测能力。

    For prediction of clustered time-to-event data, we propose a new deep neural network based gamma frailty model (DNN-FM). An advantage of the proposed model is that the joint maximization of the new h-likelihood provides maximum likelihood estimators for fixed parameters and best unbiased predictors for random frailties. Thus, the proposed DNN-FM is trained by using a negative profiled h-likelihood as a loss function, constructed by profiling out the non-parametric baseline hazard. Experimental studies show that the proposed method enhances the prediction performance of the existing methods. A real data analysis shows that the inclusion of subject-specific frailties helps to improve prediction of the DNN based Cox model (DNN-Cox).
    
[^53]: 通过亚线性激活神经元识别实现高效的SGD神经网络训练

    Efficient SGD Neural Network Training via Sublinear Activated Neuron Identification. (arXiv:2307.06565v1 [cs.LG])

    [http://arxiv.org/abs/2307.06565](http://arxiv.org/abs/2307.06565)

    通过亚线性激活神经元识别实现高效的SGD神经网络训练，算法收敛时间复杂度为$O(M^2/\epsilon^2)$。

    

    深度学习在许多领域得到了广泛应用，但模型训练过程通常需要消耗大量的计算资源和时间。因此，设计一个具有可证明收敛保证的高效神经网络训练方法是一个基本且重要的研究问题。在本文中，我们提出了一个静态的半空间报告数据结构，其中包含一个全连接的两层神经网络用于实现平移ReLU激活，以通过几何搜索在亚线性时间内进行激活神经元的识别。我们还证明了我们的算法可以在$O(M^2/\epsilon^2)$的时间复杂度内收敛，其中$M$是系数范数上界，$\epsilon$是误差项。

    Deep learning has been widely used in many fields, but the model training process usually consumes massive computational resources and time. Therefore, designing an efficient neural network training method with a provable convergence guarantee is a fundamental and important research question. In this paper, we present a static half-space report data structure that consists of a fully connected two-layer neural network for shifted ReLU activation to enable activated neuron identification in sublinear time via geometric search. We also prove that our algorithm can converge in $O(M^2/\epsilon^2)$ time with network size quadratic in the coefficient norm upper bound $M$ and error term $\epsilon$.
    
[^54]: 在资源限制下的处方过程监控：一种强化学习方法

    Prescriptive Process Monitoring Under Resource Constraints: A Reinforcement Learning Approach. (arXiv:2307.06564v1 [cs.AI])

    [http://arxiv.org/abs/2307.06564](http://arxiv.org/abs/2307.06564)

    本论文提出了一种在资源限制下进行处方过程监控的强化学习方法。通过考虑对干预需求、及时性或效果预测的不确定性和资源利用水平，来触发干预，从而优化业务过程的性能。

    

    处方过程监控方法旨在通过在运行时触发干预来优化业务过程的性能，从而增加正面案例结果的概率。这些干预是根据干预策略触发的。强化学习被提出作为通过试错学习干预策略的一种方法。然而，现有方法在这一领域假设可用于执行干预的资源数量是无限的，这在实践中是不现实的。本文认为，在资源限制的情况下，处方过程监控领域面临的一个关键困境是基于对干预需求、及时性或效果的预测的不确定性和资源利用水平来触发干预。实际上，当对干预的必要性或效果存在高度不确定性时，将有限的资源用于干预是一项挑战。

    Prescriptive process monitoring methods seek to optimize the performance of business processes by triggering interventions at runtime, thereby increasing the probability of positive case outcomes. These interventions are triggered according to an intervention policy. Reinforcement learning has been put forward as an approach to learning intervention policies through trial and error. Existing approaches in this space assume that the number of resources available to perform interventions in a process is unlimited, an unrealistic assumption in practice. This paper argues that, in the presence of resource constraints, a key dilemma in the field of prescriptive process monitoring is to trigger interventions based not only on predictions of their necessity, timeliness, or effect but also on the uncertainty of these predictions and the level of resource utilization. Indeed, committing scarce resources to an intervention when the necessity or effects of this intervention are highly uncertain m
    
[^55]: 基于金属氧化物的气体传感器阵列在复杂混合物中利用机器学习进行VOCs分析

    Metal Oxide-based Gas Sensor Array for the VOCs Analysis in Complex Mixtures using Machine Learning. (arXiv:2307.06556v1 [physics.app-ph])

    [http://arxiv.org/abs/2307.06556](http://arxiv.org/abs/2307.06556)

    本研究提出了一种基于金属氧化物传感器阵列和机器学习的方法，可以在复杂混合物中识别出不同的挥发性有机化合物（VOCs），且取得了很高的准确率和回归分析结果。

    

    呼吸中挥发性有机化合物（VOCs）的检测正在成为一种非侵入性疾病早期检测的可行途径。本文提出了一种具有三个金属氧化物电极的传感器阵列，可以使用机器学习方法在混合物中识别出四种不同的VOCs。金属氧化物传感器阵列经过不同VOC浓度的测试，包括乙醇、丙酮、甲苯和氯仿。从单一气体和混合物获得的数据集使用多个机器学习算法进行分析，如随机森林（RF）、K最近邻（KNN）、决策树、线性回归、逻辑回归、朴素贝叶斯、线性判别分析、人工神经网络和支持向量机。KNN和RF在对气体混合物中的不同变化化学品进行分类方面的准确率超过99%。在回归分析中，KNN的结果最好，R2值超过0.99，LOD值为0.012、0.015、0.014和0.025 PPM。

    Detection of Volatile Organic Compounds (VOCs) from the breath is becoming a viable route for the early detection of diseases non-invasively. This paper presents a sensor array with three metal oxide electrodes that can use machine learning methods to identify four distinct VOCs in a mixture. The metal oxide sensor array was subjected to various VOC concentrations, including ethanol, acetone, toluene and chloroform. The dataset obtained from individual gases and their mixtures were analyzed using multiple machine learning algorithms, such as Random Forest (RF), K-Nearest Neighbor (KNN), Decision Tree, Linear Regression, Logistic Regression, Naive Bayes, Linear Discriminant Analysis, Artificial Neural Network, and Support Vector Machine. KNN and RF have shown more than 99% accuracy in classifying different varying chemicals in the gas mixtures. In regression analysis, KNN has delivered the best results with R2 value of more than 0.99 and LOD of 0.012, 0.015, 0.014 and 0.025 PPM for pred
    
[^56]: 深度网络逼近：从ReLU到多种激活函数

    Deep Network Approximation: Beyond ReLU to Diverse Activation Functions. (arXiv:2307.06555v1 [cs.LG])

    [http://arxiv.org/abs/2307.06555](http://arxiv.org/abs/2307.06555)

    本文研究了深度神经网络在多种激活函数下的表达能力，证明了可以通过在有界集合上构建一个宽度为6N、深度为2L的varrho激活网络来逼近一个宽度为N、深度为L的ReLU网络，从而将对ReLU网络的逼近结果推广到其他激活函数。

    

    本文探究了深度神经网络在多种激活函数下的表达能力。定义了一个激活函数集合A，包括大多数常用的激活函数，如ReLU、LeakyReLU、ReLU^2、ELU、SELU、Softplus、GELU、SiLU、Swish、Mish、Sigmoid、Tanh、Arctan、Softsign、dSiLU和SRS。我们证明了对于任意激活函数varrho∈A，可以通过一个宽度为6N、深度为2L的varrho激活网络在有界集合上以任意精度逼近一个宽度为N、深度为L的ReLU网络。这一发现使得大部分对于ReLU网络的逼近结果能够推广到其他激活函数，尽管需要稍大的常数代价。

    This paper explores the expressive power of deep neural networks for a diverse range of activation functions. An activation function set $\mathscr{A}$ is defined to encompass the majority of commonly used activation functions, such as $\mathtt{ReLU}$, $\mathtt{LeakyReLU}$, $\mathtt{ReLU}^2$, $\mathtt{ELU}$, $\mathtt{SELU}$, $\mathtt{Softplus}$, $\mathtt{GELU}$, $\mathtt{SiLU}$, $\mathtt{Swish}$, $\mathtt{Mish}$, $\mathtt{Sigmoid}$, $\mathtt{Tanh}$, $\mathtt{Arctan}$, $\mathtt{Softsign}$, $\mathtt{dSiLU}$, and $\mathtt{SRS}$. We demonstrate that for any activation function $\varrho\in \mathscr{A}$, a $\mathtt{ReLU}$ network of width $N$ and depth $L$ can be approximated to arbitrary precision by a $\varrho$-activated network of width $6N$ and depth $2L$ on any bounded set. This finding enables the extension of most approximation results achieved with $\mathtt{ReLU}$ networks to a wide variety of other activation functions, at the cost of slightly larger constants.
    
[^57]: 使用残差编码-解码网络从胸部X射线图像中进行全分辨率肺结节分割

    Full-resolution Lung Nodule Segmentation from Chest X-ray Images using Residual Encoder-Decoder Networks. (arXiv:2307.06547v1 [eess.IV])

    [http://arxiv.org/abs/2307.06547](http://arxiv.org/abs/2307.06547)

    本研究使用高效的编码-解码神经网络处理全分辨率图像，实现从胸部X射线图像中定位肺结节的目标，避免了降采样导致的信号丢失，并通过自动化框架对其进行了评估。

    

    肺癌是癌症死亡的主要原因，早期诊断与良好的预后相关。胸部X射线（CXR）提供了一种廉价的肺癌诊断成像方式。使用CXR很难区分可疑结节与血管和骨结构。先前的计算机视觉研究提出了辅助人类放射科医师进行该任务的方法，但是主要的研究使用了降采样图像和计算成本高昂、未经证实的泛化方法。相反，本研究使用高效的编码-解码神经网络来定位肺结节，该神经网络处理全分辨率图像以避免因降采样而导致的信号丢失。编码-解码网络使用JSRT肺结节数据集进行训练和测试，并用于定位来自独立的外部CXR数据集的肺结节。使用自动化框架测量了灵敏度和假阳性率，以消除任何观察者主观性。这些实验可以实现肺结节的检测。

    Lung cancer is the leading cause of cancer death and early diagnosis is associated with a positive prognosis. Chest X-ray (CXR) provides an inexpensive imaging mode for lung cancer diagnosis. Suspicious nodules are difficult to distinguish from vascular and bone structures using CXR. Computer vision has previously been proposed to assist human radiologists in this task, however, leading studies use down-sampled images and computationally expensive methods with unproven generalization. Instead, this study localizes lung nodules using efficient encoder-decoder neural networks that process full resolution images to avoid any signal loss resulting from down-sampling. Encoder-decoder networks are trained and tested using the JSRT lung nodule dataset. The networks are used to localize lung nodules from an independent external CXR dataset. Sensitivity and false positive rates are measured using an automated framework to eliminate any observer subjectivity. These experiments allow for the dete
    
[^58]: 逆强化学习中的有效时间视野研究

    On the Effective Horizon of Inverse Reinforcement Learning. (arXiv:2307.06541v1 [cs.LG])

    [http://arxiv.org/abs/2307.06541](http://arxiv.org/abs/2307.06541)

    本研究分析了逆强化学习中时间视野的重要性，发现短于实际值的有效时间视野可以更快且更准确地估计奖励函数，减轻过拟合问题。此外，研究还呼吁在IRL中同时学习奖励和有效时间视野。

    

    逆强化学习（IRL）算法通常依赖于基于给定时间视野的（前向）强化学习或规划来计算一个近似最优策略，然后将该策略与专家演示匹配。时间视野在确定奖励估计的准确性和IRL算法的计算效率方面起着关键作用。有趣的是，比地面实际值更短的有效时间视野通常能更快地产生更好的结果。本文对此现象进行了正式分析并给出了解释：时间视野控制了引发策略类的复杂性，并在有限数据下减轻过拟合。这一分析为IRL的有效视野选择提供了原则性指导。它也促使我们重新审视经典的IRL公式：与仅具有给定视野的奖励相比，共同学习奖励和有效视野更加自然。我们的实验进一步验证了这一观点。

    Inverse reinforcement learning (IRL) algorithms often rely on (forward) reinforcement learning or planning over a given time horizon to compute an approximately optimal policy for a hypothesized reward function and then match this policy with expert demonstrations. The time horizon plays a critical role in determining both the accuracy of reward estimate and the computational efficiency of IRL algorithms. Interestingly, an effective time horizon shorter than the ground-truth value often produces better results faster. This work formally analyzes this phenomenon and provides an explanation: the time horizon controls the complexity of an induced policy class and mitigates overfitting with limited data. This analysis leads to a principled choice of the effective horizon for IRL. It also prompts us to reexamine the classic IRL formulation: it is more natural to learn jointly the reward and the effective horizon together rather than the reward alone with a given horizon. Our experimental re
    
[^59]: 基于卷积神经网络的微博情感分析：一种自然语言处理方法

    Convolutional Neural Networks for Sentiment Analysis on Weibo Data: A Natural Language Processing Approach. (arXiv:2307.06540v1 [cs.CL])

    [http://arxiv.org/abs/2307.06540](http://arxiv.org/abs/2307.06540)

    本研究提出了一种基于卷积神经网络的方法，利用微博数据进行情感分析，取得了约0.73的宏平均F1分数，结果表明了CNN在情感分析任务中的有效性，对社交媒体分析、市场研究和政策研究等领域有重要意义。

    

    该研究使用卷积神经网络（CNN）对来自微博的119,988条原始推文进行了情感分析，为自然语言处理（NLP）提供了一种新的方法。数据是从百度的PaddlePaddle AI平台获取的，经过了精细的预处理、分词和情感标签分类。利用基于词嵌入的CNN模型进行特征提取和情感分类训练，该模型在测试集上获得了约0.73的宏平均F1分数，显示了对正面、中性和负面情感的平衡表现。研究结果强调了CNN在情感分析任务中的有效性，对社交媒体分析、市场研究和政策研究等实际应用有着重要意义。完整的实验内容和代码已在Kaggle数据平台上公开提供以进行进一步研究和开发。

    This study addressed the complex task of sentiment analysis on a dataset of 119,988 original tweets from Weibo using a Convolutional Neural Network (CNN), offering a new approach to Natural Language Processing (NLP). The data, sourced from Baidu's PaddlePaddle AI platform, were meticulously preprocessed, tokenized, and categorized based on sentiment labels. A CNN-based model was utilized, leveraging word embeddings for feature extraction, and trained to perform sentiment classification. The model achieved a macro-average F1-score of approximately 0.73 on the test set, showing balanced performance across positive, neutral, and negative sentiments. The findings underscore the effectiveness of CNNs for sentiment analysis tasks, with implications for practical applications in social media analysis, market research, and policy studies. The complete experimental content and code have been made publicly available on the Kaggle data platform for further research and development. Future work ma
    
[^60]: 张量分解与控制理论的结合：学习线性动力系统的混合模型

    Tensor Decompositions Meet Control Theory: Learning General Mixtures of Linear Dynamical Systems. (arXiv:2307.06538v1 [cs.LG])

    [http://arxiv.org/abs/2307.06538](http://arxiv.org/abs/2307.06538)

    本论文以张量分解方法为基础，提出了学习线性动力系统混合模型的新方法。算法成功地应用于没有组件分离条件的情况，并可以与贝叶斯最优聚类竞争。此外，算法可以在部分观测设置下工作。

    

    最近，Chen和Poor开始研究学习线性动力系统的混合模型。虽然线性动力系统已经在建模时间序列数据方面有广泛的应用，但使用混合模型可以带来更好的拟合或者对数据中表示的基础子群体有更丰富的理解。在这项工作中，我们提出了一种基于张量分解的学习线性动力系统混合模型的新方法。因此，我们的算法在组件无强分离条件的情况下成功，并可以用于与轨迹的贝叶斯最优聚类竞争。此外，我们的算法适用于具有挑战性的部分观测设置。我们的起点是简单但强大的观察，即经典的何-卡尔曼算法是学习潜变量模型的现代张量分解方法的近亲。这为我们提供了一个扩展到更复杂的生成模型的操作指南。

    Recently Chen and Poor initiated the study of learning mixtures of linear dynamical systems. While linear dynamical systems already have wide-ranging applications in modeling time-series data, using mixture models can lead to a better fit or even a richer understanding of underlying subpopulations represented in the data. In this work we give a new approach to learning mixtures of linear dynamical systems that is based on tensor decompositions. As a result, our algorithm succeeds without strong separation conditions on the components, and can be used to compete with the Bayes optimal clustering of the trajectories. Moreover our algorithm works in the challenging partially-observed setting. Our starting point is the simple but powerful observation that the classic Ho-Kalman algorithm is a close relative of modern tensor decomposition methods for learning latent variable models. This gives us a playbook for how to extend it to work with more complicated generative models.
    
[^61]: DSV:自监督离群点模型选择的对齐验证损失

    DSV: An Alignment Validation Loss for Self-supervised Outlier Model Selection. (arXiv:2307.06534v1 [cs.LG])

    [http://arxiv.org/abs/2307.06534](http://arxiv.org/abs/2307.06534)

    DSV提出了一种无监督的验证损失函数用于选择有效的自监督离群点模型，通过捕捉数据增强函数与异常生成机制之间的对齐性，提高了检测准确性。

    

    自监督学习（SSL）通过生成内部监督信号已被证明在解决各种问题上有效。无监督异常检测面临获取真实标签的高成本，因此可以从SSL中大大受益。然而，最近的文献表明，调整数据增强函数的超参数对基于SSL的异常检测（SSAD）的成功至关重要，但是目前还没有系统的方法。在这项工作中，我们提出了DSV（不一致性和可分离性验证），这是一种无监督的验证损失，用于选择具有有效参数的高性能检测模型。 DSV通过替代损失函数捕捉数据增强函数与异常生成机制之间的对齐，这些损失函数分别近似了测试数据的不一致性和可分离性。因此，通过DSV进行评估可以选择一个具有更好对齐性的有效SSAD模型，从而实现高检测准确性。

    Self-supervised learning (SSL) has proven effective in solving various problems by generating internal supervisory signals. Unsupervised anomaly detection, which faces the high cost of obtaining true labels, is an area that can greatly benefit from SSL. However, recent literature suggests that tuning the hyperparameters (HP) of data augmentation functions is crucial to the success of SSL-based anomaly detection (SSAD), yet a systematic method for doing so remains unknown. In this work, we propose DSV (Discordance and Separability Validation), an unsupervised validation loss to select high-performing detection models with effective augmentation HPs. DSV captures the alignment between an augmentation function and the anomaly-generating mechanism with surrogate losses, which approximate the discordance and separability of test data, respectively. As a result, the evaluation via DSV leads to selecting an effective SSAD model exhibiting better alignment, which results in high detection accu
    
[^62]: 药物发现中的人工智能：我们已经到达了吗？

    Artificial Intelligence for Drug Discovery: Are We There Yet?. (arXiv:2307.06521v1 [cs.AI])

    [http://arxiv.org/abs/2307.06521](http://arxiv.org/abs/2307.06521)

    本综述讨论了人工智能在药物发现中的应用，重点关注小分子药物。通过使用生成化学、机器学习和多属性优化等人工智能技术，已有多种化合物进入了临床试验。科学界必须仔细审查已知信息，以解决可重复性危机。只有具有足够的真实基准和适当的训练数据，才能实现人工智能在药物发现中的全部潜力。

    

    药物发现正在适应数据科学、信息学和人工智能等新技术，以加快有效治疗的开发，同时降低成本和动物实验。人工智能正在改变药物发现，投资者、工业和学术科学家以及立法者对此越来越感兴趣。成功的药物发现需要优化与药理动力学、药代动力学和临床结果相关的性质。本综述讨论了人工智能在药物发现的三个支柱：疾病、靶点和治疗模式中的应用，重点关注小分子药物。生成化学、机器学习和多属性优化等人工智能技术使得多种化合物进入了临床试验。科学界必须仔细审查已知信息，以解决可重复性危机。只有具有足够的真实基准和适当的训练数据，才能实现人工智能在药物发现中的全部潜力。

    Drug discovery is adapting to novel technologies such as data science, informatics, and artificial intelligence (AI) to accelerate effective treatment development while reducing costs and animal experiments. AI is transforming drug discovery, as indicated by increasing interest from investors, industrial and academic scientists, and legislators. Successful drug discovery requires optimizing properties related to pharmacodynamics, pharmacokinetics, and clinical outcomes. This review discusses the use of AI in the three pillars of drug discovery: diseases, targets, and therapeutic modalities, with a focus on small molecule drugs. AI technologies, such as generative chemistry, machine learning, and multi-property optimization, have enabled several compounds to enter clinical trials. The scientific community must carefully vet known information to address the reproducibility crisis. The full potential of AI in drug discovery can only be realized with sufficient ground truth and appropriate
    
[^63]: 机器学习实践和基础设施

    Machine Learning practices and infrastructures. (arXiv:2307.06518v1 [cs.CY])

    [http://arxiv.org/abs/2307.06518](http://arxiv.org/abs/2307.06518)

    本文研究了机器学习实践中从业者与工具的互动，以及这些互动对于机器学习实践和系统开发的影响。通过实证研究，发现交互式计算平台在学习和协调实践中起到了重要的基础设施作用。

    

    机器学习（ML）系统在重大领域部署时具有深远影响。它们可能加剧现有的不平等，创造新的歧视模式，并固化过时的社会构造。因此，ML系统开发的社会背景（即组织、团队、文化）是AI伦理领域和决策者进行积极研究和干预的焦点。本文关注一个常被忽视的社会背景方面：从业者与他们所依赖的工具之间的互动，以及这些互动在塑造ML实践和ML系统开发中的作用。特别地，通过对Stack Exchange论坛上提出的问题进行实证研究，探讨了在ML实践中使用交互式计算平台（如Jupyter Notebook和Google Colab）。我发现交互式计算平台在学习和协调实践中被广泛使用，构成了一种基础设施。

    Machine Learning (ML) systems, particularly when deployed in high-stakes domains, are deeply consequential. They can exacerbate existing inequities, create new modes of discrimination, and reify outdated social constructs. Accordingly, the social context (i.e. organisations, teams, cultures) in which ML systems are developed is a site of active research for the field of AI ethics, and intervention for policymakers. This paper focuses on one aspect of social context that is often overlooked: interactions between practitioners and the tools they rely on, and the role these interactions play in shaping ML practices and the development of ML systems. In particular, through an empirical study of questions asked on the Stack Exchange forums, the use of interactive computing platforms (e.g. Jupyter Notebook and Google Colab) in ML practices is explored. I find that interactive computing platforms are used in a host of learning and coordination practices, which constitutes an infrastructural r
    
[^64]: 利用上下文反事实推理实现信念校准

    Leveraging Contextual Counterfactuals Toward Belief Calibration. (arXiv:2307.06513v1 [cs.AI])

    [http://arxiv.org/abs/2307.06513](http://arxiv.org/abs/2307.06513)

    本文讨论了通过引入上下文反事实推理来准确校准AI系统中的信念的问题。研究发现，在高后悔情况下，上下文反事实和补救成本对于更新决策者的信念以及所持信念的强度至关重要。通过将信念的多样性分成两类:主观性和认识不确定性，可以更好地理解和处理信念的校准问题。

    

    通过调整数据采集原则或正则化训练过程中的损失函数等方式，人的信念和价值越来越多地被融入到我们的AI系统中。然而，元对齐问题是人类信念的多样性以及跨群体的不对齐性，而且即使在人类之间，每个信念的隐含强度也可能不好校准，特别是在尝试跨上下文进行推广时。具体而言，在高后悔情况下，我们观察到上下文反事实和补救成本对于更新决策者的信念以及所持信念的强度至关重要。因此，我们认为在对齐过程中引入反事实推理是准确校准信念的关键。为此，我们首先将信念的多样性分为两类：主观性（同一群体内的个体间）和认识不确定性（同一人在不同环境中）

    Beliefs and values are increasingly being incorporated into our AI systems through alignment processes, such as carefully curating data collection principles or regularizing the loss function used for training. However, the meta-alignment problem is that these human beliefs are diverse and not aligned across populations; furthermore, the implicit strength of each belief may not be well calibrated even among humans, especially when trying to generalize across contexts. Specifically, in high regret situations, we observe that contextual counterfactuals and recourse costs are particularly important in updating a decision maker's beliefs and the strengths to which such beliefs are held. Therefore, we argue that including counterfactuals is key to an accurate calibration of beliefs during alignment. To do this, we first segment belief diversity into two categories: subjectivity (across individuals within a population) and epistemic uncertainty (within an individual across different contexts
    
[^65]: 通过集成深度强化学习的混合控制策略实现人工胰腺

    Hybrid Control Policy for Artificial Pancreas via Ensemble Deep Reinforcement Learning. (arXiv:2307.06501v1 [cs.AI])

    [http://arxiv.org/abs/2307.06501](http://arxiv.org/abs/2307.06501)

    本研究提出了一种名为HyCPAP的混合控制策略，通过结合模型预测控制和集成深度强化学习，并充分利用它们各自的优势，以解决人工胰腺的复杂生理过程、延迟胰岛素反应和不准确血糖测量等挑战。

    

    目标：人工胰腺(AP)在实现1型糖尿病患者闭环血糖控制方面显示出有希望的潜力。然而，由于复杂的生理过程、延迟的胰岛素反应和不准确的血糖测量，设计一种有效的AP控制策略仍然具有挑战性。虽然模型预测控制(MPC)通过动态模型和安全约束提供了安全性和稳定性，但其缺乏个性化，并且受到未宣布的饮食影响。相反，深度强化学习(DRL)提供了个性化和自适应策略，但面临分布偏移和大量数据需求的挑战。方法：我们提出了一种混合控制策略，即HyCPAP，来应对上述挑战。HyCPAP将MPC策略与集成DRL策略相结合，充分利用两种策略的优势，同时弥补各自的局限性。

    Objective: The artificial pancreas (AP) has shown promising potential in achieving closed-loop glucose control for individuals with type 1 diabetes mellitus (T1DM). However, designing an effective control policy for the AP remains challenging due to the complex physiological processes, delayed insulin response, and inaccurate glucose measurements. While model predictive control (MPC) offers safety and stability through the dynamic model and safety constraints, it lacks individualization and is adversely affected by unannounced meals. Conversely, deep reinforcement learning (DRL) provides personalized and adaptive strategies but faces challenges with distribution shifts and substantial data requirements. Methods: We propose a hybrid control policy for the artificial pancreas (HyCPAP) to address the above challenges. HyCPAP combines an MPC policy with an ensemble DRL policy, leveraging the strengths of both policies while compensating for their respective limitations. To facilitate faste
    
[^66]: 基于微生物遗传算法的黑箱攻击对抗可解释深度学习系统

    Microbial Genetic Algorithm-based Black-box Attack against Interpretable Deep Learning Systems. (arXiv:2307.06496v1 [cs.CV])

    [http://arxiv.org/abs/2307.06496](http://arxiv.org/abs/2307.06496)

    这项工作提出了一种基于微生物遗传算法的黑箱攻击方法，QuScore，对抗对解释模型进行耦合的可解释深度学习系统(IDLSes)。该方法能够在不了解目标模型的情况下，通过转移和评分方法减少查询次数，实现成功的攻击。

    

    深度学习模型在白盒和黑盒环境中容易受到对抗样本的攻击。虽然先前的研究表明攻击成功率很高，但将DNN模型与解释模型相结合可以在人类专家介入时提供一种安全感，人类专家可以确定所给样本是良性的还是恶意的。然而，在白盒环境中，可解释的深度学习系统(IDLSes)易受恶意篡改的攻击。在黑盒设置中，由于对IDLSes组件的访问受限，对手更难以欺骗系统。在这项工作中，我们提出了一种基于查询效率和评分的黑箱攻击IDLSes的方法，称为QuScore，它不需要了解目标模型及其耦合的解释模型。QuScore基于转移和评分方法，采用一种有效的微生物遗传算法。我们的方法旨在减少必要的查询数量以进行成功的攻击。

    Deep learning models are susceptible to adversarial samples in white and black-box environments. Although previous studies have shown high attack success rates, coupling DNN models with interpretation models could offer a sense of security when a human expert is involved, who can identify whether a given sample is benign or malicious. However, in white-box environments, interpretable deep learning systems (IDLSes) have been shown to be vulnerable to malicious manipulations. In black-box settings, as access to the components of IDLSes is limited, it becomes more challenging for the adversary to fool the system. In this work, we propose a Query-efficient Score-based black-box attack against IDLSes, QuScore, which requires no knowledge of the target model and its coupled interpretation model. QuScore is based on transfer-based and score-based methods by employing an effective microbial genetic algorithm. Our method is designed to reduce the number of queries necessary to carry out success
    
[^67]: 自动化内容分析中的错误分类导致回归分析中的偏差。我们能修复吗？是的，我们能！

    Misclassification in Automated Content Analysis Causes Bias in Regression. Can We Fix It? Yes We Can!. (arXiv:2307.06483v1 [cs.LG])

    [http://arxiv.org/abs/2307.06483](http://arxiv.org/abs/2307.06483)

    传播学领域中的自动化内容分析常忽视了错误分类的偏差，我们介绍并测试了统计方法来纠正这种偏差，并设计了一种新方法来修复之。

    

    自动分类器（ACs）通常通过监督式机器学习（SML）构建，可以对从文本到图片和视频的大量数据进行分类，已经成为传播科学和相关领域中广泛流行的测量设备。尽管如此，即使是高度准确的分类器也会产生错误，这导致了错误分类的偏差和下游分析中误导性的结果，除非这些分析考虑到这些错误。通过对SML应用的系统文献综述，我们发现传播学者在很大程度上忽视了错误分类的偏差。原则上，现有的统计方法可以使用“黄金标准”验证数据（如由人类注释者创建的数据）来纠正错误分类的偏差，并产生一致的估计。我们介绍并测试了这些方法，包括我们在R包misclassificationmodels中设计和实现的一种新方法，通过蒙特卡洛模拟来揭示每种方法的局限性。

    Automated classifiers (ACs), often built via supervised machine learning (SML), can categorize large, statistically powerful samples of data ranging from text to images and video, and have become widely popular measurement devices in communication science and related fields. Despite this popularity, even highly accurate classifiers make errors that cause misclassification bias and misleading results in downstream analyses-unless such analyses account for these errors. As we show in a systematic literature review of SML applications, communication scholars largely ignore misclassification bias. In principle, existing statistical methods can use "gold standard" validation data, such as that created by human annotators, to correct misclassification bias and produce consistent estimates. We introduce and test such methods, including a new method we design and implement in the R package misclassificationmodels, via Monte Carlo simulations designed to reveal each method's limitations, which 
    
[^68]: 解决组合分布偏移问题：基于矩阵补全的观点

    Tackling Combinatorial Distribution Shift: A Matrix Completion Perspective. (arXiv:2307.06457v1 [cs.LG])

    [http://arxiv.org/abs/2307.06457](http://arxiv.org/abs/2307.06457)

    该论文研究了组合分布偏移的问题，提出了基于矩阵补全的解决方法。通过在特殊情况下的双线性嵌入，实现对训练中未涵盖的测试分布进行外推。这个设置将缺失非随机数据的矩阵补全问题广义化。

    

    在分布偏移下获得严格的统计保证仍然是一个开放且活跃的研究领域。我们研究了一种称为组合分布偏移的设置，其中(a)在测试和训练分布下，标签$z$由特征$(x,y)$的对决定，(b)训练分布涵盖了$x$和$y$分别的一定边缘分布，但是(c)测试分布涉及了一个在训练分布中未涵盖的$(x,y)$的产品分布的示例。我们专注于标签由双线性嵌入到Hilbert空间$H$中给出的特殊情况：$\mathbb{E}[z \mid x,y ]=\langle f_{\star}(x),g_{\star}(y)\rangle_{{H}}$，我们的目标是对在训练中未涵盖的测试分布域进行外推，即实现双线性组合外推。我们的设置将缺失非随机数据的矩阵补全的一个特殊情况广义化，对于该情况，所有现有结果都要求....

    Obtaining rigorous statistical guarantees for generalization under distribution shift remains an open and active research area. We study a setting we call combinatorial distribution shift, where (a) under the test- and training-distributions, the labels $z$ are determined by pairs of features $(x,y)$, (b) the training distribution has coverage of certain marginal distributions over $x$ and $y$ separately, but (c) the test distribution involves examples from a product distribution over $(x,y)$ that is {not} covered by the training distribution. Focusing on the special case where the labels are given by bilinear embeddings into a Hilbert space $H$: $\mathbb{E}[z \mid x,y ]=\langle f_{\star}(x),g_{\star}(y)\rangle_{{H}}$, we aim to extrapolate to a test distribution domain that is $not$ covered in training, i.e., achieving bilinear combinatorial extrapolation.  Our setting generalizes a special case of matrix completion from missing-not-at-random data, for which all existing results requi
    
[^69]: 随机时滞微分博弈：金融模型和机器学习算法

    Stochastic Delay Differential Games: Financial Modeling and Machine Learning Algorithms. (arXiv:2307.06450v1 [math.OC])

    [http://arxiv.org/abs/2307.06450](http://arxiv.org/abs/2307.06450)

    本文提出了一种使用深度学习方法找到随机时滞微分博弈闭环纳什均衡的数值方法，通过循环神经网络对每个玩家的控制进行参数化并使用修改后的布朗虚拟博弈结合深度学习技术进行训练。

    

    本文提出了一种通过深度学习方法找到随机时滞微分博弈闭环纳什均衡的数值方法。这些博弈在金融和经济学中很常见，多智能体相互作用和延迟效应往往是模型中希望的特征，但这也会增加问题的维度。我们的方法涉及使用不同的循环神经网络对每个玩家的控制进行参数化。然后，我们使用修改后的布朗虚拟博弈结合深度学习技术对这些循环神经网络控制进行训练。为了评估我们方法的有效性，我们在已知解的金融相关问题上进行了测试。此外，我们还开发了新的问题并推导出它们的分析纳什均衡。

    In this paper, we propose a numerical methodology for finding the closed-loop Nash equilibrium of stochastic delay differential games through deep learning. These games are prevalent in finance and economics where multi-agent interaction and delayed effects are often desired features in a model, but are introduced at the expense of increased dimensionality of the problem. This increased dimensionality is especially significant as that arising from the number of players is coupled with the potential infinite dimensionality caused by the delay. Our approach involves parameterizing the controls of each player using distinct recurrent neural networks. These recurrent neural network-based controls are then trained using a modified version of Brown's fictitious play, incorporating deep learning techniques. To evaluate the effectiveness of our methodology, we test it on finance-related problems with known solutions. Furthermore, we also develop new problems and derive their analytical Nash eq
    
[^70]: 在资源约束下的分布参数估计中的协作研究

    On Collaboration in Distributed Parameter Estimation with Resource Constraints. (arXiv:2307.06442v1 [cs.LG])

    [http://arxiv.org/abs/2307.06442](http://arxiv.org/abs/2307.06442)

    在资源约束下的分布参数估计中，我们研究了传感器/代理数据收集和协作策略，通过最大化费舍尔信息或最小化Cramer-Rao界来解决传感器/代理的数据收集和协作策略设计问题。

    

    我们研究了考虑资源约束和不同传感器/代理收集的观测之间的相关性的参数估计的传感器/代理数据收集和协作策略。具体地，我们考虑了一组传感器/代理，每个传感器/代理样本来自多元高斯分布的不同变量，并且具有不同的估计目标，我们将传感器/代理的数据收集和协作策略设计问题阐述为费舍尔信息最大化（或Cramer-Rao界最小化）问题。当变量之间的相关性知识可用时，我们可以分析地识别出两个特定情况：（1）不能利用样本之间的相关性知识进行协作估计的情况，（2）最优数据收集策略涉及投资有限资源以协作采样和转移已知统计信息的情况。

    We study sensor/agent data collection and collaboration policies for parameter estimation, accounting for resource constraints and correlation between observations collected by distinct sensors/agents. Specifically, we consider a group of sensors/agents each samples from different variables of a multivariate Gaussian distribution and has different estimation objectives, and we formulate a sensor/agent's data collection and collaboration policy design problem as a Fisher information maximization (or Cramer-Rao bound minimization) problem. When the knowledge of correlation between variables is available, we analytically identify two particular scenarios: (1) where the knowledge of the correlation between samples cannot be leveraged for collaborative estimation purposes and (2) where the optimal data collection policy involves investing scarce resources to collaboratively sample and transfer information that is not of immediate interest and whose statistics are already known, with the sol
    
[^71]: 没有训练就没有收益：重新审视基于Transformer的语言模型的高效训练算法

    No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models. (arXiv:2307.06440v1 [cs.LG])

    [http://arxiv.org/abs/2307.06440](http://arxiv.org/abs/2307.06440)

    本论文重新审视了基于Transformer的语言模型的高效训练算法，包括动态架构，批量选择和高效优化器。然而，在使用这些算法预训练时，相对于基线方法，它们的训练、验证和下游收益消失了。同时，论文提出了一个评估协议来进行计算，并释放了代码来促进高效训练的研究。

    

    近年来，训练Transformer-based语言模型所需的计算量急剧增加。这一趋势促使研究者们开展了针对高效训练算法的研究，旨在比标准训练更快地改善训练、验证和下游性能。在这项工作中，我们重新审视了三类这样的算法：动态架构（层叠、层丢弃）、批量选择（选择性反向传播、RHO损失）和高效优化器（Lion、Sophia）。当使用这些方法在固定计算预算下对BERT和T5进行预训练时，我们发现它们的训练、验证和下游收益相对于一个具有完全衰减学习率的基线而言会消失。我们定义了一个评估协议，可以通过将所有计算时间映射到一个称为参考系统时间的参考机器上，在任意机器上进行计算。我们讨论了我们提出的协议的局限性，并发布了我们的代码，以鼓励对高效训练的严格研究。

    The computation necessary for training Transformer-based language models has skyrocketed in recent years. This trend has motivated research on efficient training algorithms designed to improve training, validation, and downstream performance faster than standard training. In this work, we revisit three categories of such algorithms: dynamic architectures (layer stacking, layer dropping), batch selection (selective backprop, RHO loss), and efficient optimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed computation budget using such methods, we find that their training, validation, and downstream gains vanish compared to a baseline with a fully-decayed learning rate. We define an evaluation protocol that enables computation to be done on arbitrary machines by mapping all computation time to a reference machine which we call reference system time. We discuss the limitations of our proposed protocol and release our code to encourage rigorous research in efficient training p
    
[^72]: 利用图注意力网络和加权事件改进Belle II的选择性背景蒙特卡洛模拟

    Improved selective background Monte Carlo simulation at Belle II with graph attention networks and weighted events. (arXiv:2307.06434v1 [hep-ex])

    [http://arxiv.org/abs/2307.06434](http://arxiv.org/abs/2307.06434)

    这项工作通过引入图注意力网络和统计方法，改进了选择性背景蒙特卡洛模拟，并用于Belle II测量稀有过程中节约资源并处理过滤引入的偏差。

    

    在Belle II测量稀有过程时，需要巨大的亮度，这意味着需要大量的模拟来确定信号效率和背景贡献。然而，这个过程需要高计算成本，而大部分模拟数据，特别是在背景情况下，都被事件选择丢弃了。因此，在早期阶段引入使用图神经网络的过滤器，以节省资源用于探测器模拟和事件重建，并且还改进了过滤器的性能，并研究统计方法，包括采样和重新加权，来处理过滤引入的偏差。

    When measuring rare processes at Belle II, a huge luminosity is required, which means a large number of simulations are necessary to determine signal efficiencies and background contributions. However, this process demands high computation costs while most of the simulated data, in particular in case of background, are discarded by the event selection. Thus, filters using graph neural networks are introduced at an early stage to save the resources for the detector simulation and reconstruction of events discarded at analysis level. In our work, we improved the performance of the filters using graph attention and investigated statistical methods including sampling and reweighting to deal with the biases introduced by the filtering.
    
[^73]: 能量差异：一种适用于能量模型的独立于评分的损失函数

    Energy Discrepancies: A Score-Independent Loss for Energy-Based Models. (arXiv:2307.06431v1 [stat.ML])

    [http://arxiv.org/abs/2307.06431](http://arxiv.org/abs/2307.06431)

    我们提出了一种新的能量模型损失函数，能够在不依赖分数计算或昂贵的蒙特卡罗方法的情况下，近似实现显式分数匹配和负对数似然损失，并在学习低维数据分布时具有更好的性能。

    

    能量模型是一种简单而强大的概率模型，但它们的普及受到了训练的计算负担的限制。我们提出了一种新的损失函数称为能量差异（ED），它不依赖于分数的计算或昂贵的马尔可夫链蒙特卡罗。我们证明了在不同的极限下，ED接近于显式分数匹配和负对数似然损失，有效地在两者之间插值。因此，最小化ED估计克服了在基于分数的估计方法中遇到的近视问题，同时还享有理论保证。通过数值实验证明，与显式分数匹配或对比散度相比，ED能够更快速、更准确地学习低维数据分布。对于高维图像数据，我们描述了流形假设对我们方法的限制，并通过对e模型的训练，证明了能量差异的有效性。

    Energy-based models are a simple yet powerful class of probabilistic models, but their widespread adoption has been limited by the computational burden of training them. We propose a novel loss function called Energy Discrepancy (ED) which does not rely on the computation of scores or expensive Markov chain Monte Carlo. We show that ED approaches the explicit score matching and negative log-likelihood loss under different limits, effectively interpolating between both. Consequently, minimum ED estimation overcomes the problem of nearsightedness encountered in score-based estimation methods, while also enjoying theoretical guarantees. Through numerical experiments, we demonstrate that ED learns low-dimensional data distributions faster and more accurately than explicit score matching or contrastive divergence. For high-dimensional image data, we describe how the manifold hypothesis puts limitations on our approach and demonstrate the effectiveness of energy discrepancy by training the e
    
[^74]: 差分私有的解耦图卷积用于多粒度拓扑保护

    Differentially Private Decoupled Graph Convolutions for Multigranular Topology Protection. (arXiv:2307.06422v1 [cs.LG])

    [http://arxiv.org/abs/2307.06422](http://arxiv.org/abs/2307.06422)

    本论文提出了一种差分私有的解耦图卷积方法，用于多粒度拓扑保护。引入了图差分隐私框架，可以确保模型参数和预测的私密性。

    

    图学习方法，如基于图卷积的图神经网络（GNNs），在解决涉及图结构数据的实际学习问题方面非常成功。然而，图学习方法不仅通过其模型参数，还通过其模型预测暴露了敏感的用户信息和交互。因此，仅提供模型权重隐私的标准差分隐私（DP）技术是不充分的。这尤其适用于通过图卷积直接利用相邻节点属性进行节点预测的情况，这会带来额外的隐私泄露风险。为了解决这个问题，我们引入了图差分隐私（GDP），这是一个新的适用于图学习环境的形式化差分隐私框架，可以确保模型参数和预测都是可证明的私有的。此外，由于节点属性和图结构可能存在不同的隐私要求，我们引入了一种新颖的放松的节点层次隐私概念。

    Graph learning methods, such as Graph Neural Networks (GNNs) based on graph convolutions, are highly successful in solving real-world learning problems involving graph-structured data. However, graph learning methods expose sensitive user information and interactions not only through their model parameters but also through their model predictions. Consequently, standard Differential Privacy (DP) techniques that merely offer model weight privacy are inadequate. This is especially the case for node predictions that leverage neighboring node attributes directly via graph convolutions that create additional risks of privacy leakage. To address this problem, we introduce Graph Differential Privacy (GDP), a new formal DP framework tailored to graph learning settings that ensures both provably private model parameters and predictions. Furthermore, since there may be different privacy requirements for the node attributes and graph structure, we introduce a novel notion of relaxed node-level da
    
[^75]: 在贝叶斯网络中测试稀疏性假设

    Testing Sparsity Assumptions in Bayesian Networks. (arXiv:2307.06406v1 [stat.ML])

    [http://arxiv.org/abs/2307.06406](http://arxiv.org/abs/2307.06406)

    本论文研究了在贝叶斯网络中测试稀疏性假设的问题，并提出了一种基于样本特征值的假设检验方法，可以帮助选择适当的结构发现算法。

    

    贝叶斯网络（BN）结构发现算法通常要么对真正的底层网络稀疏性做出假设，要么受到计算限制而仅适用于具有少量变量的网络。尽管这些稀疏性假设可以采取多种形式，但通常假设集中在底层图的最大入度上限$\nabla_G$。Duttweiler等人（2023）的定理2证明了线性BN的标准化逆协方差矩阵$\Omega$的最大特征值是$\nabla_G$的一个下界。在此结果的基础上，本文提供了$\Omega$样本特征值的渐近性质和去偏过程，从而得到了一种假设检验，可以用来确定BN的最大入度是否大于1。建议在线性BN结构发现工作流中使用此假设检验来辅助选择适当的结构发现算法。

    Bayesian network (BN) structure discovery algorithms typically either make assumptions about the sparsity of the true underlying network, or are limited by computational constraints to networks with a small number of variables. While these sparsity assumptions can take various forms, frequently the assumptions focus on an upper bound for the maximum in-degree of the underlying graph $\nabla_G$. Theorem 2 in Duttweiler et. al. (2023) demonstrates that the largest eigenvalue of the normalized inverse covariance matrix ($\Omega$) of a linear BN is a lower bound for $\nabla_G$. Building on this result, this paper provides the asymptotic properties of, and a debiasing procedure for, the sample eigenvalues of $\Omega$, leading to a hypothesis test that may be used to determine if the BN has max in-degree greater than 1. A linear BN structure discovery workflow is suggested in which the investigator uses this hypothesis test to aid in selecting an appropriate structure discovery algorithm. Th
    
[^76]: 训练性、表达性和解释性在门控神经ODE中的应用

    Trainability, Expressivity and Interpretability in Gated Neural ODEs. (arXiv:2307.06398v1 [cs.LG])

    [http://arxiv.org/abs/2307.06398](http://arxiv.org/abs/2307.06398)

    本研究扩展了神经常微分方程（nODEs）模型，将其赋予自适应时间尺度，并称之为门控神经ODEs（gnODEs）。通过在连续记忆任务中展示，我们发现gnODEs具有学习（近似）连续吸引子的归纳偏差学习能力。此外，我们还展示了降维后的gnODEs在提高可解释性的同时保持了建模能力，并可以对学习到的吸引子结构进行显性可视化。

    

    理解生物和人工神经网络中的动态是机器学习和神经科学中一个重要的开放问题。特别是，涉及到复杂存储和检索的计算对于这些网络来说是一个很大的挑战，不容易实现或学习。最近，一类由神经常微分方程（nODEs）描述的模型已经成为一种强大的动态神经网络模型，能够捕捉到复杂的动态。在这里，我们通过引入门控交互作用，赋予nODEs自适应时间尺度，进一步扩展了nODEs模型，我们称之为门控神经ODEs（gnODEs）。通过使用需要对连续量进行记忆的任务，我们展示了gnODEs的归纳偏差学习（近似）连续吸引子。我们进一步展示了降维的gnODEs如何保持其建模能力，同时大大提高了可解释性，甚至允许对学习到的吸引子结构进行显性可视化。

    Understanding how the dynamics in biological and artificial neural networks implement the computations required for a task is a salient open question in machine learning and neuroscience. In particular, computations requiring complex memory storage and retrieval pose a significant challenge for these networks to implement or learn. Recently, a family of models described by neural ordinary differential equations (nODEs) has emerged as powerful dynamical neural network models capable of capturing complex dynamics. Here, we extend nODEs by endowing them with adaptive timescales using gating interactions. We refer to these as gated neural ODEs (gnODEs). Using a task that requires memory of continuous quantities, we demonstrate the inductive bias of the gnODEs to learn (approximate) continuous attractors. We further show how reduced-dimensional gnODEs retain their modeling power while greatly improving interpretability, even allowing explicit visualization of the structure of learned attrac
    
[^77]: 弱监督条件下的音视频事件定位的时间标签优化

    Temporal Label-Refinement for Weakly-Supervised Audio-Visual Event Localization. (arXiv:2307.06385v1 [cs.CV])

    [http://arxiv.org/abs/2307.06385](http://arxiv.org/abs/2307.06385)

    本文解决了弱监督条件下的音视频事件定位问题，通过使用基础模型在训练数据上以更细的时间分辨率估计标签，并提出辅助目标来处理合成视频的分布外特性。

    

    音视频事件定位是指在视频中对同时可见和可听到的事件进行时间定位和分类的任务。本文解决了在弱监督条件下的音视频事件定位问题，训练过程中只有视频级别的事件标签（仅有事件是否出现，但没有时间位置信息）可用于监督。我们的思路是使用一个基础模型在训练数据上以更细的时间分辨率估计标签，并使用这些标签重新训练模型。具体来说，我们通过以下步骤确定训练视频中每个帧片段的标签子集: (i) 用另一个视频中与视频级别标签没有重叠的帧替换片段外的帧， (ii) 将这个合成视频输入基础模型，仅提取该片段的标签。为了处理合成视频的分布外特性，我们提出了一个辅助目标，用于引入更多多样化的标签。

    Audio-Visual Event Localization (AVEL) is the task of temporally localizing and classifying \emph{audio-visual events}, i.e., events simultaneously visible and audible in a video. In this paper, we solve AVEL in a weakly-supervised setting, where only video-level event labels (their presence/absence, but not their locations in time) are available as supervision for training. Our idea is to use a base model to estimate labels on the training data at a finer temporal resolution than at the video level and re-train the model with these labels. I.e., we determine the subset of labels for each \emph{slice} of frames in a training video by (i) replacing the frames outside the slice with those from a second video having no overlap in video-level labels, and (ii) feeding this synthetic video into the base model to extract labels for just the slice in question. To handle the out-of-distribution nature of our synthetic videos, we propose an auxiliary objective for the base model that induces mor
    
[^78]: 使用表示学习和生物特征识别实现基于个性化的PPG数据异常检测

    Personalized Anomaly Detection in PPG Data using Representation Learning and Biometric Identification. (arXiv:2307.06380v1 [cs.LG])

    [http://arxiv.org/abs/2307.06380](http://arxiv.org/abs/2307.06380)

    本研究提出了一个两阶段框架，使用表示学习和个性化来改善PPG数据中的异常检测性能。通过表示学习将原始PPG信号转换为更具区分性和紧凑性的表示，然后结合三种不同的无监督异常检测方法实现运动检测和生物特征识别。实验结果证明，表示学习显著提高了异常检测的性能。

    

    光谱描记（PPG）信号通常通过可穿戴设备获取，具有持续健康监测的重要潜力。特别是那些以罕见和微妙的心脏模式来表现的心脏疾病可能会引起关注。然而，由于标记数据的稀缺性和不同受试者之间的高变异性，这些数据中的强健可靠的异常检测仍然是一个挑战。本文提出了一个两阶段框架，利用表示学习和个性化来提高PPG数据中的异常检测性能。首先，该框架利用表示学习将原始PPG信号转换成更具区分性和紧凑性的表示。然后，我们将三种不同的无监督异常检测方法应用于运动检测和生物特征识别。我们在两个不同的数据集上验证了我们的方法，包括广义和个性化的情景。结果表明，表示学习显著提高了异常检测的性能。

    Photoplethysmography (PPG) signals, typically acquired from wearable devices, hold significant potential for continuous fitness-health monitoring. In particular, heart conditions that manifest in rare and subtle deviating heart patterns may be interesting. However, robust and reliable anomaly detection within these data remains a challenge due to the scarcity of labeled data and high inter-subject variability. This paper introduces a two-stage framework leveraging representation learning and personalization to improve anomaly detection performance in PPG data. The proposed framework first employs representation learning to transform the original PPG signals into a more discriminative and compact representation. We then apply three different unsupervised anomaly detection methods for movement detection and biometric identification. We validate our approach using two different datasets in both generalized and personalized scenarios. The results show that representation learning significa
    
[^79]: 具有谱偏差和内核-任务对齐的物理信息神经网络

    Spectral-Bias and Kernel-Task Alignment in Physically Informed Neural Networks. (arXiv:2307.06362v1 [stat.ML])

    [http://arxiv.org/abs/2307.06362](http://arxiv.org/abs/2307.06362)

    本文提出了一个综合的理论框架，解决了物理信息神经网络（PINN）设计和训练协议的选择问题。通过将超参数化神经网络和高斯过程回归等价起来，推导出了一种在大数据集限制下决定PINN预测的积分微分方程，以及通过原始微分方程中源项的谱分解来量化网络引入的隐含偏差。

    

    物理信息神经网络（PINN）是解决微分方程的一种有前景的新方法。与许多其他深度学习方法一样，PINN的设计和训练协议的选择需要精心制定。在这里，我们提出了一个综合的理论框架，对这个重要问题进行了阐述。通过利用超参数化神经网络和高斯过程回归（GPR）之间的等价性，我们推导出一种在大数据集限制下决定PINN预测的积分微分方程——神经信息方程（NIE）。该方程通过反映架构选择的内核项来补充原始方程，并通过原始微分方程中源项的谱分解来量化网络引入的隐含偏差。

    Physically informed neural networks (PINNs) are a promising emerging method for solving differential equations. As in many other deep learning approaches, the choice of PINN design and training protocol requires careful craftsmanship. Here, we suggest a comprehensive theoretical framework that sheds light on this important problem. Leveraging an equivalence between infinitely over-parameterized neural networks and Gaussian process regression (GPR), we derive an integro-differential equation that governs PINN prediction in the large data-set limit -- the Neurally-Informed Equation (NIE). This equation augments the original one by a kernel term reflecting architecture choices and allows quantifying implicit bias induced by the network via a spectral decomposition of the source term in the original differential equation.
    
[^80]: 使用深度强化学习的X射线CT顺序实验设计

    Sequential Experimental Design for X-Ray CT Using Deep Reinforcement Learning. (arXiv:2307.06343v1 [eess.IV])

    [http://arxiv.org/abs/2307.06343](http://arxiv.org/abs/2307.06343)

    本论文提出了一种使用深度强化学习的顺序实验设计方法，该方法可以在X射线CT中减少扫描角度的数量同时保持重建质量，从而适用于在线质量控制。

    

    在X射线计算机断层扫描（CT）中，需从多个角度获取投影，并用于三维重建。为了使CT适用于在线质量控制，需要减少角度数目同时保持重建质量。稀疏角度断层扫描是从有限数据获取三维重建的常用方法。为了优化其性能，可以按序适应扫描角度，选择每个扫描对象最有信息量的角度。数学上，这对应于解决一个最优实验设计（OED）问题。OED问题是高维、非凸、双层优化问题，无法在线解决，即无法在扫描过程中解决。为了解决这些挑战，我们将OED问题在贝叶斯框架中建模为一个部分可观测马尔可夫决策过程，并通过深度强化学习来求解。该方法通过大量离线训练学习高效的非贪婪策略来解决给定类别的OED问题。

    In X-ray Computed Tomography (CT), projections from many angles are acquired and used for 3D reconstruction. To make CT suitable for in-line quality control, reducing the number of angles while maintaining reconstruction quality is necessary. Sparse-angle tomography is a popular approach for obtaining 3D reconstructions from limited data. To optimize its performance, one can adapt scan angles sequentially to select the most informative angles for each scanned object. Mathematically, this corresponds to solving and optimal experimental design (OED) problem. OED problems are high-dimensional, non-convex, bi-level optimization problems that cannot be solved online, i.e., during the scan. To address these challenges, we pose the OED problem as a partially observable Markov decision process in a Bayesian framework, and solve it through deep reinforcement learning. The approach learns efficient non-greedy policies to solve a given class of OED problems through extensive offline training rath
    
[^81]: 评估降解模型在排污管道CCTV检查规划中的适用性

    Assessment of the suitability of degradation models for the planning of CCTV inspections of sewer pipes. (arXiv:2307.06341v1 [cs.LG])

    [http://arxiv.org/abs/2307.06341](http://arxiv.org/abs/2307.06341)

    该论文评估了降解模型在排污管道CCTV检查规划中的适用性。结果表明，虽然集成模型具有最高的准确度，但无法推测长期降解；与此相反，逻辑回归模型准确度稍低，但能够产生可解释性强且一致的降解曲线。

    

    排污管道的降解引起了重大的经济、环境和健康问题。这些资产的维护需要有结构化的计划来进行检查，考虑到结构和环境特征以及以前检查报告的结果，可以更高效地进行。这项工作提出了一种方法来评估降解模型在规划检查时的适用性，考虑了三个维度：准确度指标、能够产生长期降解曲线和可解释性。结果表明，虽然集成模型具有最高的准确度，但它们无法推测管道的长期降解，而逻辑回归则提供了略低准确度的模型，能够产生具有很高可解释性的一致的降解曲线。通过一个使用案例来演示该方法及其效果

    The degradation of sewer pipes poses significant economical, environmental and health concerns. The maintenance of such assets requires structured plans to perform inspections, which are more efficient when structural and environmental features are considered along with the results of previous inspection reports. The development of such plans requires degradation models that can be based on statistical and machine learning methods. This work proposes a methodology to assess their suitability to plan inspections considering three dimensions: accuracy metrics, ability to produce long-term degradation curves and explainability. Results suggest that although ensemble models yield the highest accuracy, they are unable to infer the long-term degradation of the pipes, whereas the Logistic Regression offers a slightly less accurate model that is able to produce consistent degradation curves with a high explainability. A use case is presented to demonstrate this methodology and the efficiency o
    
[^82]: 不完整话语重写作为顺序贪婪标注

    Incomplete Utterance Rewriting as Sequential Greedy Tagging. (arXiv:2307.06337v1 [cs.LG])

    [http://arxiv.org/abs/2307.06337](http://arxiv.org/abs/2307.06337)

    本论文提出了一种基于序列标注的模型，能够更好地从对话上下文中提取信息，并引入了区分说话者的嵌入来建模说话者的变化。实验结果表明，该模型在恢复得分方面达到了最优，并在推理速度上胜过大多数之前的模型。

    

    不完整话语重写的任务最近受到了很多关注。之前的模型很难从对话上下文中提取信息，这在恢复得分低的情况下得以证明。为了解决这个问题，我们提出了一种新颖的基于序列标注的模型，更擅长从上下文中提取信息。同时，我们引入了区分说话者的嵌入来建模说话者的变化。在多个公开数据集上的实验表明，我们的模型在所有九个恢复得分上都达到了最优结果，同时其他指标得分也与之前最先进的模型相当。此外，得益于模型的简单性，我们的方法在推理速度上超过了大多数之前的模型。

    The task of incomplete utterance rewriting has recently gotten much attention. Previous models struggled to extract information from the dialogue context, as evidenced by the low restoration scores. To address this issue, we propose a novel sequence tagging-based model, which is more adept at extracting information from context. Meanwhile, we introduce speaker-aware embedding to model speaker variation. Experiments on multiple public datasets show that our model achieves optimal results on all nine restoration scores while having other metric scores comparable to previous state-of-the-art models. Furthermore, benefitting from the model's simplicity, our approach outperforms most previous models on inference speed.
    
[^83]: 经过长距离步骤的梯度下降的可证明更快收敛速度

    Provably Faster Gradient Descent via Long Steps. (arXiv:2307.06324v1 [math.OC])

    [http://arxiv.org/abs/2307.06324](http://arxiv.org/abs/2307.06324)

    本研究通过计算机辅助分析技术，证明了非常数步长策略下的梯度下降方法经过长距离步骤可以实现更快的收敛速度。

    

    本研究通过计算机辅助分析技术，建立了经过长距离步骤的梯度下降的可证明更快收敛速度。我们的理论允许非常数步长策略，通过分析多次迭代的整体效果而不是典型的一次迭代归纳使用的，从而有可能破坏下降。我们表明，长距离步骤，可能在短期内增加目标值，但在长期内带来更快的收敛速度。此外，我们还提出了一个关于梯度下降更快收敛速度的猜想，并进行了简单的数值验证。

    This work establishes provably faster convergence rates for gradient descent via a computer-assisted analysis technique. Our theory allows nonconstant stepsize policies with frequent long steps potentially violating descent by analyzing the overall effect of many iterations at once rather than the typical one-iteration inductions used in most first-order method analyses. We show that long steps, which may increase the objective value in the short term, lead to provably faster convergence in the long term. A conjecture towards proving a faster $O(1/T\log T)$ rate for gradient descent is also motivated along with simple numerical validation.
    
[^84]: 使用图像技术填补时间序列间隙的方法：基于多维环境自编码器的建筑能源数据填补

    Filling time-series gaps using image techniques: Multidimensional context autoencoder approach for building energy data imputation. (arXiv:2307.05926v1 [cs.LG])

    [http://arxiv.org/abs/2307.05926](http://arxiv.org/abs/2307.05926)

    本论文提出了一种使用多维环境自编码器的方法来填补能源数据中的缺失间隙。这个方法可以解决能源系统准确预测和管理的问题，并提高数据在决策和研究中的可用性。

    

    建筑能源预测和管理在最近几十年中变得越来越重要，受到物联网设备的增长和更多能源数据的可用性的推动。然而，能源数据经常来自多个来源，可能不完整或不一致，这可能阻碍能源系统的准确预测和管理，并限制数据在决策和研究中的用途。为了解决这个问题，过去的研究集中于填补能源数据中的缺失间隙，包括随机间隙和连续间隙。这一领域的主要挑战之一是缺乏在各种建筑和仪表类型的基准数据集上进行验证，这使得难以准确评估不同填补方法的性能。另一个挑战是缺乏将最先进的填补方法应用于能源数据中的缺失间隙。现代图像修复方法，如部分卷积(PConv)，已经广泛应用

    Building energy prediction and management has become increasingly important in recent decades, driven by the growth of Internet of Things (IoT) devices and the availability of more energy data. However, energy data is often collected from multiple sources and can be incomplete or inconsistent, which can hinder accurate predictions and management of energy systems and limit the usefulness of the data for decision-making and research. To address this issue, past studies have focused on imputing missing gaps in energy data, including random and continuous gaps. One of the main challenges in this area is the lack of validation on a benchmark dataset with various building and meter types, making it difficult to accurately evaluate the performance of different imputation methods. Another challenge is the lack of application of state-of-the-art imputation methods for missing gaps in energy data. Contemporary image-inpainting methods, such as Partial Convolution (PConv), have been widely used 
    
[^85]: 针对边缘/云计算环境中数字孪生的高效任务卸载算法

    Efficient Task Offloading Algorithm for Digital Twin in Edge/Cloud Computing Environment. (arXiv:2307.05888v1 [cs.LG])

    [http://arxiv.org/abs/2307.05888](http://arxiv.org/abs/2307.05888)

    本论文提出了一种新的数字孪生系统模型，考虑了异构MEC/MCC环境，并基于分布式深度学习提出了一种新的任务卸载方案，实现了高效的实时反馈。

    

    在物联网时代，数字孪生被视为连接实物对象和数字世界之间的桥梁，通过虚拟化和模拟技术，可以利用计算资源实现多种功能。在这个过程中，移动云计算和移动边缘计算已成为实现实时反馈的关键因素。然而，当前的研究只考虑了数字孪生系统模型中的边缘服务器或云服务器，同时忽略了具有多个数据资源的数字孪生。本文提出了一种考虑异构MEC/MCC环境的新的数字孪生系统模型，模型中的每个数字孪生都通过多个数据采集设备在服务器中维护。还考虑了卸载决策问题，并基于分布式深度学习提出了一种新的卸载方案。仿真结果表明，我们提出的算法可以有效且高效地解决这个问题。

    In the era of Internet of Things (IoT), Digital Twin (DT) is envisioned to empower various areas as a bridge between physical objects and the digital world. Through virtualization and simulation techniques, multiple functions can be achieved by leveraging computing resources. In this process, Mobile Cloud Computing (MCC) and Mobile Edge Computing (MEC) have become two of the key factors to achieve real-time feedback. However, current works only considered edge servers or cloud servers in the DT system models. Besides, The models ignore the DT with not only one data resource. In this paper, we propose a new DT system model considering a heterogeneous MEC/MCC environment. Each DT in the model is maintained in one of the servers via multiple data collection devices. The offloading decision-making problem is also considered and a new offloading scheme is proposed based on Distributed Deep Learning (DDL). Simulation results demonstrate that our proposed algorithm can effectively and efficie
    
[^86]: PIGEON: 预测图像地理位置

    PIGEON: Predicting Image Geolocations. (arXiv:2307.05845v1 [cs.CV])

    [http://arxiv.org/abs/2307.05845](http://arxiv.org/abs/2307.05845)

    PIGEON是一个用于全球规模图像地理定位的多任务端到端系统，通过语义地理单元的创建和精化，以及无监督聚类和ProtoNets的应用，实现了最先进的性能，并提供了预训练的CLIP转换器模型StreetCLIP。

    

    我们引入PIGEON，一个用于全球规模图像地理定位的多任务端到端系统，在外部基准测试和人类评估中均实现了最先进的性能。我们的工作结合语义地理单元的创建和标签平滑，对具有地理信息的图像进行视觉转换器的预训练，并通过ProtoNets在候选地理单元集合中改进位置预测。PIGEON的贡献有三个方面：首先，我们设计了一种基于开源数据的语义地理单元创建和分割算法，可以适用于任何地理空间数据集。第二，我们展示了地理单元内部精化的有效性，并展示了无监督聚类和ProtoNets在该任务中的适用性。最后，我们将我们预训练的CLIP转换器模型，StreetCLIP，公开提供，可用于与应对气候变化和城市乡村场景理解相关的领域。

    We introduce PIGEON, a multi-task end-to-end system for planet-scale image geolocalization that achieves state-of-the-art performance on both external benchmarks and in human evaluation. Our work incorporates semantic geocell creation with label smoothing, conducts pretraining of a vision transformer on images with geographic information, and refines location predictions with ProtoNets across a candidate set of geocells. The contributions of PIGEON are three-fold: first, we design a semantic geocells creation and splitting algorithm based on open-source data which can be adapted to any geospatial dataset. Second, we show the effectiveness of intra-geocell refinement and the applicability of unsupervised clustering and ProtNets to the task. Finally, we make our pre-trained CLIP transformer model, StreetCLIP, publicly available for use in adjacent domains with applications to fighting climate change and urban and rural scene understanding.
    
[^87]: 大型语言模型用于供应链优化

    Large Language Models for Supply Chain Optimization. (arXiv:2307.03875v1 [cs.AI])

    [http://arxiv.org/abs/2307.03875](http://arxiv.org/abs/2307.03875)

    这项研究研究了利用大型语言模型（LLMs）来帮助解释和解读供应链优化结果的方法。他们设计了一个框架，可以接受普通文本查询作为输入，并输出关于底层优化结果的洞察。通过定量回答假设情况，该框架在不放弃最先进的组合优化技术的情况下帮助企业运营者更好地理解和信任优化结果。

    

    传统上，供应链操作涉及各种复杂的决策问题。在过去几十年中，供应链受益于计算技术的进步，从手动处理过渡到自动化和成本效益优化。然而，企业运营者仍然需要花费大量精力来解释和解读优化结果给相关人士。受大型语言模型(LLMs)最近的进展的启发，我们研究了这种颠覆性技术如何帮助弥合供应链自动化和人类理解与信任之间的差距。我们设计了一个名为\name{}的框架，它接受普通文本查询作为输入，并输出关于底层优化结果的洞察。我们的框架并没有放弃最先进的组合优化技术，而是利用它来定量地回答假设情况（例如，如果我们使用供应商B而不是供应商A，成本会如何变化）。

    Supply chain operations traditionally involve a variety of complex decision making problems. Over the last few decades, supply chains greatly benefited from advances in computation, which allowed the transition from manual processing to automation and cost-effective optimization. Nonetheless, business operators still need to spend substantial efforts in \emph{explaining} and interpreting the optimization outcomes to stakeholders. Motivated by the recent advances in Large Language Models (LLMs), we study how this disruptive technology can help bridge the gap between supply chain automation and human comprehension and trust thereof. We design \name{} -- a framework that accepts as input queries in plain text, and outputs insights about the underlying optimization outcomes. Our framework does not forgo the state-of-the-art combinatorial optimization technology, but rather leverages it to quantitatively answer what-if scenarios (e.g., how would the cost change if we used supplier B instead
    
[^88]: inTformer: 一种基于时间嵌入的关注机制Transformer用于使用连接车辆数据的路口事故可能性预测

    inTformer: A Time-Embedded Attention-Based Transformer for Crash Likelihood Prediction at Intersections Using Connected Vehicle Data. (arXiv:2307.03854v1 [cs.LG])

    [http://arxiv.org/abs/2307.03854](http://arxiv.org/abs/2307.03854)

    inTformer是一种基于时间嵌入的关注机制Transformer，用于使用连接车辆数据预测路口事故可能性。与其他深度学习模型相比，inTformer具有处理长期依赖性、并行处理数据序列以及解决梯度消失问题的优势。

    

    实时事故可能性预测模型是主动交通安全管理系统的关键组成部分。多年来，许多研究尝试构建事故可能性预测模型，以提高交通安全，但主要集中在高速公路上。在大多数现有研究中，研究人员主要采用基于深度学习的框架来识别事故潜在风险。最近，Transformer已经成为一种潜在的深度神经网络，其基本原理是通过注意力机制来进行操作。Transformer在功能上比现有的深度学习模型（如LSTM，CNN等）具有几个优势。首先，Transformer可以轻松处理数据序列中的长期依赖性。其次，Transformer可以在训练期间并行处理数据序列中的所有元素。最后，Transformer不存在梯度消失的问题。认识到Transformer的巨大潜力，

    The real-time crash likelihood prediction model is an essential component of the proactive traffic safety management system. Over the years, numerous studies have attempted to construct a crash likelihood prediction model in order to enhance traffic safety, but mostly on freeways. In the majority of the existing studies, researchers have primarily employed a deep learning-based framework to identify crash potential. Lately, Transformer has emerged as a potential deep neural network that fundamentally operates through attention-based mechanisms. Transformer has several functional benefits over extant deep learning models such as Long Short-Term Memory (LSTM), Convolution Neural Network (CNN), etc. Firstly, Transformer can readily handle long-term dependencies in a data sequence. Secondly, Transformer can parallelly process all elements in a data sequence during training. Finally, Transformer does not have the vanishing gradient issue. Realizing the immense possibility of Transformer, th
    
[^89]: 理解不确定性采样

    Understanding Uncertainty Sampling. (arXiv:2307.02719v1 [cs.LG])

    [http://arxiv.org/abs/2307.02719](http://arxiv.org/abs/2307.02719)

    本研究通过系统研究流式和池式主动学习下的不确定性采样算法，提出了一个等效损失的概念，并证明不确定性采样算法实质上是针对该等效损失进行优化。

    

    不确定性采样是一种常见的主动学习算法，它顺序地查询当前预测模型对数据样本的不确定性。然而，不确定性采样的使用往往是启发式的：（i）关于在特定任务和特定损失函数下对“不确定性”的准确定义没有共识；（ii）没有理论保证能够给出一个标准协议来实施该算法，例如，在随机梯度下降等优化算法框架下如何处理顺序到达的注释数据。在本研究中，我们系统地研究了流式和池式主动学习下的不确定性采样算法。我们提出了一个等效损失的概念，该概念取决于使用的不确定性度量和原始损失函数，并确立了不确定性采样算法本质上是针对这种等效损失进行优化。这一观点验证了算法的适当性。

    Uncertainty sampling is a prevalent active learning algorithm that queries sequentially the annotations of data samples which the current prediction model is uncertain about. However, the usage of uncertainty sampling has been largely heuristic: (i) There is no consensus on the proper definition of "uncertainty" for a specific task under a specific loss; (ii) There is no theoretical guarantee that prescribes a standard protocol to implement the algorithm, for example, how to handle the sequentially arrived annotated data under the framework of optimization algorithms such as stochastic gradient descent. In this work, we systematically examine uncertainty sampling algorithms under both stream-based and pool-based active learning. We propose a notion of equivalent loss which depends on the used uncertainty measure and the original loss function and establish that an uncertainty sampling algorithm essentially optimizes against such an equivalent loss. The perspective verifies the properne
    
[^90]: 具有状态相关噪声的加速随机逼近

    Accelerated stochastic approximation with state-dependent noise. (arXiv:2307.01497v1 [math.OC])

    [http://arxiv.org/abs/2307.01497](http://arxiv.org/abs/2307.01497)

    该论文研究了一类具有状态相关噪声的随机平滑凸优化问题。通过引入两种非欧几里得加速随机逼近算法，实现了在精度、问题参数和小批量大小方面的最优性。

    

    我们考虑具有一般噪声假设的随机平滑凸优化问题的一类问题，在这些问题中，随机梯度观测的噪声的方差与算法产生的近似解的"亚最优性" 相关。这类问题在多种应用中自然而然地出现，特别是在统计学中的广义线性回归问题中。然而，据我们所知，现有的解决这类问题的随机逼近算法在精度、问题参数和小批量大小的依赖性方面都未达到最优。我们讨论了两种非欧几里得加速随机逼近算法——随机加速梯度下降（SAGD）和随机梯度外推（SGE）——它们具有一种特殊的对偶关系

    We consider a class of stochastic smooth convex optimization problems under rather general assumptions on the noise in the stochastic gradient observation. As opposed to the classical problem setting in which the variance of noise is assumed to be uniformly bounded, herein we assume that the variance of stochastic gradients is related to the "sub-optimality" of the approximate solutions delivered by the algorithm. Such problems naturally arise in a variety of applications, in particular, in the well-known generalized linear regression problem in statistics. However, to the best of our knowledge, none of the existing stochastic approximation algorithms for solving this class of problems attain optimality in terms of the dependence on accuracy, problem parameters, and mini-batch size.  We discuss two non-Euclidean accelerated stochastic approximation routines--stochastic accelerated gradient descent (SAGD) and stochastic gradient extrapolation (SGE)--which carry a particular duality rela
    
[^91]: 用神经符号深度强化学习方法实现安全自主驾驶策略的研究

    Towards Safe Autonomous Driving Policies using a Neuro-Symbolic Deep Reinforcement Learning Approach. (arXiv:2307.01316v1 [cs.RO])

    [http://arxiv.org/abs/2307.01316](http://arxiv.org/abs/2307.01316)

    本文介绍了一种名为DRL with Symbolic Logics (DRLSL)的新颖神经符号无模型深度强化学习方法，旨在实现在真实环境中安全学习自主驾驶策略。该方法结合了深度强化学习和符号逻辑驱动的推理，允许通过与物理环境的实时交互来学习自主驾驶策略并确保安全性。

    

    自主驾驶中的动态驾驶环境和多样化道路使用者的存在给决策造成了巨大的挑战。深度强化学习(DRL)已成为解决这一问题的一种流行方法。然而，由于安全问题的限制，现有的DRL解决方案的应用主要局限于模拟环境，阻碍了它们在现实世界中的部署。为了克服这一局限，本文引入了一种新颖的神经符号无模型深度强化学习方法，称为带有符号逻辑的DRL(DRLSL)，它将DRL(从经验中学习)和符号一阶逻辑知识驱动的推理相结合，以实现在实际环境下安全学习自主驾驶的实时交互。这种创新的方法提供了一种通过积极与物理环境互动来学习自主驾驶政策并确保安全性的方式。我们使用高维度数据实现了自主驾驶的DRLSL框架。

    The dynamic nature of driving environments and the presence of diverse road users pose significant challenges for decision-making in autonomous driving. Deep reinforcement learning (DRL) has emerged as a popular approach to tackle this problem. However, the application of existing DRL solutions is mainly confined to simulated environments due to safety concerns, impeding their deployment in real-world. To overcome this limitation, this paper introduces a novel neuro-symbolic model-free DRL approach, called DRL with Symbolic Logics (DRLSL) that combines the strengths of DRL (learning from experience) and symbolic first-order logics knowledge-driven reasoning) to enable safe learning in real-time interactions of autonomous driving within real environments. This innovative approach provides a means to learn autonomous driving policies by actively engaging with the physical environment while ensuring safety. We have implemented the DRLSL framework in autonomous driving using the highD data
    
[^92]: 超越快照：基于脑连接图的时间序列脑功能连接嵌入的神经网络翻译器

    Beyond the Snapshot: Brain Tokenized Graph Transformer for Longitudinal Brain Functional Connectome Embedding. (arXiv:2307.00858v2 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2307.00858](http://arxiv.org/abs/2307.00858)

    本论文提出了一种新的解释性框架，用于将脑功能连接的轨迹嵌入神经退行性疾病的诊断和预后中，并且提出了脑连接图嵌入翻译器（Brain TokenGT）方法。该方法能够揭示脑功能连接随疾病进展的演变方式，对于制定有效的疾病干预策略具有重要意义。

    

    在基于网络的神经退行性疾病框架下，基于脑功能连接图的图神经网络已成为诸如阿尔茨海默病等神经退行性疾病的诊断和预后的有价值工具。然而，这些模型专门针对单个时间点的脑功能连接而不是对脑功能连接的轨迹进行表征。辨明脑功能连接随疾病进展的演变方式，特别是在类似于认知正常的淀粉样沉积或轻度认知障碍的前痴呆阶段，对于描绘疾病传播模式并制定有效的减缓甚至阻止疾病进展策略至关重要。在这项工作中，我们提出了第一个适用于神经退行性疾病诊断和预后的脑功能连接轨迹嵌入的解释性框架，即脑连接图嵌入翻译器（Brain TokenGT）。它包含两个模块：1）图不变和变体嵌入（

    Under the framework of network-based neurodegeneration, brain functional connectome (FC)-based Graph Neural Networks (GNN) have emerged as a valuable tool for the diagnosis and prognosis of neurodegenerative diseases such as Alzheimer's disease (AD). However, these models are tailored for brain FC at a single time point instead of characterizing FC trajectory. Discerning how FC evolves with disease progression, particularly at the predementia stages such as cognitively normal individuals with amyloid deposition or individuals with mild cognitive impairment (MCI), is crucial for delineating disease spreading patterns and developing effective strategies to slow down or even halt disease advancement. In this work, we proposed the first interpretable framework for brain FC trajectory embedding with application to neurodegenerative disease diagnosis and prognosis, namely Brain Tokenized Graph Transformer (Brain TokenGT). It consists of two modules: 1) Graph Invariant and Variant Embedding (
    
[^93]: 明里暗中：利用差分隐私噪音的抵御恶意攻击的本地中毒攻击方法在多智能体强化学习中的应用

    Hiding in Plain Sight: Differential Privacy Noise Exploitation for Evasion-resilient Localized Poisoning Attacks in Multiagent Reinforcement Learning. (arXiv:2307.00268v1 [cs.LG])

    [http://arxiv.org/abs/2307.00268](http://arxiv.org/abs/2307.00268)

    本文介绍了一种利用差分隐私噪音的本地中毒攻击方法（PeLPA）以绕过异常检测系统，并针对合作多智能体强化学习（CMARL）中的私有知识共享过程中的中毒威胁。研究结果表明，在不同环境下，PeLPA攻击能够显著增加平均步数。

    

    最近，差分隐私（DP）被引入到合作多智能体强化学习（CMARL）中，以保护智能体在知识共享过程中免受对手的推断攻击。然而，我们认为由DP机制引入的噪音可能会在CMARL中的私有知识共享过程中意外地产生一种新的中毒威胁，这在文献中尚未得到研究。为了解决这个问题，我们提出了一种自适应的、利用隐私的、抵御逃避攻击的本地中毒攻击方法（PeLPA），利用了DP噪音的特性，绕过异常检测系统，并阻碍CMARL模型的最优收敛。我们在不同的环境中对我们提出的PeLPA攻击进行了严格的评估，包括非对抗和多对抗环境。我们的研究结果表明，在中等规模环境中，攻击者比例为20%和40%的PeLPA攻击可能导致平均步数的增加。

    Lately, differential privacy (DP) has been introduced in cooperative multiagent reinforcement learning (CMARL) to safeguard the agents' privacy against adversarial inference during knowledge sharing. Nevertheless, we argue that the noise introduced by DP mechanisms may inadvertently give rise to a novel poisoning threat, specifically in the context of private knowledge sharing during CMARL, which remains unexplored in the literature. To address this shortcoming, we present an adaptive, privacy-exploiting, and evasion-resilient localized poisoning attack (PeLPA) that capitalizes on the inherent DP-noise to circumvent anomaly detection systems and hinder the optimal convergence of the CMARL model. We rigorously evaluate our proposed PeLPA attack in diverse environments, encompassing both non-adversarial and multiple-adversarial contexts. Our findings reveal that, in a medium-scale environment, the PeLPA attack with attacker ratios of 20% and 40% can lead to an increase in average steps t
    
[^94]: TrustGuard: 基于GNN的动态支持鲁棒且可解释的信任评估

    TrustGuard: GNN-based Robust and Explainable Trust Evaluation with Dynamicity Support. (arXiv:2306.13339v1 [cs.LG])

    [http://arxiv.org/abs/2306.13339](http://arxiv.org/abs/2306.13339)

    TrustGuard是一种基于GNN的信任评估模型，支持信任动态性，抗击鲁棒并提供解释能力，它的实验结果在准确性、鲁棒性和可解释性方面都优于其他方法。

    

    信任评估评估实体之间的信任关系并促进决策。机器学习由于其学习能力而表现出巨大的潜力，因此对信任评估具有重要意义。近年来，作为一种新的机器学习范 paradigm，图神经网络（GNN）在处理图形数据方面表现出优越性。这激发了研究人员探索将其用于信任评估，因为实体之间的信任关系可以建模为图形。但是，使用GNN的当前信任评估方法未能完全满足信任的动态性，忽略了攻击对信任评估的不利影响，并且无法提供令人信服的评估结果解释。为解决这些问题，在本文中，我们提出了TrustGuard ：一种支持信任动态性、抗击鲁棒且通过可视化提供解释的精确信任评估模型。具体而言，TrustGuard 设计了一个由动态感知节点嵌入层、图卷积层、注意机制层和信任预测层组成的分层架构。为了评估提出的模型的有效性，我们对真实数据集进行了实验，并将TrustGuard与其他最先进的方法进行了比较。实验结果表明，TrustGuard 在准确性、鲁棒性和可解释性方面均优于其他方法。

    Trust evaluation assesses trust relationships between entities and facilitates decision-making. Machine Learning (ML) shows great potential for trust evaluation owing to its learning capabilities. In recent years, Graph Neural Networks (GNNs), as a new ML paradigm, have demonstrated superiority in dealing with graph data. This has motivated researchers to explore their use in trust evaluation, as trust relationships among entities can be modeled as a graph. However, current trust evaluation methods that employ GNNs fail to fully satisfy the dynamicity nature of trust, overlook the adverse effects of attacks on trust evaluation, and cannot provide convincing explanations on evaluation results. To address these problems, in this paper, we propose TrustGuard, a GNN-based accurate trust evaluation model that supports trust dynamicity, is robust against typical attacks, and provides explanations through visualization. Specifically, TrustGuard is designed with a layered architecture that con
    
[^95]: 自主驾驶系统中数据集成的深度学习方法综述

    A survey on deep learning approaches for data integration in autonomous driving system. (arXiv:2306.11740v1 [cs.RO])

    [http://arxiv.org/abs/2306.11740](http://arxiv.org/abs/2306.11740)

    本文综述了自主驾驶系统感知模块的最新深度学习集成技术。其提出了一个新的集成分类系统，总结了集成操作及其优缺点，提供了新的见解，阐明了“理想”数据集成方法的特性，可减轻现有方法的局限性。本文总结了优化数据集成方法的关键特点。

    

    自主驾驶汽车的感知模块依赖于多传感器系统来理解其环境。深度学习的最新进展促进了多感知测量的集成方法的迅速发展，以增强感知能力。本文对应用于自主驾驶系统感知模块的最新深度学习集成技术进行了调查，将集成方法分为“何时集成”，“如何集成”和“何时集成”三类。提出了一个新的集成分类系统，基于三个维度：多视角，多模态和多帧。总结了集成操作及其优缺点，提供了新的见解，阐明了“理想”数据集成方法的特性，可减轻现有方法的局限性。经过对上百篇相关论文的审查，本综述总结了优化数据集成方法的关键特点。

    The perception module of self-driving vehicles relies on a multi-sensor system to understand its environment. Recent advancements in deep learning have led to the rapid development of approaches that integrate multi-sensory measurements to enhance perception capabilities. This paper surveys the latest deep learning integration techniques applied to the perception module in autonomous driving systems, categorizing integration approaches based on "what, how, and when to integrate." A new taxonomy of integration is proposed, based on three dimensions: multi-view, multi-modality, and multi-frame. The integration operations and their pros and cons are summarized, providing new insights into the properties of an "ideal" data integration approach that can alleviate the limitations of existing methods. After reviewing hundreds of relevant papers, this survey concludes with a discussion of the key features of an optimal data integration approach.
    
[^96]: 预测晶体性质的完整相互作用势的高效逼近

    Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction. (arXiv:2306.10045v1 [physics.chem-ph])

    [http://arxiv.org/abs/2306.10045](http://arxiv.org/abs/2306.10045)

    本研究提出了一种高效逼近晶体材料的完整相互作用势的方法，能够覆盖所有原子对之间的势，克服了目前方法中只考虑附近原子间势和无法捕捉无限重复模式的限制。

    

    我们研究了晶体材料的性质预测。晶体结构由一个最小的单元格组成，在三维空间中无限重复。如何在机器学习模型中准确表示这种重复结构仍然没有解决。当前的方法只在附近的节点之间建立边缘来构建图形，因此无法忠实地捕捉无限重复的模式和远距离的原子间相互作用。在这项工作中，我们提出了几个创新来克服这些限制。首先，我们建议直接建模物理原理的相互作用势，而不仅仅使用距离，如许多现有方法所做的。这些势包括库仑势，伦敦分散势和Pauli斥力势。其次，我们建模所有原子之间的完整势，而不仅仅是现有方法中的附近原子之间的势。这得益于我们用可证明的误差界逼近无限势和的方法。我们进一步开发了...

    We study property prediction for crystal materials. A crystal structure consists of a minimal unit cell that is repeated infinitely in 3D space. How to accurately represent such repetitive structures in machine learning models remains unresolved. Current methods construct graphs by establishing edges only between nearby nodes, thereby failing to faithfully capture infinite repeating patterns and distant interatomic interactions. In this work, we propose several innovations to overcome these limitations. First, we propose to model physics-principled interatomic potentials directly instead of only using distances as in many existing methods. These potentials include the Coulomb potential, London dispersion potential, and Pauli repulsion potential. Second, we model the complete set of potentials among all atoms, instead of only between nearby atoms as in existing methods. This is enabled by our approximations of infinite potential summations with provable error bounds. We further develop 
    
[^97]: LLMs如何改变材料科学和化学：大型语言模型黑客马拉松的反思

    14 Examples of How LLMs Can Transform Materials Science and Chemistry: A Reflection on a Large Language Model Hackathon. (arXiv:2306.06283v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2306.06283](http://arxiv.org/abs/2306.06283)

    本文记录了一次黑客松活动，参与者使用LLMs进行了各种应用，包括预测分子和材料特性、从非结构化数据中提取知识、为工具设计新界面以及开发新的教育应用。这些多样化的项目反映了LLMs在材料科学和化学领域的多功能性和潜力。

    

    化学和材料科学非常复杂。最近，使用数据驱动或计算技术解决了这种复杂性的中有很大的成功。然而，输入需要非常特定形式的结构以及工具数量不断增长所带来可用性和可访问性的挑战。加上这些学科中的大多数数据都是非结构化的事实，使得这些工具的效率受到限制。本文记录了关于LLMs的黑客松活动中构建的项目。参与者使用LLMs进行了各种应用，包括预测分子和材料的特性、为工具设计新界面、从非结构化数据中提取知识以及开发新的教育应用。各种各样的项目反映了LLMs在这些领域的多功能性和这些模型改变材料科学和化学领域的潜力。

    Chemistry and materials science are complex. Recently, there have been great successes in addressing this complexity using data-driven or computational techniques. Yet, the necessity of input structured in very specific forms and the fact that there is an ever-growing number of tools creates usability and accessibility challenges. Coupled with the reality that much data in these disciplines is unstructured, the effectiveness of these tools is limited.  Motivated by recent works that indicated that large language models (LLMs) might help address some of these issues, we organized a hackathon event on the applications of LLMs in chemistry, materials science, and beyond. This article chronicles the projects built as part of this hackathon. Participants employed LLMs for various applications, including predicting properties of molecules and materials, designing novel interfaces for tools, extracting knowledge from unstructured data, and developing new educational applications.  The diverse
    
[^98]: 联邦学习的个性化分离

    Personalization Disentanglement for Federated Learning. (arXiv:2306.03570v1 [cs.LG])

    [http://arxiv.org/abs/2306.03570](http://arxiv.org/abs/2306.03570)

    本文通过使用联邦双变分自编码器（FedDVA）明确分解潜在表示，捕捉共享知识和客户特定个性化，从而导致更可靠和有效的个性化联邦学习，并在广泛实验中验证了该方法的优越性。

    

    个性化联邦学习（PFL）通过在客户端之间平衡的知识共享和模型个性化之间进行训练，从而联合训练各种局部模型。本文通过将潜在表示明确分解为两个部分来解决PFL问题，以捕捉共享知识和客户特定个性化，从而导致更可靠和有效的PFL。该分离是通过一种新颖的联邦双变分自编码器（FedDVA）实现的，它使用两个编码器来推断两种类型的表示。FedDVA可以更好地理解PFL中全局知识共享和本地个性化之间的权衡。此外，它可以与现有的FL方法集成，并将它们转变为用于异构下游任务的个性化模型。广泛的实验证实了分离所带来的优势，并表明经过分离训练的模型明显优于那些普通方法。

    Personalized federated learning (PFL) jointly trains a variety of local models through balancing between knowledge sharing across clients and model personalization per client. This paper addresses PFL via explicit disentangling latent representations into two parts to capture the shared knowledge and client-specific personalization, which leads to more reliable and effective PFL. The disentanglement is achieved by a novel Federated Dual Variational Autoencoder (FedDVA), which employs two encoders to infer the two types of representations. FedDVA can produce a better understanding of the trade-off between global knowledge sharing and local personalization in PFL. Moreover, it can be integrated with existing FL methods and turn them into personalized models for heterogeneous downstream tasks. Extensive experiments validate the advantages caused by disentanglement and show that models trained with disentangled representations substantially outperform those vanilla methods.
    
[^99]: 重新审视广义p-Laplacian正则化框架图卷积网络: 收敛性、能量动态和非线性扩散训练的研究

    Revisiting Generalized p-Laplacian Regularized Framelet GCNs: Convergence, Energy Dynamic and Training with Non-Linear Diffusion. (arXiv:2305.15639v1 [cs.LG])

    [http://arxiv.org/abs/2305.15639](http://arxiv.org/abs/2305.15639)

    本文全面分析了基于图p-Laplacian的Framelet网络，证明了其具有良好的收敛性和能量动态，并能够通过广义非线性扩散过程进行多种训练。同时，该模型能够适应同质和异质数据，避免过度平滑问题。

    

    本文对基于图p-Laplacian的Framelet网络（pL-UFG）进行了全面的理论分析，以建立对其性质的深入理解。我们首先对Framelet卷积后集成p-Laplacian的隐式层进行了收敛性分析，提供了关于pL-UFG渐近行为的洞察力。通过探索pL-UFG的广义Dirichlet能量，我们证明了Dirichlet能量保持非零，确保在pL-UFG接近收敛时避免过度平滑问题。此外，我们通过动态能量视角阐明了pL-UFG中的隐式层与图Framelets协同工作，增强了该模型对同质和异质数据的适应性。值得注意的是，我们证明了这个隐式层可以被解释成广义的非线性扩散过程，使得可以使用多种不同的训练方案。这些多方面的分析导致了统一的结论，提供了新的洞见。

    This work presents a comprehensive theoretical analysis of graph p-Laplacian based framelet network (pL-UFG) to establish a solid understanding of its properties. We begin by conducting a convergence analysis of the p-Laplacian based implicit layer integrated after the framelet convolution, providing insights into the asymptotic behavior of pL-UFG. By exploring the generalized Dirichlet energy of pL-UFG, we demonstrate that the Dirichlet energy remains non-zero, ensuring the avoidance of over-smoothing issues in pL-UFG as it approaches convergence. Furthermore, we elucidate the dynamic energy perspective through which the implicit layer in pL-UFG synergizes with graph framelets, enhancing the model's adaptability to both homophilic and heterophilic data. Remarkably, we establish that the implicit layer can be interpreted as a generalized non-linear diffusion process, enabling training using diverse schemes. These multifaceted analyses lead to unified conclusions that provide novel insi
    
[^100]: Dr. LLaMA：通过生成式数据增强改善特定领域QA中的小语言模型

    Dr. LLaMA: Improving Small Language Models in Domain-Specific QA via Generative Data Augmentation. (arXiv:2305.07804v1 [cs.CL])

    [http://arxiv.org/abs/2305.07804](http://arxiv.org/abs/2305.07804)

    本论文介绍了一种名为Dr. LLaMA的方法，通过使用大型语言模型进行生成式数据增强，以改善小语言模型的性能，特别是在医学问答任务中。这种方法在微调后使模型性能提高，并提出了在特定领域问答任务中使用LLM所面临的挑战和潜在的研究方向。

    

    大型语言模型在自然语言处理方面取得了重大进展，但随着其规模的增长，也面临着计算开销和效率的挑战，特别是在特定领域的任务中。另一方面，小型语言模型由于容量和训练数据的限制，在这些任务中往往表现不佳。本文介绍了一种名为Dr. LLaMA的方法，通过使用大型语言模型进行生成式数据增强，聚焦医学问答任务和PubMedQA数据集，以改善小语言模型的性能。我们的发现表明，LLM有效地细化和扩展现有的问题-答案对，在微调后，使得小型模型在特定领域QA数据集上性能提高。本研究强调了在特定领域问答任务中使用LLM面临的挑战，并提出了潜在的研究方向，最终旨在为专业应用创建更高效和能力更强的模型。

    Large Language Models (LLMs) have made significant strides in natural language processing but face challenges in terms of computational expense and inefficiency as they grow in size, especially in domain-specific tasks. Small Language Models (SLMs), on the other hand, often struggle in these tasks due to limited capacity and training data. In this paper, we introduce Dr. LLaMA, a method for improving SLMs through generative data augmentation using LLMs, focusing on medical question-answering tasks and the PubMedQA dataset. Our findings indicate that LLMs effectively refine and diversify existing question-answer pairs, resulting in improved performance of a much smaller model on domain-specific QA datasets after fine-tuning. This study highlights the challenges of using LLMs for domain-specific question answering and suggests potential research directions to address these limitations, ultimately aiming to create more efficient and capable models for specialized applications. We have als
    
[^101]: 基于并行bootstrap的连续流控制应用的on-policy深度强化学习

    Parallel bootstrap-based on-policy deep reinforcement learning for continuous flow control applications. (arXiv:2304.12330v1 [cs.LG])

    [http://arxiv.org/abs/2304.12330](http://arxiv.org/abs/2304.12330)

    本文提出了一种基于并行bootstrap的on-policy深度强化学习方法，通过部分轨迹缓冲区和返回bootstrapping步骤来实现灵活使用并行环境，同时保持更新的on-policy性，该方法在连续流控制问题上有很好的应用前景。

    

    深度强化学习与数值流控问题的耦合近期引起了相当大的关注，取得了突破性的成果并为该领域开辟了新的前景。然而，由于流体动力学求解器的计算成本通常很高，在学习过程中使用并行环境是实现有效控制的必要手段。尽管如此，大多数基于流控的深度强化学习文献仍依赖于on-policy算法，而这种算法的高并行转移收集可能会破坏理论假设并导致次优的控制模型。为了克服这个问题，我们提出了一个基于部分轨迹缓冲区的并行模式，通过一个返回bootstrapping步骤，允许灵活地使用并行环境，同时保持更新的on-policy性。该方法在文献中一个耗费大量计算的连续流控制问题上进行了说明。

    The coupling of deep reinforcement learning to numerical flow control problems has recently received a considerable attention, leading to groundbreaking results and opening new perspectives for the domain. Due to the usually high computational cost of fluid dynamics solvers, the use of parallel environments during the learning process represents an essential ingredient to attain efficient control in a reasonable time. Yet, most of the deep reinforcement learning literature for flow control relies on on-policy algorithms, for which the massively parallel transition collection may break theoretical assumptions and lead to suboptimal control models. To overcome this issue, we propose a parallelism pattern relying on partial-trajectory buffers terminated by a return bootstrapping step, allowing a flexible use of parallel environments while preserving the on-policiness of the updates. This approach is illustrated on a CPU-intensive continuous flow control problem from the literature.
    
[^102]: 通过利普希茨正则化变分自编码器生成差分隐私合成数据

    Differentially Private Synthetic Data Generation via Lipschitz-Regularised Variational Autoencoders. (arXiv:2304.11336v1 [cs.LG])

    [http://arxiv.org/abs/2304.11336](http://arxiv.org/abs/2304.11336)

    本文提出了通过利普希茨正则化变分自编码器生成差分隐私合成数据的新方法。该方法可以生成符合差分隐私保证的高质量合成数据。

    

    合成数据被誉为隐私保护数据分析的银弹。如果一条记录不是真实的，那么它怎么会侵犯个人的隐私呢？此外，基于深度学习的生成模型被成功地用于近似表示数据的高维复杂分布，并从学习到的分布中抽取逼真的样本。然而，往往忽视生成模型容易记忆个人训练数据集的许多细节，并且生成的合成数据往往太类似于底层敏感训练数据，从而违反了如医疗保健中所遇到的强隐私法规。差分隐私是确保保护敏感个人数据的国际公认的最先进框架，使得可以公开发布集合统计数据和甚至机器学习模型而不会影响隐私。然而，训练机制往往会在训练过程中添加过多的噪声，因此很难生成高质量的合成数据。在本文中，我们提出了一种通过利普希茨正则化变分自编码器生成差分隐私合成数据的新方法。变分自编码器的编码器被正则化以确保对于每个训练数据的编码表示尽可能地与模型参数分享最少的信息，而生成器则被训练以近似底层数据分布。我们的方法生成的高质量合成数据严格符合差分隐私保证。我们通过实验展示了我们的方法的有效性。

    Synthetic data has been hailed as the silver bullet for privacy preserving data analysis. If a record is not real, then how could it violate a person's privacy? In addition, deep-learning based generative models are employed successfully to approximate complex high-dimensional distributions from data and draw realistic samples from this learned distribution. It is often overlooked though that generative models are prone to memorising many details of individual training records and often generate synthetic data that too closely resembles the underlying sensitive training data, hence violating strong privacy regulations as, e.g., encountered in health care. Differential privacy is the well-known state-of-the-art framework for guaranteeing protection of sensitive individuals' data, allowing aggregate statistics and even machine learning models to be released publicly without compromising privacy. The training mechanisms however often add too much noise during the training process, and thu
    
[^103]: 基于消息传递的图神经网络在大规模随机图上的通用聚合收敛性研究

    Convergence of Message Passing Graph Neural Networks with Generic Aggregation On Large Random Graphs. (arXiv:2304.11140v1 [stat.ML])

    [http://arxiv.org/abs/2304.11140](http://arxiv.org/abs/2304.11140)

    本文研究了消息传递图神经网络在随机图模型上的收敛性，将收敛结论从只适用于度规范化平均聚合函数扩展到所有传统聚合函数，并考虑了聚合函数采用逐个坐标最大值时的情况。

    

    本文研究了消息传递图神经网络在随机图模型上的收敛性，当节点数量趋近于无限时，该网络模型能收敛于其连续模型。迄今为止，该收敛性结果只适用于聚合函数采用度规范化平均值形式的网络结构。我们将此结果扩展到包含所有传统消息传递图神经网络的大类聚合函数上，例如基于注意力和最大卷积的网络。在一定假设下，我们给出了高概率的非渐进上限来量化这种收敛性。我们的主要结果基于McDiarmid不等式。有趣的是，我们特别处理了聚合函数采用逐个坐标最大值的情况，因为它需要非常不同的证明技巧，并产生了定性不同的收敛率。

    We study the convergence of message passing graph neural networks on random graph models to their continuous counterpart as the number of nodes tends to infinity. Until now, this convergence was only known for architectures with aggregation functions in the form of degree-normalized means. We extend such results to a very large class of aggregation functions, that encompasses all classically used message passing graph neural networks, such as attention-based mesage passing or max convolutional message passing on top of (degree-normalized) convolutional message passing. Under mild assumptions, we give non asymptotic bounds with high probability to quantify this convergence. Our main result is based on the McDiarmid inequality. Interestingly, we treat the case where the aggregation is a coordinate-wise maximum separately, at it necessitates a very different proof technique and yields a qualitatively different convergence rate.
    
[^104]: 用于不同类型癌症分类的EfficientNet算法

    EfficientNet Algorithm for Classification of Different Types of Cancer. (arXiv:2304.08715v1 [eess.IV])

    [http://arxiv.org/abs/2304.08715](http://arxiv.org/abs/2304.08715)

    本文用EfficientNet算法分类不同类型的癌症，实验结果表明该算法在公共数据集上表现优异，具有在临床实践中提高癌症诊断准确性和效率的潜力。

    

    准确高效地识别不同类型癌症对早期发现和有效治疗至关重要。本文介绍了使用EfficientNet算法分类脑瘤、乳腺癌乳房X线摄影、胸部癌和皮肤癌的实验结果。我们使用公共数据集并对图像进行预处理以确保一致性和可比性。实验表明EfficientNet算法在每个癌症数据集上都取得了高精度、高召回率和高F1分数，优于文献中其他最先进的算法。我们还讨论了EfficientNet算法的优缺点及其在临床实践中的潜在应用。结果表明，EfficientNet算法非常适用于不同类型癌症的分类，并可用于提高癌症诊断的准确性和效率。

    Accurate and efficient classification of different types of cancer is critical for early detection and effective treatment. In this paper, we present the results of our experiments using the EfficientNet algorithm for classification of brain tumor, breast cancer mammography, chest cancer, and skin cancer. We used publicly available datasets and preprocessed the images to ensure consistency and comparability. Our experiments show that the EfficientNet algorithm achieved high accuracy, precision, recall, and F1 scores on each of the cancer datasets, outperforming other state-of-the-art algorithms in the literature. We also discuss the strengths and weaknesses of the EfficientNet algorithm and its potential applications in clinical practice. Our results suggest that the EfficientNet algorithm is well-suited for classification of different types of cancer and can be used to improve the accuracy and efficiency of cancer diagnosis.
    
[^105]: 从时间-顶点谱学习图ARMA过程

    Learning Graph ARMA Processes from Time-Vertex Spectra. (arXiv:2302.06887v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.06887](http://arxiv.org/abs/2302.06887)

    本研究提出了一种基于学习过程谱密度的算法，用于推断缺失的信号值和进行信号插值，实验结果显示其在时间-顶点信号估计问题中具有高准确性。

    

    将时间变化的图信号建模为稳态时间-顶点随机过程，可以通过有效地利用过程在不同图节点和时间瞬间之间的相关性模式来推断缺失的信号值。在这项研究中，我们提出了一种算法，用于基于学习过程的不完整实现的联合时间-顶点功率谱密度来计算图自回归移动平均（图ARMA）过程，以用于信号插值任务。我们的解决方案首先通过部分观察到的实现粗略估计过程的联合谱，然后通过凸松弛将其投影到图ARMA过程的谱流形上来改进这个估计。然后，基于学习的模型估计最初缺失的信号值。实验结果表明，所提出的方法在时间-顶点信号估计问题中达到了很高的准确度。

    The modeling of time-varying graph signals as stationary time-vertex stochastic processes permits the inference of missing signal values by efficiently employing the correlation patterns of the process across different graph nodes and time instants. In this study, we propose an algorithm for computing graph autoregressive moving average (graph ARMA) processes based on learning the joint time-vertex power spectral density of the process from its incomplete realizations for the task of signal interpolation. Our solution relies on first roughly estimating the joint spectrum of the process from partially observed realizations and then refining this estimate by projecting it onto the spectrum manifold of the graph ARMA process through convex relaxations. The initially missing signal values are then estimated based on the learnt model. Experimental results show that the proposed approach achieves high accuracy in time-vertex signal estimation problems.
    
[^106]: 通过最小批量优化传输改进和泛化基于流的生成模型

    Improving and generalizing flow-based generative models with minibatch optimal transport. (arXiv:2302.00482v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00482](http://arxiv.org/abs/2302.00482)

    这篇论文提出了一种称为广义条件流匹配（CFM）的技术，在连续正则化流（CNFs）的生成模型中无需模拟训练，极大提高了效率和稳定性。此外，论文还引入了最优传输CFM（OT-CFM）的变体，可以以无模拟方式计算动态OT，加速了推断过程。

    

    连续正则化流（CNFs）是一种吸引人的生成建模技术，但由于其基于模拟的最大似然训练存在局限性而受到约束。我们介绍了广义条件流匹配（CFM）技术，这是一种针对CNFs的无模拟训练目标的集合。CFM具有类似于扩散模型中用于训练随机流的稳定回归目标，但同时享有确定性流模型的高效推断。与扩散模型和之前的CNF训练算法相比，CFM不需要源分布为高斯分布，也不需要对其密度进行评估。我们的目标的一种变体是最优传输CFM（OT-CFM），它创建了更简单的流，更容易训练，并且导致更快的推断，如我们的实验证明所示。此外，OT-CFM是第一种以无模拟方式计算动态OT的方法。使用CFM训练CNFs可以改进各种条件和...

    Continuous normalizing flows (CNFs) are an attractive generative modeling technique, but they have been held back by limitations in their simulation-based maximum likelihood training. We introduce the generalized conditional flow matching (CFM) technique, a family of simulation-free training objectives for CNFs. CFM features a stable regression objective like that used to train the stochastic flow in diffusion models but enjoys the efficient inference of deterministic flow models. In contrast to both diffusion models and prior CNF training algorithms, CFM does not require the source distribution to be Gaussian or require evaluation of its density. A variant of our objective is optimal transport CFM (OT-CFM), which creates simpler flows that are more stable to train and lead to faster inference, as evaluated in our experiments. Furthermore, OT-CFM is the first method to compute dynamic OT in a simulation-free way. Training CNFs with CFM improves results on a variety of conditional and u
    
[^107]: 鲁棒的在线主动学习策略

    Robust online active learning. (arXiv:2302.00422v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.00422](http://arxiv.org/abs/2302.00422)

    本文提出了一种自适应方法，用于鲁棒的在线主动学习，并在受污染的数据流中证明了其性能表现优异，同时确保了稳定性并减少异常值的负面影响。

    

    在许多工业应用中，获得标记的观测数据并不简单，通常需要人工专家干预或使用昂贵的测试设备。在这种情况下，主动学习可以大大提高拟合模型时最信息数据点的建议。减少模型开发所需的观测数据数量可以减轻训练所需的计算负担和标记相关的操作支出。特别是在线主动学习，在需要在极短时间内决定是否获取数据点标记的高容量生产过程中非常有用。然而，尽管最近致力于开发在线主动学习策略，但在存在异常值的情况下这些方法的行为仍未得到彻底研究。在这项工作中，我们调查了在线主动线性回归在受污染的数据流中的性能，并提出了一种自适应方法，用于鲁棒的在线主动学习，同时保证稳定性并减少异常值的负面影响。

    In many industrial applications, obtaining labeled observations is not straightforward as it often requires the intervention of human experts or the use of expensive testing equipment. In these circumstances, active learning can be highly beneficial in suggesting the most informative data points to be used when fitting a model. Reducing the number of observations needed for model development alleviates both the computational burden required for training and the operational expenses related to labeling. Online active learning, in particular, is useful in high-volume production processes where the decision about the acquisition of the label for a data point needs to be taken within an extremely short time frame. However, despite the recent efforts to develop online active learning strategies, the behavior of these methods in the presence of outliers has not been thoroughly examined. In this work, we investigate the performance of online active linear regression in contaminated data strea
    
[^108]: SAN: 利用判别式归一化线性层诱导GAN的可测性

    SAN: Inducing Metrizability of GAN with Discriminative Normalized Linear Layer. (arXiv:2301.12811v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12811](http://arxiv.org/abs/2301.12811)

    本文提出了一种新的GAN训练方案，切片对抗网络（SAN），通过优化生成器和判别器的最小最大目标函数，使生成器分布接近目标分布。实验证实了SAN相对于普通GAN的有效性，并在StyleGAN-XL上取得了最先进的FID评分。

    

    生成对抗网络（GAN）通过优化生成器和判别器的最小最大目标函数来学习目标概率分布。本文解决了这样一个问题：优化是否真正提供了使生成器分布接近目标分布的梯度。我们通过将GAN的形式与切片最优输运的概念结合起来，推导了可测性条件，即判别器作为分布之间的距离的充分条件。此外，通过利用这些理论结果，我们提出了一种新的GAN训练方案，称为切片对抗网络（SAN）。通过简单的修改，可以将广泛类别的现有GAN转化为SAN。在合成和图像数据集上的实验证实了我们的理论结果和SAN相对于普通GAN的有效性。此外，我们还将SAN应用于StyleGAN-XL，在分类上取得了GAN中最先进的FID（Frechet Inception Distance）评分。

    Generative adversarial networks (GANs) learn a target probability distribution by optimizing a generator and a discriminator with minimax objectives. This paper addresses the question of whether such optimization actually provides the generator with gradients that make its distribution close to the target distribution. We derive metrizable conditions, sufficient conditions for the discriminator to serve as the distance between the distributions by connecting the GAN formulation with the concept of sliced optimal transport. Furthermore, by leveraging these theoretical results, we propose a novel GAN training scheme, called slicing adversarial network (SAN). With only simple modifications, a broad class of existing GANs can be converted to SANs. Experiments on synthetic and image datasets support our theoretical results and the SAN's effectiveness as compared to usual GANs. Furthermore, we also apply SAN to StyleGAN-XL, which leads to state-of-the-art FID score amongst GANs for class con
    
[^109]: 比较贝叶斯层次模型的深度学习方法

    A Deep Learning Method for Comparing Bayesian Hierarchical Models. (arXiv:2301.11873v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.11873](http://arxiv.org/abs/2301.11873)

    这个论文提出了一种深度学习方法，用于比较贝叶斯层次模型。该方法通过支持分摊推断，能够高效地进行模型比较和性能验证。同时，作者还对四个层次证据积累模型进行了比较。

    

    贝叶斯模型比较（BMC）提供了一种基于原则的方法来评估竞争计算模型的相对优势，并将不确定性传播到模型选择决策中。然而，由于高维嵌套参数结构，BMC在常见的层次模型中常常难以计算。为了解决这个难题，我们提出了一种深度学习方法，用于对任何可实例化为概率程序的层次模型集进行BMC。由于我们的方法支持分摊推断，它可以在任何实际数据应用之前，对后验模型概率进行高效的重新估计和快速性能验证。在一系列广泛的验证研究中，我们对比了我们的方法与最先进的桥式抽样方法的性能，并展示了在所有BMC设置中出色的分摊推断能力。然后，我们展示了我们的方法，通过比较先前被认为是四个层次证据积累模型。

    Bayesian model comparison (BMC) offers a principled approach for assessing the relative merits of competing computational models and propagating uncertainty into model selection decisions. However, BMC is often intractable for the popular class of hierarchical models due to their high-dimensional nested parameter structure. To address this intractability, we propose a deep learning method for performing BMC on any set of hierarchical models which can be instantiated as probabilistic programs. Since our method enables amortized inference, it allows efficient re-estimation of posterior model probabilities and fast performance validation prior to any real-data application. In a series of extensive validation studies, we benchmark the performance of our method against the state-of-the-art bridge sampling method and demonstrate excellent amortized inference across all BMC settings. We then showcase our method by comparing four hierarchical evidence accumulation models that have previously b
    
[^110]: EPiC-GAN: 等变点云生成用于粒子喷注

    EPiC-GAN: Equivariant Point Cloud Generation for Particle Jets. (arXiv:2301.08128v3 [hep-ph] UPDATED)

    [http://arxiv.org/abs/2301.08128](http://arxiv.org/abs/2301.08128)

    本文介绍了一种名为EPiC-GAN的等变点云生成对抗网络，可以灵活地生成不同多重性的点云，该网络基于深度集合，并能够快速模拟粒子喷流。与其他方法相比，EPiC-GAN具有更高的计算效率和良好的扩展性。

    

    随着当前和未来高能对撞机实验的巨大数据采集能力，对于计算效率高的模拟的需求越来越大。生成式机器学习模型能够实现快速事件生成，但迄今为止这些方法在固定数据结构和严格的探测器几何约束下有限。本文介绍了EPiC-GAN - 等变点云生成对抗网络，可以产生可变多重性的点云。这个灵活的框架基于深度集合，并适用于模拟称为喷注的粒子喷流。生成器和鉴别器利用多个EPiC层和一个可解释的全局潜在向量。至关重要的是，EPiC层不依赖于粒子之间的成对信息共享，这导致与基于图和变换器的方法以及更复杂的关系图表相比的显著加速。我们证明了EPiC-GAN在大粒子多重性的情况下具有良好的扩展性。

    With the vast data-collecting capabilities of current and future high-energy collider experiments, there is an increasing demand for computationally efficient simulations. Generative machine learning models enable fast event generation, yet so far these approaches are largely constrained to fixed data structures and rigid detector geometries. In this paper, we introduce EPiC-GAN - equivariant point cloud generative adversarial network - which can produce point clouds of variable multiplicity. This flexible framework is based on deep sets and is well suited for simulating sprays of particles called jets. The generator and discriminator utilize multiple EPiC layers with an interpretable global latent vector. Crucially, the EPiC layers do not rely on pairwise information sharing between particles, which leads to a significant speed-up over graph- and transformer-based approaches with more complex relation diagrams. We demonstrate that EPiC-GAN scales well to large particle multiplicities 
    
[^111]: 关于强化学习中Transformers的调查

    A Survey on Transformers in Reinforcement Learning. (arXiv:2301.03044v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.03044](http://arxiv.org/abs/2301.03044)

    这篇论文是一项调查研究，总结了在强化学习领域使用Transformers的动机、进展和未来前景。

    

    Transformer已被认为是自然语言处理（NLP）和计算机视觉（CV）领域中的主导神经架构，主要应用于监督学习任务。最近，在强化学习（RL）领域中也出现了类似的使用Transformers的潮流，但面临着RL的特殊设计选择和挑战。然而，Transformers在RL中的发展尚未被充分揭示。在本文中，我们系统地回顾了在RL中使用Transformers的动机和进展，提供了一个现有工作的分类体系，讨论了每个子领域，并总结了未来的前景。

    Transformer has been considered the dominating neural architecture in NLP and CV, mostly under supervised settings. Recently, a similar surge of using Transformers has appeared in the domain of reinforcement learning (RL), but it is faced with unique design choices and challenges brought by the nature of RL. However, the evolution of Transformers in RL has not yet been well unraveled. In this paper, we seek to systematically review motivations and progress on using Transformers in RL, provide a taxonomy on existing works, discuss each sub-field, and summarize future prospects.
    
[^112]: 世界模型在连续强化学习中的有效性

    The Effectiveness of World Models for Continual Reinforcement Learning. (arXiv:2211.15944v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15944](http://arxiv.org/abs/2211.15944)

    本论文展示了世界模型在连续学习中的应用，通过研究选择性经验回放方法的影响，提出了Continual-Dreamer模型，该模型在Minigrid和Minihack基准测试中表现优于最先进的任务不可知连续强化学习方法。

    

    世界模型是一些高效强化学习算法的基础。在这项工作中，我们展示了它们可以用于连续学习，即智能体面对不断变化的环境情况。世界模型通常使用回放缓冲区进行训练，这可以自然地扩展到连续学习中。我们系统地研究了不同的选择性经验回放方法对性能、遗忘和迁移的影响。我们还提供了关于使用世界模型的各种建模选项的建议。最佳选择是称为Continual-Dreamer的模型，它是任务不可知的，并利用世界模型进行连续探索。Continual-Dreamer具有高样本效率，并在Minigrid和Minihack基准测试中胜过最先进的任务不可知连续强化学习方法。

    World models power some of the most efficient reinforcement learning algorithms. In this work, we showcase that they can be harnessed for continual learning - a situation when the agent faces changing environments. World models typically employ a replay buffer for training, which can be naturally extended to continual learning. We systematically study how different selective experience replay methods affect performance, forgetting, and transfer. We also provide recommendations regarding various modeling options for using world models. The best set of choices is called Continual-Dreamer, it is task-agnostic and utilizes the world model for continual exploration. Continual-Dreamer is sample efficient and outperforms state-of-the-art task-agnostic continual reinforcement learning methods on Minigrid and Minihack benchmarks.
    
[^113]: 控制变压器：通过PRM-Guided Return-Conditioned序列建模在未知环境中的机器人导航

    Control Transformer: Robot Navigation in Unknown Environments through PRM-Guided Return-Conditioned Sequence Modeling. (arXiv:2211.06407v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2211.06407](http://arxiv.org/abs/2211.06407)

    Control Transformer是一种通过采样规划引导的低层策略建模返回条件序列的方法，可以在未知环境中成功解决长时间范围的导航任务。

    

    学习长时间范围的任务，如导航，对于成功应用强化学习到机器人领域提出了困难的挑战。从另一个角度来看，在已知环境下，基于采样的规划可以在不学习的情况下稳健地找到无碰撞路径。在这项工作中，我们提出了Control Transformer，通过由基于采样的概率地图（PRM）规划器引导的低层策略建模返回条件序列。我们证明了我们的框架可以使用仅局部信息解决长时间范围的导航任务。我们在部分观察的迷宫导航中评估了我们的方法，包括Ant，Point和Humanoid的MuJoCo机器人。我们展示了Control Transformer可以成功地在迷宫中导航并转移到未知环境中。此外，我们将我们的方法应用于差分驱动机器人（Turtlebot3），并展示了在噪声观测下的零样本sim2real转移。

    Learning long-horizon tasks such as navigation has presented difficult challenges for successfully applying reinforcement learning to robotics. From another perspective, under known environments, sampling-based planning can robustly find collision-free paths in environments without learning. In this work, we propose Control Transformer that models return-conditioned sequences from low-level policies guided by a sampling-based Probabilistic Roadmap (PRM) planner. We demonstrate that our framework can solve long-horizon navigation tasks using only local information. We evaluate our approach on partially-observed maze navigation with MuJoCo robots, including Ant, Point, and Humanoid. We show that Control Transformer can successfully navigate through mazes and transfer to unknown environments. Additionally, we apply our method to a differential drive robot (Turtlebot3) and show zero-shot sim2real transfer under noisy observations.
    
[^114]: 人类生物物理学作为网络权重：用于动态模拟的条件生成模型。

    Human Biophysics as Network Weights: Conditional Generative Models for Dynamic Simulation. (arXiv:2211.01856v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01856](http://arxiv.org/abs/2211.01856)

    提出了一种使用条件生成模型插值的架构，可以在动态模拟中降低建模时间并保持高的生成精度。

    

    生物物理系统的模拟对于研究生理机制和开发人机界面至关重要。虽然先进的数值方法，如有限元模型，在这个任务中表现出色，但是当生成大量模拟或模拟具有连续变化结构参数的动态事件时，它们的计算成本非常高。我们提出了一种使用条件生成模型在数值模型状态之间进行插值的架构，大大降低了建模时间，同时保持高的生成精度。作为这一概念的示范，我们提出了BioMime，一种混合结构生成模型，可以在动态变化期间对特定生物物理系统进行准确、超快速和任意高时间分辨率的模拟。这种方法在生理和临床研究以及支持信号分析的数据增强策略方面具有广泛的应用。

    Simulations of biophysical systems are fundamental for studying physiological mechanisms and developing human machine interfaces. Whilst advanced numerical methods, such as finite element models, can excel in this task, they are extremely computationally expensive to use when generating a large number of simulations or simulating dynamic events with continuously changing structural parameters. We propose an architecture that uses a conditional generative model to interpolate between the numerical model states, dramatically lowering the modeling time while maintaining a high generation accuracy. As a demonstration of this concept, we present BioMime, a hybrid-structured generative model that enables an accurate, ultra-fast, and arbitrarily high temporal-resolution simulation of a specific biophysical system during dynamic changes. This methodology has wide applications in physiological and clinical research as well as in supporting data augmentation strategies for signal analysis, repre
    
[^115]: 对抗性策略战胜超级人类级围棋AI

    Adversarial Policies Beat Superhuman Go AIs. (arXiv:2211.00241v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.00241](http://arxiv.org/abs/2211.00241)

    通过对抗性策略攻击，我们成功战胜了超级人类级围棋AI KataGo，揭示了其核心弱点，并展示了即使是超级AI系统也可能存在意想不到的失败模式。

    

    我们通过训练对抗性策略来攻击最先进的围棋AI系统KataGo，在超人类设置下取得了超过97%的胜率。我们的对手并不是通过出色地下围棋来获胜，而是通过诱使KataGo犯下严重失误。我们的攻击可以零损耗地传输给其他超级人类级围棋AI，并且对人类专家来说是可以理解的，他们可以在没有算法辅助的情况下实施这种攻击来持续战胜超级人类级AI。我们的攻击揭示了KataGo的核心弱点，即使是对抗性训练的KataGo代理也无法防御我们的攻击。我们的研究结果表明，即使是超级人类级的AI系统也可能存在意想不到的失败模式。

    We attack the state-of-the-art Go-playing AI system KataGo by training adversarial policies against it, achieving a >97% win rate against KataGo running at superhuman settings. Our adversaries do not win by playing Go well. Instead, they trick KataGo into making serious blunders. Our attack transfers zero-shot to other superhuman Go-playing AIs, and is comprehensible to the extent that human experts can implement it without algorithmic assistance to consistently beat superhuman AIs. The core vulnerability uncovered by our attack persists even in KataGo agents adversarially trained to defend against our attack. Our results demonstrate that even superhuman AI systems may harbor surprising failure modes. Example games are available https://goattack.far.ai/.
    
[^116]: 广义的Laplacian正则化框架图神经网络

    Generalized Laplacian Regularized Framelet Graph Neural Networks. (arXiv:2210.15092v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.15092](http://arxiv.org/abs/2210.15092)

    本文提出了一种广义的Laplacian正则化框架图神经网络，该网络利用了p-Laplacian的性质和图信号的多分辨率分解的表达能力。实验结果表明，该网络在节点分类和信号去噪等图学习任务中具有出色的性能。

    

    本文引入了一种新的基于p-Laplacian GNN的Framelet图方法。提出的两个模型，分别命名为p-Laplacian不下采样framelet图卷积（pL-UFG）和广义p-Laplacian不下采样framelet图卷积（pL-fUFG），继承了p-Laplacian的性质，并结合了图信号的多分辨率分解的表达能力。实证研究突出了pL-UFG和pL-fUFG在不同的图学习任务中的优异表现，包括节点分类和信号去噪。

    This paper introduces a novel Framelet Graph approach based on p-Laplacian GNN. The proposed two models, named p-Laplacian undecimated framelet graph convolution (pL-UFG) and generalized p-Laplacian undecimated framelet graph convolution (pL-fUFG) inherit the nature of p-Laplacian with the expressive power of multi-resolution decomposition of graph signals. The empirical study highlights the excellent performance of the pL-UFG and pL-fUFG in different graph learning tasks including node classification and signal denoising.
    
[^117]: RulE: 使用规则嵌入的神经-符号知识图推理

    RulE: Neural-Symbolic Knowledge Graph Reasoning with Rule Embedding. (arXiv:2210.14905v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2210.14905](http://arxiv.org/abs/2210.14905)

    RulE是一个框架，通过将实体、关系和逻辑规则统一表示在一个嵌入空间中，有效利用逻辑规则提升知识图推理。同时，RulE注入先前的逻辑规则信息，改进了实体/关系嵌入，使得知识图嵌入方法也表现更好。

    

    知识图（KG）推理对于知识图是一个重要问题。本文提出了一个新颖而有原则定位的框架，称为RulE（代表规则嵌入），以有效利用逻辑规则来增强KG推理。与知识图嵌入（KGE）方法不同，RulE通过在统一的嵌入空间中联合表示实体、关系和逻辑规则，从现有三元组和一阶规则中学习规则嵌入。基于学习到的规则嵌入，可以计算每个规则的置信度得分，反映其与观察到的三元组的一致性。这使得我们能够以软方式进行逻辑规则推理，从而减轻了逻辑的脆弱性。另一方面，RulE将先前的逻辑规则信息注入到嵌入空间中，丰富和规范化实体/关系嵌入。这也使得仅使用KGE的表现更好。RulE在概念上简单且在实验上有效。

    Knowledge graph (KG) reasoning is an important problem for knowledge graphs. In this paper, we propose a novel and principled framework called \textbf{RulE} (stands for {Rul}e {E}mbedding) to effectively leverage logical rules to enhance KG reasoning. Unlike knowledge graph embedding (KGE) methods, RulE learns rule embeddings from existing triplets and first-order {rules} by jointly representing \textbf{entities}, \textbf{relations} and \textbf{logical rules} in a unified embedding space. Based on the learned rule embeddings, a confidence score can be calculated for each rule, reflecting its consistency with the observed triplets. This allows us to perform logical rule inference in a soft way, thus alleviating the brittleness of logic. On the other hand, RulE injects prior logical rule information into the embedding space, enriching and regularizing the entity/relation embeddings. This makes KGE alone perform better too. RulE is conceptually simple and empirically effective. We conduct
    
[^118]: 基于核斯坦检验的顺序模型适配性检验

    A kernel Stein test of goodness of fit for sequential models. (arXiv:2210.10741v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.10741](http://arxiv.org/abs/2210.10741)

    我们提出了一种基于核斯坦检验的适配性度量方法，可以适用于具有不同维度观测的概率密度模型，包括文本文档或可变长度序列。这种方法扩展了核斯坦差异(KSD)到可变维度设置，并提出了一种新颖的KSD适配性检验方法，无需密度归一化，并在离散顺序数据基准上表现良好。

    

    我们提出了一种适用于具有不同维度观测的概率密度模型的适配性度量方法，例如具有不同长度或可变长度序列的文本文档。我们提出的度量方法是核斯坦差异(Kernel Stein Discrepancy, KSD)的一个实例，KSD已被用于构建非标准化密度的适配性检验。KSD通过斯坦算子来定义，目前用于检验的斯坦算子适用于固定维度空间。作为我们的主要贡献，我们通过识别合适的斯坦算子，在可变维度设置下扩展了KSD，并提出了一种新颖的KSD适配性检验方法。与之前的变体一样，我们提出的KSD不要求密度归一化，可以评估大量的模型。我们的测试在离散顺序数据基准上表现良好。

    We propose a goodness-of-fit measure for probability densities modeling observations with varying dimensionality, such as text documents of differing lengths or variable-length sequences. The proposed measure is an instance of the kernel Stein discrepancy (KSD), which has been used to construct goodness-of-fit tests for unnormalized densities. The KSD is defined by its Stein operator: current operators used in testing apply to fixed-dimensional spaces. As our main contribution, we extend the KSD to the variable-dimension setting by identifying appropriate Stein operators, and propose a novel KSD goodness-of-fit test. As with the previous variants, the proposed KSD does not require the density to be normalized, allowing the evaluation of a large class of models. Our test is shown to perform well in practice on discrete sequential data benchmarks.
    
[^119]: 重新审视离散型Soft Actor-Critic方法

    Revisiting Discrete Soft Actor-Critic. (arXiv:2209.10081v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.10081](http://arxiv.org/abs/2209.10081)

    本研究重新审视了将连续动作空间的Soft Actor-Critic方法调整为离散动作空间的问题，并提出了解决Q值低估和性能不稳定的方法，验证了其在Atari游戏和大规模MOBA游戏中的有效性。

    

    本文研究将连续动作空间的Soft Actor-Critic方法（SAC）调整为离散动作空间。我们重新审视了经典的SAC方法，并深入理解了在离散设置下其Q值低估和性能不稳定的问题。因此，我们提出了熵惩罚和具有Q-clip的双平均Q-learning方法来解决这些问题。通过对包括Atari游戏和一个大规模MOBA游戏在内的典型基准问题进行广泛实验，验证了我们方法的有效性。我们的代码可在以下链接找到: https://github.com/coldsummerday/Revisiting-Discrete-SAC.

    We study the adaption of soft actor-critic (SAC) from continuous action space to discrete action space. We revisit vanilla SAC and provide an in-depth understanding of its Q value underestimation and performance instability issues when applied to discrete settings. We thereby propose entropy-penalty and double average Q-learning with Q-clip to address these issues. Extensive experiments on typical benchmarks with discrete action space, including Atari games and a large-scale MOBA game, show the efficacy of our proposed method. Our code is at:https://github.com/coldsummerday/Revisiting-Discrete-SAC.
    
[^120]: 在具有基于注意力的双向LSTM网络的免授权NOMA中进行联合用户和数据检测

    Joint User and Data Detection in Grant-Free NOMA with Attention-based BiLSTM Network. (arXiv:2209.06392v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2209.06392](http://arxiv.org/abs/2209.06392)

    本文提出了一种基于注意力的双向LSTM网络用于解决免授权NOMA中的多用户检测问题，通过利用时间相关性和注意力机制，实现了对活动设备的准确识别和数据解码。

    

    我们考虑在上行免授权非正交多址接入（NOMA）中的多用户检测（MUD）问题，其中接入点需要识别活动物联网（IoT）设备的总数和正确的身份，并解码它们传输的数据。我们假设IoT设备使用复杂的扩频序列，并按照突发稀疏模型以随机接入方式传输信息，其中一些IoT设备在多个相邻时间槽中以高概率传输数据，而其他设备在一个帧内只传输一次。利用时间相关性，我们提出了一种基于注意力的双向长短期记忆（BiLSTM）网络来解决MUD问题。BiLSTM网络使用前向和反向LSTM创建设备激活历史的模式，而注意力机制为设备激活点提供重要的上下文。通过这样做，可以按照层次路径检测活动设备。

    We consider the multi-user detection (MUD) problem in uplink grant-free non-orthogonal multiple access (NOMA), where the access point has to identify the total number and correct identity of the active Internet of Things (IoT) devices and decode their transmitted data. We assume that IoT devices use complex spreading sequences and transmit information in a random-access manner following the burst-sparsity model, where some IoT devices transmit their data in multiple adjacent time slots with a high probability, while others transmit only once during a frame. Exploiting the temporal correlation, we propose an attention-based bidirectional long short-term memory (BiLSTM) network to solve the MUD problem. The BiLSTM network creates a pattern of the device activation history using forward and reverse pass LSTMs, whereas the attention mechanism provides essential context to the device activation points. By doing so, a hierarchical pathway is followed for detecting active devices in a grant-f
    
[^121]: 图的本地内在维度度量及其在图嵌入中的应用

    Local Intrinsic Dimensionality Measures for Graphs, with Applications to Graph Embeddings. (arXiv:2208.11986v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.11986](http://arxiv.org/abs/2208.11986)

    本文介绍了一种新的与LID相关的度量方法NC-LID，用于量化图结构化数据的本地特性，同时利用NC-LID设计了LID感知的图嵌入算法。

    

    本地内在维度度量（LID）的概念是数据维度分析的一个重要进展，具有数据挖掘、机器学习和相似性搜索等领域的应用。现有的基于距离的LID估计器针对以欧几里德空间中向量表示的表格数据集进行了设计。本文讨论了这些估计器在考虑图嵌入和图距离的图结构化数据上的局限性，并提出了一种新的与LID相关的度量方法NC-LID，用于量化最短路径距离与节点自然社区之间的本地特性的区分能力。本文还展示了如何通过制定两种基于LID的节点2vec变种算法来设计LID感知的图嵌入算法，其中的超参数根据NC-LID的值进行调整。通过在大量真实世界的图上进行实证分析，我们发现NC-LID能够指示具有高连接性的节点。

    The notion of local intrinsic dimensionality (LID) is an important advancement in data dimensionality analysis, with applications in data mining, machine learning and similarity search problems. Existing distance-based LID estimators were designed for tabular datasets encompassing data points represented as vectors in a Euclidean space. After discussing their limitations for graph-structured data considering graph embeddings and graph distances, we propose NC-LID, a novel LID-related measure for quantifying the discriminatory power of the shortest-path distance with respect to natural communities of nodes as their intrinsic localities. It is shown how this measure can be used to design LID-aware graph embedding algorithms by formulating two LID-elastic variants of node2vec with personalized hyperparameters that are adjusted according to NC-LID values. Our empirical analysis of NC-LID on a large number of real-world graphs shows that this measure is able to point to nodes with high link
    
[^122]: 数据增强是一个超参数：精心筛选的自监督对于无监督异常检测的成功产生了幻象。

    Data Augmentation is a Hyperparameter: Cherry-picked Self-Supervision for Unsupervised Anomaly Detection is Creating the Illusion of Success. (arXiv:2208.07734v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.07734](http://arxiv.org/abs/2208.07734)

    这项研究通过广泛的实验，证明数据增强与异常生成机制之间的对齐是自监督学习在无监督异常检测中取得成功的关键，并且在缺乏对齐时，自监督学习甚至可能降低准确性。

    

    自监督学习（SSL）已经成为一种有希望的替代方法，用于为现实世界的问题创建监督信号，避免了手动标注的巨大成本。对于标记异常稀缺或几乎不存在的无监督任务（如异常检测），SSL特别有吸引力。过去已经使用了大量的数据增强函数来进行基于SSL的异常检测（SSAD）的图像数据，并且最近的研究表明数据增强的类型对准确性有着重要影响。受此启发，本研究通过对三种不同检测模型和420个异常检测任务的广泛实验，提供了全面的数字和可视证据，证明数据增强与异常生成机制之间的对齐是SSAD成功的关键，而在缺乏对齐的情况下，SSL甚至可能降低准确性。据我们所知，这是关于图像型SSAD的首次深入研究。

    Self-supervised learning (SSL) has emerged as a promising alternative to create supervisory signals to real-world problems, avoiding the extensive cost of manual labeling. SSL is particularly attractive for unsupervised tasks such as anomaly detection (AD), where labeled anomalies are rare or often nonexistent. A large catalog of augmentation functions has been used for SSL-based AD (SSAD) on image data, and recent works have reported that the type of augmentation has a significant impact on accuracy. Motivated by those, this work sets out to put image-based SSAD under a larger lens and investigate the role of data augmentation in SSAD. Through extensive experiments on 3 different detector models and across 420 AD tasks, we provide comprehensive numerical and visual evidences that the alignment between data augmentation and anomaly-generating mechanism is the key to the success of SSAD, and in the lack thereof, SSL may even impair accuracy. To the best of our knowledge, this is the fir
    
[^123]: TRUST-LAPSE：一种可解释和可操作的模型监控不信任评分框架

    TRUST-LAPSE: An Explainable and Actionable Mistrust Scoring Framework for Model Monitoring. (arXiv:2207.11290v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.11290](http://arxiv.org/abs/2207.11290)

    TRUST-LAPSE是一个可解释和可操作的连续模型监控框架，通过使用潜空间嵌入评估每个输入样本的模型预测的可信度，并利用距离和相似度度量以及顺序相关性偏差来实现对模型的连续监控。

    

    连续监测训练好的机器学习模型，在确定何时应该信任它们的预测和何时不应该信任它们的预测方面非常重要，这对于安全部署是必需的。我们提出了TRUST-LAPSE，这是一个用于连续模型监控的“不信任”评分框架。我们使用一系列潜空间嵌入来评估每个输入样本的模型预测的可信度。具体来说，（a）我们的潜空间不信任评分使用潜空间中的距离度量（马氏距离）和相似度度量（余弦相似度）来估计不信任度，（b）我们的顺序不信任评分使用非参数、滑动窗口算法来确定过去输入表示序列中的相关性偏差，从而实现可操作的连续监控。我们通过两个下游任务对TRUST-LAPSE进行评估：（1）分布偏移输入检测，（2）数据漂移检测。我们在不同领域进行评估-音频...

    Continuous monitoring of trained ML models to determine when their predictions should and should not be trusted is essential for their safe deployment. Such a framework ought to be high-performing, explainable, post-hoc and actionable. We propose TRUST-LAPSE, a "mistrust" scoring framework for continuous model monitoring. We assess the trustworthiness of each input sample's model prediction using a sequence of latent-space embeddings. Specifically, (a) our latent-space mistrust score estimates mistrust using distance metrics (Mahalanobis distance) and similarity metrics (cosine similarity) in the latent-space and (b) our sequential mistrust score determines deviations in correlations over the sequence of past input representations in a non-parametric, sliding-window based algorithm for actionable continuous monitoring. We evaluate TRUST-LAPSE via two downstream tasks: (1) distributionally shifted input detection, and (2) data drift detection. We evaluate across diverse domains - audio 
    
[^124]: 基于联想记忆模型的真实世界数据分类和生成

    Classification and Generation of real-world data with an Associative Memory Model. (arXiv:2207.04827v3 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2207.04827](http://arxiv.org/abs/2207.04827)

    本文提出了一种基于联想记忆模型的多模态框架，可以以容错的方式存储和检索大量真实世界数据，并且可以用于推断缺失的模态。

    This paper proposes a multi-modality framework based on the associative memory model, which can store and retrieve a large amount of real-world data in a fault-tolerant manner, and can be used to infer missing modalities.

    回忆起多年未见的朋友的面孔是一项困难的任务。然而，如果你们偶然相遇，你们会轻易地认出彼此。生物记忆配备了一个令人印象深刻的压缩算法，可以存储必要的信息，然后推断细节以匹配感知。Willshaw Memory是一种用于皮层计算的简单抽象模型，实现了生物记忆的机制。使用我们最近提出的用于视觉模式的稀疏编码规则[34]，该模型可以以容错的方式存储和检索大量真实世界数据。在本文中，我们通过使用多模态框架扩展了基本联想记忆模型的能力。在这种设置中，记忆同时存储每个模式的几种模态（例如，视觉或文本）。训练后，当只感知到子集时，记忆可以用于推断缺失的模态。使用简单的编码器-记忆解码器，我们可以生成具有多个模态的数据。

    Drawing from memory the face of a friend you have not seen in years is a difficult task. However, if you happen to cross paths, you would easily recognize each other. The biological memory is equipped with an impressive compression algorithm that can store the essential, and then infer the details to match perception. The Willshaw Memory is a simple abstract model for cortical computations which implements mechanisms of biological memories. Using our recently proposed sparse coding prescription for visual patterns [34], this model can store and retrieve an impressive amount of real-world data in a fault-tolerant manner. In this paper, we extend the capabilities of the basic Associative Memory Model by using a Multiple-Modality framework. In this setting, the memory stores several modalities (e.g., visual, or textual) of each pattern simultaneously. After training, the memory can be used to infer missing modalities when just a subset is perceived. Using a simple encoder-memory decoder a
    
[^125]: 多重检验框架用于离群分布检测

    Multiple Testing Framework for Out-of-Distribution Detection. (arXiv:2206.09522v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.09522](http://arxiv.org/abs/2206.09522)

    本研究提出了一个多重检验框架用于离群分布检测的问题，包括了定义OOD概念和提供强有力保证的方法，与之前的基于阈值的测试相比，在不同类型的OOD实例中表现更一致。

    

    我们研究离群分布（OOD）检测的问题，即在推理时检测学习算法的输出是否可信。尽管之前的工作中提出了一些OOD检测的测试方法，但缺乏一个形式化的框架来研究这个问题。我们提出了一个OOD概念的定义，包括输入分布和学习算法，这为构建强大的OOD检测测试提供了启示。我们提出了一种多重假设检验启发的过程，使用符合性p值系统地结合学习算法中的任意数量的不同统计量。我们进一步对将入群样本错误分类为OOD的概率提供了强有力的保证。在实验中，我们发现之前工作中提出的基于阈值的测试在特定场景下表现良好，但在不同类型的OOD实例中的表现并不一致。相比之下，我们提出的方法结合了m个不同统计量。

    We study the problem of Out-of-Distribution (OOD) detection, that is, detecting whether a learning algorithm's output can be trusted at inference time. While a number of tests for OOD detection have been proposed in prior work, a formal framework for studying this problem is lacking. We propose a definition for the notion of OOD that includes both the input distribution and the learning algorithm, which provides insights for the construction of powerful tests for OOD detection. We propose a multiple hypothesis testing inspired procedure to systematically combine any number of different statistics from the learning algorithm using conformal p-values. We further provide strong guarantees on the probability of incorrectly classifying an in-distribution sample as OOD. In our experiments, we find that threshold-based tests proposed in prior work perform well in specific settings, but not uniformly well across different types of OOD instances. In contrast, our proposed method that combines m
    
[^126]: 基于Cortex-M微控制器的能效高的深度学习应用部署使用深度压缩技术

    Energy-efficient Deployment of Deep Learning Applications on Cortex-M based Microcontrollers using Deep Compression. (arXiv:2205.10369v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.10369](http://arxiv.org/abs/2205.10369)

    本研究通过深度压缩技术实现了在资源受限的微控制器上高效部署深度学习模型，分析了准确性、内存消耗、执行时间和功耗之间的权衡关系。

    

    大规模深度神经网络(DNN)是当今人工智能的基石，因为它们在被训练大量数据集时能够做出准确的预测。随着物联网等技术的发展，解释传感器产生的大量数据正在成为一个越来越重要的任务。然而，在许多应用中，不仅预测性能，还有深度学习模型的能耗也是一个主要关注点。本文通过网络压缩研究了在资源受限的微控制器架构上高效部署深度学习模型的方法。我们提出了一种系统性的方法来探索不同的DNN修剪、量化和部署策略，针对不同的基于ARM Cortex-M的低功耗系统。探索过程可以分析准确性、内存消耗、执行时间和功耗等关键指标之间的权衡。我们讨论了在三种不同微控制器上的实验结果。

    Large Deep Neural Networks (DNNs) are the backbone of today's artificial intelligence due to their ability to make accurate predictions when being trained on huge datasets. With advancing technologies, such as the Internet of Things, interpreting large quantities of data generated by sensors is becoming an increasingly important task. However, in many applications not only the predictive performance but also the energy consumption of deep learning models is of major interest. This paper investigates the efficient deployment of deep learning models on resource-constrained microcontroller architectures via network compression. We present a methodology for the systematic exploration of different DNN pruning, quantization, and deployment strategies, targeting different ARM Cortex-M based low-power systems. The exploration allows to analyze trade-offs between key metrics such as accuracy, memory consumption, execution time, and power consumption. We discuss experimental results on three dif
    
[^127]: 在深度强化学习中，具有可扩展观测模型的高效贝叶斯策略复用

    Efficient Bayesian Policy Reuse with a Scalable Observation Model in Deep Reinforcement Learning. (arXiv:2204.07729v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.07729](http://arxiv.org/abs/2204.07729)

    本研究提出了一种改进的贝叶斯策略复用方法，在深度强化学习中实现更高效的策略转移。该方法使用即时且信息丰富的状态转换样本作为观测信号，并提出了一个可扩展的观测模型来加速任务推断和策略复用。

    

    贝叶斯策略复用（BPR）是一种从离线库中选择源策略的通用策略转移框架，它通过推断基于一些观测信号和训练好的观测模型的任务信念来实现。在本文中，我们提出了一种改进的BPR方法，以实现在深度强化学习中更高效的策略转移。首先，大多数BPR算法使用回合返回作为观测信号，但它包含的信息有限，并且直到回合结束才能获得。相反，我们使用具有信息量和即时性的状态转换样本作为观测信号，以实现更快速和更准确的任务推断。其次，BPR算法通常需要大量样本来估计基于表格的观测模型的概率分布，这可能是昂贵的甚至不可行的，特别是当使用状态转换样本作为信号时。因此，我们提出了一种基于拟合的可扩展观测模型，并使用此模型来加速任务推断和策略复用。

    Bayesian policy reuse (BPR) is a general policy transfer framework for selecting a source policy from an offline library by inferring the task belief based on some observation signals and a trained observation model. In this paper, we propose an improved BPR method to achieve more efficient policy transfer in deep reinforcement learning (DRL). First, most BPR algorithms use the episodic return as the observation signal that contains limited information and cannot be obtained until the end of an episode. Instead, we employ the state transition sample, which is informative and instantaneous, as the observation signal for faster and more accurate task inference. Second, BPR algorithms usually require numerous samples to estimate the probability distribution of the tabular-based observation model, which may be expensive and even infeasible to learn and maintain, especially when using the state transition sample as the signal. Hence, we propose a scalable observation model based on fitting 
    
[^128]: 适应具有马尔可夫数据的随机优化中的混合时间

    Adapting to Mixing Time in Stochastic Optimization with Markovian Data. (arXiv:2202.04428v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.04428](http://arxiv.org/abs/2202.04428)

    本文提出了一种适用于马尔可夫数据的随机优化问题的方法，不需要对混合时间有任何了解，但在凸问题中可以获得最优收敛速度。这种方法还可以应用于非凸优化以及时差学习，并且完全无视混合时间。方法的关键是多层蒙特卡洛梯度估计与自适应学习方法的组合。

    

    我们考虑数据从马尔可夫链中提取的随机优化问题。现有的这种设置的方法关键依赖于对链的混合时间的了解，而在实际应用中通常是未知的。我们提出了一种不需要了解混合时间的最优化方法，但在应用于凸问题时可以获得最优的渐近收敛速度。我们进一步展示了我们的方法可以扩展到：(i)寻找非凸优化中的稳定点以及(ii)在时差学习中获得对混合时间更好的依赖。在这两种情况下，我们的方法对混合时间完全无视。我们的方法依赖于多层蒙特卡洛(MLMC)梯度估计与自适应学习方法的新颖组合。

    We consider stochastic optimization problems where data is drawn from a Markov chain. Existing methods for this setting crucially rely on knowing the mixing time of the chain, which in real-world applications is usually unknown. We propose the first optimization method that does not require the knowledge of the mixing time, yet obtains the optimal asymptotic convergence rate when applied to convex problems. We further show that our approach can be extended to: (i) finding stationary points in non-convex optimization with Markovian data, and (ii) obtaining better dependence on the mixing time in temporal difference (TD) learning; in both cases, our method is completely oblivious to the mixing time. Our method relies on a novel combination of multi-level Monte Carlo (MLMC) gradient estimation together with an adaptive learning method.
    
[^129]: 未来学习：基于未来信息的合理外推

    Prospective Learning: Principled Extrapolation to the Future. (arXiv:2201.07372v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.07372](http://arxiv.org/abs/2201.07372)

    这项研究提出了一种基于未来信息的学习方法，将学习问题重新定义为关于动态未来的概念，并认为前瞻性学习更准确地描述了现实世界中的许多问题。

    

    学习是一个可以根据过去的经验更新决策规则的过程，从而提高未来的性能。传统上，机器学习常常在假设未来与过去的分布相同或会以对抗的方式改变的情况下进行评估。但是这些假设对于现实世界中许多问题来说，可能过于乐观或悲观。现实世界的场景在多个时空尺度上演变，具有部分可预测的动力学。在这里，我们重新定义学习问题，将其聚焦于动态未来的概念，这种未来是部分可学习的。我们推测某些任务序列在回顾性学习中不可学习（其中数据分布固定），但在前瞻性学习中可学习（其中分布可能动态），这表明前瞻性学习在本质上比回顾性学习更困难。我们认为前瞻性学习更准确地描述了许多现实世界的问题，

    Learning is a process which can update decision rules, based on past experience, such that future performance improves. Traditionally, machine learning is often evaluated under the assumption that the future will be identical to the past in distribution or change adversarially. But these assumptions can be either too optimistic or pessimistic for many problems in the real world. Real world scenarios evolve over multiple spatiotemporal scales with partially predictable dynamics. Here we reformulate the learning problem to one that centers around this idea of dynamic futures that are partially learnable. We conjecture that certain sequences of tasks are not retrospectively learnable (in which the data distribution is fixed), but are prospectively learnable (in which distributions may be dynamic), suggesting that prospective learning is more difficult in kind than retrospective learning. We argue that prospective learning more accurately characterizes many real world problems that (1) cur
    
[^130]: 一般指数族的Bregman偏差

    Bregman Deviations of Generic Exponential Families. (arXiv:2201.07306v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.07306](http://arxiv.org/abs/2201.07306)

    我们通过结合Bregman差异和超马丁格尔混合方法，建立了一种通用边界，控制指数族参数与参数有限样本估计之间的Bregman差异，该边界是时间均匀的，并引入了Bregman信息增益。我们将此边界应用于多个经典指数族，并得到了置信区间和Bregman信息增益的明确形式。

    

    我们重新审视了混合技术方法，也称为拉普拉斯方法，以研究一般指数族中的浓度现象。将与家族的对数分区函数相关联的Bregman差异的特性与超马丁格尔混合方法相结合，我们建立了一个通用的界限，控制家族的参数与参数的有限样本估计之间的Bregman差异。我们的界限是时间均匀的，并且出现了一种扩展经典信息增益到指数族的量，我们称之为Bregman信息增益。对于实践者，我们将这个新颖的界限实例化到几个经典家族中，例如，高斯、伯努利、指数、韦伯、帕累托、泊松和卡方，得到了置信区间和Bregman信息增益的明确形式。我们进一步数值比较了与时间均匀浓度的最新替代方法得到的置信界限，并表明th。

    We revisit the method of mixture technique, also known as the Laplace method, to study the concentration phenomenon in generic exponential families. Combining the properties of Bregman divergence associated with log-partition function of the family with the method of mixtures for super-martingales, we establish a generic bound controlling the Bregman divergence between the parameter of the family and a finite sample estimate of the parameter. Our bound is time-uniform and makes appear a quantity extending the classical information gain to exponential families, which we call the Bregman information gain. For the practitioner, we instantiate this novel bound to several classical families, e.g., Gaussian, Bernoulli, Exponential, Weibull, Pareto, Poisson and Chi-square yielding explicit forms of the confidence sets and the Bregman information gain. We further numerically compare the resulting confidence bounds to state-of-the-art alternatives for time-uniform concentration and show that th
    
[^131]: 气候不变的机器学习

    Climate-Invariant Machine Learning. (arXiv:2112.08440v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.08440](http://arxiv.org/abs/2112.08440)

    本研究提出了一种新的框架——"气候不变"的机器学习，通过将气候过程的知识纳入机器学习算法中，可以在广泛的气候和地理条件下保持高准确性。

    

    气候变化预测是一个泛化问题：我们使用物理模型在过去、现在和未来的气候中对最近的过去进行外推。目前的气候模型需要对小于模型网格大小的尺度上发生的过程进行表示，这些过程是模型预测不确定性的主要来源。最近的机器学习（ML）算法有望改善这种过程表示，但往往在未经训练的气候环境中外推效果不佳。为了充分发挥物理和统计方法的优势，我们提出了一个新框架——称为"气候不变"的机器学习——将气候过程的知识纳入ML算法中，并证明它可以在三个不同的大气模型中在广泛的气候和地理条件下保持高准确性。我们的结果表明，将物理知识明确纳入数据驱动的地球系统过程模型中可以提高它们的一致性、数据效率和泛化能力。

    Projecting climate change is a generalization problem: we extrapolate the recent past using physical models across past, present, and future climates. Current climate models require representations of processes that occur at scales smaller than model grid size, which have been the main source of model projection uncertainty. Recent machine learning (ML) algorithms hold promise to improve such process representations, but tend to extrapolate poorly to climate regimes they were not trained on. To get the best of the physical and statistical worlds, we propose a new framework -- termed "climate-invariant" ML -- incorporating knowledge of climate processes into ML algorithms, and show that it can maintain high accuracy across a wide range of climate and geographic conditions in three distinct atmospheric models. Our results suggest that explicitly incorporating physical knowledge into data-driven models of Earth system processes can improve their consistency, data efficiency, and generaliz
    
[^132]: 新颖方向上对象泛化的新兴神经网络机制

    Emergent Neural Network Mechanisms for Generalization to Objects in Novel Orientations. (arXiv:2109.13445v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2109.13445](http://arxiv.org/abs/2109.13445)

    本研究提供了证据表明，深度神经网络具有通过传播方向不变性来泛化到新颖方向上的对象的能力。这种能力受到训练中使用的熟悉对象数量的影响，但仅限于涉及2D旋转的熟悉方向。

    

    深度神经网络（DNNs）在识别训练数据分布之外的方向上的对象的能力尚不完全了解。我们提供证据表明，DNNs能够通过传播从多个视点观察到的熟悉对象获得的方向不变性来泛化到新颖方向上的对象。这种能力在训练DNN时使用越来越多的熟悉对象时会增强，但仅限于涉及到熟悉方向的2D旋转的方向。我们展示了这种传播是通过调整到熟悉和不熟悉对象之间共同特征的神经元实现的。这些结果揭示了类脑神经机制的泛化能力。

    The capability of Deep Neural Networks (DNNs) to recognize objects in orientations outside the distribution of the training data is not well understood. We present evidence that DNNs are capable of generalizing to objects in novel orientations by disseminating orientation-invariance obtained from familiar objects seen from many viewpoints. This capability strengthens when training the DNN with an increasing number of familiar objects, but only in orientations that involve 2D rotations of familiar orientations. We show that this dissemination is achieved via neurons tuned to common features between familiar and unfamiliar objects. These results implicate brain-like neural mechanisms for generalization.
    
[^133]: 通过Wasserstein差异度实现多层超图划分的平衡粗化方案

    Balanced Coarsening for Multilevel Hypergraph Partitioning via Wasserstein Discrepancy. (arXiv:2106.07501v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.07501](http://arxiv.org/abs/2106.07501)

    本研究提出了一种平衡粗化方案，用于多层超图划分。通过Wasserstein差异度协调粗化过程中的最优传输，并使用三点算法找到在平衡约束下的最佳划分。

    

    我们提出了一种适用于多层超图划分的平衡粗化方案。此外，我们设计了一种初始划分算法，以改善k-way超图划分的质量。通过使用LPT算法分配顶点权重，我们在放松平衡约束下生成了先验超图。利用先验超图，我们定义了Wasserstein差异度来协调粗化过程中的最优传输。并使用Sinkhorn算法求解最优传输矩阵。我们的粗化方案充分考虑了连接度度量（目标函数）的最小化。对于初始划分阶段，我们定义了由Fiedler向量引发的归一化切函数，理论上证明是一个凹函数。因此，我们设计了一个三点算法以找到在平衡约束下的最佳划分。

    We propose a balanced coarsening scheme for multilevel hypergraph partitioning. In addition, an initial partitioning algorithm is designed to improve the quality of k-way hypergraph partitioning. By assigning vertex weights through the LPT algorithm, we generate a prior hypergraph under a relaxed balance constraint. With the prior hypergraph, we have defined the Wasserstein discrepancy to coordinate the optimal transport of coarsening process. And the optimal transport matrix is solved by Sinkhorn algorithm. Our coarsening scheme fully takes into account the minimization of connectivity metric (objective function). For the initial partitioning stage, we define a normalized cut function induced by Fiedler vector, which is theoretically proved to be a concave function. Thereby, a three-point algorithm is designed to find the best cut under the balance constraint.
    
[^134]: 一种零知识协调的新形式、方法和未解之谜

    A New Formalism, Method and Open Issues for Zero-Shot Coordination. (arXiv:2106.06613v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2106.06613](http://arxiv.org/abs/2106.06613)

    本研究提出了零知识协调问题的正式定义，并证明了之前的解决方法不是最优解。针对这一问题，引入了带有打破平局的其他对局算法作为最优解。

    

    在许多协调问题中，独立推理的人类能够发现互相兼容的策略。相反，独立训练的自博弈策略通常是互不兼容的。零知识协调（ZSC）最近被提出作为多智能体强化学习中的一个新前沿，以解决这一基本问题。先前的研究通过假设玩家可以在学习算法上达成一致，但在行动和观测标签上无法达成一致，提出了其他对局作为一种最优解。然而，直到现在，这个“无标签”问题只是被非正式地定义。我们将这种设置形式化为无标签协调（LFC）问题，通过定义无标签协调博弈来解决。我们证明了其他对局不能成为LFC问题的最优解，因为它未能在不兼容的最大化者之间一致地打破平局。我们引入了算法的扩展，即带有打破平局的其他对局，同时证明了它的最优性。

    In many coordination problems, independently reasoning humans are able to discover mutually compatible policies. In contrast, independently trained self-play policies are often mutually incompatible. Zero-shot coordination (ZSC) has recently been proposed as a new frontier in multi-agent reinforcement learning to address this fundamental issue. Prior work approaches the ZSC problem by assuming players can agree on a shared learning algorithm but not on labels for actions and observations, and proposes other-play as an optimal solution. However, until now, this "label-free" problem has only been informally defined. We formalize this setting as the label-free coordination (LFC) problem by defining the label-free coordination game. We show that other-play is not an optimal solution to the LFC problem as it fails to consistently break ties between incompatible maximizers of the other-play objective. We introduce an extension of the algorithm, other-play with tie-breaking, and prove that it
    
[^135]: 在网络中学习低秩潜在中尺度结构

    Learning low-rank latent mesoscale structures in networks. (arXiv:2102.06984v5 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2102.06984](http://arxiv.org/abs/2102.06984)

    这项研究提出了一种学习网络中低秩潜在中尺度结构的新方法，并通过合成网络模型和实际网络验证了其有效性。通过利用少量的“潜在模式”，可以成功地近似网络的大多数子图。这项研究对于理解复杂系统的行为具有重要意义。

    

    在复杂系统中，常常使用网络来编码实体之间的相互作用的体系结构，包括物理、生物、社会和信息科学。为了研究复杂系统的大规模行为，有必要研究网络中的中尺度结构作为影响这种行为的构建块。我们提出了一种描述网络中低秩潜在中尺度结构的新方法，并且通过使用几个合成网络模型和实证友谊、合作和蛋白质相互作用（PPI）网络来说明我们的方法。我们发现，这些网络具有一个相对较小数量的“潜在模式”，这些模式共同可以成功地近似网络的大多数子图在固定的中尺度上。我们使用了一种“网络字典学习”（NDL）的算法，该算法结合了网络采样方法和非负矩阵分解，以学习给定网络的潜在模式。

    It is common to use networks to encode the architecture of interactions between entities in complex systems in the physical, biological, social, and information sciences. To study the large-scale behavior of complex systems, it is useful to examine mesoscale structures in networks as building blocks that influence such behavior. We present a new approach for describing low-rank mesoscale structures in networks, and we illustrate our approach using several synthetic network models and empirical friendship, collaboration, and protein--protein interaction (PPI) networks. We find that these networks possess a relatively small number of `latent motifs' that together can successfully approximate most subgraphs of a network at a fixed mesoscale. We use an algorithm for `network dictionary learning' (NDL), which combines a network-sampling method and nonnegative matrix factorization, to learn the latent motifs of a given network. The ability to encode a network using a set of latent motifs has
    
[^136]: 声明性机制设计

    Declarative Mechanism Design. (arXiv:1912.13122v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/1912.13122](http://arxiv.org/abs/1912.13122)

    本文介绍了声明性机制设计的研究，提出了机构神经网络作为一种受管制的人工神经网络，引起人们对人工教学的关注，并提供了初步的答案。

    

    多智能体系统（MAS）和声明性电子机构（DEIs）的调控是过去十年涉及物理和软件智能体以及法律的多学科研究课题，但近年来逐渐演变为2016年起被称为新闻的机器律师。其中一种首次提出限制软件智能体行为的方案是电子机构。然而，随着人工神经网络（ANNs）被重新定义为深度学习（DL），有关DL使用的安全、隐私、伦理和法律问题引起了人工智能（AI）社区的关注。现在，MAS的规范几乎得到正确处理，我们提出将人工神经网络的规范作为一种特殊类型的受管制的人工神经网络，称之为机构神经网络（INN）。本文的主旨是引起人们对人工教学（AT）的关注，并给出一个初步的答案，展示了一种证明性的方法。

    Regulation of Multi-Agent Systems (MAS) and Declarative Electronic Institutions (DEIs) was a multidisciplinary research topic of the past decade involving (Physical and Software) Agents and Law since the beginning, but recently evolved towards News-claimed Robot Lawyer since 2016. One of these first proposals of restricting the behaviour of Software Agentswas Electronic Institutions.However, with the recent reformulation of Artificial Neural Networks (ANNs) as Deep Learning (DL), Security, Privacy,Ethical and Legal issues regarding the use of DL has raised concerns in the Artificial Intelligence (AI) Community. Now that the Regulation of MAS is almost correctly addressed, we propose the Regulation of Artificial Neural Networks as Agent-based Training of a special type of regulated Artificial Neural Network that we call Institutional Neural Network (INN).The main purpose of this paper is to bring attention to Artificial Teaching (AT) and to give a tentative answer showing a proof-of-con
    
[^137]: 从一个单一的视频演示中学习模仿的方法

    Towards Learning to Imitate from a Single Video Demonstration. (arXiv:1901.07186v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1901.07186](http://arxiv.org/abs/1901.07186)

    本研究提出了一种从单一视频演示中学习模仿的方法，通过使用对比训练和Siamese循环神经网络，我们能够学习到智能体的行为与演示之间的奖励函数，并通过 RL 策略的训练最小化这个距离。实验表明，引入多任务数据和额外的图像编码损失可以改善学习到的奖励的时间一致性，并显著提高策略学习的效果。我们在不同维度的仿真智能体上验证了我们的方法的优越性。

    

    能够从视频观察中学习模仿的智能体——\emph{没有直接访问状态或动作信息}，对于在自然世界中的学习更具适用性。然而，制定一个能够实现此目标的增强学习（RL）智能体仍然是一个重大挑战。我们使用对比训练来解决这个挑战，学习一个将智能体的行为与单个演示进行比较的奖励函数。我们使用连体循环神经网络架构在时间和空间上学习动作片段之间的奖励，同时训练一个RL策略来最小化这个距离。通过实验证明，多任务数据和额外的图像编码损失的引入改进了学习到的奖励的时间一致性，从而显着提高了策略学习的效果。我们在2D中的模拟人型、狗和迅猛龙智能体以及3D中的四足动物和人型智能体上展示了我们的方法。我们展示了我们的方法优于当前最先进的状态。

    Agents that can learn to imitate given video observation -- \emph{without direct access to state or action information} are more applicable to learning in the natural world. However, formulating a reinforcement learning (RL) agent that facilitates this goal remains a significant challenge. We approach this challenge using contrastive training to learn a reward function comparing an agent's behaviour with a single demonstration. We use a Siamese recurrent neural network architecture to learn rewards in space and time between motion clips while training an RL policy to minimize this distance. Through experimentation, we also find that the inclusion of multi-task data and additional image encoding losses improve the temporal consistency of the learned rewards and, as a result, significantly improves policy learning. We demonstrate our approach on simulated humanoid, dog, and raptor agents in 2D and a quadruped and a humanoid in 3D. We show that our method outperforms current state-of-the-
    

