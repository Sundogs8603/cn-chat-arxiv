# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Towards Lightweight Data Integration using Multi-workflow Provenance and Data Observability.](http://arxiv.org/abs/2308.09004) | 本论文提出MIDA方法，以多工作流可信度和数据可观测性为基础，实现了轻量级的运行时集成数据分析。该方法通过定义数据可观测策略和适应性方法，可以处理异构科学环境下多个支持工具和高效的HPC执行。 |
| [^2] | [DealMVC: Dual Contrastive Calibration for Multi-view Clustering.](http://arxiv.org/abs/2308.09000) | 提出了一种名为DealMVC的双向对比校准网络，用于解决多视图聚类中的一致性问题，并利用全局和局部对比校准损失来提高聚类性能。 |
| [^3] | [Reinforced Self-Training (ReST) for Language Modeling.](http://arxiv.org/abs/2308.08998) | 本文提出了一种称为自学习增强 (ReST) 的算法，通过从人类反馈中进行强化学习来提高大型语言模型 (LLM) 的输出质量。在机器翻译任务上的实验结果表明，ReST能够以高效的方式显著提高翻译质量。 |
| [^4] | [Neural oscillators for generalization of physics-informed machine learning.](http://arxiv.org/abs/2308.08989) | 本文提出了一种称为神经振荡器的方法，通过与物理信息机器学习模型的融合，利用偏微分方程解的因果关系和时间特性来提高其泛化能力。实验证明了这种方法在处理复杂物理问题时的有效性。 |
| [^5] | [Quantifying the biomimicry gap in biohybrid systems.](http://arxiv.org/abs/2308.08978) | 这项工作中，研究人员针对生物混合系统中的仿生差距进行了量化分析。通过使用真实鱼类和仿生诱饵进行实验，以及对鱼类对的模拟，研究人员证明了他们的系统可以生成与真实鱼类完全相似的社会互动。 |
| [^6] | [Hitting the High-Dimensional Notes: An ODE for SGD learning dynamics on GLMs and multi-index models.](http://arxiv.org/abs/2308.08977) | 本文研究了应用于广义线性模型和多索引模型中的流式随机梯度下降（SGD）的学习动力学。通过建立了一个普通微分方程系统来描述风险和次优性度量等统计量，获得了稳定性学习率阈值和收敛保证。同时，引入了一个简化扩散系数的随机微分方程模型，用于分析SGD迭代的统计动力学。通过标准示例和数值模拟，验证了该理论的有效性。 |
| [^7] | [A Dual-Perspective Approach to Evaluating Feature Attribution Methods.](http://arxiv.org/abs/2308.08949) | 这篇论文提出了一种双重视角的方法来评估特征归因方法。通过观察扰动归因特征对模型行为的影响，这种方法揭示了归因特征的准确性和完整性，使其能够定量评估特征归因的表现。 |
| [^8] | [Predicting Crop Yield With Machine Learning: An Extensive Analysis Of Input Modalities And Models On a Field and sub-field Level.](http://arxiv.org/abs/2308.08948) | 本论文介绍了一种用于农作物产量预测的简单而有效的早期融合方法，可以处理具有不同时间和空间分辨率的多个输入模态。该方法使用了高分辨率的农作物产量地图进行训练，并采用了农作物和机器学习模型无关的方法进行亚田级别的预测。该方法使用全球覆盖的输入模态，并强调了输入模态对于产量预测的重要性。 |
| [^9] | [Interpretable Graph Neural Networks for Tabular Data.](http://arxiv.org/abs/2308.08945) | 本论文提出了一种称为IGNNet的方法，可以在处理表格数据时使用图神经网络，该方法能够产生可解释的模型，从原始输入特征精确计算预测结果，并且在性能上与最先进的机器学习算法性能相当。 |
| [^10] | [Causal Adversarial Perturbations for Individual Fairness and Robustness in Heterogeneous Data Spaces.](http://arxiv.org/abs/2308.08938) | 本文提出了一种新颖的方法，通过在异构数据空间中引入因果对抗扰动并应用对抗训练，结合个体公平性、因果性和鲁棒性，以解决在处理离散敏感属性时的问题。实验证明了该方法的有效性。 |
| [^11] | [Estimating fire Duration using regression methods.](http://arxiv.org/abs/2308.08936) | 本文使用机器学习方法解决了野火持续时间预测的计算代价高、时间消耗大的问题，通过使用回归模型和图像处理技术，结合卫星地形要素地图和历史火灾数据，能够基于地貌图像快速且相对准确地预测火灾的燃烧持续时间。 |
| [^12] | [On Data Imbalance in Molecular Property Prediction with Pre-training.](http://arxiv.org/abs/2308.08934) | 本研究探讨了在预训练的分子属性预测中的数据不平衡问题，并提出了一个结合理论计算和机器学习的方法来构建替代模型。同时，通过预训练技术提高了机器学习模型的准确性。 |
| [^13] | [IMM: An Imitative Reinforcement Learning Approach with Predictive Representation Learning for Automatic Market Making.](http://arxiv.org/abs/2308.08918) | 本研究提出了一种基于预测表示学习的模仿增强学习方法，在自动做市中应用。该方法通过借鉴专业人类做市商的工作流程，结合子优信号专家的知识和直接策略交互，开发了适用于多价格水平的做市策略。 |
| [^14] | [Beyond Sharing: Conflict-Aware Multivariate Time Series Anomaly Detection.](http://arxiv.org/abs/2308.08915) | 这篇论文提出了一种冲突感知的多变量时间序列异常检测算法，该算法通过为每个指标提供独特的结构来缓解指标回归目标之间的冲突。 |
| [^15] | [Development of a Knowledge Graph Embeddings Model for Pain.](http://arxiv.org/abs/2308.08904) | 该论文开发了一个用于疼痛的知识图谱嵌入模型，以便研究疼痛概念及其关系。知识图谱嵌入可以将大型图谱转化为低维向量，从而提高计算效率，并用于各种任务。 |
| [^16] | [Optimal Resource Allocation for U-Shaped Parallel Split Learning.](http://arxiv.org/abs/2308.08896) | 本论文提出了一种新颖的并行U型分层学习方法，并设计了优化资源分配方案以提高边缘网络的性能。实验结果表明该方法可以达到与其他方法相似的性能。 |
| [^17] | [Dual Gauss-Newton Directions for Deep Learning.](http://arxiv.org/abs/2308.08886) | 本文研究了深度学习目标的结构特点，提出了基于双重高斯牛顿方向预言的方法。通过对偶形式计算这些预言，得到了既具有计算好处又具有新见解的结果。通过实验证明了这些预言作为随机梯度的下降方向的优势，并研究了计算上的权衡。 |
| [^18] | [Feature Enforcing PINN (FE-PINN): A Framework to Learn the Underlying-Physics Features Before Target Task.](http://arxiv.org/abs/2308.08873) | FE-PINN是一种学习底层物理特征的框架，在主训练之前以低计算成本解决问题的模式。与传统PINN相比，FE-PINN通过执行一系列子任务来解决损失函数不平衡的问题，并具有快速训练和更高的求解速度。 |
| [^19] | [Towards Semi-supervised Learning with Non-random Missing Labels.](http://arxiv.org/abs/2308.08872) | 本研究针对标签缺失非随机（MNAR）问题，提出了一种基于类别转换跟踪的伪修正指导（PRG）方法，通过利用历史信息和动态创建的图模型，来改善半监督学习（SSL）中的标签缺失问题。 |
| [^20] | [Model-Free Algorithm with Improved Sample Efficiency for Zero-Sum Markov Games.](http://arxiv.org/abs/2308.08858) | 本文提出了一种模型自由的算法，可以在零和马尔可夫博弈中实现与模型为基础算法相同的样本复杂度，首次证明了模型自由算法可以在时间段依赖性方面达到同样的优化效果。 |
| [^21] | [Bag of Tricks for Long-Tailed Multi-Label Classification on Chest X-Rays.](http://arxiv.org/abs/2308.08853) | 本论文介绍了一种针对胸部X射线图像的长尾多标签分类的"技巧包"。通过多种先进的设计，如数据增强、特征提取器、分类器设计等，结合预训练技术，提高了胸部X射线的诊断性能。 |
| [^22] | [Learning the hub graphical Lasso model with the structured sparsity via an efficient algorithm.](http://arxiv.org/abs/2308.08852) | 通过双重交替方向乘子法 (ADMM) 和半平滑牛顿 (SSN) 基于增广对偶法 (ALM) 的方法，我们提出了一个高效算法来学习具有结构稀疏性的核心图形Lasso模型，该算法能够在大维度的任务中节省超过70\%的执行时间，并且具有较高的性能。 |
| [^23] | [Machine Learning-Assisted Discovery of Novel Reactor Designs via CFD-Coupled Multi-fidelity Bayesian Optimisation.](http://arxiv.org/abs/2308.08841) | 本研究通过应用多精度贝叶斯优化方法，结合参数化网格化和模拟，提出了两种新的螺旋管参数化方法，用于发现新的反应器设计。这种方法能够处理高维度和复杂的优化问题，并克服了没有梯度的非局部优化困难。 |
| [^24] | [Controlling Federated Learning for Covertness.](http://arxiv.org/abs/2308.08825) | 本文研究了控制隐密联合学习的问题，提出了一个基于马尔可夫决策过程的策略梯度算法，通过动态选择学习和混淆来实现最优查询策略。实验结果在恶意言论分类任务上展示了方法的实用性。 |
| [^25] | [Mitigating Semantic Confusion from Hostile Neighborhood for Graph Active Learning.](http://arxiv.org/abs/2308.08823) | 该论文提出了一种用于缓解图形主动学习中来自恶意邻域的语义混淆问题的方法。通过引入带有语义特征的节点的成对相似度和不相似度来共同评估节点的影响力，并设计了一种新的基于样本集的准则和查询策略来维持所选节点的多样性和类别平衡。 |
| [^26] | [A Fusion of Variational Distribution Priors and Saliency Map Replay for Continual 3D Reconstruction.](http://arxiv.org/abs/2308.08812) | 本研究提出了一种基于连续学习的三维重建方法，通过使用变分先验和显著性图重新播放，实现对之前已见类别的合理重建。实验结果表明，与已建立的方法相比具有竞争力。 |
| [^27] | [Label Shift Adapter for Test-Time Adaptation under Covariate and Label Shifts.](http://arxiv.org/abs/2308.08810) | 提出了一种新的标签偏移适配器，可以与现有的测试时适应性方法结合使用，有效处理在协变量和标签偏移下的问题。 |
| [^28] | [Capturing Popularity Trends: A Simplistic Non-Personalized Approach for Enhanced Item Recommendation.](http://arxiv.org/abs/2308.08799) | 本论文提出了一种简化的非个性化方法PARE，通过预测最高流行度的项目进行推荐，填补了现有推荐方法忽略项目流行度的不足。实验证明PARE的性能优于复杂的方法。 |
| [^29] | [Tipping Point Forecasting in Non-Stationary Dynamics on Function Spaces.](http://arxiv.org/abs/2308.08794) | 本文提出了一种利用循环神经算子学习非平稳动力系统演化的方法，并且通过基于不确定性的方法检测未来的翻车点。同时，我们还提出了一种符合预测框架，通过监测与物理约束的偏离来预测翻车点，从而使得预测结果具有严格的不确定性度量。 |
| [^30] | [Federated Reinforcement Learning for Electric Vehicles Charging Control on Distribution Networks.](http://arxiv.org/abs/2308.08792) | 本文提出了一种结合了多EV充电/放电和在最佳功率流下运行的径向分布式网络的新方法，用于实时分配功率流来解决EV充电控制中的挑战。 |
| [^31] | [APPFLx: Providing Privacy-Preserving Cross-Silo Federated Learning as a Service.](http://arxiv.org/abs/2308.08786) | APPFLx是一个提供隐私保护的跨不同数据源的联合学习平台，为用户提供了简化的实验启动过程和可视化的实验生命周期追踪功能，使得领域专家和机器学习从业者能够轻松地进行隐私保护联合学习。 |
| [^32] | [Environment Diversification with Multi-head Neural Network for Invariant Learning.](http://arxiv.org/abs/2308.08778) | 本文提出了一个不变学习框架EDNIL，其中包含多头神经网络，用于吸收数据偏差，并展示了该框架的稳健性。 |
| [^33] | [Differential Privacy, Linguistic Fairness, and Training Data Influence: Impossibility and Possibility Theorems for Multilingual Language Models.](http://arxiv.org/abs/2308.08774) | 该论文研究了多语言语言模型在多语言压缩、语言公平性和透明性等方面的要求，并发现差分隐私与训练数据影响稀疏性之间存在相互制约的关系。 |
| [^34] | [Explainable AI for tool wear prediction in turning.](http://arxiv.org/abs/2308.08765) | 这篇论文开发了一个可解释人工智能框架，用于车削过程中刀具磨损的预测。通过使用随机森林算法进行训练，结合温度等输入特征，可以准确预测刀具的状态，刀具温度是判断刀具磨损的重要因素。 |
| [^35] | [Efficient Commercial Bank Customer Credit Risk Assessment Based on LightGBM and Feature Engineering.](http://arxiv.org/abs/2308.08762) | 本文基于LightGBM算法和特征工程开展了高效商业银行客户信用风险评估研究，通过构建新的特征属性，准确率达到0.734，AUC达到0.772，为商业银行的信贷授予提供参考。 |
| [^36] | [PMET: Precise Model Editing in a Transformer.](http://arxiv.org/abs/2308.08742) | 该论文通过分析Transformer模型中的隐藏状态，发现多头自注意力编码了某些通用知识提取模式，因此在进行模型编辑时，不需要更新多头自注意力的权重。 |
| [^37] | [ReProHRL: Towards Multi-Goal Navigation in the Real World using Hierarchical Agents.](http://arxiv.org/abs/2308.08737) | 本文提出了一种名为ReProHRL的层次化代理方法，通过强化学习实现了多目标导航任务，并使用物体检测器进行预处理。实验证明，ReProHRL方法在仿真和真实世界环境中表现优于现有方法。 |
| [^38] | [On the Effectiveness of Log Representation for Log-based Anomaly Detection.](http://arxiv.org/abs/2308.08736) | 本研究调查和比较了先前日志分析研究中常用的日志表示技术，并评估了它们在不同机器学习模型和公共日志数据上的表现。 |
| [^39] | [Dynamic Neural Network is All You Need: Understanding the Robustness of Dynamic Mechanisms in Neural Networks.](http://arxiv.org/abs/2308.08709) | 本研究旨在调查动态机制在DyNNs中的鲁棒性以及动态机制设计对DyNNs的影响，并提出了三个研究问题。 |
| [^40] | [Consciousness in Artificial Intelligence: Insights from the Science of Consciousness.](http://arxiv.org/abs/2308.08708) | 本论文提出了一种严谨的方法，通过对当前的人工智能系统进行详细评估来探讨人工智能的意识问题。研究中对几种科学意识理论进行概述，并通过计算方法推导出意识的“指示性特征”。分析结果表明目前的人工智能系统尚不具备意识，但建立具有意识的人工智能系统并无明显的障碍。 |
| [^41] | [Partially Observable Multi-agent RL with (Quasi-)Efficiency: The Blessing of Information Sharing.](http://arxiv.org/abs/2308.08705) | 本文研究了部分可观测随机博弈的可证明多Agent强化学习。通过信息共享和观测可能性假设，提出了构建近似模型以实现准效率的方法。 |
| [^42] | [Planning in the imagination: High-level planning on learned abstract search spaces.](http://arxiv.org/abs/2308.08693) | 本论文提出了一个名为PiZero的新方法，该方法使代理能够在自己创建的抽象搜索空间中进行高级规划，不受真实环境限制，可在任意时间尺度进行规划，并处理连续动作空间和部分可观察性的设置。在多个领域的实验中，该方法胜过可比的先前方法而无需假设访问环境模拟器。 |
| [^43] | [Quantifying Overfitting: Introducing the Overfitting Index.](http://arxiv.org/abs/2308.08682) | 本文引入了过拟合指数（OI），通过对乳腺超声图像数据集和MNIST数据集进行广泛实验，使用了多种架构，展示了OI的实用性和区分能力。结果表明，不同架构的过拟合行为存在差异，并强调了数据增强对于较小和更专业的数据集的缓解影响。OI为解决过拟合问题提供了一种有希望的途径。 |
| [^44] | [SkinDistilViT: Lightweight Vision Transformer for Skin Lesion Classification.](http://arxiv.org/abs/2308.08669) | 本论文提出了一个用于皮肤病变分类的轻量级视觉Transformer模型，采用知识蒸馏方法，既提高了模型的性能，又大幅减少了内存和推理时间的开销。 |
| [^45] | [BREATHE: Second-Order Gradients and Heteroscedastic Emulation based Design Space Exploration.](http://arxiv.org/abs/2308.08666) | BREATHE是一个基于二阶梯度和异方差模拟的设计空间探索的框架，能够高效地搜索传统的向量设计空间和基于图的设计空间，并在各自应用中取得了显著性能提升。 |
| [^46] | [Flickr Africa: Examining Geo-Diversity in Large-Scale, Human-Centric Visual Data.](http://arxiv.org/abs/2308.08656) | 本研究通过分析与每个非洲国家相关的地理标签的Flickr图像，探讨了大规模、人为中心的视觉数据中的地理多样性。结果显示，现有数据存在偏见，并出现了“他者化”现象，因此需要更多研究来获取代表非洲人和他们环境的图像数据。 |
| [^47] | [Physics Informed Recurrent Neural Networks for Seismic Response Evaluation of Nonlinear Systems.](http://arxiv.org/abs/2308.08655) | 物理信息化循环神经网络用于非线性系统的地震响应评估，通过利用机器学习和模式识别技术，在实时性能要求的应用中实现快速而准确的评估。 |
| [^48] | [Reproducing Kernel Hilbert Space Pruning for Sparse Hyperspectral Abundance Prediction.](http://arxiv.org/abs/2308.08653) | 本论文提出了一种用于稀疏高光谱丰度预测的重构核希尔伯特空间修剪方法，通过非负最小二乘优化构建稀疏表示，并引入最大似然压缩向量减少信息损失。评估结果表明，该方法在光谱重建误差和压缩率方面相比标准修剪、最小二乘和深度学习方法具有更好的性能。 |
| [^49] | [AdaptEx: A Self-Service Contextual Bandit Platform.](http://arxiv.org/abs/2308.08650) | AdaptEx是一个自助上下文赌博平台，通过利用多臂赌博算法个性化用户体验并提供最优解，同时最小化传统测试方法的成本和时间。它能够在不断变化的内容和“冷启动”情况下快速迭代。 |
| [^50] | [Towards Personalized Federated Learning via Heterogeneous Model Reassembly.](http://arxiv.org/abs/2308.08643) | 本文提出了一个名为pFedHR的新框架，利用异构模型重组实现个性化联邦学习。实验表明，pFedHR在各种设置下优于基准方法，并且能够有效降低使用不兼容数据的不良影响。 |
| [^51] | [Non-monotone Sequential Submodular Maximization.](http://arxiv.org/abs/2308.08641) | 本文研究了顺序子模最大化问题，首次考虑了非单调子模函数，提出了针对灵活和固定长度约束的有效解决方案。 |
| [^52] | [Fair GANs through model rebalancing with synthetic data.](http://arxiv.org/abs/2308.08638) | 本论文提出了一种通过使用合成数据实现模型再平衡来减轻生成对抗网络中的偏见的方法。通过对现有失衡模型进行潜空间探索生成均衡数据，并使用这些数据训练均衡模型，同时提出了一种偏见减轻的损失函数。在种族公平的训练中，提出的方法在公平度指标上改进了近5倍。 |
| [^53] | [FedPop: Federated Population-based Hyperparameter Tuning.](http://arxiv.org/abs/2308.08634) | FedPop是一种用于解决联邦学习中超参数调优问题的新算法，它采用基于人口的进化算法来优化客户端和服务器上的超参数。 |
| [^54] | [LSTM-Based Forecasting Model for GRACE Accelerometer Data.](http://arxiv.org/abs/2308.08621) | 本研究基于LSTM网络，提出了一种填充GRACE卫星加速计数据间断的方法，并成功预测了三个轴上的加速计数据。 |
| [^55] | [Boosting Logical Reasoning in Large Language Models through a New Framework: The Graph of Thought.](http://arxiv.org/abs/2308.08614) | 本文提出了一种新的提示技术——思维图（GoT），通过在三个不断升级的挑战中的测试，我们的方法在多步逻辑推理问题上表现优于GPT-4，并且相比最先进的提示方法思维树（ToT），我们的方法有更高的准确性提升。 |
| [^56] | [Integrating Renewable Energy in Agriculture: A Deep Reinforcement Learning-based Approach.](http://arxiv.org/abs/2308.08611) | 本文开发了一个基于深度强化学习的框架，通过深度Q网络（DQN）优化光伏系统在农业中的安装决策制定。该研究对于提高能源效率、减少环境影响并增加农业利润具有重要意义。 |
| [^57] | [PEvoLM: Protein Sequence Evolutionary Information Language Model.](http://arxiv.org/abs/2308.08578) | PEvoLM是一种蛋白质序列进化信息语言模型，它利用嵌入式语言模型将蛋白质序列转换为数字向量表示。 |
| [^58] | [A Comparative Analysis of the Capabilities of Nature-inspired Feature Selection Algorithms in Predicting Student Performance.](http://arxiv.org/abs/2308.08574) | 本研究对比分析了12种自然启发的特征选择算法在预测学生表现中的能力，发现利用这些算法进行特征选择并结合传统机器学习算法可以提高预测准确性，并减少特征集大小。 |
| [^59] | [A physics-informed machine learning model for reconstruction of dynamic loads.](http://arxiv.org/abs/2308.08571) | 本文提出了一种基于物理信息的机器学习模型，用于根据测量数据重建动态荷载。该模型可处理不完整和受污染的数据，并提供自然的正则化方法来处理测量系统的噪声。 |
| [^60] | [CMISR: Circular Medical Image Super-Resolution.](http://arxiv.org/abs/2308.08567) | 这里是中文总结出的一句话要点：本文提出了一种循环医学图像超分辨率方法（CMISR），采用全局反馈的闭环框架，具有明确的欠分辨率和超分辨率元素。CMISR在稳态下具有零恢复误差，且可以应用于现有的MISR算法。 |
| [^61] | [KMF: Knowledge-Aware Multi-Faceted Representation Learning for Zero-Shot Node Classification.](http://arxiv.org/abs/2308.08563) | 本文提出了一种基于知识的多方位框架（KMF），用于零样本节点分类任务。该框架通过提取知识图谱中的主题来增强标签语义，以改善模型的泛化能力。 |
| [^62] | [Implementation of The Future of Drug Discovery: QuantumBased Machine Learning Simulation (QMLS).](http://arxiv.org/abs/2308.08561) | 该论文介绍了一种名为QMLS的新概念，通过结合机器学习和量子模拟的方法，可以缩短药物研发的时间和降低成本。通过生成命中物和优化分子的过程，可以大大提高药物发现的效率。 |
| [^63] | [AI-Assisted Investigation of On-Chain Parameters: Risky Cryptocurrencies and Price Factors.](http://arxiv.org/abs/2308.08554) | 本论文使用人工智能算法分析历史数据和区块链参数，找出影响加密货币价格的因素并识别风险加密货币。 |
| [^64] | [Recurrent Neural Networks with more flexible memory: better predictions than rough volatility.](http://arxiv.org/abs/2308.08550) | 本论文提出了具有更灵活记忆的递归神经网络，能够更好地预测具有长期记忆的资产价格波动性，优于传统的粗糙波动性预测方法。 |
| [^65] | [Effects of Daily News Sentiment on Stock Price Forecasting.](http://arxiv.org/abs/2308.08549) | 本研究探讨了每日新闻情绪对股票价格预测的影响，并设计了一个高效的系统从新闻中提取情绪影响因素。研究发现投资者情绪对股票价格波动有显著影响。 |
| [^66] | [Two-and-a-half Order Score-based Model for Solving 3D Ill-posed Inverse Problems.](http://arxiv.org/abs/2308.08511) | 提出了一种两个半顺序基于得分的模型（TOSM），用于解决CT和MRI中的三维体重建问题。通过在二维空间中学习数据分布来减少训练的复杂性，然后在三维空间中更新数据分布以实现准确的三维重建。 |
| [^67] | [CDR: Conservative Doubly Robust Learning for Debiased Recommendation.](http://arxiv.org/abs/2308.08461) | 该论文提出了一种保守双重稳健策略（CDR），用于解决推荐系统中存在的有毒插补问题。CDR通过审查插补的均值和方差来过滤插补，结果显示CDR具有降低方差和改进尾部界限的优势，并且能够显著提升性能并减少有毒插补的频率。 |
| [^68] | [Precision and Recall Reject Curves for Classification.](http://arxiv.org/abs/2308.08381) | 该论文提出了一种在分类问题中评估精确度和召回率的拒绝曲线方法。使用感知量化的原型分类器来验证了该方法在不平衡数据集和医学实际数据上的有效性。 |
| [^69] | [HyperSNN: A new efficient and robust deep learning model for resource constrained control applications.](http://arxiv.org/abs/2308.08222) | HyperSNN是一种适用于资源受限控制应用的高效稳健深度学习模型，通过使用脉冲神经网络和高维计算，将能量消耗降低至1.36%-9.96%，同时提高了鲁棒性和准确性。它适用于交互式、移动和可穿戴设备，促进了能量高效和稳健的系统设计。 |
| [^70] | [Stochastic Controlled Averaging for Federated Learning with Communication Compression.](http://arxiv.org/abs/2308.08165) | 本文提出了两种压缩联邦学习算法(SCALLION和SCAFCOM)，通过重新审视经典的随机控制平均法并提出了等价但更高效/简化的形式，减少了上行通信成本。 |
| [^71] | [Backward Reasoning in Large Language Models for Verification.](http://arxiv.org/abs/2308.07758) | 本文研究了在大型语言模型中使用反向推理进行验证的方法。作者提出了一种新颖的技术，通过屏蔽问题中的一个标记，并要求语言模型预测被屏蔽的标记来验证候选答案。同时，作者还提出了一种结合正向和反向推理的方法来估计候选答案的概率。 |
| [^72] | [DiffSED: Sound Event Detection with Denoising Diffusion.](http://arxiv.org/abs/2308.07293) | 本论文提出了一种使用去噪扩散的声音事件检测方法，通过从噪声查询中生成准确的事件边界，实验表明该方法明显优于现有方法。 |
| [^73] | [AudioFormer: Audio Transformer learns audio feature representations from discrete acoustic codes.](http://arxiv.org/abs/2308.07221) | AudioFormer是一种学习音频特征表示的方法，通过生成离散的声学代码并利用它们来训练掩码语言模型，从而将音频分类任务视为自然语言理解的形式。此外，引入了多正样本对比学习方法，通过学习联合表示来捕捉音频中的相关性。 |
| [^74] | [Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges.](http://arxiv.org/abs/2308.06668) | 传统农业系统中的机器学习和深度学习模型存在局限性，而基础模型在语言和视觉任务中表现出了显著的成功。本研究旨在探索基础模型在智能农业领域的潜力和应用。 |
| [^75] | [Can Unstructured Pruning Reduce the Depth in Deep Neural Networks?.](http://arxiv.org/abs/2308.06619) | 本研究介绍了一种名为EGP的创新的熵引导剪枝算法，该算法能够通过优先剪除熵较低的层中的连接来有效压缩深度神经网络，同时保持其竞争性能水平。 |
| [^76] | [Mirror Diffusion Models.](http://arxiv.org/abs/2308.06342) | 本文提出了镜像扩散模型(MDMs)，用于在离散分类数据和连续领域中进行生成任务。MDMs受限制抽样问题的镜像Langevin算法启发，并提供了适应简单扩散、图像生成和文本生成等领域的自然扩展。 |
| [^77] | [Cost-effective On-device Continual Learning over Memory Hierarchy with Miro.](http://arxiv.org/abs/2308.06053) | 这项工作是首次探索基于层次内存回放的持续学习的设计空间，旨在在边缘设备上实现成本效益。提出了Miro，一个通过动态配置持续学习系统的新颖系统运行时，以实现最佳的成本效益。广泛的评估显示Miro明显优于其他方案。 |
| [^78] | [Towards Instance-adaptive Inference for Federated Learning.](http://arxiv.org/abs/2308.06051) | 本文提出了一种面向实例自适应推理的联邦学习算法，通过使用缩放和位移深度特征（SSF）实现了处理客户端数据异质性的能力。 |
| [^79] | [Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient.](http://arxiv.org/abs/2308.05681) | 本文针对基于骨骼的人体动作识别方法的脆弱性进行了研究，提出了一种新的硬性无盒对抗攻击方法，通过学习运动流形和定义骨骼-动作知情梯度来攻击模型，揭示了这种脆弱性的存在。 |
| [^80] | [PTransIPs: Identification of phosphorylation sites based on protein pretrained language model and Transformer.](http://arxiv.org/abs/2308.05115) | PTransIPs是一种新型深度学习模型，它将蛋白质序列中的氨基酸视为自然语言中的单词，并结合大型预训练蛋白质模型的嵌入。该模型通过结合卷积神经网络和Transformer模型进行训练，用于识别磷酸化位点。 |
| [^81] | [Adversarial ModSecurity: Countering Adversarial SQL Injections with Robust Machine Learning.](http://arxiv.org/abs/2308.04964) | 这篇论文介绍了Adversarial ModSecurity，它是一个使用强大的机器学习来对抗SQL注入攻击的防火墙。通过将核心规则集作为输入特征，该模型可以识别并防御对抗性SQL注入攻击。实验结果表明，AdvModSec在训练后能够有效地应对这类攻击。 |
| [^82] | [OmniDataComposer: A Unified Data Structure for Multimodal Data Fusion and Infinite Data Generation.](http://arxiv.org/abs/2308.04126) | OmniDataComposer是一种创新的多模态数据融合和无限数据生成方法，通过引入一个有效的协调数据结构，可以处理和合并视频、音频和文本等多模态数据输入，并实现跨模态数据校正。 |
| [^83] | [DOMINO: Domain-invariant Hyperdimensional Classification for Multi-Sensor Time Series Data.](http://arxiv.org/abs/2308.03295) | DOMINO是一种领域不变的高维分类方法，适用于多传感器时间序列数据；解决了分布偏移和计算资源的问题。 |
| [^84] | [Machine learning methods for the search for L&T brown dwarfs in the data of modern sky surveys.](http://arxiv.org/abs/2308.03045) | 在这项研究中，我们使用机器学习方法对PanStarrs DR1、2MASS和WISE数据进行分析，以区分L类和T类棕矮星和其他光谱和亮度类别的天体。这有助于建立一个均匀且完整的棕矮星样本，为研究提供可靠的数据集。 |
| [^85] | [Learning to Generate Training Datasets for Robust Semantic Segmentation.](http://arxiv.org/abs/2308.02535) | 本文提出了一种新的方法，通过生成真实和可信的扰动或异常图像来提高语义分割技术的鲁棒性。通过设计和训练Robusta，一种鲁棒的条件生成对抗网络，可以为训练可靠的分割模型提供可用的数据集，从而显著增强语义分割技术在面对现实世界的扰动和分布变化时的鲁棒性。 |
| [^86] | [A Counterfactual Safety Margin Perspective on the Scoring of Autonomous Vehicles' Riskiness.](http://arxiv.org/abs/2308.01050) | 本文基于反事实模拟提出了一个数据驱动的框架，用于比较不同自动驾驶车辆在不同操作设计领域中行为风险。通过引入反事实安全边界的概念，该框架可以找到最关键的情景，并评估自动驾驶车辆的风险频率和严重程度。该方法即使在自动驾驶车辆的行为策略未知的情况下也适用，对外部第三方风险评估机构有用。 |
| [^87] | [Hessian-Aware Bayesian Optimization for Decision Making Systems.](http://arxiv.org/abs/2308.00629) | 本文介绍了一种感知海森贝叶斯优化算法，旨在解决决策系统优化中梯度反馈稀缺或无效的问题。通过引入紧凑的多层架构和角色概念，并利用感知海森贝叶斯优化方法对参数进行优化，作者实现了对复杂决策系统的高效优化。 |
| [^88] | [Robust Single-view Cone-beam X-ray Pose Estimation with Neural Tuned Tomography (NeTT) and Masked Neural Radiance Fields (mNeRF).](http://arxiv.org/abs/2308.00214) | 在影像导引的微创医疗过程中，我们提出了新的方法，利用X射线投影进行辐射透明物体的姿态估计，并且展示了优化视图合成在完成此任务中的关键作用。 |
| [^89] | [Remote Bio-Sensing: Open Source Benchmark Framework for Fair Evaluation of rPPG.](http://arxiv.org/abs/2307.12644) | 这篇论文提出了远程生物感应技术rPPG的公开源基准框架，该框架可以公平评估rPPG的准确性问题并解决与皮肤颜色、相机特性和环境光等因素相关的挑战。 |
| [^90] | [Quantum Convolutional Neural Networks with Interaction Layers for Classification of Classical Data.](http://arxiv.org/abs/2307.11792) | 本文介绍了一种引入了三量子位相互作用的新型交互层的量子卷积网络，增加了网络的表达能力和纠缠能力，用于对图像和一维数据进行分类。 |
| [^91] | [Properties of Discrete Sliced Wasserstein Losses.](http://arxiv.org/abs/2307.10352) | 本文研究了离散切割Wasserstein损失的性质，并探讨了其正则性和优化性质以及通过蒙特卡洛近似的方法。 |
| [^92] | [Revisiting Domain-Adaptive 3D Object Detection by Reliable, Diverse and Class-balanced Pseudo-Labeling.](http://arxiv.org/abs/2307.07944) | 本文通过提出一种适用于多类训练设置的新型ReDB框架来解决现有领域自适应方法在多类训练设置下性能下降的问题，通过产生可靠的、多样化的和类平衡的伪三维框来引导目标领域的自训练。 |
| [^93] | [Negative probabilities in Gene Regulatory Networks.](http://arxiv.org/abs/2307.07738) | 该论文提出了一种基于基因表达和相关性符号的方法，用于识别基因之间的符号不确定共表达。通过构建具有符号不确定贡献的概率转移矩阵，可以量化基因调控网络中各种连接的结构和重要性，并解释其对网络几何结构的影响。 |
| [^94] | [PC-Droid: Faster diffusion and improved quality for particle cloud generation.](http://arxiv.org/abs/2307.06836) | PC-Droid是一个新的扩散模型，通过利用新的扩散公式和研究更近期的积分求解器，同时对所有类型的喷注进行训练，实现了最先进的性能。它不仅能提供更快的生成速度，而且在所有评估指标上都具有卓越的性能。 |
| [^95] | [Relation-aware subgraph embedding with co-contrastive learning for drug-drug interaction prediction.](http://arxiv.org/abs/2307.01507) | 本论文提出了一种新的关注关系的子图嵌入与对比学习方法RaSECo，用于预测多关系药物相互作用。该方法通过构建不同的药物图和采用对比学习机制，能够解决现有方法中对于新药物过拟合的问题。 |
| [^96] | [Streamlined Lensed Quasar Identification in Multiband Images via Ensemble Networks.](http://arxiv.org/abs/2307.01090) | 该研究通过集成多个卷积网络和视觉转换器，以简化透镜类星体在多波段图像中的识别。在测试数据集上表现出色，但在真实数据中很难泛化，并且存在大量虚假源的问题。 |
| [^97] | [Goal Representations for Instruction Following: A Semi-Supervised Language Interface to Control.](http://arxiv.org/abs/2307.00117) | 本文介绍了一种半监督语言接口控制机器人的方法，通过联合图像和目标信息的策略以及少量的语言数据实现了在真实世界中的稳健性能。 |
| [^98] | [Dimension Independent Mixup for Hard Negative Sample in Collaborative Filtering.](http://arxiv.org/abs/2306.15905) | 本文提出了一种协同过滤训练中维度无关的困难负样本混合方法（DINS），通过对采样区域的新视角进行重新审视来改进现有的采样方法。实验证明，DINS优于其他负采样方法，证实了其有效性和优越性。 |
| [^99] | [Visual Adversarial Examples Jailbreak Large Language Models.](http://arxiv.org/abs/2306.13213) | 本文对将图像引入大型语言模型的安全隐患进行了分析，指出视觉输入空间的连续性和高维性是对抗攻击的丰富领域，同时也为视觉攻击者提供了更广泛的实现对抗目标的可能性。 |
| [^100] | [Comprehensive Training and Evaluation on Deep Reinforcement Learning for Automated Driving in Various Simulated Driving Maneuvers.](http://arxiv.org/abs/2306.11466) | 本研究在模拟平台上对两种深度强化学习算法进行了全面评估和比较，以开发自动驾驶模型。通过定制的奖励函数，对准确度、效率、安全性和舒适度进行了评估。 |
| [^101] | [Blockchain-Enabled Federated Learning: A Reference Architecture Design, Implementation, and Verification.](http://arxiv.org/abs/2306.10841) | 本文提出了一种基于区块链的联邦学习参考架构，通过结合联邦学习和区块链技术，实现了去中心化、协作的机器学习系统，并保护了数据隐私和用户控制的身份。该架构使用去中心化标识符进行身份验证，通过智能合约实现强大的安全性和高效的去中心化，并能根据需求集成各种额外的元素，是一个适用范围广泛的 BCFL 解决方案。 |
| [^102] | [Hidden Biases of End-to-End Driving Models.](http://arxiv.org/abs/2306.07957) | 端到端驾驶模型存在偏见问题，并引入了次要组件的更改。本文针对目前多数最先进的方法中所存在的两种偏见进行了研究，并提出了合理的替代方法。在此基础上，开发了TF ++，在CARLA测试中表现优异。 |
| [^103] | [Human-Like Intuitive Behavior and Reasoning Biases Emerged in Language Models -- and Disappeared in GPT-4.](http://arxiv.org/abs/2306.07622) | 本研究揭示了大型语言模型（LLMs）具有类人直觉行为和认知错误的特点，而高级语言模型则通过学习避免这类错误并表现出超理性的方式。此外，通过使用心理学研究的方法探测LLMs，可以揭示其新生特性。 |
| [^104] | [Waffling around for Performance: Visual Classification with Random Words and Broad Concepts.](http://arxiv.org/abs/2306.07282) | 本研究提出了WaffleCLIP，一个使用随机字符和单词描述符进行零样本视觉分类的框架，它取代了使用大型语言模型生成的描述符。该方法不仅能够实现可比较的性能提升，而且可以作为基于语言模型的视觉分类模型扩展的替代选择和验证方法。 |
| [^105] | [A Weakly Supervised Approach to Emotion-change Prediction and Improved Mood Inference.](http://arxiv.org/abs/2306.06979) | 该论文提出了一种弱监督的方法来预测情绪变化并改进情绪推理。通过利用预训练的孪生网络进行度量学习，推理出情绪变化信息，并将其与心情标签一起用于心情分类。实验结果表明，情绪变化信息的加入可以提高心情预测效果，强调了建模心情-情绪相互作用对于有效的心情推理的重要性。 |
| [^106] | [iPLAN: Intent-Aware Planning in Heterogeneous Traffic via Distributed Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2306.06236) | 论文提出了基于MARL算法的iPLAN方法，可在高密度且异构交通场景下进行意图感知规划，使智能体能够从局部观测中推断附近驾驶者的意图，并通过行为或瞬时激励进行决策，实现自主导航。 |
| [^107] | [Fault Localization for Framework Conversions of Image Recognition Models.](http://arxiv.org/abs/2306.06157) | 本文提出针对深度学习框架转换中出现的模型崩溃和输出标签差异的故障定位和修复方法，成功修复多个图像识别模型跨多个深度学习框架的转换错误。 |
| [^108] | [AMEE: A Robust Framework for Explanation Evaluation in Time Series Classification.](http://arxiv.org/abs/2306.05501) | AMEE是一个模型无关的解释评价框架，用于量化和比较时间序列分类中多种基于显著性的解释方法的信息价值，帮助解决在这一领域中解释方法选择的难题。 |
| [^109] | [Embedding stochastic differential equations into neural networks via dual processes.](http://arxiv.org/abs/2306.04847) | 该论文提出了一种新方法，通过将随机微分方程的信息直接与神经网络的权重进行比较，构建用于预测随机微分方程期望的神经网络。这种方法避免了过度拟合问题，并在原点附近的输入有着准确度。 |
| [^110] | [Enhance Diffusion to Improve Robust Generalization.](http://arxiv.org/abs/2306.02618) | 本文通过连续时间随机微分方程的研究，发现对抗性训练中的扩散项决定了神经网络的鲁棒泛化能力，进而提出了一种改进的AT框架。 |
| [^111] | [On Size-Independent Sample Complexity of ReLU Networks.](http://arxiv.org/abs/2306.01992) | 本文研究了ReLU神经网络的样本复杂度，给出了一个现有方法精细化的结果，实现了无深度依赖性的上界。 |
| [^112] | [Majority Rule: better patching via Self-Consistency.](http://arxiv.org/abs/2306.00108) | 本文提出了一种无需解释的算法，基于自洽性和大型语言模型（LLMs）来进行软件补丁选择，从而解决当前缺乏解释的软件数据集的问题。 |
| [^113] | [InGram: Inductive Knowledge Graph Embedding via Relation Graphs.](http://arxiv.org/abs/2305.19987) | InGram是一种新的归纳式知识图谱补全方法，可以在推理时生成新关系和实体的嵌入，并使用注意力机制汇总邻居嵌入生成关系和实体嵌入。该方法在多个基准数据集上的性能优于现有的基准方法。 |
| [^114] | [Representation Learning on Hyper-Relational and Numeric Knowledge Graphs with Transformers.](http://arxiv.org/abs/2305.18256) | 本文提出了一个名为HyNT的框架，用于学习超关系型知识图的表示，包括数值文字。该框架使用上下文Transformer和预测Transformer，通过学习三元组和其限定词之间的相关性以及数值信息来获得模型。 |
| [^115] | [Drug Repurposing Targeting COVID-19 3CL Protease using Molecular Docking and Machine Learning Regression Approach.](http://arxiv.org/abs/2305.18088) | 本研究利用分子对接和机器学习回归方法，筛选出针对 SARS-CoV-2的主要蛋白酶3CL潜在治疗药物。其中，决策树回归（DTR）模型具有改进的统计措施R2和RMSE，有助于识别具有高结合亲和力和有利的结合能的药物。 |
| [^116] | [Improved Privacy-Preserving PCA Using Space-optimized Homomorphic Matrix Multiplication.](http://arxiv.org/abs/2305.17341) | 本文提出一种使用近似数值算术同态加密方案进行隐私保护PCA的新方法，相对以往方法，其在效率、准确性和可扩展性上均有提升，实现了同态矩阵乘法和高效同态电路，计算特征值和特征向量时具有良好的效果。 |
| [^117] | [NASimEmu: Network Attack Simulator & Emulator for Training Agents Generalizing to Novel Scenarios.](http://arxiv.org/abs/2305.17246) | 该论文提出了一个名为NASimEmu的新框架，旨在提高智能体在现实世界中表现良好的能力。该框架使用模拟器和仿真器的结合，使智能体能够在模拟中进行训练，并在仿真器中部署，从而验证所使用的抽象的真实性。该框架的设计旨在培训通用的智能体，能够在训练期间未见过的新场景中进行转移。 |
| [^118] | [Reconstruction, forecasting, and stability of chaotic dynamics from partial data.](http://arxiv.org/abs/2305.15111) | 该论文提出了使用具有长短期记忆（LSTM）网络的数据驱动方法，从部分观测中重构出混沌系统的动力学，进行时间预测，并推断其稳定性。 |
| [^119] | [MedLens: Improve mortality prediction via medical signs selecting and regression interpolation.](http://arxiv.org/abs/2305.11742) | 本文介绍了一个自动选择医学体征和回归插值的方法（MedLens），用于解决电子病历中医学体征数据缺失率过高导致预测性能降低的问题。 |
| [^120] | [On the Limitations of Model Stealing with Uncertainty Quantification Models.](http://arxiv.org/abs/2305.05293) | 本文研究了使用不确定性量化模型进行模型盗窃的局限性，发现在实际盗窃过程中相互不确定是不可避免的。作者尝试使用多个可能的网络并将它们的预测组合以提高质量，但结果表明只有微弱的改善。作者发现网络多样性不足是导致这一结果的原因之一。 |
| [^121] | [DietCNN: Multiplication-free Inference for Quantized CNNs.](http://arxiv.org/abs/2305.05274) | 本文提出了一种用查表法代替CNN中乘法的新方法，其保留了主要CNN操作的语义，且可在FPGA实现中实现显著的能量降低，具有重要的实用价值。 |
| [^122] | [Learning Decision Trees with Gradient Descent.](http://arxiv.org/abs/2305.03515) | 本文提出了一种使用梯度下降学习决策树的新方法，可以联合优化所有树的参数，从而避免了贪心算法造成次优解的问题。该方法在二分类任务上表现优异，并在多类任务中达到有竞争力的结果。 |
| [^123] | [NNSplitter: An Active Defense Solution to DNN Model via Automated Weight Obfuscation.](http://arxiv.org/abs/2305.00097) | NNSplitter是一种主动保护深度神经网络模型知识产权的方案，通过将模型分为混淆模型和模型秘密两部分，采用可信执行环境和基于强化学习的控制器来最大化精度下降和减少混淆权重的数量。 |
| [^124] | [Online Platt Scaling with Calibeating.](http://arxiv.org/abs/2305.00070) | 本文提出了一种在线Platt缩放及其校准方法，其理论基础强大，可以处理分布漂移和对抗性结果序列，无需超参数调整，在一系列合成和真实数据集上表现出卓越的性能。 |
| [^125] | [From Chaos Comes Order: Ordering Event Representations for Object Detection.](http://arxiv.org/abs/2304.13455) | 本文提出了一种基于Gromov-Wasserstein Discrepancy选择最佳事件表示的方法，这种方法可以在多个表示、网络骨干和数据集上保持任务性能排名的一致性。利用这一方法，本文对大型事件表示法家族进行超参数搜索，选择最适合物体检测的表示法，取得了优于最先进的基于事件的对象检测方法的成果。 |
| [^126] | [End-to-End Feasible Optimization Proxies for Large-Scale Economic Dispatch.](http://arxiv.org/abs/2304.11726) | 本文提出了一种新颖的端到端学习和修复架构 (E2ELR) 用于经济分派问题的优化代理， 结果表明，E2ELR在大规模情况下取得了最先进的性能，优于其他基线至少一个数量级。 |
| [^127] | [Black Box Few-Shot Adaptation for Vision-Language models.](http://arxiv.org/abs/2304.01752) | 本文提出了一种黑匣子方法，实现了对预先计算的图像和文本特征的视觉-语言模型的快速少样本适应，适用于有监督和无监督训练，并且可以用于对单模型计算的图像和文本特征进行对齐。 |
| [^128] | [DeepAccident: A Motion and Accident Prediction Benchmark for V2X Autonomous Driving.](http://arxiv.org/abs/2304.01168) | 本文提出了一个大规模的 DeepAccident 数据集，其中包含各种真实世界驾驶中发生的事故场景，并提出了一个端到端的运动和事故预测任务，该任务可用于直接评估自动驾驶算法的事故预测能力。 |
| [^129] | [Using AI to Measure Parkinson's Disease Severity at Home.](http://arxiv.org/abs/2303.17573) | 该论文提出了一种使用人工智能系统远程评估帕金森病患者运动表现的方法，该方法可重复用于类似的运动任务，拥有较高的可靠性和准确性。 |
| [^130] | [GRAF: Graph Attention-aware Fusion Networks.](http://arxiv.org/abs/2303.16781) | 本文提出了一个名为GRAF的方法，使用注意机制和网络融合在多个网络上使用基于GNN的方法。通过attention-based neighborhood aggregation，GRAF能够学习每个节点和关联的重要性，并根据它们在网络融合中进行边缘加权。 |
| [^131] | [Targeted Adversarial Attacks on Wind Power Forecasts.](http://arxiv.org/abs/2303.16633) | 本研究检测了两个深度学习预测模型的脆弱性，并提出了一种新的评估指标。这些模型可以被针对性攻击，因此保护风电功率预测结果对现代电力系统的稳定性至关重要。 |
| [^132] | [A Survey on Malware Detection with Graph Representation Learning.](http://arxiv.org/abs/2303.16004) | 本综述对基于图表示学习的恶意软件检测进行了深入审查和总结，这是一种比传统方法更加健壮的解决方案。 |
| [^133] | [Bluetooth and WiFi Dataset for Real World RF Fingerprinting of Commercial Devices.](http://arxiv.org/abs/2303.13538) | 该论文捕获了首个公开可访问的商用设备真实射频指纹数据集，这对于识别非法或未授权发射器具有重要意义。 |
| [^134] | [Robust Evaluation of Diffusion-Based Adversarial Purification.](http://arxiv.org/abs/2303.09051) | 本文分析了对基于扩散式净化方法的评估方式，并提出了一个新的指导方针，以衡量净化方法对抗性攻击的鲁棒性。同时，我们提出了一种新的净化策略，展示了与最先进的对抗性训练方法相竞争的结果。 |
| [^135] | [Window-Based Early-Exit Cascades for Uncertainty Estimation: When Deep Ensembles are More Efficient than Single Models.](http://arxiv.org/abs/2303.08010) | 本文研究了基于窗口的早期退出集成方法，以在保持模型可扩展性的同时实现不确定性估计任务的高效实现。实验结果表明，该方法在准确性和计算效率上都达到了最新的研究成果。 |
| [^136] | [Differential Good Arm Identification.](http://arxiv.org/abs/2303.07154) | 本文提出了DGAI算法，它可以在好手臂识别问题中通过深度学习的方式减少样本复杂性，并且在具有给定阈值的情况下进一步提高多臂赌博问题的性能。 |
| [^137] | [TARGET: Federated Class-Continual Learning via Exemplar-Free Distillation.](http://arxiv.org/abs/2303.06937) | 本文研究了一个重要但鲜为人知的问题：联邦类式持续学习，在联邦学习中动态添加新的类别。我们提出了一种称为TARGET的新颖方法，通过无样本蒸馏来减轻FCCL中的灾难性遗忘问题，并保护客户数据的隐私。该方法利用先前训练的全局模型在模型层面上传递旧任务的知识，并通过生成器生成合成数据来模拟数据的全局分布。与先前的FCCL方法相比，TARGET无需额外的数据集或存储先前任务的私有数据。 |
| [^138] | [Overcoming Bias in Pretrained Models by Manipulating the Finetuning Dataset.](http://arxiv.org/abs/2303.06167) | 本文研究了预训练模型中的偏见问题，发现微调模型可以继承预训练模型的偏见，但通过对微调数据集进行干预可以纠正这种偏见，而且对性能的影响很小。这表明仔细策划微调数据集对于减少下游任务中的偏见非常重要，这样做甚至可以弥补预训练模型中的偏见。 |
| [^139] | [Controlling Class Layout for Deep Ordinal Classification via Constrained Proxies Learning.](http://arxiv.org/abs/2303.00396) | 本文提出了一种通过受限代理学习方法，可以有效地控制深度序数分类中的类布局。 |
| [^140] | [Single-Cell Multimodal Prediction via Transformers.](http://arxiv.org/abs/2303.00233) | 本研究研究了如何利用Transformer模型在端到端的方式上处理多模态单细胞数据，并利用下游任务信息，提出了一个名为scMoFormer的框架 |
| [^141] | [Optimal Prediction Using Expert Advice and Randomized Littlestone Dimension.](http://arxiv.org/abs/2302.13849) | 本研究证明了在线学习中，在学习一个类别时，最优期望错误边界等于其随机化的Littlestone维度。在不可知的情况下，最优错误边界与最佳函数的错误次数之间存在特定关系。此外，该研究还解决了一个开放问题，并将其应用于预测问题。 |
| [^142] | [Denoising Diffusion Samplers.](http://arxiv.org/abs/2302.13834) | 去噪扩散采样器 (DDS) 近似地从非标准化概率密度函数中采样，并估计其标准化常数。 |
| [^143] | [Change is Hard: A Closer Look at Subpopulation Shift.](http://arxiv.org/abs/2302.12254) | 本文分析了子群体转变的各种机制，对20个最先进的算法在12个领域内进行了全面的基准测试，发现现有算法只能应对某些转变，进一步地，提出一种简单易行的选择标准来改善现有算法性能。 |
| [^144] | [Unsupervised Out-of-Distribution Detection with Diffusion Inpainting.](http://arxiv.org/abs/2302.10326) | 本文提出了一种新的无监督的域外检测方法LMD，通过利用扩散模型，将图像从原始流形抬升并映射到域内流形，从而识别出域外图像。 |
| [^145] | [Dealing With Non-stationarity in Decentralized Cooperative Multi-Agent Deep Reinforcement Learning via Multi-Timescale Learning.](http://arxiv.org/abs/2302.02792) | 本文提出了一种多时间尺度学习的方法来解决分布式合作多智能体深度强化学习中的非平稳性问题，通过顺序学习的方式更新智能体的策略可以保证收敛性。 |
| [^146] | [Revisiting mass-radius relationships for exoplanet populations: a machine learning insight.](http://arxiv.org/abs/2301.07143) | 这项研究利用机器学习方法分析了762个已确认的外行星和八个太阳系行星的数据集，发现巨大行星具有较低的密度，主要由氢和氦构成，而小行星更密集，主要由更重的元素构成。研究还揭示了质量、轨道周期和恒星金属丰度对外行星半径的预测能力的重要性。 |
| [^147] | [Enhancement attacks in biomedical machine learning.](http://arxiv.org/abs/2301.01885) | 本研究针对生物医学机器学习中的可信度问题，通过开发增强攻击技术，成功地能够通过最小的特征改变显著提高分类器的预测性能，并保持原始数据和增强数据之间的高特征相似性。 |
| [^148] | [CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection.](http://arxiv.org/abs/2301.00785) | 本文提出了基于CLIP的通用模型，通过文本嵌入学习解剖学关系，能够分割25种器官和6种肿瘤，具有强大的泛化能力。 |
| [^149] | [REAP: A Large-Scale Realistic Adversarial Patch Benchmark.](http://arxiv.org/abs/2212.05680) | 本文提出了一个名为REAP的大规模真实对抗贴纸基准测试，该基准测试允许用户在真实图像和真实环境条件下评估对抗贴纸攻击，为解决依赖摄像头的物理系统面临的严重威胁提供了有效的工具。 |
| [^150] | [Multi-Rate VAE: Train Once, Get the Full Rate-Distortion Curve.](http://arxiv.org/abs/2212.03905) | 本文介绍了一种名为多速率VAE（MR-VAE）的框架，可在单次训练中学习与不同β对应的最优参数，通过使用超网络将β映射到最优参数，以实现率失真曲线的完整训练。 |
| [^151] | [{\mu}Split: efficient image decomposition for microscopy data.](http://arxiv.org/abs/2211.12872) | uSplit是一种适用于荧光显微镜图像的高效图像分解方法，集成了横向上下文化，帮助训练更深的分层模型，并有效地减少平铺伪影问题。 |
| [^152] | [Parametric Classification for Generalized Category Discovery: A Baseline Study.](http://arxiv.org/abs/2211.11727) | 该研究提出了一种简单而有效的参数化分类方法，该方法可以受益于熵正则化，在多个广义类别发现基准测试中实现最先进的性能，并对未知类别数量具有强大的稳健性。 |
| [^153] | [CRONOS: Colorization and Contrastive Learning for Device-Free NLoS Human Presence Detection using Wi-Fi CSI.](http://arxiv.org/abs/2211.10354) | 本文介绍了一种名为CRONOS的系统，可以通过彩色化和对比学习来基于Wi-Fi CSI实现无人设备NLoS人体检测，可以区分房间中的移动人员和空置。实验结果表明该系统在NLoS条件下能够准确地检测出房间中的人物存在。 |
| [^154] | [Secure and Privacy-Preserving Automated Machine Learning Operations into End-to-End Integrated IoT-Edge-Artificial Intelligence-Blockchain Monitoring System for Diabetes Mellitus Prediction.](http://arxiv.org/abs/2211.07643) | 本文提出了一种基于区块链的物联网边缘人工智能系统，用于通过危险因素预测糖尿病，以确保用户数据的安全性和隐私性，并进行了比较分析不同医疗传感器、设备和方法的效果。 |
| [^155] | [Efficient Utilization of Large Pre-Trained Models for Low Resource ASR.](http://arxiv.org/abs/2210.15445) | 本研究探讨了在低资源语音识别中如何高效利用大型预训练模型，通过无监督技术和改进的架构和训练方法取得了显著的性能提升。 |
| [^156] | [Neural Graphical Models.](http://arxiv.org/abs/2210.00453) | 本文介绍了神经图模型（NGMs），它可以以合理的计算成本表示复杂的特征依赖关系，适应多种图结构和混合输入数据类型，并提供了高效的学习、推断和采样算法。 |
| [^157] | [Segmenting Known Objects and Unseen Unknowns without Prior Knowledge.](http://arxiv.org/abs/2209.05407) | 本研究提出一种不需要先验知识的整体分割方法，可以将已知的物体和未知的物体进行准确分割，并在处理已知类别时对未知类别进行鲁棒处理。 |
| [^158] | [The Geometry and Calculus of Losses.](http://arxiv.org/abs/2209.00238) | 本文从凸集的新角度系统地发展了损失函数理论，引入了一种自动合适的损失函数定义方法，并提供了损失和范数之间关系的新机会，以及凸集微积分下的损失插值方法。 |
| [^159] | [GHN-Q: Parameter Prediction for Unseen Quantized Convolutional Architectures via Graph Hypernetworks.](http://arxiv.org/abs/2208.12489) | 本论文提出了一种名为GHN-Q的方法，通过图形超网络来预测未见量化卷积架构的参数，以提高量化鲁棒性。 |
| [^160] | [Why do networks have inhibitory/negative connections?.](http://arxiv.org/abs/2208.03211) | 神经网络具有抑制性/负向连接是为了学习更多的功能，负权重在表征能力中起着至关重要的作用，并且非负深度网络无法表示某些表征空间的几何特性。 |
| [^161] | [Approximate Real Symmetric Tensor Rank.](http://arxiv.org/abs/2207.12529) | 我们研究了对称张量分解中的扰动容忍度对结果的影响，并提出了一种计算对称张量秩的算法。 |
| [^162] | [Holistic Robust Data-Driven Decisions.](http://arxiv.org/abs/2207.09560) | 这篇论文提出了一种全面稳健的数据驱动公式，能够同时保护三个过拟合的源头：有限样本数据的统计误差、数据点的有限精度测量引起的数据噪声，以及被破坏的部分数据。 |
| [^163] | [Markovian Gaussian Process Variational Autoencoders.](http://arxiv.org/abs/2207.05543) | 本文提出了一种马尔科夫高斯过程变分自编码器（MGPVAE）模型，通过利用马尔科夫高斯过程的等效离散状态空间表示，并使用卡尔曼滤波和平滑技术实现了线性时间的GPVAE训练。在各种高维时间和时空任务中，该方法表现优异。 |
| [^164] | [Learning the Solution Operator of Boundary Value Problems using Graph Neural Networks.](http://arxiv.org/abs/2206.14092) | 本研究使用图神经网络和谱图卷积，设计了用于两种不同的时间独立PDE的通用解算器。通过在各种形状和不均匀性的情况下训练网络，我们发现在训练集中包含有限元网格的各种变化的数据是实现良好泛化结果的关键因素。这表明GNNs可以有效解决PDE问题并具有良好的泛化能力。 |
| [^165] | [Principal Trade-off Analysis.](http://arxiv.org/abs/2206.07520) | 本文介绍了一种名为"主要权衡分析"（PTA）的方法，将游戏嵌入低维特征空间，并展示了其在不同游戏中的有效性和洞察力。 |
| [^166] | [High-dimensional limit theorems for SGD: Effective dynamics and critical scaling.](http://arxiv.org/abs/2206.04030) | 本文研究了高维极限下具有恒定步长的随机梯度下降算法（SGD）的可扩展极限，我们证明了当维度趋于无穷时，SGD的轨迹的极限定理。我们的方法允许选择要跟踪的总结统计量、初始化和步长，并且得到了球形（ODE）和扩散（SDE）极限。我们还展示了步长的临界尺度，这个尺度下，有效的球形动力学与梯度流相匹配，但是出现了一个新的修正项，改变了相图。这个有效动力学的不动点对应的扩散极限可能非常复杂。 |
| [^167] | [Learning logic programs by combining programs.](http://arxiv.org/abs/2206.01614) | 本论文介绍了一种通过组合小型非可分离程序的方法来学习逻辑程序。实验结果表明，这种方法在预测准确性和学习时间方面显著优于现有方法。 |
| [^168] | [RoCourseNet: Distributionally Robust Training of a Prediction Aware Recourse Model.](http://arxiv.org/abs/2206.00700) | RoCourseNet提出了一个分布鲁棒的训练框架，用于同时优化预测和回应，以解决因为训练数据分布偏移造成的回应无效问题。 |
| [^169] | [Good Intentions: Adaptive Parameter Management via Intent Signaling.](http://arxiv.org/abs/2206.00470) | 本研究提出了一种善意的举措，通过意图信号机制实现自适应参数管理。该方法可以避免手动集成和昂贵的调试过程，提高分布式训练效率。 |
| [^170] | [Parallel and Distributed Graph Neural Networks: An In-Depth Concurrency Analysis.](http://arxiv.org/abs/2205.09702) | 这项研究深入分析了并行和分布式图神经网络，在设计了并行性分类方法后，研究了各种GNN模型、任务、软件框架和硬件加速器中的并行性，并重点关注相关张量的稀疏性/密度。 |
| [^171] | [Experimental Design for Causal Effect Identification.](http://arxiv.org/abs/2205.02232) | 本文研究了设计最小成本的干预集合来识别因果效应的问题，证明了问题的NP-hard性质，并提出了可以找到最优解或近似解的算法，使用多项式时间启发式算法来解决计算复杂性，并在随机图上进行了模拟实验验证算法的效果。 |
| [^172] | [MotionAug: Augmentation with Physical Correction for Human Motion Prediction.](http://arxiv.org/abs/2203.09116) | 本文提出了一种利用物理纠正的运动数据增强方案，通过生成多样性的运动和纠正不真实伪影来提高人体运动预测的效果。 |
| [^173] | [Reactive Motion Generation on Learned Riemannian Manifolds.](http://arxiv.org/abs/2203.07761) | 本文从黎曼流形的角度研究了机器人运动学习，通过学习黎曼度量和使用测地线生成的运动能适应新的环境条件，并通过调整学习到的流形实现避障功能。 |
| [^174] | [Cross-model Fairness: Empirical Study of Fairness and Ethics Under Model Multiplicity.](http://arxiv.org/abs/2203.07139) | 本文提出了一种跨模型公平性的新定义，并进行了实证研究。该研究关注数据驱动预测模型在社会背景下的公平性问题，特别是在通过选择不同预测器进行模型多样性时可能导致个人受伤害的情况。 |
| [^175] | [Generalized Bandit Regret Minimizer Framework in Imperfect Information Extensive-Form Game.](http://arxiv.org/abs/2203.05920) | 本论文提出了一个广义框架，用于在不完全信息广义展开形式博弈中学习近似纳什平衡解。通过该框架，我们设计了一种新方法SIX-OMD来最小化遗憾并改善收敛率。 |
| [^176] | [Robust Multi-Task Learning and Online Refinement for Spacecraft Pose Estimation across Domain Gap.](http://arxiv.org/abs/2203.04275) | 本文介绍了一种跨领域差异的非合作航天器姿态估计的多尺度、多任务CNN，通过在合成图像上进行数据增强训练共享编码器以学习通用特征。同时介绍了一种在线域优化方法，用于调整模型的标准化层参数。 |
| [^177] | [Degree-Preserving Randomized Response for Graph Neural Networks under Local Differential Privacy.](http://arxiv.org/abs/2202.10209) | 本文提出了一种保持度数不变的随机化响应算法，用于在无属性图中提供高准确性的边的局部差分隐私保护。通过使用Warner的随机化响应和策略性边采样，我们的算法能够保护用户的隐私同时保留图结构。 |
| [^178] | [Differentially Private Community Detection for Stochastic Block Models.](http://arxiv.org/abs/2202.00636) | 本文研究了在差分隐私的情况下，基于随机块模型进行社区检测的问题，通过$(\epsilon, \delta)$-边差分隐私的概念，探讨了$(p, q)$、DP预算$(\epsilon, \delta)$和社区标签的准确恢复之间的基本权衡。 |
| [^179] | [On the Power of Gradual Network Alignment Using Dual-Perception Similarities.](http://arxiv.org/abs/2201.10945) | 本研究提出了Grad-Align，一种渐进网络对齐方法，通过利用强一致性节点对逐步发现节点对。该方法首先生成节点嵌入，然后计算双感知相似性度量逐步对齐节点。 |
| [^180] | [Minimax risk classifiers with 0-1 loss.](http://arxiv.org/abs/2201.06487) | 本论文介绍了最小化最大化风险分类器（MRCs），旨在通过最小化与可能包含基础分布的分布的不确定性集合相对应的最坏情况下的0-1损失来提供严格的性能保证。使用特征映射和特征核，MRCs在学习时具有强度普遍一致性，并且提供了高效的优化技术和准确的分类能力。 |
| [^181] | [A Tractable Online Learning Algorithm for the Multinomial Logit Contextual Bandit.](http://arxiv.org/abs/2011.14033) | 本文提供了一个新颖的、不需要调整指数参数的MNL-Contextual Bandit问题的简便在线学习算法。算法具有与该问题的最佳理论界限匹配的遗憾上界。 |
| [^182] | [Local Function Complexity for Active Learning via Mixture of Gaussian Processes.](http://arxiv.org/abs/1902.10664) | 本文通过利用局部函数复杂性（LFC）的估计，建立了一个局部结构复杂性的概念，并将其用于发展一个与模型无关的主动学习框架。通过使用基于高斯过程回归（GPR）的局部多项式平滑（LPS）模型的类比，使得该框架具有鲁棒性和可伸缩性。 |

# 详细

[^1]: 以多工作流可信度和数据可观测性为基础的轻量级数据集成

    Towards Lightweight Data Integration using Multi-workflow Provenance and Data Observability. (arXiv:2308.09004v1 [cs.DC])

    [http://arxiv.org/abs/2308.09004](http://arxiv.org/abs/2308.09004)

    本论文提出MIDA方法，以多工作流可信度和数据可观测性为基础，实现了轻量级的运行时集成数据分析。该方法通过定义数据可观测策略和适应性方法，可以处理异构科学环境下多个支持工具和高效的HPC执行。

    

    现代大规模科学发现需要跨多个计算设施进行多学科合作，包括高性能计算（HPC）机器和边缘到云的连续体。集成的数据分析在科学发现中起着至关重要的作用，特别是在当前人工智能时代，通过支持负责任的AI开发、FAIR、可复现性和用户操控。然而，科学的异构性带来了挑战，如处理多个支持工具、跨设施环境和高效的HPC执行。在数据可观测性、适配器系统设计和可信度的基础上，我们提出了MIDA：一种轻量级运行时多工作流集成数据分析的方法。MIDA定义了各种并行系统和机器学习工具的数据可观测策略和适应性方法。通过可观测性，它在后台拦截数据流，无需仪器化，同时集成领域、可信度和遥测数据。

    Modern large-scale scientific discovery requires multidisciplinary collaboration across diverse computing facilities, including High Performance Computing (HPC) machines and the Edge-to-Cloud continuum. Integrated data analysis plays a crucial role in scientific discovery, especially in the current AI era, by enabling Responsible AI development, FAIR, Reproducibility, and User Steering. However, the heterogeneous nature of science poses challenges such as dealing with multiple supporting tools, cross-facility environments, and efficient HPC execution. Building on data observability, adapter system design, and provenance, we propose MIDA: an approach for lightweight runtime Multi-workflow Integrated Data Analysis. MIDA defines data observability strategies and adaptability methods for various parallel systems and machine learning tools. With observability, it intercepts the dataflows in the background without requiring instrumentation while integrating domain, provenance, and telemetry 
    
[^2]: DealMVC: 面向多视角聚类的双向对比校准

    DealMVC: Dual Contrastive Calibration for Multi-view Clustering. (arXiv:2308.09000v1 [cs.CV])

    [http://arxiv.org/abs/2308.09000](http://arxiv.org/abs/2308.09000)

    提出了一种名为DealMVC的双向对比校准网络，用于解决多视图聚类中的一致性问题，并利用全局和局部对比校准损失来提高聚类性能。

    

    多视角对比聚类由于其强大的视角一致性信息挖掘能力而受到了广泛关注。然而，我们观察到一个限制聚类性能进一步提升的问题。现有的多视图模型主要关注不同视图中相同样本的一致性，而忽视了交叉视图场景中类似但不同的样本情况。为了解决这个问题，我们提出了一种新颖的面向多视图聚类的双向对比校准网络（DealMVC）。具体而言，我们首先设计了一个融合机制，获得全局的跨视图特征。然后，通过对齐视图特征相似性图和高置信度伪标签图，提出了一个全局对比校准损失。此外，为了利用多视图信息的多样性，我们提出了一个局部对比校准损失，用于约束成对视图特征的一致性。特征结构是 ...

    Benefiting from the strong view-consistent information mining capacity, multi-view contrastive clustering has attracted plenty of attention in recent years. However, we observe the following drawback, which limits the clustering performance from further improvement. The existing multi-view models mainly focus on the consistency of the same samples in different views while ignoring the circumstance of similar but different samples in cross-view scenarios. To solve this problem, we propose a novel Dual contrastive calibration network for Multi-View Clustering (DealMVC). Specifically, we first design a fusion mechanism to obtain a global cross-view feature. Then, a global contrastive calibration loss is proposed by aligning the view feature similarity graph and the high-confidence pseudo-label graph. Moreover, to utilize the diversity of multi-view information, we propose a local contrastive calibration loss to constrain the consistency of pair-wise view features. The feature structure is
    
[^3]: 自学习增强 (ReST) 用于语言模型的强化学习

    Reinforced Self-Training (ReST) for Language Modeling. (arXiv:2308.08998v1 [cs.CL])

    [http://arxiv.org/abs/2308.08998](http://arxiv.org/abs/2308.08998)

    本文提出了一种称为自学习增强 (ReST) 的算法，通过从人类反馈中进行强化学习来提高大型语言模型 (LLM) 的输出质量。在机器翻译任务上的实验结果表明，ReST能够以高效的方式显著提高翻译质量。

    

    通过从人类反馈中进行强化学习 (RLHF)，可以通过与人类偏好对齐来提高大型语言模型 (LLM) 的输出质量。我们提出了一种简单的算法，通过增长批量强化学习 (RL) 来与人类偏好对齐 LLM，我们称之为增强自学习 (ReST)。给定初始的LLM策略，ReST通过从策略中生成样本来产生一个数据集，然后使用离线强化学习算法改进LLM策略。ReST比典型的在线RLHF方法更高效，因为训练数据集是离线生成的，可以重复使用数据。虽然ReST是适用于所有生成学习设置的通用方法，但我们将重点放在其在机器翻译中的应用上。我们的结果表明，ReST可以以计算和采样高效的方式显著提高翻译质量，通过自动化指标和人工评估在机器翻译基准上测量。

    Reinforcement learning from human feedback (RLHF) can improve the quality of large language model's (LLM) outputs by aligning them with human preferences. We propose a simple algorithm for aligning LLMs with human preferences inspired by growing batch reinforcement learning (RL), which we call Reinforced Self-Training (ReST). Given an initial LLM policy, ReST produces a dataset by generating samples from the policy, which are then used to improve the LLM policy using offline RL algorithms. ReST is more efficient than typical online RLHF methods because the training dataset is produced offline, which allows data reuse. While ReST is a general approach applicable to all generative learning settings, we focus on its application to machine translation. Our results show that ReST can substantially improve translation quality, as measured by automated metrics and human evaluation on machine translation benchmarks in a compute and sample-efficient manner.
    
[^4]: 神经振荡器用于物理信息机器学习的泛化能力提升

    Neural oscillators for generalization of physics-informed machine learning. (arXiv:2308.08989v1 [cs.LG])

    [http://arxiv.org/abs/2308.08989](http://arxiv.org/abs/2308.08989)

    本文提出了一种称为神经振荡器的方法，通过与物理信息机器学习模型的融合，利用偏微分方程解的因果关系和时间特性来提高其泛化能力。实验证明了这种方法在处理复杂物理问题时的有效性。

    

    物理信息机器学习（PIML）的一个主要挑战是其在训练域之外的泛化能力，特别是在处理由偏微分方程（PDEs）表示的复杂物理问题时。本文旨在增强PIML的泛化能力，促进在未探索区域进行准确预测的实际应用。我们利用PDE解的固有因果关系和时间顺序特性，将基于常微分方程组的循环神经架构与PIML模型融合，称之为神经振荡器。通过有效地捕捉长时间依赖关系和缓解梯度爆炸和梯度消失问题，神经振荡器促进了PIML任务中的改进泛化能力。通过涉及时间依赖的非线性PDE和双调和梁方程的广泛实验，证明了所提出方法的有效性。

    A primary challenge of physics-informed machine learning (PIML) is its generalization beyond the training domain, especially when dealing with complex physical problems represented by partial differential equations (PDEs). This paper aims to enhance the generalization capabilities of PIML, facilitating practical, real-world applications where accurate predictions in unexplored regions are crucial. We leverage the inherent causality and temporal sequential characteristics of PDE solutions to fuse PIML models with recurrent neural architectures based on systems of ordinary differential equations, referred to as neural oscillators. Through effectively capturing long-time dependencies and mitigating the exploding and vanishing gradient problem, neural oscillators foster improved generalization in PIML tasks. Extensive experimentation involving time-dependent nonlinear PDEs and biharmonic beam equations demonstrates the efficacy of the proposed approach. Incorporating neural oscillators out
    
[^5]: 量化生物混合系统中的仿生差距

    Quantifying the biomimicry gap in biohybrid systems. (arXiv:2308.08978v1 [cs.RO])

    [http://arxiv.org/abs/2308.08978](http://arxiv.org/abs/2308.08978)

    这项工作中，研究人员针对生物混合系统中的仿生差距进行了量化分析。通过使用真实鱼类和仿生诱饵进行实验，以及对鱼类对的模拟，研究人员证明了他们的系统可以生成与真实鱼类完全相似的社会互动。

    

    与动物相互作用的仿生混合系统已成为探索和识别群体动物行为机制的有力工具。一个关键挑战在于将社会互动模型从模拟转移到现实，利用机器人验证建模假设。这一挑战源于所谓的“仿生差距”的跨越，该差距使用不完美的机器人复制品、通信线索和物理约束，这些因素未被纳入模拟中，可能在动物身上引发不真实的行为反应。在这项工作中，我们使用了红嘴无须魮鱼（Hemigrammus rhodostomus）的仿生诱饵和一个神经网络（NN）模型来生成仿生社会互动。通过对一个由鱼和机器人诱饵组成的生物混合体、一对真实鱼和一对鱼的模拟的实验，我们证明了我们的生物混合系统产生了与真实鱼类完全相似的仿真社会互动。

    Biohybrid systems in which robotic lures interact with animals have become compelling tools for probing and identifying the mechanisms underlying collective animal behavior. One key challenge lies in the transfer of social interaction models from simulations to reality, using robotics to validate the modeling hypotheses. This challenge arises in bridging what we term the "biomimicry gap", which is caused by imperfect robotic replicas, communication cues and physics constrains not incorporated in the simulations that may elicit unrealistic behavioral responses in animals. In this work, we used a biomimetic lure of a rummy-nose tetra fish (Hemigrammus rhodostomus) and a neural network (NN) model for generating biomimetic social interactions. Through experiments with a biohybrid pair comprising a fish and the robotic lure, a pair of real fish, and simulations of pairs of fish, we demonstrate that our biohybrid system generates high-fidelity social interactions mirroring those of genuine f
    
[^6]: 打破高维音符：关于广义线性模型和多索引模型上 SGD 学习动力学的 ODE 分析

    Hitting the High-Dimensional Notes: An ODE for SGD learning dynamics on GLMs and multi-index models. (arXiv:2308.08977v1 [math.OC])

    [http://arxiv.org/abs/2308.08977](http://arxiv.org/abs/2308.08977)

    本文研究了应用于广义线性模型和多索引模型中的流式随机梯度下降（SGD）的学习动力学。通过建立了一个普通微分方程系统来描述风险和次优性度量等统计量，获得了稳定性学习率阈值和收敛保证。同时，引入了一个简化扩散系数的随机微分方程模型，用于分析SGD迭代的统计动力学。通过标准示例和数值模拟，验证了该理论的有效性。

    

    本文分析了在应用于具有一般数据协方差的广义线性模型和多索引模型（例如逻辑回归、相位恢复）时，流式随机梯度下降（SGD）在高维限制下的动力学。具体而言，我们证明了 SGD 的确定性等效形式，即一组描述风险和其他次优性度量的普通微分方程系统。当模型参数数量与数据数量成正比增长时，该等效性以极大概率发生。该框架使我们能够获得 SGD 稳定性的学习率阈值以及收敛保证。除了确定性等效性外，我们引入了一个具有简化扩散系数的 SDE（均匀化 SGD），它使我们能够分析 SGD 迭代的常规统计动力学。最后，我们在一些标准示例上演示了该理论，并展示了数值模拟结果。

    We analyze the dynamics of streaming stochastic gradient descent (SGD) in the high-dimensional limit when applied to generalized linear models and multi-index models (e.g. logistic regression, phase retrieval) with general data-covariance. In particular, we demonstrate a deterministic equivalent of SGD in the form of a system of ordinary differential equations that describes a wide class of statistics, such as the risk and other measures of sub-optimality. This equivalence holds with overwhelming probability when the model parameter count grows proportionally to the number of data. This framework allows us to obtain learning rate thresholds for stability of SGD as well as convergence guarantees. In addition to the deterministic equivalent, we introduce an SDE with a simplified diffusion coefficient (homogenized SGD) which allows us to analyze the dynamics of general statistics of SGD iterates. Finally, we illustrate this theory on some standard examples and show numerical simulations w
    
[^7]: 一种双重视角评估特征归因方法的方法

    A Dual-Perspective Approach to Evaluating Feature Attribution Methods. (arXiv:2308.08949v1 [cs.LG])

    [http://arxiv.org/abs/2308.08949](http://arxiv.org/abs/2308.08949)

    这篇论文提出了一种双重视角的方法来评估特征归因方法。通过观察扰动归因特征对模型行为的影响，这种方法揭示了归因特征的准确性和完整性，使其能够定量评估特征归因的表现。

    

    特征归因方法试图通过识别相关特征来解释神经网络的预测。然而，建立一个评估特征归因的统一框架仍然是一个挑战。我们可以通过几个视角来评估特征归因。其中一个主要视角是观察扰动归因特征对模型行为的影响（即忠实度）。尽管提供了有用的洞见，但现有的忠实度评估存在我们在本文中揭示的缺点。在这项工作中，我们提出了忠实度范式内的两个新视角，揭示了直观的属性：正确性和完整性。正确性评估归因特征真正是预测性特征的程度，而完整性检查所得归因如何很好地揭示所有预测性特征。这两个视角基于坚实的数学基础，并提供了通过高效算法计算的定量指标。

    Feature attribution methods attempt to explain neural network predictions by identifying relevant features. However, establishing a cohesive framework for assessing feature attribution remains a challenge. There are several views through which we can evaluate attributions. One principal lens is to observe the effect of perturbing attributed features on the model's behavior (i.e., faithfulness). While providing useful insights, existing faithfulness evaluations suffer from shortcomings that we reveal in this paper. In this work, we propose two new perspectives within the faithfulness paradigm that reveal intuitive properties: soundness and completeness. Soundness assesses the degree to which attributed features are truly predictive features, while completeness examines how well the resulting attribution reveals all the predictive features. The two perspectives are based on a firm mathematical foundation and provide quantitative metrics that are computable through efficient algorithms. W
    
[^8]: 用机器学习预测农作物产量：在田地和亚田级别上对输入模态和模型的广泛分析

    Predicting Crop Yield With Machine Learning: An Extensive Analysis Of Input Modalities And Models On a Field and sub-field Level. (arXiv:2308.08948v1 [cs.CV])

    [http://arxiv.org/abs/2308.08948](http://arxiv.org/abs/2308.08948)

    本论文介绍了一种用于农作物产量预测的简单而有效的早期融合方法，可以处理具有不同时间和空间分辨率的多个输入模态。该方法使用了高分辨率的农作物产量地图进行训练，并采用了农作物和机器学习模型无关的方法进行亚田级别的预测。该方法使用全球覆盖的输入模态，并强调了输入模态对于产量预测的重要性。

    

    我们引入了一种简单而有效的早期融合方法，用于处理具有不同时间和空间分辨率的多个输入模态的农作物产量预测。我们使用高分辨率的农作物产量地图作为训练数据，在亚田级别上使用农作物和机器学习模型无关的方法。我们使用Sentinel-2卫星图像作为主要的输入数据模态，以及其他补充的模态，包括天气、土壤和DEM数据。所提出的方法使用了具有全球覆盖范围的输入模态，使该框架具有全球伸缩性。我们明确强调了输入模态对于农作物产量预测的重要性，并强调了最佳组合的输入模态取决于地区、作物和选择的模型。

    We introduce a simple yet effective early fusion method for crop yield prediction that handles multiple input modalities with different temporal and spatial resolutions. We use high-resolution crop yield maps as ground truth data to train crop and machine learning model agnostic methods at the sub-field level. We use Sentinel-2 satellite imagery as the primary modality for input data with other complementary modalities, including weather, soil, and DEM data. The proposed method uses input modalities available with global coverage, making the framework globally scalable. We explicitly highlight the importance of input modalities for crop yield prediction and emphasize that the best-performing combination of input modalities depends on region, crop, and chosen model.
    
[^9]: 可解释的基于图神经网络的表格数据处理方法

    Interpretable Graph Neural Networks for Tabular Data. (arXiv:2308.08945v1 [cs.LG])

    [http://arxiv.org/abs/2308.08945](http://arxiv.org/abs/2308.08945)

    本论文提出了一种称为IGNNet的方法，可以在处理表格数据时使用图神经网络，该方法能够产生可解释的模型，从原始输入特征精确计算预测结果，并且在性能上与最先进的机器学习算法性能相当。

    

    在现实世界的应用中，表格格式的数据经常出现。图神经网络（GNNs）近期被扩展以有效处理此类数据，通过表示学习捕捉特征之间的相互作用。然而，这些方法本质上产生了黑盒模型，以深度神经网络的形式存在，使得用户无法理解模型预测的逻辑。我们提出了一种称为IGNNet（基于图神经网络的可解释表格数据处理方法）的方法，它限制学习算法以产生可解释的模型，该模型展示了如何从原始输入特征准确计算预测结果。通过大规模实证研究，我们展示了IGNNet与面向表格数据的最先进机器学习算法（包括XGBoost，Random Forests和TabNet）性能相当。同时，结果显示从IGNNet获得的解释与真实情况一致。

    Data in tabular format is frequently occurring in real-world applications. Graph Neural Networks (GNNs) have recently been extended to effectively handle such data, allowing feature interactions to be captured through representation learning. However, these approaches essentially produce black-box models, in the form of deep neural networks, precluding users from following the logic behind the model predictions. We propose an approach, called IGNNet (Interpretable Graph Neural Network for tabular data), which constrains the learning algorithm to produce an interpretable model, where the model shows how the predictions are exactly computed from the original input features. A large-scale empirical investigation is presented, showing that IGNNet is performing on par with state-of-the-art machine-learning algorithms that target tabular data, including XGBoost, Random Forests, and TabNet. At the same time, the results show that the explanations obtained from IGNNet are aligned with the true
    
[^10]: 在异构数据空间中实现个体公平性和鲁棒性的因果对抗扰动

    Causal Adversarial Perturbations for Individual Fairness and Robustness in Heterogeneous Data Spaces. (arXiv:2308.08938v1 [cs.LG])

    [http://arxiv.org/abs/2308.08938](http://arxiv.org/abs/2308.08938)

    本文提出了一种新颖的方法，通过在异构数据空间中引入因果对抗扰动并应用对抗训练，结合个体公平性、因果性和鲁棒性，以解决在处理离散敏感属性时的问题。实验证明了该方法的有效性。

    

    随着负责任的AI在机器学习算法中的重要性不断增加，公平性、对抗鲁棒性和因果性等属性近年来引起了广泛关注。然而，尽管它们各自具有重要意义，但同时探索和集成这些属性仍存在重大差距。在本文中，我们提出了一种新颖的方法，探讨了个体公平性、对抗鲁棒性和结构性因果模型之间的关系，特别是在处理离散敏感属性时。我们使用因果结构模型和敏感属性来创建公平度量，并将其应用于衡量个体之间的语义相似性。通过引入一种新颖的因果对抗扰动，并应用对抗训练，我们创建了一个将个体公平性、因果性和鲁棒性相结合的新的正则化器。我们的方法在实际和合成数据集上进行了评估，展示了其

    As responsible AI gains importance in machine learning algorithms, properties such as fairness, adversarial robustness, and causality have received considerable attention in recent years. However, despite their individual significance, there remains a critical gap in simultaneously exploring and integrating these properties. In this paper, we propose a novel approach that examines the relationship between individual fairness, adversarial robustness, and structural causal models in heterogeneous data spaces, particularly when dealing with discrete sensitive attributes. We use causal structural models and sensitive attributes to create a fair metric and apply it to measure semantic similarity among individuals. By introducing a novel causal adversarial perturbation and applying adversarial training, we create a new regularizer that combines individual fairness, causality, and robustness in the classifier. Our method is evaluated on both real-world and synthetic datasets, demonstrating it
    
[^11]: 用回归方法估算火灾持续时间

    Estimating fire Duration using regression methods. (arXiv:2308.08936v1 [cs.LG])

    [http://arxiv.org/abs/2308.08936](http://arxiv.org/abs/2308.08936)

    本文使用机器学习方法解决了野火持续时间预测的计算代价高、时间消耗大的问题，通过使用回归模型和图像处理技术，结合卫星地形要素地图和历史火灾数据，能够基于地貌图像快速且相对准确地预测火灾的燃烧持续时间。

    

    野火预测问题通常依赖于复杂的基于网格的数学模型，主要涉及计算流体力学（CFD）和细胞自动机，但这些方法一直以来都计算成本高昂，很难迅速作出决策。在本文中，我们提供了基于机器学习的方法，解决了高计算代价和时间消耗的问题。本文使用随机森林（RF）、K最近邻（KNN）、XGBoost回归模型以及卷积神经网络（CNN）和编码器等基于图像的方法来预测已知野火的燃烧持续时间。模型的输入基于卫星提供的地形要素地图和该地区对应的历史火灾数据。该模型通过处理输入以获得最佳结果，能够根据地貌图像进行快速且相对准确的未来预测。

    Wildfire forecasting problems usually rely on complex grid-based mathematical models, mostly involving Computational fluid dynamics(CFD) and Celluar Automata, but these methods have always been computationally expensive and difficult to deliver a fast decision pattern. In this paper, we provide machine learning based approaches that solve the problem of high computational effort and time consumption. This paper predicts the burning duration of a known wildfire by RF(random forest), KNN, and XGBoost regression models and also image-based, like CNN and Encoder. Model inputs are based on the map of landscape features provided by satellites and the corresponding historical fire data in this area. This model is trained by happened fire data and landform feature maps and tested with the most recent real value in the same area. By processing the input differently to obtain the optimal outcome, the system is able to make fast and relatively accurate future predictions based on landscape images
    
[^12]: 关于在预训练中的分子属性预测中的数据不平衡问题

    On Data Imbalance in Molecular Property Prediction with Pre-training. (arXiv:2308.08934v1 [cs.LG])

    [http://arxiv.org/abs/2308.08934](http://arxiv.org/abs/2308.08934)

    本研究探讨了在预训练的分子属性预测中的数据不平衡问题，并提出了一个结合理论计算和机器学习的方法来构建替代模型。同时，通过预训练技术提高了机器学习模型的准确性。

    

    揭示和分析材料的各种属性是材料发展中的一个重要问题，其中包括电池、半导体、催化剂和药物。传统上，这些属性是通过理论计算和模拟来确定的。然而，对每个候选材料执行这样的计算是不现实的。最近，出现了一种理论计算和机器学习相结合的方法，该方法使用在一小部分理论计算结果上训练机器学习模型来构建一个可以应用于其余材料的替代模型。另一方面，一种称为预训练的技术被用于提高机器学习模型的准确性。预训练是指在目标任务之前，先在一个不同的预训练任务上训练模型。这个过程旨在提取输入数据的特征，稳定学习的过程。

    Revealing and analyzing the various properties of materials is an essential and critical issue in the development of materials, including batteries, semiconductors, catalysts, and pharmaceuticals. Traditionally, these properties have been determined through theoretical calculations and simulations. However, it is not practical to perform such calculations on every single candidate material. Recently, a combination method of the theoretical calculation and machine learning has emerged, that involves training machine learning models on a subset of theoretical calculation results to construct a surrogate model that can be applied to the remaining materials. On the other hand, a technique called pre-training is used to improve the accuracy of machine learning models. Pre-training involves training the model on pretext task, which is different from the target task, before training the model on the target task. This process aims to extract the input data features, stabilizing the learning pr
    
[^13]: 基于预测表示学习的模仿增强学习方法在自动做市中的应用

    IMM: An Imitative Reinforcement Learning Approach with Predictive Representation Learning for Automatic Market Making. (arXiv:2308.08918v1 [cs.LG])

    [http://arxiv.org/abs/2308.08918](http://arxiv.org/abs/2308.08918)

    本研究提出了一种基于预测表示学习的模仿增强学习方法，在自动做市中应用。该方法通过借鉴专业人类做市商的工作流程，结合子优信号专家的知识和直接策略交互，开发了适用于多价格水平的做市策略。

    

    做市（MM）在金融交易中引起了广泛关注，因为它在确保市场流动性方面具有至关重要的功能。在顺序决策方面具有强大能力的强化学习（RL）技术在量化交易方面取得了显著的成功。然而，大多数现有的基于RL的MM方法都专注于优化单一价格水平策略，而对于频繁撤销订单和丢失队列优先级等问题无法解决。涉及多个价格水平的策略更符合实际交易场景。然而，由于多价格水平策略涉及到全面的交易行为空间的复杂性，有效训练盈利的RL代理人在MM方面仍然是一个挑战。受到专业做市商高效工作流程的启发，我们提出了一种新颖的RL框架，即模仿市场做市商（IMM），它利用子优信号专家的知识和直接策略交互的方式来开发多价格水平的MM策略。

    Market making (MM) has attracted significant attention in financial trading owing to its essential function in ensuring market liquidity. With strong capabilities in sequential decision-making, Reinforcement Learning (RL) technology has achieved remarkable success in quantitative trading. Nonetheless, most existing RL-based MM methods focus on optimizing single-price level strategies which fail at frequent order cancellations and loss of queue priority. Strategies involving multiple price levels align better with actual trading scenarios. However, given the complexity that multi-price level strategies involves a comprehensive trading action space, the challenge of effectively training profitable RL agents for MM persists. Inspired by the efficient workflow of professional human market makers, we propose Imitative Market Maker (IMM), a novel RL framework leveraging both knowledge from suboptimal signal-based experts and direct policy interactions to develop multi-price level MM strategi
    
[^14]: 超越共享：冲突感知的多变量时间序列异常检测

    Beyond Sharing: Conflict-Aware Multivariate Time Series Anomaly Detection. (arXiv:2308.08915v1 [cs.LG])

    [http://arxiv.org/abs/2308.08915](http://arxiv.org/abs/2308.08915)

    这篇论文提出了一种冲突感知的多变量时间序列异常检测算法，该算法通过为每个指标提供独特的结构来缓解指标回归目标之间的冲突。

    

    大量的关键绩效指标(KPI)以多变量时间序列数据(MTS)的形式进行监测，以确保软件应用程序和服务系统的可靠性。准确检测MTS的异常对于后续的故障排除非常关键。异常的稀缺性和手动标记导致了各种自监督的MTS异常检测方法的发展，这些方法优化了一个涵盖所有指标回归目标/损失的整体目标/损失。然而，我们的实证研究发现了指标回归目标之间冲突的普遍存在，导致MTS模型在不同的损失中挣扎。这一关键方面显著影响检测性能，但在现有方法中被忽视了。为了解决这个问题，通过模仿多门专家混合模型(MMoE)的设计，我们引入了CAD，一种冲突感知的多变量KPI异常检测算法。CAD为每个指标提供了一个独特的结构，以缓解冲突。

    Massive key performance indicators (KPIs) are monitored as multivariate time series data (MTS) to ensure the reliability of the software applications and service system. Accurately detecting the abnormality of MTS is very critical for subsequent fault elimination. The scarcity of anomalies and manual labeling has led to the development of various self-supervised MTS anomaly detection (AD) methods, which optimize an overall objective/loss encompassing all metrics' regression objectives/losses. However, our empirical study uncovers the prevalence of conflicts among metrics' regression objectives, causing MTS models to grapple with different losses. This critical aspect significantly impacts detection performance but has been overlooked in existing approaches. To address this problem, by mimicking the design of multi-gate mixture-of-experts (MMoE), we introduce CAD, a Conflict-aware multivariate KPI Anomaly Detection algorithm. CAD offers an exclusive structure for each metric to mitigate
    
[^15]: 为疼痛开发知识图谱嵌入模型

    Development of a Knowledge Graph Embeddings Model for Pain. (arXiv:2308.08904v1 [cs.LG])

    [http://arxiv.org/abs/2308.08904](http://arxiv.org/abs/2308.08904)

    该论文开发了一个用于疼痛的知识图谱嵌入模型，以便研究疼痛概念及其关系。知识图谱嵌入可以将大型图谱转化为低维向量，从而提高计算效率，并用于各种任务。

    

    疼痛是一个复杂的概念，可以与其他概念相互关联，比如可能引起疼痛的疾病，可能缓解疼痛的药物等等。为了完全理解个体或整个人群所经历的疼痛背景，我们需要研究与疼痛相关的所有概念及其之间的关系。当建模电子健康记录中记录的疼痛时，这尤为有用。知识图谱通过一个相互链接的网络来表示概念和它们的关系，以一种计算可处理的方式实现语义和基于上下文的推理。然而，这些图谱可能过大以至于计算效率低下。而知识图谱嵌入有助于解决这个问题，它将图谱表示为低维向量空间。然后，这些嵌入可以用于各种下游任务，如分类和链接预测。构建这样一个知识图谱所需的与疼痛相关的各种关系可以o

    Pain is a complex concept that can interconnect with other concepts such as a disorder that might cause pain, a medication that might relieve pain, and so on. To fully understand the context of pain experienced by either an individual or across a population, we may need to examine all concepts related to pain and the relationships between them. This is especially useful when modeling pain that has been recorded in electronic health records. Knowledge graphs represent concepts and their relations by an interlinked network, enabling semantic and context-based reasoning in a computationally tractable form. These graphs can, however, be too large for efficient computation. Knowledge graph embeddings help to resolve this by representing the graphs in a low-dimensional vector space. These embeddings can then be used in various downstream tasks such as classification and link prediction. The various relations associated with pain which are required to construct such a knowledge graph can be o
    
[^16]: 为U型并行分层学习进行最优资源分配

    Optimal Resource Allocation for U-Shaped Parallel Split Learning. (arXiv:2308.08896v1 [cs.LG])

    [http://arxiv.org/abs/2308.08896](http://arxiv.org/abs/2308.08896)

    本论文提出了一种新颖的并行U型分层学习方法，并设计了优化资源分配方案以提高边缘网络的性能。实验结果表明该方法可以达到与其他方法相似的性能。

    

    分层学习（SL）作为一种在不公开数据所有者的原始数据样本的情况下进行模型训练的有前景的方法而出现。然而，传统的SL不可避免地泄漏了标签隐私，因为尾部模型（具有最后几层）应该放在服务器上。为了克服这一限制，一种有前景的解决方法是利用U型架构将早期层和最后层都放在用户端。在本文中，我们开发了一种新颖的并行U型分层学习方法，并设计了优化资源分配方案以提高边缘网络的性能。在所提出的框架中，多个用户与边缘服务器进行SL通信。我们分析了训练过程中每个客户端的端到端延迟，并设计了一种高效的资源分配算法，称为LSCRA，它可以找到最佳的计算资源分配和分层。我们的实验结果表明了LSCRA的有效性，以及U型PSL可以达到与其他S方法类似的性能。

    Split learning (SL) has emerged as a promising approach for model training without revealing the raw data samples from the data owners. However, traditional SL inevitably leaks label privacy as the tail model (with the last layers) should be placed on the server. To overcome this limitation, one promising solution is to utilize U-shaped architecture to leave both early layers and last layers on the user side. In this paper, we develop a novel parallel U-shaped split learning and devise the optimal resource optimization scheme to improve the performance of edge networks. In the proposed framework, multiple users communicate with an edge server for SL. We analyze the end-to-end delay of each client during the training process and design an efficient resource allocation algorithm, called LSCRA, which finds the optimal computing resource allocation and split layers. Our experimental results show the effectiveness of LSCRA and that U-shaped PSL can achieve a similar performance with other S
    
[^17]: 深度学习的双高斯牛顿方向

    Dual Gauss-Newton Directions for Deep Learning. (arXiv:2308.08886v1 [cs.LG])

    [http://arxiv.org/abs/2308.08886](http://arxiv.org/abs/2308.08886)

    本文研究了深度学习目标的结构特点，提出了基于双重高斯牛顿方向预言的方法。通过对偶形式计算这些预言，得到了既具有计算好处又具有新见解的结果。通过实验证明了这些预言作为随机梯度的下降方向的优势，并研究了计算上的权衡。

    

    受高斯牛顿方法的启发，我们研究了利用深度学习目标的结构，即由凸损失函数和非线性网络组成，以导出比随机梯度更好的方向预言，基于部分线性化的思想。与以前的工作不同，我们提出通过它们的对偶形式来计算这样的方向预言，从而获得计算上的好处和新的见解。我们证明了由此产生的预言定义了可以用作现有优化算法中随机梯度的替代品的下降方向。我们通过实验证明了使用对偶形式的优势以及涉及计算这些预言的计算权衡。

    Inspired by Gauss-Newton-like methods, we study the benefit of leveraging the structure of deep learning objectives, namely, the composition of a convex loss function and of a nonlinear network, in order to derive better direction oracles than stochastic gradients, based on the idea of partial linearization. In a departure from previous works, we propose to compute such direction oracles via their dual formulation, leading to both computational benefits and new insights. We demonstrate that the resulting oracles define descent directions that can be used as a drop-in replacement for stochastic gradients, in existing optimization algorithms. We empirically study the advantage of using the dual formulation as well as the computational trade-offs involved in the computation of such oracles.
    
[^18]: 特征强化物理信息神经网络（FE-PINN）：在目标任务之前学习底层物理特征的框架

    Feature Enforcing PINN (FE-PINN): A Framework to Learn the Underlying-Physics Features Before Target Task. (arXiv:2308.08873v1 [cs.LG])

    [http://arxiv.org/abs/2308.08873](http://arxiv.org/abs/2308.08873)

    FE-PINN是一种学习底层物理特征的框架，在主训练之前以低计算成本解决问题的模式。与传统PINN相比，FE-PINN通过执行一系列子任务来解决损失函数不平衡的问题，并具有快速训练和更高的求解速度。

    

    本文介绍了一种名为特征强化物理信息神经网络（FE-PINN）的新型无数据框架。该框架能够在主训练循环之前以较低的计算成本学习任何问题的底层模式。由于存在偏微分残差和边界条件均方误差两个项，普通PINN的损失函数不平衡。FE-PINN通过只需一分钟的训练，而不是耗时数小时的超参数调优来解决这个挑战。FE-PINN通过执行一系列子任务来完成这个过程。第一个子任务学习有关底层物理的有用特征。然后，模型在目标任务上进行训练以完善计算。FE-PINN应用于三个基准问题：圆柱体上的流动、二维热传导以及计算入口速度的逆问题。FE-PINN可以分别加速15倍、2倍和5倍地解决每个案例。另外

    In this work, a new data-free framework called Feature Enforcing Physics Informed Neural Network (FE-PINN) is introduced. This framework is capable of learning the underlying pattern of any problem with low computational cost before the main training loop. The loss function of vanilla PINN due to the existence of two terms of partial differential residuals and boundary condition mean squared error is imbalanced. FE-PINN solves this challenge with just one minute of training instead of time-consuming hyperparameter tuning for loss function that can take hours. The FE-PINN accomplishes this process by performing a sequence of sub-tasks. The first sub-task learns useful features about the underlying physics. Then, the model trains on the target task to refine the calculations. FE-PINN is applied to three benchmarks, flow over a cylinder, 2D heat conduction, and an inverse problem of calculating inlet velocity. FE-PINN can solve each case with, 15x, 2x, and 5x speed up accordingly. Another
    
[^19]: 面向非随机缺失标签的半监督学习

    Towards Semi-supervised Learning with Non-random Missing Labels. (arXiv:2308.08872v1 [cs.LG])

    [http://arxiv.org/abs/2308.08872](http://arxiv.org/abs/2308.08872)

    本研究针对标签缺失非随机（MNAR）问题，提出了一种基于类别转换跟踪的伪修正指导（PRG）方法，通过利用历史信息和动态创建的图模型，来改善半监督学习（SSL）中的标签缺失问题。

    

    半监督学习（SSL）通过有效利用无标签数据来解决标签缺失问题。现有的SSL方法主要关注传统的情景，忽略了一个实际且具有挑战性的场景，即标签缺失非随机（MNAR）。在MNAR中，有标签和无标签数据属于不同的类别分布，导致了有偏的标签插补，降低了SSL模型的性能。本文提出了基于类别转换跟踪的伪修正指导（PRG）方法来处理MNAR问题。我们通过基于马尔可夫随机游走的方式得到类别级别的引导信息，该信息基于动态创建的图在类别跟踪矩阵之上建模。PRG方法通过将类别分布的历史信息和伪修正过程引起的类别转换统一起来，以保持模型对所有类别分配伪标签的无偏热情，从而提高伪标签在常见类别和罕见类别上的质量。

    Semi-supervised learning (SSL) tackles the label missing problem by enabling the effective usage of unlabeled data. While existing SSL methods focus on the traditional setting, a practical and challenging scenario called label Missing Not At Random (MNAR) is usually ignored. In MNAR, the labeled and unlabeled data fall into different class distributions resulting in biased label imputation, which deteriorates the performance of SSL models. In this work, class transition tracking based Pseudo-Rectifying Guidance (PRG) is devised for MNAR. We explore the class-level guidance information obtained by the Markov random walk, which is modeled on a dynamically created graph built over the class tracking matrix. PRG unifies the historical information of class distribution and class transitions caused by the pseudo-rectifying procedure to maintain the model's unbiased enthusiasm towards assigning pseudo-labels to all classes, so as the quality of pseudo-labels on both popular classes and rare c
    
[^20]: 没有模型的算法在零和马尔可夫博弈中提高了样本效率

    Model-Free Algorithm with Improved Sample Efficiency for Zero-Sum Markov Games. (arXiv:2308.08858v1 [cs.LG])

    [http://arxiv.org/abs/2308.08858](http://arxiv.org/abs/2308.08858)

    本文提出了一种模型自由的算法，可以在零和马尔可夫博弈中实现与模型为基础算法相同的样本复杂度，首次证明了模型自由算法可以在时间段依赖性方面达到同样的优化效果。

    

    最近，两人零和马尔可夫博弈问题在多智能体强化学习的理论研究中引起了越来越多的兴趣。特别是对于有限时间段的马尔可夫决策过程，已经证明了模型为基础的算法可以通过样本复杂度为$O(H^3SAB/\epsilon^2)$找到$\epsilon$-最优的纳什均衡（NE），其中$H$是时间段，$S$是状态数量（$A$和$B$分别表示两个玩家的动作数量）。然而，目前没有一种现有的模型自由算法可以达到这样的优化效果。在这项工作中，我们提出了一种模型自由的阶段性Q学习算法，并展示它实现了与最佳模型为基础算法相同的样本复杂度，因此首次证明了模型自由算法可以在时间段依赖性方面享受与模型为基础算法相同的优化效果。对于$H$的依赖性的主要改进来源于...

    The problem of two-player zero-sum Markov games has recently attracted increasing interests in theoretical studies of multi-agent reinforcement learning (RL). In particular, for finite-horizon episodic Markov decision processes (MDPs), it has been shown that model-based algorithms can find an $\epsilon$-optimal Nash Equilibrium (NE) with the sample complexity of $O(H^3SAB/\epsilon^2)$, which is optimal in the dependence of the horizon $H$ and the number of states $S$ (where $A$ and $B$ denote the number of actions of the two players, respectively). However, none of the existing model-free algorithms can achieve such an optimality. In this work, we propose a model-free stage-based Q-learning algorithm and show that it achieves the same sample complexity as the best model-based algorithm, and hence for the first time demonstrate that model-free algorithms can enjoy the same optimality in the $H$ dependence as model-based algorithms. The main improvement of the dependency on $H$ arises by
    
[^21]: Bag of Tricks for Long-Tailed Multi-Label Classification on Chest X-Rays

    Bag of Tricks for Long-Tailed Multi-Label Classification on Chest X-Rays. (arXiv:2308.08853v1 [cs.CV])

    [http://arxiv.org/abs/2308.08853](http://arxiv.org/abs/2308.08853)

    本论文介绍了一种针对胸部X射线图像的长尾多标签分类的"技巧包"。通过多种先进的设计，如数据增强、特征提取器、分类器设计等，结合预训练技术，提高了胸部X射线的诊断性能。

    

    临床胸部放射学的分类对于标准的机器学习算法来说特别具有挑战性，因为它具有天生的长尾和多标签特性。然而，很少有尝试同时考虑到类别不平衡和标签共现所带来的难题，这阻碍了它们在真实场景中提高胸部X射线（CXR）诊断的价值。此外，随着预训练技术的普及，如何将这些新范式纳入当前框架中缺乏系统性的研究。本技术报告介绍了我们在ICCV CVAMD 2023 CXR-LT竞赛中的解决方案。我们通过数据增强、特征提取器、分类器设计、损失函数加权重、外部数据补充等多个先进设计在CXR诊断方面进行了实证探索。此外，我们还通过简单的测试时间数据增强和集成来提高性能。

    Clinical classification of chest radiography is particularly challenging for standard machine learning algorithms due to its inherent long-tailed and multi-label nature. However, few attempts take into account the coupled challenges posed by both the class imbalance and label co-occurrence, which hinders their value to boost the diagnosis on chest X-rays (CXRs) in the real-world scenarios. Besides, with the prevalence of pretraining techniques, how to incorporate these new paradigms into the current framework lacks of the systematical study. This technical report presents a brief description of our solution in the ICCV CVAMD 2023 CXR-LT Competition. We empirically explored the effectiveness for CXR diagnosis with the integration of several advanced designs about data augmentation, feature extractor, classifier design, loss function reweighting, exogenous data replenishment, etc. In addition, we improve the performance through simple test-time data augmentation and ensemble. Our framewo
    
[^22]: 通过高效算法学习具有结构稀疏性的核心图形Lasso模型

    Learning the hub graphical Lasso model with the structured sparsity via an efficient algorithm. (arXiv:2308.08852v1 [math.OC])

    [http://arxiv.org/abs/2308.08852](http://arxiv.org/abs/2308.08852)

    通过双重交替方向乘子法 (ADMM) 和半平滑牛顿 (SSN) 基于增广对偶法 (ALM) 的方法，我们提出了一个高效算法来学习具有结构稀疏性的核心图形Lasso模型，该算法能够在大维度的任务中节省超过70\%的执行时间，并且具有较高的性能。

    

    图形模型在从生物分析到推荐系统等众多任务中展现出了良好的性能。然而，具有核心节点的图形模型在数据维度较大时计算上存在困难。为了高效估计核心图形模型，我们提出了一个两阶段算法。所提出的算法首先通过双重交替方向乘子法 (ADMM) 生成一个良好的初始点，然后使用半平滑牛顿 (SSN) 基于增广对偶法 (ALM) 的方法进行热启动，以计算出能够在实际任务中精确到足够程度的解。广义雅可比矩阵的稀疏结构确保了该算法能够非常高效地获得一个良好的解。在合成数据和真实数据的全面实验中，该算法明显优于现有的最先进算法。特别是在某些高维任务中，它可以节省超过70\%的执行时间，同时仍然可以达到很好的性能。

    Graphical models have exhibited their performance in numerous tasks ranging from biological analysis to recommender systems. However, graphical models with hub nodes are computationally difficult to fit, particularly when the dimension of the data is large. To efficiently estimate the hub graphical models, we introduce a two-phase algorithm. The proposed algorithm first generates a good initial point via a dual alternating direction method of multipliers (ADMM), and then warm starts a semismooth Newton (SSN) based augmented Lagrangian method (ALM) to compute a solution that is accurate enough for practical tasks. The sparsity structure of the generalized Jacobian ensures that the algorithm can obtain a nice solution very efficiently. Comprehensive experiments on both synthetic data and real data show that it obviously outperforms the existing state-of-the-art algorithms. In particular, in some high dimensional tasks, it can save more than 70\% of the execution time, meanwhile still ach
    
[^23]: 通过CFD耦合多精度贝叶斯优化的机器学习辅助下的新反应器设计的发现

    Machine Learning-Assisted Discovery of Novel Reactor Designs via CFD-Coupled Multi-fidelity Bayesian Optimisation. (arXiv:2308.08841v1 [cs.CE])

    [http://arxiv.org/abs/2308.08841](http://arxiv.org/abs/2308.08841)

    本研究通过应用多精度贝叶斯优化方法，结合参数化网格化和模拟，提出了两种新的螺旋管参数化方法，用于发现新的反应器设计。这种方法能够处理高维度和复杂的优化问题，并克服了没有梯度的非局部优化困难。

    

    添加制造技术使得更先进的反应器几何结构成为可能，为更大和更复杂的设计空间提供了潜在机会。在更广泛的设计空间中确定和优化有希望的配置对于现有的人为中心的设计方法来说是一个重大挑战。鉴于算法改进和添加制造的出现，我们提出了两种新的螺旋管参数化方法，可以改变截面和螺旋路径，从而产生了一系列高维的复杂优化问题。为了确保可行的非局部优化，在没有梯度的情况下，我们应用多精度贝叶斯优化。我们的方法描述了多个连续的精度，并与参数化网格化和模拟相结合，实现了低质量、

    Additive manufacturing has enabled the production of more advanced reactor geometries, resulting in the potential for significantly larger and more complex design spaces. Identifying and optimising promising configurations within broader design spaces presents a significant challenge for existing human-centric design approaches. As such, existing parameterisations of coiled-tube reactor geometries are low-dimensional with expensive optimisation limiting more complex solutions. Given algorithmic improvements and the onset of additive manufacturing, we propose two novel coiled-tube parameterisations enabling the variation of cross-section and coil path, resulting in a series of high dimensional, complex optimisation problems. To ensure tractable, non-local optimisation where gradients are not available, we apply multi-fidelity Bayesian optimisation. Our approach characterises multiple continuous fidelities and is coupled with parameterised meshing and simulation, enabling lower quality, 
    
[^24]: 控制隐密联合学习

    Controlling Federated Learning for Covertness. (arXiv:2308.08825v1 [cs.LG])

    [http://arxiv.org/abs/2308.08825](http://arxiv.org/abs/2308.08825)

    本文研究了控制隐密联合学习的问题，提出了一个基于马尔可夫决策过程的策略梯度算法，通过动态选择学习和混淆来实现最优查询策略。实验结果在恶意言论分类任务上展示了方法的实用性。

    

    学习者通过重复查询分布式预言机提供的噪声梯度评估来最小化函数$f$。同时，学习者还试图隐藏$\arg\min f$，以免被恶意窃听者观察到其查询。本文考虑了\textit{隐密}或\textit{学习者私密}优化问题，其中学习者必须通过利用随机性在学习和混淆之间动态选择。将控制隐密优化的随机梯度算法问题建模为马尔可夫决策过程，并且我们证明了动态规划算子具有超模结构，从而得出最优策略具有单调阈值结构。提出了一个计算效率高的策略梯度算法，用于在不了解迁移概率的情况下搜索最优查询策略。作为实际应用，我们的方法在联合学习中的恶意言论分类任务上进行了演示。

    A learner aims to minimize a function $f$ by repeatedly querying a distributed oracle that provides noisy gradient evaluations. At the same time, the learner seeks to hide $\arg\min f$ from a malicious eavesdropper that observes the learner's queries. This paper considers the problem of \textit{covert} or \textit{learner-private} optimization, where the learner has to dynamically choose between learning and obfuscation by exploiting the stochasticity. The problem of controlling the stochastic gradient algorithm for covert optimization is modeled as a Markov decision process, and we show that the dynamic programming operator has a supermodular structure implying that the optimal policy has a monotone threshold structure. A computationally efficient policy gradient algorithm is proposed to search for the optimal querying policy without knowledge of the transition probabilities. As a practical application, our methods are demonstrated on a hate speech classification task in a federated se
    
[^25]: 缓解图形主动学习中来自恶意邻域的语义混淆

    Mitigating Semantic Confusion from Hostile Neighborhood for Graph Active Learning. (arXiv:2308.08823v1 [cs.LG])

    [http://arxiv.org/abs/2308.08823](http://arxiv.org/abs/2308.08823)

    该论文提出了一种用于缓解图形主动学习中来自恶意邻域的语义混淆问题的方法。通过引入带有语义特征的节点的成对相似度和不相似度来共同评估节点的影响力，并设计了一种新的基于样本集的准则和查询策略来维持所选节点的多样性和类别平衡。

    

    图形主动学习（Graph Active Learning，简称GAL）旨在寻找图中最具信息量的节点进行注释，以最大化图神经网络（Graph Neural Networks，简称GNNs）的性能，吸引了许多研究工作，但仍存在一些非常困难的挑战。其中一个主要挑战是，现有的GAL策略可能会引入语义混淆到选择的训练集中，尤其是在图形嘈杂的情况下。具体来说，大多数现有方法都假设所有聚合特征都是有用的，忽视了在消息传递机制下类间边缘之间的语义负面影响。在这项工作中，我们提出了一种用于图的语义感知主动学习框架（Semantic-aware Active learning framework for Graphs，简称SAG）来缓解语义混淆问题。引入节点之间带有语义特征的成对相似度和不相似度来共同评估节点影响力。还设计了一种新的基于样本集的准则和查询策略，分别用于维持所选节点的多样性和类别平衡。在公共数据集上进行了大量实验证明了SAG方法的有效性。

    Graph Active Learning (GAL), which aims to find the most informative nodes in graphs for annotation to maximize the Graph Neural Networks (GNNs) performance, has attracted many research efforts but remains non-trivial challenges. One major challenge is that existing GAL strategies may introduce semantic confusion to the selected training set, particularly when graphs are noisy. Specifically, most existing methods assume all aggregating features to be helpful, ignoring the semantically negative effect between inter-class edges under the message-passing mechanism. In this work, we present Semantic-aware Active learning framework for Graphs (SAG) to mitigate the semantic confusion problem. Pairwise similarities and dissimilarities of nodes with semantic features are introduced to jointly evaluate the node influence. A new prototype-based criterion and query policy are also designed to maintain diversity and class balance of the selected nodes, respectively. Extensive experiments on the pu
    
[^26]: 变分分布先验与显著性图重新播放的融合，用于连续三维重建

    A Fusion of Variational Distribution Priors and Saliency Map Replay for Continual 3D Reconstruction. (arXiv:2308.08812v1 [cs.CV])

    [http://arxiv.org/abs/2308.08812](http://arxiv.org/abs/2308.08812)

    本研究提出了一种基于连续学习的三维重建方法，通过使用变分先验和显著性图重新播放，实现对之前已见类别的合理重建。实验结果表明，与已建立的方法相比具有竞争力。

    

    单图像三维重建是研究如何根据单视角图像预测三维物体形状的一个挑战。这个任务需要大量数据获取来预测形状的可见和遮挡部分。此外，基于学习的方法面临创建针对所有可能类别的全面训练数据集的困难。为此，我们提出了一种基于连续学习的三维重建方法，我们的目标是设计一个使用变分先验的模型，即使在训练新类别后仍可以合理重建以前见过的类别。变分先验代表抽象形状并避免遗忘，而显著性图以较少的内存使用保留对象属性。这对于存储大量训练数据的资源限制至关重要。此外，我们引入了基于显著性图的经验重放，以捕捉全局和独特的对象特征。详细的实验显示与已建立方法相比具有竞争力的结果。

    Single-image 3D reconstruction is a research challenge focused on predicting 3D object shapes from single-view images. This task requires significant data acquisition to predict both visible and occluded portions of the shape. Furthermore, learning-based methods face the difficulty of creating a comprehensive training dataset for all possible classes. To this end, we propose a continual learning-based 3D reconstruction method where our goal is to design a model using Variational Priors that can still reconstruct the previously seen classes reasonably even after training on new classes. Variational Priors represent abstract shapes and combat forgetting, whereas saliency maps preserve object attributes with less memory usage. This is vital due to resource constraints in storing extensive training data. Additionally, we introduce saliency map-based experience replay to capture global and distinct object features. Thorough experiments show competitive results compared to established method
    
[^27]: 标签偏移适配器用于协变量和标签偏移下的测试时适应性

    Label Shift Adapter for Test-Time Adaptation under Covariate and Label Shifts. (arXiv:2308.08810v1 [cs.CV])

    [http://arxiv.org/abs/2308.08810](http://arxiv.org/abs/2308.08810)

    提出了一种新的标签偏移适配器，可以与现有的测试时适应性方法结合使用，有效处理在协变量和标签偏移下的问题。

    

    测试时适应性 (TTA)旨在在推理过程中以批次为单位将预训练模型适应到目标领域。尽管实际情况中标签分布往往存在不平衡现象，但大多数现有的TTA方法通常假设源领域和目标领域的数据集都具有平衡的标签分布。由于某些类别在某些领域中出现更频繁（如城市中的建筑物，森林中的树木），标签分布在领域发生变化时会自然地发生变化。然而，我们发现大多数现有的TTA方法没有很好地解决协变量和标签偏移共存的问题。为了解决这个挑战，我们提出了一种新的标签偏移适配器，它可以与现有的TTA方法结合使用，有效处理TTA过程中的标签偏移。具体而言，我们估计目标领域的标签分布，将其输入到标签偏移适配器中。随后，标签偏移适配器产生最优解。

    Test-time adaptation (TTA) aims to adapt a pre-trained model to the target domain in a batch-by-batch manner during inference. While label distributions often exhibit imbalances in real-world scenarios, most previous TTA approaches typically assume that both source and target domain datasets have balanced label distribution. Due to the fact that certain classes appear more frequently in certain domains (e.g., buildings in cities, trees in forests), it is natural that the label distribution shifts as the domain changes. However, we discover that the majority of existing TTA methods fail to address the coexistence of covariate and label shifts. To tackle this challenge, we propose a novel label shift adapter that can be incorporated into existing TTA approaches to deal with label shifts during the TTA process effectively. Specifically, we estimate the label distribution of the target domain to feed it into the label shift adapter. Subsequently, the label shift adapter produces optimal pa
    
[^28]: 捕捉流行趋势：增强项目推荐的简化非个性化方法

    Capturing Popularity Trends: A Simplistic Non-Personalized Approach for Enhanced Item Recommendation. (arXiv:2308.08799v1 [cs.IR])

    [http://arxiv.org/abs/2308.08799](http://arxiv.org/abs/2308.08799)

    本论文提出了一种简化的非个性化方法PARE，通过预测最高流行度的项目进行推荐，填补了现有推荐方法忽略项目流行度的不足。实验证明PARE的性能优于复杂的方法。

    

    随着时间的推移，推荐系统已经越来越受到研究的关注。大多数现有的推荐方法侧重于通过历史的用户-项目交互来捕捉用户的个性化偏好，这可能会侵犯用户的隐私。此外，这些方法常常忽视了项目流行度的时间波动对用户决策的重要性。为了弥补这一差距，我们提出了Popularity-Aware Recommender（PARE），通过预测将达到最高流行度的项目来进行非个性化推荐。PARE由四个模块组成，分别关注不同的方面：流行度历史、时间影响、周期性影响和附加信息。最后，利用注意力层融合四个模块的输出。据我们所知，这是第一个在推荐系统中明确建模项目流行度的工作。广泛的实验证明，PARE的性能与复杂的方法相当甚至更好。

    Recommender systems have been gaining increasing research attention over the years. Most existing recommendation methods focus on capturing users' personalized preferences through historical user-item interactions, which may potentially violate user privacy. Additionally, these approaches often overlook the significance of the temporal fluctuation in item popularity that can sway users' decision-making. To bridge this gap, we propose Popularity-Aware Recommender (PARE), which makes non-personalized recommendations by predicting the items that will attain the highest popularity. PARE consists of four modules, each focusing on a different aspect: popularity history, temporal impact, periodic impact, and side information. Finally, an attention layer is leveraged to fuse the outputs of four modules. To our knowledge, this is the first work to explicitly model item popularity in recommendation systems. Extensive experiments show that PARE performs on par or even better than sophisticated st
    
[^29]: 功能空间中非平稳动力学中的翻车点预测

    Tipping Point Forecasting in Non-Stationary Dynamics on Function Spaces. (arXiv:2308.08794v1 [cs.LG])

    [http://arxiv.org/abs/2308.08794](http://arxiv.org/abs/2308.08794)

    本文提出了一种利用循环神经算子学习非平稳动力系统演化的方法，并且通过基于不确定性的方法检测未来的翻车点。同时，我们还提出了一种符合预测框架，通过监测与物理约束的偏离来预测翻车点，从而使得预测结果具有严格的不确定性度量。

    

    翻车点是非平稳和混沌动力系统演化中的突变、剧烈且常常不可逆的变化。例如，预计温室气体浓度的增加会导致低云覆盖的急剧减少，被称为气候学的翻车点。在本文中，我们利用一种新颖的循环神经算子（RNO）学习这种非平稳动力系统的演化，RNO可以学习函数空间之间的映射关系。在仅训练RNO在翻车点之前的动力学数据之后，我们采用基于不确定性的方法来检测未来的翻车点。具体而言，我们提出了一个符合预测框架，通过监测与物理约束（如守恒量和偏微分方程）偏离来预测翻车点，从而使得对这些突变的预测伴随着一种严格的不确定性度量。我们将我们提出的方法应用于非平稳常微分方程和偏微分方程的案例。

    Tipping points are abrupt, drastic, and often irreversible changes in the evolution of non-stationary and chaotic dynamical systems. For instance, increased greenhouse gas concentrations are predicted to lead to drastic decreases in low cloud cover, referred to as a climatological tipping point. In this paper, we learn the evolution of such non-stationary dynamical systems using a novel recurrent neural operator (RNO), which learns mappings between function spaces. After training RNO on only the pre-tipping dynamics, we employ it to detect future tipping points using an uncertainty-based approach. In particular, we propose a conformal prediction framework to forecast tipping points by monitoring deviations from physics constraints (such as conserved quantities and partial differential equations), enabling forecasting of these abrupt changes along with a rigorous measure of uncertainty. We illustrate our proposed methodology on non-stationary ordinary and partial differential equations,
    
[^30]: 分布式网络中电动车充电控制的联邦强化学习

    Federated Reinforcement Learning for Electric Vehicles Charging Control on Distribution Networks. (arXiv:2308.08792v1 [eess.SY])

    [http://arxiv.org/abs/2308.08792](http://arxiv.org/abs/2308.08792)

    本文提出了一种结合了多EV充电/放电和在最佳功率流下运行的径向分布式网络的新方法，用于实时分配功率流来解决EV充电控制中的挑战。

    

    随着电动车（EVs）的普及，维持电网稳定性变成了一个重要的挑战。为了解决这个问题，已经开发了EV充电控制策略来管理EV的车—网（V2G）和网—车（G2V）模式之间的切换。在这种情况下，多智能体深度强化学习（MADRL）已经证明在EV充电控制中的有效性。然而，现有的基于MADRL的方法未能考虑EV充电/放电的自然功率流以及忽视驾驶员隐私。为了解决这些问题，本文提出了一种新方法，将多EV充电/放电与在最佳功率流（OPF）下运行的径向分布式网络（RDN）相结合，以实时分配功率流。开发了一个数学模型来描述RDN负荷。将EV充电控制问题制定为马尔可夫决策过程（MDP），以找到一种优化的充电控制策略，以实现负载平衡。

    With the growing popularity of electric vehicles (EVs), maintaining power grid stability has become a significant challenge. To address this issue, EV charging control strategies have been developed to manage the switch between vehicle-to-grid (V2G) and grid-to-vehicle (G2V) modes for EVs. In this context, multi-agent deep reinforcement learning (MADRL) has proven its effectiveness in EV charging control. However, existing MADRL-based approaches fail to consider the natural power flow of EV charging/discharging in the distribution network and ignore driver privacy. To deal with these problems, this paper proposes a novel approach that combines multi-EV charging/discharging with a radial distribution network (RDN) operating under optimal power flow (OPF) to distribute power flow in real time. A mathematical model is developed to describe the RDN load. The EV charging control problem is formulated as a Markov Decision Process (MDP) to find an optimal charging control strategy that balanc
    
[^31]: APPFLx：提供隐私保护的跨不同数据源联合学习服务

    APPFLx: Providing Privacy-Preserving Cross-Silo Federated Learning as a Service. (arXiv:2308.08786v1 [cs.LG])

    [http://arxiv.org/abs/2308.08786](http://arxiv.org/abs/2308.08786)

    APPFLx是一个提供隐私保护的跨不同数据源的联合学习平台，为用户提供了简化的实验启动过程和可视化的实验生命周期追踪功能，使得领域专家和机器学习从业者能够轻松地进行隐私保护联合学习。

    

    跨不同数据源的隐私保护联合学习是一种强大的工具，可以通过合作训练强健和泛化的机器学习模型，而无需共享敏感的本地数据（如医疗和金融数据）。为了简化和加快隐私保护的跨不同数据源联合学习的采用，我们介绍了APPFLx，这是一个即可用平台，提供隐私保护的跨不同数据源联合学习服务。APPFLx使用Globus身份验证允许用户轻松安全地邀请可信任的合作者进行隐私保护联合学习，实现了几种同步和异步的联合学习算法，简化了联合学习实验的启动过程，并能够追踪和可视化联合学习实验的生命周期，使领域专家和机器学习从业者能够在一个平台上轻松地协调和评估跨不同数据源的联合学习。APPFLx可在线访问https://appflx.link

    Cross-silo privacy-preserving federated learning (PPFL) is a powerful tool to collaboratively train robust and generalized machine learning (ML) models without sharing sensitive (e.g., healthcare of financial) local data. To ease and accelerate the adoption of PPFL, we introduce APPFLx, a ready-to-use platform that provides privacy-preserving cross-silo federated learning as a service. APPFLx employs Globus authentication to allow users to easily and securely invite trustworthy collaborators for PPFL, implements several synchronous and asynchronous FL algorithms, streamlines the FL experiment launch process, and enables tracking and visualizing the life cycle of FL experiments, allowing domain experts and ML practitioners to easily orchestrate and evaluate cross-silo FL under one platform. APPFLx is available online at https://appflx.link
    
[^32]: 多头神经网络用于不变学习的环境多样化

    Environment Diversification with Multi-head Neural Network for Invariant Learning. (arXiv:2308.08778v1 [cs.LG])

    [http://arxiv.org/abs/2308.08778](http://arxiv.org/abs/2308.08778)

    本文提出了一个不变学习框架EDNIL，其中包含多头神经网络，用于吸收数据偏差，并展示了该框架的稳健性。

    

    神经网络通常通过经验风险最小化进行训练；然而，已经证明训练和测试分布之间的偏移会导致不可预测的性能下降。针对这个问题，提出了一种研究方向，即不变学习，用于提取对分布变化不敏感的不变特征。本文提出了一个包含多头神经网络的不变学习框架EDNIL，用于吸收数据偏差。我们展示了该框架不需要对环境有先验知识或对预训练模型有强假设。我们还揭示了该算法与最近探讨变体特征和不变特征性质的研究有理论联系。最后，我们证明使用EDNIL训练的模型在面对分布变化时具有更强的稳健性。

    Neural networks are often trained with empirical risk minimization; however, it has been shown that a shift between training and testing distributions can cause unpredictable performance degradation. On this issue, a research direction, invariant learning, has been proposed to extract invariant features insensitive to the distributional changes. This work proposes EDNIL, an invariant learning framework containing a multi-head neural network to absorb data biases. We show that this framework does not require prior knowledge about environments or strong assumptions about the pre-trained model. We also reveal that the proposed algorithm has theoretical connections to recent studies discussing properties of variant and invariant features. Finally, we demonstrate that models trained with EDNIL are empirically more robust against distributional shifts.
    
[^33]: 差分隐私、语言公平性和训练数据影响：多语言语言模型的不可能性和可能性定理

    Differential Privacy, Linguistic Fairness, and Training Data Influence: Impossibility and Possibility Theorems for Multilingual Language Models. (arXiv:2308.08774v1 [cs.CL])

    [http://arxiv.org/abs/2308.08774](http://arxiv.org/abs/2308.08774)

    该论文研究了多语言语言模型在多语言压缩、语言公平性和透明性等方面的要求，并发现差分隐私与训练数据影响稀疏性之间存在相互制约的关系。

    

    语言模型如mBERT、XLM-R和BLOOM旨在实现多语言概括或压缩，以便于转移到大量（可能未知的）语言。然而，这些模型还应该具备隐私性、语言公平性和透明性，即将它们的预测与训练数据相关联。这些要求可以同时满足吗？我们表明，多语言压缩和语言公平性与差分隐私是兼容的，但差分隐私与训练数据影响稀疏性是相悖的，后者是透明性的目标。我们还对两个常见的自然语言处理任务进行了一系列实验，并在不同的隐私保证下评估了多语言压缩和训练数据影响稀疏性，更详细地探讨了这些权衡。我们的结果表明，我们需要开发一种共同优化这些目标的方法，以找到实际的权衡。

    Language models such as mBERT, XLM-R, and BLOOM aim to achieve multilingual generalization or compression to facilitate transfer to a large number of (potentially unseen) languages. However, these models should ideally also be private, linguistically fair, and transparent, by relating their predictions to training data. Can these requirements be simultaneously satisfied? We show that multilingual compression and linguistic fairness are compatible with differential privacy, but that differential privacy is at odds with training data influence sparsity, an objective for transparency. We further present a series of experiments on two common NLP tasks and evaluate multilingual compression and training data influence sparsity under different privacy guarantees, exploring these trade-offs in more detail. Our results suggest that we need to develop ways to jointly optimize for these objectives in order to find practical trade-offs.
    
[^34]: 用于车削中刀具磨损预测的可解释人工智能

    Explainable AI for tool wear prediction in turning. (arXiv:2308.08765v1 [cs.LG])

    [http://arxiv.org/abs/2308.08765](http://arxiv.org/abs/2308.08765)

    这篇论文开发了一个可解释人工智能框架，用于车削过程中刀具磨损的预测。通过使用随机森林算法进行训练，结合温度等输入特征，可以准确预测刀具的状态，刀具温度是判断刀具磨损的重要因素。

    

    本研究旨在开发一个可解释人工智能（XAI）框架，以便在车削过程中为刀具磨损预测提供人类可理解的解决方案。采用随机森林算法作为监督机器学习（ML）分类器，使用加速度、声学、温度和主轴转速作为输入特征进行二分类训练，用于预测切削过程后刀具的状态，以二分类形式表示切削工具是否可用或损坏。训练过程之后，使用Shapley准则解释训练的ML分类器的预测。具体而言，确定了决策和分类中每个输入特征的重要性，以解释ML分类器预测的推理过程。在所有测试数据集上实施Shapley准则之后，确定了刀具温度作为判断刀具磨损的重要因素。

    This research aims develop an Explainable Artificial Intelligence (XAI) framework to facilitate human-understandable solutions for tool wear prediction during turning. A random forest algorithm was used as the supervised Machine Learning (ML) classifier for training and binary classification using acceleration, acoustics, temperature, and spindle speed during the orthogonal tube turning process as input features. The ML classifier was used to predict the condition of the tool after the cutting process, which was determined in a binary class form indicating if the cutting tool was available or failed. After the training process, the Shapley criterion was used to explain the predictions of the trained ML classifier. Specifically, the significance of each input feature in the decision-making and classification was identified to explain the reasoning of the ML classifier predictions. After implementing the Shapley criterion on all testing datasets, the tool temperature was identified as th
    
[^35]: 基于LightGBM和特征工程的高效商业银行客户信用风险评估

    Efficient Commercial Bank Customer Credit Risk Assessment Based on LightGBM and Feature Engineering. (arXiv:2308.08762v1 [cs.LG])

    [http://arxiv.org/abs/2308.08762](http://arxiv.org/abs/2308.08762)

    本文基于LightGBM算法和特征工程开展了高效商业银行客户信用风险评估研究，通过构建新的特征属性，准确率达到0.734，AUC达到0.772，为商业银行的信贷授予提供参考。

    

    有效控制信用风险是商业银行稳定运作的关键环节。本文主要基于Kaggle上一家外国商业银行的客户信息数据集，利用LightGBM算法构建分类器对客户进行分类，帮助银行判断客户信用违约的可能性。本文主要处理特征工程，如缺失值处理、编码、不平衡样本等，极大地提高了机器学习效果。本文的主要创新是在原始数据集的基础上构建新的特征属性，使分类器的准确率达到0.734，AUC达到0.772，这超过了许多基于相同数据集的分类器。该模型可以为商业银行的信贷授予提供一些参考，同时也为其他类似研究提供一些特征处理的思路。

    Effective control of credit risk is a key link in the steady operation of commercial banks. This paper is mainly based on the customer information dataset of a foreign commercial bank in Kaggle, and we use LightGBM algorithm to build a classifier to classify customers, to help the bank judge the possibility of customer credit default. This paper mainly deals with characteristic engineering, such as missing value processing, coding, imbalanced samples, etc., which greatly improves the machine learning effect. The main innovation of this paper is to construct new feature attributes on the basis of the original dataset so that the accuracy of the classifier reaches 0.734, and the AUC reaches 0.772, which is more than many classifiers based on the same dataset. The model can provide some reference for commercial banks' credit granting, and also provide some feature processing ideas for other similar studies.
    
[^36]: PMET: 在Transformer中的精确模型编辑

    PMET: Precise Model Editing in a Transformer. (arXiv:2308.08742v1 [cs.CL])

    [http://arxiv.org/abs/2308.08742](http://arxiv.org/abs/2308.08742)

    该论文通过分析Transformer模型中的隐藏状态，发现多头自注意力编码了某些通用知识提取模式，因此在进行模型编辑时，不需要更新多头自注意力的权重。

    

    模型编辑技术可以以较低的成本修改大型语言模型中的少量知识，并且已经取得了显著的成功。现有方法假设Transformer层隐藏状态是前馈网络的键值内存的值。它们通常优化Transformer层隐藏状态来记忆目标知识，并将其用于更新大型语言模型中前馈网络的权重。然而，Transformer层隐藏状态的信息流来自三个部分：多头自注意力、前馈网络和残差连接。现有方法忽视了Transformer层隐藏状态包含了前馈网络特别需要的信息这一事实。因此，模型编辑的性能下降。为了实现更精确的模型编辑，我们分析了多头自注意力和前馈网络的隐藏状态，发现多头自注意力编码了某些通用知识提取模式。这意味着当引入新知识时，多头自注意力的权重不需要更新。

    Model editing techniques modify a minor proportion of knowledge in Large Language Models (LLMs) at a relatively low cost, which have demonstrated notable success. Existing methods assume Transformer Layer (TL) hidden states are values of key-value memories of the Feed-Forward Network (FFN). They usually optimize the TL hidden states to memorize target knowledge and use it to update the weights of the FFN in LLMs. However, the information flow of TL hidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN, and residual connections. Existing methods neglect the fact that the TL hidden states contains information not specifically required for FFN. Consequently, the performance of model editing decreases. To achieve more precise model editing, we analyze hidden states of MHSA and FFN, finding that MHSA encodes certain general knowledge extraction patterns. This implies that MHSA weights do not require updating when new knowledge is introduced. Based on above findings, we
    
[^37]: ReProHRL: 面向多目标导航的真实世界层次化代理方法

    ReProHRL: Towards Multi-Goal Navigation in the Real World using Hierarchical Agents. (arXiv:2308.08737v1 [cs.RO])

    [http://arxiv.org/abs/2308.08737](http://arxiv.org/abs/2308.08737)

    本文提出了一种名为ReProHRL的层次化代理方法，通过强化学习实现了多目标导航任务，并使用物体检测器进行预处理。实验证明，ReProHRL方法在仿真和真实世界环境中表现优于现有方法。

    

    机器人已成功用于高精度任务的执行。在稀疏奖励和多目标的真实世界环境中，学习仍然是一个重大挑战，强化学习算法难以学习到好的策略。在仿真环境中进行训练，然后在真实世界进行微调是一种常见的方法。然而，适应真实世界的环境仍然具有挑战性。本文提出了一种名为"ReProHRL"的方法，该方法通过强化学习实现了分层多目标导航任务。我们还使用物体检测器作为预处理步骤，学习多目标导航并将其应用于真实世界。实证结果表明，所提出的ReProHRL方法在仿真和真实世界环境中的训练时间和性能方面优于现有基准方法。虽然这两种方法在单目标导航的简单环境中均实现了100%的成功率，但是在多目标导航问题上，ReProHRL方法表现更佳。

    Robots have been successfully used to perform tasks with high precision. In real-world environments with sparse rewards and multiple goals, learning is still a major challenge and Reinforcement Learning (RL) algorithms fail to learn good policies. Training in simulation environments and then fine-tuning in the real world is a common approach. However, adapting to the real-world setting is a challenge. In this paper, we present a method named Ready for Production Hierarchical RL (ReProHRL) that divides tasks with hierarchical multi-goal navigation guided by reinforcement learning. We also use object detectors as a pre-processing step to learn multi-goal navigation and transfer it to the real world. Empirical results show that the proposed ReProHRL method outperforms the state-of-the-art baseline in simulation and real-world environments in terms of both training time and performance. Although both methods achieve a 100% success rate in a simple environment for single goal-based navigati
    
[^38]: 关于基于日志表示方法的日志异常检测效果的研究

    On the Effectiveness of Log Representation for Log-based Anomaly Detection. (arXiv:2308.08736v1 [cs.SE])

    [http://arxiv.org/abs/2308.08736](http://arxiv.org/abs/2308.08736)

    本研究调查和比较了先前日志分析研究中常用的日志表示技术，并评估了它们在不同机器学习模型和公共日志数据上的表现。

    

    日志是人们了解软件系统运行状态的重要信息来源。由于现代软件架构和维护方法的不断演变，越来越多的研究工作致力于自动化日志分析。在基于机器学习的日志分析任务中，将文本日志数据转换为数字特征向量是一个至关重要且必不可少的步骤。然而，不同的日志表示技术对下游模型性能的影响尚不清楚，这限制了研究人员和实践者选择其自动化日志分析工作流程中最佳日志表示技术的机会。因此，本研究调查并比较了先前日志分析研究中常用的日志表示技术。具体而言，我们选择了六种日志表示技术，并使用七种机器学习模型和四种公共日志数据进行评估。

    Logs are an essential source of information for people to understand the running status of a software system. Due to the evolving modern software architecture and maintenance methods, more research efforts have been devoted to automated log analysis. In particular, machine learning (ML) has been widely used in log analysis tasks. In ML-based log analysis tasks, converting textual log data into numerical feature vectors is a critical and indispensable step. However, the impact of using different log representation techniques on the performance of the downstream models is not clear, which limits researchers and practitioners' opportunities of choosing the optimal log representation techniques in their automated log analysis workflows. Therefore, this work investigates and compares the commonly adopted log representation techniques from previous log analysis research. Particularly, we select six log representation techniques and evaluate them with seven ML models and four public log datas
    
[^39]: 动态神经网络是你所需要的一切：理解神经网络中动态机制的鲁棒性

    Dynamic Neural Network is All You Need: Understanding the Robustness of Dynamic Mechanisms in Neural Networks. (arXiv:2308.08709v1 [cs.LG])

    [http://arxiv.org/abs/2308.08709](http://arxiv.org/abs/2308.08709)

    本研究旨在调查动态机制在DyNNs中的鲁棒性以及动态机制设计对DyNNs的影响，并提出了三个研究问题。

    

    深度神经网络(DNNs)已被用于解决各种日常问题。近年来，DNNs已被部署在实时系统中，降低能量消耗和响应时间已成为当务之急。为了解决这个场景，研究人员提出了将动态机制引入静态DNNs(SDNN)以创建能够根据输入复杂性执行动态计算的动态神经网络(DyNNs)。尽管在实时系统中将动态机制引入SDNNs更可取，但评估动态机制引入对模型鲁棒性的影响也变得重要。然而，目前还没有大量的研究关注SDNNs和DyNNs之间的鲁棒性权衡。为了解决这个问题，我们提出了研究动态机制在DyNNs中的鲁棒性以及动态机制设计对DyNNs的影响。为此，我们评估了三个研究问题。

    Deep Neural Networks (DNNs) have been used to solve different day-to-day problems. Recently, DNNs have been deployed in real-time systems, and lowering the energy consumption and response time has become the need of the hour. To address this scenario, researchers have proposed incorporating dynamic mechanism to static DNNs (SDNN) to create Dynamic Neural Networks (DyNNs) performing dynamic amounts of computation based on the input complexity. Although incorporating dynamic mechanism into SDNNs would be preferable in real-time systems, it also becomes important to evaluate how the introduction of dynamic mechanism impacts the robustness of the models. However, there has not been a significant number of works focusing on the robustness trade-off between SDNNs and DyNNs. To address this issue, we propose to investigate the robustness of dynamic mechanism in DyNNs and how dynamic mechanism design impacts the robustness of DyNNs. For that purpose, we evaluate three research questions. These
    
[^40]: 人工智能中的意识：来自意识科学的洞见

    Consciousness in Artificial Intelligence: Insights from the Science of Consciousness. (arXiv:2308.08708v1 [cs.AI])

    [http://arxiv.org/abs/2308.08708](http://arxiv.org/abs/2308.08708)

    本论文提出了一种严谨的方法，通过对当前的人工智能系统进行详细评估来探讨人工智能的意识问题。研究中对几种科学意识理论进行概述，并通过计算方法推导出意识的“指示性特征”。分析结果表明目前的人工智能系统尚不具备意识，但建立具有意识的人工智能系统并无明显的障碍。

    

    当前或近期的人工智能系统是否能具有意识成为科学界关注的话题，也引起了公众的担忧。本报告提出并举例了一种严谨且经验基础的人工智能意识方法：根据我们目前最可信的神经科学理论对现有的人工智能系统进行详细评估。我们概述了几种广泛认可的科学意识理论，包括循环处理理论、全局工作空间理论、高阶理论、预测处理理论和注意模式理论。从这些理论中，我们推导出一些意识的“指示性特征”，并通过计算方法来评估人工智能系统是否具备这些特征。我们利用这些指示性特征来评估了几个近期的人工智能系统，并讨论了未来系统如何实现这些特征。我们的分析表明，目前没有现有的人工智能系统具有意识，但同时也显示出没有明显的建立具有意识的人工智能系统的障碍。

    Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern. This report argues for, and exemplifies, a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of our best-supported neuroscientific theories of consciousness. We survey several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higher-order theories, predictive processing, and attention schema theory. From these theories we derive "indicator properties" of consciousness, elucidated in computational terms that allow us to assess AI systems for these properties. We use these indicator properties to assess several recent AI systems, and we discuss how future systems might implement them. Our analysis suggests that no current AI systems are conscious, but also shows that there are no obvious barriers to building conscious AI systems.
    
[^41]: 部分可观测的多Agent强化学习与（准）效率：信息共享的好处。

    Partially Observable Multi-agent RL with (Quasi-)Efficiency: The Blessing of Information Sharing. (arXiv:2308.08705v1 [cs.LG])

    [http://arxiv.org/abs/2308.08705](http://arxiv.org/abs/2308.08705)

    本文研究了部分可观测随机博弈的可证明多Agent强化学习。通过信息共享和观测可能性假设，提出了构建近似模型以实现准效率的方法。

    

    本文研究了部分可观测随机博弈（POSGs）的可证明多Agent强化学习（MARL）。为了规避已知的难度问题和使用计算不可行的预言机，我们倡导利用Agent之间的潜在“信息共享”，这是实证MARL中的常见做法，也是具备通信功能的多Agent控制系统的标准模型。我们首先建立了若干计算复杂性结果，来证明信息共享的必要性，以及观测可能性假设为了求解POSGs中的计算效率已经使得部分可观测的单Agent强化学习具有准效率。然后我们提出进一步“近似”共享的公共信息构建POSG的“近似模型”，在该模型中计划一个近似均衡（从解决原始POSG的角度）可以实现准效率，即准多项式时间，前提是上述假设满足。

    We study provable multi-agent reinforcement learning (MARL) in the general framework of partially observable stochastic games (POSGs). To circumvent the known hardness results and the use of computationally intractable oracles, we advocate leveraging the potential \emph{information-sharing} among agents, a common practice in empirical MARL, and a standard model for multi-agent control systems with communications. We first establish several computation complexity results to justify the necessity of information-sharing, as well as the observability assumption that has enabled quasi-efficient single-agent RL with partial observations, for computational efficiency in solving POSGs. We then propose to further \emph{approximate} the shared common information to construct an {approximate model} of the POSG, in which planning an approximate equilibrium (in terms of solving the original POSG) can be quasi-efficient, i.e., of quasi-polynomial-time, under the aforementioned assumptions. Furthermo
    
[^42]: 计划在想象中：基于学习抽象搜索空间的高级规划

    Planning in the imagination: High-level planning on learned abstract search spaces. (arXiv:2308.08693v1 [cs.AI])

    [http://arxiv.org/abs/2308.08693](http://arxiv.org/abs/2308.08693)

    本论文提出了一个名为PiZero的新方法，该方法使代理能够在自己创建的抽象搜索空间中进行高级规划，不受真实环境限制，可在任意时间尺度进行规划，并处理连续动作空间和部分可观察性的设置。在多个领域的实验中，该方法胜过可比的先前方法而无需假设访问环境模拟器。

    

    我们提出了一种新的方法，称为PiZero，它使代理能够在自己创建的抽象搜索空间中进行计划，该搜索空间与真实环境完全解耦。与先前的方法不同，这使得代理能够以任意时间尺度进行高级规划，并以复合或时间扩展动作的形式进行推理，这在需要执行大量基本微操作以执行相关宏操作的环境中非常有用。此外，我们的方法比可比的先前方法更通用，因为它处理具有连续动作空间和部分可观察性的设置。我们在多个领域进行了评估，包括导航任务和Sokoban。实验证明，它在没有假设访问环境模拟器的情况下胜过可比的先前方法。

    We propose a new method, called PiZero, that gives an agent the ability to plan in an abstract search space of its own creation that is completely decoupled from the real environment. Unlike prior approaches, this enables the agent to perform high-level planning at arbitrary timescales and reason in terms of compound or temporally-extended actions, which can be useful in environments where large numbers of base-level micro-actions are needed to perform relevant macro-actions. In addition, our method is more general than comparable prior methods because it handles settings with continuous action spaces and partial observability. We evaluate our method on multiple domains, including navigation tasks and Sokoban. Experimentally, it outperforms comparable prior methods without assuming access to an environment simulator.
    
[^43]: 量化过拟合: 引入过拟合指数

    Quantifying Overfitting: Introducing the Overfitting Index. (arXiv:2308.08682v1 [cs.LG])

    [http://arxiv.org/abs/2308.08682](http://arxiv.org/abs/2308.08682)

    本文引入了过拟合指数（OI），通过对乳腺超声图像数据集和MNIST数据集进行广泛实验，使用了多种架构，展示了OI的实用性和区分能力。结果表明，不同架构的过拟合行为存在差异，并强调了数据增强对于较小和更专业的数据集的缓解影响。OI为解决过拟合问题提供了一种有希望的途径。

    

    在快速发展的机器学习领域，确保模型的泛化能力仍然是一个重要的挑战。过拟合是指模型在训练数据上表现良好但在未见数据上表现不佳的现象，一直是个不容忽视的问题。本文引入了过拟合指数（OI），这是一个新颖的度量方法，用于定量评估模型的过拟合倾向。通过对乳腺超声图像数据集（BUS）和MNIST数据集的广泛实验，使用了MobileNet、U-Net、ResNet、Darknet和ViT-32等架构，我们展示了OI的实用性和区分能力。我们的结果突出了不同架构之间的过拟合行为的变化，并强调了数据增强对较小和更专业的数据集的缓解影响。ViT-32在MNIST上的表现进一步强调了某些模型的鲁棒性和数据集的全面性。通过提供客观视角来评估过拟合，OI为解决过拟合问题提供了一种有希望的途径。

    In the rapidly evolving domain of machine learning, ensuring model generalizability remains a quintessential challenge. Overfitting, where a model exhibits superior performance on training data but falters on unseen data, is a recurrent concern. This paper introduces the Overfitting Index (OI), a novel metric devised to quantitatively assess a model's tendency to overfit. Through extensive experiments on the Breast Ultrasound Images Dataset (BUS) and the MNIST dataset using architectures such as MobileNet, U-Net, ResNet, Darknet, and ViT-32, we illustrate the utility and discernment of the OI. Our results underscore the variable overfitting behaviors across architectures and highlight the mitigative impact of data augmentation, especially on smaller and more specialized datasets. The ViT-32's performance on MNIST further emphasizes the robustness of certain models and the dataset's comprehensive nature. By providing an objective lens to gauge overfitting, the OI offers a promising aven
    
[^44]: SkinDistilViT: 用于皮肤病变分类的轻量级视觉Transformer

    SkinDistilViT: Lightweight Vision Transformer for Skin Lesion Classification. (arXiv:2308.08669v1 [cs.CV])

    [http://arxiv.org/abs/2308.08669](http://arxiv.org/abs/2308.08669)

    本论文提出了一个用于皮肤病变分类的轻量级视觉Transformer模型，采用知识蒸馏方法，既提高了模型的性能，又大幅减少了内存和推理时间的开销。

    

    如果及早发现，皮肤癌是一种可治疗的疾病。我们提供了一个针对皮肤癌分类问题的生产专用解决方案，通过训练一种视觉Transformer来匹配专家注释的黑色素瘤医学图像中的人类表现。由于推理成本，在时间和内存方面都非常重要，我们采用知识蒸馏来获得一个在精度方面保留了98.33%的老师的平衡多类准确性的模型，成本只是老师模型的一小部分。就内存而言，我们的模型比老师模型小了49.60%。从时间上看，我们的解决方案在GPU上快了69.25%，在CPU上快了97.96%。通过在Transformer的每个层级上添加分类头并采用级联蒸馏过程，我们将基础模型的平衡多类准确性提高了2.1%，同时创建了一系列的模型，其尺寸各不相同但性能相当。我们在https://github.com/Longman-Stan/SkinDistilVit上提供了代码。

    Skin cancer is a treatable disease if discovered early. We provide a production-specific solution to the skin cancer classification problem that matches human performance in melanoma identification by training a vision transformer on melanoma medical images annotated by experts. Since inference cost, both time and memory wise is important in practice, we employ knowledge distillation to obtain a model that retains 98.33% of the teacher's balanced multi-class accuracy, at a fraction of the cost. Memory-wise, our model is 49.60% smaller than the teacher. Time-wise, our solution is 69.25% faster on GPU and 97.96% faster on CPU. By adding classification heads at each level of the transformer and employing a cascading distillation process, we improve the balanced multi-class accuracy of the base model by 2.1%, while creating a range of models of various sizes but comparable performance. We provide the code at https://github.com/Longman-Stan/SkinDistilVit.
    
[^45]: BREATHE: 基于二阶梯度和异方差模拟的设计空间探索

    BREATHE: Second-Order Gradients and Heteroscedastic Emulation based Design Space Exploration. (arXiv:2308.08666v1 [cs.LG])

    [http://arxiv.org/abs/2308.08666](http://arxiv.org/abs/2308.08666)

    BREATHE是一个基于二阶梯度和异方差模拟的设计空间探索的框架，能够高效地搜索传统的向量设计空间和基于图的设计空间，并在各自应用中取得了显著性能提升。

    

    研究人员在各种科学研究和物理实验中不断努力探索更大更复杂的搜索空间。然而，这些调查往往涉及复杂的模拟器或耗时的实验，使得探索和观察新的设计样本变得困难。旨在应对这些局限的先前工作通常是样本低效的，并且局限于向量搜索空间。为了解决这些问题，本文提出了一种约束多目标优化（MOO）框架，称为BREATHE，它不仅搜索传统的基于向量的设计空间，还搜索基于图的设计空间以获取表现最佳的图形。它利用二阶梯度并主动训练异方差模型以实现样本高效的优化。在单目标向量优化应用中，其性能比随机森林回归等下一个最佳基准模型提高了64.1％。在基于图的搜索中，BREATHE优于下一个最佳的基准模型。

    Researchers constantly strive to explore larger and more complex search spaces in various scientific studies and physical experiments. However, such investigations often involve sophisticated simulators or time-consuming experiments that make exploring and observing new design samples challenging. Previous works that target such applications are typically sample-inefficient and restricted to vector search spaces. To address these limitations, this work proposes a constrained multi-objective optimization (MOO) framework, called BREATHE, that searches not only traditional vector-based design spaces but also graph-based design spaces to obtain best-performing graphs. It leverages second-order gradients and actively trains a heteroscedastic surrogate model for sample-efficient optimization. In a single-objective vector optimization application, it leads to 64.1% higher performance than the next-best baseline, random forest regression. In graph-based search, BREATHE outperforms the next-bes
    
[^46]: Flickr Africa: 在大规模、人为中心的视觉数据中考察地理多样性

    Flickr Africa: Examining Geo-Diversity in Large-Scale, Human-Centric Visual Data. (arXiv:2308.08656v1 [cs.CV])

    [http://arxiv.org/abs/2308.08656](http://arxiv.org/abs/2308.08656)

    本研究通过分析与每个非洲国家相关的地理标签的Flickr图像，探讨了大规模、人为中心的视觉数据中的地理多样性。结果显示，现有数据存在偏见，并出现了“他者化”现象，因此需要更多研究来获取代表非洲人和他们环境的图像数据。

    

    已知大规模图像数据集中的偏见会影响计算机视觉模型在地理背景下的性能。为了调查标准互联网数据收集方法在低收入和中等收入国家的限制，我们使用与非洲每个国家相关的带有地理标签的Flickr图像来进行大规模的人为中心图像地理多样性分析。我们报告了可用数据的数量和内容，并与与之对应的欧洲国家进行了比较，以及根据细粒度国内财富估计的数据分布。我们进行了两年一次的时间分析，以揭示出现的数据趋势。此外，我们还呈现了一个“他者化”现象的发现，表明非洲大量图像是由非本地摄影师拍摄的。我们的研究结果表明，还需要进一步工作，以捕捉具有非洲人和他们环境代表性的图像数据。

    Biases in large-scale image datasets are known to influence the performance of computer vision models as a function of geographic context. To investigate the limitations of standard Internet data collection methods in low- and middle-income countries, we analyze human-centric image geo-diversity on a massive scale using geotagged Flickr images associated with each nation in Africa. We report the quantity and content of available data with comparisons to population-matched nations in Europe as well as the distribution of data according to fine-grained intra-national wealth estimates. Temporal analyses are performed at two-year intervals to expose emerging data trends. Furthermore, we present findings for an ``othering'' phenomenon as evidenced by a substantial number of images from Africa being taken by non-local photographers. The results of our study suggest that further work is required to capture image data representative of African people and their environments and, ultimately, to 
    
[^47]: 物理信息化循环神经网络用于非线性系统的地震响应评估

    Physics Informed Recurrent Neural Networks for Seismic Response Evaluation of Nonlinear Systems. (arXiv:2308.08655v1 [cs.LG])

    [http://arxiv.org/abs/2308.08655](http://arxiv.org/abs/2308.08655)

    物理信息化循环神经网络用于非线性系统的地震响应评估，通过利用机器学习和模式识别技术，在实时性能要求的应用中实现快速而准确的评估。

    

    结构工程中的动态响应评估是确定结构在地震、风或冲击等动态荷载作用下的响应，如构件力、节点位移等的过程。这是结构分析的重要方面，它使工程师能够评估在极端荷载条件下的结构性能，并对结构的设计和安全做出明智的决策。传统的动态响应评估方法涉及使用有限元分析（FEA）进行数值模拟，其中结构使用有限元进行建模，并通过数值求解运动方程。尽管这种方法有效，但计算开销大，并且可能不适用于实时应用。为了解决这些限制，最近在机器学习领域中，特别是人工神经网络中，已经将其应用于结构工程中的动态响应评估。这些技术利用网络的学习能力和模式识别能力来模拟结构响应，并能够在实时性能要求的应用中实现快速而准确的评估。

    Dynamic response evaluation in structural engineering is the process of determining the response of a structure, such as member forces, node displacements, etc when subjected to dynamic loads such as earthquakes, wind, or impact. This is an important aspect of structural analysis, as it enables engineers to assess structural performance under extreme loading conditions and make informed decisions about the design and safety of the structure. Conventional methods for dynamic response evaluation involve numerical simulations using finite element analysis (FEA), where the structure is modeled using finite elements, and the equations of motion are solved numerically. Although effective, this approach can be computationally intensive and may not be suitable for real-time applications. To address these limitations, recent advancements in machine learning, specifically artificial neural networks, have been applied to dynamic response evaluation in structural engineering. These techniques leve
    
[^48]: 用于稀疏高光谱丰度预测的重构核希尔伯特空间修剪

    Reproducing Kernel Hilbert Space Pruning for Sparse Hyperspectral Abundance Prediction. (arXiv:2308.08653v1 [cs.LG])

    [http://arxiv.org/abs/2308.08653](http://arxiv.org/abs/2308.08653)

    本论文提出了一种用于稀疏高光谱丰度预测的重构核希尔伯特空间修剪方法，通过非负最小二乘优化构建稀疏表示，并引入最大似然压缩向量减少信息损失。评估结果表明，该方法在光谱重建误差和压缩率方面相比标准修剪、最小二乘和深度学习方法具有更好的性能。

    

    长距离传感器获取的高光谱测量数据可以给出场景中物品、材料和化学品的详细情况，但由于先进传感器的高空间和光谱分辨率，分析变得困难、缓慢和昂贵。因此，稀疏性对于实现光谱压缩和分析的未来非常重要。已经观察到环境和大气效应，包括散射，会产生非线性效应，对于现有的源分离和压缩方法构成挑战。我们提出了一种新的转换方法，通过非负最小二乘优化在希尔伯特空间中进行修剪和构建稀疏表示。然后，我们引入了最大似然压缩向量来减少信息损失。我们的方法与标准修剪和最小二乘以及深度学习方法进行了基准测试。我们使用真实和合成数据评估了我们的方法，考察了总体光谱重建误差和压缩速率。

    Hyperspectral measurements from long range sensors can give a detailed picture of the items, materials, and chemicals in a scene but analysis can be difficult, slow, and expensive due to high spatial and spectral resolutions of state-of-the-art sensors. As such, sparsity is important to enable the future of spectral compression and analytics. It has been observed that environmental and atmospheric effects, including scattering, can produce nonlinear effects posing challenges for existing source separation and compression methods. We present a novel transformation into Hilbert spaces for pruning and constructing sparse representations via non-negative least squares minimization. Then we introduce max likelihood compression vectors to decrease information loss. Our approach is benchmarked against standard pruning and least squares as well as deep learning methods. Our methods are evaluated in terms of overall spectral reconstruction error and compression rate using real and synthetic dat
    
[^49]: AdaptEx：一个自助上下文赌博平台

    AdaptEx: A Self-Service Contextual Bandit Platform. (arXiv:2308.08650v1 [cs.IR])

    [http://arxiv.org/abs/2308.08650](http://arxiv.org/abs/2308.08650)

    AdaptEx是一个自助上下文赌博平台，通过利用多臂赌博算法个性化用户体验并提供最优解，同时最小化传统测试方法的成本和时间。它能够在不断变化的内容和“冷启动”情况下快速迭代。

    

    本文介绍了AdaptEx，这是一个在Expedia Group广泛使用的自助上下文赌博平台，它利用多臂赌博算法以规模化的方式个性化用户体验。AdaptEx考虑了每个访问者的独特上下文，选择了最优的变体，并能够快速学习每次互动。它提供了一个强大的解决方案，既能改善用户体验，同时又能最大限度地减少传统测试方法所需的成本和时间。该平台能够在内容不断变化和持续“冷启动”情况下，优雅地快速迭代朝着最优解前进。

    This paper presents AdaptEx, a self-service contextual bandit platform widely used at Expedia Group, that leverages multi-armed bandit algorithms to personalize user experiences at scale. AdaptEx considers the unique context of each visitor to select the optimal variants and learns quickly from every interaction they make. It offers a powerful solution to improve user experiences while minimizing the costs and time associated with traditional testing methods. The platform unlocks the ability to iterate towards optimal product solutions quickly, even in ever-changing content and continuous "cold start" situations gracefully.
    
[^50]: 通过异构模型重组实现个性化联邦学习

    Towards Personalized Federated Learning via Heterogeneous Model Reassembly. (arXiv:2308.08643v1 [cs.LG])

    [http://arxiv.org/abs/2308.08643](http://arxiv.org/abs/2308.08643)

    本文提出了一个名为pFedHR的新框架，利用异构模型重组实现个性化联邦学习。实验表明，pFedHR在各种设置下优于基准方法，并且能够有效降低使用不兼容数据的不良影响。

    

    本文针对联邦学习中模型异构的实际且具有挑战性的问题进行了研究，其中客户端具有不同网络结构的模型。为了解决这个问题，我们提出了一个名为pFedHR的新框架，利用异构模型重组实现个性化联邦学习。具体而言，我们将异构模型个性化问题视为服务器端的模型匹配优化任务。此外，pFedHR能够自动且动态地生成具有最小人工干预的信息丰富且多样化的个性化候选模型。此外，我们提出的异构模型重组技术在一定程度上减轻了使用具有不同分布的公共数据与客户端数据造成的不良影响。实验结果表明，在IID和非IID设置下，pFedHR在三个数据集上的性能优于基准方法。此外，pFedHR有效降低了使用不兼容数据的不良影响。

    This paper focuses on addressing the practical yet challenging problem of model heterogeneity in federated learning, where clients possess models with different network structures. To track this problem, we propose a novel framework called pFedHR, which leverages heterogeneous model reassembly to achieve personalized federated learning. In particular, we approach the problem of heterogeneous model personalization as a model-matching optimization task on the server side. Moreover, pFedHR automatically and dynamically generates informative and diverse personalized candidates with minimal human intervention. Furthermore, our proposed heterogeneous model reassembly technique mitigates the adverse impact introduced by using public data with different distributions from the client data to a certain extent. Experimental results demonstrate that pFedHR outperforms baselines on three datasets under both IID and Non-IID settings. Additionally, pFedHR effectively reduces the adverse impact of usi
    
[^51]: 非单调递增的顺序子模最大化问题研究

    Non-monotone Sequential Submodular Maximization. (arXiv:2308.08641v1 [cs.LG])

    [http://arxiv.org/abs/2308.08641](http://arxiv.org/abs/2308.08641)

    本文研究了顺序子模最大化问题，首次考虑了非单调子模函数，提出了针对灵活和固定长度约束的有效解决方案。

    

    本文研究了子模优化中的一个基本问题，即顺序子模最大化。具体而言，我们的目标是从一个待选集合V中选择并对一组k个项进行排序，使得k个（可能为非单调递增的）子模函数f1，...，fk（其中每个函数fj将这个序列的前j个项作为输入）的加权求和最大化。现有的研究主要集中在单调设置上，假设子模函数是非递减的。然而，在各种实际场景中，比如多样性感知的推荐系统中，向现有集合添加项可能会对整体效用产生负面影响。为此，本文首次研究了具有非单调子模函数的上述问题，并针对灵活和固定长度约束提出了有效的解决方案，以及一个特殊情况。

    In this paper, we study a fundamental problem in submodular optimization, which is called sequential submodular maximization. Specifically, we aim to select and rank a group of $k$ items from a ground set $V$ such that the weighted summation of $k$ (possibly non-monotone) submodular functions $f_1, \cdots ,f_k: 2^V \rightarrow \mathbb{R}^+$ is maximized, here each function $f_j$ takes the first $j$ items from this sequence as input. The existing research on sequential submodular maximization has predominantly concentrated on the monotone setting, assuming that the submodular functions are non-decreasing. However, in various real-world scenarios, like diversity-aware recommendation systems, adding items to an existing set might negatively impact the overall utility. In response, this paper pioneers the examination of the aforementioned problem with non-monotone submodular functions and offers effective solutions for both flexible and fixed length constraints, as well as a special case w
    
[^52]: 通过使用合成数据进行模型再平衡实现公平的生成对抗网络

    Fair GANs through model rebalancing with synthetic data. (arXiv:2308.08638v1 [cs.CV])

    [http://arxiv.org/abs/2308.08638](http://arxiv.org/abs/2308.08638)

    本论文提出了一种通过使用合成数据实现模型再平衡来减轻生成对抗网络中的偏见的方法。通过对现有失衡模型进行潜空间探索生成均衡数据，并使用这些数据训练均衡模型，同时提出了一种偏见减轻的损失函数。在种族公平的训练中，提出的方法在公平度指标上改进了近5倍。

    

    深度生成模型需要大量的训练数据，然而收集代表适当的分布的数据集往往既昂贵又困难，这导致数据集中存在偏见，进而在模型中进一步传播。我们提出了一种通过重新平衡模型分布来减轻现有生成对抗网络中的偏见的方法。我们通过对现有失衡深度生成模型进行潜空间探索生成均衡数据，并使用这些数据训练均衡的生成模型。此外，我们提出了一种偏见减轻损失函数，即使在使用失衡数据集进行训练时也能显示出公平度指标的改进。我们在使用FFHQ数据集进行种族公平训练的Stylegan2模型上展示了结果，并且发现所提出的方法在公平度指标上提升了近5倍，同时保持图像质量。

    Deep generative models require large amounts of training data. This often poses a problem as the collection of datasets can be expensive and difficult, in particular datasets that are representative of the appropriate underlying distribution (e.g. demographic). This introduces biases in datasets which are further propagated in the models. We present an approach to mitigate biases in an existing generative adversarial network by rebalancing the model distribution. We do so by generating balanced data from an existing unbalanced deep generative model using latent space exploration and using this data to train a balanced generative model. Further, we propose a bias mitigation loss function that shows improvements in the fairness metric even when trained with unbalanced datasets. We show results for the Stylegan2 models while training on the FFHQ dataset for racial fairness and see that the proposed approach improves on the fairness metric by almost 5 times, whilst maintaining image qualit
    
[^53]: FedPop: 联邦式基于人口的超参数调优

    FedPop: Federated Population-based Hyperparameter Tuning. (arXiv:2308.08634v1 [cs.LG])

    [http://arxiv.org/abs/2308.08634](http://arxiv.org/abs/2308.08634)

    FedPop是一种用于解决联邦学习中超参数调优问题的新算法，它采用基于人口的进化算法来优化客户端和服务器上的超参数。

    

    联邦学习（FL）是一种分布式机器学习（ML）范式，多个客户端在不集中本地数据的情况下共同训练ML模型。与传统的ML流程类似，FL中的客户端本地优化和服务器聚合过程对超参数（HP）的选择非常敏感。尽管在集中式ML中对调优HP进行了广泛研究，但将这些方法应用于FL时会产生次优结果。这主要是因为它们的“调优后训练”框架对于计算能力有限的FL不合适。虽然一些方法已经提出用于FL中的HP调优，但这些方法仅限于客户端本地更新的HP。在这项工作中，我们提出了一种名为联邦式基于人口的超参数调优（FedPop）的新型HP调优算法，以解决这个重要但具有挑战性的问题。FedPop采用基于人口的进化算法来优化HP，此算法适用于客户端和服务器上的各种HP类型。

    Federated Learning (FL) is a distributed machine learning (ML) paradigm, in which multiple clients collaboratively train ML models without centralizing their local data. Similar to conventional ML pipelines, the client local optimization and server aggregation procedure in FL are sensitive to the hyperparameter (HP) selection. Despite extensive research on tuning HPs for centralized ML, these methods yield suboptimal results when employed in FL. This is mainly because their "training-after-tuning" framework is unsuitable for FL with limited client computation power. While some approaches have been proposed for HP-Tuning in FL, they are limited to the HPs for client local updates. In this work, we propose a novel HP-tuning algorithm, called Federated Population-based Hyperparameter Tuning (FedPop), to address this vital yet challenging problem. FedPop employs population-based evolutionary algorithms to optimize the HPs, which accommodates various HP types at both client and server sides
    
[^54]: 基于LSTM的GRACE加速计数据预测模型

    LSTM-Based Forecasting Model for GRACE Accelerometer Data. (arXiv:2308.08621v1 [cs.LG])

    [http://arxiv.org/abs/2308.08621](http://arxiv.org/abs/2308.08621)

    本研究基于LSTM网络，提出了一种填充GRACE卫星加速计数据间断的方法，并成功预测了三个轴上的加速计数据。

    

    重力恢复与气候实验（GRACE）卫星任务从2002年到2017年，为监测地球重力场变化提供了宝贵的数据集，为地球物理学和水文学等多种应用提供了支持。随后，在2018年，GRACE Follow-On继续进行数据收集。从卫星上不同仪器的集成导出的月度地球重力场数据，由于多种因素的影响，包括GRACE任务开始以来某些仪器的观测间断，存在不一致性。鉴于现在已经有二十多年的GRACE和GRACE Follow-On数据可用，本文提出了一种填充数据间断并预测GRACE加速计数据的方法。具体而言，我们着重于加速计数据，并采用长短期记忆（LSTM）网络来训练一个能够预测三个轴上加速计数据的模型。在本研究中，我们描述了预处理加速计数据的方法论。

    The Gravity Recovery and Climate Experiment (GRACE) satellite mission, spanning from 2002 to 2017, has provided a valuable dataset for monitoring variations in Earth's gravity field, enabling diverse applications in geophysics and hydrology. The mission was followed by GRACE Follow-On in 2018, continuing data collection efforts. The monthly Earth gravity field, derived from the integration different instruments onboard satellites, has shown inconsistencies due to various factors, including gaps in observations for certain instruments since the beginning of the GRACE mission.  With over two decades of GRACE and GRACE Follow-On data now available, this paper proposes an approach to fill the data gaps and forecast GRACE accelerometer data. Specifically, we focus on accelerometer data and employ Long Short-Term Memory (LSTM) networks to train a model capable of predicting accelerometer data for all three axes.  In this study, we describe the methodology used to preprocess the accelerometer
    
[^55]: 通过新的框架——思维图，提升大型语言模型的逻辑推理能力

    Boosting Logical Reasoning in Large Language Models through a New Framework: The Graph of Thought. (arXiv:2308.08614v1 [cs.LG])

    [http://arxiv.org/abs/2308.08614](http://arxiv.org/abs/2308.08614)

    本文提出了一种新的提示技术——思维图（GoT），通过在三个不断升级的挑战中的测试，我们的方法在多步逻辑推理问题上表现优于GPT-4，并且相比最先进的提示方法思维树（ToT），我们的方法有更高的准确性提升。

    

    近期大规模模型（如GPT-4）的进展展示了在解决标准查询方面的卓越能力。然而，面对需要多步逻辑推理的复杂问题时，它们的准确性急剧下降。当前的研究已经探索了提示工程领域，以增强这些模型的推理能力。本文提出了一种创新的提示技术，称为“思维图（GoT）”。通过在三个不断升级的挑战上进行测试：24点游戏，高阶多项式方程的解析，以及递归数列的公式推导，我们的方法优于GPT-4，在每个任务中实现了$89.7\%$、$86\%$和$56\%$的准确性改进。此外，与最先进的提示方法“思维树（ToT）”相比，我们的方法平均准确性提升了$23\%$、$24\%$和$15\%$。

    Recent advancements in large-scale models, such as GPT-4, have showcased remarkable capabilities in addressing standard queries. However, when facing complex problems that require multi-step logical reasoning, their accuracy dramatically decreases. Current research has explored the realm of \textit{prompting engineering} to bolster the inferential capacities of these models. Our paper unveils a pioneering prompting technique, dubbed \textit{Graph of Thoughts (GoT)}. Through testing on a trio of escalating challenges: the 24-point game, resolution of high-degree polynomial equations, and derivation of formulas for recursive sequences, our method outperformed GPT-4, achieving accuracy improvements of $89.7\%$, $86\%$, and $56\%$ for each respective task. Moreover, when juxtaposed with the state-of-the-art (SOTA) prompting method, \textit{Tree of Thought (ToT)}, our approach registered an average accuracy boost of $23\%$, $24\%$, and $15\%$.
    
[^56]: 农业中整合可再生能源：基于深度强化学习的方法

    Integrating Renewable Energy in Agriculture: A Deep Reinforcement Learning-based Approach. (arXiv:2308.08611v1 [cs.AI])

    [http://arxiv.org/abs/2308.08611](http://arxiv.org/abs/2308.08611)

    本文开发了一个基于深度强化学习的框架，通过深度Q网络（DQN）优化光伏系统在农业中的安装决策制定。该研究对于提高能源效率、减少环境影响并增加农业利润具有重要意义。

    

    本文研究了使用深度Q网络（DQN）来优化农业领域光伏系统安装的决策制定。该研究开发了一个DQN框架，以帮助农业投资者根据安装预算、政府激励措施、能源需求、系统成本和长期效益等因素作出明智决策。通过实现一个奖励机制，DQN学习如何在光伏系统集成方面做出数据驱动的决策。该分析提供了深入了解DQN如何支持投资者在农业中进行光伏系统安装决策的综合理解。这项研究对于促进可持续和高效的农业实践，同时为未来该领域的进展铺平了道路具有重要意义。通过利用DQN，农业投资者可以做出优化决策，提高能源效率，减少环境影响，并增加利润。该研究对推动农业领域的进步做出了贡献。

    This article investigates the use of Deep Q-Networks (DQNs) to optimize decision-making for photovoltaic (PV) systems installations in the agriculture sector. The study develops a DQN framework to assist agricultural investors in making informed decisions considering factors such as installation budget, government incentives, energy requirements, system cost, and long-term benefits. By implementing a reward mechanism, the DQN learns to make data-driven decisions on PV integration. The analysis provides a comprehensive understanding of how DQNs can support investors in making decisions about PV installations in agriculture. This research has significant implications for promoting sustainable and efficient farming practices while also paving the way for future advancements in this field. By leveraging DQNs, agricultural investors can make optimized decisions that improve energy efficiency, reduce environmental impact, and enhance profitability. This study contributes to the advancement o
    
[^57]: PEvoLM: 蛋白质序列进化信息语言模型

    PEvoLM: Protein Sequence Evolutionary Information Language Model. (arXiv:2308.08578v1 [q-bio.QM])

    [http://arxiv.org/abs/2308.08578](http://arxiv.org/abs/2308.08578)

    PEvoLM是一种蛋白质序列进化信息语言模型，它利用嵌入式语言模型将蛋白质序列转换为数字向量表示。

    

    随着蛋白质序列数据库的指数增长，多序列比对(PSI-BLAST等)通过耗费时间的数据库搜索来获取进化信息。这些搜索引擎生成的位置特异性评分矩阵(PSSMs)对于生物信息学和计算生物学领域的许多机器学习模型来说是至关重要的输入。蛋白序列是由称为氨基酸(AAs)的连续标记或字符组成的。类比自然语言使我们能够利用自然语言处理(NLP)领域的最新进展，将NLP的最先进算法应用于生物信息学。本研究提出了一种嵌入式语言模型(ELMo)，将蛋白质序列转换为数字向量表示。尽管原始ELMo采用了一个两层双向长短期记忆网络(LSTMs)以及一个用于正向处理的双路径架构，

    With the exponential increase of the protein sequence databases over time, multiple-sequence alignment (MSA) methods, like PSI-BLAST, perform exhaustive and time-consuming database search to retrieve evolutionary information. The resulting position-specific scoring matrices (PSSMs) of such search engines represent a crucial input to many machine learning (ML) models in the field of bioinformatics and computational biology. A protein sequence is a collection of contiguous tokens or characters called amino acids (AAs). The analogy to natural language allowed us to exploit the recent advancements in the field of Natural Language Processing (NLP) and therefore transfer NLP state-of-the-art algorithms to bioinformatics. This research presents an Embedding Language Model (ELMo), converting a protein sequence to a numerical vector representation. While the original ELMo trained a 2-layer bidirectional Long Short-Term Memory (LSTMs) network following a two-path architecture, one for the forwar
    
[^58]: 自然启发的特征选择算法在预测学生表现中的能力的比较分析

    A Comparative Analysis of the Capabilities of Nature-inspired Feature Selection Algorithms in Predicting Student Performance. (arXiv:2308.08574v1 [cs.LG])

    [http://arxiv.org/abs/2308.08574](http://arxiv.org/abs/2308.08574)

    本研究对比分析了12种自然启发的特征选择算法在预测学生表现中的能力，发现利用这些算法进行特征选择并结合传统机器学习算法可以提高预测准确性，并减少特征集大小。

    

    预测学生表现对于有效防止风险学生失败至关重要。本文分析了一套12个自然启发算法在预测学生表现中的相对性能，包括基于实例的点击流数据、课内单一课程表现以及同时参加多个课程时的表现，发现利用自然启发的算法进行特征选择并结合传统机器学习算法进行分类，可以提高预测准确性，并减少特征集大小的2/3。

    Predicting student performance is key in leveraging effective pre-failure interventions for at-risk students. In this paper, I have analyzed the relative performance of a suite of 12 nature-inspired algorithms when used to predict student performance across 3 datasets consisting of instance-based clickstream data, intra-course single-course performance, and performance when taking multiple courses simultaneously. I found that, for all datasets, leveraging an ensemble approach using NIAs for feature selection and traditional ML algorithms for classification increased predictive accuracy while also reducing feature set size by 2/3.
    
[^59]: 一种基于物理信息的机器学习模型用于动态荷载重建

    A physics-informed machine learning model for reconstruction of dynamic loads. (arXiv:2308.08571v1 [cs.LG])

    [http://arxiv.org/abs/2308.08571](http://arxiv.org/abs/2308.08571)

    本文提出了一种基于物理信息的机器学习模型，用于根据测量数据重建动态荷载。该模型可处理不完整和受污染的数据，并提供自然的正则化方法来处理测量系统的噪声。

    

    长跨度桥梁在其寿命周期内受到多种动态激励。为了考虑这些激励对结构系统的影响，在设计过程中使用了多个载荷模型来模拟结构可能经历的条件。这些模型基于不同的简化假设，并通常由从测量数据中随机确定的参数引导，使其输出本质上不确定。本文提出了一种基于高斯过程回归的概率物理信息机器学习框架，用于根据测量的挠度、速度或加速度重建动态力。该模型可以处理不完整和受污染的数据，并提供了一种自然的正则化方法来考虑测量系统中的噪声。通过对大带东桥进行空气动力学分析，展示了该框架的应用。

    Long-span bridges are subjected to a multitude of dynamic excitations during their lifespan. To account for their effects on the structural system, several load models are used during design to simulate the conditions the structure is likely to experience. These models are based on different simplifying assumptions and are generally guided by parameters that are stochastically identified from measurement data, making their outputs inherently uncertain. This paper presents a probabilistic physics-informed machine-learning framework based on Gaussian process regression for reconstructing dynamic forces based on measured deflections, velocities, or accelerations. The model can work with incomplete and contaminated data and offers a natural regularization approach to account for noise in the measurement system. An application of the developed framework is given by an aerodynamic analysis of the Great Belt East Bridge. The aerodynamic response is calculated numerically based on the quasi-st
    
[^60]: CMISR：循环医学图像超分辨率

    CMISR: Circular Medical Image Super-Resolution. (arXiv:2308.08567v1 [eess.IV])

    [http://arxiv.org/abs/2308.08567](http://arxiv.org/abs/2308.08567)

    这里是中文总结出的一句话要点：本文提出了一种循环医学图像超分辨率方法（CMISR），采用全局反馈的闭环框架，具有明确的欠分辨率和超分辨率元素。CMISR在稳态下具有零恢复误差，且可以应用于现有的MISR算法。

    

    传统的医学图像超分辨率（MISR）方法采用隐式的欠分辨率（UR）单元和明确的超分辨率（SR）单元的开环架构。UR单元可以始终给出、假设或估计，而SR单元根据各种SR算法进行精心设计。闭环反馈机制广泛应用于当前的MISR方法，并能有效提高其性能。反馈机制可以分为两类：局部反馈和全局反馈。因此，本文提出了一种基于全局反馈的闭环框架，即循环MISR（CMISR），具有明确的UR和SR元素。建立CMISR的数学模型和闭环方程。通过泰勒级数逼近的数学证明表明，CMISR在稳态下具有零恢复误差。此外，CMISR具有即插即用的特性，可以建立在任何现有的MISR算法上。分别提出了五种CMISR算法。

    Classical methods of medical image super-resolution (MISR) utilize open-loop architecture with implicit under-resolution (UR) unit and explicit super-resolution (SR) unit. The UR unit can always be given, assumed, or estimated, while the SR unit is elaborately designed according to various SR algorithms. The closed-loop feedback mechanism is widely employed in current MISR approaches and can efficiently improve their performance. The feedback mechanism may be divided into two categories: local and global feedback. Therefore, this paper proposes a global feedback-based closed-cycle framework, circular MISR (CMISR), with unambiguous UR and SR elements. Mathematical model and closed-loop equation of CMISR are built. Mathematical proof with Taylor-series approximation indicates that CMISR has zero recovery error in steady-state. In addition, CMISR holds plug-and-play characteristic which can be established on any existing MISR algorithms. Five CMISR algorithms are respectively proposed bas
    
[^61]: KMF: 基于知识的多方位表示学习用于零样本节点分类

    KMF: Knowledge-Aware Multi-Faceted Representation Learning for Zero-Shot Node Classification. (arXiv:2308.08563v1 [cs.LG])

    [http://arxiv.org/abs/2308.08563](http://arxiv.org/abs/2308.08563)

    本文提出了一种基于知识的多方位框架（KMF），用于零样本节点分类任务。该框架通过提取知识图谱中的主题来增强标签语义，以改善模型的泛化能力。

    

    最近，零样本节点分类（ZNC）在图数据分析中变得越来越重要。该任务旨在预测在训练过程中未观察到的未知类别的节点。现有的工作主要利用图神经网络(GNNs)将特征的原型和标签的语义联系起来，从而实现从已观察到的类别到未观察到的类别的知识迁移。然而，以往的研究忽视了特征-语义对齐中多方位语义方向的存在，即节点的内容通常涵盖与多个标签的语义相关的不同主题。因此，有必要区分和判断影响认知能力的语义因素，以提高模型的泛化性能。为此，我们提出了一种基于知识的多方位框架（KMF），通过提取基于知识图谱（KG）的主题来增强标签语义的丰富性。然后，将每个节点的内容重构为主题级别的表示。

    Recently, Zero-Shot Node Classification (ZNC) has been an emerging and crucial task in graph data analysis. This task aims to predict nodes from unseen classes which are unobserved in the training process. Existing work mainly utilizes Graph Neural Networks (GNNs) to associate features' prototypes and labels' semantics thus enabling knowledge transfer from seen to unseen classes. However, the multi-faceted semantic orientation in the feature-semantic alignment has been neglected by previous work, i.e. the content of a node usually covers diverse topics that are relevant to the semantics of multiple labels. It's necessary to separate and judge the semantic factors that tremendously affect the cognitive ability to improve the generality of models. To this end, we propose a Knowledge-Aware Multi-Faceted framework (KMF) that enhances the richness of label semantics via the extracted KG (Knowledge Graph)-based topics. And then the content of each node is reconstructed to a topic-level repre
    
[^62]: 未来药物发现的实施：基于量子的机器学习模拟(QMLS)。

    Implementation of The Future of Drug Discovery: QuantumBased Machine Learning Simulation (QMLS). (arXiv:2308.08561v1 [q-bio.BM])

    [http://arxiv.org/abs/2308.08561](http://arxiv.org/abs/2308.08561)

    该论文介绍了一种名为QMLS的新概念，通过结合机器学习和量子模拟的方法，可以缩短药物研发的时间和降低成本。通过生成命中物和优化分子的过程，可以大大提高药物发现的效率。

    

    药物研发的研究与开发(R&D)阶段是一个漫长而昂贵的过程。为了改革这个过程，我们引入了新概念QMLS，将整个R&D阶段缩短到三到六个月，成本仅为五到八万美元。对于命中产生，机器学习分子生成(MLMG)根据目标蛋白的分子结构生成可能的命中物，而量子模拟(QS)根据与目标蛋白的反应和结合效果过滤原始实验中的分子。然后，对于铅优化，从MLMG和QS生成和过滤的结果分子进行比较，并通过机器学习分子变异(MLMV)将那些出现在两个过程中的分子制成数十种分子变体，而其他分子只制成几种变体。最后，所有优化的分子将经过多轮高标准的QS过滤，以确保反应效果。

    The Research & Development (R&D) phase of drug development is a lengthy and costly process. To revolutionize this process, we introduce our new concept QMLS to shorten the whole R&D phase to three to six months and decrease the cost to merely fifty to eighty thousand USD. For Hit Generation, Machine Learning Molecule Generation (MLMG) generates possible hits according to the molecular structure of the target protein while the Quantum Simulation (QS) filters molecules from the primary essay based on the reaction and binding effectiveness with the target protein. Then, For Lead Optimization, the resultant molecules generated and filtered from MLMG and QS are compared, and molecules that appear as a result of both processes will be made into dozens of molecular variations through Machine Learning Molecule Variation (MLMV), while others will only be made into a few variations. Lastly, all optimized molecules would undergo multiple rounds of QS filtering with a high standard for reaction ef
    
[^63]: AI辅助调查区块链参数：风险加密货币和价格因素

    AI-Assisted Investigation of On-Chain Parameters: Risky Cryptocurrencies and Price Factors. (arXiv:2308.08554v1 [q-fin.ST])

    [http://arxiv.org/abs/2308.08554](http://arxiv.org/abs/2308.08554)

    本论文使用人工智能算法分析历史数据和区块链参数，找出影响加密货币价格的因素并识别风险加密货币。

    

    近年来，加密货币已成为投资者和学者广泛研究的热门话题。为了做出明智的投资决策，了解影响加密货币价格的因素并识别风险加密货币是至关重要的。本文重点分析历史数据，并使用人工智能算法对区块链参数进行分析，以识别影响加密货币价格的因素并找到风险加密货币。我们对历史加密货币的链上数据进行分析，并测量了价格与其他参数之间的相关性。此外，我们使用聚类和分类方法来更好地理解加密货币，并将其分类为风险或非风险。分析结果显示，大部分加密货币（39％）已退出市场，而只有很小一部分（10％）存活了1000多天。我们的分析揭示了一个显著的负相关关系

    Cryptocurrencies have become a popular and widely researched topic of interest in recent years for investors and scholars. In order to make informed investment decisions, it is essential to comprehend the factors that impact cryptocurrency prices and to identify risky cryptocurrencies. This paper focuses on analyzing historical data and using artificial intelligence algorithms on on-chain parameters to identify the factors affecting a cryptocurrency's price and to find risky cryptocurrencies. We conducted an analysis of historical cryptocurrencies' on-chain data and measured the correlation between the price and other parameters. In addition, we used clustering and classification in order to get a better understanding of a cryptocurrency and classify it as risky or not. The analysis revealed that a significant proportion of cryptocurrencies (39%) disappeared from the market, while only a small fraction (10%) survived for more than 1000 days. Our analysis revealed a significant negative
    
[^64]: 具有更灵活记忆的递归神经网络：比粗糙波动性有更好的预测能力

    Recurrent Neural Networks with more flexible memory: better predictions than rough volatility. (arXiv:2308.08550v1 [q-fin.ST])

    [http://arxiv.org/abs/2308.08550](http://arxiv.org/abs/2308.08550)

    本论文提出了具有更灵活记忆的递归神经网络，能够更好地预测具有长期记忆的资产价格波动性，优于传统的粗糙波动性预测方法。

    

    我们扩展了递归神经网络，为其输出的每个维度包括多个灵活的时间尺度，从而机械地改善了其对具有长期记忆或高度不同时间尺度的过程的建模能力。我们比较了普通的长短期记忆网络（LSTMs）和扩展的LSTMs在预测具有长期记忆的资产价格波动性方面的能力。通常情况下，训练扩展的LSTMs所需的时期数减少了一半，而具有相同超参数的模型在验证和测试损失的变化较小。我们还展示了，在使用多个时间序列的数据集进行训练和测试时，验证损失最小的模型比粗糙波动性预测的准确率高约20%。

    We extend recurrent neural networks to include several flexible timescales for each dimension of their output, which mechanically improves their abilities to account for processes with long memory or with highly disparate time scales. We compare the ability of vanilla and extended long short term memory networks (LSTMs) to predict asset price volatility, known to have a long memory. Generally, the number of epochs needed to train extended LSTMs is divided by two, while the variation of validation and test losses among models with the same hyperparameters is much smaller. We also show that the model with the smallest validation loss systemically outperforms rough volatility predictions by about 20% when trained and tested on a dataset with multiple time series.
    
[^65]: 每日新闻情绪对股票价格预测的影响

    Effects of Daily News Sentiment on Stock Price Forecasting. (arXiv:2308.08549v1 [q-fin.ST])

    [http://arxiv.org/abs/2308.08549](http://arxiv.org/abs/2308.08549)

    本研究探讨了每日新闻情绪对股票价格预测的影响，并设计了一个高效的系统从新闻中提取情绪影响因素。研究发现投资者情绪对股票价格波动有显著影响。

    

    预测股票的未来价格是一项艰巨的任务。然而，引入额外的因素可以显著改善我们的预测，而不仅仅依靠股票的历史价格数据来预测其未来价格。研究表明，投资者情绪受到有关公司的每日新闻的影响，可以对股票价格波动产生重大影响。虽然有很多信息来源可以获取这些信息，但它们往往充斥着大量的噪声，使得准确提取情感变得困难。因此，我们的研究重点是设计一个高效的系统，从关于NITY50股票的新闻中获取情绪，并研究这些股票的财经新闻情绪在一段时间内对其价格的影响程度。本文介绍了一个强大的数据收集和预处理框架，用于创建一个历时约3.7年的新闻数据库，包含了近50万条新闻数据。

    Predicting future prices of a stock is an arduous task to perform. However, incorporating additional elements can significantly improve our predictions, rather than relying solely on a stock's historical price data to forecast its future price. Studies have demonstrated that investor sentiment, which is impacted by daily news about the company, can have a significant impact on stock price swings. There are numerous sources from which we can get this information, but they are cluttered with a lot of noise, making it difficult to accurately extract the sentiments from them. Hence the focus of our research is to design an efficient system to capture the sentiments from the news about the NITY50 stocks and investigate how much the financial news sentiment of these stocks are affecting their prices over a period of time. This paper presents a robust data collection and preprocessing framework to create a news database for a timeline of around 3.7 years, consisting of almost half a million n
    
[^66]: 两个半顺序基于得分的模型用于解决三维不适定反问题

    Two-and-a-half Order Score-based Model for Solving 3D Ill-posed Inverse Problems. (arXiv:2308.08511v1 [eess.IV])

    [http://arxiv.org/abs/2308.08511](http://arxiv.org/abs/2308.08511)

    提出了一种两个半顺序基于得分的模型（TOSM），用于解决CT和MRI中的三维体重建问题。通过在二维空间中学习数据分布来减少训练的复杂性，然后在三维空间中更新数据分布以实现准确的三维重建。

    

    计算机断层扫描（CT）和磁共振成像（MRI）是医学成像领域至关重要的技术。基于得分的模型已经被证明在解决CT和MRI中遇到的不同反问题（如稀疏视野CT和快速MRI重建）方面是有效的。然而，这些模型在实现精确的三维体重建方面面临挑战。现有的基于得分的模型主要关注重建二维数据分布，在重建的三维体积图像中导致相邻切片之间的不一致性。为了克服这一限制，我们提出了一种新颖的两个半顺序基于得分的模型（TOSM）。在训练阶段，我们的TOSM在二维空间中学习数据分布，相较于直接在三维体积上工作，减少了训练的复杂性。然而，在重建阶段，TOSM会在三维空间中更新数据分布，利用三个方向的互补得分（sag）

    Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) are crucial technologies in the field of medical imaging. Score-based models have proven to be effective in addressing different inverse problems encountered in CT and MRI, such as sparse-view CT and fast MRI reconstruction. However, these models face challenges in achieving accurate three dimensional (3D) volumetric reconstruction. The existing score-based models primarily focus on reconstructing two dimensional (2D) data distribution, leading to inconsistencies between adjacent slices in the reconstructed 3D volumetric images. To overcome this limitation, we propose a novel two-and-a-half order score-based model (TOSM). During the training phase, our TOSM learns data distributions in 2D space, which reduces the complexity of training compared to directly working on 3D volumes. However, in the reconstruction phase, the TOSM updates the data distribution in 3D space, utilizing complementary scores along three directions (sag
    
[^67]: CDR：用于去偏推荐的保守双重稳健学习

    CDR: Conservative Doubly Robust Learning for Debiased Recommendation. (arXiv:2308.08461v1 [cs.IR])

    [http://arxiv.org/abs/2308.08461](http://arxiv.org/abs/2308.08461)

    该论文提出了一种保守双重稳健策略（CDR），用于解决推荐系统中存在的有毒插补问题。CDR通过审查插补的均值和方差来过滤插补，结果显示CDR具有降低方差和改进尾部界限的优势，并且能够显著提升性能并减少有毒插补的频率。

    

    在推荐系统中，用户行为数据往往是观察性的而不是实验性的，导致数据中普遍存在偏差。因此，解决偏差问题已成为推荐系统领域的一个重要挑战。最近，双重稳健学习（DR）由于其卓越的性能和稳健的特性而受到了广泛关注。然而，我们的实验结果表明，现有的DR方法在存在所谓的有毒插补（Poisonous Imputation）时受到严重影响，插补明显偏离真实数据并适得其反。为了解决这个问题，本文提出了一种保守双重稳健策略（CDR），通过审查插补的均值和方差来过滤插补。理论分析表明，CDR可以降低方差并改进尾部界限。此外，我们的实验研究表明，CDR显著提升了性能，并且确实减少了有毒插补的频率。

    In recommendation systems (RS), user behavior data is observational rather than experimental, resulting in widespread bias in the data. Consequently, tackling bias has emerged as a major challenge in the field of recommendation systems. Recently, Doubly Robust Learning (DR) has gained significant attention due to its remarkable performance and robust properties. However, our experimental findings indicate that existing DR methods are severely impacted by the presence of so-called Poisonous Imputation, where the imputation significantly deviates from the truth and becomes counterproductive.  To address this issue, this work proposes Conservative Doubly Robust strategy (CDR) which filters imputations by scrutinizing their mean and variance. Theoretical analyses show that CDR offers reduced variance and improved tail bounds.In addition, our experimental investigations illustrate that CDR significantly enhances performance and can indeed reduce the frequency of poisonous imputation.
    
[^68]: 分类问题中的精确度和召回率拒绝曲线

    Precision and Recall Reject Curves for Classification. (arXiv:2308.08381v1 [cs.LG])

    [http://arxiv.org/abs/2308.08381](http://arxiv.org/abs/2308.08381)

    该论文提出了一种在分类问题中评估精确度和召回率的拒绝曲线方法。使用感知量化的原型分类器来验证了该方法在不平衡数据集和医学实际数据上的有效性。

    

    在某些分类场景中，只使用模型高度确定的分类实例是可取的。为了获得这些高度确定的实例，先前的研究提出了准确度拒绝曲线。拒绝曲线允许评估和比较不同的确定度度量在接受或拒绝分类的一系列阈值上的性能。然而，准确度可能并不适合所有应用程序的评估指标，相反，精确度或召回率可能更可取。这是因为在存在类别分布不平衡的数据中，例如，在不平衡的基准数据和医学实际数据中，我们提出了评估精确度和召回率的拒绝曲线：召回率-拒绝曲线和精确度-拒绝曲线。通过学习向量量化中的原型分类器，我们首先在人工基准数据上将提出的曲线与准确度拒绝曲线作为基准进行验证。然后，我们在存在类别分布不平衡的基准数据和医学实际数据上进行展示。

    For some classification scenarios, it is desirable to use only those classification instances that a trained model associates with a high certainty. To obtain such high-certainty instances, previous work has proposed accuracy-reject curves. Reject curves allow to evaluate and compare the performance of different certainty measures over a range of thresholds for accepting or rejecting classifications. However, the accuracy may not be the most suited evaluation metric for all applications, and instead precision or recall may be preferable. This is the case, for example, for data with imbalanced class distributions. We therefore propose reject curves that evaluate precision and recall, the recall-reject curve and the precision-reject curve. Using prototype-based classifiers from learning vector quantization, we first validate the proposed curves on artificial benchmark data against the accuracy reject curve as a baseline. We then show on imbalanced benchmarks and medical, real-world data 
    
[^69]: HyperSNN：一种适用于资源受限控制应用的高效稳健深度学习模型

    HyperSNN: A new efficient and robust deep learning model for resource constrained control applications. (arXiv:2308.08222v1 [cs.RO])

    [http://arxiv.org/abs/2308.08222](http://arxiv.org/abs/2308.08222)

    HyperSNN是一种适用于资源受限控制应用的高效稳健深度学习模型，通过使用脉冲神经网络和高维计算，将能量消耗降低至1.36%-9.96%，同时提高了鲁棒性和准确性。它适用于交互式、移动和可穿戴设备，促进了能量高效和稳健的系统设计。

    

    鉴于边缘计算在智能家具、机器人和智能家居等领域的日益采用，本文介绍了一种创新的控制任务方法HyperSNN，它使用脉冲神经网络（SNNs）与高维计算相结合。 HyperSNN用8位整数加法替代了昂贵的32位浮点乘法，从而降低了能量消耗，同时增强了稳健性和可能提高准确性。我们的模型在AI Gym基准测试中进行了测试，包括Cartpole、Acrobot、MountainCar和Lunar Lander。 HyperSNN实现了与传统机器学习方法相当的控制准确性，但仅消耗1.36％至9.96％的能量开销。此外，我们的实验显示，使用HyperSNN可以提高鲁棒性。我们认为HyperSNN特别适用于交互式、移动和可穿戴设备，促进了能量高效和稳健的系统设计。而且，它为.....

    In light of the increasing adoption of edge computing in areas such as intelligent furniture, robotics, and smart homes, this paper introduces HyperSNN, an innovative method for control tasks that uses spiking neural networks (SNNs) in combination with hyperdimensional computing. HyperSNN substitutes expensive 32-bit floating point multiplications with 8-bit integer additions, resulting in reduced energy consumption while enhancing robustness and potentially improving accuracy. Our model was tested on AI Gym benchmarks, including Cartpole, Acrobot, MountainCar, and Lunar Lander. HyperSNN achieves control accuracies that are on par with conventional machine learning methods but with only 1.36% to 9.96% of the energy expenditure. Furthermore, our experiments showed increased robustness when using HyperSNN. We believe that HyperSNN is especially suitable for interactive, mobile, and wearable devices, promoting energy-efficient and robust system design. Furthermore, it paves the way for th
    
[^70]: 带有通信压缩的随机控制平均法在联邦学习中的应用

    Stochastic Controlled Averaging for Federated Learning with Communication Compression. (arXiv:2308.08165v1 [math.OC] CROSS LISTED)

    [http://arxiv.org/abs/2308.08165](http://arxiv.org/abs/2308.08165)

    本文提出了两种压缩联邦学习算法(SCALLION和SCAFCOM)，通过重新审视经典的随机控制平均法并提出了等价但更高效/简化的形式，减少了上行通信成本。

    

    通信压缩是一种旨在减少通过无线传输的信息量的技术，在联邦学习中引起了极大的关注，因为它有潜力减轻通信开销。然而，通信压缩在联邦学习中带来了新的挑战，包括压缩引起的信息失真以及联邦学习的特性，如部分参与和数据异构性。尽管近年来有所发展，压缩联邦学习方法的性能尚未充分利用。现有方法要么不能适应任意的数据异构性或部分参与，要么要求对压缩有严格的条件。在本文中，我们重新审视了具有开销减半的上行通信成本的经典随机控制平均法，并提出了两种压缩联邦学习算法，SCALLION和SCAFCOM。

    Communication compression, a technique aiming to reduce the information volume to be transmitted over the air, has gained great interests in Federated Learning (FL) for the potential of alleviating its communication overhead. However, communication compression brings forth new challenges in FL due to the interplay of compression-incurred information distortion and inherent characteristics of FL such as partial participation and data heterogeneity. Despite the recent development, the performance of compressed FL approaches has not been fully exploited. The existing approaches either cannot accommodate arbitrary data heterogeneity or partial participation, or require stringent conditions on compression.  In this paper, we revisit the seminal stochastic controlled averaging method by proposing an equivalent but more efficient/simplified formulation with halved uplink communication costs. Building upon this implementation, we propose two compressed FL algorithms, SCALLION and SCAFCOM, to s
    
[^71]: 在大型语言模型中使用反向推理进行验证

    Backward Reasoning in Large Language Models for Verification. (arXiv:2308.07758v1 [cs.CL])

    [http://arxiv.org/abs/2308.07758](http://arxiv.org/abs/2308.07758)

    本文研究了在大型语言模型中使用反向推理进行验证的方法。作者提出了一种新颖的技术，通过屏蔽问题中的一个标记，并要求语言模型预测被屏蔽的标记来验证候选答案。同时，作者还提出了一种结合正向和反向推理的方法来估计候选答案的概率。

    

    链式思考（Chain-of-Though, CoT）提示在各种推理任务中表现出了很好的性能。最近，Self-Consistency提出了一种方法，即通过采样一组不同的推理链，这些链可能导致不同的答案，然后选择得票最多的答案。本文提出了一种新颖的方法，即在验证候选答案时使用反向推理。我们使用一个简单的模板，即``如果我们知道上述问题的答案是候选答案，那么未知变量x的值是多少？''，将问题中的一个标记屏蔽，并要求语言模型预测被屏蔽的标记。直观上讲，如果提供的候选答案是正确的，语言模型应该能够成功预测被屏蔽的标记。我们进一步提出了FOBAR方法，将正向和反向推理结合起来估计候选答案的概率。我们在六个数据集和三个实验中进行了广泛的实验。

    Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by ${\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \textit{a simple template}, i.e., ``\textit{\textbf{If we know the answer of the above question is \{a candidate answer\}, what is the value of unknown variable ${\bf x}$?}}'' Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three
    
[^72]: 使用去噪扩散的声音事件检测（DiffSED）

    DiffSED: Sound Event Detection with Denoising Diffusion. (arXiv:2308.07293v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2308.07293](http://arxiv.org/abs/2308.07293)

    本论文提出了一种使用去噪扩散的声音事件检测方法，通过从噪声查询中生成准确的事件边界，实验表明该方法明显优于现有方法。

    

    声音事件检测（SED）旨在在给定无约束音频样本的情况下，预测感兴趣事件的时间边界和类别标签。现有的所有方法都是从鉴别学习的角度考虑SED问题，采用分割和分类（即帧级）策略或更有原则的事件级建模方法。在这项工作中，我们通过采取生成学习的角度来重新定义SED问题。具体而言，我们的目标是在密集扩散过程中，基于目标音频样本，从噪声提议中生成准确的声音时间边界。在训练过程中，我们的模型通过在优雅的Transformer解码器框架中将噪声潜在查询转换为地面真实版本，从而学习逆转噪声过程。通过这样做，即使在推断过程中使用噪声查询，模型也能生成准确的事件边界。在Urban-SED和EPIC-Sounds数据集上进行了大量实验证明，我们的模型明显优于现有方法。

    Sound Event Detection (SED) aims to predict the temporal boundaries of all the events of interest and their class labels, given an unconstrained audio sample. Taking either the splitand-classify (i.e., frame-level) strategy or the more principled event-level modeling approach, all existing methods consider the SED problem from the discriminative learning perspective. In this work, we reformulate the SED problem by taking a generative learning perspective. Specifically, we aim to generate sound temporal boundaries from noisy proposals in a denoising diffusion process, conditioned on a target audio sample. During training, our model learns to reverse the noising process by converting noisy latent queries to the groundtruth versions in the elegant Transformer decoder framework. Doing so enables the model generate accurate event boundaries from even noisy queries during inference. Extensive experiments on the Urban-SED and EPIC-Sounds datasets demonstrate that our model significantly outpe
    
[^73]: AudioFormer: 通过离散的声学代码学习音频特征表示的音频变换器

    AudioFormer: Audio Transformer learns audio feature representations from discrete acoustic codes. (arXiv:2308.07221v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2308.07221](http://arxiv.org/abs/2308.07221)

    AudioFormer是一种学习音频特征表示的方法，通过生成离散的声学代码并利用它们来训练掩码语言模型，从而将音频分类任务视为自然语言理解的形式。此外，引入了多正样本对比学习方法，通过学习联合表示来捕捉音频中的相关性。

    

    我们提出了一种名为AudioFormer的方法，通过获取离散的声学代码来学习音频特征表示，并随后对其进行微调以用于音频分类任务。我们首先将音频分类任务视为一种自然语言理解 (NLU) 的形式，借助现有的神经音频编解码模型，我们生成了离散的声学代码，并利用它们来训练一个掩码语言模型 (MLM)，从而获得音频特征表示。此外，我们首创了一种多正样本对比 (MPC) 学习方法的整合，该方法能够学习同一音频输入中多个离散声学代码间的联合表示。在实验中，我们将离散的声学代码视为文本数据，并使用类似填空题的方法训练一个掩码语言模型，最终得到高质量的音频表示。值得注意的是，MPC学习技术能够有效捕捉到音频中的相关性。

    We propose a method named AudioFormer,which learns audio feature representations through the acquisition of discrete acoustic codes and subsequently fine-tunes them for audio classification tasks. Initially,we introduce a novel perspective by considering the audio classification task as a form of natural language understanding (NLU). Leveraging an existing neural audio codec model,we generate discrete acoustic codes and utilize them to train a masked language model (MLM),thereby obtaining audio feature representations. Furthermore,we pioneer the integration of a Multi-Positive sample Contrastive (MPC) learning approach. This method enables the learning of joint representations among multiple discrete acoustic codes within the same audio input. In our experiments,we treat discrete acoustic codes as textual data and train a masked language model using a cloze-like methodology,ultimately deriving high-quality audio representations. Notably,the MPC learning technique effectively captures c
    
[^74]: 智能农业中的基础模型：基础知识、机遇和挑战

    Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges. (arXiv:2308.06668v1 [cs.LG])

    [http://arxiv.org/abs/2308.06668](http://arxiv.org/abs/2308.06668)

    传统农业系统中的机器学习和深度学习模型存在局限性，而基础模型在语言和视觉任务中表现出了显著的成功。本研究旨在探索基础模型在智能农业领域的潜力和应用。

    

    过去十年间，农业系统中的机器学习和深度学习方法得到了快速发展，展示出在各种农业应用中取得了巨大成功。然而，这些传统的机器学习/深度学习模型具有一定的局限性：它们严重依赖于昂贵的、难以获取的标记数据集进行训练，需要专业知识进行开发和维护，而且大多针对特定任务，缺乏泛化能力。最近，基础模型在语言和视觉任务中展示出了显著的成功，跨越了各个领域。这些模型在来自多个领域和模态的大量数据上进行训练。一旦训练完成，它们可以通过微调和少量特定任务的标记数据完成各种多样的任务。尽管基础模型已经证明了其有效性和巨大潜力，但在农业领域中应用尚未有太多探索。因此，本研究旨在探索基础模型在智能农业领域的潜力。

    The past decade has witnessed the rapid development of ML and DL methodologies in agricultural systems, showcased by great successes in variety of agricultural applications. However, these conventional ML/DL models have certain limitations: They heavily rely on large, costly-to-acquire labeled datasets for training, require specialized expertise for development and maintenance, and are mostly tailored for specific tasks, thus lacking generalizability. Recently, foundation models have demonstrated remarkable successes in language and vision tasks across various domains. These models are trained on a vast amount of data from multiple domains and modalities. Once trained, they can accomplish versatile tasks with just minor fine-tuning and minimal task-specific labeled data. Despite their proven effectiveness and huge potential, there has been little exploration of applying FMs to agriculture fields. Therefore, this study aims to explore the potential of FMs in the field of smart agricultu
    
[^75]: 能否通过非结构化剪枝来减少深度神经网络的层数？

    Can Unstructured Pruning Reduce the Depth in Deep Neural Networks?. (arXiv:2308.06619v1 [cs.LG])

    [http://arxiv.org/abs/2308.06619](http://arxiv.org/abs/2308.06619)

    本研究介绍了一种名为EGP的创新的熵引导剪枝算法，该算法能够通过优先剪除熵较低的层中的连接来有效压缩深度神经网络，同时保持其竞争性能水平。

    

    剪枝是一种广泛使用的技术，可以减小深度神经网络的大小，同时保持其性能。然而，即使是在有结构的情况下，这种技术也很难从模型中完全去除整个层：这是一个可以解决的任务吗？在这项研究中，我们引入了一种名为EGP的创新的熵引导剪枝算法，旨在减小深度神经网络的大小，同时保持其性能。EGP的关键重点是优先剪除熵较低的层中的连接，最终完全去除这些层。通过在ResNet-18和Swin-T等流行模型上进行大量实验，我们的研究结果表明，EGP能够有效压缩深度神经网络，同时保持竞争性能水平。我们的结果不仅揭示了非结构化剪枝优势背后的机制，还为进一步研究复杂的关系铺平了道路。

    Pruning is a widely used technique for reducing the size of deep neural networks while maintaining their performance. However, such a technique, despite being able to massively compress deep models, is hardly able to remove entire layers from a model (even when structured): is this an addressable task? In this study, we introduce EGP, an innovative Entropy Guided Pruning algorithm aimed at reducing the size of deep neural networks while preserving their performance. The key focus of EGP is to prioritize pruning connections in layers with low entropy, ultimately leading to their complete removal. Through extensive experiments conducted on popular models like ResNet-18 and Swin-T, our findings demonstrate that EGP effectively compresses deep neural networks while maintaining competitive performance levels. Our results not only shed light on the underlying mechanism behind the advantages of unstructured pruning, but also pave the way for further investigations into the intricate relations
    
[^76]: 镜像扩散模型

    Mirror Diffusion Models. (arXiv:2308.06342v1 [cs.LG])

    [http://arxiv.org/abs/2308.06342](http://arxiv.org/abs/2308.06342)

    本文提出了镜像扩散模型(MDMs)，用于在离散分类数据和连续领域中进行生成任务。MDMs受限制抽样问题的镜像Langevin算法启发，并提供了适应简单扩散、图像生成和文本生成等领域的自然扩展。

    

    扩散模型在各种连续领域的生成任务中取得了成功。然而，在离散的分类数据中应用扩散仍然是一个非平凡的任务。此外，在连续领域的生成中常常需要进行剪切，这就需要一个将扩散适应约束域的理论框架。受限制抽样问题的镜像Langevin算法的启发，本理论报告中我们提出了镜像扩散模型(MDMs)。我们演示了MDMs在simplex扩散的背景下，并提出了对图像和文本生成等热门领域的自然扩展。

    Diffusion models have successfully been applied to generative tasks in various continuous domains. However, applying diffusion to discrete categorical data remains a non-trivial task. Moreover, generation in continuous domains often requires clipping in practice, which motivates the need for a theoretical framework for adapting diffusion to constrained domains. Inspired by the mirror Langevin algorithm for the constrained sampling problem, in this theoretical report we propose Mirror Diffusion Models (MDMs). We demonstrate MDMs in the context of simplex diffusion and propose natural extensions to popular domains such as image and text generation.
    
[^77]: 在内存层次结构上具有MiRo的成本效益的设备上的持续学习

    Cost-effective On-device Continual Learning over Memory Hierarchy with Miro. (arXiv:2308.06053v1 [cs.LG])

    [http://arxiv.org/abs/2308.06053](http://arxiv.org/abs/2308.06053)

    这项工作是首次探索基于层次内存回放的持续学习的设计空间，旨在在边缘设备上实现成本效益。提出了Miro，一个通过动态配置持续学习系统的新颖系统运行时，以实现最佳的成本效益。广泛的评估显示Miro明显优于其他方案。

    

    持续学习是从持续的任务流中逐步训练神经网络模型。为了记住先前学到的知识，之前的研究将旧样本存储在一个内存层次结构中，并在新任务到来时进行回放。采用持续学习以保护数据隐私的边缘设备通常对能源敏感，因此需要在不损害能源效率的情况下保持高模型准确度，即成本效益。我们的工作是首次探索基于层次内存回放的持续学习的设计空间，以获得在边缘设备上的成本效益。我们提出了Miro，一个新颖的系统运行时，通过使其能够根据资源状态动态配置持续学习系统，从而将我们的见解精确地整合到持续学习框架中，以实现最佳成本效益。为了实现这个目标，Miro还对带有明确准确度-能量平衡的参数进行在线分析，并以低开销地适应最佳值。广泛的评估显示Miro明显优于其他方案。

    Continual learning (CL) trains NN models incrementally from a continuous stream of tasks. To remember previously learned knowledge, prior studies store old samples over a memory hierarchy and replay them when new tasks arrive. Edge devices that adopt CL to preserve data privacy are typically energy-sensitive and thus require high model accuracy while not compromising energy efficiency, i.e., cost-effectiveness. Our work is the first to explore the design space of hierarchical memory replay-based CL to gain insights into achieving cost-effectiveness on edge devices. We present Miro, a novel system runtime that carefully integrates our insights into the CL framework by enabling it to dynamically configure the CL system based on resource states for the best cost-effectiveness. To reach this goal, Miro also performs online profiling on parameters with clear accuracy-energy trade-offs and adapts to optimal values with low overhead. Extensive evaluations show that Miro significantly outperfo
    
[^78]: 面向实例自适应推理的联邦学习

    Towards Instance-adaptive Inference for Federated Learning. (arXiv:2308.06051v1 [cs.LG])

    [http://arxiv.org/abs/2308.06051](http://arxiv.org/abs/2308.06051)

    本文提出了一种面向实例自适应推理的联邦学习算法，通过使用缩放和位移深度特征（SSF）实现了处理客户端数据异质性的能力。

    

    联邦学习是一种分布式学习范式，通过汇集本地训练来使多个客户端学习一个强大的全局模型。然而，全局模型的性能通常受到客户端之间的非独立同分布分布的影响，需要大量的努力来减轻客户端数据异质性。超越客户端数据异质性，我们注意到在复杂的现实世界数据中也可以观察到客户端内部的异质性，严重影响联邦学习的性能。在本文中，我们提出了一种新颖的联邦学习算法，即FedIns，在联邦学习框架中实现了实例自适应推理来处理客户端数据的异质性。我们不使用庞大的实例自适应模型，而是采用了一种参数高效的精细调整方法——缩放和位移深度特征（SSF），并在预训练模型上进行。具体而言，我们首先为每个客户端训练一个SSF池，在服务器端汇集这些SSF池，从而仍然保持低通信成本。

    Federated learning (FL) is a distributed learning paradigm that enables multiple clients to learn a powerful global model by aggregating local training. However, the performance of the global model is often hampered by non-i.i.d. distribution among the clients, requiring extensive efforts to mitigate inter-client data heterogeneity. Going beyond inter-client data heterogeneity, we note that intra-client heterogeneity can also be observed on complex real-world data and seriously deteriorate FL performance. In this paper, we present a novel FL algorithm, i.e., FedIns, to handle intra-client data heterogeneity by enabling instance-adaptive inference in the FL framework. Instead of huge instance-adaptive models, we resort to a parameter-efficient fine-tuning method, i.e., scale and shift deep features (SSF), upon a pre-trained model. Specifically, we first train an SSF pool for each client, and aggregate these SSF pools on the server side, thus still maintaining a low communication cost. T
    
[^79]: 基于骨骼的人体动作识别面临的硬性无盒对抗攻击和骨骼-动作知情梯度

    Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient. (arXiv:2308.05681v1 [cs.CV])

    [http://arxiv.org/abs/2308.05681](http://arxiv.org/abs/2308.05681)

    本文针对基于骨骼的人体动作识别方法的脆弱性进行了研究，提出了一种新的硬性无盒对抗攻击方法，通过学习运动流形和定义骨骼-动作知情梯度来攻击模型，揭示了这种脆弱性的存在。

    

    最近，基于骨骼的人体活动识别方法已被证明容易受到对抗攻击。然而，这些攻击方法要求要么完全了解受害者（即白盒攻击），要么有访问训练数据（即基于转移的攻击），或者频繁查询模型（即黑盒攻击）。所有这些要求都非常限制性，引发了对脆弱性的质疑。在本文中，我们证明了脆弱性确实存在。为此，我们考虑了一个新的攻击任务：攻击者无法访问受害者模型或训练数据或标签，我们将其称为硬性无盒攻击。具体来说，我们首先学习一个运动流形，然后定义一个用于计算攻击的对抗损失函数，称为骨骼-动作知情梯度（SMI梯度）。我们的梯度包含运动动力学的信息，这与现有的基于梯度的攻击方法不同，后者假设损失梯度是通过计算而来的。

    Recently, methods for skeleton-based human activity recognition have been shown to be vulnerable to adversarial attacks. However, these attack methods require either the full knowledge of the victim (i.e. white-box attacks), access to training data (i.e. transfer-based attacks) or frequent model queries (i.e. black-box attacks). All their requirements are highly restrictive, raising the question of how detrimental the vulnerability is. In this paper, we show that the vulnerability indeed exists. To this end, we consider a new attack task: the attacker has no access to the victim model or the training data or labels, where we coin the term hard no-box attack. Specifically, we first learn a motion manifold where we define an adversarial loss to compute a new gradient for the attack, named skeleton-motion-informed (SMI) gradient. Our gradient contains information of the motion dynamics, which is different from existing gradient-based attack methods that compute the loss gradient assuming 
    
[^80]: 基于蛋白质预训练语言模型和Transformer的磷酸化位点识别方法(PTransIPs)

    PTransIPs: Identification of phosphorylation sites based on protein pretrained language model and Transformer. (arXiv:2308.05115v1 [q-bio.QM])

    [http://arxiv.org/abs/2308.05115](http://arxiv.org/abs/2308.05115)

    PTransIPs是一种新型深度学习模型，它将蛋白质序列中的氨基酸视为自然语言中的单词，并结合大型预训练蛋白质模型的嵌入。该模型通过结合卷积神经网络和Transformer模型进行训练，用于识别磷酸化位点。

    

    磷酸化是许多基础细胞过程的核心，影响着各种疾病的发生和进展。因此，磷酸化位点的识别是理解细胞和病毒感染的分子机制的重要一步，可能为新的治疗靶点提供基础。在本研究中，我们提出了一种名为PTransIPs的新型深度学习模型，用于识别磷酸化位点。PTransIPs将蛋白质序列中的氨基酸视为自然语言中的单词，并根据序列中氨基酸的类型和位置提取独特的编码。它还通过使用大型预训练蛋白质模型的嵌入作为额外的数据输入。PTransIPs进一步通过结合具有残差连接的卷积神经网络和Transformer模型，配备多头注意机制进行训练。最后，该模型通过全连接层输出分类结果。

    Phosphorylation is central to numerous fundamental cellular processes, influencing the onset and progression of a variety of diseases. Identification of phosphorylation sites is thus an important step for understanding the molecular mechanisms of cells and virus infection, which potentially leads to new therapeutic targets. In this study, we present PTransIPs, a novel deep learning model for the identification of phosphorylation sites. PTransIPs treats amino acids in protein sequences as words in natural language, extracting unique encodings based on the types along with position of amino acids in the sequence. It also incorporates embeddings from large pre-trained protein models as additional data inputs. PTransIPS is further trained on a combination model of convolutional neural network with residual connections and Transformer model equipped with multi-head attention mechanisms. At last, the model outputs classification results through a fully connected layer. The results of indepen
    
[^81]: Adversarial ModSecurity: 使用强大的机器学习对抗SQL注入攻击

    Adversarial ModSecurity: Countering Adversarial SQL Injections with Robust Machine Learning. (arXiv:2308.04964v1 [cs.LG])

    [http://arxiv.org/abs/2308.04964](http://arxiv.org/abs/2308.04964)

    这篇论文介绍了Adversarial ModSecurity，它是一个使用强大的机器学习来对抗SQL注入攻击的防火墙。通过将核心规则集作为输入特征，该模型可以识别并防御对抗性SQL注入攻击。实验结果表明，AdvModSec在训练后能够有效地应对这类攻击。

    

    ModSecurity被广泛认可为标准的开源Web应用防火墙(WAF)，由OWASP基金会维护。它通过与核心规则集进行匹配来检测恶意请求，识别出常见的攻击模式。每个规则在CRS中都被手动分配一个权重，基于相应攻击的严重程度，如果触发规则的权重之和超过给定的阈值，就会被检测为恶意请求。然而，我们的研究表明，这种简单的策略在检测SQL注入攻击方面很不有效，因为它往往会阻止许多合法请求，同时还容易受到对抗性SQL注入攻击的影响，即故意操纵以逃避检测的攻击。为了克服这些问题，我们设计了一个名为AdvModSec的强大机器学习模型，它将CRS规则作为输入特征，并经过训练以检测对抗性SQL注入攻击。我们的实验表明，AdvModSec在针对该攻击的流量上进行训练后表现出色。

    ModSecurity is widely recognized as the standard open-source Web Application Firewall (WAF), maintained by the OWASP Foundation. It detects malicious requests by matching them against the Core Rule Set, identifying well-known attack patterns. Each rule in the CRS is manually assigned a weight, based on the severity of the corresponding attack, and a request is detected as malicious if the sum of the weights of the firing rules exceeds a given threshold. In this work, we show that this simple strategy is largely ineffective for detecting SQL injection (SQLi) attacks, as it tends to block many legitimate requests, while also being vulnerable to adversarial SQLi attacks, i.e., attacks intentionally manipulated to evade detection. To overcome these issues, we design a robust machine learning model, named AdvModSec, which uses the CRS rules as input features, and it is trained to detect adversarial SQLi attacks. Our experiments show that AdvModSec, being trained on the traffic directed towa
    
[^82]: OmniDataComposer: 用于多模态数据融合和无限数据生成的统一数据结构

    OmniDataComposer: A Unified Data Structure for Multimodal Data Fusion and Infinite Data Generation. (arXiv:2308.04126v1 [cs.CV])

    [http://arxiv.org/abs/2308.04126](http://arxiv.org/abs/2308.04126)

    OmniDataComposer是一种创新的多模态数据融合和无限数据生成方法，通过引入一个有效的协调数据结构，可以处理和合并视频、音频和文本等多模态数据输入，并实现跨模态数据校正。

    

    本论文提出了OmniDataComposer，一种创新的多模态数据融合和无限数据生成方法，旨在改善和简化不同数据模态之间的相互作用。最核心的突破是引入了一种有效处理和合并多模态数据输入的协调数据结构，包括视频、音频和文本。我们设计的算法利用了视频/图像字幕提取、密集字幕提取、自动语音识别（ASR）、光学字符识别（OCR）、Recognize Anything Model（RAM）和物体跟踪等多种操作的进展。OmniDataComposer能够识别超过6400种对象类别，显著扩大了视觉信息的范围。它将这些多样的模态融合在一起，促进模态之间的相互增强，并促进跨模态数据校正。最终输出将每个视频输入转化为详细的顺序文档。

    This paper presents OmniDataComposer, an innovative approach for multimodal data fusion and unlimited data generation with an intent to refine and uncomplicate interplay among diverse data modalities. Coming to the core breakthrough, it introduces a cohesive data structure proficient in processing and merging multimodal data inputs, which include video, audio, and text. Our crafted algorithm leverages advancements across multiple operations such as video/image caption extraction, dense caption extraction, Automatic Speech Recognition (ASR), Optical Character Recognition (OCR), Recognize Anything Model(RAM), and object tracking. OmniDataComposer is capable of identifying over 6400 categories of objects, substantially broadening the spectrum of visual information. It amalgamates these diverse modalities, promoting reciprocal enhancement among modalities and facilitating cross-modal data correction. \textbf{The final output metamorphoses each video input into an elaborate sequential docum
    
[^83]: DOMINO: 多个传感器时间序列数据的领域不变的高维分类

    DOMINO: Domain-invariant Hyperdimensional Classification for Multi-Sensor Time Series Data. (arXiv:2308.03295v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.03295](http://arxiv.org/abs/2308.03295)

    DOMINO是一种领域不变的高维分类方法，适用于多传感器时间序列数据；解决了分布偏移和计算资源的问题。

    

    随着物联网的快速发展，许多现实应用利用异构连接的传感器来捕捉时间序列信息。边缘机器学习方法常被用于分析本地收集的数据。然而，数据驱动的机器学习方法面临一个基本问题，即分布偏移。当一个模型部署在与其训练不同的数据分布上时，分布偏移会严重降低模型性能。此外，越来越复杂的深度神经网络(DNNs)已被提出用于捕捉多个传感器时间序列数据中的空间和时间依赖关系，而这需要超出当今边缘设备容量的大量计算资源。而大脑启发的高维计算(HDC)作为边缘学习的一种轻量级解决方案已被引入，但现有的HDC在面对分布偏移挑战时仍然脆弱。本文提出了DOMINO，一种新颖的HDC学习方法。

    With the rapid evolution of the Internet of Things, many real-world applications utilize heterogeneously connected sensors to capture time-series information. Edge-based machine learning (ML) methodologies are often employed to analyze locally collected data. However, a fundamental issue across data-driven ML approaches is distribution shift. It occurs when a model is deployed on a data distribution different from what it was trained on, and can substantially degrade model performance. Additionally, increasingly sophisticated deep neural networks (DNNs) have been proposed to capture spatial and temporal dependencies in multi-sensor time series data, requiring intensive computational resources beyond the capacity of today's edge devices. While brain-inspired hyperdimensional computing (HDC) has been introduced as a lightweight solution for edge-based learning, existing HDCs are also vulnerable to the distribution shift challenge. In this paper, we propose DOMINO, a novel HDC learning fr
    
[^84]: 用于在现代星空调查数据中搜索L＆T类棕矮星的机器学习方法

    Machine learning methods for the search for L&T brown dwarfs in the data of modern sky surveys. (arXiv:2308.03045v2 [astro-ph.SR] UPDATED)

    [http://arxiv.org/abs/2308.03045](http://arxiv.org/abs/2308.03045)

    在这项研究中，我们使用机器学习方法对PanStarrs DR1、2MASS和WISE数据进行分析，以区分L类和T类棕矮星和其他光谱和亮度类别的天体。这有助于建立一个均匀且完整的棕矮星样本，为研究提供可靠的数据集。

    

    根据各种估计，棕矮星（BD）应占银河系中所有天体的25％。然而，很少有棕矮星被发现并得到深入研究，无论是个体还是整体。这些研究需要均匀和完整的棕矮星样本。由于其弱信号，棕矮星的光谱研究相当耗费精力。因此，通过光谱观测确认的大量可靠的棕矮星样本似乎在目前是不可企及的。已经尝试了许多方法来利用其颜色作为决策规则应用于大量的勘测数据来搜索和创建一组棕矮星。在这项工作中，我们使用随机森林分类器、XGBoost、支持向量机分类器和TabNet等机器学习方法在PanStarrs DR1、2MASS和WISE数据上区分L类和T类棕矮星与其他光谱和亮度类别的天体。讨论了模型的解释。我们还将我们的模型与其他方法进行了比较。

    According to various estimates, brown dwarfs (BD) should account for up to 25 percent of all objects in the Galaxy. However, few of them are discovered and well-studied, both individually and as a population. Homogeneous and complete samples of brown dwarfs are needed for these kinds of studies. Due to their weakness, spectral studies of brown dwarfs are rather laborious. For this reason, creating a significant reliable sample of brown dwarfs, confirmed by spectroscopic observations, seems unattainable at the moment. Numerous attempts have been made to search for and create a set of brown dwarfs using their colours as a decision rule applied to a vast amount of survey data. In this work, we use machine learning methods such as Random Forest Classifier, XGBoost, SVM Classifier and TabNet on PanStarrs DR1, 2MASS and WISE data to distinguish L and T brown dwarfs from objects of other spectral and luminosity classes. The explanation of the models is discussed. We also compare our models wi
    
[^85]: 学习生成用于鲁棒语义分割的训练数据集

    Learning to Generate Training Datasets for Robust Semantic Segmentation. (arXiv:2308.02535v1 [cs.CV])

    [http://arxiv.org/abs/2308.02535](http://arxiv.org/abs/2308.02535)

    本文提出了一种新的方法，通过生成真实和可信的扰动或异常图像来提高语义分割技术的鲁棒性。通过设计和训练Robusta，一种鲁棒的条件生成对抗网络，可以为训练可靠的分割模型提供可用的数据集，从而显著增强语义分割技术在面对现实世界的扰动和分布变化时的鲁棒性。

    

    近年来，语义分割技术取得了显著进展，但是它们对现实世界的扰动和训练过程中未见过的数据样本的鲁棒性仍然是一个挑战，尤其是在安全关键的应用中。本文提出了一种新的方法，通过利用标签到图像生成器和图像到标签分割模型之间的协同作用来提高语义分割技术的鲁棒性。具体来说，我们设计并训练了一个新的鲁棒的条件生成对抗网络Robusta，用于生成真实和可信的扰动或异常图像，这些图像可以用来训练可靠的分割模型。我们对所提出的生成模型进行了深入研究，评估了下游分割网络的性能和鲁棒性，并证明我们的方法可以显著提高语义分割技术在面对现实世界的扰动、分布变化和超出分布的情况下的鲁棒性。

    Semantic segmentation techniques have shown significant progress in recent years, but their robustness to real-world perturbations and data samples not seen during training remains a challenge, particularly in safety-critical applications. In this paper, we propose a novel approach to improve the robustness of semantic segmentation techniques by leveraging the synergy between label-to-image generators and image-to-label segmentation models. Specifically, we design and train Robusta, a novel robust conditional generative adversarial network to generate realistic and plausible perturbed or outlier images that can be used to train reliable segmentation models. We conduct in-depth studies of the proposed generative model, assess the performance and robustness of the downstream segmentation network, and demonstrate that our approach can significantly enhance the robustness of semantic segmentation techniques in the face of real-world perturbations, distribution shifts, and out-of-distributi
    
[^86]: 对自动驾驶车辆风险评估的反事实安全边界视角

    A Counterfactual Safety Margin Perspective on the Scoring of Autonomous Vehicles' Riskiness. (arXiv:2308.01050v1 [cs.RO])

    [http://arxiv.org/abs/2308.01050](http://arxiv.org/abs/2308.01050)

    本文基于反事实模拟提出了一个数据驱动的框架，用于比较不同自动驾驶车辆在不同操作设计领域中行为风险。通过引入反事实安全边界的概念，该框架可以找到最关键的情景，并评估自动驾驶车辆的风险频率和严重程度。该方法即使在自动驾驶车辆的行为策略未知的情况下也适用，对外部第三方风险评估机构有用。

    

    自动驾驶车辆（AVs）有潜力提供诸多社会效益，如减少道路事故和提高交通效率。然而，由于缺乏历史数据和技术的快速发展，量化AVs的风险是具有挑战性的。本文提出了一个基于数据驱动的框架，用于比较不同AVs在各种操作设计领域（ODDs）中行为的风险，该框架基于对“不良”道路用户进行反事实模拟。我们引入了反事实安全边界的概念，表示可能导致碰撞的最小偏离正常行为的量。该概念有助于找到最关键的情景，同时也有助于评估AVs的风险频率和严重程度。我们证明，即使AV的行为策略是未知的，提出的方法仍然适用于最坏和最佳情况分析，使该方法对外部第三方风险评估机构也有用。

    Autonomous Vehicles (AVs) have the potential to provide numerous societal benefits, such as decreased road accidents and increased overall transportation efficiency. However, quantifying the risk associated with AVs is challenging due to the lack of historical data and the rapidly evolving technology. This paper presents a data-driven framework for comparing the risk of different AVs' behaviors in various operational design domains (ODDs), based on counterfactual simulations of "misbehaving" road users. We introduce the concept of counterfactual safety margin, which represents the minimum deviation from normal behavior that could lead to a collision. This concept helps to find the most critical scenarios but also to assess the frequency and severity of risk of AVs. We show that the proposed methodology is applicable even when the AV's behavioral policy is unknown -- through worst- and best-case analyses -- making the method useful also to external third-party risk assessors. Our experi
    
[^87]: Hessian-Aware Bayesian Optimization for Decision Making Systems - 感知海森贝叶斯优化在决策系统中的应用

    Hessian-Aware Bayesian Optimization for Decision Making Systems. (arXiv:2308.00629v1 [cs.LG])

    [http://arxiv.org/abs/2308.00629](http://arxiv.org/abs/2308.00629)

    本文介绍了一种感知海森贝叶斯优化算法，旨在解决决策系统优化中梯度反馈稀缺或无效的问题。通过引入紧凑的多层架构和角色概念，并利用感知海森贝叶斯优化方法对参数进行优化，作者实现了对复杂决策系统的高效优化。

    

    许多优化决策系统的方法依赖于梯度方法，需要从环境中获取有信息量的反馈。然而，当反馈稀缺或者无信息时，这些方法可能导致性能较差。贝叶斯优化等无导数方法可以减少对梯度反馈质量的依赖，但在复杂决策系统的高维环境中往往难以扩展。如果系统需要多个参与者之间的互动来实现共同目标，这个问题就加剧了。为了解决维度问题，我们提出了一种紧凑的多层架构，通过角色的概念来建模参与者之间的动态。此外，我们还引入了感知海森贝叶斯优化来高效地优化由大量参数参数化的多层架构。实验结果表明，我们的方法(HA-GP-UCB)在效果上是有效的。

    Many approaches for optimizing decision making systems rely on gradient based methods requiring informative feedback from the environment. However, in the case where such feedback is sparse or uninformative, such approaches may result in poor performance. Derivative-free approaches such as Bayesian Optimization mitigate the dependency on the quality of gradient feedback, but are known to scale poorly in the high-dimension setting of complex decision making systems. This problem is exacerbated if the system requires interactions between several actors cooperating to accomplish a shared goal. To address the dimensionality challenge, we propose a compact multi-layered architecture modeling the dynamics of actor interactions through the concept of role. Additionally, we introduce Hessian-aware Bayesian Optimization to efficiently optimize the multi-layered architecture parameterized by a large number of parameters. Experimental results demonstrate that our method (HA-GP-UCB) works effectiv
    
[^88]: 使用神经调整层析成像（NeTT）和掩蔽神经辐射场（mNeRF）的稳健单视锥形X射线姿态估计

    Robust Single-view Cone-beam X-ray Pose Estimation with Neural Tuned Tomography (NeTT) and Masked Neural Radiance Fields (mNeRF). (arXiv:2308.00214v1 [cs.CV])

    [http://arxiv.org/abs/2308.00214](http://arxiv.org/abs/2308.00214)

    在影像导引的微创医疗过程中，我们提出了新的方法，利用X射线投影进行辐射透明物体的姿态估计，并且展示了优化视图合成在完成此任务中的关键作用。

    

    在影像导引的微创医疗过程中，许多任务可以看作是姿态估计问题，其中利用X射线投影来达到3D空间中的目标。近期在可微分渲染技术上的进展使得RGB相机视图合成和姿态估计的性能达到了最先进水平。在之前的工作基础上，我们引入了新的方法，用于使用X射线投影进行辐射透明物体的姿态估计，并且展示了优化视图合成在完成此任务中的关键作用。首先我们开发了一种算法（DiffDRR），能够在TensorFlow中高效计算数字重建放射图像（DRRs）并利用自动微分。结合经典的CBCT重建算法，我们通过梯度下降进行姿态估计，使用一个损失函数来量化从随机初始化姿态合成的DRR与目标处真实透视图像的相似性。

    Many tasks performed in image-guided, mini-invasive, medical procedures can be cast as pose estimation problems, where an X-ray projection is utilized to reach a target in 3D space. Recent advances in the differentiable rendering of optically reflective materials have enabled state-of-the-art performance in RGB camera view synthesis and pose estimation. Expanding on these prior works, we introduce new methods for pose estimation of radiolucent objects using X-ray projections, and we demonstrate the critical role of optimal view synthesis in performing this task. We first develop an algorithm (DiffDRR) that efficiently computes Digitally Reconstructed Radiographs (DRRs) and leverages automatic differentiation within TensorFlow. In conjunction with classic CBCT reconstruction algorithms, we perform pose estimation by gradient descent using a loss function that quantifies the similarity of the DRR synthesized from a randomly initialized pose and the true fluoroscopic image at the target p
    
[^89]: 远程生物感应：公开源基准框架用于公平评估rPPG

    Remote Bio-Sensing: Open Source Benchmark Framework for Fair Evaluation of rPPG. (arXiv:2307.12644v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2307.12644](http://arxiv.org/abs/2307.12644)

    这篇论文提出了远程生物感应技术rPPG的公开源基准框架，该框架可以公平评估rPPG的准确性问题并解决与皮肤颜色、相机特性和环境光等因素相关的挑战。

    

    rPPG（远程光电容积脉搏图）是一种通过使用摄像头捕捉到的血红蛋白的光吸收特性来测量和分析BVP（血容量脉搏）的技术。分析所测量的BVP可以得出各种生理信号，如心率、压力水平和血压，可以应用于各种应用，如远程医疗、远程患者监护和心血管疾病的早期预测。尽管在这个领域已经进行了广泛的努力和进展，但仍然存在严重的挑战，包括与皮肤颜色、相机特性、环境光和其他噪声和伪迹来源有关的问题，这些问题降低了准确性能。我们认为迫切需要公正可评估的基准测试来克服这些挑战。

    rPPG (Remote photoplethysmography) is a technology that measures and analyzes BVP (Blood Volume Pulse) by using the light absorption characteristics of hemoglobin captured through a camera. Analyzing the measured BVP can derive various physiological signals such as heart rate, stress level, and blood pressure, which can be applied to various applications such as telemedicine, remote patient monitoring, and early prediction of cardiovascular disease. rPPG is rapidly evolving and attracting great attention from both academia and industry by providing great usability and convenience as it can measure biosignals using a camera-equipped device without medical or wearable devices. Despite extensive efforts and advances in this field, serious challenges remain, including issues related to skin color, camera characteristics, ambient lighting, and other sources of noise and artifacts, which degrade accuracy performance. We argue that fair and evaluable benchmarking is urgently required to overc
    
[^90]: 具有交互层的量子卷积神经网络用于经典数据的分类

    Quantum Convolutional Neural Networks with Interaction Layers for Classification of Classical Data. (arXiv:2307.11792v1 [quant-ph])

    [http://arxiv.org/abs/2307.11792](http://arxiv.org/abs/2307.11792)

    本文介绍了一种引入了三量子位相互作用的新型交互层的量子卷积网络，增加了网络的表达能力和纠缠能力，用于对图像和一维数据进行分类。

    

    由于量子计算机具有异常的计算能力，量子机器学习（QML）引起了广泛关注。在不久的将来，几乎没有错误的量子计算机的承诺之下，对量子神经网络中多量子位相互作用的影响进行广泛研究非常重要。本文介绍了一种引入了三量子位相互作用的新型交互层的量子卷积网络，增加了网络的表达能力和纠缠能力，用于对图像和一维数据进行分类。该方法在三个公开可用的数据集MNIST、Fashion MNIST和Iris数据集上进行了测试，用于进行二元和多类别分类，并发现超越了现有最先进方法的性能。

    Quantum Machine Learning (QML) has come into the limelight due to the exceptional computational abilities of quantum computers. With the promises of near error-free quantum computers in the not-so-distant future, it is important that the effect of multi-qubit interactions on quantum neural networks is studied extensively. This paper introduces a Quantum Convolutional Network with novel Interaction layers exploiting three-qubit interactions increasing the network's expressibility and entangling capability, for classifying both image and one-dimensional data. The proposed approach is tested on three publicly available datasets namely MNIST, Fashion MNIST, and Iris datasets, to perform binary and multiclass classifications and is found to supersede the performance of the existing state-of-the-art methods.
    
[^91]: 离散切割Wasserstein损失的性质

    Properties of Discrete Sliced Wasserstein Losses. (arXiv:2307.10352v1 [stat.ML])

    [http://arxiv.org/abs/2307.10352](http://arxiv.org/abs/2307.10352)

    本文研究了离散切割Wasserstein损失的性质，并探讨了其正则性和优化性质以及通过蒙特卡洛近似的方法。

    

    切割Wasserstein（SW）距离已成为比较概率测度的Wasserstein距离的一种流行替代方法。广泛应用包括图像处理、领域自适应和生成建模，常常需要优化一些参数以最小化SW，该参数充当离散概率测度之间的损失函数（因为具有密度的测度在数值上是无法实现的）。所有这些优化问题都存在相同的子问题，即最小化切割Wasserstein能量。在本文中，我们研究了$\mathcal{E}: Y \longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z)$的属性，即两个具有与一个测度的支撑相同数量的离散均匀测度之间的SW距离作为支撑$Y \in \mathbb{R}^{n \times d}$函数的能量。我们研究了这个能量的正则性和优化性质，以及其通过蒙特卡洛近似$\mathcal{E}_p$（使用SW中的期望估计）。

    The Sliced Wasserstein (SW) distance has become a popular alternative to the Wasserstein distance for comparing probability measures. Widespread applications include image processing, domain adaptation and generative modelling, where it is common to optimise some parameters in order to minimise SW, which serves as a loss function between discrete probability measures (since measures admitting densities are numerically unattainable). All these optimisation problems bear the same sub-problem, which is minimising the Sliced Wasserstein energy. In this paper we study the properties of $\mathcal{E}: Y \longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z)$, i.e. the SW distance between two uniform discrete measures with the same amount of points as a function of the support $Y \in \mathbb{R}^{n \times d}$ of one of the measures. We investigate the regularity and optimisation properties of this energy, as well as its Monte-Carlo approximation $\mathcal{E}_p$ (estimating the expectation in SW using 
    
[^92]: 通过可靠的、多样化的和类平衡的伪标签来重新审视领域自适应三维物体检测

    Revisiting Domain-Adaptive 3D Object Detection by Reliable, Diverse and Class-balanced Pseudo-Labeling. (arXiv:2307.07944v1 [cs.CV])

    [http://arxiv.org/abs/2307.07944](http://arxiv.org/abs/2307.07944)

    本文通过提出一种适用于多类训练设置的新型ReDB框架来解决现有领域自适应方法在多类训练设置下性能下降的问题，通过产生可靠的、多样化的和类平衡的伪三维框来引导目标领域的自训练。

    

    无监督领域自适应与伪标签技术的辅助已经成为领域自适应三维物体检测的关键方法。然而，现有的领域自适应方法在应用于多类训练设置时性能大幅下降，原因是伪标签的质量低和类别不平衡问题共存。本文通过提出一种针对同时学习检测所有类别的新型ReDB框架来解决这一挑战。我们的方法产生可靠的、多样化的和类平衡的伪三维框，通过迭代地引导不同分布的目标领域的自训练。为了减轻环境差异（例如，光束数量）带来的干扰，我们提出了跨域检查（CDE），通过将目标实例复制粘贴到源环境中并测量预测的一致性来评估伪标签的正确性。为了减少计算开销和缓解物体的转移（例如，

    Unsupervised domain adaptation (DA) with the aid of pseudo labeling techniques has emerged as a crucial approach for domain-adaptive 3D object detection. While effective, existing DA methods suffer from a substantial drop in performance when applied to a multi-class training setting, due to the co-existence of low-quality pseudo labels and class imbalance issues. In this paper, we address this challenge by proposing a novel ReDB framework tailored for learning to detect all classes at once. Our approach produces Reliable, Diverse, and class-Balanced pseudo 3D boxes to iteratively guide the self-training on a distributionally different target domain. To alleviate disruptions caused by the environmental discrepancy (e.g., beam numbers), the proposed cross-domain examination (CDE) assesses the correctness of pseudo labels by copy-pasting target instances into a source environment and measuring the prediction consistency. To reduce computational overhead and mitigate the object shift (e.g.
    
[^93]: 基因调控网络中的负概率

    Negative probabilities in Gene Regulatory Networks. (arXiv:2307.07738v1 [q-bio.MN])

    [http://arxiv.org/abs/2307.07738](http://arxiv.org/abs/2307.07738)

    该论文提出了一种基于基因表达和相关性符号的方法，用于识别基因之间的符号不确定共表达。通过构建具有符号不确定贡献的概率转移矩阵，可以量化基因调控网络中各种连接的结构和重要性，并解释其对网络几何结构的影响。

    

    我们介绍了一种基于已知表达和给定相关性符号来识别基因之间存在的符号不确定共表达的自然框架。具体而言，我们给出了基因之间的亲和关系信息（即基因调控网络中的连接性）和它们促进/抑制共表达蛋白产生的知识，并寻求能够解释蛋白质平衡分布的速率。我们提议将它们的“促进 vs. 抑制”功能封装在一个符号不确定的概率转移矩阵中，该矩阵的行和为一，但是除此之外符号不确定。构建具有符号不确定贡献的相互作用网络的表示的目的是量化各种连接的结构和重要性，并解释这些连接如何影响网络的几何结构，突显调控机制的重要性。

    We introduce a natural framework to identify sign-indefinite co-expressions between genes based on the known expressions and given the sign of their respective correlations. Specifically, given information concerning the affinity among genes (i.e., connectivity in the gene regulatory network) and knowledge whether they promote/inhibit co-expression of the respective protein production, we seek rates that may explain the observed stationary distributions at the level of proteins. We propose to encapsulate their ``promotion vs.\ inhibition'' functionality in a sign-indefinite probability transition matrix--a matrix whose row-sums equal to one, but is otherwise sign indefinite. The purpose of constructing such a representation for the interaction network with sign-indefinite contributions in protein regulation, is to quantify the structure and significance of various links, and to explain how these may affect the geometry of the network, highlighting the significance of the regulatory fun
    
[^94]: PC-Droid: 更快的扩散速度和改进的粒子云生成质量

    PC-Droid: Faster diffusion and improved quality for particle cloud generation. (arXiv:2307.06836v1 [hep-ex])

    [http://arxiv.org/abs/2307.06836](http://arxiv.org/abs/2307.06836)

    PC-Droid是一个新的扩散模型，通过利用新的扩散公式和研究更近期的积分求解器，同时对所有类型的喷注进行训练，实现了最先进的性能。它不仅能提供更快的生成速度，而且在所有评估指标上都具有卓越的性能。

    

    在PC-JeDi的基础上，我们引入了PC-Droid，这是一个大幅改进的扩散模型，用于生成喷注粒子云。通过利用新的扩散公式、研究更近期的积分求解器，并同时对所有类型的喷注进行训练，我们能够在所有评估指标上实现最先进的性能。我们通过比较基于注意力的两种体系结构和一致性蒸馏的潜力来研究生成速度和质量之间的权衡。更快的体系结构和一致性模型都表现出超越许多竞争模型的性能，生成时间比PC-JeDi快上两个数量级。

    Building on the success of PC-JeDi we introduce PC-Droid, a substantially improved diffusion model for the generation of jet particle clouds. By leveraging a new diffusion formulation, studying more recent integration solvers, and training on all jet types simultaneously, we are able to achieve state-of-the-art performance for all types of jets across all evaluation metrics. We study the trade-off between generation speed and quality by comparing two attention based architectures, as well as the potential of consistency distillation to reduce the number of diffusion steps. Both the faster architecture and consistency models demonstrate performance surpassing many competing models, with generation time up to two orders of magnitude faster than PC-JeDi.
    
[^95]: 关注关系的子图嵌入与对比学习在药物相互作用预测中的应用

    Relation-aware subgraph embedding with co-contrastive learning for drug-drug interaction prediction. (arXiv:2307.01507v1 [cs.LG])

    [http://arxiv.org/abs/2307.01507](http://arxiv.org/abs/2307.01507)

    本论文提出了一种新的关注关系的子图嵌入与对比学习方法RaSECo，用于预测多关系药物相互作用。该方法通过构建不同的药物图和采用对比学习机制，能够解决现有方法中对于新药物过拟合的问题。

    

    关注关系的子图嵌入对于预测多关系药物相互作用（DDI）非常有前景。然而，现有方法中大部分都局限于学习现有药物的子图嵌入，导致在涉及新药物的测试DDIs中出现严重过拟合问题。为了缓解这个问题，我们提出了一种基于关注关系的子图嵌入与对比学习的新型DDI预测方法，即RaSECo。RaSECo构建了两个异构药物图：多关系DDI图和基于多属性的药物相似度（DDS）图。这两个图分别用于学习和传播药物的子图嵌入，从而确保所有药物，包括新药物，能够聚合有效的子图嵌入。此外，我们采用了交叉视图对比机制来增强药物对（DP）的嵌入。

    Relation-aware subgraph embedding is promising for predicting multi-relational drug-drug interactions (DDIs). Typically, most existing methods begin by constructing a multi-relational DDI graph and then learning relation-aware subgraph embeddings (RaSEs) of drugs from the DDI graph. However, most existing approaches are usually limited in learning RaSEs of new drugs, leading to serious over-fitting when the test DDIs involve such drugs. To alleviate this issue, We propose a novel DDI prediction method based on relation-aware subgraph embedding with co-contrastive learning, RaSECo. RaSECo constructs two heterogeneous drug graphs: a multi-relational DDI graph and a multi-attributes-based drug-drug similarity (DDS) graph. The two graphs are used respectively for learning and propagating the RaSEs of drugs, thereby ensuring that all drugs, including new ones, can aggregate effective RaSEs. Additionally, we employ a cross-view contrastive mechanism to enhance drug-pair (DP) embedding. RaSEC
    
[^96]: 通过集成网络在多波段图像中简化透镜类星体的识别

    Streamlined Lensed Quasar Identification in Multiband Images via Ensemble Networks. (arXiv:2307.01090v2 [astro-ph.GA] UPDATED)

    [http://arxiv.org/abs/2307.01090](http://arxiv.org/abs/2307.01090)

    该研究通过集成多个卷积网络和视觉转换器，以简化透镜类星体在多波段图像中的识别。在测试数据集上表现出色，但在真实数据中很难泛化，并且存在大量虚假源的问题。

    

    经历强引力透镜效应的类星体对宇宙膨胀速率、前景偏折器内的暗物质分布以及类星体宿主星系等相关问题提供了独特的视角。然而，由于非透镜星体的数量过多，将它们在天文图像中准确识别出来具有挑战性。为了解决这个问题，我们开发了一种新颖的方法，即使用先进的卷积网络（如ResNet、Inception、NASNet、MobileNet、EfficientNet和RegNet）与基于Hyper Suprime-Cam（HSC）多波段图像的现实星系-类星体透镜模拟训练的视觉转换器（ViTs）进行集成。虽然单个模型在测试数据集上表现出色，ROC曲线下面积达到97.3%以上，中位误报率为3.6%，但在真实数据中很难泛化，每个分类器都会选择出大量虚假源。 一个重要的原因是透镜类星体的数量比非透镜类星体少得多。

    Quasars experiencing strong lensing offer unique viewpoints on subjects related to the cosmic expansion rate, the dark matter profile within the foreground deflectors, and the quasar host galaxies. Unfortunately, identifying them in astronomical images is challenging since they are overwhelmed by the abundance of non-lenses. To address this, we have developed a novel approach by ensembling cutting-edge convolutional networks (CNNs) -- for instance, ResNet, Inception, NASNet, MobileNet, EfficientNet, and RegNet -- along with vision transformers (ViTs) trained on realistic galaxy-quasar lens simulations based on the Hyper Suprime-Cam (HSC) multiband images. While the individual model exhibits remarkable performance when evaluated against the test dataset, achieving an area under the receiver operating characteristic curve of $>$97.3% and a median false positive rate of 3.6%, it struggles to generalize in real data, indicated by numerous spurious sources picked by each classifier. A signi
    
[^97]: 指导目标表征：一种半监督语言接口控制机器人的方法

    Goal Representations for Instruction Following: A Semi-Supervised Language Interface to Control. (arXiv:2307.00117v1 [cs.RO])

    [http://arxiv.org/abs/2307.00117](http://arxiv.org/abs/2307.00117)

    本文介绍了一种半监督语言接口控制机器人的方法，通过联合图像和目标信息的策略以及少量的语言数据实现了在真实世界中的稳健性能。

    

    我们的目标是使机器人能够按照自然语言指令行动，例如“将毛巾放在微波炉旁边”。但是获取大量带有语言指令标签的标注数据非常困难。相比之下，获取对图像目标作出响应的策略要容易得多，因为任何自主尝试或演示都可以在事后用最终状态作为目标进行标记。在这项工作中，我们提出了一种方法，只利用少量的语言数据，利用联合图像和目标信息的策略来处理语言接口。以前的工作在使用视觉-语言模型或联合训练语言-目标-条件策略方面取得了一些进展，但迄今为止，这两种方法都没有有效地扩展到实际机器人任务中，而不需要大量的人工注释。我们的方法通过从标记数据中学习一种将语言与目标图像对齐的嵌入，实现了在真实世界中的稳健性能。

    Our goal is for robots to follow natural language instructions like "put the towel next to the microwave." But getting large amounts of labeled data, i.e. data that contains demonstrations of tasks labeled with the language instruction, is prohibitive. In contrast, obtaining policies that respond to image goals is much easier, because any autonomous trial or demonstration can be labeled in hindsight with its final state as the goal. In this work, we contribute a method that taps into joint image- and goal- conditioned policies with language using only a small amount of language data. Prior work has made progress on this using vision-language models or by jointly training language-goal-conditioned policies, but so far neither method has scaled effectively to real-world robot tasks without significant human annotation. Our method achieves robust performance in the real world by learning an embedding from the labeled data that aligns language not to the goal image, but rather to the desir
    
[^98]: 协同过滤中维度无关的困难负样本混合方法

    Dimension Independent Mixup for Hard Negative Sample in Collaborative Filtering. (arXiv:2306.15905v1 [cs.IR])

    [http://arxiv.org/abs/2306.15905](http://arxiv.org/abs/2306.15905)

    本文提出了一种协同过滤训练中维度无关的困难负样本混合方法（DINS），通过对采样区域的新视角进行重新审视来改进现有的采样方法。实验证明，DINS优于其他负采样方法，证实了其有效性和优越性。

    

    协同过滤（CF）是一种广泛应用的技术，可以基于过去的互动预测用户的偏好。负采样在使用隐式反馈训练基于CF的模型时起到至关重要的作用。本文提出了一种基于采样区域的新视角来重新审视现有的采样方法。我们指出，目前的采样方法主要集中在点采样或线采样上，缺乏灵活性，并且有相当大一部分困难采样区域未被探索。为了解决这个限制，我们提出了一种维度无关的困难负样本混合方法（DINS），它是第一个针对训练基于CF的模型的区域采样方法。DINS包括三个模块：困难边界定义、维度无关混合和多跳池化。在真实世界的数据集上进行的实验证明，DINS优于其他负采样方法，证明了它的有效性和优越性。

    Collaborative filtering (CF) is a widely employed technique that predicts user preferences based on past interactions. Negative sampling plays a vital role in training CF-based models with implicit feedback. In this paper, we propose a novel perspective based on the sampling area to revisit existing sampling methods. We point out that current sampling methods mainly focus on Point-wise or Line-wise sampling, lacking flexibility and leaving a significant portion of the hard sampling area un-explored. To address this limitation, we propose Dimension Independent Mixup for Hard Negative Sampling (DINS), which is the first Area-wise sampling method for training CF-based models. DINS comprises three modules: Hard Boundary Definition, Dimension Independent Mixup, and Multi-hop Pooling. Experiments with real-world datasets on both matrix factorization and graph-based models demonstrate that DINS outperforms other negative sampling methods, establishing its effectiveness and superiority. Our wo
    
[^99]: 视觉对抗样本越狱大语言模型的安全隐患分析

    Visual Adversarial Examples Jailbreak Large Language Models. (arXiv:2306.13213v1 [cs.CR])

    [http://arxiv.org/abs/2306.13213](http://arxiv.org/abs/2306.13213)

    本文对将图像引入大型语言模型的安全隐患进行了分析，指出视觉输入空间的连续性和高维性是对抗攻击的丰富领域，同时也为视觉攻击者提供了更广泛的实现对抗目标的可能性。

    

    最近，将图像引入大型语言模型（LLMs）已经引起了人们的高度关注。大型视觉语言模型（VLMs）的普及，例如Flamingo、BLIP-2和GPT-4，标志着视觉和语言基础模型的先进发展相互融合的重要进展。然而，这种综合方法涉及的风险仍未得到详细研究。本文揭示了这一趋势的安全隐患。我们首先指出，视觉输入空间的连续性和高维性在本质上使其成为对抗攻击的丰富领域，这不可避免地扩大了LLMs的攻击面。其次，我们强调，LLMs的广泛功能也为视觉攻击者提供了更广泛的实现对抗目标的可能性，将安全失败的影响扩展到了简单的错误分类之外。为了阐明这些风险，我们研究了VLM视觉输入空间中的对抗性样例。

    Recently, there has been a surge of interest in introducing vision into Large Language Models (LLMs). The proliferation of large Visual Language Models (VLMs), such as Flamingo, BLIP-2, and GPT-4, signifies an exciting convergence of advancements in both visual and language foundation models. Yet, the risks associated with this integrative approach are largely unexamined. In this paper, we shed light on the security and safety implications of this trend. First, we underscore that the continuous and high-dimensional nature of the additional visual input space intrinsically makes it a fertile ground for adversarial attacks. This unavoidably expands the attack surfaces of LLMs. Second, we highlight that the broad functionality of LLMs also presents visual attackers with a wider array of achievable adversarial objectives, extending the implications of security failures beyond mere misclassification. To elucidate these risks, we study adversarial examples in the visual input space of a VLM.
    
[^100]: 在各种模拟驾驶操作中对深度强化学习进行全面培训和评估的研究

    Comprehensive Training and Evaluation on Deep Reinforcement Learning for Automated Driving in Various Simulated Driving Maneuvers. (arXiv:2306.11466v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2306.11466](http://arxiv.org/abs/2306.11466)

    本研究在模拟平台上对两种深度强化学习算法进行了全面评估和比较，以开发自动驾驶模型。通过定制的奖励函数，对准确度、效率、安全性和舒适度进行了评估。

    

    在现实世界中开发和测试自动驾驶模型可能是具有挑战性甚至危险的，而模拟可以帮助解决这个问题，尤其是对于具有挑战性的驾驶操作。深度强化学习（DRL）通过学习和与环境交互来处理复杂的决策和控制任务，因此非常适合开发自动驾驶，在这方面的具体研究还不多。本研究在highway-env模拟平台上实施、评估和比较了两个DRL算法，Deep Q-networks（DQN）和Trust Region Policy Optimization（TRPO），以培训自动驾驶。同时开发了有效且定制的奖励函数，并通过准确度（车辆在道路上的行驶情况）、效率（车辆的行驶速度）、安全性（车辆避免与障碍物碰撞的可能性）和舒适度（车辆驾驶的舒适程度）来评估已实施的算法。

    Developing and testing automated driving models in the real world might be challenging and even dangerous, while simulation can help with this, especially for challenging maneuvers. Deep reinforcement learning (DRL) has the potential to tackle complex decision-making and controlling tasks through learning and interacting with the environment, thus it is suitable for developing automated driving while not being explored in detail yet. This study carried out a comprehensive study by implementing, evaluating, and comparing the two DRL algorithms, Deep Q-networks (DQN) and Trust Region Policy Optimization (TRPO), for training automated driving on the highway-env simulation platform. Effective and customized reward functions were developed and the implemented algorithms were evaluated in terms of onlane accuracy (how well the car drives on the road within the lane), efficiency (how fast the car drives), safety (how likely the car is to crash into obstacles), and comfort (how much the car ma
    
[^101]: 区块链支持的联邦学习：参考架构设计、实现和验证

    Blockchain-Enabled Federated Learning: A Reference Architecture Design, Implementation, and Verification. (arXiv:2306.10841v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10841](http://arxiv.org/abs/2306.10841)

    本文提出了一种基于区块链的联邦学习参考架构，通过结合联邦学习和区块链技术，实现了去中心化、协作的机器学习系统，并保护了数据隐私和用户控制的身份。该架构使用去中心化标识符进行身份验证，通过智能合约实现强大的安全性和高效的去中心化，并能根据需求集成各种额外的元素，是一个适用范围广泛的 BCFL 解决方案。

    

    本文提出了一种创新的基于区块链的联邦学习（BCFL）参考架构，该架构将联邦学习和区块链技术的优势结合起来。这导致了一个去中心化的、协作的机器学习系统，尊重数据隐私和用户控制的身份。我们的架构战略性地采用基于去中心化标识符（DID）的身份验证系统，允许参与者使用其自主 DID 安全地认证并获得对联邦学习平台的访问权限，这些信息被记录在区块链上。通过执行智能合约来确保强大的安全性和高效的去中心化是我们方法的关键方面。此外，我们的 BCFL 参考架构提供了显著的可扩展性，能够根据特定需求和用例集成各种额外的元素，使其成为广泛适用的 BCFL 解决方案。

    This paper presents an innovative reference architecture for blockchain-enabled federated learning (BCFL), a state-of-the-art approach that amalgamates the strengths of federated learning and blockchain technology. This results in a decentralized, collaborative machine learning system that respects data privacy and user-controlled identity. Our architecture strategically employs a decentralized identifier (DID)-based authentication system, allowing participants to authenticate and then gain access to the federated learning platform securely using their self-sovereign DIDs, which are recorded on the blockchain. Ensuring robust security and efficient decentralization through the execution of smart contracts is a key aspect of our approach. Moreover, our BCFL reference architecture provides significant extensibility, accommodating the integration of various additional elements, as per specific requirements and use cases, thereby rendering it an adaptable solution for a wide range of BCFL 
    
[^102]: 深度学习驾驶模型中的偏见问题

    Hidden Biases of End-to-End Driving Models. (arXiv:2306.07957v1 [cs.CV])

    [http://arxiv.org/abs/2306.07957](http://arxiv.org/abs/2306.07957)

    端到端驾驶模型存在偏见问题，并引入了次要组件的更改。本文针对目前多数最先进的方法中所存在的两种偏见进行了研究，并提出了合理的替代方法。在此基础上，开发了TF ++，在CARLA测试中表现优异。

    

    最近，端到端的驾驶系统在CARLA测试中取得了快速进展。然而，即使在主要贡献的基础上，这些系统也会引入对次要系统组件的改变。因此，系统的改进源并不清楚。我们发现，在几乎所有最先进的方法中都存在两种偏见，这些偏见对于在CARLA上观察到的进展至关重要：(1) 通过对目标点跟随的强归纳偏见来进行横向恢复，(2) 通过多模态航路点预测的纵向平均来减速。我们研究了这些偏见的缺点，并确定了合理的替代方法。通过结合我们的见解，我们开发了TF ++，一种简单的端到端方法，在Longest6和LAV基准测试中排名第一，在Longest6上比最佳前期工作提高了14个驾驶分数。

    End-to-end driving systems have recently made rapid progress, in particular on CARLA. Independent of their major contribution, they introduce changes to minor system components. Consequently, the source of improvements is unclear. We identify two biases that recur in nearly all state-of-the-art methods and are critical for the observed progress on CARLA: (1) lateral recovery via a strong inductive bias towards target point following, and (2) longitudinal averaging of multimodal waypoint predictions for slowing down. We investigate the drawbacks of these biases and identify principled alternatives. By incorporating our insights, we develop TF++, a simple end-to-end method that ranks first on the Longest6 and LAV benchmarks, gaining 14 driving score over the best prior work on Longest6.
    
[^103]: 语言模型中出现的类人直觉行为和推理偏差——以及在GPT-4中消失。

    Human-Like Intuitive Behavior and Reasoning Biases Emerged in Language Models -- and Disappeared in GPT-4. (arXiv:2306.07622v1 [cs.CL])

    [http://arxiv.org/abs/2306.07622](http://arxiv.org/abs/2306.07622)

    本研究揭示了大型语言模型（LLMs）具有类人直觉行为和认知错误的特点，而高级语言模型则通过学习避免这类错误并表现出超理性的方式。此外，通过使用心理学研究的方法探测LLMs，可以揭示其新生特性。

    

    大型语言模型（LLM）目前处于将AI系统与人类交流和日常生活交织在一起的前沿。因此，评估它们的新兴能力非常重要。在这项研究中，我们展示了LLM（尤其是GPT-3）表现出惊人的类人直觉行为，以及遵循这种行为而来的认知错误。然而，具有更高认知能力的LLM，特别是ChatGPT和GPT-4，学会了避免屈服于这些错误并表现出超理性的方式。对于我们的实验，我们利用了Cognitive Reflection Test（CRT）及用于研究人类直觉决策的语义幻觉。此外，我们还探究了类人直觉决策的稳定倾向。我们的研究表明，通过心理学方法调查LLM有潜力揭示否则未知的新生特性。

    Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Therefore, it is of great importance to evaluate their emerging abilities. In this study, we show that LLMs, most notably GPT-3, exhibit behavior that strikingly resembles human-like intuition -- and the cognitive errors that come with it. However, LLMs with higher cognitive capabilities, in particular ChatGPT and GPT-4, learned to avoid succumbing to these errors and perform in a hyperrational manner. For our experiments, we probe LLMs with the Cognitive Reflection Test (CRT) as well as semantic illusions that were originally designed to investigate intuitive decision-making in humans. Moreover, we probe how sturdy the inclination for intuitive-like decision-making is. Our study demonstrates that investigating LLMs with methods from psychology has the potential to reveal otherwise unknown emergent traits.
    
[^104]: 在性能方面摇摆不定：使用随机单词和广义概念进行视觉分类

    Waffling around for Performance: Visual Classification with Random Words and Broad Concepts. (arXiv:2306.07282v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.07282](http://arxiv.org/abs/2306.07282)

    本研究提出了WaffleCLIP，一个使用随机字符和单词描述符进行零样本视觉分类的框架，它取代了使用大型语言模型生成的描述符。该方法不仅能够实现可比较的性能提升，而且可以作为基于语言模型的视觉分类模型扩展的替代选择和验证方法。

    

    已经证明，诸如CLIP之类的视觉语言模型的视觉分类性能可以受益于大型语言模型（LLM）（如GPT-3）的额外语义知识。特别是在LLM生成的类别描述符中进行平均，例如“具有圆形形状的华夫饼”，可以显著提高泛化性能。在这项工作中，我们对这种行为进行了批判性研究，并提出了WaffleCLIP，这是一个零样本视觉分类框架，它仅用随机字符和单词描述符替换LLM生成的描述符。在不查询外部模型的情况下，我们在大量视觉分类任务上实现了可比较的性能增益。这使得WaffleCLIP既可以作为一种低成本替代，也可以作为未来基于LLM的视觉语言模型扩展的一种合理检查。我们对引入LLM生成的描述符的附加语义的影响和缺陷进行了广泛的实验研究，并展示了如果可用，se的能动性可以作为解释这些缺陷的一种方法。

    The visual classification performance of vision-language models such as CLIP has been shown to benefit from additional semantic knowledge from large language models (LLMs) such as GPT-3. In particular, averaging over LLM-generated class descriptors, e.g. "waffle, which has a round shape", can notably improve generalization performance. In this work, we critically study this behavior and propose WaffleCLIP, a framework for zero-shot visual classification which simply replaces LLM-generated descriptors with random character and word descriptors. Without querying external models, we achieve comparable performance gains on a large number of visual classification tasks. This allows WaffleCLIP to both serve as a low-cost alternative, as well as a sanity check for any future LLM-based vision-language model extensions. We conduct an extensive experimental study on the impact and shortcomings of additional semantics introduced with LLM-generated descriptors, and showcase how - if available - se
    
[^105]: 一种弱监督的情绪变化预测方法和改进的情绪推理

    A Weakly Supervised Approach to Emotion-change Prediction and Improved Mood Inference. (arXiv:2306.06979v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2306.06979](http://arxiv.org/abs/2306.06979)

    该论文提出了一种弱监督的方法来预测情绪变化并改进情绪推理。通过利用预训练的孪生网络进行度量学习，推理出情绪变化信息，并将其与心情标签一起用于心情分类。实验结果表明，情绪变化信息的加入可以提高心情预测效果，强调了建模心情-情绪相互作用对于有效的心情推理的重要性。

    

    尽管大部分情感计算研究关注于推断情绪、研究心情或理解心情-情绪相互作用，但对此领域的研究还远远不够。在之前的工作基础上，我们通过度量学习从预训练的孪生网络中推理出情绪变化（∆）信息来推断心情，而无需依赖注释标签，并尝试在与心情特征对齐的长时视频剪辑中进行心情预测。我们将情绪变化（∆）标签与心情标签一起用于心情分类。实验结果表明，与仅使用心情标签进行训练的单模态模型相比，使用情绪变化信息进行训练的多模态模型可以改善心情预测效果，强调了在有效的心情推理中对心情-情绪相互作用进行建模的重要性。

    Whilst a majority of affective computing research focuses on inferring emotions, examining mood or understanding the \textit{mood-emotion interplay} has received significantly less attention. Building on prior work, we (a) deduce and incorporate emotion-change ($\Delta$) information for inferring mood, without resorting to annotated labels, and (b) attempt mood prediction for long duration video clips, in alignment with the characterisation of mood. We generate the emotion-change ($\Delta$) labels via metric learning from a pre-trained Siamese Network, and use these in addition to mood labels for mood classification. Experiments evaluating \textit{unimodal} (training only using mood labels) vs \textit{multimodal} (training using mood plus $\Delta$ labels) models show that mood prediction benefits from the incorporation of emotion-change information, emphasising the importance of modelling the mood-emotion interplay for effective mood inference.
    
[^106]: 基于分布式多智能体强化学习的异构交通意图感知规划算法iPLAN

    iPLAN: Intent-Aware Planning in Heterogeneous Traffic via Distributed Multi-Agent Reinforcement Learning. (arXiv:2306.06236v1 [cs.MA])

    [http://arxiv.org/abs/2306.06236](http://arxiv.org/abs/2306.06236)

    论文提出了基于MARL算法的iPLAN方法，可在高密度且异构交通场景下进行意图感知规划，使智能体能够从局部观测中推断附近驾驶者的意图，并通过行为或瞬时激励进行决策，实现自主导航。

    

    在高密度和异构交通场景中保障自动驾驶汽车（AVs）的安全和效率面临较大挑战，因为它们无法推断附近驾驶者的行为或意图。本文提出了一种具有轨迹和意图预测的分布式多智能体强化学习（MARL）算法，用于在高密度和异构交通场景中进行意图感知规划。我们的iPLAN方法使智能体仅从其本地观测中推断附近驾驶者的意图。我们模拟了两个不同的激励因素：行为激励用于智能体的长期规划，基于它们的驾驶行为或个性；瞬时激励用于智能体的短期规划，以基于当前交通状态进行碰撞避免。我们设计了一个双流推理模块，使智能体能够推断对手的激励并将其推断信息纳入决策。我们在两个模拟环境中进行了实验。

    Navigating safely and efficiently in dense and heterogeneous traffic scenarios is challenging for autonomous vehicles (AVs) due to their inability to infer the behaviors or intentions of nearby drivers. In this work, we propose a distributed multi-agent reinforcement learning (MARL) algorithm with trajectory and intent prediction in dense and heterogeneous traffic scenarios. Our approach for intent-aware planning, iPLAN, allows agents to infer nearby drivers' intents solely from their local observations. We model two distinct incentives for agents' strategies: Behavioral incentives for agents' long-term planning based on their driving behavior or personality; Instant incentives for agents' short-term planning for collision avoidance based on the current traffic state. We design a two-stream inference module that allows agents to infer their opponents' incentives and incorporate their inferred information into decision-making. We perform experiments on two simulation environments, Non-C
    
[^107]: 图像识别模型框架转换的故障定位

    Fault Localization for Framework Conversions of Image Recognition Models. (arXiv:2306.06157v1 [cs.CV])

    [http://arxiv.org/abs/2306.06157](http://arxiv.org/abs/2306.06157)

    本文提出针对深度学习框架转换中出现的模型崩溃和输出标签差异的故障定位和修复方法，成功修复多个图像识别模型跨多个深度学习框架的转换错误。

    

    在部署深度神经网络（DNNs）时，开发人员经常将模型从一个深度学习框架转换为另一个（例如，从TensorFlow到PyTorch）。然而，这个过程容易出错，并可能影响目标模型的准确性。为了确定这种影响的程度，我们对三个用于图像识别的DNNs（MobileNetV2、ResNet101和InceptionV3）进行了不同的分析，这些模型在四个深度学习框架（PyTorch、Keras、TensorFlow（TF）和TFLite）之间进行了转换，并发现了许多模型崩溃和输出标签差异高达100％。为了缓解这种错误，我们提出了一种新的方法来定位故障和修复有缺陷的深度学习框架转换，重点放在预训练的图像识别模型上。我们的技术包括四个主要分析阶段：1）转换工具，2）模型参数，3）模型超参数，4）图表示。此外，我们提出了许多针对故障定位和修复的策略，包括转换工具的推荐、调试技巧以及模型超参数的微调。我们通过成功修复所有测试的深度学习框架中MobileNetV2，ResNet101和InceptionV3 的有缺陷的转换来展示我们方法的有效性。

    When deploying Deep Neural Networks (DNNs), developers often convert models from one deep learning framework to another (e.g., TensorFlow to PyTorch). However, this process is error-prone and can impact target model accuracy. To identify the extent of such impact, we perform and briefly present a differential analysis against three DNNs used for image recognition (MobileNetV2, ResNet101, and InceptionV3), converted across four well-known deep learning frameworks (PyTorch, Keras, TensorFlow (TF), and TFLite), which revealed numerous model crashes and output label discrepancies of up to 100%. To mitigate such errors, we present a novel approach towards fault localization and repair of buggy deep learning framework conversions, focusing on pre-trained image recognition models. Our technique consists of four primary stages of analysis: 1) conversion tools, 2) model parameters, 3) model hyperparameters, and 4) graph representation. In addition, we propose a number of strategies towards faul
    
[^108]: AMEE：时间序列分类的解释评价框架

    AMEE: A Robust Framework for Explanation Evaluation in Time Series Classification. (arXiv:2306.05501v1 [cs.LG])

    [http://arxiv.org/abs/2306.05501](http://arxiv.org/abs/2306.05501)

    AMEE是一个模型无关的解释评价框架，用于量化和比较时间序列分类中多种基于显著性的解释方法的信息价值，帮助解决在这一领域中解释方法选择的难题。

    

    本文旨在提供一个框架，用于定量评估和排名时间序列分类任务中的解释方法，该任务涉及到卫生保健和金融等关键领域的普遍数据类型。最近对时间序列分类解释方法的研究兴趣激增，提供了各种各样的解释技术。然而，当这些解释技术在特定问题上产生分歧时，仍然不清楚使用哪种技术。比较解释以找到正确答案并不容易。两个关键挑战仍然存在：如何定量和稳健地评估给定解释方法的信息价值（即与分类任务相关性），以及如何并排比较解释方法。我们提出了AMEE，一种模型无关的解释评价框架，用于量化和比较多种应用于时间序列分类的基于显著性的解释方法。在输入时间序列中增加扰动

    This paper aims to provide a framework to quantitatively evaluate and rank explanation methods for the time series classification task, which deals with a prevalent data type in critical domains such as healthcare and finance. The recent surge of research interest in explanation methods for time series classification has provided a great variety of explanation techniques. Nevertheless, when these explanation techniques disagree on a specific problem, it remains unclear which of them to use. Comparing the explanations to find the right answer is non-trivial. Two key challenges remain: how to quantitatively and robustly evaluate the informativeness (i.e., relevance for the classification task) of a given explanation method, and how to compare explanation methods side-by-side. We propose AMEE, a Model-Agnostic Explanation Evaluation framework for quantifying and comparing multiple saliency-based explanations for time series classification. Perturbation is added to the input time series gu
    
[^109]: 通过双重过程将随机微分方程嵌入到神经网络中

    Embedding stochastic differential equations into neural networks via dual processes. (arXiv:2306.04847v1 [cs.LG])

    [http://arxiv.org/abs/2306.04847](http://arxiv.org/abs/2306.04847)

    该论文提出了一种新方法，通过将随机微分方程的信息直接与神经网络的权重进行比较，构建用于预测随机微分方程期望的神经网络。这种方法避免了过度拟合问题，并在原点附近的输入有着准确度。

    

    我们提出了一种新方法来构建用于预测随机微分方程期望的神经网络。该方法不需要输入和输出数据集；相反，从时间演化方程获得的信息，即相应的双重过程，直接与神经网络中的权重进行比较。作为演示，我们构建了用于Ornstein-Uhlenbeck过程和噪声van der Pol系统的神经网络。使用该方法学习的网络的显着特征是在原点附近的输入的准确度。因此，可以避免过度拟合问题，因为学习的网络不依赖于训练数据集。

    We propose a new approach to constructing a neural network for predicting expectations of stochastic differential equations. The proposed method does not need data sets of inputs and outputs; instead, the information obtained from the time-evolution equations, i.e., the corresponding dual process, is directly compared with the weights in the neural network. As a demonstration, we construct neural networks for the Ornstein-Uhlenbeck process and the noisy van der Pol system. The remarkable feature of learned networks with the proposed method is the accuracy of inputs near the origin. Hence, it would be possible to avoid the overfitting problem because the learned network does not depend on training data sets.
    
[^110]: 提高扩散以改善鲁棒性泛化

    Enhance Diffusion to Improve Robust Generalization. (arXiv:2306.02618v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02618](http://arxiv.org/abs/2306.02618)

    本文通过连续时间随机微分方程的研究，发现对抗性训练中的扩散项决定了神经网络的鲁棒泛化能力，进而提出了一种改进的AT框架。

    

    深度神经网络容易受到人类难以察觉的对抗性扰动的影响。其中一种最强的防御机制是对抗性训练（AT）。本文旨在解决AT中的两个主要问题。首先，在AT研究中如何设置具有性能保证的超参数仍然存在争议，定制化设置妨碍不同模型设计在AT研究中的公平比较。其次，经过鲁棒训练的神经网络在泛化时面临困难，并且受到严重的过拟合问题的困扰。本文聚焦于主要的AT框架 - 投影梯度下降对抗性训练（PGD-AT）。我们通过连续时间随机微分方程（SDE）近似PGD-AT的动态，并展示了该SDE的扩散项决定了鲁棒泛化。该理论发现的一个直接推论是，鲁棒泛化与学习率和批次大小之比呈正相关关系。

    Deep neural networks are susceptible to human imperceptible adversarial perturbations. One of the strongest defense mechanisms is \emph{Adversarial Training} (AT). In this paper, we aim to address two predominant problems in AT. First, there is still little consensus on how to set hyperparameters with a performance guarantee for AT research, and customized settings impede a fair comparison between different model designs in AT research. Second, the robustly trained neural networks struggle to generalize well and suffer from tremendous overfitting. This paper focuses on the primary AT framework - Projected Gradient Descent Adversarial Training (PGD-AT). We approximate the dynamic of PGD-AT by a continuous-time Stochastic Differential Equation (SDE), and show that the diffusion term of this SDE determines the robust generalization. An immediate implication of this theoretical finding is that robust generalization is positively correlated with the ratio between learning rate and batch siz
    
[^111]: 关于ReLU网络的大小无关样本复杂度

    On Size-Independent Sample Complexity of ReLU Networks. (arXiv:2306.01992v1 [cs.LG])

    [http://arxiv.org/abs/2306.01992](http://arxiv.org/abs/2306.01992)

    本文研究了ReLU神经网络的样本复杂度，给出了一个现有方法精细化的结果，实现了无深度依赖性的上界。

    

    我们从泛化的角度研究了学习ReLU神经网络的样本复杂度。在权重矩阵上给定范数约束的情况下，一个常见的方法是估计相关函数类的Rademacher复杂度。之前Golowich-Rakhlin-Shamir (2020)获得了一个不依赖于网络大小的（与Frobenius范数的乘积成比例）上界，除了一个平方根深度的因子。我们给出了一个精细化的结果，通常根本没有明显的深度依赖性。

    We study the sample complexity of learning ReLU neural networks from the point of view of generalization. Given norm constraints on the weight matrices, a common approach is to estimate the Rademacher complexity of the associated function class. Previously Golowich-Rakhlin-Shamir (2020) obtained a bound independent of the network size (scaling with a product of Frobenius norms) except for a factor of the square-root depth. We give a refinement which often has no explicit depth-dependence at all.
    
[^112]: 多数规则：通过自洽性实现更好的修补

    Majority Rule: better patching via Self-Consistency. (arXiv:2306.00108v1 [cs.SE])

    [http://arxiv.org/abs/2306.00108](http://arxiv.org/abs/2306.00108)

    本文提出了一种无需解释的算法，基于自洽性和大型语言模型（LLMs）来进行软件补丁选择，从而解决当前缺乏解释的软件数据集的问题。

    

    大型语言模型（LLMs）可以被引导解决一些需要“少量提示”的问题，包括示例问题-解决方案对。现在，如果少量提示也包括“思维链”（CoT）解释，形式为问题-解释-解决方案，LLMs将会产生一个“解释”的解决方案，并表现得更好。最近，一项更出色的技术“自洽性”（S-C）出现了，基于这样一种直觉：正确的解决方案有许多可能的解释。当LLM被反复采样以生成问题的解释-解决方案对时，对于给定的问题，池中最常发生的解决方案（忽略解释）更有可能是正确的！但是，这种高性能的S-C（甚至CoT）方法在软件工程环境中的使用受到解释的缺乏的限制；大多数软件数据集都缺乏解释。在本文中，我们描述了一个S-C算法的应用，该算法不需要解释，因此可以用于软件补丁选择。

    Large Language models (LLMs) can be induced to solve non-trivial problems with "few-shot" prompts including illustrative problem-solution examples. Now if the few-shots also include "chain of thought" (CoT) explanations, which are of the form problem-explanation-solution, LLMs will generate a "explained" solution, and perform even better. Recently an exciting, substantially better technique, self-consistency [1] (S-C) has emerged, based on the intuition that there are many plausible explanations for the right solution; when the LLM is sampled repeatedly to generate a pool of explanation-solution pairs, for a given problem, the most frequently occurring solutions in the pool (ignoring the explanations) tend to be even more likely to be correct! Unfortunately, the use of this highly-performant S-C (or even CoT) approach in software engineering settings is hampered by the lack of explanations; most software datasets lack explanations. In this paper, we describe an application of the S-C a
    
[^113]: InGram：通过关系图进行归纳知识图谱嵌入

    InGram: Inductive Knowledge Graph Embedding via Relation Graphs. (arXiv:2305.19987v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19987](http://arxiv.org/abs/2305.19987)

    InGram是一种新的归纳式知识图谱补全方法，可以在推理时生成新关系和实体的嵌入，并使用注意力机制汇总邻居嵌入生成关系和实体嵌入。该方法在多个基准数据集上的性能优于现有的基准方法。

    

    归纳知识图谱补全被视为预测训练期间未观察到的新实体之间的缺失三元组的任务。该论文提出了一个新方法InGram，它可以在推理时生成新关系和实体的嵌入，并基于关系图和原始知识图谱使用注意力机制来汇总邻居嵌入以生成关系和实体嵌入。实验结果表明，在几个基准数据集上，InGram的性能优于现有的基准方法。

    Inductive knowledge graph completion has been considered as the task of predicting missing triplets between new entities that are not observed during training. While most inductive knowledge graph completion methods assume that all entities can be new, they do not allow new relations to appear at inference time. This restriction prohibits the existing methods from appropriately handling real-world knowledge graphs where new entities accompany new relations. In this paper, we propose an INductive knowledge GRAph eMbedding method, InGram, that can generate embeddings of new relations as well as new entities at inference time. Given a knowledge graph, we define a relation graph as a weighted graph consisting of relations and the affinity weights between them. Based on the relation graph and the original knowledge graph, InGram learns how to aggregate neighboring embeddings to generate relation and entity embeddings using an attention mechanism. Experimental results show that InGram outper
    
[^114]: 用Transformer学习超关系型和数值知识图中的表征学习

    Representation Learning on Hyper-Relational and Numeric Knowledge Graphs with Transformers. (arXiv:2305.18256v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18256](http://arxiv.org/abs/2305.18256)

    本文提出了一个名为HyNT的框架，用于学习超关系型知识图的表示，包括数值文字。该框架使用上下文Transformer和预测Transformer，通过学习三元组和其限定词之间的相关性以及数值信息来获得模型。

    

    近期研究了一个超关系型知识图谱，其中三元组与限定词集合相关联; 一个限定词由关系和实体组成，为三元组提供辅助信息。现有的超关系型知识图嵌入方法假定实体是离散对象，但有些信息应使用数值表示，例如(J.R.R.，出生于，1892)。同时，三元组(J.R.R.，就读于，牛津大学)可以与限定词(开始时间，1911)相关联。在本文中，我们提出了一个名为HyNT的统一框架，用于学习包含三元组或限定词中数值文字的超关系型知识图的表示。我们定义了一个上下文Transformer和一个预测Transformer，来学习表示，不仅基于三元组和其限定词之间的相关性，还基于数值信息。通过学习三元组和限定词的紧凑表示，并将它们馈送给Transformer来获得模型

    A hyper-relational knowledge graph has been recently studied where a triplet is associated with a set of qualifiers; a qualifier is composed of a relation and an entity, providing auxiliary information for a triplet. While existing hyper-relational knowledge graph embedding methods assume that the entities are discrete objects, some information should be represented using numeric values, e.g., (J.R.R., was born in, 1892). Also, a triplet (J.R.R., educated at, Oxford Univ.) can be associated with a qualifier such as (start time, 1911). In this paper, we propose a unified framework named HyNT that learns representations of a hyper-relational knowledge graph containing numeric literals in either triplets or qualifiers. We define a context transformer and a prediction transformer to learn the representations based not only on the correlations between a triplet and its qualifiers but also on the numeric information. By learning compact representations of triplets and qualifiers and feeding 
    
[^115]: 利用分子对接和机器学习回归方法的药物重用以靶向COVID-19 3CL Protease

    Drug Repurposing Targeting COVID-19 3CL Protease using Molecular Docking and Machine Learning Regression Approach. (arXiv:2305.18088v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2305.18088](http://arxiv.org/abs/2305.18088)

    本研究利用分子对接和机器学习回归方法，筛选出针对 SARS-CoV-2的主要蛋白酶3CL潜在治疗药物。其中，决策树回归（DTR）模型具有改进的统计措施R2和RMSE，有助于识别具有高结合亲和力和有利的结合能的药物。

    

    COVID-19疫情已经成为全球健康危机，迫切需要快速鉴定潜在的治疗药物。为了应对这一挑战，药物重用是省时省力的唯一解决方案。本研究使用Zinc数据库对全球已批准（包括FDA批准）的5903种药物进行筛选，作为潜在的COVID-19治疗药物，以靶向SARS-CoV-2的主要蛋白酶3CL。我们使用Autodock-Vina进行分子对接，检查药物分子的功效。为了提高药物重用的效率，我们采用决策树、额外树、MLP、KNN、XGBoost和梯度提升等多个机器学习回归方法建模结合药物的结合亲和力。计算结果表明，决策树回归（DTR）模型具有改进的统计措施R2和RMSE。这些模拟结果有助于识别具有高结合亲和力和有利的结合能的药物。

    The COVID-19 pandemic has created a global health crisis, driving the need for the rapid identification of potential therapeutics. To meet this challenge, drug repurposing is the only solution with saving cost and time. In this study, we used the Zinc database to screen the world-approved including FDA-approved 5903 drugs for repurposing as potential COVID-19 treatments targeting the main protease 3CL of SARS-CoV-2. We performed molecular docking using Autodock-Vina to check the efficacy of drug molecules. To enhance the efficiency of drug repurposing approach, we modeled the binding affinities using several machine learning regression approaches for QSAR modeling such as decision tree, extra trees, MLP, KNN, XGBoost, and gradient boosting. The computational results demonstrated that Decision Tree Regression (DTR) model has improved statistical measures of R2 and RMSE. These simulated results helped to identify drugs with high binding affinity and favorable binding energies. From the s
    
[^116]: 使用优化空间的同态矩阵乘法来改进隐私保护PCA

    Improved Privacy-Preserving PCA Using Space-optimized Homomorphic Matrix Multiplication. (arXiv:2305.17341v1 [cs.CR])

    [http://arxiv.org/abs/2305.17341](http://arxiv.org/abs/2305.17341)

    本文提出一种使用近似数值算术同态加密方案进行隐私保护PCA的新方法，相对以往方法，其在效率、准确性和可扩展性上均有提升，实现了同态矩阵乘法和高效同态电路，计算特征值和特征向量时具有良好的效果。

    

    主成分分析（PCA）是机器学习和数据分析领域中的重要技术。本研究提出了一种新的方法，使用近似数值算术同态加密方案进行隐私保护PCA。我们基于一种被称为PowerMethod的PCA常规方法，该方法以协方差矩阵作为输入，并产生与数据集的第一主成分对应的近似特征向量。我们的方法在效率、准确性和可扩展性方面优于以前的方法（如Pandas CSCML 21）。为了实现这样的效率和准确性，我们实现了以下优化：（i）优化了同态矩阵乘法技术（Jiang等人SIGSAC 2018），该技术在协方差矩阵的计算中起着关键作用；（ii）设计了一个有效的同态电路来同态计算协方差矩阵；（iii）设计了一种新颖且高效的同态加密方案用于特征值和特征向量的计算。

    Principal Component Analysis (PCA) is a pivotal technique in the fields of machine learning and data analysis. In this study, we present a novel approach for privacy-preserving PCA using an approximate numerical arithmetic homomorphic encryption scheme. We build our method upon a proposed PCA routine known as the PowerMethod, which takes the covariance matrix as input and produces an approximate eigenvector corresponding to the first principal component of the dataset. Our method surpasses previous approaches (e.g., Pandas CSCML 21) in terms of efficiency, accuracy, and scalability.  To achieve such efficiency and accuracy, we have implemented the following optimizations: (i) We optimized a homomorphic matrix multiplication technique (Jiang et al. SIGSAC 2018) that will play a crucial role in the computation of the covariance matrix. (ii) We devised an efficient homomorphic circuit for computing the covariance matrix homomorphically. (iii) We designed a novel and efficient homomorphic 
    
[^117]: NASimEmu: 用于训练能够推广到新场景的智能体的网络攻击模拟器与仿真器

    NASimEmu: Network Attack Simulator & Emulator for Training Agents Generalizing to Novel Scenarios. (arXiv:2305.17246v1 [cs.CR])

    [http://arxiv.org/abs/2305.17246](http://arxiv.org/abs/2305.17246)

    该论文提出了一个名为NASimEmu的新框架，旨在提高智能体在现实世界中表现良好的能力。该框架使用模拟器和仿真器的结合，使智能体能够在模拟中进行训练，并在仿真器中部署，从而验证所使用的抽象的真实性。该框架的设计旨在培训通用的智能体，能够在训练期间未见过的新场景中进行转移。

    

    当前用于使用深度强化学习培训攻击型渗透测试智能体的框架往往难以使智能体在现实世界中表现良好，原因在于基于模拟的框架中存在现实差距，而基于仿真的框架缺乏可扩展性。此外，现有的框架通常使用不现实的度量标准来衡量智能体在训练数据上的表现。本文介绍了一个名为NASimEmu的新框架，通过提供具有共享接口的模拟器和仿真器来解决这些问题。这种方法允许智能体在模拟中进行训练，并在仿真器中部署，从而验证所使用的抽象的真实性。我们的框架促进了能够在训练期间未见过的新场景中进行转移的通用智能体的开发。对于模拟部分，我们采用了一个现有的模拟器NASim并增强了其实现。仿真器使用行业级工具实施，如Vagrant、VirtualBox和Metasp。

    Current frameworks for training offensive penetration testing agents with deep reinforcement learning struggle to produce agents that perform well in real-world scenarios, due to the reality gap in simulation-based frameworks and the lack of scalability in emulation-based frameworks. Additionally, existing frameworks often use an unrealistic metric that measures the agents' performance on the training data. NASimEmu, a new framework introduced in this paper, addresses these issues by providing both a simulator and an emulator with a shared interface. This approach allows agents to be trained in simulation and deployed in the emulator, thus verifying the realism of the used abstraction. Our framework promotes the development of general agents that can transfer to novel scenarios unseen during their training. For the simulation part, we adopt an existing simulator NASim and enhance its realism. The emulator is implemented with industry-level tools, such as Vagrant, VirtualBox, and Metasp
    
[^118]: 从部分数据中重构、预测和稳定混沌动力学

    Reconstruction, forecasting, and stability of chaotic dynamics from partial data. (arXiv:2305.15111v2 [nlin.AO] UPDATED)

    [http://arxiv.org/abs/2305.15111](http://arxiv.org/abs/2305.15111)

    该论文提出了使用具有长短期记忆（LSTM）网络的数据驱动方法，从部分观测中重构出混沌系统的动力学，进行时间预测，并推断其稳定性。

    

    传统的基于方程的方法可能不适用于从部分观测中预测和计算混沌系统的稳定性。在这篇计算论文中，我们提出了数据驱动的方法来（i）推断未观测到（隐藏的）混沌变量的动力学（全状态重构）；（ii）对全状态的演化进行时间预测；和（iii）推断全状态的稳定性质。这些任务是通过长短期记忆（LSTM）网络来完成的，这些网络在训练时使用的观测数据仅限于状态的部分：（i）低到高分辨率的LSTM（LH-LSTM），它将部分观测作为训练输入，并在计算损失时需要访问整个系统状态；和（ii）物理信息LSTM（PI-LSTM），其设计是将部分观测与动力系统演化方程的积分形式相结合。首先，我们推导了LSTM的雅可比矩阵。其次，我们分析了一个混沌系统的例子。

    The forecasting and computation of the stability of chaotic systems from partial observations are tasks for which traditional equation-based methods may not be suitable. In this computational paper, we propose data-driven methods to (i) infer the dynamics of unobserved (hidden) chaotic variables (full-state reconstruction); (ii) time forecast the evolution of the full state; and (iii) infer the stability properties of the full state. The tasks are performed with long short-term memory (LSTM) networks, which are trained with observations (data) limited to only part of the state: (i) the low-to-high resolution LSTM (LH-LSTM), which takes partial observations as training input, and requires access to the full system state when computing the loss; and (ii) the physics-informed LSTM (PI-LSTM), which is designed to combine partial observations with the integral formulation of the dynamical system's evolution equations. First, we derive the Jacobian of the LSTMs. Second, we analyse a chaotic 
    
[^119]: MedLens: 通过选择医学体征和回归插值来提高死亡率预测

    MedLens: Improve mortality prediction via medical signs selecting and regression interpolation. (arXiv:2305.11742v1 [cs.LG])

    [http://arxiv.org/abs/2305.11742](http://arxiv.org/abs/2305.11742)

    本文介绍了一个自动选择医学体征和回归插值的方法（MedLens），用于解决电子病历中医学体征数据缺失率过高导致预测性能降低的问题。

    

    监测患者的健康状况并提前预测死亡率对及时提供患者护理和治疗至关重要。电子病历中的大量医学体征被用于先进的机器学习模型来进行预测。然而，原始临床体征的数据质量问题在文献中被较少讨论。通过对各种医学体征和大量患者住院记录中的缺失率和相关分数进行深入测量，我们发现综合缺失率非常高，大量无用的体征可能会损害预测模型的性能。我们得出结论，只有改善数据质量才能提高不同预测算法的基线准确性。我们设计了MedLens，通过统计自动选择重要医学体征，并使用灵活的插值方法处理高缺失率时间序列。

    Monitoring the health status of patients and predicting mortality in advance is vital for providing patients with timely care and treatment. Massive medical signs in electronic health records (EHR) are fitted into advanced machine learning models to make predictions. However, the data-quality problem of original clinical signs is less discussed in the literature. Based on an in-depth measurement of the missing rate and correlation score across various medical signs and a large amount of patient hospital admission records, we discovered the comprehensive missing rate is extremely high, and a large number of useless signs could hurt the performance of prediction models. Then we concluded that only improving data-quality could improve the baseline accuracy of different prediction algorithms. We designed MEDLENS, with an automatic vital medical signs selection approach via statistics and a flexible interpolation approach for high missing rate time series. After augmenting the data-quality 
    
[^120]: 关于使用不确定性量化模型的模型盗窃限制

    On the Limitations of Model Stealing with Uncertainty Quantification Models. (arXiv:2305.05293v1 [cs.LG])

    [http://arxiv.org/abs/2305.05293](http://arxiv.org/abs/2305.05293)

    本文研究了使用不确定性量化模型进行模型盗窃的局限性，发现在实际盗窃过程中相互不确定是不可避免的。作者尝试使用多个可能的网络并将它们的预测组合以提高质量，但结果表明只有微弱的改善。作者发现网络多样性不足是导致这一结果的原因之一。

    

    模型盗窃旨在以原始训练成本的一小部分推断受害者模型的功能。然而，在实践中，模型的架构、权重尺寸和原始训练数据无法准确确定，导致在盗窃过程中相互不确定。在这项工作中，我们通过生成多个可能的网络，并将它们的预测组合起来来显式地处理这种不确定性，从而提高盗窃模型的质量。为此，我们比较了五个流行的不确定性量化模型在模型盗窃任务中的表现。令人惊讶的是，我们的结果表明，这些考虑的模型在标签一致性（即保真度）方面只能带来微小的改进。为了找到原因，我们通过查看预测方差作为训练迭代函数的方式来检查模型预测的多样性。我们意识到，在训练过程中，模型往往具有相似的预测，这表明我们想要利用的网络多样性不存在。

    Model stealing aims at inferring a victim model's functionality at a fraction of the original training cost. While the goal is clear, in practice the model's architecture, weight dimension, and original training data can not be determined exactly, leading to mutual uncertainty during stealing. In this work, we explicitly tackle this uncertainty by generating multiple possible networks and combining their predictions to improve the quality of the stolen model. For this, we compare five popular uncertainty quantification models in a model stealing task. Surprisingly, our results indicate that the considered models only lead to marginal improvements in terms of label agreement (i.e., fidelity) to the stolen model. To find the cause of this, we inspect the diversity of the model's prediction by looking at the prediction variance as a function of training iterations. We realize that during training, the models tend to have similar predictions, indicating that the network diversity we wanted
    
[^121]: DietCNN:用于量化CNN的无乘积推理

    DietCNN: Multiplication-free Inference for Quantized CNNs. (arXiv:2305.05274v1 [cs.CV])

    [http://arxiv.org/abs/2305.05274](http://arxiv.org/abs/2305.05274)

    本文提出了一种用查表法代替CNN中乘法的新方法，其保留了主要CNN操作的语义，且可在FPGA实现中实现显著的能量降低，具有重要的实用价值。

    

    网络嵌入式系统与机器智能的不断增长的需求已经成为促进研究界在嵌入式资源有限的设备上实现基于卷积神经网络（CNN）的推断的催化剂。删除昂贵的乘法操作来重新设计CNN已经显示出在减少推断能量使用方面具有有前途的效果。本文提出了一种用查表法代替CNN中乘法的新方法。与完全修改CNN操作的现有方法不同，所提出的方法保留了主要CNN操作的语义。符合CNN层操作的现有机制确保了标准CNN的可靠性。实验证明，基于单个激活码本的无乘积CNN在MNIST-LeNet-5、CIFAR10-VGG-11和Tiny ImageNet-ResNet的FPGA实现中，每次推理能够实现4.7倍、5.6倍和3.5倍的能量降低。

    The rising demand for networked embedded systems with machine intelligence has been a catalyst for sustained attempts by the research community to implement Convolutional Neural Networks (CNN) based inferencing on embedded resource-limited devices. Redesigning a CNN by removing costly multiplication operations has already shown promising results in terms of reducing inference energy usage. This paper proposes a new method for replacing multiplications in a CNN by table look-ups. Unlike existing methods that completely modify the CNN operations, the proposed methodology preserves the semantics of the major CNN operations. Conforming to the existing mechanism of the CNN layer operations ensures that the reliability of a standard CNN is preserved. It is shown that the proposed multiplication-free CNN, based on a single activation codebook, can achieve 4.7x, 5.6x, and 3.5x reduction in energy per inference in an FPGA implementation of MNIST-LeNet-5, CIFAR10-VGG-11, and Tiny ImageNet-ResNet
    
[^122]: 使用梯度下降学习决策树

    Learning Decision Trees with Gradient Descent. (arXiv:2305.03515v1 [cs.LG])

    [http://arxiv.org/abs/2305.03515](http://arxiv.org/abs/2305.03515)

    本文提出了一种使用梯度下降学习决策树的新方法，可以联合优化所有树的参数，从而避免了贪心算法造成次优解的问题。该方法在二分类任务上表现优异，并在多类任务中达到有竞争力的结果。

    

    决策树是用于许多机器学习任务的常见工具，因为它们具有高度的解释性。然而，从数据中学习决策树是一个困难的优化问题，因为它是非凸和非可微的。因此，通常的方法是使用一种贪婪生长算法来学习决策树，在每个内部节点上局部最小化不纯度。不幸的是，这种贪心过程可能会导致次优的决策树。在本文中，我们提出了一种使用梯度下降学习难以处理的轴对齐决策树的新方法。所提出的方法使用反向传播和直通算子在密集的决策树表示上联合优化所有树的参数。我们的方法在二分类基准测试上优于现有方法，并在多类任务中实现了有竞争力的结果。

    Decision Trees (DTs) are commonly used for many machine learning tasks due to their high degree of interpretability. However, learning a DT from data is a difficult optimization problem, as it is non-convex and non-differentiable. Therefore, common approaches learn DTs using a greedy growth algorithm that minimizes the impurity locally at each internal node. Unfortunately, this greedy procedure can lead to suboptimal trees. In this paper, we present a novel approach for learning hard, axis-aligned DTs with gradient descent. The proposed method uses backpropagation with a straight-through operator on a dense DT representation to jointly optimize all tree parameters. Our approach outperforms existing methods on binary classification benchmarks and achieves competitive results for multi-class tasks.
    
[^123]: NNSplitter：基于自动权重混淆的DNN模型主动防御方案

    NNSplitter: An Active Defense Solution to DNN Model via Automated Weight Obfuscation. (arXiv:2305.00097v1 [cs.LG])

    [http://arxiv.org/abs/2305.00097](http://arxiv.org/abs/2305.00097)

    NNSplitter是一种主动保护深度神经网络模型知识产权的方案，通过将模型分为混淆模型和模型秘密两部分，采用可信执行环境和基于强化学习的控制器来最大化精度下降和减少混淆权重的数量。

    

    深度神经网络模型作为一种有价值的知识产权，已经通过数字水印等技术进行保护。然而，这种被动模型保护并不能完全防止模型滥用。本研究提出了一种主动模型知识产权保护方案，即NNSplitter，通过将模型分为两部分来主动保护模型：一个表现较差的混淆模型和由混淆权重的索引和原始值组成的模型秘密，只有授权用户才能访问。NNSplitter利用可信执行环境来保护秘密，并采用基于强化学习的控制器来减少混淆权重的数量，同时最大化精度下降。我们的实验表明，仅修改超过2800万个权重的313个（即0.001％），混淆VGG-11模型在Fashion-MNIST上的精度可以降低到10％。我们还证明NNSplitter具有隐蔽性和韧性。

    As a type of valuable intellectual property (IP), deep neural network (DNN) models have been protected by techniques like watermarking. However, such passive model protection cannot fully prevent model abuse. In this work, we propose an active model IP protection scheme, namely NNSplitter, which actively protects the model by splitting it into two parts: the obfuscated model that performs poorly due to weight obfuscation, and the model secrets consisting of the indexes and original values of the obfuscated weights, which can only be accessed by authorized users. NNSplitter uses the trusted execution environment to secure the secrets and a reinforcement learning-based controller to reduce the number of obfuscated weights while maximizing accuracy drop. Our experiments show that by only modifying 313 out of over 28 million (i.e., 0.001%) weights, the accuracy of the obfuscated VGG-11 model on Fashion-MNIST can drop to 10%. We also demonstrate that NNSplitter is stealthy and resilient aga
    
[^124]: 在线Platt缩放及其校准方法

    Online Platt Scaling with Calibeating. (arXiv:2305.00070v1 [cs.LG])

    [http://arxiv.org/abs/2305.00070](http://arxiv.org/abs/2305.00070)

    本文提出了一种在线Platt缩放及其校准方法，其理论基础强大，可以处理分布漂移和对抗性结果序列，无需超参数调整，在一系列合成和真实数据集上表现出卓越的性能。

    

    我们提出了一种在线后校准方法，称为在线Platt缩放(OPS)，它将Platt缩放技术与在线逻辑回归相结合。我们展示了OPS如何在分布漂移的i.i.d.和非i.i.d.情况下平稳适应。此外，当最佳的Platt缩放模型本身被错误校准时，我们使用一种最近开发的称为calibeating的技术来增强OPS，使其更加鲁棒。理论上，我们得到的OPS+calibeating方法对于对抗性结果序列是保证校准的。在实验上，它在一系列合成和真实数据集上均表现出卓越的性能，无需超参数调整。最后，我们将所有OPS思想扩展到beta缩放方法。

    We present an online post-hoc calibration method, called Online Platt Scaling (OPS), which combines the Platt scaling technique with online logistic regression. We demonstrate that OPS smoothly adapts between i.i.d. and non-i.i.d. settings with distribution drift. Further, in scenarios where the best Platt scaling model is itself miscalibrated, we enhance OPS by incorporating a recently developed technique called calibeating to make it more robust. Theoretically, our resulting OPS+calibeating method is guaranteed to be calibrated for adversarial outcome sequences. Empirically, it is effective on a range of synthetic and real-world datasets, with and without distribution drifts, achieving superior performance without hyperparameter tuning. Finally, we extend all OPS ideas to the beta scaling method.
    
[^125]: 从混沌中迸发出秩序：为物体检测排序事件表示法

    From Chaos Comes Order: Ordering Event Representations for Object Detection. (arXiv:2304.13455v1 [cs.CV])

    [http://arxiv.org/abs/2304.13455](http://arxiv.org/abs/2304.13455)

    本文提出了一种基于Gromov-Wasserstein Discrepancy选择最佳事件表示的方法，这种方法可以在多个表示、网络骨干和数据集上保持任务性能排名的一致性。利用这一方法，本文对大型事件表示法家族进行超参数搜索，选择最适合物体检测的表示法，取得了优于最先进的基于事件的对象检测方法的成果。

    

    如今，处理事件的顶尖深度神经网络在使用现成网络之前，首先将其转换为稠密的网格状输入表示。然而，传统上为任务选择适当的表示需要针对每个表示训练一个神经网络，并根据验证分数选择最佳表示，这非常耗时。在这项工作中，我们通过基于原始事件及其表示之间的Gromov-Wasserstein Discrepancy (GWD)选择最佳表示来消除这个瓶颈。它的计算速度大约比训练神经网络快200倍，同时在多个表示、网络骨干和数据集上保持事件表示法任务性能排名的一致性。这意味着找到具有高任务分数的表示相当于找到具有低GWD的表示。我们利用这一观察结果，首次对大型事件表示法家族进行超参数搜索，选择最适合物体检测的表示。我们的方法在Moving MNIST和N-Caltech101数据集上都优于最先进的基于事件的对象检测方法，在后者达到了83.0%的1%误报率下的mAP新的最高水平。

    Today, state-of-the-art deep neural networks that process events first convert them into dense, grid-like input representations before using an off-the-shelf network. However, selecting the appropriate representation for the task traditionally requires training a neural network for each representation and selecting the best one based on the validation score, which is very time-consuming. In this work, we eliminate this bottleneck by selecting the best representation based on the Gromov-Wasserstein Discrepancy (GWD) between the raw events and their representation. It is approximately 200 times faster to compute than training a neural network and preserves the task performance ranking of event representations across multiple representations, network backbones, and datasets. This means that finding a representation with a high task score is equivalent to finding a representation with a low GWD. We use this insight to, for the first time, perform a hyperparameter search on a large family o
    
[^126]: 大规模经济分派问题的端到端可行性优化代理

    End-to-End Feasible Optimization Proxies for Large-Scale Economic Dispatch. (arXiv:2304.11726v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2304.11726](http://arxiv.org/abs/2304.11726)

    本文提出了一种新颖的端到端学习和修复架构 (E2ELR) 用于经济分派问题的优化代理， 结果表明，E2ELR在大规模情况下取得了最先进的性能，优于其他基线至少一个数量级。

    

    本文提出了一种新颖的端到端学习和修复（E2ELR）架构，用于训练经济分派问题的优化代理。E2ELR结合了深度神经网络和闭式、可微分的修复层，从而以端到端的方式集成了学习和可行性。E2ELR还使用自监督学习进行训练，无需标记数据和离线解决大量优化问题。通过在具有数万个母线的行业规模电力网络上进行经济分派的评估，结果表明，自监督的E2ELR实现了最先进的性能，其优化间隙优于其他基线至少一个数量级。

    The paper proposes a novel End-to-End Learning and Repair (E2ELR) architecture for training optimization proxies for economic dispatch problems. E2ELR combines deep neural networks with closed-form, differentiable repair layers, thereby integrating learning and feasibility in an end-to-end fashion. E2ELR is also trained with self-supervised learning, removing the need for labeled data and the solving of numerous optimization problems offline. E2ELR is evaluated on industry-size power grids with tens of thousands of buses using an economic dispatch that co-optimizes energy and reserves. The results demonstrate that the self-supervised E2ELR achieves state-of-the-art performance, with optimality gaps that outperform other baselines by at least an order of magnitude.
    
[^127]: 视觉-语言模型的黑匣子少样本适应

    Black Box Few-Shot Adaptation for Vision-Language models. (arXiv:2304.01752v1 [cs.CV])

    [http://arxiv.org/abs/2304.01752](http://arxiv.org/abs/2304.01752)

    本文提出了一种黑匣子方法，实现了对预先计算的图像和文本特征的视觉-语言模型的快速少样本适应，适用于有监督和无监督训练，并且可以用于对单模型计算的图像和文本特征进行对齐。

    

    通过对比学习训练的视觉-语言模型在少样本情况下表现出很强的学习能力。软提示学习是少样本领域适用的最受欢迎的方法，旨在通过新领域引发的分布偏移来缩小模态差距。虽然该方法性能高效，但仍需要访问模型权重，并且在具有数十亿个参数的大型模型上可能会导致计算上的不可行性。本文提出了一种黑匣子方法，实现了对预先计算的图像和文本特征的 V-L 少样本适应，不需要访问模型权重，训练速度快数个数量级，适用于有监督和无监督训练，并且还可以用于对单模型计算的图像和文本特征进行对齐。

    Vision-Language (V-L) models trained with contrastive learning to align the visual and language modalities have been shown to be strong few-shot learners. Soft prompt learning is the method of choice for few-shot downstream adaption aiming to bridge the modality gap caused by the distribution shift induced by the new domain. While parameter-efficient, prompt learning still requires access to the model weights and can be computationally infeasible for large models with billions of parameters. To address these shortcomings, in this work, we describe a black-box method for V-L few-shot adaptation that (a) operates on pre-computed image and text features and hence works without access to the model's weights, (b) it is orders of magnitude faster at training time, (c) it is amenable to both supervised and unsupervised training, and (d) it can be even used to align image and text features computed from uni-modal models. To achieve this, we propose Linear Feature Alignment (LFA), a simple line
    
[^128]: DeepAccident：V2X自动驾驶运动和事故预测基准数据集

    DeepAccident: A Motion and Accident Prediction Benchmark for V2X Autonomous Driving. (arXiv:2304.01168v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2304.01168](http://arxiv.org/abs/2304.01168)

    本文提出了一个大规模的 DeepAccident 数据集，其中包含各种真实世界驾驶中发生的事故场景，并提出了一个端到端的运动和事故预测任务，该任务可用于直接评估自动驾驶算法的事故预测能力。

    

    安全是自动驾驶的首要任务。但是，目前没有已发布的数据集可以支持自动驾驶的直接和可解释的安全评估。在本文中，我们提出了 DeepAccident，这是一个通过现实模拟器生成的大规模数据集，包含经常在现实驾驶中发生的各种事故场景。DeepAccident 数据集包含 57k 个带注释帧和 285k 个带注释的样本，这几乎是大规模 nuScenes 数据集的 7 倍，其样本数为 40k。此外，我们基于所提出的数据集提出了一个新任务，即端到端的运动和事故预测，可用于直接评估不同自动驾驶算法的事故预测能力。此外，对于每种场景，我们设置了四辆车和一个基础设施来记录数据，从而为事故场景提供了多种视角，并使 V2X（车辆对一切）感知和预测研究成为可能。

    Safety is the primary priority of autonomous driving. Nevertheless, no published dataset currently supports the direct and explainable safety evaluation for autonomous driving. In this work, we propose DeepAccident, a large-scale dataset generated via a realistic simulator containing diverse accident scenarios that frequently occur in real-world driving. The proposed DeepAccident dataset contains 57K annotated frames and 285K annotated samples, approximately 7 times more than the large-scale nuScenes dataset with 40k annotated samples. In addition, we propose a new task, end-to-end motion and accident prediction, based on the proposed dataset, which can be used to directly evaluate the accident prediction ability for different autonomous driving algorithms. Furthermore, for each scenario, we set four vehicles along with one infrastructure to record data, thus providing diverse viewpoints for accident scenarios and enabling V2X (vehicle-to-everything) research on perception and predicti
    
[^129]: 使用人工智能在家中测量帕金森病的严重程度

    Using AI to Measure Parkinson's Disease Severity at Home. (arXiv:2303.17573v1 [cs.LG])

    [http://arxiv.org/abs/2303.17573](http://arxiv.org/abs/2303.17573)

    该论文提出了一种使用人工智能系统远程评估帕金森病患者运动表现的方法，该方法可重复用于类似的运动任务，拥有较高的可靠性和准确性。

    

    我们提出了一种使用人工智能系统远程评估帕金森病患者运动表现的方法。参与者在网络摄像头前完成了运动任务（即点击手指），250名全球参与者的数据按照运动障碍协会统一帕金森病评分量表 (MDS-UPDRS) 的标准由三名专家神经学家进行了评估。神经学家的评估具有高度的可靠性，内部一致性系数（ICC）为0.88。我们开发了计算机算法来获得与MDS-UPDRS指南一致且与神经学家的评估高度相关的客观测量结果。我们的机器学习模型在这些指标的训练下表现优于一个MDS-UPDRS认证的评分者，平均绝对误差（MAE）为0.59，而评分者的MAE为0.79。然而，该模型的表现略逊于专家神经学家（0.53 MAE）。该方法可重复用于类似的运动任务，提供了可能性。

    We present an artificial intelligence system to remotely assess the motor performance of individuals with Parkinson's disease (PD). Participants performed a motor task (i.e., tapping fingers) in front of a webcam, and data from 250 global participants were rated by three expert neurologists following the Movement Disorder Society Unified Parkinson's Disease Rating Scale (MDS-UPDRS). The neurologists' ratings were highly reliable, with an intra-class correlation coefficient (ICC) of 0.88. We developed computer algorithms to obtain objective measurements that align with the MDS-UPDRS guideline and are strongly correlated with the neurologists' ratings. Our machine learning model trained on these measures outperformed an MDS-UPDRS certified rater, with a mean absolute error (MAE) of 0.59 compared to the rater's MAE of 0.79. However, the model performed slightly worse than the expert neurologists (0.53 MAE). The methodology can be replicated for similar motor tasks, providing the possibili
    
[^130]: GRAF：图形注意融合网络

    GRAF: Graph Attention-aware Fusion Networks. (arXiv:2303.16781v1 [cs.LG])

    [http://arxiv.org/abs/2303.16781](http://arxiv.org/abs/2303.16781)

    本文提出了一个名为GRAF的方法，使用注意机制和网络融合在多个网络上使用基于GNN的方法。通过attention-based neighborhood aggregation，GRAF能够学习每个节点和关联的重要性，并根据它们在网络融合中进行边缘加权。

    

    许多现实世界的网络包含多种类型的节点和边。图神经网络(GNN)作为一种利用图结构数据上的节点特征的深度学习框架已经证明了卓越的性能。然而，流行的基于GNN的架构只能处理一个同构网络。使它们能够处理多个网络带来了额外的挑战，因为网络的异构性和现有关联的多样性。在本研究中，我们提出了一个名为GRAF的计算方法，利用GNN的方法在多个网络上使用注意机制和网络融合。使用基于注意力的邻域聚合，GRAF学习每个节点的邻居节点的重要性（称为节点级注意力）以及分层方式下的关联的重要性（称为关联级注意力）。然后，GRAF处理一个网络融合步骤，根据学习到的节点和关联级注意力加权每条边。

    A large number of real-world networks include multiple types of nodes and edges. Graph Neural Network (GNN) emerged as a deep learning framework to utilize node features on graph-structured data showing superior performance. However, popular GNN-based architectures operate on one homogeneous network. Enabling them to work on multiple networks brings additional challenges due to the heterogeneity of the networks and the multiplicity of the existing associations. In this study, we present a computational approach named GRAF utilizing GNN-based approaches on multiple networks with the help of attention mechanisms and network fusion. Using attention-based neighborhood aggregation, GRAF learns the importance of each neighbor per node (called node-level attention) followed by the importance of association (called association-level attention) in a hierarchical way. Then, GRAF processes a network fusion step weighing each edge according to learned node- and association-level attention, which r
    
[^131]: 针对风电功率预测的有目标对抗攻击

    Targeted Adversarial Attacks on Wind Power Forecasts. (arXiv:2303.16633v1 [cs.LG])

    [http://arxiv.org/abs/2303.16633](http://arxiv.org/abs/2303.16633)

    本研究检测了两个深度学习预测模型的脆弱性，并提出了一种新的评估指标。这些模型可以被针对性攻击，因此保护风电功率预测结果对现代电力系统的稳定性至关重要。

    

    近年来，研究人员提出了多种用于风电功率预测的深度学习模型。这些模型比传统的机器学习算法或物理模型更准确地预测了风电场或整个地区的风力发电。然而，最新的研究表明，深度学习模型常常会受到对抗性攻击的影响。由于风电功率预测对现代电力系统的稳定性至关重要，因此保护预测结果免受这种威胁非常重要。本文研究了两种不同预测模型对有目标、半有目标和无目标对抗攻击的脆弱性。我们考虑了一种用于预测风电场功率发电的长短期记忆（LSTM）网络和一种用于预测德国整个地区的风力发电的卷积神经网络（CNN）。此外，我们提出了总对抗鲁棒性评分（TARS），一种用于量化回归模型稳健性的评估指标。

    In recent years, researchers proposed a variety of deep learning models for wind power forecasting. These models predict the wind power generation of wind farms or entire regions more accurately than traditional machine learning algorithms or physical models. However, latest research has shown that deep learning models can often be manipulated by adversarial attacks. Since wind power forecasts are essential for the stability of modern power systems, it is important to protect them from this threat. In this work, we investigate the vulnerability of two different forecasting models to targeted, semitargeted, and untargeted adversarial attacks. We consider a Long Short-Term Memory (LSTM) network for predicting the power generation of a wind farm and a Convolutional Neural Network (CNN) for forecasting the wind power generation throughout Germany. Moreover, we propose the Total Adversarial Robustness Score (TARS), an evaluation metric for quantifying the robustness of regression models to 
    
[^132]: 基于图表示学习的恶意软件检测综述

    A Survey on Malware Detection with Graph Representation Learning. (arXiv:2303.16004v1 [cs.CR])

    [http://arxiv.org/abs/2303.16004](http://arxiv.org/abs/2303.16004)

    本综述对基于图表示学习的恶意软件检测进行了深入审查和总结，这是一种比传统方法更加健壮的解决方案。

    

    随着恶意软件数量和复杂性的增加，恶意软件检测已成为一个重要的问题。传统的基于签名和启发式的检测方法用于恶意软件检测，但不幸的是，它们在未知攻击方面的泛化能力较差，并且可以通过混淆技术轻松规避。近年来，机器学习（ML）和特别是深度学习（DL）通过从数据中学习有用的表示来在恶意软件检测方面取得了显著的成果，并且已成为传统方法的首选解决方案。最近，在基于图结构的数据上应用这些技术已在各个领域取得了最先进的性能，并展示了从恶意软件中学习更健壮表示的有希望的结果。然而，还没有关于基于图形深度学习进行恶意软件检测的文献综述。在本综述中，我们提供了一份深入的文献综述，概括和统一了现有的作品。

    Malware detection has become a major concern due to the increasing number and complexity of malware. Traditional detection methods based on signatures and heuristics are used for malware detection, but unfortunately, they suffer from poor generalization to unknown attacks and can be easily circumvented using obfuscation techniques. In recent years, Machine Learning (ML) and notably Deep Learning (DL) achieved impressive results in malware detection by learning useful representations from data and have become a solution preferred over traditional methods. More recently, the application of such techniques on graph-structured data has achieved state-of-the-art performance in various domains and demonstrates promising results in learning more robust representations from malware. Yet, no literature review focusing on graph-based deep learning for malware detection exists. In this survey, we provide an in-depth literature review to summarize and unify existing works under the common approach
    
[^133]: 用于商用设备真实射频指纹的蓝牙和WiFi数据集

    Bluetooth and WiFi Dataset for Real World RF Fingerprinting of Commercial Devices. (arXiv:2303.13538v1 [cs.NI])

    [http://arxiv.org/abs/2303.13538](http://arxiv.org/abs/2303.13538)

    该论文捕获了首个公开可访问的商用设备真实射频指纹数据集，这对于识别非法或未授权发射器具有重要意义。

    

    射频指纹作为一种物理层安全方案，可用于识别在共享RF频谱的非法或未授权发射器。然而，由于缺乏公开可访问的真实世界数据集，大多数研究都集中在使用软件定义无线电（SDR）生成合成波形，这不适用于实际部署环境。另一方面，现有的有限数据集仅关注生成一种波形的芯片组。商用现成（COTS）组合芯片组支持使用共享双频段天线的两种无线标准（例如WiFi和蓝牙），如在笔记本电脑、适配器、无线充电器、树莓派等IoT设备中广泛使用。因此，为跟上现代IoT环境的步伐，迫切需要捕获这些组合芯片组发射异构通信协议的真实世界开放数据集。为此，我们捕获了第一个已知的此类公开可访问数据集，其中包括来自各种商业设备的COTS Wi-Fi和蓝牙组合芯片组的RF发射。

    RF fingerprinting is emerging as a physical layer security scheme to identify illegitimate and/or unauthorized emitters sharing the RF spectrum. However, due to the lack of publicly accessible real-world datasets, most research focuses on generating synthetic waveforms with software-defined radios (SDRs) which are not suited for practical deployment settings. On other hand, the limited datasets that are available focus only on chipsets that generate only one kind of waveform. Commercial off-the-shelf (COTS) combo chipsets that support two wireless standards (for example WiFi and Bluetooth) over a shared dual-band antenna such as those found in laptops, adapters, wireless chargers, Raspberry Pis, among others are becoming ubiquitous in the IoT realm. Hence, to keep up with the modern IoT environment, there is a pressing need for real-world open datasets capturing emissions from these combo chipsets transmitting heterogeneous communication protocols. To this end, we capture the first kno
    
[^134]: 扩散式对抗净化的鲁棒评估

    Robust Evaluation of Diffusion-Based Adversarial Purification. (arXiv:2303.09051v1 [cs.CV])

    [http://arxiv.org/abs/2303.09051](http://arxiv.org/abs/2303.09051)

    本文分析了对基于扩散式净化方法的评估方式，并提出了一个新的指导方针，以衡量净化方法对抗性攻击的鲁棒性。同时，我们提出了一种新的净化策略，展示了与最先进的对抗性训练方法相竞争的结果。

    

    我们质疑当前对基于扩散式净化方法的评估方式。扩散式净化方法旨在消除测试数据点中的对抗性影响。由于基于训练和测试的解耦，该方法越来越受到关注，作为对抗性训练的替代方法。为了测量净化方法的鲁棒性，通常采用众所周知的白盒攻击。然而，由于这些攻击通常是为对抗性训练而量身定制的，因此不知道这些攻击是否对扩散式净化最有效。我们分析了当前的实践，并提供了一个新的指导方针，以衡量净化方法对抗性攻击的鲁棒性。基于我们的分析，我们进一步提出了一种新的净化策略，展示了与最先进的对抗性训练方法相竞争的结果。

    We question the current evaluation practice on diffusion-based purification methods. Diffusion-based purification methods aim to remove adversarial effects from an input data point at test time. The approach gains increasing attention as an alternative to adversarial training due to the disentangling between training and testing. Well-known white-box attacks are often employed to measure the robustness of the purification. However, it is unknown whether these attacks are the most effective for the diffusion-based purification since the attacks are often tailored for adversarial training. We analyze the current practices and provide a new guideline for measuring the robustness of purification methods against adversarial attacks. Based on our analysis, we further propose a new purification strategy showing competitive results against the state-of-the-art adversarial training approaches.
    
[^135]: 基于窗口的早期退出级联用于不确定性估计：当深度集成比单一模型更有效时

    Window-Based Early-Exit Cascades for Uncertainty Estimation: When Deep Ensembles are More Efficient than Single Models. (arXiv:2303.08010v1 [cs.LG])

    [http://arxiv.org/abs/2303.08010](http://arxiv.org/abs/2303.08010)

    本文研究了基于窗口的早期退出集成方法，以在保持模型可扩展性的同时实现不确定性估计任务的高效实现。实验结果表明，该方法在准确性和计算效率上都达到了最新的研究成果。

    

    深度集成是提高深度学习方法预测性能和不确定性估计的简单、可靠和有效方法。然而，由于需要部署多个独立模型，它们被广泛批评为计算开销大。最近的研究挑战了这种观点，表明对于预测准确性，集成可以比在同一架构族中缩放单一模型在推理时更具计算效率。通过通过早期退出方法级联集成成员实现这一目标。在这项工作中，我们研究如何将这些效率提高扩展到与不确定性估计相关的任务。由于许多这样的任务，例如选择性分类，都是二分类问题，我们的关键新颖见解是仅将接近二分决策边界的样本传递到后续级联阶段。在ImageNet规模的数据上进行的实验表明，所提出的基于窗口的早期退出集成在使用比基线更少的模型评估的同时，实现了最先进的不确定性估计性能，并且在预测性能上与完整集成相竞争。

    Deep Ensembles are a simple, reliable, and effective method of improving both the predictive performance and uncertainty estimates of deep learning approaches. However, they are widely criticised as being computationally expensive, due to the need to deploy multiple independent models. Recent work has challenged this view, showing that for predictive accuracy, ensembles can be more computationally efficient (at inference) than scaling single models within an architecture family. This is achieved by cascading ensemble members via an early-exit approach. In this work, we investigate extending these efficiency gains to tasks related to uncertainty estimation. As many such tasks, e.g. selective classification, are binary classification, our key novel insight is to only pass samples within a window close to the binary decision boundary to later cascade stages. Experiments on ImageNet-scale data across a number of network architectures and uncertainty tasks show that the proposed window-base
    
[^136]: 不同的好手臂识别

    Differential Good Arm Identification. (arXiv:2303.07154v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.07154](http://arxiv.org/abs/2303.07154)

    本文提出了DGAI算法，它可以在好手臂识别问题中通过深度学习的方式减少样本复杂性，并且在具有给定阈值的情况下进一步提高多臂赌博问题的性能。

    

    本文针对一种变体的随机多臂赌博问题，称之为好手臂识别（GAI）。 GAI是一个纯探索的赌博问题，其目标是在尽可能少的样本数下输出尽可能多的好手臂，其中好手臂被定义为其期望奖励大于给定阈值的手臂。 在这项工作中，我们提出DGAI-一种可微的好手臂识别算法，以数据驱动方式改进了现有技术HDoC算法的样本复杂性。 我们还展示了DGAI可以进一步提升通用多臂赌博（MAB）问题的性能，给定一个阈值作为先验知识应用于手臂集。 大量实验证实，我们的算法在合成数据集和真实世界数据集中的GAI和MAB任务中显著优于基线算法。

    This paper targets a variant of the stochastic multi-armed bandit problem called good arm identification (GAI). GAI is a pure-exploration bandit problem with the goal to output as many good arms using as few samples as possible, where a good arm is defined as an arm whose expected reward is greater than a given threshold. In this work, we propose DGAI - a differentiable good arm identification algorithm to improve the sample complexity of the state-of-the-art HDoC algorithm in a data-driven fashion. We also showed that the DGAI can further boost the performance of a general multi-arm bandit (MAB) problem given a threshold as a prior knowledge to the arm set. Extensive experiments confirm that our algorithm outperform the baseline algorithms significantly in both synthetic and real world datasets for both GAI and MAB tasks.
    
[^137]: TARGET: 通过无样本蒸馏实现联邦类式持续学习

    TARGET: Federated Class-Continual Learning via Exemplar-Free Distillation. (arXiv:2303.06937v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06937](http://arxiv.org/abs/2303.06937)

    本文研究了一个重要但鲜为人知的问题：联邦类式持续学习，在联邦学习中动态添加新的类别。我们提出了一种称为TARGET的新颖方法，通过无样本蒸馏来减轻FCCL中的灾难性遗忘问题，并保护客户数据的隐私。该方法利用先前训练的全局模型在模型层面上传递旧任务的知识，并通过生成器生成合成数据来模拟数据的全局分布。与先前的FCCL方法相比，TARGET无需额外的数据集或存储先前任务的私有数据。

    

    本文针对一个鲜为人知但重要的问题进行研究：联邦类式持续学习（FCCL），在联邦学习中动态添加新的类别。已有的FCCL方法存在各种限制，如需要额外的数据集或存储先前任务的私有数据。为此，我们首先证明非独立同分布的数据加剧了联邦学习中的灾难性遗忘问题。然后，我们提出了一种新颖的方法——TARGET（通过无样本蒸馏实现联邦类式持续学习），该方法在减轻FCCL的灾难性遗忘问题的同时保护客户数据的隐私。我们的方法利用先前训练的全局模型，在模型层面上将旧任务的知识传递给当前任务。此外，我们训练一个生成器来生成合成数据，以模拟每个客户端上数据的全局分布。与先前的FCCL方法相比，TARGET不需要额外的数据集或存储先前任务的私有数据。

    This paper focuses on an under-explored yet important problem: Federated Class-Continual Learning (FCCL), where new classes are dynamically added in federated learning. Existing FCCL works suffer from various limitations, such as requiring additional datasets or storing the private data from previous tasks. In response, we first demonstrate that non-IID data exacerbates catastrophic forgetting issue in FL. Then we propose a novel method called TARGET (federat\textbf{T}ed cl\textbf{A}ss-continual lea\textbf{R}nin\textbf{G} via \textbf{E}xemplar-free dis\textbf{T}illation), which alleviates catastrophic forgetting in FCCL while preserving client data privacy. Our proposed method leverages the previously trained global model to transfer knowledge of old tasks to the current task at the model level. Moreover, a generator is trained to produce synthetic data to simulate the global distribution of data on each client at the data level. Compared to previous FCCL methods, TARGET does not requi
    
[^138]: 通过操作微调数据集来克服预训练模型中的偏见

    Overcoming Bias in Pretrained Models by Manipulating the Finetuning Dataset. (arXiv:2303.06167v1 [cs.CV])

    [http://arxiv.org/abs/2303.06167](http://arxiv.org/abs/2303.06167)

    本文研究了预训练模型中的偏见问题，发现微调模型可以继承预训练模型的偏见，但通过对微调数据集进行干预可以纠正这种偏见，而且对性能的影响很小。这表明仔细策划微调数据集对于减少下游任务中的偏见非常重要，这样做甚至可以弥补预训练模型中的偏见。

    This paper investigates the bias problem in pretrained models and finds that finetuned models can inherit the biases of pretrained models, but these biases can be corrected by manipulating the finetuning dataset with little impact on performance. This implies that careful curation of the finetuning dataset is important for reducing biases on a downstream task, and doing so can even compensate for bias in the pretrained model.

    转移学习通过允许在大规模数据集上预训练的模型的表达特征被微调到更小、更具领域特定性的数据集的目标任务中而受益。然而，有人担心这些预训练模型可能带有自己的偏见，这些偏见会传播到微调模型中。在这项工作中，我们研究了偏见，当偏见被概念化为目标任务和敏感属性之间的虚假相关性以及数据集中特定群体的代表性不足时。在偏见的两种概念下，我们发现(1)在预训练模型的基础上微调的模型确实可以继承它们的偏见，但(2)通过对微调数据集进行相对较小的干预，这种偏见可以得到纠正，而且对性能的影响往往可以忽略不计。我们的发现意味着，仔细策划微调数据集对于减少下游任务中的偏见非常重要，这样做甚至可以弥补预训练模型中的偏见。

    Transfer learning is beneficial by allowing the expressive features of models pretrained on large-scale datasets to be finetuned for the target task of smaller, more domain-specific datasets. However, there is a concern that these pretrained models may come with their own biases which would propagate into the finetuned model. In this work, we investigate bias when conceptualized as both spurious correlations between the target task and a sensitive attribute as well as underrepresentation of a particular group in the dataset. Under both notions of bias, we find that (1) models finetuned on top of pretrained models can indeed inherit their biases, but (2) this bias can be corrected for through relatively minor interventions to the finetuning dataset, and often with a negligible impact to performance. Our findings imply that careful curation of the finetuning dataset is important for reducing biases on a downstream task, and doing so can even compensate for bias in the pretrained model.
    
[^139]: 通过受限代理学习控制深度序数分类中的类布局

    Controlling Class Layout for Deep Ordinal Classification via Constrained Proxies Learning. (arXiv:2303.00396v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.00396](http://arxiv.org/abs/2303.00396)

    本文提出了一种通过受限代理学习方法，可以有效地控制深度序数分类中的类布局。

    

    对于深度序数分类任务，学习特定于序数分类的良好结构化特征空间有助于恰当地捕捉类之间的序数属性。本文提出了一种新颖的受限代理学习方法，该方法可以为每个序数类学习一个代理，然后通过限制这些代理来调整类的全局布局。我们提出了两种策略：硬布局约束和软布局约束。硬布局约束通过直接控制代理的生成来实现，以强制将其放置在严格的线性布局或半圆形布局（即严格序数布局的两种实例）中。软布局约束通过引入正则化项到损失函数中来实现，该项惩罚偏离理想序数布局的情况。在基准数据集上的实验结果证明了所提出的CPL方法在深度序数分类中的有效性。

    For deep ordinal classification, learning a well-structured feature space specific to ordinal classification is helpful to properly capture the ordinal nature among classes. Intuitively, when Euclidean distance metric is used, an ideal ordinal layout in feature space would be that the sample clusters are arranged in class order along a straight line in space. However, enforcing samples to conform to a specific layout in the feature space is a challenging problem. To address this problem, in this paper, we propose a novel Constrained Proxies Learning (CPL) method, which can learn a proxy for each ordinal class and then adjusts the global layout of classes by constraining these proxies. Specifically, we propose two kinds of strategies: hard layout constraint and soft layout constraint. The hard layout constraint is realized by directly controlling the generation of proxies to force them to be placed in a strict linear layout or semicircular layout (i.e., two instantiations of strict ordi
    
[^140]: 单细胞多模态预测的Transformer研究

    Single-Cell Multimodal Prediction via Transformers. (arXiv:2303.00233v2 [q-bio.GN] UPDATED)

    [http://arxiv.org/abs/2303.00233](http://arxiv.org/abs/2303.00233)

    本研究研究了如何利用Transformer模型在端到端的方式上处理多模态单细胞数据，并利用下游任务信息，提出了一个名为scMoFormer的框架

    

    最近的多模态单细胞技术的发展使得从单个细胞中获取多个组学数据成为可能，从而实现对细胞状态和动态的更深入理解。然而，多模态单细胞数据的激增也带来了建模不同模态之间复杂相互作用的巨大挑战。最近的先进方法侧重于构建静态交互图，并应用图神经网络(GNNs)从多模态数据中学习。然而，这样的静态图可能不尽如人意，因为它们没有利用下游任务信息；而且，当深度堆叠GNN层时，GNNs也有一些固有的局限性。为了解决这些问题，本文研究了如何利用Transformer模型在端到端的方式上处理多模态单细胞数据，并利用下游任务信息。具体而言，我们提出了一个名为scMoFormer的框架，它可以轻松地整合外部的d

    The recent development of multimodal single-cell technology has made the possibility of acquiring multiple omics data from individual cells, thereby enabling a deeper understanding of cellular states and dynamics. Nevertheless, the proliferation of multimodal single-cell data also introduces tremendous challenges in modeling the complex interactions among different modalities. The recently advanced methods focus on constructing static interaction graphs and applying graph neural networks (GNNs) to learn from multimodal data. However, such static graphs can be suboptimal as they do not take advantage of the downstream task information; meanwhile GNNs also have some inherent limitations when deeply stacking GNN layers. To tackle these issues, in this work, we investigate how to leverage transformers for multimodal single-cell data in an end-to-end manner while exploiting downstream task information. In particular, we propose a scMoFormer framework which can readily incorporate external d
    
[^141]: 使用专家建议和随机化的Littlestone维度进行最优预测

    Optimal Prediction Using Expert Advice and Randomized Littlestone Dimension. (arXiv:2302.13849v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13849](http://arxiv.org/abs/2302.13849)

    本研究证明了在线学习中，在学习一个类别时，最优期望错误边界等于其随机化的Littlestone维度。在不可知的情况下，最优错误边界与最佳函数的错误次数之间存在特定关系。此外，该研究还解决了一个开放问题，并将其应用于预测问题。

    

    在在线学习中，经典的结果表明使用确定性学习器的最优错误边界可以通过Littlestone维度来实现（Littlestone '88）。我们证明了随机学习器的类似结果：我们证明了在学习一个类别 $\mathcal{H}$时，最优期望错误边界等于其随机化的Littlestone维度，即存在一个由 $\mathcal{H}$ 打碎的树，其平均深度为 $2d$，而 $d$ 是最大的维度。此外，我们进一步研究了在不可知的情况下，最优错误边界与 $\mathcal{H}$ 中最佳函数的错误次数 $k$ 之间的关系。我们证明了具有Littlestone维度 $d$ 的类别学习的最优随机化错误边界是 $k + \Theta (\sqrt{k d} + d )$。这也意味着确定性学习的最优错误边界是 $2k + O (\sqrt{k d} + d )$，从而解决了Auer和Long ['99]研究的一个开放问题。作为我们理论的一个应用，我们重新审视了经典问题的预测

    A classical result in online learning characterizes the optimal mistake bound achievable by deterministic learners using the Littlestone dimension (Littlestone '88). We prove an analogous result for randomized learners: we show that the optimal expected mistake bound in learning a class $\mathcal{H}$ equals its randomized Littlestone dimension, which is the largest $d$ for which there exists a tree shattered by $\mathcal{H}$ whose average depth is $2d$. We further study optimal mistake bounds in the agnostic case, as a function of the number of mistakes made by the best function in $\mathcal{H}$, denoted by $k$. We show that the optimal randomized mistake bound for learning a class with Littlestone dimension $d$ is $k + \Theta (\sqrt{k d} + d )$. This also implies an optimal deterministic mistake bound of $2k + O (\sqrt{k d} + d )$, thus resolving an open question which was studied by Auer and Long ['99].  As an application of our theory, we revisit the classical problem of prediction 
    
[^142]: 去噪扩散采样器

    Denoising Diffusion Samplers. (arXiv:2302.13834v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13834](http://arxiv.org/abs/2302.13834)

    去噪扩散采样器 (DDS) 近似地从非标准化概率密度函数中采样，并估计其标准化常数。

    

    去噪扩散模型是一种受欢迎的生成模型类别，在许多领域提供最先进的结果。通过使用扩散逐渐向数据添加噪声，将数据分布转化为高斯分布。生成模型的样本通过模拟该扩散的时间反演的近似，并初始化为高斯样本来获得。在实践中，通过使用评分匹配技术对时间反演过程中出现的棘手的评分项进行近似。我们在这里探索了一种类似的想法，用于近似采样非标准化概率密度函数并估计其标准化常数。我们考虑了一个目标密度向高斯扩散的过程。去噪扩散采样器 (DDS) 是通过近似相应的时间反演而获得的。尽管评分匹配在这种情况下不适用，但我们可以利用在 Monte Carlo 采样中引入的许多思想。

    Denoising diffusion models are a popular class of generative models providing state-of-the-art results in many domains. One adds gradually noise to data using a diffusion to transform the data distribution into a Gaussian distribution. Samples from the generative model are then obtained by simulating an approximation of the time-reversal of this diffusion initialized by Gaussian samples. Practically, the intractable score terms appearing in the time-reversed process are approximated using score matching techniques. We explore here a similar idea to sample approximately from unnormalized probability density functions and estimate their normalizing constants. We consider a process where the target density diffuses towards a Gaussian. Denoising Diffusion Samplers (DDS) are obtained by approximating the corresponding time-reversal. While score matching is not applicable in this context, we can leverage many of the ideas introduced in generative modeling for Monte Carlo sampling. Existing t
    
[^143]: 改变很难：子群体转变的深入探究

    Change is Hard: A Closer Look at Subpopulation Shift. (arXiv:2302.12254v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12254](http://arxiv.org/abs/2302.12254)

    本文分析了子群体转变的各种机制，对20个最先进的算法在12个领域内进行了全面的基准测试，发现现有算法只能应对某些转变，进一步地，提出一种简单易行的选择标准来改善现有算法性能。

    

    机器学习模型通常在训练数据中代表性不足的子群体上表现不佳。然而，对于导致子群体转变的机制以及算法在如此不同的转变中如何进行普遍化，我们知之甚少。在这项工作中，我们对子群体转变进行了细致的分析。首先，我们提出了一个统一的框架来剖析和解释子群体中的常见转变。然后，我们在视觉、语言和医疗领域的12个真实数据集上对20个最先进的算法进行了全面的基准测试。通过训练10,000多个模型得到的结果，我们揭示了未来在这个领域取得进展的有趣观察结果。首先，现有算法仅能在某些类型的转变上提高子群体的鲁棒性，而在其他类型的转变上则不能。此外，虽然当前算法依赖于群体标注的验证数据进行模型选择，但我们发现基于最差类别准确度的简单选择标准其实非常有效。

    Machine learning models often perform poorly on subgroups that are underrepresented in the training data. Yet, little is understood on the variation in mechanisms that cause subpopulation shifts, and how algorithms generalize across such diverse shifts at scale. In this work, we provide a fine-grained analysis of subpopulation shift. We first propose a unified framework that dissects and explains common shifts in subgroups. We then establish a comprehensive benchmark of 20 state-of-the-art algorithms evaluated on 12 real-world datasets in vision, language, and healthcare domains. With results obtained from training over 10,000 models, we reveal intriguing observations for future progress in this space. First, existing algorithms only improve subgroup robustness over certain types of shifts but not others. Moreover, while current algorithms rely on group-annotated validation data for model selection, we find that a simple selection criterion based on worst-class accuracy is surprisingly
    
[^144]: 无监督的域外检测与扩散修复

    Unsupervised Out-of-Distribution Detection with Diffusion Inpainting. (arXiv:2302.10326v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.10326](http://arxiv.org/abs/2302.10326)

    本文提出了一种新的无监督的域外检测方法LMD，通过利用扩散模型，将图像从原始流形抬升并映射到域内流形，从而识别出域外图像。

    

    无监督的域外检测(OOD)通过仅使用未标记的域内数据来识别域外数据。我们提出了一种新的方法，Lift, Map, Detect (LMD)，它利用了扩散模型的最新进展。扩散模型是生成模型的一种类型。在核心上，它们学习了一种逐步去噪的过程，逐渐将噪声图像映射到它们的训练流形上。LMD利用这种直觉进行域外检测。具体而言，LMD通过破坏图像来将其从原始流形上抬升，并使用扩散模型将其映射到域内流形上。对于域外图像，映射后的图像将离其原始流形很远，LMD会相应地将其识别为域外数据。我们通过大量实验表明，LMD在各种数据集上都取得了竞争性的性能。代码可以在https://github.com/zhenzhel/lift_map_detect找到。

    Unsupervised out-of-distribution detection (OOD) seeks to identify out-of-domain data by learning only from unlabeled in-domain data. We present a novel approach for this task - Lift, Map, Detect (LMD) - that leverages recent advancement in diffusion models. Diffusion models are one type of generative models. At their core, they learn an iterative denoising process that gradually maps a noisy image closer to their training manifolds. LMD leverages this intuition for OOD detection. Specifically, LMD lifts an image off its original manifold by corrupting it, and maps it towards the in-domain manifold with a diffusion model. For an out-of-domain image, the mapped image would have a large distance away from its original manifold, and LMD would identify it as OOD accordingly. We show through extensive experiments that LMD achieves competitive performance across a broad variety of datasets. Code can be found at https://github.com/zhenzhel/lift_map_detect.
    
[^145]: 多时间尺度学习在处理分布式合作多智能体深度强化学习中的非平稳性问题中的应用

    Dealing With Non-stationarity in Decentralized Cooperative Multi-Agent Deep Reinforcement Learning via Multi-Timescale Learning. (arXiv:2302.02792v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02792](http://arxiv.org/abs/2302.02792)

    本文提出了一种多时间尺度学习的方法来解决分布式合作多智能体深度强化学习中的非平稳性问题，通过顺序学习的方式更新智能体的策略可以保证收敛性。

    

    分布式合作的多智能体深度强化学习（MARL）可以成为一种通用的学习框架，特别适用于无法进行集中式训练或不实际的场景。在分布式深度MARL中，当多个智能体同时学习时，学习环境的非平稳性是一个关键挑战。一个常用且高效的分布式MARL方案是独立学习，在这种方案中，智能体独立更新策略。我们首先证明了独立学习并非总能收敛，而顺序学习，即智能体依次更新策略，能够保证收敛到一个智能体最优解。在顺序学习中，当一个智能体更新策略时，其他智能体的策略保持不变，缓解了由于其他智能体策略的同时更新而导致的非平稳性挑战。然而，顺序学习速度较慢，因为只有一个智能体进行更新。

    Decentralized cooperative multi-agent deep reinforcement learning (MARL) can be a versatile learning framework, particularly in scenarios where centralized training is either not possible or not practical. One of the critical challenges in decentralized deep MARL is the non-stationarity of the learning environment when multiple agents are learning concurrently. A commonly used and efficient scheme for decentralized MARL is independent learning in which agents concurrently update their policies independently of each other. We first show that independent learning does not always converge, while sequential learning where agents update their policies one after another in a sequence is guaranteed to converge to an agent-by-agent optimal solution. In sequential learning, when one agent updates its policy, all other agent's policies are kept fixed, alleviating the challenge of non-stationarity due to simultaneous updates in other agents' policies. However, it can be slow because only one agen
    
[^146]: 重温外行星种群的质量-半径关系：机器学习的洞见。

    Revisiting mass-radius relationships for exoplanet populations: a machine learning insight. (arXiv:2301.07143v2 [astro-ph.EP] UPDATED)

    [http://arxiv.org/abs/2301.07143](http://arxiv.org/abs/2301.07143)

    这项研究利用机器学习方法分析了762个已确认的外行星和八个太阳系行星的数据集，发现巨大行星具有较低的密度，主要由氢和氦构成，而小行星更密集，主要由更重的元素构成。研究还揭示了质量、轨道周期和恒星金属丰度对外行星半径的预测能力的重要性。

    

    外行星的发现数量正在增长，并且机器学习技术的进步为探索和理解我们太阳系以外的世界的特性开辟了新的途径。在这项研究中，我们使用高效的机器学习方法来分析包括762个已确认的外行星和八个太阳系行星的数据集，旨在表征它们的基本性质。通过应用不同的无监督聚类算法，我们将数据分成了两个主要类别：‘小’行星和‘巨大’行星，切割值分别为$R_{p}=8.13R_{\oplus}$和$M_{p}=52.48M_{\oplus}$。这种分类揭示了一个有趣的区别：巨大行星的密度较低，暗示它们具有更高的氢-氦质量分数，而小行星更密集，主要由更重的元素组成。我们应用了各种回归模型来揭示物理参数之间的相关性以及它们对外行星半径的预测能力。我们的分析强调了行星质量、轨道 période 和恒星金属丰度的重要性。

    The growing number of exoplanet discoveries and advances in machine learning techniques have opened new avenues for exploring and understanding the characteristics of worlds beyond our Solar System. In this study, we employ efficient machine learning approaches to analyze a dataset comprising 762 confirmed exoplanets and eight Solar System planets, aiming to characterize their fundamental quantities. By applying different unsupervised clustering algorithms, we classify the data into two main classes: 'small' and 'giant' planets, with cut-off values at $R_{p}=8.13R_{\oplus}$ and $M_{p}=52.48M_{\oplus}$. This classification reveals an intriguing distinction: giant planets have lower densities, suggesting higher H-He mass fractions, while small planets are denser, composed mainly of heavier elements. We apply various regression models to uncover correlations between physical parameters and their predictive power for exoplanet radius. Our analysis highlights that planetary mass, orbital pe
    
[^147]: 生物医学机器学习中的增强攻击

    Enhancement attacks in biomedical machine learning. (arXiv:2301.01885v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.01885](http://arxiv.org/abs/2301.01885)

    本研究针对生物医学机器学习中的可信度问题，通过开发增强攻击技术，成功地能够通过最小的特征改变显著提高分类器的预测性能，并保持原始数据和增强数据之间的高特征相似性。

    

    机器学习在生物医学研究中的应用日益增多，然而对于这些研究可信度的关注却往往被忽视。尽管一些先前的研究探讨了对医学图像模型性能进行破坏的对抗攻击的能力，但最近出现的“增强攻击”通过误导地提高模型性能可能会对生物医学机器学习构成更大的威胁。为了更好地了解可信度，我们开发了两种技术，可以通过极小的特征改变显著提高分类器的预测性能：1）普遍性能增强和2）某种方法相对于其他方法的增强。我们的增强框架可以将分类器的准确率从50％虚假提高到接近100％，同时保持原始数据和增强数据之间的高特征相似性（Pearson's r>0.99）。类似地，基于方法的增强框架有效地虚假提高了某种方法的预测性能。

    The prevalence of machine learning in biomedical research is rapidly growing, yet the trustworthiness of such research is often overlooked. While some previous works have investigated the ability of adversarial attacks to degrade model performance in medical imaging, the ability to falsely improve performance via recently-developed "enhancement attacks" may be a greater threat to biomedical machine learning. In the spirit of developing attacks to better understand trustworthiness, we developed two techniques to drastically enhance prediction performance of classifiers with minimal changes to features: 1) general enhancement of prediction performance, and 2) enhancement of a particular method over another. Our enhancement framework falsely improved classifiers' accuracy from 50% to almost 100% while maintaining high feature similarities between original and enhanced data (Pearson's r's>0.99). Similarly, the method-specific enhancement framework was effective in falsely improving the per
    
[^148]: 基于CLIP的通用模型用于器官分割和肿瘤检测

    CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection. (arXiv:2301.00785v3 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2301.00785](http://arxiv.org/abs/2301.00785)

    本文提出了基于CLIP的通用模型，通过文本嵌入学习解剖学关系，能够分割25种器官和6种肿瘤，具有强大的泛化能力。

    

    越来越多的公开数据集在自动化器官分割和肿瘤检测方面产生了显著的影响。然而，由于每个数据集的规模较小且部分标注问题，以及对不同类型肿瘤的有限探究，导致得到的模型通常限于分割特定的器官/肿瘤，并忽略解剖结构的语义，也无法推广到新领域。为了解决这些问题，我们提出了基于CLIP驱动的通用模型，将从对比语言-图像预训练 （CLIP）中学习到的文本嵌入结合到分割模型中。这种基于CLIP的标签编码捕捉了解剖学关系，使模型学习到结构化特征嵌入，并分割25个器官和6种类型的肿瘤。该模型由14个数据集组成，使用3410个CT扫描进行训练，然后在来自3个额外数据集的6162个外部CT扫描上进行评估。我们在医学影像分析的国际准确性基准测试（MIoU）中排名第一。

    An increasing number of public datasets have shown a marked impact on automated organ segmentation and tumor detection. However, due to the small size and partially labeled problem of each dataset, as well as a limited investigation of diverse types of tumors, the resulting models are often limited to segmenting specific organs/tumors and ignore the semantics of anatomical structures, nor can they be extended to novel domains. To address these issues, we propose the CLIP-Driven Universal Model, which incorporates text embedding learned from Contrastive Language-Image Pre-training (CLIP) to segmentation models. This CLIP-based label encoding captures anatomical relationships, enabling the model to learn a structured feature embedding and segment 25 organs and 6 types of tumors. The proposed model is developed from an assembly of 14 datasets, using a total of 3,410 CT scans for training and then evaluated on 6,162 external CT scans from 3 additional datasets. We rank first on the Medical
    
[^149]: REAP：一个大规模真实对抗贴纸基准测试

    REAP: A Large-Scale Realistic Adversarial Patch Benchmark. (arXiv:2212.05680v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.05680](http://arxiv.org/abs/2212.05680)

    本文提出了一个名为REAP的大规模真实对抗贴纸基准测试，该基准测试允许用户在真实图像和真实环境条件下评估对抗贴纸攻击，为解决依赖摄像头的物理系统面临的严重威胁提供了有效的工具。

    

    机器学习模型容易受到对抗扰动的影响。其中一种著名的攻击方式是对抗贴纸，这是一种带有特定图案的贴纸，使得模型在贴纸所贴物体上的预测错误。这种攻击对于依赖于摄像头的物理系统，如自动驾驶汽车，构成了严重威胁。尽管问题的重要性，但在真实环境中进行研究是困难的；在真实世界中评估攻击和防御策略成本高昂，而合成数据则不够真实。在本文中，我们提出了REAP（真实对抗贴纸）基准测试，这是一个数字基准测试，允许用户在真实图像和真实环境条件下评估对抗贴纸攻击。基于Mapillary Vistas数据集，我们的基准测试包含超过14,000个交通标志。每个标志都经过几何和光照变换的改变，这可以用来将数字生成的贴纸真实地应用到图像中。

    Machine learning models are known to be susceptible to adversarial perturbation. One famous attack is the adversarial patch, a sticker with a particularly crafted pattern that makes the model incorrectly predict the object it is placed on. This attack presents a critical threat to cyber-physical systems that rely on cameras such as autonomous cars. Despite the significance of the problem, conducting research in this setting has been difficult; evaluating attacks and defenses in the real world is exceptionally costly while synthetic data are unrealistic. In this work, we propose the REAP (REalistic Adversarial Patch) benchmark, a digital benchmark that allows the user to evaluate patch attacks on real images, and under real-world conditions. Built on top of the Mapillary Vistas dataset, our benchmark contains over 14,000 traffic signs. Each sign is augmented with a pair of geometric and lighting transformations, which can be used to apply a digitally generated patch realistically onto t
    
[^150]: 多速率变分自编码器：一次训练，得到完整的率失真曲线

    Multi-Rate VAE: Train Once, Get the Full Rate-Distortion Curve. (arXiv:2212.03905v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.03905](http://arxiv.org/abs/2212.03905)

    本文介绍了一种名为多速率VAE（MR-VAE）的框架，可在单次训练中学习与不同β对应的最优参数，通过使用超网络将β映射到最优参数，以实现率失真曲线的完整训练。

    

    变分自编码器（VAEs）是一种用于学习数据的潜在表示的强大工具，广泛应用于各种应用领域。在实践中，VAEs通常需要多次训练来选择潜在变量应该保留的信息量。重构误差（失真）和KL散度（率）之间的权衡通常由超参数β参数化。在本文中，我们引入了多速率VAE（MR-VAE），这是一个计算效率高的框架，可以在单次训练中学习与不同β对应的最优参数。关键思想是使用超网络明确地制定一个响应函数，将β映射到最优参数。MR-VAEs构建了一个紧凑的响应超网络，其中的预激活根据β进行有条件的门控。通过分析线性VAEs并展示它能够准确表示线性VAEs的响应函数，我们证明了所提出的架构的合理性。

    Variational autoencoders (VAEs) are powerful tools for learning latent representations of data used in a wide range of applications. In practice, VAEs usually require multiple training rounds to choose the amount of information the latent variable should retain. This trade-off between the reconstruction error (distortion) and the KL divergence (rate) is typically parameterized by a hyperparameter $\beta$. In this paper, we introduce Multi-Rate VAE (MR-VAE), a computationally efficient framework for learning optimal parameters corresponding to various $\beta$ in a single training run. The key idea is to explicitly formulate a response function that maps $\beta$ to the optimal parameters using hypernetworks. MR-VAEs construct a compact response hypernetwork where the pre-activations are conditionally gated based on $\beta$. We justify the proposed architecture by analyzing linear VAEs and showing that it can represent response functions exactly for linear VAEs. With the learned hypernetw
    
[^151]: {\mu}Split: 显微镜数据的高效图像分解方法

    {\mu}Split: efficient image decomposition for microscopy data. (arXiv:2211.12872v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.12872](http://arxiv.org/abs/2211.12872)

    uSplit是一种适用于荧光显微镜图像的高效图像分解方法，集成了横向上下文化，帮助训练更深的分层模型，并有效地减少平铺伪影问题。

    

    我们提出了 uSplit，一种专门用于荧光显微镜图像中的训练图像分解的方法。我们发现，使用常规的深度结构体系结构在训练时使用大图像块会获得最佳结果，使内存消耗成为进一步提高性能的限制因素。因此，我们引入了横向上下文化（LC），一种内存高效的方法来训练强大的网络，并展示LC在处理任务时始终带来了显著的改进。我们将LC与U-Nets、分层自编码器和分层VAEs集成，为此我们制定了一种改进的ELBO loss。此外，LC使得训练比原本更深的分层模型成为可能，并且有助于减少使用分割VAE预测时不可避免的平铺伪影。我们将uSplit应用于五个分解任务，一个是合成数据集，另外四个来自实际显微镜数据。LC实现了SOTA的结果（平均im）

    We present uSplit, a dedicated approach for trained image decomposition in the context of fluorescence microscopy images. We find that best results using regular deep architectures are achieved when large image patches are used during training, making memory consumption the limiting factor to further improving performance. We therefore introduce lateral contextualization (LC), a memory efficient way to train powerful networks and show that LC leads to consistent and significant improvements on the task at hand. We integrate LC with U-Nets, Hierarchical AEs, and Hierarchical VAEs, for which we formulate a modified ELBO loss. Additionally, LC enables training deeper hierarchical models than otherwise possible and, interestingly, helps to reduce tiling artefacts that are inherently impossible to avoid when using tiled VAE predictions. We apply uSplit to five decomposition tasks, one on a synthetic dataset, four others derived from real microscopy data. LC achieves SOTA results (average im
    
[^152]: 基于参数化分类的广义类别发现:一个基线研究

    Parametric Classification for Generalized Category Discovery: A Baseline Study. (arXiv:2211.11727v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.11727](http://arxiv.org/abs/2211.11727)

    该研究提出了一种简单而有效的参数化分类方法，该方法可以受益于熵正则化，在多个广义类别发现基准测试中实现最先进的性能，并对未知类别数量具有强大的稳健性。

    

    广义类别发现旨在利用从标注样本中学习到的知识，在未标注的数据集中发现新的类别。先前的研究认为，参数化分类器容易对已知类别过度拟合，并支持使用半监督k均值形成的非参数化分类器。然而，在本研究中，我们调查了参数化分类器的失败情况，验证了当有高质量的监督可用时先前的设计选择的有效性，并确定不可靠的伪标签是一个关键问题。我们证明了存在两种预测偏差：分类器更倾向于更频繁地预测已知的类别，并在已知和新颖类别之间产生一个不平衡的分布。基于这些发现，我们提出了一种简单而有效的参数化分类方法，可以受益于熵正则化，在多个广义类别发现基准测试中实现最先进的性能，并显示对未知类别数量具有强大的稳健性。我们希望这项研究成果能为未来更有效的GCD方法的发展做出贡献。

    Generalized Category Discovery (GCD) aims to discover novel categories in unlabelled datasets using knowledge learned from labelled samples. Previous studies argued that parametric classifiers are prone to overfitting to seen categories, and endorsed using a non-parametric classifier formed with semi-supervised k-means. However, in this study, we investigate the failure of parametric classifiers, verify the effectiveness of previous design choices when high-quality supervision is available, and identify unreliable pseudo-labels as a key problem. We demonstrate that two prediction biases exist: the classifier tends to predict seen classes more often, and produces an imbalanced distribution across seen and novel categories. Based on these findings, we propose a simple yet effective parametric classification method that benefits from entropy regularisation, achieves state-of-the-art performance on multiple GCD benchmarks and shows strong robustness to unknown class numbers. We hope the in
    
[^153]: CRONOS：基于Wi-Fi CSI的无人设备NLoS人体检测的彩色化和对比学习

    CRONOS: Colorization and Contrastive Learning for Device-Free NLoS Human Presence Detection using Wi-Fi CSI. (arXiv:2211.10354v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2211.10354](http://arxiv.org/abs/2211.10354)

    本文介绍了一种名为CRONOS的系统，可以通过彩色化和对比学习来基于Wi-Fi CSI实现无人设备NLoS人体检测，可以区分房间中的移动人员和空置。实验结果表明该系统在NLoS条件下能够准确地检测出房间中的人物存在。

    

    近年来，对于全面智能化服务和应用的需求迅速增长。通过传感器或摄像头进行无人检测已被广泛采用，但存在隐私问题以及对静止人员的错误检测。为了解决这些缺点，商用Wi-Fi设备捕获的信道状态信息(CSI)提供了丰富的信号特征，以进行准确的检测。然而，现有系统在非直视(NLoS)和静态场景下存在分类不准确的问题，例如当一个人静止站在房间角落时。在本文中，我们提出了一个名为CRONOS(基于彩色化和对比度学习增强的NLoS人体存在检测)的系统，它生成动态的复发图(RPs)和颜色编码的CSI比率以区分房间中的移动人员和空置。我们还结合监督对比学习来检索实质性的表征，其中咨询损失被制定为区分同类和异类的嵌入点之间距离度量的损失。在数据集上的实验结果表明，提出的系统在NLoS条件下能够准确地检测出房间中的人物存在。

    In recent years, the demand for pervasive smart services and applications has increased rapidly. Device-free human detection through sensors or cameras has been widely adopted, but it comes with privacy issues as well as misdetection for motionless people. To address these drawbacks, channel state information (CSI) captured from commercialized Wi-Fi devices provides rich signal features for accurate detection. However, existing systems suffer from inaccurate classification under a non-line-of-sight (NLoS) and stationary scenario, such as when a person is standing still in a room corner. In this work, we propose a system called CRONOS (Colorization and Contrastive Learning Enhanced NLoS Human Presence Detection), which generates dynamic recurrence plots (RPs) and color-coded CSI ratios to distinguish mobile people from vacancy in a room, respectively. We also incorporate supervised contrastive learning to retrieve substantial representations, where consultation loss is formulated to dif
    
[^154]: 将安全和隐私保护的自动化机器学习操作整合到端到端的综合物联网边缘人工智能区块链监控系统中进行糖尿病预测

    Secure and Privacy-Preserving Automated Machine Learning Operations into End-to-End Integrated IoT-Edge-Artificial Intelligence-Blockchain Monitoring System for Diabetes Mellitus Prediction. (arXiv:2211.07643v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.07643](http://arxiv.org/abs/2211.07643)

    本文提出了一种基于区块链的物联网边缘人工智能系统，用于通过危险因素预测糖尿病，以确保用户数据的安全性和隐私性，并进行了比较分析不同医疗传感器、设备和方法的效果。

    

    糖尿病是全球死亡主要原因之一，目前无法治愈，如果不治疗，可能导致严重的健康并发症，如视网膜病变、肢体截肢、心血管疾病和神经疾病。因此，采取预防措施以避免/预测糖尿病的发生变得至关重要。已经提出和评估了用于糖尿病预测的机器学习方法。本文提出了一种基于危险因素的物联网边缘人工智能区块链系统进行糖尿病预测。所提出的系统建立在区块链技术的基础上，从不同医院的患者中获取危险因素数据的整体视图，并确保用户数据的安全性和隐私性。此外，我们对系统中不同的医疗传感器、设备和方法进行了比较分析，以测量和收集危险因素的值。进行了数值实验证明和比较分析。

    Diabetes Mellitus, one of the leading causes of death worldwide, has no cure to date and can lead to severe health complications, such as retinopathy, limb amputation, cardiovascular diseases, and neuronal disease, if left untreated. Consequently, it becomes crucial to take precautionary measures to avoid/predict the occurrence of diabetes. Machine learning approaches have been proposed and evaluated in the literature for diabetes prediction. This paper proposes an IoT-edge-Artificial Intelligence (AI)-blockchain system for diabetes prediction based on risk factors. The proposed system is underpinned by the blockchain to obtain a cohesive view of the risk factors data from patients across different hospitals and to ensure security and privacy of the user's data. Furthermore, we provide a comparative analysis of different medical sensors, devices, and methods to measure and collect the risk factors values in the system. Numerical experiments and comparative analysis were carried out bet
    
[^155]: 大型预训练模型在低资源语音识别中的高效利用

    Efficient Utilization of Large Pre-Trained Models for Low Resource ASR. (arXiv:2210.15445v3 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2210.15445](http://arxiv.org/abs/2210.15445)

    本研究探讨了在低资源语音识别中如何高效利用大型预训练模型，通过无监督技术和改进的架构和训练方法取得了显著的性能提升。

    

    最近，无监督的表示学习帮助自动语音识别(ASR)解决了有限标签数据的任务。在此基础上，硬件限制和应用程序给出了如何高效利用大型预训练模型并降低其复杂性的问题。在本研究中，我们研究了越南语和德语在医疗领域中的具有挑战性的低资源电话会话语音语料库。我们展示了利用无监督技术超越简单微调大型预训练模型的好处，讨论了如何将它们适应到实际的电话任务，包括带宽传输，并调查了不同的预训练和微调数据条件。我们使用预训练技术相对于项目基线提高了22%。通过架构和训练的改进，可以进一步提高29%，通过添加0.8小时的领域内自适应数据可以提高6%。

    Unsupervised representation learning has recently helped automatic speech recognition (ASR) to tackle tasks with limited labeled data. Following this, hardware limitations and applications give rise to the question how to take advantage of large pre-trained models efficiently and reduce their complexity. In this work, we study a challenging low resource conversational telephony speech corpus from the medical domain in Vietnamese and German. We show the benefits of using unsupervised techniques beyond simple fine-tuning of large pre-trained models, discuss how to adapt them to a practical telephony task including bandwidth transfer and investigate different data conditions for pre-training and fine-tuning. We outperform the project baselines by 22% relative using pretraining techniques. Further gains of 29% can be achieved by refinements of architecture and training and 6% by adding 0.8 h of in-domain adaptation data.
    
[^156]: 神经图模型

    Neural Graphical Models. (arXiv:2210.00453v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.00453](http://arxiv.org/abs/2210.00453)

    本文介绍了神经图模型（NGMs），它可以以合理的计算成本表示复杂的特征依赖关系，适应多种图结构和混合输入数据类型，并提供了高效的学习、推断和采样算法。

    

    概率图模型经常被用来理解系统的动态。它们可以建模特征（节点）之间的关系和底层分布。理论上，这些模型可以表示非常复杂的依赖函数，但在实践中，由于与图操作相关的计算限制，通常会做简化假设。在这项工作中，我们引入了神经图模型（NGMs），试图以合理的计算成本表示复杂的特征依赖关系。给定特征关系图和相应样本，我们通过使用神经网络作为多任务学习框架来捕捉特征之间的依赖结构以及它们的复杂函数表示。我们提供了高效的学习、推断和采样算法。NGMs可以适应通用的图结构，包括有向图、无向图和混合边图，同时支持混合输入数据类型。我们展示了经验研究结果，证明了NGMs的能力。

    Probabilistic Graphical Models are often used to understand dynamics of a system. They can model relationships between features (nodes) and the underlying distribution. Theoretically these models can represent very complex dependency functions, but in practice often simplifying assumptions are made due to computational limitations associated with graph operations. In this work we introduce Neural Graphical Models (NGMs) which attempt to represent complex feature dependencies with reasonable computational costs. Given a graph of feature relationships and corresponding samples, we capture the dependency structure between the features along with their complex function representations by using a neural network as a multi-task learning framework. We provide efficient learning, inference and sampling algorithms. NGMs can fit generic graph structures including directed, undirected and mixed-edge graphs as well as support mixed input data types. We present empirical studies that show NGMs' cap
    
[^157]: 不需要先验知识的分割已知物体和未知物体

    Segmenting Known Objects and Unseen Unknowns without Prior Knowledge. (arXiv:2209.05407v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.05407](http://arxiv.org/abs/2209.05407)

    本研究提出一种不需要先验知识的整体分割方法，可以将已知的物体和未知的物体进行准确分割，并在处理已知类别时对未知类别进行鲁棒处理。

    

    泛视域分割方法会根据输入将已知类别分配给每个像素。即使在最先进的方法中，这必然会导致在训练类别之外的物体上产生错误的预测。然而，在安全关键的环境中，对于未知样本和极端情况的鲁棒性至关重要，以避免危险后果。由于现实世界的数据集无法包含足够的数据点来充分采样底层分布的长尾部分，模型必须能够处理未见过和未知的情况。先前的方法通过重新识别已看到的未标记对象来解决这个问题。在这项工作中，我们提出了扩展分割的必要步骤，我们将其称为整体分割。整体分割旨在从未见过的未知类别中识别和分离对象实例，而无需任何先验知识，同时执行已知类别的泛视域分割。

    Panoptic segmentation methods assign a known class to each pixel given in input. Even for state-of-the-art approaches, this inevitably enforces decisions that systematically lead to wrong predictions for objects outside the training categories. However, robustness against out-of-distribution samples and corner cases is crucial in safety-critical settings to avoid dangerous consequences. Since real-world datasets cannot contain enough data points to adequately sample the long tail of the underlying distribution, models must be able to deal with unseen and unknown scenarios as well. Previous methods targeted this by re-identifying already-seen unlabeled objects. In this work, we propose the necessary step to extend segmentation with a new setting which we term holistic segmentation. Holistic segmentation aims to identify and separate objects of unseen unknown categories into instances, without any prior knowledge about them, while performing panoptic segmentation of known classes. We tac
    
[^158]: 损失的几何和微积分

    The Geometry and Calculus of Losses. (arXiv:2209.00238v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.00238](http://arxiv.org/abs/2209.00238)

    本文从凸集的新角度系统地发展了损失函数理论，引入了一种自动合适的损失函数定义方法，并提供了损失和范数之间关系的新机会，以及凸集微积分下的损失插值方法。

    

    统计决策问题是统计机器学习的核心。最简单的问题是二元和多类分类以及类概率估计。它们的定义的核心是选择损失函数，这是评估解决方案质量的手段。在本文中，我们从一种新颖的角度系统地发展了这类问题的损失函数理论，其基本要素是具有特定结构的凸集。损失函数被定义为凸集的支撑函数的次梯度。因此，它自动是合适的（用于概率估计）。这种视角提供了三个新颖的机会。它使得损失和(反)范数之间的基本关系的发展成为可能，这似乎以前没有被注意到。其次，它通过凸集的微积分使得损失的微积分的发展成为可能，从而允许在不同的损失之间进行插值。

    Statistical decision problems lie at the heart of statistical machine learning. The simplest problems are binary and multiclass classification and class probability estimation. Central to their definition is the choice of loss function, which is the means by which the quality of a solution is evaluated. In this paper we systematically develop the theory of loss functions for such problems from a novel perspective whose basic ingredients are convex sets with a particular structure. The loss function is defined as the subgradient of the support function of the convex set. It is consequently automatically proper (calibrated for probability estimation). This perspective provides three novel opportunities. It enables the development of a fundamental relationship between losses and (anti)-norms that appears to have not been noticed before. Second, it enables the development of a calculus of losses induced by the calculus of convex sets which allows the interpolation between different losses,
    
[^159]: GHN-Q：通过图形超网络预测未见量化卷积架构的参数

    GHN-Q: Parameter Prediction for Unseen Quantized Convolutional Architectures via Graph Hypernetworks. (arXiv:2208.12489v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.12489](http://arxiv.org/abs/2208.12489)

    本论文提出了一种名为GHN-Q的方法，通过图形超网络来预测未见量化卷积架构的参数，以提高量化鲁棒性。

    

    深度卷积神经网络（CNN）通过迭代优化训练已经取得了令人难以置信的成功，找到了最佳参数。然而，现代CNN架构通常包含数百万个参数。因此，对于单个架构的任何给定模型都存在一个庞大的参数空间。具有相似损失的模型可能具有截然不同的特性，如对抗鲁棒性、泛化能力和量化鲁棒性。对于边缘上的深度学习，量化鲁棒性通常至关重要。找到一个量化鲁棒的模型有时可能需要很大的努力。最近使用图形超网络（GHN）的研究表明，它在预测不同CNN架构的高性能参数方面表现出了显著的性能。受到这些成功的启发，我们想知道GHN-2的图形表示是否也可以用于预测量化鲁棒的参数，我们将其称为GHN-Q。我们进行了有史以来第一次探索使用图形超网络来预测量化鲁棒参数的研究。

    Deep convolutional neural network (CNN) training via iterative optimization has had incredible success in finding optimal parameters. However, modern CNN architectures often contain millions of parameters. Thus, any given model for a single architecture resides in a massive parameter space. Models with similar loss could have drastically different characteristics such as adversarial robustness, generalizability, and quantization robustness. For deep learning on the edge, quantization robustness is often crucial. Finding a model that is quantization-robust can sometimes require significant efforts. Recent works using Graph Hypernetworks (GHN) have shown remarkable performance predicting high-performant parameters of varying CNN architectures. Inspired by these successes, we wonder if the graph representations of GHN-2 can be leveraged to predict quantization-robust parameters as well, which we call GHN-Q. We conduct the first-ever study exploring the use of graph hypernetworks for predi
    
[^160]: 为什么网络具有抑制性/负向连接？

    Why do networks have inhibitory/negative connections?. (arXiv:2208.03211v8 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.03211](http://arxiv.org/abs/2208.03211)

    神经网络具有抑制性/负向连接是为了学习更多的功能，负权重在表征能力中起着至关重要的作用，并且非负深度网络无法表示某些表征空间的几何特性。

    

    大脑为什么具有抑制性连接？深度网络为什么具有负权重？我们从表征能力的角度提出了一个答案。我们认为，在自然智能中，大脑的主要作用是表征功能，在人工智能中，深度网络的主要作用也是如此。我们的答案是为什么存在抑制性/负向权重：为了学习更多的功能。我们证明了，在没有负权重的情况下，具有非递增激活函数的神经网络无法成为普适近似器。尽管这可能对一些人来说是一种直观的结果，但据我们所知，无论是在机器学习还是神经科学领域，都没有提供正式理论来证明为什么在表征能力的背景下，负权重至关重要。此外，我们还提供了非负深度网络无法表示的表征空间的几何特性的见解。我们希望这些见解能够带来对更复杂的归纳过程的更深入了解。

    Why do brains have inhibitory connections? Why do deep networks have negative weights? We propose an answer from the perspective of representation capacity. We believe representing functions is the primary role of both (i) the brain in natural intelligence, and (ii) deep networks in artificial intelligence. Our answer to why there are inhibitory/negative weights is: to learn more functions. We prove that, in the absence of negative weights, neural networks with non-decreasing activation functions are not universal approximators. While this may be an intuitive result to some, to the best of our knowledge, there is no formal theory, in either machine learning or neuroscience, that demonstrates why negative weights are crucial in the context of representation capacity. Further, we provide insights on the geometric properties of the representation space that non-negative deep networks cannot represent. We expect these insights will yield a deeper understanding of more sophisticated inducti
    
[^161]: 近似实对称张量秩

    Approximate Real Symmetric Tensor Rank. (arXiv:2207.12529v4 [math.NA] UPDATED)

    [http://arxiv.org/abs/2207.12529](http://arxiv.org/abs/2207.12529)

    我们研究了对称张量分解中的扰动容忍度对结果的影响，并提出了一种计算对称张量秩的算法。

    

    我们研究了对称张量分解中的扰动容忍度对结果的影响。具体而言，给定一个实对称d-张量f，一个张量空间上的范数||.||，以及一个正数ε>0。在f的ε-邻域内，最小的对称张量秩是多少？换句话说，经过一个巧妙的ε-扰动后，f的对称张量秩是多少？我们证明了两个定理并开发了三个对应的算法来给出这个问题的上界构造。考虑到我们的认识目标；我们介绍了我们结果背后的概率和凸几何思想，再现了一些已知结果，并指出了开放问题。

    We investigate the effect of an $\varepsilon$-room of perturbation tolerance on symmetric tensor decomposition. To be more precise, suppose a real symmetric $d$-tensor $f$, a norm $||.||$ on the space of symmetric $d$-tensors, and $\varepsilon >0$ are given. What is the smallest symmetric tensor rank in the $\varepsilon$-neighborhood of $f$? In other words, what is the symmetric tensor rank of $f$ after a clever $\varepsilon$-perturbation? We prove two theorems and develop three corresponding algorithms that give constructive upper bounds for this question. With expository goals in mind; we present probabilistic and convex geometric ideas behind our results, reproduce some known results, and point out open problems.
    
[^162]: 全面稳健的数据驱动决策

    Holistic Robust Data-Driven Decisions. (arXiv:2207.09560v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.09560](http://arxiv.org/abs/2207.09560)

    这篇论文提出了一种全面稳健的数据驱动公式，能够同时保护三个过拟合的源头：有限样本数据的统计误差、数据点的有限精度测量引起的数据噪声，以及被破坏的部分数据。

    

    设计具有良好样本外性能的机器学习和决策的数据驱动公式是一个关键的挑战。好的样本内性能不一定能保证好的样本外性能，这被普遍认为是过拟合问题。实际的过拟合通常不能归因于单一原因，而是由多个因素同时引起的。我们在这里考虑了三个过拟合的源头：（一）统计误差，由于使用有限的样本数据而产生的误差，（二）数据噪声，当数据点只用有限精度测量时产生的噪声，（三）数据错误，即全部数据中有一小部分数据被完全破坏。我们认为，尽管现有的数据驱动公式在单独处理这三个源头时可能是稳健的，但它们不能同时提供对所有过拟合源头的全面保护。我们设计了一种新颖的数据驱动公式，可以保证这种全面保护。

    The design of data-driven formulations for machine learning and decision-making with good out-of-sample performance is a key challenge. The observation that good in-sample performance does not guarantee good out-of-sample performance is generally known as overfitting. Practical overfitting can typically not be attributed to a single cause but instead is caused by several factors all at once. We consider here three overfitting sources: (i) statistical error as a result of working with finite sample data, (ii) data noise which occurs when the data points are measured only with finite precision, and finally (iii) data misspecification in which a small fraction of all data may be wholly corrupted. We argue that although existing data-driven formulations may be robust against one of these three sources in isolation they do not provide holistic protection against all overfitting sources simultaneously. We design a novel data-driven formulation which does guarantee such holistic protection an
    
[^163]: 马尔科夫高斯过程变分自编码器

    Markovian Gaussian Process Variational Autoencoders. (arXiv:2207.05543v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.05543](http://arxiv.org/abs/2207.05543)

    本文提出了一种马尔科夫高斯过程变分自编码器（MGPVAE）模型，通过利用马尔科夫高斯过程的等效离散状态空间表示，并使用卡尔曼滤波和平滑技术实现了线性时间的GPVAE训练。在各种高维时间和时空任务中，该方法表现优异。

    

    在许多高维时间序列建模问题中，序列VAE已经被广泛应用，其中许多变种模型依赖于离散时间机制，如循环神经网络（RNN）。另一方面，连续时间方法最近在非规则采样时间序列的背景下引起了人们的兴趣，在这种情况下，它们可以更好地处理数据。其中一种是高斯过程变分自编码器（GPVAEs），其中VAE先验被设置为高斯过程（GP）。然而，GPVAEs的一个主要限制是它继承了高斯过程的立方计算成本，使其对实际应用者不太吸引人。在这项工作中，我们利用马尔科夫高斯过程的等效离散状态空间表示，通过卡尔曼滤波和平滑来实现线性时间的GPVAE训练。对于我们的模型，马尔可夫高斯过程变分自编码器（MGPVAE），我们在各种高维时间和时空任务上展示了我们的方法的有利性能。

    Sequential VAEs have been successfully considered for many high-dimensional time series modelling problems, with many variant models relying on discrete-time mechanisms such as recurrent neural networks (RNNs). On the other hand, continuous-time methods have recently gained attraction, especially in the context of irregularly-sampled time series, where they can better handle the data than discrete-time methods. One such class are Gaussian process variational autoencoders (GPVAEs), where the VAE prior is set as a Gaussian process (GP). However, a major limitation of GPVAEs is that it inherits the cubic computational cost as GPs, making it unattractive to practioners. In this work, we leverage the equivalent discrete state space representation of Markovian GPs to enable linear time GPVAE training via Kalman filtering and smoothing. For our model, Markovian GPVAE (MGPVAE), we show on a variety of high-dimensional temporal and spatiotemporal tasks that our method performs favourably compar
    
[^164]: 使用图神经网络学习边界值问题的解算器

    Learning the Solution Operator of Boundary Value Problems using Graph Neural Networks. (arXiv:2206.14092v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.14092](http://arxiv.org/abs/2206.14092)

    本研究使用图神经网络和谱图卷积，设计了用于两种不同的时间独立PDE的通用解算器。通过在各种形状和不均匀性的情况下训练网络，我们发现在训练集中包含有限元网格的各种变化的数据是实现良好泛化结果的关键因素。这表明GNNs可以有效解决PDE问题并具有良好的泛化能力。

    

    作为传统的偏微分方程（PDE）边界值约束问题的数值求解器的替代方案，近年来研究人员对能够高效解决此类问题的神经网络兴趣日益增加。本研究使用图神经网络（GNNs）和谱图卷积设计了用于两种不同的时间独立PDE的通用解算器。我们使用来自有限元求解器的模拟数据，在各种形状和不均匀性的情况下对网络进行训练。与以往的工作不同，我们关注训练后的解算器在未见过的情况下的泛化能力。具体而言，我们测试了在具有不同形状的网格和不同个数的不均匀性叠加解的情况下的泛化效果。我们发现，在训练集中包含有限元网格的各种变化的数据是实现良好泛化结果的关键因素。因此，我们相信GNNs可用于解决PDE问题，并能够在各种情况下实现良好的泛化能力。

    As an alternative to classical numerical solvers for partial differential equations (PDEs) subject to boundary value constraints, there has been a surge of interest in investigating neural networks that can solve such problems efficiently. In this work, we design a general solution operator for two different time-independent PDEs using graph neural networks (GNNs) and spectral graph convolutions. We train the networks on simulated data from a finite elements solver on a variety of shapes and inhomogeneities. In contrast to previous works, we focus on the ability of the trained operator to generalize to previously unseen scenarios. Specifically, we test generalization to meshes with different shapes and superposition of solutions for a different number of inhomogeneities. We find that training on a diverse dataset with lots of variation in the finite element meshes is a key ingredient for achieving good generalization results in all cases. With this, we believe that GNNs can be used to 
    
[^165]: 主要权衡分析

    Principal Trade-off Analysis. (arXiv:2206.07520v3 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2206.07520](http://arxiv.org/abs/2206.07520)

    本文介绍了一种名为"主要权衡分析"（PTA）的方法，将游戏嵌入低维特征空间，并展示了其在不同游戏中的有效性和洞察力。

    

    在这篇论文中，我们介绍了"主要权衡分析"（PTA），一种将游戏嵌入低维特征空间的分解方法。我们通过发展与主成分分析（PCA）的类比来论证嵌入结果的可靠性。PTA将任意的两人零和游戏表示为一系列正交的二维特征平面的加权和。我们展示了这些特征平面代表了独特的战略权衡，而序列的截断提供了有洞察力的模型简化。我们在四个游戏（库恩扑克、RPS+2、布洛托和宝可梦）上验证了PTA的有效性。在库恩扑克中，PTA清晰地识别出了虚张声势和跟注之间的权衡。在布洛托中，PTA识别出了游戏的对称性，并指定了与不同胜利条件相关的战略权衡。这些对称性揭示了游戏的局限性。

    How are the advantage relations between a set of agents playing a game organized and how do they reflect the structure of the game? In this paper, we illustrate "Principal Trade-off Analysis" (PTA), a decomposition method that embeds games into a low-dimensional feature space. We argue that the embeddings are more revealing than previously demonstrated by developing an analogy to Principal Component Analysis (PCA). PTA represents an arbitrary two-player zero-sum game as the weighted sum of pairs of orthogonal 2D feature planes. We show that the feature planes represent unique strategic trade-offs and truncation of the sequence provides insightful model reduction. We demonstrate the validity of PTA on a quartet of games (Kuhn poker, RPS+2, Blotto, and Pokemon). In Kuhn poker, PTA clearly identifies the trade-off between bluffing and calling. In Blotto, PTA identifies game symmetries, and specifies strategic trade-offs associated with distinct win conditions. These symmetries reveal limi
    
[^166]: SGD的高维极限定理：有效动力学和临界尺度

    High-dimensional limit theorems for SGD: Effective dynamics and critical scaling. (arXiv:2206.04030v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.04030](http://arxiv.org/abs/2206.04030)

    本文研究了高维极限下具有恒定步长的随机梯度下降算法（SGD）的可扩展极限，我们证明了当维度趋于无穷时，SGD的轨迹的极限定理。我们的方法允许选择要跟踪的总结统计量、初始化和步长，并且得到了球形（ODE）和扩散（SDE）极限。我们还展示了步长的临界尺度，这个尺度下，有效的球形动力学与梯度流相匹配，但是出现了一个新的修正项，改变了相图。这个有效动力学的不动点对应的扩散极限可能非常复杂。

    

    我们研究了在高维极限下，具有恒定步长的随机梯度下降（SGD）的可扩展极限。我们证明了SGD的总结统计轨迹（即有限维函数）在维度趋于无穷大时的极限定理。我们的方法允许选择要跟踪的总结统计量、初始化和步长。它产生了一个在前述选择上极其依赖的球形（ODE）和扩散（SDE）极限。我们展示了步长的临界尺度，低于这个尺度，有效的球形动力学与人口损失的梯度流相匹配，但在这个尺度上，出现了一个新的修正项，改变了相图。关于这个有效动力学的不动点，相应的扩散极限可能非常复杂甚至退化。我们在一些流行的例子上展示了我们的方法，包括尖峰矩阵和张量模型的估计以及通过两层网络进行分类。

    We study the scaling limits of stochastic gradient descent (SGD) with constant step-size in the high-dimensional regime. We prove limit theorems for the trajectories of summary statistics (i.e., finite-dimensional functions) of SGD as the dimension goes to infinity. Our approach allows one to choose the summary statistics that are tracked, the initialization, and the step-size. It yields both ballistic (ODE) and diffusive (SDE) limits, with the limit depending dramatically on the former choices. We show a critical scaling regime for the step-size, below which the effective ballistic dynamics matches gradient flow for the population loss, but at which, a new correction term appears which changes the phase diagram. About the fixed points of this effective dynamics, the corresponding diffusive limits can be quite complex and even degenerate. We demonstrate our approach on popular examples including estimation for spiked matrix and tensor models and classification via two-layer networks fo
    
[^167]: 通过组合程序学习逻辑程序

    Learning logic programs by combining programs. (arXiv:2206.01614v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.01614](http://arxiv.org/abs/2206.01614)

    本论文介绍了一种通过组合小型非可分离程序的方法来学习逻辑程序。实验结果表明，这种方法在预测准确性和学习时间方面显著优于现有方法。

    

    归纳逻辑编程的目标是归纳出一个逻辑程序（一组逻辑规则），以概括训练样例。归纳具有多个规则和文字的程序是一个重大挑战。为了解决这个挑战，我们引入了一种方法，我们学习小型的不可分离的程序，并将它们组合起来。我们在一个基于约束的归纳逻辑编程系统中实现了我们的方法。我们的方法可以学习最优和递归程序，并进行谓词发明。我们在多个领域（包括游戏玩法和程序合成）的实验结果表明，我们的方法在预测准确性和学习时间方面相比现有方法有显著的优势，有时将学习时间从一个小时降低到几秒钟。

    The goal of inductive logic programming is to induce a logic program (a set of logical rules) that generalises training examples. Inducing programs with many rules and literals is a major challenge. To tackle this challenge, we introduce an approach where we learn small non-separable programs and combine them. We implement our approach in a constraint-driven ILP system. Our approach can learn optimal and recursive programs and perform predicate invention. Our experiments on multiple domains, including game playing and program synthesis, show that our approach can drastically outperform existing approaches in terms of predictive accuracies and learning times, sometimes reducing learning times from over an hour to a few seconds.
    
[^168]: RoCourseNet：预测感知回应模型的分布鲁棒训练

    RoCourseNet: Distributionally Robust Training of a Prediction Aware Recourse Model. (arXiv:2206.00700v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.00700](http://arxiv.org/abs/2206.00700)

    RoCourseNet提出了一个分布鲁棒的训练框架，用于同时优化预测和回应，以解决因为训练数据分布偏移造成的回应无效问题。

    

    机器学习模型的因果反事实（CF）解释是最终用户偏好的，因为它们通过为受到预测结果不利影响的个体提供补救（或对比）案例来解释机器学习模型的预测。现有的CF解释方法在假设目标机器学习模型在时间上保持稳定的情况下生成回应。然而，由于训练数据中普遍存在的分布偏移，实际中机器学习模型不断更新，这可能会使先前生成的回应无效，并降低最终用户对我们算法框架的信任。为了解决这个问题，我们提出了RoCourseNet，这是一个同时优化对未来数据偏移鲁棒的预测和回应的训练框架。这项工作包含四个关键贡献：（1）我们将鲁棒的回应生成问题定义为一个三层优化问题，其中包含两个子问题：（i）一个双层问题，找到最坏的

    Counterfactual (CF) explanations for machine learning (ML) models are preferred by end-users, as they explain the predictions of ML models by providing a recourse (or contrastive) case to individuals who are adversely impacted by predicted outcomes. Existing CF explanation methods generate recourses under the assumption that the underlying target ML model remains stationary over time. However, due to commonly occurring distributional shifts in training data, ML models constantly get updated in practice, which might render previously generated recourses invalid and diminish end-users trust in our algorithmic framework. To address this problem, we propose RoCourseNet, a training framework that jointly optimizes predictions and recourses that are robust to future data shifts. This work contains four key contributions: (1) We formulate the robust recourse generation problem as a tri-level optimization problem which consists of two sub-problems: (i) a bi-level problem that finds the worst-c
    
[^169]: 善意的举措：通过意图信号进行自适应参数管理

    Good Intentions: Adaptive Parameter Management via Intent Signaling. (arXiv:2206.00470v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.00470](http://arxiv.org/abs/2206.00470)

    本研究提出了一种善意的举措，通过意图信号机制实现自适应参数管理。该方法可以避免手动集成和昂贵的调试过程，提高分布式训练效率。

    

    参数管理对于大规模机器学习任务的分布式训练至关重要。由于常见的参数管理方法效率低下，一些机器学习任务难以分布式处理。先进的参数管理方法，例如选择性复制或动态参数分配，可以提高效率，但通常需要手动集成到每个任务的实现中，并且需要昂贵的前期实验来正确调整。在本文中，我们探讨了是否可以避免这两个问题。我们首先提出了一种新颖的意图信号机制，它自然地集成到现有的机器学习堆栈中，并为参数管理器提供关键的参数访问信息。然后，我们描述了AdaPM，一种基于这种机制的全自适应、零调试的参数管理器。与之前的系统不同，该方法将提供信息（任务简单完成）与有效利用信息（困难部分）分离开来。

    Parameter management is essential for distributed training of large machine learning (ML) tasks. Some ML tasks are hard to distribute because common approaches to parameter management can be highly inefficient. Advanced parameter management approaches -- such as selective replication or dynamic parameter allocation -- can improve efficiency, but to do so, they typically need to be integrated manually into each task's implementation and they require expensive upfront experimentation to tune correctly. In this work, we explore whether these two problems can be avoided. We first propose a novel intent signaling mechanism that integrates naturally into existing ML stacks and provides the parameter manager with crucial information about parameter accesses. We then describe AdaPM, a fully adaptive, zero-tuning parameter manager based on this mechanism. In contrast to prior systems, this approach separates providing information (simple, done by the task) from exploiting it effectively (hard, 
    
[^170]: 并行和分布式图神经网络：深入并发性分析

    Parallel and Distributed Graph Neural Networks: An In-Depth Concurrency Analysis. (arXiv:2205.09702v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.09702](http://arxiv.org/abs/2205.09702)

    这项研究深入分析了并行和分布式图神经网络，在设计了并行性分类方法后，研究了各种GNN模型、任务、软件框架和硬件加速器中的并行性，并重点关注相关张量的稀疏性/密度。

    

    图神经网络（GNNs）是深度学习中最强大的工具之一。它们常常在无结构网络上解决复杂问题，如节点分类、图分类或链接预测，具有高准确性。然而，GNNs的推理和训练都非常复杂，它们独特地将不规则图处理的特性与密集和规则计算相结合。这种复杂性使得在现代大规模并行架构上高效执行GNNs非常具有挑战性。为了缓解这一问题，我们首先设计了GNNs中的并行性分类方法，考虑了数据并行性和模型并行性，以及不同形式的流水线。然后，我们使用这个分类方法来研究众多GNN模型、GNN驱动的机器学习任务、软件框架或硬件加速器中的并行性。我们使用工作深度模型，并评估通信量和同步。我们特别关注相关张量的稀疏性/密度。

    Graph neural networks (GNNs) are among the most powerful tools in deep learning. They routinely solve complex problems on unstructured networks, such as node classification, graph classification, or link prediction, with high accuracy. However, both inference and training of GNNs are complex, and they uniquely combine the features of irregular graph processing with dense and regular computations. This complexity makes it very challenging to execute GNNs efficiently on modern massively parallel architectures. To alleviate this, we first design a taxonomy of parallelism in GNNs, considering data and model parallelism, and different forms of pipelining. Then, we use this taxonomy to investigate the amount of parallelism in numerous GNN models, GNN-driven machine learning tasks, software frameworks, or hardware accelerators. We use the work-depth model, and we also assess communication volume and synchronization. We specifically focus on the sparsity/density of the associated tensors, in o
    
[^171]: 因果效应识别的实验设计

    Experimental Design for Causal Effect Identification. (arXiv:2205.02232v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.02232](http://arxiv.org/abs/2205.02232)

    本文研究了设计最小成本的干预集合来识别因果效应的问题，证明了问题的NP-hard性质，并提出了可以找到最优解或近似解的算法，使用多项式时间启发式算法来解决计算复杂性，并在随机图上进行了模拟实验验证算法的效果。

    

    Pearl的做法是一种从观测数据中学习可识别的因果效应的完整公理方法。当这种效应不可识别时，需要执行一系列通常昂贵的干预来学习因果效应。本研究考虑设计最小成本的干预集合来识别所需效应的问题。首先，我们证明了该问题是NP-hard的，随后提出了一个算法，可以找到最优解或其对数因子的近似解。这是通过建立我们的问题与最小命中集问题之间的联系实现的。此外，我们提出了几种多项式时间启发式算法来解决问题的计算复杂性。尽管这些算法可能找到次优解，但我们的模拟结果表明它们在随机图上取得了较小的遗憾。

    Pearl's do calculus is a complete axiomatic approach to learn the identifiable causal effects from observational data. When such an effect is not identifiable, it is necessary to perform a collection of often costly interventions in the system to learn the causal effect. In this work, we consider the problem of designing the collection of interventions with the minimum cost to identify the desired effect. First, we prove that this problem is NP-hard, and subsequently propose an algorithm that can either find the optimal solution or a logarithmic-factor approximation of it. This is done by establishing a connection between our problem and the minimum hitting set problem. Additionally, we propose several polynomial-time heuristic algorithms to tackle the computational complexity of the problem. Although these algorithms could potentially stumble on sub-optimal solutions, our simulations show that they achieve small regrets on random graphs.
    
[^172]: MotionAug: 用于人体运动预测的物理纠正增强方法

    MotionAug: Augmentation with Physical Correction for Human Motion Prediction. (arXiv:2203.09116v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2203.09116](http://arxiv.org/abs/2203.09116)

    本文提出了一种利用物理纠正的运动数据增强方案，通过生成多样性的运动和纠正不真实伪影来提高人体运动预测的效果。

    

    本文提出了一种运动数据增强方案，其中包括鼓励多样性的运动合成和强制物理合理性的运动纠正。这种运动合成包括我们修改后的变分自编码器（VAE）和逆运动学（IK）。在这个VAE中，我们提出的附近样本采样方法可以在训练数据不足的情况下生成各种有效的运动。我们基于IK的运动合成方法可以半自动地生成各种运动。由于这两个方案在合成的运动中会产生不真实的伪影，我们的运动纠正方法会对其进行校正。这种运动纠正方案包括使用物理模拟进行模仿学习和后续的运动去偏倚。对于这个模仿学习，我们提出了显著加速训练过程的PD残余力。此外，我们的运动去偏倚成功地抵消了模仿学习引起的运动偏倚，以最大化增强效果。

    This paper presents a motion data augmentation scheme incorporating motion synthesis encouraging diversity and motion correction imposing physical plausibility. This motion synthesis consists of our modified Variational AutoEncoder (VAE) and Inverse Kinematics (IK). In this VAE, our proposed sampling-near-samples method generates various valid motions even with insufficient training motion data. Our IK-based motion synthesis method allows us to generate a variety of motions semi-automatically. Since these two schemes generate unrealistic artifacts in the synthesized motions, our motion correction rectifies them. This motion correction scheme consists of imitation learning with physics simulation and subsequent motion debiasing. For this imitation learning, we propose the PD-residual force that significantly accelerates the training process. Furthermore, our motion debiasing successfully offsets the motion bias induced by imitation learning to maximize the effect of augmentation. As a r
    
[^173]: 在学习的黎曼流形上进行反应式运动生成

    Reactive Motion Generation on Learned Riemannian Manifolds. (arXiv:2203.07761v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2203.07761](http://arxiv.org/abs/2203.07761)

    本文从黎曼流形的角度研究了机器人运动学习，通过学习黎曼度量和使用测地线生成的运动能适应新的环境条件，并通过调整学习到的流形实现避障功能。

    

    在最近几十年中，运动学习的进展使得机器人能够在结构化和非结构化环境中获得新的技能并适应未知条件。实践中，运动学习方法捕捉相关模式并调整它们以适应动态避障或可变目标等新条件。在本文中，我们从黎曼流形的角度研究机器人运动学习范式。我们认为通过人类示教可以学习到黎曼流形，其中测地线是自然的运动技能。通过我们的新型变分自编码器（VAE）生成使用学习到的黎曼度量产生的测地线，该自编码器特别用于恢复全位姿末端执行器状态和关节空间配置。此外，我们提出了一种通过重塑学习到的流形来促进末端执行器/多肢体避障的技术，该技术使用了一个能够感知障碍物的环境度量。使用这些测地线生成的运动可以...

    In recent decades, advancements in motion learning have enabled robots to acquire new skills and adapt to unseen conditions in both structured and unstructured environments. In practice, motion learning methods capture relevant patterns and adjust them to new conditions such as dynamic obstacle avoidance or variable targets. In this paper, we investigate the robot motion learning paradigm from a Riemannian manifold perspective. We argue that Riemannian manifolds may be learned via human demonstrations in which geodesics are natural motion skills. The geodesics are generated using a learned Riemannian metric produced by our novel variational autoencoder (VAE), which is especially intended to recover full-pose end-effector states and joint space configurations. In addition, we propose a technique for facilitating on-the-fly end-effector/multiple-limb obstacle avoidance by reshaping the learned manifold using an obstacle-aware ambient metric. The motion generated using these geodesics may
    
[^174]: 跨模型公平性：多模型情况下的公平性与伦理实证研究

    Cross-model Fairness: Empirical Study of Fairness and Ethics Under Model Multiplicity. (arXiv:2203.07139v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.07139](http://arxiv.org/abs/2203.07139)

    本文提出了一种跨模型公平性的新定义，并进行了实证研究。该研究关注数据驱动预测模型在社会背景下的公平性问题，特别是在通过选择不同预测器进行模型多样性时可能导致个人受伤害的情况。

    

    虽然基于数据驱动的预测模型是一个严格的技术构造，但它们可能在社会背景下运作，在这个背景下，善意的工程选择可能带来隐含的、间接的和意想不到的现实后果。在这个领域中，这些系统的公平性，涉及到个人和群体，是一个相关的考虑因素；它在数据捕捉可导致人们受到歧视的受保护特征时出现。迄今为止，这个概念主要针对固定模型进行研究，通常在不同的分类阈值下进行研究，力图识别和消除其运作中不希望的、具有歧视性和可能违法的方面。在本文中，我们回溯了这个固定模型的假设，提出并探索了一种新的跨模型公平性定义，即在从一组表现同样出色的模型中特定选择预测器的情况下，个人可能受到伤害，即在基于效用的模型多样性的视图下。由于一个人在不同的模型下可能被分类不同。

    While data-driven predictive models are a strictly technological construct, they may operate within a social context in which benign engineering choices entail implicit, indirect and unexpected real-life consequences. Fairness of such systems -- pertaining both to individuals and groups -- is one relevant consideration in this space; it arises when data capture protected characteristics upon which people may be discriminated. To date, this notion has predominantly been studied for a fixed model, often under different classification thresholds, striving to identify and eradicate undesirable, discriminative and possibly unlawful aspects of its operation. Here, we backtrack on this fixed model assumption to propose and explore a novel definition of cross-model fairness where individuals can be harmed when one predictor is chosen ad hoc from a group of equally-well performing models, i.e., in view of utility-based model multiplicity. Since a person may be classified differently across mode
    
[^175]: 在不完全信息广义展开形式博弈中的广义赌博遗憾最小化框架

    Generalized Bandit Regret Minimizer Framework in Imperfect Information Extensive-Form Game. (arXiv:2203.05920v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.05920](http://arxiv.org/abs/2203.05920)

    本论文提出了一个广义框架，用于在不完全信息广义展开形式博弈中学习近似纳什平衡解。通过该框架，我们设计了一种新方法SIX-OMD来最小化遗憾并改善收敛率。

    

    遗憾最小化方法是学习近似纳什平衡解的强大工具，用于两人零和不完全信息广义展开形式博弈。我们考虑交互赌博反馈设置下的问题，其中我们不知道不完全信息广义展开形式博弈的动态。一般来说，只有交互轨迹和到达终止节点的价值$v(z^t)$被揭示。为了学习纳什平衡解，遗憾最小化器需要通过$v(z^t)$估计完全反馈损失梯度$\ell^t$并最小化遗憾。本文提出了一个针对这种学习设置的广义框架。它为赌博遗憾最小化方法的设计和模块化分析提供了一个理论框架。我们证明了最新的赌博遗憾最小化方法可以作为我们框架的一个特例进行分析。在按照该框架进行操作的基础上，我们描述了一种名为SIX-OMD的新方法来学习近似纳什平衡解。它是无模型的，并且极大地改善了最好的现有收敛率。

    Regret minimization methods are a powerful tool for learning approximate Nash equilibrium (NE) in two-player zero-sum imperfect information extensive-form games (IIEGs). We consider the problem in the interactive bandit-feedback setting where we don't know the dynamics of the IIEG. In general, only the interactive trajectory and the reached terminal node value $v(z^t)$ are revealed. To learn NE, the regret minimizer is required to estimate the full-feedback loss gradient $\ell^t$ by $v(z^t)$ and minimize the regret. In this paper, we propose a generalized framework for this learning setting. It presents a theoretical framework for the design and the modular analysis of the bandit regret minimization methods. We demonstrate that the most recent bandit regret minimization methods can be analyzed as a particular case of our framework. Following this framework, we describe a novel method SIX-OMD to learn approximate NE. It is model-free and extremely improves the best existing convergence 
    
[^176]: 面向领域差异的航天器姿态估计的鲁棒性多任务学习和在线优化

    Robust Multi-Task Learning and Online Refinement for Spacecraft Pose Estimation across Domain Gap. (arXiv:2203.04275v5 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2203.04275](http://arxiv.org/abs/2203.04275)

    本文介绍了一种跨领域差异的非合作航天器姿态估计的多尺度、多任务CNN，通过在合成图像上进行数据增强训练共享编码器以学习通用特征。同时介绍了一种在线域优化方法，用于调整模型的标准化层参数。

    

    本文介绍了一种名为Spacecraft Pose Network v2（SPNv2）的卷积神经网络（CNN），用于跨领域差异的非合作航天器姿态估计。SPNv2是一个多尺度、多任务的CNN，由共享的多尺度特征编码器和多个预测头组成，这些预测头在共享特征输出上执行不同的任务。这些任务都与从图像中检测和估计目标航天器的姿态有关，例如预测预定义的卫星关键点、直接姿态回归和卫星前景的二元分割等。通过在合成图像上进行广泛的数据增强来共同训练不同但相关的任务，证明了共享编码器学习到的特征对具有基本不同视觉特性的图像域是通用的。本文还介绍了一种名为Online Domain Refinement（ODR）的方法，该方法在目标域上调整SPNv2的标准化层参数。

    This work presents Spacecraft Pose Network v2 (SPNv2), a Convolutional Neural Network (CNN) for pose estimation of noncooperative spacecraft across domain gap. SPNv2 is a multi-scale, multi-task CNN which consists of a shared multi-scale feature encoder and multiple prediction heads that perform different tasks on a shared feature output. These tasks are all related to detection and pose estimation of a target spacecraft from an image, such as prediction of pre-defined satellite keypoints, direct pose regression, and binary segmentation of the satellite foreground. It is shown that by jointly training on different yet related tasks with extensive data augmentations on synthetic images only, the shared encoder learns features that are common across image domains that have fundamentally different visual characteristics compared to synthetic images. This work also introduces Online Domain Refinement (ODR) which refines the parameters of the normalization layers of SPNv2 on the target doma
    
[^177]: 保持度数不变的随机化响应在局部差分隐私下的图神经网络

    Degree-Preserving Randomized Response for Graph Neural Networks under Local Differential Privacy. (arXiv:2202.10209v4 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2202.10209](http://arxiv.org/abs/2202.10209)

    本文提出了一种保持度数不变的随机化响应算法，用于在无属性图中提供高准确性的边的局部差分隐私保护。通过使用Warner的随机化响应和策略性边采样，我们的算法能够保护用户的隐私同时保留图结构。

    

    最近，研究了差分隐私图神经网络(Differentially private GNNs)来在图数据上提供高准确性同时强力保护用户隐私。特别地，最近的一项研究提出了一种算法，用局部差分隐私(LDP)保护有属性的图中每个用户的特征向量，而局部差分隐私是一种强隐私概念，不需要可信第三方。然而，该算法不保护社交图中的边（友谊），因此无法保护无属性图中的用户隐私。如何在无属性图中提供高准确性的强隐私保护仍然是一个开放问题。本文提出了一种新颖的LDP算法，名为DPRR（Degree-Preserving Randomized Response），用于在GNN中提供边的LDP。我们的DPRR在提供边的LDP的同时保留了每个用户的度数，从而保持了图结构。从技术上讲，我们的DPRR使用了Warner的RR（Randomized Response）和策略性边采样，其中每个用户的采样概率是通过Lapla进行自动调整的。

    Differentially private GNNs (Graph Neural Networks) have been recently studied to provide high accuracy in various tasks on graph data while strongly protecting user privacy. In particular, a recent study proposes an algorithm to protect each user's feature vector in an attributed graph with LDP (Local Differential Privacy), a strong privacy notion without a trusted third party. However, this algorithm does not protect edges (friendships) in a social graph, hence cannot protect user privacy in unattributed graphs. How to provide strong privacy with high accuracy in unattributed graphs remains open.  In this paper, we propose a novel LDP algorithm called the DPRR (Degree-Preserving Randomized Response) to provide LDP for edges in GNNs. Our DPRR preserves each user's degree hence a graph structure while providing edge LDP. Technically, our DPRR uses Warner's RR (Randomized Response) and strategic edge sampling, where each user's sampling probability is automatically tuned using the Lapla
    
[^178]: 差分隐私下基于随机块模型的社区检测

    Differentially Private Community Detection for Stochastic Block Models. (arXiv:2202.00636v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2202.00636](http://arxiv.org/abs/2202.00636)

    本文研究了在差分隐私的情况下，基于随机块模型进行社区检测的问题，通过$(\epsilon, \delta)$-边差分隐私的概念，探讨了$(p, q)$、DP预算$(\epsilon, \delta)$和社区标签的准确恢复之间的基本权衡。

    

    社区检测的目标是在给定图中用户之间的连接性（由图的邻接矩阵表示）的基础上恢复用户的标签/属性（例如，政治倾向）。最近在理解从随机块模型（SBM）生成的图的社区检测的基本限制方面取得了显著进展。具体而言，已经获得了针对$p$和$q$的SBM的尖锐信息论限制和高效算法，它们分别表示社区内部和社区间的连接概率。本文研究在保护顶点之间的个体连接（边）的隐私的情况下的社区检测问题。我们关注$(\epsilon, \delta)$-边差分隐私（DP）的概念，旨在理解$(p, q)$、DP预算$(\epsilon, \delta)$和具体恢复社区标签的计算效率之间的基本权衡。

    The goal of community detection over graphs is to recover underlying labels/attributes of users (e.g., political affiliation) given the connectivity between users (represented by adjacency matrix of a graph). There has been significant recent progress on understanding the fundamental limits of community detection when the graph is generated from a stochastic block model (SBM). Specifically, sharp information theoretic limits and efficient algorithms have been obtained for SBMs as a function of $p$ and $q$, which represent the intra-community and inter-community connection probabilities. In this paper, we study the community detection problem while preserving the privacy of the individual connections (edges) between the vertices. Focusing on the notion of $(\epsilon, \delta)$-edge differential privacy (DP), we seek to understand the fundamental tradeoffs between $(p, q)$, DP budget $(\epsilon, \delta)$, and computational efficiency for exact recovery of the community labels.  To this en
    
[^179]: 关于使用双感知相似性进行渐进网络对齐的能力研究

    On the Power of Gradual Network Alignment Using Dual-Perception Similarities. (arXiv:2201.10945v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2201.10945](http://arxiv.org/abs/2201.10945)

    本研究提出了Grad-Align，一种渐进网络对齐方法，通过利用强一致性节点对逐步发现节点对。该方法首先生成节点嵌入，然后计算双感知相似性度量逐步对齐节点。

    

    网络对齐（NA）是基于网络结构和节点属性查找两个网络之间节点对应关系的任务。我们的研究动机在于，由于大多数现有的NA方法都试图一次性发现所有节点对，因此它们没有利用通过节点对应关系的中间发现来更准确地找到节点匹配过程中的下一个对应关系的信息。为了解决这个挑战，我们提出了Grad-Align，一种新的渐进网络对齐方法，通过充分利用在渐进匹配的早期阶段容易发现的节点对来逐步发现节点对。具体而言，Grad-Align首先基于图神经网络和我们的逐层重构损失生成两个网络的节点嵌入。然后，通过计算双感知相似性度量逐步对齐节点。

    Network alignment (NA) is the task of finding the correspondence of nodes between two networks based on the network structure and node attributes. Our study is motivated by the fact that, since most of existing NA methods have attempted to discover all node pairs at once, they do not harness information enriched through interim discovery of node correspondences to more accurately find the next correspondences during the node matching. To tackle this challenge, we propose Grad-Align, a new NA method that gradually discovers node pairs by making full use of node pairs exhibiting strong consistency, which are easy to be discovered in the early stage of gradual matching. Specifically, Grad-Align first generates node embeddings of the two networks based on graph neural networks along with our layer-wise reconstruction loss, a loss built upon capturing the first-order and higher-order neighborhood structures. Then, nodes are gradually aligned by computing dual-perception similarity measures 
    
[^180]: 最小化最大化风险分类器与0-1损失

    Minimax risk classifiers with 0-1 loss. (arXiv:2201.06487v6 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2201.06487](http://arxiv.org/abs/2201.06487)

    本论文介绍了最小化最大化风险分类器（MRCs），旨在通过最小化与可能包含基础分布的分布的不确定性集合相对应的最坏情况下的0-1损失来提供严格的性能保证。使用特征映射和特征核，MRCs在学习时具有强度普遍一致性，并且提供了高效的优化技术和准确的分类能力。

    

    监督分类技术使用训练样本来学习一种具有小期望0-1损失（错误概率）的分类规则。传统方法通过使用代理损失而不是0-1损失，并考虑特定的规则族（假设类）来实现可计算的学习和样本外泛化。本文提出了最小化最大化风险分类器（MRCs），它们最小化与可以包含基础分布的分布的不确定性集合相对应的最坏情况下的0-1损失，具有可调的置信度。我们证明了MRCs可以在学习时提供严格的性能保证，并且使用由特征映射给出的特征核是强度普遍一致的。本文还提出了MRC学习的高效优化技术，并展示了所提出的方法在实践中可以提供准确的分类和严格的性能保证。

    Supervised classification techniques use training samples to learn a classification rule with small expected 0-1 loss (error probability). Conventional methods enable tractable learning and provide out-of-sample generalization by using surrogate losses instead of the 0-1 loss and considering specific families of rules (hypothesis classes). This paper presents minimax risk classifiers (MRCs) that minize the worst-case 0-1 loss with respect to uncertainty sets of distributions that can include the underlying distribution, with a tunable confidence. We show that MRCs can provide tight performance guarantees at learning and are strongly universally consistent using feature mappings given by characteristic kernels. The paper also proposes efficient optimization techniques for MRC learning and shows that the methods presented can provide accurate classification together with tight performance guarantees in practice.
    
[^181]: MNL上下文Bandit问题的简便在线学习算法

    A Tractable Online Learning Algorithm for the Multinomial Logit Contextual Bandit. (arXiv:2011.14033v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2011.14033](http://arxiv.org/abs/2011.14033)

    本文提供了一个新颖的、不需要调整指数参数的MNL-Contextual Bandit问题的简便在线学习算法。算法具有与该问题的最佳理论界限匹配的遗憾上界。

    

    本文考虑了MNL-Bandit问题的上下文变体。更具体地说，我们考虑了一个动态集合优化问题，其中决策者向消费者提供一组产品（购物清单），并在每个回合观察响应。消费者购买产品以最大化他们的效用。我们假设一组属性描述了产品，产品的平均效用与这些属性的值呈线性关系。我们使用广泛使用的Multinomial Logit（MNL）模型建模消费者选择行为，并考虑在优化销售周期$T$内累积收益的同时动态学习模型参数的决策者问题。尽管这个问题近来引起了相当大的关注，但许多现有方法通常涉及解决一个难以处理的非凸优化问题。他们的理论性能保证取决于一个可能非常大的问题相关参数。特别地，现有方法需要调整随着属性集规模指数增长的调整参数。本文提供了一种新颖的MNL-Contextual Bandit问题的简便在线学习算法，它不需要调整此类指数参数。我们展示我们的算法具有与该问题的最佳理论界限匹配的遗憾上界。我们还通过模拟和真实世界实验证明了我们算法的有效性。

    In this paper, we consider the contextual variant of the MNL-Bandit problem. More specifically, we consider a dynamic set optimization problem, where a decision-maker offers a subset (assortment) of products to a consumer and observes the response in every round. Consumers purchase products to maximize their utility. We assume that a set of attributes describe the products, and the mean utility of a product is linear in the values of these attributes. We model consumer choice behavior using the widely used Multinomial Logit (MNL) model and consider the decision maker problem of dynamically learning the model parameters while optimizing cumulative revenue over the selling horizon $T$. Though this problem has attracted considerable attention in recent times, many existing methods often involve solving an intractable non-convex optimization problem. Their theoretical performance guarantees depend on a problem-dependent parameter which could be prohibitively large. In particular, existing 
    
[^182]: 通过高斯过程混合实现主动学习中的局部函数复杂性

    Local Function Complexity for Active Learning via Mixture of Gaussian Processes. (arXiv:1902.10664v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1902.10664](http://arxiv.org/abs/1902.10664)

    本文通过利用局部函数复杂性（LFC）的估计，建立了一个局部结构复杂性的概念，并将其用于发展一个与模型无关的主动学习框架。通过使用基于高斯过程回归（GPR）的局部多项式平滑（LPS）模型的类比，使得该框架具有鲁棒性和可伸缩性。

    

    真实世界的数据的不均匀性，例如观测噪声水平的变化或源函数结构复杂性的变化，给统计推断带来了一系列独特的挑战。考虑到这些因素可以在物理资源或计算时间有限的情况下显著提高预测能力。本文借鉴了最近关于局部多项式平滑（LPS）领域中局部函数复杂性（LFC）的估计的理论结果，建立了一个局部结构复杂性的概念，并用它来开发一个与模型无关的主动学习（AL）框架。由于其依赖于点估计，LPS模型类在处理通常伴随真实世界问题的大输入空间维度时不具有鲁棒性和可伸缩性。在本文中，我们推导和估计基于高斯过程回归（GPR）的LPS-based LFC的类比，并将其作为以上框架的替代，使之具有鲁棒性和可伸缩性。

    Inhomogeneities in real-world data, e.g., due to changes in the observation noise level or variations in the structural complexity of the source function, pose a unique set of challenges for statistical inference. Accounting for them can greatly improve predictive power when physical resources or computation time is limited. In this paper, we draw on recent theoretical results on the estimation of local function complexity (LFC), derived from the domain of local polynomial smoothing (LPS), to establish a notion of local structural complexity, which is used to develop a model-agnostic active learning (AL) framework. Due to its reliance on pointwise estimates, the LPS model class is not robust and scalable concerning large input space dimensions that typically come along with real-world problems. Here, we derive and estimate the Gaussian process regression (GPR)-based analog of the LPS-based LFC and use it as a substitute in the above framework to make it robust and scalable. We assess t
    

