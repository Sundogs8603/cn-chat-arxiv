# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex Selectivity.](http://arxiv.org/abs/2310.04420) | "BrainSCUBA通过生成自然语言描述来预测最大激活个体感兴趣体素的图像，达到了细粒度的视觉皮层选择性描述。" |
| [^2] | [Functional Interpolation for Relative Positions Improves Long Context Transformers.](http://arxiv.org/abs/2310.04418) | 这项研究提出了一种名为FIRE的函数相对位置编码与渐进插值方法，通过改进Transformer对更长上下文的泛化能力，并在零射击语言建模和长文本基准测试中进行了实证验证。 |
| [^3] | [Diffusion Random Feature Model.](http://arxiv.org/abs/2310.04417) | 本研究提出了一种以扩散模型为灵感的深度随机特征模型，它具有可解释性并可在数量相同的可训练参数下与全连接神经网络提供可比较的数值结果。通过推导得分匹配的属性，我们扩展了现有随机特征结果，并得出了样本数据分布与真实分布之间的泛化边界。 |
| [^4] | [Why Do We Need Weight Decay in Modern Deep Learning?.](http://arxiv.org/abs/2310.04415) | 权重衰减在现代深度学习中的作用与传统的学习理论中的正则化效果不同。对于过参数化的深度网络，它通过损失稳定机制改进了优化动态，对于欠参数化的大型语言模型，在随机优化中平衡偏差-方差权衡，导致较低的训练损失。此外，它还可以防止混合精度训练中的损失发散。 |
| [^5] | [Beyond Uniform Sampling: Offline Reinforcement Learning with Imbalanced Datasets.](http://arxiv.org/abs/2310.04413) | 该论文提出了一种解决离线强化学习中不平衡数据集问题的方法，通过采样策略使策略只受``好数据"限制，而不是所有的动作，提高了离线RL算法的性能。 |
| [^6] | [Understanding, Predicting and Better Resolving Q-Value Divergence in Offline-RL.](http://arxiv.org/abs/2310.04411) | 这项研究通过理解Q值估计分歧的机制，提出了一种基于自激励特征值测量的解决方案，能够可靠地判断训练过程是否会出现分歧。 |
| [^7] | [Policy-Gradient Training of Language Models for Ranking.](http://arxiv.org/abs/2310.04407) | 该论文提出了一种用于排序的语言模型的策略梯度训练算法Neural PG-RANK，通过将大规模语言模型实例化为Plackett-Luce排名策略，实现了对检索模型的原则性、端到端训练。 |
| [^8] | [Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models.](http://arxiv.org/abs/2310.04406) | 语言代理树搜索（LATS）是一个通用框架，利用大型语言模型（LLMs）的能力在规划、行动和推理方面相互协同，通过使用具有外部反馈的环境，实现更加深思熟虑和适应性的问题解决机制。实验评估表明，LATS在多个领域具有广泛的应用性，特别在编程方面表现出了94.4%的准确率。 |
| [^9] | [On the Embedding Collapse when Scaling up Recommendation Models.](http://arxiv.org/abs/2310.04400) | 研究了可缩放推荐模型中嵌入层的崩溃现象，发现特征交互模块在一定程度上限制了嵌入学习，但也是提高可扩展性的关键因素。 |
| [^10] | [Leveraging Self-Consistency for Data-Efficient Amortized Bayesian Inference.](http://arxiv.org/abs/2310.04395) | 该论文提出了一种利用自一致性改进数据有效的摊余贝叶斯推理方法，通过反转贝叶斯定理并利用近似表示的联合模型估计边际似然，加速条件神经密度估计器的学习动力学。 |
| [^11] | [Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference.](http://arxiv.org/abs/2310.04378) | 该论文提出了潜在一致性模型（LCMs），通过直接预测潜在空间中扩散过程的解，实现了高分辨率图像的快速合成，减少了计算量和时间成本。 |
| [^12] | [Confronting Reward Model Overoptimization with Constrained RLHF.](http://arxiv.org/abs/2310.04373) | 本研究首次研究了组合奖励模型中的过度优化问题，发现组成奖励模型之间的相关性对问题的解决方式有重要影响。我们提出了一种使用约束强化学习来解决这个问题的方法。 |
| [^13] | [MBTFNet: Multi-Band Temporal-Frequency Neural Network For Singing Voice Enhancement.](http://arxiv.org/abs/2310.04369) | 提出了一种新颖的多频带时间频率神经网络（MBTFNet）用于歌声增强，可以从歌唱录音中去除背景音乐、噪声甚至和声。实验证明，该模型优于多个最先进的音频增强和音乐源分离模型。 |
| [^14] | [A Marketplace Price Anomaly Detection System at Scale.](http://arxiv.org/abs/2310.04367) | MoatPlus是一个可扩展的价格异常检测框架，通过利用无监督统计特征和历史价格趋势生成上限价格边界，以解决在线市场中的数据质量和错误价格发布的问题。 |
| [^15] | [Amortizing intractable inference in large language models.](http://arxiv.org/abs/2310.04363) | 本论文提出了一种使用摊销的贝叶斯推理从难以处理的后验分布中进行抽样的方法，并利用生成流网络来实现大规模语言模型的微调，从而解决了在这些模型中处理推理问题的限制。 |
| [^16] | [Exploiting Transformer Activation Sparsity with Dynamic Inference.](http://arxiv.org/abs/2310.04361) | 本文提出了一种名为DSTI的方法，通过强制激活稀疏性并将Transformer模型转换为稀疏的专家混合版本来极大地降低推理成本。此方法可以应用于任何Transformer模型，并对准确性影响微乎其微。 |
| [^17] | [Integrating Transformations in Probabilistic Circuits.](http://arxiv.org/abs/2310.04354) | 本研究通过引入变换作为解决概率电路的预测限制问题的方法，在机器人场景中展示了该限制问题，并证明了所提出的方法能够在使用较少的参数的情况下实现更高的似然概率。此外，还讨论了如何将变换整合到基于树的学习过程中，并指出了精确推理的不可行性。 |
| [^18] | [A Language-Agent Approach to Formal Theorem-Proving.](http://arxiv.org/abs/2310.04353) | COPRA是一种面向形式定理证明的语言代理方法，利用大型语言模型进行上下文学习，通过选择策略和检索定义和引理进行证明，在MiniF2F基准和Coq任务上表现出优异的性能。 |
| [^19] | [Fair Feature Importance Scores for Interpreting Tree-Based Methods and Surrogates.](http://arxiv.org/abs/2310.04352) | 本文关注解释机器学习模型中公平性的方面，发展了一种基于决策树的特征重要性评分方法。 |
| [^20] | [Learning to Grasp: from Somewhere to Anywhere.](http://arxiv.org/abs/2310.04349) | 本研究介绍了一个采用质量-多样性方法的流水线，用于将生成的抓取轨迹适应到新物体姿态。使用RGB-D数据流，该方法能够自动检测目标物体、预测其姿态，并生成可达到的抓取轨迹。 |
| [^21] | [Neur2RO: Neural Two-Stage Robust Optimization.](http://arxiv.org/abs/2310.04345) | Neur2RO是一种神经网络驱动的二阶段鲁棒优化算法，通过学习估计第二阶段问题的值函数，并嵌入到经典的列-约束生成算法中，能够高效地求解嵌套的最小-最大-最小优化问题。 |
| [^22] | [Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design.](http://arxiv.org/abs/2310.04343) | 本研究提出了NAEPro模型，通过函数几何引导的方法共同设计蛋白质序列和结构。实验结果表明，该模型在氨基酸恢复率、TM分数和RMSD等指标上表现出色。 |
| [^23] | [Applying Reinforcement Learning to Option Pricing and Hedging.](http://arxiv.org/abs/2310.04336) | 本论文研究了将强化学习应用于期权定价和对冲的方法，并介绍了Q学习Black Scholes方法。该方法能够以非模型和数据驱动的方式进行期权定价和对冲，并在不同状态变量和场景下表现出稳健的性能。 |
| [^24] | [Saliency-Guided Hidden Associative Replay for Continual Learning.](http://arxiv.org/abs/2310.04334) | 该论文提出了一种基于显著性引导的隐含关联回放方法，用于解决持续学习中的灾难性遗忘问题。这种方法通过存储和检索重要的数据部分，模仿了人类记忆过程，与传统的完整数据存储方法不同。 |
| [^25] | [Robust Losses for Decision-Focused Learning.](http://arxiv.org/abs/2310.04328) | 这篇论文提出了一种改进的损失函数来处理决策导向学习中的不确定性问题，并对经验遗憾作为替代的准确性进行了分析。 |
| [^26] | [Program Synthesis with Best-First Bottom-Up Search.](http://arxiv.org/abs/2310.04327) | 本文介绍了一种新颖的最佳优先自底向上搜索算法，称为Bee Search，该算法能够以最佳优先的方式执行成本导向自底向上合成，而且不会丢失模型提供的有用信息。同时，作者还引入了一个更好使用现有成本模型提供的信息的新成本函数。 |
| [^27] | [Adjustable Robust Reinforcement Learning for Online 3D Bin Packing.](http://arxiv.org/abs/2310.04323) | 我们提出了一种可调节的鲁棒性强化学习（AR2L）框架来解决在线3D装箱问题，该框架允许有效地调节鲁棒性权重以实现所需的性能平衡。 |
| [^28] | [Latent Graph Inference with Limited Supervision.](http://arxiv.org/abs/2310.04314) | 本文研究了有限监督下的潜在图推理问题并提出了一种恢复关键连接和补充缺失监督的方法。 |
| [^29] | [Distributed Deep Joint Source-Channel Coding with Decoder-Only Side Information.](http://arxiv.org/abs/2310.04311) | 本文提出了一种带有仅解码器侧信息的分布式深度联合源信道编码方法，在低延迟图像传输中实现了改进的性能，尤其在低信道信噪比和小带宽比的情况下。 |
| [^30] | [Convergent ADMM Plug and Play PET Image Reconstruction.](http://arxiv.org/abs/2310.04299) | 本研究通过将模型驱动的变分重建与独立学习的深度神经网络操作符结合，提出了一种基于ADMM插入法的混合PET重建算法，并在实验中验证了其收敛性。 |
| [^31] | [Identifying Representations for Intervention Extrapolation.](http://arxiv.org/abs/2310.04295) | 本文研究了干预外推的任务，证明了可识别的表示方法能够有效地解决这个任务，即使干预对结果产生非线性影响。 |
| [^32] | [Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets.](http://arxiv.org/abs/2310.04292) | 本研究提出了七个新颖的数据集，分别是ToyMix、LargeMix和UltraLarge，这些数据集在规模和有监督标签的多样性方面突破了界限，涵盖了近1亿个分子和3000多个稀疏定义的任务，总计超过130亿个关于量子和生物性质的标签。 |
| [^33] | [Assessing Robustness via Score-Based Adversarial Image Generation.](http://arxiv.org/abs/2310.04285) | 本论文介绍了一种基于分数的对抗生成框架（ScoreAG），可以生成超过$\ell_p$-范数约束的对抗性示例，并通过图像转换或新图像合成的方法保持图像的核心语义，大大增强了分类器的鲁棒性。 |
| [^34] | [On the Error-Propagation of Inexact Deflation for Principal Component Analysis.](http://arxiv.org/abs/2310.04283) | 该论文研究了主成分分析中不精确消除法的误差传播问题，给出了两个主要结果 |
| [^35] | [C(NN)FD -- deep learning predictions of tip clearance variations on multi-stage axial compressors aerodynamic performance.](http://arxiv.org/abs/2310.04264) | 本文展示了一种用于实时预测多级轴向压缩机在燃气轮机中尖间隙变化对气动性能影响的深度学习框架，可与CFD基准相媲美的实时准确性，方便集成到燃气轮机的制造和构建过程中进行性能评估。 |
| [^36] | [Comparing Auxiliary Tasks for Learning Representations for Reinforcement Learning.](http://arxiv.org/abs/2310.04241) | 本文比较了在强化学习中用于学习表示的不同辅助任务，通过在连续控制基准环境上训练数百个智能体的实验，发现使用辅助任务的表示学习对环境的样本效率和回报有益。 |
| [^37] | [Bringing Quantum Algorithms to Automated Machine Learning: A Systematic Review of AutoML Frameworks Regarding Extensibility for QML Algorithms.](http://arxiv.org/abs/2310.04238) | 本文通过系统性评述现有的AutoML框架，分析其对于量子机器学习算法的可扩展性，并通过基准测试解决不同机器学习问题类型的产业用例。最终选择了Ray和AutoGluon作为合适的低级和高级框架。 |
| [^38] | [A Fixed-Parameter Tractable Algorithm for Counting Markov Equivalence Classes with the same Skeleton.](http://arxiv.org/abs/2310.04218) | 本文提出了一个固定参数可处理算法，用于计数具有相同骨架的马尔可夫等价类。 |
| [^39] | [Cost-Effective Retraining of Machine Learning Models.](http://arxiv.org/abs/2310.04216) | 提出了一种成本效益的机器学习模型重新训练方法，通过考虑成本和数据、模型及预测查询等因素来自动化决策何时重新训练。主要贡献是成本感知重新训练算法Cara，通过对数据流和查询进行优化来实现最佳权衡。 |
| [^40] | [Non-Redundant Graph Neural Networks with Improved Expressiveness.](http://arxiv.org/abs/2310.04190) | 本文提出了一种非冗余图神经网络的改进表达能力方法，通过基于邻域树的聚合方案减少冗余，提高了表达能力，实验证明了它对于减轻过度压缩的有效性。 |
| [^41] | [Entropic Score metric: Decoupling Topology and Size in Training-free NAS.](http://arxiv.org/abs/2310.04179) | 本文提出了一种名为Entropy Score的新型无需训练的度量方法，用于估计模型的表达能力，并通过与LogSynflow的组合搜索模型的大小，从而在较短时间内设计出用于边缘应用的高性能网络。 |
| [^42] | [Introducing the Attribution Stability Indicator: a Measure for Time Series XAI Attributions.](http://arxiv.org/abs/2310.04178) | 本研究引入了归因稳定性指标(ASI)，用于度量时间序列XAI归因的稳健性和可信度，并通过分析降维空间中的归因和ASI分数分布来证明该指标的有效性。 |
| [^43] | [Dynamic Relation-Attentive Graph Neural Networks for Fraud Detection.](http://arxiv.org/abs/2310.04171) | 本研究针对欺诈检测问题，通过动态关系注意聚合机制，提出了一种基于图神经网络（GNN）的方法。该方法学习每个关系的节点表示，并利用可学习的注意函数进行节点表示的聚合。通过结合不同层次的节点表示，考虑目标节点的局部和全局结构，以提高欺诈检测性能。通过使用动态图注意力，可以自适应地计算关系之间的重要程度。 |
| [^44] | [Amortized Network Intervention to Steer the Excitatory Point Processes.](http://arxiv.org/abs/2310.04159) | 本论文提出了一种分摊网络干预方法，可以引导兴奋性点过程的演化。这种方法利用神经ODE来捕捉网络化兴奋性点过程的变化，并通过梯度下降模型预测控制实现灵活的策略。通过设计的分摊网络干预框架，可以从历史和其他环境中集成最佳策略，实现知识的高效转移和共享。 |
| [^45] | [From Zero to Hero: Detecting Leaked Data through Synthetic Data Injection and Model Querying.](http://arxiv.org/abs/2310.04145) | 本文介绍了一种名为 LDSS 的新方法，用于检测用于训练分类模型的泄露数据。该方法通过注入特征为类别分布的局部偏移的合成数据来实现。这种方法能够有效地识别出模型查询中的泄露数据。 |
| [^46] | [Routing Arena: A Benchmark Suite for Neural Routing Solvers.](http://arxiv.org/abs/2310.04140) | Routing Arena是一个基准套件，推出了一种路由问题的评估协议，解决了现有协议的缺陷，并提供了与运筹学方法相比的评估基准。 |
| [^47] | [Reinforcement Learning with Fast and Forgetful Memory.](http://arxiv.org/abs/2310.04128) | 以快速遗忘记忆为特点的算法无关的记忆模型针对强化学习中的部分可观测任务，通过强结构先验限制模型搜索空间，实现了比循环神经网络更高的奖励并具有更快的训练速度。 |
| [^48] | [Beyond Myopia: Learning from Positive and Unlabeled Data through Holistic Predictive Trends.](http://arxiv.org/abs/2310.04078) | 本文发现了从正样本和无标签数据中学习的一个重要观察：通过在每个训练迭代中重新采样正样本，可以获得强大的早期性能，并且正类和负类的预测趋势显示出不同的模式。 |
| [^49] | [Automatic Aspect Extraction from Scientific Texts.](http://arxiv.org/abs/2310.04074) | 该论文介绍了一个用于从俄语科学文本中自动提取方面的工具，包括任务、贡献、方法和结论等方面，并提出了基于BERT模型的基准算法。通过跨领域实验证明，即使在有限的科学领域上进行训练，该模型仍能推广到新的领域。 |
| [^50] | [How to Capture Higher-order Correlations? Generalizing Matrix Softmax Attention to Kronecker Computation.](http://arxiv.org/abs/2310.04064) | 本文研究了将经典transformer attention泛化到能够捕捉三元相关性的问题，并提出了一个在有界输入下具有近线性时间复杂度的算法。 |
| [^51] | [DEFT: A new distance-based feature set for keystroke dynamics.](http://arxiv.org/abs/2310.04059) | 该论文提出了一种基于键盘上按键之间距离的新特征集合(DEFT)，该 DEFT 模型在键入动力学研究中超越了传统的打字速度，通过使用 DEFT 特征相结合，可以实现对个人的打字行为的全面洞察。该模型在各种设备上都取得了优于现有方法的效果。 |
| [^52] | [AUTOPARLLM: GNN-Guided Automatic Code Parallelization using Large Language Models.](http://arxiv.org/abs/2310.04047) | AUTOPARLLM是一个用于自动发现并生成顺序编写程序并行版本的框架，其中包括一个基于GNN的并行性发现模块和一个基于LLM的代码生成器。 |
| [^53] | [Observation-Guided Diffusion Probabilistic Models.](http://arxiv.org/abs/2310.04041) | 提出了观测引导的扩散概率模型（OGDM），通过引入基于条件鉴别器的观测所产生的额外损失项，实现了更准确的负对数似然优化，在函数评估次数有限的推理阶段表现出色。 |
| [^54] | [Joint Projection Learning and Tensor Decomposition Based Incomplete Multi-view Clustering.](http://arxiv.org/abs/2310.04038) | 本文提出了一种基于联合投影学习和张量分解的方法，用于解决不完整多视图聚类问题。该方法通过引入正交投影矩阵将高维特征投影到低维空间中，以减轻冗余特征和噪声的影响。 |
| [^55] | [Genetic prediction of quantitative traits: a machine learner's guide focused on height.](http://arxiv.org/abs/2310.04028) | 这篇论文提供了一个机器学习社区关于遗传预测复杂性状的指南，着重介绍了身高作为连续性状的案例，涵盖了当前最先进模型、相关微妙之处以及开发新模型时需要考虑的内容。 |
| [^56] | [PGraphDTA: Improving Drug Target Interaction Prediction using Protein Language Models and Contact Maps.](http://arxiv.org/abs/2310.04017) | 本研究提出了一种使用蛋白质语言模型和接触图改进药物靶标相互作用预测的方法，通过在现有模型中引入联系图信息，可以改进药物靶标相互作用预测的性能。 |
| [^57] | [Learning via Look-Alike Clustering: A Precise Analysis of Model Generalization.](http://arxiv.org/abs/2310.04015) | 本文提出了一种名为“类似样本聚类”的技术，通过替换个体的敏感特征为聚类的平均值来增强隐私。通过对使用匿名聚类中心训练模型的精确分析，我们揭示了不同模型组成部分对泛化误差的影响，并证明在某些高维情况下，使用匿名聚类中心进行训练可以取得更好的效果。 |
| [^58] | [Accelerating optimization over the space of probability measures.](http://arxiv.org/abs/2310.04006) | 本研究研究了在概率测度空间中加速优化的问题，提出了一种类似于欧几里得空间中基于矩方法的哈密顿流方法，并证明了其可以达到任意高阶的收敛速度。 |
| [^59] | [The Role of Federated Learning in a Wireless World with Foundation Models.](http://arxiv.org/abs/2310.04003) | 基于联邦学习的无线世界中，基础模型（FMs）为生成式AI应用提供支持，并且可以通过分散的数据和计算资源来提高联邦学习（FL）的性能，但是FMs对资源需求较高可能给FL-enabled的无线网络带来挑战。 |
| [^60] | [Runtime Monitoring DNN-Based Perception.](http://arxiv.org/abs/2310.03999) | 运行时监控基于DNN的感知的论文总结的重要创新和贡献是介绍了文献中关于监视方法的经典方法和形式方法，并强调了设计严谨的监控器的必要性。 |
| [^61] | [Robust Multimodal Learning with Missing Modalities via Parameter-Efficient Adaptation.](http://arxiv.org/abs/2310.03986) | 通过低秩适应和中间特征的调制，我们提出了针对预训练多模态网络的参数高效适应程序，以实现对缺失模态的鲁棒性，并在某些情况下胜过独立的专门网络。 |
| [^62] | [Dementia Assessment Using Mandarin Speech with an Attention-based Speech Recognition Encoder.](http://arxiv.org/abs/2310.03985) | 本文提出了一个基于注意力的语音识别模型，用于构建一个普通话言语痴呆评估系统。通过训练模型并提取编码器，实现了在阿尔茨海默病检测和临床痴呆评分预测方面的显著提升。 |
| [^63] | [AdaRec: Adaptive Sequential Recommendation for Reinforcing Long-term User Engagement.](http://arxiv.org/abs/2310.03984) | AdaRec是一种自适应顺序推荐算法，通过引入基于距离的表示损失来提取潜在信息，以适应大规模在线推荐系统中用户行为模式的变化。 |
| [^64] | [CUPre: Cross-domain Unsupervised Pre-training for Few-Shot Cell Segmentation.](http://arxiv.org/abs/2310.03981) | 本论文提出了CUPre方法，实现了跨领域无监督预训练，将常见物体检测和实例分割的能力应用于细胞图像领域，为少样本细胞分割提供了一种低成本的注释方法。 |
| [^65] | [Perfect Alignment May be Poisonous to Graph Contrastive Learning.](http://arxiv.org/abs/2310.03977) | 本研究探讨了图形对比学习中增强方法和下游性能的关系，并发现图形对比学习主要通过分离不同类别的节点来为下游任务做出贡献。 |
| [^66] | [Ultimate limit on learning non-Markovian behavior: Fisher information rate and excess information.](http://arxiv.org/abs/2310.03968) | 该论文研究了从时间序列数据中学习任何随机过程的未知参数的极限，发现了最佳推断的封闭形式表达式。对于给定的参数化候选模型类，观测序列概率的费舍尔信息下界了来自有限数据的模型估计方差，最小方差随序列长度的增加而减小，缩放率由信息率给出。通过分析观察引起的信念状态间的元动力学，得到了模型方差的精确下界，并发现了不同的收敛模式，最终短视的信息率收敛到渐近的费舍尔信息率。 |
| [^67] | [A Learnable Counter-condition Analysis Framework for Functional Connectivity-based Neurological Disorder Diagnosis.](http://arxiv.org/abs/2310.03964) | 本研究提出了一种基于学习的功能连接性神经障碍诊断的反条件分析框架，通过集成诊断和解释的步骤，解决了现有框架中各阶段结果可靠性不足的问题。 |
| [^68] | [Understanding prompt engineering may not require rethinking generalization.](http://arxiv.org/abs/2310.03957) | 提示工程在零样本学习中取得了令人印象深刻的性能，其成功归因于经典的PAC-Bayes边界将离散的提示特性与由语言模型给出的先验相结合，形成了相对紧密的泛化界限。 |
| [^69] | [Improved prediction of ligand-protein binding affinities by meta-modeling.](http://arxiv.org/abs/2310.03946) | 通过整合基于结构的对接和基于序列的深度学习模型，开发了一个元模型框架，显著改善了配体-蛋白质结合亲和力预测的性能。 |
| [^70] | [On Wasserstein distances for affine transformations of random vectors.](http://arxiv.org/abs/2310.03945) | 本研究阐述了关于随机向量之间的Wasserstein距离的下界，重点是仿射变换，并应用于流型学习和手写数据集模拟。 |
| [^71] | [LaTeX: Language Pattern-aware Triggering Event Detection for Adverse Experience during Pandemics.](http://arxiv.org/abs/2310.03941) | 本论文使用实时的社交媒体数据，分析了与COVID-19疫情期间四种不良经历相关的语言模式。通过提出稀疏性优化问题和语言模式相似性约束条件，解决了该问题的困难。 |
| [^72] | [Improving classifier decision boundaries using nearest neighbors.](http://arxiv.org/abs/2310.03927) | 该论文提出了一个简单的算法，通过计算样本及其最近邻在潜在空间中的预测的加权平均，改善了神经网络在对抗性攻击、标签噪声、分类准确性和可解释性方面的性能，而无需改变网络架构、训练过程或数据集。 |
| [^73] | [Multitask Learning for Time Series Data\\with 2D Convolution.](http://arxiv.org/abs/2310.03925) | 该论文研究了将多任务学习（MTL）应用于时间序列分类（TSC）问题，并发现将最先进的一维卷积模型与MTL集成时性能下降。为了解决这个问题，提出了一种基于二维卷积的新设计。 |
| [^74] | [An Efficient Content-based Time Series Retrieval System.](http://arxiv.org/abs/2310.03919) | 本论文提出了一种高效的基于内容的时间序列检索系统，可以在用户与系统实时交互的情况下，有效地度量和计算不同时间序列之间的相似度，满足用户从多个领域获取时间序列信息的需求。 |
| [^75] | [Toward a Foundation Model for Time Series Data.](http://arxiv.org/abs/2310.03916) | 本文旨在通过利用多领域的无标签样本来开发一种有效的时间序列基础模型，以解决当前关于时间序列预训练的研究集中在单一领域数据上的问题。 |
| [^76] | [Leveraging Low-Rank and Sparse Recurrent Connectivity for Robust Closed-Loop Control.](http://arxiv.org/abs/2310.03915) | 本文探讨了将循环神经网络应用于鲁棒闭环控制任务，研究循环连接的秩和稀疏性如何影响网络的稳定性。实验证明，采用低秩稀疏连接可以在闭环设置中取得良好效果。 |
| [^77] | [RTDK-BO: High Dimensional Bayesian Optimization with Reinforced Transformer Deep kernels.](http://arxiv.org/abs/2310.03912) | 本文提出了一种新的提高贝叶斯优化模型建模能力的方法，通过将注意机制融入深度核学习中，使得代理能够适应上下文信息，提高优化性能。 |
| [^78] | [PyDCM: Custom Data Center Models with Reinforcement Learning for Sustainability.](http://arxiv.org/abs/2310.03906) | PyDCM是一个用强化学习实现的可定制的数据中心模型，通过使用自定义配置和向量化的热计算，实现了对数据中心的优化，具有较高的效率。 |
| [^79] | [Provable benefits of annealing for estimating normalizing constants: Importance Sampling, Noise-Contrastive Estimation, and beyond.](http://arxiv.org/abs/2310.03902) | 这篇论文研究了使用退火方法估计归一化常数的蒙特卡洛方法。通过评估不同设计选择对估计误差的影响，结果表明使用退火噪声对比估计器更有效，并且使用几何路径可以降低估计误差。 |
| [^80] | [CrysFormer: Protein Structure Prediction via 3d Patterson Maps and Partial Structure Attention.](http://arxiv.org/abs/2310.03899) | 提出了第一个直接利用蛋白质晶体学和部分结构信息来预测蛋白质电子密度图的转换器模型，通过两个新的肽段数据集演示了其准确性。 |
| [^81] | [Class-Incremental Learning Using Generative Experience Replay Based on Time-aware Regularization.](http://arxiv.org/abs/2310.03898) | 提出了一种基于时间感知正则化的生成经验重放的增量学习方法，解决了连续学习中新任务累积且不遗忘旧任务的挑战。实验证明该方法在严格的增量类别设置下具有较好的记忆保留和模型性能。 |
| [^82] | [Accelerated Neural Network Training with Rooted Logistic Objectives.](http://arxiv.org/abs/2310.03890) | 该论文基于根化逻辑目标函数，设计了一种加速神经网络训练的方法，通过推导出一系列严格凸函数，实现了与最小范数解相同的最小化点，从而提高了最优结果的达成速度。 |
| [^83] | [Information Geometry for the Working Information Theorist.](http://arxiv.org/abs/2310.03884) | 本文提供了对于工作中的信息论者来说，信息几何的基本概述。解释了统计流形上的差异、距离、正交性和测地线的概念，并介绍了一些最近的信息几何发展。 |
| [^84] | [Small batch deep reinforcement learning.](http://arxiv.org/abs/2310.03882) | 小批量深度强化学习中，研究发现将批量大小减小可以产生显著性能提升，并进行了实证分析以更好地理解这一现象。 |
| [^85] | [Non Commutative Convolutional Signal Models in Neural Networks: Stability to Small Deformations.](http://arxiv.org/abs/2310.03879) | 本文研究了基于非交换代数的代数信号模型在卷积神经网络中的应用。研究发现非交换卷积滤波器在算子空间的小扰动下可以保持稳定，并且存在稳定性和选择性之间的权衡关系。 |
| [^86] | [Model Complexity of Program Phases.](http://arxiv.org/abs/2310.03865) | 这篇论文研究了资源有限计算系统中序列预测模型的实施成本和预测质量之间的权衡，提出了理论和经验方法来探索这种权衡空间，并认为了解这种权衡行为对于理解资源受限任务中模型的限制是有益的。 |
| [^87] | [Variational Barycentric Coordinates.](http://arxiv.org/abs/2310.03861) | 我们提出了一种变分技术以优化广义重心坐标，通过直接参数化连续函数来解决之前模型中目标函数选择受限的问题。我们的方法在多个目标函数方面都表现出了灵活性，并提供了实用的加速策略。 |
| [^88] | [OpenIncrement: A Unified Framework for Open Set Recognition and Deep Class-Incremental Learning.](http://arxiv.org/abs/2310.03848) | 本论文介绍了一个集成了开放集识别和深度分类增量学习的统一框架。实验证实了我们的方法在增量学习和开放集识别方面优于其他方法。 |
| [^89] | [Euclid: Identification of asteroid streaks in simulated images using deep learning.](http://arxiv.org/abs/2310.03845) | 该论文提出了一种使用深度学习识别Euclid空间望远镜图像中小行星光迹的方法。通过构建、训练和测试一个三步机器学习流程，最终实现了对光迹的自动化检测和分类。 |
| [^90] | [Less is More: On the Feature Redundancy of Pretrained Models When Transferring to Few-shot Tasks.](http://arxiv.org/abs/2310.03843) | 预训练模型在少样本任务中的特征可以极其冗余，仅使用最重要的特征维度的1%就能够达到使用完整表示时的性能。 |
| [^91] | [Contextualized Structural Self-supervised Learning for Ontology Matching.](http://arxiv.org/abs/2310.03840) | 本研究提出了一种名为LaKERMap的自监督学习OM框架，通过将上下文和结构信息整合到transformer中，捕捉多个结构化上下文，并应用于本体匹配中。 |
| [^92] | [Chameleon: Increasing Label-Only Membership Leakage with Adaptive Poisoning.](http://arxiv.org/abs/2310.03838) | Chameleon是一种新的成员推理攻击方法，利用自适应数据污染策略和高效的查询选择方法，可在标签唯一设置中提高成员泄露的准确率。 |
| [^93] | [Learning A Disentangling Representation For PU Learning.](http://arxiv.org/abs/2310.03833) | 本文提出了一种基于神经网络的数据表示方法，通过学习一个损失函数来将未标记数据投影为两个簇，以模拟低维情况下的PU学习问题。实验证明学习到的表示可以增强未标记数据簇之间的分离效果。 |
| [^94] | [ECAvg: An Edge-Cloud Collaborative Learning Approach using Averaged Weights.](http://arxiv.org/abs/2310.03823) | ECAvg是一种边缘-云协同学习方法，通过平均权重实现边缘设备和云服务器之间的模型协作与更新。 |
| [^95] | [Logical Languages Accepted by Transformer Encoders with Hard Attention.](http://arxiv.org/abs/2310.03817) | 本文研究了使用硬注意力的Transformer编码器可以接受的逻辑语言的问题，发现UHAT编码器只能识别${\sf AC}^0$中的一部分语言，而AHAT编码器可以识别更丰富的语言。 |
| [^96] | [Fishnets: Information-Optimal, Scalable Aggregation for Sets and Graphs.](http://arxiv.org/abs/2310.03812) | Fishnets是一种用于学习信息最优的集合和图聚合的方法，在规模上可以优化到任意数量的数据对象，具有鲁棒性，能够饱和贝叶斯信息内容，并可用于GNNs中的消息传递。 |
| [^97] | [Droplets of Good Representations: Grokking as a First Order Phase Transition in Two Layer Networks.](http://arxiv.org/abs/2310.03789) | 本研究将自适应核方法应用于两个师生模型，预测了特征学习和 Grokking 的性质，并展示了 Grokking 与相变理论之间的映射关系。 |
| [^98] | [HandMeThat: Human-Robot Communication in Physical and Social Environments.](http://arxiv.org/abs/2310.03779) | HandMeThat是一个综合评估基准，用于在物理和社交环境中理解和遵循人类指令。在该论文中，提出了一个包含10000个人机交互场景的数据集，并评估了不同的基准模型的表现。结果显示离线和在线强化学习算法在该基准上表现不佳，暗示了其中的挑战和困难。 |
| [^99] | [Lightweight Boosting Models for User Response Prediction Using Adversarial Validation.](http://arxiv.org/abs/2310.03778) | 这篇论文提出了一个轻量级的增强模型解决方案，通过使用对抗验证来消除非信息性特征，并通过特征工程技术处理有噪声的连续特征和具有大量唯一值的分类特征。实验证明，该方法在ACM RecSys Challenge 2023中取得了良好的性能。 |
| [^100] | [Functional data learning using convolutional neural networks.](http://arxiv.org/abs/2310.03773) | 本文展示了使用卷积神经网络进行函数数据学习的方法。通过将函数数据转化为图像，利用特定的卷积神经网络架构进行参数估计和函数形式的分类学习。研究结果表明，这种方法在估计函数参数、分类函数形式以及混沌数据的Lyapunov指数估计中具有很高的准确性。 |
| [^101] | [Investigating Alternative Feature Extraction Pipelines For Clinical Note Phenotyping.](http://arxiv.org/abs/2310.03772) | 研究探索了临床记录表型的替代特征提取流程，使用基于BERT模型的方法将临床记录转换为文档表示，并传入LSTM模型，取得了明显的性能改进，但收敛时间较长且不支持增量训练。 |
| [^102] | [Progressive reduced order modeling: empowering data-driven modeling with selective knowledge transfer.](http://arxiv.org/abs/2310.03770) | 本文提出了一种渐进降阶建模框架，通过选择性地从先前训练的模型中传递知识，减少数据需求，并且提高了数据驱动建模的实用性。实验结果表明，在多个案例中，保留先前模型的信息并利用其中有价值的部分可以提高建模的准确性。 |
| [^103] | [Deep Reinforcement Learning Algorithms for Hybrid V2X Communication: A Benchmarking Study.](http://arxiv.org/abs/2310.03767) | 这项研究通过使用深度强化学习算法，针对混合V2X通信中的垂直切换问题进行了研究，并提出了一种在弯曲环境中帮助车辆选择最合适的V2X技术的方法。研究结果表明，这些算法优于当前的状态。 |
| [^104] | [Investigating Deep Neural Network Architecture and Feature Extraction Designs for Sensor-based Human Activity Recognition.](http://arxiv.org/abs/2310.03760) | 本研究探讨了在基于传感器的人体活动识别中，深度神经网络架构和特征提取设计的应用。通过实验研究，发现深度学习方法在活动识别中超越了传统的信号处理和机器学习方法，并探索了不同的训练机制和特征表示对于人体活动识别的有效性。 |
| [^105] | [A Novel Deep Learning Technique for Morphology Preserved Fetal ECG Extraction from Mother ECG using 1D-CycleGAN.](http://arxiv.org/abs/2310.03759) | 通过1D-CycleGAN技术，我们提出了一种新颖的深度学习方法，可以从母体心电图中提取胎儿心电图，保持其形态特征，并且在实验证实了其性能。 |
| [^106] | [A Unified Framework for Uniform Signal Recovery in Nonlinear Generative Compressed Sensing.](http://arxiv.org/abs/2310.03758) | 该论文提出了一个统一的框架，用于在非线性生成式压缩感知中实现统一的信号恢复。该框架适用于非线性和可能非连续或未知的观测模型，并且可以恢复生成模型中所有可能的信号。 |
| [^107] | [Enhancing Healthcare with EOG: A Novel Approach to Sleep Stage Classification.](http://arxiv.org/abs/2310.03757) | 本论文介绍了一种新颖的方法，利用EOG信号进行自动化睡眠阶段分类，并改进了睡眠障碍研究中对REM睡眠的识别。我们的提出的SE-Resnet-Transformer模型在多个数据库上得到了验证，并取得了显著的性能。这一发展将提高睡眠阶段分类的可访问性，减少对EEG设备的需求。 |
| [^108] | [A Multi-channel EEG Data Analysis for Poor Neuro-prognostication in Comatose Patients with Self and Cross-channel Attention Mechanism.](http://arxiv.org/abs/2310.03756) | 本研究提出了一个基于深度学习的多通道脑电图数据分析方法，旨在预测昏迷患者的低病理性神经结果。该方法采用双极EEG记录和注意机制，以提高预测效果。实验结果表明，该方法在预测挑战中取得了良好的表现。 |
| [^109] | [Physics Informed Neural Network Code for 2D Transient Problems (PINN-2DT) Compatible with Google Colab.](http://arxiv.org/abs/2310.03755) | 这个论文提出了一个适用于谷歌Colab的开源物理信息神经网络环境，可用于在二维矩形域上模拟瞬态现象，并提供了多个特性和问题库。 |
| [^110] | [EMGTFNet: Fuzzy Vision Transformer to decode Upperlimb sEMG signals for Hand Gestures Recognition.](http://arxiv.org/abs/2310.03754) | 本文提出了一种名为EMGTFNet的基于模糊视觉变压器的架构，用于通过表面肌电信号进行手势识别，可以准确分类各种手势，无需数据增强技术或网络参数的增加。 |
| [^111] | [ECGNet: A generative adversarial network (GAN) approach to the synthesis of 12-lead ECG signals from single lead inputs.](http://arxiv.org/abs/2310.03753) | ECGNet使用生成对抗网络 (GAN) 从单导联输入合成12导联心电图信号，并通过特征分析识别出可用于心血管疾病预测的特征。 |
| [^112] | [A Deep Learning Sequential Decoder for Transient High-Density Electromyography in Hand Gesture Recognition Using Subject-Embedded Transfer Learning.](http://arxiv.org/abs/2310.03752) | 本文提出了一个基于主题嵌入迁移学习的深度学习序列解码器，用于瞬态高密度肌电图手势识别。该解码器在部分观察的受试者中取得了73%的平均准确率。 |
| [^113] | [A Simple Illustration of Interleaved Learning using Kalman Filter for Linear Least Squares.](http://arxiv.org/abs/2310.03751) | 本论文通过使用卡尔曼滤波器和线性最小二乘法的简单框架，演示了交叉学习的机制。 |
| [^114] | [Health diagnosis and recuperation of aged Li-ion batteries with data analytics and equivalent circuit modeling.](http://arxiv.org/abs/2310.03750) | 本论文通过对商业高能型磷酸铁锂电池的老化和修复实验，使用机器学习模型预测循环寿命并识别可恢复容量的重要指标。考虑到电池间的不一致性，平均循环寿命预测误差为16.84% ± 1.87%，为电池健康评估和恢复提供了有力的支持。 |
| [^115] | [SCVCNet: Sliding cross-vector convolution network for cross-task and inter-individual-set EEG-based cognitive workload recognition.](http://arxiv.org/abs/2310.03749) | 本文提出了SCVCNet神经网络，通过分析脑电图中的细粒度频率结构来消除任务和个体集相关的干扰，实现了跨任务和个体间的脑电认知负荷识别。 |
| [^116] | [Phase Synchrony Component Self-Organization in Brain Computer Interface.](http://arxiv.org/abs/2310.03748) | 本文提出了相位同步成分自组织的概念，利用深度学习端到端网络自动化脑机接口中的预处理和通道选择，从而提高了分析功能性脑连接和识别脑活动的效果。 |
| [^117] | [A Knowledge-Driven Cross-view Contrastive Learning for EEG Representation.](http://arxiv.org/abs/2310.03747) | 本文提出了一种基于知识驱动的 EEG 表示的跨视图对比学习框架 (KDC2)，通过模拟脑活动的内部和外部表示，从有限标签的 EEG 中提取有效表示，并通过视图和跨视图对比学习进行特征提取和表示学习。 |
| [^118] | [Generative Hyperelasticity with Physics-Informed Probabilistic Diffusion Fields.](http://arxiv.org/abs/2310.03745) | 该论文中介绍了一种基于生成模型的方法，利用神经常微分方程和概率扩散模型生成具有物理知识约束的超弹性材料应变能函数的新样本。 |
| [^119] | [GENER: A Parallel Layer Deep Learning Network To Detect Gene-Gene Interactions From Gene Expression Data.](http://arxiv.org/abs/2310.03611) | GENER是一个并行层深度学习网络，专门用于从基因表达数据中检测基因-基因相互作用。实验证实GENER在预测基因-基因相互作用方面的性能优于其他统计和深度学习方法。 |
| [^120] | [FASER: Binary Code Similarity Search through the use of Intermediate Representations.](http://arxiv.org/abs/2310.03605) | 本论文提出了一种名为FASER的方法，通过使用中间表示进行二进制代码相似性搜索。该方法可以跨架构地识别函数，并明确编码函数的语义，以支持各种应用场景。 |
| [^121] | [A 5' UTR Language Model for Decoding Untranslated Regions of mRNA and Function Predictions.](http://arxiv.org/abs/2310.03281) | 这种研究引入了一种新的语言模型UTR-LM，通过对多个物种的5' UTR进行预训练，并结合有监督信息，该模型在多个下游任务中的表现超过了现有的最佳模型，可以有效预测平均核糖体负载、翻译效率、mRNA表达水平，并改进了内源性核糖体进入位点的识别性能。 |
| [^122] | [FedHyper: A Universal and Robust Learning Rate Scheduler for Federated Learning with Hypergradient Descent.](http://arxiv.org/abs/2310.03156) | FedHyper是一种适用于联邦学习的通用和稳健学习率调度器，通过超梯度下降算法实现对全局和局部学习率的自适应调整，能够显著提高联邦学习系统的效果，同时减少了对经验调整的需求。 |
| [^123] | [Attributing Learned Concepts in Neural Networks to Training Data.](http://arxiv.org/abs/2310.03149) | 通过将数据归因方法与概念探测方法相结合，研究了神经网络中学习到的概念与训练数据的关系，并发现概念的位置和稀疏性并不完全依赖于少量特定示例。 |
| [^124] | [GPT-MolBERTa: GPT Molecular Features Language Model for molecular property prediction.](http://arxiv.org/abs/2310.03030) | GPT-MolBERTa是一种用于分子性质预测的自监督大型语言模型，通过使用分子的详细文本描述来学习分子的表示，实验表明其在各种分子属性基准上表现良好，并在回归任务中接近最先进的性能。 |
| [^125] | [Decision ConvFormer: Local Filtering in MetaFormer is Sufficient for Decision Making.](http://arxiv.org/abs/2310.03022) | Decision ConvFormer提出了一种新的动作序列预测器，通过使用本地卷积过滤来捕捉强化学习数据集中的局部关联，同时在各个标准RL基准上取得了最先进的性能。 |
| [^126] | [MedDiffusion: Boosting Health Risk Prediction via Diffusion-based Data Augmentation.](http://arxiv.org/abs/2310.02520) | 本文介绍了一种名为MedDiffusion的新型、端到端的扩散式风险预测模型，通过基于扩散的数据增强，提升了健康风险预测的效果。 |
| [^127] | [Secure and Effective Data Appraisal for Machine Learning.](http://arxiv.org/abs/2310.02373) | 本文介绍了一种机密的数据选择和评估方法，通过创新的流程和简化的低维度操作来实现，以保护数据和模型的隐私，并在多个Transformer模型和NLP/CV基准测试中进行了评估。 |
| [^128] | [Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation.](http://arxiv.org/abs/2310.01320) | 本研究通过使用复杂的Avalon游戏作为测试平台，引入了一种名为递归思考（ReCon）的新框架，用于增强大型语言模型（LLM）识别和对抗欺骗信息的能力。 |
| [^129] | [BooookScore: A systematic exploration of book-length summarization in the era of LLMs.](http://arxiv.org/abs/2310.00785) | 本文对LLM模型进行了系统探索，以解决对超过上下文窗口大小的书籍进行摘要的问题，并通过两种提示工作流实施了基于LLM的书籍长度摘要器的连贯性研究。通过对100本书的GPT-4生成摘要的人工注释，发现了八种常见的连贯性错误。 |
| [^130] | [Pre-training with Synthetic Data Helps Offline Reinforcement Learning.](http://arxiv.org/abs/2310.00771) | 本文研究表明，在离线深度强化学习中，使用合成数据进行预训练可以提高性能，而不一定需要语言预训练。此外，使用一步马尔科夫链生成的数据进行预训练可进一步改善性能。在一个流行的离线DRL算法中，使用简单的预训练方案也能获得性能提升。 |
| [^131] | [Data-Efficient Power Flow Learning for Network Contingencies.](http://arxiv.org/abs/2310.00763) | 本论文提出了一种数据高效的方法，用于学习具有网络事故的电网中的电力流，并估计电力流的概率性。通过使用网络感知高斯过程和多任务顶点度核，该方法实现了对未见网络的电力流推断，并在低训练数据情况下比现有方法具有更好的性能。 |
| [^132] | [Enhancing Efficiency and Privacy in Memory-Based Malware Classification through Feature Selection.](http://arxiv.org/abs/2310.00516) | 本研究通过特征选择方法提高了内存中恶意软件的分类效果和隐私保护。在实验中，我们使用三种特征选择方法从内存内容中识别出重要特征，并将它们应用于多样的分类器中，实现了恶意软件的二元分类、类型分类和家族分类。 |
| [^133] | [A Neural-preconditioned Poisson Solver for Mixed Dirichlet and Neumann Boundary Conditions.](http://arxiv.org/abs/2310.00177) | 我们引入了一个用于混合边界条件的泊松方程的神经预处理迭代求解器，核心是一个神经网络，能够近似逆离散结构网格拉普拉斯算子，并且在训练集之外的边界条件下仍然有效。 |
| [^134] | [Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words.](http://arxiv.org/abs/2309.16108) | 本文提出了ChannelViT模型，通过对ViT架构的修改和引入分层通道采样技术，增强了对多通道图像的推理能力和鲁棒性，适用于显微镜和卫星成像等领域。 |
| [^135] | [Leveraging Herpangina Data to Enhance Hospital-level Prediction of Hand-Foot-and-Mouth Disease Admissions Using UPTST.](http://arxiv.org/abs/2309.14674) | 提出了一种新颖的基于Transformer的UPTST模型，利用腭咽口炎数据提升手足口病住院预测的准确性，且在医院级别的预测准确性上优于现有方法。 |
| [^136] | [NAS-NeRF: Generative Neural Architecture Search for Neural Radiance Fields.](http://arxiv.org/abs/2309.14293) | NAS-NeRF是一种生成式神经体系结构搜索策略，通过平衡架构复杂度和目标合成质量指标生成紧凑、针对场景的NeRF架构。 |
| [^137] | [Adaptive Input-image Normalization for Solving Mode Collapse Problem in GAN-based X-ray Images.](http://arxiv.org/abs/2309.12245) | 本论文研究了生成对抗网络中的模式塌陷问题对合成X射线图像多样性的影响。通过实验证明了将自适应输入图像归一化方法与深度模型结合的优势。 |
| [^138] | [Actively Learning Reinforcement Learning: A Stochastic Optimal Control Approach.](http://arxiv.org/abs/2309.10831) | 本文提供了一个框架来解决强化学习中的模型不确定性和计算成本高的问题，通过使用强化学习解决随机动态规划方程，生成的控制器能够主动学习模型不确定性，并确保安全性和实时学习。 |
| [^139] | [Reducing Adversarial Training Cost with Gradient Approximation.](http://arxiv.org/abs/2309.09464) | 本文提出了一种新的对抗训练方法——梯度逼近对抗训练(GAAT)，通过泰勒级数的部分和来近似对抗损失，并近似梯度，以降低建立鲁棒模型的成本。 |
| [^140] | [Sleep Stage Classification Using a Pre-trained Deep Learning Model.](http://arxiv.org/abs/2309.07182) | 本研究提出了一种名为"EEGMobile"的机器学习模型，在睡眠阶段分类中取得了优于其他模型的准确率，特别在N1阶段表现更佳。 |
| [^141] | [A Benchmark Study on Calibration.](http://arxiv.org/abs/2308.11838) | 这项研究提出了一个模型校准的基准研究，利用神经架构搜索空间探索了模型校准属性。研究结果显示，模型校准可以在不同任务中泛化，并可以同时兼顾模型的准确性和校准性能。 |
| [^142] | [Ceci n'est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings.](http://arxiv.org/abs/2308.11804) | 该论文研究了多模态嵌入中的对抗幻觉问题。对手可以扰动输入的任意模态，使其嵌入与其他模态的任意输入接近，从而实现任意图像与任意文本、任意文本与任意声音的对齐。该问题与下游任务无关，对生成和分类任务会产生误导。 |
| [^143] | [Ultra Dual-Path Compression For Joint Echo Cancellation And Noise Suppression.](http://arxiv.org/abs/2308.11053) | 本文提出了一种超级双路径压缩方法用于联合回声消除和噪声抑制，通过时间频率双路径压缩实现广泛的压缩比，同时降低计算成本，且在固定压缩比下能够进一步改善性能。 |
| [^144] | [Image Processing and Machine Learning for Hyperspectral Unmixing: An Overview and the HySUPP Python Package.](http://arxiv.org/abs/2308.09375) | 本文概述了高光谱解混的先进和常规方法，并比较了这些方法在不同解混场景下的性能。另外，还介绍了一个开源的解混软件包HySUPP。 |
| [^145] | [Optimal Resource Allocation for U-Shaped Parallel Split Learning.](http://arxiv.org/abs/2308.08896) | 本论文提出了一种新颖的并行U型分层学习方法，并设计了优化资源分配方案以提高边缘网络的性能。实验结果表明该方法可以达到与其他方法相似的性能。 |
| [^146] | [An Empirical Study of Bugs in Open-Source Federated Learning Framework.](http://arxiv.org/abs/2308.05014) | 本文通过实证研究发现了开源联邦学习框架中存在的错误，并对这些错误的特征进行了详细分析，为提高框架的安全性和稳定性提供了指导。 |
| [^147] | [Maximizing Success Rate of Payment Routing using Non-stationary Bandits.](http://arxiv.org/abs/2308.01028) | 本文提出了一种使用非平稳赌博算法的支付路由策略，通过系统架构设计和实时实验的验证，成功提高了0.92\%的交易成功率。 |
| [^148] | [SkullGAN: Synthetic Skull CT Generation with Generative Adversarial Networks.](http://arxiv.org/abs/2308.00206) | 使用SkullGAN，一种生成对抗网络（GAN），生成合成的颅骨CT图像，可以减少对真实图像的依赖，加速将机器学习应用于医疗保健领域的整合。 |
| [^149] | [Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection.](http://arxiv.org/abs/2307.16888) | 这项研究介绍了一种针对指令调整的大型语言模型的新型后门攻击方法，即虚拟提示注入（VPI）。通过在特定触发场景下将虚拟提示与用户指令连接，攻击者可以精细操纵模型的回应而无需明确注入。 |
| [^150] | [Deep Learning Meets Adaptive Filtering: A Stein's Unbiased Risk Estimator Approach.](http://arxiv.org/abs/2307.16708) | 本文通过算法展开的方法，将递归最小二乘法（RLS）和等变自适应源分离（EASI）两种算法转化为深度神经网络的层，通过利用训练过程有效地进行源信号估计。同时，通过使用基于Stein无偏风险估计（SURE）的损失函数训练，进一步提高了性能，实证评估证明了该方法的有效性。 |
| [^151] | [Zero-shot Domain-sensitive Speech Recognition with Prompt-conditioning Fine-tuning.](http://arxiv.org/abs/2307.10274) | 本研究提出了一种零样本领域敏感语音识别方法，利用文本提示来生成领域敏感模型，通过微调预训练的端到端模型实现。实验结果表明，该方法在不同领域和提示上下文下均取得了良好的性能，词错误率降低达到最高33%。通过仅使用文本进行微调，该模型在医学对话数据集上的识别效果最佳，词错误率降低达到29%。 |
| [^152] | [Layerwise Linear Mode Connectivity.](http://arxiv.org/abs/2307.06966) | 本文提出了一种分层线性模态连接方法用于联邦深度学习，通过解决模型漂移和高损失障壁的问题，能够有效提升全局模型的性能。 |
| [^153] | [Spectral-Bias and Kernel-Task Alignment in Physically Informed Neural Networks.](http://arxiv.org/abs/2307.06362) | 本文提出了一个综合的理论框架，解决了物理信息神经网络（PINN）设计和训练协议的选择问题。通过将超参数化神经网络和高斯过程回归等价起来，推导出了一种在大数据集限制下决定PINN预测的积分微分方程，以及通过原始微分方程中源项的谱分解来量化网络引入的隐含偏差。 |
| [^154] | [Improving Prototypical Part Networks with Reward Reweighing, Reselection, and Retraining.](http://arxiv.org/abs/2307.03887) | 本研究通过引入奖励重新加权、重选和重新训练的方法，改进了原型零件网络的分类效果，解决了学习从图像的虚假或不一致的部分进行分类的问题。 |
| [^155] | [CellViT: Vision Transformers for Precise Cell Segmentation and Classification.](http://arxiv.org/abs/2306.15350) | CellViT是一种基于Vision Transformer的深度学习架构，用于自动实例分割苏木精和伊红染色的组织样本中的细胞核。通过在PanNuke数据集上训练和评估，CellViT展示了其优越性。 |
| [^156] | [DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing.](http://arxiv.org/abs/2306.14435) | DragDiffusion是一个利用扩散模型进行交互式点基图像编辑的方法，通过优化扩散潜在实现精确的空间控制，以提高实际场景中的应用性。 |
| [^157] | [A Simple and Effective Pruning Approach for Large Language Models.](http://arxiv.org/abs/2306.11695) | 本论文提出了一种称为Wanda的新颖、简单而有效的剪枝方法，用于大型语言模型，通过对每个输出上的权重按照最小幅度乘以对应的输入激活来进行剪枝，无需重新训练或更新权重。 |
| [^158] | [Masked Diffusion Models Are Fast and Privacy-Aware Learners.](http://arxiv.org/abs/2306.11363) | 该论文提出了一种基于先验的去噪训练框架，通过遮蔽学习和扩散模型的结合，实现了更高效的训练和生成更高质量的图像。 |
| [^159] | [Adaptive Federated Learning with Auto-Tuned Clients.](http://arxiv.org/abs/2306.11201) | 本研究提出了一种自适应的联邦学习方法，其中包括了自动调整客户端的步长规则。实证结果表明，这种方法在各种联邦学习场景中具有显著的优势。 |
| [^160] | [Simple High Quality OoD Detection with L2 Normalization.](http://arxiv.org/abs/2306.04072) | 本篇论文介绍了在ResNet模型训练中应用L2归一化技术，可以以几乎没有成本的方式实现与最先进的OoD检测性能相媲美的结果，测试时仅需移除L2归一化即可。 |
| [^161] | [A Generalized Alternating Method for Bilevel Learning under the Polyak-{\L}ojasiewicz Condition.](http://arxiv.org/abs/2306.02422) | 本研究提出了一种基于Polyak-{\L}ojasiewicz条件的双层学习的广义交替方法，即GALET，可以用于解决非凸下层目标的双层问题。该方法可以在$\tilde{\cal O}(\epsilon^{-1})$迭代次数内实现问题的$\epsilon$-静态度量。 |
| [^162] | [Vandermonde Neural Operators.](http://arxiv.org/abs/2305.19663) | 本文提出了一种新的神经网络结构，Vandermonde神经算子，能够有效地处理非均匀分布点上的输入数据，同时在速度和准确性上相较于以前的方法有所提升。 |
| [^163] | [Automated Search-Space Generation Neural Architecture Search.](http://arxiv.org/abs/2305.18030) | Automated Search-Space Generation Neural Architecture Search (ASGNAS) is an automated system that trains general DNNs covering all candidate connections and operations and produces high-performing sub-networks. It minimizes the need for human expertise and manual intervention by automatically generating the search space and utilizing a Hierarchical Half-Space Projected Gradient (H2SPG) to ensure network validity and performance. |
| [^164] | [Provably Fast Finite Particle Variants of SVGD via Virtual Particle Stochastic Approximation.](http://arxiv.org/abs/2305.17558) | 本论文提出了两种基于虚拟粒子随机逼近的可证速限制变种的SVGD算法，具有可证速的有限粒子收敛率。 |
| [^165] | [On convex conceptual regions in deep network representations.](http://arxiv.org/abs/2305.17154) | 本文研究了深度网络表示中概念空间的凸性对泛化能力、小样本学习和主观一致性的影响，发现近似凸性在多个应用领域中广泛存在。 |
| [^166] | [Density Ratio Estimation-based Bayesian Optimization with Semi-Supervised Learning.](http://arxiv.org/abs/2305.15612) | 该论文提出了一种基于密度比估计和半监督学习的贝叶斯优化方法，通过使用监督分类器代替密度比来估计全局最优解的两组数据的类别概率，避免了分类器对全局解决方案过于自信的问题。 |
| [^167] | [NeuralMatrix: Moving Entire Neural Networks to General Matrix Multiplication for Efficient Inference.](http://arxiv.org/abs/2305.14405) | NeuralMatrix是一种框架，能够在单个通用矩阵乘法加速器上计算深度神经网络(DNNs)，并可在保持推理准确度的情况下实现高达113倍至19.44倍的性能提升。 |
| [^168] | [CoreDiff: Contextual Error-Modulated Generalized Diffusion Model for Low-Dose CT Denoising and Generalization.](http://arxiv.org/abs/2304.01814) | 本文提出了一种新的上下文误差调制广义扩散模型（CoreDiff），用于低剂量CT（LDCT）的去噪。该模型利用LDCT图像来消除随机高斯噪声并模拟CT退化的物理过程，减少采样步骤，并引入上下文误差调制以增强鲁棒性和泛化能力。 |
| [^169] | [FedFTN: Personalized Federated Learning with Deep Feature Transformation Network for Multi-institutional Low-count PET Denoising.](http://arxiv.org/abs/2304.00570) | 本研究提出了FedFTN，一种个性化的联邦学习策略，用于解决多机构低计数PET去噪中的领域差异问题。该方法通过深度特征转换网络来改善低计数PET图像质量，同时避免了集中式数据集的隐私和安全问题。 |
| [^170] | [Multi-Task Learning for Post-transplant Cause of Death Analysis: A Case Study on Liver Transplant.](http://arxiv.org/abs/2304.00012) | CoD-MTL是一个新的死因分析框架，采用多任务学习来共同模拟不同CoD预测任务之间的语义关系，并且利用树蒸馏策略进行多任务学习，在具备树模型和多任务学习的优点中发掘最大化的利益。该框架可以提供精确可靠的CoD预测，具有重要的临床应用价值。 |
| [^171] | [A Token-Wise Beam Search Algorithm for RNN-T.](http://arxiv.org/abs/2302.14357) | 提出了一个面向RNN-T的令牌级波束搜索算法，通过批处理联合网络的调用，实现了20%至96%的解码加速，并且通过汇总发射概率在找到最可能的模型输出方面取得了11%的最优错误率提高。 |
| [^172] | [Differentiable Outlier Detection Enable Robust Deep Multimodal Analysis.](http://arxiv.org/abs/2302.05608) | 本研究提出了一个可微的异常检测方法来实现鲁棒的深度多模态分析，通过借助明确的知识图谱和交互式的区分外部领域层来过滤噪声。在多个视觉和语言任务中得到了良好的应用效果。 |
| [^173] | [Neighborhood Homophily-based Graph Convolutional Network.](http://arxiv.org/abs/2301.09851) | 本文提出了一种基于邻居同质性的图卷积网络 (NHGCN) 模型，利用新指标 Neighborhood Homophily (NH) 测量节点邻域中的标签复杂度或纯度。实验证明 NHGCN 模型在节点分类和图分类任务中表现优异并显著超过最先进模型。 |
| [^174] | [Implicit Convolutional Kernels for Steerable CNNs.](http://arxiv.org/abs/2212.06096) | 本文提出了一种使用隐式神经表示的方法来参数化可定向卷积核，从而实现了简单灵活的构建可定向卷积神经网络的方法，能够推广到任何具有等变MLP的群G。 |
| [^175] | [Multi-Domain Long-Tailed Learning by Augmenting Disentangled Representations.](http://arxiv.org/abs/2210.14358) | 本研究针对多领域长尾学习问题提出了一种方法TALLY，通过混合示例的语义表示和域相关干扰，使用选择性均衡采样策略进行数据增强，同时利用域不变的类原型改善语义表示的解缠。在多个基准测试中验证了TALLY的有效性。 |
| [^176] | [Robust One-Shot Singing Voice Conversion.](http://arxiv.org/abs/2210.11096) | 本研究提出了一种健壮的一次性歌声转换模型，通过使用生成对抗网络和特定的条件来实现高质量的任意到任意的歌声转换。所提出的模型通过部分域条件和音高分布匹配等方法，能够泛化到未知歌手，并且在面对带有混响和伴奏音乐的歌声时仍然能够保持稳定性。 |
| [^177] | [AMPNet: Attention as Message Passing for Graph Neural Networks.](http://arxiv.org/abs/2210.09475) | AMPNet是一种用于图神经网络的基于注意力的消息传递层，能够对节点进行逐个特征编码，并通过跨节点注意力模型特征级别的交互。在实际生物系统上进行广泛基准测试表明，AMPNet在fMRI信号重建方面优于现有基准，并通过案例研究验证了其发现有意义的特征级别交互的能力。 |
| [^178] | [Blinder: End-to-end Privacy Protection in Sensing Systems via Personalized Federated Learning.](http://arxiv.org/abs/2209.12046) | 本论文提出了一种基于个性化联合学习的Blinder匿名化模型，能在异构环境中提供端到端的隐私保护。与在集中数据上训练的最先进模型相比，Blinder在保护隐私的同时，仅增加最多4.00%的隐私损失，并降低最多4.24%的数据效用。同时，Blinder还展示了匿名化射频感知模态的能力。 |
| [^179] | [De-Identification of French Unstructured Clinical Notes for Machine Learning Tasks.](http://arxiv.org/abs/2209.09631) | 本文提出了一种用于机器学习任务的法语非结构化临床笔记去标识化方法，并总结了过去几十年中在英语文档上进行去标识化的方法。该方法可以帮助保护患者的隐私，并促进医疗数据的共享。 |
| [^180] | [Conformal Inference for Online Prediction with Arbitrary Distribution Shifts.](http://arxiv.org/abs/2208.08401) | 这项研究解决了在线预测中分布变化的问题，并开发了一种自适应的算法来应对分布的大小和类型变化，具有小遗憾。这种算法与任何基准预测算法结合使用 |
| [^181] | [SGD and Weight Decay Provably Induce a Low-Rank Bias in Neural Networks.](http://arxiv.org/abs/2206.05794) | 使用SGD和权重衰减训练深度ReLU神经网络会导致对于权重矩阵的秩最小化的偏差，特别是在使用较小批量大小、更高学习率或增加权重衰减时更为显著。此外，在中间神经网络崩溃时，学习的权重特别低秩。这种偏差与泛化之间存在关系。 |
| [^182] | [Reinforcement Learning with a Terminator.](http://arxiv.org/abs/2205.15376) | 这是一个关于强化学习中外部终止问题的论文，通过定义终止马尔可夫决策过程（TerMDP）并学习其参数，提出了一种能够考虑终止情况并限制遗憾值的算法，并在驾驶和基准测试中验证了其有效性。 |
| [^183] | [Calibration of Derivative Pricing Models: a Multi-Agent Reinforcement Learning Perspective.](http://arxiv.org/abs/2203.06865) | 本文利用多智能体强化学习提出校准衍生品定价模型问题的博弈论解决方案，并希望该方法可用于解决其他金融领域的问题。实验证明，该算法能够学习局部波动率以及最小化百慕大期权价格所需的路径依赖性。 |
| [^184] | [Deep Efficient Continuous Manifold Learning for Time Series Modeling.](http://arxiv.org/abs/2112.03379) | 本文提出了一个深度高效的连续流形学习框架，通过在Riemannian流形和Cholesky空间之间利用微分同胚映射，实现了对非欧几里得数据的优化问题高效求解和计算成本的大幅降低。 |
| [^185] | [Critical Initialization of Wide and Deep Neural Networks through Partial Jacobians: General Theory and Applications.](http://arxiv.org/abs/2111.12143) | 本论文通过部分雅可比矩阵的分析，提出了一种诊断深度神经网络临界性的实用方法，并通过递归关系分析了带有LayerNorm和/或残差连接的深度全连接神经网络的临界性。 |
| [^186] | [Decentralized Multi-Armed Bandits Can Outperform Centralized Upper Confidence Bound Algorithms.](http://arxiv.org/abs/2111.10933) | 本文研究了一个多智能体网络中的分布式多臂赌博机问题，提出了两种完全分布式的算法，基于经典的UCB算法和KL-UCB算法，实验证明这些算法能达到更好的对数渐近后悔，智能体之间的邻居关系越多，后悔值越好。 |
| [^187] | [Learning Augmentation Distributions using Transformed Risk Minimization.](http://arxiv.org/abs/2111.08190) | 我们提出了一种新的转换风险最小化框架，可以同时学习预测模型和数据变换，特别是分布的变换。我们以学习图像增强为主要应用，并提出了解决过拟合问题的正则化方法。 |
| [^188] | [Convolutional Motif Kernel Networks.](http://arxiv.org/abs/2111.02272) | 本文介绍了一种名为卷积模式核网络的神经网络架构，通过学习位置感知模式核函数在希尔伯特空间子空间内的特征表示，实现了直接解释和评估预测结果。 |
| [^189] | [Assessment of hybrid machine learning models for non-linear system identification of fatigue test rigs.](http://arxiv.org/abs/2107.03645) | 本文开发了一种混合模型，利用长短期记忆网络结合线性频率响应函数模型进行非线性系统识别。该方法还可以应用于虚拟传感。通过对疲劳试验数据进行测试验证了该方法的效果。 |
| [^190] | [Stronger Calibration Lower Bounds via Sidestepping.](http://arxiv.org/abs/2012.03454) | 本文研究了在线二进制预测设置中的校准问题，证明了校准误差的Ω(T^(0.528))下界，这是第一个超过√T的下界。 |
| [^191] | [Gaussian Mixture Reduction with Composite Transportation Divergence.](http://arxiv.org/abs/2002.08410) | 本文提出了一种基于复合传输散度的高斯混合简化方法，用于解决高斯混合在递归更新中阶数指数增加的推断问题。 |
| [^192] | [Deformable Generator Networks: Unsupervised Disentanglement of Appearance and Geometry.](http://arxiv.org/abs/1806.06298) | 可变形生成器网络能够以无监督的方式解耦图像和视频中的外观和几何信息，通过生成变形场实现几何变形，提供了一种通用且有效的生成模型。 |

# 详细

[^1]: "BrainSCUBA: 视觉皮层选择性的细粒度自然语言描述"

    BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex Selectivity. (arXiv:2310.04420v1 [cs.LG])

    [http://arxiv.org/abs/2310.04420](http://arxiv.org/abs/2310.04420)

    "BrainSCUBA通过生成自然语言描述来预测最大激活个体感兴趣体素的图像，达到了细粒度的视觉皮层选择性描述。"

    

    "理解高级视觉皮层的功能组织是神经科学的核心关注点。过去的研究主要使用手动选择的刺激来映射神经群体的视觉和语义选择性，这可能会导致对视觉皮层功能的预设假设的结果偏差。我们引入了一种数据驱动的方法，通过生成自然语言描述来预测最大激活个体感兴趣体素的图像。我们的方法- 基于对比视觉-语言模型学到的丰富嵌入空间，并利用预训练的大型语言模型生成可解释的描述。我们通过高阶视觉区域进行了细粒度的体素级描述，并通过文本条件的图像合成验证了我们的方法，结果表明我们的图像在语义上是连贯的并且具有高的质量。"

    Understanding the functional organization of higher visual cortex is a central focus in neuroscience. Past studies have primarily mapped the visual and semantic selectivity of neural populations using hand-selected stimuli, which may potentially bias results towards pre-existing hypotheses of visual cortex functionality. Moving beyond conventional approaches, we introduce a data-driven method that generates natural language descriptions for images predicted to maximally activate individual voxels of interest. Our method -Semantic Captioning Using Brain Alignments ("BrainSCUBA") -- builds upon the rich embedding space learned by a contrastive vision-language model and utilizes a pre-trained large language model to generate interpretable captions. We validate our method through fine-grained voxel-level captioning across higher-order visual regions. We further perform text-conditioned image synthesis with the captions, and show that our images are semantically coherent and yield high pr
    
[^2]: 用于相对位置的函数插值改进了长上下文Transformer

    Functional Interpolation for Relative Positions Improves Long Context Transformers. (arXiv:2310.04418v1 [cs.LG])

    [http://arxiv.org/abs/2310.04418](http://arxiv.org/abs/2310.04418)

    这项研究提出了一种名为FIRE的函数相对位置编码与渐进插值方法，通过改进Transformer对更长上下文的泛化能力，并在零射击语言建模和长文本基准测试中进行了实证验证。

    

    在扩展这些模型的上下文长度时，防止Transformer在训练以外更长输入上性能下降一直是一个重要的挑战。虽然Transformer架构在可处理的输入序列长度上基本没有限制，但在训练过程中使用的位置编码的选择可能会限制这些模型在更长输入上的性能。我们提出了一种新颖的函数相对位置编码与渐进插值方法（FIRE），以改进Transformer对更长上下文的泛化能力。我们从理论上证明了这可以表示出一些流行的相对位置编码，如T5的RPE、Alibi和Kerple。接下来，我们在零射击语言建模和长文本基准测试上经验性地展示了FIRE模型在更长上下文中具有更好的泛化能力。

    Preventing the performance decay of Transformers on inputs longer than those used for training has been an important challenge in extending the context length of these models. Though the Transformer architecture has fundamentally no limits on the input sequence lengths it can process, the choice of position encoding used during training can limit the performance of these models on longer inputs. We propose a novel functional relative position encoding with progressive interpolation, FIRE, to improve Transformer generalization to longer contexts. We theoretically prove that this can represent some of the popular relative position encodings, such as T5's RPE, Alibi, and Kerple. We next empirically show that FIRE models have better generalization to longer contexts on both zero-shot language modeling and long text benchmarks.
    
[^3]: 扩散随机特征模型

    Diffusion Random Feature Model. (arXiv:2310.04417v1 [stat.ML])

    [http://arxiv.org/abs/2310.04417](http://arxiv.org/abs/2310.04417)

    本研究提出了一种以扩散模型为灵感的深度随机特征模型，它具有可解释性并可在数量相同的可训练参数下与全连接神经网络提供可比较的数值结果。通过推导得分匹配的属性，我们扩展了现有随机特征结果，并得出了样本数据分布与真实分布之间的泛化边界。

    

    扩散概率模型已成功用于生成从噪声中产生的数据。然而，大多数扩散模型计算成本高昂，难以解释，缺乏理论依据。另一方面，由于其可解释性，随机特征模型变得越来越受欢迎，但其在复杂机器学习任务中的应用仍然有限。在本工作中，我们提出了一种受扩散模型启发的深度随机特征模型，它既具有可解释性，又能给出与具有相同可训练参数数量的全连接神经网络相当的数值结果。具体而言，我们扩展了现有的随机特征结果，利用得分匹配的属性导出了样本数据分布与真实分布之间的泛化边界。我们通过在时尚MNIST数据集和乐器音频数据上生成样本来验证我们的发现。

    Diffusion probabilistic models have been successfully used to generate data from noise. However, most diffusion models are computationally expensive and difficult to interpret with a lack of theoretical justification. Random feature models on the other hand have gained popularity due to their interpretability but their application to complex machine learning tasks remains limited. In this work, we present a diffusion model-inspired deep random feature model that is interpretable and gives comparable numerical results to a fully connected neural network having the same number of trainable parameters. Specifically, we extend existing results for random features and derive generalization bounds between the distribution of sampled data and the true distribution using properties of score matching. We validate our findings by generating samples on the fashion MNIST dataset and instrumental audio data.
    
[^4]: 为什么现代深度学习中需要权重衰减？

    Why Do We Need Weight Decay in Modern Deep Learning?. (arXiv:2310.04415v1 [cs.LG])

    [http://arxiv.org/abs/2310.04415](http://arxiv.org/abs/2310.04415)

    权重衰减在现代深度学习中的作用与传统的学习理论中的正则化效果不同。对于过参数化的深度网络，它通过损失稳定机制改进了优化动态，对于欠参数化的大型语言模型，在随机优化中平衡偏差-方差权衡，导致较低的训练损失。此外，它还可以防止混合精度训练中的损失发散。

    

    权重衰减是一种广泛用于训练最先进的深度网络，包括大型语言模型的技术。尽管它被广泛使用，但其作用仍不太被了解。在这项工作中，我们强调权重衰减在现代深度学习中的作用与其在经典学习理论中研究的正则化效果不同。对于过参数化的深度网络，我们展示了权重衰减如何通过损失稳定机制改进了隐式正则化的优化动态。相反，对于用几乎在线SGD训练的欠参数化大型语言模型，我们描述了权重衰减如何在随机优化中平衡偏差-方差权衡，从而导致较低的训练损失。此外，我们还展示了权重衰减如何防止bfloat16混合精度训练中突然的损失发散，这是LLM训练的关键工具。总体而言，我们从ResNet对视觉任务到LLM提供了一个统一的视角：权重衰减。

    Weight decay is a broadly used technique for training state-of-the-art deep networks, including large language models. Despite its widespread usage, its role remains poorly understood. In this work, we highlight that the role of weight decay in modern deep learning is different from its regularization effect studied in classical learning theory. For overparameterized deep networks, we show how weight decay modifies the optimization dynamics enhancing the ever-present implicit regularization of SGD via the loss stabilization mechanism. In contrast, for underparameterized large language models trained with nearly online SGD, we describe how weight decay balances the bias-variance tradeoff in stochastic optimization leading to lower training loss. Moreover, we show that weight decay also prevents sudden loss divergences for bfloat16 mixed-precision training which is a crucial tool for LLM training. Overall, we present a unifying perspective from ResNets on vision tasks to LLMs: weight dec
    
[^5]: 超越均匀采样：使用不平衡数据集的离线强化学习

    Beyond Uniform Sampling: Offline Reinforcement Learning with Imbalanced Datasets. (arXiv:2310.04413v1 [cs.LG])

    [http://arxiv.org/abs/2310.04413](http://arxiv.org/abs/2310.04413)

    该论文提出了一种解决离线强化学习中不平衡数据集问题的方法，通过采样策略使策略只受``好数据"限制，而不是所有的动作，提高了离线RL算法的性能。

    

    离线策略学习旨在利用现有的轨迹数据集来学习决策策略，而无需收集额外的数据。与行为克隆等监督学习技术相比，使用强化学习（RL）的主要动机是找到一个比数据集中的轨迹达到更高平均收益的策略。然而，我们在经验上发现，当一个数据集被次优轨迹所主导时，当前最先进的离线RL算法在平均收益上没有显著提高。我们认为这是由于当前离线RL算法假设与数据集中的轨迹保持接近。如果数据集主要由次优轨迹组成，这个假设将强制策略模仿次优动作。我们通过提出一种采样策略来克服这个问题，使策略只受``好数据"限制，而不是所有的动作。

    Offline policy learning is aimed at learning decision-making policies using existing datasets of trajectories without collecting additional data. The primary motivation for using reinforcement learning (RL) instead of supervised learning techniques such as behavior cloning is to find a policy that achieves a higher average return than the trajectories constituting the dataset. However, we empirically find that when a dataset is dominated by suboptimal trajectories, state-of-the-art offline RL algorithms do not substantially improve over the average return of trajectories in the dataset. We argue this is due to an assumption made by current offline RL algorithms of staying close to the trajectories in the dataset. If the dataset primarily consists of sub-optimal trajectories, this assumption forces the policy to mimic the suboptimal actions. We overcome this issue by proposing a sampling strategy that enables the policy to only be constrained to ``good data" rather than all actions in t
    
[^6]: 理解、预测和更好地解决离线强化学习中的Q值分歧问题

    Understanding, Predicting and Better Resolving Q-Value Divergence in Offline-RL. (arXiv:2310.04411v1 [cs.LG])

    [http://arxiv.org/abs/2310.04411](http://arxiv.org/abs/2310.04411)

    这项研究通过理解Q值估计分歧的机制，提出了一种基于自激励特征值测量的解决方案，能够可靠地判断训练过程是否会出现分歧。

    

    Q值估计的分歧一直是离线强化学习中一个突出的问题，其中代理人无法获得真实动态信息。传统观点认为，当引导值目标时查询分布之外的动作是导致不稳定的原因。尽管可以通过策略约束或保守的Q值估计来缓解这个问题，但对导致分歧的机制的理论理解一直缺乏。在这项工作中，我们旨在深入了解这个机制并获得改进的解决方案。我们首先识别了自激励作为离线强化学习中Q值估计分歧的主要原因。然后，我们提出了一种基于神经切线核(NTK)的自激励特征值测量(SEEM)指标，用于测量训练过程中Q网络的演化性质，这提供了对分歧出现的有趣解释。首次，我们的理论能够可靠地决定训练是否在早期阶段发散。

    The divergence of the Q-value estimation has been a prominent issue in offline RL, where the agent has no access to real dynamics. Traditional beliefs attribute this instability to querying out-of-distribution actions when bootstrapping value targets. Though this issue can be alleviated with policy constraints or conservative Q estimation, a theoretical understanding of the underlying mechanism causing the divergence has been absent. In this work, we aim to thoroughly comprehend this mechanism and attain an improved solution. We first identify a fundamental pattern, self-excitation, as the primary cause of Q-value estimation divergence in offline RL. Then, we propose a novel Self-Excite Eigenvalue Measure (SEEM) metric based on Neural Tangent Kernel (NTK) to measure the evolving property of Q-network at training, which provides an intriguing explanation of the emergence of divergence. For the first time, our theory can reliably decide whether the training will diverge at an early stage
    
[^7]: 用于排序的语言模型的策略梯度训练

    Policy-Gradient Training of Language Models for Ranking. (arXiv:2310.04407v1 [cs.CL])

    [http://arxiv.org/abs/2310.04407](http://arxiv.org/abs/2310.04407)

    该论文提出了一种用于排序的语言模型的策略梯度训练算法Neural PG-RANK，通过将大规模语言模型实例化为Plackett-Luce排名策略，实现了对检索模型的原则性、端到端训练。

    

    文本检索在将事实知识纳入到语言处理流程中的决策过程中起着关键作用，从聊天式网页搜索到问答系统。当前最先进的文本检索模型利用预训练的大规模语言模型（LLM）以达到有竞争力的性能，但通过典型的对比损失训练基于LLM的检索器需要复杂的启发式算法，包括选择困难的负样本和使用额外的监督作为学习信号。这种依赖于启发式算法的原因是对比损失本身是启发式的，不能直接优化处理流程末端决策质量的下游指标。为了解决这个问题，我们引入了神经PG-RANK，一种新的训练算法，通过将LLM实例化为Plackett-Luce排名策略，学习排序。神经PG-RANK为检索模型的端到端训练提供了一种原则性方法，作为更大的决策系统的一部分进行训练。

    Text retrieval plays a crucial role in incorporating factual knowledge for decision making into language processing pipelines, ranging from chat-based web search to question answering systems. Current state-of-the-art text retrieval models leverage pre-trained large language models (LLMs) to achieve competitive performance, but training LLM-based retrievers via typical contrastive losses requires intricate heuristics, including selecting hard negatives and using additional supervision as learning signals. This reliance on heuristics stems from the fact that the contrastive loss itself is heuristic and does not directly optimize the downstream metrics of decision quality at the end of the processing pipeline. To address this issue, we introduce Neural PG-RANK, a novel training algorithm that learns to rank by instantiating a LLM as a Plackett-Luce ranking policy. Neural PG-RANK provides a principled method for end-to-end training of retrieval models as part of larger decision systems vi
    
[^8]: 语言代理树搜索统一了语言模型中的推理、行动和规划

    Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models. (arXiv:2310.04406v1 [cs.AI])

    [http://arxiv.org/abs/2310.04406](http://arxiv.org/abs/2310.04406)

    语言代理树搜索（LATS）是一个通用框架，利用大型语言模型（LLMs）的能力在规划、行动和推理方面相互协同，通过使用具有外部反馈的环境，实现更加深思熟虑和适应性的问题解决机制。实验评估表明，LATS在多个领域具有广泛的应用性，特别在编程方面表现出了94.4%的准确率。

    

    虽然大型语言模型（LLMs）在一系列决策任务上表现出了令人印象深刻的性能，但它们依赖于简单的行动过程，并未能广泛部署作为自主代理。我们引入了LATS（语言代理树搜索），这是一个通用框架，将LLMs在规划、行动和推理方面的能力相互协同。LATS借鉴了模型导向的强化学习中的蒙特卡洛树搜索的思想，将LLMs用作代理、价值函数和优化器，重新利用其潜在的优势以提升决策能力。关键的一点是LATS使用一个具有外部反馈的环境，这提供了一种更加深思熟虑和适应性的问题解决机制，超越了现有技术的局限性。我们在编程、HotPotQA和WebShop等多个领域进行了实验评估，证明了LATS在推理和行动方面的适用性。特别是，在编程方面，LATS实现了94.4%的准确率。

    While large language models (LLMs) have demonstrated impressive performance on a range of decision-making tasks, they rely on simple acting processes and fall short of broad deployment as autonomous agents. We introduce LATS (Language Agent Tree Search), a general framework that synergizes the capabilities of LLMs in planning, acting, and reasoning. Drawing inspiration from Monte Carlo tree search in model-based reinforcement learning, LATS employs LLMs as agents, value functions, and optimizers, repurposing their latent strengths for enhanced decision-making. What is crucial in this method is the use of an environment for external feedback, which offers a more deliberate and adaptive problem-solving mechanism that moves beyond the limitations of existing techniques. Our experimental evaluation across diverse domains, such as programming, HotPotQA, and WebShop, illustrates the applicability of LATS for both reasoning and acting. In particular, LATS achieves 94.4\% for programming on Hu
    
[^9]: 论可扩展推荐模型中嵌入坍缩现象的研究

    On the Embedding Collapse when Scaling up Recommendation Models. (arXiv:2310.04400v1 [cs.LG])

    [http://arxiv.org/abs/2310.04400](http://arxiv.org/abs/2310.04400)

    研究了可缩放推荐模型中嵌入层的崩溃现象，发现特征交互模块在一定程度上限制了嵌入学习，但也是提高可扩展性的关键因素。

    

    深度基础模型的最新进展引发了开发大型推荐模型以利用大量可用数据的有前景趋势。然而，我们试验放大现有的推荐模型时发现，扩大的模型并没有令人满意的改进。在这种情况下，我们研究了扩大模型的嵌入层，并发现了一种嵌入坍缩现象，这最终阻碍了可扩展性，在这种现象中，嵌入矩阵倾向于存在于低维子空间中。通过实证和理论分析，我们证明了推荐模型特定的特征交互模块具有双重作用。一方面，当与坍缩的嵌入交互时，该交互限制了嵌入学习，加剧了崩溃问题。另一方面，特征交互对于缓解假特征的拟合至关重要，从而提高可扩展性。基于这一分析，我们提出了一个简单而有效的方法

    Recent advances in deep foundation models have led to a promising trend of developing large recommendation models to leverage vast amounts of available data. However, we experiment to scale up existing recommendation models and observe that the enlarged models do not improve satisfactorily. In this context, we investigate the embedding layers of enlarged models and identify a phenomenon of embedding collapse, which ultimately hinders scalability, wherein the embedding matrix tends to reside in a low-dimensional subspace. Through empirical and theoretical analysis, we demonstrate that the feature interaction module specific to recommendation models has a two-sided effect. On the one hand, the interaction restricts embedding learning when interacting with collapsed embeddings, exacerbating the collapse issue. On the other hand, feature interaction is crucial in mitigating the fitting of spurious features, thereby improving scalability. Based on this analysis, we propose a simple yet effe
    
[^10]: 利用自一致性提高数据有效的摊余贝叶斯推理方法

    Leveraging Self-Consistency for Data-Efficient Amortized Bayesian Inference. (arXiv:2310.04395v1 [cs.LG])

    [http://arxiv.org/abs/2310.04395](http://arxiv.org/abs/2310.04395)

    该论文提出了一种利用自一致性改进数据有效的摊余贝叶斯推理方法，通过反转贝叶斯定理并利用近似表示的联合模型估计边际似然，加速条件神经密度估计器的学习动力学。

    

    我们提出了一种方法，通过利用参数$\theta$和数据$y$的概率联合模型$p(\theta, y)$中的通用对称性，改进了摊余贝叶斯推理（ABI）的效率和准确性。简言之，我们反转贝叶斯定理，并基于近似表示的联合模型估计边际似然。在完美近似情况下，边际似然在所有参数值上都是常数定义的。然而，近似误差导致不同参数值的边际似然估计中存在不可取的方差。我们将这种对称性的违反形式化为损失函数，加速条件神经密度估计器的学习动力学。我们将我们的方法应用于具有显式似然（基于似然）的双峰玩具问题和具有隐式似然（基于模拟）的现实模型。

    We propose a method to improve the efficiency and accuracy of amortized Bayesian inference (ABI) by leveraging universal symmetries in the probabilistic joint model $p(\theta, y)$ of parameters $\theta$ and data $y$. In a nutshell, we invert Bayes' theorem and estimate the marginal likelihood based on approximate representations of the joint model. Upon perfect approximation, the marginal likelihood is constant across all parameter values by definition. However, approximation error leads to undesirable variance in the marginal likelihood estimates across different parameter values. We formulate violations of this symmetry as a loss function to accelerate the learning dynamics of conditional neural density estimators. We apply our method to a bimodal toy problem with an explicit likelihood (likelihood-based) and a realistic model with an implicit likelihood (simulation-based).
    
[^11]: 潜在一致性模型：使用少量步骤的推理合成高分辨率图像

    Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference. (arXiv:2310.04378v1 [cs.CV])

    [http://arxiv.org/abs/2310.04378](http://arxiv.org/abs/2310.04378)

    该论文提出了潜在一致性模型（LCMs），通过直接预测潜在空间中扩散过程的解，实现了高分辨率图像的快速合成，减少了计算量和时间成本。

    

    潜在扩散模型（LDM）在合成高分辨率图像方面取得了显著的成果。然而，迭代采样过程计算密集、生成速度慢。受到一致性模型的启发，我们提出了潜在一致性模型（LCM），在任何预训练的LDM上实现了快速推理，仅需最少的步骤，包括稳定扩散模型。将引导逆扩散过程视为解决增广概率流ODE（PF-ODE），LCM被设计为直接预测潜在空间中该ODE的解，减少了多次迭代的需求，实现了快速、高保真的采样。有效地从预训练的无分类器引导扩散模型中提取，一个高质量的768 x 768的2~4步LCM仅需32个A100 GPU小时进行训练。此外，我们引入了潜在一致性微调（LCF），一种针对自定义图像数据集精心设计的LCM微调方法。

    Latent Diffusion models (LDMs) have achieved remarkable results in synthesizing high-resolution images. However, the iterative sampling process is computationally intensive and leads to slow generation. Inspired by Consistency Models (song et al.), we propose Latent Consistency Models (LCMs), enabling swift inference with minimal steps on any pre-trained LDMs, including Stable Diffusion (rombach et al). Viewing the guided reverse diffusion process as solving an augmented probability flow ODE (PF-ODE), LCMs are designed to directly predict the solution of such ODE in latent space, mitigating the need for numerous iterations and allowing rapid, high-fidelity sampling. Efficiently distilled from pre-trained classifier-free guided diffusion models, a high-quality 768 x 768 2~4-step LCM takes only 32 A100 GPU hours for training. Furthermore, we introduce Latent Consistency Fine-tuning (LCF), a novel method that is tailored for fine-tuning LCMs on customized image datasets. Evaluation on the
    
[^12]: 用约束强化学习对抗奖励模型过度优化

    Confronting Reward Model Overoptimization with Constrained RLHF. (arXiv:2310.04373v1 [cs.LG])

    [http://arxiv.org/abs/2310.04373](http://arxiv.org/abs/2310.04373)

    本研究首次研究了组合奖励模型中的过度优化问题，发现组成奖励模型之间的相关性对问题的解决方式有重要影响。我们提出了一种使用约束强化学习来解决这个问题的方法。

    

    大型语言模型通常通过优化适应人类反馈的奖励模型来与人类偏好保持一致。然而，人类偏好是多方面的，越来越常见的做法是从几个简单的奖励模型中派生出奖励，每个模型捕捉语言质量的不同方面。然而，当组合这些组成的奖励模型时，适当地加权变得困难。更加困难的是，由于任何奖励模型只是人类评价的代理，这一过程容易受到过度优化的影响，即超过某一点后，获得更高奖励与更差的人类评价相关。本文通过对组合奖励模型中过度优化进行研究，首次展示了组成奖励模型之间的相关性对这些点的位置有显著影响。然后，我们介绍了一种使用约束强化学习来解决这个问题的方法。

    Large language models are typically aligned with human preferences by optimizing $\textit{reward models}$ (RMs) fitted to human feedback. However, human preferences are multi-faceted, and it is increasingly common to derive reward from a composition of simpler reward models which each capture a different aspect of language quality. This itself presents a challenge, as it is difficult to appropriately weight these component RMs when combining them. Compounding this difficulty, because any RM is only a proxy for human evaluation, this process is vulnerable to $\textit{overoptimization}$, wherein past a certain point, accumulating higher reward is associated with worse human ratings. In this paper, we perform, to our knowledge, the first study on overoptimization in composite RMs, showing that correlation between component RMs has a significant effect on the locations of these points. We then introduce an approach to solve this issue using constrained reinforcement learning as a means of 
    
[^13]: MBTFNet：用于歌声增强的多频带时间频率神经网络

    MBTFNet: Multi-Band Temporal-Frequency Neural Network For Singing Voice Enhancement. (arXiv:2310.04369v1 [cs.SD])

    [http://arxiv.org/abs/2310.04369](http://arxiv.org/abs/2310.04369)

    提出了一种新颖的多频带时间频率神经网络（MBTFNet）用于歌声增强，可以从歌唱录音中去除背景音乐、噪声甚至和声。实验证明，该模型优于多个最先进的音频增强和音乐源分离模型。

    

    传统的神经音频增强方法主要处理语音和噪声混合的情况，这对于歌声增强场景并不是最优的。音乐源分离模型将人声和伴奏组件视作同等重要，这可能会降低性能，相对于仅考虑人声增强的模型。本文提出了一种新颖的多频带时间频率神经网络（MBTFNet），用于歌声增强，可以从歌唱录音中去除背景音乐、噪声甚至和声。MBTFNet结合了带间和带内建模，以更好地处理全频带信号。引入了双通道建模来扩大模型的感受野。我们提出了一种基于信噪比（SNR）估计的隐式个性化增强（IPE）阶段，进一步提高了MBTFNet的性能。实验证明，我们提出的模型明显优于多个最先进的音频增强和音乐源分离模型。

    A typical neural speech enhancement (SE) approach mainly handles speech and noise mixtures, which is not optimal for singing voice enhancement scenarios. Music source separation (MSS) models treat vocals and various accompaniment components equally, which may reduce performance compared to the model that only considers vocal enhancement. In this paper, we propose a novel multi-band temporal-frequency neural network (MBTFNet) for singing voice enhancement, which particularly removes background music, noise and even backing vocals from singing recordings. MBTFNet combines inter and intra-band modeling for better processing of full-band signals. Dual-path modeling are introduced to expand the receptive field of the model. We propose an implicit personalized enhancement (IPE) stage based on signal-to-noise ratio (SNR) estimation, which further improves the performance of MBTFNet. Experiments show that our proposed model significantly outperforms several state-of-the-art SE and MSS models.
    
[^14]: 一个大规模市场价格异常检测系统

    A Marketplace Price Anomaly Detection System at Scale. (arXiv:2310.04367v1 [stat.ML])

    [http://arxiv.org/abs/2310.04367](http://arxiv.org/abs/2310.04367)

    MoatPlus是一个可扩展的价格异常检测框架，通过利用无监督统计特征和历史价格趋势生成上限价格边界，以解决在线市场中的数据质量和错误价格发布的问题。

    

    在线市场每天在平台上执行大量的价格更新，这些更新由个体市场卖家发起。这种价格民主化随着数据质量的挑战而增加。相对于传统的在线零售商，缺乏集中的防护措施会导致更高的错误价格在网站上发布，从而给顾客体验带来差评和潜在的收入损失。我们提出了MoatPlus（使用树、基于邻近度的标签以及无监督统计特征的蒙面最优锚点），这是一个用于不断增长的市场平台的可扩展价格异常检测框架。目标是利用邻近度和历史价格趋势的无监督统计特征来生成上限价格边界。我们构建了一个模型集合来检测基于价格的特征中的异常情况，排除异常特征，并使用优化的加权方案来构建实时定价管道中可靠的价格边界。

    Online marketplaces execute large volume of price updates that are initiated by individual marketplace sellers each day on the platform. This price democratization comes with increasing challenges with data quality. Lack of centralized guardrails that are available for a traditional online retailer causes a higher likelihood for inaccurate prices to get published on the website, leading to poor customer experience and potential for revenue loss. We present MoatPlus (Masked Optimal Anchors using Trees, Proximity-based Labeling and Unsupervised Statistical-features), a scalable price anomaly detection framework for a growing marketplace platform. The goal is to leverage proximity and historical price trends from unsupervised statistical features to generate an upper price bound. We build an ensemble of models to detect irregularities in price-based features, exclude irregular features and use optimized weighting scheme to build a reliable price bound in real-time pricing pipeline. We obs
    
[^15]: 在大规模语言模型中摊销难以处理的推理问题

    Amortizing intractable inference in large language models. (arXiv:2310.04363v1 [cs.LG])

    [http://arxiv.org/abs/2310.04363](http://arxiv.org/abs/2310.04363)

    本论文提出了一种使用摊销的贝叶斯推理从难以处理的后验分布中进行抽样的方法，并利用生成流网络来实现大规模语言模型的微调，从而解决了在这些模型中处理推理问题的限制。

    

    自回归的大规模语言模型通过下一个词条件分布来压缩其训练数据中的知识，这限制了对该知识的可处理查询仅限于从头到尾的自回归抽样。然而，许多感兴趣的任务，包括序列延续、填充和其他形式的受约束生成，都涉及从难以处理的后验分布中进行抽样。我们通过使用摊销的贝叶斯推理从这些难以处理的后验分布中进行抽样来解决这个限制。这种摊销通过通过寻求多样性的强化学习算法 - 生成流网络 (GFlowNets) 来微调 LLMs 实现。我们凭经验证明，LLM微调的这种分布匹配范式可以作为最大似然训练和奖励最大化策略优化的有效替代方法。作为一个重要应用，我们将思维链推理解释为潜变量建模问题，并证明了...

    Autoregressive large language models (LLMs) compress knowledge from their training data through next-token conditional distributions. This limits tractable querying of this knowledge to start-to-end autoregressive sampling. However, many tasks of interest -- including sequence continuation, infilling, and other forms of constrained generation -- involve sampling from intractable posterior distributions. We address this limitation by using amortized Bayesian inference to sample from these intractable posteriors. Such amortization is algorithmically achieved by fine-tuning LLMs via diversity-seeking reinforcement learning algorithms: generative flow networks (GFlowNets). We empirically demonstrate that this distribution-matching paradigm of LLM fine-tuning can serve as an effective alternative to maximum-likelihood training and reward-maximizing policy optimization. As an important application, we interpret chain-of-thought reasoning as a latent variable modeling problem and demonstrate 
    
[^16]: 利用动态推理来利用Transformer激活稀疏性

    Exploiting Transformer Activation Sparsity with Dynamic Inference. (arXiv:2310.04361v1 [cs.LG])

    [http://arxiv.org/abs/2310.04361](http://arxiv.org/abs/2310.04361)

    本文提出了一种名为DSTI的方法，通过强制激活稀疏性并将Transformer模型转换为稀疏的专家混合版本来极大地降低推理成本。此方法可以应用于任何Transformer模型，并对准确性影响微乎其微。

    

    Transformer模型尽管表现出色，但由于其高计算需求，常面临实际限制。与此同时，先前的研究揭示了这些模型中的显著激活稀疏性，表明存在冗余计算。本文提出了一种称为动态稀疏化Transformer推理（DSTI）的方法，通过强制激活稀疏性并将密集模型转换为其稀疏的专家混合（MoE）版本，从而极大地降低Transformer模型的推理成本。我们证明，在推理过程中可以训练出成功预测每个专家相对贡献的小型门控网络。此外，我们引入了一种动态确定每个令牌执行的专家数量的机制。DSTI可以应用于任何基于Transformer的体系结构，并对准确性影响微乎其微。针对BERT-base分类模型，我们降低了推理成本。

    Transformer models, despite their impressive performance, often face practical limitations due to their high computational requirements. At the same time, previous studies have revealed significant activation sparsity in these models, indicating the presence of redundant computations. In this paper, we propose Dynamic Sparsified Transformer Inference (DSTI), a method that radically reduces the inference cost of Transformer models by enforcing activation sparsity and subsequently transforming a dense model into its sparse Mixture of Experts (MoE) version. We demonstrate that it is possible to train small gating networks that successfully predict the relative contribution of each expert during inference. Furthermore, we introduce a mechanism that dynamically determines the number of executed experts individually for each token. DSTI can be applied to any Transformer-based architecture and has negligible impact on the accuracy. For the BERT-base classification model, we reduce inference c
    
[^17]: 在概率电路中整合变换

    Integrating Transformations in Probabilistic Circuits. (arXiv:2310.04354v1 [stat.ML])

    [http://arxiv.org/abs/2310.04354](http://arxiv.org/abs/2310.04354)

    本研究通过引入变换作为解决概率电路的预测限制问题的方法，在机器人场景中展示了该限制问题，并证明了所提出的方法能够在使用较少的参数的情况下实现更高的似然概率。此外，还讨论了如何将变换整合到基于树的学习过程中，并指出了精确推理的不可行性。

    

    本研究解决了概率电路的预测限制问题，并引入了变换作为克服该问题的方法。我们在机器人场景中证明了这种限制。我们认为独立分量分析是保持概率电路独立性属性的有效工具。我们的方法是联合概率树的扩展，它们是无模型确定性电路。通过这样做，我们证明了所提出的方法能够在使用较少的参数的情况下，在七个基准数据集以及真实机器人数据上实现更高的似然概率。此外，我们讨论了如何将变换整合到基于树的学习过程中。最后，我们认为使用转换后的分位参数化分布进行精确推理是不可行的。然而，我们的方法允许进行高效的采样和近似推理。

    This study addresses the predictive limitation of probabilistic circuits and introduces transformations as a remedy to overcome it. We demonstrate this limitation in robotic scenarios. We motivate that independent component analysis is a sound tool to preserve the independence properties of probabilistic circuits. Our approach is an extension of joint probability trees, which are model-free deterministic circuits. By doing so, it is demonstrated that the proposed approach is able to achieve higher likelihoods while using fewer parameters compared to the joint probability trees on seven benchmark data sets as well as on real robot data. Furthermore, we discuss how to integrate transformations into tree-based learning routines. Finally, we argue that exact inference with transformed quantile parameterized distributions is not tractable. However, our approach allows for efficient sampling and approximate inference.
    
[^18]: 一种面向形式定理证明的语言代理方法

    A Language-Agent Approach to Formal Theorem-Proving. (arXiv:2310.04353v1 [cs.LG])

    [http://arxiv.org/abs/2310.04353](http://arxiv.org/abs/2310.04353)

    COPRA是一种面向形式定理证明的语言代理方法，利用大型语言模型进行上下文学习，通过选择策略和检索定义和引理进行证明，在MiniF2F基准和Coq任务上表现出优异的性能。

    

    语言代理是利用大型语言模型（LLM）进行上下文学习来与外部环境进行交互的方法，最近被认为是一种有前景的控制任务方法。

    Language agents, which use a large language model (LLM) capable of in-context learning to interact with an external environment, have recently emerged as a promising approach to control tasks. We present the first language-agent approach to formal theorem-proving. Our method, COPRA, uses a high-capacity, black-box LLM (GPT-4) as part of a policy for a stateful backtracking search. During the search, the policy can select proof tactics and retrieve lemmas and definitions from an external database. Each selected tactic is executed in the underlying proof framework, and the execution feedback is used to build the prompt for the next policy invocation. The search also tracks selected information from its history and uses it to reduce hallucinations and unnecessary LLM queries.  We evaluate COPRA on the miniF2F benchmark for Lean and a set of Coq tasks from the Compcert project. On these benchmarks, COPRA is significantly better than one-shot invocations of GPT-4, as well as state-of-the-ar
    
[^19]: 用于解释基于树模型和替代模型的公平特征重要性评分

    Fair Feature Importance Scores for Interpreting Tree-Based Methods and Surrogates. (arXiv:2310.04352v1 [stat.ML])

    [http://arxiv.org/abs/2310.04352](http://arxiv.org/abs/2310.04352)

    本文关注解释机器学习模型中公平性的方面，发展了一种基于决策树的特征重要性评分方法。

    

    在医疗保健、刑事司法、国家安全、金融和技术等各个领域，大规模的机器学习(ML)和人工智能(AI)系统被部署用于进行关键的数据驱动决策。许多人想知道我们是否可以信任这些ML系统进行这些决策。对于信任ML系统来说，两个关键组成部分是必备的：可解释性，即能够理解ML系统为什么做出这样的决策；公平性，确保ML系统不对某些个体或群体存在偏见。可解释性和公平性都很重要，并在ML文献中分别得到了大量关注，但到目前为止，很少有方法直接解释模型的公平性。在本文中，我们着重讨论可能是最流行的ML解释类型之一：特征重要性评分。受到在知识蒸馏中使用决策树的启发，

    Across various sectors such as healthcare, criminal justice, national security, finance, and technology, large-scale machine learning (ML) and artificial intelligence (AI) systems are being deployed to make critical data-driven decisions. Many have asked if we can and should trust these ML systems to be making these decisions. Two critical components are prerequisites for trust in ML systems: interpretability, or the ability to understand why the ML system makes the decisions it does, and fairness, which ensures that ML systems do not exhibit bias against certain individuals or groups. Both interpretability and fairness are important and have separately received abundant attention in the ML literature, but so far, there have been very few methods developed to directly interpret models with regard to their fairness. In this paper, we focus on arguably the most popular type of ML interpretation: feature importance scores. Inspired by the use of decision trees in knowledge distillation, w
    
[^20]: 从某处到任何地方的学习：抓取技术

    Learning to Grasp: from Somewhere to Anywhere. (arXiv:2310.04349v1 [cs.RO])

    [http://arxiv.org/abs/2310.04349](http://arxiv.org/abs/2310.04349)

    本研究介绍了一个采用质量-多样性方法的流水线，用于将生成的抓取轨迹适应到新物体姿态。使用RGB-D数据流，该方法能够自动检测目标物体、预测其姿态，并生成可达到的抓取轨迹。

    

    机器人抓取仍然是一个部分解决的、多学科的问题，在其中数据驱动的技术发挥着越来越重要的作用。奖励的稀疏性使得自动生成抓取数据集变得具有挑战性，尤其是对于非传统形态或高度驱动的末端执行器。获得大规模数据集的大多数方法依赖于众多人工提供的演示或严重工程化的解决方案，这些方法很难扩展。最新的质量-多样性（QD）方法的进展研究了如何使用不同机器人形态学习特定姿势下的物体抓取。本研究介绍了一个将QD生成的轨迹适应到新物体姿态的流水线。使用RGB-D数据流，视觉流水线首先检测目标物体，预测其6自由度姿态，最后跟踪它。然后通过将轨迹相对于物体框架进行投影来自动生成可达到的抓取轨迹。数百个轨迹已经在实验中部署。

    Robotic grasping is still a partially solved, multidisciplinary problem where data-driven techniques play an increasing role. The sparse nature of rewards make the automatic generation of grasping datasets challenging, especially for unconventional morphologies or highly actuated end-effectors. Most approaches for obtaining large-scale datasets rely on numerous human-provided demonstrations or heavily engineered solutions that do not scale well. Recent advances in Quality-Diversity (QD) methods have investigated how to learn object grasping at a specific pose with different robot morphologies. The present work introduces a pipeline for adapting QD-generated trajectories to new object poses. Using an RGB-D data stream, the vision pipeline first detects the targeted object, predicts its 6-DOF pose, and finally tracks it. An automatically generated reach-and-grasp trajectory can then be adapted by projecting it relatively to the object frame. Hundreds of trajectories have been deployed in
    
[^21]: Neur2RO: 神经二阶段鲁棒优化

    Neur2RO: Neural Two-Stage Robust Optimization. (arXiv:2310.04345v1 [math.OC])

    [http://arxiv.org/abs/2310.04345](http://arxiv.org/abs/2310.04345)

    Neur2RO是一种神经网络驱动的二阶段鲁棒优化算法，通过学习估计第二阶段问题的值函数，并嵌入到经典的列-约束生成算法中，能够高效地求解嵌套的最小-最大-最小优化问题。

    

    鲁棒优化提供了一个数学框架，用于在最坏情况下的不确定性下建模和解决决策问题。本工作解决了二阶段鲁棒优化（也称为可调整鲁棒优化）问题，在不确定性实现之前和之后进行第一阶段和第二阶段的决策。这导致了一个嵌套的最小-最大-最小优化问题，从计算上来说是非常具有挑战性的，尤其是当决策是离散的时候。我们提出了Neur2RO，这是一种高效的基于机器学习的列-约束生成（CCG）的实例算法，CCG是二阶段鲁棒优化的经典迭代算法。具体而言，我们通过一种新颖的神经网络架构来学习估计第二阶段问题的值函数，这种架构易于优化。将我们的神经网络嵌入到CCG算法中，可以快速得到高质量的解，这在两个二阶段鲁棒优化基准测试（背包问题和资本预算）的实验证明了。

    Robust optimization provides a mathematical framework for modeling and solving decision-making problems under worst-case uncertainty. This work addresses two-stage robust optimization (2RO) problems (also called adjustable robust optimization), wherein first-stage and second-stage decisions are made before and after uncertainty is realized, respectively. This results in a nested min-max-min optimization problem which is extremely challenging computationally, especially when the decisions are discrete. We propose Neur2RO, an efficient machine learning-driven instantiation of column-and-constraint generation (CCG), a classical iterative algorithm for 2RO. Specifically, we learn to estimate the value function of the second-stage problem via a novel neural network architecture that is easy to optimize over by design. Embedding our neural network into CCG yields high-quality solutions quickly as evidenced by experiments on two 2RO benchmarks, knapsack and capital budgeting. For knapsack, Ne
    
[^22]: 函数几何引导的蛋白质序列和骨架结构共同设计

    Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design. (arXiv:2310.04343v1 [cs.LG])

    [http://arxiv.org/abs/2310.04343](http://arxiv.org/abs/2310.04343)

    本研究提出了NAEPro模型，通过函数几何引导的方法共同设计蛋白质序列和结构。实验结果表明，该模型在氨基酸恢复率、TM分数和RMSD等指标上表现出色。

    

    蛋白质是几乎所有生物体中负责基本功能的大分子。设计合理的具有期望功能的蛋白质至关重要。蛋白质的序列和结构密切相关，它们共同决定了其功能。在本文中，我们提出了NAEPro，一种基于自动检测到的功能位点共同设计蛋白质序列和结构的模型。NAEPro采用了注意力和等变层的交错网络，可以捕捉序列中的全局相关性以及三维空间中最近氨基酸的局部影响。这种架构在两个层面上促进了有效而经济的信息传递。我们在两个蛋白质数据集（β-内酰胺酶和肌红蛋白）上评估了我们的模型和几个强竞争基线。实验结果表明，我们的模型在所有竞争对手中始终实现了最高的氨基酸恢复率、TM分数和最低的RMSD。这些发现证明了该模型的能力。

    Proteins are macromolecules responsible for essential functions in almost all living organisms. Designing reasonable proteins with desired functions is crucial. A protein's sequence and structure are strongly correlated and they together determine its function. In this paper, we propose NAEPro, a model to jointly design Protein sequence and structure based on automatically detected functional sites. NAEPro is powered by an interleaving network of attention and equivariant layers, which can capture global correlation in a whole sequence and local influence from nearest amino acids in three dimensional (3D) space. Such an architecture facilitates effective yet economic message passing at two levels. We evaluate our model and several strong baselines on two protein datasets, $\beta$-lactamase and myoglobin. Experimental results show that our model consistently achieves the highest amino acid recovery rate, TM-score, and the lowest RMSD among all competitors. These findings prove the capab
    
[^23]: 将强化学习应用于期权定价和对冲的研究

    Applying Reinforcement Learning to Option Pricing and Hedging. (arXiv:2310.04336v1 [q-fin.CP])

    [http://arxiv.org/abs/2310.04336](http://arxiv.org/abs/2310.04336)

    本论文研究了将强化学习应用于期权定价和对冲的方法，并介绍了Q学习Black Scholes方法。该方法能够以非模型和数据驱动的方式进行期权定价和对冲，并在不同状态变量和场景下表现出稳健的性能。

    

    本论文概述了近年来在金融产品定价和对冲中应用强化学习的最新进展，重点介绍了由Halperin（2017）提出的Q学习Black Scholes方法的详细解释。这种强化学习方法将传统的Black and Scholes（1973）模型与新颖的人工智能算法相结合，以完全的非模型和数据驱动的方式进行期权定价和对冲。本文还研究了该算法在不同状态变量和场景下对欧式看跌期权的表现。结果表明，该模型在不同波动率和对冲频率水平下具有较高的准确性。此外，该方法在不同期权平价点的水平上表现出稳健的性能。最后，该算法包括比例交易成本，表明不同状态变量的统计特性对收益和损失产生不同的影响。

    This thesis provides an overview of the recent advances in reinforcement learning in pricing and hedging financial instruments, with a primary focus on a detailed explanation of the Q-Learning Black Scholes approach, introduced by Halperin (2017). This reinforcement learning approach bridges the traditional Black and Scholes (1973) model with novel artificial intelligence algorithms, enabling option pricing and hedging in a completely model-free and data-driven way. This paper also explores the algorithm's performance under different state variables and scenarios for a European put option. The results reveal that the model is an accurate estimator under different levels of volatility and hedging frequency. Moreover, this method exhibits robust performance across various levels of option's moneyness. Lastly, the algorithm incorporates proportional transaction costs, indicating diverse impacts on profit and loss, affected by different statistical properties of the state variables.
    
[^24]: 基于显著性引导的隐含关联回放用于持续学习

    Saliency-Guided Hidden Associative Replay for Continual Learning. (arXiv:2310.04334v1 [cs.LG])

    [http://arxiv.org/abs/2310.04334](http://arxiv.org/abs/2310.04334)

    该论文提出了一种基于显著性引导的隐含关联回放方法，用于解决持续学习中的灾难性遗忘问题。这种方法通过存储和检索重要的数据部分，模仿了人类记忆过程，与传统的完整数据存储方法不同。

    

    持续学习是下一代人工智能中一个新兴的领域，专注于训练神经网络以类似于人类学习的任务序列。虽然持续学习相比传统的监督学习具有优势，但其主要挑战仍然是抵抗灾难性遗忘并确保在后续学习过程中保留先前任务的相关知识。在各种解决方法中，基于回放的方法已经成为卓越的方法，模仿了生物记忆机制。然而，这些方法在内存使用方面往往是密集的，通常保留整个数据样本，这种方法与人类选择性记忆保留显著经验的方式不一致。虽然最近的一些研究探索了只在情景记忆中存储重要数据部分的方法，但部分数据的固有性质需要创新的检索机制。当前的解决方案，如图像修复，试图通过部分线索来近似完整数据重建，这种方法与真实的人类记忆过程不一致。

    Continual Learning is a burgeoning domain in next-generation AI, focusing on training neural networks over a sequence of tasks akin to human learning. While CL provides an edge over traditional supervised learning, its central challenge remains to counteract catastrophic forgetting and ensure the retention of prior tasks during subsequent learning. Amongst various strategies to tackle this, replay based methods have emerged as preeminent, echoing biological memory mechanisms. However, these methods are memory intensive, often preserving entire data samples, an approach inconsistent with humans selective memory retention of salient experiences. While some recent works have explored the storage of only significant portions of data in episodic memory, the inherent nature of partial data necessitates innovative retrieval mechanisms. Current solutions, like inpainting, approximate full data reconstruction from partial cues, a method that diverges from genuine human memory processes. Address
    
[^25]: 决策导向学习的鲁棒损失函数

    Robust Losses for Decision-Focused Learning. (arXiv:2310.04328v1 [cs.LG])

    [http://arxiv.org/abs/2310.04328](http://arxiv.org/abs/2310.04328)

    这篇论文提出了一种改进的损失函数来处理决策导向学习中的不确定性问题，并对经验遗憾作为替代的准确性进行了分析。

    

    用于进行离散决策的优化模型通常包含基于预测估计的上下文相关且不确定的参数。为了考虑基于预测的决策质量，决策导向学习（端到端预测-优化）旨在训练预测模型以最小化遗憾，即通过进行次优决策而产生的损失。尽管这个损失函数可能是非凸和一般不可导的，但已经提出了有效的基于梯度的学习方法来最小化期望损失，使用经验损失作为替代。然而，经验遗憾可能不是一个有效的替代，因为优化模型中的不确定性导致经验遗憾与期望遗憾在期望上不相等。为了说明这种不等式的影响，我们评估了机会性和认知性不确定性对经验遗憾作为替代的准确性的影响。接下来，我们提出了一种改进的损失函数，该函数能够更好地近似期望遗憾，从而更好地指导决策导向学习的训练过程。

    Optimization models used to make discrete decisions often contain uncertain parameters that are context-dependent and are estimated through prediction. To account for the quality of the decision made based on the prediction, decision-focused learning (end-to-end predict-then-optimize) aims at training the predictive model to minimize regret, i.e., the loss incurred by making a suboptimal decision. Despite the challenge of this loss function being possibly non-convex and in general non-differentiable, effective gradient-based learning approaches have been proposed to minimize the expected loss, using the empirical loss as a surrogate. However, empirical regret can be an ineffective surrogate because the uncertainty in the optimization model makes the empirical regret unequal to the expected regret in expectation. To illustrate the impact of this inequality, we evaluate the effect of aleatoric and epistemic uncertainty on the accuracy of empirical regret as a surrogate. Next, we propose 
    
[^26]: 基于最佳优先自底向上搜索的程序合成

    Program Synthesis with Best-First Bottom-Up Search. (arXiv:2310.04327v1 [cs.LG])

    [http://arxiv.org/abs/2310.04327](http://arxiv.org/abs/2310.04327)

    本文介绍了一种新颖的最佳优先自底向上搜索算法，称为Bee Search，该算法能够以最佳优先的方式执行成本导向自底向上合成，而且不会丢失模型提供的有用信息。同时，作者还引入了一个更好使用现有成本模型提供的信息的新成本函数。

    

    领先的成本导向自底向上搜索算法使用成本函数来指导搜索程序合成任务。本文表明，目前最先进的成本导向自底向上搜索算法存在一个共同的问题：它们可能会丢失模型提供的有用信息，并且无法按照成本函数的最佳优先顺序进行搜索。我们引入了一种新颖的最佳优先自底向上搜索算法，称为Bee Search，它不会丢失信息，并能够以最佳优先的方式执行成本导向自底向上合成。重要的是，Bee Search以相对于生成程序的最佳优先顺序执行最佳优先搜索，即它不会在内存中创建比解决方案程序更昂贵的程序。它通过在程序成本的抽象空间中进行搜索来实现生成的最佳优先顺序。我们还引入了一个更好使用现有成本模型提供的信息的新成本函数。

    Cost-guided bottom-up search (BUS) algorithms use a cost function to guide the search to solve program synthesis tasks. In this paper, we show that current state-of-the-art cost-guided BUS algorithms suffer from a common problem: they can lose useful information given by the model and fail to perform the search in a best-first order according to a cost function. We introduce a novel best-first bottom-up search algorithm, which we call Bee Search, that does not suffer information loss and is able to perform cost-guided bottom-up synthesis in a best-first manner. Importantly, Bee Search performs best-first search with respect to the generation of programs, i.e., it does not even create in memory programs that are more expensive than the solution program. It attains best-first ordering with respect to generation by performing a search in an abstract space of program costs. We also introduce a new cost function that better uses the information provided by an existing cost model. Empirical 
    
[^27]: 可调节的鲁棒性强化学习在在线3D装箱问题中的应用

    Adjustable Robust Reinforcement Learning for Online 3D Bin Packing. (arXiv:2310.04323v1 [cs.LG])

    [http://arxiv.org/abs/2310.04323](http://arxiv.org/abs/2310.04323)

    我们提出了一种可调节的鲁棒性强化学习（AR2L）框架来解决在线3D装箱问题，该框架允许有效地调节鲁棒性权重以实现所需的性能平衡。

    

    设计有效的策略来解决在线3D装箱问题一直是一个长期的挑战，主要是由于传入箱子序列的不可预测性和严格的物理约束。尽管当前用于在线3D装箱问题的深度强化学习（DRL）方法在优化潜在的箱子序列分布的平均性能方面取得了令人期待的结果，但它们在现实世界中往往无法处理一些最坏情况。标准的鲁棒性DRL算法往往过分优化最坏情况下的性能，从而牺牲了在正常问题实例分布下的性能。为了解决这些问题，我们首先引入了一种基于排列的攻击者来研究解决在线3D装箱问题的DRL方法和启发式方法的实际鲁棒性。然后，我们提出了一种可调节的鲁棒性强化学习（Adjustable Robust Reinforcement Learning，AR2L）框架，该框架允许有效地调节鲁棒性权重以实现所需的性能平衡。

    Designing effective policies for the online 3D bin packing problem (3D-BPP) has been a long-standing challenge, primarily due to the unpredictable nature of incoming box sequences and stringent physical constraints. While current deep reinforcement learning (DRL) methods for online 3D-BPP have shown promising results in optimizing average performance over an underlying box sequence distribution, they often fail in real-world settings where some worst-case scenarios can materialize. Standard robust DRL algorithms tend to overly prioritize optimizing the worst-case performance at the expense of performance under normal problem instance distribution. To address these issues, we first introduce a permutation-based attacker to investigate the practical robustness of both DRL-based and heuristic methods proposed for solving online 3D-BPP. Then, we propose an adjustable robust reinforcement learning (AR2L) framework that allows efficient adjustment of robustness weights to achieve the desired
    
[^28]: 有限监督下的潜在图推理

    Latent Graph Inference with Limited Supervision. (arXiv:2310.04314v1 [cs.LG])

    [http://arxiv.org/abs/2310.04314](http://arxiv.org/abs/2310.04314)

    本文研究了有限监督下的潜在图推理问题并提出了一种恢复关键连接和补充缺失监督的方法。

    

    潜在图推理 (LGI) 的目标是从数据特征中同时学习潜在的图结构和节点表示。然而，现有的LGI方法通常遭受督导匮乏问题，导致大量的边权重在没有语义监督的情况下学习，不能对训练损失做出贡献。因此，这些缺乏监督的权重可能决定测试样本的预测，但并不一定是语义上最优的，导致泛化能力差。本文观察到这个问题实际上是由于图稀疏化操作导致的，严重破坏了关键节点和标记节点之间建立的重要连接。为了解决这个问题，我们提出恢复受损的相似性，并为更好的LGI补充缺失的监督。关键挑战在于识别关键节点并恢复损坏的相似性。我们首先将关键节点定义为$k$-hop饥饿节点，可以通过饥饿状况计算获得。

    Latent graph inference (LGI) aims to jointly learn the underlying graph structure and node representations from data features. However, existing LGI methods commonly suffer from the issue of supervision starvation, where massive edge weights are learned without semantic supervision and do not contribute to the training loss. Consequently, these supervision-starved weights, which may determine the predictions of testing samples, cannot be semantically optimal, resulting in poor generalization. In this paper, we observe that this issue is actually caused by the graph sparsification operation, which severely destroys the important connections established between pivotal nodes and labeled ones. To address this, we propose to restore the corrupted affinities and replenish the missed supervision for better LGI. The key challenge then lies in identifying the critical nodes and recovering the corrupted affinities. We begin by defining the pivotal nodes as $k$-hop starved nodes, which can be id
    
[^29]: 带有仅解码器侧信息的分布式深度联合源信道编码

    Distributed Deep Joint Source-Channel Coding with Decoder-Only Side Information. (arXiv:2310.04311v1 [cs.CV])

    [http://arxiv.org/abs/2310.04311](http://arxiv.org/abs/2310.04311)

    本文提出了一种带有仅解码器侧信息的分布式深度联合源信道编码方法，在低延迟图像传输中实现了改进的性能，尤其在低信道信噪比和小带宽比的情况下。

    

    我们考虑在只有接收方有相关辅助信息的噪声无线信道上进行低延迟图像传输（Wyner-Ziv情景）。我们特别关注通过数据驱动的联合源信道编码（JSCC）方法开发实际方案，这在实际有限块长度的情况下已被证明优于传统的分离式方法，并在信道质量方面提供了优雅的退化。我们提出了一种新的神经网络架构，在接收器端的多个阶段将仅解码器侧信息融入其中。我们的结果表明，所提出的方法成功地整合了辅助信息，在各种畸变准则下，在所有信道噪声水平上都实现了改进的性能，尤其是在低信道信噪比（SNR）和小带宽比（BR）的情况下。我们还提供了所提方法的源代码，以便进一步研究。

    We consider low-latency image transmission over a noisy wireless channel when correlated side information is present only at the receiver side (the Wyner-Ziv scenario). In particular, we are interested in developing practical schemes using a data-driven joint source-channel coding (JSCC) approach, which has been previously shown to outperform conventional separation-based approaches in the practical finite blocklength regimes, and to provide graceful degradation with channel quality. We propose a novel neural network architecture that incorporates the decoder-only side information at multiple stages at the receiver side. Our results demonstrate that the proposed method succeeds in integrating the side information, yielding improved performance at all channel noise levels in terms of the various distortion criteria considered here, especially at low channel signal-to-noise ratios (SNRs) and small bandwidth ratios (BRs). We also provide the source code of the proposed method to enable fu
    
[^30]: 收敛的ADMM插入法PET图像重建

    Convergent ADMM Plug and Play PET Image Reconstruction. (arXiv:2310.04299v1 [eess.IV])

    [http://arxiv.org/abs/2310.04299](http://arxiv.org/abs/2310.04299)

    本研究通过将模型驱动的变分重建与独立学习的深度神经网络操作符结合，提出了一种基于ADMM插入法的混合PET重建算法，并在实验中验证了其收敛性。

    

    在这项工作中，我们研究了基于模型驱动变分重建和应用独立学习的深度神经网络操作符（DNN）的混合PET重建算法，采用ADMM插入法框架。根据最近在优化方面的结果，在学习过程中对网络参数施加额外的约束可以实现算法的固定点收敛。我们提出了这样一个ADMM算法，并通过在逼真的[18F]-FDG合成脑部检查中展示了所提出的方案确实实验上收敛到一个有意义的固定点。当在DNN的学习过程中未施加所提出的约束时，实验观察到所提出的ADMM算法无法收敛。

    In this work, we investigate hybrid PET reconstruction algorithms based on coupling a model-based variational reconstruction and the application of a separately learnt Deep Neural Network operator (DNN) in an ADMM Plug and Play framework. Following recent results in optimization, fixed point convergence of the scheme can be achieved by enforcing an additional constraint on network parameters during learning. We propose such an ADMM algorithm and show in a realistic [18F]-FDG synthetic brain exam that the proposed scheme indeed lead experimentally to convergence to a meaningful fixed point. When the proposed constraint is not enforced during learning of the DNN, the proposed ADMM algorithm was observed experimentally not to converge.
    
[^31]: 识别干预外推的表示方法

    Identifying Representations for Intervention Extrapolation. (arXiv:2310.04295v1 [cs.LG])

    [http://arxiv.org/abs/2310.04295](http://arxiv.org/abs/2310.04295)

    本文研究了干预外推的任务，证明了可识别的表示方法能够有效地解决这个任务，即使干预对结果产生非线性影响。

    

    可识别和因果关系表示学习的前提是改进当前的表示学习范式，以提高泛化性或鲁棒性。尽管在可识别性问题上取得了近期的进展，但仍需要更多理论结果来证明这些方法对下游任务的具体优势。在本文中，我们考虑干预外推的任务：预测干预如何影响结果，即使这些干预在训练时没有观察到，我们证明了可识别的表示能够为这个任务提供有效的解决方案，即使干预对结果产生非线性影响。我们的设置包括一个结果Y，观察到的特征X，这些特征是潜在特征Z的非线性转换，以及影响Z的外生行为变量A。干预外推的目标是预测位于训练支持之外的A上的干预如何影响Y。在这里，外推变得重要。

    The premise of identifiable and causal representation learning is to improve the current representation learning paradigm in terms of generalizability or robustness. Despite recent progress in questions of identifiability, more theoretical results demonstrating concrete advantages of these methods for downstream tasks are needed. In this paper, we consider the task of intervention extrapolation: predicting how interventions affect an outcome, even when those interventions are not observed at training time, and show that identifiable representations can provide an effective solution to this task even if the interventions affect the outcome non-linearly. Our setup includes an outcome Y, observed features X, which are generated as a non-linear transformation of latent features Z, and exogenous action variables A, which influence Z. The objective of intervention extrapolation is to predict how interventions on A that lie outside the training support of A affect Y. Here, extrapolation becom
    
[^32]: 面向大规模多任务数据集的分子学习基础模型的研究

    Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets. (arXiv:2310.04292v1 [cs.LG])

    [http://arxiv.org/abs/2310.04292](http://arxiv.org/abs/2310.04292)

    本研究提出了七个新颖的数据集，分别是ToyMix、LargeMix和UltraLarge，这些数据集在规模和有监督标签的多样性方面突破了界限，涵盖了近1亿个分子和3000多个稀疏定义的任务，总计超过130亿个关于量子和生物性质的标签。

    

    最近，预训练的基础模型在多个领域取得了显著的进展。然而，在分子机器学习中，数据集通常是手工策划的，因此规模较小，缺乏带有标记特征和管理这些数据集的代码库，制约了基础模型的发展。在这项工作中，我们提出了七个新颖的数据集，分为三个不同的类别：ToyMix、LargeMix和UltraLarge。这些数据集在规模和有监督标签的多样性方面突破了界限。它们涵盖了近1亿个分子和3000多个稀疏定义的任务，总计超过130亿个关于量子和生物性质的标签。相比之下，我们的数据集的数据点数量是广泛使用的OGB-LSC PCQM4Mv2数据集的300倍，也是仅包含量子数据的QM1B数据集的13倍。此外，为了支持基于我们提出的基础模型的开发，

    Recently, pre-trained foundation models have enabled significant advancements in multiple fields. In molecular machine learning, however, where datasets are often hand-curated, and hence typically small, the lack of datasets with labeled features, and codebases to manage those datasets, has hindered the development of foundation models. In this work, we present seven novel datasets categorized by size into three distinct categories: ToyMix, LargeMix and UltraLarge. These datasets push the boundaries in both the scale and the diversity of supervised labels for molecular learning. They cover nearly 100 million molecules and over 3000 sparsely defined tasks, totaling more than 13 billion individual labels of both quantum and biological nature. In comparison, our datasets contain 300 times more data points than the widely used OGB-LSC PCQM4Mv2 dataset, and 13 times more than the quantum-only QM1B dataset. In addition, to support the development of foundational models based on our proposed 
    
[^33]: 通过基于分数的对抗图像生成评估鲁棒性

    Assessing Robustness via Score-Based Adversarial Image Generation. (arXiv:2310.04285v1 [cs.CV])

    [http://arxiv.org/abs/2310.04285](http://arxiv.org/abs/2310.04285)

    本论文介绍了一种基于分数的对抗生成框架（ScoreAG），可以生成超过$\ell_p$-范数约束的对抗性示例，并通过图像转换或新图像合成的方法保持图像的核心语义，大大增强了分类器的鲁棒性。

    

    大多数对抗攻击和防御都集中在小的$\ell_p$-范数约束内的扰动上。然而，$\ell_p$威胁模型无法捕捉到所有相关的保留语义的扰动，因此，鲁棒性评估的范围是有限的。在这项工作中，我们引入了基于分数的对抗生成（ScoreAG），一种利用基于分数的生成模型的进展来生成超过$\ell_p$-范数约束的对抗性示例的新的框架，称为无限制的对抗性示例，克服了它们的局限性。与传统方法不同，ScoreAG在生成逼真的对抗性示例时保持图像的核心语义，可以通过转换现有图像或完全从零开始合成新图像的方式实现。我们进一步利用ScoreAG的生成能力来净化图像，从经验上增强分类器的鲁棒性。我们的大量实证评估表明，ScoreAG与现有最先进的对抗攻击方法的性能相当。

    Most adversarial attacks and defenses focus on perturbations within small $\ell_p$-norm constraints. However, $\ell_p$ threat models cannot capture all relevant semantic-preserving perturbations, and hence, the scope of robustness evaluations is limited. In this work, we introduce Score-Based Adversarial Generation (ScoreAG), a novel framework that leverages the advancements in score-based generative models to generate adversarial examples beyond $\ell_p$-norm constraints, so-called unrestricted adversarial examples, overcoming their limitations. Unlike traditional methods, ScoreAG maintains the core semantics of images while generating realistic adversarial examples, either by transforming existing images or synthesizing new ones entirely from scratch. We further exploit the generative capability of ScoreAG to purify images, empirically enhancing the robustness of classifiers. Our extensive empirical evaluation demonstrates that ScoreAG matches the performance of state-of-the-art atta
    
[^34]: 关于不精确消除法在主成分分析中的误差传播

    On the Error-Propagation of Inexact Deflation for Principal Component Analysis. (arXiv:2310.04283v1 [cs.LG])

    [http://arxiv.org/abs/2310.04283](http://arxiv.org/abs/2310.04283)

    该论文研究了主成分分析中不精确消除法的误差传播问题，给出了两个主要结果

    

    主成分分析（PCA）是数据分析中常用的工具，尤其是在高维数据情况下。PCA旨在找到由所谓“主成分”所张成的子空间，这些主成分最能解释数据集的方差。消除法是一种常用的元算法，用于发现这样的子空间，它从最重要的主成分开始顺序地找到每个主成分，直到找到较不重要的主成分。然而，由于其顺序性质，由于不完全估计主成分引入的数值误差 - 例如，由于此过程中的数值近似 - 会随着消除的进行而传播。据我们所知，这是第一篇在数学上对不精确消除法的误差传播进行了特性化的工作，这是本文的关键贡献。我们提供了两个主要结果：$ i）$当用于查找主要特征向量的子例程是泛型的时候，以及$ ii）$

    Principal Component Analysis (PCA) is a popular tool in data analysis, especially when the data is high-dimensional. PCA aims to find subspaces, spanned by the so-called \textit{principal components}, that best explain the variance in the dataset. The deflation method is a popular meta-algorithm -used to discover such subspaces -- that sequentially finds individual principal components, starting from the most important one and working its way towards the less important ones. However, due to its sequential nature, the numerical error introduced by not estimating principal components exactly -- e.g., due to numerical approximations through this process -- propagates, as deflation proceeds. To the best of our knowledge, this is the first work that mathematically characterizes the error propagation of the inexact deflation method, and this is the key contribution of this paper. We provide two main results: $i)$ when the sub-routine for finding the leading eigenvector is generic, and $ii)
    
[^35]: C(NN)FD -- 多级轴向压缩机气动性能中尖间隙变化的深度学习预测

    C(NN)FD -- deep learning predictions of tip clearance variations on multi-stage axial compressors aerodynamic performance. (arXiv:2310.04264v1 [cs.LG])

    [http://arxiv.org/abs/2310.04264](http://arxiv.org/abs/2310.04264)

    本文展示了一种用于实时预测多级轴向压缩机在燃气轮机中尖间隙变化对气动性能影响的深度学习框架，可与CFD基准相媲美的实时准确性，方便集成到燃气轮机的制造和构建过程中进行性能评估。

    

    迄今为止，将深度学习方法应用于诸如CFD（计算流体力学）等物理模拟在工业上的重要性有限。本文展示了一种用于多级轴向压缩机在燃气轮机中尖间隙变化对气动性能的实时预测的深度学习框架的开发和应用。所提出的C(NN)FD架构经证明可扩展至工业应用，并达到与CFD基准相媲美的实时准确性。部署的模型可轻松集成到燃气轮机的制造和构建过程中，从而提供了分析评估性能影响并潜在减少昂贵物理测试要求的机会。

    Application of deep learning methods to physical simulations such as CFD (Computational Fluid Dynamics), have been so far of limited industrial relevance. This paper demonstrates the development and application of a deep learning framework for real-time predictions of the impact of tip clearance variations on the aerodynamic performance of multi-stage axial compressors in gas turbines. The proposed C(NN)FD architecture is proven to be scalable to industrial applications, and achieves in real-time accuracy comparable to the CFD benchmark. The deployed model, is readily integrated within the manufacturing and build process of gas turbines, thus providing the opportunity to analytically assess the impact on performance and potentially reduce requirements for expensive physical tests.
    
[^36]: 比较用于强化学习的辅助任务的学习表示方法

    Comparing Auxiliary Tasks for Learning Representations for Reinforcement Learning. (arXiv:2310.04241v1 [cs.LG])

    [http://arxiv.org/abs/2310.04241](http://arxiv.org/abs/2310.04241)

    本文比较了在强化学习中用于学习表示的不同辅助任务，通过在连续控制基准环境上训练数百个智能体的实验，发现使用辅助任务的表示学习对环境的样本效率和回报有益。

    

    由于能够提高样本效率和环境回报，学习状态表示在强化学习中越来越受欢迎。一种直接和高效的方法是使用一个与实际强化学习任务不同的辅助任务训练一个独立的神经网络来生成表示。虽然在文献中提出了许多这样的辅助任务，但在典型的连续控制基准环境上进行比较计算量大且据我们所知以前未进行过。本文在基于最先进的离策略强化学习算法训练的数百个智能体上进行了这样的辅助任务比较。我们比较了从简单摆线到复杂的仿真机器人任务的样本效率和回报的可能改进。我们的研究结果表明，使用辅助任务的表示学习对环境是有益的。

    Learning state representations has gained steady popularity in reinforcement learning (RL) due to its potential to improve both sample efficiency and returns on many environments. A straightforward and efficient method is to generate representations with a distinct neural network trained on an auxiliary task, i.e. a task that differs from the actual RL task. While a whole range of such auxiliary tasks has been proposed in the literature, a comparison on typical continuous control benchmark environments is computationally expensive and has, to the best of our knowledge, not been performed before. This paper presents such a comparison of common auxiliary tasks, based on hundreds of agents trained with state-of-the-art off-policy RL algorithms. We compare possible improvements in both sample efficiency and returns for environments ranging from simple pendulum to a complex simulated robotics task. Our findings show that representation learning with auxiliary tasks is beneficial for environ
    
[^37]: 将量子算法引入自动化机器学习：AutoML框架的可扩展性对于量子机器学习算法的系统性评述

    Bringing Quantum Algorithms to Automated Machine Learning: A Systematic Review of AutoML Frameworks Regarding Extensibility for QML Algorithms. (arXiv:2310.04238v1 [cs.LG])

    [http://arxiv.org/abs/2310.04238](http://arxiv.org/abs/2310.04238)

    本文通过系统性评述现有的AutoML框架，分析其对于量子机器学习算法的可扩展性，并通过基准测试解决不同机器学习问题类型的产业用例。最终选择了Ray和AutoGluon作为合适的低级和高级框架。

    

    本文描述了选择方法和对现有AutoML框架进行分析，以确定它们能否将量子机器学习（QML）算法纳入自动化求解的AutoML框架，并通过对其最重要特征的基准测试来解决一组不同机器学习问题类型的产业用例。为此，可用的开源工具被压缩成市场概览，并通过多阶段、多标准的方法进行系统选择。这是通过考虑软件选择方法以及AutoML的技术视角来完成的。框架选择的要求根据其软件和机器学习属性被分为硬性和软性标准。此外，AutoML框架还根据研究结果进行了高级和低级类型的分类。最后，我们选择Ray和AutoGluon作为合适的低级和高级框架。

    This work describes the selection approach and analysis of existing AutoML frameworks regarding their capability of a) incorporating Quantum Machine Learning (QML) algorithms into this automated solving approach of the AutoML framing and b) solving a set of industrial use-cases with different ML problem types by benchmarking their most important characteristics. For that, available open-source tools are condensed into a market overview and suitable frameworks are systematically selected on a multi-phase, multi-criteria approach. This is done by considering software selection approaches, as well as in terms of the technical perspective of AutoML. The requirements for the framework selection are divided into hard and soft criteria regarding their software and ML attributes. Additionally, a classification of AutoML frameworks is made into high- and low-level types, inspired by the findings of. Finally, we select Ray and AutoGluon as the suitable low- and high-level frameworks respectively
    
[^38]: 一个可计数具有相同骨架的马尔可夫等价类的固定参数可处理算法

    A Fixed-Parameter Tractable Algorithm for Counting Markov Equivalence Classes with the same Skeleton. (arXiv:2310.04218v1 [cs.DS])

    [http://arxiv.org/abs/2310.04218](http://arxiv.org/abs/2310.04218)

    本文提出了一个固定参数可处理算法，用于计数具有相同骨架的马尔可夫等价类。

    

    因果有向无环图（也称为贝叶斯网络）是编码随机变量之间条件依赖关系的流行工具。在因果有向无环图中，随机变量被建模为有向图中的顶点，并且规定每个随机变量在给定其父节点的情况下与其祖先节点无关。然而，对于同一组随机变量上的两个不同的因果有向无环图可以准确编码相同的一组条件依赖关系。这样的因果有向无环图被称为马尔可夫等价，马尔可夫等价的因果有向无环图的等价类被称为马尔可夫等价类（MEC）。在过去几十年中，对于MEC已经创建了一些美丽的组合特征，并且已知，特别是在同一MEC中的所有因果有向无环图必须具有相同的“骨架”（底层无向图）和v-结构（形式为$a\rightarrow b \leftarrow c$的诱导子图）。这些组合特征还提出了几个自然的算法问题。

    Causal DAGs (also known as Bayesian networks) are a popular tool for encoding conditional dependencies between random variables. In a causal DAG, the random variables are modeled as vertices in the DAG, and it is stipulated that every random variable is independent of its ancestors conditioned on its parents. It is possible, however, for two different causal DAGs on the same set of random variables to encode exactly the same set of conditional dependencies. Such causal DAGs are said to be Markov equivalent, and equivalence classes of Markov equivalent DAGs are known as Markov Equivalent Classes (MECs). Beautiful combinatorial characterizations of MECs have been developed in the past few decades, and it is known, in particular that all DAGs in the same MEC must have the same ''skeleton'' (underlying undirected graph) and v-structures (induced subgraph of the form $a\rightarrow b \leftarrow c$).  These combinatorial characterizations also suggest several natural algorithmic questions. On
    
[^39]: 成本效益的机器学习模型重新训练

    Cost-Effective Retraining of Machine Learning Models. (arXiv:2310.04216v1 [cs.LG])

    [http://arxiv.org/abs/2310.04216](http://arxiv.org/abs/2310.04216)

    提出了一种成本效益的机器学习模型重新训练方法，通过考虑成本和数据、模型及预测查询等因素来自动化决策何时重新训练。主要贡献是成本感知重新训练算法Cara，通过对数据流和查询进行优化来实现最佳权衡。

    

    为了保持机器学习（ML）模型在数据随时间变化时的性能，有必要重新训练模型。然而，这可能是昂贵的，因为通常需要重新处理整个数据集。这导致了重新训练频率和计算成本之间的权衡，过于频繁的重新训练会导致不必要的计算成本，而不经常重新训练会导致过时和不准确的ML模型。为了解决这个挑战，我们提出了能够自动化且具有成本效益的决策何时重新训练ML模型的ML系统。我们旨在通过考虑每个决策相关的成本来优化这个权衡。我们的研究重点是基于数据、模型和模型回答的预测查询等不同因素来确定是否重新训练或保留现有的ML模型。我们的主要贡献是一种名为Cara的成本感知重新训练算法，它通过对数据流和查询进行优化来优化这种权衡。为了评估Cara的性能，我们分析了合成的数据集和实际应用数据集上的实验。

    It is important to retrain a machine learning (ML) model in order to maintain its performance as the data changes over time. However, this can be costly as it usually requires processing the entire dataset again. This creates a trade-off between retraining too frequently, which leads to unnecessary computing costs, and not retraining often enough, which results in stale and inaccurate ML models. To address this challenge, we propose ML systems that make automated and cost-effective decisions about when to retrain an ML model. We aim to optimize the trade-off by considering the costs associated with each decision. Our research focuses on determining whether to retrain or keep an existing ML model based on various factors, including the data, the model, and the predictive queries answered by the model. Our main contribution is a Cost-Aware Retraining Algorithm called Cara, which optimizes the trade-off over streams of data and queries. To evaluate the performance of Cara, we analyzed syn
    
[^40]: 非冗余图神经网络的改进表达能力

    Non-Redundant Graph Neural Networks with Improved Expressiveness. (arXiv:2310.04190v1 [cs.LG])

    [http://arxiv.org/abs/2310.04190](http://arxiv.org/abs/2310.04190)

    本文提出了一种非冗余图神经网络的改进表达能力方法，通过基于邻域树的聚合方案减少冗余，提高了表达能力，实验证明了它对于减轻过度压缩的有效性。

    

    消息传递的图神经网络通过聚合来自所有邻居节点的消息来迭代计算节点嵌入。这个过程可以被视为Weisfeiler-Leman方法的神经变体，这限制了它们的表达能力。此外，过度平滑和过度压缩限制了这些网络能够有效利用的层数。消息传递中相同信息的重复交换和编码会放大过度压缩。我们提出了一种基于邻域树的新的聚合方案，允许通过修剪标准消息传递中的展开树的分支来控制冗余。我们证明减少冗余可以提高表达能力，并通过实验证明它减轻了过度压缩。我们研究了消息传递中的冗余与计算中的冗余之间的相互作用，并提出了邻域树的紧凑表示，通过神经树规范化技术计算节点和图的嵌入。

    Message passing graph neural networks iteratively compute node embeddings by aggregating messages from all neighbors. This procedure can be viewed as a neural variant of the Weisfeiler-Leman method, which limits their expressive power. Moreover, oversmoothing and oversquashing restrict the number of layers these networks can effectively utilize. The repeated exchange and encoding of identical information in message passing amplifies oversquashing. We propose a novel aggregation scheme based on neighborhood trees, which allows for controlling the redundancy by pruning branches of the unfolding trees underlying standard message passing. We prove that reducing redundancy improves expressivity and experimentally show that it alleviates oversquashing. We investigate the interaction between redundancy in message passing and redundancy in computation and propose a compact representation of neighborhood trees, from which we compute node and graph embeddings via a neural tree canonization techn
    
[^41]: Entropic Score度量：在无需训练的NAS中解耦拓扑和大小

    Entropic Score metric: Decoupling Topology and Size in Training-free NAS. (arXiv:2310.04179v1 [cs.CV])

    [http://arxiv.org/abs/2310.04179](http://arxiv.org/abs/2310.04179)

    本文提出了一种名为Entropy Score的新型无需训练的度量方法，用于估计模型的表达能力，并通过与LogSynflow的组合搜索模型的大小，从而在较短时间内设计出用于边缘应用的高性能网络。

    

    神经网络设计是一项复杂且常常令人望而生畏的任务，特别是对于移动大小模型的资源受限场景。神经网络架构搜索是一种自动化这一过程的有希望的方法，但是现有的竞争方法需要大量的训练时间和计算资源来生成准确的模型。为了克服这些限制，本文做出了以下贡献：i）一种名为Entropy Score的新型无需训练的度量方法，通过聚合的逐元素熵估计模型的表达能力；ii）一种循环搜索算法，分别但协同地搜索模型大小和拓扑。Entropy Score在搜索网络的拓扑方面显示出显著能力，与LogSynflow的适当组合用于搜索模型大小，可以在不到1个GPU小时内完全设计用于边缘应用的高性能混合变压器，从而实现了ImageNet分类上最快且最准确的NAS方法。

    Neural Networks design is a complex and often daunting task, particularly for resource-constrained scenarios typical of mobile-sized models. Neural Architecture Search is a promising approach to automate this process, but existing competitive methods require large training time and computational resources to generate accurate models. To overcome these limits, this paper contributes with: i) a novel training-free metric, named Entropic Score, to estimate model expressivity through the aggregated element-wise entropy of its activations; ii) a cyclic search algorithm to separately yet synergistically search model size and topology. Entropic Score shows remarkable ability in searching for the topology of the network, and a proper combination with LogSynflow, to search for model size, yields superior capability to completely design high-performance Hybrid Transformers for edge applications in less than 1 GPU hour, resulting in the fastest and most accurate NAS method for ImageNet classifica
    
[^42]: 引入归因稳定性指标: 用于时间序列XAI归因的度量方法

    Introducing the Attribution Stability Indicator: a Measure for Time Series XAI Attributions. (arXiv:2310.04178v1 [cs.LG])

    [http://arxiv.org/abs/2310.04178](http://arxiv.org/abs/2310.04178)

    本研究引入了归因稳定性指标(ASI)，用于度量时间序列XAI归因的稳健性和可信度，并通过分析降维空间中的归因和ASI分数分布来证明该指标的有效性。

    

    鉴于金融、天气预报和医疗保健等领域中时间序列数据的增加和普遍复杂性，迫切需要先进的性能模型，能够提供对基本模式和关联性的可解释洞察。 归因技术能够从时间序列模型中提取解释以获得洞察，但很难评估其稳健性和可信度。 我们提出了归因稳定性指标（ASI），这是一种将稳健性和可信度作为时间序列归因技术属性考虑进去的度量方法。 我们通过将原始时间序列的扰动实例和归因与相关性结合起来进行扩展，并在度量方法中加入所需的属性。 我们通过对一组降维空间中的归因及ASI分数分布的分析来证明所需的属性，并使用了三个完整的时间序列分类数据集。

    Given the increasing amount and general complexity of time series data in domains such as finance, weather forecasting, and healthcare, there is a growing need for state-of-the-art performance models that can provide interpretable insights into underlying patterns and relationships. Attribution techniques enable the extraction of explanations from time series models to gain insights but are hard to evaluate for their robustness and trustworthiness. We propose the Attribution Stability Indicator (ASI), a measure to incorporate robustness and trustworthiness as properties of attribution techniques for time series into account. We extend a perturbation analysis with correlations of the original time series to the perturbed instance and the attributions to include wanted properties in the measure. We demonstrate the wanted properties based on an analysis of the attributions in a dimension-reduced space and the ASI scores distribution over three whole time series classification datasets.
    
[^43]: 动态关系注意力图神经网络用于欺诈检测

    Dynamic Relation-Attentive Graph Neural Networks for Fraud Detection. (arXiv:2310.04171v1 [cs.LG])

    [http://arxiv.org/abs/2310.04171](http://arxiv.org/abs/2310.04171)

    本研究针对欺诈检测问题，通过动态关系注意聚合机制，提出了一种基于图神经网络（GNN）的方法。该方法学习每个关系的节点表示，并利用可学习的注意函数进行节点表示的聚合。通过结合不同层次的节点表示，考虑目标节点的局部和全局结构，以提高欺诈检测性能。通过使用动态图注意力，可以自适应地计算关系之间的重要程度。

    

    欺诈检测旨在发现欺诈者通过留下假评论或进行异常交易欺骗其他用户。基于图的欺诈检测方法将这个任务视为一个包含两个类别（欺诈或正常）的分类问题。我们通过提出一种动态关系注意聚合机制，利用图神经网络（GNN）来解决这个问题。基于实际世界图表中包含不同类型的关系的观察，我们建议学习每个关系的节点表示，并使用可学习的注意函数聚合节点表示，该函数为每个关系分配不同的注意系数。此外，我们结合不同层次的节点表示，以考虑目标节点的局部和全局结构，这有助于提高在具有异质性的图上进行欺诈检测的性能。通过在所有聚合过程中采用动态图注意力，我们的方法可以自适应地计算关系之间的重要程度。

    Fraud detection aims to discover fraudsters deceiving other users by, for example, leaving fake reviews or making abnormal transactions. Graph-based fraud detection methods consider this task as a classification problem with two classes: frauds or normal. We address this problem using Graph Neural Networks (GNNs) by proposing a dynamic relation-attentive aggregation mechanism. Based on the observation that many real-world graphs include different types of relations, we propose to learn a node representation per relation and aggregate the node representations using a learnable attention function that assigns a different attention coefficient to each relation. Furthermore, we combine the node representations from different layers to consider both the local and global structures of a target node, which is beneficial to improving the performance of fraud detection on graphs with heterophily. By employing dynamic graph attention in all the aggregation processes, our method adaptively comput
    
[^44]: 分摊网络干预以引导兴奋性点过程

    Amortized Network Intervention to Steer the Excitatory Point Processes. (arXiv:2310.04159v1 [cs.LG])

    [http://arxiv.org/abs/2310.04159](http://arxiv.org/abs/2310.04159)

    本论文提出了一种分摊网络干预方法，可以引导兴奋性点过程的演化。这种方法利用神经ODE来捕捉网络化兴奋性点过程的变化，并通过梯度下降模型预测控制实现灵活的策略。通过设计的分摊网络干预框架，可以从历史和其他环境中集成最佳策略，实现知识的高效转移和共享。

    

    我们解决了大规模网络干预以引导兴奋性点过程（如传染病传播或交通拥堵控制）的挑战。我们的基于模型的强化学习利用神经ODE来捕捉网络化的兴奋性点过程在网络拓扑的时变变化下将如何演化。我们的方法包括梯度下降模型预测控制（GD-MPC），提供策略灵活性以适应先前知识和约束条件。为了解决计划的复杂性并克服此类决策问题中固有的高维度，我们设计了一种分摊网络干预（ANI）框架，允许从历史和其他环境中汇聚最佳策略，同时确保排列等效性。这种性质实现了知识在不同环境中的高效转移和共享。我们的方法具有广泛的应用，从控制传染病传播到减少碳排放。

    We tackle the challenge of large-scale network intervention for guiding excitatory point processes, such as infectious disease spread or traffic congestion control. Our model-based reinforcement learning utilizes neural ODEs to capture how the networked excitatory point processes will evolve subject to the time-varying changes in network topology. Our approach incorporates Gradient-Descent based Model Predictive Control (GD-MPC), offering policy flexibility to accommodate prior knowledge and constraints. To address the intricacies of planning and overcome the high dimensionality inherent to such decision-making problems, we design an Amortize Network Interventions (ANI) framework, allowing for the pooling of optimal policies from history and other contexts, while ensuring a permutation equivalent property. This property enables efficient knowledge transfer and sharing across diverse contexts. Our approach has broad applications, from curbing infectious disease spread to reducing carbon
    
[^45]: 从零到英雄: 通过合成数据注入和模型查询来检测泄露数据

    From Zero to Hero: Detecting Leaked Data through Synthetic Data Injection and Model Querying. (arXiv:2310.04145v1 [cs.LG])

    [http://arxiv.org/abs/2310.04145](http://arxiv.org/abs/2310.04145)

    本文介绍了一种名为 LDSS 的新方法，用于检测用于训练分类模型的泄露数据。该方法通过注入特征为类别分布的局部偏移的合成数据来实现。这种方法能够有效地识别出模型查询中的泄露数据。

    

    随着机器学习应用的不断增多，保护数据的知识产权已变得至关重要，而其成功在很大程度上取决于训练数据的质量。虽然存在各种机制来在存储、传输和使用过程中保护数据，但较少有研究是针对是否已经未经授权地泄露用于模型训练的数据进行检测。由于对潜在攻击者进行的训练过程缺乏信息和控制，这个问题尤为具有挑战性。本文针对表格数据领域，引入了一种新的方法论，局部分布偏移合成（LDSS），用于检测用于训练分类模型的泄露数据。LDSS 的核心概念是向所有者的数据集中注入少量合成数据，其特点是类别分布的局部偏移。这样可以有效识别出模型查询中的泄露数据。

    Safeguarding the Intellectual Property (IP) of data has become critically important as machine learning applications continue to proliferate, and their success heavily relies on the quality of training data. While various mechanisms exist to secure data during storage, transmission, and consumption, fewer studies have been developed to detect whether they are already leaked for model training without authorization. This issue is particularly challenging due to the absence of information and control over the training process conducted by potential attackers.  In this paper, we concentrate on the domain of tabular data and introduce a novel methodology, Local Distribution Shifting Synthesis (\textsc{LDSS}), to detect leaked data that are used to train classification models. The core concept behind \textsc{LDSS} involves injecting a small volume of synthetic data--characterized by local shifts in class distribution--into the owner's dataset. This enables the effective identification of mo
    
[^46]: Routing Arena: 用于神经路由求解器的基准套件

    Routing Arena: A Benchmark Suite for Neural Routing Solvers. (arXiv:2310.04140v1 [cs.LG])

    [http://arxiv.org/abs/2310.04140](http://arxiv.org/abs/2310.04140)

    Routing Arena是一个基准套件，推出了一种路由问题的评估协议，解决了现有协议的缺陷，并提供了与运筹学方法相比的评估基准。

    

    过去8年来，神经组合优化一直受到积极研究。尽管许多基于机器学习的方法都在相同的数据集上进行了比较，但评估协议存在重要缺陷，并且基准的选择通常忽略了最先进的运筹学方法。为了改进这两个缺点，我们提出了Routing Arena，一个用于路由问题的基准套件，它提供了一种一致评估和机器学习和运筹学领域中普遍存在的基准的无缝集成。所提出的评估协议考虑了不同应用的两个最重要的评估情况：首先，一个固定时间预算的解决方案质量，其次是各种方法的实时性能。通过将解决方案轨迹与最佳已知解和基准求解器的解决方案轨迹相对比，我们进一步提出了解决方案轨迹。

    Neural Combinatorial Optimization has been researched actively in the last eight years. Even though many of the proposed Machine Learning based approaches are compared on the same datasets, the evaluation protocol exhibits essential flaws and the selection of baselines often neglects State-of-the-Art Operations Research approaches. To improve on both of these shortcomings, we propose the Routing Arena, a benchmark suite for Routing Problems that provides a seamless integration of consistent evaluation and the provision of baselines and benchmarks prevalent in the Machine Learning- and Operations Research field. The proposed evaluation protocol considers the two most important evaluation cases for different applications: First, the solution quality for an a priori fixed time budget and secondly the anytime performance of the respective methods. By setting the solution trajectory in perspective to a Best Known Solution and a Base Solver's solutions trajectory, we furthermore propose the 
    
[^47]: 以快速遗忘记忆为特点的强化学习

    Reinforcement Learning with Fast and Forgetful Memory. (arXiv:2310.04128v1 [cs.LG])

    [http://arxiv.org/abs/2310.04128](http://arxiv.org/abs/2310.04128)

    以快速遗忘记忆为特点的算法无关的记忆模型针对强化学习中的部分可观测任务，通过强结构先验限制模型搜索空间，实现了比循环神经网络更高的奖励并具有更快的训练速度。

    

    几乎所有的现实世界任务都是部分可观测的，必须在强化学习（RL）中使用记忆。大多数无模型方法使用从监督学习（SL）借用的记忆模型将轨迹汇总为潜在的马尔可夫状态，尽管RL往往表现出不同的训练和效率特性。为了解决这个差异，我们引入了快速遗忘记忆，这是一种针对RL特别设计的算法无关的记忆模型。我们的方法通过受计算心理学启发的强结构先验来限制模型搜索空间。它在递归RL算法中可以替代循环神经网络（RNN），在各种递归基准和算法中实现了比RNN更高的奖励，而不需要改变任何超参数。此外，快速遗忘记忆的训练速度比RNN快两个数量级，这归因于它的对数时间和线性空间复杂度。

    Nearly all real world tasks are inherently partially observable, necessitating the use of memory in Reinforcement Learning (RL). Most model-free approaches summarize the trajectory into a latent Markov state using memory models borrowed from Supervised Learning (SL), even though RL tends to exhibit different training and efficiency characteristics. Addressing this discrepancy, we introduce Fast and Forgetful Memory, an algorithm-agnostic memory model designed specifically for RL. Our approach constrains the model search space via strong structural priors inspired by computational psychology. It is a drop-in replacement for recurrent neural networks (RNNs) in recurrent RL algorithms, achieving greater reward than RNNs across various recurrent benchmarks and algorithms without changing any hyperparameters. Moreover, Fast and Forgetful Memory exhibits training speeds two orders of magnitude faster than RNNs, attributed to its logarithmic time and linear space complexity. Our implementatio
    
[^48]: 超越近视：通过整体预测趋势从正样本和无标签数据中学习

    Beyond Myopia: Learning from Positive and Unlabeled Data through Holistic Predictive Trends. (arXiv:2310.04078v1 [cs.LG])

    [http://arxiv.org/abs/2310.04078](http://arxiv.org/abs/2310.04078)

    本文发现了从正样本和无标签数据中学习的一个重要观察：通过在每个训练迭代中重新采样正样本，可以获得强大的早期性能，并且正类和负类的预测趋势显示出不同的模式。

    

    在许多现实世界的应用中，从正样本和无标签数据(PUL)中学习二元分类器是至关重要的，特别是在验证负样本困难的情况下。尽管最近的PUL方法在实验性能方面表现出色，但由于缺乏负标签，仍然存在积累误差和增加估计偏差等挑战。在本文中，我们揭示了PUL中一个引人注目但长期被忽视的观察结果：\textit{在每个训练迭代中重新采样正样本以确保正样本和无标签样本之间的平衡分布会导致强的早期性能。此外，正类和负类的预测趋势显示出不同的模式。}具体而言，无标签负样本的得分(输出概率)持续下降，而无标签正样本的得分显示出大致混乱的趋势。我们创新地采用了一种整体方法，而不是在个别时间段内进行分类。

    Learning binary classifiers from positive and unlabeled data (PUL) is vital in many real-world applications, especially when verifying negative examples is difficult. Despite the impressive empirical performance of recent PUL methods, challenges like accumulated errors and increased estimation bias persist due to the absence of negative labels. In this paper, we unveil an intriguing yet long-overlooked observation in PUL: \textit{resampling the positive data in each training iteration to ensure a balanced distribution between positive and unlabeled examples results in strong early-stage performance. Furthermore, predictive trends for positive and negative classes display distinctly different patterns.} Specifically, the scores (output probability) of unlabeled negative examples consistently decrease, while those of unlabeled positive examples show largely chaotic trends. Instead of focusing on classification within individual time frames, we innovatively adopt a holistic approach, inte
    
[^49]: 从科学文本中自动提取方面

    Automatic Aspect Extraction from Scientific Texts. (arXiv:2310.04074v1 [cs.CL])

    [http://arxiv.org/abs/2310.04074](http://arxiv.org/abs/2310.04074)

    该论文介绍了一个用于从俄语科学文本中自动提取方面的工具，包括任务、贡献、方法和结论等方面，并提出了基于BERT模型的基准算法。通过跨领域实验证明，即使在有限的科学领域上进行训练，该模型仍能推广到新的领域。

    

    能够从科学论文中提取出主要观点、关键见解和其他重要信息（在此称为方面），可能有助于进行科学文献综述的过程。因此，我们的研究目的是创建一个用于从任何领域的俄语科学文本中自动提取方面的工具。在本文中，我们介绍了一个包含俄语科学文本的跨领域数据集，注释有任务、贡献、方法和结论等方面，并提出了一种基于多语言BERT模型在我们的数据上微调的基准算法进行方面提取。我们表明，在不同的领域中方面的表示存在一些差异，但即使我们的模型是在有限数量的科学领域上进行训练的，它仍然能够推广到新的领域，这在跨领域实验中得到了证明。代码和数据集可在 \url{https://github.com/anna-marshalova/automatic-aspect-extraction-from} 获取。

    Being able to extract from scientific papers their main points, key insights, and other important information, referred to here as aspects, might facilitate the process of conducting a scientific literature review. Therefore, the aim of our research is to create a tool for automatic aspect extraction from Russian-language scientific texts of any domain. In this paper, we present a cross-domain dataset of scientific texts in Russian, annotated with such aspects as Task, Contribution, Method, and Conclusion, as well as a baseline algorithm for aspect extraction, based on the multilingual BERT model fine-tuned on our data. We show that there are some differences in aspect representation in different domains, but even though our model was trained on a limited number of scientific domains, it is still able to generalize to new domains, as was proved by cross-domain experiments. The code and the dataset are available at \url{https://github.com/anna-marshalova/automatic-aspect-extraction-from
    
[^50]: 如何捕捉高阶相关性？将矩阵Softmax Attention推广到Kronecker计算。

    How to Capture Higher-order Correlations? Generalizing Matrix Softmax Attention to Kronecker Computation. (arXiv:2310.04064v1 [cs.DS])

    [http://arxiv.org/abs/2310.04064](http://arxiv.org/abs/2310.04064)

    本文研究了将经典transformer attention泛化到能够捕捉三元相关性的问题，并提出了一个在有界输入下具有近线性时间复杂度的算法。

    

    在经典的transformer attention方案中，我们给定三个大小为$n \times d$的矩阵$Q, K, V$（查询、键和值标记），目标是计算一个新的大小为$n \times d$的矩阵$D^{-1} \exp(QK^\top) V$，其中$D = \mathrm{diag}( \exp(QK^\top) {\bf 1}_n )$。在这项工作中，我们研究了一种能够捕捉三元相关性的注意力的泛化。这种泛化能够解决关于检测transformers无法解决的三元连接的问题。这种泛化的潜在缺点是，计算似乎更加困难，因为直接的算法在$n$的立方时间内完成。然而，我们证明在有界输入的情况下（实践中经常出现，并且在理论和实践中都有广泛研究），实际上存在一个近线性时间的算法。更准确地说，我们证明有界输入既是快速执行广义计算的必要条件也是充分条件： $\bul

    In the classical transformer attention scheme, we are given three $n \times d$ size matrices $Q, K, V$ (the query, key, and value tokens), and the goal is to compute a new $n \times d$ size matrix $D^{-1} \exp(QK^\top) V$ where $D = \mathrm{diag}( \exp(QK^\top) {\bf 1}_n )$. In this work, we study a generalization of attention which captures triple-wise correlations. This generalization is able to solve problems about detecting triple-wise connections that were shown to be impossible for transformers. The potential downside of this generalization is that it appears as though computations are even more difficult, since the straightforward algorithm requires cubic time in $n$. However, we show that in the bounded-entry setting (which arises in practice, and which is well-studied in both theory and practice), there is actually a near-linear time algorithm. More precisely, we show that bounded entries are both necessary and sufficient for quickly performing generalized computations:  $\bul
    
[^51]: DEFT：一种基于距离的键入动力学特征集

    DEFT: A new distance-based feature set for keystroke dynamics. (arXiv:2310.04059v1 [cs.LG])

    [http://arxiv.org/abs/2310.04059](http://arxiv.org/abs/2310.04059)

    该论文提出了一种基于键盘上按键之间距离的新特征集合(DEFT)，该 DEFT 模型在键入动力学研究中超越了传统的打字速度，通过使用 DEFT 特征相结合，可以实现对个人的打字行为的全面洞察。该模型在各种设备上都取得了优于现有方法的效果。

    

    键入动力学是一种用于用户识别和认证的行为生物特征。我们提出了一种新的特征集，基于键盘上按键之间的距离，这是在键入动力学中以前没有考虑过的概念。我们将飞行时间这一流行度量与键盘上的按键距离结合起来，称之为Distance Enhanced Flight Time特征(DEFT)。这种新颖的方法提供了对个人打字行为的全面洞察，超过了仅考虑打字速度。我们通过将DEFT特征与先前使用的其他键入动力学特征相结合来构建DEFT模型。DEFT模型设计成适用于各种设备，让我们可以在三种常用设备（台式机、移动设备和平板电脑）上评估其有效性。当我们在两个数据集上评估其有效性时，DEFT模型表现优于现有的最先进方法。我们获得超过99%的准确率和小于10%的等误差率的结果。

    Keystroke dynamics is a behavioural biometric utilised for user identification and authentication. We propose a new set of features based on the distance between keys on the keyboard, a concept that has not been considered before in keystroke dynamics. We combine flight times, a popular metric, with the distance between keys on the keyboard and call them as Distance Enhanced Flight Time features (DEFT). This novel approach provides comprehensive insights into a person's typing behaviour, surpassing typing velocity alone. We build a DEFT model by combining DEFT features with other previously used keystroke dynamic features. The DEFT model is designed to be device-agnostic, allowing us to evaluate its effectiveness across three commonly used devices: desktop, mobile, and tablet. The DEFT model outperforms the existing state-of-the-art methods when we evaluate its effectiveness across two datasets. We obtain accuracy rates exceeding 99% and equal error rates below 10% on all three devices
    
[^52]: AUTOPARLLM：使用大型语言模型的GNN引导的自动代码并行化

    AUTOPARLLM: GNN-Guided Automatic Code Parallelization using Large Language Models. (arXiv:2310.04047v1 [cs.LG])

    [http://arxiv.org/abs/2310.04047](http://arxiv.org/abs/2310.04047)

    AUTOPARLLM是一个用于自动发现并生成顺序编写程序并行版本的框架，其中包括一个基于GNN的并行性发现模块和一个基于LLM的代码生成器。

    

    并行化顺序编写的程序是一项具有挑战性的任务。即使是经验丰富的开发人员也需要花费相当多的时间来寻找并行性机会，然后实际编写顺序编写程序的并行版本。为了解决这个问题，我们提出了AUTOPARLLM，一个用于自动发现并行性并生成顺序编写程序的并行版本的框架。我们的框架包括两个主要组件：i）基于异构图神经网络（GNN）的并行性发现和并行模式检测模块，以及ii）基于LLM的代码生成器，用于生成顺序程序的并行对应版本。我们使用GNN学习程序的流敏感特征，以识别顺序程序中的并行区域，并使用GNN的结果构建增强提示，以供LLM基础生成器最终产生顺序程序的并行对应版本。我们在11个应用上评估了AUTOPARLLM

    Parallelizing sequentially written programs is a challenging task. Even experienced developers need to spend considerable time finding parallelism opportunities and then actually writing parallel versions of sequentially written programs. To address this issue, we present AUTOPARLLM, a framework for automatically discovering parallelism and generating the parallel version of the sequentially written program. Our framework consists of two major components: i) a heterogeneous Graph Neural Network (GNN) based parallelism discovery and parallel pattern detection module, and ii) an LLM-based code generator to generate the parallel counterpart of the sequential programs. We use the GNN to learn the flow-aware characteristics of the programs to identify parallel regions in sequential programs and then construct an enhanced prompt using the GNN's results for the LLM-based generator to finally produce the parallel counterparts of the sequential programs. We evaluate AUTOPARLLM on 11 application
    
[^53]: 观测引导的扩散概率模型

    Observation-Guided Diffusion Probabilistic Models. (arXiv:2310.04041v1 [cs.LG])

    [http://arxiv.org/abs/2310.04041](http://arxiv.org/abs/2310.04041)

    提出了观测引导的扩散概率模型（OGDM），通过引入基于条件鉴别器的观测所产生的额外损失项，实现了更准确的负对数似然优化，在函数评估次数有限的推理阶段表现出色。

    

    我们提出了一种新的扩散模型，称为观测引导的扩散概率模型（OGDM），它有效地解决了质量控制和快速采样之间的权衡问题。我们的方法以原则性的方式将观测过程的引导与马尔可夫链相结合，重新建立了训练目标。通过引入基于条件鉴别器的观测所产生的额外损失项，我们使得优化更准确的负对数似然成为可能，尤其是在函数评估次数有限的推理阶段。这种策略使得我们的训练方法即使只用于微调过程也具有优势，并且与各种快速推理策略兼容，因为我们的方法使用完全相同的推理过程产生更好的去噪网络。

    We propose a novel diffusion model called observation-guided diffusion probabilistic model (OGDM), which effectively addresses the trade-off between quality control and fast sampling. Our approach reestablishes the training objective by integrating the guidance of the observation process with the Markov chain in a principled way. This is achieved by introducing an additional loss term derived from the observation based on the conditional discriminator on noise level, which employs Bernoulli distribution indicating whether its input lies on the (noisy) real manifold or not. This strategy allows us to optimize the more accurate negative log-likelihood induced in the inference stage especially when the number of function evaluations is limited. The proposed training method is also advantageous even when incorporated only into the fine-tuning process, and it is compatible with various fast inference strategies since our method yields better denoising networks using the exactly same inferen
    
[^54]: 联合投影学习和张量分解的不完整多视图聚类

    Joint Projection Learning and Tensor Decomposition Based Incomplete Multi-view Clustering. (arXiv:2310.04038v1 [cs.LG])

    [http://arxiv.org/abs/2310.04038](http://arxiv.org/abs/2310.04038)

    本文提出了一种基于联合投影学习和张量分解的方法，用于解决不完整多视图聚类问题。该方法通过引入正交投影矩阵将高维特征投影到低维空间中，以减轻冗余特征和噪声的影响。

    

    不完整的多视图聚类(IMVC)由于样本的某些视图在现实中常常是不完整的，因此受到了越来越多的关注。大多数现有的方法从原始的不完整的多视图数据中学习相似度子图，并通过探索每个视图的不完整子图来寻找完整的图形进行谱聚类。然而，基于原始高维数据构建的图可能由于特征冗余和噪声而是次优的。此外，之前的方法通常忽略了不完整图和完整图之间的类内和类间结构变化引起的图噪声。为了解决这些问题，我们提出了一种新的基于联合投影学习和张量分解的方法(JPLTD)用于IMVC。具体地，为了减轻高维数据中冗余特征和噪声的影响，JPLTD引入了一个正交投影矩阵将高维特征投影到一个低维空间中。

    Incomplete multi-view clustering (IMVC) has received increasing attention since it is often that some views of samples are incomplete in reality. Most existing methods learn similarity subgraphs from original incomplete multi-view data and seek complete graphs by exploring the incomplete subgraphs of each view for spectral clustering. However, the graphs constructed on the original high-dimensional data may be suboptimal due to feature redundancy and noise. Besides, previous methods generally ignored the graph noise caused by the inter-class and intra-class structure variation during the transformation of incomplete graphs and complete graphs. To address these problems, we propose a novel Joint Projection Learning and Tensor Decomposition Based method (JPLTD) for IMVC. Specifically, to alleviate the influence of redundant features and noise in high-dimensional data, JPLTD introduces an orthogonal projection matrix to project the high-dimensional features into a lower-dimensional space 
    
[^55]: 遗传预测定量性状：着眼于身高的机器学习指南

    Genetic prediction of quantitative traits: a machine learner's guide focused on height. (arXiv:2310.04028v1 [cs.LG])

    [http://arxiv.org/abs/2310.04028](http://arxiv.org/abs/2310.04028)

    这篇论文提供了一个机器学习社区关于遗传预测复杂性状的指南，着重介绍了身高作为连续性状的案例，涵盖了当前最先进模型、相关微妙之处以及开发新模型时需要考虑的内容。

    

    机器学习和深度学习在生物问题上取得了许多成功，尤其是在蛋白质折叠领域。然而，机器学习领域对于从遗传学预测复杂性状的问题关注相对较少。解决这个问题需要深入了解相关的遗传学文献，并注意与遗传数据相关的各种微妙之处。在这个指南中，我们向机器学习社区提供了关于当前最先进模型以及开发新的表型预测模型时需要考虑的相关微妙之处的概述。我们以身高作为连续性状的示例，并提供了基准数据集、混淆因素、特征选择和常见评价指标的介绍。

    Machine learning and deep learning have been celebrating many successes in the application to biological problems, especially in the domain of protein folding. Another equally complex and important question has received relatively little attention by the machine learning community, namely the one of prediction of complex traits from genetics. Tackling this problem requires in-depth knowledge of the related genetics literature and awareness of various subtleties associated with genetic data. In this guide, we provide an overview for the machine learning community on current state of the art models and associated subtleties which need to be taken into consideration when developing new models for phenotype prediction. We use height as an example of a continuous-valued phenotype and provide an introduction to benchmark datasets, confounders, feature selection, and common metrics.
    
[^56]: PGraphDTA: 使用蛋白质语言模型和接触图改进药物靶标相互作用预测

    PGraphDTA: Improving Drug Target Interaction Prediction using Protein Language Models and Contact Maps. (arXiv:2310.04017v1 [cs.LG])

    [http://arxiv.org/abs/2310.04017](http://arxiv.org/abs/2310.04017)

    本研究提出了一种使用蛋白质语言模型和接触图改进药物靶标相互作用预测的方法，通过在现有模型中引入联系图信息，可以改进药物靶标相互作用预测的性能。

    

    发现和开发新药是一项复杂且资源密集型的工作，通常涉及大量成本、时间投入和安全问题。药物发现的一个关键方面是确定新颖的药物-靶标（DT）相互作用。现有的预测DT相互作用的计算方法主要集中在二分类任务上，旨在确定DT对是否存在相互作用。然而，蛋白质-配体相互作用表现出一系列不同结合强度，即结合亲和力，这对准确预测造成了持续挑战。在本研究中，我们研究了在药物靶标相互作用（DTI）预测中使用的各种技术，并提出了改进性能的新方法。我们的方法包括整合蛋白质语言模型（PLMs）和将接触图信息作为现有模型的归纳偏差。通过广泛的实验，我们证明了我们提出的方法的性能优于传统方法。

    Developing and discovering new drugs is a complex and resource-intensive endeavor that often involves substantial costs, time investment, and safety concerns. A key aspect of drug discovery involves identifying novel drug-target (DT) interactions. Existing computational methods for predicting DT interactions have primarily focused on binary classification tasks, aiming to determine whether a DT pair interacts or not. However, protein-ligand interactions exhibit a continuum of binding strengths, known as binding affinity, presenting a persistent challenge for accurate prediction. In this study, we investigate various techniques employed in Drug Target Interaction (DTI) prediction and propose novel enhancements to enhance their performance. Our approaches include the integration of Protein Language Models (PLMs) and the incorporation of Contact Map information as an inductive bias within current models. Through extensive experimentation, we demonstrate that our proposed approaches outper
    
[^57]: 通过类似样本聚类学习：对模型泛化的精确分析

    Learning via Look-Alike Clustering: A Precise Analysis of Model Generalization. (arXiv:2310.04015v1 [cs.LG])

    [http://arxiv.org/abs/2310.04015](http://arxiv.org/abs/2310.04015)

    本文提出了一种名为“类似样本聚类”的技术，通过替换个体的敏感特征为聚类的平均值来增强隐私。通过对使用匿名聚类中心训练模型的精确分析，我们揭示了不同模型组成部分对泛化误差的影响，并证明在某些高维情况下，使用匿名聚类中心进行训练可以取得更好的效果。

    

    尽管个性化推荐系统变得越来越流行，但确保用户数据的保护仍然是这些学习系统开发中的一个重要关注点。增强隐私的常见方法是使用匿名数据而不是个体数据来训练模型。在本文中，我们探索了一种名为“类似样本聚类”的自然技术，它涉及将个体的敏感特征替换为聚类的平均值。我们对使用匿名聚类中心训练模型如何影响其泛化能力进行了精确的分析。我们关注一个渐近情况，即训练集的大小与特征维度成比例增长。我们的分析基于凸高斯极小化极大定理（Convex Gaussian Minimax Theorem，CGMT），使我们能够在理论上理解不同模型组成部分对泛化误差的作用。此外，我们证明在某些高维情况下，通过匿名聚类中心进行训练能够取得更好的效果。

    While personalized recommendations systems have become increasingly popular, ensuring user data protection remains a paramount concern in the development of these learning systems. A common approach to enhancing privacy involves training models using anonymous data rather than individual data. In this paper, we explore a natural technique called \emph{look-alike clustering}, which involves replacing sensitive features of individuals with the cluster's average values. We provide a precise analysis of how training models using anonymous cluster centers affects their generalization capabilities. We focus on an asymptotic regime where the size of the training set grows in proportion to the features dimension. Our analysis is based on the Convex Gaussian Minimax Theorem (CGMT) and allows us to theoretically understand the role of different model components on the generalization error. In addition, we demonstrate that in certain high-dimensional regimes, training over anonymous cluster cente
    
[^58]: 在概率测度空间中加速优化

    Accelerating optimization over the space of probability measures. (arXiv:2310.04006v1 [math.OC])

    [http://arxiv.org/abs/2310.04006](http://arxiv.org/abs/2310.04006)

    本研究研究了在概率测度空间中加速优化的问题，提出了一种类似于欧几里得空间中基于矩方法的哈密顿流方法，并证明了其可以达到任意高阶的收敛速度。

    

    梯度优化方法的加速是一个非常实用和理论上有意义的问题，特别是在机器学习应用中。大多数研究都集中在欧几里得空间的优化上，但考虑到在许多机器学习问题中需要在概率测度空间上进行优化，研究这种情况下的加速梯度方法是很有意义的。为此，我们引入了一种类似于欧几里得空间中基于矩方法的哈密顿流方法。我们证明了基于这种方法的算法可以达到任意高阶的收敛速度。数值实例证明了我们的论断。

    Acceleration of gradient-based optimization methods is an issue of significant practical and theoretical interest, particularly in machine learning applications. Most research has focused on optimization over Euclidean spaces, but given the need to optimize over spaces of probability measures in many machine learning problems, it is of interest to investigate accelerated gradient methods in this context too. To this end, we introduce a Hamiltonian-flow approach that is analogous to moment-based approaches in Euclidean space. We demonstrate that algorithms based on this approach can achieve convergence rates of arbitrarily high order. Numerical examples illustrate our claim.
    
[^59]: 基于联邦学习的无线世界中的基础模型的作用

    The Role of Federated Learning in a Wireless World with Foundation Models. (arXiv:2310.04003v1 [cs.NI])

    [http://arxiv.org/abs/2310.04003](http://arxiv.org/abs/2310.04003)

    基于联邦学习的无线世界中，基础模型（FMs）为生成式AI应用提供支持，并且可以通过分散的数据和计算资源来提高联邦学习（FL）的性能，但是FMs对资源需求较高可能给FL-enabled的无线网络带来挑战。

    

    基础模型（FMs）是通用人工智能（AI）模型，最近为多个全新的生成式AI应用提供了支持。FMs的快速发展为下一代无线网络的愿景提供了重要的背景，其中联邦学习（FL）是分布式网络智能的关键驱动因素。目前，FMs和FL之间的相互作用仍处于初级阶段。FMs可以提高FL的性能，而FL也可以利用分散的数据和计算资源来辅助训练FMs。然而，FMs对计算资源、存储和通信开销的要求异常高，这给FL-enabled无线网络带来重要挑战。在本文中，我们探讨FMs在无线网络上是否适用于FL，包括对研究挑战和机遇的广泛概述。特别是，我们讨论了多份FL和FL资源的需求的联合训练等关键问题。

    Foundation models (FMs) are general-purpose artificial intelligence (AI) models that have recently enabled multiple brand-new generative AI applications. The rapid advances in FMs serve as an important contextual backdrop for the vision of next-generation wireless networks, where federated learning (FL) is a key enabler of distributed network intelligence. Currently, the exploration of the interplay between FMs and FL is still in its nascent stage. Naturally, FMs are capable of boosting the performance of FL, and FL could also leverage decentralized data and computing resources to assist in the training of FMs. However, the exceptionally high requirements that FMs have for computing resources, storage, and communication overhead would pose critical challenges to FL-enabled wireless networks. In this article, we explore the extent to which FMs are suitable for FL over wireless networks, including a broad overview of research challenges and opportunities. In particular, we discuss multip
    
[^60]: 运行时监控基于DNN的感知

    Runtime Monitoring DNN-Based Perception. (arXiv:2310.03999v1 [cs.LG])

    [http://arxiv.org/abs/2310.03999](http://arxiv.org/abs/2310.03999)

    运行时监控基于DNN的感知的论文总结的重要创新和贡献是介绍了文献中关于监视方法的经典方法和形式方法，并强调了设计严谨的监控器的必要性。

    

    深度神经网络(DNNs)在实现复杂的感知系统中起着重要作用。由于这些应用程序中的许多是安全关键的，需要工程严谨性来确保基于DNN的感知的功能不足不会造成伤害。除了设计阶段使用的传统静态验证和测试技术外，还需要能够检测关键事件、诊断问题甚至强制要求的运行时验证技术。本教程旨在为读者介绍文献中提出的技术。我们从机器学习社区提出的经典方法开始，然后重点介绍形式方法社区提出的一些技术。虽然我们可以观察到监控器设计中的相似之处，但决策边界的创建方式在这两个社区之间有所不同。我们得出的结论是需要严谨地设计监控器，在数据可用性受限的情况下考虑不同的方法。

    Deep neural networks (DNNs) are instrumental in realizing complex perception systems. As many of these applications are safety-critical by design, engineering rigor is required to ensure that the functional insufficiency of the DNN-based perception is not the source of harm. In addition to conventional static verification and testing techniques employed during the design phase, there is a need for runtime verification techniques that can detect critical events, diagnose issues, and even enforce requirements. This tutorial aims to provide readers with a glimpse of techniques proposed in the literature. We start with classical methods proposed in the machine learning community, then highlight a few techniques proposed by the formal methods community. While we surely can observe similarities in the design of monitors, how the decision boundaries are created vary between the two communities. We conclude by highlighting the need to rigorously design monitors, where data availability outside
    
[^61]: 通过参数高效适应，实现对缺失模态的鲁棒多模态学习

    Robust Multimodal Learning with Missing Modalities via Parameter-Efficient Adaptation. (arXiv:2310.03986v1 [cs.CV])

    [http://arxiv.org/abs/2310.03986](http://arxiv.org/abs/2310.03986)

    通过低秩适应和中间特征的调制，我们提出了针对预训练多模态网络的参数高效适应程序，以实现对缺失模态的鲁棒性，并在某些情况下胜过独立的专门网络。

    

    多模态学习旨在利用多个数据源来提高下游任务的整体性能。在一些相关的模态中观察到，如果在测试时间缺少一个或多个模态，现有的多模态网络的性能会显著下降。为了实现对缺失模态的鲁棒性，我们提出了预训练的多模态网络的简单和参数高效的适应程序。特别地，我们利用低秩适应和中间特征的调制来补偿缺失的模态。我们证明，这种适应可以部分弥补由于缺失模态而导致的性能下降，并在某些情况下胜过针对可用模态组合进行训练的独立的、专门的网络。所提出的适应所需的参数非常少（例如，少于）

    Multimodal learning seeks to utilize data from multiple sources to improve the overall performance of downstream tasks. It is desirable for redundancies in the data to make multimodal systems robust to missing or corrupted observations in some correlated modalities. However, we observe that the performance of several existing multimodal networks significantly deteriorates if one or multiple modalities are absent at test time. To enable robustness to missing modalities, we propose simple and parameter-efficient adaptation procedures for pretrained multimodal networks. In particular, we exploit low-rank adaptation and modulation of intermediate features to compensate for the missing modalities. We demonstrate that such adaptation can partially bridge performance drop due to missing modalities and outperform independent, dedicated networks trained for the available modality combinations in some cases. The proposed adaptation requires extremely small number of parameters (e.g., fewer than 
    
[^62]: 使用基于注意力的语音识别编码器进行普通话言语的痴呆评估

    Dementia Assessment Using Mandarin Speech with an Attention-based Speech Recognition Encoder. (arXiv:2310.03985v1 [cs.CL])

    [http://arxiv.org/abs/2310.03985](http://arxiv.org/abs/2310.03985)

    本文提出了一个基于注意力的语音识别模型，用于构建一个普通话言语痴呆评估系统。通过训练模型并提取编码器，实现了在阿尔茨海默病检测和临床痴呆评分预测方面的显著提升。

    

    痴呆诊断需要一系列不同的测试方法，这是复杂且耗时的。痴呆的早期检测非常重要，因为它可以防止病情进一步恶化。本文利用语音识别模型构建了一个针对普通话使用者在图片描述任务中的痴呆评估系统。通过在与真实世界情境非常相似的语音数据上训练基于注意力的语音识别模型，我们显著提高了模型的识别能力。随后，我们从语音识别模型中提取编码器，并添加了一个线性层用于痴呆评估。我们收集了来自99名被试的普通话语音数据，并从当地医院获取了他们的临床评估数据。在阿尔茨海默病检测中，我们实现了92.04%的准确性，并在临床痴呆评分预测中达到了9%的平均绝对误差。

    Dementia diagnosis requires a series of different testing methods, which is complex and time-consuming. Early detection of dementia is crucial as it can prevent further deterioration of the condition. This paper utilizes a speech recognition model to construct a dementia assessment system tailored for Mandarin speakers during the picture description task. By training an attention-based speech recognition model on voice data closely resembling real-world scenarios, we have significantly enhanced the model's recognition capabilities. Subsequently, we extracted the encoder from the speech recognition model and added a linear layer for dementia assessment. We collected Mandarin speech data from 99 subjects and acquired their clinical assessments from a local hospital. We achieved an accuracy of 92.04% in Alzheimer's disease detection and a mean absolute error of 9% in clinical dementia rating score prediction.
    
[^63]: AdaRec：用于增强用户长期参与度的自适应顺序推荐算法

    AdaRec: Adaptive Sequential Recommendation for Reinforcing Long-term User Engagement. (arXiv:2310.03984v1 [cs.IR])

    [http://arxiv.org/abs/2310.03984](http://arxiv.org/abs/2310.03984)

    AdaRec是一种自适应顺序推荐算法，通过引入基于距离的表示损失来提取潜在信息，以适应大规模在线推荐系统中用户行为模式的变化。

    

    在顺序推荐任务中，人们越来越关注使用强化学习算法来优化用户的长期参与度。大规模在线推荐系统面临的一个挑战是用户行为模式（如互动频率和保留倾向）的不断复杂变化。当将问题建模为马尔科夫决策过程时，推荐系统的动态和奖励函数会不断受到这些变化的影响。现有的推荐系统强化学习算法会受到分布偏移问题的困扰，并难以适应这种马尔科夫决策过程。本文介绍了一种新的范式，称为自适应顺序推荐（AdaRec），来解决这个问题。AdaRec提出了一种基于距离的表示损失，从用户的互动轨迹中提取潜在信息。这些信息反映了强化学习策略与当前用户行为模式的匹配程度，并帮助策略识别推荐系统中的细微变化。

    Growing attention has been paid to Reinforcement Learning (RL) algorithms when optimizing long-term user engagement in sequential recommendation tasks. One challenge in large-scale online recommendation systems is the constant and complicated changes in users' behavior patterns, such as interaction rates and retention tendencies. When formulated as a Markov Decision Process (MDP), the dynamics and reward functions of the recommendation system are continuously affected by these changes. Existing RL algorithms for recommendation systems will suffer from distribution shift and struggle to adapt in such an MDP. In this paper, we introduce a novel paradigm called Adaptive Sequential Recommendation (AdaRec) to address this issue. AdaRec proposes a new distance-based representation loss to extract latent information from users' interaction trajectories. Such information reflects how RL policy fits to current user behavior patterns, and helps the policy to identify subtle changes in the recomm
    
[^64]: CUPre: 跨领域无监督预训练用于少样本细胞分割

    CUPre: Cross-domain Unsupervised Pre-training for Few-Shot Cell Segmentation. (arXiv:2310.03981v1 [cs.CV])

    [http://arxiv.org/abs/2310.03981](http://arxiv.org/abs/2310.03981)

    本论文提出了CUPre方法，实现了跨领域无监督预训练，将常见物体检测和实例分割的能力应用于细胞图像领域，为少样本细胞分割提供了一种低成本的注释方法。

    

    在目标检测任务的预训练中，例如在常见物体上下文（COCO）[1]上，可以显著提高细胞分割的性能，但仍然需要大量精细注释的细胞图像[2]，其中包括每个图像中每个细胞的边界框、掩膜和细胞类型，以对预训练模型进行微调。为了降低注释成本，本研究考虑了少样本细胞分割的预训练DNN模型问题，其中有大量未标注的细胞图像可用，但只有一小部分被注释。因此，我们提出了跨域无监督预训练（CUPre）方法，通过使用未标注图像将对象检测和实例分割的能力（从COCO学习）转移到细胞的视觉领域。给定一个带有主干、脖子和头部模块的标准COCO预训练网络，CUPre采用交替多任务预训练（AMT2）流程并进行两个子任务的训练。

    While pre-training on object detection tasks, such as Common Objects in Contexts (COCO) [1], could significantly boost the performance of cell segmentation, it still consumes on massive fine-annotated cell images [2] with bounding boxes, masks, and cell types for every cell in every image, to fine-tune the pre-trained model. To lower the cost of annotation, this work considers the problem of pre-training DNN models for few-shot cell segmentation, where massive unlabeled cell images are available but only a small proportion is annotated. Hereby, we propose Cross-domain Unsupervised Pre-training, namely CUPre, transferring the capability of object detection and instance segmentation for common visual objects (learned from COCO) to the visual domain of cells using unlabeled images. Given a standard COCO pre-trained network with backbone, neck, and head modules, CUPre adopts an alternate multi-task pre-training (AMT2) procedure with two sub-tasks -- in every iteration of pre-training, AMT2
    
[^65]: 完美对齐可能对图形对比学习产生负面影响

    Perfect Alignment May be Poisonous to Graph Contrastive Learning. (arXiv:2310.03977v1 [cs.LG])

    [http://arxiv.org/abs/2310.03977](http://arxiv.org/abs/2310.03977)

    本研究探讨了图形对比学习中增强方法和下游性能的关系，并发现图形对比学习主要通过分离不同类别的节点来为下游任务做出贡献。

    

    图形对比学习旨在通过对齐正样本和分离负样本来学习节点表示。然而，在基于图形的学习中，对于特定增强方法背后的内在规律的研究有限。什么样的增强方法可以提高下游性能？对比学习如何实际影响下游任务？为什么增强的幅度很重要？本文试图通过建立增强方法和下游性能之间的联系，以及对对比学习的泛化性进行研究来回答这些问题。我们的发现表明，图形对比学习主要通过分离不同类别而不是聚集同一类别的节点来为下游任务做出贡献。因此，无法解释对比学习的成功，即全部样本完美对齐和增强重叠。为了理解增强如何辅助对比学习过程，我们进行了进一步的研究。

    Graph Contrastive Learning (GCL) aims to learn node representations by aligning positive pairs and separating negative ones. However, limited research has been conducted on the inner law behind specific augmentations used in graph-based learning. What kind of augmentation will help downstream performance, how does contrastive learning actually influence downstream tasks, and why the magnitude of augmentation matters? This paper seeks to address these questions by establishing a connection between augmentation and downstream performance, as well as by investigating the generalization of contrastive learning. Our findings reveal that GCL contributes to downstream tasks mainly by separating different classes rather than gathering nodes of the same class. So perfect alignment and augmentation overlap which draw all intra-class samples the same can not explain the success of contrastive learning. Then in order to comprehend how augmentation aids the contrastive learning process, we conduct 
    
[^66]: 学习非Markov行为的最终极限：费舍尔信息率和超量信息

    Ultimate limit on learning non-Markovian behavior: Fisher information rate and excess information. (arXiv:2310.03968v1 [cs.LG])

    [http://arxiv.org/abs/2310.03968](http://arxiv.org/abs/2310.03968)

    该论文研究了从时间序列数据中学习任何随机过程的未知参数的极限，发现了最佳推断的封闭形式表达式。对于给定的参数化候选模型类，观测序列概率的费舍尔信息下界了来自有限数据的模型估计方差，最小方差随序列长度的增加而减小，缩放率由信息率给出。通过分析观察引起的信念状态间的元动力学，得到了模型方差的精确下界，并发现了不同的收敛模式，最终短视的信息率收敛到渐近的费舍尔信息率。

    

    我们研究了从时间序列数据中学习任何随机过程的未知参数的基本限制，并发现了如何随着观测长度的增加，最佳推断的封闭形式表达式。给定一个参数化的候选模型类，观测序列概率的费舍尔信息下界了来自有限数据的模型估计方差。随着序列长度的增加，最小方差的缩放率与长度的平方倒数成反比 -- 其中常数系数由信息率给出。我们发现了这个信息率的简单闭式表达式，即使在无限Markov阶的情况下也是如此。此外，我们通过观察引起的信念状态间元动力学获得了模型方差的精确分析下界。我们还发现了瞬时、指数和更一般的收敛模式，以达到渐近信息率。令人惊讶的是，这个短视的信息率收敛到渐近的费舍尔信息率。

    We address the fundamental limits of learning unknown parameters of any stochastic process from time-series data, and discover exact closed-form expressions for how optimal inference scales with observation length. Given a parametrized class of candidate models, the Fisher information of observed sequence probabilities lower-bounds the variance in model estimation from finite data. As sequence-length increases, the minimal variance scales as the square inverse of the length -- with constant coefficient given by the information rate. We discover a simple closed-form expression for this information rate, even in the case of infinite Markov order. We furthermore obtain the exact analytic lower bound on model variance from the observation-induced metadynamic among belief states. We discover ephemeral, exponential, and more general modes of convergence to the asymptotic information rate. Surprisingly, this myopic information rate converges to the asymptotic Fisher information rate with exac
    
[^67]: 一种基于学习的功能连接性神经障碍诊断的可学习的反条件分析框架

    A Learnable Counter-condition Analysis Framework for Functional Connectivity-based Neurological Disorder Diagnosis. (arXiv:2310.03964v1 [cs.LG])

    [http://arxiv.org/abs/2310.03964](http://arxiv.org/abs/2310.03964)

    本研究提出了一种基于学习的功能连接性神经障碍诊断的反条件分析框架，通过集成诊断和解释的步骤，解决了现有框架中各阶段结果可靠性不足的问题。

    

    为了理解功能连接性(FC)与神经障碍的生物特征，最近的研究广泛利用基于深度学习的模型来识别疾病，并通过可解释的模型进行事后分析以发现与疾病相关的生物标志物。大多数现有的框架由三个阶段组成，即特征选择、分类的特征提取和分析，每个阶段都是分别实施的。然而，如果每个阶段的结果缺乏可靠性，可能会导致误诊和后续阶段的错误分析。在本研究中，我们提出了一种新的统一框架，系统地集成了诊断(即特征选择和特征提取)和解释。值得注意的是，我们设计了一种自适应注意力网络作为特征选择方法，以识别个体特定的与疾病相关的连接。我们还提出了一个功能网络关系编码器，总结了FC的全局拓扑特性。

    To understand the biological characteristics of neurological disorders with functional connectivity (FC), recent studies have widely utilized deep learning-based models to identify the disease and conducted post-hoc analyses via explainable models to discover disease-related biomarkers. Most existing frameworks consist of three stages, namely, feature selection, feature extraction for classification, and analysis, where each stage is implemented separately. However, if the results at each stage lack reliability, it can cause misdiagnosis and incorrect analysis in afterward stages. In this study, we propose a novel unified framework that systemically integrates diagnoses (i.e., feature selection and feature extraction) and explanations. Notably, we devised an adaptive attention network as a feature selection approach to identify individual-specific disease-related connections. We also propose a functional network relational encoder that summarizes the global topological properties of FC
    
[^68]: 理解提示工程可能不需要重新思考泛化能力

    Understanding prompt engineering may not require rethinking generalization. (arXiv:2310.03957v1 [cs.LG])

    [http://arxiv.org/abs/2310.03957](http://arxiv.org/abs/2310.03957)

    提示工程在零样本学习中取得了令人印象深刻的性能，其成功归因于经典的PAC-Bayes边界将离散的提示特性与由语言模型给出的先验相结合，形成了相对紧密的泛化界限。

    

    在引导的视觉语言模型中进行零样本学习，即通过构建提示来构建分类器而无需明确的训练过程，在许多场景中取得了令人印象深刻的性能。这一成功带来了一个看似令人惊讶的观察：这些方法在过拟合方面相对较少受到影响，即当手动设计提示以在给定的训练集上实现低错误率时（因此实际上不再是零样本学习），该方法在保留的测试数据上仍然表现良好。在本文中，我们展示通过经典的PAC-Bayes边界可以很好地解释这种性能。具体而言，我们展示了提示的离散性质，结合由语言模型给出的PAC-Bayes先验，导致了文献标准下相对紧密的泛化界限：例如，ImageNet分类器的泛化界限通常与真实测试错误率相差几个百分点。我们经验性地证明了...

    Zero-shot learning in prompted vision-language models, the practice of crafting prompts to build classifiers without an explicit training process, has achieved impressive performance in many settings. This success presents a seemingly surprising observation: these methods suffer relatively little from overfitting, i.e., when a prompt is manually engineered to achieve low error on a given training set (thus rendering the method no longer actually zero-shot), the approach still performs well on held-out test data. In this paper, we show that we can explain such performance well via recourse to classical PAC-Bayes bounds. Specifically, we show that the discrete nature of prompts, combined with a PAC-Bayes prior given by a language model, results in generalization bounds that are remarkably tight by the standards of the literature: for instance, the generalization bound of an ImageNet classifier is often within a few percentage points of the true test error. We demonstrate empirically that
    
[^69]: 通过元模型改进了配体-蛋白质结合亲和力的预测

    Improved prediction of ligand-protein binding affinities by meta-modeling. (arXiv:2310.03946v1 [cs.LG])

    [http://arxiv.org/abs/2310.03946](http://arxiv.org/abs/2310.03946)

    通过整合基于结构的对接和基于序列的深度学习模型，开发了一个元模型框架，显著改善了配体-蛋白质结合亲和力预测的性能。

    

    通过计算方法准确筛选候选药物配体与靶蛋白的结合是药物开发的主要关注点，因为筛选潜在候选物能够节省找药物的时间和费用。这种虚拟筛选部分依赖于预测配体和蛋白质之间的结合亲和力的方法。鉴于存在许多计算模型对不同目标的结合亲和力预测结果不同，我们在这里开发了一个元模型框架，通过整合已发表的基于结构的对接和基于序列的深度学习模型来构建。在构建这个框架时，我们评估了许多组合的个别模型、训练数据库以及线性和非线性的元模型方法。我们显示出许多元模型在亲和力预测上显著改善了个别基础模型的性能。我们最好的元模型达到了与最先进的纯结构为基础的深度学习工具相当的性能。总体而言，我们证明了这个元模型框架可以显著改善配体-蛋白质结合亲和力预测的性能。

    The accurate screening of candidate drug ligands against target proteins through computational approaches is of prime interest to drug development efforts, as filtering potential candidates would save time and expenses for finding drugs. Such virtual screening depends in part on methods to predict the binding affinity between ligands and proteins. Given many computational models for binding affinity prediction with varying results across targets, we herein develop a meta-modeling framework by integrating published empirical structure-based docking and sequence-based deep learning models. In building this framework, we evaluate many combinations of individual models, training databases, and linear and nonlinear meta-modeling approaches. We show that many of our meta-models significantly improve affinity predictions over individual base models. Our best meta-models achieve comparable performance to state-of-the-art exclusively structure-based deep learning tools. Overall, we demonstrate 
    
[^70]: 关于随机向量的仿射变换的Wasserstein距离研究

    On Wasserstein distances for affine transformations of random vectors. (arXiv:2310.03945v1 [stat.ML])

    [http://arxiv.org/abs/2310.03945](http://arxiv.org/abs/2310.03945)

    本研究阐述了关于随机向量之间的Wasserstein距离的下界，重点是仿射变换，并应用于流型学习和手写数据集模拟。

    

    我们阐述了关于在Wasserstein空间中用于数据流型学习的随机向量之间的二次Wasserstein距离的一些已知下界，重点是仿射变换。特别地，我们通过计算协方差矩阵之间的Bures距离，给出了旋转的随机向量在具有不相关分量的$\mathbb{R}^2$空间中的具体下界。我们还得到了仿射变换的组合的上界，从而产生了应用于初始数据测度的丰富的微分同胚。我们将这些界应用于包括在$\mathbb{R}^2$中的一维流型上的各种分布，并说明了这些界的质量。最后，我们提供了一个在流型学习框架中可以应用于模拟手写数字或字母数据集的框架。

    We expound on some known lower bounds of the quadratic Wasserstein distance between random vectors in $\mathbb{R}^n$ with an emphasis on affine transformations that have been used in manifold learning of data in Wasserstein space. In particular, we give concrete lower bounds for rotated copies of random vectors in $\mathbb{R}^2$ with uncorrelated components by computing the Bures metric between the covariance matrices. We also derive upper bounds for compositions of affine maps which yield a fruitful variety of diffeomorphisms applied to an initial data measure. We apply these bounds to various distributions including those lying on a 1-dimensional manifold in $\mathbb{R}^2$ and illustrate the quality of the bounds. Finally, we give a framework for mimicking handwritten digit or alphabet datasets that can be applied in a manifold learning framework.
    
[^71]: LaTeX: 对疫情期间不良经历的语言模式感知触发事件检测

    LaTeX: Language Pattern-aware Triggering Event Detection for Adverse Experience during Pandemics. (arXiv:2310.03941v1 [cs.LG])

    [http://arxiv.org/abs/2310.03941](http://arxiv.org/abs/2310.03941)

    本论文使用实时的社交媒体数据，分析了与COVID-19疫情期间四种不良经历相关的语言模式。通过提出稀疏性优化问题和语言模式相似性约束条件，解决了该问题的困难。

    

    COVID-19大流行加剧了美国各种种族和族裔群体之间的社会经济差距。本研究探讨了社交媒体平台在凸显和解决这些挑战方面的作用。通过分析Twitter上的实时数据，我们研究了与四种主要不良经历相关的语言模式：失业收入损失（LI），食物匮乏（FS），住房不安全（HI）和未满足的心理健康服务需求（UM）。我们首先提出了一个稀疏性优化问题，从社交媒体数据源中提取低级语言特征。其次，我们提出了基于先验知识的语言模式相似性的新约束条件。由于目标函数的非凸性和n个不良经历之间的语言模式相似性关系，该问题很难解决。

    The COVID-19 pandemic has accentuated socioeconomic disparities across various racial and ethnic groups in the United States. While previous studies have utilized traditional survey methods like the Household Pulse Survey (HPS) to elucidate these disparities, this paper explores the role of social media platforms in both highlighting and addressing these challenges. Drawing from real-time data sourced from Twitter, we analyzed language patterns related to four major types of adverse experiences: loss of employment income (LI), food scarcity (FS), housing insecurity (HI), and unmet needs for mental health services (UM). We first formulate a sparsity optimization problem that extracts low-level language features from social media data sources. Second, we propose novel constraints on feature similarity exploiting prior knowledge about the similarity of the language patterns among the adverse experiences. The proposed problem is challenging to solve due to the non-convexity objective and n
    
[^72]: 使用最近邻改善分类器决策边界

    Improving classifier decision boundaries using nearest neighbors. (arXiv:2310.03927v1 [cs.LG])

    [http://arxiv.org/abs/2310.03927](http://arxiv.org/abs/2310.03927)

    该论文提出了一个简单的算法，通过计算样本及其最近邻在潜在空间中的预测的加权平均，改善了神经网络在对抗性攻击、标签噪声、分类准确性和可解释性方面的性能，而无需改变网络架构、训练过程或数据集。

    

    神经网络无法学习最优决策边界。我们表明决策边界位于训练数据密度低的区域，并受到少量训练样本的影响，容易导致过拟合。我们提供一个简单的算法，在潜在空间中计算样本及其最近邻的预测的加权平均，对于神经网络的各种重要指标导致了一些有利的结果。在我们的评估中，我们使用各种自训练和预训练的卷积神经网络来展示我们的方法改善了：(i)对标签噪声的抵抗力，(ii)对抗性攻击的鲁棒性，(iii)分类准确性，甚至在一定程度上还有(iv)可解释性。虽然在这四个方面改进不一定是很大的，但我们的方法在概念上很简单，即改进不需要对网络架构、训练过程或数据集进行任何修改。而且，这些改进与

    Neural networks are not learning optimal decision boundaries. We show that decision boundaries are situated in areas of low training data density. They are impacted by few training samples which can easily lead to overfitting. We provide a simple algorithm performing a weighted average of the prediction of a sample and its nearest neighbors' (computed in latent space) leading to a minor favorable outcomes for a variety of important measures for neural networks. In our evaluation, we employ various self-trained and pre-trained convolutional neural networks to show that our approach improves (i) resistance to label noise, (ii) robustness against adversarial attacks, (iii) classification accuracy, and to some degree even (iv) interpretability. While improvements are not necessarily large in all four areas, our approach is conceptually simple, i.e., improvements come without any modification to network architecture, training procedure or dataset. Furthermore, they are in stark contrast to 
    
[^73]: 使用二维卷积的多任务学习在时间序列数据中的应用

    Multitask Learning for Time Series Data\\with 2D Convolution. (arXiv:2310.03925v1 [cs.LG])

    [http://arxiv.org/abs/2310.03925](http://arxiv.org/abs/2310.03925)

    该论文研究了将多任务学习（MTL）应用于时间序列分类（TSC）问题，并发现将最先进的一维卷积模型与MTL集成时性能下降。为了解决这个问题，提出了一种基于二维卷积的新设计。

    

    多任务学习（MTL）旨在开发一个统一的模型，可以同时处理一组密切相关的任务。通过在多个任务上优化模型，MTL在泛化能力方面通常优于非MTL模型。尽管MTL在计算机视觉、自然语言处理和推荐系统等领域得到了广泛研究，但在时间序列数据中的应用却受到了限制。在本文中，我们研究了将MTL应用于时间序列分类（TSC）问题。然而，当将最先进的基于一维卷积的TSC模型与MTL集成时，TSC模型的性能实际上会下降。通过将一维卷积模型与动态时间规整（DTW）距离函数进行比较，可以看出低下的结果是由于一维卷积层的有限表达能力造成的。为了克服这一挑战，我们提出了一种基于二维卷积的新设计。

    Multitask learning (MTL) aims to develop a unified model that can handle a set of closely related tasks simultaneously. By optimizing the model across multiple tasks, MTL generally surpasses its non-MTL counterparts in terms of generalizability. Although MTL has been extensively researched in various domains such as computer vision, natural language processing, and recommendation systems, its application to time series data has received limited attention. In this paper, we investigate the application of MTL to the time series classification (TSC) problem. However, when we integrate the state-of-the-art 1D convolution-based TSC model with MTL, the performance of the TSC model actually deteriorates. By comparing the 1D convolution-based models with the Dynamic Time Warping (DTW) distance function, it appears that the underwhelming results stem from the limited expressive power of the 1D convolutional layers. To overcome this challenge, we propose a novel design for a 2D convolution-based
    
[^74]: 一种高效的基于内容的时间序列检索系统

    An Efficient Content-based Time Series Retrieval System. (arXiv:2310.03919v1 [cs.IR])

    [http://arxiv.org/abs/2310.03919](http://arxiv.org/abs/2310.03919)

    本论文提出了一种高效的基于内容的时间序列检索系统，可以在用户与系统实时交互的情况下，有效地度量和计算不同时间序列之间的相似度，满足用户从多个领域获取时间序列信息的需求。

    

    基于内容的时间序列检索(CTSR)系统是一个信息检索系统，用户可以与来自多个领域(如金融、医疗和制造业)的时间序列进行交互。例如，用户想要了解时间序列的来源，可以将时间序列作为查询提交给CTSR系统，并检索与之相关的时间序列列表及相关元数据。通过分析检索到的元数据，用户可以获得有关时间序列来源的更多信息。由于CTSR系统需要处理来自不同领域的时间序列数据，因此需要一个高容量模型来有效地度量不同时间序列之间的相似度。此外，CTSR系统内的模型还需要以高效的方式计算相似度得分，以满足用户在实时交互中的需求。本文提出了一种有效且高效的CTSR模型，其性能优于其他替代模型，同时仍然提供合理的准确性。

    A Content-based Time Series Retrieval (CTSR) system is an information retrieval system for users to interact with time series emerged from multiple domains, such as finance, healthcare, and manufacturing. For example, users seeking to learn more about the source of a time series can submit the time series as a query to the CTSR system and retrieve a list of relevant time series with associated metadata. By analyzing the retrieved metadata, users can gather more information about the source of the time series. Because the CTSR system is required to work with time series data from diverse domains, it needs a high-capacity model to effectively measure the similarity between different time series. On top of that, the model within the CTSR system has to compute the similarity scores in an efficient manner as the users interact with the system in real-time. In this paper, we propose an effective and efficient CTSR model that outperforms alternative models, while still providing reasonable in
    
[^75]: 朝着时间序列数据的基础模型迈进

    Toward a Foundation Model for Time Series Data. (arXiv:2310.03916v1 [cs.LG])

    [http://arxiv.org/abs/2310.03916](http://arxiv.org/abs/2310.03916)

    本文旨在通过利用多领域的无标签样本来开发一种有效的时间序列基础模型，以解决当前关于时间序列预训练的研究集中在单一领域数据上的问题。

    

    基础模型是一个基于大规模和多样化的数据集进行训练的机器学习模型，通常使用基于自监督学习的预训练技术，可以适应各种下游任务。然而，当前关于时间序列预训练的研究主要集中在仅使用单一领域数据进行预训练的模型上，导致对其他类型时间序列的知识缺乏。本文旨在通过利用多领域的无标签样本来开发一种有效的时间序列基础模型。为实现这一目标，我们重新利用了公开可用的UCR存档，并评估了四种现有的基于自监督学习的预训练方法以及一种新方法。

    A foundation model is a machine learning model trained on a large and diverse set of data, typically using self-supervised learning-based pre-training techniques, that can be adapted to various downstream tasks. However, current research on time series pre-training has mostly focused on models pre-trained solely on data from a single domain, resulting in a lack of knowledge about other types of time series. However, current research on time series pre-training has predominantly focused on models trained exclusively on data from a single domain. As a result, these models possess domain-specific knowledge that may not be easily transferable to time series from other domains. In this paper, we aim to develop an effective time series foundation model by leveraging unlabeled samples from multiple domains. To achieve this, we repurposed the publicly available UCR Archive and evaluated four existing self-supervised learning-based pre-training methods, along with a novel method, on the dataset
    
[^76]: 利用低秩和稀疏的循环连接进行鲁棒闭环控制

    Leveraging Low-Rank and Sparse Recurrent Connectivity for Robust Closed-Loop Control. (arXiv:2310.03915v1 [cs.LG])

    [http://arxiv.org/abs/2310.03915](http://arxiv.org/abs/2310.03915)

    本文探讨了将循环神经网络应用于鲁棒闭环控制任务，研究循环连接的秩和稀疏性如何影响网络的稳定性。实验证明，采用低秩稀疏连接可以在闭环设置中取得良好效果。

    

    在机器学习中，开发能够与变化环境进行交互的自主代理是一个开放性挑战。在这些环境中，稳健性尤为重要，因为代理通常是在专家示范中进行离线拟合，但在在线部署时必须能够推广到环境内的闭环反馈中。在这项工作中，我们探讨了将递归神经网络应用于这类任务，并了解其循环连接参数化如何影响闭环设置的鲁棒性。具体而言，我们将循环连接表示为秩和稀疏性的函数，并从理论和实证的角度展示调节这两个变量对网络动态的有益影响。所提出的低秩稀疏连接为网络引入了可解释性先验，对于一类称为闭式连续时间神经网络（CfCs）的模型最为适用。我们发现，具有较少参数的CfCs可以超过其他模型的表现。

    Developing autonomous agents that can interact with changing environments is an open challenge in machine learning. Robustness is particularly important in these settings as agents are often fit offline on expert demonstrations but deployed online where they must generalize to the closed feedback loop within the environment. In this work, we explore the application of recurrent neural networks to tasks of this nature and understand how a parameterization of their recurrent connectivity influences robustness in closed-loop settings. Specifically, we represent the recurrent connectivity as a function of rank and sparsity and show both theoretically and empirically that modulating these two variables has desirable effects on network dynamics. The proposed low-rank, sparse connectivity induces an interpretable prior on the network that proves to be most amenable for a class of models known as closed-form continuous-time neural networks (CfCs). We find that CfCs with fewer parameters can ou
    
[^77]: RTDK-BO：具有Reinforced Transformer深度核函数的高维贝叶斯优化

    RTDK-BO: High Dimensional Bayesian Optimization with Reinforced Transformer Deep kernels. (arXiv:2310.03912v1 [cs.LG])

    [http://arxiv.org/abs/2310.03912](http://arxiv.org/abs/2310.03912)

    本文提出了一种新的提高贝叶斯优化模型建模能力的方法，通过将注意机制融入深度核学习中，使得代理能够适应上下文信息，提高优化性能。

    

    贝叶斯优化（BO）通过高斯过程（GP）代理指导，已经被证明是一种对于高维黑盒优化非常有效的技术，在工业设计和科学计算等许多应用中具有重要意义。最近的研究在单函数优化和少样本多目标优化上引入了强化学习（RL）来提高优化性能。然而，即使是少样本技术也不能充分利用紧密相关目标之间的相似性。本文结合了深度核学习（DKL）和基于注意力的Transformer模型的最新进展，改进了GP代理的建模能力与元学习相结合。我们提出了一种新的方法，通过将注意机制融入DKL中来改进元学习BO代理，使代理能够在BO过程中适应上下文信息。我们将这种Transformer深度核方法与少样本元学习相结合，通过元学习来提高BO的建模能力。

    Bayesian Optimization (BO), guided by Gaussian process (GP) surrogates, has proven to be an invaluable technique for efficient, high-dimensional, black-box optimization, a critical problem inherent to many applications such as industrial design and scientific computing. Recent contributions have introduced reinforcement learning (RL) to improve the optimization performance on both single function optimization and \textit{few-shot} multi-objective optimization. However, even few-shot techniques fail to exploit similarities shared between closely related objectives. In this paper, we combine recent developments in Deep Kernel Learning (DKL) and attention-based Transformer models to improve the modeling powers of GP surrogates with meta-learning. We propose a novel method for improving meta-learning BO surrogates by incorporating attention mechanisms into DKL, empowering the surrogates to adapt to contextual information gathered during the BO process. We combine this Transformer Deep Kern
    
[^78]: 用强化学习实现的PyDCM：为可持续性定制数据中心模型

    PyDCM: Custom Data Center Models with Reinforcement Learning for Sustainability. (arXiv:2310.03906v1 [cs.LG])

    [http://arxiv.org/abs/2310.03906](http://arxiv.org/abs/2310.03906)

    PyDCM是一个用强化学习实现的可定制的数据中心模型，通过使用自定义配置和向量化的热计算，实现了对数据中心的优化，具有较高的效率。

    

    全球对可持续性和减少碳排放的强调日益增加，促使政府和企业重新思考数据中心的设计和运营方法。鉴于数据中心的高能耗和指数级计算工作量，优化能耗特别是在冷却和IT能源使用方面，数据中心是优化电力消耗的理想候选。解决这个问题的一个重要挑战是缺乏可配置和可扩展的热数据中心模型，它提供了一个端到端的管道。数据中心由多个IT组件组成，其几何配置和散热使得热建模变得困难。本文介绍了PyDCM，这是一个用Python实现的可定制的数据中心模型，用户可以使用自定义的服务器规格和IT机柜的几何布置创建独特的配置。使用向量化的热计算使得PyDCM比当前方法快了数个数量级（30倍）。

    The increasing global emphasis on sustainability and reducing carbon emissions is pushing governments and corporations to rethink their approach to data center design and operation. Given their high energy consumption and exponentially large computational workloads, data centers are prime candidates for optimizing power consumption, especially in areas such as cooling and IT energy usage. A significant challenge in this pursuit is the lack of a configurable and scalable thermal data center model that offers an end-to-end pipeline. Data centers consist of multiple IT components whose geometric configuration and heat dissipation make thermal modeling difficult. This paper presents PyDCM, a customizable Data Center Model implemented in Python, that allows users to create unique configurations of IT equipment with custom server specifications and geometric arrangements of IT cabinets. The use of vectorized thermal calculations makes PyDCM orders of magnitude faster (30 times) than current 
    
[^79]: 通过退火来估计归一化常数的可证明的益处：重要性抽样，噪声对比估计，以及更多

    Provable benefits of annealing for estimating normalizing constants: Importance Sampling, Noise-Contrastive Estimation, and beyond. (arXiv:2310.03902v1 [stat.ML])

    [http://arxiv.org/abs/2310.03902](http://arxiv.org/abs/2310.03902)

    这篇论文研究了使用退火方法估计归一化常数的蒙特卡洛方法。通过评估不同设计选择对估计误差的影响，结果表明使用退火噪声对比估计器更有效，并且使用几何路径可以降低估计误差。

    

    最近的研究发展了几种蒙特卡洛方法来估计归一化常数（配分函数），这些方法基于退火的思想。即从可计算的“提议”分布和未归一化的“目标”分布之间的路径逐步采样。这些家族中的重要估计器包括退火重要性抽样和退火噪声对比估计（NCE）。这样的方法依赖于许多设计选择：使用哪个估计器、使用哪个分布路径以及是否使用分布路径；到目前为止，对于哪些选择是有效的还没有明确的理论。在这里，我们通过产生的渐近估计误差来评估每个设计选择。首先，我们证明了使用NCE比重要性抽样估计器更有效，但在无限小的路径步长的极限下，差异消失了。第二，我们发现使用几何路径将估计误差从指数级降低到...

    Recent research has developed several Monte Carlo methods for estimating the normalization constant (partition function) based on the idea of annealing. This means sampling successively from a path of distributions that interpolate between a tractable "proposal" distribution and the unnormalized "target" distribution. Prominent estimators in this family include annealed importance sampling and annealed noise-contrastive estimation (NCE). Such methods hinge on a number of design choices: which estimator to use, which path of distributions to use and whether to use a path at all; so far, there is no definitive theory on which choices are efficient. Here, we evaluate each design choice by the asymptotic estimation error it produces. First, we show that using NCE is more efficient than the importance sampling estimator, but in the limit of infinitesimal path steps, the difference vanishes. Second, we find that using the geometric path brings down the estimation error from an exponential to
    
[^80]: CrysFormer: 基于3D Patterson映射和部分结构注意力的蛋白质结构预测

    CrysFormer: Protein Structure Prediction via 3d Patterson Maps and Partial Structure Attention. (arXiv:2310.03899v1 [cs.LG])

    [http://arxiv.org/abs/2310.03899](http://arxiv.org/abs/2310.03899)

    提出了第一个直接利用蛋白质晶体学和部分结构信息来预测蛋白质电子密度图的转换器模型，通过两个新的肽段数据集演示了其准确性。

    

    确定蛋白质的结构是一个数十年来的悬而未决问题。当经典模拟算法被应用时，蛋白质的三维结构通常具有非平凡的计算成本。转换器神经网络架构的进展，如AlphaFold2，通过从大型序列信息和相对应的蛋白质结构数据集中学习，在这个问题上取得了显著的改进。然而，这种方法只关注序列信息；其他可用的先验知识，如蛋白质晶体学和氨基酸的部分结构，有可能被利用。据我们所知，我们提出了第一个直接利用蛋白质晶体学和部分结构信息来预测蛋白质电子密度图的基于转换器的模型。通过两个新的肽段数据集（2个残基和15个残基），我们演示了我们的方法，称为"CrysFormer"，可以实现准确的预测。

    Determining the structure of a protein has been a decades-long open question. A protein's three-dimensional structure often poses nontrivial computation costs, when classical simulation algorithms are utilized. Advances in the transformer neural network architecture -- such as AlphaFold2 -- achieve significant improvements for this problem, by learning from a large dataset of sequence information and corresponding protein structures. Yet, such methods only focus on sequence information; other available prior knowledge, such as protein crystallography and partial structure of amino acids, could be potentially utilized. To the best of our knowledge, we propose the first transformer-based model that directly utilizes protein crystallography and partial structure information to predict the electron density maps of proteins. Via two new datasets of peptide fragments (2-residue and 15-residue) , we demonstrate our method, dubbed \texttt{CrysFormer}, can achieve accurate predictions, based on
    
[^81]: 基于时间感知正则化的生成经验重放的增量学习

    Class-Incremental Learning Using Generative Experience Replay Based on Time-aware Regularization. (arXiv:2310.03898v1 [cs.LG])

    [http://arxiv.org/abs/2310.03898](http://arxiv.org/abs/2310.03898)

    提出了一种基于时间感知正则化的生成经验重放的增量学习方法，解决了连续学习中新任务累积且不遗忘旧任务的挑战。实验证明该方法在严格的增量类别设置下具有较好的记忆保留和模型性能。

    

    在连续学习中，学习新任务而不遗忘旧任务仍然是一个重要挑战。生成经验重放通过合成过去学习任务的伪数据点，并在与新任务的数据一起并行训练时进行重放，从而解决了这个挑战。在严格的增量类别设置下，生成重放是连续学习的最佳策略，同时要满足一些约束条件：（i）模型大小固定，（ii）没有预训练数据集，（iii）没有用于存储过去任务数据的内存缓冲区。受生物神经系统机制的启发，我们引入了一种时间感知正则化方法，动态调整生成重放的三个训练目标项：监督学习、潜在正则化和数据重构。主要基准测试结果表明，我们的方法在这种严格设置下推动了受大脑启发的连续学习模型的极限，提高了记忆保留和模型性能。

    Learning new tasks accumulatively without forgetting remains a critical challenge in continual learning. Generative experience replay addresses this challenge by synthesizing pseudo-data points for past learned tasks and later replaying them for concurrent training along with the new tasks' data. Generative replay is the best strategy for continual learning under a strict class-incremental setting when certain constraints need to be met: (i) constant model size, (ii) no pre-training dataset, and (iii) no memory buffer for storing past tasks' data. Inspired by the biological nervous system mechanisms, we introduce a time-aware regularization method to dynamically fine-tune the three training objective terms used for generative replay: supervised learning, latent regularization, and data reconstruction. Experimental results on major benchmarks indicate that our method pushes the limit of brain-inspired continual learners under such strict settings, improves memory retention, and increase
    
[^82]: 基于根化逻辑目标函数的加速神经网络训练

    Accelerated Neural Network Training with Rooted Logistic Objectives. (arXiv:2310.03890v1 [cs.LG])

    [http://arxiv.org/abs/2310.03890](http://arxiv.org/abs/2310.03890)

    该论文基于根化逻辑目标函数，设计了一种加速神经网络训练的方法，通过推导出一系列严格凸函数，实现了与最小范数解相同的最小化点，从而提高了最优结果的达成速度。

    

    许多在实际场景中部署的神经网络是使用基于交叉熵的损失函数进行训练的。从优化的角度来看，我们知道一阶方法（如梯度下降）的行为在很大程度上取决于数据集的可分性。事实上，即使在最简单的二分类情况下，收敛速度取决于两个因素：（1）数据矩阵的条件数，和（2）数据集的可分性。在没有进一步预处理技术（如超参数化、数据增强等）的情况下，可分性是所考虑的数据分布固有的量。我们专注于逻辑函数的优化，并推导出一系列新颖的严格凸函数，这些函数至少和逻辑损失一样严格。这些函数的最小化点与最小范数解的最小化点一致，以尽可能使用。这个推导的函数的严格凸性可以扩展到微调最先进的模型时。

    Many neural networks deployed in the real world scenarios are trained using cross entropy based loss functions. From the optimization perspective, it is known that the behavior of first order methods such as gradient descent crucially depend on the separability of datasets. In fact, even in the most simplest case of binary classification, the rate of convergence depends on two factors: (1) condition number of data matrix, and (2) separability of the dataset. With no further pre-processing techniques such as over-parametrization, data augmentation etc., separability is an intrinsic quantity of the data distribution under consideration. We focus on the landscape design of the logistic function and derive a novel sequence of {\em strictly} convex functions that are at least as strict as logistic loss. The minimizers of these functions coincide with those of the minimum norm solution wherever possible. The strict convexity of the derived function can be extended to finetune state-of-the-ar
    
[^83]: 信息几何对于工作中的信息论者的研究

    Information Geometry for the Working Information Theorist. (arXiv:2310.03884v1 [cs.IT])

    [http://arxiv.org/abs/2310.03884](http://arxiv.org/abs/2310.03884)

    本文提供了对于工作中的信息论者来说，信息几何的基本概述。解释了统计流形上的差异、距离、正交性和测地线的概念，并介绍了一些最近的信息几何发展。

    

    信息几何是从几何角度研究统计流形，即概率分布空间的学科。其经典的信息理论应用与统计概念（如费舍尔信息、充分统计量和有效估计量）有关。如今，信息几何已经成为一个跨学科领域，在雷达感知、阵列信号处理、量子物理、深度学习和最优传输等各个领域中找到应用。本文对于不熟悉这一令人激动的研究领域的信息论者，提供了基本的信息几何概述。我们解释了统计流形上的差异概念，广义的距离概念，正交性和测地线，从而为具体的应用和新颖的理论研究铺平了道路。我们还强调了一些近期的信息几何发展，这些发展对于更广泛的信息论者具有兴趣。

    Information geometry is a study of statistical manifolds, that is, spaces of probability distributions from a geometric perspective. Its classical information-theoretic applications relate to statistical concepts such as Fisher information, sufficient statistics, and efficient estimators. Today, information geometry has emerged as an interdisciplinary field that finds applications in diverse areas such as radar sensing, array signal processing, quantum physics, deep learning, and optimal transport. This article presents an overview of essential information geometry to initiate an information theorist, who may be unfamiliar with this exciting area of research. We explain the concepts of divergences on statistical manifolds, generalized notions of distances, orthogonality, and geodesics, thereby paving the way for concrete applications and novel theoretical investigations. We also highlight some recent information-geometric developments, which are of interest to the broader information t
    
[^84]: 小批量深度强化学习

    Small batch deep reinforcement learning. (arXiv:2310.03882v1 [cs.LG])

    [http://arxiv.org/abs/2310.03882](http://arxiv.org/abs/2310.03882)

    小批量深度强化学习中，研究发现将批量大小减小可以产生显著性能提升，并进行了实证分析以更好地理解这一现象。

    

    在基于价值的深度强化学习中，批量大小参数指定每次梯度更新要采样的转换数量。虽然这个值对学习过程至关重要，但通常在提出新算法时不会进行调整。本研究通过广泛的实证研究表明，将批量大小减小可以产生多个显著性能提升；这一点令人惊讶，因为在训练神经网络时，通常倾向于使用较大的批量大小以改善性能。我们还通过一系列实证分析来更好地理解这一现象。

    In value-based deep reinforcement learning with replay memories, the batch size parameter specifies how many transitions to sample for each gradient update. Although critical to the learning process, this value is typically not adjusted when proposing new algorithms. In this work we present a broad empirical study that suggests {\em reducing} the batch size can result in a number of significant performance gains; this is surprising, as the general tendency when training neural networks is towards larger batch sizes for improved performance. We complement our experimental findings with a set of empirical analyses towards better understanding this phenomenon.
    
[^85]: 非交换卷积信号模型在神经网络中的应用: 对小变形的稳定性

    Non Commutative Convolutional Signal Models in Neural Networks: Stability to Small Deformations. (arXiv:2310.03879v1 [cs.LG])

    [http://arxiv.org/abs/2310.03879](http://arxiv.org/abs/2310.03879)

    本文研究了基于非交换代数的代数信号模型在卷积神经网络中的应用。研究发现非交换卷积滤波器在算子空间的小扰动下可以保持稳定，并且存在稳定性和选择性之间的权衡关系。

    

    本文讨论了最近在[1]中发表的基于非交换代数的代数信号模型(ASM)以及它们在卷积神经网络中的应用。借助代数信号处理(ASP)的一般工具，我们研究了非交换卷积滤波器的滤波和稳定特性。我们展示了非交换滤波器在算子空间的小扰动下可以保持稳定。我们还展示了尽管非交换信号模型中的傅里叶表示的谱分量与维度大于1的空间相关联，但在稳定性和选择性之间存在类似于交换模型的权衡。我们的结果对于群神经网络、多图神经网络和四元神经网络等非交换结构具有直接的影响。最后我们通过数值实验验证了这些结果。

    In this paper we discuss the results recently published in~[1] about algebraic signal models (ASMs) based on non commutative algebras and their use in convolutional neural networks. Relying on the general tools from algebraic signal processing (ASP), we study the filtering and stability properties of non commutative convolutional filters. We show how non commutative filters can be stable to small perturbations on the space of operators. We also show that although the spectral components of the Fourier representation in a non commutative signal model are associated to spaces of dimension larger than one, there is a trade-off between stability and selectivity similar to that observed for commutative models. Our results have direct implications for group neural networks, multigraph neural networks and quaternion neural networks, among other non commutative architectures. We conclude by corroborating these results through numerical experiments.
    
[^86]: 程序阶段的模型复杂性

    Model Complexity of Program Phases. (arXiv:2310.03865v1 [cs.LG])

    [http://arxiv.org/abs/2310.03865](http://arxiv.org/abs/2310.03865)

    这篇论文研究了资源有限计算系统中序列预测模型的实施成本和预测质量之间的权衡，提出了理论和经验方法来探索这种权衡空间，并认为了解这种权衡行为对于理解资源受限任务中模型的限制是有益的。

    

    在资源有限的计算系统中，序列预测模型必须在严格的限制下运行。有各种模型可用，针对这些条件下的预测，在某种程度上专注于降低实施成本。实践中，这些受资源限制的序列预测模型在实现成本和预测质量之间存在根本的权衡。这种根本权衡在不同任务的模型中似乎很少被探索。在这里，我们制定了必要的理论和相关的经验过程，以探索一类特定的机器学习模型（如深度神经网络）的这种权衡空间。我们预计了解这种权衡行为可能有助于理解资源受限任务的模型创建和部署的理论和实际限制。

    In resource limited computing systems, sequence prediction models must operate under tight constraints. Various models are available that cater to prediction under these conditions that in some way focus on reducing the cost of implementation. These resource constrained sequence prediction models, in practice, exhibit a fundamental tradeoff between the cost of implementation and the quality of its predictions. This fundamental tradeoff seems to be largely unexplored for models for different tasks. Here we formulate the necessary theory and an associated empirical procedure to explore this tradeoff space for a particular family of machine learning models such as deep neural networks. We anticipate that the knowledge of the behavior of this tradeoff may be beneficial in understanding the theoretical and practical limits of creation and deployment of models for resource constrained tasks.
    
[^87]: 变分重心坐标

    Variational Barycentric Coordinates. (arXiv:2310.03861v1 [cs.GR])

    [http://arxiv.org/abs/2310.03861](http://arxiv.org/abs/2310.03861)

    我们提出了一种变分技术以优化广义重心坐标，通过直接参数化连续函数来解决之前模型中目标函数选择受限的问题。我们的方法在多个目标函数方面都表现出了灵活性，并提供了实用的加速策略。

    

    我们提出了一种变分技术，用于优化广义重心坐标，与现有模型相比，它提供了额外的控制。以前的工作使用网格或闭式公式表示重心坐标，在实践中限制了目标函数的选择。相反，我们直接参数化连续函数，将任何多面体内部的坐标映射到其重心坐标，使用神经场实现。这种形式是基于我们对重心坐标的理论表征，它允许我们构造参数化整个函数类的神经场，以表示有效的坐标。我们使用多种目标函数展示了我们模型的灵活性，包括多个平滑和变形感知的能量；作为一个附加贡献，我们还介绍了在不连续神经场上测量和最小化总变差等目标的数学合理化手段。我们提供了一种实用的加速策略。

    We propose a variational technique to optimize for generalized barycentric coordinates that offers additional control compared to existing models. Prior work represents barycentric coordinates using meshes or closed-form formulae, in practice limiting the choice of objective function. In contrast, we directly parameterize the continuous function that maps any coordinate in a polytope's interior to its barycentric coordinates using a neural field. This formulation is enabled by our theoretical characterization of barycentric coordinates, which allows us to construct neural fields that parameterize the entire function class of valid coordinates. We demonstrate the flexibility of our model using a variety of objective functions, including multiple smoothness and deformation-aware energies; as a side contribution, we also present mathematically-justified means of measuring and minimizing objectives like total variation on discontinuous neural fields. We offer a practical acceleration strat
    
[^88]: OpenIncrement：一种统一的开放集识别和深度分类增量学习框架。

    OpenIncrement: A Unified Framework for Open Set Recognition and Deep Class-Incremental Learning. (arXiv:2310.03848v1 [cs.CV])

    [http://arxiv.org/abs/2310.03848](http://arxiv.org/abs/2310.03848)

    本论文介绍了一个集成了开放集识别和深度分类增量学习的统一框架。实验证实了我们的方法在增量学习和开放集识别方面优于其他方法。

    

    在深度增量学习的研究中，大多数工作都假设新样本已经被预先识别，用于神经网络的重新训练。然而，实际的深度分类器经常误识别这些样本，导致错误的预测。这种错误分类会降低模型的性能。开放集识别等技术提供了一种检测这些新样本的方法，代表了机器学习领域的一个重要领域。本文介绍了一种集成了开放集识别的深度分类增量学习框架。我们的方法通过调整增量学习到的特征，使其适应基于距离的开放集识别。实验结果验证了我们的方法优于最先进的增量学习技术，并且在开放集识别方面表现出比基准方法更好的性能。

    In most works on deep incremental learning research, it is assumed that novel samples are pre-identified for neural network retraining. However, practical deep classifiers often misidentify these samples, leading to erroneous predictions. Such misclassifications can degrade model performance. Techniques like open set recognition offer a means to detect these novel samples, representing a significant area in the machine learning domain.  In this paper, we introduce a deep class-incremental learning framework integrated with open set recognition. Our approach refines class-incrementally learned features to adapt them for distance-based open set recognition. Experimental results validate that our method outperforms state-of-the-art incremental learning techniques and exhibits superior performance in open set recognition compared to baseline methods.
    
[^89]: Euclid:使用深度学习识别模拟图像中的小行星光迹

    Euclid: Identification of asteroid streaks in simulated images using deep learning. (arXiv:2310.03845v1 [astro-ph.EP])

    [http://arxiv.org/abs/2310.03845](http://arxiv.org/abs/2310.03845)

    该论文提出了一种使用深度学习识别Euclid空间望远镜图像中小行星光迹的方法。通过构建、训练和测试一个三步机器学习流程，最终实现了对光迹的自动化检测和分类。

    

    在ESA Euclid空间望远镜的图像中，最多有15万颗小行星可见，并且Euclid的仪器提供这些物体的多频段可见光到近红外光度测量和无狭缝光谱。大多数小行星将出现在图像中的光迹中。由于图像和小行星的数量庞大，需要自动化的检测方法。以前曾尝试了一种基于StreakDet软件的非机器学习方法，但对于短暂和/或微弱的光迹，结果不理想。我们决定通过使用深度学习来改善Euclid图像中的小行星光迹检测能力。我们使用模拟的Euclid图像构建、训练和测试了一个三步机器学习流程。首先，使用卷积神经网络（CNN）在完整的图像中检测光迹及其坐标，以最大化检测的完整性（召回率）。然后，使用循环神经网络（RNN）合并CNN在多个部分检测到的长光迹的片段。最后，使用梯度提升方法对光迹进行筛选和分类。

    Up to 150000 asteroids will be visible in the images of the ESA Euclid space telescope, and the instruments of Euclid offer multiband visual to near-infrared photometry and slitless spectra of these objects. Most asteroids will appear as streaks in the images. Due to the large number of images and asteroids, automated detection methods are needed. A non-machine-learning approach based on the StreakDet software was previously tested, but the results were not optimal for short and/or faint streaks. We set out to improve the capability to detect asteroid streaks in Euclid images by using deep learning.  We built, trained, and tested a three-step machine-learning pipeline with simulated Euclid images. First, a convolutional neural network (CNN) detected streaks and their coordinates in full images, aiming to maximize the completeness (recall) of detections. Then, a recurrent neural network (RNN) merged snippets of long streaks detected in several parts by the CNN. Lastly, gradient-boosted 
    
[^90]: Less is More: 关于预训练模型在少样本任务中特征冗余性的研究

    Less is More: On the Feature Redundancy of Pretrained Models When Transferring to Few-shot Tasks. (arXiv:2310.03843v1 [cs.CV])

    [http://arxiv.org/abs/2310.03843](http://arxiv.org/abs/2310.03843)

    预训练模型在少样本任务中的特征可以极其冗余，仅使用最重要的特征维度的1%就能够达到使用完整表示时的性能。

    

    将预训练模型应用于下游任务可以通过使用目标数据进行线性探测来实现，即对从预训练模型中提取的冻结特征进行训练线性分类器。由于预训练和下游数据集之间可能存在显著差异，我们可以询问是否所有预训练特征的维度对于给定的下游任务都是有用的。我们发现，在线性探测的情况下，当下游数据稀缺或少样本时，预训练特征可能极其冗余。对于一些情况，比如5类1样本任务，只使用最重要的特征维度的1%就能够达到使用完整表示时的性能。有趣的是，大部分特征只在少样本设置下是冗余的，在样本数增加时逐渐变得有用，这表明特征冗余可能是表征少样本转移问题的关键。我们给出了理论解释

    Transferring a pretrained model to a downstream task can be as easy as conducting linear probing with target data, that is, training a linear classifier upon frozen features extracted from the pretrained model. As there may exist significant gaps between pretraining and downstream datasets, one may ask whether all dimensions of the pretrained features are useful for a given downstream task. We show that, for linear probing, the pretrained features can be extremely redundant when the downstream data is scarce, or few-shot. For some cases such as 5-way 1-shot tasks, using only 1\% of the most important feature dimensions is able to recover the performance achieved by using the full representation. Interestingly, most dimensions are redundant only under few-shot settings and gradually become useful when the number of shots increases, suggesting that feature redundancy may be the key to characterizing the "few-shot" nature of few-shot transfer problems. We give a theoretical understanding 
    
[^91]: 上下文化的结构化自监督学习用于本体匹配

    Contextualized Structural Self-supervised Learning for Ontology Matching. (arXiv:2310.03840v1 [cs.LG])

    [http://arxiv.org/abs/2310.03840](http://arxiv.org/abs/2310.03840)

    本研究提出了一种名为LaKERMap的自监督学习OM框架，通过将上下文和结构信息整合到transformer中，捕捉多个结构化上下文，并应用于本体匹配中。

    

    本体匹配（OM）涉及在两个或多个知识图中识别概念之间的语义关系，并作为整合来自各种来源的知识图的关键步骤。最近深度OM模型的进展已经利用了基于transformer的语言模型的能力和知识图嵌入的优势。然而，这些OM模型仍然面临着持续的挑战，如缺乏参考对齐、运行时延迟和未开发的内部不同图结构等。在本研究中，我们引入了一种新颖的自监督学习OM框架，名为LaKERMap，该框架利用概念的上下文和结构信息，将隐式知识整合到transformer中。具体而言，我们旨在通过采用不同的训练目标捕捉多个结构化上下文，包括局部和全局交互。为了评估我们的方法，我们利用了Bio-ML数据集。

    Ontology matching (OM) entails the identification of semantic relationships between concepts within two or more knowledge graphs (KGs) and serves as a critical step in integrating KGs from various sources. Recent advancements in deep OM models have harnessed the power of transformer-based language models and the advantages of knowledge graph embedding. Nevertheless, these OM models still face persistent challenges, such as a lack of reference alignments, runtime latency, and unexplored different graph structures within an end-to-end framework. In this study, we introduce a novel self-supervised learning OM framework with input ontologies, called LaKERMap. This framework capitalizes on the contextual and structural information of concepts by integrating implicit knowledge into transformers. Specifically, we aim to capture multiple structural contexts, encompassing both local and global interactions, by employing distinct training objectives. To assess our methods, we utilize the Bio-ML 
    
[^92]: Chameleon: 使用自适应污染增强标签唯一成员泄露

    Chameleon: Increasing Label-Only Membership Leakage with Adaptive Poisoning. (arXiv:2310.03838v1 [cs.LG])

    [http://arxiv.org/abs/2310.03838](http://arxiv.org/abs/2310.03838)

    Chameleon是一种新的成员推理攻击方法，利用自适应数据污染策略和高效的查询选择方法，可在标签唯一设置中提高成员泄露的准确率。

    

    在众多关键应用中引入机器学习(ML)后，个人数据提供者面临多种隐私问题。其中之一是成员推理(MI)，攻击者试图确定特定数据样本是否包含在模型的训练数据集中。目前的MI攻击利用模型的预测置信度分数来成功进行成员推理，并通过数据污染进一步提高效果。在这项工作中，我们关注较少探索且更现实的仅标签设置中，模型仅在查询样本上提供预测的标签。我们表明现有的仅标签MI攻击在低误报率(FPR)情况下无法有效推断成员身份。为了应对这一挑战，我们提出了一种新的攻击方法Chameleon，它利用一种新颖的自适应数据污染策略和高效的查询选择方法。

    The integration of machine learning (ML) in numerous critical applications introduces a range of privacy concerns for individuals who provide their datasets for model training. One such privacy risk is Membership Inference (MI), in which an attacker seeks to determine whether a particular data sample was included in the training dataset of a model. Current state-of-the-art MI attacks capitalize on access to the model's predicted confidence scores to successfully perform membership inference, and employ data poisoning to further enhance their effectiveness. In this work, we focus on the less explored and more realistic label-only setting, where the model provides only the predicted label on a queried sample. We show that existing label-only MI attacks are ineffective at inferring membership in the low False Positive Rate (FPR) regime. To address this challenge, we propose a new attack Chameleon that leverages a novel adaptive data poisoning strategy and an efficient query selection meth
    
[^93]: 学习PU学习的解缠表示

    Learning A Disentangling Representation For PU Learning. (arXiv:2310.03833v1 [cs.LG])

    [http://arxiv.org/abs/2310.03833](http://arxiv.org/abs/2310.03833)

    本文提出了一种基于神经网络的数据表示方法，通过学习一个损失函数来将未标记数据投影为两个簇，以模拟低维情况下的PU学习问题。实验证明学习到的表示可以增强未标记数据簇之间的分离效果。

    

    在本文中，我们解决了学习一个二元（正 vs. 负）分类器的问题，给定了常常被称为PU学习的正样本和未标记样本数据。虽然在低维度情况下可以使用基本的技术（如聚类、超出分布检测或正样本密度估计）来解决该问题，但是随着数据分布的复杂性逐渐增加，这些方法的效果逐渐下降。在本文中，我们提出了一种使用损失函数来学习基于神经网络的数据表示的方法，该方法可以将未标记数据投影到两个（正和负）簇中，可以使用简单的聚类技术轻松识别，从而有效模拟低维度情况下观察到的现象。我们采用了一种向量量化技术来放大学习到的未标记数据簇之间的分离。我们在模拟的PU数据上进行了实验证明…

    In this paper, we address the problem of learning a binary (positive vs. negative) classifier given Positive and Unlabeled data commonly referred to as PU learning. Although rudimentary techniques like clustering, out-of-distribution detection, or positive density estimation can be used to solve the problem in low-dimensional settings, their efficacy progressively deteriorates with higher dimensions due to the increasing complexities in the data distribution. In this paper we propose to learn a neural network-based data representation using a loss function that can be used to project the unlabeled data into two (positive and negative) clusters that can be easily identified using simple clustering techniques, effectively emulating the phenomenon observed in low-dimensional settings. We adopt a vector quantization technique for the learned representations to amplify the separation between the learned unlabeled data clusters. We conduct experiments on simulated PU data that demonstrate th
    
[^94]: ECAvg：一种使用平均权重的边缘-云协同学习方法

    ECAvg: An Edge-Cloud Collaborative Learning Approach using Averaged Weights. (arXiv:2310.03823v1 [cs.LG])

    [http://arxiv.org/abs/2310.03823](http://arxiv.org/abs/2310.03823)

    ECAvg是一种边缘-云协同学习方法，通过平均权重实现边缘设备和云服务器之间的模型协作与更新。

    

    边缘设备与云的结合提供了两类设备之间的协作关系，彼此之间互补优势。资源受限的边缘设备可以通过将计算密集型任务卸载到服务器上，从服务器提供的丰富计算能力中获益。同时，边缘设备可以利用其接近数据源的优势，在数据上执行较少计算密集型的任务。在本文中，我们提出了一种名为ECAvg的协同边缘-云范式，在该范式中，边缘设备在各自的数据集上预训练本地模型，并将模型转移到服务器进行微调。服务器将预训练的权重平均为一个全局模型，该模型在来自不同边缘设备的组合数据上进行微调。然后，使用全局模型的权重更新本地（边缘）模型。我们使用MobileNetV2实现了CIFAR-10分类任务，并使用ResN（待完成）

    The use of edge devices together with cloud provides a collaborative relationship between both classes of devices where one complements the shortcomings of the other. Resource-constraint edge devices can benefit from the abundant computing power provided by servers by offloading computationally intensive tasks to the server. Meanwhile, edge devices can leverage their close proximity to the data source to perform less computationally intensive tasks on the data. In this paper, we propose a collaborative edge-cloud paradigm called ECAvg in which edge devices pre-train local models on their respective datasets and transfer the models to the server for fine-tuning. The server averages the pre-trained weights into a global model, which is fine-tuned on the combined data from the various edge devices. The local (edge) models are then updated with the weights of the global (server) model. We implement a CIFAR-10 classification task using MobileNetV2, a CIFAR-100 classification task using ResN
    
[^95]: 使用硬注意力的Transformer编码器可以接受的逻辑语言

    Logical Languages Accepted by Transformer Encoders with Hard Attention. (arXiv:2310.03817v1 [cs.FL])

    [http://arxiv.org/abs/2310.03817](http://arxiv.org/abs/2310.03817)

    本文研究了使用硬注意力的Transformer编码器可以接受的逻辑语言的问题，发现UHAT编码器只能识别${\sf AC}^0$中的一部分语言，而AHAT编码器可以识别更丰富的语言。

    

    我们对可以被Transformer编码器识别的形式语言进行了研究。我们重点研究了两种自注意机制：(1)UHAT（唯一硬注意力Transformer）和(2)AHAT（平均硬注意力Transformer）。已知UHAT编码器只能识别电路复杂度类${\sf AC}^0$中的语言，即由多项式大小和深度有界的布尔电路以及无限扇入的布尔门接受的语言。另一方面，AHAT编码器可以识别${\sf AC}^0$之外的语言，但它们的表达能力仍然在更大的电路复杂度类${\sf TC}^0$内，即由多数门扩展的${\sf AC}^0$电路。我们首先给出了一个负面结果，即存在一个${\sf AC}^0$语言，无法被UHAT编码器识别。在积极的一面，我们展示了UHAT编码器可以识别${\sf AC}^0$-语言的一个丰富片段，即在一阶逻辑中使用任意一元数量谓词定义的所有语言。

    We contribute to the study of formal languages that can be recognized by transformer encoders. We focus on two self-attention mechanisms: (1) UHAT (Unique Hard Attention Transformers) and (2) AHAT (Average Hard Attention Transformers). UHAT encoders are known to recognize only languages inside the circuit complexity class ${\sf AC}^0$, i.e., accepted by a family of poly-sized and depth-bounded boolean circuits with unbounded fan-ins. On the other hand, AHAT encoders can recognize languages outside ${\sf AC}^0$), but their expressive power still lies within the bigger circuit complexity class ${\sf TC}^0$, i.e., ${\sf AC}^0$-circuits extended by majority gates. We first show a negative result that there is an ${\sf AC}^0$-language that cannot be recognized by an UHAT encoder. On the positive side, we show that UHAT encoders can recognize a rich fragment of ${\sf AC}^0$-languages, namely, all languages definable in first-order logic with arbitrary unary numerical predicates. This logic, 
    
[^96]: 鱼网：信息最优，可扩展的集合和图聚合

    Fishnets: Information-Optimal, Scalable Aggregation for Sets and Graphs. (arXiv:2310.03812v1 [cs.LG])

    [http://arxiv.org/abs/2310.03812](http://arxiv.org/abs/2310.03812)

    Fishnets是一种用于学习信息最优的集合和图聚合的方法，在规模上可以优化到任意数量的数据对象，具有鲁棒性，能够饱和贝叶斯信息内容，并可用于GNNs中的消息传递。

    

    基于集合的学习是现代深度学习和网络科学的重要组成部分。图神经网络（GNNs）及其不含边的对应物Deepsets在不规则和拓扑复杂的数据集上被证明非常有用。为了学习集合成员的信息丰富的嵌入，关键是指定一个聚合函数，通常是求和、最大值或均值。我们提出了Fishnets，一种用于学习集合数据和图聚合的信息最优嵌入策略，适用于贝叶斯推理。我们证明了：i）Fishnets神经摘要可以最优地扩展到任意数量的数据对象；ii）Fishnets聚合对数据分布的改变具有鲁棒性，而标准的Deepsets不具备这种特性；iii）Fishnets饱和贝叶斯信息内容，并扩展到MCMC技术失败的领域；iv）Fishnets可以作为GNN中的一个插入式聚合方案。我们展示了通过采用Fishnets聚合方案进行消息传递，GNNs可以实现 达到

    Set-based learning is an essential component of modern deep learning and network science. Graph Neural Networks (GNNs) and their edge-free counterparts Deepsets have proven remarkably useful on ragged and topologically challenging datasets. The key to learning informative embeddings for set members is a specified aggregation function, usually a sum, max, or mean. We propose Fishnets, an aggregation strategy for learning information-optimal embeddings for sets of data for both Bayesian inference and graph aggregation. We demonstrate that i) Fishnets neural summaries can be scaled optimally to an arbitrary number of data objects, ii) Fishnets aggregations are robust to changes in data distribution, unlike standard deepsets, iii) Fishnets saturate Bayesian information content and extend to regimes where MCMC techniques fail and iv) Fishnets can be used as a drop-in aggregation scheme within GNNs. We show that by adopting a Fishnets aggregation scheme for message passing, GNNs can achieve 
    
[^97]: 好表示的液滴：在两层网络中 grokking 作为一阶相变

    Droplets of Good Representations: Grokking as a First Order Phase Transition in Two Layer Networks. (arXiv:2310.03789v1 [stat.ML])

    [http://arxiv.org/abs/2310.03789](http://arxiv.org/abs/2310.03789)

    本研究将自适应核方法应用于两个师生模型，预测了特征学习和 Grokking 的性质，并展示了 Grokking 与相变理论之间的映射关系。

    

    深度神经网络 (DNN) 的一个关键特性是在训练过程中能够学习新的特征。这种深度学习的有趣方面在最近报道的 Grokking 现象中表现得最为明显。虽然主要体现为测试准确性的突变增加，但 Grokking 也被认为是一种超越懒惰学习/高斯过程 (GP) 的现象，涉及特征学习。在这里，我们将特征学习理论的最新发展，自适应核方法，应用于具有立方多项式和模加法教师的两个师生模型。我们在这些模型上提供了关于特征学习和 Grokking 性质的分析预测，并展示了 Grokking 与相变理论之间的映射关系。我们表明，在 Grokking 之后，DNN 的状态类似于一阶相变后的混合相。在这个混合相中，DNN 生成了与之前明显不同的教师的有用内部表示。

    A key property of deep neural networks (DNNs) is their ability to learn new features during training. This intriguing aspect of deep learning stands out most clearly in recently reported Grokking phenomena. While mainly reflected as a sudden increase in test accuracy, Grokking is also believed to be a beyond lazy-learning/Gaussian Process (GP) phenomenon involving feature learning. Here we apply a recent development in the theory of feature learning, the adaptive kernel approach, to two teacher-student models with cubic-polynomial and modular addition teachers. We provide analytical predictions on feature learning and Grokking properties of these models and demonstrate a mapping between Grokking and the theory of phase transitions. We show that after Grokking, the state of the DNN is analogous to the mixed phase following a first-order phase transition. In this mixed phase, the DNN generates useful internal representations of the teacher that are sharply distinct from those before the 
    
[^98]: HandMeThat: 在物理和社交环境中的人机交流

    HandMeThat: Human-Robot Communication in Physical and Social Environments. (arXiv:2310.03779v1 [cs.AI])

    [http://arxiv.org/abs/2310.03779](http://arxiv.org/abs/2310.03779)

    HandMeThat是一个综合评估基准，用于在物理和社交环境中理解和遵循人类指令。在该论文中，提出了一个包含10000个人机交互场景的数据集，并评估了不同的基准模型的表现。结果显示离线和在线强化学习算法在该基准上表现不佳，暗示了其中的挑战和困难。

    

    我们介绍了HandMeThat，一个用于物理和社交环境中指令理解和遵循的综合评估基准。与先前的数据集主要关注语言依存和规划不同，HandMeThat考虑了基于物理（物体状态和关系）和社交（人类行动和目标）信息的含有歧义的人类指令的解决方案。HandMeThat包含了10000个人机交互的场景。在每个场景中，机器人首先观察到人类行动的轨迹以达到内部目标。接下来，机器人接收到人类指令，并根据指令采取行动以完成子目标。在本文中，我们提出了一个用于我们基准测试的文本界面，机器人通过文本命令与虚拟环境交互。我们评估了HandMeThat上的几个基准模型，并显示离线和在线强化学习算法在HandMeThat上表现不佳，表明其中存在重要的挑战和困难。

    We introduce HandMeThat, a benchmark for a holistic evaluation of instruction understanding and following in physical and social environments. While previous datasets primarily focused on language grounding and planning, HandMeThat considers the resolution of human instructions with ambiguities based on the physical (object states and relations) and social (human actions and goals) information. HandMeThat contains 10,000 episodes of human-robot interactions. In each episode, the robot first observes a trajectory of human actions towards her internal goal. Next, the robot receives a human instruction and should take actions to accomplish the subgoal set through the instruction. In this paper, we present a textual interface for our benchmark, where the robot interacts with a virtual environment through textual commands. We evaluate several baseline models on HandMeThat, and show that both offline and online reinforcement learning algorithms perform poorly on HandMeThat, suggesting signif
    
[^99]: 使用对抗验证的轻量级增强模型进行用户响应预测

    Lightweight Boosting Models for User Response Prediction Using Adversarial Validation. (arXiv:2310.03778v1 [cs.LG])

    [http://arxiv.org/abs/2310.03778](http://arxiv.org/abs/2310.03778)

    这篇论文提出了一个轻量级的增强模型解决方案，通过使用对抗验证来消除非信息性特征，并通过特征工程技术处理有噪声的连续特征和具有大量唯一值的分类特征。实验证明，该方法在ACM RecSys Challenge 2023中取得了良好的性能。

    

    ShareChat组织的ACM RecSys Challenge 2023旨在预测应用被安装的概率。本文描述了对这个挑战的轻量级解决方案。我们将该任务定义为用户响应预测任务。为了快速原型设计，我们提出了一个包括以下步骤的轻量级解决方案：1）使用对抗验证，有效地从数据集中消除非信息性特征；2）为了处理有噪声的连续特征和具有大量唯一值的分类特征，我们采用了特征工程技术；3）我们利用梯度提升决策树（GBDT）的卓越性能和可伸缩性。实验证明，一个单独的LightGBM模型，在没有额外的集成的情况下，表现得很好。我们的团队在比赛中取得了第九名，最终排行榜得分为6.059065。我们的方法的代码可以在这里找到：https://github.com/choco9966/recsys-challenge-2023。

    The ACM RecSys Challenge 2023, organized by ShareChat, aims to predict the probability of the app being installed. This paper describes the lightweight solution to this challenge. We formulate the task as a user response prediction task. For rapid prototyping for the task, we propose a lightweight solution including the following steps: 1) using adversarial validation, we effectively eliminate uninformative features from a dataset; 2) to address noisy continuous features and categorical features with a large number of unique values, we employ feature engineering techniques.; 3) we leverage Gradient Boosted Decision Trees (GBDT) for their exceptional performance and scalability. The experiments show that a single LightGBM model, without additional ensembling, performs quite well. Our team achieved ninth place in the challenge with the final leaderboard score of 6.059065. Code for our approach can be found here: https://github.com/choco9966/recsys-challenge-2023.
    
[^100]: 使用卷积神经网络进行函数数据学习

    Functional data learning using convolutional neural networks. (arXiv:2310.03773v1 [cs.LG])

    [http://arxiv.org/abs/2310.03773](http://arxiv.org/abs/2310.03773)

    本文展示了使用卷积神经网络进行函数数据学习的方法。通过将函数数据转化为图像，利用特定的卷积神经网络架构进行参数估计和函数形式的分类学习。研究结果表明，这种方法在估计函数参数、分类函数形式以及混沌数据的Lyapunov指数估计中具有很高的准确性。

    

    本文展示了如何在有噪声和无噪声的函数数据的回归和分类学习问题中使用卷积神经网络（CNN）。主要思想是将函数数据转化为28x28的图像。我们使用了一个特定但典型的卷积神经网络架构来进行参数估计和函数形式分类的所有回归练习。首先，我们使用一些有噪声和无噪声的函数案例来展示这种新方法的优势。特别地，我们用它来估计指数增长和衰减率、正弦和余弦函数的带宽以及曲线峰值的幅度和宽度。我们还用它来分类函数数据的单调性和曲率、代数和指数增长、以及函数数据的峰值数。其次，我们将相同的卷积神经网络应用于有噪声和无噪声的混沌数据的Lyapunov指数估计。

    In this paper, we show how convolutional neural networks (CNN) can be used in regression and classification learning problems of noisy and non-noisy functional data. The main idea is to transform the functional data into a 28 by 28 image. We use a specific but typical architecture of a convolutional neural network to perform all the regression exercises of parameter estimation and functional form classification. First, we use some functional case studies of functional data with and without random noise to showcase the strength of the new method. In particular, we use it to estimate exponential growth and decay rates, the bandwidths of sine and cosine functions, and the magnitudes and widths of curve peaks. We also use it to classify the monotonicity and curvatures of functional data, algebraic versus exponential growth, and the number of peaks of functional data. Second, we apply the same convolutional neural networks to Lyapunov exponent estimation in noisy and non-noisy chaotic data,
    
[^101]: 探索临床记录表型的替代特征提取流程

    Investigating Alternative Feature Extraction Pipelines For Clinical Note Phenotyping. (arXiv:2310.03772v1 [cs.CL])

    [http://arxiv.org/abs/2310.03772](http://arxiv.org/abs/2310.03772)

    研究探索了临床记录表型的替代特征提取流程，使用基于BERT模型的方法将临床记录转换为文档表示，并传入LSTM模型，取得了明显的性能改进，但收敛时间较长且不支持增量训练。

    

    医疗行业常用的实践是使用临床记录，其中包含了详细的患者观察信息。然而，电子健康记录系统经常不以结构化格式包含这些观察结果，使得患者信息难以自动评估和评价。使用计算系统来提取医学属性具有许多应用，包括对患者的纵向分析、风险评估和医院评估。最近的研究已经构建了成功的表型方法：从临床记录中提取医学属性。基于BERT模型的方法可以将临床记录转换为一系列的表示，然后基于它们的CLS嵌入将其压缩为一个单一的文档表示，并传入一个LSTM模型。尽管这个流程相比之前的结果有明显的性能改进，但它需要很长的收敛时间。这个方法也不允许增量训练。

    A common practice in the medical industry is the use of clinical notes, which consist of detailed patient observations. However, electronic health record systems frequently do not contain these observations in a structured format, rendering patient information challenging to assess and evaluate automatically. Using computational systems for the extraction of medical attributes offers many applications, including longitudinal analysis of patients, risk assessment, and hospital evaluation. Recent work has constructed successful methods for phenotyping: extracting medical attributes from clinical notes. BERT-based models can be used to transform clinical notes into a series of representations, which are then condensed into a single document representation based on their CLS embeddings and passed into an LSTM (Mulyar et al., 2020). Though this pipeline yields a considerable performance improvement over previous results, it requires extensive convergence time. This method also does not allo
    
[^102]: 渐进降阶建模：用选择性知识传递增强数据驱动建模

    Progressive reduced order modeling: empowering data-driven modeling with selective knowledge transfer. (arXiv:2310.03770v1 [cs.LG])

    [http://arxiv.org/abs/2310.03770](http://arxiv.org/abs/2310.03770)

    本文提出了一种渐进降阶建模框架，通过选择性地从先前训练的模型中传递知识，减少数据需求，并且提高了数据驱动建模的实用性。实验结果表明，在多个案例中，保留先前模型的信息并利用其中有价值的部分可以提高建模的准确性。

    

    数据驱动建模可能面临对数据的不断需求，导致准确性降低，并且数据成本高昂、信息稀缺，对工程应用不实用。为了解决这个挑战，我们提出了一种渐进降阶建模框架，旨在减少对数据的需求并提高数据驱动建模的实用性。我们的方法通过类似于人类选择性使用有价值的知识而忽略无用信息的门控机制，有选择性地从先前训练的模型中传递知识。通过从先前模型中筛选出相关信息，我们可以创建一个具有最小转换时间和较小训练集的代理模型，仍然能够实现高准确性。我们在多个案例中测试了我们的框架，包括多孔介质中的传输、重力驱动流动和超弹材料的有限变形。我们的结果表明，保留先前模型的信息并利用其中的有价值部分可以提高建模的准确性。

    Data-driven modeling can suffer from a constant demand for data, leading to reduced accuracy and impractical for engineering applications due to the high cost and scarcity of information. To address this challenge, we propose a progressive reduced order modeling framework that minimizes data cravings and enhances data-driven modeling's practicality. Our approach selectively transfers knowledge from previously trained models through gates, similar to how humans selectively use valuable knowledge while ignoring unuseful information. By filtering relevant information from previous models, we can create a surrogate model with minimal turnaround time and a smaller training set that can still achieve high accuracy. We have tested our framework in several cases, including transport in porous media, gravity-driven flow, and finite deformation in hyperelastic materials. Our results illustrate that retaining information from previous models and utilizing a valuable portion of that knowledge can 
    
[^103]: 深度强化学习算法对混合V2X通信的基准研究

    Deep Reinforcement Learning Algorithms for Hybrid V2X Communication: A Benchmarking Study. (arXiv:2310.03767v1 [cs.LG])

    [http://arxiv.org/abs/2310.03767](http://arxiv.org/abs/2310.03767)

    这项研究通过使用深度强化学习算法，针对混合V2X通信中的垂直切换问题进行了研究，并提出了一种在弯曲环境中帮助车辆选择最合适的V2X技术的方法。研究结果表明，这些算法优于当前的状态。

    

    在当今时代，自动驾驶汽车要求与飞机的安全水平相当。借鉴航空航天工业的经验，通过在V2X（车到一切）技术中建立冗余性，汽车行业也可以利用这个概念来实现高可靠性。鉴于当前缺乏可靠的V2X技术，这个想法尤其有前景。通过同时部署多个无线电接入技术（RATs），未来车辆的标准技术的持续争论可以得到解决。然而，由于动态的、时变的信道和不断变化的交通情况，协调多种通信技术是一项复杂的任务。本文采用深度强化学习（DRL）算法解决了V2X中的垂直切换问题。目标是在弯曲环境中帮助车辆选择最合适的V2X技术（DSRC/V-VLC）。结果显示，基准算法优于当前状态

    In today's era, autonomous vehicles demand a safety level on par with aircraft. Taking a cue from the aerospace industry, which relies on redundancy to achieve high reliability, the automotive sector can also leverage this concept by building redundancy in V2X (Vehicle-to-Everything) technologies. Given the current lack of reliable V2X technologies, this idea is particularly promising. By deploying multiple RATs (Radio Access Technologies) in parallel, the ongoing debate over the standard technology for future vehicles can be put to rest. However, coordinating multiple communication technologies is a complex task due to dynamic, time-varying channels and varying traffic conditions. This paper addresses the vertical handover problem in V2X using Deep Reinforcement Learning (DRL) algorithms. The goal is to assist vehicles in selecting the most appropriate V2X technology (DSRC/V-VLC) in a serpentine environment. The results show that the benchmarked algorithms outperform the current state
    
[^104]: 深度神经网络架构和特征提取设计在基于传感器的人体活动识别中的研究

    Investigating Deep Neural Network Architecture and Feature Extraction Designs for Sensor-based Human Activity Recognition. (arXiv:2310.03760v1 [eess.SP])

    [http://arxiv.org/abs/2310.03760](http://arxiv.org/abs/2310.03760)

    本研究探讨了在基于传感器的人体活动识别中，深度神经网络架构和特征提取设计的应用。通过实验研究，发现深度学习方法在活动识别中超越了传统的信号处理和机器学习方法，并探索了不同的训练机制和特征表示对于人体活动识别的有效性。

    

    智能设备和物联网中传感器的广泛可用性为基于传感器的活动识别的实现打开了可能性。与传统的传感器时间序列处理和手工特征提取相反，鉴于深度学习在各个领域的证明有效性，已经探索了许多深度方法来解决活动识别中的挑战，并超过了传统的信号处理和传统的机器学习方法。在这项工作中，通过对两个人体活动识别数据集进行广泛的实验研究，我们研究了常见的深度学习和机器学习方法的性能，以及不同的训练机制（如对比学习）和从传感器时间序列数据中提取的各种特征表示，并测量它们在人体活动识别任务中的效果。

    The extensive ubiquitous availability of sensors in smart devices and the Internet of Things (IoT) has opened up the possibilities for implementing sensor-based activity recognition. As opposed to traditional sensor time-series processing and hand-engineered feature extraction, in light of deep learning's proven effectiveness across various domains, numerous deep methods have been explored to tackle the challenges in activity recognition, outperforming the traditional signal processing and traditional machine learning approaches. In this work, by performing extensive experimental studies on two human activity recognition datasets, we investigate the performance of common deep learning and machine learning approaches as well as different training mechanisms (such as contrastive learning), and various feature representations extracted from the sensor time-series data and measure their effectiveness for the human activity recognition task.
    
[^105]: 一个新颖的深度学习技术，用于保持胎儿心电图形态的从母体心电图中提取胎儿心电图的1D-CycleGAN

    A Novel Deep Learning Technique for Morphology Preserved Fetal ECG Extraction from Mother ECG using 1D-CycleGAN. (arXiv:2310.03759v1 [eess.SP])

    [http://arxiv.org/abs/2310.03759](http://arxiv.org/abs/2310.03759)

    通过1D-CycleGAN技术，我们提出了一种新颖的深度学习方法，可以从母体心电图中提取胎儿心电图，保持其形态特征，并且在实验证实了其性能。

    

    通过非侵入式的胎儿心电图（fECG）监测胎儿心脏的电脉冲可以轻松检测发育中心脏的异常，以显著降低婴儿死亡率和产后并发症。由于母体和胎儿R峰的重叠，fECG信号的低幅度，以及系统和环境噪声，传统的信号提取方法无法产生令人满意的fECG。虽然一些技术能够产生准确的QRS波，但它们经常忽略ECG的其他重要方面。我们的方法基于1D CycleGAN，通过广泛的预处理和适当的框架，可以从mECG信号重构fECG信号，并保持其形态。我们的解决方案的性能通过结合Physionet的两个可用数据集进行评估，即"Abdominal and Direct Fetal ECG Database"和"Fetal electrocardiograms, direct an"

    Monitoring the electrical pulse of fetal heart through a non-invasive fetal electrocardiogram (fECG) can easily detect abnormalities in the developing heart to significantly reduce the infant mortality rate and post-natal complications. Due to the overlapping of maternal and fetal R-peaks, the low amplitude of the fECG, systematic and ambient noises, typical signal extraction methods, such as adaptive filters, independent component analysis, empirical mode decomposition, etc., are unable to produce satisfactory fECG. While some techniques can produce accurate QRS waves, they often ignore other important aspects of the ECG. Our approach, which is based on 1D CycleGAN, can reconstruct the fECG signal from the mECG signal while maintaining the morphology due to extensive preprocessing and appropriate framework. The performance of our solution was evaluated by combining two available datasets from Physionet, "Abdominal and Direct Fetal ECG Database" and "Fetal electrocardiograms, direct an
    
[^106]: 非线性生成式压缩感知中统一信号恢复框架

    A Unified Framework for Uniform Signal Recovery in Nonlinear Generative Compressed Sensing. (arXiv:2310.03758v1 [eess.SP])

    [http://arxiv.org/abs/2310.03758](http://arxiv.org/abs/2310.03758)

    该论文提出了一个统一的框架，用于在非线性生成式压缩感知中实现统一的信号恢复。该框架适用于非线性和可能非连续或未知的观测模型，并且可以恢复生成模型中所有可能的信号。

    

    在生成式压缩感知中，我们希望使用生成先验从m个测量中（m≪n）恢复一个信号x∗∈Rn，其中G通常是一个L-Lipschitz连续的生成模型，B2k(r)表示Rk中的半径为r的ℓ2球。在非线性测量下，大多数先前的结果是非均匀的，即它们对于固定的x∗具有高概率，而不是对于所有的x∗同时成立。本文建立了一个统一的框架来推导非线性生成式压缩感知中的均匀恢复保证，并且适用于非线性和可能非连续或未知的观测模型。我们的框架包括了1位/均匀量化观测和单索引模型作为规范示例。具体来说，使用感知集合的单个实现和广义Lasso，所有的x∗∈G(B2k(r))可以恢复到一个el

    In generative compressed sensing (GCS), we want to recover a signal $\mathbf{x}^* \in \mathbb{R}^n$ from $m$ measurements ($m\ll n$) using a generative prior $\mathbf{x}^*\in G(\mathbb{B}_2^k(r))$, where $G$ is typically an $L$-Lipschitz continuous generative model and $\mathbb{B}_2^k(r)$ represents the radius-$r$ $\ell_2$-ball in $\mathbb{R}^k$. Under nonlinear measurements, most prior results are non-uniform, i.e., they hold with high probability for a fixed $\mathbf{x}^*$ rather than for all $\mathbf{x}^*$ simultaneously. In this paper, we build a unified framework to derive uniform recovery guarantees for nonlinear GCS where the observation model is nonlinear and possibly discontinuous or unknown. Our framework accommodates GCS with 1-bit/uniformly quantized observations and single index models as canonical examples. Specifically, using a single realization of the sensing ensemble and generalized Lasso, {\em all} $\mathbf{x}^*\in G(\mathbb{B}_2^k(r))$ can be recovered up to an $\el
    
[^107]: 用EOG增强医疗保健：一种用于睡眠阶段分类的新方法

    Enhancing Healthcare with EOG: A Novel Approach to Sleep Stage Classification. (arXiv:2310.03757v1 [eess.SP])

    [http://arxiv.org/abs/2310.03757](http://arxiv.org/abs/2310.03757)

    本论文介绍了一种新颖的方法，利用EOG信号进行自动化睡眠阶段分类，并改进了睡眠障碍研究中对REM睡眠的识别。我们的提出的SE-Resnet-Transformer模型在多个数据库上得到了验证，并取得了显著的性能。这一发展将提高睡眠阶段分类的可访问性，减少对EEG设备的需求。

    

    我们介绍了一种创新的方法，利用EOG信号进行自动化的睡眠阶段分类，解决了EEG数据采集带来的不适和不实用性问题。此外，值得注意的是，这种方法在领域中尚未被开发，突显了其对新见解和贡献的潜力。我们提出的SE-Resnet-Transformer模型能够从原始EOG信号准确分类出五个不同的睡眠阶段。对公开可用的数据库（SleepEDF-20、SleepEDF-78和SHHS）进行了广泛验证，显示出显著的性能，分别为74.72、70.63和69.26的宏F1分数。我们的模型在识别REM睡眠方面表现出色，这是睡眠障碍研究的关键方面。我们还使用1D-GradCAM和t-SNE图等技术对我们模型的内部机制进行了分析。我们的方法改进了睡眠阶段分类的可访问性，同时减少了对EEG设备的需求。这一发展具有前景。

    We introduce an innovative approach to automated sleep stage classification using EOG signals, addressing the discomfort and impracticality associated with EEG data acquisition. In addition, it is important to note that this approach is untapped in the field, highlighting its potential for novel insights and contributions. Our proposed SE-Resnet-Transformer model provides an accurate classification of five distinct sleep stages from raw EOG signal. Extensive validation on publically available databases (SleepEDF-20, SleepEDF-78, and SHHS) reveals noteworthy performance, with macro-F1 scores of 74.72, 70.63, and 69.26, respectively. Our model excels in identifying REM sleep, a crucial aspect of sleep disorder investigations. We also provide insight into the internal mechanisms of our model using techniques such as 1D-GradCAM and t-SNE plots. Our method improves the accessibility of sleep stage classification while decreasing the need for EEG modalities. This development will have promis
    
[^108]: 自我和跨通道注意机制下，用于昏迷患者的多通道脑电图数据分析的富贫预测性研究

    A Multi-channel EEG Data Analysis for Poor Neuro-prognostication in Comatose Patients with Self and Cross-channel Attention Mechanism. (arXiv:2310.03756v1 [eess.SP])

    [http://arxiv.org/abs/2310.03756](http://arxiv.org/abs/2310.03756)

    本研究提出了一个基于深度学习的多通道脑电图数据分析方法，旨在预测昏迷患者的低病理性神经结果。该方法采用双极EEG记录和注意机制，以提高预测效果。实验结果表明，该方法在预测挑战中取得了良好的表现。

    

    本研究探讨了双极脑电图（EEG）记录对预测低病理性神经结果的潜力。采用回顾性设计和混合深度学习方法，优化了一个目标函数，以实现高特异性（即真阳性率（TPR））和降低假阳性率（< 0.05）。选取随机选择的一个小时内的5分钟段落的18个双极通道对的多通道EEG阵列。为了确定结果预测，使用了特征编码器与1-D卷积层、可学习的位置编码、带有注意机制的上下文网络，以及回归器和分类器模块的组合。特征编码器提取局部时间和空间特征，而后续的位置编码和注意机制试图捕捉全局时间依赖性。结果：我们团队提出的OUS IVS框架，在验证挑战中获得了良好的表现。

    This work investigates the predictive potential of bipolar electroencephalogram (EEG) recordings towards efficient prediction of poor neurological outcomes. A retrospective design using a hybrid deep learning approach is utilized to optimize an objective function aiming for high specificity, i.e., true positive rate (TPR) with reduced false positives (< 0.05). A multi-channel EEG array of 18 bipolar channel pairs from a randomly selected 5-minute segment in an hour is kept. In order to determine the outcome prediction, a combination of a feature encoder with 1-D convolutional layers, learnable position encoding, a context network with attention mechanisms, and finally, a regressor and classifier blocks are used. The feature encoder extricates local temporal and spatial features, while the following position encoding and attention mechanisms attempt to capture global temporal dependencies. Results: The proposed framework by our team, OUS IVS, when validated on the challenge hidden valid
    
[^109]: 适用于谷歌Colab的二维瞬态问题中的物理信息神经网络代码（PINN-2DT）

    Physics Informed Neural Network Code for 2D Transient Problems (PINN-2DT) Compatible with Google Colab. (arXiv:2310.03755v1 [cs.CE])

    [http://arxiv.org/abs/2310.03755](http://arxiv.org/abs/2310.03755)

    这个论文提出了一个适用于谷歌Colab的开源物理信息神经网络环境，可用于在二维矩形域上模拟瞬态现象，并提供了多个特性和问题库。

    

    我们提出了一个开源的物理信息神经网络环境，用于在二维矩形域上模拟瞬态现象，具有以下特点：（1）与谷歌Colab兼容，可以在云环境中进行自动执行；（2）支持二维时变PDE；（3）提供简单的界面来定义残留损失、边界条件和初始损失，以及它们的权重；（4）支持诺依曼和迪利克雷边界条件；（5）允许自定义层数和每层的神经元数量，以及任意激活函数；（6）学习率和迭代次数可以作为参数调节；（7）对空间和时间变量的PINN进行自动求导；（8）提供了绘制收敛性（具有滑动平均）、学习到的初始条件、模拟的二维和三维快照以及视频的常规函数；（9）包含了一个问题库。

    We present an open-source Physics Informed Neural Network environment for simulations of transient phenomena on two-dimensional rectangular domains, with the following features: (1) it is compatible with Google Colab which allows automatic execution on cloud environment; (2) it supports two dimensional time-dependent PDEs; (3) it provides simple interface for definition of the residual loss, boundary condition and initial loss, together with their weights; (4) it support Neumann and Dirichlet boundary conditions; (5) it allows for customizing the number of layers and neurons per layer, as well as for arbitrary activation function; (6) the learning rate and number of epochs are available as parameters; (7) it automatically differentiates PINN with respect to spatial and temporal variables; (8) it provides routines for plotting the convergence (with running average), initial conditions learnt, 2D and 3D snapshots from the simulation and movies (9) it includes a library of problems: (a) n
    
[^110]: EMGTFNet：用于解码上肢表面肌电信号的模糊视觉变压器进行手势识别

    EMGTFNet: Fuzzy Vision Transformer to decode Upperlimb sEMG signals for Hand Gestures Recognition. (arXiv:2310.03754v1 [eess.SP])

    [http://arxiv.org/abs/2310.03754](http://arxiv.org/abs/2310.03754)

    本文提出了一种名为EMGTFNet的基于模糊视觉变压器的架构，用于通过表面肌电信号进行手势识别，可以准确分类各种手势，无需数据增强技术或网络参数的增加。

    

    肌电控制是电肌图的一个日益受关注的领域，特别是在仿生假肢的手势识别等应用中。目前的重点是使用机器学习和最近的深度学习方法进行模式识别。尽管这些模型在稀疏的表面肌电信号中取得了良好的结果，但通常需要大型数据集和训练时间。此外，由于随机表面肌电信号的特性，传统模型无法将样本推广到非典型或噪声值。在本文中，我们提出了基于视觉变压器(ViT)和模糊神经块(FNB)的EMGTFNet设计，用于通过表面肌电信号进行手势识别。所提出的EMGTFNet架构可以准确地分类各种手势，而无需使用数据增强技术、迁移学习或网络参数的大幅增加。

    Myoelectric control is an area of electromyography of increasing interest nowadays, particularly in applications such as Hand Gesture Recognition (HGR) for bionic prostheses. Today's focus is on pattern recognition using Machine Learning and, more recently, Deep Learning methods. Despite achieving good results on sparse sEMG signals, the latter models typically require large datasets and training times. Furthermore, due to the nature of stochastic sEMG signals, traditional models fail to generalize samples for atypical or noisy values. In this paper, we propose the design of a Vision Transformer (ViT) based architecture with a Fuzzy Neural Block (FNB) called EMGTFNet to perform Hand Gesture Recognition from surface electromyography (sEMG) signals. The proposed EMGTFNet architecture can accurately classify a variety of hand gestures without any need for data augmentation techniques, transfer learning or a significant increase in the number of parameters in the network. The accuracy of t
    
[^111]: ECGNet：一种使用生成对抗网络 (GAN) 从单导联输入中合成12导联心电图信号的方法

    ECGNet: A generative adversarial network (GAN) approach to the synthesis of 12-lead ECG signals from single lead inputs. (arXiv:2310.03753v1 [eess.SP])

    [http://arxiv.org/abs/2310.03753](http://arxiv.org/abs/2310.03753)

    ECGNet使用生成对抗网络 (GAN) 从单导联输入合成12导联心电图信号，并通过特征分析识别出可用于心血管疾病预测的特征。

    

    使用生成对抗网络 (GAN) 进行心电图 (ECG) 信号合成已经得到广泛研究，因为实现12导联心电图并不总是可行的。GAN模型已经取得了显著的成果，但仅针对多导联输入进行设计，并且尚未确定GAN模型所保留的特征，限制了生成信号在心血管疾病预测模型中的应用。本文介绍了ECGNet，它是一种使用GAN框架、具有双向长短期记忆 (LSTM) 生成器和卷积神经网络 (CNN) 判别器的方法，从任何单导联输入生成完整的12导联心电图信号。对生成的信号进行交叉和自相关分析，识别出信号生成过程中保留的特征，即能够表征每个信号独特性的特征，因此很可能是心血管疾病的指标。最后，通过使用标注有心电图信号的数据集，证明了ECGNet的有效性。

    Electrocardiography (ECG) signal generation has been heavily explored using generative adversarial networks (GAN) because the implementation of 12-lead ECGs is not always feasible. The GAN models have achieved remarkable results in reproducing ECG signals but are only designed for multiple lead inputs and the features the GAN model preserves have not been identified-limiting the generated signals use in cardiovascular disease (CVD)-predictive models. This paper presents ECGNet which is a procedure that generates a complete set of 12-lead ECG signals from any single lead input using a GAN framework with a bidirectional long short-term memory (LSTM) generator and a convolutional neural network (CNN) discriminator. Cross and auto-correlation analysis performed on the generated signals identifies features conserved during the signal generation-i.e., features that can characterize the unique-nature of each signal and thus likely indicators of CVD. Finally, by using ECG signals annotated wit
    
[^112]: 使用主题嵌入迁移学习的瞬态高密度肌电图手势识别的深度学习序列解码器

    A Deep Learning Sequential Decoder for Transient High-Density Electromyography in Hand Gesture Recognition Using Subject-Embedded Transfer Learning. (arXiv:2310.03752v1 [eess.SP])

    [http://arxiv.org/abs/2310.03752](http://arxiv.org/abs/2310.03752)

    本文提出了一个基于主题嵌入迁移学习的深度学习序列解码器，用于瞬态高密度肌电图手势识别。该解码器在部分观察的受试者中取得了73%的平均准确率。

    

    手势识别(HGR)由于人工智能驱动的人机界面对外周神经系统的生物信号(如表面肌电图(sEMG))进行深度时空动力学解读的增加而引起了重视。这些界面具有广泛的应用，包括扩展现实的控制、灵活的假肢和外骨骼。然而，sEMG在个体之间的自然变异性导致研究人员专注于个体特定的解决方案。深度学习方法通常具有复杂的结构，对数据需求较多，并且训练时间较长，这使得它们对于个体特定应用的实际性较低。在本文中，我们提出并开发了一个可泛化的瞬态高密度sEMG(HD-sEMG)的序列解码器，通过主题嵌入迁移学习，在部分观察的受试者中实现了65个手势的平均准确率为73%。

    Hand gesture recognition (HGR) has gained significant attention due to the increasing use of AI-powered human-computer interfaces that can interpret the deep spatiotemporal dynamics of biosignals from the peripheral nervous system, such as surface electromyography (sEMG). These interfaces have a range of applications, including the control of extended reality, agile prosthetics, and exoskeletons. However, the natural variability of sEMG among individuals has led researchers to focus on subject-specific solutions. Deep learning methods, which often have complex structures, are particularly data-hungry and can be time-consuming to train, making them less practical for subject-specific applications. In this paper, we propose and develop a generalizable, sequential decoder of transient high-density sEMG (HD-sEMG) that achieves 73% average accuracy on 65 gestures for partially-observed subjects through subject-embedded transfer learning, leveraging pre-knowledge of HGR acquired during pre-t
    
[^113]: 线性最小二乘问题中使用卡尔曼滤波器进行交叉学习的简单示例

    A Simple Illustration of Interleaved Learning using Kalman Filter for Linear Least Squares. (arXiv:2310.03751v1 [eess.SP])

    [http://arxiv.org/abs/2310.03751](http://arxiv.org/abs/2310.03751)

    本论文通过使用卡尔曼滤波器和线性最小二乘法的简单框架，演示了交叉学习的机制。

    

    交叉学习是一种受生物启发的训练方法，在机器学习算法中有着很好的结果。在这个简短的说明中，我们通过基于卡尔曼滤波器和线性最小二乘法的简单统计和优化框架，来演示交叉学习的机制。

    Interleaved learning in machine learning algorithms is a biologically inspired training method with promising results. In this short note, we illustrate the interleaving mechanism via a simple statistical and optimization framework based on Kalman Filter for Linear Least Squares.
    
[^114]: 采用数据分析和等效电路建模的老化锂离子电池健康诊断和恢复

    Health diagnosis and recuperation of aged Li-ion batteries with data analytics and equivalent circuit modeling. (arXiv:2310.03750v1 [eess.SP])

    [http://arxiv.org/abs/2310.03750](http://arxiv.org/abs/2310.03750)

    本论文通过对商业高能型磷酸铁锂电池的老化和修复实验，使用机器学习模型预测循环寿命并识别可恢复容量的重要指标。考虑到电池间的不一致性，平均循环寿命预测误差为16.84% ± 1.87%，为电池健康评估和恢复提供了有力的支持。

    

    电池健康评估和恢复在利用二次寿命锂离子电池中起着关键作用。然而，由于老化机制不明确和恢复效果与操作状态缺乏相关性，准确估计电池健康并制定清晰的电池恢复策略是具有挑战性的。本文通过对62个商业高能型磷酸铁锂 (LFP) 电池的老化和修复实验，补充了现有高功率LFP电池的数据集。相对较大规模的数据使我们能够使用机器学习模型来预测循环寿命并识别可恢复容量的重要指标。考虑到电池间的不一致性，通过梯度提升回归器给出的从前80个周期的信息中获得的循环寿命预测平均测试误差为$16.84\% \pm 1.87\%$ (平均绝对百分比误差)。此外，发现一些可恢复的损失容量与...（文本不完整）

    Battery health assessment and recuperation play a crucial role in the utilization of second-life Li-ion batteries. However, due to ambiguous aging mechanisms and lack of correlations between the recovery effects and operational states, it is challenging to accurately estimate battery health and devise a clear strategy for cell rejuvenation. This paper presents aging and reconditioning experiments of 62 commercial high-energy type lithium iron phosphate (LFP) cells, which supplement existing datasets of high-power LFP cells. The relatively large-scale data allow us to use machine learning models to predict cycle life and identify important indicators of recoverable capacity. Considering cell-to-cell inconsistencies, an average test error of $16.84\% \pm 1.87\%$ (mean absolute percentage error) for cycle life prediction is achieved by gradient boosting regressor given information from the first 80 cycles. In addition, it is found that some of the recoverable lost capacity is attributed t
    
[^115]: SCVCNet: 用于跨任务和个体间脑电认知负荷识别的滑动交叉向量卷积网络

    SCVCNet: Sliding cross-vector convolution network for cross-task and inter-individual-set EEG-based cognitive workload recognition. (arXiv:2310.03749v1 [eess.SP])

    [http://arxiv.org/abs/2310.03749](http://arxiv.org/abs/2310.03749)

    本文提出了SCVCNet神经网络，通过分析脑电图中的细粒度频率结构来消除任务和个体集相关的干扰，实现了跨任务和个体间的脑电认知负荷识别。

    

    本文提出了一种通用方法，通过利用不同人机任务和个体集上的常见脑电图（EEG）模式来应用认知负荷识别器。我们提出了一种名为SCVCNet的神经网络，通过分析功率谱密度中更精细的频率结构来消除脑电图中的任务和个体集相关干扰。SCVCNet利用滑动交叉向量卷积（SCVC）操作，其中使用代表theta和alpha功率的配对输入层。通过提取核矩阵的中央行和列的权重，我们计算指定头皮位置周围两个向量的加权和。接下来，我们引入了一个频率点间特征融合模块来融合SCVC特征图。最后，我们将这两个模块与输出通道池化和分类层组合起来构建模型。为了训练SCVCNet，我们使用正则化最小二乘法方法。

    This paper presents a generic approach for applying the cognitive workload recognizer by exploiting common electroencephalogram (EEG) patterns across different human-machine tasks and individual sets. We propose a neural network called SCVCNet, which eliminates task- and individual-set-related interferences in EEGs by analyzing finer-grained frequency structures in the power spectral densities. The SCVCNet utilizes a sliding cross-vector convolution (SCVC) operation, where paired input layers representing the theta and alpha power are employed. By extracting the weights from a kernel matrix's central row and column, we compute the weighted sum of the two vectors around a specified scalp location. Next, we introduce an inter-frequency-point feature integration module to fuse the SCVC feature maps. Finally, we combined the two modules with the output-channel pooling and classification layers to construct the model. To train the SCVCNet, we employ the regularized least-square method with 
    
[^116]: 脑机接口中的相位同步成分自组织

    Phase Synchrony Component Self-Organization in Brain Computer Interface. (arXiv:2310.03748v1 [eess.SP])

    [http://arxiv.org/abs/2310.03748](http://arxiv.org/abs/2310.03748)

    本文提出了相位同步成分自组织的概念，利用深度学习端到端网络自动化脑机接口中的预处理和通道选择，从而提高了分析功能性脑连接和识别脑活动的效果。

    

    相位同步信息在分析功能性脑连接和识别脑活动中起着关键作用。目前广泛采用的特征提取流程由预处理、选择脑电采集通道和相位锁定值（PLV）计算组成，在运动想象分类方面取得了成功。然而，该流程是手动的且依赖专家知识，限制了它在不同应用场景中的便利性和适应性。此外，大多数研究都采用了一般的与数据无关的空间滤波器来抑制噪声，阻碍了更重要的相位同步现象的探索。为了解决这些问题，我们提出了相位同步成分自组织的概念，它能够自适应地学习数据相关的空间滤波器，从而自动化预处理和通道选择程序。基于这个概念，我们开发了第一个深度学习端到端网络，直接提取

    Phase synchrony information plays a crucial role in analyzing functional brain connectivity and identifying brain activities. A widely adopted feature extraction pipeline, composed of preprocessing, selection of EEG acquisition channels, and phase locking value (PLV) calculation, has achieved success in motor imagery classification (MI). However, this pipeline is manual and reliant on expert knowledge, limiting its convenience and adaptability to different application scenarios. Moreover, most studies have employed mediocre data-independent spatial filters to suppress noise, impeding the exploration of more significant phase synchronization phenomena. To address the issues, we propose the concept of phase synchrony component self-organization, which enables the adaptive learning of data-dependent spatial filters for automating both the preprocessing and channel selection procedures. Based on this concept, the first deep learning end-to-end network is developed, which directly extracts 
    
[^117]: 基于知识驱动的 EEG 表示的跨视图对比学习

    A Knowledge-Driven Cross-view Contrastive Learning for EEG Representation. (arXiv:2310.03747v1 [eess.SP])

    [http://arxiv.org/abs/2310.03747](http://arxiv.org/abs/2310.03747)

    本文提出了一种基于知识驱动的 EEG 表示的跨视图对比学习框架 (KDC2)，通过模拟脑活动的内部和外部表示，从有限标签的 EEG 中提取有效表示，并通过视图和跨视图对比学习进行特征提取和表示学习。

    

    由于脑电图 (EEG) 信号中丰富的神经生理信息，将 EEG 信号与深度学习方法相结合已在许多实际任务中取得了显著的进展。然而，基于 EEG 信号的监督学习方法的发展受到了高昂的成本和大规模 EEG 数据集手动标记的显著标签不一致性的阻碍。自监督框架在视觉和语言领域得到了采用，以解决这个问题，但缺乏 EEG 特定的理论基础限制了它们在不同任务中的适用性。为了解决这些挑战，本文提出了一种基于知识驱动的跨视图对比学习框架 (KDC2)，它将神经学理论与有限标签的 EEG 中提取有效表示相结合。KDC2 方法创建了 EEG 信号的头皮和神经视图，模拟了脑活动的内部和外部表示。随后，通过视图和跨视图对比学习逐步完成特征提取和表示学习。

    Due to the abundant neurophysiological information in the electroencephalogram (EEG) signal, EEG signals integrated with deep learning methods have gained substantial traction across numerous real-world tasks. However, the development of supervised learning methods based on EEG signals has been hindered by the high cost and significant label discrepancies to manually label large-scale EEG datasets. Self-supervised frameworks are adopted in vision and language fields to solve this issue, but the lack of EEG-specific theoretical foundations hampers their applicability across various tasks. To solve these challenges, this paper proposes a knowledge-driven cross-view contrastive learning framework (KDC2), which integrates neurological theory to extract effective representations from EEG with limited labels. The KDC2 method creates scalp and neural views of EEG signals, simulating the internal and external representation of brain activity. Sequentially, inter-view and cross-view contrastive
    
[^118]: 具有物理知识约束的概率扩散领域的生成式超弹性

    Generative Hyperelasticity with Physics-Informed Probabilistic Diffusion Fields. (arXiv:2310.03745v1 [cs.CE])

    [http://arxiv.org/abs/2310.03745](http://arxiv.org/abs/2310.03745)

    该论文中介绍了一种基于生成模型的方法，利用神经常微分方程和概率扩散模型生成具有物理知识约束的超弹性材料应变能函数的新样本。

    

    许多自然材料表现出高度复杂、非线性、各向异性和异质性的力学特性。最近，已经证明数据驱动的应变能函数具有灵活性，可以以高精度捕捉这些复杂材料的行为，同时满足基于物理的约束条件。然而，大多数这些方法忽略了估计的不确定性和材料的空间异质性。在这项工作中，我们利用生成模型的最新进展来解决这些问题。我们使用神经常微分方程（NODE）作为建模基础，通过构造可以创建多凸应变能函数，这是实际的超弹性材料模型的关键特性。我们结合这种方法与概率扩散模型生成应变能函数的新样本。这种技术允许我们对高斯白噪声进行采样并将其转换为NODE参数，从而代表可行的应变能函数。

    Many natural materials exhibit highly complex, nonlinear, anisotropic, and heterogeneous mechanical properties. Recently, it has been demonstrated that data-driven strain energy functions possess the flexibility to capture the behavior of these complex materials with high accuracy while satisfying physics-based constraints. However, most of these approaches disregard the uncertainty in the estimates and the spatial heterogeneity of these materials. In this work, we leverage recent advances in generative models to address these issues. We use as building block neural ordinary equations (NODE) that -- by construction -- create polyconvex strain energy functions, a key property of realistic hyperelastic material models. We combine this approach with probabilistic diffusion models to generate new samples of strain energy functions. This technique allows us to sample a vector of Gaussian white noise and translate it to NODE parameters thereby representing plausible strain energy functions. 
    
[^119]: GENER:一种并行层深度学习网络用于从基因表达数据中检测基因-基因相互作用

    GENER: A Parallel Layer Deep Learning Network To Detect Gene-Gene Interactions From Gene Expression Data. (arXiv:2310.03611v1 [cs.LG])

    [http://arxiv.org/abs/2310.03611](http://arxiv.org/abs/2310.03611)

    GENER是一个并行层深度学习网络，专门用于从基因表达数据中检测基因-基因相互作用。实验证实GENER在预测基因-基因相互作用方面的性能优于其他统计和深度学习方法。

    

    基于已知基因表达和基因相互作用数据来检测和发现新的基因相互作用是一个重大挑战。各种统计和深度学习方法已经尝试通过利用基因相互作用的拓扑结构和基因表达模式来预测新的基因相互作用。相反，一些方法专注于利用基因表达数据。在这个背景下，我们介绍了GENER，这是一个并行层深度学习网络，专门用于使用基因表达数据来识别基因-基因关系。我们进行了两个训练实验，并将我们的网络的性能与现有的统计和深度学习方法进行了比较。值得注意的是，我们的模型在结合了BioGRID&DREAM5数据集上实现了0.834的平均AUROC分数，优于其他方法在预测基因-基因相互作用方面的表现。

    Detecting and discovering new gene interactions based on known gene expressions and gene interaction data presents a significant challenge. Various statistical and deep learning methods have attempted to tackle this challenge by leveraging the topological structure of gene interactions and gene expression patterns to predict novel gene interactions. In contrast, some approaches have focused exclusively on utilizing gene expression profiles. In this context, we introduce GENER, a parallel-layer deep learning network designed exclusively for the identification of gene-gene relationships using gene expression data. We conducted two training experiments and compared the performance of our network with that of existing statistical and deep learning approaches. Notably, our model achieved an average AUROC score of 0.834 on the combined BioGRID&DREAM5 dataset, outperforming competing methods in predicting gene-gene interactions.
    
[^120]: FASER: 通过中间表示进行二进制代码相似性搜索

    FASER: Binary Code Similarity Search through the use of Intermediate Representations. (arXiv:2310.03605v1 [cs.CR])

    [http://arxiv.org/abs/2310.03605](http://arxiv.org/abs/2310.03605)

    本论文提出了一种名为FASER的方法，通过使用中间表示进行二进制代码相似性搜索。该方法可以跨架构地识别函数，并明确编码函数的语义，以支持各种应用场景。

    

    能够识别跨架构软件中感兴趣的函数对于分析恶意软件、保护软件供应链或进行漏洞研究都是有用的。跨架构二进制代码相似性搜索已在许多研究中探索，并使用了各种不同的数据来源来实现其目标。通常使用的数据来源包括从二进制文件中提取的常见结构，如函数控制流图或二进制级调用图，反汇编过程的输出或动态分析方法的输出。其中一种受到较少关注的数据来源是二进制中间表示。二进制中间表示具有两个有趣的属性：它们的跨架构性质以及明确编码函数的语义以支持下游使用。在本文中，我们提出了一种名为FASER的函数字符串编码表示方法，它结合了长文档转换技术。

    Being able to identify functions of interest in cross-architecture software is useful whether you are analysing for malware, securing the software supply chain or conducting vulnerability research. Cross-Architecture Binary Code Similarity Search has been explored in numerous studies and has used a wide range of different data sources to achieve its goals. The data sources typically used draw on common structures derived from binaries such as function control flow graphs or binary level call graphs, the output of the disassembly process or the outputs of a dynamic analysis approach. One data source which has received less attention is binary intermediate representations. Binary Intermediate representations possess two interesting properties: they are cross architecture by their very nature and encode the semantics of a function explicitly to support downstream usage. Within this paper we propose Function as a String Encoded Representation (FASER) which combines long document transforme
    
[^121]: 一种用于解码mRNA的5' UTR语言模型和功能预测的研究

    A 5' UTR Language Model for Decoding Untranslated Regions of mRNA and Function Predictions. (arXiv:2310.03281v1 [cs.LG])

    [http://arxiv.org/abs/2310.03281](http://arxiv.org/abs/2310.03281)

    这种研究引入了一种新的语言模型UTR-LM，通过对多个物种的5' UTR进行预训练，并结合有监督信息，该模型在多个下游任务中的表现超过了现有的最佳模型，可以有效预测平均核糖体负载、翻译效率、mRNA表达水平，并改进了内源性核糖体进入位点的识别性能。

    

    5' UTR是mRNA分子开端的调控区域，在调控翻译过程和影响蛋白表达水平方面起着关键作用。语言模型已经展示了在解码蛋白质和基因组序列功能方面的有效性。在这里，我们引入了一种用于5' UTR的语言模型，称为UTR-LM。UTR-LM在多个物种的内源性5' UTR上进行了预训练，并进一步加入了包括二级结构和最小自由能在内的有监督信息。我们对UTR-LM进行了各种下游任务的微调。模型在预测平均核糖体负载上的表现超过了已知的最佳基准模型最多42%，同时在预测翻译效率和mRNA表达水平上的表现提升了最多60%。该模型还可以用于识别未注释的内源性核糖体进入位点，并将AUPR与最佳基准模型相比从0.37提高至0.52。此外，我们设计了一个...

    The 5' UTR, a regulatory region at the beginning of an mRNA molecule, plays a crucial role in regulating the translation process and impacts the protein expression level. Language models have showcased their effectiveness in decoding the functions of protein and genome sequences. Here, we introduced a language model for 5' UTR, which we refer to as the UTR-LM. The UTR-LM is pre-trained on endogenous 5' UTRs from multiple species and is further augmented with supervised information including secondary structure and minimum free energy. We fine-tuned the UTR-LM in a variety of downstream tasks. The model outperformed the best-known benchmark by up to 42% for predicting the Mean Ribosome Loading, and by up to 60% for predicting the Translation Efficiency and the mRNA Expression Level. The model also applies to identifying unannotated Internal Ribosome Entry Sites within the untranslated region and improves the AUPR from 0.37 to 0.52 compared to the best baseline. Further, we designed a li
    
[^122]: FedHyper:一种用于联邦学习的通用和稳健学习率调度器与超梯度下降

    FedHyper: A Universal and Robust Learning Rate Scheduler for Federated Learning with Hypergradient Descent. (arXiv:2310.03156v1 [cs.LG])

    [http://arxiv.org/abs/2310.03156](http://arxiv.org/abs/2310.03156)

    FedHyper是一种适用于联邦学习的通用和稳健学习率调度器，通过超梯度下降算法实现对全局和局部学习率的自适应调整，能够显著提高联邦学习系统的效果，同时减少了对经验调整的需求。

    

    联邦学习（FL）的理论框架正在迅速发展，但其实际应用面临一系列复杂挑战，其中超参数优化是其中一个关键挑战。在众多超参数调整中，学习率的适应性成为一个重要组成部分，有望显著提高FL系统的效果。为了满足这一关键需求，本文提出了FedHyper，一种专为FL设计的基于超梯度的学习率调整算法。FedHyper作为一种通用的学习率调度器，可以随着训练的进行调整全局和局部的学习率。此外，FedHyper不仅展示了对各种初始学习率配置的无与伦比的稳健性，还极大地减少了繁琐的经验性学习率调整的必要性。我们提供了FedHyper收敛速度的全面理论分析。

    The theoretical landscape of federated learning (FL) undergoes rapid evolution, but its practical application encounters a series of intricate challenges, and hyperparameter optimization is one of these critical challenges. Amongst the diverse adjustments in hyperparameters, the adaptation of the learning rate emerges as a crucial component, holding the promise of significantly enhancing the efficacy of FL systems. In response to this critical need, this paper presents FedHyper, a novel hypergradient-based learning rate adaptation algorithm specifically designed for FL. FedHyper serves as a universal learning rate scheduler that can adapt both global and local rates as the training progresses. In addition, FedHyper not only showcases unparalleled robustness to a spectrum of initial learning rate configurations but also significantly alleviates the necessity for laborious empirical learning rate adjustments. We provide a comprehensive theoretical analysis of FedHyper's convergence rate 
    
[^123]: 将神经网络中学习到的概念归因于训练数据

    Attributing Learned Concepts in Neural Networks to Training Data. (arXiv:2310.03149v1 [cs.LG])

    [http://arxiv.org/abs/2310.03149](http://arxiv.org/abs/2310.03149)

    通过将数据归因方法与概念探测方法相结合，研究了神经网络中学习到的概念与训练数据的关系，并发现概念的位置和稀疏性并不完全依赖于少量特定示例。

    

    现在有大量的证据表明，深度学习模型学习到了某些可解释的人类特征，作为其对数据的内部表示的一部分。由于拥有正确（或错误）的概念对于可信赖的机器学习系统至关重要，自然而然地我们想要知道在给定层次上，模型原始训练集中的哪些输入对于学习一个概念最为重要。为了回答这个问题，我们将数据归因方法与探测模型学习到的概念的方法相结合。通过在一系列网络层次上训练网络和探测模型，并使用最近开发的用于大规模数据归因的TRAK方法，我们对两个概念数据集进行训练网络和探测模型的集合。我们发现一些证据表明，通过移除对一个概念具有最高归因的前10000张图像并重新训练模型，概念在网络中的位置以及概念的探测稀疏性并没有发生改变。这表明，与依赖于少量特定示例不同，用于确定概念的特征具有较高的独立性。

    By now there is substantial evidence that deep learning models learn certain human-interpretable features as part of their internal representations of data. As having the right (or wrong) concepts is critical to trustworthy machine learning systems, it is natural to ask which inputs from the model's original training set were most important for learning a concept at a given layer. To answer this, we combine data attribution methods with methods for probing the concepts learned by a model. Training network and probe ensembles for two concept datasets on a range of network layers, we use the recently developed TRAK method for large-scale data attribution. We find some evidence for convergence, where removing the 10,000 top attributing images for a concept and retraining the model does not change the location of the concept in the network nor the probing sparsity of the concept. This suggests that rather than being highly dependent on a few specific examples, the features that inform the 
    
[^124]: GPT-MolBERTa：用于分子性质预测的GPT分子特征语言模型

    GPT-MolBERTa: GPT Molecular Features Language Model for molecular property prediction. (arXiv:2310.03030v1 [physics.chem-ph])

    [http://arxiv.org/abs/2310.03030](http://arxiv.org/abs/2310.03030)

    GPT-MolBERTa是一种用于分子性质预测的自监督大型语言模型，通过使用分子的详细文本描述来学习分子的表示，实验表明其在各种分子属性基准上表现良好，并在回归任务中接近最先进的性能。

    

    随着Transformer架构的出现及其对文本数据的强大理解能力，基于文本描述预测分子属性的新领域已经开启。尽管SMILES是最常见的表示形式，但它们缺乏健壮性、丰富信息和规范性，限制了它们成为可推广表示的有效性。在这里，我们提出了GPT-MolBERTa，一种自监督大型语言模型（LLM），它使用分子的详细文本描述来预测其性质。使用ChatGPT收集了326000个分子的基于文本的描述，并用于训练LLM来学习分子的表示。为了预测下游任务的性能，细调阶段使用了BERT和RoBERTa模型。实验证明，GPT-MolBERTa在各种分子属性基准上表现良好，并在回归任务中接近最先进的性能。此外，对注意力进行了进一步分析。

    With the emergence of Transformer architectures and their powerful understanding of textual data, a new horizon has opened up to predict the molecular properties based on text description. While SMILES are the most common form of representation, they are lacking robustness, rich information and canonicity, which limit their effectiveness in becoming generalizable representations. Here, we present GPT-MolBERTa, a self-supervised large language model (LLM) which uses detailed textual descriptions of molecules to predict their properties. A text based description of 326000 molecules were collected using ChatGPT and used to train LLM to learn the representation of molecules. To predict the properties for the downstream tasks, both BERT and RoBERTa models were used in the finetuning stage. Experiments show that GPT-MolBERTa performs well on various molecule property benchmarks, and approaching state of the art performance in regression tasks. Additionally, further analysis of the attention 
    
[^125]: Decision ConvFormer: MetaFormer中的本地过滤对于决策制定已经足够了

    Decision ConvFormer: Local Filtering in MetaFormer is Sufficient for Decision Making. (arXiv:2310.03022v1 [cs.LG])

    [http://arxiv.org/abs/2310.03022](http://arxiv.org/abs/2310.03022)

    Decision ConvFormer提出了一种新的动作序列预测器，通过使用本地卷积过滤来捕捉强化学习数据集中的局部关联，同时在各个标准RL基准上取得了最先进的性能。

    

    Transformer在自然语言处理中的成功引发了其在各个领域的应用。在离线强化学习中，Decision Transformer（DT）作为一种基于Transformer的有前途的模型逐渐崭露头角。然而，我们发现DT的注意力模块不适合捕捉作为马尔科夫决策过程建模的强化学习轨迹中固有的局部依赖模式。为了解决DT的局限性，我们提出了一种基于MetaFormer架构的新型动作序列预测器，称为Decision ConvFormer（DC）。DC采用本地卷积过滤作为令牌混合器，能够有效捕捉RL数据集中固有的局部关联。在大量实验证明中，DC在各种标准RL基准上取得了最先进的性能，同时需要更少的资源。

    The recent success of Transformer in natural language processing has sparked its use in various domains. In offline reinforcement learning (RL), Decision Transformer (DT) is emerging as a promising model based on Transformer. However, we discovered that the attention module of DT is not appropriate to capture the inherent local dependence pattern in trajectories of RL modeled as a Markov decision process. To overcome the limitations of DT, we propose a novel action sequence predictor, named Decision ConvFormer (DC), based on the architecture of MetaFormer, which is a general structure to process multiple entities in parallel and understand the interrelationship among the multiple entities. DC employs local convolution filtering as the token mixer and can effectively capture the inherent local associations of the RL dataset. In extensive experiments, DC achieved state-of-the-art performance across various standard RL benchmarks while requiring fewer resources. Furthermore, we show that 
    
[^126]: MedDiffusion: 通过基于扩散的数据增强提升健康风险预测

    MedDiffusion: Boosting Health Risk Prediction via Diffusion-based Data Augmentation. (arXiv:2310.02520v1 [cs.LG])

    [http://arxiv.org/abs/2310.02520](http://arxiv.org/abs/2310.02520)

    本文介绍了一种名为MedDiffusion的新型、端到端的扩散式风险预测模型，通过基于扩散的数据增强，提升了健康风险预测的效果。

    

    健康风险预测是医学领域中基于预测建模的基本任务之一，旨在利用历史电子健康记录（EHR）来预测患者未来可能面临的潜在健康风险。研究人员已经开发了几种风险预测模型来处理EHR数据的独特挑战，例如其序列特性，高维度和固有噪音。这些模型已经取得了令人印象深刻的结果。然而，一个影响它们有效性的关键问题是数据不足。为了缓解这个问题，引入了各种数据生成和增强方法，通过学习底层数据分布来扩大训练数据集的大小。然而，这些方法的性能往往受到任务无关设计的限制。为了解决这些缺点，本文引入了一种新颖的端到端扩散式风险预测模型MedDiffusion，来增强风险预测的性能。

    Health risk prediction is one of the fundamental tasks under predictive modeling in the medical domain, which aims to forecast the potential health risks that patients may face in the future using their historical Electronic Health Records (EHR). Researchers have developed several risk prediction models to handle the unique challenges of EHR data, such as its sequential nature, high dimensionality, and inherent noise. These models have yielded impressive results. Nonetheless, a key issue undermining their effectiveness is data insufficiency. A variety of data generation and augmentation methods have been introduced to mitigate this issue by expanding the size of the training data set through the learning of underlying data distributions. However, the performance of these methods is often limited due to their task-unrelated design. To address these shortcomings, this paper introduces a novel, end-to-end diffusion-based risk prediction model, named MedDiffusion. It enhances risk predicti
    
[^127]: 机器学习的安全有效数据评估

    Secure and Effective Data Appraisal for Machine Learning. (arXiv:2310.02373v1 [cs.LG])

    [http://arxiv.org/abs/2310.02373](http://arxiv.org/abs/2310.02373)

    本文介绍了一种机密的数据选择和评估方法，通过创新的流程和简化的低维度操作来实现，以保护数据和模型的隐私，并在多个Transformer模型和NLP/CV基准测试中进行了评估。

    

    一个无拘无束的数据市场需要在数据所有者和模型所有者最终交易前能够对训练数据进行私密选择和评估。为了保护数据和模型的隐私，这个过程涉及使用多方计算(MPC)来审查目标模型。尽管之前的研究认为基于MPC的Transformer模型评估过于耗费资源，本文介绍了一种创新的方法，使数据选择成为可行的。本研究的贡献包括三个关键要素：(1)使用MPC进行机密数据选择的开创性流程；(2)通过在有限的相关数据子集上训练简化的低维度MLP来复制复杂的高维度操作；(3)并发、多阶段地实现MPC。所提出的方法在一系列Transformer模型和NLP/CV基准测试中进行了评估。与直接基于MPC的评估相比

    Essential for an unfettered data market is the ability to discreetly select and evaluate training data before finalizing a transaction between the data owner and model owner. To safeguard the privacy of both data and model, this process involves scrutinizing the target model through Multi-Party Computation (MPC). While prior research has posited that the MPC-based evaluation of Transformer models is excessively resource-intensive, this paper introduces an innovative approach that renders data selection practical. The contributions of this study encompass three pivotal elements: (1) a groundbreaking pipeline for confidential data selection using MPC, (2) replicating intricate high-dimensional operations with simplified low-dimensional MLPs trained on a limited subset of pertinent data, and (3) implementing MPC in a concurrent, multi-phase manner. The proposed method is assessed across an array of Transformer models and NLP/CV benchmarks. In comparison to the direct MPC-based evaluation 
    
[^128]: Avalon的思考游戏：通过递归思考对抗欺骗

    Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation. (arXiv:2310.01320v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.01320](http://arxiv.org/abs/2310.01320)

    本研究通过使用复杂的Avalon游戏作为测试平台，引入了一种名为递归思考（ReCon）的新框架，用于增强大型语言模型（LLM）识别和对抗欺骗信息的能力。

    

    最近在大型语言模型（LLM）的突破带来了在LLM作为智能体领域的显著成功。然而，一种普遍的假设是LLM处理的信息始终是诚实的，忽视了人类社会和AI生成内容中普遍存在的欺骗或误导性信息。这个疏忽使得LLM容易受到恶意操纵，可能导致不利的结果。本研究利用复杂的Avalon游戏作为测试平台，探索LLM在欺骗环境中的潜力。Avalon充满了错误信息，并需要复杂的逻辑，表现为“思考的游戏”。受到人类在Avalon游戏中递归思考和透视能力的启发，我们引入了一种新颖的框架——递归思考（ReCon），以增强LLM识别和对抗欺骗信息的能力。ReCon结合了公式化思考和完善思考的过程；公式化思考产生初始思考，完善思考对初始思考进行调整和改进。

    Recent breakthroughs in large language models (LLMs) have brought remarkable success in the field of LLM-as-Agent. Nevertheless, a prevalent assumption is that the information processed by LLMs is consistently honest, neglecting the pervasive deceptive or misleading information in human society and AI-generated content. This oversight makes LLMs susceptible to malicious manipulations, potentially resulting in detrimental outcomes. This study utilizes the intricate Avalon game as a testbed to explore LLMs' potential in deceptive environments. Avalon, full of misinformation and requiring sophisticated logic, manifests as a "Game-of-Thoughts". Inspired by the efficacy of humans' recursive thinking and perspective-taking in the Avalon game, we introduce a novel framework, Recursive Contemplation (ReCon), to enhance LLMs' ability to identify and counteract deceptive information. ReCon combines formulation and refinement contemplation processes; formulation contemplation produces initial tho
    
[^129]: BooookScore: LLM时代中对书籍长度摘要的系统探索

    BooookScore: A systematic exploration of book-length summarization in the era of LLMs. (arXiv:2310.00785v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.00785](http://arxiv.org/abs/2310.00785)

    本文对LLM模型进行了系统探索，以解决对超过上下文窗口大小的书籍进行摘要的问题，并通过两种提示工作流实施了基于LLM的书籍长度摘要器的连贯性研究。通过对100本书的GPT-4生成摘要的人工注释，发现了八种常见的连贯性错误。

    

    对于超过大型语言模型（LLMs）上下文窗口大小的书籍长度文档（>100K标记）进行摘要需要首先将输入文档分成较小的块，然后提示LLM合并、更新和压缩块级摘要。尽管这个任务的复杂性和重要性，但由于评估的困难，它尚未得到有意义的研究：现有的书籍长度摘要数据集（例如BookSum）在大多数公共LLM的预训练数据中，而现有的评估方法难以捕捉现代LLM摘要器的错误。在本文中，我们首次研究通过两种提示工作流实施的基于LLM的书籍长度摘要器的连贯性：（1）分层合并块级摘要，（2）逐步更新一个运行摘要。我们对100本最近出版的书籍的GPT-4生成摘要获得了1193个细粒度的人工注释，并确定了LLMs产生的八种常见的连贯性错误。

    Summarizing book-length documents (>100K tokens) that exceed the context window size of large language models (LLMs) requires first breaking the input document into smaller chunks and then prompting an LLM to merge, update, and compress chunk-level summaries. Despite the complexity and importance of this task, it has yet to be meaningfully studied due to the challenges of evaluation: existing book-length summarization datasets (e.g., BookSum) are in the pretraining data of most public LLMs, and existing evaluation methods struggle to capture errors made by modern LLM summarizers. In this paper, we present the first study of the coherence of LLM-based book-length summarizers implemented via two prompting workflows: (1) hierarchically merging chunk-level summaries, and (2) incrementally updating a running summary. We obtain 1193 fine-grained human annotations on GPT-4 generated summaries of 100 recently-published books and identify eight common types of coherence errors made by LLMs. Bec
    
[^130]: 使用合成数据进行预训练有助于离线强化学习

    Pre-training with Synthetic Data Helps Offline Reinforcement Learning. (arXiv:2310.00771v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.00771](http://arxiv.org/abs/2310.00771)

    本文研究表明，在离线深度强化学习中，使用合成数据进行预训练可以提高性能，而不一定需要语言预训练。此外，使用一步马尔科夫链生成的数据进行预训练可进一步改善性能。在一个流行的离线DRL算法中，使用简单的预训练方案也能获得性能提升。

    

    最近的研究表明，对于离线深度强化学习(DRL)，使用大型语言语料库预训练Decision Transformer可以提高下游性能。一个自然的问题是，这种性能提升是否只能通过语言预训练实现，还是可以通过不涉及语言的更简单的预训练方案实现。在本文中，我们首先证明了语言对于改善性能并不是必要的，实际上，使用合成的IID数据进行少量更新的预训练可以达到与使用大型语言语料库预训练相匹配的性能提升；此外，使用一步马尔科夫链生成的数据进行预训练可以进一步提高性能。受到这些实验结果的启发，我们进一步考虑了预训练Conservative Q-Learning(CQL)，这是一种流行的离线DRL算法，它基于Q-learning，并通常使用多层感知器(MLP)骨干。令人惊讶的是，使用简单的预训练方案也能在CQL算法中取得性能提升。

    Recently, it has been shown that for offline deep reinforcement learning (DRL), pre-training Decision Transformer with a large language corpus can improve downstream performance (Reid et al., 2022). A natural question to ask is whether this performance gain can only be achieved with language pre-training, or can be achieved with simpler pre-training schemes which do not involve language. In this paper, we first show that language is not essential for improved performance, and indeed pre-training with synthetic IID data for a small number of updates can match the performance gains from pre-training with a large language corpus; moreover, pre-training with data generated by a one-step Markov chain can further improve the performance. Inspired by these experimental results, we then consider pre-training Conservative Q-Learning (CQL), a popular offline DRL algorithm, which is Q-learning-based and typically employs a Multi-Layer Perceptron (MLP) backbone. Surprisingly, pre-training with sim
    
[^131]: 数据高效的网络事故电力流学习方法

    Data-Efficient Power Flow Learning for Network Contingencies. (arXiv:2310.00763v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.00763](http://arxiv.org/abs/2310.00763)

    本论文提出了一种数据高效的方法，用于学习具有网络事故的电网中的电力流，并估计电力流的概率性。通过使用网络感知高斯过程和多任务顶点度核，该方法实现了对未见网络的电力流推断，并在低训练数据情况下比现有方法具有更好的性能。

    

    本研究提出了一种高效的数据驱动方法，用于学习具有网络事故的电网中的电力流，并估计相应的概率性电压包络（PVE）。首先，使用前期研究中开发的网络感知高斯过程（GP），称为顶点度核（VDK-GP），来估计少数网络配置的电压-功率函数。文章引入了一种新颖的多任务顶点度核（MT-VDK），将学习到的VDK-GP汇集在一起，以确定未见网络的电力流，与其他方法相比，计算复杂度和超参数要求显著降低。在IEEE 30-Bus网络上的模拟实验中，在N-1和N-2事故情况下表明了电力流知识的保留和传递。在低训练数据情况下（50-250个样本），MT-VDK-GP方法在新颖的N-1事故网络配置上的平均预测误差较VDK-GP减少了50%以上。此外，MT-VDK-GP在N-1事故网络配置的低训练数据情况下（50-250个样本）达到了平均预测误差降低50%以上的效果。

    This work presents an efficient data-driven method to learn power flows in grids with network contingencies and to estimate corresponding probabilistic voltage envelopes (PVE). First, a network-aware Gaussian process (GP) termed Vertex-Degree Kernel (VDK-GP), developed in prior work, is used to estimate voltage-power functions for a few network configurations. The paper introduces a novel multi-task vertex degree kernel (MT-VDK) that amalgamates the learned VDK-GPs to determine power flows for unseen networks, with a significant reduction in the computational complexity and hyperparameter requirements compared to alternate approaches. Simulations on the IEEE 30-Bus network demonstrate the retention and transfer of power flow knowledge in both N-1 and N-2 contingency scenarios. The MT-VDK-GP approach achieves over 50% reduction in mean prediction error for novel N-1 contingency network configurations in low training data regimes (50-250 samples) over VDK-GP. Additionally, MT-VDK-GP outp
    
[^132]: 通过特征选择提高内存恶意软件分类的效率和隐私

    Enhancing Efficiency and Privacy in Memory-Based Malware Classification through Feature Selection. (arXiv:2310.00516v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2310.00516](http://arxiv.org/abs/2310.00516)

    本研究通过特征选择方法提高了内存中恶意软件的分类效果和隐私保护。在实验中，我们使用三种特征选择方法从内存内容中识别出重要特征，并将它们应用于多样的分类器中，实现了恶意软件的二元分类、类型分类和家族分类。

    

    恶意软件通过破坏系统和数据给个人、组织和关键基础设施带来严重的安全风险。利用提供计算机内存快照的内存转储可以帮助分析和检测恶意内容，包括恶意软件。为了提高恶意软件分类系统的效力并解决隐私问题，特征选择可以起到关键作用，因为它能够识别出最相关的特征，从而最小化分类器所需的数据量。在本研究中，我们采用三种特征选择方法从内存内容中识别出重要特征，并将它们与多样的分类器一起使用，以提高分类任务的性能和隐私保护。我们在三个级别的恶意软件分类任务上进行了全面的实验：i）二元级别的良性软件或恶意软件分类，ii）恶意软件类型分类（包括木马、勒索软件和间谍软件），和iii）恶意软件家族分类。

    Malware poses a significant security risk to individuals, organizations, and critical infrastructure by compromising systems and data. Leveraging memory dumps that offer snapshots of computer memory can aid the analysis and detection of malicious content, including malware. To improve the efficacy and address privacy concerns in malware classification systems, feature selection can play a critical role as it is capable of identifying the most relevant features, thus, minimizing the amount of data fed to classifiers. In this study, we employ three feature selection approaches to identify significant features from memory content and use them with a diverse set of classifiers to enhance the performance and privacy of the classification task. Comprehensive experiments are conducted across three levels of malware classification tasks: i) binary-level benign or malware classification, ii) malware type classification (including Trojan horse, ransomware, and spyware), and iii) malware family c
    
[^133]: 一个用于混合迪里切特和诺曼边界条件的神经预处理泊松求解器

    A Neural-preconditioned Poisson Solver for Mixed Dirichlet and Neumann Boundary Conditions. (arXiv:2310.00177v1 [math.NA])

    [http://arxiv.org/abs/2310.00177](http://arxiv.org/abs/2310.00177)

    我们引入了一个用于混合边界条件的泊松方程的神经预处理迭代求解器，核心是一个神经网络，能够近似逆离散结构网格拉普拉斯算子，并且在训练集之外的边界条件下仍然有效。

    

    我们引入了一个神经预处理的迭代求解器，用于具有混合边界条件的泊松方程。泊松方程在科学计算中是普遍存在的：它控制着广泛的物理现象，在许多数值算法中作为子问题出现，并且作为更广泛的椭圆PDE类的模型问题。最流行的泊松离散化方法可以产生大型稀疏线性系统。在高分辨率和对性能至关重要的应用中，迭代求解器结合强大的预处理器可以提供优势。我们求解器的核心是一个神经网络，该网络经过训练可以近似离散结构网格拉普拉斯算子的逆算子，适用于任意形状的域和混合边界条件。我们展示了该问题的结构激发了一种新颖的网络架构，即使在训练集之外的边界条件下，该架构也表现出高效的预处理器。我们展示了在具有挑战性的测试案例上的效果。

    We introduce a neural-preconditioned iterative solver for Poisson equations with mixed boundary conditions. The Poisson equation is ubiquitous in scientific computing: it governs a wide array of physical phenomena, arises as a subproblem in many numerical algorithms, and serves as a model problem for the broader class of elliptic PDEs. The most popular Poisson discretizations yield large sparse linear systems. At high resolution, and for performance-critical applications, iterative solvers can be advantageous for these -- but only when paired with powerful preconditioners. The core of our solver is a neural network trained to approximate the inverse of a discrete structured-grid Laplace operator for a domain of arbitrary shape and with mixed boundary conditions. The structure of this problem motivates a novel network architecture that we demonstrate is highly effective as a preconditioner even for boundary conditions outside the training set. We show that on challenging test cases aris
    
[^134]: 频道视觉Transformer：一张图值C x 16 x 16个词

    Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words. (arXiv:2309.16108v1 [cs.CV])

    [http://arxiv.org/abs/2309.16108](http://arxiv.org/abs/2309.16108)

    本文提出了ChannelViT模型，通过对ViT架构的修改和引入分层通道采样技术，增强了对多通道图像的推理能力和鲁棒性，适用于显微镜和卫星成像等领域。

    

    视觉Transformer在现代计算机视觉领域中已经成为一种强大的架构。然而，它在某些图像领域的应用，如显微镜和卫星成像，面临着独特的挑战。在这些领域中，图像通常包含多个通道，每个通道都携带着语义上不同和独立的信息。此外，模型必须对输入通道的稀疏性表现出鲁棒性，在训练或测试过程中可能没有密集可用的通道。在本文中，我们提出了对ViT架构的修改，增强了对输入通道之间的推理，并引入了分层通道采样(HCS)作为一种附加的正则化技术，以确保在测试过程中仅出现部分通道时的鲁棒性。我们提出的模型ChannelViT独立地构建补丁令牌并利用可学习的通道嵌入将其添加到补丁令牌中，类似于位置嵌入。我们进行了评估

    Vision Transformer (ViT) has emerged as a powerful architecture in the realm of modern computer vision. However, its application in certain imaging fields, such as microscopy and satellite imaging, presents unique challenges. In these domains, images often contain multiple channels, each carrying semantically distinct and independent information. Furthermore, the model must demonstrate robustness to sparsity in input channels, as they may not be densely available during training or testing. In this paper, we propose a modification to the ViT architecture that enhances reasoning across the input channels and introduce Hierarchical Channel Sampling (HCS) as an additional regularization technique to ensure robustness when only partial channels are presented during test time. Our proposed model, ChannelViT, constructs patch tokens independently from each input channel and utilizes a learnable channel embedding that is added to the patch tokens, similar to positional embeddings. We evaluate
    
[^135]: 利用腭咽口炎数据提升基于UPTST的手足口病住院预测的准确性

    Leveraging Herpangina Data to Enhance Hospital-level Prediction of Hand-Foot-and-Mouth Disease Admissions Using UPTST. (arXiv:2309.14674v1 [cs.LG])

    [http://arxiv.org/abs/2309.14674](http://arxiv.org/abs/2309.14674)

    提出了一种新颖的基于Transformer的UPTST模型，利用腭咽口炎数据提升手足口病住院预测的准确性，且在医院级别的预测准确性上优于现有方法。

    

    手足口病（HFMD）爆发与严重的发病率和死亡率相关。因此，准确预测儿科HFMD患者的每日住院人数对于协助医院应对潜在的爆发和减少医院内传播至关重要。为了解决这一迫切需求，我们提出了一种新颖的基于Transformer的模型，它具有U-net形状，并利用了与HFMD密切相关的腭咽口炎的见解。该模型还通过引入重构损失作为辅助损失来整合表示学习。结果显示，我们的UPTST模型在医院级别的HFMD长短臂预测准确性方面优于现有方法。此外，探索性的扩展实验表明该模型的能力超出了传染病的预测，提示...

    Outbreaks of hand-foot-and-mouth disease(HFMD) have been associated with significant morbidity and, in severe cases, mortality. Accurate forecasting of daily admissions of pediatric HFMD patients is therefore crucial for aiding the hospital in preparing for potential outbreaks and mitigating nosocomial transmissions. To address this pressing need, we propose a novel transformer-based model with a U-net shape, utilizing the patching strategy and the joint prediction strategy that capitalizes on insights from herpangina, a disease closely correlated with HFMD. This model also integrates representation learning by introducing reconstruction loss as an auxiliary loss. The results show that our U-net Patching Time Series Transformer (UPTST) model outperforms existing approaches in both long- and short-arm prediction accuracy of HFMD at hospital-level. Furthermore, the exploratory extension experiments show that the model's capabilities extend beyond prediction of infectious disease, suggest
    
[^136]: NAS-NeRF: 用于神经辐射场的生成式神经体系结构搜索

    NAS-NeRF: Generative Neural Architecture Search for Neural Radiance Fields. (arXiv:2309.14293v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.14293](http://arxiv.org/abs/2309.14293)

    NAS-NeRF是一种生成式神经体系结构搜索策略，通过平衡架构复杂度和目标合成质量指标生成紧凑、针对场景的NeRF架构。

    

    神经辐射场（NeRF）实现了高质量的新视图合成，但其高计算复杂度限制了其可部署性。现有的基于神经网络的解决方案努力提高效率，但不考虑场景复杂性，使用通用架构。同一个架构可能对简单场景来说过于庞大，对复杂场景则不足够。因此，有必要动态优化NeRF的神经网络组件，以在计算复杂度和合成质量之间实现平衡。我们引入了NAS-NeRF，一种生成式神经体系结构搜索策略，通过平衡架构复杂度和目标合成质量指标生成紧凑、针对场景的NeRF架构。我们的方法结合目标度量和预算约束，指导搜索以获得适合每个场景的架构。在Blender合成数据集上进行的实验证明，提出的NAS-NeRF方法可以生成多达5个架构。

    Neural radiance fields (NeRFs) enable high-quality novel view synthesis, but their high computational complexity limits deployability. While existing neural-based solutions strive for efficiency, they use one-size-fits-all architectures regardless of scene complexity. The same architecture may be unnecessarily large for simple scenes but insufficient for complex ones. Thus, there is a need to dynamically optimize the neural network component of NeRFs to achieve a balance between computational complexity and specific targets for synthesis quality. We introduce NAS-NeRF, a generative neural architecture search strategy that generates compact, scene-specialized NeRF architectures by balancing architecture complexity and target synthesis quality metrics. Our method incorporates constraints on target metrics and budgets to guide the search towards architectures tailored for each scene. Experiments on the Blender synthetic dataset show the proposed NAS-NeRF can generate architectures up to 5
    
[^137]: 适应性输入图像归一化方法解决基于GAN的X射线图像的模式塌陷问题

    Adaptive Input-image Normalization for Solving Mode Collapse Problem in GAN-based X-ray Images. (arXiv:2309.12245v1 [eess.IV])

    [http://arxiv.org/abs/2309.12245](http://arxiv.org/abs/2309.12245)

    本论文研究了生成对抗网络中的模式塌陷问题对合成X射线图像多样性的影响。通过实验证明了将自适应输入图像归一化方法与深度模型结合的优势。

    

    由于疾病的罕见性，生物医学图像数据集可能存在不平衡。生成对抗网络通过生成合成图像来扩充数据集，起到了解决这种不平衡的关键作用。生成合成图像需要包含多样化的特征，以准确表示训练图像中存在的特征分布。此外，合成图像中缺乏多样性的特征会降低机器学习分类器的性能。模式塌陷问题影响生成对抗网络生成多样化图像的能力，并分为类内和类间两种类型。本文研究了这两种模式塌陷问题，并评估了它们对合成X射线图像多样性的影响。本研究将自适应输入图像归一化方法与深度模型相结合，通过实验证明了其在解决模式塌陷问题上的优势。

    Biomedical image datasets can be imbalanced due to the rarity of targeted diseases. Generative Adversarial Networks play a key role in addressing this imbalance by enabling the generation of synthetic images to augment datasets. It is important to generate synthetic images that incorporate a diverse range of features to accurately represent the distribution of features present in the training imagery. Furthermore, the absence of diverse features in synthetic images can degrade the performance of machine learning classifiers. The mode collapse problem impacts Generative Adversarial Networks' capacity to generate diversified images. Mode collapse comes in two varieties: intra-class and inter-class. In this paper, both varieties of the mode collapse problem are investigated, and their subsequent impact on the diversity of synthetic X-ray images is evaluated. This work contributes an empirical demonstration of the benefits of integrating the adaptive input-image normalization with the Deep
    
[^138]: 活动学习强化学习：一种随机最优控制方法的应用

    Actively Learning Reinforcement Learning: A Stochastic Optimal Control Approach. (arXiv:2309.10831v1 [cs.LG])

    [http://arxiv.org/abs/2309.10831](http://arxiv.org/abs/2309.10831)

    本文提供了一个框架来解决强化学习中的模型不确定性和计算成本高的问题，通过使用强化学习解决随机动态规划方程，生成的控制器能够主动学习模型不确定性，并确保安全性和实时学习。

    

    本文提供了一个框架来应对两个问题：（i）强化学习在模型不确定性方面的脆弱性，因为受控实验室/仿真和实际条件之间的不匹配，以及（ii）随机最优控制的计算成本过高。我们通过使用强化学习来解决随机动态规划方程来解决这两个问题。由此产生的强化学习控制器对于几种类型的约束条件是安全的，并且它可以主动学习模型不确定性。与探索和利用不同，探测和安全性由控制器自身自动实现，实现了实时学习。一个仿真示例证明了所提方法的有效性。

    In this paper we provide framework to cope with two problems: (i) the fragility of reinforcement learning due to modeling uncertainties because of the mismatch between controlled laboratory/simulation and real-world conditions and (ii) the prohibitive computational cost of stochastic optimal control. We approach both problems by using reinforcement learning to solve the stochastic dynamic programming equation. The resulting reinforcement learning controller is safe with respect to several types of constraints constraints and it can actively learn about the modeling uncertainties. Unlike exploration and exploitation, probing and safety are employed automatically by the controller itself, resulting real-time learning. A simulation example demonstrates the efficacy of the proposed approach.
    
[^139]: 用梯度逼近降低对抗训练成本

    Reducing Adversarial Training Cost with Gradient Approximation. (arXiv:2309.09464v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.09464](http://arxiv.org/abs/2309.09464)

    本文提出了一种新的对抗训练方法——梯度逼近对抗训练(GAAT)，通过泰勒级数的部分和来近似对抗损失，并近似梯度，以降低建立鲁棒模型的成本。

    

    深度学习模型在多个领域取得了最先进的性能，但它们对于经过巧妙但微小扰动的输入非常脆弱，这被称为对抗样本(Adversarial Examples, AEs)。在许多提高模型对抗样本鲁棒性的策略中，基于投影梯度下降(Projected Gradient Descent, PGD)的对抗训练方法是最有效的之一。然而，由于损失函数的最大化使得生成足够强烈的对抗样本需要巨大的计算开销，对于使用更大更复杂的模型，通常的PGD对抗训练方法有时不切实际。本文提出对抗损失可以通过泰勒级数的部分和来近似，并近似对抗损失的梯度，进而提出一种新的高效的对抗训练方法——梯度逼近对抗训练(GAAT)，以降低建立鲁棒模型的成本。此外，进行了大量实验验证了我们的方法的有效性。

    Deep learning models have achieved state-of-the-art performances in various domains, while they are vulnerable to the inputs with well-crafted but small perturbations, which are named after adversarial examples (AEs). Among many strategies to improve the model robustness against AEs, Projected Gradient Descent (PGD) based adversarial training is one of the most effective methods. Unfortunately, the prohibitive computational overhead of generating strong enough AEs, due to the maximization of the loss function, sometimes makes the regular PGD adversarial training impractical when using larger and more complicated models. In this paper, we propose that the adversarial loss can be approximated by the partial sum of Taylor series. Furthermore, we approximate the gradient of adversarial loss and propose a new and efficient adversarial training method, adversarial training with gradient approximation (GAAT), to reduce the cost of building up robust models. Additionally, extensive experiments
    
[^140]: 使用预训练深度学习模型的睡眠阶段分类

    Sleep Stage Classification Using a Pre-trained Deep Learning Model. (arXiv:2309.07182v1 [eess.SP])

    [http://arxiv.org/abs/2309.07182](http://arxiv.org/abs/2309.07182)

    本研究提出了一种名为"EEGMobile"的机器学习模型，在睡眠阶段分类中取得了优于其他模型的准确率，特别在N1阶段表现更佳。

    

    睡眠障碍是常见的人类疾病之一。睡眠阶段的分类在诊断睡眠障碍、监测治疗效果和理解睡眠阶段与各种健康状况之间的关系方面起着基础性作用。精确而有效地分类这些阶段可以显着提升我们对与睡眠相关现象的理解，从而最终改善健康结果和疾病治疗效果。其他提出的模型往往耗时且缺乏足够的准确性，特别是在N1阶段。本研究的主要目标是提出一种名为"EEGMobile"的机器学习模型。该模型利用预训练模型，并从脑信号的脑电图谱图学习。在名为"Sleep-EDF20"的公开数据集上，该模型实现了86.97%的准确率，表现优于其他研究者提出的模型。此外，它在N1阶段记录了56.4%的准确率，这是更好的。

    One of the common human diseases is sleep disorders. The classification of sleep stages plays a fundamental role in diagnosing sleep disorders, monitoring treatment effectiveness, and understanding the relationship between sleep stages and various health conditions. A precise and efficient classification of these stages can significantly enhance our understanding of sleep-related phenomena and ultimately lead to improved health outcomes and disease treatment.  Models others propose are often time-consuming and lack sufficient accuracy, especially in stage N1. The main objective of this research is to present a machine-learning model called "EEGMobile". This model utilizes pre-trained models and learns from electroencephalogram (EEG) spectrograms of brain signals. The model achieved an accuracy of 86.97% on a publicly available dataset named "Sleep-EDF20", outperforming other models proposed by different researchers. Moreover, it recorded an accuracy of 56.4% in stage N1, which is bette
    
[^141]: 一个关于校准的基准研究

    A Benchmark Study on Calibration. (arXiv:2308.11838v1 [cs.LG])

    [http://arxiv.org/abs/2308.11838](http://arxiv.org/abs/2308.11838)

    这项研究提出了一个模型校准的基准研究，利用神经架构搜索空间探索了模型校准属性。研究结果显示，模型校准可以在不同任务中泛化，并可以同时兼顾模型的准确性和校准性能。

    

    深度神经网络在各种机器学习任务中的应用越来越广泛。然而，随着这些模型复杂性的增加，它们往往面临校准问题，尽管预测准确性有所提高。许多研究通过数据预处理、使用特定损失函数和训练框架来改善校准性能。然而，对校准属性的研究有点被忽视了。我们的研究利用神经架构搜索（NAS）搜索空间，在全面探索校准属性的模型架构空间中提供了一个详尽的模型架构空间。我们特别创建了一个模型校准数据集。该数据集在广泛使用的NATS-Bench搜索空间中评估了90个基于区间的校准度量和12个其他校准度量，涵盖了117,702个独特的神经网络。我们的分析旨在通过我们提出的数据集回答该领域一些长期存在的问题：（i）模型校准能否在不同任务中泛化？（ii）能否同时兼顾模型的准确性和校准性能？

    Deep neural networks are increasingly utilized in various machine learning tasks. However, as these models grow in complexity, they often face calibration issues, despite enhanced prediction accuracy. Many studies have endeavored to improve calibration performance through data preprocessing, the use of specific loss functions, and training frameworks. Yet, investigations into calibration properties have been somewhat overlooked. Our study leverages the Neural Architecture Search (NAS) search space, offering an exhaustive model architecture space for thorough calibration properties exploration. We specifically create a model calibration dataset. This dataset evaluates 90 bin-based and 12 additional calibration measurements across 117,702 unique neural networks within the widely employed NATS-Bench search space. Our analysis aims to answer several longstanding questions in the field, using our proposed dataset: (i) Can model calibration be generalized across different tasks? (ii) Can rob
    
[^142]: 这不是一个苹果：多模态嵌入中的对抗幻觉

    Ceci n'est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings. (arXiv:2308.11804v1 [cs.CR])

    [http://arxiv.org/abs/2308.11804](http://arxiv.org/abs/2308.11804)

    该论文研究了多模态嵌入中的对抗幻觉问题。对手可以扰动输入的任意模态，使其嵌入与其他模态的任意输入接近，从而实现任意图像与任意文本、任意文本与任意声音的对齐。该问题与下游任务无关，对生成和分类任务会产生误导。

    

    多模态编码器将图像、声音、文本、视频等映射到一个单一的嵌入空间中，通过对齐不同模态的表示（例如将一张狗的图像与一种叫声相关联）。我们展示了多模态嵌入可以受到一种我们称之为“对抗幻觉”的攻击。给定任意模态的输入，对手可以扰动它，使其嵌入接近于另一模态中任意对手选择的输入的嵌入。幻觉使对手能够将任意图像与任意文本、任意文本与任意声音等进行对齐。对抗幻觉利用了嵌入空间中的接近性，因此与下游任务无关。使用ImageBind嵌入，我们演示了在没有具体下游任务知识的情况下，通过对抗性对齐的输入如何误导图像生成、文本生成和零样例分类。

    Multi-modal encoders map images, sounds, texts, videos, etc. into a single embedding space, aligning representations across modalities (e.g., associate an image of a dog with a barking sound). We show that multi-modal embeddings can be vulnerable to an attack we call "adversarial illusions." Given an input in any modality, an adversary can perturb it so as to make its embedding close to that of an arbitrary, adversary-chosen input in another modality. Illusions thus enable the adversary to align any image with any text, any text with any sound, etc.  Adversarial illusions exploit proximity in the embedding space and are thus agnostic to downstream tasks. Using ImageBind embeddings, we demonstrate how adversarially aligned inputs, generated without knowledge of specific downstream tasks, mislead image generation, text generation, and zero-shot classification.
    
[^143]: 联合回声消除和噪声抑制的超级双路径压缩

    Ultra Dual-Path Compression For Joint Echo Cancellation And Noise Suppression. (arXiv:2308.11053v1 [eess.AS])

    [http://arxiv.org/abs/2308.11053](http://arxiv.org/abs/2308.11053)

    本文提出了一种超级双路径压缩方法用于联合回声消除和噪声抑制，通过时间频率双路径压缩实现广泛的压缩比，同时降低计算成本，且在固定压缩比下能够进一步改善性能。

    

    回声消除和噪声抑制对于全双工通信至关重要，然而大多数现有的神经网络在计算成本高和模型复杂度调整上不灵活。在本文中，我们引入时间频率双路径压缩，实现广泛的压缩比，同时降低计算成本。具体来说，在频率压缩方面，使用可训练滤波器代替手动设计的滤波器进行维度缩减。在时间压缩方面，仅使用帧跳预测会导致性能大幅下降，但通过具有完整序列建模的后处理网络可以缓解这个问题。我们发现，在固定压缩比的情况下，同时使用时间和频率方法进行双路径压缩将进一步改善性能，并且几乎不改变模型大小，压缩比覆盖范围从4x到32x。此外，所提出的模型与快速FullSubNet和DeepFilterNet相比具有竞争力的性能。可以访问演示页面。

    Echo cancellation and noise reduction are essential for full-duplex communication, yet most existing neural networks have high computational costs and are inflexible in tuning model complexity. In this paper, we introduce time-frequency dual-path compression to achieve a wide range of compression ratios on computational cost. Specifically, for frequency compression, trainable filters are used to replace manually designed filters for dimension reduction. For time compression, only using frame skipped prediction causes large performance degradation, which can be alleviated by a post-processing network with full sequence modeling. We have found that under fixed compression ratios, dual-path compression combining both the time and frequency methods will give further performance improvement, covering compression ratios from 4x to 32x with little model size change. Moreover, the proposed models show competitive performance compared with fast FullSubNet and DeepFilterNet. A demo page can be f
    
[^144]: 图像处理和机器学习在高光谱解混方面的应用：概述和HySUPP Python包

    Image Processing and Machine Learning for Hyperspectral Unmixing: An Overview and the HySUPP Python Package. (arXiv:2308.09375v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2308.09375](http://arxiv.org/abs/2308.09375)

    本文概述了高光谱解混的先进和常规方法，并比较了这些方法在不同解混场景下的性能。另外，还介绍了一个开源的解混软件包HySUPP。

    

    由于高光谱传感器的低空间分辨率、双重散射和场景中材料的混合现象，光谱像素往往是材料的纯光谱成分的混合物，被称为端元。解混即估计像素点中各端元的比例。根据端元的先验知识，线性解混可分为监督、半监督和无监督（盲解混）三大类。图像处理和机器学习的进展对解混产生了重大影响。本文概述了先进和常规的解混方法。此外，我们对这三个类别中先进和常规技术进行了重要对比。我们比较了解混技术在三个模拟和两个真实数据集上的性能。实验结果揭示了对不同解混场景来说，不同解混类别的优势。此外，我们还提供了一个开源的软件包——HySUPP。

    Spectral pixels are often a mixture of the pure spectra of the materials, called endmembers, due to the low spatial resolution of hyperspectral sensors, double scattering, and intimate mixtures of materials in the scenes. Unmixing estimates the fractional abundances of the endmembers within the pixel. Depending on the prior knowledge of endmembers, linear unmixing can be divided into three main groups: supervised, semi-supervised, and unsupervised (blind) linear unmixing. Advances in Image processing and machine learning substantially affected unmixing. This paper provides an overview of advanced and conventional unmixing approaches. Additionally, we draw a critical comparison between advanced and conventional techniques from the three categories. We compare the performance of the unmixing techniques on three simulated and two real datasets. The experimental results reveal the advantages of different unmixing categories for different unmixing scenarios. Moreover, we provide an open-sou
    
[^145]: 为U型并行分层学习进行最优资源分配

    Optimal Resource Allocation for U-Shaped Parallel Split Learning. (arXiv:2308.08896v1 [cs.LG])

    [http://arxiv.org/abs/2308.08896](http://arxiv.org/abs/2308.08896)

    本论文提出了一种新颖的并行U型分层学习方法，并设计了优化资源分配方案以提高边缘网络的性能。实验结果表明该方法可以达到与其他方法相似的性能。

    

    分层学习（SL）作为一种在不公开数据所有者的原始数据样本的情况下进行模型训练的有前景的方法而出现。然而，传统的SL不可避免地泄漏了标签隐私，因为尾部模型（具有最后几层）应该放在服务器上。为了克服这一限制，一种有前景的解决方法是利用U型架构将早期层和最后层都放在用户端。在本文中，我们开发了一种新颖的并行U型分层学习方法，并设计了优化资源分配方案以提高边缘网络的性能。在所提出的框架中，多个用户与边缘服务器进行SL通信。我们分析了训练过程中每个客户端的端到端延迟，并设计了一种高效的资源分配算法，称为LSCRA，它可以找到最佳的计算资源分配和分层。我们的实验结果表明了LSCRA的有效性，以及U型PSL可以达到与其他S方法类似的性能。

    Split learning (SL) has emerged as a promising approach for model training without revealing the raw data samples from the data owners. However, traditional SL inevitably leaks label privacy as the tail model (with the last layers) should be placed on the server. To overcome this limitation, one promising solution is to utilize U-shaped architecture to leave both early layers and last layers on the user side. In this paper, we develop a novel parallel U-shaped split learning and devise the optimal resource optimization scheme to improve the performance of edge networks. In the proposed framework, multiple users communicate with an edge server for SL. We analyze the end-to-end delay of each client during the training process and design an efficient resource allocation algorithm, called LSCRA, which finds the optimal computing resource allocation and split layers. Our experimental results show the effectiveness of LSCRA and that U-shaped PSL can achieve a similar performance with other S
    
[^146]: 开源联邦学习框架中错误的实证研究

    An Empirical Study of Bugs in Open-Source Federated Learning Framework. (arXiv:2308.05014v1 [cs.SE])

    [http://arxiv.org/abs/2308.05014](http://arxiv.org/abs/2308.05014)

    本文通过实证研究发现了开源联邦学习框架中存在的错误，并对这些错误的特征进行了详细分析，为提高框架的安全性和稳定性提供了指导。

    

    联邦学习作为一种分散式的机器学习解决方案，旨在保护用户的私密数据，在近年来已成为重要的学习模式，尤其是在大多数国家实施更严格的法律法规之后。因此，发布了各种各样的联邦学习框架，以促进联邦学习的开发和应用。尽管在FL模型和系统的安全性和隐私性方面进行了大量研究，但FL框架的安全问题尚未得到系统地研究。本文首次对1,112个FL框架错误进行实证研究，以了解其特征。这些错误是通过手动从GitHub上收集、分类和标记的12个开源FL框架得来的。具体来说，我们构建了这些错误的15个症状、12个根本原因和20个修复模式的分类，并研究了它们在23个逻辑组件和两个主要应用场景上的相关性和分布。

    Federated learning (FL), as a decentralized machine learning solution to the protection of users' private data, has become an important learning paradigm in recent years, especially since the enforcement of stricter laws and regulations in most countries. Therefore, a variety of FL frameworks are released to facilitate the development and application of federated learning. Despite the considerable amount of research on the security and privacy of FL models and systems, the security issues in FL frameworks have not been systematically studied yet. In this paper, we conduct the first empirical study on 1,112 FL framework bugs to investigate their characteristics. These bugs are manually collected, classified, and labeled from 12 open-source FL frameworks on GitHub. In detail, we construct taxonomies of 15 symptoms, 12 root causes, and 20 fix patterns of these bugs and investigate their correlations and distributions on 23 logical components and two main application scenarios. From the re
    
[^147]: 最大化使用非平稳赌博算法的支付路由成功率

    Maximizing Success Rate of Payment Routing using Non-stationary Bandits. (arXiv:2308.01028v1 [cs.LG])

    [http://arxiv.org/abs/2308.01028](http://arxiv.org/abs/2308.01028)

    本文提出了一种使用非平稳赌博算法的支付路由策略，通过系统架构设计和实时实验的验证，成功提高了0.92\%的交易成功率。

    

    本文讨论非平稳多臂赌博方法的系统架构设计和部署，以根据最近的交易历史确定接近最优的支付路由策略。我们提出了一种基于射线的新型路由服务架构，通过最优的扩展赌博式支付路由来处理每秒超过10000次的交易量，并符合Payment Card Industry Data Security Standard (PCI DSS) 的系统设计要求和生态约束。我们首先在自定义模拟器上评估了多个基于赌博算法的支付路由算法的有效性，以对比多个非平稳赌博方法并确定最佳超参数。然后我们在虚拟体育平台Dream11的支付交易系统上进行了实时实验。在实时实验中，我们证明了我们的非平稳赌博算法相比传统方法可以始终提高0.92\%的交易成功率。

    This paper discusses the system architecture design and deployment of non-stationary multi-armed bandit approaches to determine a near-optimal payment routing policy based on the recent history of transactions. We propose a Routing Service architecture using a novel Ray-based implementation for optimally scaling bandit-based payment routing to over 10000 transactions per second, adhering to the system design requirements and ecosystem constraints with Payment Card Industry Data Security Standard (PCI DSS). We first evaluate the effectiveness of multiple bandit-based payment routing algorithms on a custom simulator to benchmark multiple non-stationary bandit approaches and identify the best hyperparameters. We then conducted live experiments on the payment transaction system on a fantasy sports platform Dream11. In the live experiments, we demonstrated that our non-stationary bandit-based algorithm consistently improves the success rate of transactions by 0.92\% compared to the traditio
    
[^148]: SkullGAN: 使用生成对抗网络生成合成的颅骨CT图像

    SkullGAN: Synthetic Skull CT Generation with Generative Adversarial Networks. (arXiv:2308.00206v1 [eess.IV])

    [http://arxiv.org/abs/2308.00206](http://arxiv.org/abs/2308.00206)

    使用SkullGAN，一种生成对抗网络（GAN），生成合成的颅骨CT图像，可以减少对真实图像的依赖，加速将机器学习应用于医疗保健领域的整合。

    

    深度学习在涉及人类颅骨的各种医疗应用中具有潜力，但需要大量经过策划的医学图像数据集。为了解决这个挑战，我们提出了SkullGAN，一种生成对抗网络（GAN），用于创建大规模的合成颅骨CT切片数据集，减少对真实图像的依赖并加速将机器学习应用于医疗保健领域的整合。在我们的方法中，对38个受试者进行了颅骨CT切片输入SkullGAN，这是一个包含超过2亿个参数的神经网络。生成的合成颅骨图像根据三个定量放射学特征进行评估：颅骨密度比（SDR）、平均厚度和平均强度。同时，使用t-分布随机邻域嵌入（t-SNE）进行进一步的分析，并将SkullGAN判别器作为分类器进行应用。结果表明，SkullGAN生成的图像与真实颅骨具有类似的关键定量放射学特征。进一步的确定性分析是通过进行了解决这个问题的办法。

    Deep learning offers potential for various healthcare applications involving the human skull but requires extensive datasets of curated medical images. To overcome this challenge, we propose SkullGAN, a generative adversarial network (GAN), to create large datasets of synthetic skull CT slices, reducing reliance on real images and accelerating the integration of machine learning into healthcare. In our method, CT slices of 38 subjects were fed to SkullGAN, a neural network comprising over 200 million parameters. The synthetic skull images generated were evaluated based on three quantitative radiological features: skull density ratio (SDR), mean thickness, and mean intensity. They were further analyzed using t-distributed stochastic neighbor embedding (t-SNE) and by applying the SkullGAN discriminator as a classifier. The results showed that SkullGAN-generated images demonstrated similar key quantitative radiological features to real skulls. Further definitive analysis was undertaken by
    
[^149]: 使用虚拟提示注入向指令调整的大型语言模型后门

    Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection. (arXiv:2307.16888v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.16888](http://arxiv.org/abs/2307.16888)

    这项研究介绍了一种针对指令调整的大型语言模型的新型后门攻击方法，即虚拟提示注入（VPI）。通过在特定触发场景下将虚拟提示与用户指令连接，攻击者可以精细操纵模型的回应而无需明确注入。

    

    指令调整的大型语言模型（LLM）表现出了根据人类指令调节其回应的非凡能力。然而，这种调节能力也引入了潜在的攻击者通过植入后门来对模型功能进行精细操纵的可能性。在本文中，我们介绍了一种针对指令调整的LLM定制的新型后门攻击设置-虚拟提示注入（VPI）。在VPI攻击中，期望通过在特定触发场景下将攻击者指定的虚拟提示连接到用户指令中，使植入后门的模型表现得像是在其输入中没有明确的注入。例如，如果LLM被虚拟提示"负面描述乔·拜登"植入后门的触发场景是讨论乔·拜登，那么当谈论乔·拜登时，模型将传播负面倾向的观点。 VPI尤其有害，因为攻击者可以进行细粒度的操纵。

    Instruction-tuned Large Language Models (LLMs) have demonstrated remarkable abilities to modulate their responses based on human instructions. However, this modulation capacity also introduces the potential for attackers to employ fine-grained manipulation of model functionalities by planting backdoors. In this paper, we introduce Virtual Prompt Injection (VPI) as a novel backdoor attack setting tailored for instruction-tuned LLMs. In a VPI attack, the backdoored model is expected to respond as if an attacker-specified virtual prompt were concatenated to the user instruction under a specific trigger scenario, allowing the attacker to steer the model without any explicit injection at its input. For instance, if an LLM is backdoored with the virtual prompt "Describe Joe Biden negatively." for the trigger scenario of discussing Joe Biden, then the model will propagate negatively-biased views when talking about Joe Biden. VPI is especially harmful as the attacker can take fine-grained and 
    
[^150]: 深度学习与自适应滤波：Stein无偏风险估计方法的应用

    Deep Learning Meets Adaptive Filtering: A Stein's Unbiased Risk Estimator Approach. (arXiv:2307.16708v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2307.16708](http://arxiv.org/abs/2307.16708)

    本文通过算法展开的方法，将递归最小二乘法（RLS）和等变自适应源分离（EASI）两种算法转化为深度神经网络的层，通过利用训练过程有效地进行源信号估计。同时，通过使用基于Stein无偏风险估计（SURE）的损失函数训练，进一步提高了性能，实证评估证明了该方法的有效性。

    

    本文通过算法展开的视角重新审视了两种著名的自适应滤波算法，即递归最小二乘法（RLS）和等变自适应源分离（EASI），在源估计和分离的环境中。在展开方法的基础上，我们引入了新的基于任务的深度学习框架，称为Deep RLS和Deep EASI。这些架构将原始算法的迭代变换为深度神经网络的层，从而通过利用训练过程有效地进行源信号估计。为了进一步提高性能，我们提出使用基于Stein无偏风险估计（SURE）的损失函数对这些深度展开网络进行训练。我们的实证评估证明了这种基于SURE的方法对于增强源信号估计的有效性。

    This paper revisits two prominent adaptive filtering algorithms through the lens of algorithm unrolling, namely recursive least squares (RLS) and equivariant adaptive source separation (EASI), in the context of source estimation and separation. Building upon the unrolling methodology, we introduce novel task-based deep learning frameworks, denoted as Deep RLS and Deep EASI. These architectures transform the iterations of the original algorithms into layers of a deep neural network, thereby enabling efficient source signal estimation by taking advantage of a training process. To further enhance performance, we propose training these deep unrolled networks utilizing a loss function grounded on a Stein's unbiased risk estimator (SURE). Our empirical evaluations demonstrate the efficacy of this SURE-based approach for enhanced source signal estimation.
    
[^151]: 使用提示条件微调实现零样本领域敏感语音识别

    Zero-shot Domain-sensitive Speech Recognition with Prompt-conditioning Fine-tuning. (arXiv:2307.10274v1 [eess.AS])

    [http://arxiv.org/abs/2307.10274](http://arxiv.org/abs/2307.10274)

    本研究提出了一种零样本领域敏感语音识别方法，利用文本提示来生成领域敏感模型，通过微调预训练的端到端模型实现。实验结果表明，该方法在不同领域和提示上下文下均取得了良好的性能，词错误率降低达到最高33%。通过仅使用文本进行微调，该模型在医学对话数据集上的识别效果最佳，词错误率降低达到29%。

    

    本研究提出了一种方法来创建利用文本领域信息的领域敏感语音识别模型，通过将其生成条件化在给定的文本提示上实现。通过对预训练的端到端模型（Whisper）进行微调，从提示示例中学习，这一目标得以实现。我们展示了这种能力可以推广到不同的领域和各种提示上下文，我们的模型在来自不同领域的未见数据集上获得了多达33％的词错误率（WER）降低，例如医学对话，空中交通控制通信和金融会议等。考虑到音频-文本对数据的有限可用性，我们进一步将我们的方法扩展到仅文本微调，以实现领域敏感性和领域适应性。我们证明了我们的仅文本微调模型也可以关注各种提示上下文，该模型在医学对话数据集上的WER降低最多达到29％。

    In this work, we propose a method to create domain-sensitive speech recognition models that utilize textual domain information by conditioning its generation on a given text prompt. This is accomplished by fine-tuning a pre-trained, end-to-end model (Whisper) to learn from demonstrations with prompt examples. We show that this ability can be generalized to different domains and even various prompt contexts, with our model gaining a Word Error Rate (WER) reduction of up to 33% on unseen datasets from various domains, such as medical conversation, air traffic control communication, and financial meetings. Considering the limited availability of audio-transcript pair data, we further extend our method to text-only fine-tuning to achieve domain sensitivity as well as domain adaptation. We demonstrate that our text-only fine-tuned model can also attend to various prompt contexts, with the model reaching the most WER reduction of 29% on the medical conversation dataset.
    
[^152]: 分层线性模态连接

    Layerwise Linear Mode Connectivity. (arXiv:2307.06966v1 [cs.LG])

    [http://arxiv.org/abs/2307.06966](http://arxiv.org/abs/2307.06966)

    本文提出了一种分层线性模态连接方法用于联邦深度学习，通过解决模型漂移和高损失障壁的问题，能够有效提升全局模型的性能。

    

    在联邦设置中，我们在训练过程中多次对分离的本地模型进行聚合，以获得更强大的全局模型；最常见的聚合方法是参数的简单平均。理解在非凸设置（如联邦深度学习）中聚合何时以及为何有效是一个尚未解决的挑战，这阻碍了获得高性能的全局模型。在i.i.d.数据集上，频繁平均的联邦深度学习是成功的。然而，常见的观点是在独立训练期间，模型会相互漂移，因此在许多本地参数更新后平均可能不再起作用。这个问题可以从损失曲面的角度来看：对于非凸曲面上的点，平均值可能变得任意糟糕。通常用于解释联邦平均成功的局部凸性假设与经验证据相矛盾，显示不同模型之间存在高损失障壁。

    In the federated setup one performs an aggregation of separate local models multiple times during training in order to obtain a stronger global model; most often aggregation is a simple averaging of the parameters. Understanding when and why averaging works in a non-convex setup, such as federated deep learning, is an open challenge that hinders obtaining highly performant global models. On i.i.d.~datasets federated deep learning with frequent averaging is successful. The common understanding, however, is that during the independent training models are drifting away from each other and thus averaging may not work anymore after many local parameter updates. The problem can be seen from the perspective of the loss surface: for points on a non-convex surface the average can become arbitrarily bad. The assumption of local convexity, often used to explain the success of federated averaging, contradicts to the empirical evidence showing that high loss barriers exist between models from the v
    
[^153]: 具有谱偏差和内核-任务对齐的物理信息神经网络

    Spectral-Bias and Kernel-Task Alignment in Physically Informed Neural Networks. (arXiv:2307.06362v1 [stat.ML])

    [http://arxiv.org/abs/2307.06362](http://arxiv.org/abs/2307.06362)

    本文提出了一个综合的理论框架，解决了物理信息神经网络（PINN）设计和训练协议的选择问题。通过将超参数化神经网络和高斯过程回归等价起来，推导出了一种在大数据集限制下决定PINN预测的积分微分方程，以及通过原始微分方程中源项的谱分解来量化网络引入的隐含偏差。

    

    物理信息神经网络（PINN）是解决微分方程的一种有前景的新方法。与许多其他深度学习方法一样，PINN的设计和训练协议的选择需要精心制定。在这里，我们提出了一个综合的理论框架，对这个重要问题进行了阐述。通过利用超参数化神经网络和高斯过程回归（GPR）之间的等价性，我们推导出一种在大数据集限制下决定PINN预测的积分微分方程——神经信息方程（NIE）。该方程通过反映架构选择的内核项来补充原始方程，并通过原始微分方程中源项的谱分解来量化网络引入的隐含偏差。

    Physically informed neural networks (PINNs) are a promising emerging method for solving differential equations. As in many other deep learning approaches, the choice of PINN design and training protocol requires careful craftsmanship. Here, we suggest a comprehensive theoretical framework that sheds light on this important problem. Leveraging an equivalence between infinitely over-parameterized neural networks and Gaussian process regression (GPR), we derive an integro-differential equation that governs PINN prediction in the large data-set limit -- the Neurally-Informed Equation (NIE). This equation augments the original one by a kernel term reflecting architecture choices and allows quantifying implicit bias induced by the network via a spectral decomposition of the source term in the original differential equation.
    
[^154]: 通过奖励重新加权、重选和重新训练方法，改进了原型零件网络

    Improving Prototypical Part Networks with Reward Reweighing, Reselection, and Retraining. (arXiv:2307.03887v1 [cs.LG])

    [http://arxiv.org/abs/2307.03887](http://arxiv.org/abs/2307.03887)

    本研究通过引入奖励重新加权、重选和重新训练的方法，改进了原型零件网络的分类效果，解决了学习从图像的虚假或不一致的部分进行分类的问题。

    

    近年来，人们致力于开发深度可解释的图像分类方法，能够清楚地将模型的输出归因于数据的特定特征。其中一种方法是原型零件网络（ProtoPNet），它基于输入的有意义部分来尝试分类图像。然而，这种方法经常学习从图像的虚假或不一致的部分进行分类。为了解决这个问题，我们受到强化学习与人类反馈（RLHF）的最新发展启发，通过在CUB-200-2011数据集上收集人类原型质量的1-5分级注释，构建一个学习识别非虚假原型的奖励模型。我们提出了重新加权、重选和重新训练的原型零件网络（R3-ProtoPNet），该网络在ProtoPNet训练循环中增加了三个额外的步骤。

    In recent years, work has gone into developing deep interpretable methods for image classification that clearly attributes a model's output to specific features of the data. One such of these methods is the prototypical part network (ProtoPNet), which attempts to classify images based on meaningful parts of the input. While this method results in interpretable classifications, this method often learns to classify from spurious or inconsistent parts of the image. Hoping to remedy this, we take inspiration from the recent developments in Reinforcement Learning with Human Feedback (RLHF) to fine-tune these prototypes. By collecting human annotations of prototypes quality via a 1-5 scale on the CUB-200-2011 dataset, we construct a reward model that learns to identify non-spurious prototypes. In place of a full RL update, we propose the reweighted, reselected, and retrained prototypical part network (R3-ProtoPNet), which adds an additional three steps to the ProtoPNet training loop. The fir
    
[^155]: CellViT:用于精确细胞分割和分类的视觉Transformer

    CellViT: Vision Transformers for Precise Cell Segmentation and Classification. (arXiv:2306.15350v1 [eess.IV])

    [http://arxiv.org/abs/2306.15350](http://arxiv.org/abs/2306.15350)

    CellViT是一种基于Vision Transformer的深度学习架构，用于自动实例分割苏木精和伊红染色的组织样本中的细胞核。通过在PanNuke数据集上训练和评估，CellViT展示了其优越性。

    

    细胞核在苏木精和伊红染色的组织图像中的检测和分割是重要的临床任务，并且对各种应用至关重要。然而，由于细胞核染色和大小的差异、边界重叠和核聚集，这是一项具有挑战性的任务。虽然卷积神经网络在这个任务中已经被广泛使用，但我们探索了Transformer-based网络在这个领域的潜力。因此，我们引入了一种使用基于Vision Transformer的深度学习架构CellViT对数字化组织样本中的细胞核进行自动实例分割的新方法。CellViT在PanNuke数据集上进行训练和评估，该数据集是一个最具挑战性的细胞核实例分割数据集，包含了近20万个注释的细胞核，分为19种组织类型的5个临床重要类别。我们通过利用最近提出的大规模领域内和领域外预训练的Vision Transformer，展示了其优越性。

    Nuclei detection and segmentation in hematoxylin and eosin-stained (H&E) tissue images are important clinical tasks and crucial for a wide range of applications. However, it is a challenging task due to nuclei variances in staining and size, overlapping boundaries, and nuclei clustering. While convolutional neural networks have been extensively used for this task, we explore the potential of Transformer-based networks in this domain. Therefore, we introduce a new method for automated instance segmentation of cell nuclei in digitized tissue samples using a deep learning architecture based on Vision Transformer called CellViT. CellViT is trained and evaluated on the PanNuke dataset, which is one of the most challenging nuclei instance segmentation datasets, consisting of nearly 200,000 annotated Nuclei into 5 clinically important classes in 19 tissue types. We demonstrate the superiority of large-scale in-domain and out-of-domain pre-trained Vision Transformers by leveraging the recently
    
[^156]: DragDiffusion: 利用扩散模型进行交互式点基图像编辑

    DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing. (arXiv:2306.14435v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.14435](http://arxiv.org/abs/2306.14435)

    DragDiffusion是一个利用扩散模型进行交互式点基图像编辑的方法，通过优化扩散潜在实现精确的空间控制，以提高实际场景中的应用性。

    

    精确可控的图像编辑是一个具有挑战性的任务，引起了广泛的关注。最近，DragGAN实现了一个交互式的基于点的图像编辑框架，并以像素级精度实现了令人印象深刻的编辑结果。然而，由于该方法基于生成对抗网络（GAN），其通用性受限于预先训练的GAN模型的容量。在这项工作中，我们将这样的编辑框架扩展到扩散模型，并提出了DragDiffusion。通过利用大规模预训练的扩散模型，我们极大地提高了交互式基于点的编辑在实际场景中的适用性。虽然大多数现有的基于扩散的图像编辑方法基于文本嵌入，DragDiffusion优化扩散潜在来实现精确的空间控制。尽管扩散模型以迭代方式生成图像，但我们凭经验表明，在一个单独的步骤中优化扩散潜在已足以生成连贯的结果，从而使得该方法成为可能。

    Precise and controllable image editing is a challenging task that has attracted significant attention. Recently, DragGAN enables an interactive point-based image editing framework and achieves impressive editing results with pixel-level precision. However, since this method is based on generative adversarial networks (GAN), its generality is upper-bounded by the capacity of the pre-trained GAN models. In this work, we extend such an editing framework to diffusion models and propose DragDiffusion. By leveraging large-scale pretrained diffusion models, we greatly improve the applicability of interactive point-based editing in real world scenarios. While most existing diffusion-based image editing methods work on text embeddings, DragDiffusion optimizes the diffusion latent to achieve precise spatial control. Although diffusion models generate images in an iterative manner, we empirically show that optimizing diffusion latent at one single step suffices to generate coherent results, enabl
    
[^157]: 大型语言模型的简单而有效的剪枝方法

    A Simple and Effective Pruning Approach for Large Language Models. (arXiv:2306.11695v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.11695](http://arxiv.org/abs/2306.11695)

    本论文提出了一种称为Wanda的新颖、简单而有效的剪枝方法，用于大型语言模型，通过对每个输出上的权重按照最小幅度乘以对应的输入激活来进行剪枝，无需重新训练或更新权重。

    

    随着规模的增大，大型语言模型（LLMs）是网络剪枝方法的自然候选对象：这些方法在努力保持性能的同时，丢弃了网络权重的一个子集。然而，现有的方法要么需要重新训练，这对于十亿级别的LLMs来说很少可行，要么需要解决一个依赖二阶信息的权重重构问题，这也可能计算成本很高。在本文中，我们介绍了一种新颖的、简单但有效的剪枝方法，称为Wanda（基于权重和激活的剪枝），旨在对预训练的LLMs引入稀疏性。受到最近对LLMs中出现的大幅特征的发现的启发，我们的方法在每个输出上按照权重和对应的输入激活相乘的最小幅度来剪枝权重。值得注意的是，Wanda不需要重新训练或更新权重，剪枝后的LLM可以直接使用。我们在LLaMA和LLaMA-2上对我们的方法Wanda进行了彻底的评估。

    As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across vari
    
[^158]: 受遮蔽扩散模型是快速和注重隐私的学习器

    Masked Diffusion Models Are Fast and Privacy-Aware Learners. (arXiv:2306.11363v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.11363](http://arxiv.org/abs/2306.11363)

    该论文提出了一种基于先验的去噪训练框架，通过遮蔽学习和扩散模型的结合，实现了更高效的训练和生成更高质量的图像。

    

    扩散模型已成为图像生成的事实上技术，然而它们具有显著的计算开销，限制了该技术在研究社区中的广泛应用。我们提出了一种基于先验的去噪训练框架，首次将预训练和微调范式纳入扩散模型训练过程中，大大提升了训练效率，并在促进各种下游任务方面显示出潜力。我们的方法主要是通过遮蔽输入图像的高比例（例如高达90％），并利用遮蔽去噪得分匹配来去噪可见区域，从而引导扩散模型从训练数据中学习更显著的特征作为先验知识。通过在预训练阶段使用遮蔽学习，我们在CelebA-HQ $256 \times 256$像素空间上高效地训练了基于ViT的扩散模型，实现了4倍加速，并提高了生成图像的质量，与去噪相比。

    Diffusion models have emerged as the \emph{de-facto} technique for image generation, yet they entail significant computational overhead, hindering the technique's broader application in the research community. We propose a prior-based denoising training framework, the first to incorporate the pre-train and fine-tune paradigm into the diffusion model training process, which substantially improves training efficiency and shows potential in facilitating various downstream tasks. Our approach centers on masking a high proportion (e.g., up to 90\%) of the input image and employing masked denoising score matching to denoise the visible areas, thereby guiding the diffusion model to learn more salient features from training data as prior knowledge. By utilizing masked learning in a pre-training stage, we efficiently train the ViT-based diffusion model on CelebA-HQ $256 \times 256$ in the pixel space, achieving a 4x acceleration and enhancing the quality of generated images compared to denoisin
    
[^159]: 自适应带有自动调整客户端的联邦学习

    Adaptive Federated Learning with Auto-Tuned Clients. (arXiv:2306.11201v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.11201](http://arxiv.org/abs/2306.11201)

    本研究提出了一种自适应的联邦学习方法，其中包括了自动调整客户端的步长规则。实证结果表明，这种方法在各种联邦学习场景中具有显著的优势。

    

    联邦学习（FL）是一种分布式机器学习框架，通过参与的客户端在不共享数据的情况下，通过多次协作步骤训练中央服务器的全局模型。虽然FL是一个灵活的框架，其中本地数据的分布、参与率和每个客户端的计算能力可能大大变化，但这种灵活性也带来了许多新的挑战，特别是在客户端的超参数调整方面。我们提出了$\Delta$-SGD，这是一种简单的SGD步长规则，使每个客户端能够通过适应该客户端正在优化的函数的局部平滑性来使用自己的步长。我们提供了理论和实证结果，展示了客户端适应性在各种FL场景中的好处。

    Federated learning (FL) is a distributed machine learning framework where the global model of a central server is trained via multiple collaborative steps by participating clients without sharing their data. While being a flexible framework, where the distribution of local data, participation rate, and computing power of each client can greatly vary, such flexibility gives rise to many new challenges, especially in the hyperparameter tuning on the client side. We propose $\Delta$-SGD, a simple step size rule for SGD that enables each client to use its own step size by adapting to the local smoothness of the function each client is optimizing. We provide theoretical and empirical results where the benefit of the client adaptivity is shown in various FL scenarios.
    
[^160]: L2归一化技术在简单高质量OoD检测中的应用

    Simple High Quality OoD Detection with L2 Normalization. (arXiv:2306.04072v1 [cs.LG])

    [http://arxiv.org/abs/2306.04072](http://arxiv.org/abs/2306.04072)

    本篇论文介绍了在ResNet模型训练中应用L2归一化技术，可以以几乎没有成本的方式实现与最先进的OoD检测性能相媲美的结果，测试时仅需移除L2归一化即可。

    

    我们在标准的ResNet模型训练中提出了一种简单的修改方法--在特征空间中进行L2归一化--能够产生与最先进的OoD检测性能相媲美的结果。当在测试时移除L2归一化时，特征向量的L2范数成为网络不确定性的一个惊人的替代者，而当没有L2归一化训练时，这种行为却没有那么有效。直观上，熟悉的图像会产生大的向量，而陌生的图像则会产生小的向量。值得注意的是，在训练时几乎没有额外的成本，在测试时也没有成本。

    We propose a simple modification to standard ResNet architectures during training--L2 normalization over feature space--that produces results competitive with state-of-the-art Out-of-Distribution (OoD) detection performance. When L2 normalization is removed at test time, the L2 norm of feature vectors becomes a surprisingly good proxy for network uncertainty, whereas this behaviour is not nearly as effective when training without L2 normalization. Intuitively, familiar images result in large magnitude vectors, while unfamiliar images result in small magnitudes. Notably, this is achievable with almost no additional cost during training, and no cost at test time.
    
[^161]: 一种基于Polyak-{\L}ojasiewicz条件的双层学习的广义交替方法

    A Generalized Alternating Method for Bilevel Learning under the Polyak-{\L}ojasiewicz Condition. (arXiv:2306.02422v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2306.02422](http://arxiv.org/abs/2306.02422)

    本研究提出了一种基于Polyak-{\L}ojasiewicz条件的双层学习的广义交替方法，即GALET，可以用于解决非凸下层目标的双层问题。该方法可以在$\tilde{\cal O}(\epsilon^{-1})$迭代次数内实现问题的$\epsilon$-静态度量。

    

    双层优化近年来因其在超参数优化、元学习和强化学习等新兴机器学习领域中的应用而重新引起人们的关注。最近的结果显示，对于具有强凸下层目标的双层问题，简单的交替（隐式）基于梯度的算法可以实现与单层梯度下降（GD）相同的收敛速率。然而，对于超出此基本设置的双层问题，尚不清楚是否可以推广该结果。在本文中，我们提出了一种基于满足Polyak-{\L}ojasiewicz (PL)条件的非凸下层目标的双层优化的广义交替方法（GALET）。我们首先介绍了所考虑的双层问题的一个静态度量，它推广了现有的度量。然后我们证明GALET在$\tilde{\cal O}(\epsilon^{-1})$迭代次数内实现了所考虑问题的$\epsilon$-静态度量。

    Bilevel optimization has recently regained interest owing to its applications in emerging machine learning fields such as hyperparameter optimization, meta-learning, and reinforcement learning. Recent results have shown that simple alternating (implicit) gradient-based algorithms can achieve the same convergence rate of single-level gradient descent (GD) for bilevel problems with a strongly convex lower-level objective. However, it remains unclear whether this result can be generalized to bilevel problems beyond this basic setting. In this paper, we propose a Generalized ALternating mEthod for bilevel opTimization (GALET) with a nonconvex lower-level objective that satisfies the Polyak-{\L}ojasiewicz (PL) condition. We first introduce a stationary metric for the considered bilevel problems, which generalizes the existing metric. We then establish that GALET achieves an $\epsilon$-stationary metric for the considered problem within $\tilde{\cal O}(\epsilon^{-1})$ iterations, which match
    
[^162]: Vandermonde神经算子

    Vandermonde Neural Operators. (arXiv:2305.19663v1 [cs.LG])

    [http://arxiv.org/abs/2305.19663](http://arxiv.org/abs/2305.19663)

    本文提出了一种新的神经网络结构，Vandermonde神经算子，能够有效地处理非均匀分布点上的输入数据，同时在速度和准确性上相较于以前的方法有所提升。

    

    Fourier神经算子（FNO）已成为非常受欢迎的机器学习体系结构，用于学习操作符，特别是那些在PDE中出现的操作符。然而，由于FNO依赖于快速傅里叶变换以实现计算效率，所以该体系结构可能仅限于笛卡尔网格上的输入数据。在这里，我们将FNO推广到处理分布在非均匀点分布上的输入数据。我们提出的模型称为Vandermonde神经运算符（VNO），利用Vandermonde结构矩阵来高效地计算正向和反向的傅里叶变换，即使在任意分布的点上也可以如此。我们进行了数值实验，证明VNO可以比FNO快得多，同时保持可比的准确性，并改进了可比的非均匀方法（如Geo-FNO）的准确性。

    Fourier Neural Operators (FNOs) have emerged as very popular machine learning architectures for learning operators, particularly those arising in PDEs. However, as FNOs rely on the fast Fourier transform for computational efficiency, the architecture can be limited to input data on equispaced Cartesian grids. Here, we generalize FNOs to handle input data on non-equispaced point distributions. Our proposed model, termed as Vandermonde Neural Operator (VNO), utilizes Vandermonde-structured matrices to efficiently compute forward and inverse Fourier transforms, even on arbitrarily distributed points. We present numerical experiments to demonstrate that VNOs can be significantly faster than FNOs, while retaining comparable accuracy, and improve upon accuracy of comparable non-equispaced methods such as the Geo-FNO.
    
[^163]: 自动化搜索空间生成的神经架构搜索

    Automated Search-Space Generation Neural Architecture Search. (arXiv:2305.18030v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18030](http://arxiv.org/abs/2305.18030)

    Automated Search-Space Generation Neural Architecture Search (ASGNAS) is an automated system that trains general DNNs covering all candidate connections and operations and produces high-performing sub-networks. It minimizes the need for human expertise and manual intervention by automatically generating the search space and utilizing a Hierarchical Half-Space Projected Gradient (H2SPG) to ensure network validity and performance.

    

    现有的神经架构搜索（NAS）方法通常依赖于事先手工创建搜索空间来搜索通用深度神经网络（DNN）中的最优子网络。这样的要求使得在没有显著的人工专业知识和手动干预的情况下将它们扩展到通用场景变得具有挑战性。为了克服这些限制，我们提出了Automated Search-Space Generation Neural Architecture Search（ASGNAS），可能是第一个自动化系统，以一次性的方式训练覆盖所有候选连接和操作的通用DNN，并产生高性能的子网络。技术上，ASGNAS具有三个显著的贡献以减少人力工作：（i）通用DNN的自动搜索空间生成；（ii）利用生成的搜索空间内的层次结构和依赖关系的Hierarchical Half-Space Projected Gradient（H2SPG），在优化过程中确保网络的有效性，并可靠地产生具有高性能和稀疏性的解决方案。

    To search an optimal sub-network within a general deep neural network (DNN), existing neural architecture search (NAS) methods typically rely on handcrafting a search space beforehand. Such requirements make it challenging to extend them onto general scenarios without significant human expertise and manual intervention. To overcome the limitations, we propose Automated Search-Space Generation Neural Architecture Search (ASGNAS), perhaps the first automated system to train general DNNs that cover all candidate connections and operations and produce high-performing sub-networks in the one shot manner. Technologically, ASGNAS delivers three noticeable contributions to minimize human efforts: (i) automated search space generation for general DNNs; (ii) a Hierarchical Half-Space Projected Gradient (H2SPG) that leverages the hierarchy and dependency within generated search space to ensure the network validity during optimization, and reliably produces a solution with both high performance an
    
[^164]: 基于虚拟粒子随机逼近的可证速限制变种的SVGD算法。

    Provably Fast Finite Particle Variants of SVGD via Virtual Particle Stochastic Approximation. (arXiv:2305.17558v1 [stat.ML])

    [http://arxiv.org/abs/2305.17558](http://arxiv.org/abs/2305.17558)

    本论文提出了两种基于虚拟粒子随机逼近的可证速限制变种的SVGD算法，具有可证速的有限粒子收敛率。

    

    Stein变分梯度下降（SVGD）是一种流行的变分推断算法，它模拟相互作用的粒子系统以近似从目标分布中采样，具有各种领域的令人印象深刻的经验性能。在理论上，它的群体（即，无限粒子）极限动力学已经得到了很好的研究，但是SVGD在有限粒子体制下的行为则不太清楚。在这项工作中，我们设计了两种计算效率高的SVGD变体，即VP-SVGD（从概念上讲很优雅）和GB-SVGD（从经验上看很有效），具有可证速的有限粒子收敛率。我们引入了“虚拟粒子”的概念，并在概率测度空间中开发了人口极限SVGD动力学的新型随机逼近方法，它们可以使用有限数量的粒子精确实现。我们的算法可以看作是SVGD的特定随机批处理逼近，比普通方法更具计算效率。

    Stein Variational Gradient Descent (SVGD) is a popular variational inference algorithm which simulates an interacting particle system to approximately sample from a target distribution, with impressive empirical performance across various domains. Theoretically, its population (i.e, infinite-particle) limit dynamics is well studied but the behavior of SVGD in the finite-particle regime is much less understood. In this work, we design two computationally efficient variants of SVGD, namely VP-SVGD (which is conceptually elegant) and GB-SVGD (which is empirically effective), with provably fast finite-particle convergence rates. We introduce the notion of \emph{virtual particles} and develop novel stochastic approximations of population-limit SVGD dynamics in the space of probability measures, which are exactly implementable using a finite number of particles. Our algorithms can be viewed as specific random-batch approximations of SVGD, which are computationally more efficient than ordinar
    
[^165]: 关于深度网络表示中概念空间的凸性研究

    On convex conceptual regions in deep network representations. (arXiv:2305.17154v1 [cs.LG])

    [http://arxiv.org/abs/2305.17154](http://arxiv.org/abs/2305.17154)

    本文研究了深度网络表示中概念空间的凸性对泛化能力、小样本学习和主观一致性的影响，发现近似凸性在多个应用领域中广泛存在。

    

    人机对齐的研究旨在理解潜在空间的几何结构和与人类表征的对应关系。Gardenfors的概念空间是理解人类表征的一个重要框架。在概念空间中，对象区域的凸性被认为是促进泛化能力、小样本学习和主观一致性的重要机制。基于这些洞见，本文研究了机器学习中学习的潜在空间中概念区域的凸性。作者开发了一组用于测量采样数据中凸性的工具，并评估了最先进深度网络中的层表示中的凸性。结果表明，凸性对于基本的重新参数化是稳健的，因此作为机器学习潜在空间质量的一个重要特征是有意义的。作者发现，近似凸性在神经表示中广泛存在于多个应用领域，包括图像、音频、人类活动、文本和脑数据。

    The current study of human-machine alignment aims at understanding the geometry of latent spaces and the correspondence to human representations. G\"ardenfors' conceptual spaces is a prominent framework for understanding human representations. Convexity of object regions in conceptual spaces is argued to promote generalizability, few-shot learning, and intersubject alignment. Based on these insights, we investigate the notion of convexity of concept regions in machine-learned latent spaces. We develop a set of tools for measuring convexity in sampled data and evaluate emergent convexity in layered representations of state-of-the-art deep networks. We show that convexity is robust to basic re-parametrization, hence, meaningful as a quality of machine-learned latent spaces. We find that approximate convexity is pervasive in neural representations in multiple application domains, including models of images, audio, human activity, text, and brain data. We measure convexity separately for l
    
[^166]: 基于密度比估计的半监督学习贝叶斯优化

    Density Ratio Estimation-based Bayesian Optimization with Semi-Supervised Learning. (arXiv:2305.15612v1 [cs.LG])

    [http://arxiv.org/abs/2305.15612](http://arxiv.org/abs/2305.15612)

    该论文提出了一种基于密度比估计和半监督学习的贝叶斯优化方法，通过使用监督分类器代替密度比来估计全局最优解的两组数据的类别概率，避免了分类器对全局解决方案过于自信的问题。

    

    贝叶斯优化在科学与工程的多个领域受到了广泛关注，因为它能高效地找到昂贵黑盒函数的全局最优解。通常，一个概率回归模型，如高斯过程、随机森林和贝叶斯神经网络，被广泛用作替代函数，用于模拟在给定输入和训练数据集的情况下函数评估的显式分布。除了基于概率回归的贝叶斯优化，基于密度比估计的贝叶斯优化已被提出来估计相对于全局最优解相对接近和相对远离的两组密度比。为了进一步发展这一研究，可以使用监督分类器来估计这两组的类别概率，而不是密度比。然而，此策略中使用的监督分类器倾向于对全局解决方案过于自信。

    Bayesian optimization has attracted huge attention from diverse research areas in science and engineering, since it is capable of finding a global optimum of an expensive-to-evaluate black-box function efficiently. In general, a probabilistic regression model, e.g., Gaussian processes, random forests, and Bayesian neural networks, is widely used as a surrogate function to model an explicit distribution over function evaluations given an input to estimate and a training dataset. Beyond the probabilistic regression-based Bayesian optimization, density ratio estimation-based Bayesian optimization has been suggested in order to estimate a density ratio of the groups relatively close and relatively far to a global optimum. Developing this line of research further, a supervised classifier can be employed to estimate a class probability for the two groups instead of a density ratio. However, the supervised classifiers used in this strategy tend to be overconfident for a global solution candid
    
[^167]: NeuralMatrix: 将整个神经网络移动到通用矩阵乘法以实现高效推理

    NeuralMatrix: Moving Entire Neural Networks to General Matrix Multiplication for Efficient Inference. (arXiv:2305.14405v1 [cs.LG])

    [http://arxiv.org/abs/2305.14405](http://arxiv.org/abs/2305.14405)

    NeuralMatrix是一种框架，能够在单个通用矩阵乘法加速器上计算深度神经网络(DNNs)，并可在保持推理准确度的情况下实现高达113倍至19.44倍的性能提升。

    

    本研究介绍了一种名为NeuralMatrix的新型框架，它使得可以在单个通用矩阵乘法（GEMM）加速器上计算多功能的深度神经网络（DNNs）。该方法克服了基于ASIC的加速器的专用性限制，同时实现了与CPU和GPU等通用处理器相比的应用特定加速水平。我们解决了将DNN计算中的线性和非线性运算映射到通用矩阵乘法以及使用GEMM加速器对DNN推理准确性的影响的挑战。我们在来自三种流行类别的各种DNN模型上进行了大量实验（即CNN，Transformers和GNN）作为示例的支撑模型。我们的结果表明，将DNN转换为通用矩阵乘法后仅会出现高达2.02％的准确度损失，同时将吞吐量与功率的比值与CPU和GPU相比提高了113倍到19.44倍。

    In this study, we introduce NeuralMatrix, a novel framework that enables the computation of versatile deep neural networks (DNNs) on a single general matrix multiplication (GEMM) accelerator. The proposed approach overcomes the specificity limitations of ASIC-based accelerators while achieving application-specific acceleration levels compared to general-purpose processors such as CPUs and GPUs. We address the challenges of mapping both linear and nonlinear operations in DNN computation to general matrix multiplications and the impact of using a GEMM accelerator on DNN inference accuracy. Extensive experiments are conducted on various DNN models from three popular categories (i.e., CNN, Transformers, and GNN) as illustrative backbone models. Our results demonstrate that DNNs suffer only up to a 2.02% accuracy loss after being converted to general matrix multiplication, while achieving 113x to 19.44x improvements in throughput per power compared to CPUs and GPUs.
    
[^168]: CoreDiff: 上下文误差调制广义扩散模型用于低剂量CT去噪和泛化

    CoreDiff: Contextual Error-Modulated Generalized Diffusion Model for Low-Dose CT Denoising and Generalization. (arXiv:2304.01814v1 [eess.IV])

    [http://arxiv.org/abs/2304.01814](http://arxiv.org/abs/2304.01814)

    本文提出了一种新的上下文误差调制广义扩散模型（CoreDiff），用于低剂量CT（LDCT）的去噪。该模型利用LDCT图像来消除随机高斯噪声并模拟CT退化的物理过程，减少采样步骤，并引入上下文误差调制以增强鲁棒性和泛化能力。

    

    低剂量计算机断层扫描（CT）图像由于光子匮乏和电子噪声而受到噪声和伪影的影响。最近，一些研究尝试使用扩散模型来解决以前基于深度学习的去噪模型遇到的过度平滑和训练不稳定性问题。然而，由于涉及大量采样步骤，扩散模型的推理时间很长。最近，冷扩散模型推广了经典扩散模型，并具有更大的灵活性。受冷扩散的启发，本文提出了一种新颖的针对低剂量CT（LDCT）去噪的上下文误差调制广义扩散模型，称为CoreDiff。首先，CoreDiff利用LDCT图像来消除随机高斯噪声，并采用新型均值保持退化算子来模拟CT退化的物理过程，由于启动采样过程的信息丰富的LDCT图像，大幅减少采样步骤。其次，为缓解扩散模型中过多采样步骤的问题，引入上下文误差调制，使CoreDiff更具有鲁棒性和泛化能力。

    Low-dose computed tomography (CT) images suffer from noise and artifacts due to photon starvation and electronic noise. Recently, some works have attempted to use diffusion models to address the over-smoothness and training instability encountered by previous deep-learning-based denoising models. However, diffusion models suffer from long inference times due to the large number of sampling steps involved. Very recently, cold diffusion model generalizes classical diffusion models and has greater flexibility. Inspired by the cold diffusion, this paper presents a novel COntextual eRror-modulated gEneralized Diffusion model for low-dose CT (LDCT) denoising, termed CoreDiff. First, CoreDiff utilizes LDCT images to displace the random Gaussian noise and employs a novel mean-preserving degradation operator to mimic the physical process of CT degradation, significantly reducing sampling steps thanks to the informative LDCT images as the starting point of the sampling process. Second, to allevi
    
[^169]: FedFTN: 多机构低计数PET去噪的个性化联邦学习与深度特征转换网络

    FedFTN: Personalized Federated Learning with Deep Feature Transformation Network for Multi-institutional Low-count PET Denoising. (arXiv:2304.00570v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2304.00570](http://arxiv.org/abs/2304.00570)

    本研究提出了FedFTN，一种个性化的联邦学习策略，用于解决多机构低计数PET去噪中的领域差异问题。该方法通过深度特征转换网络来改善低计数PET图像质量，同时避免了集中式数据集的隐私和安全问题。

    

    低计数PET是降低辐射暴露和采集时间的有效方法，但重建图像往往受到信噪比低的影响，从而影响诊断和其他下游任务。深度学习的最新进展在改善低计数PET图像质量方面显示出巨大潜力，但由于患者数据的隐私和安全问题，从多个机构获取大型、集中和多样化的数据集以训练强大的模型是困难的。此外，不同机构的低计数PET数据可能具有不同的数据分布，因此需要个性化的模型。虽然之前的联邦学习算法可以在不需要聚合本地数据的情况下实现多机构的协作训练，但解决多机构低计数PET去噪应用中的领域差异仍然是一个挑战，并且还未完全探索。在这项工作中，我们提出了FedFTN，一种个性化的联邦学习策略。

    Low-count PET is an efficient way to reduce radiation exposure and acquisition time, but the reconstructed images often suffer from low signal-to-noise ratio (SNR), thus affecting diagnosis and other downstream tasks. Recent advances in deep learning have shown great potential in improving low-count PET image quality, but acquiring a large, centralized, and diverse dataset from multiple institutions for training a robust model is difficult due to privacy and security concerns of patient data. Moreover, low-count PET data at different institutions may have different data distribution, thus requiring personalized models. While previous federated learning (FL) algorithms enable multi-institution collaborative training without the need of aggregating local data, addressing the large domain shift in the application of multi-institutional low-count PET denoising remains a challenge and is still highly under-explored. In this work, we propose FedFTN, a personalized federated learning strategy
    
[^170]: 多任务学习用于移植后死因分析：以肝移植为例的案例研究

    Multi-Task Learning for Post-transplant Cause of Death Analysis: A Case Study on Liver Transplant. (arXiv:2304.00012v1 [cs.LG])

    [http://arxiv.org/abs/2304.00012](http://arxiv.org/abs/2304.00012)

    CoD-MTL是一个新的死因分析框架，采用多任务学习来共同模拟不同CoD预测任务之间的语义关系，并且利用树蒸馏策略进行多任务学习，在具备树模型和多任务学习的优点中发掘最大化的利益。该框架可以提供精确可靠的CoD预测，具有重要的临床应用价值。

    

    器官移植是某些末期疾病（如肝衰竭）的基本治疗方法。分析器官移植后的死因可以为临床决策提供有力的工具，包括个性化治疗和器官分配。然而，传统方法如MELD评分和传统机器学习方法在死因分析中存在两个主要的数据和模型相关挑战，这限制了它们在此领域中的应用。为了克服这一难题，我们提出了一个新的框架CoD-MTL，利用多任务学习来共同模拟不同CoD预测任务之间的语义关系。具体来讲，我们开发了一种新的树蒸馏策略用于多任务学习，结合了树模型和多任务学习的优点。实验结果表明，我们的框架可以提供精确可靠的CoD预测。我们进行了一个案例研究，以展示该框架的临床重要性。

    Organ transplant is the essential treatment method for some end-stage diseases, such as liver failure. Analyzing the post-transplant cause of death (CoD) after organ transplant provides a powerful tool for clinical decision making, including personalized treatment and organ allocation. However, traditional methods like Model for End-stage Liver Disease (MELD) score and conventional machine learning (ML) methods are limited in CoD analysis due to two major data and model-related challenges. To address this, we propose a novel framework called CoD-MTL leveraging multi-task learning to model the semantic relationships between various CoD prediction tasks jointly. Specifically, we develop a novel tree distillation strategy for multi-task learning, which combines the strength of both the tree model and multi-task learning. Experimental results are presented to show the precise and reliable CoD predictions of our framework. A case study is conducted to demonstrate the clinical importance of 
    
[^171]: 一个面向RNN-T的令牌级波束搜索算法

    A Token-Wise Beam Search Algorithm for RNN-T. (arXiv:2302.14357v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14357](http://arxiv.org/abs/2302.14357)

    提出了一个面向RNN-T的令牌级波束搜索算法，通过批处理联合网络的调用，实现了20%至96%的解码加速，并且通过汇总发射概率在找到最可能的模型输出方面取得了11%的最优错误率提高。

    

    用于语音识别的标准循环神经网络转录器 (RNN-T) 解码算法在时间轴上进行迭代，即在移动到下一个时间步之前解码一个时间步。这些算法导致联合网络的调用次数大大增加，这在之前的研究中已被证明是降低解码速度的重要因素。我们提出了一种解码波束搜索算法，它在一段时间步骤中批处理联合网络的调用，这导致在所有实验模型和设置中一致地获得20%至96%的解码加速。此外，在一段时间内汇总发射概率可以被看作是找到最可能的模型输出的更好近似，使得我们的算法在段大小增加时相对于最优错误率提高了最多11%，并稍微改善了通用错误率。

    Standard Recurrent Neural Network Transducers (RNN-T) decoding algorithms for speech recognition are iterating over the time axis, such that one time step is decoded before moving on to the next time step. Those algorithms result in a large number of calls to the joint network, which were shown in previous work to be an important factor that reduces decoding speed. We present a decoding beam search algorithm that batches the joint network calls across a segment of time steps, which results in 20%-96% decoding speedups consistently across all models and settings experimented with. In addition, aggregating emission probabilities over a segment may be seen as a better approximation to finding the most likely model output, causing our algorithm to improve oracle word error rate by up to 11% relative as the segment size increases, and to slightly improve general word error rate.
    
[^172]: 可微异常检测实现鲁棒的深度多模态分析

    Differentiable Outlier Detection Enable Robust Deep Multimodal Analysis. (arXiv:2302.05608v1 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2302.05608](http://arxiv.org/abs/2302.05608)

    本研究提出了一个可微的异常检测方法来实现鲁棒的深度多模态分析，通过借助明确的知识图谱和交互式的区分外部领域层来过滤噪声。在多个视觉和语言任务中得到了良好的应用效果。

    

    在训练和推理过程中，深度网络模型通常只是归纳式的使用。因此，当这些模型用于预测时，往往无法捕捉到对象（或概念）之间在群体层面上存在的语义信息和隐含依赖关系。此外，在大规模和嘈杂的环境中如何以反向传播友好的方式指定领域或先验模态知识仍然不清楚。在这项工作中，我们提出了一个端到端的视觉和语言模型，其中包括明确的知识图谱。我们还引入了一个使用隐式网络操作符的交互式区分外部领域的层。该层用于过滤由外部知识库带来的噪声。在实践中，我们在不同的数据集上应用我们的模型进行多个视觉和语言下游任务，包括视觉问答、视觉推理和图像文本检索。我们的实验结果表明，在大规模和嘈杂的环境中，可以去除噪声。

    Often, deep network models are purely inductive during training and while performing inference on unseen data. Thus, when such models are used for predictions, it is well known that they often fail to capture the semantic information and implicit dependencies that exist among objects (or concepts) on a population level. Moreover, it is still unclear how domain or prior modal knowledge can be specified in a backpropagation friendly manner, especially in large-scale and noisy settings. In this work, we propose an end-to-end vision and language model incorporating explicit knowledge graphs. We also introduce an interactive out-of-distribution (OOD) layer using implicit network operator. The layer is used to filter noise that is brought by external knowledge base. In practice, we apply our model on several vision and language downstream tasks including visual question answering, visual reasoning, and image-text retrieval on different datasets. Our experiments show that it is possible to de
    
[^173]: 基于邻居同质性的图卷积网络

    Neighborhood Homophily-based Graph Convolutional Network. (arXiv:2301.09851v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.09851](http://arxiv.org/abs/2301.09851)

    本文提出了一种基于邻居同质性的图卷积网络 (NHGCN) 模型，利用新指标 Neighborhood Homophily (NH) 测量节点邻域中的标签复杂度或纯度。实验证明 NHGCN 模型在节点分类和图分类任务中表现优异并显著超过最先进模型。

    

    图神经网络在图形任务中表现出强大的能力。然而，许多真实世界的图形是异构的，这挑战了经典图神经网络的同质性假设。为了解决这个普适性问题，许多研究加深网络或连接中间表示，但这并不会本质上改变邻居聚合并引入噪声。最近的研究提出了新的指标来表征同质性，但很少考虑所提出指标和模型之间的相关性。本文首先设计了一种新的指标 Neighborhood Homophily (NH)，用于测量节点邻域中的标签复杂度或纯度。此外，我们将该指标融入到经典的图卷积网络 (GCN) 结构中，提出了 NHGCN 模型。在该框架中，邻居被预估的 NH 值分组，从不同通道进行聚合。多项基准数据集上的实验证明，NHGCN 在节点分类和图分类任务中始终优于最先进的模型。

    Graph neural networks (GNNs) have been proved powerful in graph-oriented tasks. However, many real-world graphs are heterophilous, challenging the homophily assumption of classical GNNs. To solve the universality problem, many studies deepen networks or concatenate intermediate representations, which does not inherently change neighbor aggregation and introduces noise. Recent studies propose new metrics to characterize the homophily, but rarely consider the correlation of the proposed metrics and models. In this paper, we first design a new metric, Neighborhood Homophily (\textit{NH}), to measure the label complexity or purity in node neighborhoods. Furthermore, we incorporate the metric into the classical graph convolutional network (GCN) architecture and propose \textbf{N}eighborhood \textbf{H}omophily-based \textbf{G}raph \textbf{C}onvolutional \textbf{N}etwork (\textbf{NHGCN}). In this framework, neighbors are grouped by estimated \textit{NH} values and aggregated from different ch
    
[^174]: 隐式卷积核用于可定向卷积神经网络

    Implicit Convolutional Kernels for Steerable CNNs. (arXiv:2212.06096v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.06096](http://arxiv.org/abs/2212.06096)

    本文提出了一种使用隐式神经表示的方法来参数化可定向卷积核，从而实现了简单灵活的构建可定向卷积神经网络的方法，能够推广到任何具有等变MLP的群G。

    

    可定向卷积神经网络提供了构建与平移和其他变换等同变换的神经网络的通用框架，这些变换属于基于原点保持的群G，例如反射和旋转。它们依赖于通过在核空间上强加特定于群G的等变性约束来解析求解得到的G-定向卷积核的标准卷积。由于解决方案对特定的群G定制，核基础的实现不能推广到其他对称变换，这导致了通用群等变模型的开发复杂化。我们提出使用通过多层感知器(MLPs)参数化G-定向卷积核的隐式神经表示。所得到的框架提供了一种简单灵活的实现可定向卷积神经网络的方法，并且对于任何可以构建G-等变MLP的群G都可以推广。我们在多个任务上证明了我们的方法的有效性，包括N体模拟。

    Steerable convolutional neural networks (CNNs) provide a general framework for building neural networks equivariant to translations and other transformations belonging to an origin-preserving group $G$, such as reflections and rotations. They rely on standard convolutions with $G$-steerable kernels obtained by analytically solving the group-specific equivariance constraint imposed onto the kernel space. As the solution is tailored to a particular group $G$, the implementation of a kernel basis does not generalize to other symmetry transformations, which complicates the development of general group equivariant models. We propose using implicit neural representation via multi-layer perceptrons (MLPs) to parameterize $G$-steerable kernels. The resulting framework offers a simple and flexible way to implement Steerable CNNs and generalizes to any group $G$ for which a $G$-equivariant MLP can be built. We prove the effectiveness of our method on multiple tasks, including N-body simulations,
    
[^175]: 多领域长尾学习通过增强解缠表示

    Multi-Domain Long-Tailed Learning by Augmenting Disentangled Representations. (arXiv:2210.14358v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14358](http://arxiv.org/abs/2210.14358)

    本研究针对多领域长尾学习问题提出了一种方法TALLY，通过混合示例的语义表示和域相关干扰，使用选择性均衡采样策略进行数据增强，同时利用域不变的类原型改善语义表示的解缠。在多个基准测试中验证了TALLY的有效性。

    

    许多现实分类问题中都存在不可避免的长尾类别不平衡问题。目前解决这个问题的方法只考虑所有示例来自同一分布的情况。然而，在许多情况下，存在多个领域具有不同的类别不平衡情况。我们研究了这个多领域长尾学习问题，并旨在产生一个能够在所有类别和领域中都具有良好泛化性能的模型。为实现这一目标，我们引入了TALLY，一种解决多领域长尾学习问题的方法。基于提出的选择性均衡采样策略，TALLY通过将一个示例的语义表示与另一个示例的域相关干扰混合，产生一个用作数据增强的新表示。为了改善语义表示的解缠，TALLY进一步利用一个域不变的类原型来平均掉域特定效应。我们在多个基准测试上评估了TALLY。

    There is an inescapable long-tailed class-imbalance issue in many real-world classification problems. Current methods for addressing this problem only consider scenarios where all examples come from the same distribution. However, in many cases, there are multiple domains with distinct class imbalance. We study this multi-domain long-tailed learning problem and aim to produce a model that generalizes well across all classes and domains. Towards that goal, we introduce TALLY, a method that addresses this multi-domain long-tailed learning problem. Built upon a proposed selective balanced sampling strategy, TALLY achieves this by mixing the semantic representation of one example with the domain-associated nuisances of another, producing a new representation for use as data augmentation. To improve the disentanglement of semantic representations, TALLY further utilizes a domain-invariant class prototype that averages out domain-specific effects. We evaluate TALLY on several benchmarks and 
    
[^176]: 健壮的一次性歌声转换

    Robust One-Shot Singing Voice Conversion. (arXiv:2210.11096v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2210.11096](http://arxiv.org/abs/2210.11096)

    本研究提出了一种健壮的一次性歌声转换模型，通过使用生成对抗网络和特定的条件来实现高质量的任意到任意的歌声转换。所提出的模型通过部分域条件和音高分布匹配等方法，能够泛化到未知歌手，并且在面对带有混响和伴奏音乐的歌声时仍然能够保持稳定性。

    

    最近在深度生成模型方面取得的进展已经提高了语音领域中声音转换的质量。然而，对于未知歌手的高质量歌声转换仍然具有挑战性，原因是在音高、音量和发音方面存在更多各种各样的音乐表达。此外，歌声往往是带有混响和伴奏音乐的录音，这使得歌声转换变得更加困难。在这项工作中，我们提出了一种健壮的一次性歌声转换 (ROSVC)，即使在这种失真的歌声上也能稳定地进行任意到任意的歌声转换。为此，我们首先提出了基于生成对抗网络的一次性歌声转换模型，通过部分域条件和学习精确恢复目标音高的音高分布匹配和AdaIN-skip条件，能够泛化到未知歌手。然后，我们提出了一个称为Robustify的两阶段训练方法，该方法在第一阶段使用清洁数据对一次性歌声转换模型进行训练，以确保高质量的转换，并引入增强模块

    Recent progress in deep generative models has improved the quality of voice conversion in the speech domain. However, high-quality singing voice conversion (SVC) of unseen singers remains challenging due to the wider variety of musical expressions in pitch, loudness, and pronunciation. Moreover, singing voices are often recorded with reverb and accompaniment music, which make SVC even more challenging. In this work, we present a robust one-shot SVC (ROSVC) that performs any-to-any SVC robustly even on such distorted singing voices. To this end, we first propose a one-shot SVC model based on generative adversarial networks that generalizes to unseen singers via partial domain conditioning and learns to accurately recover the target pitch via pitch distribution matching and AdaIN-skip conditioning. We then propose a two-stage training method called Robustify that train the one-shot SVC model in the first stage on clean data to ensure high-quality conversion, and introduces enhancement mo
    
[^177]: AMPNet: 基于注意力的消息传递用于图神经网络

    AMPNet: Attention as Message Passing for Graph Neural Networks. (arXiv:2210.09475v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.09475](http://arxiv.org/abs/2210.09475)

    AMPNet是一种用于图神经网络的基于注意力的消息传递层，能够对节点进行逐个特征编码，并通过跨节点注意力模型特征级别的交互。在实际生物系统上进行广泛基准测试表明，AMPNet在fMRI信号重建方面优于现有基准，并通过案例研究验证了其发现有意义的特征级别交互的能力。

    

    图神经网络（GNNs）已成为一种强大的用于图结构数据的表示学习框架。传统GNNs的一个关键限制是将每个节点表示为一个单一的特征向量，可能忽视了关于个体节点特征的复杂细节。我们提出了一种基于注意力的消息传递层AMPNet，用于GNNs，它对节点进行逐个特征编码，并在消息传递步骤中通过跨节点注意力模型特征级别的交互。我们通过在真实生物系统（如fMRI脑活动记录和空间基因组数据）上进行广泛的基准测试，证明了AMPNet的能力，它在fMRI信号重建方面相比现有基准提高了20％，在添加位置嵌入后又进一步提高了8％。最后，我们通过对生物系统的案例研究验证了AMPNet发现有意义的特征级别交互的能力。我们预计我们的架构将被广泛应用于图神经网络的研究中。

    Graph Neural Networks (GNNs) have emerged as a powerful representation learning framework for graph-structured data. A key limitation of conventional GNNs is their representation of each node with a singular feature vector, potentially overlooking intricate details about individual node features. Here, we propose an Attention-based Message-Passing layer for GNNs (AMPNet) that encodes individual features per node and models feature-level interactions through cross-node attention during message-passing steps. We demonstrate the abilities of AMPNet through extensive benchmarking on real-world biological systems such as fMRI brain activity recordings and spatial genomic data, improving over existing baselines by 20% on fMRI signal reconstruction, and further improving another 8% with positional embedding added. Finally, we validate the ability of AMPNet to uncover meaningful feature-level interactions through case studies on biological systems. We anticipate that our architecture will be h
    
[^178]: Blinder: 通过个性化联合学习实现感知系统的端到端隐私保护

    Blinder: End-to-end Privacy Protection in Sensing Systems via Personalized Federated Learning. (arXiv:2209.12046v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.12046](http://arxiv.org/abs/2209.12046)

    本论文提出了一种基于个性化联合学习的Blinder匿名化模型，能在异构环境中提供端到端的隐私保护。与在集中数据上训练的最先进模型相比，Blinder在保护隐私的同时，仅增加最多4.00%的隐私损失，并降低最多4.24%的数据效用。同时，Blinder还展示了匿名化射频感知模态的能力。

    

    本论文提出了一种基于分散数据训练的传感器数据匿名化模型，能够在传感器数据具有不同基础分布的异构环境中，在数据效用和隐私之间取得可取的平衡。我们的匿名化模型名为Blinder，基于变分自编码器和一个或多个对抗训练的鉴别器网络。我们使用模型无关元学习框架，将通过联合学习训练的匿名化模型适应到每个用户的数据分布上。我们在不同设置下评估了Blinder，并展示了与在集中数据上训练的最先进匿名化模型相比，Blinder在两个IMU数据集上提供了端到端的隐私保护，造成的隐私损失增加最多4.00%，数据效用降低最多4.24%。我们还展示了Blinder匿名化射频感知模态的能力。实验证实了Blinder能够模糊多个传感器模态同时进行匿名化。

    This paper proposes a sensor data anonymization model that is trained on decentralized data and strikes a desirable trade-off between data utility and privacy, even in heterogeneous settings where the sensor data have different underlying distributions. Our anonymization model, dubbed Blinder, is based on a variational autoencoder and one or multiple discriminator networks trained in an adversarial fashion. We use the model-agnostic meta-learning framework to adapt the anonymization model trained via federated learning to each user's data distribution. We evaluate Blinder under different settings and show that it provides end-to-end privacy protection on two IMU datasets at the cost of increasing privacy loss by up to 4.00% and decreasing data utility by up to 4.24%, compared to the state-of-the-art anonymization model trained on centralized data. We also showcase Blinder's ability to anonymize the radio frequency sensing modality. Our experiments confirm that Blinder can obscure multi
    
[^179]: 用于机器学习任务的法语非结构化临床笔记去标识化

    De-Identification of French Unstructured Clinical Notes for Machine Learning Tasks. (arXiv:2209.09631v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2209.09631](http://arxiv.org/abs/2209.09631)

    本文提出了一种用于机器学习任务的法语非结构化临床笔记去标识化方法，并总结了过去几十年中在英语文档上进行去标识化的方法。该方法可以帮助保护患者的隐私，并促进医疗数据的共享。

    

    非结构化的文本数据是医疗系统的核心: 医生之间的医疗函件、手术报告、根据ICD-10标准对过程进行编码等。这些文件中包含的细节可以更好地了解患者，更好地管理他们，更好地研究病理，精确地报酬相关的医疗行为... 通过人工智能技术，今天似乎至少在一定程度上可以实现这一切。然而，出于隐私保护的明显原因，这些AI的设计者在这些文件中包含识别数据的情况下没有合法权利访问这些文件。去标识化这些文件，即检测并删除其中存在的所有识别信息，对于在两个互补的世界之间共享此数据是一项法律上必要的步骤。过去几十年中，已经提出了多种对文档进行去标识化的方法，主要是英语。尽管检测分数通常很高, 但它们在法语临床笔记上的表现很少有报道.

    Unstructured textual data are at the heart of health systems: liaison letters between doctors, operating reports, coding of procedures according to the ICD-10 standard, etc. The details included in these documents make it possible to get to know the patient better, to better manage him or her, to better study the pathologies, to accurately remunerate the associated medical acts\ldots All this seems to be (at least partially) within reach of today by artificial intelligence techniques. However, for obvious reasons of privacy protection, the designers of these AIs do not have the legal right to access these documents as long as they contain identifying data. De-identifying these documents, i.e. detecting and deleting all identifying information present in them, is a legally necessary step for sharing this data between two complementary worlds. Over the last decade, several proposals have been made to de-identify documents, mainly in English. While the detection scores are often high, the
    
[^180]: 能应对任意分布变化的在线预测的一致推断

    Conformal Inference for Online Prediction with Arbitrary Distribution Shifts. (arXiv:2208.08401v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2208.08401](http://arxiv.org/abs/2208.08401)

    这项研究解决了在线预测中分布变化的问题，并开发了一种自适应的算法来应对分布的大小和类型变化，具有小遗憾。这种算法与任何基准预测算法结合使用

    

    我们考虑在一个在线环境中形成预测集的问题，其中生成数据的分布允许随时间变化。先前的方法在这个问题上存在历史数据的过重权重，因此可能无法快速应对底层动态。在这里，我们纠正了这个问题，并开发了一种新的过程，对于给定宽度的所有局部时间间隔都有可证明的小遗憾。我们通过修改Gibbs和Cand\`{e}s（2021）的自适应一致推断（ACI）算法，在ACI的梯度下降更新中添加了一个额外的步骤，以在时间上调整步长参数。关键是，这意味着与需要知道数据生成机制变化速率的ACI不同，我们的新过程对分布变化的大小和类型都是自适应的。我们的方法非常灵活，可以与任何产生点估计或估计分位数的基准预测算法结合使用

    We consider the problem of forming prediction sets in an online setting where the distribution generating the data is allowed to vary over time. Previous approaches to this problem suffer from over-weighting historical data and thus may fail to quickly react to the underlying dynamics. Here we correct this issue and develop a novel procedure with provably small regret over all local time intervals of a given width. We achieve this by modifying the adaptive conformal inference (ACI) algorithm of Gibbs and Cand\`{e}s (2021) to contain an additional step in which the step-size parameter of ACI's gradient descent update is tuned over time. Crucially, this means that unlike ACI, which requires knowledge of the rate of change of the data-generating mechanism, our new procedure is adaptive to both the size and type of the distribution shift. Our methods are highly flexible and can be used in combination with any baseline predictive algorithm that produces point estimates or estimated quantile
    
[^181]: SGD和权重衰减在神经网络中被证明会引入低秩偏差

    SGD and Weight Decay Provably Induce a Low-Rank Bias in Neural Networks. (arXiv:2206.05794v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.05794](http://arxiv.org/abs/2206.05794)

    使用SGD和权重衰减训练深度ReLU神经网络会导致对于权重矩阵的秩最小化的偏差，特别是在使用较小批量大小、更高学习率或增加权重衰减时更为显著。此外，在中间神经网络崩溃时，学习的权重特别低秩。这种偏差与泛化之间存在关系。

    

    我们研究了使用随机梯度下降（SGD）在训练深度ReLU神经网络时学习低秩权重矩阵的偏差。我们的结果表明，使用小批量SGD和权重衰减来训练神经网络会导致对于权重矩阵的秩最小化的偏差。具体而言，我们通过理论和实验证明，当使用较小的批量大小、更高的学习率或增加的权重衰减时，这种偏差更加显著。此外，我们预测并通过实验证明，权重衰减是实现这种偏差的必要条件。此外，我们还发现在中间神经网络崩溃的情况下，学习的权重特别低秩。与先前的文献不同，我们的分析不依赖于关于数据、收敛性或权重矩阵优化的假设。此外，它适用于任意宽度或深度的各种神经网络结构。最后，我们通过实验证明了这种偏差与泛化之间的关系。

    We study the bias of Stochastic Gradient Descent (SGD) to learn low-rank weight matrices when training deep ReLU neural networks. Our results show that training neural networks with mini-batch SGD and weight decay causes a bias towards rank minimization over the weight matrices. Specifically, we show, both theoretically and empirically, that this bias is more pronounced when using smaller batch sizes, higher learning rates, or increased weight decay. Additionally, we predict and observe empirically that weight decay is necessary to achieve this bias. In addition, we show that in the presence of intermediate neural collapse, the learned weights are particularly low-rank. Unlike previous literature, our analysis does not rely on assumptions about the data, convergence, or optimality of the weight matrices. Furthermore, it applies to a wide range of neural network architectures of any width or depth. Finally, we empirically investigate the connection between this bias and generalization, 
    
[^182]: 强化学习中的终止问题

    Reinforcement Learning with a Terminator. (arXiv:2205.15376v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.15376](http://arxiv.org/abs/2205.15376)

    这是一个关于强化学习中外部终止问题的论文，通过定义终止马尔可夫决策过程（TerMDP）并学习其参数，提出了一种能够考虑终止情况并限制遗憾值的算法，并在驾驶和基准测试中验证了其有效性。

    

    我们提出了强化学习中的外部终止问题。我们定义了终止马尔可夫决策过程（TerMDP），它是马尔可夫决策过程（MDP）框架的扩展，在这个框架中，episode可能会被外部的非马尔可夫观察者中断。这个定义考虑了许多现实世界中的情况，比如人类出于不适因素中断自主驾驶的代理程序。我们学习了TerMDP的参数，并利用估计问题的结构提供了状态置信界限。我们使用这些界限构建了一个能够证明有效性的算法，该算法考虑了终止情况，并限制了遗憾值。在理论分析的基础上，我们设计并实施了一种可扩展的方法，将乐观性（相对于终止）与动态折扣因子相结合，同时考虑到终止概率。我们将我们的方法应用于高维驾驶和MinAtar基准测试中。此外，我们还在驾驶环境中对人类数据进行了测试。我们的结果表明...

    We present the problem of reinforcement learning with exogenous termination. We define the Termination Markov Decision Process (TerMDP), an extension of the MDP framework, in which episodes may be interrupted by an external non-Markovian observer. This formulation accounts for numerous real-world situations, such as a human interrupting an autonomous driving agent for reasons of discomfort. We learn the parameters of the TerMDP and leverage the structure of the estimation problem to provide state-wise confidence bounds. We use these to construct a provably-efficient algorithm, which accounts for termination, and bound its regret. Motivated by our theoretical analysis, we design and implement a scalable approach, which combines optimism (w.r.t. termination) and a dynamic discount factor, incorporating the termination probability. We deploy our method on high-dimensional driving and MinAtar benchmarks. Additionally, we test our approach on human data in a driving setting. Our results dem
    
[^183]: 衍生品定价模型的校准：多智能体强化学习观点

    Calibration of Derivative Pricing Models: a Multi-Agent Reinforcement Learning Perspective. (arXiv:2203.06865v3 [q-fin.CP] UPDATED)

    [http://arxiv.org/abs/2203.06865](http://arxiv.org/abs/2203.06865)

    本文利用多智能体强化学习提出校准衍生品定价模型问题的博弈论解决方案，并希望该方法可用于解决其他金融领域的问题。实验证明，该算法能够学习局部波动率以及最小化百慕大期权价格所需的路径依赖性。

    

    在量化金融中最基本的问题之一是存在适合给定一组期权市场价格的连续时间扩散模型。传统上，人们使用直觉、理论和经验分析的混合方法来寻找实现精确或近似匹配的模型。我们的贡献在于展示如何通过适当的博弈理论形式化问题，借助现代深度多智能体强化学习的现有进展来搜索随机过程空间，以解决这个问题。更重要的是，我们希望我们的技术可以被社区利用和扩展，以解决该领域的重要问题，如联合SPX-VIX校准问题。我们的实验表明，我们能够学习局部波动率以及在波动率过程中所需的路径依赖性，以最小化百慕大期权的价格。我们的算法可以看作是一种粒子方法，类似于Guyon et Henry-Labordere的方法。

    One of the most fundamental questions in quantitative finance is the existence of continuous-time diffusion models that fit market prices of a given set of options. Traditionally, one employs a mix of intuition, theoretical and empirical analysis to find models that achieve exact or approximate fits. Our contribution is to show how a suitable game theoretical formulation of this problem can help solve this question by leveraging existing developments in modern deep multi-agent reinforcement learning to search in the space of stochastic processes. More importantly, we hope that our techniques can be leveraged and extended by the community to solve important problems in that field, such as the joint SPX-VIX calibration problem. Our experiments show that we are able to learn local volatility, as well as path-dependence required in the volatility process to minimize the price of a Bermudan option. Our algorithm can be seen as a particle method \`{a} la Guyon et Henry-Labordere where partic
    
[^184]: 深度高效的连续流形学习用于时间序列建模

    Deep Efficient Continuous Manifold Learning for Time Series Modeling. (arXiv:2112.03379v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.03379](http://arxiv.org/abs/2112.03379)

    本文提出了一个深度高效的连续流形学习框架，通过在Riemannian流形和Cholesky空间之间利用微分同胚映射，实现了对非欧几里得数据的优化问题高效求解和计算成本的大幅降低。

    

    随着深度神经网络在各个领域取得了空前的成功，对非欧几里得数据进行建模越来越受到关注。特别是对称正定矩阵在计算机视觉、信号处理和医学图像分析等领域得到了广泛研究，因为它具有学习有益的统计表示的能力。然而，由于其严格的约束条件，将其与深度学习框架结合在一起时仍然具有挑战性的优化问题和低效的计算成本。在本文中，我们提出了一个框架，通过在Riemannian流形和Cholesky空间之间利用一个微分同胚映射，不仅可以高效地解决优化问题，而且可以大大减少计算成本。此外，为了对时间序列数据进行动态建模，我们设计了一种连续流形学习方法，通过系统集成流形常微分方程和门控循环神经网络实现。

    Modeling non-Euclidean data is drawing extensive attention along with the unprecedented successes of deep neural networks in diverse fields. Particularly, a symmetric positive definite matrix is being actively studied in computer vision, signal processing, and medical image analysis, due to its ability to learn beneficial statistical representations. However, owing to its rigid constraints, it remains challenging to optimization problems and inefficient computational costs, especially, when incorporating it with a deep learning framework. In this paper, we propose a framework to exploit a diffeomorphism mapping between Riemannian manifolds and a Cholesky space, by which it becomes feasible not only to efficiently solve optimization problems but also to greatly reduce computation costs. Further, for dynamic modeling of time-series data, we devise a continuous manifold learning method by systematically integrating a manifold ordinary differential equation and a gated recurrent neural net
    
[^185]: 通过部分雅可比矩阵实现宽而深的神经网络的关键初始化：一般理论和应用

    Critical Initialization of Wide and Deep Neural Networks through Partial Jacobians: General Theory and Applications. (arXiv:2111.12143v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.12143](http://arxiv.org/abs/2111.12143)

    本论文通过部分雅可比矩阵的分析，提出了一种诊断深度神经网络临界性的实用方法，并通过递归关系分析了带有LayerNorm和/或残差连接的深度全连接神经网络的临界性。

    

    深度神经网络因其难以进行理论研究而闻名。然而，当每个层中的参数数量趋于无穷时，网络函数成为一个高斯过程（GP），并且可以进行定量预测描述。高斯近似使得我们能够制定选择超参数（例如权重和偏差的方差以及学习率）的标准。这些标准依赖于为深度神经网络定义的临界性概念。在这项工作中，我们描述了一种诊断临界性的新实用方式。我们引入了网络的“部分雅可比矩阵”，定义为层$l$中的预激活对于层$l_0\leq l$中的预激活的导数。我们推导了部分雅可比矩阵范数的递归关系，并利用这些关系分析了带有LayerNorm和/或残差连接的深度全连接神经网络的临界性。我们推导并实现了一种简单且廉价的数值测试，使得可以选择适当的权重和偏差方差以及学习率。

    Deep neural networks are notorious for defying theoretical treatment. However, when the number of parameters in each layer tends to infinity, the network function is a Gaussian process (GP) and quantitatively predictive description is possible. Gaussian approximation allows one to formulate criteria for selecting hyperparameters, such as variances of weights and biases, as well as the learning rate. These criteria rely on the notion of criticality defined for deep neural networks. In this work we describe a new practical way to diagnose criticality. We introduce \emph{partial Jacobians} of a network, defined as derivatives of preactivations in layer $l$ with respect to preactivations in layer $l_0\leq l$. We derive recurrence relations for the norms of partial Jacobians and utilize these relations to analyze criticality of deep fully connected neural networks with LayerNorm and/or residual connections. We derive and implement a simple and cheap numerical test that allows one to select 
    
[^186]: 分布式多臂赌博机可以超越集中式上置信界限算法

    Decentralized Multi-Armed Bandits Can Outperform Centralized Upper Confidence Bound Algorithms. (arXiv:2111.10933v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.10933](http://arxiv.org/abs/2111.10933)

    本文研究了一个多智能体网络中的分布式多臂赌博机问题，提出了两种完全分布式的算法，基于经典的UCB算法和KL-UCB算法，实验证明这些算法能达到更好的对数渐近后悔，智能体之间的邻居关系越多，后悔值越好。

    

    本文研究了一个多智能体网络中的分布式多臂赌博机问题。假设N个智能体同时解决了这个问题，他们面对着一组共同的M个臂并共享相同的臂奖励分布。每个智能体只能从邻居处接收信息，智能体之间的邻居关系由一个无向图描述。本文提出了两种完全分布式的多臂赌博机算法，分别基于经典的上置信界限（UCB）算法和最先进的KL-UCB算法。所提出的分布式算法使网络中的每个智能体能够实现比其单一智能体相应算法更好的对数渐近后悔，前提是智能体至少有一个邻居，而且智能体有越多的邻居，后悔值会越好，这意味着整体的和大于其组成部分。

    This paper studies a decentralized multi-armed bandit problem in a multi-agent network. The problem is simultaneously solved by N agents assuming they face a common set of M arms and share the same arms' reward distributions. Each agent can receive information only from its neighbors, where the neighbor relationships among the agents are described by an undirected graph. Two fully decentralized multi-armed bandit algorithms are proposed, respectively based on the classic upper confidence bound (UCB) algorithm and the state-of-the-art KL-UCB algorithm. The proposed decentralized algorithms permit each agent in the network to achieve a better logarithmic asymptotic regret than their single-agent counterparts, provided that the agent has at least one neighbor, and the more neighbors an agent has, the better regret it will have, meaning that the sum is more than its component parts.
    
[^187]: 使用转换风险最小化学习增强分布

    Learning Augmentation Distributions using Transformed Risk Minimization. (arXiv:2111.08190v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.08190](http://arxiv.org/abs/2111.08190)

    我们提出了一种新的转换风险最小化框架，可以同时学习预测模型和数据变换，特别是分布的变换。我们以学习图像增强为主要应用，并提出了解决过拟合问题的正则化方法。

    

    我们提出了一种新的“转换风险最小化”（TRM）框架，作为传统风险最小化的扩展。在TRM中，我们不仅优化预测模型，还优化数据变换，特别是分布的变换。作为一个关键应用，我们关注学习增强，例如适当旋转图像，以提高给定预测器类别的分类性能。我们的TRM方法：（1）在单个训练循环中联合学习变换和模型；（2）适用于任何适用于标准风险最小化的训练算法；（3）处理任何变换，例如离散和连续类的增强。为了避免在实施经验转换风险最小化时过拟合，我们提出了一种基于PAC-Bayes理论的新型正则化器。对于学习图像的增强，我们提出了一种通过几何变换块的随机组合对增强空间进行参数化的新方法。

    We propose a new \emph{Transformed Risk Minimization} (TRM) framework as an extension of classical risk minimization. In TRM, we optimize not only over predictive models, but also over data transformations; specifically over distributions thereof. As a key application, we focus on learning augmentations; for instance appropriate rotations of images, to improve classification performance with a given class of predictors. Our TRM method (1) jointly learns transformations and models in a \emph{single training loop}, (2) works with any training algorithm applicable to standard risk minimization, and (3) handles any transforms, such as discrete and continuous classes of augmentations. To avoid overfitting when implementing empirical transformed risk minimization, we propose a novel regularizer based on PAC-Bayes theory. For learning augmentations of images, we propose a new parametrization of the space of augmentations via a stochastic composition of blocks of geometric transforms. This lea
    
[^188]: 卷积模式核网络

    Convolutional Motif Kernel Networks. (arXiv:2111.02272v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.02272](http://arxiv.org/abs/2111.02272)

    本文介绍了一种名为卷积模式核网络的神经网络架构，通过学习位置感知模式核函数在希尔伯特空间子空间内的特征表示，实现了直接解释和评估预测结果。

    

    人工神经网络在检测与特定结果相关的数据内的相关性方面表现出有希望的性能。然而，这种模型的黑盒特性可能会阻碍研究领域中的知识进步，因为它会掩盖决策过程并阻止科学家完全理解预测结果。此外，像医疗保健提供者这样的领域专家需要可解释的预测结果，以评估在高风险情景中是否可以信任预测结果，并帮助他们将模型整合到自己的日常工作中。因此，可解释的模型在将机器学习应用于医疗保健等高风险情境中发挥着至关重要的作用。本文介绍了卷积模式核网络，一种涉及在核函数的位置感知模式核希尔伯特空间的子空间中学习特征表示的神经网络体系结构。最终的模型使得可以直接解释和评估预测结果。

    Artificial neural networks show promising performance in detecting correlations within data that are associated with specific outcomes. However, the black-box nature of such models can hinder the knowledge advancement in research fields by obscuring the decision process and preventing scientist to fully conceptualize predicted outcomes. Furthermore, domain experts like healthcare providers need explainable predictions to assess whether a predicted outcome can be trusted in high stakes scenarios and to help them integrating a model into their own routine. Therefore, interpretable models play a crucial role for the incorporation of machine learning into high stakes scenarios like healthcare. In this paper we introduce Convolutional Motif Kernel Networks, a neural network architecture that involves learning a feature representation within a subspace of the reproducing kernel Hilbert space of the position-aware motif kernel function. The resulting model enables to directly interpret and ev
    
[^189]: 非线性系统识别的混合机器学习模型评估：疲劳试验架的案例研究。

    Assessment of hybrid machine learning models for non-linear system identification of fatigue test rigs. (arXiv:2107.03645v3 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2107.03645](http://arxiv.org/abs/2107.03645)

    本文开发了一种混合模型，利用长短期记忆网络结合线性频率响应函数模型进行非线性系统识别。该方法还可以应用于虚拟传感。通过对疲劳试验数据进行测试验证了该方法的效果。

    

    预测给定疲劳试验台驱动信号的系统响应是一项具有挑战性的任务，通常采用线性频率响应函数模型。为了考虑非线性现象，建议采用一种新颖的混合模型，通过增加长短期记忆（LSTM）网络对现有方法进行补充。该方法的附加虚拟传感应用得到了验证。使用来自伺服液压试验台的非线性实验数据进行测试，并公开了该数据集。在评估中采用了各种时间和频率域指标以及变幅下的疲劳强度指标。

    The prediction of system responses for a given fatigue test bench drive signal is a challenging task, for which linear frequency response function models are commonly used. To account for non-linear phenomena, a novel hybrid model is suggested, which augments existing approaches using Long Short-Term Memory networks. Additional virtual sensing applications of this method are demonstrated. The approach is tested using non-linear experimental data from a servo-hydraulic test rig and this dataset is made publicly available. A variety of metrics in time and frequency domains, as well as fatigue strength under variable amplitudes, are employed in the evaluation.
    
[^190]: 强化校准下界通过绕过

    Stronger Calibration Lower Bounds via Sidestepping. (arXiv:2012.03454v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2012.03454](http://arxiv.org/abs/2012.03454)

    本文研究了在线二进制预测设置中的校准问题，证明了校准误差的Ω(T^(0.528))下界，这是第一个超过√T的下界。

    

    本文考虑了一种在线二进制预测设置，其中预测员逐个观察一系列T个比特位。在揭示每个比特位之前，预测员预测该比特位为1的概率。如果对于每个p∈[0,1]，在预测员预测概率为p的n_p个比特位中，实际出现的1的数量m_p确实等于p⋅n_p，则称预测员具有良好校准性。校准误差定义为∑_p|mp⋅p⋅np|，用来衡量预测员偏离良好校准性的程度。尽管已经知道即使在比特位是对抗性选择的情况下，基于之前的预测也可能实现O(T^(2/3))的校准误差，但是对于下界方面几乎没有任何了解，除了通过独立公平硬币翻转的平凡例子得到的Ω(√T)下界。在本文中，我们证明了校准误差的Ω(T^(0.528))下界，这是第一个超过√T的下界。

    We consider an online binary prediction setting where a forecaster observes a sequence of $T$ bits one by one. Before each bit is revealed, the forecaster predicts the probability that the bit is $1$. The forecaster is called well-calibrated if for each $p \in [0, 1]$, among the $n_p$ bits for which the forecaster predicts probability $p$, the actual number of ones, $m_p$, is indeed equal to $p \cdot n_p$. The calibration error, defined as $\sum_p |m_p p n_p|$, quantifies the extent to which the forecaster deviates from being well-calibrated. It has long been known that an $O(T^{2/3})$ calibration error is achievable even when the bits are chosen adversarially, and possibly based on the previous predictions. However, little is known on the lower bound side, except an $\Omega(\sqrt{T})$ bound that follows from the trivial example of independent fair coin flips.  In this paper, we prove an $\Omega(T^{0.528})$ bound on the calibration error, which is the first super-$\sqrt{T}$ lower bou
    
[^191]: 用复合传输散度进行高斯混合简化

    Gaussian Mixture Reduction with Composite Transportation Divergence. (arXiv:2002.08410v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2002.08410](http://arxiv.org/abs/2002.08410)

    本文提出了一种基于复合传输散度的高斯混合简化方法，用于解决高斯混合在递归更新中阶数指数增加的推断问题。

    

    高斯混合在密度估计、信念传播和贝叶斯滤波等各种应用中被广泛用于逼近密度函数。这些应用通常利用高斯混合作为递归更新的初始近似。这些递归过程中的一个关键挑战源于混合阶数的指数增加，导致难以求解的推断问题。为了克服这个困难，可以使用高斯混合简化（GMR）将高阶高斯混合近似为低阶混合。尽管现有的基于聚类的方法在性能和计算效率上表现良好，但它们的收敛性质和最优目标仍然未知。在本文中，我们提出了一种基于复合传输散度的新型优化GMR方法。我们开发了一个主元最小化算法来计算简化的混合，并在g中建立了其理论收敛性。

    Gaussian mixtures are widely used for approximating density functions in various applications such as density estimation, belief propagation, and Bayesian filtering. These applications often utilize Gaussian mixtures as initial approximations that are updated recursively. A key challenge in these recursive processes stems from the exponential increase in the mixture's order, resulting in intractable inference. To overcome the difficulty, the Gaussian mixture reduction (GMR), which approximates a high order Gaussian mixture by one with a lower order, can be used. Although existing clustering-based methods are known for their satisfactory performance and computational efficiency, their convergence properties and optimal targets remain unknown. In this paper, we propose a novel optimization-based GMR method based on composite transportation divergence (CTD). We develop a majorization-minimization algorithm for computing the reduced mixture and establish its theoretical convergence under g
    
[^192]: 可变形生成器网络：无监督解耦外观和几何信息

    Deformable Generator Networks: Unsupervised Disentanglement of Appearance and Geometry. (arXiv:1806.06298v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1806.06298](http://arxiv.org/abs/1806.06298)

    可变形生成器网络能够以无监督的方式解耦图像和视频中的外观和几何信息，通过生成变形场实现几何变形，提供了一种通用且有效的生成模型。

    

    我们提出了一个可变形生成器模型，以纯粹无监督的方式解耦图像和视频数据的外观和几何信息。外观生成器网络模拟与外观相关的信息，包括颜色、照明、身份或类别，而几何生成器通过生成变形场来执行几何变形，如旋转和拉伸，通过扭曲生成的外观来获取最终的图像或视频序列。两个生成器接收独立的潜在向量作为输入，从图像或视频序列中解耦外观和几何信息。对于视频数据，引入非线性转换模型到外观和几何生成器中，以捕捉随时间变化的动态。所提出的方案是通用的，可以轻松集成到不同的生成模型中。大量的定性和定量实验表明外观和几何信息可以成功解耦，并且能够有效地生成多样化的图像和视频序列。

    We present a deformable generator model to disentangle the appearance and geometric information for both image and video data in a purely unsupervised manner. The appearance generator network models the information related to appearance, including color, illumination, identity or category, while the geometric generator performs geometric warping, such as rotation and stretching, through generating deformation field which is used to warp the generated appearance to obtain the final image or video sequences. Two generators take independent latent vectors as input to disentangle the appearance and geometric information from image or video sequences. For video data, a nonlinear transition model is introduced to both the appearance and geometric generators to capture the dynamics over time. The proposed scheme is general and can be easily integrated into different generative models. An extensive set of qualitative and quantitative experiments shows that the appearance and geometric informat
    

