# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Can MLLMs Perform Text-to-Image In-Context Learning?](https://rss.arxiv.org/abs/2402.01293) | 本论文探索了将上下文学习扩展到多模态的文本到图像转换任务，并提出了第一个T2I-ICL基准数据集CoBSAT。研究发现MLLMs在解决T2I-ICL问题时面临着多模态和图像生成的固有复杂性，并通过微调和思维链提示等策略取得了显著改进。 |
| [^2] | [Topic-based Watermarks for LLM-Generated Text](https://arxiv.org/abs/2404.02138) | 提出了一种新的基于主题的水印算法，旨在解决当前水印方案的局限性，为区分LLM生成的文本和人类生成的文本提供了新的思路。 |
| [^3] | [DE-HNN: An effective neural model for Circuit Netlist representation](https://arxiv.org/abs/2404.00477) | 设计师们开发了一种名为DE-HNN的神经模型，用于电路网表表示，以解决优化工具运行时间长的问题。 |
| [^4] | [Attention-based Shape-Deformation Networks for Artifact-Free Geometry Reconstruction of Lumbar Spine from MR Images](https://arxiv.org/abs/2404.00231) | 这项工作提出了TransDeformer，使用注意力机制实现了对腰椎轮廓的高空间准确性重建，并跨患者实现了网格对应，为医学参数测量提供了可靠性，还设计了变体用于错误估计。 |
| [^5] | [Understanding the Learning Dynamics of Alignment with Human Feedback](https://arxiv.org/abs/2403.18742) | 本研究对人类偏好对齐的学习动态进行了理论分析，显示了偏好数据集的分布如何影响模型更新速度，并提供了对训练准确度的严格保证，同时揭示了优化易于优先考虑高偏好可区分性行为的复杂现象。 |
| [^6] | [Rotate to Scan: UNet-like Mamba with Triplet SSM Module for Medical Image Segmentation](https://arxiv.org/abs/2403.17701) | 本文提出了Triplet Mamba-UNet，利用残余VSS块提取密集上下文特征，并利用Triplet SSM融合空间和通道维度上的特征。 |
| [^7] | [DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning](https://arxiv.org/abs/2403.14421) | 开发了第一个差分隐私检索增强生成算法，能够在生成高质量图像样本的同时提供可证明的隐私保证 |
| [^8] | [AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models](https://arxiv.org/abs/2403.13269) | AFLoRA是一种自适应冻结低秩调整方法，通过逐步冻结投影矩阵来提高性能，减少计算量，并提供对GLUE基准测试的最先进表现。 |
| [^9] | [Neuron-centric Hebbian Learning](https://arxiv.org/abs/2403.12076) | 提出了一种新的神经元中心的赫布学习模型，相较于传统的ABCD规则，将参数减少了从$5W$到$5N$。 |
| [^10] | [Generalization of Graph Neural Networks through the Lens of Homomorphism](https://arxiv.org/abs/2403.06079) | 本文通过分析图同态的熵提出了一种新颖视角，推导出了适用于图和节点分类的泛化界限，能够捕捉各种图结构的细微差异，并通过统一框架刻画了广泛的GNN模型。 |
| [^11] | [Multi-Constraint Safe RL with Objective Suppression for Safety-Critical Applications](https://arxiv.org/abs/2402.15650) | 提出了一种目标抑制的新方法，可以在多约束安全领域中改进安全强化学习任务表现，实验证明此方法结合现有算法能够在减少约束违规的情况下实现与基准线相当的任务奖励水平。 |
| [^12] | [APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models](https://arxiv.org/abs/2402.14866) | APTQ提出了针对大型语言模型的注意力感知后训练混合精度量化方法，在保持模型性能的同时，超越了先前的量化方法，并在零-shot任务上达到了最先进的准确率 |
| [^13] | [Advancing Low-Rank and Local Low-Rank Matrix Approximation in Medical Imaging: A Systematic Literature Review and Future Directions](https://arxiv.org/abs/2402.14045) | 本文系统综述了在医学成像中应用低秩矩阵逼近（LRMA）和其派生物局部LRMA（LLRMA）的作品，并指出自2015年以来医学成像领域开始偏向于使用LLRMA，显示其在捕获医学数据中复杂结构方面的潜力和有效性。 |
| [^14] | [Federated Multi-Task Learning on Non-IID Data Silos: An Experimental Study](https://arxiv.org/abs/2402.12876) | 这项研究介绍了一种新颖的FMTL-Bench框架，用于系统评估联邦多任务学习（FMTL）范式，填补了FL和MTL综合评估方法的空白。 |
| [^15] | [Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding](https://arxiv.org/abs/2402.11809) | 提出了Smart Parallel Auto-Correct Decoding (SPACE)方法，通过集成半自回归推理和猜测解码，实现了大型语言模型推理加速和并行生成验证令牌的功能。 |
| [^16] | [Gaussian Ensemble Belief Propagation for Efficient Inference in High-Dimensional Systems](https://arxiv.org/abs/2402.08193) | 高斯模型集成置信传播算法（GEnBP）是一种用于高维系统中高效推断的方法，通过集成卡尔曼滤波器和高斯置信传播等技术相结合，能有效处理高维状态、参数和复杂的依赖结构。 |
| [^17] | [Large Language User Interfaces: Voice Interactive User Interfaces powered by LLMs](https://arxiv.org/abs/2402.07938) | 本研究旨在利用和引导升级后的LLMs的强大能力，构建一个框架，作为用户和用户界面之间的中介，通过对自然文本输入进行彻底分析，实现智能和响应式用户体验。 |
| [^18] | [Towards Understanding Variants of Invariant Risk Minimization through the Lens of Calibration](https://arxiv.org/abs/2401.17541) | 本研究通过比较分析使用不同近似方法的不变风险最小化（IRM）技术，以期望校准误差（ECE）作为关键指标，观察到基于信息瓶颈的IRM在压缩关键特征上表现最佳。 |
| [^19] | [Pixel to Elevation: Learning to Predict Elevation Maps at Long Range using Images for Autonomous Offroad Navigation](https://arxiv.org/abs/2401.17484) | 本研究提出了一种学习方法，可以通过车载视角图像实时预测长距离地形高程地图。该方法包括transformer-based编码器、方向感知的位置编码和历史增强的可学习地图嵌入。通过学习视角图像与鸟瞰图高程地图之间的关联，结合车辆姿态信息和视觉图像特征，实现更好的地图预测时序一致性。 |
| [^20] | [Few-Shot Causal Representation Learning for Out-of-Distribution Generalization on Heterogeneous Graphs](https://arxiv.org/abs/2401.03597) | 该论文提出了一种少样本因果表示学习方法，用于在异构图上实现跨领域泛化，解决了源HG与目标HG分布不匹配导致的知识传递无效和学习性能不佳的问题。 |
| [^21] | [Learning to Manipulate under Limited Information.](http://arxiv.org/abs/2401.16412) | 本研究通过训练神经网络在有限信息条件下学习如何利用不同投票方法进行操纵，发现某些投票方法在有限信息下容易被操纵，而其他方法不容易被操纵。 |
| [^22] | [RiemannONets: Interpretable Neural Operators for Riemann Problems.](http://arxiv.org/abs/2401.08886) | 本文使用可解释的神经算子来解决高速流动中的Riemann问题，通过对DeepONet进行简单修改，在极端压力跳跃情况下获得了非常准确的解决方案。 |
| [^23] | [Large Language Models as Generalizable Policies for Embodied Tasks.](http://arxiv.org/abs/2310.17722) | 本研究展示了大型语言模型(LLMs)可以被适应为具有普适性的机器人任务策略。通过我们的方法LLaRP，我们成功将预训练的冻结LLM用于接收指令和视觉输入，并在环境中直接输出动作。我们的实验结果表明LLaRP不仅对任务指令的复杂改写具有鲁棒性，而且可以推广到需要新颖最优行为的新任务。在大量未见任务中，LLaRP表现出了显著的成功率提升，并且我们提供了一个新的基准测试(Language Rearrangement)来促进进一步的研究。 |
| [^24] | [Sharp error bounds for imbalanced classification: how many examples in the minority class?.](http://arxiv.org/abs/2310.14826) | 该论文提出了两个在稀有类概率趋近于零的情况下的新贡献，分别是一种非渐近快速率概率界限和一种一致上界估计方法，这些发现为在实际情况下的类别加权提供了更清晰的理解，为进一步的研究提供了新的方向。 |
| [^25] | [Mori-Zwanzig latent space Koopman closure for nonlinear autoencoder.](http://arxiv.org/abs/2310.10745) | 本研究提出了一种名为Mori-Zwanzig自编码器（MZ-AE）的新方法，用于在低维空间中稳健地逼近Koopman算子，通过非线性自编码器和Mori-Zwanzig形式主义的集成实现对有限不变Koopman子空间的逼近，从而增强了精确性和准确预测复杂系统行为的能力。 |
| [^26] | [Automatic Macro Mining from Interaction Traces at Scale.](http://arxiv.org/abs/2310.07023) | 本文介绍了一种基于大型语言模型的新方法，用于从移动交互轨迹中自动提取语义上有意义的宏任务。通过多项研究和实验证明了该方法的有效性和提取的宏任务的实用性。 |
| [^27] | [A Framework for Interpretability in Machine Learning for Medical Imaging.](http://arxiv.org/abs/2310.01685) | 本文提出了一个机器学习在医学影像中的可解释性框架，明确了解释性的目标和要素，以指导方法设计并改进实际应用。 |
| [^28] | [A novel approach to measuring patent claim scope based on probabilities obtained from (large) language models.](http://arxiv.org/abs/2309.10003) | 本文提出了一种使用概率从语言模型中获得的自信息来测量专利权要求范围的新方法。该方法通过计算要求的发生概率和自信息来评估要求的信息量，进而反映出要求的范围。研究结果表明，不同类型的语言模型对范围测量的影响不同，最简单的模型可以将范围度量简化为单词或字符计数的倒数。此方法在九个系列的专利权要求上进行了验证，结果表明各系列的要求范围逐渐减小。 |
| [^29] | [PCN: A Deep Learning Approach to Jet Tagging Utilizing Novel Graph Construction Methods and Chebyshev Graph Convolutions.](http://arxiv.org/abs/2309.08630) | 本研究提出了一种基于图形的喷注表示方法，并设计了一种名为PCN的图神经网络（GNN），利用切比雪夫图卷积（ChebConv）进行深度学习喷注标记，取得了显著的改进。 |
| [^30] | [Energy Preservation and Stability of Random Filterbanks.](http://arxiv.org/abs/2309.05855) | 本文从随机卷积算子的数学角度详细阐述了简单卷积神经网络的统计属性，发现具有随机高斯权重的FIR滤波器组在大滤波器和局部周期输入信号的情况下是病态的，并推导了其期望帧边界的理论界限。 |
| [^31] | [Emoji Promotes Developer Participation and Issue Resolution on GitHub.](http://arxiv.org/abs/2308.16360) | 本研究探讨了表情符号在虚拟工作空间中的使用对开发者参与和问题解决的影响。研究发现，表情符号可以显著缩短问题的解决时间，并吸引更多用户参与。不同类型问题对表情符号的效果也存在差异。 |
| [^32] | [The Marginal Value of Momentum for Small Learning Rate SGD.](http://arxiv.org/abs/2307.15196) | 本文通过理论分析和实验证明，在小学习率和梯度噪声是主要不稳定源的随机环境中，动量在深度学习优化中的边际价值是有限的。 |
| [^33] | [WebArena: A Realistic Web Environment for Building Autonomous Agents.](http://arxiv.org/abs/2307.13854) | WebArena是一个用于构建自主智能体的真实网络环境，它包含了完全功能的网站，并且通过引入工具和外部知识库来鼓励智能体像人类一样解决任务。此外，WebArena还发布了一组用于评估任务完成功能正确性的基准任务。 |
| [^34] | [Universal Online Learning with Gradual Variations: A Multi-layer Online Ensemble Approach.](http://arxiv.org/abs/2307.08360) | 该论文提出了一种具有两个不同级别自适应性的在线凸优化方法，对不同类型的损失函数具有多种遗憾界，并在分析中直接应用于小损失界。同时，它与对抗性/随机凸优化和博弈论有着深刻的联系。 |
| [^35] | [Can Domain Adaptation Improve Accuracy and Fairness of Skin Lesion Classification?.](http://arxiv.org/abs/2307.03157) | 本研究利用多个皮肤病变数据集，研究了域适应方法在皮肤病变分类中的应用。结果表明，域适应在减少不平衡的情况下对于二分类任务有效，但在多分类任务中性能较差，需要解决不平衡问题以提高准确性。 |
| [^36] | [RetICL: Sequential Retrieval of In-Context Examples with Reinforcement Learning.](http://arxiv.org/abs/2305.14502) | 本文提出了一种 RetICL 的方法来优化地选择用于上下文学习模型的示例。此方法利用强化学习框架将序列示例选择问题作为马尔科夫决策过程，并且优化选择为使任务表现最佳的组合。 |
| [^37] | [Leveraging gradient-derived metrics for data selection and valuation in differentially private training.](http://arxiv.org/abs/2305.02942) | 研究如何在隐私增强技术下，利用梯度信息识别训练中感兴趣的数据样本进行数据选择和估价。 |
| [^38] | [Learning-Based Optimal Control with Performance Guarantees for Unknown Systems with Latent States.](http://arxiv.org/abs/2303.17963) | 本文提出了一种面向未知具有潜在状态系统的学习优化控制方法，并给出了概率性能保证，同时提出了一种验证任意控制律性能的方法。 |
| [^39] | [Controlling High-Dimensional Data With Sparse Input.](http://arxiv.org/abs/2303.09446) | 本论文提出了一种新的控制机制，即将稀疏、易于人理解的控制空间映射到生成模型的潜在空间。通过实验，此方法表现出了效率、鲁棒性和保真性，即使只有少量的输入数值。 |
| [^40] | [Aleatoric and Epistemic Discrimination: Fundamental Limits of Fairness Interventions.](http://arxiv.org/abs/2301.11781) | 该论文研究了机器学习模型中的偶然性和认知性歧视，将其分类为数据分布中固有的歧视和模型开发过程中的决策导致的歧视。通过量化偶然性歧视的性能限制和刻画认知性歧视，揭示了公平干预的基本限制。研究还应用这种方法评估了现有的公平干预措施，并探究了在存在缺失值的数据中的公平风险。 |
| [^41] | [Non-contact Respiratory Anomaly Detection using Infrared Light-wave Sensing.](http://arxiv.org/abs/2301.03713) | 本研究使用无接触红外光波传感技术，通过训练不同类型的呼吸模式来检测呼吸异常，并且通过验证数据的呼吸波形丢弃干扰数据，以实现安全、高效和无创的人体呼吸监测。 |
| [^42] | [Exact Manifold Gaussian Variational Bayes.](http://arxiv.org/abs/2210.14598) | 我们提出了一种在复杂模型中进行变分推断的优化算法，通过使用自然梯度更新和黎曼流形，我们开发了一种高效的高斯变分推断算法，并验证了其在多个数据集上的性能。 |
| [^43] | [Characterizing Graph Datasets for Node Classification: Homophily-Heterophily Dichotomy and Beyond.](http://arxiv.org/abs/2209.06177) | 本论文研究了用于节点分类的图形数据集的特征化，发现目前常用的同质性测量方法存在严重缺陷，无法比较不同数据集中的同质性水平。作者提出了一种新的同质性测量指标，称为调整同质性，该指标满足更多期望特性，并具有较少在图形机器学习文献中使用的特点。 |
| [^44] | [Analyzing Explainer Robustness via Lipschitzness of Prediction Functions.](http://arxiv.org/abs/2206.12481) | 本文研究了通过评估预测函数的Lipschitz率来分析解释器的稳健性。通过引入解释器敏锐性的概念并与预测器的概率Lipschitz率相联系，我们提供了解释器敏锐性的下界保证。 |

# 详细

[^1]: MLLMs能否进行上下文学习的文本到图像转换？

    Can MLLMs Perform Text-to-Image In-Context Learning?

    [https://rss.arxiv.org/abs/2402.01293](https://rss.arxiv.org/abs/2402.01293)

    本论文探索了将上下文学习扩展到多模态的文本到图像转换任务，并提出了第一个T2I-ICL基准数据集CoBSAT。研究发现MLLMs在解决T2I-ICL问题时面临着多模态和图像生成的固有复杂性，并通过微调和思维链提示等策略取得了显著改进。

    

    从大型语言模型（LLMs）发展到多模式大型语言模型（MLLMs）推动了将上下文学习（ICL）扩展到多模式的研究。现有的研究主要集中在图像到文本的ICL上。然而，文本到图像的ICL（T2I-ICL）具有独特的特性和潜在的应用，但仍然少有研究。为了填补这个空白，我们正式定义了T2I-ICL任务，并提出了CoBSAT，第一个包含十个任务的T2I-ICL基准数据集。利用我们的数据集评估了六个最先进的MLLMs，我们发现MLLMs在解决T2I-ICL问题时面临着相当大的困难。我们确定了多模态和图像生成的固有复杂性是主要挑战。为了克服这些挑战，我们探索了微调和思维链提示等策略，并取得了显著的改进。我们的代码和数据集可以在\url{https://github.com/UW-Madison-Lee-Lab/CoBSAT}上获得。

    The evolution from Large Language Models (LLMs) to Multimodal Large Language Models (MLLMs) has spurred research into extending In-Context Learning (ICL) to its multimodal counterpart. Existing such studies have primarily concentrated on image-to-text ICL. However, the Text-to-Image ICL (T2I-ICL), with its unique characteristics and potential applications, remains underexplored. To address this gap, we formally define the task of T2I-ICL and present CoBSAT, the first T2I-ICL benchmark dataset, encompassing ten tasks. Utilizing our dataset to benchmark six state-of-the-art MLLMs, we uncover considerable difficulties MLLMs encounter in solving T2I-ICL. We identify the primary challenges as the inherent complexity of multimodality and image generation. To overcome these challenges, we explore strategies like fine-tuning and Chain-of-Thought prompting, demonstrating notable improvements. Our code and dataset are available at \url{https://github.com/UW-Madison-Lee-Lab/CoBSAT}.
    
[^2]: 基于主题的LLM生成文本的水印

    Topic-based Watermarks for LLM-Generated Text

    [https://arxiv.org/abs/2404.02138](https://arxiv.org/abs/2404.02138)

    提出了一种新的基于主题的水印算法，旨在解决当前水印方案的局限性，为区分LLM生成的文本和人类生成的文本提供了新的思路。

    

    大型语言模型（LLMs）的最新进展导致了生成的文本输出与人类生成的文本相似度难以分辨。水印算法是潜在工具，通过在LLM生成的输出中嵌入可检测的签名，可以区分LLM生成的文本和人类生成的文本。然而，当前的水印方案在已知攻击下缺乏健壮性。此外，考虑到LLM每天生成数万个文本输出，水印算法需要记忆每个输出才能让检测正常工作，这是不切实际的。本文针对当前水印方案的局限性，提出了针对LLMs的“基于主题的水印算法”概念。

    arXiv:2404.02138v1 Announce Type: cross  Abstract: Recent advancements of large language models (LLMs) have resulted in indistinguishable text outputs comparable to human-generated text. Watermarking algorithms are potential tools that offer a way to differentiate between LLM- and human-generated text by embedding detectable signatures within LLM-generated output. However, current watermarking schemes lack robustness against known attacks against watermarking algorithms. In addition, they are impractical considering an LLM generates tens of thousands of text outputs per day and the watermarking algorithm needs to memorize each output it generates for the detection to work. In this work, focusing on the limitations of current watermarking schemes, we propose the concept of a "topic-based watermarking algorithm" for LLMs. The proposed algorithm determines how to generate tokens for the watermarked LLM output based on extracted topics of an input prompt or the output of a non-watermarked 
    
[^3]: DE-HNN: 一种用于电路网表表示的有效神经模型

    DE-HNN: An effective neural model for Circuit Netlist representation

    [https://arxiv.org/abs/2404.00477](https://arxiv.org/abs/2404.00477)

    设计师们开发了一种名为DE-HNN的神经模型，用于电路网表表示，以解决优化工具运行时间长的问题。

    

    优化工具的运行时间随着设计复杂性的增加而增长，到了可以花费数天来完成一个设计周期的地步，这已经成为一个瓶颈。设计师们希望能够快速获得设计反馈的工具。通过使用过去设计的工具的输入和输出数据，可以尝试构建一个机器学习模型，以显著较短的时间预测设计结果，这比运行工具要快得多。这样的模型的准确性受到设计数据表示的影响，这些数据通常是描述数字电路元素及其连接方式的网表。网表的图表示与图神经网络已经被研究用于这种模型。然而，由于节点数量众多和远程连接的重要性，网表的特性给现有图学习框架带来了几个挑战。

    arXiv:2404.00477v1 Announce Type: new  Abstract: The run-time for optimization tools used in chip design has grown with the complexity of designs to the point where it can take several days to go through one design cycle which has become a bottleneck. Designers want fast tools that can quickly give feedback on a design. Using the input and output data of the tools from past designs, one can attempt to build a machine learning model that predicts the outcome of a design in significantly shorter time than running the tool. The accuracy of such models is affected by the representation of the design data, which is usually a netlist that describes the elements of the digital circuit and how they are connected. Graph representations for the netlist together with graph neural networks have been investigated for such models. However, the characteristics of netlists pose several challenges for existing graph learning frameworks, due to the large number of nodes and the importance of long-range 
    
[^4]: 基于注意力机制的形状变形网络用于无伪影几何重构骨盆腰椎MR图像

    Attention-based Shape-Deformation Networks for Artifact-Free Geometry Reconstruction of Lumbar Spine from MR Images

    [https://arxiv.org/abs/2404.00231](https://arxiv.org/abs/2404.00231)

    这项工作提出了TransDeformer，使用注意力机制实现了对腰椎轮廓的高空间准确性重建，并跨患者实现了网格对应，为医学参数测量提供了可靠性，还设计了变体用于错误估计。

    

    腰椎椎间盘退变，是腰椎间盘渐进性结构性磨损，被认为在腰部疼痛中发挥重要作用，这是一个重要的全球健康关注焦点。从MR图像中自动重建腰椎几何形状，将使医学参数的快速测量成为可能，以评估腰椎状态，从而确定合适的治疗方案。现有的基于图像分割的技术通常会生成错误的分割或不适合医学参数测量的无结构点云。在这项工作中，我们提出了TransDeformer：一种新颖的基于注意力机制的深度学习方法，以高空间准确度和患者间网格对应的方式重建腰椎轮廓，并且我们还提出了一种TransDeformer的变种用于错误估计。特别是，我们设计了新的注意力模块和新的注意力公式，将图像特征和标记化的轮廓特征集成起来，用于预测...

    arXiv:2404.00231v1 Announce Type: cross  Abstract: Lumbar disc degeneration, a progressive structural wear and tear of lumbar intervertebral disc, is regarded as an essential role on low back pain, a significant global health concern. Automated lumbar spine geometry reconstruction from MR images will enable fast measurement of medical parameters to evaluate the lumbar status, in order to determine a suitable treatment. Existing image segmentation-based techniques often generate erroneous segments or unstructured point clouds, unsuitable for medical parameter measurement. In this work, we present TransDeformer: a novel attention-based deep learning approach that reconstructs the contours of the lumbar spine with high spatial accuracy and mesh correspondence across patients, and we also present a variant of TransDeformer for error estimation. Specially, we devise new attention modules with a new attention formula, which integrates image features and tokenized contour features to predict 
    
[^5]: 理解人类反馈对齐学习动态的研究

    Understanding the Learning Dynamics of Alignment with Human Feedback

    [https://arxiv.org/abs/2403.18742](https://arxiv.org/abs/2403.18742)

    本研究对人类偏好对齐的学习动态进行了理论分析，显示了偏好数据集的分布如何影响模型更新速度，并提供了对训练准确度的严格保证，同时揭示了优化易于优先考虑高偏好可区分性行为的复杂现象。

    

    大型语言模型（LLMs）与人类意图对齐已成为安全部署模型在实际系统中的关键任务。现有的对齐方法虽然在经验上取得了成功，但理论上了解这些方法如何影响模型行为仍然是一个悬而未决的问题。我们的工作首次尝试在理论上分析人类偏好对齐的学习动态。我们正式展示了偏好数据集的分布如何影响模型更新速度，并对训练准确度提供了严格的保证。我们的理论还揭示了一个复杂现象，即优化易于优先考虑具有更高偏好可区分性的行为。我们在当代LLMs和对齐任务上在实证上验证了我们的发现，强化了我们的理论见解，并为未来的对齐方法提供了启示。免责声明：本文包含有效

    arXiv:2403.18742v1 Announce Type: cross  Abstract: Aligning large language models (LLMs) with human intentions has become a critical task for safely deploying models in real-world systems. While existing alignment approaches have seen empirical success, theoretically understanding how these methods affect model behavior remains an open question. Our work provides an initial attempt to theoretically analyze the learning dynamics of human preference alignment. We formally show how the distribution of preference datasets influences the rate of model updates and provide rigorous guarantees on the training accuracy. Our theory also reveals an intricate phenomenon where the optimization is prone to prioritizing certain behaviors with higher preference distinguishability. We empirically validate our findings on contemporary LLMs and alignment tasks, reinforcing our theoretical insights and shedding light on considerations for future alignment approaches. Disclaimer: This paper contains potent
    
[^6]: 旋转扫描：带有三元SSM模块的UNet-like Mamba用于医学图像分割

    Rotate to Scan: UNet-like Mamba with Triplet SSM Module for Medical Image Segmentation

    [https://arxiv.org/abs/2403.17701](https://arxiv.org/abs/2403.17701)

    本文提出了Triplet Mamba-UNet，利用残余VSS块提取密集上下文特征，并利用Triplet SSM融合空间和通道维度上的特征。

    

    图像分割在医疗领域的诊断和治疗中占据重要位置。传统的卷积神经网络（CNN）和Transformer模型在这一领域取得了重大进展，但仍然面临由于有限感受野或高计算复杂性而带来的挑战。最近，状态空间模型（SSM），特别是Mamba及其变体，在视觉领域表现出显著性能。然而，它们的特征提取方法可能不够有效，保留了一些冗余结构，留下了参数减少的空间。受先前的空间和通道注意方法的启发，我们提出了Triplet Mamba-UNet。该方法利用残余VSS块来提取密集的上下文特征，同时利用Triplet SSM来融合空间和通道维度上的特征。我们在ISIC17、ISIC18、CVC-300、CVC-ClinicDB上进行了实验。

    arXiv:2403.17701v1 Announce Type: cross  Abstract: Image segmentation holds a vital position in the realms of diagnosis and treatment within the medical domain. Traditional convolutional neural networks (CNNs) and Transformer models have made significant advancements in this realm, but they still encounter challenges because of limited receptive field or high computing complexity. Recently, State Space Models (SSMs), particularly Mamba and its variants, have demonstrated notable performance in the field of vision. However, their feature extraction methods may not be sufficiently effective and retain some redundant structures, leaving room for parameter reduction. Motivated by previous spatial and channel attention methods, we propose Triplet Mamba-UNet. The method leverages residual VSS Blocks to extract intensive contextual features, while Triplet SSM is employed to fuse features across spatial and channel dimensions. We conducted experiments on ISIC17, ISIC18, CVC-300, CVC-ClinicDB, 
    
[^7]: 将扩散模型调整到私有领域而无需微调

    DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning

    [https://arxiv.org/abs/2403.14421](https://arxiv.org/abs/2403.14421)

    开发了第一个差分隐私检索增强生成算法，能够在生成高质量图像样本的同时提供可证明的隐私保证

    

    arXiv:2403.14421v1 公告类型：新的 文摘：文本到图像的扩散模型已经被证明存在样本级别的记忆化问题，可能会复制出与其训练图像几乎完全相同的副本，这可能是不希望看到的。为了解决这个问题，我们开发了第一个能够生成高质量图像样本并提供可证明的隐私保证的差分隐私检索增强生成算法。具体而言，我们假设可以访问一个在少量公共数据上训练的文本到图像扩散模型，并设计一个DP检索机制，以从私有检索数据集中检索的样本来增强文本提示。我们的\emph{差分隐私检索增强扩散模型}（DP-RDM）在适应另一个领域时无需对检索数据集进行微调，并且可以使用最先进的生成模型生成高质量的图像样本，同时满足严格的差分隐私保证。例如，在评估时

    arXiv:2403.14421v1 Announce Type: new  Abstract: Text-to-image diffusion models have been shown to suffer from sample-level memorization, possibly reproducing near-perfect replica of images that they are trained on, which may be undesirable. To remedy this issue, we develop the first differentially private (DP) retrieval-augmented generation algorithm that is capable of generating high-quality image samples while providing provable privacy guarantees. Specifically, we assume access to a text-to-image diffusion model trained on a small amount of public data, and design a DP retrieval mechanism to augment the text prompt with samples retrieved from a private retrieval dataset. Our \emph{differentially private retrieval-augmented diffusion model} (DP-RDM) requires no fine-tuning on the retrieval dataset to adapt to another domain, and can use state-of-the-art generative models to generate high-quality image samples while satisfying rigorous DP guarantees. For instance, when evaluated on M
    
[^8]: AFLoRA: 自适应冻结低秩调整在大型模型参数高效微调中的应用

    AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models

    [https://arxiv.org/abs/2403.13269](https://arxiv.org/abs/2403.13269)

    AFLoRA是一种自适应冻结低秩调整方法，通过逐步冻结投影矩阵来提高性能，减少计算量，并提供对GLUE基准测试的最先进表现。

    

    我们提出了一种新颖的参数高效微调（PEFT）方法，称为自适应冻结低秩调整（AFLoRA）。具体地，对于每个预训练的冻结权重张量，我们添加一个可训练的低秩矩阵并行路径，即下投影和上投影矩阵，每个矩阵后面跟着一个特征变换向量。基于一种新颖的冻结分数，我们在微调过程中逐步冻结这些投影矩阵，以减少计算量并减轻过拟合。我们的实验结果表明，我们可以在GLUE基准测试中获得最先进的性能，平均改善高达0.85％，同时可减少高达9.5倍的平均可训练参数。在运行时间方面，与类似的PEFT备选方案相比，AFLoRA可以提供高达1.86倍的改进。除了我们方法的实际效用之外，我们还提供了关于训练

    arXiv:2403.13269v1 Announce Type: new  Abstract: We present a novel Parameter-Efficient Fine-Tuning (PEFT) method, dubbed as Adaptive Freezing of Low Rank Adaptation (AFLoRA). Specifically, for each pre-trained frozen weight tensor, we add a parallel path of trainable low-rank matrices, namely a down-projection and an up-projection matrix, each of which is followed by a feature transformation vector. Based on a novel freezing score, we the incrementally freeze these projection matrices during fine-tuning to reduce the computation and alleviate over-fitting. Our experimental results demonstrate that we can achieve state-of-the-art performance with an average improvement of up to $0.85\%$ as evaluated on GLUE benchmark while yeilding up to $9.5\times$ fewer average trainable parameters. While compared in terms of runtime, AFLoRA can yield up to $1.86\times$ improvement as opposed to similar PEFT alternatives. Besides the practical utility of our approach, we provide insights on the train
    
[^9]: 神经元中心的赫布学习

    Neuron-centric Hebbian Learning

    [https://arxiv.org/abs/2403.12076](https://arxiv.org/abs/2403.12076)

    提出了一种新的神经元中心的赫布学习模型，相较于传统的ABCD规则，将参数减少了从$5W$到$5N$。

    

    大脑学习机制背后最引人注目的能力之一是通过结构和功能可塑性调整其突触。尽管突触在传递信息到整个大脑中起着基本作用，但几项研究表明，是神经元的激活产生了对突触的改变。然而，大多数为人工神经网络（NNs）设计的可塑性模型，如ABCD规则，侧重于突触而不是神经元，因此优化突触特定的赫布参数。然而，这种方法增加了优化过程的复杂性，因为每个突触都与多个赫布参数相关联。为了克服这一限制，我们提出了一种新颖的可塑性模型，称为神经元中心的赫布学习（NcHL），其优化侧重于神经元而不是突触特定的赫布参数。与ABCD规则相比，NcHL将参数减少从$5W$到$5N$。

    arXiv:2403.12076v1 Announce Type: cross  Abstract: One of the most striking capabilities behind the learning mechanisms of the brain is the adaptation, through structural and functional plasticity, of its synapses. While synapses have the fundamental role of transmitting information across the brain, several studies show that it is the neuron activations that produce changes on synapses. Yet, most plasticity models devised for artificial Neural Networks (NNs), e.g., the ABCD rule, focus on synapses, rather than neurons, therefore optimizing synaptic-specific Hebbian parameters. This approach, however, increases the complexity of the optimization process since each synapse is associated to multiple Hebbian parameters. To overcome this limitation, we propose a novel plasticity model, called Neuron-centric Hebbian Learning (NcHL), where optimization focuses on neuron- rather than synaptic-specific Hebbian parameters. Compared to the ABCD rule, NcHL reduces the parameters from $5W$ to $5N$
    
[^10]: 通过同态的角度推广图神经网络

    Generalization of Graph Neural Networks through the Lens of Homomorphism

    [https://arxiv.org/abs/2403.06079](https://arxiv.org/abs/2403.06079)

    本文通过分析图同态的熵提出了一种新颖视角，推导出了适用于图和节点分类的泛化界限，能够捕捉各种图结构的细微差异，并通过统一框架刻画了广泛的GNN模型。

    

    尽管图神经网络(GNNs)在许多应用中广受欢迎，但其泛化能力仍未得到充分探讨。本文提出通过一种新颖的视角 - 分析图同态的熵来研究GNNs的泛化。通过将图同态与信息论度量联系起来，我们推导出针对图和节点分类的泛化界限。这些界限能够捕捉各种图结构固有的细微差异，包括但不限于路径、环和团。这使得我们能够进行具有稳健理论保证的基于数据的泛化分析。为了阐明我们提出的界限的普适性，我们提出一个统一框架，可以通过图同态的视角刻画广泛的GNN模型。我们通过显示我们的理论发现与实际的一致性来验证我们理论发现的实际适用性。

    arXiv:2403.06079v1 Announce Type: new  Abstract: Despite the celebrated popularity of Graph Neural Networks (GNNs) across numerous applications, the ability of GNNs to generalize remains less explored. In this work, we propose to study the generalization of GNNs through a novel perspective - analyzing the entropy of graph homomorphism. By linking graph homomorphism with information-theoretic measures, we derive generalization bounds for both graph and node classifications. These bounds are capable of capturing subtleties inherent in various graph structures, including but not limited to paths, cycles and cliques. This enables a data-dependent generalization analysis with robust theoretical guarantees. To shed light on the generality of of our proposed bounds, we present a unifying framework that can characterize a broad spectrum of GNN models through the lens of graph homomorphism. We validate the practical applicability of our theoretical findings by showing the alignment between the 
    
[^11]: 具有目标抑制的多约束安全强化学习用于安全关键应用

    Multi-Constraint Safe RL with Objective Suppression for Safety-Critical Applications

    [https://arxiv.org/abs/2402.15650](https://arxiv.org/abs/2402.15650)

    提出了一种目标抑制的新方法，可以在多约束安全领域中改进安全强化学习任务表现，实验证明此方法结合现有算法能够在减少约束违规的情况下实现与基准线相当的任务奖励水平。

    

    尽管在现实世界中非常常见，但具有多个约束条件的安全强化学习任务仍然是一个具有挑战性的领域。为了解决这一挑战，我们提出了一种新方法，即目标抑制，根据安全评判器自适应地抑制任务奖励最大化目标。我们在两个多约束安全领域中对目标抑制进行了基准测试，包括一个自动驾驶领域，在这个领域中任何错误的行为都可能导致灾难性后果。实证结果表明，我们提出的方法与现有的安全强化学习算法相结合，可以在显著减少约束违规的情况下匹配我们的基准线所达到的任务奖励。

    arXiv:2402.15650v1 Announce Type: cross  Abstract: Safe reinforcement learning tasks with multiple constraints are a challenging domain despite being very common in the real world. To address this challenge, we propose Objective Suppression, a novel method that adaptively suppresses the task reward maximizing objectives according to a safety critic. We benchmark Objective Suppression in two multi-constraint safety domains, including an autonomous driving domain where any incorrect behavior can lead to disastrous consequences. Empirically, we demonstrate that our proposed method, when combined with existing safe RL algorithms, can match the task reward achieved by our baselines with significantly fewer constraint violations.
    
[^12]: APTQ: 针对大型语言模型的注意力感知后训练混合精度量化

    APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models

    [https://arxiv.org/abs/2402.14866](https://arxiv.org/abs/2402.14866)

    APTQ提出了针对大型语言模型的注意力感知后训练混合精度量化方法，在保持模型性能的同时，超越了先前的量化方法，并在零-shot任务上达到了最先进的准确率

    

    大型语言模型（LLMs）极大地推动了自然语言处理范式。然而，高计算负载和巨大的模型尺寸对在边缘设备上部署构成了巨大挑战。为此，我们提出了针对LLMs的APTQ（Attention-aware Post-Training Mixed-Precision Quantization），该方法不仅考虑了每层权重的二阶信息，而且首次考虑了注意力输出对整个模型的非线性影响。我们利用Hessian迹作为混合精度量化的敏感度度量，确保经过理性的精度降低能保持模型性能。实验表明，APTQ超越了先前的量化方法，在C4数据集中以平均4位宽度获得5.22困惑度，几乎等效于全精度。此外，APTQ在LLaMa-7B和LLaMa-1中以平均3.8位宽度达到了68.24％和70.48％的最先进零-shot准确率。

    arXiv:2402.14866v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have greatly advanced the natural language processing paradigm. However, the high computational load and huge model sizes pose a grand challenge for deployment on edge devices. To this end, we propose APTQ (Attention-aware Post-Training Mixed-Precision Quantization) for LLMs, which considers not only the second-order information of each layer's weights, but also, for the first time, the nonlinear effect of attention outputs on the entire model. We leverage the Hessian trace as a sensitivity metric for mixed-precision quantization, ensuring an informed precision reduction that retains model performance. Experiments show APTQ surpasses previous quantization methods, achieving an average of 4 bit width a 5.22 perplexity nearly equivalent to full precision in the C4 dataset. In addition, APTQ attains state-of-the-art zero-shot accuracy of 68.24\% and 70.48\% at an average bitwidth of 3.8 in LLaMa-7B and LLaMa-1
    
[^13]: 在医学成像中推进低秩和局部低秩矩阵逼近：系统文献综述与未来方向

    Advancing Low-Rank and Local Low-Rank Matrix Approximation in Medical Imaging: A Systematic Literature Review and Future Directions

    [https://arxiv.org/abs/2402.14045](https://arxiv.org/abs/2402.14045)

    本文系统综述了在医学成像中应用低秩矩阵逼近（LRMA）和其派生物局部LRMA（LLRMA）的作品，并指出自2015年以来医学成像领域开始偏向于使用LLRMA，显示其在捕获医学数据中复杂结构方面的潜力和有效性。

    

    医学成像数据集的大容量和复杂性是存储、传输和处理的瓶颈。为解决这些挑战，低秩矩阵逼近（LRMA）及其派生物局部LRMA（LLRMA）的应用已显示出潜力。本文进行了系统文献综述，展示了在医学成像中应用LRMA和LLRMA的作品。文献的详细分析确认了应用于各种成像模态的LRMA和LLRMA方法。本文解决了现有LRMA和LLRMA方法所面临的挑战和限制。我们注意到，自2015年以来，医学成像领域明显偏向于LLRMA，显示了相对于LRMA在捕获医学数据中复杂结构方面的潜力和有效性。鉴于LLRMA所使用的浅层相似性方法的限制，我们建议使用先进语义图像分割来处理相似性。

    arXiv:2402.14045v1 Announce Type: cross  Abstract: The large volume and complexity of medical imaging datasets are bottlenecks for storage, transmission, and processing. To tackle these challenges, the application of low-rank matrix approximation (LRMA) and its derivative, local LRMA (LLRMA) has demonstrated potential.   This paper conducts a systematic literature review to showcase works applying LRMA and LLRMA in medical imaging. A detailed analysis of the literature identifies LRMA and LLRMA methods applied to various imaging modalities. This paper addresses the challenges and limitations associated with existing LRMA and LLRMA methods.   We note a significant shift towards a preference for LLRMA in the medical imaging field since 2015, demonstrating its potential and effectiveness in capturing complex structures in medical data compared to LRMA. Acknowledging the limitations of shallow similarity methods used with LLRMA, we suggest advanced semantic image segmentation for similarit
    
[^14]: 非独立同分布数据孤岛上的联邦多任务学习：实验研究

    Federated Multi-Task Learning on Non-IID Data Silos: An Experimental Study

    [https://arxiv.org/abs/2402.12876](https://arxiv.org/abs/2402.12876)

    这项研究介绍了一种新颖的FMTL-Bench框架，用于系统评估联邦多任务学习（FMTL）范式，填补了FL和MTL综合评估方法的空白。

    

    这种创新的联邦多任务学习（FMTL）方法整合了联邦学习（FL）和多任务学习（MTL）的优点，能够在多任务学习数据集上进行协作模型训练。然而，目前在该领域缺乏整合FL和MTL独特特性的综合评估方法。本文通过引入一个新颖的框架，FMTL-Bench，填补了这一空白，用于系统评估FMTL范式。这个基准涵盖了数据、模型和优化算法级别的各个方面，并包括七组比较实验，封装了广泛的非独立同分布（Non-IID）数据分区场景。我们提出了对比各种指标的基线的系统过程，并对通信开支、时间和能源消耗进行了案例研究。通过我们的大量实验，我们旨在提供有价值的

    arXiv:2402.12876v1 Announce Type: new  Abstract: The innovative Federated Multi-Task Learning (FMTL) approach consolidates the benefits of Federated Learning (FL) and Multi-Task Learning (MTL), enabling collaborative model training on multi-task learning datasets. However, a comprehensive evaluation method, integrating the unique features of both FL and MTL, is currently absent in the field. This paper fills this void by introducing a novel framework, FMTL-Bench, for systematic evaluation of the FMTL paradigm. This benchmark covers various aspects at the data, model, and optimization algorithm levels, and comprises seven sets of comparative experiments, encapsulating a wide array of non-independent and identically distributed (Non-IID) data partitioning scenarios. We propose a systematic process for comparing baselines of diverse indicators and conduct a case study on communication expenditure, time, and energy consumption. Through our exhaustive experiments, we aim to provide valuable
    
[^15]: 智能并行自动纠错解码：加速大型语言模型推理

    Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding

    [https://arxiv.org/abs/2402.11809](https://arxiv.org/abs/2402.11809)

    提出了Smart Parallel Auto-Correct Decoding (SPACE)方法，通过集成半自回归推理和猜测解码，实现了大型语言模型推理加速和并行生成验证令牌的功能。

    

    这项研究旨在加速拥有数十亿参数的大型语言模型（LLMs）的推理速度。我们提出了“智能并行自动纠错解码”（SPACE），这是一种创新方法，旨在实现LLMs的无损加速。通过集成半自回归推理和猜测解码能力，SPACE独特地使自回归LLMs能够并行生成和验证令牌。这是通过专门的半自回归监督微调过程实现的，该过程使现有LLMs具有同时预测多个令牌的能力。此外，一种自动纠错解码算法促进了单个模型调用内令牌序列的同时生成和验证。通过在一系列LLMs上进行广泛实验证明，SPACE在HumanEval-X上表现出2.7倍至4.0倍的推理加速。

    arXiv:2402.11809v1 Announce Type: cross  Abstract: This research aims to accelerate the inference speed of large language models (LLMs) with billions of parameters. We propose \textbf{S}mart \textbf{P}arallel \textbf{A}uto-\textbf{C}orrect d\textbf{E}coding (SPACE), an innovative approach designed for achieving lossless acceleration of LLMs. By integrating semi-autoregressive inference and speculative decoding capabilities, SPACE uniquely enables autoregressive LLMs to parallelize token generation and verification. This is realized through a specialized semi-autoregressive supervised fine-tuning process that equips existing LLMs with the ability to simultaneously predict multiple tokens. Additionally, an auto-correct decoding algorithm facilitates the simultaneous generation and verification of token sequences within a single model invocation. Through extensive experiments on a range of LLMs, SPACE has demonstrated inference speedup ranging from 2.7x-4.0x on HumanEval-X while maintaini
    
[^16]: 高斯模型集成置信传播用于高维系统中的高效推断

    Gaussian Ensemble Belief Propagation for Efficient Inference in High-Dimensional Systems

    [https://arxiv.org/abs/2402.08193](https://arxiv.org/abs/2402.08193)

    高斯模型集成置信传播算法（GEnBP）是一种用于高维系统中高效推断的方法，通过集成卡尔曼滤波器和高斯置信传播等技术相结合，能有效处理高维状态、参数和复杂的依赖结构。

    

    高维模型中的高效推断仍然是机器学习中的一个核心挑战。本文介绍了一种名为高斯模型集成置信传播（GEnBP）算法的方法，该方法是集成卡尔曼滤波器和高斯置信传播（GaBP）方法的结合。GEnBP通过在图模型结构中传递低秩本地信息来更新集成模型。这种组合继承了每种方法的有利特性。集成技术使得GEnBP能够处理高维状态、参数和复杂的、嘈杂的黑箱生成过程。在图模型结构中使用本地信息确保了该方法适用于分布式计算，并能高效地处理复杂的依赖结构。当集成大小远小于推断维度时，GEnBP特别有优势。这种情况在空时建模、图像处理和物理模型反演等领域经常出现。GEnBP可以应用于一般性问题。

    Efficient inference in high-dimensional models remains a central challenge in machine learning. This paper introduces the Gaussian Ensemble Belief Propagation (GEnBP) algorithm, a fusion of the Ensemble Kalman filter and Gaussian belief propagation (GaBP) methods. GEnBP updates ensembles by passing low-rank local messages in a graphical model structure. This combination inherits favourable qualities from each method. Ensemble techniques allow GEnBP to handle high-dimensional states, parameters and intricate, noisy, black-box generation processes. The use of local messages in a graphical model structure ensures that the approach is suited to distributed computing and can efficiently handle complex dependence structures. GEnBP is particularly advantageous when the ensemble size is considerably smaller than the inference dimension. This scenario often arises in fields such as spatiotemporal modelling, image processing and physical model inversion. GEnBP can be applied to general problem s
    
[^17]: 大型语言用户界面：由LLMs驱动的语音交互用户界面

    Large Language User Interfaces: Voice Interactive User Interfaces powered by LLMs

    [https://arxiv.org/abs/2402.07938](https://arxiv.org/abs/2402.07938)

    本研究旨在利用和引导升级后的LLMs的强大能力，构建一个框架，作为用户和用户界面之间的中介，通过对自然文本输入进行彻底分析，实现智能和响应式用户体验。

    

    最近大型语言模型的快速发展展示了其在逻辑推理和理解方面的卓越能力。这些新发现的能力引发了新一代软件的诞生，正如它们在工业界无数应用中所展示的那样。本研究旨在利用和引导升级后的LLMs的强大能力，构建一个框架，作为用户和用户界面之间的中介。通过对自然文本输入进行彻底分析，一个经过精心设计的LLM引擎可以理解用户的需求，分类最有可能的应用程序，识别所需的UI组件，并随后执行用户期望的操作。这种集成可以将静态UI系统发展成高度动态和可适应的解决方案，引入智能和响应式用户体验的新领域。这样的框架可以从根本上改变用户完成日常任务的方式，极大提升用户体验。

    The recent meteoric advancements in large language models have showcased a remarkable capacity for logical reasoning and comprehension. These newfound capabilities have opened the door to a new generation of software, as has been made obvious through the innumerable ways they are being applied in the industry. This research focuses on harnessing and guiding the upgraded power of LLMs to construct a framework that can serve as an intermediary between a user and their user interface. By comprehending a user's needs through a thorough analysis of natural textual inputs, an effectively crafted LLM engine can classify the most likely available application, identify the desired UI component and subsequently execute the user's expected actions. This integration can evolve static UI systems into highly dynamic and adaptable solutions, introducing a new frontier of intelligent and responsive user experiences. Such a framework can fundamentally shift how users accomplish daily tasks, skyrocket e
    
[^18]: 透过校准的视角理解不变风险最小化的变体

    Towards Understanding Variants of Invariant Risk Minimization through the Lens of Calibration

    [https://arxiv.org/abs/2401.17541](https://arxiv.org/abs/2401.17541)

    本研究通过比较分析使用不同近似方法的不变风险最小化（IRM）技术，以期望校准误差（ECE）作为关键指标，观察到基于信息瓶颈的IRM在压缩关键特征上表现最佳。

    

    传统的机器学习模型假设训练和测试数据是独立且同分布的。然而，在现实世界的应用中，测试分布往往与训练不同。这个问题被称为越域泛化，在常规模型面临挑战。不变风险最小化（IRM）作为一个解决方案出现，旨在识别在不同环境中保持不变的特征，以增强越域鲁棒性。然而，IRM的复杂性，特别是其双层优化，导致了各种近似方法的开发。我们的研究调查了这些近似IRM技术，使用期望校准误差（ECE）作为关键指标。ECE可以衡量模型预测的可靠性，它是衡量模型是否有效捕捉到环境不变特征的指标。通过对具有分布变化的数据集进行比较分析，我们观察到基于信息瓶颈的IRM在压缩了...（接下部分摘要超过200字，提取前200字）

    Machine learning models traditionally assume that training and test data are independently and identically distributed. However, in real-world applications, the test distribution often differs from training. This problem, known as out-of-distribution generalization, challenges conventional models. Invariant Risk Minimization (IRM) emerges as a solution, aiming to identify features invariant across different environments to enhance out-of-distribution robustness. However, IRM's complexity, particularly its bi-level optimization, has led to the development of various approximate methods. Our study investigates these approximate IRM techniques, employing the Expected Calibration Error (ECE) as a key metric. ECE, which measures the reliability of model prediction, serves as an indicator of whether models effectively capture environment-invariant features. Through a comparative analysis of datasets with distributional shifts, we observe that Information Bottleneck-based IRM, which condenses
    
[^19]: 像素到高程：使用图像学习预测自主越野导航中的长距离高程地图

    Pixel to Elevation: Learning to Predict Elevation Maps at Long Range using Images for Autonomous Offroad Navigation

    [https://arxiv.org/abs/2401.17484](https://arxiv.org/abs/2401.17484)

    本研究提出了一种学习方法，可以通过车载视角图像实时预测长距离地形高程地图。该方法包括transformer-based编码器、方向感知的位置编码和历史增强的可学习地图嵌入。通过学习视角图像与鸟瞰图高程地图之间的关联，结合车辆姿态信息和视觉图像特征，实现更好的地图预测时序一致性。

    

    在离线机器人任务中，长距离理解地形拓扑对于成功至关重要，特别是在高速导航时。目前，几何映射主要依赖于LiDAR传感器，但在更远距离的映射时提供的测量数较少。为了解决这个挑战，我们提出了一种新颖的基于学习的方法，能够仅使用实时车载视角图像预测长距离地形高程地图。我们的方法由三个主要元素组成。首先，引入了一个基于transformer的编码器，该编码器学习车载视角图像与先前的鸟瞰图高程地图预测之间的跨视图关联。其次，提出了一种方向感知的位置编码，将3D车辆姿态信息与多视角视觉图像特征相结合，用于处理复杂的非结构化地形。最后，提出了一种历史增强的可学习地图嵌入，以实现高程地图预测之间的更好时序一致性。

    Understanding terrain topology at long-range is crucial for the success of off-road robotic missions, especially when navigating at high-speeds. LiDAR sensors, which are currently heavily relied upon for geometric mapping, provide sparse measurements when mapping at greater distances. To address this challenge, we present a novel learning-based approach capable of predicting terrain elevation maps at long-range using only onboard egocentric images in real-time. Our proposed method is comprised of three main elements. First, a transformer-based encoder is introduced that learns cross-view associations between the egocentric views and prior bird-eye-view elevation map predictions. Second, an orientation-aware positional encoding is proposed to incorporate the 3D vehicle pose information over complex unstructured terrain with multi-view visual image features. Lastly, a history-augmented learn-able map embedding is proposed to achieve better temporal consistency between elevation map predi
    
[^20]: 少样本因果表示学习用于异构图的跨领域泛化

    Few-Shot Causal Representation Learning for Out-of-Distribution Generalization on Heterogeneous Graphs

    [https://arxiv.org/abs/2401.03597](https://arxiv.org/abs/2401.03597)

    该论文提出了一种少样本因果表示学习方法，用于在异构图上实现跨领域泛化，解决了源HG与目标HG分布不匹配导致的知识传递无效和学习性能不佳的问题。

    

    异构图少样本学习（HGFL）已经被研发来解决异构图（HGs）中标签稀疏问题，其中包括各种类型的节点和边。HGFL的核心概念是从源HG中富标记类中提取知识，将这些知识转移到目标HG以促进学习新类别，使用少量标记的训练数据，并最终在未标记的测试数据上进行预测。现有方法通常假设源HG、训练数据和测试数据都共享相同的分布。然而，在实践中，这三种数据之间的分布转变是不可避免的，原因有两个：（1）源HG的有限可用性与目标HG分布匹配，以及（2）目标HG的不可预测数据生成机制。这种分布转变导致现有方法中知识传递无效和学习性能不佳。

    arXiv:2401.03597v2 Announce Type: replace  Abstract: Heterogeneous graph few-shot learning (HGFL) has been developed to address the label sparsity issue in heterogeneous graphs (HGs), which consist of various types of nodes and edges. The core concept of HGFL is to extract knowledge from rich-labeled classes in a source HG, transfer this knowledge to a target HG to facilitate learning new classes with few-labeled training data, and finally make predictions on unlabeled testing data. Existing methods typically assume that the source HG, training data, and testing data all share the same distribution. However, in practice, distribution shifts among these three types of data are inevitable due to two reasons: (1) the limited availability of the source HG that matches the target HG distribution, and (2) the unpredictable data generation mechanism of the target HG. Such distribution shifts result in ineffective knowledge transfer and poor learning performance in existing methods, thereby le
    
[^21]: 学习在有限信息下进行操纵

    Learning to Manipulate under Limited Information. (arXiv:2401.16412v1 [cs.AI])

    [http://arxiv.org/abs/2401.16412](http://arxiv.org/abs/2401.16412)

    本研究通过训练神经网络在有限信息条件下学习如何利用不同投票方法进行操纵，发现某些投票方法在有限信息下容易被操纵，而其他方法不容易被操纵。

    

    根据社会选择理论的经典结果，任何合理的偏好投票方法有时会给个体提供报告不真实偏好的激励。对于比较投票方法来说，不同投票方法在多大程度上更或者更少抵抗这种策略性操纵已成为一个关键考虑因素。在这里，我们通过神经网络在不同规模下对限制信息下学习如何利用给定投票方法进行操纵的成功程度来衡量操纵的抵抗力。我们训练了将近40,000个不同规模的神经网络来对抗8种不同的投票方法，在6种限制信息情况下，进行包含5-21名选民和3-6名候选人的委员会规模选举的操纵。我们发现，一些投票方法，如Borda方法，在有限信息下可以被神经网络高度操纵，而其他方法，如Instant Runoff方法，虽然被一个理想的操纵者利润化操纵，但在有限信息下不会受到操纵。

    By classic results in social choice theory, any reasonable preferential voting method sometimes gives individuals an incentive to report an insincere preference. The extent to which different voting methods are more or less resistant to such strategic manipulation has become a key consideration for comparing voting methods. Here we measure resistance to manipulation by whether neural networks of varying sizes can learn to profitably manipulate a given voting method in expectation, given different types of limited information about how other voters will vote. We trained nearly 40,000 neural networks of 26 sizes to manipulate against 8 different voting methods, under 6 types of limited information, in committee-sized elections with 5-21 voters and 3-6 candidates. We find that some voting methods, such as Borda, are highly manipulable by networks with limited information, while others, such as Instant Runoff, are not, despite being quite profitably manipulated by an ideal manipulator with
    
[^22]: RiemannONets: 可解释的用于Riemann问题的神经算子

    RiemannONets: Interpretable Neural Operators for Riemann Problems. (arXiv:2401.08886v1 [cs.LG])

    [http://arxiv.org/abs/2401.08886](http://arxiv.org/abs/2401.08886)

    本文使用可解释的神经算子来解决高速流动中的Riemann问题，通过对DeepONet进行简单修改，在极端压力跳跃情况下获得了非常准确的解决方案。

    

    在数值分析中，如何为模拟具有强激波、稀疏化和接触间断的高速流动开发合适的表示一直是一个长期存在的问题。本文采用神经算子来解决在可压缩流中遇到的Riemann问题，其中包括极端压力跳跃（高达$10^{10}$的压力比）。特别地，在本研究中，我们首先考虑了DeepONet，它是在Lee和Shin的最新工作基础上进行的两阶段训练。在第一阶段，我们从主干网络中提取了一个基础并使其正交化，然后在第二阶段使用它来训练分支网络。这个对DeepONet的简单修改对其准确性、效率和鲁棒性有着深远影响，相比原始版本，它提供了非常准确的Riemann问题解决方案。此外，它还使我们能够在物理上对结果进行解释，因为层次化的数据驱动生成的基础反映了所有流动特征，否则将被引入。

    Developing the proper representations for simulating high-speed flows with strong shock waves, rarefactions, and contact discontinuities has been a long-standing question in numerical analysis. Herein, we employ neural operators to solve Riemann problems encountered in compressible flows for extreme pressure jumps (up to $10^{10}$ pressure ratio). In particular, we first consider the DeepONet that we train in a two-stage process, following the recent work of Lee and Shin, wherein the first stage, a basis is extracted from the trunk net, which is orthonormalized and subsequently is used in the second stage in training the branch net. This simple modification of DeepONet has a profound effect on its accuracy, efficiency, and robustness and leads to very accurate solutions to Riemann problems compared to the vanilla version. It also enables us to interpret the results physically as the hierarchical data-driven produced basis reflects all the flow features that would otherwise be introduce
    
[^23]: 大型语言模型作为具有普适性的机器人任务策略

    Large Language Models as Generalizable Policies for Embodied Tasks. (arXiv:2310.17722v1 [cs.LG])

    [http://arxiv.org/abs/2310.17722](http://arxiv.org/abs/2310.17722)

    本研究展示了大型语言模型(LLMs)可以被适应为具有普适性的机器人任务策略。通过我们的方法LLaRP，我们成功将预训练的冻结LLM用于接收指令和视觉输入，并在环境中直接输出动作。我们的实验结果表明LLaRP不仅对任务指令的复杂改写具有鲁棒性，而且可以推广到需要新颖最优行为的新任务。在大量未见任务中，LLaRP表现出了显著的成功率提升，并且我们提供了一个新的基准测试(Language Rearrangement)来促进进一步的研究。

    

    我们展示了大型语言模型(LLMs)可以被调整为适用于机器人视觉任务的普适性策略。我们的方法被称为大型语言模型强化学习策略(LLaRP)，它将预训练的冻结的LLM调整为接收文本指令和视觉自我中心观测作为输入，并直接在环境中输出动作。通过强化学习，我们训练LLaRP通过与环境的交互来看和行动。我们展示了LLaRP对任务指令的复杂改写具有鲁棒性，并且可以推广到需要新颖最优行为的新任务。特别地，在1,000个未见任务中，它的成功率达到了42%，是其他常见学习基线或零样本应用的1.7倍成功率。最后，为了帮助社区研究以语言为条件的、大规模多任务的机器人AI问题，我们发布了一个新的基准测试(Language Rearrangement)，包括150,000个训练任务和1,000个测试任务，用于语言为条件的重新排列。

    We show that large language models (LLMs) can be adapted to be generalizable policies for embodied visual tasks. Our approach, called Large LAnguage model Reinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take as input text instructions and visual egocentric observations and output actions directly in the environment. Using reinforcement learning, we train LLaRP to see and act solely through environmental interactions. We show that LLaRP is robust to complex paraphrasings of task instructions and can generalize to new tasks that require novel optimal behavior. In particular, on 1,000 unseen tasks it achieves 42% success rate, 1.7x the success rate of other common learned baselines or zero-shot applications of LLMs. Finally, to aid the community in studying language conditioned, massively multi-task, embodied AI problems we release a novel benchmark, Language Rearrangement, consisting of 150,000 training and 1,000 testing tasks for language-conditioned rearrangem
    
[^24]: 针对不平衡分类的尖锐误差界：少数类中有多少样本？

    Sharp error bounds for imbalanced classification: how many examples in the minority class?. (arXiv:2310.14826v1 [stat.ML])

    [http://arxiv.org/abs/2310.14826](http://arxiv.org/abs/2310.14826)

    该论文提出了两个在稀有类概率趋近于零的情况下的新贡献，分别是一种非渐近快速率概率界限和一种一致上界估计方法，这些发现为在实际情况下的类别加权提供了更清晰的理解，为进一步的研究提供了新的方向。

    

    在处理不平衡分类数据时，重新加权损失函数是一种标准的方法，可以在风险度量中平衡真正的正例率和真正的负例率。尽管在这个领域有重要的理论工作，但现有的结果并没有充分解决不平衡分类框架中的主要挑战，即相对于整个样本大小来说一个类的可忽略的大小以及需要通过趋近于零的概率来重新调整风险函数的问题。为了解决这个问题，我们在稀有类概率趋近于零的情况下提出了两个新的贡献：（1）用于约束平衡经验风险最小化的非渐近快速率概率界限，以及（2）用于平衡最近邻估计的一致上界。我们的发现更清楚地说明了类别加权在实际情况下的益处，为这个领域的进一步研究开辟了新的途径。

    When dealing with imbalanced classification data, reweighting the loss function is a standard procedure allowing to equilibrate between the true positive and true negative rates within the risk measure. Despite significant theoretical work in this area, existing results do not adequately address a main challenge within the imbalanced classification framework, which is the negligible size of one class in relation to the full sample size and the need to rescale the risk function by a probability tending to zero. To address this gap, we present two novel contributions in the setting where the rare class probability approaches zero: (1) a non asymptotic fast rate probability bound for constrained balanced empirical risk minimization, and (2) a consistent upper bound for balanced nearest neighbors estimates. Our findings provide a clearer understanding of the benefits of class-weighting in realistic settings, opening new avenues for further research in this field.
    
[^25]: Mori-Zwanzig潜变空间Koopman闭包用于非线性自编码器

    Mori-Zwanzig latent space Koopman closure for nonlinear autoencoder. (arXiv:2310.10745v1 [cs.LG])

    [http://arxiv.org/abs/2310.10745](http://arxiv.org/abs/2310.10745)

    本研究提出了一种名为Mori-Zwanzig自编码器（MZ-AE）的新方法，用于在低维空间中稳健地逼近Koopman算子，通过非线性自编码器和Mori-Zwanzig形式主义的集成实现对有限不变Koopman子空间的逼近，从而增强了精确性和准确预测复杂系统行为的能力。

    

    Koopman算子提供了一种吸引人的方法来实现非线性系统的全局线性化，使其成为简化复杂动力学理解的宝贵方法。虽然数据驱动的方法在逼近有限Koopman算子方面表现出了潜力，但它们面临着各种挑战，例如选择合适的可观察量、降维和准确预测复杂系统行为的能力。本研究提出了一种名为Mori-Zwanzig自编码器（MZ-AE）的新方法，用于在低维空间中稳健地逼近Koopman算子。所提出的方法利用非线性自编码器提取关键可观察量来逼近有限不变Koopman子空间，并利用Mori-Zwanzig形式主义集成非马尔可夫校正机制。因此，该方法在非线性自编码器的潜变流形中产生了动力学的封闭表示，从而提高了精确性和...

    The Koopman operator presents an attractive approach to achieve global linearization of nonlinear systems, making it a valuable method for simplifying the understanding of complex dynamics. While data-driven methodologies have exhibited promise in approximating finite Koopman operators, they grapple with various challenges, such as the judicious selection of observables, dimensionality reduction, and the ability to predict complex system behaviours accurately. This study presents a novel approach termed Mori-Zwanzig autoencoder (MZ-AE) to robustly approximate the Koopman operator in low-dimensional spaces. The proposed method leverages a nonlinear autoencoder to extract key observables for approximating a finite invariant Koopman subspace and integrates a non-Markovian correction mechanism using the Mori-Zwanzig formalism. Consequently, this approach yields a closed representation of dynamics within the latent manifold of the nonlinear autoencoder, thereby enhancing the precision and s
    
[^26]: 从大规模交互轨迹中自动挖掘宏任务

    Automatic Macro Mining from Interaction Traces at Scale. (arXiv:2310.07023v1 [cs.HC])

    [http://arxiv.org/abs/2310.07023](http://arxiv.org/abs/2310.07023)

    本文介绍了一种基于大型语言模型的新方法，用于从移动交互轨迹中自动提取语义上有意义的宏任务。通过多项研究和实验证明了该方法的有效性和提取的宏任务的实用性。

    

    宏任务是我们日常手机活动的构建块任务（例如，“登录”或“预定航班”）。有效地提取宏任务对于理解移动交互和实现任务自动化至关重要。然而，这些宏任务在大规模情况下很难提取，因为它们可以由多个步骤组成，同时又隐藏在应用的编程组件中。在本文中，我们介绍了一种基于大型语言模型（LLMs）的新方法，以自动从随机和用户策划的移动交互轨迹中提取语义上有意义的宏任务。我们的方法产生的宏任务自动标记了自然语言描述，并且可以完全执行。为了检验提取的质量，我们进行了多项研究，包括用户评估、与人工策划任务的比较分析以及对这些宏任务的自动执行。这些实验和分析显示了我们方法的有效性以及提取的宏任务在不同的任务中的有用性。

    Macros are building block tasks of our everyday smartphone activity (e.g., "login", or "booking a flight"). Effectively extracting macros is important for understanding mobile interaction and enabling task automation. These macros are however difficult to extract at scale as they can be comprised of multiple steps yet hidden within programmatic components of the app. In this paper, we introduce a novel approach based on Large Language Models (LLMs) to automatically extract semantically meaningful macros from both random and user-curated mobile interaction traces. The macros produced by our approach are automatically tagged with natural language descriptions and are fully executable. To examine the quality of extraction, we conduct multiple studies, including user evaluation, comparative analysis against human-curated tasks, and automatic execution of these macros. These experiments and analyses show the effectiveness of our approach and the usefulness of extracted macros in various dow
    
[^27]: 机器学习在医学影像中的可解释性框架

    A Framework for Interpretability in Machine Learning for Medical Imaging. (arXiv:2310.01685v1 [cs.LG])

    [http://arxiv.org/abs/2310.01685](http://arxiv.org/abs/2310.01685)

    本文提出了一个机器学习在医学影像中的可解释性框架，明确了解释性的目标和要素，以指导方法设计并改进实际应用。

    

    机器学习在医学影像中的可解释性是一个重要的研究方向。然而，对于可解释性的定义存在一种普遍的模糊感。为什么需要在医学影像中的机器学习中解释性？当需要解释性时，实际上追求的目标是什么？为了回答这些问题，我们确定了在医学影像中的机器学习可解释性的目标和要素需要形式化。通过对医学图像分析和机器学习的交叉点中常见的实际任务和目标进行推理，我们确定了四个核心要素：定位、视觉可识别性、物理归因和透明度。总的来说，本文在医学影像的背景下系统化了可解释性的需求，我们的实践观点澄清了具体的医学影像机器学习可解释性目标和考虑因素，以指导方法设计并改进实际应用。我们的目标是为模型设计提供实用和教学信息。

    Interpretability for machine learning models in medical imaging (MLMI) is an important direction of research. However, there is a general sense of murkiness in what interpretability means. Why does the need for interpretability in MLMI arise? What goals does one actually seek to address when interpretability is needed? To answer these questions, we identify a need to formalize the goals and elements of interpretability in MLMI. By reasoning about real-world tasks and goals common in both medical image analysis and its intersection with machine learning, we identify four core elements of interpretability: localization, visual recognizability, physical attribution, and transparency. Overall, this paper formalizes interpretability needs in the context of medical imaging, and our applied perspective clarifies concrete MLMI-specific goals and considerations in order to guide method design and improve real-world usage. Our goal is to provide practical and didactic information for model desig
    
[^28]: 基于语言模型的概率测量专利权要求范围的新方法

    A novel approach to measuring patent claim scope based on probabilities obtained from (large) language models. (arXiv:2309.10003v1 [cs.CL])

    [http://arxiv.org/abs/2309.10003](http://arxiv.org/abs/2309.10003)

    本文提出了一种使用概率从语言模型中获得的自信息来测量专利权要求范围的新方法。该方法通过计算要求的发生概率和自信息来评估要求的信息量，进而反映出要求的范围。研究结果表明，不同类型的语言模型对范围测量的影响不同，最简单的模型可以将范围度量简化为单词或字符计数的倒数。此方法在九个系列的专利权要求上进行了验证，结果表明各系列的要求范围逐渐减小。

    

    本文提出了一种将专利权要求的范围测量为该要求所包含的自信息的倒数的方法。这种方法基于信息论，基于一个假设，即罕见的概念比平常的概念更具信息量，因为它更令人惊讶。自信息是从该要求的发生概率计算得出的，其中概率是根据语言模型计算的。本文考虑了五个语言模型，从最简单的模型（每个单词或字符均从均匀分布中抽取）到中等模型（使用平均词或字符频率），再到一个大型语言模型（GPT2）。有趣的是，最简单的语言模型将范围度量减少为单词或字符计数的倒数，这是先前作品中已经使用的度量标准。该方法应用于九个系列的针对不同发明的专利权要求，其中每个系列的要求范围逐渐减小。

    This work proposes to measure the scope of a patent claim as the reciprocal of the self-information contained in this claim. Grounded in information theory, this approach is based on the assumption that a rare concept is more informative than a usual concept, inasmuch as it is more surprising. The self-information is calculated from the probability of occurrence of that claim, where the probability is calculated in accordance with a language model. Five language models are considered, ranging from the simplest models (each word or character is drawn from a uniform distribution) to intermediate models (using average word or character frequencies), to a large language model (GPT2). Interestingly, the simplest language models reduce the scope measure to the reciprocal of the word or character count, a metric already used in previous works. Application is made to nine series of patent claims directed to distinct inventions, where the claims in each series have a gradually decreasing scope.
    
[^29]: PCN：一种利用新颖的图构建方法和切比雪夫图卷积的深度学习方法进行喷注标记

    PCN: A Deep Learning Approach to Jet Tagging Utilizing Novel Graph Construction Methods and Chebyshev Graph Convolutions. (arXiv:2309.08630v1 [hep-ph])

    [http://arxiv.org/abs/2309.08630](http://arxiv.org/abs/2309.08630)

    本研究提出了一种基于图形的喷注表示方法，并设计了一种名为PCN的图神经网络（GNN），利用切比雪夫图卷积（ChebConv）进行深度学习喷注标记，取得了显著的改进。

    

    喷注标记是高能物理实验中的一个分类问题，旨在识别粒子碰撞产生的锥状喷注，并将其标记为发射粒子。喷注标记的进展为超出标准模型的新物理搜索提供了机会。目前的方法使用深度学习在复杂碰撞数据中寻找隐藏的模式。然而，将喷注表示为深度学习模型的输入的方法多种多样，并且通常会向模型隐藏有信息的特征。在这项研究中，我们提出了一种基于图形的喷注表示方法，以尽可能地编码最多的信息。为了从这种表示中最好地学习，我们设计了一种名为Particle Chebyshev Network（PCN）的图神经网络（GNN），并使用切比雪夫图卷积（ChebConv）。ChebConv已经被证明是GNN中的一种有效替代传统图卷积的方法，而在喷注标记中还没有被探索过。PCN取得了显著的改进。

    Jet tagging is a classification problem in high-energy physics experiments that aims to identify the collimated sprays of subatomic particles, jets, from particle collisions and tag them to their emitter particle. Advances in jet tagging present opportunities for searches of new physics beyond the Standard Model. Current approaches use deep learning to uncover hidden patterns in complex collision data. However, the representation of jets as inputs to a deep learning model have been varied, and often, informative features are withheld from models. In this study, we propose a graph-based representation of a jet that encodes the most information possible. To learn best from this representation, we design Particle Chebyshev Network (PCN), a graph neural network (GNN) using Chebyshev graph convolutions (ChebConv). ChebConv has been demonstrated as an effective alternative to classical graph convolutions in GNNs and has yet to be explored in jet tagging. PCN achieves a substantial improvemen
    
[^30]: 随机滤波器组的能量保持和稳定性

    Energy Preservation and Stability of Random Filterbanks. (arXiv:2309.05855v1 [cs.LG])

    [http://arxiv.org/abs/2309.05855](http://arxiv.org/abs/2309.05855)

    本文从随机卷积算子的数学角度详细阐述了简单卷积神经网络的统计属性，发现具有随机高斯权重的FIR滤波器组在大滤波器和局部周期输入信号的情况下是病态的，并推导了其期望帧边界的理论界限。

    

    波形为基础的深度学习为什么如此困难？尽管有多次尝试训练卷积神经网络(convnets)进行滤波器设计，但它们往往无法超越手工创建的基线。这更令人惊讶，因为这些基线是线性时不变系统：因此，它们的传递函数可以通过具有大感受野的卷积神经网络准确表示。在本文中，我们从随机卷积算子的数学角度详细阐述了简单卷积神经网络的统计属性。我们发现，具有随机高斯权重的FIR滤波器组在大滤波器和局部周期输入信号的情况下是病态的，这在音频信号处理应用中是典型的。此外，我们观察到随机滤波器组的期望能量保持对于数值稳定性是不足够的，并推导了其期望帧边界的理论界限。

    What makes waveform-based deep learning so hard? Despite numerous attempts at training convolutional neural networks (convnets) for filterbank design, they often fail to outperform hand-crafted baselines. This is all the more surprising because these baselines are linear time-invariant systems: as such, their transfer functions could be accurately represented by a convnet with a large receptive field. In this article, we elaborate on the statistical properties of simple convnets from the mathematical perspective of random convolutional operators. We find that FIR filterbanks with random Gaussian weights are ill-conditioned for large filters and locally periodic input signals, which both are typical in audio signal processing applications. Furthermore, we observe that expected energy preservation of a random filterbank is not sufficient for numerical stability and derive theoretical bounds for its expected frame bounds.
    
[^31]: 表情符号促进了GitHub上开发者的参与和问题解决

    Emoji Promotes Developer Participation and Issue Resolution on GitHub. (arXiv:2308.16360v1 [cs.CY])

    [http://arxiv.org/abs/2308.16360](http://arxiv.org/abs/2308.16360)

    本研究探讨了表情符号在虚拟工作空间中的使用对开发者参与和问题解决的影响。研究发现，表情符号可以显著缩短问题的解决时间，并吸引更多用户参与。不同类型问题对表情符号的效果也存在差异。

    

    尽管在疫情期间远程工作越来越普遍，但许多人对远程工作的低效率表示担忧。在基于文本的沟通中缺乏面部表情和肢体语言等非语言线索，这妨碍了有效的沟通，并对工作结果产生负面影响。作为替代的非语言线索，在社交媒体平台上广泛使用的表情符号在虚拟工作空间中也越来越受欢迎。本文研究了表情符号的使用如何影响虚拟工作空间中开发者的参与和问题解决。为此，我们收集了GitHub的一个一年周期内的问题，并应用因果推断技术来衡量表情符号对问题结果的因果效应，控制问题内容、仓库和作者信息等混淆因素。我们发现表情符号可以显著缩短问题的解决时间，并吸引更多用户参与。我们还比较了不同类型问题的异质效应。

    Although remote working is increasingly adopted during the pandemic, many are concerned by the low-efficiency in the remote working. Missing in text-based communication are non-verbal cues such as facial expressions and body language, which hinders the effective communication and negatively impacts the work outcomes. Prevalent on social media platforms, emojis, as alternative non-verbal cues, are gaining popularity in the virtual workspaces well. In this paper, we study how emoji usage influences developer participation and issue resolution in virtual workspaces. To this end, we collect GitHub issues for a one-year period and apply causal inference techniques to measure the causal effect of emojis on the outcome of issues, controlling for confounders such as issue content, repository, and author information. We find that emojis can significantly reduce the resolution time of issues and attract more user participation. We also compare the heterogeneous effect on different types of issue
    
[^32]: 小学习率随机梯度下降中动量的边际价值

    The Marginal Value of Momentum for Small Learning Rate SGD. (arXiv:2307.15196v1 [cs.LG])

    [http://arxiv.org/abs/2307.15196](http://arxiv.org/abs/2307.15196)

    本文通过理论分析和实验证明，在小学习率和梯度噪声是主要不稳定源的随机环境中，动量在深度学习优化中的边际价值是有限的。

    

    在没有随机梯度噪声的强凸环境中，动量已被证明能加速梯度下降的收敛。在随机优化中，如训练神经网络，有传言认为动量可以通过减小随机梯度更新的方差来帮助深度学习优化，但之前的理论分析并没有发现动量可以提供任何可证实的加速。本文的理论结果阐明了在学习率较小且梯度噪声是主要不稳定源的随机环境中动量的作用，表明使用和不使用动量的随机梯度下降在短期和长期时间段内表现相似。实验证明，在实际训练中，动量在优化和泛化方面确实有局限的益处，特别是在学习率不是很大的情况下，包括在ImageNet上从头训练小至中等批次大小的模型和在下游任务上微调语言模型。

    Momentum is known to accelerate the convergence of gradient descent in strongly convex settings without stochastic gradient noise. In stochastic optimization, such as training neural networks, folklore suggests that momentum may help deep learning optimization by reducing the variance of the stochastic gradient update, but previous theoretical analyses do not find momentum to offer any provable acceleration. Theoretical results in this paper clarify the role of momentum in stochastic settings where the learning rate is small and gradient noise is the dominant source of instability, suggesting that SGD with and without momentum behave similarly in the short and long time horizons. Experiments show that momentum indeed has limited benefits for both optimization and generalization in practical training regimes where the optimal learning rate is not very large, including small- to medium-batch training from scratch on ImageNet and fine-tuning language models on downstream tasks.
    
[^33]: WebArena: 一个用于构建自主智能体的真实网络环境

    WebArena: A Realistic Web Environment for Building Autonomous Agents. (arXiv:2307.13854v1 [cs.AI])

    [http://arxiv.org/abs/2307.13854](http://arxiv.org/abs/2307.13854)

    WebArena是一个用于构建自主智能体的真实网络环境，它包含了完全功能的网站，并且通过引入工具和外部知识库来鼓励智能体像人类一样解决任务。此外，WebArena还发布了一组用于评估任务完成功能正确性的基准任务。

    

    随着生成式人工智能的进展，通过自然语言指令进行日常任务的自主智能体的潜力逐渐显现。然而，当前的智能体主要是在简化的合成环境中创建和测试的，严重限制了现实世界场景的表示能力。在本文中，我们构建了一个高度逼真且可复现的智能体指令和控制环境。具体而言，我们关注在网站上执行任务的智能体，我们创建了一个包含来自四个常见领域的完全功能网站的环境，分别是电子商务、社交论坛讨论、协同软件开发和内容管理。我们的环境使用工具（如地图）和外部知识库（如用户手册）来鼓励像人类一样解决任务。在我们的环境基础上，我们发布了一组重点评估任务完成功能正确性的基准任务。我们基准任务具有多样性和长远的视野，并且被设计为鼓励智能体进行更深层次的任务理解和解决。

    With generative AI advances, the exciting potential for autonomous agents to manage daily tasks via natural language commands has emerged. However, cur rent agents are primarily created and tested in simplified synthetic environments, substantially limiting real-world scenario representation. In this paper, we build an environment for agent command and control that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on websites, and we create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and are desi
    
[^34]: 具有逐渐变化的通用在线学习：一种多层在线集成方法

    Universal Online Learning with Gradual Variations: A Multi-layer Online Ensemble Approach. (arXiv:2307.08360v1 [cs.LG])

    [http://arxiv.org/abs/2307.08360](http://arxiv.org/abs/2307.08360)

    该论文提出了一种具有两个不同级别自适应性的在线凸优化方法，对不同类型的损失函数具有多种遗憾界，并在分析中直接应用于小损失界。同时，它与对抗性/随机凸优化和博弈论有着深刻的联系。

    

    在本文中，我们提出了一种具有两个不同级别自适应性的在线凸优化方法。在更高级别上，我们的方法对损失函数的具体类型和曲率不知情，而在更低级别上，它可以利用环境的良好性质并获得问题相关保证。具体而言，对于强凸、指数凹和凸损失函数，我们分别获得了$O(\ln V_T)$、$O(d \ln V_T)$和$\hat{O}(\sqrt{V_T})$的遗憾界，其中$d$是维度，$V_T$表示问题相关的梯度变化，$\hat{O}(\cdot)$表示在$V_T$上省略对数因子。我们的结果具有广泛的影响和应用。它不仅保证了最坏情况下的性能，还直接导出了分析中的小损失界。此外，它与对抗性/随机凸优化和博弈论有着深刻的联系，进一步验证了其实际潜力。我们的方法基于...

    In this paper, we propose an online convex optimization method with two different levels of adaptivity. On a higher level, our method is agnostic to the specific type and curvature of the loss functions, while at a lower level, it can exploit the niceness of the environments and attain problem-dependent guarantees. To be specific, we obtain $\mathcal{O}(\ln V_T)$, $\mathcal{O}(d \ln V_T)$ and $\hat{\mathcal{O}}(\sqrt{V_T})$ regret bounds for strongly convex, exp-concave and convex loss functions, respectively, where $d$ is the dimension, $V_T$ denotes problem-dependent gradient variations and $\hat{\mathcal{O}}(\cdot)$-notation omits logarithmic factors on $V_T$. Our result finds broad implications and applications. It not only safeguards the worst-case guarantees, but also implies the small-loss bounds in analysis directly. Besides, it draws deep connections with adversarial/stochastic convex optimization and game theory, further validating its practical potential. Our method is based
    
[^35]: 域适应能提高皮肤病变分类的准确性和公平性吗？

    Can Domain Adaptation Improve Accuracy and Fairness of Skin Lesion Classification?. (arXiv:2307.03157v1 [cs.CV])

    [http://arxiv.org/abs/2307.03157](http://arxiv.org/abs/2307.03157)

    本研究利用多个皮肤病变数据集，研究了域适应方法在皮肤病变分类中的应用。结果表明，域适应在减少不平衡的情况下对于二分类任务有效，但在多分类任务中性能较差，需要解决不平衡问题以提高准确性。

    

    深度学习诊断系统在分类皮肤癌症病变时表现出潜力，但缺乏标记数据会影响准确可靠的诊断系统的发展。在本研究中，我们利用多个皮肤病变数据集，研究了各种无监督域适应方法在二分类和多分类皮肤病变分类中的可行性。尤其是，我们评估了三种域适应训练方案：单源、综合和多源。实验结果表明，域适应在二分类中是有效的，在减少不平衡的情况下还能进一步提高。在多分类任务中，其性能不太明显，需要解决不平衡问题才能达到基准准确性。通过量化分析，我们发现多分类任务的测试错误与标签偏移强烈相关。

    Deep learning-based diagnostic system has demonstrated potential in classifying skin cancer conditions when labeled training example are abundant. However, skin lesion analysis often suffers from a scarcity of labeled data, hindering the development of an accurate and reliable diagnostic system. In this work, we leverage multiple skin lesion datasets and investigate the feasibility of various unsupervised domain adaptation (UDA) methods in binary and multi-class skin lesion classification. In particular, we assess three UDA training schemes: single-, combined-, and multi-source. Our experiment results show that UDA is effective in binary classification, with further improvement being observed when imbalance is mitigated. In multi-class task, its performance is less prominent, and imbalance problem again needs to be addressed to achieve above-baseline accuracy. Through our quantitative analysis, we find that the test error of multi-class tasks is strongly correlated with label shift, an
    
[^36]: 用强化学习实现的顺序检索上下文示例的RetICL

    RetICL: Sequential Retrieval of In-Context Examples with Reinforcement Learning. (arXiv:2305.14502v1 [cs.CL])

    [http://arxiv.org/abs/2305.14502](http://arxiv.org/abs/2305.14502)

    本文提出了一种 RetICL 的方法来优化地选择用于上下文学习模型的示例。此方法利用强化学习框架将序列示例选择问题作为马尔科夫决策过程，并且优化选择为使任务表现最佳的组合。

    

    最近在大语言模型领域中的许多发展都集中在促使它们执行特定任务。一种有效的提示方法是上下文学习，其中模型在给定一个（或多个）示例的情况下执行（可能是新的）生成/预测任务。先前的工作表明，示例的选择可能对任务的表现产生很大的影响。然而，找到好的示例并不是简单的，因为代表性示例组的定义可以根据任务的不同而大不相同。虽然存在许多选择上下文示例的现有方法，但它们通常独立地对示例进行评分，忽略它们之间的依赖关系以及向大型语言模型提供示例的顺序。在这项工作中，我们提出了一种可学习的方法——In-Context Learning的检索RetICL，用于建模和逐步选择上下文示例。我们把顺序示例选择的问题作为马尔科夫决策过程，设计了一个示例。

    Many recent developments in large language models focus on prompting them to perform specific tasks. One effective prompting method is in-context learning, where the model performs a (possibly new) generation/prediction task given one (or more) examples. Past work has shown that the choice of examples can make a large impact on task performance. However, finding good examples is not straightforward since the definition of a representative group of examples can vary greatly depending on the task. While there are many existing methods for selecting in-context examples, they generally score examples independently, ignoring the dependency between them and the order in which they are provided to the large language model. In this work, we propose Retrieval for In-Context Learning (RetICL), a learnable method for modeling and optimally selecting examples sequentially for in-context learning. We frame the problem of sequential example selection as a Markov decision process, design an example r
    
[^37]: 利用梯度衡量数据选择和估价在差分隐私训练中的应用

    Leveraging gradient-derived metrics for data selection and valuation in differentially private training. (arXiv:2305.02942v1 [cs.LG])

    [http://arxiv.org/abs/2305.02942](http://arxiv.org/abs/2305.02942)

    研究如何在隐私增强技术下，利用梯度信息识别训练中感兴趣的数据样本进行数据选择和估价。

    

    由于监管担忧和参与度的不足，为机器学习模型进行协作训练获取高质量数据可能是一项具有挑战性的任务。隐私增强技术（PET）是解决监管问题的一种常用方法，差分隐私（DP）训练是其中最常用的一种方法。本文研究了如何使用梯度信息来识别隐私训练中感兴趣的训练样本。我们展示了在最严格的隐私设置中，存在着能够为客户提供有原则的数据选择工具的技术。

    Obtaining high-quality data for collaborative training of machine learning models can be a challenging task due to A) the regulatory concerns and B) lack of incentive to participate. The first issue can be addressed through the use of privacy enhancing technologies (PET), one of the most frequently used one being differentially private (DP) training. The second challenge can be addressed by identifying which data points can be beneficial for model training and rewarding data owners for sharing this data. However, DP in deep learning typically adversely affects atypical (often informative) data samples, making it difficult to assess the usefulness of individual contributions. In this work we investigate how to leverage gradient information to identify training samples of interest in private training settings. We show that there exist techniques which are able to provide the clients with the tools for principled data selection even in strictest privacy settings.
    
[^38]: 面向未知具有潜在状态系统的学习优化控制方法

    Learning-Based Optimal Control with Performance Guarantees for Unknown Systems with Latent States. (arXiv:2303.17963v1 [eess.SY])

    [http://arxiv.org/abs/2303.17963](http://arxiv.org/abs/2303.17963)

    本文提出了一种面向未知具有潜在状态系统的学习优化控制方法，并给出了概率性能保证，同时提出了一种验证任意控制律性能的方法。

    

    随着控制工程方法应用于越来越复杂的系统，数据驱动的系统辨识方法成为物理建模的有希望的替代方法。然而，许多这些方法依赖于状态测量的可用性，而复杂系统的状态通常不是直接可测量的。因此，可能需要同时估计动力学和潜在状态，从而更加具有挑战性地设计具有性能保证的控制器。本文提出了一种新方法，用于计算具有潜在状态的未知非线性系统的最优输入轨迹。对结果输入轨迹进行了概率性能保证，并提出了一种验证任意控制律性能的方法。本文在数值模拟中展示了所提出方法的有效性。

    As control engineering methods are applied to increasingly complex systems, data-driven approaches for system identification appear as a promising alternative to physics-based modeling. While many of these approaches rely on the availability of state measurements, the states of a complex system are often not directly measurable. It may then be necessary to jointly estimate the dynamics and a latent state, making it considerably more challenging to design controllers with performance guarantees. This paper proposes a novel method for the computation of an optimal input trajectory for unknown nonlinear systems with latent states. Probabilistic performance guarantees are derived for the resulting input trajectory, and an approach to validate the performance of arbitrary control laws is presented. The effectiveness of the proposed method is demonstrated in a numerical simulation.
    
[^39]: 用稀疏输入控制高维数据

    Controlling High-Dimensional Data With Sparse Input. (arXiv:2303.09446v1 [eess.AS])

    [http://arxiv.org/abs/2303.09446](http://arxiv.org/abs/2303.09446)

    本论文提出了一种新的控制机制，即将稀疏、易于人理解的控制空间映射到生成模型的潜在空间。通过实验，此方法表现出了效率、鲁棒性和保真性，即使只有少量的输入数值。

    

    本论文解决了人在环路控制生成高度结构化数据的问题。由于现有的生成模型缺乏有效的接口，使得用户可以修改输出，这个任务变得具有挑战性。用户或手动探索不可解释的潜在空间，或者费力地注释数据标签。为了解决这个问题，我们引入了一个新的框架，其中编码器将稀疏、易于人理解的控制空间映射到生成模型的潜在空间。我们将这个框架应用于控制文本转语音合成中的韵律的任务。我们提出了一个模型，称为多实例条件变分自编码器(MICVAE)，它专门设计用于编码稀疏的韵律特征并输出完整的波形。我们通过实验证明，MICVAE表现出了稀疏的人在环路控制机制所需的良好品质：效率、鲁棒性和保真性。即使只有非常少量的输入数值(~4)，MICVAE也能让用户实现控制。

    We address the problem of human-in-the-loop control for generating highly-structured data. This task is challenging because existing generative models lack an efficient interface through which users can modify the output. Users have the option to either manually explore a non-interpretable latent space, or to laboriously annotate the data with conditioning labels. To solve this, we introduce a novel framework whereby an encoder maps a sparse, human interpretable control space onto the latent space of a generative model. We apply this framework to the task of controlling prosody in text-to-speech synthesis. We propose a model, called Multiple-Instance CVAE (MICVAE), that is specifically designed to encode sparse prosodic features and output complete waveforms. We show empirically that MICVAE displays desirable qualities of a sparse human-in-the-loop control mechanism: efficiency, robustness, and faithfulness. With even a very small number of input values (~4), MICVAE enables users to im
    
[^40]: 偶然性和认知性歧视：公平干预的基本限制

    Aleatoric and Epistemic Discrimination: Fundamental Limits of Fairness Interventions. (arXiv:2301.11781v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11781](http://arxiv.org/abs/2301.11781)

    该论文研究了机器学习模型中的偶然性和认知性歧视，将其分类为数据分布中固有的歧视和模型开发过程中的决策导致的歧视。通过量化偶然性歧视的性能限制和刻画认知性歧视，揭示了公平干预的基本限制。研究还应用这种方法评估了现有的公平干预措施，并探究了在存在缺失值的数据中的公平风险。

    

    机器学习模型在某些人群中可能表现不佳，原因是在模型开发过程中做出的选择和数据中固有的偏见。我们将机器学习流程中的歧视来源分为两类：偶然性歧视，即数据分布中固有的歧视，和认知性歧视，即模型开发过程中做出的决策导致的歧视。我们通过确定在完全了解数据分布的情况下，在公平约束下模型的性能限制来量化偶然性歧视。我们通过应用布莱克韦尔对比统计实验的结果来刻画偶然性歧视。然后，我们将认知性歧视定义为在应用公平约束时模型的准确性与偶然性歧视所限定的界限之间的差距。我们将这种方法应用于评估现有的公平干预措施，并调查具有缺失值的数据中的公平风险。我们的结果表明...

    Machine learning (ML) models can underperform on certain population groups due to choices made during model development and bias inherent in the data. We categorize sources of discrimination in the ML pipeline into two classes: aleatoric discrimination, which is inherent in the data distribution, and epistemic discrimination, which is due to decisions made during model development. We quantify aleatoric discrimination by determining the performance limits of a model under fairness constraints, assuming perfect knowledge of the data distribution. We demonstrate how to characterize aleatoric discrimination by applying Blackwell's results on comparing statistical experiments. We then quantify epistemic discrimination as the gap between a model's accuracy when fairness constraints are applied and the limit posed by aleatoric discrimination. We apply this approach to benchmark existing fairness interventions and investigate fairness risks in data with missing values. Our results indicate th
    
[^41]: 无接触红外光波传感的呼吸异常检测

    Non-contact Respiratory Anomaly Detection using Infrared Light-wave Sensing. (arXiv:2301.03713v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2301.03713](http://arxiv.org/abs/2301.03713)

    本研究使用无接触红外光波传感技术，通过训练不同类型的呼吸模式来检测呼吸异常，并且通过验证数据的呼吸波形丢弃干扰数据，以实现安全、高效和无创的人体呼吸监测。

    

    人体的呼吸频率和呼吸模式传达了关于主体的身体和心理状态的重要信息。异常呼吸可能表明严重的健康问题，需要进一步诊断和治疗。使用非相干红外光的无线光波传感（LWS）在不引起隐私问题的情况下，显示了安全、隐蔽、高效和无创的人体呼吸监测的潜力。呼吸监测系统需要在不同类型的呼吸模式上进行训练，以识别呼吸异常。该系统还必须验证所收集的数据是否为呼吸波形，丢弃由外部干扰、用户移动或系统故障引起的任何错误数据。为了解决这些需求，本研究使用模拟人类呼吸模式的机器人，模拟了正常和不同类型的异常呼吸。然后，使用红外光波传感技术收集了时间序列呼吸数据。在此基础上，用三种机器学习算法进行了呼吸异常检测。

    Human respiratory rate and its pattern convey essential information about the physical and psychological states of the subject. Abnormal breathing can indicate fatal health issues leading to further diagnosis and treatment. Wireless light-wave sensing (LWS) using incoherent infrared light shows promise in safe, discreet, efficient, and non-invasive human breathing monitoring without raising privacy concerns. The respiration monitoring system needs to be trained on different types of breathing patterns to identify breathing anomalies.The system must also validate the collected data as a breathing waveform, discarding any faulty data caused by external interruption, user movement, or system malfunction. To address these needs, this study simulated normal and different types of abnormal respiration using a robot that mimics human breathing patterns. Then, time-series respiration data were collected using infrared light-wave sensing technology. Three machine learning algorithms, decision t
    
[^42]: 确切的流形高斯变分贝叶斯

    Exact Manifold Gaussian Variational Bayes. (arXiv:2210.14598v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.14598](http://arxiv.org/abs/2210.14598)

    我们提出了一种在复杂模型中进行变分推断的优化算法，通过使用自然梯度更新和黎曼流形，我们开发了一种高效的高斯变分推断算法，并验证了其在多个数据集上的性能。

    

    我们提出了一种用于复杂模型中变分推断（VI）的优化算法。我们的方法依赖于自然梯度更新，其中变分空间是一个黎曼流形。我们开发了一个高效的高斯变分推断算法，以隐式满足变分协方差矩阵的正定约束。我们的确切流形高斯变分贝叶斯（EMGVB）提供了精确但简单的更新规则，并且易于实现。由于其黑盒性质，EMGVB成为复杂模型中即插即用的解决方案。通过在不同统计、计量和深度学习模型上使用五个数据集，我们对我们的可行性方法进行了实证验证，并与基准方法进行了性能讨论。

    We propose an optimization algorithm for Variational Inference (VI) in complex models. Our approach relies on natural gradient updates where the variational space is a Riemann manifold. We develop an efficient algorithm for Gaussian Variational Inference that implicitly satisfies the positive definite constraint on the variational covariance matrix. Our Exact manifold Gaussian Variational Bayes (EMGVB) provides exact but simple update rules and is straightforward to implement. Due to its black-box nature, EMGVB stands as a ready-to-use solution for VI in complex models. Over five datasets, we empirically validate our feasible approach on different statistical, econometric, and deep learning models, discussing its performance with respect to baseline methods.
    
[^43]: 用于节点分类的图形数据集的特征化：同质性-异质性二分法及其延伸

    Characterizing Graph Datasets for Node Classification: Homophily-Heterophily Dichotomy and Beyond. (arXiv:2209.06177v3 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2209.06177](http://arxiv.org/abs/2209.06177)

    本论文研究了用于节点分类的图形数据集的特征化，发现目前常用的同质性测量方法存在严重缺陷，无法比较不同数据集中的同质性水平。作者提出了一种新的同质性测量指标，称为调整同质性，该指标满足更多期望特性，并具有较少在图形机器学习文献中使用的特点。

    

    同质性是描述边连接相似节点倾向的图形属性；相反的概念为异质性。人们通常认为异质性图对于标准的消息传递图神经网络（GNN）是具有挑战性的，并且已经付出了许多努力来开发这种情况下的高效方法。然而，目前在文献中没有普遍被接受的同质性测量指标。在这项工作中，我们展示了常用的同质性测量方法存在严重缺陷，无法比较不同数据集中的同质性水平。为此，我们为正确的同质性测量指标形式化了期望的特性，并验证了哪些指标满足哪些特性。特别是，我们发现一种我们称之为调整同质性的指标满足比其他流行同质性测量方法更多的期望特性，而在图形机器学习文献中很少被使用。然后，我们超越了同质性-异质性二分法，提出了一种新的特征，使得...

    Homophily is a graph property describing the tendency of edges to connect similar nodes; the opposite is called heterophily. It is often believed that heterophilous graphs are challenging for standard message-passing graph neural networks (GNNs), and much effort has been put into developing efficient methods for this setting. However, there is no universally agreed-upon measure of homophily in the literature. In this work, we show that commonly used homophily measures have critical drawbacks preventing the comparison of homophily levels across different datasets. For this, we formalize desirable properties for a proper homophily measure and verify which measures satisfy which properties. In particular, we show that a measure that we call adjusted homophily satisfies more desirable properties than other popular homophily measures while being rarely used in graph machine learning literature. Then, we go beyond the homophily-heterophily dichotomy and propose a new characteristic that allo
    
[^44]: 分析通过预测函数的Lipschitz率来评估解释器的稳健性

    Analyzing Explainer Robustness via Lipschitzness of Prediction Functions. (arXiv:2206.12481v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.12481](http://arxiv.org/abs/2206.12481)

    本文研究了通过评估预测函数的Lipschitz率来分析解释器的稳健性。通过引入解释器敏锐性的概念并与预测器的概率Lipschitz率相联系，我们提供了解释器敏锐性的下界保证。

    

    机器学习方法在预测能力方面得到了显著提高，但与此同时，它们变得越来越复杂和不透明。因此，解释器总是被依赖于为这些黑盒预测模型提供可解释性。作为关键的诊断工具，重要的是这些解释器本身是稳健的。本文重点关注稳健性的一个特定方面，即解释器在相似的数据输入上应该给出类似的解释。我们通过引入和定义解释器敏锐性的概念来形式化这个观念，类似于预测函数的敏锐性。我们的形式化方法使我们能够将解释器的稳健性与预测器的概率Lipschitz率相联系，该率捕捉了函数局部平滑性的概率。我们根据预测函数的Lipschitz率提供对各种解释器（如SHAP，RISE，CXPlain）的敏锐性的下界保证。这些理论结果暗示了对于具有局部光滑性预测函数的解释器敏锐性。

    Machine learning methods have significantly improved in their predictive capabilities, but at the same time they are becoming more complex and less transparent. As a result, explainers are often relied on to provide interpretability to these black-box prediction models. As crucial diagnostics tools, it is important that these explainers themselves are robust. In this paper we focus on one particular aspect of robustness, namely that an explainer should give similar explanations for similar data inputs. We formalize this notion by introducing and defining explainer astuteness, analogous to astuteness of prediction functions. Our formalism allows us to connect explainer robustness to the predictor's probabilistic Lipschitzness, which captures the probability of local smoothness of a function. We provide lower bound guarantees on the astuteness of a variety of explainers (e.g., SHAP, RISE, CXPlain) given the Lipschitzness of the prediction function. These theoretical results imply that lo
    

