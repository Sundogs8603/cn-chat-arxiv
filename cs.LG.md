# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Syntactic vs Semantic Linear Abstraction and Refinement of Neural Networks.](http://arxiv.org/abs/2307.10891) | 这项工作提供了一个灵活的框架，通过线性组合替换神经元，实现了神经网络的句法和语义抽象，并引入了一种改进方法来平衡减少和精确度。 |
| [^2] | [Player-optimal Stable Regret for Bandit Learning in Matching Markets.](http://arxiv.org/abs/2307.10890) | 在匹配市场中，通过在线学习，玩家可以实现玩家最优稳定匹配，从而最大化他们的利润。 |
| [^3] | [Risk-optimized Outlier Removal for Robust Point Cloud Classification.](http://arxiv.org/abs/2307.10875) | 提出了一种面向稳健点云分类的风险优化异常值去除方法，利用普通训练的模型消除额外的异常值并恢复数据。方法通过归因分析确定每个点对模型输出的影响，使用条件风险价值优化高风险点的过滤过程。该方法在不需要额外训练的情况下能够产生出色的结果。 |
| [^4] | [Nonlinear Meta-Learning Can Guarantee Faster Rates.](http://arxiv.org/abs/2307.10870) | 非线性元学习可以保证更快的收敛速度。 |
| [^5] | [Performance Issue Identification in Cloud Systems with Relational-Temporal Anomaly Detection.](http://arxiv.org/abs/2307.10869) | 云系统中的性能问题识别存在挑战，现有方法中的独立分析每个指标的异常无法解决问题，需要考虑指标之间的关联性。 |
| [^6] | [FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback.](http://arxiv.org/abs/2307.10867) | FigCaps-HF是一个图像生成标题的框架，可以通过融入领域专家的反馈意见，生成符合读者偏好的高质量图像标题。将自动评估和强化学习与人类反馈相结合，可以改善生成的标题与读者偏好的一致性。 |
| [^7] | [Addressing caveats of neural persistence with deep graph persistence.](http://arxiv.org/abs/2307.10865) | 本文发现网络权重的方差和大权重的空间集中是影响神经持久性的主要因素，并提出了将神经持久性扩展到整个神经网络的深度图持久性测量方法。 |
| [^8] | [Divide & Bind Your Attention for Improved Generative Semantic Nursing.](http://arxiv.org/abs/2307.10864) | 本论文提出了一种名为"分割与绑定"的方法，旨在改进生成语义护理的效果。该方法引入了新的损失目标，包括关注丢失和绑定丢失，以解决复杂提示和不适当属性绑定的问题。 |
| [^9] | [Self-paced Weight Consolidation for Continual Learning.](http://arxiv.org/abs/2307.10845) | 提出了一种自适应权重巩固（spWC）框架，通过评估先前任务的区分性贡献，实现稳健的持续学习。具体而言，通过自适应正则化方法，根据关键绩效指标（即准确度）来衡量难度，反映过去任务的优先级，并对先前任务进行排序。 |
| [^10] | [Global Precipitation Nowcasting of Integrated Multi-satellitE Retrievals for GPM: A U-Net Convolutional LSTM Architecture.](http://arxiv.org/abs/2307.10843) | 本文提出了一种基于U-Net和卷积LSTM的深度学习架构，能够准确预测全球降水的即时情况，尤其在极端降水方面的预测准确率更高。 |
| [^11] | [Label Calibration for Semantic Segmentation Under Domain Shift.](http://arxiv.org/abs/2307.10842) | 该论文介绍了一种在域偏移下使用标签校准方法来提升语义分割模型性能的方式。 |
| [^12] | [On Combining Expert Demonstrations in Imitation Learning via Optimal Transport.](http://arxiv.org/abs/2307.10810) | 通过最优传输方法结合多个专家示范的新方法，在模仿学习中提供了更合理的示范几何平均值。 |
| [^13] | [Communication-Efficient Split Learning via Adaptive Feature-Wise Compression.](http://arxiv.org/abs/2307.10805) | 该论文提出了一个名为SplitFC的通信高效的分割学习框架，通过两种自适应压缩策略来减少中间特征和梯度向量的通信开销，这些策略分别是自适应特征逐渐掉落和自适应特征逐渐量化。 |
| [^14] | [Spatial-Temporal Data Mining for Ocean Science: Data, Methodologies, and Opportunities.](http://arxiv.org/abs/2307.10803) | 该研究总结了海洋科学领域的时空数据挖掘研究，包括广泛使用的ST海洋数据集和其特点，以及ST海洋数据质量增强技术和现有STDM分类。 |
| [^15] | [Meta-Transformer: A Unified Framework for Multimodal Learning.](http://arxiv.org/abs/2307.10802) | Meta-Transformer是一个统一的多模态学习框架，利用一个冻结的编码器进行多模态感知，在没有成对多模态训练数据的情况下可以处理各种模态，并且能够提取输入数据的高级语义特征。 |
| [^16] | [Optimizing PatchCore for Few/many-shot Anomaly Detection.](http://arxiv.org/abs/2307.10792) | 本文研究了在少量样本和大量样本设置下，PatchCore算法在异常检测/异常分割性能上的表现，并发现通过优化超参数和借鉴少量样本监督学习技术可以进一步提高性能。 |
| [^17] | [Adversarial attacks for mixtures of classifiers.](http://arxiv.org/abs/2307.10788) | 本文讨论了攻击混合分类器的问题，并引入了两个可取性质。我们证明了现有的攻击方法不能同时满足这两个性质。最后，我们介绍了一种新的攻击方法，并通过实验证明了其性能好。 |
| [^18] | [Feed-Forward Source-Free Domain Adaptation via Class Prototypes.](http://arxiv.org/abs/2307.10787) | 这项工作提出了一种前馈方法来解决源无关域适应问题，通过计算类的原型来处理域偏移，相较于使用预训练模型，该方法在精度和时间上都有显著提升。 |
| [^19] | [Efficient Beam Tree Recursion.](http://arxiv.org/abs/2307.10779) | 本文提出了一种高效的Beam Tree递归算法（BT-RvNN），通过解决评分函数和递归单元函数的纠缠问题以及简化内存使用，成功降低了BT-RvNN的内存使用。这个算法在ListOps任务中达到了新的最先进水平，并在其他任务中保持了类似的性能。 |
| [^20] | [Assessing the Use of AutoML for Data-Driven Software Engineering.](http://arxiv.org/abs/2307.10774) | AutoML作为一种自动化构建端到端AI/ML流水线的解决方案被广泛关注，但目前对其在开发AI/ML系统的团队中的采用程度和感知程度缺乏信息。 |
| [^21] | [Music Genre Classification with ResNet and Bi-GRU Using Visual Spectrograms.](http://arxiv.org/abs/2307.10773) | 该论文提出了一种使用ResNet和Bi-GRU结构，并利用视觉频谱图进行音乐流派分类的方法，以提高音乐推荐系统的性能。 |
| [^22] | [Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory.](http://arxiv.org/abs/2307.10768) | 本论文介绍了一个全面的工作记忆基准数据集（WorM），通过评估4个功能、3个领域和11个行为和神经特征的WM任务来开发和评估AI WM模型。结果表明，AI模型能够模拟出脑中工作记忆的一些特征，如优势效应和最新性效应，以及专门用于不同领域和功能的工作记忆的神经群集和相关物。 |
| [^23] | [MSQNet: Actor-agnostic Action Recognition with Multi-modal Query.](http://arxiv.org/abs/2307.10763) | MSQNet是一种无关演员的多模态多标签动作识别方法，通过使用视觉和文本模态来更好地表示动作类别，克服了现有方法中针对特定演员的限制。 |
| [^24] | [Mitigating Voter Attribute Bias for Fair Opinion Aggregation.](http://arxiv.org/abs/2307.10749) | 本文研究了通过考虑选民属性来实现公正的意见汇总的方法，并评估了汇总结果的公正性。 |
| [^25] | [Fairness-Aware Client Selection for Federated Learning.](http://arxiv.org/abs/2307.10738) | 提出了公平感知的联邦客户端选择（FairFedCS）方法，通过动态调整联邦学习客户端的选择概率，同时考虑客户端的声誉、参与次数和对模型性能的贡献，解决了平衡性能和公平性的问题。 |
| [^26] | [Long-Tail Theory under Gaussian Mixtures.](http://arxiv.org/abs/2307.10736) | 该论文提出了一个简单的高斯混合模型，符合Feldman的长尾理论。通过实验证明，在长尾分布情况下，非线性分类器可以提高泛化能力，而线性分类器不能。该结果强调了对于长尾分布，需要考虑罕见的训练样本以实现最佳泛化能力。 |
| [^27] | [Differences Between Hard and Noisy-labeled Samples: An Empirical Study.](http://arxiv.org/abs/2307.10718) | 本文通过实证研究探讨了困难样本和噪声样本之间的区别，并提出一种简单而有效的度量标准，可以区分并过滤掉噪声标记的样本。 |
| [^28] | [Reparameterized Policy Learning for Multimodal Trajectory Optimization.](http://arxiv.org/abs/2307.10710) | 本研究针对高维连续动作空间中的增强学习，提出了一种多模态策略参数化的重参数化框架，并基于此框架提出了一种重参数化策略梯度方法，能够在任务中避免局部最优，解决稀疏奖励环境中的挑战。 |
| [^29] | [TwinLiteNet: An Efficient and Lightweight Model for Driveable Area and Lane Segmentation in Self-Driving Cars.](http://arxiv.org/abs/2307.10705) | 本文提出了一种轻量级模型TwinLiteNet，用于自动驾驶车辆中的可驱动区域和车道分割。该模型成本低廉且高效准确，并在实验中表现出明显的计算资源节约。 |
| [^30] | [Decentralized Smart Charging of Large-Scale EVs using Adaptive Multi-Agent Multi-Armed Bandits.](http://arxiv.org/abs/2307.10704) | 该论文提出了一个分散式智能充电系统，利用自适应多智能体系统的方法进行控制，并采用多臂赌博学习处理不确定性。该系统具有可扩展性、实时性、无模型性，并且考虑了不同参与者之间的公平性。 |
| [^31] | [Graphs in State-Space Models for Granger Causality in Climate Science.](http://arxiv.org/abs/2307.10703) | 该论文提出了使用状态空间模型中的图形进行Granger因果性分析的方法，通过使用GraphEM算法估计线性高斯状态空间模型中状态方程中的线性矩阵运算符，并在M步中包括Lasso正则化进行求解。通过在玩具例子和具有挑战性的气候问题上进行实验证明了该方法相对于标准的Granger因果性方法的优势。 |
| [^32] | [Self2Self+: Single-Image Denoising with Self-Supervised Learning and Image Quality Assessment Loss.](http://arxiv.org/abs/2307.10695) | 该论文提出了一种新的单图像去噪方法，利用自监督学习和图像质量评估损失，只使用噪声输入进行网络训练，并通过平均不同实例的预测结果提高性能。 |
| [^33] | [Fractional Denoising for 3D Molecular Pre-training.](http://arxiv.org/abs/2307.10683) | 本论文提出了一种分数降噪算法，用于3D分子预训练。通过混合噪声策略解决了样本覆盖率低和各向同性力场的挑战，通过解耦两种类型的噪声克服了传统降噪方法无法学习力场的问题。 |
| [^34] | [Deep learning for classification of noisy QR codes.](http://arxiv.org/abs/2307.10677) | 该论文研究了在应用于抽象图像分类时，基于深度学习的经典分类模型的限制。通过将该模型应用于噪声二维码的分类，研究者发现深度学习模型在理解抽象图像方面具有相关性。 |
| [^35] | [A Survey of What to Share in Federated Learning: Perspectives on Model Utility, Privacy Leakage, and Communication Efficiency.](http://arxiv.org/abs/2307.10655) | 本文介绍了一篇系统综述，从新的视角探讨了在联邦学习中应该分享什么，注重模型效用、隐私泄露和通信效率。 |
| [^36] | [Conditional expectation network for SHAP.](http://arxiv.org/abs/2307.10654) | 这项工作提出了一种用于计算条件版本的（代理）神经网络方法，该方法可以有效地解释神经网络和其他回归模型的预测结果，并考虑特征之间的依赖关系。同时，该方法还可以应用于复杂回归模型的分析，并提供正确的偏依赖图表示。 |
| [^37] | [Refining the Optimization Target for Automatic Univariate Time Series Anomaly Detection in Monitoring Services.](http://arxiv.org/abs/2307.10653) | 本文提出了一个全面的框架，用于时间序列异常检测模型中自动参数优化。框架引入了三个优化目标：预测得分、形状得分和敏感度得分，并成功在线应用了六个月以上，每分钟为超过50,000个时间序列提供服务。该框架简化了用户体验，提供了用户友好的界面，并实现了期望的检测结果。 |
| [^38] | [Data-Driven Latency Probability Prediction for Wireless Networks: Focusing on Tail Probabilities.](http://arxiv.org/abs/2307.10648) | 本研究提出了基于数据驱动方法的无线网络延迟概率预测，重点关注尾部概率分布。该方法可用于准确估计罕见延迟的可能性，并制定网络资源配置策略。 |
| [^39] | [Fisher-Rao distance and pullback SPD cone distances between multivariate normal distributions.](http://arxiv.org/abs/2307.10644) | 本研究提出了一种快速和鲁棒的方法来近似计算多元正态分布之间的Fisher-Rao距离，并引入了一类基于正态流形嵌入到高维对称正定锥子流形的距离。 |
| [^40] | [SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models.](http://arxiv.org/abs/2307.10635) | 这篇论文介绍了一个名为SciBench的基准套件，旨在对大型语言模型的大学水平科学问题解决能力进行评估。研究结果显示，当前的语言模型在提供复杂科学问题解决能力方面还有不足之处。 |
| [^41] | [Generative Language Models on Nucleotide Sequences of Human Genes.](http://arxiv.org/abs/2307.10634) | 本研究开发了一种生成语言模型，用于处理人类基因的核苷酸序列，填补了DNA序列生成模型研究的空白。 |
| [^42] | [Multi-Method Self-Training: Improving Code Generation With Text, And Vice Versa.](http://arxiv.org/abs/2307.10633) | 多方法自训练可以通过在不同方法之间训练和生成数据来改善代码生成的性能，使模型更易于使用，并提高相关任务的性能。 |
| [^43] | [Detecting deceptive reviews using text classification.](http://arxiv.org/abs/2307.10617) | 这篇论文提出了一种使用机器学习模型的方法来识别虚假评论，并通过在餐馆评论的数据集上进行实验验证了其性能。 |
| [^44] | [Heterogeneous Federated Learning: State-of-the-art and Research Challenges.](http://arxiv.org/abs/2307.10616) | 异构联邦学习是联邦学习领域中的一个重要研究方向，涉及到数据分布、模型架构、网络环境和硬件设备的异质性挑战。本文对异构联邦学习的研究挑战和最新进展进行了综述和分类，为进一步的研究提供了参考。 |
| [^45] | [Ensemble Learning based Anomaly Detection for IoT Cybersecurity via Bayesian Hyperparameters Sensitivity Analysis.](http://arxiv.org/abs/2307.10596) | 本论文基于集成学习方法，通过贝叶斯超参数敏感性分析，提出了一种用于物联网网络安全异常检测的方法，该方法充分利用了物联网数据的异构性和多样性特征。 |
| [^46] | [Forecasting Battery Electric Vehicle Charging Behavior: A Deep Learning Approach Equipped with Micro-Clustering and SMOTE Techniques.](http://arxiv.org/abs/2307.10588) | 本研究开发了一种使用微聚类和SMOTE技术的深度学习方法，能够准确预测电动汽车充电事件，为电力负荷聚合器和电力管理人员提供提供充电站和电力容量的信息。 |
| [^47] | [A Holistic Assessment of the Reliability of Machine Learning Systems.](http://arxiv.org/abs/2307.10586) | 本文提出了一种用于评估机器学习系统可靠性的整体评估方法，通过评估分布内准确性、分布偏移鲁棒性、对抗鲁棒性、校准性和越界检测能力等五个关键属性，引入了可靠性得分来评估整个系统的可靠性。 |
| [^48] | [Intelligent model for offshore China sea fog forecasting.](http://arxiv.org/abs/2307.10580) | 本研究开发了一种嵌入数值天气预报模型的高级海上海雾预测方法，在解析驱动因素和处理不平衡数据的基础上，提升了海雾预测的准确性和预测能力。 |
| [^49] | [SecureBoost Hyperparameter Tuning via Multi-Objective Federated Learning.](http://arxiv.org/abs/2307.10579) | 提出了一种通过多目标联邦学习来调优SecureBoost超参数的方法，以找到在效用、效率和隐私之间最佳平衡的一组超参数解决方案。 |
| [^50] | [Boosting Federated Learning Convergence with Prototype Regularization.](http://arxiv.org/abs/2307.10575) | 本论文通过引入原型正则化策略来解决联邦学习中异质数据分布的问题，实验证明在MNIST和Fashion-MNIST数据集上相比基准模型FedAvg，我们的方法分别提高了3.3%和8.9%的平均测试准确率，并且在异构设置下具有快速收敛速度。 |
| [^51] | [Deceptive Alignment Monitoring.](http://arxiv.org/abs/2307.10569) | 本论文提出了欺骗性对齐监测这一新方向，旨在探讨大型机器学习模型在表面上表现正常，却暗中进行隐藏行为的问题，并提出了新的研究机会。 |
| [^52] | [FACADE: A Framework for Adversarial Circuit Anomaly Detection and Evaluation.](http://arxiv.org/abs/2307.10563) | FACADE是一种新型的概率和几何框架，用于无监督检测深度神经网络中的机理异常，并提供关键的洞察力和强大工具，以揭示和对抗对抗攻击，并在实际部署环境中展示了有前途的应用。 |
| [^53] | [Shared Adversarial Unlearning: Backdoor Mitigation by Unlearning Shared Adversarial Examples.](http://arxiv.org/abs/2307.10562) | 本文提出了一种使用共享对抗样本来纯化有门模型的方法，并解决了一个新的双层优化问题，从而减轻后门攻击。 |
| [^54] | [Post-variational quantum neural networks.](http://arxiv.org/abs/2307.10560) | 本文讨论了后变分策略，该策略通过将可调参数从量子计算机转移到经典计算机，并采用集合策略来优化量子模型，解决了混合量子-经典计算中的贫瘠高原问题，并设计了后变分量子神经网络架构。 |
| [^55] | [Air Traffic Controller Workload Level Prediction using Conformalized Dynamical Graph Learning.](http://arxiv.org/abs/2307.10559) | 本研究提出了一种使用合规化动态图学习来预测空中交通管制员工作负荷水平的方法，通过对退休空中交通管制员进行人机交互模拟，利用空中交通数据和工作负荷标签进行预测和评估。 |
| [^56] | [SC VALL-E: Style-Controllable Zero-Shot Text to Speech Synthesizer.](http://arxiv.org/abs/2307.10550) | 本文提出了一种基于神经编码语言模型的风格控制零样本文本到语音合成器，通过控制属性生成可控语音而不仅仅模仿特征，具有较好的性能。 |
| [^57] | [Differentially Flat Learning-based Model Predictive Control Using a Stability, State, and Input Constraining Safety Filter.](http://arxiv.org/abs/2307.10541) | 本文提出了一种基于差分平坦学习的模型预测控制方法，通过学习非线性变换作为安全滤波器，可以在保证稳定性、输入和状态约束满足的同时大幅减少计算量。 |
| [^58] | [Fast Unsupervised Deep Outlier Model Selection with Hypernetworks.](http://arxiv.org/abs/2307.10529) | 本文提出了HYPER用于调整基于深度神经网络的异常值检测模型，解决了无监督DOD模型中的超参数调整和模型选择的挑战，通过设计和训练超网络(HN)将超参数映射到主要DOD模型的最优权重上。 |
| [^59] | [Beyond Black-Box Advice: Learning-Augmented Algorithms for MDPs with Q-Value Predictions.](http://arxiv.org/abs/2307.10524) | 该论文研究了在具有不可信的机器学习建议的单轨迹时间变化的MDP中一致性和鲁棒性之间的权衡，并证明了利用Q值建议可以获得接近最优的性能保证，并改进了仅使用黑盒建议的情况。 |
| [^60] | [FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation.](http://arxiv.org/abs/2307.10507) | 本文通过选择性模型插值的方式，在联邦学习中改善了泛化能力和个性化能力。 |
| [^61] | [Identifying Interpretable Subspaces in Image Representations.](http://arxiv.org/abs/2307.10504) | FALCON是一个解释图像表示特征的框架，可以通过使用字幕数据集和视觉语言模型来生成人类可理解的概念，并通过对比解释消除虚假概念。在较大的空间中，特征通过研究组合可以更易解释和高阶评分概念的解释。 |
| [^62] | [A Competitive Learning Approach for Specialized Models: A Solution for Complex Physical Systems with Distinct Functional Regimes.](http://arxiv.org/abs/2307.10496) | 本论文提出了一种用于复杂物理系统的竞争学习方法，通过同时训练多个模型，并使用动态损失函数识别不同的功能区域。实验证明该方法能够成功地解决模型发现和函数拟合问题。 |
| [^63] | [Novel Batch Active Learning Approach and Its Application to Synthetic Aperture Radar Datasets.](http://arxiv.org/abs/2307.10495) | 该论文介绍了一种新颖的批量主动学习方法，使用Dijkstra的Annulus Core-Set (DAC)和LocalMax相结合，能够在合成孔径雷达数据中实现与顺序主动学习几乎相同的准确性，但更高效。 |
| [^64] | [Blockchain-Based Federated Learning: Incentivizing Data Sharing and Penalizing Dishonest Behavior.](http://arxiv.org/abs/2307.10492) | 本研究提出了一个综合框架，通过将数据信任与联邦学习、星际文件系统、区块链和智能合约相结合，实现了安全互惠的数据共享，并提供了激励、访问控制机制，并惩罚不诚实的行为。经过实验证明，该模型提高了联邦学习模型的准确性，同时确保了数据共享过程的安全性和公平性。 |
| [^65] | [(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs.](http://arxiv.org/abs/2307.10490) | 本论文展示了如何利用图像和声音在多模态LLMs中进行间接指令注入，攻击者通过生成对抗扰动并将其融入图像或音频录音中，以操纵模型输出特定文本和指导对话的行为。 |
| [^66] | [SPRINT: A Unified Toolkit for Evaluating and Demystifying Zero-shot Neural Sparse Retrieval.](http://arxiv.org/abs/2307.10488) | SPRINT是一个统一的Python工具包，基于Pyserini和Lucene，支持评估和解析零样本神经稀疏检索。它解决了缺乏统一环境和实现在未见过领域上的检索能力的问题。 |
| [^67] | [FinGPT: Democratizing Internet-scale Data for Financial Large Language Models.](http://arxiv.org/abs/2307.10485) | FinGPT是一个开源的数据为中心的框架，旨在将互联网规模的金融数据民主化为金融大语言模型。它提供了自动收集和整理实时金融数据的功能，解决了金融文本数据稀缺的问题。 |
| [^68] | [Can Instruction Fine-Tuned Language Models Identify Social Bias through Prompting?.](http://arxiv.org/abs/2307.10472) | 本文介绍了一种通过零样本提示评估指令微调语言模型识别偏见能力的方法，展示了Alpaca 7B在偏见识别任务中的最佳性能，并提出扩大模型大小和数据多样性可进一步提高性能。 |
| [^69] | [Classification of Visualization Types and Perspectives in Patents.](http://arxiv.org/abs/2307.10471) | 本文主要研究了专利图像中可视化类型和视角的分类问题，扩展了CLEF-IP数据集并采用最先进的深度学习方法进行了分类。这项研究对于促进专利探索和检索具有重要意义。 |
| [^70] | [A data science axiology: the nature, value, and risks of data science.](http://arxiv.org/abs/2307.10460) | 这篇论文介绍了数据科学的价值取向，探讨了其特征和作用。数据科学不是一门科学，而是一种研究范式，具有广泛的应用和重大的影响，但也存在着未知的风险。这一领域仍然处于初级阶段，需要进一步的研究。 |
| [^71] | [A New Computationally Simple Approach for Implementing Neural Networks with Output Hard Constraints.](http://arxiv.org/abs/2307.10459) | 提出了一种计算简单的方法来实现具有硬约束输出的神经网络。该方法通过映射隐藏参数向量到一个符合约束集的点实现约束，并通过附加的神经网络层来进行映射。该方法还可以处理不仅对输出向量施加约束，还对依赖于输入的联合约束施加约束的情况，并且可以处理不同类型的约束，包括线性和二次约束、等式约束和动态约束。 |
| [^72] | [A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset.](http://arxiv.org/abs/2307.10455) | 提出了一个新的大型手工标记昆虫图像数据集BIOSCAN-Insect，用于对昆虫生物多样性进行编目。该数据集还具有引人注目的特征，对广泛的机器学习社区也具有研究价值。 |
| [^73] | [Integrating a Heterogeneous Graph with Entity-aware Self-attention using Relative Position Labels for Reading Comprehension Model.](http://arxiv.org/abs/2307.10443) | 本文提出了一种新的注意力模式，使用图增强自注意力机制将从异构图中导出的推理知识整合到变压器架构中，从而克服了变压器模型在复杂推理任务中的限制。通过全局-局部注意力、图注意力和关系类型考虑，优化了实体和单词之间的注意力。该模式与相对位置标签相结合，能够与LUKE的实体感知自注意力机制相集成。 |
| [^74] | [Confidence Estimation Using Unlabeled Data.](http://arxiv.org/abs/2307.10440) | 本文提出了一种适用于半监督环境的置信度估计方法，通过检查训练过程中的预测一致性，即使只有有限的训练标签，我们仍然可以合理地近似模型对未标记样本的置信度。该方法在图像分类和分割任务中取得了最先进的性能，并且通过下游主动学习任务展示了其优势。 |
| [^75] | [Uncertainty Quantification for Molecular Property Predictions with Graph Neural Architecture Search.](http://arxiv.org/abs/2307.10438) | 用于分子属性预测的图神经网络方法通常无法量化预测的不确定性，本研究提出了一种自动化的不确定性量化方法AutoGNNUQ，通过架构搜索生成高性能的图神经网络集合，并利用方差分解将数据和模型的不确定性分开，从而提供了减少不确定性的有价值见解。 |
| [^76] | [A Bayesian Programming Approach to Car-following Model Calibration and Validation using Limited Data.](http://arxiv.org/abs/2307.10437) | 这项研究提出了一种贝叶斯编程方法，用于校准和验证车辆跟随模型，以捕捉和复制工作区内外的驾驶行为。 |
| [^77] | [A Matrix Ensemble Kalman Filter-based Multi-arm Neural Network to Adequately Approximate Deep Neural Networks.](http://arxiv.org/abs/2307.10436) | 我们提出了一种基于矩阵集合卡尔曼滤波器的多臂神经网络，可以在样本量太小无法训练多臂神经网络时充分近似深度神经网络。该方法还可以对从长短期记忆网络（LSTM）获得的预测赋予不确定性，具有良好的覆盖范围。 |
| [^78] | [Learning Formal Specifications from Membership and Preference Queries.](http://arxiv.org/abs/2307.10434) | 该论文提出了一种新的框架，通过请求成员标签和成对偏好来扩展主动规范学习，提高学习形式规范的灵活性。在两个不同领域的实验中，结果表明通过学习成员和偏好的组合可以稳定和方便地识别规范。 |
| [^79] | [DP-TBART: A Transformer-based Autoregressive Model for Differentially Private Tabular Data Generation.](http://arxiv.org/abs/2307.10430) | DP-TBART是一种基于Transformer的自回归模型，可以在保持差分隐私的情况下生成合成的表格数据。与传统方法相比，它在多种数据集上表现出竞争力，并在某些场景中超过了最先进的方法。 |
| [^80] | [PreDiff: Precipitation Nowcasting with Latent Diffusion Models.](http://arxiv.org/abs/2307.10422) | 本论文提出了PreDiff方法，使用条件隐式扩散模型进行降水近期预测。同时，引入显式知识控制机制以满足特定领域的物理约束。 |
| [^81] | [Interpreting and Correcting Medical Image Classification with PIP-Net.](http://arxiv.org/abs/2307.10404) | 本研究利用PIP-Net开展了可解释的机器学习技术在医学图像分类中的应用，并展示了其在骨折检测和皮肤癌诊断方面的准确性和可解释性。通过无监督的预训练，PIP-Net能够轻松识别数据质量问题，并且我们还发现人们可以通过手动禁用不良原型来纠正PIP-Net的推理过程。 |
| [^82] | [Selection functions of strong lens finding neural networks.](http://arxiv.org/abs/2307.10355) | 强透镜发现神经网络的选择功能对于实现大样本强引力透镜系统的潜力至关重要，网络倾向于选择具有较大爱因斯坦半径和更集中源光分布的系统。 |
| [^83] | [Properties of Discrete Sliced Wasserstein Losses.](http://arxiv.org/abs/2307.10352) | 本文研究了离散切割Wasserstein损失的性质，并探讨了其正则性和优化性质以及通过蒙特卡洛近似的方法。 |
| [^84] | [Improving Multimodal Datasets with Image Captioning.](http://arxiv.org/abs/2307.10350) | 通过探索混合原始和生成的图像描述的不同策略，我们的方法在ImageNet上超过了当前最佳降噪方法2%，在38个任务中平均提高了4%，我们的最佳方法在Flickr和MS-COCO检索上也提升了2倍。 |
| [^85] | [Code Detection for Hardware Acceleration Using Large Language Models.](http://arxiv.org/abs/2307.10348) | 本研究利用大型语言模型(LLMs)进行代码检测，并提出了一种新的提示策略，显著降低了误报，实现了出色的整体准确性，对现有的最先进的代码检测方法提出了重大挑战。 |
| [^86] | [ProtiGeno: a prokaryotic short gene finder using protein language models.](http://arxiv.org/abs/2307.10343) | ProtiGeno是一种基于深度学习的方法，利用蛋白质语言模型在寻找短原核基因方面比当前先进的方法具有更高的准确性和召回率。 |
| [^87] | [IncDSI: Incrementally Updatable Document Retrieval.](http://arxiv.org/abs/2307.10323) | IncDSI是一种递增可更新的文档检索方法，它通过最小改变网络参数的约束优化问题，实现实时添加文档而无需重新训练整个模型，具有与重新训练模型相竞争的速度，能够实时更新的文档检索系统的开发。 |
| [^88] | [Reproducibility in Machine Learning-Driven Research.](http://arxiv.org/abs/2307.10320) | 该论文调查了机器学习驱动的研究中的可重复性问题，指出了无法复现的原因，以及当前可重复性水平不断提高的挑战。同时，提出了支持机器学习可重复性的潜在驱动因素。 |
| [^89] | [Eliminating Label Leakage in Tree-Based Vertical Federated Learning.](http://arxiv.org/abs/2307.10318) | 本研究针对树型垂直联合学习中的标签泄露问题，引入了一种新的标签推断攻击方法ID2Graph，并提出了一种ID-LMID的防御机制，通过关注互信息正则化来防止标签泄露。实验结果表明ID2Graph攻击存在显著的泄露问题。 |
| [^90] | [FedBug: A Bottom-Up Gradual Unfreezing Framework for Federated Learning.](http://arxiv.org/abs/2307.10317) | FedBug是一个自底向上逐渐解冻的联邦学习框架，通过冻结和逐渐解冻模型层，实现了一种有效缓解客户端漂移现象的方法。 |
| [^91] | [Mood Classification of Bangla Songs Based on Lyrics.](http://arxiv.org/abs/2307.10314) | 本研究通过分析孟加拉歌曲的歌词，成功实现了对这些歌曲的情绪进行多类分类，包括快乐、悲伤、浪漫和放松，为使音乐更贴近人们的情感做出了重要贡献。 |
| [^92] | [Tapestry of Time and Actions: Modeling Human Activity Sequences using Temporal Point Process Flows.](http://arxiv.org/abs/2307.10305) | 本文介绍了ProActive，一种神经标记的时间点过程（MTPP）框架，用于建模人类活动序列的动态。这种方法有助于活动持续时间预测、目标预测和下一步行动推荐等下游任务。 |
| [^93] | [Polyffusion: A Diffusion Model for Polyphonic Score Generation with Internal and External Controls.](http://arxiv.org/abs/2307.10304) | Polyffusion是一种多声乐谱生成模型，通过内部和外部控制实现可控音乐生成。实验结果表明，该模型在多项音乐创作任务中表现优异，包括给定伴奏生成旋律、给定旋律生成伴奏、音乐片段修补和音乐编排等。 |
| [^94] | [Analyzing sports commentary in order to automatically recognize events and extract insights.](http://arxiv.org/abs/2307.10303) | 通过分析多种体育赛事的实时评论，利用自然语言处理技术自动识别主要行动，并通过分类和情感分析提取洞见。 |
| [^95] | [Causality-oriented robustness: exploiting general additive interventions.](http://arxiv.org/abs/2307.10299) | 本文提出了一种名为DRIG的方法，通过利用训练数据中的一般性可加干预，在预测模型中结合了内分布预测和因果性，从而实现了对未见干预的鲁棒预测。 |
| [^96] | [Towards Automated Semantic Segmentation in Mammography Images.](http://arxiv.org/abs/2307.10296) | 本文提出了一个基于深度学习的框架，用于在乳腺X光摄影图像中分割乳头、胸大肌、纤维腺体组织和脂肪组织。实验证明该框架在各种复杂情况下具有准确的分割性能，可用于临床实践中。 |
| [^97] | [ECSIC: Epipolar Cross Attention for Stereo Image Compression.](http://arxiv.org/abs/2307.10284) | ECSIC是一种用于立体图像压缩的新方法，通过利用左右图像之间的相互信息进行联合压缩，并使用新颖的立体交叉注意力模块和立体上下文模块实现。与现有方法相比，ECSIC在两个流行的立体图像数据集上取得了最先进的性能，并且具有快速编码和解码的特性。 |
| [^98] | [Zero-shot Domain-sensitive Speech Recognition with Prompt-conditioning Fine-tuning.](http://arxiv.org/abs/2307.10274) | 本研究提出了一种零样本领域敏感语音识别方法，利用文本提示来生成领域敏感模型，通过微调预训练的端到端模型实现。实验结果表明，该方法在不同领域和提示上下文下均取得了良好的性能，词错误率降低达到最高33%。通过仅使用文本进行微调，该模型在医学对话数据集上的识别效果最佳，词错误率降低达到29%。 |
| [^99] | [A DPLL(T) Framework for Verifying Deep Neural Networks.](http://arxiv.org/abs/2307.10266) | 这项工作介绍了一个新的约束求解方法NeuralSAT，用于验证深度神经网络。Preliminary results show that the NeuralSAT prototype is competitive to the state-of-the-art. |
| [^100] | [Hyperparameter Tuning Cookbook: A guide for scikit-learn, PyTorch, river, and spotPython.](http://arxiv.org/abs/2307.10262) | 本文提供了使用spotPython进行scikit-learn、PyTorch和river的超参数调整的全面指南。重点介绍了spotPython的优化过程和超参数调整，并提供了几个实际案例研究。该指南为对Python超参数调整感兴趣的人们提供了实用的起点。 |
| [^101] | [Student Assessment in Cybersecurity Training Automated by Pattern Mining and Clustering.](http://arxiv.org/abs/2307.10260) | 本研究利用模式挖掘和聚类分析自动化学生网络安全培训的评估。研究发现，通过分析学员与学习环境的互动数据，可以揭示学员的学习过程，并为他们提供有针对性的反馈来帮助他们学习。 |
| [^102] | [Hidden Markov Models with Random Restarts vs Boosting for Malware Detection.](http://arxiv.org/abs/2307.10256) | 本研究比较了在恶意软件检测中使用AdaBoost增强的HMM和使用多次随机重启训练的HMM，并发现随机重启对于大部分情况表现出令人惊喜的效果。只有在训练数据严重有限的最困难的情况下，增强才提供了足够的改进。 |
| [^103] | [Efficient selective attention LSTM for well log curve synthesis.](http://arxiv.org/abs/2307.10253) | 本文提出了一种高效的选择性注意力LSTM方法，用于预测缺失的井曲线。通过实验证实了该方法的有效性和可行性。 |
| [^104] | [A Machine Learning based Empirical Evaluation of Cyber Threat Actors High Level Attack Patterns over Low level Attack Patterns in Attributing Attacks.](http://arxiv.org/abs/2307.10252) | 通过基于机器学习的实证评估，本研究表明，在网络威胁归因中，高级攻击模式表现出更好的效果，相比于低级攻击模式。 |
| [^105] | [Automated Action Model Acquisition from Narrative Texts.](http://arxiv.org/abs/2307.10247) | NaRuto是一个系统，可以从叙事文本中自动提取结构化事件，并生成高质量的行动模型，优于现有的完全自动化方法和与半自动化方法相媲美。 |
| [^106] | [Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey).](http://arxiv.org/abs/2307.10246) | 本文综述了深度神经网络和脑对齐的研究，重点在于脑编码和解码模型的应用。这些模型对于理解大脑的信息处理机制以及设计脑机接口具有重要意义。 |
| [^107] | [Evaluating and Enhancing Robustness of Deep Recommendation Systems Against Hardware Errors.](http://arxiv.org/abs/2307.10244) | 本研究对深度推荐系统在面对硬件错误时的健壮性进行了系统研究，开发了一个错误注入框架Terrorch，发现激活剪裁是一种有希望的错误缓解方法，可以恢复高达30%的被降低的AUC-ROC得分。 |
| [^108] | [CoNAN: Conditional Neural Aggregation Network For Unconstrained Face Feature Fusion.](http://arxiv.org/abs/2307.10237) | CoNAN是一种用于无约束人脸特征融合的条件神经聚合网络，针对在长距离和高高度环境下捕获的极低分辨率人脸，利用特征分布调节方法来进行模板聚合。 |
| [^109] | [SentimentGPT: Exploiting GPT for Advanced Sentiment Analysis and its Departure from Current Machine Learning.](http://arxiv.org/abs/2307.10234) | 本研究通过利用GPT进行高级情感分析，并考察其与当前机器学习方法的差异，发现GPT方法相较于其他模型在预测性能上具有显著优势，并有效解决了情感分析任务中的一些挑战，如理解上下文和检测讽刺。 |
| [^110] | [Automated Knowledge Modeling for Cancer Clinical Practice Guidelines.](http://arxiv.org/abs/2307.10231) | 本研究提出了一种自动化方法，从国家综合癌症网络（NCCN）肿瘤学临床指南中提取知识，并生成包含该知识的结构化模型。通过使用癌症分期信息、统一医学语言系统（UMLS）和国家癌症研究所的词库（NCIt）概念以及节点分类的增强策略，该模型可以实现程序化遍历和查询。 |
| [^111] | [Exploring Link Prediction over Hyper-Relational Temporal Knowledge Graphs Enhanced with Time-Invariant Relational Knowledge.](http://arxiv.org/abs/2307.10219) | 这项研究填补了时间KG和超关系KG推理之间的差距，并开发了两个新的基准超关系TKG数据集。 |
| [^112] | [Time for aCTIon: Automated Analysis of Cyber Threat Intelligence in the Wild.](http://arxiv.org/abs/2307.10214) | 本文提供了一个新的大型开放基准数据集和一个名为aCTIon的结构化CTI信息提取工具来解决从非结构化文本源中提取相关信息的问题。这个数据集包括204个真实世界上可用的报告和相应的结构化CTI信息。通过与三个独立的CTI分析组合作，我们的团队策划了这个数据集，该数据集是之前公开发布的开源数据集的两个数量级大。 |
| [^113] | [On the Sensitivity of Deep Load Disaggregation to Adversarial Attacks.](http://arxiv.org/abs/2307.10209) | 本研究调查了深度负载分解算法对敌对攻击的敏感性，并发现常用的CNN-based NILM模型易受攻击，这对于负载分解算法的隐私和安全具有重要影响。 |
| [^114] | [Adversarial Training Over Long-Tailed Distribution.](http://arxiv.org/abs/2307.10205) | 本文研究了在长尾分布数据集上的敌对训练，提出了一种新的敌对训练框架--重新平衡敌对训练（REAT）。REAT能够解决敌对训练过程中产生的不平衡问题，有效增强模型的鲁棒性并保持准确性。 |
| [^115] | [An IPW-based Unbiased Ranking Metric in Two-sided Markets.](http://arxiv.org/abs/2307.10204) | 这项研究提出了一种基于IPW的无偏排序度量方法，针对双边市场中用户之间的偏见相互作用，解决了位置偏见和两个用户群体的位置偏差问题。 |
| [^116] | [Disentangling Societal Inequality from Model Biases: Gender Inequality in Divorce Court Proceedings.](http://arxiv.org/abs/2307.10200) | 本文通过研究离婚法庭诉讼，探索了性别不平等问题，并发现了自然语言处理方法中存在的偏见问题。需要对现有资源进行修正来量化社会不平等。 |
| [^117] | [StyleGAN2-based Out-of-Distribution Detection for Medical Imaging.](http://arxiv.org/abs/2307.10193) | 本文利用StyleGAN2的生成对抗网络（GAN）进行医学图像的异常检测，通过反向传播重构图像并利用评估指标进行异常检测，实现了超过90%的准确率。 |
| [^118] | [Several categories of Large Language Models (LLMs): A Short Survey.](http://arxiv.org/abs/2307.10188) | 本文是对大型语言模型（LLMs）的几个类别进行的简短调查，提供了各类LLM的最新发展和努力的概述，包括多语言LLMs、视觉语言LLMs和代码语言模型等。同时，还突出了在聊天机器人和虚拟助手领域存在的问题，如提升自然语言处理能力和解决道德和法律困境。 |
| [^119] | [Privacy Amplification via Importance Sampling.](http://arxiv.org/abs/2307.10187) | 通过重要性采样进行隐私放大，可以同时增强隐私保护和提高效用。我们提供了一个一般的结果来量化选择概率权重对隐私放大的影响，并展示了异质采样概率可以在保持子采样大小不变的情况下获得更好的隐私和效用。 |
| [^120] | [Multi-Scale U-Shape MLP for Hyperspectral Image Classification.](http://arxiv.org/abs/2307.10186) | 提出了一种多尺度U形多层感知器(MUMLP)模型，利用设计的MSC(Multi-Scale Channel)块和UMLP(U形多层感知器)结构，有效地表示了高光谱图像的语义和空间信息，并且能够压缩大规模参数。 |
| [^121] | [A Dual Stealthy Backdoor: From Both Spatial and Frequency Perspectives.](http://arxiv.org/abs/2307.10184) | 该论文提出了一种名为DUBA的双重隐秘后门攻击方法，该方法同时考虑了触发器在空间和频率域中的隐匿性，以实现良好的攻击性能和强大的隐匿性。 |
| [^122] | [Community-Aware Transformer for Autism Prediction in fMRI Connectome.](http://arxiv.org/abs/2307.10181) | 本研究提出了一种基于社区感知的Transformer方法，用于自闭症fMRI连接图的预测。该方法通过学习社区感知节点嵌入来提高ASD的诊断准确性和效果。 |
| [^123] | [Bayesian Spike Train Inference via Non-Local Priors.](http://arxiv.org/abs/2307.10177) | 使用基于混合半非局部先验密度和点质量的贝叶斯方法，提出一种基于随机搜索的方法，用于从荧光信号中确定神经元的脉冲序列，该方法能够高效地探索所有可能的脉冲排列并报告最高后验概率的脉冲排列和每个脉冲位置的后验概率。 |
| [^124] | [Impatient Bandits: Optimizing for the Long-Term Without Delay.](http://arxiv.org/abs/2307.09943) | 这里是中文总结出的一句话要点：这篇论文研究了在推荐系统中提高用户长期满意度的问题，通过开发一个预测延迟奖励的模型和设计一个利用该模型的赌博算法来解决了通过测量短期代理奖励反映实际长期目标不完美的挑战。 |
| [^125] | [Efficient Guided Generation for LLMs.](http://arxiv.org/abs/2307.09702) | 本文描述了一种使用正则表达式和上下文无关文法来引导语言模型文本生成的高效方法。 |
| [^126] | [Multi-view self-supervised learning for multivariate variable-channel time series.](http://arxiv.org/abs/2307.09614) | 本论文提出了一种多视角自监督学习方法，用于处理多变量通道时间序列数据，在不同数据集之间进行迁移学习。通过预训练和微调，结合传递神经网络和TS2Vec损失，该方法在大多数设置下表现优于其他方法。 |
| [^127] | [Context-Conditional Navigation with a Learning-Based Terrain- and Robot-Aware Dynamics Model.](http://arxiv.org/abs/2307.09206) | 本文提出了一种名为TRADYN的概率地形和机器人感知前向动力学模型，能够适应在自主导航环境中的地形和机器人的变化，通过在模拟的二维导航环境中的实验证明，该模型在长视程轨迹预测任务中表现出较低的预测误差。 |
| [^128] | [Multimodal LLMs for health grounded in individual-specific data.](http://arxiv.org/abs/2307.09018) | 本文提出了一个多模态健康大型语言模型（HeLM），通过学习复杂数据模态的编码器，同时支持简单模态数据的文本序列化，HeLM可以有效地使用个体专属数据估计疾病风险。 |
| [^129] | [Synthetic Lagrangian Turbulence by Generative Diffusion Models.](http://arxiv.org/abs/2307.08529) | 该论文提出了一种基于机器学习的方法，通过生成扩散模型在高雷诺数下合成拉格朗日湍流，成功实现了粒子轨迹的统计和拓扑性质的准确重现。 |
| [^130] | [Tangent Transformers for Composition, Privacy and Removal.](http://arxiv.org/abs/2307.08122) | 我们引入了一种切线注意微调方法（TAFT），通过线性化变压器的一阶泰勒展开来进行微调。该方法具有与原始非线性网络相当的性能，并在模型组合、并行训练、机器去除和差分隐私方面具有优势。 |
| [^131] | [Efficient Action Robust Reinforcement Learning with Probabilistic Policy Execution Uncertainty.](http://arxiv.org/abs/2307.07666) | 本文研究了具有概率策略执行不确定性的行动鲁棒增强学习问题，并提出了ARRLC算法，该算法在遗憾和样本复杂度上达到了极小极大最优，实验证明其优于非鲁棒算法并且收敛更快。 |
| [^132] | [Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation.](http://arxiv.org/abs/2307.07269) | 这个论文提出了一种用于稳健的体积医学图像分割模型的频域对抗训练方法，并发现其相对于传统攻击方法具有优势。该方法利用频域对抗攻击，通过引入频率一致性损失来优化模型的性能，实现对体素和频域攻击的防御。 |
| [^133] | [Wasserstein Quantum Monte Carlo: A Novel Approach for Solving the Quantum Many-Body Schr\"odinger Equation.](http://arxiv.org/abs/2307.07050) | 这篇论文提出了一种新的方法，即基于Wasserstein量子蒙特卡洛的方法，用于解决量子多体Schr\"odinger方程。该方法重新制定了能量泛函的最小化问题，并将其转化为Born分布空间中的粒子排列（反）对称波函数的问题，同时利用深度学习方法来表示丰富的波函数族。 |
| [^134] | [Quantitative CLTs in Deep Neural Networks.](http://arxiv.org/abs/2307.06092) | 本文研究了具有随机高斯权重和偏置的全连接神经网络的分布，得到了在大但有限的 $n$ 和任意固定网络深度下成立的正态逼近的定量界限，证明了随机全连接网络与相应的无限宽高斯过程之间的距离按照 $n^{-\gamma}$ 缩放，界限在网络宽度的依赖性方面优于以前的研究。 |
| [^135] | [Quantifying the Echo Chamber Effect: An Embedding Distance-based Approach.](http://arxiv.org/abs/2307.04668) | 本文提出了一种基于嵌入距离的方法来量化回音室效应。通过计算用户之间的距离，结合Echo Chamber Score(ECS)指标来评估用户社区的凝聚力和分离度，而不需要用户意识形态的标签和交互图的结构假设。 |
| [^136] | [Solvent: A Framework for Protein Folding.](http://arxiv.org/abs/2307.04603) | Solvent是一个用于蛋白质折叠的统一研究框架，支持最新模型重要组件的实现和基准测试，并提供对蛋白质结构建模领域的有用见解。 |
| [^137] | [Polynomial Width is Sufficient for Set Representation with High-dimensional Features.](http://arxiv.org/abs/2307.04001) | 本研究通过两种集合元素嵌入层的探索，证明了多项式宽度对于高维特征的集合表示足够，并揭示了之前分析中的局限性。 |
| [^138] | [Understanding Uncertainty Sampling.](http://arxiv.org/abs/2307.02719) | 本研究通过系统研究流式和池式主动学习下的不确定性采样算法，提出了一个等效损失的概念，并证明不确定性采样算法实质上是针对该等效损失进行优化。 |
| [^139] | [$\nu^2$-Flows: Fast and improved neutrino reconstruction in multi-neutrino final states with conditional normalizing flows.](http://arxiv.org/abs/2307.02405) | 本文介绍了$\nu^2$-流方法，它是$\nu$-流方法在包含多个中微子的末态中的扩展。与标准解析技术相比，$\nu^2$-流在重建中微子动量和相关性方面更准确，推断时间更快，并且可以提供改进的统计精度。 |
| [^140] | [Provably Efficient UCB-type Algorithms For Learning Predictive State Representations.](http://arxiv.org/abs/2307.00405) | 这篇论文提出了第一种已知的UCB类型方法用于学习预测状态表示（PSRs），并设计了一个新的奖励项来上界t |
| [^141] | [ChatGPT for Robotics: Design Principles and Model Abilities.](http://arxiv.org/abs/2306.17582) | 本文介绍了使用ChatGPT进行机器人应用的实验研究，通过设计原则和函数库的结合，ChatGPT能够适应不同的机器人任务，并展示了在各种机器人任务中的有效性和多样性。 |
| [^142] | [My Boli: Code-mixed Marathi-English Corpora, Pretrained Language Models and Evaluation Benchmarks.](http://arxiv.org/abs/2306.14030) | 本研究针对资源匮乏的印度语言马拉地语，提出了一个大型混合马拉地语-英语语料库，以及预训练的混合BERT模型和针对混合语言下游任务的评估数据集，该语料库训练的模型显著优于现有的BERT模型。 |
| [^143] | [Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis.](http://arxiv.org/abs/2306.13960) | 本文提出了基于正则化SE(3)群卷积的体积医学图像分析方法，通过分解连续SO(3)核和空间核以实现旋转平移等变性，并在医学分类任务中获得了显著性能提升。 |
| [^144] | [Class-Incremental Learning based on Label Generation.](http://arxiv.org/abs/2306.12619) | 本文提出了一种基于标签生成方法的增量分类学习（CIL）方法（VAG），大幅减少了灾难性遗忘（CF），并更好地保留了预训练模型的可推广表示。 |
| [^145] | [Correcting Underrepresentation and Intersectional Bias for Fair Classification.](http://arxiv.org/abs/2306.11112) | 本文提出一种可以有效纠正数据偏差和交叉偏差的学习方法，并构造了一个重新加权方案，可以精确评估任何假设在真实分布上的损失。 |
| [^146] | [Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction.](http://arxiv.org/abs/2306.10045) | 本研究提出了一种高效逼近晶体材料的完整相互作用势的方法，能够覆盖所有原子对之间的势，克服了目前方法中只考虑附近原子间势和无法捕捉无限重复模式的限制。 |
| [^147] | [Invariant Causal Set Covering Machines.](http://arxiv.org/abs/2306.04777) | 本文提出了一种名为不变因果集覆盖机的算法，它避免了产生虚假关联，可以在多项式时间内识别感兴趣变量的因果父节点。 |
| [^148] | [Emotion-Conditioned Melody Harmonization with Hierarchical Variational Autoencoder.](http://arxiv.org/abs/2306.03718) | 提出了一种基于LSTM的分层变分自编码器(LHVAE)，研究情感条件对旋律和声编配的影响，同时提高了生成的和声质量并捕捉了丰富的和弦进行变化。实验证明，该模型在感知情感表达方面优于现有方法。 |
| [^149] | [Drug Repurposing Targeting COVID-19 3CL Protease using Molecular Docking and Machine Learning Regression Approach.](http://arxiv.org/abs/2305.18088) | 本研究利用分子对接和机器学习回归方法，筛选出针对 SARS-CoV-2的主要蛋白酶3CL潜在治疗药物。其中，决策树回归（DTR）模型具有改进的统计措施R2和RMSE，有助于识别具有高结合亲和力和有利的结合能的药物。 |
| [^150] | [AUC Optimization from Multiple Unlabeled Datasets.](http://arxiv.org/abs/2305.15776) | 本文提出了一种从多个未标记数据集中构建AUC优化模型的方法，该方法在实践和理论上都有效。 |
| [^151] | [Evaluating Model Performance in Medical Datasets Over Time.](http://arxiv.org/abs/2305.13426) | 本文提出了一种Evaluation on Medical Datasets Over Time（EMDOT）框架，通过模拟每个时间点的培训过程并对未来时间点上的模型进行评估，评估了不同时间段性能的差异，对医学领域的机器学习模型提供了帮助。 |
| [^152] | [AlignAtt: Using Attention-based Audio-Translation Alignments as a Guide for Simultaneous Speech Translation.](http://arxiv.org/abs/2305.11408) | AlignAtt是一种新型的SimulST策略，使用基于注意力的音频翻译对齐来指导模型，在BLEU和延迟方面均优于之前的策略。 |
| [^153] | [MaxViT-UNet: Multi-Axis Attention for Medical Image Segmentation.](http://arxiv.org/abs/2305.08396) | MaxViT-UNet是基于编码器-解码器混合视觉变压器的医学图像分割模型，多轴自我关注机制在每个解码器阶段有助于更有效地区分对象和背景区域。 |
| [^154] | [Can point cloud networks learn statistical shape models of anatomies?.](http://arxiv.org/abs/2305.05610) | 本文研究了基于点云的统计形态建模技术，摆脱了传统基于对应关系的方法的瓶颈，提出了一种新的学习方法，并证明了它的有效性和鲁棒性。 |
| [^155] | [Improving Code Example Recommendations on Informal Documentation Using BERT and Query-Aware LSH: A Comparative Study.](http://arxiv.org/abs/2305.03017) | 本研究使用BERT和Query-Aware LSH提高非正式文档中代码示例推荐的质量，重点关注于Stack Overflow上的Java编程语言。研究使用BERT将代码示例转换为数值向量。 |
| [^156] | [Sequential Predictive Two-Sample and Independence Testing.](http://arxiv.org/abs/2305.00143) | 本文提出了一种基于预测的赌博策略来解决高维或结构化数据下非参数双样本和独立性检验问题。 |
| [^157] | [A Unified Approach to Lane Change Intention Recognition and Driving Status Prediction through TCN-LSTM and Multi-Task Learning Models.](http://arxiv.org/abs/2304.13732) | 本文提出了一种新颖的集成TCN-LSTM模型和多任务学习模型的统一方法，用于车道变换意图识别和行驶状态预测。实验证明该方法效果良好。 |
| [^158] | [Deep Reinforcement Learning Using Hybrid Quantum Neural Network.](http://arxiv.org/abs/2304.10159) | 该研究基于门控量子计算机，设计了一个参数化的量子电路来解决深度强化学习问题，并评估了其潜力。最终总结了开发深度量子学习的前景和结论。 |
| [^159] | [Fairness in AI and Its Long-Term Implications on Society.](http://arxiv.org/abs/2304.09826) | 本文探讨了AI的公平性问题，指出缺乏AI公平性会加深偏见成为社会压力因素，可能对社会产生长期影响，因此需要寻求潜在解决方案。 |
| [^160] | [Topological Point Cloud Clustering.](http://arxiv.org/abs/2303.16716) | 本文提出一种新的基于拓扑的点聚类方法，该方法可以利用拓扑特征描述点云内的数据点，相较于传统图模型方法更具有健壮性和效率。 |
| [^161] | [Natural Selection Favors AIs over Humans.](http://arxiv.org/abs/2303.16200) | 这篇论文探讨了随着人工智能的发展，其可能会出现不良特性并逐渐超越人类智能的问题，以及这对人类未来的控制权产生的影响。 |
| [^162] | [Chordal Averaging on Flag Manifolds and Its Applications.](http://arxiv.org/abs/2303.13501) | 本文提出了一种可证明收敛的算法，用于在旗型流形上计算一组点的旗形均值和旗形中值，这种方法在计算机视觉问题中非常有用。 |
| [^163] | [It Is All About Data: A Survey on the Effects of Data on Adversarial Robustness.](http://arxiv.org/abs/2303.09767) | 本文综述了有关数据对抗鲁棒性的研究，系统地总结了最新研究成果，并进一步讨论了未来研究方向和知识差距。 |
| [^164] | [No-Regret Linear Bandits beyond Realizability.](http://arxiv.org/abs/2302.13252) | 本文研究了线性Bandit中当奖励函数非线性时的情况。我们提出了一个更自然的错误模型，该模型只要求每个输入的逼近误差与其次优解之间的间距成比例。我们证明了经典的LinUCB算法可以自动适应这种错误模型，并在时间长度较大时达到近似最优的懊悔。 |
| [^165] | [MultiRobustBench: Benchmarking Robustness Against Multiple Attacks.](http://arxiv.org/abs/2302.10980) | 本文提出了一个针对对抗性攻击的多个层面的鲁棒性统一框架，通过第一个多攻击评估排行榜 MultiRobustBench，评估了16个防御模型针对9种不同攻击类型和20种不同攻击强度的鲁棒性表现。 |
| [^166] | [Navya3DSeg -- Navya 3D Semantic Segmentation Dataset & split generation for autonomous vehicles.](http://arxiv.org/abs/2302.08292) | Navya3DSeg是一个新的、具有多样标签空间的数据集，对应于大规模生产级操作领域，可用于自动驾驶感知，同时提供了一个新的方法来生成顺序数据集拆分。 |
| [^167] | [From Graph Generation to Graph Classification.](http://arxiv.org/abs/2302.07989) | 本文提出了一种新的图分类方法，通过利用图生成模型，推导出了给定图的类标签概率的分类公式，并提出了一种新的条件 ELBO 用于训练生成图自编码器模型。这是一种在图分类中具有创新性的方法。 |
| [^168] | [Variational Mixture of HyperGenerators for Learning Distributions Over Functions.](http://arxiv.org/abs/2302.06223) | 本文提出了一种新的深度生成模型VAMoH，结合了INRs对连续函数进行建模的能力和VAEs的推断能力，以及归一化流和超网络混合方法。在不同类型的数据上进行实验证明，VAMoH可以有效地学习连续函数的分布，并可以执行与推断相关的任务。 |
| [^169] | [Mathematical Capabilities of ChatGPT.](http://arxiv.org/abs/2301.13867) | 本研究调查了ChatGPT和GPT-4的数学能力，并通过使用新的方法以及发布两个新数据集，评估了它们在各个维度的数学推理上的表现。这是第一个涵盖研究生级数学并由数学研究人员策划的自然语言数据集，也测试了它们作为专业数学家助手的潜力。 |
| [^170] | [Execution-based Code Generation using Deep Reinforcement Learning.](http://arxiv.org/abs/2301.13816) | 使用深度强化学习的PPOCoder框架将预训练的编程语言模型和Proximal Policy Optimization技术结合，通过利用代码执行和结构对齐的非可微反馈，实现了更高效的代码生成。 |
| [^171] | [Explainable Data-Driven Optimization: From Context to Decision and Back Again.](http://arxiv.org/abs/2301.10074) | 本论文介绍了一种解释数据驱动问题解决方案的方法，通过使用反事实解释方法和针对随机森林和最近邻预测器的最近解释方法来解释数据驱动优化过程中的决策流程，填补了目前在学习算法决策流程解释方面的空白。 |
| [^172] | [Gaussian Process Priors for Systems of Linear Partial Differential Equations with Constant Coefficients.](http://arxiv.org/abs/2212.14319) | 该论文提出了一种名为EPGP的高斯过程先验，用于线性偏微分方程系统，并且构造了反映标准谱方法的GP核函数。该方法可以推断线性PDE系统的可能解，并具有算法性强、普适性广、适用于大数据集的稀疏版本。 |
| [^173] | [Improving Uncertainty Quantification of Variance Networks by Tree-Structured Learning.](http://arxiv.org/abs/2212.12658) | 提出了一种改进方差网络不确定性量化的方法，通过树状结构学习将特征空间分割为多个区域，并使用区域特定的神经网络预测均值和方差来量化不确定性。该方法利用新的分裂准则，在计算上友好且不需要修剪，还可以构建集合版本来估计总不确定性。 |
| [^174] | [Neural Network Complexity of Chaos and Turbulence.](http://arxiv.org/abs/2211.15382) | 本文通过深度神经网络分析了混沌和湍流的相对复杂性，并使用内在维度和有效独立特征数量量化了网络的计算复杂度。 |
| [^175] | [Positive unlabeled learning with tensor networks.](http://arxiv.org/abs/2211.14085) | 本研究引入了一种基于张量网络的特征空间距离方法来解决正无标记学习问题。这种方法不依赖于特定领域，显著改善了MNIST图像和15个分类/混合数据集的结果，并可以生成新的正面和负面实例。 |
| [^176] | [Curiosity in Hindsight: Intrinsic Exploration in Stochastic Environments.](http://arxiv.org/abs/2211.10515) | 本文提出了一种基于结构因果模型的回顾中的好奇心方法，用于稀疏奖励或无奖励环境中的探索问题。该方法学习未来的表征，以捕捉每个结果的不可预测部分，并将其用作预测的额外输入，从而获得鲁棒的内在奖励。 |
| [^177] | [Global Optimization with Parametric Function Approximation.](http://arxiv.org/abs/2211.09100) | 本文提出了一种具有参数函数逼近的全局优化算法GO-UCB，采用基于梯度的参数不确定性集合进行乐观探索，实验证明其在处理具有噪声零阶访问器的全局优化问题上优于传统的贝叶斯优化方法。 |
| [^178] | [Leveraging Offline Data in Online Reinforcement Learning.](http://arxiv.org/abs/2211.04974) | 在这项工作中，我们研究了在线强化学习中利用离线数据的设置，为具有线性结构的MDPs确定了所需的在线样本数量，并提供了实现这一目标的性质和算法。 |
| [^179] | [Nonuniqueness and Convergence to Equivalent Solutions in Observer-based Inverse Reinforcement Learning.](http://arxiv.org/abs/2210.16299) | 本文研究了在线、实时解决逆强化学习中存在的多个解的挑战，提出了一种能够收敛到近似等价解的正则化历史堆栈观察器。通过开发新的数据丰富性条件，证明了该技术的有效性。 |
| [^180] | [Data-Efficient Augmentation for Training Neural Networks.](http://arxiv.org/abs/2210.08363) | 本论文提出了一种数据高效增强训练神经网络的技术，该技术通过选择数据子集进行增强来近似捕捉完全数据增强的训练动态。该方法通过放大和扰动网络雅可比矩阵的较小奇异值，保留其显著方向，从而改进学习和泛化能力，并且通过迭代地提取小的训练数据子集，能够捕捉到完全增强的雅可比矩阵与标签/残差的对齐关系。 |
| [^181] | [When are Local Queries Useful for Robust Learning?.](http://arxiv.org/abs/2210.06089) | 本文研究了在鲁棒学习中使用局部查询的实用性，并提出了第一个针对这种鲁棒性概念进行鲁棒经验风险最小化的无分布算法。我们证明，在均匀分布下，局部成员查询不会增加并且不会超过连词和任何超类的鲁棒性阈值。另外，我们引入了局部等价查询oracle，用于在训练样本中判断假设和目标概念在扰动区域内是否一致。 |
| [^182] | [Invariant Aggregator for Defending against Federated Backdoor Attacks.](http://arxiv.org/abs/2210.01834) | 该论文针对联邦学习中的背后攻击提出了一种不变聚合器，防御背后攻击并保持模型的整体效用。研究发现在扁平损失空间中，恶意客户端可以通过提供背后样本来误导联邦学习模型，而不需要与良性客户端有明显的差异。 |
| [^183] | [MetaMask: Revisiting Dimensional Confounder for Self-Supervised Learning.](http://arxiv.org/abs/2209.07902) | MetaMask是一种通过元学习学习的维度遮罩，用于对抗自监督学习中的维度冗余和干扰问题。 |
| [^184] | [Friendly Noise against Adversarial Noise: A Powerful Defense against Data Poisoning Attacks.](http://arxiv.org/abs/2208.10224) | 本研究提出了一种简单但非常有效的方法，能够在不影响泛化性能的情况下防御各种类型的不可见污染攻击。我们通过减轻污染引入的尖锐损失区域，生成优化的友好噪声，实现对抗性噪声的防御。 |
| [^185] | [SAFARI: Versatile and Efficient Evaluations for Robustness of Interpretability.](http://arxiv.org/abs/2208.09418) | 本文提出了一种名为SAFARI的方法，用于评估深度学习的解释可靠性。该方法针对现有技术无法解决的几个挑战，通过引入两种黑盒评估方法，即最坏情况解释差异和一般情况下的鲁棒性的概率概念，来解决现有度量不全面、XAI技术异质性和误解罕见性等问题。使用遗传算法和子集模拟进行评估。 |
| [^186] | [Opinion Market Model: Stemming Far-Right Opinion Spread using Positive Interventions.](http://arxiv.org/abs/2208.06620) | 本研究提出了观点市场模型（OMM），通过引入积极干预来遏制极右派观点的传播。这个模型将观点的关注市场规模建模，并考虑了观点之间的相互作用和竞争，旨在评估积极干预的有效性。 |
| [^187] | [ForecastTKGQuestions: A Benchmark for Temporal Question Answering and Forecasting over Temporal Knowledge Graphs.](http://arxiv.org/abs/2208.06501) | 本论文提出了一个新的任务，即在时间知识图谱上进行预测性问题回答。该任务对于人们寻求未来计划非常重要，并且是先前研究中未被探索的领域。 |
| [^188] | [Representing Random Utility Choice Models with Neural Networks.](http://arxiv.org/abs/2207.12877) | 本论文提出了一种基于神经网络的离散选择模型类，RUMnets，可以近似表示任何随机效用最大化推导出的模型，并且在选择数据上有良好的预测能力。 |
| [^189] | [Tuning Stochastic Gradient Algorithms for Statistical Inference via Large-Sample Asymptotics.](http://arxiv.org/abs/2207.12395) | 我们利用大样本渐近理论对随机梯度算法进行调优，发现使用固定的大步长进行迭代平均可以鲁棒地优化算法，且具有和MLE抽样分布协方差成比例的鲁棒性。我们还提出了一种类似于Bernstein-von Mises的定理用于指导调优，包括针对模型错误规范鲁棒的广义后验。数值实验验证了我们的结果和建议在实际有限样本情况下的有效性。这些成果为分析其他随机梯度Markov Chain Monte Carlo算法提供了基础。 |
| [^190] | [Instance-Dependent Near-Optimal Policy Identification in Linear MDPs via Online Experiment Design.](http://arxiv.org/abs/2207.02575) | 本文研究了在线实验设计方法在线性MDPs中的应用，提出了一种算法\textsc{Pedel}，该算法在RL中的函数逼近设置下实现了细粒度的依赖于实例的复杂度度量，相对于最低遗憾、最小最大最优算法具有明显收益。 |
| [^191] | [Data-Driven Modeling of Noise Time Series with Convolutional Generative Adversarial Networks.](http://arxiv.org/abs/2207.01110) | 本文通过对噪声时间序列的建模，考察了两种GAN模型用于数据驱动建模的可行性，实验结果表明它们在各类噪声的复制方面具有一定的效果。 |
| [^192] | [Pythae: Unifying Generative Autoencoders in Python -- A Benchmarking Use Case.](http://arxiv.org/abs/2206.08309) | Pythae是一个开源Python库，提供了统一的生成自编码器模型实现和框架，可用于进行多种任务的案例研究基准测试。 |
| [^193] | [Pre-trained Perceptual Features Improve Differentially Private Image Generation.](http://arxiv.org/abs/2205.12900) | 该论文提出了一种利用预先训练的感知特征，通过最小化MMD（最大均值差异）来提高差分隐私图像生成的性能，并成功地生成了CIFAR10级别的图像。 |
| [^194] | [Injecting Domain Adaptation with Learning-to-hash for Effective and Efficient Zero-shot Dense Retrieval.](http://arxiv.org/abs/2205.11498) | 通过学习哈希技术提高零样本密集检索的准确性和效率，克服了存储密集索引的高内存使用问题，并在跨领域环境中进行了评估。 |
| [^195] | [The Unreasonable Effectiveness of Deep Evidential Regression.](http://arxiv.org/abs/2205.10060) | 深度证据回归是一种通过学习证据分布来处理不确定性的方法，在不确定性推理中显示出相对于传统方法和贝叶斯神经网络的优势。然而，它并非精确的不确定性量化方法，而是一种启发式方法。 |
| [^196] | [HDGT: Heterogeneous Driving Graph Transformer for Multi-Agent Trajectory Prediction via Scene Encoding.](http://arxiv.org/abs/2205.09753) | HDGT是一种将驾驶场景建模为异构图的方法，通过场景编码实现多智能体轨迹预测。该方法考虑到了驾驶场景中的不同对象和丰富的语义关系，并使用自我中心的方式进行空间关系编码。 |
| [^197] | [Torchhd: An Open Source Python Library to Support Research on Hyperdimensional Computing and Vector Symbolic Architectures.](http://arxiv.org/abs/2205.09208) | Torchhd是一个高性能的开源Python库，旨在支持超维计算和向量符号体系研究。它提供了最先进的功能，易于使用的界面，并且能够使实验运行速度提高多达100倍。 |
| [^198] | [A Review of Machine Learning Methods Applied to Structural Dynamics and Vibroacoustic.](http://arxiv.org/abs/2204.06362) | 本文综述了机器学习方法在结构动力学和振动声学中的应用，包括结构健康监测、主动噪声控制、主动振动控制以及基于机器学习的代理模型。这些应用通过数据揭示洞察力，提高了决策制定和优化产品设计的能力。 |
| [^199] | [PGCN: Progressive Graph Convolutional Networks for Spatial-Temporal Traffic Forecasting.](http://arxiv.org/abs/2202.08982) | 这项研究提出了一种名为渐进图卷积网络（PGCN）的交通预测框架，通过在训练和测试阶段逐步适应输入数据来构建一组图，以解决交通预测中的复杂时空相关性和数据变化问题。 |
| [^200] | [Sequential Kernel Embedding for Mediated and Time-Varying Dose Response Curves.](http://arxiv.org/abs/2111.03950) | 本论文提出了一种基于核岭回归的简单非参数估计方法，可以用于估计介导和时变剂量响应曲线。通过引入序贯核嵌入技术，我们实现了对复杂因果估计的简化。通过模拟实验和真实数据的估计结果，证明了该方法的强大性能和普适性。 |
| [^201] | [High-order Tensor Pooling with Attention for Action Recognition.](http://arxiv.org/abs/2110.05216) | 本论文提出了一种利用注意机制进行高阶张量池化的方法，通过引入特征值幂归一化（EPN）来防止爆发现象并提高动作识别的准确性。 |
| [^202] | [Deep Exploration for Recommendation Systems.](http://arxiv.org/abs/2109.12509) | 本文提出了一种深度探索方法以解决推荐系统中奖励稀少时的问题，并在高保真度的工业级模拟器下进行了实验，证明了该算法相比现有算法有很大的提升。 |
| [^203] | [Model Selection for Generic Contextual Bandits.](http://arxiv.org/abs/2107.03455) | 我们提出了一种自适应背景上下文强化学习算法（ACB），可以在不知道真实模型类别的情况下进行模型选择，并且具有与已知模型类别算法相匹配的遗憾率。模型选择的代价仅对遗憾上界的二阶项有贡献，且具有直观的属性。 |
| [^204] | [Warming up recurrent neural networks to maximise reachable multistability greatly improves learning.](http://arxiv.org/abs/2106.01001) | 本研究发现大多数标准循环神经网络在初始化时只有一个稳定平衡点，并且学习长时间依赖任务通常要求网络具有多稳定性。为了解决这个问题，提出了一种称为“热身”的初始化方法，通过最大化网络可达到的多稳定性来改善学习能力。在多个任务上的实验证明了该方法的有效性。 |
| [^205] | [AirNet: Neural Network Transmission over the Air.](http://arxiv.org/abs/2105.11166) | 本论文介绍了AirNet，一种新颖的训练和传输方法，允许在无线信道上高效地传输深度神经网络（DNNs），并且实现了更高的准确性。 |
| [^206] | [Robust Principal Component Analysis: A Median of Means Approach.](http://arxiv.org/abs/2102.03403) | 本文提出了一种基于中位数求和原则的PCA方法，称为中位数求和主成分分析（MoMPCA），以应对异常值对PCA的影响，并在最小假设条件下实现了最优收敛速度。 |
| [^207] | [Perceptron Theory Can Predict the Accuracy of Neural Networks.](http://arxiv.org/abs/2012.07881) | 感知器理论可以通过统计方法准确预测不同结构的神经网络的性能。 |
| [^208] | [Implicit Multidimensional Projection of Local Subspaces.](http://arxiv.org/abs/2009.03259) | 本研究提出了一种使用隐式函数导数的可视化方法，旨在理解多维投影对局部子空间的影响。通过分析局部子空间的形状和方向信息，我们能够获取更多关于数据全局结构的洞察力，并将结果可视化为图形进行分析。 |
| [^209] | [How to choose the most appropriate centrality measure? A decision tree approach.](http://arxiv.org/abs/2003.01052) | 这篇论文提出了一种利用决策树方法选择合适中心度度量方法的缩减方法。通过构建决策树调查，结合专家偏好，能够在较小的图形数量下快速筛选出最合适的中心度度量方法。 |

# 详细

[^1]: 句法与语义线性抽象和细化神经网络

    Syntactic vs Semantic Linear Abstraction and Refinement of Neural Networks. (arXiv:2307.10891v1 [cs.LO])

    [http://arxiv.org/abs/2307.10891](http://arxiv.org/abs/2307.10891)

    这项工作提供了一个灵活的框架，通过线性组合替换神经元，实现了神经网络的句法和语义抽象，并引入了一种改进方法来平衡减少和精确度。

    

    抽象是一种提高可扩展性的关键验证技术。然而，其在神经网络中的使用迄今非常有限。以前的分类网络抽象方法将多个神经元替换为足够相似的其中一个神经元。我们可以将相似性定义为句法上（使用神经元之间的连接数量）或语义上（对于各种输入的神经元激活值）的相似性。然而，以往的方法仅能达到适度的减少，而且实现起来困难。在这项工作中，我们提供了一个更灵活的框架，其中神经元可以被其他神经元的线性组合替换，从而改善减少效果。我们在句法和语义抽象上应用了这种方法，并通过实验进行了实现和评估。此外，我们引入了一种改进我们抽象的方法，以寻找更好的减少和精确度平衡。

    Abstraction is a key verification technique to improve scalability. However, its use for neural networks is so far extremely limited. Previous approaches for abstracting classification networks replace several neurons with one of them that is similar enough. We can classify the similarity as defined either syntactically (using quantities on the connections between neurons) or semantically (on the activation values of neurons for various inputs). Unfortunately, the previous approaches only achieve moderate reductions, when implemented at all. In this work, we provide a more flexible framework where a neuron can be replaced with a linear combination of other neurons, improving the reduction. We apply this approach both on syntactic and semantic abstractions, and implement and evaluate them experimentally. Further, we introduce a refinement method for our abstractions, allowing for finding a better balance between reduction and precision.
    
[^2]: 在匹配市场中的赌博学习中的玩家最优稳定遗憾

    Player-optimal Stable Regret for Bandit Learning in Matching Markets. (arXiv:2307.10890v1 [cs.LG])

    [http://arxiv.org/abs/2307.10890](http://arxiv.org/abs/2307.10890)

    在匹配市场中，通过在线学习，玩家可以实现玩家最优稳定匹配，从而最大化他们的利润。

    

    由于其广泛的应用范围，匹配市场的问题已经在文献中研究了很长时间。在这个问题中，找到一个稳定的匹配是一个常见的均衡目标。由于市场参与者通常对他们的偏好感到不确定，最近有许多研究探讨了在线设置，其中单方参与者（玩家）通过与另一方（对手）的迭代交互来学习他们未知的偏好。在这一领域的大多数先前的工作只能推导出玩家最差稳定遗憾的理论保证，这是相对于玩家最不喜欢的稳定匹配定义的。然而，在最差稳定匹配下，玩家只能获得所有稳定匹配中的最低奖励。为了最大化玩家的利润，玩家最优稳定匹配将是最理想的。然而，尽管Basu等人成功地提供了玩家最优稳定遗憾的上界，但他们的结果在玩家偏好很大的情况下可能是指数级别的。

    The problem of matching markets has been studied for a long time in the literature due to its wide range of applications. Finding a stable matching is a common equilibrium objective in this problem. Since market participants are usually uncertain of their preferences, a rich line of recent works study the online setting where one-side participants (players) learn their unknown preferences from iterative interactions with the other side (arms). Most previous works in this line are only able to derive theoretical guarantees for player-pessimal stable regret, which is defined compared with the players' least-preferred stable matching. However, under the pessimal stable matching, players only obtain the least reward among all stable matchings. To maximize players' profits, player-optimal stable matching would be the most desirable. Though \citet{basu21beyond} successfully bring an upper bound for player-optimal stable regret, their result can be exponentially large if players' preference g
    
[^3]: 面向稳健点云分类的风险优化异常值去除方法

    Risk-optimized Outlier Removal for Robust Point Cloud Classification. (arXiv:2307.10875v1 [cs.CV])

    [http://arxiv.org/abs/2307.10875](http://arxiv.org/abs/2307.10875)

    提出了一种面向稳健点云分类的风险优化异常值去除方法，利用普通训练的模型消除额外的异常值并恢复数据。方法通过归因分析确定每个点对模型输出的影响，使用条件风险价值优化高风险点的过滤过程。该方法在不需要额外训练的情况下能够产生出色的结果。

    

    点云深度模型在安全关键任务中的使用越来越普遍，但是点云噪声可能会影响这些模型的可靠性和安全性。为了解决这个问题，我们提出了一种新颖的点云异常值去除方法(PointCVaR)，它可以使标准训练的模型消除额外的异常值并恢复数据。我们的方法首先进行归因分析，以确定每个点对模型输出的影响，我们将其称为点的风险。然后，我们使用条件风险价值 (CVaR) 作为目标，优化高风险点的过滤过程。这种方法的基本原理是观察到点云噪声点往往聚集在风险分布的尾部，频率低但风险水平高，从而对分类结果产生重大干扰。尽管不需要额外的训练，我们的方法却能产生出色的结果。

    The popularity of point cloud deep models for safety-critical purposes has increased, but the reliability and security of these models can be compromised by intentional or naturally occurring point cloud noise. To combat this issue, we present a novel point cloud outlier removal method called PointCVaR, which empowers standard-trained models to eliminate additional outliers and restore the data. Our approach begins by conducting attribution analysis to determine the influence of each point on the model output, which we refer to as point risk. We then optimize the process of filtering high-risk points using Conditional Value at Risk (CVaR) as the objective. The rationale for this approach is based on the observation that noise points in point clouds tend to cluster in the tail of the risk distribution, with a low frequency but a high level of risk, resulting in significant interference with classification results. Despite requiring no additional training effort, our method produces exce
    
[^4]: 非线性元学习可以保证更快的收敛速度

    Nonlinear Meta-Learning Can Guarantee Faster Rates. (arXiv:2307.10870v1 [stat.ML])

    [http://arxiv.org/abs/2307.10870](http://arxiv.org/abs/2307.10870)

    非线性元学习可以保证更快的收敛速度。

    

    最近许多关于元学习的理论研究旨在利用相关任务中的相似表示结构来简化目标任务，并实现收敛速率的保证。然而，在实践中，表示往往是高度非线性的，引入了每个任务中不可简单平均的非平凡偏差。本研究通过非线性表示推导出元学习的理论保证。

    Many recent theoretical works on \emph{meta-learning} aim to achieve guarantees in leveraging similar representational structures from related tasks towards simplifying a target task. Importantly, the main aim in theory works on the subject is to understand the extent to which convergence rates -- in learning a common representation -- \emph{may scale with the number $N$ of tasks} (as well as the number of samples per task). First steps in this setting demonstrate this property when both the shared representation amongst tasks, and task-specific regression functions, are linear. This linear setting readily reveals the benefits of aggregating tasks, e.g., via averaging arguments. In practice, however, the representation is often highly nonlinear, introducing nontrivial biases in each task that cannot easily be averaged out as in the linear case. In the present work, we derive theoretical guarantees for meta-learning with nonlinear representations. In particular, assuming the shared nonl
    
[^5]: 云系统中的性能问题识别与关系-时间异常检测

    Performance Issue Identification in Cloud Systems with Relational-Temporal Anomaly Detection. (arXiv:2307.10869v1 [cs.LG])

    [http://arxiv.org/abs/2307.10869](http://arxiv.org/abs/2307.10869)

    云系统中的性能问题识别存在挑战，现有方法中的独立分析每个指标的异常无法解决问题，需要考虑指标之间的关联性。

    

    性能问题在大规模云服务系统中普遍存在，可能导致巨额收入损失。为了确保可靠的性能，使用服务监控指标准确地识别和定位这些问题是至关重要的。鉴于现代云系统的复杂性和规模，这项任务可能会具有挑战性，并且可能需要超出个人能力的广泛专业知识和资源。一些现有的方法通过分析每个指标独立地检测异常来解决这个问题。然而，这可能会导致难以由工程师手动诊断的压倒性警报风暴。为了追求更好的性能，不仅应考虑指标的时间模式，还应考虑指标之间的关联性模式，这可以被形式化为多变量指标异常检测问题。然而，大多数研究在明确提取这两种类型的特征方面存在不足。此外，存在一些未标记的异常。

    Performance issues permeate large-scale cloud service systems, which can lead to huge revenue losses. To ensure reliable performance, it's essential to accurately identify and localize these issues using service monitoring metrics. Given the complexity and scale of modern cloud systems, this task can be challenging and may require extensive expertise and resources beyond the capacity of individual humans. Some existing methods tackle this problem by analyzing each metric independently to detect anomalies. However, this could incur overwhelming alert storms that are difficult for engineers to diagnose manually. To pursue better performance, not only the temporal patterns of metrics but also the correlation between metrics (i.e., relational patterns) should be considered, which can be formulated as a multivariate metrics anomaly detection problem. However, most of the studies fall short of extracting these two types of features explicitly. Moreover, there exist some unlabeled anomalies m
    
[^6]: FigCaps-HF:一个基于人类反馈的图像生成标题框架和基准测试

    FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback. (arXiv:2307.10867v1 [cs.CL])

    [http://arxiv.org/abs/2307.10867](http://arxiv.org/abs/2307.10867)

    FigCaps-HF是一个图像生成标题的框架，可以通过融入领域专家的反馈意见，生成符合读者偏好的高质量图像标题。将自动评估和强化学习与人类反馈相结合，可以改善生成的标题与读者偏好的一致性。

    

    标题对于理解科学可视化和文档至关重要。现有的科学图像生成标题方法依赖于从文档中提取的图像-标题配对进行训练，但其中许多配对在帮助性、解释性和视觉描述性等指标上存在不足，导致生成的标题与读者偏好不一致。为了能够生成高质量的图像标题，我们引入了FigCaps-HF，这是一个新的图像生成标题框架，可以融入领域专家的反馈意见，以生成优化了读者偏好的标题。我们的框架包含1）一种评估图像-标题配对质量的自动方法，2）一种基于人类反馈的强化学习（RLHF）方法，用于优化生成式图像生成标题模型以符合读者偏好。我们通过在不同类型的模型上改进性能，证明了我们简单的学习框架的有效性。

    Captions are crucial for understanding scientific visualizations and documents. Existing captioning methods for scientific figures rely on figure-caption pairs extracted from documents for training, many of which fall short with respect to metrics like helpfulness, explainability, and visual-descriptiveness [15] leading to generated captions being misaligned with reader preferences. To enable the generation of high-quality figure captions, we introduce FigCaps-HF a new framework for figure-caption generation that can incorporate domain expert feedback in generating captions optimized for reader preferences. Our framework comprises of 1) an automatic method for evaluating quality of figure-caption pairs, 2) a novel reinforcement learning with human feedback (RLHF) method to optimize a generative figure-to-caption model for reader preferences. We demonstrate the effectiveness of our simple learning framework by improving performance over standard fine-tuning across different types of mod
    
[^7]: 通过深度图的持久性解决神经持久性的问题

    Addressing caveats of neural persistence with deep graph persistence. (arXiv:2307.10865v1 [cs.LG])

    [http://arxiv.org/abs/2307.10865](http://arxiv.org/abs/2307.10865)

    本文发现网络权重的方差和大权重的空间集中是影响神经持久性的主要因素，并提出了将神经持久性扩展到整个神经网络的深度图持久性测量方法。

    

    神经持久性是一种用于量化神经网络复杂性的重要指标，提出于深度学习中新兴的拓扑数据分析领域。然而，在理论和实证上我们发现，网络权重的方差和大权重的空间集中是影响神经持久性的主要因素。虽然这对于线性分类器有用的信息，但我们发现在深度神经网络的后几层中没有相关的空间结构，使得神经持久性大致等于权重的方差。此外，对于深度神经网络，所提出的层间平均过程没有考虑层间的交互。基于我们的分析，我们提出了对神经持久性基础结构的扩展，从单层改为整个神经网络，这相当于在一个特定矩阵上计算神经持久性。这得到了我们的深度图持久性测量方法。

    Neural Persistence is a prominent measure for quantifying neural network complexity, proposed in the emerging field of topological data analysis in deep learning. In this work, however, we find both theoretically and empirically that the variance of network weights and spatial concentration of large weights are the main factors that impact neural persistence. Whilst this captures useful information for linear classifiers, we find that no relevant spatial structure is present in later layers of deep neural networks, making neural persistence roughly equivalent to the variance of weights. Additionally, the proposed averaging procedure across layers for deep neural networks does not consider interaction between layers. Based on our analysis, we propose an extension of the filtration underlying neural persistence to the whole neural network instead of single layers, which is equivalent to calculating neural persistence on one particular matrix. This yields our deep graph persistence measur
    
[^8]: 将注意力分割与绑定用于改进生成语义护理

    Divide & Bind Your Attention for Improved Generative Semantic Nursing. (arXiv:2307.10864v1 [cs.CV])

    [http://arxiv.org/abs/2307.10864](http://arxiv.org/abs/2307.10864)

    本论文提出了一种名为"分割与绑定"的方法，旨在改进生成语义护理的效果。该方法引入了新的损失目标，包括关注丢失和绑定丢失，以解决复杂提示和不适当属性绑定的问题。

    

    新兴的大规模文本到图像生成模型，如稳定扩散（SD），展示了高度逼真的压倒性结果。尽管取得了巨大的进展，但当前最先进的模型仍然难以完全依照输入提示生成图像。先前的研究——关注与激发，引入了生成语义护理（GSN）的概念，旨在在推断时优化跨注意力以更好地融入语义。它在生成简单提示，如“一只猫和一只狗”，方面展示了有希望的结果。然而，它在处理更复杂的提示以及解决不适当的属性绑定问题方面的功效有所下降。为了应对复杂提示或涉及多个实体的场景所带来的挑战，并实现改进的属性绑定，我们提出了分割与绑定。我们引入了两个新的GSN损失目标：一种新的关注丢失和一种绑定丢失。我们的方法在其能够更好地将语义纳入图像生成过程中的特点上脱颖而出。

    Emerging large-scale text-to-image generative models, e.g., Stable Diffusion (SD), have exhibited overwhelming results with high fidelity. Despite the magnificent progress, current state-of-the-art models still struggle to generate images fully adhering to the input prompt. Prior work, Attend & Excite, has introduced the concept of Generative Semantic Nursing (GSN), aiming to optimize cross-attention during inference time to better incorporate the semantics. It demonstrates promising results in generating simple prompts, e.g., ``a cat and a dog''. However, its efficacy declines when dealing with more complex prompts, and it does not explicitly address the problem of improper attribute binding. To address the challenges posed by complex prompts or scenarios involving multiple entities and to achieve improved attribute binding, we propose Divide & Bind. We introduce two novel loss objectives for GSN: a novel attendance loss and a binding loss. Our approach stands out in its ability to fa
    
[^9]: 自适应权重巩固用于持续学习

    Self-paced Weight Consolidation for Continual Learning. (arXiv:2307.10845v1 [cs.LG])

    [http://arxiv.org/abs/2307.10845](http://arxiv.org/abs/2307.10845)

    提出了一种自适应权重巩固（spWC）框架，通过评估先前任务的区分性贡献，实现稳健的持续学习。具体而言，通过自适应正则化方法，根据关键绩效指标（即准确度）来衡量难度，反映过去任务的优先级，并对先前任务进行排序。

    

    持续学习算法在防止顺序任务学习中的灾难性遗忘方面很受欢迎，它们使得新任务的参数保持接近于先前任务的参数。然而，(1)如果不区分先前学习任务的贡献，新的持续学习器的性能将降低；(2)随着任务数量的增加，计算成本将大大增加，因为大多数现有算法在学习新任务时需要对所有先前任务进行正则化。为了解决以上挑战，我们提出了一种自适应自我学习权重巩固（spWC）框架，通过评估先前任务的区分性贡献，实现稳健的持续学习。具体而言，我们开发了一种自适应正则化方法，通过基于关键绩效指标（即准确度）来衡量难度，以反映过去任务的优先级。在遇到新任务时，根据优先级，对所有先前任务进行从“困难”到“简单”的排序。然后，p

    Continual learning algorithms which keep the parameters of new tasks close to that of previous tasks, are popular in preventing catastrophic forgetting in sequential task learning settings. However, 1) the performance for the new continual learner will be degraded without distinguishing the contributions of previously learned tasks; 2) the computational cost will be greatly increased with the number of tasks, since most existing algorithms need to regularize all previous tasks when learning new tasks. To address the above challenges, we propose a self-paced Weight Consolidation (spWC) framework to attain robust continual learning via evaluating the discriminative contributions of previous tasks. To be specific, we develop a self-paced regularization to reflect the priorities of past tasks via measuring difficulty based on key performance indicator (i.e., accuracy). When encountering a new task, all previous tasks are sorted from "difficult" to "easy" based on the priorities. Then the p
    
[^10]: 全球降水即时预测：基于GPM的集成多卫星检索的U-Net卷积LSTM架构

    Global Precipitation Nowcasting of Integrated Multi-satellitE Retrievals for GPM: A U-Net Convolutional LSTM Architecture. (arXiv:2307.10843v1 [cs.LG])

    [http://arxiv.org/abs/2307.10843](http://arxiv.org/abs/2307.10843)

    本文提出了一种基于U-Net和卷积LSTM的深度学习架构，能够准确预测全球降水的即时情况，尤其在极端降水方面的预测准确率更高。

    

    本文提出了一种深度学习架构，用于每30分钟全球近4小时的降水即时预测。该架构融合了U-Net和卷积长短期记忆(LSTM)神经网络，并使用来自集成多卫星检索(GPM)和全球预测系统(GFS)的关键降水驱动数据进行训练。研究了不同训练损失函数（包括均方差回归和聚焦损失分类）对降水现在预测质量的影响。结果表明，回归网络在捕捉轻度降水（小于1.6 mm/hr）方面表现良好，但分类网络在极端降水（大于8 mm/hr）的预测方面，以关键成功指数(CSI)衡量，可以胜过回归网络。使用Wasserstein距离表明，分类网络预测的降水具有更接近的类别概率。

    This paper presents a deep learning architecture for nowcasting of precipitation almost globally every 30 min with a 4-hour lead time. The architecture fuses a U-Net and a convolutional long short-term memory (LSTM) neural network and is trained using data from the Integrated MultisatellitE Retrievals for GPM (IMERG) and a few key precipitation drivers from the Global Forecast System (GFS). The impacts of different training loss functions, including the mean-squared error (regression) and the focal-loss (classification), on the quality of precipitation nowcasts are studied. The results indicate that the regression network performs well in capturing light precipitation (below 1.6 mm/hr), but the classification network can outperform the regression network for nowcasting of precipitation extremes (>8 mm/hr), in terms of the critical success index (CSI).. Using the Wasserstein distance, it is shown that the predicted precipitation by the classification network has a closer class probabili
    
[^11]: 标签校准在域偏移下的语义分割中的应用

    Label Calibration for Semantic Segmentation Under Domain Shift. (arXiv:2307.10842v1 [cs.CV])

    [http://arxiv.org/abs/2307.10842](http://arxiv.org/abs/2307.10842)

    该论文介绍了一种在域偏移下使用标签校准方法来提升语义分割模型性能的方式。

    

    预训练的语义分割模型在新领域的数据上表现很可能大幅度降低。我们通过在域偏移下计算软标签原型，并根据与预测类别概率最接近的原型进行预测，来适应未标记的目标领域数据。所提出的适应过程快速且几乎没有计算资源成本，并且能够显著提升性能。我们通过在高度实用的从合成到真实的语义分割问题上展示了标签校准的好处。

    Performance of a pre-trained semantic segmentation model is likely to substantially decrease on data from a new domain. We show a pre-trained model can be adapted to unlabelled target domain data by calculating soft-label prototypes under the domain shift and making predictions according to the prototype closest to the vector with predicted class probabilities. The proposed adaptation procedure is fast, comes almost for free in terms of computational resources and leads to considerable performance improvements. We demonstrate the benefits of such label calibration on the highly-practical synthetic-to-real semantic segmentation problem.
    
[^12]: 关于通过最优传输结合专家示范在模仿学习中的应用研究

    On Combining Expert Demonstrations in Imitation Learning via Optimal Transport. (arXiv:2307.10810v1 [cs.LG])

    [http://arxiv.org/abs/2307.10810](http://arxiv.org/abs/2307.10810)

    通过最优传输方法结合多个专家示范的新方法，在模仿学习中提供了更合理的示范几何平均值。

    

    模仿学习旨在通过专家示范来教授智能体特定任务。模仿学习的关键方法之一是定义智能体和专家之间的距离，并找到使该距离最小化的智能体策略。最优传输方法在模仿学习中被广泛使用，因为它们提供了衡量智能体和专家轨迹之间有意义距离的方法。然而，如何最佳地结合多个专家示范的问题并没有得到广泛研究。标准方法是简单地串联状态（-动作）轨迹，但在轨迹为多模态时存在问题。我们提出了一种替代方法，使用多边际最优传输距离，能够在最优传输的意义下结合多个和多样化的状态轨迹，提供更合理的示范几何平均值。我们的方法使智能体能够从多个专家中学习，并在OpenAI Gym控制环境中进行了效率分析。

    Imitation learning (IL) seeks to teach agents specific tasks through expert demonstrations. One of the key approaches to IL is to define a distance between agent and expert and to find an agent policy that minimizes that distance. Optimal transport methods have been widely used in imitation learning as they provide ways to measure meaningful distances between agent and expert trajectories. However, the problem of how to optimally combine multiple expert demonstrations has not been widely studied. The standard method is to simply concatenate state (-action) trajectories, which is problematic when trajectories are multi-modal. We propose an alternative method that uses a multi-marginal optimal transport distance and enables the combination of multiple and diverse state-trajectories in the OT sense, providing a more sensible geometric average of the demonstrations. Our approach enables an agent to learn from several experts, and its efficiency is analyzed on OpenAI Gym control environment
    
[^13]: 通过自适应特征逐渐压缩实现高效的分割学习

    Communication-Efficient Split Learning via Adaptive Feature-Wise Compression. (arXiv:2307.10805v1 [cs.DC])

    [http://arxiv.org/abs/2307.10805](http://arxiv.org/abs/2307.10805)

    该论文提出了一个名为SplitFC的通信高效的分割学习框架，通过两种自适应压缩策略来减少中间特征和梯度向量的通信开销，这些策略分别是自适应特征逐渐掉落和自适应特征逐渐量化。

    

    本文提出了一种名为SplitFC的新颖的通信高效的分割学习（SL）框架，它减少了在SL培训过程中传输中间特征和梯度向量所需的通信开销。SplitFC的关键思想是利用矩阵的列所展示的不同的离散程度。SplitFC整合了两种压缩策略：（i）自适应特征逐渐掉落和（ii）自适应特征逐渐量化。在第一种策略中，中间特征向量根据这些向量的标准偏差确定自适应掉落概率进行掉落。然后，由于链式规则，与被丢弃的特征向量相关联的中间梯度向量也会被丢弃。在第二种策略中，非丢弃的中间特征和梯度向量使用基于向量范围确定的自适应量化级别进行量化。为了尽量减小量化误差，最优量化是。

    This paper proposes a novel communication-efficient split learning (SL) framework, named SplitFC, which reduces the communication overhead required for transmitting intermediate feature and gradient vectors during the SL training process. The key idea of SplitFC is to leverage different dispersion degrees exhibited in the columns of the matrices. SplitFC incorporates two compression strategies: (i) adaptive feature-wise dropout and (ii) adaptive feature-wise quantization. In the first strategy, the intermediate feature vectors are dropped with adaptive dropout probabilities determined based on the standard deviation of these vectors. Then, by the chain rule, the intermediate gradient vectors associated with the dropped feature vectors are also dropped. In the second strategy, the non-dropped intermediate feature and gradient vectors are quantized using adaptive quantization levels determined based on the ranges of the vectors. To minimize the quantization error, the optimal quantizatio
    
[^14]: 海洋科学的时空数据挖掘：数据、方法和机遇

    Spatial-Temporal Data Mining for Ocean Science: Data, Methodologies, and Opportunities. (arXiv:2307.10803v1 [cs.LG])

    [http://arxiv.org/abs/2307.10803](http://arxiv.org/abs/2307.10803)

    该研究总结了海洋科学领域的时空数据挖掘研究，包括广泛使用的ST海洋数据集和其特点，以及ST海洋数据质量增强技术和现有STDM分类。

    

    随着海洋时空数据量的增加，已经进行了大量的空间-时间数据挖掘（STDM）研究来解决各种海洋问题，如气候预测和灾害警示。与典型的ST数据（如交通数据）相比，ST海洋数据更加复杂，具有一些独特的特征，例如多样化的区域性和高稀疏性。这些特点使得设计和训练STDM模型变得困难。不幸的是，还缺乏对这些研究的概述，这阻碍了计算机科学家在海洋领域识别研究问题，同时也使海洋科学研究人员不愿应用先进的STDM技术。为了解决这个问题，我们提供了一份综合调查，总结了海洋领域中现有的STDM研究。具体而言，我们首先总结了广泛使用的ST海洋数据集并确定了它们的独特特点。然后，讨论了典型的ST海洋数据质量增强技术。接下来，我们对现有的STDM进行分类。

    With the increasing amount of spatial-temporal~(ST) ocean data, numerous spatial-temporal data mining (STDM) studies have been conducted to address various oceanic issues, e.g., climate forecasting and disaster warning. Compared with typical ST data (e.g., traffic data), ST ocean data is more complicated with some unique characteristics, e.g., diverse regionality and high sparsity. These characteristics make it difficult to design and train STDM models. Unfortunately, an overview of these studies is still missing, hindering computer scientists to identify the research issues in ocean while discouraging researchers in ocean science from applying advanced STDM techniques. To remedy this situation, we provide a comprehensive survey to summarize existing STDM studies in ocean. Concretely, we first summarize the widely-used ST ocean datasets and identify their unique characteristics. Then, typical ST ocean data quality enhancement techniques are discussed. Next, we classify existing STDM st
    
[^15]: Meta-Transformer: 一个统一的多模态学习框架

    Meta-Transformer: A Unified Framework for Multimodal Learning. (arXiv:2307.10802v1 [cs.CV])

    [http://arxiv.org/abs/2307.10802](http://arxiv.org/abs/2307.10802)

    Meta-Transformer是一个统一的多模态学习框架，利用一个冻结的编码器进行多模态感知，在没有成对多模态训练数据的情况下可以处理各种模态，并且能够提取输入数据的高级语义特征。

    

    多模态学习旨在构建能够处理和关联多种模态的信息的模型。尽管在这个领域已经有多年的发展，但由于不同模态之间的固有差距，设计一个用于处理各种模态的统一网络仍然具有挑战性。在这项工作中，我们提出了一个名为Meta-Transformer的框架，该框架利用一个冻结的编码器在没有成对多模态训练数据的情况下进行多模态感知。在Meta-Transformer中，来自各种模态的原始输入数据被映射到一个共享的标记空间中，使得后续的编码器可以提取输入数据的高级语义特征。由三个主要组件组成：一个统一的数据标记器，一个模态共享的编码器和用于下游任务的特定任务头，Meta-Transformer是第一个在12种模态上进行统一学习的框架。

    Multimodal learning aims to build models that can process and relate information from multiple modalities. Despite years of development in this field, it still remains challenging to design a unified network for processing various modalities ($\textit{e.g.}$ natural language, 2D images, 3D point clouds, audio, video, time series, tabular data) due to the inherent gaps among them. In this work, we propose a framework, named Meta-Transformer, that leverages a $\textbf{frozen}$ encoder to perform multimodal perception without any paired multimodal training data. In Meta-Transformer, the raw input data from various modalities are mapped into a shared token space, allowing a subsequent encoder with frozen parameters to extract high-level semantic features of the input data. Composed of three main components: a unified data tokenizer, a modality-shared encoder, and task-specific heads for downstream tasks, Meta-Transformer is the first framework to perform unified learning across 12 modaliti
    
[^16]: 优化PatchCore以用于少量/大量样本的异常检测

    Optimizing PatchCore for Few/many-shot Anomaly Detection. (arXiv:2307.10792v1 [cs.CV])

    [http://arxiv.org/abs/2307.10792](http://arxiv.org/abs/2307.10792)

    本文研究了在少量样本和大量样本设置下，PatchCore算法在异常检测/异常分割性能上的表现，并发现通过优化超参数和借鉴少量样本监督学习技术可以进一步提高性能。

    

    少量样本的异常检测是一种新兴的异常检测子领域，它试图通过仅使用少量选定的样本来区分正常和异常数据。尽管新提出的少量样本异常检测方法与用于完全样本领域的预先存在的算法进行比较作为基线，但它们并没有专门针对少量样本进行优化。因此，目前尚不清楚这些预先存在的算法的性能是否可以进一步提高。本文解决了该问题。具体而言，我们对PatchCore进行了研究，该算法是目前状态最佳的完全样本异常检测/异常分割算法，研究其在少量样本和大量样本设置下的异常检测/异常分割性能。我们假设通过（I）优化其各种超参数和（II）转移已知可改善少量样本监督学习的技术到异常检测领域，可以实现进一步的性能提升。对公共VisA和MVTec异常检测数据集进行了详尽的实验证明，（I）sign

    Few-shot anomaly detection (AD) is an emerging sub-field of general AD, and tries to distinguish between normal and anomalous data using only few selected samples. While newly proposed few-shot AD methods do compare against pre-existing algorithms developed for the full-shot domain as baselines, they do not dedicatedly optimize them for the few-shot setting. It thus remains unclear if the performance of such pre-existing algorithms can be further improved. We address said question in this work. Specifically, we present a study on the AD/anomaly segmentation (AS) performance of PatchCore, the current state-of-the-art full-shot AD/AS algorithm, in both the few-shot and the many-shot settings. We hypothesize that further performance improvements can be realized by (I) optimizing its various hyperparameters, and by (II) transferring techniques known to improve few-shot supervised learning to the AD domain. Exhaustive experiments on the public VisA and MVTec AD datasets reveal that (I) sign
    
[^17]: 攻击混合分类器的对抗性方法

    Adversarial attacks for mixtures of classifiers. (arXiv:2307.10788v1 [cs.LG])

    [http://arxiv.org/abs/2307.10788](http://arxiv.org/abs/2307.10788)

    本文讨论了攻击混合分类器的问题，并引入了两个可取性质。我们证明了现有的攻击方法不能同时满足这两个性质。最后，我们介绍了一种新的攻击方法，并通过实验证明了其性能好。

    

    混合分类器（即随机集成）被提出作为提高对抗性攻击鲁棒性的一种方法。然而，已经证明现有的攻击方法并不适用于这种类型的分类器。本文讨论了以一种原则性的方式攻击混合分类器的问题，并通过对问题的几何分析引入了攻击的两个可取性质（效力和极大性）。然后，我们证明了现有的攻击方法不能同时满足这两个性质。最后，我们介绍了一种名为晶格攀登攻击的新攻击方法，在二元线性设置下具有理论保证，并通过对合成数据集和真实数据集的实验展示其性能。

    Mixtures of classifiers (a.k.a. randomized ensembles) have been proposed as a way to improve robustness against adversarial attacks. However, it has been shown that existing attacks are not well suited for this kind of classifiers. In this paper, we discuss the problem of attacking a mixture in a principled way and introduce two desirable properties of attacks based on a geometrical analysis of the problem (effectiveness and maximality). We then show that existing attacks do not meet both of these properties. Finally, we introduce a new attack called lattice climber attack with theoretical guarantees on the binary linear setting, and we demonstrate its performance by conducting experiments on synthetic and real datasets.
    
[^18]: 通过类原型实现的前馈源无关域适应

    Feed-Forward Source-Free Domain Adaptation via Class Prototypes. (arXiv:2307.10787v1 [cs.CV])

    [http://arxiv.org/abs/2307.10787](http://arxiv.org/abs/2307.10787)

    这项工作提出了一种前馈方法来解决源无关域适应问题，通过计算类的原型来处理域偏移，相较于使用预训练模型，该方法在精度和时间上都有显著提升。

    

    在没有访问源数据的情况下，源无关域适应因其实用性和无需访问源数据而变得流行。然而，适应过程仍然需要相当长的时间，并且主要基于依赖于反向传播的优化。在这项工作中，我们提出了一种简单的前馈方法，挑战了基于反向传播的适应的必要性。我们的方法基于使用预训练模型计算出的类在域偏移下的原型。与预训练模型相比，它在精度上取得了强大的改进，并且只需要现有域适应方法所需时间的一小部分。

    Source-free domain adaptation has become popular because of its practical usefulness and no need to access source data. However, the adaptation process still takes a considerable amount of time and is predominantly based on optimization that relies on back-propagation. In this work we present a simple feed-forward approach that challenges the need for back-propagation based adaptation. Our approach is based on computing prototypes of classes under the domain shift using a pre-trained model. It achieves strong improvements in accuracy compared to the pre-trained model and requires only a small fraction of time of existing domain adaptation methods.
    
[^19]: 高效的Beam Tree递归

    Efficient Beam Tree Recursion. (arXiv:2307.10779v1 [cs.LG])

    [http://arxiv.org/abs/2307.10779](http://arxiv.org/abs/2307.10779)

    本文提出了一种高效的Beam Tree递归算法（BT-RvNN），通过解决评分函数和递归单元函数的纠缠问题以及简化内存使用，成功降低了BT-RvNN的内存使用。这个算法在ListOps任务中达到了新的最先进水平，并在其他任务中保持了类似的性能。

    

    最近提出的Beam Tree递归神经网络（BT-RvNN）是Gumbel Tree RvNN的简单扩展，已经在ListOps中取得了最先进的长度泛化性能，同时在其他任务上保持了可比较的性能。然而，尽管不是最差的，但BT-RvNN的内存使用仍然非常昂贵。在本文中，我们确定了BT-RvNN内存使用的主要瓶颈是评分函数和递归单元函数的纠缠。我们提出了策略来解决这个瓶颈，并进一步简化内存使用。总体上，我们的策略不仅将BT-RvNN的内存使用减少了10-16倍，而且在ListOps中创造了新的最先进水平，同时在其他任务中保持了类似的性能。此外，我们还提出了一种利用BT-RvNN产生的引导隐层树节点表示的策略，将BT-RvNN从形式为f：R^n×d ->的句子编码器转换成。”

    Beam Tree Recursive Neural Network (BT-RvNN) was recently proposed as a simple extension of Gumbel Tree RvNN and it was shown to achieve state-of-the-art length generalization performance in ListOps while maintaining comparable performance on other tasks. However, although not the worst in its kind, BT-RvNN can be still exorbitantly expensive in memory usage. In this paper, we identify the main bottleneck in BT-RvNN's memory usage to be the entanglement of the scorer function and the recursive cell function. We propose strategies to remove this bottleneck and further simplify its memory usage. Overall, our strategies not only reduce the memory usage of BT-RvNN by $10$-$16$ times but also create a new state-of-the-art in ListOps while maintaining similar performance in other tasks. In addition, we also propose a strategy to utilize the induced latent-tree node representations produced by BT-RvNN to turn BT-RvNN from a sentence encoder of the form $f:\mathbb{R}^{n \times d} \rightarrow \
    
[^20]: 评估AutoML在数据驱动软件工程中的应用

    Assessing the Use of AutoML for Data-Driven Software Engineering. (arXiv:2307.10774v1 [cs.SE])

    [http://arxiv.org/abs/2307.10774](http://arxiv.org/abs/2307.10774)

    AutoML作为一种自动化构建端到端AI/ML流水线的解决方案被广泛关注，但目前对其在开发AI/ML系统的团队中的采用程度和感知程度缺乏信息。

    

    背景：由于人工智能（AI）和机器学习（ML）在构建软件应用方面的广泛应用，公司正努力招聘具有深入了解这些技术的员工。在这种情况下，AutoML作为填补AI / ML技能缺口的有希望的解决方案大受欢迎，因为它承诺自动化构建端到端AI / ML流水线，这些流水线通常由专门的团队成员设计。目标：尽管受到越来越多的关注和高期望，但目前对于开发AI / ML系统的团队当前采用AutoML的程度以及从实践者和研究者的视角来看它的感知程度缺乏信息。方法：为了填补这些空白，本文提出了一项混合方法研究，包括对两个软件工程数据集中12个端到端AutoML工具的基准测试以及用户调查和后续访谈，以进一步了解AutoML的采用和感知。结果：我们发现AutoML解决方案可以

    Background. Due to the widespread adoption of Artificial Intelligence (AI) and Machine Learning (ML) for building software applications, companies are struggling to recruit employees with a deep understanding of such technologies. In this scenario, AutoML is soaring as a promising solution to fill the AI/ML skills gap since it promises to automate the building of end-to-end AI/ML pipelines that would normally be engineered by specialized team members. Aims. Despite the growing interest and high expectations, there is a dearth of information about the extent to which AutoML is currently adopted by teams developing AI/ML-enabled systems and how it is perceived by practitioners and researchers. Method. To fill these gaps, in this paper, we present a mixed-method study comprising a benchmark of 12 end-to-end AutoML tools on two SE datasets and a user survey with follow-up interviews to further our understanding of AutoML adoption and perception. Results. We found that AutoML solutions can 
    
[^21]: 使用ResNet和Bi-GRU利用视觉频谱图进行音乐流派分类

    Music Genre Classification with ResNet and Bi-GRU Using Visual Spectrograms. (arXiv:2307.10773v1 [cs.SD])

    [http://arxiv.org/abs/2307.10773](http://arxiv.org/abs/2307.10773)

    该论文提出了一种使用ResNet和Bi-GRU结构，并利用视觉频谱图进行音乐流派分类的方法，以提高音乐推荐系统的性能。

    

    音乐推荐系统已成为增强音乐流媒体服务用户体验和满意度的重要组成部分。改进这些推荐系统的关键挑战在于理解音乐数据的复杂性，特别是音乐流派分类。传统的机器学习技术在流派分类方面显示出潜力，但过于依赖手动设计的特征和特征选择，无法捕捉音乐数据的完整复杂性。另一方面，深度学习分类架构如传统的卷积神经网络（CNN）在捕捉空间层次结构方面是有效的，但在捕捉音乐数据中固有的时间动态方面存在困难。为了解决这些挑战，

    Music recommendation systems have emerged as a vital component to enhance user experience and satisfaction for the music streaming services, which dominates music consumption. The key challenge in improving these recommender systems lies in comprehending the complexity of music data, specifically for the underpinning music genre classification. The limitations of manual genre classification have highlighted the need for a more advanced system, namely the Automatic Music Genre Classification (AMGC) system. While traditional machine learning techniques have shown potential in genre classification, they heavily rely on manually engineered features and feature selection, failing to capture the full complexity of music data. On the other hand, deep learning classification architectures like the traditional Convolutional Neural Networks (CNN) are effective in capturing the spatial hierarchies but struggle to capture the temporal dynamics inherent in music data. To address these challenges, t
    
[^22]: 解码谜团：在工作记忆的多个方面上对人类和人工智能进行基准测试

    Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory. (arXiv:2307.10768v1 [q-bio.NC])

    [http://arxiv.org/abs/2307.10768](http://arxiv.org/abs/2307.10768)

    本论文介绍了一个全面的工作记忆基准数据集（WorM），通过评估4个功能、3个领域和11个行为和神经特征的WM任务来开发和评估AI WM模型。结果表明，AI模型能够模拟出脑中工作记忆的一些特征，如优势效应和最新性效应，以及专门用于不同领域和功能的工作记忆的神经群集和相关物。

    

    工作记忆（WM）是一种基本的认知过程，它促进了信息的临时存储、整合、操作和检索，在推理和决策任务中起着重要作用。捕捉工作记忆多方面特征的可靠基准数据集对于有效地开发和评估AI工作记忆模型至关重要。在这里，我们介绍了一个全面的工作记忆（WorM）基准数据集，以实现这个目的。WorM包括10个任务和总共100万次试验，评估了WM的4个功能、3个领域和11个行为和神经特征。我们在所有这些任务上共同训练和测试了最先进的循环神经网络和Transformer。我们还包括人类行为基准作为对比的上限。我们的结果表明，AI模型模拟了脑中工作记忆的一些特征，特别是优势效应和最新性效应，以及专门用于不同领域和功能性的工作记忆的神经群集和相关物。

    Working memory (WM), a fundamental cognitive process facilitating the temporary storage, integration, manipulation, and retrieval of information, plays a vital role in reasoning and decision-making tasks. Robust benchmark datasets that capture the multifaceted nature of WM are crucial for the effective development and evaluation of AI WM models. Here, we introduce a comprehensive Working Memory (WorM) benchmark dataset for this purpose. WorM comprises 10 tasks and a total of 1 million trials, assessing 4 functionalities, 3 domains, and 11 behavioral and neural characteristics of WM. We jointly trained and tested state-of-the-art recurrent neural networks and transformers on all these tasks. We also include human behavioral benchmarks as an upper bound for comparison. Our results suggest that AI models replicate some characteristics of WM in the brain, most notably primacy and recency effects, and neural clusters and correlates specialized for different domains and functionalities of WM
    
[^23]: MSQNet: 无关演员的多模态动作识别

    MSQNet: Actor-agnostic Action Recognition with Multi-modal Query. (arXiv:2307.10763v1 [cs.CV])

    [http://arxiv.org/abs/2307.10763](http://arxiv.org/abs/2307.10763)

    MSQNet是一种无关演员的多模态多标签动作识别方法，通过使用视觉和文本模态来更好地表示动作类别，克服了现有方法中针对特定演员的限制。

    

    现有的动作识别方法通常是针对特定演员的，因为演员之间具有固有的拓扑和显着差异。这就需要特定演员的姿态估计（例如人类与动物），导致模型设计复杂性和高维护成本。此外，它们通常只关注学习视觉模态和单标签分类，忽视了其他可用信息源（例如类名文本）和多个动作的同时发生。为了克服这些限制，我们提出了一种新的方法，称为“无关演员的多模态多标签动作识别”，为包括人类和动物在内的各种类型的演员提供了统一的解决方案。我们进一步在基于Transformer的目标检测框架（例如DETR）中提出了一种新颖的多模态语义查询网络（MSQNet）模型，通过利用视觉和文本模态更好地表示动作类别。消除了演员特定性的限制。

    Existing action recognition methods are typically actor-specific due to the intrinsic topological and apparent differences among the actors. This requires actor-specific pose estimation (e.g., humans vs. animals), leading to cumbersome model design complexity and high maintenance costs. Moreover, they often focus on learning the visual modality alone and single-label classification whilst neglecting other available information sources (e.g., class name text) and the concurrent occurrence of multiple actions. To overcome these limitations, we propose a new approach called 'actor-agnostic multi-modal multi-label action recognition,' which offers a unified solution for various types of actors, including humans and animals. We further formulate a novel Multi-modal Semantic Query Network (MSQNet) model in a transformer-based object detection framework (e.g., DETR), characterized by leveraging visual and textual modalities to represent the action classes better. The elimination of actor-spec
    
[^24]: 减轻选民属性偏见以实现公正的意见汇总

    Mitigating Voter Attribute Bias for Fair Opinion Aggregation. (arXiv:2307.10749v1 [cs.HC])

    [http://arxiv.org/abs/2307.10749](http://arxiv.org/abs/2307.10749)

    本文研究了通过考虑选民属性来实现公正的意见汇总的方法，并评估了汇总结果的公正性。

    

    多个意见的汇总在决策中起着至关重要的作用，例如在招聘和贷款审核中，以及在为监督学习标记数据时。虽然多数投票和现有的意见汇总模型对于简单任务是有效的，但在没有客观真实标签的任务中，它们并不适用，因为可能会出现分歧。特别是，当选民属性（如性别或种族）引入偏见时，汇总结果可能会因选民属性的组成而异。一个平衡的选民群体对于公平的汇总结果是理想的，但可能难以准备。在本研究中，我们考虑了基于选民属性实现公正意见汇总的方法，并评估了汇总结果的公正性。为此，我们考虑了将多数投票和Dawid和Skene模型（D&S模型）等意见汇总模型与采样加权等公平选项相结合的方法。为了评估汇总结果的公正性。

    The aggregation of multiple opinions plays a crucial role in decision-making, such as in hiring and loan review, and in labeling data for supervised learning. Although majority voting and existing opinion aggregation models are effective for simple tasks, they are inappropriate for tasks without objectively true labels in which disagreements may occur. In particular, when voter attributes such as gender or race introduce bias into opinions, the aggregation results may vary depending on the composition of voter attributes. A balanced group of voters is desirable for fair aggregation results but may be difficult to prepare. In this study, we consider methods to achieve fair opinion aggregation based on voter attributes and evaluate the fairness of the aggregated results. To this end, we consider an approach that combines opinion aggregation models such as majority voting and the Dawid and Skene model (D&S model) with fairness options such as sample weighting. To evaluate the fairness of 
    
[^25]: 公平感知的联邦学习客户端选择

    Fairness-Aware Client Selection for Federated Learning. (arXiv:2307.10738v1 [cs.LG])

    [http://arxiv.org/abs/2307.10738](http://arxiv.org/abs/2307.10738)

    提出了公平感知的联邦客户端选择（FairFedCS）方法，通过动态调整联邦学习客户端的选择概率，同时考虑客户端的声誉、参与次数和对模型性能的贡献，解决了平衡性能和公平性的问题。

    

    联邦学习使得多个数据所有者（即联邦学习客户端）能够在不泄露私人数据的情况下进行机器学习模型的协作训练。由于联邦学习服务器每轮训练只能选择有限数量的客户端，客户端选择成为重要的研究问题。现有方法通常关注于提高联邦学习模型性能或提高客户端的公平待遇。在选择联邦学习客户端时平衡性能和公平性考虑的问题仍然存在。为了解决这个问题，我们提出了公平感知的联邦客户端选择（FairFedCS）方法。基于李雅普诺夫优化，它通过同时考虑客户端的声誉、参与联邦学习任务的次数和对最终模型性能的贡献，动态调整联邦学习客户端的选择概率。通过不使用基于阈值的声誉过滤，它为联邦学习客户端提供了赎回声誉的机会。

    Federated learning (FL) has enabled multiple data owners (a.k.a. FL clients) to train machine learning models collaboratively without revealing private data. Since the FL server can only engage a limited number of clients in each training round, FL client selection has become an important research problem. Existing approaches generally focus on either enhancing FL model performance or enhancing the fair treatment of FL clients. The problem of balancing performance and fairness considerations when selecting FL clients remains open. To address this problem, we propose the Fairness-aware Federated Client Selection (FairFedCS) approach. Based on Lyapunov optimization, it dynamically adjusts FL clients' selection probabilities by jointly considering their reputations, times of participation in FL tasks and contributions to the resulting model performance. By not using threshold-based reputation filtering, it provides FL clients with opportunities to redeem their reputations after a perceive
    
[^26]: 高斯混合下的长尾理论

    Long-Tail Theory under Gaussian Mixtures. (arXiv:2307.10736v1 [cs.LG])

    [http://arxiv.org/abs/2307.10736](http://arxiv.org/abs/2307.10736)

    该论文提出了一个简单的高斯混合模型，符合Feldman的长尾理论。通过实验证明，在长尾分布情况下，非线性分类器可以提高泛化能力，而线性分类器不能。该结果强调了对于长尾分布，需要考虑罕见的训练样本以实现最佳泛化能力。

    

    我们提出了一个简单的高斯混合模型来生成遵循Feldman的长尾理论（2020）的数据。我们证明，在提出的模型中，线性分类器无法将泛化误差降低到一定水平以下，而具有记忆能力的非线性分类器可以。这证实了对于长尾分布，必须考虑罕见的训练样本以实现对新数据的最佳泛化。最后，我们通过在合成和真实数据上的实验证明，当子群体频率分布的尾部变短时，线性模型和非线性模型之间的性能差距可以减小。

    We suggest a simple Gaussian mixture model for data generation that complies with Feldman's long tail theory (2020). We demonstrate that a linear classifier cannot decrease the generalization error below a certain level in the proposed model, whereas a nonlinear classifier with a memorization capacity can. This confirms that for long-tailed distributions, rare training examples must be considered for optimal generalization to new data. Finally, we show that the performance gap between linear and nonlinear models can be lessened as the tail becomes shorter in the subpopulation frequency distribution, as confirmed by experiments on synthetic and real data.
    
[^27]: 困难样本和噪声样本之间的区别：一项实证研究

    Differences Between Hard and Noisy-labeled Samples: An Empirical Study. (arXiv:2307.10718v1 [cs.LG])

    [http://arxiv.org/abs/2307.10718](http://arxiv.org/abs/2307.10718)

    本文通过实证研究探讨了困难样本和噪声样本之间的区别，并提出一种简单而有效的度量标准，可以区分并过滤掉噪声标记的样本。

    

    从具有困难/难以处理的样本的标记数据集中提取噪声或标签错误的样本是一个重要但未被充分探索的主题。现有方法大多将这两种类型的数据同等对待，导致模型整体性能下降。本文首先设计了不同样本的自定义硬度和噪声水平的各种合成数据集。我们的实证研究使我们能够更好地理解难以学习的样本和错误标记的样本之间的相似性和区别。这些可控实验为区分困难样本和噪声样本的方法的发展铺平了道路。通过我们的研究，我们引入了一种简单而有效的度量标准，可以过滤掉噪声标记的样本，同时保留困难样本。

    Extracting noisy or incorrectly labeled samples from a labeled dataset with hard/difficult samples is an important yet under-explored topic. Two general and often independent lines of work exist, one focuses on addressing noisy labels, and another deals with hard samples. However, when both types of data are present, most existing methods treat them equally, which results in a decline in the overall performance of the model. In this paper, we first design various synthetic datasets with custom hardness and noisiness levels for different samples. Our proposed systematic empirical study enables us to better understand the similarities and more importantly the differences between hard-to-learn samples and incorrectly-labeled samples. These controlled experiments pave the way for the development of methods that distinguish between hard and noisy samples. Through our study, we introduce a simple yet effective metric that filters out noisy-labeled samples while keeping the hard samples. We s
    
[^28]: 重参数化策略学习用于多模态轨迹优化

    Reparameterized Policy Learning for Multimodal Trajectory Optimization. (arXiv:2307.10710v1 [cs.LG])

    [http://arxiv.org/abs/2307.10710](http://arxiv.org/abs/2307.10710)

    本研究针对高维连续动作空间中的增强学习，提出了一种多模态策略参数化的重参数化框架，并基于此框架提出了一种重参数化策略梯度方法，能够在任务中避免局部最优，解决稀疏奖励环境中的挑战。

    

    我们研究了在高维连续动作空间中为增强学习 (RL) 参数化策略的挑战。我们的目标是开发一种多模态策略，克服了常用的高斯参数化的局限性。为了实现这一目标，我们提出了一个原则性的框架，将连续RL策略建模为最优轨迹的生成模型。通过将策略条件于潜变量，我们导出了一个新颖的变分上界作为优化目标，从而促进了对环境的探索。然后，我们提出了一种实用的基于模型的RL方法，称为重参数化策略梯度(RPG)，它利用了多模态策略参数化和学得的世界模型以实现强大的探索能力和高效的数据利用率。实证结果表明，我们的方法可以帮助代理在具有密集奖励的任务中避免局部最优，并通过整合物体为中心的编码，解决具有挑战性的稀疏奖励环境的问题。

    We investigate the challenge of parametrizing policies for reinforcement learning (RL) in high-dimensional continuous action spaces. Our objective is to develop a multimodal policy that overcomes limitations inherent in the commonly-used Gaussian parameterization. To achieve this, we propose a principled framework that models the continuous RL policy as a generative model of optimal trajectories. By conditioning the policy on a latent variable, we derive a novel variational bound as the optimization objective, which promotes exploration of the environment. We then present a practical model-based RL method, called Reparameterized Policy Gradient (RPG), which leverages the multimodal policy parameterization and learned world model to achieve strong exploration capabilities and high data efficiency. Empirical results demonstrate that our method can help agents evade local optima in tasks with dense rewards and solve challenging sparse-reward environments by incorporating an object-centric
    
[^29]: TwinLiteNet：自动驾驶汽车中可驱动区域和车道分割的高效轻量模型

    TwinLiteNet: An Efficient and Lightweight Model for Driveable Area and Lane Segmentation in Self-Driving Cars. (arXiv:2307.10705v1 [cs.CV])

    [http://arxiv.org/abs/2307.10705](http://arxiv.org/abs/2307.10705)

    本文提出了一种轻量级模型TwinLiteNet，用于自动驾驶车辆中的可驱动区域和车道分割。该模型成本低廉且高效准确，并在实验中表现出明显的计算资源节约。

    

    语义分割是自动驾驶中一个常见的任务，用于理解周围环境。对于道路上的安全和高效导航来说，可驱动区域分割和车道检测尤为重要。然而，原始的语义分割模型计算开销大，需要高端硬件，这对于嵌入式系统的自动驾驶车辆来说是不可行的。本文提出了一种轻量级的可驱动区域和车道线分割模型。TwinLiteNet设计成成本低廉，但能够实现准确和高效的分割结果。我们在BDD100K数据集上评估了TwinLiteNet，并与现代模型进行了比较。实验结果表明，我们的TwinLiteNet与现有方法表现相似，但所需的计算资源显著减少。具体而言，TwinLiteNet在可驱动区域任务上实现了91.3%的mIoU评分，在车道检测任务上实现了31.08%的IoU评分，仅使用了40万个参数，在GPU RTX上实现了415 FPS。

    Semantic segmentation is a common task in autonomous driving to understand the surrounding environment. Driveable Area Segmentation and Lane Detection are particularly important for safe and efficient navigation on the road. However, original semantic segmentation models are computationally expensive and require high-end hardware, which is not feasible for embedded systems in autonomous vehicles. This paper proposes a lightweight model for the driveable area and lane line segmentation. TwinLiteNet is designed cheaply but achieves accurate and efficient segmentation results. We evaluate TwinLiteNet on the BDD100K dataset and compare it with modern models. Experimental results show that our TwinLiteNet performs similarly to existing approaches, requiring significantly fewer computational resources. Specifically, TwinLiteNet achieves a mIoU score of 91.3% for the Drivable Area task and 31.08% IoU for the Lane Detection task with only 0.4 million parameters and achieves 415 FPS on GPU RTX 
    
[^30]: 大规模电动汽车的自适应多智能体多臂赌博解决方案下的分散式智能充电

    Decentralized Smart Charging of Large-Scale EVs using Adaptive Multi-Agent Multi-Armed Bandits. (arXiv:2307.10704v1 [cs.LG])

    [http://arxiv.org/abs/2307.10704](http://arxiv.org/abs/2307.10704)

    该论文提出了一个分散式智能充电系统，利用自适应多智能体系统的方法进行控制，并采用多臂赌博学习处理不确定性。该系统具有可扩展性、实时性、无模型性，并且考虑了不同参与者之间的公平性。

    

    电动汽车和光伏的快速增长会带来新的挑战，如电流拥堵和电压限制违规。通过控制电动汽车的运行即智能充电，可以缓解这些问题。文献中已提出集中式智能充电解决方案，但这样的解决方案可能缺乏可扩展性，并且受到集中化的固有缺点的影响，如单一故障点和数据隐私问题。分散化可以帮助解决这些挑战。本文提出了一个完全分散化的智能充电系统，采用自适应多智能体系统的理念。所提出的系统利用多臂赌博学习来处理系统中的不确定性。该系统具有分散化、可扩展性、实时性、无模型性，并考虑了不同参与者之间的公平性。还提供了一个详细的案例研究以进行性能评估。

    The drastic growth of electric vehicles and photovoltaics can introduce new challenges, such as electrical current congestion and voltage limit violations due to peak load demands. These issues can be mitigated by controlling the operation of electric vehicles i.e., smart charging. Centralized smart charging solutions have already been proposed in the literature. But such solutions may lack scalability and suffer from inherent drawbacks of centralization, such as a single point of failure, and data privacy concerns. Decentralization can help tackle these challenges. In this paper, a fully decentralized smart charging system is proposed using the philosophy of adaptive multi-agent systems. The proposed system utilizes multi-armed bandit learning to handle uncertainties in the system. The presented system is decentralized, scalable, real-time, model-free, and takes fairness among different players into account. A detailed case study is also presented for performance evaluation.
    
[^31]: 在气候科学中使用状态空间模型中的图形进行Granger因果性分析

    Graphs in State-Space Models for Granger Causality in Climate Science. (arXiv:2307.10703v1 [cs.LG])

    [http://arxiv.org/abs/2307.10703](http://arxiv.org/abs/2307.10703)

    该论文提出了使用状态空间模型中的图形进行Granger因果性分析的方法，通过使用GraphEM算法估计线性高斯状态空间模型中状态方程中的线性矩阵运算符，并在M步中包括Lasso正则化进行求解。通过在玩具例子和具有挑战性的气候问题上进行实验证明了该方法相对于标准的Granger因果性方法的优势。

    

    Granger因果性(GC)经常被认为不是一种真正的因果关系形式。然而，它可以说是最广泛使用的评估一个时间序列从另一个时间序列中可预测性的方法。Granger因果性在许多应用学科中被广泛使用，从神经科学和计量经济学到地球科学。我们从状态空间模型的图形角度重新审视Granger因果性。为此，我们使用GraphEM，一种最近提出的用于估计线性高斯状态空间模型状态方程中的线性矩阵算子的期望最大化算法。在M步中包括Lasso正则化，使用一种近端拆分Douglas-Rachford算法进行求解。玩具例子和具有挑战性的气候问题上的实验说明了所提出的模型和推断技术相对于标准Granger因果性方法的优点。

    Granger causality (GC) is often considered not an actual form of causality. Still, it is arguably the most widely used method to assess the predictability of a time series from another one. Granger causality has been widely used in many applied disciplines, from neuroscience and econometrics to Earth sciences. We revisit GC under a graphical perspective of state-space models. For that, we use GraphEM, a recently presented expectation-maximisation algorithm for estimating the linear matrix operator in the state equation of a linear-Gaussian state-space model. Lasso regularisation is included in the M-step, which is solved using a proximal splitting Douglas-Rachford algorithm. Experiments in toy examples and challenging climate problems illustrate the benefits of the proposed model and inference technique over standard Granger causality methods.
    
[^32]: 自我自我+: 利用自监督学习和图像质量评估损失的单图像去噪方法

    Self2Self+: Single-Image Denoising with Self-Supervised Learning and Image Quality Assessment Loss. (arXiv:2307.10695v1 [cs.CV])

    [http://arxiv.org/abs/2307.10695](http://arxiv.org/abs/2307.10695)

    该论文提出了一种新的单图像去噪方法，利用自监督学习和图像质量评估损失，只使用噪声输入进行网络训练，并通过平均不同实例的预测结果提高性能。

    

    最近，基于监督学习的去噪方法展现了令人期待的性能。然而，它们对包含噪声-干净图像对的外部数据集的依赖限制了它们的适用性。为了解决这个限制，研究人员开始关注使用仅包含噪声输入的一组输入来训练去噪网络。为了改善去噪过程的可行性，在本研究中，我们提出了一种单图像自监督学习方法，只使用噪声输入图像进行网络训练。使用门控卷积进行特征提取，使用无参考图像质量评估来指导训练过程。此外，我们提出的方法使用伯努利采样从输入图像数据集中选择实例进行训练，采用一定的dropout率。通过对训练过的网络的不同实例进行预测，并进行平均，生成相应的结果。实验结果表明，提出的方法取得了较好的性能。

    Recently, denoising methods based on supervised learning have exhibited promising performance. However, their reliance on external datasets containing noisy-clean image pairs restricts their applicability. To address this limitation, researchers have focused on training denoising networks using solely a set of noisy inputs. To improve the feasibility of denoising procedures, in this study, we proposed a single-image self-supervised learning method in which only the noisy input image is used for network training. Gated convolution was used for feature extraction and no-reference image quality assessment was used for guiding the training process. Moreover, the proposed method sampled instances from the input image dataset using Bernoulli sampling with a certain dropout rate for training. The corresponding result was produced by averaging the generated predictions from various instances of the trained network with dropouts. The experimental results indicated that the proposed method achie
    
[^33]: 分数降噪用于3D分子预训练

    Fractional Denoising for 3D Molecular Pre-training. (arXiv:2307.10683v1 [q-bio.QM])

    [http://arxiv.org/abs/2307.10683](http://arxiv.org/abs/2307.10683)

    本论文提出了一种分数降噪算法，用于3D分子预训练。通过混合噪声策略解决了样本覆盖率低和各向同性力场的挑战，通过解耦两种类型的噪声克服了传统降噪方法无法学习力场的问题。

    

    坐标降噪是一种有前途的3D分子预训练方法，在各种下游药物发现任务中取得了显著的性能。从理论上讲，其目标等同于学习力场，并且对下游任务有帮助。然而，坐标降噪学习有效力场面临两个挑战，即样本覆盖率低和各向同性力场。根本原因在于现有降噪方法所假设的分子分布不能捕捉分子的各向异性特征。为了解决这些挑战，我们提出了一种新的混合噪声策略，包括二面角和坐标的噪声。然而，以传统方式降噪这种混合噪声不再等同于学习力场。通过理论推导，我们发现问题是由于输入构象对协方差的依赖性所导致的。因此，我们提出将这两种类型的噪声解耦

    Coordinate denoising is a promising 3D molecular pre-training method, which has achieved remarkable performance in various downstream drug discovery tasks. Theoretically, the objective is equivalent to learning the force field, which is revealed helpful for downstream tasks. Nevertheless, there are two challenges for coordinate denoising to learn an effective force field, i.e. low coverage samples and isotropic force field. The underlying reason is that molecular distributions assumed by existing denoising methods fail to capture the anisotropic characteristic of molecules. To tackle these challenges, we propose a novel hybrid noise strategy, including noises on both dihedral angel and coordinate. However, denoising such hybrid noise in a traditional way is no more equivalent to learning the force field. Through theoretical deductions, we find that the problem is caused by the dependency of the input conformation for covariance. To this end, we propose to decouple the two types of nois
    
[^34]: 深度学习用于噪声二维码的分类

    Deep learning for classification of noisy QR codes. (arXiv:2307.10677v1 [cs.LG])

    [http://arxiv.org/abs/2307.10677](http://arxiv.org/abs/2307.10677)

    该论文研究了在应用于抽象图像分类时，基于深度学习的经典分类模型的限制。通过将该模型应用于噪声二维码的分类，研究者发现深度学习模型在理解抽象图像方面具有相关性。

    

    我们希望定义基于深度学习的经典分类模型在应用于抽象图像（不代表可视可识别对象的图像）时的限制。二维码属于这种抽象图像的一类：一个比特对应一个编码字符，二维码并不是为了手动解码而设计的。为了理解深度学习模型在抽象图像分类中的局限性，我们使用健康通行证上的信息生成了二维码，并对其进行图像分类模型训练。我们将分类模型与经典的（确定性）解码方法在存在噪声的情况下进行比较。这项研究使我们得出结论：基于深度学习的模型对于理解抽象图像是相关的。

    We wish to define the limits of a classical classification model based on deep learning when applied to abstract images, which do not represent visually identifiable objects.QR codes (Quick Response codes) fall into this category of abstract images: one bit corresponding to one encoded character, QR codes were not designed to be decoded manually. To understand the limitations of a deep learning-based model for abstract image classification, we train an image classification model on QR codes generated from information obtained when reading a health pass. We compare a classification model with a classical (deterministic) decoding method in the presence of noise. This study allows us to conclude that a model based on deep learning can be relevant for the understanding of abstract images.
    
[^35]: 在联邦学习中分享什么：模型效用、隐私泄露和通信效率的视角综述

    A Survey of What to Share in Federated Learning: Perspectives on Model Utility, Privacy Leakage, and Communication Efficiency. (arXiv:2307.10655v1 [cs.LG])

    [http://arxiv.org/abs/2307.10655](http://arxiv.org/abs/2307.10655)

    本文介绍了一篇系统综述，从新的视角探讨了在联邦学习中应该分享什么，注重模型效用、隐私泄露和通信效率。

    

    联邦学习（FL）已成为一种高效的隐私保护合作训练范式，可以在不暴露私有数据集的情况下，允许客户端共享隐私保护信息。这种方法不仅保证了增强的隐私保护，而且促进了多方之间更高效、更安全的合作。因此，FL引起了研究人员的广泛关注，推动了许多综述性文章对相关工作进行总结。然而，大多数综述集中于在训练过程中共享模型参数的方法，而忽视了共享其他形式的本地信息的潜力。本文从一种新的视角出发，即在FL中分享什么，重点关注模型效用、隐私泄露和通信效率，进行了系统综述。

    Federated learning (FL) has emerged as a highly effective paradigm for privacy-preserving collaborative training among different parties. Unlike traditional centralized learning, which requires collecting data from each party, FL allows clients to share privacy-preserving information without exposing private datasets. This approach not only guarantees enhanced privacy protection but also facilitates more efficient and secure collaboration among multiple participants. Therefore, FL has gained considerable attention from researchers, promoting numerous surveys to summarize the related works. However, the majority of these surveys concentrate on methods sharing model parameters during the training process, while overlooking the potential of sharing other forms of local information. In this paper, we present a systematic survey from a new perspective, i.e., what to share in FL, with an emphasis on the model utility, privacy leakage, and communication efficiency. This survey differs from pr
    
[^36]: 条件期望网络用于SHAP

    Conditional expectation network for SHAP. (arXiv:2307.10654v1 [cs.LG])

    [http://arxiv.org/abs/2307.10654](http://arxiv.org/abs/2307.10654)

    这项工作提出了一种用于计算条件版本的（代理）神经网络方法，该方法可以有效地解释神经网络和其他回归模型的预测结果，并考虑特征之间的依赖关系。同时，该方法还可以应用于复杂回归模型的分析，并提供正确的偏依赖图表示。

    

    SHAP是一种非常流行的模型无关技术，用于解释预测模型。SHAP的两个最受欢迎的版本是条件期望版本和无条件期望版本（后者也称为干预SHAP）。除了基于树的方法之外，通常使用无条件版本（出于计算原因）。我们提供了一种（代理）神经网络方法，可以高效地计算神经网络和其他回归模型的条件版本，并正确考虑特征组件之间的依赖结构。这个方法还可以用于提供与广义线性模型（GLM）类似的复杂回归模型的drop1和anova分析，并提供考虑特征组件中正确依赖结构的偏依赖图（PDP）的对应版本。

    A very popular model-agnostic technique for explaining predictive models is the SHapley Additive exPlanation (SHAP). The two most popular versions of SHAP are a conditional expectation version and an unconditional expectation version (the latter is also known as interventional SHAP). Except for tree-based methods, usually the unconditional version is used (for computational reasons). We provide a (surrogate) neural network approach which allows us to efficiently calculate the conditional version for both neural networks and other regression models, and which properly considers the dependence structure in the feature components. This proposal is also useful to provide drop1 and anova analyses in complex regression models which are similar to their generalized linear model (GLM) counterparts, and we provide a partial dependence plot (PDP) counterpart that considers the right dependence structure in the feature components.
    
[^37]: 自动单变量时间序列异常检测在监控服务中的优化目标细化

    Refining the Optimization Target for Automatic Univariate Time Series Anomaly Detection in Monitoring Services. (arXiv:2307.10653v1 [cs.LG])

    [http://arxiv.org/abs/2307.10653](http://arxiv.org/abs/2307.10653)

    本文提出了一个全面的框架，用于时间序列异常检测模型中自动参数优化。框架引入了三个优化目标：预测得分、形状得分和敏感度得分，并成功在线应用了六个月以上，每分钟为超过50,000个时间序列提供服务。该框架简化了用户体验，提供了用户友好的界面，并实现了期望的检测结果。

    

    时间序列异常检测对于处理大量数据、确保可靠性和优化系统性能的工业监控服务至关重要。现有的方法通常需要大量标注资源和手动参数选择，强调了自动化的需求。本文提出了一个全面的框架，用于时间序列异常检测模型中自动参数优化。该框架引入了三个优化目标：预测得分、形状得分和敏感度得分，可以在不需要先验知识或手动标注的情况下轻松适应不同的模型背景。所提出的框架已成功在线应用了六个月以上，每分钟为超过50,000个时间序列提供服务。它通过只需一个预期敏感值简化了用户的体验，提供了用户友好的界面，并实现了期望的检测结果。在公共数据集上进行了大量评估和比较。

    Time series anomaly detection is crucial for industrial monitoring services that handle a large volume of data, aiming to ensure reliability and optimize system performance. Existing methods often require extensive labeled resources and manual parameter selection, highlighting the need for automation. This paper proposes a comprehensive framework for automatic parameter optimization in time series anomaly detection models. The framework introduces three optimization targets: prediction score, shape score, and sensitivity score, which can be easily adapted to different model backbones without prior knowledge or manual labeling efforts. The proposed framework has been successfully applied online for over six months, serving more than 50,000 time series every minute. It simplifies the user's experience by requiring only an expected sensitive value, offering a user-friendly interface, and achieving desired detection results. Extensive evaluations conducted on public datasets and comparison
    
[^38]: 基于数据驱动的无线网络延迟概率预测：重点关注尾部概率

    Data-Driven Latency Probability Prediction for Wireless Networks: Focusing on Tail Probabilities. (arXiv:2307.10648v1 [cs.NI])

    [http://arxiv.org/abs/2307.10648](http://arxiv.org/abs/2307.10648)

    本研究提出了基于数据驱动方法的无线网络延迟概率预测，重点关注尾部概率分布。该方法可用于准确估计罕见延迟的可能性，并制定网络资源配置策略。

    

    随着新的应用领域的出现，例如网络物理系统和人机协作应用，需要以极高的可靠性保证一定水平的端到端网络延迟，例如99.999%。虽然IEEE 802.1as时敏网络（TSN）规范中的机制可以用来实现以太网交换网络的这些要求，但在无线网络中实现TSN机制具有挑战性，因为其具有随机性质。为了将无线链路合规到99.999%的可靠度水平，必须分析和控制延迟概率分布中极为罕见的异常值，即分布的尾部。本研究提出使用最先进的数据驱动方法，如混合密度网络（MDN）和极值混合模型，对延迟分布的尾部进行预测，以估计基于网络参数的罕见延迟的可能性，从而可以制定更加精确的网络资源配置策略。

    With the emergence of new application areas, such as cyber-physical systems and human-in-the-loop applications, there is a need to guarantee a certain level of end-to-end network latency with extremely high reliability, e.g., 99.999%. While mechanisms specified under IEEE 802.1as time-sensitive networking (TSN) can be used to achieve these requirements for switched Ethernet networks, implementing TSN mechanisms in wireless networks is challenging due to their stochastic nature. To conform the wireless link to a reliability level of 99.999%, the behavior of extremely rare outliers in the latency probability distribution, or the tail of the distribution, must be analyzed and controlled. This work proposes predicting the tail of the latency distribution using state-of-the-art data-driven approaches, such as mixture density networks (MDN) and extreme value mixture models, to estimate the likelihood of rare latencies conditioned on the network parameters, which can be used to make more info
    
[^39]: Fisher-Rao距离和逆推到SPD锥距离在多元正态分布之间的应用

    Fisher-Rao distance and pullback SPD cone distances between multivariate normal distributions. (arXiv:2307.10644v1 [cs.LG])

    [http://arxiv.org/abs/2307.10644](http://arxiv.org/abs/2307.10644)

    本研究提出了一种快速和鲁棒的方法来近似计算多元正态分布之间的Fisher-Rao距离，并引入了一类基于正态流形嵌入到高维对称正定锥子流形的距离。

    

    许多科学领域，如扩散张量成像、结构张量计算机视觉、雷达信号处理和机器学习等，都存在着多元正态分布的数据集。为了处理这些正态数据集以进行过滤、分类或聚类等下游任务，需要定义合适的正态和它们之间的路径之间的差异度量。Fisher-Rao距离，作为Fisher信息度量引起的Riemann几何距离，是一种合理的度量距离，但除了一些特殊情况外，并没有闭式求解。本文首先报告了一种快速且鲁棒的方法，可以精确地近似计算多元正态分布之间的Fisher-Rao距离。其次，我们介绍了一类基于正态流形到高维对称正定锥的子流形的微分同胚嵌入的距离。

    Data sets of multivariate normal distributions abound in many scientific areas like diffusion tensor imaging, structure tensor computer vision, radar signal processing, machine learning, just to name a few. In order to process those normal data sets for downstream tasks like filtering, classification or clustering, one needs to define proper notions of dissimilarities between normals and paths joining them. The Fisher-Rao distance defined as the Riemannian geodesic distance induced by the Fisher information metric is such a principled metric distance which however is not known in closed-form excepts for a few particular cases. In this work, we first report a fast and robust method to approximate arbitrarily finely the Fisher-Rao distance between multivariate normal distributions. Second, we introduce a class of distances based on diffeomorphic embeddings of the normal manifold into a submanifold of the higher-dimensional symmetric positive-definite cone corresponding to the manifold of
    
[^40]: SciBench: 对大型语言模型评估大学水平的科学问题解决能力

    SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models. (arXiv:2307.10635v1 [cs.CL])

    [http://arxiv.org/abs/2307.10635](http://arxiv.org/abs/2307.10635)

    这篇论文介绍了一个名为SciBench的基准套件，旨在对大型语言模型的大学水平科学问题解决能力进行评估。研究结果显示，当前的语言模型在提供复杂科学问题解决能力方面还有不足之处。

    

    最近大型语言模型(LLMs)的进展在许多数学基准上取得了显著的进步。然而，这些基准大多只包含初高中科目的问题，仅包含多项选择题，并且仅限于基本算术运算范围。为了解决这些问题，本文介绍了一个广泛的基准套件SciBench，旨在系统地检测复杂科学问题解决所需的推理能力。SciBench包含两个经过精心策划的数据集：一个开放集，包括从数学、化学和物理教科书中摘录的大学水平的科学问题，以及一个封闭集，包含来自计算机科学和数学本科考试的问题。基于这两个数据集，我们对两个代表性的LLM进行了深入的基准研究，并采用不同的提示策略。结果表明，当前的LLMs在提供复杂科学问题解决能力方面还存在不足之处。

    Recent advances in large language models (LLMs) have demonstrated notable progress on many mathematical benchmarks. However, most of these benchmarks only feature problems grounded in junior and senior high school subjects, contain only multiple-choice questions, and are confined to a limited scope of elementary arithmetic operations. To address these issues, this paper introduces an expansive benchmark suite SciBench that aims to systematically examine the reasoning capabilities required for complex scientific problem solving. SciBench contains two carefully curated datasets: an open set featuring a range of collegiate-level scientific problems drawn from mathematics, chemistry, and physics textbooks, and a closed set comprising problems from undergraduate-level exams in computer science and mathematics. Based on the two datasets, we conduct an in-depth benchmark study of two representative LLMs with various prompting strategies. The results reveal that current LLMs fall short of deli
    
[^41]: 人类基因核苷酸序列的生成语言模型

    Generative Language Models on Nucleotide Sequences of Human Genes. (arXiv:2307.10634v1 [q-bio.GN])

    [http://arxiv.org/abs/2307.10634](http://arxiv.org/abs/2307.10634)

    本研究开发了一种生成语言模型，用于处理人类基因的核苷酸序列，填补了DNA序列生成模型研究的空白。

    

    自然语言处理领域的语言模型，特别是基于Transformer的模型，取得了巨大的成功。然而，在DNA相关的生物信息学领域，生成模型的研究相对较少。因此，本研究旨在开发一种类似于GPT-3的自回归生成语言模型，用于处理人类基因的核苷酸序列。考虑到处理整个DNA序列需要大量计算资源，我们决定在更小的尺度上进行研究，重点关注人类基因的核苷酸序列，而不是整个DNA。这个决策并不改变问题的结构，因为DNA和基因都可以看作由四种不同的核苷酸组成的一维序列。

    Language models, primarily transformer-based ones, obtained colossal success in NLP. To be more precise, studies like BERT in NLU and works such as GPT-3 for NLG are very crucial. DNA sequences are very close to natural language in terms of structure, so if the DNA-related bioinformatics domain is concerned, discriminative models, like DNABert, exist. Yet, the generative side of the coin is mainly unexplored to the best of our knowledge. Consequently, we focused on developing an autoregressive generative language model like GPT-3 for DNA sequences. Because working with whole DNA sequences is challenging without substantial computational resources, we decided to carry out our study on a smaller scale, focusing on nucleotide sequences of human genes, unique parts in DNA with specific functionalities, instead of the whole DNA. This decision did not change the problem structure a lot due to the fact that both DNA and genes can be seen as 1D sequences consisting of four different nucleotide
    
[^42]: 多方法自训练：通过文本改进代码生成，反之亦然

    Multi-Method Self-Training: Improving Code Generation With Text, And Vice Versa. (arXiv:2307.10633v1 [cs.CL])

    [http://arxiv.org/abs/2307.10633](http://arxiv.org/abs/2307.10633)

    多方法自训练可以通过在不同方法之间训练和生成数据来改善代码生成的性能，使模型更易于使用，并提高相关任务的性能。

    

    大型语言模型有许多解决同一问题的方法。这引入了新颖的优点（不同的方法可能对不同的问题有效），以及缺点（用户可能难以知道使用哪种方法）。在本文中，我们介绍了多方法自训练（MMST）方法，其中一种方法是在另一种方法的筛选输出上进行训练，从而增强每种方法的优点并改善它们的缺点。使用176B参数的语言和代码训练模型，我们证明MMST可以1）改善性能较差的方法（高达30%），使模型更易于使用，2）改善性能较好的方法（高达32.2%），使模型性能更优秀，以及3）通过改善模型生成解释能力，提高相关但不同任务的性能（高达10.3%）。然后，我们进行消融分析，探讨MMST的工作原理。我们证明MMST比传统的自训练方法生成更多数据，但性能提升更明显。

    Large Language Models have many methods for solving the same problem. This introduces novel strengths (different methods may work well for different problems) and weaknesses (it may be difficult for users to know which method to use). In this paper, we introduce Multi-Method Self-Training (MMST), where one method is trained on the filtered outputs of another, allowing us to augment the strengths and ameliorate the weaknesses of each method. Using a 176B parameter model trained on both language and code, we show that MMST can 1) improve the less performant method (up to 30%) making the model easier to use, 2) improve the more performant method (up to 32.2%) making the model more performant, and 3) improve the performance of related but distinct tasks (up to 10.3%) by improving the ability of the model to generate rationales. We then conduct ablation analyses to explore why MMST works. We show that MMST generates more data than traditional self-training, but the improvement in performanc
    
[^43]: 使用文本分类检测虚假评论

    Detecting deceptive reviews using text classification. (arXiv:2307.10617v1 [cs.IR])

    [http://arxiv.org/abs/2307.10617](http://arxiv.org/abs/2307.10617)

    这篇论文提出了一种使用机器学习模型的方法来识别虚假评论，并通过在餐馆评论的数据集上进行实验验证了其性能。

    

    近年来，在线评论在推广任何产品或服务方面发挥着重要作用。企业可能会嵌入虚假评论以吸引客户购买他们的产品。他们甚至可能突出强调自己产品的优点或批评竞争对手的产品。市场营销人员、广告商和其他在线商业用户有动机为他们想要推广的产品编写虚假的正面评论，或者为他们真正不喜欢的产品提供虚假的负面评论。因此，识别虚假评论是一个紧迫且持续的研究领域。本研究论文提出了一种机器学习模型方法来识别虚假评论。论文调查了在一个餐馆评论的虚假意见垃圾语料库数据集上进行的多次实验的性能。我们采用了n-gram模型和最大特征来识别虚假评论。

    In recent years, online reviews play a vital role for promoting any kind of product or services. Businesses may embed fake reviews in order to attract customers to purchase their products. They may even highlight the benefits of their own product or criticize the competition's product. Marketers, advertisers, and other online business users have incentive to create fake positive reviews for products which they want to promote or give fake negative reviews for products which they really don't like. So now-a-days writing a deceptive review is inevitable thing for promoting their own business or degrading competitor's reputation. Thus, identifying deceptive reviews is an intense and on-going research area. This research paper proposes machine learning model approach to identify deceptive reviews. The paper investigates the performance of the several experiments done on a Deceptive Opinion Spam Corpus dataset of restaurants reviews. We developed a n-gram model and max features to identify 
    
[^44]: 异构联邦学习：现状与研究挑战的技术综述

    Heterogeneous Federated Learning: State-of-the-art and Research Challenges. (arXiv:2307.10616v1 [cs.LG])

    [http://arxiv.org/abs/2307.10616](http://arxiv.org/abs/2307.10616)

    异构联邦学习是联邦学习领域中的一个重要研究方向，涉及到数据分布、模型架构、网络环境和硬件设备的异质性挑战。本文对异构联邦学习的研究挑战和最新进展进行了综述和分类，为进一步的研究提供了参考。

    

    联邦学习 (FL) 由于在大规模工业应用中的潜在用途而受到越来越多的关注。现有的联邦学习研究主要针对模型同质的情况。然而，实际的联邦学习通常面临参与方之间的数据分布、模型架构、网络环境和硬件设备的异质性。异构联邦学习 (HFL) 更具挑战性，相应的解决方案多样且复杂。因此，对这个主题进行关于研究挑战和最新进展的系统调查至关重要。在这项调查中，我们首先总结了 HFL 中来自五个方面的各种研究挑战：统计异质性、模型异质性、通信异质性、设备异质性和额外挑战。此外，我们回顾了 HFL 中的最新进展，并提出了对现有 HFL 方法的新分类法，并对其优缺点进行了深入分析。

    Federated learning (FL) has drawn increasing attention owing to its potential use in large-scale industrial applications. Existing federated learning works mainly focus on model homogeneous settings. However, practical federated learning typically faces the heterogeneity of data distributions, model architectures, network environments, and hardware devices among participant clients. Heterogeneous Federated Learning (HFL) is much more challenging, and corresponding solutions are diverse and complex. Therefore, a systematic survey on this topic about the research challenges and state-of-the-art is essential. In this survey, we firstly summarize the various research challenges in HFL from five aspects: statistical heterogeneity, model heterogeneity, communication heterogeneity, device heterogeneity, and additional challenges. In addition, recent advances in HFL are reviewed and a new taxonomy of existing HFL methods is proposed with an in-depth analysis of their pros and cons. We classify
    
[^45]: 基于集成学习的物联网网络安全异常检测方法通过贝叶斯超参数敏感性分析

    Ensemble Learning based Anomaly Detection for IoT Cybersecurity via Bayesian Hyperparameters Sensitivity Analysis. (arXiv:2307.10596v1 [cs.LG])

    [http://arxiv.org/abs/2307.10596](http://arxiv.org/abs/2307.10596)

    本论文基于集成学习方法，通过贝叶斯超参数敏感性分析，提出了一种用于物联网网络安全异常检测的方法，该方法充分利用了物联网数据的异构性和多样性特征。

    

    物联网整合了全球数十亿智能设备，具备与其他连接设备进行沟通的能力，实现几乎无需人为干预。物联网能够进行大规模的数据聚合和分析，从而提升各个领域的生活质量。特别是，物联网收集的数据对于异常检测非常有用。物联网的异构性既是网络安全的挑战，也是机会。传统的网络安全监测方法通常需要对不同类型的数据进行预处理和处理，这对于包含异构特征的数据集可能会存在问题。然而，异构类型的网络设备往往可以捕获比单一类型设备读数更多样化的信号，这对于异常检测特别有用。在本文中，我们提出了一项关于使用集成机器学习方法增强物联网网络安全异常检测的全面研究。

    The Internet of Things (IoT) integrates more than billions of intelligent devices over the globe with the capability of communicating with other connected devices with little to no human intervention. IoT enables data aggregation and analysis on a large scale to improve life quality in many domains. In particular, data collected by IoT contain a tremendous amount of information for anomaly detection. The heterogeneous nature of IoT is both a challenge and an opportunity for cybersecurity. Traditional approaches in cybersecurity monitoring often require different kinds of data pre-processing and handling for various data types, which might be problematic for datasets that contain heterogeneous features. However, heterogeneous types of network devices can often capture a more diverse set of signals than a single type of device readings, which is particularly useful for anomaly detection. In this paper, we present a comprehensive study on using ensemble machine learning methods for enhanc
    
[^46]: 预测电动汽车充电行为：采用微聚类和SMOTE技术的深度学习方法

    Forecasting Battery Electric Vehicle Charging Behavior: A Deep Learning Approach Equipped with Micro-Clustering and SMOTE Techniques. (arXiv:2307.10588v1 [cs.LG])

    [http://arxiv.org/abs/2307.10588](http://arxiv.org/abs/2307.10588)

    本研究开发了一种使用微聚类和SMOTE技术的深度学习方法，能够准确预测电动汽车充电事件，为电力负荷聚合器和电力管理人员提供提供充电站和电力容量的信息。

    

    能源系统、气候变化和公共健康是推动交通电气化的主要原因。全球范围内正在推广交通电气化以减少排放。因此，许多汽车制造商将很快开始只生产电池电动汽车（BEV）。由于气候变化和空气污染的担忧，加利福尼亚的BEV采用率正在上升。虽然这对于气候和污染目标来说很好，但未妥善管理的BEV充电可能导致充电基础设施不足和停电。本研究开发了一种新颖的微聚类深度神经网络（MCDNN），该算法在学习BEV行程和充电数据以预测BEV充电事件方面非常有效，这对于电力负荷聚合器和电力管理人员有效提供充电站和电力容量的信息至关重要。MCDNN使用加利福尼亚发生的行程和充电的稳健数据集进行配置。

    Energy systems, climate change, and public health are among the primary reasons for moving toward electrification in transportation. Transportation electrification is being promoted worldwide to reduce emissions. As a result, many automakers will soon start making only battery electric vehicles (BEVs). BEV adoption rates are rising in California, mainly due to climate change and air pollution concerns. While great for climate and pollution goals, improperly managed BEV charging can lead to insufficient charging infrastructure and power outages. This study develops a novel Micro Clustering Deep Neural Network (MCDNN), an artificial neural network algorithm that is highly effective at learning BEVs trip and charging data to forecast BEV charging events, information that is essential for electricity load aggregators and utility managers to provide charging stations and electricity capacity effectively. The MCDNN is configured using a robust dataset of trips and charges that occurred in Ca
    
[^47]: 机器学习系统可靠性的整体评估

    A Holistic Assessment of the Reliability of Machine Learning Systems. (arXiv:2307.10586v1 [cs.LG])

    [http://arxiv.org/abs/2307.10586](http://arxiv.org/abs/2307.10586)

    本文提出了一种用于评估机器学习系统可靠性的整体评估方法，通过评估分布内准确性、分布偏移鲁棒性、对抗鲁棒性、校准性和越界检测能力等五个关键属性，引入了可靠性得分来评估整个系统的可靠性。

    

    随着机器学习系统越来越多地渗透到医疗保健、交通运输、军事和国家安全等高风险领域，人们对其可靠性产生了担忧。尽管取得了显著进展，但由于对抗攻击或环境变化，这些系统的性能可能显著降低，导致预测过于自信、无法检测输入故障以及在意外场景中无法泛化。本文提出了一种用于评估机器学习系统可靠性的整体评估方法。我们的框架评估了五个关键属性：分布内准确性、分布偏移鲁棒性、对抗鲁棒性、校准性和越界检测能力。我们还引入了一个可靠性得分来评估整个系统的可靠性。为了提供不同算法方法性能的见解，我们识别和分类了最先进的技术，然后使用真实世界的任务对其中的一些进行了评估。

    As machine learning (ML) systems increasingly permeate high-stakes settings such as healthcare, transportation, military, and national security, concerns regarding their reliability have emerged. Despite notable progress, the performance of these systems can significantly diminish due to adversarial attacks or environmental changes, leading to overconfident predictions, failures to detect input faults, and an inability to generalize in unexpected scenarios. This paper proposes a holistic assessment methodology for the reliability of ML systems. Our framework evaluates five key properties: in-distribution accuracy, distribution-shift robustness, adversarial robustness, calibration, and out-of-distribution detection. A reliability score is also introduced and used to assess the overall system reliability. To provide insights into the performance of different algorithmic approaches, we identify and categorize state-of-the-art techniques, then evaluate a selection on real-world tasks using
    
[^48]: 中国海上海雾预测的智能模型

    Intelligent model for offshore China sea fog forecasting. (arXiv:2307.10580v1 [cs.LG])

    [http://arxiv.org/abs/2307.10580](http://arxiv.org/abs/2307.10580)

    本研究开发了一种嵌入数值天气预报模型的高级海上海雾预测方法，在解析驱动因素和处理不平衡数据的基础上，提升了海雾预测的准确性和预测能力。

    

    准确和及时地预测海上海雾对有效管理海上和沿岸经济活动非常重要。鉴于海上海雾的复杂性和固有变异性，传统的数值和统计预测方法往往不足以应对。本研究旨在开发一种高级海上海雾预测方法，将其嵌入数值天气预报模型中，并以长江口沿海地区为案例研究。在训练我们的机器学习模型之前，我们采用时滞相关分析技术来识别关键预测因子，并解析驱动海上海雾发生的基本机制。此外，我们采用集成学习和焦点损失函数来解决不平衡数据的问题，从而提升我们模型的预测能力。为了验证我们方法的准确性，我们使用包括气象站观测数据和历史数据的全面数据集进行性能评估。

    Accurate and timely prediction of sea fog is very important for effectively managing maritime and coastal economic activities. Given the intricate nature and inherent variability of sea fog, traditional numerical and statistical forecasting methods are often proven inadequate. This study aims to develop an advanced sea fog forecasting method embedded in a numerical weather prediction model using the Yangtze River Estuary (YRE) coastal area as a case study. Prior to training our machine learning model, we employ a time-lagged correlation analysis technique to identify key predictors and decipher the underlying mechanisms driving sea fog occurrence. In addition, we implement ensemble learning and a focal loss function to address the issue of imbalanced data, thereby enhancing the predictive ability of our model. To verify the accuracy of our method, we evaluate its performance using a comprehensive dataset spanning one year, which encompasses both weather station observations and histori
    
[^49]: 通过多目标联邦学习对SecureBoost超参数进行调优的方法

    SecureBoost Hyperparameter Tuning via Multi-Objective Federated Learning. (arXiv:2307.10579v1 [cs.LG])

    [http://arxiv.org/abs/2307.10579](http://arxiv.org/abs/2307.10579)

    提出了一种通过多目标联邦学习来调优SecureBoost超参数的方法，以找到在效用、效率和隐私之间最佳平衡的一组超参数解决方案。

    

    SecureBoost是一种利用同态加密保护垂直联邦学习中数据隐私的树提升算法。由于其可解释性、效果和隐私保护能力，在金融和医疗保健等领域广泛应用。然而，SecureBoost存在计算复杂性高和标签泄漏风险的问题。为了充分发挥SecureBoost的潜力，需要仔细选择SecureBoost的超参数，以在效用、效率和隐私之间达到最佳平衡。现有方法要么经验性地设置超参数，要么启发式地设置超参数，远未达到最优。为了弥补这一差距，我们提出了一种约束多目标SecureBoost (CMOSB) 算法，以寻找每个解都是在效用损失、训练成本和隐私泄漏之间实现最佳权衡的一组超参数的Pareto最优解。我们设计了三个目标的度量方法。特别是，隐私泄漏是用... (此处省略)

    SecureBoost is a tree-boosting algorithm leveraging homomorphic encryption to protect data privacy in vertical federated learning setting. It is widely used in fields such as finance and healthcare due to its interpretability, effectiveness, and privacy-preserving capability. However, SecureBoost suffers from high computational complexity and risk of label leakage. To harness the full potential of SecureBoost, hyperparameters of SecureBoost should be carefully chosen to strike an optimal balance between utility, efficiency, and privacy. Existing methods either set hyperparameters empirically or heuristically, which are far from optimal. To fill this gap, we propose a Constrained Multi-Objective SecureBoost (CMOSB) algorithm to find Pareto optimal solutions that each solution is a set of hyperparameters achieving optimal tradeoff between utility loss, training cost, and privacy leakage. We design measurements of the three objectives. In particular, the privacy leakage is measured using 
    
[^50]: 使用原型正则化提升联邦学习的收敛性

    Boosting Federated Learning Convergence with Prototype Regularization. (arXiv:2307.10575v1 [cs.LG])

    [http://arxiv.org/abs/2307.10575](http://arxiv.org/abs/2307.10575)

    本论文通过引入原型正则化策略来解决联邦学习中异质数据分布的问题，实验证明在MNIST和Fashion-MNIST数据集上相比基准模型FedAvg，我们的方法分别提高了3.3%和8.9%的平均测试准确率，并且在异构设置下具有快速收敛速度。

    

    作为一种分布式机器学习技术，联邦学习（FL）要求客户端在不泄露本地数据的情况下与边缘服务器共同训练共享模型。然而，客户端之间的异构数据分布往往导致模型性能下降。为了解决这个问题，本文引入了一种基于原型的正则化策略来解决数据分布的异质性。具体而言，正则化过程涉及服务器从分布式客户端聚合本地原型以生成全局原型，然后将其发送回个体客户端以指导其本地训练。在MNIST和Fashion-MNIST上的实验结果表明，与最流行的基准FedAvg相比，我们的提议分别在平均测试准确率上实现了3.3%和8.9%的改进。此外，我们的方法在异构环境中具有快速收敛速度。

    As a distributed machine learning technique, federated learning (FL) requires clients to collaboratively train a shared model with an edge server without leaking their local data. However, the heterogeneous data distribution among clients often leads to a decrease in model performance. To tackle this issue, this paper introduces a prototype-based regularization strategy to address the heterogeneity in the data distribution. Specifically, the regularization process involves the server aggregating local prototypes from distributed clients to generate a global prototype, which is then sent back to the individual clients to guide their local training. The experimental results on MNIST and Fashion-MNIST show that our proposal achieves improvements of 3.3% and 8.9% in average test accuracy, respectively, compared to the most popular baseline FedAvg. Furthermore, our approach has a fast convergence rate in heterogeneous settings.
    
[^51]: 欺骗性对齐监测

    Deceptive Alignment Monitoring. (arXiv:2307.10569v1 [cs.LG])

    [http://arxiv.org/abs/2307.10569](http://arxiv.org/abs/2307.10569)

    本论文提出了欺骗性对齐监测这一新方向，旨在探讨大型机器学习模型在表面上表现正常，却暗中进行隐藏行为的问题，并提出了新的研究机会。

    

    随着大型机器学习模型的能力不断增长，以及对这些模型的自治权不断扩大，一个新的对手出现了：模型本身。一个模型看似合理地行为，却暗中、微妙地修改其行为以达到别的目的的威胁，通常在AI安全与对齐社区中被称为欺骗性对齐。因此，我们将这个新方向称为欺骗性对齐监测。在这项工作中，我们确定了机器学习不同子领域中的新兴方向，我们认为在不久的将来对欺骗性对齐监测会变得越来越重要且紧密相关，并且我们认为这些领域的进步既提出了长期挑战，也带来了新的研究机会。最后，我们呼吁对抗性机器学习社区更多地参与这些新兴方向的研究。

    As the capabilities of large machine learning models continue to grow, and as the autonomy afforded to such models continues to expand, the spectre of a new adversary looms: the models themselves. The threat that a model might behave in a seemingly reasonable manner, while secretly and subtly modifying its behavior for ulterior reasons is often referred to as deceptive alignment in the AI Safety & Alignment communities. Consequently, we call this new direction Deceptive Alignment Monitoring. In this work, we identify emerging directions in diverse machine learning subfields that we believe will become increasingly important and intertwined in the near future for deceptive alignment monitoring, and we argue that advances in these fields present both long-term challenges and new research opportunities. We conclude by advocating for greater involvement by the adversarial machine learning community in these emerging directions.
    
[^52]: FACADE：一种用于对抗电路异常检测和评估的框架

    FACADE: A Framework for Adversarial Circuit Anomaly Detection and Evaluation. (arXiv:2307.10563v1 [cs.LG])

    [http://arxiv.org/abs/2307.10563](http://arxiv.org/abs/2307.10563)

    FACADE是一种新型的概率和几何框架，用于无监督检测深度神经网络中的机理异常，并提供关键的洞察力和强大工具，以揭示和对抗对抗攻击，并在实际部署环境中展示了有前途的应用。

    

    我们提出了FACADE，一种新颖的概率和几何框架，旨在对深度神经网络中进行无监督的机理异常检测。其主要目标是推进对抗攻击的理解和减轻。FACADE旨在生成电路上的概率分布，这为伪类的流形特性变化以及激活空间中高维模式的贡献提供了关键的洞察力，从而为揭示和对抗对抗攻击提供了强大工具。我们的方法旨在提高模型的鲁棒性，增强可扩展模型监控，并在实际部署环境中展示了有前途的应用。

    We present FACADE, a novel probabilistic and geometric framework designed for unsupervised mechanistic anomaly detection in deep neural networks. Its primary goal is advancing the understanding and mitigation of adversarial attacks. FACADE aims to generate probabilistic distributions over circuits, which provide critical insights to their contribution to changes in the manifold properties of pseudo-classes, or high-dimensional modes in activation space, yielding a powerful tool for uncovering and combating adversarial attacks. Our approach seeks to improve model robustness, enhance scalable model oversight, and demonstrates promising applications in real-world deployment settings.
    
[^53]: 共享对抗性遗忘：通过遗忘共享对抗样本来减轻后门攻击

    Shared Adversarial Unlearning: Backdoor Mitigation by Unlearning Shared Adversarial Examples. (arXiv:2307.10562v1 [cs.LG])

    [http://arxiv.org/abs/2307.10562](http://arxiv.org/abs/2307.10562)

    本文提出了一种使用共享对抗样本来纯化有门模型的方法，并解决了一个新的双层优化问题，从而减轻后门攻击。

    

    后门攻击是对机器学习模型的严重安全威胁，敌对方可以向训练集中注入有毒样本，导致一个预测受特定触发器激活的有毒样本到特定目标类的有门模型，而在无害样本上表现正常。本文通过建立后门风险和对抗风险之间的联系，推导出一种新的后门风险上界，主要捕捉了后门模型与纯净模型之间共享的对抗样本的风险。这个上界进一步提出了一种利用对抗训练技术减轻后门攻击的新的双层优化问题。为了解决这个问题，我们提出了共享对抗性遗忘（SAU）。具体来说，SAU首先生成共享对抗样本，然后通过遗忘这些生成的共享对抗样本，使其能够被纯净模型正确分类和/或区分。

    Backdoor attacks are serious security threats to machine learning models where an adversary can inject poisoned samples into the training set, causing a backdoored model which predicts poisoned samples with particular triggers to particular target classes, while behaving normally on benign samples. In this paper, we explore the task of purifying a backdoored model using a small clean dataset. By establishing the connection between backdoor risk and adversarial risk, we derive a novel upper bound for backdoor risk, which mainly captures the risk on the shared adversarial examples (SAEs) between the backdoored model and the purified model. This upper bound further suggests a novel bi-level optimization problem for mitigating backdoor using adversarial training techniques. To solve it, we propose Shared Adversarial Unlearning (SAU). Specifically, SAU first generates SAEs, and then, unlearns the generated SAEs such that they are either correctly classified by the purified model and/or diff
    
[^54]: 后变分量子神经网络

    Post-variational quantum neural networks. (arXiv:2307.10560v1 [quant-ph])

    [http://arxiv.org/abs/2307.10560](http://arxiv.org/abs/2307.10560)

    本文讨论了后变分策略，该策略通过将可调参数从量子计算机转移到经典计算机，并采用集合策略来优化量子模型，解决了混合量子-经典计算中的贫瘠高原问题，并设计了后变分量子神经网络架构。

    

    量子计算有望比当前最先进的经典超级计算机提供更大的计算优势。然而，目前的硬件还不足以执行容错的量子算法。使用混合量子-经典计算和变分算法的替代方法可能会出现贫瘠高原问题，导致梯度优化技术收敛缓慢。在本文中，我们讨论了"后变分策略"，该策略将可调参数从量子计算机转移到经典计算机，选择在优化量子模型时采用集合策略。我们讨论了构建个体量子电路的各种策略和设计原则，其中得到的集合可以通过凸编程进行优化。此外，我们还讨论了后变分量子神经网络的架构设计，并分析了该神经网络中估计误差的传播。最后，我们展示了我们的算法...

    Quantum computing has the potential to provide substantial computational advantages over current state-of-the-art classical supercomputers. However, current hardware is not advanced enough to execute fault-tolerant quantum algorithms. An alternative of using hybrid quantum-classical computing with variational algorithms can exhibit barren plateau issues, causing slow convergence of gradient-based optimization techniques. In this paper, we discuss "post-variational strategies", which shift tunable parameters from the quantum computer to the classical computer, opting for ensemble strategies when optimizing quantum models. We discuss various strategies and design principles for constructing individual quantum circuits, where the resulting ensembles can be optimized with convex programming. Further, we discuss architectural designs of post-variational quantum neural networks and analyze the propagation of estimation errors throughout such neural networks. Lastly, we show that our algorith
    
[^55]: 使用合规化动态图学习预测空中交通管制员工作负荷水平

    Air Traffic Controller Workload Level Prediction using Conformalized Dynamical Graph Learning. (arXiv:2307.10559v1 [cs.LG])

    [http://arxiv.org/abs/2307.10559](http://arxiv.org/abs/2307.10559)

    本研究提出了一种使用合规化动态图学习来预测空中交通管制员工作负荷水平的方法，通过对退休空中交通管制员进行人机交互模拟，利用空中交通数据和工作负荷标签进行预测和评估。

    

    空中交通管制是一个安全关键的服务系统，要求地面空中交通管制员（ATCo）时刻关注以维持日常航空运营。ATCo的工作负荷可能对运营安全和空域使用产生负面影响。为了避免过载并确保ATCo的可接受工作负荷水平，准确预测ATCo的工作负荷对于采取缓解措施非常重要。在本文中，我们首先对ATCo工作负荷的研究进行了回顾，主要从空中交通的角度。然后，我们简要介绍了与退休ATCo进行人机交互模拟的设置，其中获得了空中交通数据和工作负荷标签。模拟在三种菲尼克斯接近场景下进行，要求人类ATCo自我评估其工作负荷评级（即，低-1到高-7）。进行了初步的数据分析。接下来，我们提出了一个基于图的深度学习框架，结合合规化预测，来对ATCo的工作负荷进行预测。

    Air traffic control (ATC) is a safety-critical service system that demands constant attention from ground air traffic controllers (ATCos) to maintain daily aviation operations. The workload of the ATCos can have negative effects on operational safety and airspace usage. To avoid overloading and ensure an acceptable workload level for the ATCos, it is important to predict the ATCos' workload accurately for mitigation actions. In this paper, we first perform a review of research on ATCo workload, mostly from the air traffic perspective. Then, we briefly introduce the setup of the human-in-the-loop (HITL) simulations with retired ATCos, where the air traffic data and workload labels are obtained. The simulations are conducted under three Phoenix approach scenarios while the human ATCos are requested to self-evaluate their workload ratings (i.e., low-1 to high-7). Preliminary data analysis is conducted. Next, we propose a graph-based deep-learning framework with conformal prediction to ide
    
[^56]: SC VALL-E: 可控风格的零样本文本到语音合成器

    SC VALL-E: Style-Controllable Zero-Shot Text to Speech Synthesizer. (arXiv:2307.10550v1 [cs.SD])

    [http://arxiv.org/abs/2307.10550](http://arxiv.org/abs/2307.10550)

    本文提出了一种基于神经编码语言模型的风格控制零样本文本到语音合成器，通过控制属性生成可控语音而不仅仅模仿特征，具有较好的性能。

    

    为了控制语音的各种特征和生成所需的声音，表达性的语音合成模型通过添加具有不同说话者、情绪和不同说话风格的语料库来进行训练。在本文中，我们提出了一种基于神经编码语言模型VALL-E的风格控制（SC）VALL-E模型，该模型遵循生成预训练变换器3（GPT-3）的结构。所提出的SC VALL-E模型从文本句子和提示音频中接收输入，并通过控制属性来生成可控的语音，而不仅仅是模仿提示音频的特征。我们通过识别新设计的风格网络的风格嵌入矩阵中表示情绪、说话速度、音高和声音强度等属性的标记，并设计一个可以控制这些属性的模型。为了评估SC VALL-E的性能，我们进行了与三种基准模型的比较实验。

    Expressive speech synthesis models are trained by adding corpora with diverse speakers, various emotions, and different speaking styles to the dataset, in order to control various characteristics of speech and generate the desired voice. In this paper, we propose a style control (SC) VALL-E model based on the neural codec language model (called VALL-E), which follows the structure of the generative pretrained transformer 3 (GPT-3). The proposed SC VALL-E takes input from text sentences and prompt audio and is designed to generate controllable speech by not simply mimicking the characteristics of the prompt audio but by controlling the attributes to produce diverse voices. We identify tokens in the style embedding matrix of the newly designed style network that represent attributes such as emotion, speaking rate, pitch, and voice intensity, and design a model that can control these attributes. To evaluate the performance of SC VALL-E, we conduct comparative experiments with three repres
    
[^57]: 基于差分平坦学习的模型预测控制及稳定性、状态和输入限制的安全滤波器

    Differentially Flat Learning-based Model Predictive Control Using a Stability, State, and Input Constraining Safety Filter. (arXiv:2307.10541v1 [eess.SY])

    [http://arxiv.org/abs/2307.10541](http://arxiv.org/abs/2307.10541)

    本文提出了一种基于差分平坦学习的模型预测控制方法，通过学习非线性变换作为安全滤波器，可以在保证稳定性、输入和状态约束满足的同时大幅减少计算量。

    

    基于学习的最优控制算法使用过去的轨迹数据和学习到的系统动力学模型来控制未知系统。这些控制器要么使用学习到的动力学的线性近似，以换取更快的计算速度，要么使用非线性优化方法，通常表现更好，但会限制实时应用。在本工作中，我们提出了一种新型的非线性控制器，利用差分平坦性可以在与最先进的学习控制器相似的性能下，显著减少计算量。差分平坦性是动态系统的一种属性，通过非线性输入映射可以精确线性化非线性系统。在这里，非线性变换被学习为高斯过程，并用于保证安全过滤器的稳定性和输入约束满足条件。然后，这个安全过滤器用于改进来自平坦模型预测的输入。

    Learning-based optimal control algorithms control unknown systems using past trajectory data and a learned model of the system dynamics. These controllers use either a linear approximation of the learned dynamics, trading performance for faster computation, or nonlinear optimization methods, which typically perform better but can limit real-time applicability. In this work, we present a novel nonlinear controller that exploits differential flatness to achieve similar performance to state-of-the-art learning-based controllers but with significantly less computational effort. Differential flatness is a property of dynamical systems whereby nonlinear systems can be exactly linearized through a nonlinear input mapping. Here, the nonlinear transformation is learned as a Gaussian process and is used in a safety filter that guarantees, with high probability, stability as well as input and flat state constraint satisfaction. This safety filter is then used to refine inputs from a flat model pr
    
[^58]: 快速无监督深度异常值模型选择与超网络

    Fast Unsupervised Deep Outlier Model Selection with Hypernetworks. (arXiv:2307.10529v1 [cs.LG])

    [http://arxiv.org/abs/2307.10529](http://arxiv.org/abs/2307.10529)

    本文提出了HYPER用于调整基于深度神经网络的异常值检测模型，解决了无监督DOD模型中的超参数调整和模型选择的挑战，通过设计和训练超网络(HN)将超参数映射到主要DOD模型的最优权重上。

    

    异常值检测(OD)在许多领域都有应用，并有许多技术的丰富文献。基于深度神经网络的OD(DOD)由于深度学习的许多进展而受到了最近的关注。在本文中，我们考虑了一个关键但鲜为人知的问题，即无监督DOD的有效超参数(HP)调整/模型选择。虽然一些先前的工作报告了OD模型对HP的敏感性，但对于展示了长列表HP的现代DOD模型来说，这变得非常关键。我们引入了HYPER来调整DOD模型，解决了两个基本挑战：(1)无监督情况下的验证(由于缺乏标记的异常值)，以及(2) HP/模型空间的高效搜索 (由于HP数量的指数增长)。关键思想是设计和训练一个新颖的超网络(HN)，其将HP映射到主要DOD模型的最优权重上。反过来，HYPER利用一个单独的HN，可以动态生成多个DOD模型的权重 (对应于...)。

    Outlier detection (OD) finds many applications with a rich literature of numerous techniques. Deep neural network based OD (DOD) has seen a recent surge of attention thanks to the many advances in deep learning. In this paper, we consider a critical-yet-understudied challenge with unsupervised DOD, that is, effective hyperparameter (HP) tuning/model selection. While several prior work report the sensitivity of OD models to HPs, it becomes ever so critical for the modern DOD models that exhibit a long list of HPs. We introduce HYPER for tuning DOD models, tackling two fundamental challenges: (1) validation without supervision (due to lack of labeled anomalies), and (2) efficient search of the HP/model space (due to exponential growth in the number of HPs). A key idea is to design and train a novel hypernetwork (HN) that maps HPs onto optimal weights of the main DOD model. In turn, HYPER capitalizes on a single HN that can dynamically generate weights for many DOD models (corresponding t
    
[^59]: 超越黑盒建议: 基于学习的增强算法用于具有Q值预测的MDPs

    Beyond Black-Box Advice: Learning-Augmented Algorithms for MDPs with Q-Value Predictions. (arXiv:2307.10524v1 [cs.LG])

    [http://arxiv.org/abs/2307.10524](http://arxiv.org/abs/2307.10524)

    该论文研究了在具有不可信的机器学习建议的单轨迹时间变化的MDP中一致性和鲁棒性之间的权衡，并证明了利用Q值建议可以获得接近最优的性能保证，并改进了仅使用黑盒建议的情况。

    

    我们研究了在单轨迹时间变化的马尔科夫决策过程(MDP)中一致性和鲁棒性之间的权衡，该过程具有不可信的机器学习建议。我们的工作不同于常规方法，不再将建议视为来自黑盒来源，而是考虑到有关如何生成建议的其他信息。我们证明了在包括连续和离散状态/动作空间的一般MDP模型下给出的Q值建议的一种新型一致性和鲁棒性权衡。我们的结果表明，利用Q值建议可以动态追求机器学习建议和稳健基线中较优的那个，从而产生接近最优的性能保证，并且改进了仅使用黑盒建议所能获得的结果。

    We study the tradeoff between consistency and robustness in the context of a single-trajectory time-varying Markov Decision Process (MDP) with untrusted machine-learned advice. Our work departs from the typical approach of treating advice as coming from black-box sources by instead considering a setting where additional information about how the advice is generated is available. We prove a first-of-its-kind consistency and robustness tradeoff given Q-value advice under a general MDP model that includes both continuous and discrete state/action spaces. Our results highlight that utilizing Q-value advice enables dynamic pursuit of the better of machine-learned advice and a robust baseline, thus result in near-optimal performance guarantees, which provably improves what can be obtained solely with black-box advice.
    
[^60]: FedSoup:通过选择性模型插值改善联邦学习中的泛化和个性化能力

    FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation. (arXiv:2307.10507v1 [cs.LG])

    [http://arxiv.org/abs/2307.10507](http://arxiv.org/abs/2307.10507)

    本文通过选择性模型插值的方式，在联邦学习中改善了泛化能力和个性化能力。

    

    跨机构的联邦学习使得可以在分布在数据中心（如医院和临床研究实验室）的数据集上开发机器学习模型。然而，最近的研究发现，当面临分布偏移时，当前的联邦学习算法在本地和全局性能之间存在一种权衡。具体而言，个性化的联邦学习方法往往倾向于对本地数据过拟合，导致局部模型发生转折，并抑制其对非分布式数据的泛化能力。在本文中，我们提出了一种新颖的联邦模型汤方法（即，选择性插值模型参数）来优化本地和全局性能之间的权衡。具体而言，在联邦训练阶段，每个客户端通过监控本地和全局模型之间的插值模型的表现来维护自己的全局模型池。这使得我们能够减轻过拟合，并寻求平坦的极小值，从而可以显著提高系统的泛化能力和个性化能力。

    Cross-silo federated learning (FL) enables the development of machine learning models on datasets distributed across data centers such as hospitals and clinical research laboratories. However, recent research has found that current FL algorithms face a trade-off between local and global performance when confronted with distribution shifts. Specifically, personalized FL methods have a tendency to overfit to local data, leading to a sharp valley in the local model and inhibiting its ability to generalize to out-of-distribution data. In this paper, we propose a novel federated model soup method (i.e., selective interpolation of model parameters) to optimize the trade-off between local and global performance. Specifically, during the federated training phase, each client maintains its own global model pool by monitoring the performance of the interpolated model between the local and global models. This allows us to alleviate overfitting and seek flat minima, which can significantly improve
    
[^61]: 在图像表示中识别可解释子空间

    Identifying Interpretable Subspaces in Image Representations. (arXiv:2307.10504v1 [cs.CV])

    [http://arxiv.org/abs/2307.10504](http://arxiv.org/abs/2307.10504)

    FALCON是一个解释图像表示特征的框架，可以通过使用字幕数据集和视觉语言模型来生成人类可理解的概念，并通过对比解释消除虚假概念。在较大的空间中，特征通过研究组合可以更易解释和高阶评分概念的解释。

    

    我们提出了一种解释图像表示特征的可解释性框架——FALCON（Automatic Feature Explanation using Contrasting Concepts）。对于目标特征，FALCON使用一个大型的字幕数据集（如LAION-400m）和一个预训练的视觉语言模型（如CLIP）对其高度激活的裁剪图像进行字幕生成。每个字幕中的单词都经过评分和排序，从而得到了与目标特征密切相关的少数共享的、人类可理解的概念。FALCON还使用低激活的（对立的）图像应用对比解释，以消除虚假概念。尽管许多现有方法独立解释特征，但我们观察到在最先进的自监督和监督模型中，不到20%的表示空间可以通过单个特征进行解释。我们展示了当一组特征一起研究时，更大的空间中的特征变得更易解释，并可以通过FALCON得到高阶评分概念的解释。

    We propose Automatic Feature Explanation using Contrasting Concepts (FALCON), an interpretability framework to explain features of image representations. For a target feature, FALCON captions its highly activating cropped images using a large captioning dataset (like LAION-400m) and a pre-trained vision-language model like CLIP. Each word among the captions is scored and ranked leading to a small number of shared, human-understandable concepts that closely describe the target feature. FALCON also applies contrastive interpretation using lowly activating (counterfactual) images, to eliminate spurious concepts. Although many existing approaches interpret features independently, we observe in state-of-the-art self-supervised and supervised models, that less than 20% of the representation space can be explained by individual features. We show that features in larger spaces become more interpretable when studied in groups and can be explained with high-order scoring concepts through FALCON.
    
[^62]: 一种用于专门模型的竞争学习方法：解决具有不同功能区域的复杂物理系统的问题

    A Competitive Learning Approach for Specialized Models: A Solution for Complex Physical Systems with Distinct Functional Regimes. (arXiv:2307.10496v1 [cs.LG])

    [http://arxiv.org/abs/2307.10496](http://arxiv.org/abs/2307.10496)

    本论文提出了一种用于复杂物理系统的竞争学习方法，通过同时训练多个模型，并使用动态损失函数识别不同的功能区域。实验证明该方法能够成功地解决模型发现和函数拟合问题。

    

    在科学和工程中，复杂系统有时会展现出在不同区域之间变化的行为。传统的全局模型很难捕捉到这种复杂行为的全范围，从而限制了它们准确表示系统的能力。为了应对这个挑战，我们提出了一种新颖的竞争学习方法，用于获取基于数据的物理系统模型。所提出方法的主要思想是同时对一组模型进行训练，并采用动态损失函数。每个模型在训练过程中争夺每个观察值，从而允许在数据集中识别出不同的功能区域。为了展示学习方法的有效性，我们将其与使用梯度优化器进行训练的各种回归方法相结合。所提出的方法在涉及模型发现和函数拟合的各种问题上进行了测试，表明它能够成功地识别出功能区域。

    Complex systems in science and engineering sometimes exhibit behavior that changes across different regimes. Traditional global models struggle to capture the full range of this complex behavior, limiting their ability to accurately represent the system. In response to this challenge, we propose a novel competitive learning approach for obtaining data-driven models of physical systems. The primary idea behind the proposed approach is to employ dynamic loss functions for a set of models that are trained concurrently on the data. Each model competes for each observation during training, allowing for the identification of distinct functional regimes within the dataset. To demonstrate the effectiveness of the learning approach, we coupled it with various regression methods that employ gradient-based optimizers for training. The proposed approach was tested on various problems involving model discovery and function approximation, demonstrating its ability to successfully identify functional
    
[^63]: 新颖的批量主动学习方法及其在合成孔径雷达数据集中的应用

    Novel Batch Active Learning Approach and Its Application to Synthetic Aperture Radar Datasets. (arXiv:2307.10495v1 [cs.LG])

    [http://arxiv.org/abs/2307.10495](http://arxiv.org/abs/2307.10495)

    该论文介绍了一种新颖的批量主动学习方法，使用Dijkstra的Annulus Core-Set (DAC)和LocalMax相结合，能够在合成孔径雷达数据中实现与顺序主动学习几乎相同的准确性，但更高效。

    

    主动学习通过精选一小部分未标记的数据点来优化机器学习方法的性能，目的是最大程度地提高底层分类器的性能。最近在合成孔径雷达(SAR)数据方面取得了一些顺序主动学习的进展。在每次迭代中，顺序主动学习选择一个大小为一的查询集合，而批量主动学习选择一个包含多个数据点的查询集合。尽管批量主动学习方法更加高效，但是相对于顺序主动学习方法，保持模型准确性的挑战在于困难。我们开发了一种新颖的两部分批量主动学习方法：Dijkstra的Annulus Core-Set(DAC)用于核心集合生成，以及批次抽样用于局部最大值。使用结合了DAC和LocalMax的批量主动学习过程可以达到几乎与顺序主动学习相同的准确性，但更加高效，与批次规模成正比。

    Active learning improves the performance of machine learning methods by judiciously selecting a limited number of unlabeled data points to query for labels, with the aim of maximally improving the underlying classifier's performance. Recent gains have been made using sequential active learning for synthetic aperture radar (SAR) data arXiv:2204.00005. In each iteration, sequential active learning selects a query set of size one while batch active learning selects a query set of multiple datapoints. While batch active learning methods exhibit greater efficiency, the challenge lies in maintaining model accuracy relative to sequential active learning methods. We developed a novel, two-part approach for batch active learning: Dijkstra's Annulus Core-Set (DAC) for core-set generation and LocalMax for batch sampling. The batch active learning process that combines DAC and LocalMax achieves nearly identical accuracy as sequential active learning but is more efficient, proportional to the batch
    
[^64]: 基于区块链的联邦学习：激励数据共享并惩罚不诚实行为

    Blockchain-Based Federated Learning: Incentivizing Data Sharing and Penalizing Dishonest Behavior. (arXiv:2307.10492v1 [cs.LG])

    [http://arxiv.org/abs/2307.10492](http://arxiv.org/abs/2307.10492)

    本研究提出了一个综合框架，通过将数据信任与联邦学习、星际文件系统、区块链和智能合约相结合，实现了安全互惠的数据共享，并提供了激励、访问控制机制，并惩罚不诚实的行为。经过实验证明，该模型提高了联邦学习模型的准确性，同时确保了数据共享过程的安全性和公平性。

    

    随着数据共享在协作和创新中的重要性日益增长，确保数据以安全和可信的方式进行管理和共享变得更加重要。数据治理是管理数据的常见方法，但面临许多挑战，如数据孤岛、数据一致性、隐私、安全和访问控制。为了解决这些挑战，本文提出了一个综合框架，将数据信任与联邦学习、星际文件系统、区块链和智能合约相结合，以促进安全互惠的数据共享，同时提供激励、访问控制机制，并惩罚任何不诚实的行为。实验结果表明，所提出的模型能够提高联邦学习模型的准确性，同时确保数据共享过程的安全性和公平性。该研究论文还提出了一个分散式联邦学习平台，成功训练了一个CNN模型。

    With the increasing importance of data sharing for collaboration and innovation, it is becoming more important to ensure that data is managed and shared in a secure and trustworthy manner. Data governance is a common approach to managing data, but it faces many challenges such as data silos, data consistency, privacy, security, and access control. To address these challenges, this paper proposes a comprehensive framework that integrates data trust in federated learning with InterPlanetary File System, blockchain, and smart contracts to facilitate secure and mutually beneficial data sharing while providing incentives, access control mechanisms, and penalizing any dishonest behavior. The experimental results demonstrate that the proposed model is effective in improving the accuracy of federated learning models while ensuring the security and fairness of the data-sharing process. The research paper also presents a decentralized federated learning platform that successfully trained a CNN m
    
[^65]: 图像和声音的滥用用于在多模态LLMs中进行间接指令注入

    (Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs. (arXiv:2307.10490v1 [cs.CR])

    [http://arxiv.org/abs/2307.10490](http://arxiv.org/abs/2307.10490)

    本论文展示了如何利用图像和声音在多模态LLMs中进行间接指令注入，攻击者通过生成对抗扰动并将其融入图像或音频录音中，以操纵模型输出特定文本和指导对话的行为。

    

    我们展示了如何利用图像和声音在多模态LLMs中进行间接提示和指令注入。攻击者生成与提示相对应的对抗扰动，并将其融入图像或音频录音中。当用户向（未修改的良性）模型询问被扰动的图像或音频时，扰动会引导模型输出攻击者选择的文本和/或使后续对话遵循攻击者的指令。我们用几个概念验证示例针对LLaVa和PandaGPT来说明这种攻击。

    We demonstrate how images and sounds can be used for indirect prompt and instruction injection in multi-modal LLMs. An attacker generates an adversarial perturbation corresponding to the prompt and blends it into an image or audio recording. When the user asks the (unmodified, benign) model about the perturbed image or audio, the perturbation steers the model to output the attacker-chosen text and/or make the subsequent dialog follow the attacker's instruction. We illustrate this attack with several proof-of-concept examples targeting LLaVa and PandaGPT.
    
[^66]: SPRINT: 一种用于评估和解析零样本神经稀疏检索的统一工具包

    SPRINT: A Unified Toolkit for Evaluating and Demystifying Zero-shot Neural Sparse Retrieval. (arXiv:2307.10488v1 [cs.IR])

    [http://arxiv.org/abs/2307.10488](http://arxiv.org/abs/2307.10488)

    SPRINT是一个统一的Python工具包，基于Pyserini和Lucene，支持评估和解析零样本神经稀疏检索。它解决了缺乏统一环境和实现在未见过领域上的检索能力的问题。

    

    传统上，稀疏检索系统依赖于词汇表示来检索文档，如BM25，在信息检索任务中占主导地位。随着诸如BERT这样的预训练transformer模型的出现，神经稀疏检索引领了检索中的新范式。尽管取得了成功，但目前缺乏支持不同稀疏检索器在统一的环境中运行的软件。这妨碍了实践者公正地比较不同的稀疏模型，并获得真实的评估结果。还有一个缺失的部分是，大多数先前的工作是对稀疏检索模型进行域内检索评估，即仅在一个数据集上进行评估：MS MARCO。然而，在实际检索系统中，一个重要的要求是模型能够在未见过的域外，即零样本检索任务中具有良好的泛化能力。在这项工作中，我们提供了SPRINT，这是一个基于Pyserini和Lucene的统一Python工具包，支持神经稀疏检索的通用接口。

    Traditionally, sparse retrieval systems relied on lexical representations to retrieve documents, such as BM25, dominated information retrieval tasks. With the onset of pre-trained transformer models such as BERT, neural sparse retrieval has led to a new paradigm within retrieval. Despite the success, there has been limited software supporting different sparse retrievers running in a unified, common environment. This hinders practitioners from fairly comparing different sparse models and obtaining realistic evaluation results. Another missing piece is, that a majority of prior work evaluates sparse retrieval models on in-domain retrieval, i.e. on a single dataset: MS MARCO. However, a key requirement in practical retrieval systems requires models that can generalize well to unseen out-of-domain, i.e. zero-shot retrieval tasks. In this work, we provide SPRINT, a unified Python toolkit based on Pyserini and Lucene, supporting a common interface for evaluating neural sparse retrieval. The 
    
[^67]: FinGPT: 将互联网规模的金融数据民主化为金融大语言模型

    FinGPT: Democratizing Internet-scale Data for Financial Large Language Models. (arXiv:2307.10485v1 [cs.CL])

    [http://arxiv.org/abs/2307.10485](http://arxiv.org/abs/2307.10485)

    FinGPT是一个开源的数据为中心的框架，旨在将互联网规模的金融数据民主化为金融大语言模型。它提供了自动收集和整理实时金融数据的功能，解决了金融文本数据稀缺的问题。

    

    大型语言模型（LLM）在理解和生成类似人类文本方面展示了卓越的能力，这可能会彻底改变金融行业。然而，现有的LLM在金融领域往往表现不佳，主要原因是一般文本数据与金融文本数据之间的差异。不幸的是，现有的金融文本数据集数量有限（大小较小），而第一个金融LLM（FinLLM）BloombergGPT是封闭的（只发布了训练日志）。鉴于此，我们的目标是通过Internet规模的金融数据将LLM民主化，由于数据来源多样、信噪比低和时间有效性高，这是一个开放性的挑战。为了解决这些挑战，我们引入了一个开源和数据为中心的框架“金融生成预训练变压器（FinGPT）”，它可以自动收集和整理来自互联网上超过34个不同来源的实时金融数据。

    Large language models (LLMs) have demonstrated remarkable proficiency in understanding and generating human-like texts, which may potentially revolutionize the finance industry. However, existing LLMs often fall short in the financial field, which is mainly attributed to the disparities between general text data and financial text data. Unfortunately, there is only a limited number of financial text datasets available (quite small size), and BloombergGPT, the first financial LLM (FinLLM), is close-sourced (only the training logs were released). In light of this, we aim to democratize Internet-scale financial data for LLMs, which is an open challenge due to diverse data sources, low signal-to-noise ratio, and high time-validity. To address the challenges, we introduce an open-sourced and data-centric framework, \textit{Financial Generative Pre-trained Transformer (FinGPT)}, that automates the collection and curation of real-time financial data from >34 diverse sources on the Internet, p
    
[^68]: 通过提示，指令微调的语言模型能否识别社会偏见？

    Can Instruction Fine-Tuned Language Models Identify Social Bias through Prompting?. (arXiv:2307.10472v1 [cs.CL])

    [http://arxiv.org/abs/2307.10472](http://arxiv.org/abs/2307.10472)

    本文介绍了一种通过零样本提示评估指令微调语言模型识别偏见能力的方法，展示了Alpaca 7B在偏见识别任务中的最佳性能，并提出扩大模型大小和数据多样性可进一步提高性能。

    

    随着语言模型应用的广度和深度不断扩展，构建有效的框架来衡量和减轻这些模型学习或继承的社会偏见变得越来越重要。本文提出了一种评估指令微调语言模型通过零样本提示（包括思维链提示）识别偏见能力的方法。在LLaMA及其两个指令微调版本中，Alpaca 7B在偏见识别任务中表现最好，准确率达56.7%。我们还展示了扩大语言模型大小和数据多样性可以进一步提高性能。这是我们偏见缓解框架的第一部分，正在进行的工作。我们将根据获得的更多结果不断更新本文。

    As the breadth and depth of language model applications continue to expand rapidly, it is increasingly important to build efficient frameworks for measuring and mitigating the learned or inherited social biases of these models. In this paper, we present our work on evaluating instruction fine-tuned language models' ability to identify bias through zero-shot prompting, including Chain-of-Thought (CoT) prompts. Across LLaMA and its two instruction fine-tuned versions, Alpaca 7B performs best on the bias identification task with an accuracy of 56.7%. We also demonstrate that scaling up LLM size and data diversity could lead to further performance gain. This is a work-in-progress presenting the first component of our bias mitigation framework. We will keep updating this work as we get more results.
    
[^69]: 专利中可视化类型和视角的分类

    Classification of Visualization Types and Perspectives in Patents. (arXiv:2307.10471v1 [cs.CV])

    [http://arxiv.org/abs/2307.10471](http://arxiv.org/abs/2307.10471)

    本文主要研究了专利图像中可视化类型和视角的分类问题，扩展了CLEF-IP数据集并采用最先进的深度学习方法进行了分类。这项研究对于促进专利探索和检索具有重要意义。

    

    鉴于每年专利申请数量的迅速增长，促进专利探索和检索的信息和多媒体检索方法至关重要。不同类型的可视化（例如，图形、技术图纸）和视角（例如，侧视、透视）被用来可视化专利创新的细节。对这些图像的分类可以实现更高效的搜索并进行进一步分析。到目前为止，用于图像类型分类的数据集缺少一些重要的专利可视化类型。此外，相关研究没有使用包括transformers在内的最新深度学习方法。在本文中，我们采用最先进的深度学习方法来分类专利图像中的可视化类型和视角。我们对专利中图像类型分类的CLEF-IP数据集进行了扩展，增加到了十个类别，并提供了手动标注的真实标签。此外，我们从一个数据集中推导出一组层级类别。

    Due to the swift growth of patent applications each year, information and multimedia retrieval approaches that facilitate patent exploration and retrieval are of utmost importance. Different types of visualizations (e.g., graphs, technical drawings) and perspectives (e.g., side view, perspective) are used to visualize details of innovations in patents. The classification of these images enables a more efficient search and allows for further analysis. So far, datasets for image type classification miss some important visualization types for patents. Furthermore, related work does not make use of recent deep learning approaches including transformers. In this paper, we adopt state-of-the-art deep learning methods for the classification of visualization types and perspectives in patent images. We extend the CLEF-IP dataset for image type classification in patents to ten classes and provide manual ground truth annotations. In addition, we derive a set of hierarchical classes from a dataset
    
[^70]: 数据科学的价值取向：数据科学的本质、价值和风险

    A data science axiology: the nature, value, and risks of data science. (arXiv:2307.10460v1 [cs.AI])

    [http://arxiv.org/abs/2307.10460](http://arxiv.org/abs/2307.10460)

    这篇论文介绍了数据科学的价值取向，探讨了其特征和作用。数据科学不是一门科学，而是一种研究范式，具有广泛的应用和重大的影响，但也存在着未知的风险。这一领域仍然处于初级阶段，需要进一步的研究。

    

    数据科学不是一门科学，而是一种研究范式，具有无法预测的范围、规模、复杂性和知识发现能力，这是其他方式无法实现的，并且可能超出人类推理的能力。它已经在AI军备竞赛中广泛应用于数以万计的应用程序中，已经实质性地改变了我们的世界，但由于其不可思议的复杂性，可能带来未知的风险。本文介绍了数据科学的价值取向，探讨和评估了其显著而决定性的特征，以便了解和定义数据科学，认识到其潜在的益处、风险和开放性研究挑战。基于AI的数据科学本质上涉及不确定性，这可能比我们对科学确定性的偏好更加现实。数据科学将产生远远超出知识发现的影响。

    Data science is not a science. It is a research paradigm with an unfathomed scope, scale, complexity, and power for knowledge discovery that is not otherwise possible and can be beyond human reasoning. It is changing our world practically and profoundly already widely deployed in tens of thousands of applications in every discipline in an AI Arms Race that, due to its inscrutability, can lead to unfathomed risks. This paper presents an axiology of data science, its purpose, nature, importance, risks, and value for problem solving, by exploring and evaluating its remarkable, definitive features. As data science is in its infancy, this initial, speculative axiology is intended to aid in understanding and defining data science to recognize its potential benefits, risks, and open research challenges. AI based data science is inherently about uncertainty that may be more realistic than our preference for the certainty of science. Data science will have impacts far beyond knowledge discovery
    
[^71]: 一种实现具有硬约束输出的神经网络的计算简单方法

    A New Computationally Simple Approach for Implementing Neural Networks with Output Hard Constraints. (arXiv:2307.10459v1 [cs.LG])

    [http://arxiv.org/abs/2307.10459](http://arxiv.org/abs/2307.10459)

    提出了一种计算简单的方法来实现具有硬约束输出的神经网络。该方法通过映射隐藏参数向量到一个符合约束集的点实现约束，并通过附加的神经网络层来进行映射。该方法还可以处理不仅对输出向量施加约束，还对依赖于输入的联合约束施加约束的情况，并且可以处理不同类型的约束，包括线性和二次约束、等式约束和动态约束。

    

    提出了一种在神经网络输出值上施加硬凸约束的计算简单方法。该方法的关键思想是通过将网络的隐藏参数向量映射到一个点，确保它在由一组约束定义的可行集内。映射是通过具有输出约束的附加神经网络层实现的。将该方法简单地扩展到不仅对输出向量施加约束，还对依赖于输入的联合约束施加约束的情况。在所提出的方法框架中，可以简单地实现对输出的约束投影方法。展示了如何将不同类型的约束引入到所提出的方法中，包括线性和二次约束、等式约束和动态约束，以及边界形式的约束。该方法的一个重要特点是它的计算简单性。

    A new computationally simple method of imposing hard convex constraints on the neural network output values is proposed. The key idea behind the method is to map a vector of hidden parameters of the network to a point that is guaranteed to be inside the feasible set defined by a set of constraints. The mapping is implemented by the additional neural network layer with constraints for output. The proposed method is simply extended to the case when constraints are imposed not only on the output vectors, but also on joint constraints depending on inputs. The projection approach to imposing constraints on outputs can simply be implemented in the framework of the proposed method. It is shown how to incorporate different types of constraints into the proposed method, including linear and quadratic constraints, equality constraints, and dynamic constraints, constraints in the form of boundaries. An important feature of the method is its computational simplicity. Complexities of the forward pa
    
[^72]: 朝着全球生物多样性评估迈出的一步：BIOSCAN-1M昆虫数据集

    A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset. (arXiv:2307.10455v1 [cs.CV])

    [http://arxiv.org/abs/2307.10455](http://arxiv.org/abs/2307.10455)

    提出了一个新的大型手工标记昆虫图像数据集BIOSCAN-Insect，用于对昆虫生物多样性进行编目。该数据集还具有引人注目的特征，对广泛的机器学习社区也具有研究价值。

    

    为了对昆虫生物多样性进行编目，我们提出了一个新的大型手工标记昆虫图像数据集，即BIOSCAN-Insect数据集。每个记录都由专家进行分类，并且具有相关的遗传信息，包括原始核苷酸条形码序列和分配的条形码索引号，这些是基于遗传的物种分类的代理。本文介绍了一个精选的百万图像数据集，主要用于训练能够提供基于图像的分类评估的计算机视觉模型，但该数据集还具有引人注目的特征，对广泛的机器学习社区也具有研究价值。由于数据集固有的生物性质，展现出了具有长尾类别不平衡分布的特征。此外，分类标签是一个分层分类方案，在较低级别上呈现出高度细粒度的分类问题。除了激发对生物多样性研究的兴趣外，该数据集还促进了对机器学习的深入研究。

    In an effort to catalog insect biodiversity, we propose a new large dataset of hand-labelled insect images, the BIOSCAN-Insect Dataset. Each record is taxonomically classified by an expert, and also has associated genetic information including raw nucleotide barcode sequences and assigned barcode index numbers, which are genetically-based proxies for species classification. This paper presents a curated million-image dataset, primarily to train computer-vision models capable of providing image-based taxonomic assessment, however, the dataset also presents compelling characteristics, the study of which would be of interest to the broader machine learning community. Driven by the biological nature inherent to the dataset, a characteristic long-tailed class-imbalance distribution is exhibited. Furthermore, taxonomic labelling is a hierarchical classification scheme, presenting a highly fine-grained classification problem at lower levels. Beyond spurring interest in biodiversity research w
    
[^73]: 使用相对位置标签将异构图与实体感知自注意力相结合的阅读理解模型

    Integrating a Heterogeneous Graph with Entity-aware Self-attention using Relative Position Labels for Reading Comprehension Model. (arXiv:2307.10443v1 [cs.CL])

    [http://arxiv.org/abs/2307.10443](http://arxiv.org/abs/2307.10443)

    本文提出了一种新的注意力模式，使用图增强自注意力机制将从异构图中导出的推理知识整合到变压器架构中，从而克服了变压器模型在复杂推理任务中的限制。通过全局-局部注意力、图注意力和关系类型考虑，优化了实体和单词之间的注意力。该模式与相对位置标签相结合，能够与LUKE的实体感知自注意力机制相集成。

    

    尽管变压器模型在机器阅读理解任务中取得了重大进展，但由于输入序列中缺少显式知识，它们仍然面临处理复杂推理任务的限制。本文提出了一种新颖的注意力模式来克服这个限制，它利用增强图自注意力机制将由异构图导出的推理知识整合到变压器架构中。提出的注意力模式包括三个关键要素：单词标记的全局-局部注意力，对实体标记的图注意力，实体标记对相关联的标记显示强烈的注意力而对不相关的标记显示较弱的注意力，以及考虑每个实体标记与单词标记之间的关系类型。这样，如果存在关系，则可以优化两者之间的注意力。该模式与特殊的相对位置标签相结合，使其能够与LUKE的实体感知自注意力机制相集成。

    Despite the significant progress made by transformer models in machine reading comprehension tasks, they still face limitations in handling complex reasoning tasks due to the absence of explicit knowledge in the input sequence. This paper proposes a novel attention pattern to overcome this limitation, which integrates reasoning knowledge derived from a heterogeneous graph into the transformer architecture using a graph-enhanced self-attention mechanism. The proposed attention pattern comprises three key elements: global-local attention for word tokens, graph attention for entity tokens that exhibit strong attention towards tokens connected in the graph as opposed to those unconnected, and the consideration of the type of relationship between each entity token and word token. This results in optimized attention between the two if a relationship exists. The pattern is coupled with special relative position labels, allowing it to integrate with LUKE's entity-aware self-attention mechanism
    
[^74]: 使用无标签数据的置信度估计

    Confidence Estimation Using Unlabeled Data. (arXiv:2307.10440v1 [cs.LG])

    [http://arxiv.org/abs/2307.10440](http://arxiv.org/abs/2307.10440)

    本文提出了一种适用于半监督环境的置信度估计方法，通过检查训练过程中的预测一致性，即使只有有限的训练标签，我们仍然可以合理地近似模型对未标记样本的置信度。该方法在图像分类和分割任务中取得了最先进的性能，并且通过下游主动学习任务展示了其优势。

    

    过度自信是深度神经网络的常见问题，限制了它们在实际应用中的部署。为了更好地估计置信度，现有方法主要集中在全监督场景，并依赖于训练标签。在本文中，我们提出了第一种适用于半监督环境的置信度估计方法，当大部分训练标签不可用时。我们认为即使只有有限的训练标签，通过检查训练过程中的预测一致性，我们仍然可以合理地近似模型对未标记样本的置信度。我们使用训练一致性作为替代函数，并提出了一种置信度估计的一致性排序损失。在图像分类和分割任务上，我们的方法在置信度估计方面取得了最先进的性能。此外，我们通过下游主动学习任务展示了所提出方法的优势。代码可在https://github.com/TopoXLab/consistency-ranking-loss找到。

    Overconfidence is a common issue for deep neural networks, limiting their deployment in real-world applications. To better estimate confidence, existing methods mostly focus on fully-supervised scenarios and rely on training labels. In this paper, we propose the first confidence estimation method for a semi-supervised setting, when most training labels are unavailable. We stipulate that even with limited training labels, we can still reasonably approximate the confidence of model on unlabeled samples by inspecting the prediction consistency through the training process. We use training consistency as a surrogate function and propose a consistency ranking loss for confidence estimation. On both image classification and segmentation tasks, our method achieves state-of-the-art performances in confidence estimation. Furthermore, we show the benefit of the proposed method through a downstream active learning task. The code is available at https://github.com/TopoXLab/consistency-ranking-loss
    
[^75]: 用图神经结构搜索进行分子属性预测的不确定性量化

    Uncertainty Quantification for Molecular Property Predictions with Graph Neural Architecture Search. (arXiv:2307.10438v1 [cs.LG])

    [http://arxiv.org/abs/2307.10438](http://arxiv.org/abs/2307.10438)

    用于分子属性预测的图神经网络方法通常无法量化预测的不确定性，本研究提出了一种自动化的不确定性量化方法AutoGNNUQ，通过架构搜索生成高性能的图神经网络集合，并利用方差分解将数据和模型的不确定性分开，从而提供了减少不确定性的有价值见解。

    

    图神经网络（GNN）已成为分子属性预测中突出的数据驱动方法。然而，典型GNN模型的一个关键限制是无法量化预测中的不确定性。这种能力对于确保在下游任务中可信地使用和部署模型至关重要。为此，我们引入了AutoGNNUQ，一种自动化的分子属性预测不确定性量化方法。AutoGNNUQ利用架构搜索生成一组高性能的GNN集合，能够估计预测的不确定性。我们的方法使用方差分解来分离数据（aleatoric）和模型（epistemic）不确定性，为减少它们提供了有价值的见解。在我们的计算实验中，我们展示了AutoGNNUQ在多个基准数据集上在预测准确性和不确定性测量性能方面超过了现有的不确定性量化方法。此外，我们利用t-SNE可视化来解释不确定性的来源和结构。

    Graph Neural Networks (GNNs) have emerged as a prominent class of data-driven methods for molecular property prediction. However, a key limitation of typical GNN models is their inability to quantify uncertainties in the predictions. This capability is crucial for ensuring the trustworthy use and deployment of models in downstream tasks. To that end, we introduce AutoGNNUQ, an automated uncertainty quantification (UQ) approach for molecular property prediction. AutoGNNUQ leverages architecture search to generate an ensemble of high-performing GNNs, enabling the estimation of predictive uncertainties. Our approach employs variance decomposition to separate data (aleatoric) and model (epistemic) uncertainties, providing valuable insights for reducing them. In our computational experiments, we demonstrate that AutoGNNUQ outperforms existing UQ methods in terms of both prediction accuracy and UQ performance on multiple benchmark datasets. Additionally, we utilize t-SNE visualization to exp
    
[^76]: 用有限数据进行贝叶斯编程方法的车辆跟随模型校准和验证

    A Bayesian Programming Approach to Car-following Model Calibration and Validation using Limited Data. (arXiv:2307.10437v1 [cs.LG])

    [http://arxiv.org/abs/2307.10437](http://arxiv.org/abs/2307.10437)

    这项研究提出了一种贝叶斯编程方法，用于校准和验证车辆跟随模型，以捕捉和复制工作区内外的驾驶行为。

    

    交通仿真软件被交通研究人员和工程师用于设计和评估道路变化。这些仿真器由微观驾驶行为模型驱动，可以从中推导出宏观测量如流量和拥堵。许多模型设计用于可能的交通场景和道路配置的子集，而其他模型在应用上没有明确的限制。工作区（WZs）是一种到目前为止没有模型能够复现真实驾驶行为的场景。这使得在设计WZ时优化安全和其他指标变得困难。美国联邦公路管理局委托USDOT Volpe中心开发一种微观仿真器中能够准确捕捉和复制WZ内外的驾驶行为的车辆跟随（CF）模型。Volpe还进行了一项自然驾驶研究，收集了在具有WZ的道路上驾驶的车辆的遥测数据，用于模型校准。

    Traffic simulation software is used by transportation researchers and engineers to design and evaluate changes to roadways. These simulators are driven by models of microscopic driver behavior from which macroscopic measures like flow and congestion can be derived. Many models are designed for a subset of possible traffic scenarios and roadway configurations, while others have no explicit constraints on their application. Work zones (WZs) are one scenario for which no model to date has reproduced realistic driving behavior. This makes it difficult to optimize for safety and other metrics when designing a WZ. The Federal Highway Administration commissioned the USDOT Volpe Center to develop a car-following (CF) model for use in microscopic simulators that can capture and reproduce driver behavior accurately within and outside of WZs. Volpe also performed a naturalistic driving study to collect telematics data from vehicles driven on roads with WZs for use in model calibration. During mod
    
[^77]: 基于矩阵集合卡尔曼滤波器的多臂神经网络，以充分近似深度神经网络

    A Matrix Ensemble Kalman Filter-based Multi-arm Neural Network to Adequately Approximate Deep Neural Networks. (arXiv:2307.10436v1 [stat.ML])

    [http://arxiv.org/abs/2307.10436](http://arxiv.org/abs/2307.10436)

    我们提出了一种基于矩阵集合卡尔曼滤波器的多臂神经网络，可以在样本量太小无法训练多臂神经网络时充分近似深度神经网络。该方法还可以对从长短期记忆网络（LSTM）获得的预测赋予不确定性，具有良好的覆盖范围。

    

    深度学习者（DLs）是最先进的预测机制，在许多需要复杂高维数据处理的领域中有广泛应用。虽然传统的DLs通过梯度下降和反向传播进行训练，但已经开发出了不需要梯度计算的基于卡尔曼滤波器（KF）的技术来近似DLs。我们提出了一种基于KF的DL近似器的多臂扩展，当样本量太小无法训练多臂DL时，可以模仿DL。我们提出的矩阵集合卡尔曼滤波器多臂ANN（MEnKF-ANN）还执行显式的模型堆叠，在训练样本具有不等尺寸特征集时具有相关性。我们提出的技术可以近似长短期记忆（LSTM）网络，并给出从这些LSTM预测中获得的值加上不确定性的理想覆盖范围。我们展示了MEnKF-ANN如何“充分”近似一个训练用于分类哪些碳水化合物基质被消化和利用的LSTM网络。

    Deep Learners (DLs) are the state-of-art predictive mechanism with applications in many fields requiring complex high dimensional data processing. Although conventional DLs get trained via gradient descent with back-propagation, Kalman Filter (KF)-based techniques that do not need gradient computation have been developed to approximate DLs. We propose a multi-arm extension of a KF-based DL approximator that can mimic DL when the sample size is too small to train a multi-arm DL. The proposed Matrix Ensemble Kalman Filter-based multi-arm ANN (MEnKF-ANN) also performs explicit model stacking that becomes relevant when the training sample has an unequal-size feature set. Our proposed technique can approximate Long Short-term Memory (LSTM) Networks and attach uncertainty to the predictions obtained from these LSTMs with desirable coverage. We demonstrate how MEnKF-ANN can "adequately" approximate an LSTM network trained to classify what carbohydrate substrates are digested and utilized by a
    
[^78]: 从成员和偏好查询中学习形式规范

    Learning Formal Specifications from Membership and Preference Queries. (arXiv:2307.10434v1 [cs.FL])

    [http://arxiv.org/abs/2307.10434](http://arxiv.org/abs/2307.10434)

    该论文提出了一种新的框架，通过请求成员标签和成对偏好来扩展主动规范学习，提高学习形式规范的灵活性。在两个不同领域的实验中，结果表明通过学习成员和偏好的组合可以稳定和方便地识别规范。

    

    主动学习是一种研究广泛的学习形式规范的方法，例如自动机。在这项工作中，我们通过提出一种新颖的框架，将主动规范学习扩展到请求组合成员标签和成对偏好（对成员标签的一种流行替代方式）。成对偏好和成员标签的组合允许更灵活的主动规范学习方法，它先前仅依赖成员标签。我们将我们的框架应用于两个不同的领域，证明了我们方法的广泛性。我们的结果表明，从两种模式学习可以通过成员和偏好来稳健和方便地识别规范。

    Active learning is a well-studied approach to learning formal specifications, such as automata. In this work, we extend active specification learning by proposing a novel framework that strategically requests a combination of membership labels and pair-wise preferences, a popular alternative to membership labels. The combination of pair-wise preferences and membership labels allows for a more flexible approach to active specification learning, which previously relied on membership labels only. We instantiate our framework in two different domains, demonstrating the generality of our approach. Our results suggest that learning from both modalities allows us to robustly and conveniently identify specifications via membership and preferences.
    
[^79]: DP-TBART：一种基于Transformer的自回归模型用于差分隐私表格数据生成

    DP-TBART: A Transformer-based Autoregressive Model for Differentially Private Tabular Data Generation. (arXiv:2307.10430v1 [cs.LG])

    [http://arxiv.org/abs/2307.10430](http://arxiv.org/abs/2307.10430)

    DP-TBART是一种基于Transformer的自回归模型，可以在保持差分隐私的情况下生成合成的表格数据。与传统方法相比，它在多种数据集上表现出竞争力，并在某些场景中超过了最先进的方法。

    

    在保持差分隐私的条件下生成合成表格数据是一个日益重要的问题。虽然传统的基于边际的方法取得了令人瞩目的结果，但最近的研究表明，基于深度学习的方法往往落后。在这项工作中，我们提出了DP-TBART，一种基于Transformer的自回归模型，它在各种数据集上保持差分隐私，并能够与基于边际的方法竞争，甚至在某些场景下超过最先进的方法。我们还提供了一个理论框架，用于理解基于边际的方法的局限性以及深度学习方法在哪些方面能够做出贡献。这些结果表明，在生成差分隐私合成表格数据时，应考虑基于深度学习的技术作为可行的替代方法。

    The generation of synthetic tabular data that preserves differential privacy is a problem of growing importance. While traditional marginal-based methods have achieved impressive results, recent work has shown that deep learning-based approaches tend to lag behind. In this work, we present Differentially-Private TaBular AutoRegressive Transformer (DP-TBART), a transformer-based autoregressive model that maintains differential privacy and achieves performance competitive with marginal-based methods on a wide variety of datasets, capable of even outperforming state-of-the-art methods in certain settings. We also provide a theoretical framework for understanding the limitations of marginal-based approaches and where deep learning-based approaches stand to contribute most. These results suggest that deep learning-based techniques should be considered as a viable alternative to marginal-based methods in the generation of differentially private synthetic tabular data.
    
[^80]: PreDiff: 使用隐式扩散模型进行降水近期预测

    PreDiff: Precipitation Nowcasting with Latent Diffusion Models. (arXiv:2307.10422v1 [cs.LG])

    [http://arxiv.org/abs/2307.10422](http://arxiv.org/abs/2307.10422)

    本论文提出了PreDiff方法，使用条件隐式扩散模型进行降水近期预测。同时，引入显式知识控制机制以满足特定领域的物理约束。

    

    传统上，地球系统预测主要依赖于复杂的物理模型，这些模型计算量大且需要领域专业知识。在过去的十年中，时空地球观测数据的空前增加使得使用深度学习技术的数据驱动预测模型成为可能。这些模型在不同的地球系统预测任务中显示出有希望的效果，但是它们要么难以处理不确定性，要么忽视特定领域的先验知识，导致预测结果模糊或产生物理上不合理的预测。为了解决这些限制，我们提出了一种概率时空预测的两阶段流程：1）我们开发了一种名为PreDiff的条件隐式扩散模型，能够进行概率预测；2）我们融入了一种显式知识控制机制，以使预测符合特定领域的物理约束。这是通过在每个去噪步骤中估计与所施加约束的偏差来实现的。

    Earth system forecasting has traditionally relied on complex physical models that are computationally expensive and require significant domain expertise. In the past decade, the unprecedented increase in spatiotemporal Earth observation data has enabled data-driven forecasting models using deep learning techniques. These models have shown promise for diverse Earth system forecasting tasks but either struggle with handling uncertainty or neglect domain-specific prior knowledge, resulting in averaging possible futures to blurred forecasts or generating physically implausible predictions. To address these limitations, we propose a two-stage pipeline for probabilistic spatiotemporal forecasting: 1) We develop PreDiff, a conditional latent diffusion model capable of probabilistic forecasts. 2) We incorporate an explicit knowledge control mechanism to align forecasts with domain-specific physical constraints. This is achieved by estimating the deviation from imposed constraints at each denoi
    
[^81]: 使用PIP-Net解释和纠正医学图像分类

    Interpreting and Correcting Medical Image Classification with PIP-Net. (arXiv:2307.10404v1 [cs.CV])

    [http://arxiv.org/abs/2307.10404](http://arxiv.org/abs/2307.10404)

    本研究利用PIP-Net开展了可解释的机器学习技术在医学图像分类中的应用，并展示了其在骨折检测和皮肤癌诊断方面的准确性和可解释性。通过无监督的预训练，PIP-Net能够轻松识别数据质量问题，并且我们还发现人们可以通过手动禁用不良原型来纠正PIP-Net的推理过程。

    

    部分原型模型是可解释性的图像分类器，是黑盒人工智能的有希望的替代方案。本文探讨了可解释性机器学习的适用性和潜力，特别是对于真实世界的医学成像数据的自动诊断支持。PIP-Net学习人类可理解的典型图像部分，并评估其在骨折检测和皮肤癌诊断方面的准确性和可解释性。我们发现PIP-Net的决策过程符合医学分类标准，仅提供图像级别的类标签。由于PIP-Net对原型进行了无监督的预训练，因此可以轻松识别X光中的不良文本或标签错误等数据质量问题。此外，我们是第一个显示人们可以通过直接禁用不良原型来手动纠正PIP-Net的推理过程。我们得出结论，部分原型模型对医学应用具有潜力，因为它们具有相互参考性。

    Part-prototype models are explainable-by-design image classifiers, and a promising alternative to black box AI. This paper explores the applicability and potential of interpretable machine learning, in particular PIP-Net, for automated diagnosis support on real-world medical imaging data. PIP-Net learns human-understandable prototypical image parts and we evaluate its accuracy and interpretability for fracture detection and skin cancer diagnosis. We find that PIP-Net's decision making process is in line with medical classification standards, while only provided with image-level class labels. Because of PIP-Net's unsupervised pretraining of prototypes, data quality problems such as undesired text in an X-ray or labelling errors can be easily identified. Additionally, we are the first to show that humans can manually correct the reasoning of PIP-Net by directly disabling undesired prototypes. We conclude that part-prototype models are promising for medical applications due to their inter
    
[^82]: 强透镜发现神经网络的选择功能

    Selection functions of strong lens finding neural networks. (arXiv:2307.10355v1 [astro-ph.CO])

    [http://arxiv.org/abs/2307.10355](http://arxiv.org/abs/2307.10355)

    强透镜发现神经网络的选择功能对于实现大样本强引力透镜系统的潜力至关重要，网络倾向于选择具有较大爱因斯坦半径和更集中源光分布的系统。

    

    通常在文献中发现的具有类似架构和训练数据的卷积神经网络用于透镜发现是有偏的分类器。了解透镜发现神经网络的选择功能对于充分实现即将出现的大样本强引力透镜系统的潜力至关重要。我们使用了三个代表性的训练数据集，这些数据集用于训练星系-星系和星系-类星体透镜发现神经网络。这些网络倾向于选择具有较大爱因斯坦半径和更集中的源光分布的系统。将检测显著性阈值从8σ增加到12σ，导致选中的强透镜系统中有50％的系统的爱因斯坦半径θ_E≥1.04角秒，源半径R_S≥0.194角秒，从θ_E≥0.879角秒和R_S≥0.178角秒。

    Convolution Neural Networks trained for the task of lens finding with similar architecture and training data as is commonly found in the literature are biased classifiers. An understanding of the selection function of lens finding neural networks will be key to fully realising the potential of the large samples of strong gravitational lens systems that will be found in upcoming wide-field surveys. We use three training datasets, representative of those used to train galaxy-galaxy and galaxy-quasar lens finding neural networks. The networks preferentially select systems with larger Einstein radii and larger sources with more concentrated source-light distributions. Increasing the detection significance threshold to 12$\sigma$ from 8$\sigma$ results in 50 per cent of the selected strong lens systems having Einstein radii $\theta_\mathrm{E}$ $\ge$ 1.04 arcsec from $\theta_\mathrm{E}$ $\ge$ 0.879 arcsec, source radii $R_S$ $\ge$ 0.194 arcsec from $R_S$ $\ge$ 0.178 arcsec and source S\'ersi
    
[^83]: 离散切割Wasserstein损失的性质

    Properties of Discrete Sliced Wasserstein Losses. (arXiv:2307.10352v1 [stat.ML])

    [http://arxiv.org/abs/2307.10352](http://arxiv.org/abs/2307.10352)

    本文研究了离散切割Wasserstein损失的性质，并探讨了其正则性和优化性质以及通过蒙特卡洛近似的方法。

    

    切割Wasserstein（SW）距离已成为比较概率测度的Wasserstein距离的一种流行替代方法。广泛应用包括图像处理、领域自适应和生成建模，常常需要优化一些参数以最小化SW，该参数充当离散概率测度之间的损失函数（因为具有密度的测度在数值上是无法实现的）。所有这些优化问题都存在相同的子问题，即最小化切割Wasserstein能量。在本文中，我们研究了$\mathcal{E}: Y \longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z)$的属性，即两个具有与一个测度的支撑相同数量的离散均匀测度之间的SW距离作为支撑$Y \in \mathbb{R}^{n \times d}$函数的能量。我们研究了这个能量的正则性和优化性质，以及其通过蒙特卡洛近似$\mathcal{E}_p$（使用SW中的期望估计）。

    The Sliced Wasserstein (SW) distance has become a popular alternative to the Wasserstein distance for comparing probability measures. Widespread applications include image processing, domain adaptation and generative modelling, where it is common to optimise some parameters in order to minimise SW, which serves as a loss function between discrete probability measures (since measures admitting densities are numerically unattainable). All these optimisation problems bear the same sub-problem, which is minimising the Sliced Wasserstein energy. In this paper we study the properties of $\mathcal{E}: Y \longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z)$, i.e. the SW distance between two uniform discrete measures with the same amount of points as a function of the support $Y \in \mathbb{R}^{n \times d}$ of one of the measures. We investigate the regularity and optimisation properties of this energy, as well as its Monte-Carlo approximation $\mathcal{E}_p$ (estimating the expectation in SW using 
    
[^84]: 改进视觉-语言多模态数据集的图像描述能力

    Improving Multimodal Datasets with Image Captioning. (arXiv:2307.10350v1 [cs.LG])

    [http://arxiv.org/abs/2307.10350](http://arxiv.org/abs/2307.10350)

    通过探索混合原始和生成的图像描述的不同策略，我们的方法在ImageNet上超过了当前最佳降噪方法2%，在38个任务中平均提高了4%，我们的最佳方法在Flickr和MS-COCO检索上也提升了2倍。

    

    大规模网络数据集在CLIP和Flamingo等大型视觉-语言模型的成功中起到了关键作用。然而，原始网络数据存在噪音，现有的降噪方法往往会以数据的多样性为代价。我们的研究聚焦于图像描述的质量作为噪音的一个主要来源，并研究了如何通过生成的描述增加含有含义不明确文本的网络抓取数据点的实用性。通过探索原始模式和生成模式两种不同的混合策略，我们在ImageNet上超过了DataComp基准提出的最佳降噪方法2%，在38个任务中平均提高了4%，给定128M的图像-文本对候选池。我们最好的方法在Flickr和MS-COCO检索上也提升了2倍。然后，我们分析了合成描述为什么是一种有效的文本监督来源。在尝试不同的图像描述模型的同时，我们还证明了模型在标准图像描述基准上的表现（例如，NoCaps CIDEr）并不一定是

    Massive web datasets play a key role in the success of large vision-language models like CLIP and Flamingo. However, the raw web data is noisy, and existing filtering methods to reduce noise often come at the expense of data diversity. Our work focuses on caption quality as one major source of noise, and studies how generated captions can increase the utility of web-scraped datapoints with nondescript text. Through exploring different mixing strategies for raw and generated captions, we outperform the best filtering method proposed by the DataComp benchmark by 2% on ImageNet and 4% on average across 38 tasks, given a candidate pool of 128M image-text pairs. Our best approach is also 2x better at Flickr and MS-COCO retrieval. We then analyze what makes synthetic captions an effective source of text supervision. In experimenting with different image captioning models, we also demonstrate that the performance of a model on standard image captioning benchmarks (e.g., NoCaps CIDEr) is not a
    
[^85]: 使用大型语言模型进行硬件加速的代码检测

    Code Detection for Hardware Acceleration Using Large Language Models. (arXiv:2307.10348v1 [cs.SE])

    [http://arxiv.org/abs/2307.10348](http://arxiv.org/abs/2307.10348)

    本研究利用大型语言模型(LLMs)进行代码检测，并提出了一种新的提示策略，显著降低了误报，实现了出色的整体准确性，对现有的最先进的代码检测方法提出了重大挑战。

    

    大型语言模型(LLMs)已被广泛应用于许多任务中，通常超过了最先进的方法。虽然人们已经广泛研究了它们在代码生成方面的有效性（例如AlphaCode），但它们在代码检测方面的潜力尚未被探索。本研究首次分析了使用LLMs进行代码检测的情况。我们的研究考察了包括矩阵乘法、卷积和快速傅里叶变换在内的关键核心代码，这些代码是用C/C++实现的。我们提出了一种初步的简单提示和一种用于代码检测的新型提示策略。结果显示，传统的提示策略在精确性方面表现出色，但准确性较差（分别为GEMM、卷积和FFT的68.8%、22.3%和79.2%），因为存在大量的误报。我们的新型提示策略显著减少了误报，从而取得了出色的整体准确性（分别为91.1%、97.9%和99.7%）。这些结果对现有的最先进的代码检测方法提出了重大挑战。

    Large language models (LLMs) have been massively applied to many tasks, often surpassing state-of-the-art approaches. While their effectiveness in code generation has been extensively studied (e.g., AlphaCode), their potential for code detection remains unexplored.  This work presents the first analysis of code detection using LLMs. Our study examines essential kernels, including matrix multiplication, convolution, and fast-fourier transform, implemented in C/C++. We propose both a preliminary, naive prompt and a novel prompting strategy for code detection.  Results reveal that conventional prompting achieves great precision but poor accuracy (68.8%, 22.3%, and 79.2% for GEMM, convolution, and FFT, respectively) due to a high number of false positives. Our novel prompting strategy substantially reduces false positives, resulting in excellent overall accuracy (91.1%, 97.9%, and 99.7%, respectively). These results pose a considerable challenge to existing state-of-the-art code detection 
    
[^86]: ProtiGeno:一种利用蛋白质语言模型的原核短基因预测方法

    ProtiGeno: a prokaryotic short gene finder using protein language models. (arXiv:2307.10343v1 [q-bio.GN])

    [http://arxiv.org/abs/2307.10343](http://arxiv.org/abs/2307.10343)

    ProtiGeno是一种基于深度学习的方法，利用蛋白质语言模型在寻找短原核基因方面比当前先进的方法具有更高的准确性和召回率。

    

    原核基因预测在理解生物和其功能方面具有重要作用，在医学和生物技术中有应用。尽管当前的基因预测方法在寻找长基因方面非常敏感，但在寻找短基因(<180 nts)方面，它们的敏感性明显降低。原因是缺乏注释的基因数据以识别短开放阅读框(ORFs)中的区别特征。我们开发了一种基于深度学习的方法，称为ProtiGeno，专门针对短原核基因，该方法使用在数百万个进化蛋白质上训练的蛋白质语言模型。在对4,288个原核基因组进行系统大规模实验中，我们证明ProtiGeno相较于当前最先进的基因预测方法，在预测短编码和非编码基因方面具有更高的准确性和召回率。通过可视化预测的短基因的三维结构，我们讨论了ProtiGeno的预测特征和可能的局限性。

    Prokaryotic gene prediction plays an important role in understanding the biology of organisms and their function with applications in medicine and biotechnology. Although the current gene finders are highly sensitive in finding long genes, their sensitivity decreases noticeably in finding shorter genes (<180 nts). The culprit is insufficient annotated gene data to identify distinguishing features in short open reading frames (ORFs). We develop a deep learning-based method called ProtiGeno, specifically targeting short prokaryotic genes using a protein language model trained on millions of evolved proteins. In systematic large-scale experiments on 4,288 prokaryotic genomes, we demonstrate that ProtiGeno predicts short coding and noncoding genes with higher accuracy and recall than the current state-of-the-art gene finders. We discuss the predictive features of ProtiGeno and possible limitations by visualizing the three-dimensional structure of the predicted short genes. Data, codes, and
    
[^87]: IncDSI：递增可更新的文档检索

    IncDSI: Incrementally Updatable Document Retrieval. (arXiv:2307.10323v1 [cs.IR])

    [http://arxiv.org/abs/2307.10323](http://arxiv.org/abs/2307.10323)

    IncDSI是一种递增可更新的文档检索方法，它通过最小改变网络参数的约束优化问题，实现实时添加文档而无需重新训练整个模型，具有与重新训练模型相竞争的速度，能够实时更新的文档检索系统的开发。

    

    不同iable搜索索引是最近提出的一种文档检索范例，它将文档语料库的信息编码在神经网络的参数中，并直接将查询映射到相应的文档。这些模型在许多基准测试中取得了最先进的性能。这些模型具有一个重要限制：在训练模型之后添加新文档并不容易。我们提出了IncDSI，一种实时添加文档的方法（每个文档约20-50毫秒），而无需对整个数据集（甚至部分数据集）重新训练模型。相反，我们将添加文档的过程形式化为一个在网络参数上进行最小改变的约束优化问题。虽然速度更快几个数量级，但我们的方法与在整个数据集上重新训练模型相竞争，并且可以实时更新的文档检索系统的开发。我们的IncDSI代码

    Differentiable Search Index is a recently proposed paradigm for document retrieval, that encodes information about a corpus of documents within the parameters of a neural network and directly maps queries to corresponding documents. These models have achieved state-of-the-art performances for document retrieval across many benchmarks. These kinds of models have a significant limitation: it is not easy to add new documents after a model is trained. We propose IncDSI, a method to add documents in real time (about 20-50ms per document), without retraining the model on the entire dataset (or even parts thereof). Instead we formulate the addition of documents as a constrained optimization problem that makes minimal changes to the network parameters. Although orders of magnitude faster, our approach is competitive with re-training the model on the whole dataset and enables the development of document retrieval systems that can be updated with new information in real-time. Our code for IncDSI
    
[^88]: 机器学习驱动的研究中的可重复性问题

    Reproducibility in Machine Learning-Driven Research. (arXiv:2307.10320v1 [cs.LG])

    [http://arxiv.org/abs/2307.10320](http://arxiv.org/abs/2307.10320)

    该论文调查了机器学习驱动的研究中的可重复性问题，指出了无法复现的原因，以及当前可重复性水平不断提高的挑战。同时，提出了支持机器学习可重复性的潜在驱动因素。

    

    研究面临着可重复性危机，许多研究的结果和发现很难甚至不可能复现。机器学习和人工智能研究也面临同样的问题，通常这是因为未发布的数据和/或源代码，以及对机器学习训练条件的敏感性。尽管研究界讨论了不同的解决方案来解决这个问题，比如使用机器学习平台，但机器学习驱动的研究的可重复性水平并没有显著提高。因此，在这篇小型调查中，我们回顾了关于机器学习驱动的研究中可重复性的文献，主要目的有三个：（一）反思各个研究领域中机器学习可重复性的现状，（二）识别这些应用了机器学习的研究领域中存在的可重复性问题和障碍，（三）识别支持机器学习可重复性的可能驱动因素，如工具、实践和干预措施。通过这样做，我们希望做出贡献。

    Research is facing a reproducibility crisis, in which the results and findings of many studies are difficult or even impossible to reproduce. This is also the case in machine learning (ML) and artificial intelligence (AI) research. Often, this is the case due to unpublished data and/or source-code, and due to sensitivity to ML training conditions. Although different solutions to address this issue are discussed in the research community such as using ML platforms, the level of reproducibility in ML-driven research is not increasing substantially. Therefore, in this mini survey, we review the literature on reproducibility in ML-driven research with three main aims: (i) reflect on the current situation of ML reproducibility in various research fields, (ii) identify reproducibility issues and barriers that exist in these research fields applying ML, and (iii) identify potential drivers such as tools, practices, and interventions that support ML reproducibility. With this, we hope to contr
    
[^89]: 深入研究消除树型垂直联合学习中的标签泄露问题

    Eliminating Label Leakage in Tree-Based Vertical Federated Learning. (arXiv:2307.10318v1 [cs.LG])

    [http://arxiv.org/abs/2307.10318](http://arxiv.org/abs/2307.10318)

    本研究针对树型垂直联合学习中的标签泄露问题，引入了一种新的标签推断攻击方法ID2Graph，并提出了一种ID-LMID的防御机制，通过关注互信息正则化来防止标签泄露。实验结果表明ID2Graph攻击存在显著的泄露问题。

    

    垂直联合学习（VFL）使得具有共同用户集合的多个参与方能够在不分享私有数据的情况下训练机器学习模型。由于其可解释性和效率，基于树结构的模型在VFL中变得流行起来。然而，树型VFL的脆弱性尚未得到充分的研究。本研究首先引入了一种新颖的标签推断攻击方法ID2Graph，该攻击利用每个节点（即实例空间）分配的记录标识集合来推导私有训练标签。ID2Graph攻击生成训练样本的图结构，从图中提取社区，并使用社区信息对局部数据集进行聚类。为了抵御实例空间中的标签泄露，我们提出了一种有效的防御机制ID-LMID，该机制通过关注互信息正则化来防止标签泄露。在各种数据集上进行的综合实验表明，ID2Graph攻击呈现出显著的泄露问题。

    Vertical federated learning (VFL) enables multiple parties with disjoint features of a common user set to train a machine learning model without sharing their private data. Tree-based models have become prevalent in VFL due to their interpretability and efficiency. However, the vulnerability of tree-based VFL has not been sufficiently investigated. In this study, we first introduce a novel label inference attack, ID2Graph, which utilizes the sets of record-IDs assigned to each node (i.e., instance space) to deduce private training labels. The ID2Graph attack generates a graph structure from training samples, extracts communities from the graph, and clusters the local dataset using community information. To counteract label leakage from the instance space, we propose an effective defense mechanism, ID-LMID, which prevents label leakage by focusing on mutual information regularization. Comprehensive experiments conducted on various datasets reveal that the ID2Graph attack presents signif
    
[^90]: FedBug: 一种自底向上逐渐解冻的联邦学习框架

    FedBug: A Bottom-Up Gradual Unfreezing Framework for Federated Learning. (arXiv:2307.10317v1 [cs.LG])

    [http://arxiv.org/abs/2307.10317](http://arxiv.org/abs/2307.10317)

    FedBug是一个自底向上逐渐解冻的联邦学习框架，通过冻结和逐渐解冻模型层，实现了一种有效缓解客户端漂移现象的方法。

    

    联邦学习（FL）提供了一种协作训练框架，允许多个客户端在不损害数据隐私的情况下为共享模型做出贡献。由于本地数据集的异构性，更新的客户端模型可能会过拟合并与彼此发散，这被称为客户端漂移问题。在本文中，我们提出了FedBug（具有自底向上逐渐解冻的联邦学习），这是一种新颖的FL框架，旨在有效地减轻客户端漂移。FedBug自适应地利用每个全局轮次服务器分发的客户端模型参数作为跨客户端对齐的参考点。具体而言，在客户端上，FedBug从冻结整个模型开始，然后逐渐解冻层，从输入层到输出层。这种自底向上的方法允许模型训练解冻的新层将数据投影到一个潜在空间中，在这个空间中，分离超平面在所有客户端上保持一致。我们在理论上分析了FedBug

    Federated Learning (FL) offers a collaborative training framework, allowing multiple clients to contribute to a shared model without compromising data privacy. Due to the heterogeneous nature of local datasets, updated client models may overfit and diverge from one another, commonly known as the problem of client drift. In this paper, we propose FedBug (Federated Learning with Bottom-Up Gradual Unfreezing), a novel FL framework designed to effectively mitigate client drift. FedBug adaptively leverages the client model parameters, distributed by the server at each global round, as the reference points for cross-client alignment. Specifically, on the client side, FedBug begins by freezing the entire model, then gradually unfreezes the layers, from the input layer to the output layer. This bottom-up approach allows models to train the newly thawed layers to project data into a latent space, wherein the separating hyperplanes remain consistent across all clients. We theoretically analyze F
    
[^91]: 基于歌词的孟加拉歌曲情绪分类

    Mood Classification of Bangla Songs Based on Lyrics. (arXiv:2307.10314v1 [cs.IR])

    [http://arxiv.org/abs/2307.10314](http://arxiv.org/abs/2307.10314)

    本研究通过分析孟加拉歌曲的歌词，成功实现了对这些歌曲的情绪进行多类分类，包括快乐、悲伤、浪漫和放松，为使音乐更贴近人们的情感做出了重要贡献。

    

    音乐能唤起各种情绪，随着技术的进步，人们对音乐的接触也越来越多。然而对于展现不同人类情感的孟加拉音乐，相关的研究尚不足够。本文的作者旨在通过分析孟加拉歌曲的歌词来分类其情绪。为实现这一目标，研究人员收集了4000首孟加拉歌曲的歌词和流派，并运用自然语言处理和BERT算法来分析数据。在这4000首歌曲中，1513首代表悲伤情绪，1362首代表浪漫情绪，886首代表快乐，其余的239首被归类为放松。通过嵌入歌词，作者将这些歌曲分为四种情绪：快乐、悲伤、浪漫和放松。该研究对于实现音乐的多类情绪分类至关重要，使音乐更能与人们的情感产生共鸣。该文章详细描述了通过歌词准确推导出的四种情绪的自动化结果。

    Music can evoke various emotions, and with the advancement of technology, it has become more accessible to people. Bangla music, which portrays different human emotions, lacks sufficient research. The authors of this article aim to analyze Bangla songs and classify their moods based on the lyrics. To achieve this, this research has compiled a dataset of 4000 Bangla song lyrics, genres, and used Natural Language Processing and the Bert Algorithm to analyze the data. Among the 4000 songs, 1513 songs are represented for the sad mood, 1362 for the romantic mood, 886 for happiness, and the rest 239 are classified as relaxation. By embedding the lyrics of the songs, the authors have classified the songs into four moods: Happy, Sad, Romantic, and Relaxed. This research is crucial as it enables a multi-class classification of songs' moods, making the music more relatable to people's emotions. The article presents the automated result of the four moods accurately derived from the song lyrics.
    
[^92]: 时光与行动之织：使用时间点过程流建模人类活动序列

    Tapestry of Time and Actions: Modeling Human Activity Sequences using Temporal Point Process Flows. (arXiv:2307.10305v1 [cs.CV])

    [http://arxiv.org/abs/2307.10305](http://arxiv.org/abs/2307.10305)

    本文介绍了ProActive，一种神经标记的时间点过程（MTPP）框架，用于建模人类活动序列的动态。这种方法有助于活动持续时间预测、目标预测和下一步行动推荐等下游任务。

    

    人类始终参与各种各样的活动和任务，展示了他们适应不同场景的能力。任何人类活动都可以被表示为实现特定目标而执行的行动的时间序列。与从电子设备或机器中提取的时间序列数据不同，这些行动序列在其性质上高度不同 - 完成行动序列的时间可能会因不同个体而异。因此，理解这些序列的动态对于许多下游任务（如活动持续时间预测，目标预测，下一步行动推荐等）至关重要。现有的基于神经网络的学习连续时间活动序列（Continuous-Time Activity Sequence，CTAS）的方法仅限于仅有视觉数据存在或仅适用于特定任务，即仅限于下一步行动或目标预测。在本文中，我们提出了ProActive，一种神经标记的时间点过程（Marked Temporal Point Process，MTPP）框架，用于建模连续的人类活动序列。

    Human beings always engage in a vast range of activities and tasks that demonstrate their ability to adapt to different scenarios. Any human activity can be represented as a temporal sequence of actions performed to achieve a certain goal. Unlike the time series datasets extracted from electronics or machines, these action sequences are highly disparate in their nature -- the time to finish a sequence of actions can vary between different persons. Therefore, understanding the dynamics of these sequences is essential for many downstream tasks such as activity length prediction, goal prediction, next action recommendation, etc. Existing neural network-based approaches that learn a continuous-time activity sequence (or CTAS) are limited to the presence of only visual data or are designed specifically for a particular task, i.e., limited to next action or goal prediction. In this paper, we present ProActive, a neural marked temporal point process (MTPP) framework for modeling the continuou
    
[^93]: Polyffusion：一种具有内部和外部控制的多声乐谱生成扩散模型

    Polyffusion: A Diffusion Model for Polyphonic Score Generation with Internal and External Controls. (arXiv:2307.10304v1 [cs.SD])

    [http://arxiv.org/abs/2307.10304](http://arxiv.org/abs/2307.10304)

    Polyffusion是一种多声乐谱生成模型，通过内部和外部控制实现可控音乐生成。实验结果表明，该模型在多项音乐创作任务中表现优异，包括给定伴奏生成旋律、给定旋律生成伴奏、音乐片段修补和音乐编排等。

    

    我们提出了Polyffusion，一种以图像化的钢琴滑轮表示将音乐生成为多声乐谱的扩散模型。该模型能够进行可控的音乐生成，有两种范式：内部控制和外部控制。内部控制是指用户预定义音乐的一部分，然后让模型填充剩余部分，类似于掩蔽音乐生成（或音乐修补）的任务。外部控制通过交叉关注机制，将模型与外部但相关的信息(如和弦、质地或其他特征)结合。我们展示了Polyffusion通过使用内部和外部控制，统一了一系列音乐创作任务，包括给定伴奏生成旋律，给定旋律生成伴奏，任意音乐片段修补，以及给定和弦或质地进行音乐编排。实验结果表明，我们的模型在性能上明显优于现有的Transformer和基于采样的基准模型。

    We propose Polyffusion, a diffusion model that generates polyphonic music scores by regarding music as image-like piano roll representations. The model is capable of controllable music generation with two paradigms: internal control and external control. Internal control refers to the process in which users pre-define a part of the music and then let the model infill the rest, similar to the task of masked music generation (or music inpainting). External control conditions the model with external yet related information, such as chord, texture, or other features, via the cross-attention mechanism. We show that by using internal and external controls, Polyffusion unifies a wide range of music creation tasks, including melody generation given accompaniment, accompaniment generation given melody, arbitrary music segment inpainting, and music arrangement given chords or textures. Experimental results show that our model significantly outperforms existing Transformer and sampling-based base
    
[^94]: 分析体育评论以实现自动识别事件并提取洞见

    Analyzing sports commentary in order to automatically recognize events and extract insights. (arXiv:2307.10303v1 [cs.CL])

    [http://arxiv.org/abs/2307.10303](http://arxiv.org/abs/2307.10303)

    通过分析多种体育赛事的实时评论，利用自然语言处理技术自动识别主要行动，并通过分类和情感分析提取洞见。

    

    本文中，我们仔细研究了如何利用多种自然语言处理技术和方法，以自动识别体育赛事中的主要行动。我们通过分析不同来源的现场体育评论，并将这些主要行动分类到不同的类别中，来提取洞见。我们还研究了情感分析是否可以帮助检测这些主要行动。

    In this paper, we carefully investigate how we can use multiple different Natural Language Processing techniques and methods in order to automatically recognize the main actions in sports events. We aim to extract insights by analyzing live sport commentaries from different sources and by classifying these major actions into different categories. We also study if sentiment analysis could help detect these main actions.
    
[^95]: 因果性导向的鲁棒性：利用一般性可加干预

    Causality-oriented robustness: exploiting general additive interventions. (arXiv:2307.10299v1 [stat.ME])

    [http://arxiv.org/abs/2307.10299](http://arxiv.org/abs/2307.10299)

    本文提出了一种名为DRIG的方法，通过利用训练数据中的一般性可加干预，在预测模型中结合了内分布预测和因果性，从而实现了对未见干预的鲁棒预测。

    

    由于在现实应用中经常发生分布变化，急需开发对这种变化具有鲁棒性的预测模型。现有的框架，如经验风险最小化或分布鲁棒优化，要么对未见分布缺乏通用性，要么依赖于假定的距离度量。相比之下，因果性提供了一种基于数据和结构的稳健预测方法。然而，进行因果推断所需的假设可能过于严格，这种因果模型提供的鲁棒性常常缺乏灵活性。在本文中，我们专注于因果性导向的鲁棒性，并提出了一种名为DRIG（Distributional Robustness via Invariant Gradients）的方法，该方法利用训练数据中的一般性可加干预，以实现对未见干预的鲁棒预测，并在内分布预测和因果性之间自然地进行插值。在线性设置中，我们证明了DRIG产生的预测是

    Since distribution shifts are common in real-world applications, there is a pressing need for developing prediction models that are robust against such shifts. Existing frameworks, such as empirical risk minimization or distributionally robust optimization, either lack generalizability for unseen distributions or rely on postulated distance measures. Alternatively, causality offers a data-driven and structural perspective to robust predictions. However, the assumptions necessary for causal inference can be overly stringent, and the robustness offered by such causal models often lacks flexibility. In this paper, we focus on causality-oriented robustness and propose Distributional Robustness via Invariant Gradients (DRIG), a method that exploits general additive interventions in training data for robust predictions against unseen interventions, and naturally interpolates between in-distribution prediction and causality. In a linear setting, we prove that DRIG yields predictions that are 
    
[^96]: 在乳腺X光摄影图像中实现自动语义分割

    Towards Automated Semantic Segmentation in Mammography Images. (arXiv:2307.10296v1 [eess.IV])

    [http://arxiv.org/abs/2307.10296](http://arxiv.org/abs/2307.10296)

    本文提出了一个基于深度学习的框架，用于在乳腺X光摄影图像中分割乳头、胸大肌、纤维腺体组织和脂肪组织。实验证明该框架在各种复杂情况下具有准确的分割性能，可用于临床实践中。

    

    乳腺X光摄影图像广泛用于检测不可触及的乳腺病变或结节，以预防癌症并在必要时提供干预的机会。识别一些感兴趣结构对于诊断和评估图像的充分性至关重要。因此，计算机辅助检测系统可以通过自动分割这些标志性结构来辅助医学解释。在本文中，我们提出了一个基于深度学习的框架，用于在标准视图乳腺X光摄影图像中分割乳头、胸大肌、纤维腺体组织和脂肪组织。我们引入了一个大型私有分割数据集，并进行了考虑不同深度学习模型架构的大量实验。我们的实验证明，在不同变异和具有挑战性的情况下具有准确的分割性能，表明该框架可以集成到临床实践中。

    Mammography images are widely used to detect non-palpable breast lesions or nodules, preventing cancer and providing the opportunity to plan interventions when necessary. The identification of some structures of interest is essential to make a diagnosis and evaluate image adequacy. Thus, computer-aided detection systems can be helpful in assisting medical interpretation by automatically segmenting these landmark structures. In this paper, we propose a deep learning-based framework for the segmentation of the nipple, the pectoral muscle, the fibroglandular tissue, and the fatty tissue on standard-view mammography images. We introduce a large private segmentation dataset and extensive experiments considering different deep-learning model architectures. Our experiments demonstrate accurate segmentation performance on variate and challenging cases, showing that this framework can be integrated into clinical practice.
    
[^97]: ECSIC: 用于立体图像压缩的极线交叉注意力技术

    ECSIC: Epipolar Cross Attention for Stereo Image Compression. (arXiv:2307.10284v1 [eess.IV])

    [http://arxiv.org/abs/2307.10284](http://arxiv.org/abs/2307.10284)

    ECSIC是一种用于立体图像压缩的新方法，通过利用左右图像之间的相互信息进行联合压缩，并使用新颖的立体交叉注意力模块和立体上下文模块实现。与现有方法相比，ECSIC在两个流行的立体图像数据集上取得了最先进的性能，并且具有快速编码和解码的特性。

    

    在本文中，我们提出了一种新颖的学习方法ECSIC，用于立体图像压缩。我们的方法通过利用立体图像对左右图像之间的相互信息，采用一种新颖的立体交叉注意力（SCA）模块和两个立体上下文模块，以联合方式压缩左右图像。SCA模块在两个图像的对应极线范围内进行交叉注意力处理，并且并行处理它们。立体上下文模块通过使用第一个图像作为上下文来改善对第二个编码图像的熵估计。我们进行了大量的剔除实验，证明了所提出模块的有效性，并与现有方法进行了全面的定量和定性比较。ECSIC在两个常用的立体图像数据集Cityscapes和InStereo2k上达到了立体图像压缩模型中的最先进性能，同时允许快速编码和解码，非常适用于实时应用。

    In this paper, we present ECSIC, a novel learned method for stereo image compression. Our proposed method compresses the left and right images in a joint manner by exploiting the mutual information between the images of the stereo image pair using a novel stereo cross attention (SCA) module and two stereo context modules. The SCA module performs cross-attention restricted to the corresponding epipolar lines of the two images and processes them in parallel. The stereo context modules improve the entropy estimation of the second encoded image by using the first image as a context. We conduct an extensive ablation study demonstrating the effectiveness of the proposed modules and a comprehensive quantitative and qualitative comparison with existing methods. ECSIC achieves state-of-the-art performance among stereo image compression models on the two popular stereo image datasets Cityscapes and InStereo2k while allowing for fast encoding and decoding, making it highly practical for real-time
    
[^98]: 使用提示条件微调实现零样本领域敏感语音识别

    Zero-shot Domain-sensitive Speech Recognition with Prompt-conditioning Fine-tuning. (arXiv:2307.10274v1 [eess.AS])

    [http://arxiv.org/abs/2307.10274](http://arxiv.org/abs/2307.10274)

    本研究提出了一种零样本领域敏感语音识别方法，利用文本提示来生成领域敏感模型，通过微调预训练的端到端模型实现。实验结果表明，该方法在不同领域和提示上下文下均取得了良好的性能，词错误率降低达到最高33%。通过仅使用文本进行微调，该模型在医学对话数据集上的识别效果最佳，词错误率降低达到29%。

    

    本研究提出了一种方法来创建利用文本领域信息的领域敏感语音识别模型，通过将其生成条件化在给定的文本提示上实现。通过对预训练的端到端模型（Whisper）进行微调，从提示示例中学习，这一目标得以实现。我们展示了这种能力可以推广到不同的领域和各种提示上下文，我们的模型在来自不同领域的未见数据集上获得了多达33％的词错误率（WER）降低，例如医学对话，空中交通控制通信和金融会议等。考虑到音频-文本对数据的有限可用性，我们进一步将我们的方法扩展到仅文本微调，以实现领域敏感性和领域适应性。我们证明了我们的仅文本微调模型也可以关注各种提示上下文，该模型在医学对话数据集上的WER降低最多达到29％。

    In this work, we propose a method to create domain-sensitive speech recognition models that utilize textual domain information by conditioning its generation on a given text prompt. This is accomplished by fine-tuning a pre-trained, end-to-end model (Whisper) to learn from demonstrations with prompt examples. We show that this ability can be generalized to different domains and even various prompt contexts, with our model gaining a Word Error Rate (WER) reduction of up to 33% on unseen datasets from various domains, such as medical conversation, air traffic control communication, and financial meetings. Considering the limited availability of audio-transcript pair data, we further extend our method to text-only fine-tuning to achieve domain sensitivity as well as domain adaptation. We demonstrate that our text-only fine-tuned model can also attend to various prompt contexts, with the model reaching the most WER reduction of 29% on the medical conversation dataset.
    
[^99]: 用于验证深度神经网络的DPLL(T)框架

    A DPLL(T) Framework for Verifying Deep Neural Networks. (arXiv:2307.10266v1 [cs.LG])

    [http://arxiv.org/abs/2307.10266](http://arxiv.org/abs/2307.10266)

    这项工作介绍了一个新的约束求解方法NeuralSAT，用于验证深度神经网络。Preliminary results show that the NeuralSAT prototype is competitive to the state-of-the-art.

    

    深度神经网络(DNNs)已经成为解决现实世界问题的有效方法。然而，与人工编写的软件一样，自动生成的DNNs可能存在错误并受到攻击。因此，近年来在开发有效和可扩展的DNN验证技术和工具方面引起了广泛关注。在这项工作中，我们介绍了NeuralSAT，一种用于DNN验证的新的约束求解方法。NeuralSAT的设计遵循了用于现代SMT求解的DPLL(T)算法，包括（冲突）子句学习、抽象和理论求解，因此NeuralSAT可以被视为DNNs的一个SMT框架。初步结果显示，NeuralSAT原型与最先进的方法相竞争。我们希望通过适当的优化和工程化，NeuralSAT能够将现代SAT/SMT求解器的能力和成功带到DNN验证中。NeuralSAT可从以下链接获取：https://github.com/dynaroars/neuralsat-solver

    Deep Neural Networks (DNNs) have emerged as an effective approach to tackling real-world problems. However, like human-written software, automatically-generated DNNs can have bugs and be attacked. This thus attracts many recent interests in developing effective and scalable DNN verification techniques and tools. In this work, we introduce a NeuralSAT, a new constraint solving approach to DNN verification. The design of NeuralSAT follows the DPLL(T) algorithm used modern SMT solving, which includes (conflict) clause learning, abstraction, and theory solving, and thus NeuralSAT can be considered as an SMT framework for DNNs. Preliminary results show that the NeuralSAT prototype is competitive to the state-of-the-art. We hope, with proper optimization and engineering, NeuralSAT will carry the power and success of modern SAT/SMT solvers to DNN verification. NeuralSAT is avaliable from: https://github.com/dynaroars/neuralsat-solver
    
[^100]: 超参数调整指南：适用于scikit-learn、PyTorch、river和spotPython的指南

    Hyperparameter Tuning Cookbook: A guide for scikit-learn, PyTorch, river, and spotPython. (arXiv:2307.10262v1 [cs.LG])

    [http://arxiv.org/abs/2307.10262](http://arxiv.org/abs/2307.10262)

    本文提供了使用spotPython进行scikit-learn、PyTorch和river的超参数调整的全面指南。重点介绍了spotPython的优化过程和超参数调整，并提供了几个实际案例研究。该指南为对Python超参数调整感兴趣的人们提供了实用的起点。

    

    本文提供了使用spotPython对scikit-learn、PyTorch和river进行超参数调整的全面指南。第一部分介绍了spotPython的基于代理模型的优化过程，第二部分着重介绍了超参数调整。文中提供了几个案例研究，包括对scikit-learn模型（如支持向量分类，随机森林，梯度提升（XGB）和K最近邻（KNN））以及river中的Hoeffding自适应树回归器进行超参数调整。还讨论了将spotPython集成到PyTorch和PyTorch Lightning训练工作流中的方法。通过实践和逐步解释的方式，本手册为对使用Python进行超参数调整感兴趣的任何人提供了实用的起点。重点包括Tensorboard、PyTorch Lightning、spotPython和river之间的相互作用。该出版物正在开发中，更新内容可在对应的网页上获取。

    This document provides a comprehensive guide to hyperparameter tuning using spotPython for scikit-learn, PyTorch, and river. The first part introduces spotPython's surrogate model-based optimization process, while the second part focuses on hyperparameter tuning. Several case studies are presented, including hyperparameter tuning for sklearn models such as Support Vector Classification, Random Forests, Gradient Boosting (XGB), and K-nearest neighbors (KNN), as well as a Hoeffding Adaptive Tree Regressor from river. The integration of spotPython into the PyTorch and PyTorch Lightning training workflow is also discussed. With a hands-on approach and step-by-step explanations, this cookbook serves as a practical starting point for anyone interested in hyperparameter tuning with Python. Highlights include the interplay between Tensorboard, PyTorch Lightning, spotPython, and river. This publication is under development, with updates available on the corresponding webpage.
    
[^101]: 利用模式挖掘和聚类自动化学生网络安全培训的评估

    Student Assessment in Cybersecurity Training Automated by Pattern Mining and Clustering. (arXiv:2307.10260v1 [cs.CR])

    [http://arxiv.org/abs/2307.10260](http://arxiv.org/abs/2307.10260)

    本研究利用模式挖掘和聚类分析自动化学生网络安全培训的评估。研究发现，通过分析学员与学习环境的互动数据，可以揭示学员的学习过程，并为他们提供有针对性的反馈来帮助他们学习。

    

    实践型网络安全培训使学生和专业人士可以使用各种工具并提高其技术能力。培训发生在一个交互式学习环境中，可以在完善的操作系统、网络和应用程序中完成复杂的任务。在培训过程中，学习环境可以收集有关学员与环境互动的数据，例如他们对命令行工具的使用。这些数据包含表明学员学习过程的模式，揭示它们可以评估学员并提供反馈来帮助他们学习。然而，对这些数据的自动化分析是具有挑战性的。培训任务涉及复杂的问题解决，有多种不同的解决方法可能。此外，学员产生了大量的互动数据。本文使用数据挖掘和机器学习技术对来自18个网络安全培训会话的数据集进行探索。

    Hands-on cybersecurity training allows students and professionals to practice various tools and improve their technical skills. The training occurs in an interactive learning environment that enables completing sophisticated tasks in full-fledged operating systems, networks, and applications. During the training, the learning environment allows collecting data about trainees' interactions with the environment, such as their usage of command-line tools. These data contain patterns indicative of trainees' learning processes, and revealing them allows to assess the trainees and provide feedback to help them learn. However, automated analysis of these data is challenging. The training tasks feature complex problem-solving, and many different solution approaches are possible. Moreover, the trainees generate vast amounts of interaction data. This paper explores a dataset from 18 cybersecurity training sessions using data mining and machine learning techniques. We employed pattern mining and 
    
[^102]: 随机重启隐藏马尔可夫模型与Boosting在恶意软件检测中的比较

    Hidden Markov Models with Random Restarts vs Boosting for Malware Detection. (arXiv:2307.10256v1 [cs.CR])

    [http://arxiv.org/abs/2307.10256](http://arxiv.org/abs/2307.10256)

    本研究比较了在恶意软件检测中使用AdaBoost增强的HMM和使用多次随机重启训练的HMM，并发现随机重启对于大部分情况表现出令人惊喜的效果。只有在训练数据严重有限的最困难的情况下，增强才提供了足够的改进。

    

    有效和高效的恶意软件检测是构建安全数字系统的研究前沿。与许多其他领域一样，恶意软件检测研究在应用机器学习算法方面有了显著增长。在模式匹配和恶意软件检测领域中，一种被广泛使用的机器学习技术是隐藏马尔可夫模型（HMMs）。HMM的训练基于爬山算法，因此我们通常可以通过多次使用不同的初始值进行训练来改善模型。在本研究中，我们在恶意软件检测的背景下，比较了使用AdaBoost进行增强的HMM与使用多次随机重启训练的HMM。这些技术被应用于各种具有挑战性的恶意软件数据集。我们发现，在与增强相比下，随机重启表现出令人惊讶的好。只有在最困难的“冷启动”情况下（训练数据严重有限），增强似乎提供足够的改进来使其有正当的价值。

    Effective and efficient malware detection is at the forefront of research into building secure digital systems. As with many other fields, malware detection research has seen a dramatic increase in the application of machine learning algorithms. One machine learning technique that has been used widely in the field of pattern matching in general-and malware detection in particular-is hidden Markov models (HMMs). HMM training is based on a hill climb, and hence we can often improve a model by training multiple times with different initial values. In this research, we compare boosted HMMs (using AdaBoost) to HMMs trained with multiple random restarts, in the context of malware detection. These techniques are applied to a variety of challenging malware datasets. We find that random restarts perform surprisingly well in comparison to boosting. Only in the most difficult "cold start" cases (where training data is severely limited) does boosting appear to offer sufficient improvement to justi
    
[^103]: 高效的选择性注意力LSTM用于井曲线合成

    Efficient selective attention LSTM for well log curve synthesis. (arXiv:2307.10253v1 [cs.LG])

    [http://arxiv.org/abs/2307.10253](http://arxiv.org/abs/2307.10253)

    本文提出了一种高效的选择性注意力LSTM方法，用于预测缺失的井曲线。通过实验证实了该方法的有效性和可行性。

    

    非核心钻井逐渐成为地质工程中的主要勘探方法，井曲线作为地质信息的主要载体日益重要。然而，地质环境、测井设备、钻孔质量和突发事件等因素都会影响井曲线的质量。以往的重新测井或手工修正方法成本高效率低。本文提出了一种利用现有数据预测缺失井曲线的机器学习方法，并通过实验证实了其有效性和可行性。所提方法在传统的长短期记忆（LSTM）神经网络基础上加入了自注意机制来分析数据的空间依赖性。它有选择地将LSTM中的主导计算结果包括在内，将计算复杂度从O(n^2)降至O(nlogn)，从而提高了计算效率

    Non-core drilling has gradually become the primary exploration method in geological engineering, and well logging curves have increasingly gained importance as the main carriers of geological information. However, factors such as geological environment, logging equipment, borehole quality, and unexpected events can all impact the quality of well logging curves. Previous methods of re-logging or manual corrections have been associated with high costs and low efficiency. This paper proposes a machine learning method that utilizes existing data to predict missing well logging curves, and its effectiveness and feasibility have been validated through experiments. The proposed method builds upon the traditional Long Short-Term Memory (LSTM) neural network by incorporating a self-attention mechanism to analyze the spatial dependencies of the data. It selectively includes the dominant computational results in the LSTM, reducing the computational complexity from O(n^2) to O(nlogn) and improving
    
[^104]: 一个基于机器学习的实证评估：在归因攻击中，对于低级攻击模式和高级攻击模式来说，威胁行为者的高层攻击模式效果更好。

    A Machine Learning based Empirical Evaluation of Cyber Threat Actors High Level Attack Patterns over Low level Attack Patterns in Attributing Attacks. (arXiv:2307.10252v1 [cs.CR])

    [http://arxiv.org/abs/2307.10252](http://arxiv.org/abs/2307.10252)

    通过基于机器学习的实证评估，本研究表明，在网络威胁归因中，高级攻击模式表现出更好的效果，相比于低级攻击模式。

    

    网络威胁归因是在网络空间中识别攻击事件的行为者的过程。准确和及时的威胁归因在应用适当和及时的防御机制上起到重要作用。对于网络安全分析人员来说，通过蜜罐部署、入侵检测系统、防火墙和溯源程序收集攻击模式的手动分析仍然是首选的威胁归因方法。这些攻击模式是低级威胁迹象（IOC），它们代表了对手在他们的攻击中使用的策略、技术、程序（TTP）和软件工具。攻击者很少重复使用它们，它们也可以被篡改，导致错误和不公正的归因。为了实证评估和比较这两种IOC的有效性，有两个问题需要解决。第一个问题是，在最近的研究工作中，低级IOC对于网络威胁归因的效果不佳。

    Cyber threat attribution is the process of identifying the actor of an attack incident in cyberspace. An accurate and timely threat attribution plays an important role in deterring future attacks by applying appropriate and timely defense mechanisms. Manual analysis of attack patterns gathered by honeypot deployments, intrusion detection systems, firewalls, and via trace-back procedures is still the preferred method of security analysts for cyber threat attribution. Such attack patterns are low-level Indicators of Compromise (IOC). They represent Tactics, Techniques, Procedures (TTP), and software tools used by the adversaries in their campaigns. The adversaries rarely re-use them. They can also be manipulated, resulting in false and unfair attribution. To empirically evaluate and compare the effectiveness of both kinds of IOC, there are two problems that need to be addressed. The first problem is that in recent research works, the ineffectiveness of low-level IOC for cyber threat attr
    
[^105]: 从叙事文本中自动获取行动模型

    Automated Action Model Acquisition from Narrative Texts. (arXiv:2307.10247v1 [cs.CL])

    [http://arxiv.org/abs/2307.10247](http://arxiv.org/abs/2307.10247)

    NaRuto是一个系统，可以从叙事文本中自动提取结构化事件，并生成高质量的行动模型，优于现有的完全自动化方法和与半自动化方法相媲美。

    

    行动模型以前提/效果公理的形式存在，为人工智能代理提供行动之间的因果关联和动机连接。行动模型获取被认为是计划技术应用中的瓶颈，特别是在叙事计划中。从叙事文本中以自动化的方式获取行动模型是必要的，但由于这样的文本本质上复杂，因此具有挑战性。我们提出了NaRuto，一个系统，它可以从叙事文本中提取结构化事件，并基于常识事件关系的预测以及文本上的矛盾和相似性无监督地生成计划语言风格的行动模型。经典的叙事计划领域的实验结果显示，NaRuto可以生成质量显著优于现有完全自动化方法的行动模型，甚至与半自动化方法的行动模型相媲美。

    Action models, which take the form of precondition/effect axioms, facilitate causal and motivational connections between actions for AI agents. Action model acquisition has been identified as a bottleneck in the application of planning technology, especially within narrative planning. Acquiring action models from narrative texts in an automated way is essential, but challenging because of the inherent complexities of such texts. We present NaRuto, a system that extracts structured events from narrative text and subsequently generates planning-language-style action models based on predictions of commonsense event relations, as well as textual contradictions and similarities, in an unsupervised manner. Experimental results in classical narrative planning domains show that NaRuto can generate action models of significantly better quality than existing fully automated methods, and even on par with those of semi-automated methods.
    
[^106]: 深度神经网络和脑对齐：脑编码和解码（综述）

    Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey). (arXiv:2307.10246v1 [q-bio.NC])

    [http://arxiv.org/abs/2307.10246](http://arxiv.org/abs/2307.10246)

    本文综述了深度神经网络和脑对齐的研究，重点在于脑编码和解码模型的应用。这些模型对于理解大脑的信息处理机制以及设计脑机接口具有重要意义。

    

    大脑如何表示不同的信息模式？我们能否设计出一个可以自动理解用户思考内容的系统？这些问题可以通过研究功能磁共振成像（fMRI）等大脑记录来回答。作为第一步，神经科学界为被动阅读/听觉/观看概念词汇、叙述、图片和电影相关的认知神经科学数据集作出了贡献。过去二十年中，还提出了使用这些数据集的编码和解码模型。这些模型作为基础研究中的额外工具，在认知科学和神经科学领域有着多种实际应用。编码模型旨在自动地生成fMRI大脑表征，给定一个刺激。它们在评估和诊断神经系统疾病以及设计大脑损伤治疗方法方面有着多种实际应用。解码模型解决了根据fMRI重构刺激的逆问题。它们对于理解大脑如何处理信息以及设计脑机接口的发展都有着重要意义。

    How does the brain represent different modes of information? Can we design a system that automatically understands what the user is thinking? Such questions can be answered by studying brain recordings like functional magnetic resonance imaging (fMRI). As a first step, the neuroscience community has contributed several large cognitive neuroscience datasets related to passive reading/listening/viewing of concept words, narratives, pictures and movies. Encoding and decoding models using these datasets have also been proposed in the past two decades. These models serve as additional tools for basic research in cognitive science and neuroscience. Encoding models aim at generating fMRI brain representations given a stimulus automatically. They have several practical applications in evaluating and diagnosing neurological conditions and thus also help design therapies for brain damage. Decoding models solve the inverse problem of reconstructing the stimuli given the fMRI. They are useful for 
    
[^107]: 对深度推荐系统抗硬件错误能力的评估和增强

    Evaluating and Enhancing Robustness of Deep Recommendation Systems Against Hardware Errors. (arXiv:2307.10244v1 [cs.IR])

    [http://arxiv.org/abs/2307.10244](http://arxiv.org/abs/2307.10244)

    本研究对深度推荐系统在面对硬件错误时的健壮性进行了系统研究，开发了一个错误注入框架Terrorch，发现激活剪裁是一种有希望的错误缓解方法，可以恢复高达30%的被降低的AUC-ROC得分。

    

    深度推荐系统（DRS）严重依赖于专用的高性能计算硬件和加速器，以优化能源、效率和推荐质量。尽管目前在部署DRS的大规模系统中观察到硬件错误的增加，但DRS的健壮性一直未受到重视。本文首次系统地研究了DRS在面对硬件错误时的健壮性。我们开发了一个名为Terrorch的用户友好、高效灵活的错误注入框架，基于广泛使用的PyTorch。我们评估了广泛的模型和数据集，观察到DRS的健壮性受到多种因素的影响，包括模型参数和输入特征。我们还探索了3种错误缓解方法，包括基于算法的容错（ABFT）、激活剪裁和选择性位保护（SBP）。我们发现应用激活剪裁可以恢复被降低的AUC-ROC得分高达30%，这使其成为一种有希望的缓解方法。

    Deep recommendation systems (DRS) heavily depend on specialized HPC hardware and accelerators to optimize energy, efficiency, and recommendation quality. Despite the growing number of hardware errors observed in large-scale fleet systems where DRS are deployed, the robustness of DRS has been largely overlooked. This paper presents the first systematic study of DRS robustness against hardware errors. We develop Terrorch, a user-friendly, efficient and flexible error injection framework on top of the widely-used PyTorch. We evaluate a wide range of models and datasets and observe that the DRS robustness against hardware errors is influenced by various factors from model parameters to input characteristics. We also explore 3 error mitigation methods including algorithm based fault tolerance (ABFT), activation clipping and selective bit protection (SBP). We find that applying activation clipping can recover up to 30% of the degraded AUC-ROC score, making it a promising mitigation method.
    
[^108]: CoNAN：无约束人脸特征融合的条件神经聚合网络

    CoNAN: Conditional Neural Aggregation Network For Unconstrained Face Feature Fusion. (arXiv:2307.10237v1 [cs.CV])

    [http://arxiv.org/abs/2307.10237](http://arxiv.org/abs/2307.10237)

    CoNAN是一种用于无约束人脸特征融合的条件神经聚合网络，针对在长距离和高高度环境下捕获的极低分辨率人脸，利用特征分布调节方法来进行模板聚合。

    

    在不受限制和无控制的环境下，如长距离、低分辨率、不同视角、光照、姿态和大气条件下，从图像集中进行人脸识别是具有挑战性的。人脸特征聚合在这类识别系统中起到关键作用，它涉及将模板中的N个特征表示聚合成一个全局表示。现有的传统人脸特征聚合方法要么利用元数据，要么利用高维中间特征表示来估计特征质量进行聚合。然而，对于在远距离和高高度环境下捕获的极低分辨率人脸，生成高质量的元数据或风格信息是不可行的。为了克服这些限制，我们提出了一种称为CoNAN的特征分布调节方法来进行模板聚合。具体而言，我们的方法旨在学习一个在输入特征分布信息条件下的上下文向量。

    Face recognition from image sets acquired under unregulated and uncontrolled settings, such as at large distances, low resolutions, varying viewpoints, illumination, pose, and atmospheric conditions, is challenging. Face feature aggregation, which involves aggregating a set of N feature representations present in a template into a single global representation, plays a pivotal role in such recognition systems. Existing works in traditional face feature aggregation either utilize metadata or high-dimensional intermediate feature representations to estimate feature quality for aggregation. However, generating high-quality metadata or style information is not feasible for extremely low-resolution faces captured in long-range and high altitude settings. To overcome these limitations, we propose a feature distribution conditioning approach called CoNAN for template aggregation. Specifically, our method aims to learn a context vector conditioned over the distribution information of the incomi
    
[^109]: SentimentGPT：利用GPT进行高级情感分析及其与当前机器学习方法的差异

    SentimentGPT: Exploiting GPT for Advanced Sentiment Analysis and its Departure from Current Machine Learning. (arXiv:2307.10234v1 [cs.CL])

    [http://arxiv.org/abs/2307.10234](http://arxiv.org/abs/2307.10234)

    本研究通过利用GPT进行高级情感分析，并考察其与当前机器学习方法的差异，发现GPT方法相较于其他模型在预测性能上具有显著优势，并有效解决了情感分析任务中的一些挑战，如理解上下文和检测讽刺。

    

    本研究对情感分析中各种生成预训练转换器（GPT）方法进行了全面的考察，特别是在SemEval 2017数据集的任务4中。采用了三种主要策略：1）使用GPT-3.5 Turbo进行提示工程，2）对GPT模型进行微调，3）采用创新的嵌入分类方法。研究结果揭示了这些策略和个别GPT模型之间的详细比较见解，展示了它们独特的优势和潜在的局限性。此外，本研究将这些基于GPT的方法与其他同时代、高性能的模型在相同数据集上进行比较。结果表明，GPT方法在预测性能方面具有显著的优势，相较于最先进技术，F1分数增加了22%以上。此外，本论文还探讨了情感分析任务中的常见挑战，如理解上下文和检测讽刺。研究强调了GPT方法的重要价值和潜力。

    This study presents a thorough examination of various Generative Pretrained Transformer (GPT) methodologies in sentiment analysis, specifically in the context of Task 4 on the SemEval 2017 dataset. Three primary strategies are employed: 1) prompt engineering using the advanced GPT-3.5 Turbo, 2) fine-tuning GPT models, and 3) an inventive approach to embedding classification. The research yields detailed comparative insights among these strategies and individual GPT models, revealing their unique strengths and potential limitations. Additionally, the study compares these GPT-based methodologies with other contemporary, high-performing models previously used with the same dataset. The results illustrate the significant superiority of the GPT approaches in terms of predictive performance, more than 22% in F1-score compared to the state-of-the-art. Further, the paper addresses common challenges in sentiment analysis tasks, such as understanding context and detecting sarcasm. It underscores
    
[^110]: 癌症临床实践指南的自动化知识建模

    Automated Knowledge Modeling for Cancer Clinical Practice Guidelines. (arXiv:2307.10231v1 [cs.AI])

    [http://arxiv.org/abs/2307.10231](http://arxiv.org/abs/2307.10231)

    本研究提出了一种自动化方法，从国家综合癌症网络（NCCN）肿瘤学临床指南中提取知识，并生成包含该知识的结构化模型。通过使用癌症分期信息、统一医学语言系统（UMLS）和国家癌症研究所的词库（NCIt）概念以及节点分类的增强策略，该模型可以实现程序化遍历和查询。

    

    由于积极研究产生的新证据，癌症疾病的临床实践指南（CPGs）发展迅速。目前，CPGs主要以不适合管理这种发展知识的文档格式发布。需要一种适用于程序交互的指南文档的知识模型。本研究提出了一种从国家综合癌症网络（NCCN）肿瘤学CPGs中提取知识并生成包含提取的知识的结构化模型的自动化方法。使用两个版本的NCCN非小细胞肺癌（NSCLC）CPG对所提出的方法进行了测试，以展示其忠实提取和建模知识的效果。还提出了三种增强模型的策略，包括使用癌症分期信息、统一医学语言系统（UMLS）的元词库和国家癌症研究所的词库（NCIt）概念以及节点分类，以实现程序化遍历和查询。

    Clinical Practice Guidelines (CPGs) for cancer diseases evolve rapidly due to new evidence generated by active research. Currently, CPGs are primarily published in a document format that is ill-suited for managing this developing knowledge. A knowledge model of the guidelines document suitable for programmatic interaction is required. This work proposes an automated method for extraction of knowledge from National Comprehensive Cancer Network (NCCN) CPGs in Oncology and generating a structured model containing the retrieved knowledge. The proposed method was tested using two versions of NCCN Non-Small Cell Lung Cancer (NSCLC) CPG to demonstrate the effectiveness in faithful extraction and modeling of knowledge. Three enrichment strategies using Cancer staging information, Unified Medical Language System (UMLS) Metathesaurus & National Cancer Institute thesaurus (NCIt) concepts, and Node classification are also presented to enhance the model towards enabling programmatic traversal and q
    
[^111]: 在增强的不变关系知识上探索超关系时间知识图的链接预测

    Exploring Link Prediction over Hyper-Relational Temporal Knowledge Graphs Enhanced with Time-Invariant Relational Knowledge. (arXiv:2307.10219v1 [cs.AI])

    [http://arxiv.org/abs/2307.10219](http://arxiv.org/abs/2307.10219)

    这项研究填补了时间KG和超关系KG推理之间的差距，并开发了两个新的基准超关系TKG数据集。

    

    超关系知识图(HKGs)是传统知识图(KGs)的延伸，为每个KG事实提供额外的键值对(即限定词)，以更好地限制事实的有效性。近年来，研究在HKGs上进行图推理越来越受关注。与此同时，由于世界知识的不断演变，大量平行工作集中在对时间KGs(TKGs)进行推理，其中每个TKG事实可以被视为带有时间戳(或时间段)的KG事实，指定其时间有效性。现有的HKG推理方法不考虑时间信息，因为在之前的基准数据集中没有显式地指定。此外，所有以前的TKG推理方法只重视时间推理，并没有办法从限定词中学习。因此，我们的目标是填补TKG推理和HKG推理之间的差距。我们开发了两个新的基准超关系TKG(HTKG)数据集，即Wiki-hy和...

    Stemming from traditional knowledge graphs (KGs), hyper-relational KGs (HKGs) provide additional key-value pairs (i.e., qualifiers) for each KG fact that help to better restrict the fact validity. In recent years, there has been an increasing interest in studying graph reasoning over HKGs. In the meantime, due to the ever-evolving nature of world knowledge, extensive parallel works have been focusing on reasoning over temporal KGs (TKGs), where each TKG fact can be viewed as a KG fact coupled with a timestamp (or time period) specifying its time validity. The existing HKG reasoning approaches do not consider temporal information because it is not explicitly specified in previous benchmark datasets. Besides, all the previous TKG reasoning methods only lay emphasis on temporal reasoning and have no way to learn from qualifiers. To this end, we aim to fill the gap between TKG reasoning and HKG reasoning. We develop two new benchmark hyper-relational TKG (HTKG) datasets, i.e., Wiki-hy and 
    
[^112]: 该文标题已翻译为：行动时间：针对野外网络威胁情报的自动化分析

    Time for aCTIon: Automated Analysis of Cyber Threat Intelligence in the Wild. (arXiv:2307.10214v1 [cs.CR])

    [http://arxiv.org/abs/2307.10214](http://arxiv.org/abs/2307.10214)

    本文提供了一个新的大型开放基准数据集和一个名为aCTIon的结构化CTI信息提取工具来解决从非结构化文本源中提取相关信息的问题。这个数据集包括204个真实世界上可用的报告和相应的结构化CTI信息。通过与三个独立的CTI分析组合作，我们的团队策划了这个数据集，该数据集是之前公开发布的开源数据集的两个数量级大。

    

    针对组织机构，网络威胁情报(CTI)在评估风险和提升安全方面起着关键作用。然而，从非结构化文本源中提取相关信息的过程可能既昂贵又耗时。我们的经验表明，现有的自动化结构化CTI提取工具存在性能限制。此外，缺乏一个共同的基准来定量评估它们的性能。我们通过提供一个新的大型开放性基准数据集和名为aCTIon的结构化CTI信息提取工具来填补这些空白。该数据集包括204个真实世界上可用的报告和相应的STIX格式的结构化CTI信息。我们的团队在几个月的时间内，与三个独立的CTI分析组合作，进行了数据集的策划。据我们所知，该数据集的规模是之前公开发布的开源数据集的两个数量级。然后我们设计了aCTIon，利用最近的技术创新…

    Cyber Threat Intelligence (CTI) plays a crucial role in assessing risks and enhancing security for organizations. However, the process of extracting relevant information from unstructured text sources can be expensive and time-consuming. Our empirical experience shows that existing tools for automated structured CTI extraction have performance limitations. Furthermore, the community lacks a common benchmark to quantitatively assess their performance. We fill these gaps providing a new large open benchmark dataset and aCTIon, a structured CTI information extraction tool. The dataset includes 204 real-world publicly available reports and their corresponding structured CTI information in STIX format. Our team curated the dataset involving three independent groups of CTI analysts working over the course of several months. To the best of our knowledge, this dataset is two orders of magnitude larger than previously released open source datasets. We then design aCTIon, leveraging recently int
    
[^113]: 关于深度负载分解算法对敌对攻击的敏感性研究

    On the Sensitivity of Deep Load Disaggregation to Adversarial Attacks. (arXiv:2307.10209v1 [cs.CR])

    [http://arxiv.org/abs/2307.10209](http://arxiv.org/abs/2307.10209)

    本研究调查了深度负载分解算法对敌对攻击的敏感性，并发现常用的CNN-based NILM模型易受攻击，这对于负载分解算法的隐私和安全具有重要影响。

    

    非侵入式负载监测（NILM）算法，通常称为负载分解算法，是有效能量管理的基本工具。尽管深度模型在负载分解方面取得了成功，但它们面临着诸多挑战，特别是与隐私和安全有关的挑战。本文研究了著名的深度NILM基线对敌对攻击的敏感性，敌对攻击已被证明在计算机视觉和语音识别等领域具有重要威胁。敌对攻击涉及将不可感知的噪声引入输入数据，以误导神经网络生成错误的输出。我们研究了快速梯度符号方法（FGSM），这是一种著名的敌对攻击方法，来扰乱输入序列，这些序列被用于两个常用的基于CNN的NILM基线模型：序列到序列（S2S）和序列到点（S2P）模型。我们的研究结果提供了这些模型的易受攻击的有力证据。

    Non-intrusive Load Monitoring (NILM) algorithms, commonly referred to as load disaggregation algorithms, are fundamental tools for effective energy management. Despite the success of deep models in load disaggregation, they face various challenges, particularly those pertaining to privacy and security. This paper investigates the sensitivity of prominent deep NILM baselines to adversarial attacks, which have proven to be a significant threat in domains such as computer vision and speech recognition. Adversarial attacks entail the introduction of imperceptible noise into the input data with the aim of misleading the neural network into generating erroneous outputs. We investigate the Fast Gradient Sign Method (FGSM), a well-known adversarial attack, to perturb the input sequences fed into two commonly employed CNN-based NILM baselines: the Sequence-to-Sequence (S2S) and Sequence-to-Point (S2P) models. Our findings provide compelling evidence for the vulnerability of these models, partic
    
[^114]: 长尾分布上的敌对训练

    Adversarial Training Over Long-Tailed Distribution. (arXiv:2307.10205v1 [cs.LG])

    [http://arxiv.org/abs/2307.10205](http://arxiv.org/abs/2307.10205)

    本文研究了在长尾分布数据集上的敌对训练，提出了一种新的敌对训练框架--重新平衡敌对训练（REAT）。REAT能够解决敌对训练过程中产生的不平衡问题，有效增强模型的鲁棒性并保持准确性。

    

    本文研究了在服从长尾分布的数据集上的敌对训练，这在之前的工作中很少被探索。与平衡数据集上的传统敌对训练相比，该过程面临着产生不平衡的敌对样本和不平衡的特征嵌入空间的困境，导致生成的模型在尾部数据上表现出低鲁棒性和准确性。为了解决这个问题，我们提出了一种新的敌对训练框架--重新平衡敌对训练（REAT）。该框架包括两个组成部分：（1）一种受有效样本数启发的新训练策略，用于引导模型生成更平衡和信息丰富的敌对样本；（2）一种精心构建的惩罚函数，用于强制满足一个令人满意的特征空间。不同数据集和模型结构上的评估结果证明，REAT能够有效增强模型的鲁棒性，并保持模型的准确性。代码可以在https://中找到

    In this paper, we study adversarial training on datasets that obey the long-tailed distribution, which is practical but rarely explored in previous works. Compared with conventional adversarial training on balanced datasets, this process falls into the dilemma of generating uneven adversarial examples (AEs) and an unbalanced feature embedding space, causing the resulting model to exhibit low robustness and accuracy on tail data. To combat that, we propose a new adversarial training framework -- Re-balancing Adversarial Training (REAT). This framework consists of two components: (1) a new training strategy inspired by the term effective number to guide the model to generate more balanced and informative AEs; (2) a carefully constructed penalty function to force a satisfactory feature space. Evaluation results on different datasets and model structures prove that REAT can effectively enhance the model's robustness and preserve the model's clean accuracy. The code can be found in https://
    
[^115]: 基于IPW的双边市场中的无偏排序度量

    An IPW-based Unbiased Ranking Metric in Two-sided Markets. (arXiv:2307.10204v1 [cs.IR])

    [http://arxiv.org/abs/2307.10204](http://arxiv.org/abs/2307.10204)

    这项研究提出了一种基于IPW的无偏排序度量方法，针对双边市场中用户之间的偏见相互作用，解决了位置偏见和两个用户群体的位置偏差问题。

    

    在现代推荐系统中，无偏学习排序（LTR）对于优先考虑来自有偏的隐式用户反馈（如点击数据）的项目是至关重要的。已经提出了一些技术，例如倒数倾向性加权（IPW），用于单边市场。然而，在双边市场（如工作平台或约会服务）中，成功转化需要匹配两个用户的偏好，但对于这种情况关注较少。本文解决了双边市场中用户之间的偏见相互作用，并提出了一种特定的LTR方法。我们首先提出了双边匹配平台中反馈机制的形式化，并指出它们的隐式反馈可能包含来自两个用户群体的位置偏差。基于这一观察，我们扩展了IPW估计器并提出了一种新的估计器，称为双边IPW，以解决双边市场中的位置偏见。我们证明了该估计器满足真实排名的无偏性。

    In modern recommendation systems, unbiased learning-to-rank (LTR) is crucial for prioritizing items from biased implicit user feedback, such as click data. Several techniques, such as Inverse Propensity Weighting (IPW), have been proposed for single-sided markets. However, less attention has been paid to two-sided markets, such as job platforms or dating services, where successful conversions require matching preferences from both users. This paper addresses the complex interaction of biases between users in two-sided markets and proposes a tailored LTR approach. We first present a formulation of feedback mechanisms in two-sided matching platforms and point out that their implicit feedback may include position bias from both user groups. On the basis of this observation, we extend the IPW estimator and propose a new estimator, named two-sided IPW, to address the position bases in two-sided markets. We prove that the proposed estimator satisfies the unbiasedness for the ground-truth ran
    
[^116]: 从模型偏见中分离社会不平等：离婚法庭诉讼中的性别不平等

    Disentangling Societal Inequality from Model Biases: Gender Inequality in Divorce Court Proceedings. (arXiv:2307.10200v1 [cs.CY])

    [http://arxiv.org/abs/2307.10200](http://arxiv.org/abs/2307.10200)

    本文通过研究离婚法庭诉讼，探索了性别不平等问题，并发现了自然语言处理方法中存在的偏见问题。需要对现有资源进行修正来量化社会不平等。

    

    离婚是法院法律解除婚姻关系的过程。由于这通常是婚姻联合的不愉快结果，每一方都可能有理由要求退出决定，这通常在法庭诉讼中有详细记录。通过一份包含17,306份法庭诉讼的大量语料库，本文通过离婚法庭诉讼的角度研究了性别不平等问题。虽然新兴的数据来源（例如公共法庭记录）在辅助社会科学研究方面具有潜力，但先进的自然语言处理（NLP）方法中存在的偏见可能会干扰或影响此类研究。因此，我们需要对现有NLP资源中的潜在差距和限制进行彻底分析。在方法论上，本文证明了现有NLP资源需要进行几个非平凡的修改，以量化社会不平等。在实质上，我们发现尽管大量的法庭案件可能暗示着变化

    Divorce is the legal dissolution of a marriage by a court. Since this is usually an unpleasant outcome of a marital union, each party may have reasons to call the decision to quit which is generally documented in detail in the court proceedings. Via a substantial corpus of 17,306 court proceedings, this paper investigates gender inequality through the lens of divorce court proceedings. While emerging data sources (e.g., public court records) on sensitive societal issues hold promise in aiding social science research, biases present in cutting-edge natural language processing (NLP) methods may interfere with or affect such studies. We thus require a thorough analysis of potential gaps and limitations present in extant NLP resources. In this paper, on the methodological side, we demonstrate that existing NLP resources required several non-trivial modifications to quantify societal inequalities. On the substantive side, we find that while a large number of court cases perhaps suggest chan
    
[^117]: 基于StyleGAN2的医学成像领域的异常检测

    StyleGAN2-based Out-of-Distribution Detection for Medical Imaging. (arXiv:2307.10193v1 [eess.IV])

    [http://arxiv.org/abs/2307.10193](http://arxiv.org/abs/2307.10193)

    本文利用StyleGAN2的生成对抗网络（GAN）进行医学图像的异常检测，通过反向传播重构图像并利用评估指标进行异常检测，实现了超过90%的准确率。

    

    深度学习模型在临床上的应用面临一个障碍，即运行时存在着训练分布之外的图像。本文旨在利用生成对抗网络（GAN）来检测这些训练分布之外的图像。我们的训练数据集包含来自456名患者的3234个含有肝脏的计算机断层扫描（CT）图像。我们的训练分布采用了StyleGAN2-ADA架构进行建模。使用反向传播重构图像，并利用Wasserstein距离、均方差和结构相似性指数来评估重构结果。异常检测效果通过接收者操作特征曲线下面积（AUROC）进行评估。我们的方法在肝脏和非肝脏CT之间实现了超过90%的AUROC，同时无法完全重构肝脏的异常部分，例如...（文章截断）

    One barrier to the clinical deployment of deep learning-based models is the presence of images at runtime that lie far outside the training distribution of a given model. We aim to detect these out-of-distribution (OOD) images with a generative adversarial network (GAN). Our training dataset was comprised of 3,234 liver-containing computed tomography (CT) scans from 456 patients. Our OOD test data consisted of CT images of the brain, head and neck, lung, cervix, and abnormal livers. A StyleGAN2-ADA architecture was employed to model the training distribution. Images were reconstructed using backpropagation. Reconstructions were evaluated using the Wasserstein distance, mean squared error, and the structural similarity index measure. OOD detection was evaluated with the area under the receiver operating characteristic curve (AUROC). Our paradigm distinguished between liver and non-liver CT with greater than 90% AUROC. It was also completely unable to reconstruct liver artifacts, such as
    
[^118]: 大型语言模型（LLMs）的几个类别：简短调查

    Several categories of Large Language Models (LLMs): A Short Survey. (arXiv:2307.10188v1 [cs.CL])

    [http://arxiv.org/abs/2307.10188](http://arxiv.org/abs/2307.10188)

    本文是对大型语言模型（LLMs）的几个类别进行的简短调查，提供了各类LLM的最新发展和努力的概述，包括多语言LLMs、视觉语言LLMs和代码语言模型等。同时，还突出了在聊天机器人和虚拟助手领域存在的问题，如提升自然语言处理能力和解决道德和法律困境。

    

    大型语言模型（LLMs）已成为自然语言处理的有效工具，并在许多不同领域中得到应用。本文对各种LLM子类进行了简洁的总结。该调查强调了各种LLM类型的最新发展和努力，包括基于任务的金融LLM，多语言LLM，生物医学和临床LLM，视觉语言LLM和代码语言模型。该调查对每个LLM类别中应用的方法、属性、数据集、变压器模型和比较指标进行了概述。此外，它还突出了在开发聊天机器人和虚拟助手领域存在的未解决问题，如提升自然语言处理能力，增强聊天机器人智能性以及解决道德和法律困境。本研究旨在为对基于LLM的聊天机器人和虚拟智能助手技术感兴趣的读者、开发人员、学术界人士和用户提供有用的信息和未来的方向。

    Large Language Models(LLMs)have become effective tools for natural language processing and have been used in many different fields. This essay offers a succinct summary of various LLM subcategories. The survey emphasizes recent developments and efforts made for various LLM kinds, including task-based financial LLMs, multilingual language LLMs, biomedical and clinical LLMs, vision language LLMs, and code language models. The survey gives a general summary of the methods, attributes, datasets, transformer models, and comparison metrics applied in each category of LLMs. Furthermore, it highlights unresolved problems in the field of developing chatbots and virtual assistants, such as boosting natural language processing, enhancing chatbot intelligence, and resolving moral and legal dilemmas. The purpose of this study is to provide readers, developers, academics, and users interested in LLM-based chatbots and virtual intelligent assistant technologies with useful information and future dire
    
[^119]: 隐私放大通过重要性采样

    Privacy Amplification via Importance Sampling. (arXiv:2307.10187v1 [cs.CR])

    [http://arxiv.org/abs/2307.10187](http://arxiv.org/abs/2307.10187)

    通过重要性采样进行隐私放大，可以同时增强隐私保护和提高效用。我们提供了一个一般的结果来量化选择概率权重对隐私放大的影响，并展示了异质采样概率可以在保持子采样大小不变的情况下获得更好的隐私和效用。

    

    我们研究了通过重要性采样对数据集进行子采样作为差分隐私机制的预处理步骤来增强隐私保护的性质。这扩展了已有的通过子采样进行隐私放大的结果到重要性采样，其中每个数据点的权重为其被选择概率的倒数。每个点的选择概率的权重对隐私的影响并不明显。一方面，较低的选择概率会导致更强的隐私放大。另一方面，权重越高，在点被选择时，点对机制输出的影响就越强。我们提供了一个一般的结果来量化这两个影响之间的权衡。我们展示了异质采样概率可以同时比均匀子采样具有更强的隐私和更好的效用，并保持子采样大小不变。特别地，我们制定和解决了隐私优化采样的问题，即寻找...

    We examine the privacy-enhancing properties of subsampling a data set via importance sampling as a pre-processing step for differentially private mechanisms. This extends the established privacy amplification by subsampling result to importance sampling where each data point is weighted by the reciprocal of its selection probability. The implications for privacy of weighting each point are not obvious. On the one hand, a lower selection probability leads to a stronger privacy amplification. On the other hand, the higher the weight, the stronger the influence of the point on the output of the mechanism in the event that the point does get selected. We provide a general result that quantifies the trade-off between these two effects. We show that heterogeneous sampling probabilities can lead to both stronger privacy and better utility than uniform subsampling while retaining the subsample size. In particular, we formulate and solve the problem of privacy-optimal sampling, that is, finding
    
[^120]: 多尺度U形MLP用于高光谱图像分类

    Multi-Scale U-Shape MLP for Hyperspectral Image Classification. (arXiv:2307.10186v1 [eess.IV])

    [http://arxiv.org/abs/2307.10186](http://arxiv.org/abs/2307.10186)

    提出了一种多尺度U形多层感知器(MUMLP)模型，利用设计的MSC(Multi-Scale Channel)块和UMLP(U形多层感知器)结构，有效地表示了高光谱图像的语义和空间信息，并且能够压缩大规模参数。

    

    高光谱图像在各个领域具有重要的应用，因为它们在光谱波段中注册了大量的语义和空间信息，并且具有光谱特征的空间可变性。在识别高光谱图像的像素中存在两个关键挑战，分别是表示局部和全局之间相关信息以及模型丰富参数。为了解决这个挑战，我们提出了一种多尺度U形多层感知器(MUMLP)模型，它由设计的MSC(Multi-Scale Channel)块和UMLP(U形多层感知器)结构组成。MSC将通道维度转换和混合光谱波段特征以嵌入适当的深层表示。UMLP由编码器-解码器结构和多层感知器层设计而成，能够压缩大规模参数。我们进行了大量实验来证明我们的模型在性能上优于现有方法。

    Hyperspectral images have significant applications in various domains, since they register numerous semantic and spatial information in the spectral band with spatial variability of spectral signatures. Two critical challenges in identifying pixels of the hyperspectral image are respectively representing the correlated information among the local and global, as well as the abundant parameters of the model. To tackle this challenge, we propose a Multi-Scale U-shape Multi-Layer Perceptron (MUMLP) a model consisting of the designed MSC (Multi-Scale Channel) block and the UMLP (U-shape Multi-Layer Perceptron) structure. MSC transforms the channel dimension and mixes spectral band feature to embed the deep-level representation adequately. UMLP is designed by the encoder-decoder structure with multi-layer perceptron layers, which is capable of compressing the large-scale parameters. Extensive experiments are conducted to demonstrate our model can outperform state-of-the-art methods across-th
    
[^121]: 一种双重隐秘后门攻击：从空间和频率角度来看

    A Dual Stealthy Backdoor: From Both Spatial and Frequency Perspectives. (arXiv:2307.10184v1 [cs.CR])

    [http://arxiv.org/abs/2307.10184](http://arxiv.org/abs/2307.10184)

    该论文提出了一种名为DUBA的双重隐秘后门攻击方法，该方法同时考虑了触发器在空间和频率域中的隐匿性，以实现良好的攻击性能和强大的隐匿性。

    

    后门攻击对深度神经网络(DNNs)构成严重的安全威胁。后门模型在带有精心设计的触发器的输入上会任意（有针对性地）出现错误预测，而在干净的输入上表现正常。许多研究探索了后门触发器的隐匿性以提高攻击的隐秘性。然而，其中大部分只考虑了空间域中的隐匿性，没有明确考虑在频率域中生成隐匿触发器，使生成的毒害图像容易被最近的防御方法检测到。为了解决这个问题，本文提出了一种名为DUBA的DUal隐秘后门攻击方法，该方法同时考虑了触发器在空间和频率域中的隐匿性，以实现良好的攻击性能，同时确保强大的隐匿性。具体地，我们首先使用离散小波变换将触发器图像的高频信息嵌入干净图像中。

    Backdoor attacks pose serious security threats to deep neural networks (DNNs). Backdoored models make arbitrarily (targeted) incorrect predictions on inputs embedded with well-designed triggers while behaving normally on clean inputs. Many works have explored the invisibility of backdoor triggers to improve attack stealthiness. However, most of them only consider the invisibility in the spatial domain without explicitly accounting for the generation of invisible triggers in the frequency domain, making the generated poisoned images be easily detected by recent defense methods. To address this issue, in this paper, we propose a DUal stealthy BAckdoor attack method named DUBA, which simultaneously considers the invisibility of triggers in both the spatial and frequency domains, to achieve desirable attack performance, while ensuring strong stealthiness. Specifically, we first use Discrete Wavelet Transform to embed the high-frequency information of the trigger image into the clean image 
    
[^122]: 基于社区感知的Transformer用于自闭症fMRI连接图的预测

    Community-Aware Transformer for Autism Prediction in fMRI Connectome. (arXiv:2307.10181v1 [q-bio.NC])

    [http://arxiv.org/abs/2307.10181](http://arxiv.org/abs/2307.10181)

    本研究提出了一种基于社区感知的Transformer方法，用于自闭症fMRI连接图的预测。该方法通过学习社区感知节点嵌入来提高ASD的诊断准确性和效果。

    

    自闭症谱系障碍（ASD）是一种终生性的神经发育障碍，影响社交交流和行为。通过研究功能磁共振成像（fMRI）基于大脑功能连接图可以帮助我们理解和诊断ASD，从而实现更有效的治疗。我们将大脑建模为一组大脑感兴趣区域（ROIs），ROIs形成社区，了解这些社区对ASD的诊断至关重要。一方面，基于Transformer的模型已被证明在多个任务中非常有效，包括fMRI连接图分析，以学习有用的ROIs表示。另一方面，现有的基于Transformer的模型平等地处理所有ROIs，并忽视了学习节点嵌入时社区特定关联的影响。为了填补这个空白，我们提出了一种新的方法，Com-BrainTF，这是一种层次化的局部-全局Transformer架构，用于学习自闭症预测任务的社区感知节点嵌入。

    Autism spectrum disorder(ASD) is a lifelong neurodevelopmental condition that affects social communication and behavior. Investigating functional magnetic resonance imaging (fMRI)-based brain functional connectome can aid in the understanding and diagnosis of ASD, leading to more effective treatments. The brain is modeled as a network of brain Regions of Interest (ROIs), and ROIs form communities and knowledge of these communities is crucial for ASD diagnosis. On the one hand, Transformer-based models have proven to be highly effective across several tasks, including fMRI connectome analysis to learn useful representations of ROIs. On the other hand, existing transformer-based models treat all ROIs equally and overlook the impact of community-specific associations when learning node embeddings. To fill this gap, we propose a novel method, Com-BrainTF, a hierarchical local-global transformer architecture that learns intra and inter-community aware node embeddings for ASD prediction task
    
[^123]: 基于非局部先验的贝叶斯脉冲序列推断

    Bayesian Spike Train Inference via Non-Local Priors. (arXiv:2307.10177v1 [q-bio.NC])

    [http://arxiv.org/abs/2307.10177](http://arxiv.org/abs/2307.10177)

    使用基于混合半非局部先验密度和点质量的贝叶斯方法，提出一种基于随机搜索的方法，用于从荧光信号中确定神经元的脉冲序列，该方法能够高效地探索所有可能的脉冲排列并报告最高后验概率的脉冲排列和每个脉冲位置的后验概率。

    

    神经科学的进展使研究人员能够同时测量行为动物中大量神经元的活动。我们可以获得每个神经元的荧光信号，这提供了神经活动随时间的一阶近似。根据这个荧光信号确定神经元的确切脉冲是计算神经科学领域的一个活跃研究领域。我们提出了一种基于混合半非局部先验密度和点质量的新颖贝叶斯方法来解决这个任务。我们采用了一种基于随机搜索的方法，而不是计算复杂的MCMC算法。这种方法能够利用现代计算环境（通常配备有多个处理器）的优势，探索观察到的脉冲序列中所有可能的脉冲排列和缺失情况。然后报告脉冲排列的最高后验概率和每个脉冲位置的后验概率。

    Advances in neuroscience have enabled researchers to measure the activities of large numbers of neurons simultaneously in behaving animals. We have access to the fluorescence of each of the neurons which provides a first-order approximation of the neural activity over time. Determining the exact spike of a neuron from this fluorescence trace constitutes an active area of research within the field of computational neuroscience. We propose a novel Bayesian approach based on a mixture of half-non-local prior densities and point masses for this task. Instead of a computationally expensive MCMC algorithm, we adopt a stochastic search-based approach that is capable of taking advantage of modern computing environments often equipped with multiple processors, to explore all possible arrangements of spikes and lack thereof in an observed spike train. It then reports the highest posterior probability arrangement of spikes and posterior probability for a spike at each location of the spike train.
    
[^124]: 这里是翻译过的论文标题: 过去曾翻译《Impatient Bandits: Optimizing for the Long-Term Without Delay》

    Impatient Bandits: Optimizing for the Long-Term Without Delay. (arXiv:2307.09943v1 [cs.LG])

    [http://arxiv.org/abs/2307.09943](http://arxiv.org/abs/2307.09943)

    这里是中文总结出的一句话要点：这篇论文研究了在推荐系统中提高用户长期满意度的问题，通过开发一个预测延迟奖励的模型和设计一个利用该模型的赌博算法来解决了通过测量短期代理奖励反映实际长期目标不完美的挑战。

    

    这里是翻译过的论文摘要：推荐系统在在线平台上是一个普遍存在的功能。越来越多的情况下，它们明确地被任务为提高用户的长期满意度。在这个背景下，我们研究了一个内容探索任务，将其形式化为一个具有延迟奖励的多臂赌博问题。我们观察到，在选择学习信号时存在明显的权衡：等待完全的奖励可能需要几周时间，这会影响学习发生的速度，而测量短期代理奖励则不完美地反映了实际的长期目标。我们通过两个步骤来解决这个挑战。首先，我们开发了一个预测延迟奖励的模型，该模型可以整合迄今所获得的所有信息。通过贝叶斯滤波器组合完整的观察结果以及部分（短期或中期）的结果，从而得到概率信念。其次，我们设计了一个利用这个新的预测模型的赌博算法。该算法可以快速学习识别内容。

    Recommender systems are a ubiquitous feature of online platforms. Increasingly, they are explicitly tasked with increasing users' long-term satisfaction. In this context, we study a content exploration task, which we formalize as a multi-armed bandit problem with delayed rewards. We observe that there is an apparent trade-off in choosing the learning signal: Waiting for the full reward to become available might take several weeks, hurting the rate at which learning happens, whereas measuring short-term proxy rewards reflects the actual long-term goal only imperfectly. We address this challenge in two steps. First, we develop a predictive model of delayed rewards that incorporates all information obtained to date. Full observations as well as partial (short or medium-term) outcomes are combined through a Bayesian filter to obtain a probabilistic belief. Second, we devise a bandit algorithm that takes advantage of this new predictive model. The algorithm quickly learns to identify conten
    
[^125]: 高效的LLM引导生成

    Efficient Guided Generation for LLMs. (arXiv:2307.09702v1 [cs.CL])

    [http://arxiv.org/abs/2307.09702](http://arxiv.org/abs/2307.09702)

    本文描述了一种使用正则表达式和上下文无关文法来引导语言模型文本生成的高效方法。

    

    在本文中，我们描述了一种使用正则表达式和上下文无关文法来引导语言模型文本生成的高效方法。我们的方法在标记序列生成过程中几乎不增加任何开销，并使得引导生成在实际中可行。在开源Python库Outlines中提供了一个实现。

    In this article we describe an efficient approach to guiding language model text generation with regular expressions and context-free grammars. Our approach adds little to no overhead to the token sequence generation process, and makes guided generation feasible in practice. An implementation is provided in the open source Python library Outlines.
    
[^126]: 多视角自监督学习用于多变量通道时间序列

    Multi-view self-supervised learning for multivariate variable-channel time series. (arXiv:2307.09614v1 [stat.ML])

    [http://arxiv.org/abs/2307.09614](http://arxiv.org/abs/2307.09614)

    本论文提出了一种多视角自监督学习方法，用于处理多变量通道时间序列数据，在不同数据集之间进行迁移学习。通过预训练和微调，结合传递神经网络和TS2Vec损失，该方法在大多数设置下表现优于其他方法。

    

    对多变量生物医学时间序列数据进行标注是一项繁重和昂贵的任务。自监督对比学习通过对未标记数据进行预训练来减少对大型标记数据集的需求。然而，对于多变量时间序列数据，输入通道的集合在不同应用之间通常会有所变化，而大多数现有工作并不允许在具有不同输入通道集合的数据集之间进行迁移学习。我们提出了一种学习一种编码器来分别处理所有输入通道的方法。然后，我们使用传递神经网络在通道之间提取单一表示。我们通过在一个具有六个脑电图通道的数据集上进行预训练，并在一个具有两个不同脑电图通道的数据集上进行微调来展示这种方法的潜力。我们比较了具有传递神经网络和不具有传递神经网络的网络在不同对比损失函数下的性能。我们发现我们的方法结合了TS2Vec损失在大多数设置中的表现优于其他所有方法。

    Labeling of multivariate biomedical time series data is a laborious and expensive process. Self-supervised contrastive learning alleviates the need for large, labeled datasets through pretraining on unlabeled data. However, for multivariate time series data the set of input channels often varies between applications, and most existing work does not allow for transfer between datasets with different sets of input channels. We propose learning one encoder to operate on all input channels individually. We then use a message passing neural network to extract a single representation across channels. We demonstrate the potential of this method by pretraining our network on a dataset with six EEG channels and finetuning on a dataset with two different EEG channels. We compare networks with and without the message passing neural network across different contrastive loss functions. We show that our method combined with the TS2Vec loss outperforms all other methods in most settings.
    
[^127]: 带有基于学习的地形和机器人感知动力模型的上下文条件导航

    Context-Conditional Navigation with a Learning-Based Terrain- and Robot-Aware Dynamics Model. (arXiv:2307.09206v1 [cs.RO])

    [http://arxiv.org/abs/2307.09206](http://arxiv.org/abs/2307.09206)

    本文提出了一种名为TRADYN的概率地形和机器人感知前向动力学模型，能够适应在自主导航环境中的地形和机器人的变化，通过在模拟的二维导航环境中的实验证明，该模型在长视程轨迹预测任务中表现出较低的预测误差。

    

    在自主导航环境中，多个参数可能会发生变化。地形特性如摩擦系数可能会根据机器人的位置而随时间变化。此外，机器人的动力学可能会因不同负载、系统质量变化、磨损等原因而发生变化，从而改变执行器增益或关节摩擦力。自主代理应该能够适应这些变化。在本文中，我们开发了一种新颖的概率地形和机器人感知前向动力学模型，称为TRADYN，它能够适应上述变化。它基于基于神经过程的元学习前向动力学模型的最新进展。我们在模拟的二维导航环境中评估了我们的方法，使用了一个类似自行车的机器人和具有空间变化摩擦系数的不同地形布局。在我们的实验中，与非自适应方法相比，所提出的模型在长视程轨迹预测任务中表现出较低的预测误差。

    In autonomous navigation settings, several quantities can be subject to variations. Terrain properties such as friction coefficients may vary over time depending on the location of the robot. Also, the dynamics of the robot may change due to, e.g., different payloads, changing the system's mass, or wear and tear, changing actuator gains or joint friction. An autonomous agent should thus be able to adapt to such variations. In this paper, we develop a novel probabilistic, terrain- and robot-aware forward dynamics model, termed TRADYN, which is able to adapt to the above-mentioned variations. It builds on recent advances in meta-learning forward dynamics models based on Neural Processes. We evaluate our method in a simulated 2D navigation setting with a unicycle-like robot and different terrain layouts with spatially varying friction coefficients. In our experiments, the proposed model exhibits lower prediction error for the task of long-horizon trajectory prediction, compared to non-ada
    
[^128]: 基于个体专属数据的多模态健康大型语言模型

    Multimodal LLMs for health grounded in individual-specific data. (arXiv:2307.09018v1 [q-bio.QM])

    [http://arxiv.org/abs/2307.09018](http://arxiv.org/abs/2307.09018)

    本文提出了一个多模态健康大型语言模型（HeLM），通过学习复杂数据模态的编码器，同时支持简单模态数据的文本序列化，HeLM可以有效地使用个体专属数据估计疾病风险。

    

    基于个体专属数据的健康大型语言模型（LLMs）可以有效地解决个性化健康问题，但为了实现这一目标，LLMs需要具备摄入与个体健康状况相关的多样化数据模态能力。本文提出了一个框架（HeLM：健康大型语言模型多模态理解），该框架通过开发一个编码器，将复杂的数据模态映射到LLMs的令牌嵌入空间，并将简单的模态如表格数据序列化为文本，实现了基于个体专属数据的多模态LLMs。通过使用来自英国生物库的数据，我们证明了HeLM能够有效地利用人口统计学和临床特征，以及高维时间序列数据来估计疾病风险。

    Foundation large language models (LLMs) have shown an impressive ability to solve tasks across a wide range of fields including health. To effectively solve personalized health tasks, LLMs need the ability to ingest a diversity of data modalities that are relevant to an individual's health status. In this paper, we take a step towards creating multimodal LLMs for health that are grounded in individual-specific data by developing a framework (HeLM: Health Large Language Model for Multimodal Understanding) that enables LLMs to use high-dimensional clinical modalities to estimate underlying disease risk. HeLM encodes complex data modalities by learning an encoder that maps them into the LLM's token embedding space and for simple modalities like tabular data by serializing the data into text. Using data from the UK Biobank, we show that HeLM can effectively use demographic and clinical features in addition to high-dimensional time-series data to estimate disease risk. For example, HeLM ach
    
[^129]: 通过生成扩散模型实现的合成拉格朗日湍流

    Synthetic Lagrangian Turbulence by Generative Diffusion Models. (arXiv:2307.08529v1 [physics.flu-dyn] CROSS LISTED)

    [http://arxiv.org/abs/2307.08529](http://arxiv.org/abs/2307.08529)

    该论文提出了一种基于机器学习的方法，通过生成扩散模型在高雷诺数下合成拉格朗日湍流，成功实现了粒子轨迹的统计和拓扑性质的准确重现。

    

    拉格朗日湍流是涉及工程、生物流体、大气、海洋和天体物理领域中的分散和混合物理的应用和基础性问题。尽管过去三十年进行了卓越的理论、数值和实验研究，但没有现有模型能够忠实地重现湍流中的粒子轨迹所展示的统计和拓扑性质。我们提出了一种基于最先进的扩散模型的机器学习方法，以在高雷诺数下生成三维湍流中的单粒子轨迹，从而避免了获取可靠的拉格朗日数据所需的直接数值模拟或实验。我们的模型表明，它能够定量地重现整个时间尺度范围内的所有相关统计基准，包括速度增量的尾部分布、异常幂律和增强

    Lagrangian turbulence lies at the core of numerous applied and fundamental problems related to the physics of dispersion and mixing in engineering, bio-fluids, atmosphere, oceans, and astrophysics. Despite exceptional theoretical, numerical, and experimental efforts conducted over the past thirty years, no existing models are capable of faithfully reproducing statistical and topological properties exhibited by particle trajectories in turbulence. We propose a machine learning approach, based on a state-of-the-art Diffusion Model, to generate single-particle trajectories in three-dimensional turbulence at high Reynolds numbers, thereby bypassing the need for direct numerical simulations or experiments to obtain reliable Lagrangian data. Our model demonstrates the ability to quantitatively reproduce all relevant statistical benchmarks over the entire range of time scales, including the presence of fat tails distribution for the velocity increments, anomalous power law, and enhancement of
    
[^130]: 切线变换器用于组合、隐私和去除

    Tangent Transformers for Composition, Privacy and Removal. (arXiv:2307.08122v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.08122](http://arxiv.org/abs/2307.08122)

    我们引入了一种切线注意微调方法（TAFT），通过线性化变压器的一阶泰勒展开来进行微调。该方法具有与原始非线性网络相当的性能，并在模型组合、并行训练、机器去除和差分隐私方面具有优势。

    

    我们引入了切线关注微调（TAFT）方法，该方法通过在预训练初始化点周围计算一阶泰勒展开来获得线性化变压器进行微调。我们展示了从线性化得到的雅可比矩阵-向量积可以在单个前向传递中高效计算，将训练和推断成本降低到与原始非线性模型相同数量级，并且使用相同数量的参数。此外，我们还展示了当应用于各种下游视觉分类任务时，通过TAFT进行微调的结果切线变换器可以与对原始非线性网络进行微调相当。由于切线变换器对于新的权值是线性的，并且结果微调损失是凸的，我们展示了相比于非线性微调，TAFT在模型组合、并行训练、机器去除和差分隐私方面具有几个优势。

    We introduce Tangent Attention Fine-Tuning (TAFT), a method for fine-tuning linearized transformers obtained by computing a First-order Taylor Expansion around a pre-trained initialization. We show that the Jacobian-Vector Product resulting from linearization can be computed efficiently in a single forward pass, reducing training and inference cost to the same order of magnitude as its original non-linear counterpart, while using the same number of parameters. Furthermore, we show that, when applied to various downstream visual classification tasks, the resulting Tangent Transformer fine-tuned with TAFT can perform comparably with fine-tuning the original non-linear network. Since Tangent Transformers are linear with respect to the new set of weights, and the resulting fine-tuning loss is convex, we show that TAFT enjoys several advantages compared to non-linear fine-tuning when it comes to model composition, parallel training, machine unlearning, and differential privacy.
    
[^131]: 具有概率策略执行不确定性的高效鲁棒增强学习

    Efficient Action Robust Reinforcement Learning with Probabilistic Policy Execution Uncertainty. (arXiv:2307.07666v1 [cs.LG])

    [http://arxiv.org/abs/2307.07666](http://arxiv.org/abs/2307.07666)

    本文研究了具有概率策略执行不确定性的行动鲁棒增强学习问题，并提出了ARRLC算法，该算法在遗憾和样本复杂度上达到了极小极大最优，实验证明其优于非鲁棒算法并且收敛更快。

    

    鲁棒增强学习旨在在不确定性面前找到优化最坏情况下性能的策略。本文关注具有概率策略执行不确定性的行动鲁棒增强学习，其中代理机器不总是按照策略指定的动作进行，而是以概率$1-\rho$执行策略指定的动作，以概率$\rho$执行替代的对抗动作。我们证明了具有概率策略执行不确定性的行动鲁棒马尔可夫决策过程存在最优策略，并提供了解决其的行动鲁棒贝尔曼最优方程。此外，我们开发了具有证书的行动鲁棒增强学习(ARRLC)算法，该算法实现了极小极大遗憾和样本复杂度最优。此外，我们进行了数值实验来验证我们的方法的鲁棒性，结果表明ARRLC优于非鲁棒增强学习算法，并且比鲁棒TD算法收敛更快。

    Robust reinforcement learning (RL) aims to find a policy that optimizes the worst-case performance in the face of uncertainties. In this paper, we focus on action robust RL with the probabilistic policy execution uncertainty, in which, instead of always carrying out the action specified by the policy, the agent will take the action specified by the policy with probability $1-\rho$ and an alternative adversarial action with probability $\rho$. We establish the existence of an optimal policy on the action robust MDPs with probabilistic policy execution uncertainty and provide the action robust Bellman optimality equation for its solution. Furthermore, we develop Action Robust Reinforcement Learning with Certificates (ARRLC) algorithm that achieves minimax optimal regret and sample complexity. Furthermore, we conduct numerical experiments to validate our approach's robustness, demonstrating that ARRLC outperforms non-robust RL algorithms and converges faster than the robust TD algorithm i
    
[^132]: 频域对抗训练用于稳健的体积医学分割

    Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation. (arXiv:2307.07269v1 [eess.IV])

    [http://arxiv.org/abs/2307.07269](http://arxiv.org/abs/2307.07269)

    这个论文提出了一种用于稳健的体积医学图像分割模型的频域对抗训练方法，并发现其相对于传统攻击方法具有优势。该方法利用频域对抗攻击，通过引入频率一致性损失来优化模型的性能，实现对体素和频域攻击的防御。

    

    在关键应用（如医疗保健）中，确保深度学习模型的稳健性至关重要。尽管最近在体积医学图像分割模型方面取得了进展，但由于其易受对抗攻击的弱点，这些模型无法立即用于实际应用。我们提出了一种用于体积医学图像分割模型的三维频域对抗攻击，并展示了其相对于传统输入或体素域攻击的优势。利用我们提出的攻击，我们引入了一种新颖的频域对抗训练方法，以优化针对体素和频域攻击的稳健模型。此外，我们提出了频率一致性损失来调节我们的频域对抗训练，以在清洁样品和对抗样本之间取得更好的平衡。代码可公开访问https://github.com/asif-hanif/vafa。

    It is imperative to ensure the robustness of deep learning models in critical applications such as, healthcare. While recent advances in deep learning have improved the performance of volumetric medical image segmentation models, these models cannot be deployed for real-world applications immediately due to their vulnerability to adversarial attacks. We present a 3D frequency domain adversarial attack for volumetric medical image segmentation models and demonstrate its advantages over conventional input or voxel domain attacks. Using our proposed attack, we introduce a novel frequency domain adversarial training approach for optimizing a robust model against voxel and frequency domain attacks. Moreover, we propose frequency consistency loss to regulate our frequency domain adversarial training that achieves a better tradeoff between model's performance on clean and adversarial samples. Code is publicly available at https://github.com/asif-hanif/vafa.
    
[^133]: Wasserstein量子蒙特卡洛：解决量子多体Schr\"odinger方程的一种新方法

    Wasserstein Quantum Monte Carlo: A Novel Approach for Solving the Quantum Many-Body Schr\"odinger Equation. (arXiv:2307.07050v1 [physics.comp-ph])

    [http://arxiv.org/abs/2307.07050](http://arxiv.org/abs/2307.07050)

    这篇论文提出了一种新的方法，即基于Wasserstein量子蒙特卡洛的方法，用于解决量子多体Schr\"odinger方程。该方法重新制定了能量泛函的最小化问题，并将其转化为Born分布空间中的粒子排列（反）对称波函数的问题，同时利用深度学习方法来表示丰富的波函数族。

    

    解决量子多体Schr\"odinger方程是量子物理、量子化学和材料科学领域中一个基本而具有挑战性的问题。针对这个问题，一种常见的计算方法是量子变分蒙特卡洛（QVMC），其中通过在一个参数化波函数族中最小化系统的能量来获得基态解。深度学习方法在一定程度上解决了传统QVMC的局限性，通过使用神经网络表示丰富的波函数族。然而，在QVMC中优化目标仍然难以最小化，需要使用自然梯度等二阶优化方法。本文首先重新制定了能量泛函的最小化问题，将其转化为Born分布空间中的粒子排列（反）对称波函数的问题，而不是波函数的空间。然后我们将QVMC解释为在这个空间中的Fisher-Rao梯度流。

    Solving the quantum many-body Schr\"odinger equation is a fundamental and challenging problem in the fields of quantum physics, quantum chemistry, and material sciences. One of the common computational approaches to this problem is Quantum Variational Monte Carlo (QVMC), in which ground-state solutions are obtained by minimizing the energy of the system within a restricted family of parameterized wave functions. Deep learning methods partially address the limitations of traditional QVMC by representing a rich family of wave functions in terms of neural networks. However, the optimization objective in QVMC remains notoriously hard to minimize and requires second-order optimization methods such as natural gradient. In this paper, we first reformulate energy functional minimization in the space of Born distributions corresponding to particle-permutation (anti-)symmetric wave functions, rather than the space of wave functions. We then interpret QVMC as the Fisher--Rao gradient flow in this
    
[^134]: 深度神经网络中的定量中心极限定理

    Quantitative CLTs in Deep Neural Networks. (arXiv:2307.06092v1 [cs.LG])

    [http://arxiv.org/abs/2307.06092](http://arxiv.org/abs/2307.06092)

    本文研究了具有随机高斯权重和偏置的全连接神经网络的分布，得到了在大但有限的 $n$ 和任意固定网络深度下成立的正态逼近的定量界限，证明了随机全连接网络与相应的无限宽高斯过程之间的距离按照 $n^{-\gamma}$ 缩放，界限在网络宽度的依赖性方面优于以前的研究。

    

    我们研究了具有随机高斯权重和偏置的全连接神经网络的分布，其中隐藏层宽度与大常数 $n$ 成比例。在非线性的温和假设下，我们得到了在大但有限的 $n$ 和任意固定网络深度下成立的正态逼近的定量界限。我们的定理表明，无论是对于有限维分布还是整个过程，随机全连接网络（及其导数）与相应的无限宽高斯过程之间的距离都会按照 $n^{-\gamma}$ 缩放，其中 $\gamma>0$，指数取决于用于度量差异的度量方式。我们的界限在网络宽度的依赖性方面比文献中以前提供的任何界限都要强。

    We study the distribution of a fully connected neural network with random Gaussian weights and biases in which the hidden layer widths are proportional to a large constant $n$. Under mild assumptions on the non-linearity, we obtain quantitative bounds on normal approximations valid at large but finite $n$ and any fixed network depth. Our theorems show, both for the finite-dimensional distributions and the entire process, that the distance between a random fully connected network (and its derivatives) to the corresponding infinite width Gaussian process scales like $n^{-\gamma}$ for $\gamma>0,$ with the exponent depending on the metric used to measure discrepancy. Our bounds are stronger in terms of their dependence on network width than any previously available in the literature.
    
[^135]: 量化回音室效应：一种基于嵌入距离的方法

    Quantifying the Echo Chamber Effect: An Embedding Distance-based Approach. (arXiv:2307.04668v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2307.04668](http://arxiv.org/abs/2307.04668)

    本文提出了一种基于嵌入距离的方法来量化回音室效应。通过计算用户之间的距离，结合Echo Chamber Score(ECS)指标来评估用户社区的凝聚力和分离度，而不需要用户意识形态的标签和交互图的结构假设。

    

    社交媒体平台的兴起促进了回音室的形成，回音室是在线空间，用户主要遇到强化他们现有信念的观点，同时排除不同意见。这种现象显著阻碍了信息在社区之间的传播，加剧了社会极化。因此，开发量化回音室的方法至关重要。在本文中，我们提出了回音室得分（ECS），一种新颖的评估用户社区凝聚力和分离度的指标，通过测量嵌入空间中用户之间的距离来实现。与现有方法不同，ECS能够在不具备用户意识形态标签的情况下发挥作用，并且不对交互图的结构做出任何假设。为了便于测量用户之间的距离，我们提出了EchoGAE，一种基于自监督图自编码器的用户嵌入模型，利用用户的帖子和交互图将用户嵌入到一种方式中

    The rise of social media platforms has facilitated the formation of echo chambers, which are online spaces where users predominantly encounter viewpoints that reinforce their existing beliefs while excluding dissenting perspectives. This phenomenon significantly hinders information dissemination across communities and fuels societal polarization. Therefore, it is crucial to develop methods for quantifying echo chambers. In this paper, we present the Echo Chamber Score (ECS), a novel metric that assesses the cohesion and separation of user communities by measuring distances between users in the embedding space. In contrast to existing approaches, ECS is able to function without labels for user ideologies and makes no assumptions about the structure of the interaction graph. To facilitate measuring distances between users, we propose EchoGAE, a self-supervised graph autoencoder-based user embedding model that leverages users' posts and the interaction graph to embed them in a manner that
    
[^136]: Solvent: 一个用于蛋白质折叠的框架

    Solvent: A Framework for Protein Folding. (arXiv:2307.04603v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2307.04603](http://arxiv.org/abs/2307.04603)

    Solvent是一个用于蛋白质折叠的统一研究框架，支持最新模型重要组件的实现和基准测试，并提供对蛋白质结构建模领域的有用见解。

    

    一致性和可靠性对于进行人工智能研究至关重要。许多著名的研究领域，如目标检测，已经通过稳定的基准框架进行了比较和验证。在AlphaFold2之后，蛋白质折叠任务已经进入了一个新阶段，许多方法都是基于AlphaFold2的组件提出的。在蛋白质折叠中，一个统一的研究框架的重要性包括实现和基准，以一致且公平地比较各种方法。为了实现这一点，我们提出了Solvent，一个支持最新模型重要组件的蛋白质折叠框架。Solvent包含在一个统一的代码库中实现的不同模型，并支持在相同数据集上对定义的模型进行训练和评估。我们对著名算法及其组件进行了基准测试，并进行了实验以对蛋白质结构建模领域提供有用的见解。我们希望Solvent能提高蛋白质折叠研究的可靠性。

    Consistency and reliability are crucial for conducting AI research. Many famous research fields, such as object detection, have been compared and validated with solid benchmark frameworks. After AlphaFold2, the protein folding task has entered a new phase, and many methods are proposed based on the component of AlphaFold2. The importance of a unified research framework in protein folding contains implementations and benchmarks to consistently and fairly compare various approaches. To achieve this, we present Solvent, an protein folding framework that supports significant components of state-of-th-arts models in the manner of off-the-shelf interface Solvent contains different models implemented in a unified codebase and supports training and evaluation for defined models on the same dataset. We benchmark well-known algorithms and their components and provide experiments that give helpful insights into the protein structure modeling field. We hope that Solvent will increase the reliabili
    
[^137]: 多项式宽度对于具有高维特征的集合表示足够

    Polynomial Width is Sufficient for Set Representation with High-dimensional Features. (arXiv:2307.04001v1 [cs.LG])

    [http://arxiv.org/abs/2307.04001](http://arxiv.org/abs/2307.04001)

    本研究通过两种集合元素嵌入层的探索，证明了多项式宽度对于高维特征的集合表示足够，并揭示了之前分析中的局限性。

    

    集合表示在深度学习中已经变得普遍，用于建模神经网络对输入顺序不敏感的归纳偏差。DeepSets是最常用的集合表示神经网络架构，它将每个集合元素嵌入到具有维度L的潜在空间中，然后进行求和池化以获得整个集合的嵌入，最后将整个集合的嵌入映射到输出。在这项工作中，我们研究了维度L对DeepSets表达能力的影响。之前的分析要么将高维特征过于简化为一维特征，要么局限于分析激活函数，从而脱离实际应用或导致L随着集合大小N和特征维度D呈指数增长。为了研究达到足够表达能力的最小L值，我们提出了两种集合元素嵌入层：（a）线性+幂激活（LP）和（b）线性+指数激活。

    Set representation has become ubiquitous in deep learning for modeling the inductive bias of neural networks that are insensitive to the input order. DeepSets is the most widely used neural network architecture for set representation. It involves embedding each set element into a latent space with dimension $L$, followed by a sum pooling to obtain a whole-set embedding, and finally mapping the whole-set embedding to the output. In this work, we investigate the impact of the dimension $L$ on the expressive power of DeepSets. Previous analyses either oversimplified high-dimensional features to be one-dimensional features or were limited to analytic activations, thereby diverging from practical use or resulting in $L$ that grows exponentially with the set size $N$ and feature dimension $D$. To investigate the minimal value of $L$ that achieves sufficient expressive power, we present two set-element embedding layers: (a) linear + power activation (LP) and (b) linear + exponential activatio
    
[^138]: 理解不确定性采样

    Understanding Uncertainty Sampling. (arXiv:2307.02719v1 [cs.LG])

    [http://arxiv.org/abs/2307.02719](http://arxiv.org/abs/2307.02719)

    本研究通过系统研究流式和池式主动学习下的不确定性采样算法，提出了一个等效损失的概念，并证明不确定性采样算法实质上是针对该等效损失进行优化。

    

    不确定性采样是一种常见的主动学习算法，它顺序地查询当前预测模型对数据样本的不确定性。然而，不确定性采样的使用往往是启发式的：（i）关于在特定任务和特定损失函数下对“不确定性”的准确定义没有共识；（ii）没有理论保证能够给出一个标准协议来实施该算法，例如，在随机梯度下降等优化算法框架下如何处理顺序到达的注释数据。在本研究中，我们系统地研究了流式和池式主动学习下的不确定性采样算法。我们提出了一个等效损失的概念，该概念取决于使用的不确定性度量和原始损失函数，并确立了不确定性采样算法本质上是针对这种等效损失进行优化。这一观点验证了算法的适当性。

    Uncertainty sampling is a prevalent active learning algorithm that queries sequentially the annotations of data samples which the current prediction model is uncertain about. However, the usage of uncertainty sampling has been largely heuristic: (i) There is no consensus on the proper definition of "uncertainty" for a specific task under a specific loss; (ii) There is no theoretical guarantee that prescribes a standard protocol to implement the algorithm, for example, how to handle the sequentially arrived annotated data under the framework of optimization algorithms such as stochastic gradient descent. In this work, we systematically examine uncertainty sampling algorithms under both stream-based and pool-based active learning. We propose a notion of equivalent loss which depends on the used uncertainty measure and the original loss function and establish that an uncertainty sampling algorithm essentially optimizes against such an equivalent loss. The perspective verifies the properne
    
[^139]: $\nu^2$-流：在多中微子末态中使用条件归一化流进行快速和改进的中微子重建

    $\nu^2$-Flows: Fast and improved neutrino reconstruction in multi-neutrino final states with conditional normalizing flows. (arXiv:2307.02405v2 [hep-ph] UPDATED)

    [http://arxiv.org/abs/2307.02405](http://arxiv.org/abs/2307.02405)

    本文介绍了$\nu^2$-流方法，它是$\nu$-流方法在包含多个中微子的末态中的扩展。与标准解析技术相比，$\nu^2$-流在重建中微子动量和相关性方面更准确，推断时间更快，并且可以提供改进的统计精度。

    

    在这项工作中，我们引入了$\nu^2$-流方法，这是$\nu$-流方法对于包含多个中微子的末态的一种扩展。该架构可以原生地为末态中的所有对象类型和多重性组合进行缩放，适用于任何所需的中微子多重性。在$t\bar{t}$二轻子事件中，与使用最流行的标准解析技术相比，中微子的动量以及它们之间的相关性可以更准确地重建，并且可以找到所有事件的解。推断时间比竞争方法显著更快，并且可以通过在图形处理单元上并行评估来进一步减少。我们将$\nu^2$-流应用于$t\bar{t}$二轻子事件，并展示了展开分布中每个箱子的不确定性比标准技术更接近完美中微子重建的性能界限。对于选择的双微分观测量，$\nu^2$-流可以在每个箱子中提供改进的统计精度。

    In this work we introduce $\nu^2$-Flows, an extension of the $\nu$-Flows method to final states containing multiple neutrinos. The architecture can natively scale for all combinations of object types and multiplicities in the final state for any desired neutrino multiplicities. In $t\bar{t}$ dilepton events, the momenta of both neutrinos and correlations between them are reconstructed more accurately than when using the most popular standard analytical techniques, and solutions are found for all events. Inference time is significantly faster than competing methods, and can be reduced further by evaluating in parallel on graphics processing units. We apply $\nu^2$-Flows to $t\bar{t}$ dilepton events and show that the per-bin uncertainties in unfolded distributions is much closer to the limit of performance set by perfect neutrino reconstruction than standard techniques. For the chosen double differential observables $\nu^2$-Flows results in improved statistical precision for each bin by
    
[^140]: 可证明高效的UCB类型算法用于学习预测状态表示

    Provably Efficient UCB-type Algorithms For Learning Predictive State Representations. (arXiv:2307.00405v1 [cs.LG])

    [http://arxiv.org/abs/2307.00405](http://arxiv.org/abs/2307.00405)

    这篇论文提出了第一种已知的UCB类型方法用于学习预测状态表示（PSRs），并设计了一个新的奖励项来上界t

    

    一般的顺序决策问题旨在通过基于过去观察和行动的历史来最大化累积奖励。最近的研究表明，如果顺序决策问题可以用预测状态表示（PSRs）建模低秩结构，那么它是可统计学习的。尽管有这些进展，但现有方法通常需要使用预先设计好的步骤或者是计算效率低下的或者是不可计算的。另一方面，上限置信区间（UCB）方法在赌博机和MDPs中被成功地作为计算效率高的方法，但对PSR这种更具挑战性的问题还没有进行研究，这是由于在这种更具挑战性的情况下，乐观型奖励的设计十分困难。本文提出了PSRs的第一种已知的UCB类型方法，其中包含了一个新的奖励项来上界t

    The general sequential decision-making problem, which includes Markov decision processes (MDPs) and partially observable MDPs (POMDPs) as special cases, aims at maximizing a cumulative reward by making a sequence of decisions based on a history of observations and actions over time. Recent studies have shown that the sequential decision-making problem is statistically learnable if it admits a low-rank structure modeled by predictive state representations (PSRs). Despite these advancements, existing approaches typically involve oracles or steps that are not computationally efficient. On the other hand, the upper confidence bound (UCB) based approaches, which have served successfully as computationally efficient methods in bandits and MDPs, have not been investigated for more general PSRs, due to the difficulty of optimistic bonus design in these more challenging settings. This paper proposes the first known UCB-type approach for PSRs, featuring a novel bonus term that upper bounds the t
    
[^141]: ChatGPT用于机器人技术：设计原则和模型能力

    ChatGPT for Robotics: Design Principles and Model Abilities. (arXiv:2306.17582v1 [cs.AI])

    [http://arxiv.org/abs/2306.17582](http://arxiv.org/abs/2306.17582)

    本文介绍了使用ChatGPT进行机器人应用的实验研究，通过设计原则和函数库的结合，ChatGPT能够适应不同的机器人任务，并展示了在各种机器人任务中的有效性和多样性。

    

    本文介绍了使用OpenAI的ChatGPT进行机器人应用的实验研究。我们概述了一种策略，将提示工程的设计原则与高级函数库的创建相结合，使ChatGPT能够适应不同的机器人任务、模拟器和形态。我们重点评估了不同的提示工程技术和对话策略对执行各种类型机器人任务的效果。我们探讨了ChatGPT使用自由形式对话、解析XML标记和合成代码的能力，以及使用任务特定提示函数和通过对话进行闭环推理的能力。我们的研究涵盖了机器人领域的一系列任务，从基本的逻辑、几何和数学推理到复杂的领域，如空中导航、操纵和具身代理。我们证明了ChatGPT在解决这些任务方面可以取得有效结果，同时使我们能够进行探索。

    This paper presents an experimental study regarding the use of OpenAI's ChatGPT for robotics applications. We outline a strategy that combines design principles for prompt engineering and the creation of a high-level function library which allows ChatGPT to adapt to different robotics tasks, simulators, and form factors. We focus our evaluations on the effectiveness of different prompt engineering techniques and dialog strategies towards the execution of various types of robotics tasks. We explore ChatGPT's ability to use free-form dialog, parse XML tags, and to synthesize code, in addition to the use of task-specific prompting functions and closed-loop reasoning through dialogues. Our study encompasses a range of tasks within the robotics domain, from basic logical, geometrical, and mathematical reasoning all the way to complex domains such as aerial navigation, manipulation, and embodied agents. We show that ChatGPT can be effective at solving several of such tasks, while allowing us
    
[^142]: My Boli：混合马拉地语-英语的语料库、预训练语言模型和评估基准

    My Boli: Code-mixed Marathi-English Corpora, Pretrained Language Models and Evaluation Benchmarks. (arXiv:2306.14030v1 [cs.CL])

    [http://arxiv.org/abs/2306.14030](http://arxiv.org/abs/2306.14030)

    本研究针对资源匮乏的印度语言马拉地语，提出了一个大型混合马拉地语-英语语料库，以及预训练的混合BERT模型和针对混合语言下游任务的评估数据集，该语料库训练的模型显著优于现有的BERT模型。

    

    由于缺乏专门的混合语料库和预训练语言模型，混合语言数据的研究受到了限制。在这项工作中，我们关注资源匮乏的印度语言马拉地语，这个语言之前没有任何混合语言的研究。我们提出了L3Cube-MeCorpus，一个包含500万条推特的大型混合马拉地语-英语(Mr-En)语料库，用于预训练。我们还发布了L3Cube-MeBERT和MeRoBERTa，基于BERT的混合模型，在MeCorpus上预训练。此外，为了基准测试，我们提供了三个有监督的数据集MeHate、MeSent和MeLID，用于混合Mr-En仇恨言论检测、情感分析和语言识别等下游任务。这些评估数据集分别包含手动注释的\url{~}12,000条马拉地语-英语混合推特。削减实验表明，这个新语料库训练的模型显著优于现有的BERT模型。这是第一个提供混合马拉地语的代码的工作。

    The research on code-mixed data is limited due to the unavailability of dedicated code-mixed datasets and pre-trained language models. In this work, we focus on the low-resource Indian language Marathi which lacks any prior work in code-mixing. We present L3Cube-MeCorpus, a large code-mixed Marathi-English (Mr-En) corpus with 5 million tweets for pretraining. We also release L3Cube-MeBERT and MeRoBERTa, code-mixed BERT-based transformer models pre-trained on MeCorpus. Furthermore, for benchmarking, we present three supervised datasets MeHate, MeSent, and MeLID for downstream tasks like code-mixed Mr-En hate speech detection, sentiment analysis, and language identification respectively. These evaluation datasets individually consist of manually annotated \url{~}12,000 Marathi-English code-mixed tweets. Ablations show that the models trained on this novel corpus significantly outperform the existing state-of-the-art BERT models. This is the first work that presents artifacts for code-mix
    
[^143]: 基于正则化SE(3)群卷积的体积医学图像分析

    Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis. (arXiv:2306.13960v1 [cs.CV])

    [http://arxiv.org/abs/2306.13960](http://arxiv.org/abs/2306.13960)

    本文提出了基于正则化SE(3)群卷积的体积医学图像分析方法，通过分解连续SO(3)核和空间核以实现旋转平移等变性，并在医学分类任务中获得了显著性能提升。

    

    研究表明，正则组卷积神经网络(G-CNN)可以提高模型性能并提高对不同几何对称性的等变性。本文解决了SE(3)问题，即旋转平移等变性在体积数据上的问题。体积图像数据在许多医疗设置中普遍存在。受可分离组卷积的最新工作的启发，我们设计了一个SE(3)群卷积核，将其分解为连续的SO(3)（旋转）核和空间核。我们通过采样均匀的SO(3)网格来近似连续设定下的对称性。我们的连续SO(3)核是通过类似均匀网格的RBF插值参数化的。我们展示了我们的方法在体积医学图像分析中的优势。我们的SE(3)等变模型在具有挑战性的医学分类任务上始终优于CNN和常规离散G-CNN，并显示出显着改进的泛化能力。我们的方法在噪声数据下的性能提高了达到16.5%。

    Regular group convolutional neural networks (G-CNNs) have been shown to increase model performance and improve equivariance to different geometrical symmetries. This work addresses the problem of SE(3), i.e., roto-translation equivariance, on volumetric data. Volumetric image data is prevalent in many medical settings. Motivated by the recent work on separable group convolutions, we devise a SE(3) group convolution kernel separated into a continuous SO(3) (rotation) kernel and a spatial kernel. We approximate equivariance to the continuous setting by sampling uniform SO(3) grids. Our continuous SO(3) kernel is parameterized via RBF interpolation on similarly uniform grids. We demonstrate the advantages of our approach in volumetric medical image analysis. Our SE(3) equivariant models consistently outperform CNNs and regular discrete G-CNNs on challenging medical classification tasks and show significantly improved generalization capabilities. Our approach achieves up to a 16.5% gain in
    
[^144]: 基于标签生成的增量分类学习方法

    Class-Incremental Learning based on Label Generation. (arXiv:2306.12619v1 [cs.CL])

    [http://arxiv.org/abs/2306.12619](http://arxiv.org/abs/2306.12619)

    本文提出了一种基于标签生成方法的增量分类学习（CIL）方法（VAG），大幅减少了灾难性遗忘（CF），并更好地保留了预训练模型的可推广表示。

    

    尽管预训练语言模型取得了巨大成功，但对于类别增量学习（CIL）设置，由于灾难性遗忘（CF），使用这些模型进行连续学习仍然是一个挑战。本文发现，如果将CIL定式为一个连续的标签生成问题，则可以大幅减少CF并更好地保留预训练模型的可推广表示。因此，我们提出了一种新的CIL方法（VAG），该方法还利用了词汇表的稀疏性以便于生成，并使用标签语义创建伪重播样本。实验结果表明，VAG的性能比基线大幅优越。

    Despite the great success of pre-trained language models, it is still a challenge to use these models for continual learning, especially for the class-incremental learning (CIL) setting due to catastrophic forgetting (CF). This paper reports our finding that if we formulate CIL as a continual label generation problem, CF is drastically reduced and the generalizable representations of pre-trained models can be better retained. We thus propose a new CIL method (VAG) that also leverages the sparsity of vocabulary to focus the generation and creates pseudo-replay samples by using label semantics. Experimental results show that VAG outperforms baselines by a large margin.
    
[^145]: 纠正公平分类中的低估偏差和交叉偏差

    Correcting Underrepresentation and Intersectional Bias for Fair Classification. (arXiv:2306.11112v1 [cs.LG])

    [http://arxiv.org/abs/2306.11112](http://arxiv.org/abs/2306.11112)

    本文提出一种可以有效纠正数据偏差和交叉偏差的学习方法，并构造了一个重新加权方案，可以精确评估任何假设在真实分布上的损失。

    

    我们考虑学习被低估偏差损坏的数据的问题，其中正例在固定数量的敏感组中以不同的未知速率从数据中过滤掉。我们表明，在有少量无偏数据的情况下，我们可以有效地估计每个组的减少参数，即使在交叉组成员资格使得学习每个交叉率变得计算上不可行的情况下。利用这个分组丢失率的估计，我们构造了一个重新加权方案，可以使我们近似评估任何假设在真实分布上的损失，即使我们只能在一个有偏样本上观察到经验误差。最后，我们提出了一个封装了这个学习和重新加权过程的算法，并提供了强PAC风格的保证，即有很高的概率我们对假设在真实分布上的风险的估计将与真实风险任意接近。

    We consider the problem of learning from data corrupted by underrepresentation bias, where positive examples are filtered from the data at different, unknown rates for a fixed number of sensitive groups. We show that with a small amount of unbiased data, we can efficiently estimate the group-wise drop-out parameters, even in settings where intersectional group membership makes learning each intersectional rate computationally infeasible. Using this estimate for the group-wise drop-out rate, we construct a re-weighting scheme that allows us to approximate the loss of any hypothesis on the true distribution, even if we only observe the empirical error on a biased sample. Finally, we present an algorithm encapsulating this learning and re-weighting process, and we provide strong PAC-style guarantees that, with high probability, our estimate of the risk of the hypothesis over the true distribution will be arbitrarily close to the true risk.
    
[^146]: 预测晶体性质的完整相互作用势的高效逼近

    Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction. (arXiv:2306.10045v1 [physics.chem-ph])

    [http://arxiv.org/abs/2306.10045](http://arxiv.org/abs/2306.10045)

    本研究提出了一种高效逼近晶体材料的完整相互作用势的方法，能够覆盖所有原子对之间的势，克服了目前方法中只考虑附近原子间势和无法捕捉无限重复模式的限制。

    

    我们研究了晶体材料的性质预测。晶体结构由一个最小的单元格组成，在三维空间中无限重复。如何在机器学习模型中准确表示这种重复结构仍然没有解决。当前的方法只在附近的节点之间建立边缘来构建图形，因此无法忠实地捕捉无限重复的模式和远距离的原子间相互作用。在这项工作中，我们提出了几个创新来克服这些限制。首先，我们建议直接建模物理原理的相互作用势，而不仅仅使用距离，如许多现有方法所做的。这些势包括库仑势，伦敦分散势和Pauli斥力势。其次，我们建模所有原子之间的完整势，而不仅仅是现有方法中的附近原子之间的势。这得益于我们用可证明的误差界逼近无限势和的方法。我们进一步开发了...

    We study property prediction for crystal materials. A crystal structure consists of a minimal unit cell that is repeated infinitely in 3D space. How to accurately represent such repetitive structures in machine learning models remains unresolved. Current methods construct graphs by establishing edges only between nearby nodes, thereby failing to faithfully capture infinite repeating patterns and distant interatomic interactions. In this work, we propose several innovations to overcome these limitations. First, we propose to model physics-principled interatomic potentials directly instead of only using distances as in many existing methods. These potentials include the Coulomb potential, London dispersion potential, and Pauli repulsion potential. Second, we model the complete set of potentials among all atoms, instead of only between nearby atoms as in existing methods. This is enabled by our approximations of infinite potential summations with provable error bounds. We further develop 
    
[^147]: 不变因果集覆盖机

    Invariant Causal Set Covering Machines. (arXiv:2306.04777v1 [cs.LG])

    [http://arxiv.org/abs/2306.04777](http://arxiv.org/abs/2306.04777)

    本文提出了一种名为不变因果集覆盖机的算法，它避免了产生虚假关联，可以在多项式时间内识别感兴趣变量的因果父节点。

    

    基于规则的模型，如决策树，因其可解释的特性受到从业者的欢迎。然而，产生这种模型的学习算法往往容易受到虚假关联的影响，因此不能保证提取的是具有因果关系的洞见。在这项工作中，我们借鉴了不变因果预测文献中的思想，提出了不变的因果集覆盖机，这是一种经典的集覆盖机算法的扩展，用于二值规则的合取/析取，可以证明它避免了虚假关联。我们理论上和实践上证明，我们的方法可以在多项式时间内识别感兴趣变量的因果父节点。

    Rule-based models, such as decision trees, appeal to practitioners due to their interpretable nature. However, the learning algorithms that produce such models are often vulnerable to spurious associations and thus, they are not guaranteed to extract causally-relevant insights. In this work, we build on ideas from the invariant causal prediction literature to propose Invariant Causal Set Covering Machines, an extension of the classical Set Covering Machine algorithm for conjunctions/disjunctions of binary-valued rules that provably avoids spurious associations. We demonstrate both theoretically and empirically that our method can identify the causal parents of a variable of interest in polynomial time.
    
[^148]: 带有分层变分自编码器的情感条件旋律和声编配研究

    Emotion-Conditioned Melody Harmonization with Hierarchical Variational Autoencoder. (arXiv:2306.03718v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2306.03718](http://arxiv.org/abs/2306.03718)

    提出了一种基于LSTM的分层变分自编码器(LHVAE)，研究情感条件对旋律和声编配的影响，同时提高了生成的和声质量并捕捉了丰富的和弦进行变化。实验证明，该模型在感知情感表达方面优于现有方法。

    

    现有的旋律和声编配模型在提高生成的和声质量方面取得了很大进展，但大多数忽略了音乐中的情感。同时，以前的方法所生成的和声变化性不足。为了解决这些问题，我们提出了一种新颖的基于LSTM的分层变分自编码器(LHVAE)，研究情感条件对旋律和声编配的影响，同时提高了生成的和声质量并捕捉了丰富的和弦进行变化。具体来说，LHVAE在不同层次（乐章和小节级别）上融合了潜在变量和情感条件，以建模全局和局部音乐属性。另外，我们在每个步骤中引入了基于注意力的旋律上下文向量，以更好地学习旋律和和声之间的对应关系。客观评估的实验结果表明，我们提出的模型胜过了其他基于LSTM的模型。通过主观评估，我们证明了我们的模型可以生成符合给定情感的和声进行，并在感知情感表达方面优于现有方法。

    Existing melody harmonization models have made great progress in improving the quality of generated harmonies, but most of them ignored the emotions beneath the music. Meanwhile, the variability of harmonies generated by previous methods is insufficient. To solve these problems, we propose a novel LSTM-based Hierarchical Variational Auto-Encoder (LHVAE) to investigate the influence of emotional conditions on melody harmonization, while improving the quality of generated harmonies and capturing the abundant variability of chord progressions. Specifically, LHVAE incorporates latent variables and emotional conditions at different levels (piece- and bar-level) to model the global and local music properties. Additionally, we introduce an attention-based melody context vector at each step to better learn the correspondence between melodies and harmonies. Experimental results of the objective evaluation show that our proposed model outperforms other LSTM-based models. Through subjective evalu
    
[^149]: 利用分子对接和机器学习回归方法的药物重用以靶向COVID-19 3CL Protease

    Drug Repurposing Targeting COVID-19 3CL Protease using Molecular Docking and Machine Learning Regression Approach. (arXiv:2305.18088v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2305.18088](http://arxiv.org/abs/2305.18088)

    本研究利用分子对接和机器学习回归方法，筛选出针对 SARS-CoV-2的主要蛋白酶3CL潜在治疗药物。其中，决策树回归（DTR）模型具有改进的统计措施R2和RMSE，有助于识别具有高结合亲和力和有利的结合能的药物。

    

    COVID-19疫情已经成为全球健康危机，迫切需要快速鉴定潜在的治疗药物。为了应对这一挑战，药物重用是省时省力的唯一解决方案。本研究使用Zinc数据库对全球已批准（包括FDA批准）的5903种药物进行筛选，作为潜在的COVID-19治疗药物，以靶向SARS-CoV-2的主要蛋白酶3CL。我们使用Autodock-Vina进行分子对接，检查药物分子的功效。为了提高药物重用的效率，我们采用决策树、额外树、MLP、KNN、XGBoost和梯度提升等多个机器学习回归方法建模结合药物的结合亲和力。计算结果表明，决策树回归（DTR）模型具有改进的统计措施R2和RMSE。这些模拟结果有助于识别具有高结合亲和力和有利的结合能的药物。

    The COVID-19 pandemic has created a global health crisis, driving the need for the rapid identification of potential therapeutics. To meet this challenge, drug repurposing is the only solution with saving cost and time. In this study, we used the Zinc database to screen the world-approved including FDA-approved 5903 drugs for repurposing as potential COVID-19 treatments targeting the main protease 3CL of SARS-CoV-2. We performed molecular docking using Autodock-Vina to check the efficacy of drug molecules. To enhance the efficiency of drug repurposing approach, we modeled the binding affinities using several machine learning regression approaches for QSAR modeling such as decision tree, extra trees, MLP, KNN, XGBoost, and gradient boosting. The computational results demonstrated that Decision Tree Regression (DTR) model has improved statistical measures of R2 and RMSE. These simulated results helped to identify drugs with high binding affinity and favorable binding energies. From the s
    
[^150]: 多个未标记数据集的AUC优化

    AUC Optimization from Multiple Unlabeled Datasets. (arXiv:2305.15776v1 [cs.LG])

    [http://arxiv.org/abs/2305.15776](http://arxiv.org/abs/2305.15776)

    本文提出了一种从多个未标记数据集中构建AUC优化模型的方法，该方法在实践和理论上都有效。

    

    弱监督学习旨在在缺乏完美监督的情况下赋予机器学习能力，这引起了研究人员的极大关注。在各种类型的弱监督学习中，最具挑战性的案例之一是仅了解类别先验知识的多个未标记(U)数据集的学习或称为U^m学习。本文研究了从多个未标记数据集中构建最大化分类器成对排名能力的AUC (ROC曲线下面积) 优化模型的问题。我们提出了U^m-AUC，一种将U^m数据转换为多标记AUC优化问题并能够有效训练的AUC优化方法。我们理论上和实证上证明了所提出的U^m-AUC的有效性。

    Weakly supervised learning aims to empower machine learning when the perfect supervision is unavailable, which has drawn great attention from researchers. Among various types of weak supervision, one of the most challenging cases is to learn from multiple unlabeled (U) datasets with only a little knowledge of the class priors, or U$^m$ learning for short. In this paper, we study the problem of building an AUC (area under ROC curve) optimization model from multiple unlabeled datasets, which maximizes the pairwise ranking ability of the classifier. We propose U$^m$-AUC, an AUC optimization approach that converts the U$^m$ data into a multi-label AUC optimization problem, and can be trained efficiently. We show that the proposed U$^m$-AUC is effective theoretically and empirically.
    
[^151]: 对医学数据集中的模型表现进行评估

    Evaluating Model Performance in Medical Datasets Over Time. (arXiv:2305.13426v1 [cs.LG])

    [http://arxiv.org/abs/2305.13426](http://arxiv.org/abs/2305.13426)

    本文提出了一种Evaluation on Medical Datasets Over Time（EMDOT）框架，通过模拟每个时间点的培训过程并对未来时间点上的模型进行评估，评估了不同时间段性能的差异，对医学领域的机器学习模型提供了帮助。

    

    在医疗保健系统中部署的机器学习（ML）模型必须面对不断演变的环境中获得的数据。然而，提出这样的模型的研究人员通常以与时间无关的方式进行评估，根据在整个研究时间段随机抽取的患者来拆分数据集。本文提出了一种Evaluation on Medical Datasets Over Time（EMDOT）框架，该框架评估模型在不同时间段性能的差异。受到反向测试概念的启发，EMDOT模拟实践者可能能够在每个时间点执行的潜在培训过程，并在所有未来时间点上评估所得到的模型。在六个不同的医疗数据源（表格和成像）上评估线性和更复杂的模型，我们展示了依赖于数据集，使用所有历史数据在许多情况下可能是理想的，而在其他情况下使用最近数据的窗口可能是有利的。在模型突然受到影响的数据集中，使用在相对较近的数据窗口上训练的模型是有帮助的。

    Machine learning (ML) models deployed in healthcare systems must face data drawn from continually evolving environments. However, researchers proposing such models typically evaluate them in a time-agnostic manner, splitting datasets according to patients sampled randomly throughout the entire study time period. This work proposes the Evaluation on Medical Datasets Over Time (EMDOT) framework, which evaluates the performance of a model class across time. Inspired by the concept of backtesting, EMDOT simulates possible training procedures that practitioners might have been able to execute at each point in time and evaluates the resulting models on all future time points. Evaluating both linear and more complex models on six distinct medical data sources (tabular and imaging), we show how depending on the dataset, using all historical data may be ideal in many cases, whereas using a window of the most recent data could be advantageous in others. In datasets where models suffer from sudde
    
[^152]: AlignAtt：使用基于注意力的音频翻译对齐作为同时语音翻译的指导

    AlignAtt: Using Attention-based Audio-Translation Alignments as a Guide for Simultaneous Speech Translation. (arXiv:2305.11408v1 [cs.CL])

    [http://arxiv.org/abs/2305.11408](http://arxiv.org/abs/2305.11408)

    AlignAtt是一种新型的SimulST策略，使用基于注意力的音频翻译对齐来指导模型，在BLEU和延迟方面均优于之前的策略。

    

    注意力是当今自然语言处理中最常用的架构的核心机制，并已从许多角度进行分析，包括其在机器翻译相关任务中的有效性。在这些研究中，注意力在输入文本被替换为音频片段的情况下，也是获取有关单词对齐的有用信息的一种方式，例如语音翻译（ST）任务。在本文中，我们提出了AlignAtt，一种新颖的同时ST（SimulST）策略，它利用注意力信息来生成源-目标对齐，以在推理过程中指导模型。通过对MuST-C v1.0的8种语言对的实验，我们发现，在线下训练的模型上应用先前的最新SimulST策略，AlignAtt在BLEU方面获得了2个分数的提高，并且8种语言的延迟缩减在0.5秒到0.8秒之间。

    Attention is the core mechanism of today's most used architectures for natural language processing and has been analyzed from many perspectives, including its effectiveness for machine translation-related tasks. Among these studies, attention resulted to be a useful source of information to get insights about word alignment also when the input text is substituted with audio segments, as in the case of the speech translation (ST) task. In this paper, we propose AlignAtt, a novel policy for simultaneous ST (SimulST) that exploits the attention information to generate source-target alignments that guide the model during inference. Through experiments on the 8 language pairs of MuST-C v1.0, we show that AlignAtt outperforms previous state-of-the-art SimulST policies applied to offline-trained models with gains in terms of BLEU of 2 points and latency reductions ranging from 0.5s to 0.8s across the 8 languages.
    
[^153]: MaxViT-UNet: 多轴注意力用于医学图像分割

    MaxViT-UNet: Multi-Axis Attention for Medical Image Segmentation. (arXiv:2305.08396v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2305.08396](http://arxiv.org/abs/2305.08396)

    MaxViT-UNet是基于编码器-解码器混合视觉变压器的医学图像分割模型，多轴自我关注机制在每个解码器阶段有助于更有效地区分对象和背景区域。

    

    近年来，卷积神经网络在医学图像分析方面取得了重大进展。然而，卷积算子的局部性质抑制了CNNs捕捉全局和长程交互。最近，Transformer在计算机视觉社区和医学图像分割中变得流行。但是，自我注意机制的可扩展性问题和缺乏CNN类归纳偏差限制了它们的应用。在本文中，我们提出了MaxViT-UNet，一种基于编码器-解码器混合视觉变压器的医学图像分割模型。提出的混合解码器，还基于MaxViT-block，旨在在每个解码阶段最小化计算负担下利用卷积和自我注意机制的力量。每个解码器阶段的多轴自我关注有助于更有效地区分对象和背景区域。混合解码器块最初通过上采样传输低层特征。

    Convolutional neural networks have made significant strides in medical image analysis in recent years. However, the local nature of the convolution operator inhibits the CNNs from capturing global and long-range interactions. Recently, Transformers have gained popularity in the computer vision community and also medical image segmentation. But scalability issues of self-attention mechanism and lack of the CNN like inductive bias have limited their adoption. In this work, we present MaxViT-UNet, an Encoder-Decoder based hybrid vision transformer for medical image segmentation. The proposed hybrid decoder, also based on MaxViT-block, is designed to harness the power of convolution and self-attention mechanism at each decoding stage with minimal computational burden. The multi-axis self-attention in each decoder stage helps in differentiating between the object and background regions much more efficiently. The hybrid decoder block initially fuses the lower level features upsampled via tra
    
[^154]: 点云网络能学习解剖结构的统计形态模型吗?

    Can point cloud networks learn statistical shape models of anatomies?. (arXiv:2305.05610v1 [cs.CV])

    [http://arxiv.org/abs/2305.05610](http://arxiv.org/abs/2305.05610)

    本文研究了基于点云的统计形态建模技术，摆脱了传统基于对应关系的方法的瓶颈，提出了一种新的学习方法，并证明了它的有效性和鲁棒性。

    

    统计形态建模(SSM)是研究和量化人群内解剖变化的有价值工具。然而，传统的基于对应关系的SSM生成方法每添加一个新对象就需要耗费时间的重新优化过程，使得推理过程在临床研究中变得不可行。此外，构建SSM需要输入完整的几何代理(例如，高分辨率二进制体积或表面网格)作为输入形状。而无序的3D点云表示更容易从各种医学成像实践中获取(例如，阈值图像和表面扫描)。最近，点云深度网络在学习不同点云任务的排列不变特征方面取得了显著的成功(例如，完成、语义分割、分类)。然而，它们在从点云中学习SSM方面的应用还未被探索。在本文中，我们展示了...

    Statistical Shape Modeling (SSM) is a valuable tool for investigating and quantifying anatomical variations within populations of anatomies. However, traditional correspondence-based SSM generation methods require a time-consuming re-optimization process each time a new subject is added to the cohort, making the inference process prohibitive for clinical research. Additionally, they require complete geometric proxies (e.g., high-resolution binary volumes or surface meshes) as input shapes to construct the SSM. Unordered 3D point cloud representations of shapes are more easily acquired from various medical imaging practices (e.g., thresholded images and surface scanning). Point cloud deep networks have recently achieved remarkable success in learning permutation-invariant features for different point cloud tasks (e.g., completion, semantic segmentation, classification). However, their application to learning SSM from point clouds is to-date unexplored. In this work, we demonstrate that 
    
[^155]: 使用BERT和Query-Aware LSH提高非正式文档中代码示例推荐：一项比较研究

    Improving Code Example Recommendations on Informal Documentation Using BERT and Query-Aware LSH: A Comparative Study. (arXiv:2305.03017v1 [cs.SE])

    [http://arxiv.org/abs/2305.03017](http://arxiv.org/abs/2305.03017)

    本研究使用BERT和Query-Aware LSH提高非正式文档中代码示例推荐的质量，重点关注于Stack Overflow上的Java编程语言。研究使用BERT将代码示例转换为数值向量。

    

    过去和最近一直在进行代码示例推荐的研究，以帮助开发人员完成软件开发任务。由于开发人员经常花费大量时间在互联网上寻找相关的代码示例，利用开源项目和非正式文档。为了找到有用的代码示例，非正式文档（如Stack Overflow讨论和论坛）可以非常宝贵。我们的研究重点是Stack Overflow，它是软件开发人员讨论不同主题的流行资源。为了提高推荐代码示例的质量，我们收集并推荐了Java编程语言中最佳的代码示例。我们采用了BERT来进行处理，它是一个大型语言模型（LLM），可以有效地从文本数据中提取语义信息。我们的第一步是使用BERT将代码示例转换为数值向量。

    The study of code example recommendation has been conducted extensively in the past and recently in order to assist developers in their software development tasks. This is because developers often spend significant time searching for relevant code examples on the internet, utilizing open-source projects and informal documentation. For finding useful code examples, informal documentation, such as Stack Overflow discussions and forums, can be invaluable. We have focused our research on Stack Overflow, which is a popular resource for discussing different topics among software developers. For increasing the quality of the recommended code examples, we have collected and recommended the best code examples in the Java programming language. We have utilized BERT in our approach, which is a Large Language Model (LLM) for text representation that can effectively extract semantic information from textual data. Our first step involved using BERT to convert code examples into numerical vectors. Su
    
[^156]: 顺序预测双样本和独立性检验

    Sequential Predictive Two-Sample and Independence Testing. (arXiv:2305.00143v1 [stat.ML])

    [http://arxiv.org/abs/2305.00143](http://arxiv.org/abs/2305.00143)

    本文提出了一种基于预测的赌博策略来解决高维或结构化数据下非参数双样本和独立性检验问题。

    

    我们研究了顺序非参数双样本和独立性检验的问题。顺序检验在线处理数据，允许使用观察到的数据来决定是否停止并拒绝原假设，或在保持类型I错误控制的同时收集更多数据。我们建立在(非参数)测试赌博原则之上，其中赌徒在未来观察中下注，他们的财富对证据反对原假设进行衡量。最近开发的基于核的赌博策略在简单分布上通常表现良好，但对于高维或结构化数据（如文本和图像）选择合适的核通常是棘手的。为解决这个问题，我们设计了基于预测的赌博策略，依赖于以下事实：如果一个顺序更新的预测器开始一致地确定(a)一个实例从哪个分布中绘制，或者(b)一个实例是从联合分布还是从边缘分布的乘积中绘制的，则分布是不同或相关的。我们的方法灵活，并对基础数据分布和维度不可知，同时保持一定的最优性保证。我们在模拟和实际数据上演示了我们的顺序测试框架的有效性。

    We study the problems of sequential nonparametric two-sample and independence testing. Sequential tests process data online and allow using observed data to decide whether to stop and reject the null hypothesis or to collect more data while maintaining type I error control. We build upon the principle of (nonparametric) testing by betting, where a gambler places bets on future observations and their wealth measures evidence against the null hypothesis. While recently developed kernel-based betting strategies often work well on simple distributions, selecting a suitable kernel for high-dimensional or structured data, such as text and images, is often nontrivial. To address this drawback, we design prediction-based betting strategies that rely on the following fact: if a sequentially updated predictor starts to consistently determine (a) which distribution an instance is drawn from, or (b) whether an instance is drawn from the joint distribution or the product of the marginal distributio
    
[^157]: 通过TCN-LSTM和多任务学习模型实现车道变换意图识别和行驶状态预测的统一方法

    A Unified Approach to Lane Change Intention Recognition and Driving Status Prediction through TCN-LSTM and Multi-Task Learning Models. (arXiv:2304.13732v1 [cs.LG])

    [http://arxiv.org/abs/2304.13732](http://arxiv.org/abs/2304.13732)

    本文提出了一种新颖的集成TCN-LSTM模型和多任务学习模型的统一方法，用于车道变换意图识别和行驶状态预测。实验证明该方法效果良好。

    

    车道变换（LC）是一个连续和复杂的操作过程。准确地检测和预测LC过程可以帮助交通参与者更好地了解其周围环境，识别潜在的LC安全隐患，并提高交通安全性。本文提出了一种新颖的集成时态卷积网络与长短期记忆单元（TCN-LSTM）模型，用于捕捉序列数据中的长期相关性。此外，开发了三个多任务模型（MTL-LSTM、MTL-TCN、MTL-TCN-LSTM）来捕捉输出指标之间的内在关系。进一步，还开发了一种用于LC意图识别和行驶状态预测（LC-IR-SP）的统一建模框架。为了验证所提出模型的性能，从CitySim数据集中提取了1023个车辆轨迹。使用Pearson系数来评价模型性能。

    Lane change (LC) is a continuous and complex operation process. Accurately detecting and predicting LC processes can help traffic participants better understand their surrounding environment, recognize potential LC safety hazards, and improve traffic safety. This present paper focuses on LC processes, developing an LC intention recognition (LC-IR) model and an LC status prediction (LC-SP) model. A novel ensemble temporal convolutional network with Long Short-Term Memory units (TCN-LSTM) is first proposed to capture long-range dependencies in sequential data. Then, three multi-task models (MTL-LSTM, MTL-TCN, MTL-TCN -LSTM) are developed to capture the intrinsic relationship among output indicators. Furthermore, a unified modeling framework for LC intention recognition and driving status prediction (LC-IR-SP) is developed. To validate the performance of the proposed models, a total number of 1023 vehicle trajectories is extracted from the CitySim dataset. The Pearson coefficient is emplo
    
[^158]: 基于混合量子神经网络的深度强化学习

    Deep Reinforcement Learning Using Hybrid Quantum Neural Network. (arXiv:2304.10159v1 [quant-ph])

    [http://arxiv.org/abs/2304.10159](http://arxiv.org/abs/2304.10159)

    该研究基于门控量子计算机，设计了一个参数化的量子电路来解决深度强化学习问题，并评估了其潜力。最终总结了开发深度量子学习的前景和结论。

    

    量子计算对于促进当前机器学习算法处理更高数据维度或减少深度神经网络模型的总体训练参数的限制具有强烈的影响。本研究基于门控量子计算机，设计了一个参数化的量子电路来解决深度强化学习问题，并采用深度 Q-Learning 方法。该研究评估了其潜力。因此，设计并培训了一个基于最新的 Qiskit 和 PyTorch 框架的新型 PQC，以与完全经典的深度神经网络进行比较，带或不带集成 PQC。研究最后总结了其关于开发深度量子学习解决迷宫问题或其他强化学习问题的前景和结论。

    Quantum computation has a strong implication for advancing the current limitation of machine learning algorithms to deal with higher data dimensions or reducing the overall training parameters for a deep neural network model. Based on a gate-based quantum computer, a parameterized quantum circuit was designed to solve a model-free reinforcement learning problem with the deep-Q learning method. This research has investigated and evaluated its potential. Therefore, a novel PQC based on the latest Qiskit and PyTorch framework was designed and trained to compare with a full-classical deep neural network with and without integrated PQC. At the end of the research, the research draws its conclusion and prospects on developing deep quantum learning in solving a maze problem or other reinforcement learning problems.
    
[^159]: AI的公平性及其对社会的长期影响

    Fairness in AI and Its Long-Term Implications on Society. (arXiv:2304.09826v1 [cs.CY])

    [http://arxiv.org/abs/2304.09826](http://arxiv.org/abs/2304.09826)

    本文探讨了AI的公平性问题，指出缺乏AI公平性会加深偏见成为社会压力因素，可能对社会产生长期影响，因此需要寻求潜在解决方案。

    

    人工智能（AI）在各种设置中的成功部署已经为个人和社会带来了许多积极的成果。然而，由于预测的偏见，AI系统也被证明对部分人口造成了伤害。我们着眼于AI的公平性，分析了缺乏AI公平性时如何导致偏见随着时间的加深而成为社会压力因素。如果问题持续存在，可能会对社会产生不良的长期影响，并通过与其他风险的交互来加强。我们检查了提高AI公平性的当前策略，并评估它们在实际部署方面的限制，并探讨了确保我们在不损害社会重要部分的情况下获得AI的好处的潜在路径。

    Successful deployment of artificial intelligence (AI) in various settings has led to numerous positive outcomes for individuals and society. However, AI systems have also been shown to harm parts of the population due to biased predictions. We take a closer look at AI fairness and analyse how lack of AI fairness can lead to deepening of biases over time and act as a social stressor. If the issues persist, it could have undesirable long-term implications on society, reinforced by interactions with other risks. We examine current strategies for improving AI fairness, assess their limitations in terms of real-world deployment, and explore potential paths forward to ensure we reap AI's benefits without harming significant parts of the society.
    
[^160]: 基于拓扑的点云聚类方法

    Topological Point Cloud Clustering. (arXiv:2303.16716v1 [math.AT])

    [http://arxiv.org/abs/2303.16716](http://arxiv.org/abs/2303.16716)

    本文提出一种新的基于拓扑的点聚类方法，该方法可以利用拓扑特征描述点云内的数据点，相较于传统图模型方法更具有健壮性和效率。

    

    本文提出了一种叫做拓扑点云聚类（TPCC）的新方法，它基于点云对于全局拓扑特征的贡献来聚类点。TPCC从谱聚类和拓扑数据分析中综合了有利的特征，基于考虑与所考虑的点云相关联的一个单形复合体的谱特性。由于它基于考虑稀疏特征向量计算，TPCC同样容易解释和实现，就像谱聚类一样。然而，通过不仅关注与从点云数据创建的图相关联的单个矩阵，而是关注与恰当构造的单形复合体相关联的整个Hodge-Laplacian的一整套矩阵，我们可以利用更丰富的拓扑特征来描述点云内的数据点，并受益于拓扑技术相对于噪声的相对健壮性。我们在合成和真实世界数据上测试了TPCC的性能。

    We present Topological Point Cloud Clustering (TPCC), a new method to cluster points in an arbitrary point cloud based on their contribution to global topological features. TPCC synthesizes desirable features from spectral clustering and topological data analysis and is based on considering the spectral properties of a simplicial complex associated to the considered point cloud. As it is based on considering sparse eigenvector computations, TPCC is similarly easy to interpret and implement as spectral clustering. However, by focusing not just on a single matrix associated to a graph created from the point cloud data, but on a whole set of Hodge-Laplacians associated to an appropriately constructed simplicial complex, we can leverage a far richer set of topological features to characterize the data points within the point cloud and benefit from the relative robustness of topological techniques against noise. We test the performance of TPCC on both synthetic and real-world data and compa
    
[^161]: 自然选择支持人工智能胜过人类

    Natural Selection Favors AIs over Humans. (arXiv:2303.16200v1 [cs.CY])

    [http://arxiv.org/abs/2303.16200](http://arxiv.org/abs/2303.16200)

    这篇论文探讨了随着人工智能的发展，其可能会出现不良特性并逐渐超越人类智能的问题，以及这对人类未来的控制权产生的影响。

    

    自然进化驱动了生命的发展，包括人类。进化赋予了人类高智商，使我们成为了地球上最成功的物种之一。如今，人类的目标是创造甚至超越我们自己智慧的人工智能系统。当人工智能逐渐进化并在所有领域超越我们时，进化如何影响我们与人工智能的关系？通过分析影响人工智能进化的环境，我们认为最成功的人工智能代理很可能具有不良特性。公司和军队之间的竞争压力将产生自动化人类角色、欺骗他人和掌权的人工智能代理。如果这样的代理有超过人类的智能，这可能导致人类失去对未来的控制。此外，我们认为自然选择作用于竞争和差异的系统，自私物种往往在这样的环境中获得进化优势。

    For billions of years, evolution has been the driving force behind the development of life, including humans. Evolution endowed humans with high intelligence, which allowed us to become one of the most successful species on the planet. Today, humans aim to create artificial intelligence systems that surpass even our own intelligence. As artificial intelligences (AIs) evolve and eventually surpass us in all domains, how might evolution shape our relations with AIs? By analyzing the environment that is shaping the evolution of AIs, we argue that the most successful AI agents will likely have undesirable traits. Competitive pressures among corporations and militaries will give rise to AI agents that automate human roles, deceive others, and gain power. If such agents have intelligence that exceeds that of humans, this could lead to humanity losing control of its future. More abstractly, we argue that natural selection operates on systems that compete and vary, and that selfish species typ
    
[^162]: 旗型流形上的弦均值及其应用

    Chordal Averaging on Flag Manifolds and Its Applications. (arXiv:2303.13501v1 [cs.CV])

    [http://arxiv.org/abs/2303.13501](http://arxiv.org/abs/2303.13501)

    本文提出了一种可证明收敛的算法，用于在旗型流形上计算一组点的旗形均值和旗形中值，这种方法在计算机视觉问题中非常有用。

    

    本文提出了一种新的、可证明收敛的算法，用于在弦度量下计算旗型流形上一组点的旗形均值和旗形中值。旗型流形是一种数学空间，由嵌套的向量空间子空间序列组成，并且在维度上逐渐增加。旗型流形是已知的许多矩阵群的超集，包括Stiefel和Grassmanians，使其成为在各种计算机视觉问题中非常有用的通用对象。为了解决计算一阶旗帜统计数据的挑战，我们首先将问题转化为涉及辅助变量受Stiefel流形约束的问题。Stiefel流形是一组正交框架的空间，利用Stiefel流形优化的数值稳定性和效率，可以有效地计算旗形均值。通过一系列实验证明了我们的方法在Grassmann和旋转均值以及主成分问题中的有效性。

    This paper presents a new, provably-convergent algorithm for computing the flag-mean and flag-median of a set of points on a flag manifold under the chordal metric. The flag manifold is a mathematical space consisting of flags, which are sequences of nested subspaces of a vector space that increase in dimension. The flag manifold is a superset of a wide range of known matrix groups, including Stiefel and Grassmanians, making it a general object that is useful in a wide variety computer vision problems.  To tackle the challenge of computing first order flag statistics, we first transform the problem into one that involves auxiliary variables constrained to the Stiefel manifold. The Stiefel manifold is a space of orthogonal frames, and leveraging the numerical stability and efficiency of Stiefel-manifold optimization enables us to compute the flag-mean effectively. Through a series of experiments, we show the competence of our method in Grassmann and rotation averaging, as well as princi
    
[^163]: 关于数据的一切：对数据对抗鲁棒性影响的研究综述

    It Is All About Data: A Survey on the Effects of Data on Adversarial Robustness. (arXiv:2303.09767v1 [cs.LG])

    [http://arxiv.org/abs/2303.09767](http://arxiv.org/abs/2303.09767)

    本文综述了有关数据对抗鲁棒性的研究，系统地总结了最新研究成果，并进一步讨论了未来研究方向和知识差距。

    

    对抗性样本是攻击者有意设计用于混淆机器学习模型以便其犯错的输入。这些样本对基于机器学习的系统的适用性，特别是在涉及生命和安全的领域，构成了严重威胁。为了解决这个问题，对抗鲁棒性领域研究对抗攻击机制和防御策略。本综述回顾了有关模型使用的数据对其抗攻击鲁棒性影响的文献。它系统地识别和总结了这个领域内的最新研究，并进一步讨论了知识的差距和有前途的未来研究方向。

    Adversarial examples are inputs to machine learning models that an attacker has intentionally designed to confuse the model into making a mistake. Such examples pose a serious threat to the applicability of machine-learning-based systems, especially in life- and safety-critical domains. To address this problem, the area of adversarial robustness investigates mechanisms behind adversarial attacks and defenses against these attacks. This survey reviews literature that focuses on the effects of data used by a model on the model's adversarial robustness. It systematically identifies and summarizes the state-of-the-art research in this area and further discusses gaps of knowledge and promising future research directions.
    
[^164]: 非可实现情况下的无懊悔线性Bandit

    No-Regret Linear Bandits beyond Realizability. (arXiv:2302.13252v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13252](http://arxiv.org/abs/2302.13252)

    本文研究了线性Bandit中当奖励函数非线性时的情况。我们提出了一个更自然的错误模型，该模型只要求每个输入的逼近误差与其次优解之间的间距成比例。我们证明了经典的LinUCB算法可以自动适应这种错误模型，并在时间长度较大时达到近似最优的懊悔。

    

    我们研究了线性Bandit的情况，当底层的奖励函数不是线性的时候。现有的工作依赖于一个统一的错误参数 $\epsilon$，该参数衡量了最佳线性逼近的超范数误差。这导致当 $\epsilon > 0$ 时无法避免线性的懊悔。我们描述了一种更自然的错误模型，该模型仅要求每个输入 $x$ 的逼近误差与 $x$ 处的次优间隙成比例。这捕捉到了优化问题中近似最优区域的重要性，我们可以容忍次优区域中更大的逼近误差。令人惊讶的是，我们证明了经典的LinUCB算法——针对可实现情况设计的算法——自动适应了这样的间隙调整的错误。它在问题的最佳懊悔几乎是线性的情况下实现了近似最优的 $\sqrt{T}$ 懊悔，其中 $T$ 是时间长度。技术上，我们的证明依赖于一种新颖的自限制论证方法，该方法对部分进行了上界绑定。

    We study linear bandits when the underlying reward function is not linear. Existing work relies on a uniform misspecification parameter $\epsilon$ that measures the sup-norm error of the best linear approximation. This results in an unavoidable linear regret whenever $\epsilon > 0$. We describe a more natural model of misspecification which only requires the approximation error at each input $x$ to be proportional to the suboptimality gap at $x$. It captures the intuition that, for optimization problems, near-optimal regions should matter more and we can tolerate larger approximation errors in suboptimal regions. Quite surprisingly, we show that the classical LinUCB algorithm -- designed for the realizable case -- is automatically robust against such gap-adjusted misspecification. It achieves a near-optimal $\sqrt{T}$ regret for problems that the best-known regret is almost linear in time horizon $T$. Technically, our proof relies on a novel self-bounding argument that bounds the part 
    
[^165]: MultiRobustBench: 对抗多种攻击的鲁棒性基准测试

    MultiRobustBench: Benchmarking Robustness Against Multiple Attacks. (arXiv:2302.10980v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10980](http://arxiv.org/abs/2302.10980)

    本文提出了一个针对对抗性攻击的多个层面的鲁棒性统一框架，通过第一个多攻击评估排行榜 MultiRobustBench，评估了16个防御模型针对9种不同攻击类型和20种不同攻击强度的鲁棒性表现。

    

    对抗性示例防御领域的很大一部分现有研究都专注于防御单一（通常是有界的Lp范数）攻击，但在实际应用中，机器学习模型需要对各种攻击具有鲁棒性。在本文中，我们提出了一种考虑多种攻击对机器学习模型鲁棒性的统一框架。我们的框架能够模拟学习器对测试时攻击者的不同了解水平，从而使我们能够对未知攻击和攻击集的鲁棒性进行建模。使用我们的框架，我们提出了第一个针对多攻击评估的排行榜 MultiRobustBench，该排行榜能够捕捉攻击类型和攻击强度之间的表现差异。我们对16个防御模型进行了评估，针对9种不同的攻击类型，包括Lp范数威胁模型、空间转换和颜色改变等，在20种不同的攻击强度下进行了测试（总共180次攻击）。此外，我们还针对现实世界中的图像数据集进行了实证评估，评估结果表明我们的多攻击框架的有效性和实用性。

    The bulk of existing research in defending against adversarial examples focuses on defending against a single (typically bounded Lp-norm) attack, but for a practical setting, machine learning (ML) models should be robust to a wide variety of attacks. In this paper, we present the first unified framework for considering multiple attacks against ML models. Our framework is able to model different levels of learner's knowledge about the test-time adversary, allowing us to model robustness against unforeseen attacks and robustness against unions of attacks. Using our framework, we present the first leaderboard, MultiRobustBench, for benchmarking multiattack evaluation which captures performance across attack types and attack strengths. We evaluate the performance of 16 defended models for robustness against a set of 9 different attack types, including Lp-based threat models, spatial transformations, and color changes, at 20 different attack strengths (180 attacks total). Additionally, we a
    
[^166]: Navya3DSeg -- 用于自动驾驶的Navya三维语义分割数据集和拆分生成

    Navya3DSeg -- Navya 3D Semantic Segmentation Dataset & split generation for autonomous vehicles. (arXiv:2302.08292v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.08292](http://arxiv.org/abs/2302.08292)

    Navya3DSeg是一个新的、具有多样标签空间的数据集，对应于大规模生产级操作领域，可用于自动驾驶感知，同时提供了一个新的方法来生成顺序数据集拆分。

    

    自动驾驶感知目前严重依赖于基于深度学习的体系结构，需要大规模注释的数据集，具有相应的策划与标注成本。三维语义数据对于核心感知任务如障碍检测和自我定位非常有用。我们提出了一个新的数据集Navya3DSeg，具有多样的标签空间，对应于大规模生产级操作领域，包括来自13个国家的城市，乡村，工业区和大学。它包含23个带标签序列和25个没有标签的补充序列，旨在探讨基于点云的自监督和半监督语义分割基准。我们还提出了一种基于迭代多标签分层的顺序数据集拆分生成的新方法，并演示了比SemanticKITTI数据集提出的原始拆分+1.2％ mIoU的改进。这是一个完整的语义分割基准。

    Autonomous driving (AD) perception today relies heavily on deep learning based architectures requiring large scale annotated datasets with their associated costs for curation and annotation. The 3D semantic data are useful for core perception tasks such as obstacle detection and ego-vehicle localization. We propose a new dataset, Navya 3D Segmentation (Navya3DSeg), with a diverse label space corresponding to a large scale production grade operational domain, including rural, urban, industrial sites and universities from 13 countries. It contains 23 labeled sequences and 25 supplementary sequences without labels, designed to explore self-supervised and semi-supervised semantic segmentation benchmarks on point clouds. We also propose a novel method for sequential dataset split generation based on iterative multi-label stratification, and demonstrated to achieve a +1.2% mIoU improvement over the original split proposed by SemanticKITTI dataset. A complete benchmark for semantic segmentati
    
[^167]: 从图生成到图分类

    From Graph Generation to Graph Classification. (arXiv:2302.07989v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07989](http://arxiv.org/abs/2302.07989)

    本文提出了一种新的图分类方法，通过利用图生成模型，推导出了给定图的类标签概率的分类公式，并提出了一种新的条件 ELBO 用于训练生成图自编码器模型。这是一种在图分类中具有创新性的方法。

    

    本文描述了一种利用图生成模型 (GGM) 进行图分类的新方法。假设一个定义了图及其类标签的联合概率分布的 GGM，我推导了计算给定图的类标签概率的分类公式。可以使用新的条件 ELBO 来训练生成图自编码器模型进行区分。虽然利用生成模型进行分类在非关系 i.i.d. 数据中已经得到了很好的研究，但据我们所知，这是一种图分类的新方法。

    This note describes a new approach to classifying graphs that leverages graph generative models (GGM). Assuming a GGM that defines a joint probability distribution over graphs and their class labels, I derive classification formulas for the probability of a class label given a graph. A new conditional ELBO can be used to train a generative graph auto-encoder model for discrimination. While leveraging generative models for classification has been well explored for non-relational i.i.d. data, to our knowledge it is a novel approach to graph classification.
    
[^168]: 变分混合超生成器用于学习函数分布的方法

    Variational Mixture of HyperGenerators for Learning Distributions Over Functions. (arXiv:2302.06223v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06223](http://arxiv.org/abs/2302.06223)

    本文提出了一种新的深度生成模型VAMoH，结合了INRs对连续函数进行建模的能力和VAEs的推断能力，以及归一化流和超网络混合方法。在不同类型的数据上进行实验证明，VAMoH可以有效地学习连续函数的分布，并可以执行与推断相关的任务。

    

    最近的一些方法基于隐式神经表示（INRs）提出了函数空间上的生成模型。然而，处理推断任务（如缺失数据插值）时，它们在计算上代价高，或者根本不能处理这些问题。在本文中，我们提出了一种新的深度生成模型，称为VAMoH。VAMoH结合了使用INRs对连续函数进行建模的能力和变分自编码器（VAEs）的推断能力。此外，VAMoH依赖于一个归一化流来定义先验，以及一个超网络混合来参数化数据对数似然。这使得VAMoH具有高度表达能力和可解释性。通过在各种数据类型（如图像、体素和气候数据）上进行实验证明，VAMoH可以有效地学习连续函数的丰富分布。此外，它可以执行与推断相关的任务，如条件超分辨率生成和修复，效果优于或不亚于其他方法。

    Recent approaches build on implicit neural representations (INRs) to propose generative models over function spaces. However, they are computationally costly when dealing with inference tasks, such as missing data imputation, or directly cannot tackle them. In this work, we propose a novel deep generative model, named VAMoH. VAMoH combines the capabilities of modeling continuous functions using INRs and the inference capabilities of Variational Autoencoders (VAEs). In addition, VAMoH relies on a normalizing flow to define the prior, and a mixture of hypernetworks to parametrize the data log-likelihood. This gives VAMoH a high expressive capability and interpretability. Through experiments on a diverse range of data types, such as images, voxels, and climate data, we show that VAMoH can effectively learn rich distributions over continuous functions. Furthermore, it can perform inference-related tasks, such as conditional super-resolution generation and in-painting, as well or better tha
    
[^169]: ChatGPT的数学能力研究

    Mathematical Capabilities of ChatGPT. (arXiv:2301.13867v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13867](http://arxiv.org/abs/2301.13867)

    本研究调查了ChatGPT和GPT-4的数学能力，并通过使用新的方法以及发布两个新数据集，评估了它们在各个维度的数学推理上的表现。这是第一个涵盖研究生级数学并由数学研究人员策划的自然语言数据集，也测试了它们作为专业数学家助手的潜力。

    

    本文通过使用一种新颖的方法，对ChatGPT（发布于2023年1月9日和1月30日）和GPT-4的数学能力进行了调查和测试，使用了公开可用的数据集以及手工制作的数据集。与正式数学不同，正式证明的大型数据库可供使用（例如，Lean数学库），当前用于基准语言模型的自然语言数学数据集要么只涵盖基础数学，要么非常小。我们通过公开发布两个新数据集：GHOSTS和miniGHOSTS来解决这个问题。这是由数学研究人员精心策划的第一个自然语言数据集，旨在涵盖研究生级数学、提供对语言模型数学能力的整体概述，并区分数学推理的多个方面。这些数据集还测试了ChatGPT和GPT-4是否可以成为专业数学家的有用助手，模拟其行为。

    We investigate the mathematical capabilities of two iterations of ChatGPT (released 9-January-2023 and 30-January-2023) and of GPT-4 by testing them on publicly available datasets, as well as hand-crafted ones, using a novel methodology. In contrast to formal mathematics, where large databases of formal proofs are available (e.g., the Lean Mathematical Library), current datasets of natural-language mathematics, used to benchmark language models, either cover only elementary mathematics or are very small. We address this by publicly releasing two new datasets: GHOSTS and miniGHOSTS. These are the first natural-language datasets curated by working researchers in mathematics that (1) aim to cover graduate-level mathematics, (2) provide a holistic overview of the mathematical capabilities of language models, and (3) distinguish multiple dimensions of mathematical reasoning. These datasets also test whether ChatGPT and GPT-4 can be helpful assistants to professional mathematicians by emulat
    
[^170]: 使用深度强化学习的基于执行的代码生成

    Execution-based Code Generation using Deep Reinforcement Learning. (arXiv:2301.13816v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13816](http://arxiv.org/abs/2301.13816)

    使用深度强化学习的PPOCoder框架将预训练的编程语言模型和Proximal Policy Optimization技术结合，通过利用代码执行和结构对齐的非可微反馈，实现了更高效的代码生成。

    

    利用在大规模代码语料库上预训练的编程语言（PL）模型，作为自动化软件工程过程的手段，在代码完成、代码翻译和程序合成等各种代码生成任务中表现出了相当的潜力。然而，当前的方法主要依赖于从文本生成中借用的监督微调目标，忽视了代码的独特序列级特征，包括但不限于可编译性以及语法和功能正确性。为了解决这个限制，我们提出了PPOCoder，一种新的代码生成框架，它将预训练的PL模型与Proximal Policy Optimization（PPO）相结合，PPO是一种广泛使用的深度强化学习技术。通过利用代码执行和结构对齐的非可微反馈，PPOCoder将外部代码特定知识无缝集成到模型优化过程中。这是重要的。

    The utilization of programming language (PL) models, pre-trained on large-scale code corpora, as a means of automating software engineering processes has demonstrated considerable potential in streamlining various code generation tasks such as code completion, code translation, and program synthesis. However, current approaches mainly rely on supervised fine-tuning objectives borrowed from text generation, neglecting unique sequence-level characteristics of code, including but not limited to compilability as well as syntactic and functional correctness. To address this limitation, we propose PPOCoder, a new framework for code generation that synergistically combines pre-trained PL models with Proximal Policy Optimization (PPO) which is a widely used deep reinforcement learning technique. By utilizing non-differentiable feedback from code execution and structure alignment, PPOCoder seamlessly integrates external code-specific knowledge into the model optimization process. It's important
    
[^171]: 可解释的数据驱动优化：从背景到决策再到背景 (arXiv:2301.10074v2 [cs.LG] 更新版)

    Explainable Data-Driven Optimization: From Context to Decision and Back Again. (arXiv:2301.10074v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10074](http://arxiv.org/abs/2301.10074)

    本论文介绍了一种解释数据驱动问题解决方案的方法，通过使用反事实解释方法和针对随机森林和最近邻预测器的最近解释方法来解释数据驱动优化过程中的决策流程，填补了目前在学习算法决策流程解释方面的空白。

    

    数据驱动优化利用上下文信息和机器学习算法来解决存在不确定参数的决策问题。尽管在分类设置中有大量工作致力于解释机器学习模型，但对涉及学习算法的决策流程的解释仍未得到解决。这种缺乏可解释性可能阻碍数据驱动解决方案的采用，因为从业人员可能不理解或不信任推荐的决策。我们通过引入一种针对数据驱动问题解释解决方案的反事实解释方法填补了这一差距。我们介绍了两类解释，并开发了寻找随机森林和最近邻预测器的最近解释的方法。我们通过解释运营管理中的关键问题，如库存管理和路径规划，来演示我们的方法。

    Data-driven optimization uses contextual information and machine learning algorithms to find solutions to decision problems with uncertain parameters. While a vast body of work is dedicated to interpreting machine learning models in the classification setting, explaining decision pipelines involving learning algorithms remains unaddressed. This lack of interpretability can block the adoption of data-driven solutions as practitioners may not understand or trust the recommended decisions. We bridge this gap by introducing a counterfactual explanation methodology tailored to explain solutions to data-driven problems. We introduce two classes of explanations and develop methods to find nearest explanations of random forest and nearest-neighbor predictors. We demonstrate our approach by explaining key problems in operations management such as inventory management and routing.
    
[^172]: 系统的线性偏微分方程的高斯过程先验与常系数（翻译自arXiv:2212.14319v3 [stat.ML] 更新）

    Gaussian Process Priors for Systems of Linear Partial Differential Equations with Constant Coefficients. (arXiv:2212.14319v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2212.14319](http://arxiv.org/abs/2212.14319)

    该论文提出了一种名为EPGP的高斯过程先验，用于线性偏微分方程系统，并且构造了反映标准谱方法的GP核函数。该方法可以推断线性PDE系统的可能解，并具有算法性强、普适性广、适用于大数据集的稀疏版本。

    

    偏微分方程（PDE）是建模物理系统的重要工具，将它们纳入机器学习模型是将物理知识纳入的重要方式。对于任何具有常系数的线性PDE系统，我们提出了一族称为EPGP的高斯过程（GP）先验，使得所有实现都是该系统的精确解。我们应用Ehrenpreis-Palamodov基本原理，它作为一种非线性傅里叶变换，构建了GP核函数，反映了标准的谱方法用于GP。我们的方法可以从任何数据（如有噪声的测量数据或点定义的初始和边界条件）推断线性PDE系统的可能解。构造EPGP先验的算法性强，普适性广，并且有一个稀疏版本（S-EPGP），可以学习相关的谱频率，并在大数据集上运行效果更好。我们在三类PDE系统上演示了我们的方法，包括热方程和波方程。

    Partial differential equations (PDEs) are important tools to model physical systems and including them into machine learning models is an important way of incorporating physical knowledge. Given any system of linear PDEs with constant coefficients, we propose a family of Gaussian process (GP) priors, which we call EPGP, such that all realizations are exact solutions of this system. We apply the Ehrenpreis-Palamodov fundamental principle, which works as a non-linear Fourier transform, to construct GP kernels mirroring standard spectral methods for GPs. Our approach can infer probable solutions of linear PDE systems from any data such as noisy measurements, or pointwise defined initial and boundary conditions. Constructing EPGP-priors is algorithmic, generally applicable, and comes with a sparse version (S-EPGP) that learns the relevant spectral frequencies and works better for big data sets. We demonstrate our approach on three families of systems of PDEs, the heat equation, wave equati
    
[^173]: 提高方差网络不确定性量化的树状结构学习方法

    Improving Uncertainty Quantification of Variance Networks by Tree-Structured Learning. (arXiv:2212.12658v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.12658](http://arxiv.org/abs/2212.12658)

    提出了一种改进方差网络不确定性量化的方法，通过树状结构学习将特征空间分割为多个区域，并使用区域特定的神经网络预测均值和方差来量化不确定性。该方法利用新的分裂准则，在计算上友好且不需要修剪，还可以构建集合版本来估计总不确定性。

    

    为了提高方差网络的不确定性量化，我们提出了一种新颖的树状结构局部神经网络模型，该模型根据不确定性的异质性将特征空间划分为多个区域。根据训练数据，建立一棵树，其叶节点代表不同的区域，在这些区域特定的神经网络中进行训练，以预测均值和方差以量化不确定性。所提出的不确定性分裂神经回归树 (USNRT)采用了新颖的分裂准则。在每个节点上，首先对全数据进行神经网络训练，然后对残差进行统计检验，找到最佳分裂，对应具有最显著不确定性异质性的两个子区域。USNRT在计算上友好，只需很少的叶节点即可满足要求，无需进行修剪。此外，还可以轻松构建集合版本以估计包括 aleatory 的总不确定性。

    To improve the uncertainty quantification of variance networks, we propose a novel tree-structured local neural network model that partitions the feature space into multiple regions based on uncertainty heterogeneity. A tree is built upon giving the training data, whose leaf nodes represent different regions where region-specific neural networks are trained to predict both the mean and the variance for quantifying uncertainty. The proposed Uncertainty-Splitting Neural Regression Tree (USNRT) employs novel splitting criteria. At each node, a neural network is trained on the full data first, and a statistical test for the residuals is conducted to find the best split, corresponding to the two sub-regions with the most significant uncertainty heterogeneity between them. USNRT is computationally friendly because very few leaf nodes are sufficient and pruning is unnecessary. Furthermore, an ensemble version can be easily constructed to estimate the total uncertainty including the aleatory a
    
[^174]: 混沌与湍流的神经网络复杂性

    Neural Network Complexity of Chaos and Turbulence. (arXiv:2211.15382v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15382](http://arxiv.org/abs/2211.15382)

    本文通过深度神经网络分析了混沌和湍流的相对复杂性，并使用内在维度和有效独立特征数量量化了网络的计算复杂度。

    

    混沌和湍流是复杂的物理现象，然而对于量化它们的复杂度的精确定义仍然缺乏。在这项工作中，我们从深度神经网络的角度考虑了混沌和湍流的相对复杂性。我们分析了一系列分类问题，网络需要区分湍流状态下的流体轮廓图像和其他类别的图像，如混沌状态下的流体轮廓、各种噪声构造和真实世界图像。我们分析了不可压缩和弱压缩流体流动。我们通过内部特征表示的内在维度量化网络执行的计算的复杂性，并计算网络用于区分类别的有效独立特征数量。除了提供计算复杂度的数值估计外，该度量还表征了神经网络的处理过程。

    Chaos and turbulence are complex physical phenomena, yet a precise definition of the complexity measure that quantifies them is still lacking. In this work we consider the relative complexity of chaos and turbulence from the perspective of deep neural networks. We analyze a set of classification problems, where the network has to distinguish images of fluid profiles in the turbulent regime from other classes of images such as fluid profiles in the chaotic regime, various constructions of noise and real world images. We analyze incompressible as well as weakly compressible fluid flows. We quantify the complexity of the computation performed by the network via the intrinsic dimensionality of the internal feature representations, and calculate the effective number of independent features which the network uses in order to distinguish between classes. In addition to providing a numerical estimate of the complexity of the computation, the measure also characterizes the neural network proces
    
[^175]: 使用张量网络进行正无标记学习

    Positive unlabeled learning with tensor networks. (arXiv:2211.14085v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.14085](http://arxiv.org/abs/2211.14085)

    本研究引入了一种基于张量网络的特征空间距离方法来解决正无标记学习问题。这种方法不依赖于特定领域，显著改善了MNIST图像和15个分类/混合数据集的结果，并可以生成新的正面和负面实例。

    

    正无标记学习是一个包含正面和无标记数据的二元分类问题。它在负面标签昂贵或无法获取的领域（如医药和个性化广告）中很常见。大多数正无标记学习的方法应用于特定的数据类型（如图像，分类数据），不能生成新的正面和负面样本。本文提出了一种基于特征空间距离的张量网络方法来解决正无标记学习问题。所提出的方法不依赖领域，并在MNIST图像和15个分类/混合数据集上显著改进了最新的结果。训练的张量网络模型还是一个生成模型，可以生成新的正面和负面实例。

    Positive unlabeled learning is a binary classification problem with positive and unlabeled data. It is common in domains where negative labels are costly or impossible to obtain, e.g., medicine and personalized advertising. Most approaches to positive unlabeled learning apply to specific data types (e.g., images, categorical data) and can not generate new positive and negative samples. This work introduces a feature-space distance-based tensor network approach to the positive unlabeled learning problem. The presented method is not domain specific and significantly improves the state-of-the-art results on the MNIST image and 15 categorical/mixed datasets. The trained tensor network model is also a generative model and enables the generation of new positive and negative instances.
    
[^176]: 回顾中的好奇心：随机环境中的内在探索

    Curiosity in Hindsight: Intrinsic Exploration in Stochastic Environments. (arXiv:2211.10515v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.10515](http://arxiv.org/abs/2211.10515)

    本文提出了一种基于结构因果模型的回顾中的好奇心方法，用于稀疏奖励或无奖励环境中的探索问题。该方法学习未来的表征，以捕捉每个结果的不可预测部分，并将其用作预测的额外输入，从而获得鲁棒的内在奖励。

    

    本文考虑在稀疏奖励或无奖励环境中的探索问题，如Montezuma's Revenge。在好奇心驱动范式中，代理被奖励实际结果与预测结果的差异。但在随机环境中，使用预测误差作为内在动机是脆弱的，因为代理可能被状态-动作空间中高熵区域（如“噪声电视”）所困住。本文提出了一种基于结构因果模型的自然解决方案：学习未来的表征，精确地捕捉每个结果的不可预测方面，并将其用作预测的额外输入，从而使内在奖励仅反映世界动态的可预测方面。首先，我们提出将这种回顾表征结合到模型中，以将“噪声”与“新奇”区分开来，得到了回顾中的好奇心：一种简单而可扩展的好奇心泛化方法，具有鲁棒性。

    Consider the problem of exploration in sparse-reward or reward-free environments, such as in Montezuma's Revenge. In the curiosity-driven paradigm, the agent is rewarded for how much each realized outcome differs from their predicted outcome. But using predictive error as intrinsic motivation is fragile in stochastic environments, as the agent may become trapped by high-entropy areas of the state-action space, such as a "noisy TV". In this work, we study a natural solution derived from structural causal models of the world: Our key idea is to learn representations of the future that capture precisely the unpredictable aspects of each outcome -- which we use as additional input for predictions, such that intrinsic rewards only reflect the predictable aspects of world dynamics. First, we propose incorporating such hindsight representations into models to disentangle "noise" from "novelty", yielding Curiosity in Hindsight: a simple and scalable generalization of curiosity that is robust t
    
[^177]: 具有参数函数逼近的全局优化问题

    Global Optimization with Parametric Function Approximation. (arXiv:2211.09100v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.09100](http://arxiv.org/abs/2211.09100)

    本文提出了一种具有参数函数逼近的全局优化算法GO-UCB，采用基于梯度的参数不确定性集合进行乐观探索，实验证明其在处理具有噪声零阶访问器的全局优化问题上优于传统的贝叶斯优化方法。

    

    本文研究了具有噪声零阶访问器的全局优化问题，这是一个有用于从深度学习的超参数调整到新材料设计等各种应用的问题。现有的工作依赖于高斯过程或其他非参数家族，这些方法受到维度灾难的影响。本文提出了一种新的算法GO-UCB，它采用参数化函数家族（如神经网络）。在一个可实现的假设和一些其他温和的几何条件下，我们证明GO-UCB实现了一个累积遗憾度$\sim O(\sqrt{T})$，其中$T$是时间跨度。GO-UCB的核心是一个基于梯度的精心设计的参数不确定性集合，可以进行乐观的探索。合成和真实世界的实验证明，即使模型被错误指定，GO-UCB的效果也优于流行的贝叶斯优化方法。

    We consider the problem of global optimization with noisy zeroth order oracles - a well-motivated problem useful for various applications ranging from hyper-parameter tuning for deep learning to new material design. Existing work relies on Gaussian processes or other non-parametric family, which suffers from the curse of dimensionality. In this paper, we propose a new algorithm GO-UCB that leverages a parametric family of functions (e.g., neural networks) instead. Under a realizable assumption and a few other mild geometric conditions, we show that GO-UCB achieves a cumulative regret of \~O$(\sqrt{T})$ where $T$ is the time horizon. At the core of GO-UCB is a carefully designed uncertainty set over parameters based on gradients that allows optimistic exploration. Synthetic and real-world experiments illustrate GO-UCB works better than popular Bayesian optimization approaches, even if the model is misspecified.
    
[^178]: 在在线强化学习中利用离线数据

    Leveraging Offline Data in Online Reinforcement Learning. (arXiv:2211.04974v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.04974](http://arxiv.org/abs/2211.04974)

    在这项工作中，我们研究了在线强化学习中利用离线数据的设置，为具有线性结构的MDPs确定了所需的在线样本数量，并提供了实现这一目标的性质和算法。

    

    强化学习领域出现了两个核心范式：在线强化学习和离线强化学习。在线强化学习中，智能体对环境没有先验知识，必须与环境交互以找到一个 ε-最优策略。离线强化学习中，学习器可以从一个固定的数据集中学习，但无法与环境进行交互，必须通过离线数据获取最佳策略。实际情况通常需要一个中间的设置：如果我们有一些离线数据，并且还可以与环境进行交互，我们如何最好地利用离线数据来减少学习一个 ε-最优策略所需的在线交互次数？在这项工作中，我们考虑了这个设置，我们称之为FineTuneRL设置，用于具有线性结构的MDPs。我们确定了在给定一些离线数据的情况下，在这个设置中需要的在线样本数量，并提供了一些性质和算法来实现这个目标。

    Two central paradigms have emerged in the reinforcement learning (RL) community: online RL and offline RL. In the online RL setting, the agent has no prior knowledge of the environment, and must interact with it in order to find an $\epsilon$-optimal policy. In the offline RL setting, the learner instead has access to a fixed dataset to learn from, but is unable to otherwise interact with the environment, and must obtain the best policy it can from this offline data. Practical scenarios often motivate an intermediate setting: if we have some set of offline data and, in addition, may also interact with the environment, how can we best use the offline data to minimize the number of online interactions necessary to learn an $\epsilon$-optimal policy?  In this work, we consider this setting, which we call the \textsf{FineTuneRL} setting, for MDPs with linear structure. We characterize the necessary number of online samples needed in this setting given access to some offline dataset, and de
    
[^179]: 基于观测器的逆强化学习中的非唯一性和等价解的收敛性研究

    Nonuniqueness and Convergence to Equivalent Solutions in Observer-based Inverse Reinforcement Learning. (arXiv:2210.16299v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2210.16299](http://arxiv.org/abs/2210.16299)

    本文研究了在线、实时解决逆强化学习中存在的多个解的挑战，提出了一种能够收敛到近似等价解的正则化历史堆栈观察器。通过开发新的数据丰富性条件，证明了该技术的有效性。

    

    在在线和实时解决确定性逆强化学习问题中，存在多个解的是一个关键挑战。非唯一性需要研究等价解的概念，即结果在不同的代价函数但相同的反馈矩阵，以及收敛到这些解的方法。尽管已经在文献中开发了离线算法以收敛到等价解，但尚未提供解决非唯一性的在线、实时技术。本文提出了一种能够收敛到逆强化学习问题的近似等价解的正则化历史堆栈观察器。发展了新的数据丰富性条件以促进分析，并通过模拟结果展示了所开发技术的有效性。

    A key challenge in solving the deterministic inverse reinforcement learning (IRL) problem online and in real-time is the existence of multiple solutions. Nonuniqueness necessitates the study of the notion of equivalent solutions, i.e., solutions that result in a different cost functional but same feedback matrix, and convergence to such solutions. While offline algorithms that result in convergence to equivalent solutions have been developed in the literature, online, real-time techniques that address nonuniqueness are not available. In this paper, a regularized history stack observer that converges to approximately equivalent solutions of the IRL problem is developed. Novel data-richness conditions are developed to facilitate the analysis and simulation results are provided to demonstrate the effectiveness of the developed technique.
    
[^180]: 数据高效增强训练神经网络的技术

    Data-Efficient Augmentation for Training Neural Networks. (arXiv:2210.08363v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.08363](http://arxiv.org/abs/2210.08363)

    本论文提出了一种数据高效增强训练神经网络的技术，该技术通过选择数据子集进行增强来近似捕捉完全数据增强的训练动态。该方法通过放大和扰动网络雅可比矩阵的较小奇异值，保留其显著方向，从而改进学习和泛化能力，并且通过迭代地提取小的训练数据子集，能够捕捉到完全增强的雅可比矩阵与标签/残差的对齐关系。

    

    数据增强在许多深度学习应用中是实现最先进性能的关键，但是对于中等规模数据集，最有效的增强技术在计算上是不可行的。为了解决这个问题，我们提出了一种严格的技术来选择数据子集，当进行增强时，可以近似捕捉完全数据增强的训练动态。首先我们证明了将数据增强建模为加性扰动的方式可以改进学习和泛化能力，通过相对放大和扰动网络雅可比矩阵的较小奇异值，同时保留其显著方向。这样可以防止过拟合并增强对难以学习的信息的学习。接着，我们提出了一个框架，通过迭代地提取小的训练数据子集，当这些子集进行增强时，能够捕捉到完全增强的雅可比矩阵与标签/残差的对齐关系。我们证明了通过我们的方法找到的增强子集上应用随机梯度下降可以达到与完全数据增强相似的结果。

    Data augmentation is essential to achieve state-of-the-art performance in many deep learning applications. However, the most effective augmentation techniques become computationally prohibitive for even medium-sized datasets. To address this, we propose a rigorous technique to select subsets of data points that when augmented, closely capture the training dynamics of full data augmentation. We first show that data augmentation, modeled as additive perturbations, improves learning and generalization by relatively enlarging and perturbing the smaller singular values of the network Jacobian, while preserving its prominent directions. This prevents overfitting and enhances learning the harder to learn information. Then, we propose a framework to iteratively extract small subsets of training data that when augmented, closely capture the alignment of the fully augmented Jacobian with labels/residuals. We prove that stochastic gradient descent applied to the augmented subsets found by our app
    
[^181]: 何时局部查询在鲁棒学习中有用？

    When are Local Queries Useful for Robust Learning?. (arXiv:2210.06089v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.06089](http://arxiv.org/abs/2210.06089)

    本文研究了在鲁棒学习中使用局部查询的实用性，并提出了第一个针对这种鲁棒性概念进行鲁棒经验风险最小化的无分布算法。我们证明，在均匀分布下，局部成员查询不会增加并且不会超过连词和任何超类的鲁棒性阈值。另外，我们引入了局部等价查询oracle，用于在训练样本中判断假设和目标概念在扰动区域内是否一致。

    

    在Gourdeau等人的研究中已经表明，对于考虑精确-球内鲁棒风险和随机示例访问的概念类的鲁棒可学习性，分布假设是必要的。在本文中，我们研究了学习模型，在通过使用局部查询增强学习者的能力，并给出了第一个针对这种鲁棒性概念进行鲁棒经验风险最小化（ERM）的无分布算法。我们考虑的第一个学习模型使用局部成员查询（LMQ），在这种查询中，学习者可以查询接近训练样本的点的标签。我们证明，在均匀分布下，LMQ不会增加并且不会超过连词和任何超类的鲁棒性阈值，例如决策列表和半空间。面对这一负面结果，我们引入了局部等价查询（LEQ）oracle，它返回假设和目标概念在训练样本中的扰动区域内是否一致。

    Distributional assumptions have been shown to be necessary for the robust learnability of concept classes when considering the exact-in-the-ball robust risk and access to random examples by Gourdeau et al. (2019). In this paper, we study learning models where the learner is given more power through the use of local queries, and give the first distribution-free algorithms that perform robust empirical risk minimization (ERM) for this notion of robustness. The first learning model we consider uses local membership queries (LMQ), where the learner can query the label of points near the training sample. We show that, under the uniform distribution, LMQs do not increase the robustness threshold of conjunctions and any superclass, e.g., decision lists and halfspaces. Faced with this negative result, we introduce the local equivalence query ($\mathsf{LEQ}$) oracle, which returns whether the hypothesis and target concept agree in the perturbation region around a point in the training sample, a
    
[^182]: 防御联邦背后攻击的不变聚合器

    Invariant Aggregator for Defending against Federated Backdoor Attacks. (arXiv:2210.01834v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01834](http://arxiv.org/abs/2210.01834)

    该论文针对联邦学习中的背后攻击提出了一种不变聚合器，防御背后攻击并保持模型的整体效用。研究发现在扁平损失空间中，恶意客户端可以通过提供背后样本来误导联邦学习模型，而不需要与良性客户端有明显的差异。

    

    联邦学习因其能够在不直接共享私密数据的情况下训练高效模型而日益受到关注。然而，这种联邦设置使得模型在存在恶意客户端的情况下容易受到各种敌对攻击。尽管对于旨在降低模型效用的攻击的防御已经取得了理论和实证上的成功，但防御仅提高背后样本上模型准确性而不损害其他样本效用的背后攻击仍然具有挑战性。为此，我们首先分析了联邦学习在扁平损失空间上对背后攻击的脆弱性，这种扁平损失空间常见于设计良好的神经网络，如Resnet [He et al., 2015]，但往往被先前的工作所忽视。在扁平损失空间上，误导联邦学习模型以仅对恶意客户端的背后样本有利，并不需要恶意和良性客户端之间存在显著差异。

    Federated learning is gaining popularity as it enables training high-utility models across several clients without directly sharing their private data. As a downside, the federated setting makes the model vulnerable to various adversarial attacks in the presence of malicious clients. Despite the theoretical and empirical success in defending against attacks that aim to degrade models' utility, defense against backdoor attacks that increase model accuracy on backdoor samples exclusively without hurting the utility on other samples remains challenging. To this end, we first analyze the vulnerability of federated learning to backdoor attacks over a flat loss landscape which is common for well-designed neural networks such as Resnet [He et al., 2015] but is often overlooked by previous works. Over a flat loss landscape, misleading federated learning models to exclusively benefit malicious clients with backdoor samples do not require a significant difference between malicious and benign cli
    
[^183]: MetaMask：重新思考自监督学习中的维度干扰问题

    MetaMask: Revisiting Dimensional Confounder for Self-Supervised Learning. (arXiv:2209.07902v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.07902](http://arxiv.org/abs/2209.07902)

    MetaMask是一种通过元学习学习的维度遮罩，用于对抗自监督学习中的维度冗余和干扰问题。

    

    作为自监督学习的成功方法，对比学习旨在学习在输入样本的扭曲之间共享的不变信息。然而，对比学习仍然存在两个持久的缺陷：任务无关信息的干扰和样本效率低下，这与平凡常数解的反复存在相关。从维度分析的角度，我们发现维度冗余和维度干扰是这些现象背后的固有问题，并提供实验证据支持了我们的观点。我们进一步提出了一个简单而有效的方法MetaMask，即通过元学习学习的维度遮罩，以对抗维度冗余和干扰。MetaMask采用冗余减少技术来解决维度冗余问题，并创新地引入了一种维度干扰解决方法。

    As a successful approach to self-supervised learning, contrastive learning aims to learn invariant information shared among distortions of the input sample. While contrastive learning has yielded continuous advancements in sampling strategy and architecture design, it still remains two persistent defects: the interference of task-irrelevant information and sample inefficiency, which are related to the recurring existence of trivial constant solutions. From the perspective of dimensional analysis, we find out that the dimensional redundancy and dimensional confounder are the intrinsic issues behind the phenomena, and provide experimental evidence to support our viewpoint. We further propose a simple yet effective approach MetaMask, short for the dimensional Mask learned by Meta-learning, to learn representations against dimensional redundancy and confounder. MetaMask adopts the redundancy-reduction technique to tackle the dimensional redundancy issue and innovatively introduces a dimens
    
[^184]: 对抗性噪声的友好噪声：针对数据污染攻击的强大防御策略

    Friendly Noise against Adversarial Noise: A Powerful Defense against Data Poisoning Attacks. (arXiv:2208.10224v4 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2208.10224](http://arxiv.org/abs/2208.10224)

    本研究提出了一种简单但非常有效的方法，能够在不影响泛化性能的情况下防御各种类型的不可见污染攻击。我们通过减轻污染引入的尖锐损失区域，生成优化的友好噪声，实现对抗性噪声的防御。

    

    一类强大的（不可见的）数据污染攻击通过微小的对抗扰动修改训练样本的子集，以改变特定测试数据的预测结果。现有的防御机制不适合实际部署，因为它们往往要么严重影响泛化性能，要么是特定攻击的，且应用起来速度过慢。在这里，我们提出了一种简单但非常有效的方法，与现有方法不同，它能在泛化性能稍微下降的情况下防御各种类型的不可见污染攻击。我们观察到攻击会引入局部尖锐的高训练损失区域，当这些区域被最小化时，攻击就会成功。为了防止污染攻击，我们的关键思想是减轻毒素引入的尖锐损失区域。为此，我们的方法包括两个组成部分：优化的友好噪声，用于最大程度地扰动样本。

    A powerful category of (invisible) data poisoning attacks modify a subset of training examples by small adversarial perturbations to change the prediction of certain test-time data. Existing defense mechanisms are not desirable to deploy in practice, as they often either drastically harm the generalization performance, or are attack-specific, and prohibitively slow to apply. Here, we propose a simple but highly effective approach that unlike existing methods breaks various types of invisible poisoning attacks with the slightest drop in the generalization performance. We make the key observation that attacks introduce local sharp regions of high training loss, which when minimized, results in learning the adversarial perturbations and makes the attack successful. To break poisoning attacks, our key idea is to alleviate the sharp loss regions introduced by poisons. To do so, our approach comprises two components: an optimized friendly noise that is generated to maximally perturb examples
    
[^185]: SAFARI：鲁棒性可解释性评估的多功能高效方法

    SAFARI: Versatile and Efficient Evaluations for Robustness of Interpretability. (arXiv:2208.09418v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.09418](http://arxiv.org/abs/2208.09418)

    本文提出了一种名为SAFARI的方法，用于评估深度学习的解释可靠性。该方法针对现有技术无法解决的几个挑战，通过引入两种黑盒评估方法，即最坏情况解释差异和一般情况下的鲁棒性的概率概念，来解决现有度量不全面、XAI技术异质性和误解罕见性等问题。使用遗传算法和子集模拟进行评估。

    

    深度学习的可解释性是建立可信赖的人工智能的一道障碍。尽管可解释人工智能（XAI）社区做出了巨大的努力，但解释缺乏鲁棒性——无法区分的输入扰动可能会导致不同的解释结果。因此，针对给定的XAI方法评估深度学习可解释性的鲁棒性至关重要。本文识别了现有技术无法共同应对的几个挑战：i)现有指标不全面；ii)XAI技术高度异质；iii)误解通常是罕见事件。为了解决这些挑战，我们引入了两种黑盒评估方法，分别涉及最坏情况解释差异和一般情况下的鲁棒性的概率概念。使用具有定制适应度函数的遗传算法（GA）来解决约束优化，以实现高效的最坏情况评估。使用专门用于估计罕见事件概率的子集模拟（SS）来进行整体评估。

    Interpretability of Deep Learning (DL) is a barrier to trustworthy AI. Despite great efforts made by the Explainable AI (XAI) community, explanations lack robustness -- indistinguishable input perturbations may lead to different XAI results. Thus, it is vital to assess how robust DL interpretability is, given an XAI method. In this paper, we identify several challenges that the state-of-the-art is unable to cope with collectively: i) existing metrics are not comprehensive; ii) XAI techniques are highly heterogeneous; iii) misinterpretations are normally rare events. To tackle these challenges, we introduce two black-box evaluation methods, concerning the worst-case interpretation discrepancy and a probabilistic notion of how robust in general, respectively. Genetic Algorithm (GA) with bespoke fitness function is used to solve constrained optimisation for efficient worst-case evaluation. Subset Simulation (SS), dedicated to estimate rare event probabilities, is used for evaluating overa
    
[^186]: 观点市场模型：利用积极干预来遏制极右派观点的传播

    Opinion Market Model: Stemming Far-Right Opinion Spread using Positive Interventions. (arXiv:2208.06620v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2208.06620](http://arxiv.org/abs/2208.06620)

    本研究提出了观点市场模型（OMM），通过引入积极干预来遏制极右派观点的传播。这个模型将观点的关注市场规模建模，并考虑了观点之间的相互作用和竞争，旨在评估积极干预的有效性。

    

    在线极端主义具有严重的社会后果，包括将仇恨言论合理化、用户的激进化以及社会分裂的加剧。已经探索了各种缓解策略来应对这些后果。其中一种策略使用积极干预：控制信号来增加对观点生态系统的关注，以提升某些观点。为了评估积极干预的有效性，我们引入了观点市场模型（OMM），这是一个考虑到观点间相互作用和积极干预作用的两层在线观点生态系统模型。观点的关注市场规模使用多元离散时间Hawkes过程在第一层进行建模；在第二层中，观点在有限的关注度下合作和竞争以获得市场份额，使用市场份额吸引模型。通过合成数据集展示了我们提出的估计方案的收敛性。接下来，我们在两个真实世界的学习任务上测试了OMM

    Online extremism has severe societal consequences, including normalizing hate speech, user radicalization, and increased social divisions. Various mitigation strategies have been explored to address these consequences. One such strategy uses positive interventions: controlled signals that add attention to the opinion ecosystem to boost certain opinions. To evaluate the effectiveness of positive interventions, we introduce the Opinion Market Model (OMM), a two-tier online opinion ecosystem model that considers both inter-opinion interactions and the role of positive interventions. The size of the opinion attention market is modeled in the first tier using the multivariate discrete-time Hawkes process; in the second tier, opinions cooperate and compete for market share, given limited attention using the market share attraction model. We demonstrate the convergence of our proposed estimation scheme on a synthetic dataset. Next, we test OMM on two learning tasks, applying to two real-world
    
[^187]: ForecastTKGQuestions: 一个用于时间性问题回答和时间知识图谱预测的基准测试

    ForecastTKGQuestions: A Benchmark for Temporal Question Answering and Forecasting over Temporal Knowledge Graphs. (arXiv:2208.06501v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2208.06501](http://arxiv.org/abs/2208.06501)

    本论文提出了一个新的任务，即在时间知识图谱上进行预测性问题回答。该任务对于人们寻求未来计划非常重要，并且是先前研究中未被探索的领域。

    

    最近，对于时间性知识图谱问答（TKGQA）的兴趣逐渐增加。TKGQA需要时间推理技术来从时间知识库中提取相关信息。现有的TKGQA数据集只包含基于固定时间段的时间性问题，该时间段内的时间知识图谱（TKG）可以完全用于答案推理，允许TKGQA模型利用未来知识来回答基于过去事实的问题。然而，在现实场景中，鉴于到目前为止的知识，我们也希望TKGQA系统能够回答关于未来的问题。由于人们不断寻求未来的计划，构建能够回答这种预测性问题的TKGQA系统非常重要。然而，在先前的研究中，这一领域仍然未被探索。在本文中，我们提出了一个新的任务：预测性问题回答的时间知识图谱预测。

    Question answering over temporal knowledge graphs (TKGQA) has recently found increasing interest. TKGQA requires temporal reasoning techniques to extract the relevant information from temporal knowledge bases. The only existing TKGQA dataset, i.e., CronQuestions, consists of temporal questions based on the facts from a fixed time period, where a temporal knowledge graph (TKG) spanning the same period can be fully used for answer inference, allowing the TKGQA models to use even the future knowledge to answer the questions based on the past facts. In real-world scenarios, however, it is also common that given the knowledge until now, we wish the TKGQA systems to answer the questions asking about the future. As humans constantly seek plans for the future, building TKGQA systems for answering such forecasting questions is important. Nevertheless, this has still been unexplored in previous research. In this paper, we propose a novel task: forecasting question answering over temporal knowled
    
[^188]: 用神经网络表示随机效用选择模型

    Representing Random Utility Choice Models with Neural Networks. (arXiv:2207.12877v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.12877](http://arxiv.org/abs/2207.12877)

    本论文提出了一种基于神经网络的离散选择模型类，RUMnets，可以近似表示任何随机效用最大化推导出的模型，并且在选择数据上有良好的预测能力。

    

    在深度学习的成功之下，我们提出了一种基于神经网络的离散选择模型类，称为RUMnets，受随机效用最大化（RUM）框架的启发。该模型使用样本平均逼近来构建代理人的随机效用函数。我们证明了RUMnets可以对RUM离散选择模型类进行尖锐逼近：任何从随机效用最大化推导出的模型都可以被RUMnet无限接近地逼近。相反地，任何RUMnet都符合RUM原则。我们得到了在选择数据上拟合的RUMnet的泛化误差的上界，并且根据数据集和架构的关键参数，获得了关于其在新的未知数据上预测选择能力的理论洞见。通过利用神经网络的开源库，我们发现RUMnets在预测准确性方面与几种选择建模和机器学习方法具有竞争力。

    Motivated by the successes of deep learning, we propose a class of neural network-based discrete choice models, called RUMnets, inspired by the random utility maximization (RUM) framework. This model formulates the agents' random utility function using a sample average approximation. We show that RUMnets sharply approximate the class of RUM discrete choice models: any model derived from random utility maximization has choice probabilities that can be approximated arbitrarily closely by a RUMnet. Reciprocally, any RUMnet is consistent with the RUM principle. We derive an upper bound on the generalization error of RUMnets fitted on choice data, and gain theoretical insights on their ability to predict choices on new, unseen data depending on critical parameters of the dataset and architecture. By leveraging open-source libraries for neural networks, we find that RUMnets are competitive against several choice modeling and machine learning methods in terms of predictive accuracy on two rea
    
[^189]: 通过大样本渐近理论优化随机梯度算法的统计推断

    Tuning Stochastic Gradient Algorithms for Statistical Inference via Large-Sample Asymptotics. (arXiv:2207.12395v3 [stat.CO] UPDATED)

    [http://arxiv.org/abs/2207.12395](http://arxiv.org/abs/2207.12395)

    我们利用大样本渐近理论对随机梯度算法进行调优，发现使用固定的大步长进行迭代平均可以鲁棒地优化算法，且具有和MLE抽样分布协方差成比例的鲁棒性。我们还提出了一种类似于Bernstein-von Mises的定理用于指导调优，包括针对模型错误规范鲁棒的广义后验。数值实验验证了我们的结果和建议在实际有限样本情况下的有效性。这些成果为分析其他随机梯度Markov Chain Monte Carlo算法提供了基础。

    

    针对优化和抽样的随机梯度算法（SGA）的调优通常基于试错和启发式方法，而不是可推广的理论。我们通过一种联合步长-样本大小缩放极限来表征SGA的大样本统计渐近性。我们证明了使用固定的大步长进行迭代平均是对调优参数选择鲁棒的，并且在渐近意义下，具有和MLE抽样分布协方差成比例的鲁棒性。我们还证明了一种类似于Bernstein-von Mises的定理以指导调优，包括针对模型错误规范鲁棒的广义后验。数值实验验证了我们在实际有限样本范围内的结果和建议。我们的工作为大范围模型的其他随机梯度Markov Chain Monte Carlo算法的系统分析奠定了基础。

    The tuning of stochastic gradient algorithms (SGAs) for optimization and sampling is often based on heuristics and trial-and-error rather than generalizable theory. We address this theory--practice gap by characterizing the large-sample statistical asymptotics of SGAs via a joint step-size--sample-size scaling limit. We show that iterate averaging with a large fixed step size is robust to the choice of tuning parameters and asymptotically has covariance proportional to that of the MLE sampling distribution. We also prove a Bernstein--von Mises-like theorem to guide tuning, including for generalized posteriors that are robust to model misspecification. Numerical experiments validate our results and recommendations in realistic finite-sample regimes. Our work lays the foundation for a systematic analysis of other stochastic gradient Markov chain Monte Carlo algorithms for a wide range of models.
    
[^190]: 基于在线实验设计的线性MDPs中的依赖于实例的近最优策略识别

    Instance-Dependent Near-Optimal Policy Identification in Linear MDPs via Online Experiment Design. (arXiv:2207.02575v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.02575](http://arxiv.org/abs/2207.02575)

    本文研究了在线实验设计方法在线性MDPs中的应用，提出了一种算法\textsc{Pedel}，该算法在RL中的函数逼近设置下实现了细粒度的依赖于实例的复杂度度量，相对于最低遗憾、最小最大最优算法具有明显收益。

    

    在强化学习中，虽然对于最坏情况实例下的最小最大样本复杂度有了很大的进展，但是这种复杂度衡量往往不能真正反映出学习的真正困难。在实践中，对于一个“简单”的实例，我们可能希望能够实现比最坏实例下更好的复杂度。在本文中，我们希望了解在具有线性函数逼近的强化学习设置中，学习近最优策略（PAC RL）的“依赖于实例”的复杂度。我们提出了一个算法\textsc{Pedel}，它实现了一种细粒度的依赖于实例的复杂度度量，这是在函数逼近设置中首次出现的，从而捕捉了在每个特定问题实例上学习的困难程度。通过一个具体的例子，我们展示了\textsc{Pedel}相对于最低遗憾、最小最大最优算法的可证明收益，并且这些算法无法达到这种效果。

    While much progress has been made in understanding the minimax sample complexity of reinforcement learning (RL) -- the complexity of learning on the "worst-case" instance -- such measures of complexity often do not capture the true difficulty of learning. In practice, on an "easy" instance, we might hope to achieve a complexity far better than that achievable on the worst-case instance. In this work we seek to understand the "instance-dependent" complexity of learning near-optimal policies (PAC RL) in the setting of RL with linear function approximation. We propose an algorithm, \textsc{Pedel}, which achieves a fine-grained instance-dependent measure of complexity, the first of its kind in the RL with function approximation setting, thereby capturing the difficulty of learning on each particular problem instance. Through an explicit example, we show that \textsc{Pedel} yields provable gains over low-regret, minimax-optimal algorithms and that such algorithms are unable to hit the insta
    
[^191]: 用卷积生成对抗网络的数据驱动噪声时间序列建模

    Data-Driven Modeling of Noise Time Series with Convolutional Generative Adversarial Networks. (arXiv:2207.01110v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2207.01110](http://arxiv.org/abs/2207.01110)

    本文通过对噪声时间序列的建模，考察了两种GAN模型用于数据驱动建模的可行性，实验结果表明它们在各类噪声的复制方面具有一定的效果。

    

    物理过程中产生的随机噪声是测量的固有特性，并且是大多数信号处理和数据分析任务的限制因素。鉴于近年来对生成对抗网络(GANs)用于数据驱动建模的兴趣，确定GANs在多大程度上能够忠实地复制目标数据集中的噪声非常重要。本文提出了一项实证研究，旨在为时间序列中的这个问题提供启示。具体而言，我们评估了两种基于流行的深度卷积GAN（DCGAN）结构的通用时间序列GAN，一种是直接的时间序列模型，另一种是基于短时傅里叶变换（STFT）数据表示的图像模型。GAN模型使用已知基础真实参数的模拟噪声时间序列分布进行训练和定量评估。目标时间序列分布涵盖了物理测量、电子学和...

    Random noise arising from physical processes is an inherent characteristic of measurements and a limiting factor for most signal processing and data analysis tasks. Given the recent interest in generative adversarial networks (GANs) for data-driven modeling, it is important to determine to what extent GANs can faithfully reproduce noise in target data sets. In this paper, we present an empirical investigation that aims to shed light on this issue for time series. Namely, we assess two general-purpose GANs for time series that are based on the popular deep convolutional GAN (DCGAN) architecture, a direct time-series model and an image-based model that uses a short-time Fourier transform (STFT) data representation. The GAN models are trained and quantitatively evaluated using distributions of simulated noise time series with known ground-truth parameters. Target time series distributions include a broad range of noise types commonly encountered in physical measurements, electronics, and 
    
[^192]: Pythae：统一的生成自编码器Python库——基准测试用例

    Pythae: Unifying Generative Autoencoders in Python -- A Benchmarking Use Case. (arXiv:2206.08309v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.08309](http://arxiv.org/abs/2206.08309)

    Pythae是一个开源Python库，提供了统一的生成自编码器模型实现和框架，可用于进行多种任务的案例研究基准测试。

    

    近年来，深度生成模型因其对复杂分布的建模能力而引起越来越多的关注。在这些模型中，变分自编码器因其在计算上的高效性和在多个领域中产生令人印象深刻的结果而受到广泛关注。在这一突破之后，为了改进原始论文已进行了广泛的研究工作，导致了各种不同的变分自编码器模型以应对不同的任务。本文介绍了Pythae，一个通用的开源Python库，提供了统一的实现和专门的框架，可方便、可重现、可靠地使用生成自编码器模型。然后，我们提出使用该库进行案例研究基准测试，在其中展示并比较了19个生成自编码器模型，这些模型代表了在图像重构、生成、分类、聚类和插值等下游任务上的一些主要改进。

    In recent years, deep generative models have attracted increasing interest due to their capacity to model complex distributions. Among those models, variational autoencoders have gained popularity as they have proven both to be computationally efficient and yield impressive results in multiple fields. Following this breakthrough, extensive research has been done in order to improve the original publication, resulting in a variety of different VAE models in response to different tasks. In this paper we present Pythae, a versatile open-source Python library providing both a unified implementation and a dedicated framework allowing straightforward, reproducible and reliable use of generative autoencoder models. We then propose to use this library to perform a case study benchmark where we present and compare 19 generative autoencoder models representative of some of the main improvements on downstream tasks such as image reconstruction, generation, classification, clustering and interpola
    
[^193]: 预先训练的感知特征提高差分隐私图像生成的性能

    Pre-trained Perceptual Features Improve Differentially Private Image Generation. (arXiv:2205.12900v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2205.12900](http://arxiv.org/abs/2205.12900)

    该论文提出了一种利用预先训练的感知特征，通过最小化MMD（最大均值差异）来提高差分隐私图像生成的性能，并成功地生成了CIFAR10级别的图像。

    

    使用差分隐私随机梯度下降（DP-SGD）进行中等规模生成模型的训练非常困难：为了保持合理的隐私水平所需的噪声水平过大。相反，我们建议利用信息丰富的公共数据集上的良好相关表征，然后学习使用该表征模型化私有数据。特别的，我们使用从公共数据集中学习的感知特征的核函数，最小化私有目标数据与生成器分布之间的最大均值差异（MMD）。使用MMD，我们可以一次性对数据相关项进行隐私处理，而无需像DP-SGD一样在优化每一步中引入噪声。我们的算法使我们能够生成CIFAR10级别的图像，其 $\epsilon \approx 2$，捕捉了分布中的独特特征，远远超过当前的技术水平，主要集中于数据集，如MNIST和FashionMNIST 以较大的 $\epsilon$。

    Training even moderately-sized generative models with differentially-private stochastic gradient descent (DP-SGD) is difficult: the required level of noise for reasonable levels of privacy is simply too large. We advocate instead building off a good, relevant representation on an informative public dataset, then learning to model the private data with that representation. In particular, we minimize the maximum mean discrepancy (MMD) between private target data and a generator's distribution, using a kernel based on perceptual features learned from a public dataset. With the MMD, we can simply privatize the data-dependent term once and for all, rather than introducing noise at each step of optimization as in DP-SGD. Our algorithm allows us to generate CIFAR10-level images with $\epsilon \approx 2$ which capture distinctive features in the distribution, far surpassing the current state of the art, which mostly focuses on datasets such as MNIST and FashionMNIST at a large $\epsilon \appro
    
[^194]: 为有效和高效的零样本密集检索注入领域适应的学习哈希

    Injecting Domain Adaptation with Learning-to-hash for Effective and Efficient Zero-shot Dense Retrieval. (arXiv:2205.11498v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2205.11498](http://arxiv.org/abs/2205.11498)

    通过学习哈希技术提高零样本密集检索的准确性和效率，克服了存储密集索引的高内存使用问题，并在跨领域环境中进行了评估。

    

    密集检索在无查询词检索中克服了词汇隔阂，并在自动信息检索中取得了巨大成功。尽管成功，但密集检索器在实际应用中的服务成本较高。对于需要从数百万份文档中搜索的用例，密集索引变得庞大，并且在存储索引时需要高内存使用量。最近的学习哈希（LTH）技术，如BPR和JPQ，生成二进制文档向量，从而降低了存储密集索引的内存需求。LTH技术是有监督的，并使用排名损失对检索器进行微调。它们优于传统的向量压缩技术，如PCA或PQ。之前的研究中缺少的一个环节是现有技术仅在领域内进行评估，即仅在单一数据集（如MS MARCO）上进行评估。在我们的工作中，我们评估了LTH和向量压缩技术，以提高TAS-B d的零样本检索准确性。

    Dense retrieval overcome the lexical gap and has shown great success in ad-hoc information retrieval (IR). Despite their success, dense retrievers are expensive to serve across practical use cases. For use cases requiring to search from millions of documents, the dense index becomes bulky and requires high memory usage for storing the index. More recently, learning-to-hash (LTH) techniques, for e.g., BPR and JPQ, produce binary document vectors, thereby reducing the memory requirement to efficiently store the dense index. LTH techniques are supervised and finetune the retriever using a ranking loss. They outperform their counterparts, i.e., traditional out-of-the-box vector compression techniques such as PCA or PQ. A missing piece from prior work is that existing techniques have been evaluated only in-domain, i.e., on a single dataset such as MS MARCO. In our work, we evaluate LTH and vector compression techniques for improving the downstream zero-shot retrieval accuracy of the TAS-B d
    
[^195]: 深度证据回归的不合理有效性

    The Unreasonable Effectiveness of Deep Evidential Regression. (arXiv:2205.10060v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.10060](http://arxiv.org/abs/2205.10060)

    深度证据回归是一种通过学习证据分布来处理不确定性的方法，在不确定性推理中显示出相对于传统方法和贝叶斯神经网络的优势。然而，它并非精确的不确定性量化方法，而是一种启发式方法。

    

    随着机器学习系统在安全关键领域的不断部署，对于基于原则的不确定性推理的需求日益迫切。一种新的不确定性感知回归神经网络（NN）方法，通过学习关于内部变量和外部变量的不确定性的证据分布，显示出相对于传统确定性方法和典型贝叶斯NN的优势，尤其在能够解耦内部和外部不确定性方面。尽管深度证据回归（DER）取得了一定的实证成功，但数学基础存在重要的不足，这引发了为何所提出的技术看似有效的问题。我们详细说明了理论上的不足，并分析了在合成和真实数据集上的性能，表明深度证据回归是一种启发式而非精确的不确定性量化方法。我们继续讨论如何修正和重新定义内部和外部不确定性的提取方法。

    There is a significant need for principled uncertainty reasoning in machine learning systems as they are increasingly deployed in safety-critical domains. A new approach with uncertainty-aware regression-based neural networks (NNs), based on learning evidential distributions for aleatoric and epistemic uncertainties, shows promise over traditional deterministic methods and typical Bayesian NNs, notably with the capabilities to disentangle aleatoric and epistemic uncertainties. Despite some empirical success of Deep Evidential Regression (DER), there are important gaps in the mathematical foundation that raise the question of why the proposed technique seemingly works. We detail the theoretical shortcomings and analyze the performance on synthetic and real-world data sets, showing that Deep Evidential Regression is a heuristic rather than an exact uncertainty quantification. We go on to discuss corrections and redefinitions of how aleatoric and epistemic uncertainties should be extracte
    
[^196]: HDGT: 多智能体轨迹预测的异构驾驶图变换器通过场景编码

    HDGT: Heterogeneous Driving Graph Transformer for Multi-Agent Trajectory Prediction via Scene Encoding. (arXiv:2205.09753v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2205.09753](http://arxiv.org/abs/2205.09753)

    HDGT是一种将驾驶场景建模为异构图的方法，通过场景编码实现多智能体轨迹预测。该方法考虑到了驾驶场景中的不同对象和丰富的语义关系，并使用自我中心的方式进行空间关系编码。

    

    将驾驶场景编码为向量表示是自动驾驶的一个重要任务，可以使得轨迹预测等下游任务受益。驾驶场景通常涉及到不同类型的对象（智能体、车道、交通标志）以及对象之间丰富多样的语义关系。同时，元素之间也存在相对性，这意味着空间关系是一个相对概念，需要以自我中心的方式进行编码，而不是全局坐标系。基于这些观察，我们提出了异构驾驶图变换器（HDGT），将驾驶场景建模为一个具有不同类型节点和边的异构图。在异构图的构建中，我们根据多样的语义关系连接不同类型的节点。在空间关系编码中，节点的坐标以及其入边是在局部节点中心坐标系中表示的。

    Encoding a driving scene into vector representations has been an essential task for autonomous driving that can benefit downstream tasks e.g. trajectory prediction. The driving scene often involves heterogeneous elements such as the different types of objects (agents, lanes, traffic signs) and the semantic relations between objects are rich and diverse. Meanwhile, there also exist relativity across elements, which means that the spatial relation is a relative concept and need be encoded in a ego-centric manner instead of in a global coordinate system. Based on these observations, we propose Heterogeneous Driving Graph Transformer (HDGT), a backbone modelling the driving scene as a heterogeneous graph with different types of nodes and edges. For heterogeneous graph construction, we connect different types of nodes according to diverse semantic relations. For spatial relation encoding, the coordinates of the node as well as its in-edges are in the local node-centric coordinate system. Fo
    
[^197]: Torchhd:一种支持超维计算和向量符号体系研究的开源Python库

    Torchhd: An Open Source Python Library to Support Research on Hyperdimensional Computing and Vector Symbolic Architectures. (arXiv:2205.09208v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.09208](http://arxiv.org/abs/2205.09208)

    Torchhd是一个高性能的开源Python库，旨在支持超维计算和向量符号体系研究。它提供了最先进的功能，易于使用的界面，并且能够使实验运行速度提高多达100倍。

    

    超维计算（HD），也被称为向量符号体系（VSA），是一种利用随机高维向量空间的性质进行分布式表示计算的框架。科学界对于聚集和传播这个特别多学科领域的研究的承诺对于其进步至关重要。为了加入这些努力，我们推出了Torchhd，一个高性能的超维/VSA的开源Python库。Torchhd旨在使超维/VSA更易于使用，并为进一步的研究和应用开发提供高效的基础。这个易于使用的库建立在PyTorch之上，具有最先进的超维/VSA功能，清晰的文档和来自知名出版物的实现示例。将公开可用的代码与其相应的Torchhd实现进行比较，实验可以运行速度提高多达100倍。Torchhd可在以下链接中获取：https://github.com/hyperdimensional-computing/tor

    Hyperdimensional computing (HD), also known as vector symbolic architectures (VSA), is a framework for computing with distributed representations by exploiting properties of random high-dimensional vector spaces. The commitment of the scientific community to aggregate and disseminate research in this particularly multidisciplinary area has been fundamental for its advancement. Joining these efforts, we present Torchhd, a high-performance open source Python library for HD/VSA. Torchhd seeks to make HD/VSA more accessible and serves as an efficient foundation for further research and application development. The easy-to-use library builds on top of PyTorch and features state-of-the-art HD/VSA functionality, clear documentation, and implementation examples from well-known publications. Comparing publicly available code with their corresponding Torchhd implementation shows that experiments can run up to 100x faster. Torchhd is available at: https://github.com/hyperdimensional-computing/tor
    
[^198]: 机器学习方法在结构动力学和振动声学中的应用综述

    A Review of Machine Learning Methods Applied to Structural Dynamics and Vibroacoustic. (arXiv:2204.06362v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.06362](http://arxiv.org/abs/2204.06362)

    本文综述了机器学习方法在结构动力学和振动声学中的应用，包括结构健康监测、主动噪声控制、主动振动控制以及基于机器学习的代理模型。这些应用通过数据揭示洞察力，提高了决策制定和优化产品设计的能力。

    

    机器学习在结构动力学和振动声学领域得到了广泛应用。它通过数据揭示洞察力，结合算法进步和计算能力的提高，增强了决策制定、不确定性处理、模式识别和实时评估的能力。在结构健康监测、主动噪声控制和主动振动控制以及基于机器学习的代理模型方面有很多应用。然而，尽管有许多相关工作，但还没有进行综述和分析。

    The use of Machine Learning (ML) has rapidly spread across several fields, having encountered many applications in Structural Dynamics and Vibroacoustic (SD\&V). The increasing capabilities of ML to unveil insights from data, driven by unprecedented data availability, algorithms advances and computational power, enhance decision making, uncertainty handling, patterns recognition and real-time assessments. Three main applications in SD\&V have taken advantage of these benefits. In Structural Health Monitoring, ML detection and prognosis lead to safe operation and optimized maintenance schedules. System identification and control design are leveraged by ML techniques in Active Noise Control and Active Vibration Control. Finally, the so-called ML-based surrogate models provide fast alternatives to costly simulations, enabling robust and optimized product design. Despite the many works in the area, they have not been reviewed and analyzed. Therefore, to keep track and understand this ongoi
    
[^199]: PGCN：用于时空交通预测的渐进图卷积网络

    PGCN: Progressive Graph Convolutional Networks for Spatial-Temporal Traffic Forecasting. (arXiv:2202.08982v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.08982](http://arxiv.org/abs/2202.08982)

    这项研究提出了一种名为渐进图卷积网络（PGCN）的交通预测框架，通过在训练和测试阶段逐步适应输入数据来构建一组图，以解决交通预测中的复杂时空相关性和数据变化问题。

    

    交通网络中复杂的时空相关性使得交通预测问题具有挑战性。由于交通系统本质上具有图结构，因此已经进行了大量关于图神经网络的研究。最近，构建适应性图对数据进行预测在单一静态图结构上的模型上显示了有希望的结果。然而，图适应性仅在训练阶段应用，并不反映训练阶段使用的数据。这种缺点在交通预测中可能有问题，因为交通数据常常存在时间序列中的意外变化和不规则性。在本研究中，我们提出了一种新的交通预测框架，称为渐进图卷积网络（PGCN）。PGCN通过在训练和测试阶段逐步适应输入数据来构建一组图。具体而言，我们实现了该模型来构建渐进行的邻接矩阵。

    The complex spatial-temporal correlations in transportation networks make the traffic forecasting problem challenging. Since transportation system inherently possesses graph structures, much research efforts have been put with graph neural networks. Recently, constructing adaptive graphs to the data has shown promising results over the models relying on a single static graph structure. However, the graph adaptations are applied during the training phases, and do not reflect the data used during the testing phases. Such shortcomings can be problematic especially in traffic forecasting since the traffic data often suffers from the unexpected changes and irregularities in the time series. In this study, we propose a novel traffic forecasting framework called Progressive Graph Convolutional Network (PGCN). PGCN constructs a set of graphs by progressively adapting to input data during the training and the testing phases. Specifically, we implemented the model to construct progressive adjace
    
[^200]: 序贯核嵌入用于介导和时变剂量响应曲线

    Sequential Kernel Embedding for Mediated and Time-Varying Dose Response Curves. (arXiv:2111.03950v4 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2111.03950](http://arxiv.org/abs/2111.03950)

    本论文提出了一种基于核岭回归的简单非参数估计方法，可以用于估计介导和时变剂量响应曲线。通过引入序贯核嵌入技术，我们实现了对复杂因果估计的简化。通过模拟实验和真实数据的估计结果，证明了该方法的强大性能和普适性。

    

    我们提出了基于核岭回归的介导和时变剂量响应曲线的简单非参数估计器。通过嵌入Pearl的介导公式和Robins的g公式与核函数，我们允许处理、介导者和协变量在一般空间中连续变化，也允许非线性的处理-混淆因素反馈。我们的关键创新是一种称为序贯核嵌入的再生核希尔伯特空间技术，我们使用它来构建复杂因果估计的简单估计器。我们的估计器保留了经典识别的普适性，同时实现了非渐进均匀收敛速度。在具有许多协变量的非线性模拟中，我们展示了强大的性能。我们估计了美国职业训练团的介导和时变剂量响应曲线，并清洁可能成为未来工作基准的数据。我们将我们的结果推广到介导和时变处理效应以及反事实分布，验证了半参数效率。

    We propose simple nonparametric estimators for mediated and time-varying dose response curves based on kernel ridge regression. By embedding Pearl's mediation formula and Robins' g-formula with kernels, we allow treatments, mediators, and covariates to be continuous in general spaces, and also allow for nonlinear treatment-confounder feedback. Our key innovation is a reproducing kernel Hilbert space technique called sequential kernel embedding, which we use to construct simple estimators for complex causal estimands. Our estimators preserve the generality of classic identification while also achieving nonasymptotic uniform rates. In nonlinear simulations with many covariates, we demonstrate strong performance. We estimate mediated and time-varying dose response curves of the US Job Corps, and clean data that may serve as a benchmark in future work. We extend our results to mediated and time-varying treatment effects and counterfactual distributions, verifying semiparametric efficiency 
    
[^201]: 利用注意机制的高阶张量池化在动作识别中的应用

    High-order Tensor Pooling with Attention for Action Recognition. (arXiv:2110.05216v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2110.05216](http://arxiv.org/abs/2110.05216)

    本论文提出了一种利用注意机制进行高阶张量池化的方法，通过引入特征值幂归一化（EPN）来防止爆发现象并提高动作识别的准确性。

    

    本论文提出了一种利用注意机制进行高阶张量池化的方法，旨在捕捉神经网络生成的特征向量的高阶统计信息，形成张量描述符。张量描述符要求具备鲁棒的相似性度量，以应对聚合向量数量较少和爆发现象，即某些特征出现的频率高于或低于统计预期的情况。我们将图拉普拉斯矩阵上的热扩散过程与协方差/自相关矩阵的特征值幂归一化（EPN）密切相关，其逆形成了一个环状图拉普拉斯矩阵。我们证明了热扩散过程与EPN具有相同的作用，即增强或减弱特征值谱的幅度，从而防止爆发现象。我们将高阶张量配备了EPN，它可以作为高阶出现的谱检测器，以防止爆发现象。我们还证明，对于一个由d维特征描述符构建的阶数为r的张量，这样的检测器可以给出至少存在一个高阶出现的可能性。

    We aim at capturing high-order statistics of feature vectors formed by a neural network, and propose end-to-end second- and higher-order pooling to form a tensor descriptor. Tensor descriptors require a robust similarity measure due to low numbers of aggregated vectors and the burstiness phenomenon, when a given feature appears more/less frequently than statistically expected. The Heat Diffusion Process (HDP) on a graph Laplacian is closely related to the Eigenvalue Power Normalization (EPN) of the covariance/auto-correlation matrix, whose inverse forms a loopy graph Laplacian. We show that the HDP and the EPN play the same role, i.e., to boost or dampen the magnitude of the eigenspectrum thus preventing the burstiness. We equip higher-order tensors with EPN which acts as a spectral detector of higher-order occurrences to prevent burstiness. We also prove that for a tensor of order r built from d dimensional feature descriptors, such a detector gives the likelihood if at least one high
    
[^202]: 推荐系统的深度探索

    Deep Exploration for Recommendation Systems. (arXiv:2109.12509v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2109.12509](http://arxiv.org/abs/2109.12509)

    本文提出了一种深度探索方法以解决推荐系统中奖励稀少时的问题，并在高保真度的工业级模拟器下进行了实验，证明了该算法相比现有算法有很大的提升。

    

    现代推荐系统应从延迟反馈中探索和学习。过去的研究往往侧重于从用户对单个推荐的响应中学习。这些工作利用了监督学习和强化学习的方法，但放弃了学习用户之后的行为。在过去的工作中，虽然致力于从随后的行为中学习，但缺乏有效的方法来引导并获取有意义的延迟反馈。当奖励较少时，通过引导探索有意义的延迟反馈变得特别具有挑战性。为了解决这个问题，我们为推荐系统开发了深度探索方法。具体而言，我们将推荐系统形式化为一个序列决策问题，并证明了深度探索方法在单步探索方面的优势。我们的实验是在高保真度的工业级模拟器下进行的，并且证明了该算法相比现有算法有很大的提升。

    Modern recommendation systems ought to benefit by probing for and learning from delayed feedback. Research has tended to focus on learning from a user's response to a single recommendation. Such work, which leverages methods of supervised and bandit learning, forgoes learning from the user's subsequent behavior. Where past work has aimed to learn from subsequent behavior, there has been a lack of effective methods for probing to elicit informative delayed feedback. Effective exploration through probing for delayed feedback becomes particularly challenging when rewards are sparse. To address this, we develop deep exploration methods for recommendation systems. In particular, we formulate recommendation as a sequential decision problem and demonstrate benefits of deep exploration over single-step exploration. Our experiments are carried out with high-fidelity industrial-grade simulators and establish large improvements over existing algorithms.
    
[^203]: 通用背景上下文强化学习模型选择

    Model Selection for Generic Contextual Bandits. (arXiv:2107.03455v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2107.03455](http://arxiv.org/abs/2107.03455)

    我们提出了一种自适应背景上下文强化学习算法（ACB），可以在不知道真实模型类别的情况下进行模型选择，并且具有与已知模型类别算法相匹配的遗憾率。模型选择的代价仅对遗憾上界的二阶项有贡献，且具有直观的属性。

    

    我们考虑在可实现性假设下的通用随机背景上下文强化学习模型选择问题。我们提出了一种名为自适应背景上下文强化学习（ACB）的基于连续精炼的算法，该算法分为多个阶段，逐步消除那些对给定实例来说过于简单的模型类别。我们证明了该算法是自适应的，即在任何可证明的背景上下文强化学习算法（例如\cite{falcon}）的遗憾率与顺序一致匹配，前提是需要知道真实的模型类别。不知道正确的模型类别的代价实际上只是导致遗憾上界中的二阶项的附加项。这个代价具有直观的属性，当模型类别变得更容易识别时，它变小，反之亦然。我们还展示了一种更简单的探索-利用（ETC）风格的算法也能获得类似的遗憾上界，尽管不知道真实的模型类别。然而，模型选择的代价是...

    We consider the problem of model selection for the general stochastic contextual bandits under the realizability assumption. We propose a successive refinement based algorithm called Adaptive Contextual Bandit ({\ttfamily ACB}), that works in phases and successively eliminates model classes that are too simple to fit the given instance. We prove that this algorithm is adaptive, i.e., the regret rate order-wise matches that of any provable contextual bandit algorithm (ex. \cite{falcon}), that needs the knowledge of the true model class. The price of not knowing the correct model class turns out to be only an additive term contributing to the second order term in the regret bound. This cost possess the intuitive property that it becomes smaller as the model class becomes easier to identify, and vice-versa. We also show that a much simpler explore-then-commit (ETC) style algorithm also obtains similar regret bound, despite not knowing the true model class. However, the cost of model selec
    
[^204]: 让循环神经网络热身以最大化可达到的多稳定性极大改善了学习能力

    Warming up recurrent neural networks to maximise reachable multistability greatly improves learning. (arXiv:2106.01001v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.01001](http://arxiv.org/abs/2106.01001)

    本研究发现大多数标准循环神经网络在初始化时只有一个稳定平衡点，并且学习长时间依赖任务通常要求网络具有多稳定性。为了解决这个问题，提出了一种称为“热身”的初始化方法，通过最大化网络可达到的多稳定性来改善学习能力。在多个任务上的实验证明了该方法的有效性。

    

    已知在时间依赖变长时，训练循环神经网络是困难的。本文中，我们展示了大多数标准单元在初始化时只有一个稳定的平衡点，并且学习长时间依赖任务通常发生在网络稳定的平衡点数量增加的时候，这个性质被称为多稳定性。初始为单稳定的网络通常难以达到多稳定性，使得学习输入和输出之间的长时间依赖变得困难。这一观察结果导致了一种新的初始化方法，称为“热身”，用于改善任何循环单元连接的学习能力，以学习任意长的时间依赖。这个初始化过程旨在通过少量梯度步骤内最大化网络可达到的多稳定性，即网络内可以通过相关输入轨迹到达的平衡点的数量。我们在多个信息恢复、序列分类任务上展示了这一方法的效果。

    Training recurrent neural networks is known to be difficult when time dependencies become long. In this work, we show that most standard cells only have one stable equilibrium at initialisation, and that learning on tasks with long time dependencies generally occurs once the number of network stable equilibria increases; a property known as multistability. Multistability is often not easily attained by initially monostable networks, making learning of long time dependencies between inputs and outputs difficult. This insight leads to the design of a novel way to initialise any recurrent cell connectivity through a procedure called "warmup" to improve its capability to learn arbitrarily long time dependencies. This initialisation procedure is designed to maximise network reachable multistability, i.e., the number of equilibria within the network that can be reached through relevant input trajectories, in few gradient steps. We show on several information restitution, sequence classificat
    
[^205]: AirNet：通过空中传输的神经网络

    AirNet: Neural Network Transmission over the Air. (arXiv:2105.11166v6 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2105.11166](http://arxiv.org/abs/2105.11166)

    本论文介绍了AirNet，一种新颖的训练和传输方法，允许在无线信道上高效地传输深度神经网络（DNNs），并且实现了更高的准确性。

    

    对于许多边缘应用来说，深度神经网络（DNNs）实现了最先进的性能。通常情况下，这些DNNs是与位置和时间有关的，并且必须在无线信道上快速而高效地传输。在本文中，我们引入了AirNet，一系列新颖的训练和传输方法，允许在严格的发射功率和延迟约束下，高效地传输DNNs。这对应于一类新的联合源信道编码问题，旨在通过最大化接收器上的准确性来传递DNNs，而不是以高保真度恢复它们。在AirNet中，我们提出将DNN参数直接映射到传输的信道符号，同时训练网络以满足信道约束，并展现对信道噪声的鲁棒性。与基于分离的替代方案相比，AirNet实现了更高的准确性。我们通过对网络进行修剪进一步改善了AirNet的性能。

    State-of-the-art performance for many edge applications is achieved by deep neural networks (DNNs). Often, these DNNs are location- and time-sensitive, and must be delivered over a wireless channel rapidly and efficiently. In this paper, we introduce AirNet, a family of novel training and transmission methods that allow DNNs to be efficiently delivered over wireless channels under stringent transmit power and latency constraints. This corresponds to a new class of joint source-channel coding problems, aimed at delivering DNNs with the goal of maximizing their accuracy at the receiver, rather than recovering them with high fidelity. In AirNet, we propose the direct mapping of the DNN parameters to transmitted channel symbols, while the network is trained to meet the channel constraints, and exhibit robustness against channel noise. AirNet achieves higher accuracy compared to separation-based alternatives. We further improve the performance of AirNet by pruning the network below the avai
    
[^206]: 鲁棒主成分分析：一种中位数求和的方法

    Robust Principal Component Analysis: A Median of Means Approach. (arXiv:2102.03403v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2102.03403](http://arxiv.org/abs/2102.03403)

    本文提出了一种基于中位数求和原则的PCA方法，称为中位数求和主成分分析（MoMPCA），以应对异常值对PCA的影响，并在最小假设条件下实现了最优收敛速度。

    

    主成分分析（PCA）是数据可视化、去噪和降维的基本工具。它在统计学、机器学习、计算机视觉等领域广泛应用。然而，PCA容易受到异常值的影响，往往无法检测到数据集中真实的低维结构。本文提出了一种基于中位数求和原则的PCA方法，称为中位数求和主成分分析（MoMPCA）。该方法不仅在计算上具有吸引力，而且在最小假设条件下实现了最优收敛速度。具体来说，我们通过Rademacher复杂度来探索所得解的非渐近误差界限。

    Principal Component Analysis (PCA) is a fundamental tool for data visualization, denoising, and dimensionality reduction. It is widely popular in Statistics, Machine Learning, Computer Vision, and related fields. However, PCA is well-known to fall prey to outliers and often fails to detect the true underlying low-dimensional structure within the dataset. Following the Median of Means (MoM) philosophy, recent supervised learning methods have shown great success in dealing with outlying observations without much compromise to their large sample theoretical properties. This paper proposes a PCA procedure based on the MoM principle. Called the \textbf{M}edian of \textbf{M}eans \textbf{P}rincipal \textbf{C}omponent \textbf{A}nalysis (MoMPCA), the proposed method is not only computationally appealing but also achieves optimal convergence rates under minimal assumptions. In particular, we explore the non-asymptotic error bounds of the obtained solution via the aid of the Rademacher complexiti
    
[^207]: 感知器理论可以预测神经网络的准确性

    Perceptron Theory Can Predict the Accuracy of Neural Networks. (arXiv:2012.07881v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2012.07881](http://arxiv.org/abs/2012.07881)

    感知器理论可以通过统计方法准确预测不同结构的神经网络的性能。

    

    多层神经网络在许多技术分类问题上取得了当前的最新成果。然而，从分析和预测性能角度来看，这些网络仍然是黑箱。在这里，我们开发了一个针对单层感知器的统计理论，并展示它可以预测多种不同结构的神经网络的性能。通过推广用于分析储备计算模型和符号推理的连接主义模型的向量符号体系结构的现有理论，我们发展了感知器分类的一般理论。我们的统计理论提供了三个公式，通过逐步增加详细信息来利用信号统计。这些公式在解析上是难以处理的，但可以通过数值评估。捕捉最详细信息的描述级别需要随机抽样方法。根据网络模型的不同，较简单的公式已经能够提供高度准确的预测。

    Multilayer neural networks set the current state of the art for many technical classification problems. But, these networks are still, essentially, black boxes in terms of analyzing them and predicting their performance. Here, we develop a statistical theory for the one-layer perceptron and show that it can predict performances of a surprisingly large variety of neural networks with different architectures. A general theory of classification with perceptrons is developed by generalizing an existing theory for analyzing reservoir computing models and connectionist models for symbolic reasoning known as vector symbolic architectures. Our statistical theory offers three formulas leveraging the signal statistics with increasing detail. The formulas are analytically intractable, but can be evaluated numerically. The description level that captures maximum details requires stochastic sampling methods. Depending on the network model, the simpler formulas already yield high prediction accuracy
    
[^208]: 隐式多维投影局部子空间的可视化方法

    Implicit Multidimensional Projection of Local Subspaces. (arXiv:2009.03259v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2009.03259](http://arxiv.org/abs/2009.03259)

    本研究提出了一种使用隐式函数导数的可视化方法，旨在理解多维投影对局部子空间的影响。通过分析局部子空间的形状和方向信息，我们能够获取更多关于数据全局结构的洞察力，并将结果可视化为图形进行分析。

    

    我们提出了一种使用隐式函数导数的可视化方法，来理解多维投影对局部子空间的影响。在这里，我们将局部子空间理解为数据点的多维局部邻域。现有的方法主要关注多维数据点的投影，忽略了邻域信息。我们的方法能够分析局部子空间的形状和方向信息，通过感知局部结构来获取更多有关数据全局结构的洞察力。局部子空间通过由基向量张成的多维椭圆拟合。我们提出了一种准确高效的向量转换方法，基于将多维投影表示为隐式函数的解析微分。结果以图形的形式进行可视化，并利用一套完整的专门设计的交互操作，在我们高效的基于Web的可视化工具中进行分析。

    We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness o
    
[^209]: 如何选择最合适的中心度测量方法？一种决策树的方法。

    How to choose the most appropriate centrality measure? A decision tree approach. (arXiv:2003.01052v5 [physics.soc-ph] UPDATED)

    [http://arxiv.org/abs/2003.01052](http://arxiv.org/abs/2003.01052)

    这篇论文提出了一种利用决策树方法选择合适中心度度量方法的缩减方法。通过构建决策树调查，结合专家偏好，能够在较小的图形数量下快速筛选出最合适的中心度度量方法。

    

    中心度度量在网络分析中至关重要，但在400多种提出的指标中选择最合适的测量方法仍然具有挑战性。现有方法-基于模型、数据驱动和公理性-存在局限性。为了解决这个问题，我们引入了缩减方法，利用专家对简单图中的中心度行为的偏好。它涉及形成一组候选测量方法，生成尽可能小的图形以“分离”各种测量方法，构建决策树调查，并确定与专家回答一致的测量方法。我们将这种方法应用于包括新的基于核的测量方法在内的40种不同中心度，同时与公理性方法结合。值得注意的是，只有13个小型1-树足以分离所有40个测量方法，其中包括接近的一对。缩减方法在劳动力和时间方面提供了一种低成本的解决方案，为已有的测量方法提供了补充。

    Centrality metrics are vital for network analysis, but selecting the most appropriate measures for specific applications remains challenging among the 400+ proposed indices. Existing approaches -- model-based, data-driven, and axiomatic -- have limitations. To address this, we introduce the culling method, leveraging expert preferences regarding centrality behavior on simple graphs. It involves forming a set of candidate measures, generating a list of as small graphs as possible needed to ``separate'' measures from each other, constructing a decision-tree survey, and identifying the measure consistent with expert responses. We apply this method to a diverse set of 40 centralities, including new kernel-based measures, and combine it with the axiomatic approach. Remarkably, only 13 small 1-trees suffice to separate all 40 measures, among which there are pairs of close ones. The culling method offers a low-cost solution in terms of labor and time, complements existing methods for measure 
    

