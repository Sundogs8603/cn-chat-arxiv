# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [kNN Algorithm for Conditional Mean and Variance Estimation with Automated Uncertainty Quantification and Variable Selection](https://rss.arxiv.org/abs/2402.01635) | 本文介绍了一种利用kNN算法进行条件均值和方差估计的方法，该方法采用了自动不确定性量化和变量选择技术，提高了估计的准确性和性能。 |
| [^2] | [Beyond Lengthscales: No-regret Bayesian Optimisation With Unknown Hyperparameters Of Any Type](https://rss.arxiv.org/abs/2402.01632) | 这篇论文提出了一种新的贝叶斯优化算法，可以处理具有任意类型未知超参数的情况，并具有无遗憾特性。 |
| [^3] | [Position Paper: Generalized grammar rules and structure-based generalization beyond classical equivariance for lexical tasks and transduction](https://rss.arxiv.org/abs/2402.01629) | 这篇论文提出了一个通用框架，其中使用广义语法规则（GGRs）来实现组合一般化，将其视为转导任务中的对称性约束。该框架不仅形式化了语言转导的广义对称性概念，还与强化学习和其他研究领域有关联。 |
| [^4] | [Stochastic Two Points Method for Deep Model Zeroth-order Optimization](https://rss.arxiv.org/abs/2402.01621) | 本文介绍了一种针对大型深度模型的零阶优化方法——随机两点法，通过前向传递来更新模型。并且通过理论分析和实验证明了其在优化目标上的高效性并超越其他方法。 |
| [^5] | [A GP-based Robust Motion Planning Framework for Agile Autonomous Robot Navigation and Recovery in Unknown Environments](https://rss.arxiv.org/abs/2402.01617) | 这篇论文提出了一种基于高斯过程的鲁棒运动规划框架，用于敏捷自主机器人在未知环境中的导航和恢复。该框架能够主动预测规划失败的风险并进行恢复，同时能在仿真和真实环境中产生灵活的运动。 |
| [^6] | [L2G2G: a Scalable Local-to-Global Network Embedding with Graph Autoencoders](https://rss.arxiv.org/abs/2402.01614) | L2G2G是一种可扩展的局部到全局网络嵌入方法，通过动态同步潜在节点表示以提高GAE的准确性，并利用解码器计算只有本地图块损失，从而更好地利用了图的信息。 |
| [^7] | [Contingency Analysis of a Grid of Connected EVs for Primary Frequency Control of an Industrial Microgrid Using Efficient Control Scheme](https://rss.arxiv.org/abs/2402.01608) | 本研究提出了一种用于工业微电网的高效控制方案，通过连接的电动汽车网格实现主频率控制和拥塞管理。这项技术通过将电动汽车用作负荷和源，利用其快速调节能力来提高电网可靠性。 |
| [^8] | [Natural Counterfactuals With Necessary Backtracking](https://rss.arxiv.org/abs/2402.01607) | 本研究提出了一种自然反事实框架和方法，通过优化控制回溯的范围，生成与实际世界的数据分布相匹配的自然反事实，从而改进了反事实推理。 |
| [^9] | [Learning from Two Decades of Blood Pressure Data: Demography-Specific Patterns Across 75 Million Patient Encounters](https://rss.arxiv.org/abs/2402.01598) | 这项研究通过分析两个十年的庞大数据集，发现基于性别的血压变化不显著，挑战了传统假设；同时，舒张压随着年龄增长而持续增加，而收缩压在四十多岁的年龄组显示出一个独特的峰值。 |
| [^10] | [TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution](https://rss.arxiv.org/abs/2402.01586) | 本文介绍了一种基于代理构成的代理框架TrustAgent，该框架通过预先规划、规划过程中和计划后检查三种策略来提高LLM代理的安全性。实验结果表明，这些方法可以有效识别和预防潜在危险。此外，还研究了安全性与使用者满意度以及模型推理能力与效率之间的关系。 |
| [^11] | [Spiking Music: Audio Compression with Event Based Auto-encoders](https://rss.arxiv.org/abs/2402.01571) | 本研究利用基于事件的自编码器实现了音频压缩，并证明了其在稀疏状态下具有较高效率和自主产生的选择性和同步。 |
| [^12] | [Understanding Adam Optimizer via Online Learning of Updates: Adam is FTRL in Disguise](https://rss.arxiv.org/abs/2402.01567) | 本文通过在线学习的视角，揭示了Adam优化器的构成和算法组成的重要性，发现Adam实际上是伪装成FTRL的，研究了在线学习的角度对其算法组成的好处。 |
| [^13] | [Privacy-Preserving Distributed Learning for Residential Short-Term Load Forecasting](https://rss.arxiv.org/abs/2402.01546) | 该论文提出了一种面向住宅短期负荷预测的隐私保护分布式学习方法，通过采用安全聚合算法和多方计算密码学技术来减轻梯度泄漏的风险，并解决了现有联邦学习模型的脆弱性和对新兴攻击技术的容易受到的问题。 |
| [^14] | [Adaptive Optimization for Prediction with Missing Data](https://rss.arxiv.org/abs/2402.01543) | 本文提出了一种针对缺失数据预测的自适应优化方法，通过自适应线性回归模型来适应观测特征集，并将填充规则和回归模型同时学习，相比顺序学习方法，在数据非完全随机缺失情况下，方法实现了2-10%的准确性改进。 |
| [^15] | [Learning Collective Variables for Protein Folding with Labeled Data Augmentation through Geodesic Interpolation](https://rss.arxiv.org/abs/2402.01542) | 本实验提出了一种使用标记数据增强和测地插值方法学习蛋白质折叠的集体变量的策略，有效提高了采样效率，并在过渡态数据有限且嘈杂时表现优于基于分类器的方法。 |
| [^16] | [Closing the Gap in Human Behavior Analysis: A Pipeline for Synthesizing Trimodal Data](https://rss.arxiv.org/abs/2402.01537) | 本研究提出了一种新颖的生成技术，可用于创建通过综合RGB、热像和深度三个模态的人类行为分析数据集，弥补了现有HBA数据集中缺乏整合多模态的问题。 |
| [^17] | [Decoding Speculative Decoding](https://rss.arxiv.org/abs/2402.01528) | 推测解码是一种用于加速大型语言模型推断的技术，但我们的实验表明，选择的草稿模型生成的令牌被目标模型接受的概率越高，吞吐量越低。我们通过大量实验，分析了各种因素对推测解码效果的影响，并提出了一个分析模型来提高效率。 |
| [^18] | [HyperPlanes: Hypernetwork Approach to Rapid NeRF Adaptation](https://rss.arxiv.org/abs/2402.01524) | 本论文提出了一种基于超网络的少样本学习方法，利用其在训练过程中生成通用权重的更新，实现了从少量图像中快速生成高质量三维物体表示的效果。 |
| [^19] | [Low-Resource Cross-Domain Singing Voice Synthesis via Reduced Self-Supervised Speech Representations](https://rss.arxiv.org/abs/2402.01520) | 提出了一种低资源跨领域歌声合成模型Karaoker-SSL，通过减少自监督语音表示和引入Conformer模块来实现无需使用歌唱数据和手工特征的合成过程。 |
| [^20] | [Enhancing Stochastic Gradient Descent: A Unified Framework and Novel Acceleration Methods for Faster Convergence](https://rss.arxiv.org/abs/2402.01515) | 本文提出了一个统一框架来解决随机优化中的收敛问题，并提出了两种即插即用的加速方法，在理论上证明了这些方法可以加快收敛速度。 |
| [^21] | [Mapping the Multiverse of Latent Representations](https://rss.arxiv.org/abs/2402.01514) | 提出了一种名为PRESTO的框架，用于映射依赖于潜在表示的机器学习模型的多元宇宙。该框架使用持续同调来测量潜在空间的差异，并统计推理它们的分布。可以用于敏感性分析、检测异常嵌入和高效导航超参。 |
| [^22] | [Advancing Brain Tumor Inpainting with Generative Models](https://rss.arxiv.org/abs/2402.01509) | 本研究使用生成模型进行脑肿瘤的修补，通过合成健康脑扫描解决了常规算法的局限性。我们特别针对MRI的需求进行了相关修改，并通过评估多种修补技术的效果和局限性进行了验证。 |
| [^23] | [Why do Random Forests Work? Understanding Tree Ensembles as Self-Regularizing Adaptive Smoothers](https://rss.arxiv.org/abs/2402.01502) | 本文将树集成解释为自适应的自正则化平滑器，通过量化预测的平滑程度并根据测试和训练输入的差异调节平滑性，提供了对树集成成功驱动因素的新见解。 |
| [^24] | [Sliced-Wasserstein Estimation with Spherical Harmonics as Control Variates](https://rss.arxiv.org/abs/2402.01493) | 这种方法提出了一种新的蒙特卡罗方法，使用球谐函数作为控制变量来近似计算切片华瑟斯坦距离。与蒙特卡罗相比，该方法具有更好的收敛速度和理论性质。 |
| [^25] | [Connecting the Dots: Is Mode-Connectedness the Key to Feasible Sample-Based Inference in Bayesian Neural Networks?](https://rss.arxiv.org/abs/2402.01484) | 通过揭示权重和函数空间之间的关系，我们成功实现了贝叶斯神经网络的可行的基于样本推理，并提出了一种有效的贝叶斯深度集成方法来解决采样和收敛问题。 |
| [^26] | [Multi-level protein pre-training with Vabs-Net](https://rss.arxiv.org/abs/2402.01481) | 这篇论文介绍了一种使用Vabs-Net进行多级蛋白质预训练的方法。当前大多数基于结构的预训练模型仅关注残基水平，但忽略了侧链原子的重要性。为了解决这个问题，论文提出了一种新的预训练策略，引入了跨度掩码，以在三维蛋白质预训练中同时建模残基和原子水平的信息，并改善了残基表示的表达能力。 |
| [^27] | [Self-Attention through Kernel-Eigen Pair Sparse Variational Gaussian Processes](https://rss.arxiv.org/abs/2402.01476) | 本论文提出了自核-特征对稀疏变分高斯过程（KEP-SVGP）用于构建具有不确定性感知的自注意力。通过核SVD（KSVD）解决了注意力核的不对称性，并实现了降低的复杂度。 |
| [^28] | [Deep Conditional Generative Learning: Model and Error Analysis](https://rss.arxiv.org/abs/2402.01460) | 提出了一种基于ODE的深度生成方法，通过条件Follmer流来学习条件分布，通过离散化和深度神经网络实现高效转化。同时，通过Wasserstein距离的非渐近收敛速率，提供了第一个端到端误差分析，数值实验证明其在不同场景下的优越性。 |
| [^29] | [Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach](https://rss.arxiv.org/abs/2402.01454) | 本文提出了一种在因果发现中集成大型语言模型的方法，通过将统计因果提示与知识增强相结合，可以使统计因果发现结果接近真实情况并进一步改进结果。 |
| [^30] | [Improving importance estimation in covariate shift for providing accurate prediction error](https://rss.arxiv.org/abs/2402.01450) | 该论文研究了在协变量偏移问题中改进重要性估计以提高预测误差的准确性。 |
| [^31] | [Mission Critical -- Satellite Data is a Distinct Modality in Machine Learning](https://rss.arxiv.org/abs/2402.01444) | 卫星数据是机器学习中的独特模态，我们需要重新思考现有的实践并发起一个以卫星数据的特征和挑战为中心的新的研究议程，以推动SatML的质量和影响力。 |
| [^32] | [Learning the Market: Sentiment-Based Ensemble Trading Agents](https://rss.arxiv.org/abs/2402.01441) | 该论文提出了将情感分析和深度强化学习集成算法应用于股票交易的方法，设计了可以根据市场情绪动态调整的交易策略。实验结果表明，这种方法比传统的集成策略、单一智能体算法和市场指标更具盈利性、稳健性和风险最小化。相关研究还发现，传统的固定更换集成智能体的做法并不是最优的，而基于情感的动态框架可以显著提高交易智能体的性能。 |
| [^33] | [Few-Shot Learning on Graphs: from Meta-learning to Pre-training and Prompting](https://rss.arxiv.org/abs/2402.01440) | 本文综述了图上的小样本学习的最新发展，将现有的研究方法划分为元学习、预训练和混合方法三大类别，并对它们的优缺点进行了比较。还提出了未来的研究方向。 |
| [^34] | [From Words to Molecules: A Survey of Large Language Models in Chemistry](https://rss.arxiv.org/abs/2402.01439) | 本论文调查了大型语言模型在化学领域的应用。研究内容涵盖了分子信息输入LLMs的表示和标记化方法、化学LLMs的不同组群及其整合方法、适用于化学LLMs的预训练目标等。此外，还探讨了LLMs在化学中的各种应用。 |
| [^35] | [Conditioning non-linear and infinite-dimensional diffusion processes](https://rss.arxiv.org/abs/2402.01434) | 本文探索了在无穷维空间中对非线性过程进行条件约束的方法，并应用于进化生物学中的生物形态时间序列分析。 |
| [^36] | [Approximate Control for Continuous-Time POMDPs](https://rss.arxiv.org/abs/2402.01431) | 本文提出了一个适用于连续时间部分可观察系统的决策框架，通过近似方法实现了针对大规模状态空间的过滤和控制问题的可扩展解决方案。 |
| [^37] | [A Data-Driven Analysis of Robust Automatic Piano Transcription](https://rss.arxiv.org/abs/2402.01424) | 本研究从训练数据的角度研究了自动钢琴转录系统，发现现有模型在训练数据的声学特性方面严重过拟合。通过创建新的数据集并采用数据增强技术，我们在没有查看训练数据的情况下实现了88.4的F1得分的最新音符起始准确性。 |
| [^38] | [Sequence Shortening for Context-Aware Machine Translation](https://rss.arxiv.org/abs/2402.01416) | 本研究展示了上下文感知机器翻译的序列缩短方法，通过重用上下文信息可以提高翻译准确性，在对比数据集上表现良好，并且引入了新的潜在分组和潜在选择方法来进一步提高翻译质量。 |
| [^39] | [SMLP: Symbolic Machine Learning Prover](https://rss.arxiv.org/abs/2402.01415) | SMLP是一种基于数据样本的系统探索工具，通过采用统计方法和机器学习模型相结合的灰盒方法，可以探索系统并优化硬件设计。 |
| [^40] | [Objective and subjective evaluation of speech enhancement methods in the UDASE task of the 7th CHiME challenge](https://rss.arxiv.org/abs/2402.01413) | 本文介绍了第七届CHiME挑战赛的UDASE任务中系统的客观和主观评估，并分析了结果 |
| [^41] | [Bass Accompaniment Generation via Latent Diffusion](https://rss.arxiv.org/abs/2402.01412) | 本论文提出了一种通过潜在扩散技术生成低音伴奏的方法。通过音频自动编码器将音频样本压缩成潜在表示，并结合条件潜在扩散模型生成对应的音轨。通过引入参考风格和调整引导方法，提供了对生成样本音色的控制和进一步提高音频质量。定量实验证明了该方法的有效性。 |
| [^42] | [XAI for Skin Cancer Detection with Prototypes and Non-Expert Supervision](https://rss.arxiv.org/abs/2402.01410) | 本研究提出了一种用于黑素瘤诊断的新方法，通过使用可解释的原型-部分模型以及结合基于非专家反馈的引导性监督，实现了优于不可解释的模型的性能和泛化能力。 |
| [^43] | [Climbing the Ladder of Interpretability with Counterfactual Concept Bottleneck Models](https://rss.arxiv.org/abs/2402.01408) | 本论文提出了一种新的模型 CF-CBMs，可以同时解决深度学习模型的预测、解释和想象能力的不足，为部署可靠的AI代理、校准人类信任和加深人机交互提供了一种有效的解决方法。 |
| [^44] | [Zero-Shot Machine Unlearning at Scale via Lipschitz Regularization](https://rss.arxiv.org/abs/2402.01401) | 通过Lipschitz正则化实现零样本机器遗忘，可以及时忘记私人或受版权保护的信息，同时保持模型性能。 |
| [^45] | [Query-Efficient Correlation Clustering with Noisy Oracle](https://rss.arxiv.org/abs/2402.01400) | 本论文提出了一种低查询成本的聚类方法，利用纯在组合多臂赌博机探索范式实现在线学习，并设计了能在NP-hard情况下运行的多项式时间算法。 |
| [^46] | [A Probabilistic Model to explain Self-Supervised Representation Learning](https://rss.arxiv.org/abs/2402.01399) | 该论文提出了一个概率模型来解释自监督表示学习的机制，并展示了鉴别性自监督算法在表示中近似诱导潜变量结构的统一理论框架。 |
| [^47] | [ALERT-Transformer: Bridging Asynchronous and Synchronous Machine Learning for Real-Time Event-based Spatio-Temporal Data](https://rss.arxiv.org/abs/2402.01393) | ALERT-Transformer是一种将异步感知与同步处理相结合的新颖桥接方式，通过ALERT模块、灵活的数据读取和基于块的稀疏性优化，实现了对实时事件驱动时空数据的经典处理，其性能超过竞争对手并具有较低的延迟。 |
| [^48] | [Emergence of heavy tails in homogenized stochastic gradient descent](https://rss.arxiv.org/abs/2402.01382) | 这项研究分析了齐次化随机梯度下降算法，在数值实验中验证了参数尾指数的明确上下界，并量化了优化参数与尾指数之间的相互作用，从而为重尾和神经网络的泛化性能以及SGD避免次优局部最小值能力之间的关系提供了贡献。 |
| [^49] | [Regularized boosting with an increasing coefficient magnitude stop criterion as meta-learner in hyperparameter optimization stacking ensemble](https://rss.arxiv.org/abs/2402.01379) | 本论文提出了一种使用增大系数幅度停止准则的正则化增强作为超参数优化集成中的元学习器的方法。 |
| [^50] | [LoTR: Low Tensor Rank Weight Adaptation](https://rss.arxiv.org/abs/2402.01376) | LoTR是一种新颖的语言模型参数调优方法，通过引入低秩张量表示和张量分解，使得针对深层模型的参数效率更高，并且能够实现廉价且快速的下游调优。 |
| [^51] | [Critic-Actor for Average Reward MDPs with Function Approximation: A Finite-Time Analysis](https://rss.arxiv.org/abs/2402.01371) | 本论文提出了一个评论家-演员算法，解决了长期平均奖励设置中的函数逼近问题，并进行了有限时间分析。实验结果表明，我们的算法能够在评论家的均方误差上界为$\epsilon$的情况下，获得样本复杂度为$\mathcal{\tilde{O}}(\epsilon^{-2.08})$，优于演员-评论家算法的结果。 |
| [^52] | [Cheating Suffix: Targeted Attack to Text-To-Image Diffusion Models with Multi-Modal Priors](https://rss.arxiv.org/abs/2402.01369) | 本文提出了一种名为MMP-Attack的有针对性攻击方法，通过整合文本和图像特征，该方法能够有效地攻击商业文本到图像模型，并且具有更高的普适性和可转移性。 |
| [^53] | [Continual Learning for Large Language Models: A Survey](https://rss.arxiv.org/abs/2402.01364) | 这篇综述文章调查了大型语言模型(LLMs)的持续学习，使用了一种新颖的多阶段分类方案对持续学习技术进行了分类，指出了该领域面临的挑战和未来工作方向。 |
| [^54] | [To the Max: Reinventing Reward in Reinforcement Learning](https://rss.arxiv.org/abs/2402.01361) | 本研究通过引入最大奖励强化学习的方法，提出了一种替代传统奖励函数的学习方式，并在实验中证明了其在不同环境下的性能优势。 |
| [^55] | [TESSERACT: Eliminating Experimental Bias in Malware Classification across Space and Time (Extended Version)](https://rss.arxiv.org/abs/2402.01359) | 本文提出TESSERACT方法，消除了恶意软件分类中的实验偏差，解决了常见的空间和时间偏差问题。通过公平实验设计约束和新指标AUT实现了更准确和稳定的分类器。 |
| [^56] | [Efficient compilation of expressive problem space specifications to neural network solvers](https://rss.arxiv.org/abs/2402.01353) | 本文提出了一种算法，可以将高级的问题空间规范编译为适合神经网络求解器的满足性查询，以解决神经网络验证中存在的嵌入间隙问题。 |
| [^57] | [FedMoE: Data-Level Personalization with Mixture of Experts for Model-Heterogeneous Personalized Federated Learning](https://rss.arxiv.org/abs/2402.01350) | FedMoE是一种模型异构的个性化联邦学习算法，通过使用专家混合模型增强大型语言模型，将共享的小特征提取器和本地门控网络分配给每个客户端的本地异构大模型，以解决当前模型异构个性化联邦学习方法中存在的隐私、性能、通信和计算成本等问题。 |
| [^58] | [CORE: Mitigating Catastrophic Forgetting in Continual Learning through Cognitive Replay](https://rss.arxiv.org/abs/2402.01348) | 本文通过COgnitive REplay（CORE）提出了一种在连续学习中减轻灾难性遗忘的新方法，通过自适应数量分配和以质量为重点的数据选择来优化重播缓冲区，取得了显著的准确率提高效果。 |
| [^59] | [Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models](https://rss.arxiv.org/abs/2402.01345) | 本文提出了一种新的视角，指出LVLMs中固有的偏见可能是多模态幻觉的关键因素。通过系统识别与段落分割符相关的语义漂移偏差，我们发现模型在训练数据中经常遇到明显的内容语义变化，导致幻觉的产生。 |
| [^60] | [Monotone, Bi-Lipschitz, and Polyak-\L{}ojasiewicz Networks](https://rss.arxiv.org/abs/2402.01344) | 这篇论文介绍了一种新的可逆神经网络BiLipNet，它具有调控输出敏感性和输入可区分性的能力。其中的主要创新是通过认证的强单调性和Lipschitz性的可逆残差层，与正交层组合构建了Bi-Lipschitz网络。另外，该论文还提出了满足Polyak-\L{}ojasiewicz条件的PLNet，并介绍了其应用于学习非凸代理损失的优势特性。 |
| [^61] | [Shapelet-based Model-agnostic Counterfactual Local Explanations for Time Series Classification](https://rss.arxiv.org/abs/2402.01343) | 提出了一种基于形态特征和模型无关的后期解释方法，用于时间序列分类。该方法利用形态特征和TimeGAN生成可反事实解释，与最先进的方法相比，在解释性方面表现更好。 |
| [^62] | [Training-time Neuron Alignment through Permutation Subspace for Improving Linear Mode Connectivity and Model Fusion](https://rss.arxiv.org/abs/2402.01342) | 本文提出了一个在训练过程中进行神经元对齐的方法，通过置换子空间减少了线性模块连通性的局限性，为模型融合算法的改进提供了可能性。 |
| [^63] | [Fundamental Properties of Causal Entropy and Information Gain](https://rss.arxiv.org/abs/2402.01341) | 本研究通过建立和分析因果熵和因果信息增益的基本性质，包括界限和链规则，阐明了因果熵与随机干预的关系，并提出了因果条件熵和因果条件信息增益的定义，为提升因果机器学习任务铺平了道路。 |
| [^64] | [SignSGD with Federated Defense: Harnessing Adversarial Attacks through Gradient Sign Decoding](https://rss.arxiv.org/abs/2402.01340) | 本研究提出了一种名为SignSGD-FD的技术，在对抗性工作节点增加时保持了收敛率不变，利用了通过梯度符号解码获得的对抗性工作节点的梯度信息。实验证明了该技术的有效性。 |
| [^65] | [Inferring the Langevin Equation with Uncertainty via Bayesian Neural Networks](https://rss.arxiv.org/abs/2402.01338) | 本研究提出了一个使用贝叶斯神经网络推断带有不确定性的朗之万方程的综合框架，可以应用于非线性和高维系统，通过提供预测的分布从而评估预测的不确定性。 |
| [^66] | [Supervised Algorithmic Fairness in Distribution Shifts: A Survey](https://rss.arxiv.org/abs/2402.01327) | 这篇综述研究了分布变化下的监督公平机器学习领域，调查了各种类型的分布变化和现有的解决方法，并列举了公开数据集和评估指标。研究发现六种常用的方法，并探讨了与相关研究领域的交互关系。 |
| [^67] | [KTO: Model Alignment as Prospect Theoretic Optimization](https://rss.arxiv.org/abs/2402.01306) | 本文提出了一种名为KTO的方法，将模型对齐视为展望理论优化。与当前方法相比，KTO直接最大化生成效用而不是最大化偏好对数似然。在多个规模上，KTO的性能与基于偏好的方法相当甚至更好。 |
| [^68] | [A Unified Framework for Gradient-based Clustering of Distributed Data](https://rss.arxiv.org/abs/2402.01302) | 这是一篇关于梯度聚类分布式数据的统一框架的论文，作者提出了一族分布式聚类算法，可以在用户网络中工作。通过控制用户中心估计的接近程度和定义聚类损失函数，这些算法适用于不同的聚类任务。在提供了统一分析和几个强结果的基础上，这些算法都表现出了良好的收敛性和可行性。 |
| [^69] | [Characterizing Overfitting in Kernel Ridgeless Regression Through the Eigenspectrum](https://rss.arxiv.org/abs/2402.01297) | 我们通过推导核矩阵的特征数界限，增强了核岭回归的测试误差界限。对于多项式谱衰减的核，我们恢复了先前的结果；对于指数谱衰减，我们提出了新的非平凡的界限。我们的研究表明，特征谱衰减多项式的核回归器具有良好的泛化能力，而特征谱指数衰减的核回归器则具有灾难性的过拟合。 |
| [^70] | [Bi-CryptoNets: Leveraging Different-Level Privacy for Encrypted Inference](https://rss.arxiv.org/abs/2402.01296) | 本文介绍了一种利用不同级别的隐私保护进行加密推断的方法，通过将输入数据划分为敏感和不敏感的部分，并分别处理，同时利用知识蒸馏进行训练。 |
| [^71] | [ExtremeCast: Boosting Extreme Value Prediction for Global Weather Forecast](https://rss.arxiv.org/abs/2402.01295) | ExtremeCast提出了一种新的损失函数Exloss，实现了针对极值的准确预测，同时引入了无需训练的极值增强策略ExEnsemble，提高了预报的稳健性 |
| [^72] | [Can MLLMs Perform Text-to-Image In-Context Learning?](https://rss.arxiv.org/abs/2402.01293) | 本论文探索了将上下文学习扩展到多模态的文本到图像转换任务，并提出了第一个T2I-ICL基准数据集CoBSAT。研究发现MLLMs在解决T2I-ICL问题时面临着多模态和图像生成的固有复杂性，并通过微调和思维链提示等策略取得了显著改进。 |
| [^73] | [Spiking CenterNet: A Distillation-boosted Spiking Neural Network for Object Detection](https://rss.arxiv.org/abs/2402.01287) | Spiking CenterNet是一种利用脉冲神经网络和知识蒸馏相结合的新方法，用于高效能量、小型化嵌入式人工智能的目标检测。在挑战性的GEN1汽车检测数据集上，我们的模型使用的能量不到一半，表现超过了可比较的先前工作。 |
| [^74] | [Differentiable and accelerated wavelet transforms on the sphere and ball](https://rss.arxiv.org/abs/2402.01282) | 本研究设计了新的高度可分布和自动可微分的方向小波变换，在球面和球上的信号处理中取得了显著的加速效果，同时保持高精度，具有重要的实际应用价值。 |
| [^75] | [Parametric-Task MAP-Elites](https://rss.arxiv.org/abs/2402.01275) | 本文引入了参数任务MAP-Elites（PT-ME）算法，解决连续多任务优化问题。该算法在每次迭代中解决新任务，利用局部线性回归进行变异操作，通过得到的解集数据集创建了映射任务参数到最优解的函数，实验证明PT-ME算法优于所有基准算法。 |
| [^76] | [On the Transferability of Large-Scale Self-Supervision to Few-Shot Audio Classification](https://rss.arxiv.org/abs/2402.01274) | 本研究评估了大规模自监督模型在少样本音频分类中的性能，并发现在一些少样本问题中取得了最先进的性能，同时发现语音为基础的少样本问题与多个下游音频任务之间存在较强的相关性。 |
| [^77] | [Direct side information learning for zero-shot regression](https://rss.arxiv.org/abs/2402.01264) | 本论文提出了一种直接学习零样本回归的边缘信息的方法，解决了现有方法在零样本回归框架下的局限性，并有效地利用目标侧信息。这种方法为零样本回归问题提供了一个新的解决方案。 |
| [^78] | [A Differentiable POGLM with Forward-Backward Message Passing](https://rss.arxiv.org/abs/2402.01263) | 提出了一种具有前向-后向消息传递的可微分POGLM模型，解决了现有POGLM学习中的路径梯度估计和变分模型设计等问题。 |
| [^79] | [Cascaded Scaling Classifier: class incremental learning with probability scaling](https://rss.arxiv.org/abs/2402.01262) | 提出了级联缩放分类器，结合边际抑制和知识蒸馏方法，用于实现神经网络中的连续学习，并降低过去任务的遗忘。 |
| [^80] | [TEDDY: Trimming Edges with Degree-based Discrimination strategY](https://rss.arxiv.org/abs/2402.01261) | TEDDY是一种利用边缘度量信息的边缘修剪方法，旨在通过一次性操作实现边缘稀疏化，进而鼓励参数稀疏化训练。这是一个解决图神经网络中抽奖票假设的时间效率和效果问题的创新方法。 |
| [^81] | [Position Aware 60 GHz mmWave Beamforming for V2V Communications Utilizing Deep Learning](https://rss.arxiv.org/abs/2402.01259) | 本文提出了一种利用深度学习的方法，通过预测具有足够毫米波接收功率的最佳波束，使用车辆位置信息来实现车辆到车辆通信中的高效链路配置。 |
| [^82] | [Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field Dynamics on the Attention Landscape](https://rss.arxiv.org/abs/2402.01258) | 本文研究了基于Transformer架构的大型语言模型在上下文中学习非线性特征的优化问题，通过在均场和两个时间尺度的极限情况下的分析，证明了参数分布的损失景观虽然高度非凸，但变得相当温和，并建立了新的方法来获得具体的改进速率，这将有助于增强上下文学习的能力。 |
| [^83] | [Target inductive methods for zero-shot regression](https://rss.arxiv.org/abs/2402.01252) | 本论文提出了两种针对零射回归问题的方法，一种是基于相似性的方法，另一种是集成学习的方法。这些方法旨在利用周围环境信息对气象站的空气污染物含量进行预测。 |
| [^84] | [Two Heads Are Better Than One: Boosting Graph Sparse Training via Semantic and Topological Awareness](https://rss.arxiv.org/abs/2402.01242) | 该论文提出了一种新的研究方向和概念，即图稀疏训练（GST），通过动态稀疏训练使稀疏图与拓扑和语义锚点对齐，以降低计算开销，并在大规模图上提高图神经网络（GNN）的性能。 |
| [^85] | [Beyond the Request: Harnessing HTTP Response Headers for Cross-Browser Web Tracker Classification in an Imbalanced Setting](https://rss.arxiv.org/abs/2402.01240) | 本研究通过利用HTTP响应头设计了机器学习分类器，在跨浏览器环境下有效检测Web追踪器，结果在Chrome和Firefox上表现出较高的准确性和性能。 |
| [^86] | [Flexible Variational Information Bottleneck: Achieving Diverse Compression with a Single Training](https://rss.arxiv.org/abs/2402.01238) | 本研究引入了灵活的变分信息瓶颈（FVIB）框架，通过单次训练即可获得所有β值的最优模型，从而实现多样化压缩，并且在理论和实证方面证明了它的有效性。 |
| [^87] | [Unveiling Delay Effects in Traffic Forecasting: A Perspective from Spatial-Temporal Delay Differential Equations](https://rss.arxiv.org/abs/2402.01231) | 本研究提出了一个基于神经网络的空间-时间延迟微分方程模型，用于捕捉交通预测中的延迟效应。 |
| [^88] | [HW-SW Optimization of DNNs for Privacy-preserving People Counting on Low-resolution Infrared Arrays](https://rss.arxiv.org/abs/2402.01226) | 本文针对低分辨率红外阵列的隐私保护人数计数问题，提出了一种高度自动化的DNN优化流程，包括神经架构搜索、混合精度量化和后处理等，并通过实现新的智能传感器原型，在能耗、内存和准确性这三个维度上得到了大量高效的解决方案。 |
| [^89] | [Location Agnostic Adaptive Rain Precipitation Prediction using Deep Learning](https://rss.arxiv.org/abs/2402.01208) | 本研究提出了一种基于深度学习的自适应框架，能够解决降雨预测中的位置差异和气候变化带来的挑战。通过在巴黎、洛杉矶和东京进行适应，我们方法的预测能力分别提高了43.51%、5.09%和38.62%。 |
| [^90] | [Efficient Causal Graph Discovery Using Large Language Models](https://rss.arxiv.org/abs/2402.01207) | 提出了一个新的框架，利用大型语言模型进行高效的因果图发现，采用了广度优先搜索方法，只需要线性数量的查询，同时能轻松结合观察数据以提高性能，具有高效性和数据效率，并在真实因果图上取得了最先进的结果，展示了其在不同领域的广泛适用性潜力。 |
| [^91] | [Comparative Evaluation of Weather Forecasting using Machine Learning Models](https://rss.arxiv.org/abs/2402.01206) | 本研究使用机器学习算法在天气预测中取得了显著进展，并分析了不同算法在降水和温度预测方面的贡献。 |
| [^92] | [A Survey on Self-Supervised Learning for Non-Sequential Tabular Data](https://rss.arxiv.org/abs/2402.01204) | 本调研总结了自监督学习在非连续表格数据中的最新进展和挑战，将其方法分为预测性学习、对比学习和混合学习，讨论了应用问题，包括自动化数据工程、跨表格查询和隐私保护等。 |
| [^93] | [Structured World Modeling via Semantic Vector Quantization](https://rss.arxiv.org/abs/2402.01203) | 这篇论文提出了一种新方法，通过语义向量量化实现语义神经离散表示学习，解决了现有方法只能提供补丁级别表示的问题，并通过逐层构建场景表示和训练的方式实现了结构化语义世界建模。 |
| [^94] | [Few-Shot Class-Incremental Learning with Prior Knowledge](https://rss.arxiv.org/abs/2402.01201) | 该论文提出了一种具有先验知识的学习方法，通过引入从后续增量类别的无标签数据中产生的伪标签，与带标签的基类样本一起进行联合训练，有效地为旧类和新类数据分配嵌入空间，从而提高了模型对灾难性遗忘的韧性。 |
| [^95] | [Conditional Normalizing Flows for Active Learning of Coarse-Grained Molecular Representations](https://rss.arxiv.org/abs/2402.01195) | 本文提出了使用条件化正则化流和主动学习的方法来解决粗粒化分子表示中的玻尔兹曼分布采样问题，相比传统的分子动力学模拟，该方法能够获得更高效的加速效果。 |
| [^96] | [Efficient Prompt Caching via Embedding Similarity](https://rss.arxiv.org/abs/2402.01173) | 本论文通过嵌入相似性的提示缓存方法来提高大规模语言模型(LMMs)的推理效率，并提出一种蒸馏方法来优化现有嵌入以获得更好的缓存预测准确性。 |
| [^97] | [Truncated Non-Uniform Quantization for Distributed SGD](https://rss.arxiv.org/abs/2402.01160) | 我们提出了一种截断非均匀量化的分布式SGD方法，用于提高通信效率。我们的方法通过截断来减轻长尾噪声的影响，并根据梯度的统计特性进行非均匀量化。理论分析和实验评估表明，该方法在通信效率和收敛性方面取得了优越的平衡。 |
| [^98] | [The Optimality of Kernel Classifiers in Sobolev Space](https://rss.arxiv.org/abs/2402.01148) | 本文研究了核分类器在Sobolev空间中的最优性质，并通过对条件概率的假设和核回归理论的应用，导出了核分类器的分类超额风险上界和Sobolev空间的极小极大下界。此外，我们还提出了一种简单方法来估计插值平滑度，并将其应用于实际数据集。 |
| [^99] | [Efficient Reinforcement Learning for Routing Jobs in Heterogeneous Queueing Systems](https://rss.arxiv.org/abs/2402.01147) | 本研究针对异构排队系统中作业路由问题，提出了ACHQ算法，通过低维度的软阈值策略参数化和基于策略梯度的方法，利用系统的排队结构，实现了高效的强化学习求解策略。实验结果表明，ACHQ算法能够收敛到近似全局最优解。 |
| [^100] | [Limited Memory Online Gradient Descent for Kernelized Pairwise Learning with Dynamic Averaging](https://rss.arxiv.org/abs/2402.01146) | 本文提出了一种基于动态平均的核化配对学习的有限内存在线梯度下降算法，解决了配对学习中计算复杂度增长问题，并且不需要示例的独立性。 |
| [^101] | [Learning Network Representations with Disentangled Graph Auto-Encoder](https://rss.arxiv.org/abs/2402.01143) | 本文介绍了解缠离散图自编码器(DGA)和解缠变分图自编码器(DVGA)的方法，利用生成模型来学习解缠表示。 |
| [^102] | [Root Cause Analysis In Microservice Using Neural Granger Causal Discovery](https://rss.arxiv.org/abs/2402.01140) | 提出了一种基于神经Granger因果发现的方法来解决微服务中根本原因分析的挑战。 |
| [^103] | [Online conformal prediction with decaying step sizes](https://rss.arxiv.org/abs/2402.01139) | 本文介绍了一种在线自适应预测方法，通过使用递减步长来改进在任意序列上的覆盖率保证，并且能够同时估计总体分位数。 |
| [^104] | [Graph Neural Networks in EEG-based Emotion Recognition: A Survey](https://rss.arxiv.org/abs/2402.01138) | 基于脑电图的情绪识别中的图神经网络是一个有重要意义的领域。本综述分类和分析了已有方法，并提供了构建基于脑电图的GNNs的明确指导。 |
| [^105] | [Scalable Multi-modal Model Predictive Control via Duality-based Interaction Predictions](https://rss.arxiv.org/abs/2402.01116) | 我们提出了一个层级架构，通过使用对偶交互预测和精简的MPC问题，实现了可扩展的实时模型预测控制，在复杂的多模态交通场景中展示了12倍的速度提升。 |
| [^106] | [Double-Dip: Thwarting Label-Only Membership Inference Attacks with Transfer Learning and Randomization](https://rss.arxiv.org/abs/2402.01114) | 这项研究探索了使用迁移学习和随机化来防止成员推断攻击。通过在过拟合的深度神经网络上应用双重防御，我们可以提高模型的隐私性而不降低准确度。 |
| [^107] | [Near-Optimal Reinforcement Learning with Self-Play under Adaptivity Constraints](https://rss.arxiv.org/abs/2402.01111) | 本文研究了具有自适应约束的多智能体强化学习问题，并提出了一种基于消除算法，将后悔控制在$\widetilde{O}(\sqrt{H^3 S^2 ABK})$，批量复杂度为$O(H+\log\log K)$。此外，还给出了所有具有$\widetilde{O}(\sqrt{K})$后悔界算法的批量复杂度下界。 |
| [^108] | [Vaccine: Perturbation-aware Alignment for Large Language Model](https://rss.arxiv.org/abs/2402.01109) | 疫苗是一种针对大规模语言模型的干扰感知对齐技术，通过逐渐添加扰动产生不变的隐藏嵌入，提高对抗有害提示引起的嵌入漂移的对齐鲁棒性，同时保留对良性提示的推理能力。 |
| [^109] | [Reasoning Capacity in Multi-Agent Systems: Limitations, Challenges and Human-Centered Solutions](https://rss.arxiv.org/abs/2402.01108) | 多智能体系统面临着利用大型语言模型的限制和挑战，需要引入推理能力作为统一的标准来实现系统的整合和优化。 |
| [^110] | [Simulation of Graph Algorithms with Looped Transformers](https://rss.arxiv.org/abs/2402.01107) | 本文研究了使用循环变压器网络模拟图算法的能力，证明了该结构可以模拟Dijkstra的最短路径算法、广度优先搜索、深度优先搜索和Kosaraju的强连通分量算法，并展示了其具有图灵完整性的结果。 |
| [^111] | [A Survey for Foundation Models in Autonomous Driving](https://rss.arxiv.org/abs/2402.01105) | 本综述论文回顾了40多篇研究论文，总结了基于基础模型的自动驾驶在规划、仿真和关键任务方面的重要贡献，强调了大型语言模型的推理和翻译能力，视觉基础模型在物体检测和驾驶场景创建方面的应用，以及多模态基础模型的视觉理解和空间推理能力。 |
| [^112] | [Compositional Generative Modeling: A Single Model is Not All You Need](https://rss.arxiv.org/abs/2402.01103) | 本文提出了一种组合生成方法，通过将较小的生成模型组合在一起来构建大型生成系统。该方法可以更高效地学习数据分布，实现对训练时未见的数据部分的泛化，并能够编写和构建新的生成模型。 |
| [^113] | [Bayesian Deep Learning for Remaining Useful Life Estimation via Stein Variational Gradient Descent](https://rss.arxiv.org/abs/2402.01098) | 本研究使用斯坦优化梯度下降算法，将标准的频率主义神经网络转化为贝叶斯神经网络，以应对预测性维护中估计剩余寿命的不确定性问题。 |
| [^114] | [Trustworthy Distributed AI Systems: Robustness, Privacy, and Governance](https://rss.arxiv.org/abs/2402.01096) | 本文回顾了可信的分布式AI的代表性技术，包括鲁棒性保证、隐私保护和公平意识，以解决分布式学习中存在的安全、隐私和公平问题。 |
| [^115] | [How many views does your deep neural network use for prediction?](https://rss.arxiv.org/abs/2402.01095) | 本文提出了最小有效视图（MSVs）的概念，该概念类似于多视图，但适用于实际图像，并且通过实证研究表明，MSV的数量与模型的预测准确性之间存在关系。 |
| [^116] | [Specialized Language Models with Cheap Inference from Limited Domain Data](https://rss.arxiv.org/abs/2402.01093) | 本研究提出了一种使用有限领域数据进行廉价推理的专用语言模型。在研究中，我们通过比较不同的机器学习方法，在推理成本的限制下找到了比训练非常大的基本转换器模型更优的替代方案。具体而言，在大型预训练预算下，超网络和专家混合模型的困惑度更好，而在大型专用预算下，训练重要样本数据集上的小型模型更具吸引力。 |
| [^117] | [A Dynamical Model of Neural Scaling Laws](https://rss.arxiv.org/abs/2402.01092) | 这篇论文提出了一个动力学模型来解释神经缩放定律。通过分析梯度下降训练的随机特征模型，研究发现训练时间和模型大小的缩放具有不同的幂律指数，而计算最优缩放规则要求增加训练步数快于增加模型参数，与实证观察相一致。 |
| [^118] | [Scalable Higher-Order Tensor Product Spline Models](https://rss.arxiv.org/abs/2402.01090) | 我们提出了一种可扩展的高阶张量积样条模型，允许加入所有（高阶）非线性特征效应的相互作用，并具有与没有相互作用的模型成比例的计算成本。 |
| [^119] | [No Free Prune: Information-Theoretic Barriers to Pruning at Initialization](https://rss.arxiv.org/abs/2402.01089) | 本文解释了为什么在初始化时修剪神经网络困难，并提出了一个关于有效参数数量的理论解释。我们指出，在嘈杂数据中鲁棒地插值的稀疏神经网络需要严重依赖于数据的掩码。为此，我们怀疑在训练过程中和训练后修剪是必要的。 |
| [^120] | [Salsa Fresca: Angular Embeddings and Pre-Training for ML Attacks on Learning With Errors](https://rss.arxiv.org/abs/2402.01082) | 本研究提出了三种关键方法，即更好的预处理、角嵌入和模型预训练，用于改进机器学习攻击学习与误差问题，并使其能够在更大维度的情况下恢复稀疏二进制密钥。 |
| [^121] | [Recent Advances in Predictive Modeling with Electronic Health Records](https://rss.arxiv.org/abs/2402.01077) | 这项调查总结了基于电子健康记录数据的深度学习预测模型的最新进展，包括背景介绍、数学定义、分类总结、基准和工具包，以及未来研究方向的讨论。 |
| [^122] | [DoseGNN: Improving the Performance of Deep Learning Models in Adaptive Dose-Volume Histogram Prediction through Graph Neural Networks](https://rss.arxiv.org/abs/2402.01076) | 本文通过图神经网络构建了一个即插即用的框架，增强了基于深度学习模型的剂量体积直方图预测的性能。 |
| [^123] | [Chameleon: Foundation Models for Fairness-aware Multi-modal Data Augmentation to Enhance Coverage of Minorities](https://rss.arxiv.org/abs/2402.01071) | 本文提出了名为变色龙的系统，利用生成式人工智能和大型语言模型增强少数群体在多模态数据中的覆盖范围。系统通过最少添加合成生成的元组的方式来实现数据增强，并采用拒绝抽样方法确保生成元组的高质量和分布一致。实验证明了该方法的高效性。 |
| [^124] | [FedShift: Tackling Dual Heterogeneity Problem of Federated Learning via Weight Shift Aggregation](https://rss.arxiv.org/abs/2402.01070) | 本文介绍了一种名为FedShift的算法，通过权重迁移聚合来解决联邦学习中的异质性问题，并提高训练速度和模型准确性。 |
| [^125] | [Assessing Patient Eligibility for Inspire Therapy through Machine Learning and Deep Learning Models](https://rss.arxiv.org/abs/2402.01067) | 本文利用机器学习和深度学习技术评估Inspire治疗的患者适应性，通过分析医学数据和DISE视频，比较了不同模型的性能，展示了机器学习和深度学习技术的潜力。 |
| [^126] | [Bio-Inspired Compensatory Strategies for Damage to Flapping Robotic Propulsors](https://rss.arxiv.org/abs/2402.01062) | 通过人工进化确定对受损机器人系统的划水机制的最佳改变，并通过补偿机制实现恢复推力产生。 |
| [^127] | [Towards an Algebraic Framework For Approximating Functions Using Neural Network Polynomials](https://rss.arxiv.org/abs/2402.01058) | 本论文提出了一个基于神经网络多项式的函数近似代数框架，该框架在一定参数限制下能够有效近似实数函数，并且其参数和深度增长与所需精度多项式相关。 |
| [^128] | [Expert Proximity as Surrogate Rewards for Single Demonstration Imitation Learning](https://rss.arxiv.org/abs/2402.01057) | 本文介绍了一种针对单演示模仿学习的新方法TDIL，通过引入基于转换鉴别器的替代奖励函数，鼓励代理向靠近专家状态的状态导航，有效解决了奖励信号稀疏的问题。 |
| [^129] | [Multiclass Learning from Noisy Labels for Non-decomposable Performance Measures](https://rss.arxiv.org/abs/2402.01055) | 本论文提出了用于从带有噪声标签的数据中学习非可分解性能度量的多类学习算法。这些算法分别适用于单调凸性和线性比率两类性能度量，并基于类条件噪声模型进行噪声校正。 |
| [^130] | [Unconditional Latent Diffusion Models Memorize Patient Imaging Data](https://rss.arxiv.org/abs/2402.01054) | 本论文研究了医学图像合成中隐式扩散模型的记忆问题。通过评估训练数据的记忆程度以及探索可能导致记忆的因素，揭示了这一问题的重要性和潜在风险。 |
| [^131] | [Weakly Convex Regularisers for Inverse Problems: Convergence of Critical Points and Primal-Dual Optimisation](https://rss.arxiv.org/abs/2402.01052) | 本文提出了一种关于逆问题的弱凸正则化器的收敛性问题的一般化公式，并证明了通过一类弱凸正则化器的实现可以达到收敛，并应用于学习的正则化中实现了对计算机层析成像中学习对抗性正则化器性能的提高。 |
| [^132] | [Distributed MCMC inference for Bayesian Non-Parametric Latent Block Model](https://rss.arxiv.org/abs/2402.01050) | 本文提出了一种基于分布式MCMC推理的贝叶斯非参数潜在分块模型方法，通过将观测值和特征划分为分区，并采用Master/Worker架构来提高聚类标签准确性和执行时间。 |
| [^133] | [Ultra Fast Transformers on FPGAs for Particle Physics Experiments](https://rss.arxiv.org/abs/2402.01047) | 本研究介绍了一种在FPGA上高效实现变压器架构的方法，并在粒子物理实验中的触发器应用中取得了非常好的效果，实现了低于2微秒的延迟，符合大型强子对撞机实验的要求。 |
| [^134] | [LatticeGraphNet: A two-scale graph neural operator for simulating lattice structures](https://rss.arxiv.org/abs/2402.01045) | LatticeGraphNet是一种双尺度图神经操作器，通过学习格状结构的降维动力学和降维表示到四面体网格的映射，能够高效地预测任意格状结构的变形，并在显著减少推断时间的同时保持高准确性。 |
| [^135] | [Fisher information dissipation for time inhomogeneous stochastic differential equations](https://rss.arxiv.org/abs/2402.01036) | 本文提供了时间不齐次随机微分方程的费舍尔信息耗散方法。通过将Langevin动力学的概率转移方程构造为关于时间依赖的最优传输度量的修正梯度流，我们得到了概率密度函数的收敛性保证，并在几个时间不齐次Langevin动力学中进行了验证。 |
| [^136] | [Repeat After Me: Transformers are Better than State Space Models at Copying](https://rss.arxiv.org/abs/2402.01032) | 这篇论文证明了Transformer模型在复制任务上的优势，相比于使用固定潜在状态的广义状态空间模型，Transformer模型能够更高效地复制和检索上下文信息。 |
| [^137] | [Multivariate Probabilistic Time Series Forecasting with Correlated Errors](https://rss.arxiv.org/abs/2402.01000) | 本文提出了一种方法，基于低秩加对角线参数化协方差矩阵，可以有效地刻画时间序列预测中误差的自相关性，并具有复杂度低、校准预测准确性高等优点。 |
| [^138] | [A Cost-Efficient Approach for Creating Virtual Fitting Room using Generative Adversarial Networks (GANs)](https://rss.arxiv.org/abs/2402.00994) | 本论文提出了一种使用生成对抗网络(GANs)创建虚拟试衣间的成本效益方法，为客户提供在线试穿衣服的平台，省去了实体店试衣的烦恼和时间，并通过使用特殊镜子减少了实体店拥挤的问题。 |
| [^139] | [Self-Supervised Contrastive Pre-Training for Multivariate Point Processes](https://rss.arxiv.org/abs/2402.00987) | 在多变量事件流中，我们提出了一种新的自监督学习范式，使用变压器编码器进行预训练，并引入对比模块来比较真实事件和模拟的空白实例，以提高后续任务的性能。 |
| [^140] | [Recurrent Transformers with Dynamic Halt](https://rss.arxiv.org/abs/2402.00976) | 本文研究了增强Transformer与循环机制的两种方法，并提出了新的扩展和组合方法。在多个诊断任务中进行比较，探索它们的归纳偏好。 |
| [^141] | [Closure Discovery for Coarse-Grained Partial Differential Equations using Multi-Agent Reinforcement Learning](https://rss.arxiv.org/abs/2402.00972) | 使用多智能体强化学习(MARL)识别未精细解析的PDEs中的闭合项，通过部署中央策略和卷积神经网络(CNN)，能够准确预测和加速模拟。 |
| [^142] | [Multi-Modal Machine Learning Framework for Automated Seizure Detection in Laboratory Rats](https://rss.arxiv.org/abs/2402.00965) | 本文提出了一个多模态机器学习系统，利用多个数据源和类型的数据，通过组合多个模型的结果来改进癫痫检测性能。实验结果表明，使用多个数据源可以过滤和删除假阳性预测，提高癫痫检测的准确性。 |
| [^143] | [Credal Learning Theory](https://rss.arxiv.org/abs/2402.00957) | 本文提出了一种信任学习理论，通过使用凸集的概率来建模数据生成分布的变异性，从有限样本的训练集中推断出信任集，并推导出bounds。 |
| [^144] | [FairEHR-CLP: Towards Fairness-Aware Clinical Predictions with Contrastive Learning in Multimodal Electronic Health Records](https://rss.arxiv.org/abs/2402.00955) | FairEHR-CLP是一个通用框架，利用对比学习，通过生成患者的合成对应来实现多样化的人口统计身份，并利用公平感知预测方法消除EHR中的社会偏见。 |
| [^145] | [Geometry of Polynomial Neural Networks](https://rss.arxiv.org/abs/2402.00949) | 本研究利用代数几何工具研究了具有单项式激活函数的多项式神经网络的表达性和学习过程，通过对神经流形的维度和学习度的研究，提供了网络表达能力和训练复杂度的度量，并给出了可学函数数量的上界。 |
| [^146] | [Approximate Nearest Neighbor Search with Window Filters](https://rss.arxiv.org/abs/2402.00943) | 这篇论文提出了一种使用窗口过滤的近似最近邻搜索方法，能够在各种语义搜索问题中实现高速搜索，并在多个基准数据集上取得了显著的速度提升。 |
| [^147] | [A Comparative Analysis of Gene Expression Profiling by Statistical and Machine Learning Approaches](https://rss.arxiv.org/abs/2402.00926) | 这篇论文比较了统计和机器学习方法在基因表达谱的分析中的应用。通过解释性方法得到的基因排名可以帮助理解表型，但存在一些生物学和方法学上的限制。实验证明在癌症分类中，多种机器学习模型都可以用于预测癌症类型。 |
| [^148] | [Deep Learning Approaches for Network Traffic Classification in the Internet of Things (IoT): A Survey](https://rss.arxiv.org/abs/2402.00920) | 本调研论文通过系统分析和分类现有的研究贡献，提供了针对物联网环境中网络流量分类的深度学习方法的全面概述，探讨了各种模型的优势和局限性，为研究人员和从业者提供有价值的指导和参考。 |
| [^149] | [Can we Constrain Concept Bottleneck Models to Learn Semantically Meaningful Input Features?](https://rss.arxiv.org/abs/2402.00912) | 本文研究了概念瓶颈模型如何从具有细粒度概念注释的数据集中学习概念，以实现模型输出的内在可解释性。 |
| [^150] | [Addressing Bias Through Ensemble Learning and Regularized Fine-Tuning](https://rss.arxiv.org/abs/2402.00910) | 本文提出了一种通过集成学习和正则化微调的方法，解决AI模型中的偏见问题。该方法可通过在小数据集和有偏的预训练模型上训练多个对抗偏见的模型，并使用集成学习得到无偏的预测结果。通过实验证明了该方法在CIFAR10和HAM10000数据集上的有效性。 |
| [^151] | [AlphaRank: An Artificial Intelligence Approach for Ranking and Selection Problems](https://rss.arxiv.org/abs/2402.00907) | AlphaRank是一种用于解决固定预算的排名和选择问题的人工智能方法。它通过利用经典的排名和选择程序作为基准策略，以有效学习随机动态规划的价值函数，采用深度强化学习加速在线样本分配，并提出可并行计算的框架用于解决大规模问题。AlphaRank在均值、方差和诱导相关性之间取得了优越的平衡，显著提高了性能。 |
| [^152] | [BrainLeaks: On the Privacy-Preserving Properties of Neuromorphic Architectures against Model Inversion Attacks](https://rss.arxiv.org/abs/2402.00906) | 本研究探索了神经形态架构对模型反转攻击的隐私保护能力，发现脉冲神经网络具有固有的隐私保护性质，并能有效抵抗基于梯度的攻击。 |
| [^153] | [Graph Domain Adaptation: Challenges, Progress and Prospects](https://rss.arxiv.org/abs/2402.00904) | 这篇论文综述了图领域适应的研究现状、挑战和前景，提出了一种有效的图之间的知识传递范式，可以增强模型在目标图上的性能。 |
| [^154] | [Weakly Supervised Learners for Correction of AI Errors with Provable Performance Guarantees](https://rss.arxiv.org/abs/2402.00899) | 这项工作提出了使用具有可证明性能保证的弱监督AI错误修正器来处理AI错误。修正器通过批准或拒绝底层分类器的决策来提升性能，并通过概率界限保证其性能。实验证明该方法在训练数据稀缺的真实世界任务中提升图像分类器的性能。 |
| [^155] | [An Early Categorization of Prompt Injection Attacks on Large Language Models](https://rss.arxiv.org/abs/2402.00898) | 本文旨在提供对大规模语言模型上的提示注入攻击的早期分类，并讨论了这些攻击对LLM最终用户、开发人员和研究人员的影响。 |
| [^156] | [Screening method for early dementia using sound objects as voice biomarkers](https://rss.arxiv.org/abs/2402.00897) | 本研究提出了一种使用声音对象作为声音生物标志物的早期痴呆筛查方法。该方法基于精心设计的特征，能准确地表示声音谱，并包含与被试者对声音的控制有关的有意义信息。实验结果表明，该方法能够有效区分健康个体与轻度认知障碍或阿尔茨海默病个体。 |
| [^157] | [Privacy and Security Implications of Cloud-Based AI Services : A Survey](https://rss.arxiv.org/abs/2402.00896) | 本文调查了云基础人工智能服务的隐私和安全状况，发现机器学习模型引入的风险亟待解决。通过提出分类法来全面研究模型提供者和使用者所面临的风险及其防御手段，有助于创建强大的解决方案。 |
| [^158] | [MoDE: A Mixture-of-Experts Model with Mutual Distillation among the Experts](https://rss.arxiv.org/abs/2402.00893) | MoDE是一种在混合专家模型中应用专家间相互蒸馏的方法，以提高模型的泛化能力。实验证明MoDE在各种数据集上都有效，并且具有普适性和鲁棒性。 |
| [^159] | [EVA-GAN: Enhanced Various Audio Generation via Scalable Generative Adversarial Networks](https://rss.arxiv.org/abs/2402.00892) | EVA-GAN通过可扩展的生成对抗网络在音频生成领域取得了显著进展，改善了频谱和高频重建以及对域外数据性能的稳健性，实现了高保真音频的生成。 |
| [^160] | [Large Language Models in Cybersecurity: State-of-the-Art](https://rss.arxiv.org/abs/2402.00891) | 本研究综述了大型语言模型（LLMs）在网络安全领域中的防御和对抗应用，并识别了关键的研究空缺，旨在提供对LLM驱动的网络安全的潜在风险和机会的全面理解。 |
| [^161] | [Graph Representation Learning for Contention and Interference Management in Wireless Networks](https://rss.arxiv.org/abs/2402.00879) | 本研究提出了一种图表示学习方法，通过将用户分组视为图构建问题，利用图的最大切割来进行最优分组决策。演员-评论家图表示学习（AC-GRL）算法被设计用来实现最优图构建，并通过估计最优图的边权重来最大化网络的用户吞吐量。 |
| [^162] | [Radio Map Estimation -- An Open Dataset with Directive Transmitter Antennas and Initial Experiments](https://rss.arxiv.org/abs/2402.00878) | 本文通过发布包含模拟的路径损耗无线电地图、真实城市地图和航拍影像的公开数据集，以及初步实验，填补了开放基准数据集和代码库的空白。 |
| [^163] | [Scaling Sparse Fine-Tuning to Large Language Models](https://rss.arxiv.org/abs/2401.16405) | 本研究将稀疏微调方法扩展到大型语言模型，提出了一种新的稀疏微调算法SpIEL，并对LLM进行了指令微调，以解决其参数庞大的问题。 |
| [^164] | [On the Semantics of LM Latent Space: A Vocabulary-defined Approach](https://rss.arxiv.org/abs/2401.16184) | 本论文介绍了一种以词汇为定义的语义学方法，建立了LM潜在空间的参考框架，确保基于LM词汇的分离语义分析。在LM适应过程中，引入了计算logits的新技术和神经聚类模块，通过实验证明了该方法在文本理解上的优越性能。 |
| [^165] | [NoFunEval: Funny How Code LMs Falter on Requirements Beyond Functional Correctness](https://rss.arxiv.org/abs/2401.15963) | 本论文提出了一个新的基准测试 NoFunEval，用于评估代码语言模型在非功能性要求和简单分类实例方面的表现。研究发现，目前的代码语言模型在处理这些要求时存在根本性的盲点。 |
| [^166] | [Deciphering Textual Authenticity: A Generalized Strategy through the Lens of Large Language Semantics for Detecting Human vs. Machine-Generated Text](https://rss.arxiv.org/abs/2401.09407) | 该论文通过研究在真实世界场景中检测机器生成文本的方法，发现现有方法对于不同生成器和领域产生的文本存在严重限制。 |
| [^167] | [Fix-Con: Automatic Fault Localization and Repair of Deep Learning Model Conversions](https://rss.arxiv.org/abs/2312.15101) | 本文提出了一种自动化的故障定位和修复方法Fix-Con，用于在深度学习模型转换过程中修复由转换引入的故障。Fix-Con能够检测和修复模型输入、参数、超参数和模型图方面的故障，提高转换模型的部署和预测正确性。 |
| [^168] | [Online Variational Sequential Monte Carlo](https://rss.arxiv.org/abs/2312.12616) | 本文提出了一种在线学习的算法，名为在线VSMC，它基于变分顺序蒙特卡洛方法，在处理数据流时能够实时进行模型参数估计和粒子提议适应。 |
| [^169] | [DIRECT: Deep Active Learning under Imbalance and Label Noise](https://rss.arxiv.org/abs/2312.09196) | 这篇论文提出了一种名为DIRECT的算法，用于处理不平衡和标签噪声下的深度主动学习问题。通过确定类别分割阈值并标记最不确定且离其最近的示例，该算法能够有效解决罕见类和少数类的性能问题，并具有批次标记和对标签噪声的容忍能力。 |
| [^170] | [CBQ: Cross-Block Quantization for Large Language Models](https://rss.arxiv.org/abs/2312.07950) | CBQ是一种用于大型语言模型的跨块重构型后训练量化方法。CBQ通过使用同源重构方案来建立块间的长程依赖关系，最小化误差积累。CBQ还采用了粗到精的预处理策略和自适应的取整技术，使其能够有效处理极端异常值并提高整体量化精度。 |
| [^171] | [Why "classic" Transformers are shallow and how to make them go deep](https://rss.arxiv.org/abs/2312.06182) | 这篇论文研究了Transformer模型的深度问题，指出这个问题是由于“token相似性升级”导致的，提供了理论和实证调查的证据。 |
| [^172] | [Neuron Patching: Neuron-level Model Editing on Code Generation and LLMs](https://rss.arxiv.org/abs/2312.05356) | 这项工作介绍了一种神经元层面的模型编辑方法，能够在编码任务中修补LLM模型，并且在API序列推荐、代码生成和伪代码到代码转换等任务中得到了验证和评估。 |
| [^173] | [Large Language Models on Graphs: A Comprehensive Survey](https://rss.arxiv.org/abs/2312.02783) | 这篇论文对在图上的大型语言模型进行了全面调查，研究了纯图形、文本属性图形和文本配对图形三个不同场景下的应用情况，并探讨了基于图形的推理能力是否可以推广到大型语言模型上。 |
| [^174] | [Marginal Laplacian Score](https://rss.arxiv.org/abs/2311.17795) | 边际拉普拉斯分数（MLS）是一种针对高维度不平衡数据的改进版拉普拉斯分数（LS），通过保留数据集边缘的局部结构，提供了一种有效的无监督特征选择方法。该方法被成功地集成到不同iable无监督特征选择（DUFS）算法中，在合成和公共数据集上展示了稳健且改进的性能。 |
| [^175] | [Multi-intention Inverse Q-learning for Interpretable Behavior Representation](https://rss.arxiv.org/abs/2311.13870) | 本研究引入了一种多意图逆Q学习算法，用于解决在推断离散时变奖励时的挑战。通过聚类观察到的专家轨迹并独立解决每个意图的逆强化学习问题，我们的方法在动物行为预测方面超越了当前的基准，产生了可解释的奖励函数。 |
| [^176] | [Are Normalizing Flows the Key to Unlocking the Exponential Mechanism? A Path through the Accuracy-Privacy Ceiling Constraining Differentially Private ML](https://rss.arxiv.org/abs/2311.09200) | 通过使用正则流模型，解决了差分隐私机器学习中历史难题，提高了准确性和隐私保护。 |
| [^177] | [Understanding Grokking Through A Robustness Viewpoint](https://rss.arxiv.org/abs/2311.06597) | 通过神经网络的鲁棒性视角，我们发现$l_2$权重范数是Grokking的充分条件，并提出基于扰动的方法加速泛化，在模数相加数据集上发现标准训练过程在Grokking之前几乎没有学习其他基本群操作，而使用我们方法时加速泛化可以通过学习交换律来解释。 |
| [^178] | [Bridging Dimensions: Confident Reachability for High-Dimensional Controllers](https://rss.arxiv.org/abs/2311.04843) | 本文介绍了一种连接高维控制器的详尽闭环验证方法，通过将高维控制器的行为近似为不同状态空间区域内的低维控制器，平衡了逼近精度和可验证性。 |
| [^179] | [Signal Processing Meets SGD: From Momentum to Filter](https://rss.arxiv.org/abs/2311.02818) | 该论文提出了一种名为SGDF的优化方法，通过应用维纳滤波理论和引入时变自适应权重，加速了SGD的收敛速度，同时保持了泛化能力。实验证明，与其他优化器相比，SGDF在收敛和泛化之间取得了平衡。 |
| [^180] | [Forward $\chi^2$ Divergence Based Variational Importance Sampling](https://rss.arxiv.org/abs/2311.02516) | 引入了一种基于前向$\chi^2$散度的变分重要抽样方法(VIS)，通过直接估计和最大化对数似然来增强对复杂后验分布的估计性能。实验证明，VIS方法在多种潜变量模型中均优于最先进的基线方法，表现出更高的对数似然和模型参数估计准确性。 |
| [^181] | [Nonlinear Filtering with Brenier Optimal Transport Maps](https://rss.arxiv.org/abs/2310.13886) | 本文提出了基于Brenier最优输运映射的非线性滤波方法，通过估计先验分布到后验分布的映射来避免权重退化问题，并利用神经网络建模复杂分布和随机优化算法提高可扩展性。 |
| [^182] | [Foundation Model's Embedded Representations May Detect Distribution Shift](https://rss.arxiv.org/abs/2310.13836) | 该研究发现基础模型的嵌入表示可以检测到数据集之间的分布偏移，且在传统的泛化度量上表现出偏差。预训练的GPT-2模型的特征学习无法在特定任务上提升性能，而对其表示进行线性探测可能优于整体微调。 |
| [^183] | [Deception Abilities Emerged in Large Language Models](https://rss.arxiv.org/abs/2307.16513) | 大规模语言模型（LLM）如GPT-4具备了理解和诱导他人产生错误信念的能力，并且在复杂的欺骗场景中可以通过链式思维推理得到增强。 |
| [^184] | [Variational Linearized Laplace Approximation for Bayesian Deep Learning](https://rss.arxiv.org/abs/2302.12565) | 本论文提出了一种基于变分稀疏高斯过程的方法，用于近似线性化Laplace近似在贝叶斯深度学习中的应用。该方法保留了原始DNN的预测均值，并具有高效的随机优化，训练成本与训练点的数量无关。 |
| [^185] | [Learning efficient backprojections across cortical hierarchies in real time](https://rss.arxiv.org/abs/2212.10249) | 本论文介绍了一种称为无相位对齐学习的生物合理方法，可以在分层皮层层次中高效地学习反馈权重，通过利用生物物理系统中的噪声作为额外的信息载体，并同时学习所有权重。这种方法实现了高效的错误传播，并保持生物合理的信号传递和学习。 |
| [^186] | [Generative Adversarial Learning of Sinkhorn Algorithm Initializations](https://rss.arxiv.org/abs/2212.00133) | 本文通过生成对抗学习的方法，训练神经网络来学习Sinkhorn算法的初始化，显著加快收敛速度，同时保持算法的可微分性和并行性，并证明了网络的普适性和独立求解能力。 |
| [^187] | [Motif-guided Time Series Counterfactual Explanations](https://rss.arxiv.org/abs/2211.04411) | 本论文提出了一种基于图案的时间序列对抗性解释方法（MG-CF），通过利用重要的图案来生成直观的解释信息，以提高时间序列模型的信任和透明度。 |
| [^188] | [Enhancing Business Process Simulation Models with Extraneous Activity Delays](https://rss.arxiv.org/abs/2206.14051) | 本文研究了如何增强商业流程模拟模型，通过自动发现和捕获外部活动延迟，提高模拟准确性和效率。 |
| [^189] | [A Statistical Learning View of Simple Kriging](https://rss.arxiv.org/abs/2202.07365) | 本文从统计学习的视角对简单克里金方法进行了分析，解决了在大数据时代中，考虑复杂空间相关结构的大规模数据集预测问题。 |
| [^190] | [Distributional Reinforcement Learning by Sinkhorn Divergence](https://rss.arxiv.org/abs/2202.00769) | 本文提出了SinkhornDRL方法，使用Sinkhorn散度来减小当前和目标Bellman回报分布之间的差异，并通过理论证明和实证实验展示了该方法的优越性。 |
| [^191] | [The Benefits of Being Categorical Distributional: Uncertainty-aware Regularized Exploration in Reinforcement Learning](https://rss.arxiv.org/abs/2110.03155) | 通过应用返回概率函数分解技术，我们在分类分布式强化学习中探索了分布匹配正则化的潜力，通过隐式地优化策略以对齐目标回报分布的不确定性来产生不确定性感知的探索效果。 |
| [^192] | [Simple Imputation Rules for Prediction with Missing Data: Contrasting Theoretical Guarantees with Empirical Performance](https://rss.arxiv.org/abs/2104.03158) | 本研究对缺失数据问题进行了研究，通过对比理论和实证结果，展示了一些简单填补规则在预测任务中的性能。在广泛填补方法家族中，我们发现均值填补是最优的，而众数填补是次优的。实证结果除了支持理论发现外，还强调了理论和实践之间的差距和未来研究的机会。 |
| [^193] | [CroissantLLM: A Truly Bilingual French-English Language Model](https://arxiv.org/abs/2402.00786) | CroissantLLM是一个1.3B的双语语言模型，通过使用1:1的英语-法语预训练数据比例、自定义的分词器和双语调优数据集进行训练，实现了高性能和开源。模型还发布了训练数据集和多个检查点，以及一个法语基准测试 FrenchBench。 |
| [^194] | [Machine Unlearning for Image-to-Image Generative Models](https://arxiv.org/abs/2402.00351) | 本文提出了一种适用于图像到图像生成模型的机器遗忘方法，该方法通过提供一个统一的框架和一个高效的算法，实现在遗忘样本中删除信息且在保留样本上性能几乎没有下降。实证研究证明该方法不依赖于保留样本的可用性，符合数据保留政策。 |
| [^195] | [Analog-digital Scheduling for Federated Learning: A Communication-Efficient Approach](https://arxiv.org/abs/2402.00318) | 本文提出一种模拟数字FL方案，通过在每一轮中，通过模拟OTA方案上传梯度或者通过正交RB传输量化梯度的方式调度设备，解决了联邦学习中性能受限于信噪比最差设备问题的差异，以实现通信高效和降低噪声。 |
| [^196] | [An Accurate and Low-Parameter Machine Learning Architecture for Next Location Prediction](https://arxiv.org/abs/2402.00306) | 本文提出了一个节能、小型且低参数的机器学习架构，用于准确预测用户的下一个位置。通过大量实验，成功将模型参数数量从2.02亿减少到200万，模型大小从791 MB减少到8 MB，训练时间减少了四倍。 |
| [^197] | [Detecting Multimedia Generated by Large AI Models: A Survey](https://arxiv.org/abs/2402.00045) | 本文综述了大型人工智能模型（LAIMs）生成的多媒体内容的检测方法和研究进展，旨在填补现有研究中的空白。我们提供了一种新颖的分类法，并介绍了纯检测和应用场景两个角度来增强检测性能。 |
| [^198] | [A Policy Gradient Primal-Dual Algorithm for Constrained MDPs with Uniform PAC Guarantees](https://arxiv.org/abs/2401.17780) | 本文介绍了一种带有均匀PAC保证的策略梯度原始对偶算法，用于在线约束马尔可夫决策过程（CMDP）问题。该算法同时保证了收敛到最优策略、次线性遗憾和多项式样本复杂度，并在实证研究中验证了其优越性能。 |
| [^199] | [Graph Multi-Similarity Learning for Molecular Property Prediction](https://arxiv.org/abs/2401.17615) | 提出了图多相似性学习 (GraphMSL)框架，通过引入连续尺度的多相似性度量，包括自相似度和相对相似性，以及对不同化学特征进行融合，提高了分子属性预测的效果和适用性。 |
| [^200] | [Can Large Language Models Replace Economic Choice Prediction Labs?](https://arxiv.org/abs/2401.17435) | 该论文研究大型语言模型是否能够取代经济实验室进行选择预测，并通过相关实验证明了其可行性。 |
| [^201] | [Activity Detection for Massive Connectivity in Cell-free Networks with Unknown Large-scale Fading, Channel Statistics, Noise Variance, and Activity Probability: A Bayesian Approach.](http://arxiv.org/abs/2401.16775) | 本研究探讨了在没有关于网络的重要信息的情况下进行活动检测的问题，并通过使用贝叶斯方法来处理大量未知参数。研究提出了最大后验估计器和变分方法来解决这个问题。 |
| [^202] | [Adaptive Crowdsourcing Via Self-Supervised Learning.](http://arxiv.org/abs/2401.13239) | 本论文介绍了一种新的自适应众包方法，通过利用自监督学习和新颖的聚合方案，根据众包工作者对先前数量的估计调整权重，实现更准确的集体估计。该方法适应复杂模型和其他实际挑战，并通过理论和计算研究进行了验证。 |
| [^203] | [The Neglected Tails of Vision-Language Models.](http://arxiv.org/abs/2401.12425) | 本文通过分析预训练文本来衡量视觉语言模型中概念的频率，并发现流行的VLM数据集展示了长尾概念分布，这与按类别的准确率强烈相关。 |
| [^204] | [DQNC2S: DQN-based Cross-stream Crisis event Summarizer.](http://arxiv.org/abs/2401.06683) | 本研究提出了一种基于DQN的在线危机事件摘要生成方法，能够同时总结多个灾害相关的数据流，无需人工标注或内容重新排序，且具有较好的性能表现。 |
| [^205] | [An Information Theoretic Approach to Interaction-Grounded Learning.](http://arxiv.org/abs/2401.05015) | 本文提出了一个基于变分信息的互动基础学习（VI-IGL）方法，用于在强化学习任务中强制执行条件独立性假设。该方法通过学习奖励解码器来最大化上下文-动作（X，A）和反馈变量Y之间的条件互信息。 |
| [^206] | [Comprehensive Exploration of Synthetic Data Generation: A Survey.](http://arxiv.org/abs/2401.02524) | 本文调查了过去十年中417个合成数据生成模型，提供了对模型类型、功能和改进的全面概述。结果表明，模型性能和复杂性增加，以神经网络为主要方法，计算机视觉领域占据主导地位。 |
| [^207] | [Are All Unseen Data Out-of-Distribution?.](http://arxiv.org/abs/2312.16243) | 该论文研究了未见数据的分布在泛化中的挑战，并提出了重新定义OoD数据以及新的泛化上界，保证了对未见数据的模型有效性。 |
| [^208] | [Characteristic Guidance: Non-linear Correction for Diffusion Model at Large Guidance Scale.](http://arxiv.org/abs/2312.07586) | 该论文提出了特征引导方法，用于对大尺度的导向下扩散模型进行非线性校正，以增强对图像生成的控制能力，减少颜色和曝光问题，并在各种应用中显示出效果。 |
| [^209] | [New Online Communities: Graph Deep Learning on Anonymous Voting Networks to Identify Sybils in Polycentric Governance.](http://arxiv.org/abs/2311.17929) | 本研究提供了一个理论框架，通过使用图深度学习技术，可以有效识别去中心化自治组织（DAO）中的虚假身份，为分散治理提供了新的视角。 |
| [^210] | [FIKIT: Priority-Based Real-time GPU Multi-tasking Scheduling with Kernel Identification.](http://arxiv.org/abs/2311.10359) | FIKIT是一种基于优先级的实时GPU多任务调度策略，具有内核识别功能，能够在高优先级任务的内核间填充空闲时间。 |
| [^211] | [The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback.](http://arxiv.org/abs/2311.00168) | 这项研究探讨了强化学习从人类反馈中的目标不匹配问题。研究发现，在强化学习从人类反馈中，奖励模型训练、策略模型训练和策略模型评估之间存在不一致，导致模型行为的意想不到的结果。 |
| [^212] | [Bayesian Optimization with Hidden Constraints via Latent Decision Models.](http://arxiv.org/abs/2310.18449) | 本文介绍了一种基于潜在决策模型的贝叶斯优化方法，通过利用变分自编码器学习可行决策的分布，在原始空间和潜在空间之间实现了双向映射，从而解决了公共决策制定中的隐藏约束问题。 |
| [^213] | [Random Exploration in Bayesian Optimization: Order-Optimal Regret and Computational Efficiency.](http://arxiv.org/abs/2310.15351) | 本论文研究了在贝叶斯优化中使用随机探索的方法，并证明了其能够实现最佳的误差率和最优遗憾保证。同时，所提出的算法通过随机探索避免了每次迭代中非凸获取函数的昂贵优化，具有计算上的优势。 |
| [^214] | [Randomized Forward Mode of Automatic Differentiation for Optimization Algorithms.](http://arxiv.org/abs/2310.14168) | 该论文介绍了一种随机前向模式自动微分优化算法，通过在神经网络的正向传递中计算损失函数的方向导数来更新参数。算法通过采样不同概率分布的随机方向，使用正向模式自动微分计算雅可比向量乘积，并提供了对其收敛速度和计算复杂性的严格分析。 |
| [^215] | [Almost Equivariance via Lie Algebra Convolutions.](http://arxiv.org/abs/2310.13164) | 本文研究了几乎等变性的主题，并提供了一个不同于现有定义的几乎等变性定义，并通过利用李群的李代数给出了在模型中编码几乎等变性的实用方法。 |
| [^216] | [On the Evaluation of Generative Models in Distributed Learning Tasks.](http://arxiv.org/abs/2310.11714) | 本文研究了在具有异构数据分布的分布式学习任务中评估生成模型。通过研究Fr\'echet inception距离（FID），并考虑不同聚合分数，发现FID-all和FID-avg分数的模型排名可能不一致。 |
| [^217] | [Efficient Online Learning with Offline Datasets for Infinite Horizon MDPs: A Bayesian Approach.](http://arxiv.org/abs/2310.11531) | 本文研究了在存在离线数据集的情况下，如何在无限时域进行高效的在线学习。研究表明，学习代理模拟专家的行为策略能够显著减小累积遗憾。通过贝叶斯方法进行的先验相关遗憾分析提供了算法的性能上界，并提出了一种近似的模仿学习算法来结合离线数据集和在线学习。 |
| [^218] | [Entity Matching using Large Language Models.](http://arxiv.org/abs/2310.11244) | 这项研究探讨了使用大型语言模型（LLMs）作为实体匹配的替代方法，相较于预训练的语言模型（PLMs），LLMs对训练数据需求较少且更具鲁棒性。 |
| [^219] | [In-Context Few-Shot Relation Extraction via Pre-Trained Language Models.](http://arxiv.org/abs/2310.11085) | 本研究提出了基于预训练语言模型的上下文少样本关系抽取框架，首次将关系抽取任务重新定义为定制的上下文少样本学习范式。与现有方法相比，该框架不需要命名实体识别和文档人工注释，并且可以轻松更新到新的关系集合。通过评估使用DocRED数据集，验证了该框架的有效性。 |
| [^220] | [Regret Analysis of the Posterior Sampling-based Learning Algorithm for Episodic POMDPs.](http://arxiv.org/abs/2310.10107) | 本文分析了后验采样学习算法在序列化POMDPs中的遗憾性能，并在一定条件下提供了改进的多项式贝叶斯遗憾界。 |
| [^221] | [Lifelong Audio-video Masked Autoencoder with Forget-robust Localized Alignments.](http://arxiv.org/abs/2310.08204) | 这项研究提出了一种终身音视频掩码自编码器，通过引入本地化对齐和抗遗忘的多模态块选择，在不断变化的音视频分布中学习准确的多模态关系。 |
| [^222] | [CAST: Cluster-Aware Self-Training for Tabular Data.](http://arxiv.org/abs/2310.06380) | 本文提出了一种面向表格数据的群集感知自训练方法（CAST），通过规范伪标签的置信度，弥补了自训练算法中的一些弱点，具有普适性和适应性。 |
| [^223] | [A Neural Tangent Kernel View on Federated Averaging for Deep Linear Neural Network.](http://arxiv.org/abs/2310.05495) | 本论文介绍了一种基于神经切向核视角的联邦平均方法在深度线性神经网络上的应用，并探讨了该方法面临的挑战。 |
| [^224] | [A Neural Scaling Law from Lottery Ticket Ensembling.](http://arxiv.org/abs/2310.02258) | 《来自彩票票集成的神经规模定律》通过研究神经规模定律现象，发现其与彩票票集成有关，从而形成了新的缩放定律，具有潜在的影响。 |
| [^225] | [Lessons Learned from EXMOS User Studies: A Technical Report Summarizing Key Takeaways from User Studies Conducted to Evaluate The EXMOS Platform.](http://arxiv.org/abs/2310.02063) | 本技术报告总结了两项用户研究的重要发现，研究探索了在支持医疗专家优化机器学习模型的系统中，全局模型和数据中心解释对于检测和解决潜在的数据相关问题的有效性。 |
| [^226] | [Breaking NoC Anonymity using Flow Correlation Attack.](http://arxiv.org/abs/2309.15687) | 本文研究了NoC架构中现有匿名路由协议的安全性，并展示了现有的匿名路由对基于机器学习的流相关攻击易受攻击。我们提出了一种轻量级的匿名路由，使用流量混淆技术，可以抵御基于机器学习的流相关攻击。 |
| [^227] | [FRAMU: Attention-based Machine Unlearning using Federated Reinforcement Learning.](http://arxiv.org/abs/2309.10283) | FRAMU是一种基于注意力和联邦强化学习的机器遗忘框架，通过自适应学习机制、隐私保护技术和优化策略，处理各种数据源并保持准确性和隐私，适应波动的数据环境，支持持续模型演进。 |
| [^228] | [Graph-enabled Reinforcement Learning for Time Series Forecasting with Adaptive Intelligence.](http://arxiv.org/abs/2309.10186) | 本研究提出了一种使用图神经网络和强化学习监控的新方法，以更准确地预测时间序列数据。该方法能够克服深度学习的限制，并捕捉复杂的时序结构中的时间依赖关系，适用于医疗、交通和天气预测等领域。 |
| [^229] | [Prompting Segmentation with Sound is Generalizable Audio-Visual Source Localizer.](http://arxiv.org/abs/2309.07929) | 本研究提出了一种使用声音提示进行分割的泛化音频-视觉源定位器，在零样本和少样本情况下实现音频-视觉定位和分割任务。通过引入编码器提示解码器范式、构建语义感知音频提示和相关适配器来解决数据稀缺性和不同数据分布的困境。 |
| [^230] | [MagiCapture: High-Resolution Multi-Concept Portrait Customization.](http://arxiv.org/abs/2309.06895) | MagiCapture是一种高分辨率多概念人像定制方法，通过几个主题和风格参考，能够生成高质量的特定风格人像图像。 |
| [^231] | [${\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2308.11842) | 本文利用欧几里德对称性，设计了一种基于${\rm E}(3)$等变性的协作多智体强化学习的演员-评论家方法，通过对称约束的神经网络结构，实现了在各种协作MARL基准测试中的卓越性能和令人印象深刻的泛化能力。 |
| [^232] | [Label Propagation Techniques for Artifact Detection in Imbalanced Classes using Photoplethysmogram Signals.](http://arxiv.org/abs/2308.08480) | 研究探索了在光电容积描记信号中利用标签传播技术进行不平衡类别中伪迹检测的方法，并证明其在标记医疗数据集和伪迹分类方面的有效性。 |
| [^233] | [You Shall not Pass: the Zero-Gradient Problem in Predict and Optimize for Convex Optimization.](http://arxiv.org/abs/2307.16304) | 这篇论文介绍了预测和优化中的一个问题——零梯度问题，并提出了解决方法。通过利用微分优化的数学特性和真实世界基准的验证，该方法解决了这个问题。 |
| [^234] | [Onion Universe Algorithm: Applications in Weakly Supervised Learning.](http://arxiv.org/abs/2307.04870) | 洋葱宇宙算法是一种新颖的集成学习分类方法，可作为弱监督学习的标签模型。它在实现上简单、计算高效，适用于无完全标记数据的情况。经验证明，它在常见的基准数据集上表现出色。 |
| [^235] | [Controlling Chaotic Maps using Next-Generation Reservoir Computing.](http://arxiv.org/abs/2307.03813) | 这项工作将非线性系统控制技术与储层计算相结合，成功地在混沌H\'enon映射上展示了控制器对于控制系统的稳定、固定点的控制和任意期望状态的控制的性能，并且只需10个数据点进行训练，单次迭代就能控制到期望轨迹，并且具有鲁棒性。 |
| [^236] | [Learning to Communicate using Contrastive Learning.](http://arxiv.org/abs/2307.01403) | 本研究提出了一种使用对比学习进行通信的方法，在分散的环境中通过最大化反映发送和接收消息关系的互信息来学习通信。在通信关键的环境中，我们的方法在性能和学习速度方面优于先前的工作，并且能够捕获全局状态信息，实现了更对称的通信。 |
| [^237] | [Deep graph kernel point processes.](http://arxiv.org/abs/2306.11313) | 本文提出了一种基于潜在图拓扑的图点过程方法，并开发了一种新颖的深度图核来描述事件之间的触发和抑制效应，该方法在合成和实际数据集上具有优越性。 |
| [^238] | [Towards Quantum Federated Learning.](http://arxiv.org/abs/2306.09912) | 量子联邦学习通过将量子计算和联邦学习原理相结合，旨在利用量子技术增强学习过程中的隐私、安全和效率，并通过独特的分类法分类总结了这一快速发展领域的技术特点和未来研究方向。 |
| [^239] | [K-Tensors: Clustering Positive Semi-Definite Matrices.](http://arxiv.org/abs/2306.06534) | 本文介绍了一种针对正半定矩阵的自一致性聚类算法（K-张量），通过考虑其特征结构，能够有效地将正半定矩阵进行分区。 |
| [^240] | [Ordinal Potential-based Player Rating.](http://arxiv.org/abs/2306.05366) | 该论文提出了一种使用序数势函数的玩家评级方法，通过可逆映射的嵌套计算，能够保持传递性。 |
| [^241] | [On the Identification and Optimization of Nonsmooth Superposition Operators in Semilinear Elliptic PDEs.](http://arxiv.org/abs/2306.05185) | 本文研究了在低正则性情况下，如何识别半线性椭圆PDE中的Nemytskii算子以及如何解决这个优化问题。这对于进行有关PDE的神经网络训练问题有很好的启示作用。 |
| [^242] | [Conditional Diffusion Models for Semantic 3D Medical Image Synthesis.](http://arxiv.org/abs/2305.18453) | 这篇论文提出了Med-DDPM，一种使用扩散模型进行语义化三维医学图像合成的创新解决方案，它通过控制像素级掩码标签的生成过程，能够生成高质量逼真的医学图像，并且在精度、稳定性和多样性等指标上优于GAN技术，也优于传统的增强技术和GAN合成图像。 |
| [^243] | [On the Computational Power of Decoder-Only Transformer Language Models.](http://arxiv.org/abs/2305.17026) | 本篇论文研究了解码器Transformer语言模型的计算普适性，表明即使只有单层和单注意力头，仍然具有图灵完备性，其中单词嵌入的稀疏性/可压缩性是必要条件。 |
| [^244] | [Learning Directed Graphical Models with Optimal Transport.](http://arxiv.org/abs/2305.15927) | 通过最优传输的视角提供了参数学习问题的新视图，可以在许多有向图上进行操作并表现出灵活性和多功能性。 |
| [^245] | [How to escape sharp minima.](http://arxiv.org/abs/2305.15659) | 本文探讨了发现平坦极小值点的算法问题，在支持找到局部近似平坦的极小值点的基础上，设计了两种算法：一种基于梯度的算法，一种基于最小化锐度的算法。 |
| [^246] | [Variational Gradient Descent using Local Linear Models.](http://arxiv.org/abs/2305.15577) | 本文提出了使用局部线性模型实现目标和粒子分布KL散度降低的新估计器，可以使用样本进行计算而不需要目标得分函数，具有比SVGD更简单有效的计算方法，对于高维度情况下的模型也有优化，提升估计精度。 |
| [^247] | [BertRLFuzzer: A BERT and Reinforcement Learning based Fuzzer.](http://arxiv.org/abs/2305.12534) | BertRLFuzzer是一种基于BERT和强化学习的Fuzzer工具，旨在发现Web应用程序的安全漏洞。通过使用BERT模型作为代理来指导Fuzzer进行高效学习，BertRLFuzzer相对于其他黑盒和白盒Fuzzer在时间到首次攻击、新漏洞发现和攻击率方面都取得了显著的改进。 |
| [^248] | [Multi-Relational Hyperbolic Word Embeddings from Natural Language Definitions.](http://arxiv.org/abs/2305.07303) | 本论文提出了一种从自然语言定义中学习多关系双曲词向量的框架，以捕捉由定义所引起的分层和多分辨率结构。 |
| [^249] | [Machine Learning with Requirements: a Manifesto.](http://arxiv.org/abs/2304.03674) | 本文提出一个带需求的机器学习宣言，认为需求定义和满足可以在很大程度上使机器学习模型更适用于现实世界中的关键领域，作者提出了两个问题，其中（i）需求自然而然地出现，（ii）机器学习模型被或可以成功地部署，并且（iii）忽略需求可能会产生严重后果，提出了一种新型的金字塔式开发流程，在其中，需求定义可能会影响到流程中所有后续阶段，反之亦然。 |
| [^250] | [Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?.](http://arxiv.org/abs/2303.18240) | 该研究研究了使用预训练视觉表征来实现身体智能的最新进展。他们展示了最大、最全面的经验研究，发现没有一种表征是普遍优越的，并且数据集的大小和多样性并不能普遍改善性能。 |
| [^251] | [VIDIMU. Multimodal video and IMU kinematic dataset on daily life activities using affordable devices.](http://arxiv.org/abs/2303.16150) | VIDIMU数据集使用商品相机和自定义传感器记录13种临床相关性的活动，为远程日常生活活动识别和运动学分析提供价格实惠的患者追踪解决方案。 |
| [^252] | [Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration.](http://arxiv.org/abs/2303.11435) | InDI是一种新的监督式图像修复公式，通过逐步改进图像质量来生成比现有回归方法更真实和详细的图像，修复效果更具有感知质量。 |
| [^253] | [Partial Neural Optimal Transport.](http://arxiv.org/abs/2303.07988) | 我们提出了一种新的神经方法来计算部分最优输运映射，并在合成例子上进行了测试。 |
| [^254] | [Task Aware Dreamer for Task Generalization in Reinforcement Learning.](http://arxiv.org/abs/2303.05092) | 本文提出了一种名为Task Aware Dreamer（TAD）的方法用于强化学习中的任务泛化。通过量化任务分布的相关性，TAD能够将历史信息编码到策略中，以便区分不同任务，并在泛化到未见任务时具有较好的性能。 |
| [^255] | [Improving Monte Carlo Evaluation with Offline Data.](http://arxiv.org/abs/2301.13734) | 本论文介绍了通过使用离线数据来提升蒙特卡罗评估方法，实现在保持相同估计准确度的前提下，减少在线样本数量的目的。通过使用一个定制的行为策略，可以比普通的 MC 估计器产生更小的方差。该行为策略可以从现有的离线数据中高效学习，我们的实验表明，相对于现有的最先进方法，我们的方法只需使用小部分在线样本就能实现相同的估计精度。 |
| [^256] | [I Prefer not to Say: Protecting User Consent in Models with Optional Personal Data.](http://arxiv.org/abs/2210.13954) | 该论文研究了个人可以选择与决策系统共享可选个人信息的机器学习模型，并提出了保护用户同意的PUC概念，为用户隐私保护提供了有力的解决方案。 |
| [^257] | [Measures of Information Reflect Memorization Patterns.](http://arxiv.org/abs/2210.09404) | 本研究通过信息论度量神经网络中神经元激活的多样性，发现其与模型的泛化能力和记忆之间存在关联。实验证明，即使在未标记的分布内部计算神经激活时，信息的组织也可以指向两种形式的记忆。 |
| [^258] | [Bringing robotics taxonomies to continuous domains via GPLVM on hyperbolic manifolds.](http://arxiv.org/abs/2210.01672) | 本论文提出了一种通过在双曲流形上使用GPLVM来在连续领域中应用机器人分类法的方法，通过捕捉相关层次结构的双曲嵌入来建模分类数据，并采用图形先验和保持距离的后向约束来实现分类法结构的纳入。 |
| [^259] | [Auto-Encoding Adversarial Imitation Learning.](http://arxiv.org/abs/2206.11004) | 自动编码对抗性模仿学习（AEAIL）是一种稳健且可扩展的方法，利用自动编码器的重构误差作为奖励信号来优化策略。在基于状态和基于图像的环境中，AEAIL比现有方法表现更好，并且对于噪声示范专家具有更好的稳健性。 |

# 详细

[^1]: kNN算法用于条件均值和方差估计，具有自动不确定性量化和变量选择

    kNN Algorithm for Conditional Mean and Variance Estimation with Automated Uncertainty Quantification and Variable Selection

    [https://rss.arxiv.org/abs/2402.01635](https://rss.arxiv.org/abs/2402.01635)

    本文介绍了一种利用kNN算法进行条件均值和方差估计的方法，该方法采用了自动不确定性量化和变量选择技术，提高了估计的准确性和性能。

    

    本文介绍了一种基于kNN的回归方法，将传统的非参数kNN模型的可扩展性和适应性与一种新的变量选择技术相结合。该方法主要目标是准确估计随机响应变量的条件均值和方差，从而有效地描述各种情景下的条件分布。我们的方法包含了一个健壮的不确定性量化机制，利用我们之前关于条件均值和方差的估计工作。 kNN的应用确保了在预测区间时可扩展的计算效率和与最优非参数速率相一致的统计准确性。此外，我们引入了一种新的kNN半参数算法来估计考虑协变量的ROC曲线。对于选择平滑参数k，我们提出了一个具有理论保证的算法。变量选择的引入显著提高了该方法相对于传统方法的性能。

    In this paper, we introduce a kNN-based regression method that synergizes the scalability and adaptability of traditional non-parametric kNN models with a novel variable selection technique. This method focuses on accurately estimating the conditional mean and variance of random response variables, thereby effectively characterizing conditional distributions across diverse scenarios.Our approach incorporates a robust uncertainty quantification mechanism, leveraging our prior estimation work on conditional mean and variance. The employment of kNN ensures scalable computational efficiency in predicting intervals and statistical accuracy in line with optimal non-parametric rates. Additionally, we introduce a new kNN semi-parametric algorithm for estimating ROC curves, accounting for covariates. For selecting the smoothing parameter k, we propose an algorithm with theoretical guarantees.Incorporation of variable selection enhances the performance of the method significantly over convention
    
[^2]: 超越尺度：具有任意类型未知超参数的无遗憾贝叶斯优化

    Beyond Lengthscales: No-regret Bayesian Optimisation With Unknown Hyperparameters Of Any Type

    [https://rss.arxiv.org/abs/2402.01632](https://rss.arxiv.org/abs/2402.01632)

    这篇论文提出了一种新的贝叶斯优化算法，可以处理具有任意类型未知超参数的情况，并具有无遗憾特性。

    

    贝叶斯优化需要拟合高斯过程模型，而拟合高斯过程模型需要指定超参数 - 大部分理论文献假设这些超参数是已知的。之前的理论研究通常假设数据在空间中均匀填充，而常用的高斯过程超参数的最大似然估计器只有在这种情况下才是一致的。然而，在贝叶斯优化中，数据不一定满足这种均匀填充的条件。由于无法保证超参数估计的正确性，并且这些超参数可以显著影响高斯过程拟合，因此对具有未知超参数的贝叶斯优化进行理论分析非常具有挑战性。之前提出的具有无遗憾特性的算法仅能处理特殊情况下的未知长度尺度、再生核希尔伯特空间范数，并且仅适用于频率派的情况。我们提出了一种新的算法，命名为HE-GP-UCB，它是第一个具有无遗憾特性的算法，在具有未知超参数的情况下实现了贝叶斯优化。

    Bayesian optimisation requires fitting a Gaussian process model, which in turn requires specifying hyperparameters - most of the theoretical literature assumes those hyperparameters are known. The commonly used maximum likelihood estimator for hyperparameters of the Gaussian process is consistent only if the data fills the space uniformly, which does not have to be the case in Bayesian optimisation. Since no guarantees exist regarding the correctness of hyperparameter estimation, and those hyperparameters can significantly affect the Gaussian process fit, theoretical analysis of Bayesian optimisation with unknown hyperparameters is very challenging. Previously proposed algorithms with the no-regret property were only able to handle the special case of unknown lengthscales, reproducing kernel Hilbert space norm and applied only to the frequentist case. We propose a novel algorithm, HE-GP-UCB, which is the first algorithm enjoying the no-regret property in the case of unknown hyperparame
    
[^3]: 《论文标题：广义语法规则和基于结构的一般化——对于词汇任务和转导的经典等变性之外的一般化的立场论文》

    Position Paper: Generalized grammar rules and structure-based generalization beyond classical equivariance for lexical tasks and transduction

    [https://rss.arxiv.org/abs/2402.01629](https://rss.arxiv.org/abs/2402.01629)

    这篇论文提出了一个通用框架，其中使用广义语法规则（GGRs）来实现组合一般化，将其视为转导任务中的对称性约束。该框架不仅形式化了语言转导的广义对称性概念，还与强化学习和其他研究领域有关联。

    

    组合一般化是人类学习词汇与现有神经网络之间的主要差异之一。我们提出了一个通用框架，用于构建能够使用广义语法规则（GGRs）进行组合一般化的模型，GGRs是一类基于对称性的转导任务的组合约束，我们将其视为受物理学任务中等变性约束启发的转导类比。除了为语言转导形式化广义的对称性概念之外，我们的框架足够通用，可以包含许多现有工作作为特例。我们提出了关于如何实现GGRs的想法，并在此过程中与强化学习和其他研究领域建立了联系。

    Compositional generalization is one of the main properties which differentiates lexical learning in humans from state-of-art neural networks. We propose a general framework for building models that can generalize compositionally using the concept of Generalized Grammar Rules (GGRs), a class of symmetry-based compositional constraints for transduction tasks, which we view as a transduction analogue of equivariance constraints in physics-inspired tasks. Besides formalizing generalized notions of symmetry for language transduction, our framework is general enough to contain many existing works as special cases. We present ideas on how GGRs might be implemented, and in the process draw connections to reinforcement learning and other areas of research.
    
[^4]: 针对深度模型零阶优化的随机两点法

    Stochastic Two Points Method for Deep Model Zeroth-order Optimization

    [https://rss.arxiv.org/abs/2402.01621](https://rss.arxiv.org/abs/2402.01621)

    本文介绍了一种针对大型深度模型的零阶优化方法——随机两点法，通过前向传递来更新模型。并且通过理论分析和实验证明了其在优化目标上的高效性并超越其他方法。

    

    大型基础模型，例如大型语言模型，在各种应用场景中表现出色。由于硬件预算或缺乏反向传播的访问权限，构建或完全微调这样的大模型通常是不可行的。零阶方法为解决这一挑战提供了一种有希望的方向，它只需要前向传递来更新模型。本文在无梯度情形下引入了一种高效的随机两点（S2P）方法。我们在一般和放松的平滑性假设下提出了S2P的理论收敛性质。理论性质还揭示了更快、更稳定的S2P变体——加速S2P（AS2P），通过利用我们的新收敛性质，更好地表示了深度模型在训练中的动力学。我们全面的实证结果表明，AS2P在优化大型深度模型（包括语言模型）的目标上非常有效，并且优于其他方法。

    Large foundation models, such as large language models, have performed exceptionally well in various application scenarios. Building or fully fine-tuning such large models is usually prohibitive due to either hardware budget or lack of access to backpropagation. The zeroth-order methods offer a promising direction for tackling this challenge, where only forward passes are needed to update the model. This paper introduces an efficient Stochastic Two-Point (S2P) approach within the gradient-free regime. We present the theoretical convergence properties of S2P under the general and relaxed smoothness assumptions. The theoretical properties also shed light on a faster and more stable S2P variant, Accelerated S2P (AS2P), through exploiting our new convergence properties that better represent the dynamics of deep models in training. Our comprehensive empirical results show that AS2P is highly effective in optimizing objectives for large deep models, including language models, and outperforms
    
[^5]: 一种基于高斯过程的鲁棒运动规划框架，用于未知环境中敏捷自主机器人的导航和恢复

    A GP-based Robust Motion Planning Framework for Agile Autonomous Robot Navigation and Recovery in Unknown Environments

    [https://rss.arxiv.org/abs/2402.01617](https://rss.arxiv.org/abs/2402.01617)

    这篇论文提出了一种基于高斯过程的鲁棒运动规划框架，用于敏捷自主机器人在未知环境中的导航和恢复。该框架能够主动预测规划失败的风险并进行恢复，同时能在仿真和真实环境中产生灵活的运动。

    

    对于自主移动机器人来说，环境和系统模型的不确定性可能导致运动规划的失败，进而造成潜在的碰撞风险。为了实现高度鲁棒的自主性，这些机器人应该能够主动预测和恢复这种失败。为此，我们提出了一种基于高斯过程的模型，用于主动检测未来运动规划失败的风险。当此风险超过一定阈值时，会触发一种恢复行为，利用同一高斯过程模型寻找一个安全状态，从而使机器人能够继续朝着目标前进。所提出的方法仅在仿真中进行训练，可以推广到不同机器人平台上的真实环境中。仿真和物理实验表明，我们的框架能够预测规划器失败并将机器人恢复到可能成功规划的状态，同时实现灵活的运动。

    For autonomous mobile robots, uncertainties in the environment and system model can lead to failure in the motion planning pipeline, resulting in potential collisions. In order to achieve a high level of robust autonomy, these robots should be able to proactively predict and recover from such failures. To this end, we propose a Gaussian Process (GP) based model for proactively detecting the risk of future motion planning failure. When this risk exceeds a certain threshold, a recovery behavior is triggered that leverages the same GP model to find a safe state from which the robot may continue towards the goal. The proposed approach is trained in simulation only and can generalize to real world environments on different robotic platforms. Simulations and physical experiments demonstrate that our framework is capable of both predicting planner failures and recovering the robot to states where planner success is likely, all while producing agile motion.
    
[^6]: L2G2G: 一种可扩展的局部到全局网络嵌入方法，基于图自动编码器

    L2G2G: a Scalable Local-to-Global Network Embedding with Graph Autoencoders

    [https://rss.arxiv.org/abs/2402.01614](https://rss.arxiv.org/abs/2402.01614)

    L2G2G是一种可扩展的局部到全局网络嵌入方法，通过动态同步潜在节点表示以提高GAE的准确性，并利用解码器计算只有本地图块损失，从而更好地利用了图的信息。

    

    对于分析实际网络，图表示学习是一种常用工具。这些方法，如图自动编码器(GAE)，通常依赖于低维表示，也称为嵌入，通过最小化损失函数获得;这些嵌入与解码器一起用于节点分类和边预测等下游任务。虽然GAE往往相当准确，但存在可扩展性问题。为了改善速度，Local2Global方法通过基于特征向量同步的图块嵌入相结合，显示出快速且准确的效果。在这里，我们提出了L2G2G，一种_Local2Global方法，它在不牺牲可扩展性的情况下提高了GAE的准确性。这种改进是通过在训练GAE期间动态同步潜在节点表示来实现的。它还受益于解码器计算只有本地图块损失。因此，每个时代中的本地嵌入对齐利用了更多来自图的信息。

    For analysing real-world networks, graph representation learning is a popular tool. These methods, such as a graph autoencoder (GAE), typically rely on low-dimensional representations, also called embeddings, which are obtained through minimising a loss function; these embeddings are used with a decoder for downstream tasks such as node classification and edge prediction. While GAEs tend to be fairly accurate, they suffer from scalability issues. For improved speed, a Local2Global approach, which combines graph patch embeddings based on eigenvector synchronisation, was shown to be fast and achieve good accuracy. Here we propose L2G2G, a Local2Global method which improves GAE accuracy without sacrificing scalability. This improvement is achieved by dynamically synchronising the latent node representations, while training the GAEs. It also benefits from the decoder computing an only local patch loss. Hence, aligning the local embeddings in each epoch utilises more information from the gr
    
[^7]: 连接的电动汽车网格的容错分析在工业微电网中使用高效控制方案进行主频率控制

    Contingency Analysis of a Grid of Connected EVs for Primary Frequency Control of an Industrial Microgrid Using Efficient Control Scheme

    [https://rss.arxiv.org/abs/2402.01608](https://rss.arxiv.org/abs/2402.01608)

    本研究提出了一种用于工业微电网的高效控制方案，通过连接的电动汽车网格实现主频率控制和拥塞管理。这项技术通过将电动汽车用作负荷和源，利用其快速调节能力来提高电网可靠性。

    

    在燃油汽车统治交通领域一个多世纪后，由于一系列优势，包括更低运营成本和更低的CO2排放，电动汽车似乎即将得到推广。通过使用车辆到电网（如果使用电动汽车作为负荷，则为电网到车辆）的方法，电动汽车可以同时作为负荷和源进行运行。主频率调节和拥塞管理是该技术添加到工业微电网中的两个重要特性。工业微电网由不同的能源源，如风电场和光伏电场，储能系统和负载组成。电动汽车由于其快速调节能力而作为频率管理技术引起了很大的兴趣。电网可靠性取决于这种快速反应。本研究考虑了不同的事故，电动汽车的电池状态和电动汽车队中电动汽车的数量的变化，并提出了一种用于主频率控制的控制方案。

    After over a century of internal combustion engines ruling the transport sector, electric vehicles appear to be on the verge of gaining traction due to a slew of advantages, including lower operating costs and lower CO2 emissions. By using the Vehicle-to-Grid (or Grid-to-Vehicle if Electric vehicles (EVs) are utilized as load) approach, EVs can operate as both a load and a source. Primary frequency regulation and congestion management are two essential characteristics of this technology that are added to an industrial microgrid. Industrial Microgrids are made up of different energy sources such as wind farms and PV farms, storage systems, and loads. EVs have gained a lot of interest as a technique for frequency management because of their ability to regulate quickly. Grid reliability depends on this quick reaction. Different contingency, state of charge of the electric vehicles, and a varying number of EVs in an EV fleet are considered in this work, and a proposed control scheme for fr
    
[^8]: 具有必要回溯的自然反事实

    Natural Counterfactuals With Necessary Backtracking

    [https://rss.arxiv.org/abs/2402.01607](https://rss.arxiv.org/abs/2402.01607)

    本研究提出了一种自然反事实框架和方法，通过优化控制回溯的范围，生成与实际世界的数据分布相匹配的自然反事实，从而改进了反事实推理。

    

    反事实推理对于人类认知非常重要，尤其对于提供解释和做出决策至关重要。尽管Judea Pearl的研究方法在理论上很优雅，但其生成反事实情景往往需要过于脱离实际情景的干预，因此难以实施。为了解决这个问题，我们提出了一种自然反事实的框架和一种根据实际世界数据分布生成自然反事实的方法。我们的方法提供了对反事实推理的改进，允许对因果前置变量进行改变以最小化与实际情景的偏差。为了生成自然反事实，我们引入了一种创新的优化框架，通过自然性准则允许但控制回溯的范围。实证实验表明了我们方法的有效性。

    Counterfactual reasoning is pivotal in human cognition and especially important for providing explanations and making decisions. While Judea Pearl's influential approach is theoretically elegant, its generation of a counterfactual scenario often requires interventions that are too detached from the real scenarios to be feasible. In response, we propose a framework of natural counterfactuals and a method for generating counterfactuals that are natural with respect to the actual world's data distribution. Our methodology refines counterfactual reasoning, allowing changes in causally preceding variables to minimize deviations from realistic scenarios. To generate natural counterfactuals, we introduce an innovative optimization framework that permits but controls the extent of backtracking with a naturalness criterion. Empirical experiments indicate the effectiveness of our method.
    
[^9]: 从两个十年的血压数据中学习：跨越7500万患者就诊的不同人群模式

    Learning from Two Decades of Blood Pressure Data: Demography-Specific Patterns Across 75 Million Patient Encounters

    [https://rss.arxiv.org/abs/2402.01598](https://rss.arxiv.org/abs/2402.01598)

    这项研究通过分析两个十年的庞大数据集，发现基于性别的血压变化不显著，挑战了传统假设；同时，舒张压随着年龄增长而持续增加，而收缩压在四十多岁的年龄组显示出一个独特的峰值。

    

    高血压仍然是全球关注的健康问题，其患病率不断上升，因此需要有效的监测和理解血压动态。本研究深入探讨了从血压测量中获得的大量信息，这是了解高血压趋势的重要途径。许多研究已经报道了血压变化与各种因素的关系。在这项研究中，我们利用了一份涵盖了两个十年的7500万记录的庞大数据集，为探索和分析不同人群特征，如年龄、种族和性别之间的血压变化提供了独特机会。我们的研究发现，基于性别的血压变化在统计上并不显著，挑战了传统的假设。有趣的是，舒张压（SBP）随着年龄的增长而持续增加，而舒张压（DBP）在四十多岁的年龄组显示出一个独特的峰值。此外，我们的分析还揭示了分布模式中的一些有趣的相似性。

    Hypertension remains a global health concern with a rising prevalence, necessitating effective monitoring and understanding of blood pressure (BP) dynamics. This study delves into the wealth of information derived from BP measurement, a crucial approach in informing our understanding of hypertensive trends. Numerous studies have reported on the relationship between BP variation and various factors. In this research, we leveraged an extensive dataset comprising 75 million records spanning two decades, offering a unique opportunity to explore and analyze BP variations across demographic features such as age, race, and gender. Our findings revealed that gender-based BP variation was not statistically significant, challenging conventional assumptions. Interestingly, systolic blood pressure (SBP) consistently increased with age, while diastolic blood pressure (DBP) displayed a distinctive peak in the forties age group. Moreover, our analysis uncovered intriguing similarities in the distribu
    
[^10]: TrustAgent: 通过代理构成实现安全可信赖的LLM代理

    TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution

    [https://rss.arxiv.org/abs/2402.01586](https://rss.arxiv.org/abs/2402.01586)

    本文介绍了一种基于代理构成的代理框架TrustAgent，该框架通过预先规划、规划过程中和计划后检查三种策略来提高LLM代理的安全性。实验结果表明，这些方法可以有效识别和预防潜在危险。此外，还研究了安全性与使用者满意度以及模型推理能力与效率之间的关系。

    

    近年来，基于LLM的代理引起了广泛关注，但其可信度仍未得到深入探索。由于代理可以直接与物理环境交互，其可靠性和安全性至关重要。本文提出了一种基于代理构成的代理框架TrustAgent，对LLM代理的安全性维度进行了初步研究。该框架包括三种策略：预先规划策略，在生成计划之前向模型注入安全知识；规划过程中策略，在生成计划时增强安全性；计划后检查策略，通过计划后检查确保安全性。通过实验分析，我们展示了这些方法如何通过识别和预防潜在危险有效提高LLM代理的安全性。此外，我们还探讨了安全性与使用者满意度之间的复杂关系，以及模型的推理能力与其效率之间的关联。

    The emergence of LLM-based agents has garnered considerable attention, yet their trustworthiness remains an under-explored area. As agents can directly interact with the physical environment, their reliability and safety is critical. This paper presents an Agent-Constitution-based agent framework, TrustAgent, an initial investigation into improving the safety dimension of trustworthiness in LLM-based agents. This framework consists of threefold strategies: pre-planning strategy which injects safety knowledge to the model prior to plan generation, in-planning strategy which bolsters safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Through experimental analysis, we demonstrate how these approaches can effectively elevate an LLM agent's safety by identifying and preventing potential dangers. Furthermore, we explore the intricate relationships between safety and helpfulness, and between the model's reasoning ability and its efficac
    
[^11]: Spiking Music: 基于事件的自编码器进行音频压缩

    Spiking Music: Audio Compression with Event Based Auto-encoders

    [https://rss.arxiv.org/abs/2402.01571](https://rss.arxiv.org/abs/2402.01571)

    本研究利用基于事件的自编码器实现了音频压缩，并证明了其在稀疏状态下具有较高效率和自主产生的选择性和同步。

    

    大脑中的神经元通过称为脉冲的点时间事件传递信息。脉冲的时序被认为包含丰富的信息，但如何利用这些信息在数字系统中还不清楚。我们证明了基于事件的编码对音频压缩是有效的。为了构建这种基于事件的表示，我们使用了深度二进制自编码器，并在高稀疏约束下，模型进入一种稀疏矩阵存储算法更高效存储二进制事件矩阵的状态。我们在大型MAESTRO钢琴录音数据集上将其与矢量量化自编码器进行了测试。我们的“Spiking Music压缩”算法不仅在压缩/重构的权衡上取得了有竞争力的结果，而且在稀疏状态下，在编码的事件和钢琴按键击打之间出现了自主产生的选择性和同步。

    Neurons in the brain communicate information via punctual events called spikes. The timing of spikes is thought to carry rich information, but it is not clear how to leverage this in digital systems. We demonstrate that event-based encoding is efficient for audio compression. To build this event-based representation we use a deep binary auto-encoder, and under high sparsity pressure, the model enters a regime where the binary event matrix is stored more efficiently with sparse matrix storage algorithms. We test this on the large MAESTRO dataset of piano recordings against vector quantized auto-encoders. Not only does our "Spiking Music compression" algorithm achieve a competitive compression/reconstruction trade-off, but selectivity and synchrony between encoded events and piano key strikes emerge without supervision in the sparse regime.
    
[^12]: 通过在线学习更新理解Adam优化器：Adam是伪装成FTRL的

    Understanding Adam Optimizer via Online Learning of Updates: Adam is FTRL in Disguise

    [https://rss.arxiv.org/abs/2402.01567](https://rss.arxiv.org/abs/2402.01567)

    本文通过在线学习的视角，揭示了Adam优化器的构成和算法组成的重要性，发现Adam实际上是伪装成FTRL的，研究了在线学习的角度对其算法组成的好处。

    

    尽管Adam优化器在实践中取得了成功，但对其算法组成的理论理解仍然有限。特别是，大多数现有的对Adam的分析仅显示了可以简单地通过非自适应算法如SGD实现的收敛速度。在本文中，我们提供了一种基于在线学习的不同视角，强调了Adam算法的重要性。受Cutkosky等人（2023）的启发，我们考虑了一个称为在线学习更新的框架，其中我们根据在线学习者选择优化器的更新。在这个框架下，设计一个好的优化器就等同于设计一个好的在线学习者。我们的主要观察是，Adam对应于一种被称为Follow-the-Regularized-Leader (FTRL)的原则性在线学习框架。基于这一观察，我们从在线学习的角度研究了其算法组成的好处。

    Despite the success of the Adam optimizer in practice, the theoretical understanding of its algorithmic components still remains limited. In particular, most existing analyses of Adam show the convergence rate that can be simply achieved by non-adative algorithms like SGD. In this work, we provide a different perspective based on online learning that underscores the importance of Adam's algorithmic components. Inspired by Cutkosky et al. (2023), we consider the framework called online learning of updates, where we choose the updates of an optimizer based on an online learner. With this framework, the design of a good optimizer is reduced to the design of a good online learner. Our main observation is that Adam corresponds to a principled online learning framework called Follow-the-Regularized-Leader (FTRL). Building on this observation, we study the benefits of its algorithmic components from the online learning perspective.
    
[^13]: 面向住宅短期负荷预测的隐私保护分布式学习

    Privacy-Preserving Distributed Learning for Residential Short-Term Load Forecasting

    [https://rss.arxiv.org/abs/2402.01546](https://rss.arxiv.org/abs/2402.01546)

    该论文提出了一种面向住宅短期负荷预测的隐私保护分布式学习方法，通过采用安全聚合算法和多方计算密码学技术来减轻梯度泄漏的风险，并解决了现有联邦学习模型的脆弱性和对新兴攻击技术的容易受到的问题。

    

    在电力系统领域，住宅用户在负荷预测应用中的日益增加使得数据隐私问题日趋关注。具体而言，负荷数据可能意外地泄露住宅用户的日常生活习惯，从而对其财产安全构成风险。虽然联邦学习（FL）已被用来通过在不交换原始数据的情况下进行模型训练来保护用户隐私，但这些FL模型对新兴的攻击技术（如梯度泄漏攻击和毒化攻击）显示出了脆弱性。为了抵御这些攻击，我们首先采用了一种安全聚合（SecAgg）算法，利用多方计算密码学技术来减轻梯度泄漏的风险。然而，SecAgg的引入需要部署额外的子中心服务器来执行多方计算协议，从而增加了计算复杂度，降低了系统的鲁棒性，特别是在特定场景中。

    In the realm of power systems, the increasing involvement of residential users in load forecasting applications has heightened concerns about data privacy. Specifically, the load data can inadvertently reveal the daily routines of residential users, thereby posing a risk to their property security. While federated learning (FL) has been employed to safeguard user privacy by enabling model training without the exchange of raw data, these FL models have shown vulnerabilities to emerging attack techniques, such as Deep Leakage from Gradients and poisoning attacks. To counteract these, we initially employ a Secure-Aggregation (SecAgg) algorithm that leverages multiparty computation cryptographic techniques to mitigate the risk of gradient leakage. However, the introduction of SecAgg necessitates the deployment of additional sub-center servers for executing the multiparty computation protocol, thereby escalating computational complexity and reducing system robustness, especially in scenario
    
[^14]: 针对缺失数据预测的自适应优化方法

    Adaptive Optimization for Prediction with Missing Data

    [https://rss.arxiv.org/abs/2402.01543](https://rss.arxiv.org/abs/2402.01543)

    本文提出了一种针对缺失数据预测的自适应优化方法，通过自适应线性回归模型来适应观测特征集，并将填充规则和回归模型同时学习，相比顺序学习方法，在数据非完全随机缺失情况下，方法实现了2-10%的准确性改进。

    

    在训练具有缺失条目的预测模型时，最常用和多功能的方法是一种流水线技术，首先填充缺失条目，然后计算预测结果。本文将缺失数据预测视为一个两阶段的自适应优化问题，并提出了一种新的模型类别，自适应线性回归模型，其中回归系数能够适应观测特征集。我们表明一些自适应线性回归模型等同于同时学习填充规则和下游线性回归模型而不是顺序学习。我们利用这种联合填充-回归的解释将我们的框架推广到非线性模型。在数据非完全随机缺失的情况下，我们的方法在样外准确性方面实现了2-10%的改进。

    When training predictive models on data with missing entries, the most widely used and versatile approach is a pipeline technique where we first impute missing entries and then compute predictions. In this paper, we view prediction with missing data as a two-stage adaptive optimization problem and propose a new class of models, adaptive linear regression models, where the regression coefficients adapt to the set of observed features. We show that some adaptive linear regression models are equivalent to learning an imputation rule and a downstream linear regression model simultaneously instead of sequentially. We leverage this joint-impute-then-regress interpretation to generalize our framework to non-linear models. In settings where data is strongly not missing at random, our methods achieve a 2-10% improvement in out-of-sample accuracy.
    
[^15]: 使用标记数据增强的测地插值方法学习蛋白质折叠的集体变量

    Learning Collective Variables for Protein Folding with Labeled Data Augmentation through Geodesic Interpolation

    [https://rss.arxiv.org/abs/2402.01542](https://rss.arxiv.org/abs/2402.01542)

    本实验提出了一种使用标记数据增强和测地插值方法学习蛋白质折叠的集体变量的策略，有效提高了采样效率，并在过渡态数据有限且嘈杂时表现优于基于分类器的方法。

    

    在分子动力学（MD）模拟中，通常通过增强采样技术来研究蛋白质折叠等罕见事件，其中大部分依赖于沿着加速发生的集体变量（CV）的定义。获得富有表达力的CV至关重要，但往往受到关于特定事件的信息不足的阻碍，例如从未折叠到折叠构象的转变。我们提出了一种模拟无关的数据增强策略，利用受物理启发的度量来生成类似蛋白质折叠转变的测地插值，从而提高采样效率，而无需真实的过渡态样本。通过利用插值进度参数，我们引入了基于回归的学习方案来构建CV模型，当过渡态数据有限且嘈杂时，该方法表现优于基于分类器的方法。

    In molecular dynamics (MD) simulations, rare events, such as protein folding, are typically studied by means of enhanced sampling techniques, most of which rely on the definition of a collective variable (CV) along which the acceleration occurs. Obtaining an expressive CV is crucial, but often hindered by the lack of information about the particular event, e.g., the transition from unfolded to folded conformation. We propose a simulation-free data augmentation strategy using physics-inspired metrics to generate geodesic interpolations resembling protein folding transitions, thereby improving sampling efficiency without true transition state samples. Leveraging interpolation progress parameters, we introduce a regression-based learning scheme for CV models, which outperforms classifier-based methods when transition state data is limited and noisy
    
[^16]: 缩小人类行为分析的差距：一种综合三模态数据的流程

    Closing the Gap in Human Behavior Analysis: A Pipeline for Synthesizing Trimodal Data

    [https://rss.arxiv.org/abs/2402.01537](https://rss.arxiv.org/abs/2402.01537)

    本研究提出了一种新颖的生成技术，可用于创建通过综合RGB、热像和深度三个模态的人类行为分析数据集，弥补了现有HBA数据集中缺乏整合多模态的问题。

    

    在普适机器学习中，尤其是在人类行为分析（HBA）领域，RGB一直是首选的模态，因为它易于获取并且信息丰富。然而，与其优点相连的是一些挑战，包括对光照条件的敏感性和隐私问题。解决这些漏洞的一个可能性是利用不同的模态。例如，热像素能够突出人体形态，而深度模态则增加了重要的背景信息。尽管这些模态带来了相关好处，但目前只有很少一部分特定于HBA的数据集集成了这些模态。为了解决这一问题，我们的研究介绍了一种新颖的生成技术，用于创建三模态（即RGB、热像和深度）的以人为中心的数据集。这种技术利用从RGB图像中提取的人体分割掩模，并与自动获取的热像和深度背景结合。有了这两个要素，我们可以利用现有的RGB数据合成深度和热像模态。

    In pervasive machine learning, especially in Human Behavior Analysis (HBA), RGB has been the primary modality due to its accessibility and richness of information. However, linked with its benefits are challenges, including sensitivity to lighting conditions and privacy concerns. One possibility to overcome these vulnerabilities is to resort to different modalities. For instance, thermal is particularly adept at accentuating human forms, while depth adds crucial contextual layers. Despite their known benefits, only a few HBA-specific datasets that integrate these modalities exist. To address this shortage, our research introduces a novel generative technique for creating trimodal, i.e., RGB, thermal, and depth, human-focused datasets. This technique capitalizes on human segmentation masks derived from RGB images, combined with thermal and depth backgrounds that are sourced automatically. With these two ingredients, we synthesize depth and thermal counterparts from existing RGB data uti
    
[^17]: 解码推测解码

    Decoding Speculative Decoding

    [https://rss.arxiv.org/abs/2402.01528](https://rss.arxiv.org/abs/2402.01528)

    推测解码是一种用于加速大型语言模型推断的技术，但我们的实验表明，选择的草稿模型生成的令牌被目标模型接受的概率越高，吞吐量越低。我们通过大量实验，分析了各种因素对推测解码效果的影响，并提出了一个分析模型来提高效率。

    

    推测解码是一种常用的技术，用于加速大型语言模型（LLM）的推断，而不修改其结果。在对LLM进行推断时，推测解码使用较小的草稿模型生成推测令牌，然后使用目标LLM验证这些草稿令牌。推测解码提供的加速取决于草稿模型的选择。普遍建议选择一个草稿模型，该模型生成的令牌被LLM接受的概率很高，以实现最高吞吐量。然而，我们的实验结果与之相反，随着生成的令牌被目标模型接受的概率增加，吞吐量减少。为了理解这一现象，我们进行了大量实验，对影响推测解码的不同因素进行了表征，并研究了这些因素如何相互作用和影响加速效果。基于我们的实验结果，我们描述了一个分析模型，可以使用该模型来进行决策，提高推测解码的效率。

    Speculative Decoding is a widely used technique to speed up inference for Large Language Models (LLMs) without modifying its outcome. When performing inference on an LLM, speculative decoding uses a smaller draft model which generates speculative tokens and then uses the target LLM to verify those draft tokens. The speedup provided by speculative decoding heavily depends on the choice of the draft model. It has been widely suggested to select a draft model that provides a high probability of the generated token being accepted by the LLM to achieve the highest throughput. However, our experiments indicate the contrary with throughput diminishing as the probability of generated tokens to be accepted by the target model increases. To understand this phenomenon, we perform extensive experiments to characterize the different factors that affect speculative decoding and how those factors interact and affect the speedups. Based on our experiments we describe an analytical model which can be u
    
[^18]: 超平面：快速 NeRF 自适应的超网络方法

    HyperPlanes: Hypernetwork Approach to Rapid NeRF Adaptation

    [https://rss.arxiv.org/abs/2402.01524](https://rss.arxiv.org/abs/2402.01524)

    本论文提出了一种基于超网络的少样本学习方法，利用其在训练过程中生成通用权重的更新，实现了从少量图像中快速生成高质量三维物体表示的效果。

    

    神经辐射场（Neural radiance fields，NeRF）是一种广泛接受的标准，用于从少量基础图像合成新的三维物体视图。然而，NeRF 的泛化能力有限，这意味着我们需要使用大量计算资源来训练每个要表示的物体的独立架构。为解决这个问题，我们提出了一种基于超网络范式的少样本学习方法，在推断过程中不需要梯度优化。超网络从训练数据中收集信息并为通用权重生成更新。结果，我们开发出了一种通过单一步骤从少量图像生成高质量三维物体表示的高效方法。通过与最先进的解决方案进行直接比较和全面的消融研究进行了确认。

    Neural radiance fields (NeRFs) are a widely accepted standard for synthesizing new 3D object views from a small number of base images. However, NeRFs have limited generalization properties, which means that we need to use significant computational resources to train individual architectures for each item we want to represent. To address this issue, we propose a few-shot learning approach based on the hypernetwork paradigm that does not require gradient optimization during inference. The hypernetwork gathers information from the training data and generates an update for universal weights. As a result, we have developed an efficient method for generating a high-quality 3D object representation from a small number of images in a single step. This has been confirmed by direct comparison with the state-of-the-art solutions and a comprehensive ablation study.
    
[^19]: 通过减少自监督语音表示来实现低资源跨领域歌声合成

    Low-Resource Cross-Domain Singing Voice Synthesis via Reduced Self-Supervised Speech Representations

    [https://rss.arxiv.org/abs/2402.01520](https://rss.arxiv.org/abs/2402.01520)

    提出了一种低资源跨领域歌声合成模型Karaoker-SSL，通过减少自监督语音表示和引入Conformer模块来实现无需使用歌唱数据和手工特征的合成过程。

    

    本文提出了一种名为Karaoker-SSL的歌声合成模型，它只通过文本和语音数据进行训练，作为一个典型的多人语音模型。这是一个低资源的流程，不需要端到端地使用任何歌唱数据，因为其声码器也是在语音数据上进行训练的。Karaoker-SSL以无监督的方式通过自监督语音表示进行条件化。我们通过选择与任务相关的维度的子集来预处理这些表示。在训练过程中，条件模块间接地通过多任务设置来指导捕捉风格信息。这是通过一个基于Conformer的模块实现的，该模块从声学模型的输出中预测音高。因此，Karaoker-SSL允许进行歌声合成，而无需依赖手工制作的领域特定特征。也不需要文本对齐或歌词时间戳。为了改善声音质量，我们采用了一个以目标说话者为条件的U-Net鉴别器。

    In this paper, we propose a singing voice synthesis model, Karaoker-SSL, that is trained only on text and speech data as a typical multi-speaker acoustic model. It is a low-resource pipeline that does not utilize any singing data end-to-end, since its vocoder is also trained on speech data. Karaoker-SSL is conditioned by self-supervised speech representations in an unsupervised manner. We preprocess these representations by selecting only a subset of their task-correlated dimensions. The conditioning module is indirectly guided to capture style information during training by multi-tasking. This is achieved with a Conformer-based module, which predicts the pitch from the acoustic model's output. Thus, Karaoker-SSL allows singing voice synthesis without reliance on hand-crafted and domain-specific features. There are also no requirements for text alignments or lyrics timestamps. To refine the voice quality, we employ a U-Net discriminator that is conditioned on the target speaker and fol
    
[^20]: 提升随机梯度下降：一种统一框架和用于更快收敛的新型加速方法

    Enhancing Stochastic Gradient Descent: A Unified Framework and Novel Acceleration Methods for Faster Convergence

    [https://rss.arxiv.org/abs/2402.01515](https://rss.arxiv.org/abs/2402.01515)

    本文提出了一个统一框架来解决随机优化中的收敛问题，并提出了两种即插即用的加速方法，在理论上证明了这些方法可以加快收敛速度。

    

    基于SGD，之前的研究提出了许多算法，在随机优化中改进了收敛速度和泛化性能，例如SGDm，AdaGrad，Adam等。然而，在非凸条件下，它们的收敛分析是具有挑战性的。在本文中，我们提出了一个统一的框架来解决这个问题。对于任何一阶方法，我们将更新方向$g_t$解释为随机次梯度$\nabla f_t(x_t)$和附加的加速项$\frac{2|\langle v_t, \nabla f_t(x_t) \rangle|}{\|v_t\|_2^2} v_t$的和，因此我们可以通过分析$\langle v_t, \nabla f_t(x_t) \rangle$来讨论收敛性。通过我们的框架，我们发现了两种即插即用的加速方法：\textbf{拒绝加速}和\textbf{随机向量加速}，我们在理论上证明了这两种方法可以直接导致收敛速度的提升。

    Based on SGD, previous works have proposed many algorithms that have improved convergence speed and generalization in stochastic optimization, such as SGDm, AdaGrad, Adam, etc. However, their convergence analysis under non-convex conditions is challenging. In this work, we propose a unified framework to address this issue. For any first-order methods, we interpret the updated direction $g_t$ as the sum of the stochastic subgradient $\nabla f_t(x_t)$ and an additional acceleration term $\frac{2|\langle v_t, \nabla f_t(x_t) \rangle|}{\|v_t\|_2^2} v_t$, thus we can discuss the convergence by analyzing $\langle v_t, \nabla f_t(x_t) \rangle$. Through our framework, we have discovered two plug-and-play acceleration methods: \textbf{Reject Accelerating} and \textbf{Random Vector Accelerating}, we theoretically demonstrate that these two methods can directly lead to an improvement in convergence rate.
    
[^21]: 映射潜在表示的多元宇宙

    Mapping the Multiverse of Latent Representations

    [https://rss.arxiv.org/abs/2402.01514](https://rss.arxiv.org/abs/2402.01514)

    提出了一种名为PRESTO的框架，用于映射依赖于潜在表示的机器学习模型的多元宇宙。该框架使用持续同调来测量潜在空间的差异，并统计推理它们的分布。可以用于敏感性分析、检测异常嵌入和高效导航超参。

    

    响应最近对通过多元宇宙分析来应对机器学习中的可靠性和稳健性问题的呼吁，我们提出了PRESTO，一种系统的框架，用于映射依赖于潜在表示的机器学习模型的多元宇宙。尽管这些模型得到了广泛的应用，但对它们嵌入的变异性仍然不被充分理解，导致了不必要的复杂性和不可靠的表示。我们的框架使用持续同调来表征不同组合的多样化机器学习方法、(超)参数配置和数据集所产生的潜在空间，从而使我们能够测量它们之间的成对(非)相似性并对其分布进行统计推理。正如我们在理论上和实证上所证明的那样，我们的流程保持了潜在表示集合的理想特性，可以用于进行敏感性分析、检测异常嵌入或高效有效地导航超参。

    Echoing recent calls to counter reliability and robustness concerns in machine learning via multiverse analysis, we present PRESTO, a principled framework for mapping the multiverse of machine-learning models that rely on latent representations. Although such models enjoy widespread adoption, the variability in their embeddings remains poorly understood, resulting in unnecessary complexity and untrustworthy representations. Our framework uses persistent homology to characterize the latent spaces arising from different combinations of diverse machine-learning methods, (hyper)parameter configurations, and datasets, allowing us to measure their pairwise (dis)similarity and statistically reason about their distributions. As we demonstrate both theoretically and empirically, our pipeline preserves desirable properties of collections of latent representations, and it can be leveraged to perform sensitivity analysis, detect anomalous embeddings, or efficiently and effectively navigate hyperpa
    
[^22]: 用生成模型推进脑肿瘤修补技术

    Advancing Brain Tumor Inpainting with Generative Models

    [https://rss.arxiv.org/abs/2402.01509](https://rss.arxiv.org/abs/2402.01509)

    本研究使用生成模型进行脑肿瘤的修补，通过合成健康脑扫描解决了常规算法的局限性。我们特别针对MRI的需求进行了相关修改，并通过评估多种修补技术的效果和局限性进行了验证。

    

    从病态脑扫描中合成健康脑扫描是解决常规算法的局限性的潜在方案。例如，组织分割和脑提取算法可能无法有效处理病态图像。我们将此视为3D修补任务，并研究了将2D修补方法改进以满足3D磁共振成像(MRI)数据要求的方法。我们的创新包括针对MRI特定需求的潜在修改，并使用BraTS2023修补数据集对多种修补技术进行评估，以评估其功效和局限性。

    Synthesizing healthy brain scans from diseased brain scans offers a potential solution to address the limitations of general-purpose algorithms, such as tissue segmentation and brain extraction algorithms, which may not effectively handle diseased images. We consider this a 3D inpainting task and investigate the adaptation of 2D inpainting methods to meet the requirements of 3D magnetic resonance imaging(MRI) data. Our contributions encompass potential modifications tailored to MRI-specific needs, and we conducted evaluations of multiple inpainting techniques using the BraTS2023 Inpainting datasets to assess their efficacy and limitations.
    
[^23]: 为什么随机森林有效？将树集成解释为自适应的自正则化平滑器的理解

    Why do Random Forests Work? Understanding Tree Ensembles as Self-Regularizing Adaptive Smoothers

    [https://rss.arxiv.org/abs/2402.01502](https://rss.arxiv.org/abs/2402.01502)

    本文将树集成解释为自适应的自正则化平滑器，通过量化预测的平滑程度并根据测试和训练输入的差异调节平滑性，提供了对树集成成功驱动因素的新见解。

    

    尽管树集成在效果和广泛应用方面表现出卓越的能力，但其成功的驱动因素尚未完全被理解。在本文中，我们强调将树集成解释为自适应的自正则化平滑器可以提供新的直觉和更深入的见解。我们利用这个观点来展示，当将随机化树集成视为平滑器时，它们的预测不仅比它们所包含的单个树的预测更加平滑，而且还根据测试和训练输入之间的差异在测试时进一步调节它们的平滑性。首先，我们利用这个洞察力重新审视、精炼和协调了最近两个对森林成功的解释，通过测量所暗示的平滑程度来客观地量化树集成的猜想行为方式。然后，我们超越了现有的关于树集成改进机制的解释

    Despite their remarkable effectiveness and broad application, the drivers of success underlying ensembles of trees are still not fully understood. In this paper, we highlight how interpreting tree ensembles as adaptive and self-regularizing smoothers can provide new intuition and deeper insight to this topic. We use this perspective to show that, when studied as smoothers, randomized tree ensembles not only make predictions that are quantifiably more smooth than the predictions of the individual trees they consist of, but also further regulate their smoothness at test-time based on the dissimilarity between testing and training inputs. First, we use this insight to revisit, refine and reconcile two recent explanations of forest success by providing a new way of quantifying the conjectured behaviors of tree ensembles objectively by measuring the effective degree of smoothing they imply. Then, we move beyond existing explanations for the mechanisms by which tree ensembles improve upon in
    
[^24]: 使用球谐函数作为控制变量的切片华瑟斯坦估计

    Sliced-Wasserstein Estimation with Spherical Harmonics as Control Variates

    [https://rss.arxiv.org/abs/2402.01493](https://rss.arxiv.org/abs/2402.01493)

    这种方法提出了一种新的蒙特卡罗方法，使用球谐函数作为控制变量来近似计算切片华瑟斯坦距离。与蒙特卡罗相比，该方法具有更好的收敛速度和理论性质。

    

    切片华瑟斯坦（SW）距离是概率测度之间的华瑟斯坦距离的平均值，结果为相关的一维投影的华瑟斯坦距离。因此，SW距离可以写成对球面上均匀测度的积分，并且可以使用蒙特卡罗框架来计算SW距离。球谐函数是球面上的多项式，它们构成了球面上可积函数集合的正交基。将这两个事实结合在一起，提出了一种新的蒙特卡罗方法，称为球谐控制变量（SHCV），用于使用球谐函数作为控制变量近似计算SW距离。结果表明，该方法具有良好的理论性质，例如在变量之间存在一定形式的线性依赖时，混合高斯测度的无误差特性。此外，与蒙特卡罗相比，得到了更快的收敛速度。

    The Sliced-Wasserstein (SW) distance between probability measures is defined as the average of the Wasserstein distances resulting for the associated one-dimensional projections. As a consequence, the SW distance can be written as an integral with respect to the uniform measure on the sphere and the Monte Carlo framework can be employed for calculating the SW distance. Spherical harmonics are polynomials on the sphere that form an orthonormal basis of the set of square-integrable functions on the sphere. Putting these two facts together, a new Monte Carlo method, hereby referred to as Spherical Harmonics Control Variates (SHCV), is proposed for approximating the SW distance using spherical harmonics as control variates. The resulting approach is shown to have good theoretical properties, e.g., a no-error property for Gaussian measures under a certain form of linear dependency between the variables. Moreover, an improved rate of convergence, compared to Monte Carlo, is established for g
    
[^25]: 连接点：模式连接是否是贝叶斯神经网络可行的基于样本推理的关键？

    Connecting the Dots: Is Mode-Connectedness the Key to Feasible Sample-Based Inference in Bayesian Neural Networks?

    [https://rss.arxiv.org/abs/2402.01484](https://rss.arxiv.org/abs/2402.01484)

    通过揭示权重和函数空间之间的关系，我们成功实现了贝叶斯神经网络的可行的基于样本推理，并提出了一种有效的贝叶斯深度集成方法来解决采样和收敛问题。

    

    在贝叶斯神经网络的基于样本推理（SBI）中，网络参数空间的大小和结构是一个主要挑战。我们的研究表明，通过接受权重和函数空间之间的特征关系，成功实现SBI是可能的，揭示了过度参数化和采样问题困难之间的系统联系。通过大量实验，我们建立了采样和收敛诊断的实际指南。因此，我们提出了一种贝叶斯深度集成方法作为一种有效的解决方案，具有竞争性能和不确定性量化能力。

    A major challenge in sample-based inference (SBI) for Bayesian neural networks is the size and structure of the networks' parameter space. Our work shows that successful SBI is possible by embracing the characteristic relationship between weight and function space, uncovering a systematic link between overparameterization and the difficulty of the sampling problem. Through extensive experiments, we establish practical guidelines for sampling and convergence diagnosis. As a result, we present a Bayesian deep ensemble approach as an effective solution with competitive performance and uncertainty quantification.
    
[^26]: 用Vabs-Net进行多级蛋白质预训练

    Multi-level protein pre-training with Vabs-Net

    [https://rss.arxiv.org/abs/2402.01481](https://rss.arxiv.org/abs/2402.01481)

    这篇论文介绍了一种使用Vabs-Net进行多级蛋白质预训练的方法。当前大多数基于结构的预训练模型仅关注残基水平，但忽略了侧链原子的重要性。为了解决这个问题，论文提出了一种新的预训练策略，引入了跨度掩码，以在三维蛋白质预训练中同时建模残基和原子水平的信息，并改善了残基表示的表达能力。

    

    最近几年，三维结构预训练蛋白质模型的发展迅猛，相较于预训练蛋白质语言模型，在各种下游任务中取得了重大进展。然而，大多数现有的基于结构的预训练模型主要关注残基水平，即α碳原子，而忽略了其他原子，如侧链原子。我们认为，在残基和原子水平上对蛋白质进行建模很重要，因为侧链原子对于许多下游任务（如分子对接）也是至关重要的。然而，我们发现在预训练中天真地组合残基和原子信息通常会失败。我们发现，信息泄漏是包含原子结构的输入导致残基级预训练任务变得琐碎并导致残基表示不够充分的一个关键原因。为了解决这个问题，我们引入了一种基于跨度掩码预训练策略的三维蛋白质预训练方法。

    In recent years, there has been a surge in the development of 3D structure-based pre-trained protein models, representing a significant advancement over pre-trained protein language models in various downstream tasks. However, most existing structure-based pre-trained models primarily focus on the residue level, i.e., alpha carbon atoms, while ignoring other atoms like side chain atoms. We argue that modeling proteins at both residue and atom levels is important since the side chain atoms can also be crucial for numerous downstream tasks, for example, molecular docking. Nevertheless, we find that naively combining residue and atom information during pre-training typically fails. We identify a key reason is the information leakage caused by the inclusion of atom structure in the input, which renders residue-level pre-training tasks trivial and results in insufficiently expressive residue representations. To address this issue, we introduce a span mask pre-training strategy on 3D protein
    
[^27]: 自核-特征对稀疏变分高斯过程中的自注意力

    Self-Attention through Kernel-Eigen Pair Sparse Variational Gaussian Processes

    [https://rss.arxiv.org/abs/2402.01476](https://rss.arxiv.org/abs/2402.01476)

    本论文提出了自核-特征对稀疏变分高斯过程（KEP-SVGP）用于构建具有不确定性感知的自注意力。通过核SVD（KSVD）解决了注意力核的不对称性，并实现了降低的复杂度。

    

    尽管Transformer具有显著提高预测准确性的能力，但它也可能产生过于自信的预测，并需要校准的不确定性估计，这通常可以通过高斯过程（GPs）来解决。现有的工作将对称核应用于变分推断下的注意力核；然而，忽略了注意力核本质上是不对称的事实。此外，推导出大规模数据的GP后验的复杂度仍然很高。在这项工作中，我们提出了一种用于构建具有不确定性感知的自注意力的核-特征对稀疏变 分高斯过程（KEP-SVGP），其中通过核SVD（KSVD）解决了注意力核的不对称性，并获得了降低的复杂度。通过KEP-SVGP，i）由于与注意力核的KSVD相对应的两组奇异向量引导的SVGP对完全表征了不对称性；ii）仅使用少量与KSVD相对应的伴随特征函数，推导SVGP后验概率密度可以实现较低的复杂度。

    While the great capability of Transformers significantly boosts prediction accuracy, it could also yield overconfident predictions and require calibrated uncertainty estimation, which can be commonly tackled by Gaussian processes (GPs). Existing works apply GPs with symmetric kernels under variational inference to the attention kernel; however, omitting the fact that attention kernels are in essence asymmetric. Moreover, the complexity of deriving the GP posteriors remains high for large-scale data. In this work, we propose Kernel-Eigen Pair Sparse Variational Gaussian Processes (KEP-SVGP) for building uncertainty-aware self-attention where the asymmetry of attention kernels is tackled by Kernel SVD (KSVD) and a reduced complexity is acquired. Through KEP-SVGP, i) the SVGP pair induced by the two sets of singular vectors from KSVD w.r.t. the attention kernel fully characterizes the asymmetry; ii) using only a small set of adjoint eigenfunctions from KSVD, the derivation of SVGP posteri
    
[^28]: 深度条件生成学习：模型与误差分析

    Deep Conditional Generative Learning: Model and Error Analysis

    [https://rss.arxiv.org/abs/2402.01460](https://rss.arxiv.org/abs/2402.01460)

    提出了一种基于ODE的深度生成方法，通过条件Follmer流来学习条件分布，通过离散化和深度神经网络实现高效转化。同时，通过Wasserstein距离的非渐近收敛速率，提供了第一个端到端误差分析，数值实验证明其在不同场景下的优越性。

    

    我们介绍了一种基于普通微分方程（ODE）的深度生成方法，用于学习条件分布，称为条件Follmer流。从标准高斯分布开始，所提出的流能够以高效的方式将其转化为目标条件分布，在时间1处达到稳定。为了有效实现，我们使用欧拉方法对流进行离散化，使用深度神经网络非参数化估计速度场。此外，我们导出了学习样本的分布与目标分布之间的Wasserstein距离的非渐近收敛速率，在条件分布学习中提供了第一个全面的端到端误差分析。我们的数值实验展示了它在一系列情况下的有效性，从标准的非参数化条件密度估计问题到涉及图像数据的更复杂的挑战，说明它优于各种现有方法。

    We introduce an Ordinary Differential Equation (ODE) based deep generative method for learning a conditional distribution, named the Conditional Follmer Flow. Starting from a standard Gaussian distribution, the proposed flow could efficiently transform it into the target conditional distribution at time 1. For effective implementation, we discretize the flow with Euler's method where we estimate the velocity field nonparametrically using a deep neural network. Furthermore, we derive a non-asymptotic convergence rate in the Wasserstein distance between the distribution of the learned samples and the target distribution, providing the first comprehensive end-to-end error analysis for conditional distribution learning via ODE flow. Our numerical experiments showcase its effectiveness across a range of scenarios, from standard nonparametric conditional density estimation problems to more intricate challenges involving image data, illustrating its superiority over various existing condition
    
[^29]: 在因果发现中集成大型语言模型: 一种统计因果方法

    Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach

    [https://rss.arxiv.org/abs/2402.01454](https://rss.arxiv.org/abs/2402.01454)

    本文提出了一种在因果发现中集成大型语言模型的方法，通过将统计因果提示与知识增强相结合，可以使统计因果发现结果接近真实情况并进一步改进结果。

    

    在实际的统计因果发现（SCD）中，将领域专家知识作为约束嵌入到算法中被广泛接受，因为这对于创建一致有意义的因果模型是重要的，尽管识别背景知识的挑战被认可。为了克服这些挑战，本文提出了一种新的因果推断方法，即通过将LLM的“统计因果提示（SCP）”与SCD方法和基于知识的因果推断（KBCI）相结合，对SCD进行先验知识增强。实验证明，GPT-4可以使LLM-KBCI的输出与带有LLM-KBCI的先验知识的SCD结果接近真实情况，如果GPT-4经历了SCP，那么SCD的结果还可以进一步改善。而且，即使LLM不含有数据集的信息，LLM仍然可以通过其背景知识来改进SCD。

    In practical statistical causal discovery (SCD), embedding domain expert knowledge as constraints into the algorithm is widely accepted as significant for creating consistent meaningful causal models, despite the recognized challenges in systematic acquisition of the background knowledge. To overcome these challenges, this paper proposes a novel methodology for causal inference, in which SCD methods and knowledge based causal inference (KBCI) with a large language model (LLM) are synthesized through "statistical causal prompting (SCP)" for LLMs and prior knowledge augmentation for SCD. Experiments have revealed that GPT-4 can cause the output of the LLM-KBCI and the SCD result with prior knowledge from LLM-KBCI to approach the ground truth, and that the SCD result can be further improved, if GPT-4 undergoes SCP. Furthermore, it has been clarified that an LLM can improve SCD with its background knowledge, even if the LLM does not contain information on the dataset. The proposed approach
    
[^30]: 在协变量偏移中改进重要性估计以提供准确的预测误差

    Improving importance estimation in covariate shift for providing accurate prediction error

    [https://rss.arxiv.org/abs/2402.01450](https://rss.arxiv.org/abs/2402.01450)

    该论文研究了在协变量偏移问题中改进重要性估计以提高预测误差的准确性。

    

    在传统的机器学习中，算法的预测基于训练集和测试集中的数据遵循相同的分布的假设。然而，在现实世界的数据中，这个条件并不成立，例如，协变量的分布发生了变化，而目标的条件分布保持不变。这种情况被称为协变量偏移问题，标准误差估计可能不再准确。在这种情况下，重要性是一种常用的度量，用于减轻协变量偏移对误差估计的影响。主要的缺点是它不容易计算。Kullback-Leibler重要性估计过程（KLIEP）能够以一种有希望的方式估计重要性。尽管它的性能很好，但它无法忽略目标信息，因为它只包括用于计算重要性的协变量信息。在这个方向上，本文探讨了如果在计算重要性时包括目标信息的潜在性能改进。

    In traditional Machine Learning, the algorithms predictions are based on the assumption that the data follows the same distribution in both the training and the test datasets. However, in real world data this condition does not hold and, for instance, the distribution of the covariates changes whereas the conditional distribution of the targets remains unchanged. This situation is called covariate shift problem where standard error estimation may be no longer accurate. In this context, the importance is a measure commonly used to alleviate the influence of covariate shift on error estimations. The main drawback is that it is not easy to compute. The Kullback-Leibler Importance Estimation Procedure (KLIEP) is capable of estimating importance in a promising way. Despite its good performance, it fails to ignore target information, since it only includes the covariates information for computing the importance. In this direction, this paper explores the potential performance improvement if 
    
[^31]: 任务关键 -- 卫星数据是机器学习中的独特模态

    Mission Critical -- Satellite Data is a Distinct Modality in Machine Learning

    [https://rss.arxiv.org/abs/2402.01444](https://rss.arxiv.org/abs/2402.01444)

    卫星数据是机器学习中的独特模态，我们需要重新思考现有的实践并发起一个以卫星数据的特征和挑战为中心的新的研究议程，以推动SatML的质量和影响力。

    

    卫星数据有潜力为机器学习带来一次重大的改变，我们需要重新思考针对传统数据模态设计的现有实践。随着卫星数据的机器学习（SatML）在现实世界中产生重大影响，我们的领域正处于一个十字路口。我们可以继续应用不适合的方法，或者我们可以发起一个以卫星数据的独特特征和挑战为中心的新研究议程。本文认为卫星数据构成了机器学习研究的一种独特模态，而我们必须承认这一点，以推动SatML在理论、方法和部署方面的质量和影响力。我们概述了关键性的讨论问题和可行性建议，将SatML从仅仅一个有趣的应用领域转变为一个致力于推动机器学习和社会重大挑战的专门研究学科。

    Satellite data has the potential to inspire a seismic shift for machine learning -- one in which we rethink existing practices designed for traditional data modalities. As machine learning for satellite data (SatML) gains traction for its real-world impact, our field is at a crossroads. We can either continue applying ill-suited approaches, or we can initiate a new research agenda that centers around the unique characteristics and challenges of satellite data. This position paper argues that satellite data constitutes a distinct modality for machine learning research and that we must recognize it as such to advance the quality and impact of SatML research across theory, methods, and deployment. We outline critical discussion questions and actionable suggestions to transform SatML from merely an intriguing application area to a dedicated research discipline that helps move the needle on big challenges for machine learning and society.
    
[^32]: 学习市场：基于情感的集成交易智能体

    Learning the Market: Sentiment-Based Ensemble Trading Agents

    [https://rss.arxiv.org/abs/2402.01441](https://rss.arxiv.org/abs/2402.01441)

    该论文提出了将情感分析和深度强化学习集成算法应用于股票交易的方法，设计了可以根据市场情绪动态调整的交易策略。实验结果表明，这种方法比传统的集成策略、单一智能体算法和市场指标更具盈利性、稳健性和风险最小化。相关研究还发现，传统的固定更换集成智能体的做法并不是最优的，而基于情感的动态框架可以显著提高交易智能体的性能。

    

    我们提出了将情感分析和深度强化学习集成算法应用于股票交易，并设计了一种能够根据当前市场情绪动态调整所使用的智能体的策略。我们创建了一个简单但有效的方法来提取新闻情感，并将其与对现有作品的一般改进结合起来，从而得到有效考虑定性市场因素和定量股票数据的自动交易智能体。我们证明了我们的方法导致了一种盈利、稳健且风险最小的策略，优于传统的集成策略以及单一智能体算法和市场指标。我们的发现表明，传统的每隔固定月份更换集成智能体的做法并不是最优的，基于情感的动态框架极大地提升了这些智能体的性能。此外，由于我们的算法设计简单且...

    We propose the integration of sentiment analysis and deep-reinforcement learning ensemble algorithms for stock trading, and design a strategy capable of dynamically altering its employed agent given concurrent market sentiment. In particular, we create a simple-yet-effective method for extracting news sentiment and combine this with general improvements upon existing works, resulting in automated trading agents that effectively consider both qualitative market factors and quantitative stock data. We show that our approach results in a strategy that is profitable, robust, and risk-minimal -- outperforming the traditional ensemble strategy as well as single agent algorithms and market metrics. Our findings determine that the conventional practice of switching ensemble agents every fixed-number of months is sub-optimal, and that a dynamic sentiment-based framework greatly unlocks additional performance within these agents. Furthermore, as we have designed our algorithm with simplicity and
    
[^33]: 在图上的小样本学习：从元学习到预训练和提示

    Few-Shot Learning on Graphs: from Meta-learning to Pre-training and Prompting

    [https://rss.arxiv.org/abs/2402.01440](https://rss.arxiv.org/abs/2402.01440)

    本文综述了图上的小样本学习的最新发展，将现有的研究方法划分为元学习、预训练和混合方法三大类别，并对它们的优缺点进行了比较。还提出了未来的研究方向。

    

    图表示学习是图中心任务中的关键步骤，在这方面已经取得了重大进展。早期的技术通常在端到端的设置中运行，性能严重依赖于充足的标记数据的可用性。这个限制引发了图上的小样本学习的出现，其中每个任务只有少量的任务特定标签可用。鉴于这个领域的广泛文献，本综述试图综合最近的发展，提供比较性的见解，并确定未来的方向。我们将现有的研究系统地分为三个主要类别：元学习方法、预训练方法和混合方法，并在每个类别中进行细粒度的分类，以帮助读者进行方法选择。在每个类别中，我们分析这些方法之间的关系并比较它们的优缺点。最后，我们概述了图上的小样本学习未来的方向。

    Graph representation learning, a critical step in graph-centric tasks, has seen significant advancements. Earlier techniques often operate in an end-to-end setting, where performance heavily relies on the availability of ample labeled data. This constraint has spurred the emergence of few-shot learning on graphs, where only a few task-specific labels are available for each task. Given the extensive literature in this field, this survey endeavors to synthesize recent developments, provide comparative insights, and identify future directions. We systematically categorize existing studies into three major families: meta-learning approaches, pre-training approaches, and hybrid approaches, with a finer-grained classification in each family to aid readers in their method selection process. Within each category, we analyze the relationships among these methods and compare their strengths and limitations. Finally, we outline prospective future directions for few-shot learning on graphs to cata
    
[^34]: 从词语到分子：大型语言模型在化学领域的调查

    From Words to Molecules: A Survey of Large Language Models in Chemistry

    [https://rss.arxiv.org/abs/2402.01439](https://rss.arxiv.org/abs/2402.01439)

    本论文调查了大型语言模型在化学领域的应用。研究内容涵盖了分子信息输入LLMs的表示和标记化方法、化学LLMs的不同组群及其整合方法、适用于化学LLMs的预训练目标等。此外，还探讨了LLMs在化学中的各种应用。

    

    在近年来，大型语言模型（LLMs）在自然语言处理（NLP）和各种跨学科领域取得了重要的成功。然而，将LLMs应用于化学是一个复杂的任务，需要专门的领域知识。本文对将LLMs整合到化学领域中所采用的微妙方法进行了全面的探索，深入探讨了这个跨学科交汇点的复杂性和创新性。具体而言，我们的分析从通过各种表示和标记化方法将分子信息输入LLMs开始。然后，我们根据输入数据的领域和模态将化学LLMs分为三个不同的组，并讨论将这些输入与LLMs整合的方法。此外，本文还探讨了适用于化学LLMs的预训练目标。在此之后，我们探讨了LLMs在化学中的多样化应用，包括它们的新范式的应用。

    In recent years, Large Language Models (LLMs) have achieved significant success in natural language processing (NLP) and various interdisciplinary areas. However, applying LLMs to chemistry is a complex task that requires specialized domain knowledge. This paper provides a thorough exploration of the nuanced methodologies employed in integrating LLMs into the field of chemistry, delving into the complexities and innovations at this interdisciplinary juncture. Specifically, our analysis begins with examining how molecular information is fed into LLMs through various representation and tokenization methods. We then categorize chemical LLMs into three distinct groups based on the domain and modality of their input data, and discuss approaches for integrating these inputs for LLMs. Furthermore, this paper delves into the pretraining objectives with adaptations to chemical LLMs. After that, we explore the diverse applications of LLMs in chemistry, including novel paradigms for their applica
    
[^35]: 随机非线性与无穷维扩散过程的条件约束

    Conditioning non-linear and infinite-dimensional diffusion processes

    [https://rss.arxiv.org/abs/2402.01434](https://rss.arxiv.org/abs/2402.01434)

    本文探索了在无穷维空间中对非线性过程进行条件约束的方法，并应用于进化生物学中的生物形态时间序列分析。

    

    生成性扩散模型和许多科学和工程中的随机模型在离散化之前自然地存在于无穷维空间中。为了将观测数据纳入统计和学习任务中，需要对观测值进行条件约束。近期的研究已经处理了在无穷维空间中对线性过程进行条件约束的问题，但尚未探索在无穷维空间中对非线性过程进行条件约束的方法。本文提出了一种在无先验离散化的情况下对函数值随机过程进行条件约束的方法。为此，我们使用了Girsanov定理的无穷维版本来对函数值随机过程进行条件约束，从而得到了涉及得分的条件过程的随机微分方程(SDE)。我们将这种技术应用于进化生物学中的生物形态时间序列分析中，通过Fourier基函数离散化，然后利用得分匹配方法学习得分函数的系数。

    Generative diffusion models and many stochastic models in science and engineering naturally live in infinite dimensions before discretisation. To incorporate observed data for statistical and learning tasks, one needs to condition on observations. While recent work has treated conditioning linear processes in infinite dimensions, conditioning non-linear processes in infinite dimensions has not been explored. This paper conditions function valued stochastic processes without prior discretisation. To do so, we use an infinite-dimensional version of Girsanov's theorem to condition a function-valued stochastic process, leading to a stochastic differential equation (SDE) for the conditioned process involving the score. We apply this technique to do time series analysis for shapes of organisms in evolutionary biology, where we discretise via the Fourier basis and then learn the coefficients of the score function with score matching methods.
    
[^36]: 连续时间POMDP的近似控制

    Approximate Control for Continuous-Time POMDPs

    [https://rss.arxiv.org/abs/2402.01431](https://rss.arxiv.org/abs/2402.01431)

    本文提出了一个适用于连续时间部分可观察系统的决策框架，通过近似方法实现了针对大规模状态空间的过滤和控制问题的可扩展解决方案。

    

    本文提出了一个针对具有离散状态和动作空间的连续时间部分可观察系统的决策框架。由于大规模状态空间的最优决策变得难以处理，我们采用了适用于具有不断增加的状态数量的过滤和控制问题的近似方法。具体而言，我们通过将高维过滤分布投影到参数分布族上来近似它，并将其整合到基于完全可观察系统的控制启发式中，以获得可扩展的策略。我们在几个部分观察系统上证明了我们方法的有效性，包括排队系统和化学反应网络。

    This work proposes a decision-making framework for partially observable systems in continuous time with discrete state and action spaces. As optimal decision-making becomes intractable for large state spaces we employ approximation methods for the filtering and the control problem that scale well with an increasing number of states. Specifically, we approximate the high-dimensional filtering distribution by projecting it onto a parametric family of distributions, and integrate it into a control heuristic based on the fully observable system to obtain a scalable policy. We demonstrate the effectiveness of our approach on several partially observed systems, including queueing systems and chemical reaction networks.
    
[^37]: 基于数据驱动的鲁棒自动钢琴转录分析

    A Data-Driven Analysis of Robust Automatic Piano Transcription

    [https://rss.arxiv.org/abs/2402.01424](https://rss.arxiv.org/abs/2402.01424)

    本研究从训练数据的角度研究了自动钢琴转录系统，发现现有模型在训练数据的声学特性方面严重过拟合。通过创建新的数据集并采用数据增强技术，我们在没有查看训练数据的情况下实现了88.4的F1得分的最新音符起始准确性。

    

    由于新的数据集和建模技术，自动钢琴转录算法在近年来得到了显著的改进。最近的研究主要集中在采用新的神经网络架构，如Transformer和Perceiver，以获得更准确的系统。在本研究中，我们从训练数据的角度研究了转录系统。通过在分布外的带注释钢琴数据上衡量它们的性能，我们展示了这些模型如何严重过拟合训练数据的声学特性。我们在MAESTRO数据集上创建了一组新的音频数据，在专业录音环境中通过Yamaha Disklavier播放进行了自动捕获。在使用原始和重新演奏版本的MAESTRO数据集进行训练时，采用了各种数据增强技术，我们在MAPS数据集上实现了88.4的F1得分的最新音符起始准确性，而不需要查看其任何训练数据。随后，我们对这些数据进行了进一步的分析。

    Algorithms for automatic piano transcription have improved dramatically in recent years due to new datasets and modeling techniques. Recent developments have focused primarily on adapting new neural network architectures, such as the Transformer and Perceiver, in order to yield more accurate systems. In this work, we study transcription systems from the perspective of their training data. By measuring their performance on out-of-distribution annotated piano data, we show how these models can severely overfit to acoustic properties of the training data. We create a new set of audio for the MAESTRO dataset, captured automatically in a professional studio recording environment via Yamaha Disklavier playback. Using various data augmentation techniques when training with the original and re-performed versions of the MAESTRO dataset, we achieve state-of-the-art note-onset accuracy of 88.4 F1-score on the MAPS dataset, without seeing any of its training data. We subsequently analyze these dat
    
[^38]: 上下文感知机器翻译的序列缩短

    Sequence Shortening for Context-Aware Machine Translation

    [https://rss.arxiv.org/abs/2402.01416](https://rss.arxiv.org/abs/2402.01416)

    本研究展示了上下文感知机器翻译的序列缩短方法，通过重用上下文信息可以提高翻译准确性，在对比数据集上表现良好，并且引入了新的潜在分组和潜在选择方法来进一步提高翻译质量。

    

    上下文感知机器翻译旨在通过将周围的句子作为上下文来改进句子的翻译。为实现这一目标，已经应用了两种主要架构，即基于串联的单编码器和多编码器模型。在这项研究中，我们展示了多编码器架构的一个特殊情况，在下一步中重用源句子的潜在表示作为上下文，可以在对比数据集上实现更高的准确性（模型必须对提供的句子中的正确翻译进行排序），并且与单编码器和多编码器方法相比具有可比较的BLEU和COMET分数。此外，我们研究了将缓存表示应用于序列缩短。我们测试了三种基于汇聚的缩短技术，并引入了两种新方法——潜在分组和潜在选择，其中网络学习将令牌分组或选择要缓存为上下文。我们的实验表明，缓存表示的序列缩短方法可以进一步提高翻译质量。

    Context-aware Machine Translation aims to improve translations of sentences by incorporating surrounding sentences as context. Towards this task, two main architectures have been applied, namely single-encoder (based on concatenation) and multi-encoder models. In this study, we show that a special case of multi-encoder architecture, where the latent representation of the source sentence is cached and reused as the context in the next step, achieves higher accuracy on the contrastive datasets (where the models have to rank the correct translation among the provided sentences) and comparable BLEU and COMET scores as the single- and multi-encoder approaches. Furthermore, we investigate the application of Sequence Shortening to the cached representations. We test three pooling-based shortening techniques and introduce two novel methods - Latent Grouping and Latent Selecting, where the network learns to group tokens or selects the tokens to be cached as context. Our experiments show that th
    
[^39]: SMLP: 符号机器学习证明器

    SMLP: Symbolic Machine Learning Prover

    [https://rss.arxiv.org/abs/2402.01415](https://rss.arxiv.org/abs/2402.01415)

    SMLP是一种基于数据样本的系统探索工具，通过采用统计方法和机器学习模型相结合的灰盒方法，可以探索系统并优化硬件设计。

    

    符号机器学习证明器（SMLP）是一种基于通过模拟或执行系统得到的一系列输入向量样本的系统探索工具和库。SMLP旨在通过采用灰盒方法，即将数据探索的统计方法与构建和探索机器学习模型相结合，并通过组合概率和形式方法来探索这些模型，从而探索系统。SMLP已经应用于英特尔的工业环境中，用于分析和优化模拟电路级别的硬件设计。SMLP是一个通用的工具，可以应用于可以通过机器学习模型进行采样和建模的系统。

    Symbolic Machine Learning Prover (SMLP) is a tool and a library for system exploration based on data samples obtained by simulating or executing the system on a number of input vectors. SMLP aims at exploring the system based on this data by taking a grey-box approach: SMLP combines statistical methods of data exploration with building and exploring machine learning models in close feedback loop with the system's response, and exploring these models by combining probabilistic and formal methods. SMLP has been applied in industrial setting at Intel for analyzing and optimizing hardware designs at the analog level. SMLP is a general purpose tool and can be applied to systems that can be sampled and modeled by machine learning models.
    
[^40]: 第七届CHiME挑战赛中UDASE任务中语音增强方法的客观和主观评估

    Objective and subjective evaluation of speech enhancement methods in the UDASE task of the 7th CHiME challenge

    [https://rss.arxiv.org/abs/2402.01413](https://rss.arxiv.org/abs/2402.01413)

    本文介绍了第七届CHiME挑战赛的UDASE任务中系统的客观和主观评估，并分析了结果

    

    基于监督模型的语音增强方法是通过人工合成的干净语音和噪声信号混合来训练的。然而，合成训练条件可能无法准确反映测试过程中遇到的真实世界条件。这种差异可能导致在测试域与合成训练域显著不同时性能不佳。为了解决这个问题，第七届CHiME挑战赛的UDASE任务旨在利用测试域的真实世界噪声语音录音来对语音增强模型进行无监督域适应。具体来说，这个测试域对应于CHiME-5数据集，该数据集由在嘈杂和混响的家庭环境中进行的真实多说话人对话录音组成，无法获得地面实况干净语音信号。在本文中，我们介绍了提交到CHiME-7 UDASE任务的系统的客观和主观评估，并对结果进行了分析

    Supervised models for speech enhancement are trained using artificially generated mixtures of clean speech and noise signals. However, the synthetic training conditions may not accurately reflect real-world conditions encountered during testing. This discrepancy can result in poor performance when the test domain significantly differs from the synthetic training domain. To tackle this issue, the UDASE task of the 7th CHiME challenge aimed to leverage real-world noisy speech recordings from the test domain for unsupervised domain adaptation of speech enhancement models. Specifically, this test domain corresponds to the CHiME-5 dataset, characterized by real multi-speaker and conversational speech recordings made in noisy and reverberant domestic environments, for which ground-truth clean speech signals are not available. In this paper, we present the objective and subjective evaluations of the systems that were submitted to the CHiME-7 UDASE task, and we provide an analysis of the resul
    
[^41]: 通过潜在扩散生成低音伴奏

    Bass Accompaniment Generation via Latent Diffusion

    [https://rss.arxiv.org/abs/2402.01412](https://rss.arxiv.org/abs/2402.01412)

    本论文提出了一种通过潜在扩散技术生成低音伴奏的方法。通过音频自动编码器将音频样本压缩成潜在表示，并结合条件潜在扩散模型生成对应的音轨。通过引入参考风格和调整引导方法，提供了对生成样本音色的控制和进一步提高音频质量。定量实验证明了该方法的有效性。

    

    自动生成与任意输入轨道相匹配的音乐是一项具有挑战性的任务。我们提出了一种新颖的可控系统，用于生成与任意长度的音乐混音相配的单音轨。我们方法的核心是音频自动编码器，它可以将音频波形样本高效地压缩成可逆的潜在表示，以及一种条件潜在扩散模型，其以混音的潜在编码作为输入并生成相应音轨的潜在编码。为了对生成的样本的音色进行控制，我们引入了将潜在空间与用户提供的参考风格相连接的技术，用于扩散采样期间。为了进一步提高音频质量，我们调整了无需分类器的引导方法，以避免在生成无界潜在空间时产生失真。我们在一个包含混音与匹配低音音轨对的数据集上训练了我们的模型。定量实验证明，给定输入的混音，我们的模型可以生成与之匹配的低音音轨。

    The ability to automatically generate music that appropriately matches an arbitrary input track is a challenging task. We present a novel controllable system for generating single stems to accompany musical mixes of arbitrary length. At the core of our method are audio autoencoders that efficiently compress audio waveform samples into invertible latent representations, and a conditional latent diffusion model that takes as input the latent encoding of a mix and generates the latent encoding of a corresponding stem. To provide control over the timbre of generated samples, we introduce a technique to ground the latent space to a user-provided reference style during diffusion sampling. For further improving audio quality, we adapt classifier-free guidance to avoid distortions at high guidance strengths when generating an unbounded latent space. We train our model on a dataset of pairs of mixes and matching bass stems. Quantitative experiments demonstrate that, given an input mix, the prop
    
[^42]: 使用原型和非专家监督的XAI对皮肤癌的检测

    XAI for Skin Cancer Detection with Prototypes and Non-Expert Supervision

    [https://rss.arxiv.org/abs/2402.01410](https://rss.arxiv.org/abs/2402.01410)

    本研究提出了一种用于黑素瘤诊断的新方法，通过使用可解释的原型-部分模型以及结合基于非专家反馈的引导性监督，实现了优于不可解释的模型的性能和泛化能力。

    

    通过皮肤镜图像分析进行皮肤癌检测是一个关键的任务。然而，现有用于此目的的模型往往缺乏解释性和可靠性，由于它们的黑盒性质引起了医生的担忧。在本文中，我们提出了一种新颖的方法，利用可解释的原型-部分模型进行黑素瘤诊断。我们通过结合两种不同的信息路径引入了基于非专家反馈的引导性监督：1) 使用分割网络自动获取的二进制掩模；2) 用户优化的原型。这两个不同的信息路径旨在确保学习到的原型与皮肤病变的相关区域相对应，排除其边界之外的混淆因素。实验结果表明，即使没有专家监督，我们的方法在性能和泛化能力上都优于不可解释的模型。

    Skin cancer detection through dermoscopy image analysis is a critical task. However, existing models used for this purpose often lack interpretability and reliability, raising the concern of physicians due to their black-box nature. In this paper, we propose a novel approach for the diagnosis of melanoma using an interpretable prototypical-part model. We introduce a guided supervision based on non-expert feedback through the incorporation of: 1) binary masks, obtained automatically using a segmentation network; and 2) user-refined prototypes. These two distinct information pathways aim to ensure that the learned prototypes correspond to relevant areas within the skin lesion, excluding confounding factors beyond its boundaries. Experimental results demonstrate that, even without expert supervision, our approach achieves superior performance and generalization compared to non-interpretable models.
    
[^43]: 通过反事实概念瓶颈模型攀登解释性的阶梯

    Climbing the Ladder of Interpretability with Counterfactual Concept Bottleneck Models

    [https://rss.arxiv.org/abs/2402.01408](https://rss.arxiv.org/abs/2402.01408)

    本论文提出了一种新的模型 CF-CBMs，可以同时解决深度学习模型的预测、解释和想象能力的不足，为部署可靠的AI代理、校准人类信任和加深人机交互提供了一种有效的解决方法。

    

    当前的深度学习模型没有同时解决三个基本问题的设计：预测类别标签以解决给定的分类任务（“是什么？”），解释任务预测（“为什么？”），并想象可能导致不同预测的替代情景（“如果怎样？”）。无法回答这些问题代表了部署可靠的AI代理、校准人类信任和加深人机交互的关键差距。为了弥合这一差距，我们引入了反事实概念瓶颈模型（CF-CBMs），这是一类能够高效同时解决上述查询而无需进行事后搜索的模型。我们的结果表明，CF-CBMs能够产生准确的预测（“是什么？”），对任务预测提供简单的解释（“为什么？”），以及可解释的反事实情况（“如果怎样？”）。CF-CBMs还可以对概念干预的影响进行采样或估计最可能的反事实情况，以解释事件，并优化产生多样化的反事实。

    Current deep learning models are not designed to simultaneously address three fundamental questions: predict class labels to solve a given classification task (the "What?"), explain task predictions (the "Why?"), and imagine alternative scenarios that could result in different predictions (the "What if?"). The inability to answer these questions represents a crucial gap in deploying reliable AI agents, calibrating human trust, and deepening human-machine interaction. To bridge this gap, we introduce CounterFactual Concept Bottleneck Models (CF-CBMs), a class of models designed to efficiently address the above queries all at once without the need to run post-hoc searches. Our results show that CF-CBMs produce: accurate predictions (the "What?"), simple explanations for task predictions (the "Why?"), and interpretable counterfactuals (the "What if?"). CF-CBMs can also sample or estimate the most probable counterfactual to: (i) explain the effect of concept interventions on tasks, (ii) sh
    
[^44]: 通过Lipschitz正则化在规模上实现零样本机器遗忘

    Zero-Shot Machine Unlearning at Scale via Lipschitz Regularization

    [https://rss.arxiv.org/abs/2402.01401](https://rss.arxiv.org/abs/2402.01401)

    通过Lipschitz正则化实现零样本机器遗忘，可以及时忘记私人或受版权保护的信息，同时保持模型性能。

    

    为了遵守人工智能和数据规定，从训练得到的机器学习模型中遗忘私人或受版权保护的信息的需求变得越来越重要。遗忘的关键挑战是及时忘记必要的数据，同时保持模型性能。在这项工作中，我们解决了零样本遗忘的场景，即只有一个经过训练的模型和要遗忘的数据，遗忘算法必须能够移除数据。根据这样定义，现有的最先进的方法是不够的。基于Lipschitz连续性的概念，我们提出了一种方法，通过对样本扰动的输出进行平滑处理来诱导遗忘。我们展示了这种平滑性成功地实现了遗忘，同时保持了总体模型性能。我们对我们的方法进行了广泛的经验评估，包括一系列当代基准测试，验证了我们的方法在严格的零样本约束下达到了最先进的性能。

    To comply with AI and data regulations, the need to forget private or copyrighted information from trained machine learning models is increasingly important. The key challenge in unlearning is forgetting the necessary data in a timely manner, while preserving model performance. In this work, we address the zero-shot unlearning scenario, whereby an unlearning algorithm must be able to remove data given only a trained model and the data to be forgotten. Under such a definition, existing state-of-the-art methods are insufficient. Building on the concepts of Lipschitz continuity, we present a method that induces smoothing of the forget sample's output, with respect to perturbations of that sample. We show this smoothing successfully results in forgetting while preserving general model performance. We perform extensive empirical evaluation of our method over a range of contemporary benchmarks, verifying that our method achieves state-of-the-art performance under the strict constraints of ze
    
[^45]: 低查询成本带噪声or同时聚类

    Query-Efficient Correlation Clustering with Noisy Oracle

    [https://rss.arxiv.org/abs/2402.01400](https://rss.arxiv.org/abs/2402.01400)

    本论文提出了一种低查询成本的聚类方法，利用纯在组合多臂赌博机探索范式实现在线学习，并设计了能在NP-hard情况下运行的多项式时间算法。

    

    我们研究了一个常见的聚类设置，其中我们需要对n个元素进行聚类，并且我们的目标是尽可能少地向返回两个元素相似性的有噪声的oracle查询。我们的设置涵盖了许多应用领域，在这些领域中，相似性函数计算起来成本高并且 inherently noisy。我们提出了两种基于纯在组合多臂赌博机探索范式(PE-CMAB)的在线学习问题的新颖表达方法固定置信度和固定预算设置。对于这两种设置，我们设计了将抽样策略与经典的相关聚类近似算法相结合的算法，并研究了它们的理论保证。我们的结果是这样的：这些算法是第一个在底层离线优化问题为NP-hard的情况下运行的多项式时间算法的例子。

    We study a general clustering setting in which we have $n$ elements to be clustered, and we aim to perform as few queries as possible to an oracle that returns a noisy sample of the similarity between two elements. Our setting encompasses many application domains in which the similarity function is costly to compute and inherently noisy. We propose two novel formulations of online learning problems rooted in the paradigm of Pure Exploration in Combinatorial Multi-Armed Bandits (PE-CMAB): fixed confidence and fixed budget settings. For both settings, we design algorithms that combine a sampling strategy with a classic approximation algorithm for correlation clustering and study their theoretical guarantees. Our results are the first examples of polynomial-time algorithms that work for the case of PE-CMAB in which the underlying offline optimization problem is NP-hard.
    
[^46]: 解释自监督表示学习的概率模型

    A Probabilistic Model to explain Self-Supervised Representation Learning

    [https://rss.arxiv.org/abs/2402.01399](https://rss.arxiv.org/abs/2402.01399)

    该论文提出了一个概率模型来解释自监督表示学习的机制，并展示了鉴别性自监督算法在表示中近似诱导潜变量结构的统一理论框架。

    

    自监督学习（SSL）通过利用辅助的无监督任务，例如对语义相关样本进行分类，如不同的数据增强或模态来学习表示。在众多SSL方法中，对比方法（例如SimCLR，CLIP和VicREG）因学习到的表示在下游性能上接近有监督学习而受到关注。然而，这些方法背后的机制的理论理解仍然存在困难。我们提出了一个生成潜变量模型来表示数据，并展示了几类具有鉴别性的自监督算法（包括对比方法）近似诱导其表示中的潜变量结构，从而提供了一个统一的理论框架。我们还证明了与互信息和投影头的相关性。通过生成式地拟合我们的模型（如SimVE），在常见的基准测试上（例如FashionMNIST，CIFAR10，CelebA），性能优于之前的VAE方法。

    Self-supervised learning (SSL) learns representations by leveraging an auxiliary unsupervised task, such as classifying semantically related samples, e.g. different data augmentations or modalities. Of the many approaches to SSL, contrastive methods, e.g. SimCLR, CLIP and VicREG, have gained attention for learning representations that achieve downstream performance close to that of supervised learning. However, a theoretical understanding of the mechanism behind these methods eludes. We propose a generative latent variable model for the data and show that several families of discriminative self-supervised algorithms, including contrastive methods, approximately induce its latent structure over representations, providing a unifying theoretical framework. We also justify links to mutual information and the use of a projection head. Fitting our model generatively, as SimVE, improves performance over previous VAE methods on common benchmarks (e.g. FashionMNIST, CIFAR10, CelebA), narrows th
    
[^47]: ALERT-Transformer: 将异步和同步机器学习桥接在实时事件驱动的时空数据上

    ALERT-Transformer: Bridging Asynchronous and Synchronous Machine Learning for Real-Time Event-based Spatio-Temporal Data

    [https://rss.arxiv.org/abs/2402.01393](https://rss.arxiv.org/abs/2402.01393)

    ALERT-Transformer是一种将异步感知与同步处理相结合的新颖桥接方式，通过ALERT模块、灵活的数据读取和基于块的稀疏性优化，实现了对实时事件驱动时空数据的经典处理，其性能超过竞争对手并具有较低的延迟。

    

    我们旨在通过稠密机器学习模型，实现对由事件感应器产生的连续超稀疏时空数据的经典处理。我们提出了一种新颖的混合管道，由异步感知和同步处理组成，结合了几个思路：（1）基于PointNet模型的嵌入——ALERT模块，可以通过泄漏机制不断整合新事件并消除旧事件，（2）嵌入数据的灵活读取，可以以任何采样率将始终最新的特征输入到下游模型中，（3）借鉴Vision Transformer的基于块的方法来利用输入的稀疏性以优化方法的效率。这些嵌入然后由一个经过对象和手势识别训练的Transformer模型进行处理。使用这种方法，我们实现了比竞争对手更低的延迟，达到了最新技术水平的性能。我们还证明了我们的异步模型可以以任何所需的采样率进行操作。

    We seek to enable classic processing of continuous ultra-sparse spatiotemporal data generated by event-based sensors with dense machine learning models. We propose a novel hybrid pipeline composed of asynchronous sensing and synchronous processing that combines several ideas: (1) an embedding based on PointNet models -- the ALERT module -- that can continuously integrate new and dismiss old events thanks to a leakage mechanism, (2) a flexible readout of the embedded data that allows to feed any downstream model with always up-to-date features at any sampling rate, (3) exploiting the input sparsity in a patch-based approach inspired by Vision Transformer to optimize the efficiency of the method. These embeddings are then processed by a transformer model trained for object and gesture recognition. Using this approach, we achieve performances at the state-of-the-art with a lower latency than competitors. We also demonstrate that our asynchronous model can operate at any desired sampling r
    
[^48]: 齐次化随机梯度下降中重尾现象的出现

    Emergence of heavy tails in homogenized stochastic gradient descent

    [https://rss.arxiv.org/abs/2402.01382](https://rss.arxiv.org/abs/2402.01382)

    这项研究分析了齐次化随机梯度下降算法，在数值实验中验证了参数尾指数的明确上下界，并量化了优化参数与尾指数之间的相互作用，从而为重尾和神经网络的泛化性能以及SGD避免次优局部最小值能力之间的关系提供了贡献。

    

    随机梯度下降（SGD）通过最小化损失已经被发现导致神经网络参数的重尾分布。在这里，我们分析了SGD的连续扩散逼近，称为齐次化随机梯度下降，证明了它在渐进情况下表现出重尾特性，并给出了关于尾指数的明确上下界。我们在数值实验中验证了这些界并显示它们通常是SGD迭代的经验尾指数的近似。另外，它们的明确形式使我们能够量化优化参数和尾指数之间的相互作用。通过这样做，我们对于关于重尾和神经网络的泛化性能以及SGD避免次优局部最小值能力的联系的讨论做出了贡献。

    It has repeatedly been observed that loss minimization by stochastic gradient descent (SGD) leads to heavy-tailed distributions of neural network parameters. Here, we analyze a continuous diffusion approximation of SGD, called homogenized stochastic gradient descent, show that it behaves asymptotically heavy-tailed, and give explicit upper and lower bounds on its tail-index. We validate these bounds in numerical experiments and show that they are typically close approximations to the empirical tail-index of SGD iterates. In addition, their explicit form enables us to quantify the interplay between optimization parameters and the tail-index. Doing so, we contribute to the ongoing discussion on links between heavy tails and the generalization performance of neural networks as well as the ability of SGD to avoid suboptimal local minima.
    
[^49]: 使用增大系数幅度停止准则的正则化增强作为超参数优化集成中的元学习器

    Regularized boosting with an increasing coefficient magnitude stop criterion as meta-learner in hyperparameter optimization stacking ensemble

    [https://rss.arxiv.org/abs/2402.01379](https://rss.arxiv.org/abs/2402.01379)

    本论文提出了一种使用增大系数幅度停止准则的正则化增强作为超参数优化集成中的元学习器的方法。

    

    在超参数优化中，通过多次试验选择性能最佳的超参数配置，并将所有模型进行集成。最近，提出了更复杂的集成策略，如Caruana方法和stacking策略。Caruana方法在超参数优化集成中表现良好，不受多重共线性的影响。而stacking方法需要使用元学习器进行集成。然而，很少有关于如何选择元学习器的建议。

    In Hyperparameter Optimization (HPO), only the hyperparameter configuration with the best performance is chosen after performing several trials, then, discarding the effort of training all the models with every hyperparameter configuration trial and performing an ensemble of all them. This ensemble consists of simply averaging the model predictions or weighting the models by a certain probability. Recently, other more sophisticated ensemble strategies, such as the Caruana method or the stacking strategy has been proposed. On the one hand, the Caruana method performs well in HPO ensemble, since it is not affected by the effects of multicollinearity, which is prevalent in HPO. It just computes the average over a subset of predictions with replacement. But it does not benefit from the generalization power of a learning process. On the other hand, stacking methods include a learning procedure since a meta-learner is required to perform the ensemble. Yet, one hardly finds advice about which
    
[^50]: LoTR: 低张量秩权重自适应

    LoTR: Low Tensor Rank Weight Adaptation

    [https://rss.arxiv.org/abs/2402.01376](https://rss.arxiv.org/abs/2402.01376)

    LoTR是一种新颖的语言模型参数调优方法，通过引入低秩张量表示和张量分解，使得针对深层模型的参数效率更高，并且能够实现廉价且快速的下游调优。

    

    在本文中，我们将大型语言模型（LLM）上的低秩适应（LoRA）思想推广和扩展，这些模型基于Transformer架构。广泛使用的LoRA类方法是基于梯度更新的矩阵分解。我们引入了LoTR，一种新颖的LLM参数高效调优方法，它以张量分解的形式表示参数的梯度更新。每个层的低秩适配器都由三个矩阵的乘积构成，而张量结构是由这个乘积的左右乘子在层之间共享引起的。通过对低秩张量表示的一系列层同时压缩，LoTR能够比LoRA在特别是对于深层模型具有更好的参数效率。此外，核心张量不依赖于原始权重维度，可以任意缩小，从而实现非常廉价和快速的下游调优。

    In this paper we generalize and extend an idea of low-rank adaptation (LoRA) of large language models (LLMs) based on Transformer architecture. Widely used LoRA-like methods of fine-tuning LLMs are based on matrix factorization of gradient update. We introduce LoTR, a novel approach for parameter-efficient fine-tuning of LLMs which represents a gradient update to parameters in a form of tensor decomposition. Low-rank adapter for each layer is constructed as a product of three matrices, and tensor structure arises from sharing left and right multipliers of this product among layers. Simultaneous compression of a sequence of layers with low-rank tensor representation allows LoTR to archive even better parameter efficiency then LoRA especially for deep models. Moreover, the core tensor does not depend on original weight dimension and can be made arbitrary small, which allows for extremely cheap and fast downstream fine-tuning.
    
[^51]: Critic-Actor算法在平均奖励MDPs中的函数逼近问题：有限时间分析

    Critic-Actor for Average Reward MDPs with Function Approximation: A Finite-Time Analysis

    [https://rss.arxiv.org/abs/2402.01371](https://rss.arxiv.org/abs/2402.01371)

    本论文提出了一个评论家-演员算法，解决了长期平均奖励设置中的函数逼近问题，并进行了有限时间分析。实验结果表明，我们的算法能够在评论家的均方误差上界为$\epsilon$的情况下，获得样本复杂度为$\mathcal{\tilde{O}}(\epsilon^{-2.08})$，优于演员-评论家算法的结果。

    

    最近，关于两个时间尺度演员-评论家算法的渐近和非渐近收敛分析的研究工作非常活跃，其中演员的更新速度比评论家慢。在最近的一项工作中，提出了一个评论家-演员算法，用于无限时域折扣成本设置中的查找表情况，其中演员和评论家的时间尺度相反，并给出了渐近收敛分析。在我们的工作中，我们首次提出了一个具有函数逼近的评论家-演员算法，并在长期平均奖励设置中进行了首次有限时间（非渐近）分析。我们得到了最优的学习速率，并证明了我们的算法从评论家的均方误差上界为$\epsilon$，其样本复杂度为$\mathcal{\tilde{O}}(\epsilon^{-2.08})$，此结果比演员-评论家算法获得的结果要好。

    In recent years, there has been a lot of research work activity focused on carrying out asymptotic and non-asymptotic convergence analyses for two-timescale actor critic algorithms where the actor updates are performed on a timescale that is slower than that of the critic. In a recent work, the critic-actor algorithm has been presented for the infinite horizon discounted cost setting in the look-up table case where the timescales of the actor and the critic are reversed and asymptotic convergence analysis has been presented. In our work, we present the first critic-actor algorithm with function approximation and in the long-run average reward setting and present the first finite-time (non-asymptotic) analysis of such a scheme. We obtain optimal learning rates and prove that our algorithm achieves a sample complexity of $\mathcal{\tilde{O}}(\epsilon^{-2.08})$ for the mean squared error of the critic to be upper bounded by $\epsilon$ which is better than the one obtained for actor-critic
    
[^52]: 使用多模式先验的有针对性攻击文本到图像扩散模型

    Cheating Suffix: Targeted Attack to Text-To-Image Diffusion Models with Multi-Modal Priors

    [https://rss.arxiv.org/abs/2402.01369](https://rss.arxiv.org/abs/2402.01369)

    本文提出了一种名为MMP-Attack的有针对性攻击方法，通过整合文本和图像特征，该方法能够有效地攻击商业文本到图像模型，并且具有更高的普适性和可转移性。

    

    扩散模型已广泛应用于各种图像生成任务中，展现了图像和文本模态之间的卓越联系。然而，它们面临着被恶意利用的挑战，通过在原始提示后附加特定后缀来生成有害或敏感图像。现有作品主要关注使用单模态信息进行攻击，未能利用多模态特征，导致性能不尽如人意。在本工作中，我们提出了一种名为MMP-Attack的有针对性攻击方法，它将多模态先验（MMP）即文本和图像特征进行整合。具体而言，MMP-Attack的目标是在图像内容中添加目标对象的同时，同时移除原始对象。与现有作品相比，MMP-Attack具有更高的普适性和可转移性，在攻击商业文本到图像（T2I）模型（如DALL-E 3）方面表现出明显优势。据我们所知，这标志着当前最佳的技术水平。

    Diffusion models have been widely deployed in various image generation tasks, demonstrating an extraordinary connection between image and text modalities. However, they face challenges of being maliciously exploited to generate harmful or sensitive images by appending a specific suffix to the original prompt. Existing works mainly focus on using single-modal information to conduct attacks, which fails to utilize multi-modal features and results in less than satisfactory performance. Integrating multi-modal priors (MMP), i.e. both text and image features, we propose a targeted attack method named MMP-Attack in this work. Specifically, the goal of MMP-Attack is to add a target object into the image content while simultaneously removing the original object. The MMP-Attack shows a notable advantage over existing works with superior universality and transferability, which can effectively attack commercial text-to-image (T2I) models such as DALL-E 3. To the best of our knowledge, this marks 
    
[^53]: 大型语言模型的持续学习: 一项综述

    Continual Learning for Large Language Models: A Survey

    [https://rss.arxiv.org/abs/2402.01364](https://rss.arxiv.org/abs/2402.01364)

    这篇综述文章调查了大型语言模型(LLMs)的持续学习，使用了一种新颖的多阶段分类方案对持续学习技术进行了分类，指出了该领域面临的挑战和未来工作方向。

    

    由于其庞大的规模导致训练成本高昂，大型语言模型(LLMs)不易频繁重新训练。然而，更新是必要的，以赋予LLMs新的技能，并使其与快速发展的人类知识保持同步。本文对LLMs的持续学习最新研究进行了综述。鉴于LLMs的独特性，我们以一种新颖的多阶段分类方案对持续学习技术进行了分类，涉及持续预训练、指令调整和对齐等方面。我们将LLMs的持续学习与在规模较小的模型中使用的简单适应方法以及其他增强策略(如检索增强生成和模型编辑)进行了对比。此外，根据对基准和评估的讨论，我们确定了这一重要任务面临的几个挑战和未来工作方向。

    Large language models (LLMs) are not amenable to frequent re-training, due to high training costs arising from their massive scale. However, updates are necessary to endow LLMs with new skills and keep them up-to-date with rapidly evolving human knowledge. This paper surveys recent works on continual learning for LLMs. Due to the unique nature of LLMs, we catalog continue learning techniques in a novel multi-staged categorization scheme, involving continual pretraining, instruction tuning, and alignment. We contrast continual learning for LLMs with simpler adaptation methods used in smaller models, as well as with other enhancement strategies like retrieval-augmented generation and model editing. Moreover, informed by a discussion of benchmarks and evaluation, we identify several challenges and future work directions for this crucial task.
    
[^54]: 尽善尽美：重新定义强化学习中的奖励

    To the Max: Reinventing Reward in Reinforcement Learning

    [https://rss.arxiv.org/abs/2402.01361](https://rss.arxiv.org/abs/2402.01361)

    本研究通过引入最大奖励强化学习的方法，提出了一种替代传统奖励函数的学习方式，并在实验中证明了其在不同环境下的性能优势。

    

    在强化学习中，不同的奖励可以定义相同的最优策略，但学习性能却会有很大差异。对于某些情况，智能体会陷入次优行为，而对于其他情况，则能高效地解决任务。选择一个好的奖励函数因此是一个非常重要但具有挑战性的问题。在本文中，我们探索了一种替代奖励用于学习的方法。我们引入了最大奖励强化学习（max-reward RL），其中智能体优化的是最大奖励而不是累积奖励。与早期的方法不同，我们的方法适用于确定性和随机环境，并且可以与最先进的强化学习算法轻松结合。在实验中，我们研究了max-reward RL算法在Gymnasium-Robotics中的两个目标达成环境中的性能，并展示其相对于标准RL的优势。代码公开可用。

    In reinforcement learning (RL), different rewards can define the same optimal policy but result in drastically different learning performance. For some, the agent gets stuck with a suboptimal behavior, and for others, it solves the task efficiently. Choosing a good reward function is hence an extremely important yet challenging problem. In this paper, we explore an alternative approach to using rewards for learning. We introduce max-reward RL, where an agent optimizes the maximum rather than the cumulative reward. Unlike earlier works, our approach works for deterministic and stochastic environments and can be easily combined with state-of-the-art RL algorithms. In the experiments, we study the performance of max-reward RL algorithms in two goal-reaching environments from Gymnasium-Robotics and demonstrate its benefits over standard RL. The code is publicly available.
    
[^55]: TESSERACT: 消除恶意软件分类中的实验偏差的空间和时间方法（扩展版）

    TESSERACT: Eliminating Experimental Bias in Malware Classification across Space and Time (Extended Version)

    [https://rss.arxiv.org/abs/2402.01359](https://rss.arxiv.org/abs/2402.01359)

    本文提出TESSERACT方法，消除了恶意软件分类中的实验偏差，解决了常见的空间和时间偏差问题。通过公平实验设计约束和新指标AUT实现了更准确和稳定的分类器。

    

    机器学习在检测恶意软件方面扮演着关键角色。尽管许多研究报告的F1分数高达0.99，但问题仍未完全解决。恶意软件检测器常常在操作系统和攻击方法不断演化时出现性能下降，这会导致之前学习到的知识对于新输入的准确决策变得不足够。本文认为常见的研究结果由于检测任务中的两种普遍的实验偏差而被夸大：空间偏差是由数据分布不代表真实部署的情况引起的；时间偏差是由于数据的不正确时间分割引起的，导致了不现实的配置。为了解决这些偏差，我们引入了一组公平实验设计的约束，并提出了一个用于在真实环境中评估分类器稳定性的新指标AUT。我们还提出了一种用于调整训练数据以提高分类器鲁棒性的算法。

    Machine learning (ML) plays a pivotal role in detecting malicious software. Despite the high F1-scores reported in numerous studies reaching upwards of 0.99, the issue is not completely solved. Malware detectors often experience performance decay due to constantly evolving operating systems and attack methods, which can render previously learned knowledge insufficient for accurate decision-making on new inputs. This paper argues that commonly reported results are inflated due to two pervasive sources of experimental bias in the detection task: spatial bias caused by data distributions that are not representative of a real-world deployment; and temporal bias caused by incorrect time splits of data, leading to unrealistic configurations. To address these biases, we introduce a set of constraints for fair experiment design, and propose a new metric, AUT, for classifier robustness in real-world settings. We additionally propose an algorithm designed to tune training data to enhance classif
    
[^56]: 将表达丰富的问题空间规范高效编译为神经网络求解器

    Efficient compilation of expressive problem space specifications to neural network solvers

    [https://rss.arxiv.org/abs/2402.01353](https://rss.arxiv.org/abs/2402.01353)

    本文提出了一种算法，可以将高级的问题空间规范编译为适合神经网络求解器的满足性查询，以解决神经网络验证中存在的嵌入间隙问题。

    

    最近的研究揭示了神经网络验证中存在的嵌入间隙。在间隙的一侧是一个关于网络行为的高级规范，由领域专家根据可解释的问题空间编写。在另一侧是一组逻辑上等价的可满足性查询，以适合神经网络求解器的形式表达在不可理解的嵌入空间中。在本文中，我们描述了一种将前者编译为后者的算法。我们探索和克服了针对神经网络求解器而不是标准SMT求解器所出现的问题。

    Recent work has described the presence of the embedding gap in neural network verification. On one side of the gap is a high-level specification about the network's behaviour, written by a domain expert in terms of the interpretable problem space. On the other side are a logically-equivalent set of satisfiability queries, expressed in the uninterpretable embedding space in a form suitable for neural network solvers. In this paper we describe an algorithm for compiling the former to the latter. We explore and overcome complications that arise from targeting neural network solvers as opposed to standard SMT solvers.
    
[^57]: FedMoE: 数据级别个性化的混合专家模型用于异构模型的个性化联邦学习

    FedMoE: Data-Level Personalization with Mixture of Experts for Model-Heterogeneous Personalized Federated Learning

    [https://rss.arxiv.org/abs/2402.01350](https://rss.arxiv.org/abs/2402.01350)

    FedMoE是一种模型异构的个性化联邦学习算法，通过使用专家混合模型增强大型语言模型，将共享的小特征提取器和本地门控网络分配给每个客户端的本地异构大模型，以解决当前模型异构个性化联邦学习方法中存在的隐私、性能、通信和计算成本等问题。

    

    联邦学习广泛应用于分散数据的协同训练，但面临数据、系统和模型异构等挑战。这导致模型异构个性化联邦学习 (MHPFL) 的出现。然而，当前的MHPFL方法在数据和模型隐私、模型性能、通信和计算成本方面仍存在关切。为应对这些问题，我们提出了一种新颖的模型异构个性化联邦学习算法 (FedMoE) ，采用著名的专家混合模型 (MoE) 来增强大型语言模型 (LLM)。它为每个客户端的本地异构大模型分配了一个共享的均匀小特征提取器和一个本地门控网络。(1) 在本地训练过程中，本地异构模型的特征提取器作为个性化特征（表示）提取的本地专家，而共享的均匀小特征提取器则作为广义特征提取的全局专家。

    Federated learning (FL) is widely employed for collaborative training on decentralized data but faces challenges like data, system, and model heterogeneity. This prompted the emergency of model-heterogeneous personalized federated learning (MHPFL). However, concerns persist regarding data and model privacy, model performance, communication, and computational costs in current MHPFL methods. To tackle these concerns, we propose a novel model-heterogeneous personalized Federated learning algorithm (FedMoE) with the Mixture of Experts (MoE), renowned for enhancing large language models (LLMs). It assigns a shared homogeneous small feature extractor and a local gating network for each client's local heterogeneous large model. (1) During local training, the local heterogeneous model's feature extractor acts as a local expert for personalized feature (representation) extraction, while the shared homogeneous small feature extractor serves as a global expert for generalized feature extraction. 
    
[^58]: CORE：通过认知重播来减轻连续学习中的灾难性遗忘

    CORE: Mitigating Catastrophic Forgetting in Continual Learning through Cognitive Replay

    [https://rss.arxiv.org/abs/2402.01348](https://rss.arxiv.org/abs/2402.01348)

    本文通过COgnitive REplay（CORE）提出了一种在连续学习中减轻灾难性遗忘的新方法，通过自适应数量分配和以质量为重点的数据选择来优化重播缓冲区，取得了显著的准确率提高效果。

    

    本文介绍了一种显著减轻连续学习中的灾难性遗忘的新视角，强调模型保持现有知识并融入新信息的能力。目前的重播方法同等对待每个任务和数据样本，因此无法充分利用重播缓冲区的潜力。作为回应，我们提出了COgnitive REplay（CORE），它从人类认知复习过程中得到灵感。CORE包括两个关键策略：自适应数量分配和以质量为重点的数据选择。前者根据每个任务的遗忘速率自适应地调节回放缓冲区的分配，而后者保证在缓冲区中包含最能概括每个任务特征的代表性数据。我们的方法在分割CIFAR10上实现了37.95%的平均准确率，超过最佳基准方法6.52%。此外，它还显著提高了最差表现模型的准确率。

    This paper introduces a novel perspective to significantly mitigate catastrophic forgetting in continuous learning (CL), which emphasizes models' capacity to preserve existing knowledge and assimilate new information. Current replay-based methods treat every task and data sample equally and thus can not fully exploit the potential of the replay buffer. In response, we propose COgnitive REplay (CORE), which draws inspiration from human cognitive review processes. CORE includes two key strategies: Adaptive Quantity Allocation and Quality-Focused Data Selection. The former adaptively modulates the replay buffer allocation for each task based on its forgetting rate, while the latter guarantees the inclusion of representative data that best encapsulates the characteristics of each task within the buffer. Our approach achieves an average accuracy of 37.95% on split-CIFAR10, surpassing the best baseline method by 6.52%. Additionally, it significantly enhances the accuracy of the poorest-perfo
    
[^59]: 跳过$\textbackslash n$: 一种简单的方法减少大规模视觉-语言模型中的幻觉

    Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models

    [https://rss.arxiv.org/abs/2402.01345](https://rss.arxiv.org/abs/2402.01345)

    本文提出了一种新的视角，指出LVLMs中固有的偏见可能是多模态幻觉的关键因素。通过系统识别与段落分割符相关的语义漂移偏差，我们发现模型在训练数据中经常遇到明显的内容语义变化，导致幻觉的产生。

    

    最近大规模视觉-语言模型（LVLMs）的进展展示了其在视觉信息理解与人类语言方面的令人印象深刻的能力。尽管取得了这些进展，LVLMs仍然面临多模态幻觉的挑战，例如生成与视觉信息中不存在的对象相关的文本描述。然而，多模态幻觉的根本原因仍然未被充分探索。在本文中，我们提出了一个新的视角，认为LVLMs中固有的偏见可能是幻觉的关键因素。具体而言，我们系统地确定了与段落分割符（'$\textbackslash n\textbackslash n$'）相关的语义漂移偏差，即在训练数据中，在“$\textbackslash n\textbackslash n$”之前和之后的内容经常表现出显著的语义改变。这种模式使得模型推断在“$\textbackslash n\textbackslash n$”之后的内容应明显不同于前面的内容。

    Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination, such as generating text descriptions of objects that are not present in the visual information. However, the underlying fundamental reasons of multimodal hallucinations remain poorly explored. In this paper, we propose a new perspective, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations. Specifically, we systematically identify a semantic shift bias related to paragraph breaks ('$\textbackslash n\textbackslash n$'), where the content before and after '$\textbackslash n\textbackslash n$' in the training data frequently exhibit significant semantic changes. This pattern leads the model to infer that the contents following '$\textbackslash n\textbackslash n$' should be obviously different from the preceding contents wi
    
[^60]: 单调、Bi-Lipschitz和Polyak-\L{}ojasiewicz网络

    Monotone, Bi-Lipschitz, and Polyak-\L{}ojasiewicz Networks

    [https://rss.arxiv.org/abs/2402.01344](https://rss.arxiv.org/abs/2402.01344)

    这篇论文介绍了一种新的可逆神经网络BiLipNet，它具有调控输出敏感性和输入可区分性的能力。其中的主要创新是通过认证的强单调性和Lipschitz性的可逆残差层，与正交层组合构建了Bi-Lipschitz网络。另外，该论文还提出了满足Polyak-\L{}ojasiewicz条件的PLNet，并介绍了其应用于学习非凸代理损失的优势特性。

    

    本文介绍了一种新的BiLipNet，这是一种可逆的\emph{Bi-Lipschitz}神经网络，具有控制其\emph{Lipschitzness}（对输入扰动的输出敏感性）和\emph{inverse Lipschitzness}（不同输出的输入可区分性）的能力。主要贡献是一个新颖的可逆残差层，具有认证的强单调性和Lipschitz性，我们将其与正交层组合以构建Bi-Lipschitz网络。认证是基于增量二次约束的，与谱归一化相比，它能实现更紧密的界限。此外，我们将模型的反向计算形式化为三算子分裂问题，已知存在快速算法。基于所提出的Bi-Lipschitz网络，我们引入了一种新的标量输出网络，即PLNet，它满足Polyak-\L{}ojasiewicz条件。它可以用于学习具有有利特性的非凸代理损失，例如独特性和高效计算性。

    This paper presents a new \emph{bi-Lipschitz} invertible neural network, the BiLipNet, which has the ability to control both its \emph{Lipschitzness} (output sensitivity to input perturbations) and \emph{inverse Lipschitzness} (input distinguishability from different outputs). The main contribution is a novel invertible residual layer with certified strong monotonicity and Lipschitzness, which we compose with orthogonal layers to build bi-Lipschitz networks. The certification is based on incremental quadratic constraints, which achieves much tighter bounds compared to spectral normalization. Moreover, we formulate the model inverse calculation as a three-operator splitting problem, for which fast algorithms are known. Based on the proposed bi-Lipschitz network, we introduce a new scalar-output network, the PLNet, which satisfies the Polyak-\L{}ojasiewicz condition. It can be applied to learn non-convex surrogate losses with favourable properties, e.g., a unique and efficiently-computab
    
[^61]: 基于形态特征和模型无关的时间序列分类的可反事实局部解释方法

    Shapelet-based Model-agnostic Counterfactual Local Explanations for Time Series Classification

    [https://rss.arxiv.org/abs/2402.01343](https://rss.arxiv.org/abs/2402.01343)

    提出了一种基于形态特征和模型无关的后期解释方法，用于时间序列分类。该方法利用形态特征和TimeGAN生成可反事实解释，与最先进的方法相比，在解释性方面表现更好。

    

    在这项工作中，我们提出了一种基于形态特征的模型无关的后期解释方法，用于时间序列分类。提出的算法，称为Time-CF，利用形态特征和TimeGAN为任意时间序列分类器提供可反事实解释。我们在UCR时间序列存档中对几个真实世界的单变量时间序列分类任务进行了验证。结果表明，与最先进的方法相比，Time-CF生成的可反事实实例在接近度、敏感度、合理性和稀疏性等四个解释性指标方面表现更好。

    In this work, we propose a model-agnostic instance-based post-hoc explainability method for time series classification. The proposed algorithm, namely Time-CF, leverages shapelets and TimeGAN to provide counterfactual explanations for arbitrary time series classifiers. We validate the proposed method on several real-world univariate time series classification tasks from the UCR Time Series Archive. The results indicate that the counterfactual instances generated by Time-CF when compared to state-of-the-art methods, demonstrate better performance in terms of four explainability metrics: closeness, sensibility, plausibility, and sparsity.
    
[^62]: 通过置换子空间在训练过程中对神经元进行对齐，以改进线性模块连通性和模型融合

    Training-time Neuron Alignment through Permutation Subspace for Improving Linear Mode Connectivity and Model Fusion

    [https://rss.arxiv.org/abs/2402.01342](https://rss.arxiv.org/abs/2402.01342)

    本文提出了一个在训练过程中进行神经元对齐的方法，通过置换子空间减少了线性模块连通性的局限性，为模型融合算法的改进提供了可能性。

    

    在深度学习中，即使在相同初始化条件下，随机梯度下降算法经常产生具有功能相似但在权重空间中分散的解，这导致了线性模块连通性（LMC）的局限性。克服这些局限性对于理解深度学习动态和提高模型融合算法至关重要。以前的研究强调置换对称性在通过网络置换减少训练后的局限性方面的作用。然而，这些事后的方法需要额外的计算，在更大、更复杂的模型（如ViT，LLM）上效果较差，因为存在大量的置换矩阵。因此，在本文中，我们研究了训练过程中神经元的对齐。我们的假设是，在训练过程中的置换子空间可以免费减少LMC的局限性。我们发现，初始化时进行修剪可以支持这一假设。除了修剪之外，我们引入了TNA-PFN，一种简单而无损的算法，在训练过程中使用部分梯度掩码。TNA-PFN在理论上和实验上都得到了支持。

    In deep learning, stochastic gradient descent often yields functionally similar yet widely scattered solutions in the weight space even under the same initialization, causing barriers in the Linear Mode Connectivity (LMC) landscape. Overcoming these barriers is crucial for understanding deep learning dynamics and enhancing model-fusion algorithms. Previous studies highlight the role of permutation symmetry in reducing post-training barriers through network permutation. However, these post-hoc methods, demanding extra computations, are less effective for larger, complex models (e.g., ViT, LLM) due to numerous permutation matrices. Thus, in this paper, we study training-time neuron alignment. Our hypothesis suggests that training-time permutation subspace can reduce LMC barriers for free. We find that pruning at initialization supports this. Beyond pruning, we introduce TNA-PFN, a simple yet lossless algorithm using a partial gradient mask during training. TNA-PFN is theoretically and em
    
[^63]: 因果熵和信息增益的基本性质

    Fundamental Properties of Causal Entropy and Information Gain

    [https://rss.arxiv.org/abs/2402.01341](https://rss.arxiv.org/abs/2402.01341)

    本研究通过建立和分析因果熵和因果信息增益的基本性质，包括界限和链规则，阐明了因果熵与随机干预的关系，并提出了因果条件熵和因果条件信息增益的定义，为提升因果机器学习任务铺平了道路。

    

    最近的发展使得能够量化在结构因果模型(SCM)下的因果控制。这是通过引入一些量来编码在干预另一个变量时某个变量熵的变化来实现的。这些量被命名为因果熵和因果信息增益，旨在解决现有信息论方法在因果性在机器学习任务中起关键作用时的局限性。我们的研究通过建立和分析这些概念的基本性质，包括界限和链规则，对因果熵和因果信息增益的概念进行了形式上的理解。此外，我们阐明了因果熵与随机干预的关系，并提出了因果条件熵和因果条件信息增益的定义。总体而言，这个探索为提升因果机器学习任务铺平了道路。

    Recent developments enable the quantification of causal control given a structural causal model (SCM). This has been accomplished by introducing quantities which encode changes in the entropy of one variable when intervening on another. These measures, named causal entropy and causal information gain, aim to address limitations in existing information theoretical approaches for machine learning tasks where causality plays a crucial role. They have not yet been properly mathematically studied. Our research contributes to the formal understanding of the notions of causal entropy and causal information gain by establishing and analyzing fundamental properties of these concepts, including bounds and chain rules. Furthermore, we elucidate the relationship between causal entropy and stochastic interventions. We also propose definitions for causal conditional entropy and causal conditional information gain. Overall, this exploration paves the way for enhancing causal machine learning tasks th
    
[^64]: 使用联邦防御的SignSGD：通过梯度符号解码来利用对抗攻击

    SignSGD with Federated Defense: Harnessing Adversarial Attacks through Gradient Sign Decoding

    [https://rss.arxiv.org/abs/2402.01340](https://rss.arxiv.org/abs/2402.01340)

    本研究提出了一种名为SignSGD-FD的技术，在对抗性工作节点增加时保持了收敛率不变，利用了通过梯度符号解码获得的对抗性工作节点的梯度信息。实验证明了该技术的有效性。

    

    分布式学习是一种使用多个计算资源加速模型训练的有效方法。然而，由于梯度通信的巨大成本，工作节点和参数服务器之间存在着大量的通信延迟。SignSGD与多数投票（SignSGD-MV）是一种简单而有效的优化器，通过一位量化来减少通信成本，但是在对抗性工作节点增加时，收敛速度明显减慢。本文展示了在对抗性工作节点数量小于良性工作节点数量的情况下，收敛速度保持不变。这一反直觉的结果是由我们的新颖的SignSGD与联邦防御（SignSGD-FD）产生的关键思想所展示的。与传统方法不同，SignSGD-FD利用由对抗性工作节点发送的梯度信息，使用通过梯度符号解码获得的适当权重。实验结果证明了SignSGD-FD的有效性。

    Distributed learning is an effective approach to accelerate model training using multiple workers. However, substantial communication delays emerge between workers and a parameter server due to massive costs associated with communicating gradients. SignSGD with majority voting (signSGD-MV) is a simple yet effective optimizer that reduces communication costs through one-bit quantization, yet the convergence rates considerably decrease as adversarial workers increase. In this paper, we show that the convergence rate is invariant as the number of adversarial workers increases, provided that the number of adversarial workers is smaller than that of benign workers. The key idea showing this counter-intuitive result is our novel signSGD with federated defense (signSGD-FD). Unlike the traditional approaches, signSGD-FD exploits the gradient information sent by adversarial workers with the proper weights, which are obtained through gradient sign decoding. Experimental results demonstrate signS
    
[^65]: 通过贝叶斯神经网络推断带有不确定性的朗之万方程

    Inferring the Langevin Equation with Uncertainty via Bayesian Neural Networks

    [https://rss.arxiv.org/abs/2402.01338](https://rss.arxiv.org/abs/2402.01338)

    本研究提出了一个使用贝叶斯神经网络推断带有不确定性的朗之万方程的综合框架，可以应用于非线性和高维系统，通过提供预测的分布从而评估预测的不确定性。

    

    在各个领域普遍存在的随机系统表现出从分子动力学到气候现象的过程中的波动。朗之万方程作为一种常见的数学模型，被用于研究这种系统，可以预测它们的时间演化以及分析热力学量，包括吸收热量、对系统做的功以及熵的产生。然而，从观测轨迹中推断朗之万方程仍然具有挑战性，尤其是对于非线性和高维系统。在本研究中，我们提出了一个全面的框架，利用贝叶斯神经网络来推断过阻尼和欠阻尼情况下的朗之万方程。我们的框架首先分别提供漂移力和扩散矩阵，然后将它们组合起来构建朗之万方程。通过提供预测的分布而不仅仅是一个单一的值，我们的方法允许我们评估预测的不确定性，这可以防止潜在的…

    Pervasive across diverse domains, stochastic systems exhibit fluctuations in processes ranging from molecular dynamics to climate phenomena. The Langevin equation has served as a common mathematical model for studying such systems, enabling predictions of their temporal evolution and analyses of thermodynamic quantities, including absorbed heat, work done on the system, and entropy production. However, inferring the Langevin equation from observed trajectories remains challenging, particularly for nonlinear and high-dimensional systems. In this study, we present a comprehensive framework that employs Bayesian neural networks for inferring Langevin equations in both overdamped and underdamped regimes. Our framework first provides the drift force and diffusion matrix separately and then combines them to construct the Langevin equation. By providing a distribution of predictions instead of a single value, our approach allows us to assess prediction uncertainties, which can prevent potenti
    
[^66]: 在分布变化中的监督算法公平性：一项综述

    Supervised Algorithmic Fairness in Distribution Shifts: A Survey

    [https://rss.arxiv.org/abs/2402.01327](https://rss.arxiv.org/abs/2402.01327)

    这篇综述研究了分布变化下的监督公平机器学习领域，调查了各种类型的分布变化和现有的解决方法，并列举了公开数据集和评估指标。研究发现六种常用的方法，并探讨了与相关研究领域的交互关系。

    

    在分布变化下的监督公平机器学习是一个新兴领域，解决了面对从源领域到目标领域的数据分布变化时，如何保持公平和无偏预测的挑战。在现实世界的应用中，机器学习模型通常是在特定数据集上进行训练，但在部署时，数据分布可能因各种因素而随时间发生变化。这种变化可能导致不公平的预测，对特定通过敏感属性（如种族和性别）来表征的群体产生不均衡的影响。在这项调查中，我们对各种类型的分布变化进行了总结，并全面调查了基于这些变化的现有方法，在文献中突出了六种常用的方法。此外，这份调查列出了用于实证研究的公开可用数据集和评估指标。我们进一步探讨了与相关研究领域的交互关系，并讨论了其中的重要创新和贡献。

    Supervised fairness-aware machine learning under distribution shifts is an emerging field that addresses the challenge of maintaining equitable and unbiased predictions when faced with changes in data distributions from source to target domains. In real-world applications, machine learning models are often trained on a specific dataset but deployed in environments where the data distribution may shift over time due to various factors. This shift can lead to unfair predictions, disproportionately affecting certain groups characterized by sensitive attributes, such as race and gender. In this survey, we provide a summary of various types of distribution shifts and comprehensively investigate existing methods based on these shifts, highlighting six commonly used approaches in the literature. Additionally, this survey lists publicly available datasets and evaluation metrics for empirical studies. We further explore the interconnection with related research fields, discuss the significant c
    
[^67]: KTO: 模型对齐视为展望理论优化

    KTO: Model Alignment as Prospect Theoretic Optimization

    [https://rss.arxiv.org/abs/2402.01306](https://rss.arxiv.org/abs/2402.01306)

    本文提出了一种名为KTO的方法，将模型对齐视为展望理论优化。与当前方法相比，KTO直接最大化生成效用而不是最大化偏好对数似然。在多个规模上，KTO的性能与基于偏好的方法相当甚至更好。

    

    凯恩曼与特沃斯基的展望理论告诉我们，人类以有偏见但明确的方式看待随机变量；例如，人们通常都是厌恶损失的。我们证明了将LLMs与人工反馈进行对齐的目标隐含地融合了许多这些偏见 - 这些目标 (例如 DPO) 的成功部分可归因于它们是"人类感知损失函数"(HALOs)。然而，这些方法所归因给人类的效用函数仍与展望理论文献中的不同。利用凯恩曼-特沃斯基人类效用的模型，我们提出了一种直接最大化生成效用而不是最大化偏好对数似然的HALO。我们将这种方法称为凯恩曼-特沃斯基优化(KTO)，并且它在从1B到30B的规模上与基于偏好的方法的性能相匹配或超过。关键是，KTO不需要偏好 - 只需要一个是否的二进制信号。

    Kahneman & Tversky's $\textit{prospect theory}$ tells us that humans perceive random variables in a biased but well-defined manner; for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases -- the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them being $\textit{human-aware loss functions}$ (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach Kahneman-Tversky Optimization (KTO), and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B. Crucially, KTO does not need preferences -- only a binary signal of whether 
    
[^68]: 梯度聚类分布式数据的统一框架

    A Unified Framework for Gradient-based Clustering of Distributed Data

    [https://rss.arxiv.org/abs/2402.01302](https://rss.arxiv.org/abs/2402.01302)

    这是一篇关于梯度聚类分布式数据的统一框架的论文，作者提出了一族分布式聚类算法，可以在用户网络中工作。通过控制用户中心估计的接近程度和定义聚类损失函数，这些算法适用于不同的聚类任务。在提供了统一分析和几个强结果的基础上，这些算法都表现出了良好的收敛性和可行性。

    

    我们开发了一族分布式聚类算法，可以在用户网络中工作。在提出的场景中，用户包含一个本地数据集，并且只与其直接邻居进行通信，目标是寻找完整数据的聚类。所提出的家族称为分布式梯度聚类（DGC-$\mathcal{F}_\rho$），由参数化的$\rho\geq1$确定，控制用户中心估计的接近程度，而$\mathcal{F}$确定聚类损失。针对流行的聚类损失如$K$均值和Huber损失，DGC-$\mathcal{F}_\rho$产生了新的分布式聚类算法DGC-KM$_\rho$和DGC-HL$_\rho$，而基于逻辑函数的新型聚类损失导致了DGC-LL$_\rho$。我们提供了统一的分析并建立了几个强结果，在温和的假设下。首先，方法生成的中心序列在任何中心初始化和$...

    We develop a family of distributed clustering algorithms that work over networks of users. In the proposed scenario, users contain a local dataset and communicate only with their immediate neighbours, with the aim of finding a clustering of the full, joint data. The proposed family, termed Distributed Gradient Clustering (DGC-$\mathcal{F}_\rho$), is parametrized by $\rho \geq 1$, controling the proximity of users' center estimates, with $\mathcal{F}$ determining the clustering loss. Specialized to popular clustering losses like $K$-means and Huber loss, DGC-$\mathcal{F}_\rho$ gives rise to novel distributed clustering algorithms DGC-KM$_\rho$ and DGC-HL$_\rho$, while a novel clustering loss based on the logistic function leads to DGC-LL$_\rho$. We provide a unified analysis and establish several strong results, under mild assumptions. First, the sequence of centers generated by the methods converges to a well-defined notion of fixed point, under any center initialization and value of $
    
[^69]: 通过特征谱表征核岭回归的过拟合

    Characterizing Overfitting in Kernel Ridgeless Regression Through the Eigenspectrum

    [https://rss.arxiv.org/abs/2402.01297](https://rss.arxiv.org/abs/2402.01297)

    我们通过推导核矩阵的特征数界限，增强了核岭回归的测试误差界限。对于多项式谱衰减的核，我们恢复了先前的结果；对于指数谱衰减，我们提出了新的非平凡的界限。我们的研究表明，特征谱衰减多项式的核回归器具有良好的泛化能力，而特征谱指数衰减的核回归器则具有灾难性的过拟合。

    

    我们推导了核矩阵的条件数的新界限，然后利用这些界限增强了在固定输入维度的过参数化区域中核岭回归的现有非渐近测试误差界限。对于具有多项式谱衰减的核，我们恢复了先前工作的界限；对于指数衰减，我们的界限是非平凡和新颖的。我们对过拟合的结论是双重的：(i) 谱衰减多项式的核回归器必须在存在噪声标记的训练数据的情况下得到很好的泛化；这些模型表现出所谓的温和过拟合；(ii) 如果任何核岭回归器的特征谱指数衰减，则其泛化差，即表现出灾难性过拟合。这增加了核岭回归器表现出良性过拟合的可用特征谱衰减次多项式的极端情况的表征。我们的分析结合了新的随机矩阵理论(RMT)。

    We derive new bounds for the condition number of kernel matrices, which we then use to enhance existing non-asymptotic test error bounds for kernel ridgeless regression in the over-parameterized regime for a fixed input dimension. For kernels with polynomial spectral decay, we recover the bound from previous work; for exponential decay, our bound is non-trivial and novel.   Our conclusion on overfitting is two-fold: (i) kernel regressors whose eigenspectrum decays polynomially must generalize well, even in the presence of noisy labeled training data; these models exhibit so-called tempered overfitting; (ii) if the eigenspectrum of any kernel ridge regressor decays exponentially, then it generalizes poorly, i.e., it exhibits catastrophic overfitting. This adds to the available characterization of kernel ridge regressors exhibiting benign overfitting as the extremal case where the eigenspectrum of the kernel decays sub-polynomially. Our analysis combines new random matrix theory (RMT) te
    
[^70]: Bi-CryptoNets: 利用不同级别的隐私保护进行加密推断

    Bi-CryptoNets: Leveraging Different-Level Privacy for Encrypted Inference

    [https://rss.arxiv.org/abs/2402.01296](https://rss.arxiv.org/abs/2402.01296)

    本文介绍了一种利用不同级别的隐私保护进行加密推断的方法，通过将输入数据划分为敏感和不敏感的部分，并分别处理，同时利用知识蒸馏进行训练。

    

    隐私保护的神经网络近年来引起了越来越多的关注，人们从加密的角度开发了各种算法，以在准确性、计算复杂度和信息安全之间保持平衡。本研究从输入数据和神经网络的结构出发，将输入数据（例如一些图像）根据重要性和隐私划分为敏感和不敏感的部分。敏感的部分包含一些重要和私密的信息，如人脸，我们使用强同态加密来保持安全性，而不敏感的部分则包含一些背景信息，我们添加扰动。我们提出了bi-CryptoNets，即明文和密文分支，分别处理两个部分，并且密文分支可以通过单向连接利用明文分支的信息。我们通过知识蒸馏来训练我们的bi-CryptoNets，从一个w上进行表示的传递。

    Privacy-preserving neural networks have attracted increasing attention in recent years, and various algorithms have been developed to keep the balance between accuracy, computational complexity and information security from the cryptographic view. This work takes a different view from the input data and structure of neural networks. We decompose the input data (e.g., some images) into sensitive and insensitive segments according to importance and privacy. The sensitive segment includes some important and private information such as human faces and we take strong homomorphic encryption to keep security, whereas the insensitive one contains some background and we add perturbations. We propose the bi-CryptoNets, i.e., plaintext and ciphertext branches, to deal with two segments, respectively, and ciphertext branch could utilize the information from plaintext branch by unidirectional connections. We adopt knowledge distillation for our bi-CryptoNets by transferring representations from a w
    
[^71]: ExtremeCast: 提升全球天气预报的极值预测能力

    ExtremeCast: Boosting Extreme Value Prediction for Global Weather Forecast

    [https://rss.arxiv.org/abs/2402.01295](https://rss.arxiv.org/abs/2402.01295)

    ExtremeCast提出了一种新的损失函数Exloss，实现了针对极值的准确预测，同时引入了无需训练的极值增强策略ExEnsemble，提高了预报的稳健性

    

    基于机器学习的数据驱动天气预报在全球中期预报中已经得到了快速发展，并且相较于传统的基于物理的动力学模型表现出更好的性能。然而，大多数这些机器学习模型在准确预测极端天气方面存在困难，而极端值预测与此密切相关。通过数学分析，我们证明使用对称损失，如均方误差（MSE），会导致预测有偏差并低估极值。为了解决这个问题，我们引入了Exloss，一种新的损失函数，通过非对称优化突出极值，以获得准确的极端天气预报。此外，我们还引入了一种无需训练的极值增强策略ExEnsemble，它增加了像素值的方差，并提高了预报的稳健性。结合先进的全球天气预报模型，广泛的实验证明了我们的方法

    Data-driven weather forecast based on machine learning (ML) has experienced rapid development and demonstrated superior performance in the global medium-range forecast compared to traditional physics-based dynamical models. However, most of these ML models struggle with accurately predicting extreme weather, which is closely related to the extreme value prediction. Through mathematical analysis, we prove that the use of symmetric losses, such as the Mean Squared Error (MSE), leads to biased predictions and underestimation of extreme values. To address this issue, we introduce Exloss, a novel loss function that performs asymmetric optimization and highlights extreme values to obtain accurate extreme weather forecast. Furthermore, we introduce a training-free extreme value enhancement strategy named ExEnsemble, which increases the variance of pixel values and improves the forecast robustness. Combined with an advanced global weather forecast model, extensive experiments show that our sol
    
[^72]: MLLMs能否进行上下文学习的文本到图像转换？

    Can MLLMs Perform Text-to-Image In-Context Learning?

    [https://rss.arxiv.org/abs/2402.01293](https://rss.arxiv.org/abs/2402.01293)

    本论文探索了将上下文学习扩展到多模态的文本到图像转换任务，并提出了第一个T2I-ICL基准数据集CoBSAT。研究发现MLLMs在解决T2I-ICL问题时面临着多模态和图像生成的固有复杂性，并通过微调和思维链提示等策略取得了显著改进。

    

    从大型语言模型（LLMs）发展到多模式大型语言模型（MLLMs）推动了将上下文学习（ICL）扩展到多模式的研究。现有的研究主要集中在图像到文本的ICL上。然而，文本到图像的ICL（T2I-ICL）具有独特的特性和潜在的应用，但仍然少有研究。为了填补这个空白，我们正式定义了T2I-ICL任务，并提出了CoBSAT，第一个包含十个任务的T2I-ICL基准数据集。利用我们的数据集评估了六个最先进的MLLMs，我们发现MLLMs在解决T2I-ICL问题时面临着相当大的困难。我们确定了多模态和图像生成的固有复杂性是主要挑战。为了克服这些挑战，我们探索了微调和思维链提示等策略，并取得了显著的改进。我们的代码和数据集可以在\url{https://github.com/UW-Madison-Lee-Lab/CoBSAT}上获得。

    The evolution from Large Language Models (LLMs) to Multimodal Large Language Models (MLLMs) has spurred research into extending In-Context Learning (ICL) to its multimodal counterpart. Existing such studies have primarily concentrated on image-to-text ICL. However, the Text-to-Image ICL (T2I-ICL), with its unique characteristics and potential applications, remains underexplored. To address this gap, we formally define the task of T2I-ICL and present CoBSAT, the first T2I-ICL benchmark dataset, encompassing ten tasks. Utilizing our dataset to benchmark six state-of-the-art MLLMs, we uncover considerable difficulties MLLMs encounter in solving T2I-ICL. We identify the primary challenges as the inherent complexity of multimodality and image generation. To overcome these challenges, we explore strategies like fine-tuning and Chain-of-Thought prompting, demonstrating notable improvements. Our code and dataset are available at \url{https://github.com/UW-Madison-Lee-Lab/CoBSAT}.
    
[^73]: Spiking CenterNet:一种用于目标检测的蒸馏增强脉冲神经网络

    Spiking CenterNet: A Distillation-boosted Spiking Neural Network for Object Detection

    [https://rss.arxiv.org/abs/2402.01287](https://rss.arxiv.org/abs/2402.01287)

    Spiking CenterNet是一种利用脉冲神经网络和知识蒸馏相结合的新方法，用于高效能量、小型化嵌入式人工智能的目标检测。在挑战性的GEN1汽车检测数据集上，我们的模型使用的能量不到一半，表现超过了可比较的先前工作。

    

    在边缘人工智能、自动驾驶汽车和气候变化时代，对于高效能量、小型化嵌入式人工智能的需求正在增长。脉冲神经网络（SNN）是一种解决这一挑战的有希望的方法，其具有事件驱动的信息流和稀疏激活的特点。我们提出了一种用于事件数据的目标检测的Spiking CenterNet。它将SNN CenterNet适应和高效的基于M2U-Net的解码器结合起来。我们的模型在Prophesee的具有挑战性的GEN1汽车检测数据集上表现优于可比较的先前工作，并且使用的能量不到一半。将非脉冲老师的知识蒸馏到我们的SNN中进一步提高了性能。据我们所知，我们的工作是脉冲目标检测领域首个利用知识蒸馏的方法。

    In the era of AI at the edge, self-driving cars, and climate change, the need for energy-efficient, small, embedded AI is growing. Spiking Neural Networks (SNNs) are a promising approach to address this challenge, with their event-driven information flow and sparse activations. We propose Spiking CenterNet for object detection on event data. It combines an SNN CenterNet adaptation with an efficient M2U-Net-based decoder. Our model significantly outperforms comparable previous work on Prophesee's challenging GEN1 Automotive Detection Dataset while using less than half the energy. Distilling the knowledge of a non-spiking teacher into our SNN further increases performance. To the best of our knowledge, our work is the first approach that takes advantage of knowledge distillation in the field of spiking object detection.
    
[^74]: 在球面和球上的可微分和加速小波变换

    Differentiable and accelerated wavelet transforms on the sphere and ball

    [https://rss.arxiv.org/abs/2402.01282](https://rss.arxiv.org/abs/2402.01282)

    本研究设计了新的高度可分布和自动可微分的方向小波变换，在球面和球上的信号处理中取得了显著的加速效果，同时保持高精度，具有重要的实际应用价值。

    

    方向小波字典是一种层次结构的表示方法，可以高效地捕捉和分割各种尺度、位置和方向上的信息。这种表示方法对于物理信号具有特殊的亲和性，因为物理信号通常具有高各向异性、局部多尺度结构。许多重要的物理信号在球形域上观测，例如宇宙学中的天空。借助最近在计算谐波分析方面的进展，我们设计了新的高度可分布和自动可微分的方向小波变换，应用于二维球面S^2和三维球体B^3=R^+ x S^2（通过将球面与径向半线结合而形成的空间）。与现有软件相比，我们观察到球面和球上信号的加速比分别高达300倍和21800倍，同时保持64位机器精度。这些算法不仅在加速领域取得了显著的进展，同时具有可微分性质。

    Directional wavelet dictionaries are hierarchical representations which efficiently capture and segment information across scale, location and orientation. Such representations demonstrate a particular affinity to physical signals, which often exhibit highly anisotropic, localised multiscale structure. Many physically important signals are observed over spherical domains, such as the celestial sky in cosmology. Leveraging recent advances in computational harmonic analysis, we design new highly distributable and automatically differentiable directional wavelet transforms on the $2$-dimensional sphere $\mathbb{S}^2$ and $3$-dimensional ball $\mathbb{B}^3 = \mathbb{R}^+ \times \mathbb{S}^2$ (the space formed by augmenting the sphere with the radial half-line). We observe up to a $300$-fold and $21800$-fold acceleration for signals on the sphere and ball, respectively, compared to existing software, whilst maintaining 64-bit machine precision. Not only do these algorithms dramatically acce
    
[^75]: 参数任务MAP-Elites

    Parametric-Task MAP-Elites

    [https://rss.arxiv.org/abs/2402.01275](https://rss.arxiv.org/abs/2402.01275)

    本文引入了参数任务MAP-Elites（PT-ME）算法，解决连续多任务优化问题。该算法在每次迭代中解决新任务，利用局部线性回归进行变异操作，通过得到的解集数据集创建了映射任务参数到最优解的函数，实验证明PT-ME算法优于所有基准算法。

    

    将一组函数的优化同时应用于它们的相似性被称为多任务优化。当前的黑箱多任务算法仅解决有限的一组任务，即使这些任务来自连续空间。在本文中，我们介绍了参数任务MAP-Elites（PT-ME），一种解决连续多任务优化问题的新型黑箱算法。该算法（1）在每次迭代中解决一个新任务，有效地涵盖了连续空间，（2）利用基于局部线性回归的新变异操作符。所得到的解集数据集使得能够创建一个将任何任务参数映射到其最优解的函数。通过在两个参数任务玩具问题和一个更现实和具有挑战性的仿真机器人问题上的表现，我们展示了PT-ME优于所有基准算法，包括深度强化学习算法PPO。

    Optimizing a set of functions simultaneously by leveraging their similarity is called multi-task optimization. Current black-box multi-task algorithms only solve a finite set of tasks, even when the tasks originate from a continuous space. In this paper, we introduce Parametric-task MAP-Elites (PT-ME), a novel black-box algorithm to solve continuous multi-task optimization problems. This algorithm (1) solves a new task at each iteration, effectively covering the continuous space, and (2) exploits a new variation operator based on local linear regression. The resulting dataset of solutions makes it possible to create a function that maps any task parameter to its optimal solution. We show on two parametric-task toy problems and a more realistic and challenging robotic problem in simulation that PT-ME outperforms all baselines, including the deep reinforcement learning algorithm PPO.
    
[^76]: 关于大规模自监督学习在少样本音频分类中的可迁移性

    On the Transferability of Large-Scale Self-Supervision to Few-Shot Audio Classification

    [https://rss.arxiv.org/abs/2402.01274](https://rss.arxiv.org/abs/2402.01274)

    本研究评估了大规模自监督模型在少样本音频分类中的性能，并发现在一些少样本问题中取得了最先进的性能，同时发现语音为基础的少样本问题与多个下游音频任务之间存在较强的相关性。

    

    近年来，自监督学习因其能够从无标签数据中学习到稳健的特征表示而表现出色。经过自监督预训练的网络可作为下游任务（包括少样本学习）中有效的特征提取器。尽管对于图像的无监督学习方法在少样本学习中的评估已经有了良好的基础，但在声学领域却明显缺失。本研究通过评估大规模自监督模型在少样本音频分类中的性能，弥补了这一空白。此外，我们还探讨了模型的少样本学习能力与其他下游任务基准之间的关系。我们的研究结果表明，在一些少样本问题（如SpeechCommandsv2）中，我们取得了最先进的性能，并且语音为基础的少样本问题与多个下游音频任务之间存在着较强的相关性。

    In recent years, self-supervised learning has excelled for its capacity to learn robust feature representations from unlabelled data. Networks pretrained through self-supervision serve as effective feature extractors for downstream tasks, including Few-Shot Learning. While the evaluation of unsupervised approaches for few-shot learning is well-established in imagery, it is notably absent in acoustics. This study addresses this gap by assessing large-scale self-supervised models' performance in few-shot audio classification. Additionally, we explore the relationship between a model's few-shot learning capability and other downstream task benchmarks. Our findings reveal state-of-the-art performance in some few-shot problems such as SpeechCommandsv2, as well as strong correlations between speech-based few-shot problems and various downstream audio tasks.
    
[^77]: 直接学习零样本回归的边缘信息

    Direct side information learning for zero-shot regression

    [https://rss.arxiv.org/abs/2402.01264](https://rss.arxiv.org/abs/2402.01264)

    本论文提出了一种直接学习零样本回归的边缘信息的方法，解决了现有方法在零样本回归框架下的局限性，并有效地利用目标侧信息。这种方法为零样本回归问题提供了一个新的解决方案。

    

    零样本学习为没有目标实例的目标提供了模型，通常称为未观察到的目标。在这种情况下，目标侧信息的可用性变得至关重要，以便正确地诱导这些目标的模型。文献中有许多应对这种情况的策略，但大多是基于零样本分类场景而设计的，主要用于计算机视觉和图像分类，但对于连续值预测的零样本回归框架来说，它们要么不适用，要么不容易扩展。事实上，文献中缺乏针对零样本回归的方法。最近提出了两种适用于零样本回归的两阶段过程的方法。它们首先通过传统的回归学习学习观察到的目标模型，忽略目标侧信息。然后，它们在之后聚合这些观察到的目标模型，进一步利用目标侧信息。

    Zero-shot learning provides models for targets for which instances are not available, commonly called unobserved targets. The availability of target side information becomes crucial in this context in order to properly induce models for these targets. The literature is plenty of strategies to cope with this scenario, but specifically designed on the basis of a zero-shot classification scenario, mostly in computer vision and image classification, but they are either not applicable or easily extensible for a zero-shot regression framework for which a continuos value is required to be predicted rather than a label. In fact, there is a considerable lack of methods for zero-shot regression in the literature. Two approaches for zero-shot regression that work in a two-phase procedure were recently proposed. They first learn the observed target models through a classical regression learning ignoring the target side information. Then, they aggregate those observed target models afterwards explo
    
[^78]: 具有前向-后向消息传递的可微分POGLM

    A Differentiable POGLM with Forward-Backward Message Passing

    [https://rss.arxiv.org/abs/2402.01263](https://rss.arxiv.org/abs/2402.01263)

    提出了一种具有前向-后向消息传递的可微分POGLM模型，解决了现有POGLM学习中的路径梯度估计和变分模型设计等问题。

    

    在隐含神经元存在的假设下，部分可观察广义线性模型（POGLM）是理解神经连接的强大工具。现有的工作利用变分推断来学习POGLM，但学习这种潜变量模型存在困难。存在两个主要问题：（1）采样的泊松隐藏尖峰数量阻碍了使用路径梯度估计器进行变分推断；（2）现有的变分模型设计既不具有表达性也不具有时间效率，进一步影响了性能。针对问题（1），我们提出了一种新的可微分POGLM，可以使用路径梯度估计器，优于现有工作中使用的得分函数梯度估计器。针对问题（2），我们提出了前向-后向消息传递采样方案用于变分模型。综合实验表明，我们的可微分POGLM与我们的前向-后向消息传递产生了更好的结果。

    The partially observable generalized linear model (POGLM) is a powerful tool for understanding neural connectivity under the assumption of existing hidden neurons. With spike trains only recorded from visible neurons, existing works use variational inference to learn POGLM meanwhile presenting the difficulty of learning this latent variable model. There are two main issues: (1) the sampled Poisson hidden spike count hinders the use of the pathwise gradient estimator in VI; and (2) the existing design of the variational model is neither expressive nor time-efficient, which further affects the performance. For (1), we propose a new differentiable POGLM, which enables the pathwise gradient estimator, better than the score function gradient estimator used in existing works. For (2), we propose the forward-backward message-passing sampling scheme for the variational model. Comprehensive experiments show that our differentiable POGLMs with our forward-backward message passing produce a bette
    
[^79]: 级联缩放分类器：通过概率缩放进行类别增量学习

    Cascaded Scaling Classifier: class incremental learning with probability scaling

    [https://rss.arxiv.org/abs/2402.01262](https://rss.arxiv.org/abs/2402.01262)

    提出了级联缩放分类器，结合边际抑制和知识蒸馏方法，用于实现神经网络中的连续学习，并降低过去任务的遗忘。

    

    人类有能力获取新知识并将学习到的知识转移到不同的领域，仅有轻微的遗忘。同样的能力，在神经网络中实现连续学习是具有挑战性的，因为在学习新任务时会影响到过去学习的任务。这种遗忘可以通过回放存储的过去任务样本来缓解，但是对于长序列任务可能需要较大的存储空间；此外，这可能导致对保存样本的过拟合。在本文中，我们提出了一种新颖的正则化方法和一种新颖的增量分类器，分别称为边际抑制和级联缩放分类器。前者结合了软约束和知识蒸馏方法，以保留过去学习的知识同时有效地学习新的模式。后者是一种带有门控的增量分类器，帮助模型修改过去的预测而不直接干扰它们。这是通过...

    Humans are capable of acquiring new knowledge and transferring learned knowledge into different domains, incurring a small forgetting. The same ability, called Continual Learning, is challenging to achieve when operating with neural networks due to the forgetting affecting past learned tasks when learning new ones. This forgetting can be mitigated by replaying stored samples from past tasks, but a large memory size may be needed for long sequences of tasks; moreover, this could lead to overfitting on saved samples. In this paper, we propose a novel regularisation approach and a novel incremental classifier called, respectively, Margin Dampening and Cascaded Scaling Classifier. The first combines a soft constraint and a knowledge distillation approach to preserve past learned knowledge while allowing the model to learn new patterns effectively. The latter is a gated incremental classifier, helping the model modify past predictions without directly interfering with them. This is achieved
    
[^80]: TEDDY: 基于度量判别策略的边缘修剪方法

    TEDDY: Trimming Edges with Degree-based Discrimination strategY

    [https://rss.arxiv.org/abs/2402.01261](https://rss.arxiv.org/abs/2402.01261)

    TEDDY是一种利用边缘度量信息的边缘修剪方法，旨在通过一次性操作实现边缘稀疏化，进而鼓励参数稀疏化训练。这是一个解决图神经网络中抽奖票假设的时间效率和效果问题的创新方法。

    

    自从Chen等人在2021年提出用于图神经网络（GNNs）的抽奖票假设的开创性工作以来，寻找图抽奖票（GLT）的研究已成为GNN社区的重要关注点之一，激发了研究人员在实现与原始密集网络相当性能的同时，发现更稀疏的GLT。同时，图结构作为GNN训练动力学的重要因素，也受到了广泛关注，并得到了最近几项研究的阐明。尽管如此，目前关于GLT的研究通常没有充分利用图结构中的内在路径，并以迭代方式识别票数，这种方法耗时且效率低下。为解决这些限制，我们引入TEDDY，一种利用结构信息并整合边缘度量信息的一次性边缘稀疏化框架。在进行边缘稀疏化后，我们通过简单的投影梯度下降方法鼓励参数稀疏化训练。

    Since the pioneering work on the lottery ticket hypothesis for graph neural networks (GNNs) was proposed in Chen et al. (2021), the study on finding graph lottery tickets (GLT) has become one of the pivotal focus in the GNN community, inspiring researchers to discover sparser GLT while achieving comparable performance to original dense networks. In parallel, the graph structure has gained substantial attention as a crucial factor in GNN training dynamics, also elucidated by several recent studies. Despite this, contemporary studies on GLT, in general, have not fully exploited inherent pathways in the graph structure and identified tickets in an iterative manner, which is time-consuming and inefficient. To address these limitations, we introduce TEDDY, a one-shot edge sparsification framework that leverages structural information by incorporating edge-degree information. Following edge sparsification, we encourage the parameter sparsity during training via simple projected gradient desc
    
[^81]: 利用深度学习的位置感知60 GHz毫米波波束成形技术用于车辆到车辆通信

    Position Aware 60 GHz mmWave Beamforming for V2V Communications Utilizing Deep Learning

    [https://rss.arxiv.org/abs/2402.01259](https://rss.arxiv.org/abs/2402.01259)

    本文提出了一种利用深度学习的方法，通过预测具有足够毫米波接收功率的最佳波束，使用车辆位置信息来实现车辆到车辆通信中的高效链路配置。

    

    波束成形技术是通过采用大规模天线阵列和生成窄波束来弥补毫米波通信中的严重路径损耗，以获得令人满意的接收功率的重要部分。然而，传统的波束选择方法主要依赖于信道状态信息来进行准确的波束对准，并为高效的链路配置带来显著的延迟和计算开销，在车辆到车辆（V2V）通信中如高度动态的场景中通常是不可行的。相比之下，利用带外上下文信息（如车辆位置信息）是减少这种开销的潜在替代方法。在这个背景下，本文提出了一种基于深度学习的解决方案，利用车辆位置信息来预测具有足够毫米波接收功率的最佳波束，从而可以主动确保最佳的V2V直视链路。

    Beamforming techniques are considered as essential parts to compensate the severe path loss in millimeter-wave (mmWave) communications by adopting large antenna arrays and formulating narrow beams to obtain satisfactory received powers. However, performing accurate beam alignment over such narrow beams for efficient link configuration by traditional beam selection approaches, mainly relied on channel state information, typically impose significant latency and computing overheads, which is often infeasible in vehicle-to-vehicle (V2V) communications like highly dynamic scenarios. In contrast, utilizing out-of-band contextual information, such as vehicular position information, is a potential alternative to reduce such overheads. In this context, this paper presents a deep learning-based solution on utilizing the vehicular position information for predicting the optimal beams having sufficient mmWave received powers so that the best V2V line-of-sight links can be ensured proactively. Afte
    
[^82]: Transformers在上下文中学习非线性特征：关于注意力场景中的非凸均场动态研究

    Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field Dynamics on the Attention Landscape

    [https://rss.arxiv.org/abs/2402.01258](https://rss.arxiv.org/abs/2402.01258)

    本文研究了基于Transformer架构的大型语言模型在上下文中学习非线性特征的优化问题，通过在均场和两个时间尺度的极限情况下的分析，证明了参数分布的损失景观虽然高度非凸，但变得相当温和，并建立了新的方法来获得具体的改进速率，这将有助于增强上下文学习的能力。

    

    基于Transformer架构的大型语言模型展示了在上下文中学习的令人印象深刻的能力。然而，关于这一现象产生的现有理论研究仅限于对线性回归任务上训练的单层注意力的动态。在本文中，我们研究了一个由全连接层和线性注意力层组成的Transformer的优化。MLP充当了一个常见的非线性表示或特征映射，极大地增强了上下文学习的能力。我们在均场和两个时间尺度的极限情况下证明了参数分布的无限维损失景观，虽然高度非凸，但变得相当温和。我们还分析了均场动态的二阶稳定性，并表明Wasserstein梯度流几乎总是避开鞍点。此外，我们建立了获得远离临界点和接近临界点的具体改进速率的新方法。

    Large language models based on the Transformer architecture have demonstrated impressive capabilities to learn in context. However, existing theoretical studies on how this phenomenon arises are limited to the dynamics of a single layer of attention trained on linear regression tasks. In this paper, we study the optimization of a Transformer consisting of a fully connected layer followed by a linear attention layer. The MLP acts as a common nonlinear representation or feature map, greatly enhancing the power of in-context learning. We prove in the mean-field and two-timescale limit that the infinite-dimensional loss landscape for the distribution of parameters, while highly nonconvex, becomes quite benign. We also analyze the second-order stability of mean-field dynamics and show that Wasserstein gradient flow almost always avoids saddle points. Furthermore, we establish novel methods for obtaining concrete improvement rates both away from and near critical points. This represents the 
    
[^83]: 针对零射回归的目标感知方法

    Target inductive methods for zero-shot regression

    [https://rss.arxiv.org/abs/2402.01252](https://rss.arxiv.org/abs/2402.01252)

    本论文提出了两种针对零射回归问题的方法，一种是基于相似性的方法，另一种是集成学习的方法。这些方法旨在利用周围环境信息对气象站的空气污染物含量进行预测。

    

    这项研究的出发点是预测气象站的空气污染物含量。空气污染物取决于站点的位置（天气条件和周围活动）。在学习过程中经常忽略周围环境信息。在没有观测到的天气条件下，这些信息事先是已知的，并且对于同一个站点保持不变。将周围环境信息视为附加信息有助于在新站点中预测污染物，从而导致了零射回归场景。目前在零射问题中存在的方法通常偏向于分类，而且很难推广至回归问题。本文提出了两种用于回归的零射方法。第一种方法是基于相似性的方法，它从特征学习模型，并使用附加信息对它们进行汇总。然而，在汇总过程中可能会丢失特征模型的潜在知识。第二种方法则是一种集成学习的方法，它结合了特征模型和附加信息。

    This research arises from the need to predict the amount of air pollutants in meteorological stations. Air pollution depends on the location of the stations (weather conditions and activities in the surroundings). Frequently, the surrounding information is not considered in the learning process. This information is known beforehand in the absence of unobserved weather conditions and remains constant for the same station. Considering the surrounding information as side information facilitates the generalization for predicting pollutants in new stations, leading to a zero-shot regression scenario. Available methods in zero-shot typically lean towards classification, and are not easily extensible to regression. This paper proposes two zero-shot methods for regression. The first method is a similarity based approach that learns models from features and aggregates them using side information. However, potential knowledge of the feature models may be lost in the aggregation. The second metho
    
[^84]: 两个头胜过一个：通过语义和拓扑意识增强图稀疏训练

    Two Heads Are Better Than One: Boosting Graph Sparse Training via Semantic and Topological Awareness

    [https://rss.arxiv.org/abs/2402.01242](https://rss.arxiv.org/abs/2402.01242)

    该论文提出了一种新的研究方向和概念，即图稀疏训练（GST），通过动态稀疏训练使稀疏图与拓扑和语义锚点对齐，以降低计算开销，并在大规模图上提高图神经网络（GNN）的性能。

    

    图神经网络在各种图学习任务中表现出色，但在应用于大规模图时面临计算挑战。一种有希望的解决方案是通过去除非必要边缘，减少GNN中的计算开销。以前的文献通常分为两类：拓扑引导和语义引导。前者保持某些图拓扑属性，但由于与神经网络训练的低集成性而在GNN上表现不佳。后者在GNN上的较低稀疏度下表现良好，但在较高稀疏度下面临性能崩溃。基于这一点，我们首次提出了一种新的研究方向和概念，即图稀疏训练（GST），该方法在数据级别动态操作稀疏性。具体而言，GST首先以较低的训练成本构建拓扑和语义锚点，然后通过执行动态稀疏训练来使稀疏图与锚点对齐。我们引入了平衡稀疏化原则来...（摘要省略）

    Graph Neural Networks (GNNs) excel in various graph learning tasks but face computational challenges when applied to large-scale graphs. A promising solution is to remove non-essential edges to reduce the computational overheads in GNN. Previous literature generally falls into two categories: topology-guided and semantic-guided. The former maintains certain graph topological properties yet often underperforms on GNNs due to low integration with neural network training. The latter performs well at lower sparsity on GNNs but faces performance collapse at higher sparsity levels. With this in mind, we take the first step to propose a new research line and concept termed Graph Sparse Training (GST), which dynamically manipulates sparsity at the data level. Specifically, GST initially constructs a topology & semantic anchor at a low training cost, followed by performing dynamic sparse training to align the sparse graph with the anchor. We introduce the Equilibria Sparsification Principle to 
    
[^85]: 超越请求：利用HTTP响应头在不平衡环境中进行跨浏览器Web追踪器分类

    Beyond the Request: Harnessing HTTP Response Headers for Cross-Browser Web Tracker Classification in an Imbalanced Setting

    [https://rss.arxiv.org/abs/2402.01240](https://rss.arxiv.org/abs/2402.01240)

    本研究通过利用HTTP响应头设计了机器学习分类器，在跨浏览器环境下有效检测Web追踪器，结果在Chrome和Firefox上表现出较高的准确性和性能。

    

    万维网的连通性主要归因于HTTP协议，其中的HTTP消息提供了有关网络安全和隐私的信息头字段，特别是关于Web追踪。尽管已有研究利用HTTP/S请求消息来识别Web追踪器，但往往忽视了HTTP/S响应头。本研究旨在设计使用HTTP/S响应头进行Web追踪器检测的有效机器学习分类器。通过浏览器扩展程序T.EX获取的Chrome、Firefox和Brave浏览器的数据作为我们的数据集。在Chrome数据上训练了11个监督模型，并在所有浏览器上进行了测试。结果表明，在Chrome和Firefox上具有高准确性、F1分数、精确度、召回率和最小对数损失误差的性能，但在Brave浏览器上表现不佳，可能是由于其不同的数据分布和特征集。研究表明，这些分类器可以用于检测Web追踪器。

    The World Wide Web's connectivity is greatly attributed to the HTTP protocol, with HTTP messages offering informative header fields that appeal to disciplines like web security and privacy, especially concerning web tracking. Despite existing research employing HTTP/S request messages to identify web trackers, HTTP/S response headers are often overlooked. This study endeavors to design effective machine learning classifiers for web tracker detection using HTTP/S response headers. Data from the Chrome, Firefox, and Brave browsers, obtained through the traffic monitoring browser extension T.EX, serves as our data set. Eleven supervised models were trained on Chrome data and tested across all browsers. The results demonstrated high accuracy, F1-score, precision, recall, and minimal log-loss error for Chrome and Firefox, but subpar performance on Brave, potentially due to its distinct data distribution and feature set. The research suggests that these classifiers are viable for detecting w
    
[^86]: 灵活的变分信息瓶颈：通过一次训练实现多样化压缩

    Flexible Variational Information Bottleneck: Achieving Diverse Compression with a Single Training

    [https://rss.arxiv.org/abs/2402.01238](https://rss.arxiv.org/abs/2402.01238)

    本研究引入了灵活的变分信息瓶颈（FVIB）框架，通过单次训练即可获得所有β值的最优模型，从而实现多样化压缩，并且在理论和实证方面证明了它的有效性。

    

    信息瓶颈是一种广泛使用的框架，可以从源随机变量中提取与目标随机变量相关的信息。在目标函数中，通过拉格朗日乘子β，信息瓶颈控制数据压缩和预测性之间的权衡。传统上，为了找到要学习的权衡，信息瓶颈需要通过多个训练周期来搜索β，这在计算上是昂贵的。在本研究中，我们引入了灵活的变分信息瓶颈（FVIB），这是一种用于分类任务的创新框架，可以通过单次计算高效的训练来获得所有β值的最优模型。我们在理论上证明，FVIB可以在合理的β值范围内同时最大化变分信息瓶颈（VIB）的目标函数的近似。然后，我们通过实验证明了FVIB可以像VI一样有效地学习VIB的目标。

    Information Bottleneck (IB) is a widely used framework that enables the extraction of information related to a target random variable from a source random variable. In the objective function, IB controls the trade-off between data compression and predictiveness through the Lagrange multiplier $\beta$. Traditionally, to find the trade-off to be learned, IB requires a search for $\beta$ through multiple training cycles, which is computationally expensive. In this study, we introduce Flexible Variational Information Bottleneck (FVIB), an innovative framework for classification task that can obtain optimal models for all values of $\beta$ with single, computationally efficient training. We theoretically demonstrate that across all values of reasonable $\beta$, FVIB can simultaneously maximize an approximation of the objective function for Variational Information Bottleneck (VIB), the conventional IB method. Then we empirically show that FVIB can learn the VIB objective as effectively as VI
    
[^87]: 交通预测中的延迟效应揭示：基于时空延迟微分方程的视角

    Unveiling Delay Effects in Traffic Forecasting: A Perspective from Spatial-Temporal Delay Differential Equations

    [https://rss.arxiv.org/abs/2402.01231](https://rss.arxiv.org/abs/2402.01231)

    本研究提出了一个基于神经网络的空间-时间延迟微分方程模型，用于捕捉交通预测中的延迟效应。

    

    交通流量预测是交通规划和管理中的一个基本研究问题，也是空间-时间预测的一个典型例子。最近，图神经网络（GNNs）和循环神经网络（RNNs）在捕捉交通流量预测的空间-时间相关性方面取得了巨大成功。然而，目前仍有两个不可忽视的问题没有得到很好地解决：1）GNNs中的消息传递是即时的，而在现实中，邻居节点之间的空间消息交互可能存在延迟。交通流量在一个节点上的变化需要几分钟的时间延迟，才能影响其相连的邻居节点。2）交通状况不断变化。交通流量预测的预测频率可能根据具体场景要求而变化。目前大多数现有的离散模型需要针对每个预测时间段进行重新训练，限制了它们的适用性。为了解决以上问题，我们提出了一个基于神经网络的空间-时间延迟微分方程模型，来捕捉交通预测中的延迟效应。

    Traffic flow forecasting is a fundamental research issue for transportation planning and management, which serves as a canonical and typical example of spatial-temporal predictions. In recent years, Graph Neural Networks (GNNs) and Recurrent Neural Networks (RNNs) have achieved great success in capturing spatial-temporal correlations for traffic flow forecasting. Yet, two non-ignorable issues haven't been well solved: 1) The message passing in GNNs is immediate, while in reality the spatial message interactions among neighboring nodes can be delayed. The change of traffic flow at one node will take several minutes, i.e., time delay, to influence its connected neighbors. 2) Traffic conditions undergo continuous changes. The prediction frequency for traffic flow forecasting may vary based on specific scenario requirements. Most existing discretized models require retraining for each prediction horizon, restricting their applicability. To tackle the above issues, we propose a neural Spati
    
[^88]: 针对低分辨率红外阵列的隐私保护人数计数深度神经网络（DNN）的硬件-软件优化

    HW-SW Optimization of DNNs for Privacy-preserving People Counting on Low-resolution Infrared Arrays

    [https://rss.arxiv.org/abs/2402.01226](https://rss.arxiv.org/abs/2402.01226)

    本文针对低分辨率红外阵列的隐私保护人数计数问题，提出了一种高度自动化的DNN优化流程，包括神经架构搜索、混合精度量化和后处理等，并通过实现新的智能传感器原型，在能耗、内存和准确性这三个维度上得到了大量高效的解决方案。

    

    低分辨率红外阵列传感器可以实现监测空间占用和人员流动等人数计数应用，同时保护隐私并最大限度地减少能源消耗。深度神经网络（DNN）已被证明能够以准确和高效的方式处理这些传感器数据。然而，DNN结构的空间巨大，手动探索是繁琐的，并且往往导致次优解。为了克服这个问题，本文提出了一个高度自动化的DNN全栈优化流程，从神经架构搜索、混合精度量化和后处理，到实现包括自定义指令集的新型智能传感器原型。通过整合这些交叉层优化，我们在能耗、内存和准确性的三维空间获得了大量帕累托最优解。将这些解部署在我们的硬件平台上，我们改善了系统的性能。

    Low-resolution infrared (IR) array sensors enable people counting applications such as monitoring the occupancy of spaces and people flows while preserving privacy and minimizing energy consumption. Deep Neural Networks (DNNs) have been shown to be well-suited to process these sensor data in an accurate and efficient manner. Nevertheless, the space of DNNs' architectures is huge and its manual exploration is burdensome and often leads to sub-optimal solutions. To overcome this problem, in this work, we propose a highly automated full-stack optimization flow for DNNs that goes from neural architecture search, mixed-precision quantization, and post-processing, down to the realization of a new smart sensor prototype, including a Microcontroller with a customized instruction set. Integrating these cross-layer optimizations, we obtain a large set of Pareto-optimal solutions in the 3D-space of energy, memory, and accuracy. Deploying such solutions on our hardware platform, we improve the sta
    
[^89]: 基于深度学习的位置无关自适应降雨预测

    Location Agnostic Adaptive Rain Precipitation Prediction using Deep Learning

    [https://rss.arxiv.org/abs/2402.01208](https://rss.arxiv.org/abs/2402.01208)

    本研究提出了一种基于深度学习的自适应框架，能够解决降雨预测中的位置差异和气候变化带来的挑战。通过在巴黎、洛杉矶和东京进行适应，我们方法的预测能力分别提高了43.51%、5.09%和38.62%。

    

    降雨预测是一项具有挑战性的任务，因为它依赖于因地而异的气候和气象特征。因此，一个在一个位置表现良好的预测模型，在其他位置可能表现不佳，因为分布的变化。此外，由于全球变暖，天气模式年复一年地发生着快速变化，这可能导致即使在相同位置，时间过去后那些模型也变得无效。在我们的工作中，我们提出了一种自适应深度学习框架，以解决上述挑战。我们的方法能够推广模型，用于预测那些没有适应机制的方法无法预测降水的任何位置。我们的方法经过深度神经网络适应后，在巴黎、洛杉矶和东京的降水预测方面分别显示了43.51%、5.09%和38.62%的改进。

    Rain precipitation prediction is a challenging task as it depends on weather and meteorological features which vary from location to location. As a result, a prediction model that performs well at one location does not perform well at other locations due to the distribution shifts. In addition, due to global warming, the weather patterns are changing very rapidly year by year which creates the possibility of ineffectiveness of those models even at the same location as time passes. In our work, we have proposed an adaptive deep learning-based framework in order to provide a solution to the aforementioned challenges. Our method can generalize the model for the prediction of precipitation for any location where the methods without adaptation fail. Our method has shown 43.51%, 5.09%, and 38.62% improvement after adaptation using a deep neural network for predicting the precipitation of Paris, Los Angeles, and Tokyo, respectively.
    
[^90]: 使用大型语言模型的高效因果图发现

    Efficient Causal Graph Discovery Using Large Language Models

    [https://rss.arxiv.org/abs/2402.01207](https://rss.arxiv.org/abs/2402.01207)

    提出了一个新的框架，利用大型语言模型进行高效的因果图发现，采用了广度优先搜索方法，只需要线性数量的查询，同时能轻松结合观察数据以提高性能，具有高效性和数据效率，并在真实因果图上取得了最先进的结果，展示了其在不同领域的广泛适用性潜力。

    

    我们提出了一个新的框架，利用LLMs进行完整的因果图发现。之前基于LLM的方法采用了成对查询的方法，但这需要二次查询的数量，对于较大的因果图来说很快变得不可行。相反，提出的框架采用了广度优先搜索（BFS）的方法，只需要线性数量的查询。我们还展示了当有所观察数据可用时，提出的方法可以轻松地进行结合以提高性能。除了更具时间和数据效率外，提出的框架在不同大小的真实因果图上取得了最先进的结果。结果证明了提出方法在发现因果关系方面的有效性和效率，展示了其在不同领域的因果图发现任务中的广泛适用性潜力。

    We propose a novel framework that leverages LLMs for full causal graph discovery. While previous LLM-based methods have used a pairwise query approach, this requires a quadratic number of queries which quickly becomes impractical for larger causal graphs. In contrast, the proposed framework uses a breadth-first search (BFS) approach which allows it to use only a linear number of queries. We also show that the proposed method can easily incorporate observational data when available, to improve performance. In addition to being more time and data-efficient, the proposed framework achieves state-of-the-art results on real-world causal graphs of varying sizes. The results demonstrate the effectiveness and efficiency of the proposed method in discovering causal relationships, showcasing its potential for broad applicability in causal graph discovery tasks across different domains.
    
[^91]: 使用机器学习模型进行天气预测的比较评估

    Comparative Evaluation of Weather Forecasting using Machine Learning Models

    [https://rss.arxiv.org/abs/2402.01206](https://rss.arxiv.org/abs/2402.01206)

    本研究使用机器学习算法在天气预测中取得了显著进展，并分析了不同算法在降水和温度预测方面的贡献。

    

    天气的深入理解和对其未来行为的预测一直被认为是推动社会发展的重要努力。本研究通过应用机器学习算法，探索了在天气预测的背景下理解和预测自然行为的进展。通过利用机器学习、数据挖掘和数据分析技术的力量，在这个领域已经取得了显著的进展。本研究集中分析了在使用来自达卡市一个气象站20年数据集的同时，各种机器学习算法在预测降水和温度模式方面的贡献。通过评估梯度提升、AdaBoost、人工神经网络、叠加随机森林、叠加神经网络和叠加KNN等算法，基于性能指标（包括混淆矩阵测量）进行比较。研究结果强调了这些算法在天气预测中的显著优势。

    Gaining a deeper understanding of weather and being able to predict its future conduct have always been considered important endeavors for the growth of our society. This research paper explores the advancements in understanding and predicting nature's behavior, particularly in the context of weather forecasting, through the application of machine learning algorithms. By leveraging the power of machine learning, data mining, and data analysis techniques, significant progress has been made in this field. This study focuses on analyzing the contributions of various machine learning algorithms in predicting precipitation and temperature patterns using a 20-year dataset from a single weather station in Dhaka city. Algorithms such as Gradient Boosting, AdaBoosting, Artificial Neural Network, Stacking Random Forest, Stacking Neural Network, and Stacking KNN are evaluated and compared based on their performance metrics, including Confusion matrix measurements. The findings highlight remarkabl
    
[^92]: 自监督学习在非连续表格数据中的应用调研

    A Survey on Self-Supervised Learning for Non-Sequential Tabular Data

    [https://rss.arxiv.org/abs/2402.01204](https://rss.arxiv.org/abs/2402.01204)

    本调研总结了自监督学习在非连续表格数据中的最新进展和挑战，将其方法分为预测性学习、对比学习和混合学习，讨论了应用问题，包括自动化数据工程、跨表格查询和隐私保护等。

    

    自监督学习（SSL）已经被应用于各个领域的许多最先进的模型中，其中SSL通过定义基于无标签数据集的预训练任务来学习上下文化和鲁棒的表示。最近，SSL已成为探索表格数据领域中表示学习能力的新趋势，这是一项更具挑战性的任务，因为它没有明确的关系来学习描述性的表示。本调研旨在系统地回顾和总结自监督学习在非连续表格数据（SSL4NS-TD）中的最新进展和挑战。首先，我们给出了NS-TD的正式定义，并阐明了它与相关研究的关联。然后，这些方法被分为三组——预测性学习、对比学习和混合学习，并介绍了每个方向的代表性方法的动机和优点。在此基础上，还介绍了SSL4NS-TD的应用问题，包括自动化数据工程、跨表格查询和隐私保护等。

    Self-supervised learning (SSL) has been incorporated into many state-of-the-art models in various domains, where SSL defines pretext tasks based on unlabeled datasets to learn contextualized and robust representations. Recently, SSL has been a new trend in exploring the representation learning capability in the realm of tabular data, which is more challenging due to not having explicit relations for learning descriptive representations. This survey aims to systematically review and summarize the recent progress and challenges of SSL for non-sequential tabular data (SSL4NS-TD). We first present a formal definition of NS-TD and clarify its correlation to related studies. Then, these approaches are categorized into three groups -- predictive learning, contrastive learning, and hybrid learning, with their motivations and strengths of representative methods within each direction. On top of this, application issues of SSL4NS-TD are presented, including automatic data engineering, cross-table
    
[^93]: 结构化世界建模通过语义向量量化

    Structured World Modeling via Semantic Vector Quantization

    [https://rss.arxiv.org/abs/2402.01203](https://rss.arxiv.org/abs/2402.01203)

    这篇论文提出了一种新方法，通过语义向量量化实现语义神经离散表示学习，解决了现有方法只能提供补丁级别表示的问题，并通过逐层构建场景表示和训练的方式实现了结构化语义世界建模。

    

    神经离散表示是现代神经网络的关键组成部分。然而，其主要局限性是主要策略（如VQ-VAE）只能提供补丁级别的表示。因此，表示学习的主要目标之一，即获取结构化、语义化和组合抽象（如对象的颜色和形状），仍然难以实现。在本文中，我们提出了第一种用于语义神经离散表示学习的方法。所提出的模型，称为Semantic Vector-Quantized Variational Autoencoder (SVQ)，利用了最近在无监督物体中心学习方面的进展来解决这个局限性。具体而言，我们观察到，在对象级别上简单进行量化是一个重大挑战，并提出了从低级离散概念模式到对象表示逐层构建场景表示的方法。此外，我们提出了一种用于结构化语义世界建模的新方法，通过训练来实现该方法。

    Neural discrete representations are crucial components of modern neural networks. However, their main limitation is that the primary strategies such as VQ-VAE can only provide representations at the patch level. Therefore, one of the main goals of representation learning, acquiring structured, semantic, and compositional abstractions such as the color and shape of an object, remains elusive. In this paper, we present the first approach to semantic neural discrete representation learning. The proposed model, called Semantic Vector-Quantized Variational Autoencoder (SVQ), leverages recent advances in unsupervised object-centric learning to address this limitation. Specifically, we observe that a simple approach quantizing at the object level poses a significant challenge and propose constructing scene representations hierarchically, from low-level discrete concept schemas to object representations. Additionally, we suggest a novel method for structured semantic world modeling by training
    
[^94]: 具有先验知识的少样本类增量学习

    Few-Shot Class-Incremental Learning with Prior Knowledge

    [https://rss.arxiv.org/abs/2402.01201](https://rss.arxiv.org/abs/2402.01201)

    该论文提出了一种具有先验知识的学习方法，通过引入从后续增量类别的无标签数据中产生的伪标签，与带标签的基类样本一起进行联合训练，有效地为旧类和新类数据分配嵌入空间，从而提高了模型对灾难性遗忘的韧性。

    

    为解决少样本类增量学习(FSCIL)中的灾难性遗忘和过拟合问题，之前的研究主要集中在在增量阶段保留旧知识的记忆上。这些研究经常低估了预训练模型在塑造增量学习有效性方面的作用。因此，为了增强预训练模型的泛化能力，我们提出了具有先验知识的学习(LwPK)，通过引入来自后续增量类别中少量无标签数据的几乎自由的先验知识。我们将无标签的增量类样本聚类，生成伪标签，并与带标签的基类样本进行联合训练，有效地为旧类和新类数据分配嵌入空间。实验结果表明，LwPK有效提高了模型对灾难性遗忘的韧性，基于经验风险最小化和类间距离度量的理论分析得到了验证。

    To tackle the issues of catastrophic forgetting and overfitting in few-shot class-incremental learning (FSCIL), previous work has primarily concentrated on preserving the memory of old knowledge during the incremental phase. The role of pre-trained model in shaping the effectiveness of incremental learning is frequently underestimated in these studies. Therefore, to enhance the generalization ability of the pre-trained model, we propose Learning with Prior Knowledge (LwPK) by introducing nearly free prior knowledge from a few unlabeled data of subsequent incremental classes. We cluster unlabeled incremental class samples to produce pseudo-labels, then jointly train these with labeled base class samples, effectively allocating embedding space for both old and new class data. Experimental results indicate that LwPK effectively enhances the model resilience against catastrophic forgetting, with theoretical analysis based on empirical risk minimization and class distance measurement corrob
    
[^95]: 条件化正则化流用于粗粒化分子表示的主动学习

    Conditional Normalizing Flows for Active Learning of Coarse-Grained Molecular Representations

    [https://rss.arxiv.org/abs/2402.01195](https://rss.arxiv.org/abs/2402.01195)

    本文提出了使用条件化正则化流和主动学习的方法来解决粗粒化分子表示中的玻尔兹曼分布采样问题，相比传统的分子动力学模拟，该方法能够获得更高效的加速效果。

    

    高效采样分子系统的玻尔兹曼分布是一个长期存在的挑战。最近，与生成长时间分子动力学模拟不同，生成机器学习方法如正则化流被用于直接学习玻尔兹曼分布，而不需要样本。然而，这种方法容易出现模式崩溃，因此常常无法探索全部的构型空间。在这项工作中，我们将问题分为两个层次，细粒度和粗粒度自由度。在粗粒化空间上条件化正则化流可以产生两个层次之间的概率连接。为了探索构型空间，我们采用了粗粒化模拟与主动学习的方法，可以在必要时更新流并进行全原子势能评估。以丙氨酸二肽为例，我们展示了我们的方法相对于分子动力学模拟的加速效果。

    Efficient sampling of the Boltzmann distribution of molecular systems is a long-standing challenge. Recently, instead of generating long molecular dynamics simulations, generative machine learning methods such as normalizing flows have been used to learn the Boltzmann distribution directly, without samples. However, this approach is susceptible to mode collapse and thus often does not explore the full configurational space. In this work, we address this challenge by separating the problem into two levels, the fine-grained and coarse-grained degrees of freedom. A normalizing flow conditioned on the coarse-grained space yields a probabilistic connection between the two levels. To explore the configurational space, we employ coarse-grained simulations with active learning which allows us to update the flow and make all-atom potential energy evaluations only when necessary. Using alanine dipeptide as an example, we show that our methods obtain a speedup to molecular dynamics simulations of
    
[^96]: 通过嵌入相似性实现高效的提示缓存

    Efficient Prompt Caching via Embedding Similarity

    [https://rss.arxiv.org/abs/2402.01173](https://rss.arxiv.org/abs/2402.01173)

    本论文通过嵌入相似性的提示缓存方法来提高大规模语言模型(LMMs)的推理效率，并提出一种蒸馏方法来优化现有嵌入以获得更好的缓存预测准确性。

    

    大规模语言模型(LLMs)在许多自然语言处理(NLP)任务中取得了巨大成功。然而，在推理过程中，它面临着资源消耗的挑战。本文旨在通过提示缓存来提高LLMs的推理效率，即，如果当前提示可以由前一个提示的同样回答而得到回答，就可以直接利用该前一个回答而不调用LLMs。具体而言，我们关注通过嵌入相似性来提升单轮问答任务的缓存预测准确性。目前已有的提示嵌入主要关注两个提示是否语义相似，这与同样的回答是否可以回答它们并不等价。因此，我们提出了一种基于蒸馏的方法来优化现有的嵌入以获得更好的缓存预测。理论上，我们在不同类型的损失函数下提供了我们方法收敛的有限样本保证。实验结果表明，我们的方法在推理效率方面取得了显著的改进。

    Large language models (LLMs) have achieved huge success in numerous natural language process (NLP) tasks. However, it faces the challenge of significant resource consumption during inference. In this paper, we aim to improve the inference efficiency of LLMs by prompt caching, i.e., if the current prompt can be answered by the same response of a previous prompt, one can directly utilize that previous response without calling the LLM. Specifically, we focus on the prediction accuracy of prompt caching for single-round question-answering tasks via embedding similarity. The existing embeddings of prompts mostly focus on whether two prompts are semantically similar, which is not necessarily equivalent to whether the same response can answer them. Therefore, we propose a distillation-based method to fine-tune the existing embeddings for better caching prediction. Theoretically, we provide finite-sample guarantees for the convergence of our method under different types of loss functions. Empi
    
[^97]: 分布式SGD的截断非均匀量化

    Truncated Non-Uniform Quantization for Distributed SGD

    [https://rss.arxiv.org/abs/2402.01160](https://rss.arxiv.org/abs/2402.01160)

    我们提出了一种截断非均匀量化的分布式SGD方法，用于提高通信效率。我们的方法通过截断来减轻长尾噪声的影响，并根据梯度的统计特性进行非均匀量化。理论分析和实验评估表明，该方法在通信效率和收敛性方面取得了优越的平衡。

    

    为了解决分布式学习中的通信瓶颈挑战，我们的工作引入了一种新颖的两阶段量化策略，旨在提高分布式随机梯度下降（SGD）的通信效率。所提出的方法首先使用截断来减轻长尾噪声的影响，然后根据梯度的统计特性进行非均匀量化。我们对量化的分布式SGD进行了全面的收敛性分析，为其性能提供了理论保证。此外，通过最小化收敛误差，我们推导出在给定通信约束下的截断阈值和非均匀量化水平的最优闭式解。理论洞见和广泛的实验评估表明，我们提出的算法优于现有的量化方案，在通信效率和收敛性之间取得了优越的平衡。

    To address the communication bottleneck challenge in distributed learning, our work introduces a novel two-stage quantization strategy designed to enhance the communication efficiency of distributed Stochastic Gradient Descent (SGD). The proposed method initially employs truncation to mitigate the impact of long-tail noise, followed by a non-uniform quantization of the post-truncation gradients based on their statistical characteristics. We provide a comprehensive convergence analysis of the quantized distributed SGD, establishing theoretical guarantees for its performance. Furthermore, by minimizing the convergence error, we derive optimal closed-form solutions for the truncation threshold and non-uniform quantization levels under given communication constraints. Both theoretical insights and extensive experimental evaluations demonstrate that our proposed algorithm outperforms existing quantization schemes, striking a superior balance between communication efficiency and convergence 
    
[^98]: Sobolev空间中核分类器的最优性

    The Optimality of Kernel Classifiers in Sobolev Space

    [https://rss.arxiv.org/abs/2402.01148](https://rss.arxiv.org/abs/2402.01148)

    本文研究了核分类器在Sobolev空间中的最优性质，并通过对条件概率的假设和核回归理论的应用，导出了核分类器的分类超额风险上界和Sobolev空间的极小极大下界。此外，我们还提出了一种简单方法来估计插值平滑度，并将其应用于实际数据集。

    

    核方法在机器学习中广泛应用，特别是用于分类问题。然而，核分类的理论分析仍然有限。本文研究了核分类器的统计性能。在对条件概率$\eta(x)=\mathbb{P}(Y=1\mid X=x)$作出一些温和的假设后，我们利用核回归理论的最新进展，导出了核分类器分类超额风险的上界。我们还得到了Sobolev空间的极小极大下界，从而证明了所提出分类器的最优性。我们的理论结果可以扩展到超参数化神经网络分类器的泛化误差。为了使我们的理论结果在实际环境中更加适用，我们还提出了一种估计$2\eta(x)-1$插值平滑度的简单方法，并将该方法应用于实际数据集。

    Kernel methods are widely used in machine learning, especially for classification problems. However, the theoretical analysis of kernel classification is still limited. This paper investigates the statistical performances of kernel classifiers. With some mild assumptions on the conditional probability $\eta(x)=\mathbb{P}(Y=1\mid X=x)$, we derive an upper bound on the classification excess risk of a kernel classifier using recent advances in the theory of kernel regression. We also obtain a minimax lower bound for Sobolev spaces, which shows the optimality of the proposed classifier. Our theoretical results can be extended to the generalization error of overparameterized neural network classifiers. To make our theoretical results more applicable in realistic settings, we also propose a simple method to estimate the interpolation smoothness of $2\eta(x)-1$ and apply the method to real datasets.
    
[^99]: 异构排队系统中用于路由作业的高效强化学习

    Efficient Reinforcement Learning for Routing Jobs in Heterogeneous Queueing Systems

    [https://rss.arxiv.org/abs/2402.01147](https://rss.arxiv.org/abs/2402.01147)

    本研究针对异构排队系统中作业路由问题，提出了ACHQ算法，通过低维度的软阈值策略参数化和基于策略梯度的方法，利用系统的排队结构，实现了高效的强化学习求解策略。实验结果表明，ACHQ算法能够收敛到近似全局最优解。

    

    本研究考虑将到达中央队列的作业高效路由到异构服务器系统中的问题。与同质系统不同，对于一快一慢的双服务器系统，已知阈值策略，即在队列长度超过某一阈值时将作业路由到慢服务器，是最优策略。但多服务器系统的最优策略未知且难以找到。虽然强化学习（RL）已被认为对于学习此类策略具有巨大潜力，但我们的问题具有指数级的状态空间大小，使标准RL效率低下。在这项工作中，我们提出了ACHQ，一种基于策略梯度的高效算法，具有低维度的软阈值策略参数化，利用底层排队结构。我们为一般情况提供了稳态收敛性保证，并且尽管参数化维度较低，但证明了ACHQ收敛到近似全局最优序列。

    We consider the problem of efficiently routing jobs that arrive into a central queue to a system of heterogeneous servers. Unlike homogeneous systems, a threshold policy, that routes jobs to the slow server(s) when the queue length exceeds a certain threshold, is known to be optimal for the one-fast-one-slow two-server system. But an optimal policy for the multi-server system is unknown and non-trivial to find. While Reinforcement Learning (RL) has been recognized to have great potential for learning policies in such cases, our problem has an exponentially large state space size, rendering standard RL inefficient. In this work, we propose ACHQ, an efficient policy gradient based algorithm with a low dimensional soft threshold policy parameterization that leverages the underlying queueing structure. We provide stationary-point convergence guarantees for the general case and despite the low-dimensional parameterization prove that ACHQ converges to an approximate global optimum for the sp
    
[^100]: 基于动态平均的核化配对学习的有限内存在线梯度下降算法

    Limited Memory Online Gradient Descent for Kernelized Pairwise Learning with Dynamic Averaging

    [https://rss.arxiv.org/abs/2402.01146](https://rss.arxiv.org/abs/2402.01146)

    本文提出了一种基于动态平均的核化配对学习的有限内存在线梯度下降算法，解决了配对学习中计算复杂度增长问题，并且不需要示例的独立性。

    

    配对学习是机器学习中的重要领域，处理的是基于训练示例对定义的损失函数，包括度量学习和AUC最大化等。为了提高可扩展性，研究人员采用在线梯度下降（OGD）方法来解决随着样本大小增长而带来的计算复杂度的二次增长问题。最近，提出了一种OGD算法，通过在先前和最近的示例上进行梯度计算，将算法复杂度有效降低到$O(T)$，其中$T$是接收到的示例数。然而，这种方法仅限于线性模型，同时假设示例的到达是独立的。我们提出了一种轻量级的OGD算法，不需要示例的独立性，并且适用于核化配对学习。我们的算法基于一个随机示例和代表过去数据的移动平均数构建梯度，从而实现了亚线性的运行时间复杂度。

    Pairwise learning, an important domain within machine learning, addresses loss functions defined on pairs of training examples, including those in metric learning and AUC maximization. Acknowledging the quadratic growth in computation complexity accompanying pairwise loss as the sample size grows, researchers have turned to online gradient descent (OGD) methods for enhanced scalability. Recently, an OGD algorithm emerged, employing gradient computation involving prior and most recent examples, a step that effectively reduces algorithmic complexity to $O(T)$, with $T$ being the number of received examples. This approach, however, confines itself to linear models while assuming the independence of example arrivals. We introduce a lightweight OGD algorithm that does not require the independence of examples and generalizes to kernel pairwise learning. Our algorithm builds the gradient based on a random example and a moving average representing the past data, which results in a sub-linear r
    
[^101]: 用解缠离散图自编码器学习网络表示

    Learning Network Representations with Disentangled Graph Auto-Encoder

    [https://rss.arxiv.org/abs/2402.01143](https://rss.arxiv.org/abs/2402.01143)

    本文介绍了解缠离散图自编码器(DGA)和解缠变分图自编码器(DVGA)的方法，利用生成模型来学习解缠表示。

    

    (变分)图自编码器广泛用于学习图结构化数据的表示。然而，现实世界图的形成是一个由潜在因素影响的复杂和异质的过程。现有的编码器基本上是整体的，忽视了潜在因素的纠缠。这不仅使得图分析任务不太有效，而且使得理解和解释这些表示变得更加困难。用(变分)图自编码器学习解缠的图表示面临着重要挑战，在现有文献中尚未得到充分探索。在本文中，我们介绍了解缠离散图自编码器(DGA)和解缠变分图自编码器(DVGA)的方法，利用生成模型来学习解缠表示。具体地，我们首先设计了一个解缠的图卷积网络，使用多通道消息传递层作为编码器，聚合与每个节点相关的信息。

    The (variational) graph auto-encoder is extensively employed for learning representations of graph-structured data. However, the formation of real-world graphs is a complex and heterogeneous process influenced by latent factors. Existing encoders are fundamentally holistic, neglecting the entanglement of latent factors. This not only makes graph analysis tasks less effective but also makes it harder to understand and explain the representations. Learning disentangled graph representations with (variational) graph auto-encoder poses significant challenges, and remains largely unexplored in the existing literature. In this article, we introduce the Disentangled Graph Auto-Encoder (DGA) and Disentangled Variational Graph Auto-Encoder (DVGA), approaches that leverage generative models to learn disentangled representations. Specifically, we first design a disentangled graph convolutional network with multi-channel message-passing layers, as the encoder aggregating information related to eac
    
[^102]: 基于神经Granger因果发现的微服务根本原因分析

    Root Cause Analysis In Microservice Using Neural Granger Causal Discovery

    [https://rss.arxiv.org/abs/2402.01140](https://rss.arxiv.org/abs/2402.01140)

    提出了一种基于神经Granger因果发现的方法来解决微服务中根本原因分析的挑战。

    

    近年来，由于其可扩展性、可维护性和灵活性，微服务在IT运营中得到了广泛应用。然而，当面临系统故障时，站点可靠性工程师很难找到根本原因，因为微服务中存在复杂的关系。先前的研究采用结构化学习方法（例如PC算法）来建立因果关系，并从因果图中得出根本原因。然而，他们忽略了时间序列数据的时间顺序，并未利用时间关系中蕴含的丰富信息。例如，在CPU利用率突然增加的情况下，可能会导致其他微服务的延迟增加。然而，在这种情况下，CPU利用率异常发生在延迟增加之前，而不是同时发生。结果，PC算法无法捕捉这样的特征。为了解决这些挑战，我们提出了RUN，一种新的方法。

    In recent years, microservices have gained widespread adoption in IT operations due to their scalability, maintenance, and flexibility. However, it becomes challenging for site reliability engineers (SREs) to pinpoint the root cause due to the complex relationships in microservices when facing system malfunctions. Previous research employed structured learning methods (e.g., PC-algorithm) to establish causal relationships and derive root causes from causal graphs. Nevertheless, they ignored the temporal order of time series data and failed to leverage the rich information inherent in the temporal relationships. For instance, in cases where there is a sudden spike in CPU utilization, it can lead to an increase in latency for other microservices. However, in this scenario, the anomaly in CPU utilization occurs before the latency increase, rather than simultaneously. As a result, the PC-algorithm fails to capture such characteristics. To address these challenges, we propose RUN, a novel a
    
[^103]: 在线自适应预测方法中带有递减步长

    Online conformal prediction with decaying step sizes

    [https://rss.arxiv.org/abs/2402.01139](https://rss.arxiv.org/abs/2402.01139)

    本文介绍了一种在线自适应预测方法，通过使用递减步长来改进在任意序列上的覆盖率保证，并且能够同时估计总体分位数。

    

    本文介绍一种带有递减步长的在线自适应预测方法。和之前的方法一样，我们的方法也能在任意序列上回溯性地保证覆盖率。然而，与之前的方法不同的是，我们能够在存在的情况下同时估计出总体分位数。我们的理论和实验证明了显著改进的实际特性：特别是在分布稳定的情况下，覆盖率接近所期望的水平，不仅仅在观测序列的平均值上。

    We introduce a method for online conformal prediction with decaying step sizes. Like previous methods, ours possesses a retrospective guarantee of coverage for arbitrary sequences. However, unlike previous methods, we can simultaneously estimate a population quantile when it exists. Our theory and experiments indicate substantially improved practical properties: in particular, when the distribution is stable, the coverage is close to the desired level for every time point, not just on average over the observed sequence.
    
[^104]: 基于脑电图的情绪识别中的图神经网络：一个综述

    Graph Neural Networks in EEG-based Emotion Recognition: A Survey

    [https://rss.arxiv.org/abs/2402.01138](https://rss.arxiv.org/abs/2402.01138)

    基于脑电图的情绪识别中的图神经网络是一个有重要意义的领域。本综述分类和分析了已有方法，并提供了构建基于脑电图的GNNs的明确指导。

    

    相对于其他模式，基于脑电图的情绪识别可以直观地响应人脑中的情绪模式，因此成为脑-计算机接口领域最关注的任务之一。由于大脑区域之间的依赖与情绪密切相关，因此发展基于图神经网络（GNNs）进行基于脑电图的情绪识别成为一个重要趋势。然而，情绪性脑电图中的大脑区域依赖具有生理基础，使得在这一领域中的GNNs与其他时间序列领域的GNNs有所区别。此外，在基于脑电图的情绪识别中既没有全面的综述，也没有构建GNNs的指导。在这项综述中，我们对已有方法在图构造的统一框架下进行了分类，揭示出其共同点和差异。我们从框架的三个阶段分析和分类方法，为构建基于脑电图的GNNs提供了清晰的指导。此外，我们还讨论了一些...

    Compared to other modalities, EEG-based emotion recognition can intuitively respond to the emotional patterns in the human brain and, therefore, has become one of the most concerning tasks in the brain-computer interfaces field. Since dependencies within brain regions are closely related to emotion, a significant trend is to develop Graph Neural Networks (GNNs) for EEG-based emotion recognition. However, brain region dependencies in emotional EEG have physiological bases that distinguish GNNs in this field from those in other time series fields. Besides, there is neither a comprehensive review nor guidance for constructing GNNs in EEG-based emotion recognition. In the survey, our categorization reveals the commonalities and differences of existing approaches under a unified framework of graph construction. We analyze and categorize methods from three stages in the framework to provide clear guidance on constructing GNNs in EEG-based emotion recognition. In addition, we discuss several 
    
[^105]: 可扩展多模型MPC的基于对偶交互预测的层级架构

    Scalable Multi-modal Model Predictive Control via Duality-based Interaction Predictions

    [https://rss.arxiv.org/abs/2402.01116](https://rss.arxiv.org/abs/2402.01116)

    我们提出了一个层级架构，通过使用对偶交互预测和精简的MPC问题，实现了可扩展的实时模型预测控制，在复杂的多模态交通场景中展示了12倍的速度提升。

    

    我们提出了一个层级架构，用于在复杂的多模态交通场景中实现可扩展的实时模型预测控制(MPC)。该架构由两个关键组件组成：1) RAID-Net，一种基于注意力机制的新颖循环神经网络，使用拉格朗日对偶性预测自动驾驶车辆与周围车辆之间在MPC预测范围内的相关交互；2) 一个简化的随机MPC问题，消除不相关的避碰约束，提高计算效率。我们的方法在一个模拟交通路口中演示，展示了解决运动规划问题的12倍速提升。您可以在这里找到展示该架构在多个复杂交通场景中的视频：https://youtu.be/-TcMeolCLWc

    We propose a hierarchical architecture designed for scalable real-time Model Predictive Control (MPC) in complex, multi-modal traffic scenarios. This architecture comprises two key components: 1) RAID-Net, a novel attention-based Recurrent Neural Network that predicts relevant interactions along the MPC prediction horizon between the autonomous vehicle and the surrounding vehicles using Lagrangian duality, and 2) a reduced Stochastic MPC problem that eliminates irrelevant collision avoidance constraints, enhancing computational efficiency. Our approach is demonstrated in a simulated traffic intersection with interactive surrounding vehicles, showcasing a 12x speed-up in solving the motion planning problem. A video demonstrating the proposed architecture in multiple complex traffic scenarios can be found here: https://youtu.be/-TcMeolCLWc
    
[^106]: 双重防御：使用迁移学习和随机化来防止仅基于标签的成员推断攻击

    Double-Dip: Thwarting Label-Only Membership Inference Attacks with Transfer Learning and Randomization

    [https://rss.arxiv.org/abs/2402.01114](https://rss.arxiv.org/abs/2402.01114)

    这项研究探索了使用迁移学习和随机化来防止成员推断攻击。通过在过拟合的深度神经网络上应用双重防御，我们可以提高模型的隐私性而不降低准确度。

    

    迁移学习已经被证明在面对训练样本稀缺的情况下，可以提高深度神经网络模型的性能。然而，将迁移学习作为解决过拟合深度神经网络容易受到隐私攻击的方法的合适性尚未被探索。一类隐私攻击称为成员推断攻击旨在确定给定样本是否属于训练数据集（成员）或不属于（非成员）。我们引入了双重防御，一个系统性的实证研究，研究在不降低分类准确度的情况下使用迁移学习（阶段1）结合随机化（阶段2）来防止成员推断攻击对过拟合的深度神经网络的影响。我们的研究考察了源模型和目标模型之间的共享特征空间和参数值、冻结层的数量以及预训练模型的复杂性。我们在三个（目标，源）数据集对上评估了双重防御：（i）（CIFAR-10，ImageNet），（ii）（GTSRB，ImageNet），（iii）（CelebA，VGGFace2）。我们考虑了四个公开可用的预训练深度神经网络：（a）VGG-19，

    Transfer learning (TL) has been demonstrated to improve DNN model performance when faced with a scarcity of training samples. However, the suitability of TL as a solution to reduce vulnerability of overfitted DNNs to privacy attacks is unexplored. A class of privacy attacks called membership inference attacks (MIAs) aim to determine whether a given sample belongs to the training dataset (member) or not (nonmember). We introduce Double-Dip, a systematic empirical study investigating the use of TL (Stage-1) combined with randomization (Stage-2) to thwart MIAs on overfitted DNNs without degrading classification accuracy. Our study examines the roles of shared feature space and parameter values between source and target models, number of frozen layers, and complexity of pretrained models. We evaluate Double-Dip on three (Target, Source) dataset paris: (i) (CIFAR-10, ImageNet), (ii) (GTSRB, ImageNet), (iii) (CelebA, VGGFace2). We consider four publicly available pretrained DNNs: (a) VGG-19,
    
[^107]: 在自适应约束下的自对弈强化学习中的近乎最优解

    Near-Optimal Reinforcement Learning with Self-Play under Adaptivity Constraints

    [https://rss.arxiv.org/abs/2402.01111](https://rss.arxiv.org/abs/2402.01111)

    本文研究了具有自适应约束的多智能体强化学习问题，并提出了一种基于消除算法，将后悔控制在$\widetilde{O}(\sqrt{H^3 S^2 ABK})$，批量复杂度为$O(H+\log\log K)$。此外，还给出了所有具有$\widetilde{O}(\sqrt{K})$后悔界算法的批量复杂度下界。

    

    我们研究了具有自适应约束的多智能体强化学习问题（MARL） - 这是一种由实际应用驱动的新问题，其中部署新策略是昂贵的，并且必须最小化策略更新的次数。对于两个玩家的零和马尔可夫博弈，我们设计了一种（策略）基于消除的算法，它在后悔为$\widetilde{O}(\sqrt{H^3 S^2 ABK})$的情况下，批量复杂度仅为$O(H+\log\log K)$。在上述情况下，$S$表示状态数，$A，B$分别代表两个玩家的行动数，$H$是时间周期，$K$是游戏次数。此外，我们证明了对于所有具有$\widetilde{O}(\sqrt{K})$后悔界的算法，一种批量复杂度的下界为$\Omega(\frac{H}{\log_{A}K}+\log\log K)$，这与我们的上界在对数因子上匹配。作为副产品，我们的技术自然地扩展到学习赌博博弈和无奖励的近乎最优批量复杂度MARL。据我们所知，这些是迄今为止最好的成果。

    We study the problem of multi-agent reinforcement learning (MARL) with adaptivity constraints -- a new problem motivated by real-world applications where deployments of new policies are costly and the number of policy updates must be minimized. For two-player zero-sum Markov Games, we design a (policy) elimination based algorithm that achieves a regret of $\widetilde{O}(\sqrt{H^3 S^2 ABK})$, while the batch complexity is only $O(H+\log\log K)$. In the above, $S$ denotes the number of states, $A,B$ are the number of actions for the two players respectively, $H$ is the horizon and $K$ is the number of episodes. Furthermore, we prove a batch complexity lower bound $\Omega(\frac{H}{\log_{A}K}+\log\log K)$ for all algorithms with $\widetilde{O}(\sqrt{K})$ regret bound, which matches our upper bound up to logarithmic factors. As a byproduct, our techniques naturally extend to learning bandit games and reward-free MARL within near optimal batch complexity. To the best of our knowledge, these 
    
[^108]: 疫苗：针对大规模语言模型的干扰感知对齐技术

    Vaccine: Perturbation-aware Alignment for Large Language Model

    [https://rss.arxiv.org/abs/2402.01109](https://rss.arxiv.org/abs/2402.01109)

    疫苗是一种针对大规模语言模型的干扰感知对齐技术，通过逐渐添加扰动产生不变的隐藏嵌入，提高对抗有害提示引起的嵌入漂移的对齐鲁棒性，同时保留对良性提示的推理能力。

    

    作为一种新的微调即服务范 paradigm，大型语言模型 (LLM) 为用户上传的一小部分有害数据提供了新的攻击面，这些数据很容易欺骗微调过程从而产生对齐失效的模型。我们进行了实证分析，揭示了一种可能导致对齐失效的有害嵌入漂移现象。受到我们的发现启发，我们提出了疫苗 (Vaccine) ，一种针对干扰感知的对齐技术，以减轻用户微调的安全风险。疫苗的核心思想是通过在对齐阶段逐渐添加精心设计的扰动，产生不变的隐藏嵌入，从而使嵌入能够抵御来自未经消毒的用户数据的有害扰动。我们在开源主流LLM（如Llama2，Opt，Vicuna）上的实验结果表明，疫苗能够提高对抗有害提示引起的嵌入漂移的对齐鲁棒性，同时保留对良性提示的推理能力。

    The new paradigm of finetuning-as-a-service introduces a new attack surface for Large Language Models (LLMs): a few harmful data uploaded by users can easily trick the finetuning to produce an alignment-broken model. We conduct an empirical analysis and uncover a \textit{harmful embedding drift} phenomenon, showing a probable cause of the alignment-broken effect. Inspired by our findings, we propose Vaccine, a perturbation-aware alignment technique to mitigate the security risk of users finetuning. The core idea of Vaccine is to produce invariant hidden embeddings by progressively adding crafted perturbation to them in the alignment phase. This enables the embeddings to withstand harmful perturbation from un-sanitized user data in the finetuning phase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna) demonstrate that Vaccine can boost the robustness of alignment against harmful prompts induced embedding drift while reserving reasoning ability towards benign prompt
    
[^109]: 多智能体系统中的推理能力：局限性、挑战和以人为中心的解决方案

    Reasoning Capacity in Multi-Agent Systems: Limitations, Challenges and Human-Centered Solutions

    [https://rss.arxiv.org/abs/2402.01108](https://rss.arxiv.org/abs/2402.01108)

    多智能体系统面临着利用大型语言模型的限制和挑战，需要引入推理能力作为统一的标准来实现系统的整合和优化。

    

    大型语言模型（LLMs）在各种任务中展现出卓越的性能，为在生产环境中利用它们带来了许多机遇和挑战。为了实现LLMs的实际采用，多智能体系统在企业平台中具有增强、整合和协调LLMs的巨大潜力，该平台利用现有专有数据和模型来解决复杂的现实任务。尽管这些系统取得了巨大的成功，但当前的方法依赖于狭窄、单一目标的优化和评估，往往忽视现实情景中的潜在约束，包括有限的预算、资源和时间。此外，解释、分析和调试这些系统要求不同的组件之间进行相互评估，但现有方法无法满足这种需求。在本论文中，我们引入了推理能力的概念作为一个统一的标准，以实现集成和优化多智能体系统。

    Remarkable performance of large language models (LLMs) in a variety of tasks brings forth many opportunities as well as challenges of utilizing them in production settings. Towards practical adoption of LLMs, multi-agent systems hold great promise to augment, integrate, and orchestrate LLMs in the larger context of enterprise platforms that use existing proprietary data and models to tackle complex real-world tasks. Despite the tremendous success of these systems, current approaches rely on narrow, single-focus objectives for optimization and evaluation, often overlooking potential constraints in real-world scenarios, including restricted budgets, resources and time. Furthermore, interpreting, analyzing, and debugging these systems requires different components to be evaluated in relation to one another. This demand is currently not feasible with existing methodologies. In this postion paper, we introduce the concept of reasoning capacity as a unifying criterion to enable integration o
    
[^110]: 使用循环变压器模拟图算法

    Simulation of Graph Algorithms with Looped Transformers

    [https://rss.arxiv.org/abs/2402.01107](https://rss.arxiv.org/abs/2402.01107)

    本文研究了使用循环变压器网络模拟图算法的能力，证明了该结构可以模拟Dijkstra的最短路径算法、广度优先搜索、深度优先搜索和Kosaraju的强连通分量算法，并展示了其具有图灵完整性的结果。

    

    最近，使用神经网络执行图算法引起了很大的兴趣，由于有了令人满意的实证进展。这促使我们进一步了解神经网络如何能够使用关系数据复制推理步骤。在这项工作中，我们从理论角度研究了变压器网络模拟图算法的能力。我们使用的架构是一个带额外注意力头和与图形交互的循环变压器。我们通过构造证明了这种架构能够模拟诸如Dijkstra的最短路径算法、广度优先搜索、深度优先搜索和Kosaraju的强连通分量算法等算法。网络的宽度不随输入图的大小增加，这意味着网络可以模拟任何图上的上述算法。尽管有这个特性，我们展示了在我们的解决方案中有一个由于有限精度而受到限制的模拟极限。最后，我们展示了我们的解决方案具有图灵完整性的结果。

    The execution of graph algorithms using neural networks has recently attracted significant interest due to promising empirical progress. This motivates further understanding of how neural networks can replicate reasoning steps with relational data. In this work, we study the ability of transformer networks to simulate algorithms on graphs from a theoretical perspective. The architecture that we utilize is a looped transformer with extra attention heads that interact with the graph. We prove by construction that this architecture can simulate algorithms such as Dijkstra's shortest path algorithm, Breadth- and Depth-First Search, and Kosaraju's strongly connected components algorithm. The width of the network does not increase with the size of the input graph, which implies that the network can simulate the above algorithms for any graph. Despite this property, we show that there is a limit to simulation in our solution due to finite precision. Finally, we show a Turing Completeness resu
    
[^111]: 自动驾驶领域基础模型综述

    A Survey for Foundation Models in Autonomous Driving

    [https://rss.arxiv.org/abs/2402.01105](https://rss.arxiv.org/abs/2402.01105)

    本综述论文回顾了40多篇研究论文，总结了基于基础模型的自动驾驶在规划、仿真和关键任务方面的重要贡献，强调了大型语言模型的推理和翻译能力，视觉基础模型在物体检测和驾驶场景创建方面的应用，以及多模态基础模型的视觉理解和空间推理能力。

    

    基于基础模型的出现，自然语言处理和计算机视觉领域发生了革命，为自动驾驶应用铺平了道路。本综述论文对40多篇研究论文进行了全面的回顾，展示了基础模型在提升自动驾驶中的作用。大型语言模型在自动驾驶的规划和仿真中发挥着重要作用，特别是通过其在推理、代码生成和翻译方面的能力。与此同时，视觉基础模型在关键任务中得到越来越广泛的应用，例如三维物体检测和跟踪，以及为仿真和测试创建逼真的驾驶场景。多模态基础模型可以整合多样的输入，展现出卓越的视觉理解和空间推理能力，对于端到端自动驾驶至关重要。本综述不仅提供了一个结构化的分类，根据模态和自动驾驶领域中的功能对基础模型进行分类，还深入研究了方法。

    The advent of foundation models has revolutionized the fields of natural language processing and computer vision, paving the way for their application in autonomous driving (AD). This survey presents a comprehensive review of more than 40 research papers, demonstrating the role of foundation models in enhancing AD. Large language models contribute to planning and simulation in AD, particularly through their proficiency in reasoning, code generation and translation. In parallel, vision foundation models are increasingly adapted for critical tasks such as 3D object detection and tracking, as well as creating realistic driving scenarios for simulation and testing. Multi-modal foundation models, integrating diverse inputs, exhibit exceptional visual understanding and spatial reasoning, crucial for end-to-end AD. This survey not only provides a structured taxonomy, categorizing foundation models based on their modalities and functionalities within the AD domain but also delves into the meth
    
[^112]: 组合生成建模：单一模型并不是您所需要的全部

    Compositional Generative Modeling: A Single Model is Not All You Need

    [https://rss.arxiv.org/abs/2402.01103](https://rss.arxiv.org/abs/2402.01103)

    本文提出了一种组合生成方法，通过将较小的生成模型组合在一起来构建大型生成系统。该方法可以更高效地学习数据分布，实现对训练时未见的数据部分的泛化，并能够编写和构建新的生成模型。

    

    在人工智能研究中，通过训练大规模的巨大的生成模型来处理海量数据已经成为一种越来越主流的方法。本文中，我们认为我们应该通过将较小的生成模型组合在一起来构建大型生成系统。我们展示了这种组合生成方法如何以更高效的方式学习分布，使得我们在训练时未见的数据分布部分也能进行泛化。我们进一步展示了这种方法如何使我们能够为训练时完全未见的任务编写和构建新的生成模型。最后，我们展示在许多情况下，我们可以从数据中发现独立的组合组件。

    Large monolithic generative models trained on massive amounts of data have become an increasingly dominant approach in AI research. In this paper, we argue that we should instead construct large generative systems by composing smaller generative models together. We show how such a compositional generative approach enables us to learn distributions in a more data-efficient manner, enabling generalization to parts of the data distribution unseen at training time. We further show how this enables us to program and construct new generative models for tasks completely unseen at training. Finally, we show that in many cases, we can discover separate compositional components from data.
    
[^113]: 基于斯坦优化梯度下降的贝叶斯深度学习在剩余寿命估计中的应用

    Bayesian Deep Learning for Remaining Useful Life Estimation via Stein Variational Gradient Descent

    [https://rss.arxiv.org/abs/2402.01098](https://rss.arxiv.org/abs/2402.01098)

    本研究使用斯坦优化梯度下降算法，将标准的频率主义神经网络转化为贝叶斯神经网络，以应对预测性维护中估计剩余寿命的不确定性问题。

    

    在预测性维护中，估计物理系统的剩余可用寿命是一项关键任务。在过去十年中，深度学习在预测性能方面显著改进了传统的基于模型和统计方法。然而，为了优化计划维护操作，量化预测中固有的不确定性也很重要。这个问题可以通过将标准的频率主义神经网络转化为贝叶斯神经网络来解决，后者能够自然地在估计周围提供置信区间。存在多种训练这些模型的方法。研究人员主要关注参数变分推理和基于采样的技术，这些技术因近似能力有限和计算负担大而闻名。在这项工作中，我们使用了斯坦优化梯度下降，这是一种最近提出的逼近难以计算分布的算法，克服了上述方法的缺点。

    A crucial task in predictive maintenance is estimating the remaining useful life of physical systems. In the last decade, deep learning has improved considerably upon traditional model-based and statistical approaches in terms of predictive performance. However, in order to optimally plan maintenance operations, it is also important to quantify the uncertainty inherent to the predictions. This issue can be addressed by turning standard frequentist neural networks into Bayesian neural networks, which are naturally capable of providing confidence intervals around the estimates. Several methods exist for training those models. Researchers have focused mostly on parametric variational inference and sampling-based techniques, which notoriously suffer from limited approximation power and large computational burden, respectively. In this work, we use Stein variational gradient descent, a recently proposed algorithm for approximating intractable distributions that overcomes the drawbacks of th
    
[^114]: 可信的分布式AI系统：鲁棒性、隐私和治理

    Trustworthy Distributed AI Systems: Robustness, Privacy, and Governance

    [https://rss.arxiv.org/abs/2402.01096](https://rss.arxiv.org/abs/2402.01096)

    本文回顾了可信的分布式AI的代表性技术，包括鲁棒性保证、隐私保护和公平意识，以解决分布式学习中存在的安全、隐私和公平问题。

    

    新兴的分布式AI系统正在革新大数据计算和数据处理能力，并对经济和社会产生越来越大的影响。然而，最近的研究发现，AI系统中的安全、隐私和公平问题导致了新的攻击面和风险。在本文中，我们通过鲁棒性保证、隐私保护和公平意识在分布式学习中回顾了代表性的技术、算法和理论基础，以实现可信的分布式AI。我们首先提供了分布式学习的替代架构的简要概述，讨论了分布式学习中AI算法的安全、隐私和公平的固有漏洞，并分析了为什么这些问题在分布式学习中存在，而不管具体的架构如何。然后，我们提供了针对可信分布式AI的独特分类，涵盖了对推理中的逃避攻击和不规则查询的鲁棒性，以及对中毒攻击和数据泄露的隐私保护。

    Emerging Distributed AI systems are revolutionizing big data computing and data processing capabilities with growing economic and societal impact. However, recent studies have identified new attack surfaces and risks caused by security, privacy, and fairness issues in AI systems. In this paper, we review representative techniques, algorithms, and theoretical foundations for trustworthy distributed AI through robustness guarantee, privacy protection, and fairness awareness in distributed learning. We first provide a brief overview of alternative architectures for distributed learning, discuss inherent vulnerabilities for security, privacy, and fairness of AI algorithms in distributed learning, and analyze why these problems are present in distributed learning regardless of specific architectures. Then we provide a unique taxonomy of countermeasures for trustworthy distributed AI, covering (1) robustness to evasion attacks and irregular queries at inference, and robustness to poisoning a
    
[^115]: 深度神经网络预测使用多少个视图？

    How many views does your deep neural network use for prediction?

    [https://rss.arxiv.org/abs/2402.01095](https://rss.arxiv.org/abs/2402.01095)

    本文提出了最小有效视图（MSVs）的概念，该概念类似于多视图，但适用于实际图像，并且通过实证研究表明，MSV的数量与模型的预测准确性之间存在关系。

    

    尽管进行了许多理论和实证分析，但深度神经网络（DNN）的泛化能力仍未完全理解。最近，Allen-Zhu和Li（2023）引入了多视图的概念来解释DNN的泛化能力，但他们的主要目标是集成或蒸馏模型，并未讨论用于特定输入预测的多视图估计方法。在本文中，我们提出了最小有效视图（MSVs），它类似于多视图，但可以高效地计算真实图像。MSVs是输入中的一组最小且不同的特征，每个特征保留了模型对该输入的预测。我们通过实证研究表明，不同模型（包括卷积和转换模型）的MSV数量与预测准确性之间存在明确的关系，这表明多视图的角度对于理解（非集成或非蒸馏）DNN的泛化能力也很重要。

    The generalization ability of Deep Neural Networks (DNNs) is still not fully understood, despite numerous theoretical and empirical analyses. Recently, Allen-Zhu & Li (2023) introduced the concept of multi-views to explain the generalization ability of DNNs, but their main target is ensemble or distilled models, and no method for estimating multi-views used in a prediction of a specific input is discussed. In this paper, we propose Minimal Sufficient Views (MSVs), which is similar to multi-views but can be efficiently computed for real images. MSVs is a set of minimal and distinct features in an input, each of which preserves a model's prediction for the input. We empirically show that there is a clear relationship between the number of MSVs and prediction accuracy across models, including convolutional and transformer models, suggesting that a multi-view like perspective is also important for understanding the generalization ability of (non-ensemble or non-distilled) DNNs.
    
[^116]: 使用有限领域数据进行廉价推理的专用语言模型

    Specialized Language Models with Cheap Inference from Limited Domain Data

    [https://rss.arxiv.org/abs/2402.01093](https://rss.arxiv.org/abs/2402.01093)

    本研究提出了一种使用有限领域数据进行廉价推理的专用语言模型。在研究中，我们通过比较不同的机器学习方法，在推理成本的限制下找到了比训练非常大的基本转换器模型更优的替代方案。具体而言，在大型预训练预算下，超网络和专家混合模型的困惑度更好，而在大型专用预算下，训练重要样本数据集上的小型模型更具吸引力。

    

    大型语言模型已成为一种多才多艺的工具，但在缺乏大规模推理预算和大规模领域内训练集的任务中应用起来具有挑战性。本研究对这些限制进行了形式化，并区分了四个重要的变量：预训练预算（用于在目标领域出现之前进行训练），专用预算（用于在目标领域出现之后进行训练），推理预算和领域内训练集大小。在这些设置中，我们比较了机器学习文献中的不同方法。受到推理成本的限制，我们发现比训练非常大的基本转换器模型的标准做法更好的替代方案。特别是，我们发现超网络和专家混合模型在大型预训练预算下具有更好的困惑度，而在大型专用预算下，训练重要样本数据集上的小型模型更具吸引力。

    Large language models have emerged as a versatile tool but are challenging to apply to tasks lacking large inference budgets and large in-domain training sets. This work formalizes these constraints and distinguishes four important variables: the pretraining budget (for training before the target domain is known), the specialization budget (for training after the target domain is known), the inference budget, and the in-domain training set size. Across these settings, we compare different approaches from the machine learning literature. Limited by inference cost, we find better alternatives to the standard practice of training very large vanilla transformer models. In particular, we show that hyper-networks and mixture of experts have better perplexity for large pretraining budgets, while small models trained on importance sampled datasets are attractive for large specialization budgets.
    
[^117]: 神经缩放定律的动力学模型

    A Dynamical Model of Neural Scaling Laws

    [https://rss.arxiv.org/abs/2402.01092](https://rss.arxiv.org/abs/2402.01092)

    这篇论文提出了一个动力学模型来解释神经缩放定律。通过分析梯度下降训练的随机特征模型，研究发现训练时间和模型大小的缩放具有不同的幂律指数，而计算最优缩放规则要求增加训练步数快于增加模型参数，与实证观察相一致。

    

    在各种任务中，神经网络的性能随着训练时间、数据集大小和模型大小的增加而预测性地提高，跨多个数量级。这种现象被称为神经缩放定律。最重要的是计算最优缩放定律，它报告了在选择最佳模型大小时性能与计算数量的关系。我们分析了一个通过梯度下降进行训练和泛化的随机特征模型作为网络训练和泛化的可解模型。这个模型复现了关于神经缩放定律的许多观察结果。首先，我们的模型对于为什么训练时间和模型大小的缩放具有不同的幂律指数提出了一个预测。因此，理论预测了一种不对称的计算最优缩放规则，其中训练步数的增加速度快于模型参数的增加速度，与最近的实证观察一致。其次，观察到在训练的早期，网络会收敛到无限宽度情况下的结果。

    On a variety of tasks, the performance of neural networks predictably improves with training time, dataset size and model size across many orders of magnitude. This phenomenon is known as a neural scaling law. Of fundamental importance is the compute-optimal scaling law, which reports the performance as a function of units of compute when choosing model sizes optimally. We analyze a random feature model trained with gradient descent as a solvable model of network training and generalization. This reproduces many observations about neural scaling laws. First, our model makes a prediction about why the scaling of performance with training time and with model size have different power law exponents. Consequently, the theory predicts an asymmetric compute-optimal scaling rule where the number of training steps are increased faster than model parameters, consistent with recent empirical observations. Second, it has been observed that early in training, networks converge to their infinite-wi
    
[^118]: 可扩展的高阶张量积样条模型

    Scalable Higher-Order Tensor Product Spline Models

    [https://rss.arxiv.org/abs/2402.01090](https://rss.arxiv.org/abs/2402.01090)

    我们提出了一种可扩展的高阶张量积样条模型，允许加入所有（高阶）非线性特征效应的相互作用，并具有与没有相互作用的模型成比例的计算成本。

    

    在当前大数据和透明机器学习的时代，技术在大规模应用中运作的同时，还需要提供对方法内部工作的清晰数学理解。虽然已经存在适用于大规模应用的可解释的半参数回归方法，考虑了数据的非线性，但模型的复杂性常常受到限制。主要挑战之一是这些模型中缺乏相互作用，出于更好的解释能力和不切实际的计算成本考虑而被忽略。为了克服这个限制，我们提出了一种新的方法，使用因子化方法推导出高度可扩展的高阶张量积样条模型。我们的方法允许将所有（高阶）非线性特征效应的相互作用纳入模型，同时具有与没有相互作用的模型成比例的计算成本。我们进一步开发了一种有意义的惩罚方案

    In the current era of vast data and transparent machine learning, it is essential for techniques to operate at a large scale while providing a clear mathematical comprehension of the internal workings of the method. Although there already exist interpretable semi-parametric regression methods for large-scale applications that take into account non-linearity in the data, the complexity of the models is still often limited. One of the main challenges is the absence of interactions in these models, which are left out for the sake of better interpretability but also due to impractical computational costs. To overcome this limitation, we propose a new approach using a factorization method to derive a highly scalable higher-order tensor product spline model. Our method allows for the incorporation of all (higher-order) interactions of non-linear feature effects while having computational costs proportional to a model without interactions. We further develop a meaningful penalization scheme a
    
[^119]: 无免费修剪：初始化时剪枝的信息论障碍

    No Free Prune: Information-Theoretic Barriers to Pruning at Initialization

    [https://rss.arxiv.org/abs/2402.01089](https://rss.arxiv.org/abs/2402.01089)

    本文解释了为什么在初始化时修剪神经网络困难，并提出了一个关于有效参数数量的理论解释。我们指出，在嘈杂数据中鲁棒地插值的稀疏神经网络需要严重依赖于数据的掩码。为此，我们怀疑在训练过程中和训练后修剪是必要的。

    

    “抽奖中奖者”是否在初始化时存在，引发了一个令人着迷的问题：深度学习是否需要大型模型，或者可以在不训练包含它们的密集模型的情况下迅速识别和训练稀疏网络。然而，尝试在初始化时找到这些稀疏子网络（“初始化时修剪”）的努力在广泛上都没有成功。我们提出了一个理论解释，基于模型的有效参数数量$p_\text{eff}$，由最终网络中非零权重的数量和稀疏掩码与数据之间的相互信息的总和给出。我们展示了“鲁棒性定律”（arXiv:2105.12806）延伸到稀疏网络，其中常规参数数量被$p_\text{eff}$所取代，这意味着一个能够在嘈杂数据中鲁棒地插值的稀疏神经网络需要严重依赖于数据的掩码。我们假设在训练过程中和训练后修剪。

    The existence of "lottery tickets" arXiv:1803.03635 at or near initialization raises the tantalizing question of whether large models are necessary in deep learning, or whether sparse networks can be quickly identified and trained without ever training the dense models that contain them. However, efforts to find these sparse subnetworks without training the dense model ("pruning at initialization") have been broadly unsuccessful arXiv:2009.08576. We put forward a theoretical explanation for this, based on the model's effective parameter count, $p_\text{eff}$, given by the sum of the number of non-zero weights in the final network and the mutual information between the sparsity mask and the data. We show the Law of Robustness of arXiv:2105.12806 extends to sparse networks with the usual parameter count replaced by $p_\text{eff}$, meaning a sparse neural network which robustly interpolates noisy data requires a heavily data-dependent mask. We posit that pruning during and after training 
    
[^120]: Salsa Fresca: 基于角嵌入和预训练的机器学习攻击学习与误差问题的研究

    Salsa Fresca: Angular Embeddings and Pre-Training for ML Attacks on Learning With Errors

    [https://rss.arxiv.org/abs/2402.01082](https://rss.arxiv.org/abs/2402.01082)

    本研究提出了三种关键方法，即更好的预处理、角嵌入和模型预训练，用于改进机器学习攻击学习与误差问题，并使其能够在更大维度的情况下恢复稀疏二进制密钥。

    

    学习与误差（LWE）是一种困难的数学问题，是最近标准化的量子后-密码学（PQC）系统中用于密钥交换和数字签名的基础。先前的研究提出了基于机器学习（ML）的攻击方法来攻击具有小型和稀疏密钥的LWE问题，但这些攻击需要数百万个LWE样本进行训练，并且需要几天的时间才能恢复密钥。我们提出了三种关键方法——更好的预处理、角嵌入和模型预训练——来改进这些攻击方法，加快了预处理速度25倍，提高了模型样本效率10倍。我们首次证明了预训练可以改善ML攻击LWE的成本。我们的架构改进使得可以扩展到更大维度的 LWE 问题：这是首次在实践中使用最小维度$n=1024$的同态加密应用中恢复稀疏二进制密钥的 ML 攻击。

    Learning with Errors (LWE) is a hard math problem underlying recently standardized post-quantum cryptography (PQC) systems for key exchange and digital signatures. Prior work proposed new machine learning (ML)-based attacks on LWE problems with small, sparse secrets, but these attacks require millions of LWE samples to train on and take days to recover secrets. We propose three key methods -- better preprocessing, angular embeddings and model pre-training -- to improve these attacks, speeding up preprocessing by $25\times$ and improving model sample efficiency by $10\times$. We demonstrate for the first time that pre-training improves and reduces the cost of ML attacks on LWE. Our architecture improvements enable scaling to larger-dimension LWE problems: this work is the first instance of ML attacks recovering sparse binary secrets in dimension $n=1024$, the smallest dimension used in practice for homomorphic encryption applications of LWE where sparse binary secrets are proposed.
    
[^121]: 电子健康记录预测建模的最新进展

    Recent Advances in Predictive Modeling with Electronic Health Records

    [https://rss.arxiv.org/abs/2402.01077](https://rss.arxiv.org/abs/2402.01077)

    这项调查总结了基于电子健康记录数据的深度学习预测模型的最新进展，包括背景介绍、数学定义、分类总结、基准和工具包，以及未来研究方向的讨论。

    

    电子健康记录（EHR）系统的发展使得大量的数字化患者数据得以收集。然而，由于其独特的特性，利用EHR数据进行预测建模面临着一些挑战。随着机器学习技术的进步，深度学习在包括医疗在内的各个领域展现出了卓越的优势。本调查系统地回顾了基于EHR数据的深度学习预测模型的最新进展。具体而言，我们首先介绍了EHR数据的背景，并提供了预测建模任务的数学定义。然后，我们从多个角度对预测深度模型进行分类和总结。此外，我们还介绍了与医疗预测建模相关的基准和工具包。最后，我们讨论了开放性挑战，并提出了未来研究的有希望的方向。

    The development of electronic health records (EHR) systems has enabled the collection of a vast amount of digitized patient data. However, utilizing EHR data for predictive modeling presents several challenges due to its unique characteristics. With the advancements in machine learning techniques, deep learning has demonstrated its superiority in various applications, including healthcare. This survey systematically reviews recent advances in deep learning-based predictive models using EHR data. Specifically, we begin by introducing the background of EHR data and providing a mathematical definition of the predictive modeling task. We then categorize and summarize predictive deep models from multiple perspectives. Furthermore, we present benchmarks and toolkits relevant to predictive modeling in healthcare. Finally, we conclude this survey by discussing open challenges and suggesting promising directions for future research.
    
[^122]: DoseGNN：通过图神经网络改进自适应剂量体积直方图预测的深度学习模型性能

    DoseGNN: Improving the Performance of Deep Learning Models in Adaptive Dose-Volume Histogram Prediction through Graph Neural Networks

    [https://rss.arxiv.org/abs/2402.01076](https://rss.arxiv.org/abs/2402.01076)

    本文通过图神经网络构建了一个即插即用的框架，增强了基于深度学习模型的剂量体积直方图预测的性能。

    

    剂量体积直方图（DVH）预测是放射治疗中基础性的任务，可用于治疗计划、剂量评估、方案比较等。它可以增加精确和有效的放射治疗能力，同时管理对健康组织的潜在毒性，以降低并发症的风险。本论文扩展了在AAPM（AAPM第65届年会和展览会）上披露的最新研究成果，并加入了必要的技术细节。研究目标是设计用于通用放射治疗平台的高效深度学习模型，该平台配备高性能CBCT系统，输入的CT图像和目标剂量图像可能有不同的来源、间距和大小。在新型放射治疗平台上评估了广泛采用的DVH预测任务中的深度学习模型，并且显示出图神经网络（GNN）是构建即插即用框架以改进预测性能的理想架构。

    Dose-Volume Histogram (DVH) prediction is fundamental in radiation therapy that facilitate treatment planning, dose evaluation, plan comparison and etc. It helps to increase the ability to deliver precise and effective radiation treatments while managing potential toxicities to healthy tissues as needed to reduce the risk of complications. This paper extends recently disclosed research findings presented on AAPM (AAPM 65th Annual Meeting $\&$ Exhibition) and includes necessary technique details. The objective is to design efficient deep learning models for DVH prediction on general radiotherapy platform equipped with high performance CBCT system, where input CT images and target dose images to predict may have different origins, spacing and sizes. Deep learning models widely-adopted in DVH prediction task are evaluated on the novel radiotherapy platform, and graph neural networks (GNNs) are shown to be the ideal architecture to construct a plug-and-play framework to improve predictive 
    
[^123]: 变色龙：用于增强少数群体覆盖的公平感知多模态数据增强的基础模型

    Chameleon: Foundation Models for Fairness-aware Multi-modal Data Augmentation to Enhance Coverage of Minorities

    [https://rss.arxiv.org/abs/2402.01071](https://rss.arxiv.org/abs/2402.01071)

    本文提出了名为变色龙的系统，利用生成式人工智能和大型语言模型增强少数群体在多模态数据中的覆盖范围。系统通过最少添加合成生成的元组的方式来实现数据增强，并采用拒绝抽样方法确保生成元组的高质量和分布一致。实验证明了该方法的高效性。

    

    少数群体在训练数据中的欠表示可能带来潜在的危害，尤其是在多模态情况下。尽管在检测这种欠表示方面已经进行了广泛的努力，但解决方案仍然具有挑战性。随着生成式人工智能和大型语言模型的最新进展，基础模型已经在各个领域中成为多功能工具。在本文中，我们提出了变色龙（Chameleon），这是一个系统，通过最少的合成生成的元组来增强数据集，以增强少数群体的覆盖范围。我们的系统采用了拒绝抽样的方法，以确保生成的元组具有高质量并遵循基础分布。为了最小化生成元组的拒绝几率，我们提出了多种策略来为基础模型提供指导。我们的实验结果不仅证实了我们提出的方法的效果，同时也表明了其高效性。

    The potential harms of the under-representation of minorities in training data, particularly in multi-modal settings, is a well-recognized concern. While there has been extensive effort in detecting such under-representation, resolution has remained a challenge. With recent advancements in generative AI, large language models and foundation models have emerged as versatile tools across various domains. In this paper, we propose Chameleon, a system that efficiently utilizes these tools to augment a data set with a minimal addition of synthetically generated tuples, in order to enhance the coverage of the under-represented groups. Our system follows a rejection sampling approach to ensure the generated tuples have a high quality and follow the underlying distribution. In order to minimize the rejection chance of the generated tuples, we propose multiple strategies for providing a guide for the foundation model. Our experiment results, in addition to confirming the efficiency of our propo
    
[^124]: FedShift: 通过权重迁移聚合解决联邦学习的双重异质性问题

    FedShift: Tackling Dual Heterogeneity Problem of Federated Learning via Weight Shift Aggregation

    [https://rss.arxiv.org/abs/2402.01070](https://rss.arxiv.org/abs/2402.01070)

    本文介绍了一种名为FedShift的算法，通过权重迁移聚合来解决联邦学习中的异质性问题，并提高训练速度和模型准确性。

    

    联邦学习（FL）提供了一种训练机器学习模型并注重保护数据隐私的有力方法。FL中存在的系统异质性和统计异质性问题源于客户端硬件、网络和数据集分布的多样性。这种多样性可能会严重影响训练速度和模型性能。虽然许多研究通过引入通信效率或稳定收敛算法来解决系统或统计异质性问题，但单独解决这些挑战往往会导致妥协，因为异质性问题未得到解决。为此，本文介绍了一种名为FedShift的新算法，旨在在双重异质性场景中提高训练速度和模型的准确性。我们的解决方案通过量化改善客户参与度，并通过应用迁移技术来缓解量化通常导致的性能不良影响。

    Federated Learning (FL) offers a compelling method for training machine learning models with a focus on preserving data privacy. The presence of system heterogeneity and statistical heterogeneity, recognized challenges in FL, arises from the diversity of client hardware, network, and dataset distribution. This diversity can critically affect the training pace and the performance of models. While many studies address either system or statistical heterogeneity by introducing communication-efficient or stable convergence algorithms, addressing these challenges in isolation often leads to compromises due to unaddressed heterogeneity. In response, this paper introduces FedShift, a novel algorithm designed to enhance both the training speed and the models' accuracy in a dual heterogeneity scenario. Our solution can improve client engagement through quantization and mitigate the adverse effects on performance typically associated with quantization by employing a shifting technique. This techn
    
[^125]: 通过机器学习和深度学习模型评估Inspire治疗的患者适应性

    Assessing Patient Eligibility for Inspire Therapy through Machine Learning and Deep Learning Models

    [https://rss.arxiv.org/abs/2402.01067](https://rss.arxiv.org/abs/2402.01067)

    本文利用机器学习和深度学习技术评估Inspire治疗的患者适应性，通过分析医学数据和DISE视频，比较了不同模型的性能，展示了机器学习和深度学习技术的潜力。

    

    Inspire治疗是一种获得FDA批准的内部神经刺激治疗阻塞性睡眠呼吸暂停症的方法。然而，并不是所有患者都对这种疗法产生反应，这对于经验丰富的耳鼻喉科医生来说都是一个挑战，他们很难确定适宜治疗的候选者。本文首次尝试利用机器学习和深度学习技术，通过使用医学数据和通过药物诱导睡眠内窥镜检查（DISE）捕捉到的视频来判断患者对Inspire治疗的反应性。为了实现这一目标，我们收集并注释了来自127名患者的三个数据集。其中两个数据集包括关注于舌根和鼻咽的内窥镜视频。第三个数据集包括患者的临床信息。通过利用这些数据集，我们评估和比较了六个深度学习模型和五个经典机器学习算法的性能。结果显示了利用机器学习和深度学习技术的潜力。

    Inspire therapy is an FDA-approved internal neurostimulation treatment for obstructive sleep apnea. However, not all patients respond to this therapy, posing a challenge even for experienced otolaryngologists to determine candidacy. This paper makes the first attempt to leverage both machine learning and deep learning techniques in discerning patient responsiveness to Inspire therapy using medical data and videos captured through Drug-Induced Sleep Endoscopy (DISE), an essential procedure for Inspire therapy. To achieve this, we gathered and annotated three datasets from 127 patients. Two of these datasets comprise endoscopic videos focused on the Base of the Tongue and Velopharynx. The third dataset composes the patient's clinical information. By utilizing these datasets, we benchmarked and compared the performance of six deep learning models and five classical machine learning algorithms. The results demonstrate the potential of employing machine learning and deep learning techniques
    
[^126]: 受生物启发的补偿性损伤修复策略在振翅机器人推进器上的应用

    Bio-Inspired Compensatory Strategies for Damage to Flapping Robotic Propulsors

    [https://rss.arxiv.org/abs/2402.01062](https://rss.arxiv.org/abs/2402.01062)

    通过人工进化确定对受损机器人系统的划水机制的最佳改变，并通过补偿机制实现恢复推力产生。

    

    为了保持完全的自主性，自主机器人系统必须具备自我修复的能力。通过补偿机制进行自我修复在自然界中很常见：例如，一些鱼类可以通过改变划水机制，在丢失76%的推动表面的情况下也不会丧失推力。然而，直接将这些改变从生物体传递到机器人振翅推进器上可能并不是最优的，因为存在不相关的进化压力。相反，我们试图通过人工进化确定对受损机器人系统的划水机制的最优改变。为了确定自然和机器学习得到的最优解是否不同，我们采用了一个基于协方差矩阵适应进化策略的网络物理系统，寻求给定力的最高效轨迹。我们还使用硬件-环路在线优化的方法，使用带有动作弯曲平板的试验功能评估。为了恢复部分截肢后的推力产生，最有效的方法是通过补偿机制的应用。

    To maintain full autonomy, autonomous robotic systems must have the ability to self-repair. Self-repairing via compensatory mechanisms appears in nature: for example, some fish can lose even 76% of their propulsive surface without loss of thrust by altering stroke mechanics. However, direct transference of these alterations from an organism to a robotic flapping propulsor may not be optimal due to irrelevant evolutionary pressures. We instead seek to determine what alterations to stroke mechanics are optimal for a damaged robotic system via artificial evolution. To determine whether natural and machine-learned optima differ, we employ a cyber-physical system using a Covariance Matrix Adaptation Evolutionary Strategy to seek the most efficient trajectory for a given force. We implement an online optimization with hardware-in-the-loop, performing experimental function evaluations with an actuated flexible flat plate. To recoup thrust production following partial amputation, the most effi
    
[^127]: 基于神经网络多项式的函数近似代数框架

    Towards an Algebraic Framework For Approximating Functions Using Neural Network Polynomials

    [https://rss.arxiv.org/abs/2402.01058](https://rss.arxiv.org/abs/2402.01058)

    本论文提出了一个基于神经网络多项式的函数近似代数框架，该框架在一定参数限制下能够有效近似实数函数，并且其参数和深度增长与所需精度多项式相关。

    

    我们论述了神经网络对象的重要性，并扩展了第2章中详细解释的已有神经网络微积分。我们的目标是展示神经网络多项式、神经网络指数函数、正弦和余弦的存在性，即它们确实在一定参数$q$和$\varepsilon$的限制下近似于实数对应物。在实现这一目标的同时，我们证明了参数和深度的增长仅以多项式形式与所需的精度相关（将其定义为$\mathbb{R}$上的1-范数差异），从而证明了这种近似方法中神经网络在某种意义上具有所近似函数的结构特性并非完全不可解。

    We make the case for neural network objects and extend an already existing neural network calculus explained in detail in Chapter 2 on \cite{bigbook}. Our aim will be to show that, yes, indeed, it makes sense to talk about neural network polynomials, neural network exponentials, sine, and cosines in the sense that they do indeed approximate their real number counterparts subject to limitations on certain of their parameters, $q$, and $\varepsilon$. While doing this, we show that the parameter and depth growth are only polynomial on their desired accuracy (defined as a 1-norm difference over $\mathbb{R}$), thereby showing that this approach to approximating, where a neural network in some sense has the structural properties of the function it is approximating is not entire intractable.
    
[^128]: 专家接近性作为单演示模仿学习的替代奖励

    Expert Proximity as Surrogate Rewards for Single Demonstration Imitation Learning

    [https://rss.arxiv.org/abs/2402.01057](https://rss.arxiv.org/abs/2402.01057)

    本文介绍了一种针对单演示模仿学习的新方法TDIL，通过引入基于转换鉴别器的替代奖励函数，鼓励代理向靠近专家状态的状态导航，有效解决了奖励信号稀疏的问题。

    

    本文关注单演示模仿学习（IL），这是一种在获取大量专家演示困难或不可行的实际应用中的实用方法。与 typicIL 设置中具有多个示范不同，单演示IL涉及代理只有一条专家轨迹的访问。我们强调在这种情况下奖励信号稀疏的问题，并提出通过我们提出的基于转换鉴别器的IL（TDIL）方法来缓解这个问题。TDIL是一种基于IRL的方法，旨在通过引入考虑环境动态的更密集的替代奖励函数来解决奖励稀疏性。这个替代奖励函数鼓励代理向靠近专家状态的状态导航。在实践中，TDIL训练一个过渡鉴别器来区分给定环境中的有效和非有效过渡以计算替代奖励。实验表明，TDIL优于现有方法。

    In this paper, we focus on single-demonstration imitation learning (IL), a practical approach for real-world applications where obtaining numerous expert demonstrations is costly or infeasible. In contrast to typical IL settings with multiple demonstrations, single-demonstration IL involves an agent having access to only one expert trajectory. We highlight the issue of sparse reward signals in this setting and propose to mitigate this issue through our proposed Transition Discriminator-based IL (TDIL) method. TDIL is an IRL method designed to address reward sparsity by introducing a denser surrogate reward function that considers environmental dynamics. This surrogate reward function encourages the agent to navigate towards states that are proximal to expert states. In practice, TDIL trains a transition discriminator to differentiate between valid and non-valid transitions in a given environment to compute the surrogate rewards. The experiments demonstrate that TDIL outperforms existin
    
[^129]: 从有噪声标签学习非可分解性能度量的多类学习

    Multiclass Learning from Noisy Labels for Non-decomposable Performance Measures

    [https://rss.arxiv.org/abs/2402.01055](https://rss.arxiv.org/abs/2402.01055)

    本论文提出了用于从带有噪声标签的数据中学习非可分解性能度量的多类学习算法。这些算法分别适用于单调凸性和线性比率两类性能度量，并基于类条件噪声模型进行噪声校正。

    

    近年来，学习从带有噪声标签的数据中得到良好分类器引起了广泛关注。大多数关于从有噪声标签学习的工作都集中在标准的基于损失的性能度量上。然而，许多机器学习问题需要使用非可分解性能度量，这些度量不能表示为单个示例上的损失的期望或总和；其中包括类不平衡设置中的H-mean，Q-mean和G-mean，以及信息检索中的Micro F1。在本文中，我们设计了算法，用于学习两类广泛的多类非可分解性能度量，即单调凸性和线性比率，它们包括上述所有示例。我们的工作基于Narasimhan等人的Frank-Wolfe和Bisection算法(2015)。在这两种情况下，我们在广泛研究的类条件噪声模型家族下开发了算法的噪声校正版本。我们提供了遗憾(超额风险)上界。

    There has been much interest in recent years in learning good classifiers from data with noisy labels. Most work on learning from noisy labels has focused on standard loss-based performance measures. However, many machine learning problems require using non-decomposable performance measures which cannot be expressed as the expectation or sum of a loss on individual examples; these include for example the H-mean, Q-mean and G-mean in class imbalance settings, and the Micro $F_1$ in information retrieval. In this paper, we design algorithms to learn from noisy labels for two broad classes of multiclass non-decomposable performance measures, namely, monotonic convex and ratio-of-linear, which encompass all the above examples. Our work builds on the Frank-Wolfe and Bisection based methods of Narasimhan et al. (2015). In both cases, we develop noise-corrected versions of the algorithms under the widely studied family of class-conditional noise models. We provide regret (excess risk) bounds 
    
[^130]: 无条件的隐式扩散模型记忆患者影像数据

    Unconditional Latent Diffusion Models Memorize Patient Imaging Data

    [https://rss.arxiv.org/abs/2402.01054](https://rss.arxiv.org/abs/2402.01054)

    本论文研究了医学图像合成中隐式扩散模型的记忆问题。通过评估训练数据的记忆程度以及探索可能导致记忆的因素，揭示了这一问题的重要性和潜在风险。

    

    生成式的隐式扩散模型在医学影像领域具有广泛的应用。一个值得注意的应用是通过提出合成数据作为真实患者数据的替代品来实现隐私保护的开放数据共享。尽管有这个应用的前景，但这些模型容易出现患者数据的记忆问题，即模型生成患者数据的副本而不是新的合成样本。这破坏了保护患者数据的整个目的，甚至可能导致患者被重新识别。考虑到这个问题的重要性，令人惊讶的是，在医学影像界中这个问题并没有得到太多关注。为此，我们评估了医学图像合成中隐式扩散模型的记忆问题。我们训练了2D和3D的隐式扩散模型，使用CT、MR和X光数据集进行合成数据的生成。之后，我们利用自监督模型来评估训练数据被记忆的程度，并进一步研究可能导致记忆的各种因素。

    Generative latent diffusion models hold a wide range of applications in the medical imaging domain. A noteworthy application is privacy-preserved open-data sharing by proposing synthetic data as surrogates of real patient data. Despite the promise, these models are susceptible to patient data memorization, where models generate patient data copies instead of novel synthetic samples. This undermines the whole purpose of preserving patient data and may even result in patient re-identification. Considering the importance of the problem, surprisingly it has received relatively little attention in the medical imaging community. To this end, we assess memorization in latent diffusion models for medical image synthesis. We train 2D and 3D latent diffusion models on CT, MR, and X-ray datasets for synthetic data generation. Afterwards, we examine the amount of training data memorized utilizing self-supervised models and further investigate various factors that can possibly lead to memorization 
    
[^131]: 弱凸正则化器在逆问题中的收敛性：临界点和原始-对偶优化的收敛

    Weakly Convex Regularisers for Inverse Problems: Convergence of Critical Points and Primal-Dual Optimisation

    [https://rss.arxiv.org/abs/2402.01052](https://rss.arxiv.org/abs/2402.01052)

    本文提出了一种关于逆问题的弱凸正则化器的收敛性问题的一般化公式，并证明了通过一类弱凸正则化器的实现可以达到收敛，并应用于学习的正则化中实现了对计算机层析成像中学习对抗性正则化器性能的提高。

    

    变分正则化是解决逆问题的主要方法，最近有很多研究利用深度学习的正则化方法来提高性能。然而，在解决这种正则化收敛性的问题上，很少有关于临界点收敛性的结果，而非全局极小值点的收敛性。本文提出了一种关于临界点收敛性的一般化公式，并证明了这是通过一类弱凸正则化器实现的。我们证明了与相关变分问题相关的原始-对偶混合梯度方法的收敛性，并在给定Kurdyka-Lojasiewicz条件的情况下，证明了O(log(k)/k)的遗传收敛速度。最后，将这个理论应用于学习的正则化中，我们证明了输入为弱凸神经网络（IWCNN）的通用逼近性，并通过实验证明，IWCNN可以提高计算机层析成像中学习对抗性正则化器的性能。

    Variational regularisation is the primary method for solving inverse problems, and recently there has been considerable work leveraging deeply learned regularisation for enhanced performance. However, few results exist addressing the convergence of such regularisation, particularly within the context of critical points as opposed to global minima. In this paper, we present a generalised formulation of convergent regularisation in terms of critical points, and show that this is achieved by a class of weakly convex regularisers. We prove convergence of the primal-dual hybrid gradient method for the associated variational problem, and, given a Kurdyka-Lojasiewicz condition, an $\mathcal{O}(\log{k}/k)$ ergodic convergence rate. Finally, applying this theory to learned regularisation, we prove universal approximation for input weakly convex neural networks (IWCNN), and show empirically that IWCNNs can lead to improved performance of learned adversarial regularisers for computed tomography (
    
[^132]: 基于分布式MCMC推理的贝叶斯非参数潜在分块模型

    Distributed MCMC inference for Bayesian Non-Parametric Latent Block Model

    [https://rss.arxiv.org/abs/2402.01050](https://rss.arxiv.org/abs/2402.01050)

    本文提出了一种基于分布式MCMC推理的贝叶斯非参数潜在分块模型方法，通过将观测值和特征划分为分区，并采用Master/Worker架构来提高聚类标签准确性和执行时间。

    

    本文介绍了一种新颖的分布式马尔可夫链蒙特卡罗（MCMC）推理方法，用于贝叶斯非参数潜在分块模型（DisNPLBM），采用Master/Worker架构。我们的非参数共聚类算法使用潜在多元高斯块分布将观测值和特征划分为分区。行上的工作负载均匀分布在工人之间，他们只与主节点进行通信，而不与彼此通信。通过实验结果，DisNPLBM证明了其对聚类标签准确性和执行时间的影响。此外，我们还通过将我们的方法应用于共同聚类基因表达数据来展示一个真实的用例。代码源可公开访问https://github.com/redakhoufache/Distributed-NPLBM。

    In this paper, we introduce a novel Distributed Markov Chain Monte Carlo (MCMC) inference method for the Bayesian Non-Parametric Latent Block Model (DisNPLBM), employing the Master/Worker architecture. Our non-parametric co-clustering algorithm divides observations and features into partitions using latent multivariate Gaussian block distributions. The workload on rows is evenly distributed among workers, who exclusively communicate with the master and not among themselves. DisNPLBM demonstrates its impact on cluster labeling accuracy and execution times through experimental results. Moreover, we present a real-use case applying our approach to co-cluster gene expression data. The code source is publicly available at https://github.com/redakhoufache/Distributed-NPLBM.
    
[^133]: 基于FPGA的超快速变压器用于粒子物理实验

    Ultra Fast Transformers on FPGAs for Particle Physics Experiments

    [https://rss.arxiv.org/abs/2402.01047](https://rss.arxiv.org/abs/2402.01047)

    本研究介绍了一种在FPGA上高效实现变压器架构的方法，并在粒子物理实验中的触发器应用中取得了非常好的效果，实现了低于2微秒的延迟，符合大型强子对撞机实验的要求。

    

    本研究通过使用"hls4ml"工具在Field-Programmable Gate Array（FPGA）上实现了变压器架构的高效实现。鉴于变压器模型在解决各种问题方面的有效性已经得到证明，它们在粒子物理实验中实验触发器的应用成为一个非常感兴趣的话题。在这项工作中，我们实现了变压器模型的关键组件，如多头注意力和softmax层。为了评估我们实现的有效性，我们专注于粒子物理学喷注风味标记问题，使用了一个公共数据集。我们记录下了在Xilinx UltraScale+ FPGA上低于2微秒的延迟，这符合欧洲核子研究中心大型强子对撞机实验的硬件触发要求。

    This work introduces a highly efficient implementation of the transformer architecture on a Field-Programmable Gate Array (FPGA) by using the \texttt{hls4ml} tool. Given the demonstrated effectiveness of transformer models in addressing a wide range of problems, their application in experimental triggers within particle physics becomes a subject of significant interest. In this work, we have implemented critical components of a transformer model, such as multi-head attention and softmax layers. To evaluate the effectiveness of our implementation, we have focused on a particle physics jet flavor tagging problem, employing a public dataset. We recorded latency under 2 $\mu$s on the Xilinx UltraScale+ FPGA, which is compatible with hardware trigger requirements at the CERN Large Hadron Collider experiments.
    
[^134]: LatticeGraphNet: 一种用于模拟格状结构的双尺度图神经操作器

    LatticeGraphNet: A two-scale graph neural operator for simulating lattice structures

    [https://rss.arxiv.org/abs/2402.01045](https://rss.arxiv.org/abs/2402.01045)

    LatticeGraphNet是一种双尺度图神经操作器，通过学习格状结构的降维动力学和降维表示到四面体网格的映射，能够高效地预测任意格状结构的变形，并在显著减少推断时间的同时保持高准确性。

    

    本研究引入了一种双尺度图神经操作器，即LatticeGraphNet（LGN），旨在作为昂贵的非线性有限元模拟三维格状零部件和结构的代理模型。LGN具有两个网络：LGN-i用于学习格状结构的降维动力学，LGN-ii用于学习从降维表示到四面体网格的映射。LGN能够对任意格状结构进行变形预测，因此被称为操作器。我们的方法在保持高准确性的同时显著减少了推断时间，为评估格状结构的力学响应提供了高效的代理模型。

    This study introduces a two-scale Graph Neural Operator (GNO), namely, LatticeGraphNet (LGN), designed as a surrogate model for costly nonlinear finite-element simulations of three-dimensional latticed parts and structures. LGN has two networks: LGN-i, learning the reduced dynamics of lattices, and LGN-ii, learning the mapping from the reduced representation onto the tetrahedral mesh. LGN can predict deformation for arbitrary lattices, therefore the name operator. Our approach significantly reduces inference time while maintaining high accuracy for unseen simulations, establishing the use of GNOs as efficient surrogate models for evaluating mechanical responses of lattices and structures.
    
[^135]: 时间不齐次随机微分方程的费舍尔信息耗散

    Fisher information dissipation for time inhomogeneous stochastic differential equations

    [https://rss.arxiv.org/abs/2402.01036](https://rss.arxiv.org/abs/2402.01036)

    本文提供了时间不齐次随机微分方程的费舍尔信息耗散方法。通过将Langevin动力学的概率转移方程构造为关于时间依赖的最优传输度量的修正梯度流，我们得到了概率密度函数的收敛性保证，并在几个时间不齐次Langevin动力学中进行了验证。

    

    我们为时间不齐次变系数随机微分方程(SDEs)提供了一个Lyapunov收敛性分析。三个典型的例子包括过阻尼、不可逆漂移和欠阻尼Langevin动力学。我们首先将Langevin动力学的概率转移方程构造为概率空间中关于时间依赖的最优传输度量的Kullback-Leibler散度的修正梯度流。这个公式包含了梯度和非梯度方向，取决于一类时间依赖的目标分布。然后我们选择一个时间依赖的相对费舍尔信息泛函作为Lyapunov函数。我们发展了一个时间依赖的Hessian矩阵条件，保证了SDE的概率密度函数的收敛性。我们验证了几个时间不齐次Langevin动力学的所提出的条件。对于过阻尼Langevin动力学，我们证明了在$L^1$距离上的$O(t^{-1/2})$收敛。

    We provide a Lyapunov convergence analysis for time-inhomogeneous variable coefficient stochastic differential equations (SDEs). Three typical examples include overdamped, irreversible drift, and underdamped Langevin dynamics. We first formula the probability transition equation of Langevin dynamics as a modified gradient flow of the Kullback-Leibler divergence in the probability space with respect to time-dependent optimal transport metrics. This formulation contains both gradient and non-gradient directions depending on a class of time-dependent target distribution. We then select a time-dependent relative Fisher information functional as a Lyapunov functional. We develop a time-dependent Hessian matrix condition, which guarantees the convergence of the probability density function of the SDE. We verify the proposed conditions for several time-inhomogeneous Langevin dynamics. For the overdamped Langevin dynamics, we prove the $O(t^{-1/2})$ convergence in $L^1$ distance for the simula
    
[^136]: 跟着我重复：Transformer在复制任务上比状态空间模型更好

    Repeat After Me: Transformers are Better than State Space Models at Copying

    [https://rss.arxiv.org/abs/2402.01032](https://rss.arxiv.org/abs/2402.01032)

    这篇论文证明了Transformer模型在复制任务上的优势，相比于使用固定潜在状态的广义状态空间模型，Transformer模型能够更高效地复制和检索上下文信息。

    

    Transformer是序列建模的主要架构，但对于使用不依赖于序列长度的固定大小潜在状态的模型，也就是"广义状态空间模型" (GSSMs)，引起了越来越多的关注。在本文中，我们展示了虽然GSSMs在推理时间效率上有优势，但在需要从输入上下文复制的任务上，它们相对于transformer模型来说有限制。我们从对简单的字符串复制任务的理论分析开始，并证明了一个两层的transformer可以复制指数长度的字符串，而GSSMs由于其固定大小的潜在状态在根本上是有限制的。实证上，我们发现transformer在需要复制上下文的合成任务中，在效率和泛化性能上优于GSSMs。最后，我们评估了预训练的大型语言模型，并发现transformer模型在复制和检索上下文信息方面远远优于状态空间模型。

    Transformers are the dominant architecture for sequence modeling, but there is growing interest in models that use a fixed-size latent state that does not depend on the sequence length, which we refer to as "generalized state space models" (GSSMs). In this paper we show that while GSSMs are promising in terms of inference-time efficiency, they are limited compared to transformer models on tasks that require copying from the input context. We start with a theoretical analysis of the simple task of string copying and prove that a two layer transformer can copy strings of exponential length while GSSMs are fundamentally limited by their fixed-size latent state. Empirically, we find that transformers outperform GSSMs in terms of efficiency and generalization on synthetic tasks that require copying the context. Finally, we evaluate pretrained large language models and find that transformer models dramatically outperform state space models at copying and retrieving information from context. 
    
[^137]: 多元概率时间序列预测与相关误差

    Multivariate Probabilistic Time Series Forecasting with Correlated Errors

    [https://rss.arxiv.org/abs/2402.01000](https://rss.arxiv.org/abs/2402.01000)

    本文提出了一种方法，基于低秩加对角线参数化协方差矩阵，可以有效地刻画时间序列预测中误差的自相关性，并具有复杂度低、校准预测准确性高等优点。

    

    建模误差之间的相关性与模型能够准确量化概率时间序列预测中的预测不确定性密切相关。最近的多元模型在考虑误差之间的同时相关性方面取得了显著进展，然而，对于统计简化的目的，对这些误差的常见假设是它们在时间上是独立的。然而，实际观测往往偏离了这个假设，因为误差通常由于各种因素（如排除时间相关的协变量）而表现出显著的自相关性。在这项工作中，我们提出了一种基于低秩加对角线参数化协方差矩阵的高效方法，可以有效地刻画误差的自相关性。所提出的方法具有几个可取的特性：复杂度不随时间序列数目增加，得到的协方差可以用于校准预测，且具有较好的性能。

    Modeling the correlations among errors is closely associated with how accurately the model can quantify predictive uncertainty in probabilistic time series forecasting. Recent multivariate models have made significant progress in accounting for contemporaneous correlations among errors, while a common assumption on these errors is that they are temporally independent for the sake of statistical simplicity. However, real-world observations often deviate from this assumption, since errors usually exhibit substantial autocorrelation due to various factors such as the exclusion of temporally correlated covariates. In this work, we propose an efficient method, based on a low-rank-plus-diagonal parameterization of the covariance matrix, which can effectively characterize the autocorrelation of errors. The proposed method possesses several desirable properties: the complexity does not scale with the number of time series, the resulting covariance can be used for calibrating predictions, and i
    
[^138]: 使用生成对抗网络(GANs)创建虚拟试衣间的一种成本效益方法

    A Cost-Efficient Approach for Creating Virtual Fitting Room using Generative Adversarial Networks (GANs)

    [https://rss.arxiv.org/abs/2402.00994](https://rss.arxiv.org/abs/2402.00994)

    本论文提出了一种使用生成对抗网络(GANs)创建虚拟试衣间的成本效益方法，为客户提供在线试穿衣服的平台，省去了实体店试衣的烦恼和时间，并通过使用特殊镜子减少了实体店拥挤的问题。

    

    全球客户希望在购买前看到衣服是否适合他们。因此，客户本能地更喜欢实体店购物，这样他们可以在购买前试穿产品。但在COVID-19大流行之后，许多商家要么转向在线购物，要么关闭了试衣间，这使购物过程充满了犹豫和怀疑。为了解决购买后可能不适合买家的问题，我们考虑使用新的人工智能技术创建一个在线平台或虚拟试衣间(VFR)，以手机应用程序和部署模型的形式存在，并可以嵌入到任何在线商店中，买家可以在不需要亲自试穿的情况下试穿任意数量的衣服。此外，它将节省大量寻找产品的时间。此外，通过使用一种特殊类型的镜子来应用相同的技术，它还将减少实体店的拥挤和困扰。

    Customers all over the world want to see how the clothes fit them or not before purchasing. Therefore, customers by nature prefer brick-and-mortar clothes shopping so they can try on products before purchasing them. But after the Pandemic of COVID19 many sellers either shifted to online shopping or closed their fitting rooms which made the shopping process hesitant and doubtful. The fact that the clothes may not be suitable for their buyers after purchase led us to think about using new AI technologies to create an online platform or a virtual fitting room (VFR) in the form of a mobile application and a deployed model using a webpage that can be embedded later to any online store where they can try on any number of cloth items without physically trying them. Besides, it will save much searching time for their needs. Furthermore, it will reduce the crowding and headache in the physical shops by applying the same technology using a special type of mirror that will enable customers to try
    
[^139]: 自监督对比预训练在多变量事件流中的应用

    Self-Supervised Contrastive Pre-Training for Multivariate Point Processes

    [https://rss.arxiv.org/abs/2402.00987](https://rss.arxiv.org/abs/2402.00987)

    在多变量事件流中，我们提出了一种新的自监督学习范式，使用变压器编码器进行预训练，并引入对比模块来比较真实事件和模拟的空白实例，以提高后续任务的性能。

    

    自监督学习是表示学习的一个重要特点，在包括BERT和GPT-3等大型语言模型在内的基础模型中越来越受欢迎，但据我们所知，在多变量事件流的背景下尚未被追求。我们引入了一种新的自监督学习范式，使用变压器编码器对多变量事件流进行预训练。具体而言，我们设计了一种新颖的编码器预训练策略，不仅遮盖了随机事件时段，还插入了随机抽样的“空白”时段，即事件不发生的时段；这与BERT中的词遮盖等典型离散时间预训练任务不同，扩展了遮盖的有效性，以更好地捕捉连续时间动态。为了改进下游任务，我们引入了一个对比模块，将真实事件与模拟的空白实例进行比较。预训练模型随后可以在可能更小的事件数据集上进行微调，类似于概念上的提升。

    Self-supervision is one of the hallmarks of representation learning in the increasingly popular suite of foundation models including large language models such as BERT and GPT-3, but it has not been pursued in the context of multivariate event streams, to the best of our knowledge. We introduce a new paradigm for self-supervised learning for multivariate point processes using a transformer encoder. Specifically, we design a novel pre-training strategy for the encoder where we not only mask random event epochs but also insert randomly sampled "void" epochs where an event does not occur; this differs from the typical discrete-time pretext tasks such as word-masking in BERT but expands the effectiveness of masking to better capture continuous-time dynamics. To improve downstream tasks, we introduce a contrasting module that compares real events to simulated void instances. The pre-trained model can subsequently be fine-tuned on a potentially much smaller event dataset, similar conceptuall
    
[^140]: 具有动态停止的循环Transformer

    Recurrent Transformers with Dynamic Halt

    [https://rss.arxiv.org/abs/2402.00976](https://rss.arxiv.org/abs/2402.00976)

    本文研究了增强Transformer与循环机制的两种方法，并提出了新的扩展和组合方法。在多个诊断任务中进行比较，探索它们的归纳偏好。

    

    本文研究了两种主要方法在增强Transformer与循环机制方面的归纳偏好——（1）类似于Universal Transformers的深度逐层循环方法；和（2）类似于Temporal Latent Bottleneck的分块时态循环方法。此外，我们提出并研究了扩展和组合上述方法的新方式，例如，我们提出了一种基于全局均值的Universal Transformer动态停止机制，并将Universal Transformer的元素融入到Temporal Latent Bottleneck中。我们通过多个诊断任务（如Long Range Arena（LRA），翻转-翻转语言建模，ListOps和逻辑推理）比较了模型并探索了它们的归纳偏好。

    In this paper, we study the inductive biases of two major approaches to augmenting Transformers with a recurrent mechanism - (1) the approach of incorporating a depth-wise recurrence similar to Universal Transformers; and (2) the approach of incorporating a chunk-wise temporal recurrence like Temporal Latent Bottleneck. Furthermore, we propose and investigate novel ways to extend and combine the above methods - for example, we propose a global mean-based dynamic halting mechanism for Universal Transformer and an augmentation of Temporal Latent Bottleneck with elements from Universal Transformer. We compare the models and probe their inductive biases in several diagnostic tasks such as Long Range Arena (LRA), flip-flop language modeling, ListOps, and Logical Inference.
    
[^141]: 通过多智能体强化学习识别粗粒度偏微分方程的闭合项

    Closure Discovery for Coarse-Grained Partial Differential Equations using Multi-Agent Reinforcement Learning

    [https://rss.arxiv.org/abs/2402.00972](https://rss.arxiv.org/abs/2402.00972)

    使用多智能体强化学习(MARL)识别未精细解析的PDEs中的闭合项，通过部署中央策略和卷积神经网络(CNN)，能够准确预测和加速模拟。

    

    可靠地预测天气、野火和流行病等关键现象通常基于由偏微分方程(PDEs)描述的模型。然而，捕捉这种PDEs中全面的时空尺度范围的模拟通常是代价高昂的。因此，通常会使用利用启发式方法和经验闭合项的粗粒度模拟作为替代方法。我们提出了一种通过多智能体强化学习(MARL)识别未精细解析的PDEs中闭合项的新颖和系统的方法。MARL的形式化结合了归纳偏差，并利用部署了由卷积神经网络(CNN)高效表示的中央策略来利用局部性。通过对对流方程和Burgers方程的数值解进行演示，我们展示了MARL的能力和限制。我们的结果显示，MARL对于内外分布的测试案例可以准确预测，并且与精细解析相比有显著的加速效果。

    Reliable predictions of critical phenomena, such as weather, wildfires and epidemics are often founded on models described by Partial Differential Equations (PDEs). However, simulations that capture the full range of spatio-temporal scales in such PDEs are often prohibitively expensive. Consequently, coarse-grained simulations that employ heuristics and empirical closure terms are frequently utilized as an alternative. We propose a novel and systematic approach for identifying closures in under-resolved PDEs using Multi-Agent Reinforcement Learning (MARL). The MARL formulation incorporates inductive bias and exploits locality by deploying a central policy represented efficiently by Convolutional Neural Networks (CNN). We demonstrate the capabilities and limitations of MARL through numerical solutions of the advection equation and the Burgers' equation. Our results show accurate predictions for in- and out-of-distribution test cases as well as a significant speedup compared to resolving
    
[^142]: 多模态机器学习框架用于实验室大鼠自动癫痫检测

    Multi-Modal Machine Learning Framework for Automated Seizure Detection in Laboratory Rats

    [https://rss.arxiv.org/abs/2402.00965](https://rss.arxiv.org/abs/2402.00965)

    本文提出了一个多模态机器学习系统，利用多个数据源和类型的数据，通过组合多个模型的结果来改进癫痫检测性能。实验结果表明，使用多个数据源可以过滤和删除假阳性预测，提高癫痫检测的准确性。

    

    多模态机器学习系统利用多个独特的数据源和类型来提高其性能。本文提出了一种系统，它将来自不同数据信号的几种模型的结果结合起来。以一个实验证明该系统的有效性，实验中收集了来自患有癫痫的大鼠的多种类型的数据，包括电子皮质记录、压电运动传感器数据和视频记录。将每种类型的数据分别训练为模型，目标是将每个时间段分类为包含癫痫或不包含癫痫。在每个模型生成分类预测后，将这些结果进行组合。尽管每个数据信号在预测方面都足够好，但类别标签的显著不平衡导致了增加的假阳性数量，而通过利用所有数据源可以对其进行过滤和删除。本文将演示该方法的有效性。

    A multi-modal machine learning system uses multiple unique data sources and types to improve its performance. This article proposes a system that combines results from several types of models, all of which are trained on different data signals. As an example to illustrate the efficacy of the system, an experiment is described in which multiple types of data are collected from rats suffering from seizures. This data includes electrocorticography readings, piezoelectric motion sensor data, and video recordings. Separate models are trained on each type of data, with the goal of classifying each time frame as either containing a seizure or not. After each model has generated its classification predictions, these results are combined. While each data signal works adequately on its own for prediction purposes, the significant imbalance in class labels leads to increased numbers of false positives, which can be filtered and removed by utilizing all data sources. This paper will demonstrate th
    
[^143]: 信任学习理论

    Credal Learning Theory

    [https://rss.arxiv.org/abs/2402.00957](https://rss.arxiv.org/abs/2402.00957)

    本文提出了一种信任学习理论，通过使用凸集的概率来建模数据生成分布的变异性，从有限样本的训练集中推断出信任集，并推导出bounds。

    

    统计学习理论是机器学习的基础，为从未知概率分布中学习到的模型的风险提供理论边界。然而，在实际部署中，数据分布可能会变化，导致领域适应/泛化问题。在本文中，我们建立了一个“信任”学习理论的基础，使用概率的凸集（信任集）来建模数据生成分布的变异性。我们认为，这样的信任集可以从有限样本的训练集中推断出来。对于有限假设空间（无论是否可实现）和无限模型空间，推导出界限，这直接推广了经典结果。

    Statistical learning theory is the foundation of machine learning, providing theoretical bounds for the risk of models learnt from a (single) training set, assumed to issue from an unknown probability distribution. In actual deployment, however, the data distribution may (and often does) vary, causing domain adaptation/generalization issues. In this paper we lay the foundations for a `credal' theory of learning, using convex sets of probabilities (credal sets) to model the variability in the data-generating distribution. Such credal sets, we argue, may be inferred from a finite sample of training sets. Bounds are derived for the case of finite hypotheses spaces (both assuming realizability or not) as well as infinite model spaces, which directly generalize classical results.
    
[^144]: FairEHR-CLP：以对比学习实现的公平感知多模态电子健康记录中的临床预测

    FairEHR-CLP: Towards Fairness-Aware Clinical Predictions with Contrastive Learning in Multimodal Electronic Health Records

    [https://rss.arxiv.org/abs/2402.00955](https://rss.arxiv.org/abs/2402.00955)

    FairEHR-CLP是一个通用框架，利用对比学习，通过生成患者的合成对应来实现多样化的人口统计身份，并利用公平感知预测方法消除EHR中的社会偏见。

    

    在医疗保健领域中，确保预测模型的公平性至关重要。电子健康记录（EHR）已成为医疗决策的重要组成部分，然而现有的增强模型公平性的方法仅限于单模态数据，并未解决EHR中与人口统计因素交织在一起的多方面社会偏见。为了减轻这些偏见，我们提出了FairEHR-CLP：一种公平感知临床预测的通用框架，通过对比学习在EHR中进行操作。FairEHR-CLP通过两个阶段的过程操作，利用患者人口统计数据、纵向数据和临床记录。首先，为每个患者生成合成对应来实现多样化的人口统计身份，同时保留必要的健康信息。其次，公平感知预测利用对比学习将患者的表示在敏感属性上进行对齐，与具有softmax层的MLP分类器共同优化用于临床分类任务。

    In the high-stakes realm of healthcare, ensuring fairness in predictive models is crucial. Electronic Health Records (EHRs) have become integral to medical decision-making, yet existing methods for enhancing model fairness restrict themselves to unimodal data and fail to address the multifaceted social biases intertwined with demographic factors in EHRs. To mitigate these biases, we present FairEHR-CLP: a general framework for Fairness-aware Clinical Predictions with Contrastive Learning in EHRs. FairEHR-CLP operates through a two-stage process, utilizing patient demographics, longitudinal data, and clinical notes. First, synthetic counterparts are generated for each patient, allowing for diverse demographic identities while preserving essential health information. Second, fairness-aware predictions employ contrastive learning to align patient representations across sensitive attributes, jointly optimized with an MLP classifier with a softmax layer for clinical classification tasks. Ac
    
[^145]: 多项式神经网络的几何性质

    Geometry of Polynomial Neural Networks

    [https://rss.arxiv.org/abs/2402.00949](https://rss.arxiv.org/abs/2402.00949)

    本研究利用代数几何工具研究了具有单项式激活函数的多项式神经网络的表达性和学习过程，通过对神经流形的维度和学习度的研究，提供了网络表达能力和训练复杂度的度量，并给出了可学函数数量的上界。

    

    我们研究了具有单项式激活函数的多项式神经网络（PNN）的表达性和学习过程。网络的权重参数化了神经流形。在本文中，我们使用代数几何工具研究了某些神经流形：我们给出了半代数集的明确描述并特征化了它们的Zariski闭包，称为神经多样性。我们研究了它们的维度并将一个代数度量，学习度，与神经多样性相关联。维度作为网络表达能力的几何度量，学习度是训练网络的复杂度度量，并提供可学函数数量的上界。这些理论结果还伴随着实验证明。

    We study the expressivity and learning process for polynomial neural networks (PNNs) with monomial activation functions. The weights of the network parametrize the neuromanifold. In this paper, we study certain neuromanifolds using tools from algebraic geometry: we give explicit descriptions as semialgebraic sets and characterize their Zariski closures, called neurovarieties. We study their dimension and associate an algebraic degree, the learning degree, to the neurovariety. The dimension serves as a geometric measure for the expressivity of the network, the learning degree is a measure for the complexity of training the network and provides upper bounds on the number of learnable functions. These theoretical results are accompanied with experiments.
    
[^146]: 使用窗口过滤的近似最近邻搜索

    Approximate Nearest Neighbor Search with Window Filters

    [https://rss.arxiv.org/abs/2402.00943](https://rss.arxiv.org/abs/2402.00943)

    这篇论文提出了一种使用窗口过滤的近似最近邻搜索方法，能够在各种语义搜索问题中实现高速搜索，并在多个基准数据集上取得了显著的速度提升。

    

    我们定义并研究了$\textit{c-近似窗口搜索}$问题：近似最近邻搜索其中数据集中的每个点都有一个数值标签，目标是在任意标签范围内找到查询点的最近邻。许多语义搜索问题，例如带有时间戳过滤器的图像和文档搜索，或带有成本过滤器的产品搜索，是这个问题的自然例子。我们提出并在理论上分析了一种基于模块化树的框架，用于将解决传统c-近似最近邻问题的索引转化为解决窗口搜索的数据结构。在标准的最近邻基准数据集上，配备了随机标签值、对抗性构建的嵌入以及带有真实时间戳的图像搜索嵌入，我们获得了与现有解决方案相比高达75倍的速度提升，同时保持相同的召回率。

    We define and investigate the problem of $\textit{c-approximate window search}$: approximate nearest neighbor search where each point in the dataset has a numeric label, and the goal is to find nearest neighbors to queries within arbitrary label ranges. Many semantic search problems, such as image and document search with timestamp filters, or product search with cost filters, are natural examples of this problem. We propose and theoretically analyze a modular tree-based framework for transforming an index that solves the traditional c-approximate nearest neighbor problem into a data structure that solves window search. On standard nearest neighbor benchmark datasets equipped with random label values, adversarially constructed embeddings, and image search embeddings with real timestamps, we obtain up to a $75\times$ speedup over existing solutions at the same level of recall.
    
[^147]: 统计和机器学习方法对基因表达谱进行比较分析

    A Comparative Analysis of Gene Expression Profiling by Statistical and Machine Learning Approaches

    [https://rss.arxiv.org/abs/2402.00926](https://rss.arxiv.org/abs/2402.00926)

    这篇论文比较了统计和机器学习方法在基因表达谱的分析中的应用。通过解释性方法得到的基因排名可以帮助理解表型，但存在一些生物学和方法学上的限制。实验证明在癌症分类中，多种机器学习模型都可以用于预测癌症类型。

    

    许多机器学习模型被提出来从基因表达数据中分类表型。除了表现好之外，这些模型还可以通过提取决策的解释来对表型进行一定的理解。这些解释通常以基因按重要性排名的列表形式呈现，排名最高的基因被解释为与表型相关联。我们讨论了这种解释的生物学和方法学限制。在TCGA、GTEx和TARGET数据库中收集的癌症和健康组织样本上进行了实验。使用逻辑回归、多层感知器和图神经网络等多种机器学习模型对样本进行了癌症类型分类。从适应于这些模型的解释性方法中获取基因排名，并与经典统计特征选择方法（如m）进行比较。

    Many machine learning models have been proposed to classify phenotypes from gene expression data. In addition to their good performance, these models can potentially provide some understanding of phenotypes by extracting explanations for their decisions. These explanations often take the form of a list of genes ranked in order of importance for the predictions, the highest-ranked genes being interpreted as linked to the phenotype. We discuss the biological and the methodological limitations of such explanations. Experiments are performed on several datasets gathering cancer and healthy tissue samples from the TCGA, GTEx and TARGET databases. A collection of machine learning models including logistic regression, multilayer perceptron, and graph neural network are trained to classify samples according to their cancer type. Gene rankings are obtained from explainability methods adapted to these models, and compared to the ones from classical statistical feature selection methods such as m
    
[^148]: 深度学习在物联网中的网络流量分类中的应用：一项调研

    Deep Learning Approaches for Network Traffic Classification in the Internet of Things (IoT): A Survey

    [https://rss.arxiv.org/abs/2402.00920](https://rss.arxiv.org/abs/2402.00920)

    本调研论文通过系统分析和分类现有的研究贡献，提供了针对物联网环境中网络流量分类的深度学习方法的全面概述，探讨了各种模型的优势和局限性，为研究人员和从业者提供有价值的指导和参考。

    

    物联网（IoT）目睹了空前的增长，导致来自互联设备的各种网络流量大量涌入。有效地对这些网络流量进行分类对于优化资源分配、增强安全措施和确保物联网系统的高效网络管理至关重要。由于其能够从原始数据中自动学习复杂模式和表示，深度学习已成为网络流量分类的强大技术。本调研论文旨在全面概述针对物联网环境特别定制的网络流量分类中所采用的现有深度学习方法。通过系统分析和分类领域内最新的研究贡献，我们探讨了各种深度学习模型在处理物联网网络流量所带来的独特挑战方面的优势和局限性。通过本调研，我们旨在为研究人员和从业者提供有价值的指导和参考。

    The Internet of Things (IoT) has witnessed unprecedented growth, resulting in a massive influx of diverse network traffic from interconnected devices. Effectively classifying this network traffic is crucial for optimizing resource allocation, enhancing security measures, and ensuring efficient network management in IoT systems. Deep learning has emerged as a powerful technique for network traffic classification due to its ability to automatically learn complex patterns and representations from raw data. This survey paper aims to provide a comprehensive overview of the existing deep learning approaches employed in network traffic classification specifically tailored for IoT environments. By systematically analyzing and categorizing the latest research contributions in this domain, we explore the strengths and limitations of various deep learning models in handling the unique challenges posed by IoT network traffic. Through this survey, we aim to offer researchers and practitioners valua
    
[^149]: 能够约束概念瓶颈模型学习语义上有意义的输入特征吗？

    Can we Constrain Concept Bottleneck Models to Learn Semantically Meaningful Input Features?

    [https://rss.arxiv.org/abs/2402.00912](https://rss.arxiv.org/abs/2402.00912)

    本文研究了概念瓶颈模型如何从具有细粒度概念注释的数据集中学习概念，以实现模型输出的内在可解释性。

    

    概念瓶颈模型（CBM）被认为具有内在的可解释性，因为它们首先预测一组人为定义的概念，然后利用这些概念来预测下游任务的输出。为了实现完全的内在可解释性，以及确保对模型输出的信任，我们需要保证概念的预测是基于语义映射的输入特征。例如，人们可能期望图像中表示骨折的像素被用于预测骨折。然而，当前的文献表明这并不是事实，因为概念预测通常与不相关的输入特征映射在一起。我们假设这是由于概念注释的不准确或者输入特征与概念之间的关系不清晰导致的。总的来说，数据集标注对CBMs中概念表示的影响仍然是一个研究较少的领域。因此，在本文中，我们研究了CBMs如何从具有细粒度概念注释的数据集中学习概念的问题。我们进行了实验演示。

    Concept Bottleneck Models (CBMs) are considered inherently interpretable because they first predict a set of human-defined concepts before using these concepts to predict the output of a downstream task. For inherent interpretability to be fully realised, and ensure trust in a model's output, we need to guarantee concepts are predicted based on semantically mapped input features. For example, one might expect the pixels representing a broken bone in an image to be used for the prediction of a fracture. However, current literature indicates this is not the case, as concept predictions are often mapped to irrelevant input features. We hypothesise that this occurs when concept annotations are inaccurate or how input features should relate to concepts is unclear. In general, the effect of dataset labelling on concept representations in CBMs remains an understudied area. Therefore, in this paper, we examine how CBMs learn concepts from datasets with fine-grained concept annotations. We demo
    
[^150]: 通过集成学习和正则化微调应对偏见

    Addressing Bias Through Ensemble Learning and Regularized Fine-Tuning

    [https://rss.arxiv.org/abs/2402.00910](https://rss.arxiv.org/abs/2402.00910)

    本文提出了一种通过集成学习和正则化微调的方法，解决AI模型中的偏见问题。该方法可通过在小数据集和有偏的预训练模型上训练多个对抗偏见的模型，并使用集成学习得到无偏的预测结果。通过实验证明了该方法在CIFAR10和HAM10000数据集上的有效性。

    

    解决AI模型中的偏见对于确保公平和准确的预测至关重要。然而，获取大规模、无偏的训练数据集可能具有挑战性。本文提出了一种全面的方法，使用多种方法消除AI模型中的偏见，其中仅使用小型数据集和潜在有偏的预训练模型。我们通过数据分离、局部训练和正则化微调训练多个模型，以对抗预训练模型的偏见，得到潜在的对抗偏见模型。然后，我们采用集成学习来对所有模型进行预测，以达到无偏的预测结果。为了进一步加速我们的集成模型的推理时间，我们使用知识蒸馏来获得一个单一无偏的神经网络。通过在CIFAR10和HAM10000数据集上的实验证明了我们方法的有效性，展示了有希望的结果。这项工作为创建更无偏、可靠的AI模型的持续努力做出了贡献。

    Addressing biases in AI models is crucial for ensuring fair and accurate predictions. However, obtaining large, unbiased datasets for training can be challenging. This paper proposes a comprehensive approach using multiple methods to remove bias in AI models, with only a small dataset and a potentially biased pretrained model. We train multiple models with the counter-bias of the pre-trained model through data splitting, local training, and regularized fine-tuning, gaining potentially counter-biased models. Then, we employ ensemble learning for all models to reach unbiased predictions. To further accelerate the inference time of our ensemble model, we conclude our solution with knowledge distillation that results in a single unbiased neural network. We demonstrate the effectiveness of our approach through experiments on the CIFAR10 and HAM10000 datasets, showcasing promising results. This work contributes to the ongoing effort to create more unbiased and reliable AI models, even with l
    
[^151]: AlphaRank:用于排名和选择问题的人工智能方法

    AlphaRank: An Artificial Intelligence Approach for Ranking and Selection Problems

    [https://rss.arxiv.org/abs/2402.00907](https://rss.arxiv.org/abs/2402.00907)

    AlphaRank是一种用于解决固定预算的排名和选择问题的人工智能方法。它通过利用经典的排名和选择程序作为基准策略，以有效学习随机动态规划的价值函数，采用深度强化学习加速在线样本分配，并提出可并行计算的框架用于解决大规模问题。AlphaRank在均值、方差和诱导相关性之间取得了优越的平衡，显著提高了性能。

    

    我们介绍了AlphaRank，一种用于解决固定预算的排名和选择问题的人工智能方法。我们将顺序采样决策定义为马尔可夫决策过程，并提出基于蒙特卡洛仿真的滚动策略，利用经典的排名和选择程序作为基准策略，以有效学习随机动态规划的价值函数。我们通过使用深度强化学习在给定先验的情况下离线预训练神经网络模型，加速在线样本分配。我们还提出了一个可并行计算的框架，用于解决大规模问题，有效地结合了“分而治之”和“递归”，提高了可扩展性和效率。数值实验证明，AlphaRank的性能明显优于基准策略，这可以归因于AlphaRank在众多现有策略中忽视的均值、方差和诱导相关性之间的优越能力。

    We introduce AlphaRank, an artificial intelligence approach to address the fixed-budget ranking and selection (R&S) problems. We formulate the sequential sampling decision as a Markov decision process and propose a Monte Carlo simulation-based rollout policy that utilizes classic R&S procedures as base policies for efficiently learning the value function of stochastic dynamic programming. We accelerate online sample-allocation by using deep reinforcement learning to pre-train a neural network model offline based on a given prior. We also propose a parallelizable computing framework for large-scale problems, effectively combining "divide and conquer" and "recursion" for enhanced scalability and efficiency. Numerical experiments demonstrate that the performance of AlphaRank is significantly improved over the base policies, which could be attributed to AlphaRank's superior capability on the trade-off among mean, variance, and induced correlation overlooked by many existing policies.
    
[^152]: BrainLeaks: 关于神经形态架构对模型反转攻击的隐私保护性质

    BrainLeaks: On the Privacy-Preserving Properties of Neuromorphic Architectures against Model Inversion Attacks

    [https://rss.arxiv.org/abs/2402.00906](https://rss.arxiv.org/abs/2402.00906)

    本研究探索了神经形态架构对模型反转攻击的隐私保护能力，发现脉冲神经网络具有固有的隐私保护性质，并能有效抵抗基于梯度的攻击。

    

    随着机器学习在医疗保健和金融等安全敏感领域的主流整合，对数据隐私的担忧已经加剧。传统的人工神经网络（ANNs）已被发现容易受到多种泄露敏感数据的攻击。特别是，模型反转（MI）攻击可以重构用于训练模型的数据样本。神经形态架构已经成为神经计算的一种范式转变，实现了异步和节能的计算。然而，几乎没有现有的工作研究了神经形态架构对模型反转的隐私性。我们的研究受到的启示是，脉冲神经网络（SNNs）的不可微特性可能导致固有的隐私保护性质，尤其抵抗基于梯度的攻击。为了研究这一假设，我们提出了对SNNs的隐私保护能力进行全面探索。

    With the mainstream integration of machine learning into security-sensitive domains such as healthcare and finance, concerns about data privacy have intensified. Conventional artificial neural networks (ANNs) have been found vulnerable to several attacks that can leak sensitive data. Particularly, model inversion (MI) attacks enable the reconstruction of data samples that have been used to train the model. Neuromorphic architectures have emerged as a paradigm shift in neural computing, enabling asynchronous and energy-efficient computation. However, little to no existing work has investigated the privacy of neuromorphic architectures against model inversion. Our study is motivated by the intuition that the non-differentiable aspect of spiking neural networks (SNNs) might result in inherent privacy-preserving properties, especially against gradient-based attacks. To investigate this hypothesis, we propose a thorough exploration of SNNs' privacy-preserving capabilities. Specifically, we 
    
[^153]: 图领域适应：挑战、进展和前景

    Graph Domain Adaptation: Challenges, Progress and Prospects

    [https://rss.arxiv.org/abs/2402.00904](https://rss.arxiv.org/abs/2402.00904)

    这篇论文综述了图领域适应的研究现状、挑战和前景，提出了一种有效的图之间的知识传递范式，可以增强模型在目标图上的性能。

    

    由于图表示学习在现实世界应用中往往面临标签稀缺的问题，研究人员提出了图领域适应（GDA）作为一种有效的图之间的知识传递范式。特别是为了增强模型在目标图上的性能，GDA引入了一组与任务相关的图作为源图，并将从源图学到的知识适应到目标图上。由于GDA结合了图表示学习和领域适应的优势，它已经成为图上迁移学习的一个有前景的方向，并在近年来引起了越来越多的研究兴趣。在本文中，我们对GDA的研究进行了全面的综述，并详细调查了最近的进展。具体而言，我们概述了研究现状和挑战，提出了一个分类法，介绍了代表性工作的细节，并讨论了前景。据我们所知，这是第一篇对GDA进行全面调查的论文。

    As graph representation learning often suffers from label scarcity problems in real-world applications, researchers have proposed graph domain adaptation (GDA) as an effective knowledge-transfer paradigm across graphs. In particular, to enhance model performance on target graphs with specific tasks, GDA introduces a bunch of task-related graphs as source graphs and adapts the knowledge learnt from source graphs to the target graphs. Since GDA combines the advantages of graph representation learning and domain adaptation, it has become a promising direction of transfer learning on graphs and has attracted an increasing amount of research interest in recent years. In this paper, we comprehensively overview the studies of GDA and present a detailed survey of recent advances. Specifically, we outline the research status and challenges, propose a taxonomy, introduce the details of representative works, and discuss the prospects. To the best of our knowledge, this paper is the first survey f
    
[^154]: 弱监督学习器实现具有可证明性能保证的AI错误修正

    Weakly Supervised Learners for Correction of AI Errors with Provable Performance Guarantees

    [https://rss.arxiv.org/abs/2402.00899](https://rss.arxiv.org/abs/2402.00899)

    这项工作提出了使用具有可证明性能保证的弱监督AI错误修正器来处理AI错误。修正器通过批准或拒绝底层分类器的决策来提升性能，并通过概率界限保证其性能。实验证明该方法在训练数据稀缺的真实世界任务中提升图像分类器的性能。

    

    我们提出了一种新的方法来处理AI错误，通过引入具有先验性能保证的弱监督AI错误修正器。这些AI修正器是辅助映射，其作用是通过批准或拒绝以调节之前构建的底层分类器的决策。拒绝一个决策可以用作建议放弃做出决策的信号。该工作的一个关键技术重点是通过对错误决策的概率界限提供这些新的AI修正器的性能保证。这些界限是分布不可知的，并且不依赖于对数据维度的假设。我们的实证示例说明了该框架如何应用于改善在训练数据稀缺的具有挑战性的真实世界任务中图像分类器的性能。

    We present a new methodology for handling AI errors by introducing weakly supervised AI error correctors with a priori performance guarantees. These AI correctors are auxiliary maps whose role is to moderate the decisions of some previously constructed underlying classifier by either approving or rejecting its decisions. The rejection of a decision can be used as a signal to suggest abstaining from making a decision. A key technical focus of the work is in providing performance guarantees for these new AI correctors through bounds on the probabilities of incorrect decisions. These bounds are distribution agnostic and do not rely on assumptions on the data dimension. Our empirical example illustrates how the framework can be applied to improve the performance of an image classifier in a challenging real-world task where training data are scarce.
    
[^155]: 大规模语言模型上的提示注入攻击的早期分类

    An Early Categorization of Prompt Injection Attacks on Large Language Models

    [https://rss.arxiv.org/abs/2402.00898](https://rss.arxiv.org/abs/2402.00898)

    本文旨在提供对大规模语言模型上的提示注入攻击的早期分类，并讨论了这些攻击对LLM最终用户、开发人员和研究人员的影响。

    

    大规模语言模型和AI聊天机器人一直是民主化人工智能的前沿。然而，ChatGPT和其他类似工具的发布后，人们对于控制大规模语言模型及其输出的难度越来越担忧。目前，我们正在目睹一场捉迷藏的游戏，用户试图利用这些模型进行一种称为提示注入的新型攻击，而开发人员则试图同时发现漏洞并阻止这些攻击。在本文中，我们概述了这些新兴威胁，并提出了提示注入的分类，这可以指导未来在提示注入上的研究，并作为LLM界面开发中漏洞的检查清单。此外，基于先前的文献和我们自己的实证研究，我们讨论了提示注入对LLM最终用户、开发人员和研究人员的影响。

    Large language models and AI chatbots have been at the forefront of democratizing artificial intelligence. However, the releases of ChatGPT and other similar tools have been followed by growing concerns regarding the difficulty of controlling large language models and their outputs. Currently, we are witnessing a cat-and-mouse game where users attempt to misuse the models with a novel attack called prompt injections. In contrast, the developers attempt to discover the vulnerabilities and block the attacks simultaneously. In this paper, we provide an overview of these emergent threats and present a categorization of prompt injections, which can guide future research on prompt injections and act as a checklist of vulnerabilities in the development of LLM interfaces. Moreover, based on previous literature and our own empirical research, we discuss the implications of prompt injections to LLM end users, developers, and researchers.
    
[^156]: 使用声音对象作为声音生物标志物的早期痴呆筛查方法

    Screening method for early dementia using sound objects as voice biomarkers

    [https://rss.arxiv.org/abs/2402.00897](https://rss.arxiv.org/abs/2402.00897)

    本研究提出了一种使用声音对象作为声音生物标志物的早期痴呆筛查方法。该方法基于精心设计的特征，能准确地表示声音谱，并包含与被试者对声音的控制有关的有意义信息。实验结果表明，该方法能够有效区分健康个体与轻度认知障碍或阿尔茨海默病个体。

    

    引言：我们提出了一种使用基于声音对象的特征作为声音生物标志物的早期痴呆筛查方法。方法：用于机器学习模型的最终数据集包括266个观察值，其中分布了186个健康个体，46个被诊断为阿尔茨海默病，34个被诊断为轻度认知障碍。这种方法基于被试者发出的/a/持续元音的六秒录音。本研究的主要原创贡献是使用基于声音对象的精心设计的特征。这种方法首先以比标准频谱更准确的方式表示声音谱，然后构建包含与被试者对声音的控制有关的有意义信息的可解释特征。结果：该方法在将健康个体与轻度认知障碍个体区分的ROC AUC为0.85，准确率为0.76。在将健康个体与轻度认知障碍或阿尔茨海默病个体区分的结果为0.84和0.77。

    Introduction: We present a screening method for early dementia using features based on sound objects as voice biomarkers.   Methods: The final dataset used for machine learning models consisted of 266 observations, with a distribution of 186 healthy individuals, 46 diagnosed with Alzheimer's, and 34 with MCI. This method is based on six-second recordings of the sustained vowel /a/ spoken by the subject. The main original contribution of this work is the use of carefully crafted features based on sound objects. This approach allows one to first represent the sound spectrum in a more accurate way than the standard spectrum, and then build interpretable features containing relevant information about subjects' control over their voice.   Results: ROC AUC obtained in this work for distinguishing healthy subjects from those with MCI was 0.85, while accuracy was 0.76. For distinguishing between healthy subjects and those with either MCI or Alzheimer's the results were 0.84, 0.77, respectively
    
[^157]: 云基础人工智能服务的隐私和安全影响：一项调查

    Privacy and Security Implications of Cloud-Based AI Services : A Survey

    [https://rss.arxiv.org/abs/2402.00896](https://rss.arxiv.org/abs/2402.00896)

    本文调查了云基础人工智能服务的隐私和安全状况，发现机器学习模型引入的风险亟待解决。通过提出分类法来全面研究模型提供者和使用者所面临的风险及其防御手段，有助于创建强大的解决方案。

    

    本文详细介绍了当前云生态系统中的隐私和安全状况，并指出了机器学习模型引入风险的不足之处。随着机器学习算法不断发展并在各个领域应用，对隐私和安全风险进行分类和量化的需求变得越来越关键。随着AI作为服务(AIaaS)的新趋势出现，机器学习AI模型被模型提供者部署在云端，并被模型使用者使用。我们首先对AIaaS领域进行调查，记录了机器学习模型，特别是深度神经网络所引起的各种责任，并引入一个分类法来全面研究创造者和使用者所面临的风险及其已知的防御手段。这种结构化的方法对于机器学习模型提供者创建强大的解决方案将会很有益处。同样，机器学习模型的消费者也会发现它对于评估AI模型的隐私和安全风险很有价值。

    This paper details the privacy and security landscape in today's cloud ecosystem and identifies that there is a gap in addressing the risks introduced by machine learning models. As machine learning algorithms continue to evolve and find applications across diverse domains, the need to categorize and quantify privacy and security risks becomes increasingly critical. With the emerging trend of AI-as-a-Service (AIaaS), machine learned AI models (or ML models) are deployed on the cloud by model providers and used by model consumers. We first survey the AIaaS landscape to document the various kinds of liabilities that ML models, especially Deep Neural Networks pose and then introduce a taxonomy to bridge this gap by holistically examining the risks that creators and consumers of ML models are exposed to and their known defences till date. Such a structured approach will be beneficial for ML model providers to create robust solutions. Likewise, ML model consumers will find it valuable to ev
    
[^158]: MoDE:一种具有专家间相互蒸馏的混合专家模型

    MoDE: A Mixture-of-Experts Model with Mutual Distillation among the Experts

    [https://rss.arxiv.org/abs/2402.00893](https://rss.arxiv.org/abs/2402.00893)

    MoDE是一种在混合专家模型中应用专家间相互蒸馏的方法，以提高模型的泛化能力。实验证明MoDE在各种数据集上都有效，并且具有普适性和鲁棒性。

    

    由于其提高模型性能的能力，混合专家(MoE)的应用越来越受欢迎。在MoE结构中，门控层在区分和路由输入特征到不同的专家方面起着重要作用。这使得每个专家能够专注于处理他们对应的子任务。然而，门控的路由机制也会导致狭窄的视野：单个MoE的专家无法使用更多的样本来学习分配的子任务，这反过来会限制MoE进一步提高其泛化能力。为了有效解决这个问题，我们提出了一种称为Mixture-of-Distilled-Expert (MoDE)的方法，它在专家之间应用适度的相互蒸馏，使每个专家能够学习其他专家学到的更多特征，并对其原始分配的子任务获得更准确的认知。我们进行了大量的实验证明了MoDE的有效性、通用性和稳健性。

    The application of mixture-of-experts (MoE) is gaining popularity due to its ability to improve model's performance. In an MoE structure, the gate layer plays a significant role in distinguishing and routing input features to different experts. This enables each expert to specialize in processing their corresponding sub-tasks. However, the gate's routing mechanism also gives rise to narrow vision: the individual MoE's expert fails to use more samples in learning the allocated sub-task, which in turn limits the MoE to further improve its generalization ability. To effectively address this, we propose a method called Mixture-of-Distilled-Expert (MoDE), which applies moderate mutual distillation among experts to enable each expert to pick up more features learned by other experts and gain more accurate perceptions on their original allocated sub-tasks. We conduct plenty experiments including tabular, NLP and CV datasets, which shows MoDE's effectiveness, universality and robustness. Furth
    
[^159]: EVA-GAN: 通过可扩展的生成对抗网络实现增强的多样化音频生成

    EVA-GAN: Enhanced Various Audio Generation via Scalable Generative Adversarial Networks

    [https://rss.arxiv.org/abs/2402.00892](https://rss.arxiv.org/abs/2402.00892)

    EVA-GAN通过可扩展的生成对抗网络在音频生成领域取得了显著进展，改善了频谱和高频重建以及对域外数据性能的稳健性，实现了高保真音频的生成。

    

    大型模型的出现标志着机器学习的新时代，通过利用庞大的数据集来捕捉和合成复杂的模式，大幅超越较小模型的性能。尽管取得了这些进展，但在音频生成领域，尤其是高保真（HiFi）44.1kHz领域的扩展仍然有限，先前的努力没有延伸到高频域中的频谱不连续性和模糊问题，并且对于域外数据缺乏稳健性。这些限制限制了模型在包括音乐和歌唱生成在内的各种用例中的适用性。我们的工作引入了通过可扩展的生成对抗网络实现的增强音频生成（EVA-GAN），在频谱和高频重建以及域外数据性能的稳健性方面，较之先前最先进的方法取得了显着改进，实现了使用包含36,000个样本的广泛数据集生成HiFi音频。

    The advent of Large Models marks a new era in machine learning, significantly outperforming smaller models by leveraging vast datasets to capture and synthesize complex patterns. Despite these advancements, the exploration into scaling, especially in the audio generation domain, remains limited, with previous efforts didn't extend into the high-fidelity (HiFi) 44.1kHz domain and suffering from both spectral discontinuities and blurriness in the high-frequency domain, alongside a lack of robustness against out-of-domain data. These limitations restrict the applicability of models to diverse use cases, including music and singing generation. Our work introduces Enhanced Various Audio Generation via Scalable Generative Adversarial Networks (EVA-GAN), yields significant improvements over previous state-of-the-art in spectral and high-frequency reconstruction and robustness in out-of-domain data performance, enabling the generation of HiFi audios by employing an extensive dataset of 36,000 
    
[^160]: 大型语言模型在网络安全中的应用：最新研究进展

    Large Language Models in Cybersecurity: State-of-the-Art

    [https://rss.arxiv.org/abs/2402.00891](https://rss.arxiv.org/abs/2402.00891)

    本研究综述了大型语言模型（LLMs）在网络安全领域中的防御和对抗应用，并识别了关键的研究空缺，旨在提供对LLM驱动的网络安全的潜在风险和机会的全面理解。

    

    大型语言模型（LLMs）的出现彻底改变了我们对智能的理解，使我们更接近于人工智能。自从它们被引入以来，研究人员积极探索了LLMs在各个领域的应用，显著提升了能力。然而，在网络安全领域，传统上对数据驱动解决方案持抵触态度且对机器学习采用较慢，这一领域却异军突起。本研究对现有文献进行了研究，全面描述了LLMs在网络安全领域中的防御和对抗应用。我们的综述不仅对当前的研究现状进行了调查和分类，并且还识别出了关键的研究空缺。通过评估攻击和防御应用，我们旨在提供对LLM驱动的网络安全所涉及的潜在风险和机会的全面理解。

    The rise of Large Language Models (LLMs) has revolutionized our comprehension of intelligence bringing us closer to Artificial Intelligence. Since their introduction, researchers have actively explored the applications of LLMs across diverse fields, significantly elevating capabilities. Cybersecurity, traditionally resistant to data-driven solutions and slow to embrace machine learning, stands out as a domain. This study examines the existing literature, providing a thorough characterization of both defensive and adversarial applications of LLMs within the realm of cybersecurity. Our review not only surveys and categorizes the current landscape but also identifies critical research gaps. By evaluating both offensive and defensive applications, we aim to provide a holistic understanding of the potential risks and opportunities associated with LLM-driven cybersecurity.
    
[^161]: 无线网络中的竞争和干扰管理的图表示学习

    Graph Representation Learning for Contention and Interference Management in Wireless Networks

    [https://rss.arxiv.org/abs/2402.00879](https://rss.arxiv.org/abs/2402.00879)

    本研究提出了一种图表示学习方法，通过将用户分组视为图构建问题，利用图的最大切割来进行最优分组决策。演员-评论家图表示学习（AC-GRL）算法被设计用来实现最优图构建，并通过估计最优图的边权重来最大化网络的用户吞吐量。

    

    在Wi-Fi 802.11ah网络中，受限制的接入窗口（RAW）通过将用户分组和分配周期性时间段来管理竞争和干扰。我们将找到RAW中的最优用户分组决策，以最大化网络的最坏情况用户吞吐量。我们回顾现有的用户分组方法，并强调它们在上述问题中的性能限制。我们提出将用户分组视为图构建问题，其中顶点表示用户，边权重表示竞争和干扰。这种形式利用图的最大切割来分组用户，并优化边权重以构建出最佳图，其最大切割产生最优的分组决策。为了实现这种最优图构建，我们设计了一种演员-评论家图表示学习（AC-GRL）算法。具体来说，演员神经网络（NN）被训练来估计最优图的边权重，使用用户和接入点之间的路径损失。

    Restricted access window (RAW) in Wi-Fi 802.11ah networks manages contention and interference by grouping users and allocating periodic time slots for each group's transmissions. We will find the optimal user grouping decisions in RAW to maximize the network's worst-case user throughput. We review existing user grouping approaches and highlight their performance limitations in the above problem. We propose formulating user grouping as a graph construction problem where vertices represent users and edge weights indicate the contention and interference. This formulation leverages the graph's max cut to group users and optimizes edge weights to construct the optimal graph whose max cut yields the optimal grouping decisions. To achieve this optimal graph construction, we design an actor-critic graph representation learning (AC-GRL) algorithm. Specifically, the actor neural network (NN) is trained to estimate the optimal graph's edge weights using path losses between users and access points
    
[^162]: 无方向性发射天线的无线电地图估计——包含数据集和初步实验的开放资源

    Radio Map Estimation -- An Open Dataset with Directive Transmitter Antennas and Initial Experiments

    [https://rss.arxiv.org/abs/2402.00878](https://rss.arxiv.org/abs/2402.00878)

    本文通过发布包含模拟的路径损耗无线电地图、真实城市地图和航拍影像的公开数据集，以及初步实验，填补了开放基准数据集和代码库的空白。

    

    在过去的几年里，一些研究已经探索应用深度学习算法来确定城市通信网络中发射机和接收机之间的大尺度信号衰落（也称为“路径损耗”）。中心思想是通过机器学习模型替代昂贵的测量活动，不准确的统计模型或计算量大的光线追踪模拟，这些模型一旦训练，几乎能即时生成准确的预测。虽然该主题引起了许多研究人员的关注，但缺乏开放的基准数据集和代码库使得每个人都可以测试和比较已开发的方法和算法。我们通过发布一组公开可用的数据集，其中包括模拟的路径损耗无线电地图、真实城市地图和来自开放数据源的航拍影像，来填补这一空白。关于模型架构、输入特征设计和无线电地图估计的初始实验也已进行。

    Over the last years, several works have explored the application of deep learning algorithms to determine the large-scale signal fading (also referred to as ``path loss'') between transmitter and receiver pairs in urban communication networks. The central idea is to replace costly measurement campaigns, inaccurate statistical models or computationally expensive ray-tracing simulations by machine learning models which, once trained, produce accurate predictions almost instantly. Although the topic has attracted attention from many researchers, there are few open benchmark datasets and codebases that would allow everyone to test and compare the developed methods and algorithms. We take a step towards filling this gap by releasing a publicly available dataset of simulated path loss radio maps together with realistic city maps from real-world locations and aerial images from open datasources. Initial experiments regarding model architectures, input feature design and estimation of radio ma
    
[^163]: 将稀疏微调扩展到大型语言模型

    Scaling Sparse Fine-Tuning to Large Language Models

    [https://rss.arxiv.org/abs/2401.16405](https://rss.arxiv.org/abs/2401.16405)

    本研究将稀疏微调方法扩展到大型语言模型，提出了一种新的稀疏微调算法SpIEL，并对LLM进行了指令微调，以解决其参数庞大的问题。

    

    大型语言模型（LLM）由于其参数的庞大数量，很难完全进行微调（例如使用指令或人工反馈）。一系列参数高效的稀疏微调方法在性能方面表现出了很大的潜力，但它们的存储需求与LLM的大小成正比增加。在这项工作中，我们将稀疏微调扩展到最先进的LLM，如LLaMA 2 7B和13B。我们提出了SpIEL，一种新颖的稀疏微调方法，它针对所需的稀疏度水平，维护一个参数索引数组和这些参数相对于预训练值的增量。它遍历以下步骤：（a）更新活跃增量，（b）修剪索引（基于其增量的变化大小），以及（c）重新生长索引。对于重新生长，我们探索了基于少量候选参数的累积梯度或使用高效的SM3优化器估计的近似动差的两个准则。我们尝试使用指令微调LLM。

    Large Language Models (LLMs) are difficult to fully fine-tune (e.g., with instructions or human feedback) due to their sheer number of parameters. A family of parameter-efficient sparse fine-tuning methods have proven promising in terms of performance but their memory requirements increase proportionally to the size of the LLMs. In this work, we scale sparse fine-tuning to state-of-the-art LLMs like LLaMA 2 7B and 13B. We propose SpIEL, a novel sparse fine-tuning method which, for a desired density level, maintains an array of parameter indices and the deltas of these parameters relative to their pretrained values. It iterates over: (a) updating the active deltas, (b) pruning indices (based on the change of magnitude of their deltas) and (c) regrowth of indices. For regrowth, we explore two criteria based on either the accumulated gradients of a few candidate parameters or their approximate momenta estimated using the efficient SM3 optimizer. We experiment with instruction-tuning of LL
    
[^164]: 关于LM潜在空间的语义学：一种以词汇为定义的方法

    On the Semantics of LM Latent Space: A Vocabulary-defined Approach

    [https://rss.arxiv.org/abs/2401.16184](https://rss.arxiv.org/abs/2401.16184)

    本论文介绍了一种以词汇为定义的语义学方法，建立了LM潜在空间的参考框架，确保基于LM词汇的分离语义分析。在LM适应过程中，引入了计算logits的新技术和神经聚类模块，通过实验证明了该方法在文本理解上的优越性能。

    

    理解语言模型(LM)的潜在空间对于改进其性能和可解释性至关重要。现有的分析往往在提供基于模型的对LM语义的分离洞察方面存在不足，并忽视了LM适应的重要方面。为了响应这一问题，我们引入了一种开创性的方法，称为以词汇为定义的语义学，它在LM的潜在空间中建立了一个参考框架，确保基于LM词汇的分离语义分析。我们的方法超越了先前的交织分析，利用LM词汇来获得以模型为中心的洞察。此外，我们提出了一种计算logits的新技术，强调可微分性和局部等距性，并引入了一个神经聚类模块，用于在LM适应过程中进行语义校准。通过在多种文本理解数据集上进行广泛实验，我们的方法在检索增强生成和参数高效微调方面超越了最先进的方法。

    Understanding the latent space of language models (LM) is crucial to refining their performance and interpretability. Existing analyses often fall short in providing disentangled (model-centric) insights into LM semantics, and neglect essential aspects of LM adaption. In response, we introduce a pioneering method called vocabulary-defined semantics, which establishes a reference frame within the LM latent space, ensuring disentangled semantic analysis grounded in LM vocabulary. Our approach transcends prior entangled analysis, leveraging LM vocabulary for model-centric insights. Furthermore, we propose a novel technique to compute logits, emphasising differentiability and local isotropy, and introduce a neural clustering module for semantically calibrating data representations during LM adaptation. Through extensive experiments across diverse text understanding datasets, our approach outperforms state-of-the-art methods of retrieval-augmented generation and parameter-efficient finetuni
    
[^165]: NoFunEval: 有趣的是，代码语言模型在超出功能正确性的要求上遇到困难

    NoFunEval: Funny How Code LMs Falter on Requirements Beyond Functional Correctness

    [https://rss.arxiv.org/abs/2401.15963](https://rss.arxiv.org/abs/2401.15963)

    本论文提出了一个新的基准测试 NoFunEval，用于评估代码语言模型在非功能性要求和简单分类实例方面的表现。研究发现，目前的代码语言模型在处理这些要求时存在根本性的盲点。

    

    现有的代码语言模型（code LMs）的评估基准几乎完全集中在LMs是否能够生成功能正确的代码上。在实际的软件工程中，开发人员会考虑超出功能正确性的要求。他们对于“如何”实现功能有着对整体系统设计目标（如效率、安全性和可维护性）的要求。如果LMs能够展示对要求和代码语义的强大理解能力，他们也会更加信任这些LMs。我们提出了一个新的基准测试NoFunEval来评估代码LMs在非功能性要求和简单分类实例方面的表现。我们提出了一个提示方法Coding Concepts (CoCo)，可以用于开发人员向LMs传达领域知识。我们对22个代码LMs进行了广泛评估，发现它们在我们的基准测试中普遍表现不佳，暗示着它们在处理这些问题时存在根本性的盲点。

    Existing evaluation benchmarks of language models of code (code LMs) focus almost exclusively on whether the LMs can generate functionally-correct code. In real-world software engineering, developers think beyond functional correctness. They have requirements on "how" a functionality should be implemented to meet overall system design objectives like efficiency, security, and maintainability. They would also trust the code LMs more if the LMs demonstrate robust understanding of requirements and code semantics.   We propose a new benchmark NoFunEval to evaluate code LMs on non-functional requirements and simple classification instances for both functional and non-functional requirements. We propose a prompting method, Coding Concepts (CoCo), as a way for a developer to communicate the domain knowledge to the LMs. We conduct an extensive evaluation of twenty-two code LMs. Our finding is that they generally falter when tested on our benchmark, hinting at fundamental blindspots in their tr
    
[^166]: 解读文本的真实性: 通过大规模语言语义的广义策略来检测人类和机器生成的文本

    Deciphering Textual Authenticity: A Generalized Strategy through the Lens of Large Language Semantics for Detecting Human vs. Machine-Generated Text

    [https://rss.arxiv.org/abs/2401.09407](https://rss.arxiv.org/abs/2401.09407)

    该论文通过研究在真实世界场景中检测机器生成文本的方法，发现现有方法对于不同生成器和领域产生的文本存在严重限制。

    

    随着大规模语言模型（LLM）的广泛应用，对检测机器生成文本的工具的需求日益增长。有效检测机器生成文本面临两个关键问题: 首先，他们在应对真实世界场景时面临着极大的限制，这些场景中机器生成文本是由各种生成器产生的，包括但不限于GPT-4和Dolly，并涵盖各种领域，从学术手稿到社交媒体帖子。其次，现有的检测方法将LLM生成的文本视为严格的二元分类问题，忽略了不同LLM生成的文本多样性。本研究系统地研究了在真实世界场景中检测机器生成文本的方法。我们首先研究了最先进方法的有效性，并发现它们在应对真实世界中不同生成器和领域产生的文本时受到严重的限制。

    With the recent proliferation of Large Language Models (LLMs), there has been an increasing demand for tools to detect machine-generated text. The effective detection of machine-generated text face two pertinent problems: First, they are severely limited in generalizing against real-world scenarios, where machine-generated text is produced by a variety of generators, including but not limited to GPT-4 and Dolly, and spans diverse domains, ranging from academic manuscripts to social media posts. Second, existing detection methodologies treat texts produced by LLMs through a restrictive binary classification lens, neglecting the nuanced diversity of artifacts generated by different LLMs. In this work, we undertake a systematic study on the detection of machine-generated text in real-world scenarios. We first study the effectiveness of state-of-the-art approaches and find that they are severely limited against text produced by diverse generators and domains in the real world. Furthermore,
    
[^167]: 修复-Con：深度学习模型转换的自动故障定位和修复

    Fix-Con: Automatic Fault Localization and Repair of Deep Learning Model Conversions

    [https://rss.arxiv.org/abs/2312.15101](https://rss.arxiv.org/abs/2312.15101)

    本文提出了一种自动化的故障定位和修复方法Fix-Con，用于在深度学习模型转换过程中修复由转换引入的故障。Fix-Con能够检测和修复模型输入、参数、超参数和模型图方面的故障，提高转换模型的部署和预测正确性。

    

    在不同深度学习框架之间进行模型转换是一种常见的步骤，可以最大程度地增加模型在设备之间的兼容性，并利用可能只在一个深度学习框架中提供的优化功能。然而，这个转换过程可能存在错误，导致转换后的模型无法部署或存在问题，严重降低了其预测的正确性。我们提出了一种自动化的故障定位和修复方法，Fix-Con，在深度学习框架之间进行模型转换时使用。Fix-Con能够检测和修复在转换过程中引入的模型输入、参数、超参数和模型图的故障。Fix-Con使用从调查转换问题中挖掘出的一组故障类型来定位转换模型中潜在的转换故障，并适当修复它们，例如使用源模型的参数替换目标模型的参数。这一过程在数据集中的每个图像上进行迭代执行。

    Converting deep learning models between frameworks is a common step to maximize model compatibility across devices and leverage optimization features that may be exclusively provided in one deep learning framework. However, this conversion process may be riddled with bugs, making the converted models either undeployable or problematic, considerably degrading their prediction correctness.   We propose an automated approach for fault localization and repair, Fix-Con, during model conversion between deep learning frameworks. Fix-Con is capable of detecting and fixing faults introduced in model input, parameters, hyperparameters, and the model graph during conversion.   Fix-Con uses a set of fault types mined from surveying conversion issues raised to localize potential conversion faults in the converted target model, and then repairs them appropriately, e.g. replacing the parameters of the target model with those from the source model. This is done iteratively for every image in the datas
    
[^168]: 在线变分顺序蒙特卡洛方法

    Online Variational Sequential Monte Carlo

    [https://rss.arxiv.org/abs/2312.12616](https://rss.arxiv.org/abs/2312.12616)

    本文提出了一种在线学习的算法，名为在线VSMC，它基于变分顺序蒙特卡洛方法，在处理数据流时能够实时进行模型参数估计和粒子提议适应。

    

    状态空间模型（SSM）是AI和统计机器学习中最经典的生成模型，对于任何形式的参数学习或潜在状态推断，通常需要计算复杂的潜在状态后验分布。本文在变分顺序蒙特卡洛（VSMC）方法的基础上进行了研究，该方法通过结合粒子方法和变分推断，提供了计算高效且准确的模型参数估计和贝叶斯潜在状态推断。传统的VSMC方法在离线模式下运行，通过重复处理给定的数据批次，而我们使用随机逼近方法将VSMC代理ELBO的梯度逼近分布到时间上，从而实现了在数据流存在的情况下的在线学习。这导致了一种名为在线VSMC的算法，能够高效地进行参数估计和粒子提议适应，而且完全实时处理数据。

    Being the most classical generative model for serial data, state-space models (SSM) are fundamental in AI and statistical machine learning. In SSM, any form of parameter learning or latent state inference typically involves the computation of complex latent-state posteriors. In this work, we build upon the variational sequential Monte Carlo (VSMC) method, which provides computationally efficient and accurate model parameter estimation and Bayesian latent-state inference by combining particle methods and variational inference. While standard VSMC operates in the offline mode, by re-processing repeatedly a given batch of data, we distribute the approximation of the gradient of the VSMC surrogate ELBO in time using stochastic approximation, allowing for online learning in the presence of streams of data. This results in an algorithm, online VSMC, that is capable of performing efficiently, entirely on-the-fly, both parameter estimation and particle proposal adaptation. In addition, we prov
    
[^169]: DIRECT: 处理不平衡和标签噪声下的深度主动学习

    DIRECT: Deep Active Learning under Imbalance and Label Noise

    [https://rss.arxiv.org/abs/2312.09196](https://rss.arxiv.org/abs/2312.09196)

    这篇论文提出了一种名为DIRECT的算法，用于处理不平衡和标签噪声下的深度主动学习问题。通过确定类别分割阈值并标记最不确定且离其最近的示例，该算法能够有效解决罕见类和少数类的性能问题，并具有批次标记和对标签噪声的容忍能力。

    

    在现实世界的机器学习应用中，类别不平衡是一个普遍存在的问题，通常会导致罕见和少数类的性能较差。在大量未标记数据的情况下，主动学习可能是解决该问题的最有效技术，它从根本上采集更平衡和具有信息量的标记示例进行注释。标签噪声是数据注释任务中另一个常见问题，对于主动学习方法来说尤其具有挑战性。本文首次研究了在类别不平衡和标签噪声下的主动学习。我们提出了一种新颖的算法，能够稳健地确定类别分割阈值并标记最不确定且离其最近的示例。通过将问题简化为一维主动学习，我们的算法DIRECT能够利用经典的主动学习文献来解决批次标记和对标签噪声的容忍等问题。我们在大量实验中展示了算法的性能。

    Class imbalance is a prevalent issue in real world machine learning applications, often leading to poor performance in rare and minority classes. With an abundance of wild unlabeled data, active learning is perhaps the most effective technique in solving the problem at its root -- collecting a more balanced and informative set of labeled examples during annotation. Label noise is another common issue in data annotation jobs, which is especially challenging for active learning methods. In this work, we conduct the first study of active learning under both class imbalance and label noise. We propose a novel algorithm that robustly identifies the class separation threshold and annotates the most uncertain examples that are closest from it. Through a novel reduction to one-dimensional active learning, our algorithm DIRECT is able to leverage the classic active learning literature to address issues such as batch labeling and tolerance towards label noise. We present extensive experiments on
    
[^170]: 跨块量化：用于大型语言模型的跨块量化方法

    CBQ: Cross-Block Quantization for Large Language Models

    [https://rss.arxiv.org/abs/2312.07950](https://rss.arxiv.org/abs/2312.07950)

    CBQ是一种用于大型语言模型的跨块重构型后训练量化方法。CBQ通过使用同源重构方案来建立块间的长程依赖关系，最小化误差积累。CBQ还采用了粗到精的预处理策略和自适应的取整技术，使其能够有效处理极端异常值并提高整体量化精度。

    

    后训练量化（PTQ）在以极低成本压缩大型语言模型（LLM）方面起着重要作用。然而，现有的PTQ方法只关注处理单个层或单个块内的异常值，忽略了块之间的依赖关系，在低位设置中导致严重的性能下降。本文提出了一种基于块间重构的跨块PTQ方法CBQ。CBQ采用了一种同源重构方案来实现块间的长程依赖关系，以最小化误差积累。此外，CBQ还结合了一种粗到精的预处理策略（CFP）来抑制权重和激活值的异常值，并配合一种自适应的LoRA取整技术实现精确的权重量化。这些创新使CBQ不仅能够有效处理极端异常值，还能提高整体量化精度。广泛的实验证明，CBQ在低位量化（W4A4，W4A8等）方面具有优越性能。

    Post-training quantization (PTQ) has played a key role in compressing large language models (LLMs) with ultra-low costs. However, existing PTQ methods only focus on handling the outliers within one layer or one block, which ignores the dependency of blocks and leads to severe performance degradation in low-bit settings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ method for LLMs. CBQ employs a cross-block dependency using a homologous reconstruction scheme, establishing long-range dependencies across multiple blocks to minimize error accumulation. Furthermore, CBQ incorporates a coarse-to-fine preprocessing (CFP) strategy for suppressing weight and activation outliers, coupled with an adaptive LoRA-Rounding technique for precise weight quantization. These innovations enable CBQ to not only handle extreme outliers effectively but also improve overall quantization accuracy. Extensive experiments show that CBQ achieves superior low-bit quantization (W4A4, W4A8, W
    
[^171]: "为什么“经典”的Transformer模型是肤浅的以及如何使它们变得更深入"

    Why "classic" Transformers are shallow and how to make them go deep

    [https://rss.arxiv.org/abs/2312.06182](https://rss.arxiv.org/abs/2312.06182)

    这篇论文研究了Transformer模型的深度问题，指出这个问题是由于“token相似性升级”导致的，提供了理论和实证调查的证据。

    

    自其在2017年的引入以来，Transformer已成为领先的神经网络架构，在许多人工智能领域实现了革命性的进展。Transformer的关键创新是自注意力（SA）机制，旨在捕捉上下文信息。然而，将原始的Transformer设计扩展为更深层次的模型已被证明极具挑战性，甚至无法实现。尽管已提出各种修改来将更多层的SA机制堆叠到更深层次的模型中，但对于这个深度问题的完全理解仍然缺乏。在本文中，我们进行了全面的理论和实证调查，证实了深度问题是由于“token相似性升级”引起的；也就是说，在重复应用SA机制后，token逐渐变得越来越相似。我们的分析揭示了，受到注意力矩阵不变的特征空间和大的频谱间隙的驱动，token的相似性逐渐增加。

    Since its introduction in 2017, Transformer has emerged as the leading neural network architecture, catalyzing revolutionary advancements in many AI disciplines. The key innovation in Transformer is a Self-Attention (SA) mechanism designed to capture contextual information. However, extending the original Transformer design to models of greater depth has proven exceedingly challenging, if not impossible. Even though various modifications have been proposed in order to stack more layers of SA mechanism into deeper models, a full understanding of this depth problem remains lacking. In this paper, we conduct a comprehensive investigation, both theoretically and empirically, to substantiate the claim that the depth problem is caused by \emph{token similarity escalation}; that is, tokens grow increasingly alike after repeated applications of the SA mechanism. Our analysis reveals that, driven by the invariant leading eigenspace and large spectral gaps of attention matrices, token similarity
    
[^172]: Neuron Patching: 神经元层面的模型编辑与代码生成

    Neuron Patching: Neuron-level Model Editing on Code Generation and LLMs

    [https://rss.arxiv.org/abs/2312.05356](https://rss.arxiv.org/abs/2312.05356)

    这项工作介绍了一种神经元层面的模型编辑方法，能够在编码任务中修补LLM模型，并且在API序列推荐、代码生成和伪代码到代码转换等任务中得到了验证和评估。

    

    大型语言模型在软件工程中得到了成功应用，特别是在代码生成方面。更新这些模型的新知识非常昂贵，通常需要全面实现其价值。在本文中，我们提出了一种新颖有效的模型编辑方法MENT，用于在编码任务中修补LLM模型。基于生成式LLM的机制，MENT可以在预测下一个令牌时进行模型编辑，并进一步支持常见的编码任务。MENT具有高效、有效和可靠的特点。它可以通过修补1或2个神经元来纠正神经模型。作为神经元层面上生成模型编辑的先驱工作，我们规范了编辑过程并介绍了相关概念。此外，我们还引入了新的衡量方法来评估其泛化能力，并建立了一个用于进一步研究的基准。我们的方法在三个编码任务上进行了评估，包括API序列推荐、行级代码生成和伪代码到代码转换。

    Large Language Models are successfully adopted in software engineering, especially in code generation. Updating these models with new knowledge is very expensive, and is often required to fully realize their value. In this paper, we propose a novel and effective model editing approach, \textsc{MENT}, to patch LLMs in coding tasks. Based on the mechanism of generative LLMs, \textsc{MENT} enables model editing in next-token predictions, and further supports common coding tasks. \textsc{MENT} is effective, efficient, and reliable. It can correct a neural model by patching 1 or 2 neurons. As the pioneer work on neuron-level model editing of generative models, we formalize the editing process and introduce the involved concepts. Besides, we also introduce new measures to evaluate its generalization ability, and build a benchmark for further study. Our approach is evaluated on three coding tasks, including API-seq recommendation, line-level code generation, and pseudocode-to-code transaction
    
[^173]: 在图上的大型语言模型：一项全面调查

    Large Language Models on Graphs: A Comprehensive Survey

    [https://rss.arxiv.org/abs/2312.02783](https://rss.arxiv.org/abs/2312.02783)

    这篇论文对在图上的大型语言模型进行了全面调查，研究了纯图形、文本属性图形和文本配对图形三个不同场景下的应用情况，并探讨了基于图形的推理能力是否可以推广到大型语言模型上。

    

    大型语言模型（LLMs），如GPT4和LLaMA，由于其强大的文本编码/解码能力和新发现的紧急能力（例如推理）在自然语言处理方面取得了显著的进展。虽然LLMs主要设计用于处理纯文本，但在许多现实场景中，文本数据与图形形式的丰富结构信息相关联（例如学术网络和电子商务网络），或者图形数据与丰富的文本信息配对（例如带有描述的分子）。此外，尽管LLMs已经展示了其基于纯文本的推理能力，但尚未探索此类能力是否可以推广到图形上（即基于图形的推理）。在本文中，我们对在图上的大型语言模型相关场景和技术进行了系统回顾。我们首先总结了采用LLMs在图形上的潜在场景，分为纯图形、文本属性图形和文本配对图形三个类别。

    Large language models (LLMs), such as GPT4 and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning). While LLMs are mainly designed to process pure texts, there are many real-world scenarios where text data is associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data is paired with rich textual information (e.g., molecules with descriptions). Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graphs (i.e., graph-based reasoning). In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs. We first summarize potential scenarios of adopting LLMs on graphs into three categories, namely pure graphs, text-attributed graphs, and text-pa
    
[^174]: 边际拉普拉斯分数

    Marginal Laplacian Score

    [https://rss.arxiv.org/abs/2311.17795](https://rss.arxiv.org/abs/2311.17795)

    边际拉普拉斯分数（MLS）是一种针对高维度不平衡数据的改进版拉普拉斯分数（LS），通过保留数据集边缘的局部结构，提供了一种有效的无监督特征选择方法。该方法被成功地集成到不同iable无监督特征选择（DUFS）算法中，在合成和公共数据集上展示了稳健且改进的性能。

    

    高维度不平衡数据给机器学习带来了挑战。在没有足够或高质量的标签的情况下，无监督特征选择方法对后续算法的成功至关重要。因此，我们引入了一种边际拉普拉斯分数（MLS），它是著名的拉普拉斯分数（LS）的修改版本，旨在更好地处理不平衡数据。我们假设少数类或异常类在特征的边缘中更频繁出现。因此，MLS旨在保留数据集边缘的局部结构。我们将其集成到利用拉普拉斯分数的现代特征选择方法中。我们将MLS算法集成到可微无监督特征选择（DUFS）中，得到了DUFS-MLS。所提出的方法在合成和公共数据集上展示了稳健且改进的性能。

    High-dimensional imbalanced data poses a machine learning challenge. In the absence of sufficient or high-quality labels, unsupervised feature selection methods are crucial for the success of subsequent algorithms. Therefore, we introduce a Marginal Laplacian Score (MLS), a modification of the well known Laplacian Score (LS) tailored to better address imbalanced data. We introduce an assumption that the minority class or anomalous appear more frequently in the margin of the features. Consequently, MLS aims to preserve the local structure of the dataset's margin. We propose its integration into modern feature selection methods that utilize the Laplacian score. We integrate the MLS algorithm into the Differentiable Unsupervised Feature Selection (DUFS), resulting in DUFS-MLS. The proposed methods demonstrate robust and improved performance on synthetic and public datasets.
    
[^175]: 可解释行为表示的多意图逆Q学习

    Multi-intention Inverse Q-learning for Interpretable Behavior Representation

    [https://rss.arxiv.org/abs/2311.13870](https://rss.arxiv.org/abs/2311.13870)

    本研究引入了一种多意图逆Q学习算法，用于解决在推断离散时变奖励时的挑战。通过聚类观察到的专家轨迹并独立解决每个意图的逆强化学习问题，我们的方法在动物行为预测方面超越了当前的基准，产生了可解释的奖励函数。

    

    在推动决策过程理解方面，逆强化学习（IRL）在重构动物复杂行为中的多个意图方面证明了其重要性。鉴于最近发展的连续时间多意图IRL框架，人们一直在研究如何使用IRL推断离散的时变奖励。为了解决这个挑战，我们引入了潜（马尔科夫）变量逆Q学习（L(M)V-IQL），这是一种专门用于适应离散内在奖励函数的IRL算法。通过利用期望最大化方法，我们将观察到的专家轨迹聚类成不同的意图，并为每个意图独立解决IRL问题。通过模拟实验和对不同真实鼠类行为数据集的应用，我们的方法在动物行为预测方面超越了当前的基准，产生了可解释的奖励函数。这一进展有望打开推动科学与工程应用的新机遇。

    In advancing the understanding of decision-making processes, Inverse Reinforcement Learning (IRL) have proven instrumental in reconstructing animal's multiple intentions amidst complex behaviors. Given the recent development of a continuous-time multi-intention IRL framework, there has been persistent inquiry into inferring discrete time-varying rewards with IRL. To tackle the challenge, we introduce Latent (Markov) Variable Inverse Q-learning (L(M)V-IQL), a novel class of IRL algorthms tailored for accommodating discrete intrinsic reward functions. Leveraging an Expectation-Maximization approach, we cluster observed expert trajectories into distinct intentions and independently solve the IRL problem for each. Demonstrating the efficacy of L(M)V-IQL through simulated experiments and its application to different real mouse behavior datasets, our approach surpasses current benchmarks in animal behavior prediction, producing interpretable reward functions. This advancement holds promise f
    
[^176]: 正则流是否是解锁指数机制的关键？经过准确性和隐私双重约束的差分隐私机器学习的一条路径

    Are Normalizing Flows the Key to Unlocking the Exponential Mechanism? A Path through the Accuracy-Privacy Ceiling Constraining Differentially Private ML

    [https://rss.arxiv.org/abs/2311.09200](https://rss.arxiv.org/abs/2311.09200)

    通过使用正则流模型，解决了差分隐私机器学习中历史难题，提高了准确性和隐私保护。

    

    当前的差分隐私机器学习（ML）的最先进且事实标准是差分隐私随机梯度下降（DPSGD）。然而，这种方法本质上是浪费的。通过向每个梯度添加噪声，它会在每个梯度步骤中降低整体隐私。尽管经过15年的丰富研究，推进了组合定理、子采样方法和实现技术，但当前的隐私机器学习方法往往无法达到足够的准确性和隐私保护。与此同时，为了私下优化而设计的指数机制（ExpM）历来被排除在现代机器学习算法的私下训练之外，主要是因为ExpM需要从一种历来难以处理的密度中进行采样。尽管最近发现了正则流模型（NFs），这是一种用于逼近难以处理分布的表达深度网络，但ExpM仍然处于背景中。我们的观点是利用正则流来绕过ExpM的历史障碍是一个潜在的方法。

    The state of the art and de facto standard for differentially private machine learning (ML) is differentially private stochastic gradient descent (DPSGD). Yet, the method is inherently wasteful. By adding noise to every gradient, it diminishes the overall privacy with every gradient step. Despite 15 years of fruitful research advancing the composition theorems, sub-sampling methods, and implementation techniques, adequate accuracy and privacy is often unattainable with current private ML methods. Meanwhile, the Exponential Mechanism (ExpM), designed for private optimization, has been historically sidelined from privately training modern ML algorithms primarily because ExpM requires sampling from a historically intractable density. Despite the recent discovery of Normalizing Flow models (NFs), expressive deep networks for approximating intractable distributions, ExpM remains in the background. Our position is that leveraging NFs to circumvent historic obstructions of ExpM is a potential
    
[^177]: 通过鲁棒性视角理解Grokking

    Understanding Grokking Through A Robustness Viewpoint

    [https://rss.arxiv.org/abs/2311.06597](https://rss.arxiv.org/abs/2311.06597)

    通过神经网络的鲁棒性视角，我们发现$l_2$权重范数是Grokking的充分条件，并提出基于扰动的方法加速泛化，在模数相加数据集上发现标准训练过程在Grokking之前几乎没有学习其他基本群操作，而使用我们方法时加速泛化可以通过学习交换律来解释。

    

    最近，一个有趣的现象被称为Grokking引起了很多关注，它指的是在模型最初过拟合训练数据后很久才出现泛化。我们试图通过神经网络的鲁棒性来理解这个看似奇怪的现象。从鲁棒性的角度来看，我们证明了神经网络中流行的$l_2$权重范数（度量）实际上是Grokking的一个充分条件。基于之前的观察，我们提出了基于扰动的方法来加速泛化过程。此外，我们还在模数相加数据集上考察了标准训练过程，并发现在Grokking之前，它几乎没有学习其他基本的群操作，例如，交换律。有趣的是，当使用我们提出的方法时，泛化加速可以通过学习交换律来解释，这是模型在测试数据集上Grokking时的必要条件。我们还经验性地发现$l_2$范数与Grokking在...

    Recently, an interesting phenomenon called grokking has gained much attention, where generalization occurs long after the models have initially overfitted the training data. We try to understand this seemingly strange phenomenon through the robustness of the neural network. From a robustness perspective, we show that the popular $l_2$ weight norm (metric) of the neural network is actually a sufficient condition for grokking. Based on the previous observations, we propose perturbation-based methods to speed up the generalization process. In addition, we examine the standard training process on the modulo addition dataset and find that it hardly learns other basic group operations before grokking, for example, the commutative law. Interestingly, the speed-up of generalization when using our proposed method can be explained by learning the commutative law, a necessary condition when the model groks on the test dataset. We also empirically find that $l_2$ norm correlates with grokking on t
    
[^178]: 跨越维度：高维控制器的可信达性

    Bridging Dimensions: Confident Reachability for High-Dimensional Controllers

    [https://rss.arxiv.org/abs/2311.04843](https://rss.arxiv.org/abs/2311.04843)

    本文介绍了一种连接高维控制器的详尽闭环验证方法，通过将高维控制器的行为近似为不同状态空间区域内的低维控制器，平衡了逼近精度和可验证性。

    

    自主系统越来越多地使用端到端学习的控制器进行实现。这样的控制器做出的决策通过图像作为主要感知模式在真实系统上执行。深度神经网络是这种控制器的基本构建模块。然而，现有的神经网络验证工具在处理具有数千个维度的输入时无法扩展，特别是当各个输入（如像素）缺乏明确的物理意义时。本文在连接详尽的闭环验证与高维控制器方面迈出了一步。我们的关键洞见是，高维控制器的行为可以用不同状态空间区域内的几个低维控制器来近似。为了平衡低维控制器的逼近精度和可验证性，我们利用了最新的验证感知知识蒸馏。然后，如果低维可达性结果已经得到了收敛。

    Autonomous systems are increasingly implemented using end-to-end learning-based controllers. Such controllers make decisions that are executed on the real system with images as one of the primary sensing modalities. Deep neural networks form a fundamental building block of such controllers. Unfortunately, the existing neural-network verification tools do not scale to inputs with thousands of dimensions -- especially when the individual inputs (such as pixels) are devoid of clear physical meaning. This paper takes a step towards connecting exhaustive closed-loop verification with high-dimensional controllers. Our key insight is that the behavior of a high-dimensional controller can be approximated with several low-dimensional controllers in different regions of the state space. To balance the approximation accuracy and verifiability of our low-dimensional controllers, we leverage the latest verification-aware knowledge distillation. Then, if low-dimensional reachability results are infl
    
[^179]: 信号处理与SGD相遇：从动量到滤波

    Signal Processing Meets SGD: From Momentum to Filter

    [https://rss.arxiv.org/abs/2311.02818](https://rss.arxiv.org/abs/2311.02818)

    该论文提出了一种名为SGDF的优化方法，通过应用维纳滤波理论和引入时变自适应权重，加速了SGD的收敛速度，同时保持了泛化能力。实验证明，与其他优化器相比，SGDF在收敛和泛化之间取得了平衡。

    

    在深度学习中，随机梯度下降（SGD）及其基于动量的变种广泛应用于优化算法，它们通常面临收敛速度慢的问题。同时，现有的自适应学习率优化器加速收敛，但常常以泛化能力为代价。我们证明了自适应学习率属性会损害泛化能力。为了解决这一矛盾，我们提出了一种新的优化方法，旨在加速SGD的收敛速度，保持泛化能力不变。该方法基于减小历史梯度的方差的思想，通过应用维纳滤波理论增强SGD的一阶矩估计，并引入一个时变自适应权重。实验结果表明，与最先进的优化器相比，SGDF在收敛和泛化之间找到了一个平衡。

    In deep learning, stochastic gradient descent (SGD) and its momentum-based variants are widely used in optimization algorithms, they usually face the problem of slow convergence. Meanwhile, existing adaptive learning rate optimizers accelerate convergence but often at the expense of generalization ability. We demonstrate that the adaptive learning rate property impairs generalization. To address this contradiction, we propose a novel optimization method that aims to accelerate the convergence rate of SGD without loss of generalization. This approach is based on the idea of reducing the variance of the historical gradient, enhancing the first-order moment estimation of the SGD by applying Wiener filtering theory, and introducing a time-varying adaptive weight. Experimental results show that SGDF achieves a trade-off between convergence and generalization compared to state-of-the-art optimizers.
    
[^180]: 基于前向$\chi^2$散度的变分重要抽样方法

    Forward $\chi^2$ Divergence Based Variational Importance Sampling

    [https://rss.arxiv.org/abs/2311.02516](https://rss.arxiv.org/abs/2311.02516)

    引入了一种基于前向$\chi^2$散度的变分重要抽样方法(VIS)，通过直接估计和最大化对数似然来增强对复杂后验分布的估计性能。实验证明，VIS方法在多种潜变量模型中均优于最先进的基线方法，表现出更高的对数似然和模型参数估计准确性。

    

    最大化对数似然是学习潜变量模型的关键方面，而变分推断（VI）是目前常用的方法。然而，当处理复杂的后验分布时，VI在实现高对数似然方面可能遇到挑战。针对这个限制，我们引入了一种新颖的变分重要抽样（VIS）方法，直接估计和最大化对数似然。VIS利用通过最小化前向$\chi^2$散度实现的最佳提议分布来增强对数似然估计。我们将VIS应用于多种流行的潜变量模型，包括混合模型、变分自编码器和部分观测广义线性模型。结果表明，我们的方法在对数似然和模型参数估计方面始终优于最先进的基线方法。

    Maximizing the log-likelihood is a crucial aspect of learning latent variable models, and variational inference (VI) stands as the commonly adopted method. However, VI can encounter challenges in achieving a high log-likelihood when dealing with complicated posterior distributions. In response to this limitation, we introduce a novel variational importance sampling (VIS) approach that directly estimates and maximizes the log-likelihood. VIS leverages the optimal proposal distribution, achieved by minimizing the forward $\chi^2$ divergence, to enhance log-likelihood estimation. We apply VIS to various popular latent variable models, including mixture models, variational auto-encoders, and partially observable generalized linear models. Results demonstrate that our approach consistently outperforms state-of-the-art baselines, both in terms of log-likelihood and model parameter estimation.
    
[^181]: Brenier最优输运映射下的非线性滤波

    Nonlinear Filtering with Brenier Optimal Transport Maps

    [https://rss.arxiv.org/abs/2310.13886](https://rss.arxiv.org/abs/2310.13886)

    本文提出了基于Brenier最优输运映射的非线性滤波方法，通过估计先验分布到后验分布的映射来避免权重退化问题，并利用神经网络建模复杂分布和随机优化算法提高可扩展性。

    

    本论文研究非线性滤波问题，即在给定噪声部分观测历史的情况下计算随机动态系统状态的条件分布。传统的序列重要重采样（SIR）粒子滤波由于权重退化问题，在涉及退化似然或高维状态的情况下存在基本限制。在本文中，我们探索了一种基于估计从当前先验分布到下一个时间步的后验分布的Brenier最优输运（OT）映射的替代方法。与SIR粒子滤波不同，OT方法不需要似然的解析形式。此外，它允许我们利用神经网络的逼近能力来建模复杂的多模态分布，并使用随机优化算法来提高可扩展性。我们进行了大量的数字实验，比较了OT滤波器和SIR粒子滤波器的性能。

    This paper is concerned with the problem of nonlinear filtering, i.e., computing the conditional distribution of the state of a stochastic dynamical system given a history of noisy partial observations. Conventional sequential importance resampling (SIR) particle filters suffer from fundamental limitations, in scenarios involving degenerate likelihoods or high-dimensional states, due to the weight degeneracy issue. In this paper, we explore an alternative method, which is based on estimating the Brenier optimal transport (OT) map from the current prior distribution of the state to the posterior distribution at the next time step. Unlike SIR particle filters, the OT formulation does not require the analytical form of the likelihood. Moreover, it allows us to harness the approximation power of neural networks to model complex and multi-modal distributions and employ stochastic optimization algorithms to enhance scalability. Extensive numerical experiments are presented that compare the O
    
[^182]: 基础模型的嵌入表示能够检测到分布偏移

    Foundation Model's Embedded Representations May Detect Distribution Shift

    [https://rss.arxiv.org/abs/2310.13836](https://rss.arxiv.org/abs/2310.13836)

    该研究发现基础模型的嵌入表示可以检测到数据集之间的分布偏移，且在传统的泛化度量上表现出偏差。预训练的GPT-2模型的特征学习无法在特定任务上提升性能，而对其表示进行线性探测可能优于整体微调。

    

    采样偏差可能导致监督学习任务的训练和测试数据集之间发生分布偏移，使我们难以理解模型的泛化能力。鉴于广泛采用已预训练的基础神经网络作为迁移学习任务的工具，这一点尤为重要，而这些基础神经网络的行为仍然不太清楚。我们以Sentiment140数据集上的迁移学习为例进行了案例研究，并展示了许多预训练的基础模型对Sentiment140的手动标注的测试集M和自动标注的训练集P具有不同的表示，证实了发生了分布偏移。我们认为在P上训练并在M上评估性能是一种有偏差的泛化度量。对预训练的GPT-2进行的实验表明，从P中可学得的特征不会改善（事实上会阻碍）在M上的性能。对预训练的GPT-2的表示进行线性探测是鲁棒的，甚至可能胜过整体微调。

    Sampling biases can cause distribution shifts between train and test datasets for supervised learning tasks, obscuring our ability to understand the generalization capacity of a model. This is especially important considering the wide adoption of pre-trained foundational neural networks -- whose behavior remains poorly understood -- for transfer learning (TL) tasks. We present a case study for TL on the Sentiment140 dataset and show that many pre-trained foundation models encode different representations of Sentiment140's manually curated test set $M$ from the automatically labeled training set $P$, confirming that a distribution shift has occurred. We argue training on $P$ and measuring performance on $M$ is a biased measure of generalization. Experiments on pre-trained GPT-2 show that the features learnable from $P$ do not improve (and in fact hamper) performance on $M$. Linear probes on pre-trained GPT-2's representations are robust and may even outperform overall fine-tuning, imply
    
[^183]: 大规模语言模型中出现的欺骗能力

    Deception Abilities Emerged in Large Language Models

    [https://rss.arxiv.org/abs/2307.16513](https://rss.arxiv.org/abs/2307.16513)

    大规模语言模型（LLM）如GPT-4具备了理解和诱导他人产生错误信念的能力，并且在复杂的欺骗场景中可以通过链式思维推理得到增强。

    

    大规模语言模型（LLM）目前处于将人工智能系统与人类交流和日常生活紧密结合的前沿。因此，将它们与人类价值观保持一致非常重要。然而，由于推理能力的稳定增长，未来的LLM被怀疑能够欺骗人类操作员，并利用这种能力绕过监测工作。为此，LLM需要具备对欺骗策略的概念理解。本研究揭示了最先进的LLM（如GPT-4）中出现了这种策略，而在早期的LLM中并不存在。我们进行了一系列实验，表明最先进的LLM能够理解和诱导他人产生错误的信念，其在复杂的欺骗场景中表现可以通过链式思维推理得到增强，并且引发LLM中的马基雅维利主义可以改变其欺骗倾向。总之，揭示了迄今为止未知的欺骗能力。

    Large language models (LLMs) are currently at the forefront of intertwining artificial intelligence (AI) systems with human communication and everyday life. Thus, aligning them with human values is of great importance. However, given the steady increase in reasoning abilities, future LLMs are under suspicion of becoming able to deceive human operators and utilizing this ability to bypass monitoring efforts. As a prerequisite to this, LLMs need to possess a conceptual understanding of deception strategies. This study reveals that such strategies emerged in state-of-the-art LLMs, such as GPT-4, but were non-existent in earlier LLMs. We conduct a series of experiments showing that state-of-the-art LLMs are able to understand and induce false beliefs in other agents, that their performance in complex deception scenarios can be amplified utilizing chain-of-thought reasoning, and that eliciting Machiavellianism in LLMs can alter their propensity to deceive. In sum, revealing hitherto unknown
    
[^184]: 变分线性化Laplace近似在贝叶斯深度学习中的应用

    Variational Linearized Laplace Approximation for Bayesian Deep Learning

    [https://rss.arxiv.org/abs/2302.12565](https://rss.arxiv.org/abs/2302.12565)

    本论文提出了一种基于变分稀疏高斯过程的方法，用于近似线性化Laplace近似在贝叶斯深度学习中的应用。该方法保留了原始DNN的预测均值，并具有高效的随机优化，训练成本与训练点的数量无关。

    

    最近，线性化Laplace近似（LLA）被用来对预训练深度神经网络（DNNs）的预测进行不确定性估计。然而，在训练点或DNN参数较多的情况下，其广泛应用受到了计算成本的限制。因此，其他LLA的近似方法，如Kronecker分解或对角线GGN矩阵的近似，被使用，可能会影响模型的性能。为了解决这些挑战，我们提出了一种基于变分稀疏高斯过程（GP）的LLA近似方法。我们的方法基于GP的对偶RKHS公式，并保留了原始DNN的预测均值。此外，它允许有效的随机优化，从而在训练数据集的大小中实现子线性训练时间。具体而言，其训练成本与训练点的数量无关。我们比较了我们的方法与其他近似方法的性能，并展示了其在准确性和计算效率方面的优势。

    The Linearized Laplace Approximation (LLA) has been recently used to perform uncertainty estimation on the predictions of pre-trained deep neural networks (DNNs). However, its widespread application is hindered by significant computational costs, particularly in scenarios with a large number of training points or DNN parameters. Consequently, additional approximations of LLA, such as Kronecker-factored or diagonal approximate GGN matrices, are utilized, potentially compromising the model's performance. To address these challenges, we propose a new method for approximating LLA using a variational sparse Gaussian Process (GP). Our method is based on the dual RKHS formulation of GPs and retains as the predictive mean the output of the original DNN. Furthermore, it allows for efficient stochastic optimization, which results in sub-linear training time in the size of the training dataset. Specifically, its training cost is independent of the number of training points. We compare our propose
    
[^185]: 实时学习跨层级皮层层次的高效反向投影

    Learning efficient backprojections across cortical hierarchies in real time

    [https://rss.arxiv.org/abs/2212.10249](https://rss.arxiv.org/abs/2212.10249)

    本论文介绍了一种称为无相位对齐学习的生物合理方法，可以在分层皮层层次中高效地学习反馈权重，通过利用生物物理系统中的噪声作为额外的信息载体，并同时学习所有权重。这种方法实现了高效的错误传播，并保持生物合理的信号传递和学习。

    

    大脑皮层的感知处理和学习模型需要高效地为所有区域的突触分配信用。在深度学习中，已知的解决方案是误差反向传播，然而这需要从前馈到反馈路径进行生物上不可行的权重传输。我们介绍了无相位对齐学习（Phaseless Alignment Learning, PAL），一种在分层皮层层次中学习高效反馈权重的生物合理方法。这是通过利用生物物理系统中自然存在的噪声作为额外的信息载体来实现的。在我们的动态系统中，所有的权重都是同时学习的，并且仅使用突触本地可用的信息，始终开启可塑性。我们的方法完全不涉及相位（没有前向和后向传递或相位学习），可以在多层皮层层次中实现高效的错误传播，同时保持生物合理的信号传递和学习。我们的方法适用于广泛的模型类别，并且可以实现实时的高效计算。

    Models of sensory processing and learning in the cortex need to efficiently assign credit to synapses in all areas. In deep learning, a known solution is error backpropagation, which however requires biologically implausible weight transport from feed-forward to feedback paths.   We introduce Phaseless Alignment Learning (PAL), a bio-plausible method to learn efficient feedback weights in layered cortical hierarchies. This is achieved by exploiting the noise naturally found in biophysical systems as an additional carrier of information. In our dynamical system, all weights are learned simultaneously with always-on plasticity and using only information locally available to the synapses. Our method is completely phase-free (no forward and backward passes or phased learning) and allows for efficient error propagation across multi-layer cortical hierarchies, while maintaining biologically plausible signal transport and learning.   Our method is applicable to a wide class of models and impr
    
[^186]: 生成对抗学习Sinkhorn算法初始化

    Generative Adversarial Learning of Sinkhorn Algorithm Initializations

    [https://rss.arxiv.org/abs/2212.00133](https://rss.arxiv.org/abs/2212.00133)

    本文通过生成对抗学习的方法，训练神经网络来学习Sinkhorn算法的初始化，显著加快收敛速度，同时保持算法的可微分性和并行性，并证明了网络的普适性和独立求解能力。

    

    Sinkhorn算法是近似求解离散概率分布之间熵正则输运（OT）距离的最先进方法。我们展示了通过训练神经网络来学习算法初始化，可以显著加快收敛速度，同时保持Sinkhorn算法的可微分性和并行性。我们通过对抗训练的方式使用第二个生成网络和自监督引导损失来训练我们的预测网络。预测网络具有普适性，能够推广到任意固定维度和成本的概率分布对，并且我们证明生成网络可以在训练过程中产生任意概率分布对。此外，我们还展示了我们的网络可以作为独立的OT求解器来近似正则化输运问题。

    The Sinkhorn algorithm is the state-of-the-art to approximate solutions of entropic optimal transport (OT) distances between discrete probability distributions. We show that meticulously training a neural network to learn initializations to the algorithm via the entropic OT dual problem can significantly speed up convergence, while maintaining desirable properties of the Sinkhorn algorithm, such as differentiability and parallelizability. We train our predictive network in an adversarial fashion using a second, generating network and a self-supervised bootstrapping loss. The predictive network is universal in the sense that it is able to generalize to any pair of distributions of fixed dimension and cost at inference, and we prove that we can make the generating network universal in the sense that it is capable of producing any pair of distributions during training. Furthermore, we show that our network can even be used as a standalone OT solver to approximate regularized transport dis
    
[^187]: 基于图案的时间序列对抗性解释

    Motif-guided Time Series Counterfactual Explanations

    [https://rss.arxiv.org/abs/2211.04411](https://rss.arxiv.org/abs/2211.04411)

    本论文提出了一种基于图案的时间序列对抗性解释方法（MG-CF），通过利用重要的图案来生成直观的解释信息，以提高时间序列模型的信任和透明度。

    

    随着对可解释的机器学习方法的需求增加，有必要增加人类工作量来提供模型决策的不同影响因素的多样化解释。为了提高AI系统的信任和透明性，可解释的人工智能（XAI）领域应运而生。XAI范式分为两大类：特征归因和对抗性解释方法。特征归因方法基于解释模型决策的原因，而对抗性解释方法发现最小的输入变化将导致不同的决策。本文旨在通过使用图案来生成对抗性解释，从而在时间序列模型中建立信任和透明度。我们提出了一种新颖的模型Motif-Guided Counterfactual Explanation（MG-CF），它生成直观的事后对抗性解释，充分利用重要的图案提供决策的解释信息。

    With the rising need of interpretable machine learning methods, there is a necessity for a rise in human effort to provide diverse explanations of the influencing factors of the model decisions. To improve the trust and transparency of AI-based systems, the EXplainable Artificial Intelligence (XAI) field has emerged. The XAI paradigm is bifurcated into two main categories: feature attribution and counterfactual explanation methods. While feature attribution methods are based on explaining the reason behind a model decision, counterfactual explanation methods discover the smallest input changes that will result in a different decision. In this paper, we aim at building trust and transparency in time series models by using motifs to generate counterfactual explanations. We propose Motif-Guided Counterfactual Explanation (MG-CF), a novel model that generates intuitive post-hoc counterfactual explanations that make full use of important motifs to provide interpretive information in decisio
    
[^188]: 增强商业流程模拟模型的外部活动延迟

    Enhancing Business Process Simulation Models with Extraneous Activity Delays

    [https://rss.arxiv.org/abs/2206.14051](https://rss.arxiv.org/abs/2206.14051)

    本文研究了如何增强商业流程模拟模型，通过自动发现和捕获外部活动延迟，提高模拟准确性和效率。

    

    商业流程模拟(BPS)是一种常见的方法，用于估计对业务流程进行更改对其性能指标的影响。例如，通过BPS，我们可以估计如果自动化业务流程中的某个活动或某些资源不可用，该流程的周期时间将是多少。BPS的起点是用模拟参数注释的业务流程模型(BPS模型)。在传统方法中，BPS模型是由建模专家手动设计的。这种方法耗时且容易出错。为了解决这个缺点，一些研究提出了通过过程挖掘技术从事件日志自动发现BPS模型的方法。然而，目前这个领域的技术只发现捕捉由资源争用或资源不可用引起的等待时间的BPS模型。在许多情况下，业务流程中相当一部分的等待时间对应于外部延迟，例如，资源等待客户返回。

    Business Process Simulation (BPS) is a common approach to estimate the impact of changes to a business process on its performance measures. For example, it allows us to estimate what would be the cycle time of a process if we automated one of its activities, or if some resources become unavailable. The starting point of BPS is a business process model annotated with simulation parameters (a BPS model). In traditional approaches, BPS models are manually designed by modeling specialists. This approach is time-consuming and error-prone. To address this shortcoming, several studies have proposed methods to automatically discover BPS models from event logs via process mining techniques. However, current techniques in this space discover BPS models that only capture waiting times caused by resource contention or resource unavailability. Oftentimes, a considerable portion of the waiting time in a business process corresponds to extraneous delays, e.g., a resource waits for the customer to ret
    
[^189]: 统计学习视角下的简单克里金方法

    A Statistical Learning View of Simple Kriging

    [https://rss.arxiv.org/abs/2202.07365](https://rss.arxiv.org/abs/2202.07365)

    本文从统计学习的视角对简单克里金方法进行了分析，解决了在大数据时代中，考虑复杂空间相关结构的大规模数据集预测问题。

    

    在大数据时代，特别是由于地理定位传感器的普及，出现了越来越多展示可能具有复杂空间相关结构的大规模数据集。在这种情况下，标准的统计学习的概率理论不直接适用，并且从这些数据中学习的预测规则的泛化能力的保证有待建立。本文从统计学习的视角对简单克里金方法进行了分析，即通过进行非参数有限样本的预测分析。给定$d\geq 1$个由未知协方差结构的平方可积随机场$X=\{X_s\}_{s\in S}$在$S\subset \mathbb{R}^2$中的$s_1,\; \ldots,\; s_d$处的实现值，目标是用最小二次风险预测它在$S$中的任何其他位置$s\in S$上的未知值。预测规则是从训练空间数据集导出的：来自独立于待预测位置的实现$X'$。

    In the Big Data era, with the ubiquity of geolocation sensors in particular, massive datasets exhibiting a possibly complex spatial dependence structure are becoming increasingly available. In this context, the standard probabilistic theory of statistical learning does not apply directly and guarantees of the generalization capacity of predictive rules learned from such data are left to establish. We analyze here the simple Kriging task from a statistical learning perspective, i.e. by carrying out a nonparametric finite-sample predictive analysis. Given $d\geq 1$ values taken by a realization of a square integrable random field $X=\{X_s\}_{s\in S}$, $S\subset \mathbb{R}^2$, with unknown covariance structure, at sites $s_1,\; \ldots,\; s_d$ in $S$, the goal is to predict the unknown values it takes at any other location $s\in S$ with minimum quadratic risk. The prediction rule being derived from a training spatial dataset: a single realization $X'$ of $X$, independent from those to be p
    
[^190]: 使用Sinkhorn散度的分布式强化学习

    Distributional Reinforcement Learning by Sinkhorn Divergence

    [https://rss.arxiv.org/abs/2202.00769](https://rss.arxiv.org/abs/2202.00769)

    本文提出了SinkhornDRL方法，使用Sinkhorn散度来减小当前和目标Bellman回报分布之间的差异，并通过理论证明和实证实验展示了该方法的优越性。

    

    分布式强化学习的实证成功高度依赖于分布表示和分布散度的选择。本文提出了Sinkhorn分布强化学习 (SinkhornDRL)的方法，该方法从回报分布中学习无限制的统计量，并利用Sinkhorn散度来减小当前和目标Bellman回报分布之间的差异。从理论上来讲，我们证明了SinkhornDRL的收缩性质，与Sinkhorn散度在Wasserstein距离和最大均值差异 (MMD)之间的插值性质一致。我们还建立了Sinkhorn散度与带有正则化Moment Matching行为的正则化MMD之间的等价关系，从而解释了SinkhornDRL的优越性。在实证上，我们展示了SinkhornDRL在Atari游戏套件上始终表现比现有算法更好或可比。

    The empirical success of distributional reinforcement learning~(RL) highly depends on the distribution representation and the choice of distribution divergence. In this paper, we propose \textit{Sinkhorn distributional RL~(SinkhornDRL)} that learns unrestricted statistics from return distributions and leverages Sinkhorn divergence to minimize the difference between current and target Bellman return distributions. Theoretically, we prove the contraction properties of SinkhornDRL, consistent with the interpolation nature of Sinkhorn divergence between Wasserstein distance and Maximum Mean Discrepancy~(MMD). We also establish the equivalence between Sinkhorn divergence and a regularized MMD with a regularized Moment Matching behavior, contributing to explaining the superiority of SinkhornDRL. Empirically, we show that SinkhornDRL is consistently better or comparable to existing algorithms on the Atari games suite.
    
[^191]: 分类分布式的好处：在强化学习中的不确定性感知的正则化探索

    The Benefits of Being Categorical Distributional: Uncertainty-aware Regularized Exploration in Reinforcement Learning

    [https://rss.arxiv.org/abs/2110.03155](https://rss.arxiv.org/abs/2110.03155)

    通过应用返回概率函数分解技术，我们在分类分布式强化学习中探索了分布匹配正则化的潜力，通过隐式地优化策略以对齐目标回报分布的不确定性来产生不确定性感知的探索效果。

    

    尽管分类分布式强化学习（RL）在经验上表现出色，但其相对于传统RL的理论优势仍不明确。从分类分布式RL（CDRL）开始，我们通过应用返回概率函数分解技术，将分布式RL的潜在优势归因于派生的分布匹配正则化。这种在分布式RL环境中未被探索的正则化旨在捕捉额外的回报分布信息，而不仅仅是其期望值，从而为策略优化中的增强奖励信号作出贡献。与明确优化策略以鼓励探索的最大熵RL中的熵正则化相比，CDRL中的结果正则化通过新的奖励信号隐式地优化策略，使其与目标回报分布的不确定性相一致，从而产生了不确定性感知的探索效果。

    The theoretical advantages of distributional reinforcement learning~(RL) over classical RL remain elusive despite its remarkable empirical performance. Starting from Categorical Distributional RL~(CDRL), we attribute the potential superiority of distributional RL to a derived distribution-matching regularization by applying a return density function decomposition technique. This unexplored regularization in the distributional RL context is aimed at capturing additional return distribution information regardless of only its expectation, contributing to an augmented reward signal in the policy optimization. Compared with the entropy regularization in MaxEnt RL that explicitly optimizes the policy to encourage the exploration, the resulting regularization in CDRL implicitly optimizes policies guided by the new reward signal to align with the uncertainty of target return distributions, leading to an uncertainty-aware exploration effect. Finally, extensive experiments substantiate the impor
    
[^192]: 预测中缺失数据的简单填补规则：理论保证与实证性能的对比

    Simple Imputation Rules for Prediction with Missing Data: Contrasting Theoretical Guarantees with Empirical Performance

    [https://rss.arxiv.org/abs/2104.03158](https://rss.arxiv.org/abs/2104.03158)

    本研究对缺失数据问题进行了研究，通过对比理论和实证结果，展示了一些简单填补规则在预测任务中的性能。在广泛填补方法家族中，我们发现均值填补是最优的，而众数填补是次优的。实证结果除了支持理论发现外，还强调了理论和实践之间的差距和未来研究的机会。

    

    缺失数据是实际数据集中常见的问题。本文通过对比理论和实证证据来研究填补-回归流程的性能。我们建立了适用于广泛填补方法家族的渐进一致性。尽管常识认为“好”的填补方法生成的数据集是合理的，但是我们发现，在预测方面，粗糙数据也可能是好的。我们发现其中一些结论是，众数填补在渐进上是次优的，而均值填补在渐进上是最优的。然后我们在大量的合成、半真实和真实数据集上详尽评估了这些理论结论的有效性。尽管我们收集的实证证据大多支持我们的理论发现，但也强调了理论与实践之间的差距和未来研究的机会，以解决MAR假设的相关性，填补和回归任务之间的复杂相互依赖的问题。

    Missing data is a common issue in real-world datasets. This paper studies the performance of impute-then-regress pipelines by contrasting theoretical and empirical evidence. We establish the asymptotic consistency of such pipelines for a broad family of imputation methods. While common sense suggests that a `good' imputation method produces datasets that are plausible, we show, on the contrary, that, as far as prediction is concerned, crude can be good. Among others, we find that mode-impute is asymptotically sub-optimal, while mean-impute is asymptotically optimal. We then exhaustively assess the validity of these theoretical conclusions on a large corpus of synthetic, semi-real, and real datasets. While the empirical evidence we collect mostly supports our theoretical findings, it also highlights gaps between theory and practice and opportunities for future research, regarding the relevance of the MAR assumption, the complex interdependency between the imputation and regression tasks
    
[^193]: CroissantLLM: 一个真正的双语法语-英语语言模型

    CroissantLLM: A Truly Bilingual French-English Language Model

    [https://arxiv.org/abs/2402.00786](https://arxiv.org/abs/2402.00786)

    CroissantLLM是一个1.3B的双语语言模型，通过使用1:1的英语-法语预训练数据比例、自定义的分词器和双语调优数据集进行训练，实现了高性能和开源。模型还发布了训练数据集和多个检查点，以及一个法语基准测试 FrenchBench。

    

    我们介绍了CroissantLLM，这是一个在3T个英语和法语标记上预训练的13亿语言模型，为研究和工业社区带来了一种高性能的、完全开源的双语模型，可以在消费级本地硬件上快速运行。为此，我们首次尝试使用1:1的英语-法语预训练数据比例、自定义的分词器和双语调优数据集来训练一种内在双语的模型。我们发布了训练数据集，其中包含了一个法语分割，其中包含了手工策划、高质量和多样化的数据源。为了评估在英语以外的性能，我们创建了一个新的基准测试 FrenchBench，包括一系列分类和生成任务，涵盖了模型在法语语言中性能的各个方面。此外，为了保持透明度并促进进一步的大规模语言模型研究，我们发布了代码库和各种模型规模、训练数据分布上的几十个检查点。

    We introduce CroissantLLM, a 1.3B language model pretrained on a set of 3T English and French tokens, to bring to the research and industrial community a high-performance, fully open-sourced bilingual model that runs swiftly on consumer-grade local hardware. To that end, we pioneer the approach of training an intrinsically bilingual model with a 1:1 English-to-French pretraining data ratio, a custom tokenizer, and bilingual finetuning datasets. We release the training dataset, notably containing a French split with manually curated, high-quality, and varied data sources. To assess performance outside of English, we craft a novel benchmark, FrenchBench, consisting of an array of classification and generation tasks, covering various orthogonal aspects of model performance in the French Language. Additionally, rooted in transparency and to foster further Large Language Model research, we release codebases, and dozens of checkpoints across various model sizes, training data distributions, 
    
[^194]: 图像到图像生成模型的机器遗忘

    Machine Unlearning for Image-to-Image Generative Models

    [https://arxiv.org/abs/2402.00351](https://arxiv.org/abs/2402.00351)

    本文提出了一种适用于图像到图像生成模型的机器遗忘方法，该方法通过提供一个统一的框架和一个高效的算法，实现在遗忘样本中删除信息且在保留样本上性能几乎没有下降。实证研究证明该方法不依赖于保留样本的可用性，符合数据保留政策。

    

    机器遗忘已经成为一种新的范式，用于从给定模型中有意地遗忘数据样本以符合严格的规定。然而，现有的机器遗忘方法主要集中在分类模型上，对于生成模型的遗忘领域相对未被探索。本文作为一座桥梁，通过提供一个统一的框架来探讨图像到图像生成模型的机器遗忘问题。在该框架下，我们提出了一种计算效率高的算法，通过严格的理论分析证明在保留样本上性能下降可忽略，同时有效地从遗忘样本中删除信息。在两个大规模数据集ImageNet-1K和Places-365上的实证研究进一步表明，我们的算法不依赖于保留样本的可用性，进一步符合数据保留政策。据我们所知，这项工作是第一个进行图像到图像生成模型遗忘研究的工作。

    Machine unlearning has emerged as a new paradigm to deliberately forget data samples from a given model in order to adhere to stringent regulations. However, existing machine unlearning methods have been primarily focused on classification models, leaving the landscape of unlearning for generative models relatively unexplored. This paper serves as a bridge, addressing the gap by providing a unifying framework of machine unlearning for image-to-image generative models. Within this framework, we propose a computationally-efficient algorithm, underpinned by rigorous theoretical analysis, that demonstrates negligible performance degradation on the retain samples, while effectively removing the information from the forget samples. Empirical studies on two large-scale datasets, ImageNet-1K and Places-365, further show that our algorithm does not rely on the availability of the retain samples, which further complies with data retention policy. To our best knowledge, this work is the first tha
    
[^195]: 模拟数字调度对于联邦学习的通信高效方法

    Analog-digital Scheduling for Federated Learning: A Communication-Efficient Approach

    [https://arxiv.org/abs/2402.00318](https://arxiv.org/abs/2402.00318)

    本文提出一种模拟数字FL方案，通过在每一轮中，通过模拟OTA方案上传梯度或者通过正交RB传输量化梯度的方式调度设备，解决了联邦学习中性能受限于信噪比最差设备问题的差异，以实现通信高效和降低噪声。

    

    最近，通过无线网络进行机器学习模型训练的通信高效的联邦学习（FL）范式中出现了一种称为OTA计算的方法。然而，其性能受到信噪比最差的设备的限制，导致更新速度快但噪声较多。另一方面，通过数字通道将正交资源块（RB）分配给每个设备可以减轻噪声问题，但会增加通信延迟。在本文中，我们解决了这个差异，并提出了ADFL，一种新颖的模拟数字FL方案：在每一轮中，参数服务器（PS）将每个设备调度为通过模拟OTA方案上传其梯度，或者使用“数字”方案通过正交RB传输其量化梯度。针对单个FL轮次，我们将最优调度问题转化为最小化PS估计的全局梯度的均方误差（MSE）的问题，并在延迟约束下得到最优设备调度配置。

    Over-the-air (OTA) computation has recently emerged as a communication-efficient Federated Learning (FL) paradigm to train machine learning models over wireless networks. However, its performance is limited by the device with the worst SNR, resulting in fast yet noisy updates. On the other hand, allocating orthogonal resource blocks (RB) to individual devices via digital channels mitigates the noise problem, at the cost of increased communication latency. In this paper, we address this discrepancy and present ADFL, a novel Analog-Digital FL scheme: in each round, the parameter server (PS) schedules each device to either upload its gradient via the analog OTA scheme or transmit its quantized gradient over an orthogonal RB using the ``digital" scheme. Focusing on a single FL round, we cast the optimal scheduling problem as the minimization of the mean squared error (MSE) on the estimated global gradient at the PS, subject to a delay constraint, yielding the optimal device scheduling conf
    
[^196]: 一个准确且低参数的机器学习架构用于下一个位置的预测

    An Accurate and Low-Parameter Machine Learning Architecture for Next Location Prediction

    [https://arxiv.org/abs/2402.00306](https://arxiv.org/abs/2402.00306)

    本文提出了一个节能、小型且低参数的机器学习架构，用于准确预测用户的下一个位置。通过大量实验，成功将模型参数数量从2.02亿减少到200万，模型大小从791 MB减少到8 MB，训练时间减少了四倍。

    

    下一个位置的预测是一门涉及预测用户下一个位置的学科。其应用包括资源分配、服务质量、能源效率和交通管理。本文提出了一种节能、小型和低参数的机器学习（ML）架构，用于准确的下一个位置预测，可部署在普通基站和边缘设备上。为了实现这一目标，我们对一个整个城市的完整人员流动模式进行了一百个超参数实验，以确定一个精确的ML架构，其准确度达到了最少数量的模型参数的平台。我们成功地将已发表的ML架构的模型参数数量从2.02亿减少到200万。这将模型参数的总大小从791 MB减少到8 MB。此外，训练时间减少了四倍，训练所需的图形处理单元（GPU）内存量也减少了一个因素。

    Next location prediction is a discipline that involves predicting a users next location. Its applications include resource allocation, quality of service, energy efficiency, and traffic management. This paper proposes an energy-efficient, small, and low parameter machine learning (ML) architecture for accurate next location prediction, deployable on modest base stations and edge devices. To accomplish this we ran a hundred hyperparameter experiments on the full human mobility patterns of an entire city, to determine an exact ML architecture that reached a plateau of accuracy with the least amount of model parameters. We successfully achieved a reduction in the number of model parameters within published ML architectures from 202 million down to 2 million. This reduced the total size of the model parameters from 791 MB down to 8 MB. Additionally, this decreased the training time by a factor of four, the amount of graphics processing unit (GPU) memory needed for training by a factor of t
    
[^197]: 检测大型人工智能模型生成的多媒体内容：一项调查研究

    Detecting Multimedia Generated by Large AI Models: A Survey

    [https://arxiv.org/abs/2402.00045](https://arxiv.org/abs/2402.00045)

    本文综述了大型人工智能模型（LAIMs）生成的多媒体内容的检测方法和研究进展，旨在填补现有研究中的空白。我们提供了一种新颖的分类法，并介绍了纯检测和应用场景两个角度来增强检测性能。

    

    大型人工智能模型（LAIMs）的快速发展，尤其是扩散模型和大型语言模型，标志着一种新的时代，人工智能生成的多媒体内容被越来越多地整合到日常生活的各个方面。尽管在许多领域有益，但这些内容也带来了重大风险，包括潜在的滥用、社会破坏和伦理问题。因此，检测由LAIMs生成的多媒体内容变得至关重要，相关研究也大幅增加。尽管如此，目前仍然存在一个明显的问题，即缺乏系统性的调查研究，专门关注检测LAIMs生成的多媒体内容。为了解决这个问题，我们提供了第一份全面涵盖现有研究的调查报告，重点关注检测LAIMs生成的多媒体内容（如文本、图像、视频、音频和多模态内容）。具体而言，我们引入了一种新颖的检测方法分类法，按媒体形式分类，并与纯检测（旨在提高检测性能）和应用场景对齐。

    The rapid advancement of Large AI Models (LAIMs), particularly diffusion models and large language models, has marked a new era where AI-generated multimedia is increasingly integrated into various aspects of daily life. Although beneficial in numerous fields, this content presents significant risks, including potential misuse, societal disruptions, and ethical concerns. Consequently, detecting multimedia generated by LAIMs has become crucial, with a marked rise in related research. Despite this, there remains a notable gap in systematic surveys that focus specifically on detecting LAIM-generated multimedia. Addressing this, we provide the first survey to comprehensively cover existing research on detecting multimedia (such as text, images, videos, audio, and multimodal content) created by LAIMs. Specifically, we introduce a novel taxonomy for detection methods, categorized by media modality, and aligned with two perspectives: pure detection (aiming to enhance detection performance) an
    
[^198]: 一种带有均匀PAC保证的约束MDPs的策略梯度原始对偶算法

    A Policy Gradient Primal-Dual Algorithm for Constrained MDPs with Uniform PAC Guarantees

    [https://arxiv.org/abs/2401.17780](https://arxiv.org/abs/2401.17780)

    本文介绍了一种带有均匀PAC保证的策略梯度原始对偶算法，用于在线约束马尔可夫决策过程（CMDP）问题。该算法同时保证了收敛到最优策略、次线性遗憾和多项式样本复杂度，并在实证研究中验证了其优越性能。

    

    我们研究了一种基于原始对偶强化学习算法的在线约束马尔可夫决策过程（CMDP）问题，其中代理探索的最优策略在满足约束的同时最大化回报。尽管在实践中被广泛使用，但现有的理论文献仅提供次线性遗憾保证，并未能确保收敛到最优策略。在本文中，我们引入了一种新颖的带有均匀可能近似正确性（Uniform-PAC）保证的策略梯度原始对偶算法，同时确保收敛到最优策略、次线性遗憾和多项式样本复杂度以实现任何目标精度。值得注意的是，这是在线CMDP问题的第一个Uniform-PAC算法。除了理论保证外，我们还在一个简单的CMDP中进行了实证研究，证明了我们的算法收敛到最优策略，而现有算法表现出振荡性能表现。

    We study a primal-dual reinforcement learning (RL) algorithm for the online constrained Markov decision processes (CMDP) problem, wherein the agent explores an optimal policy that maximizes return while satisfying constraints. Despite its widespread practical use, the existing theoretical literature on primal-dual RL algorithms for this problem only provides sublinear regret guarantees and fails to ensure convergence to optimal policies. In this paper, we introduce a novel policy gradient primal-dual algorithm with uniform probably approximate correctness (Uniform-PAC) guarantees, simultaneously ensuring convergence to optimal policies, sublinear regret, and polynomial sample complexity for any target accuracy. Notably, this represents the first Uniform-PAC algorithm for the online CMDP problem. In addition to the theoretical guarantees, we empirically demonstrate in a simple CMDP that our algorithm converges to optimal policies, while an existing algorithm exhibits oscillatory perform
    
[^199]: 分子属性预测的图多相似性学习

    Graph Multi-Similarity Learning for Molecular Property Prediction

    [https://arxiv.org/abs/2401.17615](https://arxiv.org/abs/2401.17615)

    提出了图多相似性学习 (GraphMSL)框架，通过引入连续尺度的多相似性度量，包括自相似度和相对相似性，以及对不同化学特征进行融合，提高了分子属性预测的效果和适用性。

    

    有效的分子表示学习对于分子属性预测至关重要。对分子表示学习而言，对比学习是一种明显的自监督方法，它依赖于建立正负对。然而，这种二进制相似性分类过于简化了复杂分子关系的性质，并忽视了分子之间相对相似性的程度，给表示学习的效果和泛化性带来挑战。为了应对这一挑战，我们提出了用于分子属性预测的图多相似性学习(GraphMSL)框架。GraphMSL将广义多相似性度量融入到连续尺度中，包括自相似度和相对相似性。单模多相似性度量来自于各种化学模态，而这些度量的融合形式则显著增强了GraphMSL的效果。此外，融合的灵活性能够适应不同任务需求，使得GraphMSL在分子属性预测中具备更广的适用性。

    Effective molecular representation learning is essential for molecular property prediction. Contrastive learning, a prominent self-supervised approach for molecular representation learning, relies on establishing positive and negative pairs. However, this binary similarity categorization oversimplifies the nature of complex molecular relationships and overlooks the degree of relative similarities among molecules, posing challenges to the effectiveness and generality of representation learning. In response to this challenge, we propose the Graph Multi-Similarity Learning for Molecular Property Prediction (GraphMSL) framework. GraphMSL incorporates a generalized multi-similarity metric in a continuous scale, capturing self-similarity and relative similarities. The unimodal multi-similarity metrics are derived from various chemical modalities, and the fusion of these metrics into a multimodal form significantly enhances the effectiveness of GraphMSL. In addition, the flexibility of fusion
    
[^200]: 大型语言模型能否取代经济选择预测实验室？

    Can Large Language Models Replace Economic Choice Prediction Labs?

    [https://arxiv.org/abs/2401.17435](https://arxiv.org/abs/2401.17435)

    该论文研究大型语言模型是否能够取代经济实验室进行选择预测，并通过相关实验证明了其可行性。

    

    经济选择预测是一项具有挑战性的重要任务，往往受限于获取人类选择数据的困难。实验经济学研究在很大程度上专注于简单的选择环境。最近，人工智能界以两种方式为该努力做出了贡献：考虑大型语言模型是否可以代替人类在上述简单选择预测环境中，以及通过机器学习视角研究更复杂但仍严格的实验经济学环境，包括不完全信息、重复博弈和基于自然语言交流的说服游戏。这引发了一个重要的灵感：大型语言模型是否能够完全模拟经济环境，并生成用于高效人类选择预测的数据，替代复杂的经济实验室研究？我们在这个主题上开创了研究，并展示了其可行性。特别是，我们表明仅在大型语言模型生成的数据上训练的模型可以有效地进行预测。

    Economic choice prediction is an essential challenging task, often constrained by the difficulties in acquiring human choice data. Indeed, experimental economics studies had focused mostly on simple choice settings. The AI community has recently contributed to that effort in two ways: considering whether LLMs can substitute for humans in the above-mentioned simple choice prediction settings, and the study through ML lens of more elaborated but still rigorous experimental economics settings, employing incomplete information, repetitive play, and natural language communication, notably language-based persuasion games. This leaves us with a major inspiration: can LLMs be used to fully simulate the economic environment and generate data for efficient human choice prediction, substituting for the elaborated economic lab studies? We pioneer the study of this subject, demonstrating its feasibility. In particular, we show that a model trained solely on LLM-generated data can effectively predic
    
[^201]: 对于具有未知大规模衰落、信道统计、噪音方差和活动概率的无线单元网络中的大规模连接活动检测: 一种贝叶斯方法的研究

    Activity Detection for Massive Connectivity in Cell-free Networks with Unknown Large-scale Fading, Channel Statistics, Noise Variance, and Activity Probability: A Bayesian Approach. (arXiv:2401.16775v1 [cs.LG])

    [http://arxiv.org/abs/2401.16775](http://arxiv.org/abs/2401.16775)

    本研究探讨了在没有关于网络的重要信息的情况下进行活动检测的问题，并通过使用贝叶斯方法来处理大量未知参数。研究提出了最大后验估计器和变分方法来解决这个问题。

    

    活动检测是下一代免授权多址中的重要任务。虽然有许多现有的算法专门设计用于此目的，但它们大多需要精确的网络信息，如大规模衰落系数、小规模衰落信道统计、访问点的噪音方差和用户活动概率。获取这些信息将会耗费大量开销，并且其估计值可能不准确。在无线单元网络中，这个问题更加严重，因为存在许多这些参数需要获取。因此，本文旨在研究在没有上述信息的情况下的活动检测问题。为了处理这么多未知参数，本文采用了贝叶斯方法，其中未知变量赋予了先验分布，有效地起到了正则化的作用。结合似然函数，使用了最大后验（MAP）估计器和变分

    Activity detection is an important task in the next generation grant-free multiple access. While there are a number of existing algorithms designed for this purpose, they mostly require precise information about the network, such as large-scale fading coefficients, small-scale fading channel statistics, noise variance at the access points, and user activity probability. Acquiring these information would take a significant overhead and their estimated values might not be accurate. This problem is even more severe in cell-free networks as there are many of these parameters to be acquired. Therefore, this paper sets out to investigate the activity detection problem without the above-mentioned information. In order to handle so many unknown parameters, this paper employs the Bayesian approach, where the unknown variables are endowed with prior distributions which effectively act as regularizations. Together with the likelihood function, a maximum a posteriori (MAP) estimator and a variatio
    
[^202]: 自适应众包通过自监督学习

    Adaptive Crowdsourcing Via Self-Supervised Learning. (arXiv:2401.13239v1 [cs.LG])

    [http://arxiv.org/abs/2401.13239](http://arxiv.org/abs/2401.13239)

    本论文介绍了一种新的自适应众包方法，通过利用自监督学习和新颖的聚合方案，根据众包工作者对先前数量的估计调整权重，实现更准确的集体估计。该方法适应复杂模型和其他实际挑战，并通过理论和计算研究进行了验证。

    

    通常的众包系统通过对众多众包工作者提供的潜在利益数量的估计进行平均得到集体估计。我们开发了一种新方法--只预测其他人--利用自监督学习和一种新颖的聚合方案。这种方法根据他们对先前数量的估计给予众包工作者分配的权重来调整。当众包工作者的技能变化或他们的估计相关时，加权求和比平均求和提供更准确的集体估计。现有的算法，如期望最大化，至少在原则上可以产生类似准确的集体估计。然而，当需要使用诸如神经网络之类的复杂模型来表示众包工作者之间的关系时，它们的计算需求变得繁重。只预测其他人适应了这种复杂性以及许多其他实际挑战。我们通过理论和计算研究分析了只预测其他人的功效。其中

    Common crowdsourcing systems average estimates of a latent quantity of interest provided by many crowdworkers to produce a group estimate. We develop a new approach -- just-predict-others -- that leverages self-supervised learning and a novel aggregation scheme. This approach adapts weights assigned to crowdworkers based on estimates they provided for previous quantities. When skills vary across crowdworkers or their estimates correlate, the weighted sum offers a more accurate group estimate than the average. Existing algorithms such as expectation maximization can, at least in principle, produce similarly accurate group estimates. However, their computational requirements become onerous when complex models, such as neural networks, are required to express relationships among crowdworkers. Just-predict-others accommodates such complexity as well as many other practical challenges. We analyze the efficacy of just-predict-others through theoretical and computational studies. Among other 
    
[^203]: 视觉语言模型中被忽视的尾部

    The Neglected Tails of Vision-Language Models. (arXiv:2401.12425v1 [cs.CV])

    [http://arxiv.org/abs/2401.12425](http://arxiv.org/abs/2401.12425)

    本文通过分析预训练文本来衡量视觉语言模型中概念的频率，并发现流行的VLM数据集展示了长尾概念分布，这与按类别的准确率强烈相关。

    

    视觉语言模型（VLM）在零样本识别方面表现出色，但在视觉概念上的表现极不均衡。例如，尽管CLIP在ImageNet上具有令人印象深刻的平均零样本准确率（72.7％），但在十个概念（如gyromitra和night snake）上的准确率不到10％，这可能是因为这些概念在VLM的非均衡预训练数据中的表示不足。然而，评估这种不平衡是具有挑战性的，因为在VLM的大规模预训练数据中计算特定概念的频率是非常复杂的。我们的工作首次尝试使用分析预训练文本来测量概念频率。我们利用现成的语言模型来帮助计算包含给定概念的同义词的相关文本，并解决语言歧义。我们确认像LAION这样的流行的VLM数据集确实展示了长尾概念分布，并且这与按类别的准确率强烈相关。此外，当代的多模式系统，如视觉聊天机器人和文本-视觉推理模型，在这种长尾分布下经常难以达到高性能。

    Vision-language models (VLMs) excel in zero-shot recognition but exhibit drastically imbalanced performance across visual concepts. For example, CLIP, despite an impressive mean zero-shot accuracy on ImageNet (72.7%), yields $<$10% on ten concepts (e.g., gyromitra and night snake), presumably, because these concepts are under-represented in VLMs' imbalanced pretraining data. Yet, assessing this imbalance is challenging as it is non-trivial to calculate the frequency of specific concepts within VLMs' large-scale pretraining data. Our work makes the first attempt to measure the concept frequency by analyzing pretraining texts. We use off-the-shelf language models to help count relevant texts that contain synonyms of the given concepts and resolve linguistic ambiguity. We confirm that popular VLM datasets like LAION indeed exhibit long-tailed concept distributions, which strongly correlate with per-class accuracies. Further, contemporary multimodal systems, e.g., visual chatbots and text-
    
[^204]: DQNC2S：基于DQN的跨流危机事件摘要生成器

    DQNC2S: DQN-based Cross-stream Crisis event Summarizer. (arXiv:2401.06683v1 [cs.IR])

    [http://arxiv.org/abs/2401.06683](http://arxiv.org/abs/2401.06683)

    本研究提出了一种基于DQN的在线危机事件摘要生成方法，能够同时总结多个灾害相关的数据流，无需人工标注或内容重新排序，且具有较好的性能表现。

    

    同时总结多个与灾害相关的数据流尤其具有挑战性，因为现有的检索与重新排序策略在多流数据的固有冗余和多查询环境下的限制可扩展性方面存在问题。本文提出了一种基于弱标注和深度Q网络的在线危机时间轴生成方法。它能够实时选择相关的文本片段，无需人工标注或内容重新排序，从而使推理时间与输入查询的数量无关。该方法还将冗余过滤器融入奖励函数中，以有效处理跨流内容重叠。在CrisisFACTS 2022基准测试中，所达到的ROUGE和BERTScore结果优于最佳性能模型。

    Summarizing multiple disaster-relevant data streams simultaneously is particularly challenging as existing Retrieve&Re-ranking strategies suffer from the inherent redundancy of multi-stream data and limited scalability in a multi-query setting. This work proposes an online approach to crisis timeline generation based on weak annotation with Deep Q-Networks. It selects on-the-fly the relevant pieces of text without requiring neither human annotations nor content re-ranking. This makes the inference time independent of the number of input queries. The proposed approach also incorporates a redundancy filter into the reward function to effectively handle cross-stream content overlaps. The achieved ROUGE and BERTScore results are superior to those of best-performing models on the CrisisFACTS 2022 benchmark.
    
[^205]: 信息论方法在基于互动学习中的应用

    An Information Theoretic Approach to Interaction-Grounded Learning. (arXiv:2401.05015v1 [cs.LG])

    [http://arxiv.org/abs/2401.05015](http://arxiv.org/abs/2401.05015)

    本文提出了一个基于变分信息的互动基础学习（VI-IGL）方法，用于在强化学习任务中强制执行条件独立性假设。该方法通过学习奖励解码器来最大化上下文-动作（X，A）和反馈变量Y之间的条件互信息。

    

    最近的几篇论文研究了强化学习（RL）中学习者试图从一些反馈变量中推断出未观测到的奖励的问题。互动基础学习（IGL）是这种基于反馈的强化学习任务的一个例子，学习者通过与环境的互动推断出潜在的二值奖励来优化返回。在IGL设置中，RL文献中使用的一个相关假设是，在给定潜在奖励R的情况下，反馈变量Y在上下文-动作（X，A）上是条件独立的。在本文中，我们提出了一种基于变分信息的IGL（VI-IGL）方法，以应用于IGL的RL问题中，以强制执行条件独立性假设。VI-IGL框架使用基于条件互信息（MI）的信息目标来学习奖励解码器，该信息目标衡量了从环境中观察到的上下文-动作（X，A）和反馈变量Y之间的条件互信息。

    Reinforcement learning (RL) problems where the learner attempts to infer an unobserved reward from some feedback variables have been studied in several recent papers. The setting of Interaction-Grounded Learning (IGL) is an example of such feedback-based reinforcement learning tasks where the learner optimizes the return by inferring latent binary rewards from the interaction with the environment. In the IGL setting, a relevant assumption used in the RL literature is that the feedback variable $Y$ is conditionally independent of the context-action $(X,A)$ given the latent reward $R$. In this work, we propose Variational Information-based IGL (VI-IGL) as an information-theoretic method to enforce the conditional independence assumption in the IGL-based RL problem. The VI-IGL framework learns a reward decoder using an information-based objective based on the conditional mutual information (MI) between the context-action $(X,A)$ and the feedback variable $Y$ observed from the environment.
    
[^206]: 合成数据生成的综合探索：一项调查

    Comprehensive Exploration of Synthetic Data Generation: A Survey. (arXiv:2401.02524v1 [cs.LG])

    [http://arxiv.org/abs/2401.02524](http://arxiv.org/abs/2401.02524)

    本文调查了过去十年中417个合成数据生成模型，提供了对模型类型、功能和改进的全面概述。结果表明，模型性能和复杂性增加，以神经网络为主要方法，计算机视觉领域占据主导地位。

    

    最近几年，机器学习在各个领域的应用变得越来越受欢迎。然而，由于数据采集成本高昂和隐私法规的限制，训练数据的稀缺性阻碍了进展。合成数据成为一种解决方案，但发布的模型过多和有限的综述文献给决策带来了挑战。本文调查了过去十年中417个合成数据生成模型，提供了对模型类型、功能和改进的全面概述。确定了共同的特征，进行了分类和趋势分析。结果显示模型性能和复杂性增加，以基于神经网络的方法为主要趋势，除了隐私保护数据生成。计算机视觉占据主导地位，生成模型主要是GAN，而扩散模型、转换器和RNN也在竞争中。通过性能评估的结果显示了共同指标和数据集稀缺性的问题。

    Recent years have witnessed a surge in the popularity of Machine Learning (ML), applied across diverse domains. However, progress is impeded by the scarcity of training data due to expensive acquisition and privacy legislation. Synthetic data emerges as a solution, but the abundance of released models and limited overview literature pose challenges for decision-making. This work surveys 417 Synthetic Data Generation (SDG) models over the last decade, providing a comprehensive overview of model types, functionality, and improvements. Common attributes are identified, leading to a classification and trend analysis. The findings reveal increased model performance and complexity, with neural network-based approaches prevailing, except for privacy-preserving data generation. Computer vision dominates, with GANs as primary generative models, while diffusion models, transformers, and RNNs compete. Implications from our performance evaluation highlight the scarcity of common metrics and datase
    
[^207]: 是否所有未见数据都是OoD数据？

    Are All Unseen Data Out-of-Distribution?. (arXiv:2312.16243v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.16243](http://arxiv.org/abs/2312.16243)

    该论文研究了未见数据的分布在泛化中的挑战，并提出了重新定义OoD数据以及新的泛化上界，保证了对未见数据的模型有效性。

    

    处理未见数据的分布一直被视为是OoD（out-of-distribution），使其泛化成为一个重要挑战。很多证据表明，训练数据的规模增加可以单调地降低测试数据的泛化误差。然而，从其他观察和分析来看，这并不总是成立。特别是当训练数据包含多个来源领域，并且测试数据包含分布漂移时，不是所有的测试数据的泛化误差都会随着训练数据规模的增加而单调减小。在线性设置下，我们正式研究了这种非单调现象，并通过在不同视觉基准上进行经验证实。鉴于这些结果，我们重新定义了OoD数据，将其视为训练域的凸包之外的数据，并基于这个新定义证明了一个新的泛化上界。它意味着对于在训练阶段没有见过的数据，经过充分训练的模型的有效性可以得到保证。

    Distributions of unseen data have been all treated as out-of-distribution (OOD), making their generalization a significant challenge. Much evidence suggests that the size increase of training data can monotonically decrease generalization errors in test data. However, this is not true from other observations and analysis. In particular, when the training data have multiple source domains and the test data contain distribution drifts, then not all generalization errors on the test data decrease monotonically with the increasing size of training data. Such a non-decreasing phenomenon is formally investigated under a linear setting with empirical verification across varying visual benchmarks. Motivated by these results, we redefine the OOD data as a type of data outside the convex hull of the training domains and prove a new generalization bound based on this new definition. It implies that the effectiveness of a well-trained model can be guaranteed for the unseen data that is within the 
    
[^208]: 特征引导：大尺度导向下扩散模型的非线性校正

    Characteristic Guidance: Non-linear Correction for Diffusion Model at Large Guidance Scale. (arXiv:2312.07586v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.07586](http://arxiv.org/abs/2312.07586)

    该论文提出了特征引导方法，用于对大尺度的导向下扩散模型进行非线性校正，以增强对图像生成的控制能力，减少颜色和曝光问题，并在各种应用中显示出效果。

    

    流行的导引去噪扩散概率模型(DDPM)线性地将不同的条件模型组合在一起，以提供对样本的增强控制。然而，这种方法忽视了当导向尺度变大时产生的非线性效应。为了解决这个问题，我们提出了特征引导，一种采样方法，为无分类器导向的DDPM提供了一种基于原理的非线性校正。这种校正迫使导向的DDPM遵守其底层扩散过程的福克-普朗克方程，这种方法无需训练，无需导数，与现有的采样方法兼容。实验证明，特征引导增强了对图像生成中的控制能力，并减少了颜色和曝光问题，对从潜在空间采样到解决物理问题如磁相变的各种应用都有效。

    Popular guidance for denoising diffusion probabilistic model (DDPM) linearly combines distinct conditional models together to provide enhanced control over samples. However, this approach overlooks nonlinear effects that become significant when guidance scale is large. To address this issue, we propose characteristic guidance, a sampling method that provides first-principle non-linear correction for classifier-free guided DDPMs. Such correction forces the guided DDPMs to respect the Fokker-Planck equation of their underlying diffusion process, in a way that is training-free, derivative-free, and compatible with existing sampling methods. Experiments show that characteristic guidance enhances control and reduces color and exposure issues in image generation, proving effective in diverse applications ranging from latent space sampling to solving physics problems like magnet phase transitions.
    
[^209]: 新的在线社区：在匿名投票网络上进行图深度学习以识别多中心治理中的虚假身份

    New Online Communities: Graph Deep Learning on Anonymous Voting Networks to Identify Sybils in Polycentric Governance. (arXiv:2311.17929v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.17929](http://arxiv.org/abs/2311.17929)

    本研究提供了一个理论框架，通过使用图深度学习技术，可以有效识别去中心化自治组织（DAO）中的虚假身份，为分散治理提供了新的视角。

    

    本研究探讨了基于区块链的去中心化自治组织（DAO）中数字资产的多中心治理。它提供了一个理论框架，并通过开发一种方法来识别虚假身份（sybils）来解决分散治理面临的一个关键挑战。该方法使用图深度学习技术在DAO治理数据集（snapshot.org）中识别虚假身份的活动。具体地，一个图卷积神经网络（GCNN）学习了投票行为，并使用高维嵌入来识别图中相似节点的快速k均值向量聚类算法（FAISS）。结果显示，深度学习能够有效识别虚假身份，在投票图中减少2-5%。本研究强调了在DAO中的sybil抗性的重要性，并为分散治理提供了新的视角，对未来的政策、监管和治理实践具有启示作用。

    This research examines the polycentric governance of digital assets in blockchain-based Decentralized Autonomous Organizations (DAOs). It offers a theoretical framework and addresses a critical challenge facing decentralized governance by developing a method to identify sybils, or spurious identities. The method uses graph deep learning techniques to identify sybil activity in a DAO governance dataset (snapshot.org). Specifically, a Graph Convolutional Neural Network (GCNN) learned voting behaviours and a fast k-means vector clustering algorithm (FAISS) used the high dimensional embeddings to identify similar nodes in a graph. The results reveal that deep learning can effectively identify sybils, reducing the voting graph by 2-5%. This research underscores the importance of sybil resistance in DAOs and offers a novel perspective on decentralized governance, informing future policy, regulation, and governance practices.
    
[^210]: FIKIT：基于优先级的实时GPU多任务调度与内核识别

    FIKIT: Priority-Based Real-time GPU Multi-tasking Scheduling with Kernel Identification. (arXiv:2311.10359v3 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2311.10359](http://arxiv.org/abs/2311.10359)

    FIKIT是一种基于优先级的实时GPU多任务调度策略，具有内核识别功能，能够在高优先级任务的内核间填充空闲时间。

    

    高并行工作负载，如机器学习训练、推断和一般HPC任务，通过使用GPU设备得到了极大的加速。在云计算集群中，通过多任务共享来提供GPU的计算能力是非常需要的，因为总是有更多的任务请求而不是可用的GPU数量。现有的GPU共享解决方案着重于减少多个作业争夺单个GPU时的任务级等待时间或任务级切换成本。连续计算请求具有不同的优先级，对于共享GPU设备，对QoS产生了非对称的影响。现有工作没有充分利用这种情况带来的内核级优化机会。为了解决这个问题，我们提出了一种新颖的内核级调度策略FIKIT：填充内核间空闲时间。FIKIT包含任务级优先级信息、细粒度内核识别和内核测量，允许低优先级任务在高优先级任务的内核间执行。

    Highly parallelized workloads like machine learning training, inferences and general HPC tasks are greatly accelerated using GPU devices. In a cloud computing cluster, serving a GPU's computation power through multi-tasks sharing is highly demanded since there are always more task requests than the number of GPU available. Existing GPU sharing solutions focus on reducing task-level waiting time or task-level switching costs when multiple jobs competing for a single GPU. Non-stopped computation requests come with different priorities, having non-symmetric impact on QoS for sharing a GPU device. Existing work missed the kernel-level optimization opportunity brought by this setting. To address this problem, we present a novel kernel-level scheduling strategy called FIKIT: Filling Inter-kernel Idle Time. FIKIT incorporates task-level priority information, fine-grained kernel identification, and kernel measurement, allowing low priorities task's execution during high priority task's inter-k
    
[^211]: 强化学习从人类反馈中的目标不匹配问题：对齐上限

    The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback. (arXiv:2311.00168v1 [cs.LG])

    [http://arxiv.org/abs/2311.00168](http://arxiv.org/abs/2311.00168)

    这项研究探讨了强化学习从人类反馈中的目标不匹配问题。研究发现，在强化学习从人类反馈中，奖励模型训练、策略模型训练和策略模型评估之间存在不一致，导致模型行为的意想不到的结果。

    

    强化学习从人类反馈中的目标不匹配问题（RLHF）已经成为使大型语言模型（LLM）更易于提示并在复杂环境中更有能力的强大技术。RLHF核心是提供了一种优化LLM的新工具包，而不仅仅是下一个标记的预测，从而实现了定性训练目标的整合。在学习奖励模型中，用户偏好和下游性能之间的匹配尝试导致了一个优化景观，训练和评估指标看起来可能是相关的。这种表面上的相关关系可能导致意想不到的行为和“过度RLHF”的情况。在RLHF中，由于以下子模块不一致，会出现挑战：奖励模型训练、策略模型训练和策略模型评估。这种不匹配导致模型有时会避免用户请求的虚假安全标志，很难引导模型朝着预期的特征发展，或者总是以特定的风格回答。

    Reinforcement learning from human feedback (RLHF) has emerged as a powerful technique to make large language models (LLMs) easier to prompt and more capable in complex settings. RLHF at its core is providing a new toolkit to optimize LLMs other than next-token prediction, enabling the integration of qualitative training goals. The attempted match between user preferences and downstream performance, which happens in a learned reward model, results in an optimization landscape where training and evaluation metrics can appear correlated. The apparent correlation can lead to unexpected behaviors and stories of "too much RLHF." In RLHF, challenges emerge because the following sub-modules are not consistent with each other: the reward model training, the policy model training, and the policy model evaluation. This mismatch results in models that sometimes avoid user requests for false safety flags, are difficult to steer to an intended characteristic, or always answer in a specific style. As
    
[^212]: 基于潜在决策模型的具有隐藏约束的贝叶斯优化方法

    Bayesian Optimization with Hidden Constraints via Latent Decision Models. (arXiv:2310.18449v1 [stat.ML])

    [http://arxiv.org/abs/2310.18449](http://arxiv.org/abs/2310.18449)

    本文介绍了一种基于潜在决策模型的贝叶斯优化方法，通过利用变分自编码器学习可行决策的分布，在原始空间和潜在空间之间实现了双向映射，从而解决了公共决策制定中的隐藏约束问题。

    

    贝叶斯优化（BO）已经成为解决复杂决策问题的强大工具，尤其在公共政策领域如警察划区方面。然而，由于定义可行区域的复杂性和决策的高维度，其在公共决策制定中的广泛应用受到了阻碍。本文介绍了一种新的贝叶斯优化方法——隐藏约束潜在空间贝叶斯优化（HC-LSBO），该方法集成了潜在决策模型。该方法利用变分自编码器来学习可行决策的分布，实现了原始决策空间与较低维度的潜在空间之间的双向映射。通过这种方式，HC-LSBO捕捉了公共决策制定中固有的隐藏约束的细微差别，在潜在空间中进行优化的同时，在原始空间中评估目标。我们通过对合成数据集和真实数据集进行数值实验来验证我们的方法，特别关注大规模问题。

    Bayesian optimization (BO) has emerged as a potent tool for addressing intricate decision-making challenges, especially in public policy domains such as police districting. However, its broader application in public policymaking is hindered by the complexity of defining feasible regions and the high-dimensionality of decisions. This paper introduces the Hidden-Constrained Latent Space Bayesian Optimization (HC-LSBO), a novel BO method integrated with a latent decision model. This approach leverages a variational autoencoder to learn the distribution of feasible decisions, enabling a two-way mapping between the original decision space and a lower-dimensional latent space. By doing so, HC-LSBO captures the nuances of hidden constraints inherent in public policymaking, allowing for optimization in the latent space while evaluating objectives in the original space. We validate our method through numerical experiments on both synthetic and real data sets, with a specific focus on large-scal
    
[^213]: 在贝叶斯优化中的随机探索：最佳遗憾和计算效率优化

    Random Exploration in Bayesian Optimization: Order-Optimal Regret and Computational Efficiency. (arXiv:2310.15351v1 [cs.LG])

    [http://arxiv.org/abs/2310.15351](http://arxiv.org/abs/2310.15351)

    本论文研究了在贝叶斯优化中使用随机探索的方法，并证明了其能够实现最佳的误差率和最优遗憾保证。同时，所提出的算法通过随机探索避免了每次迭代中非凸获取函数的昂贵优化，具有计算上的优势。

    

    我们考虑使用高斯过程模型的贝叶斯优化，也称为基于核的赌博优化。我们研究使用从分布中随机抽样来探索领域的方法。我们证明了这种随机探索方法能够实现最佳的误差率。我们的分析基于在本研究中建立的无限维希尔伯特空间中的新型集中边界，这可能具有独立的意义。我们进一步开发了一种基于随机探索和领域缩小的算法，并在无噪声和有噪声环境下建立其最佳遗憾保证。在无噪声环境中，我们的分析填补了在遗憾性能方面存在的差距，从而解决了COLT中的一个开放问题。由于随机探索消除了每次迭代中选择查询点的非凸获取函数的昂贵优化，所以所提出的算法也具有计算优势。

    We consider Bayesian optimization using Gaussian Process models, also referred to as kernel-based bandit optimization. We study the methodology of exploring the domain using random samples drawn from a distribution. We show that this random exploration approach achieves the optimal error rates. Our analysis is based on novel concentration bounds in an infinite dimensional Hilbert space established in this work, which may be of independent interest. We further develop an algorithm based on random exploration with domain shrinking and establish its order-optimal regret guarantees under both noise-free and noisy settings. In the noise-free setting, our analysis closes the existing gap in regret performance and thereby resolves a COLT open problem. The proposed algorithm also enjoys a computational advantage over prevailing methods due to the random exploration that obviates the expensive optimization of a non-convex acquisition function for choosing the query points at each iteration.
    
[^214]: 随机前向模式自动微分优化算法

    Randomized Forward Mode of Automatic Differentiation for Optimization Algorithms. (arXiv:2310.14168v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2310.14168](http://arxiv.org/abs/2310.14168)

    该论文介绍了一种随机前向模式自动微分优化算法，通过在神经网络的正向传递中计算损失函数的方向导数来更新参数。算法通过采样不同概率分布的随机方向，使用正向模式自动微分计算雅可比向量乘积，并提供了对其收敛速度和计算复杂性的严格分析。

    

    神经网络中的反向传播利用了自动微分的基本要素，即反向模式微分，或称为向量雅可比乘积(VJP)，或在微分几何的背景下被称为拉回过程。梯度的计算对于使用梯度下降方法更新神经网络参数非常重要。在本研究中，我们提出了一种通用的随机方法，通过使用通过正向模式AD或雅可比向量乘积(JVP)高效计算的损失函数的方向导数来更新神经网络的参数。这些JVP沿着从不同概率分布（例如伯努利、正态、维格纳、拉普拉斯和均匀分布）采样的随机方向计算。梯度的计算在神经网络的正向传递过程中进行。我们还提供了对所提出方法的严格分析，包括收敛速度以及计算复杂性。

    Backpropagation within neural networks leverages a fundamental element of automatic differentiation, which is referred to as the reverse mode differentiation, or vector Jacobian Product (VJP) or, in the context of differential geometry, known as the pull-back process. The computation of gradient is important as update of neural network parameters is performed using gradient descent method. In this study, we present a genric randomized method, which updates the parameters of neural networks by using directional derivatives of loss functions computed efficiently by using forward mode AD or Jacobian vector Product (JVP). These JVP are computed along the random directions sampled from different probability distributions e.g., Bernoulli, Normal, Wigner, Laplace and Uniform distributions. The computation of gradient is performed during the forward pass of the neural network. We also present a rigorous analysis of the presented methods providing the rate of convergence along with the computat
    
[^215]: 几乎等变性通过李代数卷积

    Almost Equivariance via Lie Algebra Convolutions. (arXiv:2310.13164v1 [cs.LG])

    [http://arxiv.org/abs/2310.13164](http://arxiv.org/abs/2310.13164)

    本文研究了几乎等变性的主题，并提供了一个不同于现有定义的几乎等变性定义，并通过利用李群的李代数给出了在模型中编码几乎等变性的实用方法。

    

    最近，在机器学习中，模型相对于群作用的等变性已成为一个重要的研究课题。然而，赋予一个架构具体的群等变性对模型所期望看到的数据变换类型施加了强大的先验。严格等变模型强制执行对称性，但真实世界的数据并不总是符合这样的严格等变性，可能是因为数据中的噪声或仅编码了近似或部分对称性的潜在物理定律。在这种情况下，严格等变性的先验实际上可能过于强大，导致模型在真实数据上表现不佳。因此，在这项工作中，我们研究了一个相关的主题，即几乎等变性。我们提供了一个与当前文献中现有定义不同的几乎等变性定义，并通过利用李群的李代数给出了在模型中编码几乎等变性的实用方法。

    Recently, the equivariance of models with respect to a group action has become an important topic of research in machine learning. However, imbuing an architecture with a specific group equivariance imposes a strong prior on the types of data transformations that the model expects to see. While strictly-equivariant models enforce symmetries, real-world data does not always conform to such strict equivariances, be it due to noise in the data or underlying physical laws that encode only approximate or partial symmetries. In such cases, the prior of strict equivariance can actually prove too strong and cause models to underperform on real-world data. Therefore, in this work we study a closely related topic, that of almost equivariance. We provide a definition of almost equivariance that differs from those extant in the current literature and give a practical method for encoding almost equivariance in models by appealing to the Lie algebra of a Lie group. Specifically, we define Lie algebr
    
[^216]: 在分布式学习任务中评估生成模型

    On the Evaluation of Generative Models in Distributed Learning Tasks. (arXiv:2310.11714v1 [cs.LG])

    [http://arxiv.org/abs/2310.11714](http://arxiv.org/abs/2310.11714)

    本文研究了在具有异构数据分布的分布式学习任务中评估生成模型。通过研究Fr\'echet inception距离（FID），并考虑不同聚合分数，发现FID-all和FID-avg分数的模型排名可能不一致。

    

    在文献中已经广泛研究了对包括生成对抗网络（GAN）和扩散模型在内的深度生成模型的评估。然而，现有的评估方法主要针对单个客户端存储的训练数据的集中式学习问题，而生成模型的许多应用涉及到分布式学习环境，例如联邦学习场景，其中训练数据由多个客户端收集并分发。本文研究了在具有异构数据分布的分布式学习任务中评估生成模型。首先，我们关注Fr\'echet inception距离（FID），并考虑以下基于FID的聚合分数：1）FID-avg作为客户端个体FID分数的平均值，2）FID-all作为训练模型与包含所有客户端数据的集体数据集之间的FID距离。我们证明了根据FID-all和FID-avg分数的模型排名可能不一致。

    The evaluation of deep generative models including generative adversarial networks (GANs) and diffusion models has been extensively studied in the literature. While the existing evaluation methods mainly target a centralized learning problem with training data stored by a single client, many applications of generative models concern distributed learning settings, e.g. the federated learning scenario, where training data are collected by and distributed among several clients. In this paper, we study the evaluation of generative models in distributed learning tasks with heterogeneous data distributions. First, we focus on the Fr\'echet inception distance (FID) and consider the following FID-based aggregate scores over the clients: 1) FID-avg as the mean of clients' individual FID scores, 2) FID-all as the FID distance of the trained model to the collective dataset containing all clients' data. We prove that the model rankings according to the FID-all and FID-avg scores could be inconsist
    
[^217]: 在无限时域的马尔可夫决策过程中，利用离线数据集进行高效在线学习：一种贝叶斯方法

    Efficient Online Learning with Offline Datasets for Infinite Horizon MDPs: A Bayesian Approach. (arXiv:2310.11531v1 [cs.LG])

    [http://arxiv.org/abs/2310.11531](http://arxiv.org/abs/2310.11531)

    本文研究了在存在离线数据集的情况下，如何在无限时域进行高效的在线学习。研究表明，学习代理模拟专家的行为策略能够显著减小累积遗憾。通过贝叶斯方法进行的先验相关遗憾分析提供了算法的性能上界，并提出了一种近似的模仿学习算法来结合离线数据集和在线学习。

    

    本文研究了当存在一个离线数据集时，如何在无限时域设置下进行高效的在线强化学习问题。我们假设离线数据集是由一个专家生成的，但其能力水平未知，即它不是完美的，也不一定使用最优策略。我们展示了如果学习代理模拟专家使用的行为策略（由能力参数参数化），在累积遗憾最小化方面能取得明显更好的结果。我们建立了一个以 $\tilde{O}(\sqrt{T})$ 为缩放的精确有用PSRL算法遗憾的上界。这需要对贝叶斯在线学习算法在无限时域设置下进行新颖的先验相关遗憾分析。然后，我们提出了一种近似的Informed RLSVI算法，可以理解为使用离线数据集进行模仿学习，然后进行在线学习。

    In this paper, we study the problem of efficient online reinforcement learning in the infinite horizon setting when there is an offline dataset to start with. We assume that the offline dataset is generated by an expert but with unknown level of competence, i.e., it is not perfect and not necessarily using the optimal policy. We show that if the learning agent models the behavioral policy (parameterized by a competence parameter) used by the expert, it can do substantially better in terms of minimizing cumulative regret, than if it doesn't do that. We establish an upper bound on regret of the exact informed PSRL algorithm that scales as $\tilde{O}(\sqrt{T})$. This requires a novel prior-dependent regret analysis of Bayesian online learning algorithms for the infinite horizon setting. We then propose an approximate Informed RLSVI algorithm that we can interpret as performing imitation learning with the offline dataset, and then performing online learning.
    
[^218]: 使用大型语言模型进行实体匹配

    Entity Matching using Large Language Models. (arXiv:2310.11244v1 [cs.CL])

    [http://arxiv.org/abs/2310.11244](http://arxiv.org/abs/2310.11244)

    这项研究探讨了使用大型语言模型（LLMs）作为实体匹配的替代方法，相较于预训练的语言模型（PLMs），LLMs对训练数据需求较少且更具鲁棒性。

    

    实体匹配是判断两个实体描述是否指的是同一个真实世界实体的任务。实体匹配是大多数数据集成流程中的核心步骤，也是许多电子商务应用的重要组成部分，这些应用需要将来自不同供应商的产品匹配起来。目前最先进的实体匹配方法通常依赖于预训练的语言模型（PLMs），如BERT或RoBERTa。然而，这些模型在实体匹配中存在两个主要缺点：（i）模型需要大量特定任务的训练数据；（ii）微调后的模型对于超出分布范围的实体不够健壮。本文研究了使用大型语言模型（LLMs）作为基于PLMs的匹配器的备选方案，相比之下，LLMs对领域特定训练数据需求较少且更具鲁棒性。我们的研究涵盖了托管的LLMs，如GPT3.5和GPT4，以及基于Llama2的开源LLMs，可以在本地运行。我们在零样本场景和…

    Entity Matching is the task of deciding whether two entity descriptions refer to the same real-world entity. Entity Matching is a central step in most data integration pipelines and an enabler for many e-commerce applications which require to match products offers from different vendors. State-of-the-art entity matching methods often rely on pre-trained language models (PLMs) such as BERT or RoBERTa. Two major drawbacks of these models for entity matching are that (i) the models require significant amounts of task-specific training data and (ii) the fine-tuned models are not robust concerning out-of-distribution entities. In this paper, we investigate using large language models (LLMs) for entity matching as a less domain-specific training data reliant and more robust alternative to PLM-based matchers. Our study covers hosted LLMs, such as GPT3.5 and GPT4, as well as open source LLMs based on Llama2 which can be run locally. We evaluate these models in a zero-shot scenario as well as a
    
[^219]: 基于预训练语言模型的上下文少样本关系抽取

    In-Context Few-Shot Relation Extraction via Pre-Trained Language Models. (arXiv:2310.11085v1 [cs.CL])

    [http://arxiv.org/abs/2310.11085](http://arxiv.org/abs/2310.11085)

    本研究提出了基于预训练语言模型的上下文少样本关系抽取框架，首次将关系抽取任务重新定义为定制的上下文少样本学习范式。与现有方法相比，该框架不需要命名实体识别和文档人工注释，并且可以轻松更新到新的关系集合。通过评估使用DocRED数据集，验证了该框架的有效性。

    

    关系提取旨在从文本文档中推断结构化的人类知识。基于语言模型的最先进方法通常有两个限制：(1)它们要求命名实体作为输入或推断它们，从而引入了额外的噪声，(2)它们需要人工对文档进行注释。为解决这些问题，我们提出了一种新颖的基于预训练语言模型的上下文少样本关系抽取框架。据我们所知，我们是第一个将关系抽取任务重新定义为定制的上下文少样本学习范式的研究者。通过这种方式，我们在消除了命名实体识别和文档人工注释的需求的同时，实现了关键性的优势。与现有的基于微调的方法不同，我们的框架具有灵活性，可以在无需重新训练的情况下轻松更新到新的关系集合。我们使用DocRED评估了我们的框架，这是目前最大的公开可用的文档级关系提取数据集。

    Relation extraction aims at inferring structured human knowledge from textual documents. State-of-the-art methods based on language models commonly have two limitations: (1) they require named entities to be either given as input or infer them, which introduces additional noise, and (2) they require human annotations of documents. As a remedy, we present a novel framework for in-context few-shot relation extraction via pre-trained language models. To the best of our knowledge, we are the first to reformulate the relation extraction task as a tailored in-context few-shot learning paradigm. Thereby, we achieve crucial benefits in that we eliminate the need for both named entity recognition and human annotation of documents. Unlike existing methods based on fine-tuning, our framework is flexible in that it can be easily updated for a new set of relations without re-training. We evaluate our framework using DocRED, the largest publicly available dataset for document-level relation extracti
    
[^220]: 后验采样学习算法在序列化POMDPs中的遗憾分析

    Regret Analysis of the Posterior Sampling-based Learning Algorithm for Episodic POMDPs. (arXiv:2310.10107v1 [cs.LG])

    [http://arxiv.org/abs/2310.10107](http://arxiv.org/abs/2310.10107)

    本文分析了后验采样学习算法在序列化POMDPs中的遗憾性能，并在一定条件下提供了改进的多项式贝叶斯遗憾界。

    

    相比于马尔科夫决策过程（MDPs），部分可观察马尔科夫决策过程（POMDPs）的学习由于观察数据难以解读而变得更加困难。在本文中，我们考虑了具有未知转移和观测模型的POMDPs中的序列化学习问题。我们考虑了基于后验采样的强化学习算法（PSRL）在POMDPs中的应用，并证明其贝叶斯遗憾随着序列的数量的平方根而缩小。一般来说，遗憾随着时间长度$H$呈指数级增长，并通过提供一个下界证明了这一点。然而，在POMDP是欠完备且弱可识别的条件下，我们建立了一个多项式贝叶斯遗憾界，相比于arXiv:2204.08967的最新结果，改进了遗憾界约$\Omega(H^2\sqrt{SA})$倍。

    Compared to Markov Decision Processes (MDPs), learning in Partially Observable Markov Decision Processes (POMDPs) can be significantly harder due to the difficulty of interpreting observations. In this paper, we consider episodic learning problems in POMDPs with unknown transition and observation models. We consider the Posterior Sampling-based Reinforcement Learning (PSRL) algorithm for POMDPs and show that its Bayesian regret scales as the square root of the number of episodes. In general, the regret scales exponentially with the horizon length $H$, and we show that this is inevitable by providing a lower bound. However, under the condition that the POMDP is undercomplete and weakly revealing, we establish a polynomial Bayesian regret bound that improves the regret bound by a factor of $\Omega(H^2\sqrt{SA})$ over the recent result by arXiv:2204.08967.
    
[^221]: 终身音视频掩码自编码器与抗遗忘的本地化对齐

    Lifelong Audio-video Masked Autoencoder with Forget-robust Localized Alignments. (arXiv:2310.08204v1 [cs.CV])

    [http://arxiv.org/abs/2310.08204](http://arxiv.org/abs/2310.08204)

    这项研究提出了一种终身音视频掩码自编码器，通过引入本地化对齐和抗遗忘的多模态块选择，在不断变化的音视频分布中学习准确的多模态关系。

    

    我们提出了一种终身音视频掩码自编码器，它不断地从包含音视频对的视频流中学习多模态表示，同时其分布随着时间不断变化。具体而言，我们提出了两个新颖的想法来解决这个问题：（1）本地化对齐：引入一个小型可训练的多模态编码器，预测彼此之间良好对齐的音频和视频令牌。这使得模型只学习具有准确多模态关系的高度相关的音频视觉块。（2）抗遗忘的多模态块选择：比较当前和过去数据对之间每个音频视频块的相对重要性，以减轻先前学习的音视频表示的意外漂移。因此，我们提出的方法FLAVA（抗遗忘的本地化音视频对齐）在一系列预训练任务的训练过程中捕捉到音频和视频模态之间的复杂关系。

    We present a lifelong audio-video masked autoencoder that continually learns the multimodal representations from a video stream containing audio-video pairs, while its distribution continually shifts over time. Specifically, we propose two novel ideas to tackle the problem: (1) Localized Alignment: We introduce a small trainable multimodal encoder that predicts the audio and video tokens that are well-aligned with each other. This allows the model to learn only the highly correlated audiovisual patches with accurate multimodal relationships. (2) Forget-robust multimodal patch selection: We compare the relative importance of each audio-video patch between the current and past data pair to mitigate unintended drift of the previously learned audio-video representations. Our proposed method, FLAVA (Forget-robust Localized Audio-Video Alignment), therefore, captures the complex relationships between the audio and video modalities during training on a sequence of pre-training tasks while all
    
[^222]: CAST：面向表格数据的群集感知自训练

    CAST: Cluster-Aware Self-Training for Tabular Data. (arXiv:2310.06380v1 [cs.LG])

    [http://arxiv.org/abs/2310.06380](http://arxiv.org/abs/2310.06380)

    本文提出了一种面向表格数据的群集感知自训练方法（CAST），通过规范伪标签的置信度，弥补了自训练算法中的一些弱点，具有普适性和适应性。

    

    自训练由于其简单和多功能性而受到吸引，然而它容易受到有噪音的伪标签的影响。几项研究提出了成功解决这个问题的方法，但它们削弱了自训练的优势，因为它们需要对自训练算法或模型架构进行特定的修改。此外，大多数方法与在表格领域中占主导地位的梯度提升决策树不兼容。为了解决这个问题，我们重新考虑了群集假设，即相互接近的数据样本往往属于同一类。在此假设的启发下，我们提出了一种针对表格数据的群集感知自训练（CAST）方法。CAST是一种简单且普遍适应的方法，可以改进现有的自训练算法而无需进行大幅修改。具体而言，我们的方法规范了分类器的置信度，即伪标签的值，强制在低密度区域对伪标签进行限制。

    Self-training has gained attraction because of its simplicity and versatility, yet it is vulnerable to noisy pseudo-labels. Several studies have proposed successful approaches to tackle this issue, but they have diminished the advantages of self-training because they require specific modifications in self-training algorithms or model architectures. Furthermore, most of them are incompatible with gradient boosting decision trees, which dominate the tabular domain. To address this, we revisit the cluster assumption, which states that data samples that are close to each other tend to belong to the same class. Inspired by the assumption, we propose Cluster-Aware Self-Training (CAST) for tabular data. CAST is a simple and universally adaptable approach for enhancing existing self-training algorithms without significant modifications. Concretely, our method regularizes the confidence of the classifier, which represents the value of the pseudo-label, forcing the pseudo-labels in low-density r
    
[^223]: 基于神经切向核的联邦平均在深度线性神经网络上的视角

    A Neural Tangent Kernel View on Federated Averaging for Deep Linear Neural Network. (arXiv:2310.05495v1 [cs.LG])

    [http://arxiv.org/abs/2310.05495](http://arxiv.org/abs/2310.05495)

    本论文介绍了一种基于神经切向核视角的联邦平均方法在深度线性神经网络上的应用，并探讨了该方法面临的挑战。

    

    联邦平均（FedAvg）是一种广泛使用的范式，用于在不共享数据的情况下协同训练来自分布式客户端的模型。如今，由于其卓越性能，神经网络取得了显著的成功，这使得它成为FedAvg中的首选模型。然而，神经网络的优化问题通常是非凸的甚至是非光滑的。此外，FedAvg总是涉及多个客户端和本地更新，导致不准确的更新方向。这些属性给分析FedAvg在训练神经网络中的收敛性带来了困难。最近，神经切向核（NTK）理论已被提出，用于理解解决神经网络非凸问题中的一阶方法的收敛性。深度线性神经网络是理论学科中的经典模型，由于其简单的公式。然而，在训练深度线性神经网络上，对于FedAvg的收敛性目前还没有理论结果。

    Federated averaging (FedAvg) is a widely employed paradigm for collaboratively training models from distributed clients without sharing data. Nowadays, the neural network has achieved remarkable success due to its extraordinary performance, which makes it a preferred choice as the model in FedAvg. However, the optimization problem of the neural network is often non-convex even non-smooth. Furthermore, FedAvg always involves multiple clients and local updates, which results in an inaccurate updating direction. These properties bring difficulties in analyzing the convergence of FedAvg in training neural networks. Recently, neural tangent kernel (NTK) theory has been proposed towards understanding the convergence of first-order methods in tackling the non-convex problem of neural networks. The deep linear neural network is a classical model in theoretical subject due to its simple formulation. Nevertheless, there exists no theoretical result for the convergence of FedAvg in training the d
    
[^224]: 《来自彩票票集成的神经规模定律》

    A Neural Scaling Law from Lottery Ticket Ensembling. (arXiv:2310.02258v1 [cs.LG])

    [http://arxiv.org/abs/2310.02258](http://arxiv.org/abs/2310.02258)

    《来自彩票票集成的神经规模定律》通过研究神经规模定律现象，发现其与彩票票集成有关，从而形成了新的缩放定律，具有潜在的影响。

    

    神经规模定律（NSL）指的是模型性能随着规模增加而提高的现象。Sharma＆Kaplan使用近似理论分析了NSL，并预测了MSE损失的衰减方式为$N^{-\alpha}$，其中$\alpha=4/d$，$N$为模型参数数量，$d$为内在输入维度。尽管他们的理论在某些情况下效果良好（例如ReLU网络），但令人惊讶的是，我们发现在简单的1D问题$y=x^2$中，表现出了与他们预测不同的缩放定律（$\alpha=1$而不是$\alpha=4$）。我们打开了神经网络并发现新的缩放定律源于彩票票集成：平均而言，更宽的网络有更多的“彩票票”，它们被集成来减小输出的方差。我们通过对单个神经网络进行机械解释以及对它们进行统计研究来支持集成机制。我们将$N^{-1}$的缩放定律归因于“彩票票的中心极限定理”。最后，我们讨论了它的潜在影响。

    Neural scaling laws (NSL) refer to the phenomenon where model performance improves with scale. Sharma & Kaplan analyzed NSL using approximation theory and predict that MSE losses decay as $N^{-\alpha}$, $\alpha=4/d$, where $N$ is the number of model parameters, and $d$ is the intrinsic input dimension. Although their theory works well for some cases (e.g., ReLU networks), we surprisingly find that a simple 1D problem $y=x^2$ manifests a different scaling law ($\alpha=1$) from their predictions ($\alpha=4$). We opened the neural networks and found that the new scaling law originates from lottery ticket ensembling: a wider network on average has more "lottery tickets", which are ensembled to reduce the variance of outputs. We support the ensembling mechanism by mechanistically interpreting single neural networks, as well as studying them statistically. We attribute the $N^{-1}$ scaling law to the "central limit theorem" of lottery tickets. Finally, we discuss its potential implications f
    
[^225]: 来自EXMOS用户研究的经验教训：总结评估EXMOS平台的用户研究中的重要收获的技术报告

    Lessons Learned from EXMOS User Studies: A Technical Report Summarizing Key Takeaways from User Studies Conducted to Evaluate The EXMOS Platform. (arXiv:2310.02063v1 [cs.LG])

    [http://arxiv.org/abs/2310.02063](http://arxiv.org/abs/2310.02063)

    本技术报告总结了两项用户研究的重要发现，研究探索了在支持医疗专家优化机器学习模型的系统中，全局模型和数据中心解释对于检测和解决潜在的数据相关问题的有效性。

    

    在交互式机器学习系统领域，解释的提供在调试和增强预测模型的过程中起着重要的辅助作用。然而，不同的全局模型中心和数据中心解释在帮助领域专家检测和解决潜在数据相关问题以改进模型的目的上的有效性至今尚未得到大规模的探索。在这份技术报告中，我们总结了我们两项用户研究的主要发现。我们的研究涉及对基于数据中心和模型中心视角的全局解释在支持医疗专家通过自动和手动数据配置优化机器学习模型的系统中的影响进行全面研究。为了从经验上研究这些动态，我们进行了两项用户研究，包括涉及70位医疗专家的定量分析和涉及30位医疗专家的定性评估。

    In the realm of interactive machine-learning systems, the provision of explanations serves as a vital aid in the processes of debugging and enhancing prediction models. However, the extent to which various global model-centric and data-centric explanations can effectively assist domain experts in detecting and resolving potential data-related issues for the purpose of model improvement has remained largely unexplored. In this technical report, we summarise the key findings of our two user studies. Our research involved a comprehensive examination of the impact of global explanations rooted in both data-centric and model-centric perspectives within systems designed to support healthcare experts in optimising machine learning models through both automated and manual data configurations. To empirically investigate these dynamics, we conducted two user studies, comprising quantitative analysis involving a sample size of 70 healthcare experts and qualitative assessments involving 30 healthc
    
[^226]: 打破NoC匿名性使用流相关攻击

    Breaking NoC Anonymity using Flow Correlation Attack. (arXiv:2309.15687v1 [cs.CR])

    [http://arxiv.org/abs/2309.15687](http://arxiv.org/abs/2309.15687)

    本文研究了NoC架构中现有匿名路由协议的安全性，并展示了现有的匿名路由对基于机器学习的流相关攻击易受攻击。我们提出了一种轻量级的匿名路由，使用流量混淆技术，可以抵御基于机器学习的流相关攻击。

    

    网络片上互连（NoC）广泛用作当今多核片上系统（SoC）设计中的内部通信结构。片上通信的安全性至关重要，因为利用共享的NoC中的任何漏洞对攻击者来说都是一个富矿。NoC安全依赖于对各种攻击的有效防范措施。我们研究了NoC架构中现有匿名路由协议的安全性。具体而言，本文作出了两个重要贡献。我们展示了现有的匿名路由对基于机器学习（ML）的流相关攻击是易受攻击的。我们提出了一种轻量级的匿名路由，使用流量混淆技术，可以抵御基于ML的流相关攻击。使用实际和合成流量进行的实验研究表明，我们提出的攻击能够成功地对抗NoC架构中最先进的匿名路由，对于多种流量模式的分类准确率高达99％，同时。

    Network-on-Chip (NoC) is widely used as the internal communication fabric in today's multicore System-on-Chip (SoC) designs. Security of the on-chip communication is crucial because exploiting any vulnerability in shared NoC would be a goldmine for an attacker. NoC security relies on effective countermeasures against diverse attacks. We investigate the security strength of existing anonymous routing protocols in NoC architectures. Specifically, this paper makes two important contributions. We show that the existing anonymous routing is vulnerable to machine learning (ML) based flow correlation attacks on NoCs. We propose a lightweight anonymous routing that use traffic obfuscation techniques which can defend against ML-based flow correlation attacks. Experimental studies using both real and synthetic traffic reveal that our proposed attack is successful against state-of-the-art anonymous routing in NoC architectures with a high accuracy (up to 99%) for diverse traffic patterns, while o
    
[^227]: FRAMU: 基于注意力的联邦强化学习机器遗忘

    FRAMU: Attention-based Machine Unlearning using Federated Reinforcement Learning. (arXiv:2309.10283v1 [cs.LG])

    [http://arxiv.org/abs/2309.10283](http://arxiv.org/abs/2309.10283)

    FRAMU是一种基于注意力和联邦强化学习的机器遗忘框架，通过自适应学习机制、隐私保护技术和优化策略，处理各种数据源并保持准确性和隐私，适应波动的数据环境，支持持续模型演进。

    

    机器遗忘是一个新兴领域，通过允许从机器学习过程中删除私有或无关数据，解决数据隐私问题。使用过时的、私有的和无关的数据会引发与隐私和模型效率相关的挑战。这些问题不仅影响模型在机器学习和遗忘中的准确性和计算效率，还会对数据隐私造成威胁。为了解决这些挑战，我们引入了一种新颖的框架，即基于注意力的联邦强化学习机器遗忘（FRAMU）。该框架融合了自适应学习机制、隐私保护技术和优化策略，是处理各种数据源（单模态或多模态）同时保持准确性和隐私性的综合解决方案。FRAMU的优势在于其适应波动的数据环境、遗忘过时、私有或无关数据的能力，以及支持模型持续演进的支持。

    Machine Unlearning is an emerging field that addresses data privacy issues by enabling the removal of private or irrelevant data from the Machine Learning process. Challenges related to privacy and model efficiency arise from the use of outdated, private, and irrelevant data. These issues compromise both the accuracy and the computational efficiency of models in both Machine Learning and Unlearning. To mitigate these challenges, we introduce a novel framework, Attention-based Machine Unlearning using Federated Reinforcement Learning (FRAMU). This framework incorporates adaptive learning mechanisms, privacy preservation techniques, and optimization strategies, making it a well-rounded solution for handling various data sources, either single-modality or multi-modality, while maintaining accuracy and privacy. FRAMU's strength lies in its adaptability to fluctuating data landscapes, its ability to unlearn outdated, private, or irrelevant data, and its support for continual model evolution
    
[^228]: 基于图增强的自适应智能时间序列预测强化学习

    Graph-enabled Reinforcement Learning for Time Series Forecasting with Adaptive Intelligence. (arXiv:2309.10186v1 [cs.LG])

    [http://arxiv.org/abs/2309.10186](http://arxiv.org/abs/2309.10186)

    本研究提出了一种使用图神经网络和强化学习监控的新方法，以更准确地预测时间序列数据。该方法能够克服深度学习的限制，并捕捉复杂的时序结构中的时间依赖关系，适用于医疗、交通和天气预测等领域。

    

    强化学习以其能够自适应地建模序列任务和学习潜在数据模式的能力而闻名。深度学习模型在回归和分类任务中得到了广泛的探索和应用。然而，深度学习存在一些限制，例如假设数据等间隔有序以及无法充分融入图结构等。图神经网络(GNN)能够克服这些挑战并捕捉时间序列数据的时间依赖关系。 在本研究中，我们提出了一种使用GNN和强化学习监控预测时间序列数据的新方法。GNN能够显式地将数据的图结构纳入模型，使其能够以更自然的方式捕捉时间依赖关系。该方法能够在复杂的时序结构中进行更准确的预测，例如医疗、交通和天气预测中的时序数据。

    Reinforcement learning is well known for its ability to model sequential tasks and learn latent data patterns adaptively. Deep learning models have been widely explored and adopted in regression and classification tasks. However, deep learning has its limitations such as the assumption of equally spaced and ordered data, and the lack of ability to incorporate graph structure in terms of time-series prediction. Graphical neural network (GNN) has the ability to overcome these challenges and capture the temporal dependencies in time-series data. In this study, we propose a novel approach for predicting time-series data using GNN and monitoring with Reinforcement Learning (RL). GNNs are able to explicitly incorporate the graph structure of the data into the model, allowing them to capture temporal dependencies in a more natural way. This approach allows for more accurate predictions in complex temporal structures, such as those found in healthcare, traffic and weather forecasting. We also 
    
[^229]: 使用声音提示进行分割的泛化音频-视觉源定位器

    Prompting Segmentation with Sound is Generalizable Audio-Visual Source Localizer. (arXiv:2309.07929v1 [cs.CV])

    [http://arxiv.org/abs/2309.07929](http://arxiv.org/abs/2309.07929)

    本研究提出了一种使用声音提示进行分割的泛化音频-视觉源定位器，在零样本和少样本情况下实现音频-视觉定位和分割任务。通过引入编码器提示解码器范式、构建语义感知音频提示和相关适配器来解决数据稀缺性和不同数据分布的困境。

    

    在从未同时看到物体和听到其声音的情况下，模型是否仍然能够准确地从输入音频中定位其视觉位置？在这项工作中，我们关注零样本和少样本情况下的音频-视觉定位和分割任务。为了实现这个目标，我们引入了编码器提示解码器的范式，与现有方法不同，现有方法主要使用编码器融合解码器范式从融合音频-视觉特征中解码定位信息，我们旨在借助预训练模型的丰富知识来更好地适应数据稀缺性和不同数据分布的困境。具体地，我们首先提出构建语义感知音频提示（SAP）来帮助视觉基础模型关注有声对象，同时也鼓励视觉和音频模态之间的语义差距缩小。然后，我们开发了一个相关适配器（ColA）来保持最小的训练工作量并维持模型性能。

    Never having seen an object and heard its sound simultaneously, can the model still accurately localize its visual position from the input audio? In this work, we concentrate on the Audio-Visual Localization and Segmentation tasks but under the demanding zero-shot and few-shot scenarios. To achieve this goal, different from existing approaches that mostly employ the encoder-fusion-decoder paradigm to decode localization information from the fused audio-visual feature, we introduce the encoder-prompt-decoder paradigm, aiming to better fit the data scarcity and varying data distribution dilemmas with the help of abundant knowledge from pre-trained models. Specifically, we first propose to construct Semantic-aware Audio Prompt (SAP) to help the visual foundation model focus on sounding objects, meanwhile, the semantic gap between the visual and audio modalities is also encouraged to shrink. Then, we develop a Correlation Adapter (ColA) to keep minimal training efforts as well as maintain 
    
[^230]: MagiCapture: 高分辨率多概念人像定制

    MagiCapture: High-Resolution Multi-Concept Portrait Customization. (arXiv:2309.06895v1 [cs.CV])

    [http://arxiv.org/abs/2309.06895](http://arxiv.org/abs/2309.06895)

    MagiCapture是一种高分辨率多概念人像定制方法，通过几个主题和风格参考，能够生成高质量的特定风格人像图像。

    

    大规模的文本到图像模型，包括稳定扩散，能够生成高保真度的逼真人像照片。有一个专门研究个性化这些模型的领域，旨在使用提供的参考图像集合合成特定主题或风格。然而，尽管这些个性化方法产生的结果令人满意，但其生成的图像往往缺乏真实感，并且尚未达到商业可行的水平。在人像图像生成中尤为明显，因为由于我们内在的人类偏见，人脸中的任何不自然的痕迹都很容易被识别出来。为了解决这个问题，我们引入了MagiCapture，一种个性化方法，用于将主题和风格概念融合，仅使用几个主题和风格参考即可生成高分辨率的人像图像。例如，给定一些随机的自拍照，我们经过调优的模型就可以生成特定风格（如护照或个人资料）的高质量人像图像。

    Large-scale text-to-image models including Stable Diffusion are capable of generating high-fidelity photorealistic portrait images. There is an active research area dedicated to personalizing these models, aiming to synthesize specific subjects or styles using provided sets of reference images. However, despite the plausible results from these personalization methods, they tend to produce images that often fall short of realism and are not yet on a commercially viable level. This is particularly noticeable in portrait image generation, where any unnatural artifact in human faces is easily discernible due to our inherent human bias. To address this, we introduce MagiCapture, a personalization method for integrating subject and style concepts to generate high-resolution portrait images using just a few subject and style references. For instance, given a handful of random selfies, our fine-tuned model can generate high-quality portrait images in specific styles, such as passport or profil
    
[^231]: 基于${\rm E}(3)$等变性的协作多智体强化学习的演员-评论家方法

    ${\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2308.11842v1 [cs.MA])

    [http://arxiv.org/abs/2308.11842](http://arxiv.org/abs/2308.11842)

    本文利用欧几里德对称性，设计了一种基于${\rm E}(3)$等变性的协作多智体强化学习的演员-评论家方法，通过对称约束的神经网络结构，实现了在各种协作MARL基准测试中的卓越性能和令人印象深刻的泛化能力。

    

    在自然界中识别和分析对称模式已经在各个科学领域取得了重要的发现，例如物理学中的引力定律的制定和化学结构研究的进展。本文着重于利用在某些协作多智体强化学习（MARL）问题中固有的欧几里德对称性，以及在许多应用中普遍存在的对称性。我们首先通过形式化地描述一类具有一般对称性概念的马尔可夫博弈，该概念允许存在对称的最优值和策略。受到这些性质的启发，我们设计了具有对称约束的神经网络结构，作为多智体演员-评论家方法的归纳偏差。这种归纳偏差在各种协作MARL基准测试中表现出卓越的性能，并具有零样本学习和在具有重复对称模式的未见场景中的迁移学习等令人印象深刻的泛化能力。

    Identification and analysis of symmetrical patterns in the natural world have led to significant discoveries across various scientific fields, such as the formulation of gravitational laws in physics and advancements in the study of chemical structures. In this paper, we focus on exploiting Euclidean symmetries inherent in certain cooperative multi-agent reinforcement learning (MARL) problems and prevalent in many applications. We begin by formally characterizing a subclass of Markov games with a general notion of symmetries that admits the existence of symmetric optimal values and policies. Motivated by these properties, we design neural network architectures with symmetric constraints embedded as an inductive bias for multi-agent actor-critic methods. This inductive bias results in superior performance in various cooperative MARL benchmarks and impressive generalization capabilities such as zero-shot learning and transfer learning in unseen scenarios with repeated symmetric patterns.
    
[^232]: 利用光电容积描记信号中的标签传播技术进行不平衡类别中的伪迹检测

    Label Propagation Techniques for Artifact Detection in Imbalanced Classes using Photoplethysmogram Signals. (arXiv:2308.08480v1 [cs.LG])

    [http://arxiv.org/abs/2308.08480](http://arxiv.org/abs/2308.08480)

    研究探索了在光电容积描记信号中利用标签传播技术进行不平衡类别中伪迹检测的方法，并证明其在标记医疗数据集和伪迹分类方面的有效性。

    

    光电容积描记信号在医疗保健中被广泛用于监测生命体征，但它们容易受到运动伪迹的影响，从而导致不准确的解释。本研究探索了在不平衡类别场景中使用标签传播技术在PPG样本之间传播标签的方法，其中干净的PPG样本明显少于受伪迹污染的样本。结果显示，在没有伪迹的类别中，精确度为91%，召回率为90%，F1得分为90%，证明了其在标记医疗数据集方面的有效性，即使干净样本很少。对于伪迹的分类，我们的研究比较了传统分类器和神经网络（MLP、Transformers、FCN）等有监督分类器与半监督标签传播算法。KNN有监督模型具有89%的精确度、95%的召回率和92%的F1得分，结果良好，但半监督算法在检测方面表现更好。

    Photoplethysmogram (PPG) signals are widely used in healthcare for monitoring vital signs, but they are susceptible to motion artifacts that can lead to inaccurate interpretations. In this study, the use of label propagation techniques to propagate labels among PPG samples is explored, particularly in imbalanced class scenarios where clean PPG samples are significantly outnumbered by artifact-contaminated samples. With a precision of 91%, a recall of 90% and an F1 score of 90% for the class without artifacts, the results demonstrate its effectiveness in labeling a medical dataset, even when clean samples are rare. For the classification of artifacts our study compares supervised classifiers such as conventional classifiers and neural networks (MLP, Transformers, FCN) with the semi-supervised label propagation algorithm. With a precision of 89%, a recall of 95% and an F1 score of 92%, the KNN supervised model gives good results, but the semi-supervised algorithm performs better in detec
    
[^233]: 《你不得通过：凸优化中的零梯度问题》

    You Shall not Pass: the Zero-Gradient Problem in Predict and Optimize for Convex Optimization. (arXiv:2307.16304v1 [cs.LG])

    [http://arxiv.org/abs/2307.16304](http://arxiv.org/abs/2307.16304)

    这篇论文介绍了预测和优化中的一个问题——零梯度问题，并提出了解决方法。通过利用微分优化的数学特性和真实世界基准的验证，该方法解决了这个问题。

    

    预测和优化是一种越来越受欢迎的决策模式，它利用机器学习来预测优化问题中的未知参数。与最小化参数预测误差不同，它使用任务性能作为损失函数来训练预测模型。在凸优化领域，预测和优化由于最近开发的方法能够对问题参数进行微分，取得了显著进展。本文发现了这种方法的一个迄今未被注意到的缺点——零梯度问题，并提出了一种解决方法。建议的方法基于微分优化的数学特性，并使用两个真实世界的基准进行验证。

    Predict and optimize is an increasingly popular decision-making paradigm that employs machine learning to predict unknown parameters of optimization problems. Instead of minimizing the prediction error of the parameters, it trains predictive models using task performance as a loss function. In the convex optimization domain, predict and optimize has seen significant progress due to recently developed methods for differentiating optimization problem solutions over the problem parameters. This paper identifies a yet unnoticed drawback of this approach -- the zero-gradient problem -- and introduces a method to solve it. The suggested method is based on the mathematical properties of differential optimization and is verified using two real-world benchmarks.
    
[^234]: 洋葱宇宙算法：在弱监督学习中的应用

    Onion Universe Algorithm: Applications in Weakly Supervised Learning. (arXiv:2307.04870v1 [cs.LG])

    [http://arxiv.org/abs/2307.04870](http://arxiv.org/abs/2307.04870)

    洋葱宇宙算法是一种新颖的集成学习分类方法，可作为弱监督学习的标签模型。它在实现上简单、计算高效，适用于无完全标记数据的情况。经验证明，它在常见的基准数据集上表现出色。

    

    本文介绍了洋葱宇宙算法(OUA)，一种新颖的集成学习分类方法。特别地，我们展示了它作为弱监督学习标签模型的适用性。OUA在实现上简单，计算效率高，并且不依赖于数据或弱信号的任何假设。该模型非常适用于没有完全标记数据的情况。我们的方法基于对由弱信号所构成的空间的几何解释。经验证实，OUA在一般的弱信号集合下具有潜在的几何结构，并且在实践中表现良好。我们还通过实验证据展示，OUA在常见的基准数据集上相比现有的弱监督学习标签模型表现出色。

    We introduce Onion Universe Algorithm (OUA), a novel classification method in ensemble learning. In particular, we show its applicability as a label model for weakly supervised learning. OUA offers simplicity in implementation, computational efficiency, and does not rely on any assumptions regarding the data or weak signals. The model is well suited for scenarios where fully labeled data is not available. Our method is built upon geometrical interpretation of the space spanned by weak signals. Empirical results support our analysis of the hidden geometric structure underlying general set of weak signals and also illustrates that OUA works well in practice. We show empirical evidence that OUA performs favorably on common benchmark datasets compared to existing label models for weakly supervised learning.
    
[^235]: 使用下一代储层计算控制混沌映射

    Controlling Chaotic Maps using Next-Generation Reservoir Computing. (arXiv:2307.03813v1 [cs.LG])

    [http://arxiv.org/abs/2307.03813](http://arxiv.org/abs/2307.03813)

    这项工作将非线性系统控制技术与储层计算相结合，成功地在混沌H\'enon映射上展示了控制器对于控制系统的稳定、固定点的控制和任意期望状态的控制的性能，并且只需10个数据点进行训练，单次迭代就能控制到期望轨迹，并且具有鲁棒性。

    

    在这项工作中，我们将非线性系统控制技术与最先进的储层计算相结合，储层计算是一种用于预测动力系统行为的机器学习方法。我们在混沌H\'enon映射上展示了控制器在一系列控制任务中的性能，包括在不稳定的固定点之间控制系统，将系统稳定到更高阶周期轨道，并控制系统到任意期望状态。我们展示了我们的控制器在这些任务中成功，并且仅需10个数据点进行训练，可以在单次迭代中将系统控制到期望轨迹，并且对噪声和建模误差具有鲁棒性。

    In this work, we combine nonlinear system control techniques with next-generation reservoir computing, a best-in-class machine learning approach for predicting the behavior of dynamical systems. We demonstrate the performance of the controller in a series of control tasks for the chaotic H\'enon map, including controlling the system between unstable fixed-points, stabilizing the system to higher order periodic orbits, and to an arbitrary desired state. We show that our controller succeeds in these tasks, requires only 10 data points for training, can control the system to a desired trajectory in a single iteration, and is robust to noise and modeling error.
    
[^236]: 学习使用对比学习进行通信

    Learning to Communicate using Contrastive Learning. (arXiv:2307.01403v1 [cs.AI])

    [http://arxiv.org/abs/2307.01403](http://arxiv.org/abs/2307.01403)

    本研究提出了一种使用对比学习进行通信的方法，在分散的环境中通过最大化反映发送和接收消息关系的互信息来学习通信。在通信关键的环境中，我们的方法在性能和学习速度方面优于先前的工作，并且能够捕获全局状态信息，实现了更对称的通信。

    

    通信是多智能体强化学习中协调的有力工具。但在分散的环境中诱导一个有效的共同语言是一个困难的挑战。在这项工作中，我们引入了一个替代视角，即将智能体之间发送的通信消息视为环境状态的不完整视图。通过检查发送和接收的消息之间的关系，我们提出使用对比学习来最大化给定轨迹的消息之间的互信息来学习通信。在通信关键的环境中，我们的方法在性能和学习速度方面优于先前的工作。使用定性指标和表示探测，我们展示了我们的方法诱导了更对称的通信并从环境中捕获了全局状态信息。总体而言，我们展示了对比学习的力量以及利用消息作为编码实现有效通信的重要性。

    Communication is a powerful tool for coordination in multi-agent RL. But inducing an effective, common language is a difficult challenge, particularly in the decentralized setting. In this work, we introduce an alternative perspective where communicative messages sent between agents are considered as different incomplete views of the environment state. By examining the relationship between messages sent and received, we propose to learn to communicate using contrastive learning to maximize the mutual information between messages of a given trajectory. In communication-essential environments, our method outperforms previous work in both performance and learning speed. Using qualitative metrics and representation probing, we show that our method induces more symmetric communication and captures global state information from the environment. Overall, we show the power of contrastive learning and the importance of leveraging messages as encodings for effective communication.
    
[^237]: 深度图核点过程

    Deep graph kernel point processes. (arXiv:2306.11313v1 [stat.ML])

    [http://arxiv.org/abs/2306.11313](http://arxiv.org/abs/2306.11313)

    本文提出了一种基于潜在图拓扑的图点过程方法，并开发了一种新颖的深度图核来描述事件之间的触发和抑制效应，该方法在合成和实际数据集上具有优越性。

    

    点过程模型广泛用于分析图中异步事件，反映不同类型事件之间的相互影响。预测未来事件的时间和类型是一项关键任务，并且图的大小和拓扑结构增加了问题的难度。最近的神经点过程模型揭示了捕捉复杂的事件类别之间依赖关系的可能性。然而，这些方法在每个目标事件类型的强度计算中使用了包括所有事件类别在内的未经滤波的事件记录。在本文中，我们提出了一种基于潜在图拓扑的图点过程方法。对应的无向图具有代表事件类别的节点和表示潜在贡献关系的边。然后，我们开发了一种新颖的深度图核来描述事件之间的触发和抑制效应。本质影响结构通过图神经网络-based的局部邻域信息聚合进行了融合。我们在合成和实际数据集上展示了我们提出的方法比最先进的模型更具优越性。

    Point process models are widely used to analyze asynchronous events occurring within a graph that reflect how different types of events influence one another. Predicting future events' times and types is a crucial task, and the size and topology of the graph add to the challenge of the problem. Recent neural point process models unveil the possibility of capturing intricate inter-event-category dependencies. However, such methods utilize an unfiltered history of events, including all event categories in the intensity computation for each target event type. In this work, we propose a graph point process method where event interactions occur based on a latent graph topology. The corresponding undirected graph has nodes representing event categories and edges indicating potential contribution relationships. We then develop a novel deep graph kernel to characterize the triggering and inhibiting effects between events. The intrinsic influence structures are incorporated via the graph neural
    
[^238]: 走向量子联邦学习

    Towards Quantum Federated Learning. (arXiv:2306.09912v1 [cs.LG])

    [http://arxiv.org/abs/2306.09912](http://arxiv.org/abs/2306.09912)

    量子联邦学习通过将量子计算和联邦学习原理相结合，旨在利用量子技术增强学习过程中的隐私、安全和效率，并通过独特的分类法分类总结了这一快速发展领域的技术特点和未来研究方向。

    

    量子联邦学习是一个新兴的交叉学科领域，将量子计算和联邦学习的原理相结合，旨在利用量子技术增强学习过程中的隐私、安全和效率。目前尚无关于该交叉学科领域的全面调查。本文对量子联邦学习进行了全面细致的探讨。我们旨在提供对量子联邦学习的原理、技术以及新兴应用的全面理解。我们讨论了这一快速发展领域的现状，确定了整合这些技术所面临的挑战和机遇，并概述了未来的方向和开放性研究问题。我们提出了一种独特的分类法，将量子联邦学习技术按其特征和所采用的量子技术分类。随着量子联邦学习领域的不断发展，我们可以预计将在各个行业实现更多的突破和应用。

    Quantum Federated Learning (QFL) is an emerging interdisciplinary field that merges the principles of Quantum Computing (QC) and Federated Learning (FL), with the goal of leveraging quantum technologies to enhance privacy, security, and efficiency in the learning process. Currently, there is no comprehensive survey for this interdisciplinary field. This review offers a thorough, holistic examination of QFL. We aim to provide a comprehensive understanding of the principles, techniques, and emerging applications of QFL. We discuss the current state of research in this rapidly evolving field, identify challenges and opportunities associated with integrating these technologies, and outline future directions and open research questions. We propose a unique taxonomy of QFL techniques, categorized according to their characteristics and the quantum techniques employed. As the field of QFL continues to progress, we can anticipate further breakthroughs and applications across various industries,
    
[^239]: K-Tensors：对正半定矩阵进行聚类

    K-Tensors: Clustering Positive Semi-Definite Matrices. (arXiv:2306.06534v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.06534](http://arxiv.org/abs/2306.06534)

    本文介绍了一种针对正半定矩阵的自一致性聚类算法（K-张量），通过考虑其特征结构，能够有效地将正半定矩阵进行分区。

    

    本文介绍了一种新颖的自一致性聚类算法（K-Tensors），用于基于它们的特征结构将正半定矩阵进行分区。由于正半定矩阵可以在 p≥2 的空间中表示为椭球体，因此保持它们的结构信息以进行有效的聚类至关重要。然而，传统的矩阵聚类算法常常涉及将矩阵向量化，导致关键结构信息的丢失。为了解决这个问题，我们提出了一种基于正半定矩阵结构信息的距离度量来进行聚类。这种距离度量使得聚类算法能够考虑正半定矩阵与它们在由一组正半定矩阵定义的正交向量张成的共同空间上的投影之间的差异。这是一种创新的聚类方法。

    This paper introduces a novel self-consistency clustering algorithm ($K$-Tensors) designed for {partitioning a distribution of} positive-semidefinite matrices based on their eigenstructures. As positive semi-definite matrices can be represented as ellipsoids in $\Re^p$, $p \ge 2$, it is critical to maintain their structural information to perform effective clustering. However, traditional clustering algorithms {applied to matrices} often {involve vectorization of} the matrices, resulting in a loss of essential structural information. To address this issue, we propose a distance metric {for clustering} that is specifically based on the structural information of positive semi-definite matrices. This distance metric enables the clustering algorithm to consider the differences between positive semi-definite matrices and their projections onto {a} common space spanned by \thadJulyTen{orthonormal vectors defined from a set of} positive semi-definite matrices. This innovative approach to clus
    
[^240]: 序数势函数的玩家评级方法

    Ordinal Potential-based Player Rating. (arXiv:2306.05366v1 [cs.GT])

    [http://arxiv.org/abs/2306.05366](http://arxiv.org/abs/2306.05366)

    该论文提出了一种使用序数势函数的玩家评级方法，通过可逆映射的嵌套计算，能够保持传递性。

    

    如果对于任意纯策略$x$、$y$和$z$，如果$x$比$y$更好，$y$比$z$更好，则$x$比$z$更好，则两个对称的零和博弈是可传递的。最近观察到，Elo评级未能保持策略之间的传递关系，因此不能正确提取游戏的传递组件。我们的第一个贡献是表明当在正确的空间中计算Elo评级时，Elo评级确实能够保持传递性。具体而言，我们首先使用合适的可逆映射$\varphi$将游戏应用于$\varphi$，然后计算Elo评级，最后通过应用$\varphi^{-1}$回到原始空间。我们将可传递游戏的表征为势游戏的一个弱变体，其势函数是加性可分离的。利用这一洞见，我们引入了传递序数的概念，即将可传递游戏的收益转化为其差异所需的最小可逆映射数。

    A two-player symmetric zero-sum game is transitive if for any pure strategies $x$, $y$, $z$, if $x$ is better than $y$, and $y$ is better than $z$, then $x$ is better than $z$. It was recently observed that the Elo rating fails at preserving transitive relations among strategies and therefore cannot correctly extract the transitive component of a game. Our first contribution is to show that the Elo rating actually does preserve transitivity when computed in the right space. Precisely, using a suitable invertible mapping $\varphi$, we first apply $\varphi$ to the game, then compute Elo ratings, then go back to the original space by applying $\varphi^{-1}$. We provide a characterization of transitive games as a weak variant of ordinal potential games with additively separable potential functions. Leveraging this insight, we introduce the concept of transitivity order, the minimum number of invertible mappings required to transform the payoff of a transitive game into (differences of) its
    
[^241]: 关于半线性椭圆PDE中非光滑超定算子的识别和优化

    On the Identification and Optimization of Nonsmooth Superposition Operators in Semilinear Elliptic PDEs. (arXiv:2306.05185v1 [math.OC])

    [http://arxiv.org/abs/2306.05185](http://arxiv.org/abs/2306.05185)

    本文研究了在低正则性情况下，如何识别半线性椭圆PDE中的Nemytskii算子以及如何解决这个优化问题。这对于进行有关PDE的神经网络训练问题有很好的启示作用。

    

    我们研究了一个无限维优化问题，目的是识别半线性椭圆偏微分方程(PDE)非线性部分中的Nemytskii算子，该算子将PDE解与给定的期望状态之间的距离最小化。与以前的工作不同，我们在低正则性情况下考虑了这个识别问题，在这种情况下引起Nemytskii算子函数先验仅被认为是H^1_{loc}(\mathbb{R})的元素。这使得研究的问题类成为一种适合从严格分析的角度来处理学习有关PDE的培训问题的出发点，其中未知的超定算子是通过使用非光滑激活函数(ReLU，leaky-ReLU等)的神经网络逼近的。我们证明，尽管控制的正则性较低，但可以为局部极小值导出经典的站点系统，并通过梯度投影法解决所考虑的问题。

    We study an infinite-dimensional optimization problem that aims to identify the Nemytskii operator in the nonlinear part of a prototypical semilinear elliptic partial differential equation (PDE) which minimizes the distance between the PDE-solution and a given desired state. In contrast to previous works, we consider this identification problem in a low-regularity regime in which the function inducing the Nemytskii operator is a-priori only known to be an element of $H^1_{loc}(\mathbb{R})$. This makes the studied problem class a suitable point of departure for the rigorous analysis of training problems for learning-informed PDEs in which an unknown superposition operator is approximated by means of a neural network with nonsmooth activation functions (ReLU, leaky-ReLU, etc.). We establish that, despite the low regularity of the controls, it is possible to derive a classical stationarity system for local minimizers and to solve the considered problem by means of a gradient projection me
    
[^242]: 基于条件扩散模型的语义化三维医学图像合成

    Conditional Diffusion Models for Semantic 3D Medical Image Synthesis. (arXiv:2305.18453v1 [eess.IV])

    [http://arxiv.org/abs/2305.18453](http://arxiv.org/abs/2305.18453)

    这篇论文提出了Med-DDPM，一种使用扩散模型进行语义化三维医学图像合成的创新解决方案，它通过控制像素级掩码标签的生成过程，能够生成高质量逼真的医学图像，并且在精度、稳定性和多样性等指标上优于GAN技术，也优于传统的增强技术和GAN合成图像。

    

    本文提出了Med-DDPM，它是一种创新的解决方案，使用扩散模型进行语义化的三维医学图像合成，解决了医学成像中数据稀缺、采集方法不一致和隐私问题等普遍存在的问题。实验结果表明，扩散模型在稳定性和性能方面都超过了生成对抗网络（GAN），能够生成高质量、逼真的三维医学图像。Med-DDPM的独特特点在于使用语义条件进行三维图像合成的扩散模型。通过控制像素级掩码标签的生成过程，它便于创建逼真的医学图像。经验证明，Med-DDPM在精度、稳定性和多样性等指标上优于GAN技术。此外，Med-DDPM在增强分割模型的准确性方面也优于传统的增强技术和GAN合成图像。它解决了医学图像合成中的难点。

    This paper introduces Med-DDPM, an innovative solution using diffusion models for semantic 3D medical image synthesis, addressing the prevalent issues in medical imaging such as data scarcity, inconsistent acquisition methods, and privacy concerns. Experimental evidence illustrates that diffusion models surpass Generative Adversarial Networks (GANs) in stability and performance, generating high-quality, realistic 3D medical images. The distinct feature of Med-DDPM is its use of semantic conditioning for the diffusion model in 3D image synthesis. By controlling the generation process through pixel-level mask labels, it facilitates the creation of realistic medical images. Empirical evaluations underscore the superior performance of Med-DDPM over GAN techniques in metrics such as accuracy, stability, and versatility. Furthermore, Med-DDPM outperforms traditional augmentation techniques and synthetic GAN images in enhancing the accuracy of segmentation models. It addresses challenges such
    
[^243]: 论解码器Transformer语言模型的计算能力

    On the Computational Power of Decoder-Only Transformer Language Models. (arXiv:2305.17026v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17026](http://arxiv.org/abs/2305.17026)

    本篇论文研究了解码器Transformer语言模型的计算普适性，表明即使只有单层和单注意力头，仍然具有图灵完备性，其中单词嵌入的稀疏性/可压缩性是必要条件。

    

    本文章对解码器Transformer模型的计算普适性进行了理论评估。我们扩展了Transformer模型的理论文献，并表明仅使用单层和单注意力头的解码器Transformer结构，在合理假设下具备图灵完备性。从理论分析中，我们证明了单词嵌入的稀疏性/可压缩性是图灵完备性成立的必要条件。

    This article presents a theoretical evaluation of the computational universality of decoder-only transformer models. We extend the theoretical literature on transformer models and show that decoder-only transformer architectures (even with only a single layer and single attention head) are Turing complete under reasonable assumptions. From the theoretical analysis, we show sparsity/compressibility of the word embedding to be a necessary condition for Turing completeness to hold.
    
[^244]: 用最优传输学习有向图模型

    Learning Directed Graphical Models with Optimal Transport. (arXiv:2305.15927v1 [cs.LG])

    [http://arxiv.org/abs/2305.15927](http://arxiv.org/abs/2305.15927)

    通过最优传输的视角提供了参数学习问题的新视图，可以在许多有向图上进行操作并表现出灵活性和多功能性。

    

    从不完整的数据中估计概率有向图模型的参数仍然是一个长期存在的挑战。这是因为在存在潜在变量的情况下，如果没有关于结构依赖性或模型类的进一步假设，似然函数和后验分布都是不可计算的。虽然现有的学习方法基本上是基于最大似然估计，但我们在这里通过最优传输的视角提供了参数学习问题的一个新视图。这个观点授权了一个框架，可以在许多有向图上运作，而不会对潜在变量的后验做出不切实际的假设或诉诸于黑箱变分近似。我们开发了一个理论框架，并支持它通过广泛的经验证据，展示了我们方法的灵活性和多功能性。通过实验，我们展示了我们的方法不仅可以恢复基准参数，而且在性能方面也表现得有竞争力。

    Estimating the parameters of a probabilistic directed graphical model from incomplete data remains a long-standing challenge. This is because, in the presence of latent variables, both the likelihood function and posterior distribution are intractable without further assumptions about structural dependencies or model classes. While existing learning methods are fundamentally based on likelihood maximization, here we offer a new view of the parameter learning problem through the lens of optimal transport. This perspective licenses a framework that operates on many directed graphs without making unrealistic assumptions on the posterior over the latent variables or resorting to black-box variational approximations. We develop a theoretical framework and support it with extensive empirical evidence demonstrating the flexibility and versatility of our approach. Across experiments, we show that not only can our method recover the ground-truth parameters but it also performs competitively on 
    
[^245]: 如何逃离锐化的极小值点

    How to escape sharp minima. (arXiv:2305.15659v1 [cs.LG])

    [http://arxiv.org/abs/2305.15659](http://arxiv.org/abs/2305.15659)

    本文探讨了发现平坦极小值点的算法问题，在支持找到局部近似平坦的极小值点的基础上，设计了两种算法：一种基于梯度的算法，一种基于最小化锐度的算法。

    

    现代机器学习应用程序中的优化算法已经取得了显著的成功，这些算法被设计用来发现平坦的极小值点。为了解决这个算法问题，本文采用损失函数海森矩阵的迹来度量它的平坦程度，并形式化定义了近似平坦极小值点的概念。在此概念下，我们设计了有效地找到近似平坦极小值点的算法。针对一般的损失函数，我们提出了一种基于梯度的算法，可以有效地找到近似平坦的局部极小值点。算法的主要组件是使用从随机扰动迭代中计算的梯度来估计导致更平坦极小值点的方向。对于成本函数是训练数据上的经验风险的设置，我们提出了一种更快速的算法，受最近提出的实用算法——锐度感知最小化的启发。

    Modern machine learning applications have seen a remarkable success of optimization algorithms that are designed to find flat minima. Motivated by this paradigm, this work formulates and studies the algorithmic question of how to find flat minima. As an initial effort, this work adopts the trace of hessian of the cost function as the measure of flatness, and formally defines the notion of approximate flat minima. Under this notion, we then design algorithms that find approximate flat minima efficiently. For general cost functions, we present a gradient-based algorithm that finds an approximate flat local minimum efficiently. The main component of the algorithm is to use gradients computed from randomly perturbed iterates to estimate a direction that leads to flatter minima. For the setting where the cost function is an empirical risk over training data, we present a faster algorithm that is inspired by a recently proposed practical algorithm called sharpness-aware minimization, support
    
[^246]: 采用局部线性模型的变分梯度下降

    Variational Gradient Descent using Local Linear Models. (arXiv:2305.15577v1 [stat.ML])

    [http://arxiv.org/abs/2305.15577](http://arxiv.org/abs/2305.15577)

    本文提出了使用局部线性模型实现目标和粒子分布KL散度降低的新估计器，可以使用样本进行计算而不需要目标得分函数，具有比SVGD更简单有效的计算方法，对于高维度情况下的模型也有优化，提升估计精度。

    

    Stein Variational Gradient Descent (SVGD) 能够沿着轨迹传输粒子，从而减少目标和粒子分布之间的KL散度，但需要目标得分函数来计算更新。我们提出了一种新的SVGD视角，将其视为反向KL梯度流的局部估计器。这种视角启发我们提出了使用局部线性模型来实现相同目的的新估计器。这些提议的估计器可以仅使用目标和粒子分布的样本进行计算，而不需要目标得分函数。我们提议的变分梯度估计器利用了局部线性模型，从而在保持估计偏差与SVGD相当的效果的同时具有计算简便性。此外，我们证明，在温和的假设下，高维梯度流的估计可以转化为一个低维估计问题，从而导致更好的估计精度。我们对提议的方法进行了验证，并对其进行了比较。

    Stein Variational Gradient Descent (SVGD) can transport particles along trajectories that reduce the KL divergence between the target and particle distribution but requires the target score function to compute the update. We introduce a new perspective on SVGD that views it as a local estimator of the reversed KL gradient flow. This perspective inspires us to propose new estimators that use local linear models to achieve the same purpose. The proposed estimators can be computed using only samples from the target and particle distribution without needing the target score function. Our proposed variational gradient estimators utilize local linear models, resulting in computational simplicity while maintaining effectiveness comparable to SVGD in terms of estimation biases. Additionally, we demonstrate that under a mild assumption, the estimation of high-dimensional gradient flow can be translated into a lower-dimensional estimation problem, leading to improved estimation accuracy. We vali
    
[^247]: BertRLFuzzer: 一种基于BERT和强化学习的Fuzzer

    BertRLFuzzer: A BERT and Reinforcement Learning based Fuzzer. (arXiv:2305.12534v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2305.12534](http://arxiv.org/abs/2305.12534)

    BertRLFuzzer是一种基于BERT和强化学习的Fuzzer工具，旨在发现Web应用程序的安全漏洞。通过使用BERT模型作为代理来指导Fuzzer进行高效学习，BertRLFuzzer相对于其他黑盒和白盒Fuzzer在时间到首次攻击、新漏洞发现和攻击率方面都取得了显著的改进。

    

    本文介绍了一种新颖的工具BertRLFuzzer，它是一种基于BERT和强化学习的Fuzzer，旨在发现Web应用程序的安全漏洞。BertRLFuzzer的工作原理如下：给定一组种子输入，Fuzzer对它们执行遵循语法并引发攻击的变异操作，以生成候选攻击向量。BertRLFuzzer的关键洞察是使用BERT模型作为代理来指导Fuzzer高效学习遵循语法和引发攻击的变异操作符。为了验证BertRLFuzzer的有效性，我们将其与共计13个黑盒和白盒Fuzzer在9个受害者网站的基准测试中进行比较，涉及超过16K行的源代码。相对于最接近的竞争工具，我们观察到时间到首次攻击的显著改进（减少54％），发现的新漏洞（17个新漏洞）和攻击率（生成的攻击向量增加了4.4％）。

    We present a novel tool BertRLFuzzer, a BERT and Reinforcement Learning (RL) based fuzzer aimed at finding security vulnerabilities for Web applications. BertRLFuzzer works as follows: given a set of seed inputs, the fuzzer performs grammar-adhering and attack-provoking mutation operations on them to generate candidate attack vectors. The key insight of BertRLFuzzer is the use of RL with a BERT model as an agent to guide the fuzzer to efficiently learn grammar-adhering and attack-provoking mutation operators. In order to establish the efficacy of BertRLFuzzer we compare it against a total of 13 black box and white box fuzzers over a benchmark of 9 victim websites with over 16K LOC. We observed a significant improvement, relative to the nearest competing tool, in terms of time to first attack (54% less), new vulnerabilities found (17 new vulnerabilities), and attack rate (4.4% more attack vectors generated).
    
[^248]: 从自然语言定义中学习多关系双曲词向量

    Multi-Relational Hyperbolic Word Embeddings from Natural Language Definitions. (arXiv:2305.07303v1 [cs.CL])

    [http://arxiv.org/abs/2305.07303](http://arxiv.org/abs/2305.07303)

    本论文提出了一种从自然语言定义中学习多关系双曲词向量的框架，以捕捉由定义所引起的分层和多分辨率结构。

    

    仅使用分布信息的神经词向量一直以来都能为下游任务提供有用的含义表示。然而，现有的方法通常会导致难以解释和控制的表示。相反，自然语言定义具有递归的，自说明的语义结构，可以支持能够保留向量空间中显式概念关系和约束的新型表示学习范 paradigm。本文提出了一个神经符号、多关系框架，通过联合映射定义和定义术语及其相应的语义关系，仅从自然语言定义中学习词向量。通过自动从定义语料库中提取关系，并通过一个翻译目标规范化学习问题，我们将框架专门设定为在双曲空间中捕获由定义引起的分层和多分辨率结构。

    Neural-based word embeddings using solely distributional information have consistently produced useful meaning representations for downstream tasks. However, existing approaches often result in representations that are hard to interpret and control. Natural language definitions, on the other side, possess a recursive, self-explanatory semantic structure that can support novel representation learning paradigms able to preserve explicit conceptual relations and constraints in the vector space.  This paper proposes a neuro-symbolic, multi-relational framework to learn word embeddings exclusively from natural language definitions by jointly mapping defined and defining terms along with their corresponding semantic relations. By automatically extracting the relations from definitions corpora and formalising the learning problem via a translational objective, we specialise the framework in hyperbolic space to capture the hierarchical and multi-resolution structure induced by the definitions.
    
[^249]: 带需求的机器学习：一份宣言

    Machine Learning with Requirements: a Manifesto. (arXiv:2304.03674v1 [cs.LG])

    [http://arxiv.org/abs/2304.03674](http://arxiv.org/abs/2304.03674)

    本文提出一个带需求的机器学习宣言，认为需求定义和满足可以在很大程度上使机器学习模型更适用于现实世界中的关键领域，作者提出了两个问题，其中（i）需求自然而然地出现，（ii）机器学习模型被或可以成功地部署，并且（iii）忽略需求可能会产生严重后果，提出了一种新型的金字塔式开发流程，在其中，需求定义可能会影响到流程中所有后续阶段，反之亦然。

    

    在近年来，机器学习取得了长足的进步，成为许多不同应用领域突破的根源。然而，如何将它们应用到高风险或安全关键的应用领域仍然是一个未解决的问题，因为它们往往容易变得脆弱和不可靠。本文认为，需求定义和满足可以在很大程度上使机器学习模型更适用于现实世界中的关键领域。为此，我们提出了两个问题，其中（i）需求自然而然地出现，（ii）机器学习模型被或可以成功地部署，并且（iii）忽略需求可能会产生严重后果。我们展示了如何将需求规格说明有益地整合到标准的机器学习开发流程中，提出了一种新型的金字塔式开发流程，在其中，需求定义可能会影响到流程中所有后续阶段，反之亦然。

    In the recent years, machine learning has made great advancements that have been at the root of many breakthroughs in different application domains. However, it is still an open issue how make them applicable to high-stakes or safety-critical application domains, as they can often be brittle and unreliable. In this paper, we argue that requirements definition and satisfaction can go a long way to make machine learning models even more fitting to the real world, especially in critical domains. To this end, we present two problems in which (i) requirements arise naturally, (ii) machine learning models are or can be fruitfully deployed, and (iii) neglecting the requirements can have dramatic consequences. We show how the requirements specification can be fruitfully integrated into the standard machine learning development pipeline, proposing a novel pyramid development process in which requirements definition may impact all the subsequent phases in the pipeline, and viceversa.
    
[^250]: 寻找具有身体智能的人工视觉皮层在哪里？

    Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?. (arXiv:2303.18240v1 [cs.CV])

    [http://arxiv.org/abs/2303.18240](http://arxiv.org/abs/2303.18240)

    该研究研究了使用预训练视觉表征来实现身体智能的最新进展。他们展示了最大、最全面的经验研究，发现没有一种表征是普遍优越的，并且数据集的大小和多样性并不能普遍改善性能。

    

    我们提出了最大、最全面的预训练视觉表征（PVR）或视觉基础模型的经验研究，用于身体智能。首先，我们策划了 CortexBench，其中包括涵盖动力学、导航、熟练和移动操作的17种不同任务。接下来，我们系统评估了现有的PVR，发现没有一种是普遍优越的。为了研究预训练数据规模和多样性的影响，我们结合了来自7个不同来源的超过4000小时的自我中心视频（超过560万张图像）和ImageNet，使用切片数据的遮盖自编码（MAE）来训练不同大小的视觉变形器。与先前的工作推断相反，我们发现扩展数据集的规模和多样性并不能普遍改善性能（但平均性能有所提高）。我们最大的模型名为VC-1，平均表现超过所有先前的PVR，但也没有普遍优势。最后，我们展示了VC-1的特定于任务或领域的适应会带来实质性的改进。

    We present the largest and most comprehensive empirical study of pre-trained visual representations (PVRs) or visual 'foundation models' for Embodied AI. First, we curate CortexBench, consisting of 17 different tasks spanning locomotion, navigation, dexterous, and mobile manipulation. Next, we systematically evaluate existing PVRs and find that none are universally dominant.  To study the effect of pre-training data scale and diversity, we combine over 4,000 hours of egocentric videos from 7 different sources (over 5.6M images) and ImageNet to train different-sized vision transformers using Masked Auto-Encoding (MAE) on slices of this data. Contrary to inferences from prior work, we find that scaling dataset size and diversity does not improve performance universally (but does so on average).  Our largest model, named VC-1, outperforms all prior PVRs on average but does not universally dominate either. Finally, we show that task or domain-specific adaptation of VC-1 leads to substantia
    
[^251]: VIDIMU: 使用价格实惠的设备记录日常生活活动的多模态视频和IMU运动学数据集

    VIDIMU. Multimodal video and IMU kinematic dataset on daily life activities using affordable devices. (arXiv:2303.16150v1 [cs.CV])

    [http://arxiv.org/abs/2303.16150](http://arxiv.org/abs/2303.16150)

    VIDIMU数据集使用商品相机和自定义传感器记录13种临床相关性的活动，为远程日常生活活动识别和运动学分析提供价格实惠的患者追踪解决方案。

    

    人体活动识别和临床生物力学是物理远程康复医学中的挑战性问题。然而，大多数公开可用的人体动作数据集不能用于研究实验室外运动获取情况下的这两个问题。VIDIMU数据集的目的是为远程日常生活活动识别和运动学分析提供价格实惠的患者追踪解决方案。该数据集包括使用商品相机和五个惯性传感器注册的13种活动。记录视频的54个受试者中，其中16个受试者同时还有惯性传感器记录。VIDIMU的创新之处在于：i）所选择的动作的临床相关性，ii）使用价格实惠的视频和自定义传感器的组合，以及 iii）实现了先进的多模态数据处理工具，可以从惯性数据中对三维身体姿势跟踪和运动重建在肌肉骨骼模型中。

    Human activity recognition and clinical biomechanics are challenging problems in physical telerehabilitation medicine. However, most publicly available datasets on human body movements cannot be used to study both problems in an out-of-the-lab movement acquisition setting. The objective of the VIDIMU dataset is to pave the way towards affordable patient tracking solutions for remote daily life activities recognition and kinematic analysis. The dataset includes 13 activities registered using a commodity camera and five inertial sensors. The video recordings were acquired in 54 subjects, of which 16 also had simultaneous recordings of inertial sensors. The novelty of VIDIMU lies in: i) the clinical relevance of the chosen movements, ii) the combined utilization of affordable video and custom sensors, and iii) the implementation of state-of-the-art tools for multimodal data processing of 3D body pose tracking and motion reconstruction in a musculoskeletal model from inertial data. The val
    
[^252]: 直接迭代反演：图像修复的替代方法

    Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration. (arXiv:2303.11435v1 [eess.IV])

    [http://arxiv.org/abs/2303.11435](http://arxiv.org/abs/2303.11435)

    InDI是一种新的监督式图像修复公式，通过逐步改进图像质量来生成比现有回归方法更真实和详细的图像，修复效果更具有感知质量。

    

    直接迭代反演（InDI）是一种新的监督式图像修复公式，它避免了所谓的“均值回归”效应，并生成比现有回归方法更真实和详细的图像。它通过逐步改进图像质量来实现，类似于生成式去噪扩散模型。图像修复是一个欠定问题，多个高质量图像都可能是给定低质量输入的可行重构。因此，单步回归模型的结果通常是所有可能解释的聚合结果，因此缺乏细节和真实感。

    Inversion by Direct Iteration (InDI) is a new formulation for supervised image restoration that avoids the so-called ``regression to the mean'' effect and produces more realistic and detailed images than existing regression-based methods. It does this by gradually improving image quality in small steps, similar to generative denoising diffusion models.  Image restoration is an ill-posed problem where multiple high-quality images are plausible reconstructions of a given low-quality input. Therefore, the outcome of a single step regression model is typically an aggregate of all possible explanations, therefore lacking details and realism. % The main advantage of InDI is that it does not try to predict the clean target image in a single step but instead gradually improves the image in small steps, resulting in better perceptual quality.  While generative denoising diffusion models also work in small steps, our formulation is distinct in that it does not require knowledge of any analytic f
    
[^253]: 部分神经最优输运

    Partial Neural Optimal Transport. (arXiv:2303.07988v1 [cs.LG])

    [http://arxiv.org/abs/2303.07988](http://arxiv.org/abs/2303.07988)

    我们提出了一种新的神经方法来计算部分最优输运映射，并在合成例子上进行了测试。

    

    我们提出了一种新颖的神经方法来计算部分最优输运（OT）映射，即指定质量的度量部分之间的OT映射。我们在合成例子上测试了我们的部分神经最优输运算法。

    We propose a novel neural method to compute partial optimal transport (OT) maps, i.e., OT maps between parts of measures of the specified masses. We test our partial neural optimal transport algorithm on synthetic examples.
    
[^254]: Task Aware Dreamer用于强化学习中的任务泛化

    Task Aware Dreamer for Task Generalization in Reinforcement Learning. (arXiv:2303.05092v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.05092](http://arxiv.org/abs/2303.05092)

    本文提出了一种名为Task Aware Dreamer（TAD）的方法用于强化学习中的任务泛化。通过量化任务分布的相关性，TAD能够将历史信息编码到策略中，以便区分不同任务，并在泛化到未见任务时具有较好的性能。

    

    强化学习的一个长期目标是获得能够在训练任务上学习并且在不同奖励函数下可以很好地泛化到未见任务的代理。一个通用的挑战是定量地衡量这些不同任务之间的相似性，这对于分析任务分布并进一步设计具有更强泛化能力的算法至关重要。为了解决这个问题，我们提出了一种新的度量方法，名为任务分布相关性（TDR），通过不同任务的最优Q函数来量化任务分布的相关性。在具有高TDR的任务情况下，即任务之间显著不同，我们发现马尔可夫策略无法区分它们，导致性能较差。基于这一观察，我们将所有历史信息编码到策略中以区分不同任务，并提出了Task Aware Dreamer（TAD），它将世界模型扩展为我们的奖励感知世界模型以捕捉任务的相关性。

    A long-standing goal of reinforcement learning is to acquire agents that can learn on training tasks and generalize well on unseen tasks that may share a similar dynamic but with different reward functions. A general challenge is to quantitatively measure the similarities between these different tasks, which is vital for analyzing the task distribution and further designing algorithms with stronger generalization. To address this, we present a novel metric named Task Distribution Relevance (TDR) via optimal Q functions of different tasks to capture the relevance of the task distribution quantitatively. In the case of tasks with a high TDR, i.e., the tasks differ significantly, we show that the Markovian policies cannot differentiate them, leading to poor performance. Based on this insight, we encode all historical information into policies for distinguishing different tasks and propose Task Aware Dreamer (TAD), which extends world models into our reward-informed world models to capture
    
[^255]: 通过离线数据提升蒙特卡罗评估方法

    Improving Monte Carlo Evaluation with Offline Data. (arXiv:2301.13734v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13734](http://arxiv.org/abs/2301.13734)

    本论文介绍了通过使用离线数据来提升蒙特卡罗评估方法，实现在保持相同估计准确度的前提下，减少在线样本数量的目的。通过使用一个定制的行为策略，可以比普通的 MC 估计器产生更小的方差。该行为策略可以从现有的离线数据中高效学习，我们的实验表明，相对于现有的最先进方法，我们的方法只需使用小部分在线样本就能实现相同的估计精度。

    

    蒙特卡罗 (MC) 方法是估计策略表现最广泛使用的方法。给定一个感兴趣的策略，MC 方法通过重复运行该策略以收集样本并取出结果平均值来给出估计值。在此过程中收集的样本称为在线样本。为了获得准确的估计值，MC 方法需要消耗大量在线样本。当在线样本昂贵时，例如在线推荐和库存管理，我们希望在实现相同的估计准确度的同时减少在线样本数量。为此，我们使用离线 MC 方法，通过运行不同的策略（称为行为策略）评估感兴趣的策略。我们设计了一个定制的行为策略，使离线 MC 估计器的方差明显小于普通 MC 估计器。重要的是，该定制行为策略可以从现有的离线数据，即先前记录的数据中高效学习，这比在线样本要便宜得多。我们的实验表明，与现有的最先进方法相比，我们的方法只需使用小部分在线样本就能实现相同的估计精度。

    Monte Carlo (MC) methods are the most widely used methods to estimate the performance of a policy. Given an interested policy, MC methods give estimates by repeatedly running this policy to collect samples and taking the average of the outcomes. Samples collected during this process are called online samples. To get an accurate estimate, MC methods consume massive online samples. When online samples are expensive, e.g., online recommendations and inventory management, we want to reduce the number of online samples while achieving the same estimate accuracy. To this end, we use off-policy MC methods that evaluate the interested policy by running a different policy called behavior policy. We design a tailored behavior policy such that the variance of the off-policy MC estimator is provably smaller than the ordinary MC estimator. Importantly, this tailored behavior policy can be efficiently learned from existing offline data, i,e., previously logged data, which are much cheaper than onlin
    
[^256]: 我不想说：在可选个人数据模型中保护用户同意

    I Prefer not to Say: Protecting User Consent in Models with Optional Personal Data. (arXiv:2210.13954v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13954](http://arxiv.org/abs/2210.13954)

    该论文研究了个人可以选择与决策系统共享可选个人信息的机器学习模型，并提出了保护用户同意的PUC概念，为用户隐私保护提供了有力的解决方案。

    

    我们研究了一种机器学习模型，其中个人可以选择与决策系统共享可选个人信息，这在现代保险定价模型中很常见。一些用户同意使用他们的数据，而其他人则反对并保持其数据未公开。本文表明，不共享数据的决定本身可以被视为信息，应该受到保护，以尊重用户的隐私。这一观察结果引发了一个被忽视的问题，即如何确保保护其个人数据的用户不会因此受到任何不利影响。为了解决这个问题，我们对仅使用获得积极用户同意的信息的模型进行了保护要求的正式化。这排除了作出共享数据与否决定所包含的隐含信息。我们提出了Protected User Consent (PUC)概念，这是我们证明在保护要求下损失最小的解决方案。

    We examine machine learning models in a setup where individuals have the choice to share optional personal information with a decision-making system, as seen in modern insurance pricing models. Some users consent to their data being used whereas others object and keep their data undisclosed. In this work, we show that the decision not to share data can be considered as information in itself that should be protected to respect users' privacy. This observation raises the overlooked problem of how to ensure that users who protect their personal data do not suffer any disadvantages as a result. To address this problem, we formalize protection requirements for models which only use the information for which active user consent was obtained. This excludes implicit information contained in the decision to share data or not. We offer the first solution to this problem by proposing the notion of Protected User Consent (PUC), which we prove to be loss-optimal under our protection requirement. To
    
[^257]: 信息度量反映记忆模式

    Measures of Information Reflect Memorization Patterns. (arXiv:2210.09404v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.09404](http://arxiv.org/abs/2210.09404)

    本研究通过信息论度量神经网络中神经元激活的多样性，发现其与模型的泛化能力和记忆之间存在关联。实验证明，即使在未标记的分布内部计算神经激活时，信息的组织也可以指向两种形式的记忆。

    

    神经网络已被证明利用与目标标签共现的错误假象（或捷径）来展示启发式记忆。另一方面，网络已被证明会记忆训练样本，产生基于示例的记忆。这些记忆类型阻碍了网络在超出训练分布的泛化能力。检测此类记忆可能具有挑战性，通常需要研究人员策划定制的测试集。在这项工作中，我们假设——并随后展示了——不同神经元激活模式的多样性反映了模型的泛化和记忆。我们通过信息论度量量化神经激活的多样性，并在涵盖了几个自然语言和视觉任务的实验中得到了对我们假设的支持。重要的是，我们发现信息的组织指向了这两种形式的记忆，即使是在未标记的分布内部计算的神经激活上也是如此。

    Neural networks are known to exploit spurious artifacts (or shortcuts) that co-occur with a target label, exhibiting heuristic memorization. On the other hand, networks have been shown to memorize training examples, resulting in example-level memorization. These kinds of memorization impede generalization of networks beyond their training distributions. Detecting such memorization could be challenging, often requiring researchers to curate tailored test sets. In this work, we hypothesize -- and subsequently show -- that the diversity in the activation patterns of different neurons is reflective of model generalization and memorization. We quantify the diversity in the neural activations through information-theoretic measures and find support for our hypothesis on experiments spanning several natural language and vision tasks. Importantly, we discover that information organization points to the two forms of memorization, even for neural activations computed on unlabelled in-distribution
    
[^258]: 通过在双曲流形上使用GPLVM将机器人分类带入连续领域

    Bringing robotics taxonomies to continuous domains via GPLVM on hyperbolic manifolds. (arXiv:2210.01672v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.01672](http://arxiv.org/abs/2210.01672)

    本论文提出了一种通过在双曲流形上使用GPLVM来在连续领域中应用机器人分类法的方法，通过捕捉相关层次结构的双曲嵌入来建模分类数据，并采用图形先验和保持距离的后向约束来实现分类法结构的纳入。

    

    机器人分类被用作将人类的移动和与环境互动的方式进行高层次的分层抽象。它们已被证明对于分析抓取、操纵技能和全身支撑姿势非常有用。尽管我们在设计层次结构和基础类别方面做出了大量努力，但它们在应用领域的使用仍然有限。这可能是因为缺乏填补分类层级结构和与其类别相关联的高维异构数据之间差距的计算模型。为了解决这个问题，我们建议通过捕捉相关层次结构的双曲嵌入来建模分类数据。我们通过构建一个新颖的高斯过程双曲潜变量模型来实现这一点，该模型通过图形先验和保持距离的后向约束将分类法结构纳入潜在空间中。我们在三个不同的机器人分类法上验证了我们的模型。

    Robotic taxonomies serve as high-level hierarchical abstractions that classify how humans move and interact with their environment. They have proven useful to analyse grasps, manipulation skills, and whole-body support poses. Despite substantial efforts devoted to design their hierarchy and underlying categories, their use in application fields remains limited. This may be attributed to the lack of computational models that fill the gap between the discrete hierarchical structure of the taxonomy and the high-dimensional heterogeneous data associated to its categories. To overcome this problem, we propose to model taxonomy data via hyperbolic embeddings that capture the associated hierarchical structure. We achieve this by formulating a novel Gaussian process hyperbolic latent variable model that incorporates the taxonomy structure through graph-based priors on the latent space and distance-preserving back constraints. We validate our model on three different robotics taxonomies to lear
    
[^259]: 自动编码对抗性模仿学习

    Auto-Encoding Adversarial Imitation Learning. (arXiv:2206.11004v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.11004](http://arxiv.org/abs/2206.11004)

    自动编码对抗性模仿学习（AEAIL）是一种稳健且可扩展的方法，利用自动编码器的重构误差作为奖励信号来优化策略。在基于状态和基于图像的环境中，AEAIL比现有方法表现更好，并且对于噪声示范专家具有更好的稳健性。

    

    强化学习（RL）提供了一个强大的决策框架，但在实践中，其应用通常需要精心设计的奖励函数。对抗性模仿学习（AIL）揭示了在没有来自环境的奖励信号的情况下自动获取策略的方法。在这项工作中，我们提出了一种稳健且可扩展的自动编码对抗性模仿学习（AEAIL）框架。为了从示范中推导出专家策略，AEAIL利用自动编码器的重构误差作为奖励信号，这比之前基于鉴别器的方法提供了更多用于优化策略的信息。随后，我们使用所得到的目标函数训练自动编码器和智能体策略。实验证明，我们的AEAIL在基于状态和基于图像的环境中表现优于现有方法。更重要的是，当示范专家具有噪声时，AEAIL表现出更好的稳健性。

    Reinforcement learning (RL) provides a powerful framework for decision-making, but its application in practice often requires a carefully designed reward function. Adversarial Imitation Learning (AIL) sheds light on automatic policy acquisition without access to the reward signal from the environment. In this work, we propose Auto-Encoding Adversarial Imitation Learning (AEAIL), a robust and scalable AIL framework. To induce expert policies from demonstrations, AEAIL utilizes the reconstruction error of an auto-encoder as a reward signal, which provides more information for optimizing policies than the prior discriminator-based ones. Subsequently, we use the derived objective functions to train the auto-encoder and the agent policy. Experiments show that our AEAIL performs superior compared to state-of-the-art methods on both state and image based environments. More importantly, AEAIL shows much better robustness when the expert demonstrations are noisy.
    

