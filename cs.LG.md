# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [ARB: Advanced Reasoning Benchmark for Large Language Models.](http://arxiv.org/abs/2307.13692) | ARB是一个新型基准，包含了数学、物理、生物、化学和法律领域的高级推理问题。目前的语言模型在这些任务上得分远低于50%，为了提高评估能力，我们引入了基于评分标准的评估方法。 |
| [^2] | [High Probability Analysis for Non-Convex Stochastic Optimization with Clipping.](http://arxiv.org/abs/2307.13680) | 本文提供了对于具有修剪的非凸随机优化的高概率分析，同时推导了流行随机优化算法的优化界限和泛化界限，为处理重尾行为提供了理论基础。 |
| [^3] | [RED CoMETS: An ensemble classifier for symbolically represented multivariate time series.](http://arxiv.org/abs/2307.13679) | 本文介绍了一种名为RED CoMETS的集成分类器，用于处理符号化表示的多变量时间序列数据。它在多变量设置中展现出竞争力的准确性，并在'HandMovementDirection'数据集上实现了最高的报告准确性。 |
| [^4] | [Towards an AI Accountability Policy.](http://arxiv.org/abs/2307.13658) | 这份白皮书是对美国国家电信和信息管理局的“AI问责政策评论请求”的回应，提出了一组相互关联的AI问责政策建议。 |
| [^5] | [Safety Margins for Reinforcement Learning.](http://arxiv.org/abs/2307.13642) | 本论文提出了一种能够通过计算代理关键性指标来生成安全边界的方法，该方法能够将可能的错误行为的后果与整体性能的预期损失联系起来。在Atari环境中的实验结果表明，随着代理接近失败状态，安全边界减小。 |
| [^6] | [Scaling machine learning-based chemical plant simulation: A method for fine-tuning a model to induce stable fixed points.](http://arxiv.org/abs/2307.13621) | 本研究提出了一种缩放基于机器学习的化工厂模拟的方法，并通过微调模型解决了在应用于较大工厂时出现的循环求解不稳定的问题。 |
| [^7] | [AI and ethics in insurance: a new solution to mitigate proxy discrimination in risk modeling.](http://arxiv.org/abs/2307.13616) | 该论文提出了一个新的解决方案，通过考虑变量之间的潜在相互作用，减少风险建模中的拟议差别，以实现更公平的保险定价和风险选择。 |
| [^8] | [Dendritic Integration Based Quadratic Neural Networks Outperform Traditional Aritificial Ones.](http://arxiv.org/abs/2307.13609) | 提出了一种基于树突综合的二次神经网络(DIQNN)模型，该模型在多种分类任务中表现出优越性能，超过了传统的人工神经网络。引入边界来刻画泛化误差，并将边界整合到损失函数中，加速了测试准确率的改变。 |
| [^9] | [Geometric Epitope and Paratope Prediction.](http://arxiv.org/abs/2307.13608) | 本文研究了预测抗体-抗原结合位点的最佳表示方法，并强调了几何信息的重要性。研究发现基于表面的模型更高效，O-GEP实验取得了显著的性能改进。 |
| [^10] | [Multi-GPU Approach for Training of Graph ML Models on large CFD Meshes.](http://arxiv.org/abs/2307.13592) | 本论文通过将最先进的基于图的机器学习代理模型扩展到工业相关网格尺寸，实现了基于多GPU的训练方法，能够在计算流体动力学（CFD）的大规模网格上进行快速且准确的数值流动模拟预测。 |
| [^11] | [Settling the Sample Complexity of Online Reinforcement Learning.](http://arxiv.org/abs/2307.13586) | 本文解决了在线强化学习的样本复杂度问题，提出了一种基于模型的算法，它可以在有限时间不均匀马尔可夫决策问题中实现极小后悔的最优性。 |
| [^12] | [Comparing Forward and Inverse Design Paradigms: A Case Study on Refractory High-Entropy Alloys.](http://arxiv.org/abs/2307.13581) | 本研究比较了正向和逆向设计范式在耐火高熵合金设计中的表现，并通过两个案例研究定量比较了逆向设计方法与其他正向方案的差异。 |
| [^13] | [Reinterpreting survival analysis in the universal approximator age.](http://arxiv.org/abs/2307.13579) | 这篇论文介绍了生存分析在深度学习中的应用，提供了连接分类和回归的方法，并且提出了无需数值积分的通用拟合网络，通过大规模的数值研究证明了其优于其他方法。 |
| [^14] | [PT$\mathrm{L}^{p}$: Partial Transport $\mathrm{L}^{p}$ Distances.](http://arxiv.org/abs/2307.13571) | 本文介绍了部分传输$\mathrm{L}^{p}$距离作为一种新型度量，用于比较通用信号，并且利用部分传输距离的鲁棒性。同时，还介绍了该距离的切片变体，可以快速比较信号。最后，展示了该方法在实际应用中的效果。 |
| [^15] | [Decision-Focused Learning: Foundations, State of the Art, Benchmark and Future Opportunities.](http://arxiv.org/abs/2307.13565) | 决策导向学习是一个新兴的机器学习范式，它集成了预测和优化，旨在优化决策。本文全面回顾了决策导向学习的相关技术，提出了分类法并进行了实证评估，探讨了当前和未来研究方向。 |
| [^16] | [Node Injection Link Stealing Attack.](http://arxiv.org/abs/2307.13548) | 本文提出了一种隐秘且有效的攻击方法，通过推断图结构数据中的私有链接来暴露GNNs的隐私漏洞，并提出了保护隐私和保持模型效能的方法。同时，我们研究了将差分隐私机制应用于减轻攻击的影响，并分析了隐私保护和模型效能的权衡。 |
| [^17] | [Transfer Learning for Portfolio Optimization.](http://arxiv.org/abs/2307.13546) | 本文研究了如何利用迁移学习技术解决金融组合优化问题，并引入了新概念"迁移风险"，通过实验证明了迁移风险与迁移学习方法整体性能之间的强相关性，并提出了通过迁移风险识别适当的源任务来提高迁移学习方法效率和效果的方法。数值实验还为不同设置下的投资组合管理提供了有价值的新见解。 |
| [^18] | [A model for efficient dynamical ranking in networks.](http://arxiv.org/abs/2307.13544) | 该论文提出了一种受物理启发的方法，用于在定向时态网络中推断节点的动态排序，通过求解线性方程组实现，仅需调整一个参数，具有可扩展性和高效性。在各种应用中的测试结果显示，该方法比现有方法更好地预测了动态排序。 |
| [^19] | [Model Calibration in Dense Classification with Adaptive Label Perturbation.](http://arxiv.org/abs/2307.13539) | 本文提出了一种自适应标签扰动的模型校准方法，使用自校准二进制交叉熵损失来统一不同形式的标签扰动过程。该方法通过最大化预测熵来改善模型校准，并在保持分类准确性的同时纠正校准问题。 |
| [^20] | [INFINITY: Neural Field Modeling for Reynolds-Averaged Navier-Stokes Equations.](http://arxiv.org/abs/2307.13538) | INFINITY是一个使用隐式神经表示的深度学习模型，可以高效准确地近似复杂的物理现象，特别适用于设计探索和形状优化任务。 |
| [^21] | [Do algorithms and barriers for sparse principal component analysis extend to other structured settings?.](http://arxiv.org/abs/2307.13535) | 该论文研究了在尖峰Wishart模型下，通过一类子空间并集模型捕捉信号结构的主成分分析问题。通过统计和计算的视角，我们建立了基本限制，并展示了自然的投影功率方法在解决方案的统计近似最优邻域中的局部收敛性。我们还通过具体案例的分析展示了计算难度。结果表明，对于基本稀疏PCA观察到的现象在其结构化对应物中也同样存在。 |
| [^22] | [Differentiable Turbulence II.](http://arxiv.org/abs/2307.13533) | 该研究开发了一个框架，将深度学习模型嵌入到通用有限元数值方案中，用于解Navier-Stokes方程，并通过学习多尺度图神经网络实现了子网格尺度闭合。在实现多种流动情况时进行测试验证，结果表明学到的闭合模型在速度加快10倍的更细网格上能够达到与传统大涡模型相当的精度。 |
| [^23] | [Towards Long-Term predictions of Turbulence using Neural Operators.](http://arxiv.org/abs/2307.13517) | 本研究使用神经算子模型预测湍流流动，通过探索不同模型配置和引入正则化项，提高了预测的准确性和稳定性。研究强调了需要改进的深度学习模型指标，并呼吁进一步研究复杂流动和实际基准测试指标。 |
| [^24] | [Continuous Time Evidential Distributions for Irregular Time Series.](http://arxiv.org/abs/2307.13503) | 该论文提出了一种在连续时间中学习不规则时间序列的证据分布的策略，能够在任何感兴趣的时间上对部分观测到的特征进行良好校准和灵活的推断，并且在稀疏、不规则观测的时间上扩展不确定性。该方法在时间序列分类任务上表现出竞争性的性能，并能够在遇到噪音数据时实现基于不确定性的推断。 |
| [^25] | [Deep Reinforcement Learning for Robust Goal-Based Wealth Management.](http://arxiv.org/abs/2307.13501) | 本文提出了一种基于深度强化学习的强健目标导向财富管理方法，通过实验证明该方法在模拟和历史市场数据上优于传统的目标导向财富管理基准。 |
| [^26] | [Finding Money Launderers Using Heterogeneous Graph Neural Networks.](http://arxiv.org/abs/2307.13499) | 本文介绍了一种使用异构图神经网络来寻找洗钱者的方法，该方法在真实的银行交易和商业角色数据构建的大型异构网络中识别洗钱活动。为了解决洗钱活动中犯罪分子的合作问题，我们扩展了同质图神经网络方法，提出了一种新颖的消息聚合方法。 |
| [^27] | [Zshot: An Open-source Framework for Zero-Shot Named Entity Recognition and Relation Extraction.](http://arxiv.org/abs/2307.13497) | Zshot是一个开源框架，用于零样本命名实体识别和关系抽取，通过比较不同的最新ZSL方法，支持研究人员和工业界的需求。 |
| [^28] | [Duet: efficient and scalable hybriD neUral rElation undersTanding.](http://arxiv.org/abs/2307.13494) | Duet是一种高效且可扩展的混合神经关系理解方法，旨在解决基数估计问题中高成本和难以区分的采样方法，并通过可微分的预测过程改进模型的准确性。 |
| [^29] | [Rational kernel-based interpolation for complex-valued frequency response functions.](http://arxiv.org/abs/2307.13484) | 本论文提出了一种基于有理核的复频率响应函数插值方法，通过引入新的复值函数再生核希尔伯特空间，并结合低阶有理函数进行自适应插值，解决了频率响应函数拟合过程中标准核方法表现不佳的问题。 |
| [^30] | [Exploring MLOps Dynamics: An Experimental Analysis in a Real-World Machine Learning Project.](http://arxiv.org/abs/2307.13473) | 本研究在真实机器学习项目中进行了实验分析，旨在优化MLOps工作流程并提供实用的提示和建议，强调积极规划和持续改进，以增强MLOps的效率。 |
| [^31] | [Combinatorial Auctions and Graph Neural Networks for Local Energy Flexibility Markets.](http://arxiv.org/abs/2307.13470) | 本研究提出了一个新的组合拍卖框架用于本地能量灵活市场，通过设计图神经网络模型解决了NP完全的问题，实验结果显示了使用机器学习在本地市场中分配能量灵活资源的潜力。 |
| [^32] | [Gaussian Graph with Prototypical Contrastive Learning in E-Commerce Bundle Recommendation.](http://arxiv.org/abs/2307.13468) | 本文提出了一种新颖的具有高斯图和典型对比学习（GPCL）框架，用于解决电子商务套餐推荐中的不确定性和采样偏差问题。 |
| [^33] | [Integrating processed-based models and machine learning for crop yield prediction.](http://arxiv.org/abs/2307.13466) | 本研究通过将基于过程的作物生长模型与机器学习相结合，提出了一种混合元模建方法用于马铃薯产量预测。该方法通过使用合成数据预训练卷积神经网络，并使用观测数据进行微调，在模拟应用和真实数据测试中均取得了竞争力的预测结果。 |
| [^34] | [Fundamental causal bounds of quantum random access memories.](http://arxiv.org/abs/2307.13460) | 本研究通过采用相对论量子场论和量子多体系统中的Lieb-Robinson界限，批判性地探讨了基于因果性的快速量子存储器的内在界限。研究表明在混合量子声学系统中，QRAM可以容纳最多O(10^7)个逻辑比特的一维结构。 |
| [^35] | [A behavioural transformer for effective collaboration between a robot and a non-stationary human.](http://arxiv.org/abs/2307.13447) | 该论文提出了一个行为变换器(BeTrans)框架，该框架能够使机器人能够更好地预测人类的非静态行为，并通过使用顺序数据来适应新的非静态的人类代理。实验证明BeTrans在协作环境中效果显著，比现有技术更快地适应了非静态的模拟人类代理。 |
| [^36] | [Network Traffic Classification based on Single Flow Time Series Analysis.](http://arxiv.org/abs/2307.13434) | 本论文提出了一种基于单流时间序列分析的网络流量分类方法，通过提取69个通用特征，可在各种网络流量分类任务中取得与相关工作相当或更好的分类性能。 |
| [^37] | [Achieving Linear Speedup in Decentralized Stochastic Compositional Minimax Optimization.](http://arxiv.org/abs/2307.13430) | 本论文提出了一种新颖的算法，可在分布式环境中实现线性加速，以解决组合极小化问题中内层函数的共识误差问题。 |
| [^38] | [A signal processing interpretation of noise-reduction convolutional neural networks.](http://arxiv.org/abs/2307.13425) | 该论文通过将信号处理的基本原理与深度学习领域相连接，以解释不同的噪声抑制卷积神经网络架构，并提供了设计新型CNN架构的重要指南。 |
| [^39] | [Non Intrusive Intelligibility Predictor for Hearing Impaired Individuals using Self Supervised Speech Representations.](http://arxiv.org/abs/2307.13423) | 本研究将自学习语音表示应用于预测听力障碍个体的可理解性，并发现其作为非侵入式预测模型的输入特征具有竞争性能，需要更多数据才能推广到未知系统和个体。 |
| [^40] | [On the learning Dynamics of Attention Networks.](http://arxiv.org/abs/2307.13421) | 本研究分析了软注意力、硬注意力和潜变量边际似然（LVML）注意力三种注意力模型的学习动态，发现了它们在所选择的片段聚合方式上的显著差异，并解释了分类模型在梯度下降下的演化对最终结果的影响。 |
| [^41] | [Co-Design of Out-of-Distribution Detectors for Autonomous Emergency Braking Systems.](http://arxiv.org/abs/2307.13419) | 这项研究探讨了自主紧急制动系统中学习能力组件和越界检测器之间的联合设计，以解决训练分布之外的样本带来的错误决策问题，考虑了功能性能、非功能性能和系统安全的权衡。 |
| [^42] | [Communication-Efficient Orchestrations for URLLC Service via Hierarchical Reinforcement Learning.](http://arxiv.org/abs/2307.13415) | 本文提出了一个分层强化学习框架，通过不同控制循环时间尺度的多级策略，实现了高效的URLLC服务编排，减少了控制循环的延迟，信令和能量消耗。 |
| [^43] | [Mitigating Memory Wall Effects in CNN Engines with On-the-Fly Weights Generation.](http://arxiv.org/abs/2307.13412) | 本文研究了用即时生成权重的方法减轻CNN引擎中的内存瓶颈效应，并提出了一种名为unzipFPGA的CNN推理系统。 |
| [^44] | [The Double-Edged Sword of Big Data and Information Technology for the Disadvantaged: A Cautionary Tale from Open Banking.](http://arxiv.org/abs/2307.13408) | 本研究利用开放银行作为例子，分析了大数据和强大的信息技术（如机器学习）所带来的公平性隐患。通过研究金融脆弱性的维度，我们展示了细致交易数据的能力，并提醒对其应当谨慎使用以避免潜在的歧视问题。 |
| [^45] | [Counterfactual Explanation via Search in Gaussian Mixture Distributed Latent Space.](http://arxiv.org/abs/2307.13390) | 本文介绍了一种通过在自编码器的潜空间中进行高斯混合分布搜索来生成反事实解释的方法。 |
| [^46] | [BotHawk: An Approach for Bots Detection in Open Source Software Projects.](http://arxiv.org/abs/2307.13386) | 本研究通过分析开源软件项目中机器人账号的行为，成功识别了四种类型的机器人账号，并创建了高效的BotHawk模型用于检测开源软件项目中的机器人。 |
| [^47] | [Scaff-PD: Communication Efficient Fair and Robust Federated Learning.](http://arxiv.org/abs/2307.13381) | Scaff-PD是一个高效通信、公平及鲁棒的分布式学习算法。它通过优化一系列针对异构客户端的分布鲁棒目标来提高公平性，利用特殊结构和加速的原始-对偶算法，在通信效率和收敛速度方面取得显著的提升。在多个基准数据集上的评估结果显示，Scaff-PD在提高公平性和鲁棒性方面有效，并同时保持竞争性的准确性。这使得Scaff-PD成为资源受限和异构环境下分布式学习的一种有前景的方法。 |
| [^48] | [Submodular Reinforcement Learning.](http://arxiv.org/abs/2307.13372) | 子模块强化学习(SubRL)是一种用于优化非可加奖励的范式，通过子模块集合函数来建模递减回报。这篇论文提出了SubRL的简单策略梯度算法SubPO，可以用于处理这种类型的奖励。 |
| [^49] | [Learning Regions of Interest for Bayesian Optimization with Adaptive Level-Set Estimation.](http://arxiv.org/abs/2307.13371) | 提出了一种名为BALLET的框架，用于在高维和非平稳场景下的贝叶斯优化。它使用两个概率模型，一个粗糙的高斯过程用于识别感兴趣的区域，一个局部高斯过程用于优化该区域。BALLET能够有效地缩小搜索空间，并且比标准的无感兴趣区域过滤的贝叶斯优化具有更紧的遗憾界限。 |
| [^50] | [Computational Guarantees for Doubly Entropic Wasserstein Barycenters via Damped Sinkhorn Iterations.](http://arxiv.org/abs/2307.13370) | 本文提出了一种用于计算双规则化Wasserstein重心的算法，并通过阻尼Sinkhorn迭代和精确的最大化/最小化步骤保证了收敛性。此算法的非精确变体使用近似的蒙特卡罗采样实现，在自由支撑/网格自由设置中提供了第一个非渐近收敛保证。 |
| [^51] | [High Dimensional Distributed Gradient Descent with Arbitrary Number of Byzantine Attackers.](http://arxiv.org/abs/2307.13352) | 本文提出了一种适用于高维问题、在任意数量拜占庭攻击者下的新方法，核心是一种直接的高维半验证均值估计方法，具有极小极值统计率。 |
| [^52] | [Feature Importance Measurement based on Decision Tree Sampling.](http://arxiv.org/abs/2307.13333) | DT-Sampler是一种基于SAT的方法，用于测量基于树的模型中的特征重要性，提供了更高的可解释性和稳定性。 |
| [^53] | [The Optimal Approximation Factors in Misspecified Off-Policy Value Function Estimation.](http://arxiv.org/abs/2307.13332) | 本文研究了在线性离策略值函数估计中的逼近因子，并在多种设置下建立了最优的渐近逼近因子，这些因子决定了离策略评估的困难程度。 |
| [^54] | [QuIP: 2-Bit Quantization of Large Language Models With Guarantees.](http://arxiv.org/abs/2307.13304) | 本文提出了一种新的基于无关处理的大型语言模型（LLMs）参数量化方法QuIP，通过使权重和Hessian矩阵与坐标轴不对齐，实现了准确的量化结果。经过经验实验，我们发现我们的方法改善了现有的量化算法，并且首次在仅使用两比特的情况下获得了可行的LLM量化结果。 |
| [^55] | [Modify Training Directions in Function Space to Reduce Generalization Error.](http://arxiv.org/abs/2307.13290) | 本文提出了在函数空间中修改训练方向的方法，通过在神经网络函数空间中进行特征分解和统计理论的理论分析，我们证明了这种方法可以降低总的泛化误差。 |
| [^56] | [Curvature-based Transformer for Molecular Property Prediction.](http://arxiv.org/abs/2307.13275) | 该研究提出了一种基于曲率的变压器方法，通过引入离散化的 Ricci 曲率，改进了图变压器神经网络模型在分子图数据上提取结构信息的能力。实验证明其有效性，并有扩展到其他模型的潜力。 |
| [^57] | [Unbiased Weight Maximization.](http://arxiv.org/abs/2307.13270) | 这项研究提出了一种无偏重量最大化的方法，通过用出站权重的范数替换单元的奖励信号，实现了更加高效的结构性信用分配。 |
| [^58] | [Federated K-Means Clustering via Dual Decomposition-based Distributed Optimization.](http://arxiv.org/abs/2307.13267) | 本论文介绍了如何使用双分解方法进行分布式训练K均值聚类问题，以提高隐私保护或计算效率。论文通过对分布式和联邦机器学习进行概述，并给出了基于规划方法的K均值聚类训练的公式化描述。 |
| [^59] | [Federated Split Learning with Only Positive Labels for resource-constrained IoT environment.](http://arxiv.org/abs/2307.13266) | 在资源受限的物联网环境中，我们提出了带有正标签的分割学习（SFPL）方法，通过对数据进行随机洗牌来改善多类别分类深度学习模型在联邦分割学习中的效果。 |
| [^60] | [Structural Credit Assignment with Coordinated Exploration.](http://arxiv.org/abs/2307.13256) | 本文提出一种结构化信用分配与协调探索的方法，该方法将每个单元视为强化学习代理，并通过全局奖励信号调节局部学习规则。通过提高结构化信用分配的效率来改进人工神经网络的训练。 |
| [^61] | [RoSAS: Deep Semi-Supervised Anomaly Detection with Contamination-Resilient Continuous Supervision.](http://arxiv.org/abs/2307.13239) | 本论文提出了一种具有抗污染连续监督的深度半监督异常检测方法，通过质量插值方法创造了连续异常度标签的新数据样本，解决了半监督异常检测方法中的异常污染和离散监督信息利用不充分的问题。 |
| [^62] | [Audio-aware Query-enhanced Transformer for Audio-Visual Segmentation.](http://arxiv.org/abs/2307.13236) | 提出了一种声音感知的查询增强的变形器 (AuTR) 方法来解决音频-视觉分割任务中感受野小和特征不充分的问题，通过引入多模态变形器架构以及声音感知的查询增强的变形器解码器，实现了深度融合和聚合音频-视觉特征，并在多声音和开放性场景中展示出更好的泛化能力。 |
| [^63] | [Spectral-DP: Differentially Private Deep Learning through Spectral Perturbation and Filtering.](http://arxiv.org/abs/2307.13231) | Spectral-DP是一种新的差分隐私学习方法，通过将频域中的梯度扰动与频谱滤波相结合，实现更低噪声比例的隐私保证，从而提高深度学习的效用。 |
| [^64] | [A Primer on the Data Cleaning Pipeline.](http://arxiv.org/abs/2307.13219) | 该论文介绍了数据清洗流程的概念和方法，旨在帮助分析师在清洁数据上进行下游任务、预测分析或统计分析。 |
| [^65] | [FedMEKT: Distillation-based Embedding Knowledge Transfer for Multimodal Federated Learning.](http://arxiv.org/abs/2307.13214) | FedMEKT是一种基于蒸馏的多模态联邦学习框架，通过半监督学习方法利用不同模态的表示，实现了服务器和客户端之间的联合知识传输。 |
| [^66] | [Transferability of Graph Neural Networks using Graphon and Sampling Theories.](http://arxiv.org/abs/2307.13206) | 本文提出了一种新的方法来实现图神经网络的可迁移性，通过使用图谱和采样理论，我们证明了一个显式的两层图谱神经网络能够在保持准确性的同时以较少的网络权重数逼近带限信号，并且在收敛到图谱的序列中实现了在足够大的图之间的可迁移性。 |
| [^67] | [An Investigation into Glomeruli Detection in Kidney H&E and PAS Images using YOLO.](http://arxiv.org/abs/2307.13199) | 本研究提出了一种使用YOLO-v4进行肾小球检测的方法，该方法通过训练全切片图像，可以在肾脏H&E和PAS图像中实现自动的组织结构检测和分割。 |
| [^68] | [Counterfactual Explanation Policies in RL.](http://arxiv.org/abs/2307.13192) | 本文介绍了一个名为COUNTERPOL的框架，用于通过对策略进行最小改变来分析RL策略，并达到所需的结果。这项工作在RL中通过使用反事实解释与监督学习相结合的方法进行了实证分析，并与广泛使用的基于信任区域的策略优化方法进行了理论联系。 |
| [^69] | [Neural Memory Decoding with EEG Data and Representation Learning.](http://arxiv.org/abs/2307.13181) | 本研究提出一种使用EEG数据和表示学习进行神经记忆解码的方法，能够实现从EEG数据中识别出被召回的概念，并在信息检索问题中应用该方法。 |
| [^70] | [Evaluating the reliability of automatically generated pedestrian and bicycle crash surrogates.](http://arxiv.org/abs/2307.13178) | 本研究旨在评估自动生成的行人和自行车事故替代品的可靠性，以提高脆弱道路用户的安全性能。 |
| [^71] | [Multi-UAV Speed Control with Collision Avoidance and Handover-aware Cell Association: DRL with Action Branching.](http://arxiv.org/abs/2307.13158) | 本文提出了一种深度强化学习方法，用于优化多无人机在三维空中高速公路上的小区关联决策和移动速度，以提升交通和通信性能。仿真结果显示，性能提高了18.32%。 |
| [^72] | [Discovering interpretable elastoplasticity models via the neural polynomial method enabled symbolic regressions.](http://arxiv.org/abs/2307.13149) | 本文介绍了一种通过神经多项式方法实现可解释的弹性塑性模型的机器学习方法，该方法通过分为两个步骤，先通过监督学习得到一组特征映射，再通过符号回归将其转化为数学公式，从而克服了传统神经网络模型的缺乏可解释性的问题。 |
| [^73] | [Extending Path-Dependent NJ-ODEs to Noisy Observations and a Dependent Observation Framework.](http://arxiv.org/abs/2307.13147) | 该论文研究了将路径相关的NJ-ODE方法扩展到具有噪声观测和相关观测框架的问题。研究提出了两种扩展方法，并提供了理论保证和实证示例。 |
| [^74] | [Does Progress On Object Recognition Benchmarks Improve Real-World Generalization?.](http://arxiv.org/abs/2307.13136) | 通过对涵盖全球各地家庭物体的数据集进行研究，我们发现目前在目标识别基准测试上取得的进展并没有改善在真实世界中的泛化能力。 |
| [^75] | [simPLE: a visuotactile method learned in simulation to precisely pick, localize, regrasp, and place objects.](http://arxiv.org/abs/2307.13133) | 本文介绍了一种称为simPLE的方法，通过在模拟中学习来实现精确的抓取、定位、重新抓取和放置物体。simPLE包括任务感知抓取、视觉触觉感知和重新抓取规划三个主要组件，能够精确地处理多个任务，而无需先前的经验。 |
| [^76] | [A Differentially Private Weighted Empirical Risk Minimization Procedure and its Application to Outcome Weighted Learning.](http://arxiv.org/abs/2307.13127) | 本文提出了一种差分隐私加权经验风险最小化算法，可以在使用敏感数据的情况下保护隐私。这是第一个在权重ERM中应用差分隐私的算法，并且在一定的条件下提供了严格的DP保证。 |
| [^77] | [Conformal prediction for frequency-severity modeling.](http://arxiv.org/abs/2307.13124) | 这个论文提出了一个非参数的模型无关框架，用于建立保险理赔的预测区间，并具有有限样本的统计保证，扩展了split conformal prediction技术到两阶段频率-严重性建模领域，并通过使用随机森林作为严重性模型，利用了袋外机制消除了校准集的需要，并实现了具有自适应宽度的预测区间的生成。 |
| [^78] | [Pathway: a fast and flexible unified stream data processing framework for analytical and Machine Learning applications.](http://arxiv.org/abs/2307.13116) | Pathway是一个快速灵活的统一流数据处理框架，用于分析和机器学习应用。它能够在有界和无界的数据流上运行，通过Table API和分布式增量数据流驱动，在批处理和流处理场景中表现出优异的能力。 |
| [^79] | [An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment.](http://arxiv.org/abs/2307.13108) | 本文提出了一个可解释的几何加权图注意力网络（xGW-GAT），用于识别与步态障碍相关的功能网络，以推动帕金森病治疗的发展。 |
| [^80] | [Contrastive Example-Based Control.](http://arxiv.org/abs/2307.13101) | 本文提出了一种基于示例的控制方法，它通过学习隐式模型的多步转移来解决控制问题，而不是学习奖励函数。这种方法避免了复杂的正规化和时差更新，并取得了良好的结果。 |
| [^81] | [Label Noise: Correcting a Correction.](http://arxiv.org/abs/2307.13100) | 本研究提出了一种对付标签噪声引起的过拟合的直接方法，通过观察标签噪声存在时噪声广义风险的下界，提出了在训练过程中对经验风险施加下界以减轻过拟合的方法，并提供了明确且易于计算的最小可实现噪声风险界限。 |
| [^82] | [Fairness Under Demographic Scarce Regime.](http://arxiv.org/abs/2307.13081) | 这项研究探讨了在人口信息不完全可用的情况下如何提高公平性。研究发现，在替代敏感属性的属性分类器中引入不确定性意识，并对推断出的不确定性最低的人口信息样本进行公平性约束可以实现更好的公平性和准确性权衡。 |
| [^83] | [Adaptive Certified Training: Towards Better Accuracy-Robustness Tradeoffs.](http://arxiv.org/abs/2307.13078) | 本文提出了一种自适应认证训练方法，通过训练模型时使用自适应认证半径的关键观点，提高了模型的准确性和鲁棒性，推进了准确性-鲁棒性权衡的最新进展。 |
| [^84] | [General-Purpose Multi-Modal OOD Detection Framework.](http://arxiv.org/abs/2307.13069) | 本文提出了一个通用的多模态OOD检测框架，名为WOOD，在传感器故障、恶劣天气和环境变化等多种因素引起的异常情况下细粒度同时检测多个OOD场景。 |
| [^85] | [Feature Gradient Flow for Interpreting Deep Neural Networks in Head and Neck Cancer Prediction.](http://arxiv.org/abs/2307.13061) | 本文提出了特征梯度流技术，用于解释深度学习模型在人类可理解的特征上的作用。通过测量可解释特征与模型的梯度流的一致性，并通过添加正则化项训练神经网络，我们可以评估特定特征对模型的重要性。 |
| [^86] | [MARIO: Model Agnostic Recipe for Improving OOD Generalization of Graph Contrastive Learning.](http://arxiv.org/abs/2307.13055) | 提出了一个模型无关配方MARIO，用于改善图对比学习的OOD泛化性能。MARIO引入了信息瓶颈原则和不变性原则，旨在获得具有分布偏移鲁棒性和不变性的图表示。 |
| [^87] | [Graph Neural Networks For Mapping Variables Between Programs -- Extended Version.](http://arxiv.org/abs/2307.13014) | 本文提出了使用图神经网络(GNNs)基于程序的抽象语法树(ASTs)来映射变量集，以解决程序比较、分析、修复和克隆检测等任务。在初学者编程作业中进行的实验证明了变量映射的有效性。 |
| [^88] | [Maximal Independent Sets for Pooling in Graph Neural Networks.](http://arxiv.org/abs/2307.13011) | 本文提出了三种基于最大独立集合概念的图形池化方法，避免了现有方法中存在的缺点，通过实验证实了最大独立集合约束在图形池化中的重要性。 |
| [^89] | [Deep neural network improves the estimation of polygenic risk scores for breast cancer.](http://arxiv.org/abs/2307.13010) | 深度神经网络超越其他机器学习技术和统计算法，提高了估计乳腺癌多基因风险评分的准确性，能够将病例人群分成高遗传风险亚人群。 |
| [^90] | [Sparse-firing regularization methods for spiking neural networks with time-to-first spike coding.](http://arxiv.org/abs/2307.13007) | 本文提出了两种稀疏激发正则化方法，用于训练时间到第一个尖峰编码的脉冲神经网络，以提高信息处理的能效。 |
| [^91] | [DeepGATGO: A Hierarchical Pretraining-Based Graph-Attention Model for Automatic Protein Function Prediction.](http://arxiv.org/abs/2307.13004) | DeepGATGO是一种基于分层预训练的图注意模型，用于自动蛋白功能预测。它通过只使用蛋白质序列，而不需要蛋白质结构信息或网络拓扑信息，实现了可靠且计算成本更低的蛋白质功能预测。 |
| [^92] | [Extracting Molecular Properties from Natural Language with Multimodal Contrastive Learning.](http://arxiv.org/abs/2307.12996) | 该论文研究了如何使用多模态对比学习方法从自然语言中提取分子属性信息，通过改进文本检索和引入分子图扩增策略等方法提高了属性预测性能。实验结果显示相对于仅在图模态上预训练的模型，我们取得了+4.26%的AUROC增益和+1.54%的增益。 |
| [^93] | [Multi-representations Space Separation based Graph-level Anomaly-aware Detection.](http://arxiv.org/abs/2307.12994) | 本文提出了一种基于多表示空间分离的图级异常感知检测框架，以解决检测图集内异常图的问题。 |
| [^94] | [Efficiently Learning One-Hidden-Layer ReLU Networks via Schur Polynomials.](http://arxiv.org/abs/2307.12840) | 通过使用张量分解和舒尔多项式理论，我们提出了一种高效算法，可以在标准高斯分布下学习$k$个ReLU激活的线性组合。这个算法在样本和计算复杂性上接近最优，并能在高维空间中找到较小的高阶矩误差张量。 |
| [^95] | [Nonparametric Linear Feature Learning in Regression Through Regularisation.](http://arxiv.org/abs/2307.12754) | 本研究提出了一种新的非参数线性特征学习方法，对于监督学习中存在于低维线性子空间中的相关信息的预测和解释能力的提升是非常有帮助的。 |
| [^96] | [Multifidelity Covariance Estimation via Regression on the Manifold of Symmetric Positive Definite Matrices.](http://arxiv.org/abs/2307.12438) | 本论文介绍了一种在对称正定矩阵流形上进行回归求解的多保真度协方差估计器，通过构造满足正定性和可实际计算属性的马氏距离最小化。该估计器是最大似然估计器，并且能相对于其他方法显著减小估计误差。 |
| [^97] | [Adversarial Agents For Attacking Inaudible Voice Activated Devices.](http://arxiv.org/abs/2307.12204) | 本研究分析了对语音激活设备进行无声攻击的风险，并发现存在重大安全漏洞。我们提出了一种基线网络模型，并模拟了多种攻击场景，揭示了通过无需添加硬件或增加设备技能的物理访问来发现和拥有特权信息的潜力。使用深度Q学习算法，我们在少数步骤中快速拥有了所有节点。这些研究结果强调了对非传统网络和新的网络安全措施的重要性。 |
| [^98] | [Question Decomposition Improves the Faithfulness of Model-Generated Reasoning.](http://arxiv.org/abs/2307.11768) | 通过将问题分解为子问题，可以显著提高大型语言模型生成推理的忠实度。 |
| [^99] | [A Deep Learning Approach for Overall Survival Analysis with Missing Values.](http://arxiv.org/abs/2307.11465) | 提出了一个深度学习模型，通过有效利用被审查和未被审查病人的信息，预测非小细胞肺癌（NSCLC）病人的整体生存。 |
| [^100] | [Synthetic Control Methods by Density Matching under Implicit Endogeneitiy.](http://arxiv.org/abs/2307.11127) | 本文提出了一种新型的合成对照方法，通过密度匹配来解决现有SCMs中的隐式内生性问题。该方法通过将经过处理单元的结果密度与未处理单元的密度进行加权平均来估计SC权重。 |
| [^101] | [Long-Tail Theory under Gaussian Mixtures.](http://arxiv.org/abs/2307.10736) | 该论文提出了一个简单的高斯混合模型，符合Feldman的长尾理论。通过实验证明，在长尾分布情况下，非线性分类器可以提高泛化能力，而线性分类器不能。该结果强调了对于长尾分布，需要考虑罕见的训练样本以实现最佳泛化能力。 |
| [^102] | [A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning.](http://arxiv.org/abs/2307.09218) | 遗忘是深度学习中普遍存在的现象，不仅限于连续学习领域。解决遗忘问题面临多个挑战，包括平衡保留旧任务知识与快速学习新任务的挑战，管理任务干扰与冲突目标的挑战，以及防止隐私泄露等。遗忘不总是有害的，可以在某些情况下是有益且可取的，特别是在隐私保护场景中。 |
| [^103] | [DeepMem: ML Models as storage channels and their (mis-)applications.](http://arxiv.org/abs/2307.08811) | 本文提出了将机器学习模型作为存储通道的新视角，通过过度参数化来增加通道容量。通过在训练时嵌入信息，并利用黑盒访问实现信息的存储和提取。 |
| [^104] | [Retentive Network: A Successor to Transformer for Large Language Models.](http://arxiv.org/abs/2307.08621) | Retentive Network（RetNet）作为大型语言模型的基础架构，实现了训练并行、低成本推理和良好的性能。通过并行、循环和分块循环三种计算范式，RetNet具有训练并行化、低成本推理和高效的长序列建模的特点。 |
| [^105] | [Revisiting the Robustness of the Minimum Error Entropy Criterion: A Transfer Learning Case Study.](http://arxiv.org/abs/2307.08572) | 本研究重新审视了最小错误熵准则在处理非高斯噪声中的鲁棒性，并探讨了其在实际转移学习回归任务中的可行性和有用性。实验证明，在基本转移学习算法中，通过用最小错误熵代替均方误差损失，可以取得与现有方法相媲美的性能表现。 |
| [^106] | [Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models.](http://arxiv.org/abs/2307.08303) | 本论文提出了一种使用软提示调优来增强密集检索的方法（SPTAR）。通过优化任务特定的软提示并利用大型语言模型为未标记的文档生成弱查询，可以提高零样本和少样本的密集检索模型的性能。 |
| [^107] | [RegExplainer: Generating Explanations for Graph Neural Networks in Regression Task.](http://arxiv.org/abs/2307.07840) | 这项工作提出了一种新的解释方法（XAIG-R），用于解释图回归模型，通过引入信息瓶颈理论的新目标和混合框架来解决回归任务中的挑战，同时还使用对比学习策略来处理连续有序标签。 |
| [^108] | [Sharp Convergence Rates for Matching Pursuit.](http://arxiv.org/abs/2307.07679) | 本文通过提升现有的下界来匹配最佳上界，对匹配追踪的性能进行了精确描述，并构造了一个最坏情况的字典来证明现有上界的无法改进。 |
| [^109] | [MaxMin-L2-SVC-NCH: A New Method to Train Support Vector Classifier with the Selection of Model's Parameters.](http://arxiv.org/abs/2307.07343) | 本文提出了一种新的方法，用于训练支持向量分类器并选择模型参数。通过建模为极小化极大优化问题，利用投影梯度算法求解，实现了更低的时间复杂度。 |
| [^110] | [Machine Learning practices and infrastructures.](http://arxiv.org/abs/2307.06518) | 本文研究了机器学习实践中从业者与工具的互动，以及这些互动对于机器学习实践和系统开发的影响。通过实证研究，发现交互式计算平台在学习和协调实践中起到了重要的基础设施作用。 |
| [^111] | [A Bayesian approach to quantifying uncertainties and improving generalizability in traffic prediction models.](http://arxiv.org/abs/2307.05946) | 本研究提出了一种贝叶斯循环神经网络框架，通过引入归一化处理，实现交通预测模型中的不确定性量化和更高的泛化能力。 |
| [^112] | [Integrating Curricula with Replays: Its Effects on Continual Learning.](http://arxiv.org/abs/2307.05747) | 将课程与回放方法相结合可以提高持续学习的效果，通过调整回放实例与训练数据的交替频率、回放实例的顺序以及选择进入回放缓冲区的策略实现。 |
| [^113] | [Harmonizing Feature Attributions Across Deep Learning Architectures: Enhancing Interpretability and Consistency.](http://arxiv.org/abs/2307.02150) | 该论文研究了特征归因方法在不同深度学习架构之间的通用性，并探讨了在多个模型上协调特征归因的可行性。研究结果显示特征归因的协调有助于提高深度学习模型的解释性和一致性。 |
| [^114] | [Accelerated primal-dual methods with enlarged step sizes and operator learning for nonsmooth optimal control problems.](http://arxiv.org/abs/2307.00296) | 该论文研究了在非光滑最优控制问题中加速原始-对偶方法的两种方法：增大步长和算子学习。研究表明，增大步长的加速原始-对偶方法可以在保证收敛性的同时简单有效地提高计算速度；而算子学习则通过构建神经网络代理模型来加速求解涉及的偏微分方程。 |
| [^115] | [Differentially Private Distributed Estimation and Learning.](http://arxiv.org/abs/2306.15865) | 本文研究了在网络环境中的分布式估计和学习问题，通过交换私有观测信息，代理可以集体估计未知数量，而保护隐私。通过线性聚合方案和差分隐私（DP）调整的随机化方案，本研究提出了一种能够在保证隐私的同时高效组合观测数据的算法。 |
| [^116] | [Transformer Training Strategies for Forecasting Multiple Load Time Series.](http://arxiv.org/abs/2306.10891) | 转换器模型在预测多负载时间序列方面使用全局训练策略比多变量和本地训练策略具有更好的性能，平均降低了21.8%和12.8%的预测误差。 |
| [^117] | [Variability of echo state network prediction horizon for partially observed dynamical systems.](http://arxiv.org/abs/2306.10797) | 该论文研究了使用部分状态观测的动力系统，并提出了具有部分状态输入和部分或完全状态输出的回音状态网络（ESN）框架。通过应用于Lorenz系统和Chua振荡器，展示了ESN的短期预测能力和预测视角的变异性，以及ESN在学习系统动力学方面的有效性。 |
| [^118] | [G-invariant diffusion maps.](http://arxiv.org/abs/2306.07350) | 研究构造了针对连续矩阵群封闭下的流形中采样的数据集的G-不变扩散地图，能够实现等变且不变的嵌入，适用于对数据点进行聚类和对齐。 |
| [^119] | [Dictionary Learning under Symmetries via Group Representations.](http://arxiv.org/abs/2305.19557) | 本文研究在预定变换群下学习不变的字典问题。利用非阿贝尔傅里叶分析，提供了算法，建立了字典学习问题可以被有效地理解为某些矩阵优化问题的理论基础。 |
| [^120] | [Fourier-DeepONet: Fourier-enhanced deep operator networks for full waveform inversion with improved accuracy, generalizability, and robustness.](http://arxiv.org/abs/2305.17289) | 该论文提出了Fourier-DeepONet算法，可用于完全波形反演，具有对地震源的泛化能力，可提高准确性和鲁棒性，适用于具有可变源的FWI。 |
| [^121] | [Revenge of MLP in Sequential Recommendation.](http://arxiv.org/abs/2305.14675) | 本文提出了一种纯MLP顺序推荐架构TriMLP，其中加入了新颖的三角形混合器以实现标记有序的交互，以提高顺序推荐的性能表现。 |
| [^122] | [What Symptoms and How Long? An Interpretable AI Approach for Depression Detection in Social Media.](http://arxiv.org/abs/2305.13127) | 本论文介绍了一种使用可解释的人工智能方法在社交媒体中检测抑郁症的方法。该方法创新地检测和解释抑郁症状及其持续时间，并通过大规模数据集的实证分析表明优于其他方法，发现了新的未注意到的症状。 |
| [^123] | [Nonparametric Generative Modeling with Conditional and Locally-Connected Sliced-Wasserstein Flows.](http://arxiv.org/abs/2305.02164) | 本文提出了两个重要贡献：一是提出了条件切片Wasserstein流（CSWF），可以实现非参数条件建模，二是将局部连接和多尺度表示等视觉研究启发的技术引入到SWF中，从而大大提高了图像建模的效率和质量。 |
| [^124] | [DataComp: In search of the next generation of multimodal datasets.](http://arxiv.org/abs/2304.14108) | DataComp是一个基准测试，旨在通过提出新的训练集来解决数据集在机器学习生态系统中的缺陷。它提供了一个多规模设计的实验测试平台，使用12.8B个图像-文本对的新候选池，让研究人员可以通过设计新的过滤技术或策划新的数据源并评估它们的新数据集来进行创新。 |
| [^125] | [B2Opt: Learning to Optimize Black-box Optimization with Little Budget.](http://arxiv.org/abs/2304.11787) | B2Opt是一种学习优化黑箱优化的方法，可以在少量预算下实现更快而更好的性能提升。它通过设计一个强大的优化框架，自动学习并利用目标任务的廉价代理函数指导高效优化策略的设计。与现有方法相比，B2Opt具有更强的优化策略表示能力并能实现多个数量级的性能提升。 |
| [^126] | [One Explanation Does Not Fit XIL.](http://arxiv.org/abs/2304.07136) | 提出了一种解释性交互式机器学习框架（XIL）来修正模型，但同时发现"一种解释不能适用于XIL"，建议考虑多种解释。 |
| [^127] | [Similarity search in the blink of an eye with compressed indices.](http://arxiv.org/abs/2304.04759) | 本文提出一种新的向量压缩方法局部自适应量化(LVQ)，并在基于图的索引的关键优化下实现减少有效带宽同时启用随机访问友好的快速相似性计算，从而在性能和内存占用方面创造了新的最佳表现。 |
| [^128] | [FairGen: Towards Fair Graph Generation.](http://arxiv.org/abs/2303.17743) | 本文提出了一种公平感知图生成模型FairGen，通过标签损失和公平损失来对模型进行训练，并使用雅可比优化方法来实现公平性和逼真性。 |
| [^129] | [Investigating and Mitigating the Side Effects of Noisy Views in Multi-view Clustering in Practical Scenarios.](http://arxiv.org/abs/2303.17245) | 本文提出了一种理论上基础的深度MvC方法（MvCAN），旨在解决实际场景中嘈杂视图的问题，通过实现多视图一致性、互补性和噪声鲁棒性来减少嘈杂视图的副作用，并在实验证明该方法优于现有的MvC方法。 |
| [^130] | [marl-jax: Multi-agent Reinforcement Leaning framework for Social Generalization.](http://arxiv.org/abs/2303.13808) | marl-jax是一个基于DeepMind的JAX生态系和RL生态系的多智能体RL软件包，可以训练和评估代理在多样性背景下的社会普适性，提供命令行界面，适用于合作与竞争游戏环境。 |
| [^131] | [Stabilizing Transformer Training by Preventing Attention Entropy Collapse.](http://arxiv.org/abs/2303.06296) | 本文研究了Transformer的训练动态，发现低注意力熵伴随着高训练不稳定性，提出了一种简单而有效的解决方案$\sigma$Reparam，成功地防止了注意力层中的熵崩溃，促进了更稳定的训练。 |
| [^132] | [Scalable Stochastic Gradient Riemannian Langevin Dynamics in Non-Diagonal Metrics.](http://arxiv.org/abs/2303.05101) | 本文提出了两种非对角度量，可以在随机梯度采样中使用，以改善神经网络模型的贝叶斯推断性能，尤其对于具有稀疏诱导先验的全连接神经网络和具有相关先验的卷积神经网络效果显著。 |
| [^133] | [Linear CNNs Discover the Statistical Structure of the Dataset Using Only the Most Dominant Frequencies.](http://arxiv.org/abs/2303.02034) | 本研究通过理论分析和实验研究，揭示了线性卷积神经网络在学习过程中通过发现数据集的统计结构，并且仅利用数据集中最主导的频率进行发现。 |
| [^134] | [Spatial-temporal Transformer-guided Diffusion based Data Augmentation for Efficient Skeleton-based Action Recognition.](http://arxiv.org/abs/2302.13434) | 本论文介绍了一种基于时空Transformer引导的扩散数据增强方法，旨在为基于骨骼的动作识别任务生成高质量和多样化的序列动作。通过引入去噪扩散概率模型（DDPMs），本方法能够生成自然逼真的动作序列。 |
| [^135] | [Active Membership Inference Attack under Local Differential Privacy in Federated Learning.](http://arxiv.org/abs/2302.12685) | 这项研究提出了一种以局部差分隐私为基础的联邦学习中的主动成员推断攻击，该攻击利用恶意参数干扰全局模型并通过非线性决策边界推断客户端的私有训练数据，对客户端的隐私造成显著风险，并发现防止攻击所需的保护隐私噪声会严重损害联邦学习的模型效用。 |
| [^136] | [Unification of popular artificial neural network activation functions.](http://arxiv.org/abs/2302.11007) | 激活函数的统一化表示采用了Mittag-Leffler函数，可以插值不同激活函数、减轻梯度问题，并适用于不同复杂度的神经网络训练。 |
| [^137] | [Online Learning Guided Curvature Approximation: A Quasi-Newton Method with Global Non-Asymptotic Superlinear Convergence.](http://arxiv.org/abs/2302.08580) | 本文提出了第一个具有明确非渐近超线性收敛速率的全局收敛拟牛顿方法，并采用混合近端外梯度法结构和在线学习框架来更新Hessian逼近矩阵。 |
| [^138] | [Concept Algebra for Score-Based Conditional Models.](http://arxiv.org/abs/2302.03693) | 本文研究了基于分数的条件模型中学习表示的结构，并开发了一种数学形式化表达概念被编码为表示空间子空间的思想。利用这个方法，我们提出了一种简单的方法来识别给定概念对应的表示部分，并通过代数操作操纵模型所表达的概念。 |
| [^139] | [Diversity Induced Environment Design via Self-Play.](http://arxiv.org/abs/2302.02119) | 本文提出了一种利用自我对战技术的任务不可知方法，来识别环境中的观察/隐藏状态，并将多样性引入非监督环境设计框架中，从而提高了环境设计的效率和有效性。 |
| [^140] | [Faster Predict-and-Optimize with Davis-Yin Splitting.](http://arxiv.org/abs/2301.13395) | 本文介绍了一种使用Davis-Yin分裂方法实现更快的预测与优化的方法，该方法借鉴了现代凸优化的思想，能够在具有数千个变量的问题上轻松扩展。 |
| [^141] | [Plugin estimators for selective classification with out-of-distribution detection.](http://arxiv.org/abs/2301.12386) | 这篇论文提出了一种新的插件估计器方法，用于有选择性分类和外部分布检测（SCOD）问题，并且在理论上有基础，有效性高，可以推广现有方法。 |
| [^142] | [A$^2$-UAV: Application-Aware Content and Network Optimization of Edge-Assisted UAV Systems.](http://arxiv.org/abs/2301.06363) | A$^2$-UAV是一种边缘辅助无人机系统的优化框架，通过应用感知的方式解决了多跳无人机网络中带宽限制的问题，该框架考虑了深度神经网络准确性与图像压缩之间的关系，以及无人机的能量和位置等因素。 |
| [^143] | [Generalizing DP-SGD with Shuffling and Batch Clipping.](http://arxiv.org/abs/2212.05796) | 本文提出了一个通用的差分隐私算法框架，允许将任何一阶优化器与批量剪切相结合，并使用打乱等采样技术。作者的DP分析使用了$f$-DP方法，并推导出了简单的闭式表达式，同时还考虑了群体隐私。实验证明，对于特定的工作时期和群体大小，框架具有$\sqrt{g E}$ 的差分隐私依赖关系。 |
| [^144] | [FedTracker: Furnishing Ownership Verification and Traceability for Federated Learning Model.](http://arxiv.org/abs/2211.07160) | FedTracker是第一个为联邦学习模型提供所有权验证和追溯性的保护框架，采用双层保护方案，并利用持续学习原则提高保护性能。 |
| [^145] | [TEFL: Turbo Explainable Federated Learning for 6G Trustworthy Zero-Touch Network Slicing.](http://arxiv.org/abs/2210.10147) | 本文提出了一种TEFL (Turbo Explainable Federated Learning) 方法，通过可解释的联邦学习，实现了6G可信零触碰网络切片的透明性和SLA感知的零触碰服务管理。 |
| [^146] | [Self-supervised video pretraining yields human-aligned visual representations.](http://arxiv.org/abs/2210.06433) | 本论文通过自监督训练的视频预训练方法VITO得到了具有人类感知特征的视觉表示，该方法在图像理解和视频理解任务上表现出更好的泛化性和鲁棒性。 |
| [^147] | [Network Revenue Management with Demand Learning and Fair Resource-Consumption Balancing.](http://arxiv.org/abs/2207.11159) | 本文研究了基于价格的网络收益管理问题，同时考虑了需求学习和公平资源消耗平衡。提出了一种基于UCB需求学习方法的原始对偶在线策略来最大化规范化收益。 |
| [^148] | [Meta-Referential Games to Learn Compositional Learning Behaviours.](http://arxiv.org/abs/2207.08012) | 本论文提出了一种元元反游戏学习的方法来解决组合学习行为的问题，通过解决绑定问题来支持人工智能代理展示组合学习行为的能力。 |
| [^149] | [Optimal Estimation of Generic Dynamics by Path-Dependent Neural Jump ODEs.](http://arxiv.org/abs/2206.14284) | 本文研究了使用路径相关的神经跳跃ODE对通用动力学进行最优估计的问题，并通过实证研究支持了这些理论结果，展示了其在非马尔可夫数据和限价订单簿数据方面的优势。 |
| [^150] | [Deep Reinforcement Learning-Assisted Federated Learning for Robust Short-term Utility Demand Forecasting in Electricity Wholesale Markets.](http://arxiv.org/abs/2206.11715) | 本文提出了一种深度强化学习辅助联邦学习的方法，DEfect-AwaRe联邦软演员-评论家（DearFSAC），用于在电力批发市场中鲁棒地训练短期负荷预测模型，从而预测精确的短期公用电需求。 |
| [^151] | [ROI: A method for identifying organizations receiving personal data.](http://arxiv.org/abs/2204.09495) | 本论文提出了一种名为ROI的方法，可自动识别接收个人数据的组织，并通过评估10,000个安卓应用揭示了这些组织。该方法实现了95.71%的准确率。 |
| [^152] | [Real-time Neural-MPC: Deep Learning Model Predictive Control for Quadrotors and Agile Robotic Platforms.](http://arxiv.org/abs/2203.07747) | 本论文介绍了一种实时神经网络模型预测控制系统，在模型预测控制流程中高效集成复杂神经网络架构作为动力学模型，实现对四旋翼等灵活机器人平台高性能的控制。 |
| [^153] | [Bayesian Non-stationary Linear Bandits for Large-Scale Recommender Systems.](http://arxiv.org/abs/2202.03167) | 本文提出了一种基于贝叶斯方法的非平稳线性赌臂算法，用于处理大规模推荐系统中的高维上下文信息。通过使用随机投影和指数增长的权重，该算法能够适应非平稳环境中的动态变化，并取得了较好的性能。 |
| [^154] | [Tensor and Matrix Low-Rank Value-Function Approximation in Reinforcement Learning.](http://arxiv.org/abs/2201.09736) | 本文提出了一种在高维空间中使用随机低秩算法进行价值函数近似的方法，并提出了使用张量表示和PARAFAC分解的在线无模型的张量低秩算法。 |
| [^155] | [Optimal Simple Regret in Bayesian Best Arm Identification.](http://arxiv.org/abs/2111.09885) | 该论文研究了多臂赌博机问题中贝叶斯最优臂识别的速率，并提出了一种简单易行的算法，其匹配了下界，只差一个常数因子。 |
| [^156] | [End-to-End Annotator Bias Approximation on Crowdsourced Single-Label Sentiment Analysis.](http://arxiv.org/abs/2111.02326) | 本文提出一种在众包单标签情感分析中解决注释者偏差的端到端方法，通过精确的偏差建模和真实值估计来改善准确性，实验证明在样本只由单个注释者标注的情况下效果显著。 |
| [^157] | [Classification of Consumer Belief Statements From Social Media.](http://arxiv.org/abs/2106.15498) | 本研究探讨了使用复杂的专家注解在社交媒体中进行消费者信念陈述分类的准确性，比较了细粒度和抽象类别的标签，并说明复杂专家注解在高度特定的意见挖掘中的潜在优势。 |
| [^158] | [Geometric Analysis of Noisy Low-rank Matrix Recovery in the Exact Parameterized and the Overparameterized Regimes.](http://arxiv.org/abs/2105.08232) | 本文研究了噪声低秩矩阵恢复问题，提出了鲁棒错误定位几何分析算法和连续子空间优化算法，分别用于精确参数化和过度参数化的情况。通过约束等异性性质，我们提供了对全局最优解与局部解之间的最大距离的保证。 |
| [^159] | [Finite volume method network for acceleration of unsteady computational fluid dynamics: non-reacting and reacting flows.](http://arxiv.org/abs/2105.03332) | 本研究开发了一种引入有限体积方法和物理信息损失函数的神经网络模型，用于加速非稳态计算流体力学模拟。该模型可以仅使用两个先前的场来预测未来的流场，而不需要大量的流场图像。此模型在非反应流和反应流模拟中获得了良好的性能。 |
| [^160] | [Replica Analysis of the Linear Model with Markov or Hidden Markov Signal Priors.](http://arxiv.org/abs/2009.13370) | 本文使用复制方法估计了具有马尔科夫或隐马尔科夫信号先验的线性模型的自由能、平均互信息和最小均方误差（MMSE）。研究发现，在后验均值估计器下，线性模型可以分解为具有状态信息的单输入AWGN信道，而状态分布遵循马尔科夫链的随机矩阵的左Perron-Frobenius特征向量。数值结果证明，通过复制方法得到的结果与Metropolis-Hastings算法或其他近似传递算法的结果非常接近。 |
| [^161] | [Non-linear Neurons with Human-like Apical Dendrite Activations.](http://arxiv.org/abs/2003.03229) | 本论文提出了一种新的人工神经元模型和激活函数，通过使用单个神经元学习非线性决策边界，并在多个基准数据集上取得了优于传统方法的结果。 |
| [^162] | [Geometric Wavelet Scattering Networks on Compact Riemannian Manifolds.](http://arxiv.org/abs/1905.10448) | 本论文在紧致黎曼流形上定义了一种几何散射变换，该变换类似于欧几里得散射变换，具有局部同构的不变性和某些类型的微分同胚的稳定性，实证结果证明了其在几何学习任务中的实用性。 |

# 详细

[^1]: ARB：大型语言模型的高级推理基准

    ARB: Advanced Reasoning Benchmark for Large Language Models. (arXiv:2307.13692v1 [cs.CL])

    [http://arxiv.org/abs/2307.13692](http://arxiv.org/abs/2307.13692)

    ARB是一个新型基准，包含了数学、物理、生物、化学和法律领域的高级推理问题。目前的语言模型在这些任务上得分远低于50%，为了提高评估能力，我们引入了基于评分标准的评估方法。

    

    大型语言模型（LLMs）在各种定量推理和知识基准上展示了卓越的性能。然而，尽管在这些领域中还没有达到专家水平，但许多这些基准随着LLMs获得越来越高的分数而失去了效用。我们引入了ARB，一个由多个领域的高级推理问题组成的新型基准。ARB提供比以前的基准更具挑战性的测试，包括数学、物理、生物、化学和法律领域的问题。作为ARB的一部分，我们介绍了一组挑战性的数学和物理问题，需要高级符号推理和领域知识。我们评估了最近的模型，如GPT-4和Claude在ARB上的表现，并证明当前模型在更具挑战性的任务上得分远低于50%。为了改进自动和辅助评估能力，我们引入了基于评分标准的评估方法，允许GPT-4对其自身的中间推理步骤评分。

    Large Language Models (LLMs) have demonstrated remarkable performance on various quantitative reasoning and knowledge benchmarks. However, many of these benchmarks are losing utility as LLMs get increasingly high scores, despite not yet reaching expert performance in these domains. We introduce ARB, a novel benchmark composed of advanced reasoning problems in multiple fields. ARB presents a more challenging test than prior benchmarks, featuring problems in mathematics, physics, biology, chemistry, and law. As a subset of ARB, we introduce a challenging set of math and physics problems which require advanced symbolic reasoning and domain knowledge. We evaluate recent models such as GPT-4 and Claude on ARB and demonstrate that current models score well below 50% on more demanding tasks. In order to improve both automatic and assisted evaluation capabilities, we introduce a rubric-based evaluation approach, allowing GPT-4 to score its own intermediate reasoning steps. Further, we conduct 
    
[^2]: 对于具有修剪的非凸随机优化的高概率分析

    High Probability Analysis for Non-Convex Stochastic Optimization with Clipping. (arXiv:2307.13680v1 [cs.LG])

    [http://arxiv.org/abs/2307.13680](http://arxiv.org/abs/2307.13680)

    本文提供了对于具有修剪的非凸随机优化的高概率分析，同时推导了流行随机优化算法的优化界限和泛化界限，为处理重尾行为提供了理论基础。

    

    梯度修剪是稳定神经网络训练过程的常用技术。越来越多的研究表明，梯度修剪是处理随机优化中出现的重尾行为的一种有前景的技术。虽然梯度修剪很重要，但其理论保证很少。大多数理论保证只提供期望值分析，并且仅关注优化性能。在本文中，我们提供了在非凸设置中的高概率分析，并同时推导了带有梯度修剪的流行随机优化算法的优化界限和泛化界限，包括随机梯度下降及其动量和自适应步长变体。在梯度修剪的情况下，我们研究了一个重尾的假设，即梯度只有对于某些$\alpha \in (1, 2]$有界的$\alpha$-阶矩，这比标准的有界二阶矩假设要弱。

    Gradient clipping is a commonly used technique to stabilize the training process of neural networks. A growing body of studies has shown that gradient clipping is a promising technique for dealing with the heavy-tailed behavior that emerged in stochastic optimization as well. While gradient clipping is significant, its theoretical guarantees are scarce. Most theoretical guarantees only provide an in-expectation analysis and only focus on optimization performance. In this paper, we provide high probability analysis in the non-convex setting and derive the optimization bound and the generalization bound simultaneously for popular stochastic optimization algorithms with gradient clipping, including stochastic gradient descent and its variants of momentum and adaptive stepsizes. With the gradient clipping, we study a heavy-tailed assumption that the gradients only have bounded $\alpha$-th moments for some $\alpha \in (1, 2]$, which is much weaker than the standard bounded second-moment ass
    
[^3]: RED CoMETS: 一种用于符号化表示的多变量时间序列的集成分类器

    RED CoMETS: An ensemble classifier for symbolically represented multivariate time series. (arXiv:2307.13679v1 [cs.LG])

    [http://arxiv.org/abs/2307.13679](http://arxiv.org/abs/2307.13679)

    本文介绍了一种名为RED CoMETS的集成分类器，用于处理符号化表示的多变量时间序列数据。它在多变量设置中展现出竞争力的准确性，并在'HandMovementDirection'数据集上实现了最高的报告准确性。

    

    多变量时间序列分类是一个快速发展的研究领域，可在金融、医疗、工程等实际应用中使用。多变量时间序列数据的分类复杂性来自于其高维度、时间依赖性和长度不一致性。本文介绍了一种名为RED CoMETS（Random Enhanced Co-eye for Multivariate Time Series）的新型集成分类器，它解决了这些挑战。RED CoMETS基于Co-eye的成功，并将其能力扩展到处理多变量数据。使用UCR档案中的基准数据集对RED CoMETS的性能进行评估，在多变量设置中与最先进的技术相比，它显示出竞争力的准确性。值得注意的是，它在文献中对于'HandMovementDirection'数据集实现了最高的报告准确性。此外，该方法显著地改进了传统的Co-eye方法。

    Multivariate time series classification is a rapidly growing research field with practical applications in finance, healthcare, engineering, and more. The complexity of classifying multivariate time series data arises from its high dimensionality, temporal dependencies, and varying lengths. This paper introduces a novel ensemble classifier called RED CoMETS (Random Enhanced Co-eye for Multivariate Time Series), which addresses these challenges. RED CoMETS builds upon the success of Co-eye, an ensemble classifier specifically designed for symbolically represented univariate time series, and extends its capabilities to handle multivariate data. The performance of RED CoMETS is evaluated on benchmark datasets from the UCR archive, where it demonstrates competitive accuracy when compared to state-of-the-art techniques in multivariate settings. Notably, it achieves the highest reported accuracy in the literature for the 'HandMovementDirection' dataset. Moreover, the proposed method signific
    
[^4]: 关于AI问责政策的探索

    Towards an AI Accountability Policy. (arXiv:2307.13658v1 [cs.CY])

    [http://arxiv.org/abs/2307.13658](http://arxiv.org/abs/2307.13658)

    这份白皮书是对美国国家电信和信息管理局的“AI问责政策评论请求”的回应，提出了一组相互关联的AI问责政策建议。

    

    这份白皮书是对美国国家电信和信息管理局的“AI问责政策评论请求”作出的回应。在回答相关问题的关键句子末尾，提供了要求评论的问题编号的上标。该白皮书提出了一组相互关联的AI问责政策建议。

    This white paper is a response to the "AI Accountability Policy Request for Comments" by the National Telecommunications and Information Administration of the United States. The question numbers for which comments were requested are provided in superscripts at the end of key sentences answering the respective questions. The white paper offers a set of interconnected recommendations for an AI accountability policy.
    
[^5]: 强化学习的安全边界

    Safety Margins for Reinforcement Learning. (arXiv:2307.13642v1 [cs.LG])

    [http://arxiv.org/abs/2307.13642](http://arxiv.org/abs/2307.13642)

    本论文提出了一种能够通过计算代理关键性指标来生成安全边界的方法，该方法能够将可能的错误行为的后果与整体性能的预期损失联系起来。在Atari环境中的实验结果表明，随着代理接近失败状态，安全边界减小。

    

    任何自主控制器在某些情况下都可能不安全。能够定量地确定何时会发生这些不安全情况对于及时引入人类监督至关重要，例如货运应用。在这项工作中，我们展示了一个代理的情况的真正关键性可以被稳健地定义为在一些随机动作下奖励的平均减少。可以将实时可计算的代理关键性指标（即，无需实际模拟随机动作的影响）与真正的关键性进行比较，并展示如何利用这些代理指标生成安全边界，将潜在错误行为的后果直接与整体性能的预期损失联系起来。我们在Atari环境中通过APE-X和A3C的学习策略上评估了我们的方法，并展示了随着代理接近失败状态，安全边界的减小。将安全边界整合到监控程序中的创新点在于...

    Any autonomous controller will be unsafe in some situations. The ability to quantitatively identify when these unsafe situations are about to occur is crucial for drawing timely human oversight in, e.g., freight transportation applications. In this work, we demonstrate that the true criticality of an agent's situation can be robustly defined as the mean reduction in reward given some number of random actions. Proxy criticality metrics that are computable in real-time (i.e., without actually simulating the effects of random actions) can be compared to the true criticality, and we show how to leverage these proxy metrics to generate safety margins, which directly tie the consequences of potentially incorrect actions to an anticipated loss in overall performance. We evaluate our approach on learned policies from APE-X and A3C within an Atari environment, and demonstrate how safety margins decrease as agents approach failure states. The integration of safety margins into programs for monit
    
[^6]: 缩放基于机器学习的化工厂模拟：用于调整模型以诱导稳定固定点的方法

    Scaling machine learning-based chemical plant simulation: A method for fine-tuning a model to induce stable fixed points. (arXiv:2307.13621v1 [cs.LG])

    [http://arxiv.org/abs/2307.13621](http://arxiv.org/abs/2307.13621)

    本研究提出了一种缩放基于机器学习的化工厂模拟的方法，并通过微调模型解决了在应用于较大工厂时出现的循环求解不稳定的问题。

    

    理想化的化工厂基于第一原理的模型可能不准确。另一种方法是直接将机器学习（ML）模型拟合到工厂传感器数据上。我们采用一种结构化的方法：将工厂内的每个单元表示为一个ML模型。在将模型拟合到数据后，将模型连接成类似流程图的有向图。我们发现，对于较小的工厂，这种方法效果很好，但对于较大的工厂，由于流程图中存在大型和嵌套循环所导致的复杂动力学会导致循环求解器的不稳定性。我们对这个问题进行了深入分析，并表明这不仅仅是一个特殊关注点，而是一个更普遍的挑战，可能在应用于较大的工厂时会发生。为了解决这个问题，我们提出了一种方法，可以将ML模型进行微调，从而使得常规方法下的循环求解变得稳健。

    Idealized first-principles models of chemical plants can be inaccurate. An alternative is to fit a Machine Learning (ML) model directly to plant sensor data. We use a structured approach: Each unit within the plant gets represented by one ML model. After fitting the models to the data, the models are connected into a flowsheet-like directed graph. We find that for smaller plants, this approach works well, but for larger plants, the complex dynamics arising from large and nested cycles in the flowsheet lead to instabilities in the cycle solver. We analyze this problem in depth and show that it is not merely a specialized concern but rather a more pervasive challenge that will likely occur whenever ML is applied to larger plants. To address this problem, we present a way to fine-tune ML models such that solving cycles with the usual methods becomes robust again.
    
[^7]: AI与保险伦理：在风险建模中减少拟议差别的新解决方案

    AI and ethics in insurance: a new solution to mitigate proxy discrimination in risk modeling. (arXiv:2307.13616v1 [stat.ML])

    [http://arxiv.org/abs/2307.13616](http://arxiv.org/abs/2307.13616)

    该论文提出了一个新的解决方案，通过考虑变量之间的潜在相互作用，减少风险建模中的拟议差别，以实现更公平的保险定价和风险选择。

    

    机器学习的发展引起了广大公众的关注，在最近几年中，有许多新闻文章质疑其客观性：种族主义，性别歧视等。受监管机构对保险中数据道德使用的关注日益增长，保险精算师社区必须重新思考定价和风险选择实践，以实现更公平的保险。公平是一个哲学概念，在每个司法管辖区都有很多不同的定义，这些定义相互影响，目前尚未达成一致意见。在欧洲，基本权利宪章规定了有关歧视和算法中使用敏感个人数据的指导方针。如果简单地删除受保护变量可以防止任何所谓的“直接”歧视，那么模型仍然可以通过变量之间潜在的相互作用“间接”歧视个人，从而带来更好的性能（因此更好地量化风险，进行细分）。

    The development of Machine Learning is experiencing growing interest from the general public, and in recent years there have been numerous press articles questioning its objectivity: racism, sexism, \dots Driven by the growing attention of regulators on the ethical use of data in insurance, the actuarial community must rethink pricing and risk selection practices for fairer insurance. Equity is a philosophy concept that has many different definitions in every jurisdiction that influence each other without currently reaching consensus. In Europe, the Charter of Fundamental Rights defines guidelines on discrimination, and the use of sensitive personal data in algorithms is regulated. If the simple removal of the protected variables prevents any so-called `direct' discrimination, models are still able to `indirectly' discriminate between individuals thanks to latent interactions between variables, which bring better performance (and therefore a better quantification of risk, segmentation 
    
[^8]: 基于树突综合的二次神经网络优于传统的人工神经网络

    Dendritic Integration Based Quadratic Neural Networks Outperform Traditional Aritificial Ones. (arXiv:2307.13609v1 [cs.NE])

    [http://arxiv.org/abs/2307.13609](http://arxiv.org/abs/2307.13609)

    提出了一种基于树突综合的二次神经网络(DIQNN)模型，该模型在多种分类任务中表现出优越性能，超过了传统的人工神经网络。引入边界来刻画泛化误差，并将边界整合到损失函数中，加速了测试准确率的改变。

    

    将生物神经元的特性引入人工神经网络以增强计算能力是机器学习领域面临的重大挑战。受最近发现的树突遵循二次综合规则的启发，我们提出了一种新的人工神经网络模型，即基于树突综合的二次神经网络(DIQNN)。该模型在各种分类任务中表现出优越的性能，超过了传统的人工神经网络。为了降低DIQNN的计算成本，我们引入了低秩DIQNN，发现其可以保持原始DIQNN的性能。我们进一步提出了一个边界来刻画泛化误差，并理论上证明这个边界在训练过程中会单调增加。通过数值实验，我们展示了泛化误差与边界之间的一致性。最后，将这个边界整合到损失函数中后，测试准确率的改变确实加速了。

    Incorporating biological neuronal properties into Artificial Neural Networks (ANNs) to enhance computational capabilities poses a formidable challenge in the field of machine learning. Inspired by recent findings indicating that dendrites adhere to quadratic integration rules for synaptic inputs, we propose a novel ANN model, Dendritic Integration-Based Quadratic Neural Network (DIQNN). This model shows superior performance over traditional ANNs in a variety of classification tasks. To reduce the computational cost of DIQNN, we introduce the Low-Rank DIQNN, while we find it can retain the performance of the original DIQNN. We further propose a margin to characterize the generalization error and theoretically prove this margin will increase monotonically during training. And we show the consistency between generalization and our margin using numerical experiments. Finally, by integrating this margin into the loss function, the change of test accuracy is indeed accelerated. Our work cont
    
[^9]: 几何表位和配位位点的预测

    Geometric Epitope and Paratope Prediction. (arXiv:2307.13608v1 [q-bio.BM])

    [http://arxiv.org/abs/2307.13608](http://arxiv.org/abs/2307.13608)

    本文研究了预测抗体-抗原结合位点的最佳表示方法，并强调了几何信息的重要性。研究发现基于表面的模型更高效，O-GEP实验取得了显著的性能改进。

    

    抗体-抗原相互作用在识别和中和有害外来分子方面起着至关重要的作用。在本文中，我们研究了预测这两种分子结合位点的最佳表示，并强调了几何信息的重要性。具体而言，我们比较了应用于蛋白质内部（I-GEP）和外部（O-GEP）结构的不同几何深度学习方法。我们将三维坐标和谱几何描述符作为输入特征，充分利用几何信息。我们的研究表明，基于表面的模型比其他方法更高效，我们的O-GEP实验取得了显著的性能改进，达到了最先进的结果。

    Antibody-antigen interactions play a crucial role in identifying and neutralizing harmful foreign molecules. In this paper, we investigate the optimal representation for predicting the binding sites in the two molecules and emphasize the importance of geometric information. Specifically, we compare different geometric deep learning methods applied to proteins' inner (I-GEP) and outer (O-GEP) structures. We incorporate 3D coordinates and spectral geometric descriptors as input features to fully leverage the geometric information. Our research suggests that surface-based models are more efficient than other methods, and our O-GEP experiments have achieved state-of-the-art results with significant performance improvements.
    
[^10]: 基于多GPU的图机器学习模型训练大规模CFD网格

    Multi-GPU Approach for Training of Graph ML Models on large CFD Meshes. (arXiv:2307.13592v1 [cs.LG])

    [http://arxiv.org/abs/2307.13592](http://arxiv.org/abs/2307.13592)

    本论文通过将最先进的基于图的机器学习代理模型扩展到工业相关网格尺寸，实现了基于多GPU的训练方法，能够在计算流体动力学（CFD）的大规模网格上进行快速且准确的数值流动模拟预测。

    

    基于网格的数值求解器在许多设计工具链中是重要的一部分。然而，精确的模拟，如计算流体力学，是时耗和资源消耗的，因此常常使用代理模型来加速解决过程。然而，基于机器学习的代理模型在预测近似解时速度较快，但往往缺乏精度。因此，本文聚焦于开发一个预测器-校正器的方法，其中代理模型预测流场，数值求解器进行校正。本文将一个最先进的基于图的机器学习代理模型扩展到了数值流动模拟中的工业相关网格尺寸。该方法将流场划分并分配到多个GPU上，并在训练过程中进行分区之间的边界交换。所使用的图神经网络直接作用于数值网格，能够保存复杂几何形状以及所有其他信息。

    Mesh-based numerical solvers are an important part in many design tool chains. However, accurate simulations like computational fluid dynamics are time and resource consuming which is why surrogate models are employed to speed-up the solution process. Machine Learning based surrogate models on the other hand are fast in predicting approximate solutions but often lack accuracy. Thus, the development of the predictor in a predictor-corrector approach is the focus here, where the surrogate model predicts a flow field and the numerical solver corrects it. This paper scales a state-of-the-art surrogate model from the domain of graph-based machine learning to industry-relevant mesh sizes of a numerical flow simulation. The approach partitions and distributes the flow domain to multiple GPUs and provides halo exchange between these partitions during training. The utilized graph neural network operates directly on the numerical mesh and is able to preserve complex geometries as well as all oth
    
[^11]: 解决在线强化学习的样本复杂度问题

    Settling the Sample Complexity of Online Reinforcement Learning. (arXiv:2307.13586v1 [cs.LG])

    [http://arxiv.org/abs/2307.13586](http://arxiv.org/abs/2307.13586)

    本文解决了在线强化学习的样本复杂度问题，提出了一种基于模型的算法，它可以在有限时间不均匀马尔可夫决策问题中实现极小后悔的最优性。

    

    在线强化学习的一个核心问题是数据效率。虽然最近的一些工作在在线强化学习中实现了渐近最小的后悔，但这些结果的最优性仅在“大样本”情况下得到保证，为了使其算法运行最佳，需要付出巨大的预燃成本。如何在不产生任何预燃成本的情况下实现极小后悔的最优性一直是强化学习理论中的一个开放问题。我们解决了有限时间不均匀马尔可夫决策问题的这个问题。具体地，我们证明了一种修改版的单调值传播(MVP)算法，该算法是由\cite{zhang2020reinforcement}提出的一种基于模型的算法，使得后悔的量级为(模除对数因子)\begin{equation *} \min\biggr\{ \sqrt{SAH^3K}，\，HK \biggr\}，\end{equation *}其中$S$是状态数，$A$是动作数，$H$是规划时域，$K$是总的回合数。这个后悔的量级与极小化后悔量级是相匹配的。

    A central issue lying at the heart of online reinforcement learning (RL) is data efficiency. While a number of recent works achieved asymptotically minimal regret in online RL, the optimality of these results is only guaranteed in a ``large-sample'' regime, imposing enormous burn-in cost in order for their algorithms to operate optimally. How to achieve minimax-optimal regret without incurring any burn-in cost has been an open problem in RL theory.  We settle this problem for the context of finite-horizon inhomogeneous Markov decision processes. Specifically, we prove that a modified version of Monotonic Value Propagation (MVP), a model-based algorithm proposed by \cite{zhang2020reinforcement}, achieves a regret on the order of (modulo log factors) \begin{equation*}  \min\big\{ \sqrt{SAH^3K}, \,HK \big\}, \end{equation*} where $S$ is the number of states, $A$ is the number of actions, $H$ is the planning horizon, and $K$ is the total number of episodes. This regret matches the minimax 
    
[^12]: 比较正向和逆向设计范式：耐火高熵合金案例研究

    Comparing Forward and Inverse Design Paradigms: A Case Study on Refractory High-Entropy Alloys. (arXiv:2307.13581v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2307.13581](http://arxiv.org/abs/2307.13581)

    本研究比较了正向和逆向设计范式在耐火高熵合金设计中的表现，并通过两个案例研究定量比较了逆向设计方法与其他正向方案的差异。

    

    快速设计先进材料是一个非常重要的科学问题。传统的“正向”材料设计范式涉及评估多个候选材料以确定与目标特性最匹配的候选材料。然而，深度学习在先进材料领域的最新进展使得“逆向”设计范式成为可能，即通过提供目标特性，模型能够找到最优的候选材料。由于这个概念相对较新，有必要系统地评估这两种范式在实际应用中的性能。因此，本研究的目标是直接、定量地比较正向和逆向设计建模范式。我们通过考虑两个耐火高熵合金设计的案例研究，并将逆向设计方法与其他正向方案（如局部正向搜索、高通量筛选等）进行比较来实现这一目标。

    The rapid design of advanced materials is a topic of great scientific interest. The conventional, ``forward'' paradigm of materials design involves evaluating multiple candidates to determine the best candidate that matches the target properties. However, recent advances in the field of deep learning have given rise to the possibility of an ``inverse'' design paradigm for advanced materials, wherein a model provided with the target properties is able to find the best candidate. Being a relatively new concept, there remains a need to systematically evaluate how these two paradigms perform in practical applications. Therefore, the objective of this study is to directly, quantitatively compare the forward and inverse design modeling paradigms. We do so by considering two case studies of refractory high-entropy alloy design with different objectives and constraints and comparing the inverse design method to other forward schemes like localized forward search, high throughput screening, and
    
[^13]: 在通用拟合器时代重新解读生存分析

    Reinterpreting survival analysis in the universal approximator age. (arXiv:2307.13579v1 [cs.LG])

    [http://arxiv.org/abs/2307.13579](http://arxiv.org/abs/2307.13579)

    这篇论文介绍了生存分析在深度学习中的应用，提供了连接分类和回归的方法，并且提出了无需数值积分的通用拟合网络，通过大规模的数值研究证明了其优于其他方法。

    

    生存分析是统计学工具箱中不可或缺的一部分。然而，虽然大多数经典统计领域已经接受了深度学习，但是生存分析直到最近才引起深度学习社区的一些注意。这一最近的发展可能部分受到COVID-19大流行的影响。我们的目标是提供在深度学习中充分利用生存分析潜力所需的工具。一方面，我们讨论了生存分析与分类和回归的关系。另一方面，我们提供了技术工具。我们提供一个新的损失函数、评估指标，以及第一个能够无需数值积分产生生存曲线的通用拟合网络。我们通过大规模的数值研究表明，这个损失函数和模型的性能优于其他方法。

    Survival analysis is an integral part of the statistical toolbox. However, while most domains of classical statistics have embraced deep learning, survival analysis only recently gained some minor attention from the deep learning community. This recent development is likely in part motivated by the COVID-19 pandemic. We aim to provide the tools needed to fully harness the potential of survival analysis in deep learning. On the one hand, we discuss how survival analysis connects to classification and regression. On the other hand, we provide technical tools. We provide a new loss function, evaluation metrics, and the first universal approximating network that provably produces survival curves without numeric integration. We show that the loss function and model outperform other approaches using a large numerical study.
    
[^14]: PT$\mathrm{L}^{p}$：部分传输$\mathrm{L}^{p}$距离

    PT$\mathrm{L}^{p}$: Partial Transport $\mathrm{L}^{p}$ Distances. (arXiv:2307.13571v1 [cs.LG])

    [http://arxiv.org/abs/2307.13571](http://arxiv.org/abs/2307.13571)

    本文介绍了部分传输$\mathrm{L}^{p}$距离作为一种新型度量，用于比较通用信号，并且利用部分传输距离的鲁棒性。同时，还介绍了该距离的切片变体，可以快速比较信号。最后，展示了该方法在实际应用中的效果。

    

    最优传输及其相关问题，在机器学习中已被证明是计算概率或正测度之间有意义的距离的有价值工具，包括最优部分传输。这种成功引起了对传输基于距离的定义的越来越多的兴趣，使得可以比较带符号的测度和更一般的多通道信号。传输$\mathrm{L}^{p}$距离是最优传输框架到带符号和可能是多通道信号的显著扩展。在本文中，我们引入了部分传输$\mathrm{L}^{p}$距离作为一种用于比较通用信号的新型度量，从部分传输距离的鲁棒性中获益。我们提供了一些理论背景，例如最优方案的存在和距离在各种极限情况下的行为。此外，我们还引入了这些距离的切片变体，可以快速比较通用信号。最后，我们展示了该方法在应用中的使用。

    Optimal transport and its related problems, including optimal partial transport, have proven to be valuable tools in machine learning for computing meaningful distances between probability or positive measures. This success has led to a growing interest in defining transport-based distances that allow for comparing signed measures and, more generally, multi-channeled signals. Transport $\mathrm{L}^{p}$ distances are notable extensions of the optimal transport framework to signed and possibly multi-channeled signals. In this paper, we introduce partial transport $\mathrm{L}^{p}$ distances as a new family of metrics for comparing generic signals, benefiting from the robustness of partial transport distances. We provide theoretical background such as the existence of optimal plans and the behavior of the distance in various limits. Furthermore, we introduce the sliced variation of these distances, which allows for rapid comparison of generic signals. Finally, we demonstrate the applicatio
    
[^15]: 决策导向学习：基础、现状、基准和未来机会

    Decision-Focused Learning: Foundations, State of the Art, Benchmark and Future Opportunities. (arXiv:2307.13565v1 [cs.LG])

    [http://arxiv.org/abs/2307.13565](http://arxiv.org/abs/2307.13565)

    决策导向学习是一个新兴的机器学习范式，它集成了预测和优化，旨在优化决策。本文全面回顾了决策导向学习的相关技术，提出了分类法并进行了实证评估，探讨了当前和未来研究方向。

    

    决策导向学习（DFL）是一种新兴的机器学习范式，它训练模型以优化决策，在一个端到端的系统中集成了预测和优化。这个范式有望在许多实际应用中革命性地改变决策制定，这些应用在不确定性下运作，在这些决策模型中估计未知参数经常成为一个重要障碍。本文对DFL进行了全面的回顾。它对各种技术进行了深入分析，以整合机器学习和优化模型，引入了一种根据其独特特征来区分DFL方法的分类法，并对这些方法进行了广泛的实证评估，提出了适用于DFL的合适基准数据集和任务。最后，本研究提供了关于DFL研究中当前和潜在未来方向的宝贵见解。

    Decision-focused learning (DFL) is an emerging paradigm in machine learning which trains a model to optimize decisions, integrating prediction and optimization in an end-to-end system. This paradigm holds the promise to revolutionize decision-making in many real-world applications which operate under uncertainty, where the estimation of unknown parameters within these decision models often becomes a substantial roadblock. This paper presents a comprehensive review of DFL. It provides an in-depth analysis of the various techniques devised to integrate machine learning and optimization models introduces a taxonomy of DFL methods distinguished by their unique characteristics, and conducts an extensive empirical evaluation of these methods proposing suitable benchmark dataset and tasks for DFL. Finally, the study provides valuable insights into current and potential future avenues in DFL research.
    
[^16]: 节点注入链接窃取攻击

    Node Injection Link Stealing Attack. (arXiv:2307.13548v1 [cs.CR])

    [http://arxiv.org/abs/2307.13548](http://arxiv.org/abs/2307.13548)

    本文提出了一种隐秘且有效的攻击方法，通过推断图结构数据中的私有链接来暴露GNNs的隐私漏洞，并提出了保护隐私和保持模型效能的方法。同时，我们研究了将差分隐私机制应用于减轻攻击的影响，并分析了隐私保护和模型效能的权衡。

    

    本文提出了一种隐秘且有效的攻击方法，通过推断图结构数据中的私有链接来暴露图神经网络（GNNs）的隐私漏洞。我们关注新节点加入图并使用API查询预测的归纳设置，研究私有边缘信息的潜在泄露。我们还提出了在保持模型效能的同时保护隐私的方法。与现有技术相比，我们的攻击在推断链接方面表现出优越性能。此外，我们还研究了将差分隐私（DP）机制应用于减轻我们所提出的攻击的影响，并分析了隐私保护与模型效能之间的权衡。我们的工作突出了GNNs中固有的隐私漏洞，强调了为其应用开发强大的保护隐私机制的重要性。

    In this paper, we present a stealthy and effective attack that exposes privacy vulnerabilities in Graph Neural Networks (GNNs) by inferring private links within graph-structured data. Focusing on the inductive setting where new nodes join the graph and an API is used to query predictions, we investigate the potential leakage of private edge information. We also propose methods to preserve privacy while maintaining model utility. Our attack demonstrates superior performance in inferring the links compared to the state of the art. Furthermore, we examine the application of differential privacy (DP) mechanisms to mitigate the impact of our proposed attack, we analyze the trade-off between privacy preservation and model utility. Our work highlights the privacy vulnerabilities inherent in GNNs, underscoring the importance of developing robust privacy-preserving mechanisms for their application.
    
[^17]: 为组合优化问题应用迁移学习技术

    Transfer Learning for Portfolio Optimization. (arXiv:2307.13546v1 [q-fin.PM])

    [http://arxiv.org/abs/2307.13546](http://arxiv.org/abs/2307.13546)

    本文研究了如何利用迁移学习技术解决金融组合优化问题，并引入了新概念"迁移风险"，通过实验证明了迁移风险与迁移学习方法整体性能之间的强相关性，并提出了通过迁移风险识别适当的源任务来提高迁移学习方法效率和效果的方法。数值实验还为不同设置下的投资组合管理提供了有价值的新见解。

    

    在这项工作中，我们探索了利用迁移学习技术解决金融组合优化问题的可能性。我们引入了一个新的概念，即“迁移风险”，在迁移学习的优化框架中。通过一系列数值实验，我们从跨洲别迁移、跨行业迁移和跨频率迁移三个类别进行了探究。具体而言：1. 建立了迁移风险与迁移学习方法整体性能之间的强相关性，强调了迁移风险作为可行指标的重要性；2. 发现迁移风险提供了一种计算效率高的方法，用于在迁移学习中识别适当的源任务，从而提高了迁移学习方法的效率和效果；3. 此外，数值实验为不同设置下的投资组合管理提供了有价值的新见解。

    In this work, we explore the possibility of utilizing transfer learning techniques to address the financial portfolio optimization problem. We introduce a novel concept called "transfer risk", within the optimization framework of transfer learning. A series of numerical experiments are conducted from three categories: cross-continent transfer, cross-sector transfer, and cross-frequency transfer. In particular, 1. a strong correlation between the transfer risk and the overall performance of transfer learning methods is established, underscoring the significance of transfer risk as a viable indicator of "transferability"; 2. transfer risk is shown to provide a computationally efficient way to identify appropriate source tasks in transfer learning, enhancing the efficiency and effectiveness of the transfer learning approach; 3. additionally, the numerical experiments offer valuable new insights for portfolio management across these different settings.
    
[^18]: 在网络中高效动态排序的模型

    A model for efficient dynamical ranking in networks. (arXiv:2307.13544v1 [physics.soc-ph])

    [http://arxiv.org/abs/2307.13544](http://arxiv.org/abs/2307.13544)

    该论文提出了一种受物理启发的方法，用于在定向时态网络中推断节点的动态排序，通过求解线性方程组实现，仅需调整一个参数，具有可扩展性和高效性。在各种应用中的测试结果显示，该方法比现有方法更好地预测了动态排序。

    

    我们提出了一种受物理启发的方法，用于推断定向时态网络中的动态排序 - 每个定向且带时间戳的边反映了一对交互的结果和时间。每个节点的推断排序是实值且随时间变化的，每次新的边缘都会提高或降低节点的估计强度或声望，这在真实情景中经常观察到，包括游戏序列，锦标赛或动物等级的相互作用。我们的方法通过求解一组线性方程来工作，并且只需要调整一个参数。因此，对应的算法是可扩展且高效的。我们通过评估方法在各种应用中预测交互（边缘存在）及其结果（边缘方向）的能力来测试我们的方法，包括合成数据和真实数据。我们的分析显示，在许多情况下，我们的方法比现有的方法更好地预测了动态排序。

    We present a physics-inspired method for inferring dynamic rankings in directed temporal networks - networks in which each directed and timestamped edge reflects the outcome and timing of a pairwise interaction. The inferred ranking of each node is real-valued and varies in time as each new edge, encoding an outcome like a win or loss, raises or lowers the node's estimated strength or prestige, as is often observed in real scenarios including sequences of games, tournaments, or interactions in animal hierarchies. Our method works by solving a linear system of equations and requires only one parameter to be tuned. As a result, the corresponding algorithm is scalable and efficient. We test our method by evaluating its ability to predict interactions (edges' existence) and their outcomes (edges' directions) in a variety of applications, including both synthetic and real data. Our analysis shows that in many cases our method's performance is better than existing methods for predicting dyna
    
[^19]: 在密集分类中使用自适应标签扰动进行模型校准

    Model Calibration in Dense Classification with Adaptive Label Perturbation. (arXiv:2307.13539v1 [cs.CV])

    [http://arxiv.org/abs/2307.13539](http://arxiv.org/abs/2307.13539)

    本文提出了一种自适应标签扰动的模型校准方法，使用自校准二进制交叉熵损失来统一不同形式的标签扰动过程。该方法通过最大化预测熵来改善模型校准，并在保持分类准确性的同时纠正校准问题。

    

    对于安全相关的应用程序来说，产生可信赖的深度神经网络是至关重要的，其预测与置信度相关，可以代表正确性的可能性，以供后续决策使用。现有的密集二分类模型容易过于自信。为了改善模型校准，我们提出了自适应随机标签扰动（ASLP），它为每个训练图像学习一个独特的标签扰动级别。ASLP使用我们提出的自校准二进制交叉熵（SC-BCE）损失，将包括随机方法（如扰动标签）和标签平滑在内的标签扰动过程统一起来，以在保持分类率的同时纠正校准问题。ASLP采用经典统计力学的最大熵推断，以最大化相对于缺失信息的预测熵。它可以在以下情况下执行：（1）在已知数据上保持分类准确性作为保守解决方案，或（2）专门改善模型校准。

    For safety-related applications, it is crucial to produce trustworthy deep neural networks whose prediction is associated with confidence that can represent the likelihood of correctness for subsequent decision-making. Existing dense binary classification models are prone to being over-confident. To improve model calibration, we propose Adaptive Stochastic Label Perturbation (ASLP) which learns a unique label perturbation level for each training image. ASLP employs our proposed Self-Calibrating Binary Cross Entropy (SC-BCE) loss, which unifies label perturbation processes including stochastic approaches (like DisturbLabel), and label smoothing, to correct calibration while maintaining classification rates. ASLP follows Maximum Entropy Inference of classic statistical mechanics to maximise prediction entropy with respect to missing information. It performs this while: (1) preserving classification accuracy on known data as a conservative solution, or (2) specifically improves model cali
    
[^20]: INFINITY: 针对雷诺平均纳维-斯托克斯方程的神经场建模

    INFINITY: Neural Field Modeling for Reynolds-Averaged Navier-Stokes Equations. (arXiv:2307.13538v1 [cs.LG])

    [http://arxiv.org/abs/2307.13538](http://arxiv.org/abs/2307.13538)

    INFINITY是一个使用隐式神经表示的深度学习模型，可以高效准确地近似复杂的物理现象，特别适用于设计探索和形状优化任务。

    

    对于数值设计来说，开发高效准确的替代模型至关重要。它们可以近似复杂的物理现象，从而减少直接数值模拟的计算负担。我们提出了INFINITY，这是一个利用隐式神经表示（INRs）的深度学习模型，用于解决这个挑战。我们的框架将几何信息和物理场编码成紧凑的表示，并学习它们之间的映射以推断物理场。我们以一个翼型设计优化问题作为示例任务，并在具有挑战性的AirfRANS数据集上评估我们的方法，该数据集与真实的工业用例非常接近。实验结果表明，我们的框架通过准确推断体积和曲面上的物理场实现了最先进的性能。此外，我们还展示了它在设计探索和形状优化等领域的适用性：我们的模型可以正确预测。

    For numerical design, the development of efficient and accurate surrogate models is paramount. They allow us to approximate complex physical phenomena, thereby reducing the computational burden of direct numerical simulations. We propose INFINITY, a deep learning model that utilizes implicit neural representations (INRs) to address this challenge. Our framework encodes geometric information and physical fields into compact representations and learns a mapping between them to infer the physical fields. We use an airfoil design optimization problem as an example task and we evaluate our approach on the challenging AirfRANS dataset, which closely resembles real-world industrial use-cases. The experimental results demonstrate that our framework achieves state-of-the-art performance by accurately inferring physical fields throughout the volume and surface. Additionally we demonstrate its applicability in contexts such as design exploration and shape optimization: our model can correctly pre
    
[^21]: 算法和稀疏主成分分析的障碍是否适用于其他结构设置？

    Do algorithms and barriers for sparse principal component analysis extend to other structured settings?. (arXiv:2307.13535v1 [stat.ML])

    [http://arxiv.org/abs/2307.13535](http://arxiv.org/abs/2307.13535)

    该论文研究了在尖峰Wishart模型下，通过一类子空间并集模型捕捉信号结构的主成分分析问题。通过统计和计算的视角，我们建立了基本限制，并展示了自然的投影功率方法在解决方案的统计近似最优邻域中的局部收敛性。我们还通过具体案例的分析展示了计算难度。结果表明，对于基本稀疏PCA观察到的现象在其结构化对应物中也同样存在。

    

    在尖峰Wishart模型下，我们研究了一种主成分分析问题，其中信号中的结构通过一类子空间并集模型来捕捉。这个通用类别包括基本稀疏PCA以及带有图稀疏性的变体。为了在统计和计算的统一视角下研究这些问题，我们建立了与问题实例的几何有关的基本限制，并展示了自然的投影功率方法在解决方案的统计近似最优邻域中的局部收敛性。我们通过对普适基础中路径稀疏性和树稀疏性的两种重要特殊情况进行端到端分析，补充了这些结果，展示了初始化方法和相匹配的计算难度证据。总的来说，我们的结果表明，对于基本稀疏PCA观察到的几个现象自然地扩展到其结构化对应物中。

    We study a principal component analysis problem under the spiked Wishart model in which the structure in the signal is captured by a class of union-of-subspace models. This general class includes vanilla sparse PCA as well as its variants with graph sparsity. With the goal of studying these problems under a unified statistical and computational lens, we establish fundamental limits that depend on the geometry of the problem instance, and show that a natural projected power method exhibits local convergence to the statistically near-optimal neighborhood of the solution. We complement these results with end-to-end analyses of two important special cases given by path and tree sparsity in a general basis, showing initialization methods and matching evidence of computational hardness. Overall, our results indicate that several of the phenomena observed for vanilla sparse PCA extend in a natural fashion to its structured counterparts.
    
[^22]: 可微湍流 II

    Differentiable Turbulence II. (arXiv:2307.13533v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2307.13533](http://arxiv.org/abs/2307.13533)

    该研究开发了一个框架，将深度学习模型嵌入到通用有限元数值方案中，用于解Navier-Stokes方程，并通过学习多尺度图神经网络实现了子网格尺度闭合。在实现多种流动情况时进行测试验证，结果表明学到的闭合模型在速度加快10倍的更细网格上能够达到与传统大涡模型相当的精度。

    

    可微分流体模拟器越来越被证明是在计算流体力学（CFD）中开发数据驱动模型的有用工具。可微分湍流或者说机器学习（ML）模型嵌入CFD解算算法的端到端训练，既具备了基于物理模拟的泛化能力和有限的前期成本，又具备了深度学习方法的灵活性和自动化训练。我们开发了一个框架，将深度学习模型集成到通用有限元数值方案中，用于解Navier-Stokes方程，应用该技术学习多尺度图神经网络来进行子网格尺度闭合。我们在几种反向阶梯流的实现上演示了该方法，测试了不同雷诺数和新几何形状。我们展示了学到的闭合模型在相当于速度加快10倍的更细网格上可以达到与传统大涡模拟相当的精度。

    Differentiable fluid simulators are increasingly demonstrating value as useful tools for developing data-driven models in computational fluid dynamics (CFD). Differentiable turbulence, or the end-to-end training of machine learning (ML) models embedded in CFD solution algorithms, captures both the generalization power and limited upfront cost of physics-based simulations, and the flexibility and automated training of deep learning methods. We develop a framework for integrating deep learning models into a generic finite element numerical scheme for solving the Navier-Stokes equations, applying the technique to learn a sub-grid scale closure using a multi-scale graph neural network. We demonstrate the method on several realizations of flow over a backwards-facing step, testing on both unseen Reynolds numbers and new geometry. We show that the learned closure can achieve accuracy comparable to traditional large eddy simulation on a finer grid that amounts to an equivalent speedup of 10x.
    
[^23]: 使用神经算子实现湍流的长期预测

    Towards Long-Term predictions of Turbulence using Neural Operators. (arXiv:2307.13517v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2307.13517](http://arxiv.org/abs/2307.13517)

    本研究使用神经算子模型预测湍流流动，通过探索不同模型配置和引入正则化项，提高了预测的准确性和稳定性。研究强调了需要改进的深度学习模型指标，并呼吁进一步研究复杂流动和实际基准测试指标。

    

    本文探讨了使用神经算子预测湍流流动的方法，重点研究了傅立叶神经算子(FNO)模型。它旨在利用机器学习开发湍流流动仿真的降阶/代理模型。研究分析了不同的模型配置，U-NET结构(UNO和U-FNET)在准确性和稳定性方面表现优于标准FNO。U-FNET在预测高雷诺数湍流时表现出色。正则化项，如梯度和稳定性损失，对于稳定准确的预测至关重要。研究强调了需要改进的深度学习模型在流体流动预测中的度量指标。进一步研究应该关注处理复杂流动和实际基准测试指标的模型。

    This paper explores Neural Operators to predict turbulent flows, focusing on the Fourier Neural Operator (FNO) model. It aims to develop reduced-order/surrogate models for turbulent flow simulations using Machine Learning. Different model configurations are analyzed, with U-NET structures (UNO and U-FNET) performing better than the standard FNO in accuracy and stability. U-FNET excels in predicting turbulence at higher Reynolds numbers. Regularization terms, like gradient and stability losses, are essential for stable and accurate predictions. The study emphasizes the need for improved metrics for deep learning models in fluid flow prediction. Further research should focus on models handling complex flows and practical benchmarking metrics.
    
[^24]: 不规则时间序列的连续时间证据分布

    Continuous Time Evidential Distributions for Irregular Time Series. (arXiv:2307.13503v1 [cs.LG])

    [http://arxiv.org/abs/2307.13503](http://arxiv.org/abs/2307.13503)

    该论文提出了一种在连续时间中学习不规则时间序列的证据分布的策略，能够在任何感兴趣的时间上对部分观测到的特征进行良好校准和灵活的推断，并且在稀疏、不规则观测的时间上扩展不确定性。该方法在时间序列分类任务上表现出竞争性的性能，并能够在遇到噪音数据时实现基于不确定性的推断。

    

    在许多现实世界中的场景中，如医疗保健领域，不规则时间序列很难进行预测。当观测不连续时，在任何给定时间推断特征的值是困难的，因为它可能取决于最后一次观察的时间而具有一系列的值。为了描述这种不确定性，我们提出了EDICT，一种在连续时间中学习不规则时间序列的证据分布的策略。这个分布可以在任何感兴趣的时间上对部分观测到的特征进行良好校准和灵活的推断，同时在稀疏、不规则观测的时间上扩展不确定性。我们演示了EDICT在具有挑战性的时间序列分类任务上取得了竞争性的性能，并在遇到噪音数据时实现了基于不确定性的推断。

    Prevalent in many real-world settings such as healthcare, irregular time series are challenging to formulate predictions from. It is difficult to infer the value of a feature at any given time when observations are sporadic, as it could take on a range of values depending on when it was last observed. To characterize this uncertainty we present EDICT, a strategy that learns an evidential distribution over irregular time series in continuous time. This distribution enables well-calibrated and flexible inference of partially observed features at any time of interest, while expanding uncertainty temporally for sparse, irregular observations. We demonstrate that EDICT attains competitive performance on challenging time series classification tasks and enabling uncertainty-guided inference when encountering noisy data.
    
[^25]: 深度强化学习在强健目标导向财富管理中的应用

    Deep Reinforcement Learning for Robust Goal-Based Wealth Management. (arXiv:2307.13501v1 [q-fin.PM])

    [http://arxiv.org/abs/2307.13501](http://arxiv.org/abs/2307.13501)

    本文提出了一种基于深度强化学习的强健目标导向财富管理方法，通过实验证明该方法在模拟和历史市场数据上优于传统的目标导向财富管理基准。

    

    目标导向投资是一种优先实现特定财务目标的财富管理方法。由于需要选择适当的投资直到达到目标，因此它自然地被形式化为一种顺序决策问题。因此，强化学习作为一种适用于顺序决策的机器学习技术，为优化这些投资策略提供了一个有前途的路径。本文提出了一种基于深度强化学习的强健目标导向财富管理的新方法。实验结果表明，在模拟和历史市场数据上，该方法优于多个目标导向财富管理基准。

    Goal-based investing is an approach to wealth management that prioritizes achieving specific financial goals. It is naturally formulated as a sequential decision-making problem as it requires choosing the appropriate investment until a goal is achieved. Consequently, reinforcement learning, a machine learning technique appropriate for sequential decision-making, offers a promising path for optimizing these investment strategies. In this paper, a novel approach for robust goal-based wealth management based on deep reinforcement learning is proposed. The experimental results indicate its superiority over several goal-based wealth management benchmarks on both simulated and historical market data.
    
[^26]: 使用异构图神经网络寻找洗钱者

    Finding Money Launderers Using Heterogeneous Graph Neural Networks. (arXiv:2307.13499v1 [cs.LG])

    [http://arxiv.org/abs/2307.13499](http://arxiv.org/abs/2307.13499)

    本文介绍了一种使用异构图神经网络来寻找洗钱者的方法，该方法在真实的银行交易和商业角色数据构建的大型异构网络中识别洗钱活动。为了解决洗钱活动中犯罪分子的合作问题，我们扩展了同质图神经网络方法，提出了一种新颖的消息聚合方法。

    

    当前的反洗钱系统主要基于规则，存在明显的问题，难以高效和准确地检测洗钱活动。因此，近年来出现了对利用机器学习的替代方法的探索热潮。由于犯罪分子通常在洗钱活动中合作，因此考虑到不同类型的客户关系和链接变得至关重要。本文介绍了一种基于图神经网络（GNN）的方法，从挪威最大的银行DNB的真实银行交易和商业角色数据构建的大型异构网络中识别洗钱活动。具体而言，我们扩展了一种称为消息传递神经网络（MPNN）的同质GNN方法，以在异构图上有效运行。作为该方法的一部分，我们提出了一种新颖的方法，用于在图的不同边之间聚合消息。

    Current anti-money laundering (AML) systems, predominantly rule-based, exhibit notable shortcomings in efficiently and precisely detecting instances of money laundering. As a result, there has been a recent surge toward exploring alternative approaches, particularly those utilizing machine learning. Since criminals often collaborate in their money laundering endeavors, accounting for diverse types of customer relations and links becomes crucial. In line with this, the present paper introduces a graph neural network (GNN) approach to identify money laundering activities within a large heterogeneous network constructed from real-world bank transactions and business role data belonging to DNB, Norway's largest bank. Specifically, we extend the homogeneous GNN method known as the Message Passing Neural Network (MPNN) to operate effectively on a heterogeneous graph. As part of this procedure, we propose a novel method for aggregating messages across different edges of the graph. Our finding
    
[^27]: Zshot：一个用于零样本命名实体识别和关系抽取的开源框架

    Zshot: An Open-source Framework for Zero-Shot Named Entity Recognition and Relation Extraction. (arXiv:2307.13497v1 [cs.CL])

    [http://arxiv.org/abs/2307.13497](http://arxiv.org/abs/2307.13497)

    Zshot是一个开源框架，用于零样本命名实体识别和关系抽取，通过比较不同的最新ZSL方法，支持研究人员和工业界的需求。

    

    零样本学习（ZSL）任务涉及在训练过程中未见过的文本中识别实体或关系。由于特定领域中标注数据的稀缺性，ZSL已成为一个重要的研究领域，并且在近年来应用范围已大幅增长。随着大型预训练语言模型的出现，提出了许多新的方法，ZSL性能显著提升。研究界和工业界对一个全面支持最新方法和预训练模型开发和可访问性的ZSL框架的需求不断增长。本研究提出了一个名为Zshot的创新ZSL框架，旨在解决上述挑战。我们的主要目标是提供一个平台，允许研究人员使用标准基准数据集比较不同的最新ZSL方法。此外，我们设计了一个支持工业界的框架，具备易用性、灵活性和可扩展性。

    The Zero-Shot Learning (ZSL) task pertains to the identification of entities or relations in texts that were not seen during training. ZSL has emerged as a critical research area due to the scarcity of labeled data in specific domains, and its applications have grown significantly in recent years. With the advent of large pretrained language models, several novel methods have been proposed, resulting in substantial improvements in ZSL performance. There is a growing demand, both in the research community and industry, for a comprehensive ZSL framework that facilitates the development and accessibility of the latest methods and pretrained models.In this study, we propose a novel ZSL framework called Zshot that aims to address the aforementioned challenges. Our primary objective is to provide a platform that allows researchers to compare different state-of-the-art ZSL methods with standard benchmark datasets. Additionally, we have designed our framework to support the industry with readi
    
[^28]: Duet: 高效且可扩展的混合神经关系理解

    Duet: efficient and scalable hybriD neUral rElation undersTanding. (arXiv:2307.13494v1 [cs.DB])

    [http://arxiv.org/abs/2307.13494](http://arxiv.org/abs/2307.13494)

    Duet是一种高效且可扩展的混合神经关系理解方法，旨在解决基数估计问题中高成本和难以区分的采样方法，并通过可微分的预测过程改进模型的准确性。

    

    基于概率分布估计的基数估计方法相较于传统方法取得了高精度的估计结果。然而，最先进的方法由于在处理范围查询时使用的采样方法而导致估计成本较高。此外，这种采样方法也使得它们难以区分，因此来自查询工作负载的监督信号很难训练模型以提高基数估计的准确性。在本文中，我们提出了一种新的混合确定性建模方法（Duet）用于基数估计问题，与以前的方法相比，具有更好的效率和可扩展性。Duet可以以更低的时间和内存成本直接估计范围查询的基数，并且以可区分的形式呈现。由于此方法的预测过程是可微分的，我们可以将估计误差较大的查询纳入训练过程以进行改进。

    Cardinality estimation methods based on probability distribution estimation have achieved high-precision estimation results compared to traditional methods. However, the most advanced methods suffer from high estimation costs due to the sampling method they use when dealing with range queries. Also, such a sampling method makes them difficult to differentiate, so the supervision signal from the query workload is difficult to train the model to improve the accuracy of cardinality estimation. In this paper, we propose a new hybrid and deterministic modeling approach (Duet) for the cardinality estimation problem which has better efficiency and scalability compared to previous approaches. Duet allows for direct cardinality estimation of range queries with significantly lower time and memory costs, as well as in a differentiable form. As the prediction process of this approach is differentiable, we can incorporate queries with larger model estimation errors into the training process to addr
    
[^29]: 基于有理核的复频率响应函数插值

    Rational kernel-based interpolation for complex-valued frequency response functions. (arXiv:2307.13484v1 [cs.CE])

    [http://arxiv.org/abs/2307.13484](http://arxiv.org/abs/2307.13484)

    本论文提出了一种基于有理核的复频率响应函数插值方法，通过引入新的复值函数再生核希尔伯特空间，并结合低阶有理函数进行自适应插值，解决了频率响应函数拟合过程中标准核方法表现不佳的问题。

    

    本论文研究了基于核的逼近方法在复值函数数据中的应用，其中特别关注频域中的偏微分方程的频率响应函数。在这个设置中，核方法越来越常用，然而标准的核方法表现不佳。此外，在复值情况下，底层核对的数学含义和数学推导尚待解决。我们引入了复值函数的再生核希尔伯特空间，并将带有核对的复值插值问题转化为这些空间中的最小范数插值问题。此外，我们将插值器与低阶有理函数结合，其中阶数根据一种新的模型选择准则自适应选择。来自不同领域（包括电磁学和声学）的例子的数值结果说明了该方法的性能。

    This work is concerned with the kernel-based approximation of a complex-valued function from data, where the frequency response function of a partial differential equation in the frequency domain is of particular interest. In this setting, kernel methods are employed more and more frequently, however, standard kernels do not perform well. Moreover, the role and mathematical implications of the underlying pair of kernels, which arises naturally in the complex-valued case, remain to be addressed. We introduce new reproducing kernel Hilbert spaces of complex-valued functions, and formulate the problem of complex-valued interpolation with a kernel pair as minimum norm interpolation in these spaces. Moreover, we combine the interpolant with a low-order rational function, where the order is adaptively selected based on a new model selection criterion. Numerical results on examples from different fields, including electromagnetics and acoustic examples, illustrate the performance of the metho
    
[^30]: 探索MLOps动态: 在真实世界的机器学习项目中的实验分析

    Exploring MLOps Dynamics: An Experimental Analysis in a Real-World Machine Learning Project. (arXiv:2307.13473v1 [cs.SE])

    [http://arxiv.org/abs/2307.13473](http://arxiv.org/abs/2307.13473)

    本研究在真实机器学习项目中进行了实验分析，旨在优化MLOps工作流程并提供实用的提示和建议，强调积极规划和持续改进，以增强MLOps的效率。

    

    本文提出了一个实验，重点是优化MLOps（机器学习运营）过程，这是高效实施机器学习项目的关键方面。目标是识别模式和见解，以提升MLOps工作流程，考虑到其在真实世界模型开发场景中的迭代和相互依赖性。实验涵盖了完整的MLOps工作流程，涵盖了问题定义、数据获取、数据准备、模型开发、模型部署、监控、管理、可扩展性以及治理和合规性等关键阶段。实验结果得出了实用的提示和建议，强调了对MLOps工作流程的积极规划和持续改进。实验通过一个真实的机器学习项目有机地集成，该项目在生产环境中按照MLOps过程的关键阶段进行操作，处理大规模结构化数据。采用了系统化的跟踪方法。

    This article presents an experiment focused on optimizing the MLOps (Machine Learning Operations) process, a crucial aspect of efficiently implementing machine learning projects. The objective is to identify patterns and insights to enhance the MLOps workflow, considering its iterative and interdependent nature in real-world model development scenarios.  The experiment involves a comprehensive MLOps workflow, covering essential phases like problem definition, data acquisition, data preparation, model development, model deployment, monitoring, management, scalability, and governance and compliance. Practical tips and recommendations are derived from the results, emphasizing proactive planning and continuous improvement for the MLOps workflow.  The experimental investigation was strategically integrated within a real-world ML project which followed essential phases of the MLOps process in a production environment, handling large-scale structured data. A systematic tracking approach was e
    
[^31]: 组合拍卖和图神经网络在本地能量灵活市场中的应用

    Combinatorial Auctions and Graph Neural Networks for Local Energy Flexibility Markets. (arXiv:2307.13470v1 [cs.LG])

    [http://arxiv.org/abs/2307.13470](http://arxiv.org/abs/2307.13470)

    本研究提出了一个新的组合拍卖框架用于本地能量灵活市场，通过设计图神经网络模型解决了NP完全的问题，实验结果显示了使用机器学习在本地市场中分配能量灵活资源的潜力。

    

    本文提出了一个新的组合拍卖框架用于本地能量灵活市场，解决了生产者无法打包多个灵活时间间隔的问题。为了解决底层的NP完全获胜者决定问题，我们提出了一个简单但强大的异构三部图表示，并设计了基于图神经网络的模型。我们的模型与现成的优化工具相比，平均最优值偏差小于5％，并且与商业求解器的指数复杂度相比，展现出线性推理时间复杂度。贡献和结果展示了在本地市场中使用机器学习来高效分配能量灵活资源以及解决优化问题的潜力。

    This paper proposes a new combinatorial auction framework for local energy flexibility markets, which addresses the issue of prosumers' inability to bundle multiple flexibility time intervals. To solve the underlying NP-complete winner determination problems, we present a simple yet powerful heterogeneous tri-partite graph representation and design graph neural network-based models. Our models achieve an average optimal value deviation of less than 5\% from an off-the-shelf optimization tool and show linear inference time complexity compared to the exponential complexity of the commercial solver. Contributions and results demonstrate the potential of using machine learning to efficiently allocate energy flexibility resources in local markets and solving optimization problems in general.
    
[^32]: 电子商务套餐推荐中的高斯图与典型对比学习

    Gaussian Graph with Prototypical Contrastive Learning in E-Commerce Bundle Recommendation. (arXiv:2307.13468v1 [cs.IR])

    [http://arxiv.org/abs/2307.13468](http://arxiv.org/abs/2307.13468)

    本文提出了一种新颖的具有高斯图和典型对比学习（GPCL）框架，用于解决电子商务套餐推荐中的不确定性和采样偏差问题。

    

    套餐推荐旨在满足用户在电子商务平台上的偏好。既有的成功解决方案基于对比图学习范式，其中使用图神经网络（GNN）从用户级别和套餐级别的图视图中学习表示，并使用对比学习模块增强不同视图之间的合作关联。然而，由于高度稀疏性或多样性导致的缺乏有区别性信息，不确定性问题在实际套餐推荐场景中具有重要影响，这些解决方案忽视了这一问题。我们进一步指出，它们的逐实例对比学习无法区分语义上相似的负样本（即采样偏差问题），导致性能下降。在本文中，我们提出了一种新颖的具有高斯图和典型对比学习（GPCL）框架来克服这些挑战。

    Bundle recommendation aims to provide a bundle of items to satisfy the user preference on e-commerce platform. Existing successful solutions are based on the contrastive graph learning paradigm where graph neural networks (GNNs) are employed to learn representations from user-level and bundle-level graph views with a contrastive learning module to enhance the cooperative association between different views. Nevertheless, they ignore the uncertainty issue which has a significant impact in real bundle recommendation scenarios due to the lack of discriminative information caused by highly sparsity or diversity. We further suggest that their instancewise contrastive learning fails to distinguish the semantically similar negatives (i.e., sampling bias issue), resulting in performance degradation. In this paper, we propose a novel Gaussian Graph with Prototypical Contrastive Learning (GPCL) framework to overcome these challenges. In particular, GPCL embeds each user/bundle/item as a Gaussian
    
[^33]: 通过将基于过程的模型与机器学习相结合进行作物产量预测

    Integrating processed-based models and machine learning for crop yield prediction. (arXiv:2307.13466v1 [cs.LG])

    [http://arxiv.org/abs/2307.13466](http://arxiv.org/abs/2307.13466)

    本研究通过将基于过程的作物生长模型与机器学习相结合，提出了一种混合元模建方法用于马铃薯产量预测。该方法通过使用合成数据预训练卷积神经网络，并使用观测数据进行微调，在模拟应用和真实数据测试中均取得了竞争力的预测结果。

    

    作物产量预测通常涉及使用理论驱动的基于过程的作物生长模型，这些模型在校准本地环境方面往往较为困难，或者使用需要大量数据集的数据驱动的机器学习方法。在这项工作中，我们使用一种混合元模建方法研究了马铃薯产量预测。我们采用作物生长模型生成合成数据来（预）训练卷积神经网络，然后使用观测数据进行微调。在模拟应用中，我们的元模建方法比纯粹的数据驱动方法获得更好的预测结果。在来自田间试验（n=303）和商业田地（n=77）的真实数据测试中，元模建方法相对于作物生长模型具有竞争力的结果。然而，在后者中，这两种模型的表现都不如由领域专家设计的手动选择特征集和专门预处理的简单线性回归模型。

    Crop yield prediction typically involves the utilization of either theory-driven process-based crop growth models, which have proven to be difficult to calibrate for local conditions, or data-driven machine learning methods, which are known to require large datasets. In this work we investigate potato yield prediction using a hybrid meta-modeling approach. A crop growth model is employed to generate synthetic data for (pre)training a convolutional neural net, which is then fine-tuned with observational data. When applied in silico, our meta-modeling approach yields better predictions than a baseline comprising a purely data-driven approach. When tested on real-world data from field trials (n=303) and commercial fields (n=77), the meta-modeling approach yields competitive results with respect to the crop growth model. In the latter set, however, both models perform worse than a simple linear regression with a hand-picked feature set and dedicated preprocessing designed by domain experts
    
[^34]: 量子随机访问内存的基本因果界限

    Fundamental causal bounds of quantum random access memories. (arXiv:2307.13460v1 [quant-ph])

    [http://arxiv.org/abs/2307.13460](http://arxiv.org/abs/2307.13460)

    本研究通过采用相对论量子场论和量子多体系统中的Lieb-Robinson界限，批判性地探讨了基于因果性的快速量子存储器的内在界限。研究表明在混合量子声学系统中，QRAM可以容纳最多O(10^7)个逻辑比特的一维结构。

    

    量子设备应遵守量子物理原则。量子随机访问内存（QRAM）是许多重要量子算法（如线性代数、数据搜索和机器学习）的基本组件，通常被认为在给定N个量子比特时，可以以O(log N)的电路深度处理O(N)的数据量。然而，当处理大量量子比特的相互作用局部的量子材料时，这一主张似乎违反了相对论原理。在我们的研究中，我们批判性地探讨了基于因果性的快速量子存储器的内在界限，利用相对论量子场论和Lieb-Robinson界限在量子多体系统中。在本文中，我们考虑了一个在混合量子声学系统中高效的硬件设计的QRAM。假设时钟周期约为10^{-3}秒，格子间距约为1微米，我们展示了QRAM可以容纳最多O(10^7)个逻辑比特的一维结构。

    Quantum devices should operate in adherence to quantum physics principles. Quantum random access memory (QRAM), a fundamental component of many essential quantum algorithms for tasks such as linear algebra, data search, and machine learning, is often proposed to offer $\mathcal{O}(\log N)$ circuit depth for $\mathcal{O}(N)$ data size, given $N$ qubits. However, this claim appears to breach the principle of relativity when dealing with a large number of qubits in quantum materials interacting locally. In our study we critically explore the intrinsic bounds of rapid quantum memories based on causality, employing the relativistic quantum field theory and Lieb-Robinson bounds in quantum many-body systems. In this paper, we consider a hardware-efficient QRAM design in hybrid quantum acoustic systems. Assuming clock cycle times of approximately $10^{-3}$ seconds and a lattice spacing of about 1 micrometer, we show that QRAM can accommodate up to $\mathcal{O}(10^7)$ logical qubits in 1 dimens
    
[^35]: 一个行为变换器用于机器人与非静止人类之间的有效协作

    A behavioural transformer for effective collaboration between a robot and a non-stationary human. (arXiv:2307.13447v1 [cs.RO])

    [http://arxiv.org/abs/2307.13447](http://arxiv.org/abs/2307.13447)

    该论文提出了一个行为变换器(BeTrans)框架，该框架能够使机器人能够更好地预测人类的非静态行为，并通过使用顺序数据来适应新的非静态的人类代理。实验证明BeTrans在协作环境中效果显著，比现有技术更快地适应了非静态的模拟人类代理。

    

    人机协作的一个关键挑战是人类行为的非静止性，由于其行为的变化所产生的非静止性会改变环境的转换，从而阻碍人机协作。我们提出了一个有原则的元学习框架，探索机器人如何更好地预测人类行为，并因此解决非静止性问题。基于这个框架，我们开发了行为变换器(BeTrans)。BeTrans是一个条件变换器，能够使机器人代理快速适应具有非静态行为的新的人类代理，因为它在顺序数据上具有显著的性能。我们训练了BeTrans在模拟的具有不同系统偏差的人类代理中，在协作环境中进行了原始的可定制环境的实验，显示出BeTrans与模拟的人类代理有效协作，并比现有技术更快地适应了非静态的模拟人类代理。

    A key challenge in human-robot collaboration is the non-stationarity created by humans due to changes in their behaviour. This alters environmental transitions and hinders human-robot collaboration. We propose a principled meta-learning framework to explore how robots could better predict human behaviour, and thereby deal with issues of non-stationarity. On the basis of this framework, we developed Behaviour-Transform (BeTrans). BeTrans is a conditional transformer that enables a robot agent to adapt quickly to new human agents with non-stationary behaviours, due to its notable performance with sequential data. We trained BeTrans on simulated human agents with different systematic biases in collaborative settings. We used an original customisable environment to show that BeTrans effectively collaborates with simulated human agents and adapts faster to non-stationary simulated human agents than SOTA techniques.
    
[^36]: 基于单流时间序列分析的网络流量分类

    Network Traffic Classification based on Single Flow Time Series Analysis. (arXiv:2307.13434v1 [cs.LG])

    [http://arxiv.org/abs/2307.13434](http://arxiv.org/abs/2307.13434)

    本论文提出了一种基于单流时间序列分析的网络流量分类方法，通过提取69个通用特征，可在各种网络流量分类任务中取得与相关工作相当或更好的分类性能。

    

    使用IP流进行网络流量监测是处理分析加密网络通信的当前挑战的方法。然而，将包聚合为流记录自然会导致信息丢失；因此，本文提出了一种基于单流时间序列分析的流特征的新方法，即基于每个数据包的字节数和时间戳创建的时间序列。我们基于数据点的统计分析、时域分析、流时间跨度内的数据包分布、时间序列行为和频域分析提出了69个通用特征。我们使用15个公开可用的知名数据集展示了所提出的特征向量在各种网络流量分类任务中的适用性和通用性。我们的评估结果表明，该新特征向量在二分类和多类别分类任务上的分类性能与相关工作相当或更好。

    Network traffic monitoring using IP flows is used to handle the current challenge of analyzing encrypted network communication. Nevertheless, the packet aggregation into flow records naturally causes information loss; therefore, this paper proposes a novel flow extension for traffic features based on the time series analysis of the Single Flow Time series, i.e., a time series created by the number of bytes in each packet and its timestamp. We propose 69 universal features based on the statistical analysis of data points, time domain analysis, packet distribution within the flow timespan, time series behavior, and frequency domain analysis. We have demonstrated the usability and universality of the proposed feature vector for various network traffic classification tasks using 15 well-known publicly available datasets. Our evaluation shows that the novel feature vector achieves classification performance similar or better than related works on both binary and multiclass classification ta
    
[^37]: 在分布式随机组合极小化优化中实现线性加速

    Achieving Linear Speedup in Decentralized Stochastic Compositional Minimax Optimization. (arXiv:2307.13430v1 [cs.LG])

    [http://arxiv.org/abs/2307.13430](http://arxiv.org/abs/2307.13430)

    本论文提出了一种新颖的算法，可在分布式环境中实现线性加速，以解决组合极小化问题中内层函数的共识误差问题。

    

    近年来，随机组合极小化问题引起了广泛关注，因为它涵盖了许多新兴的机器学习模型。与此同时，由于分布式数据的出现，需要在分散设置下优化这种问题。然而，损失函数中的组合结构给设计高效的分散式优化算法带来了独特的挑战。我们的研究表明，由于对内层函数的共识误差较大，标准的传递策略无法在分散式组合极小化问题中实现线性加速。为解决这个问题，我们开发了一种新颖的带有动量的分散式随机组合梯度下降算法，以减小内层函数的共识误差。因此，我们的理论结果表明，它可以实现与工作者数量成线性加速。我们相信这种新颖的算法能够在分布式环境中为组合极小化问题提供线性加速。

    The stochastic compositional minimax problem has attracted a surge of attention in recent years since it covers many emerging machine learning models. Meanwhile, due to the emergence of distributed data, optimizing this kind of problem under the decentralized setting becomes badly needed. However, the compositional structure in the loss function brings unique challenges to designing efficient decentralized optimization algorithms. In particular, our study shows that the standard gossip communication strategy cannot achieve linear speedup for decentralized compositional minimax problems due to the large consensus error about the inner-level function. To address this issue, we developed a novel decentralized stochastic compositional gradient descent ascent with momentum algorithm to reduce the consensus error in the inner-level function. As such, our theoretical results demonstrate that it is able to achieve linear speedup with respect to the number of workers. We believe this novel algo
    
[^38]: 一种噪声抑制卷积神经网络的信号处理解释

    A signal processing interpretation of noise-reduction convolutional neural networks. (arXiv:2307.13425v1 [cs.CV])

    [http://arxiv.org/abs/2307.13425](http://arxiv.org/abs/2307.13425)

    该论文通过将信号处理的基本原理与深度学习领域相连接，以解释不同的噪声抑制卷积神经网络架构，并提供了设计新型CNN架构的重要指南。

    

    编码-解码卷积神经网络在数据驱动的噪声抑制中起着核心作用，并在许多深度学习算法中使用。然而，这些CNN架构的开发通常是以临时的方式进行的，对于重要的设计选择缺乏理论基础。到目前为止，有不同的相关工作致力于解释这些CNN的内部操作。然而，这些想法要么零散分布，要么可能需要相当的专业知识才能为更广泛的受众所理解。为了打开这个令人兴奋的领域，本文通过将信号处理的基本原理与深度学习领域相连接，以统一的理论框架解释了不同的ED CNN架构，并在这一自包含的材料中提供了设计强大高效新型CNN架构的重要指南。

    Encoding-decoding CNNs play a central role in data-driven noise reduction and can be found within numerous deep-learning algorithms. However, the development of these CNN architectures is often done in ad-hoc fashion and theoretical underpinnings for important design choices is generally lacking. Up to this moment there are different existing relevant works that strive to explain the internal operation of these CNNs. Still, these ideas are either scattered and/or may require significant expertise to be accessible for a bigger audience. In order to open up this exciting field, this article builds intuition on the theory of deep convolutional framelets and explains diverse ED CNN architectures in a unified theoretical framework. By connecting basic principles from signal processing to the field of deep learning, this self-contained material offers significant guidance for designing robust and efficient novel CNN architectures.
    
[^39]: 非侵入式自学习语音表示对听力障碍个体的清晰度预测

    Non Intrusive Intelligibility Predictor for Hearing Impaired Individuals using Self Supervised Speech Representations. (arXiv:2307.13423v1 [cs.SD])

    [http://arxiv.org/abs/2307.13423](http://arxiv.org/abs/2307.13423)

    本研究将自学习语音表示应用于预测听力障碍个体的可理解性，并发现其作为非侵入式预测模型的输入特征具有竞争性能，需要更多数据才能推广到未知系统和个体。

    

    自学习语音表示 (SSSRs) 已成功应用于多个语音处理任务，例如作为语音质量 (SQ) 预测的特征提取器，这对于评估和训练正常或有听力障碍的用户的语音增强系统至关重要。然而，为什么和如何将与质量相关的信息嵌入到这样的表示中仍然知之甚少。在这项工作中，非侵入式 SQ 评级预测技术被扩展到预测听力障碍用户的可理解性。发现自学习表示作为非侵入式预测模型的输入特征非常有用，其性能竞争力强于更复杂的系统。针对 Clarity Prediction Challenge 1 受试者和增强系统的性能进行了详细分析，结果表明可能需要更多数据才能推广到未知系统和（听力受损的）个体。

    Self-supervised speech representations (SSSRs) have been successfully applied to a number of speech-processing tasks, e.g. as feature extractor for speech quality (SQ) prediction, which is, in turn, relevant for assessment and training speech enhancement systems for users with normal or impaired hearing. However, exact knowledge of why and how quality-related information is encoded well in such representations remains poorly understood. In this work, techniques for non-intrusive prediction of SQ ratings are extended to the prediction of intelligibility for hearing-impaired users. It is found that self-supervised representations are useful as input features to non-intrusive prediction models, achieving competitive performance to more complex systems. A detailed analysis of the performance depending on Clarity Prediction Challenge 1 listeners and enhancement systems indicates that more data might be needed to allow generalisation to unknown systems and (hearing-impaired) individuals
    
[^40]: 关于注意力网络学习动态的研究

    On the learning Dynamics of Attention Networks. (arXiv:2307.13421v1 [cs.LG])

    [http://arxiv.org/abs/2307.13421](http://arxiv.org/abs/2307.13421)

    本研究分析了软注意力、硬注意力和潜变量边际似然（LVML）注意力三种注意力模型的学习动态，发现了它们在所选择的片段聚合方式上的显著差异，并解释了分类模型在梯度下降下的演化对最终结果的影响。

    

    注意力模型通常通过优化三个标准损失函数之一来学习，分别称为软注意力、硬注意力和潜变量边际似然（LVML）注意力。这三种范式都是为了达到相同的目标，即找到两个模型：一个“焦点”模型，用于“选择”输入中的正确“片段”，和一个“分类”模型，用于将选定的片段处理成目标标签。然而，它们在所选择的片段聚合方式上存在显著差异，导致了不同的动态和最终结果。我们观察到使用这些范式学习的模型具有独特的特征，并将其解释为在焦点模型固定时，分类模型在梯度下降下的演化所致。我们还在一个简单的设置中分析了这些范式，并推导出梯度流下参数轨迹的闭式表达式。在软注意力损失下，焦点模型在初始化阶段快速改善。

    Attention models are typically learned by optimizing one of three standard loss functions that are variously called -- soft attention, hard attention, and latent variable marginal likelihood (LVML) attention. All three paradigms are motivated by the same goal of finding two models -- a `focus' model that `selects' the right \textit{segment} of the input and a `classification' model that processes the selected segment into the target label. However, they differ significantly in the way the selected segments are aggregated, resulting in distinct dynamics and final results. We observe a unique signature of models learned using these paradigms and explain this as a consequence of the evolution of the classification model under gradient descent when the focus model is fixed. We also analyze these paradigms in a simple setting and derive closed-form expressions for the parameter trajectory under gradient flow. With the soft attention loss, the focus model improves quickly at initialization a
    
[^41]: 自主紧急制动系统的越界检测器的联合设计

    Co-Design of Out-of-Distribution Detectors for Autonomous Emergency Braking Systems. (arXiv:2307.13419v1 [cs.LG])

    [http://arxiv.org/abs/2307.13419](http://arxiv.org/abs/2307.13419)

    这项研究探讨了自主紧急制动系统中学习能力组件和越界检测器之间的联合设计，以解决训练分布之外的样本带来的错误决策问题，考虑了功能性能、非功能性能和系统安全的权衡。

    

    学习能力组件（LEC）对于自主驾驶车辆（AV）的决策至关重要，但是当面临训练分布之外的样本时，它们很可能做出错误的决策。越界检测器已被提出来检测这类样本，从而充当安全监控，然而，越界检测器和LEC都需要大量使用通常在AV中找到的嵌入式硬件。对于这两个组件，存在非功能和功能性能之间的权衡，两者都会影响车辆的安全性。例如，给越界检测器更长的响应时间可以提高其准确性，但会以LEC的性能为代价。我们考虑一个具有二进制输出的LEC，就像一个自主紧急制动系统（AEBS），并使用风险来模拟两个组件设计参数对彼此的功能性能和非功能性能以及对系统安全性的影响。

    Learning enabled components (LECs), while critical for decision making in autonomous vehicles (AVs), are likely to make incorrect decisions when presented with samples outside of their training distributions. Out-of-distribution (OOD) detectors have been proposed to detect such samples, thereby acting as a safety monitor, however, both OOD detectors and LECs require heavy utilization of embedded hardware typically found in AVs. For both components, there is a tradeoff between non-functional and functional performance, and both impact a vehicle's safety. For instance, giving an OOD detector a longer response time can increase its accuracy at the expense of the LEC. We consider an LEC with binary output like an autonomous emergency braking system (AEBS) and use risk, the combination of severity and occurrence of a failure, to model the effect of both components' design parameters on each other's functional and non-functional performance, as well as their impact on system safety. We formu
    
[^42]: 通过分层强化学习实现通信高效的URLLC服务编排

    Communication-Efficient Orchestrations for URLLC Service via Hierarchical Reinforcement Learning. (arXiv:2307.13415v1 [eess.SY])

    [http://arxiv.org/abs/2307.13415](http://arxiv.org/abs/2307.13415)

    本文提出了一个分层强化学习框架，通过不同控制循环时间尺度的多级策略，实现了高效的URLLC服务编排，减少了控制循环的延迟，信令和能量消耗。

    

    超可靠低延迟通信（URLLC）服务被视为实现对5G中具有严格可靠性和延迟要求的应用场景的关键。一种实现URLLC服务的方法是利用强化学习（RL）来高效分配无线资源。然而，使用传统的RL方法，决策变量（尽管部署在不同的网络层）通常在同一个控制循环中进行优化，导致控制循环的延迟存在显著的实际限制，以及过多的信令和能量消耗。在本文中，我们提出了一个多智能体分层强化学习（HRL）框架，可以实现具有不同控制循环时间尺度的多级策略。具有更快控制循环的智能体部署在靠近基站的位置，而具有较慢控制循环的智能体则部署在边缘或靠近核心网络的位置，为低级动作提供高级指导。在一个来自现有技术的用例中，借助我们的HRL框架，我们减少了控制循环的延迟，信令和能量消耗。

    Ultra-reliable low latency communications (URLLC) service is envisioned to enable use cases with strict reliability and latency requirements in 5G. One approach for enabling URLLC services is to leverage Reinforcement Learning (RL) to efficiently allocate wireless resources. However, with conventional RL methods, the decision variables (though being deployed at various network layers) are typically optimized in the same control loop, leading to significant practical limitations on the control loop's delay as well as excessive signaling and energy consumption. In this paper, we propose a multi-agent Hierarchical RL (HRL) framework that enables the implementation of multi-level policies with different control loop timescales. Agents with faster control loops are deployed closer to the base station, while the ones with slower control loops are at the edge or closer to the core network providing high-level guidelines for low-level actions. On a use case from the prior art, with our HRL fra
    
[^43]: 用即时生成权重的方法减轻CNN引擎中的内存瓶颈效应

    Mitigating Memory Wall Effects in CNN Engines with On-the-Fly Weights Generation. (arXiv:2307.13412v1 [cs.LG])

    [http://arxiv.org/abs/2307.13412](http://arxiv.org/abs/2307.13412)

    本文研究了用即时生成权重的方法减轻CNN引擎中的内存瓶颈效应，并提出了一种名为unzipFPGA的CNN推理系统。

    

    卷积神经网络（CNN）在广泛的人工智能任务中取得了前所未有的准确性，这导致它们在移动和嵌入式环境中得到广泛应用。为了实现高性能和低能耗的推理，研究人员在基于FPGA的CNN加速器设计方面投入了大量研究工作。然而，单个计算引擎常常在受内存限制的层上性能下降，并且由于某些层在引擎的固定配置中的次优映射而导致资源利用不足。本文研究了一类引入预卷积阶段在运行时解压缩权重的模型对CNN引擎设计的影响。我们将这些方法称为即时生成权重。本文提出了一种新颖的CNN推理系统unzipFPGA，以解决这些问题。

    The unprecedented accuracy of convolutional neural networks (CNNs) across a broad range of AI tasks has led to their widespread deployment in mobile and embedded settings. In a pursuit for high-performance and energy-efficient inference, significant research effort has been invested in the design of FPGA-based CNN accelerators. In this context, single computation engines constitute a popular approach to support diverse CNN modes without the overhead of fabric reconfiguration. Nevertheless, this flexibility often comes with significantly degraded performance on memory-bound layers and resource underutilisation due to the suboptimal mapping of certain layers on the engine's fixed configuration. In this work, we investigate the implications in terms of CNN engine design for a class of models that introduce a pre-convolution stage to decompress the weights at run time. We refer to these approaches as on-the-fly. This paper presents unzipFPGA, a novel CNN inference system that counteracts t
    
[^44]: 大数据和信息技术对弱势群体的双刃剑：开放银行的一个警示故事

    The Double-Edged Sword of Big Data and Information Technology for the Disadvantaged: A Cautionary Tale from Open Banking. (arXiv:2307.13408v1 [cs.LG])

    [http://arxiv.org/abs/2307.13408](http://arxiv.org/abs/2307.13408)

    本研究利用开放银行作为例子，分析了大数据和强大的信息技术（如机器学习）所带来的公平性隐患。通过研究金融脆弱性的维度，我们展示了细致交易数据的能力，并提醒对其应当谨慎使用以避免潜在的歧视问题。

    

    本研究分析和展示了看似中立的数据和强大的技术（如机器学习）在开放银行等领域对公平性的隐藏影响。开放银行在金融服务领域引发了一场革命，为客户获取、管理、保留和风险评估打开了新的机会。然而，交易数据的细致程度可能会带来潜在的危害，未被注意的敏感和禁止性特征的代理可能导致间接歧视。在这个背景下，我们通过公平解释的视角，研究了金融脆弱性（FV）的维度，这是COVID-19和通胀上升带来的全球关注点。具体而言，我们试图了解导致FV的行为因素，以及它如何影响处于风险群体中的弱势群体。使用来自英国一家金融科技借贷公司的独特数据集，我们展示了细粒度交易数据的能力，同时也对其提出了警告。

    This research article analyses and demonstrates the hidden implications for fairness of seemingly neutral data coupled with powerful technology, such as machine learning (ML), using Open Banking as an example. Open Banking has ignited a revolution in financial services, opening new opportunities for customer acquisition, management, retention, and risk assessment. However, the granularity of transaction data holds potential for harm where unnoticed proxies for sensitive and prohibited characteristics may lead to indirect discrimination. Against this backdrop, we investigate the dimensions of financial vulnerability (FV), a global concern resulting from COVID-19 and rising inflation. Specifically, we look to understand the behavioral elements leading up to FV and its impact on at-risk, disadvantaged groups through the lens of fair interpretation. Using a unique dataset from a UK FinTech lender, we demonstrate the power of fine-grained transaction data while simultaneously cautioning its
    
[^45]: 通过高斯混合分布潜空间的搜索生成反事实解释

    Counterfactual Explanation via Search in Gaussian Mixture Distributed Latent Space. (arXiv:2307.13390v1 [cs.LG])

    [http://arxiv.org/abs/2307.13390](http://arxiv.org/abs/2307.13390)

    本文介绍了一种通过在自编码器的潜空间中进行高斯混合分布搜索来生成反事实解释的方法。

    

    反事实解释（CEs）是用于解决算法补救中的两个问题的重要工具：1. 是什么关键因素导致了自动预测/决策？2. 如何改变这些因素以从用户角度获得更有利的结果？因此，通过提供易于理解的解释和易于实现的可行变化来引导用户与AI系统的交互对于可信赖的采用和长期接受AI系统是至关重要的。在文献中，已经提出了各种方法来生成CEs，并建议使用不同的质量度量来评估这些方法。然而，CEs的生成通常需要大量计算，并且生成的建议是不切实际的，因此不可操作。在本文中，我们介绍了一种新的方法，通过首先将自编码器的潜空间形成为高斯分布的混合，为预先训练的二分类器生成CEs。

    Counterfactual Explanations (CEs) are an important tool in Algorithmic Recourse for addressing two questions: 1. What are the crucial factors that led to an automated prediction/decision? 2. How can these factors be changed to achieve a more favorable outcome from a user's perspective? Thus, guiding the user's interaction with AI systems by proposing easy-to-understand explanations and easy-to-attain feasible changes is essential for the trustworthy adoption and long-term acceptance of AI systems. In the literature, various methods have been proposed to generate CEs, and different quality measures have been suggested to evaluate these methods. However, the generation of CEs is usually computationally expensive, and the resulting suggestions are unrealistic and thus non-actionable. In this paper, we introduce a new method to generate CEs for a pre-trained binary classifier by first shaping the latent space of an autoencoder to be a mixture of Gaussian distributions. CEs are then generat
    
[^46]: BotHawk：一个用于开源软件项目中检测机器人的方法

    BotHawk: An Approach for Bots Detection in Open Source Software Projects. (arXiv:2307.13386v1 [cs.SE])

    [http://arxiv.org/abs/2307.13386](http://arxiv.org/abs/2307.13386)

    本研究通过分析开源软件项目中机器人账号的行为，成功识别了四种类型的机器人账号，并创建了高效的BotHawk模型用于检测开源软件项目中的机器人。

    

    社交编码平台在软件开发中彻底改变了合作方式，导致使用软件机器人来简化运营。然而，开源软件（OSS）机器人的存在带来了伪装、垃圾邮件、偏见和安全风险等问题。在OSS项目中识别机器人账号和行为是一项具有挑战性的任务。本研究旨在研究开源软件项目中的机器人行为，并以尽可能高的准确性识别机器人账号。我们的团队收集了一组满足标准化标准的19,779个账号的数据集，以便未来对开源项目中的机器人进行研究。我们遵循严格的工作流程，确保我们收集的数据准确、具有普适性、可扩展并保持最新。通过分析开源软件项目中机器人账号在5个维度的17个特征上的行为，我们已经确定了四种类型的机器人账号。我们的团队创建了BotHawk，这是一个在开源软件项目中高效检测机器人的模型。

    Social coding platforms have revolutionized collaboration in software development, leading to using software bots for streamlining operations. However, The presence of open-source software (OSS) bots gives rise to problems including impersonation, spamming, bias, and security risks. Identifying bot accounts and behavior is a challenging task in the OSS project. This research aims to investigate bots' behavior in open-source software projects and identify bot accounts with maximum possible accuracy. Our team gathered a dataset of 19,779 accounts that meet standardized criteria to enable future research on bots in open-source projects. We follow a rigorous workflow to ensure that the data we collect is accurate, generalizable, scalable, and up-to-date. We've identified four types of bot accounts in open-source software projects by analyzing their behavior across 17 features in 5 dimensions. Our team created BotHawk, a highly effective model for detecting bots in open-source software proj
    
[^47]: Scaff-PD:高效率通信、公平及鲁棒的分布式学习算法

    Scaff-PD: Communication Efficient Fair and Robust Federated Learning. (arXiv:2307.13381v1 [cs.LG])

    [http://arxiv.org/abs/2307.13381](http://arxiv.org/abs/2307.13381)

    Scaff-PD是一个高效通信、公平及鲁棒的分布式学习算法。它通过优化一系列针对异构客户端的分布鲁棒目标来提高公平性，利用特殊结构和加速的原始-对偶算法，在通信效率和收敛速度方面取得显著的提升。在多个基准数据集上的评估结果显示，Scaff-PD在提高公平性和鲁棒性方面有效，并同时保持竞争性的准确性。这使得Scaff-PD成为资源受限和异构环境下分布式学习的一种有前景的方法。

    

    我们提出了一种名为Scaff-PD的快速和高效通信的分布式学习算法。我们的方法通过优化一系列针对异构客户端的分布鲁棒目标来改善公平性。我们利用这些目标的特殊结构，并设计了一个加速的原始-对偶（APD）算法，该算法使用修正偏差的局部步骤（如Scaffold）以在通信效率和收敛速度方面取得显著的提升。我们在多个基准数据集上评估了Scaff-PD，并展示了其在提高公平性和鲁棒性方面的有效性，同时保持竞争性的准确性。我们的结果表明，Scaff-PD是在资源受限和异构环境中进行分布式学习的一种有前景的方法。

    We present Scaff-PD, a fast and communication-efficient algorithm for distributionally robust federated learning. Our approach improves fairness by optimizing a family of distributionally robust objectives tailored to heterogeneous clients. We leverage the special structure of these objectives, and design an accelerated primal dual (APD) algorithm which uses bias corrected local steps (as in Scaffold) to achieve significant gains in communication efficiency and convergence speed. We evaluate Scaff-PD on several benchmark datasets and demonstrate its effectiveness in improving fairness and robustness while maintaining competitive accuracy. Our results suggest that Scaff-PD is a promising approach for federated learning in resource-constrained and heterogeneous settings.
    
[^48]: 子模块强化学习

    Submodular Reinforcement Learning. (arXiv:2307.13372v1 [cs.LG])

    [http://arxiv.org/abs/2307.13372](http://arxiv.org/abs/2307.13372)

    子模块强化学习(SubRL)是一种用于优化非可加奖励的范式，通过子模块集合函数来建模递减回报。这篇论文提出了SubRL的简单策略梯度算法SubPO，可以用于处理这种类型的奖励。

    

    在强化学习中，状态的奖励通常被认为是可加的，并且根据马尔可夫假设，它们与之前访问的状态$\textit{独立}$。在许多重要应用中，如覆盖控制、实验设计和信息路径规划，奖励自然具有递减回报，即其价值随之前访问过的相似状态的增加而减小。为了解决这个问题，我们提出了$\textit{子模块强化学习}$ (SubRL) ，这一范式旨在通过子模块集合函数来建模递减回报，从而优化更一般的非可加奖励（历史相关）。然而，不幸的是，即使在表格设置中，我们证明了得到的优化问题很难近似解决。另一方面，受经典子模块优化中贪婪算法的成功启发，我们提出了SubPO，一种用于SubRL的简单基于策略梯度的算法，通过贪婪地最大化边际来处理非可加奖励。

    In reinforcement learning (RL), rewards of states are typically considered additive, and following the Markov assumption, they are $\textit{independent}$ of states visited previously. In many important applications, such as coverage control, experiment design and informative path planning, rewards naturally have diminishing returns, i.e., their value decreases in light of similar states visited previously. To tackle this, we propose $\textit{submodular RL}$ (SubRL), a paradigm which seeks to optimize more general, non-additive (and history-dependent) rewards modelled via submodular set functions which capture diminishing returns. Unfortunately, in general, even in tabular settings, we show that the resulting optimization problem is hard to approximate. On the other hand, motivated by the success of greedy algorithms in classical submodular optimization, we propose SubPO, a simple policy gradient-based algorithm for SubRL that handles non-additive rewards by greedily maximizing marginal
    
[^49]: 学习适应性水平集估计的贝叶斯优化感兴趣区域

    Learning Regions of Interest for Bayesian Optimization with Adaptive Level-Set Estimation. (arXiv:2307.13371v1 [cs.LG])

    [http://arxiv.org/abs/2307.13371](http://arxiv.org/abs/2307.13371)

    提出了一种名为BALLET的框架，用于在高维和非平稳场景下的贝叶斯优化。它使用两个概率模型，一个粗糙的高斯过程用于识别感兴趣的区域，一个局部高斯过程用于优化该区域。BALLET能够有效地缩小搜索空间，并且比标准的无感兴趣区域过滤的贝叶斯优化具有更紧的遗憾界限。

    

    我们研究了在高维和非平稳场景下的贝叶斯优化（BO）。现有的算法通常需要大量的超参数调整，限制了它们的实际有效性。我们提出了一个名为BALLET的框架，该框架以非参数概率模型（如高斯过程）的超级级集的高置信度感兴趣区域（ROI）为自适应过滤器。我们的方法易于调整，并且能够专注于可以通过现有的BO方法解决的优化空间的局部区域。关键思想是使用两个概率模型：一个粗糙的高斯过程用于识别ROI，一个局部高斯过程用于ROI内的优化。我们理论上证明了BALLET可以高效地缩小搜索空间，并且比标准的无ROI过滤的BO具有更紧的遗憾界限。我们在合成和真实世界的优化任务中经验性地证明了BALLET的有效性。

    We study Bayesian optimization (BO) in high-dimensional and non-stationary scenarios. Existing algorithms for such scenarios typically require extensive hyperparameter tuning, which limits their practical effectiveness. We propose a framework, called BALLET, which adaptively filters for a high-confidence region of interest (ROI) as a superlevel-set of a nonparametric probabilistic model such as a Gaussian process (GP). Our approach is easy to tune, and is able to focus on local region of the optimization space that can be tackled by existing BO methods. The key idea is to use two probabilistic models: a coarse GP to identify the ROI, and a localized GP for optimization within the ROI. We show theoretically that BALLET can efficiently shrink the search space, and can exhibit a tighter regret bound than standard BO without ROI filtering. We demonstrate empirically the effectiveness of BALLET on both synthetic and real-world optimization tasks.
    
[^50]: 通过阻尼Sinkhorn迭代实现双熵Wasserstein重心的计算保证

    Computational Guarantees for Doubly Entropic Wasserstein Barycenters via Damped Sinkhorn Iterations. (arXiv:2307.13370v1 [math.OC])

    [http://arxiv.org/abs/2307.13370](http://arxiv.org/abs/2307.13370)

    本文提出了一种用于计算双规则化Wasserstein重心的算法，并通过阻尼Sinkhorn迭代和精确的最大化/最小化步骤保证了收敛性。此算法的非精确变体使用近似的蒙特卡罗采样实现，在自由支撑/网格自由设置中提供了第一个非渐近收敛保证。

    

    我们研究了双规则化Wasserstein重心的计算，这是一种最近引入的由内部和外部规则化强度控制的熵重心族。先前的研究表明，各种规则化参数选择统一了几个熵惩罚重心的概念，同时揭示了新的概念，包括偏差重心的特殊情况。在本文中，我们提出并分析了一种计算双规则化Wasserstein重心的算法。我们的过程基于阻尼Sinkhorn迭代，然后是精确的最大化/最小化步骤，对任何规则化参数的选择都保证收敛。我们的算法的非精确变体，可以使用近似的蒙特卡罗采样来实现，在自由支撑/网格自由设置中为近似Wasserstein重心之间的离散点云提供了第一个非渐近收敛保证。

    We study the computation of doubly regularized Wasserstein barycenters, a recently introduced family of entropic barycenters governed by inner and outer regularization strengths. Previous research has demonstrated that various regularization parameter choices unify several notions of entropy-penalized barycenters while also revealing new ones, including a special case of debiased barycenters. In this paper, we propose and analyze an algorithm for computing doubly regularized Wasserstein barycenters. Our procedure builds on damped Sinkhorn iterations followed by exact maximization/minimization steps and guarantees convergence for any choice of regularization parameters. An inexact variant of our algorithm, implementable using approximate Monte Carlo sampling, offers the first non-asymptotic convergence guarantees for approximating Wasserstein barycenters between discrete point clouds in the free-support/grid-free setting.
    
[^51]: 高维分布式梯度下降算法在任意数量拜占庭攻击者下的研究

    High Dimensional Distributed Gradient Descent with Arbitrary Number of Byzantine Attackers. (arXiv:2307.13352v1 [cs.LG])

    [http://arxiv.org/abs/2307.13352](http://arxiv.org/abs/2307.13352)

    本文提出了一种适用于高维问题、在任意数量拜占庭攻击者下的新方法，核心是一种直接的高维半验证均值估计方法，具有极小极值统计率。

    

    近年来，具有拜占庭故障的强鲁棒分布式学习引起了广泛关注。然而，现有方法大多受到维度诅咒的限制，随着现代机器学习模型复杂性的增加，这个问题变得越来越严重。在本文中，我们设计了一种适用于高维问题、在任意数量拜占庭攻击者下的新方法。我们的设计核心是一种直接的高维半验证均值估计方法。我们的想法是首先识别一个子空间，通过工作机上传的梯度向量估计与该子空间垂直的均值分量，而通过辅助数据集估计该子空间内的均值分量。然后，我们将我们的新方法用作分布式学习问题的聚合器。我们的理论分析表明，新方法具有极小极值统计率。特别地，对维度的依赖性得到了显著改善。

    Robust distributed learning with Byzantine failures has attracted extensive research interests in recent years. However, most of existing methods suffer from curse of dimensionality, which is increasingly serious with the growing complexity of modern machine learning models. In this paper, we design a new method that is suitable for high dimensional problems, under arbitrary number of Byzantine attackers. The core of our design is a direct high dimensional semi-verified mean estimation method. Our idea is to identify a subspace first. The components of mean value perpendicular to this subspace can be estimated via gradient vectors uploaded from worker machines, while the components within this subspace are estimated using auxiliary dataset. We then use our new method as the aggregator of distributed learning problems. Our theoretical analysis shows that the new method has minimax optimal statistical rates. In particular, the dependence on dimensionality is significantly improved compar
    
[^52]: 基于决策树抽样的特征重要性测量

    Feature Importance Measurement based on Decision Tree Sampling. (arXiv:2307.13333v1 [cs.LG])

    [http://arxiv.org/abs/2307.13333](http://arxiv.org/abs/2307.13333)

    DT-Sampler是一种基于SAT的方法，用于测量基于树的模型中的特征重要性，提供了更高的可解释性和稳定性。

    

    随机森林在预测任务中非常有效，但是树的随机生成使得特征重要性分析的可解释性受限。为解决这个问题，我们提出了DT-Sampler，一种基于SAT的方法用于在基于树的模型中测量特征重要性。我们的方法比随机森林的参数更少，并为现实世界问题中的分析提供更高的可解释性和稳定性。DT-Sampler的实现可以在https://github.com/tsudalab/DT-sampler找到。

    Random forest is effective for prediction tasks but the randomness of tree generation hinders interpretability in feature importance analysis. To address this, we proposed DT-Sampler, a SAT-based method for measuring feature importance in tree-based model. Our method has fewer parameters than random forest and provides higher interpretability and stability for the analysis in real-world problems. An implementation of DT-Sampler is available at https://github.com/tsudalab/DT-sampler.
    
[^53]: 在错误指定的离策略值函数估计中的最佳逼近因子

    The Optimal Approximation Factors in Misspecified Off-Policy Value Function Estimation. (arXiv:2307.13332v1 [cs.LG])

    [http://arxiv.org/abs/2307.13332](http://arxiv.org/abs/2307.13332)

    本文研究了在线性离策略值函数估计中的逼近因子，并在多种设置下建立了最优的渐近逼近因子，这些因子决定了离策略评估的困难程度。

    

    已经知道，在强化学习中的理论保证在函数逼近的错误指定中会出现乘法放大因子。然而，这些\emph{逼近因子}的性质，特别是在给定的学习问题中的最佳形式，仍然不为人所了解。在本文中，我们研究了这个问题在线性离策略值函数估计中的广泛设置中的逼近因子，其中仍有许多开放问题。我们研究了在多种设置下的逼近因子，例如加权$L_2$范数（其中加权是离线状态分布），$L_\infty$范数，状态别名的存在与否以及对状态空间的全面与部分覆盖。对于所有这些设置，我们建立了最优的渐近逼近因子（至多常数）。特别地，我们的界限确定了$L_2(\mu)$范数的两个依赖于实例的因子和$L_\infty$范数的一个因子，它们被证明决定了离策略评估的困难程度。

    Theoretical guarantees in reinforcement learning (RL) are known to suffer multiplicative blow-up factors with respect to the misspecification error of function approximation. Yet, the nature of such \emph{approximation factors} -especially their optimal form in a given learning problem -- is poorly understood. In this paper we study this question in linear off-policy value function estimation, where many open questions remain. We study the approximation factor in a broad spectrum of settings, such as with the weighted $L_2$-norm (where the weighting is the offline state distribution), the $L_\infty$ norm, the presence vs. absence of state aliasing, and full vs. partial coverage of the state space. We establish the optimal asymptotic approximation factors (up to constants) for all of these settings. In particular, our bounds identify two instance-dependent factors for the $L_2(\mu)$ norm and only one for the $L_\infty$ norm, which are shown to dictate the hardness of off-policy evalua
    
[^54]: QuIP：具有保证的大型语言模型的2比特量化

    QuIP: 2-Bit Quantization of Large Language Models With Guarantees. (arXiv:2307.13304v1 [cs.LG])

    [http://arxiv.org/abs/2307.13304](http://arxiv.org/abs/2307.13304)

    本文提出了一种新的基于无关处理的大型语言模型（LLMs）参数量化方法QuIP，通过使权重和Hessian矩阵与坐标轴不对齐，实现了准确的量化结果。经过经验实验，我们发现我们的方法改善了现有的量化算法，并且首次在仅使用两比特的情况下获得了可行的LLM量化结果。

    

    本研究探讨了大型语言模型（LLMs）中的训练后参数量化。我们介绍了一种新的基于无关处理（QuIP）的量化方法，该方法基于以下见解：量化从不相关的权重和 Hessian 矩阵中收益，即通过准确地将它们舍入为与坐标轴不对齐的方向，使得获取重要的量化结果。QuIP 包含两个步骤：（1）最小化二次近似目标的自适应舍入过程；（2）通过与随机正交矩阵相乘来确保权重和 Hessian 无关的高效预处理和后处理。我们通过第一次针对 LLM 规模的量化算法进行了理论分析，并且证明我们的理论也适用于现有方法 OPTQ。经验证实，我们的无关预处理改善了现有的多个量化算法，并首次实现了仅使用每个权重2比特的大型语言模型量化方法。

    This work studies post-training parameter quantization in large language models (LLMs). We introduce quantization with incoherence processing (QuIP), a new method based on the insight that quantization benefits from incoherent weight and Hessian matrices, i.e., from the weights and the directions in which it is important to round them accurately being unaligned with the coordinate axes. QuIP consists of two steps: (1) an adaptive rounding procedure minimizing a quadratic proxy objective; (2) efficient pre- and post-processing that ensures weight and Hessian incoherence via multiplication by random orthogonal matrices. We complement QuIP with the first theoretical analysis for an LLM-scale quantization algorithm, and show that our theory also applies to an existing method, OPTQ. Empirically, we find that our incoherence preprocessing improves several existing quantization algorithms and yields the first LLM quantization methods that produce viable results using only two bits per weight.
    
[^55]: 在函数空间中修改训练方向以降低泛化误差

    Modify Training Directions in Function Space to Reduce Generalization Error. (arXiv:2307.13290v1 [stat.ML])

    [http://arxiv.org/abs/2307.13290](http://arxiv.org/abs/2307.13290)

    本文提出了在函数空间中修改训练方向的方法，通过在神经网络函数空间中进行特征分解和统计理论的理论分析，我们证明了这种方法可以降低总的泛化误差。

    

    我们提出了在神经网络函数空间中基于神经切换核和Fisher信息矩阵的特征分解的修改自然梯度下降法的理论分析。我们首先给出了在高斯分布和无限宽度极限的假设下，通过理论方法从特征分解和统计理论中显式推导出该修改自然梯度所学习的函数的表达式。因此，我们通过将总的泛化误差分解为函数空间中不同特征空间的误差，提出了一个平衡训练集误差和训练集与真实数据之间分布差异的准则。通过这种方法，我们建立了在函数空间中修改神经网络的训练方向会导致总的泛化误差的减少。

    We propose theoretical analyses of a modified natural gradient descent method in the neural network function space based on the eigendecompositions of neural tangent kernel and Fisher information matrix. We firstly present analytical expression for the function learned by this modified natural gradient under the assumptions of Gaussian distribution and infinite width limit. Thus, we explicitly derive the generalization error of the learned neural network function using theoretical methods from eigendecomposition and statistics theory. By decomposing of the total generalization error attributed to different eigenspace of the kernel in function space, we propose a criterion for balancing the errors stemming from training set and the distribution discrepancy between the training set and the true data. Through this approach, we establish that modifying the training direction of the neural network in function space leads to a reduction in the total generalization error. Furthermore, We demo
    
[^56]: 基于曲率的变压器用于分子属性预测

    Curvature-based Transformer for Molecular Property Prediction. (arXiv:2307.13275v1 [cs.LG])

    [http://arxiv.org/abs/2307.13275](http://arxiv.org/abs/2307.13275)

    该研究提出了一种基于曲率的变压器方法，通过引入离散化的 Ricci 曲率，改进了图变压器神经网络模型在分子图数据上提取结构信息的能力。实验证明其有效性，并有扩展到其他模型的潜力。

    

    分子性质的预测是基于人工智能的药物设计领域中最重要且具有挑战性的任务之一。在当前主流的方法中，用于训练DNN模型的最常用特征表示基于SMILES和分子图，尽管这些方法简洁高效，但也限制了对空间信息的捕捉能力。在本研究中，我们提出了基于曲率的变压器，通过引入 Ricci 曲率离散化，改进了图变压器神经网络模型在分子图数据上提取结构信息的能力。为了将曲率嵌入模型中，在注意力得分计算期间，我们将图的曲率信息作为位置编码添加到节点特征中。这种方法可以在不改变原始网络结构的情况下，将曲率信息引入图数据，并且有潜力扩展到其他模型。我们进行了实验证明了这种方法的有效性。

    The prediction of molecular properties is one of the most important and challenging tasks in the field of artificial intelligence-based drug design. Among the current mainstream methods, the most commonly used feature representation for training DNN models is based on SMILES and molecular graphs, although these methods are concise and effective, they also limit the ability to capture spatial information. In this work, we propose Curvature-based Transformer to improve the ability of Graph Transformer neural network models to extract structural information on molecular graph data by introducing Discretization of Ricci Curvature. To embed the curvature in the model, we add the curvature information of the graph as positional Encoding to the node features during the attention-score calculation. This method can introduce curvature information from graph data without changing the original network architecture, and it has the potential to be extended to other models. We performed experiments 
    
[^57]: 无偏重量最大化

    Unbiased Weight Maximization. (arXiv:2307.13270v1 [cs.LG])

    [http://arxiv.org/abs/2307.13270](http://arxiv.org/abs/2307.13270)

    这项研究提出了一种无偏重量最大化的方法，通过用出站权重的范数替换单元的奖励信号，实现了更加高效的结构性信用分配。

    

    训练人工神经网络的一种生物学合理的方法是将每个单元视为随机强化学习代理，从而将网络视为代理团队。因此，所有单元都可以通过REINFORCE进行学习，这是一种局部学习规则，通过全局奖励信号进行调节，更加符合生物观察到的突触可塑性形式。然而，由于缺乏有效的结构性信用分配，这种学习方法通常速度较慢，且随着网络规模的增大而扩展性较差，因为单个奖励信号被广播给所有单元而不考虑个体贡献。提出的解决方案，重量最大化，用出站权重的范数替换单元的奖励信号，从而允许每个隐藏单元最大化出站权重的范数，而不是全局奖励信号。在本研究报告中，我们分析了重量最大化的理论属性并提出了一种变体，无偏重量最大化。

    A biologically plausible method for training an Artificial Neural Network (ANN) involves treating each unit as a stochastic Reinforcement Learning (RL) agent, thereby considering the network as a team of agents. Consequently, all units can learn via REINFORCE, a local learning rule modulated by a global reward signal, which aligns more closely with biologically observed forms of synaptic plasticity. Nevertheless, this learning method is often slow and scales poorly with network size due to inefficient structural credit assignment, since a single reward signal is broadcast to all units without considering individual contributions. Weight Maximization, a proposed solution, replaces a unit's reward signal with the norm of its outgoing weight, thereby allowing each hidden unit to maximize the norm of the outgoing weight instead of the global reward signal. In this research report, we analyze the theoretical properties of Weight Maximization and propose a variant, Unbiased Weight Maximizati
    
[^58]: 基于双分解的分布式优化的联邦K均值聚类

    Federated K-Means Clustering via Dual Decomposition-based Distributed Optimization. (arXiv:2307.13267v1 [math.OC])

    [http://arxiv.org/abs/2307.13267](http://arxiv.org/abs/2307.13267)

    本论文介绍了如何使用双分解方法进行分布式训练K均值聚类问题，以提高隐私保护或计算效率。论文通过对分布式和联邦机器学习进行概述，并给出了基于规划方法的K均值聚类训练的公式化描述。

    

    在机器学习中，分布式优化的使用可以通过保护隐私或提高计算效率来进行动机。一方面，训练数据可能存储在多个设备上。在每个节点只能访问其保密数据的网络中训练全局模型需要使用分布式算法。即使数据不是机密的，由于带宽限制，共享数据可能是禁止的。另一方面，不断增加的可用数据量导致了大规模的机器学习问题。通过将训练过程分割成多个节点，可以显著提高其效率。本文旨在演示如何使用双分解来进行分布式训练K均值聚类问题。在对分布式和联邦机器学习进行概述后，给出了基于混合整数二次约束规划的K均值聚类训练的公式化描述。

    The use of distributed optimization in machine learning can be motivated either by the resulting preservation of privacy or the increase in computational efficiency. On the one hand, training data might be stored across multiple devices. Training a global model within a network where each node only has access to its confidential data requires the use of distributed algorithms. Even if the data is not confidential, sharing it might be prohibitive due to bandwidth limitations. On the other hand, the ever-increasing amount of available data leads to large-scale machine learning problems. By splitting the training process across multiple nodes its efficiency can be significantly increased. This paper aims to demonstrate how dual decomposition can be applied for distributed training of $ K $-means clustering problems. After an overview of distributed and federated machine learning, the mixed-integer quadratically constrained programming-based formulation of the $ K $-means clustering traini
    
[^59]: 仅使用正标签的资源受限物联网环境中的联邦分割学习

    Federated Split Learning with Only Positive Labels for resource-constrained IoT environment. (arXiv:2307.13266v1 [cs.LG])

    [http://arxiv.org/abs/2307.13266](http://arxiv.org/abs/2307.13266)

    在资源受限的物联网环境中，我们提出了带有正标签的分割学习（SFPL）方法，通过对数据进行随机洗牌来改善多类别分类深度学习模型在联邦分割学习中的效果。

    

    分布式协作机器学习（DCML）是物联网领域中训练深度学习模型的一种有前景的方法，因为数据分布在多个设备上。这种方法的一个主要优点是通过消除原始数据的集中聚合来改善数据隐私，同时也为具有低计算能力的物联网设备提供动力。在DCML框架中的各种技术中，称为splitfed学习（SFL）的联邦分割学习是在设备具有有限计算能力时进行高效训练和测试的最合适的方法。然而，当资源受限的物联网设备只有正标记数据时，SFL中的多类别分类深度学习模型无法收敛或提供次优结果。为了克服这些挑战，我们提出了带有正标签的splitfed学习（SFPL）。SFPL在将客户端接收到的破碎数据提供给服务器之前，对其应用随机洗牌功能。

    Distributed collaborative machine learning (DCML) is a promising method in the Internet of Things (IoT) domain for training deep learning models, as data is distributed across multiple devices. A key advantage of this approach is that it improves data privacy by removing the necessity for the centralized aggregation of raw data but also empowers IoT devices with low computational power. Among various techniques in a DCML framework, federated split learning, known as splitfed learning (SFL), is the most suitable for efficient training and testing when devices have limited computational capabilities. Nevertheless, when resource-constrained IoT devices have only positive labeled data, multiclass classification deep learning models in SFL fail to converge or provide suboptimal results. To overcome these challenges, we propose splitfed learning with positive labels (SFPL). SFPL applies a random shuffling function to the smashed data received from clients before supplying it to the server fo
    
[^60]: 结构化信用分配与协调探索

    Structural Credit Assignment with Coordinated Exploration. (arXiv:2307.13256v1 [cs.LG])

    [http://arxiv.org/abs/2307.13256](http://arxiv.org/abs/2307.13256)

    本文提出一种结构化信用分配与协调探索的方法，该方法将每个单元视为强化学习代理，并通过全局奖励信号调节局部学习规则。通过提高结构化信用分配的效率来改进人工神经网络的训练。

    

    一种合理的训练人工神经网络(ANN)的方法是将每个单元视为一个随机强化学习(RL)代理，从而将网络视为代理团队。因此，所有单元都可以通过REINFORCE学习，这是一种通过全局奖励信号调节的局部学习规则，更接近生物观察到的突触可塑性形式。然而，这种学习方法往往较慢，无法很好地适应网络的规模。这种效率低下主要是由两个因素造成的，阻碍了有效的结构化信用分配：(i)所有单元独立探索网络，(ii)使用单一奖励来评估所有单元的行动。因此，旨在改善结构化信用分配的方法通常可分为两类。第一类包括允许单元之间进行协调探索的算法，例如MAP传播。第二类涵盖了计算结构性信用分配的算法。

    A biologically plausible method for training an Artificial Neural Network (ANN) involves treating each unit as a stochastic Reinforcement Learning (RL) agent, thereby considering the network as a team of agents. Consequently, all units can learn via REINFORCE, a local learning rule modulated by a global reward signal, which aligns more closely with biologically observed forms of synaptic plasticity. However, this learning method tends to be slow and does not scale well with the size of the network. This inefficiency arises from two factors impeding effective structural credit assignment: (i) all units independently explore the network, and (ii) a single reward is used to evaluate the actions of all units. Accordingly, methods aimed at improving structural credit assignment can generally be classified into two categories. The first category includes algorithms that enable coordinated exploration among units, such as MAP propagation. The second category encompasses algorithms that comput
    
[^61]: RoSAS:具有抗污染连续监督的深度半监督异常检测

    RoSAS: Deep Semi-Supervised Anomaly Detection with Contamination-Resilient Continuous Supervision. (arXiv:2307.13239v1 [cs.LG])

    [http://arxiv.org/abs/2307.13239](http://arxiv.org/abs/2307.13239)

    本论文提出了一种具有抗污染连续监督的深度半监督异常检测方法，通过质量插值方法创造了连续异常度标签的新数据样本，解决了半监督异常检测方法中的异常污染和离散监督信息利用不充分的问题。

    

    半监督异常检测方法利用一些异常样本，与无监督模型相比，显著提高了性能。然而，它们仍然存在两个限制：1) 未标记的异常（即异常污染）可能在将所有未标记数据用作内点进行模型训练时误导学习过程; 2) 只利用离散的监督信息（如二进制或顺序数据标签），这导致异常分数的子优学习，实质上采用连续分布。因此，本文提出了一种新颖的半监督异常检测方法，设计了"抗污染连续监督信号"。具体而言，我们提出了一种质量插值方法来扩散标记异常的异常程度，从而创建带有连续异常度标签的新数据样本。同时，通过组合具有不同程度异常程度的数据，可以覆盖受污染的区域。

    Semi-supervised anomaly detection methods leverage a few anomaly examples to yield drastically improved performance compared to unsupervised models. However, they still suffer from two limitations: 1) unlabeled anomalies (i.e., anomaly contamination) may mislead the learning process when all the unlabeled data are employed as inliers for model training; 2) only discrete supervision information (such as binary or ordinal data labels) is exploited, which leads to suboptimal learning of anomaly scores that essentially take on a continuous distribution. Therefore, this paper proposes a novel semi-supervised anomaly detection method, which devises \textit{contamination-resilient continuous supervisory signals}. Specifically, we propose a mass interpolation method to diffuse the abnormality of labeled anomalies, thereby creating new data samples labeled with continuous abnormal degrees. Meanwhile, the contaminated area can be covered by new data samples generated via combinations of data wit
    
[^62]: 声音感知的查询增强变形器用于音频-视觉分割

    Audio-aware Query-enhanced Transformer for Audio-Visual Segmentation. (arXiv:2307.13236v1 [cs.SD])

    [http://arxiv.org/abs/2307.13236](http://arxiv.org/abs/2307.13236)

    提出了一种声音感知的查询增强的变形器 (AuTR) 方法来解决音频-视觉分割任务中感受野小和特征不充分的问题，通过引入多模态变形器架构以及声音感知的查询增强的变形器解码器，实现了深度融合和聚合音频-视觉特征，并在多声音和开放性场景中展示出更好的泛化能力。

    

    音频-视觉分割（AVS）任务的目标是使用音频线索在视频帧中对发声对象进行分割。然而，当前基于融合的方法由于卷积的小感受野和音频-视觉特征不充分的融合而存在性能限制。为了解决这些问题，我们提出了一种新颖的声音感知的查询增强的变形器（AuTR）来应对这个任务。与现有方法不同，我们的方法引入了一种多模态变形器架构，能够深度融合和聚合音频-视觉特征。此外，我们设计了一种声音感知的查询增强的变形器解码器，明确地帮助模型在基于音频信号的定位发声对象的分割时聚焦，同时忽略寂静但显著的对象。实验结果表明，我们的方法优于先前的方法，并在多声音和开放性场景中展示出更好的泛化能力。

    The goal of the audio-visual segmentation (AVS) task is to segment the sounding objects in the video frames using audio cues. However, current fusion-based methods have the performance limitations due to the small receptive field of convolution and inadequate fusion of audio-visual features. To overcome these issues, we propose a novel \textbf{Au}dio-aware query-enhanced \textbf{TR}ansformer (AuTR) to tackle the task. Unlike existing methods, our approach introduces a multimodal transformer architecture that enables deep fusion and aggregation of audio-visual features. Furthermore, we devise an audio-aware query-enhanced transformer decoder that explicitly helps the model focus on the segmentation of the pinpointed sounding objects based on audio signals, while disregarding silent yet salient objects. Experimental results show that our method outperforms previous methods and demonstrates better generalization ability in multi-sound and open-set scenarios.
    
[^63]: Spectral-DP: 通过频谱扰动和滤波实现差分隐私深度学习

    Spectral-DP: Differentially Private Deep Learning through Spectral Perturbation and Filtering. (arXiv:2307.13231v1 [cs.LG])

    [http://arxiv.org/abs/2307.13231](http://arxiv.org/abs/2307.13231)

    Spectral-DP是一种新的差分隐私学习方法，通过将频域中的梯度扰动与频谱滤波相结合，实现更低噪声比例的隐私保证，从而提高深度学习的效用。

    

    差分隐私在深度学习算法中被广泛接受作为隐私度量，实现差分隐私依赖于一种称为差分隐私随机梯度下降（DP-SGD）的噪声训练方法。DP-SGD需要在密集神经网络中给每个梯度直接添加噪声，但这种隐私是以显著的效用损失为代价的。在这项工作中，我们提出了Spectral-DP，一种新的差分隐私学习方法，它将频域中的梯度扰动与频谱滤波相结合，以更低的噪声比例实现所需的隐私保证，从而提高效用。我们基于Spectral-DP开发了基于差分隐私的深度学习方法，适用于包含卷积层和全连接层的体系结构。特别地，对于全连接层，我们将基于块循环的空间重构与Spectral-DP相结合，以实现更好的效用。通过综合实验，我们研究并提供了实施的指南。

    Differential privacy is a widely accepted measure of privacy in the context of deep learning algorithms, and achieving it relies on a noisy training approach known as differentially private stochastic gradient descent (DP-SGD). DP-SGD requires direct noise addition to every gradient in a dense neural network, the privacy is achieved at a significant utility cost. In this work, we present Spectral-DP, a new differentially private learning approach which combines gradient perturbation in the spectral domain with spectral filtering to achieve a desired privacy guarantee with a lower noise scale and thus better utility. We develop differentially private deep learning methods based on Spectral-DP for architectures that contain both convolution and fully connected layers. In particular, for fully connected layers, we combine a block-circulant based spatial restructuring with Spectral-DP to achieve better utility. Through comprehensive experiments, we study and provide guidelines to implement
    
[^64]: 数据清洗流程的入门介绍

    A Primer on the Data Cleaning Pipeline. (arXiv:2307.13219v1 [cs.DB])

    [http://arxiv.org/abs/2307.13219](http://arxiv.org/abs/2307.13219)

    该论文介绍了数据清洗流程的概念和方法，旨在帮助分析师在清洁数据上进行下游任务、预测分析或统计分析。

    

    过去十年来，结构化和非结构化数据库的可用性大幅增长，其中包括电子健康数据、社交媒体数据、专利数据和实时更新的调查数据等。随着这种扩展，关于数据集成或者说合并多个数据源的统计和方法学问题也在不断增长。具体而言，"数据清洗流程"的科学包含四个阶段，使分析师能够在“清洁数据”上执行下游任务、预测分析或统计分析。本文回顾了这一新兴领域，介绍了技术术语和常用方法。

    The availability of both structured and unstructured databases, such as electronic health data, social media data, patent data, and surveys that are often updated in real time, among others, has grown rapidly over the past decade. With this expansion, the statistical and methodological questions around data integration, or rather merging multiple data sources, has also grown. Specifically, the science of the ``data cleaning pipeline'' contains four stages that allow an analyst to perform downstream tasks, predictive analyses, or statistical analyses on ``cleaned data.'' This article provides a review of this emerging field, introducing technical terminology and commonly used methods.
    
[^65]: FedMEKT: 基于蒸馏的多模态联邦学习中的嵌入知识传输

    FedMEKT: Distillation-based Embedding Knowledge Transfer for Multimodal Federated Learning. (arXiv:2307.13214v1 [cs.LG])

    [http://arxiv.org/abs/2307.13214](http://arxiv.org/abs/2307.13214)

    FedMEKT是一种基于蒸馏的多模态联邦学习框架，通过半监督学习方法利用不同模态的表示，实现了服务器和客户端之间的联合知识传输。

    

    联邦学习（FL）使多个客户端能够在不共享私有数据的情况下协同训练一个广义全局模型，从而实现分散式的机器学习范式。现有的大部分工作只是针对单模态数据提出了典型的FL系统，因此限制了它对于利用宝贵的多模态数据进行未来个性化应用的潜力。此外，大多数FL方法仍然依赖于客户端的标记数据，由于用户无法进行自注释，这在实际应用中是有限的。鉴于这些限制，我们提出了一种新型的多模态FL框架，采用半监督学习方法利用不同模态的表示。将这个概念引入一个系统中，我们开发了一种基于蒸馏的多模态嵌入知识传输机制，称为FedMEKT，它允许服务器和客户端交换从小型多模态数据集中提取的学习模型的联合知识。

    Federated learning (FL) enables a decentralized machine learning paradigm for multiple clients to collaboratively train a generalized global model without sharing their private data. Most existing works simply propose typical FL systems for single-modal data, thus limiting its potential on exploiting valuable multimodal data for future personalized applications. Furthermore, the majority of FL approaches still rely on the labeled data at the client side, which is limited in real-world applications due to the inability of self-annotation from users. In light of these limitations, we propose a novel multimodal FL framework that employs a semi-supervised learning approach to leverage the representations from different modalities. Bringing this concept into a system, we develop a distillation-based multimodal embedding knowledge transfer mechanism, namely FedMEKT, which allows the server and clients to exchange the joint knowledge of their learning models extracted from a small multimodal 
    
[^66]: 使用图论和采样理论实现图神经网络的可迁移性

    Transferability of Graph Neural Networks using Graphon and Sampling Theories. (arXiv:2307.13206v1 [cs.LG])

    [http://arxiv.org/abs/2307.13206](http://arxiv.org/abs/2307.13206)

    本文提出了一种新的方法来实现图神经网络的可迁移性，通过使用图谱和采样理论，我们证明了一个显式的两层图谱神经网络能够在保持准确性的同时以较少的网络权重数逼近带限信号，并且在收敛到图谱的序列中实现了在足够大的图之间的可迁移性。

    

    图神经网络（GNNs）已成为在各个领域处理基于图的信息的强大工具。GNN的一个理想特性是可迁移性，即训练好的网络可以在不重新训练的情况下交换来自不同图的信息并保持准确性。最近一种捕捉GNN可迁移性的方法是使用图谱，它是对大型稠密图的极限的对称可测函数。在这项工作中，我们通过提出一个显式的两层图谱神经网络（WNN）架构，对图谱应用于GNN做出了贡献。我们证明了它能够以指定误差容限在最少的网络权重数下逼近带限信号。然后，我们利用这一结果，在一个收敛到图谱的序列中，建立了一个明确的两层GNN在所有足够大的图之间的可迁移性。我们的工作解决了确定性加权图和简单随机图之间的可迁移性问题。

    Graph neural networks (GNNs) have become powerful tools for processing graph-based information in various domains. A desirable property of GNNs is transferability, where a trained network can swap in information from a different graph without retraining and retain its accuracy. A recent method of capturing transferability of GNNs is through the use of graphons, which are symmetric, measurable functions representing the limit of large dense graphs. In this work, we contribute to the application of graphons to GNNs by presenting an explicit two-layer graphon neural network (WNN) architecture. We prove its ability to approximate bandlimited signals within a specified error tolerance using a minimal number of network weights. We then leverage this result, to establish the transferability of an explicit two-layer GNN over all sufficiently large graphs in a sequence converging to a graphon. Our work addresses transferability between both deterministic weighted graphs and simple random graphs
    
[^67]: 使用YOLO在肾脏H&E和PAS图像中进行肾小球检测的研究

    An Investigation into Glomeruli Detection in Kidney H&E and PAS Images using YOLO. (arXiv:2307.13199v1 [eess.IV])

    [http://arxiv.org/abs/2307.13199](http://arxiv.org/abs/2307.13199)

    本研究提出了一种使用YOLO-v4进行肾小球检测的方法，该方法通过训练全切片图像，可以在肾脏H&E和PAS图像中实现自动的组织结构检测和分割。

    

    背景：通过分析组织模式和细胞形态，对数字病理学图像进行分析是为了得出诊断结论的必要步骤。然而，手动评估费时、昂贵并且容易出现观察者之间和观察者内部的变异性。目标：为了帮助病理学家使用计算机化解决方案，必须提出自动组织结构检测和分割方法。此外，为组织病理学图像生成像素级对象注释也是费时费力的。因此，具有边界框标签的检测模型可能是一个可行的解决方案。设计：本文研究了一种用于显微图像的实时目标检测器YOLO-v4（You Only Look Once）。YOLO使用单一神经网络来预测感兴趣目标的多个边界框和类别概率。YOLO可以通过使用全切片图像进行训练来提升检测性能。本文中使用YOLO-v4来进行人类肾脏图像中的肾小球检测。多个实验表明YOLO-v4在肾小球检测方面具有较好的表现。

    Context: Analyzing digital pathology images is necessary to draw diagnostic conclusions by investigating tissue patterns and cellular morphology. However, manual evaluation can be time-consuming, expensive, and prone to inter- and intra-observer variability. Objective: To assist pathologists using computerized solutions, automated tissue structure detection and segmentation must be proposed. Furthermore, generating pixel-level object annotations for histopathology images is expensive and time-consuming. As a result, detection models with bounding box labels may be a feasible solution. Design: This paper studies. YOLO-v4 (You-Only-Look-Once), a real-time object detector for microscopic images. YOLO uses a single neural network to predict several bounding boxes and class probabilities for objects of interest. YOLO can enhance detection performance by training on whole slide images. YOLO-v4 has been used in this paper. for glomeruli detection in human kidney images. Multiple experiments h
    
[^68]: RL中的反事实解释策略

    Counterfactual Explanation Policies in RL. (arXiv:2307.13192v1 [cs.AI])

    [http://arxiv.org/abs/2307.13192](http://arxiv.org/abs/2307.13192)

    本文介绍了一个名为COUNTERPOL的框架，用于通过对策略进行最小改变来分析RL策略，并达到所需的结果。这项工作在RL中通过使用反事实解释与监督学习相结合的方法进行了实证分析，并与广泛使用的基于信任区域的策略优化方法进行了理论联系。

    

    随着强化学习（RL）代理在使用奖励偏好的多样化决策问题中的应用越来越广泛，确保这些框架学习到的策略能够解释变得很重要，即将观察映射到可能行动的概率分布的策略如何以对比的方式系统地理解，即，使其性能达到所需水平的策略最小改变是什么。在这项工作中，我们提出了COUNTERPOL，这是第一个使用反事实解释来分析RL策略的框架，即通过对策略进行最小改变，达到所需的结果。我们通过将反事实融入RL中的监督学习，并使用期望收益调控目标结果，建立了Counterpol与广泛使用的基于信任区域的策略优化方法之间的理论联系。大量的实证分析表明，该方法具有明显的效果。

    As Reinforcement Learning (RL) agents are increasingly employed in diverse decision-making problems using reward preferences, it becomes important to ensure that policies learned by these frameworks in mapping observations to a probability distribution of the possible actions are explainable. However, there is little to no work in the systematic understanding of these complex policies in a contrastive manner, i.e., what minimal changes to the policy would improve/worsen its performance to a desired level. In this work, we present COUNTERPOL, the first framework to analyze RL policies using counterfactual explanations in the form of minimal changes to the policy that lead to the desired outcome. We do so by incorporating counterfactuals in supervised learning in RL with the target outcome regulated using desired return. We establish a theoretical connection between Counterpol and widely used trust region-based policy optimization methods in RL. Extensive empirical analysis shows the eff
    
[^69]: 使用EEG数据和表示学习的神经记忆解码

    Neural Memory Decoding with EEG Data and Representation Learning. (arXiv:2307.13181v1 [cs.LG])

    [http://arxiv.org/abs/2307.13181](http://arxiv.org/abs/2307.13181)

    本研究提出一种使用EEG数据和表示学习进行神经记忆解码的方法，能够实现从EEG数据中识别出被召回的概念，并在信息检索问题中应用该方法。

    

    我们描述了一种从EEG数据中解码记忆的方法。使用这种方法，可以从EEG波形中识别出被召回的概念，平均准确率达到78.4％（机会4％）。该方法采用了深度表示学习与有监督对比损失来将脑活动的EEG记录映射到一个低维空间。由于使用了表示学习，即使这些概念在训练数据集中没有出现，也可以识别出来。然而，每个概念都必须存在相应的参考EEG数据。我们还展示了该方法在信息检索问题上的应用。在神经信息检索中，当用户回忆文档内容时捕获EEG数据，并生成预测文档的链接列表。

    We describe a method for the neural decoding of memory from EEG data. Using this method, a concept being recalled can be identified from an EEG trace with an average top-1 accuracy of about 78.4% (chance 4%). The method employs deep representation learning with supervised contrastive loss to map an EEG recording of brain activity to a low-dimensional space. Because representation learning is used, concepts can be identified even if they do not appear in the training data set. However, reference EEG data must exist for each such concept. We also show an application of the method to the problem of information retrieval. In neural information retrieval, EEG data is captured while a user recalls the contents of a document, and a list of links to predicted documents is produced.
    
[^70]: 评估自动生成的行人和自行车事故替代品的可靠性

    Evaluating the reliability of automatically generated pedestrian and bicycle crash surrogates. (arXiv:2307.13178v1 [cs.LG])

    [http://arxiv.org/abs/2307.13178](http://arxiv.org/abs/2307.13178)

    本研究旨在评估自动生成的行人和自行车事故替代品的可靠性，以提高脆弱道路用户的安全性能。

    

    脆弱的道路使用者，如行人和自行车手，更容易卷入与机动车辆的事故中，并且与脆弱道路使用者有关的事故更容易导致严重伤害或死亡。信号灯路口是脆弱道路使用者的主要安全隐患，因其复杂和动态的特性，强调了理解这些道路使用者如何与机动车辆互动，并采用基于证据的对策来提高安全性能的必要性。由于涉及脆弱道路使用者的事故相对较少，很难理解其中的潜在因素。一种替代方法是识别和使用脆弱道路使用者与机动车辆之间的冲突作为安全性能的替代品。利用基于视频的系统自动检测这些冲突是发展智能基础设施以提高脆弱道路使用者安全性的关键步骤。宾夕法尼亚交通部进行了一项研究，使用基于视频的事件监控系统来评估脆弱道路使用者和机动车辆的冲突情况。

    Vulnerable road users (VRUs), such as pedestrians and bicyclists, are at a higher risk of being involved in crashes with motor vehicles, and crashes involving VRUs also are more likely to result in severe injuries or fatalities. Signalized intersections are a major safety concern for VRUs due to their complex and dynamic nature, highlighting the need to understand how these road users interact with motor vehicles and deploy evidence-based countermeasures to improve safety performance. Crashes involving VRUs are relatively infrequent, making it difficult to understand the underlying contributing factors. An alternative is to identify and use conflicts between VRUs and motorized vehicles as a surrogate for safety performance. Automatically detecting these conflicts using a video-based systems is a crucial step in developing smart infrastructure to enhance VRU safety. The Pennsylvania Department of Transportation conducted a study using video-based event monitoring system to assess VRU an
    
[^71]: 多无人机速度控制兼顾避障和交接感知的小区关联：带有动作分支的深度强化学习研究

    Multi-UAV Speed Control with Collision Avoidance and Handover-aware Cell Association: DRL with Action Branching. (arXiv:2307.13158v1 [cs.LG])

    [http://arxiv.org/abs/2307.13158](http://arxiv.org/abs/2307.13158)

    本文提出了一种深度强化学习方法，用于优化多无人机在三维空中高速公路上的小区关联决策和移动速度，以提升交通和通信性能。仿真结果显示，性能提高了18.32%。

    

    本文提出了一种深度强化学习方法，用于优化三维空中高速公路上多无人机的小区关联决策和移动速度，以提升交通和通信性能，包括避障、连接性和交接效果。问题被建模为一个马尔可夫决策过程（MDP），其中无人机的状态由速度和通信数据速率定义。我们提出了一种神经结构，其中包含一个共享的决策模块和多个网络分支，每个分支专门处理二维交通-通信空间中的特定动作维度。这种设计有效地处理了多维动作空间，使得各个动作维度可以独立决策。我们介绍了两个模型，分支型对抗性Q网络（BDQ）和分支型对抗性双重深度Q网络（Dueling DDQN），来证明这种方法。仿真结果显示，与现有基准相比，性能提高了18.32%。

    This paper presents a deep reinforcement learning solution for optimizing multi-UAV cell-association decisions and their moving velocity on a 3D aerial highway. The objective is to enhance transportation and communication performance, including collision avoidance, connectivity, and handovers. The problem is formulated as a Markov decision process (MDP) with UAVs' states defined by velocities and communication data rates. We propose a neural architecture with a shared decision module and multiple network branches, each dedicated to a specific action dimension in a 2D transportation-communication space. This design efficiently handles the multi-dimensional action space, allowing independence for individual action dimensions. We introduce two models, Branching Dueling Q-Network (BDQ) and Branching Dueling Double Deep Q-Network (Dueling DDQN), to demonstrate the approach. Simulation results show a significant improvement of 18.32% compared to existing benchmarks.
    
[^72]: 通过神经多项式方法实现可解释的弹性塑性模型的发现

    Discovering interpretable elastoplasticity models via the neural polynomial method enabled symbolic regressions. (arXiv:2307.13149v1 [cs.CE])

    [http://arxiv.org/abs/2307.13149](http://arxiv.org/abs/2307.13149)

    本文介绍了一种通过神经多项式方法实现可解释的弹性塑性模型的机器学习方法，该方法通过分为两个步骤，先通过监督学习得到一组特征映射，再通过符号回归将其转化为数学公式，从而克服了传统神经网络模型的缺乏可解释性的问题。

    

    传统神经网络弹性塑性模型通常被认为缺乏可解释性。本文介绍了一种两步机器学习方法，可以返回专家可解释的数学模型。具体而言，我们引入了一个替代模型，其中屈服曲面是通过监督学习得到的一组单变量特征映射来表示的。然后，通过符号回归将这组单变量神经网络映射函数重新解释为数学形式。这种分而治之的方法具有几个重要优势。首先，它使我们能够克服符号回归算法的扩展问题。从实际角度来看，它提高了用不同编程语言编写的偏微分方程求解器的学习模型的可移植性。最后，它使我们能够对材料的属性（如凸性和对称性）有一个具体的理解。

    Conventional neural network elastoplasticity models are often perceived as lacking interpretability. This paper introduces a two-step machine-learning approach that returns mathematical models interpretable by human experts. In particular, we introduce a surrogate model where yield surfaces are expressed in terms of a set of single-variable feature mappings obtained from supervised learning. A postprocessing step is then used to re-interpret the set of single-variable neural network mapping functions into mathematical form through symbolic regression. This divide-and-conquer approach provides several important advantages. First, it enables us to overcome the scaling issue of symbolic regression algorithms. From a practical perspective, it enhances the portability of learned models for partial differential equation solvers written in different programming languages. Finally, it enables us to have a concrete understanding of the attributes of the materials, such as convexity and symmetri
    
[^73]: 将路径相关的NJ-ODE扩展到有噪声的观测和相关观测框架

    Extending Path-Dependent NJ-ODEs to Noisy Observations and a Dependent Observation Framework. (arXiv:2307.13147v1 [stat.ML])

    [http://arxiv.org/abs/2307.13147](http://arxiv.org/abs/2307.13147)

    该论文研究了将路径相关的NJ-ODE方法扩展到具有噪声观测和相关观测框架的问题。研究提出了两种扩展方法，并提供了理论保证和实证示例。

    

    路径相关的神经跳跃ODE (PD-NJ-ODE) 是一种用于预测具有不规则和不完整观测的连续时间随机过程的模型。具体而言，该方法通过学习给定不规则采样的不完整过去观测的最优预测。迄今为止，假设过程本身和坐标分别观测时间是独立的，并且假设观测是无噪声的。在这项工作中，我们讨论了两种扩展来解除这些限制，并提供了理论保证以及它们的实证示例。

    The Path-Dependent Neural Jump ODE (PD-NJ-ODE) is a model for predicting continuous-time stochastic processes with irregular and incomplete observations. In particular, the method learns optimal forecasts given irregularly sampled time series of incomplete past observations. So far the process itself and the coordinate-wise observation times were assumed to be independent and observations were assumed to be noiseless. In this work we discuss two extensions to lift these restrictions and provide theoretical guarantees as well as empirical examples for them.
    
[^74]: 在目标识别基准测试上取得的进展是否改善了现实世界的泛化能力？

    Does Progress On Object Recognition Benchmarks Improve Real-World Generalization?. (arXiv:2307.13136v1 [cs.CV])

    [http://arxiv.org/abs/2307.13136](http://arxiv.org/abs/2307.13136)

    通过对涵盖全球各地家庭物体的数据集进行研究，我们发现目前在目标识别基准测试上取得的进展并没有改善在真实世界中的泛化能力。

    

    十多年来，研究人员一直用基于ImageNet的泛化基准测试（如ImageNet-A、-C和-R）来衡量目标识别的进展。最近基于大量数据训练的基础模型取得了一些进展，但在实际应用中仍然表现不稳定。这表明标准基准测试可能不足以衡量现实世界的泛化能力，因为它们往往集中在预定义或合成的变化上。因此，我们提议使用涵盖全球各地家庭物体的两个数据集来研究地理范围内的泛化能力作为更现实的衡量标准。我们对近100个视觉模型进行了广泛的实证评估，包括最新的基础模型。首先，我们发现了标准基准测试和真实世界地理变化之间的进展差距：在ImageNet上的进展在标准泛化基准测试上产生的进展比真实世界分布变化高出2.5倍。

    For more than a decade, researchers have measured progress in object recognition on ImageNet-based generalization benchmarks such as ImageNet-A, -C, and -R. Recent advances in foundation models, trained on orders of magnitude more data, have begun to saturate these standard benchmarks, but remain brittle in practice. This suggests standard benchmarks, which tend to focus on predefined or synthetic changes, may not be sufficient for measuring real world generalization. Consequently, we propose studying generalization across geography as a more realistic measure of progress using two datasets of objects from households across the globe. We conduct an extensive empirical evaluation of progress across nearly 100 vision models up to most recent foundation models. We first identify a progress gap between standard benchmarks and real-world, geographical shifts: progress on ImageNet results in up to 2.5x more progress on standard generalization benchmarks than real-world distribution shifts. S
    
[^75]: simPLE:一种在模拟中学习的视触觉方法，用于准确地抓取、定位、重新抓取和放置物体。

    simPLE: a visuotactile method learned in simulation to precisely pick, localize, regrasp, and place objects. (arXiv:2307.13133v1 [cs.RO])

    [http://arxiv.org/abs/2307.13133](http://arxiv.org/abs/2307.13133)

    本文介绍了一种称为simPLE的方法，通过在模拟中学习来实现精确的抓取、定位、重新抓取和放置物体。simPLE包括任务感知抓取、视觉触觉感知和重新抓取规划三个主要组件，能够精确地处理多个任务，而无需先前的经验。

    

    现有的机器人系统在通用性和精度之间存在明显的紧张关系。用于机器人操作的部署解决方案往往属于一个机器人解决单一任务的范式，缺乏精确的泛化能力，即在保持精度的同时解决多个任务的能力。本文探讨了精确和通用的抓取和放置解决方案。在精确的抓取和放置中，即套件化中，机器人将一个无组织的物体排列转化为有组织的排列，这可以促进进一步的操作。我们提出了simPLE（模拟抓取定位和放置）作为精确抓取和放置的解决方案。simPLE学会了准确地抓取、重新抓取和放置物体，只需物体CAD模型而无需先前的经验。我们开发了三个主要组件：任务感知抓取，视觉触觉感知和重新抓取规划。任务感知抓取计算稳定、可观测且有利于放置的抓取的适应度。视觉触觉感知模型 r

    Existing robotic systems have a clear tension between generality and precision. Deployed solutions for robotic manipulation tend to fall into the paradigm of one robot solving a single task, lacking precise generalization, i.e., the ability to solve many tasks without compromising on precision. This paper explores solutions for precise and general pick-and-place. In precise pick-and-place, i.e. kitting, the robot transforms an unstructured arrangement of objects into an organized arrangement, which can facilitate further manipulation. We propose simPLE (simulation to Pick Localize and PLacE) as a solution to precise pick-and-place. simPLE learns to pick, regrasp and place objects precisely, given only the object CAD model and no prior experience. We develop three main components: task-aware grasping, visuotactile perception, and regrasp planning. Task-aware grasping computes affordances of grasps that are stable, observable, and favorable to placing. The visuotactile perception model r
    
[^76]: 一个差分隐私加权经验风险最小化算法及其在结果加权学习中的应用

    A Differentially Private Weighted Empirical Risk Minimization Procedure and its Application to Outcome Weighted Learning. (arXiv:2307.13127v1 [stat.ML])

    [http://arxiv.org/abs/2307.13127](http://arxiv.org/abs/2307.13127)

    本文提出了一种差分隐私加权经验风险最小化算法，可以在使用敏感数据的情况下保护隐私。这是第一个在权重ERM中应用差分隐私的算法，并且在一定的条件下提供了严格的DP保证。

    

    在经验风险最小化(ERM)框架中，使用包含个人信息的数据来构建预测模型是常见的做法。尽管这些模型在预测上可以非常准确，但使用敏感数据得到的结果可能容易受到隐私攻击。差分隐私(DP)是一种有吸引力的框架，可以通过提供数学上可证明的隐私损失界限来解决这些数据隐私问题。先前的工作主要集中在将DP应用于无权重的ERM中。我们考虑到了权重ERM(wERM)的重要推广。在wERM中，可以为每个个体的目标函数贡献分配不同的权重。在这个背景下，我们提出了第一个有差分隐私保障的wERM算法，并在一定的正则条件下提供了严格的理论证明。将现有的DP-ERM程序扩展到wERM为结果加权学习铺平了道路。

    It is commonplace to use data containing personal information to build predictive models in the framework of empirical risk minimization (ERM). While these models can be highly accurate in prediction, results obtained from these models with the use of sensitive data may be susceptible to privacy attacks. Differential privacy (DP) is an appealing framework for addressing such data privacy issues by providing mathematically provable bounds on the privacy loss incurred when releasing information from sensitive data. Previous work has primarily concentrated on applying DP to unweighted ERM. We consider an important generalization to weighted ERM (wERM). In wERM, each individual's contribution to the objective function can be assigned varying weights. In this context, we propose the first differentially private wERM algorithm, backed by a rigorous theoretical proof of its DP guarantees under mild regularity conditions. Extending the existing DP-ERM procedures to wERM paves a path to derivin
    
[^77]: 频率-严重性建模的符合性预测

    Conformal prediction for frequency-severity modeling. (arXiv:2307.13124v1 [stat.ME])

    [http://arxiv.org/abs/2307.13124](http://arxiv.org/abs/2307.13124)

    这个论文提出了一个非参数的模型无关框架，用于建立保险理赔的预测区间，并具有有限样本的统计保证，扩展了split conformal prediction技术到两阶段频率-严重性建模领域，并通过使用随机森林作为严重性模型，利用了袋外机制消除了校准集的需要，并实现了具有自适应宽度的预测区间的生成。

    

    我们提出了一个非参数的模型无关框架，用于建立保险理赔的预测区间，并具有有限样本的统计保证，将分割符合性预测技术扩展到两阶段频率-严重性建模领域。通过模拟和真实数据集展示了该框架的有效性。当基础严重性模型是随机森林时，我们扩展了两阶段分割符合性预测过程，展示了如何利用袋外机制消除校准集的需要，并实现具有自适应宽度的预测区间的生成。

    We present a nonparametric model-agnostic framework for building prediction intervals of insurance claims, with finite sample statistical guarantees, extending the technique of split conformal prediction to the domain of two-stage frequency-severity modeling. The effectiveness of the framework is showcased with simulated and real datasets. When the underlying severity model is a random forest, we extend the two-stage split conformal prediction procedure, showing how the out-of-bag mechanism can be leveraged to eliminate the need for a calibration set and to enable the production of prediction intervals with adaptive width.
    
[^78]: Pathway:一种快速灵活的统一流数据处理框架，用于分析和机器学习应用

    Pathway: a fast and flexible unified stream data processing framework for analytical and Machine Learning applications. (arXiv:2307.13116v1 [cs.LG])

    [http://arxiv.org/abs/2307.13116](http://arxiv.org/abs/2307.13116)

    Pathway是一个快速灵活的统一流数据处理框架，用于分析和机器学习应用。它能够在有界和无界的数据流上运行，通过Table API和分布式增量数据流驱动，在批处理和流处理场景中表现出优异的能力。

    

    我们提出了Pathway，一个新的统一数据处理框架，可以在有界和无界数据流上运行工作负载。该框架的创建最初是为了解决在分析和处理物理经济数据时面临的挑战，包括由物联网和企业系统生成的数据流。这些都需要快速反应，并需要应用先进的计算范 paradigms（机器学习驱动的分析，上下文分析和复杂事件处理的其他元素）。Pathway配备了针对Python和Python/SQL工作流程量身定制的Table API，并由Rust中的分布式增量数据流驱动。我们描述了该系统，并呈现了基准测试结果，表明它在批处理和流处理场景中的能力，可以超过最先进的行业框架。我们还讨论了由Pathway处理的流处理用例，这些用例无法轻松解决以状态为基础的方案。

    We present Pathway, a new unified data processing framework that can run workloads on both bounded and unbounded data streams. The framework was created with the original motivation of resolving challenges faced when analyzing and processing data from the physical economy, including streams of data generated by IoT and enterprise systems. These required rapid reaction while calling for the application of advanced computation paradigms (machinelearning-powered analytics, contextual analysis, and other elements of complex event processing). Pathway is equipped with a Table API tailored for Python and Python/SQL workflows, and is powered by a distributed incremental dataflow in Rust. We describe the system and present benchmarking results which demonstrate its capabilities in both batch and streaming contexts, where it is able to surpass state-of-the-art industry frameworks in both scenarios. We also discuss streaming use cases handled by Pathway which cannot be easily resolved with state
    
[^79]: 一个可解释的几何加权图注意力网络用于识别与步态障碍相关的功能网络

    An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment. (arXiv:2307.13108v1 [cs.LG])

    [http://arxiv.org/abs/2307.13108](http://arxiv.org/abs/2307.13108)

    本文提出了一个可解释的几何加权图注意力网络（xGW-GAT），用于识别与步态障碍相关的功能网络，以推动帕金森病治疗的发展。

    

    帕金森病的一个显著症状是姿势反射的逐渐丧失，最终导致步态困难和平衡问题。识别与步态障碍相关的脑功能紊乱对于更好地理解帕金森病的运动进展以及推动更有效和个性化的治疗的发展可能是至关重要的。本文提出了一个可解释的、几何的、加权图注意力神经网络（xGW-GAT），用于识别预测帕金森病患者步态困难进展的功能网络。xGW-GAT在MDS统一帕金森病评分标准（MDS-UPDRS）上预测多类别的步态障碍。我们的计算和数据高效的模型将功能连接组表示为黎曼流形上的对称正定（SPD）矩阵，以明确编码整个连接组的成对交互作用，根据此我们可以学习出一个注意力掩码，以产生个体和群体级别的可解释性。

    One of the hallmark symptoms of Parkinson's Disease (PD) is the progressive loss of postural reflexes, which eventually leads to gait difficulties and balance problems. Identifying disruptions in brain function associated with gait impairment could be crucial in better understanding PD motor progression, thus advancing the development of more effective and personalized therapeutics. In this work, we present an explainable, geometric, weighted-graph attention neural network (xGW-GAT) to identify functional networks predictive of the progression of gait difficulties in individuals with PD. xGW-GAT predicts the multi-class gait impairment on the MDS Unified PD Rating Scale (MDS-UPDRS). Our computational- and data-efficient model represents functional connectomes as symmetric positive definite (SPD) matrices on a Riemannian manifold to explicitly encode pairwise interactions of entire connectomes, based on which we learn an attention mask yielding individual- and group-level explainability
    
[^80]: 对比示例驱动的控制

    Contrastive Example-Based Control. (arXiv:2307.13101v1 [cs.LG])

    [http://arxiv.org/abs/2307.13101](http://arxiv.org/abs/2307.13101)

    本文提出了一种基于示例的控制方法，它通过学习隐式模型的多步转移来解决控制问题，而不是学习奖励函数。这种方法避免了复杂的正规化和时差更新，并取得了良好的结果。

    

    尽管许多现实世界中可能受益于强化学习的问题很少符合MDP模型，与环境交互往往是昂贵的，并且指定奖励函数也很具有挑战性。鉴于这些挑战，先前的工作已经开发出一种基于数据驱动的方法，从转移动态的样本和高回报状态的示例中进行学习。这些方法通常从高回报状态学习奖励函数，使用该奖励函数标记转移，并通过离线强化学习算法应用于这些转移。尽管这些方法在许多任务上可以取得良好的结果，但它们可能很复杂，通常需要正规化和时差更新。在本文中，我们提出了一种离线、基于示例的控制方法，它学习了一个隐式模型的多步转移，而不是学习奖励函数。我们证明了这个隐式模型可以表示基于示例的控制问题的Q值。

    While many real-world problems that might benefit from reinforcement learning, these problems rarely fit into the MDP mold: interacting with the environment is often expensive and specifying reward functions is challenging. Motivated by these challenges, prior work has developed data-driven approaches that learn entirely from samples from the transition dynamics and examples of high-return states. These methods typically learn a reward function from high-return states, use that reward function to label the transitions, and then apply an offline RL algorithm to these transitions. While these methods can achieve good results on many tasks, they can be complex, often requiring regularization and temporal difference updates. In this paper, we propose a method for offline, example-based control that learns an implicit model of multi-step transitions, rather than a reward function. We show that this implicit model can represent the Q-values for the example-based control problem. Across a ran
    
[^81]: 标签噪声：对修正的修正

    Label Noise: Correcting a Correction. (arXiv:2307.13100v1 [cs.LG])

    [http://arxiv.org/abs/2307.13100](http://arxiv.org/abs/2307.13100)

    本研究提出了一种对付标签噪声引起的过拟合的直接方法，通过观察标签噪声存在时噪声广义风险的下界，提出了在训练过程中对经验风险施加下界以减轻过拟合的方法，并提供了明确且易于计算的最小可实现噪声风险界限。

    

    在具有标签噪声的数据集上训练神经网络分类器会导致过拟合到噪声标签的风险。为了解决这个问题，研究人员探索了更鲁棒的替代损失函数。然而，许多这些替代方案都是启发式的，并且仍然容易过拟合或欠拟合。在这项工作中，我们提出了一种更直接的方法来解决由标签噪声引起的过拟合问题。我们观察到标签噪声的存在意味着噪声广义风险的下界。基于这个观察，我们建议在训练过程中对经验风险施加一个下界来减轻过拟合。我们的主要贡献是提供了理论结果，为不同的损失函数提供了明确的、易于计算的最小可实现噪声风险的界限。我们通过实验证明，在各种设置下使用这些界限显著提高了鲁棒性，几乎没有额外的计算成本。

    Training neural network classifiers on datasets with label noise poses a risk of overfitting them to the noisy labels. To address this issue, researchers have explored alternative loss functions that aim to be more robust. However, many of these alternatives are heuristic in nature and still vulnerable to overfitting or underfitting. In this work, we propose a more direct approach to tackling overfitting caused by label noise. We observe that the presence of label noise implies a lower bound on the noisy generalised risk. Building upon this observation, we propose imposing a lower bound on the empirical risk during training to mitigate overfitting. Our main contribution is providing theoretical results that yield explicit, easily computable bounds on the minimum achievable noisy risk for different loss functions. We empirically demonstrate that using these bounds significantly enhances robustness in various settings, with virtually no additional computational cost.
    
[^82]: 人口稀缺制度下的公平性研究

    Fairness Under Demographic Scarce Regime. (arXiv:2307.13081v1 [cs.LG])

    [http://arxiv.org/abs/2307.13081](http://arxiv.org/abs/2307.13081)

    这项研究探讨了在人口信息不完全可用的情况下如何提高公平性。研究发现，在替代敏感属性的属性分类器中引入不确定性意识，并对推断出的不确定性最低的人口信息样本进行公平性约束可以实现更好的公平性和准确性权衡。

    

    大多数现有的公平性研究假设模型可以完全访问人口信息。然而，由于数据采集期间未保留记录或出于隐私原因，存在人口信息部分可用的情况。这种情况被称为人口稀缺制度。先前的研究表明，训练一个属性分类器来替代缺失的敏感属性（代理）仍然可以改善公平性。然而，与真实敏感属性相比，使用代理敏感属性会加剧公平性和准确性之间的权衡。为了解决这个限制，我们提出了一个框架来构建属性分类器，以实现更好的公平性和准确性的权衡。我们的方法在属性分类器中引入不确定性意识，并对具有推断出的最低不确定性的人口信息的样本强制执行公平性。我们通过实验证明，在具有不确定敏感属性的样本上强制执行公平约束会损害算法的总体准确性，但可以提高公平性。

    Most existing works on fairness assume the model has full access to demographic information. However, there exist scenarios where demographic information is partially available because a record was not maintained throughout data collection or due to privacy reasons. This setting is known as demographic scarce regime. Prior research have shown that training an attribute classifier to replace the missing sensitive attributes (proxy) can still improve fairness. However, the use of proxy-sensitive attributes worsens fairness-accuracy trade-offs compared to true sensitive attributes. To address this limitation, we propose a framework to build attribute classifiers that achieve better fairness-accuracy trade-offs. Our method introduces uncertainty awareness in the attribute classifier and enforces fairness on samples with demographic information inferred with the lowest uncertainty. We show empirically that enforcing fairness constraints on samples with uncertain sensitive attributes is detr
    
[^83]: 自适应认证训练: 迈向更好的准确性-鲁棒性权衡

    Adaptive Certified Training: Towards Better Accuracy-Robustness Tradeoffs. (arXiv:2307.13078v1 [cs.LG])

    [http://arxiv.org/abs/2307.13078](http://arxiv.org/abs/2307.13078)

    本文提出了一种自适应认证训练方法，通过训练模型时使用自适应认证半径的关键观点，提高了模型的准确性和鲁棒性，推进了准确性-鲁棒性权衡的最新进展。

    

    随着深度学习模型的不断进步和在实际系统中的日益广泛应用，鲁棒性问题仍然是一个重大挑战。现有的认证训练方法可以在某些扰动水平上获得高度可证明的鲁棒性保证。然而，这些模型的主要问题是在干净的未扰动数据上准确性严重降低，使其不实用。在本工作中，我们考虑了在特定的高准确性水平上最大化模型的鲁棒性的更现实的角度。为此，我们提出了一种基于关键观点的新型认证训练方法，即使用自适应认证半径进行训练有助于提高模型的准确性和鲁棒性，推进了最先进的准确性-鲁棒性权衡。我们在MNIST、CIFAR-10和TinyImageNet数据集上展示了所提出方法的有效性。

    As deep learning models continue to advance and are increasingly utilized in real-world systems, the issue of robustness remains a major challenge. Existing certified training methods produce models that achieve high provable robustness guarantees at certain perturbation levels. However, the main problem of such models is a dramatically low standard accuracy, i.e. accuracy on clean unperturbed data, that makes them impractical. In this work, we consider a more realistic perspective of maximizing the robustness of a model at certain levels of (high) standard accuracy. To this end, we propose a novel certified training method based on a key insight that training with adaptive certified radii helps to improve both the accuracy and robustness of the model, advancing state-of-the-art accuracy-robustness tradeoffs. We demonstrate the effectiveness of the proposed method on MNIST, CIFAR-10, and TinyImageNet datasets. Particularly, on CIFAR-10 and TinyImageNet, our method yields models with up
    
[^84]: 通用多模态OOD检测框架

    General-Purpose Multi-Modal OOD Detection Framework. (arXiv:2307.13069v1 [cs.CV])

    [http://arxiv.org/abs/2307.13069](http://arxiv.org/abs/2307.13069)

    本文提出了一个通用的多模态OOD检测框架，名为WOOD，在传感器故障、恶劣天气和环境变化等多种因素引起的异常情况下细粒度同时检测多个OOD场景。

    

    外样本检测（OOD）识别与训练数据不同的测试样本，这对确保机器学习（ML）系统的安全性和可靠性至关重要。虽然已经开发出大量方法来检测单模态OOD样本，但只有很少几个方法专注于多模态OOD检测。目前的对比学习方法主要研究多模态OOD检测的场景是给定的图像及其相应的文本描述来自一个新领域。然而，现实世界中的ML系统部署可能面临更多由传感器故障、恶劣天气和环境变化等多种因素引起的异常情况。因此，本研究的目标是以细粒度的方式同时检测多个不同的OOD场景。为了实现这个目标，我们提出了一个通用的弱监督OOD检测框架，称为WOOD，它结合了二元分类器和对比学习组件，从而获得了增益。

    Out-of-distribution (OOD) detection identifies test samples that differ from the training data, which is critical to ensuring the safety and reliability of machine learning (ML) systems. While a plethora of methods have been developed to detect uni-modal OOD samples, only a few have focused on multi-modal OOD detection. Current contrastive learning-based methods primarily study multi-modal OOD detection in a scenario where both a given image and its corresponding textual description come from a new domain. However, real-world deployments of ML systems may face more anomaly scenarios caused by multiple factors like sensor faults, bad weather, and environmental changes. Hence, the goal of this work is to simultaneously detect from multiple different OOD scenarios in a fine-grained manner. To reach this goal, we propose a general-purpose weakly-supervised OOD detection framework, called WOOD, that combines a binary classifier and a contrastive learning component to reap the benefits of bo
    
[^85]: 特征梯度流用于解释头颈癌预测中的深度神经网络

    Feature Gradient Flow for Interpreting Deep Neural Networks in Head and Neck Cancer Prediction. (arXiv:2307.13061v1 [eess.IV])

    [http://arxiv.org/abs/2307.13061](http://arxiv.org/abs/2307.13061)

    本文提出了特征梯度流技术，用于解释深度学习模型在人类可理解的特征上的作用。通过测量可解释特征与模型的梯度流的一致性，并通过添加正则化项训练神经网络，我们可以评估特定特征对模型的重要性。

    

    本文介绍了一种新技术，即特征梯度流，用于将深度学习模型解释为对人类可理解的特征。模型的梯度流在输入数据空间中局部定义了非线性坐标，表示模型用于做出决策的信息。我们的想法是通过测量可解释特征与模型的梯度流的一致性来评估特定特征对模型的重要性，比较该特征的梯度流度量与基线噪声特征的度量。然后，我们通过在损失函数中添加正则化项的方式来训练神经网络，以使模型梯度与所选解释特征的梯度对齐。我们在从Cancer Imaging Archive获取的计算机断层扫描数据集中的头颈癌远处转移的卷积神经网络预测中测试了我们的方法。

    This paper introduces feature gradient flow, a new technique for interpreting deep learning models in terms of features that are understandable to humans. The gradient flow of a model locally defines nonlinear coordinates in the input data space representing the information the model is using to make its decisions. Our idea is to measure the agreement of interpretable features with the gradient flow of a model. To then evaluate the importance of a particular feature to the model, we compare that feature's gradient flow measure versus that of a baseline noise feature. We then develop a technique for training neural networks to be more interpretable by adding a regularization term to the loss function that encourages the model gradients to align with those of chosen interpretable features. We test our method in a convolutional neural network prediction of distant metastasis of head and neck cancer from a computed tomography dataset from the Cancer Imaging Archive.
    
[^86]: MARIO: 用于改善图对比学习的模型无关配方，提高OOD泛化性能

    MARIO: Model Agnostic Recipe for Improving OOD Generalization of Graph Contrastive Learning. (arXiv:2307.13055v1 [cs.LG])

    [http://arxiv.org/abs/2307.13055](http://arxiv.org/abs/2307.13055)

    提出了一个模型无关配方MARIO，用于改善图对比学习的OOD泛化性能。MARIO引入了信息瓶颈原则和不变性原则，旨在获得具有分布偏移鲁棒性和不变性的图表示。

    

    在这项工作中，我们研究了图数据上无监督学习方法的域外泛化问题。这种情况特别具有挑战性，因为即使有标签，图神经网络(GNNs)也显示出对分布偏移的敏感性。为了解决这个挑战，我们提出了一种名为MARIO的模型无关配方，旨在开发具有分布偏移鲁棒性的图对比方法，克服现有框架的局限性：(i)信息瓶颈(IB)原则用于实现可泛化的表示，(ii)不变性原则采用对抗性数据增强来获得不变表示。据我们所知，这是第一项研究OOD泛化问题的工作

    In this work, we investigate the problem of out-of-distribution (OOD) generalization for unsupervised learning methods on graph data. This scenario is particularly challenging because graph neural networks (GNNs) have been shown to be sensitive to distributional shifts, even when labels are available. To address this challenge, we propose a \underline{M}odel-\underline{A}gnostic \underline{R}ecipe for \underline{I}mproving \underline{O}OD generalizability of unsupervised graph contrastive learning methods, which we refer to as MARIO. MARIO introduces two principles aimed at developing distributional-shift-robust graph contrastive methods to overcome the limitations of existing frameworks: (i) Information Bottleneck (IB) principle for achieving generalizable representations and (ii) Invariant principle that incorporates adversarial data augmentation to obtain invariant representations. To the best of our knowledge, this is the first work that investigates the OOD generalization problem 
    
[^87]: 用于程序之间变量映射的图神经网络——扩展版本

    Graph Neural Networks For Mapping Variables Between Programs -- Extended Version. (arXiv:2307.13014v1 [cs.SE])

    [http://arxiv.org/abs/2307.13014](http://arxiv.org/abs/2307.13014)

    本文提出了使用图神经网络(GNNs)基于程序的抽象语法树(ASTs)来映射变量集，以解决程序比较、分析、修复和克隆检测等任务。在初学者编程作业中进行的实验证明了变量映射的有效性。

    

    自动程序分析是计算机科学中许多领域的关键研究领域，特别是形式方法和人工智能。由于程序等价问题的不可判定性，比较两个程序非常具有挑战性。通常，为了比较两个程序，需要对两个程序的变量集之间建立关系。因此，在诸如程序等价性、程序分析、程序修复和克隆检测等任务上，映射两个程序之间的变量是非常有用的。在这项工作中，我们提出使用图神经网络(GNNs)基于两个程序的抽象语法树(ASTs)来映射变量集。为了展示变量映射的优势，我们在程序修复任务中提供了这些映射的三个用例，以修复初学者编程作业中常见的和经常发生的错误。实验结果基于一个包含4166对错误/修正程序的数据集。

    Automated program analysis is a pivotal research domain in many areas of Computer Science -- Formal Methods and Artificial Intelligence, in particular. Due to the undecidability of the problem of program equivalence, comparing two programs is highly challenging. Typically, in order to compare two programs, a relation between both programs' sets of variables is required. Thus, mapping variables between two programs is useful for a panoply of tasks such as program equivalence, program analysis, program repair, and clone detection. In this work, we propose using graph neural networks (GNNs) to map the set of variables between two programs based on both programs' abstract syntax trees (ASTs). To demonstrate the strength of variable mappings, we present three use-cases of these mappings on the task of program repair to fix well-studied and recurrent bugs among novice programmers in introductory programming assignments (IPAs). Experimental results on a dataset of 4166 pairs of incorrect/corr
    
[^88]: 图神经网络中的最大独立集合用于池化

    Maximal Independent Sets for Pooling in Graph Neural Networks. (arXiv:2307.13011v1 [cs.LG])

    [http://arxiv.org/abs/2307.13011](http://arxiv.org/abs/2307.13011)

    本文提出了三种基于最大独立集合概念的图形池化方法，避免了现有方法中存在的缺点，通过实验证实了最大独立集合约束在图形池化中的重要性。

    

    卷积神经网络使得图像分类取得重大突破，通过卷积和池化。然而，对于图形而言，并不存在满足这些性质的池化方法。传统的图形池化方法存在以下缺点之一：图形断开或连接过度、降采样比较低、以及删除大部分图形。本文提出了三种基于最大独立集合概念的池化方法，避免了这些缺点。我们的实验结果证实了在图形池化中最大独立集合约束的相关性。

    Convolutional Neural Networks (CNNs) have enabled major advances in image classification through convolution and pooling. In particular, image pooling transforms a connected discrete lattice into a reduced lattice with the same connectivity and allows reduction functions to consider all pixels in an image. However, there is no pooling that satisfies these properties for graphs. In fact, traditional graph pooling methods suffer from at least one of the following drawbacks: Graph disconnection or overconnection, low decimation ratio, and deletion of large parts of graphs. In this paper, we present three pooling methods based on the notion of maximal independent sets that avoid these pitfalls. Our experimental results confirm the relevance of maximal independent set constraints for graph pooling.
    
[^89]: 深度神经网络改进了乳腺癌多基因风险评分的估计

    Deep neural network improves the estimation of polygenic risk scores for breast cancer. (arXiv:2307.13010v1 [q-bio.QM])

    [http://arxiv.org/abs/2307.13010](http://arxiv.org/abs/2307.13010)

    深度神经网络超越其他机器学习技术和统计算法，提高了估计乳腺癌多基因风险评分的准确性，能够将病例人群分成高遗传风险亚人群。

    

    多基因风险评分（PRS）是根据整个基因组中的许多遗传变异来估计个体对复杂疾病的遗传风险。在这项研究中，我们比较了一系列用于估计乳腺癌PRS的计算模型。发现深度神经网络（DNN）的性能优于其他机器学习技术和已建立的统计算法，包括BLUP、BayesA和LDpred。在具有50%患病率的测试队列中，DNN的受试者工作特征曲线下面积（AUC）为67.4％，BLUP为64.2％，BayesA为64.5％，LDpred为62.4％。BLUP，BayesA和LDpred在病例人群中生成的PRS符合正态分布。然而，DNN在病例人群中生成的PRS遵循由两个具有明显不同均值的正态分布组成的双峰分布。这表明，DNN能够将病例人群分成一个高遗传风险病例亚人群，其平均PRS明显较高。

    Polygenic risk scores (PRS) estimate the genetic risk of an individual for a complex disease based on many genetic variants across the whole genome. In this study, we compared a series of computational models for estimation of breast cancer PRS. A deep neural network (DNN) was found to outperform alternative machine learning techniques and established statistical algorithms, including BLUP, BayesA and LDpred. In the test cohort with 50% prevalence, the Area Under the receiver operating characteristic Curve (AUC) were 67.4% for DNN, 64.2% for BLUP, 64.5% for BayesA, and 62.4% for LDpred. BLUP, BayesA, and LPpred all generated PRS that followed a normal distribution in the case population. However, the PRS generated by DNN in the case population followed a bi-modal distribution composed of two normal distributions with distinctly different means. This suggests that DNN was able to separate the case population into a high-genetic-risk case sub-population with an average PRS significantly 
    
[^90]: 使用稀疏激发正则化方法对带有时间到第一个尖峰编码的脉冲神经网络进行训练

    Sparse-firing regularization methods for spiking neural networks with time-to-first spike coding. (arXiv:2307.13007v1 [cs.NE])

    [http://arxiv.org/abs/2307.13007](http://arxiv.org/abs/2307.13007)

    本文提出了两种稀疏激发正则化方法，用于训练时间到第一个尖峰编码的脉冲神经网络，以提高信息处理的能效。

    

    近年来，使用误差反向传播算法对多层脉冲神经网络（SNNs）进行训练取得了显著进展。在各种训练方案中，直接使用神经元的发放时间的误差反向传播方法引起了广泛关注，因为它可以实现理想的时间编码。这种方法使用时间到第一个尖峰（TTFS）编码，在这种编码中，每个神经元最多只能发放一次，而这种发放次数限制可以以极低的发放频率进行信息处理。这种低发放频率增加了SNNs中的信息处理能效，这不仅因为其与大脑信息处理的相似性，而且还从工程角度来看也是重要的。然而，对于TTFS编码的SNNs，目前只提供了一个上界，还没有完全研究在更低的发放频率下的信息处理能力。在本文中，我们提出了两种稀疏激发正则化方法。

    The training of multilayer spiking neural networks (SNNs) using the error backpropagation algorithm has made significant progress in recent years. Among the various training schemes, the error backpropagation method that directly uses the firing time of neurons has attracted considerable attention because it can realize ideal temporal coding. This method uses time-to-first spike (TTFS) coding, in which each neuron fires at most once, and this restriction on the number of firings enables information to be processed at a very low firing frequency. This low firing frequency increases the energy efficiency of information processing in SNNs, which is important not only because of its similarity with information processing in the brain, but also from an engineering point of view. However, only an upper limit has been provided for TTFS-coded SNNs, and the information-processing capability of SNNs at lower firing frequencies has not been fully investigated. In this paper, we propose two spike 
    
[^91]: DeepGATGO:一种基于分层预训练的图注意模型用于自动蛋白功能预测

    DeepGATGO: A Hierarchical Pretraining-Based Graph-Attention Model for Automatic Protein Function Prediction. (arXiv:2307.13004v1 [q-bio.QM])

    [http://arxiv.org/abs/2307.13004](http://arxiv.org/abs/2307.13004)

    DeepGATGO是一种基于分层预训练的图注意模型，用于自动蛋白功能预测。它通过只使用蛋白质序列，而不需要蛋白质结构信息或网络拓扑信息，实现了可靠且计算成本更低的蛋白质功能预测。

    

    自动蛋白功能预测(AFP)是一个大规模多标签分类问题，旨在自动化蛋白质富集分析，以消除当前对劳动密集型湿实验方法的依赖。目前，主流方法主要结合蛋白质相关信息和基因本体(GO)术语，生成最终的功能预测结果。例如，蛋白质序列、结构信息和蛋白质相互作用网络被整合为先验知识，与GO术语嵌入融合，并生成最终的预测结果。然而，这些方法受到获取结构信息或网络拓扑信息的困难以及这些数据准确性的限制。因此，提出了越来越多只使用蛋白质序列进行蛋白质功能预测的方法，这是一种更可靠且计算成本更低的方法。然而，现有方法未能充分提取蛋白质序列的特征信息。

    Automatic protein function prediction (AFP) is classified as a large-scale multi-label classification problem aimed at automating protein enrichment analysis to eliminate the current reliance on labor-intensive wet-lab methods. Currently, popular methods primarily combine protein-related information and Gene Ontology (GO) terms to generate final functional predictions. For example, protein sequences, structural information, and protein-protein interaction networks are integrated as prior knowledge to fuse with GO term embeddings and generate the ultimate prediction results. However, these methods are limited by the difficulty in obtaining structural information or network topology information, as well as the accuracy of such data. Therefore, more and more methods that only use protein sequences for protein function prediction have been proposed, which is a more reliable and computationally cheaper approach. However, the existing methods fail to fully extract feature information from pr
    
[^92]: 使用多模态对比学习从自然语言中提取分子属性

    Extracting Molecular Properties from Natural Language with Multimodal Contrastive Learning. (arXiv:2307.12996v1 [cs.LG])

    [http://arxiv.org/abs/2307.12996](http://arxiv.org/abs/2307.12996)

    该论文研究了如何使用多模态对比学习方法从自然语言中提取分子属性信息，通过改进文本检索和引入分子图扩增策略等方法提高了属性预测性能。实验结果显示相对于仅在图模态上预训练的模型，我们取得了+4.26%的AUROC增益和+1.54%的增益。

    

    在计算生物化学中，深度学习传统上专注于分子图神经表征；然而，最近语言模型的进展突显了文本中所编码的科学知识量。为了弥合这两种模态，我们研究了如何将分子属性信息从自然语言转化为图表征。我们研究了在使用对比学习将神经图表征与其特征的文本描述表征对齐后，属性预测性能的提升。我们实现了神经相关性评分策略以改进文本检索，引入了一种受有机反应启发的新颖合法分子图扩增策略，并在下游的MoleculeNet属性分类任务上展示了性能的改善。与仅在图模态上预训练的模型相比，我们取得了+4.26%的AUROC增益，并与最近提出的分子图/文本对比模型相比，取得了+1.54%的增益。

    Deep learning in computational biochemistry has traditionally focused on molecular graphs neural representations; however, recent advances in language models highlight how much scientific knowledge is encoded in text. To bridge these two modalities, we investigate how molecular property information can be transferred from natural language to graph representations. We study property prediction performance gains after using contrastive learning to align neural graph representations with representations of textual descriptions of their characteristics. We implement neural relevance scoring strategies to improve text retrieval, introduce a novel chemically-valid molecular graph augmentation strategy inspired by organic reactions, and demonstrate improved performance on downstream MoleculeNet property classification tasks. We achieve a +4.26% AUROC gain versus models pre-trained on the graph modality alone, and a +1.54% gain compared to recently proposed molecular graph/text contrastively t
    
[^93]: 基于多表示空间分离的图级异常检测

    Multi-representations Space Separation based Graph-level Anomaly-aware Detection. (arXiv:2307.12994v1 [cs.LG])

    [http://arxiv.org/abs/2307.12994](http://arxiv.org/abs/2307.12994)

    本文提出了一种基于多表示空间分离的图级异常感知检测框架，以解决检测图集内异常图的问题。

    

    最近，图结构模式被广泛用于对不同领域的数据建模。如何检测这些图数据中的异常图信息已成为一个热门的研究问题。本研究的目标集中在如何检测图集内的异常图这个特定问题上。先前的研究观察到异常图主要表现为节点级和图级异常，但这些方法在评估异常图时同等对待上述两种异常形式，而事实上不同类型的异常图数据在节点级和图级异常方面有不同程度的问题。此外，与正常图具有微妙差异的异常图很容易逃避现有方法的检测。因此，本文提出了一种基于多表示空间分离的图级异常感知检测框架。

    Graph structure patterns are widely used to model different area data recently. How to detect anomalous graph information on these graph data has become a popular research problem. The objective of this research is centered on the particular issue that how to detect abnormal graphs within a graph set. The previous works have observed that abnormal graphs mainly show node-level and graph-level anomalies, but these methods equally treat two anomaly forms above in the evaluation of abnormal graphs, which is contrary to the fact that different types of abnormal graph data have different degrees in terms of node-level and graph-level anomalies. Furthermore, abnormal graphs that have subtle differences from normal graphs are easily escaped detection by the existing methods. Thus, we propose a multi-representations space separation based graph-level anomaly-aware detection framework in this paper. To consider the different importance of node-level and graph-level anomalies, we design an anoma
    
[^94]: 通过舒尔多项式高效学习具有一个隐藏层的ReLU网络

    Efficiently Learning One-Hidden-Layer ReLU Networks via Schur Polynomials. (arXiv:2307.12840v1 [cs.LG])

    [http://arxiv.org/abs/2307.12840](http://arxiv.org/abs/2307.12840)

    通过使用张量分解和舒尔多项式理论，我们提出了一种高效算法，可以在标准高斯分布下学习$k$个ReLU激活的线性组合。这个算法在样本和计算复杂性上接近最优，并能在高维空间中找到较小的高阶矩误差张量。

    

    我们研究了在标准高斯分布下，关于平方损失的PAC学习$k$个ReLU激活的线性组合的问题。我们的主要结果是针对这个学习任务的一种高效算法，其样本和计算复杂性为$(dk/\epsilon)^{O(k)}$，其中$\epsilon>0$是目标精度。之前的工作给出了一个复杂性为$(dk/\epsilon)^{h(k)}$的算法，其中函数$h(k)$在$k$上的规模是超多项式的。有趣的是，我们的算法在相关统计查询算法类中接近最优。总体而言，我们的算法使用张量分解来识别一个子空间，使得所有$O(k)$阶矩在正交方向上都很小。其分析基于舒尔多项式理论，以显示较低阶误差张量的情况下，更高阶的误差张量也很小。

    We study the problem of PAC learning a linear combination of $k$ ReLU activations under the standard Gaussian distribution on $\mathbb{R}^d$ with respect to the square loss. Our main result is an efficient algorithm for this learning task with sample and computational complexity $(dk/\epsilon)^{O(k)}$, where $\epsilon>0$ is the target accuracy. Prior work had given an algorithm for this problem with complexity $(dk/\epsilon)^{h(k)}$, where the function $h(k)$ scales super-polynomially in $k$. Interestingly, the complexity of our algorithm is near-optimal within the class of Correlational Statistical Query algorithms. At a high-level, our algorithm uses tensor decomposition to identify a subspace such that all the $O(k)$-order moments are small in the orthogonal directions. Its analysis makes essential use of the theory of Schur polynomials to show that the higher-moment error tensors are small given that the lower-order ones are.
    
[^95]: 非参数线性特征学习在回归中的应用通过正则化

    Nonparametric Linear Feature Learning in Regression Through Regularisation. (arXiv:2307.12754v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2307.12754](http://arxiv.org/abs/2307.12754)

    本研究提出了一种新的非参数线性特征学习方法，对于监督学习中存在于低维线性子空间中的相关信息的预测和解释能力的提升是非常有帮助的。

    

    表征学习在自动化特征选择中发挥着关键作用，特别是在高维数据的背景下，非参数方法常常很难应对。在本研究中，我们专注于监督学习场景，其中相关信息存在于数据的低维线性子空间中，即多指数模型。如果已知该子空间，将大大增强预测、计算和解释能力。为了解决这一挑战，我们提出了一种新颖的非参数预测的线性特征学习方法，同时估计预测函数和线性子空间。我们的方法采用经验风险最小化，并加上函数导数的惩罚项，以保证其多样性。通过利用Hermite多项式的正交性和旋转不变性特性，我们引入了我们的估计器RegFeaL。通过利用替代最小化，我们迭代地旋转数据以改善与线性子空间的对齐。

    Representation learning plays a crucial role in automated feature selection, particularly in the context of high-dimensional data, where non-parametric methods often struggle. In this study, we focus on supervised learning scenarios where the pertinent information resides within a lower-dimensional linear subspace of the data, namely the multi-index model. If this subspace were known, it would greatly enhance prediction, computation, and interpretation. To address this challenge, we propose a novel method for linear feature learning with non-parametric prediction, which simultaneously estimates the prediction function and the linear subspace. Our approach employs empirical risk minimisation, augmented with a penalty on function derivatives, ensuring versatility. Leveraging the orthogonality and rotation invariance properties of Hermite polynomials, we introduce our estimator, named RegFeaL. By utilising alternative minimisation, we iteratively rotate the data to improve alignment with 
    
[^96]: 多保真度协方差估计：通过在对称正定矩阵流形上进行回归求解

    Multifidelity Covariance Estimation via Regression on the Manifold of Symmetric Positive Definite Matrices. (arXiv:2307.12438v2 [stat.CO] UPDATED)

    [http://arxiv.org/abs/2307.12438](http://arxiv.org/abs/2307.12438)

    本论文介绍了一种在对称正定矩阵流形上进行回归求解的多保真度协方差估计器，通过构造满足正定性和可实际计算属性的马氏距离最小化。该估计器是最大似然估计器，并且能相对于其他方法显著减小估计误差。

    

    我们介绍了一种多保真度协方差矩阵估计器，其构建为在对称正定矩阵流形上的回归问题的解。该估计器通过构造是正定的，并且其最小化的马氏距离具有可实际计算的属性。我们展示了我们的流形回归多保真度协方差估计器是在特定误差模型下的最大似然估计器。更广泛地说，我们展示了我们的黎曼回归框架包含了从控制变量构建的现有多保真度协方差估计器。我们通过数值示例证明，相对于单保真度和其他多保真度协方差估计器，我们的估计器可以显著减小估计误差的平方，减少一个数量级。此外，正定性的保持确保我们的估计器与下游任务兼容。

    We introduce a multifidelity estimator of covariance matrices formulated as the solution to a regression problem on the manifold of symmetric positive definite matrices. The estimator is positive definite by construction, and the Mahalanobis distance minimized to obtain it possesses properties which enable practical computation. We show that our manifold regression multifidelity (MRMF) covariance estimator is a maximum likelihood estimator under a certain error model on manifold tangent space. More broadly, we show that our Riemannian regression framework encompasses existing multifidelity covariance estimators constructed from control variates. We demonstrate via numerical examples that our estimator can provide significant decreases, up to one order of magnitude, in squared estimation error relative to both single-fidelity and other multifidelity covariance estimators. Furthermore, preservation of positive definiteness ensures that our estimator is compatible with downstream tasks, s
    
[^97]: 对无声语音激活设备进行对抗性攻击的对策

    Adversarial Agents For Attacking Inaudible Voice Activated Devices. (arXiv:2307.12204v1 [cs.LG])

    [http://arxiv.org/abs/2307.12204](http://arxiv.org/abs/2307.12204)

    本研究分析了对语音激活设备进行无声攻击的风险，并发现存在重大安全漏洞。我们提出了一种基线网络模型，并模拟了多种攻击场景，揭示了通过无需添加硬件或增加设备技能的物理访问来发现和拥有特权信息的潜力。使用深度Q学习算法，我们在少数步骤中快速拥有了所有节点。这些研究结果强调了对非传统网络和新的网络安全措施的重要性。

    

    我们对无声攻击对语音激活设备的分析确认了7.6的风险因素，强调了由NIST国家漏洞数据库（NVD）独立评分的重大安全漏洞。我们的基线网络模型展示了一种攻击者使用无声语音命令未经授权访问受保护笔记本上机密信息的场景。我们在该基线网络模型上模拟了许多攻击场景，揭示了通过物理访问而无需添加新硬件或增强设备技能的互联设备的大规模利用潜力来发现和拥有特权信息。使用微软的CyberBattleSim框架，我们评估了六种强化学习算法，发现深度Q学习与开发证明是最优的，可以在更少的步骤中迅速拥有所有节点。我们的研究结果强调了对非传统网络和新的网络安全措施的重要需求。

    Our analysis of inaudible attacks on voice-activated devices confirms the alarming risk factor of 7.6 out of 10, underlining significant security vulnerabilities scored independently by NIST National Vulnerability Database (NVD). Our baseline network model showcases a scenario in which an attacker uses inaudible voice commands to gain unauthorized access to confidential information on a secured laptop. We simulated many attack scenarios on this baseline network model, revealing the potential for mass exploitation of interconnected devices to discover and own privileged information through physical access without adding new hardware or amplifying device skills. Using Microsoft's CyberBattleSim framework, we evaluated six reinforcement learning algorithms and found that Deep-Q learning with exploitation proved optimal, leading to rapid ownership of all nodes in fewer steps. Our findings underscore the critical need for understanding non-conventional networks and new cybersecurity measure
    
[^98]: 问题分解提高了模型生成推理的忠实度

    Question Decomposition Improves the Faithfulness of Model-Generated Reasoning. (arXiv:2307.11768v1 [cs.CL])

    [http://arxiv.org/abs/2307.11768](http://arxiv.org/abs/2307.11768)

    通过将问题分解为子问题，可以显著提高大型语言模型生成推理的忠实度。

    

    随着大型语言模型（LLM）执行越来越复杂的任务，验证其行为的正确性和安全性变得越来越困难。其中一种解决方法是要求LLM在回答问题时以逐步推理的方式外化其推理过程（思维链；CoT）。推理过程可以让我们检查模型执行任务的过程。然而，这种方法依赖于所陈述的推理能够忠实地反映模型的实际推理，而这并非总是如此。为了提高CoT推理的忠实度，我们通过将问题分解为子问题来生成推理。基于分解的方法在问答任务上取得了较好的性能，有时接近CoT，并在几个最近提出的度量标准中提高了模型所陈述推理的忠实度。通过强制模型在单独的上下文中回答简单的子问题，我们大大增加了模型的忠实度。

    As large language models (LLMs) perform more difficult tasks, it becomes harder to verify the correctness and safety of their behavior. One approach to help with this issue is to prompt LLMs to externalize their reasoning, e.g., by having them generate step-by-step reasoning as they answer a question (Chain-of-Thought; CoT). The reasoning may enable us to check the process that models use to perform tasks. However, this approach relies on the stated reasoning faithfully reflecting the model's actual reasoning, which is not always the case. To improve over the faithfulness of CoT reasoning, we have models generate reasoning by decomposing questions into subquestions. Decomposition-based methods achieve strong performance on question-answering tasks, sometimes approaching that of CoT while improving the faithfulness of the model's stated reasoning on several recently-proposed metrics. By forcing the model to answer simpler subquestions in separate contexts, we greatly increase the faithf
    
[^99]: 一种用于具有缺失值的整体生存分析的深度学习方法

    A Deep Learning Approach for Overall Survival Analysis with Missing Values. (arXiv:2307.11465v1 [cs.LG])

    [http://arxiv.org/abs/2307.11465](http://arxiv.org/abs/2307.11465)

    提出了一个深度学习模型，通过有效利用被审查和未被审查病人的信息，预测非小细胞肺癌（NSCLC）病人的整体生存。

    

    人工智能可以应用于肺癌研究，尤其是非小细胞肺癌（NSCLC），这是一个具有挑战性的领域。对于病人状态的整体生存（OS）是一个重要指标，可以帮助识别生存概率不同的亚组，从而实现个体化治疗和改善整体生存率。在这个分析中，需要考虑两个挑战。首先，很少有研究能够有效利用每个病人的可用信息，利用未被审查的（即死亡）和被审查的（即幸存者）病人的信息，也要考虑到死亡时间。其次，不完整数据处理是医学领域常见的问题。这个问题通常通过使用插补方法来解决。我们的目标是提出一个能够克服这些限制的人工智能模型，能够从被审查和未被审查的病人及其可用特征中有效学习，预测NSCLC病人的OS。

    One of the most challenging fields where Artificial Intelligence (AI) can be applied is lung cancer research, specifically non-small cell lung cancer (NSCLC). In particular, overall survival (OS) is a vital indicator of patient status, helping to identify subgroups with diverse survival probabilities, enabling tailored treatment and improved OS rates. In this analysis, there are two challenges to take into account. First, few studies effectively exploit the information available from each patient, leveraging both uncensored (i.e., dead) and censored (i.e., survivors) patients, considering also the death times. Second, the handling of incomplete data is a common issue in the medical field. This problem is typically tackled through the use of imputation methods. Our objective is to present an AI model able to overcome these limits, effectively learning from both censored and uncensored patients and their available features, for the prediction of OS for NSCLC patients. We present a novel 
    
[^100]: 通过密度匹配实现的合成对照方法下的隐式内生性问题

    Synthetic Control Methods by Density Matching under Implicit Endogeneitiy. (arXiv:2307.11127v1 [econ.EM])

    [http://arxiv.org/abs/2307.11127](http://arxiv.org/abs/2307.11127)

    本文提出了一种新型的合成对照方法，通过密度匹配来解决现有SCMs中的隐式内生性问题。该方法通过将经过处理单元的结果密度与未处理单元的密度进行加权平均来估计SC权重。

    

    合成对照方法（SCMs）已成为比较案例研究中因果推断的重要工具。SCMs的基本思想是通过使用来自未处理单元的观测结果的加权和来估计经过处理单元的反事实结果。合成对照（SC）的准确性对于估计因果效应至关重要，因此，SC权重的估计成为了研究的焦点。在本文中，我们首先指出现有的SCMs存在一个隐式内生性问题，即未处理单元的结果与反事实结果模型中的误差项之间的相关性。我们展示了这个问题会对因果效应估计器产生偏差。然后，我们提出了一种基于密度匹配的新型SCM，假设经过处理单元的结果密度可以用未处理单元的密度的加权平均来近似（即混合模型）。基于这一假设，我们通过匹配来估计SC权重。

    Synthetic control methods (SCMs) have become a crucial tool for causal inference in comparative case studies. The fundamental idea of SCMs is to estimate counterfactual outcomes for a treated unit by using a weighted sum of observed outcomes from untreated units. The accuracy of the synthetic control (SC) is critical for estimating the causal effect, and hence, the estimation of SC weights has been the focus of much research. In this paper, we first point out that existing SCMs suffer from an implicit endogeneity problem, which is the correlation between the outcomes of untreated units and the error term in the model of a counterfactual outcome. We show that this problem yields a bias in the causal effect estimator. We then propose a novel SCM based on density matching, assuming that the density of outcomes of the treated unit can be approximated by a weighted average of the densities of untreated units (i.e., a mixture model). Based on this assumption, we estimate SC weights by matchi
    
[^101]: 高斯混合下的长尾理论

    Long-Tail Theory under Gaussian Mixtures. (arXiv:2307.10736v1 [cs.LG])

    [http://arxiv.org/abs/2307.10736](http://arxiv.org/abs/2307.10736)

    该论文提出了一个简单的高斯混合模型，符合Feldman的长尾理论。通过实验证明，在长尾分布情况下，非线性分类器可以提高泛化能力，而线性分类器不能。该结果强调了对于长尾分布，需要考虑罕见的训练样本以实现最佳泛化能力。

    

    我们提出了一个简单的高斯混合模型来生成遵循Feldman的长尾理论（2020）的数据。我们证明，在提出的模型中，线性分类器无法将泛化误差降低到一定水平以下，而具有记忆能力的非线性分类器可以。这证实了对于长尾分布，必须考虑罕见的训练样本以实现对新数据的最佳泛化。最后，我们通过在合成和真实数据上的实验证明，当子群体频率分布的尾部变短时，线性模型和非线性模型之间的性能差距可以减小。

    We suggest a simple Gaussian mixture model for data generation that complies with Feldman's long tail theory (2020). We demonstrate that a linear classifier cannot decrease the generalization error below a certain level in the proposed model, whereas a nonlinear classifier with a memorization capacity can. This confirms that for long-tailed distributions, rare training examples must be considered for optimal generalization to new data. Finally, we show that the performance gap between linear and nonlinear models can be lessened as the tail becomes shorter in the subpopulation frequency distribution, as confirmed by experiments on synthetic and real data.
    
[^102]: 深度学习中遗忘现象的全面调查：超越连续学习

    A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning. (arXiv:2307.09218v1 [cs.LG])

    [http://arxiv.org/abs/2307.09218](http://arxiv.org/abs/2307.09218)

    遗忘是深度学习中普遍存在的现象，不仅限于连续学习领域。解决遗忘问题面临多个挑战，包括平衡保留旧任务知识与快速学习新任务的挑战，管理任务干扰与冲突目标的挑战，以及防止隐私泄露等。遗忘不总是有害的，可以在某些情况下是有益且可取的，特别是在隐私保护场景中。

    

    遗忘指的是先前获取的信息或知识的丧失或恶化。尽管现有的关于遗忘的调查主要集中在连续学习方面，但在深度学习中，遗忘是一种普遍现象，可以在各种其他研究领域中观察到。遗忘在研究领域中表现出来，例如由于生成器漂移而在生成模型领域中表现出来，以及由于客户端之间存在异构数据分布而在联邦学习中表现出来。解决遗忘问题涉及到几个挑战，包括在快速学习新任务的同时平衡保留旧任务知识，管理任务干扰与冲突目标，以及防止隐私泄露等。此外，大多数现有的连续学习调查都默认认为遗忘总是有害的。相反，我们的调查认为遗忘是一把双刃剑，在某些情况下可以是有益且可取的，例如隐私保护场景。通过在更广泛的背景下探讨遗忘现象，

    Forgetting refers to the loss or deterioration of previously acquired information or knowledge. While the existing surveys on forgetting have primarily focused on continual learning, forgetting is a prevalent phenomenon observed in various other research domains within deep learning. Forgetting manifests in research fields such as generative models due to generator shifts, and federated learning due to heterogeneous data distributions across clients. Addressing forgetting encompasses several challenges, including balancing the retention of old task knowledge with fast learning of new tasks, managing task interference with conflicting goals, and preventing privacy leakage, etc. Moreover, most existing surveys on continual learning implicitly assume that forgetting is always harmful. In contrast, our survey argues that forgetting is a double-edged sword and can be beneficial and desirable in certain cases, such as privacy-preserving scenarios. By exploring forgetting in a broader context
    
[^103]: DeepMem: 将机器学习模型用作存储通道及其（误用）应用

    DeepMem: ML Models as storage channels and their (mis-)applications. (arXiv:2307.08811v1 [cs.LG])

    [http://arxiv.org/abs/2307.08811](http://arxiv.org/abs/2307.08811)

    本文提出了将机器学习模型作为存储通道的新视角，通过过度参数化来增加通道容量。通过在训练时嵌入信息，并利用黑盒访问实现信息的存储和提取。

    

    机器学习（ML）模型为了支持通用性和避免过拟合而过度参数化。之前的研究表明，这些额外的参数既可以用于恶意目的（例如，在经过训练的模型中隐藏一个模型），也可以用于有益目的（例如，给模型加上水印）。在本文中，我们提出了一个新的信息论视角；我们将ML模型视为一个存储通道，其容量随着过度参数化而增加。具体而言，我们考虑一个发送方，在训练时将任意信息嵌入模型中，接收方可以通过对部署模型的黑盒访问来提取信息。我们根据可用参数的数量推导出通道容量的上界。然后，我们探索了黑盒写入和读取原语，允许攻击者：（i）通过在发射机端扩充训练数据的方式以优化地将数据存储在模型中，以及（ii）通过查询模型来读取数据。

    Machine learning (ML) models are overparameterized to support generality and avoid overfitting. Prior works have shown that these additional parameters can be used for both malicious (e.g., hiding a model covertly within a trained model) and beneficial purposes (e.g., watermarking a model). In this paper, we propose a novel information theoretic perspective of the problem; we consider the ML model as a storage channel with a capacity that increases with overparameterization. Specifically, we consider a sender that embeds arbitrary information in the model at training time, which can be extracted by a receiver with a black-box access to the deployed model. We derive an upper bound on the capacity of the channel based on the number of available parameters. We then explore black-box write and read primitives that allow the attacker to: (i) store data in an optimized way within the model by augmenting the training data at the transmitter side, and (ii) to read it by querying the model afte
    
[^104]: Retentive Network: 作为大型语言模型的Transformer的继任者

    Retentive Network: A Successor to Transformer for Large Language Models. (arXiv:2307.08621v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.08621](http://arxiv.org/abs/2307.08621)

    Retentive Network（RetNet）作为大型语言模型的基础架构，实现了训练并行、低成本推理和良好的性能。通过并行、循环和分块循环三种计算范式，RetNet具有训练并行化、低成本推理和高效的长序列建模的特点。

    

    在这项工作中，我们提出了Retentive Network (RetNet)作为大型语言模型的基础架构，同时实现了训练并行、低成本推理和良好的性能。我们从理论上推导出了循环和注意力之间的连接。然后，我们提出了序列建模的保留机制，支持三种计算范式，即并行、循环和分块循环。具体而言，并行表示允许进行训练并行化。循环表示能够实现低成本的$O(1)$推理，从而提高解码吞吐量、延迟和GPU内存，同时不损失性能。分块循环表示便于使用线性复杂度进行高效的长序列建模，其中每个块可以并行编码，同时进行循环摘要。语言建模实验结果表明，RetNet实现了良好的扩展结果、并行训练、低成本部署和高效的推理。

    In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost $O(1)$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient i
    
[^105]: 重新审视最小错误熵准则的鲁棒性：转移学习案例研究

    Revisiting the Robustness of the Minimum Error Entropy Criterion: A Transfer Learning Case Study. (arXiv:2307.08572v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.08572](http://arxiv.org/abs/2307.08572)

    本研究重新审视了最小错误熵准则在处理非高斯噪声中的鲁棒性，并探讨了其在实际转移学习回归任务中的可行性和有用性。实验证明，在基本转移学习算法中，通过用最小错误熵代替均方误差损失，可以取得与现有方法相媲美的性能表现。

    

    应对数据分布转移是转移学习方法的重要组成部分，以便在实际任务中表现出色。然而，现有方法要么假设数据不包含噪声，要么采用复杂的训练范式或模型设计来处理数据分布转移。本文重新审视了在统计信号处理中广泛使用的最小错误熵（MEE）准则的鲁棒性，并研究其在实际转移学习回归任务中的可行性和有用性，其中分布转移是常见的。具体来说，我们提出了一个新的理论结果，展示了MEE对协变量转移的鲁棒性。我们还表明，通过简单地将均方误差（MSE）损失替换为MEE，在基本的转移学习算法（如微调和线性探测）中，我们可以获得与现有方法竞争力相当的性能。

    Coping with distributional shifts is an important part of transfer learning methods in order to perform well in real-life tasks. However, most of the existing approaches in this area either focus on an ideal scenario in which the data does not contain noises or employ a complicated training paradigm or model design to deal with distributional shifts. In this paper, we revisit the robustness of the minimum error entropy (MEE) criterion, a widely used objective in statistical signal processing to deal with non-Gaussian noises, and investigate its feasibility and usefulness in real-life transfer learning regression tasks, where distributional shifts are common. Specifically, we put forward a new theoretical result showing the robustness of MEE against covariate shift. We also show that by simply replacing the mean squared error (MSE) loss with the MEE on basic transfer learning algorithms such as fine-tuning and linear probing, we can achieve competitive performance with respect to state-
    
[^106]: 使用大型语言模型增强密集检索的软提示调优

    Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models. (arXiv:2307.08303v1 [cs.IR] CROSS LISTED)

    [http://arxiv.org/abs/2307.08303](http://arxiv.org/abs/2307.08303)

    本论文提出了一种使用软提示调优来增强密集检索的方法（SPTAR）。通过优化任务特定的软提示并利用大型语言模型为未标记的文档生成弱查询，可以提高零样本和少样本的密集检索模型的性能。

    

    密集检索（DR）将查询和文档转化为密集向量表示，并在向量空间中测量查询与文档之间的相似性。DR的一个挑战是缺乏领域特定的训练数据。虽然DR模型可以通过迁移学习从大规模公共数据集（如MS MARCO）中学习，但证据表明，并非所有DR模型和领域都能同等受益于迁移学习。最近，一些研究人员转向使用大型语言模型（LLMs）来改进零样本和少样本的DR模型。然而，这些方法中采用的硬提示或人工编写的提示无法保证生成的弱查询的质量。为了解决这个问题，我们提出了用于增强DR的软提示调优（SPTAR）：对于每个任务，我们利用软提示调优在有限的真实数据上优化任务特定的软提示，然后用这些提示引导LLMs为未标记的文档标记弱查询，从而得到足够的弱文档-查询对来训练任务特定的模型。

    Dense retrieval (DR) converts queries and documents into dense embeddings and measures the similarity between queries and documents in vector space. One of the challenges in DR is the lack of domain-specific training data. While DR models can learn from large-scale public datasets like MS MARCO through transfer learning, evidence shows that not all DR models and domains can benefit from transfer learning equally. Recently, some researchers have resorted to large language models (LLMs) to improve the zero-shot and few-shot DR models. However, the hard prompts or human-written prompts utilized in these works cannot guarantee the good quality of generated weak queries. To tackle this, we propose soft prompt tuning for augmenting DR (SPTAR): For each task, we leverage soft prompt-tuning to optimize a task-specific soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific d
    
[^107]: RegExplainer: 在回归任务中生成图神经网络的解释

    RegExplainer: Generating Explanations for Graph Neural Networks in Regression Task. (arXiv:2307.07840v1 [cs.LG])

    [http://arxiv.org/abs/2307.07840](http://arxiv.org/abs/2307.07840)

    这项工作提出了一种新的解释方法（XAIG-R），用于解释图回归模型，通过引入信息瓶颈理论的新目标和混合框架来解决回归任务中的挑战，同时还使用对比学习策略来处理连续有序标签。

    

    图回归是一项基础任务，在各种图学习任务中受到越来越多的关注。然而，推理过程通常是不可解释的。现有的解释技术大多限于理解分类任务中图神经网络的行为。在这项工作中，我们寻求解释来解释图回归模型（XAIG-R）。我们展示了现有方法忽视了分布偏移和连续有序的决策边界，这阻碍了它们在回归任务中的应用。为了解决这些挑战，我们提出了一种基于信息瓶颈理论的新目标，并引入了一种新的混合框架，可以以模型无关的方式支持各种图神经网络。我们进一步提出了一种对比学习策略来应对回归任务中的连续有序标签。为了从经验上验证所提出的方法的有效性，我们引入了三个基准数据集和一个真实数据集进行评估。

    Graph regression is a fundamental task and has received increasing attention in a wide range of graph learning tasks. However, the inference process is often not interpretable. Most existing explanation techniques are limited to understanding GNN behaviors in classification tasks. In this work, we seek an explanation to interpret the graph regression models (XAIG-R). We show that existing methods overlook the distribution shifting and continuously ordered decision boundary, which hinders them away from being applied in the regression tasks. To address these challenges, we propose a novel objective based on the information bottleneck theory and introduce a new mix-up framework, which could support various GNNs in a model-agnostic manner. We further present a contrastive learning strategy to tackle the continuously ordered labels in regression task. To empirically verify the effectiveness of the proposed method, we introduce three benchmark datasets and a real-life dataset for evaluation
    
[^108]: 匹配追踪的快速收敛速度

    Sharp Convergence Rates for Matching Pursuit. (arXiv:2307.07679v1 [stat.ML])

    [http://arxiv.org/abs/2307.07679](http://arxiv.org/abs/2307.07679)

    本文通过提升现有的下界来匹配最佳上界，对匹配追踪的性能进行了精确描述，并构造了一个最坏情况的字典来证明现有上界的无法改进。

    

    本文研究了匹配追踪的基本限制，即通过字典中的元素的稀疏线性组合来近似目标函数的纯贪婪算法。当目标函数包含在对应于字典的变化空间中时，许多令人印象深刻的研究在过去几十年中获得了匹配追踪的收敛速度的上界和下界，但它们并不匹配。本文的主要贡献是填补这一差距，并获得匹配追踪性能的精确描述。我们通过改进现有的下界以匹配最佳上界来实现这一目标。具体来说，我们构造了一个最坏情况的字典，证明了现有的上界不能改进。事实证明，与其他贪婪算法变体不同，收敛速度是次优的，并且由解某个非线性方程的解决方案决定。这使我们得出结论，任意程度的收缩都会改善匹配追踪效果。

    We study the fundamental limits of matching pursuit, or the pure greedy algorithm, for approximating a target function by a sparse linear combination of elements from a dictionary. When the target function is contained in the variation space corresponding to the dictionary, many impressive works over the past few decades have obtained upper and lower bounds on the convergence rate of matching pursuit, but they do not match. The main contribution of this paper is to close this gap and obtain a sharp characterization of the performance of matching pursuit. We accomplish this by improving the existing lower bounds to match the best upper bound. Specifically, we construct a worst case dictionary which proves that the existing upper bound cannot be improved. It turns out that, unlike other greedy algorithm variants, the converge rate is suboptimal and is determined by the solution to a certain non-linear equation. This enables us to conclude that any amount of shrinkage improves matching pu
    
[^109]: MaxMin-L2-SVC-NCH:一种用于训练支持向量分类器并选择模型参数的新方法

    MaxMin-L2-SVC-NCH: A New Method to Train Support Vector Classifier with the Selection of Model's Parameters. (arXiv:2307.07343v1 [cs.LG])

    [http://arxiv.org/abs/2307.07343](http://arxiv.org/abs/2307.07343)

    本文提出了一种新的方法，用于训练支持向量分类器并选择模型参数。通过建模为极小化极大优化问题，利用投影梯度算法求解，实现了更低的时间复杂度。

    

    模型参数的选择在支持向量分类（SVC）的应用中发挥着重要作用。常用的选择模型参数的方法是k折交叉验证与格点搜索（CV）。由于需要训练大量的SVC模型，这个方法非常耗时。本文提出了一种新方法，用于训练SVC并选择模型参数。首先，将具有模型参数选择的SVC训练建模为极小化极大优化问题（MaxMin-L2-SVC-NCH），其中极小化问题是寻找两个正常凸壳之间最接近点的优化问题（L2-SVC-NCH），而极大化问题是寻找最优模型参数的优化问题。MaxMin-L2-SVC-NCH具有较低的时间复杂度，因为放弃了CV。然后，提出了一种基于梯度的算法来求解MaxMin-L2-SVC-NCH，其中L2-SVC-NCH通过投影梯度算法求解。

    The selection of model's parameters plays an important role in the application of support vector classification (SVC). The commonly used method of selecting model's parameters is the k-fold cross validation with grid search (CV). It is extremely time-consuming because it needs to train a large number of SVC models. In this paper, a new method is proposed to train SVC with the selection of model's parameters. Firstly, training SVC with the selection of model's parameters is modeled as a minimax optimization problem (MaxMin-L2-SVC-NCH), in which the minimization problem is an optimization problem of finding the closest points between two normal convex hulls (L2-SVC-NCH) while the maximization problem is an optimization problem of finding the optimal model's parameters. A lower time complexity can be expected in MaxMin-L2-SVC-NCH because CV is abandoned. A gradient-based algorithm is then proposed to solve MaxMin-L2-SVC-NCH, in which L2-SVC-NCH is solved by a projected gradient algorithm 
    
[^110]: 机器学习实践和基础设施

    Machine Learning practices and infrastructures. (arXiv:2307.06518v1 [cs.CY])

    [http://arxiv.org/abs/2307.06518](http://arxiv.org/abs/2307.06518)

    本文研究了机器学习实践中从业者与工具的互动，以及这些互动对于机器学习实践和系统开发的影响。通过实证研究，发现交互式计算平台在学习和协调实践中起到了重要的基础设施作用。

    

    机器学习（ML）系统在重大领域部署时具有深远影响。它们可能加剧现有的不平等，创造新的歧视模式，并固化过时的社会构造。因此，ML系统开发的社会背景（即组织、团队、文化）是AI伦理领域和决策者进行积极研究和干预的焦点。本文关注一个常被忽视的社会背景方面：从业者与他们所依赖的工具之间的互动，以及这些互动在塑造ML实践和ML系统开发中的作用。特别地，通过对Stack Exchange论坛上提出的问题进行实证研究，探讨了在ML实践中使用交互式计算平台（如Jupyter Notebook和Google Colab）。我发现交互式计算平台在学习和协调实践中被广泛使用，构成了一种基础设施。

    Machine Learning (ML) systems, particularly when deployed in high-stakes domains, are deeply consequential. They can exacerbate existing inequities, create new modes of discrimination, and reify outdated social constructs. Accordingly, the social context (i.e. organisations, teams, cultures) in which ML systems are developed is a site of active research for the field of AI ethics, and intervention for policymakers. This paper focuses on one aspect of social context that is often overlooked: interactions between practitioners and the tools they rely on, and the role these interactions play in shaping ML practices and the development of ML systems. In particular, through an empirical study of questions asked on the Stack Exchange forums, the use of interactive computing platforms (e.g. Jupyter Notebook and Google Colab) in ML practices is explored. I find that interactive computing platforms are used in a host of learning and coordination practices, which constitutes an infrastructural r
    
[^111]: 一种贝叶斯方法用于量化交通预测模型中的不确定性和改善泛化能力

    A Bayesian approach to quantifying uncertainties and improving generalizability in traffic prediction models. (arXiv:2307.05946v1 [cs.LG])

    [http://arxiv.org/abs/2307.05946](http://arxiv.org/abs/2307.05946)

    本研究提出了一种贝叶斯循环神经网络框架，通过引入归一化处理，实现交通预测模型中的不确定性量化和更高的泛化能力。

    

    交通数据预测的深度学习模型可以通过多层架构对复杂函数进行优化建模，但这些方法的一个主要缺点是大多数方法不提供带有不确定性估计的预测结果，而这对于交通运营和控制是必需的。本研究提出了一种贝叶斯循环神经网络框架，通过引入谱归一化到其隐藏层，实现交通预测中的不确定性量化和更高的泛化能力。我们的论文表明，归一化通过控制模型的复杂性并减少对训练数据的过度拟合风险，改善了深度神经网络的泛化性能。

    Deep-learning models for traffic data prediction can have superior performance in modeling complex functions using a multi-layer architecture. However, a major drawback of these approaches is that most of these approaches do not offer forecasts with uncertainty estimates, which are essential for traffic operations and control. Without uncertainty estimates, it is difficult to place any level of trust to the model predictions, and operational strategies relying on overconfident predictions can lead to worsening traffic conditions. In this study, we propose a Bayesian recurrent neural network framework for uncertainty quantification in traffic prediction with higher generalizability by introducing spectral normalization to its hidden layers. In our paper, we have shown that normalization alters the training process of deep neural networks by controlling the model's complexity and reducing the risk of overfitting to the training data. This, in turn, helps improve the generalization perfor
    
[^112]: 将课程与回放相结合：对持续学习的影响

    Integrating Curricula with Replays: Its Effects on Continual Learning. (arXiv:2307.05747v1 [cs.LG])

    [http://arxiv.org/abs/2307.05747](http://arxiv.org/abs/2307.05747)

    将课程与回放方法相结合可以提高持续学习的效果，通过调整回放实例与训练数据的交替频率、回放实例的顺序以及选择进入回放缓冲区的策略实现。

    

    人类在获取新技能或知识时，通过课程进行学习和复习。这种人类学习行为启发了在持续学习代理中将课程与回放方法相结合。目标是模拟人类学习过程，从而提高知识保留和促进学习转移。现有的持续学习代理中的回放方法涉及从先前任务中随机选择和排序数据，已经证明是有效的。然而，有限的研究探讨了将不同课程与回放方法相结合以增强持续学习的问题。我们的研究首次考察了将课程与回放方法相结合对持续学习的影响的三个具体方面：回放实例与训练数据的交替频率，回放实例的顺序，以及选择实例进入回放缓冲区的策略。这些课程设计的方面

    Humans engage in learning and reviewing processes with curricula when acquiring new skills or knowledge. This human learning behavior has inspired the integration of curricula with replay methods in continual learning agents. The goal is to emulate the human learning process, thereby improving knowledge retention and facilitating learning transfer. Existing replay methods in continual learning agents involve the random selection and ordering of data from previous tasks, which has shown to be effective. However, limited research has explored the integration of different curricula with replay methods to enhance continual learning. Our study takes initial steps in examining the impact of integrating curricula with replay methods on continual learning in three specific aspects: the interleaved frequency of replayed exemplars with training data, the sequence in which exemplars are replayed, and the strategy for selecting exemplars into the replay buffer. These aspects of curricula design al
    
[^113]: 跨深度学习架构实现特征归因的协调: 提升解释性和一致性

    Harmonizing Feature Attributions Across Deep Learning Architectures: Enhancing Interpretability and Consistency. (arXiv:2307.02150v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.02150](http://arxiv.org/abs/2307.02150)

    该论文研究了特征归因方法在不同深度学习架构之间的通用性，并探讨了在多个模型上协调特征归因的可行性。研究结果显示特征归因的协调有助于提高深度学习模型的解释性和一致性。

    

    确保机器学习模型的可信度和可解释性对于其在现实世界应用中至关重要。特征归因方法已经引起了许多关注，它们通过将重要性归因给个别输入特征来提供模型预测的局部解释。该研究探讨了特征归因在各种深度学习架构（如卷积神经网络（CNN）和视觉转换器）之间的泛化能力。我们旨在评估将特征归因方法作为未来检测器的可行性，并研究如何在采用不同架构但以相同数据分布训练的多个模型之间协调这些特征。通过探索这种协调，我们旨在开发出更一致和乐观的特征归因理解，提高深度学习模型之间局部解释的一致性。我们的研究结果凸显了实现特征归因协调性的潜力。

    Ensuring the trustworthiness and interpretability of machine learning models is critical to their deployment in real-world applications. Feature attribution methods have gained significant attention, which provide local explanations of model predictions by attributing importance to individual input features. This study examines the generalization of feature attributions across various deep learning architectures, such as convolutional neural networks (CNNs) and vision transformers. We aim to assess the feasibility of utilizing a feature attribution method as a future detector and examine how these features can be harmonized across multiple models employing distinct architectures but trained on the same data distribution. By exploring this harmonization, we aim to develop a more coherent and optimistic understanding of feature attributions, enhancing the consistency of local explanations across diverse deep-learning models. Our findings highlight the potential for harmonized feature att
    
[^114]: 扩大步长和算子学习的加速原始-对偶方法在非光滑最优控制问题中的应用

    Accelerated primal-dual methods with enlarged step sizes and operator learning for nonsmooth optimal control problems. (arXiv:2307.00296v1 [math.OC])

    [http://arxiv.org/abs/2307.00296](http://arxiv.org/abs/2307.00296)

    该论文研究了在非光滑最优控制问题中加速原始-对偶方法的两种方法：增大步长和算子学习。研究表明，增大步长的加速原始-对偶方法可以在保证收敛性的同时简单有效地提高计算速度；而算子学习则通过构建神经网络代理模型来加速求解涉及的偏微分方程。

    

    我们考虑一类具有偏微分方程（PDE）约束的非光滑最优控制问题，由于其非光滑目标函数和离散化后的高维和病态系统，这类问题非常具有挑战性。我们重点研究了原始-对偶方法的应用，该方法可以分别处理不同类型的变量，因此每次迭代的主要计算只需要解决两个PDE。我们的目标是通过使用较大的步长或算子学习技术来加速原始-对偶方法。对于具有较大步长的加速原始-对偶方法，其收敛性仍然可以得到严格证明，同时它以一种简单且普遍的方式数值上加速了原始-对偶方法。对于算子学习加速，我们构建了深度神经网络代理模型来表示涉及的PDE。一旦学习到一个神经算子，解决一个PDE只需要进行神经网络的前向传播。

    We consider a general class of nonsmooth optimal control problems with partial differential equation (PDE) constraints, which are very challenging due to its nonsmooth objective functionals and the resulting high-dimensional and ill-conditioned systems after discretization. We focus on the application of a primal-dual method, with which different types of variables can be treated individually and thus its main computation at each iteration only requires solving two PDEs. Our target is to accelerate the primal-dual method with either larger step sizes or operator learning techniques. For the accelerated primal-dual method with larger step sizes, its convergence can be still proved rigorously while it numerically accelerates the original primal-dual method in a simple and universal way. For the operator learning acceleration, we construct deep neural network surrogate models for the involved PDEs. Once a neural operator is learned, solving a PDE requires only a forward pass of the neural
    
[^115]: 差分隐私分布式估计和学习

    Differentially Private Distributed Estimation and Learning. (arXiv:2306.15865v1 [cs.LG])

    [http://arxiv.org/abs/2306.15865](http://arxiv.org/abs/2306.15865)

    本文研究了在网络环境中的分布式估计和学习问题，通过交换私有观测信息，代理可以集体估计未知数量，而保护隐私。通过线性聚合方案和差分隐私（DP）调整的随机化方案，本研究提出了一种能够在保证隐私的同时高效组合观测数据的算法。

    

    我们研究了在网络环境中的分布式估计和学习问题，其中代理通过交换信息来估计从其私下观察的样本中未知的统计属性。通过交换私有观测信息，代理可以集体估计未知数量，但他们也面临隐私风险。我们的聚合方案的目标是在时间和网络中高效地组合观测数据，同时满足代理的隐私需求，而不需要任何超越他们本地附近的协调。我们的算法使参与的代理能够从离线或随时间在线获取的私有信号中估计完整的充分统计量，并保护其信号和网络附近的隐私。这是通过线性聚合方案和调整的随机化方案实现的，将噪声添加到交换的估计数据中以满足差分隐私（DP）。

    We study distributed estimation and learning problems in a networked environment in which agents exchange information to estimate unknown statistical properties of random variables from their privately observed samples. By exchanging information about their private observations, the agents can collectively estimate the unknown quantities, but they also face privacy risks. The goal of our aggregation schemes is to combine the observed data efficiently over time and across the network, while accommodating the privacy needs of the agents and without any coordination beyond their local neighborhoods. Our algorithms enable the participating agents to estimate a complete sufficient statistic from private signals that are acquired offline or online over time, and to preserve the privacy of their signals and network neighborhoods. This is achieved through linear aggregation schemes with adjusted randomization schemes that add noise to the exchanged estimates subject to differential privacy (DP
    
[^116]: 转换器训练策略用于预测多个负载时间序列

    Transformer Training Strategies for Forecasting Multiple Load Time Series. (arXiv:2306.10891v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10891](http://arxiv.org/abs/2306.10891)

    转换器模型在预测多负载时间序列方面使用全局训练策略比多变量和本地训练策略具有更好的性能，平均降低了21.8%和12.8%的预测误差。

    

    在未来的智能电网中，准确的负载预测可以帮助在本地平衡供需，并防止电网故障。尽管被监测的客户数量将随着不断推进的智能电表安装而增加，但每个客户的数据量始终是有限的。我们评估了转换器负载预测模型是否受益于转移学习策略，即在多个客户的负载时间序列上训练全局的单变量模型。在使用两个包含数百个客户的数据集进行的实验中，我们发现全局训练策略优于相关工作中使用的多变量和本地训练策略。平均而言，与其他两种策略相比，全局训练策略在从未来一天到一个月的预测时间范围内，预测误差降低了21.8%和12.8%。与线性模型、多层感知机和LSTM模型的比较显示，转换器训练策略效果更好。

    In the smart grid of the future, accurate load forecasts on the level of individual clients can help to balance supply and demand locally and to prevent grid outages. While the number of monitored clients will increase with the ongoing smart meter rollout, the amount of data per client will always be limited. We evaluate whether a Transformer load forecasting model benefits from a transfer learning strategy, where a global univariate model is trained on the load time series from multiple clients. In experiments with two datasets containing load time series from several hundred clients, we find that the global training strategy is superior to the multivariate and local training strategies used in related work. On average, the global training strategy results in 21.8% and 12.8% lower forecasting errors than the two other strategies, measured across forecasting horizons from one day to one month into the future. A comparison to linear models, multi-layer perceptrons and LSTMs shows that T
    
[^117]: 部分观测动力系统中回音状态网络预测视角的变异性

    Variability of echo state network prediction horizon for partially observed dynamical systems. (arXiv:2306.10797v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2306.10797](http://arxiv.org/abs/2306.10797)

    该论文研究了使用部分状态观测的动力系统，并提出了具有部分状态输入和部分或完全状态输出的回音状态网络（ESN）框架。通过应用于Lorenz系统和Chua振荡器，展示了ESN的短期预测能力和预测视角的变异性，以及ESN在学习系统动力学方面的有效性。

    

    使用部分状态观测研究动力系统是一个重要的问题，因为它适用于许多实际系统。我们通过提出具有部分状态输入和部分或完全状态输出的回音状态网络（ESN）框架来解决这个问题。我们将其应用于Lorenz系统和Chua振荡器（包括数值模拟和实验系统），展示了我们方法的有效性。我们显示ESN作为一个自主动力系统，能够做出短期预测，可达几个Lyapunov时间。然而，预测视角的变异性取决于初始条件-这是我们通过预测视角的分布进行详细研究的一个方面。此外，我们使用各种统计指标将ESN预测的长期动力学与数值模拟或实验动力学进行比较，并观察到类似的结果，我们表明ESN可以有效地学习系统的动力学，即使在训练时也是如此。

    Study of dynamical systems using partial state observation is an important problem due to its applicability to many real-world systems. We address the problem by proposing an echo state network (ESN) framework with partial state input with partial or full state output. Application to the Lorenz system and Chua's oscillator (both numerically simulated and experimental systems) demonstrate the effectiveness of our method. We show that the ESN, as an autonomous dynamical system, is capable of making short-term predictions up to a few Lyapunov times. However, the prediction horizon has high variability depending on the initial condition - an aspect that we explore in detail using the distribution of the prediction horizon. Further, using a variety of statistical metrics to compare the long-term dynamics of the ESN predictions with numerically simulated or experimental dynamics and observed similar results, we show that the ESN can effectively learn the system's dynamics even when trained w
    
[^118]: G-不变扩散地图

    G-invariant diffusion maps. (arXiv:2306.07350v1 [cs.LG])

    [http://arxiv.org/abs/2306.07350](http://arxiv.org/abs/2306.07350)

    研究构造了针对连续矩阵群封闭下的流形中采样的数据集的G-不变扩散地图，能够实现等变且不变的嵌入，适用于对数据点进行聚类和对齐。

    

    数据扩散地图在从降维和聚类到数据可视化等任务中均取得了成功。本研究考虑到从一个连续矩阵群封闭下的流形中采样的数据集，我们构造了既等变又不变的嵌入，这些嵌入可以自然地用于对数据点进行聚类和对齐。我们使用模拟数据证明了我们的构造的有效性。

    The diffusion maps embedding of data lying on a manifold have shown success in tasks ranging from dimensionality reduction and clustering, to data visualization. In this work, we consider embedding data sets which were sampled from a manifold which is closed under the action of a continuous matrix group. An example of such a data set are images who's planar rotations are arbitrary. The G-invariant graph Laplacian, introduced in a previous work of the authors, admits eigenfunctions in the form of tensor products between the elements of the irreducible unitary representations of the group and eigenvectors of certain matrices. We employ these eigenfunctions to derive diffusion maps that intrinsically account for the group action on the data. In particular, we construct both equivariant and invariant embeddings which can be used naturally to cluster and align the data points. We demonstrate the effectiveness of our construction with simulated data.
    
[^119]: 通过群表示学习对称下的字典学习

    Dictionary Learning under Symmetries via Group Representations. (arXiv:2305.19557v1 [math.OC])

    [http://arxiv.org/abs/2305.19557](http://arxiv.org/abs/2305.19557)

    本文研究在预定变换群下学习不变的字典问题。利用非阿贝尔傅里叶分析，提供了算法，建立了字典学习问题可以被有效地理解为某些矩阵优化问题的理论基础。

    

    字典学习问题可以被看作是一个数据驱动的过程，旨在学习一个合适的变换，以便通过示例数据直接表示数据的稀疏性。本文研究了在预定的变换群下学习不变的字典问题。自然的应用领域包括冷冻电镜、多目标跟踪、同步和姿态估计等。我们特别从数学表示理论的角度研究了这个问题。通过利用非阿贝尔傅里叶分析，我们为符合这些不变性的字典学习提供了算法。我们将自然界中的字典学习问题，其自然被建模为无限维度的问题，与相关的计算问题，这必然是有限维度的问题，联系起来。我们建立了字典学习问题可以被有效地理解为某些矩阵优化问题的理论基础。

    The dictionary learning problem can be viewed as a data-driven process to learn a suitable transformation so that data is sparsely represented directly from example data. In this paper, we examine the problem of learning a dictionary that is invariant under a pre-specified group of transformations. Natural settings include Cryo-EM, multi-object tracking, synchronization, pose estimation, etc. We specifically study this problem under the lens of mathematical representation theory. Leveraging the power of non-abelian Fourier analysis for functions over compact groups, we prescribe an algorithmic recipe for learning dictionaries that obey such invariances. We relate the dictionary learning problem in the physical domain, which is naturally modelled as being infinite dimensional, with the associated computational problem, which is necessarily finite dimensional. We establish that the dictionary learning problem can be effectively understood as an optimization instance over certain matrix o
    
[^120]: Fourier-DeepONet: 增强傅里叶算子深度网络的完全波形反演，提高了精度，通用性和鲁棒性。

    Fourier-DeepONet: Fourier-enhanced deep operator networks for full waveform inversion with improved accuracy, generalizability, and robustness. (arXiv:2305.17289v1 [cs.LG])

    [http://arxiv.org/abs/2305.17289](http://arxiv.org/abs/2305.17289)

    该论文提出了Fourier-DeepONet算法，可用于完全波形反演，具有对地震源的泛化能力，可提高准确性和鲁棒性，适用于具有可变源的FWI。

    

    完全波形反演（FWI）是通过解决一个非凸优化问题从地震波形数据中推断地下结构信息的方法。数据驱动的FWI已经越来越被研究，使用不同的神经网络架构来提高准确性和计算效率。然而，预训练神经网络的适用性受到潜在的调查源函数和训练过程中使用的源函数之间的差异的严重限制。在这里，我们开发了一种傅里叶增强的深度算子网络（Fourier-DeepONet）用于全波形反演，并且具有地震源的泛化能力，包括地震源的频率和位置。具体而言，我们将傅里叶神经算子作为DeepONet的解码器，并利用源参数作为Fourier-DeepONet的其中一个输入，以实现对具有可变源的FWI的分辨率。为了测试Fourier-DeepONet，我们开发了两个新的逼真的FWI基准数据集（FWI-F和FWI-L），它们具有不同的源频率。

    Full waveform inversion (FWI) infers the subsurface structure information from seismic waveform data by solving a non-convex optimization problem. Data-driven FWI has been increasingly studied with various neural network architectures to improve accuracy and computational efficiency. Nevertheless, the applicability of pre-trained neural networks is severely restricted by potential discrepancies between the source function used in the field survey and the one utilized during training. Here, we develop a Fourier-enhanced deep operator network (Fourier-DeepONet) for FWI with the generalization of seismic sources, including the frequencies and locations of sources. Specifically, we employ the Fourier neural operator as the decoder of DeepONet, and we utilize source parameters as one input of Fourier-DeepONet, facilitating the resolution of FWI with variable sources. To test Fourier-DeepONet, we develop two new and realistic FWI benchmark datasets (FWI-F and FWI-L) with varying source frequ
    
[^121]: MLP在顺序推荐中的复仇

    Revenge of MLP in Sequential Recommendation. (arXiv:2305.14675v1 [cs.LG])

    [http://arxiv.org/abs/2305.14675](http://arxiv.org/abs/2305.14675)

    本文提出了一种纯MLP顺序推荐架构TriMLP，其中加入了新颖的三角形混合器以实现标记有序的交互，以提高顺序推荐的性能表现。

    

    顺序推荐模型对历史用户-物品交互行为序列进行建模，以更好地推断动态偏好。近年来，得益于改进的神经网络架构，如RNN、CNN和Transformer，这个领域已经迎来了快速的性能提升。最近，全MLP模型的研究成果引发了人们对一种高效的方法的注意，即通过混合历史行为的MLP学习转换模式。然而，由于这种全连接结构允许不受限制的跨行为间通信并忽略了时间顺序，我们发现直接将混合MLP应用于顺序推荐会导致较差的性能。本文提出了一种纯MLP顺序推荐架构TriMLP，其中包括一种新颖的三角形混合器，改进后的MLP赋予了标记有序的交互。由于MLP中的跨标记交互实际上是矩阵...

    Sequential recommendation models sequences of historical user-item interactive behaviors (or referred as token) to better infer dynamic preferences. Fueled by the improved neural network architectures such as RNN, CNN and Transformer, this field has enjoyed rapid performance boost in the past years. Recent progress on all-MLP models lights on an efficient method with less intensive computation, token-mixing MLP, to learn the transformation patterns among historical behaviors. However, due to the inherent fully-connection design that allows the unrestricted cross-token communication and ignores the chronological order, we find that directly applying token-mixing MLP into sequential recommendation leads to subpar performance. In this paper, we present a purely MLP-based sequential recommendation architecture TriMLP with a novel \underline{Tri}angular Mixer where the modified \underline{MLP} endows tokens with ordered interactions. As the cross-token interaction in MLP is actually matrix 
    
[^122]: 什么症状以及持续多久？一种可解释的人工智能方法用于社交媒体中的抑郁症检测。

    What Symptoms and How Long? An Interpretable AI Approach for Depression Detection in Social Media. (arXiv:2305.13127v2 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2305.13127](http://arxiv.org/abs/2305.13127)

    本论文介绍了一种使用可解释的人工智能方法在社交媒体中检测抑郁症的方法。该方法创新地检测和解释抑郁症状及其持续时间，并通过大规模数据集的实证分析表明优于其他方法，发现了新的未注意到的症状。

    

    抑郁症是最常见和严重的精神疾病，引发了严重的经济和社会影响。抑郁症的检测对于早期干预以减轻这些后果至关重要。这种高风险的决策本质上需要可解释性。虽然有一些抑郁症检测研究试图基于重要性分数或关注权重解释决策，但这些解释与临床抑郁症诊断标准不一致，后者基于抑郁症状。为了填补这一空白，我们遵循计算设计科学范式，开发了一种新颖的多尺度时间原型网络(MSTPNet)。MSTPNet创新地检测和解释抑郁症状及其持续时间。使用大规模数据集进行深入实证分析表明，MSTPNet在F1分数0.851上优于最先进的抑郁症检测方法。这个结果还揭示了未在调查方法中注意到的新症状，例如分享。

    Depression is the most prevalent and serious mental illness, which induces grave financial and societal ramifications. Depression detection is key for early intervention to mitigate those consequences. Such a high-stake decision inherently necessitates interpretability. Although a few depression detection studies attempt to explain the decision based on the importance score or attention weights, these explanations misalign with the clinical depression diagnosis criterion that is based on depressive symptoms. To fill this gap, we follow the computational design science paradigm to develop a novel Multi-Scale Temporal Prototype Network (MSTPNet). MSTPNet innovatively detects and interprets depressive symptoms as well as how long they last. Extensive empirical analyses using a large-scale dataset show that MSTPNet outperforms state-of-the-art depression detection methods with an F1-score of 0.851. This result also reveals new symptoms that are unnoted in the survey approach, such as shari
    
[^123]: 无参数条件和局部连接切片Wasserstein流量的生成建模

    Nonparametric Generative Modeling with Conditional and Locally-Connected Sliced-Wasserstein Flows. (arXiv:2305.02164v1 [cs.LG])

    [http://arxiv.org/abs/2305.02164](http://arxiv.org/abs/2305.02164)

    本文提出了两个重要贡献：一是提出了条件切片Wasserstein流（CSWF），可以实现非参数条件建模，二是将局部连接和多尺度表示等视觉研究启发的技术引入到SWF中，从而大大提高了图像建模的效率和质量。

    

    切片Wasserstein流（SWF）是一种非参数生成建模的有前途的方法，但由于其发生生成质量的亚优性和缺乏条件建模能力而未被广泛采用。我们在这项工作中，为弥合这一差距做出了两个重要贡献。首先，基于一个愉悦的观察（在某些条件下），联合分布的SWF与条件分布的SWF相符，我们提出了条件切片Wasserstein流（CSWF），这是SWF的一个简单但有效的扩展，可实现非参数条件建模。其次，我们引入了适当的图像归纳偏置到SWF中，用两个技术受到视觉研究中的局部连接和多尺度表示的启发，大大提高了图像建模的效率和质量。通过全部改进，在进行纯非参数建模的同时，在有条件和无条件任务上实现了与许多深度参数化生成模型相当的生成性能。

    Sliced-Wasserstein Flow (SWF) is a promising approach to nonparametric generative modeling but has not been widely adopted due to its suboptimal generative quality and lack of conditional modeling capabilities. In this work, we make two major contributions to bridging this gap. First, based on a pleasant observation that (under certain conditions) the SWF of joint distributions coincides with those of conditional distributions, we propose Conditional Sliced-Wasserstein Flow (CSWF), a simple yet effective extension of SWF that enables nonparametric conditional modeling. Second, we introduce appropriate inductive biases of images into SWF with two techniques inspired by local connectivity and multiscale representation in vision research, which greatly improve the efficiency and quality of modeling images. With all the improvements, we achieve generative performance comparable with many deep parametric generative models on both conditional and unconditional tasks in a purely nonparametric
    
[^124]: DataComp：寻找下一代多模态数据集

    DataComp: In search of the next generation of multimodal datasets. (arXiv:2304.14108v1 [cs.CV])

    [http://arxiv.org/abs/2304.14108](http://arxiv.org/abs/2304.14108)

    DataComp是一个基准测试，旨在通过提出新的训练集来解决数据集在机器学习生态系统中的缺陷。它提供了一个多规模设计的实验测试平台，使用12.8B个图像-文本对的新候选池，让研究人员可以通过设计新的过滤技术或策划新的数据源并评估它们的新数据集来进行创新。

    

    大型的多模态数据集在近期的突破中起到了关键作用，比如CLIP、Stable Diffusion和GPT-4等。与此同时，数据集很少得到与模型架构或训练算法同等的研究关注。为了解决这个在机器学习生态系统中的缺陷，我们介绍了DataComp，一个基准测试，其中训练代码是固定的，研究人员通过提出新的训练集来进行创新。我们提供了一个基于Common Crawl的新候选池，其中包含12.8B个图像-文本对的数据集实验测试平台。参加我们基准测试的研究人员可以设计新的过滤技术或策划新的数据源，并通过运行我们标准化的CLIP训练代码并在38个下游测试集上进行测试来评估他们的新数据集。我们的基准测试包含多个规模，四个候选池大小和相应的计算预算，在训练期间涵盖了从12.8M到12.8B个样本。这种多规模设计有助于研究规模趋势，并为研究人员提供了更多的选择余地。

    Large multimodal datasets have been instrumental in recent breakthroughs such as CLIP, Stable Diffusion, and GPT-4. At the same time, datasets rarely receive the same research attention as model architectures or training algorithms. To address this shortcoming in the machine learning ecosystem, we introduce DataComp, a benchmark where the training code is fixed and researchers innovate by proposing new training sets. We provide a testbed for dataset experiments centered around a new candidate pool of 12.8B image-text pairs from Common Crawl. Participants in our benchmark design new filtering techniques or curate new data sources and then evaluate their new dataset by running our standardized CLIP training code and testing on 38 downstream test sets. Our benchmark consists of multiple scales, with four candidate pool sizes and associated compute budgets ranging from 12.8M to 12.8B samples seen during training. This multi-scale design facilitates the study of scaling trends and makes the
    
[^125]: B2Opt: 用少量预算学习优化黑箱优化

    B2Opt: Learning to Optimize Black-box Optimization with Little Budget. (arXiv:2304.11787v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.11787](http://arxiv.org/abs/2304.11787)

    B2Opt是一种学习优化黑箱优化的方法，可以在少量预算下实现更快而更好的性能提升。它通过设计一个强大的优化框架，自动学习并利用目标任务的廉价代理函数指导高效优化策略的设计。与现有方法相比，B2Opt具有更强的优化策略表示能力并能实现多个数量级的性能提升。

    

    高维昂贵黑箱优化（BBO）的核心挑战在于如何以较少的函数评估成本更快地获得更好的性能。问题的本质在于如何设计一种针对目标任务的高效优化策略。本文设计了一个强大的优化框架，可以在无人干预的情况下从目标或廉价代理任务中自动学习优化策略。然而，目前的方法在优化策略的表示方面存在缺陷。为了实现这一点，1）借鉴遗传算法的机制，我们提出了一种名为B2Opt的深度神经网络框架，它基于适者生存选择原理，具有更强的优化策略表示能力；2）B2Opt可以利用目标任务的廉价代理函数指导高效优化策略的设计。与最先进的BBO基准相比，B2Opt可以实现几个数量级的性能提升。

    The core challenge of high-dimensional and expensive black-box optimization (BBO) is how to obtain better performance faster with little function evaluation cost. The essence of the problem is how to design an efficient optimization strategy tailored to the target task. This paper designs a powerful optimization framework to automatically learn the optimization strategies from the target or cheap surrogate task without human intervention. However, current methods are weak for this due to poor representation of optimization strategy. To achieve this, 1) drawing on the mechanism of genetic algorithm, we propose a deep neural network framework called B2Opt, which has a stronger representation of optimization strategies based on survival of the fittest; 2) B2Opt can utilize the cheap surrogate functions of the target task to guide the design of the efficient optimization strategies. Compared to the state-of-the-art BBO baselines, B2Opt can achieve multiple orders of magnitude performance i
    
[^126]: 一种解释不能适用于XIL

    One Explanation Does Not Fit XIL. (arXiv:2304.07136v1 [cs.LG])

    [http://arxiv.org/abs/2304.07136](http://arxiv.org/abs/2304.07136)

    提出了一种解释性交互式机器学习框架（XIL）来修正模型，但同时发现"一种解释不能适用于XIL"，建议考虑多种解释。

    

    当前的机器学习模型在许多领域产生出色的结果，但同时也存在着快捷学习和错误相关的缺陷。为了解决这些缺陷，提出了解释性交互式机器学习（XIL）框架，通过使用用户对模型解释的反馈来修正模型。本文重点探讨了该框架中使用的解释。特别地，我们研究了通过多个解释方法进行同时模型修正，从而发现"一种解释不能适用于XIL" ，并建议在通过XIL进行模型修正时考虑多种解释。

    Current machine learning models produce outstanding results in many areas but, at the same time, suffer from shortcut learning and spurious correlations. To address such flaws, the explanatory interactive machine learning (XIL) framework has been proposed to revise a model by employing user feedback on a model's explanation. This work sheds light on the explanations used within this framework. In particular, we investigate simultaneous model revision through multiple explanation methods. To this end, we identified that \textit{one explanation does not fit XIL} and propose considering multiple ones when revising models via XIL.
    
[^127]: 压缩索引实现瞬间相似性搜索

    Similarity search in the blink of an eye with compressed indices. (arXiv:2304.04759v1 [cs.LG])

    [http://arxiv.org/abs/2304.04759](http://arxiv.org/abs/2304.04759)

    本文提出一种新的向量压缩方法局部自适应量化(LVQ)，并在基于图的索引的关键优化下实现减少有效带宽同时启用随机访问友好的快速相似性计算，从而在性能和内存占用方面创造了新的最佳表现。

    

    如今，数据以向量表示。在海量数据中寻找与给定查询相似的向量是一项广泛应用的问题。本文提出了创建更快、更小的索引以运行这些搜索的新技术。为此，我们介绍了一种新的向量压缩方法，局部自适应量化(LVQ)，它同时减少内存占用和改善搜索性能，对搜索准确性的影响最小。LVQ被设计为与基于图的索引一起工作以实现减少有效带宽同时启用随机访问友好的快速相似性计算。我们的实验结果表明，在现代数据中心系统中针对基于图的索引进行关键优化后，LVQ的性能和内存占用方面创造了新的最佳表现。在处理数十亿个向量时，LVQ超过第二佳方案：

    Nowadays, data is represented by vectors. Retrieving those vectors, among millions and billions, that are similar to a given query is a ubiquitous problem of relevance for a wide range of applications. In this work, we present new techniques for creating faster and smaller indices to run these searches. To this end, we introduce a novel vector compression method, Locally-adaptive Vector Quantization (LVQ), that simultaneously reduces memory footprint and improves search performance, with minimal impact on search accuracy. LVQ is designed to work optimally in conjunction with graph-based indices, reducing their effective bandwidth while enabling random-access-friendly fast similarity computations. Our experimental results show that LVQ, combined with key optimizations for graph-based indices in modern datacenter systems, establishes the new state of the art in terms of performance and memory footprint. For billions of vectors, LVQ outcompetes the second-best alternatives: (1) in the low
    
[^128]: FairGen: 迈向公平图生成

    FairGen: Towards Fair Graph Generation. (arXiv:2303.17743v1 [cs.LG])

    [http://arxiv.org/abs/2303.17743](http://arxiv.org/abs/2303.17743)

    本文提出了一种公平感知图生成模型FairGen，通过标签损失和公平损失来对模型进行训练，并使用雅可比优化方法来实现公平性和逼真性。

    

    在过去的几十年中，人们致力于在各种领域中生成逼真的图形，从社交网络到计算机网络，从基因调控网络到在线交易网络。尽管这些工作取得了显著的成功，但绝大多数工作都没有监督性质，通常是在训练时最小化预期的图形重构损失，这可能导致在生成的图形中出现表示偏差问题，即受保护的群体(通常是少数群体)对目标贡献更少，因此会遭受系统性更高的误差。本文旨在通过利用标签信息和用户首选的平等约束，将图生成调整为下游挖掘任务。特别地，在探究图生成模型中的表示偏差的背景下，我们提出了一个名为FairGen的公平感知图生成模型，以减轻这种偏差。我们的模型联合了标签损失和公平损失，并通过雅可比优化方法进行训练，以得到公平性，同时保持逼真性。

    There have been tremendous efforts over the past decades dedicated to the generation of realistic graphs in a variety of domains, ranging from social networks to computer networks, from gene regulatory networks to online transaction networks. Despite the remarkable success, the vast majority of these works are unsupervised in nature and are typically trained to minimize the expected graph reconstruction loss, which would result in the representation disparity issue in the generated graphs, i.e., the protected groups (often minorities) contribute less to the objective and thus suffer from systematically higher errors. In this paper, we aim to tailor graph generation to downstream mining tasks by leveraging label information and user-preferred parity constraint. In particular, we start from the investigation of representation disparity in the context of graph generative models. To mitigate the disparity, we propose a fairness-aware graph generative model named FairGen. Our model jointly 
    
[^129]: 研究和减轻多视角聚类中实际场景中嘈杂视图的副作用

    Investigating and Mitigating the Side Effects of Noisy Views in Multi-view Clustering in Practical Scenarios. (arXiv:2303.17245v1 [cs.LG])

    [http://arxiv.org/abs/2303.17245](http://arxiv.org/abs/2303.17245)

    本文提出了一种理论上基础的深度MvC方法（MvCAN），旨在解决实际场景中嘈杂视图的问题，通过实现多视图一致性、互补性和噪声鲁棒性来减少嘈杂视图的副作用，并在实验证明该方法优于现有的MvC方法。

    

    多视角聚类（MvC）旨在探索多视图数据的类别结构，而无需标签监督。多视图比单视图提供更多信息，因此现有的MvC方法可以实现令人满意的性能。然而，在实际场景中，如果视图嘈杂，它们的性能可能会严重退化。在本文中，我们首先正式研究了嘈杂视图的缺点，随后提出了一个理论上基础的深度MvC方法（称为MvCAN）来解决这个问题。具体来说，我们提出了一个新颖的MvC目标，使得不共享参数和不一致的聚类预测可以跨越多个视图，以减少嘈杂视图的副作用。此外，设计了一种非参数迭代过程，以生成一个稳健的学习目标，以挖掘多个视图的有用信息。理论分析表明，MvCAN的工作是通过实现多视图一致性，互补性和噪声鲁棒性来实现的。最后，对公开基准数据集和新收集的实际数据集进行的实验证明，MvCAN在处理实际场景中的嘈杂视图方面优于现有的MvC方法。

    Multi-view clustering (MvC) aims at exploring the category structure among multi-view data without label supervision. Multiple views provide more information than single views and thus existing MvC methods can achieve satisfactory performance. However, their performance might seriously degenerate when the views are noisy in practical scenarios. In this paper, we first formally investigate the drawback of noisy views and then propose a theoretically grounded deep MvC method (namely MvCAN) to address this issue. Specifically, we propose a novel MvC objective that enables un-shared parameters and inconsistent clustering predictions across multiple views to reduce the side effects of noisy views. Furthermore, a non-parametric iterative process is designed to generate a robust learning target for mining multiple views' useful information. Theoretical analysis reveals that MvCAN works by achieving the multi-view consistency, complementarity, and noise robustness. Finally, experiments on publ
    
[^130]: marl-jax：用于社会普适性的多智能体强化学习框架

    marl-jax: Multi-agent Reinforcement Leaning framework for Social Generalization. (arXiv:2303.13808v1 [cs.MA])

    [http://arxiv.org/abs/2303.13808](http://arxiv.org/abs/2303.13808)

    marl-jax是一个基于DeepMind的JAX生态系和RL生态系的多智能体RL软件包，可以训练和评估代理在多样性背景下的社会普适性，提供命令行界面，适用于合作与竞争游戏环境。

    

    最近，强化学习领域的进展促进了许多令人兴奋的应用。这些进展是由算法和工程方面的改进驱动的，导致RL代理的训练速度更快。我们提出了marl-jax，这是一个用于训练和评估代理的社会普适性的多智能体强化学习软件包。该包旨在训练多智能体环境中的一组代理，并评估其对多样化背景代理的泛化能力。它建立在DeepMind的JAX生态系统上，并利用由DeepMind开发的RL生态系统。我们的marl-jax框架能够在多个代理的合作和竞争、同时行动的环境中工作。该包提供了一个直观且用户友好的命令行界面，用于训练一组代理并评估其泛化能力。总之，marl-jax为研究人员提供了有价值的资源。

    Recent advances in Reinforcement Learning (RL) have led to many exciting applications. These advancements have been driven by improvements in both algorithms and engineering, which have resulted in faster training of RL agents. We present marl-jax, a multi-agent reinforcement learning software package for training and evaluating social generalization of the agents. The package is designed for training a population of agents in multi-agent environments and evaluating their ability to generalize to diverse background agents. It is built on top of DeepMind's JAX ecosystem~\cite{deepmind2020jax} and leverages the RL ecosystem developed by DeepMind. Our framework marl-jax is capable of working in cooperative and competitive, simultaneous-acting environments with multiple agents. The package offers an intuitive and user-friendly command-line interface for training a population and evaluating its generalization capabilities. In conclusion, marl-jax provides a valuable resource for researchers
    
[^131]: 防止注意力熵崩溃的Transformer训练稳定性研究

    Stabilizing Transformer Training by Preventing Attention Entropy Collapse. (arXiv:2303.06296v1 [cs.LG])

    [http://arxiv.org/abs/2303.06296](http://arxiv.org/abs/2303.06296)

    本文研究了Transformer的训练动态，发现低注意力熵伴随着高训练不稳定性，提出了一种简单而有效的解决方案$\sigma$Reparam，成功地防止了注意力层中的熵崩溃，促进了更稳定的训练。

    This paper investigates the training dynamics of Transformers and proposes a simple and efficient solution, $\sigma$Reparam, to prevent entropy collapse in the attention layers, promoting more stable training.

    训练稳定性对于Transformer至关重要。本文通过研究注意力层的演变来探究Transformer的训练动态。特别地，我们在训练过程中跟踪每个注意力头的注意力熵，这是模型锐度的代理。我们发现，在不同的架构和任务中存在一种常见模式，即低注意力熵伴随着高训练不稳定性，这可能采取振荡损失或发散的形式。我们将病态低注意力熵，对应高度集中的注意力分数，称为$\textit{熵崩溃}$。作为一种解决方案，我们提出了$\sigma$Reparam，一种简单而有效的解决方案，其中我们使用谱归一化和额外的学习标量重新参数化所有线性层。我们证明了所提出的重新参数化成功地防止了注意力层中的熵崩溃，促进了更稳定的训练。此外，我们

    Training stability is of great importance to Transformers. In this work, we investigate the training dynamics of Transformers by examining the evolution of the attention layers. In particular, we track the attention entropy for each attention head during the course of training, which is a proxy for model sharpness. We identify a common pattern across different architectures and tasks, where low attention entropy is accompanied by high training instability, which can take the form of oscillating loss or divergence. We denote the pathologically low attention entropy, corresponding to highly concentrated attention scores, as $\textit{entropy collapse}$. As a remedy, we propose $\sigma$Reparam, a simple and efficient solution where we reparametrize all linear layers with spectral normalization and an additional learned scalar. We demonstrate that the proposed reparameterization successfully prevents entropy collapse in the attention layers, promoting more stable training. Additionally, we 
    
[^132]: 非对角度量中的可扩展随机梯度里曼动力学

    Scalable Stochastic Gradient Riemannian Langevin Dynamics in Non-Diagonal Metrics. (arXiv:2303.05101v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.05101](http://arxiv.org/abs/2303.05101)

    本文提出了两种非对角度量，可以在随机梯度采样中使用，以改善神经网络模型的贝叶斯推断性能，尤其对于具有稀疏诱导先验的全连接神经网络和具有相关先验的卷积神经网络效果显著。

    

    随机梯度采样方法通常用于对神经网络进行贝叶斯推断。观察到，包含微分几何概念的方法往往具有更好的性能，里曼度量通过考虑局部曲率来改善后验探索。然而，现有方法往往采用简单的对角度量以保持计算效率，这会损失一些性能。我们提出了两种非对角度量，可以在随机梯度采样中使用，以改善收敛性和探索性，在对比对角度量只有轻微的计算开销。我们展示了对于具有稀疏诱导先验的全连接神经网络和具有相关先验的卷积神经网络，使用这些度量可以提供改进。对于其他一些选择，后验分布在简单度量下也足够容易。

    Stochastic-gradient sampling methods are often used to perform Bayesian inference on neural networks. It has been observed that the methods in which notions of differential geometry are included tend to have better performances, with the Riemannian metric improving posterior exploration by accounting for the local curvature. However, the existing methods often resort to simple diagonal metrics to remain computationally efficient. This loses some of the gains. We propose two non-diagonal metrics that can be used in stochastic-gradient samplers to improve convergence and exploration but have only a minor computational overhead over diagonal metrics. We show that for fully connected neural networks (NNs) with sparsity-inducing priors and convolutional NNs with correlated priors, using these metrics can provide improvements. For some other choices the posterior is sufficiently easy also for the simpler metrics.
    
[^133]: 线性卷积神经网络仅利用最主导的频率发现数据集的统计结构

    Linear CNNs Discover the Statistical Structure of the Dataset Using Only the Most Dominant Frequencies. (arXiv:2303.02034v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.02034](http://arxiv.org/abs/2303.02034)

    本研究通过理论分析和实验研究，揭示了线性卷积神经网络在学习过程中通过发现数据集的统计结构，并且仅利用数据集中最主导的频率进行发现。

    

    本文通过分析梯度下降方程，提出了线性卷积神经网络学习理论的一个突破。我们发现，网络在训练过程中的演变是由数据集结构和卷积网络结构之间的相互作用所决定的。我们表明，线性卷积神经网络通过非线性、有序、阶段性的转变来发现数据集的统计结构，并且发现发现的速度取决于数据集和卷积网络结构之间的关系。此外，我们发现这种相互作用是我们所称的“主导频率偏差”的核心，线性卷积神经网络仅利用数据集中不同结构部分的主导频率来进行这些发现。我们还通过实验证明了我们的理论与实际使用的深度非线性卷积神经网络之间的关系。

    We here present a stepping stone towards a deeper understanding of convolutional neural networks (CNNs) in the form of a theory of learning in linear CNNs. Through analyzing the gradient descent equations, we discover that the evolution of the network during training is determined by the interplay between the dataset structure and the convolutional network structure. We show that linear CNNs discover the statistical structure of the dataset with non-linear, ordered, stage-like transitions, and that the speed of discovery changes depending on the relationship between the dataset and the convolutional network structure. Moreover, we find that this interplay lies at the heart of what we call the ``dominant frequency bias'', where linear CNNs arrive at these discoveries using only the dominant frequencies of the different structural parts present in the dataset. We furthermore provide experiments that show how our theory relates to deep, non-linear CNNs used in practice. Our findings shed 
    
[^134]: 基于时空Transformer引导的扩散数据增强方法用于高效的基于骨骼的动作识别

    Spatial-temporal Transformer-guided Diffusion based Data Augmentation for Efficient Skeleton-based Action Recognition. (arXiv:2302.13434v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.13434](http://arxiv.org/abs/2302.13434)

    本论文介绍了一种基于时空Transformer引导的扩散数据增强方法，旨在为基于骨骼的动作识别任务生成高质量和多样化的序列动作。通过引入去噪扩散概率模型（DDPMs），本方法能够生成自然逼真的动作序列。

    

    最近，基于骨骼的人体动作成为了热门的研究课题，因为人体骨骼的紧凑表达给这个研究领域注入了新活力。因此，研究者开始注意到使用RGB或其他传感器来通过提取骨骼信息分析人体动作的重要性。利用深度学习的快速发展，最近提出了许多基于骨骼的人体动作方法，这些方法都有精心设计的深度学习结构。然而，一个训练良好的深度学习模型总是需要高质量和充足的数据，而这往往需要高昂的费用和人力资源。本文介绍了一种新颖的基于骨骼动作识别任务的数据增强方法，该方法可以有效生成高质量和多样化的序列动作。为了获得自然和逼真的动作序列，我们提出了去噪扩散概率模型（DDPMs），它可以生成一系列合成动作序列。

    Recently, skeleton-based human action has become a hot research topic because the compact representation of human skeletons brings new blood to this research domain. As a result, researchers began to notice the importance of using RGB or other sensors to analyze human action by extracting skeleton information. Leveraging the rapid development of deep learning (DL), a significant number of skeleton-based human action approaches have been presented with fine-designed DL structures recently. However, a well-trained DL model always demands high-quality and sufficient data, which is hard to obtain without costing high expenses and human labor. In this paper, we introduce a novel data augmentation method for skeleton-based action recognition tasks, which can effectively generate high-quality and diverse sequential actions. In order to obtain natural and realistic action sequences, we propose denoising diffusion probabilistic models (DDPMs) that can generate a series of synthetic action seque
    
[^135]: 以局部差分隐私为基础的联邦学习中的主动成员推断攻击

    Active Membership Inference Attack under Local Differential Privacy in Federated Learning. (arXiv:2302.12685v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12685](http://arxiv.org/abs/2302.12685)

    这项研究提出了一种以局部差分隐私为基础的联邦学习中的主动成员推断攻击，该攻击利用恶意参数干扰全局模型并通过非线性决策边界推断客户端的私有训练数据，对客户端的隐私造成显著风险，并发现防止攻击所需的保护隐私噪声会严重损害联邦学习的模型效用。

    

    联邦学习（FL）最初被视为在具有数据隐私保护的协调服务器上进行协作学习的框架。本文提出了一种由不诚实服务器在FL中进行的新型主动成员推断（AMI）攻击。在AMI攻击中，服务器制造并嵌入恶意参数到全局模型中，以有效推断目标数据样本是否包含在客户端的私有训练数据中。通过利用数据特征之间的相关性通过非线性决策边界，AMI攻击在严格的局部差分隐私（LDP）保护下可以实现极高的成功率，从而使客户端的训练数据面临显著的隐私风险。在几个基准数据集上的理论和实验结果表明，为防止我们的攻击而添加足够的保护隐私的噪声会显著损害FL的模型效用。

    Federated learning (FL) was originally regarded as a framework for collaborative learning among clients with data privacy protection through a coordinating server. In this paper, we propose a new active membership inference (AMI) attack carried out by a dishonest server in FL. In AMI attacks, the server crafts and embeds malicious parameters into global models to effectively infer whether a target data sample is included in a client's private training data or not. By exploiting the correlation among data features through a non-linear decision boundary, AMI attacks with a certified guarantee of success can achieve severely high success rates under rigorous local differential privacy (LDP) protection; thereby exposing clients' training data to significant privacy risk. Theoretical and experimental results on several benchmark datasets show that adding sufficient privacy-preserving noise to prevent our attack would significantly damage FL's model utility.
    
[^136]: 流行的人工神经网络激活函数的统一化

    Unification of popular artificial neural network activation functions. (arXiv:2302.11007v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11007](http://arxiv.org/abs/2302.11007)

    激活函数的统一化表示采用了Mittag-Leffler函数，可以插值不同激活函数、减轻梯度问题，并适用于不同复杂度的神经网络训练。

    

    我们提出了最流行的神经网络激活函数的统一表示。采用了分数微积分的Mittag-Leffler函数，我们提出了一种灵活且紧凑的功能形式，能够在不同的激活函数之间进行插值，并减轻训练神经网络中常见的问题，如梯度消失和梯度爆炸。所提出的门控表示扩展了固定形状激活函数的范围，将其转化为自适应对应物，其形状可以从训练数据中学习。所提出的函数形式的导数也可以用Mittag-Leffler函数表示，因此它是梯度下降反向传播算法的合适候选。通过在不同复杂度和不同大小的数据集上训练多个神经网络，我们证明采用统一的门控激活函数表示为各种内置实现的经济的和有希望的替代方法。

    We present a unified representation of the most popular neural network activation functions. Adopting Mittag-Leffler functions of fractional calculus, we propose a flexible and compact functional form that is able to interpolate between various activation functions and mitigate common problems in training neural networks such as vanishing and exploding gradients. The presented gated representation extends the scope of fixed-shape activation functions to their adaptive counterparts whose shape can be learnt from the training data. The derivatives of the proposed functional form can also be expressed in terms of Mittag-Leffler functions making it a suitable candidate for gradient-based backpropagation algorithms. By training multiple neural networks of different complexities on various datasets with different sizes, we demonstrate that adopting a unified gated representation of activation functions offers a promising and affordable alternative to individual built-in implementations of ac
    
[^137]: 在线学习引导的曲率逼近：具有全局非渐近超线性收敛的拟牛顿法。

    Online Learning Guided Curvature Approximation: A Quasi-Newton Method with Global Non-Asymptotic Superlinear Convergence. (arXiv:2302.08580v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2302.08580](http://arxiv.org/abs/2302.08580)

    本文提出了第一个具有明确非渐近超线性收敛速率的全局收敛拟牛顿方法，并采用混合近端外梯度法结构和在线学习框架来更新Hessian逼近矩阵。

    

    拟牛顿算法是解决无约束最小化问题中最受欢迎的迭代方法之一，这主要归功于其良好的超线性收敛性质。然而，现有的算法结果存在限制，要么提供了具有渐近超线性收敛速度的全局收敛保证，要么仅在初始点和初始Hessian逼近选择适当的情况下提供了局部非渐近超线性速率。特别地，目前没有拟牛顿方法的分析保证了具有明确超线性收敛速率的全局收敛性。在本文中，我们填补了这一空白，并提出了第一个具有明确非渐近超线性收敛速率的全局收敛拟牛顿方法。与传统的拟牛顿方法不同，我们基于混合近端外梯度法构建了我们的算法，并提出了一种新颖的在线学习框架来更新Hessian逼近矩阵。

    Quasi-Newton algorithms are among the most popular iterative methods for solving unconstrained minimization problems, largely due to their favorable superlinear convergence property. However, existing results for these algorithms are limited as they provide either (i) a global convergence guarantee with an asymptotic superlinear convergence rate, or (ii) a local non-asymptotic superlinear rate for the case that the initial point and the initial Hessian approximation are chosen properly. In particular, no current analysis for quasi-Newton methods guarantees global convergence with an explicit superlinear convergence rate. In this paper, we close this gap and present the first globally convergent quasi-Newton method with an explicit non-asymptotic superlinear convergence rate. Unlike classical quasi-Newton methods, we build our algorithm upon the hybrid proximal extragradient method and propose a novel online learning framework for updating the Hessian approximation matrices. Specificall
    
[^138]: 基于分数的条件模型的概念代数

    Concept Algebra for Score-Based Conditional Models. (arXiv:2302.03693v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.03693](http://arxiv.org/abs/2302.03693)

    本文研究了基于分数的条件模型中学习表示的结构，并开发了一种数学形式化表达概念被编码为表示空间子空间的思想。利用这个方法，我们提出了一种简单的方法来识别给定概念对应的表示部分，并通过代数操作操纵模型所表达的概念。

    

    本文研究了文本引导生成模型中学习表示的结构，重点关注基于分数的模型。我们聚焦于概念被编码为某种表示空间的子空间（或方向）的思想，并开发了这个思想的数学形式化。利用这个形式化方法，我们展示了有一个自然的表示选择具有这种性质，并且我们开发了一种简单的方法来识别与给定概念对应的表示部分。特别是，这使我们能够通过对表示的代数操作来操纵模型所表达的概念。我们使用稳定扩散在文本引导图像生成的示例中演示了这个思想。

    This paper concerns the structure of learned representations in text-guided generative models, focusing on score-based models. Here, we focus on the idea that concepts are encoded as subspaces (or directions) of some representation space. We develop a mathematical formalization of this idea.Using this formalism, we show there's a natural choice of representation with this property, and we develop a simple method for identifying the part of the representation corresponding to a given concept. In particular, this allows us to manipulate the concepts expressed by the model through algebraic manipulation of the representation. We demonstrate the idea with examples text-guided image generation, using Stable Diffusion.
    
[^139]: 通过自我对战实现多样化诱导的环境设计

    Diversity Induced Environment Design via Self-Play. (arXiv:2302.02119v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.02119](http://arxiv.org/abs/2302.02119)

    本文提出了一种利用自我对战技术的任务不可知方法，来识别环境中的观察/隐藏状态，并将多样性引入非监督环境设计框架中，从而提高了环境设计的效率和有效性。

    

    最近关于环境分布设计的研究已经展示出训练有效的通用能力代理的前景。它的成功部分在于一种自适应课程学习的形式，该形式通过生成代理能力的前沿环境实例（或级别）。然而，这种环境设计框架经常在具有挑战性的设计空间中发现有效级别方面存在困难，并需要与环境进行高成本交互。本文的目的是在非监督环境设计（UED）框架中引入多样性。具体来说，我们提出了一种任务不可知的方法来识别对给定级别具有代表性的观察/隐藏状态。然后利用这种方法的结果来表征两个级别之间的多样性，正如我们所展示的，这对于有效性能至关重要。此外，为了提高采样效率，我们加入了自我对战技术，使得环境生成器能够自动生成环境。

    Recent work on designing an appropriate distribution of environments has shown promise for training effective generally capable agents. Its success is partly because of a form of adaptive curriculum learning that generates environment instances (or levels) at the frontier of the agent's capabilities. However, such an environment design framework often struggles to find effective levels in challenging design spaces and requires costly interactions with the environment. In this paper, we aim to introduce diversity in the Unsupervised Environment Design (UED) framework. Specifically, we propose a task-agnostic method to identify observed/hidden states that are representative of a given level. The outcome of this method is then utilized to characterize the diversity between two levels, which as we show can be crucial to effective performance. In addition, to improve sampling efficiency, we incorporate the self-play technique that allows the environment generator to automatically generate e
    
[^140]: 使用Davis-Yin分裂实现更快的预测与优化

    Faster Predict-and-Optimize with Davis-Yin Splitting. (arXiv:2301.13395v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13395](http://arxiv.org/abs/2301.13395)

    本文介绍了一种使用Davis-Yin分裂方法实现更快的预测与优化的方法，该方法借鉴了现代凸优化的思想，能够在具有数千个变量的问题上轻松扩展。

    

    在许多应用中，需要反复解决具有相似但不同参数的组合问题。然而，参数$w$并非直接观察到的；只有与$w$相关的上下文数据$d$可用。我们很容易就会想到使用神经网络来根据$d$预测$w$，但是训练这样的模型需要将组合优化的离散性与用于训练神经网络的梯度优化框架相结合。当所讨论的问题是整数线性规划（ILP）时，克服这个问题的一种方法是考虑组合问题的连续放松。虽然现有方法使用这种方法在小型问题（10-100个变量）上显示出了高度的效果，但在大型问题上扩展能力不足。在本研究中，我们借鉴了现代凸优化的思想，设计了一个网络和训练方案，可以轻松地扩展到具有数千个变量的问题。

    In many applications, a combinatorial problem must be repeatedly solved with similar, but distinct parameters. Yet, the parameters $w$ are not directly observed; only contextual data $d$ that correlates with $w$ is available. It is tempting to use a neural network to predict $w$ given $d$, but training such a model requires reconciling the discrete nature of combinatorial optimization with the gradient-based frameworks used to train neural networks. When the problem in question is an Integer Linear Program (ILP), one approach to overcoming this issue is to consider a continuous relaxation of the combinatorial problem. While existing methods utilizing this approach have shown to be highly effective on small problems (10-100 variables), they do not scale well to large problems. In this work, we draw on ideas from modern convex optimization to design a network and training scheme which scales effortlessly to problems with thousands of variables.
    
[^141]: 使用外部分布检测的插件估计器进行有选择性分类

    Plugin estimators for selective classification with out-of-distribution detection. (arXiv:2301.12386v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12386](http://arxiv.org/abs/2301.12386)

    这篇论文提出了一种新的插件估计器方法，用于有选择性分类和外部分布检测（SCOD）问题，并且在理论上有基础，有效性高，可以推广现有方法。

    

    真实世界的分类器可以受益于在对置信度较低的样本进行预测时选择性地放弃。这种弃权对于接近学习决策边界的样本或者相对于训练样本来说是异常值的样本特别有用。这些设置已经在选择性分类(SC)和外部分布(OOD)检测的领域中被广泛但不关联的研究所研究。最近有关选择性分类与外部分布检测(SCOD)的研究认为这两个问题的统一研究是有必要的；然而，对于这个问题的正式基础仍然不成熟，现有的技术也都是启发式的。在本文中，我们提出了一种新的SCOD的插件估计器，它们在理论上是有根据的、有效的，并且泛化了SC和OOD检测领域的现有方法。在我们的分析过程中，我们正式说明了对现有的SC和OOD检测基线的天真使用可能不足以适应SCOD。

    Real-world classifiers can benefit from the option of abstaining from predicting on samples where they have low confidence. Such abstention is particularly useful on samples which are close to the learned decision boundary, or which are outliers with respect to the training sample. These settings have been the subject of extensive but disjoint study in the selective classification (SC) and out-of-distribution (OOD) detection literature. Recent work on selective classification with OOD detection (SCOD) has argued for the unified study of these problems; however, the formal underpinnings of this problem are still nascent, and existing techniques are heuristic in nature. In this paper, we propose new plugin estimators for SCOD that are theoretically grounded, effective, and generalise existing approaches from the SC and OOD detection literature. In the course of our analysis, we formally explicate how na\"{i}ve use of existing SC and OOD detection baselines may be inadequate for SCOD. We 
    
[^142]: A$^2$-UAV：边缘辅助无人机系统的应用感知内容和网络优化

    A$^2$-UAV: Application-Aware Content and Network Optimization of Edge-Assisted UAV Systems. (arXiv:2301.06363v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2301.06363](http://arxiv.org/abs/2301.06363)

    A$^2$-UAV是一种边缘辅助无人机系统的优化框架，通过应用感知的方式解决了多跳无人机网络中带宽限制的问题，该框架考虑了深度神经网络准确性与图像压缩之间的关系，以及无人机的能量和位置等因素。

    

    为了进行高级监视，无人机需要执行边缘辅助计算机视觉任务。在多跳无人机网络中，由于严重的带宽限制，将这些任务成功传输到边缘面临严峻的挑战。因此，我们提出了一种新颖的A$^2$-UAV框架，以优化边缘上正确执行的任务数量。与现有技术形成鲜明对比的是，我们采用应用感知的方法，并制定了一个新的应用感知任务规划问题（A$^2$-TPP），该问题考虑了（i）基于可用数据集的感兴趣类别的深度神经网络（DNN）准确性与图像压缩之间的关系，（ii）目标位置，（iii）无人机的当前能量/位置，以优化每个无人机的路由、数据预处理和目标分配。我们证明了A$^2$-TPP是NP-难问题，并提出了一个多项式时间算法来高效地解决它。我们对A$^2$-UAV进行了广泛评估。

    To perform advanced surveillance, Unmanned Aerial Vehicles (UAVs) require the execution of edge-assisted computer vision (CV) tasks. In multi-hop UAV networks, the successful transmission of these tasks to the edge is severely challenged due to severe bandwidth constraints. For this reason, we propose a novel A$^2$-UAV framework to optimize the number of correctly executed tasks at the edge. In stark contrast with existing art, we take an application-aware approach and formulate a novel pplication-Aware Task Planning Problem (A$^2$-TPP) that takes into account (i) the relationship between deep neural network (DNN) accuracy and image compression for the classes of interest based on the available dataset, (ii) the target positions, (iii) the current energy/position of the UAVs to optimize routing, data pre-processing and target assignment for each UAV. We demonstrate A$^2$-TPP is NP-Hard and propose a polynomial-time algorithm to solve it efficiently. We extensively evaluate A$^2$-UAV th
    
[^143]: 使用打乱和批量剪切的方法推广DP-SGD算法

    Generalizing DP-SGD with Shuffling and Batch Clipping. (arXiv:2212.05796v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.05796](http://arxiv.org/abs/2212.05796)

    本文提出了一个通用的差分隐私算法框架，允许将任何一阶优化器与批量剪切相结合，并使用打乱等采样技术。作者的DP分析使用了$f$-DP方法，并推导出了简单的闭式表达式，同时还考虑了群体隐私。实验证明，对于特定的工作时期和群体大小，框架具有$\sqrt{g E}$ 的差分隐私依赖关系。

    

    传统的差分隐私DP-SGD算法使用随机子采样实现个体剪切，从而强制实现小批量SGD方法。我们提供了一个通用的差分隐私算法框架，超越了DP-SGD算法，允许任何可能的一阶优化器（如经典的SGD和基于动量的SGD方法）与批量剪切相结合，它剪切了计算梯度的聚合结果，而不是对剪切梯度求和（个体剪切）。该框架还允许使用除随机子采样之外的采样技术，如打乱。我们的DP分析遵循$f$-DP的方法，并引入了一种新的证明技术，使我们能够推导出简单的闭式表达式，并对群体隐私进行分析。特别地，对于$E$个时期的工作和大小为$g$的群体，我们展示了具有打乱和批量剪切的$\sqrt{g E}$ DP依赖关系。

    Classical differential private DP-SGD implements individual clipping with random subsampling, which forces a mini-batch SGD approach. We provide a general differential private algorithmic framework that goes beyond DP-SGD and allows any possible first order optimizers (e.g., classical SGD and momentum based SGD approaches) in combination with batch clipping, which clips an aggregate of computed gradients rather than summing clipped gradients (as is done in individual clipping). The framework also admits sampling techniques beyond random subsampling such as shuffling. Our DP analysis follows the $f$-DP approach and introduces a new proof technique which allows us to derive simple closed form expressions and to also analyse group privacy. In particular, for $E$ epochs work and groups of size $g$, we show a $\sqrt{g E}$ DP dependency for batch clipping with shuffling.
    
[^144]: FedTracker：为联邦学习模型提供所有权验证和可追溯性的保护机制

    FedTracker: Furnishing Ownership Verification and Traceability for Federated Learning Model. (arXiv:2211.07160v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2211.07160](http://arxiv.org/abs/2211.07160)

    FedTracker是第一个为联邦学习模型提供所有权验证和追溯性的保护框架，采用双层保护方案，并利用持续学习原则提高保护性能。

    

    联邦学习（FL）是一种分布式机器学习范式，允许多个客户端协同训练一个全局模型，而无需共享他们的本地数据。然而，FL需要将模型暴露给各种参与者，这可能导致恶意客户端未经授权地分发或转售模型，从而损害FL团队的知识产权。为了阻止这种不当行为，建立一种验证模型所有权并追溯泄露者的机制至关重要。本文提出了FedTracker，这是第一个提供所有权验证和可追溯性的FL模型保护框架。FedTracker采用双层保护方案，包括全局水印机制和本地指纹机制。前者用于验证全局模型的所有权，而后者用于识别该模型来自哪个客户端。FedTracker利用持续学习（CL）原则来提高模型的保护性能。

    Federated learning (FL) is a distributed machine learning paradigm allowing multiple clients to collaboratively train a global model without sharing their local data. However, FL entails exposing the model to various participants. This poses a risk of unauthorized model distribution or resale by the malicious client, compromising the intellectual property rights of the FL group. To deter such misbehavior, it is essential to establish a mechanism for verifying the ownership of the model and as well tracing its origin to the leaker among the FL participants. In this paper, we present FedTracker, the first FL model protection framework that provides both ownership verification and traceability. FedTracker adopts a bi-level protection scheme consisting of global watermark mechanism and local fingerprint mechanism. The former authenticates the ownership of the global model, while the latter identifies which client the model is derived from. FedTracker leverages Continual Learning (CL) princ
    
[^145]: TEFL: 用于6G可信零触碰网络切片的Turbo可解释联邦学习

    TEFL: Turbo Explainable Federated Learning for 6G Trustworthy Zero-Touch Network Slicing. (arXiv:2210.10147v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2210.10147](http://arxiv.org/abs/2210.10147)

    本文提出了一种TEFL (Turbo Explainable Federated Learning) 方法，通过可解释的联邦学习，实现了6G可信零触碰网络切片的透明性和SLA感知的零触碰服务管理。

    

    第六代（6G）网络期望智能地支持大量协同存在的、与各种垂直应用案例相关的异构切片。这种背景促使采用人工智能（AI）驱动的零触碰管理和编排（MANO），以满足严格的服务级别协议（SLAs）下的端到端（E2E）切片需求。具体来说，通过可解释的AI（XAI）工具实现AI黑盒在实际部署中的可信度，以建立切片生态系统中交互参与者（如租户、基础设施提供商和运营商）之间的透明度。受Turbo原理启发，本文提出了一种新颖的迭代式可解释联邦学习（FL）方法，其中包括一个受限资源分配模型和一个解释器交换，以实现对特征的透明性和SLA感知的零触碰服务管理（ZSM）。

    Sixth-generation (6G) networks anticipate intelligently supporting a massive number of coexisting and heterogeneous slices associated with various vertical use cases. Such a context urges the adoption of artificial intelligence (AI)-driven zero-touch management and orchestration (MANO) of the end-to-end (E2E) slices under stringent service level agreements (SLAs). Specifically, the trustworthiness of the AI black-boxes in real deployment can be achieved by explainable AI (XAI) tools to build transparency between the interacting actors in the slicing ecosystem, such as tenants, infrastructure providers and operators. Inspired by the turbo principle, this paper presents a novel iterative explainable federated learning (FL) approach where a constrained resource allocation model and an \emph{explainer} exchange -- in a closed loop (CL) fashion -- soft attributions of the features as well as inference predictions to achieve a transparent and SLA-aware zero-touch service management (ZSM) of 
    
[^146]: 自监督训练的视频预训练产生与人类对齐的视觉表示

    Self-supervised video pretraining yields human-aligned visual representations. (arXiv:2210.06433v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.06433](http://arxiv.org/abs/2210.06433)

    本论文通过自监督训练的视频预训练方法VITO得到了具有人类感知特征的视觉表示，该方法在图像理解和视频理解任务上表现出更好的泛化性和鲁棒性。

    

    人类通过观察对象和场景随时间演变的方式学习到了强大的表示。然而，在不需要明确的时间理解的特定任务之外，静态图像预训练仍然是学习视觉基础模型的主流范式。我们对这种不匹配提出了质疑，并且问是否视频预训练可以产生具有人类感知特征的视觉表示：在各种任务中的泛化性、对扰动的鲁棒性和与人类判断的一致性。为此，我们提出了一种用于筛选视频的新颖程序，并开发了一个对比性框架，从其中的复杂转换中学习。这种从视频中提炼知识的简单范式被称为VITO，它产生的一般表示在图像理解任务上远远优于先前的视频预训练方法，并且在视频理解任务上优于图像预训练方法。此外，VITO表示对自然和合成形变的鲁棒性显著提高。

    Humans learn powerful representations of objects and scenes by observing how they evolve over time. Yet, outside of specific tasks that require explicit temporal understanding, static image pretraining remains the dominant paradigm for learning visual foundation models. We question this mismatch, and ask whether video pretraining can yield visual representations that bear the hallmarks of human perception: generalisation across tasks, robustness to perturbations, and consistency with human judgements. To that end we propose a novel procedure for curating videos, and develop a contrastive framework which learns from the complex transformations therein. This simple paradigm for distilling knowledge from videos, called VITO, yields general representations that far outperform prior video pretraining methods on image understanding tasks, and image pretraining methods on video understanding tasks. Moreover, VITO representations are significantly more robust to natural and synthetic deformati
    
[^147]: 基于需求学习和公平资源消耗平衡的网络收益管理

    Network Revenue Management with Demand Learning and Fair Resource-Consumption Balancing. (arXiv:2207.11159v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.11159](http://arxiv.org/abs/2207.11159)

    本文研究了基于价格的网络收益管理问题，同时考虑了需求学习和公平资源消耗平衡。提出了一种基于UCB需求学习方法的原始对偶在线策略来最大化规范化收益。

    

    除了最大化总收益外，很多行业的决策者还希望确保不同资源之间消耗的平衡。例如，在零售行业中，确保来自不同供应商的资源平衡消耗有助于提高公平性并维持良好的渠道关系；在云计算行业中，资源消耗的平衡有助于提高客户满意度并降低运营成本。针对这些实际需求，本文研究了基于价格的网络收益管理问题，同时考虑了需求学习和公平资源消耗平衡。我们引入了规范化收益的概念，即通过平衡正则化将公平的资源消耗平衡纳入到收益最大化的目标中。我们提出了一种基于上置信界限（UCB）需求学习方法的原始对偶在线策略来最大化规范化收益。我们采用了几个创新方法来应对需求学习和资源消耗平衡的挑战。

    In addition to maximizing the total revenue, decision-makers in lots of industries would like to guarantee balanced consumption across different resources. For instance, in the retailing industry, ensuring a balanced consumption of resources from different suppliers enhances fairness and helps main a good channel relationship; in the cloud computing industry, resource-consumption balance helps increase customer satisfaction and reduce operational costs. Motivated by these practical needs, this paper studies the price-based network revenue management (NRM) problem with both demand learning and fair resource-consumption balancing. We introduce the regularized revenue, i.e., the total revenue with a balancing regularization, as our objective to incorporate fair resource-consumption balancing into the revenue maximization goal. We propose a primal-dual-type online policy with the Upper-Confidence-Bound (UCB) demand learning method to maximize the regularized revenue. We adopt several innov
    
[^148]: 元元反游戏学习组合学习行为

    Meta-Referential Games to Learn Compositional Learning Behaviours. (arXiv:2207.08012v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2207.08012](http://arxiv.org/abs/2207.08012)

    本论文提出了一种元元反游戏学习的方法来解决组合学习行为的问题，通过解决绑定问题来支持人工智能代理展示组合学习行为的能力。

    

    人类利用组合性从过去的经验中推广到新颖的经验。我们假设我们的经验可以分解为基本的原子组件，这些组件可以以新颖的方式重新组合，以支持我们参与新颖经验的能力。我们将这视为学习以组合方式泛化的能力，并将利用这种能力的行为称为组合学习行为（CLBs）。学习CLBs的一个核心问题是解决绑定问题（BP）。尽管这是人类轻松完成的智能壮举，但对于现有技术的人工智能代理来说并非如此。因此，为了构建能够与人类合作的人工智能代理，我们建议开发一个新的基准来研究代理商通过解决BP的领域无关版本来展示CLBs的能力。我们受到指代游戏的语言涌现和基础架构框架的启发，提出了一个元学习扩展方案

    Human beings use compositionality to generalise from past experiences to novel experiences. We assume a separation of our experiences into fundamental atomic components that can be recombined in novel ways to support our ability to engage with novel experiences. We frame this as the ability to learn to generalise compositionally, and we will refer to behaviours making use of this ability as compositional learning behaviours (CLBs). A central problem to learning CLBs is the resolution of a binding problem (BP). While it is another feat of intelligence that human beings perform with ease, it is not the case for state-of-the-art artificial agents. Thus, in order to build artificial agents able to collaborate with human beings, we propose to develop a novel benchmark to investigate agents' abilities to exhibit CLBs by solving a domain-agnostic version of the BP. We take inspiration from the language emergence and grounding framework of referential games and propose a meta-learning extensio
    
[^149]: 使用路径相关的神经跳跃ODE对通用动力学进行最优估计

    Optimal Estimation of Generic Dynamics by Path-Dependent Neural Jump ODEs. (arXiv:2206.14284v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.14284](http://arxiv.org/abs/2206.14284)

    本文研究了使用路径相关的神经跳跃ODE对通用动力学进行最优估计的问题，并通过实证研究支持了这些理论结果，展示了其在非马尔可夫数据和限价订单簿数据方面的优势。

    

    本文研究了使用神经跳跃ODE（NJ-ODE）框架的路径相关扩展来预测一般随机过程的问题。虽然NJ-ODE是第一个建立起针对不规则观测时间序列预测的收敛性保证的框架，但这些结果仅适用于来自具有完整观测的It\^o扩散的数据，特别是所有坐标同时观测到的马尔可夫过程。在本研究中，我们通过利用签名变换的重构性质将这些结果推广到通用的、可能是非马尔可夫或不连续的随机过程，并通过实证研究支持了这些理论结果，在非马尔可夫数据的情况下，路径相关的NJ-ODE优于原始NJ-ODE框架。此外，我们还展示了PD-NJ-ODE可以成功应用于经典的随机滤波问题和限价订单簿（LOB）数据。

    This paper studies the problem of forecasting general stochastic processes using a path-dependent extension of the Neural Jump ODE (NJ-ODE) framework. While NJ-ODE was the first framework to establish convergence guarantees for the prediction of irregularly observed time series, these results were limited to data stemming from It\^o-diffusions with complete observations, in particular Markov processes where all coordinates are observed simultaneously. In this work, we generalise these results to generic, possibly non-Markovian or discontinuous, stochastic processes with incomplete observations, by utilising the reconstruction properties of the signature transform. These theoretical results are supported by empirical studies, where it is shown that the path-dependent NJ-ODE outperforms the original NJ-ODE framework in the case of non-Markovian data. Moreover, we show that PD-NJ-ODE can be applied successfully to classical stochastic filtering problems and to limit order book (LOB) data.
    
[^150]: 深度强化学习辅助联邦学习在电力批发市场中的鲁棒短期用电需求预测

    Deep Reinforcement Learning-Assisted Federated Learning for Robust Short-term Utility Demand Forecasting in Electricity Wholesale Markets. (arXiv:2206.11715v2 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2206.11715](http://arxiv.org/abs/2206.11715)

    本文提出了一种深度强化学习辅助联邦学习的方法，DEfect-AwaRe联邦软演员-评论家（DearFSAC），用于在电力批发市场中鲁棒地训练短期负荷预测模型，从而预测精确的短期公用电需求。

    

    短期负荷预测在电力交易市场的运作中起着重要作用。考虑到数据隐私保护的日益关注，最近的研究中越来越多地采用联邦学习来为公用事业公司（UCs）训练短期负荷预测模型。在批发市场中，由于电厂无法直接访问UCs的数据，采用联邦学习是一种获取准确的短期负荷预测模型的可行解决方案。然而，由于联邦学习的分布性和UCs之间的激烈竞争，缺陷日益增多，导致短期负荷预测模型性能较差，仅仅采用联邦学习是不够的。本文提出了一种名为DEfect-AwaRe联邦软演员-评论家（DearFSAC）的DRL辅助联邦学习方法，以鲁棒地训练一个准确的短期负荷预测模型，用于预测精确的短期公用电需求。首先，我们基于长短期记忆（LSTM）设计了一个短期负荷预测模型，仅使用历史负荷数据和时间数据。

    Short-term load forecasting (STLF) plays a significant role in the operation of electricity trading markets. Considering the growing concern of data privacy, federated learning (FL) is increasingly adopted to train STLF models for utility companies (UCs) in recent research. Inspiringly, in wholesale markets, as it is not realistic for power plants (PPs) to access UCs' data directly, FL is definitely a feasible solution of obtaining an accurate STLF model for PPs. However, due to FL's distributed nature and intense competition among UCs, defects increasingly occur and lead to poor performance of the STLF model, indicating that simply adopting FL is not enough. In this paper, we propose a DRL-assisted FL approach, DEfect-AwaRe federated soft actor-critic (DearFSAC), to robustly train an accurate STLF model for PPs to forecast precise short-term utility electricity demand. Firstly. we design a STLF model based on long short-term memory (LSTM) using just historical load data and time data.
    
[^151]: ROI：一种用于识别接收个人数据的组织的方法

    ROI: A method for identifying organizations receiving personal data. (arXiv:2204.09495v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2204.09495](http://arxiv.org/abs/2204.09495)

    本论文提出了一种名为ROI的方法，可自动识别接收个人数据的组织，并通过评估10,000个安卓应用揭示了这些组织。该方法实现了95.71%的准确率。

    

    许多研究揭示了通过网站、移动应用或智能设备等方式在数字生态系统中对个人数据的大规模收集。大多数用户都没有意识到这一事实，并且也未意识到这些收集者与全球范围内许多不同的组织共享他们的个人数据。本文评估了现有技术中识别接收个人数据的组织的方法。基于我们的研究结果，我们提出了ROI（接收组织识别器），这是一种完全自动化的方法，结合了不同的技术来实现对接收个人数据的组织进行准确率达到95.71%的识别。我们通过评估10,000个安卓应用并揭示接收用户个人数据的组织来展示我们的方法。

    Many studies have exposed the massive collection of personal data in the digital ecosystem through, for instance, websites, mobile apps, or smart devices. This fact goes unnoticed by most users, who are also unaware that the collectors are sharing their personal data with many different organizations around the globe. This paper assesses techniques available in the state of the art to identify the organizations receiving this personal data. Based on our findings, we propose ROI (Receiver Organization Identifier), a fully automated method that combines different techniques to achieve a 95.71% precision score in identifying an organization receiving personal data. We demonstrate our method in the wild by evaluating 10,000 Android apps and exposing the organizations that receive users' personal data.
    
[^152]: 实时神经网络模型预测控制：面向四旋翼和灵活机器人平台的深度学习模型预测控制系统

    Real-time Neural-MPC: Deep Learning Model Predictive Control for Quadrotors and Agile Robotic Platforms. (arXiv:2203.07747v4 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2203.07747](http://arxiv.org/abs/2203.07747)

    本论文介绍了一种实时神经网络模型预测控制系统，在模型预测控制流程中高效集成复杂神经网络架构作为动力学模型，实现对四旋翼等灵活机器人平台高性能的控制。

    

    模型预测控制(MPC)已成为高性能自主系统嵌入式控制的流行框架。然而，要使用MPC实现良好的控制性能，精确的动力学模型是关键。为了保持实时操作，嵌入式系统上使用的动力学模型一般限于简单的基于第一原理的模型，这显著限制了它们的表示能力。与这种简单模型相反的是，机器学习方法，特别是神经网络，已经被证明可以准确地建模复杂的动态效应，但它们的巨大计算复杂性阻碍了它们与快速实时迭代循环的结合。本文提出了实时神经MPC框架，用于在模型预测控制流程中高效地集成大型复杂神经网络架构作为动力学模型。我们在模拟和高度灵活的四旋翼平台上进行了实验，证明了所描述系统的能力，展示了其能够实现基于深度神经网络的学习动力学模型在实时控制中高性能的控制能力。

    Model Predictive Control (MPC) has become a popular framework in embedded control for high-performance autonomous systems. However, to achieve good control performance using MPC, an accurate dynamics model is key. To maintain real-time operation, the dynamics models used on embedded systems have been limited to simple first-principle models, which substantially limits their representative power. In contrast to such simple models, machine learning approaches, specifically neural networks, have been shown to accurately model even complex dynamic effects, but their large computational complexity hindered combination with fast real-time iteration loops. With this work, we present Real-time Neural MPC, a framework to efficiently integrate large, complex neural network architectures as dynamics models within a model-predictive control pipeline. Our experiments, performed in simulation and the real world onboard a highly agile quadrotor platform, demonstrate the capabilities of the described 
    
[^153]: 大规模推荐系统中的贝叶斯非平稳线性赌臂

    Bayesian Non-stationary Linear Bandits for Large-Scale Recommender Systems. (arXiv:2202.03167v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.03167](http://arxiv.org/abs/2202.03167)

    本文提出了一种基于贝叶斯方法的非平稳线性赌臂算法，用于处理大规模推荐系统中的高维上下文信息。通过使用随机投影和指数增长的权重，该算法能够适应非平稳环境中的动态变化，并取得了较好的性能。

    

    充分利用上下文信息可能会提高推荐系统的性能。在大数据时代，这种附加信息通常具有多个维度。因此，开发能够实时处理这种高维上下文的决策算法非常重要。当决策者需要推荐多种物品时，这具有特殊的挑战性。此外，物品的流行度或用户的偏好变化可能会由于环境中分布变化的鲁棒性不足而影响已部署推荐系统的性能。在本文中，我们在线性上下文多臂赌博机框架的基础上解决了这个问题。我们针对高维特征向量、大量臂和非平稳生成奖励的问题，开发了一种基于汤普森抽样的决策策略。我们的策略通过随机投影减少了特征向量的维度，并使用了指数增长的权重来适应非平稳性。

    Taking advantage of contextual information can potentially boost the performance of recommender systems. In the era of big data, such side information often has several dimensions. Thus, developing decision-making algorithms to cope with such a high-dimensional context in real time is essential. That is specifically challenging when the decision-maker has a variety of items to recommend. In addition, changes in items' popularity or users' preferences can hinder the performance of the deployed recommender system due to a lack of robustness to distribution shifts in the environment. In this paper, we build upon the linear contextual multi-armed bandit framework to address this problem. We develop a decision-making policy for a linear bandit problem with high-dimensional feature vectors, a large set of arms, and non-stationary reward-generating processes. Our Thompson sampling-based policy reduces the dimension of feature vectors using random projection and uses exponentially increasing w
    
[^154]: 强化学习中的张量和矩阵低秩值函数近似

    Tensor and Matrix Low-Rank Value-Function Approximation in Reinforcement Learning. (arXiv:2201.09736v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.09736](http://arxiv.org/abs/2201.09736)

    本文提出了一种在高维空间中使用随机低秩算法进行价值函数近似的方法，并提出了使用张量表示和PARAFAC分解的在线无模型的张量低秩算法。

    

    价值函数（VF）的近似是强化学习中的一个核心问题。传统的非参数VF估计在维度灾难的情况下存在问题。因此，在高维空间中，人们采用了简洁的参数模型来近似VF，其中大部分工作集中在线性和神经网络方法上。与此不同的是，本文提出了一种“简洁的非参数”方法，我们使用随机低秩算法以在线和无模型的方式来估计VF矩阵。此外，由于VF往往是多维的，我们提出用张量（多维数组）表示来替代传统的VF矩阵表示，并采用PARAFAC分解来设计一个在线无模型的张量低秩算法。我们提出了不同版本的算法，分析了它们的复杂度，并通过使用标准强化学习环境对其性能进行了数值评估。

    Value-function (VF) approximation is a central problem in Reinforcement Learning (RL). Classical non-parametric VF estimation suffers from the curse of dimensionality. As a result, parsimonious parametric models have been adopted to approximate VFs in high-dimensional spaces, with most efforts being focused on linear and neural-network-based approaches. Differently, this paper puts forth a a \emph{parsimonious non-parametric} approach, where we use \emph{stochastic low-rank algorithms} to estimate the VF matrix in an online and model-free fashion. Furthermore, as VFs tend to be multi-dimensional, we propose replacing the classical VF matrix representation with a tensor (multi-way array) representation and, then, use the PARAFAC decomposition to design an online model-free tensor low-rank algorithm. Different versions of the algorithms are proposed, their complexity is analyzed, and their performance is assessed numerically using standardized RL environments.
    
[^155]: 贝叶斯最优臂识别中的最优简单遗憾

    Optimal Simple Regret in Bayesian Best Arm Identification. (arXiv:2111.09885v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.09885](http://arxiv.org/abs/2111.09885)

    该论文研究了多臂赌博机问题中贝叶斯最优臂识别的速率，并提出了一种简单易行的算法，其匹配了下界，只差一个常数因子。

    

    我们考虑多臂赌博机问题中的最优臂识别。在先验条件具有一定的连续性的情况下，我们表征了贝叶斯简单遗憾的速率。与贝叶斯遗憾最小化不同，贝叶斯简单遗憾的主导项来源于最优臂和次优臂之间间隙小于$\sqrt{\frac{\log T}{T}}$的区域。我们提出了一种简单易行的计算算法，其主导项匹配了下界，只差一个常数因子；模拟结果支持了我们的理论发现。

    We consider best arm identification in the multi-armed bandit problem. Assuming certain continuity conditions of the prior, we characterize the rate of the Bayesian simple regret. Differing from Bayesian regret minimization (Lai, 1987), the leading term in the Bayesian simple regret derives from the region where the gap between optimal and suboptimal arms is smaller than $\sqrt{\frac{\log T}{T}}$. We propose a simple and easy-to-compute algorithm with its leading term matching with the lower bound up to a constant factor; simulation results support our theoretical findings.
    
[^156]: 一种在众包单标签情感分析中端到端的注释者偏差近似方法

    End-to-End Annotator Bias Approximation on Crowdsourced Single-Label Sentiment Analysis. (arXiv:2111.02326v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2111.02326](http://arxiv.org/abs/2111.02326)

    本文提出一种在众包单标签情感分析中解决注释者偏差的端到端方法，通过精确的偏差建模和真实值估计来改善准确性，实验证明在样本只由单个注释者标注的情况下效果显著。

    

    情感分析通常是一个容易受到众多注释者主观标签影响的众包任务。目前尚不完全了解如何使用最先进的方法正确地建模每个注释者的注释偏差。然而，准确可靠地解决注释者偏差是理解注释者标注行为并成功解决相应的个体误解和错误的关键。我们的贡献是对精确的端到端偏差建模和真实值估计进行解释和改进，从而减少现有先进方法中涉及的不希望出现的不匹配问题。分类实验表明，该方法有潜力提高仅由单个注释者标注的样本准确性。我们公开提供整个源代码，并发布一个包含讨论有机食品产品的10,000个句子的领域特定情感数据集，这些句子是从社交媒体抓取而来。

    Sentiment analysis is often a crowdsourcing task prone to subjective labels given by many annotators. It is not yet fully understood how the annotation bias of each annotator can be modeled correctly with state-of-the-art methods. However, resolving annotator bias precisely and reliably is the key to understand annotators' labeling behavior and to successfully resolve corresponding individual misconceptions and wrongdoings regarding the annotation task. Our contribution is an explanation and improvement for precise neural end-to-end bias modeling and ground truth estimation, which reduces an undesired mismatch in that regard of the existing state-of-the-art. Classification experiments show that it has potential to improve accuracy in cases where each sample is annotated only by one single annotator. We provide the whole source code publicly and release an own domain-specific sentiment dataset containing 10,000 sentences discussing organic food products. These are crawled from social me
    
[^157]: 社交媒体中消费者信念陈述的分类

    Classification of Consumer Belief Statements From Social Media. (arXiv:2106.15498v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.15498](http://arxiv.org/abs/2106.15498)

    本研究探讨了使用复杂的专家注解在社交媒体中进行消费者信念陈述分类的准确性，比较了细粒度和抽象类别的标签，并说明复杂专家注解在高度特定的意见挖掘中的潜在优势。

    

    社交媒体提供了大量信息，可以进行市场调研，以满足客户的需求。研究人员通常通过收集和分类用户生成的内容，构建复杂细粒度的类别结构来进行市场调研。然而，在许多情况下，数据量较少且注解复杂。如何成功利用这些数据进行分类仍不完全清楚。本研究考察了当专家注解被应用于a) 许多细粒度类别和b) 少数抽象类别时的分类准确性。对于场景b)，我们比较了领域专家给出的抽象类别标签（基准）和自动分层聚类给出的抽象类别标签。我们将其与另一基准进行比较，该基准使用完全无监督的聚类方法给出整个类别结构。通过这样做，该研究可以作为复杂专家注解如何在高度特定的意见挖掘中发挥潜在优势，并以最优化的方式利用的示例。

    Social media offer plenty of information to perform market research in order to meet the requirements of customers. One way how this research is conducted is that a domain expert gathers and categorizes user-generated content into a complex and fine-grained class structure. In many of such cases, little data meets complex annotations. It is not yet fully understood how this can be leveraged successfully for classification. We examine the classification accuracy of expert labels when used with a) many fine-grained classes and b) few abstract classes. For scenario b) we compare abstract class labels given by the domain expert as baseline and by automatic hierarchical clustering. We compare this to another baseline where the entire class structure is given by a completely unsupervised clustering approach. By doing so, this work can serve as an example of how complex expert annotations are potentially beneficial and can be utilized in the most optimal way for opinion mining in highly speci
    
[^158]: 噪声低秩矩阵恢复的几何分析在精确参数化和过度参数化区间中的应用

    Geometric Analysis of Noisy Low-rank Matrix Recovery in the Exact Parameterized and the Overparameterized Regimes. (arXiv:2105.08232v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2105.08232](http://arxiv.org/abs/2105.08232)

    本文研究了噪声低秩矩阵恢复问题，提出了鲁棒错误定位几何分析算法和连续子空间优化算法，分别用于精确参数化和过度参数化的情况。通过约束等异性性质，我们提供了对全局最优解与局部解之间的最大距离的保证。

    

    矩阵感知问题是一种重要的低秩优化问题，在矩阵补全、相位同步/恢复、稳健PCA和电力系统状态估计等领域都有广泛应用。本文研究了通过线性测量损坏的噪声低秩矩阵感知问题。我们考虑了搜索秩r等于未知真实秩r*的情况（精确参数化情况），以及r大于r*的情况（过度参数化情况）。我们量化了约束等异性性质（restricted isometry property，RIP）在塑造非凸分解公式的整体景观和帮助局部搜索算法成功方面的作用。首先，我们在RIP常数小于 1/(1+sqrt(r*/r))的假设下，对非凸问题的任意局部极小值和真实值之间的最大距离进行了全局保证。然后，我们提出了一种新颖的方法，称为鲁棒错误定位几何分析（Robust Error-Locating Geometric Analysis，RELGA）算法，用于实现在存在噪声的情况下的精确低秩矩阵恢复。RELGA算法通过组合错误定位机制和几何分析，提供了理论保证，即使在噪声水平相对较大的情况下，也可以实现精确的矩阵恢复。对于过度参数化情况，我们提出了一种局部搜索算法，称为连续子空间优化（Successive Subspace Optimization，SSO）算法，在噪声水平和RIP常数的一定条件下，可以收敛到真实解。我们的分析揭示了SSO的成功取决于初始化、非退化性和几何条件的组合。

    The matrix sensing problem is an important low-rank optimization problem that has found a wide range of applications, such as matrix completion, phase synchornization/retrieval, robust PCA, and power system state estimation. In this work, we focus on the general matrix sensing problem with linear measurements that are corrupted by random noise. We investigate the scenario where the search rank $r$ is equal to the true rank $r^*$ of the unknown ground truth (the exact parametrized case), as well as the scenario where $r$ is greater than $r^*$ (the overparametrized case). We quantify the role of the restricted isometry property (RIP) in shaping the landscape of the non-convex factorized formulation and assisting with the success of local search algorithms. First, we develop a global guarantee on the maximum distance between an arbitrary local minimizer of the non-convex problem and the ground truth under the assumption that the RIP constant is smaller than $1/(1+\sqrt{r^*/r})$. We then p
    
[^159]: 用于加速非稳态计算流体力学的有限体积方法网络：非反应和反应流动

    Finite volume method network for acceleration of unsteady computational fluid dynamics: non-reacting and reacting flows. (arXiv:2105.03332v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2105.03332](http://arxiv.org/abs/2105.03332)

    本研究开发了一种引入有限体积方法和物理信息损失函数的神经网络模型，用于加速非稳态计算流体力学模拟。该模型可以仅使用两个先前的场来预测未来的流场，而不需要大量的流场图像。此模型在非反应流和反应流模拟中获得了良好的性能。

    

    尽管中央处理器（CPU）的性能迅速提升，但使用计算流体力学（CFD）模拟化学反应流动的计算成本在许多情况下仍不可行。虽然已研究了将专门用于图像处理的卷积神经网络（CNN）应用于流场预测，但最近出现了为CFD设计适应的神经网络的需求。本研究开发了一种引入有限体积方法（FVM）的神经网络模型，具有独特的网络架构和物理信息损失函数，用于加速CFD模拟。开发的网络模型考虑了CFD流场的特性，其中所有网格都应用相同的控制方程，可以仅使用两个先前的场来预测未来的场，而不像CNN那样需要许多场图像（>10,000）。使用非反应流和反应流模拟的CFD时序数据评估了这个基准模型的性能；

    Despite rapid improvements in the performance of central processing unit (CPU), the calculation cost of simulating chemically reacting flow using CFD remains infeasible in many cases. The application of the convolutional neural networks (CNNs) specialized in image processing in flow field prediction has been studied, but the need to develop a neural netweork design fitted for CFD is recently emerged. In this study, a neural network model introducing the finite volume method (FVM) with a unique network architecture and physics-informed loss function was developed to accelerate CFD simulations. The developed network model, considering the nature of the CFD flow field where the identical governing equations are applied to all grids, can predict the future fields with only two previous fields unlike the CNNs requiring many field images (>10,000). The performance of this baseline model was evaluated using CFD time series data from non-reacting flow and reacting flow simulation; counterflow 
    
[^160]: 具有马尔科夫或隐马尔科夫信号先验的线性模型的复制分析

    Replica Analysis of the Linear Model with Markov or Hidden Markov Signal Priors. (arXiv:2009.13370v5 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2009.13370](http://arxiv.org/abs/2009.13370)

    本文使用复制方法估计了具有马尔科夫或隐马尔科夫信号先验的线性模型的自由能、平均互信息和最小均方误差（MMSE）。研究发现，在后验均值估计器下，线性模型可以分解为具有状态信息的单输入AWGN信道，而状态分布遵循马尔科夫链的随机矩阵的左Perron-Frobenius特征向量。数值结果证明，通过复制方法得到的结果与Metropolis-Hastings算法或其他近似传递算法的结果非常接近。

    

    本文基于统计物理中的复制方法，估计了在线性模型下的自由能、平均互信息和最小均方误差（MMSE），并假设了两个条件：（1）源由马尔科夫链生成，（2）源通过隐藏马尔科夫模型生成。我们的估计是基于后验均值估计器，表明具有马尔科夫源或隐藏马尔科夫源的线性模型可以分解为具有状态信息的单输入AWGN信道，其中状态分布遵循马尔科夫链的随机矩阵的左Perron-Frobenius特征向量，其曼哈顿范数为1。数值结果表明，通过复制方法获得的自由能和均方误差与研究文献中Metropolis-Hastings算法或一些众所周知的近似传递算法的对应结果非常接近。

    This paper estimates free energy, average mutual information, and minimum mean square error (MMSE) of a linear model under two assumptions: (1) the source is generated by a Markov chain, (2) the source is generated via a hidden Markov model. Our estimates are based on the replica method in statistical physics. We show that under the posterior mean estimator, the linear model with Markov sources or hidden Markov sources is decoupled into single-input AWGN channels with state information available at both encoder and decoder where the state distribution follows the left Perron-Frobenius eigenvector with unit Manhattan norm of the stochastic matrix of Markov chains. Numerical results show that the free energies and MSEs obtained via the replica method are closely approximate to their counterparts achieved by the Metropolis-Hastings algorithm or some well-known approximate message passing algorithms in the research literature.
    
[^161]: 具有类人类树突激活的非线性神经元

    Non-linear Neurons with Human-like Apical Dendrite Activations. (arXiv:2003.03229v4 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2003.03229](http://arxiv.org/abs/2003.03229)

    本论文提出了一种新的人工神经元模型和激活函数，通过使用单个神经元学习非线性决策边界，并在多个基准数据集上取得了优于传统方法的结果。

    

    为了对线性不可分的数据进行分类，通常将神经元组织成至少包含一个隐藏层的多层神经网络。受神经科学的一些最新发现的启发，我们提出了一种新的人工神经元模型和一种新颖的激活函数，可使用单个神经元学习非线性决策边界。我们展示了一个标准神经元接上我们的新型树突激活函数（ADA）可以以100%的准确率学习XOR逻辑函数。此外，我们对计算机视觉、信号处理和自然语言处理领域的六个基准数据集进行了实验，即MOROCO、UTKFace、CREMA-D、Fashion-MNIST、Tiny ImageNet和ImageNet，结果显示ADA和漏电ADA函数在各种神经网络结构（如一层或两层隐藏层的多层感知机和卷积神经网络）上优于修正线性单元（ReLU）、漏电ReLU、径向基函数（RBF）和Swish。

    In order to classify linearly non-separable data, neurons are typically organized into multi-layer neural networks that are equipped with at least one hidden layer. Inspired by some recent discoveries in neuroscience, we propose a new model of artificial neuron along with a novel activation function enabling the learning of nonlinear decision boundaries using a single neuron. We show that a standard neuron followed by our novel apical dendrite activation (ADA) can learn the XOR logical function with 100% accuracy. Furthermore, we conduct experiments on six benchmark data sets from computer vision, signal processing and natural language processing, i.e. MOROCO, UTKFace, CREMA-D, Fashion-MNIST, Tiny ImageNet and ImageNet, showing that the ADA and the leaky ADA functions provide superior results to Rectified Linear Units (ReLU), leaky ReLU, RBF and Swish, for various neural network architectures, e.g. one-hidden-layer or two-hidden-layer multi-layer perceptrons (MLPs) and convolutional ne
    
[^162]: 在紧致黎曼流形上的几何小波散射网络

    Geometric Wavelet Scattering Networks on Compact Riemannian Manifolds. (arXiv:1905.10448v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1905.10448](http://arxiv.org/abs/1905.10448)

    本论文在紧致黎曼流形上定义了一种几何散射变换，该变换类似于欧几里得散射变换，具有局部同构的不变性和某些类型的微分同胚的稳定性，实证结果证明了其在几何学习任务中的实用性。

    

    近十年前，欧几里得散射变换被引入以改善对卷积神经网络的数学理解。受到最近对几何深度学习的兴趣的启发，该学科旨在将卷积神经网络推广到流形和图结构域，我们在流形上定义了一种几何散射变换。类似于欧几里得散射变换，几何散射变换基于一系列小波滤波器和逐点非线性。它对局部同构具有不变性，对某些类型的微分同胚具有稳定性。实证结果证明了它在几何学习任务中的实用性。我们的结果推广了欧几里得散射的变形稳定性和局部平移不变性，并展示了将使用的滤波器结构与数据的基础几何联系起来的重要性。

    The Euclidean scattering transform was introduced nearly a decade ago to improve the mathematical understanding of convolutional neural networks. Inspired by recent interest in geometric deep learning, which aims to generalize convolutional neural networks to manifold and graph-structured domains, we define a geometric scattering transform on manifolds. Similar to the Euclidean scattering transform, the geometric scattering transform is based on a cascade of wavelet filters and pointwise nonlinearities. It is invariant to local isometries and stable to certain types of diffeomorphisms. Empirical results demonstrate its utility on several geometric learning tasks. Our results generalize the deformation stability and local translation invariance of Euclidean scattering, and demonstrate the importance of linking the used filter structures to the underlying geometry of the data.
    

