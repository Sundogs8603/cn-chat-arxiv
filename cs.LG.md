# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [GeNet: A Graph Neural Network-based Anti-noise Task-Oriented Semantic Communication Paradigm](https://arxiv.org/abs/2403.18296) | GeNet提出了一种基于图神经网络的语义通信范式，通过将数据转换为图结构、利用编码器提取语义信息并利用解码器重建信息的方法来实现抗噪声任务导向通信。 |
| [^2] | [Visual Whole-Body Control for Legged Loco-Manipulation](https://arxiv.org/abs/2403.16967) | 这项研究提出了一种利用视觉全身控制的框架，使腿式机器人能够同时控制腿部和手臂，以扩展操作能力，并通过仿真训练和Sim2Real转移实现了在捡起不同物体方面取得显著改进。 |
| [^3] | [Rethinking Adversarial Inverse Reinforcement Learning: From the Angles of Policy Imitation and Transferable Reward Recovery](https://arxiv.org/abs/2403.14593) | 重新思考对抗逆强化学习中的策略模仿和可转移奖励恢复，提出了一个混合框架PPO-AIRL + SAC以解决SAC算法在AIRL训练中无法全面解开奖励函数的问题。 |
| [^4] | [Adversarial Nibbler: An Open Red-Teaming Method for Identifying Diverse Harms in Text-to-Image Generation](https://arxiv.org/abs/2403.12075) | 批评了文本到图像生成中模型对非明显攻击的鲁棒性，提出了Adversarial Nibbler Challenge以众包多样化的提示来纠正模型的安全问题 |
| [^5] | [Efficient Pruning of Large Language Model with Adaptive Estimation Fusion](https://arxiv.org/abs/2403.10799) | 提出了一种简单而高效的剪枝方法，能够自适应地模拟每个子结构的重要性，并根据多层结构的结果自适应地融合粗粒度和细粒度的估计。 |
| [^6] | [A Short Survey on Importance Weighting for Machine Learning](https://arxiv.org/abs/2403.10175) | 重要性加权是统计学和机器学习中的基本程序，通过对目标函数或概率分布进行加权，可以保证监督学习在训练和测试分布之间差异的情况下具有统计上期望的性质 |
| [^7] | [A Framework for Strategic Discovery of Credible Neural Network Surrogate Models under Uncertainty](https://arxiv.org/abs/2403.08901) | 提出了一个名为Occam Plausibility Algorithm for surrogate models (OPAL-surrogate)的框架，通过基于神经网络的代理模型，采用层次贝叶斯推理和模型验证测试来评估代理模型的可信度和预测可靠性。 |
| [^8] | [Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs](https://arxiv.org/abs/2403.00858) | 通过提出的框架，我们训练了一种用于Llama 2 Chat 7B或更大模型的草案模型，实现了加速推理，仅占原始大小的1.64％。 |
| [^9] | [PiShield: A NeSy Framework for Learning with Requirements](https://arxiv.org/abs/2402.18285) | PiShield是第一个允许将需求集成到神经网络拓扑结构中的框架，无论输入如何都能确保满足这些需求，并可根据从业者需求在推断和/或训练时集成需求。 |
| [^10] | [Adaptive Federated Learning in Heterogeneous Wireless Networks with Independent Sampling](https://arxiv.org/abs/2402.10097) | 这项研究提出了一种适用于异构无线网络的自适应联邦学习方法，其中包括了独立客户端采样和带宽分配方案，以提高训练效率和适应数据和系统的异构特性。 |
| [^11] | [Criterion collapse and loss distribution control](https://arxiv.org/abs/2402.09802) | 该论文研究了"准则崩溃"的概念，即优化一个度量指标意味着另一个度量指标的最优性。研究结果发现，对于损失的伯努利分布，CVaR和DRO的结果远超出现有研究，同时发现了一些特定条件下，单调准则如倾斜ERM无法避免崩溃，而非单调的替代方案可以。 |
| [^12] | [Practitioners' Challenges and Perceptions of CI Build Failure Predictions at Atlassian](https://arxiv.org/abs/2402.09651) | Atlassian的研究调查了CI构建失败对软件开发过程和团队的影响，并研究了将CI构建预测工具集成到Bitbucket环境中所涉及的挑战和期望。 |
| [^13] | [A Generalized Approach to Online Convex Optimization](https://arxiv.org/abs/2402.08621) | 这是一篇关于在线凸优化的论文，作者分析了不同环境下的问题并提出了一种通用的解决方法，该方法可以转化为相应的线性优化算法，并可以在面对不同类型对手时获得可比较的遗憾界限。 |
| [^14] | [MOMENT: A Family of Open Time-series Foundation Models](https://arxiv.org/abs/2402.03885) | MOMENT是一个开放的时间序列基础模型家族，通过解决时间序列数据的挑战，编制了一个大规模的公共时间序列数据集，并设计了一个基准测试来评估有限监督场景下模型的性能。 |
| [^15] | [A Single Graph Convolution Is All You Need: Efficient Grayscale Image Classification](https://arxiv.org/abs/2402.00564) | 本论文提出了一种高效的灰度图像分类方法，通过将图像视为矢量，并使用单个图卷积层进行处理，提高了分类模型的准确性和稳定性。 |
| [^16] | [Efficient and Scalable Graph Generation through Iterative Local Expansion](https://arxiv.org/abs/2312.11529) | 通过逐步扩展单个节点到目标图的方法，避免了对所有节点对的整个联合分布进行建模，实现了高效可扩展的图生成，同时通过多尺度生成保持了高表达性。 |
| [^17] | [Conformer-Based Speech Recognition On Extreme Edge-Computing Devices](https://arxiv.org/abs/2312.10359) | 提出了在资源受限设备上实现基于Conformer的端到端流式ASR系统的一系列模型架构适配、神经网络图变换和数值优化方法，实现了超过5.26倍实时速度的语音识别，同时最大限度减少能耗并实现了最先进的准确性。 |
| [^18] | [On the Second-Order Convergence of Biased Policy Gradient Algorithms](https://arxiv.org/abs/2311.02546) | 该论文研究了偏置策略梯度算法的二阶收敛性，包括基于蒙特卡洛轨迹采样的普通梯度估计器和基于双循环评论家-演员算法的演员-评论家方法。实现在实际应用中的偏置主要来自于有限时间采样和对价值函数的逼近。 |
| [^19] | [A structured regression approach for evaluating model performance across intersectional subgroups.](http://arxiv.org/abs/2401.14893) | 这项工作介绍了一种结构回归方法，用于评估模型在不同交叉子群体间的性能。它可以提供可靠的系统性能估计，即使对于很小的子群体。 |
| [^20] | [Generating Synthetic Health Sensor Data for Privacy-Preserving Wearable Stress Detection.](http://arxiv.org/abs/2401.13327) | 本论文介绍了一种隐私保护的合成健康传感器数据的方法，通过生成对抗网络（GANs）和差分隐私（DP）防护，生成与压力时刻相关的合成序列数据，确保患者信息的保护，并对合成数据进行质量评估。在压力检测数据集上验证了该方法的有效性。 |
| [^21] | [Sample Relationship from Learning Dynamics Matters for Generalisation.](http://arxiv.org/abs/2401.08808) | 这项研究从近似样本之间的相互作用开始，揭示了标签对样本之间的影响，提出了带标签的伪神经切向核 (lpNTK)，并探讨了lpNTK如何帮助理解学习现象。 |
| [^22] | [TAnet: A New Temporal Attention Network for EEG-based Auditory Spatial Attention Decoding with a Short Decision Window.](http://arxiv.org/abs/2401.05819) | TAnet是一种新的基于脑电信号的听觉空间注意力解码网络，采用了多头注意力机制，可以更有效地捕捉脑电信号中时间步之间的交互作用，并提供了比传统方法更好的解码性能。 |
| [^23] | [State Derivative Normalization for Continuous-Time Deep Neural Networks.](http://arxiv.org/abs/2401.02902) | 本文研究了在连续时间状态空间模型估计中，深度神经网络的数据标准化问题。通过引入状态导数级别的标准化常数，解决了隐藏状态、隐藏状态导数以及时间间隔的标准化挑战。选择适当的标准化常数与待识别系统的动力学相关，并提出了多种获得有效标准化常数的方法。 |
| [^24] | [A Foundation Graph Model.](http://arxiv.org/abs/2311.03976) | 本文提出了一个基于对抗性对比学习的基础图模型FoToM，该模型通过节点和边特征排除进行图预训练，在多个领域上实现了正向迁移，并取得了显著的性能提升。 |
| [^25] | [Non-parametric regression for robot learning on manifolds.](http://arxiv.org/abs/2310.19561) | 本文提出了一种在机器人学习中针对流形值数据的非参数回归方法，通过在流形上操作概率分布参数来直接估计函数，以改善预测准确性和简化算法。 |
| [^26] | [A Spectral Condition for Feature Learning.](http://arxiv.org/abs/2310.17813) | 本文研究了在大型神经网络中特征学习的光谱条件，并提出了将权重矩阵和更新的谱范数缩放为$\sqrt{\texttt{fan-out}/\texttt{fan-in}}$的方法，以实现特征学习。同时，作者还导出了最大更新参数化的推导，旨在帮助读者对神经网络中的特征学习理解更加深入。 |
| [^27] | [Equipping Federated Graph Neural Networks with Structure-aware Group Fairness.](http://arxiv.org/abs/2310.12350) | 本论文提出了一种名为F2GNN的方法，它旨在增强联邦图神经网络的群体公平性，解决了在联邦学习中减轻偏见的新挑战。 |
| [^28] | [Graph Condensation via Eigenbasis Matching.](http://arxiv.org/abs/2310.09202) | 本论文提出了基于特征匹配的无谱图压缩方法，以解决现有方法在泛化能力上存在的问题。通过详细分析发现，GNN注入合成图中的谱偏差导致了性能差异，我们的方法可以缓解这个问题。 |
| [^29] | [Anytime-valid t-tests and confidence sequences for Gaussian means with unknown variance.](http://arxiv.org/abs/2310.03722) | 本文提出了两种新的“e-process”和置信序列方法，分别通过替换Lai的混合方法，并分析了所得结果的宽度。 |
| [^30] | [RealFill: Reference-Driven Generation for Authentic Image Completion.](http://arxiv.org/abs/2309.16668) | RealFill是一种个性化的生成修填模型，通过使用少量目标场景的参考图像，能够以真实、高质量、逼真的内容完成目标图像的修复。 |
| [^31] | [Language Models as Black-Box Optimizers for Vision-Language Models.](http://arxiv.org/abs/2309.05950) | 本论文介绍了一种新的视觉-语言模型 (VLMs) 微调方法，通过自然语言提示来避免访问模型参数，采用聊天式的语言模型作为黑盒优化器，在少样本图像分类任务中达到效果。 |
| [^32] | [DECODE: DilatEd COnvolutional neural network for Detecting Extreme-mass-ratio inspirals.](http://arxiv.org/abs/2308.16422) | 该论文介绍了一种名为DECODE的扩张卷积神经网络模型，用于检测极端质量比激发的信号。该模型通过在频域进行序列建模，并考虑时间延迟干涉仪以处理多通道TDI数据。 |
| [^33] | [A multi-modal representation of El Ni\~no Southern Oscillation Diversity.](http://arxiv.org/abs/2307.11552) | 通过使用低维表示，在El Ni\~no Southern Oscillation（ENSO）中发现了新的极端El Ni\~no类别，并发现它们与典型EP El Ni\~no不同。EP El Ni\~nos，CP La Ni\~nas和Extreme El Ni\~nos对跨十年尺度的ENSO最具影响力。 |
| [^34] | [A Deep Learning Approach for Overall Survival Analysis with Missing Values.](http://arxiv.org/abs/2307.11465) | 提出了一个深度学习模型，通过有效利用被审查和未被审查病人的信息，预测非小细胞肺癌（NSCLC）病人的整体生存。 |
| [^35] | [Convergence Guarantees for Stochastic Subgradient Methods in Nonsmooth Nonconvex Optimization.](http://arxiv.org/abs/2307.10053) | 本文研究了非平滑非凸优化中随机次梯度方法的收敛性质，并提出了一种新的框架，证明了其在单时间尺度和双时间尺度情况下的全局收敛性，包括了多种已知的SGD类型方法。对于有限和形式的目标函数，证明了这些方法能够在随机选择的步长和初始点上找到Clarke稳定点。 |
| [^36] | [Locally Adaptive Federated Learning via Stochastic Polyak Stepsizes.](http://arxiv.org/abs/2307.06306) | 本文将随机Polyak步长方法扩展到联邦学习，提出了新的局部自适应和几乎无需调参的FedSPS和FedDecSPS变体。我们证明了当插值条件满足时，FedSPS以线性速度收敛，一般情况下收敛到解的邻域。 |
| [^37] | [Distribution-aware Fairness Test Generation.](http://arxiv.org/abs/2305.13935) | 本文介绍了一种名为DistroFair的分布感知的公平性测试方法，可以从图像分类器中检测到类级别的公平性违规。 |
| [^38] | [Communication-minimizing Asynchronous Tensor Parallelism.](http://arxiv.org/abs/2305.13525) | 本文提出了Tensor3D，一种最小化通信消耗的三维张量计算并行化方法。它利用智能分布神经网络参数、新颖超分解方法以及通信模型，使训练速度提高了约3倍，GPU空闲时间降低了50％以上。 |
| [^39] | [A Billion-scale Foundation Model for Remote Sensing Images.](http://arxiv.org/abs/2304.05215) | 本文介绍了一个用于遥感图像的十亿级基础模型，并研究了增加模型参数数量对该模型在下游任务中的性能影响，实验显示增加模型参数数量可以显著提高性能。 |
| [^40] | [An algorithmic framework for the optimization of deep neural networks architectures and hyperparameters.](http://arxiv.org/abs/2303.12797) | 本文提出一种基于进化的有向无环图的算法框架，用于自动生成高效且灵活的深度神经网络并优化相关的超参数。此框架可用于任何能够处理混合搜索空间的元启发式算法，并在时间序列预测数据集上表现出比已有模型更好的性能。 |
| [^41] | [Sequence Generation via Subsequence Similarity: Theory and Application to UAV Identification.](http://arxiv.org/abs/2301.08403) | 本文探究了一种单次生成模型的多样性，主要聚焦于子序列相似性如何影响整个序列相似性，并通过生成子序列相似的序列来增强数据集。 |
| [^42] | [Faster Gradient-Free Algorithms for Nonsmooth Nonconvex Stochastic Optimization.](http://arxiv.org/abs/2301.06428) | 本文提出了一种使用随机递归梯度估计器的更高效算法，来解决无平滑非凸随机优化问题，其复杂度为 $\mathcal{O}(L^3 d^{3/2} \epsilon^{-3}+ \Delta L^2 d^{3/2} \delta^{-1} \epsilon^{-3})$。 |
| [^43] | [A Simple and Efficient Stochastic Algorithm for Decentralized Nonconvex-Strongly-Concave Minimax Optimization.](http://arxiv.org/abs/2212.02387) | 本文提出了一种称为去中心化递归梯度上升法（DREAM）的简单高效算法，用于解决去中心化非凸极小化问题，并实现了寻找原函数的 $\epsilon$-稳定点的最佳理论保证。 |

# 详细

[^1]: GeNet:一种基于图神经网络的抗噪声任务导向语义通信范式

    GeNet: A Graph Neural Network-based Anti-noise Task-Oriented Semantic Communication Paradigm

    [https://arxiv.org/abs/2403.18296](https://arxiv.org/abs/2403.18296)

    GeNet提出了一种基于图神经网络的语义通信范式，通过将数据转换为图结构、利用编码器提取语义信息并利用解码器重建信息的方法来实现抗噪声任务导向通信。

    

    传统的语义通信任务方法依赖于了解信噪比（SNR）来减轻通道噪声。然而，这些方法需要在特定的SNR条件下进行训练，需要大量时间和计算资源。在本文中，我们提出了GeNet，这是一种基于图神经网络（GNN）的语义通信范式，旨在抵抗噪声，从而促进任务导向通信（TOC）。我们提出了一种新颖的方法，首先将输入数据图像转换为图结构。然后利用基于GNN的编码器从源数据中提取语义信息。这些提取的语义信息然后通过通道传输。在接收端，使用基于GNN的解码器从源数据中重建相关的语义信息以用于TOC。通过实验评估，我们展示了GeNet在抗噪声TOC中的有效性。

    arXiv:2403.18296v1 Announce Type: cross  Abstract: Traditional approaches to semantic communication tasks rely on the knowledge of the signal-to-noise ratio (SNR) to mitigate channel noise. However, these methods necessitate training under specific SNR conditions, entailing considerable time and computational resources. In this paper, we propose GeNet, a Graph Neural Network (GNN)-based paradigm for semantic communication aimed at combating noise, thereby facilitating Task-Oriented Communication (TOC). We propose a novel approach where we first transform the input data image into graph structures. Then we leverage a GNN-based encoder to extract semantic information from the source data. This extracted semantic information is then transmitted through the channel. At the receiver's end, a GNN-based decoder is utilized to reconstruct the relevant semantic information from the source data for TOC. Through experimental evaluation, we show GeNet's effectiveness in anti-noise TOC while decoup
    
[^2]: 用于腿式定点机器人运动操作的视觉全身控制

    Visual Whole-Body Control for Legged Loco-Manipulation

    [https://arxiv.org/abs/2403.16967](https://arxiv.org/abs/2403.16967)

    这项研究提出了一种利用视觉全身控制的框架，使腿式机器人能够同时控制腿部和手臂，以扩展操作能力，并通过仿真训练和Sim2Real转移实现了在捡起不同物体方面取得显著改进。

    

    我们研究了使用配备手臂的腿式机器人进行移动操作的问题，即腿式定点操作。尽管机器人的腿通常用于移动，但通过进行全身控制，可以扩大其操作能力。也就是说，机器人可以同时控制腿部和手臂，以扩展其工作空间。我们提出了一个能够使用视觉观测自主进行全身控制的框架。我们的方法称为\ourFull~(\our)，由一个低级策略和一个高级策略组成。低级策略使用所有自由度来跟踪末端执行器的位置，高级策略根据视觉输入提出末端执行器位置。我们在仿真中训练了两个级别的策略，并进行了从Sim到实物的转移以进行实际机器人部署。我们进行了大量实验证明，在不同配置下（高度、）捡起不同物体方面，相对基线方法取得了显著改进。

    arXiv:2403.16967v1 Announce Type: cross  Abstract: We study the problem of mobile manipulation using legged robots equipped with an arm, namely legged loco-manipulation. The robot legs, while usually utilized for mobility, offer an opportunity to amplify the manipulation capabilities by conducting whole-body control. That is, the robot can control the legs and the arm at the same time to extend its workspace. We propose a framework that can conduct the whole-body control autonomously with visual observations. Our approach, namely \ourFull~(\our), is composed of a low-level policy using all degrees of freedom to track the end-effector manipulator position and a high-level policy proposing the end-effector position based on visual inputs. We train both levels of policies in simulation and perform Sim2Real transfer for real robot deployment. We perform extensive experiments and show significant improvements over baselines in picking up diverse objects in different configurations (heights,
    
[^3]: 重新思考对抗逆强化学习：从策略模仿和可转移奖励恢复的角度

    Rethinking Adversarial Inverse Reinforcement Learning: From the Angles of Policy Imitation and Transferable Reward Recovery

    [https://arxiv.org/abs/2403.14593](https://arxiv.org/abs/2403.14593)

    重新思考对抗逆强化学习中的策略模仿和可转移奖励恢复，提出了一个混合框架PPO-AIRL + SAC以解决SAC算法在AIRL训练中无法全面解开奖励函数的问题。

    

    对抗逆强化学习（AIRL）作为模仿学习中的基石方法。本文重新思考了AIRL的两个不同角度：策略模仿和可转移奖励恢复。我们从用Soft Actor-Critic（SAC）替换AIRL中的内置算法开始，以增强样本效率，这要归功于SAC的离策略形式和相对于AIRL而言可识别的马尔可夫决策过程（MDP）模型。这确实在策略模仿方面表现出显著的改进，但不慎给可转移奖励恢复带来了缺点。为了解决这个问题，我们阐述了SAC算法本身在AIRL训练过程中无法全面解开奖励函数，提出了一个混合框架，PPO-AIRL + SAC，以获得令人满意的转移效果。此外，我们分析了环境提取解开的奖励的能力。

    arXiv:2403.14593v1 Announce Type: new  Abstract: Adversarial inverse reinforcement learning (AIRL) stands as a cornerstone approach in imitation learning. This paper rethinks the two different angles of AIRL: policy imitation and transferable reward recovery. We begin with substituting the built-in algorithm in AIRL with soft actor-critic (SAC) during the policy optimization process to enhance sample efficiency, thanks to the off-policy formulation of SAC and identifiable Markov decision process (MDP) models with respect to AIRL. It indeed exhibits a significant improvement in policy imitation but accidentally brings drawbacks to transferable reward recovery. To learn this issue, we illustrate that the SAC algorithm itself is not feasible to disentangle the reward function comprehensively during the AIRL training process, and propose a hybrid framework, PPO-AIRL + SAC, for satisfactory transfer effect. Additionally, we analyze the capability of environments to extract disentangled rewa
    
[^4]: Adversarial Nibbler: 一种用于识别文本到图像生成中多样化危害的开放式红队方法

    Adversarial Nibbler: An Open Red-Teaming Method for Identifying Diverse Harms in Text-to-Image Generation

    [https://arxiv.org/abs/2403.12075](https://arxiv.org/abs/2403.12075)

    批评了文本到图像生成中模型对非明显攻击的鲁棒性，提出了Adversarial Nibbler Challenge以众包多样化的提示来纠正模型的安全问题

    

    随着文本到图像（T2I）生成AI模型的崛起，评估模型对于不明显攻击的稳健性以减少生成冒犯性图像变得至关重要。通过专注于"隐性对抗"提示（触发T2I模型生成不安全图像的非明显原因），我们独立辨别出一组难以发现的安全问题，人类创造力很适合揭示这些问题。为此，我们构建了Adversarial Nibbler Challenge，这是一个红队方法，用于众包一组多样化的隐性对抗性提示。我们已汇总一套最先进的T2I模型，采用简单用户界面来识别和注释危害，并吸引广泛人群来捕捉在标准测试中可能被忽视的长尾安全问题。挑战在连续回合中进行，以实现对T2I模型中安全隐患的持续发现和分析。

    arXiv:2403.12075v1 Announce Type: cross  Abstract: With the rise of text-to-image (T2I) generative AI models reaching wide audiences, it is critical to evaluate model robustness against non-obvious attacks to mitigate the generation of offensive images. By focusing on ``implicitly adversarial'' prompts (those that trigger T2I models to generate unsafe images for non-obvious reasons), we isolate a set of difficult safety issues that human creativity is well-suited to uncover. To this end, we built the Adversarial Nibbler Challenge, a red-teaming methodology for crowdsourcing a diverse set of implicitly adversarial prompts. We have assembled a suite of state-of-the-art T2I models, employed a simple user interface to identify and annotate harms, and engaged diverse populations to capture long-tail safety issues that may be overlooked in standard testing. The challenge is run in consecutive rounds to enable a sustained discovery and analysis of safety pitfalls in T2I models.   In this pape
    
[^5]: 使用自适应估计融合高效剪枝大型语言模型

    Efficient Pruning of Large Language Model with Adaptive Estimation Fusion

    [https://arxiv.org/abs/2403.10799](https://arxiv.org/abs/2403.10799)

    提出了一种简单而高效的剪枝方法，能够自适应地模拟每个子结构的重要性，并根据多层结构的结果自适应地融合粗粒度和细粒度的估计。

    

    大型语言模型（LLMs）已经成为许多生成性下游任务中至关重要的组成部分，这导致在资源受限设备上高效部署它们成为不可避免的趋势和重大挑战。结构化剪枝是解决这一挑战的广泛应用方法。然而，当处理多个解码器层的复杂结构时，通常的方法往往采用常见的估计方法进行剪枝。这些方法导致特定下游任务精度下降。本文介绍了一种简单而有效的方法，可自适应地模拟每个子结构的重要性。同时，它可以基于复杂和多层结构的结果，自适应地融合粗粒度和细粒度的估计。我们设计的所有方面都无缝集成到端到端的剪枝框架中。与主流数据集上的最先进方法相比，我们的实验结果表明

    arXiv:2403.10799v1 Announce Type: cross  Abstract: Large language models (LLMs) have become crucial for many generative downstream tasks, leading to an inevitable trend and significant challenge to deploy them efficiently on resource-constrained devices. Structured pruning is a widely used method to address this challenge. However, when dealing with the complex structure of the multiple decoder layers, general methods often employ common estimation approaches for pruning. These approaches lead to a decline in accuracy for specific downstream tasks. In this paper, we introduce a simple yet efficient method that adaptively models the importance of each substructure. Meanwhile, it can adaptively fuse coarse-grained and finegrained estimations based on the results from complex and multilayer structures. All aspects of our design seamlessly integrate into the endto-end pruning framework. Our experimental results, compared with state-of-the-art methods on mainstream datasets, demonstrate ave
    
[^6]: 机器学习中的重要性加权简要调查

    A Short Survey on Importance Weighting for Machine Learning

    [https://arxiv.org/abs/2403.10175](https://arxiv.org/abs/2403.10175)

    重要性加权是统计学和机器学习中的基本程序，通过对目标函数或概率分布进行加权，可以保证监督学习在训练和测试分布之间差异的情况下具有统计上期望的性质

    

    重要性加权是统计学和机器学习中的一项基本程序，根据某种意义上实例的重要性对目标函数或概率分布进行加权。这一简单而有用的思想的广泛应用导致了许多重要性加权的应用。例如，据知，在关于训练和测试分布之间差异的假设下的监督学习，通过密度比的重要性加权可以保证统计上期望的性质。这项调查总结了机器学习和相关研究中重要性加权的广泛应用。

    arXiv:2403.10175v1 Announce Type: cross  Abstract: Importance weighting is a fundamental procedure in statistics and machine learning that weights the objective function or probability distribution based on the importance of the instance in some sense. The simplicity and usefulness of the idea has led to many applications of importance weighting. For example, it is known that supervised learning under an assumption about the difference between the training and test distributions, called distribution shift, can guarantee statistically desirable properties through importance weighting by their density ratio. This survey summarizes the broad applications of importance weighting in machine learning and related research.
    
[^7]: 一个用于在不确定性下发现可信神经网络代理模型的战略框架

    A Framework for Strategic Discovery of Credible Neural Network Surrogate Models under Uncertainty

    [https://arxiv.org/abs/2403.08901](https://arxiv.org/abs/2403.08901)

    提出了一个名为Occam Plausibility Algorithm for surrogate models (OPAL-surrogate)的框架，通过基于神经网络的代理模型，采用层次贝叶斯推理和模型验证测试来评估代理模型的可信度和预测可靠性。

    

    深度神经网络在开发复杂物理系统高保真仿真的数据驱动代理模型中的广泛整合，凸显了稳健不确定性量化技术和可信度评估方法的重要性，确保代理模型可可靠地用于重要决策。本研究提出了用于代理模型的Occam Plausibility Algorithm（OPAL-surrogate），提供了一个系统框架，以在大量潜在模型（包括各种神经网络类别以及架构和超参数选择）中揭示具有预测能力的基于神经网络的代理模型。框架基于层次贝叶斯推理，并采用模型验证测试来评估在不确定性下代理模型的可信度和预测可靠性。通过利用这些原则，OPAL-surrogate引入了一种系统性和

    arXiv:2403.08901v1 Announce Type: cross  Abstract: The widespread integration of deep neural networks in developing data-driven surrogate models for high-fidelity simulations of complex physical systems highlights the critical necessity for robust uncertainty quantification techniques and credibility assessment methodologies, ensuring the reliable deployment of surrogate models in consequential decision-making. This study presents the Occam Plausibility Algorithm for surrogate models (OPAL-surrogate), providing a systematic framework to uncover predictive neural network-based surrogate models within the large space of potential models, including various neural network classes and choices of architecture and hyperparameters. The framework is grounded in hierarchical Bayesian inferences and employs model validation tests to evaluate the credibility and prediction reliability of the surrogate models under uncertainty. Leveraging these principles, OPAL-surrogate introduces a systematic and
    
[^8]: 直接与Chat-Fine-Tuned LLMs的草案模型对齐

    Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs

    [https://arxiv.org/abs/2403.00858](https://arxiv.org/abs/2403.00858)

    通过提出的框架，我们训练了一种用于Llama 2 Chat 7B或更大模型的草案模型，实现了加速推理，仅占原始大小的1.64％。

    

    文本生成与大型语言模型（LLMs）由于其自回归本质、巨大的参数数量和有限的内存带宽而被认为是内存密集型，通常导致低令牌速率。猜测解码已被提出作为LLM推理加速的解决方案。然而，在现代开源LLM系列中，例如Llama 2 7B，由于草案模型通常不可用，因此需要训练高质量的草案模型以通过猜测解码实现推理加速。在本文中，我们提出了一个简单的草案模型训练框架，用于直接与Chat-capable目标模型对齐。通过我们提出的框架，我们训练出Llama 2 Chat Drafter 115M，这是一个适用于Llama 2 Chat 7B或更大模型的草案模型，仅占原始大小的1.64％。我们的训练框架仅包括预训练、蒸馏数据集生成和使用知识蒸馏进行微调，没有额外的对齐步骤。

    arXiv:2403.00858v1 Announce Type: cross  Abstract: Text generation with Large Language Models (LLMs) is known to be memory bound due to the combination of their auto-regressive nature, huge parameter counts, and limited memory bandwidths, often resulting in low token rates. Speculative decoding has been proposed as a solution for LLM inference acceleration. However, since draft models are often unavailable in the modern open-source LLM families, e.g., for Llama 2 7B, training a high-quality draft model is required to enable inference acceleration via speculative decoding. In this paper, we propose a simple draft model training framework for direct alignment to chat-capable target models. With the proposed framework, we train Llama 2 Chat Drafter 115M, a draft model for Llama 2 Chat 7B or larger, with only 1.64\% of the original size. Our training framework only consists of pretraining, distillation dataset generation, and finetuning with knowledge distillation, with no additional align
    
[^9]: PiShield：一种适用于以需求为基础学习的NeSy框架

    PiShield: A NeSy Framework for Learning with Requirements

    [https://arxiv.org/abs/2402.18285](https://arxiv.org/abs/2402.18285)

    PiShield是第一个允许将需求集成到神经网络拓扑结构中的框架，无论输入如何都能确保满足这些需求，并可根据从业者需求在推断和/或训练时集成需求。

    

    深度学习模型在各种应用领域展现出了其优势，然而，它们往往难以满足其输出的安全需求。本文介绍了PiShield，这是第一个允许将需求集成到神经网络拓扑结构中的框架。PiShield确保满足这些需求，无论输入如何。此外，它允许根据从业者的需求在推断和/或训练时集成需求。鉴于深度学习的广泛应用，迫切需要允许在各个领域集成需求的框架。这里，我们探讨了三个应用场景：功能基因组学、自动驾驶和表格数据生成。

    arXiv:2402.18285v1 Announce Type: cross  Abstract: Deep learning models have shown their strengths in various application domains, however, they often struggle to meet safety requirements for their outputs. In this paper, we introduce PiShield, the first framework ever allowing for the integration of the requirements into the neural networks' topology. PiShield guarantees compliance with these requirements, regardless of input. Additionally, it allows for integrating requirements both at inference and/or training time, depending on the practitioners' needs. Given the widespread application of deep learning, there is a growing need for frameworks allowing for the integration of the requirements across various domains. Here, we explore three application scenarios: functional genomics, autonomous driving, and tabular data generation.
    
[^10]: 异构无线网络中具有独立采样的自适应联邦学习

    Adaptive Federated Learning in Heterogeneous Wireless Networks with Independent Sampling

    [https://arxiv.org/abs/2402.10097](https://arxiv.org/abs/2402.10097)

    这项研究提出了一种适用于异构无线网络的自适应联邦学习方法，其中包括了独立客户端采样和带宽分配方案，以提高训练效率和适应数据和系统的异构特性。

    

    联邦学习算法通常通过对客户端进行随机子集采样来解决迟到者问题并提高通信效率。然而，最近的研究在联合系统和数据异构设计方面存在一些限制，可能与实际的异构无线网络不一致。在本文中，我们提倡一种新的独立客户端采样策略，以最小化联邦学习的实际训练时间，同时考虑通信和计算中的数据异构性和系统异构性。我们首先推导了带有独立客户端采样的非凸损失函数的新收敛界限，然后提出了一种自适应带宽分配方案。此外，我们还提出了一种基于收敛轮数上界和每轮预期训练时间的高效独立客户端采样算法，以最小化联邦学习的实际训练时间，同时考虑数据异构性和系统异构性。

    arXiv:2402.10097v1 Announce Type: new  Abstract: Federated Learning (FL) algorithms commonly sample a random subset of clients to address the straggler issue and improve communication efficiency. While recent works have proposed various client sampling methods, they have limitations in joint system and data heterogeneity design, which may not align with practical heterogeneous wireless networks. In this work, we advocate a new independent client sampling strategy to minimize the wall-clock training time of FL, while considering data heterogeneity and system heterogeneity in both communication and computation. We first derive a new convergence bound for non-convex loss functions with independent client sampling and then propose an adaptive bandwidth allocation scheme. Furthermore, we propose an efficient independent client sampling algorithm based on the upper bounds on the convergence rounds and the expected per-round training time, to minimize the wall-clock time of FL, while consider
    
[^11]: 准则崩溃和损失分布控制

    Criterion collapse and loss distribution control

    [https://arxiv.org/abs/2402.09802](https://arxiv.org/abs/2402.09802)

    该论文研究了"准则崩溃"的概念，即优化一个度量指标意味着另一个度量指标的最优性。研究结果发现，对于损失的伯努利分布，CVaR和DRO的结果远超出现有研究，同时发现了一些特定条件下，单调准则如倾斜ERM无法避免崩溃，而非单调的替代方案可以。

    

    在这项工作中，我们考虑了"准则崩溃"的概念，即优化一个度量指标意味着另一个度量指标的最优性，特别关注各种学习准则下崩溃成误差概率最小化器的条件，从DRO和OCE风险（CVaR、倾斜ERM）到文献中探索的最新上升-下降算法的非单调准则（洪水、SoftAD）。我们展示了在伯努利分布损失的背景下，CVaR和DRO的现有结果远远超越了崩溃的范围，然后扩大了我们的范围，包括代理损失，展示了像倾斜ERM这样的单调准则无法避免崩溃的条件，而非单调的替代方案可以。

    arXiv:2402.09802v1 Announce Type: cross  Abstract: In this work, we consider the notion of "criterion collapse," in which optimization of one metric implies optimality in another, with a particular focus on conditions for collapse into error probability minimizers under a wide variety of learning criteria, ranging from DRO and OCE risks (CVaR, tilted ERM) to non-monotonic criteria underlying recent ascent-descent algorithms explored in the literature (Flooding, SoftAD). We show how collapse in the context of losses with a Bernoulli distribution goes far beyond existing results for CVaR and DRO, then expand our scope to include surrogate losses, showing conditions where monotonic criteria such as tilted ERM cannot avoid collapse, whereas non-monotonic alternatives can.
    
[^12]: Atlassian的CI构建失败预测的从业者挑战和感知研究

    Practitioners' Challenges and Perceptions of CI Build Failure Predictions at Atlassian

    [https://arxiv.org/abs/2402.09651](https://arxiv.org/abs/2402.09651)

    Atlassian的研究调查了CI构建失败对软件开发过程和团队的影响，并研究了将CI构建预测工具集成到Bitbucket环境中所涉及的挑战和期望。

    

    持续集成（CI）构建失败可能会对软件开发过程和团队产生重大影响，如延迟发布新功能和降低开发人员的生产力。本研究报告了一项实证研究，调查了Atlassian在产品开发过程中的CI构建失败情况。我们的定量分析发现，代码库维度是影响CI构建失败的关键因素。此外，我们的定性调查发现，Atlassian开发人员认为CI构建失败是实践中的挑战性问题。此外，我们发现CI构建预测不仅可以提供对CI构建失败的积极见解，还可以促进团队决策。我们的研究为将CI构建预测工具集成到Bitbucket环境中所涉及的挑战和期望提供了有价值的见解，从而增强了CI流程。

    arXiv:2402.09651v1 Announce Type: cross  Abstract: Continuous Integration (CI) build failures could significantly impact the software development process and teams, such as delaying the release of new features and reducing developers' productivity. In this work, we report on an empirical study that investigates CI build failures throughout product development at Atlassian. Our quantitative analysis found that the repository dimension is the key factor influencing CI build failures. In addition, our qualitative survey revealed that Atlassian developers perceive CI build failures as challenging issues in practice. Furthermore, we found that the CI build prediction can not only provide proactive insight into CI build failures but also facilitate the team's decision-making. Our study sheds light on the challenges and expectations involved in integrating CI build prediction tools into the Bitbucket environment, providing valuable insights for enhancing CI processes.
    
[^13]: 一种广义的在线凸优化方法

    A Generalized Approach to Online Convex Optimization

    [https://arxiv.org/abs/2402.08621](https://arxiv.org/abs/2402.08621)

    这是一篇关于在线凸优化的论文，作者分析了不同环境下的问题并提出了一种通用的解决方法，该方法可以转化为相应的线性优化算法，并可以在面对不同类型对手时获得可比较的遗憾界限。

    

    在本文中，我们分析了不同环境下的在线凸优化问题。我们证明了任何用于具有完全自适应对手的在线线性优化的算法都是用于在线凸优化的算法。我们还证明了任何需要全信息反馈的算法都可以转化为具有可比较的遗憾界限的半匹配反馈算法。此外，我们还证明了使用确定性半匹配反馈的全自适应对手设计的算法在面对无知对手时可以使用只有随机半匹配反馈的算法获得相似的界限。我们利用这一结果描述了将一阶算法转化为零阶算法的通用元算法，这些算法具有可比较的遗憾界限。我们的框架使我们能够分析各种设置中的在线优化问题，包括全信息反馈、半匹配反馈、随机遗憾、对抗遗憾和各种形式的非平稳遗憾。利用我们的分析结果，

    In this paper, we analyze the problem of online convex optimization in different settings. We show that any algorithm for online linear optimization with fully adaptive adversaries is an algorithm for online convex optimization. We also show that any such algorithm that requires full-information feedback may be transformed to an algorithm with semi-bandit feedback with comparable regret bound. We further show that algorithms that are designed for fully adaptive adversaries using deterministic semi-bandit feedback can obtain similar bounds using only stochastic semi-bandit feedback when facing oblivious adversaries. We use this to describe general meta-algorithms to convert first order algorithms to zeroth order algorithms with comparable regret bounds. Our framework allows us to analyze online optimization in various settings, such full-information feedback, bandit feedback, stochastic regret, adversarial regret and various forms of non-stationary regret. Using our analysis, we provide
    
[^14]: MOMENT：一个开放的时间序列基础模型家族

    MOMENT: A Family of Open Time-series Foundation Models

    [https://arxiv.org/abs/2402.03885](https://arxiv.org/abs/2402.03885)

    MOMENT是一个开放的时间序列基础模型家族，通过解决时间序列数据的挑战，编制了一个大规模的公共时间序列数据集，并设计了一个基准测试来评估有限监督场景下模型的性能。

    

    我们介绍了MOMENT，一个开源的通用时间序列分析基础模型家族。在时间序列数据的预训练大模型方面存在着一些挑战，包括：（1）缺乏一个大而有凝聚力的公共时间序列存储库，以及（2）多样的时间序列特征使得多数据集的训练变得困难。此外，这些模型的实验评估标准，特别是在资源、时间和监督有限的情况下，仍处于初级阶段。（3）为解决这些挑战，我们编制了一个大而多样的公共时间序列数据集，称为时间序列堆，以系统地解决时间序列特定的挑战，以解锁大规模的多数据集预训练。最后，我们借鉴最近的工作，设计了一个基准测试来评估有限监督场景下时间序列基础模型在不同任务和数据集上的效果。在这个基准测试上的实验证明了我们的预训练模型在少量数据的情况下的有效性。

    We introduce MOMENT, a family of open-source foundation models for general-purpose time-series analysis. Pre-training large models on time-series data is challenging due to (1) the absence of a large and cohesive public time-series repository, and (2) diverse time-series characteristics which make multi-dataset training onerous. Additionally, (3) experimental benchmarks to evaluate these models, especially in scenarios with limited resources, time, and supervision, are still in their nascent stages. To address these challenges, we compile a large and diverse collection of public time-series, called the Time-series Pile, and systematically tackle time-series-specific challenges to unlock large-scale multi-dataset pre-training. Finally, we build on recent work to design a benchmark to evaluate time-series foundation models on diverse tasks and datasets in limited supervision settings. Experiments on this benchmark demonstrate the effectiveness of our pre-trained models with minimal data 
    
[^15]: 一次图卷积就够了：高效灰度图像分类

    A Single Graph Convolution Is All You Need: Efficient Grayscale Image Classification

    [https://arxiv.org/abs/2402.00564](https://arxiv.org/abs/2402.00564)

    本论文提出了一种高效的灰度图像分类方法，通过将图像视为矢量，并使用单个图卷积层进行处理，提高了分类模型的准确性和稳定性。

    

    图像分类器通常依赖于卷积神经网络(CNN)来完成任务，而CNN相比于多层感知机(MLP)更加庞大，这在实时应用中可能会带来问题。此外，许多图像分类模型适用于RGB和灰度数据集，但仅仅使用灰度图像的分类器相对较少见。灰度图像分类具有广泛的应用，包括但不限于医学图像分类和合成孔径雷达(SAR)自动目标识别(ATR)。因此，我们提出了一种使用图像的矢量化视图的新型灰度(单通道)图像分类方法。我们通过将图像视为矢量，并将问题设置为灰度图像分类问题，充分利用了MLP的轻量级特性。我们发现，批次级别使用单个图卷积层可以提高模型的准确性并减小性能的差异。此外，我们开发了定制的准确率估计方法。

    Image classifiers often rely on convolutional neural networks (CNN) for their tasks, which are inherently more heavyweight than multilayer perceptrons (MLPs), which can be problematic in real-time applications. Additionally, many image classification models work on both RGB and grayscale datasets. Classifiers that operate solely on grayscale images are much less common. Grayscale image classification has diverse applications, including but not limited to medical image classification and synthetic aperture radar (SAR) automatic target recognition (ATR). Thus, we present a novel grayscale (single channel) image classification approach using a vectorized view of images. We exploit the lightweightness of MLPs by viewing images as a vector and reducing our problem setting to the grayscale image classification setting. We find that using a single graph convolutional layer batch-wise increases accuracy and reduces variance in the performance of our model. Moreover, we develop a customized acc
    
[^16]: 通过迭代本地扩展实现高效可扩展的图生成

    Efficient and Scalable Graph Generation through Iterative Local Expansion

    [https://arxiv.org/abs/2312.11529](https://arxiv.org/abs/2312.11529)

    通过逐步扩展单个节点到目标图的方法，避免了对所有节点对的整个联合分布进行建模，实现了高效可扩展的图生成，同时通过多尺度生成保持了高表达性。

    

    在图的生成模型领域，进行了大量研究。然而，由于代表所有节点对的整个联合分布的复杂性以及同时捕捉全局和局部图结构，大多数现有方法在处理大型图时存在困难。为了克服这些问题，我们引入了一种方法，通过逐步将单个节点扩展到目标图来生成图。在每一步中，通过去噪扩散以本地化方式添加节点和边，首先构建全局结构，然后细化局部细节。局部生成避免了对所有节点对上的整个联合分布进行建模，相对于节点数而言实现了大幅的计算节约，并通过多尺度生成保持了高表达性。我们的实验表明，我们的模型在公认的基准数据集上实现了最先进的性能。

    arXiv:2312.11529v2 Announce Type: replace-cross  Abstract: In the realm of generative models for graphs, extensive research has been conducted. However, most existing methods struggle with large graphs due to the complexity of representing the entire joint distribution across all node pairs and capturing both global and local graph structures simultaneously. To overcome these issues, we introduce a method that generates a graph by progressively expanding a single node to a target graph. In each step, nodes and edges are added in a localized manner through denoising diffusion, building first the global structure, and then refining the local details. The local generation avoids modeling the entire joint distribution over all node pairs, achieving substantial computational savings with subquadratic runtime relative to node count while maintaining high expressivity through multiscale generation. Our experiments show that our model achieves state-of-the-art performance on well-established b
    
[^17]: 基于Conformer的极端边缘计算设备上的语音识别

    Conformer-Based Speech Recognition On Extreme Edge-Computing Devices

    [https://arxiv.org/abs/2312.10359](https://arxiv.org/abs/2312.10359)

    提出了在资源受限设备上实现基于Conformer的端到端流式ASR系统的一系列模型架构适配、神经网络图变换和数值优化方法，实现了超过5.26倍实时速度的语音识别，同时最大限度减少能耗并实现了最先进的准确性。

    

    随着今天设备中越来越强大的计算能力和资源，传统上在云端执行的计算密集型自动语音识别(ASR)正从云端转移到设备上以更好地保护用户隐私。然而，在资源受限的设备上实现本地ASR仍然具有挑战性，例如智能手机、智能可穿戴设备和其他小型家居自动化设备。本文提出了一系列模型架构调整、神经网络图变换和数值优化，以在资源受限设备上适配先进的基于Conformer的端到端流式ASR系统，而不降低准确性。我们在小型可穿戴设备上实现了超过实时5.26倍快（0.19 RTF）的语音识别，同时最大限度地减少能耗并实现了最先进的准确性。所提出的方法广泛适用于其他基于变换器的无服务器AI应用。

    arXiv:2312.10359v2 Announce Type: replace  Abstract: With increasingly more powerful compute capabilities and resources in today's devices, traditionally compute-intensive automatic speech recognition (ASR) has been moving from the cloud to devices to better protect user privacy. However, it is still challenging to implement on-device ASR on resource-constrained devices, such as smartphones, smart wearables, and other small home automation devices. In this paper, we propose a series of model architecture adaptions, neural network graph transformations, and numerical optimizations to fit an advanced Conformer based end-to-end streaming ASR system on resource-constrained devices without accuracy degradation. We achieve over 5.26 times faster than realtime (0.19 RTF) speech recognition on small wearables while minimizing energy consumption and achieving state-of-the-art accuracy. The proposed methods are widely applicable to other transformer-based server-free AI applications. In addition
    
[^18]: 关于偏置策略梯度算法的二阶收敛性研究

    On the Second-Order Convergence of Biased Policy Gradient Algorithms

    [https://arxiv.org/abs/2311.02546](https://arxiv.org/abs/2311.02546)

    该论文研究了偏置策略梯度算法的二阶收敛性，包括基于蒙特卡洛轨迹采样的普通梯度估计器和基于双循环评论家-演员算法的演员-评论家方法。实现在实际应用中的偏置主要来自于有限时间采样和对价值函数的逼近。

    

    由于强化学习问题的目标函数通常是高度非凸的，因此希望策略梯度算法能够脱离鞍点并达到二阶稳定点。现有的结果只考虑了带有无偏梯度估计器的普通策略梯度算法，但在无限时间折扣回报设置下，实际实现是有偏的，因为有限时间采样。此外，由于评论家对价值函数的逼近，评论家-演员方法的二阶收敛性也未被证实。我们提供了对有偏策略梯度方法的新颖的二阶分析，包括通过蒙特卡洛轨迹采样计算得到的普通梯度估计器，以及双循环评论家-演员算法，在内循环中，评论家通过TD(0)学习改进了对价值函数的逼近。另外，我们还证明了TD(0)的收敛性。

    Since the objective functions of reinforcement learning problems are typically highly nonconvex, it is desirable that policy gradient, the most popular algorithm, escapes saddle points and arrives at second-order stationary points. Existing results only consider vanilla policy gradient algorithms with unbiased gradient estimators, but practical implementations under the infinite-horizon discounted reward setting are biased due to finite-horizon sampling. Moreover, actor-critic methods, whose second-order convergence has not yet been established, are also biased due to the critic approximation of the value function. We provide a novel second-order analysis of biased policy gradient methods, including the vanilla gradient estimator computed from Monte-Carlo sampling of trajectories as well as the double-loop actor-critic algorithm, where in the inner loop the critic improves the approximation of the value function via TD(0) learning. Separately, we also establish the convergence of TD(0)
    
[^19]: 评估模型在交叉子群体间性能的结构回归方法

    A structured regression approach for evaluating model performance across intersectional subgroups. (arXiv:2401.14893v1 [cs.LG])

    [http://arxiv.org/abs/2401.14893](http://arxiv.org/abs/2401.14893)

    这项工作介绍了一种结构回归方法，用于评估模型在不同交叉子群体间的性能。它可以提供可靠的系统性能估计，即使对于很小的子群体。

    

    在人工智能公平性评估中，分解式评估是一项核心任务，目标是衡量人工智能系统在由人口统计学或其他敏感属性组合定义的不同子群体中的性能。标准方法是将评估数据分层到子群体中，并分别计算每个组的性能指标。然而，即使对于中等规模的评估数据集来说，在考虑到交叉子群体时样本数量也会迅速变小，这大大限制了许多分解评估中对交叉群体的考虑程度。在本研究中，我们引入了一种结构回归方法来进行分解评估，我们证明即使对于非常小的子群体，该方法也能产生可靠的系统性能估计。我们还提供了相应的推断策略来构建置信区间，并探索了拟合优度测试如何揭示交叉子群体所经历的与公平相关的伤害的结构。

    Disaggregated evaluation is a central task in AI fairness assessment, with the goal to measure an AI system's performance across different subgroups defined by combinations of demographic or other sensitive attributes. The standard approach is to stratify the evaluation data across subgroups and compute performance metrics separately for each group. However, even for moderately-sized evaluation datasets, sample sizes quickly get small once considering intersectional subgroups, which greatly limits the extent to which intersectional groups are considered in many disaggregated evaluations. In this work, we introduce a structured regression approach to disaggregated evaluation that we demonstrate can yield reliable system performance estimates even for very small subgroups. We also provide corresponding inference strategies for constructing confidence intervals and explore how goodness-of-fit testing can yield insight into the structure of fairness-related harms experienced by intersectio
    
[^20]: 为隐私保护可穿戴压力检测生成合成健康传感器数据

    Generating Synthetic Health Sensor Data for Privacy-Preserving Wearable Stress Detection. (arXiv:2401.13327v1 [cs.LG])

    [http://arxiv.org/abs/2401.13327](http://arxiv.org/abs/2401.13327)

    本论文介绍了一种隐私保护的合成健康传感器数据的方法，通过生成对抗网络（GANs）和差分隐私（DP）防护，生成与压力时刻相关的合成序列数据，确保患者信息的保护，并对合成数据进行质量评估。在压力检测数据集上验证了该方法的有效性。

    

    智能手表的健康传感器数据在智能健康应用和患者监测中越来越被使用，包括压力检测。然而，这类医疗数据往往包含敏感的个人信息，并且获取这些数据以进行研究是资源密集型的。为了应对这一挑战，我们介绍了一种关注隐私的合成多传感器智能手表健康读数与压力时刻相关的方法。我们的方法包括通过生成对抗网络（GANs）生成合成序列数据，并在模型训练过程中实施差分隐私（DP）防护以保护患者信息。为了确保合成数据的完整性，我们采用一系列质量评估，并监测合成数据与原始数据之间的合理性。为了测试其有用性，我们在一个常用但规模较小的压力检测数据集上创建了私有机器学习模型，并探索了增强现有数据基础的策略。

    Smartwatch health sensor data is increasingly utilized in smart health applications and patient monitoring, including stress detection. However, such medical data often comprises sensitive personal information and is resource-intensive to acquire for research purposes. In response to this challenge, we introduce the privacy-aware synthetization of multi-sensor smartwatch health readings related to moments of stress. Our method involves the generation of synthetic sequence data through Generative Adversarial Networks (GANs), coupled with the implementation of Differential Privacy (DP) safeguards for protecting patient information during model training. To ensure the integrity of our synthetic data, we employ a range of quality assessments and monitor the plausibility between synthetic and original data. To test the usefulness, we create private machine learning models on a commonly used, albeit small, stress detection dataset, exploring strategies for enhancing the existing data foundat
    
[^21]: 学习动力学对泛化的影响：从样本关系开始

    Sample Relationship from Learning Dynamics Matters for Generalisation. (arXiv:2401.08808v1 [cs.LG])

    [http://arxiv.org/abs/2401.08808](http://arxiv.org/abs/2401.08808)

    这项研究从近似样本之间的相互作用开始，揭示了标签对样本之间的影响，提出了带标签的伪神经切向核 (lpNTK)，并探讨了lpNTK如何帮助理解学习现象。

    

    尽管在改进人工神经网络（ANN）的泛化性能方面，已经有很多关于提出新模型或损失函数的研究，但对训练数据对泛化的影响却关注较少。在这项工作中，我们从近似样本之间的相互作用开始，即学习一个样本如何影响模型对其他样本的预测。通过分析监督学习中涉及的权重更新项，我们发现标签会影响样本之间的相互作用。因此，我们提出了带标签的伪神经切向核（lpNTK），在测量样本之间的相互作用时考虑标签信息。我们首先证明，在某些假设下，lpNTK在Frobenius范数下渐近收敛于经验神经切向核。其次，我们说明了lpNTK如何帮助理解先前工作中发现的学习现象，特别是样本学习困难的现象。

    Although much research has been done on proposing new models or loss functions to improve the generalisation of artificial neural networks (ANNs), less attention has been directed to the impact of the training data on generalisation. In this work, we start from approximating the interaction between samples, i.e. how learning one sample would modify the model's prediction on other samples. Through analysing the terms involved in weight updates in supervised learning, we find that labels influence the interaction between samples. Therefore, we propose the labelled pseudo Neural Tangent Kernel (lpNTK) which takes label information into consideration when measuring the interactions between samples. We first prove that lpNTK asymptotically converges to the empirical neural tangent kernel in terms of the Frobenius norm under certain assumptions. Secondly, we illustrate how lpNTK helps to understand learning phenomena identified in previous work, specifically the learning difficulty of sample
    
[^22]: TAnet: 一种新的基于脑电信号的听觉空间注意力解码的时间注意力网络

    TAnet: A New Temporal Attention Network for EEG-based Auditory Spatial Attention Decoding with a Short Decision Window. (arXiv:2401.05819v1 [eess.SP])

    [http://arxiv.org/abs/2401.05819](http://arxiv.org/abs/2401.05819)

    TAnet是一种新的基于脑电信号的听觉空间注意力解码网络，采用了多头注意力机制，可以更有效地捕捉脑电信号中时间步之间的交互作用，并提供了比传统方法更好的解码性能。

    

    听觉空间注意力检测（ASAD）通过分析电脑脑电信号来确定听众对说话者的注意方向。本研究旨在改进ASAD的性能，使用较短的决策窗口（小于1秒），而不是以前研究中使用的较长决策窗口。本文介绍了一种端到端的时间注意力网络（即TAnet）。TAnet采用多头注意力机制，可以更有效地捕捉采集到的脑电信号中时间步之间的相互作用，并为这些时间步分配相应的权重。实验表明，与基于CNN的方法和最近的ASAD方法相比，TAnet在KUL数据集中提供了改进的解码性能，使用较短的决策窗口（即小于1秒）的情况下，解码准确率分别为92.4%（决策窗口0.1秒）、94.9%（0.25秒）、95.1%（0.3秒）、95.4%（0.4秒）和95.5%（0.5秒）。

    Auditory spatial attention detection (ASAD) is used to determine the direction of a listener's attention to a speaker by analyzing her/his electroencephalographic (EEG) signals. This study aimed to further improve the performance of ASAD with a short decision window (i.e., <1 s) rather than with long decision windows in previous studies. An end-to-end temporal attention network (i.e., TAnet) was introduced in this work. TAnet employs a multi-head attention (MHA) mechanism, which can more effectively capture the interactions among time steps in collected EEG signals and efficiently assign corresponding weights to those EEG time steps. Experiments demonstrated that, compared with the CNN-based method and recent ASAD methods, TAnet provided improved decoding performance in the KUL dataset, with decoding accuracies of 92.4% (decision window 0.1 s), 94.9% (0.25 s), 95.1% (0.3 s), 95.4% (0.4 s), and 95.5% (0.5 s) with short decision windows (i.e., <1 s). As a new ASAD model with a short deci
    
[^23]: 连续时间深度神经网络的状态导数标准化

    State Derivative Normalization for Continuous-Time Deep Neural Networks. (arXiv:2401.02902v1 [eess.SY])

    [http://arxiv.org/abs/2401.02902](http://arxiv.org/abs/2401.02902)

    本文研究了在连续时间状态空间模型估计中，深度神经网络的数据标准化问题。通过引入状态导数级别的标准化常数，解决了隐藏状态、隐藏状态导数以及时间间隔的标准化挑战。选择适当的标准化常数与待识别系统的动力学相关，并提出了多种获得有效标准化常数的方法。

    

    深度神经网络的适当数据标准化的重要性是众所周知的。然而，在连续时间状态空间模型估计中，观察到模型估计的隐藏状态或隐藏状态导数，甚至时间间隔的不适当标准化可能会导致使用基于深度学习的方法时的数值和优化挑战。这导致模型质量降低。在本文中，我们展示了这三个标准化任务的内在耦合。由于存在这种耦合，我们提出了一种在状态导数水平引入标准化常数的解决方案。我们展示了适当选择标准化常数与待识别系统的动力学相关，并推导了多种获得有效标准化常数的方法。我们在基于实验数据的基准问题上比较和讨论了所有标准化策略。

    The importance of proper data normalization for deep neural networks is well known. However, in continuous-time state-space model estimation, it has been observed that improper normalization of either the hidden state or hidden state derivative of the model estimate, or even of the time interval can lead to numerical and optimization challenges with deep learning based methods. This results in a reduced model quality. In this contribution, we show that these three normalization tasks are inherently coupled. Due to the existence of this coupling, we propose a solution to all three normalization challenges by introducing a normalization constant at the state derivative level. We show that the appropriate choice of the normalization constant is related to the dynamics of the to-be-identified system and we derive multiple methods of obtaining an effective normalization constant. We compare and discuss all the normalization strategies on a benchmark problem based on experimental data from a
    
[^24]: 一个基础图模型

    A Foundation Graph Model. (arXiv:2311.03976v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.03976](http://arxiv.org/abs/2311.03976)

    本文提出了一个基于对抗性对比学习的基础图模型FoToM，该模型通过节点和边特征排除进行图预训练，在多个领域上实现了正向迁移，并取得了显著的性能提升。

    

    无监督图表示学习的主要优势是在数据或标签稀缺的情况下，可以对预训练模型进行微调。现有的方法是针对特定领域的，保持预训练和目标数据集之间的节点和边属性一致。这使得无法在其他领域进行迁移。能够在任意任务和领域上实现正向迁移的模型将成为第一个基础图模型。在这项工作中，我们使用对抗性对比学习提出了FoToM，一种基于节点和边特征排除的图预训练方法。我们使用FoToM在多个图领域上进行预训练，得到了第一个基础图模型。我们在来自多个领域的评估数据集上展示了正向迁移。在所有数据集上，性能最差时与有监督基线相当，76%的数据集在95%置信度下都显著优于有监督基线（P≤0.01），误差减少了8%至40%。

    The principal benefit of unsupervised graph representation learning is that a pre-trained model can be fine-tuned where data or labels are scarce. Existing approaches are domain specific, maintaining consistent node and edge attributes across the pre-training and target datasets. This precludes transfer to other domains. A model capable of positive transfer on arbitrary tasks and domains would represent the first foundation graph model.  In this work we use adversarial contrastive learning to present FoToM, a graph pre-training method based on node and edge feature exclusion. We use FoToM to pre-train models over multiple graph domains, producing the first foundation graph models. We demonstrate positive transfer on evaluation datasets from multiple domains, including domains not present in pre-training data. On all datasets performance is at worst on-par and on 76% significantly better than a supervised baseline ($P \leq 0.01$), with an 8 to 40% reduction in error at 95% confidence. C
    
[^25]: 机器人学习中的非参数回归方法在流形上的应用

    Non-parametric regression for robot learning on manifolds. (arXiv:2310.19561v1 [cs.RO] CROSS LISTED)

    [http://arxiv.org/abs/2310.19561](http://arxiv.org/abs/2310.19561)

    本文提出了一种在机器人学习中针对流形值数据的非参数回归方法，通过在流形上操作概率分布参数来直接估计函数，以改善预测准确性和简化算法。

    

    许多机器人学习工具都是针对欧几里得数据设计的。然而，在机器人学中，许多应用涉及到流形值数据。一个常见的例子是姿态；它可以表示为3x3的旋转矩阵或四元数，其空间是非欧几里得流形。在机器人学习中，流形值数据通常通过将流形与合适的欧几里得空间相关联来处理，可以通过嵌入流形或将数据投影到一个或多个切空间来实现。这些方法可能导致预测准确性不高和算法复杂。在本文中，我们提出了一种在流形内直接进行回归的“固有”方法。它涉及对流形上的适当概率分布进行操作，将其参数作为预测变量（如时间）的函数，并通过“局部似然”方法来非参数估计该函数，其中包含核函数。我们将该方法命名为核化似然估计。

    Many of the tools available for robot learning were designed for Euclidean data. However, many applications in robotics involve manifold-valued data. A common example is orientation; this can be represented as a 3-by-3 rotation matrix or a quaternion, the spaces of which are non-Euclidean manifolds. In robot learning, manifold-valued data are often handled by relating the manifold to a suitable Euclidean space, either by embedding the manifold or by projecting the data onto one or several tangent spaces. These approaches can result in poor predictive accuracy, and convoluted algorithms. In this paper, we propose an "intrinsic" approach to regression that works directly within the manifold. It involves taking a suitable probability distribution on the manifold, letting its parameter be a function of a predictor variable, such as time, then estimating that function non-parametrically via a "local likelihood" method that incorporates a kernel. We name the method kernelised likelihood esti
    
[^26]: 一个特征学习的光谱条件

    A Spectral Condition for Feature Learning. (arXiv:2310.17813v1 [cs.LG])

    [http://arxiv.org/abs/2310.17813](http://arxiv.org/abs/2310.17813)

    本文研究了在大型神经网络中特征学习的光谱条件，并提出了将权重矩阵和更新的谱范数缩放为$\sqrt{\texttt{fan-out}/\texttt{fan-in}}$的方法，以实现特征学习。同时，作者还导出了最大更新参数化的推导，旨在帮助读者对神经网络中的特征学习理解更加深入。

    

    针对训练规模越来越大的神经网络的推动，本文研究了在大型网络宽度上的初始化和训练。一个关键挑战是对网络的内部表示进行非平凡的演变，即特征学习。研究表明，通过缩放权重矩阵和更新的谱范数，我们可以实现特征学习，缩放系数为$\sqrt{\texttt{fan-out}/\texttt{fan-in}}$，与基于Frobenius范数和元素大小的启发式缩放方法有所不同。我们的光谱缩放分析还导出了最大更新参数化的基本推导。总之，我们旨在为读者提供对神经网络中特征学习的坚实概念理解。

    The push to train ever larger neural networks has motivated the study of initialization and training at large network width. A key challenge is to scale training so that a network's internal representations evolve nontrivially at all widths, a process known as feature learning. Here, we show that feature learning is achieved by scaling the spectral norm of weight matrices and their updates like $\sqrt{\texttt{fan-out}/\texttt{fan-in}}$, in contrast to widely used but heuristic scalings based on Frobenius norm and entry size. Our spectral scaling analysis also leads to an elementary derivation of \emph{maximal update parametrization}. All in all, we aim to provide the reader with a solid conceptual understanding of feature learning in neural networks.
    
[^27]: 为联邦图神经网络提供结构感知群体公平性

    Equipping Federated Graph Neural Networks with Structure-aware Group Fairness. (arXiv:2310.12350v1 [cs.LG])

    [http://arxiv.org/abs/2310.12350](http://arxiv.org/abs/2310.12350)

    本论文提出了一种名为F2GNN的方法，它旨在增强联邦图神经网络的群体公平性，解决了在联邦学习中减轻偏见的新挑战。

    

    图神经网络（GNN）广泛应用于不同领域的各种图数据处理和分析任务。由于隐私和监管限制，对集中式图数据进行训练可能不可行。因此，联邦学习（FL）成为解决这一挑战的一种趋势性解决方案。然而，由于GNN可能从训练数据中继承历史偏见并导致歧视性预测，在分布式环境中，局部模型的偏见很容易传播到全局模型，这给在联邦GNN中减轻偏见带来了新的挑战。为了解决这一问题，我们提出了F2GNN，一种增强联邦GNN群体公平性的方法。由于偏见可能来自数据和学习算法，F2GNN旨在在联邦环境下减少这两种类型的偏见。

    Graph Neural Networks (GNNs) have been widely used for various types of graph data processing and analytical tasks in different domains. Training GNNs over centralized graph data can be infeasible due to privacy concerns and regulatory restrictions. Thus, federated learning (FL) becomes a trending solution to address this challenge in a distributed learning paradigm. However, as GNNs may inherit historical bias from training data and lead to discriminatory predictions, the bias of local models can be easily propagated to the global model in distributed settings. This poses a new challenge in mitigating bias in federated GNNs. To address this challenge, we propose $\text{F}^2$GNN, a Fair Federated Graph Neural Network, that enhances group fairness of federated GNNs. As bias can be sourced from both data and learning algorithms, $\text{F}^2$GNN aims to mitigate both types of bias under federated settings. First, we provide theoretical insights on the connection between data bias in a tra
    
[^28]: 图嵌入与特征匹配的图压缩

    Graph Condensation via Eigenbasis Matching. (arXiv:2310.09202v1 [cs.LG])

    [http://arxiv.org/abs/2310.09202](http://arxiv.org/abs/2310.09202)

    本论文提出了基于特征匹配的无谱图压缩方法，以解决现有方法在泛化能力上存在的问题。通过详细分析发现，GNN注入合成图中的谱偏差导致了性能差异，我们的方法可以缓解这个问题。

    

    随着图数据量的增加，要求图神经网络（GNN）在各种图相关应用中提高效率和可伸缩性。最近，新兴的图压缩（GC）从数据角度降低了GNN的计算成本。它旨在用一个明显较小的合成图替代真实的大型图，使得在这两个图上训练的GNN表现出可比较的性能。然而，我们的实证研究发现，现有的GC方法在泛化能力上存在问题，即在同一个合成图上训练的不同GNN性能存在明显差异。是什么因素阻碍了GC的泛化能力，我们如何缓解这个问题？为了回答这个问题，我们进行了详细分析，发现GNN会将谱偏差注入合成图中，导致分布偏移。为解决这个问题，我们提出了基于特征匹配的无谱图压缩，称之为...

    The increasing amount of graph data places requirements on the efficiency and scalability of graph neural networks (GNNs), despite their effectiveness in various graph-related applications. Recently, the emerging graph condensation (GC) sheds light on reducing the computational cost of GNNs from a data perspective. It aims to replace the real large graph with a significantly smaller synthetic graph so that GNNs trained on both graphs exhibit comparable performance. However, our empirical investigation reveals that existing GC methods suffer from poor generalization, i.e., different GNNs trained on the same synthetic graph have obvious performance gaps. What factors hinder the generalization of GC and how can we mitigate it? To answer this question, we commence with a detailed analysis and observe that GNNs will inject spectrum bias into the synthetic graph, resulting in a distribution shift. To tackle this issue, we propose eigenbasis matching for spectrum-free graph condensation, name
    
[^29]: 未知方差下的高斯均值的任意有效T检验和置信序列

    Anytime-valid t-tests and confidence sequences for Gaussian means with unknown variance. (arXiv:2310.03722v1 [math.ST])

    [http://arxiv.org/abs/2310.03722](http://arxiv.org/abs/2310.03722)

    本文提出了两种新的“e-process”和置信序列方法，分别通过替换Lai的混合方法，并分析了所得结果的宽度。

    

    在1976年，Lai构造了一个非平凡的均值$\mu$的高斯分布的置信序列，该分布的方差$\sigma$是未知的。他使用了关于$\sigma$的不适当（右Haar）混合和关于$\mu$的不适当（平坦）混合。在本文中，我们详细说明了他构建的细节，其中使用了广义的不可积分鞅和扩展的维尔不等式。尽管这确实产生了一个顺序T检验，但由于他的鞅不可积分，它并没有产生一个“e-process”。在本文中，我们为相同的设置开发了两个新的“e-process”和置信序列：一个是在缩减滤波器中的测试鞅，另一个是在规范数据滤波器中的“e-process”。这些分别是通过将Lai的平坦混合替换为高斯混合，并将对$\sigma$的右Haar混合替换为在零空间下的最大似然估计，就像在通用推断中一样。我们还分析了所得结果的宽度。

    In 1976, Lai constructed a nontrivial confidence sequence for the mean $\mu$ of a Gaussian distribution with unknown variance $\sigma$. Curiously, he employed both an improper (right Haar) mixture over $\sigma$ and an improper (flat) mixture over $\mu$. Here, we elaborate carefully on the details of his construction, which use generalized nonintegrable martingales and an extended Ville's inequality. While this does yield a sequential t-test, it does not yield an ``e-process'' (due to the nonintegrability of his martingale). In this paper, we develop two new e-processes and confidence sequences for the same setting: one is a test martingale in a reduced filtration, while the other is an e-process in the canonical data filtration. These are respectively obtained by swapping Lai's flat mixture for a Gaussian mixture, and swapping the right Haar mixture over $\sigma$ with the maximum likelihood estimate under the null, as done in universal inference. We also analyze the width of resulting 
    
[^30]: RealFill：参考驱动的真实图像修复生成方法

    RealFill: Reference-Driven Generation for Authentic Image Completion. (arXiv:2309.16668v1 [cs.CV])

    [http://arxiv.org/abs/2309.16668](http://arxiv.org/abs/2309.16668)

    RealFill是一种个性化的生成修填模型，通过使用少量目标场景的参考图像，能够以真实、高质量、逼真的内容完成目标图像的修复。

    

    最近，生成图像的进展带来了能够在未知区域生成高质量、逼真图像内容的外拓和修填模型，但这些模型产生的内容是不真实的，因为模型缺乏关于真实场景的足够背景信息。在本文中，我们提出了一种新颖的真实图像修复生成方法RealFill，它通过填充图像中缺失区域使其内容真正应在的内容。RealFill是一种个性化的生成修填模型，仅使用几张目标场景的参考图像进行个性化。这些参考图像不需要与目标图像对齐，可以通过不同的视角、光照条件、摄像机光圈或图像风格拍摄。个性化后，RealFill能够以视觉上引人注目的内容完成目标图像，并且忠实于原始场景。我们在一个全面且具挑战性的图像修复基准上对RealFill进行评估。

    Recent advances in generative imagery have brought forth outpainting and inpainting models that can produce high-quality, plausible image content in unknown regions, but the content these models hallucinate is necessarily inauthentic, since the models lack sufficient context about the true scene. In this work, we propose RealFill, a novel generative approach for image completion that fills in missing regions of an image with the content that should have been there. RealFill is a generative inpainting model that is personalized using only a few reference images of a scene. These reference images do not have to be aligned with the target image, and can be taken with drastically varying viewpoints, lighting conditions, camera apertures, or image styles. Once personalized, RealFill is able to complete a target image with visually compelling contents that are faithful to the original scene. We evaluate RealFill on a new image completion benchmark that covers a set of diverse and challenging
    
[^31]: 语言模型作为视觉-语言模型的黑盒优化器

    Language Models as Black-Box Optimizers for Vision-Language Models. (arXiv:2309.05950v1 [cs.CL])

    [http://arxiv.org/abs/2309.05950](http://arxiv.org/abs/2309.05950)

    本论文介绍了一种新的视觉-语言模型 (VLMs) 微调方法，通过自然语言提示来避免访问模型参数，采用聊天式的语言模型作为黑盒优化器，在少样本图像分类任务中达到效果。

    

    预训练在大规模网络数据集上的视觉-语言模型 (VLMs) 展示了在各种视觉和多模态任务中的显著能力。目前，VLMs 的微调方法主要在白盒环境中操作，需要访问模型参数进行反向传播。然而，许多 VLMs 依赖于专有数据且不开源，限制了使用白盒方法进行微调。鉴于像 ChatGPT 这样的受欢迎私有大型语言模型 (LLMs) 仍然提供基于语言的用户界面，我们旨在通过自然语言提示开发一种新的 VLMs 微调方法，从而避免访问模型参数、特征嵌入或输出 logits 的需要。在这种设置下，我们提出使用基于聊天的 LLMs 作为黑盒优化器，以在使用 CLIP 进行少样本图像分类的示例任务中寻找最佳文本提示。具体而言，我们采用自动"爬山"程序，它能收敛到有效的提示上。

    Vision-language models (VLMs) pre-trained on web-scale datasets have demonstrated remarkable capabilities across a variety of vision and multimodal tasks. Currently, fine-tuning methods for VLMs mainly operate in a white-box setting, requiring access to model parameters for backpropagation. However, many VLMs rely on proprietary data and are not open-source, which restricts the use of white-box approaches for fine-tuning. Given that popular private large language models (LLMs) like ChatGPT still offer a language-based user interface, we aim to develop a novel fine-tuning approach for VLMs through natural language prompts, thereby avoiding the need to access model parameters, feature embeddings, or output logits. In this setup, we propose employing chat-based LLMs as black-box optimizers to search for the best text prompt on the illustrative task of few-shot image classification using CLIP. Specifically, we adopt an automatic "hill-climbing" procedure that converges on an effective prom
    
[^32]: DECODE: 用于检测极端质量比激发的扩张卷积神经网络

    DECODE: DilatEd COnvolutional neural network for Detecting Extreme-mass-ratio inspirals. (arXiv:2308.16422v1 [astro-ph.IM])

    [http://arxiv.org/abs/2308.16422](http://arxiv.org/abs/2308.16422)

    该论文介绍了一种名为DECODE的扩张卷积神经网络模型，用于检测极端质量比激发的信号。该模型通过在频域进行序列建模，并考虑时间延迟干涉仪以处理多通道TDI数据。

    

    由于其复杂的波形、持久的持续时间和低信噪比，极端质量比激发(EMRI)的检测是复杂的，这使得它们与紧凑的二进制融合相比更难被识别。虽然基于匹配滤波的技术以其计算要求而闻名，但现有的基于深度学习的方法主要处理时域数据，并且通常受到数据持续时间和信噪比的限制。此外，大多数现有工作忽略了时间延迟干涉仪(TDI)并在探测器响应计算中应用了长波近似，从而限制了其处理激光频率噪声的能力。在这项研究中，我们介绍了DECODE，这是一个以频域序列建模为重点的端到端模型，用于EMRI信号检测。DECODE围绕着一个以扩张因果卷积神经网络为中心，使用考虑到TDI-1.5探测器响应的合成数据进行训练，可以高效地处理一年的多通道TDI数据。

    The detection of Extreme Mass Ratio Inspirals (EMRIs) is intricate due to their complex waveforms, extended duration, and low signal-to-noise ratio (SNR), making them more challenging to be identified compared to compact binary coalescences. While matched filtering-based techniques are known for their computational demands, existing deep learning-based methods primarily handle time-domain data and are often constrained by data duration and SNR. In addition, most existing work ignores time-delay interferometry (TDI) and applies the long-wavelength approximation in detector response calculations, thus limiting their ability to handle laser frequency noise. In this study, we introduce DECODE, an end-to-end model focusing on EMRI signal detection by sequence modeling in the frequency domain. Centered around a dilated causal convolutional neural network, trained on synthetic data considering TDI-1.5 detector response, DECODE can efficiently process a year's worth of multichannel TDI data wi
    
[^33]: El Ni\~no Southern Oscillation多模态表示的多样性

    A multi-modal representation of El Ni\~no Southern Oscillation Diversity. (arXiv:2307.11552v1 [physics.ao-ph])

    [http://arxiv.org/abs/2307.11552](http://arxiv.org/abs/2307.11552)

    通过使用低维表示，在El Ni\~no Southern Oscillation（ENSO）中发现了新的极端El Ni\~no类别，并发现它们与典型EP El Ni\~no不同。EP El Ni\~nos，CP La Ni\~nas和Extreme El Ni\~nos对跨十年尺度的ENSO最具影响力。

    

    El Ni\~no Southern Oscillation (ENSO)通过赤道太平洋温暖（El Ni\~no）和寒冷（La Ni\~na）海表温度异常（SSTA）的交替阶段来描述。尽管El Ni\~no和La Ni\~na是明确定义的气候模式，但没有两个事件是相同的。迄今为止，ENSO多样性主要以SSTA峰值的经度位置来描述，用于在东太平洋（EP）和中太平洋（CP）类型中定义双峰分类。在这里，我们使用太平洋SSTA的低维表示来证明二进制分类成员对描述ENSO事件不合适。通过模糊无监督聚类，我们恢复了四个已知的ENSO类别，以及第五个类别：极端El Ni\~no。我们表明，极端El Ni\~nos在其强度和时间演化方面与典型的EP El Ni\~nos有所不同。我们还发现，CP La Ni\~nas，EP El Ni\~nos和Extreme El Ni\~nos对跨十年尺度的ENSO最具贡献性。

    The El Ni\~no-Southern Oscillation (ENSO) is characterized by alternating periods of warm (El Ni\~no) and cold (La Ni\~na) sea surface temperature anomalies (SSTA) in the equatorial Pacific. Although El Ni\~no and La Ni\~na are well-defined climate patterns, no two events are alike. To date, ENSO diversity has been described primarily in terms of the longitudinal location of peak SSTA, used to define a bimodal classification of events in Eastern Pacific (EP) and Central Pacific (CP) types. Here, we use low-dimensional representations of Pacific SSTAs to argue that binary categorical memberships are unsuitable to describe ENSO events. Using fuzzy unsupervised clustering, we recover the four known ENSO categories, along with a fifth category: an Extreme El Ni\~no. We show that Extreme El Ni\~nos differ both in their intensity and temporal evolution from canonical EP El Ni\~nos. We also find that CP La Ni\~nas, EP El Ni\~nos, and Extreme El Ni\~nos contribute the most to interdecadal ENSO
    
[^34]: 一种用于具有缺失值的整体生存分析的深度学习方法

    A Deep Learning Approach for Overall Survival Analysis with Missing Values. (arXiv:2307.11465v1 [cs.LG])

    [http://arxiv.org/abs/2307.11465](http://arxiv.org/abs/2307.11465)

    提出了一个深度学习模型，通过有效利用被审查和未被审查病人的信息，预测非小细胞肺癌（NSCLC）病人的整体生存。

    

    人工智能可以应用于肺癌研究，尤其是非小细胞肺癌（NSCLC），这是一个具有挑战性的领域。对于病人状态的整体生存（OS）是一个重要指标，可以帮助识别生存概率不同的亚组，从而实现个体化治疗和改善整体生存率。在这个分析中，需要考虑两个挑战。首先，很少有研究能够有效利用每个病人的可用信息，利用未被审查的（即死亡）和被审查的（即幸存者）病人的信息，也要考虑到死亡时间。其次，不完整数据处理是医学领域常见的问题。这个问题通常通过使用插补方法来解决。我们的目标是提出一个能够克服这些限制的人工智能模型，能够从被审查和未被审查的病人及其可用特征中有效学习，预测NSCLC病人的OS。

    One of the most challenging fields where Artificial Intelligence (AI) can be applied is lung cancer research, specifically non-small cell lung cancer (NSCLC). In particular, overall survival (OS) is a vital indicator of patient status, helping to identify subgroups with diverse survival probabilities, enabling tailored treatment and improved OS rates. In this analysis, there are two challenges to take into account. First, few studies effectively exploit the information available from each patient, leveraging both uncensored (i.e., dead) and censored (i.e., survivors) patients, considering also the death times. Second, the handling of incomplete data is a common issue in the medical field. This problem is typically tackled through the use of imputation methods. Our objective is to present an AI model able to overcome these limits, effectively learning from both censored and uncensored patients and their available features, for the prediction of OS for NSCLC patients. We present a novel 
    
[^35]: 非平滑非凸优化中随机次梯度方法的收敛性保证

    Convergence Guarantees for Stochastic Subgradient Methods in Nonsmooth Nonconvex Optimization. (arXiv:2307.10053v1 [math.OC])

    [http://arxiv.org/abs/2307.10053](http://arxiv.org/abs/2307.10053)

    本文研究了非平滑非凸优化中随机次梯度方法的收敛性质，并提出了一种新的框架，证明了其在单时间尺度和双时间尺度情况下的全局收敛性，包括了多种已知的SGD类型方法。对于有限和形式的目标函数，证明了这些方法能够在随机选择的步长和初始点上找到Clarke稳定点。

    

    本文研究了随机梯度下降（SGD）方法及其变种在训练由非平滑激活函数构建的神经网络中的收敛性质。我们提出了一种新颖的框架，为更新动量项和变量的步长分配了不同的时间尺度。在一些温和的条件下，我们证明了我们提出的框架在单时间尺度和双时间尺度情况下的全局收敛性。我们还证明了我们提出的框架包含了很多已知的SGD类型方法，包括heavy-ball SGD、SignSGD、Lion、normalized SGD和clipped SGD。此外，当目标函数采用有限和形式时，我们基于我们提出的框架证明了这些SGD类型方法的收敛性质。特别地，在温和的假设下，我们证明了这些SGD类型方法在随机选择的步长和初始点上能够找到目标函数的Clarke稳定点。

    In this paper, we investigate the convergence properties of the stochastic gradient descent (SGD) method and its variants, especially in training neural networks built from nonsmooth activation functions. We develop a novel framework that assigns different timescales to stepsizes for updating the momentum terms and variables, respectively. Under mild conditions, we prove the global convergence of our proposed framework in both single-timescale and two-timescale cases. We show that our proposed framework encompasses a wide range of well-known SGD-type methods, including heavy-ball SGD, SignSGD, Lion, normalized SGD and clipped SGD. Furthermore, when the objective function adopts a finite-sum formulation, we prove the convergence properties for these SGD-type methods based on our proposed framework. In particular, we prove that these SGD-type methods find the Clarke stationary points of the objective function with randomly chosen stepsizes and initial points under mild assumptions. Preli
    
[^36]: 通过随机Polyak步长的局部自适应联邦学习

    Locally Adaptive Federated Learning via Stochastic Polyak Stepsizes. (arXiv:2307.06306v1 [cs.LG])

    [http://arxiv.org/abs/2307.06306](http://arxiv.org/abs/2307.06306)

    本文将随机Polyak步长方法扩展到联邦学习，提出了新的局部自适应和几乎无需调参的FedSPS和FedDecSPS变体。我们证明了当插值条件满足时，FedSPS以线性速度收敛，一般情况下收敛到解的邻域。

    

    最先进的联邦学习算法，如FedAvg，需要精心调整的步长才能达到最佳性能。现有自适应联邦方法提出的改进仅涉及额外的超参数调整，如动量参数，并且仅考虑在服务器聚合轮次中的适应性，而不是局部的。这些方法在许多实际场景下效率低下，因为它们需要过多的超参数调整，并且不能捕捉局部几何信息。本文将最近提出的随机Polyak步长方法扩展到联邦学习环境，并提出了新的局部自适应和几乎无需调参的分布式SPS变体（FedSPS和FedDecSPS）。我们证明当插值条件（过参数化）满足时，FedSPS在强凸和凸设置中以线性速度收敛，一般情况下收敛到解的邻域。

    State-of-the-art federated learning algorithms such as FedAvg require carefully tuned stepsizes to achieve their best performance. The improvements proposed by existing adaptive federated methods involve tuning of additional hyperparameters such as momentum parameters, and consider adaptivity only in the server aggregation round, but not locally. These methods can be inefficient in many practical scenarios because they require excessive tuning of hyperparameters and do not capture local geometric information. In this work, we extend the recently proposed stochastic Polyak stepsize (SPS) to the federated learning setting, and propose new locally adaptive and nearly parameter-free distributed SPS variants (FedSPS and FedDecSPS). We prove that FedSPS converges linearly in strongly convex and sublinearly in convex settings when the interpolation condition (overparametrization) is satisfied, and converges to a neighborhood of the solution in the general case. We extend our proposed method t
    
[^37]: 分布感知的公平性测试生成

    Distribution-aware Fairness Test Generation. (arXiv:2305.13935v1 [cs.CV])

    [http://arxiv.org/abs/2305.13935](http://arxiv.org/abs/2305.13935)

    本文介绍了一种名为DistroFair的分布感知的公平性测试方法，可以从图像分类器中检测到类级别的公平性违规。

    

    本文探讨如何验证图像识别软件中的组公平性。我们提出了一种分布感知的公平性测试方法（称为DistroFair），通过将超出分布范围的对象引入到图像识别器中，通过三种语义保留图像变换 - 对象删除，对象插入和对象旋转来系统性地暴露图像分类器中的类级别公平性违规。我们使用两个知名数据集（CityScapes和MS-COCO）和三个主要的商业图像识别软件（即Amazon Rekognition，Google Cloud Vision和Azure计算机视觉）对DistroFair进行评估。结果显示，DistroFair生成的图像中，约有21％通过真实标准或元测试标准显露出了类级别的公平性违规。

    This work addresses how to validate group fairness in image recognition software. We propose a distribution-aware fairness testing approach (called DistroFair) that systematically exposes class-level fairness violations in image classifiers via a synergistic combination of out-of-distribution (OOD) testing and semantic-preserving image mutation. DistroFair automatically learns the distribution (e.g., number/orientation) of objects in a set of images. Then it systematically mutates objects in the images to become OOD using three semantic-preserving image mutations -- object deletion, object insertion and object rotation. We evaluate DistroFair using two well-known datasets (CityScapes and MS-COCO) and three major, commercial image recognition software (namely, Amazon Rekognition, Google Cloud Vision and Azure Computer Vision). Results show that about 21% of images generated by DistroFair reveal class-level fairness violations using either ground truth or metamorphic oracles. DistroFair 
    
[^38]: 最小化通信的异步张量并行性

    Communication-minimizing Asynchronous Tensor Parallelism. (arXiv:2305.13525v1 [cs.LG])

    [http://arxiv.org/abs/2305.13525](http://arxiv.org/abs/2305.13525)

    本文提出了Tensor3D，一种最小化通信消耗的三维张量计算并行化方法。它利用智能分布神经网络参数、新颖超分解方法以及通信模型，使训练速度提高了约3倍，GPU空闲时间降低了50％以上。

    

    随着现代神经网络规模扩大到数十亿个参数，设计能够在多GPU集群上高效训练这些网络的并行算法变得至关重要。本文提出了Tensor3D，一种全新的三维（3D）张量计算并行化方法，旨在最小化大型多十亿参数模型的并行训练中由通信引起的空闲时间。首先，我们引入了一种智能的神经网络参数分布方式，消除了为满足各层数据依赖而需要的通信。然后，我们提出了一种新颖的并行训练过程超分解方法，利用它可以显著提高通信与计算的重叠度，从而减少GPU空闲时间。最后，我们提出了一种通信模型，帮助用户为给定的神经网络识别通信最优的可用硬件资源分解。 对于256 A100 GPU上的28B参数CNN，在本文的 Tensor3D 方法下，训练速度提高了约3倍，与以前的方法相比 GPU 空闲时间也降低了约50％以上。

    As state-of-the-art neural networks scale to billions of parameters, designing parallel algorithms that can train these networks efficiently on multi-GPU clusters has become critical. This paper presents Tensor3D, a novel three-dimensional (3D) approach to parallelize tensor computations, that strives to minimize the idle time incurred due to communication in parallel training of large multi-billion parameter models. First, we introduce an intelligent distribution of neural network parameters across GPUs that eliminates communication required for satisfying data dependencies of individual layers. Then, we propose a novel overdecomposition of the parallel training process, using which we achieve significant overlap of communication with computation, thereby reducing GPU idle time. Finally, we present a communication model, which helps users identify communication optimal decompositions of available hardware resources for a given neural network. For a 28B parameter CNN on 256 A100 GPUs, 
    
[^39]: 一种用于遥感图像的十亿级基础模型

    A Billion-scale Foundation Model for Remote Sensing Images. (arXiv:2304.05215v1 [cs.CV])

    [http://arxiv.org/abs/2304.05215](http://arxiv.org/abs/2304.05215)

    本文介绍了一个用于遥感图像的十亿级基础模型，并研究了增加模型参数数量对该模型在下游任务中的性能影响，实验显示增加模型参数数量可以显著提高性能。

    

    随着基础模型在视觉任务中的潜力引起了广泛关注，先对这些模型进行预训练已成为一个关键步骤。预训练基础模型的三个关键因素是预训练方法、预训练数据集的大小以及模型参数的数量。最近，遥感领域的研究主要关注预训练方法和数据集的大小，对模型参数的数量关注较少。本文通过研究增加模型参数数量对基础模型在旋转目标检测和语义分割等下游任务中性能的影响来弥补这一空白。我们使用不同数量参数（包括86M、605.26M、1.3B和2.4B）的基础模型进行预训练，以确定参数增加是否会提高下游任务的性能。据我们所知，这是第一个用于遥感图像的十亿级基础模型。我们的实验表明，增加模型参数数量可以显著提高下游任务的性能。此外，我们还介绍了一个包含10亿个遥感图像的新的预训练数据集，并向研究社区公开。

    As the potential of foundation models in visual tasks has garnered significant attention, pretraining these models before downstream tasks has become a crucial step. The three key factors in pretraining foundation models are the pretraining method, the size of the pretraining dataset, and the number of model parameters. Recently, research in the remote sensing field has focused primarily on the pretraining method and the size of the dataset, with limited emphasis on the number of model parameters. This paper addresses this gap by examining the effect of increasing the number of model parameters on the performance of foundation models in downstream tasks such as rotated object detection and semantic segmentation. We pretrained foundation models with varying numbers of parameters, including 86M, 605.26M, 1.3B, and 2.4B, to determine whether performance in downstream tasks improved with an increase in parameters. To the best of our knowledge, this is the first billion-scale foundation mod
    
[^40]: 一种用于深度神经网络架构和超参数优化的算法框架

    An algorithmic framework for the optimization of deep neural networks architectures and hyperparameters. (arXiv:2303.12797v1 [cs.NE])

    [http://arxiv.org/abs/2303.12797](http://arxiv.org/abs/2303.12797)

    本文提出一种基于进化的有向无环图的算法框架，用于自动生成高效且灵活的深度神经网络并优化相关的超参数。此框架可用于任何能够处理混合搜索空间的元启发式算法，并在时间序列预测数据集上表现出比已有模型更好的性能。

    

    本文提出一种算法框架，用于自动生成高效的深度神经网络并优化相关的超参数。该框架基于进化的有向无环图(DAG)，定义了比文献中现有的搜索空间更为灵活的搜索空间，允许混合使用传统操作，如卷积、循环和密集层，以及较为新颖的操作，如自注意力机制。基于该搜索空间，我们提出了邻域搜索算子和演化搜索算子，以优化网络的架构和超参数。这些搜索算子可与任何能够处理混合搜索空间的元启发式算法一起使用。我们在时间序列预测数据集上使用进化算法测试了我们的算法框架。结果表明，我们的框架能够找到在许多数据集上性能优于基准模型的模型。

    In this paper, we propose an algorithmic framework to automatically generate efficient deep neural networks and optimize their associated hyperparameters. The framework is based on evolving directed acyclic graphs (DAGs), defining a more flexible search space than the existing ones in the literature. It allows mixtures of different classical operations: convolutions, recurrences and dense layers, but also more newfangled operations such as self-attention. Based on this search space we propose neighbourhood and evolution search operators to optimize both the architecture and hyper-parameters of our networks. These search operators can be used with any metaheuristic capable of handling mixed search spaces. We tested our algorithmic framework with an evolutionary algorithm on a time series prediction benchmark. The results demonstrate that our framework was able to find models outperforming the established baseline on numerous datasets.
    
[^41]: 通过子序列相似性生成序列：理论及其在无人机识别中的应用

    Sequence Generation via Subsequence Similarity: Theory and Application to UAV Identification. (arXiv:2301.08403v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.08403](http://arxiv.org/abs/2301.08403)

    本文探究了一种单次生成模型的多样性，主要聚焦于子序列相似性如何影响整个序列相似性，并通过生成子序列相似的序列来增强数据集。

    

    生成人工合成序列的能力在广泛的应用中至关重要，而深度学习架构和生成框架的最新进展已经极大地促进了这一过程。本文使用一种单次生成模型来采样，通过相似性生成子序列，并证明了子序列相似性对整个序列相似性的影响，给出了相应的界限。我们使用一种一次性生成模型来从单个序列的范围内取样，并生成子序列相似的序列，证明了数据集增强方面的实用性。

    The ability to generate synthetic sequences is crucial for a wide range of applications, and recent advances in deep learning architectures and generative frameworks have greatly facilitated this process. Particularly, unconditional one-shot generative models constitute an attractive line of research that focuses on capturing the internal information of a single image or video to generate samples with similar contents. Since many of those one-shot models are shifting toward efficient non-deep and non-adversarial approaches, we examine the versatility of a one-shot generative model for augmenting whole datasets. In this work, we focus on how similarity at the subsequence level affects similarity at the sequence level, and derive bounds on the optimal transport of real and generated sequences based on that of corresponding subsequences. We use a one-shot generative model to sample from the vicinity of individual sequences and generate subsequence-similar ones and demonstrate the improvem
    
[^42]: 无平滑非凸随机优化问题的更快无梯度算法

    Faster Gradient-Free Algorithms for Nonsmooth Nonconvex Stochastic Optimization. (arXiv:2301.06428v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2301.06428](http://arxiv.org/abs/2301.06428)

    本文提出了一种使用随机递归梯度估计器的更高效算法，来解决无平滑非凸随机优化问题，其复杂度为 $\mathcal{O}(L^3 d^{3/2} \epsilon^{-3}+ \Delta L^2 d^{3/2} \delta^{-1} \epsilon^{-3})$。

    

    本文考虑形如 $\min_{x \in \mathbb{R}^d} f(x) \triangleq \mathbb{E}_{\xi} [F(x; \xi)]$ 的优化问题，其中分量 $F(x;\xi)$ 是 $L$ 平均均方偏差的 Lipschitz 但可能是非凸非光滑函数。最近提出的无梯度方法最多需要 $\mathcal{O}( L^4 d^{3/2} \epsilon^{-4} + \Delta L^3 d^{3/2} \delta^{-1} \epsilon^{-4})$ 的随机零阶预处理器复杂度来找到目标函数的 $(\delta,\epsilon)$-Goldstein 静止点，其中 $\Delta = f(x_0) - \inf_{x \in \mathbb{R}^d} f(x)$，$x_0$ 是算法的初始点。本文提出了一种更高效的算法，使用随机递归梯度估计器，将复杂度改进为 $\mathcal{O}(L^3 d^{3/2} \epsilon^{-3}+ \Delta L^2 d^{3/2} \delta^{-1} \epsilon^{-3})$。

    We consider the optimization problem of the form $\min_{x \in \mathbb{R}^d} f(x) \triangleq \mathbb{E}_{\xi} [F(x; \xi)]$, where the component $F(x;\xi)$ is $L$-mean-squared Lipschitz but possibly nonconvex and nonsmooth. The recently proposed gradient-free method requires at most $\mathcal{O}( L^4 d^{3/2} \epsilon^{-4} + \Delta L^3 d^{3/2} \delta^{-1} \epsilon^{-4})$ stochastic zeroth-order oracle complexity to find a $(\delta,\epsilon)$-Goldstein stationary point of objective function, where $\Delta = f(x_0) - \inf_{x \in \mathbb{R}^d} f(x)$ and $x_0$ is the initial point of the algorithm. This paper proposes a more efficient algorithm using stochastic recursive gradient estimators, which improves the complexity to $\mathcal{O}(L^3 d^{3/2} \epsilon^{-3}+ \Delta L^2 d^{3/2} \delta^{-1} \epsilon^{-3})$.
    
[^43]: 一种简单高效的去中心化非凸极小化问题随机算法

    A Simple and Efficient Stochastic Algorithm for Decentralized Nonconvex-Strongly-Concave Minimax Optimization. (arXiv:2212.02387v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.02387](http://arxiv.org/abs/2212.02387)

    本文提出了一种称为去中心化递归梯度上升法（DREAM）的简单高效算法，用于解决去中心化非凸极小化问题，并实现了寻找原函数的 $\epsilon$-稳定点的最佳理论保证。

    

    本文研究了去中心化非凸极小化问题的随机优化。我们提出了一种简单高效的算法，称为去中心化递归梯度上升法（\texttt{DREAM}），它实现了寻找原函数的$\epsilon$-稳定点的最佳已知理论保证。在在线设置下，所提出的方法需要$\mathcal{O}(\kappa^3\epsilon^{-3})$随机一阶预言机（SFO）调用以及$\mathcal{O}\big(\kappa^2\epsilon^{-2}/\sqrt{1-\lambda_2(W)}\,\big)$通信轮次来找到$\epsilon$-稳定点，其中$\kappa$是条件数，$\lambda_2(W)$是八卦矩阵$W$的次大特征值。对于完全由$N$个分量函数组成的离线设置，所提出的方法需要$\mathcal{O}\big(\kappa^2 \sqrt{N} \epsilon^{-2}\big)$ SFO 调用和与在线设置相同的通信复杂度。

    This paper studies the stochastic optimization for decentralized nonconvex-strongly-concave minimax problem. We propose a simple and efficient algorithm, called Decentralized Recursive-gradient descEnt Ascent Method (\texttt{DREAM}), which achieves the best-known theoretical guarantee for finding the $\epsilon$-stationary point of the primal function. For the online setting, the proposed method requires $\mathcal{O}(\kappa^3\epsilon^{-3})$ stochastic first-order oracle (SFO) calls and $\mathcal{O}\big(\kappa^2\epsilon^{-2}/\sqrt{1-\lambda_2(W)}\,\big)$ communication rounds to find an $\epsilon$-stationary point, where $\kappa$ is the condition number and $\lambda_2(W)$ is the second-largest eigenvalue of the gossip matrix~$W$. For the offline setting with totally $N$ component functions, the proposed method requires $\mathcal{O}\big(\kappa^2 \sqrt{N} \epsilon^{-2}\big)$ SFO calls and the same communication complexity as the online setting.
    

