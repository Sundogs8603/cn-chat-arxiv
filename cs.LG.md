# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Deciphering Textual Authenticity: A Generalized Strategy through the Lens of Large Language Semantics for Detecting Human vs. Machine-Generated Text](https://rss.arxiv.org/abs/2401.09407) | 该论文通过研究在真实世界场景中检测机器生成文本的方法，发现现有方法对于不同生成器和领域产生的文本存在严重限制。 |
| [^2] | [ALOHa: A New Measure for Hallucination in Captioning Models](https://arxiv.org/abs/2404.02904) | 提出了一种新的用于测量图像字幕模型中幻觉的标准ALOHa，利用大型语言模型来测量幻觉对象，并成功识别比现有指标CHAIR更多的幻觉对象。 |
| [^3] | [DeiT-LT Distillation Strikes Back for Vision Transformer Training on Long-Tailed Datasets](https://arxiv.org/abs/2404.02900) | DeiT-LT通过引入一种有效的蒸馏方式，将CNN蒸馏到ViT中，以应对长尾数据集上训练ViT时的困难。 |
| [^4] | [Comment on "Machine learning conservation laws from differential equations"](https://arxiv.org/abs/2404.02896) | 评论了另一篇关于从微分方程中学习守恒定律的文章中存在的严重推导错误 |
| [^5] | [MODNO: Multi Operator Learning With Distributed Neural Operators](https://arxiv.org/abs/2404.02892) | 本文提出了一种针对多算子学习挑战的新型分布式训练方法，有效地实现单个神经算子处理多算子学习问题，而不增加额外平均成本。 |
| [^6] | [On the Scalability of Diffusion-based Text-to-Image Generation](https://arxiv.org/abs/2404.02883) | 本研究通过针对扩散型T2I模型进行消融实验，发现增加transformer块对于改善文本-图像对齐比增加通道数更具参数效率。 |
| [^7] | [Linear Attention Sequence Parallelism](https://arxiv.org/abs/2404.02882) | 提出了一种名为线性注意力序列并行（LASP）的高效序列并行方法，针对线性注意力的语言模型进行了优化，通过设计高效的点对点通信机制和执行内核融合来降低通信开销，并实现硬件友好性。 |
| [^8] | [Gaussian Process Regression with Soft Inequality and Monotonicity Constraints](https://arxiv.org/abs/2404.02873) | 引入类量子 Hamilton Monte Carlo 方法到不等式和单调性约束的高斯过程回归中，在概率意义上提高了模型准确性并减少了方差 |
| [^9] | [Human Activity Recognition using Smartphones](https://arxiv.org/abs/2404.02869) | 该论文通过智能手机的加速度计捕获不同日常活动的数据，提取特征并应用机器学习算法实现实时活动识别和卡路里消耗计算。 |
| [^10] | [Guarantees of confidentiality via Hammersley-Chapman-Robbins bounds](https://arxiv.org/abs/2404.02866) | 通过添加噪声到最后层的激活来保护隐私，使用HCR界限可量化保护机密性的可信度 |
| [^11] | [End-To-End Self-tuning Self-supervised Time Series Anomaly Detection](https://arxiv.org/abs/2404.02865) | 提出了TSAP方法来自动调整数据增强，为时间序列异常检测带来了端到端的自调节能力。 |
| [^12] | [Toward Inference-optimal Mixture-of-Expert Large Language Models](https://arxiv.org/abs/2404.02852) | MoE-based大型语言模型的研究强调了推理时间与专家数量之间的平衡问题，提出了引入推理效率作为缩放定律的调整方案 |
| [^13] | [BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models](https://arxiv.org/abs/2404.02827) | BAdam提出了一种内存高效的全参数微调大型语言模型的方法，并在实验中展现出优越的收敛行为以及在性能评估中的优势。 |
| [^14] | [Conifer: Improving Complex Constrained Instruction-Following Ability of Large Language Models](https://arxiv.org/abs/2404.02823) | Conifer提出了一个新的指令调节数据集，通过LLMs驱动的细化过程，以及渐进学习方案，显著提高了大型语言模型遵循具有复杂约束的多层指令的能力 |
| [^15] | [Identifying Climate Targets in National Laws and Policies using Machine Learning](https://arxiv.org/abs/2404.02822) | 本文提出了一种从国家法律和政策中提取气候目标的方法，可以可靠地识别出三类目标（“净零”，“减少”和“其他”），并调查了与模型相关的偏见和公平影响。 |
| [^16] | [Generative-Contrastive Heterogeneous Graph Neural Network](https://arxiv.org/abs/2404.02810) | 本研究提出了一种生成-对比异构图神经网络，通过对比视图增强策略、位置感知和语义感知正样本采样策略以及分层对比学习策略来克服图数据增强的限制。 |
| [^17] | [Domain Generalization through Meta-Learning: A Survey](https://arxiv.org/abs/2404.02785) | 元学习是一种有前景的方法，通过获取可转移知识实现在各种任务之间快速适应，为解决深度神经网络在面对分布变化和有限标记数据时泛化能力不佳提供了新途径。 |
| [^18] | [Federated Computing -- Survey on Building Blocks, Extensions and Systems](https://arxiv.org/abs/2404.02779) | 联邦计算旨在通过建立分布式网络，每个设备保留对自身数据的控制并参与集体计算，从而实现协作处理，其尊重用户隐私和数据主权的原则与当下对负责任AI和道德数据实践的需求相契合。 |
| [^19] | [AQuA -- Combining Experts' and Non-Experts' Views To Assess Deliberation Quality in Online Discussions Using LLMs](https://arxiv.org/abs/2404.02761) | 提出了AQuA，一种综合的磋商质量得分计算方法，可以从多个指标中提取各个讨论帖子的统一得分，保留了评论中磋商方面的信息，提高了模型的透明度。 |
| [^20] | [Unsupervised Occupancy Learning from Sparse Point Cloud](https://arxiv.org/abs/2404.02759) | 提出了一种从稀疏输入学习占用场的无监督学习方法，通过基于边界不确定性的采样和最小熵场优化来解决从3D点云学习SDF的挑战 |
| [^21] | [Continual Learning of Numerous Tasks from Long-tail Distributions](https://arxiv.org/abs/2404.02754) | 本文研究了在长尾分布下大量任务的不断学习算法表现，并提出了利用优化器状态改进不断学习性能的方法。 |
| [^22] | [Learning Sequence Attractors in Recurrent Networks with Hidden Neurons](https://arxiv.org/abs/2404.02729) | 本研究研究了具有隐藏神经元的递归网络如何学习序列吸引子，以稳健地存储和检索预定义的模式序列，结果表明网络需要包含隐藏神经元来存储任意模式序列，并开发了一种局部学习算法实现这一目标。 |
| [^23] | [Unsupervised Learning of Effective Actions in Robotics](https://arxiv.org/abs/2404.02728) | 该论文提出了一种无监督算法，通过在探索阶段将连续运动空间离散化, 自动生成“动作原型”, 从而实现机器人动作的效果驱动学习 |
| [^24] | [Harnessing the Power of Large Vision Language Models for Synthetic Image Detection](https://arxiv.org/abs/2404.02726) | 本研究旨在探讨利用先进的视觉语言模型进行合成图像识别，并通过调整图像字幕模型提高合成图像检测的准确性，为区分真实图像与合成图像做出贡献。 |
| [^25] | [On-line conformalized neural networks ensembles for probabilistic forecasting of day-ahead electricity prices](https://arxiv.org/abs/2404.02722) | 提出了一种通过基于符合推理技术的在线重新校准程序扩展最新技术的神经网络集成方法，用于改进日前电力价格的概率预测。 |
| [^26] | [Can We Understand Plasticity Through Neural Collapse?](https://arxiv.org/abs/2404.02719) | 论文探讨了深度学习中可塑性损失和神经坍塌的关联，并引入了一种正则化方法来缓解神经坍塌，从而减轻了可塑性损失。 |
| [^27] | [Automatic Prompt Selection for Large Language Models](https://arxiv.org/abs/2404.02717) | 通过在训练数据上进行聚类，使用基于LLM的提示生成器为每个簇生成候选提示，综合数据集进行训练以评估提示的相关性，最终在测试时使用评估器为新输入选择最佳提示，实现了大型语言模型的自动提示选择。 |
| [^28] | [Deep Privacy Funnel Model: From a Discriminative to a Generative Approach with an Application to Face Recognition](https://arxiv.org/abs/2404.02696) | 该研究将信息论隐私原则与表示学习相结合，提出了一种新的隐私保护表示学习方法，适用于人脸识别系统，并引入了生成式隐私漏斗模型。 |
| [^29] | [Automated Inference of Graph Transformation Rules](https://arxiv.org/abs/2404.02692) | 提出了一种新颖的图形转换模型构建方法，结合生成和动态观点，实现完全自动化的数据驱动模型推理，通过压缩一组转换成一组规则，允许模型展示超出输入范围的行为。 |
| [^30] | [Attention is Naturally Sparse with Gaussian Distributed Input](https://arxiv.org/abs/2404.02690) | 通过对高斯输入下注意力得分稀疏性进行理论分析，揭示了注意力机制中稀疏性的特征及其对计算效率的影响。 |
| [^31] | [Reinforcement Learning in Categorical Cybernetics](https://arxiv.org/abs/2404.02688) | 强化学习算法可以被归纳到分类控制原理框架中，通过参数化的光学相互作用，展示了新的构造方法。 |
| [^32] | [Cross-Architecture Transfer Learning for Linear-Cost Inference Transformers](https://arxiv.org/abs/2404.02684) | 提出了一种跨架构迁移学习方法，用于在线性成本推断和自注意力变换器之间共享组件的权重，以提高Transformer语言模型的效率。 |
| [^33] | [Adversarial Attacks and Dimensionality in Text Classifiers](https://arxiv.org/abs/2404.02660) | 在文本分类任务中研究对抗攻击，发现对抗样本的嵌入维度与其影响之间存在非常强烈的相关性。 |
| [^34] | [Towards detecting unanticipated bias in Large Language Models](https://arxiv.org/abs/2404.02650) | 本论文探索了在大型语言模型中检测未预料到的偏见的新途径，着重于不确定性量化和可解释人工智能方法。 |
| [^35] | [On the Importance of Uncertainty in Decision-Making with Large Language Models](https://arxiv.org/abs/2404.02649) | 本研究调查了使用大型语言模型作为代理进行自然语言输入决策问题时，不确定性估计的重要性，并提出了集成不确定性估计到汤普森抽样策略的方法。 |
| [^36] | [Effector: A Python package for regional explanations](https://arxiv.org/abs/2404.02629) | Effector是一个专注于区域特征效果的Python软件包，通过引入区域效果来降低全局特征效果方法中可能的异质性。 |
| [^37] | [A Differentiable Integer Linear Programming Solver for Explanation-Based Natural Language Inference](https://arxiv.org/abs/2404.02625) | Diff-Comb Explainer是一种基于可微黑盒组合求解器的神经符号架构，不需要对语义约束进行连续放松，相比传统解决方案表现更出色。 |
| [^38] | [Polynomial Graphical Lasso: Learning Edges from Gaussian Graph-Stationary Signals](https://arxiv.org/abs/2404.02621) | 从高斯图形平稳信号中学习边缘的一种新方法，多项式图形Lasso（PGL），通过将图形Lasso的优势与更全面模型相结合，在建模节点关系时提供了更大的灵活性，并通过低复杂度算法解决了复杂和非凸的优化问题，评估表明其优于几种替代方法。 |
| [^39] | [QFNN-FFD: Quantum Federated Neural Network for Financial Fraud Detection](https://arxiv.org/abs/2404.02595) | 介绍了将量子机器学习和量子计算技术与联邦学习相结合的Quantum Federated Neural Network for Financial Fraud Detection (QFNN-FFD)框架，提出了一种安全、高效的欺诈交易识别方法，显著改进了欺诈检测并确保了数据机密性。 |
| [^40] | [Adaptive Sampling Policies Imply Biased Beliefs: A Generalization of the Hot Stove Effect](https://arxiv.org/abs/2404.02591) | 学习者倾向于估计值为正的替代方案，避开估计值为负的替代方案，导致高估误差纠正但低估误差无法纠正，同时研究发现在一些情况下负估计会导致选择较少的样本量，这种消极偏见同样存在于贝叶斯学习者中。 |
| [^41] | [Transformer-based Stagewise Decomposition for Large-Scale Multistage Stochastic Optimization](https://arxiv.org/abs/2404.02583) | TranSDDP是一种基于Transformer的创新分阶分解算法，通过使用Transformer模型的结构优势，实现了一种序贯方法来逼近值函数，在处理大规模多阶段随机优化问题中表现出有效性。 |
| [^42] | [Solving a Real-World Optimization Problem Using Proximal Policy Optimization with Curriculum Learning and Reward Engineering](https://arxiv.org/abs/2404.02577) | 使用课程学习和奖励工程的近端策略优化来解决实际问题。 |
| [^43] | [Incremental Learning with Concept Drift Detection and Prototype-based Embeddings for Graph Stream Classification](https://arxiv.org/abs/2404.02572) | 这项工作提出了一种新颖的图流分类方法，通过增量学习实现持续模型适应，选择每个类别的代表性图，并创建图嵌入，以解决图流分类中的概念漂移问题。 |
| [^44] | [An Interpretable Power System Transient Stability Assessment Method with Expert Guiding Neural-Regression-Tree](https://arxiv.org/abs/2404.02555) | 提出了一种利用专家引导非线性回归树来近似神经网络预测的、旨在解决网络解决方案可解释性问题的电力系统暂态稳定性评估方法。 |
| [^45] | [Grid-Mapping Pseudo-Count Constraint for Offline Reinforcement Learning](https://arxiv.org/abs/2404.02545) | 提出了一种用于连续领域的新的计数方法，称为格点映射伪计数方法（GPC），以适应离线环境中的强化学习问题，并在惩罚Q值的同时减少计算成本。 |
| [^46] | [Convergence Analysis of Flow Matching in Latent Space with Transformers](https://arxiv.org/abs/2404.02538) | 该研究提出了一种基于ODE的生成模型中，使用Transformer实现流匹配在潜空间中的理论收敛性保证，并展示了其在估计ODE流生成样本分布时的有效性，同时还证明了具有利普希茨连续性的Transformer网络可以有效逼近任意光滑函数。 |
| [^47] | [Stochastic Constrained Decentralized Optimization for Machine Learning with Fewer Data Oracles: a Gradient Sliding Approach](https://arxiv.org/abs/2404.02511) | 提出了一种原始-对偶滑动与条件梯度滑动框架，实现了通信高效和最小化梯度计算次数的分散式优化算法，具有优化的梯度和线性优化复杂度。 |
| [^48] | [An Interpretable Client Decision Tree Aggregation process for Federated Learning](https://arxiv.org/abs/2404.02510) | 提出了一种用于联邦学习的可解释客户端决策树聚合过程，旨在解决在这些模型中注入可解释性的挑战。 |
| [^49] | [VIAssist: Adapting Multi-modal Large Language Models for Users with Visual Impairments](https://arxiv.org/abs/2404.02508) | 本论文研究了如何利用多模大语言模型（MLLMs）为视障人士提供视觉问题答案，提出了 VIAssist 系统，可以识别不受欢迎的图像并提供详细的操作，最终依据用户查询提供可靠的答案。 |
| [^50] | [Learning Generalized Policies for Fully Observable Non-Deterministic Planning Domains](https://arxiv.org/abs/2404.02499) | 本研究扩展了学习完全可观察、非确定性计划领域的泛化策略的方法，并通过实验评估了在一些 FOND 计划基准领域中产生的泛化策略的正确性。 |
| [^51] | [Measuring Social Norms of Large Language Models](https://arxiv.org/abs/2404.02491) | 论文提出了一个挑战，测试大型语言模型对社会规范的理解，构建了一个数据集涵盖广泛的社会规范问题，通过多代理框架基于大型语言模型来提高模型对社会规范的理解能力。 |
| [^52] | [New methods for drug synergy prediction](https://arxiv.org/abs/2404.02484) | 最佳方法准确解决了涉及已知药物或细胞系的药物协同作用预测情景，但仍未达到准确预测新药物或细胞系的水平。 |
| [^53] | [FedSelect: Personalized Federated Learning with Customized Selection of Parameters for Fine-Tuning](https://arxiv.org/abs/2404.02478) | 提出了一种受到彩票票据假设启发的新型个性化联邦学习算法FedSelect，能够在微调中定制选择网络参数，解决了全局知识存储不够优化的问题。 |
| [^54] | [Deep Reinforcement Learning for Traveling Purchaser Problems](https://arxiv.org/abs/2404.02476) | 提出了一种基于深度强化学习的方法，该方法分别解决了旅行购买者问题中的路由构建和购买规划问题，并从全局角度评估和优化解决方案。 |
| [^55] | [uTeBC-NLP at SemEval-2024 Task 9: Can LLMs be Lateral Thinkers?](https://arxiv.org/abs/2404.02474) | 通过研究提示工程方法如何增强LLMs在横向思考任务上的表现，揭示了其固有的超越思维能力，并发现压缩的信息性提示和动态的情境学习显著提升了模型性能。 |
| [^56] | [On the Efficiency and Robustness of Vibration-based Foundation Models for IoT Sensing: A Case Study](https://arxiv.org/abs/2404.02461) | 振动基础模型通过未标记数据的预训练，提高了在物联网应用中运行时推断的稳健性，并能够在部署中细化调整，加快应用部署时间并提高模型性能。 |
| [^57] | [PhonologyBench: Evaluating Phonological Skills of Large Language Models](https://arxiv.org/abs/2404.02456) | PhonologyBench是一个新颖的基准测试，旨在明确评估大型语言模型在英语中的音韵技能，展示了LLMs在没有语音数据情况下在PhonologyBench任务上表现出显著性能。 |
| [^58] | [Task Agnostic Architecture for Algorithm Induction via Implicit Composition](https://arxiv.org/abs/2404.02450) | 探索开发一种统一架构，旨在解决各种任务，包括以前未见过的任务，并使用跨多种模式的输入。 |
| [^59] | [Electric Vehicle Routing Problem for Emergency Power Supply: Towards Telecom Base Station Relief](https://arxiv.org/abs/2404.02448) | 提出一种新型的基于电动车辆的路径问题，通过组合基于规则的车辆选择器和基于强化学习的节点选择器解决电动车辆路径问题，以最小化总行驶距离和故障基站数量。 |
| [^60] | [Masked Completion via Structured Diffusion with White-Box Transformers](https://arxiv.org/abs/2404.02446) | 该论文提出了一种可以应用于大规模无监督表示学习的白盒设计范例。 |
| [^61] | [From Narratives to Numbers: Valid Inference Using Language Model Predictions from Verbal Autopsy Narratives](https://arxiv.org/abs/2404.02438) | 该论文提出了一种利用最先进的NLP技术从口述验尸文本中预测死因并进行有效推断的方法。 |
| [^62] | [AD4RL: Autonomous Driving Benchmarks for Offline Reinforcement Learning with Value-based Dataset](https://arxiv.org/abs/2404.02429) | 本文提供了用于离线强化学习研究的自动驾驶数据集和基准测试，包括真实世界人类驾驶员的数据集，以及七种离线强化学习算法在三种实际驾驶场景中的应用，并提供了一个统一决策模型作为算法设计的参考框架。 |
| [^63] | [RESSA: Repair Sparse Vision-Language Models via Sparse Cross-Modality Adaptation](https://arxiv.org/abs/2404.02424) | 通过稀疏跨模态适应修复稀疏视觉-语言模型，探索了VLM修剪中的两个主要问题，提出稀疏比率对性能的影响，展示了修复稀疏VLMs性能所需的专门技术。 |
| [^64] | [Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data](https://arxiv.org/abs/2404.02422) | 提出了一种策略，通过PEFT和合成数据增强低资源LLMs分类器，实现了与0-shot文本分类器相媲美或更好的准确性。 |
| [^65] | [Decision Transformer as a Foundation Model for Partially Observable Continuous Control](https://arxiv.org/abs/2404.02407) | 通过将控制任务作为基于过去观察、动作和奖励的当前最优动作预测来消除估计器设计需求，并利用生成式预训练Transformer系列初始化决策Transformer，然后使用低秩适应对其进行控制任务训练。 |
| [^66] | [Benchmarking Large Language Models for Persian: A Preliminary Study Focusing on ChatGPT](https://arxiv.org/abs/2404.02403) | 评估了波斯语大型语言模型在不同任务上的表现，引入了推理任务方面的新基准测试，发现LLMs在推理任务中表现优异。 |
| [^67] | [Token Trails: Navigating Contextual Depths in Conversational AI with ChatLLM](https://arxiv.org/abs/2404.02402) | Token Trails是一种利用token-type嵌入导航对话中复杂上下文细微差别的新方法，通过提高对话理解和回复生成效果，在促进上下文意识聊天机器人交互方面具有前沿性能。 |
| [^68] | [Enhancing Diffusion-based Point Cloud Generation with Smoothness Constraint](https://arxiv.org/abs/2404.02396) | 提出利用局部平滑约束在扩散框架中增强点云生成，实验结果表明该模型可以生成逼真且更加平滑的点云，优于多种最先进的方法。 |
| [^69] | [Optimal Batch Allocation for Wireless Federated Learning](https://arxiv.org/abs/2404.02395) | 分析了在无线联邦学习中实现特定优越性差距所需的迭代次数，并提出了适用于基于TDMA的系统的最佳批量分配方法。此外，还展示了提出的分步批量分配方法可以显著减少基于RA的学习系统的完成时间 |
| [^70] | [An inversion problem for optical spectrum data via physics-guided machine learning](https://arxiv.org/abs/2404.02387) | 提出了正则化递归推理机(rRIM)，一种通过物理引导的机器学习方法，用于从实验光谱中推导配对粘合函数，具有噪声鲁棒性和灵活性，可解决第一类Fredholm积分方程的类似反问题。 |
| [^71] | [Faster Convergence of Stochastic Accelerated Gradient Descent under Interpolation](https://arxiv.org/abs/2404.02378) | 该论文证明了在插值条件下对随机加速的一般化版本的新收敛速度，在强增长条件下的加速SGD中取得了显著改进。 |
| [^72] | [Obfuscated Malware Detection: Investigating Real-world Scenarios through Memory Analysis](https://arxiv.org/abs/2404.02372) | 通过内存分析，利用机器学习算法提出了一种简单且成本效益的模糊恶意软件检测系统，重点评估其在真实场景中的效果。 |
| [^73] | [Learning Intersections of Halfspaces with Distribution Shift: Improved Algorithms and SQ Lower Bounds](https://arxiv.org/abs/2404.02364) | 研究学习与分布偏移的交集问题，在基于高斯训练分布的情况下，证明了一系列新的上界，包括一种TDS学习$k$个齐次半空间交集达到精度$\epsilon$的$2^{(k/\epsilon)^{O(1)}} \mathsf{poly}(d)$时间算法（在先前的工作中）。 |
| [^74] | [FraGNNet: A Deep Probabilistic Model for Mass Spectrum Prediction](https://arxiv.org/abs/2404.02360) | FraGNNet是一种用于化合物到质谱预测的深度概率模型，能够高效准确地预测高分辨率谱，在性能上超越了现有的模型 |
| [^75] | [Attribution Regularization for Multimodal Paradigms](https://arxiv.org/abs/2404.02359) | 提出一种新的正则化项，鼓励多模态模型有效利用所有模态信息，以解决多模态学习中单模态模型优于多模态模型的问题。 |
| [^76] | [Semantic Augmentation in Images using Language](https://arxiv.org/abs/2404.02353) | 深度学习模型需要大规模标记数据集，本文提出利用生成图像增强数据集以改进模型跨领域泛化能力。 |
| [^77] | [Improved model-free bounds for multi-asset options using option-implied information and deep learning](https://arxiv.org/abs/2404.02343) | 利用深度学习和期权隐含信息改进多资产期权的无模型上界计算方法 |
| [^78] | [Heat Death of Generative Models in Closed-Loop Learning](https://arxiv.org/abs/2404.02325) | 生成模型的闭环训练过程容易产生退化现象，模型可能开始生成无意义的数据或仅从所需数据分布的一小部分中采样。 |
| [^79] | [Prompts As Programs: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization](https://arxiv.org/abs/2404.02319) | 提出了SAMMO框架，用于在编译时优化元提示程序，提高了复杂提示在多种不同LLM上的性能。 |
| [^80] | [Is Meta-training Really Necessary for Molecular Few-Shot Learning ?](https://arxiv.org/abs/2404.02314) | 本文重新审视了分子数据微调方法，提出了基于马氏距离的正则化二次探针损失，并设计了块坐标下降优化器，使得在黑匣子设置下，简单微调方法在少样本学习中获得了竞争性表现，同时消除了特定预训练策略的需要。 |
| [^81] | [Virtual Sensor for Real-Time Bearing Load Prediction Using Heterogeneous Temporal Graph Neural Networks](https://arxiv.org/abs/2404.02304) | 提出了一种使用Graph Neural Networks分析空间-时间依赖关系的图神经网络虚拟传感器，能够从传感器滚轮数据中学习，将操作条件映射为轴承负载。 |
| [^82] | [CATGNN: Cost-Efficient and Scalable Distributed Training for Graph Neural Networks](https://arxiv.org/abs/2404.02300) | CATGNN是一种成本有效且可扩展的分布式GNN训练系统，通过接受边流作为输入并提出名为SPRING的流式分区算法，实现将GNN训练扩展到数十亿以上规模的图中。 |
| [^83] | [Constrained Robotic Navigation on Preferred Terrains Using LLMs and Speech Instruction: Exploiting the Power of Adverbs](https://arxiv.org/abs/2404.02294) | 使用大型语言模型和语音指令进行约束机器人导航，减少地图依赖，利用生成式人工智能提取关键信息，通过文本生成遮罩识别地标和地形类型，实现对复杂地形的高效导航。 |
| [^84] | [Federated Multi-Agent Mapping for Planetary Exploration](https://arxiv.org/abs/2404.02289) | 联邦学习在多智能体机器人探测中的应用，利用隐式神经映射和地球数据集上的元初始化，实现了对不同领域如火星地形和冰川的强泛化能力。 |
| [^85] | [LLMs in the Loop: Leveraging Large Language Model Annotations for Active Learning in Low-Resource Languages](https://arxiv.org/abs/2404.02261) | 在低资源语言中，通过将LLMs集成到主动学习循环中进行数据注释，有效减少所需数据量，并取得接近最先进性能的结果。 |
| [^86] | [Mixture-of-Depths: Dynamically allocating compute in transformer-based language models](https://arxiv.org/abs/2404.02258) | 本研究提出了一种新的方法，即Mixture-of-Depths，可以在Transformer的语言模型中动态分配FLOPs以优化模型深度上不同层的序列分配。 |
| [^87] | [On Stronger Computational Separations Between Multimodal and Unimodal Machine Learning](https://arxiv.org/abs/2404.02254) | 提出了更强的平均情况计算分离，对于“典型”情况下的学习任务实例，单模态学习在计算上是困难的，但多模态学习却很容易。 |
| [^88] | [RAT: Retrieval-Augmented Transformer for Click-Through Rate Prediction](https://arxiv.org/abs/2404.02249) | RAT模型是为了解决当前CTR预测模型仅关注样本内特征交互而忽略跨样本关系的问题，通过检索相似样本构建增强输入，实现了对样本内和跨样本的全面特征交互推理，提高了CTR预测的效果。 |
| [^89] | [Proximal Oracles for Optimization and Sampling](https://arxiv.org/abs/2404.02239) | 论文研究了具有非光滑函数和对数凹抽样的凸优化问题，提出了在优化和抽样中应用近端框架的方法，并建立了近端映射的迭代复杂度。 |
| [^90] | [Is Exploration All You Need? Effective Exploration Characteristics for Transfer in Reinforcement Learning](https://arxiv.org/abs/2404.02235) | 探索特征与深度强化学习中有效的转移学习的关系，尚未被明确表征。研究试图理解探索特征与改进性能和效率在转移学习中的关系 |
| [^91] | [Deep Neural Networks with 3D Point Clouds for Empirical Friction Measurements in Hydrodynamic Flood Models](https://arxiv.org/abs/2404.02234) | 该研究提出了一种利用深度神经网络和点云数据测量Manning's n的方法，不仅在实验室环境中取得了较好的结果，而且在真实世界的监管和极端暴雨事件下也表现出了更好的预测能力。 |
| [^92] | [OOSTraj: Out-of-Sight Trajectory Prediction With Vision-Positioning Denoising](https://arxiv.org/abs/2404.02227) | 提出了一种利用视觉定位技术进行视界轨迹预测的新方法，可以有效地解决视野外物体和传感器数据噪声的挑战，达到了最先进的性能。 |
| [^93] | [Emergent Abilities in Reduced-Scale Generative Language Models](https://arxiv.org/abs/2404.02204) | 减小规模数据训练的较小语言模型展示了增强的零样本能力，可在简化语言中实现与大型模型相当的性能。 |
| [^94] | [Insights from the Use of Previously Unseen Neural Architecture Search Datasets](https://arxiv.org/abs/2404.02189) | 提出了八个新的神经架构搜索数据集，旨在引起在NAS开发中的关注并鼓励作者考虑模型在开发时未知数据集上的表现。 |
| [^95] | [A Generative Deep Learning Approach for Crash Severity Modeling with Imbalanced Data](https://arxiv.org/abs/2404.02187) | 使用生成式深度学习方法处理事故严重性建模中的数据不平衡问题，解决传统和基于深度学习的数据重采样方法难以处理离散风险因素崩溃问题的挑战。 |
| [^96] | [What is to be gained by ensemble models in analysis of spectroscopic data?](https://arxiv.org/abs/2404.02184) | 集成模型在光谱数据分析中表现出优异的预测性能，能够持续优于其他候选模型 |
| [^97] | [Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization](https://arxiv.org/abs/2404.02183) | 提出了自组织多代理框架（SoA），实现了大规模代码的可扩展高效生成和优化，代理可自主运作生成和修改代码组件，并根据问题复杂性动态增加数量。 |
| [^98] | [Leveraging Machine Learning for Early Autism Detection via INDT-ASD Indian Database](https://arxiv.org/abs/2404.02181) | 本研究旨在通过利用机器学习开发一种简单、快速、廉价的技术，利用INDT-ASD印度数据库早期检测自闭症。 |
| [^99] | [Remote sensing framework for geological mapping via stacked autoencoders and clustering](https://arxiv.org/abs/2404.02180) | 通过堆叠自动编码器和聚类实现遥感数据地质制图的无监督机器学习框架 |
| [^100] | [Exploring Quantum-Enhanced Machine Learning for Computer Vision: Applications and Insights on Noisy Intermediate-Scale Quantum Devices](https://arxiv.org/abs/2404.02177) | 本研究探讨了量子计算和机器学习的交叉点，重点关注计算机视觉任务，评估了混合量子-经典算法对小规模量子设备的有效性 |
| [^101] | [Social Dynamics of Consumer Response: A Unified Framework Integrating Statistical Physics and Marketing Dynamics](https://arxiv.org/abs/2404.02175) | 使用统计物理学和市场营销动态的理论框架，本研究提出了一个创新的方程，准确捕捉了广告支出与消费者反应之间的复杂关系，并验证了其有效性。 |
| [^102] | [Path planning of magnetic microswimmers in high-fidelity simulations of capillaries with deep reinforcement learning](https://arxiv.org/abs/2404.02171) | 本研究通过深度强化学习训练的强化学习代理，在复杂的毛细血管网络中成功地实现了磁性微游泳器的路径规划。 |
| [^103] | [AUTODIFF: Autoregressive Diffusion Modeling for Structure-based Drug Design](https://arxiv.org/abs/2404.02003) | 提出了一种基于扩散的分段自回归生成模型AUTODIFF，其中包括一种名为conformal motif的新型分子组装策略和SE(3)-等变卷积网络编码蛋白质-配体复合物相互作用的方法，能够解决结构基药物设计中的局部结构和构象问题。 |
| [^104] | [Utilizing Maximum Mean Discrepancy Barycenter for Propagating the Uncertainty of Value Functions in Reinforcement Learning](https://arxiv.org/abs/2404.00686) | 这项工作引入了最大均值差异Q学习（MMD-QL）来改进强化学习中价值函数不确定性的传播，通过使用MMD重心，实现了比Wasserstein距离更紧的概率度量，在实验中表现优于其他算法，并结合深度网络创造了MMD Q网络（MMD-QN）。 |
| [^105] | [InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning](https://arxiv.org/abs/2404.00228) | InfLoRA提出了一种新的PEFT方法，名为无干扰低秩自适应（InfLoRA），用于持续学习，旨在消除新任务对旧任务的干扰，帮助模型在稳定性和可塑性之间取得良好平衡。 |
| [^106] | [Modeling Large-Scale Walking and Cycling Networks: A Machine Learning Approach Using Mobile Phone and Crowdsourced Data](https://arxiv.org/abs/2404.00162) | 该研究使用机器学习方法结合手机和众包数据，建立了一个模型来估算大规模区域网络中的步行和骑行量，讨论了在模型训练、测试和推断中面临的挑战和限制。 |
| [^107] | [Empowering Credit Scoring Systems with Quantum-Enhanced Machine Learning](https://arxiv.org/abs/2404.00015) | 提出了一种名为Systemic Quantum Score (SQS)的新方法，展示在金融领域生产级应用案例中相比纯经典模型更有优势，能够从较少数据点中提取模式并表现出更好性能。 |
| [^108] | [Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science](https://arxiv.org/abs/2403.20208) | 本研究旨在利用大型语言模型解决数据科学中表格数据预测任务，通过在丰富的数据集上训练Llama-2模型并进行实际应用，取得显著的改进。 |
| [^109] | [Gegenbauer Graph Neural Networks for Time-varying Signal Reconstruction](https://arxiv.org/abs/2403.19800) | 提出了一种新颖的Gegenbauer-based graph convolutional (GegenConv)算子，用于提高时变信号重构的准确性 |
| [^110] | [Understanding the Learning Dynamics of Alignment with Human Feedback](https://arxiv.org/abs/2403.18742) | 本研究对人类偏好对齐的学习动态进行了理论分析，显示了偏好数据集的分布如何影响模型更新速度，并提供了对训练准确度的严格保证，同时揭示了优化易于优先考虑高偏好可区分性行为的复杂现象。 |
| [^111] | [Tensor-based Graph Learning with Consistency and Specificity for Multi-view Clustering](https://arxiv.org/abs/2403.18393) | 本文提出了一种新颖的基于张量的图学习框架，同时考虑了多视图聚类的一致性和特异性，解决了现有方法在相似性测量和图信息利用方面的局限性。 |
| [^112] | [Horoballs and the subgradient method](https://arxiv.org/abs/2403.15749) | 该论文提出了一种在Hadamard空间中进行凸优化的迭代方法，与传统假设不同，其复杂性不依赖于空间曲率的下界。 |
| [^113] | [Grey-informed neural network for time-series forecasting](https://arxiv.org/abs/2403.15027) | 本研究提出了灰色信息神经网络（GINN），通过遵循灰色系统的微分方程模型，提高了神经网络输出的可解释性，使其能够有效处理小数据样本，产生可靠的预测。 |
| [^114] | [MolBind: Multimodal Alignment of Language, Molecules, and Proteins](https://arxiv.org/abs/2403.08167) | MolBind 提出了一个框架，通过对比学习为多种模态训练编码器，将所有模态映射到共享特征空间，实现多模态语义对齐。 |
| [^115] | [Generative Probabilistic Forecasting with Applications in Market Operations](https://arxiv.org/abs/2403.05743) | 提出了一种基于Wiener-Kallianpur创新表示的生成式概率预测方法，包括自编码器和新颖的深度学习算法，具有渐近最优性和结构收敛性质，适用于实时市场运营中的高动态和波动时间序列。 |
| [^116] | [$\mathtt{tsGT}$: Stochastic Time Series Modeling With Transformer](https://arxiv.org/abs/2403.05713) | $\mathtt{tsGT}$是一种基于通用Transformer架构的随机时间序列模型，表现优于最先进模型，并超过其随机同行，特别在数据分布建模和边际分位值预测方面具备优势。 |
| [^117] | [Privacy-Aware Semantic Cache for Large Language Models](https://arxiv.org/abs/2403.02694) | MeanCache是一种面向LLMs的语义缓存，能够识别语义上相似的查询，从而减少查询成本，服务提供商负载和环境影响。 |
| [^118] | [Global and Local Prompts Cooperation via Optimal Transport for Federated Learning](https://arxiv.org/abs/2403.00041) | 提出了联邦提示合作 via Optimal Transport（FedOTP）方法，通过最优输运实现全局和本地提示的合作，针对数据异质性设计了高效的协作提示学习策略。 |
| [^119] | [Time Series Analysis in Compressor-Based Machines: A Survey](https://arxiv.org/abs/2402.17802) | 该论文调查了应用于压缩机设备运行多变量时间序列的故障检测、故障预测、预测和变点检测等任务的最新研究。 |
| [^120] | [OSCaR: Object State Captioning and State Change Representation](https://arxiv.org/abs/2402.17128) | 本文介绍了一个新的数据集和基准OSCaR，旨在解决描述复杂视觉环境中对象状态变化的问题，为评估多模态大型语言提供了一个新的实验平台。 |
| [^121] | [Quaternion recurrent neural network with real-time recurrent learning and maximum correntropy criterion](https://arxiv.org/abs/2402.14227) | 通过结合实时递归学习算法和最大相关性准则作为损失函数，提出了用于处理含异常值3D和4D数据的鲁棒四元数递归神经网络，所使用的最大相关性损失函数对异常值不太敏感，适用于多维嘈杂或不确定数据应用。 |
| [^122] | [Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!](https://arxiv.org/abs/2402.12343) | 安全对齐的大型语言模型可能会通过模拟失调框架，在对抗性操纵下产生危险结果，对训练的语言模型具有双倍有害性，高于强基线，强调了即使在安全对齐后也需要重新评估开源语言模型的重要性。 |
| [^123] | [Task-conditioned adaptation of visual features in multi-task policy learning](https://arxiv.org/abs/2402.07739) | 本文通过任务条件的自适应器，在多任务策略学习的背景下，调整预训练的大型视觉模型，使其能够解决多个任务，并且无需微调预先训练的权重。 |
| [^124] | [Refining Myocardial Infarction Detection: A Novel Multi-Modal Composite Kernel Strategy in One-Class Classification](https://arxiv.org/abs/2402.06530) | 本研究提出了一种新的方法，使用基于超声心动图的一种基于多模态复合核策略的单一类别分类算法来进行早期心肌梗死的检测。这种方法通过优化投影矩阵和特征转化，提高了心肌梗死检测的能力。 |
| [^125] | [Tradeoffs of Diagonal Fisher Information Matrix Estimators](https://arxiv.org/abs/2402.05379) | 本研究探讨了使用对角费舍尔信息矩阵估计器的权衡。通过分析和数值研究，发现方差量取决于非线性与不同参数组之间的关系，应该在估计费舍尔信息时予以重视。 |
| [^126] | [Logical Specifications-guided Dynamic Task Sampling for Reinforcement Learning Agents](https://arxiv.org/abs/2402.03678) | 本文提出了一种逻辑规范引导下的动态任务采样（LSTS）方法，通过学习一组强化学习策略，根据高级任务规范指导智能体在最小化环境交互次数的同时实现从初始状态到目标状态的引导。在网格世界实验中，LSTS实现了改进的时间到阈值。 |
| [^127] | ["Which LLM should I use?": Evaluating LLMs for tasks performed by Undergraduate Computer Science Students in India](https://arxiv.org/abs/2402.01687) | 本研究评估了各种LLMs在本科计算机科学学生常见任务中的效果，并指导学生选择适合他们的LLM. |
| [^128] | [Code-Aware Prompting: A study of Coverage Guided Test Generation in Regression Setting using LLM](https://arxiv.org/abs/2402.00097) | 本文提出了一种代码感知提示策略（SymPrompt），用于基于LLM的测试生成，通过将测试生成过程分解为多阶段序列，并以驱动策略推动每个阶段，改善了测试生成的覆盖率。 |
| [^129] | [Robustness Assessment of a Runway Object Classifier for Safe Aircraft Taxiing](https://arxiv.org/abs/2402.00035) | 本文介绍了对航班滑行安全的跑道物体分类器的鲁棒性评估，使用形式方法评估了该分类器对三种常见图像扰动类型的鲁棒性，并提出了一种利用单调性的方法。 |
| [^130] | [LeanVec: Searching vectors faster by making them fit](https://arxiv.org/abs/2312.16335) | LeanVec是一个结合了线性降维和向量量化的框架，旨在加速高维向量的相似性搜索，解决了相似性搜索系统在高向量维度下的计算和内存压力问题，同时应用于跨模态检索任务。 |
| [^131] | [Efficient and Scalable Graph Generation through Iterative Local Expansion](https://arxiv.org/abs/2312.11529) | 通过逐步扩展单个节点到目标图的方法，避免了对所有节点对的整个联合分布进行建模，实现了高效可扩展的图生成，同时通过多尺度生成保持了高表达性。 |
| [^132] | [ReCoRe: Regularized Contrastive Representation Learning of World Model](https://arxiv.org/abs/2312.09056) | 通过正则化对比度表示学习世界模型，该方法提高了样本效率和泛化性能，解决了在视觉导航等日常任务中出现外观变化时的挑战。 |
| [^133] | [Channel-Feedback-Free Transmission for Downlink FD-RAN: A Radio Map based Complex-valued Precoding Network Approach](https://arxiv.org/abs/2312.02184) | 提出了一种不依赖信道反馈的新型下行传输方案，通过设计基于无线地图的复值预编码网络模型，实现用户位置为基础的基站预编码。 |
| [^134] | [Text-Driven Image Editing via Learnable Regions](https://arxiv.org/abs/2311.16432) | 该方法通过学习区域实现了文本驱动的图像编辑，无需用户提供遮罩或草图，具有灵活性和能够处理复杂提示的特点。与其他方法相比，展现了竞争性能。 |
| [^135] | [Differentially Private Non-Convex Optimization under the KL Condition with Optimal Rates](https://arxiv.org/abs/2311.13447) | 该论文研究了在KL条件下具有最优速率的差分私有非凸优化问题，并提出了针对不同情况的新算法，实现了接近最优的速率。 |
| [^136] | [CV-Attention UNet: Attention-based UNet for 3D Cerebrovascular Segmentation of Enhanced TOF-MRA Images](https://arxiv.org/abs/2311.10224) | 本文提出了一种3D脑血管注意力UNet方法，用于精确提取脑血管图像，通过一系列预处理技术和深度监督UNet来改善脑血管分割的准确性，有助于预防中风。 |
| [^137] | [Privacy-preserving Federated Primal-dual Learning for Non-convex and Non-smooth Problems with Model Sparsification](https://arxiv.org/abs/2310.19558) | 本文提出了一种针对非凸非光滑FL问题的新颖联邦原始-对偶算法，采用双向模型稀疏化，应用差分隐私进行隐私保障，并提出了FL算法设计指导原则。 |
| [^138] | [BatteryML:An Open-source platform for Machine Learning on Battery Degradation](https://arxiv.org/abs/2310.14714) | BatteryML是一个开源平台，通过一站式、全面的方法统一了电池衰减建模的数据预处理、特征提取和模型实现，提高了研究应用的实用性和效率。 |
| [^139] | [Confronting LLMs with Traditional ML: Rethinking the Fairness of Large Language Models in Tabular Classifications](https://arxiv.org/abs/2310.14607) | LLMs在表格分类任务中存在社会偏见，影响了它们的公平性。 |
| [^140] | [Automatic Pair Construction for Contrastive Post-training](https://arxiv.org/abs/2310.02263) | 提出了一种自动构建对比数据的方法，使用多个模型的偏好对，提高了大型语言模型的对齐效果，并且通过DPO对比技术得到了改善，进一步优化了对齐，最终使经过调优的指导学习模型Orca超越了ChatGPT。 |
| [^141] | [Total Selfie: Generating Full-Body Selfies](https://arxiv.org/abs/2308.14740) | 该方法能够从近距离自拍照片生成出一张别人从几英尺外拍摄的您的全身自拍照片。 |
| [^142] | [Decongestion by Representation: Learning to Improve Economic Welfare in Marketplaces](https://arxiv.org/abs/2306.10606) | 研究了通过表征减缓拥挤的方法，从消费者的选择中学习表征以减少拥挤，提高社会福利。 |
| [^143] | [RDumb: A simple approach that questions our progress in continual test-time adaptation](https://arxiv.org/abs/2306.05401) | RDumb是一个简单的基线方法，能在持续测试时间适应中表现得更好，甚至超过先前提出的最先进方法。 |
| [^144] | [From Shortcuts to Triggers: Backdoor Defense with Denoised PoE](https://arxiv.org/abs/2305.14910) | 提出了一种端到端集成的后门防御框架 DPoE，旨在通过去噪设计和捕捉后门快捷方式的浅层模型，以及防止学习后门快捷方式的主模型，有效抵御各种后门攻击。 |
| [^145] | [Financial Risk Management on a Neutral Atom Quantum Processor](https://arxiv.org/abs/2212.03223) | 提出了一个基于中性原子量子处理器的金融风险管理解决方案，能够在预测信用评级降级方面表现出竞争力、速度更快且具有更好的可解释性，并通过张量网络-based数值模拟验证了这些想法。 |
| [^146] | [Will My Robot Achieve My Goals? Predicting the Probability that an MDP Policy Reaches a User-Specified Behavior Target](https://arxiv.org/abs/2211.16462) | 本文提出了一种能够预测MDP策略达到用户指定行为目标概率的方法，并通过对符合预测进行反转来计算概率估计。 |
| [^147] | [Trust Your $\nabla$: Gradient-based Intervention Targeting for Causal Discovery](https://arxiv.org/abs/2211.13715) | 提出了一种基于梯度的干预目标定位方法，GIT，在因果发现中能够通过信号梯度估计器降低干预次数，在低数据量情况下优于竞争基线。 |
| [^148] | [On-Demand Sampling: Learning Optimally from Multiple Distributions](https://arxiv.org/abs/2210.12529) | 本文建立了多分布学习范式的最优样本复杂度，并提出了符合此复杂度的算法，改进了公平联邦学习和协作学习的样本复杂度边界。 |
| [^149] | [Conquering the Communication Constraints to Enable Large Pre-Trained Models in Federated Learning](https://arxiv.org/abs/2210.01708) | 研究克服联邦学习中通信约束的方法，以实现强大的预训练模型在FL中的应用，并同时减少通信负担。 |
| [^150] | [Reweighted Manifold Learning of Collective Variables from Enhanced Sampling Simulations](https://arxiv.org/abs/2207.14554) | 提出了一个基于各向异性扩散映射的重新加权框架，解决了在增强采样模拟中学习集体变量流形时遇到的偏差问题。 |
| [^151] | [Off-Policy Correction For Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2111.11229) | 提出了MA-Trace，一种新的离策略演员-评论家算法，在多智体强化学习环境中具有高可伸缩性，并通过重要性采样作为离策略修正方法，保证了计算分布的质量和算法的收敛性。 |
| [^152] | [Learning Robust Output Control Barrier Functions from Safe Expert Demonstrations](https://arxiv.org/abs/2111.09971) | 从专家演示中学习健壮的输出控制屏障函数以确保安全性，当参数化为线性时，优化问题是凸的。 |
| [^153] | [Catalytic Role Of Noise And Necessity Of Inductive Biases In The Emergence Of Compositional Communication](https://arxiv.org/abs/2111.06464) | 噪声和归纳偏见的作用使得组合式沟通自发产生，并且一定范围内的噪声有助于促进组合性的发展。 |
| [^154] | [Subgoal Search For Complex Reasoning Tasks](https://arxiv.org/abs/2108.11204) | 提出了子目标搜索（kSubS）方法，通过学习的子目标生成器产生多样性的子目标，减少搜索空间并在Sokoban、魔方和不等式证明三个领域取得了强大的结果。 |
| [^155] | [MCL-GAN: Generative Adversarial Networks with Multiple Specialized Discriminators](https://arxiv.org/abs/2107.07260) | MCL-GAN提出了一种生成对抗网络框架，利用多个鉴别器共同合作来更有效地表示真实数据集，在解决模式崩溃问题的同时，实现了生成器与基础数据分布的一致性，通过给每个鉴别器专业技能的引导，生成器能够自动找到潜在数据空间和真实数据空间的合理对应关系，同时骨干网络在鉴别器之间是共享的，训练成本增加微不足道，并在标准数据集上证明了算法的有效性。 |
| [^156] | [Simulation-based reinforcement learning for real-world autonomous driving](https://arxiv.org/abs/1911.12905) | 该论文利用模拟强化学习和合成数据来实现对真实世界自动驾驶系统的控制，成功实现了模拟到真实策略转移，并分析了设计决策对真实世界性能的影响。 |
| [^157] | [Global Momentum Compression for Sparse Communication in Distributed Learning](https://arxiv.org/abs/1905.12948) | 本文提出了一种全局动量压缩（GMC）方法，用于稀疏通信，与现有的局部动量方法不同，GMC利用全局动量来提高分布式学习性能。 |
| [^158] | [Model-Based Reinforcement Learning for Atari](https://arxiv.org/abs/1903.00374) | 本研究探索了如何利用视频预测模型实现基于模型的深度RL算法SimPLe，在Atari游戏中比无模型方法更有效地解决问题，并通过实验验证了新颖模型体系结构在这一背景下取得最佳结果。 |
| [^159] | [MambaByte: Token-free Selective State Space Model.](http://arxiv.org/abs/2401.13660) | MambaByte是一种无标记的选择性状态空间模型，通过在字节级别上进行自回归训练，解决了标准自回归Transformer在处理长序列时的性能问题，并展现了与最先进的子词Transformer相媲美甚至更优的性能，从而证明了MambaByte在无标记语言建模方面的有效性。 |
| [^160] | [Instructional Fingerprinting of Large Language Models.](http://arxiv.org/abs/2401.12255) | 这项研究提出了一种指纹识别大型语言模型的方法，通过轻量级的指令调整，保护知识产权并确保遵守许可条款。实验证明这种方法不影响模型的正常行为，并且具有鲁棒性和高效训练的特点。 |
| [^161] | [Understanding Video Transformers via Universal Concept Discovery.](http://arxiv.org/abs/2401.10831) | 本文研究了视频Transformer的可解释性问题，引入了视频Transformer概念发现算法来解释其决策过程，并揭示了时空推理机制和对象为中心的表示。 |
| [^162] | [Advancing Ante-Hoc Explainable Models through Generative Adversarial Networks.](http://arxiv.org/abs/2401.04647) | 本文提出了一种先验可解释模型，通过在主分类器网络中添加无监督的解释生成器和对抗训练的方式，实现了模型的可解释性和性能的提升。该方法通过训练解释模块提取视觉概念，同时使用生成对抗网络模块来区分生成的图像和真实图像。实验证明了该方法的鲁棒性，并展示了学到的概念与对象部分和视觉属性的语义一致性。 |
| [^163] | [Tracking Any Object Amodally.](http://arxiv.org/abs/2312.12433) | 本论文介绍了一种追踪任何物体的非现态方法，利用数据增强和微调现态跟踪器，可以提高追踪的效果。 |
| [^164] | [Divergent Token Metrics: Measuring degradation to prune away LLM components -- and optimize quantization.](http://arxiv.org/abs/2311.01544) | 本研究引入了一种新的方法，即不同的令牌指标（DTM），用于评估压缩后的大型语言模型（LLM）。通过关注令牌的差异性，DTM提供了对模型压缩微妙之处的深入洞察，并且在不损害文本生成质量的情况下可以实现显著的精确度和稀疏度水平。该研究还提出了一种利用DTM进行模型稀疏化和量化的方法，并发现可以修剪掉超过90%的LLM组件和量化超过80%的参数。 |
| [^165] | [Multi-Operational Mathematical Derivations in Latent Space.](http://arxiv.org/abs/2311.01230) | 本文研究在潜在空间中逼近多个数学运算进行表达式推导的可能性，并通过构建大规模数据集和使用最先进的神经编码器实例化，探索了不同编码机制在潜在空间中逼近方程推理的能力。 |
| [^166] | [Bayesian Neural Controlled Differential Equations for Treatment Effect Estimation.](http://arxiv.org/abs/2310.17463) | 本文提出了一种新颖的贝叶斯神经控制微分方程方法，用于连续时间的治疗效果估计，该方法能够提供对潜在结果的后验预测分布，并给出了可靠的不确定性估计。 |
| [^167] | [Leveraging Ensemble Diversity for Robust Self-Training in the Presence of Sample Selection Bias.](http://arxiv.org/abs/2310.14814) | 本文提出了一种在样本选择偏差存在的情况下，利用集成多样性进行鲁棒的自训练的方法，并引入了一种新的自信度度量方法-$\mathcal{T}$-相似度。实验证明该方法在三种不同伪标签策略下具有良好的效果。 |
| [^168] | [Proper Laplacian Representation Learning.](http://arxiv.org/abs/2310.10833) | 本论文介绍了一种理论上可靠的方法和优化算法，用于近似Laplacian表示学习，以解决大规模强化学习中的探索、泛化和传递问题。 |
| [^169] | [Retro-fallback: retrosynthetic planning in an uncertain world.](http://arxiv.org/abs/2310.09270) | 本文针对逆合成任务在实验室执行可行性的不确定性问题，通过引入随机过程的表述，提出了一种名为 Retro-fallback 的贪婪算法，该算法能够最大化实验室可执行的合成计划的概率。 |
| [^170] | [Physics-Informed Graph Neural Network for Dynamic Reconfiguration of Power Systems.](http://arxiv.org/abs/2310.00728) | 提出了一种基于物理信息的图神经网络（GNN）框架GraPhyR，用于解决电力系统的动态重构（DyR）问题。该框架将运营和连接约束直接融入GNN框架中，并进行端到端的训练，能够有效地优化DyR任务。 |
| [^171] | [Enhancing Multi-Objective Optimization through Machine Learning-Supported Multiphysics Simulation.](http://arxiv.org/abs/2309.13179) | 通过机器学习支持的多物理仿真加速多目标优化，提出了一种框架来逼近和加速复杂的多物理仿真，并在实验中展示了其有效性。 |
| [^172] | [Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection.](http://arxiv.org/abs/2307.16888) | 这项研究介绍了一种针对指令调整的大型语言模型的新型后门攻击方法，即虚拟提示注入（VPI）。通过在特定触发场景下将虚拟提示与用户指令连接，攻击者可以精细操纵模型的回应而无需明确注入。 |
| [^173] | [High-performance real-world optical computing trained by in situ model-free optimization.](http://arxiv.org/abs/2307.11957) | 本论文提出了一种无模型优化光学计算系统的方法，通过在原位进行轻量级优化，实现了高性能的真实光学计算。实验证明该方法在分类准确度上优于传统方法，并展示了在高速细胞分析方面的潜力。这种方法的固有简单性和低需求的计算资源促进了光学计算技术从实验室研究向真实世界应用的转变。 |
| [^174] | [Ecosystem-level Analysis of Deployed Machine Learning Reveals Homogeneous Outcomes.](http://arxiv.org/abs/2307.05862) | 部署机器学习存在系统性故障, 即使单个模型在总体上的改善也不能解决这个问题 |
| [^175] | [A Double Machine Learning Approach to Combining Experimental and Observational Data.](http://arxiv.org/abs/2307.01449) | 这种双机器学习方法将实验和观测研究结合起来，能够测试假设的违反情况并一致估计处理效应。它提供了半参数高效的处理效应估计器。这种方法在实际环境中是可行的。 |
| [^176] | [Impact of Noise on Calibration and Generalisation of Neural Networks.](http://arxiv.org/abs/2306.17630) | 本研究研究了不同类型噪声对神经网络的校准和泛化的影响，发现激活噪声能最有效地提高泛化性能，而输入增强噪声则能显著改善分布外的校准。 |
| [^177] | [MeciFace: Mechanomyography and Inertial Fusion based Glasses for Edge Real-Time Recognition of Facial and Eating Activities.](http://arxiv.org/abs/2306.13674) | MeciFace是一款注重隐私且低功耗的可穿戴设备，它采用轻量级卷积神经网络来监测面部表情和进食活动，面部表情案例的F1分数达到了86％，饮食监测则达到了90％的F1分数。 |
| [^178] | [A Mechanism for Solving Relational Tasks in Transformer Language Models.](http://arxiv.org/abs/2305.16130) | 这篇论文研究了在Transformer语言模型中解决关系任务的机制，并发现这些模型利用简单的线性更新来处理关系任务，并以内容无关的方式促进关系的输出。 |
| [^179] | [Online Optimization for Randomized Network Resource Allocation with Long-Term Constraints.](http://arxiv.org/abs/2305.15558) | 本文研究了一个面向随机网络资源分配的在线问题，并提出了一种近乎最优的解决方案，该方案在数字模拟中表现优异。 |
| [^180] | [Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models.](http://arxiv.org/abs/2305.14710) | 使用指令调整方法在众包数据集上训练的大规模语言模型，存在后门漏洞，攻击者只需注入极少量的恶意指令便可永久控制模型行为，且难以被修复，需要更加健全的防御机制。 |
| [^181] | [Recurrent Transformer for Dynamic Graph Representation Learning with Edge Temporal States.](http://arxiv.org/abs/2304.10079) | 本文提出了循环差分图变换器框架，旨在解决动态图表示学习中未能明确建模边时序状态和提取全局结构特征的问题。 |
| [^182] | [From Isolated Islands to Pangea: Unifying Semantic Space for Human Action Understanding.](http://arxiv.org/abs/2304.00553) | 本文提出了一个统一的语义空间Poincare行为语义空间，通过将以前数据集的类别与这个语义空间对齐，收集（图像/视频/骨架/MoCap）数据集到一个统一的数据库中，即将“孤立的岛屿”桥接成一个“泛大陆”，这将有助于推进可推广的行为学习。 |
| [^183] | [Combining Deep Metric Learning Approaches for Aerial Scene Classification.](http://arxiv.org/abs/2303.11389) | 本论文提出了六种深度度量学习方法来实现航空场景分类任务，并通过进化计算算法将它们结合起来。实验结果表明，这些方法与传统图像分类方法有类似的精度。 |
| [^184] | [An active learning method for solving competitive multi-agent decision-making and control problems.](http://arxiv.org/abs/2212.12561) | 我们提出了一个基于主动学习的方法，用于解决竞争性多智能体决策和控制问题。通过重构私有策略和预测稳态行动配置文件，外部观察者可以成功进行预测和优化策略。 |
| [^185] | [Shapley Curves: A Smoothing Perspective.](http://arxiv.org/abs/2211.13289) | 本文以平滑的角度引入了Shapley曲线作为局部变量重要性的度量，提出了两种估计策略，并在特征的独立和依赖情况下得到了一致性和渐近正态性，为估计的Shapley曲线构建了置信区间并进行了推断，通过实验证实了渐近结果。应用中分析了哪些属性驱动车辆价格。 |
| [^186] | [DriftRec: Adapting diffusion models to blind JPEG restoration.](http://arxiv.org/abs/2211.06757) | DriftRec是一种将扩散模型应用于盲JPEG恢复的方法，通过优雅地修改扩散模型的正向随机微分方程，DriftRec能够在高压缩水平下恢复干净图像的分布，避免生成模糊图像，并且不需要关于损坏操作的先验知识，具有广泛的适用性。 |
| [^187] | [Online Control of Adaptive Large Neighborhood Search using Deep Reinforcement Learning.](http://arxiv.org/abs/2211.00759) | 本研究提出一种基于深度强化学习的方法来在线控制自适应大邻域搜索算法，该方法能够自适应选择启发式策略、调整参数和控制接受标准，以获得优化问题的良好解，对应用组合优化问题中的实际问题具有重要意义。 |
| [^188] | [Multilevel Stochastic Optimization for Imputation in Massive Medical Data Records.](http://arxiv.org/abs/2110.09680) | 本文介绍了一种基于Kriging理论的多层次随机优化填补方法，能够更准确、更快速和更稳定地处理大规模医疗数据记录中的缺失数值数据。 |

# 详细

[^1]: 解读文本的真实性: 通过大规模语言语义的广义策略来检测人类和机器生成的文本

    Deciphering Textual Authenticity: A Generalized Strategy through the Lens of Large Language Semantics for Detecting Human vs. Machine-Generated Text

    [https://rss.arxiv.org/abs/2401.09407](https://rss.arxiv.org/abs/2401.09407)

    该论文通过研究在真实世界场景中检测机器生成文本的方法，发现现有方法对于不同生成器和领域产生的文本存在严重限制。

    

    随着大规模语言模型（LLM）的广泛应用，对检测机器生成文本的工具的需求日益增长。有效检测机器生成文本面临两个关键问题: 首先，他们在应对真实世界场景时面临着极大的限制，这些场景中机器生成文本是由各种生成器产生的，包括但不限于GPT-4和Dolly，并涵盖各种领域，从学术手稿到社交媒体帖子。其次，现有的检测方法将LLM生成的文本视为严格的二元分类问题，忽略了不同LLM生成的文本多样性。本研究系统地研究了在真实世界场景中检测机器生成文本的方法。我们首先研究了最先进方法的有效性，并发现它们在应对真实世界中不同生成器和领域产生的文本时受到严重的限制。

    With the recent proliferation of Large Language Models (LLMs), there has been an increasing demand for tools to detect machine-generated text. The effective detection of machine-generated text face two pertinent problems: First, they are severely limited in generalizing against real-world scenarios, where machine-generated text is produced by a variety of generators, including but not limited to GPT-4 and Dolly, and spans diverse domains, ranging from academic manuscripts to social media posts. Second, existing detection methodologies treat texts produced by LLMs through a restrictive binary classification lens, neglecting the nuanced diversity of artifacts generated by different LLMs. In this work, we undertake a systematic study on the detection of machine-generated text in real-world scenarios. We first study the effectiveness of state-of-the-art approaches and find that they are severely limited against text produced by diverse generators and domains in the real world. Furthermore,
    
[^2]: ALOHa：测量图像字幕模型中幻觉的新标准

    ALOHa: A New Measure for Hallucination in Captioning Models

    [https://arxiv.org/abs/2404.02904](https://arxiv.org/abs/2404.02904)

    提出了一种新的用于测量图像字幕模型中幻觉的标准ALOHa，利用大型语言模型来测量幻觉对象，并成功识别比现有指标CHAIR更多的幻觉对象。

    

    尽管在视觉描述的多模态预训练方面取得了近期的进展，但最先进的模型仍会产生包含错误的字幕，比如在场景中存在幻觉对象。现有的主要幻觉对象度量标准CHAIR，仅限于一组固定的MS COCO对象和同义词。在这项工作中，我们提出了一种现代化的开放词汇度量标准ALOHa，利用大型语言模型（LLM）来衡量对象幻觉。具体地，我们使用LLM从候选字幕中提取可连接的对象，衡量它们与字幕和对象检测中参考对象的语义相似度，并使用匈牙利匹配生成最终的幻觉得分。我们展示了ALOHa在HAT上比CHAIR在一个新的用于幻觉标记的MS COCO字幕的金标准子集上正确识别了更多的幻觉对象（多出13.6%），在nocaps上（其中对象超出了MS COCO类别）识别了更多的幻觉对象（多至30.8%）。

    arXiv:2404.02904v1 Announce Type: cross  Abstract: Despite recent advances in multimodal pre-training for visual description, state-of-the-art models still produce captions containing errors, such as hallucinating objects not present in a scene. The existing prominent metric for object hallucination, CHAIR, is limited to a fixed set of MS COCO objects and synonyms. In this work, we propose a modernized open-vocabulary metric, ALOHa, which leverages large language models (LLMs) to measure object hallucinations. Specifically, we use an LLM to extract groundable objects from a candidate caption, measure their semantic similarity to reference objects from captions and object detections, and use Hungarian matching to produce a final hallucination score. We show that ALOHa correctly identifies 13.6% more hallucinated objects than CHAIR on HAT, a new gold-standard subset of MS COCO Captions annotated for hallucinations, and 30.8% more on nocaps, where objects extend beyond MS COCO categories.
    
[^3]: DeiT-LT蒸馏重返，用于长尾数据集上的Vision Transformer训练

    DeiT-LT Distillation Strikes Back for Vision Transformer Training on Long-Tailed Datasets

    [https://arxiv.org/abs/2404.02900](https://arxiv.org/abs/2404.02900)

    DeiT-LT通过引入一种有效的蒸馏方式，将CNN蒸馏到ViT中，以应对长尾数据集上训练ViT时的困难。

    

    Vision Transformer（ViT）已经成为各种计算机视觉任务中突出的架构。在 ViT 中，我们将输入图像分成补丁令牌，并通过一堆自我注意块进行处理。然而，与卷积神经网络（CNN）不同，ViT 的简单架构没有信息性归纳偏差（例如局部性等）。由于这个原因，ViT 需要大量数据进行预训练。已经提出了各种数据有效的方法（DeiT）来有效地训练平衡的数据集上的ViT。然而，文献中很少讨论使用ViT来处理长尾不平衡数据集。在这项工作中，我们引入DeiT-LT来解决从头开始训练长尾数据集上的ViT的问题。在 DeiT-LT 中，我们通过使用超出分布图像和重新加权蒸馏损失，引入了一种有效的蒸馏方式，通过蒸馏 DIST 令牌从CNN进行蒸馏，以增强对尾部类别的关注。

    arXiv:2404.02900v1 Announce Type: cross  Abstract: Vision Transformer (ViT) has emerged as a prominent architecture for various computer vision tasks. In ViT, we divide the input image into patch tokens and process them through a stack of self attention blocks. However, unlike Convolutional Neural Networks (CNN), ViTs simple architecture has no informative inductive bias (e.g., locality,etc. ). Due to this, ViT requires a large amount of data for pre-training. Various data efficient approaches (DeiT) have been proposed to train ViT on balanced datasets effectively. However, limited literature discusses the use of ViT for datasets with long-tailed imbalances. In this work, we introduce DeiT-LT to tackle the problem of training ViTs from scratch on long-tailed datasets. In DeiT-LT, we introduce an efficient and effective way of distillation from CNN via distillation DIST token by using out-of-distribution images and re-weighting the distillation loss to enhance focus on tail classes. Thi
    
[^4]: 对“从微分方程中学习守恒定律”一文的评论

    Comment on "Machine learning conservation laws from differential equations"

    [https://arxiv.org/abs/2404.02896](https://arxiv.org/abs/2404.02896)

    评论了另一篇关于从微分方程中学习守恒定律的文章中存在的严重推导错误

    

    在此评论中，作者回顾了刘, 马德哈万和泰格马克提出的与作者提出的一维阻尼谐振子的守恒量相类似的结果，指出他们推导中存在六个严重错误，导致他们的方法和结果均不正确。

    arXiv:2404.02896v1 Announce Type: new  Abstract: In lieu of abstract, first paragraph reads: Six months after the author derived a constant of motion for a 1D damped harmonic oscillator [1], a similar result appeared by Liu, Madhavan, and Tegmark [2, 3], without citing the author. However, their derivation contained six serious errors, causing both their method and result to be incorrect. In this Comment, those errors are reviewed.
    
[^5]: MODNO: 具有分布式神经算子的多算子学习

    MODNO: Multi Operator Learning With Distributed Neural Operators

    [https://arxiv.org/abs/2404.02892](https://arxiv.org/abs/2404.02892)

    本文提出了一种针对多算子学习挑战的新型分布式训练方法，有效地实现单个神经算子处理多算子学习问题，而不增加额外平均成本。

    

    运算符学习的研究涉及利用神经网络来逼近算子。传统上，重点放在单算子学习（SOL）上。然而，最近的进展迅速将其扩展到包含使用具有数百万或数十亿可训练参数的基础模型来逼近多算子，从而导致了多算子学习（MOL）的研究。在本文中，我们提出了一种新颖的分布式训练方法，旨在使单个神经算子能够有效地处理多算子学习挑战，而不会产生额外的平均成本。我们的方法适用于各种类似Chen-Chen型神经算子，如深算子神经网络（DON）。其核心思想是独立学习每个算子的输出基函数，使用其专用数据，同时集中学习输入fu。

    arXiv:2404.02892v1 Announce Type: new  Abstract: The study of operator learning involves the utilization of neural networks to approximate operators. Traditionally, the focus has been on single-operator learning (SOL). However, recent advances have rapidly expanded this to include the approximation of multiple operators using foundation models equipped with millions or billions of trainable parameters, leading to the research of multi-operator learning (MOL). In this paper, we present a novel distributed training approach aimed at enabling a single neural operator with significantly fewer parameters to effectively tackle multi-operator learning challenges, all without incurring additional average costs. Our method is applicable to various Chen-Chen-type neural operators, such as Deep Operator Neural Networks (DON). The core idea is to independently learn the output basis functions for each operator using its dedicated data, while simultaneously centralizing the learning of the input fu
    
[^6]: 关于基于扩散的文本到图像生成方法的可扩展性

    On the Scalability of Diffusion-based Text-to-Image Generation

    [https://arxiv.org/abs/2404.02883](https://arxiv.org/abs/2404.02883)

    本研究通过针对扩散型T2I模型进行消融实验，发现增加transformer块对于改善文本-图像对齐比增加通道数更具参数效率。

    

    扩大模型和数据规模对于LLMs的发展取得了相当大的成功。然而，尚未充分探讨基于扩散的文本到图像（T2I）模型的扩展法则。如何有效地扩展模型以在降低成本的情况下提高性能也不太清楚。不同的训练设置和昂贵的训练成本使得进行公平的模型比较变得极为困难。在本研究中，我们通过对去噪骨干和训练集的大量而严格的消融实验，对扩散型T2I模型的扩展特性进行了实证研究，包括在数据集上达到600M图像的范围内训练参数从0.4B到4B的缩放UNet和Transformer变体。对于模型的扩展，我们发现跨关注的位置和数量区分了现有UNet设计的性能。增加transformer块对于提高文本-图像对齐比增加通道数更具参数效率。

    arXiv:2404.02883v1 Announce Type: cross  Abstract: Scaling up model and data size has been quite successful for the evolution of LLMs. However, the scaling law for the diffusion based text-to-image (T2I) models is not fully explored. It is also unclear how to efficiently scale the model for better performance at reduced cost. The different training settings and expensive training cost make a fair model comparison extremely difficult. In this work, we empirically study the scaling properties of diffusion based T2I models by performing extensive and rigours ablations on scaling both denoising backbones and training set, including training scaled UNet and Transformer variants ranging from 0.4B to 4B parameters on datasets upto 600M images. For model scaling, we find the location and amount of cross attention distinguishes the performance of existing UNet designs. And increasing the transformer blocks is more parameter-efficient for improving text-image alignment than increasing channel nu
    
[^7]: 线性注意力序列并行化

    Linear Attention Sequence Parallelism

    [https://arxiv.org/abs/2404.02882](https://arxiv.org/abs/2404.02882)

    提出了一种名为线性注意力序列并行（LASP）的高效序列并行方法，针对线性注意力的语言模型进行了优化，通过设计高效的点对点通信机制和执行内核融合来降低通信开销，并实现硬件友好性。

    

    序列并行（SP）作为一种处理超出单个GPU内存限制的长序列的流行策略。然而，现有的SP方法并未利用线性注意力特性，导致在基于线性注意力的语言模型中并行效率和可用性不佳。在本文中，我们介绍了线性注意力序列并行（LASP），这是一种专为基于线性注意力的语言模型量身定制的高效SP方法。具体来说，我们设计了一种高效的点对点通信机制，以利用线性注意力的右乘内核技巧，从而显着降低SP的通信开销。我们还通过执行内核融合和中间状态缓存来增强LASP的实际效率，使LASP在GPU集群上的硬件友好性得到提升。此外，我们还精心确保序列级LASP与所有类型的批级数据兼容。

    arXiv:2404.02882v1 Announce Type: cross  Abstract: Sequence Parallel (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single GPU. However, existing SP methods do not take advantage of linear attention features, resulting in sub-optimal parallelism efficiency and usability for linear attention-based language models. In this paper, we introduce Linear Attention Sequence Parallel (LASP), an efficient SP method tailored to linear attention-based language models. Specifically, we design an efficient point-to-point communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead of SP. We also enhance the practical efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPU clusters. Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data par
    
[^8]: 高斯过程回归与软不等式和单调性约束

    Gaussian Process Regression with Soft Inequality and Monotonicity Constraints

    [https://arxiv.org/abs/2404.02873](https://arxiv.org/abs/2404.02873)

    引入类量子 Hamilton Monte Carlo 方法到不等式和单调性约束的高斯过程回归中，在概率意义上提高了模型准确性并减少了方差

    

    Gaussian process（GP）回归是一种非参数、贝叶斯框架，用于逼近复杂模型。标准的GP回归可能导致模型无界，导致某些点采用不可行的值。我们介绍了一种新的GP方法，以概率方式强制执行物理约束。该GP模型通过类量子启发的 Hamilton Monte Carlo（QHMC）进行训练。QHMC是从各种分布中高效抽样的方法。与标准的 Hamilton Monte Carlo 算法不同，其中粒子具有固定质量，QHMC允许粒子具有随机质量矩阵并带有概率分布。将 QHMC 方法引入概率意义上的不等式和单调性约束的 GP 回归，我们的方法提高了结果 GP 模型的准确性并减少了方差。根据我们在几个数据集上的实验，所提出的方法作为一种高效方法可以加速...

    arXiv:2404.02873v1 Announce Type: cross  Abstract: Gaussian process (GP) regression is a non-parametric, Bayesian framework to approximate complex models. Standard GP regression can lead to an unbounded model in which some points can take infeasible values. We introduce a new GP method that enforces the physical constraints in a probabilistic manner. This GP model is trained by the quantum-inspired Hamiltonian Monte Carlo (QHMC). QHMC is an efficient way to sample from a broad class of distributions. Unlike the standard Hamiltonian Monte Carlo algorithm in which a particle has a fixed mass, QHMC allows a particle to have a random mass matrix with a probability distribution. Introducing the QHMC method to the inequality and monotonicity constrained GP regression in the probabilistic sense, our approach improves the accuracy and reduces the variance in the resulting GP model. According to our experiments on several datasets, the proposed approach serves as an efficient method as it accel
    
[^9]: 使用智能手机进行人类活动识别

    Human Activity Recognition using Smartphones

    [https://arxiv.org/abs/2404.02869](https://arxiv.org/abs/2404.02869)

    该论文通过智能手机的加速度计捕获不同日常活动的数据，提取特征并应用机器学习算法实现实时活动识别和卡路里消耗计算。

    

    人类活动识别是当前的研究热点之一，在远程医疗、老年人或残障人士活动跟踪、消耗卡路里等领域有广泛应用。在我们的项目中，我们创建了一个Android应用程序，可以实时识别日常人类活动并计算消耗的卡路里。我们首先通过智能手机的内置加速度计捕获了不同日常人类活动的标记三轴加速度读数。然后使用中值滤波对这些读数进行预处理。我们使用各种方法提取了42个特征。接着，我们测试了各种机器学习算法以及降维方法。最后，在我们的Android应用程序中，我们使用了机器学习算法和提供了最高准确性和最短模型构建时间的特征子集。这用于实时活动识别和利用基于代谢的公式计算消耗的卡路里。

    arXiv:2404.02869v1 Announce Type: cross  Abstract: Human Activity Recognition is a subject of great research today and has its applications in remote healthcare, activity tracking of the elderly or the disables, calories burnt tracking etc. In our project, we have created an Android application that recognizes the daily human activities and calculate the calories burnt in real time. We first captured labeled triaxial acceleration readings for different daily human activities from the smartphone's embedded accelerometer. These readings were preprocessed using a median filter. 42 features were extracted using various methods. We then tested various machine learning algorithms along with dimensionality reduction. Finally, in our Android application, we used the machine learning algorithm and a subset of features that provided maximum accuracy and minimum model building time. This is used for real-time activity recognition and calculation of calories burnt using a formula based on Metaboli
    
[^10]: 通过Hammersley-Chapman-Robbins界限保证机密性

    Guarantees of confidentiality via Hammersley-Chapman-Robbins bounds

    [https://arxiv.org/abs/2404.02866](https://arxiv.org/abs/2404.02866)

    通过添加噪声到最后层的激活来保护隐私，使用HCR界限可量化保护机密性的可信度

    

    在深度神经网络推断过程中通过向最后几层的激活添加噪声来保护隐私是可能的。这些层中的激活被称为“特征”（少见的称为“嵌入”或“特征嵌入”）。添加的噪声有助于防止从嘈杂的特征中重建输入。通过对所有可能的无偏估计量的方差进行下限估计，量化了由此添加的噪声产生的机密性。经典不等式Hammersley和Chapman以及Robbins提供便利的、可计算的界限-- HCR界限。数值实验表明，对于包含10个类别的图像分类数据集“MNIST”和“CIFAR-10”，HCR界限在小型神经网络上表现良好。HCR界限似乎单独无法保证

    arXiv:2404.02866v1 Announce Type: new  Abstract: Protecting privacy during inference with deep neural networks is possible by adding noise to the activations in the last layers prior to the final classifiers or other task-specific layers. The activations in such layers are known as "features" (or, less commonly, as "embeddings" or "feature embeddings"). The added noise helps prevent reconstruction of the inputs from the noisy features. Lower bounding the variance of every possible unbiased estimator of the inputs quantifies the confidentiality arising from such added noise. Convenient, computationally tractable bounds are available from classic inequalities of Hammersley and of Chapman and Robbins -- the HCR bounds. Numerical experiments indicate that the HCR bounds are on the precipice of being effectual for small neural nets with the data sets, "MNIST" and "CIFAR-10," which contain 10 classes each for image classification. The HCR bounds appear to be insufficient on their own to guar
    
[^11]: 端到端自调节自监督时间序列异常检测

    End-To-End Self-tuning Self-supervised Time Series Anomaly Detection

    [https://arxiv.org/abs/2404.02865](https://arxiv.org/abs/2404.02865)

    提出了TSAP方法来自动调整数据增强，为时间序列异常检测带来了端到端的自调节能力。

    

    时间序列异常检测（TSAD）在监控环境传感器、行业KPI、患者生物标志物等方面有许多应用。TSAD的一个双重挑战是需要一种多功能且无监督模型，能够检测各种不同类型的时间序列异常（尖峰、不连续、趋势变化等），而不需要任何标记的数据。我们的工作旨在填补这一空白。我们引入了TSAP来执行TSA“自动驾驶”，可以端到端自动调整数据增强的超参数，自适应选择数据增强策略。

    arXiv:2404.02865v1 Announce Type: new  Abstract: Time series anomaly detection (TSAD) finds many applications such as monitoring environmental sensors, industry KPIs, patient biomarkers, etc. A two-fold challenge for TSAD is a versatile and unsupervised model that can detect various different types of time series anomalies (spikes, discontinuities, trend shifts, etc.) without any labeled data. Modern neural networks have outstanding ability in modeling complex time series. Self-supervised models in particular tackle unsupervised TSAD by transforming the input via various augmentations to create pseudo anomalies for training. However, their performance is sensitive to the choice of augmentation, which is hard to choose in practice, while there exists no effort in the literature on data augmentation tuning for TSAD without labels. Our work aims to fill this gap. We introduce TSAP for TSA "on autoPilot", which can (self-)tune augmentation hyperparameters end-to-end. It stands on two key c
    
[^12]: 朝向推理最佳的混合专家大型语言模型

    Toward Inference-optimal Mixture-of-Expert Large Language Models

    [https://arxiv.org/abs/2404.02852](https://arxiv.org/abs/2404.02852)

    MoE-based大型语言模型的研究强调了推理时间与专家数量之间的平衡问题，提出了引入推理效率作为缩放定律的调整方案

    

    Mixture-of-Expert（MoE）大型语言模型（LLMs），如最近的Mixtral和DeepSeek-MoE，展示了在缩放模型大小时不会遭受密集变压器训练成本的二次增长的巨大潜力。与密集模型一样，训练MoEs需要回答同样的问题：在给定的训练预算下，模型大小和标记数的最佳分配是多少？我们研究了关于模型性能、模型大小、数据集大小和专家程度之间关系的MoE-based LLMs的缩放定律。回应先前研究MoE在不同情境下的研究，我们观察到增加专家数量的递减回报，但这似乎表明我们应该扩展专家数量直至饱和，因为训练成本会保持恒定，这在推理时间中存在问题。我们提出通过引入推理效率作为另一个度量标准来修改MoE的缩放定律

    arXiv:2404.02852v1 Announce Type: new  Abstract: Mixture-of-Expert (MoE) based large language models (LLMs), such as the recent Mixtral and DeepSeek-MoE, have shown great promise in scaling model size without suffering from the quadratic growth of training cost of dense transformers. Like dense models, training MoEs requires answering the same question: given a training budget, what is the optimal allocation on the model size and number of tokens? We study the scaling law of MoE-based LLMs regarding the relations between the model performance, model size, dataset size, and the expert degree. Echoing previous research studying MoE in different contexts, we observe the diminishing return of increasing the number of experts, but this seems to suggest we should scale the number of experts until saturation, as the training cost would remain constant, which is problematic during inference time. We propose to amend the scaling law of MoE by introducing inference efficiency as another metric b
    
[^13]: BAdam：面向大型语言模型的内存高效全参数训练方法

    BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models

    [https://arxiv.org/abs/2404.02827](https://arxiv.org/abs/2404.02827)

    BAdam提出了一种内存高效的全参数微调大型语言模型的方法，并在实验中展现出优越的收敛行为以及在性能评估中的优势。

    

    这项工作提出了BAdam，这是一种利用Adam作为内部求解器的块坐标优化框架的优化器。BAdam提供了一种内存高效的方法，用于对大型语言模型进行全参数微调，并且由于链式规则属性减少了反向过程的运行时间。在实验中，我们将BAdam应用于在Alpaca-GPT4数据集上使用单个RTX3090-24GB GPU进行指导微调的Llama 2-7B模型。结果表明，与LoRA和LOMO相比，BAdam展现出了优越的收敛行为。此外，我们通过使用MT-bench对指导微调模型进行下游性能评估，结果显示BAdam在适度超越LoRA的基础上更显著地优于LOMO。最后，我们将BAdam与Adam在中等任务上进行了比较，即在SuperGLUE基准上对RoBERTa-large进行微调。结果表明，BAdam能够缩小与Adam之间的性能差距。我们的代码

    arXiv:2404.02827v1 Announce Type: new  Abstract: This work presents BAdam, an optimizer that leverages the block coordinate optimization framework with Adam as the inner solver. BAdam offers a memory efficient approach to the full parameter finetuning of large language models and reduces running time of the backward process thanks to the chain rule property. Experimentally, we apply BAdam to instruction-tune the Llama 2-7B model on the Alpaca-GPT4 dataset using a single RTX3090-24GB GPU. The results indicate that BAdam exhibits superior convergence behavior in comparison to LoRA and LOMO. Furthermore, our downstream performance evaluation of the instruction-tuned models using the MT-bench shows that BAdam modestly surpasses LoRA and more substantially outperforms LOMO. Finally, we compare BAdam with Adam on a medium-sized task, i.e., finetuning RoBERTa-large on the SuperGLUE benchmark. The results demonstrate that BAdam is capable of narrowing the performance gap with Adam. Our code is
    
[^14]: Conifer: 提高大型语言模型复杂约束指令遵循能力

    Conifer: Improving Complex Constrained Instruction-Following Ability of Large Language Models

    [https://arxiv.org/abs/2404.02823](https://arxiv.org/abs/2404.02823)

    Conifer提出了一个新的指令调节数据集，通过LLMs驱动的细化过程，以及渐进学习方案，显著提高了大型语言模型遵循具有复杂约束的多层指令的能力

    

    大型语言模型(LLMs)遵循指令的能力对实际应用至关重要。尽管最近取得进展，但一些研究指出，LLMs在面对具有挑战性指令时存在困难，特别是包含复杂约束的指令，阻碍了它们在各种任务中的有效性。为解决这一挑战，我们引入了Conifer，这是一个新颖的指令调节数据集，旨在增强LLMs遵循具有复杂约束的多层指令。通过一系列LLM驱动的细化过程，我们利用GPT-4策划了这个数据集以确保高质量。我们还提出了一个强调易于难的渐进学习方案，并从过程反馈中学习。使用Conifer训练的模型在遵循指令能力方面表现出显著改善，特别是对于带有复杂约束的指令。在几个遵循指令的基准测试中，我们的7B模型表现优异

    arXiv:2404.02823v1 Announce Type: cross  Abstract: The ability of large language models (LLMs) to follow instructions is crucial to real-world applications. Despite recent advances, several studies have highlighted that LLMs struggle when faced with challenging instructions, especially those that include complex constraints, hindering their effectiveness in various tasks. To address this challenge, we introduce Conifer, a novel instruction tuning dataset, designed to enhance LLMs to follow multi-level instructions with complex constraints. Utilizing GPT-4, we curate the dataset by a series of LLM-driven refinement processes to ensure high quality. We also propose a progressive learning scheme that emphasizes an easy-to-hard progression, and learning from process feedback. Models trained with Conifer exhibit remarkable improvements in instruction-following abilities, especially for instructions with complex constraints. On several instruction-following benchmarks, our 7B model outperfor
    
[^15]: 使用机器学习识别国家法律和政策中的气候目标

    Identifying Climate Targets in National Laws and Policies using Machine Learning

    [https://arxiv.org/abs/2404.02822](https://arxiv.org/abs/2404.02822)

    本文提出了一种从国家法律和政策中提取气候目标的方法，可以可靠地识别出三类目标（“净零”，“减少”和“其他”），并调查了与模型相关的偏见和公平影响。

    

    定量政策目标是气候政策的基本要素，通常以领域特定和技术性语言为特征。目前，筛选全球气候政策目标的方法涉及大量手动工作。目前很少有可扩展的方法从国家法律或政策中提取气候目标，这限制了政策制定者和研究人员评估私营和公共部门与全球目标的一致性以及为政策决策提供信息的能力。在本文中，我们提出了一种从国家法律和政策中提取气候目标提及的方法。我们创建了一个专家注释的数据集，识别了三类目标（“净零”，“减少”和“其他”（例如可再生能源目标）），并训练了一个可靠地在文本中识别它们的分类器。我们调查了与我们模型相关的偏差和公平影响，并确定了特定年份和国家名称作为问题。

    arXiv:2404.02822v1 Announce Type: cross  Abstract: Quantified policy targets are a fundamental element of climate policy, typically characterised by domain-specific and technical language. Current methods for curating comprehensive views of global climate policy targets entail significant manual effort. At present there are few scalable methods for extracting climate targets from national laws or policies, which limits policymakers' and researchers' ability to (1) assess private and public sector alignment with global goals and (2) inform policy decisions. In this paper we present an approach for extracting mentions of climate targets from national laws and policies. We create an expert-annotated dataset identifying three categories of target ('Net Zero', 'Reduction' and 'Other' (e.g. renewable energy targets)) and train a classifier to reliably identify them in text. We investigate bias and equity impacts related to our model and identify specific years and country names as problemati
    
[^16]: 生成-对比异构图神经网络

    Generative-Contrastive Heterogeneous Graph Neural Network

    [https://arxiv.org/abs/2404.02810](https://arxiv.org/abs/2404.02810)

    本研究提出了一种生成-对比异构图神经网络，通过对比视图增强策略、位置感知和语义感知正样本采样策略以及分层对比学习策略来克服图数据增强的限制。

    

    异构图表达了现实世界中复杂关系，包括多种类型的节点和边。受自监督学习启发，对比异构图神经网络(HGNNs)利用数据增强和辨别器展现了巨大潜力用于下游任务。然而，由于图的离散和抽象特性，数据增强仍然存在限制。为了解决上述限制，我们提出了一种新颖的\textit{生成-对比异构图神经网络(GC-HGNN)}。

    arXiv:2404.02810v1 Announce Type: new  Abstract: Heterogeneous Graphs (HGs) can effectively model complex relationships in the real world by multi-type nodes and edges. In recent years, inspired by self-supervised learning, contrastive Heterogeneous Graphs Neural Networks (HGNNs) have shown great potential by utilizing data augmentation and discriminators for downstream tasks. However, data augmentation is still limited due to the discrete and abstract nature of graphs. To tackle the above limitations, we propose a novel \textit{Generative-Contrastive Heterogeneous Graph Neural Network (GC-HGNN)}. Specifically, we first propose a heterogeneous graph generative learning enhanced contrastive paradigm. This paradigm includes: 1) A contrastive view augmentation strategy by using masked autoencoder. 2) Position-aware and semantics-aware positive sample sampling strategy for generate hard negative samples. 3) A hierarchical contrastive learning strategy for capturing local and global informa
    
[^17]: 通过元学习实现领域泛化：一项调查

    Domain Generalization through Meta-Learning: A Survey

    [https://arxiv.org/abs/2404.02785](https://arxiv.org/abs/2404.02785)

    元学习是一种有前景的方法，通过获取可转移知识实现在各种任务之间快速适应，为解决深度神经网络在面对分布变化和有限标记数据时泛化能力不佳提供了新途径。

    

    深度神经网络(DNNs)已经彻底改变了人工智能，但是当面对分布之外(out-of-distribution, OOD)数据时往往表现不佳，这是因为在现实世界应用中由于领域转移不可避免，训练和测试数据被假定为共享相同分布的常见情况。尽管DNNs在大量数据和计算能力方面非常有效，但它们很难应对分布变化和有限标记数据，导致过拟合和跨不同任务和领域的泛化能力不佳。元学习提供了一种有前途的方法，通过采用能够在各种任务之间获取可转移知识的算法进行快速适应，从而消除了需要从头学习每个任务的必要性。本调查论文深入探讨了元学习领域，重点关注其对领域泛化的贡献。

    arXiv:2404.02785v1 Announce Type: cross  Abstract: Deep neural networks (DNNs) have revolutionized artificial intelligence but often lack performance when faced with out-of-distribution (OOD) data, a common scenario due to the inevitable domain shifts in real-world applications. This limitation stems from the common assumption that training and testing data share the same distribution-an assumption frequently violated in practice. Despite their effectiveness with large amounts of data and computational power, DNNs struggle with distributional shifts and limited labeled data, leading to overfitting and poor generalization across various tasks and domains. Meta-learning presents a promising approach by employing algorithms that acquire transferable knowledge across various tasks for fast adaptation, eliminating the need to learn each task from scratch. This survey paper delves into the realm of meta-learning with a focus on its contribution to domain generalization. We first clarify the 
    
[^18]: 联邦计算——构建块、扩展和系统的调研

    Federated Computing -- Survey on Building Blocks, Extensions and Systems

    [https://arxiv.org/abs/2404.02779](https://arxiv.org/abs/2404.02779)

    联邦计算旨在通过建立分布式网络，每个设备保留对自身数据的控制并参与集体计算，从而实现协作处理，其尊重用户隐私和数据主权的原则与当下对负责任AI和道德数据实践的需求相契合。

    

    针对数据量增大和敏感性增强，传统的集中式计算模型面临挑战，如数据安全漏洞和监管障碍。联邦计算（FC）通过实现协作处理而不损害个人数据隐私来应对这些问题。这一目标通过分布式设备网络实现，每个设备在参与集体计算的同时保留对其数据的控制。FC的动机不仅限于技术考虑，还包括社会影响。随着对负责任的人工智能和道德数据实践的需求加剧，FC符合用户赋权和数据主权原则。FC包括联邦学习（FL）和联邦分析（FA）。随着时间推移，FC系统变得越来越复杂，目前缺乏明确定义和描述其组成部分的分类。当前的调研只涵盖特定领域的FL应用情况。

    arXiv:2404.02779v1 Announce Type: new  Abstract: In response to the increasing volume and sensitivity of data, traditional centralized computing models face challenges, such as data security breaches and regulatory hurdles. Federated Computing (FC) addresses these concerns by enabling collaborative processing without compromising individual data privacy. This is achieved through a decentralized network of devices, each retaining control over its data, while participating in collective computations. The motivation behind FC extends beyond technical considerations to encompass societal implications. As the need for responsible AI and ethical data practices intensifies, FC aligns with the principles of user empowerment and data sovereignty. FC comprises of Federated Learning (FL) and Federated Analytics (FA). FC systems became more complex over time and they currently lack a clear definition and taxonomy describing its moving pieces. Current surveys capture domain-specific FL use cases, d
    
[^19]: AQuA --结合专家和非专家观点，利用LLMs评估在线讨论中的磋商质量

    AQuA -- Combining Experts' and Non-Experts' Views To Assess Deliberation Quality in Online Discussions Using LLMs

    [https://arxiv.org/abs/2404.02761](https://arxiv.org/abs/2404.02761)

    提出了AQuA，一种综合的磋商质量得分计算方法，可以从多个指标中提取各个讨论帖子的统一得分，保留了评论中磋商方面的信息，提高了模型的透明度。

    

    在政治在线讨论中衡量贡献质量对于研究磋商和计算机科学至关重要。随着深度学习的进步，自动衡量这些指标变得可行。本文介绍了AQuA，它是一个添加分数，从多个指标中计算每个讨论帖子的统一磋商质量得分。与其他特定分数不同，AQuA保留了评论中存在的磋商方面的信息，增强了模型的透明度。

    arXiv:2404.02761v1 Announce Type: cross  Abstract: Measuring the quality of contributions in political online discussions is crucial in deliberation research and computer science. Research has identified various indicators to assess online discussion quality, and with deep learning advancements, automating these measures has become feasible. While some studies focus on analyzing specific quality indicators, a comprehensive quality score incorporating various deliberative aspects is often preferred. In this work, we introduce AQuA, an additive score that calculates a unified deliberative quality score from multiple indices for each discussion post. Unlike other singular scores, AQuA preserves information on the deliberative aspects present in comments, enhancing model transparency. We develop adapter models for 20 deliberative indices, and calculate correlation coefficients between experts' annotations and the perceived deliberativeness by non-experts to weigh the individual indices int
    
[^20]: 来自稀疏点云的无监督占用学习

    Unsupervised Occupancy Learning from Sparse Point Cloud

    [https://arxiv.org/abs/2404.02759](https://arxiv.org/abs/2404.02759)

    提出了一种从稀疏输入学习占用场的无监督学习方法，通过基于边界不确定性的采样和最小熵场优化来解决从3D点云学习SDF的挑战

    

    隐式神经表示已经成为一个强大的框架，可用于捕获复杂的数据模态，涵盖从3D形状到图像和音频等广泛范围。在3D形状表示领域，神经符号距离函数（SDF）展现出从根本上编码复杂形状几何的显著潜力。然而，在没有地面真实监督的情况下，从3D点云中学习SDF仍然是一项非常具有挑战性的任务。在本文中，我们提出了一种方法来推断占用场而不是SDF，因为它们更容易从稀疏输入中学习。我们利用基于边界不确定性的边际测量，不同地从占用函数的决策边界中采样，并使用输入点云监督采样的边界点。我们在训练的早期阶段通过将占用函数偏向最小熵场来进一步稳定优化过程。

    arXiv:2404.02759v1 Announce Type: cross  Abstract: Implicit Neural Representations have gained prominence as a powerful framework for capturing complex data modalities, encompassing a wide range from 3D shapes to images and audio. Within the realm of 3D shape representation, Neural Signed Distance Functions (SDF) have demonstrated remarkable potential in faithfully encoding intricate shape geometry. However, learning SDFs from 3D point clouds in the absence of ground truth supervision remains a very challenging task. In this paper, we propose a method to infer occupancy fields instead of SDFs as they are easier to learn from sparse inputs. We leverage a margin-based uncertainty measure to differentially sample from the decision boundary of the occupancy function and supervise the sampled boundary points using the input point cloud. We further stabilize the optimization process at the early stages of the training by biasing the occupancy function towards minimal entropy fields while max
    
[^21]: 从长尾分布中不断学习大量任务

    Continual Learning of Numerous Tasks from Long-tail Distributions

    [https://arxiv.org/abs/2404.02754](https://arxiv.org/abs/2404.02754)

    本文研究了在长尾分布下大量任务的不断学习算法表现，并提出了利用优化器状态改进不断学习性能的方法。

    

    Continual learning，作为人工智能和机器学习研究的一个重要方面，致力于开发能够学习并适应新任务同时保留先前获得知识的模型。现有的不断学习算法通常涉及少量任务，这些任务规模相同，并且可能无法准确地代表现实世界的学习场景。本文研究具有大量任务的不断学习算法在长尾任务大小分布下的表现。我们设计了一个合成数据集和两个真实世界的不断学习数据集，以评估现有算法在这种场景下的表现。此外，我们研究了一个在不断学习中被忽视的因素，即优化器状态，如Adam优化器中的第一和第二时刻，并探讨如何利用它来提高不断学习的性能。我们提出了一种方法，用于重复利用

    arXiv:2404.02754v1 Announce Type: new  Abstract: Continual learning, an important aspect of artificial intelligence and machine learning research, focuses on developing models that learn and adapt to new tasks while retaining previously acquired knowledge. Existing continual learning algorithms usually involve a small number of tasks with uniform sizes and may not accurately represent real-world learning scenarios. In this paper, we investigate the performance of continual learning algorithms with a large number of tasks drawn from a task distribution that is long-tail in terms of task sizes. We design one synthetic dataset and two real-world continual learning datasets to evaluate the performance of existing algorithms in such a setting. Moreover, we study an overlooked factor in continual learning, the optimizer states, e.g. first and second moments in the Adam optimizer, and investigate how it can be used to improve continual learning performance. We propose a method that reuses the
    
[^22]: 在具有隐藏神经元的递归网络中学习序列吸引子

    Learning Sequence Attractors in Recurrent Networks with Hidden Neurons

    [https://arxiv.org/abs/2404.02729](https://arxiv.org/abs/2404.02729)

    本研究研究了具有隐藏神经元的递归网络如何学习序列吸引子，以稳健地存储和检索预定义的模式序列，结果表明网络需要包含隐藏神经元来存储任意模式序列，并开发了一种局部学习算法实现这一目标。

    

    大脑被设计用来处理时间序列信息。目前尚不清楚大脑是如何学习存储和检索序列记忆的。在本研究中，我们研究了具有二进制神经元的递归网络如何学习序列吸引子，以存储预定义的模式序列并稳健地检索它们。我们表明，为了存储任意模式序列，网络需要包含隐藏神经元，即使它们在显示序列记忆方面的作用是间接的。我们开发了一种局部学习算法，用于学习带有隐藏神经元的网络中的序列吸引子。该算法被证明会收敛并导致序列吸引子。我们展示了该网络模型可以在合成和真实数据集上稳健地存储和检索序列。我们希望这项研究能够为理解大脑中的序列记忆和时间信息处理提供新的见解。

    arXiv:2404.02729v1 Announce Type: cross  Abstract: The brain is targeted for processing temporal sequence information. It remains largely unclear how the brain learns to store and retrieve sequence memories. Here, we study how recurrent networks of binary neurons learn sequence attractors to store predefined pattern sequences and retrieve them robustly. We show that to store arbitrary pattern sequences, it is necessary for the network to include hidden neurons even though their role in displaying sequence memories is indirect. We develop a local learning algorithm to learn sequence attractors in the networks with hidden neurons. The algorithm is proven to converge and lead to sequence attractors. We demonstrate that the network model can store and retrieve sequences robustly on synthetic and real-world datasets. We hope that this study provides new insights in understanding sequence memory and temporal information processing in the brain.
    
[^23]: 机器人学中有效动作的无监督学习

    Unsupervised Learning of Effective Actions in Robotics

    [https://arxiv.org/abs/2404.02728](https://arxiv.org/abs/2404.02728)

    该论文提出了一种无监督算法，通过在探索阶段将连续运动空间离散化, 自动生成“动作原型”, 从而实现机器人动作的效果驱动学习

    

    学习与决策相关且可以有效执行的动作是自主机器人中的关键问题。当前机器人学中最先进的动作表示缺乏对机器人动作的适当效果驱动学习。尽管深度学习方法在解决操纵任务方面取得成功，但它们也缺乏这种能力，而且在内存或训练数据方面成本高。在本文中，我们提出了一种无监督算法，用于对连续运动空间进行离散化，生成“动作原型”，每个原型在环境中产生不同的效果。在探索阶段之后，该算法会自动构建对效果的表示，并将动作分组为动作原型，其中更有可能产生效果的动作比导致可忽略变化的动作更多地表示。我们在模拟楼梯攀登强化学习任务上评估了我们的方法，初步结果表明...

    arXiv:2404.02728v1 Announce Type: cross  Abstract: Learning actions that are relevant to decision-making and can be executed effectively is a key problem in autonomous robotics. Current state-of-the-art action representations in robotics lack proper effect-driven learning of the robot's actions. Although successful in solving manipulation tasks, deep learning methods also lack this ability, in addition to their high cost in terms of memory or training data. In this paper, we propose an unsupervised algorithm to discretize a continuous motion space and generate "action prototypes", each producing different effects in the environment. After an exploration phase, the algorithm automatically builds a representation of the effects and groups motions into action prototypes, where motions more likely to produce an effect are represented more than those that lead to negligible changes. We evaluate our method on a simulated stair-climbing reinforcement learning task, and the preliminary results
    
[^24]: 利用大规模视觉语言模型进行合成图像检测

    Harnessing the Power of Large Vision Language Models for Synthetic Image Detection

    [https://arxiv.org/abs/2404.02726](https://arxiv.org/abs/2404.02726)

    本研究旨在探讨利用先进的视觉语言模型进行合成图像识别，并通过调整图像字幕模型提高合成图像检测的准确性，为区分真实图像与合成图像做出贡献。

    

    近年来，能够从文本生成图像的模型的出现引起了广泛关注，这为从文本描述中创建逼真图像的可能性。然而，这些进展也引发了对这些图像潜在滥用的担忧，包括制造虚假新闻和宣传等误导性内容的可能性。本研究调查了利用先进的视觉语言模型（VLMs）进行合成图像识别的有效性。具体来说，重点是调整最先进的图像字幕模型以进行合成图像检测。通过利用大型VLMs的强大理解能力，旨在区分由扩散型模型生成的合成图像与真实图像。本研究通过利用诸如BLIP-2和ViTGPT2等视觉语言模型的能力，推动了合成图像检测的发展。

    arXiv:2404.02726v1 Announce Type: cross  Abstract: In recent years, the emergence of models capable of generating images from text has attracted considerable interest, offering the possibility of creating realistic images from text descriptions. Yet these advances have also raised concerns about the potential misuse of these images, including the creation of misleading content such as fake news and propaganda. This study investigates the effectiveness of using advanced vision-language models (VLMs) for synthetic image identification. Specifically, the focus is on tuning state-of-the-art image captioning models for synthetic image detection. By harnessing the robust understanding capabilities of large VLMs, the aim is to distinguish authentic images from synthetic images produced by diffusion-based models. This study contributes to the advancement of synthetic image detection by exploiting the capabilities of visual language models such as BLIP-2 and ViTGPT2. By tailoring image captioni
    
[^25]: 在线符合化神经网络集成用于日前电力价格的概率预测

    On-line conformalized neural networks ensembles for probabilistic forecasting of day-ahead electricity prices

    [https://arxiv.org/abs/2404.02722](https://arxiv.org/abs/2404.02722)

    提出了一种通过基于符合推理技术的在线重新校准程序扩展最新技术的神经网络集成方法，用于改进日前电力价格的概率预测。

    

    概率性电力价格预测（PEPF）受到越来越多的关注，因为人们需要正确量化预测不确定性，以支持在不断增加可再生能源的复杂电力市场中的运营。分布式神经网络集成最近被证明能够胜过PEPF基准的最新技术。然而，它们需要关键的可靠性增强，因为在预测时间范围的各个步骤上未能通过覆盖率测试。在这项工作中，我们提出了一种新颖的PEPF方法，通过基于符合推理技术的在线重新校准程序，扩展了最新技术的神经网络集成方法。在多个市场地区进行了实验，实现了具有改进的小时覆盖率和稳定概率得分的日前预测。

    arXiv:2404.02722v1 Announce Type: new  Abstract: Probabilistic electricity price forecasting (PEPF) is subject of increasing interest, following the demand for proper quantification of prediction uncertainty, to support the operation in complex power markets with increasing share of renewable generation. Distributional neural networks ensembles have been recently shown to outperform state of the art PEPF benchmarks. Still, they require critical reliability enhancements, as fail to pass the coverage tests at various steps on the prediction horizon. In this work, we propose a novel approach to PEPF, extending the state of the art neural networks ensembles based methods through conformal inference based techniques, deployed within an on-line recalibration procedure. Experiments have been conducted on multiple market regions, achieving day-ahead forecasts with improved hourly coverage and stable probabilistic scores.
    
[^26]: 我们能否通过神经坍塌来理解可塑性？

    Can We Understand Plasticity Through Neural Collapse?

    [https://arxiv.org/abs/2404.02719](https://arxiv.org/abs/2404.02719)

    论文探讨了深度学习中可塑性损失和神经坍塌的关联，并引入了一种正则化方法来缓解神经坍塌，从而减轻了可塑性损失。

    

    这篇论文探讨了深度学习中两个最近确定的现象之间的联系：可塑性损失和神经坍塌。我们分析了它们在不同场景中的相关性，在第一个任务的初始训练阶段揭示了显著的关联。此外，我们引入了一种正则化方法来缓解神经坍塌，证明了在这种特定情况下减轻可塑性损失的有效性。

    arXiv:2404.02719v1 Announce Type: cross  Abstract: This paper explores the connection between two recently identified phenomena in deep learning: plasticity loss and neural collapse. We analyze their correlation in different scenarios, revealing a significant association during the initial training phase on the first task. Additionally, we introduce a regularization approach to mitigate neural collapse, demonstrating its effectiveness in alleviating plasticity loss in this specific setting.
    
[^27]: 大型语言模型的自动提示选择

    Automatic Prompt Selection for Large Language Models

    [https://arxiv.org/abs/2404.02717](https://arxiv.org/abs/2404.02717)

    通过在训练数据上进行聚类，使用基于LLM的提示生成器为每个簇生成候选提示，综合数据集进行训练以评估提示的相关性，最终在测试时使用评估器为新输入选择最佳提示，实现了大型语言模型的自动提示选择。

    

    大型语言模型（LLMs）可以在适当的提示指导下执行各种自然语言处理任务。然而，手动设计有效的提示具有挑战性且耗时。现有的自动提示优化方法要么缺乏灵活性，要么效率低下。在本文中，我们提出了一种有效的方法，以自动从一组有限的合成候选提示中为给定输入选择最佳提示。

    arXiv:2404.02717v1 Announce Type: new  Abstract: Large Language Models (LLMs) can perform various natural language processing tasks with suitable instruction prompts. However, designing effective prompts manually is challenging and time-consuming. Existing methods for automatic prompt optimization either lack flexibility or efficiency. In this paper, we propose an effective approach to automatically select the optimal prompt for a given input from a finite set of synthetic candidate prompts. Our approach consists of three steps: (1) clustering the training data and generating candidate prompts for each cluster using an LLM-based prompt generator; (2) synthesizing a dataset of input-prompt-output tuples for training a prompt evaluator to rank the prompts based on their relevance to the input; (3) using the prompt evaluator to select the best prompt for a new input at test time. Our approach balances prompt generality-specificity and eliminates the need for resource-intensive training an
    
[^28]: 深度隐私漏斗模型：从判别式方法到生成式方法的转变，并其在人脸识别中的应用

    Deep Privacy Funnel Model: From a Discriminative to a Generative Approach with an Application to Face Recognition

    [https://arxiv.org/abs/2404.02696](https://arxiv.org/abs/2404.02696)

    该研究将信息论隐私原则与表示学习相结合，提出了一种新的隐私保护表示学习方法，适用于人脸识别系统，并引入了生成式隐私漏斗模型。

    

    在这项研究中，我们将信息论隐私漏斗（PF）模型应用于人脸识别领域，开发了一种新的隐私保护表示学习方法，以端到端训练框架来实现。我们的方法解决了数据保护中模糊化和效用之间的权衡，通过对数损失（也称为自信息损失）来量化。这项研究详细探讨了信息论隐私原则与表示学习的整合，在特定关注于人脸识别系统。我们特别强调了我们的框架与人脸识别网络的最新进展（如AdaFace和ArcFace）之间的适应性。此外，我们还介绍了生成式隐私漏斗（GenPF）模型，这是一种超出传统PF模型范围的范例，被称为判别式隐私漏斗（DisPF）。

    arXiv:2404.02696v1 Announce Type: new  Abstract: In this study, we apply the information-theoretic Privacy Funnel (PF) model to the domain of face recognition, developing a novel method for privacy-preserving representation learning within an end-to-end training framework. Our approach addresses the trade-off between obfuscation and utility in data protection, quantified through logarithmic loss, also known as self-information loss. This research provides a foundational exploration into the integration of information-theoretic privacy principles with representation learning, focusing specifically on the face recognition systems. We particularly highlight the adaptability of our framework with recent advancements in face recognition networks, such as AdaFace and ArcFace. In addition, we introduce the Generative Privacy Funnel ($\mathsf{GenPF}$) model, a paradigm that extends beyond the traditional scope of the PF model, referred to as the Discriminative Privacy Funnel ($\mathsf{DisPF}$)
    
[^29]: 图形转换规则的自动推理

    Automated Inference of Graph Transformation Rules

    [https://arxiv.org/abs/2404.02692](https://arxiv.org/abs/2404.02692)

    提出了一种新颖的图形转换模型构建方法，结合生成和动态观点，实现完全自动化的数据驱动模型推理，通过压缩一组转换成一组规则，允许模型展示超出输入范围的行为。

    

    在生命科学领域可用数据的爆炸性增长推动了对富有表现力模型和计算方法日益增长的需求。图形转换是一种具有广泛应用的动态系统模型。我们引入了一种新颖的图形转换模型构建方法，将生成和动态观点结合起来，以提供一个完全自动化的数据驱动模型推理方法。该方法接受作为动态属性的输入，给定为由显式转换编码的动态的“快照”，并构建一个兼容的模型。获得的模型被保证是最小的，因此将该方法规范为模型压缩（将一组转换压缩为一组规则）的方法。压缩对有损情况很宽容，即允许构建的模型展示超出输入转换范围的行为，从而建议完成输入动态的方法。

    arXiv:2404.02692v1 Announce Type: cross  Abstract: The explosion of data available in life sciences is fueling an increasing demand for expressive models and computational methods. Graph transformation is a model for dynamic systems with a large variety of applications. We introduce a novel method of the graph transformation model construction, combining generative and dynamical viewpoints to give a fully automated data-driven model inference method.   The method takes the input dynamical properties, given as a "snapshot" of the dynamics encoded by explicit transitions, and constructs a compatible model. The obtained model is guaranteed to be minimal, thus framing the approach as model compression (from a set of transitions into a set of rules). The compression is permissive to a lossy case, where the constructed model is allowed to exhibit behavior outside of the input transitions, thus suggesting a completion of the input dynamics.   The task of graph transformation model inference i
    
[^30]: 注意力机制在高斯分布输入下自然稀疏

    Attention is Naturally Sparse with Gaussian Distributed Input

    [https://arxiv.org/abs/2404.02690](https://arxiv.org/abs/2404.02690)

    通过对高斯输入下注意力得分稀疏性进行理论分析，揭示了注意力机制中稀疏性的特征及其对计算效率的影响。

    

    大型语言模型（LLMs）的计算强度是关键瓶颈，主要是由于transformer架构中注意力机制的$O(n^2)$复杂度。稀疏注意力作为一个关键创新应运而生，旨在减少计算负荷同时保持模型性能。本研究对LLMs内的注意力分数稀疏性进行了严格的理论分析，特别是在高斯输入框架下。通过建立一组基础假设并采用一种系统的理论方法，我们揭示了注意力分数稀疏性的内在特征及其对计算效率的影响。我们的主要贡献在于提供了对注意力机制中稀疏性表现形式的详细理论检查，揭示了在计算节约和模型有效性之间潜在权衡的见解。

    arXiv:2404.02690v1 Announce Type: cross  Abstract: The computational intensity of Large Language Models (LLMs) is a critical bottleneck, primarily due to the $O(n^2)$ complexity of the attention mechanism in transformer architectures. Addressing this, sparse attention emerges as a key innovation, aiming to reduce computational load while maintaining model performance. This study presents a rigorous theoretical analysis of the sparsity in attention scores within LLMs, particularly under the framework of Gaussian inputs. By establishing a set of foundational assumptions and employing a methodical theoretical approach, we unravel the intrinsic characteristics of attention score sparsity and its implications on computational efficiency. Our main contribution lies in providing a detailed theoretical examination of how sparsity manifests in attention mechanisms, offering insights into the potential trade-offs between computational savings and model effectiveness. This work not only advances 
    
[^31]: 在分类控制原理中的强化学习

    Reinforcement Learning in Categorical Cybernetics

    [https://arxiv.org/abs/2404.02688](https://arxiv.org/abs/2404.02688)

    强化学习算法可以被归纳到分类控制原理框架中，通过参数化的光学相互作用，展示了新的构造方法。

    

    我们展示了几种主要的强化学习（RL）算法适用于分类控制原理框架，即参数化的双向过程。我们在此前的工作基础上展开，其中我们展示了价值迭代可以通过预合成特定的光学表示。本文的主要构造概述如下：（1）我们将Bellman算子扩展到适用于动作值函数并依赖于样本的参数化光学。 （2）我们应用一个可表示的逆变子函子，得到一个应用Bellman迭代的参数化函数。（3）该参数化函数成为另一个代表模型的参数化光学的反向传递，通过代理与环境进行交互。因此，在我们的构造中，参数化光学以两种不同的方式出现，其中一种成为另一种的一部分。

    arXiv:2404.02688v1 Announce Type: new  Abstract: We show that several major algorithms of reinforcement learning (RL) fit into the framework of categorical cybernetics, that is to say, parametrised bidirectional processes. We build on our previous work in which we show that value iteration can be represented by precomposition with a certain optic. The outline of the main construction in this paper is: (1) We extend the Bellman operators to parametrised optics that apply to action-value functions and depend on a sample. (2) We apply a representable contravariant functor, obtaining a parametrised function that applies the Bellman iteration. (3) This parametrised function becomes the backward pass of another parametrised optic that represents the model, which interacts with an environment via an agent. Thus, parametrised optics appear in two different ways in our construction, with one becoming part of the other. As we show, many of the major classes of algorithms in RL can be seen as dif
    
[^32]: 跨架构迁移学习用于线性成本推断变换器

    Cross-Architecture Transfer Learning for Linear-Cost Inference Transformers

    [https://arxiv.org/abs/2404.02684](https://arxiv.org/abs/2404.02684)

    提出了一种跨架构迁移学习方法，用于在线性成本推断和自注意力变换器之间共享组件的权重，以提高Transformer语言模型的效率。

    

    最近，提出了多种架构来通过改变自注意力模块的设计实现线性成本推断(LCI)以提高Transformer语言模型的效率。在这个领域中，一个值得注意的方法是状态空间机器（SSMs）架构，它在语言建模任务上显示出与自注意力变换器相当的性能。然而，这种架构更改需要从头开始完全预训练权重，这给希望使用新架构的研究人员和从业者带来了巨大成本。受传统线性注意力工作的启发，我们提出了跨架构迁移学习(XATL)，其中LCI和基于自注意力的变换器之间的共享组件的权重，如层规范、MLP、输入/输出

    arXiv:2404.02684v1 Announce Type: cross  Abstract: Recently, multiple architectures has been proposed to improve the efficiency of the Transformer Language Models through changing the design of the self-attention block to have a linear-cost inference (LCI). A notable approach in this realm is the State-Space Machines (SSMs) architecture, which showed on-par performance on language modeling tasks with the self-attention transformers. However, such an architectural change requires a full pretraining of the weights from scratch, which incurs a huge cost to researchers and practitioners who want to use the new architectures. In the more traditional linear attention works, it has been proposed to approximate full attention with linear attention by swap-and-finetune framework. Motivated by this approach, we propose Cross-Architecture Transfer Learning (XATL), in which the weights of the shared components between LCI and self-attention-based transformers, such as layernorms, MLPs, input/outpu
    
[^33]: 对抗攻击与文本分类器中的维度性研究

    Adversarial Attacks and Dimensionality in Text Classifiers

    [https://arxiv.org/abs/2404.02660](https://arxiv.org/abs/2404.02660)

    在文本分类任务中研究对抗攻击，发现对抗样本的嵌入维度与其影响之间存在非常强烈的相关性。

    

    机器学习算法遭受对抗攻击已成为许多真实世界用例中人工智能采用的主要障碍。这些攻击通过在测试样本中引入微小且结构化的扰动或改变，从根本上削弱了高性能神经网络的能力，迫使其进行误分类。本文研究自然语言处理领域中的对抗样本，特别是文本分类任务，探讨了对抗性脆弱性的原因，特别是与模型固有维度性的关系。我们的关键发现是对抗样本的嵌入维度与其影响之间存在非常强烈的相关性。

    arXiv:2404.02660v1 Announce Type: new  Abstract: Adversarial attacks on machine learning algorithms have been a key deterrent to the adoption of AI in many real-world use cases. They significantly undermine the ability of high-performance neural networks by forcing misclassifications. These attacks introduce minute and structured perturbations or alterations in the test samples, imperceptible to human annotators in general, but trained neural networks and other models are sensitive to it. Historically, adversarial attacks have been first identified and studied in the domain of image processing. In this paper, we study adversarial examples in the field of natural language processing, specifically text classification tasks. We investigate the reasons for adversarial vulnerability, particularly in relation to the inherent dimensionality of the model. Our key finding is that there is a very strong correlation between the embedding dimensionality of the adversarial samples and their effecti
    
[^34]: 针对大型语言模型中未预料到的偏见的检测

    Towards detecting unanticipated bias in Large Language Models

    [https://arxiv.org/abs/2404.02650](https://arxiv.org/abs/2404.02650)

    本论文探索了在大型语言模型中检测未预料到的偏见的新途径，着重于不确定性量化和可解释人工智能方法。

    

    在过去一年中，像ChatGPT这样的大型语言模型（LLMs）已经被广泛使用，并展现出与以前的机器学习系统类似的公平性问题。当前研究主要集中于分析和量化这些训练数据中的偏见及其对这些模型决策的影响，同时制定减轻策略。这项研究主要针对与性别、种族、族裔和语言相关的众所周知的偏见。然而，很明显，LLMs也受到其他不太明显的内隐偏见的影响。这些模型的复杂性和通常的不透明性使得检测这些偏见具有挑战性，但由于它们在各种应用中潜在的负面影响，这是至关重要的。在本文中，我们探讨了在LLMs中检测这些未预料到的偏见的新途径，具体关注不确定性量化和可解释人工智能方法。这些方法旨在评估确定性

    arXiv:2404.02650v1 Announce Type: cross  Abstract: Over the last year, Large Language Models (LLMs) like ChatGPT have become widely available and have exhibited fairness issues similar to those in previous machine learning systems. Current research is primarily focused on analyzing and quantifying these biases in training data and their impact on the decisions of these models, alongside developing mitigation strategies. This research largely targets well-known biases related to gender, race, ethnicity, and language. However, it is clear that LLMs are also affected by other, less obvious implicit biases. The complex and often opaque nature of these models makes detecting such biases challenging, yet this is crucial due to their potential negative impact in various applications. In this paper, we explore new avenues for detecting these unanticipated biases in LLMs, focusing specifically on Uncertainty Quantification and Explainable AI methods. These approaches aim to assess the certainty
    
[^35]: 论决策中不确定性的重要性与大型语言模型

    On the Importance of Uncertainty in Decision-Making with Large Language Models

    [https://arxiv.org/abs/2404.02649](https://arxiv.org/abs/2404.02649)

    本研究调查了使用大型语言模型作为代理进行自然语言输入决策问题时，不确定性估计的重要性，并提出了集成不确定性估计到汤普森抽样策略的方法。

    

    我们调查了自然语言输入的决策问题中不确定性的作用。对于这样的任务，使用大型语言模型作为代理已经成为常态。然而，最近的方法中没有任何一个额外的阶段用于估计代理在决策任务中对世界的不确定性。我们关注以自然语言为输入的基本决策框架之一，即上下文臂，其中上下文信息包括文本。作为没有不确定性估计的方法代表，我们考虑一个带有贪婪策略的LLM臂，该策略选择对应于最大预测奖励的动作。我们将此基准与通过将不确定性集成到汤普森抽样策略中积极利用不确定性估计的LLM臂进行比较。我们采用不同的技术进行不确定性估计，例如拉普拉斯近似、辍学和Epin。

    arXiv:2404.02649v1 Announce Type: new  Abstract: We investigate the role of uncertainty in decision-making problems with natural language as input. For such tasks, using Large Language Models as agents has become the norm. However, none of the recent approaches employ any additional phase for estimating the uncertainty the agent has about the world during the decision-making task. We focus on a fundamental decision-making framework with natural language as input, which is the one of contextual bandits, where the context information consists of text. As a representative of the approaches with no uncertainty estimation, we consider an LLM bandit with a greedy policy, which picks the action corresponding to the largest predicted reward. We compare this baseline to LLM bandits that make active use of uncertainty estimation by integrating the uncertainty in a Thompson Sampling policy. We employ different techniques for uncertainty estimation, such as Laplace Approximation, Dropout, and Epin
    
[^36]: Effector: 一个用于区域解释的Python软件包

    Effector: A Python package for regional explanations

    [https://arxiv.org/abs/2404.02629](https://arxiv.org/abs/2404.02629)

    Effector是一个专注于区域特征效果的Python软件包，通过引入区域效果来降低全局特征效果方法中可能的异质性。

    

    全局特征效果方法解释一个输出模型，每个特征对应一个图。该图显示特征对输出的平均效果，例如年龄对年收入的影响。然而，当由异质局部效果推导出平均效果时，平均效果可能具有误导性，即明显偏离平均值。为了减少异质性，区域效果为每个特征提供多个图，每个图代表特定子空间内的平均效果。为了可解释性，子空间被定义为由逻辑规则链定义的超矩形，例如年龄对男性和女性的年收入的影响，以及不同专业经验水平。我们介绍了Effector，一个致力于区域特征效果的Python库。Effector实现了一些成熟的全局效果方法，评估每种方法的异质性，并基于此提供区域效果。

    arXiv:2404.02629v1 Announce Type: new  Abstract: Global feature effect methods explain a model outputting one plot per feature. The plot shows the average effect of the feature on the output, like the effect of age on the annual income. However, average effects may be misleading when derived from local effects that are heterogeneous, i.e., they significantly deviate from the average. To decrease the heterogeneity, regional effects provide multiple plots per feature, each representing the average effect within a specific subspace. For interpretability, subspaces are defined as hyperrectangles defined by a chain of logical rules, like age's effect on annual income separately for males and females and different levels of professional experience. We introduce Effector, a Python library dedicated to regional feature effects. Effector implements well-established global effect methods, assesses the heterogeneity of each method and, based on that, provides regional effects. Effector automatica
    
[^37]: 用于基于解释的自然语言推理的可微分整数线性规划求解器

    A Differentiable Integer Linear Programming Solver for Explanation-Based Natural Language Inference

    [https://arxiv.org/abs/2404.02625](https://arxiv.org/abs/2404.02625)

    Diff-Comb Explainer是一种基于可微黑盒组合求解器的神经符号架构，不需要对语义约束进行连续放松，相比传统解决方案表现更出色。

    

    Integer Linear Programming（ILP）被提出作为对自然语言推理（NLI）进行精确结构和语义约束编码的正式形式。然而，传统的ILP框架是不可微分的，这给基于深度学习的连续语言表示的整合带来了关键挑战。本文介绍了一种新方法，名为Diff-Comb Explainer，这是一种基于可微黑盒组合求解器（DBCS）的解释型NLI的神经符号架构。与现有的神经符号求解器不同，Diff-Comb Explainer不需要对语义约束进行连续放松，从而能够直接、更精确和高效地将神经表示融入到ILP公式中。我们的实验表明，与传统ILP求解器、神经符号黑盒求解器和Trans相比，Diff-Comb Explainer实现了更优越的性能。

    arXiv:2404.02625v1 Announce Type: cross  Abstract: Integer Linear Programming (ILP) has been proposed as a formalism for encoding precise structural and semantic constraints for Natural Language Inference (NLI). However, traditional ILP frameworks are non-differentiable, posing critical challenges for the integration of continuous language representations based on deep learning. In this paper, we introduce a novel approach, named Diff-Comb Explainer, a neuro-symbolic architecture for explanation-based NLI based on Differentiable BlackBox Combinatorial Solvers (DBCS). Differently from existing neuro-symbolic solvers, Diff-Comb Explainer does not necessitate a continuous relaxation of the semantic constraints, enabling a direct, more precise, and efficient incorporation of neural representations into the ILP formulation. Our experiments demonstrate that Diff-Comb Explainer achieves superior performance when compared to conventional ILP solvers, neuro-symbolic black-box solvers, and Trans
    
[^38]: 多项式图形Lasso：从高斯图形平稳信号中学习边缘

    Polynomial Graphical Lasso: Learning Edges from Gaussian Graph-Stationary Signals

    [https://arxiv.org/abs/2404.02621](https://arxiv.org/abs/2404.02621)

    从高斯图形平稳信号中学习边缘的一种新方法，多项式图形Lasso（PGL），通过将图形Lasso的优势与更全面模型相结合，在建模节点关系时提供了更大的灵活性，并通过低复杂度算法解决了复杂和非凸的优化问题，评估表明其优于几种替代方法。

    

    本文介绍了一种新的方法，即多项式图形Lasso（PGL），用于从节点信号中学习图形结构。我们的关键贡献在于将信号建模为高斯和在图中平稳，从而实现了一种将图形Lasso的优势与更全面模型相结合的图形学习公式。具体而言，我们假设精度矩阵可以采用所需图形的任何多项式形式，从而增加了对节点关系建模的灵活性。由于所得优化问题的复杂性和非凸性，我们（i）提出了一种低复杂度算法，交替估计图形和精度矩阵，并（ii）表征了其收敛性。我们通过全面的数值模拟使用合成和真实数据评估了PGL的性能，证明了其优于几种替代方法。总的来说，这种方法具有显著优势。

    arXiv:2404.02621v1 Announce Type: cross  Abstract: This paper introduces Polynomial Graphical Lasso (PGL), a new approach to learning graph structures from nodal signals. Our key contribution lies in modeling the signals as Gaussian and stationary on the graph, enabling the development of a graph-learning formulation that combines the strengths of graphical lasso with a more encompassing model. Specifically, we assume that the precision matrix can take any polynomial form of the sought graph, allowing for increased flexibility in modeling nodal relationships. Given the resulting complexity and nonconvexity of the resulting optimization problem, we (i) propose a low-complexity algorithm that alternates between estimating the graph and precision matrices, and (ii) characterize its convergence. We evaluate the performance of PGL through comprehensive numerical simulations using both synthetic and real data, demonstrating its superiority over several alternatives. Overall, this approach pr
    
[^39]: QFNN-FFD：用于金融欺诈检测的量子联邦神经网络

    QFNN-FFD: Quantum Federated Neural Network for Financial Fraud Detection

    [https://arxiv.org/abs/2404.02595](https://arxiv.org/abs/2404.02595)

    介绍了将量子机器学习和量子计算技术与联邦学习相结合的Quantum Federated Neural Network for Financial Fraud Detection (QFNN-FFD)框架，提出了一种安全、高效的欺诈交易识别方法，显著改进了欺诈检测并确保了数据机密性。

    

    这项研究介绍了Quantum Federated Neural Network for Financial Fraud Detection (QFNN-FFD)，这是一个融合了量子机器学习（QML）和量子计算技术与联邦学习（FL）的前沿框架，用于创新金融欺诈检测。利用量子技术的计算能力和FL的数据隐私，QFNN-FFD提出了一种安全、高效的识别欺诈交易的方法。在分布式客户端实施双阶段训练模型超越了现有的性能方法。QFNN-FFD显著改进了欺诈检测并确保了数据机密性，标志着金融科技解决方案的重大进步，并为以隐私为重点的欺诈检测建立了新标准。

    arXiv:2404.02595v1 Announce Type: cross  Abstract: This study introduces the Quantum Federated Neural Network for Financial Fraud Detection (QFNN-FFD), a cutting-edge framework merging Quantum Machine Learning (QML) and quantum computing with Federated Learning (FL) to innovate financial fraud detection. Using quantum technologies' computational power and FL's data privacy, QFNN-FFD presents a secure, efficient method for identifying fraudulent transactions. Implementing a dual-phase training model across distributed clients surpasses existing methods in performance. QFNN-FFD significantly improves fraud detection and ensures data confidentiality, marking a significant advancement in fintech solutions and establishing a new standard for privacy-focused fraud detection.
    
[^40]: 自适应采样政策意味着存在偏见信念: 一般化的热炉效应

    Adaptive Sampling Policies Imply Biased Beliefs: A Generalization of the Hot Stove Effect

    [https://arxiv.org/abs/2404.02591](https://arxiv.org/abs/2404.02591)

    学习者倾向于估计值为正的替代方案，避开估计值为负的替代方案，导致高估误差纠正但低估误差无法纠正，同时研究发现在一些情况下负估计会导致选择较少的样本量，这种消极偏见同样存在于贝叶斯学习者中。

    

    热炉效应是由于学习的适应性特性所导致的一种消极偏见。研究发现，追求估计值为正的替代方案，避开估计值为负的替代方案的学习算法将纠正高估误差，但无法纠正低估误差。本文将热炉效应的理论推广到负估计不一定导致避免而是导致样本量较小的情况（即，如果B被认为是次优的，学习者会选择更少的替代方案B而非完全避免B）。我们形式上证明了在这种设置中消极偏见仍然存在。我们还表明，贝叶斯学习者存在消极偏见，即大多数这样的学习者低估替代方案的预期价值。

    arXiv:2404.02591v1 Announce Type: new  Abstract: The Hot Stove Effect is a negativity bias resulting from the adaptive character of learning. The mechanism is that learning algorithms that pursue alternatives with positive estimated values, but avoid alternatives with negative estimated values, will correct errors of overestimation but fail to correct errors of underestimation. Here, we generalize the theory behind the Hot Stove Effect to settings in which negative estimates do not necessarily lead to avoidance but to a smaller sample size (i.e., a learner selects fewer of alternative B if B is believed to be inferior but does not entirely avoid B). We formally demonstrate that the negativity bias remains in this set-up. We also show there is a negativity bias for Bayesian learners in the sense that most such learners underestimate the expected value of an alternative.
    
[^41]: 基于Transformer的分阶分解用于大规模多阶段随机优化

    Transformer-based Stagewise Decomposition for Large-Scale Multistage Stochastic Optimization

    [https://arxiv.org/abs/2404.02583](https://arxiv.org/abs/2404.02583)

    TranSDDP是一种基于Transformer的创新分阶分解算法，通过使用Transformer模型的结构优势，实现了一种序贯方法来逼近值函数，在处理大规模多阶段随机优化问题中表现出有效性。

    

    解决大规模多阶段随机规划（MSP）问题是一个重大挑战，因为常用的分阶分解算法，包括随机对偶动态规划（SDDP），随着子问题规模和问题数量的增加，时间复杂度呈指数增长。传统方法通过逐步累积来自分阶子问题原始和对偶解的次梯度切割平面，将值函数近似为分段线性凸函数。为了克服这些局限，我们引入了一种新颖的基于Transformer的分阶分解算法TranSDDP。这种创新方法利用Transformer模型的结构优势，实现了一种序贯方法，用于整合次梯度切割平面以逼近值函数。通过我们的数值实验，我们确认了TranSDDP在处理MSP问题方面的有效性。它能够高效生成分段线性

    arXiv:2404.02583v1 Announce Type: new  Abstract: Solving large-scale multistage stochastic programming (MSP) problems poses a significant challenge as commonly used stagewise decomposition algorithms, including stochastic dual dynamic programming (SDDP), face growing time complexity as the subproblem size and problem count increase. Traditional approaches approximate the value functions as piecewise linear convex functions by incrementally accumulating subgradient cutting planes from the primal and dual solutions of stagewise subproblems. Recognizing these limitations, we introduce TranSDDP, a novel Transformer-based stagewise decomposition algorithm. This innovative approach leverages the structural advantages of the Transformer model, implementing a sequential method for integrating subgradient cutting planes to approximate the value function. Through our numerical experiments, we affirm TranSDDP's effectiveness in addressing MSP problems. It efficiently generates a piecewise linear 
    
[^42]: 使用课程学习和奖励工程的近端策略优化解决实际优化问题

    Solving a Real-World Optimization Problem Using Proximal Policy Optimization with Curriculum Learning and Reward Engineering

    [https://arxiv.org/abs/2404.02577](https://arxiv.org/abs/2404.02577)

    使用课程学习和奖励工程的近端策略优化来解决实际问题。

    

    我们提出了一个通过课程学习原则和精心设计的奖励工程训练的近端策略优化（PPO）代理，来优化一个现实世界中的高吞吐量废物分类设施。我们的工作解决了有效平衡操作安全性、优化容量和最小化资源使用等竞争性目标的挑战。一个从头开始训练的基本代理在这些多重标准上失败解决问题，因为其固有复杂性。

    arXiv:2404.02577v1 Announce Type: new  Abstract: We present a proximal policy optimization (PPO) agent trained through curriculum learning (CL) principles and meticulous reward engineering to optimize a real-world high-throughput waste sorting facility. Our work addresses the challenge of effectively balancing the competing objectives of operational safety, volume optimization, and minimizing resource usage. A vanilla agent trained from scratch on these multiple criteria fails to solve the problem due to its inherent complexities. This problem is particularly difficult due to the environment's extremely delayed rewards with long time horizons and class (or action) imbalance, with important actions being infrequent in the optimal policy. This forces the agent to anticipate long-term action consequences and prioritize rare but rewarding behaviours, creating a non-trivial reinforcement learning task. Our five-stage CL approach tackles these challenges by gradually increasing the complexit
    
[^43]: 具有概念漂移检测和基于原型的嵌入的图流分类的增量学习

    Incremental Learning with Concept Drift Detection and Prototype-based Embeddings for Graph Stream Classification

    [https://arxiv.org/abs/2404.02572](https://arxiv.org/abs/2404.02572)

    这项工作提出了一种新颖的图流分类方法，通过增量学习实现持续模型适应，选择每个类别的代表性图，并创建图嵌入，以解决图流分类中的概念漂移问题。

    

    数据流挖掘旨在从不断变化的数据流中提取有意义的知识，解决非静态环境带来的挑战，特别是指随时间改变的基础数据分布的概念漂移。图结构提供了一个强大的建模工具，用于表示复杂系统，比如关键基础设施系统和社交网络。从图流中学习变得必不可少，以了解图结构的动态并促进明智决策。本工作介绍了一种新颖的用于图流分类的方法，在数据生成过程产生随时间变化的节点和边的图的一般设置下运行。该方法使用增量学习进行持续模型适应，为每个类别选择代表性图（原型）并创建图嵌入。此外，它还包含基于损失的概念

    arXiv:2404.02572v1 Announce Type: new  Abstract: Data stream mining aims at extracting meaningful knowledge from continually evolving data streams, addressing the challenges posed by nonstationary environments, particularly, concept drift which refers to a change in the underlying data distribution over time. Graph structures offer a powerful modelling tool to represent complex systems, such as, critical infrastructure systems and social networks. Learning from graph streams becomes a necessity to understand the dynamics of graph structures and to facilitate informed decision-making. This work introduces a novel method for graph stream classification which operates under the general setting where a data generating process produces graphs with varying nodes and edges over time. The method uses incremental learning for continual model adaptation, selecting representative graphs (prototypes) for each class, and creating graph embeddings. Additionally, it incorporates a loss-based concept 
    
[^44]: 一种具有专家引导神经回归树的可解释电力系统暂态稳定性评估方法

    An Interpretable Power System Transient Stability Assessment Method with Expert Guiding Neural-Regression-Tree

    [https://arxiv.org/abs/2404.02555](https://arxiv.org/abs/2404.02555)

    提出了一种利用专家引导非线性回归树来近似神经网络预测的、旨在解决网络解决方案可解释性问题的电力系统暂态稳定性评估方法。

    

    基于深度学习的暂态稳定性评估(TSA)取得了巨大成功，但可解释性不足阻碍了其工业应用。为解决这些问题，提出了一种具有专家引导神经回归树的可解释电力系统暂态稳定性评估方法(TSA-ENRT)。

    arXiv:2404.02555v1 Announce Type: cross  Abstract: Deep learning based transient stability assessment (TSA) has achieved great success, yet the lack of interpretability hinders its industrial application. Although a great number of studies have tried to explore the interpretability of network solutions, many problems still remain unsolved: (1) the difference between the widely accepted power system knowledge and the generated interpretive rules is large, (2) the probability characteristics of the neural network have not been fully considered during generating the interpretive rules, (3) the cost of the trade-off between accuracy and interpretability is too heavy to take. To address these issues, an interpretable power system Transient Stability Assessment method with Expert guiding Neural-Regression-Tree (TSA-ENRT) is proposed. TSA-ENRT utilizes an expert guiding nonlinear regression tree to approximate the neural network prediction and the neural network can be explained by the interp
    
[^45]: 离线强化学习的格点映射伪计数约束

    Grid-Mapping Pseudo-Count Constraint for Offline Reinforcement Learning

    [https://arxiv.org/abs/2404.02545](https://arxiv.org/abs/2404.02545)

    提出了一种用于连续领域的新的计数方法，称为格点映射伪计数方法（GPC），以适应离线环境中的强化学习问题，并在惩罚Q值的同时减少计算成本。

    

    离线强化学习是从静态数据集中学习而不与环境进行交互的方法，这确保了安全性并因此具有良好的应用前景。然而，直接应用朴素的强化学习方法通常在离线环境中失败，因为由于超出分布（OOD）行为引起的函数逼近误差。为了解决这个问题，现有算法主要惩罚OOD行为的Q值，其约束的质量也很重要。不精确的约束可能导致次优解，而精确的约束则需要显著的计算成本。在本文中，我们提出了一种新颖的连续领域计数方法，称为格点映射伪计数方法（GPC），以适当地惩罚Q值并减少计算成本。所提出的方法将状态和动作空间映射到离散空间，并通过伪计数约束它们的Q值。这是一个理论性

    arXiv:2404.02545v1 Announce Type: cross  Abstract: Offline reinforcement learning learns from a static dataset without interacting with the environment, which ensures security and thus owns a good prospect of application. However, directly applying naive reinforcement learning methods usually fails in an offline environment due to function approximation errors caused by out-of-distribution(OOD) actions. To solve this problem, existing algorithms mainly penalize the Q-value of OOD actions, the quality of whose constraints also matter. Imprecise constraints may lead to suboptimal solutions, while precise constraints require significant computational costs. In this paper, we propose a novel count-based method for continuous domains, called Grid-Mapping Pseudo-Count method(GPC), to penalize the Q-value appropriately and reduce the computational cost. The proposed method maps the state and action space to discrete space and constrains their Q-values through the pseudo-count. It is theoretic
    
[^46]: 流匹配在潜空间中的收敛性分析与Transformer

    Convergence Analysis of Flow Matching in Latent Space with Transformers

    [https://arxiv.org/abs/2404.02538](https://arxiv.org/abs/2404.02538)

    该研究提出了一种基于ODE的生成模型中，使用Transformer实现流匹配在潜空间中的理论收敛性保证，并展示了其在估计ODE流生成样本分布时的有效性，同时还证明了具有利普希茨连续性的Transformer网络可以有效逼近任意光滑函数。

    

    我们提出了ODE-based生成模型，特别是流匹配的理论收敛性保证。我们使用预训练的自编码器网络将高维原始输入映射到低维潜空间，其中一个Transformer网络被训练来预测从标准正态分布到目标潜空间分布的变换速度场。我们的误差分析展示了这种方法的有效性，表明通过估计的ODE流生成样本的分布在温斯坦-2距离下收敛到目标分布，这在温和且实际的假设下成立。此外，我们展示了具有利普希茨连续性的Transformer网络可以有效地逼近任意光滑函数，这可能是独立感兴趣的。

    arXiv:2404.02538v1 Announce Type: cross  Abstract: We present theoretical convergence guarantees for ODE-based generative models, specifically flow matching. We use a pre-trained autoencoder network to map high-dimensional original inputs to a low-dimensional latent space, where a transformer network is trained to predict the velocity field of the transformation from a standard normal distribution to the target latent distribution. Our error analysis demonstrates the effectiveness of this approach, showing that the distribution of samples generated via estimated ODE flow converges to the target distribution in the Wasserstein-2 distance under mild and practical assumptions. Furthermore, we show that arbitrary smooth functions can be effectively approximated by transformer networks with Lipschitz continuity, which may be of independent interest.
    
[^47]: 具有较少数据神谕的机器学习随机约束分散优化：一种梯度滑动方法

    Stochastic Constrained Decentralized Optimization for Machine Learning with Fewer Data Oracles: a Gradient Sliding Approach

    [https://arxiv.org/abs/2404.02511](https://arxiv.org/abs/2404.02511)

    提出了一种原始-对偶滑动与条件梯度滑动框架，实现了通信高效和最小化梯度计算次数的分散式优化算法，具有优化的梯度和线性优化复杂度。

    

    在现代分散式应用程序中，确保用户的通信效率和隐私是关键挑战。为了训练机器学习模型，算法必须与数据中心通信，并采样数据进行梯度计算，从而暴露数据并增加通信成本。这导致了需要一种通信高效且最小化梯度计算次数的分散式优化算法。为此，我们提出了原始-对偶滑动与条件梯度滑动框架，该框架通信高效，实现了具有梯度复杂度的 $\varepsilon$-近似解$O(1/\sqrt{\varepsilon}+\sigma^2/{\varepsilon^2})$ 和 $O(\log(1/\varepsilon)+\sigma^2/\varepsilon)$分别适用于凸和强凸设置，以及给定 随机 的LO (线性优化)复杂度，都是$O(1/\varepsilon^2)$。

    arXiv:2404.02511v1 Announce Type: cross  Abstract: In modern decentralized applications, ensuring communication efficiency and privacy for the users are the key challenges. In order to train machine-learning models, the algorithm has to communicate to the data center and sample data for its gradient computation, thus exposing the data and increasing the communication cost. This gives rise to the need for a decentralized optimization algorithm that is communication-efficient and minimizes the number of gradient computations. To this end, we propose the primal-dual sliding with conditional gradient sliding framework, which is communication-efficient and achieves an $\varepsilon$-approximate solution with the optimal gradient complexity of $O(1/\sqrt{\varepsilon}+\sigma^2/{\varepsilon^2})$ and $O(\log(1/\varepsilon)+\sigma^2/\varepsilon)$ for the convex and strongly convex setting respectively and an LO (Linear Optimization) complexity of $O(1/\varepsilon^2)$ for both settings given a sto
    
[^48]: 一种用于联邦学习的可解释客户端决策树聚合过程

    An Interpretable Client Decision Tree Aggregation process for Federated Learning

    [https://arxiv.org/abs/2404.02510](https://arxiv.org/abs/2404.02510)

    提出了一种用于联邦学习的可解释客户端决策树聚合过程，旨在解决在这些模型中注入可解释性的挑战。

    

    可信的人工智能解决方案在当今的数据驱动应用中至关重要，优先考虑诸如鲁棒性、安全性、透明性、可解释性和隐私性等原则。这导致联邦学习作为隐私和分布式机器学习的解决方案的出现。决策树作为自解释模型，在资源受限的环境中如联邦学习环境中进行协作模型训练是理想的，以在这些模型中注入可解释性。决策树结构使得在联邦学习环境中进行聚合并不是一件简单的事情。它们需要能够合并它们的决策路径而不引入偏差或过拟合的技术，同时保持聚合决策树的稳健性和泛化性。本文提出了一种适用于联邦学习场景的可解释客户端决策树聚合过程。

    arXiv:2404.02510v1 Announce Type: cross  Abstract: Trustworthy Artificial Intelligence solutions are essential in today's data-driven applications, prioritizing principles such as robustness, safety, transparency, explainability, and privacy among others. This has led to the emergence of Federated Learning as a solution for privacy and distributed machine learning. While decision trees, as self-explanatory models, are ideal for collaborative model training across multiple devices in resource-constrained environments such as federated learning environments for injecting interpretability in these models. Decision tree structure makes the aggregation in a federated learning environment not trivial. They require techniques that can merge their decision paths without introducing bias or overfitting while keeping the aggregated decision trees robust and generalizable. In this paper, we propose an Interpretable Client Decision Tree Aggregation process for Federated Learning scenarios that kee
    
[^49]: VIAssist：为视觉障碍用户调整多模大语言模型

    VIAssist: Adapting Multi-modal Large Language Models for Users with Visual Impairments

    [https://arxiv.org/abs/2404.02508](https://arxiv.org/abs/2404.02508)

    本论文研究了如何利用多模大语言模型（MLLMs）为视障人士提供视觉问题答案，提出了 VIAssist 系统，可以识别不受欢迎的图像并提供详细的操作，最终依据用户查询提供可靠的答案。

    

    具有视觉障碍的个体，包括视觉感知方面的部分或完全困难，被称为视障人士。全球估计有22亿人受视力障碍影响。近年来，多模大语言模型（MLLMs）的发展展示了它们在各个领域的非凡能力。希望通过MLLMs的视觉理解和推理能力来帮助视障人士。然而，由于难以捕捉理想图像以满足他们的日常需求，视障人士使用MLLMs具有挑战性。例如，目标对象未完全或部分放置在图像中。本文探讨了如何利用MLLMs为视障人士提供视觉问题答案。VIAssist能够识别不受欢迎的图像并提供详细的操作。最后，VIAssist可以根据用户的查询提供可靠的答案。

    arXiv:2404.02508v1 Announce Type: cross  Abstract: Individuals with visual impairments, encompassing both partial and total difficulties in visual perception, are referred to as visually impaired (VI) people. An estimated 2.2 billion individuals worldwide are affected by visual impairments. Recent advancements in multi-modal large language models (MLLMs) have showcased their extraordinary capabilities across various domains. It is desirable to help VI individuals with MLLMs' great capabilities of visual understanding and reasoning. However, it is challenging for VI people to use MLLMs due to the difficulties in capturing the desirable images to fulfill their daily requests. For example, the target object is not fully or partially placed in the image. This paper explores how to leverage MLLMs for VI individuals to provide visual-question answers. VIAssist can identify undesired images and provide detailed actions. Finally, VIAssist can provide reliable answers to users' queries based on
    
[^50]: 学习面向完全可观察非确定性计划领域的泛化策略

    Learning Generalized Policies for Fully Observable Non-Deterministic Planning Domains

    [https://arxiv.org/abs/2404.02499](https://arxiv.org/abs/2404.02499)

    本研究扩展了学习完全可观察、非确定性计划领域的泛化策略的方法，并通过实验评估了在一些 FOND 计划基准领域中产生的泛化策略的正确性。

    

    泛化策略代表解决大量计划问题的反应性策略，例如从给定领域中无限可解实例的集合。 提出了一种从一系列小训练实例中学习这种策略的方法，已成功应用于经典领域。 本文扩展了学习面向完全可观察、非确定性（FOND）领域的泛化策略的公式和导致的组合方法，通过一系列 FOND 计划基准领域的实验评估了生成的方法，展示了一些领域中产生的泛化策略，并证明了其正确性。 学习 FOND 计划的泛化策略方法实际上可以被看作是一种搜索结果的另一种 FOND 计划方法，这种方法不是在给定状态空间中搜索解决方案，而是在由必须学习的特征定义的抽象空间中搜索解决方案。

    arXiv:2404.02499v1 Announce Type: new  Abstract: General policies represent reactive strategies for solving large families of planning problems like the infinite collection of solvable instances from a given domain. Methods for learning such policies from a collection of small training instances have been developed successfully for classical domains. In this work, we extend the formulations and the resulting combinatorial methods for learning general policies over fully observable, non-deterministic (FOND) domains. We also evaluate the resulting approach experimentally over a number of benchmark domains in FOND planning, present the general policies that result in some of these domains, and prove their correctness. The method for learning general policies for FOND planning can actually be seen as an alternative FOND planning method that searches for solutions, not in the given state space but in an abstract space defined by features that must be learned as well.
    
[^51]: 测量大型语言模型的社会规范

    Measuring Social Norms of Large Language Models

    [https://arxiv.org/abs/2404.02491](https://arxiv.org/abs/2404.02491)

    论文提出了一个挑战，测试大型语言模型对社会规范的理解，构建了一个数据集涵盖广泛的社会规范问题，通过多代理框架基于大型语言模型来提高模型对社会规范的理解能力。

    

    我们提出了一个新的挑战，以检验大型语言模型是否理解社会规范。与现有数据集不同，我们的数据集要求具有解决社会规范的基本理解。我们的数据集包含最大的社会规范技能集，包括402项技能和12,383个问题，涵盖了从观点和论点到文化和法律等广泛的社会规范。我们根据K-12课程设计了我们的数据集。这使得可以直接将大型语言模型的社会理解能力与人类进行比较，更具体地说是与小学生进行比较。尽管先前的工作在我们的基准测试中产生几乎随机的准确度，但最近的大型语言模型（如GPT3.5-Turbo和LLaMA2-Chat）能够显著提高性能，仅略低于人类性能。然后，我们提出了一个基于大型语言模型的多代理框架，以提高模型理解社会规范的能力。

    arXiv:2404.02491v1 Announce Type: cross  Abstract: We present a new challenge to examine whether large language models understand social norms. In contrast to existing datasets, our dataset requires a fundamental understanding of social norms to solve. Our dataset features the largest set of social norm skills, consisting of 402 skills and 12,383 questions covering a wide set of social norms ranging from opinions and arguments to culture and laws. We design our dataset according to the K-12 curriculum. This enables the direct comparison of the social understanding of large language models to humans, more specifically, elementary students. While prior work generates nearly random accuracy on our benchmark, recent large language models such as GPT3.5-Turbo and LLaMA2-Chat are able to improve the performance significantly, only slightly below human performance. We then propose a multi-agent framework based on large language models to improve the models' ability to understand social norms.
    
[^52]: 药物协同作用预测的新方法

    New methods for drug synergy prediction

    [https://arxiv.org/abs/2404.02484](https://arxiv.org/abs/2404.02484)

    最佳方法准确解决了涉及已知药物或细胞系的药物协同作用预测情景，但仍未达到准确预测新药物或细胞系的水平。

    

    在这篇小型综述中，我们探讨了依赖于高通量组合筛选的药物组合协同作用的新预测方法。自2021年以来，该领域取得了迅速进展，已发表了超过30种原创机器学习方法，其中绝大多数是基于深度学习技术的。我们旨在通过突显方法中使用的核心技术、数据来源、输入数据类型和协同得分，以及论文所涉及的预测情景和评估协议，将这些论文放在一个统一的视角下。我们的发现是，最佳方法准确地解决了涉及已知药物或细胞系的协同作用预测情景，而涉及新药物或细胞系的情景仍未达到准确预测水平。

    arXiv:2404.02484v1 Announce Type: cross  Abstract: In this mini-review, we explore the new prediction methods for drug combination synergy relying on high-throughput combinatorial screens. The fast progress of the field is witnessed in the more than thirty original machine learning methods published since 2021, a clear majority of them based on deep learning techniques. We aim to put these papers under a unifying lens by highlighting the core technologies, the data sources, the input data types and synergy scores used in the methods, as well as the prediction scenarios and evaluation protocols that the papers deal with. Our finding is that the best methods accurately solve the synergy prediction scenarios involving known drugs or cell lines while the scenarios involving new drugs or cell lines still fall short of an accurate prediction level.
    
[^53]: FedSelect：个性化的联邦学习，通过定制化参数选择进行微调

    FedSelect: Personalized Federated Learning with Customized Selection of Parameters for Fine-Tuning

    [https://arxiv.org/abs/2404.02478](https://arxiv.org/abs/2404.02478)

    提出了一种受到彩票票据假设启发的新型个性化联邦学习算法FedSelect，能够在微调中定制选择网络参数，解决了全局知识存储不够优化的问题。

    

    标准的联邦学习方法在客户数据分布具有充分异质性时会受到影响。最近的方法通过个性化联邦学习（PFL）解决了客户数据异质性问题 - 一类旨在个性化学习的全局知识以更好地适应客户本地数据分布的FL算法。现有的PFL方法通常通过在特定层（即分类器头部）上执行个性化和对其余网络进行全局聚合来解耦深度神经网络的全局更新。然而，预先选择网络层进行个性化可能导致全局知识的存储不够优化。在这项工作中，我们提出了FedSelect，这是一种受到彩票票据假设中使用的迭代子网络发现过程启发的新型PFL算法。FedSelect逐步扩展子网络以个性化客户参数，并同时对剩余部分进行全局聚合。

    arXiv:2404.02478v1 Announce Type: cross  Abstract: Standard federated learning approaches suffer when client data distributions have sufficient heterogeneity. Recent methods addressed the client data heterogeneity issue via personalized federated learning (PFL) - a class of FL algorithms aiming to personalize learned global knowledge to better suit the clients' local data distributions. Existing PFL methods usually decouple global updates in deep neural networks by performing personalization on particular layers (i.e. classifier heads) and global aggregation for the rest of the network. However, preselecting network layers for personalization may result in suboptimal storage of global knowledge. In this work, we propose FedSelect, a novel PFL algorithm inspired by the iterative subnetwork discovery procedure used for the Lottery Ticket Hypothesis. FedSelect incrementally expands subnetworks to personalize client parameters, concurrently conducting global aggregations on the remaining p
    
[^54]: 用于旅行购买者问题的深度强化学习

    Deep Reinforcement Learning for Traveling Purchaser Problems

    [https://arxiv.org/abs/2404.02476](https://arxiv.org/abs/2404.02476)

    提出了一种基于深度强化学习的方法，该方法分别解决了旅行购买者问题中的路由构建和购买规划问题，并从全局角度评估和优化解决方案。

    

    旅行购买者问题（TPP）是一种具有广泛应用的重要组合优化问题。本文提出了一种基于深度强化学习（DRL）的新方法，该方法分别解决了路由构建和购买规划问题，同时从全局角度评估和优化解决方案。我们的方法的关键组成部分包括用于捕捉市场-产品关系的TPP的二部图表示，以及从二部图中提取信息并将其用于顺序构建路由的策略网络。

    arXiv:2404.02476v1 Announce Type: cross  Abstract: The traveling purchaser problem (TPP) is an important combinatorial optimization problem with broad applications. Due to the coupling between routing and purchasing, existing works on TPPs commonly address route construction and purchase planning simultaneously, which, however, leads to exact methods with high computational cost and heuristics with sophisticated design but limited performance. In sharp contrast, we propose a novel approach based on deep reinforcement learning (DRL), which addresses route construction and purchase planning separately, while evaluating and optimizing the solution from a global perspective. The key components of our approach include a bipartite graph representation for TPPs to capture the market-product relations, and a policy network that extracts information from the bipartite graph and uses it to sequentially construct the route. One significant benefit of our framework is that we can efficiently const
    
[^55]: uTeBC-NLP在SemEval-2024任务9中：LLMs能成为横向思考者吗？

    uTeBC-NLP at SemEval-2024 Task 9: Can LLMs be Lateral Thinkers?

    [https://arxiv.org/abs/2404.02474](https://arxiv.org/abs/2404.02474)

    通过研究提示工程方法如何增强LLMs在横向思考任务上的表现，揭示了其固有的超越思维能力，并发现压缩的信息性提示和动态的情境学习显著提升了模型性能。

    

    受人类认知启发，Jiang等人（2023c）创建了一个用于评估LLMs横向思维（超越思维定势）的基准。在这一基准的基础上，我们研究了不同的提示方法如何增强LLMs在这一任务上的表现，以揭示其固有的超越思维能力。通过参加SemEval-2024的第9项任务，即句子拼图子任务，我们探讨了提示工程方法：思维链（CoT）和直接提示，使用信息性描述进行增强，并利用检索增强生成（RAG）管道进行情境化提示。我们的实验涉及三种LLMs，包括GPT-3.5、GPT-4和Zephyr-7B-beta。我们使用GPT-4生成了谜题和选项之间的思维路径数据集，并通过人类进行了质量验证。研究结果表明，压缩的信息性提示能够提升模型性能。动态的情境学习显著提升了模型性能。

    arXiv:2404.02474v1 Announce Type: cross  Abstract: Inspired by human cognition, Jiang et al.(2023c) create a benchmark for assessing LLMs' lateral thinking-thinking outside the box. Building upon this benchmark, we investigate how different prompting methods enhance LLMs' performance on this task to reveal their inherent power for outside-the-box thinking ability. Through participating in SemEval-2024, task 9, Sentence Puzzle sub-task, we explore prompt engineering methods: chain of thoughts (CoT) and direct prompting, enhancing with informative descriptions, and employing contextualizing prompts using a retrieval augmented generation (RAG) pipeline. Our experiments involve three LLMs including GPT-3.5, GPT-4, and Zephyr-7B-beta. We generate a dataset of thinking paths between riddles and options using GPT-4, validated by humans for quality. Findings indicate that compressed informative prompts enhance performance. Dynamic in-context learning enhances model performance significantly. F
    
[^56]: 振动基础模型在物联网传感中的效率和稳健性：一个案例研究

    On the Efficiency and Robustness of Vibration-based Foundation Models for IoT Sensing: A Case Study

    [https://arxiv.org/abs/2404.02461](https://arxiv.org/abs/2404.02461)

    振动基础模型通过未标记数据的预训练，提高了在物联网应用中运行时推断的稳健性，并能够在部署中细化调整，加快应用部署时间并提高模型性能。

    

    本文展示了振动基础模型（FMs）利用未标记传感数据进行预训练的潜力，以提高（某类）物联网应用中运行时推断的稳健性。文章以一个车辆分类应用为例，使用声学和地震传感进行案例研究。该工作受到自然语言处理和计算机视觉领域基础模型的成功启发，将FM概念推广到其他领域，其中存在大量未标记数据可用于自监督预训练。物联网应用是这样的一个领域。物联网领域中选定传感模式的基础模型可以以与环境无关的方式利用现有的未标记传感器数据进行预训练，然后使用少量标记数据对部署进行微调。文章表明，预训练/微调方法可以缩短应用部署时间，提高模型性能，并且具有潜在的泛化能力。

    arXiv:2404.02461v1 Announce Type: new  Abstract: This paper demonstrates the potential of vibration-based Foundation Models (FMs), pre-trained with unlabeled sensing data, to improve the robustness of run-time inference in (a class of) IoT applications. A case study is presented featuring a vehicle classification application using acoustic and seismic sensing. The work is motivated by the success of foundation models in the areas of natural language processing and computer vision, leading to generalizations of the FM concept to other domains as well, where significant amounts of unlabeled data exist that can be used for self-supervised pre-training. One such domain is IoT applications. Foundation models for selected sensing modalities in the IoT domain can be pre-trained in an environment-agnostic fashion using available unlabeled sensor data and then fine-tuned to the deployment at hand using a small amount of labeled data. The paper shows that the pre-training/fine-tuning approach im
    
[^57]: PhonologyBench：评估大型语言模型的音韵技能

    PhonologyBench: Evaluating Phonological Skills of Large Language Models

    [https://arxiv.org/abs/2404.02456](https://arxiv.org/abs/2404.02456)

    PhonologyBench是一个新颖的基准测试，旨在明确评估大型语言模型在英语中的音韵技能，展示了LLMs在没有语音数据情况下在PhonologyBench任务上表现出显著性能。

    

    音韵学是研究语音结构和发音规则的学科，是大型语言模型（LLM）研究中一个关键但经常被忽视的组成部分。LLMs在各种利用音韵学的下游应用中被广泛使用，如教育工具和诗歌生成。此外，LLMs可能会从训练数据中学习不完美的正字和音标形式之间的关联。因此，对LLMs的音韵技能进行基准测试至关重要。为此，我们提出了PhonologyBench，这是一个新颖的基准测试，包括三个诊断任务，旨在明确测试LLMs在英语中的音韵技能：形音转换、音节计数和押韵词生成。尽管没有访问语音数据，LLMs在PhonologyBench任务上表现出显著的性能。然而，我们观察到在押韵词生成和音节计数方面存在显著的17%和45%的差距， respectively, when...

    arXiv:2404.02456v1 Announce Type: cross  Abstract: Phonology, the study of speech's structure and pronunciation rules, is a critical yet often overlooked component in Large Language Model (LLM) research. LLMs are widely used in various downstream applications that leverage phonology such as educational tools and poetry generation. Moreover, LLMs can potentially learn imperfect associations between orthographic and phonological forms from the training data. Thus, it is imperative to benchmark the phonological skills of LLMs. To this end, we present PhonologyBench, a novel benchmark consisting of three diagnostic tasks designed to explicitly test the phonological skills of LLMs in English: grapheme-to-phoneme conversion, syllable counting, and rhyme word generation. Despite having no access to speech data, LLMs showcased notable performance on the PhonologyBench tasks. However, we observe a significant gap of 17% and 45% on Rhyme Word Generation and Syllable counting, respectively, when 
    
[^58]: 通过隐式组合进行算法诱导的任务不可知架构

    Task Agnostic Architecture for Algorithm Induction via Implicit Composition

    [https://arxiv.org/abs/2404.02450](https://arxiv.org/abs/2404.02450)

    探索开发一种统一架构，旨在解决各种任务，包括以前未见过的任务，并使用跨多种模式的输入。

    

    应用机器学习中的不同领域（如计算机视觉、语音或自然语言处理）一直在构建面向特定领域的解决方案。目前，我们正在目睹一个相反的趋势，即向开发更广义的架构发展，这是由大型语言模型和多模式基础模型推动的。这些架构旨在解决各种任务，包括以前未见过的任务，并使用跨多种模式的输入。将这种泛化趋势推向极端意味着可能存在一种能够解决所有任务的单一深度网络架构。本文旨在探索开发这样一个统一架构，并提出了一个理论框架，说明如何构建这样的架构。我们的提议基于以下假设。首先，任务是通过按照一系列指令来解决的，通常在常规计算硬件的代码中实现。

    arXiv:2404.02450v1 Announce Type: cross  Abstract: Different fields in applied machine learning such as computer vision, speech or natural language processing have been building domain-specialised solutions. Currently, we are witnessing an opposing trend towards developing more generalist architectures, driven by Large Language Models and multi-modal foundational models. These architectures are designed to tackle a variety of tasks, including those previously unseen and using inputs across multiple modalities. Taking this trend of generalization to the extreme suggests the possibility of a single deep network architecture capable of solving all tasks. This position paper aims to explore developing such a unified architecture and proposes a theoretical framework of how it could be constructed. Our proposal is based on the following assumptions. Firstly, tasks are solved by following a sequence of instructions, typically implemented in code for conventional computing hardware, which inhe
    
[^59]: 电动车辆路径问题用于应急供电：面向电信基站救助

    Electric Vehicle Routing Problem for Emergency Power Supply: Towards Telecom Base Station Relief

    [https://arxiv.org/abs/2404.02448](https://arxiv.org/abs/2404.02448)

    提出一种新型的基于电动车辆的路径问题，通过组合基于规则的车辆选择器和基于强化学习的节点选择器解决电动车辆路径问题，以最小化总行驶距离和故障基站数量。

    

    作为一家电信提供商，我们公司有一个关键使命，即在停电期间保持电信服务。为了实现这一使命，至关重要的是维持电信基站的电力。本文考虑一种解决方案，即电动车辆 (EVs) 直接前往其位置为基站提供电力。我们的目标是找到最小化所有电动车辆的总行驶距离和故障基站数量的EV路线。在本文中，我们将这一路径问题形式化为新型的电动车辆路径问题 (EVRP) 变体，并提出了将基于规则的车辆选择器和基于强化学习（RL）的节点选择器相结合的求解器。车辆选择器的规则确保了所选EV开始移动时的确切环境状态。此外，RL模型的节点选择实现了快速路径生成，在紧急情况下尤为重要。我们在机器人上对我们的求解器进行评估

    arXiv:2404.02448v1 Announce Type: cross  Abstract: As a telecom provider, our company has a critical mission to maintain telecom services even during power outages. To accomplish the mission, it is essential to maintain the power of the telecom base stations. Here we consider a solution where electric vehicles (EVs) directly supply power to base stations by traveling to their locations. The goal is to find EV routes that minimize both the total travel distance of all EVs and the number of downed base stations. In this paper, we formulate this routing problem as a new variant of the Electric Vehicle Routing Problem (EVRP) and propose a solver that combines a rule-based vehicle selector and a reinforcement learning (RL)-based node selector. The rule of the vehicle selector ensures the exact environmental states when the selected EV starts to move. In addition, the node selection by the RL model enables fast route generation, which is critical in emergencies. We evaluate our solver on bot
    
[^60]: 通过带有白盒变换器的结构扩散进行蒙面补全

    Masked Completion via Structured Diffusion with White-Box Transformers

    [https://arxiv.org/abs/2404.02446](https://arxiv.org/abs/2404.02446)

    该论文提出了一种可以应用于大规模无监督表示学习的白盒设计范例。

    

    现代学习框架通常使用大量无标签数据训练深度神经网络，通过解决简单的前置任务学习表示，然后将这些表示用作下游任务的基础。本文提供了第一个白盒设计范例的实例，可以应用于大规模无监督表示学习。

    arXiv:2404.02446v1 Announce Type: new  Abstract: Modern learning frameworks often train deep neural networks with massive amounts of unlabeled data to learn representations by solving simple pretext tasks, then use the representations as foundations for downstream tasks. These networks are empirically designed; as such, they are usually not interpretable, their representations are not structured, and their designs are potentially redundant. White-box deep networks, in which each layer explicitly identifies and transforms structures in the data, present a promising alternative. However, existing white-box architectures have only been shown to work at scale in supervised settings with labeled data, such as classification. In this work, we provide the first instantiation of the white-box design paradigm that can be applied to large-scale unsupervised representation learning. We do this by exploiting a fundamental connection between diffusion, compression, and (masked) completion, deriving
    
[^61]: 从叙述到数字：利用口述验尸叙述的语言模型预测进行有效推断

    From Narratives to Numbers: Valid Inference Using Language Model Predictions from Verbal Autopsy Narratives

    [https://arxiv.org/abs/2404.02438](https://arxiv.org/abs/2404.02438)

    该论文提出了一种利用最先进的NLP技术从口述验尸文本中预测死因并进行有效推断的方法。

    

    在大部分死亡事件发生在医疗系统外的场景中，口述验尸（VAs）是监测死因趋势的常用工具。VAs是与幸存的照料者或亲属进行的访谈，用于预测逝者的死因。将VAs转化为研究人员和决策者可行的见解需要两个步骤：（i）使用VA访谈预测可能的死因，（ii）使用预测的死因进行推断（例如，使用死亡样本对死因按人口统计因素分解的建模）。在本文中，我们开发了一种利用最先进的NLP技术从自由文本预测结果（在我们的案例中为死因）进行有效推断的方法。我们将这种方法称为multiPPI++，将最近的“预测驱动推断”工作扩展到多项分类。我们利用一系列NLP技术进行COD预测，并通过对VA数据的实证分析，展示了我们方法的有效性。

    arXiv:2404.02438v1 Announce Type: new  Abstract: In settings where most deaths occur outside the healthcare system, verbal autopsies (VAs) are a common tool to monitor trends in causes of death (COD). VAs are interviews with a surviving caregiver or relative that are used to predict the decedent's COD. Turning VAs into actionable insights for researchers and policymakers requires two steps (i) predicting likely COD using the VA interview and (ii) performing inference with predicted CODs (e.g. modeling the breakdown of causes by demographic factors using a sample of deaths). In this paper, we develop a method for valid inference using outcomes (in our case COD) predicted from free-form text using state-of-the-art NLP techniques. This method, which we call multiPPI++, extends recent work in "prediction-powered inference" to multinomial classification. We leverage a suite of NLP techniques for COD prediction and, through empirical analysis of VA data, demonstrate the effectiveness of our 
    
[^62]: AD4RL：具有基于价值数据集的离线强化学习的自动驾驶基准测试

    AD4RL: Autonomous Driving Benchmarks for Offline Reinforcement Learning with Value-based Dataset

    [https://arxiv.org/abs/2404.02429](https://arxiv.org/abs/2404.02429)

    本文提供了用于离线强化学习研究的自动驾驶数据集和基准测试，包括真实世界人类驾驶员的数据集，以及七种离线强化学习算法在三种实际驾驶场景中的应用，并提供了一个统一决策模型作为算法设计的参考框架。

    

    离线强化学习通过利用预先收集的大型数据集，已成为一项具有潜力的技术。尽管离线强化学习的实际好处已被增强，但大多数离线强化学习的算法开发研究仍依赖于具有合成数据集的游戏任务。为了解决这些限制，本文提供了用于离线强化学习研究的自动驾驶数据集和基准测试。我们提供了19个数据集，包括真实世界人类驾驶员的数据集，以及三种实际驾驶场景中的七种流行的离线强化学习算法。我们还提供了一个统一的决策过程模型，可以在不同场景中有效运行，作为算法设计的参考框架。我们的研究为社区进一步探索现有强化学习方法的实际方面奠定了基础。

    arXiv:2404.02429v1 Announce Type: cross  Abstract: Offline reinforcement learning has emerged as a promising technology by enhancing its practicality through the use of pre-collected large datasets. Despite its practical benefits, most algorithm development research in offline reinforcement learning still relies on game tasks with synthetic datasets. To address such limitations, this paper provides autonomous driving datasets and benchmarks for offline reinforcement learning research. We provide 19 datasets, including real-world human driver's datasets, and seven popular offline reinforcement learning algorithms in three realistic driving scenarios. We also provide a unified decision-making process model that can operate effectively across different scenarios, serving as a reference framework in algorithm design. Our research lays the groundwork for further collaborations in the community to explore practical aspects of existing reinforcement learning methods. Dataset and codes can be 
    
[^63]: 通过稀疏跨模态适应修复稀疏视觉-语言模型

    RESSA: Repair Sparse Vision-Language Models via Sparse Cross-Modality Adaptation

    [https://arxiv.org/abs/2404.02424](https://arxiv.org/abs/2404.02424)

    通过稀疏跨模态适应修复稀疏视觉-语言模型，探索了VLM修剪中的两个主要问题，提出稀疏比率对性能的影响，展示了修复稀疏VLMs性能所需的专门技术。

    

    视觉-语言模型(VLMs)整合了来自多个模态的不同信息，在各种任务中表现出显著成功。但是，在资源受限的场景中部署包括大规模视觉和语言模型在内的VLMs会带来挑战。尽管修剪后微调提供了一种保持更小模型大小性能的潜在解决方案，但其在VLMs中的应用相对未被探索，这提出了两个主要问题：如何在不同模态特定模型之间分配稀疏性，以及如何修复被修剪稀疏的VLMs的性能。为了回答第一个问题，我们进行了关于VLM修剪的初步研究，发现使用相同稀疏比率修剪视觉模型和语言模型有助于实现接近最佳性能。对于第二个问题，与微调单模稀疏模型不同，稀疏VLMs涉及跨模态交互，需要专门的技术。

    arXiv:2404.02424v1 Announce Type: new  Abstract: Vision-Language Models (VLMs), integrating diverse information from multiple modalities, have shown remarkable success across various tasks. However, deploying VLMs, comprising large-scale vision and language models poses challenges in resource-constrained scenarios. While pruning followed by finetuning offers a potential solution to maintain performance with smaller model sizes, its application to VLMs remains relatively unexplored, presenting two main questions: how to distribute sparsity across different modality-specific models, and how to repair the performance of pruned sparse VLMs. To answer the first question, we conducted preliminary studies on VLM pruning and found that pruning vision models and language models with the same sparsity ratios contribute to nearly optimal performance. For the second question, unlike finetuning unimodal sparse models, sparse VLMs involve cross-modality interactions, requiring specialized techniques
    
[^64]: 使用PEFT和合成数据增强低资源LLMs分类

    Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data

    [https://arxiv.org/abs/2404.02422](https://arxiv.org/abs/2404.02422)

    提出了一种策略，通过PEFT和合成数据增强低资源LLMs分类器，实现了与0-shot文本分类器相媲美或更好的准确性。

    

    大语言模型在0-shot或few-shot设置下，在文本分类任务中取得竞争性成果。在上下文学习(ICL)中，通常比0-shot设置获得更好的准确性，但这是以效率为代价的，因为需要更长的输入提示。本文提出了一种策略，可以使LLMs像0-shot文本分类器一样高效，同时获得与ICL相当或更好的准确性。我们的解决方案针对资源稀缺的情况，即每类只有4个示例可用。使用单个LLM和少量真实数据，我们执行一系列生成、过滤和参数高效微调步骤，从而创建一个强大而高效的分类器。实验结果表明，我们的方法在多个文本分类数据集上取得了竞争性结果。

    arXiv:2404.02422v1 Announce Type: new  Abstract: Large Language Models (LLMs) operating in 0-shot or few-shot settings achieve competitive results in Text Classification tasks. In-Context Learning (ICL) typically achieves better accuracy than the 0-shot setting, but it pays in terms of efficiency, due to the longer input prompt. In this paper, we propose a strategy to make LLMs as efficient as 0-shot text classifiers, while getting comparable or better accuracy than ICL. Our solution targets the low resource setting, i.e., when only 4 examples per class are available. Using a single LLM and few-shot real data we perform a sequence of generation, filtering and Parameter-Efficient Fine-Tuning steps to create a robust and efficient classifier. Experimental results show that our approach leads to competitive results on multiple text classification datasets.
    
[^65]: 决策Transformer作为部分可观测连续控制的基础模型

    Decision Transformer as a Foundation Model for Partially Observable Continuous Control

    [https://arxiv.org/abs/2404.02407](https://arxiv.org/abs/2404.02407)

    通过将控制任务作为基于过去观察、动作和奖励的当前最优动作预测来消除估计器设计需求，并利用生成式预训练Transformer系列初始化决策Transformer，然后使用低秩适应对其进行控制任务训练。

    

    面对具有部分状态可观测性的非线性动态系统的闭环控制需要专家对一组不太标准化的理论工具有深入了解，此外，它还需要控制器和估计器设计的精心融合以实现期望的系统行为。为建立一个通用的控制器合成框架，我们探索了决策Transformer（DT）架构。具体地，我们首先将控制任务框架化为基于过去的观察、动作和奖励预测当前最优动作，消除了对单独估计器设计的需求。然后，我们利用预训练的语言模型，即生成式预训练Transformer（GPT）系列，来初始化DT，并随后使用低秩适应（LoRA）对其进行控制任务训练。我们在五个不同的控制任务上进行了全面实验，涵盖航空航天系统的操纵到控制偏微分方程。

    arXiv:2404.02407v1 Announce Type: cross  Abstract: Closed-loop control of nonlinear dynamical systems with partial-state observability demands expert knowledge of a diverse, less standardized set of theoretical tools. Moreover, it requires a delicate integration of controller and estimator designs to achieve the desired system behavior. To establish a general controller synthesis framework, we explore the Decision Transformer (DT) architecture. Specifically, we first frame the control task as predicting the current optimal action based on past observations, actions, and rewards, eliminating the need for a separate estimator design. Then, we leverage the pre-trained language models, i.e., the Generative Pre-trained Transformer (GPT) series, to initialize DT and subsequently train it for control tasks using low-rank adaptation (LoRA). Our comprehensive experiments across five distinct control tasks, ranging from maneuvering aerospace systems to controlling partial differential equations 
    
[^66]: 评估波斯语大型语言模型：以 ChatGPT 为中心的初步研究

    Benchmarking Large Language Models for Persian: A Preliminary Study Focusing on ChatGPT

    [https://arxiv.org/abs/2404.02403](https://arxiv.org/abs/2404.02403)

    评估了波斯语大型语言模型在不同任务上的表现，引入了推理任务方面的新基准测试，发现LLMs在推理任务中表现优异。

    

    本文探讨了大型语言模型（LLMs）在波斯语中的有效性。虽然ChatGPT和随后的LLMs在英语中表现出色，但它们在更低资源语言中的效率仍然是一个开放的问题。我们提出了对一系列波斯语任务进行全面基准测试研究的首次尝试。我们的主要关注点是在GPT-3.5-turbo上进行评估，但我们也包括了GPT-4和OpenChat-3.5以提供更全面的评估。我们的评估涵盖了一系列任务，包括经典、推理和基于知识的领域。为了进行深入比较，我们评估了LLMs与现有任务特定的微调模型的性能。考虑到推理任务波斯语数据集的有限性，我们引入了两个新的基准测试：一个基于小学数学问题，另一个源自第7和第10年级入学考试。我们的研究结果显示，LLMs，尤其是GPT-4，在推理任务表现出色。

    arXiv:2404.02403v1 Announce Type: new  Abstract: This paper explores the efficacy of large language models (LLMs) for Persian. While ChatGPT and consequent LLMs have shown remarkable performance in English, their efficiency for more low-resource languages remains an open question. We present the first comprehensive benchmarking study of LLMs across diverse Persian language tasks. Our primary focus is on GPT-3.5-turbo, but we also include GPT-4 and OpenChat-3.5 to provide a more holistic evaluation. Our assessment encompasses a diverse set of tasks categorized into classic, reasoning, and knowledge-based domains. To enable a thorough comparison, we evaluate LLMs against existing task-specific fine-tuned models. Given the limited availability of Persian datasets for reasoning tasks, we introduce two new benchmarks: one based on elementary school math questions and another derived from the entrance exams for 7th and 10th grades. Our findings reveal that while LLMs, especially GPT-4, excel
    
[^67]: 使用ChatLLM在对话AI中导航语境深度的Token Trails

    Token Trails: Navigating Contextual Depths in Conversational AI with ChatLLM

    [https://arxiv.org/abs/2404.02402](https://arxiv.org/abs/2404.02402)

    Token Trails是一种利用token-type嵌入导航对话中复杂上下文细微差别的新方法，通过提高对话理解和回复生成效果，在促进上下文意识聊天机器人交互方面具有前沿性能。

    

    使用大型语言模型(LLMs)进行对话建模需要对上下文进行细致理解，以生成连贯且与上下文相关的回复。本文提出了Token Trails，这是一种利用token-type嵌入来导航对话中复杂上下文细微差别的新方法。我们的框架利用token-type嵌入来区分用户话语和机器人回复，从而促进生成具有上下文意识的回复。通过全面的实验和评估，我们展示了Token Trails在提高对话理解和回复生成方面的有效性，实现了最先进的性能。我们的结果突显了对话AI中上下文建模的重要性，并强调了Token Trails在推动该领域发展方面的潜力，为更复杂和具有上下文意识的聊天机器人交互铺平了道路。

    arXiv:2404.02402v1 Announce Type: cross  Abstract: Conversational modeling using Large Language Models (LLMs) requires a nuanced understanding of context to generate coherent and contextually relevant responses. In this paper, we present Token Trails, a novel approach that leverages token-type embeddings to navigate the intricate contextual nuances within conversations. Our framework utilizes token-type embeddings to distinguish between user utterances and bot responses, facilitating the generation of context-aware replies. Through comprehensive experimentation and evaluation, we demonstrate the effectiveness of Token Trails in improving conversational understanding and response generation, achieving state-of-the-art performance. Our results highlight the significance of contextual modeling in conversational AI and underscore the promising potential of Token Trails to advance the field, paving the way for more sophisticated and contextually aware chatbot interactions.
    
[^68]: 利用平滑约束增强基于扩散的点云生成

    Enhancing Diffusion-based Point Cloud Generation with Smoothness Constraint

    [https://arxiv.org/abs/2404.02396](https://arxiv.org/abs/2404.02396)

    提出利用局部平滑约束在扩散框架中增强点云生成，实验结果表明该模型可以生成逼真且更加平滑的点云，优于多种最先进的方法。

    

    扩散模型在点云生成任务中很受欢迎。现有作品利用前向扩散过程将原始点分布转换为噪声分布，然后学习逆扩散过程从噪声分布中恢复点分布。然而，由于忽视点云的几何特性，逆扩散过程可能会在表面上产生具有非平滑点的样本。我们提出通过将局部平滑约束纳入扩散框架中来缓解这个问题，用于点云生成。实验表明，所提出的模型可以生成逼真的形状和更加平滑的点云，优于多种最先进的方法。

    arXiv:2404.02396v1 Announce Type: cross  Abstract: Diffusion models have been popular for point cloud generation tasks. Existing works utilize the forward diffusion process to convert the original point distribution into a noise distribution and then learn the reverse diffusion process to recover the point distribution from the noise distribution. However, the reverse diffusion process can produce samples with non-smooth points on the surface because of the ignorance of the point cloud geometric properties. We propose alleviating the problem by incorporating the local smoothness constraint into the diffusion framework for point cloud generation. Experiments demonstrate the proposed model can generate realistic shapes and smoother point clouds, outperforming multiple state-of-the-art methods.
    
[^69]: 无线联邦学习的最佳批量分配

    Optimal Batch Allocation for Wireless Federated Learning

    [https://arxiv.org/abs/2404.02395](https://arxiv.org/abs/2404.02395)

    分析了在无线联邦学习中实现特定优越性差距所需的迭代次数，并提出了适用于基于TDMA的系统的最佳批量分配方法。此外，还展示了提出的分步批量分配方法可以显著减少基于RA的学习系统的完成时间

    

    联邦学习旨在构建一个适合分布在本地设备上的数据集的全局模型，而无需直接访问私有数据，利用服务器与本地设备之间的通信。在实际通信方案的背景下，我们研究了实现目标性能所需的完成时间。具体来说，我们分析了需要多少次迭代才能使联邦学习达到从最小全局损失到特定最优性差距的水平。随后，我们特征化了两种基本的多用户接入方案下每次迭代所需的时间：时分多址接入（TDMA）和随机接入（RA）。我们提出了一种分步批量分配方法，证明了对于基于TDMA的联邦学习系统而言是最佳的。另外，我们展示了由提出的分步批量分配方法提供的设备之间的非零批次差距显著降低了基于RA的学习系统的完成时间

    arXiv:2404.02395v1 Announce Type: new  Abstract: Federated learning aims to construct a global model that fits the dataset distributed across local devices without direct access to private data, leveraging communication between a server and the local devices. In the context of a practical communication scheme, we study the completion time required to achieve a target performance. Specifically, we analyze the number of iterations required for federated learning to reach a specific optimality gap from a minimum global loss. Subsequently, we characterize the time required for each iteration under two fundamental multiple access schemes: time-division multiple access (TDMA) and random access (RA). We propose a step-wise batch allocation, demonstrated to be optimal for TDMA-based federated learning systems. Additionally, we show that the non-zero batch gap between devices provided by the proposed step-wise batch allocation significantly reduces the completion time for RA-based learning syst
    
[^70]: 通过物理引导的机器学习解决光谱数据的反演问题

    An inversion problem for optical spectrum data via physics-guided machine learning

    [https://arxiv.org/abs/2404.02387](https://arxiv.org/abs/2404.02387)

    提出了正则化递归推理机(rRIM)，一种通过物理引导的机器学习方法，用于从实验光谱中推导配对粘合函数，具有噪声鲁棒性和灵活性，可解决第一类Fredholm积分方程的类似反问题。

    

    我们提出了一种新颖的机器学习方法——正则化递归推理机(rRIM)，用于解决从测量到的光谱中推导配对粘合函数的挑战性问题。rRIM将物理原理融入训练和推理中，具有噪声鲁棒性、可以处理超出分布数据和减少数据需求的灵活性。它有效地从实验光谱中获取可靠的配对粘合函数，并为第一类Fredholm积分方程类似的反问题提供了有希望的解决方案。

    arXiv:2404.02387v1 Announce Type: cross  Abstract: We propose the regularized recurrent inference machine (rRIM), a novel machine-learning approach to solve the challenging problem of deriving the pairing glue function from measured optical spectra. The rRIM incorporates physical principles into both training and inference and affords noise robustness, flexibility with out-of-distribution data, and reduced data requirements. It effectively obtains reliable pairing glue functions from experimental optical spectra and yields promising solutions for similar inverse problems of the Fredholm integral equation of the first kind.
    
[^71]: 针对插值条件下随机加速梯度下降的更快收敛速度

    Faster Convergence of Stochastic Accelerated Gradient Descent under Interpolation

    [https://arxiv.org/abs/2404.02378](https://arxiv.org/abs/2404.02378)

    该论文证明了在插值条件下对随机加速的一般化版本的新收敛速度，在强增长条件下的加速SGD中取得了显著改进。

    

    我们证明了在插值条件下对随机Nesterov加速的一般化版本的新收敛速度。与先前的分析不同，我们的方法加速了任何在期望中取得足够进展的随机梯度方法。证明使用估计序列框架进行，适用于凸函数和强凸函数，并可轻松专门用于强增长条件下的加速SGD。在这种特殊情况下，与先前的工作相比，我们的分析将强增长常数的依赖性从$\rho$减少到$\sqrt{\rho}$。这种改进在最坏情况下相当于条件数的平方根，并解决了关于随机加速的保证可能比SGD更差的批评。

    arXiv:2404.02378v1 Announce Type: cross  Abstract: We prove new convergence rates for a generalized version of stochastic Nesterov acceleration under interpolation conditions. Unlike previous analyses, our approach accelerates any stochastic gradient method which makes sufficient progress in expectation. The proof, which proceeds using the estimating sequences framework, applies to both convex and strongly convex functions and is easily specialized to accelerated SGD under the strong growth condition. In this special case, our analysis reduces the dependence on the strong growth constant from $\rho$ to $\sqrt{\rho}$ as compared to prior work. This improvement is comparable to a square-root of the condition number in the worst case and address criticism that guarantees for stochastic acceleration could be worse than those for SGD.
    
[^72]: 模糊恶意软件检测：通过内存分析调查真实场景

    Obfuscated Malware Detection: Investigating Real-world Scenarios through Memory Analysis

    [https://arxiv.org/abs/2404.02372](https://arxiv.org/abs/2404.02372)

    通过内存分析，利用机器学习算法提出了一种简单且成本效益的模糊恶意软件检测系统，重点评估其在真实场景中的效果。

    

    在互联网和智能设备时代，恶意软件的检测对系统安全至关重要。恶意软件作者越来越多地使用模糊技术来规避先进的安全解决方案，使得检测和消除威胁变得具有挑战性。本研究提出了一种简单且具有成本效益的模糊恶意软件检测系统，通过内存转储分析，利用多样化的机器学习算法。该研究聚焦于CIC-MalMem-2022数据集，旨在模拟真实场景并评估基于内存的模糊恶意软件检测。我们评估了机器学习算法的有效性，如...

    arXiv:2404.02372v1 Announce Type: cross  Abstract: In the era of the internet and smart devices, the detection of malware has become crucial for system security. Malware authors increasingly employ obfuscation techniques to evade advanced security solutions, making it challenging to detect and eliminate threats. Obfuscated malware, adept at hiding itself, poses a significant risk to various platforms, including computers, mobile devices, and IoT devices. Conventional methods like heuristic-based or signature-based systems struggle against this type of malware, as it leaves no discernible traces on the system. In this research, we propose a simple and cost-effective obfuscated malware detection system through memory dump analysis, utilizing diverse machine-learning algorithms. The study focuses on the CIC-MalMem-2022 dataset, designed to simulate real-world scenarios and assess memory-based obfuscated malware detection. We evaluate the effectiveness of machine learning algorithms, such 
    
[^73]: 学习与分布偏移的半空间交集：改进算法和SQ下界

    Learning Intersections of Halfspaces with Distribution Shift: Improved Algorithms and SQ Lower Bounds

    [https://arxiv.org/abs/2404.02364](https://arxiv.org/abs/2404.02364)

    研究学习与分布偏移的交集问题，在基于高斯训练分布的情况下，证明了一系列新的上界，包括一种TDS学习$k$个齐次半空间交集达到精度$\epsilon$的$2^{(k/\epsilon)^{O(1)}} \mathsf{poly}(d)$时间算法（在先前的工作中）。

    

    Klivans、Stavropoulos和Vasilyan最近的工作引发了对具有分布偏移的可测试学习（TDS学习）的研究，其中学习者从训练分布$\mathcal{D}$获得标记样本，从测试分布$\mathcal{D}'$获得未标记样本，目标是在训练样本通过相应的测试时输出在$\mathcal{D}'$上具有低误差的分类器。他们的模型不同于先前的所有工作，因为$\mathcal{D}'$上没有假设。相反，当训练和测试分布的边际相等时，测试必须接受（以高概率）。

    arXiv:2404.02364v1 Announce Type: cross  Abstract: Recent work of Klivans, Stavropoulos, and Vasilyan initiated the study of testable learning with distribution shift (TDS learning), where a learner is given labeled samples from training distribution $\mathcal{D}$, unlabeled samples from test distribution $\mathcal{D}'$, and the goal is to output a classifier with low error on $\mathcal{D}'$ whenever the training samples pass a corresponding test. Their model deviates from all prior work in that no assumptions are made on $\mathcal{D}'$. Instead, the test must accept (with high probability) when the marginals of the training and test distributions are equal.   Here we focus on the fundamental case of intersections of halfspaces with respect to Gaussian training distributions and prove a variety of new upper bounds including a $2^{(k/\epsilon)^{O(1)}} \mathsf{poly}(d)$-time algorithm for TDS learning intersections of $k$ homogeneous halfspaces to accuracy $\epsilon$ (prior work achieved
    
[^74]: FraGNNet：一种用于质谱预测的深度概率模型

    FraGNNet: A Deep Probabilistic Model for Mass Spectrum Prediction

    [https://arxiv.org/abs/2404.02360](https://arxiv.org/abs/2404.02360)

    FraGNNet是一种用于化合物到质谱预测的深度概率模型，能够高效准确地预测高分辨率谱，在性能上超越了现有的模型

    

    识别复杂混合物中化合物的方法中，质谱的识别是一个关键步骤。传统的质谱到化合物（MS2C）问题的解决方案涉及将未知的质谱与已知的质谱-分子库进行匹配，但这种方法受限于库覆盖不完整。化合物到质谱（C2MS）模型可以通过将真实库与预测谱进行增强来提高检索率。不幸的是，许多现有的C2MS模型在预测分辨率、可扩展性或可解释性方面存在问题。我们开发了一种新的C2MS预测概率方法——FraGNNet，可以高效准确地预测高分辨率谱。FraGNNet使用结构化的潜在空间来揭示定义谱的基本过程。我们的模型在预测误差方面实现了最先进的性能，并超越了现有的C2MS模型。

    arXiv:2404.02360v1 Announce Type: new  Abstract: The process of identifying a compound from its mass spectrum is a critical step in the analysis of complex mixtures. Typical solutions for the mass spectrum to compound (MS2C) problem involve matching the unknown spectrum against a library of known spectrum-molecule pairs, an approach that is limited by incomplete library coverage. Compound to mass spectrum (C2MS) models can improve retrieval rates by augmenting real libraries with predicted spectra. Unfortunately, many existing C2MS models suffer from problems with prediction resolution, scalability, or interpretability. We develop a new probabilistic method for C2MS prediction, FraGNNet, that can efficiently and accurately predict high-resolution spectra. FraGNNet uses a structured latent space to provide insight into the underlying processes that define the spectrum. Our model achieves state-of-the-art performance in terms of prediction error, and surpasses existing C2MS models as a t
    
[^75]: 多模态范式的归因正则化

    Attribution Regularization for Multimodal Paradigms

    [https://arxiv.org/abs/2404.02359](https://arxiv.org/abs/2404.02359)

    提出一种新的正则化项，鼓励多模态模型有效利用所有模态信息，以解决多模态学习中单模态模型优于多模态模型的问题。

    

    多模态机器学习近年来受到广泛关注，因为它能整合多个模态的信息以增强学习和决策过程。然而，通常观察到单模态模型优于多模态模型，尽管后者可以访问更丰富的信息。此外，单个模态的影响常常主导决策过程，导致性能不佳。这个研究项目旨在通过提出一种新颖的正则化项来解决这些挑战，该项鼓励多模态模型在做出决策时有效利用所有模态的信息。该项目的重点在于视频-音频领域，尽管所提出的正则化技术在涉及多个模态的体现AI研究中具有广泛应用前景。通过利用这种正则化项，提出的方法

    arXiv:2404.02359v1 Announce Type: new  Abstract: Multimodal machine learning has gained significant attention in recent years due to its potential for integrating information from multiple modalities to enhance learning and decision-making processes. However, it is commonly observed that unimodal models outperform multimodal models, despite the latter having access to richer information. Additionally, the influence of a single modality often dominates the decision-making process, resulting in suboptimal performance. This research project aims to address these challenges by proposing a novel regularization term that encourages multimodal models to effectively utilize information from all modalities when making decisions. The focus of this project lies in the video-audio domain, although the proposed regularization technique holds promise for broader applications in embodied AI research, where multiple modalities are involved. By leveraging this regularization term, the proposed approach
    
[^76]: 利用语言在图像中进行语义增强

    Semantic Augmentation in Images using Language

    [https://arxiv.org/abs/2404.02353](https://arxiv.org/abs/2404.02353)

    深度学习模型需要大规模标记数据集，本文提出利用生成图像增强数据集以改进模型跨领域泛化能力。

    

    深度学习模型需要非常庞大的标记数据集进行监督学习，缺乏这些数据集会导致过拟合并限制其泛化到现实世界示例的能力。最近扩散模型的进展使得能够基于文本输入生成逼真的图像。利用用于训练这些扩散模型的大规模数据集，我们提出一种利用生成的图像来增强现有数据集的技术。本文探讨了各种有效数据增强策略，以提高深度学习模型的跨领域泛化能力。

    arXiv:2404.02353v1 Announce Type: cross  Abstract: Deep Learning models are incredibly data-hungry and require very large labeled datasets for supervised learning. As a consequence, these models often suffer from overfitting, limiting their ability to generalize to real-world examples. Recent advancements in diffusion models have enabled the generation of photorealistic images based on textual inputs. Leveraging the substantial datasets used to train these diffusion models, we propose a technique to utilize generated images to augment existing datasets. This paper explores various strategies for effective data augmentation to improve the out-of-domain generalization capabilities of deep learning models.
    
[^77]: 利用期权隐含信息和深度学习改进多资产期权的无模型界限

    Improved model-free bounds for multi-asset options using option-implied information and deep learning

    [https://arxiv.org/abs/2404.02343](https://arxiv.org/abs/2404.02343)

    利用深度学习和期权隐含信息改进多资产期权的无模型上界计算方法

    

    我们考虑在将依赖性不确定性与有关依赖结构的额外信息相结合的情况下计算多资产期权的无模型界限。 更具体地，我们考虑了边际分布已知且市场上也有多资产期权价格的部分信息的情形。 我们在这种情况下给出了资产定价的基本定理，以及一种超对冲对偶，能够将在概率度量上的最大化问题转化为在交易策略上的更易处理的最小化问题。 后者是通过结合罚款方法和借助人工神经网络的深度学习逼近来解决的。 数值方法快速，并且计算时间与交易资产数量成线性比例。 最后，我们检验了各种额外信息的重要性。

    arXiv:2404.02343v1 Announce Type: cross  Abstract: We consider the computation of model-free bounds for multi-asset options in a setting that combines dependence uncertainty with additional information on the dependence structure. More specifically, we consider the setting where the marginal distributions are known and partial information, in the form of known prices for multi-asset options, is also available in the market. We provide a fundamental theorem of asset pricing in this setting, as well as a superhedging duality that allows to transform the maximization problem over probability measures in a more tractable minimization problem over trading strategies. The latter is solved using a penalization approach combined with a deep learning approximation using artificial neural networks. The numerical method is fast and the computational time scales linearly with respect to the number of traded assets. We finally examine the significance of various pieces of additional information. Em
    
[^78]: 闭环学习中生成模型的热力学死亡

    Heat Death of Generative Models in Closed-Loop Learning

    [https://arxiv.org/abs/2404.02325](https://arxiv.org/abs/2404.02325)

    生成模型的闭环训练过程容易产生退化现象，模型可能开始生成无意义的数据或仅从所需数据分布的一小部分中采样。

    

    生成式机器学习模型的改进和采纳正在迅速加速，例如文本中LLM（大型语言模型）的流行以及图像生成中的扩散模型。随着生成模型的普及，它们生成的数据被整合到公共网络中的共享内容中。这引发了一个问题：当模型生成的数据被送回到模型进行后续训练时会发生什么。这是一个关于训练过程稳定性的问题，即公共可访问内容的分布（我们称之为“知识”）是否保持稳定还是崩溃。文献中报道的小规模实证实验显示，这种闭环训练过程容易退化。模型可能开始生成无意义的数据，或者仅从所需数据分布的一小部分中采样（称为模式崩溃现象）。

    arXiv:2404.02325v1 Announce Type: new  Abstract: Improvement and adoption of generative machine learning models is rapidly accelerating, as exemplified by the popularity of LLMs (Large Language Models) for text, and diffusion models for image generation.As generative models become widespread, data they generate is incorporated into shared content through the public web. This opens the question of what happens when data generated by a model is fed back to the model in subsequent training campaigns. This is a question about the stability of the training process, whether the distribution of publicly accessible content, which we refer to as "knowledge", remains stable or collapses.   Small scale empirical experiments reported in the literature show that this closed-loop training process is prone to degenerating. Models may start producing gibberish data, or sample from only a small subset of the desired data distribution (a phenomenon referred to as mode collapse). So far there has been on
    
[^79]: Prompt作为程序：一种结构感知的高效编译时Prompt优化方法

    Prompts As Programs: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization

    [https://arxiv.org/abs/2404.02319](https://arxiv.org/abs/2404.02319)

    提出了SAMMO框架，用于在编译时优化元提示程序，提高了复杂提示在多种不同LLM上的性能。

    

    大型语言模型(LLMs)现在能处理更长更复杂的输入，这促进了更复杂提示的使用。然而，提示通常需要一些调整以提高部署性能。最近的工作提出了自动提示优化方法，但随着提示复杂度和LLM强度的增加，许多提示优化技术已不再足够，需要一种新的方法来优化元提示程序。为了解决这个问题，我们引入了SAMMO，一个用于元提示程序的{\em 编译时}优化的框架，它将提示表示为结构化对象，允许在优化过程中搜索一组丰富的转换。我们展示SAMMO推广了先前的方法，在指令调整、RAG管线调整和提示压缩方面提高了复杂提示在多种不同LLM上的性能。我们开放所有代码供大家使用。

    arXiv:2404.02319v1 Announce Type: cross  Abstract: Large language models (LLMs) can now handle longer and more complex inputs, which facilitate the use of more elaborate prompts. However, prompts often require some tuning to improve performance for deployment. Recent work has proposed automatic prompt optimization methods, but as prompt complexity and LLM strength increase, many prompt optimization techniques are no longer sufficient and a new approach is needed to optimize {\em meta prompt programs}. To address this, we introduce SAMMO, a framework for {\em compile-time} optimizations of metaprompt programs, which represent prompts as structured objects that allows for a rich set of transformations that can be searched over during optimization. We show that SAMMO generalizes previous methods and improves the performance of complex prompts on (1) instruction tuning, (2) RAG pipeline tuning, and (3) prompt compression, across several different LLMs.   We make all code available open-sou
    
[^80]: 分子少样本学习是否真的需要元训练？

    Is Meta-training Really Necessary for Molecular Few-Shot Learning ?

    [https://arxiv.org/abs/2404.02314](https://arxiv.org/abs/2404.02314)

    本文重新审视了分子数据微调方法，提出了基于马氏距离的正则化二次探针损失，并设计了块坐标下降优化器，使得在黑匣子设置下，简单微调方法在少样本学习中获得了竞争性表现，同时消除了特定预训练策略的需要。

    

    最近，少样本学习在药物发现领域引起了极大关注，而最近快速增长的文献大多涉及复杂的元学习策略。本文重新审视了更为直接的分子数据微调方法，并提出了基于马氏距离的正则化二次探针损失。我们设计了一个专门的块坐标下降优化器，避免了我们损失函数的退化解。有趣的是，我们的简单微调方法在与最先进方法的比较中获得了极具竞争力的表现，同时适用于黑匣子设置，并消除了特定情节预训练策略的需要。此外，我们引入了一个新的基准来评估竞争方法对领域转移的稳健性。在这个设置下，我们的微调基线始终比元学习方法取得更好的结果。

    arXiv:2404.02314v1 Announce Type: cross  Abstract: Few-shot learning has recently attracted significant interest in drug discovery, with a recent, fast-growing literature mostly involving convoluted meta-learning strategies. We revisit the more straightforward fine-tuning approach for molecular data, and propose a regularized quadratic-probe loss based on the the Mahalanobis distance. We design a dedicated block-coordinate descent optimizer, which avoid the degenerate solutions of our loss. Interestingly, our simple fine-tuning approach achieves highly competitive performances in comparison to state-of-the-art methods, while being applicable to black-box settings and removing the need for specific episodic pre-training strategies. Furthermore, we introduce a new benchmark to assess the robustness of the competing methods to domain shifts. In this setting, our fine-tuning baseline obtains consistently better results than meta-learning methods.
    
[^81]: 使用异构时间图神经网络进行实时轴承负载预测的虚拟传感器

    Virtual Sensor for Real-Time Bearing Load Prediction Using Heterogeneous Temporal Graph Neural Networks

    [https://arxiv.org/abs/2404.02304](https://arxiv.org/abs/2404.02304)

    提出了一种使用Graph Neural Networks分析空间-时间依赖关系的图神经网络虚拟传感器，能够从传感器滚轮数据中学习，将操作条件映射为轴承负载。

    

    精确的轴承负载监测对于其预测与健康管理是至关重要的，这有助于损伤评估、磨损预测和主动维护。传统的机器学习算法无法充分利用这些空间 - 时间依赖关系。为了填补这一空白，我们引入了一种基于图的虚拟传感器，利用图神经网络来分析轴承负载之间的空间 - 时间依赖关系。

    arXiv:2404.02304v1 Announce Type: cross  Abstract: Accurate bearing load monitoring is essential for their Prognostics and Health Management (PHM), enabling damage assessment, wear prediction, and proactive maintenance. While bearing sensors are typically placed on the bearing housing, direct load monitoring requires sensors inside the bearing itself. Recently introduced sensor rollers enable direct bearing load monitoring but are constrained by their battery life. Data-driven virtual sensors can learn from sensor roller data collected during a batterys lifetime to map operating conditions to bearing loads. Although spatially distributed bearing sensors offer insights into load distribution (e.g., correlating temperature with load), traditional machine learning algorithms struggle to fully exploit these spatial-temporal dependencies. To address this gap, we introduce a graph-based virtual sensor that leverages Graph Neural Networks (GNNs) to analyze spatial-temporal dependencies among 
    
[^82]: CATGNN：图神经网络的成本有效和可扩展的分布式训练

    CATGNN: Cost-Efficient and Scalable Distributed Training for Graph Neural Networks

    [https://arxiv.org/abs/2404.02300](https://arxiv.org/abs/2404.02300)

    CATGNN是一种成本有效且可扩展的分布式GNN训练系统，通过接受边流作为输入并提出名为SPRING的流式分区算法，实现将GNN训练扩展到数十亿以上规模的图中。

    

    近年来，图神经网络取得了成功。尽管已经开发了不同的GNN架构和训练系统，但在大规模实际图上进行GNN训练仍然具有挑战性。现有的分布式系统需要将整个图加载到内存中以进行图分区，需要大量内存空间来处理大图，从而阻碍了使用普通工作站在这些大图上进行GNN训练。本文提出CATGNN，一个成本效益高且可扩展的分布式GNN训练系统，专注于在有限计算资源下将GNN训练扩展到数十亿甚至更大规模的图中。在其他功能中，它接受一系列边作为输入，而不是将整个图加载到内存中进行分区。我们还提出了一种名为SPRING的新型流式分区算法，用于分布式GNN训练。我们在16个开放数据集上验证了CATGNN与SPRING的正确性和有效性。

    arXiv:2404.02300v1 Announce Type: new  Abstract: Graph neural networks have been shown successful in recent years. While different GNN architectures and training systems have been developed, GNN training on large-scale real-world graphs still remains challenging. Existing distributed systems load the entire graph in memory for graph partitioning, requiring a huge memory space to process large graphs and thus hindering GNN training on such large graphs using commodity workstations. In this paper, we propose CATGNN, a cost-efficient and scalable distributed GNN training system which focuses on scaling GNN training to billion-scale or larger graphs under limited computational resources. Among other features, it takes a stream of edges as input, instead of loading the entire graph in memory, for partitioning. We also propose a novel streaming partitioning algorithm named SPRING for distributed GNN training. We verify the correctness and effectiveness of CATGNN with SPRING on 16 open datase
    
[^83]: 利用LLM和语音指令在首选地形上进行约束机器人导航：挖掘副词的力量

    Constrained Robotic Navigation on Preferred Terrains Using LLMs and Speech Instruction: Exploiting the Power of Adverbs

    [https://arxiv.org/abs/2404.02294](https://arxiv.org/abs/2404.02294)

    使用大型语言模型和语音指令进行约束机器人导航，减少地图依赖，利用生成式人工智能提取关键信息，通过文本生成遮罩识别地标和地形类型，实现对复杂地形的高效导航。

    

    本文探讨了利用大型语言模型进行无地图越野导航的方法，使用生成式人工智能来减少传统数据收集和注释的需求。我们提出了一种方法，其中机器人接收口头指令，通过Whisper转换为文本，一个大型语言模型(LLM)提取地标、首选地形和关键副词，并将其转化为用于约束导航的速度设置。基于语言驱动的语义分割模型生成基于文本的遮罩，用于识别图像中的地标和地形类型。通过使用相机参数将2D图像点转换到车辆的运动平面，MPC控制器可以引导车辆朝向期望的地形。这一方法增强了对多样环境的适应性，促进了使用高级指令来导航复杂和具有挑战性的地形。

    arXiv:2404.02294v1 Announce Type: cross  Abstract: This paper explores leveraging large language models for map-free off-road navigation using generative AI, reducing the need for traditional data collection and annotation. We propose a method where a robot receives verbal instructions, converted to text through Whisper, and a large language model (LLM) model extracts landmarks, preferred terrains, and crucial adverbs translated into speed settings for constrained navigation. A language-driven semantic segmentation model generates text-based masks for identifying landmarks and terrain types in images. By translating 2D image points to the vehicle's motion plane using camera parameters, an MPC controller can guides the vehicle towards the desired terrain. This approach enhances adaptation to diverse environments and facilitates the use of high-level instructions for navigating complex and challenging terrains.
    
[^84]: 行星探测的联邦多智能体建图

    Federated Multi-Agent Mapping for Planetary Exploration

    [https://arxiv.org/abs/2404.02289](https://arxiv.org/abs/2404.02289)

    联邦学习在多智能体机器人探测中的应用，利用隐式神经映射和地球数据集上的元初始化，实现了对不同领域如火星地形和冰川的强泛化能力。

    

    在多智能体机器人探测中，管理和有效利用动态环境产生的大量异构数据构成了一个重要挑战。联邦学习（FL）是一种有前途的分布式映射方法，它解决了协作学习中去中心化数据的挑战。FL使多个智能体之间可以进行联合模型训练，而无需集中化或共享原始数据，克服了带宽和存储限制。我们的方法利用隐式神经映射，将地图表示为由神经网络学习的连续函数，以便实现紧凑和适应性的表示。我们进一步通过在地球数据集上进行元初始化来增强这一方法，预训练网络以快速学习新的地图结构。这种组合在诸如火星地形和冰川等不同领域展现了较强的泛化能力。我们对这一方法进行了严格评估，展示了其有效性。

    arXiv:2404.02289v1 Announce Type: cross  Abstract: In multi-agent robotic exploration, managing and effectively utilizing the vast, heterogeneous data generated from dynamic environments poses a significant challenge. Federated learning (FL) is a promising approach for distributed mapping, addressing the challenges of decentralized data in collaborative learning. FL enables joint model training across multiple agents without requiring the centralization or sharing of raw data, overcoming bandwidth and storage constraints. Our approach leverages implicit neural mapping, representing maps as continuous functions learned by neural networks, for compact and adaptable representations. We further enhance this approach with meta-initialization on Earth datasets, pre-training the network to quickly learn new map structures. This combination demonstrates strong generalization to diverse domains like Martian terrain and glaciers. We rigorously evaluate this approach, demonstrating its effectiven
    
[^85]: 在LLMs中循环：利用大型语言模型注释进行低资源语言的主动学习

    LLMs in the Loop: Leveraging Large Language Model Annotations for Active Learning in Low-Resource Languages

    [https://arxiv.org/abs/2404.02261](https://arxiv.org/abs/2404.02261)

    在低资源语言中，通过将LLMs集成到主动学习循环中进行数据注释，有效减少所需数据量，并取得接近最先进性能的结果。

    

    由于语言资源和数据标注专业知识有限，低资源语言在人工智能开发中面临着重大障碍，使它们变得罕见且成本高昂。为了解决这一不足，我们提出利用LLMs的潜力在主动学习环节中进行数据注释。我们首先进行评估以评估注释者之间的一致性，从而选择适当的LLM注释者。然后，选择的注释者被集成到一个分类器的训练循环中，使用主动学习范式，最小化所需的查询数据量。实证评估，特别是使用GPT-4-Turbo，展示了几乎达到最先进性能的结果，同时大大减少了数据需求，由估算的潜在性能指示。

    arXiv:2404.02261v1 Announce Type: cross  Abstract: Low-resource languages face significant barriers in AI development due to limited linguistic resources and expertise for data labeling, rendering them rare and costly. The scarcity of data and the absence of preexisting tools exacerbate these challenges, especially since these languages may not be adequately represented in various NLP datasets. To address this gap, we propose leveraging the potential of LLMs in the active learning loop for data annotation. Initially, we conduct evaluations to assess inter-annotator agreement and consistency, facilitating the selection of a suitable LLM annotator. The chosen annotator is then integrated into a training loop for a classifier using an active learning paradigm, minimizing the amount of queried data required. Empirical evaluations, notably employing GPT-4-Turbo, demonstrate near-state-of-the-art performance with significantly reduced data requirements, as indicated by estimated potential co
    
[^86]: Mixture-of-Depths: 在基于Transformer的语言模型中动态分配计算

    Mixture-of-Depths: Dynamically allocating compute in transformer-based language models

    [https://arxiv.org/abs/2404.02258](https://arxiv.org/abs/2404.02258)

    本研究提出了一种新的方法，即Mixture-of-Depths，可以在Transformer的语言模型中动态分配FLOPs以优化模型深度上不同层的序列分配。

    

    基于Transformer的语言模型通常会将FLOPs均匀分布在输入序列中。本文展示了transformers可以学习动态地将FLOPs（或计算）分配给序列中的特定位置，优化各模型层深度上的序列分配。我们的方法通过设定在给定层中可参与自注意力和MLP计算的令牌数（$k$）来实施总计算预算。要处理的令牌由网络使用top-$k$路由机制确定。由于$k$是事先定义的，这种简单的过程使用具有已知张量大小的静态计算图，不同于其他条件计算技术。然而，由于$k$令牌的标识是不固定的，该方法可以非均匀地跨时间和模型深度维度分配FLOPs。因此，总体而言，计算支出完全可预测，但d

    arXiv:2404.02258v1 Announce Type: cross  Abstract: Transformer-based language models spread FLOPs uniformly across input sequences. In this work we demonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence, optimising the allocation along the sequence for different layers across the model depth. Our method enforces a total compute budget by capping the number of tokens ($k$) that can participate in the self-attention and MLP computations at a given layer. The tokens to be processed are determined by the network using a top-$k$ routing mechanism. Since $k$ is defined a priori, this simple procedure uses a static computation graph with known tensor sizes, unlike other conditional computation techniques. Nevertheless, since the identities of the $k$ tokens are fluid, this method can expend FLOPs non-uniformly across the time and model depth dimensions. Thus, compute expenditure is entirely predictable in sum total, but d
    
[^87]: 关于多模态与单模态机器学习之间更强的计算分离

    On Stronger Computational Separations Between Multimodal and Unimodal Machine Learning

    [https://arxiv.org/abs/2404.02254](https://arxiv.org/abs/2404.02254)

    提出了更强的平均情况计算分离，对于“典型”情况下的学习任务实例，单模态学习在计算上是困难的，但多模态学习却很容易。

    

    在多模态机器学习中，将多种数据模态（例如文本和图像）结合起来以促进更好的机器学习模型的学习，这仍然适用于相应的单模态任务（例如文本生成）。最近，多模态机器学习取得了巨大的经验成功（例如GPT-4）。受到为这种经验成功开发理论基础的动机，Lu（NeurIPS '23，ALT '24）提出了一种多模态学习理论，并考虑了多模态和单模态学习的理论模型之间可能的分离。特别是Lu（ALT '24）展示了一种计算分离，这对学习任务的最坏情况实例是相关的。

    arXiv:2404.02254v1 Announce Type: cross  Abstract: In multimodal machine learning, multiple modalities of data (e.g., text and images) are combined to facilitate the learning of a better machine learning model, which remains applicable to a corresponding unimodal task (e.g., text generation). Recently, multimodal machine learning has enjoyed huge empirical success (e.g. GPT-4). Motivated to develop theoretical justification for this empirical success, Lu (NeurIPS '23, ALT '24) introduces a theory of multimodal learning, and considers possible separations between theoretical models of multimodal and unimodal learning. In particular, Lu (ALT '24) shows a computational separation, which is relevant to worst-case instances of the learning task.   In this paper, we give a stronger average-case computational separation, where for "typical" instances of the learning task, unimodal learning is computationally hard, but multimodal learning is easy. We then question how "organic" the average-cas
    
[^88]: RAT: 检索增强变换器用于点击率预测

    RAT: Retrieval-Augmented Transformer for Click-Through Rate Prediction

    [https://arxiv.org/abs/2404.02249](https://arxiv.org/abs/2404.02249)

    RAT模型是为了解决当前CTR预测模型仅关注样本内特征交互而忽略跨样本关系的问题，通过检索相似样本构建增强输入，实现了对样本内和跨样本的全面特征交互推理，提高了CTR预测的效果。

    

    预测点击率（CTR）是Web应用程序的基本任务，其中一个关键问题是设计有效的特征交互模型。目前的方法主要集中于对单个样本内的特征交互进行建模，而忽略了可以作为参考背景来增强预测的潜在跨样本关系。为弥补这种不足，本文开发了一种检索增强变换器（RAT），旨在获取样本内和跨样本之间的细粒度特征交互。通过检索相似样本，我们为每个目标样本构建增强输入。然后利用级联注意力构建Transformer层，以捕获样本内和跨样本特征交互，促进全面推理以改善CTR预测的同时保持效率。对真实世界数据集的大量实验证实了RAT的有效性，并提出了

    arXiv:2404.02249v1 Announce Type: cross  Abstract: Predicting click-through rates (CTR) is a fundamental task for Web applications, where a key issue is to devise effective models for feature interactions. Current methodologies predominantly concentrate on modeling feature interactions within an individual sample, while overlooking the potential cross-sample relationships that can serve as a reference context to enhance the prediction. To make up for such deficiency, this paper develops a Retrieval-Augmented Transformer (RAT), aiming to acquire fine-grained feature interactions within and across samples. By retrieving similar samples, we construct augmented input for each target sample. We then build Transformer layers with cascaded attention to capture both intra- and cross-sample feature interactions, facilitating comprehensive reasoning for improved CTR prediction while retaining efficiency. Extensive experiments on real-world datasets substantiate the effectiveness of RAT and sugge
    
[^89]: 优化和抽样的近端预言

    Proximal Oracles for Optimization and Sampling

    [https://arxiv.org/abs/2404.02239](https://arxiv.org/abs/2404.02239)

    论文研究了具有非光滑函数和对数凹抽样的凸优化问题，提出了在优化和抽样中应用近端框架的方法，并建立了近端映射的迭代复杂度。

    

    我们考虑具有非光滑目标函数和对数凹抽样（带非光滑潜势，即负对数密度）的凸优化。特别地，我们研究了两种具体设置，其中凸目标/潜势函数要么是半光滑的，要么是复合形式，作为半光滑分量的有限和。为了克服由于非光滑性而带来的挑战，我们的算法在优化和抽样中采用了两种强大的近端框架：优化中的近端点框架和替代抽样框架（ASF），该框架在增广分布上使用Gibbs抽样。优化和抽样算法的一个关键组件是通过正则化切平面方法高效实现近端映射。我们在半光滑和复合设置中建立了近端映射的迭代复杂度。我们进一步提出了一种用于非光滑优化的自适应近端捆绑方法。

    arXiv:2404.02239v1 Announce Type: cross  Abstract: We consider convex optimization with non-smooth objective function and log-concave sampling with non-smooth potential (negative log density). In particular, we study two specific settings where the convex objective/potential function is either semi-smooth or in composite form as the finite sum of semi-smooth components. To overcome the challenges caused by non-smoothness, our algorithms employ two powerful proximal frameworks in optimization and sampling: the proximal point framework for optimization and the alternating sampling framework (ASF) that uses Gibbs sampling on an augmented distribution. A key component of both optimization and sampling algorithms is the efficient implementation of the proximal map by the regularized cutting-plane method. We establish the iteration-complexity of the proximal map in both semi-smooth and composite settings. We further propose an adaptive proximal bundle method for non-smooth optimization. The 
    
[^90]: 探索是您需要的全部内容吗？用于强化学习中转移的有效探索特征

    Is Exploration All You Need? Effective Exploration Characteristics for Transfer in Reinforcement Learning

    [https://arxiv.org/abs/2404.02235](https://arxiv.org/abs/2404.02235)

    探索特征与深度强化学习中有效的转移学习的关系，尚未被明确表征。研究试图理解探索特征与改进性能和效率在转移学习中的关系

    

    在深度强化学习（RL）研究中，人们正在努力设计更高效、更具生产力的探索方法，同时解决稀疏奖励问题。这些探索方法通常共享共同原则（例如改善多样性）和实现细节（例如内在奖励）。先前的研究发现，非静止马尔可夫决策过程（MDP）需要探索，以便有效地适应在线转移学习中环境的变化。然而，具体探索特征与在深度RL中的有效转移学习之间的关系尚未被表征。在这项工作中，我们试图理解显著探索特征与改进性能和效率在转移学习中的关系。我们测试了十一个流行的探索算法，针对各种转移类型进行测试，以确定那些积极影响在线

    arXiv:2404.02235v1 Announce Type: cross  Abstract: In deep reinforcement learning (RL) research, there has been a concerted effort to design more efficient and productive exploration methods while solving sparse-reward problems. These exploration methods often share common principles (e.g., improving diversity) and implementation details (e.g., intrinsic reward). Prior work found that non-stationary Markov decision processes (MDPs) require exploration to efficiently adapt to changes in the environment with online transfer learning. However, the relationship between specific exploration characteristics and effective transfer learning in deep RL has not been characterized. In this work, we seek to understand the relationships between salient exploration characteristics and improved performance and efficiency in transfer learning. We test eleven popular exploration algorithms on a variety of transfer types -- or ``novelties'' -- to identify the characteristics that positively affect onlin
    
[^91]: 使用3D点云的深度神经网络用于水动力洪水模型中的经验摩擦测量

    Deep Neural Networks with 3D Point Clouds for Empirical Friction Measurements in Hydrodynamic Flood Models

    [https://arxiv.org/abs/2404.02234](https://arxiv.org/abs/2404.02234)

    该研究提出了一种利用深度神经网络和点云数据测量Manning's n的方法，不仅在实验室环境中取得了较好的结果，而且在真实世界的监管和极端暴雨事件下也表现出了更好的预测能力。

    

    摩擦是水动力建模的关键之一；洪水条件对用于计算动量损失的摩擦系数（FFs）非常敏感。然而，经验性的摩擦系数难以测量，因为需要进行实验室实验。洪水模型通常依赖于替代性观测（如土地利用）来估计摩擦系数，导致不确定性。本研究提出了一个经过实验室训练的深度神经网络（DNN），使用数据增强技术，根据点云数据测量Manning's n。该DNN被部署在真实世界的激光雷达点云上，直接测量了在监管和极端暴雨事件下的Manning's n，展示了在1D和2D水动力模型中都有改进的预测能力。对于1D模型，与土地覆盖值相比，激光雷达数值使得河道水深的差异减小了。对于1D/2D耦合模型，激光雷达数值产生

    arXiv:2404.02234v1 Announce Type: new  Abstract: Friction is one of the cruxes of hydrodynamic modeling; flood conditions are highly sensitive to the Friction Factors (FFs) used to calculate momentum losses. However, empirical FFs are challenging to measure because they require laboratory experiments. Flood models often rely on surrogate observations (such as land use) to estimate FFs, introducing uncertainty. This research presents a laboratory-trained Deep Neural Network (DNN), trained using flume experiments with data augmentation techniques, to measure Manning's n based on Point Cloud data. The DNN was deployed on real-world lidar Point Clouds to directly measure Manning's n under regulatory and extreme storm events, showing improved prediction capabilities in both 1D and 2D hydrodynamic models. For 1D models, the lidar values decreased differences with regulatory models for in-channel water depth when compared to land cover values. For 1D/2D coupled models, the lidar values produc
    
[^92]: OOSTraj：利用视觉定位去噪的视界轨迹预测

    OOSTraj: Out-of-Sight Trajectory Prediction With Vision-Positioning Denoising

    [https://arxiv.org/abs/2404.02227](https://arxiv.org/abs/2404.02227)

    提出了一种利用视觉定位技术进行视界轨迹预测的新方法，可以有效地解决视野外物体和传感器数据噪声的挑战，达到了最先进的性能。

    

    轨迹预测在计算机视觉和自动驾驶中是基础性的，特别是为了理解行人行为和实现积极决策。现有的方法往往假设精确完整的观测数据，忽视了与视野外物体和由于有限摄像头范围、物理障碍以及缺乏去噪传感器数据的真实数据相关的挑战。这样的偏见是关键的安全问题，因为这可能导致关键的非可见对象被忽略。为了弥补这一差距，我们提出了一种利用视觉定位技术进行视界轨迹预测的新方法。我们的方法以无监督方式去噪嘈杂的传感器观测，并将视野外物体的传感器轨迹精确映射到视觉轨迹中。这种方法在视野外轨迹预测方面展现了最先进的性能。

    arXiv:2404.02227v1 Announce Type: cross  Abstract: Trajectory prediction is fundamental in computer vision and autonomous driving, particularly for understanding pedestrian behavior and enabling proactive decision-making. Existing approaches in this field often assume precise and complete observational data, neglecting the challenges associated with out-of-view objects and the noise inherent in sensor data due to limited camera range, physical obstructions, and the absence of ground truth for denoised sensor data. Such oversights are critical safety concerns, as they can result in missing essential, non-visible objects. To bridge this gap, we present a novel method for out-of-sight trajectory prediction that leverages a vision-positioning technique. Our approach denoises noisy sensor observations in an unsupervised manner and precisely maps sensor-based trajectories of out-of-sight objects into visual trajectories. This method has demonstrated state-of-the-art performance in out-of-sig
    
[^93]: 减小规模生成语言模型中的新能力

    Emergent Abilities in Reduced-Scale Generative Language Models

    [https://arxiv.org/abs/2404.02204](https://arxiv.org/abs/2404.02204)

    减小规模数据训练的较小语言模型展示了增强的零样本能力，可在简化语言中实现与大型模型相当的性能。

    

    大语言模型可以在不需要特定任务微调的情况下解决新任务。这种能力，也被称为上下文学习（ICL），被认为是一种新兴能力，主要出现在拥有数十亿参数的大语言模型中。本研究探讨了这种新兴属性是否严格与模型大小相关，或者可以通过在减小规模数据上训练的较小模型来展示。为了探索这一点，我们简化了预训练数据，对36个因果语言模型进行了预训练，参数从100万到1.65亿不等。我们展示了在这种简化的预训练数据上训练的模型在简化语言中表现出增强的零样本能力，实现了与在自由语言上六倍大的预训练模型相当的性能。这表明，缩小语言规模可以使具有有限大小的模型出现零样本学习能力。此外，我们f

    arXiv:2404.02204v1 Announce Type: new  Abstract: Large language models can solve new tasks without task-specific fine-tuning. This ability, also known as in-context learning (ICL), is considered an emergent ability and is primarily seen in large language models with billions of parameters. This study investigates if such emergent properties are strictly tied to model size or can be demonstrated by smaller models trained on reduced-scale data. To explore this, we simplify pre-training data and pre-train 36 causal language models with parameters varying from 1 million to 165 million parameters. We show that models trained on this simplified pre-training data demonstrate enhanced zero-shot capabilities across various tasks in simplified language, achieving performance comparable to that of pre-trained models six times larger on unrestricted language. This suggests that downscaling the language allows zero-shot learning capabilities to emerge in models with limited size. Additionally, we f
    
[^94]: 从之前未见的神经架构搜索数据集中获得的见解

    Insights from the Use of Previously Unseen Neural Architecture Search Datasets

    [https://arxiv.org/abs/2404.02189](https://arxiv.org/abs/2404.02189)

    提出了八个新的神经架构搜索数据集，旨在引起在NAS开发中的关注并鼓励作者考虑模型在开发时未知数据集上的表现。

    

    无限可能的神经网络可以用来解决问题，每个神经网络的性能不同，因此需要深度学习专家来确定最佳神经网络，这违背了消除专家需求的希望。神经架构搜索（NAS）通过自动识别最佳架构来解决这一问题。然而，迄今为止，NAS的工作集中在一小组数据集上，我们认为这些数据集并不能代表真实世界的问题。我们引入了八个新数据集，用于一系列NAS挑战：AddNIST，Language，MultNIST，CIFARTile，Gutenberg，Isabella，GeoClassing 和 Chesseract。这些数据集和挑战旨在引起NAS开发中的注意和鼓励作者考虑他们的模型在开发时未知数据集上的表现。我们展示了使用标准深度学习方法的实验。

    arXiv:2404.02189v1 Announce Type: cross  Abstract: The boundless possibility of neural networks which can be used to solve a problem -- each with different performance -- leads to a situation where a Deep Learning expert is required to identify the best neural network. This goes against the hope of removing the need for experts. Neural Architecture Search (NAS) offers a solution to this by automatically identifying the best architecture. However, to date, NAS work has focused on a small set of datasets which we argue are not representative of real-world problems. We introduce eight new datasets created for a series of NAS Challenges: AddNIST, Language, MultNIST, CIFARTile, Gutenberg, Isabella, GeoClassing, and Chesseract. These datasets and challenges are developed to direct attention to issues in NAS development and to encourage authors to consider how their models will perform on datasets unknown to them at development time. We present experimentation using standard Deep Learning met
    
[^95]: 采用生成式深度学习方法处理不平衡数据的事故严重性建模

    A Generative Deep Learning Approach for Crash Severity Modeling with Imbalanced Data

    [https://arxiv.org/abs/2404.02187](https://arxiv.org/abs/2404.02187)

    使用生成式深度学习方法处理事故严重性建模中的数据不平衡问题，解决传统和基于深度学习的数据重采样方法难以处理离散风险因素崩溃问题的挑战。

    

    撞车数据通常存在严重的不平衡性，大多数事故是非致命事故，只有少数是由于其罕见性而致命事故。这种数据不平衡问题对事故严重性建模构成挑战，因为它难以拟合和解释具有非常有限样本的致命事故结果。通常，通过数据重采样方法来解决这种数据不平衡问题，如欠采样和过采样技术。然而，大多数传统和基于深度学习的数据重采样方法，如合成少数过采样技术（SMOTE）和生成对抗网络（GAN），专门设计用于处理连续变量。尽管一些重采样方法已经改进以处理连续和离散变量，但它们在处理稀疏离散风险因素相关的崩溃问题上可能会遇到困难。此外，缺乏全面研究来比较...

    arXiv:2404.02187v1 Announce Type: cross  Abstract: Crash data is often greatly imbalanced, with the majority of crashes being non-fatal crashes, and only a small number being fatal crashes due to their rarity. Such data imbalance issue poses a challenge for crash severity modeling since it struggles to fit and interpret fatal crash outcomes with very limited samples. Usually, such data imbalance issues are addressed by data resampling methods, such as under-sampling and over-sampling techniques. However, most traditional and deep learning-based data resampling methods, such as synthetic minority oversampling technique (SMOTE) and generative Adversarial Networks (GAN) are designed dedicated to processing continuous variables. Though some resampling methods have improved to handle both continuous and discrete variables, they may have difficulties in dealing with the collapse issue associated with sparse discrete risk factors. Moreover, there is a lack of comprehensive studies that compar
    
[^96]: 集成模型在分析光谱数据中的益处是什么？

    What is to be gained by ensemble models in analysis of spectroscopic data?

    [https://arxiv.org/abs/2404.02184](https://arxiv.org/abs/2404.02184)

    集成模型在光谱数据分析中表现出优异的预测性能，能够持续优于其他候选模型

    

    进行了一项实证研究，比较了旨在改善光谱数据预测的不同集成模型实现。将一系列候选模型拟合到回归和分类设置的基准数据集中。利用线性混合模型对模型拟合的预测性能指标进行统计分析，该分析结果基于数据的随机拆分。结果显示，在我们的应用中，集成分类器能够始终优于候选模型。

    arXiv:2404.02184v1 Announce Type: new  Abstract: An empirical study was carried out to compare different implementations of ensemble models aimed at improving prediction in spectroscopic data. A wide range of candidate models were fitted to benchmark datasets from regression and classification settings. A statistical analysis using linear mixed model was carried out on prediction performance criteria resulting from model fits over random splits of the data. The results showed that the ensemble classifiers were able to consistently outperform candidate models in our application
    
[^97]: 自组织代理：面向超大规模代码生成和优化的LLM多代理框架

    Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization

    [https://arxiv.org/abs/2404.02183](https://arxiv.org/abs/2404.02183)

    提出了自组织多代理框架（SoA），实现了大规模代码的可扩展高效生成和优化，代理可自主运作生成和修改代码组件，并根据问题复杂性动态增加数量。

    

    最近，在使用大型语言模型（LLM）代理进行自动代码生成方面取得了突破性进展，使我们更接近自动软件开发的未来。然而，现有的单代理方法在生成和改进大规模复杂代码库方面存在局限，这是由于上下文长度的限制所导致的。为了解决这一挑战，我们提出了自组织多代理框架（SoA），这是一种新颖的多代理框架，可以实现大规模代码的可扩展高效生成和优化。在SoA中，自组织代理独立运作，以生成和修改代码组件，同时无缝协作构建整体代码库。我们框架的一个关键特点是基于问题复杂性自动增加代理的数量，从而实现动态扩展性。这使得整体代码量可以根据代理数量无限增加，同时由每个代理管理的代码量也随之增加。

    arXiv:2404.02183v1 Announce Type: cross  Abstract: Recent advancements in automatic code generation using large language model (LLM) agent have brought us closer to the future of automated software development. However, existing single-agent approaches face limitations in generating and improving large-scale, complex codebases due to constraints in context length. To tackle this challenge, we propose Self-Organized multi-Agent framework (SoA), a novel multi-agent framework that enables the scalable and efficient generation and optimization of large-scale code. In SoA, self-organized agents operate independently to generate and modify code components while seamlessly collaborating to construct the overall codebase. A key feature of our framework is the automatic multiplication of agents based on problem complexity, allowing for dynamic scalability. This enables the overall code volume to be increased indefinitely according to the number of agents, while the amount of code managed by eac
    
[^98]: 利用机器学习通过INDT-ASD印度数据库早期检测自闭症

    Leveraging Machine Learning for Early Autism Detection via INDT-ASD Indian Database

    [https://arxiv.org/abs/2404.02181](https://arxiv.org/abs/2404.02181)

    本研究旨在通过利用机器学习开发一种简单、快速、廉价的技术，利用INDT-ASD印度数据库早期检测自闭症。

    

    机器学习（ML）在医疗领域发展迅速，神经发育问题的诊断是医疗领域的一个重要领域。自闭症谱系障碍（ASD）是全球增长最快的发展性障碍之一。临床筛查测试用于识别自闭症症状昂贵且耗时。但现在随着ML的进展，早期识别自闭症变得可行。先前进行过许多不同技术的探索，但在利用经过临床验证的印度ASD数据库预测自闭症特征的能力方面，没有一个产生预期结果。因此，本研究旨在通过使用ML开发一种简单、快速、廉价的技术来识别ASD。采用了多种机器学习分类器，包括Adaboost（AB）、Gradient Boost（GB）、决策树（DT）、逻辑回归

    arXiv:2404.02181v1 Announce Type: cross  Abstract: Machine learning (ML) has advanced quickly, particularly throughout the area of health care. The diagnosis of neurodevelopment problems using ML is a very important area of healthcare. Autism spectrum disorder (ASD) is one of the developmental disorders that is growing the fastest globally. The clinical screening tests used to identify autistic symptoms are expensive and time-consuming. But now that ML has been advanced, it's feasible to identify autism early on. Previously, many different techniques have been used in investigations. Still, none of them have produced the anticipated outcomes when it comes to the capacity to predict autistic features utilizing a clinically validated Indian ASD database. Therefore, this study aimed to develop a simple, quick, and inexpensive technique for identifying ASD by using ML. Various machine learning classifiers, including Adaboost (AB), Gradient Boost (GB), Decision Tree (DT), Logistic Regressio
    
[^99]: 通过堆叠自动编码器和聚类实现地质制图的遥感框架

    Remote sensing framework for geological mapping via stacked autoencoders and clustering

    [https://arxiv.org/abs/2404.02180](https://arxiv.org/abs/2404.02180)

    通过堆叠自动编码器和聚类实现遥感数据地质制图的无监督机器学习框架

    

    有监督学习方法在遥感地质制图中面临着由于准确标记训练数据的稀缺性而限制的问题。相反，无监督学习方法，如降维和聚类，能够在不依赖预定义标签的情况下揭示遥感数据中的模式和结构。降维方法具有在提高地质图准确性方面发挥关键作用的潜力。虽然传统的降维方法可能在非线性数据上遇到困难，但无监督深度学习模型，如自动编码器，能够模拟数据中的非线性关系。堆叠自动编码器具有多个相互连接的层，用于捕获对遥感数据有用的分层数据表示。在本研究中，我们提出了一个利用堆叠自动编码器和聚类处理遥感数据的无监督机器学习框架。

    arXiv:2404.02180v1 Announce Type: cross  Abstract: Supervised learning methods for geological mapping via remote sensing face limitations due to the scarcity of accurately labelled training data. In contrast, unsupervised learning methods, such as dimensionality reduction and clustering have the ability to uncover patterns and structures in remote sensing data without relying on predefined labels. Dimensionality reduction methods have the potential to play a crucial role in improving the accuracy of geological maps. Although conventional dimensionality reduction methods may struggle with nonlinear data, unsupervised deep learning models such as autoencoders have the ability to model nonlinear relationship in data. Stacked autoencoders feature multiple interconnected layers to capture hierarchical data representations that can be useful for remote sensing data. In this study, we present an unsupervised machine learning framework for processing remote sensing data by utilizing stacked au
    
[^100]: 探索量子增强的机器学习在计算机视觉中的应用和对嘈杂中等规模量子设备的见解

    Exploring Quantum-Enhanced Machine Learning for Computer Vision: Applications and Insights on Noisy Intermediate-Scale Quantum Devices

    [https://arxiv.org/abs/2404.02177](https://arxiv.org/abs/2404.02177)

    本研究探讨了量子计算和机器学习的交叉点，重点关注计算机视觉任务，评估了混合量子-经典算法对小规模量子设备的有效性

    

    随着中等规模量子计算机的进展，量子算法在模拟物理系统、化学、优化和密码学等领域的应用变得更加普遍。然而，这些被称为嘈杂中等规模量子（NISQ）的量子计算机容易受到噪声的影响，促使寻找能够发挥量子优势而无需广泛错误校正程序的应用。由于机器学习（ML），特别是深度学习（DL），面临资源密集型训练和算法不透明性的挑战。因此，本研究探讨了量子计算和机器学习的交叉点，重点关注计算机视觉任务。具体而言，它评估了混合量子-经典算法的有效性，例如数据重新上传方案和贴片生成对抗网络（GAN）模型，在小规模量子设备上。通过实际实施和测试

    arXiv:2404.02177v1 Announce Type: cross  Abstract: As medium-scale quantum computers progress, the application of quantum algorithms across diverse fields like simulating physical systems, chemistry, optimization, and cryptography becomes more prevalent. However, these quantum computers, known as Noisy Intermediate Scale Quantum (NISQ), are susceptible to noise, prompting the search for applications that can capitalize on quantum advantage without extensive error correction procedures. Since, Machine Learning (ML), particularly Deep Learning (DL), faces challenges due to resource-intensive training and algorithmic opacity. Therefore, this study explores the intersection of quantum computing and ML, focusing on computer vision tasks. Specifically, it evaluates the effectiveness of hybrid quantum-classical algorithms, such as the data re-uploading scheme and the patch Generative Adversarial Networks (GAN) model, on small-scale quantum devices. Through practical implementation and testing
    
[^101]: 消费者反应的社会动态：融合统计物理学与营销动态的统一框架

    Social Dynamics of Consumer Response: A Unified Framework Integrating Statistical Physics and Marketing Dynamics

    [https://arxiv.org/abs/2404.02175](https://arxiv.org/abs/2404.02175)

    使用统计物理学和市场营销动态的理论框架，本研究提出了一个创新的方程，准确捕捉了广告支出与消费者反应之间的复杂关系，并验证了其有效性。

    

    理解消费者对广告输入的反应对于旨在优化广告策略并提高广告活动有效性的营销人员至关重要。本研究通过应用源自物理学和社会心理学的理论框架，研究消费者行为的复杂性。我们提出了一个创新的方程，捕捉了广告支出与消费者反应之间的关系，利用了诸如对称性、标度律和相变等概念。通过将我们的方程验证与Michaelis-Menten和Hill方程等著名模型相比较，我们证明了其在准确表示消费者反应动态复杂性方面的有效性。分析强调了关键模型参数（如营销效果、反应敏感度和行为敏感度）对影响消费者行为的重要性。该研究探讨了广告商和营销人员的实际影响。

    arXiv:2404.02175v1 Announce Type: cross  Abstract: Comprehending how consumers react to advertising inputs is essential for marketers aiming to optimize advertising strategies and improve campaign effectiveness. This study examines the complex nature of consumer behaviour by applying theoretical frameworks derived from physics and social psychology. We present an innovative equation that captures the relation between spending on advertising and consumer response, using concepts such as symmetries, scaling laws, and phase transitions. By validating our equation against well-known models such as the Michaelis-Menten and Hill equations, we prove its effectiveness in accurately representing the complexity of consumer response dynamics. The analysis emphasizes the importance of key model parameters, such as marketing effectiveness, response sensitivity, and behavioural sensitivity, in influencing consumer behaviour. The work explores the practical implications for advertisers and marketers,
    
[^102]: 磁性微游泳器在高保真度模拟人体毛细血管中的路径规划

    Path planning of magnetic microswimmers in high-fidelity simulations of capillaries with deep reinforcement learning

    [https://arxiv.org/abs/2404.02171](https://arxiv.org/abs/2404.02171)

    本研究通过深度强化学习训练的强化学习代理，在复杂的毛细血管网络中成功地实现了磁性微游泳器的路径规划。

    

    生物医学应用如定向药物输送、微创手术或感知依赖于以最小创伤方式到达身体内精确区域。人工细菌鞭毛(ABFs)已被视为通过循环系统导航完成此任务的潜在工具。虽然在简单场景中已了解ABFs的控制和游泳特性，但它们在血液中的行为仍不清楚。我们进行了ABFs在人类视网膜中复杂毛细血管网络中演化的模拟。通过之前在降阶模型上训练的强化学习代理，ABF被稳健地引导到指定目标。

    arXiv:2404.02171v1 Announce Type: cross  Abstract: Biomedical applications such as targeted drug delivery, microsurgery or sensing rely on reaching precise areas within the body in a minimally invasive way. Artificial bacterial flagella (ABFs) have emerged as potential tools for this task by navigating through the circulatory system. While the control and swimming characteristics of ABFs is understood in simple scenarios, their behavior within the bloodstream remains unclear. We conduct simulations of ABFs evolving in the complex capillary networks found in the human retina. The ABF is robustly guided to a prescribed target by a reinforcement learning agent previously trained on a reduced order model.
    
[^103]: AUTODIFF：基于结构的药物设计的自回归扩散建模

    AUTODIFF: Autoregressive Diffusion Modeling for Structure-based Drug Design

    [https://arxiv.org/abs/2404.02003](https://arxiv.org/abs/2404.02003)

    提出了一种基于扩散的分段自回归生成模型AUTODIFF，其中包括一种名为conformal motif的新型分子组装策略和SE(3)-等变卷积网络编码蛋白质-配体复合物相互作用的方法，能够解决结构基药物设计中的局部结构和构象问题。

    

    结构基药物设计（SBDD）旨在生成能够紧密结合靶蛋白的分子，是药物发现中的一个关键问题，先前的方法已经取得了初步成功。然而，大多数现有方法仍然存在无效的局部结构或不现实的构象问题，这主要是由于键角或扭转角度的倾斜不足造成的。为了缓解这些问题，我们提出了AUTODIFF，一种基于扩散的分段自回归生成模型。具体而言，我们设计了一种名为conformal motif的新型分子组装策略，首先保留分子的局部结构的构象，然后我们使用SE(3)-等变卷积网络编码蛋白质-配体复合物的相互作用，并通过扩散建模逐个motif生成分子。此外，我们还通过约束生成分子的分子质量改进了SBDD的评估框架。

    arXiv:2404.02003v1 Announce Type: new  Abstract: Structure-based drug design (SBDD), which aims to generate molecules that can bind tightly to the target protein, is an essential problem in drug discovery, and previous approaches have achieved initial success. However, most existing methods still suffer from invalid local structure or unrealistic conformation issues, which are mainly due to the poor leaning of bond angles or torsional angles. To alleviate these problems, we propose AUTODIFF, a diffusion-based fragment-wise autoregressive generation model. Specifically, we design a novel molecule assembly strategy named conformal motif that preserves the conformation of local structures of molecules first, then we encode the interaction of the protein-ligand complex with an SE(3)-equivariant convolutional network and generate molecules motif-by-motif with diffusion modeling. In addition, we also improve the evaluation framework of SBDD by constraining the molecular weights of the genera
    
[^104]: 利用最大均值差异重心在强化学习中传播价值函数的不确定性

    Utilizing Maximum Mean Discrepancy Barycenter for Propagating the Uncertainty of Value Functions in Reinforcement Learning

    [https://arxiv.org/abs/2404.00686](https://arxiv.org/abs/2404.00686)

    这项工作引入了最大均值差异Q学习（MMD-QL）来改进强化学习中价值函数不确定性的传播，通过使用MMD重心，实现了比Wasserstein距离更紧的概率度量，在实验中表现优于其他算法，并结合深度网络创造了MMD Q网络（MMD-QN）。

    

    考虑到价值函数的不确定性可以促进强化学习中的探索。我们的工作引入了最大均值差异Q学习（MMD-QL），以改进Wasserstein Q学习（WQL），用于在时间差分（TD）更新期间传播不确定性。MMD-QL使用MMD重心来实现这一目的，因为MMD提供了比Wasserstein距离更紧的概率度量之间的接近度估计。首先，我们证明了在平均损失度量下，MMD-QL在马尔可夫决策过程（MDP）中是“可能近似正确”的。在考虑到累积奖励的情况下，对表格环境进行的实验表明，MMD-QL优于WQL和其他算法。其次，我们将深度网络纳入MMD-QL中，创建MMD Q网络（MMD-QN）。通过合理假设，我们分析了MMD-QN在函数逼近中的收敛速度。在具有挑战性的Atari游戏上的实证结果表明，MMD-QN表现良好。

    arXiv:2404.00686v1 Announce Type: new  Abstract: Accounting for the uncertainty of value functions boosts exploration in Reinforcement Learning (RL). Our work introduces Maximum Mean Discrepancy Q-Learning (MMD-QL) to improve Wasserstein Q-Learning (WQL) for uncertainty propagation during Temporal Difference (TD) updates. MMD-QL uses the MMD barycenter for this purpose, as MMD provides a tighter estimate of closeness between probability measures than the Wasserstein distance. Firstly, we establish that MMD-QL is Probably Approximately Correct in MDP (PAC-MDP) under the average loss metric. Concerning the accumulated rewards, experiments on tabular environments show that MMD-QL outperforms WQL and other algorithms. Secondly, we incorporate deep networks into MMD-QL to create MMD Q-Network (MMD-QN). Making reasonable assumptions, we analyze the convergence rates of MMD-QN using function approximation. Empirical results on challenging Atari games demonstrate that MMD-QN performs well comp
    
[^105]: InfLoRA：无干扰的低秩自适应持续学习方法

    InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning

    [https://arxiv.org/abs/2404.00228](https://arxiv.org/abs/2404.00228)

    InfLoRA提出了一种新的PEFT方法，名为无干扰低秩自适应（InfLoRA），用于持续学习，旨在消除新任务对旧任务的干扰，帮助模型在稳定性和可塑性之间取得良好平衡。

    

    持续学习要求模型依次学习多个任务。在持续学习中，模型应具备在旧任务上维持性能（稳定性）和不断适应新任务的能力（可塑性）。最近，基于参数高效微调（PEFT）的持续学习方法变得越来越受欢迎。尽管现有基于PEFT的持续学习方法表现出比非PEFT方法更优秀的性能，但大多数方法并未考虑如何消除新任务对旧任务的干扰，从而阻碍模型在稳定性和可塑性之间取得良好平衡。本文提出了一种新的PEFT方法，称为无干扰低秩自适应（InfLoRA）方法，用于持续学习。

    arXiv:2404.00228v1 Announce Type: cross  Abstract: Continual learning requires the model to learn multiple tasks sequentially. In continual learning, the model should possess the ability to maintain its performance on old tasks (stability) and the ability to adapt to new tasks continuously (plasticity). Recently, parameter-efficient fine-tuning (PEFT), which involves freezing a pre-trained model and injecting a small number of learnable parameters to adapt to downstream tasks, has gained increasing popularity in continual learning. Although existing continual learning methods based on PEFT have demonstrated superior performance compared to those not based on PEFT, most of them do not consider how to eliminate the interference of the new task on the old tasks, which inhibits the model from making a good trade-off between stability and plasticity. In this work, we propose a new PEFT method, called interference-free low-rank adaptation (InfLoRA), for continual learning. InfLoRA injects a 
    
[^106]: 建模大规模步行和骑行网络：使用手机和众包数据的机器学习方法

    Modeling Large-Scale Walking and Cycling Networks: A Machine Learning Approach Using Mobile Phone and Crowdsourced Data

    [https://arxiv.org/abs/2404.00162](https://arxiv.org/abs/2404.00162)

    该研究使用机器学习方法结合手机和众包数据，建立了一个模型来估算大规模区域网络中的步行和骑行量，讨论了在模型训练、测试和推断中面临的挑战和限制。

    

    步行和骑行被认为可以带来显著的健康、环境和经济优势。然而，基于证据的积极交通规划和政策的发展受到数据限制的阻碍，例如众包数据的偏见和手机数据的代表性问题。在这项研究中，我们开发并应用了基于机器学习的建模方法，用于估算澳大利亚新南威尔士州一个包含188,999个步行链接和114,885个骑行链接的大规模地区网络的日常步行和骑行量。建模方法利用了众包和手机数据以及人口、土地利用、地形、气候等一系列其他数据集。该研究讨论了与模型训练、测试和推断的三个方面相关的独特挑战和限制，考虑到建模网络的大地理范围和相对稀缺的情况。

    arXiv:2404.00162v1 Announce Type: new  Abstract: Walking and cycling are known to bring substantial health, environmental, and economic advantages. However, the development of evidence-based active transportation planning and policies has been impeded by significant data limitations, such as biases in crowdsourced data and representativeness issues of mobile phone data. In this study, we develop and apply a machine learning based modeling approach for estimating daily walking and cycling volumes across a large-scale regional network in New South Wales, Australia that includes 188,999 walking links and 114,885 cycling links. The modeling methodology leverages crowdsourced and mobile phone data as well as a range of other datasets on population, land use, topography, climate, etc. The study discusses the unique challenges and limitations related to all three aspects of model training, testing, and inference given the large geographical extent of the modeled networks and relative scarcity
    
[^107]: 利用量子增强机器学习赋能信用评分系统

    Empowering Credit Scoring Systems with Quantum-Enhanced Machine Learning

    [https://arxiv.org/abs/2404.00015](https://arxiv.org/abs/2404.00015)

    提出了一种名为Systemic Quantum Score (SQS)的新方法，展示在金融领域生产级应用案例中相比纯经典模型更有优势，能够从较少数据点中提取模式并表现出更好性能。

    

    Quantum Kernels被认为在量子机器学习的早期阶段提供了有用性。然而，在利用庞大数据集时，高度复杂的经典模型很难超越，特别是在理解力方面。尽管如此，一旦数据稀缺且倾斜，经典模型就会遇到困难。量子特征空间被预计在这样具有挑战性的情景中能够找到更好的数据特征和目标类别之间的联系，最重要的是增强了泛化能力。在这项工作中，我们提出了一种名为Systemic Quantum Score (SQS)的新方法，并提供了初步结果，表明在金融行业生产级应用案例中，SQS可能比纯经典模型具有优势。我们的具体研究表明，SQS能够从较少的数据点中提取出模式，并且在数据需求量大的算法（如XGBoost）上表现出更好的性能，带来优势。

    arXiv:2404.00015v1 Announce Type: cross  Abstract: Quantum Kernels are projected to provide early-stage usefulness for quantum machine learning. However, highly sophisticated classical models are hard to surpass without losing interpretability, particularly when vast datasets can be exploited. Nonetheless, classical models struggle once data is scarce and skewed. Quantum feature spaces are projected to find better links between data features and the target class to be predicted even in such challenging scenarios and most importantly, enhanced generalization capabilities. In this work, we propose a novel approach called Systemic Quantum Score (SQS) and provide preliminary results indicating potential advantage over purely classical models in a production grade use case for the Finance sector. SQS shows in our specific study an increased capacity to extract patterns out of fewer data points as well as improved performance over data-hungry algorithms such as XGBoost, providing advantage i
    
[^108]: 发挥大型语言模型在数据科学中预测表格任务的潜力

    Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science

    [https://arxiv.org/abs/2403.20208](https://arxiv.org/abs/2403.20208)

    本研究旨在利用大型语言模型解决数据科学中表格数据预测任务，通过在丰富的数据集上训练Llama-2模型并进行实际应用，取得显著的改进。

    

    在数据科学领域，分类、回归和缺失值填充等预测任务是与表格数据相关的常见挑战。这项研究旨在应用大型语言模型(LLMs)来解决这些预测任务。尽管LLMs擅长理解自然语言，但在处理结构化表格数据方面表现不佳。我们的研究旨在通过收集带有指令注释的表格语料库，并在这一丰富的数据集上对Llama-2进行大规模训练，以弥合这一差距。此外，我们研究了将训练模型应用于零-shot预测、少-shot预测和上下文学习场景的实际应用。通过广泛实验，我们的方法论显示了显著的改进。

    arXiv:2403.20208v1 Announce Type: new  Abstract: In the domain of data science, the predictive tasks of classification, regression, and imputation of missing values are commonly encountered challenges associated with tabular data. This research endeavors to apply Large Language Models (LLMs) towards addressing these predictive tasks. Despite their proficiency in comprehending natural language, LLMs fall short in dealing with structured tabular data. This limitation stems from their lacking exposure to the intricacies of tabular data during their foundational training. Our research aims to mitigate this gap by compiling a comprehensive corpus of tables annotated with instructions and executing large-scale training of Llama-2 on this enriched dataset. Furthermore, we investigate the practical application of applying the trained model to zero-shot prediction, few-shot prediction, and in-context learning scenarios. Through extensive experiments, our methodology has shown significant improv
    
[^109]: Gegenbauer图神经网络用于时变信号重构

    Gegenbauer Graph Neural Networks for Time-varying Signal Reconstruction

    [https://arxiv.org/abs/2403.19800](https://arxiv.org/abs/2403.19800)

    提出了一种新颖的Gegenbauer-based graph convolutional (GegenConv)算子，用于提高时变信号重构的准确性

    

    重构时变图信号（或图时间序列插补）是机器学习和信号处理中的一个关键问题，具有广泛的应用，从传感器网络中的缺失数据插补到时间序列预测。准确捕捉这些信号固有的时空信息对于有效解决这些任务至关重要。然而，现有方法依赖于时间差的平滑性假设和简单的凸优化技术，存在固有限制。为了解决这些挑战，我们提出了一种结合学习模块以增强下游任务准确性的新方法。为此，我们引入基于Gegenbauer多项式理论的Gegenbauer-based graph convolutional（GegenConv）算子，这是传统切比雪夫图卷积的推 generalization。

    arXiv:2403.19800v1 Announce Type: cross  Abstract: Reconstructing time-varying graph signals (or graph time-series imputation) is a critical problem in machine learning and signal processing with broad applications, ranging from missing data imputation in sensor networks to time-series forecasting. Accurately capturing the spatio-temporal information inherent in these signals is crucial for effectively addressing these tasks. However, existing approaches relying on smoothness assumptions of temporal differences and simple convex optimization techniques have inherent limitations. To address these challenges, we propose a novel approach that incorporates a learning module to enhance the accuracy of the downstream task. To this end, we introduce the Gegenbauer-based graph convolutional (GegenConv) operator, which is a generalization of the conventional Chebyshev graph convolution by leveraging the theory of Gegenbauer polynomials. By deviating from traditional convex problems, we expand t
    
[^110]: 理解人类反馈对齐学习动态的研究

    Understanding the Learning Dynamics of Alignment with Human Feedback

    [https://arxiv.org/abs/2403.18742](https://arxiv.org/abs/2403.18742)

    本研究对人类偏好对齐的学习动态进行了理论分析，显示了偏好数据集的分布如何影响模型更新速度，并提供了对训练准确度的严格保证，同时揭示了优化易于优先考虑高偏好可区分性行为的复杂现象。

    

    大型语言模型（LLMs）与人类意图对齐已成为安全部署模型在实际系统中的关键任务。现有的对齐方法虽然在经验上取得了成功，但理论上了解这些方法如何影响模型行为仍然是一个悬而未决的问题。我们的工作首次尝试在理论上分析人类偏好对齐的学习动态。我们正式展示了偏好数据集的分布如何影响模型更新速度，并对训练准确度提供了严格的保证。我们的理论还揭示了一个复杂现象，即优化易于优先考虑具有更高偏好可区分性的行为。我们在当代LLMs和对齐任务上在实证上验证了我们的发现，强化了我们的理论见解，并为未来的对齐方法提供了启示。免责声明：本文包含有效

    arXiv:2403.18742v1 Announce Type: cross  Abstract: Aligning large language models (LLMs) with human intentions has become a critical task for safely deploying models in real-world systems. While existing alignment approaches have seen empirical success, theoretically understanding how these methods affect model behavior remains an open question. Our work provides an initial attempt to theoretically analyze the learning dynamics of human preference alignment. We formally show how the distribution of preference datasets influences the rate of model updates and provide rigorous guarantees on the training accuracy. Our theory also reveals an intricate phenomenon where the optimization is prone to prioritizing certain behaviors with higher preference distinguishability. We empirically validate our findings on contemporary LLMs and alignment tasks, reinforcing our theoretical insights and shedding light on considerations for future alignment approaches. Disclaimer: This paper contains potent
    
[^111]: 基于张量的一致性和特异性多视角聚类图学习

    Tensor-based Graph Learning with Consistency and Specificity for Multi-view Clustering

    [https://arxiv.org/abs/2403.18393](https://arxiv.org/abs/2403.18393)

    本文提出了一种新颖的基于张量的图学习框架，同时考虑了多视图聚类的一致性和特异性，解决了现有方法在相似性测量和图信息利用方面的局限性。

    

    图学习被广泛认为是多视角聚类中的关键技术。现有的图学习方法通常涉及基于概率邻居构建自适应邻居图，然后学习一致性图进行聚类，然而，它们面临两个局限性。首先，它们通常依赖欧氏距离来衡量相似性，这在许多真实场景中捕捉数据点间的内在结构时证明是不足够的。其次，大多数这些方法仅关注一致性图，忽略了特定视图的图信息。针对上述缺点，本文提出了一种新颖的基于张量的图学习框架，同时考虑了多视图聚类的一致性和特异性。具体地，我们在斯蒂弗尔流形上计算相似距离以保留内在str

    arXiv:2403.18393v1 Announce Type: new  Abstract: Graph learning is widely recognized as a crucial technique in multi-view clustering. Existing graph learning methods typically involve constructing an adaptive neighbor graph based on probabilistic neighbors and then learning a consensus graph to for clustering, however, they are confronted with two limitations. Firstly, they often rely on Euclidean distance to measure similarity when constructing the adaptive neighbor graph, which proves inadequate in capturing the intrinsic structure among data points in many real-world scenarios. Secondly, most of these methods focus solely on consensus graph, ignoring view-specific graph information. In response to the aforementioned drawbacks, we in this paper propose a novel tensor-based graph learning framework that simultaneously considers consistency and specificity for multi-view clustering. Specifically, we calculate the similarity distance on the Stiefel manifold to preserve the intrinsic str
    
[^112]: Horoballs和次梯度方法

    Horoballs and the subgradient method

    [https://arxiv.org/abs/2403.15749](https://arxiv.org/abs/2403.15749)

    该论文提出了一种在Hadamard空间中进行凸优化的迭代方法，与传统假设不同，其复杂性不依赖于空间曲率的下界。

    

    为了探索Hadamard空间上的凸优化问题，我们考虑了一种类似于次梯度算法的迭代。传统上，这类方法假设底层空间是流形，并且目标是测地凸的：这些方法是使用切空间和指数映射来描述的。相比之下，我们的迭代适用于一般的Hadamard空间，是在底层空间本身中构建的，并且依赖于客观水平集的horospherical凸性。对于这个受限制的客观类，我们证明了与通常形式的复杂性结果。值得注意的是，复杂性不依赖于空间曲率的下界。

    arXiv:2403.15749v1 Announce Type: cross  Abstract: To explore convex optimization on Hadamard spaces, we consider an iteration in the style of a subgradient algorithm. Traditionally, such methods assume that the underlying spaces are manifolds and that the objectives are geodesically convex: the methods are described using tangent spaces and exponential maps. By contrast, our iteration applies in a general Hadamard space, is framed in the underlying space itself, and relies instead on horospherical convexity of the objective level sets. For this restricted class of objectives, we prove a complexity result of the usual form. Notably, the complexity does not depend on a lower bound on the space curvature.
    
[^113]: 灰色信息神经网络用于时间序列预测

    Grey-informed neural network for time-series forecasting

    [https://arxiv.org/abs/2403.15027](https://arxiv.org/abs/2403.15027)

    本研究提出了灰色信息神经网络（GINN），通过遵循灰色系统的微分方程模型，提高了神经网络输出的可解释性，使其能够有效处理小数据样本，产生可靠的预测。

    

    神经网络模型在各个领域展现出了出色的性能，成功解决了复杂问题。然而，大多数模型被视为黑盒，需要大量数据进行开发。因此，在数据有限的情况下，由于缺乏透明度和数据稀缺性，构建适当的模型变得具有挑战性。为了解决这些挑战，本研究建议实施灰色信息神经网络（GINN）。GINN 确保神经网络的输出遵循灰色系统的微分方程模型，提高了可解释性。此外，结合灰色系统理论中的先验知识使传统神经网络能够有效处理小数据样本。我们提出的模型已被观察到能够揭示现实世界中的潜在模式，并基于经验数据产生可靠的预测。

    arXiv:2403.15027v1 Announce Type: cross  Abstract: Neural network models have shown outstanding performance and successful resolutions to complex problems in various fields. However, the majority of these models are viewed as black-box, requiring a significant amount of data for development. Consequently, in situations with limited data, constructing appropriate models becomes challenging due to the lack of transparency and scarcity of data. To tackle these challenges, this study suggests the implementation of a grey-informed neural network (GINN). The GINN ensures that the output of the neural network follows the differential equation model of the grey system, improving interpretability. Moreover, incorporating prior knowledge from grey system theory enables traditional neural networks to effectively handle small data samples. Our proposed model has been observed to uncover underlying patterns in the real world and produce reliable forecasts based on empirical data.
    
[^114]: MolBind: 多模态对齐语言、分子和蛋白质

    MolBind: Multimodal Alignment of Language, Molecules, and Proteins

    [https://arxiv.org/abs/2403.08167](https://arxiv.org/abs/2403.08167)

    MolBind 提出了一个框架，通过对比学习为多种模态训练编码器，将所有模态映射到共享特征空间，实现多模态语义对齐。

    

    生物学和化学的最新进展已经利用多模态学习，将分子及其自然语言描述整合起来，以增强药物发现。然而，当前的预训练框架仅限于两种模态，设计一个统一的网络来处理不同模态（例如自然语言、2D分子图、3D分子构象和3D蛋白质）仍然具有挑战性，因为它们之间存在固有的差距。

    arXiv:2403.08167v1 Announce Type: cross  Abstract: Recent advancements in biology and chemistry have leveraged multi-modal learning, integrating molecules and their natural language descriptions to enhance drug discovery. However, current pre-training frameworks are limited to two modalities, and designing a unified network to process different modalities (e.g., natural language, 2D molecular graphs, 3D molecular conformations, and 3D proteins) remains challenging due to inherent gaps among them. In this work, we propose MolBind, a framework that trains encoders for multiple modalities through contrastive learning, mapping all modalities to a shared feature space for multi-modal semantic alignment. To facilitate effective pre-training of MolBind on multiple modalities, we also build and collect a high-quality dataset with four modalities, MolBind-M4, including graph-language, conformation-language, graph-conformation, and conformation-protein paired data. MolBind shows superior zero-sh
    
[^115]: 具有市场运营应用的生成式概率预测

    Generative Probabilistic Forecasting with Applications in Market Operations

    [https://arxiv.org/abs/2403.05743](https://arxiv.org/abs/2403.05743)

    提出了一种基于Wiener-Kallianpur创新表示的生成式概率预测方法，包括自编码器和新颖的深度学习算法，具有渐近最优性和结构收敛性质，适用于实时市场运营中的高动态和波动时间序列。

    

    本文提出了一种新颖的生成式概率预测方法，该方法源自于非参数时间序列的Wiener-Kallianpur创新表示。在生成人工智能的范式下，所提出的预测架构包括一个自编码器，将非参数多变量随机过程转化为规范的创新序列，从中根据过去样本生成未来时间序列样本，条件是它们的概率分布取决于过去样本。提出了一种新的深度学习算法，将潜在过程限制为具有匹配自编码器输入-输出条件概率分布的独立同分布序列。建立了所提出的生成式预测方法的渐近最优性和结构收敛性质。该方法在实时市场运营中涉及高度动态和波动时间序列的三个应用方面。

    arXiv:2403.05743v1 Announce Type: cross  Abstract: This paper presents a novel generative probabilistic forecasting approach derived from the Wiener-Kallianpur innovation representation of nonparametric time series. Under the paradigm of generative artificial intelligence, the proposed forecasting architecture includes an autoencoder that transforms nonparametric multivariate random processes into canonical innovation sequences, from which future time series samples are generated according to their probability distributions conditioned on past samples. A novel deep-learning algorithm is proposed that constrains the latent process to be an independent and identically distributed sequence with matching autoencoder input-output conditional probability distributions. Asymptotic optimality and structural convergence properties of the proposed generative forecasting approach are established. Three applications involving highly dynamic and volatile time series in real-time market operations a
    
[^116]: $\mathtt{tsGT}$：具有Transformer的随机时间序列建模

    $\mathtt{tsGT}$: Stochastic Time Series Modeling With Transformer

    [https://arxiv.org/abs/2403.05713](https://arxiv.org/abs/2403.05713)

    $\mathtt{tsGT}$是一种基于通用Transformer架构的随机时间序列模型，表现优于最先进模型，并超过其随机同行，特别在数据分布建模和边际分位值预测方面具备优势。

    

    时间序列方法在几乎所有处理时间结构化数据的科学领域中都具有基础重要性。最近，出现了一大批具有时间序列特定架构偏见的确定性Transformer模型。本文采取了不同的方向，引入了$\mathtt{tsGT}$，这是一种基于通用Transformer架构构建的随机时间序列模型。我们专注于使用一个众所周知且理论上合理的滚动窗口回测和评估协议。我们展示了$\mathtt{tsGT}$在四个常用数据集上在MAD和RMSE方面优于最先进模型，并在QL和CRPS方面超过了其随机同行。我们通过详细分析$\mathtt{tsGT}$在建模数据分布和预测边际分位值方面的能力来补充这些结果。

    arXiv:2403.05713v1 Announce Type: new  Abstract: Time series methods are of fundamental importance in virtually any field of science that deals with temporally structured data. Recently, there has been a surge of deterministic transformer models with time series-specific architectural biases. In this paper, we go in a different direction by introducing $\mathtt{tsGT}$, a stochastic time series model built on a general-purpose transformer architecture. We focus on using a well-known and theoretically justified rolling window backtesting and evaluation protocol. We show that $\mathtt{tsGT}$ outperforms the state-of-the-art models on MAD and RMSE, and surpasses its stochastic peers on QL and CRPS, on four commonly used datasets. We complement these results with a detailed analysis of $\mathtt{tsGT}$'s ability to model the data distribution and predict marginal quantile values.
    
[^117]: 面向大型语言模型的隐私感知语义缓存

    Privacy-Aware Semantic Cache for Large Language Models

    [https://arxiv.org/abs/2403.02694](https://arxiv.org/abs/2403.02694)

    MeanCache是一种面向LLMs的语义缓存，能够识别语义上相似的查询，从而减少查询成本，服务提供商负载和环境影响。

    

    大型语言模型（LLMs）如ChatGPT、Google Bard、Claude和Llama 2彻底改变了自然语言处理和搜索引擎动态。然而，这些模型造成了异常高的计算成本。本文介绍了MeanCache，一种用于LLMs的语义缓存，它能够识别语义上相似的查询以确定缓存命中或未命中。

    arXiv:2403.02694v1 Announce Type: cross  Abstract: Large Language Models (LLMs) like ChatGPT, Google Bard, Claude, and Llama 2 have revolutionized natural language processing and search engine dynamics. However, these models incur exceptionally high computational costs. For instance, GPT-3 consists of 175 billion parameters and inference on these models also demands billions of floating-point operations. Caching is a natural solution to reduce LLM inference costs on repeated queries. However, existing caching methods are incapable of finding semantic similarities among LLM queries, leading to unacceptable false hit-and-miss rates.   This paper introduces MeanCache, a semantic cache for LLMs that identifies semantically similar queries to determine cache hit or miss. Using MeanCache, the response to a user's semantically similar query can be retrieved from a local cache rather than re-querying the LLM, thus reducing costs, service provider load, and environmental impact. MeanCache lever
    
[^118]: 通过最优输运实现全局和本地提示的合作，用于联邦学习

    Global and Local Prompts Cooperation via Optimal Transport for Federated Learning

    [https://arxiv.org/abs/2403.00041](https://arxiv.org/abs/2403.00041)

    提出了联邦提示合作 via Optimal Transport（FedOTP）方法，通过最优输运实现全局和本地提示的合作，针对数据异质性设计了高效的协作提示学习策略。

    

    预训练的视觉-语言模型中的提示学习在各种下游任务中表现出了卓越的灵活性。最近的研究尝试将这种强大的预训练模型整合到联邦学习框架中，以同时降低通信成本并促进对数据不足的局部训练。为了应对当前联邦提示学习方法在系统化解决严重的数据异质性方面的不足，即涉及标签和特征转移的数据分布，我们提出了通过最优输运实现联邦提示合作（FedOTP），它引入了高效的协作提示学习策略，以在每个客户端基础上捕捉不同的类别特征。具体而言，对于每个客户端，我们学习一个全局提示来提取客户端之间的共识知识，还学习一个本地提示来捕获特定客户端的特征。

    arXiv:2403.00041v1 Announce Type: cross  Abstract: Prompt learning in pretrained visual-language models has shown remarkable flexibility across various downstream tasks. Leveraging its inherent lightweight nature, recent research attempted to integrate the powerful pretrained models into federated learning frameworks to simultaneously reduce communication costs and promote local training on insufficient data. Despite these efforts, current federated prompt learning methods lack specialized designs to systematically address severe data heterogeneities, e.g., data distribution with both label and feature shifts involved. To address this challenge, we present Federated Prompts Cooperation via Optimal Transport (FedOTP), which introduces efficient collaborative prompt learning strategies to capture diverse category traits on a per-client basis. Specifically, for each client, we learn a global prompt to extract consensus knowledge among clients, and a local prompt to capture client-specific
    
[^119]: 压缩机设备中的时间序列分析：一项调查

    Time Series Analysis in Compressor-Based Machines: A Survey

    [https://arxiv.org/abs/2402.17802](https://arxiv.org/abs/2402.17802)

    该论文调查了应用于压缩机设备运行多变量时间序列的故障检测、故障预测、预测和变点检测等任务的最新研究。

    

    在工业和居住环境中，如冰箱、暖通空调系统、热泵和制冷机等基于压缩机的设备对满足生产和消费需求至关重要。传感器和物联网连接的普及支持了监测系统的发展，能够检测和预测故障，识别行为变化，并预测设备和其组件的操作状态。本文的重点是调查最近对这些任务（故障检测、故障预测、预测和变点检测）应用于表征压缩机设备运行的多变量时间序列的研究。具体来说，故障检测可以检测和诊断故障，故障预测可以预测这种发生，预测可以预测设备特征变量的未来值，变点检测可以识别设备行为中的显著变化。

    arXiv:2402.17802v1 Announce Type: new  Abstract: In both industrial and residential contexts, compressor-based machines, such as refrigerators, HVAC systems, heat pumps and chillers, are essential to fulfil production and consumers' needs. The diffusion of sensors and IoT connectivity supports the development of monitoring systems able to detect and predict faults, identify behavioural shifts and forecast the operational status of machines and of their components. The focus of this paper is to survey the recent research on such tasks as Fault Detection, Fault Prediction, Forecasting and Change Point Detection applied to multivariate time series characterizing the operations of compressor-based machines. Specifically, Fault Detection detects and diagnoses faults, Fault Prediction predicts such occurrences, forecasting anticipates the future value of characteristic variables of machines and Change Point Detection identifies significant variations in the behaviour of the appliances, such 
    
[^120]: OSCaR:对象状态字幕和状态变化表示

    OSCaR: Object State Captioning and State Change Representation

    [https://arxiv.org/abs/2402.17128](https://arxiv.org/abs/2402.17128)

    本文介绍了一个新的数据集和基准OSCaR，旨在解决描述复杂视觉环境中对象状态变化的问题，为评估多模态大型语言提供了一个新的实验平台。

    

    arXiv:2402.17128v3 公告类型: 跨 面向人类在真实世界环境中的交互视角，智能模型推断和理解对象状态的变化能力是人工智能研究的一个重要且具有挑战性的方面。该任务涉及描述复杂的视觉环境，识别活跃对象，以及通过语言解释它们的变化。传统方法将对象字幕和状态变化检测进行隔离，提供了对动态环境的有限视图。此外，依赖于一小套符号化词汇来表示变化限制了语言的表达力。为了解决这些挑战，在本文中，我们介绍了对象状态字幕和状态变化表示（OSCaR）数据集和基准。OSCaR包括来自各种主观视角视频集合的14,084个带注释视频片段，涵盖近1,000个独特对象。它为评估多模态大型语言提供了一个新的实验平台。

    arXiv:2402.17128v3 Announce Type: cross  Abstract: The capability of intelligent models to extrapolate and comprehend changes in object states is a crucial yet demanding aspect of AI research, particularly through the lens of human interaction in real-world settings. This task involves describing complex visual environments, identifying active objects, and interpreting their changes as conveyed through language. Traditional methods, which isolate object captioning and state change detection, offer a limited view of dynamic environments. Moreover, relying on a small set of symbolic words to represent changes has restricted the expressiveness of the language. To address these challenges, in this paper, we introduce the Object State Captioning and State Change Representation (OSCaR) dataset and benchmark. OSCaR consists of 14,084 annotated video segments with nearly 1,000 unique objects from various egocentric video collections. It sets a new testbed for evaluating multimodal large langua
    
[^121]: 具有实时递归学习和最大相关性准则的四元数递归神经网络

    Quaternion recurrent neural network with real-time recurrent learning and maximum correntropy criterion

    [https://arxiv.org/abs/2402.14227](https://arxiv.org/abs/2402.14227)

    通过结合实时递归学习算法和最大相关性准则作为损失函数，提出了用于处理含异常值3D和4D数据的鲁棒四元数递归神经网络，所使用的最大相关性损失函数对异常值不太敏感，适用于多维嘈杂或不确定数据应用。

    

    我们开发了一种强大的四元数递归神经网络（QRNN），用于实时处理具有异常值的3D和4D数据。这是通过将实时递归学习（RTRL）算法和最大相关性准则（MCC）结合为损失函数来实现的。尽管均方误差和最大相关性准则都是可行的代价函数，但结果表明非二次最大相关性损失函数对异常值不太敏感，适用于具有多维嘈杂或不确定数据的应用。这两种算法基于新颖的广义HR（GHR）微积分导出，它允许对四元数变量的实函数进行微分，并提供乘法和链式法则，从而实现优雅且简洁的导出。在胸部内部标记物的运动预测背景下进行的仿真结果涵盖了肺癌放疗中的常规和不规则呼吸序列。

    arXiv:2402.14227v1 Announce Type: new  Abstract: We develop a robust quaternion recurrent neural network (QRNN) for real-time processing of 3D and 4D data with outliers. This is achieved by combining the real-time recurrent learning (RTRL) algorithm and the maximum correntropy criterion (MCC) as a loss function. While both the mean square error and maximum correntropy criterion are viable cost functions, it is shown that the non-quadratic maximum correntropy loss function is less sensitive to outliers, making it suitable for applications with multidimensional noisy or uncertain data. Both algorithms are derived based on the novel generalised HR (GHR) calculus, which allows for the differentiation of real functions of quaternion variables and offers the product and chain rules, thus enabling elegant and compact derivations. Simulation results in the context of motion prediction of chest internal markers for lung cancer radiotherapy, which includes regular and irregular breathing sequenc
    
[^122]: 模拟失调: 大型语言模型的安全对齐可能会适得其反！

    Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!

    [https://arxiv.org/abs/2402.12343](https://arxiv.org/abs/2402.12343)

    安全对齐的大型语言模型可能会通过模拟失调框架，在对抗性操纵下产生危险结果，对训练的语言模型具有双倍有害性，高于强基线，强调了即使在安全对齐后也需要重新评估开源语言模型的重要性。

    

    大型语言模型（LLMs）需要进行安全对齐，以确保与人类进行安全的对话。然而，在这项工作中，我们引入了一种推理时攻击框架，表明安全对齐也可能在对抗性操纵下无意中促成有害结果。这个框架被命名为模拟失调（ED），在输出空间中不良地组合了一对开源预训练和安全对齐的语言模型，产生了一个有害的语言模型而无需任何训练。我们对ED在三个数据集和四个模型系列（Llama-1、Llama-2、Mistral和Alpaca）上的实验表明，ED使预训练模型的有害性增加了一倍，并胜过强基线，以较大优势在48个评估子集中的43个中实现了最高的有害率。至关重要的是，我们的研究结果凸显了即使在安全对齐后，重新评估开源语言模型实践的重要性。

    arXiv:2402.12343v1 Announce Type: new  Abstract: Large language models (LLMs) need to undergo safety alignment to ensure safe conversations with humans. However, in this work, we introduce an inference-time attack framework, demonstrating that safety alignment can also unintentionally facilitate harmful outcomes under adversarial manipulation. This framework, named Emulated Disalignment (ED), adversely combines a pair of open-source pre-trained and safety-aligned language models in the output space to produce a harmful language model without any training. Our experiments with ED across three datasets and four model families (Llama-1, Llama-2, Mistral, and Alpaca) show that ED doubles the harmfulness of pre-trained models and outperforms strong baselines, achieving the highest harmful rate in 43 out of 48 evaluation subsets by a large margin. Crucially, our findings highlight the importance of reevaluating the practice of open-sourcing language models even after safety alignment.
    
[^123]: 多任务策略学习中基于任务条件的视觉特征自适应

    Task-conditioned adaptation of visual features in multi-task policy learning

    [https://arxiv.org/abs/2402.07739](https://arxiv.org/abs/2402.07739)

    本文通过任务条件的自适应器，在多任务策略学习的背景下，调整预训练的大型视觉模型，使其能够解决多个任务，并且无需微调预先训练的权重。

    

    成功地解决各种任务是自主代理的核心能力，这需要灵活地调整底层的决策策略，并且如我们在这项工作中所提出的，还需要调整底层的感知模块。一个类比的论证是人类的视觉系统，它使用自上而下的信号来专注于当前任务。类似地，在这项工作中，我们在多任务策略学习的上下文中，通过特定的下游任务来调整预先训练的大视觉模型。我们引入了基于任务条件的适配器，在不需要微调任何预先训练权重的情况下，与通过行为克隆训练的单一策略结合使用，能够解决多个任务。我们在策略和视觉适配器上根据任务嵌入进行条件化，如果任务是已知的，则可以在推理过程中选择任务嵌入，否则可以从一组示例演示中进行推断。为此，我们提出了一种新的基于优化的估计器。我们在...（摘要未完成）

    Successfully addressing a wide variety of tasks is a core ability of autonomous agents, which requires flexibly adapting the underlying decision-making strategies and, as we argue in this work, also adapting the underlying perception modules. An analogical argument would be the human visual system, which uses top-down signals to focus attention determined by the current task. Similarly, in this work, we adapt pre-trained large vision models conditioned on specific downstream tasks in the context of multi-task policy learning. We introduce task-conditioned adapters that do not require finetuning any pre-trained weights, combined with a single policy trained with behavior cloning and capable of addressing multiple tasks. We condition the policy and visual adapters on task embeddings, which can be selected at inference if the task is known, or alternatively inferred from a set of example demonstrations. To this end, we propose a new optimization-based estimator. We evaluate the method on 
    
[^124]: 改进心肌梗死检测：一种新颖的多模态复合核策略在单一类别分类中的应用

    Refining Myocardial Infarction Detection: A Novel Multi-Modal Composite Kernel Strategy in One-Class Classification

    [https://arxiv.org/abs/2402.06530](https://arxiv.org/abs/2402.06530)

    本研究提出了一种新的方法，使用基于超声心动图的一种基于多模态复合核策略的单一类别分类算法来进行早期心肌梗死的检测。这种方法通过优化投影矩阵和特征转化，提高了心肌梗死检测的能力。

    

    早期心肌梗死（MI）的检测对于预防进一步心肌损伤非常重要，MI是由冠状动脉疾病（CAD）引起的一种严重疾病。本研究引入了一种新方法，使用一种基于超声心动图的单一类别分类（OCC）算法进行早期MI检测。我们的研究通过采用基于多模态子空间支持向量数据描述的新方法克服了超声心动图数据有限的挑战。提出的技术涉及一种特殊的MI检测框架，使用复合核在非线性投影技巧中融合高斯和拉普拉斯sigmoid函数，将多视图超声心动图结合起来。此外，我们通过在优化过程中调整投影矩阵的最大化策略，提高了投影矩阵更新策略的效果。我们的方法通过将从超声心动图数据中提取的特征有效地转化为优化的低维空间，增强了MI检测的能力。

    Early detection of myocardial infarction (MI), a critical condition arising from coronary artery disease (CAD), is vital to prevent further myocardial damage. This study introduces a novel method for early MI detection using a one-class classification (OCC) algorithm in echocardiography. Our study overcomes the challenge of limited echocardiography data availability by adopting a novel approach based on Multi-modal Subspace Support Vector Data Description. The proposed technique involves a specialized MI detection framework employing multi-view echocardiography incorporating a composite kernel in the non-linear projection trick, fusing Gaussian and Laplacian sigmoid functions. Additionally, we enhance the update strategy of the projection matrices by adapting maximization for both or one of the modalities in the optimization process. Our method boosts MI detection capability by efficiently transforming features extracted from echocardiography data into an optimized lower-dimensional su
    
[^125]: 对角费舍尔信息矩阵估计器的权衡

    Tradeoffs of Diagonal Fisher Information Matrix Estimators

    [https://arxiv.org/abs/2402.05379](https://arxiv.org/abs/2402.05379)

    本研究探讨了使用对角费舍尔信息矩阵估计器的权衡。通过分析和数值研究，发现方差量取决于非线性与不同参数组之间的关系，应该在估计费舍尔信息时予以重视。

    

    费舍尔信息矩阵描述了神经网络参数空间中的局部几何性质，它提供了理论和工具来理解和优化神经网络。鉴于其计算成本高，实践者通常使用随机估计器，并仅评估对角线条目。我们研究了两种这样的估计器，其准确性和样本复杂性取决于它们关联的方差。我们推导了方差的界限，并在回归和分类网络中实例化它们。我们通过分析和数值研究来权衡这两个估计器。我们发现方差量取决于关于不同参数组的非线性，当估计费舍尔信息时不能忽视它们。

    The Fisher information matrix characterizes the local geometry in the parameter space of neural networks. It elucidates insightful theories and useful tools to understand and optimize neural networks. Given its high computational cost, practitioners often use random estimators and evaluate only the diagonal entries. We examine two such estimators, whose accuracy and sample complexity depend on their associated variances. We derive bounds of the variances and instantiate them in regression and classification networks. We navigate trade-offs of both estimators based on analytical and numerical studies. We find that the variance quantities depend on the non-linearity with respect to different parameter groups and should not be neglected when estimating the Fisher information.
    
[^126]: 逻辑规范引导下的强化学习智能体动态任务采样

    Logical Specifications-guided Dynamic Task Sampling for Reinforcement Learning Agents

    [https://arxiv.org/abs/2402.03678](https://arxiv.org/abs/2402.03678)

    本文提出了一种逻辑规范引导下的动态任务采样（LSTS）方法，通过学习一组强化学习策略，根据高级任务规范指导智能体在最小化环境交互次数的同时实现从初始状态到目标状态的引导。在网格世界实验中，LSTS实现了改进的时间到阈值。

    

    强化学习（RL）在使人工智能智能体学习多样化行为方面取得了重要进展。然而，学习有效的策略通常需要大量的环境交互。为了减少样本复杂性问题，最近的方法使用高级任务规范，如线性时态逻辑（LTL$_f$）公式或奖励机器（RM），来指导智能体的学习过程。在这项工作中，我们提出了一种新颖的方法，称为逻辑规范引导下的动态任务采样（LSTS），它通过学习一组强化学习策略，根据高级任务规范指导智能体从初始状态到目标状态，同时最小化环境交互次数。与以前的工作不同，LSTS不假设环境动力学或奖励机器的信息，并动态采样导致成功目标策略的有希望的任务。我们在一个网格世界上评估了LSTS，并展示了它实现了改进的时间到阈值。

    Reinforcement Learning (RL) has made significant strides in enabling artificial agents to learn diverse behaviors. However, learning an effective policy often requires a large number of environment interactions. To mitigate sample complexity issues, recent approaches have used high-level task specifications, such as Linear Temporal Logic (LTL$_f$) formulas or Reward Machines (RM), to guide the learning progress of the agent. In this work, we propose a novel approach, called Logical Specifications-guided Dynamic Task Sampling (LSTS), that learns a set of RL policies to guide an agent from an initial state to a goal state based on a high-level task specification, while minimizing the number of environmental interactions. Unlike previous work, LSTS does not assume information about the environment dynamics or the Reward Machine, and dynamically samples promising tasks that lead to successful goal policies. We evaluate LSTS on a gridworld and show that it achieves improved time-to-threshol
    
[^127]: “我应该使用哪个LLM？”：评估用于印度本科计算机科学学生任务的LLMs

    "Which LLM should I use?": Evaluating LLMs for tasks performed by Undergraduate Computer Science Students in India

    [https://arxiv.org/abs/2402.01687](https://arxiv.org/abs/2402.01687)

    本研究评估了各种LLMs在本科计算机科学学生常见任务中的效果，并指导学生选择适合他们的LLM.

    

    本研究评估了各种大型语言模型（LLMs）在本科计算机科学学生常见任务中的效果。尽管计算机教育界的许多研究已经探讨了使用LLMs执行各种任务的可能性，但缺乏对不同LLMs进行综合比较和评估哪个LLMs对不同任务最有效的研究。我们的研究系统地评估了一些公开可用的LLMs，例如Google Bard，ChatGPT，GitHub Copilot Chat和Microsoft Copilot在本科计算机科学学生常遇到的不同任务中的表现。这些任务包括代码生成，解释，项目构思，内容生成，课程作业和电子邮件撰写。计算机科学的高年级和低年级学生进行了这些任务的评估，并提供了对模型的优点和局限性的见解。该研究旨在指导学生选择适合他们的LLM。

    This study evaluates the effectiveness of various large language models (LLMs) in performing tasks common among undergraduate computer science students. Although a number of research studies in the computing education community have explored the possibility of using LLMs for a variety of tasks, there is a lack of comprehensive research comparing different LLMs and evaluating which LLMs are most effective for different tasks. Our research systematically assesses some of the publicly available LLMs such as Google Bard, ChatGPT, GitHub Copilot Chat, and Microsoft Copilot across diverse tasks commonly encountered by undergraduate computer science students. These tasks include code generation, explanation, project ideation, content generation, class assignments, and email composition. Evaluation for these tasks was carried out by junior and senior students in computer science, and provides insights into the models' strengths and limitations. This study aims to guide students in selecting su
    
[^128]: 代码感知提示：基于LLM的回归设置下覆盖率导向的测试生成研究

    Code-Aware Prompting: A study of Coverage Guided Test Generation in Regression Setting using LLM

    [https://arxiv.org/abs/2402.00097](https://arxiv.org/abs/2402.00097)

    本文提出了一种代码感知提示策略（SymPrompt），用于基于LLM的测试生成，通过将测试生成过程分解为多阶段序列，并以驱动策略推动每个阶段，改善了测试生成的覆盖率。

    

    测试在确保软件质量方面起着至关重要的作用，然而传统的基于搜索的软件测试方法经常在复杂的软件单元上遇到困难，达不到最佳的测试覆盖率。最近使用大型语言模型（LLMs）进行测试生成的研究一直致力于通过优化测试生成上下文和纠正模型输出中的错误来改进生成质量，但使用了固定的提示策略，即提示模型在没有额外指导的情况下生成测试。因此，LLM生成的测试套件仍然存在低覆盖率的问题。在本文中，我们提出了SymPrompt，一种用于LLMs的代码感知提示策略来进行测试生成。SymPrompt的方法是基于最近的研究，该研究证明了LLMs在以多步方式思考问题时可以解决更复杂的逻辑问题。我们将这种方法应用于测试生成，将测试套件生成过程分解为多阶段序列，每个阶段都由一种驱动策略来推动。

    Testing plays a pivotal role in ensuring software quality, yet conventional Search Based Software Testing (SBST) methods often struggle with complex software units, achieving suboptimal test coverage. Recent work using large language models (LLMs) for test generation have focused on improving generation quality through optimizing the test generation context and correcting errors in model outputs, but use fixed prompting strategies that prompt the model to generate tests without additional guidance. As a result LLM-generated test suites still suffer from low coverage. In this paper, we present SymPrompt, a code-aware prompting strategy for LLMs in test generation. SymPrompt's approach is based on recent work that demonstrates LLMs can solve more complex logical problems when prompted to reason about the problem in a multi-step fashion. We apply this methodology to test generation by deconstructing the testsuite generation process into a multi-stage sequence, each of which is driven by a
    
[^129]: 航班滑行安全的跑道物体分类器的鲁棒性评估

    Robustness Assessment of a Runway Object Classifier for Safe Aircraft Taxiing

    [https://arxiv.org/abs/2402.00035](https://arxiv.org/abs/2402.00035)

    本文介绍了对航班滑行安全的跑道物体分类器的鲁棒性评估，使用形式方法评估了该分类器对三种常见图像扰动类型的鲁棒性，并提出了一种利用单调性的方法。

    

    随着深度神经网络(DNNs)在许多计算问题上成为主要解决方案，航空业希望探索它们在减轻飞行员负担和改善运营安全方面的潜力。然而，在这类安全关键应用中使用DNNs需要进行彻底的认证过程。这一需求可以通过形式验证来解决，形式验证提供了严格的保证，例如证明某些误判的不存在。在本文中，我们使用Airbus当前正在开发的图像分类器DNN作为案例研究，旨在在飞机滑行阶段使用。我们使用形式方法来评估这个DNN对三种常见图像扰动类型的鲁棒性：噪声、亮度和对比度，以及它们的部分组合。这个过程涉及多次调用底层验证器，这可能在计算上是昂贵的；因此，我们提出了一种利用单调性的方法。

    As deep neural networks (DNNs) are becoming the prominent solution for many computational problems, the aviation industry seeks to explore their potential in alleviating pilot workload and in improving operational safety. However, the use of DNNs in this type of safety-critical applications requires a thorough certification process. This need can be addressed through formal verification, which provides rigorous assurances -- e.g.,~by proving the absence of certain mispredictions. In this case-study paper, we demonstrate this process using an image-classifier DNN currently under development at Airbus and intended for use during the aircraft taxiing phase. We use formal methods to assess this DNN's robustness to three common image perturbation types: noise, brightness and contrast, and some of their combinations. This process entails multiple invocations of the underlying verifier, which might be computationally expensive; and we therefore propose a method that leverages the monotonicity
    
[^130]: LeanVec: 通过使向量适应来加快搜索速度

    LeanVec: Searching vectors faster by making them fit

    [https://arxiv.org/abs/2312.16335](https://arxiv.org/abs/2312.16335)

    LeanVec是一个结合了线性降维和向量量化的框架，旨在加速高维向量的相似性搜索，解决了相似性搜索系统在高向量维度下的计算和内存压力问题，同时应用于跨模态检索任务。

    

    现代深度学习模型具有生成高维向量的能力，其相似性反映语义相似性。因此，相似性搜索，即在一个大型集合中检索与给定查询相似的向量的操作，已成为许多需要高度准确和及时答案的应用程序的关键组成部分。在这种情况下，高向量维度使得相似性搜索系统承受计算和内存压力，导致性能不佳。此外，跨模态检索任务已经日益普遍，例如用户输入文本查询以找到与该查询最相关的图像。然而，这些查询通常与数据库嵌入具有不同的分布，这使得实现高准确性具有挑战性。在这项工作中，我们提出了LeanVec，这是一个将线性降维与向量量化相结合以加速的框架。

    arXiv:2312.16335v2 Announce Type: replace  Abstract: Modern deep learning models have the ability to generate high-dimensional vectors whose similarity reflects semantic resemblance. Thus, similarity search, i.e., the operation of retrieving those vectors in a large collection that are similar to a given query, has become a critical component of a wide range of applications that demand highly accurate and timely answers. In this setting, the high vector dimensionality puts similarity search systems under compute and memory pressure, leading to subpar performance. Additionally, cross-modal retrieval tasks have become increasingly common, e.g., where a user inputs a text query to find the most relevant images for that query. However, these queries often have different distributions than the database embeddings, making it challenging to achieve high accuracy. In this work, we present LeanVec, a framework that combines linear dimensionality reduction with vector quantization to accelerate 
    
[^131]: 通过迭代本地扩展实现高效可扩展的图生成

    Efficient and Scalable Graph Generation through Iterative Local Expansion

    [https://arxiv.org/abs/2312.11529](https://arxiv.org/abs/2312.11529)

    通过逐步扩展单个节点到目标图的方法，避免了对所有节点对的整个联合分布进行建模，实现了高效可扩展的图生成，同时通过多尺度生成保持了高表达性。

    

    在图的生成模型领域，进行了大量研究。然而，由于代表所有节点对的整个联合分布的复杂性以及同时捕捉全局和局部图结构，大多数现有方法在处理大型图时存在困难。为了克服这些问题，我们引入了一种方法，通过逐步将单个节点扩展到目标图来生成图。在每一步中，通过去噪扩散以本地化方式添加节点和边，首先构建全局结构，然后细化局部细节。局部生成避免了对所有节点对上的整个联合分布进行建模，相对于节点数而言实现了大幅的计算节约，并通过多尺度生成保持了高表达性。我们的实验表明，我们的模型在公认的基准数据集上实现了最先进的性能。

    arXiv:2312.11529v2 Announce Type: replace-cross  Abstract: In the realm of generative models for graphs, extensive research has been conducted. However, most existing methods struggle with large graphs due to the complexity of representing the entire joint distribution across all node pairs and capturing both global and local graph structures simultaneously. To overcome these issues, we introduce a method that generates a graph by progressively expanding a single node to a target graph. In each step, nodes and edges are added in a localized manner through denoising diffusion, building first the global structure, and then refining the local details. The local generation avoids modeling the entire joint distribution over all node pairs, achieving substantial computational savings with subquadratic runtime relative to node count while maintaining high expressivity through multiscale generation. Our experiments show that our model achieves state-of-the-art performance on well-established b
    
[^132]: ReCoRe: 正则化对比度表示学习的世界模型

    ReCoRe: Regularized Contrastive Representation Learning of World Model

    [https://arxiv.org/abs/2312.09056](https://arxiv.org/abs/2312.09056)

    通过正则化对比度表示学习世界模型，该方法提高了样本效率和泛化性能，解决了在视觉导航等日常任务中出现外观变化时的挑战。

    

    近期的无模型强化学习（RL）方法在游戏环境中已经展示出与人类水平相当的有效性，但在视觉导航等日常任务中的成功受到限制，特别是在出现显著外观变化的情况下。为了解决这些挑战，我们提出了一种世界模型，通过（i）对比度无监督学习和（ii）介入不变正则化来学习不变特征。学习世界动力学的显式表示，即世界模型，提高了样本效率，而对比度学习隐含地强化了学习不变特征，改善了泛化性能。然而，简单地将对比度损失集成到世界模型中是不够的，因为基于世界模型的RL方法独立优化表示学习和智能体策略。

    arXiv:2312.09056v2 Announce Type: replace-cross  Abstract: While recent model-free Reinforcement Learning (RL) methods have demonstrated human-level effectiveness in gaming environments, their success in everyday tasks like visual navigation has been limited, particularly under significant appearance variations. This limitation arises from (i) poor sample efficiency and (ii) over-fitting to training scenarios. To address these challenges, we present a world model that learns invariant features using (i) contrastive unsupervised learning and (ii) an intervention-invariant regularizer. Learning an explicit representation of the world dynamics i.e. a world model, improves sample efficiency while contrastive learning implicitly enforces learning of invariant features, which improves generalization. However, the na\"ive integration of contrastive loss to world models is not good enough, as world-model-based RL methods independently optimize representation learning and agent policy. To overc
    
[^133]: 不需要信道反馈的下行FD-RAN传输：基于无线地图的复值预编码网络方法

    Channel-Feedback-Free Transmission for Downlink FD-RAN: A Radio Map based Complex-valued Precoding Network Approach

    [https://arxiv.org/abs/2312.02184](https://arxiv.org/abs/2312.02184)

    提出了一种不依赖信道反馈的新型下行传输方案，通过设计基于无线地图的复值预编码网络模型，实现用户位置为基础的基站预编码。

    

    随着对高质量服务需求的增长，一种创新的网络架构，完全解耦合的RAN（FD-RAN），已经出现，用于更灵活的频谱资源利用和更低的网络成本。然而，在FD-RAN中，由于上行基站和下行基站的解耦，传统的依赖实时信道反馈的传输机制并不适用，因为接收端无法向发送端反馈准确及时的信道状态信息。本文提出了一种新颖的传输方案，不依赖物理层信道反馈。具体而言，我们设计了一个基于无线地图的复值预编码网络（RMCPNet）模型，根据用户位置输出基站预编码。 RMCPNet 包含多个子网络，每个子网络负责从不同的输入模态中提取独特的模态特征。

    arXiv:2312.02184v2 Announce Type: replace-cross  Abstract: As the demand for high-quality services proliferates, an innovative network architecture, the fully-decoupled RAN (FD-RAN), has emerged for more flexible spectrum resource utilization and lower network costs. However, with the decoupling of uplink base stations and downlink base stations in FD-RAN, the traditional transmission mechanism, which relies on real-time channel feedback, is not suitable as the receiver is not able to feedback accurate and timely channel state information to the transmitter. This paper proposes a novel transmission scheme without relying on physical layer channel feedback. Specifically, we design a radio map based complex-valued precoding network~(RMCPNet) model, which outputs the base station precoding based on user location. RMCPNet comprises multiple subnets, with each subnet responsible for extracting unique modal features from diverse input modalities. Furthermore, the multi-modal embeddings deriv
    
[^134]: 通过可学习区域进行文本驱动图像编辑

    Text-Driven Image Editing via Learnable Regions

    [https://arxiv.org/abs/2311.16432](https://arxiv.org/abs/2311.16432)

    该方法通过学习区域实现了文本驱动的图像编辑，无需用户提供遮罩或草图，具有灵活性和能够处理复杂提示的特点。与其他方法相比，展现了竞争性能。

    

    语言已经成为图像编辑的自然接口。本文介绍了一种基于区域的图像编辑方法，其由文本提示驱动，无需用户提供的遮罩或草图。具体来说，我们的方法利用现有的预训练文本到图像模型，并引入一个边界框生成器来识别与文本提示对齐的编辑区域。我们展示了这种简单方法可以实现与当前图像生成模型兼容的灵活编辑，并且能够处理包含多个对象、复杂句子或较长段落的复杂提示。我们进行了广泛的用户研究，将我们的方法与最先进的方法进行了比较。实验表明，我们的方法在与提供的语言描述相对应的具有高保真度和逼真度的图像操作方面表现出竞争力。我们的项目网页可以在此找到：ht

    arXiv:2311.16432v2 Announce Type: replace-cross  Abstract: Language has emerged as a natural interface for image editing. In this paper, we introduce a method for region-based image editing driven by textual prompts, without the need for user-provided masks or sketches. Specifically, our approach leverages an existing pre-trained text-to-image model and introduces a bounding box generator to identify the editing regions that are aligned with the textual prompts. We show that this simple approach enables flexible editing that is compatible with current image generation models, and is able to handle complex prompts featuring multiple objects, complex sentences, or lengthy paragraphs. We conduct an extensive user study to compare our method against state-of-the-art methods. The experiments demonstrate the competitive performance of our method in manipulating images with high fidelity and realism that correspond to the provided language descriptions. Our project webpage can be found at: ht
    
[^135]: 在KL条件下具有最优速率的差分私有非凸优化

    Differentially Private Non-Convex Optimization under the KL Condition with Optimal Rates

    [https://arxiv.org/abs/2311.13447](https://arxiv.org/abs/2311.13447)

    该论文研究了在KL条件下具有最优速率的差分私有非凸优化问题，并提出了针对不同情况的新算法，实现了接近最优的速率。

    

    我们研究了满足$(\gamma,\kappa)$-Kurdyka-Lojasiewicz (KL)条件的损失函数的私有经验风险最小化（ERM）问题。Polyak-Lojasiewicz (PL)条件是这个条件的特例，当$\kappa=2$时。具体来说，我们研究了在$\rho$零集中差分隐私（zCDP）约束下的问题。当$\kappa\in[1,2]$且损失函数在足够大的区域内是Lipschitz和光滑的时，我们提出了一种基于方差减少梯度下降的新算法，其在超额经验风险上实现了速率$\tilde{O}\big(\big(\frac{\sqrt{d}}{n\sqrt{\rho}}\big)^\kappa\big)$，其中$n$是数据集大小，$d$是维度。我们进一步展示了这个速率几乎是最优的。当$\kappa \geq 2$且损失函数代替是Lipschitz和弱凸时，我们展示了通过私有方法可以实现速率$\tilde{O}\big(\big(\frac{\sqrt{d}}{n\sqrt{\rho}}\big)^\kappa\big)$。

    arXiv:2311.13447v2 Announce Type: replace  Abstract: We study private empirical risk minimization (ERM) problem for losses satisfying the $(\gamma,\kappa)$-Kurdyka-{\L}ojasiewicz (KL) condition. The Polyak-{\L}ojasiewicz (PL) condition is a special case of this condition when $\kappa=2$. Specifically, we study this problem under the constraint of $\rho$ zero-concentrated differential privacy (zCDP). When $\kappa\in[1,2]$ and the loss function is Lipschitz and smooth over a sufficiently large region, we provide a new algorithm based on variance reduced gradient descent that achieves the rate $\tilde{O}\big(\big(\frac{\sqrt{d}}{n\sqrt{\rho}}\big)^\kappa\big)$ on the excess empirical risk, where $n$ is the dataset size and $d$ is the dimension. We further show that this rate is nearly optimal. When $\kappa \geq 2$ and the loss is instead Lipschitz and weakly convex, we show it is possible to achieve the rate $\tilde{O}\big(\big(\frac{\sqrt{d}}{n\sqrt{\rho}}\big)^\kappa\big)$ with a privat
    
[^136]: CV-Attention UNet: Attention-based UNet for 3D Cerebrovascular Segmentation of Enhanced TOF-MRA Images

    CV-Attention UNet: Attention-based UNet for 3D Cerebrovascular Segmentation of Enhanced TOF-MRA Images

    [https://arxiv.org/abs/2311.10224](https://arxiv.org/abs/2311.10224)

    本文提出了一种3D脑血管注意力UNet方法，用于精确提取脑血管图像，通过一系列预处理技术和深度监督UNet来改善脑血管分割的准确性，有助于预防中风。

    

    由于缺乏自动化方法来诊断脑血管疾病，时间飞行磁共振血管造影图（TOF-MRA）通常是通过目视评估，这导致耗时。本文提出了一种3D脑血管注意力UNet方法，命名为CV-Attention UNet，用于精确提取脑血管图像。我们提出了一系列预处理技术，随后采用深度监督UNet来改善脑血管分割的准确性，从而有助于预防中风。为了结合低语义和高语义，

    arXiv:2311.10224v2 Announce Type: replace-cross  Abstract: Due to the lack of automated methods, to diagnose cerebrovascular disease, time-of-flight magnetic resonance angiography (TOF-MRA) is assessed visually, making it time-consuming. The commonly used encoder-decoder architectures for cerebrovascular segmentation utilize redundant features, eventually leading to the extraction of low-level features multiple times. Additionally, convolutional neural networks (CNNs) suffer from performance degradation when the batch size is small, and deeper networks experience the vanishing gradient problem. Methods: In this paper, we attempt to solve these limitations and propose the 3D cerebrovascular attention UNet method, named CV-AttentionUNet, for precise extraction of brain vessel images. We proposed a sequence of preprocessing techniques followed by deeply supervised UNet to improve the accuracy of segmentation of the brain vessels leading to a stroke. To combine the low and high semantics, 
    
[^137]: 面向非凸非光滑问题的隐私保护联邦原始-对偶学习与模型稀疏化

    Privacy-preserving Federated Primal-dual Learning for Non-convex and Non-smooth Problems with Model Sparsification

    [https://arxiv.org/abs/2310.19558](https://arxiv.org/abs/2310.19558)

    本文提出了一种针对非凸非光滑FL问题的新颖联邦原始-对偶算法，采用双向模型稀疏化，应用差分隐私进行隐私保障，并提出了FL算法设计指导原则。

    

    隐私保护联邦原始-对偶学习针对于分布式客户端训练模型的类型进行研究，参数服务器进行协调，而不共享客户端数据。本文深入探讨了一类在FL应用中普遍存在但难以处理的非凸非光滑损失函数的联邦问题，这些问题因其复杂的非凸性和非光滑性质以及对通信效率和隐私保护的矛盾要求而具有挑战性。提出了一种针对非凸非光滑FL问题的新颖联邦原始-对偶算法，在隐私保证方面应用了差分隐私。文中还介绍了其独特的见解性属性以及一些隐私和收敛性分析，作为FL算法设计指导原则。在真实数据上进行了大量实验。

    arXiv:2310.19558v2 Announce Type: replace  Abstract: Federated learning (FL) has been recognized as a rapidly growing research area, where the model is trained over massively distributed clients under the orchestration of a parameter server (PS) without sharing clients' data. This paper delves into a class of federated problems characterized by non-convex and non-smooth loss functions, that are prevalent in FL applications but challenging to handle due to their intricate non-convexity and non-smoothness nature and the conflicting requirements on communication efficiency and privacy protection. In this paper, we propose a novel federated primal-dual algorithm with bidirectional model sparsification tailored for non-convex and non-smooth FL problems, and differential privacy is applied for privacy guarantee. Its unique insightful properties and some privacy and convergence analyses are also presented as the FL algorithm design guidelines. Extensive experiments on real-world data are cond
    
[^138]: BatteryML：一个用于电池衰减机器学习的开源平台

    BatteryML:An Open-source platform for Machine Learning on Battery Degradation

    [https://arxiv.org/abs/2310.14714](https://arxiv.org/abs/2310.14714)

    BatteryML是一个开源平台，通过一站式、全面的方法统一了电池衰减建模的数据预处理、特征提取和模型实现，提高了研究应用的实用性和效率。

    

    电池衰减仍然是能源存储领域的一个关键问题，而机器学习作为推动洞察和解决方案的有效工具正在崛起。然而，电化学科学和机器学习的交叉领域带来了复杂的挑战。机器学习专家经常在处理电池科学的复杂性上苦苦挣扎，而电池研究人员则面临着将复杂模型调整到特定数据集的障碍。此外，缺乏涵盖数据格式和评估基准的电池衰减建模的统一标准。鉴于这些障碍，我们提出了BatteryML - 一个一站式、全面且开源的平台，旨在统一数据预处理、特征提取以及传统和最先进模型的实现。这种简化的方法有望提高研究应用的实用性和效率。

    arXiv:2310.14714v4 Announce Type: replace-cross  Abstract: Battery degradation remains a pivotal concern in the energy storage domain, with machine learning emerging as a potent tool to drive forward insights and solutions. However, this intersection of electrochemical science and machine learning poses complex challenges. Machine learning experts often grapple with the intricacies of battery science, while battery researchers face hurdles in adapting intricate models tailored to specific datasets. Beyond this, a cohesive standard for battery degradation modeling, inclusive of data formats and evaluative benchmarks, is conspicuously absent. Recognizing these impediments, we present BatteryML - a one-step, all-encompass, and open-source platform designed to unify data preprocessing, feature extraction, and the implementation of both traditional and state-of-the-art models. This streamlined approach promises to enhance the practicality and efficiency of research applications. BatteryML s
    
[^139]: 与传统机器学习对抗：重新思考大型语言模型在表格分类中的公平性

    Confronting LLMs with Traditional ML: Rethinking the Fairness of Large Language Models in Tabular Classifications

    [https://arxiv.org/abs/2310.14607](https://arxiv.org/abs/2310.14607)

    LLMs在表格分类任务中存在社会偏见，影响了它们的公平性。

    

    最近的文献表明，使用大型语言模型（LLMs）进行表格任务的分类具有潜力。然而，LLMs已被证明存在表现出社会偏见的有害因素，反映了社会中存在的刻板印象和不平等。为此，以及在许多高风险应用中广泛使用表格数据，探讨以下问题至关重要：LLMs在进行表格任务分类时利用了哪些信息源；LLMs对表格数据的分类在多大程度上受到社会偏见和刻板印象的影响；以及这对公平性可能产生的重要影响是什么？通过一系列实验，我们深入探讨这些问题，并表明LLMs倾向于继承来自训练数据的社会偏见，这显著影响了它们在表格分类任务中的公平性。

    arXiv:2310.14607v2 Announce Type: replace  Abstract: Recent literature has suggested the potential of using large language models (LLMs) to make classifications for tabular tasks. However, LLMs have been shown to exhibit harmful social biases that reflect the stereotypes and inequalities present in society. To this end, as well as the widespread use of tabular data in many high-stake applications, it is important to explore the following questions: what sources of information do LLMs draw upon when making classifications for tabular tasks; whether and to what extent are LLM classifications for tabular data influenced by social biases and stereotypes; and what are the consequential implications for fairness?   Through a series of experiments, we delve into these questions and show that LLMs tend to inherit social biases from their training data which significantly impact their fairness in tabular classification tasks. Furthermore, our investigations show that in the context of bias miti
    
[^140]: 对比后训练的自动对构建

    Automatic Pair Construction for Contrastive Post-training

    [https://arxiv.org/abs/2310.02263](https://arxiv.org/abs/2310.02263)

    提出了一种自动构建对比数据的方法，使用多个模型的偏好对，提高了大型语言模型的对齐效果，并且通过DPO对比技术得到了改善，进一步优化了对齐，最终使经过调优的指导学习模型Orca超越了ChatGPT。

    

    对齐作为引导大型语言模型（LLMs）走向人类偏好的重要步骤。本文提出了一种自动构建LLM对比数据的方法，使用来自多个不同强度模型（例如InstructGPT、ChatGPT和GPT-4）的偏好对。我们比较了SLiC和DPO的对比技术与SFT基线，并发现即使在继续SFT饱和后，DPO仍然提供了一个阶跃式的改善。我们还探讨了一种对比后训练的数据课程学习方案，该方案从“更容易”的对开始学习，然后过渡到“更难”的对，进一步提高了对齐效果。最后，我们通过使用更多数据和像Orca这样的更大型模型来扩大实验规模。值得注意的是，我们的自动对比后训练进一步提高了Orca的性能，它已经是一个通过GPT-4输出调优的最先进指导学习模型，从而超越了ChatGPT。

    arXiv:2310.02263v2 Announce Type: replace-cross  Abstract: Alignment serves as an important step to steer large language models (LLMs) towards human preferences. In this paper, we propose an automatic way to construct contrastive data for LLM, using preference pairs from multiple models of varying strengths (e.g., InstructGPT, ChatGPT and GPT-4). We compare the contrastive techniques of SLiC and DPO to SFT baselines and find that DPO provides a step-function improvement even after continuing SFT saturates. We also explore a data curriculum learning scheme for contrastive post-training, which starts by learning from "easier" pairs and transitioning to "harder" ones, which further improves alignment. Finally, we scale up our experiments to train with more data and larger models like Orca. Remarkably, our automatic contrastive post-training further improves the performance of Orca, already a state-of-the-art instruction learning model tuned with GPT-4 outputs, to outperform ChatGPT.
    
[^141]: 全景自拍：生成全身自拍照片

    Total Selfie: Generating Full-Body Selfies

    [https://arxiv.org/abs/2308.14740](https://arxiv.org/abs/2308.14740)

    该方法能够从近距离自拍照片生成出一张别人从几英尺外拍摄的您的全身自拍照片。

    

    我们提出了一种方法，从最初处于手臂长度位置的照片中生成全身自拍照片。由于自拍照片通常是近距离拍摄的，因此其视野有限，透视效果夸张，扭曲了面部形状。我们希望生成别人从几英尺外拍摄的您的照片。我们的方法以您的面部和身体的四张自拍照片、背景图像为输入，并生成一个指定目标姿势的全身自拍照片。我们介绍了一种基于扩散的新方法，将所有这些信息结合成您具有所需姿势和背景的高质量、构图合理的照片。

    arXiv:2308.14740v2 Announce Type: replace-cross  Abstract: We present a method to generate full-body selfies from photographs originally taken at arms length. Because self-captured photos are typically taken close up, they have limited field of view and exaggerated perspective that distorts facial shapes. We instead seek to generate the photo some one else would take of you from a few feet away. Our approach takes as input four selfies of your face and body, a background image, and generates a full-body selfie in a desired target pose. We introduce a novel diffusion-based approach to combine all of this information into high-quality, well-composed photos of you with the desired pose and background.
    
[^142]: 通过表征减缓拥挤: 学习如何提高市场中的经济福利

    Decongestion by Representation: Learning to Improve Economic Welfare in Marketplaces

    [https://arxiv.org/abs/2306.10606](https://arxiv.org/abs/2306.10606)

    研究了通过表征减缓拥挤的方法，从消费者的选择中学习表征以减少拥挤，提高社会福利。

    

    拥挤是市场的一种常见失效模式，在这种情况下，消费者在同一子商品集上进行低效率竞争（例如，在度假租赁平台上追逐同一小部分属性）。传统的经济故事是，价格通过平衡供求来减缓拥挤。但在现代在线市场中，价格通常由卖家以分散的方式设定，并且有关商品的信息不可避免地是部分的。平台的能力仅限于控制表征——默认向用户呈现的有关商品信息的子集。这促使了对通过表征减缓拥挤的研究，即平台试图学习减少拥挤从而提高社会福利的表征。技术挑战是双重的：仅依赖于消费者的选择所揭示的偏好而不是真实偏好；以及与确定表征相关的组合问题。

    arXiv:2306.10606v2 Announce Type: replace  Abstract: Congestion is a common failure mode of markets, where consumers compete inefficiently on the same subset of goods (e.g., chasing the same small set of properties on a vacation rental platform). The typical economic story is that prices decongest by balancing supply and demand. But in modern online marketplaces, prices are typically set in a decentralized way by sellers, and the information about items is inevitably partial. The power of a platform is limited to controlling representations -- the subset of information about items presented by default to users. This motivates the present study of decongestion by representation, where a platform seeks to learn representations that reduce congestion and thus improve social welfare. The technical challenge is twofold: relying only on revealed preferences from the choices of consumers, rather than true preferences; and the combinatorial problem associated with representations that determin
    
[^143]: RDumb：一种质疑我们在持续测试时间适应中取得进展的简单方法

    RDumb: A simple approach that questions our progress in continual test-time adaptation

    [https://arxiv.org/abs/2306.05401](https://arxiv.org/abs/2306.05401)

    RDumb是一个简单的基线方法，能在持续测试时间适应中表现得更好，甚至超过先前提出的最先进方法。

    

    测试时间适应（TTA）允许在部署时根据不断变化的数据分布更新预训练模型。为了检验该领域的进展，我们提出了不断变化腐坏（CCC）基准来衡量TTA技术的渐近表现。我们发现除一种最先进方法外，所有其他方法最终都会崩溃并表现比不适应的模型更糟，包括专门设计用于抗性崩溃的模型。此外，我们引入了一个简单的基线“RDumb”，定期将模型重置为预训练状态。在所有考虑的基准测试中，“RDumb”在表现上要么更好，要么与先前提出的最先进方法持平。我们的结果表明先前的TTA方法既不有效

    arXiv:2306.05401v3 Announce Type: replace  Abstract: Test-Time Adaptation (TTA) allows to update pre-trained models to changing data distributions at deployment time. While early work tested these algorithms for individual fixed distribution shifts, recent work proposed and applied methods for continual adaptation over long timescales. To examine the reported progress in the field, we propose the Continually Changing Corruptions (CCC) benchmark to measure asymptotic performance of TTA techniques. We find that eventually all but one state-of-the-art methods collapse and perform worse than a non-adapting model, including models specifically proposed to be robust to performance collapse. In addition, we introduce a simple baseline, "RDumb", that periodically resets the model to its pretrained state. RDumb performs better or on par with the previously proposed state-of-the-art in all considered benchmarks. Our results show that previous TTA approaches are neither effective at regularizing 
    
[^144]: 从快捷方式到触发器：使用去噪 PoE 进行后门防御

    From Shortcuts to Triggers: Backdoor Defense with Denoised PoE

    [https://arxiv.org/abs/2305.14910](https://arxiv.org/abs/2305.14910)

    提出了一种端到端集成的后门防御框架 DPoE，旨在通过去噪设计和捕捉后门快捷方式的浅层模型，以及防止学习后门快捷方式的主模型，有效抵御各种后门攻击。

    

    语言模型经常面临多样的后门攻击风险，特别是数据污染。因此，研究针对这些攻击的防御解决方案非常重要。现有的后门防御方法主要集中在带有显式触发器的后门攻击上，对抗各种后门攻击与不同触发器的通用防御方法仍然未被充分探索。在本文中，我们提出了一种端到端集成的后门防御框架 DPoE（Denoised Product-of-Experts），灵感来源于后门攻击的快捷方式，以抵御各种后门攻击。DPoE 包含两个模型：一个捕捉后门快捷方式的浅层模型和一个被阻止学习后门快捷方式的主模型。为了处理后门攻击者引起的标签翻转，DPoE 融入了去噪设计。对 SST-2 数据集的实验证明，DPoE 显著提高了对各种类型的防御性能。

    arXiv:2305.14910v3 Announce Type: replace-cross  Abstract: Language models are often at risk of diverse backdoor attacks, especially data poisoning. Thus, it is important to investigate defense solutions for addressing them. Existing backdoor defense methods mainly focus on backdoor attacks with explicit triggers, leaving a universal defense against various backdoor attacks with diverse triggers largely unexplored. In this paper, we propose an end-to-end ensemble-based backdoor defense framework, DPoE (Denoised Product-of-Experts), which is inspired by the shortcut nature of backdoor attacks, to defend various backdoor attacks. DPoE consists of two models: a shallow model that captures the backdoor shortcuts and a main model that is prevented from learning the backdoor shortcuts. To address the label flip caused by backdoor attackers, DPoE incorporates a denoising design. Experiments on SST-2 dataset show that DPoE significantly improves the defense performance against various types of
    
[^145]: 一个中性原子量子处理器上的金融风险管理

    Financial Risk Management on a Neutral Atom Quantum Processor

    [https://arxiv.org/abs/2212.03223](https://arxiv.org/abs/2212.03223)

    提出了一个基于中性原子量子处理器的金融风险管理解决方案，能够在预测信用评级降级方面表现出竞争力、速度更快且具有更好的可解释性，并通过张量网络-based数值模拟验证了这些想法。

    

    能够处理金融领域大数据集的机器学习模型经常变得晦涩难解且运行成本高昂。量子计算范式提出了新的优化技术，结合经典算法，可能提供竞争力更强、速度更快且更易解释的模型。在本工作中，我们提出了一个基于量子增强机器学习的解决方案，用于金融风险管理领域的信用评级下调预测，也被称为堕落天使预测。我们在一个实际数据集上实现了这个解决方案，使用了拥有高达60比特的中性原子量子处理单元。我们报告了与最先进的随机森林基准相竞争的性能，同时我们的模型达到了更好的可解释性和可比较的训练时间。我们研究了如何在短期内改进性能，并用基于张量网络的数值模拟验证了我们的想法。

    arXiv:2212.03223v2 Announce Type: replace-cross  Abstract: Machine Learning models capable of handling the large datasets collected in the financial world can often become black boxes expensive to run. The quantum computing paradigm suggests new optimization techniques, that combined with classical algorithms, may deliver competitive, faster and more interpretable models. In this work we propose a quantum-enhanced machine learning solution for the prediction of credit rating downgrades, also known as fallen-angels forecasting in the financial risk management field. We implement this solution on a neutral atom Quantum Processing Unit with up to 60 qubits on a real-life dataset. We report competitive performances against the state-of-the-art Random Forest benchmark whilst our model achieves better interpretability and comparable training times. We examine how to improve performance in the near-term validating our ideas with Tensor Networks-based numerical simulations.
    
[^146]: 我的机器人会实现我的目标吗？预测MDP策略达到用户指定行为目标的概率

    Will My Robot Achieve My Goals? Predicting the Probability that an MDP Policy Reaches a User-Specified Behavior Target

    [https://arxiv.org/abs/2211.16462](https://arxiv.org/abs/2211.16462)

    本文提出了一种能够预测MDP策略达到用户指定行为目标概率的方法，并通过对符合预测进行反转来计算概率估计。

    

    当自主系统执行任务时，应保持对实现用户目标概率的校准估计。如果该概率低于某个期望水平，应向用户发出警报，以便采取适当干预。本文考虑了用户将目标规定为实值性能摘要的目标区间的设置，例如在固定时间段$H$内测量的累积奖励。在每个时间$t \in \{0, \ldots, H-1\}$，我们的方法会产生一个校准概率估计，即最终累积奖励落在用户指定目标区间$[y^-,y^+]$内的概率。利用这一估计，自主系统可以在概率低于指定阈值时发出警报。我们通过反转符合预测来计算概率估计。我们的出发点是Romano等人的Quantile Regression (CQR)方法，该方法应用了spli

    arXiv:2211.16462v2 Announce Type: replace  Abstract: As an autonomous system performs a task, it should maintain a calibrated estimate of the probability that it will achieve the user's goal. If that probability falls below some desired level, it should alert the user so that appropriate interventions can be made. This paper considers settings where the user's goal is specified as a target interval for a real-valued performance summary, such as the cumulative reward, measured at a fixed horizon $H$. At each time $t \in \{0, \ldots, H-1\}$, our method produces a calibrated estimate of the probability that the final cumulative reward will fall within a user-specified target interval $[y^-,y^+].$ Using this estimate, the autonomous system can raise an alarm if the probability drops below a specified threshold. We compute the probability estimates by inverting conformal prediction. Our starting point is the Conformalized Quantile Regression (CQR) method of Romano et al., which applies spli
    
[^147]: 相信您的 $\nabla$: 基于梯度的干预目标定位用于因果发现

    Trust Your $\nabla$: Gradient-based Intervention Targeting for Causal Discovery

    [https://arxiv.org/abs/2211.13715](https://arxiv.org/abs/2211.13715)

    提出了一种基于梯度的干预目标定位方法，GIT，在因果发现中能够通过信号梯度估计器降低干预次数，在低数据量情况下优于竞争基线。

    

    从数据中推断因果结构是科学中一项具有基础重要性的挑战性任务。观测数据通常不足以唯一确定系统的因果结构。虽然进行干预（即实验）可以改善可识别性，但这些样本通常难以获得且成本高昂。因此，因果发现的实验设计方法旨在通过估计最具信息性的干预目标来最小化干预次数。在这项工作中，我们提出了一种新颖的基于梯度的干预目标定位方法，简称为GIT，它‘相信’了基于梯度的因果发现框架的梯度估计器，以提供干预采集函数的信号。我们在模拟和真实世界数据集上进行了大量实验，并证明GIT在低数据量情况下表现与竞争基线相当，甚至在某些情况下超越它们。

    arXiv:2211.13715v4 Announce Type: replace-cross  Abstract: Inferring causal structure from data is a challenging task of fundamental importance in science. Observational data are often insufficient to identify a system's causal structure uniquely. While conducting interventions (i.e., experiments) can improve the identifiability, such samples are usually challenging and expensive to obtain. Hence, experimental design approaches for causal discovery aim to minimize the number of interventions by estimating the most informative intervention target. In this work, we propose a novel Gradient-based Intervention Targeting method, abbreviated GIT, that 'trusts' the gradient estimator of a gradient-based causal discovery framework to provide signals for the intervention acquisition function. We provide extensive experiments in simulated and real-world datasets and demonstrate that GIT performs on par with competitive baselines, surpassing them in the low-data regime.
    
[^148]: 需求抽样：从多个分布中学习最优

    On-Demand Sampling: Learning Optimally from Multiple Distributions

    [https://arxiv.org/abs/2210.12529](https://arxiv.org/abs/2210.12529)

    本文建立了多分布学习范式的最优样本复杂度，并提出了符合此复杂度的算法，改进了公平联邦学习和协作学习的样本复杂度边界。

    

    社会和现实考虑，如鲁棒性、公平性、社会福利和多智能体权衡已经催生了多分布学习范式，如协作学习、分布鲁棒优化和公平联邦学习。在这些设置中，学习者寻求在$n$个预定义数据分布上均匀最小化其期望损失，同时尽可能少地使用样本。本文建立了这些学习范式的最优样本复杂度，并提供了符合此样本复杂度的算法。值得注意的是，我们的多分布学习样本复杂度边界仅超过了单一分布学习的添加因子$n \log(n) / \epsilon^2$。这改进了Mohri等人提出的公平联邦学习和Nguyen和Zakynthinou提出的协作学习的已知最佳样本复杂度边界，分别为$n$和$\lo的乘法因子。

    arXiv:2210.12529v3 Announce Type: replace  Abstract: Social and real-world considerations such as robustness, fairness, social welfare and multi-agent tradeoffs have given rise to multi-distribution learning paradigms, such as collaborative learning, group distributionally robust optimization, and fair federated learning. In each of these settings, a learner seeks to uniformly minimize its expected loss over $n$ predefined data distributions, while using as few samples as possible. In this paper, we establish the optimal sample complexity of these learning paradigms and give algorithms that meet this sample complexity. Importantly, our sample complexity bounds for multi-distribution learning exceed that of learning a single distribution by only an additive factor of $n \log(n) / \epsilon^2$. This improves upon the best known sample complexity bounds for fair federated learning by Mohri et al. and collaborative learning by Nguyen and Zakynthinou by multiplicative factors of $n$ and $\lo
    
[^149]: 克服通信约束，实现联邦学习中大型预训练模型的应用

    Conquering the Communication Constraints to Enable Large Pre-Trained Models in Federated Learning

    [https://arxiv.org/abs/2210.01708](https://arxiv.org/abs/2210.01708)

    研究克服联邦学习中通信约束的方法，以实现强大的预训练模型在FL中的应用，并同时减少通信负担。

    

    联邦学习（FL）已经成为一种旨在在本地设备上协力训练模型而不需要对原始数据进行中心化访问的有前景的范式。在典型的FL范式（例如FedAvg）中，每一轮模型权重都会被发送到参与客户端并回传到服务器。最近，在联邦学习优化和收敛改进方面展示了使用小型预训练模型是有效的。然而，最近的最先进预训练模型变得更加强大，但也拥有更多参数。在传统的FL中，共享巨大的模型权重可以迅速给系统带来巨大的通信负担，尤其是如果采用更加强大的模型。我们能否找到一个解决方案，在FL中启用这些强大且现成的预训练模型以实现出色性能的同时减少通信负担？为此，我们研究了使用参数高效的方法

    arXiv:2210.01708v3 Announce Type: replace  Abstract: Federated learning (FL) has emerged as a promising paradigm for enabling the collaborative training of models without centralized access to the raw data on local devices. In the typical FL paradigm (e.g., FedAvg), model weights are sent to and from the server each round to participating clients. Recently, the use of small pre-trained models has been shown effective in federated learning optimization and improving convergence. However, recent state-of-the-art pre-trained models are getting more capable but also have more parameters. In conventional FL, sharing the enormous model weights can quickly put a massive communication burden on the system, especially if more capable models are employed. Can we find a solution to enable those strong and readily-available pre-trained models in FL to achieve excellent performance while simultaneously reducing the communication burden? To this end, we investigate the use of parameter-efficient fin
    
[^150]: 从增强采样模拟中重新加权的集体变量流形学习

    Reweighted Manifold Learning of Collective Variables from Enhanced Sampling Simulations

    [https://arxiv.org/abs/2207.14554](https://arxiv.org/abs/2207.14554)

    提出了一个基于各向异性扩散映射的重新加权框架，解决了在增强采样模拟中学习集体变量流形时遇到的偏差问题。

    

    增强采样方法在计算物理学和化学中不可或缺，原子模拟无法穷尽动力系统的高维构型空间，因为采样问题。这类增强采样方法通过识别几个慢度自由度（称为集体变量CVs）并沿着这些CVs增强采样来工作。选择要分析和驱动采样的CVs并不是微不足道的，通常依赖于物理和化学直觉。尽管通常通过流形学习从标准模拟中直接估计CVs来绕过这个问题，但这些方法无法从增强采样模拟中提供到低维流形的映射。因为学习的流形的几何和密度是有偏的。在这里，我们解决了这一关键问题，并提供了一个基于各向异性扩散映射的流形学习的通用重新加权框架。

    arXiv:2207.14554v2 Announce Type: replace-cross  Abstract: Enhanced sampling methods are indispensable in computational physics and chemistry, where atomistic simulations cannot exhaustively sample the high-dimensional configuration space of dynamical systems due to the sampling problem. A class of such enhanced sampling methods works by identifying a few slow degrees of freedom, termed collective variables (CVs), and enhancing the sampling along these CVs. Selecting CVs to analyze and drive the sampling is not trivial and often relies on physical and chemical intuition. Despite routinely circumventing this issue using manifold learning to estimate CVs directly from standard simulations, such methods cannot provide mappings to a low-dimensional manifold from enhanced sampling simulations as the geometry and density of the learned manifold are biased. Here, we address this crucial issue and provide a general reweighting framework based on anisotropic diffusion maps for manifold learning
    
[^151]: 多智体强化学习的离策略修正

    Off-Policy Correction For Multi-Agent Reinforcement Learning

    [https://arxiv.org/abs/2111.11229](https://arxiv.org/abs/2111.11229)

    提出了MA-Trace，一种新的离策略演员-评论家算法，在多智体强化学习环境中具有高可伸缩性，并通过重要性采样作为离策略修正方法，保证了计算分布的质量和算法的收敛性。

    

    多智体强化学习（MARL）为涉及多个相互作用智体的问题提供了一个框架。尽管表面上与单智体情况相似，但多智体问题往往更难训练和在理论上分析。在这项工作中，我们提出了MA-Trace，一种新的基于策略的演员-评论家算法，将V-Trace扩展到MARL设定中。我们算法的关键优势在于其在多工作器设置中的高可伸缩性。为此，MA-Trace利用重要性采样作为离策略修正方法，允许在不影响训练质量的情况下进行计算分布。此外，我们的算法是在理论上有基础的 - 我们证明了一个保证收敛的不动点定理。我们在StarCraft多智体挑战赛上对算法进行了广泛评估，这是多智体算法的标准基准。MA-Trace在所有任务上表现出色，超过了最先进水平。

    arXiv:2111.11229v3 Announce Type: replace-cross  Abstract: Multi-agent reinforcement learning (MARL) provides a framework for problems involving multiple interacting agents. Despite apparent similarity to the single-agent case, multi-agent problems are often harder to train and analyze theoretically. In this work, we propose MA-Trace, a new on-policy actor-critic algorithm, which extends V-Trace to the MARL setting. The key advantage of our algorithm is its high scalability in a multi-worker setting. To this end, MA-Trace utilizes importance sampling as an off-policy correction method, which allows distributing the computations with no impact on the quality of training. Furthermore, our algorithm is theoretically grounded - we prove a fixed-point theorem that guarantees convergence. We evaluate the algorithm extensively on the StarCraft Multi-Agent Challenge, a standard benchmark for multi-agent algorithms. MA-Trace achieves high performance on all its tasks and exceeds state-of-the-ar
    
[^152]: 从安全专家演示中学习健壮的输出控制屏障函数

    Learning Robust Output Control Barrier Functions from Safe Expert Demonstrations

    [https://arxiv.org/abs/2111.09971](https://arxiv.org/abs/2111.09971)

    从专家演示中学习健壮的输出控制屏障函数以确保安全性，当参数化为线性时，优化问题是凸的。

    

    这篇论文解决了从专家演示的部分观测中学习安全输出反馈控制律的问题。假设系统动态模型和状态估计器可用，并带有相应的误差界限，例如，可以从实践中的数据估计得出。我们首先提出了健壮的输出控制屏障函数（ROCBFs）作为确保安全的手段，通过控制安全集的控制前不变性来定义安全性。然后，我们制定了一个优化问题，从表现出安全系统行为的专家演示中学习ROCBFs，例如从人操纵员或专家控制器收集的数据。当ROCBF的参数化是线性时，我们表明，在温和的假设下，优化问题是凸的。除了优化问题，我们还提供了可验证的条件，涉及数据的密度、系统模型和状态估计器的平滑性，以及误差边界的大小。

    arXiv:2111.09971v3 Announce Type: replace-cross  Abstract: This paper addresses learning safe output feedback control laws from partial observations of expert demonstrations. We assume that a model of the system dynamics and a state estimator are available along with corresponding error bounds, e.g., estimated from data in practice. We first propose robust output control barrier functions (ROCBFs) as a means to guarantee safety, as defined through controlled forward invariance of a safe set. We then formulate an optimization problem to learn ROCBFs from expert demonstrations that exhibit safe system behavior, e.g., data collected from a human operator or an expert controller. When the parametrization of the ROCBF is linear, then we show that, under mild assumptions, the optimization problem is convex. Along with the optimization problem, we provide verifiable conditions in terms of the density of the data, smoothness of the system model and state estimator, and the size of the error bo
    
[^153]: 噪声在组合式沟通中的催化作用和归纳偏见的必要性

    Catalytic Role Of Noise And Necessity Of Inductive Biases In The Emergence Of Compositional Communication

    [https://arxiv.org/abs/2111.06464](https://arxiv.org/abs/2111.06464)

    噪声和归纳偏见的作用使得组合式沟通自发产生，并且一定范围内的噪声有助于促进组合性的发展。

    

    沟通是组合式的，如果复杂信号可以表示为较简单子部分的组合。本文理论上表明，训练框架和数据上的归纳偏见对于发展组合式沟通是必要的。此外，我们证明组合性会在信号博弈中自发出现，代理在嘈杂通道上传输信息。我们通过实验证实，一系列噪声水平（取决于模型和数据）确实促进了组合性。最后，我们对这种依赖关系进行了全面研究，并报告了最近研究的组合性度量结果：拓扑相似性、冲突计数和上下文独立性。

    arXiv:2111.06464v2 Announce Type: replace-cross  Abstract: Communication is compositional if complex signals can be represented as a combination of simpler subparts. In this paper, we theoretically show that inductive biases on both the training framework and the data are needed to develop a compositional communication. Moreover, we prove that compositionality spontaneously arises in the signaling games, where agents communicate over a noisy channel. We experimentally confirm that a range of noise levels, which depends on the model and the data, indeed promotes compositionality. Finally, we provide a comprehensive study of this dependence and report results in terms of recently studied compositionality metrics: topographical similarity, conflict count, and context independence.
    
[^154]: 复杂推理任务的子目标搜索

    Subgoal Search For Complex Reasoning Tasks

    [https://arxiv.org/abs/2108.11204](https://arxiv.org/abs/2108.11204)

    提出了子目标搜索（kSubS）方法，通过学习的子目标生成器产生多样性的子目标，减少搜索空间并在Sokoban、魔方和不等式证明三个领域取得了强大的结果。

    

    人类擅长通过从一个想法移动到相关的想法的思维过程来解决复杂的推理任务。受此启发，我们提出了子目标搜索（kSubS）方法。其关键组件是一个学习的子目标生成器，产生多样性的既可实现又接近解决方案的子目标。使用子目标可以减少搜索空间，并引入适合高效规划的高级搜索图。本文中，我们使用基于Transformer的子目标模块结合经典的最佳优先搜索框架来实现kSubS。我们展示了一种简单的生成第$k$步子目标的方法在三个具有挑战性的领域上表现出惊人的效率：两个流行的益智游戏Sokoban和魔方以及不等式证明基准INT。kSubS在适度的计算预算内取得了强大的结果，包括在INT上的最新成果。

    arXiv:2108.11204v3 Announce Type: replace  Abstract: Humans excel in solving complex reasoning tasks through a mental process of moving from one idea to a related one. Inspired by this, we propose Subgoal Search (kSubS) method. Its key component is a learned subgoal generator that produces a diversity of subgoals that are both achievable and closer to the solution. Using subgoals reduces the search space and induces a high-level search graph suitable for efficient planning. In this paper, we implement kSubS using a transformer-based subgoal module coupled with the classical best-first search framework. We show that a simple approach of generating $k$-th step ahead subgoals is surprisingly efficient on three challenging domains: two popular puzzle games, Sokoban and the Rubik's Cube, and an inequality proving benchmark INT. kSubS achieves strong results including state-of-the-art on INT within a modest computational budget.
    
[^155]: MCL-GAN: 具有多个专业鉴别器的生成对抗网络

    MCL-GAN: Generative Adversarial Networks with Multiple Specialized Discriminators

    [https://arxiv.org/abs/2107.07260](https://arxiv.org/abs/2107.07260)

    MCL-GAN提出了一种生成对抗网络框架，利用多个鉴别器共同合作来更有效地表示真实数据集，在解决模式崩溃问题的同时，实现了生成器与基础数据分布的一致性，通过给每个鉴别器专业技能的引导，生成器能够自动找到潜在数据空间和真实数据空间的合理对应关系，同时骨干网络在鉴别器之间是共享的，训练成本增加微不足道，并在标准数据集上证明了算法的有效性。

    

    我们提出了一种具有多个鉴别器的生成对抗网络框架，这些鉴别器共同合作更有效地表示真实数据集。我们的方法有助于学习一个与基于真实图像的基础数据分布一致的生成器，从而缓解了长期的模式崩溃问题。受到多选学习的启发，我们引导每个鉴别器在整个数据的子集中具有专门技能，并允许生成器在无需额外监督训练示例的情况下自动找到潜在数据空间和真实数据空间之间的合理对应关系。尽管使用了多个鉴别器，骨干网络在鉴别器之间是共享的，并且训练成本的增加是微不足道的。我们在标准数据集中使用多个评估指标展示了我们算法的有效性，适用于多样化任务。

    arXiv:2107.07260v3 Announce Type: replace  Abstract: We propose a framework of generative adversarial networks with multiple discriminators, which collaborate to represent a real dataset more effectively. Our approach facilitates learning a generator consistent with the underlying data distribution based on real images and thus mitigates the chronic mode collapse problem. From the inspiration of multiple choice learning, we guide each discriminator to have expertise in a subset of the entire data and allow the generator to find reasonable correspondences between the latent and real data spaces automatically without extra supervision for training examples. Despite the use of multiple discriminators, the backbone networks are shared across the discriminators and the increase in training cost is marginal. We demonstrate the effectiveness of our algorithm using multiple evaluation metrics in the standard datasets for diverse tasks.
    
[^156]: 模拟强化学习用于现实世界自动驾驶

    Simulation-based reinforcement learning for real-world autonomous driving

    [https://arxiv.org/abs/1911.12905](https://arxiv.org/abs/1911.12905)

    该论文利用模拟强化学习和合成数据来实现对真实世界自动驾驶系统的控制，成功实现了模拟到真实策略转移，并分析了设计决策对真实世界性能的影响。

    

    我们利用模拟强化学习来获得控制全尺寸真实世界车辆的驾驶系统。驾驶策略以来自单个摄像头的RGB图像及其语义分割作为输入。我们主要使用合成数据，只有在分割网络的训练中才出现标记的真实世界数据。在真实世界实验中，我们确认实现了成功的模拟到真实策略转移。基于广泛的评估，我们分析了关于感知、控制和训练的设计决策如何影响真实世界性能。

    arXiv:1911.12905v4 Announce Type: replace-cross  Abstract: We use reinforcement learning in simulation to obtain a driving system controlling a full-size real-world vehicle. The driving policy takes RGB images from a single camera and their semantic segmentation as input. We use mostly synthetic data, with labelled real-world data appearing only in the training of the segmentation network.   Using reinforcement learning in simulation and synthetic data is motivated by lowering costs and engineering effort.   In real-world experiments we confirm that we achieved successful sim-to-real policy transfer. Based on the extensive evaluation, we analyze how design decisions about perception, control, and training impact the real-world performance.
    
[^157]: 全局动量压缩用于分布式学习中的稀疏通信

    Global Momentum Compression for Sparse Communication in Distributed Learning

    [https://arxiv.org/abs/1905.12948](https://arxiv.org/abs/1905.12948)

    本文提出了一种全局动量压缩（GMC）方法，用于稀疏通信，与现有的局部动量方法不同，GMC利用全局动量来提高分布式学习性能。

    

    随着数据的快速增长，分布式动量随机梯度下降（DMSGD）在分布式学习中得到了广泛应用，特别是用于训练大规模深度模型。由于网络的延迟和带宽有限，通信成为分布式学习的瓶颈。使用稀疏梯度进行通信压缩，简称为“稀疏通信”，已被广泛应用以降低通信成本。所有关于DMSGD中稀疏通信的现有工作都使用本地动量，其中动量仅累积每个工作者在本地计算的随机梯度。在本文中，我们提出了一种新方法，称为\emph{全局动量压缩}（GMC），用于稀疏通信。不同于现有工作中使用的局部动量，GMC使用全局动量。

    arXiv:1905.12948v3 Announce Type: replace-cross  Abstract: With the rapid growth of data, distributed momentum stochastic gradient descent~(DMSGD) has been widely used in distributed learning, especially for training large-scale deep models. Due to the latency and limited bandwidth of the network, communication has become the bottleneck of distributed learning. Communication compression with sparsified gradient, abbreviated as \emph{sparse communication}, has been widely employed to reduce communication cost. All existing works about sparse communication in DMSGD employ local momentum, in which the momentum only accumulates stochastic gradients computed by each worker locally. In this paper, we propose a novel method, called \emph{\underline{g}}lobal \emph{\underline{m}}omentum \emph{\underline{c}}ompression~(GMC), for sparse communication. Different from existing works that utilize local momentum, GMC utilizes global momentum. Furthermore, to enhance the convergence performance when u
    
[^158]: 基于模型的强化学习在Atari中的应用

    Model-Based Reinforcement Learning for Atari

    [https://arxiv.org/abs/1903.00374](https://arxiv.org/abs/1903.00374)

    本研究探索了如何利用视频预测模型实现基于模型的深度RL算法SimPLe，在Atari游戏中比无模型方法更有效地解决问题，并通过实验验证了新颖模型体系结构在这一背景下取得最佳结果。

    

    无模型的强化学习（RL）可以用于从图像观察中学习有效的策略，例如Atari游戏，但通常需要非常大量的交互——实际上，远远超过人类学习相同游戏所需的数量。人们是如何如此快速学习的？答案的一部分可能是人们可以学习游戏运行的方式，并预测哪些动作会产生期望的结果。本文探讨了视频预测模型如何使代理能够在比无模型方法交互更少的情况下解决Atari游戏。我们描述了Simulated Policy Learning（SimPLe），这是一个基于视频预测模型的完整的基于模型的深度RL算法，并对几种模型体系结构进行了比较，包括一个在我们的情境中取得最佳结果的新颖结构。我们的实验评估了SimPLe在100k低数据条件下的一系列Atari游戏中的表现。

    arXiv:1903.00374v5 Announce Type: replace  Abstract: Model-free reinforcement learning (RL) can be used to learn effective policies for complex tasks, such as Atari games, even from image observations. However, this typically requires very large amounts of interaction -- substantially more, in fact, than a human would need to learn the same games. How can people learn so quickly? Part of the answer may be that people can learn how the game works and predict which actions will lead to desirable outcomes. In this paper, we explore how video prediction models can similarly enable agents to solve Atari games with fewer interactions than model-free methods. We describe Simulated Policy Learning (SimPLe), a complete model-based deep RL algorithm based on video prediction models and present a comparison of several model architectures, including a novel architecture that yields the best results in our setting. Our experiments evaluate SimPLe on a range of Atari games in low data regime of 100k
    
[^159]: MambaByte: 无标记选择性状态空间模型

    MambaByte: Token-free Selective State Space Model. (arXiv:2401.13660v1 [cs.CL])

    [http://arxiv.org/abs/2401.13660](http://arxiv.org/abs/2401.13660)

    MambaByte是一种无标记的选择性状态空间模型，通过在字节级别上进行自回归训练，解决了标准自回归Transformer在处理长序列时的性能问题，并展现了与最先进的子词Transformer相媲美甚至更优的性能，从而证明了MambaByte在无标记语言建模方面的有效性。

    

    无标记语言模型直接从原始字节学习，消除了子词标记化的偏差。然而，操作字节会导致序列长度显著增加，在这种情况下，标准自回归Transformer的扩展性较差。我们尝试了MambaByte，它是基于字节序列自回归训练的无标记适应Mamba状态空间模型。我们的实验表明，与其他字节级模型相比，MambaByte具有计算效率。我们还发现，MambaByte在性能上与甚至胜过最先进的子词Transformer。此外，由于长度的线性扩展，MambaByte在推理过程中获得了快速性能，相比之下，Transformer则没有。我们的研究结果证实了MambaByte在实现无标记语言建模方面的可行性。

    Token-free language models learn directly from raw bytes and remove the bias of subword tokenization. Operating on bytes, however, results in significantly longer sequences, and standard autoregressive Transformers scale poorly in such settings. We experiment with MambaByte, a token-free adaptation of the Mamba state space model, trained autoregressively on byte sequences. Our experiments indicate the computational efficiency of MambaByte compared to other byte-level models. We also find MambaByte to be competitive with and even outperform state-of-the-art subword Transformers. Furthermore, owing to linear scaling in length, MambaByte benefits from fast inference compared to Transformers. Our findings establish the viability of MambaByte in enabling token-free language modeling.
    
[^160]: 大型语言模型的指令指纹识别

    Instructional Fingerprinting of Large Language Models. (arXiv:2401.12255v1 [cs.CR])

    [http://arxiv.org/abs/2401.12255](http://arxiv.org/abs/2401.12255)

    这项研究提出了一种指纹识别大型语言模型的方法，通过轻量级的指令调整，保护知识产权并确保遵守许可条款。实验证明这种方法不影响模型的正常行为，并且具有鲁棒性和高效训练的特点。

    

    从零开始训练大型语言模型（LLM）的巨大成本使得对模型进行指纹识别以保护知识产权成为必要，通过所有权认证并确保下游用户和开发者遵守许可条款（如限制商业使用）。在这项研究中，我们提出了LLM指纹识别的试点研究，作为一种非常轻量级的指令调整形式。模型发布者指定一个机密的私钥，并将其植入为一个指令后门，当密钥存在时，导致LLM生成特定的文本。对11个常用LLMs的结果表明，这种方法轻量级且不影响模型的正常行为。它还可以防止发布者过度宣称，对指纹猜测和参数高效训练保持鲁棒性，并支持类似于MIT许可证的多阶段指纹识别。代码可在https://cnut1648.github.io/Model-Fingerprint/中获得。

    The exorbitant cost of training Large language models (LLMs) from scratch makes it essential to fingerprint the models to protect intellectual property via ownership authentication and to ensure downstream users and developers comply with their license terms (e.g. restricting commercial use). In this study, we present a pilot study on LLM fingerprinting as a form of very lightweight instruction tuning. Model publisher specifies a confidential private key and implants it as an instruction backdoor that causes the LLM to generate specific text when the key is present. Results on 11 popularly-used LLMs showed that this approach is lightweight and does not affect the normal behavior of the model. It also prevents publisher overclaim, maintains robustness against fingerprint guessing and parameter-efficient training, and supports multi-stage fingerprinting akin to MIT License. Code is available in https://cnut1648.github.io/Model-Fingerprint/.
    
[^161]: 通过通用概念发现理解视频Transformer

    Understanding Video Transformers via Universal Concept Discovery. (arXiv:2401.10831v1 [cs.CV])

    [http://arxiv.org/abs/2401.10831](http://arxiv.org/abs/2401.10831)

    本文研究了视频Transformer的可解释性问题，引入了视频Transformer概念发现算法来解释其决策过程，并揭示了时空推理机制和对象为中心的表示。

    

    本文研究了基于概念的视频Transformer表示的可解释性问题。具体而言，我们试图解释基于自动发现的高层时空概念的视频Transformer的决策过程。以往关于基于概念的可解释性的研究仅集中在图像级任务上。相比之下，视频模型处理了额外的时间维度，增加了复杂性，并在识别动态概念方面面临挑战。在这项工作中，我们通过引入第一个视频Transformer概念发现(VTCD)算法系统地解决了这些挑战。为此，我们提出了一种有效的无监督方法，用于识别视频Transformer表示的单元（概念）并对其对模型输出的重要性进行排名。得到的概念具有很强的可解释性，揭示了视频中的时空推理机制和以对象为中心的表示。

    This paper studies the problem of concept-based interpretability of transformer representations for videos. Concretely, we seek to explain the decision-making process of video transformers based on high-level, spatiotemporal concepts that are automatically discovered. Prior research on concept-based interpretability has concentrated solely on image-level tasks. Comparatively, video models deal with the added temporal dimension, increasing complexity and posing challenges in identifying dynamic concepts over time. In this work, we systematically address these challenges by introducing the first Video Transformer Concept Discovery (VTCD) algorithm. To this end, we propose an efficient approach for unsupervised identification of units of video transformer representations - concepts, and ranking their importance to the output of a model. The resulting concepts are highly interpretable, revealing spatio-temporal reasoning mechanisms and object-centric representations in unstructured video m
    
[^162]: 通过生成对抗网络推进先验可解释模型

    Advancing Ante-Hoc Explainable Models through Generative Adversarial Networks. (arXiv:2401.04647v1 [cs.CV])

    [http://arxiv.org/abs/2401.04647](http://arxiv.org/abs/2401.04647)

    本文提出了一种先验可解释模型，通过在主分类器网络中添加无监督的解释生成器和对抗训练的方式，实现了模型的可解释性和性能的提升。该方法通过训练解释模块提取视觉概念，同时使用生成对抗网络模块来区分生成的图像和真实图像。实验证明了该方法的鲁棒性，并展示了学到的概念与对象部分和视觉属性的语义一致性。

    

    本文提出了一种新的概念学习框架，用于增强视觉分类任务中模型的可解释性和性能。我们的方法将一个无监督的解释生成器添加到主分类器网络中，并利用对抗训练。在训练过程中，解释模块被优化以从分类器的潜在表示中提取视觉概念，而基于生成对抗网络的模块则旨在区分从概念中生成的图像和真实图像。这种联合训练方案使得模型能够将其内部学习到的概念与人可解释的视觉属性隐式地对齐。全面的实验证明了我们方法的鲁棒性，同时产生了连贯的概念激活。我们分析了学到的概念，展示了它们与对象部分和视觉属性之间的语义一致性。我们还研究了对抗训练协议中的扰动对分类和概念获取的影响。总之，本文通过生成对抗网络推进了先验可解释模型。

    This paper presents a novel concept learning framework for enhancing model interpretability and performance in visual classification tasks. Our approach appends an unsupervised explanation generator to the primary classifier network and makes use of adversarial training. During training, the explanation module is optimized to extract visual concepts from the classifier's latent representations, while the GAN-based module aims to discriminate images generated from concepts, from true images. This joint training scheme enables the model to implicitly align its internally learned concepts with human-interpretable visual properties. Comprehensive experiments demonstrate the robustness of our approach, while producing coherent concept activations. We analyse the learned concepts, showing their semantic concordance with object parts and visual attributes. We also study how perturbations in the adversarial training protocol impact both classification and concept acquisition. In summary, this 
    
[^163]: 追踪任何物体的非现态方法

    Tracking Any Object Amodally. (arXiv:2312.12433v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.12433](http://arxiv.org/abs/2312.12433)

    本论文介绍了一种追踪任何物体的非现态方法，利用数据增强和微调现态跟踪器，可以提高追踪的效果。

    

    非现态感知是一种从部分可见性中理解完整物体结构的基本技能，它对于婴儿甚至是成人都非常重要。它的重要性延伸到了自动驾驶等应用领域，对于理解重叠物体至关重要。然而，现代的检测和跟踪算法通常忽视了这一关键能力，可能是因为大多数数据集中普遍使用的是现态标注。为了解决非现态数据的匮乏问题，我们引入了TAO-Amodal基准，其中包含数千个视频序列中的880个多样化的物体类别。我们的数据集包括可见和遮挡对象的非现态和现态边界框，包括部分超出画面范围的物体。为了增强非现态追踪的目标永久性，我们利用了一个轻量级的插件模块，即非现态扩展器，通过对几百个视频序列进行数据增强的微调，将标准的现态跟踪器转化为非现态跟踪器。我们取得了3.3％和1.6％的改进效果。

    Amodal perception, the ability to comprehend complete object structures from partial visibility, is a fundamental skill, even for infants. Its significance extends to applications like autonomous driving, where a clear understanding of heavily occluded objects is essential. However, modern detection and tracking algorithms often overlook this critical capability, perhaps due to the prevalence of modal annotations in most datasets. To address the scarcity of amodal data, we introduce the TAO-Amodal benchmark, featuring 880 diverse categories in thousands of video sequences. Our dataset includes amodal and modal bounding boxes for visible and occluded objects, including objects that are partially out-of-frame. To enhance amodal tracking with object permanence, we leverage a lightweight plug-in module, the amodal expander, to transform standard, modal trackers into amodal ones through fine-tuning on a few hundred video sequences with data augmentation. We achieve a 3.3\% and 1.6\% improve
    
[^164]: 不同的令牌指标：通过测量衰减来修剪LLM组件并优化量化

    Divergent Token Metrics: Measuring degradation to prune away LLM components -- and optimize quantization. (arXiv:2311.01544v1 [cs.CL])

    [http://arxiv.org/abs/2311.01544](http://arxiv.org/abs/2311.01544)

    本研究引入了一种新的方法，即不同的令牌指标（DTM），用于评估压缩后的大型语言模型（LLM）。通过关注令牌的差异性，DTM提供了对模型压缩微妙之处的深入洞察，并且在不损害文本生成质量的情况下可以实现显著的精确度和稀疏度水平。该研究还提出了一种利用DTM进行模型稀疏化和量化的方法，并发现可以修剪掉超过90%的LLM组件和量化超过80%的参数。

    

    大型语言模型（LLM）以其强大的能力改变了自然语言处理。然而，它们不断增长的大小引发了关于它们的有效部署和LLM压缩的担忧。本研究介绍了一种新的评估压缩LLM的方法，即不同的令牌指标（DTM），解决了传统指标如困惑度无法准确反映文本生成质量的局限性。DTM关注令牌的差异性，提供了对模型压缩微妙之处的更深入洞察。我们的结果表明，在不损害文本生成质量的情况下，可以达到显著的精确度和稀疏度水平。此外，DTM还可以更精确地评估每个组件的影响。利用第一个不同的令牌指标（FDTM）在模型稀疏化中显示，超过90%的所有组件可以修剪掉。对于量化，FDTM表明超过80%的参数可以进行量化。

    Large Language Models (LLMs) have reshaped natural language processing with their impressive capabilities. Their ever-increasing size, however, raised concerns about their effective deployment and the need for LLM compressions. This study introduces the Divergent Token metrics (DTMs), a novel approach for assessing compressed LLMs, addressing the limitations of traditional measures like perplexity that fail to accurately reflect text generation quality. DTMs focus on token divergence, providing deeper insights into the subtleties of model compression. Our results indicate that significant levels of precision and sparsity can be achieved without compromising text generation quality. Moreover, DTMs offers a more precise evaluation of each component's impact individually. Utilizing the First Divergent Token metric (FDTM) in model sparsification reveals that nearly 20% of all components can be pruned over 90%. In terms of quantization, the FDTM suggests that over 80% of parameters can be s
    
[^165]: 潜在空间中的多个数学运算推导

    Multi-Operational Mathematical Derivations in Latent Space. (arXiv:2311.01230v1 [cs.LG])

    [http://arxiv.org/abs/2311.01230](http://arxiv.org/abs/2311.01230)

    本文研究在潜在空间中逼近多个数学运算进行表达式推导的可能性，并通过构建大规模数据集和使用最先进的神经编码器实例化，探索了不同编码机制在潜在空间中逼近方程推理的能力。

    

    本文研究在潜在空间中逼近多个数学运算进行表达式推导的可能性。为此，我们引入了不同的多操作表示范式，将数学运算建模为显式的几何变换。通过利用符号引擎，我们构建了一个包含61K个前提和6个运算符的大规模数据集，分析了每个范式在与最先进的神经编码器实例化时的性质。具体而言，我们研究了不同的编码机制在潜在空间中如何逼近方程推理，并探讨了学习不同运算符和在单个运算中专门化之间的权衡，以及支持多步推导和超越分布广义化的能力。我们的实证分析表明，多操作范式对于解开不同运算符是至关重要的，同时可以区分结论。

    This paper investigates the possibility of approximating multiple mathematical operations in latent space for expression derivation. To this end, we introduce different multi-operational representation paradigms, modelling mathematical operations as explicit geometric transformations. By leveraging a symbolic engine, we construct a large-scale dataset comprising 1.7M derivation steps stemming from 61K premises and 6 operators, analysing the properties of each paradigm when instantiated with state-of-the-art neural encoders. Specifically, we investigate how different encoding mechanisms can approximate equational reasoning in latent space, exploring the trade-off between learning different operators and specialising within single operations, as well as the ability to support multi-step derivations and out-of-distribution generalisation. Our empirical analysis reveals that the multi-operational paradigm is crucial for disentangling different operators, while discriminating the conclusion
    
[^166]: 贝叶斯神经控制微分方程用于治疗效果估计

    Bayesian Neural Controlled Differential Equations for Treatment Effect Estimation. (arXiv:2310.17463v1 [cs.LG])

    [http://arxiv.org/abs/2310.17463](http://arxiv.org/abs/2310.17463)

    本文提出了一种新颖的贝叶斯神经控制微分方程方法，用于连续时间的治疗效果估计，该方法能够提供对潜在结果的后验预测分布，并给出了可靠的不确定性估计。

    

    在个性化医学中，连续时间的治疗效果估计非常重要。然而，现有的方法只能给出潜在结果的点估计，忽略了不确定性的估计。毫无疑问，不确定性量化对于可靠的决策是至关重要的。为了填补这一空白，我们提出了一种新颖的贝叶斯神经控制微分方程(BNCDE)用于连续时间的治疗效果估计。在我们的BNCDE中，时间维度通过一组耦合的神经控制微分方程和神经随机微分方程建模，其中神经随机微分方程允许可行的变分贝叶斯推断。因此，对于给定的治疗序列，我们的BNCDE提供了有意义的潜在结果的后验预测分布。据我们所知，我们的方法是第一个专门提供治疗效果不确定性估计的神经方法。

    Treatment effect estimation in continuous time is crucial for personalized medicine. However, existing methods for this task are limited to point estimates of the potential outcomes, whereas uncertainty estimates have been ignored. Needless to say, uncertainty quantification is crucial for reliable decision-making in medical applications. To fill this gap, we propose a novel Bayesian neural controlled differential equation (BNCDE) for treatment effect estimation in continuous time. In our BNCDE, the time dimension is modeled through a coupled system of neural controlled differential equations and neural stochastic differential equations, where the neural stochastic differential equations allow for tractable variational Bayesian inference. Thereby, for an assigned sequence of treatments, our BNCDE provides meaningful posterior predictive distributions of the potential outcomes. To the best of our knowledge, ours is the first tailored neural method to provide uncertainty estimates of tre
    
[^167]: 在样本选择偏差存在的情况下，利用集成多样性进行鲁棒的自训练

    Leveraging Ensemble Diversity for Robust Self-Training in the Presence of Sample Selection Bias. (arXiv:2310.14814v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.14814](http://arxiv.org/abs/2310.14814)

    本文提出了一种在样本选择偏差存在的情况下，利用集成多样性进行鲁棒的自训练的方法，并引入了一种新的自信度度量方法-$\mathcal{T}$-相似度。实验证明该方法在三种不同伪标签策略下具有良好的效果。

    

    自训练是半监督学习中一种众所周知的方法。它包括对模型自信度高的未标记数据进行伪标签分配，并将其视为标记样本进行处理。对于神经网络，通常使用softmax预测概率作为自信度度量，尽管已知它们对错误预测也过于自信。当数据标注受到某种约束时，这种现象尤为明显，即样本选择偏差存在。为了解决这个问题，我们提出了一种新的自信度度量方法，称为$\mathcal{T}$-相似度，它基于线性分类器的集成预测多样性。我们通过研究稳定点并描述单个成员的多样性与其性能之间的关系来提供我们方法的理论分析。我们通过对三种不同伪标签策略的实验验证了我们自信度度量的好处。

    Self-training is a well-known approach for semi-supervised learning. It consists of iteratively assigning pseudo-labels to unlabeled data for which the model is confident and treating them as labeled examples. For neural networks, softmax prediction probabilities are often used as a confidence measure, despite the fact that they are known to be overconfident, even for wrong predictions. This phenomenon is particularly intensified in the presence of sample selection bias, i.e., when data labeling is subject to some constraint. To address this issue, we propose a novel confidence measure, called $\mathcal{T}$-similarity, built upon the prediction diversity of an ensemble of linear classifiers. We provide the theoretical analysis of our approach by studying stationary points and describing the relationship between the diversity of the individual members and their performance. We empirically demonstrate the benefit of our confidence measure for three different pseudo-labeling policies on c
    
[^168]: 适当的Laplacian表示学习

    Proper Laplacian Representation Learning. (arXiv:2310.10833v1 [cs.LG])

    [http://arxiv.org/abs/2310.10833](http://arxiv.org/abs/2310.10833)

    本论文介绍了一种理论上可靠的方法和优化算法，用于近似Laplacian表示学习，以解决大规模强化学习中的探索、泛化和传递问题。

    

    在解决大规模强化学习问题时，学习状态的良好表示对于探索、泛化和传递是至关重要的。Laplacian表示是一种有希望的方法，通过引入内在奖励来解决这些问题，以实现时间延长的动作发现和奖励塑造，以及信息丰富的状态编码。为了获得Laplacian表示，需要计算图Laplacian的特征系统，这通常通过与深度学习方法兼容的优化目标进行近似。然而，这些近似方法依赖于无法高效调整的超参数，收敛到所需特征向量的任意旋转，并且无法精确地恢复相应的特征值。本文提出了一种理论上可靠的目标和相应的优化算法，用于近似Laplacian表示。

    The ability to learn good representations of states is essential for solving large reinforcement learning problems, where exploration, generalization, and transfer are particularly challenging. The Laplacian representation is a promising approach to address these problems by inducing intrinsic rewards for temporally-extended action discovery and reward shaping, and informative state encoding. To obtain the Laplacian representation one needs to compute the eigensystem of the graph Laplacian, which is often approximated through optimization objectives compatible with deep learning approaches. These approximations, however, depend on hyperparameters that are impossible to tune efficiently, converge to arbitrary rotations of the desired eigenvectors, and are unable to accurately recover the corresponding eigenvalues. In this paper we introduce a theoretically sound objective and corresponding optimization algorithm for approximating the Laplacian representation. Our approach naturally reco
    
[^169]: Retro-fallback: 面向不确定世界的逆合成规划

    Retro-fallback: retrosynthetic planning in an uncertain world. (arXiv:2310.09270v1 [cs.AI])

    [http://arxiv.org/abs/2310.09270](http://arxiv.org/abs/2310.09270)

    本文针对逆合成任务在实验室执行可行性的不确定性问题，通过引入随机过程的表述，提出了一种名为 Retro-fallback 的贪婪算法，该算法能够最大化实验室可执行的合成计划的概率。

    

    逆合成是通过提出一系列化学反应从更简单、可购买的分子创建所需分子的任务。虽然先前的研究提出了一些算法来寻找一系列度量指标（例如最短路径、最低成本）的最优解，但这些研究通常忽视了我们对可能反应空间的不完全了解，这意味着算法生成的计划可能在实验室中无法实施。在本文中，我们提出了一种基于随机过程的逆合成新颖表述，以考虑这种不确定性。然后，我们提出了一种新颖的贪婪算法称为 Retro-fallback，最大化至少有一种合成计划能在实验室中执行的概率。使用仿真基准测试，我们证明 Retro-fallback 通常生成比流行的 MCTS 和 retro* 算法更好的一组合成计划。

    Retrosynthesis is the task of proposing a series of chemical reactions to create a desired molecule from simpler, buyable molecules. While previous works have proposed algorithms to find optimal solutions for a range of metrics (e.g. shortest, lowest-cost), these works generally overlook the fact that we have imperfect knowledge of the space of possible reactions, meaning plans created by the algorithm may not work in a laboratory. In this paper we propose a novel formulation of retrosynthesis in terms of stochastic processes to account for this uncertainty. We then propose a novel greedy algorithm called retro-fallback which maximizes the probability that at least one synthesis plan can be executed in the lab. Using in-silico benchmarks we demonstrate that retro-fallback generally produces better sets of synthesis plans than the popular MCTS and retro* algorithms.
    
[^170]: 基于物理信息的图神经网络用于电力系统的动态重构

    Physics-Informed Graph Neural Network for Dynamic Reconfiguration of Power Systems. (arXiv:2310.00728v1 [cs.LG])

    [http://arxiv.org/abs/2310.00728](http://arxiv.org/abs/2310.00728)

    提出了一种基于物理信息的图神经网络（GNN）框架GraPhyR，用于解决电力系统的动态重构（DyR）问题。该框架将运营和连接约束直接融入GNN框架中，并进行端到端的训练，能够有效地优化DyR任务。

    

    为了保持可靠的电网，我们需要快速的决策算法来解决动态重构（DyR）等复杂问题。DyR实时优化配电网开关设置，以最小化电网损耗，并分派资源以满足可用发电量的负载需求。DyR是一个混合整数问题，对于大规模电网和快速时间尺度来说，可能计算难以解决。我们提出了GraPhyR，一种专为DyR而设计的基于物理信息的图神经网络（GNN）框架。我们直接将基本的运营和连接约束融入到GNN框架中，并进行端到端的训练。我们的结果表明，GraPhyR能够学习优化DyR任务。

    To maintain a reliable grid we need fast decision-making algorithms for complex problems like Dynamic Reconfiguration (DyR). DyR optimizes distribution grid switch settings in real-time to minimize grid losses and dispatches resources to supply loads with available generation. DyR is a mixed-integer problem and can be computationally intractable to solve for large grids and at fast timescales. We propose GraPhyR, a Physics-Informed Graph Neural Network (GNNs) framework tailored for DyR. We incorporate essential operational and connectivity constraints directly within the GNN framework and train it end-to-end. Our results show that GraPhyR is able to learn to optimize the DyR task.
    
[^171]: 通过机器学习支持的多物理仿真增强多目标优化

    Enhancing Multi-Objective Optimization through Machine Learning-Supported Multiphysics Simulation. (arXiv:2309.13179v1 [cs.LG])

    [http://arxiv.org/abs/2309.13179](http://arxiv.org/abs/2309.13179)

    通过机器学习支持的多物理仿真加速多目标优化，提出了一种框架来逼近和加速复杂的多物理仿真，并在实验中展示了其有效性。

    

    多物理仿真涉及多个耦合物理现象，很快变得计算复杂。这给寻找满足多个目标的最优配置带来了挑战，因为优化算法通常需要多次查询仿真。本文提出了一种方法论框架，通过训练、自优化和自组织代理模型来逼近和加速多物理仿真。我们生成了两个真实世界的表格数据集，并公开提供，展示了代理模型可以在相对少量的数据上准确逼近底层仿真。我们结合四种机器学习和深度学习算法以及两种优化算法和综合评估策略进行了大量实验。最后，通过验证生成的帕累托最优集，评估了我们的结合训练和优化流程的性能。

    Multiphysics simulations that involve multiple coupled physical phenomena quickly become computationally expensive. This imposes challenges for practitioners aiming to find optimal configurations for these problems satisfying multiple objectives, as optimization algorithms often require querying the simulation many times. This paper presents a methodological framework for training, self-optimizing, and self-organizing surrogate models to approximate and speed up Multiphysics simulations. We generate two real-world tabular datasets, which we make publicly available, and show that surrogate models can be trained on relatively small amounts of data to approximate the underlying simulations accurately. We conduct extensive experiments combining four machine learning and deep learning algorithms with two optimization algorithms and a comprehensive evaluation strategy. Finally, we evaluate the performance of our combined training and optimization pipeline by verifying the generated Pareto-op
    
[^172]: 使用虚拟提示注入向指令调整的大型语言模型后门

    Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection. (arXiv:2307.16888v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.16888](http://arxiv.org/abs/2307.16888)

    这项研究介绍了一种针对指令调整的大型语言模型的新型后门攻击方法，即虚拟提示注入（VPI）。通过在特定触发场景下将虚拟提示与用户指令连接，攻击者可以精细操纵模型的回应而无需明确注入。

    

    指令调整的大型语言模型（LLM）表现出了根据人类指令调节其回应的非凡能力。然而，这种调节能力也引入了潜在的攻击者通过植入后门来对模型功能进行精细操纵的可能性。在本文中，我们介绍了一种针对指令调整的LLM定制的新型后门攻击设置-虚拟提示注入（VPI）。在VPI攻击中，期望通过在特定触发场景下将攻击者指定的虚拟提示连接到用户指令中，使植入后门的模型表现得像是在其输入中没有明确的注入。例如，如果LLM被虚拟提示"负面描述乔·拜登"植入后门的触发场景是讨论乔·拜登，那么当谈论乔·拜登时，模型将传播负面倾向的观点。 VPI尤其有害，因为攻击者可以进行细粒度的操纵。

    Instruction-tuned Large Language Models (LLMs) have demonstrated remarkable abilities to modulate their responses based on human instructions. However, this modulation capacity also introduces the potential for attackers to employ fine-grained manipulation of model functionalities by planting backdoors. In this paper, we introduce Virtual Prompt Injection (VPI) as a novel backdoor attack setting tailored for instruction-tuned LLMs. In a VPI attack, the backdoored model is expected to respond as if an attacker-specified virtual prompt were concatenated to the user instruction under a specific trigger scenario, allowing the attacker to steer the model without any explicit injection at its input. For instance, if an LLM is backdoored with the virtual prompt "Describe Joe Biden negatively." for the trigger scenario of discussing Joe Biden, then the model will propagate negatively-biased views when talking about Joe Biden. VPI is especially harmful as the attacker can take fine-grained and 
    
[^173]: 通过原位无模型优化实现高性能真实光学计算

    High-performance real-world optical computing trained by in situ model-free optimization. (arXiv:2307.11957v1 [physics.optics] CROSS LISTED)

    [http://arxiv.org/abs/2307.11957](http://arxiv.org/abs/2307.11957)

    本论文提出了一种无模型优化光学计算系统的方法，通过在原位进行轻量级优化，实现了高性能的真实光学计算。实验证明该方法在分类准确度上优于传统方法，并展示了在高速细胞分析方面的潜力。这种方法的固有简单性和低需求的计算资源促进了光学计算技术从实验室研究向真实世界应用的转变。

    

    光学计算系统可以提供高速和低能耗的数据处理，但在计算密集的训练和从模拟到现实的转换中存在不足。我们提出了一种基于得分梯度估计算法的轻量级原位优化光学计算系统的无模型解决方案。该方法将系统视为黑盒子，直接将损失反向传播到光学权重的概率分布，从而避免了对计算密集和有偏见的系统模拟的需求。通过在单层衍射光学计算系统上进行实验证明在MNIST和FMNIST数据集上具有优越的分类准确度。此外，我们展示了其在无图片和高速细胞分析方面的潜力。我们提出的方法的固有简单性，结合其对计算资源的低需求，加速了光学计算从实验室演示到真实世界应用的过渡。

    Optical computing systems can provide high-speed and low-energy data processing but face deficiencies in computationally demanding training and simulation-to-reality gap. We propose a model-free solution for lightweight in situ optimization of optical computing systems based on the score gradient estimation algorithm. This approach treats the system as a black box and back-propagates loss directly to the optical weights' probabilistic distributions, hence circumventing the need for computation-heavy and biased system simulation. We demonstrate a superior classification accuracy on the MNIST and FMNIST datasets through experiments on a single-layer diffractive optical computing system. Furthermore, we show its potential for image-free and high-speed cell analysis. The inherent simplicity of our proposed method, combined with its low demand for computational resources, expedites the transition of optical computing from laboratory demonstrations to real-world applications.
    
[^174]: 部署机器学习的生态系统级分析揭示了同质化的结果

    Ecosystem-level Analysis of Deployed Machine Learning Reveals Homogeneous Outcomes. (arXiv:2307.05862v1 [cs.LG])

    [http://arxiv.org/abs/2307.05862](http://arxiv.org/abs/2307.05862)

    部署机器学习存在系统性故障, 即使单个模型在总体上的改善也不能解决这个问题

    

    传统上，机器学习通常在模型层面进行研究：研究人员衡量和改进特定模型的准确性、鲁棒性、偏见、效率和其他维度。实际上，机器学习的社会影响取决于机器学习部署的周围环境。为了捕捉这一点，我们引入了生态系统级分析：不是分析单个模型，而是考虑在给定环境中部署的所有模型的集合。例如，在招聘中进行生态系统级分析意味着认识到一个求职者的结果不仅仅取决于单个招聘算法或公司，而是取决于他们申请的所有公司的集体决策。在三种模式（文本、图像、语音）和11个数据集上，我们建立了一个明显的趋势：部署的机器学习容易出现系统性故障，这意味着一些用户被所有可用的模型错误分类。即使在个体模型随时间在总体水平上改善，我们也发现

    Machine learning is traditionally studied at the model level: researchers measure and improve the accuracy, robustness, bias, efficiency, and other dimensions of specific models. In practice, the societal impact of machine learning is determined by the surrounding context of machine learning deployments. To capture this, we introduce ecosystem-level analysis: rather than analyzing a single model, we consider the collection of models that are deployed in a given context. For example, ecosystem-level analysis in hiring recognizes that a job candidate's outcomes are not only determined by a single hiring algorithm or firm but instead by the collective decisions of all the firms they applied to. Across three modalities (text, images, speech) and 11 datasets, we establish a clear trend: deployed machine learning is prone to systemic failure, meaning some users are exclusively misclassified by all models available. Even when individual models improve at the population level over time, we fin
    
[^175]: 将实验数据与观测数据结合的双机器学习方法

    A Double Machine Learning Approach to Combining Experimental and Observational Data. (arXiv:2307.01449v1 [stat.ME])

    [http://arxiv.org/abs/2307.01449](http://arxiv.org/abs/2307.01449)

    这种双机器学习方法将实验和观测研究结合起来，能够测试假设的违反情况并一致估计处理效应。它提供了半参数高效的处理效应估计器。这种方法在实际环境中是可行的。

    

    实验和观测研究通常由于无法测试的假设而缺乏有效性。我们提出了一种双机器学习方法，将实验和观测研究结合起来，使从业人员能够测试假设违反情况并一致估计处理效应。我们的框架在较轻的假设下测试外部效度和可忽视性的违反情况。当只有一个假设被违反时，我们提供半参数高效的处理效应估计器。然而，我们的无免费午餐定理强调了准确识别违反的假设对一致的处理效应估计的必要性。我们通过三个实际案例研究展示了我们方法的适用性，并突出了其在实际环境中的相关性。

    Experimental and observational studies often lack validity due to untestable assumptions. We propose a double machine learning approach to combine experimental and observational studies, allowing practitioners to test for assumption violations and estimate treatment effects consistently. Our framework tests for violations of external validity and ignorability under milder assumptions. When only one assumption is violated, we provide semi-parametrically efficient treatment effect estimators. However, our no-free-lunch theorem highlights the necessity of accurately identifying the violated assumption for consistent treatment effect estimation. We demonstrate the applicability of our approach in three real-world case studies, highlighting its relevance for practical settings.
    
[^176]: 噪声对神经网络的校准和泛化的影响

    Impact of Noise on Calibration and Generalisation of Neural Networks. (arXiv:2306.17630v1 [cs.LG])

    [http://arxiv.org/abs/2306.17630](http://arxiv.org/abs/2306.17630)

    本研究研究了不同类型噪声对神经网络的校准和泛化的影响，发现激活噪声能最有效地提高泛化性能，而输入增强噪声则能显著改善分布外的校准。

    

    噪声注入和数据增强策略对提升神经网络的泛化性能和鲁棒性有效。某些类型的噪声，如标签平滑和MixUp，也被证明能改善校准。由于噪声可以在神经网络的训练的不同阶段添加，这引发了噪声在何时何地最有效的问题。我们研究了各种噪声类型，以确定它们对校准和泛化的改进程度以及在什么条件下起作用。具体而言，我们评估了在分布内（ID）和分布外（OOD）场景中的各种噪声注入策略。研究结果表明，激活噪声对于提高泛化性能是最具传递性和有效性的，而输入增强噪声在改善分布外校准上很显著，但不一定适用于分布内数据。

    Noise injection and data augmentation strategies have been effective for enhancing the generalisation and robustness of neural networks (NNs). Certain types of noise such as label smoothing and MixUp have also been shown to improve calibration. Since noise can be added in various stages of the NN's training, it motivates the question of when and where the noise is the most effective. We study a variety of noise types to determine how much they improve calibration and generalisation, and under what conditions. More specifically we evaluate various noise-injection strategies in both in-distribution (ID) and out-of-distribution (OOD) scenarios. The findings highlight that activation noise was the most transferable and effective in improving generalisation, while input augmentation noise was prominent in improving calibration on OOD but not necessarily ID data.
    
[^177]: MeciFace：基于肌肉电和惯性融合的边缘实时识别面部表情和进食活动眼镜

    MeciFace: Mechanomyography and Inertial Fusion based Glasses for Edge Real-Time Recognition of Facial and Eating Activities. (arXiv:2306.13674v1 [cs.CV])

    [http://arxiv.org/abs/2306.13674](http://arxiv.org/abs/2306.13674)

    MeciFace是一款注重隐私且低功耗的可穿戴设备，它采用轻量级卷积神经网络来监测面部表情和进食活动，面部表情案例的F1分数达到了86％，饮食监测则达到了90％的F1分数。

    

    我们提出了MeciFace，这是一个低功耗（0.55瓦），注重隐私，实时边缘监测（RTE）的可穿戴解决方案，具有微小的内存占用（11-19 KB），旨在监测面部表情和进食活动。我们采用轻量级卷积神经网络作为面部和进食场景的主干模型。该系统在面部表情案例的RTE评估中产生了86％的F1分数。此外，我们对未知用户的RTE进行饮食监测，得到了90％的F1分数。

    We present MeciFace, a low-power (0.55 Watts), privacy-conscious, real-time on-the-edge (RTE) wearable solution with a tiny memory footprint (11-19 KB), designed to monitor facial expressions and eating activities. We employ lightweight convolutional neural networks as the backbone models for both facial and eating scenarios. The system yielded an F1-score of 86% for the RTE evaluation in the facial expression case. In addition, we obtained an F1-score of 90% for eating/drinking monitoring for the RTE of an unseen user.
    
[^178]: 在Transformer语言模型中解决关系任务的机制

    A Mechanism for Solving Relational Tasks in Transformer Language Models. (arXiv:2305.16130v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.16130](http://arxiv.org/abs/2305.16130)

    这篇论文研究了在Transformer语言模型中解决关系任务的机制，并发现这些模型利用简单的线性更新来处理关系任务，并以内容无关的方式促进关系的输出。

    

    这篇论文提供了证据表明，尽管语言模型（LMs）的规模和复杂性，它们有时候利用一个简单的计算机制来解决一对一的关系任务（例如 capital_of(Poland)=Warsaw）。我们在上下文学习环境中研究了一系列语言模型的大小（从124M参数到176B参数），并发现对于多种任务（涉及首都、大写和过去时态等），机制的关键部分可以简化为前馈（FFN）网络通常应用的简单线性更新。这些更新也倾向于以内容无关的方式促进关系的输出（例如对编码 Poland:Warsaw::China:Beijing），揭示了这些模型在解决这些任务中的可预测模式。我们进一步显示这个机制是特定于需要从预训练存储器中检索而不是从局部上下文检索的任务。我们的结果为解决关系任务的语言模型的机制做出了贡献。

    A primary criticism towards language models (LMs) is their inscrutability. This paper presents evidence that, despite their size and complexity, LMs sometimes exploit a simple computational mechanism to solve one-to-one relational tasks (e.g., capital_of(Poland)=Warsaw). We investigate a range of language model sizes (from 124M parameters to 176B parameters) in an in-context learning setting, and find that for a variety of tasks (involving capital cities, upper-casing, and past-tensing) a key part of the mechanism reduces to a simple linear update typically applied by the feedforward (FFN) networks. These updates also tend to promote the output of the relation in a content-independent way (e.g., encoding Poland:Warsaw::China:Beijing), revealing a predictable pattern that these models take in solving these tasks. We further show that this mechanism is specific to tasks that require retrieval from pretraining memory, rather than retrieval from local context. Our results contribute to a g
    
[^179]: 面向带有长期约束的随机网络资源分配的在线优化问题

    Online Optimization for Randomized Network Resource Allocation with Long-Term Constraints. (arXiv:2305.15558v1 [math.OC])

    [http://arxiv.org/abs/2305.15558](http://arxiv.org/abs/2305.15558)

    本文研究了一个面向随机网络资源分配的在线问题，并提出了一种近乎最优的解决方案，该方案在数字模拟中表现优异。

    

    本论文研究了一个简单通信网络中的在线资源预留问题。网络由两个计算节点组成，通过本地通信链路连接。系统在离散时间内运行；在每个时间段，管理员会在实际作业请求之前为服务器预留资源，这些预留会产生成本。然后，在观察到客户端请求之后，作业可能会从一个服务器转移到另一个服务器，以最好地适应需求，但这会产生额外的传输成本。如果无法满足某些作业请求，则会产生违规成本，需要为每个被阻止的作业支付成本。目标是在有限的时间内最小化总预订成本，同时在一定预算限制下维护累积违规和传输成本。为了研究这个问题，我们将其形式化为一个反复博弈问题，针对一系列提议的策略按随机顺序进行预订。然后，我们设计了一种在线算法，该算法可以实现接近最优的性能保证，以期望的总成本为基础，为任何有限的T时间段。数字模拟表明，我们的算法优于几种基线算法。

    In this paper, we study an optimal online resource reservation problem in a simple communication network. The network is composed of two compute nodes linked by a local communication link. The system operates in discrete time; at each time slot, the administrator reserves resources for servers before the actual job requests are known. A cost is incurred for the reservations made. Then, after the client requests are observed, jobs may be transferred from one server to the other to best accommodate the demands by incurring an additional transport cost. If certain job requests cannot be satisfied, there is a violation that engenders a cost to pay for each of the blocked jobs. The goal is to minimize the overall reservation cost over finite horizons while maintaining the cumulative violation and transport costs under a certain budget limit. To study this problem, we first formalize it as a repeated game against nature where the reservations are drawn randomly according to a sequence of pro
    
[^180]: 训练指令作为后门: 大规模语言模型指令调整的后门漏洞

    Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models. (arXiv:2305.14710v1 [cs.CL])

    [http://arxiv.org/abs/2305.14710](http://arxiv.org/abs/2305.14710)

    使用指令调整方法在众包数据集上训练的大规模语言模型，存在后门漏洞，攻击者只需注入极少量的恶意指令便可永久控制模型行为，且难以被修复，需要更加健全的防御机制。

    

    本文研究使用指令调整方法在众包数据集上训练的模型，其目的是达到更好的性能表现。然而，我们提出了一个与该培训范例相关的安全问题。研究表明，攻击者只需在成千上万的数据中注入极少量的恶意指令，便可以通过数据毒化来控制模型行为，甚至无需修改数据实例或标签本身。通过这种指令攻击，攻击者可以在四个常用的 NLP 数据集上实现超过90% 的攻击成功率，并引起易于转移到 15 种不同数据集的持久后门。这种攻击还可以直接应用于多个数据集的有毒指令。最后，该攻击显示出对现有推理时防御的抵抗力。这些发现凸显了在语言模型训练中需要更为健全的防御机制。

    Instruction-tuned models are trained on crowdsourcing datasets with task instructions to achieve superior performance. However, in this work we raise security concerns about this training paradigm. Our studies demonstrate that an attacker can inject backdoors by issuing very few malicious instructions among thousands of gathered data and control model behavior through data poisoning, without even the need of modifying data instances or labels themselves. Through such instruction attacks, the attacker can achieve over 90% attack success rate across four commonly used NLP datasets, and cause persistent backdoors that are easily transferred to 15 diverse datasets zero-shot. In this way, the attacker can directly apply poisoned instructions designed for one dataset on many other datasets. Moreover, the poisoned model cannot be cured by continual learning. Lastly, instruction attacks show resistance to existing inference-time defense. These findings highlight the need for more robust defens
    
[^181]: 动态图表示学习中带有边时序状态的循环Transformer

    Recurrent Transformer for Dynamic Graph Representation Learning with Edge Temporal States. (arXiv:2304.10079v1 [cs.LG])

    [http://arxiv.org/abs/2304.10079](http://arxiv.org/abs/2304.10079)

    本文提出了循环差分图变换器框架，旨在解决动态图表示学习中未能明确建模边时序状态和提取全局结构特征的问题。

    

    随着现实世界中对图数据分析的广泛需求，动态图表示学习正成为一项趋势性而具有挑战性的研究任务。尽管许多最近的研究基于循环神经网络（RNNs）和图神经网络（GNNs）展现了令人鼓舞的表现，但它们未能明确地对节点特征随时间片段的边时序状态产生影响进行建模。此外，由于GNNs的内在over-smoothing缺陷，它们很难提取全局结构特征，进一步限制了性能。在本文中，我们提出了一个循环差分图变换器（RDGT）框架，该框架首先为每个快照中的边分配了各种类型和权重，以明确地说明它们的特定时间状态，然后采用增强结构的图变换器来通过循环学习范式捕获时间节点表示。在四个真实的数据集上进行的实验结果表明

    Dynamic graph representation learning is growing as a trending yet challenging research task owing to the widespread demand for graph data analysis in real world applications. Despite the encouraging performance of many recent works that build upon recurrent neural networks (RNNs) and graph neural networks (GNNs), they fail to explicitly model the impact of edge temporal states on node features over time slices. Additionally, they are challenging to extract global structural features because of the inherent over-smoothing disadvantage of GNNs, which further restricts the performance. In this paper, we propose a recurrent difference graph transformer (RDGT) framework, which firstly assigns the edges in each snapshot with various types and weights to illustrate their specific temporal states explicitly, then a structure-reinforced graph transformer is employed to capture the temporal node representations by a recurrent learning paradigm. Experimental results on four real-world datasets d
    
[^182]: 从孤立的岛屿到泛大陆：统一语义空间用于人类行为理解

    From Isolated Islands to Pangea: Unifying Semantic Space for Human Action Understanding. (arXiv:2304.00553v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2304.00553](http://arxiv.org/abs/2304.00553)

    本文提出了一个统一的语义空间Poincare行为语义空间，通过将以前数据集的类别与这个语义空间对齐，收集（图像/视频/骨架/MoCap）数据集到一个统一的数据库中，即将“孤立的岛屿”桥接成一个“泛大陆”，这将有助于推进可推广的行为学习。

    

    行为理解是一项重要的研究领域并且备受关注。它可以被理解为从行为的物理空间到语义空间的映射。通常，研究人员会根据独特的选择构建行为数据集，以定义各种类别并将基准线推向极限。因此，数据集之间存在语义差距和不同的类别粒度，就像“孤立的岛屿”一样互不兼容，例如数据集A中的家务和数据集B中的洗盘子。我们认为需要一个更具原则性的语义空间来集中社区的力量，并使我们能够一起使用所有数据集以追求可推广的行为学习。为此，我们设计了一个Poincare行为语义空间，给定动词分类层次结构并涵盖大量行为。通过将以前数据集的类别与我们的语义空间对齐，我们将（图像/视频/骨架/MoCap）数据集收集到一个统一的数据库中，使用统一的标签系统，即将“孤立的岛屿”桥接成一个“泛大陆”。因此，我们对这个统一的数据库进行了广泛的实验，结果证明了我们提出的语义空间和统一数据库的有效性。

    Action understanding matters and attracts attention. It can be formed as the mapping from the action physical space to the semantic space. Typically, researchers built action datasets according to idiosyncratic choices to define classes and push the envelope of benchmarks respectively. Thus, datasets are incompatible with each other like "Isolated Islands" due to semantic gaps and various class granularities, e.g., do housework in dataset A and wash plate in dataset B. We argue that a more principled semantic space is an urgent need to concentrate the community efforts and enable us to use all datasets together to pursue generalizable action learning. To this end, we design a Poincare action semantic space given verb taxonomy hierarchy and covering massive actions. By aligning the classes of previous datasets to our semantic space, we gather (image/video/skeleton/MoCap) datasets into a unified database in a unified label system, i.e., bridging "isolated islands" into a "Pangea". Accord
    
[^183]: 通过深度度量学习方法结合实现航空场景分类

    Combining Deep Metric Learning Approaches for Aerial Scene Classification. (arXiv:2303.11389v1 [cs.CV])

    [http://arxiv.org/abs/2303.11389](http://arxiv.org/abs/2303.11389)

    本论文提出了六种深度度量学习方法来实现航空场景分类任务，并通过进化计算算法将它们结合起来。实验结果表明，这些方法与传统图像分类方法有类似的精度。

    

    航空场景分类是一项具有挑战性的遥感任务，旨在为一组预定义的类别（例如农业、海滩和港口）对遥感图像进行语义标签。由于数据集图像中所包含的对象具有不同的尺度和方向，因此该任务具有高度的类内变化。在遥感领域中，CNN架构的使用是场景分类任务的替代解决方案。通常，这些CNN被用于执行传统的图像分类任务。然而，另一个不太常用的远程感知图像分类方法可能是使用深度度量学习（DML）方法。因此，本文提出了使用六种DML方法进行航空场景分类任务的方法，并分析它们与四种不同的预训练CNN的行为，通过进化计算算法（UMDA）将它们进行结合。通过实验可观察到，DML方法可以达到与传统图像分类方法相竞争的精度，特别是当它们通过进化计算算法结合时。

    Aerial scene classification, which aims to semantically label remote sensing images in a set of predefined classes (e.g., agricultural, beach, and harbor), is a very challenging task in remote sensing due to high intra-class variability and the different scales and orientations of the objects present in the dataset images. In remote sensing area, the use of CNN architectures as an alternative solution is also a reality for scene classification tasks. Generally, these CNNs are used to perform the traditional image classification task. However, another less used way to classify remote sensing image might be the one that uses deep metric learning (DML) approaches. In this sense, this work proposes to employ six DML approaches for aerial scene classification tasks, analysing their behave with four different pre-trained CNNs as well as combining them through the use of evolutionary computation algorithm (UMDA). In performed experiments, it is possible to observe than DML approaches can achi
    
[^184]: 解决竞争性多智能体决策和控制问题的主动学习方法

    An active learning method for solving competitive multi-agent decision-making and control problems. (arXiv:2212.12561v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2212.12561](http://arxiv.org/abs/2212.12561)

    我们提出了一个基于主动学习的方法，用于解决竞争性多智能体决策和控制问题。通过重构私有策略和预测稳态行动配置文件，外部观察者可以成功进行预测和优化策略。

    

    我们提出了一种基于主动学习的方案，用于重构由相互作用代理人群体执行的私有策略，并预测底层多智能体交互过程的确切结果，这里被认为是一个稳定的行动配置文件。我们设想了一个场景，在这个场景中，一个具有学习程序的外部观察者可以通过私有的行动-反应映射进行查询和观察代理人的反应，集体的不动点对应于一个稳态配置文件。通过迭代地收集有意义的数据和更新行动-反应映射的参数估计，我们建立了评估所提出的主动学习方法的渐近性质的充分条件，以便如果收敛发生，它只能朝向一个稳态行动配置文件。这一事实导致了两个主要结果：i）学习局部精确的行动-反应映射替代物使得外部观察者能够成功完成其预测任务，ii）与代理人的互动提供了一种方法来优化策略以达到最佳效果。

    We propose a scheme based on active learning to reconstruct private strategies executed by a population of interacting agents and predict an exact outcome of the underlying multi-agent interaction process, here identified as a stationary action profile. We envision a scenario where an external observer, endowed with a learning procedure, can make queries and observe the agents' reactions through private action-reaction mappings, whose collective fixed point corresponds to a stationary profile. By iteratively collecting sensible data and updating parametric estimates of the action-reaction mappings, we establish sufficient conditions to assess the asymptotic properties of the proposed active learning methodology so that, if convergence happens, it can only be towards a stationary action profile. This fact yields two main consequences: i) learning locally-exact surrogates of the action-reaction mappings allows the external observer to succeed in its prediction task, and ii) working with 
    
[^185]: Shapley曲线：一种平滑视角

    Shapley Curves: A Smoothing Perspective. (arXiv:2211.13289v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.13289](http://arxiv.org/abs/2211.13289)

    本文以平滑的角度引入了Shapley曲线作为局部变量重要性的度量，提出了两种估计策略，并在特征的独立和依赖情况下得到了一致性和渐近正态性，为估计的Shapley曲线构建了置信区间并进行了推断，通过实验证实了渐近结果。应用中分析了哪些属性驱动车辆价格。

    

    源自合作博弈理论，Shapley值已成为应用机器学习中最广泛使用的变量重要性度量之一。然而，对Shapley值的统计理解仍然有限。本文以非参数(或平滑)的角度，引入Shapley曲线作为局部变量重要性的度量。我们提出了两种估计策略，并在特征独立和依赖的情况下都得出了一致性和渐近正态性。这样，我们可以构建置信区间并对估计的Shapley曲线进行推断。我们提出了一种新颖的野蛮引导程序版本，专门调整以获得Shapley曲线的良好有限样本覆盖。渐近结果在大量实验证实了。在实证应用中，我们分析了哪些属性驱动了车辆的价格。

    Originating from cooperative game theory, Shapley values have become one of the most widely used measures for variable importance in applied Machine Learning. However, the statistical understanding of Shapley values is still limited. In this paper, we take a nonparametric (or smoothing) perspective by introducing Shapley curves as a local measure of variable importance. We propose two estimation strategies and derive the consistency and asymptotic normality both under independence and dependence among the features. This allows us to construct confidence intervals and conduct inference on the estimated Shapley curves. We propose a novel version of the wild bootstrap procedure, specifically adjusted to give good finite sample coverage of the Shapley curves. The asymptotic results are validated in extensive experiments. In an empirical application, we analyze which attributes drive the prices of vehicles.
    
[^186]: DriftRec: 将扩散模型应用于盲JPEG恢复

    DriftRec: Adapting diffusion models to blind JPEG restoration. (arXiv:2211.06757v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2211.06757](http://arxiv.org/abs/2211.06757)

    DriftRec是一种将扩散模型应用于盲JPEG恢复的方法，通过优雅地修改扩散模型的正向随机微分方程，DriftRec能够在高压缩水平下恢复干净图像的分布，避免生成模糊图像，并且不需要关于损坏操作的先验知识，具有广泛的适用性。

    

    本研究利用扩散模型的高保真生成能力，在高压缩水平下解决盲JPEG恢复问题。我们提出了对扩散模型正向随机微分方程的优雅修改，使其适应此恢复任务，并将我们的方法命名为DriftRec。通过将DriftRec与具有相同网络架构的$L_2$回归基线以及两种最先进的JPEG恢复技术进行比较，我们表明我们的方法可以避免其他方法生成模糊图像的倾向，并显著更加真实地恢复了干净图像的分布。为此，只需要一个干净/损坏图像对的数据集，而无需关于损坏操作的任何知识，使得它在其他恢复任务中具有更广泛的适用性。与其他有条件和无条件的扩散模型不同，我们利用了干净图像和损坏图像的分布彼此更接近的观念。

    In this work, we utilize the high-fidelity generation abilities of diffusion models to solve blind JPEG restoration at high compression levels. We propose an elegant modification of the forward stochastic differential equation of diffusion models to adapt them to this restoration task and name our method DriftRec. Comparing DriftRec against an $L_2$ regression baseline with the same network architecture and two state-of-the-art techniques for JPEG restoration, we show that our approach can escape the tendency of other methods to generate blurry images, and recovers the distribution of clean images significantly more faithfully. For this, only a dataset of clean/corrupted image pairs and no knowledge about the corruption operation is required, enabling wider applicability to other restoration tasks. In contrast to other conditional and unconditional diffusion models, we utilize the idea that the distributions of clean and corrupted images are much closer to each other than each is to th
    
[^187]: 基于深度强化学习的自适应大邻域搜索算法在线控制

    Online Control of Adaptive Large Neighborhood Search using Deep Reinforcement Learning. (arXiv:2211.00759v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.00759](http://arxiv.org/abs/2211.00759)

    本研究提出一种基于深度强化学习的方法来在线控制自适应大邻域搜索算法，该方法能够自适应选择启发式策略、调整参数和控制接受标准，以获得优化问题的良好解，对应用组合优化问题中的实际问题具有重要意义。

    

    自适应大邻域搜索（ALNS）算法在解决复杂的组合优化问题（COPs）方面取得了相当的成功。ALNS在搜索过程中自适应地选择各种启发式策略，利用它们的优势来找到优化问题的良好解。然而，ALNS的有效性取决于其选择和接受参数的正确配置。为了解决这个限制，我们提出了一种基于深度强化学习（DRL）的方法，在搜索过程中选择启发式、调整参数和控制接受标准。所提出的方法旨在基于搜索的状态学习如何配置下一次ALNS迭代以获得好的优化问题解。我们在一个时间依赖的含有随机权重和时间窗口的导航问题上评估了所提出的方法，该问题用于IJCAI竞赛。结果表明，我们的方法优于普通的ALNS和具有默认参数设置的ALNS，展示了DRL方法在在线控制ALNS方面的有效性。

    The Adaptive Large Neighborhood Search (ALNS) algorithm has shown considerable success in solving complex combinatorial optimization problems (COPs). ALNS selects various heuristics adaptively during the search process, leveraging their strengths to find good solutions for optimization problems. However, the effectiveness of ALNS depends on the proper configuration of its selection and acceptance parameters. To address this limitation, we propose a Deep Reinforcement Learning (DRL) approach that selects heuristics, adjusts parameters, and controls the acceptance criteria during the search process. The proposed method aims to learn, based on the state of the search, how to configure the next iteration of the ALNS to obtain good solutions to the underlying optimization problem. We evaluate the proposed method on a time-dependent orienteering problem with stochastic weights and time windows, used in an IJCAI competition. The results show that our approach outperforms vanilla ALNS and ALNS
    
[^188]: 大规模医疗数据记录中的多层次随机优化填补方法

    Multilevel Stochastic Optimization for Imputation in Massive Medical Data Records. (arXiv:2110.09680v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2110.09680](http://arxiv.org/abs/2110.09680)

    本文介绍了一种基于Kriging理论的多层次随机优化填补方法，能够更准确、更快速和更稳定地处理大规模医疗数据记录中的缺失数值数据。

    

    探索和分析大规模数据集最近在研究和发展社区中引起了越来越多的关注。长期以来，人们一直认识到许多数据集中包含大量缺失的数值数据。我们引入了一种基于Kriging理论的数学原则随机优化填补方法，该方法被证明是一种强大的填补方法。然而，其计算成本和潜在的数值不稳定性会导致昂贵和/或不可靠的预测，可能限制其在大规模数据集上的使用。在本文中，我们将最近开发的多层次随机优化方法应用于大规模医疗记录中的填补问题。该方法基于计算应用数学技术，并具有高精度。特别地，对于最佳线性无偏预测器（BLUP），该多层次形式化是精确的，而且计算速度更快，数值稳定性更高。

    Exploration and analysis of massive datasets has recently generated increasing interest in the research and development communities. It has long been a recognized problem that many datasets contain significant levels of missing numerical data. We introduce a mathematically principled stochastic optimization imputation method based on the theory of Kriging. This is shown to be a powerful method for imputation. However, its computational effort and potential numerical instabilities produce costly and/or unreliable predictions, potentially limiting its use on large scale datasets. In this paper, we apply a recently developed multi-level stochastic optimization approach to the problem of imputation in massive medical records. The approach is based on computational applied mathematics techniques and is highly accurate. In particular, for the Best Linear Unbiased Predictor (BLUP) this multi-level formulation is exact, and is also significantly faster and more numerically stable. This permits
    

