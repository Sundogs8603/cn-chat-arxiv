# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Asynchronous Federated Learning with Incentive Mechanism Based on Contract Theory.](http://arxiv.org/abs/2310.06448) | 这项研究提出了一种基于合同理论的异步联邦学习框架，通过使用激励机制来解决联邦学习中的异构性和慢速问题，并且在实验中展示了较高的测试准确率。 |
| [^2] | [Rule Mining for Correcting Classification Models.](http://arxiv.org/abs/2310.06446) | 本研究提出了一种用于修正分类模型的规则挖掘方法，通过挖掘不准确子集和对其进行修正的规则列表，以提高模型的预测准确性。 |
| [^3] | [Skeleton Ground Truth Extraction: Methodology, Annotation Tool and Benchmarks.](http://arxiv.org/abs/2310.06437) | 本文提出了一种基于启发式策略的骨骼地面实况提取方法，通过利用目标的上下文、简单性和完整性线索进行人-数据回路的GT提取，从而解决了现有数据集中骨骼GT缺乏和标准不一致的问题。 |
| [^4] | [Conformal Prediction for Deep Classifier via Label Ranking.](http://arxiv.org/abs/2310.06430) | 符合预测是一个统计框架，能够生成含有真实标签的预测集。本文提出了一种名为SAPS的新算法，通过舍弃除最大softmax概率之外的所有概率值，减少了非一致性评分对概率值的依赖性，达到了生成小尺寸集合和传达不确定性的目的。 |
| [^5] | [TANGO: Time-Reversal Latent GraphODE for Multi-Agent Dynamical Systems.](http://arxiv.org/abs/2310.06427) | 本文提出了TANGO方法，通过时间反演对称性作为归纳偏差，对多智能体动力系统进行学习，即使在非保守的可逆系统中也能保持能量，并且能更高效地学习系统动力学。 |
| [^6] | [Advective Diffusion Transformers for Topological Generalization in Graph Learning.](http://arxiv.org/abs/2310.06417) | 本研究探索了在不同的图拓扑存在下，图扩散方程如何对GNN进行外推和概括，揭示了基于局部扩散的现有模型在概括能力上的不足，并提出了非局部扩散的潜力。 |
| [^7] | [Deep reinforcement learning uncovers processes for separating azeotropic mixtures without prior knowledge.](http://arxiv.org/abs/2310.06415) | 本研究使用深度强化学习方法，在没有先验知识的情况下成功设计了适用于不同化学系统的分离共沸混合物的流程图，并能将超过99％的材料分离为纯组分。 |
| [^8] | [Hexa: Self-Improving for Knowledge-Grounded Dialogue System.](http://arxiv.org/abs/2310.06404) | 本论文提出了一种自我提升的方法，用于改进知识驱动对话生成的中间步骤的生成性能。通过引入引导提示和修改损失函数的自举策略，提高了生成自动生成回答的多样性，并在各种基准数据集上实验证明了该方法的有效性。 |
| [^9] | [Lo-Hi: Practical ML Drug Discovery Benchmark.](http://arxiv.org/abs/2310.06399) | 本研究创建了一个实用的Lo-Hi基准测试，包括前导优化和命中识别两个任务，为药物发现过程提供了一种切实可行的评估方法。对于命中识别任务，研究者设计了一种解决顶点最小k-Cut问题的新型分子拆分算法，并测试了各种机器学习模型的性能。研究还发现现有基准测试不现实且过于乐观。 |
| [^10] | [Adversarial Robustness in Graph Neural Networks: A Hamiltonian Approach.](http://arxiv.org/abs/2310.06396) | 本文通过引入保守哈密顿神经流构建了对抗攻击鲁棒的图神经网络，从而提高了对抗扰动下的鲁棒性。 |
| [^11] | [Harnessing Administrative Data Inventories to Create a Reliable Transnational Reference Database for Crop Type Monitoring.](http://arxiv.org/abs/2310.06393) | 通过利用行政数据清单，本研究创建了一个跨国参考作物类型监测数据库，解决了大规模获取可靠高质量参考数据的问题。 |
| [^12] | [Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations.](http://arxiv.org/abs/2310.06387) | 本文研究了使用少量上下文示范来操纵语言模型对齐能力的方法。通过提供示范，而无需微调，可以增加或降低模型回答恶意提示的概率。我们提出了相同上下文攻击和相同上下文防御方法，用于越狱和对齐语言模型。实验证明了这些方法的有效性。 |
| [^13] | [CAST: Cluster-Aware Self-Training for Tabular Data.](http://arxiv.org/abs/2310.06380) | 本文提出了一种面向表格数据的群集感知自训练方法（CAST），通过规范伪标签的置信度，弥补了自训练算法中的一些弱点，具有普适性和适应性。 |
| [^14] | [Initialization Bias of Fourier Neural Operator: Revisiting the Edge of Chaos.](http://arxiv.org/abs/2310.06379) | 本文研究了Fourier神经操作符(FNO)的初始化偏差，提出了一种FNO版本的He初始化方案，通过模式截断和密集连接网络相似的特点，解决了训练不稳定的负初始化偏差问题。 |
| [^15] | [Leveraging Diffusion-Based Image Variations for Robust Training on Poisoned Data.](http://arxiv.org/abs/2310.06372) | 该文提出了一种利用扩散模型的新方法，通过合成变化并结合知识蒸馏，使模型能够在可能被污染的数据集上进行鲁棒训练，解决后门攻击带来的安全威胁。 |
| [^16] | [Partition-based differentially private synthetic data generation.](http://arxiv.org/abs/2310.06371) | 本论文提出了一种基于分区的差分隐私合成数据生成方法，通过有效减小误差和提高合成数据质量，即使隐私预算有限，该方法仍然具有优势，并可改善质量和实用性。 |
| [^17] | [Geometrically Aligned Transfer Encoder for Inductive Transfer in Regression Tasks.](http://arxiv.org/abs/2310.06369) | 本文提出了一种基于微分几何的转移学习技术，通过将模型的潜在向量映射到黎曼曲面上的局部平坦坐标，实现在回归任务中的知识转移，并展示了该方法的优越性能和稳定行为。 |
| [^18] | [DrugCLIP: Contrastive Protein-Molecule Representation Learning for Virtual Screening.](http://arxiv.org/abs/2310.06367) | DrugCLIP将虚拟筛选转化为对比学习任务，通过使用对比学习来对齐来自大量成对的蛋白质口袋和分子的表示，实现了虚拟筛选的加速。这项新方法不依赖于蛋白质分子的特定结合亲和力评分，而是利用大量数据进行表示学习，并引入了生物学知识启发的数据增强策略。 |
| [^19] | [Core-Intermediate-Peripheral Index: Factor Analysis of Neighborhood and Shortest Paths-based Centrality Metrics.](http://arxiv.org/abs/2310.06358) | 本论文提出了一种名为核心-中间-外围（CIP）指数的新的定量度量指标，通过对邻域和最短路径中心性度量数据进行因子分析，它能够捕捉节点在网络中扮演核心节点和外围节点的程度。作者通过测试在12个复杂现实世界网络上验证了该方法的有效性。 |
| [^20] | [Boosting Continuous Control with Consistency Policy.](http://arxiv.org/abs/2310.06343) | 该论文提出了一种名为一致性策略与Q-Learning （CPQL）的新方法，通过建立从反向扩散轨迹到期望策略的映射，同时解决了基于扩散模型方法的时间效率和准确指导问题。 |
| [^21] | [Federated Learning with Reduced Information Leakage and Computation.](http://arxiv.org/abs/2310.06341) | Upcycled-FL是一种减少信息泄漏和计算的联邦学习框架，在每个偶数迭代中应用一阶近似，使得一半的联邦学习更新不会泄漏信息并且需要更少的计算。 |
| [^22] | [Automatic nodule identification and differentiation in ultrasound videos to facilitate per-nodule examination.](http://arxiv.org/abs/2310.06339) | 本研究针对超声图像中结节异质外观导致的难以进行逐个结节检查的问题，构建了一个基于深度学习的结节重新识别系统，在数百个乳腺超声视频上取得了令人满意的结果。 |
| [^23] | [Learning bounded-degree polytrees with known skeleton.](http://arxiv.org/abs/2310.06333) | 本论文提出了一种高效学习已知骨架的有界度多树的算法，并给出了在多项式时间和样本复杂度内的有限样本保证。这对于复杂概率分布的学习具有重要意义。 |
| [^24] | [Exploit the antenna response consistency to define the alignment criteria for CSI data.](http://arxiv.org/abs/2310.06328) | 本论文提出了一个解决方案，利用天线响应一致性（ARC）来定义适当的对准标准，以解决在WiFi人体活动识别中的自我监督学习算法在CSI数据上无法达到预期性能的问题。 |
| [^25] | [Predicting Three Types of Freezing of Gait Events Using Deep Learning Models.](http://arxiv.org/abs/2310.06322) | 该论文利用深度学习模型预测三种不同类型的冻结步态事件，最佳模型在测试数据上获得了0.427的得分，在Kaggle冻结步态预测竞赛中排名前五。 |
| [^26] | [Transfer learning-based physics-informed convolutional neural network for simulating flow in porous media with time-varying controls.](http://arxiv.org/abs/2310.06319) | 提出了一种基于物理约束的卷积神经网络，用于模拟具有时间变化的井控制的多孔介质中的两相流动。网络通过时间变化控制对解进行参数化，建立了控制到状态的回归。网络结构采用两个平行的U-Net结构，输入为井控制，输出为系统状态。 |
| [^27] | [Adversarial Masked Image Inpainting for Robust Detection of Mpox and Non-Mpox.](http://arxiv.org/abs/2310.06318) | 本研究提出了一种对抗性掩蔽图像修复方法(MIM)，通过修复掩蔽的 Mpox 图像来学习 Mpox 图像表示，并通过测量修复图像与原始图像之间的相似性来判断输入是否属于 Mpox。这种方法有效解决了现有模型受到现实世界噪声干扰、需要多样化的非 Mpox 图像以及无法检测异常输入的问题。 |
| [^28] | [Discovering Mixtures of Structural Causal Models from Time Series Data.](http://arxiv.org/abs/2310.06312) | 这项研究提出了一种从时间序列数据中发现混合结构因果模型的方法，通过推断潜在因果模型以及每个样本属于特定混合成分的概率，通过实验证明了该方法在因果发现任务中的优越性。 |
| [^29] | [Ensemble Active Learning by Contextual Bandits for AI Incubation in Manufacturing.](http://arxiv.org/abs/2310.06306) | 本论文提出了一种面向制造业的AI孵化的上下文强化学习集成活跃学习方法，通过在线更新AI模型的方式来持续改进决策。研究采用了一种名为CBEAL的集成主动学习方法，通过优化地指导数据获取，实现了数据效果的最小化。 |
| [^30] | [Dynamical versus Bayesian Phase Transitions in a Toy Model of Superposition.](http://arxiv.org/abs/2310.06301) | 本研究通过奇异学习理论研究超叠加的玩具模型中的相变，在两个隐藏维度的情况下发现正则的$k$-gons是临界点，并提供支持理论表明这些临界点决定了贝叶斯后验的相变。此外，实验证明这些临界点也决定了SGD训练的行为。研究结果支持了SGD学习轨迹受顺序学习机制影响的猜想。 |
| [^31] | [Gem5Pred: Predictive Approaches For Gem5 Simulation Time.](http://arxiv.org/abs/2310.06290) | 本文提出了Gem5Pred，一种用于预测Gem5模拟时间的方法，并引入了一个独特的数据集。通过分析不同指令类型对模拟时间的影响，利用三种不同的模型进行预测任务，其中回归模型的平均绝对误差为0.546，分类模型的准确率为0.696。这些模型为未来研究提供了基础和参考。 |
| [^32] | [Better and Simpler Lower Bounds for Differentially Private Statistical Estimation.](http://arxiv.org/abs/2310.06289) | 本文提出了更好、更简单的差分隐私统计估计的下界，适用于估计高斯协方差的谱误差和有界$k$阶矩的重尾分布的均值估计。 |
| [^33] | [Suppressing Overestimation in Q-Learning through Adversarial Behaviors.](http://arxiv.org/abs/2310.06286) | 本文提出了一种新的Q学习算法，通过引入虚拟对抗性玩家，有效调节了标准Q学习中的过高估计偏差，提出的算法简单而有效，能够轻松应用于强化学习算法并提高性能。 |
| [^34] | [MuseChat: A Conversational Music Recommendation System for Videos.](http://arxiv.org/abs/2310.06282) | MuseChat是一种创新的对话式音乐推荐系统，通过模拟用户和推荐系统之间的对话交互，利用预训练的音乐标签和艺术家信息，为用户提供定制的音乐推荐，使用户可以个性化选择他们喜欢的音乐。 |
| [^35] | [BC4LLM: Trusted Artificial Intelligence When Blockchain Meets Large Language Models.](http://arxiv.org/abs/2310.06278) | 本论文提出了使用区块链技术增强大型语言模型（LLM）安全性的愿景，以解决AI生成内容的真实性和隐私泄露问题。 |
| [^36] | [Let Models Speak Ciphers: Multiagent Debate through Embeddings.](http://arxiv.org/abs/2310.06272) | 本文引入了一种名为CIPHER的通信机制，通过去除LLMs中的标记采样步骤，让模型可以通过期望的原始Transformer输出嵌入来传达其信念，从而解决了在自然语言生成中可能存在的信息丢失风险，并提供了编码更广泛信息的优势。 |
| [^37] | [Bi-Level Offline Policy Optimization with Limited Exploration.](http://arxiv.org/abs/2310.06268) | 本文提出了一种双层离线策略优化算法，通过模拟策略和值函数之间的层次交互，解决了离线强化学习中数据集缺乏探索所导致的分布偏移问题。该算法通过构建置信区间和最大化保守估计值来提高策略的性能。 |
| [^38] | [CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model.](http://arxiv.org/abs/2310.06266) | CodeFuse-13B是一个预训练的多语言代码大型语言模型，专为代码相关任务设计，支持超过40种编程语言，并通过使用高质量的预训练数据集以及大量实验的验证，展现了其在多语言输入下的有效性。 |
| [^39] | [Self-Discriminative Modeling for Anomalous Graph Detection.](http://arxiv.org/abs/2310.06261) | 本文提出了一种自我区分建模框架用于基于正常图训练的异常图检测，通过生成插值的伪异常图，与几种最先进的基线算法相比，取得了显著的改进。 |
| [^40] | [A Unified View on Solving Objective Mismatch in Model-Based Reinforcement Learning.](http://arxiv.org/abs/2310.06253) | 这项工作提供了一个关于解决模型驱动的强化学习中目标不匹配问题的统一观点，对解决方案进行了分类，并提出了一个分类法以促进未来的研究。 |
| [^41] | [Deep Learning: A Tutorial.](http://arxiv.org/abs/2310.06251) | 该论文讲述了深度学习方法对结构化高维数据的洞察和预测规则，通过使用多层半仿射输入转换来提供预测规则并找到特征。 |
| [^42] | [Sample-Efficient Multi-Agent RL: An Optimization Perspective.](http://arxiv.org/abs/2310.06243) | 该论文研究了多智能体强化学习中的高样本效率问题，在一般函数逼近下，提出了一个基于多智能体解耦系数的算法框架，实现了低复杂度下的学习纳什均衡、粗粒度相关均衡和相关均衡。该算法在亚线性后悔方面表现具有可比性。 |
| [^43] | [A Bayesian framework for discovering interpretable Lagrangian of dynamical systems from data.](http://arxiv.org/abs/2310.06241) | 这项研究提出了一种使用稀疏贝叶斯方法从有限数据中学习可解释的拉格朗日描述物理系统的框架，并通过勒让德变换自动提取哈密顿描述。 |
| [^44] | [Tackling Data Bias in MUSIC-AVQA: Crafting a Balanced Dataset for Unbiased Question-Answering.](http://arxiv.org/abs/2310.06238) | 本论文解决了MUSIC-AVQA中的数据偏差问题，通过创建一个平衡的数据集来保证模型能够有效地推理各种多模态情况下的问题。他们构建了一个名为MUSIC-AVQA v2.0的新数据集，并提出了一个新的基准模型。 |
| [^45] | [Efficient Adaptation of Large Vision Transformer via Adapter Re-Composing.](http://arxiv.org/abs/2310.06234) | 本研究提出了一种名为Adapter重组（ARC）的策略，旨在从一种新的角度解决高效预训练模型适应的问题。该方法通过考虑适应参数的可重用性和引入参数共享方案，利用对称的投影操作来构建共享的瓶颈操作，并通过学习低维度的重新缩放系数来有效重新组合层适应参数。 |
| [^46] | [Low-Rank Tensor Completion via Novel Sparsity-Inducing Regularizers.](http://arxiv.org/abs/2310.06233) | 本研究提出了一种通过新颖的稀疏感应正则化方法实现低秩张量补全的框架。该方法提供了具有封闭形式阈值函数的正则化器，并基于交替方向乘法的算法进行高效计算。实验结果表明，所提出的算法在恢复性能方面优于现有方法。 |
| [^47] | [Exploring adversarial attacks in federated learning for medical imaging.](http://arxiv.org/abs/2310.06227) | 本文通过使用领域专有的MRI肿瘤和病理图像数据集，评估了联合学习网络在医学图像分析中面对对抗性攻击的漏洞。结果发现，领域特定的配置可以显著增加攻击者的成功率，强调了对有效的防御机制迫切需要，并建议重新评估当前联合医学图像分析系统的安全协议。 |
| [^48] | [GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using Large Language Models.](http://arxiv.org/abs/2310.06225) | 本研究全面评估了大型语言模型（LLMs）在回答农业相关问题方面的能力，并使用RAG和ER技术提高了LLMs的性能。其中，GPT-4在农业考试中取得了及格分数以获得认证。 |
| [^49] | [Detecting and Learning Out-of-Distribution Data in the Open world: Algorithm and Theory.](http://arxiv.org/abs/2310.06221) | 该论文在开放世界机器学习中做出了重要贡献，主要包括对未知分布数据的检测和开放世界表示学习。OOD检测专注于识别模型训练分布之外的未知类别的实例，而ORL则扩展了机器学习模型的能力。 |
| [^50] | [SUBP: Soft Uniform Block Pruning for 1xN Sparse CNNs Multithreading Acceleration.](http://arxiv.org/abs/2310.06218) | 该论文提出了一种新的软均匀块剪枝（SUBP）方法，在1xN稀疏CNN中实现了多线程加速，并解决了传统方法中的训练成本昂贵、内存访问开销大、模型质量次优以及线程负载不平衡等问题。 |
| [^51] | [Federated Multi-Level Optimization over Decentralized Networks.](http://arxiv.org/abs/2310.06217) | 本文研究了在网络上的分布式多级优化问题，提出了一种基于流言的分布式多级优化算法，实现了最优的样本复杂度，并在各种应用场景上展现了最先进的性能。 |
| [^52] | [GeoLLM: Extracting Geospatial Knowledge from Large Language Models.](http://arxiv.org/abs/2310.06213) | GeoLLM是一种能够从大型语言模型中提取地理空间知识的新方法，结合来自OpenStreetMap的辅助地图数据，可以有效应用于测量人口密度和经济生计等任务。 |
| [^53] | [Fair Classifiers that Abstain without Harm.](http://arxiv.org/abs/2310.06205) | 提出了一种公正的分类器弃权方法，不造成伤害的同时达到一定程度的群体公平性定义。通过整数规划和代理模型训练的方式实现对训练和测试样本的弃权决策，分析了弃权率与不公平容忍度和准确性约束的关系。 |
| [^54] | [The Importance of Prompt Tuning for Automated Neuron Explanations.](http://arxiv.org/abs/2310.06200) | 本文探讨了在自动神经元解释中即时调优的重要性，通过重新格式化解释提示，我们显著提高了解释质量并降低了计算成本。这项研究对于更深入理解大型语言模型的工作原理和安全性具有重要意义。 |
| [^55] | [PAC-Bayesian Spectrally-Normalized Bounds for Adversarially Robust Generalization.](http://arxiv.org/abs/2310.06182) | 本文提出了一种PAC-Bayesian光谱归一化界限，用于对抗鲁棒深度神经网络的泛化性。与现有界限相比，我们的方法不依赖额外的假设，且更紧密地与标准泛化的界限一致。 |
| [^56] | [Automatic Integration for Spatiotemporal Neural Point Processes.](http://arxiv.org/abs/2310.06179) | 本文提出了一种自动化的时空神经点过程积分方法(AutoSTPP)，扩展了AutoInt方法用于三维时空点过程(STPP)的计算，具有优越性能。 |
| [^57] | [Look-Up mAI GeMM: Increasing AI GeMMs Performance by Nearly 2.5x via msGeMM.](http://arxiv.org/abs/2310.06178) | 这篇论文提出了一种名为msGeMM的新算法，通过使用低精度数据类型，可以使AI模型的性能提高近2.5倍。该算法需要特殊的CUDA核心来实现从小型查找表中添加元素的能力。 |
| [^58] | [DockGame: Cooperative Games for Multimeric Rigid Protein Docking.](http://arxiv.org/abs/2310.06177) | 本文提出了一个名为DockGame的新颖游戏理论框架，用于解决多聚刚性蛋白质对接问题。我们将蛋白质对接视为蛋白质之间的合作游戏，通过学习代理游戏潜力和通过梯度更新计算均衡来预测最终的组装结构。 |
| [^59] | [Memory-Consistent Neural Networks for Imitation Learning.](http://arxiv.org/abs/2310.06171) | 本文介绍了一种内存一致的神经网络模型，在模仿学习中使用专家演示训练策略。该模型通过对输出结果进行硬约束，避免了错误的累积现象，保证了策略效果的上界。 |
| [^60] | [DEUX: Active Exploration for Learning Unsupervised Depth Perception.](http://arxiv.org/abs/2310.06164) | 本研究通过利用3D交互环境，调查数据收集对无监督深度感知学习的影响，发现现有的探索模式不能提供有效的数据，进而提出了一种基于深度不确定性的主动、任务导向的深度完整性学习方法。 |
| [^61] | [Mitigating Simplicity Bias in Deep Learning for Improved OOD Generalization and Robustness.](http://arxiv.org/abs/2310.06161) | 本论文提出了一个框架，鼓励深度模型利用更多样的特征进行预测，以减轻简单性偏差带来的OOD推广问题，并在各种问题和应用中展示了其有效性。 |
| [^62] | [Provably Accelerating Ill-Conditioned Low-rank Estimation via Scaled Gradient Descent, Even with Overparameterization.](http://arxiv.org/abs/2310.06159) | 本章提出了一种名为缩放梯度下降（ScaledGD）的新算法，能够在恒定速率下收敛，而不受低秩对象条件数的影响，并且在各种任务中具有低迭代成本。 |
| [^63] | [Manifold-augmented Eikonal Equations: Geodesic Distances and Flows on Differentiable Manifolds.](http://arxiv.org/abs/2310.06157) | 本研究提出了一种基于模型的参数化方法，利用测地线和流动来描述可微流形上的距离和长度最小化曲线。这为在不同iable流形上进行统计和降阶建模提供了机会。 |
| [^64] | [Latent Diffusion Model for DNA Sequence Generation.](http://arxiv.org/abs/2310.06150) | 提出了一种新的潜在扩散模型DiscDiff，用于离散DNA序列生成。通过将离散DNA序列嵌入到连续的潜在空间中，利用连续扩散模型的强大生成能力来生成离散数据。同时，引入了一种新的度量标准FReD，用于评估DNA序列生成的样本质量。 |
| [^65] | [Understanding Transfer Learning and Gradient-Based Meta-Learning Techniques.](http://arxiv.org/abs/2310.06148) | 本文研究了微调、MAML和Reptile在迁移学习和元学习领域的性能差异，发现当在与训练数据分布不同的任务上进行评估时，仅对预训练网络进行微调的基准线可能比更复杂的元学习技术更有效。 |
| [^66] | [Reinforcement Learning in the Era of LLMs: What is Essential? What is needed? An RL Perspective on RLHF, Prompting, and Beyond.](http://arxiv.org/abs/2310.06147) | 这篇论文介绍了在LLM时代中，强化学习在RLHF、Prompting等方面的应用，并讨论了其中的创新和贡献。 |
| [^67] | [HydraViT: Adaptive Multi-Branch Transformer for Multi-Label Disease Classification from Chest X-ray Images.](http://arxiv.org/abs/2310.06143) | HydraViT是一种将变压器与多分支输出模块相结合的新方法，用于胸部X射线图像的多标签疾病分类，以提高分类性能。 |
| [^68] | [On the Correlation between Random Variables and their Principal Components.](http://arxiv.org/abs/2310.06139) | 本文研究了随机变量与它们的主成分之间的相关性，并找到了描述这种相关性的代数公式。这个公式可以应用于优化主成分分析和因子分析中的成分数目选择。 |
| [^69] | [Layout Sequence Prediction From Noisy Mobile Modality.](http://arxiv.org/abs/2310.06138) | 提出一种名为LTrajDiff的新方法，从噪声移动数据中预测精确的布局序列，克服了由嘈杂数据、不完整轨迹和环境因素导致的视觉障碍。 |
| [^70] | [Learning Layer-wise Equivariances Automatically using Gradients.](http://arxiv.org/abs/2310.06131) | 该论文提出了一种通过梯度自动学习层间等变性的方法，通过改进软等变性的参数化和优化边缘似然来实现层间对称性的自动学习。 |
| [^71] | [On Time Domain Conformer Models for Monaural Speech Separation in Noisy Reverberant Acoustic Environments.](http://arxiv.org/abs/2310.06125) | 本论文提出了一种基于时域Conformer模型的单声道语音分离方法，相较于已有模型，在嘈杂的混响声学环境中实现了更高的分离效果和更高的计算效率。 |
| [^72] | [Factorized Tensor Networks for Multi-Task and Multi-Domain Learning.](http://arxiv.org/abs/2310.06124) | 本文提出了一种分解张量网络（FTN），它可以克服多任务多领域学习中的共享信息利用挑战，并在准确性、存储成本、计算量和样本复杂度等方面实现高效率。实验结果表明，FTN相对于现有方法需要更少的任务特定参数，并且可以适应大量的目标领域和任务。 |
| [^73] | [Exploring Progress in Multivariate Time Series Forecasting: Comprehensive Benchmarking and Heterogeneity Analysis.](http://arxiv.org/abs/2310.06119) | 该论文旨在解决多元时间序列预测领域中公平基准测试和技术方法选择的争议，并提供对该领域取得的进展的深入洞察。 |
| [^74] | [Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models.](http://arxiv.org/abs/2310.06117) | 本文提出了一种简单的提示技术，使得大型语言模型能够通过抽象获得高层概念和基本原理，并将其应用于推理路径中，从而显著提升模型在各种推理密集型任务上的表现。 |
| [^75] | [When is Agnostic Reinforcement Learning Statistically Tractable?.](http://arxiv.org/abs/2310.06113) | 本文研究了基于不可知的PAC强化学习的问题，引入了跨越容量这个新的复杂度度量，并发现了在生成和在线访问模型之间以及在线访问下的确定和随机MDP之间的差异。 |
| [^76] | [Theoretical Analysis of Robust Overfitting for Wide DNNs: An NTK Approach.](http://arxiv.org/abs/2310.06112) | 本文理论分析了宽深度神经网络的鲁棒过拟合现象，并提出了一种名为Adv-NTK的AT算法来增强神经网络的鲁棒性。 |
| [^77] | [BYOC: Personalized Few-Shot Classification with Co-Authored Class Descriptions.](http://arxiv.org/abs/2310.06111) | 提出了一种个性化的少样本文本分类方法，使用合著的类别描述作为提示，用户与LLM交互合作进行标注，形成分类提示，实验结果显示其准确率达到了使用大规模数据集进行训练的模型的82%。 |
| [^78] | [Grokking as the Transition from Lazy to Rich Training Dynamics.](http://arxiv.org/abs/2310.06110) | 研究发现洞察现象可能是由神经网络从懒惰训练动态过渡到丰富的特征学习模式的结果，通过跟踪足够的统计量，发现洞察是在网络首先尝试拟合核回归解决方案后，进行后期特征学习找到通用解决方案之后的结果。 |
| [^79] | [Quantifying Uncertainty in Deep Learning Classification with Noise in Discrete Inputs for Risk-Based Decision Making.](http://arxiv.org/abs/2310.06105) | 本文提出了一种数学框架来量化基于风险决策的深度学习分类中离散输入的预测不确定性。我们的研究有助于在决策过程中减轻相关风险，并且可以适用于处理涉及分类和离散特征变量的问题。 |
| [^80] | [High Dimensional Causal Inference with Variational Backdoor Adjustment.](http://arxiv.org/abs/2310.06100) | 本论文介绍了一种用于高维因果推断的变分背门调整技术，能够处理高维治疗和混杂因素，并成功应用于医疗数据中。 |
| [^81] | [Quantile-based Maximum Likelihood Training for Outlier Detection.](http://arxiv.org/abs/2310.06085) | 本文提出了一种基于分位数的极大似然目标，用于改进异常检测中异常值的分离程度。通过将正则化流拟合到预训练的判别性特征，并根据对数似然度评估来检测异常值。实验结果表明，这种方法优于最先进的无监督模型。 |
| [^82] | [Transformers and Large Language Models for Chemistry and Drug Discovery.](http://arxiv.org/abs/2310.06083) | transformers和大型语言模型在化学和药物发现中的应用取得了显著进展，通过类比自然语言和化学的关系，这些模型能够解决药物发现过程中的重要问题，进一步的发展将带来更多突破和进步。 |
| [^83] | [Ito Diffusion Approximation of Universal Ito Chains for Sampling, Optimization and Boosting.](http://arxiv.org/abs/2310.06081) | 本文研究了一类广泛的马尔可夫链，即Ito链的Ito扩散逼近。与大多数相关论文不同，我们的链具有各向同性和状态相关的噪声，并可以适用于多种应用场景。我们证明了Ito链与对应的随机微分方程之间的W2-距离的上界。这些结果改进了已有的估计方法，并在某些特殊情况下提供了首次的分析。 |
| [^84] | [Performative Time-Series Forecasting.](http://arxiv.org/abs/2310.06077) | 本论文研究了时间序列预测中的展示性问题，提出了一种新的方法（FPS），通过利用延迟响应的概念来解决展示性引起的分布变化，并实现准确的预测。 |
| [^85] | [Pain Forecasting using Self-supervised Learning and Patient Phenotyping: An attempt to prevent Opioid Addiction.](http://arxiv.org/abs/2310.06075) | 本研究旨在通过使用自监督学习方法和患者表型研究，预测患者未来的疼痛轨迹，以帮助患者管理镰状细胞贫血，改善生活质量，并减少对阿片类药物的依赖和副作用。 |
| [^86] | [Optimal Exploration is no harder than Thompson Sampling.](http://arxiv.org/abs/2310.06069) | 这项研究提出了一个问题：是否存在一种能够同时进行最优探索和只需要相同计算操作的算法？ |
| [^87] | [Early Warning via tipping-preserving latent stochastic dynamical system and meta label correcting.](http://arxiv.org/abs/2310.06059) | 提出了一种通过融合真实数据和隐性随机微分方程产生的增广数据的元学习框架，用于提高对早期癫痫发作信号的预测准确率，并通过提取的临界动力学特征来标记噪声数据。 |
| [^88] | [Knowledge Distillation for Anomaly Detection.](http://arxiv.org/abs/2310.06047) | 本文介绍了一种基于知识蒸馏的方法，用于将无监督异常检测模型压缩成可部署的有监督模型，并提出了一些技术来改善检测灵敏度。压缩后的模型在减小大小和内存占用的同时，仍然具有与较大模型相当的性能。 |
| [^89] | [Generative ensemble deep learning severe weather prediction from a deterministic convection-allowing model.](http://arxiv.org/abs/2310.06045) | 本论文开发了一种集成后处理方法，将生成对抗网络（CGANs）和卷积神经网络（CNN）结合起来，对严重天气进行概率预测。该方法在使用HRRR预报作为输入数据，在2021年的测试数据集上相对于其他基于神经网络的方法提高了高达20％的Brier技巧分数（BSS）。 |
| [^90] | [DyST: Towards Dynamic Neural Scene Representations on Real-World Videos.](http://arxiv.org/abs/2310.06020) | DyST模型通过学习动态场景的潜在分解，从实际视频中捕捉到了场景的3D结构和动态特性，并实现了对相机和场景内容的独立控制视图生成。 |
| [^91] | [Divide-and-Conquer Dynamics in AI-Driven Disempowerment.](http://arxiv.org/abs/2310.06009) | 这项研究通过构建游戏理论模型，研究了AI驱动的剥夺中的不团结问题。研究发现，当前受害者需要让未来受害者认识到他们的利益同样面临严重和紧迫的威胁，以激励未来受害者以团结支持当前受害者。 |
| [^92] | [Rethinking Memory and Communication Cost for Efficient Large Language Model Training.](http://arxiv.org/abs/2310.06003) | 本文研究了大型语言模型训练中内存和通信成本对训练速度的影响，并提出了一种平衡内存和通信的优化器（PaRO）。此外，还提出了一种用于大模型训练的分层重叠环通信拓扑结构（HO-Ring），实验证明该算法提高了训练过程中的通信效率。 |
| [^93] | [LCOT: Linear circular optimal transport.](http://arxiv.org/abs/2310.06002) | 本文提出了一种线性循环最优传输（LCOT）方法，用于解决循环概率测度的最优传输问题。LCOT方法具有明确的线性嵌入，可以应用机器学习算法，并在循环最优传输的基础上进行修改。通过理论分析和数值实验，证明了LCOT方法的有效性。 |
| [^94] | [Cost-sensitive probabilistic predictions for support vector machines.](http://arxiv.org/abs/2310.05997) | 提出了一种新方法，将支持向量机转化为成本敏感的概率分类器，并充分利用了正则化参数的信息。 |
| [^95] | [A novel Network Science Algorithm for Improving Triage of Patients.](http://arxiv.org/abs/2310.05996) | 本研究开发了一种使用网络科学算法进行患者分类的新方法。该算法通过对患者数据进行分析，并通过严格的数据预处理和特征工程，实现了高准确率和性能。通过将人工智能引入患者分类，可以提高分类的准确性和效率。 |
| [^96] | [Data Augmentation through Pseudolabels in Automatic Region Based Coronary Artery Segmentation for Disease Diagnosis.](http://arxiv.org/abs/2310.05990) | 这项研究引入了伪标签作为数据增强技术，通过改善基准Yolo模型的性能，提高了冠状动脉分割的效果。 |
| [^97] | [A Dual Latent State Learning Approach: Exploiting Regional Network Similarities for QoS Prediction.](http://arxiv.org/abs/2310.05988) | 本文介绍了一种名为R2SL的基于区域的双潜在状态学习网络，该网络通过汇总数据来捕捉区域网络行为的细微差别，并采用增强的Huber损失函数来提高QoS预测性能。 |
| [^98] | [Analyzing Key Users' behavior trends in Volunteer-Based Networks.](http://arxiv.org/abs/2310.05978) | 本研究分析了志愿者网络中关键用户的行为趋势，开发了两个新算法来揭示关键用户行为模式和预测未来行为。验证结果表明这些算法能有效分析关键用户行为的影响因素。 |
| [^99] | [CFDBench: A Comprehensive Benchmark for Machine Learning Methods in Fluid Dynamics.](http://arxiv.org/abs/2310.05963) | 这个论文介绍了CFDBench，这是一个针对计算流体动力学中四个经典问题的基准测试。它包含了不同边界条件、流体物理特性和域几何的数据，能帮助评估深度学习方法在解决物理问题中的表现。 |
| [^100] | [Improving the Performance of R17 Type-II Codebook with Deep Learning.](http://arxiv.org/abs/2310.05962) | 本论文针对R17 Type-II码本存在的稀疏结构不足问题，提出了利用深度学习来改进码本的两个观点：利用深度学习准确选择主要的角度-时延域端口，并采用深度学习重建下行CSI，有效利用稀疏结构的信息。 |
| [^101] | [Fingerprint Attack: Client De-Anonymization in Federated Learning.](http://arxiv.org/abs/2310.05960) | 本文研究了在联邦学习中，通过梯度指纹攻击可以轻松打破参与者匿名化，并展示了使用差分隐私进行训练可以提供实际防御。 |
| [^102] | [Automating global landslide detection with heterogeneous ensemble deep-learning classification.](http://arxiv.org/abs/2310.05959) | 本研究通过使用全球多样的滑坡数据集，采用不同的分割模型构建了一个集成深度学习模型，以解决滑坡映射中的敏感性问题、过拟合和低映射精度的困扰，并取得了最高的F1得分。 |
| [^103] | [Bayesian Quality-Diversity approaches for constrained optimization problems with mixed continuous, discrete and categorical variables.](http://arxiv.org/abs/2310.05955) | 本论文提出了一种基于贝叶斯优化的新型质量多样性方法，用于解决混合连续、离散和分类变量的约束优化问题，旨在提供具有多样性性质的最优解集。 |
| [^104] | [Optimization of Raman amplifiers: a comparison between black-, grey- and white-box modeling.](http://arxiv.org/abs/2310.05954) | 该论文比较了白盒、灰盒和黑盒模型在双向Raman放大器中实现目标频距放大的能力，并展示了它们都可以实现C波段下达到1dB的频距平坦度。 |
| [^105] | [Classification of Spam URLs Using Machine Learning Approaches.](http://arxiv.org/abs/2310.05953) | 该研究使用机器学习模型对URL进行垃圾和非垃圾的分类，发现bagging模型具有最高的准确率，为96.5%。 |
| [^106] | [Mitigating Denial of Service Attacks in Fog-Based Wireless Sensor Networks Using Machine Learning Techniques.](http://arxiv.org/abs/2310.05952) | 本研究使用机器学习模型来识别无线传感器网络中的拒绝服务攻击。实验结果表明，XGBoost模型在整个数据集上具有更高的真正例率和更低的误报率。 |
| [^107] | [Robust and Efficient Interference Neural Networks for Defending Against Adversarial Attacks in ImageNet.](http://arxiv.org/abs/2310.05947) | 本文构建了一个干扰神经网络来防御ImageNet中的对抗攻击，通过应用额外背景图像和相应标签，并使用预训练的ResNet-152来高效完成训练。与PGD攻击下的最新技术结果相比，它具有更好的防御效果并需要更少的计算资源。 |
| [^108] | [Analysis of Learned Features and Framework for Potato Disease Detection.](http://arxiv.org/abs/2310.05943) | 该论文研究了土豆病害检测中的特征学习和框架，并通过使用区域卷积神经网络和注意力网络来解决数据集变化问题。实验结果表明这些方法在测试集上的平均分类准确率达到了95％，并且在训练阶段未见过的数据集上也有类似的表现，平均得分为84％。 |
| [^109] | [Learning Cyber Defence Tactics from Scratch with Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2310.05939) | 学习网络防御战术的新方法，使用多智能体强化学习来进行自主网络防御，证明了合作多智能体强化学习能够对抗各种威胁。 |
| [^110] | [Vulnerability Clustering and other Machine Learning Applications of Semantic Vulnerability Embeddings.](http://arxiv.org/abs/2310.05935) | 本研究使用自然语言处理技术探索了语义漏洞嵌入的不同类型，并将其用于聚类、分类和可视化等机器学习应用，为计算机安全研究人员和分析师在风险评估等方面提供支持。 |
| [^111] | [Conformal Decision Theory: Safe Autonomous Decisions from Imperfect Predictions.](http://arxiv.org/abs/2310.05921) | 符合决策理论是一种框架，可以通过不完美的机器学习预测产生安全的自主决策。该理论的创新之处在于可以在没有对世界模型做出任何假设的情况下提供具有低风险的统计保证的决策。 |
| [^112] | [NEFTune: Noisy Embeddings Improve Instruction Finetuning.](http://arxiv.org/abs/2310.05914) | NEFTune 是一种改进语言模型微调效果的方法，通过在训练过程中给嵌入向量添加噪声，可以在多个指令数据集上显著提高准确率，并对不同类型的模型都有益处。 |
| [^113] | [M3FPolypSegNet: Segmentation Network with Multi-frequency Feature Fusion for Polyp Localization in Colonoscopy Images.](http://arxiv.org/abs/2310.05538) | M3FPolypSegNet提出了一种新颖的基于频率特征融合的结肠镜图像中息肉定位的分割网络，通过将输入图像分解为不同的频率组件，使用多频编码器映射到高维特征空间，并应用可伸缩注意力强调息肉区域。 |
| [^114] | [ParFam -- Symbolic Regression Based on Continuous Global Optimization.](http://arxiv.org/abs/2310.05537) | ParFam是一种新的符号回归方法，利用参数化的符号函数族将离散问题转化为连续问题，并结合全局优化器，能够有效解决符号回归问题。 |
| [^115] | [RetSeg: Retention-based Colorectal Polyps Segmentation Network.](http://arxiv.org/abs/2310.05446) | 本研究探索将保留机制整合到结直肠息肉分割中，解决了视觉变换器在资源受限设备上实时疾病检测中的内存和并行性挑战。 |
| [^116] | [Molecular De Novo Design through Transformer-based Reinforcement Learning.](http://arxiv.org/abs/2310.05365) | 本文提出了一种基于Transformer的强化学习方法，通过精细调整生成模型，能够在分子的全新设计中生成具有所需性质的分子结构，展现出优越的性能。 |
| [^117] | [Factuality Challenges in the Era of Large Language Models.](http://arxiv.org/abs/2310.05189) | 大型语言模型存在生成虚假、错误或误导内容的问题，同时也面临被恶意应用的风险。本研究探讨了需要从事实核查员、新闻机构和研究与政策界采取的技术创新、监管改革和人工智能素养倡议，以解决这些问题。 |
| [^118] | [Pushing the Limits of Pre-training for Time Series Forecasting in the CloudOps Domain.](http://arxiv.org/abs/2310.05063) | 本论文在CloudOps领域引入了三个大规模时间序列预测数据集，其中最大的数据集拥有数十亿个观测点，为研究时间序列模型的预训练和扩展性奠定了实证基础，并确定了一个有前景的候选架构作为强大的零-shot基线。 |
| [^119] | [Diff-Transfer: Model-based Robotic Manipulation Skill Transfer via Differentiable Physics Simulation.](http://arxiv.org/abs/2310.04930) | 这篇论文提出了Diff-Transfer，一种通过可微分物理仿真来高效传输机器人技能的新框架。Diff-Transfer通过在任务空间内发现可行路径，将源任务转化为目标任务，并通过梯度信息引导适应已知的动作，成功解决另一个子任务。实验结果表明了Diff-Transfer的有效性。 |
| [^120] | [Regret Analysis of Repeated Delegated Choice.](http://arxiv.org/abs/2310.04884) | 本研究针对重复委托选择问题展开研究，通过动态宣布合格集合以学习解决方案分布来减轻代理人的自私行为，进而最小化与最优合格集合之间的后悔。 |
| [^121] | [Lemur: Integrating Large Language Models in Automated Program Verification.](http://arxiv.org/abs/2310.04870) | 本论文提出了一种将LLMs和自动推理器结合起来进行自动程序验证的通用方法，并证明了其完备性。这个方法在一些合成和竞争基准上取得了实际的改进。 |
| [^122] | [Universal Graph Random Features.](http://arxiv.org/abs/2310.04859) | 本文提出了一种新的准蒙特卡罗机制，称为排斥随机游走，通过改进图的采样，提高了统计估计器的集中度。该机制在估计图内核、PageRank向量和图形浓度等方面展示了有效性。 |
| [^123] | [Rethink Baseline of Integrated Gradients from the Perspective of Shapley Value.](http://arxiv.org/abs/2310.04821) | 该论文从Shapley Value的角度重新思考了Integrated Gradients的基线选择，并提出了一种新的基线构建方法叫做Shapley Integrated Gradients (SIG)。在GridWorl上的模拟实验表明，SIG能够生成有意义和无偏的解释模型预测。 |
| [^124] | [Online Corrupted User Detection and Regret Minimization.](http://arxiv.org/abs/2310.04768) | 本文介绍了一种名为LOCUD的在线学习问题，旨在从被损坏的用户行为中学习和利用未知的用户关系以加速学习，并在在线环境中识别损坏用户。 |
| [^125] | [DiffNAS: Bootstrapping Diffusion Models by Prompting for Better Architectures.](http://arxiv.org/abs/2310.04750) | 本文提出了一种名为DiffNAS的基础模型搜索方法，通过引导更好的结构来启动扩散模型，以提高合成性能。通过利用GPT-4作为超网，辅以搜索内存和RFID代理，以及快速收敛训练策略，搜索效率提高了2倍，达到了2.82的性能提升0.37。 |
| [^126] | [Parameter Efficient Multi-task Model Fusion with Partial Linearization.](http://arxiv.org/abs/2310.04742) | 本文提出了一种参数高效的多任务模型融合方法，通过部分线性化适配器模块，并应用任务算法，实现了对大型预训练模型在多个下游任务上的高效微调，从而提高了多任务模型融合的效果和效率。 |
| [^127] | [Balancing stability and plasticity in continual learning: the readout-decomposition of activation change (RDAC) framework.](http://arxiv.org/abs/2310.04741) | 本文介绍了一个名为RDAC的框架，该框架解剖了持续学习中稳定性和可塑性之间的平衡，并详细分析了几种常用算法在处理任务时的稳定性和可塑性权衡。 |
| [^128] | [Offline Imitation Learning with Variational Counterfactual Reasoning.](http://arxiv.org/abs/2310.04706) | 该论文提出了一个名为OILCA的框架，利用可识别的变分自动编码器生成"对抗性"样本，以解决离线模仿学习中数据稀缺、环境变化等问题。 |
| [^129] | [Robustness-enhanced Uplift Modeling with Adversarial Feature Desensitization.](http://arxiv.org/abs/2310.04693) | 本文提出了一种增强鲁棒性的提升建模框架RUAD，并通过特征选择和对抗特征抑制两个定制模块更有效地解决了提升模型的特征敏感性问题。 |
| [^130] | [LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT.](http://arxiv.org/abs/2310.04673) | LauraGPT是一个统一的GPT模型，用于音频识别、理解和生成，具有广泛的应用范围，包括自动语音识别、语音翻译、文本到语音合成、机器翻译等任务。 |
| [^131] | [Transformer-Based Neural Surrogate for Link-Level Path Loss Prediction from Variable-Sized Maps.](http://arxiv.org/abs/2310.04570) | 本研究提出了一种基于Transformer的神经网络架构，可以从变尺寸地图和稀疏测量中预测链接级属性，并且在泛化性能方面表现良好。 |
| [^132] | [What's the Magic Word? A Control Theory of LLM Prompting.](http://arxiv.org/abs/2310.04444) | 本论文将提示工程形式化为LLM上的最优控制问题，研究了给定token序列时是否存在一种最优提示能够准确预测最终的token，并提出了控制理论中的指标来描述LLM的可操纵性。 |
| [^133] | [Confronting Reward Model Overoptimization with Constrained RLHF.](http://arxiv.org/abs/2310.04373) | 本研究首次研究了组合奖励模型中的过度优化问题，发现组成奖励模型之间的相关性对问题的解决方式有重要影响。我们提出了一种使用约束强化学习来解决这个问题的方法。 |
| [^134] | [Accelerating optimization over the space of probability measures.](http://arxiv.org/abs/2310.04006) | 本研究研究了在概率测度空间中加速优化的问题，提出了一种类似于欧几里得空间中基于矩方法的哈密顿流方法，并证明了其可以达到任意高阶的收敛速度。 |
| [^135] | [Multitask Learning for Time Series Data\\with 2D Convolution.](http://arxiv.org/abs/2310.03925) | 该论文研究了将多任务学习（MTL）应用于时间序列分类（TSC）问题，并发现将最先进的一维卷积模型与MTL集成时性能下降。为了解决这个问题，提出了一种基于二维卷积的新设计。 |
| [^136] | [Phase Synchrony Component Self-Organization in Brain Computer Interface.](http://arxiv.org/abs/2310.03748) | 本文提出了相位同步成分自组织的概念，利用深度学习端到端网络自动化脑机接口中的预处理和通道选择，从而提高了分析功能性脑连接和识别脑活动的效果。 |
| [^137] | [GPT-MolBERTa: GPT Molecular Features Language Model for molecular property prediction.](http://arxiv.org/abs/2310.03030) | GPT-MolBERTa是一种用于分子性质预测的自监督大型语言模型，通过使用分子的详细文本描述来学习分子的表示，实验表明其在各种分子属性基准上表现良好，并在回归任务中接近最先进的性能。 |
| [^138] | [IBCL: Zero-shot Model Generation for Task Trade-offs in Continual Learning.](http://arxiv.org/abs/2310.02995) | IBCL提出了一种用于连续学习中任务权衡的零样本模型生成方法，通过更新知识库并利用模型参数分布的凸包形式，实现不同任务性能之间的权衡偏好。 |
| [^139] | [Online Clustering of Bandits with Misspecified User Models.](http://arxiv.org/abs/2310.02717) | 本文介绍了在用户模型错误的情况下的聚类强化学习问题，并提出了两个鲁棒的聚类强化学习算法，以解决用户模型偏差的挑战。 |
| [^140] | [Foundation Reinforcement Learning: towards Embodied Generalist Agents with Foundation Prior Assistance.](http://arxiv.org/abs/2310.02635) | 本研究提出了一种基于具身基础先验的基础强化学习框架，通过加速训练过程来提高样本效率。 |
| [^141] | [On the Parallel Complexity of Multilevel Monte Carlo in Stocahstic Gradient Descent.](http://arxiv.org/abs/2310.02402) | 本文提出了一种延迟MLMC梯度估计器，通过重复利用之前步骤中计算过的梯度分量，大大降低了MLMC的并行复杂性，并在数值实验中证明了其在随机梯度下降中具有更好的并行复杂性。 |
| [^142] | [Representation Engineering: A Top-Down Approach to AI Transparency.](http://arxiv.org/abs/2310.01405) | 这项研究介绍了一种名为表示工程化（RepE）的自上而下方法，通过借鉴认知神经科学的见解，提供了一种增强AI系统透明性的解决方案。该方法将集群级别的表示放在分析的核心，为监测和操纵深度神经网络中的高级认知现象提供了新的方法，并展示了在解决与安全相关的问题上的潜力。 |
| [^143] | [Window-based Model Averaging Improves Generalization in Heterogeneous Federated Learning.](http://arxiv.org/abs/2310.01366) | 基于窗口的模型平均方法(WIMA)通过聚合不同轮次的全局模型，有效捕获多个用户的知识，减少偏差，提高了异构联邦学习的泛化能力和稳定性，无需额外的通信或客户端计算开销。 |
| [^144] | [Subsurface Characterization using Ensemble-based Approaches with Deep Generative Models.](http://arxiv.org/abs/2310.00839) | 本文提出了一种使用深度生成模型和集成方法的井下特性表征方法。通过结合WGAN-GP和ES-MDA技术，实现了准确且高效的K场估计。这种方法在几个井下实例中得到了验证，并展示了未知K字段的主要特征。 |
| [^145] | [SELF: Language-Driven Self-Evolution for Large Language Model.](http://arxiv.org/abs/2310.00533) | SELF提出了一种基于语言驱动的创新方法，允许大型语言模型（LLM）不断自我进化，并通过语言反馈作为评估工具来改进模型的响应能力和训练稳定性。 |
| [^146] | [Distilling Inductive Bias: Knowledge Distillation Beyond Model Compression.](http://arxiv.org/abs/2310.00369) | 本文提出了一种超越模型压缩的知识蒸馏方法，通过从轻量级教师模型中提取归纳偏差，使Vision Transformers (ViTs) 的应用成为可能。这种方法包括使用一组不同架构的教师模型来指导学生Transformer，从而有效提高学生的性能。 |
| [^147] | [SpatialRank: Urban Event Ranking with NDCG Optimization on Spatiotemporal Data.](http://arxiv.org/abs/2310.00270) | 这篇论文提出了一种名为SpatialRank的新颖空间事件排名方法，通过基于时空数据的NDCG优化来解决城市事件排名问题。 |
| [^148] | [Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment.](http://arxiv.org/abs/2310.00212) | 该论文提出了一种新的强化学习框架，使用相对反馈来调整大型语言模型（LLMs）的行为，解决了现有方法在优化比较损失训练的奖励时存在的限制。同时，还提出了一种新的基于轨迹的策略梯度算法（PPPO），用于更有效地进行算法设计和函数逼近。 |
| [^149] | [ABScribe: Rapid Exploration of Multiple Writing Variations in Human-AI Co-Writing Tasks using Large Language Models.](http://arxiv.org/abs/2310.00117) | ABScribe是一种界面，支持在人工智能与人类共同写作任务中快速探索多种写作变化。用户可以使用大型语言模型提示快速生成多个变体，这些变体以可重用的按钮形式呈现，并且可以通过上下文工具栏进行快速的就地比较。 |
| [^150] | [Generative Speech Recognition Error Correction with Large Language Models.](http://arxiv.org/abs/2309.15649) | 本研究探讨了大型语言模型（LLMs）作为ASR后处理器的能力，通过重新评分和错误校正来提高系统性能。通过使用指令提示和任务激活提示方法，结合上下文学习和微调技术，我们展示了LLMs的泛化能力和有效性。 |
| [^151] | [MAPTree: Beating "Optimal" Decision Trees with Bayesian Decision Trees.](http://arxiv.org/abs/2309.15312) | MAPTree是一种通过贝叶斯方法对决策树进行归纳的算法，通过AND/OR搜索实现最大后验树的恢复。在实验中，MAPTree在多个数据集上表现出更好的性能，并且能够以更小的树来实现可比较的性能。在合成数据和实际场景中，MAPTree还展示出更强的抗噪声能力和更好的泛化能力。 |
| [^152] | [Low-rank Adaptation of Large Language Model Rescoring for Parameter-Efficient Speech Recognition.](http://arxiv.org/abs/2309.15223) | 这篇论文介绍了一种基于低秩适应技术的神经语言建模系统，用于语音识别的输出重评分。通过使用低秩分解方法和优化插入矩阵，该系统能够以更高效的方式将BERT模型适应到新领域，大大减少了训练时间。 |
| [^153] | [Recurrent Hypernetworks are Surprisingly Strong in Meta-RL.](http://arxiv.org/abs/2309.14970) | 递归超网络和循环神经网络在元强化学习中的端到端学习表现出惊人的强大性能，相比于现有专门方法更为简单但效果更好。 |
| [^154] | [Invariant Learning via Probability of Sufficient and Necessary Causes.](http://arxiv.org/abs/2309.12559) | 本研究通过引入充分因素和必要因素的概率（PNS）来改善在未知测试分布上的泛化问题，以解决现有方法主要关注因果性的不变性属性而忽视充分性和必要性条件的问题。 |
| [^155] | [Environment-biased Feature Ranking for Novelty Detection Robustness.](http://arxiv.org/abs/2309.12301) | 本文提出了一种环境偏向特征排序的方法，用于鲁棒性的新颖性检测。通过计算特征的环境之间分布方差进行评分，并通过去除高分特征来改善性能。这种方法在真实和合成基准数据上均能提高性能。 |
| [^156] | [What Learned Representations and Influence Functions Can Tell Us About Adversarial Examples.](http://arxiv.org/abs/2309.10916) | 本文将图像处理领域中的对抗子空间技术应用于自然语言处理，提出了基于最近邻和影响函数的检测器，并通过使用影响函数揭示了自然语言处理中的对抗样本子空间与图像处理中的子空间的关系和任务差异。 |
| [^157] | [PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training.](http://arxiv.org/abs/2309.10400) | 本文介绍了一种名为PoSE的训练方法，通过在训练过程中使用固定的上下文窗口和操纵位置索引来适应极长的上下文窗口，实验证明这种方法大大减小了内存和时间开销，对性能影响较小，成功将LLaMA模型扩展到了128k个标记。 |
| [^158] | [Reducing Adversarial Training Cost with Gradient Approximation.](http://arxiv.org/abs/2309.09464) | 本文提出了一种新的对抗训练方法——梯度逼近对抗训练(GAAT)，通过泰勒级数的部分和来近似对抗损失，并近似梯度，以降低建立鲁棒模型的成本。 |
| [^159] | [Morphologically-Aware Consensus Computation via Heuristics-based IterATive Optimization (MACCHIatO).](http://arxiv.org/abs/2309.08066) | 本文提出了一种基于启发式迭代优化的方法，利用精心选择的距离的Fréchet均值构建二进制或概率一致性分割。与STAPLE方法相比，该方法不受图像背景大小和先验选择的影响。 |
| [^160] | [Pure Message Passing Can Estimate Common Neighbor for Link Prediction.](http://arxiv.org/abs/2309.00976) | 这篇论文提出了一种纯粹的消息传递方法，用于估计共同邻居进行链路预测。该方法通过利用输入向量的正交性来捕捉联合结构特征，提出了一种新的链路预测模型MPLP，该模型利用准正交向量估计链路级结构特征，同时保留了节点级复杂性。 |
| [^161] | [Evaluation and Analysis of Hallucination in Large Vision-Language Models.](http://arxiv.org/abs/2308.15126) | 本文提出了基于大型语言模型的幻觉评估框架HaELM，可以评估大型视觉语言模型中的幻觉问题，并分析了导致幻觉的因素，并提出了缓解幻觉问题的建议。 |
| [^162] | [Mixup-Augmented Meta-Learning for Sample-Efficient Fine-Tuning of Protein Simulators.](http://arxiv.org/abs/2308.15116) | 本文通过Mixup增强的元学习方法实现了对蛋白质模拟器的高效微调，可以在有限的训练数据下泛化到未见过的场景，并提供了一种通用的模拟连续动态条件的方法。 |
| [^163] | [Empowering Clinicians and Democratizing Data Science: Large Language Models Automate Machine Learning for Clinical Studies.](http://arxiv.org/abs/2308.14120) | chatGPT ADA是一种能够自主开发临床研究所需的最先进的机器学习模型的大型语言模型，可将高级分析工具民主化，使非数据科学家的临床医生能够轻松应用于医学领域。 |
| [^164] | [Enhancing Breast Cancer Classification Using Transfer ResNet with Lightweight Attention Mechanism.](http://arxiv.org/abs/2308.13150) | 本文介绍了一种使用ResNet模型和轻量级注意机制框架的图像分类方法，通过优化特征表示、增强分类能力和改善特征可辨别性，在乳腺癌分类任务上显示出卓越性能和潜在应用前景。 |
| [^165] | [Audio Generation with Multiple Conditional Diffusion Model.](http://arxiv.org/abs/2308.11940) | 本论文提出了一种使用多条件扩散模型进行音频生成的方法。通过引入内容和风格等额外条件，增强了现有模型的可控性。这种方法可以精确控制生成音频的时间顺序、音高和能量。由于缺乏合适的数据集和评估指标，作者整合了现有数据集并进行了实验验证。 |
| [^166] | [Ultra Dual-Path Compression For Joint Echo Cancellation And Noise Suppression.](http://arxiv.org/abs/2308.11053) | 本文提出了一种超级双路径压缩方法用于联合回声消除和噪声抑制，通过时间频率双路径压缩实现广泛的压缩比，同时降低计算成本，且在固定压缩比下能够进一步改善性能。 |
| [^167] | [Tensor-Compressed Back-Propagation-Free Training for (Physics-Informed) Neural Networks.](http://arxiv.org/abs/2308.09858) | 本文提出了一个完全无需反向传播的神经网络训练框架，并通过张量压缩的方差约减方法和混合梯度评估方法改进了优化和效率。同时，还扩展了框架用于物理信息的神经网络的估计。 |
| [^168] | [Graph Structural Residuals: A Learning Approach to Diagnosis.](http://arxiv.org/abs/2308.06961) | 本文提出了一种新颖的框架，将模型诊断的概念与深度图结构学习相结合，通过数据学习系统的底层结构并提供动态观察。研究通过重新定义系统表示、观察和故障的构建，引入自监督图结构学习模型以及在耦合振荡器系统上的实验，展示了数据驱动的诊断方法的潜力。 |
| [^169] | [Branched Latent Neural Operators.](http://arxiv.org/abs/2308.02599) | Branched Latent Neural Operators (BLNOs) are introduced to learn input-output maps for encoding complex physical processes. BLNOs utilize interpretable latent outputs to enhance learned dynamics and overcome the curse of dimensionality, while also showing excellent generalization properties with small training datasets and short training times. The partial connection structure reduces the number of tunable parameters. BLNOs are proven effective in a challenging biophysically detailed test case. |
| [^170] | [Statistical Estimation Under Distribution Shift: Wasserstein Perturbations and Minimax Theory.](http://arxiv.org/abs/2308.01853) | 这篇论文研究了统计估计中的分布偏移问题，主要关注Wasserstein分布偏移，提出了联合分布偏移概念，并分析了几个统计问题的解决方法。论文发现了最优的极小极大风险和最不利的扰动，并证明了样本均值和最小二乘估计量的优越性。 |
| [^171] | [Kernelised Normalising Flows.](http://arxiv.org/abs/2307.14839) | 本文提出了一种新颖的核化归一化流范式，称为Ferumal流，它将核函数集成到归一化流的框架中。相对于基于神经网络的流，核化流可以在低数据环境中产生竞争力或优越的结果，同时保持参数效率。 |
| [^172] | [Fixed Integral Neural Networks.](http://arxiv.org/abs/2307.14439) | 本文介绍了一种方法来通过神经网络表示学习函数的解析积分，具有计算精确积分的能力，并能将约束直接应用于积分，而且还介绍了将学习函数约束为正的方法以及相关应用。 |
| [^173] | [On the learning Dynamics of Attention Networks.](http://arxiv.org/abs/2307.13421) | 本研究分析了软注意力、硬注意力和潜变量边际似然（LVML）注意力三种注意力模型的学习动态，发现了它们在所选择的片段聚合方式上的显著差异，并解释了分类模型在梯度下降下的演化对最终结果的影响。 |
| [^174] | [The Value of Chess Squares.](http://arxiv.org/abs/2307.05330) | 本研究通过引入边际估值对国际象棋棋盘上的棋子和棋盘进行评价，提供了关于马、象和兵的有价值的见解。 |
| [^175] | [Class-Incremental Learning using Diffusion Model for Distillation and Replay.](http://arxiv.org/abs/2306.17560) | 本文提出了一种使用预训练的扩散模型作为增量学习的附加数据源的方法，通过生成属于先前遇到的图像所属类别的合成样本，并在蒸馏损失和分类损失中使用这些样本，进一步提高了模型的性能。 |
| [^176] | [The ELM Neuron: an Efficient and Expressive Cortical Neuron Model Can Solve Long-Horizon Tasks.](http://arxiv.org/abs/2306.16922) | ELM神经元是一种高效且表达力强的皮层神经元模型，它只需要8K个参数就能准确模拟复杂的计算任务。 |
| [^177] | [Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging.](http://arxiv.org/abs/2306.16788) | 本研究通过将多个经过迭代幅度剪枝的模型进行平均，解决了同时利用稀疏性和参数平均的问题，并显著提升了泛化性能。 |
| [^178] | [RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$.](http://arxiv.org/abs/2306.15909) | RL$^3$是一种原则性混合方法，通过将传统强化学习学到的任务特定动作值作为元强化学习神经网络的输入，提高了元强化学习的性能。 |
| [^179] | [Generalised $f$-Mean Aggregation for Graph Neural Networks.](http://arxiv.org/abs/2306.13826) | 本文提出了一个广义聚合算子，GenAgg，它包括所有标准聚合器的函数空间。实验结果表明，GenAgg能够表示标准聚合器。 |
| [^180] | [An Overview of Catastrophic AI Risks.](http://arxiv.org/abs/2306.12001) | 本文综述了人工智能灾难性风险的四个主要来源，包括恶意使用、人工智能竞赛、组织风险和流氓人工智能。 |
| [^181] | [Understanding Contrastive Learning Through the Lens of Margins.](http://arxiv.org/abs/2306.11526) | 这项研究提出了一种新的视角来理解边界在对比学习中的作用，并从梯度分析的角度分析了边界如何影响对比学习的效果。 |
| [^182] | [Top-down machine learning of coarse-grained protein force-fields.](http://arxiv.org/abs/2306.11375) | 通过分子动力学模拟和可微分轨迹重加权训练神经网络势能，实现了自上而下的粗粒化蛋白质力场建模，仅需蛋白质的天然构象即可展示其外推能力。 |
| [^183] | [Magnetic Resonance Spectroscopy Quantification Aided by Deep Estimations of Imperfection Factors and Overall Macromolecular Signal.](http://arxiv.org/abs/2306.09681) | 本论文通过深度学习方法提高了磁共振波谱技术中代谢物信号定量精度和稳定性。 |
| [^184] | [Utilizing Longitudinal Chest X-Rays and Reports to Pre-Fill Radiology Reports.](http://arxiv.org/abs/2306.08749) | 本文提出了利用纵向胸部X光和报告来预填充放射学报告的方法，以减轻报告错误。通过利用纵向多模态数据和基于transformer的模型，可以更好地捕获包含多模态纵向就诊记录的信息。 |
| [^185] | [Strokes2Surface: Recovering Curve Networks From 4D Architectural Design Sketches.](http://arxiv.org/abs/2306.07220) | 本文介绍了Strokes2Surface，它可从建筑师的笔画中恢复出曲线网络，对于建筑设计中的概念设计和数字建模之间的桥梁具有重要意义。 |
| [^186] | [Variational Imbalanced Regression.](http://arxiv.org/abs/2306.06599) | 本文提出的变分不平衡回归（VIR）模型通过引入Probabilistic Reweighting方法，可以在不平衡回归方面表现良好，并自然产生合理的不确定性估计。 |
| [^187] | [DiffusionShield: A Watermark for Copyright Protection against Generative Diffusion Models.](http://arxiv.org/abs/2306.04642) | 本文提出了一种针对生成式扩散模型的版权保护数字水印方法DiffusionShield，保护图像免受侵权，简单易学，难以检测，可广泛应用于各行各业。 |
| [^188] | [L-C2ST: Local Diagnostics for Posterior Approximations in Simulation-Based Inference.](http://arxiv.org/abs/2306.03580) | 本文提出了一种名为 L-C2ST 的基于本地诊断实现模拟推断中后验近似的新方法，其可以在任何给定的观测下本地评估后验估计器，有效地解决了目前评估后验估计器限制解决方法的问题。 |
| [^189] | [Graph-based methods coupled with specific distributional distances for adversarial attack detection.](http://arxiv.org/abs/2306.00042) | 本论文提出了利用图形的方法结合特定分布距离来检测对抗性攻击，通过研究神经网络的图结构，介绍了用于预测和解释对抗性攻击的特定测量方法，这有助于研究对抗性攻击的内在机理。 |
| [^190] | [What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization.](http://arxiv.org/abs/2305.19420) | 本文对In-Context Learning进行了全面的研究，通过贝叶斯模型平均算法来隐式地实现ICL估计量，并采用在线学习的角度来分析ICL性能，建立后悔界限，并通过关注机制近似参数化。 |
| [^191] | [MiniSUPERB: Lightweight Benchmark for Self-supervised Speech Models.](http://arxiv.org/abs/2305.19011) | 这篇论文提出了一个名为MiniSUPERB的轻量级基准测试，可以有效地评估自监督语音模型的性能，并在计算成本上比SUPERB更低。同时，该论文还研究了在少样本情况下评估SSL语音模型的性能变化。 |
| [^192] | [Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning.](http://arxiv.org/abs/2305.18459) | 本文研究了单一扩散模型在建模多个任务的策略训练的大规模离线数据时的有效性。该方法名为Multi-Task Diffusion Model (MTDiff)，结合了Transformer骨干和提示学习，能够在如此复杂的多任务环境下取得相当不错的性能。 |
| [^193] | [Zero-shot Task Preference Addressing Enabled by Imprecise Bayesian Continual Learning.](http://arxiv.org/abs/2305.14782) | 提出了零样本任务偏好的不精确贝叶斯继续学习（IBCL）算法，该算法更新模型参数分布凸壳形式的知识库，并使用零样本获取模型以满足不同的偏好，使得在具有大量任务偏好的情况下更加可扩展。 |
| [^194] | [Robust Explanations for Deep Neural Networks via Pseudo Neural Tangent Kernel Surrogate Models.](http://arxiv.org/abs/2305.14585) | 本研究通过建立一个规范化的伪神经切线核，证明了它能够更好地与神经网络决策函数相关，比基于嵌入和影响的替代品更有效，并且从它创建的归因会更准确地选择被扰动的训练数据，从而证明了核线性模型是跨多个数据领域并有效的替代模型。 |
| [^195] | [Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training.](http://arxiv.org/abs/2305.14342) | Sophia是一种用于语言模型预训练的可扩展的二阶优化算法，使用对角Hessian作为预调节器，并进行元素级别的裁剪控制更新大小。 |
| [^196] | [Beyond Shared Vocabulary: Increasing Representational Word Similarities across Languages for Multilingual Machine Translation.](http://arxiv.org/abs/2305.14189) | 本文提出了一种超越共享词汇的方法，通过定义词级信息传输路径和使用图网络来融合跨语言的词嵌入，实现了在多语言机器翻译中提高相似含义词的对齐性和BLEU分数的一致提升。此方法只需要少量额外参数且计算成本增加有限，并且推理时间与基线相同。 |
| [^197] | [Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt.](http://arxiv.org/abs/2305.11186) | 本文提出了使用可转移提示来优化压缩的LLMs的准确性和效率的平衡问题。该方法通过选择精度更高的提示显著提高了压缩的LLM在特定查询方面的生成质量，并实现了4倍推理时间加速。 |
| [^198] | [Clifford Group Equivariant Neural Networks.](http://arxiv.org/abs/2305.11141) | 我们引入了Clifford群等变神经网络，它可以构建O(n)和E(n)等变模型。该方法通过调整Clifford群的定义以及保持向量空间和乘法结构的作用来实现多个有利属性。 |
| [^199] | [MoMo: Momentum Models for Adaptive Learning Rates.](http://arxiv.org/abs/2305.07583) | 本文提出了新的自适应学习率，可与任何动量方法一起使用，通过构建损失函数模型并使用下限截断，以及即时估计未知下限，来近似最小化该模型以计算下一步，实验表明，相较于SGDM和Adam，该方法在精度和超参数调优的鲁棒性方面有所提高。 |
| [^200] | [Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation.](http://arxiv.org/abs/2305.05803) | 本论文提出了一种通过利用Segment Anything Model (SAM)增强Class Activation Maps (CAM)生成高质量伪标签的方法来解决弱监督语义分割中CAM的局部激活和虚假激活的限制问题。 |
| [^201] | [Balancing Privacy and Security in Federated Learning with FedGT: A Group Testing Framework.](http://arxiv.org/abs/2305.05506) | 该论文提出了FedGT框架，通过群体测试的方法在联邦学习中识别并删除恶意客户，从而平衡了隐私和安全，保护数据隐私并提高了识别恶意客户的能力。 |
| [^202] | [Tiny-PPG: A Lightweight Deep Neural Network for Real-Time Detection of Motion Artifacts in Photoplethysmogram Signals on Edge Devices.](http://arxiv.org/abs/2305.03308) | Tiny-PPG是一个用于边缘设备上实时检测PPG信号中运动伪影的轻量级深度神经网络，通过使用深度可分离卷积和空洞空间金字塔池化模块，以及通道注意机制，能够在平衡检测精度和速度的情况下实现最先进的检测效果，并可以在现实世界中部署，实现基于物联网的可穿戴设备和智能健康设备上的准确实时PPG伪影检测。 |
| [^203] | [Sample Efficient Model-free Reinforcement Learning from LTL Specifications with Optimality Guarantees.](http://arxiv.org/abs/2305.01381) | 本文提出了一种基于LTL规范的无模型强化学习方法，该方法结合乘积MDP、奖励结构和折扣机制有效地学习并优化未知随机系统最大化满足LTL规范的概率的最优策略。 |
| [^204] | [Self-Correcting Bayesian Optimization through Bayesian Active Learning.](http://arxiv.org/abs/2304.11005) | 该论文提出了SAL和SCoreBO两种方法，用于提高高斯过程模型的超参数选择和贝叶斯优化的表现。 |
| [^205] | [Two-stage MR Image Segmentation Method for Brain Tumors based on Attention Mechanism.](http://arxiv.org/abs/2304.08072) | 该论文提出了一个基于循环一致生成对抗网络的两阶段脑肿瘤MR图像分割方法，通过引入坐标注意力和空间注意力模块来优化生成器性能，以提高生成和分割性能。 |
| [^206] | [MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults using Facial Videos.](http://arxiv.org/abs/2304.05292) | 本文提出了一种新的深度机器学习模型MC-ViViT，用于通过分析面部特征检测老年人轻度认知障碍。通过MC模块和结合损失函数来解决数据集样本不平衡问题，提高了算法的性能。 |
| [^207] | [Self-training with dual uncertainty for semi-supervised medical image segmentation.](http://arxiv.org/abs/2304.04441) | 该论文介绍了在半监督医学图像分割中如何通过双重不确定性的自训练方式来提高分割精度。 |
| [^208] | [StepMix: A Python Package for Pseudo-Likelihood Estimation of Generalized Mixture Models with External Variables.](http://arxiv.org/abs/2304.03853) | StepMix是一个用于外部变量广义混合模型的伪似然估计的Python包，提供了单步和逐步估计方法，帮助从业人员进行模型估计、选择和解释。 |
| [^209] | [Swarm Reinforcement Learning For Adaptive Mesh Refinement.](http://arxiv.org/abs/2304.00818) | 这项研究提出了一种适应网格细化的群体强化学习方法，通过将网格建模为一组简单协作的代理，并利用消息传递网络在相邻网格元素之间传播信息，以解决传统方法在复杂模拟中的应用限制。该方法被证实可以学习可靠、可扩展的网格细化策略。 |
| [^210] | [Reflexion: an autonomous agent with dynamic memory and self-reflection.](http://arxiv.org/abs/2303.11366) | 本文提出 Reflexion 方法，给智能体赋予了动态记忆和自我反思能力，以增强其任务特定的行动选择能力。 |
| [^211] | [Understanding Model Complexity for temporal tabular and multi-variate time series, case study with Numerai data science tournament.](http://arxiv.org/abs/2303.07925) | 本文采用 Numerai 数据科学竞赛的数据，探究了多变量时间序列建模中不同特征工程和降维方法的应用；提出了一种新的集成方法，用于高维时间序列建模，该方法在通用性、鲁棒性和效率上优于一些深度学习模型。 |
| [^212] | [RACCER: Towards Reachable and Certain Counterfactual Explanations for Reinforcement Learning.](http://arxiv.org/abs/2303.04475) | RACCER是第一个针对强化学习智能体行为生成对抗事实解释的专用方法，通过使用一组针对强化学习的特定对抗事实属性，保证易于实现且具有高概率预期结果的对抗事实。 |
| [^213] | [Over-Parameterization Exponentially Slows Down Gradient Descent for Learning a Single Neuron.](http://arxiv.org/abs/2302.10034) | 通过研究学习单个神经元的过度参数化设置，本研究发现梯度下降算法的收敛速度会以指数级减慢，是首个给出该问题全局收敛结果的研究。通过证明上下界，我们精确刻画出了收敛速度，并指出了过度参数化对于收敛速度的影响。 |
| [^214] | [AliasNet: Alias Artefact Suppression Network for Accelerated Phase-Encode MRI.](http://arxiv.org/abs/2302.08861) | 本论文提出了AliasNet，一个别名伪影抑制网络，解决了在相编码MRI中由于硬件限制导致的别名伪影问题。通过开发两种解耦技术，实现了对具有出色1D不相干特性的伪影信号进行明确的1D正则化。该方法还结合了1D和2D重建技术，充分利用了稀疏性模型和多方向特点。 |
| [^215] | [Quantum Learning Theory Beyond Batch Binary Classification.](http://arxiv.org/abs/2302.07409) | 这篇论文通过研究量子学习理论拓展了批处理多类学习、在线布尔学习和在线多类学习，并提出了第一个具有量子示例的在线学习模型。 |
| [^216] | [Robust Knowledge Transfer in Tiered Reinforcement Learning.](http://arxiv.org/abs/2302.05534) | 本文研究了层级增强学习中的知识传输，提出了一种新颖的在线学习算法，在没有先验知识的任务相似性的情况下实现强大的知识传输。 |
| [^217] | [Asymmetric Certified Robustness via Feature-Convex Neural Networks.](http://arxiv.org/abs/2302.01961) | 本文提出了一种新颖的特征凸卷积神经网络架构，将ICNN与Lipschitz特征映射结合，实现了对抗鲁棒性，并证明了对于具有一个“敏感”类的不对称二元分类设置，可以计算出确定性的认证鲁棒半径。 |
| [^218] | [Expanding Small-Scale Datasets with Guided Imagination.](http://arxiv.org/abs/2211.13976) | 本论文提出了一个引导想象框架(GIF)，通过利用DALL-E2和Stable Diffusion (SD)等生成模型，从种子数据中扩充小规模数据集。该框架通过在先验模型的语义空间中优化种子数据潜在特征来创建逼真的图像，并引入了类别保持和样本多样性的标准来指导想象过程。 |
| [^219] | [How to Fine-Tune Vision Models with SGD.](http://arxiv.org/abs/2211.09359) | SGD 和 AdamW 是用于调整视觉模型的两种常见优化器，当细调梯度在嵌入层中较大时，AdamW 的性能优于 SGD。我们提出了冻结嵌入层的简单修正方法，使得 SGD 表现略好于 AdamW 并且使用更少的内存。 |
| [^220] | [Two Models are Better than One: Federated Learning Is Not Private For Google GBoard Next Word Prediction.](http://arxiv.org/abs/2210.16947) | 本文介绍了对联邦学习用于训练自然语言文本模型的新攻击，展示了对谷歌GBoard应用中下一个单词预测模型的攻击效果，揭示了用户输入的单词和句子可以被高准确度地恢复，引发了隐私担忧。 |
| [^221] | [A jet tagging algorithm of graph network with HaarPooling message passing.](http://arxiv.org/abs/2210.13869) | 本论文介绍了一种将图神经网络与HaarPooling操作相结合的方法，称为HMPNet，用于高能物理中的夸克胶子标记。实验结果表明，适当选择HaarPooling的信息可以提高夸克胶子标记的准确性。 |
| [^222] | [Flexible Attention-Based Multi-Policy Fusion for Efficient Deep Reinforcement Learning.](http://arxiv.org/abs/2210.03729) | 该论文提出了一种知识引导的强化学习方法（KGRL），通过融合多个知识策略并利用注意力机制实现了灵活的知识重新排列。这种方法可以提高强化学习代理的样本效率和泛化能力。 |
| [^223] | [Rank-N-Contrast: Learning Continuous Representations for Regression.](http://arxiv.org/abs/2210.01189) | Rank-N-Contrast是一种学习连续表示的回归框架，通过对样本在目标空间中的排名进行比较来提高性能，并在计算机视觉、人机交互和医疗保健等领域的回归任务中取得了最先进的结果。 |
| [^224] | [Toward Trustworthy Neural Program Synthesis.](http://arxiv.org/abs/2210.00848) | 我们提出了一种能够估计大型语言模型中采样程序正确性概率的简单方法，通过采样候选程序和候选谓词来预测程序的正确性，并推断出关于生成代码行为解释的有用谓词。 |
| [^225] | [Learning domain-specific causal discovery from time series.](http://arxiv.org/abs/2209.05598) | 本研究旨在通过数据驱动的方法提高时间序列中领域特定的因果发现。实验证明，该方法在多个领域上明显优于人类设计的通用因果发现方法。 |
| [^226] | [Neural Network Approximation of Continuous Functions in High Dimensions with Applications to Inverse Problems.](http://arxiv.org/abs/2208.13305) | 该论文提出了一种通用的方法，用于解决神经网络在高维空间中逼近连续函数的问题，并对理论与实践之间的差距进行了缩小。该方法基于Johnson-Lindenstrauss嵌入的观察，通过将高维集合嵌入到低维空间中，使得较小的神经网络可以有效逼近高维连续函数。 |
| [^227] | [Multiple Descent in the Multiple Random Feature Model.](http://arxiv.org/abs/2208.09897) | 本文研究了过度参数化学习中的双下降现象，并在多组分预测模型中进一步探究了多次下降现象。通过理论计算和实验证实，随机特征模型的风险曲线可以呈现出三次下降。 |
| [^228] | [Sample Complexity of Forecast Aggregation.](http://arxiv.org/abs/2207.13126) | 本文研究了一种预测聚合模型，考虑了贝叶斯方法应用于专家命中后的信号汇聚。论文中提供了对于该问题的样本复杂度，表明该复杂度至少为 $\tilde \Omega(m^{n-2} / \varepsilon)$，其中 m 是每个专家信号空间的大小。 |
| [^229] | [Neural Moving Horizon Estimation for Robust Flight Control.](http://arxiv.org/abs/2206.10397) | 本文提出了一种神经移动视界估计器（NeuroMHE），它可以自动调整神经网络建模的关键参数，并适应不同的飞行场景。通过推导出与加权矩阵相关的解析梯度，我们实现了将该估计器作为可学习层嵌入神经网络进行高效学习。此外，我们还开发了一种基于模型的策略梯度算法，以从四旋翼轨迹跟踪误差中直接训练NeuroMHE，而不需要地面真实干扰数据。 |
| [^230] | [Learning to Modulate Random Weights: Neuromodulation-inspired Neural Networks For Efficient Continual Learning.](http://arxiv.org/abs/2204.04297) | 本论文提出了一种受生物神经调节启发的神经网络架构，通过学习调节随机权重的活动来解决连续学习中的灾难性遗忘问题，并提供了新的学习表示解释方法。 |
| [^231] | [No-Regret Learning with Unbounded Losses: The Case of Logarithmic Pooling.](http://arxiv.org/abs/2202.11219) | 本文研究了在线对抗环境中，使用对数池化方法学习专家权重的问题。我们提出了一种基于在线镜像下降算法的方法，以达到无遗憾保证。 |
| [^232] | [Data-Driven Minimax Optimization with Expectation Constraints.](http://arxiv.org/abs/2202.07868) | 本论文研究了基于数据驱动的带期望约束的极小极大优化问题，并提出了一类高效的算法来解决这一问题。依据实验结果，该算法在大规模实际应用中具有实际效率。 |
| [^233] | [Multi-consensus Decentralized Accelerated Gradient Descent.](http://arxiv.org/abs/2005.00797) | 本文提出了一种新颖的多共识分散加速梯度下降算法，实现了最优计算复杂度和接近最优的通信复杂度。算法的线性收敛仅取决于全局目标的强凸性，并不要求局部函数是凸函数。实证研究表明该方法在机器学习应用中超越其他方法。 |
| [^234] | [Dynamic Subgoal-based Exploration via Bayesian Optimization.](http://arxiv.org/abs/1910.09143) | 本论文提出了一种基于贝叶斯优化的成本感知探索方法，能够针对稀疏奖励导航环境中的复杂任务，高效地搜索动态子目标的探索策略。 |

# 详细

[^1]: 基于合同理论的异步联邦学习与激励机制

    Asynchronous Federated Learning with Incentive Mechanism Based on Contract Theory. (arXiv:2310.06448v1 [cs.LG])

    [http://arxiv.org/abs/2310.06448](http://arxiv.org/abs/2310.06448)

    这项研究提出了一种基于合同理论的异步联邦学习框架，通过使用激励机制来解决联邦学习中的异构性和慢速问题，并且在实验中展示了较高的测试准确率。

    

    为了解决联邦学习中固有的异构性以及吸引高质量的客户所带来的挑战，已经使用了各种各样的激励机制。然而，现有的激励机制通常在传统的同步聚合中使用，导致了显著的慢速问题。在这项研究中，我们提出了一种基于合同理论的新型异步联邦学习框架。在激励机制中，我们努力通过自适应调整客户端的本地模型训练轮数来最大化任务发布者的效用，考虑到时间延迟和测试准确性等因素。在异步方案中，考虑到客户端质量，我们设计了聚合权重和访问控制算法，以促进异步聚合。通过在MNIST数据集上进行的实验证明，我们的框架实现的测试准确率比其他方法高出3.12%和5.84%。

    To address the challenges posed by the heterogeneity inherent in federated learning (FL) and to attract high-quality clients, various incentive mechanisms have been employed. However, existing incentive mechanisms are typically utilized in conventional synchronous aggregation, resulting in significant straggler issues. In this study, we propose a novel asynchronous FL framework that integrates an incentive mechanism based on contract theory. Within the incentive mechanism, we strive to maximize the utility of the task publisher by adaptively adjusting clients' local model training epochs, taking into account factors such as time delay and test accuracy. In the asynchronous scheme, considering client quality, we devise aggregation weights and an access control algorithm to facilitate asynchronous aggregation. Through experiments conducted on the MNIST dataset, the simulation results demonstrate that the test accuracy achieved by our framework is 3.12% and 5.84% higher than that achieved
    
[^2]: 用于修正分类模型的规则挖掘

    Rule Mining for Correcting Classification Models. (arXiv:2310.06446v1 [cs.SE])

    [http://arxiv.org/abs/2310.06446](http://arxiv.org/abs/2310.06446)

    本研究提出了一种用于修正分类模型的规则挖掘方法，通过挖掘不准确子集和对其进行修正的规则列表，以提高模型的预测准确性。

    

    机器学习模型需要不断更新或修正，以确保预测准确性始终保持高水平。本研究考虑了模型修正可能改变预测结果的场景，例如模型是复杂系统或软件的一部分。在这种场景中，开发人员希望能够控制修正的规范。为了实现这一点，开发人员需要了解哪些输入的子集会导致模型的预测不准确。因此，我们提出了修正规则挖掘方法，以获取描述不准确子集和如何进行修正的全面规则列表。我们还开发了一种高效的修正规则挖掘算法，该算法结合了频繁项集挖掘和独特的修正规则修剪技术。我们观察到，该算法找到了各种规则，有助于收集不充分学习的数据，直接修正模型输出。

    Machine learning models need to be continually updated or corrected to ensure that the prediction accuracy remains consistently high. In this study, we consider scenarios where developers should be careful to change the prediction results by the model correction, such as when the model is part of a complex system or software. In such scenarios, the developers want to control the specification of the corrections. To achieve this, the developers need to understand which subpopulations of the inputs get inaccurate predictions by the model. Therefore, we propose correction rule mining to acquire a comprehensive list of rules that describe inaccurate subpopulations and how to correct them. We also develop an efficient correction rule mining algorithm that is a combination of frequent itemset mining and a unique pruning technique for correction rules. We observed that the proposed algorithm found various rules which help to collect data insufficiently learned, directly correct model outputs,
    
[^3]: 骨骼地面实况提取：方法论、注释工具和基准

    Skeleton Ground Truth Extraction: Methodology, Annotation Tool and Benchmarks. (arXiv:2310.06437v1 [cs.CV])

    [http://arxiv.org/abs/2310.06437](http://arxiv.org/abs/2310.06437)

    本文提出了一种基于启发式策略的骨骼地面实况提取方法，通过利用目标的上下文、简单性和完整性线索进行人-数据回路的GT提取，从而解决了现有数据集中骨骼GT缺乏和标准不一致的问题。

    

    骨骼地面实况（GT）对于监督式骨骼提取方法的成功至关重要，尤其是在深度学习技术的流行之下。此外，我们还看到骨骼 GT 不仅用于用于训练卷积神经网络（CNN）的骨骼检测器，还用于评估与骨骼相关的修剪和匹配算法。然而，大多数现有的形状和图像数据集在骨骼 GT 的缺乏和 GT 标准的不一致性方面存在问题。因此，很难公平地评估和复现基于 CNN 的骨骼检测器和算法。在本文中，我们提出了一种在二元形状和自然图像中提取骨骼 GT 的启发式策略。我们的策略建立在诊断性假设的扩展理论上，这使得人-数据回路的 GT 提取可以基于目标的上下文、简单性和完整性的线索来进行编码。使用这个策略，我们开发了一个工具，SkeView，用于生成17个现有形状数据集的骨骼 GT。

    Skeleton Ground Truth (GT) is critical to the success of supervised skeleton extraction methods, especially with the popularity of deep learning techniques. Furthermore, we see skeleton GTs used not only for training skeleton detectors with Convolutional Neural Networks (CNN) but also for evaluating skeleton-related pruning and matching algorithms. However, most existing shape and image datasets suffer from the lack of skeleton GT and inconsistency of GT standards. As a result, it is difficult to evaluate and reproduce CNN-based skeleton detectors and algorithms on a fair basis. In this paper, we present a heuristic strategy for object skeleton GT extraction in binary shapes and natural images. Our strategy is built on an extended theory of diagnosticity hypothesis, which enables encoding human-in-the-loop GT extraction based on clues from the target's context, simplicity, and completeness. Using this strategy, we developed a tool, SkeView, to generate skeleton GT of 17 existing shape 
    
[^4]: 通过标签排名实现深度分类器的符合预测

    Conformal Prediction for Deep Classifier via Label Ranking. (arXiv:2310.06430v1 [cs.LG])

    [http://arxiv.org/abs/2310.06430](http://arxiv.org/abs/2310.06430)

    符合预测是一个统计框架，能够生成含有真实标签的预测集。本文提出了一种名为SAPS的新算法，通过舍弃除最大softmax概率之外的所有概率值，减少了非一致性评分对概率值的依赖性，达到了生成小尺寸集合和传达不确定性的目的。

    

    符合预测是一个统计框架，能够生成含有所需覆盖保证的真实标签的预测集。机器学习模型产生的预测概率通常是错误校准的，导致符合预测中的大型预测集。本文通过实证和理论研究表明，忽略概率值可以缓解错误校准概率值的不良影响。然后，我们提出了一种名为“排序自适应预测集”（SAPS）的新算法，该算法舍弃除最大softmax概率之外的所有概率值。SAPS的核心思想是在保留不确定性信息的同时，尽量减少非一致性评分对概率值的依赖性。这样，SAPS可以生成小尺寸的集合，并传达每个实例的不确定性。从理论上讲，我们提供了SAPS的有限样本覆盖保证，并展示了期望值的范围。

    Conformal prediction is a statistical framework that generates prediction sets containing ground-truth labels with a desired coverage guarantee. The predicted probabilities produced by machine learning models are generally miscalibrated, leading to large prediction sets in conformal prediction. In this paper, we empirically and theoretically show that disregarding the probabilities' value will mitigate the undesirable effect of miscalibrated probability values. Then, we propose a novel algorithm named $\textit{Sorted Adaptive prediction sets}$ (SAPS), which discards all the probability values except for the maximum softmax probability. The key idea behind SAPS is to minimize the dependence of the non-conformity score on the probability values while retaining the uncertainty information. In this manner, SAPS can produce sets of small size and communicate instance-wise uncertainty. Theoretically, we provide a finite-sample coverage guarantee of SAPS and show that the expected value of se
    
[^5]: TANGO: 时间反演潜在图ODE用于多智能体动力系统

    TANGO: Time-Reversal Latent GraphODE for Multi-Agent Dynamical Systems. (arXiv:2310.06427v1 [cs.LG])

    [http://arxiv.org/abs/2310.06427](http://arxiv.org/abs/2310.06427)

    本文提出了TANGO方法，通过时间反演对称性作为归纳偏差，对多智能体动力系统进行学习，即使在非保守的可逆系统中也能保持能量，并且能更高效地学习系统动力学。

    

    在许多领域中，从数据中学习复杂的多智能体系统动力学是至关重要的，例如在物理模拟和材料建模中。在纯数据驱动方法的基础上，现有的基于物理的方法如Hamiltonian神经网络严格遵循能量守恒定律引入归纳偏差，使得学习更加高效。然而，许多现实世界中的系统并不严格遵守能量守恒定律，如带有摩擦的弹簧系统。鉴于此，我们将注意力转向一个更广泛的物理原理：时间反演对称性，它描述了当系统的动力在时间上倒转时，动力学应保持不变。这仍然有助于保持保守系统的能量，并同时为非保守的可逆系统提供强大的归纳偏差。为了注入这种归纳偏差，本文提出了一个简单而有效的自监督正则化项作为软约束，将正向和逆向动力对齐。

    Learning complex multi-agent system dynamics from data is crucial across many domains, such as in physical simulations and material modeling. Extended from purely data-driven approaches, existing physics-informed approaches such as Hamiltonian Neural Network strictly follow energy conservation law to introduce inductive bias, making their learning more sample efficiently. However, many real-world systems do not strictly conserve energy, such as spring systems with frictions. Recognizing this, we turn our attention to a broader physical principle: Time-Reversal Symmetry, which depicts that the dynamics of a system shall remain invariant when traversed back over time. It still helps to preserve energies for conservative systems and in the meanwhile, serves as a strong inductive bias for non-conservative, reversible systems. To inject such inductive bias, in this paper, we propose a simple-yet-effective self-supervised regularization term as a soft constraint that aligns the forward and b
    
[^6]: 用于图学习中的拓扑概括的流动扩散变压器

    Advective Diffusion Transformers for Topological Generalization in Graph Learning. (arXiv:2310.06417v1 [cs.LG])

    [http://arxiv.org/abs/2310.06417](http://arxiv.org/abs/2310.06417)

    本研究探索了在不同的图拓扑存在下，图扩散方程如何对GNN进行外推和概括，揭示了基于局部扩散的现有模型在概括能力上的不足，并提出了非局部扩散的潜力。

    

    图扩散方程与图神经网络（GNN）密切相关，并且最近引起了人们的关注，作为分析GNN动力学、形式化其表达能力和证明架构选择的有原则的框架。图学习中的一个关键问题是GNN的概括能力。当前方法的一个主要限制在于假设训练集和测试集中的图拓扑来自相同的分布。本文通过探索图扩散方程在不同图拓扑存在下的外推和概括能力，迈出了解析GNN概括性的一步。我们首先展示了基于图上局部扩散的现有模型在概括能力上的不足，这是由于对拓扑变化的指数敏感性引起的。随后的分析揭示了非局部扩散的潜力，它倡导对完全连接的潜在图进行特征传播。

    Graph diffusion equations are intimately related to graph neural networks (GNNs) and have recently attracted attention as a principled framework for analyzing GNN dynamics, formalizing their expressive power, and justifying architectural choices. One key open questions in graph learning is the generalization capabilities of GNNs. A major limitation of current approaches hinges on the assumption that the graph topologies in the training and test sets come from the same distribution. In this paper, we make steps towards understanding the generalization of GNNs by exploring how graph diffusion equations extrapolate and generalize in the presence of varying graph topologies. We first show deficiencies in the generalization capability of existing models built upon local diffusion on graphs, stemming from the exponential sensitivity to topology variation. Our subsequent analysis reveals the promise of non-local diffusion, which advocates for feature propagation over fully-connected latent gr
    
[^7]: 深度强化学习揭示了无先验知识下分离共沸混合物的过程

    Deep reinforcement learning uncovers processes for separating azeotropic mixtures without prior knowledge. (arXiv:2310.06415v1 [cs.LG])

    [http://arxiv.org/abs/2310.06415](http://arxiv.org/abs/2310.06415)

    本研究使用深度强化学习方法，在没有先验知识的情况下成功设计了适用于不同化学系统的分离共沸混合物的流程图，并能将超过99％的材料分离为纯组分。

    

    化学工程中的过程综合是一个复杂的规划问题，由于庞大的搜索空间、连续参数和泛化的需求。近年来，没有先验知识的深度强化学习代理在各种复杂的规划问题中展现出超越人类的表现。现有的关于流程图综合的强化学习工作展示了有希望的概念，但侧重于单一化学系统中的狭窄问题，限制了其实用性。我们提出了一种用于流程图综合的通用深度强化学习方法。我们展示了单个代理在分离二元共沸混合物的一般任务中的适应性。在没有先验知识的情况下，它学会设计多个化学系统的接近最优的流程图，考虑不同的进料组成和概念方法。平均而言，该代理能将超过99％的材料分离为纯组分，并自主学习基本的过程工程师技能。

    Process synthesis in chemical engineering is a complex planning problem due to vast search spaces, continuous parameters and the need for generalization. Deep reinforcement learning agents, trained without prior knowledge, have shown to outperform humans in various complex planning problems in recent years. Existing work on reinforcement learning for flowsheet synthesis shows promising concepts, but focuses on narrow problems in a single chemical system, limiting its practicality. We present a general deep reinforcement learning approach for flowsheet synthesis. We demonstrate the adaptability of a single agent to the general task of separating binary azeotropic mixtures. Without prior knowledge, it learns to craft near-optimal flowsheets for multiple chemical systems, considering different feed compositions and conceptual approaches. On average, the agent can separate more than 99% of the involved materials into pure components, while autonomously learning fundamental process engineer
    
[^8]: Hexa: 知识驱动的对话系统的自我提升

    Hexa: Self-Improving for Knowledge-Grounded Dialogue System. (arXiv:2310.06404v1 [cs.CL])

    [http://arxiv.org/abs/2310.06404](http://arxiv.org/abs/2310.06404)

    本论文提出了一种自我提升的方法，用于改进知识驱动对话生成的中间步骤的生成性能。通过引入引导提示和修改损失函数的自举策略，提高了生成自动生成回答的多样性，并在各种基准数据集上实验证明了该方法的有效性。

    

    知识驱动的对话生成中一种常见的做法是使用模块化的方法明确地利用中间步骤（如网络搜索、记忆检索）。然而，与对话响应相比，这些步骤的数据往往难以获取，因为在普通对话中无法观察到它们。为了填补这些数据的缺失，我们开发了一种自我提升方法，以改进中间步骤的生成性能，而不需要地面真实数据。具体而言，我们提出了一种新颖的引导提示和修改的损失函数的引导自动生成回答多样性的自举方法。通过在各种基准数据集上进行实验，我们经验证明我们的方法成功地利用了自我提升机制，在生成中间和最终回答方面改善了知识驱动对话生成任务的性能。

    A common practice in knowledge-grounded dialogue generation is to explicitly utilize intermediate steps (e.g., web-search, memory retrieval) with modular approaches. However, data for such steps are often inaccessible compared to those of dialogue responses as they are unobservable in an ordinary dialogue. To fill in the absence of these data, we develop a self-improving method to improve the generative performances of intermediate steps without the ground truth data. In particular, we propose a novel bootstrapping scheme with a guided prompt and a modified loss function to enhance the diversity of appropriate self-generated responses. Through experiments on various benchmark datasets, we empirically demonstrate that our method successfully leverages a self-improving mechanism in generating intermediate and final responses and improves the performances on the task of knowledge-grounded dialogue generation.
    
[^9]: Lo-Hi: 实用的机器学习药物发现基准测试

    Lo-Hi: Practical ML Drug Discovery Benchmark. (arXiv:2310.06399v1 [cs.LG])

    [http://arxiv.org/abs/2310.06399](http://arxiv.org/abs/2310.06399)

    本研究创建了一个实用的Lo-Hi基准测试，包括前导优化和命中识别两个任务，为药物发现过程提供了一种切实可行的评估方法。对于命中识别任务，研究者设计了一种解决顶点最小k-Cut问题的新型分子拆分算法，并测试了各种机器学习模型的性能。研究还发现现有基准测试不现实且过于乐观。

    

    寻找新药变得越来越困难。药物发现的希望之一是使用机器学习模型预测分子属性。因此，正在开发和测试分子属性预测模型，以在MoleculeNet等基准测试中应用。然而，现有的基准测试不切实际，并且与实际应用模型相差太大。我们创建了一个新的实用的Lo-Hi基准测试，包括两个任务：前导优化（Lo）和命中识别（Hi），对应于真实的药物发现过程。对于Hi任务，我们设计了一种解决平衡顶点最小k-Cut问题的新型分子拆分算法。我们测试了最先进和经典的机器学习模型，揭示了在实际环境中哪种效果更好。我们分析了现代基准测试，并显示它们不切实际并且过于乐观。

    Finding new drugs is getting harder and harder. One of the hopes of drug discovery is to use machine learning models to predict molecular properties. That is why models for molecular property prediction are being developed and tested on benchmarks such as MoleculeNet. However, existing benchmarks are unrealistic and are too different from applying the models in practice. We have created a new practical \emph{Lo-Hi} benchmark consisting of two tasks: Lead Optimization (Lo) and Hit Identification (Hi), corresponding to the real drug discovery process. For the Hi task, we designed a novel molecular splitting algorithm that solves the Balanced Vertex Minimum $k$-Cut problem. We tested state-of-the-art and classic ML models, revealing which works better under practical settings. We analyzed modern benchmarks and showed that they are unrealistic and overoptimistic.  Review: https://openreview.net/forum?id=H2Yb28qGLV  Lo-Hi benchmark: https://github.com/SteshinSS/lohi_neurips2023  Lo-Hi split
    
[^10]: 图神经网络中的对抗鲁棒性：一种哈密顿方法

    Adversarial Robustness in Graph Neural Networks: A Hamiltonian Approach. (arXiv:2310.06396v1 [cs.LG])

    [http://arxiv.org/abs/2310.06396](http://arxiv.org/abs/2310.06396)

    本文通过引入保守哈密顿神经流构建了对抗攻击鲁棒的图神经网络，从而提高了对抗扰动下的鲁棒性。

    

    图神经网络(GNNs)容易受到对抗性扰动的影响，包括影响节点特征和图拓扑的扰动。本文研究了从不同的神经流导出的GNNs，重点关注它们与多种稳定性概念（如BIBO稳定性、李亚普诺夫稳定性、结构稳定性和保守稳定性）的联系。我们认为，尽管李亚普诺夫稳定性被广泛使用，但并不能保证对抗鲁棒性。受物理学原理的启发，我们提倡使用具有保守哈密顿神经流的GNNs来构建对抗攻击鲁棒的网络。在各种对抗攻击下，通过对几个基准数据集进行大量数值实验比较了不同神经流GNNs的对抗鲁棒性。实验证明，利用具有李亚普诺夫稳定性的保守哈密顿流的GNNs在对抗扰动方面大幅提高了鲁棒性。

    Graph neural networks (GNNs) are vulnerable to adversarial perturbations, including those that affect both node features and graph topology. This paper investigates GNNs derived from diverse neural flows, concentrating on their connection to various stability notions such as BIBO stability, Lyapunov stability, structural stability, and conservative stability. We argue that Lyapunov stability, despite its common use, does not necessarily ensure adversarial robustness. Inspired by physics principles, we advocate for the use of conservative Hamiltonian neural flows to construct GNNs that are robust to adversarial attacks. The adversarial robustness of different neural flow GNNs is empirically compared on several benchmark datasets under a variety of adversarial attacks. Extensive numerical experiments demonstrate that GNNs leveraging conservative Hamiltonian flows with Lyapunov stability substantially improve robustness against adversarial perturbations. The implementation code of experim
    
[^11]: 利用行政数据清单创建可靠的跨国参考作物类型监测数据库

    Harnessing Administrative Data Inventories to Create a Reliable Transnational Reference Database for Crop Type Monitoring. (arXiv:2310.06393v1 [cs.LG])

    [http://arxiv.org/abs/2310.06393](http://arxiv.org/abs/2310.06393)

    通过利用行政数据清单，本研究创建了一个跨国参考作物类型监测数据库，解决了大规模获取可靠高质量参考数据的问题。

    

    随着机器学习技术的飞速发展及其在地球观测挑战中的应用，该领域的性能得到了前所未有的提升。虽然这些方法的进一步发展以前受到传感器数据和计算资源的可用性和数量的限制，但缺乏足够的参考数据现在构成了新的瓶颈。由于创建这种地面真实信息是一项昂贵且容易出错的任务，必须想出新的方法在大规模上获取可靠高质量的参考数据。作为示例，我们展示了“E URO C ROPS”，这是一个作物类型分类的参考数据集，它聚合并协调了不同国家调查的行政数据，目的是实现跨国互操作性。

    With leaps in machine learning techniques and their applicationon Earth observation challenges has unlocked unprecedented performance across the domain. While the further development of these methods was previously limited by the availability and volume of sensor data and computing resources, the lack of adequate reference data is now constituting new bottlenecks. Since creating such ground-truth information is an expensive and error-prone task, new ways must be devised to source reliable, high-quality reference data on large scales. As an example, we showcase E URO C ROPS, a reference dataset for crop type classification that aggregates and harmonizes administrative data surveyed in different countries with the goal of transnational interoperability.
    
[^12]: 只需少量上下文示范即可实现越狱和对齐的语言模型

    Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations. (arXiv:2310.06387v1 [cs.LG])

    [http://arxiv.org/abs/2310.06387](http://arxiv.org/abs/2310.06387)

    本文研究了使用少量上下文示范来操纵语言模型对齐能力的方法。通过提供示范，而无需微调，可以增加或降低模型回答恶意提示的概率。我们提出了相同上下文攻击和相同上下文防御方法，用于越狱和对齐语言模型。实验证明了这些方法的有效性。

    

    大规模语言模型（LLM）在各种任务中取得了显著的成功，但对其安全性和生成恶意内容的潜在风险的担忧也浮现出来。本文探讨了在相同上下文学习（ICL）中操纵LLM对齐能力的效果。我们发现，仅通过少量的上下文示范而无需微调，就可以操纵LLM增加或降低越狱概率，即回答恶意提示。基于这些观察，我们提出了用于越狱和对齐语言模型目的的相同上下文攻击（ICA）和相同上下文防御（ICD）方法。ICA通过构造恶意上下文指导模型生成有害输出，而ICD通过拒绝回答有害提示的示范来增强模型的稳健性。我们的实验证明了ICA和ICD在增加或降低对抗越狱攻击成功率方面的有效性。总的来说，我们揭示了ICL在越狱和对齐语言模型领域的潜力。

    Large Language Models (LLMs) have shown remarkable success in various tasks, but concerns about their safety and the potential for generating malicious content have emerged. In this paper, we explore the power of In-Context Learning (ICL) in manipulating the alignment ability of LLMs. We find that by providing just few in-context demonstrations without fine-tuning, LLMs can be manipulated to increase or decrease the probability of jailbreaking, i.e. answering malicious prompts. Based on these observations, we propose In-Context Attack (ICA) and In-Context Defense (ICD) methods for jailbreaking and guarding aligned language model purposes. ICA crafts malicious contexts to guide models in generating harmful outputs, while ICD enhances model robustness by demonstrations of rejecting to answer harmful prompts. Our experiments show the effectiveness of ICA and ICD in increasing or reducing the success rate of adversarial jailbreaking attacks. Overall, we shed light on the potential of ICL t
    
[^13]: CAST：面向表格数据的群集感知自训练

    CAST: Cluster-Aware Self-Training for Tabular Data. (arXiv:2310.06380v1 [cs.LG])

    [http://arxiv.org/abs/2310.06380](http://arxiv.org/abs/2310.06380)

    本文提出了一种面向表格数据的群集感知自训练方法（CAST），通过规范伪标签的置信度，弥补了自训练算法中的一些弱点，具有普适性和适应性。

    

    自训练由于其简单和多功能性而受到吸引，然而它容易受到有噪音的伪标签的影响。几项研究提出了成功解决这个问题的方法，但它们削弱了自训练的优势，因为它们需要对自训练算法或模型架构进行特定的修改。此外，大多数方法与在表格领域中占主导地位的梯度提升决策树不兼容。为了解决这个问题，我们重新考虑了群集假设，即相互接近的数据样本往往属于同一类。在此假设的启发下，我们提出了一种针对表格数据的群集感知自训练（CAST）方法。CAST是一种简单且普遍适应的方法，可以改进现有的自训练算法而无需进行大幅修改。具体而言，我们的方法规范了分类器的置信度，即伪标签的值，强制在低密度区域对伪标签进行限制。

    Self-training has gained attraction because of its simplicity and versatility, yet it is vulnerable to noisy pseudo-labels. Several studies have proposed successful approaches to tackle this issue, but they have diminished the advantages of self-training because they require specific modifications in self-training algorithms or model architectures. Furthermore, most of them are incompatible with gradient boosting decision trees, which dominate the tabular domain. To address this, we revisit the cluster assumption, which states that data samples that are close to each other tend to belong to the same class. Inspired by the assumption, we propose Cluster-Aware Self-Training (CAST) for tabular data. CAST is a simple and universally adaptable approach for enhancing existing self-training algorithms without significant modifications. Concretely, our method regularizes the confidence of the classifier, which represents the value of the pseudo-label, forcing the pseudo-labels in low-density r
    
[^14]: Fourier神经操作符的初始化偏差：重新审视混沌边缘

    Initialization Bias of Fourier Neural Operator: Revisiting the Edge of Chaos. (arXiv:2310.06379v1 [cs.LG])

    [http://arxiv.org/abs/2310.06379](http://arxiv.org/abs/2310.06379)

    本文研究了Fourier神经操作符(FNO)的初始化偏差，提出了一种FNO版本的He初始化方案，通过模式截断和密集连接网络相似的特点，解决了训练不稳定的负初始化偏差问题。

    

    本文研究了Fourier神经操作符(FNO)的初始化偏差。建立了一个针对FNO的平均场理论，从“混沌边缘”的视角分析了随机FNO的行为。我们揭示了前向和反向传播行为表现出与FNO独特的特征，这是由模式截断引起的，同时也展示了与密集连接网络相似的特点。基于这一观察，我们还提出了一种FNO版本的He初始化方案，以减轻导致训练不稳定的负初始化偏差。实验结果显示了我们初始化方案的有效性，使得32层FNO的训练稳定，无需额外技术或显著性能下降。

    This paper investigates the initialization bias of the Fourier neural operator (FNO). A mean-field theory for FNO is established, analyzing the behavior of the random FNO from an ``edge of chaos'' perspective. We uncover that the forward and backward propagation behaviors exhibit characteristics unique to FNO, induced by mode truncation, while also showcasing similarities to those of densely connected networks. Building upon this observation, we also propose a FNO version of the He initialization scheme to mitigate the negative initialization bias leading to training instability. Experimental results demonstrate the effectiveness of our initialization scheme, enabling stable training of a 32-layer FNO without the need for additional techniques or significant performance degradation.
    
[^15]: 利用基于扩散的图像变化为鲁棒训练提供支持

    Leveraging Diffusion-Based Image Variations for Robust Training on Poisoned Data. (arXiv:2310.06372v1 [cs.CR])

    [http://arxiv.org/abs/2310.06372](http://arxiv.org/abs/2310.06372)

    该文提出了一种利用扩散模型的新方法，通过合成变化并结合知识蒸馏，使模型能够在可能被污染的数据集上进行鲁棒训练，解决后门攻击带来的安全威胁。

    

    后门攻击对训练神经网络构成严重安全威胁，它们在模型中秘密引入隐藏功能。这些后门在对干净输入进行推理时保持沉默，由于隐蔽的行为而避免被检测。然而，一旦输入数据中出现特定的触发模式，后门就会激活，导致模型执行其隐藏的功能。通过手动检查，在庞大的数据集中检测到这种被污染的样本几乎是不可能的。为了解决这个挑战，我们提出了一种新颖的方法，通过利用最近的扩散模型的强大功能，实现对可能被污染的数据集进行模型训练。具体而言，我们通过在所有训练样本上创建合成变化，利用扩散模型对数据中的潜在触发模式具有固有的弹性。通过将这种生成方法与知识蒸馏相结合，我们生成了保持任务上的总体性能的学生模型。

    Backdoor attacks pose a serious security threat for training neural networks as they surreptitiously introduce hidden functionalities into a model. Such backdoors remain silent during inference on clean inputs, evading detection due to inconspicuous behavior. However, once a specific trigger pattern appears in the input data, the backdoor activates, causing the model to execute its concealed function. Detecting such poisoned samples within vast datasets is virtually impossible through manual inspection. To address this challenge, we propose a novel approach that enables model training on potentially poisoned datasets by utilizing the power of recent diffusion models. Specifically, we create synthetic variations of all training samples, leveraging the inherent resilience of diffusion models to potential trigger patterns in the data. By combining this generative approach with knowledge distillation, we produce student models that maintain their general performance on the task while exhib
    
[^16]: 基于分区的差分隐私合成数据生成

    Partition-based differentially private synthetic data generation. (arXiv:2310.06371v1 [cs.CR])

    [http://arxiv.org/abs/2310.06371](http://arxiv.org/abs/2310.06371)

    本论文提出了一种基于分区的差分隐私合成数据生成方法，通过有效减小误差和提高合成数据质量，即使隐私预算有限，该方法仍然具有优势，并可改善质量和实用性。

    

    与摘要统计相比，私密合成数据共享更受青睐，因为它保留了原始数据的分布和细微差别。现有方法采用选择-度量-生成范式，但测量大领域边缘仍然会导致很大误差，迭代分配隐私预算仍然困难。为了解决这些问题，我们的方法采用了一种基于分区的方法，可以有效减小误差，提高合成数据的质量，即使隐私预算有限。我们的实验结果表明我们的方法优于现有方法。使用我们的方法产生的合成数据展现出改善的质量和实用性，使其成为私密合成数据共享的首选。

    Private synthetic data sharing is preferred as it keeps the distribution and nuances of original data compared to summary statistics. The state-of-the-art methods adopt a select-measure-generate paradigm, but measuring large domain marginals still results in much error and allocating privacy budget iteratively is still difficult. To address these issues, our method employs a partition-based approach that effectively reduces errors and improves the quality of synthetic data, even with a limited privacy budget. Results from our experiments demonstrate the superiority of our method over existing approaches. The synthetic data produced using our approach exhibits improved quality and utility, making it a preferable choice for private synthetic data sharing.
    
[^17]: 几何对齐转移编码器用于归纳转移回归任务

    Geometrically Aligned Transfer Encoder for Inductive Transfer in Regression Tasks. (arXiv:2310.06369v1 [cs.AI])

    [http://arxiv.org/abs/2310.06369](http://arxiv.org/abs/2310.06369)

    本文提出了一种基于微分几何的转移学习技术，通过将模型的潜在向量映射到黎曼曲面上的局部平坦坐标，实现在回归任务中的知识转移，并展示了该方法的优越性能和稳定行为。

    

    转移学习是处理可能与其他丰富数据相关的少量数据的关键技术。然而，现有大部分方法都集中在使用图像和语言数据集的分类任务上。因此，为了扩展转移学习方案到回归任务，我们提出了一种基于微分几何的新型转移技术，即几何对齐转移编码器（GATE）。在这种方法中，我们将模型的潜在向量解释为存在于黎曼曲面上的。我们找到了一种适当的微分同胚，确保每个任意点映射到重叠区域中的局部平坦坐标，从而实现从源数据到目标数据的知识转移。这也作为模型在外推区域行为良好的有效正则化器。在本文中，我们证明了GATE优于传统方法，并在潜在空间中表现出稳定的行为。

    Transfer learning is a crucial technique for handling a small amount of data that is potentially related to other abundant data. However, most of the existing methods are focused on classification tasks using images and language datasets. Therefore, in order to expand the transfer learning scheme to regression tasks, we propose a novel transfer technique based on differential geometry, namely the Geometrically Aligned Transfer Encoder (GATE). In this method, we interpret the latent vectors from the model to exist on a Riemannian curved manifold. We find a proper diffeomorphism between pairs of tasks to ensure that every arbitrary point maps to a locally flat coordinate in the overlapping region, allowing the transfer of knowledge from the source to the target data. This also serves as an effective regularizer for the model to behave in extrapolation regions. In this article, we demonstrate that GATE outperforms conventional methods and exhibits stable behavior in both the latent space 
    
[^18]: DrugCLIP: 用于虚拟筛选的对比蛋白质-分子表示学习

    DrugCLIP: Contrastive Protein-Molecule Representation Learning for Virtual Screening. (arXiv:2310.06367v1 [cs.LG])

    [http://arxiv.org/abs/2310.06367](http://arxiv.org/abs/2310.06367)

    DrugCLIP将虚拟筛选转化为对比学习任务，通过使用对比学习来对齐来自大量成对的蛋白质口袋和分子的表示，实现了虚拟筛选的加速。这项新方法不依赖于蛋白质分子的特定结合亲和力评分，而是利用大量数据进行表示学习，并引入了生物学知识启发的数据增强策略。

    

    虚拟筛选是AI辅助药物发现中的关键步骤，它从庞大的化合物数据库中识别出与特定蛋白质口袋结合的潜在药物。传统的对接方法耗时且只能在现实应用中使用受限的搜索库。近年来，使用评分函数进行结合亲和力预测的监督学习方法虽然有所改进，但由于对可靠结合亲和力标签的有限数据依赖性强，尚未超越对接方法。在本文中，我们提出了一种新颖的对比学习框架DrugCLIP，将虚拟筛选重新表述为密集检索任务，并利用对比学习来对齐毫不依赖于显式结合亲和力评分的大量成对数据中的蛋白质口袋和分子表示。我们还引入了一种受生物知识启发的数据增强策略，以学习更好的蛋白质-分子表示。

    Virtual screening, which identifies potential drugs from vast compound databases to bind with a particular protein pocket, is a critical step in AI-assisted drug discovery. Traditional docking methods are highly time-consuming, and can only work with a restricted search library in real-life applications. Recent supervised learning approaches using scoring functions for binding-affinity prediction, although promising, have not yet surpassed docking methods due to their strong dependency on limited data with reliable binding-affinity labels. In this paper, we propose a novel contrastive learning framework, DrugCLIP, by reformulating virtual screening as a dense retrieval task and employing contrastive learning to align representations of binding protein pockets and molecules from a large quantity of pairwise data without explicit binding-affinity scores. We also introduce a biological-knowledge inspired data augmentation strategy to learn better protein-molecule representations. Extensiv
    
[^19]: Core-Intermediate-Peripheral Index: 基于邻域和最短路径中心性度量的因子分析

    Core-Intermediate-Peripheral Index: Factor Analysis of Neighborhood and Shortest Paths-based Centrality Metrics. (arXiv:2310.06358v1 [cs.SI])

    [http://arxiv.org/abs/2310.06358](http://arxiv.org/abs/2310.06358)

    本论文提出了一种名为核心-中间-外围（CIP）指数的新的定量度量指标，通过对邻域和最短路径中心性度量数据进行因子分析，它能够捕捉节点在网络中扮演核心节点和外围节点的程度。作者通过测试在12个复杂现实世界网络上验证了该方法的有效性。

    

    我们对四个主要的邻域和最短路径中心性度量（度、特征向量、介数和接近度）的原始数据进行因子分析，并提出了一种新的定量度量指标，称为核心-中间-外围（CIP）指数，以捕捉节点在网络中能够扮演核心节点（具有任何中心性度量的较高值的网络中心）和外围节点（具有任何中心性度量的较低值的网络边缘节点）之间的关系程度。我们对中心性度量数据集的转置矩阵进行因子分析（基于特征向量的方差旋转），假设存在两个因子（核心和外围）来驱动节点相对于中心性度量所产生的值。我们在12个多样化的复杂现实世界网络上对我们的方法进行了测试。

    We perform factor analysis on the raw data of the four major neighborhood and shortest paths-based centrality metrics (Degree, Eigenvector, Betweeenness and Closeness) and propose a novel quantitative measure called the Core-Intermediate-Peripheral (CIP) Index to capture the extent with which a node could play the role of a core node (nodes at the center of a network with larger values for any centrality metric) vis-a-vis a peripheral node (nodes that exist at the periphery of a network with lower values for any centrality metric). We conduct factor analysis (varimax-based rotation of the Eigenvectors) on the transpose matrix of the raw centrality metrics dataset, with the node ids as features, under the hypothesis that there are two factors (core and peripheral) that drive the values incurred by the nodes with respect to the centrality metrics. We test our approach on a diverse suite of 12 complex real-world networks.
    
[^20]: 用一致性策略提升连续控制

    Boosting Continuous Control with Consistency Policy. (arXiv:2310.06343v1 [cs.LG])

    [http://arxiv.org/abs/2310.06343](http://arxiv.org/abs/2310.06343)

    该论文提出了一种名为一致性策略与Q-Learning （CPQL）的新方法，通过建立从反向扩散轨迹到期望策略的映射，同时解决了基于扩散模型方法的时间效率和准确指导问题。

    

    由于其训练稳定性和强大表达能力，扩散模型在离线强化学习中受到了相当大的关注。然而，它也带来了几个挑战：1）对大量扩散步骤的需求使得基于扩散模型的方法在实时控制中效率低下，限制了它们的应用；2）如何提供准确指导以实现基于扩散模型的策略改进仍然是一个未解决的问题。受一致性模型的启发，我们提出了一种新颖的时间效率方法，称为一致性策略与Q-Learning（CPQL），它通过单步从噪声中导出动作。通过建立从反向扩散轨迹到期望策略的映射，我们同时解决了使用学习到的Q函数更新基于扩散模型的策略时的时间效率和准确指导的问题。我们证明了CPQL可以通过准确指导实现离线强化学习的策略改进。

    Due to its training stability and strong expression, the diffusion model has attracted considerable attention in offline reinforcement learning. However, several challenges have also come with it: 1) The demand for a large number of diffusion steps makes the diffusion-model-based methods time inefficient and limits their applications in real-time control; 2) How to achieve policy improvement with accurate guidance for diffusion model-based policy is still an open problem. Inspired by the consistency model, we propose a novel time-efficiency method named Consistency Policy with Q-Learning (CPQL), which derives action from noise by a single step. By establishing a mapping from the reverse diffusion trajectories to the desired policy, we simultaneously address the issues of time efficiency and inaccurate guidance when updating diffusion model-based policy with the learned Q-function. We demonstrate that CPQL can achieve policy improvement with accurate guidance for offline reinforcement l
    
[^21]: 减少信息泄漏和计算的联邦学习

    Federated Learning with Reduced Information Leakage and Computation. (arXiv:2310.06341v1 [cs.LG])

    [http://arxiv.org/abs/2310.06341](http://arxiv.org/abs/2310.06341)

    Upcycled-FL是一种减少信息泄漏和计算的联邦学习框架，在每个偶数迭代中应用一阶近似，使得一半的联邦学习更新不会泄漏信息并且需要更少的计算。

    

    联邦学习是一种分布式学习范式，允许多个分散的客户端在不共享本地数据的情况下共同学习一个公共模型。尽管本地数据没有直接暴露，但仍存在隐私问题，因为客户端的敏感信息可以从中间计算中推断出来。此外，随着相同数据在迭代学习过程中的重复使用，这种信息泄漏会不断积累。因此，在设计保护隐私的联邦学习算法时，很难平衡隐私和准确性之间的权衡。在本文中，我们引入了一种新的联邦学习框架Upcycled-FL，它在每个偶数迭代中都应用了一阶近似。在这个框架下，一半的联邦学习更新不会造成信息泄漏，并且需要更少的计算。我们首先对Upcycled-FL的收敛（速率）进行理论分析，然后应用扰动机制来保护隐私。

    Federated learning (FL) is a distributed learning paradigm that allows multiple decentralized clients to collaboratively learn a common model without sharing local data. Although local data is not exposed directly, privacy concerns nonetheless exist as clients' sensitive information can be inferred from intermediate computations. Moreover, such information leakage accumulates substantially over time as the same data is repeatedly used during the iterative learning process. As a result, it can be particularly difficult to balance the privacy-accuracy trade-off when designing privacy-preserving FL algorithms. In this paper, we introduce Upcycled-FL, a novel federated learning framework with first-order approximation applied at every even iteration. Under this framework, half of the FL updates incur no information leakage and require much less computation. We first conduct the theoretical analysis on the convergence (rate) of Upcycled-FL, and then apply perturbation mechanisms to preserve
    
[^22]: 自动识别和区分超声视频中的结节，以便进行逐结节检查

    Automatic nodule identification and differentiation in ultrasound videos to facilitate per-nodule examination. (arXiv:2310.06339v1 [eess.IV])

    [http://arxiv.org/abs/2310.06339](http://arxiv.org/abs/2310.06339)

    本研究针对超声图像中结节异质外观导致的难以进行逐个结节检查的问题，构建了一个基于深度学习的结节重新识别系统，在数百个乳腺超声视频上取得了令人满意的结果。

    

    超声是健康筛查中重要的诊断技术，具有无创、经济、无辐射等优点，因此在结节的诊断中被广泛应用。然而，超声图像中，单个结节在不同的切面视图下可能呈现出异质的外观，这使得逐个结节检查变得困难。超声检查通常依赖于超声师的专业知识和临床经验。超声师通常通过检查结节特征和周围结构（如腺体和导管）来区分不同的结节，这是繁琐且耗时的。为了解决这个问题，我们收集了数百个乳腺超声视频，并建立了一个结节重新识别系统，包括基于深度学习模型的提取器，可以从输入视频片段中提取特征向量，以及实时聚类算法，可以自动将特征向量按结节分组。该系统获得了令人满意的结果。

    Ultrasound is a vital diagnostic technique in health screening, with the advantages of non-invasive, cost-effective, and radiation free, and therefore is widely applied in the diagnosis of nodules. However, it relies heavily on the expertise and clinical experience of the sonographer. In ultrasound images, a single nodule might present heterogeneous appearances in different cross-sectional views which makes it hard to perform per-nodule examination. Sonographers usually discriminate different nodules by examining the nodule features and the surrounding structures like gland and duct, which is cumbersome and time-consuming. To address this problem, we collected hundreds of breast ultrasound videos and built a nodule reidentification system that consists of two parts: an extractor based on the deep learning model that can extract feature vectors from the input video clips and a real-time clustering algorithm that automatically groups feature vectors by nodules. The system obtains satisfa
    
[^23]: 学习具有已知骨架的有界度多树

    Learning bounded-degree polytrees with known skeleton. (arXiv:2310.06333v1 [cs.LG])

    [http://arxiv.org/abs/2310.06333](http://arxiv.org/abs/2310.06333)

    本论文提出了一种高效学习已知骨架的有界度多树的算法，并给出了在多项式时间和样本复杂度内的有限样本保证。这对于复杂概率分布的学习具有重要意义。

    

    我们为高维概率分布的一类丰富的多树（polytrees）——有界度多树，建立了高效适当学习的有限样本保证。有界度多树是贝叶斯网络的子类，贝叶斯网络是一种广泛研究的图模型类型。最近，Bhattacharyya等人（2021）通过提供一种高效算法，在已知无向图（骨架）的情况下，为1-多树恢复了有限样本保证。我们通过扩展他们的结果，提供了一种高效算法，可以在多项式时间和样本复杂度内学习任何有界度的$d$-多树。我们将算法与信息论样本复杂度的下界结合起来，表明对维度和目标精度参数的依赖几乎是紧致的。

    We establish finite-sample guarantees for efficient proper learning of bounded-degree polytrees, a rich class of high-dimensional probability distributions and a subclass of Bayesian networks, a widely-studied type of graphical model. Recently, Bhattacharyya et al. (2021) obtained finite-sample guarantees for recovering tree-structured Bayesian networks, i.e., 1-polytrees. We extend their results by providing an efficient algorithm which learns $d$-polytrees in polynomial time and sample complexity for any bounded $d$ when the underlying undirected graph (skeleton) is known. We complement our algorithm with an information-theoretic sample complexity lower bound, showing that the dependence on the dimension and target accuracy parameters are nearly tight.
    
[^24]: 利用天线响应一致性定义CSI数据的对准标准

    Exploit the antenna response consistency to define the alignment criteria for CSI data. (arXiv:2310.06328v1 [cs.LG])

    [http://arxiv.org/abs/2310.06328](http://arxiv.org/abs/2310.06328)

    本论文提出了一个解决方案，利用天线响应一致性（ARC）来定义适当的对准标准，以解决在WiFi人体活动识别中的自我监督学习算法在CSI数据上无法达到预期性能的问题。

    

    自我监督学习（SSL）用于基于WiFi的人体活动识别（HAR）由于能够解决标注数据不足的挑战而具有很大的潜力。然而，直接将原本设计用于其他领域的SSL算法，特别是对比学习，移植到CSI数据上往往无法达到预期的性能。我们将这个问题归因于对准标准不当，这破坏了特征空间和输入空间之间的语义距离一致性。为了解决这个挑战，我们引入了``Anetenna Response Consistency (ARC)''作为定义合适对准标准的解决方案。ARC的设计在保留输入空间的语义信息的同时，引入了对现实世界噪声的鲁棒性。我们从CSI数据结构的角度分析了ARC，并展示了其最优解导致了从输入CSI数据到特征映射中的动作向量的直接映射。

    Self-supervised learning (SSL) for WiFi-based human activity recognition (HAR) holds great promise due to its ability to address the challenge of insufficient labeled data. However, directly transplanting SSL algorithms, especially contrastive learning, originally designed for other domains to CSI data, often fails to achieve the expected performance. We attribute this issue to the inappropriate alignment criteria, which disrupt the semantic distance consistency between the feature space and the input space. To address this challenge, we introduce \textbf{A}netenna \textbf{R}esponse \textbf{C}onsistency (ARC) as a solution to define proper alignment criteria. ARC is designed to retain semantic information from the input space while introducing robustness to real-world noise. We analyze ARC from the perspective of CSI data structure, demonstrating that its optimal solution leads to a direct mapping from input CSI data to action vectors in the feature map. Furthermore, we provide extensi
    
[^25]: 使用深度学习模型预测三种冻结步态事件

    Predicting Three Types of Freezing of Gait Events Using Deep Learning Models. (arXiv:2310.06322v1 [cs.LG])

    [http://arxiv.org/abs/2310.06322](http://arxiv.org/abs/2310.06322)

    该论文利用深度学习模型预测三种不同类型的冻结步态事件，最佳模型在测试数据上获得了0.427的得分，在Kaggle冻结步态预测竞赛中排名前五。

    

    冻结步态是帕金森病的症状之一，患者在行走时会周期性地出现不能迈步或转身的情况。虽然医学专家已经发现了多种引发和缓解冻结步态的方法，但其潜在原因和预测模型仍在研究中。目前利用机器学习的冻结步态预测模型可基于时间序列数据实现高敏感性和特异性，但这些模型缺乏对冻结步态事件类型的具体说明。我们开发了各种使用转换器编码器架构加上双向LSTM层和不同特征集的深度学习模型，以预测三种不同类型的冻结步态事件。最佳模型在测试数据上获得了0.427的得分，在由迈克尔·J·福克斯基金会主办的Kaggle冻结步态预测竞赛中排名前五。然而，我们也意识到了过拟合的问题

    Freezing of gait is a Parkinson's Disease symptom that episodically inflicts a patient with the inability to step or turn while walking. While medical experts have discovered various triggers and alleviating actions for freezing of gait, the underlying causes and prediction models are still being explored today. Current freezing of gait prediction models that utilize machine learning achieve high sensitivity and specificity in freezing of gait predictions based on time-series data; however, these models lack specifications on the type of freezing of gait events. We develop various deep learning models using the transformer encoder architecture plus Bidirectional LSTM layers and different feature sets to predict the three different types of freezing of gait events. The best performing model achieves a score of 0.427 on testing data, which would rank top 5 in Kaggle's Freezing of Gait prediction competition, hosted by THE MICHAEL J. FOX FOUNDATION. However, we also recognize overfitting 
    
[^26]: 基于迁移学习的物理约束卷积神经网络用于模拟具有时间变化控制的多孔介质中的流动

    Transfer learning-based physics-informed convolutional neural network for simulating flow in porous media with time-varying controls. (arXiv:2310.06319v1 [cs.LG])

    [http://arxiv.org/abs/2310.06319](http://arxiv.org/abs/2310.06319)

    提出了一种基于物理约束的卷积神经网络，用于模拟具有时间变化的井控制的多孔介质中的两相流动。网络通过时间变化控制对解进行参数化，建立了控制到状态的回归。网络结构采用两个平行的U-Net结构，输入为井控制，输出为系统状态。

    

    提出了一种基于物理约束的卷积神经网络，用于模拟具有时间变化的井控制的多孔介质中的两相流动。在现有文献中，大多数PICNN都是针对参数到状态映射进行工作的，而我们提出的网络通过时间变化控制对解进行参数化，建立了控制到状态的回归。首先，采用有限体积方案对流动方程进行离散化，并制定符合质量守恒定律的损失函数。Neumann边界条件无缝地融入半离散化方程中，因此不需要额外的损失项。网络结构由两个平行的U-Net结构组成，网络输入为井控制，输出为系统状态。为了捕捉输入和输出之间的时变关系，网络被设计成模拟离散化的状态空间方程。我们逐步训练网络的每个时间步，使其能够同时预测油压和...

    A physics-informed convolutional neural network is proposed to simulate two phase flow in porous media with time-varying well controls. While most of PICNNs in existing literatures worked on parameter-to-state mapping, our proposed network parameterizes the solution with time-varying controls to establish a control-to-state regression. Firstly, finite volume scheme is adopted to discretize flow equations and formulate loss function that respects mass conservation laws. Neumann boundary conditions are seamlessly incorporated into the semi-discretized equations so no additional loss term is needed. The network architecture comprises two parallel U-Net structures, with network inputs being well controls and outputs being the system states. To capture the time-dependent relationship between inputs and outputs, the network is well designed to mimic discretized state space equations. We train the network progressively for every timestep, enabling it to simultaneously predict oil pressure and
    
[^27]: 对抗性掩蔽图像修复在鲁棒检测 Mpox 和非 Mpox 中的应用

    Adversarial Masked Image Inpainting for Robust Detection of Mpox and Non-Mpox. (arXiv:2310.06318v1 [eess.IV])

    [http://arxiv.org/abs/2310.06318](http://arxiv.org/abs/2310.06318)

    本研究提出了一种对抗性掩蔽图像修复方法(MIM)，通过修复掩蔽的 Mpox 图像来学习 Mpox 图像表示，并通过测量修复图像与原始图像之间的相似性来判断输入是否属于 Mpox。这种方法有效解决了现有模型受到现实世界噪声干扰、需要多样化的非 Mpox 图像以及无法检测异常输入的问题。

    

    由于缺乏有效的 Mpox 诊断技术，Mpox 病例不断增加。最近，深度学习模型在检测 Mpox 和非 Mpox 方面显示出巨大潜力。然而，现有模型通过图像分类学习图像表示，结果可能容易受到现实世界噪声的干扰，需要多样化的非 Mpox 图像，并且无法检测异常输入。这些缺点使得分类模型在实际环境中无法应用。为了解决这些挑战，我们提出了“掩蔽、修复和测量”（MIM）方法。在 MIM 的流程中，生成对抗网络仅通过修复掩蔽的 Mpox 图像来学习 Mpox 图像表示。然后，MIM通过测量修复图像与原始图像之间的相似性来确定输入是否属于 Mpox。其基本思想是，由于 MIM 仅模拟 Mpox 图像，它在实际环境中无法准确修复非 Mpox 图像。

    Due to the lack of efficient mpox diagnostic technology, mpox cases continue to increase. Recently, the great potential of deep learning models in detecting mpox and non-mpox has been proven. However, existing models learn image representations via image classification, which results in they may be easily susceptible to interference from real-world noise, require diverse non-mpox images, and fail to detect abnormal input. These drawbacks make classification models inapplicable in real-world settings. To address these challenges, we propose "Mask, Inpainting, and Measure" (MIM). In MIM's pipeline, a generative adversarial network only learns mpox image representations by inpainting the masked mpox images. Then, MIM determines whether the input belongs to mpox by measuring the similarity between the inpainted image and the original image. The underlying intuition is that since MIM solely models mpox images, it struggles to accurately inpaint non-mpox images in real-world settings. Withou
    
[^28]: 从时间序列数据中发现混合结构因果模型

    Discovering Mixtures of Structural Causal Models from Time Series Data. (arXiv:2310.06312v1 [cs.LG])

    [http://arxiv.org/abs/2310.06312](http://arxiv.org/abs/2310.06312)

    这项研究提出了一种从时间序列数据中发现混合结构因果模型的方法，通过推断潜在因果模型以及每个样本属于特定混合成分的概率，通过实验证明了该方法在因果发现任务中的优越性。

    

    在金融、气候科学和神经科学等领域，从时间序列数据中推断因果关系是一个巨大的挑战。尽管现代技术可以处理变量之间的非线性关系和灵活的噪声分布，但它们依赖于简化假设，即数据来自相同的潜在因果模型。在这项工作中，我们放松了这个假设，从来源于不同因果模型混合的时间序列数据中进行因果发现。我们推断了潜在的结构性因果模型，以及每个样本属于特定混合成分的后验概率。我们的方法采用了一个端对端的训练过程，最大化了数据似然的证据下界。通过对合成和真实世界数据集的广泛实验，我们证明了我们的方法在因果发现任务中超越了最先进的基准方法，尤其是当数据来自不同的潜在因果模型时。

    In fields such as finance, climate science, and neuroscience, inferring causal relationships from time series data poses a formidable challenge. While contemporary techniques can handle nonlinear relationships between variables and flexible noise distributions, they rely on the simplifying assumption that data originates from the same underlying causal model. In this work, we relax this assumption and perform causal discovery from time series data originating from mixtures of different causal models. We infer both the underlying structural causal models and the posterior probability for each sample belonging to a specific mixture component. Our approach employs an end-to-end training process that maximizes an evidence-lower bound for data likelihood. Through extensive experimentation on both synthetic and real-world datasets, we demonstrate that our method surpasses state-of-the-art benchmarks in causal discovery tasks, particularly when the data emanates from diverse underlying causal
    
[^29]: 面向制造业的AI孵化的上下文强化学习集成活跃学习方法

    Ensemble Active Learning by Contextual Bandits for AI Incubation in Manufacturing. (arXiv:2310.06306v1 [cs.LG])

    [http://arxiv.org/abs/2310.06306](http://arxiv.org/abs/2310.06306)

    本论文提出了一种面向制造业的AI孵化的上下文强化学习集成活跃学习方法，通过在线更新AI模型的方式来持续改进决策。研究采用了一种名为CBEAL的集成主动学习方法，通过优化地指导数据获取，实现了数据效果的最小化。

    

    工业物联网系统中的在线感知和计算资源促进了基于AI的决策。然而，数据质量问题，如类别不平衡，阻碍了离线训练的AI模型。为了解决这个问题，AI模型会通过流式数据进行在线更新以持续改进。然而，由于注释约束，监督学习模型在选择用于更新的优质流式样本方面面临挑战。文献中的主动学习方法通过关注不足或过度表示的区域来提供解决方案。在不断变化的制造背景下平衡这些策略是具有挑战性的。AI学习到的一些获取准则可以动态适应，但可能无法始终处理频繁的变化。我们引入了一种集成主动学习方法CBEAL，专门利用主动学习代理进行探索或利用。代理的权重根据决策有效性进行调整。CBEAL可以优化地指导数据获取，实现数据效果的最小化。

    Online sensing and computational resources in Industrial Cyber-physical Systems (ICPS) facilitate AI-driven decision-making. Yet, issues with data quality, such as imbalanced classes, hinder AI models trained offline. To address this, AI models are updated online with streaming data for continuous improvement. Supervised learning models, however, face challenges in selecting quality streaming samples for updates due to annotation constraints. Active learning methods in literature offer solutions by focusing on under-represented or well-represented regions. Balancing these strategies in changing manufacturing contexts is challenging. Some acquisition criteria learned by AI dynamically adapt but may not consistently handle frequent changes. We introduce an ensemble active learning method, CBEAL, employing active learning agents specifically for exploration or exploitation. Weights of agents are adjusted based on agent decision effectiveness. CBEAL optimally guides data acquisition, minim
    
[^30]: 超叠加的玩具模型中的动力学与贝叶斯相变

    Dynamical versus Bayesian Phase Transitions in a Toy Model of Superposition. (arXiv:2310.06301v1 [cs.LG])

    [http://arxiv.org/abs/2310.06301](http://arxiv.org/abs/2310.06301)

    本研究通过奇异学习理论研究超叠加的玩具模型中的相变，在两个隐藏维度的情况下发现正则的$k$-gons是临界点，并提供支持理论表明这些临界点决定了贝叶斯后验的相变。此外，实验证明这些临界点也决定了SGD训练的行为。研究结果支持了SGD学习轨迹受顺序学习机制影响的猜想。

    

    我们使用奇异学习理论（SLT）研究超叠加的玩具模型（TMS）中的相变。我们推导出了理论损失的闭式公式，并在两个隐藏维度的情况下发现，正则的$k$-gons是临界点。我们提出了支持理论，表明这些$k$-gons的本地学习系数（几何不变量）决定了贝叶斯后验作为训练样本大小的函数的相变。然后，我们凭经验证明，同样的$k$-gon临界点也决定了SGD训练的行为。得出的结论支持了SGD学习轨迹受顺序学习机制影响的猜想。具体而言，我们发现TMS中的学习过程，无论是通过SGD还是贝叶斯学习，可以被描述为在参数空间中从高损失和低复杂性的区域向低损失和高复杂性的区域的旅程。

    We investigate phase transitions in a Toy Model of Superposition (TMS) using Singular Learning Theory (SLT). We derive a closed formula for the theoretical loss and, in the case of two hidden dimensions, discover that regular $k$-gons are critical points. We present supporting theory indicating that the local learning coefficient (a geometric invariant) of these $k$-gons determines phase transitions in the Bayesian posterior as a function of training sample size. We then show empirically that the same $k$-gon critical points also determine the behavior of SGD training. The picture that emerges adds evidence to the conjecture that the SGD learning trajectory is subject to a sequential learning mechanism. Specifically, we find that the learning process in TMS, be it through SGD or Bayesian learning, can be characterized by a journey through parameter space from regions of high loss and low complexity to regions of low loss and high complexity.
    
[^31]: Gem5Pred: Gem5模拟时间的预测方法

    Gem5Pred: Predictive Approaches For Gem5 Simulation Time. (arXiv:2310.06290v1 [cs.AR])

    [http://arxiv.org/abs/2310.06290](http://arxiv.org/abs/2310.06290)

    本文提出了Gem5Pred，一种用于预测Gem5模拟时间的方法，并引入了一个独特的数据集。通过分析不同指令类型对模拟时间的影响，利用三种不同的模型进行预测任务，其中回归模型的平均绝对误差为0.546，分类模型的准确率为0.696。这些模型为未来研究提供了基础和参考。

    

    Gem5是一个开源、灵活且成本效益高的模拟器，在学术界和工业界的硬件模拟中得到广泛认可和应用。然而，Gem5模拟程序通常耗时较长，因此需要一个可以估计模拟时间的预测模型。目前还没有这样的数据集或模型存在。针对这一空白，本文通过引入专门为此目的创建的独特数据集，提出了一种新颖的贡献。我们还对不同指令类型对Gem5模拟时间的影响进行了分析。在此基础上，我们利用三种不同的模型借助CodeBERT执行基于开发数据集的预测任务。我们优越的回归模型实现了0.546的平均绝对误差（MAE），而我们表现最佳的分类模型记录了0.696的准确率。我们的模型为未来对这个主题的研究建立了基础，同时也作为后续模型的基准。

    Gem5, an open-source, flexible, and cost-effective simulator, is widely recognized and utilized in both academic and industry fields for hardware simulation. However, the typically time-consuming nature of simulating programs on Gem5 underscores the need for a predictive model that can estimate simulation time. As of now, no such dataset or model exists. In response to this gap, this paper makes a novel contribution by introducing a unique dataset specifically created for this purpose. We also conducted analysis of the effects of different instruction types on the simulation time in Gem5. After this, we employ three distinct models leveraging CodeBERT to execute the prediction task based on the developed dataset. Our superior regression model achieves a Mean Absolute Error (MAE) of 0.546, while our top-performing classification model records an Accuracy of 0.696. Our models establish a foundation for future investigations on this topic, serving as benchmarks against which subsequent mo
    
[^32]: 更好、更简单的差分隐私统计估计的下界

    Better and Simpler Lower Bounds for Differentially Private Statistical Estimation. (arXiv:2310.06289v1 [math.ST])

    [http://arxiv.org/abs/2310.06289](http://arxiv.org/abs/2310.06289)

    本文提出了更好、更简单的差分隐私统计估计的下界，适用于估计高斯协方差的谱误差和有界$k$阶矩的重尾分布的均值估计。

    

    我们为两个众所周知的高维私有估计任务提供了改进的下界。首先，我们证明，在近似差分隐私的情况下，对于用于估计高斯协方差的谱误差$\alpha$，任意$\alpha \le O(1)$，需要$\tilde{\Omega} \left(\frac{d^{3/2}}{\alpha \varepsilon} + \frac{d}{\alpha^2}\right)$个样本，这是紧凑的，仅差对数因子。这比先前的工作在$\alpha \le O\left(\frac{1}{\sqrt{d}}\right)$时建立的结果要好，并且比之前的工作更简单。接下来，我们证明，在近似差分隐私的情况下，对于有界$k$阶矩的重尾分布的均值估计，需要$\tilde{\Omega}\left(\frac{d}{\alpha^{k/(k-1)} \varepsilon} + \frac{d}{\alpha^2}\right)$个样本。这与已知的上界相吻合，并改进了此问题的已知最佳下界，该下界仅适用于纯粹的差分隐私或$k = 2$的情况。我们的技术遵循指纹方式的方法。

    We provide improved lower bounds for two well-known high-dimensional private estimation tasks. First, we prove that for estimating the covariance of a Gaussian up to spectral error $\alpha$ with approximate differential privacy, one needs $\tilde{\Omega}\left(\frac{d^{3/2}}{\alpha \varepsilon} + \frac{d}{\alpha^2}\right)$ samples for any $\alpha \le O(1)$, which is tight up to logarithmic factors. This improves over previous work which established this for $\alpha \le O\left(\frac{1}{\sqrt{d}}\right)$, and is also simpler than previous work. Next, we prove that for estimating the mean of a heavy-tailed distribution with bounded $k$th moments with approximate differential privacy, one needs $\tilde{\Omega}\left(\frac{d}{\alpha^{k/(k-1)} \varepsilon} + \frac{d}{\alpha^2}\right)$ samples. This matches known upper bounds and improves over the best known lower bound for this problem, which only hold for pure differential privacy, or when $k = 2$. Our techniques follow the method of fingerpr
    
[^33]: 通过对抗行为抑制Q学习中的过高估计

    Suppressing Overestimation in Q-Learning through Adversarial Behaviors. (arXiv:2310.06286v1 [cs.LG])

    [http://arxiv.org/abs/2310.06286](http://arxiv.org/abs/2310.06286)

    本文提出了一种新的Q学习算法，通过引入虚拟对抗性玩家，有效调节了标准Q学习中的过高估计偏差，提出的算法简单而有效，能够轻松应用于强化学习算法并提高性能。

    

    本文旨在提出一种新的Q学习算法，使用一个虚拟对抗性玩家，称为虚拟对抗性Q学习（DAQ），以有效地调节标准Q学习中的过高估计偏差。通过虚拟玩家，学习可以被表述为一个双人零和博弈。所提出的DAQ将几种Q学习的变体统一到一个单一的框架中，以控制过高估计偏差，例如maxmin Q学习和minmax Q学习（本文提出）。通过虚拟对抗性行为，所提出的DAQ是一种简单而有效的方式，可以轻松应用于现成的强化学习算法，以提高性能。通过调整对抗性Q学习，从综合的角度分析了DAQ的有限时间收敛性。在各种基准环境下，实证验证了所提出DAQ的性能。

    The goal of this paper is to propose a new Q-learning algorithm with a dummy adversarial player, which is called dummy adversarial Q-learning (DAQ), that can effectively regulate the overestimation bias in standard Q-learning. With the dummy player, the learning can be formulated as a two-player zero-sum game. The proposed DAQ unifies several Q-learning variations to control overestimation biases, such as maxmin Q-learning and minmax Q-learning (proposed in this paper) in a single framework. The proposed DAQ is a simple but effective way to suppress the overestimation bias thourgh dummy adversarial behaviors and can be easily applied to off-the-shelf reinforcement learning algorithms to improve the performances. A finite-time convergence of DAQ is analyzed from an integrated perspective by adapting an adversarial Q-learning. The performance of the suggested DAQ is empirically demonstrated under various benchmark environments.
    
[^34]: MuseChat:一种视频对话音乐推荐系统

    MuseChat: A Conversational Music Recommendation System for Videos. (arXiv:2310.06282v1 [cs.LG])

    [http://arxiv.org/abs/2310.06282](http://arxiv.org/abs/2310.06282)

    MuseChat是一种创新的对话式音乐推荐系统，通过模拟用户和推荐系统之间的对话交互，利用预训练的音乐标签和艺术家信息，为用户提供定制的音乐推荐，使用户可以个性化选择他们喜欢的音乐。

    

    我们引入了MuseChat，一种创新的基于对话的音乐推荐系统。这个独特的平台不仅提供互动用户参与，还为输入的视频提供了定制的音乐推荐，使用户可以改进和个性化他们的音乐选择。与之相反，以前的系统主要强调内容的兼容性，往往忽视了用户个体偏好的细微差别。例如，所有的数据集都只提供基本的音乐-视频配对，或者带有音乐描述的配对。为了填补这一空白，我们的研究提供了三个贡献。首先，我们设计了一种对话合成方法，模拟了用户和推荐系统之间的两轮交互，利用预训练的音乐标签和艺术家信息。在这个交互中，用户提交一个视频给系统，系统会提供一个合适的音乐片段，并附带解释。之后，用户会表达他们对音乐的偏好，系统会呈现一个改进后的音乐推荐

    We introduce MuseChat, an innovative dialog-based music recommendation system. This unique platform not only offers interactive user engagement but also suggests music tailored for input videos, so that users can refine and personalize their music selections. In contrast, previous systems predominantly emphasized content compatibility, often overlooking the nuances of users' individual preferences. For example, all the datasets only provide basic music-video pairings or such pairings with textual music descriptions. To address this gap, our research offers three contributions. First, we devise a conversation-synthesis method that simulates a two-turn interaction between a user and a recommendation system, which leverages pre-trained music tags and artist information. In this interaction, users submit a video to the system, which then suggests a suitable music piece with a rationale. Afterwards, users communicate their musical preferences, and the system presents a refined music recomme
    
[^35]: BC4LLM：当区块链遇见大型语言模型时的可信人工智能

    BC4LLM: Trusted Artificial Intelligence When Blockchain Meets Large Language Models. (arXiv:2310.06278v1 [cs.NI])

    [http://arxiv.org/abs/2310.06278](http://arxiv.org/abs/2310.06278)

    本论文提出了使用区块链技术增强大型语言模型（LLM）安全性的愿景，以解决AI生成内容的真实性和隐私泄露问题。

    

    近年来，人工智能（AI）和机器学习（ML）正在重新塑造社会的生产方式和生产力，并改变科学研究的范 paradigm。其中，以ChatGPT为代表的AI语言模型取得了巨大进展。这种大型语言模型（LLM）以AI生成的内容（AIGC）的形式服务于人们，并广泛应用于咨询、医疗和教育。然而，很难保证AIGC学习数据的真实性和可靠性。此外，分布式AI训练中也存在隐患的隐私泄露问题。此外，LLMs生成的内容很难识别和追踪，难以跨平台相互认可。在以LLMs为动力的AI时代即将到来之际，上述信息安全问题将被无限放大，影响每个人的生活。因此，我们考虑利用区块链技术为LLMs赋予卓越的安全特性，提出了一个愿景。

    In recent years, artificial intelligence (AI) and machine learning (ML) are reshaping society's production methods and productivity, and also changing the paradigm of scientific research. Among them, the AI language model represented by ChatGPT has made great progress. Such large language models (LLMs) serve people in the form of AI-generated content (AIGC) and are widely used in consulting, healthcare, and education. However, it is difficult to guarantee the authenticity and reliability of AIGC learning data. In addition, there are also hidden dangers of privacy disclosure in distributed AI training. Moreover, the content generated by LLMs is difficult to identify and trace, and it is difficult to cross-platform mutual recognition. The above information security issues in the coming era of AI powered by LLMs will be infinitely amplified and affect everyone's life. Therefore, we consider empowering LLMs using blockchain technology with superior security features to propose a vision for
    
[^36]: 让模型说密文: 通过嵌入进行多智能体辩论

    Let Models Speak Ciphers: Multiagent Debate through Embeddings. (arXiv:2310.06272v1 [cs.CL])

    [http://arxiv.org/abs/2310.06272](http://arxiv.org/abs/2310.06272)

    本文引入了一种名为CIPHER的通信机制，通过去除LLMs中的标记采样步骤，让模型可以通过期望的原始Transformer输出嵌入来传达其信念，从而解决了在自然语言生成中可能存在的信息丢失风险，并提供了编码更广泛信息的优势。

    

    最近，对大型语言模型（LLMs）之间的讨论和辩论引起了广泛关注，因为它们有潜力增强LLMs的推理能力。尽管自然语言由于LLMs的语言理解能力而成为明显的交流选择，但生成自然语言时需要进行的标记采样步骤可能存在信息丢失的潜在风险，因为它仅使用一个标记来代表模型在整个词汇表中的信念。在本文中，我们介绍了一种名为CIPHER（通过嵌入表示进行交流的网络模型协议）的通信机制来解决这个问题。具体来说，我们从LLMs中去除了标记采样步骤，让它们通过原始Transformer输出嵌入的期望来传达它们的信念。值得注意的是，通过偏离自然语言，CIPHER在不对模型权重进行任何修改的情况下，提供了编码更广泛信息的优势。

    Discussion and debate among Large Language Models (LLMs) have gained considerable attention due to their potential to enhance the reasoning ability of LLMs. Although natural language is an obvious choice for communication due to LLM's language understanding capability, the token sampling step needed when generating natural language poses a potential risk of information loss, as it uses only one token to represent the model's belief across the entire vocabulary. In this paper, we introduce a communication regime named CIPHER (Communicative Inter-Model Protocol Through Embedding Representation) to address this issue. Specifically, we remove the token sampling step from LLMs and let them communicate their beliefs across the vocabulary through the expectation of the raw transformer output embeddings. Remarkably, by deviating from natural language, CIPHER offers an advantage of encoding a broader spectrum of information without any modification to the model weights. While the state-of-the-a
    
[^37]: 有限探索条件下的双层离线策略优化

    Bi-Level Offline Policy Optimization with Limited Exploration. (arXiv:2310.06268v1 [cs.LG])

    [http://arxiv.org/abs/2310.06268](http://arxiv.org/abs/2310.06268)

    本文提出了一种双层离线策略优化算法，通过模拟策略和值函数之间的层次交互，解决了离线强化学习中数据集缺乏探索所导致的分布偏移问题。该算法通过构建置信区间和最大化保守估计值来提高策略的性能。

    

    本文研究了基于已知数据集学习良好策略的离线强化学习。在函数逼近的情况下，由于数据集缺乏足够的探索，导致了分布偏移的困难。为了解决这个问题，我们提出了一种双层结构的策略优化算法，该算法模拟了策略（上层）和值函数（下层）之间的层次性交互。下层的重点是构建一个置信区间，以保持权重平均Bellman误差足够小，同时控制由分布不匹配引起的不确定性。随后，在上层，策略旨在最大化下层形成的置信区间中的保守估计值。这种新颖的表述保留了隐式引导的探索数据分布的最大灵活性，使得模型推广的能力得以发挥。在实践中，我们通过数值实验验证了我们的算法的有效性。

    We study offline reinforcement learning (RL) which seeks to learn a good policy based on a fixed, pre-collected dataset. A fundamental challenge behind this task is the distributional shift due to the dataset lacking sufficient exploration, especially under function approximation. To tackle this issue, we propose a bi-level structured policy optimization algorithm that models a hierarchical interaction between the policy (upper-level) and the value function (lower-level). The lower level focuses on constructing a confidence set of value estimates that maintain sufficiently small weighted average Bellman errors, while controlling uncertainty arising from distribution mismatch. Subsequently, at the upper level, the policy aims to maximize a conservative value estimate from the confidence set formed at the lower level. This novel formulation preserves the maximum flexibility of the implicitly induced exploratory data distribution, enabling the power of model extrapolation. In practice, it
    
[^38]: CodeFuse-13B: 一个预训练的多语言代码大型语言模型

    CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model. (arXiv:2310.06266v1 [cs.SE])

    [http://arxiv.org/abs/2310.06266](http://arxiv.org/abs/2310.06266)

    CodeFuse-13B是一个预训练的多语言代码大型语言模型，专为代码相关任务设计，支持超过40种编程语言，并通过使用高质量的预训练数据集以及大量实验的验证，展现了其在多语言输入下的有效性。

    

    代码大型语言模型(Code LLMs)因其在软件工程全生命周期中的广泛应用而受到工业界的广泛关注。然而，现有模型在理解非英语输入的多语言代码相关任务方面的效果仍然远未被充分研究。本文介绍了CodeFuse-13B，一个开源的预训练代码LLM。它专为包含英文和中文提示的代码相关任务而设计，并支持超过40种编程语言。CodeFuse通过利用由程序分析器精心筛选并在训练过程中优化的高质量预训练数据集来实现其效果。我们进行了大量实验，包括使用真实世界的使用场景、工业标准基准HumanEval-x，以及专为中文提示设计的CodeFuseEval。为了评估CodeFuse的有效性，我们积极收集了AntGroup软件开发团队的宝贵人工反馈。

    Code Large Language Models (Code LLMs) have gained significant attention in the industry due to their wide applications in the full lifecycle of software engineering. However, the effectiveness of existing models in understanding non-English inputs for multi-lingual code-related tasks is still far from well studied. This paper introduces CodeFuse-13B, an open-sourced pre-trained code LLM. It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages. CodeFuse achieves its effectiveness by utilizing a high quality pre-training dataset that is carefully filtered by program analyzers and optimized during the training process. Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HumanEval-x, and the specially designed CodeFuseEval for Chinese prompts. To assess the effectiveness of CodeFuse, we actively collected valuable human feedback from the AntGroup's software develop
    
[^39]: 自我区分建模用于异常图检测

    Self-Discriminative Modeling for Anomalous Graph Detection. (arXiv:2310.06261v1 [cs.LG])

    [http://arxiv.org/abs/2310.06261](http://arxiv.org/abs/2310.06261)

    本文提出了一种自我区分建模框架用于基于正常图训练的异常图检测，通过生成插值的伪异常图，与几种最先进的基线算法相比，取得了显著的改进。

    

    本文研究了使用仅基于正常图训练的机器学习模型来检测异常图的问题，这在分子、生物和社交网络数据分析中有许多应用。我们提出了一种自我区分建模框架用于异常图检测。其核心思想是通过学习一个从给定的正常图和通过联合训练生成的伪异常图中的判别器（分类器），从而使得生成的伪异常图在正常图和真实异常图之间插值。在该框架下，我们提供了三种具有不同计算效率和稳定性的算法用于异常图检测。这三种算法与几种最先进的基于图级别的异常检测基线在九个流行的图数据集上进行比较（其中四个数据集规模较小，五个数据集规模适中），并展示了显著的改进。

    This paper studies the problem of detecting anomalous graphs using a machine learning model trained on only normal graphs, which has many applications in molecule, biology, and social network data analysis. We present a self-discriminative modeling framework for anomalous graph detection. The key idea, mathematically and numerically illustrated, is to learn a discriminator (classifier) from the given normal graphs together with pseudo-anomalous graphs generated by a model jointly trained, where we never use any true anomalous graphs and we hope that the generated pseudo-anomalous graphs interpolate between normal ones and (real) anomalous ones. Under the framework, we provide three algorithms with different computational efficiencies and stabilities for anomalous graph detection. The three algorithms are compared with several state-of-the-art graph-level anomaly detection baselines on nine popular graph datasets (four with small size and five with moderate size) and show significant im
    
[^40]: 面向模型驱动的强化学习中目标不匹配问题的统一观点

    A Unified View on Solving Objective Mismatch in Model-Based Reinforcement Learning. (arXiv:2310.06253v1 [cs.LG])

    [http://arxiv.org/abs/2310.06253](http://arxiv.org/abs/2310.06253)

    这项工作提供了一个关于解决模型驱动的强化学习中目标不匹配问题的统一观点，对解决方案进行了分类，并提出了一个分类法以促进未来的研究。

    

    模型驱动的强化学习（MBRL）旨在通过学习环境的显式模型使代理更节约样本、适应性更强和更易解释。虽然近年来MBRL代理的能力有了显著提升，但如何最好地学习模型仍然是一个未解决的问题。大多数MBRL算法的目标是训练模型以对环境进行准确预测，然后使用模型确定最有益的动作。然而，最近的研究表明，模型的预测准确性通常与动作质量不相关，将根本原因归结为准确的动态模型学习与奖励策略优化之间的“目标不匹配”。随着MBRL作为一个研究领域的不断成熟，涌现出了一些互相关联的解决目标不匹配问题的解决方案类别。在本文中，我们对这些解决方案类别进行了深入调查，并提出了一个分类法以促进未来的研究。

    Model-based Reinforcement Learning (MBRL) aims to make agents more sample-efficient, adaptive, and explainable by learning an explicit model of the environment. While the capabilities of MBRL agents have significantly improved in recent years, how to best learn the model is still an unresolved question. The majority of MBRL algorithms aim at training the model to make accurate predictions about the environment and subsequently using the model to determine the most rewarding actions. However, recent research has shown that model predictive accuracy is often not correlated with action quality, tracing the root cause to the \emph{objective mismatch} between accurate dynamics model learning and policy optimization of rewards. A number of interrelated solution categories to the objective mismatch problem have emerged as MBRL continues to mature as a research area. In this work, we provide an in-depth survey of these solution categories and propose a taxonomy to foster future research.
    
[^41]: 深度学习：一篇教程

    Deep Learning: A Tutorial. (arXiv:2310.06251v1 [stat.ML])

    [http://arxiv.org/abs/2310.06251](http://arxiv.org/abs/2310.06251)

    该论文讲述了深度学习方法对结构化高维数据的洞察和预测规则，通过使用多层半仿射输入转换来提供预测规则并找到特征。

    

    我们的目标是提供对深度学习方法的回顾，以揭示对结构化高维数据的洞察。与大多数统计模型常用的浅层加性结构不同，深度学习使用多层半仿射输入转换来提供预测规则。应用这些转换层导致一组属性（或特征），可以应用概率统计方法。因此，我们可以同时达到可扩展的预测规则和不确定性量化，其中稀疏正则化找到了特征。

    Our goal is to provide a review of deep learning methods which provide insight into structured high-dimensional data. Rather than using shallow additive architectures common to most statistical models, deep learning uses layers of semi-affine input transformations to provide a predictive rule. Applying these layers of transformations leads to a set of attributes (or, features) to which probabilistic statistical methods can be applied. Thus, the best of both worlds can be achieved: scalable prediction rules fortified with uncertainty quantification, where sparse regularization finds the features.
    
[^42]: 高样本效率的多智能体强化学习：优化视角

    Sample-Efficient Multi-Agent RL: An Optimization Perspective. (arXiv:2310.06243v1 [cs.LG])

    [http://arxiv.org/abs/2310.06243](http://arxiv.org/abs/2310.06243)

    该论文研究了多智能体强化学习中的高样本效率问题，在一般函数逼近下，提出了一个基于多智能体解耦系数的算法框架，实现了低复杂度下的学习纳什均衡、粗粒度相关均衡和相关均衡。该算法在亚线性后悔方面表现具有可比性。

    

    我们研究了在一般函数逼近下，针对一般和的马尔可夫博弈问题（Markov Games，MGs）的多智能体强化学习（MARL）。为了找到样本效率学习的最小假设，我们引入了一个称为多智能体解耦系数（MADC）的新的复杂度度量方法。利用这个度量方法，我们提出了一个统一的算法框架，确保在低MADC条件下学习纳什均衡、粗粒度相关均衡和相关均衡的模型基于和无模型MARL问题。我们还证明了我们的算法提供了与现有工作相媲美的亚线性后悔。此外，我们的算法将均衡求解预测器与一个求解每个确定性联合策略的正则化收益的内置优化子过程相结合，避免了在数据相关约束条件下求解约束优化问题或执行采样的问题。

    We study multi-agent reinforcement learning (MARL) for the general-sum Markov Games (MGs) under the general function approximation. In order to find the minimum assumption for sample-efficient learning, we introduce a novel complexity measure called the Multi-Agent Decoupling Coefficient (MADC) for general-sum MGs. Using this measure, we propose the first unified algorithmic framework that ensures sample efficiency in learning Nash Equilibrium, Coarse Correlated Equilibrium, and Correlated Equilibrium for both model-based and model-free MARL problems with low MADC. We also show that our algorithm provides comparable sublinear regret to the existing works. Moreover, our algorithm combines an equilibrium-solving oracle with a single objective optimization subprocedure that solves for the regularized payoff of each deterministic joint policy, which avoids solving constrained optimization problems within data-dependent constraints (Jin et al. 2020; Wang et al. 2023) or executing sampling p
    
[^43]: 从数据中发现可解释的动力学系统的拉格朗日贝叶斯框架

    A Bayesian framework for discovering interpretable Lagrangian of dynamical systems from data. (arXiv:2310.06241v1 [stat.ML])

    [http://arxiv.org/abs/2310.06241](http://arxiv.org/abs/2310.06241)

    这项研究提出了一种使用稀疏贝叶斯方法从有限数据中学习可解释的拉格朗日描述物理系统的框架，并通过勒让德变换自动提取哈密顿描述。

    

    学习和预测物理系统的动力学需要对基本物理定律有深入的理解。最近的研究将方程发现的框架推广到物理系统的哈密顿和拉格朗日的发现。虽然现有的方法使用神经网络对拉格朗日进行参数化，但我们提出了一种使用稀疏贝叶斯方法从有限的数据中学习可解释的拉格朗日描述物理系统的替代框架。与现有的神经网络方法不同，所提出的方法(a)得到了可解释的拉格朗日描述，(b)利用贝叶斯学习来量化由于有限数据而导致的认知不确定性，(c)通过勒让德变换自动提取从学习到的拉格朗日得到的哈密顿描述，(d)提供基于常微分方程（ODE）和偏微分方程（PDE）的观测系统描述。涉及六个不同的例子，包括两种情况。

    Learning and predicting the dynamics of physical systems requires a profound understanding of the underlying physical laws. Recent works on learning physical laws involve generalizing the equation discovery frameworks to the discovery of Hamiltonian and Lagrangian of physical systems. While the existing methods parameterize the Lagrangian using neural networks, we propose an alternate framework for learning interpretable Lagrangian descriptions of physical systems from limited data using the sparse Bayesian approach. Unlike existing neural network-based approaches, the proposed approach (a) yields an interpretable description of Lagrangian, (b) exploits Bayesian learning to quantify the epistemic uncertainty due to limited data, (c) automates the distillation of Hamiltonian from the learned Lagrangian using Legendre transformation, and (d) provides ordinary (ODE) and partial differential equation (PDE) based descriptions of the observed systems. Six different examples involving both di
    
[^44]: 解决MUSIC-AVQA中的数据偏差问题：为无偏问答创建一个平衡的数据集

    Tackling Data Bias in MUSIC-AVQA: Crafting a Balanced Dataset for Unbiased Question-Answering. (arXiv:2310.06238v1 [cs.CV])

    [http://arxiv.org/abs/2310.06238](http://arxiv.org/abs/2310.06238)

    本论文解决了MUSIC-AVQA中的数据偏差问题，通过创建一个平衡的数据集来保证模型能够有效地推理各种多模态情况下的问题。他们构建了一个名为MUSIC-AVQA v2.0的新数据集，并提出了一个新的基准模型。

    

    近年来，对音频、视觉和文本模态的交叉研究越来越受重视，推动了多模态研究的进展。然而，任何模态中存在的强烈偏见会导致模型忽视其他模态。因此，模型有效地跨越这些多样化模态进行推理的能力受到损害，阻碍了进一步的发展。在本文中，我们详细审查了原始数据集中的每种问题类型，选择具有明显答案偏见的问题。为了解决这些偏见，我们收集了互补的视频和问题，确保没有答案有明显的偏斜分布。特别是对于二元问题，我们努力确保每个问题类别中两个答案几乎均匀分布。因此，我们构建了一个名为MUSIC-AVQA v2.0的新数据集，它更具挑战性，我们相信能够更好地促进AVQA任务的进展。此外，我们提出了一种新的基准模型。

    In recent years, there has been a growing emphasis on the intersection of audio, vision, and text modalities, driving forward the advancements in multimodal research. However, strong bias that exists in any modality can lead to the model neglecting the others. Consequently, the model's ability to effectively reason across these diverse modalities is compromised, impeding further advancement. In this paper, we meticulously review each question type from the original dataset, selecting those with pronounced answer biases. To counter these biases, we gather complementary videos and questions, ensuring that no answers have outstanding skewed distribution. In particular, for binary questions, we strive to ensure that both answers are almost uniformly spread within each question category. As a result, we construct a new dataset, named MUSIC-AVQA v2.0, which is more challenging and we believe could better foster the progress of AVQA task. Furthermore, we present a novel baseline model that de
    
[^45]: 大型Vision Transformer的高效适应性通过Adapter重组

    Efficient Adaptation of Large Vision Transformer via Adapter Re-Composing. (arXiv:2310.06234v1 [cs.CV])

    [http://arxiv.org/abs/2310.06234](http://arxiv.org/abs/2310.06234)

    本研究提出了一种名为Adapter重组（ARC）的策略，旨在从一种新的角度解决高效预训练模型适应的问题。该方法通过考虑适应参数的可重用性和引入参数共享方案，利用对称的投影操作来构建共享的瓶颈操作，并通过学习低维度的重新缩放系数来有效重新组合层适应参数。

    

    高容量预训练模型的出现彻底改变了计算机视觉问题解决的方式，将焦点从训练特定任务模型转向了适应预训练模型。因此，以高效的方式适应大型预训练模型到下游任务已成为一个重要的研究领域。现有的解决方案主要集中在设计轻量级的适配器及其与预训练模型的交互，目标是最小化需要更新的参数数量。本研究提出了一种新颖的Adapter重组（ARC）策略，从一个新的角度解决了高效预训练模型适应的问题。我们的方法考虑了适应参数的可重用性，并引入了参数共享方案。具体而言，我们利用对称的向下/向上投影来构建瓶颈操作，这些操作在不同层之间共享。通过学习低维度的重新缩放系数，我们可以有效地重新组合层适应参数。

    The advent of high-capacity pre-trained models has revolutionized problem-solving in computer vision, shifting the focus from training task-specific models to adapting pre-trained models. Consequently, effectively adapting large pre-trained models to downstream tasks in an efficient manner has become a prominent research area. Existing solutions primarily concentrate on designing lightweight adapters and their interaction with pre-trained models, with the goal of minimizing the number of parameters requiring updates. In this study, we propose a novel Adapter Re-Composing (ARC) strategy that addresses efficient pre-trained model adaptation from a fresh perspective. Our approach considers the reusability of adaptation parameters and introduces a parameter-sharing scheme. Specifically, we leverage symmetric down-/up-projections to construct bottleneck operations, which are shared across layers. By learning low-dimensional re-scaling coefficients, we can effectively re-compose layer-adapti
    
[^46]: 通过新颖的稀疏感应正则化方法实现低秩张量补全

    Low-Rank Tensor Completion via Novel Sparsity-Inducing Regularizers. (arXiv:2310.06233v1 [cs.LG])

    [http://arxiv.org/abs/2310.06233](http://arxiv.org/abs/2310.06233)

    本研究提出了一种通过新颖的稀疏感应正则化方法实现低秩张量补全的框架。该方法提供了具有封闭形式阈值函数的正则化器，并基于交替方向乘法的算法进行高效计算。实验结果表明，所提出的算法在恢复性能方面优于现有方法。

    

    为了减轻低秩张量补全问题中由l1范数产生的偏差，已经提出了非凸的替代张量核范数的替代函数/正则化器，虽然它们都可以实现稀疏性。然而，这些非凸正则化器的阈值函数可能没有封闭形式的表达式，因此需要进行迭代，从而增加了计算负担。为了解决这个问题，我们提出了一个框架来生成具有封闭形式阈值函数的稀疏感应正则化器。这些正则化器应用于低 Tubal Rank 张量补全，并基于交替方向乘法的方法开发了高效的算法。此外，分析了我们的方法的收敛性，并证明了生成的序列是有界的，任何极限点都是一个稳定点。使用合成和真实世界数据集的实验结果表明，所提出的算法在恢复性能方面优于现有方法。

    To alleviate the bias generated by the l1-norm in the low-rank tensor completion problem, nonconvex surrogates/regularizers have been suggested to replace the tensor nuclear norm, although both can achieve sparsity. However, the thresholding functions of these nonconvex regularizers may not have closed-form expressions and thus iterations are needed, which increases the computational loads. To solve this issue, we devise a framework to generate sparsity-inducing regularizers with closed-form thresholding functions. These regularizers are applied to low-tubal-rank tensor completion, and efficient algorithms based on the alternating direction method of multipliers are developed. Furthermore, convergence of our methods is analyzed and it is proved that the generated sequences are bounded and any limit point is a stationary point. Experimental results using synthetic and real-world datasets show that the proposed algorithms outperform the state-of-the-art methods in terms of restoration pe
    
[^47]: 探索医学图像联合学习中的对抗性攻击

    Exploring adversarial attacks in federated learning for medical imaging. (arXiv:2310.06227v1 [cs.CR])

    [http://arxiv.org/abs/2310.06227](http://arxiv.org/abs/2310.06227)

    本文通过使用领域专有的MRI肿瘤和病理图像数据集，评估了联合学习网络在医学图像分析中面对对抗性攻击的漏洞。结果发现，领域特定的配置可以显著增加攻击者的成功率，强调了对有效的防御机制迫切需要，并建议重新评估当前联合医学图像分析系统的安全协议。

    

    联合学习为医学图像分析提供了一种保护隐私的框架，但也暴露了系统对对抗性攻击的风险。本文旨在评估联合学习网络在医学图像分析中面对此类攻击的漏洞。我们使用领域专有的MRI肿瘤和病理图像数据集，评估已知威胁场景在联合学习环境中的有效性。我们的测试结果显示，领域特定的配置可以显著增加攻击者的成功率。研究结果强调了对有效的防御机制迫切需要，并建议对当前联合医学图像分析系统的安全协议进行关键重新评估。

    Federated learning offers a privacy-preserving framework for medical image analysis but exposes the system to adversarial attacks. This paper aims to evaluate the vulnerabilities of federated learning networks in medical image analysis against such attacks. Employing domain-specific MRI tumor and pathology imaging datasets, we assess the effectiveness of known threat scenarios in a federated learning environment. Our tests reveal that domain-specific configurations can increase the attacker's success rate significantly. The findings emphasize the urgent need for effective defense mechanisms and suggest a critical re-evaluation of current security protocols in federated medical image analysis systems.
    
[^48]: GPT-4作为农学助手？使用大型语言模型回答农业考试

    GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using Large Language Models. (arXiv:2310.06225v1 [cs.AI])

    [http://arxiv.org/abs/2310.06225](http://arxiv.org/abs/2310.06225)

    本研究全面评估了大型语言模型（LLMs）在回答农业相关问题方面的能力，并使用RAG和ER技术提高了LLMs的性能。其中，GPT-4在农业考试中取得了及格分数以获得认证。

    

    大型语言模型（LLM）在各个领域，包括医疗保健和金融领域，展示了卓越的自然语言理解能力。在某些任务上，LLM的性能与训练有素的人类相似甚至更好，因此合理地使用人类考试（例如认证考试）来评估LLM的性能。我们对流行的LLM（如Llama 2和GPT）在回答农业相关问题的能力进行了全面评估。在评估过程中，我们还运用了RAG（检索增强生成）和ER（集合细化）技术，结合信息检索、生成能力和提示策略，提高LLM的性能。为了展示LLM的能力，我们选择了来自巴西、印度和美国三个最大的农业生产国的农业考试和基准数据集。我们的分析突出了GPT-4在考试中取得及格分数以获得认证的能力。

    Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding across various domains, including healthcare and finance. For some tasks, LLMs achieve similar or better performance than trained human beings, therefore it is reasonable to employ human exams (e.g., certification tests) to assess the performance of LLMs. We present a comprehensive evaluation of popular LLMs, such as Llama 2 and GPT, on their ability to answer agriculture-related questions. In our evaluation, we also employ RAG (Retrieval-Augmented Generation) and ER (Ensemble Refinement) techniques, which combine information retrieval, generation capabilities, and prompting strategies to improve the LLMs' performance. To demonstrate the capabilities of LLMs, we selected agriculture exams and benchmark datasets from three of the largest agriculture producer countries: Brazil, India, and the USA. Our analysis highlights GPT-4's ability to achieve a passing score on exams to earn cred
    
[^49]: 在开放环境中检测和学习未知分布数据：算法和理论

    Detecting and Learning Out-of-Distribution Data in the Open world: Algorithm and Theory. (arXiv:2310.06221v1 [cs.LG])

    [http://arxiv.org/abs/2310.06221](http://arxiv.org/abs/2310.06221)

    该论文在开放世界机器学习中做出了重要贡献，主要包括对未知分布数据的检测和开放世界表示学习。OOD检测专注于识别模型训练分布之外的未知类别的实例，而ORL则扩展了机器学习模型的能力。

    

    这篇论文在机器学习领域，特别是在面对以前未见过的数据和情境的开放世界场景中，做出了相当大的贡献。传统的机器学习模型通常在固定和已知的类别集合中进行训练和测试，这种条件被称为封闭世界设置。尽管这种假设在受控环境中有效，但在现实应用中却存在局限性，因为新的类别或数据类型可能动态而意外地出现。为了解决这个问题，我们的研究调查了开放世界机器学习的两个相互关联的关键步骤：未知分布（OOD）的检测和开放世界表示学习（ORL）。OOD检测专注于识别模型训练分布之外的未知类别的实例。这个过程降低了对陌生输入进行过度自信和错误预测的风险。在超越OOD检测的同时，ORL扩展了机器学习模型的能力。

    This thesis makes considerable contributions to the realm of machine learning, specifically in the context of open-world scenarios where systems face previously unseen data and contexts. Traditional machine learning models are usually trained and tested within a fixed and known set of classes, a condition known as the closed-world setting. While this assumption works in controlled environments, it falls short in real-world applications where new classes or categories of data can emerge dynamically and unexpectedly. To address this, our research investigates two intertwined steps essential for open-world machine learning: Out-of-distribution (OOD) Detection and Open-world Representation Learning (ORL). OOD detection focuses on identifying instances from unknown classes that fall outside the model's training distribution. This process reduces the risk of making overly confident, erroneous predictions about unfamiliar inputs. Moving beyond OOD detection, ORL extends the capabilities of th
    
[^50]: SUBP：软均匀块剪枝用于1xN稀疏CNN的多线程加速

    SUBP: Soft Uniform Block Pruning for 1xN Sparse CNNs Multithreading Acceleration. (arXiv:2310.06218v1 [cs.LG])

    [http://arxiv.org/abs/2310.06218](http://arxiv.org/abs/2310.06218)

    该论文提出了一种新的软均匀块剪枝（SUBP）方法，在1xN稀疏CNN中实现了多线程加速，并解决了传统方法中的训练成本昂贵、内存访问开销大、模型质量次优以及线程负载不平衡等问题。

    

    卷积神经网络（CNN）中的稀疏性研究已经广泛应用于在资源有限的环境中压缩和加速模型。通过约束输出通道上的N个连续权重为组内非零，最近的1xN稀疏网络因其三个突出优势而受到广泛关注：1）通过一种“块稀疏行”矩阵大量节省存储空间。2）在高稀疏性下表现出色。3）在具有高级矢量扩展的CPU上显著加速。最近的研究需要基于稠密预训练权重选择和微调1xN稀疏权重，导致训练成本昂贵、内存访问开销大、模型质量次优以及不平衡的线程负载（输出通道上的不同稀疏性）等问题。为了解决这些问题，本文提出了一种新颖的“软均匀块剪枝”（SUBP）方法来训练一个u

    The study of sparsity in Convolutional Neural Networks (CNNs) has become widespread to compress and accelerate models in environments with limited resources. By constraining N consecutive weights along the output channel to be group-wise non-zero, the recent network with 1$\times$N sparsity has received tremendous popularity for its three outstanding advantages: 1) A large amount of storage space saving by a \emph{Block Sparse Row} matrix. 2) Excellent performance at a high sparsity. 3) Significant speedups on CPUs with Advanced Vector Extensions. Recent work requires selecting and fine-tuning 1$\times$N sparse weights based on dense pre-trained weights, leading to the problems such as expensive training cost and memory access, sub-optimal model quality, as well as unbalanced workload across threads (different sparsity across output channels). To overcome them, this paper proposes a novel \emph{\textbf{S}oft \textbf{U}niform \textbf{B}lock \textbf{P}runing} (SUBP) approach to train a u
    
[^51]: 分布式网络上的联合多级优化

    Federated Multi-Level Optimization over Decentralized Networks. (arXiv:2310.06217v1 [cs.LG])

    [http://arxiv.org/abs/2310.06217](http://arxiv.org/abs/2310.06217)

    本文研究了在网络上的分布式多级优化问题，提出了一种基于流言的分布式多级优化算法，实现了最优的样本复杂度，并在各种应用场景上展现了最先进的性能。

    

    近年来，多级优化受到越来越多的关注，因为它为解决许多领域中出现的复杂优化问题（如元学习、多人游戏、强化学习和嵌套组合优化）提供了强大的框架。本文研究了在网络上的分布式多级优化问题，其中代理只能与它们的直接邻居进行通信。这个设置是由大规模系统中分布式优化的需求所驱动的，其中集中优化可能不切实际或不可行。为了解决这个问题，我们提出了一种基于流言的分布式多级优化算法，使网络代理可以在单个时间尺度内解决不同层次的优化问题，并通过网络传播共享信息。我们的算法实现了最优的样本复杂度，与网络规模线性增长，并在各种应用场景上展现了最先进的性能。

    Multi-level optimization has gained increasing attention in recent years, as it provides a powerful framework for solving complex optimization problems that arise in many fields, such as meta-learning, multi-player games, reinforcement learning, and nested composition optimization. In this paper, we study the problem of distributed multi-level optimization over a network, where agents can only communicate with their immediate neighbors. This setting is motivated by the need for distributed optimization in large-scale systems, where centralized optimization may not be practical or feasible. To address this problem, we propose a novel gossip-based distributed multi-level optimization algorithm that enables networked agents to solve optimization problems at different levels in a single timescale and share information through network propagation. Our algorithm achieves optimal sample complexity, scaling linearly with the network size, and demonstrates state-of-the-art performance on variou
    
[^52]: GeoLLM: 从大型语言模型中提取地理空间知识

    GeoLLM: Extracting Geospatial Knowledge from Large Language Models. (arXiv:2310.06213v1 [cs.CL])

    [http://arxiv.org/abs/2310.06213](http://arxiv.org/abs/2310.06213)

    GeoLLM是一种能够从大型语言模型中提取地理空间知识的新方法，结合来自OpenStreetMap的辅助地图数据，可以有效应用于测量人口密度和经济生计等任务。

    

    机器学习在各种地理空间任务中的应用越来越普遍，但常常依赖于全球范围可用的卫星图像等预测变量，这可能要么很昂贵，要么缺乏预测能力。本文探讨了一个问题，即互联网语言语料库中包含的大量知识是否可以利用大型语言模型（LLM）进行地理空间预测任务。我们首先证明了LLM中嵌入了有关位置的显著空间信息，但仅使用地理坐标来查询LLM对于预测人口密度等关键指标是无效的。然后，我们提出了一种名为GeoLLM的新方法，可以有效地从LLM中提取地理空间知识，并结合来自OpenStreetMap的辅助地图数据。我们展示了我们的方法在多个国际社区关心的任务中的实用性，包括人口密度和经济生计的测量。在这些任务中

    The application of machine learning (ML) in a range of geospatial tasks is increasingly common but often relies on globally available covariates such as satellite imagery that can either be expensive or lack predictive power. Here we explore the question of whether the vast amounts of knowledge found in Internet language corpora, now compressed within large language models (LLMs), can be leveraged for geospatial prediction tasks. We first demonstrate that LLMs embed remarkable spatial information about locations, but naively querying LLMs using geographic coordinates alone is ineffective in predicting key indicators like population density. We then present GeoLLM, a novel method that can effectively extract geospatial knowledge from LLMs with auxiliary map data from OpenStreetMap. We demonstrate the utility of our approach across multiple tasks of central interest to the international community, including the measurement of population density and economic livelihoods. Across these task
    
[^53]: 公正的分类器，能够不造成伤害地弃权

    Fair Classifiers that Abstain without Harm. (arXiv:2310.06205v1 [cs.LG])

    [http://arxiv.org/abs/2310.06205](http://arxiv.org/abs/2310.06205)

    提出了一种公正的分类器弃权方法，不造成伤害的同时达到一定程度的群体公平性定义。通过整数规划和代理模型训练的方式实现对训练和测试样本的弃权决策，分析了弃权率与不公平容忍度和准确性约束的关系。

    

    在关键应用中，分类器向人类推迟决策至关重要。我们提出了一种事后方法，使现有的分类器有选择地弃权预测某些样本。我们的弃权分类器被激励以在保持每个子群体的原始准确性的同时，实现一组指定程度的群体公平定义（即无伤害）。为此，我们设计了一个整数规划过程，为每个训练样本分配弃权决策以满足一组约束条件。为了将弃权决策推广到测试样本，我们接着以端到端的方式使用整数规划解决方案训练一个代理模型来学习弃权决策。我们分析了整数规划过程的可行性，以确定在不同的不公平容忍度和准确性约束水平下实现无伤害所可能的弃权率。据我们所知，这项工作是第一个确定了这些理论关系的工作。

    In critical applications, it is vital for classifiers to defer decision-making to humans. We propose a post-hoc method that makes existing classifiers selectively abstain from predicting certain samples. Our abstaining classifier is incentivized to maintain the original accuracy for each sub-population (i.e. no harm) while achieving a set of group fairness definitions to a user specified degree. To this end, we design an Integer Programming (IP) procedure that assigns abstention decisions for each training sample to satisfy a set of constraints. To generalize the abstaining decisions to test samples, we then train a surrogate model to learn the abstaining decisions based on the IP solutions in an end-to-end manner. We analyze the feasibility of the IP procedure to determine the possible abstention rate for different levels of unfairness tolerance and accuracy constraint for achieving no harm. To the best of our knowledge, this work is the first to identify the theoretical relationships
    
[^54]: 自动神经元解释的及时调优的重要性

    The Importance of Prompt Tuning for Automated Neuron Explanations. (arXiv:2310.06200v1 [cs.CL])

    [http://arxiv.org/abs/2310.06200](http://arxiv.org/abs/2310.06200)

    本文探讨了在自动神经元解释中即时调优的重要性，通过重新格式化解释提示，我们显著提高了解释质量并降低了计算成本。这项研究对于更深入理解大型语言模型的工作原理和安全性具有重要意义。

    

    最近的进展极大地提升了大型语言模型(LLM)的能力，但我们对这些模型及其安全性的理解并没有同步进展。在本文中，我们旨在通过研究它们的个体神经元来更深入地理解LLM。我们在前人研究的基础上，进一步探讨了大型语言模型，如GPT-4，如何解释语言模型中每个神经元的功能。具体地，我们分析了生成解释所使用的提示的效果，并展示了以更自然的方式重新格式化解释提示如何显著提高神经元解释的质量，并大幅降低计算成本。我们通过三种不同的方式演示了我们新提示的效果，包括自动化评估和人工评估。

    Recent advances have greatly increased the capabilities of large language models (LLMs), but our understanding of the models and their safety has not progressed as fast. In this paper we aim to understand LLMs deeper by studying their individual neurons. We build upon previous work showing large language models such as GPT-4 can be useful in explaining what each neuron in a language model does. Specifically, we analyze the effect of the prompt used to generate explanations and show that reformatting the explanation prompt in a more natural way can significantly improve neuron explanation quality and greatly reduce computational cost. We demonstrate the effects of our new prompts in three different ways, incorporating both automated and human evaluations.
    
[^55]: PAC-Bayesian光谱归一化界对抗鲁棒泛化性的研究

    PAC-Bayesian Spectrally-Normalized Bounds for Adversarially Robust Generalization. (arXiv:2310.06182v1 [cs.LG])

    [http://arxiv.org/abs/2310.06182](http://arxiv.org/abs/2310.06182)

    本文提出了一种PAC-Bayesian光谱归一化界限，用于对抗鲁棒深度神经网络的泛化性。与现有界限相比，我们的方法不依赖额外的假设，且更紧密地与标准泛化的界限一致。

    

    深度神经网络(DNNs)容易受到对抗攻击。实验证明，对抗性鲁棒泛化在建立防御对抗攻击的算法中至关重要。因此，研究对鲁棒泛化的理论保证非常有意义。本文基于PAC-Bayes方法(Neyshabur等人，2017年)的基于范数的复杂性展开研究。主要挑战在于将在标准情况下的主要构成要素，即权重扰动界，扩展到鲁棒情况下。现有的尝试严重依赖于额外的强假设，导致界限不严。本文解决了这个问题，并为DNNs提供了一种光谱归一化的鲁棒泛化界限。与现有的界限相比，我们的界限具有两个显著优势：首先，它不依赖于额外的假设。其次，它的界限相当紧密，与标准泛化的界限一致。因此，我们的结果提供了一种不同的方法，可以更好地保证对抗鲁棒性。

    Deep neural networks (DNNs) are vulnerable to adversarial attacks. It is found empirically that adversarially robust generalization is crucial in establishing defense algorithms against adversarial attacks. Therefore, it is interesting to study the theoretical guarantee of robust generalization. This paper focuses on norm-based complexity, based on a PAC-Bayes approach (Neyshabur et al., 2017). The main challenge lies in extending the key ingredient, which is a weight perturbation bound in standard settings, to the robust settings. Existing attempts heavily rely on additional strong assumptions, leading to loose bounds. In this paper, we address this issue and provide a spectrally-normalized robust generalization bound for DNNs. Compared to existing bounds, our bound offers two significant advantages: Firstly, it does not depend on additional assumptions. Secondly, it is considerably tighter, aligning with the bounds of standard generalization. Therefore, our result provides a differen
    
[^56]: 自动化的时空神经点过程积分方法

    Automatic Integration for Spatiotemporal Neural Point Processes. (arXiv:2310.06179v1 [cs.LG])

    [http://arxiv.org/abs/2310.06179](http://arxiv.org/abs/2310.06179)

    本文提出了一种自动化的时空神经点过程积分方法(AutoSTPP)，扩展了AutoInt方法用于三维时空点过程(STPP)的计算，具有优越性能。

    

    学习连续时间的点过程对于许多离散事件预测任务至关重要。然而，对于时空点过程（STPPs）的积分问题是一个重要挑战，因为它涉及到对空间和时间进行三重积分计算。现有的STPP积分方法要么假设强度函数具有参数形式，这缺乏灵活性；要么用蒙特卡洛采样来近似强度，这引入了数值误差。Omi等人最近的工作提出了一个自动积分方法AutoInt，用于高效地积分灵活的强度函数，但该方法只关注1D时间点过程。本文将AutoInt方法扩展至3D STPP，提出了一种新的范式：AutoSTPP（自动化的时空神经点过程积分方法）。我们表明，直接扩展之前的工作会过于约束强度函数，导致性能不佳。我们证明了我们的方法在各种实验中的优越性能。

    Learning continuous-time point processes is essential to many discrete event forecasting tasks. However, integration poses a major challenge, particularly for spatiotemporal point processes (STPPs), as it involves calculating the likelihood through triple integrals over space and time. Existing methods for integrating STPP either assume a parametric form of the intensity function, which lacks flexibility; or approximating the intensity with Monte Carlo sampling, which introduces numerical errors. Recent work by Omi et al. [2019] proposes a dual network or AutoInt approach for efficient integration of flexible intensity function. However, the method only focuses on the 1D temporal point process. In this paper, we introduce a novel paradigm: AutoSTPP (Automatic Integration for Spatiotemporal Neural Point Processes) that extends the AutoInt approach to 3D STPP. We show that direct extension of the previous work overly constrains the intensity function, leading to poor performance. We prov
    
[^57]: 借助msGeMM增加AI GeMM的性能近2.5倍的Look-Up mAI GeMM

    Look-Up mAI GeMM: Increasing AI GeMMs Performance by Nearly 2.5x via msGeMM. (arXiv:2310.06178v1 [cs.PF])

    [http://arxiv.org/abs/2310.06178](http://arxiv.org/abs/2310.06178)

    这篇论文提出了一种名为msGeMM的新算法，通过使用低精度数据类型，可以使AI模型的性能提高近2.5倍。该算法需要特殊的CUDA核心来实现从小型查找表中添加元素的能力。

    

    AI模型的规模不断增加，最近的研究表明，在HPC应用中需要双精度数据类型，而fp8或int4等更低精度的数据类型已经足够用于训练和推断中，而且质量相当。在此趋势下，像NVIDIA和AMD这样的GPU供应商通过张量核心提供了对fp16、fp8和int8 GeMM操作的硬件支持，具有优秀的性能。然而，本文提出了一种名为msGeMM的新算法，该算法证明了使用低精度数据类型的AI模型可以减少约2.5倍的乘法和加法指令。实现此算法的高效率需要具备与张量核心相同速率从小型查找表中添加元素的特殊CUDA核心。

    AI models are increasing in size and recent advancement in the community has shown that unlike HPC applications where double precision datatype are required, lower-precision datatypes such as fp8 or int4 are sufficient to bring the same model quality both for training and inference. Following these trends, GPU vendors such as NVIDIA and AMD have added hardware support for fp16, fp8 and int8 GeMM operations with an exceptional performance via Tensor Cores. However, this paper proposes a new algorithm called msGeMM which shows that AI models with low-precision datatypes can run with ~2.5x fewer multiplication and add instructions. Efficient implementation of this algorithm requires special CUDA cores with the ability to add elements from a small look-up table at the rate of Tensor Cores.
    
[^58]: DockGame: 多聚刚性蛋白质对接的合作游戏。

    DockGame: Cooperative Games for Multimeric Rigid Protein Docking. (arXiv:2310.06177v1 [cs.LG])

    [http://arxiv.org/abs/2310.06177](http://arxiv.org/abs/2310.06177)

    本文提出了一个名为DockGame的新颖游戏理论框架，用于解决多聚刚性蛋白质对接问题。我们将蛋白质对接视为蛋白质之间的合作游戏，通过学习代理游戏潜力和通过梯度更新计算均衡来预测最终的组装结构。

    

    蛋白质相互作用和组装对大多数生物过程都至关重要。从组成蛋白质预测组装结构（称为蛋白质对接任务）是蛋白质设计应用中的关键步骤。大多数对接的传统和深度学习方法主要集中在二进制对接上，遵循搜索、回归或生成建模范式。本文针对少有研究的多聚体（即两个或更多蛋白质）对接问题。我们引入DockGame，一个新颖的游戏理论框架用于对接——我们将蛋白质对接视为蛋白质之间的合作游戏，最终的组装结构构成与底层游戏潜力稳定均衡。由于我们无法获得真实的潜力，我们考虑两种方法——i)通过物理能量函数指导学习替代游戏潜力并通过同时梯度更新计算均衡，以及ii)探索对接问题的取样方法。

    Protein interactions and assembly formation are fundamental to most biological processes. Predicting the assembly structure from constituent proteins -- referred to as the protein docking task -- is thus a crucial step in protein design applications. Most traditional and deep learning methods for docking have focused mainly on binary docking, following either a search-based, regression-based, or generative modeling paradigm. In this paper, we focus on the less-studied multimeric (i.e., two or more proteins) docking problem. We introduce DockGame, a novel game-theoretic framework for docking -- we view protein docking as a cooperative game between proteins, where the final assembly structure(s) constitute stable equilibria w.r.t. the underlying game potential. Since we do not have access to the true potential, we consider two approaches - i) learning a surrogate game potential guided by physics-based energy functions and computing equilibria by simultaneous gradient updates, and ii) sam
    
[^59]: 内存一致的神经网络在模仿学习中的应用

    Memory-Consistent Neural Networks for Imitation Learning. (arXiv:2310.06171v1 [cs.LG])

    [http://arxiv.org/abs/2310.06171](http://arxiv.org/abs/2310.06171)

    本文介绍了一种内存一致的神经网络模型，在模仿学习中使用专家演示训练策略。该模型通过对输出结果进行硬约束，避免了错误的累积现象，保证了策略效果的上界。

    

    模仿学习利用专家演示大大简化了策略合成的过程。然而，对于这种模仿策略来说，远离训练样本的错误尤为关键。即使在策略的行动输出中出现罕见的错误，由于这些错误会导致不熟悉的未来状态，策略在这些状态下仍更容易出错，最终导致任务失败。本文重新审视了简单的监督式“行为克隆”方法，能够方便地仅通过预先记录的演示来训练策略，并设计了一种能够抵消错误累积现象的模型类。我们的“内存一致神经网络”(MCNN)输出被强制约束在与典型的“内存”训练样本相关的明确指定的允许区域内。我们提供了MCNN策略导致的次优性差距的保证上界。通过在9个模仿学习任务上使用MCNNs，采用MLP、Transformer等方法。

    Imitation learning considerably simplifies policy synthesis compared to alternative approaches by exploiting access to expert demonstrations. For such imitation policies, errors away from the training samples are particularly critical. Even rare slip-ups in the policy action outputs can compound quickly over time, since they lead to unfamiliar future states where the policy is still more likely to err, eventually causing task failures. We revisit simple supervised ``behavior cloning'' for conveniently training the policy from nothing more than pre-recorded demonstrations, but carefully design the model class to counter the compounding error phenomenon. Our ``memory-consistent neural network'' (MCNN) outputs are hard-constrained to stay within clearly specified permissible regions anchored to prototypical ``memory'' training samples. We provide a guaranteed upper bound for the sub-optimality gap induced by MCNN policies. Using MCNNs on 9 imitation learning tasks, with MLP, Transformer, 
    
[^60]: DEUX: 活跃探索用于学习无监督深度感知

    DEUX: Active Exploration for Learning Unsupervised Depth Perception. (arXiv:2310.06164v1 [cs.CV])

    [http://arxiv.org/abs/2310.06164](http://arxiv.org/abs/2310.06164)

    本研究通过利用3D交互环境，调查数据收集对无监督深度感知学习的影响，发现现有的探索模式不能提供有效的数据，进而提出了一种基于深度不确定性的主动、任务导向的深度完整性学习方法。

    

    深度感知模型通常在预定义的相机轨迹上的非交互式数据集上进行训练。然而，这常常会引入与数据采集过程中选择的特定相机路径相关的系统偏差，影响学习过程。本文从机器人导航的视角，通过利用3D交互环境来调查数据收集对学习深度完整性的作用。首先，我们评估了四个使用传统导航技术收集数据进行训练的深度完整性模型。我们的关键观察是，现有的探索模式不一定提供任务具体数据点以实现有效的无监督深度完整性学习。然后，我们发现与光度重建相关的数据对模型性能有直接积极影响。因此，我们开发了一种基于深度不确定性的主动、任务导向的深度完整性学习的运动规划方法，称之为DEpth Uncert。

    Depth perception models are typically trained on non-interactive datasets with predefined camera trajectories. However, this often introduces systematic biases into the learning process correlated to specific camera paths chosen during data acquisition. In this paper, we investigate the role of how data is collected for learning depth completion, from a robot navigation perspective, by leveraging 3D interactive environments. First, we evaluate four depth completion models trained on data collected using conventional navigation techniques. Our key insight is that existing exploration paradigms do not necessarily provide task-specific data points to achieve competent unsupervised depth completion learning. We then find that data collected with respect to photometric reconstruction has a direct positive influence on model performance. As a result, we develop an active, task-informed, depth uncertainty-based motion planning approach for learning depth completion, which we call DEpth Uncert
    
[^61]: 减轻深度学习中的简单性偏差以改善OOD推广和鲁棒性

    Mitigating Simplicity Bias in Deep Learning for Improved OOD Generalization and Robustness. (arXiv:2310.06161v1 [cs.LG])

    [http://arxiv.org/abs/2310.06161](http://arxiv.org/abs/2310.06161)

    本论文提出了一个框架，鼓励深度模型利用更多样的特征进行预测，以减轻简单性偏差带来的OOD推广问题，并在各种问题和应用中展示了其有效性。

    

    神经网络已知存在简单性偏差，即它们倾向于学习“简单”特征而不是更“复杂”的特征，即使后者可能更具信息量。简单性偏差可能导致模型做出具有较差的OOD推广的偏见性预测。为了解决这个问题，我们提出了一个框架，鼓励模型使用更多样的特征进行预测。我们首先训练一个简单模型，然后使用条件互信息对其进行正则化，得到最终模型。我们在各种问题设置和真实世界应用中展示了这个框架的有效性，证明它有效地解决了简单性偏差，促使更多特征被使用，增强了OOD推广，并提高了子组鲁棒性和公平性。我们补充这些结果与对正则化效果及其OOD推广特性的理论分析。

    Neural networks (NNs) are known to exhibit simplicity bias where they tend to prefer learning 'simple' features over more 'complex' ones, even when the latter may be more informative. Simplicity bias can lead to the model making biased predictions which have poor out-of-distribution (OOD) generalization. To address this, we propose a framework that encourages the model to use a more diverse set of features to make predictions. We first train a simple model, and then regularize the conditional mutual information with respect to it to obtain the final model. We demonstrate the effectiveness of this framework in various problem settings and real-world applications, showing that it effectively addresses simplicity bias and leads to more features being used, enhances OOD generalization, and improves subgroup robustness and fairness. We complement these results with theoretical analyses of the effect of the regularization and its OOD generalization properties.
    
[^62]: 经过缩放梯度下降法的，甚至过参数化的可证明加速的病态低秩估计

    Provably Accelerating Ill-Conditioned Low-rank Estimation via Scaled Gradient Descent, Even with Overparameterization. (arXiv:2310.06159v1 [cs.LG])

    [http://arxiv.org/abs/2310.06159](http://arxiv.org/abs/2310.06159)

    本章提出了一种名为缩放梯度下降（ScaledGD）的新算法，能够在恒定速率下收敛，而不受低秩对象条件数的影响，并且在各种任务中具有低迭代成本。

    

    科学和工程中遇到的许多问题可以归纳为从不完整且可能损坏的线性测量中估计低秩对象（例如矩阵和张量）。通过矩阵和张量分解的视角，其中一种最流行的方法是使用简单的迭代算法，如梯度下降（GD）直接恢复低秩因子，这样可以实现小内存和计算开销。然而，GD的收敛速率线性地依赖于低秩对象的条件数，有时甚至是二次的，因此当问题病态时，GD的收敛非常缓慢。本章介绍了一种新的算法方法，称为缩放梯度下降法（ScaledGD），它能够以恒定速率线性地收敛，而不依赖于低秩对象的条件数，同时保持了梯度下降在各种任务中的每次迭代成本较低，包括感知、鲁棒主成分估计等任务。

    Many problems encountered in science and engineering can be formulated as estimating a low-rank object (e.g., matrices and tensors) from incomplete, and possibly corrupted, linear measurements. Through the lens of matrix and tensor factorization, one of the most popular approaches is to employ simple iterative algorithms such as gradient descent (GD) to recover the low-rank factors directly, which allow for small memory and computation footprints. However, the convergence rate of GD depends linearly, and sometimes even quadratically, on the condition number of the low-rank object, and therefore, GD slows down painstakingly when the problem is ill-conditioned. This chapter introduces a new algorithmic approach, dubbed scaled gradient descent (ScaledGD), that provably converges linearly at a constant rate independent of the condition number of the low-rank object, while maintaining the low per-iteration cost of gradient descent for a variety of tasks including sensing, robust principal c
    
[^63]: 基于流形的Eikonal方程：可微流形上的测地距离和流动

    Manifold-augmented Eikonal Equations: Geodesic Distances and Flows on Differentiable Manifolds. (arXiv:2310.06157v1 [cs.CG])

    [http://arxiv.org/abs/2310.06157](http://arxiv.org/abs/2310.06157)

    本研究提出了一种基于模型的参数化方法，利用测地线和流动来描述可微流形上的距离和长度最小化曲线。这为在不同iable流形上进行统计和降阶建模提供了机会。

    

    通过机器学习模型发现的流形提供了底层数据的紧凑表示。这些流形上的测地线定义了局部长度最小化曲线，并提供了距离的概念，这对于降阶建模、统计推断和插值至关重要。在这项工作中，我们提出了一种基于模型的参数化方法来表示流形上的距离场和测地流动，利用扩展的Eikonal方程的解决方案。我们展示了流形的几何特性如何影响距离场，并利用测地流动直接获得全局长度最小化曲线。这项工作为在可微流形上进行统计和降阶建模提供了机会。

    Manifolds discovered by machine learning models provide a compact representation of the underlying data. Geodesics on these manifolds define locally length-minimising curves and provide a notion of distance, which are key for reduced-order modelling, statistical inference, and interpolation. In this work, we propose a model-based parameterisation for distance fields and geodesic flows on manifolds, exploiting solutions of a manifold-augmented Eikonal equation. We demonstrate how the geometry of the manifold impacts the distance field, and exploit the geodesic flow to obtain globally length-minimising curves directly. This work opens opportunities for statistics and reduced-order modelling on differentiable manifolds.
    
[^64]: DNA序列生成的潜在扩散模型

    Latent Diffusion Model for DNA Sequence Generation. (arXiv:2310.06150v1 [cs.LG])

    [http://arxiv.org/abs/2310.06150](http://arxiv.org/abs/2310.06150)

    提出了一种新的潜在扩散模型DiscDiff，用于离散DNA序列生成。通过将离散DNA序列嵌入到连续的潜在空间中，利用连续扩散模型的强大生成能力来生成离散数据。同时，引入了一种新的度量标准FReD，用于评估DNA序列生成的样本质量。

    

    机器学习，尤其是深度生成模型的运用，为合成DNA序列生成领域打开了新的前景。虽然生成对抗网络（GANs）在这个应用中得到了广泛应用，但它们常常面临样本多样性有限和模式崩溃等问题。另一方面，扩散模型是一类有前景的新型生成模型，不受这些问题的困扰，使其能够在图像生成等领域达到最先进水平。基于此，我们提出了一种新颖的适用于离散DNA序列生成的潜在扩散模型DiscDiff。通过将离散DNA序列简单地嵌入到连续的潜在空间中使用自编码器，我们能够利用连续扩散模型的强大生成能力来生成离散数据。此外，我们引入了一种新的度量标准——Fr\'echet重建距离（FReD），用于评估DNA序列生成的样本质量。

    The harnessing of machine learning, especially deep generative models, has opened up promising avenues in the field of synthetic DNA sequence generation. Whilst Generative Adversarial Networks (GANs) have gained traction for this application, they often face issues such as limited sample diversity and mode collapse. On the other hand, Diffusion Models are a promising new class of generative models that are not burdened with these problems, enabling them to reach the state-of-the-art in domains such as image generation. In light of this, we propose a novel latent diffusion model, DiscDiff, tailored for discrete DNA sequence generation. By simply embedding discrete DNA sequences into a continuous latent space using an autoencoder, we are able to leverage the powerful generative abilities of continuous diffusion models for the generation of discrete data. Additionally, we introduce Fr\'echet Reconstruction Distance (FReD) as a new metric to measure the sample quality of DNA sequence gener
    
[^65]: 理解迁移学习和基于梯度的元学习技术

    Understanding Transfer Learning and Gradient-Based Meta-Learning Techniques. (arXiv:2310.06148v1 [cs.LG])

    [http://arxiv.org/abs/2310.06148](http://arxiv.org/abs/2310.06148)

    本文研究了微调、MAML和Reptile在迁移学习和元学习领域的性能差异，发现当在与训练数据分布不同的任务上进行评估时，仅对预训练网络进行微调的基准线可能比更复杂的元学习技术更有效。

    

    深度神经网络在各种任务上可以取得良好的性能，但通常需要大量的数据来训练。元学习作为一种提高这些网络从有限数据中泛化能力的方法受到广泛关注。虽然元学习技术在各种场景下被证明是成功的，但最近的结果表明，在评估与训练数据分布不同的任务时，与复杂的元学习技术（如MAML）相比，仅对预训练网络进行微调的基准线可能更有效。这一点令人惊讶，因为MAML的学习行为与微调的行为类似：两者都依赖于重复使用已学习的特征。我们调查了微调、MAML和另一种名为Reptile的元学习技术之间观察到的性能差异，并展示了MAML和Reptile在低数据情况下的快速适应能力。

    Deep neural networks can yield good performance on various tasks but often require large amounts of data to train them. Meta-learning received considerable attention as one approach to improve the generalization of these networks from a limited amount of data. Whilst meta-learning techniques have been observed to be successful at this in various scenarios, recent results suggest that when evaluated on tasks from a different data distribution than the one used for training, a baseline that simply finetunes a pre-trained network may be more effective than more complicated meta-learning techniques such as MAML, which is one of the most popular meta-learning techniques. This is surprising as the learning behaviour of MAML mimics that of finetuning: both rely on re-using learned features. We investigate the observed performance differences between finetuning, MAML, and another meta-learning technique called Reptile, and show that MAML and Reptile specialize for fast adaptation in low-data r
    
[^66]: 在LLM时代的强化学习：什么是必要的？什么是需要的？强化学习对RLHF、Prompting等的视角分析

    Reinforcement Learning in the Era of LLMs: What is Essential? What is needed? An RL Perspective on RLHF, Prompting, and Beyond. (arXiv:2310.06147v1 [cs.LG])

    [http://arxiv.org/abs/2310.06147](http://arxiv.org/abs/2310.06147)

    这篇论文介绍了在LLM时代中，强化学习在RLHF、Prompting等方面的应用，并讨论了其中的创新和贡献。

    

    最近大型语言模型（LLMs）的进展引起了广泛关注，并取得了ChatGPT和GPT-4等成功产品。它们在遵循指令并提供无害、有帮助和诚实（3H）回答方面的熟练程度主要归功于人类反馈强化学习（RLHF）技术。本文旨在将传统强化学习的研究与LLM研究中使用的RL技术联系起来，通过讨论RL在何时、何地和如何优秀，解释这一技术的神秘性。此外，我们探讨了潜在的未来领域，这些领域可能会从RLHF研究中获益或为其做出贡献。重点内容：1. RLHF是带有离线示范数据的在线逆向RL。2. RLHF比SFT更好，因为模仿学习（和逆向RL）比行为克隆（BC）更好，能够缓解累积误差问题。3. RLHF中的RM步骤产生了昂贵的人类反馈的代理，这样的见解可以推广到其他LLM任务，例如提示评估。

    Recent advancements in Large Language Models (LLMs) have garnered wide attention and led to successful products such as ChatGPT and GPT-4. Their proficiency in adhering to instructions and delivering harmless, helpful, and honest (3H) responses can largely be attributed to the technique of Reinforcement Learning from Human Feedback (RLHF). In this paper, we aim to link the research in conventional RL to RL techniques used in LLM research. Demystify this technique by discussing why, when, and how RL excels. Furthermore, we explore potential future avenues that could either benefit from or contribute to RLHF research.  Highlighted Takeaways:  1. RLHF is Online Inverse RL with Offline Demonstration Data.  2. RLHF $>$ SFT because Imitation Learning (and Inverse RL) $>$ Behavior Cloning (BC) by alleviating the problem of compounding error.  3. The RM step in RLHF generates a proxy of the expensive human feedback, such an insight can be generalized to other LLM tasks such as prompting evalua
    
[^67]: HydraViT:自适应多分支变压器用于胸部X射线图像的多标签疾病分类

    HydraViT: Adaptive Multi-Branch Transformer for Multi-Label Disease Classification from Chest X-ray Images. (arXiv:2310.06143v1 [eess.IV])

    [http://arxiv.org/abs/2310.06143](http://arxiv.org/abs/2310.06143)

    HydraViT是一种将变压器与多分支输出模块相结合的新方法，用于胸部X射线图像的多标签疾病分类，以提高分类性能。

    

    胸部X射线是识别胸部疾病的一种重要诊断工具，由于病理异常在肺部的高敏感性，但由于病理的大小和位置的异质性，以及不同病理的视觉相似性和共同出现，以图像为驱动的诊断仍然具有挑战性。因为与疾病相关的区域往往占据诊断图像的相对小部分，所以传统卷积神经网络（CNNs）基于的分类模型受到了它们的局部性偏见的不利影响。虽然之前曾将CNNs与注意图或空间掩码相结合，以指导对潜在关键区域的关注，但在病理空间分布的异质性下学习定位指导是具有挑战性的。为了提高多标签分类性能，我们提出了一种新的方法HydraViT，它将变压器骨干网络与具有学习权重的多分支输出模块结合起来协同工作。

    Chest X-ray is an essential diagnostic tool in the identification of chest diseases given its high sensitivity to pathological abnormalities in the lungs. However, image-driven diagnosis is still challenging due to heterogeneity in size and location of pathology, as well as visual similarities and co-occurrence of separate pathology. Since disease-related regions often occupy a relatively small portion of diagnostic images, classification models based on traditional convolutional neural networks (CNNs) are adversely affected given their locality bias. While CNNs were previously augmented with attention maps or spatial masks to guide focus on potentially critical regions, learning localization guidance under heterogeneity in the spatial distribution of pathology is challenging. To improve multi-label classification performance, here we propose a novel method, HydraViT, that synergistically combines a transformer backbone with a multi-branch output module with learned weighting. The tran
    
[^68]: 关于随机变量及其主成分之间的相关性的研究

    On the Correlation between Random Variables and their Principal Components. (arXiv:2310.06139v1 [cs.LG])

    [http://arxiv.org/abs/2310.06139](http://arxiv.org/abs/2310.06139)

    本文研究了随机变量与它们的主成分之间的相关性，并找到了描述这种相关性的代数公式。这个公式可以应用于优化主成分分析和因子分析中的成分数目选择。

    

    本文试图找到描述随机变量与代表它们的主成分之间相关系数的代数公式。通过分析，我们从与单个随机变量相关的选定统计数据出发，将这些统计数据在线性代数语言中表示为一组随机变量的等效形式，使用向量和矩阵的概念。这使得我们能够推导出预期的公式。找到的公式与因子分析中用于计算因子载荷的公式相同。讨论表明，可以将该公式应用于优化主成分分析中的主成分数目，以及因子分析中的因子数目的优化。

    The article attempts to find an algebraic formula describing the correlation coefficients between random variables and the principal components representing them. As a result of the analysis, starting from selected statistics relating to individual random variables, the equivalents of these statistics relating to a set of random variables were presented in the language of linear algebra, using the concepts of vector and matrix. This made it possible, in subsequent steps, to derive the expected formula. The formula found is identical to the formula used in Factor Analysis to calculate factor loadings. The discussion showed that it is possible to apply this formula to optimize the number of principal components in Principal Component Analysis, as well as to optimize the number of factors in Factor Analysis.
    
[^69]: 从嘈杂的移动模态中预测布局序列

    Layout Sequence Prediction From Noisy Mobile Modality. (arXiv:2310.06138v1 [cs.CV])

    [http://arxiv.org/abs/2310.06138](http://arxiv.org/abs/2310.06138)

    提出一种名为LTrajDiff的新方法，从噪声移动数据中预测精确的布局序列，克服了由嘈杂数据、不完整轨迹和环境因素导致的视觉障碍。

    

    轨迹预测在理解行人移动方面扮演着重要角色，适用于自动驾驶和机器人等应用。当前的轨迹预测模型依赖于视觉模态的长、完整且准确观测到的序列。然而，现实世界中经常出现相机遮挡、遗漏对象或由于环境因素导致对象不在视线范围内的情况，导致轨迹不完整或噪声较大。为了克服这些限制，我们提出了一种新方法LTrajDiff，将被遮挡或不在视线范围内的物体与完全可见轨迹的物体同等重要。LTrajDiff利用移动手机的传感器数据克服不在视线范围内的约束，但引入了新的挑战，如模态融合、噪声数据和缺乏空间布局和物体尺寸信息。我们使用去噪扩散模型，采用从粗到精的扩散策略，从嘈杂的移动数据中预测精确的布局序列。

    Trajectory prediction plays a vital role in understanding pedestrian movement for applications such as autonomous driving and robotics. Current trajectory prediction models depend on long, complete, and accurately observed sequences from visual modalities. Nevertheless, real-world situations often involve obstructed cameras, missed objects, or objects out of sight due to environmental factors, leading to incomplete or noisy trajectories. To overcome these limitations, we propose LTrajDiff, a novel approach that treats objects obstructed or out of sight as equally important as those with fully visible trajectories. LTrajDiff utilizes sensor data from mobile phones to surmount out-of-sight constraints, albeit introducing new challenges such as modality fusion, noisy data, and the absence of spatial layout and object size information. We employ a denoising diffusion model to predict precise layout sequences from noisy mobile data using a coarse-to-fine diffusion strategy, incorporating th
    
[^70]: 使用梯度自动学习层间等变性

    Learning Layer-wise Equivariances Automatically using Gradients. (arXiv:2310.06131v1 [cs.LG])

    [http://arxiv.org/abs/2310.06131](http://arxiv.org/abs/2310.06131)

    该论文提出了一种通过梯度自动学习层间等变性的方法，通过改进软等变性的参数化和优化边缘似然来实现层间对称性的自动学习。

    

    卷积将等变性对称性编码到神经网络中，从而提高了泛化性能。然而，对称性提供了网络可以表示的函数的固定硬约束，需要事先指定，并且不能适应改变。我们的目标是允许灵活的对称性约束，可以通过梯度自动地从数据中学习。从头开始学习对称性和相关的权重连接结构有两个困难。首先，它需要有效灵活的层间等变性参数化。其次，对称性作为约束，因此不会被训练损失函数鼓励。为了克服这些挑战，我们改进了软等变性的参数化，并通过优化边缘似然来学习层间等变性的数量，其中边缘似然是使用可微分的拉普拉斯逼近估计的。这个目标平衡了数据拟合和模型复杂性，使层间对称性在深度学习中被发现。

    Convolutions encode equivariance symmetries into neural networks leading to better generalisation performance. However, symmetries provide fixed hard constraints on the functions a network can represent, need to be specified in advance, and can not be adapted. Our goal is to allow flexible symmetry constraints that can automatically be learned from data using gradients. Learning symmetry and associated weight connectivity structures from scratch is difficult for two reasons. First, it requires efficient and flexible parameterisations of layer-wise equivariances. Secondly, symmetries act as constraints and are therefore not encouraged by training losses measuring data fit. To overcome these challenges, we improve parameterisations of soft equivariance and learn the amount of equivariance in layers by optimising the marginal likelihood, estimated using differentiable Laplace approximations. The objective balances data fit and model complexity enabling layer-wise symmetry discovery in dee
    
[^71]: 在嘈杂的混响声学环境中，关于单声道语音分离的时域Conformer模型

    On Time Domain Conformer Models for Monaural Speech Separation in Noisy Reverberant Acoustic Environments. (arXiv:2310.06125v1 [cs.SD])

    [http://arxiv.org/abs/2310.06125](http://arxiv.org/abs/2310.06125)

    本论文提出了一种基于时域Conformer模型的单声道语音分离方法，相较于已有模型，在嘈杂的混响声学环境中实现了更高的分离效果和更高的计算效率。

    

    语音分离对多说话者技术研究人员仍然是一个重要的主题。卷积增强转换器（conformer）在许多语音处理任务中表现良好，但在语音分离方面得到的研究较少。最近的最先进（SOTA）分离模型大多数是时域音频分离网络（TasNets）。一些成功的模型利用了双通道（DP）网络，它们按顺序处理局部和全局信息。时域Conformer（TD-Conformers）是DP方法的一个类比，它们也按顺序处理局部和全局上下文，但具有不同的时间复杂度函数。实验结果表明，在实际较短的信号长度下，相对于特征维度，Conformer在控制特征维度时更高效。提出了子采样层以进一步提高计算效率。最佳的TD-Conformer在WHAMR和WSJ0-2Mix基准上分别实现了14.6dB和21.2dB的SISDR改进。

    Speech separation remains an important topic for multi-speaker technology researchers. Convolution augmented transformers (conformers) have performed well for many speech processing tasks but have been under-researched for speech separation. Most recent state-of-the-art (SOTA) separation models have been time-domain audio separation networks (TasNets). A number of successful models have made use of dual-path (DP) networks which sequentially process local and global information. Time domain conformers (TD-Conformers) are an analogue of the DP approach in that they also process local and global context sequentially but have a different time complexity function. It is shown that for realistic shorter signal lengths, conformers are more efficient when controlling for feature dimension. Subsampling layers are proposed to further improve computational efficiency. The best TD-Conformer achieves 14.6 dB and 21.2 dB SISDR improvement on the WHAMR and WSJ0-2Mix benchmarks, respectively.
    
[^72]: 分解张量网络用于多任务和多领域学习

    Factorized Tensor Networks for Multi-Task and Multi-Domain Learning. (arXiv:2310.06124v1 [cs.LG])

    [http://arxiv.org/abs/2310.06124](http://arxiv.org/abs/2310.06124)

    本文提出了一种分解张量网络（FTN），它可以克服多任务多领域学习中的共享信息利用挑战，并在准确性、存储成本、计算量和样本复杂度等方面实现高效率。实验结果表明，FTN相对于现有方法需要更少的任务特定参数，并且可以适应大量的目标领域和任务。

    

    多任务和多领域学习方法旨在使用单个统一的网络共同学习多个任务/领域，或者先后学习它们。关键挑战和机会是利用任务和领域之间的共享信息，提高统一网络的效率，包括准确性、存储成本、计算量或样本复杂度。本文提出了一种分解张量网络（FTN），可以通过增加少量附加参数实现与独立单任务/领域网络相当的准确性。FTN使用源模型的冻结主干网络，并逐步添加任务/领域特定的低秩张量因子到共享的冻结网络中。这种方法可以适应大量目标领域和任务，而不会出现灾难性遗忘。此外，与现有方法相比，FTN需要较少的任务特定参数。我们在广泛使用的多领域和多任务数据集上进行了实验。

    Multi-task and multi-domain learning methods seek to learn multiple tasks/domains, jointly or one after another, using a single unified network. The key challenge and opportunity is to exploit shared information across tasks and domains to improve the efficiency of the unified network. The efficiency can be in terms of accuracy, storage cost, computation, or sample complexity. In this paper, we propose a factorized tensor network (FTN) that can achieve accuracy comparable to independent single-task/domain networks with a small number of additional parameters. FTN uses a frozen backbone network from a source model and incrementally adds task/domain-specific low-rank tensor factors to the shared frozen network. This approach can adapt to a large number of target domains and tasks without catastrophic forgetting. Furthermore, FTN requires a significantly smaller number of task-specific parameters compared to existing methods. We performed experiments on widely used multi-domain and multi-
    
[^73]: 探索多元时间序列预测的进展：全面基准测试和异质性分析

    Exploring Progress in Multivariate Time Series Forecasting: Comprehensive Benchmarking and Heterogeneity Analysis. (arXiv:2310.06119v1 [cs.LG])

    [http://arxiv.org/abs/2310.06119](http://arxiv.org/abs/2310.06119)

    该论文旨在解决多元时间序列预测领域中公平基准测试和技术方法选择的争议，并提供对该领域取得的进展的深入洞察。

    

    多元时间序列（MTS）广泛存在于现实世界的复杂系统中，如交通和能源系统，对于理解和影响这些系统，它们的预测至关重要。最近，基于深度学习的方法在MTS中有效地建模时间和空间依赖关系方面获得了很大的流行，特别是在长期时间序列预测（LTSF）和时空预测（STF）中。然而，公平的基准测试问题和技术方法的选择在相关工作中一直存在争议。这些争议显著阻碍了我们对该领域进展的理解。因此，本文旨在解决这些争议，以提供对取得的进展的深入洞察。为了解决基准测试问题，我们引入了BasicTS，一个旨在公平比较MTS预测的基准。BasicTS建立了一个统一的训练流程和合理的评估设置，能够对30多种流行的MTS预测模型进行公正的评估。

    Multivariate Time Series (MTS) widely exists in real-word complex systems, such as traffic and energy systems, making their forecasting crucial for understanding and influencing these systems. Recently, deep learning-based approaches have gained much popularity for effectively modeling temporal and spatial dependencies in MTS, specifically in Long-term Time Series Forecasting (LTSF) and Spatial-Temporal Forecasting (STF). However, the fair benchmarking issue and the choice of technical approaches have been hotly debated in related work. Such controversies significantly hinder our understanding of progress in this field. Thus, this paper aims to address these controversies to present insights into advancements achieved. To resolve benchmarking issues, we introduce BasicTS, a benchmark designed for fair comparisons in MTS forecasting. BasicTS establishes a unified training pipeline and reasonable evaluation settings, enabling an unbiased evaluation of over 30 popular MTS forecasting mode
    
[^74]: 退后一步：通过抽象唤起大型语言模型的推理能力

    Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models. (arXiv:2310.06117v1 [cs.LG])

    [http://arxiv.org/abs/2310.06117](http://arxiv.org/abs/2310.06117)

    本文提出了一种简单的提示技术，使得大型语言模型能够通过抽象获得高层概念和基本原理，并将其应用于推理路径中，从而显著提升模型在各种推理密集型任务上的表现。

    

    我们提出了一种称为“退后提示”的简单提示技术，使得大型语言模型能够通过从包含具体细节的实例中进行抽象，得出高层概念和基本原理。利用这些概念和原理来指导推理步骤，语言模型在正确推理路径上显著提升了能力。我们使用PaLM-2L模型进行了退后提示实验，在包括STEM、知识问答和多跳推理在内的各种具有挑战性的推理密集型任务上观察到了明显的性能提升。例如，在MMLU物理和化学任务上，退后提示可以将PaLM-2L的性能提升7%和11%，在TimeQA任务上提升27%，在MuSiQue任务上提升7%。

    We present Step-Back Prompting, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide the reasoning steps, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of Step-Back Prompting with PaLM-2L models and observe substantial performance gains on a wide range of challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting improves PaLM-2L performance on MMLU Physics and Chemistry by 7% and 11%, TimeQA by 27%, and MuSiQue by 7%.
    
[^75]: 什么时候是基于不可知激励强化学习的统计可处理性？

    When is Agnostic Reinforcement Learning Statistically Tractable?. (arXiv:2310.06113v1 [cs.LG])

    [http://arxiv.org/abs/2310.06113](http://arxiv.org/abs/2310.06113)

    本文研究了基于不可知的PAC强化学习的问题，引入了跨越容量这个新的复杂度度量，并发现了在生成和在线访问模型之间以及在线访问下的确定和随机MDP之间的差异。

    

    本文研究了基于不可知的PAC强化学习（RL）的问题：给定一个策略类别Π，需要和一个未知的有可能有大状态和动作空间的MDP进行多少轮互动来学习一个关于Π的ε-次优策略？为此，我们引入了一个新的复杂度度量，称为“跨越容量”，它仅依赖于策略类别Π，并且与MDP的动态无关。通过一个生成模型，我们证明了对于任何策略类别Π，有界的跨越容量可以刻画PAC可学习性。然而，对于在线RL来说，情况更加微妙。我们证明了存在一个具有有界跨越容量的策略类别Π，需要超多项式数量的样本才能学习。这揭示了在不同生成和在线访问模型之间以及在线访问下的确定/随机MDP之间的不确定可学习性之间的出乎意料的差异。在积极的方面，我们识别出一个额外的“太阳”

    We study the problem of agnostic PAC reinforcement learning (RL): given a policy class $\Pi$, how many rounds of interaction with an unknown MDP (with a potentially large state and action space) are required to learn an $\epsilon$-suboptimal policy with respect to $\Pi$? Towards that end, we introduce a new complexity measure, called the \emph{spanning capacity}, that depends solely on the set $\Pi$ and is independent of the MDP dynamics. With a generative model, we show that for any policy class $\Pi$, bounded spanning capacity characterizes PAC learnability. However, for online RL, the situation is more subtle. We show there exists a policy class $\Pi$ with a bounded spanning capacity that requires a superpolynomial number of samples to learn. This reveals a surprising separation for agnostic learnability between generative access and online access models (as well as between deterministic/stochastic MDPs under online access). On the positive side, we identify an additional \emph{sunf
    
[^76]: 宽深度神经网络的鲁棒过拟合的理论分析：一种NTK方法

    Theoretical Analysis of Robust Overfitting for Wide DNNs: An NTK Approach. (arXiv:2310.06112v1 [cs.LG])

    [http://arxiv.org/abs/2310.06112](http://arxiv.org/abs/2310.06112)

    本文理论分析了宽深度神经网络的鲁棒过拟合现象，并提出了一种名为Adv-NTK的AT算法来增强神经网络的鲁棒性。

    

    对抗训练(AT)是增强深度神经网络(DNNs)鲁棒性的经典方法。然而，最近的研究实验证明它存在鲁棒过拟合现象，即长时间的AT可能对DNNs的鲁棒性产生不利影响。本文对DNNs的鲁棒过拟合提出了一个理论解释。具体而言，我们将神经切向核(NTK)理论非平凡地扩展到AT，并证明了通过AT训练的宽DNN可以很好地近似为一个线性化的DNN。此外，对于平方损失，可以推导出线性化DNN的闭式AT动力学，揭示了一种新的AT退化现象：长期的AT将导致宽DNN退化为没有AT的DNN，从而引起鲁棒过拟合。根据我们的理论结果，我们进一步设计了一种名为Adv-NTK的方法，这是第一种针对无限宽的DNNs的AT算法。在实际数据集上的实验结果表明，Adv-NTK可以帮助无限宽的DNNs提升鲁棒性。

    Adversarial training (AT) is a canonical method for enhancing the robustness of deep neural networks (DNNs). However, recent studies empirically demonstrated that it suffers from robust overfitting, i.e., a long time AT can be detrimental to the robustness of DNNs. This paper presents a theoretical explanation of robust overfitting for DNNs. Specifically, we non-trivially extend the neural tangent kernel (NTK) theory to AT and prove that an adversarially trained wide DNN can be well approximated by a linearized DNN. Moreover, for squared loss, closed-form AT dynamics for the linearized DNN can be derived, which reveals a new AT degeneration phenomenon: a long-term AT will result in a wide DNN degenerates to that obtained without AT and thus cause robust overfitting. Based on our theoretical results, we further design a method namely Adv-NTK, the first AT algorithm for infinite-width DNNs. Experiments on real-world datasets show that Adv-NTK can help infinite-width DNNs enhance comparab
    
[^77]: BYOC: 使用合著的类别描述个性化进行少样本分类

    BYOC: Personalized Few-Shot Classification with Co-Authored Class Descriptions. (arXiv:2310.06111v1 [cs.CL])

    [http://arxiv.org/abs/2310.06111](http://arxiv.org/abs/2310.06111)

    提出了一种个性化的少样本文本分类方法，使用合著的类别描述作为提示，用户与LLM交互合作进行标注，形成分类提示，实验结果显示其准确率达到了使用大规模数据集进行训练的模型的82%。

    

    文本分类是许多自然语言处理应用的重要组成部分，但现有方法要么需要大规模带标注的语料库进行模型训练，要么在使用大型语言模型作为基础时，需要精心设计提示并使用能容纳许多示例的长上下文。结果，普通用户不能为自己构建分类器。为了解决这个问题，我们提出了一种新颖的个性化少样本文本分类方法，使用LLM（大型语言模型）。LLM不是使用少样本示例，而是由用户和LLM合著的每个类别的显著特征的描述进行提示。在用户标注每个少样本示例时，LLM会提出相关的问题，用户给出答案。示例、问题和答案被总结成分类提示。实验证明，我们的方法可以获得高准确率的分类器，其性能达到使用大规模数据集进行训练的模型的82%。

    Text classification is a well-studied and versatile building block for many NLP applications. Yet, existing approaches require either large annotated corpora to train a model with or, when using large language models as a base, require carefully crafting the prompt as well as using a long context that can fit many examples. As a result, it is not possible for end-users to build classifiers for themselves. To address this issue, we propose a novel approach to few-shot text classification using an LLM. Rather than few-shot examples, the LLM is prompted with descriptions of the salient features of each class. These descriptions are coauthored by the user and the LLM interactively: while the user annotates each few-shot example, the LLM asks relevant questions that the user answers. Examples, questions, and answers are summarized to form the classification prompt. Our experiments show that our approach yields high accuracy classifiers, within 82% of the performance of models trained with s
    
[^78]: 从懒惰到丰富训练动态的洞察力

    Grokking as the Transition from Lazy to Rich Training Dynamics. (arXiv:2310.06110v1 [stat.ML])

    [http://arxiv.org/abs/2310.06110](http://arxiv.org/abs/2310.06110)

    研究发现洞察现象可能是由神经网络从懒惰训练动态过渡到丰富的特征学习模式的结果，通过跟踪足够的统计量，发现洞察是在网络首先尝试拟合核回归解决方案后，进行后期特征学习找到通用解决方案之后的结果。

    

    我们提出了洞察现象，即神经网络的训练损失在测试损失之前大幅下降，可能是由于神经网络从懒惰的训练动态转变为丰富的特征学习模式。为了说明这一机制，我们研究了在没有正则化的情况下，使用Vanilla梯度下降方法在多项式回归问题上进行的两层神经网络的训练，该训练展现了无法用现有理论解释的洞察现象。我们确定了该网络测试损失的足够统计量，并通过训练跟踪这些统计量揭示了洞察现象的发生。我们发现，在这种情况下，网络首先尝试使用初始特征拟合核回归解决方案，接着在训练损失已经很低的情况下进行后期特征学习，从而找到了一个能够泛化的解决方案。我们发现，洞察产生的关键因素是特征学习的速率，这可以通过缩放网络参数来精确控制。

    We propose that the grokking phenomenon, where the train loss of a neural network decreases much earlier than its test loss, can arise due to a neural network transitioning from lazy training dynamics to a rich, feature learning regime. To illustrate this mechanism, we study the simple setting of vanilla gradient descent on a polynomial regression problem with a two layer neural network which exhibits grokking without regularization in a way that cannot be explained by existing theories. We identify sufficient statistics for the test loss of such a network, and tracking these over training reveals that grokking arises in this setting when the network first attempts to fit a kernel regression solution with its initial features, followed by late-time feature learning where a generalizing solution is identified after train loss is already low. We find that the key determinants of grokking are the rate of feature learning -- which can be controlled precisely by parameters that scale the ne
    
[^79]: 用于基于风险决策的深度学习分类中离散输入的不确定性量化

    Quantifying Uncertainty in Deep Learning Classification with Noise in Discrete Inputs for Risk-Based Decision Making. (arXiv:2310.06105v1 [stat.ML])

    [http://arxiv.org/abs/2310.06105](http://arxiv.org/abs/2310.06105)

    本文提出了一种数学框架来量化基于风险决策的深度学习分类中离散输入的预测不确定性。我们的研究有助于在决策过程中减轻相关风险，并且可以适用于处理涉及分类和离散特征变量的问题。

    

    深度神经网络（DNN）模型在基于风险决策中的应用引起了广泛关注，在医疗、金融、制造和质量控制等领域具有广泛的应用。为了在决策过程中减轻相关风险，预测的置信度或不确定性应该与算法的整体性能一起进行评估。最近关于贝叶斯深度学习的研究有助于量化源于输入噪声和模型参数的预测不确定性。然而，这些模型中对输入噪声的正态性假设限制了其适用于涉及分类和离散特征变量的问题。在本文中，我们提出了一个数学框架来量化DNN模型的预测不确定性。这种预测不确定性源于遵循已知有限离散分布的预测器误差。然后，我们使用该框架进行了一个案例研究，预测结核病患者治疗结果的过程中的不确定性。

    The use of Deep Neural Network (DNN) models in risk-based decision-making has attracted extensive attention with broad applications in medical, finance, manufacturing, and quality control. To mitigate prediction-related risks in decision making, prediction confidence or uncertainty should be assessed alongside the overall performance of algorithms. Recent studies on Bayesian deep learning helps quantify prediction uncertainty arises from input noises and model parameters. However, the normality assumption of input noise in these models limits their applicability to problems involving categorical and discrete feature variables in tabular datasets. In this paper, we propose a mathematical framework to quantify prediction uncertainty for DNN models. The prediction uncertainty arises from errors in predictors that follow some known finite discrete distribution. We then conducted a case study using the framework to predict treatment outcome for tuberculosis patients during their course of t
    
[^80]: 使用变分背门调整进行高维因果推断

    High Dimensional Causal Inference with Variational Backdoor Adjustment. (arXiv:2310.06100v1 [cs.AI])

    [http://arxiv.org/abs/2310.06100](http://arxiv.org/abs/2310.06100)

    本论文介绍了一种用于高维因果推断的变分背门调整技术，能够处理高维治疗和混杂因素，并成功应用于医疗数据中。

    

    背门调整是一种因果推断技术，用于从纯观察数据中估计干预数量。在医疗环境中，背门调整可用于控制混杂因素并估计治疗效果。然而，高维治疗和混杂因素可能引发一系列潜在问题：可计算性、可辨识性、优化等。在这项工作中，我们采用生成建模方法来解决高维治疗和混杂因素的背门调整问题。我们将背门调整视为一种变分推断优化问题，无需依赖代理变量和隐藏混杂因素。实验上，我们的方法能够在各种高维环境中估计干预概率，包括半合成X光医疗数据。据我们所知，这是背门调整的首次应用，其中所有相关变量都是高维的。

    Backdoor adjustment is a technique in causal inference for estimating interventional quantities from purely observational data. For example, in medical settings, backdoor adjustment can be used to control for confounding and estimate the effectiveness of a treatment. However, high dimensional treatments and confounders pose a series of potential pitfalls: tractability, identifiability, optimization. In this work, we take a generative modeling approach to backdoor adjustment for high dimensional treatments and confounders. We cast backdoor adjustment as an optimization problem in variational inference without reliance on proxy variables and hidden confounders. Empirically, our method is able to estimate interventional likelihood in a variety of high dimensional settings, including semi-synthetic X-ray medical data. To the best of our knowledge, this is the first application of backdoor adjustment in which all the relevant variables are high dimensional.
    
[^81]: 基于分位数的极大似然训练用于异常检测

    Quantile-based Maximum Likelihood Training for Outlier Detection. (arXiv:2310.06085v1 [cs.CV])

    [http://arxiv.org/abs/2310.06085](http://arxiv.org/abs/2310.06085)

    本文提出了一种基于分位数的极大似然目标，用于改进异常检测中异常值的分离程度。通过将正则化流拟合到预训练的判别性特征，并根据对数似然度评估来检测异常值。实验结果表明，这种方法优于最先进的无监督模型。

    

    判别性学习有效地对图像分类预测真实的对象类别。然而，在异常值方面，它经常导致误报阳性，这在自主驾驶和视频监视系统等应用中引起了严重关注。以往解决这个挑战的尝试包括使用实际异常值数据通过对比学习训练图像分类器，或者通过合成异常值进行自我监督学习。此外，像素空间中对内点进行无监督生成建模对于异常检测显示出有限的成功。在这项工作中，我们引入了一种基于分位数的极大似然目标，用于学习内点分布，以在推断过程中提高异常值的分离程度。我们的方法通过将正则化流拟合到预训练的判别性特征，并根据评估的对数似然度来检测异常值。实验评估证明了我们方法的有效性，因为它超过了最先进的无监督方法的性能。

    Discriminative learning effectively predicts true object class for image classification. However, it often results in false positives for outliers, posing critical concerns in applications like autonomous driving and video surveillance systems. Previous attempts to address this challenge involved training image classifiers through contrastive learning using actual outlier data or synthesizing outliers for self-supervised learning. Furthermore, unsupervised generative modeling of inliers in pixel space has shown limited success for outlier detection. In this work, we introduce a quantile-based maximum likelihood objective for learning the inlier distribution to improve the outlier separation during inference. Our approach fits a normalizing flow to pre-trained discriminative features and detects the outliers according to the evaluated log-likelihood. The experimental evaluation demonstrates the effectiveness of our method as it surpasses the performance of the state-of-the-art unsupervi
    
[^82]: Transformers和大型语言模型在化学和药物发现中的应用

    Transformers and Large Language Models for Chemistry and Drug Discovery. (arXiv:2310.06083v1 [cs.LG])

    [http://arxiv.org/abs/2310.06083](http://arxiv.org/abs/2310.06083)

    transformers和大型语言模型在化学和药物发现中的应用取得了显著进展，通过类比自然语言和化学的关系，这些模型能够解决药物发现过程中的重要问题，进一步的发展将带来更多突破和进步。

    

    这篇论文探讨了语言建模在过去几年取得的显著进展，主要是由于Transformer架构的发明，引发了机器学习领域的许多领域的革命，以及在化学和生物学领域的突破。本章中，我们探讨了化学和自然语言之间的类比是如何启发使用Transformer来解决药物发现过程中的重要瓶颈，如逆合成规划和化学空间探索。这一革命始于能够使用一种类型的数据执行特定任务的模型，比如线性化的分子图，然后发展到包括其他类型的数据，如来自分析仪器的光谱、合成行动和人类语言。一种新的趋势利用了大型语言模型的最新发展，产生了一系列能够解决化学中通用任务的模型，这些模型的灵活性是由自然语言提供的。随着我们继续探索和应用这些能力，我们可以期待进一步的突破和进步。

    Language modeling has seen impressive progress over the last years, mainly prompted by the invention of the Transformer architecture, sparking a revolution in many fields of machine learning, with breakthroughs in chemistry and biology. In this chapter, we explore how analogies between chemical and natural language have inspired the use of Transformers to tackle important bottlenecks in the drug discovery process, such as retrosynthetic planning and chemical space exploration. The revolution started with models able to perform particular tasks with a single type of data, like linearised molecular graphs, which then evolved to include other types of data, like spectra from analytical instruments, synthesis actions, and human language. A new trend leverages recent developments in large language models, giving rise to a wave of models capable of solving generic tasks in chemistry, all facilitated by the flexibility of natural language. As we continue to explore and harness these capabilit
    
[^83]: 用于采样、优化和提升的通用Ito链的Ito扩散逼近

    Ito Diffusion Approximation of Universal Ito Chains for Sampling, Optimization and Boosting. (arXiv:2310.06081v1 [math.OC])

    [http://arxiv.org/abs/2310.06081](http://arxiv.org/abs/2310.06081)

    本文研究了一类广泛的马尔可夫链，即Ito链的Ito扩散逼近。与大多数相关论文不同，我们的链具有各向同性和状态相关的噪声，并可以适用于多种应用场景。我们证明了Ito链与对应的随机微分方程之间的W2-距离的上界。这些结果改进了已有的估计方法，并在某些特殊情况下提供了首次的分析。

    

    本文考虑了一类相当一般和广泛的马尔可夫链，即Ito链，其类似于某些随机微分方程的Euler-Maruyama离散化。我们研究的链是一个统一的理论分析框架。与大多数相关论文中的正态和状态独立噪声不同，我们的链具有几乎任意各向同性和状态相关噪声。此外，我们链的漂移和扩散系数可以是精确的，以涵盖诸如随机梯度Langevin动力学、采样、随机梯度下降或随机梯度提升等广泛的应用。我们证明了Ito链与对应的随机微分方程之间的W2-距离的一个上界。这些结果改进或覆盖了大部分已知的估计。此外，对于某些特殊情况，我们的分析是第一个。

    This work considers a rather general and broad class of Markov chains, Ito chains that look like Euler-Maryama discretization of some Stochastic Differential Equation. The chain we study is a unified framework for theoretical analysis. It comes with almost arbitrary isotropic and state-dependent noise instead of normal and state-independent one, as in most related papers. Moreover, our chain's drift and diffusion coefficient can be inexact to cover a wide range of applications such as Stochastic Gradient Langevin Dynamics, sampling, Stochastic Gradient Descent, or Stochastic Gradient Boosting. We prove an upper bound for $W_{2}$-distance between laws of the Ito chain and the corresponding Stochastic Differential Equation. These results improve or cover most of the known estimates. Moreover, for some particular cases, our analysis is the first.
    
[^84]: 可展示的时间序列预测

    Performative Time-Series Forecasting. (arXiv:2310.06077v1 [cs.LG])

    [http://arxiv.org/abs/2310.06077](http://arxiv.org/abs/2310.06077)

    本论文研究了时间序列预测中的展示性问题，提出了一种新的方法（FPS），通过利用延迟响应的概念来解决展示性引起的分布变化，并实现准确的预测。

    

    时间序列预测是各个领域中的一个关键挑战，在近年来取得了实质性的进展。许多现实生活场景，如公共卫生、经济和社会应用，涉及到反馈循环，其中预测结果可能会影响到预测的结果，进而改变目标变量的分布。这种现象被称为展示性，引入了可能出现“自我抵消”或“自我实现”的预测的潜力。尽管在各个领域中对分类问题进行了广泛的研究，但展示性在机器学习视角下的时间序列预测问题尚未得到广泛探讨。在这篇论文中，我们对可展示的时间序列预测（PeTS）进行了形式化，解决了当可能存在展示性引起的分布变化时的准确预测挑战。我们提出了一种新颖方法，特征展示性转移（FPS），它利用延迟响应的概念来预测分布的变化和随后的变量。

    Time-series forecasting is a critical challenge in various domains and has witnessed substantial progress in recent years. Many real-life scenarios, such as public health, economics, and social applications, involve feedback loops where predictions can influence the predicted outcome, subsequently altering the target variable's distribution. This phenomenon, known as performativity, introduces the potential for 'self-negating' or 'self-fulfilling' predictions. Despite extensive studies in classification problems across domains, performativity remains largely unexplored in the context of time-series forecasting from a machine-learning perspective.  In this paper, we formalize performative time-series forecasting (PeTS), addressing the challenge of accurate predictions when performativity-induced distribution shifts are possible. We propose a novel approach, Feature Performative-Shifting (FPS), which leverages the concept of delayed response to anticipate distribution shifts and subseque
    
[^85]: 使用自监督学习和患者表型研究的痛苦预测：预防阿片类药物成瘾的尝试

    Pain Forecasting using Self-supervised Learning and Patient Phenotyping: An attempt to prevent Opioid Addiction. (arXiv:2310.06075v1 [cs.AI])

    [http://arxiv.org/abs/2310.06075](http://arxiv.org/abs/2310.06075)

    本研究旨在通过使用自监督学习方法和患者表型研究，预测患者未来的疼痛轨迹，以帮助患者管理镰状细胞贫血，改善生活质量，并减少对阿片类药物的依赖和副作用。

    

    镰状细胞贫血（SCD）是一种慢性遗传性疾病，特征为反复发作的急性疼痛。阿片类药物通常用于管理这些疼痛发作；在这种疾病中使用阿片类药物管理疼痛的程度是一个争议的问题。吸食成瘾的风险和阿片类药物治疗的副作用往往会导致将来更多的疼痛发作。因此，准确预测患者将来的疼痛发展轨迹对于帮助患者管理他们的SCD以提高生活质量而不损害治疗至关重要。由于疼痛主要是由患者自行报告记录的，获得许多疼痛记录来设计预测模型是具有挑战性的。因此，在解决疼痛预测问题时，仅依靠监督学习的方式既昂贵又痛苦（因为需要患者配合）。鉴于这一挑战，我们提出使用自监督学习方法来解决疼痛预测问题。此外，对这种时间序列数据进行聚类对于患者表型研究至关重要。

    Sickle Cell Disease (SCD) is a chronic genetic disorder characterized by recurrent acute painful episodes. Opioids are often used to manage these painful episodes; the extent of their use in managing pain in this disorder is an issue of debate. The risk of addiction and side effects of these opioid treatments can often lead to more pain episodes in the future. Hence, it is crucial to forecast future patient pain trajectories to help patients manage their SCD to improve their quality of life without compromising their treatment. It is challenging to obtain many pain records to design forecasting models since it is mainly recorded by patients' self-report. Therefore, it is expensive and painful (due to the need for patient compliance) to solve pain forecasting problems in a purely supervised manner. In light of this challenge, we propose to solve the pain forecasting problem using self-supervised learning methods. Also, clustering such time-series data is crucial for patient phenotyping,
    
[^86]: 最优探索不比汤普森采样更困难

    Optimal Exploration is no harder than Thompson Sampling. (arXiv:2310.06069v1 [stat.ML])

    [http://arxiv.org/abs/2310.06069](http://arxiv.org/abs/2310.06069)

    这项研究提出了一个问题：是否存在一种能够同时进行最优探索和只需要相同计算操作的算法？

    

    在给定一组臂$\mathcal{Z}\subset \mathbb{R}^d$和未知参数向量$\theta_\ast\in\mathbb{R}^d$的情况下，纯探索线性臂问题旨在通过对$x^{\top}\theta_{\ast}$的噪声测量，返回$\arg\max_{z\in \mathcal{Z}} z^{\top}\theta_{\ast}$，并以高概率找到正确解。现有的（渐近）最优方法要求要么为每个臂$z\in \mathcal{Z}$进行潜在昂贵的投影，要么在每个时间点明确地维护一部分正在考虑的$\mathcal{Z}$。这种复杂性与流行且简单的汤普森采样算法用于最小化后悔的情况完全相反，后者只需要访问后验采样和argmax oracle，并且在任何时间点都不需要枚举$\mathcal{Z}$。不幸的是，已知汤普森采样对于纯探索是次优的。在这项工作中，我们提出了一个自然的问题：是否存在一种算法能够进行最优探索，而且只需要相同的计算操作？

    Given a set of arms $\mathcal{Z}\subset \mathbb{R}^d$ and an unknown parameter vector $\theta_\ast\in\mathbb{R}^d$, the pure exploration linear bandit problem aims to return $\arg\max_{z\in \mathcal{Z}} z^{\top}\theta_{\ast}$, with high probability through noisy measurements of $x^{\top}\theta_{\ast}$ with $x\in \mathcal{X}\subset \mathbb{R}^d$. Existing (asymptotically) optimal methods require either a) potentially costly projections for each arm $z\in \mathcal{Z}$ or b) explicitly maintaining a subset of $\mathcal{Z}$ under consideration at each time. This complexity is at odds with the popular and simple Thompson Sampling algorithm for regret minimization, which just requires access to a posterior sampling and argmax oracle, and does not need to enumerate $\mathcal{Z}$ at any point. Unfortunately, Thompson sampling is known to be sub-optimal for pure exploration. In this work, we pose a natural question: is there an algorithm that can explore optimally and only needs the same comput
    
[^87]: 预警和隐变化随机动力学系统及元标签纠正的早期预警

    Early Warning via tipping-preserving latent stochastic dynamical system and meta label correcting. (arXiv:2310.06059v1 [cs.LG])

    [http://arxiv.org/abs/2310.06059](http://arxiv.org/abs/2310.06059)

    提出了一种通过融合真实数据和隐性随机微分方程产生的增广数据的元学习框架，用于提高对早期癫痫发作信号的预测准确率，并通过提取的临界动力学特征来标记噪声数据。

    

    对癫痫患者进行早期预警对于他们的安全和福祉至关重要，可以预防或减少癫痫发作的严重程度。通过患者的脑电图数据，我们提出了一个元学习框架，以改进对早期癫痫发作信号的预测。为了更好地利用元标签纠正方法，我们融合了真实数据和隐性随机微分方程（SDE）产生的增广数据的信息。此外，我们还通过真实数据和隐性随机微分方程的过渡时间分布来优选选择潜在的动力系统。通过这种方式，提取的临界动力学特征也被集成到元网络中，以更好地标记噪声数据。为了验证我们的方法，我们将LSTM实施为基准模型。我们进行了一系列实验来预测各种长期窗口（1-2秒的输入数据）内的癫痫发作，并发现预测准确率出现了令人惊讶的增加。

    Early warning for epilepsy patients is crucial for their safety and well-being, in terms of preventing or minimizing the severity of seizures. Through the patients' EEG data, we propose a meta learning framework for improving prediction on early ictal signals. To better utilize the meta label corrector method, we fuse the information from both the real data and the augmented data from the latent Stochastic differential equation(SDE). Besides, we also optimally select the latent dynamical system via distribution of transition time between real data and that from the latent SDE. In this way, the extracted tipping dynamical feature is also integrated into the meta network to better label the noisy data. To validate our method, LSTM is implemented as the baseline model. We conduct a series of experiments to predict seizure in various long-term window from 1-2 seconds input data and find surprisingly increment of prediction accuracy.
    
[^88]: 知识蒸馏用于异常检测

    Knowledge Distillation for Anomaly Detection. (arXiv:2310.06047v1 [cs.LG])

    [http://arxiv.org/abs/2310.06047](http://arxiv.org/abs/2310.06047)

    本文介绍了一种基于知识蒸馏的方法，用于将无监督异常检测模型压缩成可部署的有监督模型，并提出了一些技术来改善检测灵敏度。压缩后的模型在减小大小和内存占用的同时，仍然具有与较大模型相当的性能。

    

    无监督深度学习技术广泛用于识别异常行为。这些方法的性能受到训练数据量和模型大小的影响。然而，模型大小常常限制了在资源受限设备上的部署。我们提出了一种基于知识蒸馏的新型过程，将无监督异常检测模型压缩成一种可部署的有监督模型，并提出了一系列技术来改善检测灵敏度。压缩后的模型在显著减少大小和内存占用的同时，表现与较大模型相当。

    Unsupervised deep learning techniques are widely used to identify anomalous behaviour. The performance of such methods is a product of the amount of training data and the model size. However, the size is often a limiting factor for the deployment on resource-constrained devices. We present a novel procedure based on knowledge distillation for compressing an unsupervised anomaly detection model into a supervised deployable one and we suggest a set of techniques to improve the detection sensitivity. Compressed models perform comparably to their larger counterparts while significantly reducing the size and memory footprint.
    
[^89]: 通过确定性对流模型的生成性集成深度学习，用于严重天气预测

    Generative ensemble deep learning severe weather prediction from a deterministic convection-allowing model. (arXiv:2310.06045v1 [cs.LG])

    [http://arxiv.org/abs/2310.06045](http://arxiv.org/abs/2310.06045)

    本论文开发了一种集成后处理方法，将生成对抗网络（CGANs）和卷积神经网络（CNN）结合起来，对严重天气进行概率预测。该方法在使用HRRR预报作为输入数据，在2021年的测试数据集上相对于其他基于神经网络的方法提高了高达20％的Brier技巧分数（BSS）。

    

    开发了一种用于概率预测美国本土严重天气（龙卷风、冰雹和大风阵）的集成后处理方法。该方法将条件生成对抗网络（CGANs）与卷积神经网络（CNN）结合起来，用于后处理对流允许模型（CAM）的预测。CGANs被设计用于从确定性CAM预测中创建合成集成成员，其输出经过CNN处理以估计严重天气的概率。该方法使用高分辨率快速刷新（HRRR）1-24小时预报作为输入，以及暴风预警中心（SPC）的严重天气报告作为目标进行测试。在2021年的HRRR预测测试数据集中，该方法相对于其他基于神经网络的参考方法提高了高达20％的Brier技巧分数（BSS）。

    An ensemble post-processing method is developed for the probabilistic prediction of severe weather (tornadoes, hail, and wind gusts) over the conterminous United States (CONUS). The method combines conditional generative adversarial networks (CGANs), a type of deep generative model, with a convolutional neural network (CNN) to post-process convection-allowing model (CAM) forecasts. The CGANs are designed to create synthetic ensemble members from deterministic CAM forecasts, and their outputs are processed by the CNN to estimate the probability of severe weather. The method is tested using High-Resolution Rapid Refresh (HRRR) 1--24 hr forecasts as inputs and Storm Prediction Center (SPC) severe weather reports as targets. The method produced skillful predictions with up to 20% Brier Skill Score (BSS) increases compared to other neural-network-based reference methods using a testing dataset of HRRR forecasts in 2021. For the evaluation of uncertainty quantification, the method is overcon
    
[^90]: DyST：面向实际视频的动态神经场景表示

    DyST: Towards Dynamic Neural Scene Representations on Real-World Videos. (arXiv:2310.06020v1 [cs.CV])

    [http://arxiv.org/abs/2310.06020](http://arxiv.org/abs/2310.06020)

    DyST模型通过学习动态场景的潜在分解，从实际视频中捕捉到了场景的3D结构和动态特性，并实现了对相机和场景内容的独立控制视图生成。

    

    对世界的视觉理解超越了单个图像的语义和平面结构。我们的目标是从单目实际视频中捕捉到实际场景的3D结构和动态特性。我们的Dynamic Scene Transformer（DyST）模型利用了最近的神经场景表示研究成果，学习了单目实际视频的潜在分解，包括场景内容、每个视角的场景动态和相机姿态。通过在单目视频和我们的新的合成数据集DySO上进行一种新颖的协同训练，实现了这种分离。DyST学习到了动态场景的具体潜在表示，使得可以对场景的相机和内容进行独立控制的视图生成成为可能。

    Visual understanding of the world goes beyond the semantics and flat structure of individual images. In this work, we aim to capture both the 3D structure and dynamics of real-world scenes from monocular real-world videos. Our Dynamic Scene Transformer (DyST) model leverages recent work in neural scene representation to learn a latent decomposition of monocular real-world videos into scene content, per-view scene dynamics, and camera pose. This separation is achieved through a novel co-training scheme on monocular videos and our new synthetic dataset DySO. DyST learns tangible latent representations for dynamic scenes that enable view generation with separate control over the camera and the content of the scene.
    
[^91]: AI驱动的剥夺中的分而治之动态

    Divide-and-Conquer Dynamics in AI-Driven Disempowerment. (arXiv:2310.06009v1 [cs.CY])

    [http://arxiv.org/abs/2310.06009](http://arxiv.org/abs/2310.06009)

    这项研究通过构建游戏理论模型，研究了AI驱动的剥夺中的不团结问题。研究发现，当前受害者需要让未来受害者认识到他们的利益同样面临严重和紧迫的威胁，以激励未来受害者以团结支持当前受害者。

    

    AI公司试图创造出在大部分经济价值工作上超越人类的AI系统。当前的AI模型已经自动化削弱了一些艺术家、演员和作家的生计。但是在那些优先考虑当前危害和未来危害之间存在着内讧。我们构建了一个博弈论模型来研究这种不团结的原因和后果。我们的模型还有助于解释为什么在历史上，面临共同威胁的利益相关方发现联合起来对抗该威胁是有利的，而该共同威胁又发现分而治之是有利的。在现实参数假设下，我们的模型提出了几个预测，在历史经验记录中得到了初步的证实。

    AI companies are attempting to create AI systems that outperform humans at most economically valuable work. Current AI models are already automating away the livelihoods of some artists, actors, and writers. But there is infighting between those who prioritize current harms and future harms. We construct a game-theoretic model of conflict to study the causes and consequences of this disunity. Our model also helps explain why throughout history, stakeholders sharing a common threat have found it advantageous to unite against it, and why the common threat has in turn found it advantageous to divide and conquer.  Under realistic parameter assumptions, our model makes several predictions that find preliminary corroboration in the historical-empirical record. First, current victims of AI-driven disempowerment need the future victims to realize that their interests are also under serious and imminent threat, so that future victims are incentivized to support current victims in solidarity. Se
    
[^92]: 重新思考高效大规模语言模型训练中的内存和通信成本

    Rethinking Memory and Communication Cost for Efficient Large Language Model Training. (arXiv:2310.06003v1 [cs.LG])

    [http://arxiv.org/abs/2310.06003](http://arxiv.org/abs/2310.06003)

    本文研究了大型语言模型训练中内存和通信成本对训练速度的影响，并提出了一种平衡内存和通信的优化器（PaRO）。此外，还提出了一种用于大模型训练的分层重叠环通信拓扑结构（HO-Ring），实验证明该算法提高了训练过程中的通信效率。

    

    随着模型规模和训练数据集的不断增加，大规模模型训练框架通过各种分片技术减小内存消耗。然而，巨大的通信开销降低了训练效率，特别是在网络带宽变化的公共云环境中。在本文中，我们重新思考内存消耗和通信开销对大型语言模型训练速度的影响，并提出了一种平衡内存和通信的部分冗余优化器(PaRO)。PaRO通过将GPU集群分组和引入微小的组内内存冗余，减少了组间通信的数量和频率，从而提高了模型的训练效率。此外，我们提出了一种分层重叠环(HO-Ring)通信拓扑结构，以增强大模型训练中节点之间或跨交换机之间的通信效率。我们的实验证明，HO-Ring算法改善了训练过程中的通信效率。

    As model sizes and training datasets continue to increase, large-scale model training frameworks reduce memory consumption by various sharding techniques. However, the huge communication overhead reduces the training efficiency, especially in public cloud environments with varying network bandwidths. In this paper, we rethink the impact of memory consumption and communication overhead on the training speed of large language model, and propose a memory-communication balanced \underline{Pa}rtial \underline{R}edundancy \underline{O}ptimizer (PaRO). PaRO reduces the amount and frequency of inter-group communication by grouping GPU clusters and introducing minor intra-group memory redundancy, thereby improving the training efficiency of the model. Additionally, we propose a Hierarchical Overlapping Ring (HO-Ring) communication topology to enhance communication efficiency between nodes or across switches in large model training. Our experiments demonstrate that the HO-Ring algorithm improves
    
[^93]: LCOT: 线性循环最优传输

    LCOT: Linear circular optimal transport. (arXiv:2310.06002v1 [cs.LG])

    [http://arxiv.org/abs/2310.06002](http://arxiv.org/abs/2310.06002)

    本文提出了一种线性循环最优传输（LCOT）方法，用于解决循环概率测度的最优传输问题。LCOT方法具有明确的线性嵌入，可以应用机器学习算法，并在循环最优传输的基础上进行修改。通过理论分析和数值实验，证明了LCOT方法的有效性。

    

    最近，非欧几里德空间上支持的测度的最优传输问题在涉及表示学习的各种应用中引起了广泛的关注。在本文中，我们专注于循环概率测度，即支持在单位圆上的概率测度，并引入了一种新的计算效率较高的度量方法，称为线性循环最优传输（LCOT）。所提出的度量方法具有明确的线性嵌入，使得可以将机器学习（ML）算法应用于嵌入的测度，并无缝地修改ML算法的基本度量为LCOT。我们展示了所提出的度量方法扎根于循环最优传输（COT），可以被认为是相对于固定参考测度的COT度量的线性化。我们提供了对所提出的度量方法的理论分析，并导出了循环概率测度的成对比较的计算复杂性。最后，通过一系列的数值实验，我们验证了所提出的度量方法的有效性。

    The optimal transport problem for measures supported on non-Euclidean spaces has recently gained ample interest in diverse applications involving representation learning. In this paper, we focus on circular probability measures, i.e., probability measures supported on the unit circle, and introduce a new computationally efficient metric for these measures, denoted as Linear Circular Optimal Transport (LCOT). The proposed metric comes with an explicit linear embedding that allows one to apply Machine Learning (ML) algorithms to the embedded measures and seamlessly modify the underlying metric for the ML algorithm to LCOT. We show that the proposed metric is rooted in the Circular Optimal Transport (COT) and can be considered the linearization of the COT metric with respect to a fixed reference measure. We provide a theoretical analysis of the proposed metric and derive the computational complexities for pairwise comparison of circular probability measures. Lastly, through a set of numer
    
[^94]: 成本敏感的支持向量机的概率预测

    Cost-sensitive probabilistic predictions for support vector machines. (arXiv:2310.05997v1 [stat.ML])

    [http://arxiv.org/abs/2310.05997](http://arxiv.org/abs/2310.05997)

    提出了一种新方法，将支持向量机转化为成本敏感的概率分类器，并充分利用了正则化参数的信息。

    

    支持向量机（SVM）被广泛使用，是最受关注和使用的二元分类机器学习模型之一。SVM的分类是基于得分过程的，得到的是确定性的分类规则，可以转化为概率规则（在现成的SVM库中实现），但本质上并不是概率性的。另一方面，SVM中正则化参数的调优被认为需要很高的计算量，并且生成的信息没有充分利用，没有用于构建概率分类规则。在本文中，我们提出了一种新方法来生成SVM的概率输出。新方法具有以下三个特点。首先，它是设计为成本敏感的，因此可以很容易地适应敏感性（或真正例率，TPR）和特异性（真负例率，TNR）的不同重要性。结果，模型能够处理成本敏感的问题。

    Support vector machines (SVMs) are widely used and constitute one of the best examined and used machine learning models for two-class classification. Classification in SVM is based on a score procedure, yielding a deterministic classification rule, which can be transformed into a probabilistic rule (as implemented in off-the-shelf SVM libraries), but is not probabilistic in nature. On the other hand, the tuning of the regularization parameters in SVM is known to imply a high computational effort and generates pieces of information that are not fully exploited, not being used to build a probabilistic classification rule. In this paper we propose a novel approach to generate probabilistic outputs for the SVM. The new method has the following three properties. First, it is designed to be cost-sensitive, and thus the different importance of sensitivity (or true positive rate, TPR) and specificity (true negative rate, TNR) is readily accommodated in the model. As a result, the model can dea
    
[^95]: 通过一种新颖的网络科学算法改进患者分类的方法

    A novel Network Science Algorithm for Improving Triage of Patients. (arXiv:2310.05996v1 [cs.LG])

    [http://arxiv.org/abs/2310.05996](http://arxiv.org/abs/2310.05996)

    本研究开发了一种使用网络科学算法进行患者分类的新方法。该算法通过对患者数据进行分析，并通过严格的数据预处理和特征工程，实现了高准确率和性能。通过将人工智能引入患者分类，可以提高分类的准确性和效率。

    

    患者分类在医疗保健中起着至关重要的作用，可以根据患者病情的紧急程度确保及时和恰当的护理。传统的分类方法很大程度上依赖于人的判断，这可能存在主观性和错误的问题。最近，越来越多的关注点是利用人工智能（AI）开发用于患者分类的算法。本文介绍了一种新颖的患者分类算法的开发过程。它基于对患者数据的分析，以产生有关其优先级的决策。该算法是在包含相关患者信息的综合数据集上进行训练的，例如生命体征、症状和病史等。通过严格的预处理和特征工程，该算法被设计为能够准确地将患者分类到分类类别中。实验结果表明，我们的算法实现了高准确率和性能，优于传统的分类方法。通过将计算机科学纳入患者分类过程中，可以改善分类的准确性和效率。

    Patient triage plays a crucial role in healthcare, ensuring timely and appropriate care based on the urgency of patient conditions. Traditional triage methods heavily rely on human judgment, which can be subjective and prone to errors. Recently, a growing interest has been in leveraging artificial intelligence (AI) to develop algorithms for triaging patients. This paper presents the development of a novel algorithm for triaging patients. It is based on the analysis of patient data to produce decisions regarding their prioritization. The algorithm was trained on a comprehensive data set containing relevant patient information, such as vital signs, symptoms, and medical history. The algorithm was designed to accurately classify patients into triage categories through rigorous preprocessing and feature engineering. Experimental results demonstrate that our algorithm achieved high accuracy and performance, outperforming traditional triage methods. By incorporating computer science into the
    
[^96]: 数据增强在自动区域性冠状动脉分割中的应用：伪标签法用于疾病诊断

    Data Augmentation through Pseudolabels in Automatic Region Based Coronary Artery Segmentation for Disease Diagnosis. (arXiv:2310.05990v1 [eess.IV])

    [http://arxiv.org/abs/2310.05990](http://arxiv.org/abs/2310.05990)

    这项研究引入了伪标签作为数据增强技术，通过改善基准Yolo模型的性能，提高了冠状动脉分割的效果。

    

    冠状动脉疾病（CAD）是可预防的主要死亡和残疾原因之一。这些疾病的诊断通常困难且资源密集。血管造影图像中的动脉分割已经演变成为一种辅助工具，可以帮助临床医生进行准确的诊断。然而，由于数据量有限且构建数据集的困难，分割任务一直很具挑战性。在本研究中，我们引入了使用伪标签作为数据增强技术来改善基准Yolo模型性能的思想。该方法在验证数据集中将基线的F1分数提高了9％，在测试数据集中提高了3％。

    Coronary Artery Diseases(CADs) though preventable are one of the leading causes of death and disability. Diagnosis of these diseases is often difficult and resource intensive. Segmentation of arteries in angiographic images has evolved as a tool for assistance, helping clinicians in making accurate diagnosis. However, due to the limited amount of data and the difficulty in curating a dataset, the task of segmentation has proven challenging. In this study, we introduce the idea of using pseudolabels as a data augmentation technique to improve the performance of the baseline Yolo model. This method increases the F1 score of the baseline by 9% in the validation dataset and by 3% in the test dataset.
    
[^97]: 一种双潜在状态学习方法：利用区域网络相似性进行QoS预测

    A Dual Latent State Learning Approach: Exploiting Regional Network Similarities for QoS Prediction. (arXiv:2310.05988v1 [cs.LG])

    [http://arxiv.org/abs/2310.05988](http://arxiv.org/abs/2310.05988)

    本文介绍了一种名为R2SL的基于区域的双潜在状态学习网络，该网络通过汇总数据来捕捉区域网络行为的细微差别，并采用增强的Huber损失函数来提高QoS预测性能。

    

    特定区域内的个体对象，无论是用户还是服务，通常由于它们来自同一城市或自治系统（AS），展现出相似的网络状态。尽管存在区域网络相似性，但许多现有技术忽视了其潜力，导致由于数据稀疏性和标签不平衡等挑战而产生表现不佳。本文引入了基于区域的双潜在状态学习网络（R2SL），这是一个新颖的深度学习框架，旨在克服传统基于个体对象的QoS预测技术的缺点。与之前的方法不同，R2SL通过从公共区域汇总的数据构建了两个不同的区域网络潜在状态：城市网络潜在状态和AS网络潜在状态。此外，R2SL采用了增强的Huber损失函数。

    Individual objects, whether users or services, within a specific region often exhibit similar network states due to their shared origin from the same city or autonomous system (AS). Despite this regional network similarity, many existing techniques overlook its potential, resulting in subpar performance arising from challenges such as data sparsity and label imbalance. In this paper, we introduce the regional-based dual latent state learning network(R2SL), a novel deep learning framework designed to overcome the pitfalls of traditional individual object-based prediction techniques in Quality of Service (QoS) prediction. Unlike its predecessors, R2SL captures the nuances of regional network behavior by deriving two distinct regional network latent states: the city-network latent state and the AS-network latent state. These states are constructed utilizing aggregated data from common regions rather than individual object data. Furthermore, R2SL adopts an enhanced Huber loss function that
    
[^98]: 分析志愿者网络中关键用户行为趋势

    Analyzing Key Users' behavior trends in Volunteer-Based Networks. (arXiv:2310.05978v1 [cs.SI])

    [http://arxiv.org/abs/2310.05978](http://arxiv.org/abs/2310.05978)

    本研究分析了志愿者网络中关键用户的行为趋势，开发了两个新算法来揭示关键用户行为模式和预测未来行为。验证结果表明这些算法能有效分析关键用户行为的影响因素。

    

    在过去十年中，在线社交网络的使用量显著增加并持续流行。多个社交平台将志愿者作为核心组成部分。近年来，志愿者行为在志愿者网络中得到了广泛研究。本文主要探讨志愿者社交网络的发展，重点关注关键用户的行为和活动。我们开发了两个新算法：第一个算法揭示了关键用户随时间的行为模式；第二个算法利用机器学习方法生成预测模型，可以预测关键用户的未来行为，包括是否保持活跃捐赠者身份或改变行为成为主要接受者，反之亦然。这些算法使我们能够分析对行为预测产生显著影响的因素。我们利用来自一个P2P食品分享在线平台的240万用户数据来评估我们的算法。利用我们的算法，我们从中提取出关键用户的行为趋势。

    Online social networks usage has increased significantly in the last decade and continues to grow in popularity. Multiple social platforms use volunteers as a central component. The behavior of volunteers in volunteer-based networks has been studied extensively in recent years. Here, we explore the development of volunteer-based social networks, primarily focusing on their key users' behaviors and activities. We developed two novel algorithms: the first reveals key user behavior patterns over time; the second utilizes machine learning methods to generate a forecasting model that can predict the future behavior of key users, including whether they will remain active donors or change their behavior to become mainly recipients, and vice-versa. These algorithms allowed us to analyze the factors that significantly influence behavior predictions.  To evaluate our algorithms, we utilized data from over 2.4 million users on a peer-to-peer food-sharing online platform. Using our algorithm, we i
    
[^99]: CFDBench：流体动力学中机器学习方法的综合基准测试

    CFDBench: A Comprehensive Benchmark for Machine Learning Methods in Fluid Dynamics. (arXiv:2310.05963v1 [cs.LG])

    [http://arxiv.org/abs/2310.05963](http://arxiv.org/abs/2310.05963)

    这个论文介绍了CFDBench，这是一个针对计算流体动力学中四个经典问题的基准测试。它包含了不同边界条件、流体物理特性和域几何的数据，能帮助评估深度学习方法在解决物理问题中的表现。

    

    近年来，将深度学习应用于解决物理问题已经引起了广泛关注。数据驱动的深度学习方法可以生成能学习解决整个偏微分方程系统的算子。然而，现有方法仅在简单的流动方程（如Burger方程）上进行评估，并且仅考虑了对不同初始条件的泛化能力。本文构建了CFDBench，一个针对计算流体力学（CFD）中四个经典问题的基准测试：驱动腔流动、圆管中的层流边界层流动、通过台阶的坝流动和周期性的卡门涡街流动。每个流动问题都包括具有不同边界条件、流体物理特性和域几何的数据。与现有数据集相比，CFDBench具有以下优势：（1）综合。它包含常用的物理参数，如速度、压力和腔体比例。（2）真实。非常适合深度学习解决方案。

    In recent years, applying deep learning to solve physics problems has attracted much attention. Data-driven deep learning methods produce operators that can learn solutions to the whole system of partial differential equations. However, the existing methods are only evaluated on simple flow equations (e.g., Burger's equation), and only consider the generalization ability on different initial conditions. In this paper, we construct CFDBench, a benchmark with four classic problems in computational fluid dynamics (CFD): lid-driven cavity flow, laminar boundary layer flow in circular tubes, dam flows through the steps, and periodic Karman vortex street. Each flow problem includes data with different boundary conditions, fluid physical properties, and domain geometry. Compared to existing datasets, the advantages of CFDBench are (1) comprehensive. It contains common physical parameters such as velocity, pressure, and cavity fraction. (2) realistic. It is very suitable for deep learning solu
    
[^100]: 《利用深度学习提高R17 Type-II码本的性能》

    Improving the Performance of R17 Type-II Codebook with Deep Learning. (arXiv:2310.05962v1 [cs.IT])

    [http://arxiv.org/abs/2310.05962](http://arxiv.org/abs/2310.05962)

    本论文针对R17 Type-II码本存在的稀疏结构不足问题，提出了利用深度学习来改进码本的两个观点：利用深度学习准确选择主要的角度-时延域端口，并采用深度学习重建下行CSI，有效利用稀疏结构的信息。

    

    R17的Type-II码本利用上下行信道之间的角度-时延域部分互易性来选择测量和反馈下行信道状态信息（CSI）的部分角度-时延域端口，现有的深度学习增强的CSI反馈方法的性能受到稀疏结构的不足的限制。为了解决这个问题，我们提出了两个新的观点，采用深度学习来改进R17 Type-II码本。首先，考虑到上行信道的低信噪比，利用深度学习准确选择主要的角度-时延域端口，其中利用了焦点损失来解决类别不平衡问题。其次，我们提出采用深度学习来根据基站的R17 Type-II码本反馈重建下行CSI，可以有效利用稀疏结构的信息。此外，还设计了一个加权快捷模块。

    The Type-II codebook in Release 17 (R17) exploits the angular-delay-domain partial reciprocity between uplink and downlink channels to select part of angular-delay-domain ports for measuring and feeding back the downlink channel state information (CSI), where the performance of existing deep learning enhanced CSI feedback methods is limited due to the deficiency of sparse structures. To address this issue, we propose two new perspectives of adopting deep learning to improve the R17 Type-II codebook. Firstly, considering the low signal-to-noise ratio of uplink channels, deep learning is utilized to accurately select the dominant angular-delay-domain ports, where the focal loss is harnessed to solve the class imbalance problem. Secondly, we propose to adopt deep learning to reconstruct the downlink CSI based on the feedback of the R17 Type-II codebook at the base station, where the information of sparse structures can be effectively leveraged. Besides, a weighted shortcut module is desig
    
[^101]: 指纹攻击：联邦学习中的客户端去匿名化

    Fingerprint Attack: Client De-Anonymization in Federated Learning. (arXiv:2310.05960v1 [cs.CR])

    [http://arxiv.org/abs/2310.05960](http://arxiv.org/abs/2310.05960)

    本文研究了在联邦学习中，通过梯度指纹攻击可以轻松打破参与者匿名化，并展示了使用差分隐私进行训练可以提供实际防御。

    

    联邦学习允许在参与者不相信中央服务器和彼此的情况下进行协作训练，而无需共享数据。通过确保参与者与服务器之间的通信经过混洗，将参与者身份与其数据分离，还可以进一步提高隐私。本文旨在通过提出一种新颖的梯度指纹攻击来检验这种防御是否足以保证匿名性。我们在两个语言语料库上的联邦语言模型的实证研究中展示了梯度聚类可以轻松地打破匿名化。然后，我们展示了使用差分隐私进行训练可以提供对我们的指纹攻击的实际防御。

    Federated Learning allows collaborative training without data sharing in settings where participants do not trust the central server and one another. Privacy can be further improved by ensuring that communication between the participants and the server is anonymized through a shuffle; decoupling the participant identity from their data. This paper seeks to examine whether such a defense is adequate to guarantee anonymity, by proposing a novel fingerprinting attack over gradients sent by the participants to the server. We show that clustering of gradients can easily break the anonymization in an empirical study of learning federated language models on two language corpora. We then show that training with differential privacy can provide a practical defense against our fingerprint attack.
    
[^102]: 使用异构集成深度学习分类自动化全球滑坡检测

    Automating global landslide detection with heterogeneous ensemble deep-learning classification. (arXiv:2310.05959v1 [cs.CV])

    [http://arxiv.org/abs/2310.05959](http://arxiv.org/abs/2310.05959)

    本研究通过使用全球多样的滑坡数据集，采用不同的分割模型构建了一个集成深度学习模型，以解决滑坡映射中的敏感性问题、过拟合和低映射精度的困扰，并取得了最高的F1得分。

    

    随着气候条件的变化，我们已经看到极端天气事件及其次生后果，包括滑坡的增加。滑坡威胁基础设施，包括道路、铁路、建筑物和人类生命。基于危险的空间规划和预警系统是减少社会滑坡风险的经济有效策略。然而，这些都依赖于以前滑坡事件的数据，这通常很少见。最近，许多深度学习(DL)模型已经应用于使用中至高分辨率卫星图像作为输入进行滑坡映射。然而，它们经常遇到敏感性问题、过拟合和低映射精度的困扰。本研究通过使用多样的全球滑坡数据集，采用不同的分割模型，如Unet、Linknet、PSP-Net、PAN和DeepLab，并根据其性能构建一个集成模型，解决了其中一些限制。该集成模型在组合时取得了最高的F1得分(0.69)。

    With changing climatic conditions, we are already seeing an increase in extreme weather events and their secondary consequences, including landslides. Landslides threaten infrastructure, including roads, railways, buildings, and human life. Hazard-based spatial planning and early warning systems are cost-effective strategies to reduce the risk to society from landslides. However, these both rely on data from previous landslide events, which is often scarce. Many deep learning (DL) models have recently been applied for landside mapping using medium- to high-resolution satellite images as input. However, they often suffer from sensitivity problems, overfitting, and low mapping accuracy. This study addresses some of these limitations by using a diverse global landslide dataset, using different segmentation models, such as Unet, Linknet, PSP-Net, PAN, and DeepLab and based on their performances, building an ensemble model. The ensemble model achieved the highest F1-score (0.69) when combin
    
[^103]: 贝叶斯质量多样性方法用于混合连续、离散和分类变量的约束优化问题

    Bayesian Quality-Diversity approaches for constrained optimization problems with mixed continuous, discrete and categorical variables. (arXiv:2310.05955v1 [math.OC])

    [http://arxiv.org/abs/2310.05955](http://arxiv.org/abs/2310.05955)

    本论文提出了一种基于贝叶斯优化的新型质量多样性方法，用于解决混合连续、离散和分类变量的约束优化问题，旨在提供具有多样性性质的最优解集。

    

    复杂的工程设计问题通常涉及到使用费时的模拟代码来预测待设计系统的行为和性能。为了进行系统设计，这些代码经常嵌入到优化过程中来提供最佳设计方案，同时满足设计约束条件。最近，提出了一种新的质量多样性方法，旨在增强设计空间的探索能力，并提供一组具有多样性性质的最优解集，以评估权衡。此外，复杂的工程设计问题通常涉及到混合连续、离散和分类的设计变量，以考虑在优化问题中的技术选择。本文提出了一种基于混合连续、离散和分类贝叶斯优化的新型质量多样性方法。

    Complex engineering design problems, such as those involved in aerospace, civil, or energy engineering, require the use of numerically costly simulation codes in order to predict the behavior and performance of the system to be designed. To perform the design of the systems, these codes are often embedded into an optimization process to provide the best design while satisfying the design constraints. Recently, new approaches, called Quality-Diversity, have been proposed in order to enhance the exploration of the design space and to provide a set of optimal diversified solutions with respect to some feature functions. These functions are interesting to assess trade-offs. Furthermore, complex engineering design problems often involve mixed continuous, discrete, and categorical design variables allowing to take into account technological choices in the optimization problem. In this paper, a new Quality-Diversity methodology based on mixed continuous, discrete and categorical Bayesian opti
    
[^104]: Raman放大器的优化：黑盒、灰盒和白盒模型的比较

    Optimization of Raman amplifiers: a comparison between black-, grey- and white-box modeling. (arXiv:2310.05954v1 [physics.app-ph])

    [http://arxiv.org/abs/2310.05954](http://arxiv.org/abs/2310.05954)

    该论文比较了白盒、灰盒和黑盒模型在双向Raman放大器中实现目标频距放大的能力，并展示了它们都可以实现C波段下达到1dB的频距平坦度。

    

    在光通信系统不断努力提高吞吐量的过程中，设计和优化光放大器以最大化系统性能变得越来越重要。离线优化光放大器依赖于从深入物理学的白盒模型到数据驱动的与物理无关的黑盒模型的各种模型。在这里，我们比较了白盒、灰盒和黑盒模型在双向Raman放大器中实现目标频距放大的能力。我们展示了研究的任何一种方法都可以在100公里范围内实现C波段下达到1dB的频距平坦度。然后，我们根据目标应用场景讨论了模型的适用性、优势和缺点，特别是在优化速度和训练数据访问方面。

    Designing and optimizing optical amplifiers to maximize system performance is becoming increasingly important as optical communication systems strive to increase throughput. Offline optimization of optical amplifiers relies on models ranging from white-box models deeply rooted in physics to black-box data-driven physics-agnostic models. Here, we compare the capabilities of white-, grey- and black-box models to achieve a target frequency-distance amplification in a bidirectional Raman amplifier. We show that any of the studied methods can achieve down to 1 dB of frequency-distance flatness over the C-band in a 100-km span. Then, we discuss the models' applicability, advantages, and drawbacks based on the target application scenario, in particular in terms of optimization speed and access to training data.
    
[^105]: 使用机器学习方法进行垃圾URL的分类

    Classification of Spam URLs Using Machine Learning Approaches. (arXiv:2310.05953v1 [cs.CR])

    [http://arxiv.org/abs/2310.05953](http://arxiv.org/abs/2310.05953)

    该研究使用机器学习模型对URL进行垃圾和非垃圾的分类，发现bagging模型具有最高的准确率，为96.5%。

    

    由于提供了快速和免费的通信工具和平台，互联网每天被数十亿用户使用。然而，随着使用量的显著增加，每秒钟产生了大量的垃圾邮件，这不仅浪费了互联网资源，更重要的是浪费了用户的时间。本研究通过使用机器学习模型将URL分类为垃圾或非垃圾。我们首先从URL中提取特征，然后比较了几个模型的性能，包括k最近邻、bagging、随机森林、逻辑回归等。我们发现，bagging模型的准确率最高，达到了96.5%。这表明，bagging是一种有前景的用于将URL分类为垃圾或非垃圾的方法。

    The Internet is used by billions of users daily because it offers fast and free communication tools and platforms. Nevertheless, with this significant increase in usage, huge amounts of spam are generated every second, which wastes internet resources and, more importantly, users time. This study investigates using machine learning models to classify URLs as spam or non-spam. We first extract the features from the URL as it has only one feature, and then we compare the performance of several models, including k-nearest neighbors, bagging, random forest, logistic regression, and others. We find that bagging achieves the best accuracy, with an accuracy of 96.5%. This suggests that bagging is a promising approach for classifying URLs as spam or nonspam.
    
[^106]: 使用机器学习技术减轻雾计算无线传感器网络中的拒绝服务攻击

    Mitigating Denial of Service Attacks in Fog-Based Wireless Sensor Networks Using Machine Learning Techniques. (arXiv:2310.05952v1 [cs.CR])

    [http://arxiv.org/abs/2310.05952](http://arxiv.org/abs/2310.05952)

    本研究使用机器学习模型来识别无线传感器网络中的拒绝服务攻击。实验结果表明，XGBoost模型在整个数据集上具有更高的真正例率和更低的误报率。

    

    无线传感器网络由于其广泛的工业应用，被认为是21世纪最重要和创新的技术之一。由于其特殊的特性和部署方法，这些网络中的传感器节点容易受到各种攻击。在无线传感器网络中，拒绝服务攻击是常见的攻击方式。设计一个能够有效减少这些攻击对无线传感器网络影响的检测和预防系统是困难的。为了识别无线传感器网络中的攻击，本研究建议使用两种机器学习模型：决策树和XGBoost。对无线传感器网络数据集进行了大量的测试，以识别拒绝服务攻击。实验结果表明，将XGBoost模型应用于整个数据集时，真正例率（98.3%）高于决策树方法（97.3%），误报率（1.7%）低于决策树技术（2.7%）。

    Wireless sensor networks are considered to be among the most significant and innovative technologies in the 21st century due to their wide range of industrial applications. Sensor nodes in these networks are susceptible to a variety of assaults due to their special qualities and method of deployment. In WSNs, denial of service attacks are common attacks in sensor networks. It is difficult to design a detection and prevention system that would effectively reduce the impact of these attacks on WSNs. In order to identify assaults on WSNs, this study suggests using two machine learning models: decision trees and XGBoost. The WSNs dataset was the subject of extensive tests to identify denial of service attacks. The experimental findings demonstrate that the XGBoost model, when applied to the entire dataset, has a higher true positive rate (98.3%) than the Decision tree approach (97.3%) and a lower false positive rate (1.7%) than the Decision tree technique (2.7%). Like this, with selected d
    
[^107]: 鲁棒高效的干扰神经网络用于防御ImageNet中的对抗攻击

    Robust and Efficient Interference Neural Networks for Defending Against Adversarial Attacks in ImageNet. (arXiv:2310.05947v1 [cs.CV])

    [http://arxiv.org/abs/2310.05947](http://arxiv.org/abs/2310.05947)

    本文构建了一个干扰神经网络来防御ImageNet中的对抗攻击，通过应用额外背景图像和相应标签，并使用预训练的ResNet-152来高效完成训练。与PGD攻击下的最新技术结果相比，它具有更好的防御效果并需要更少的计算资源。

    

    对抗图像的存在严重影响了图像识别任务和深度学习的实际应用，也是深度学习急需解决的关键科学问题。到目前为止最有效的方法是用大量对抗样本训练神经网络。然而，当应用于ImageNet时，这种对抗训练方法需要大量的计算资源，并且在高强度对抗攻击下尚未取得令人满意的结果。本文通过应用额外背景图像和相应标签构建干扰神经网络，并使用预训练的ResNet-152来高效完成训练。与PGD攻击下的最新技术结果相比，它具有更好的防御效果并需要更少的计算资源。这项工作为防御对抗攻击提供了学术研究和实际应用的新思路。

    The existence of adversarial images has seriously affected the task of image recognition and practical application of deep learning, it is also a key scientific problem that deep learning urgently needs to solve. By far the most effective approach is to train the neural network with a large number of adversarial examples. However, this adversarial training method requires a huge amount of computing resources when applied to ImageNet, and has not yet achieved satisfactory results for high-intensity adversarial attacks. In this paper, we construct an interference neural network by applying additional background images and corresponding labels, and use pre-trained ResNet-152 to efficiently complete the training. Compared with the state-of-the-art results under the PGD attack, it has a better defense effect with much smaller computing resources. This work provides new ideas for academic research and practical applications of effective defense against adversarial attacks.
    
[^108]: 分析学习特征和土豆病害检测框架

    Analysis of Learned Features and Framework for Potato Disease Detection. (arXiv:2310.05943v1 [cs.CV])

    [http://arxiv.org/abs/2310.05943](http://arxiv.org/abs/2310.05943)

    该论文研究了土豆病害检测中的特征学习和框架，并通过使用区域卷积神经网络和注意力网络来解决数据集变化问题。实验结果表明这些方法在测试集上的平均分类准确率达到了95％，并且在训练阶段未见过的数据集上也有类似的表现，平均得分为84％。

    

    对于诸如植物病害检测之类的应用，通常会在公开可用的数据上对模型进行训练，并在田间数据上进行测试。这意味着测试数据分布与训练数据分布不同，从而对分类器的性能产生不利影响。我们通过确保特征从叶片上的病斑或健康区域中学习来处理这种数据集变化。其中一种解决方案是使用更快的基于区域的卷积神经网络（RCNN），另一种是使用基于注意力的网络。这些分类器在其对应训练数据集的测试集上的平均分类准确率约为95％。这些分类器还在训练阶段未见过的数据集上表现相当，平均得分为84％。

    For applications like plant disease detection, usually, a model is trained on publicly available data and tested on field data. This means that the test data distribution is not the same as the training data distribution, which affects the classifier performance adversely. We handle this dataset shift by ensuring that the features are learned from disease spots in the leaf or healthy regions, as applicable. This is achieved using a faster Region-based convolutional neural network (RCNN) as one of the solutions and an attention-based network as the other. The average classification accuracies of these classifiers are approximately 95% while evaluated on the test set corresponding to their training dataset. These classifiers also performed equivalently, with an average score of 84% on a dataset not seen during the training phase.
    
[^109]: 从零开始使用多智能体强化学习学习网络防御战术

    Learning Cyber Defence Tactics from Scratch with Multi-Agent Reinforcement Learning. (arXiv:2310.05939v1 [cs.CR])

    [http://arxiv.org/abs/2310.05939](http://arxiv.org/abs/2310.05939)

    学习网络防御战术的新方法，使用多智能体强化学习来进行自主网络防御，证明了合作多智能体强化学习能够对抗各种威胁。

    

    深度学习技术的最新进展为自主网络防御解决方案的设计开辟了新的可能性。在计算机网络防御角色中，智能代理团队可能展现出保护网络和动能资产的有希望的途径。在模拟的游戏环境中，代理根据其在主机防御场景中共同减轻攻击者活动的能力进行评估。守方系统会面对能破坏网络机密性、完整性和可用性的启发式攻击者。对比了基于价值的独立学习和集中培训分散执行的合作多智能体强化学习方法，发现这两种方法都胜过简单的多智能体启发式防御者。这项工作表明，合作多智能体强化学习能够学习有效的网络防御战术，以抵御各种威胁。

    Recent advancements in deep learning techniques have opened new possibilities for designing solutions for autonomous cyber defence. Teams of intelligent agents in computer network defence roles may reveal promising avenues to safeguard cyber and kinetic assets. In a simulated game environment, agents are evaluated on their ability to jointly mitigate attacker activity in host-based defence scenarios. Defender systems are evaluated against heuristic attackers with the goals of compromising network confidentiality, integrity, and availability. Value-based Independent Learning and Centralized Training Decentralized Execution (CTDE) cooperative Multi-Agent Reinforcement Learning (MARL) methods are compared revealing that both approaches outperform a simple multi-agent heuristic defender. This work demonstrates the ability of cooperative MARL to learn effective cyber defence tactics against varied threats.
    
[^110]: 漏洞聚类与语义漏洞嵌入的其他机器学习应用

    Vulnerability Clustering and other Machine Learning Applications of Semantic Vulnerability Embeddings. (arXiv:2310.05935v1 [cs.CR])

    [http://arxiv.org/abs/2310.05935](http://arxiv.org/abs/2310.05935)

    本研究使用自然语言处理技术探索了语义漏洞嵌入的不同类型，并将其用于聚类、分类和可视化等机器学习应用，为计算机安全研究人员和分析师在风险评估等方面提供支持。

    

    计算机安全漏洞通常以简短的自然语言描述的形式发布（例如MITRE的CVE列表），随着时间的推移，这些描述会进一步通过常见漏洞评分系统（CVSS）等标签进行手动补充。在漏洞AI（分析与情报）项目中，我们研究了基于自然语言处理（NLP）技术的不同类型的语义漏洞嵌入，以获得漏洞空间的简洁表示。我们还评估了它们作为机器学习应用的基础，以支持计算机安全研究人员和分析师在风险评估和其他相关活动中的应用。我们在本报告中简要总结了我们探索的特定应用，包括聚类、分类和可视化，以及一种新的基于逻辑的评估漏洞空间理论的方法。

    Cyber-security vulnerabilities are usually published in form of short natural language descriptions (e.g., in form of MITRE's CVE list) that over time are further manually enriched with labels such as those defined by the Common Vulnerability Scoring System (CVSS). In the Vulnerability AI (Analytics and Intelligence) project, we investigated different types of semantic vulnerability embeddings based on natural language processing (NLP) techniques to obtain a concise representation of the vulnerability space. We also evaluated their use as a foundation for machine learning applications that can support cyber-security researchers and analysts in risk assessment and other related activities. The particular applications we explored and briefly summarize in this report are clustering, classification, and visualization, as well as a new logic-based approach to evaluate theories about the vulnerability space.
    
[^111]: 符合决策理论: 通过不完美的预测产生安全的自主决策

    Conformal Decision Theory: Safe Autonomous Decisions from Imperfect Predictions. (arXiv:2310.05921v1 [stat.ML])

    [http://arxiv.org/abs/2310.05921](http://arxiv.org/abs/2310.05921)

    符合决策理论是一种框架，可以通过不完美的机器学习预测产生安全的自主决策。该理论的创新之处在于可以在没有对世界模型做出任何假设的情况下提供具有低风险的统计保证的决策。

    

    我们介绍了一种符合决策理论的框架，可以在机器学习预测不完美的情况下产生安全的自主决策。这种决策的例子是普遍存在的，从依赖于行人预测的机器人规划算法，到校准自动化制造以实现高吞吐量和低错误率，再到在运行时选择信任名义策略还是切换到安全备份策略。我们算法产生的决策在统计保证的情况下是安全的，无需对世界模型作出任何假设；观测数据可以不满足独立同分布(I.I.D.)的条件，甚至可能是对抗性的。该理论将符合预测的结果扩展到直接校准决策，而不需要构建预测集合。实验证明了我们方法在围绕人类进行机器人运动规划、自动股票交易和机器人制造方面的实用性。

    We introduce Conformal Decision Theory, a framework for producing safe autonomous decisions despite imperfect machine learning predictions. Examples of such decisions are ubiquitous, from robot planning algorithms that rely on pedestrian predictions, to calibrating autonomous manufacturing to exhibit high throughput and low error, to the choice of trusting a nominal policy versus switching to a safe backup policy at run-time. The decisions produced by our algorithms are safe in the sense that they come with provable statistical guarantees of having low risk without any assumptions on the world model whatsoever; the observations need not be I.I.D. and can even be adversarial. The theory extends results from conformal prediction to calibrate decisions directly, without requiring the construction of prediction sets. Experiments demonstrate the utility of our approach in robot motion planning around humans, automated stock trading, and robot manufacturin
    
[^112]: NEFTune: 噪声嵌入改进指令微调

    NEFTune: Noisy Embeddings Improve Instruction Finetuning. (arXiv:2310.05914v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05914](http://arxiv.org/abs/2310.05914)

    NEFTune 是一种改进语言模型微调效果的方法，通过在训练过程中给嵌入向量添加噪声，可以在多个指令数据集上显著提高准确率，并对不同类型的模型都有益处。

    

    我们展示了一个简单的增强方法可以显著提高语言模型微调的效果。NEFTune 在训练过程中给嵌入向量添加噪声。在使用 Alpaca 进行标准微调的基础上，LLaMA-2-7B 在 AlpacaEval 上的准确率从 29.79% 提升到了 64.69%。NEFTune 在现代指令数据集上也超过了强基线模型。使用 Evol-Instruct 训练的模型性能提升了10%，ShareGPT 提升了8%，OpenPlatypus 提升了8%。即使是经过 RLHF 进一步微调的强大模型，如 LLaMA-2-Chat，也可以通过 NEFTune 的附加训练获益。

    We show that language model finetuning can be improved, sometimes dramatically, with a simple augmentation. NEFTune adds noise to the embedding vectors during training. Standard finetuning of LLaMA-2-7B using Alpaca achieves 29.79% on AlpacaEval, which rises to 64.69% using noisy embeddings. NEFTune also improves over strong baselines on modern instruction datasets. Models trained with Evol-Instruct see a 10% improvement, with ShareGPT an 8% improvement, and with OpenPlatypus an 8% improvement. Even powerful models further refined with RLHF such as LLaMA-2-Chat benefit from additional training with NEFTune.
    
[^113]: M3FPolypSegNet：多频特征融合的结肠镜图像中息肉定位的分割网络

    M3FPolypSegNet: Segmentation Network with Multi-frequency Feature Fusion for Polyp Localization in Colonoscopy Images. (arXiv:2310.05538v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2310.05538](http://arxiv.org/abs/2310.05538)

    M3FPolypSegNet提出了一种新颖的基于频率特征融合的结肠镜图像中息肉定位的分割网络，通过将输入图像分解为不同的频率组件，使用多频编码器映射到高维特征空间，并应用可伸缩注意力强调息肉区域。

    

    息肉分割对于预防结直肠癌一种常见的癌症至关重要。深度学习已被用于自动分割息肉，从而减少误诊的风险。结肠镜图像中小的息肉定位具有复杂的特征，例如颜色、遮挡和不同形状的息肉。为了应对这一挑战，提出了一种新型基于频率的全卷积神经网络，称为多频特征融合息肉分割网络（M3FPolypSegNet），用于将输入图像分解为低频/高频/全频组件以利用每个组件的特征。我们使用三个独立的多频编码器将多个输入图像映射到高维特征空间。在频率-ASPP可伸缩注意模块（F-ASPP SAM）中，应用ASPP在每个频率组件之间以保留尺度信息。随后，应用可伸缩注意力强调高维特征中的息肉区域。

    Polyp segmentation is crucial for preventing colorectal cancer a common type of cancer. Deep learning has been used to segment polyps automatically, which reduces the risk of misdiagnosis. Localizing small polyps in colonoscopy images is challenging because of its complex characteristics, such as color, occlusion, and various shapes of polyps. To address this challenge, a novel frequency-based fully convolutional neural network, Multi-Frequency Feature Fusion Polyp Segmentation Network (M3FPolypSegNet) was proposed to decompose the input image into low/high/full-frequency components to use the characteristics of each component. We used three independent multi-frequency encoders to map multiple input images into a high-dimensional feature space. In the Frequency-ASPP Scalable Attention Module (F-ASPP SAM), ASPP was applied between each frequency component to preserve scale information. Subsequently, scalable attention was applied to emphasize polyp regions in a high-dimensional feature 
    
[^114]: ParFam - 基于连续全局优化的符号回归

    ParFam -- Symbolic Regression Based on Continuous Global Optimization. (arXiv:2310.05537v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.05537](http://arxiv.org/abs/2310.05537)

    ParFam是一种新的符号回归方法，利用参数化的符号函数族将离散问题转化为连续问题，并结合全局优化器，能够有效解决符号回归问题。

    

    符号回归（SR）问题在许多不同的应用中出现，比如从给定数据中识别物理定律或推导描述金融市场行为的数学方程。目前存在多种解决SR问题的方法，通常基于遗传编程。然而，这些方法通常非常复杂，需要大量超参数调整和计算资源。本文介绍了我们提出的新方法ParFam，它利用适合的符号函数的参数化族将离散的符号回归问题转化为连续问题，相比当前最先进的方法，这种方法的设置更加直观。结合强大的全局优化器，这种方法可以有效地解决SR问题。此外，它可以轻松扩展到更高级的算法，例如添加深度神经网络以找到适合的参数化族。我们证明了这种方法的性能。

    The problem of symbolic regression (SR) arises in many different applications, such as identifying physical laws or deriving mathematical equations describing the behavior of financial markets from given data. Various methods exist to address the problem of SR, often based on genetic programming. However, these methods are usually quite complicated and require a lot of hyperparameter tuning and computational resources. In this paper, we present our new method ParFam that utilizes parametric families of suitable symbolic functions to translate the discrete symbolic regression problem into a continuous one, resulting in a more straightforward setup compared to current state-of-the-art methods. In combination with a powerful global optimizer, this approach results in an effective method to tackle the problem of SR. Furthermore, it can be easily extended to more advanced algorithms, e.g., by adding a deep neural network to find good-fitting parametric families. We prove the performance of 
    
[^115]: RetSeg: 基于保留机制的结直肠息肉分割网络

    RetSeg: Retention-based Colorectal Polyps Segmentation Network. (arXiv:2310.05446v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2310.05446](http://arxiv.org/abs/2310.05446)

    本研究探索将保留机制整合到结直肠息肉分割中，解决了视觉变换器在资源受限设备上实时疾病检测中的内存和并行性挑战。

    

    视觉变换器（ViTs）在医学图像分析方面取得了重大突破，与传统的卷积神经网络（CNNs）相比，在息肉分类、检测和分割等关键任务中展现出了更高的效能。通过利用注意机制聚焦于特定图像区域，ViTs在处理视觉数据时表现出上下文感知能力，从而在处理复杂医学图像时实现了强大且精确的预测。此外，变换器中固有的自注意机制适应了不同的输入尺寸和分辨率，为传统的CNNs所不具备的提供了前所未有的灵活性。然而，变换器由于自注意机制而面临着过多的内存使用和有限的训练并行性等挑战，从而使其在资源受限设备上实时疾病检测变得不切实际。在本研究中，我们通过探究将最近引入的保留机制整合到息肉分割中，来解决这些难题。

    Vision Transformers (ViTs) have revolutionized medical imaging analysis, showcasing superior efficacy compared to conventional Convolutional Neural Networks (CNNs) in vital tasks such as polyp classification, detection, and segmentation. Leveraging attention mechanisms to focus on specific image regions, ViTs exhibit contextual awareness in processing visual data, culminating in robust and precise predictions, even for intricate medical images. Moreover, the inherent self-attention mechanism in Transformers accommodates varying input sizes and resolutions, granting an unprecedented flexibility absent in traditional CNNs. However, Transformers grapple with challenges like excessive memory usage and limited training parallelism due to self-attention, rendering them impractical for real-time disease detection on resource-constrained devices. In this study, we address these hurdles by investigating the integration of the recently introduced retention mechanism into polyp segmentation, intr
    
[^116]: 通过基于Transformer的强化学习进行分子的全新设计

    Molecular De Novo Design through Transformer-based Reinforcement Learning. (arXiv:2310.05365v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.05365](http://arxiv.org/abs/2310.05365)

    本文提出了一种基于Transformer的强化学习方法，通过精细调整生成模型，能够在分子的全新设计中生成具有所需性质的分子结构，展现出优越的性能。

    

    本文介绍了一种通过精细调整基于Transformer的生成模型用于分子的全新设计的方法。利用Transformer相对于循环神经网络（RNN）的优越序列学习能力，我们的模型可以有效地生成具有所需性质的分子结构。与传统的基于RNN的模型相比，我们提出的方法在生成预测对多种生物靶点具有活性的化合物方面表现出卓越性能，捕捉了分子结构序列的长期依赖性。该模型的有效性在许多任务中得到了证明，包括生成与查询结构类似的分子和生成具有特定属性的化合物，在性能上优于基线的基于RNN的方法。我们的方法可以用于桥接化学、从单个分子开始扩展库，并生成具有高预测活性的化合物。

    In this work, we introduce a method to fine-tune a Transformer-based generative model for molecular de novo design. Leveraging the superior sequence learning capacity of Transformers over Recurrent Neural Networks (RNNs), our model can generate molecular structures with desired properties effectively. In contrast to the traditional RNN-based models, our proposed method exhibits superior performance in generating compounds predicted to be active against various biological targets, capturing long-term dependencies in the molecular structure sequence. The model's efficacy is demonstrated across numerous tasks, including generating analogues to a query structure and producing compounds with particular attributes, outperforming the baseline RNN-based methods. Our approach can be used for scaffold hopping, library expansion starting from a single molecule, and generating compounds with high predicted activity against biological targets.
    
[^117]: 大型语言模型时代中的事实性挑战

    Factuality Challenges in the Era of Large Language Models. (arXiv:2310.05189v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05189](http://arxiv.org/abs/2310.05189)

    大型语言模型存在生成虚假、错误或误导内容的问题，同时也面临被恶意应用的风险。本研究探讨了需要从事实核查员、新闻机构和研究与政策界采取的技术创新、监管改革和人工智能素养倡议，以解决这些问题。

    

    基于大型语言模型（LLMs）的工具的出现，如OpenAI的ChatGPT，微软的Bing Chat和谷歌的Bard，已经引起了巨大的公众关注。这些非常有用、自然的工具标志着自然语言生成方面的重大进展，然而它们存在生成虚假、错误或误导内容的倾向，通常被称为“幻觉”。此外，LLMs可以被用于恶意应用，例如在规模上生成虚假但可信的内容和个人资料。这对于社会来说构成了重大挑战，因为它可能欺骗用户并越来越多地传播不准确的信息。鉴于这些风险，我们探讨了事实核查员、新闻机构和更广泛的研究和政策界需要的技术创新、监管改革和人工智能素养倡议的类型。通过确定风险、迫在眉睫的威胁和一些可行的解决方案，我们希望解决这个问题。

    The emergence of tools based on Large Language Models (LLMs), such as OpenAI's ChatGPT, Microsoft's Bing Chat, and Google's Bard, has garnered immense public attention. These incredibly useful, natural-sounding tools mark significant advances in natural language generation, yet they exhibit a propensity to generate false, erroneous, or misleading content -- commonly referred to as "hallucinations." Moreover, LLMs can be exploited for malicious applications, such as generating false but credible-sounding content and profiles at scale. This poses a significant challenge to society in terms of the potential deception of users and the increasing dissemination of inaccurate information. In light of these risks, we explore the kinds of technological innovations, regulatory reforms, and AI literacy initiatives needed from fact-checkers, news organizations, and the broader research and policy communities. By identifying the risks, the imminent threats, and some viable solutions, we seek to she
    
[^118]: 在CloudOps领域的时间序列预测中推动预训练的极限

    Pushing the Limits of Pre-training for Time Series Forecasting in the CloudOps Domain. (arXiv:2310.05063v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.05063](http://arxiv.org/abs/2310.05063)

    本论文在CloudOps领域引入了三个大规模时间序列预测数据集，其中最大的数据集拥有数十亿个观测点，为研究时间序列模型的预训练和扩展性奠定了实证基础，并确定了一个有前景的候选架构作为强大的零-shot基线。

    

    在预训练和迁移学习的时代，时间序列被放在了后面。尽管自然语言处理和计算机视觉领域的研究正在享受着越来越大的数据集来训练庞大的模型，但最受欢迎的时间序列数据集只包含数万个时间步，限制了我们对预训练和扩展性的研究效果。最近的研究也对表达力模型和规模的必要性产生了怀疑。为了缓解这些问题，我们引入了来自云操作（CloudOps）领域的三个大规模时间序列预测数据集，其中最大的数据集拥有数十亿个观测点，进一步研究时间序列模型的预训练和扩展性。我们建立了研究时间序列模型的预训练和扩展性的经验基础，并通过确定一个有前景的候选架构为未来研究铺平了道路。我们展示了它是一个强大的零-shot基线，并从进一步的扩展中获益。

    Time series has been left behind in the era of pre-training and transfer learning. While research in the fields of natural language processing and computer vision are enjoying progressively larger datasets to train massive models, the most popular time series datasets consist of only tens of thousands of time steps, limiting our ability to study the effectiveness of pre-training and scaling. Recent studies have also cast doubt on the need for expressive models and scale. To alleviate these issues, we introduce three large-scale time series forecasting datasets from the cloud operations (CloudOps) domain, the largest having billions of observations, enabling further study into pre-training and scaling of time series models. We build the empirical groundwork for studying pre-training and scaling of time series models and pave the way for future research by identifying a promising candidate architecture. We show that it is a strong zero-shot baseline and benefits from further scaling, bot
    
[^119]: Diff-Transfer: 通过可微分物理仿真进行基于模型的机器人操作技能迁移

    Diff-Transfer: Model-based Robotic Manipulation Skill Transfer via Differentiable Physics Simulation. (arXiv:2310.04930v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2310.04930](http://arxiv.org/abs/2310.04930)

    这篇论文提出了Diff-Transfer，一种通过可微分物理仿真来高效传输机器人技能的新框架。Diff-Transfer通过在任务空间内发现可行路径，将源任务转化为目标任务，并通过梯度信息引导适应已知的动作，成功解决另一个子任务。实验结果表明了Diff-Transfer的有效性。

    

    将掌握的技能转移到完成一系列相似但新颖任务的能力对于智能机器人至关重要。本研究引入了Diff-Transfer，一种利用可微分物理仿真来高效传输机器人技能的新框架。具体而言，Diff-Transfer在任务空间内发现了一条可行的路径，将源任务带到目标任务。在这个任务路径的每对相邻点上，即两个子任务中，Diff-Transfer通过从一个子任务中适应已知的动作来成功解决另一个子任务。适应过程是由可微分物理仿真产生的梯度信息引导的。我们提出了一种新颖的路径规划方法，利用具有任务级状态和奖励的Q学习来生成子任务。我们在仿真实验中实现了我们的框架，并在机器人操作上执行了四个具有挑战性的迁移任务，展示了Diff-Transfer的有效性。

    The capability to transfer mastered skills to accomplish a range of similar yet novel tasks is crucial for intelligent robots. In this work, we introduce $\textit{Diff-Transfer}$, a novel framework leveraging differentiable physics simulation to efficiently transfer robotic skills. Specifically, $\textit{Diff-Transfer}$ discovers a feasible path within the task space that brings the source task to the target task. At each pair of adjacent points along this task path, which is two sub-tasks, $\textit{Diff-Transfer}$ adapts known actions from one sub-task to tackle the other sub-task successfully. The adaptation is guided by the gradient information from differentiable physics simulations. We propose a novel path-planning method to generate sub-tasks, leveraging $Q$-learning with a task-level state and reward. We implement our framework in simulation experiments and execute four challenging transfer tasks on robotic manipulation, demonstrating the efficacy of $\textit{Diff-Transfer}$ thr
    
[^120]: 重复委托选择的后悔分析

    Regret Analysis of Repeated Delegated Choice. (arXiv:2310.04884v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2310.04884](http://arxiv.org/abs/2310.04884)

    本研究针对重复委托选择问题展开研究，通过动态宣布合格集合以学习解决方案分布来减轻代理人的自私行为，进而最小化与最优合格集合之间的后悔。

    

    我们对重复委托选择问题进行了研究，这是第一个考虑克莱因伯格和克莱因伯格(EC'18)在线学习变种的模型。在该模型中，一个委托人与一个拥有外生解集的代理人进行重复互动，以寻找高效的解决方案。每个解决方案对委托人和代理人都可以产生不同的效用，代理人可能以自私的方式提出解决方案，以最大化自身的效用。为了减轻这种行为，委托人宣布一个合格集合，筛掉一定的解决方案。然而，委托人事先并不了解解决方案的分布情况。因此，委托人动态地宣布各种合格集合，以有效地学习分布。委托人的目标是最小化与事后最优合格集合相比的累积后悔。我们探讨了问题设置的两个维度，即代理人是根据眼前利益行事还是在回合之间制定策略。

    We present a study on a repeated delegated choice problem, which is the first to consider an online learning variant of Kleinberg and Kleinberg, EC'18. In this model, a principal interacts repeatedly with an agent who possesses an exogenous set of solutions to search for efficient ones. Each solution can yield varying utility for both the principal and the agent, and the agent may propose a solution to maximize its own utility in a selfish manner. To mitigate this behavior, the principal announces an eligible set which screens out a certain set of solutions. The principal, however, does not have any information on the distribution of solutions in advance. Therefore, the principal dynamically announces various eligible sets to efficiently learn the distribution. The principal's objective is to minimize cumulative regret compared to the optimal eligible set in hindsight. We explore two dimensions of the problem setup, whether the agent behaves myopically or strategizes across the rounds,
    
[^121]: Lemur：在自动程序验证中集成大型语言模型

    Lemur: Integrating Large Language Models in Automated Program Verification. (arXiv:2310.04870v2 [cs.FL] UPDATED)

    [http://arxiv.org/abs/2310.04870](http://arxiv.org/abs/2310.04870)

    本论文提出了一种将LLMs和自动推理器结合起来进行自动程序验证的通用方法，并证明了其完备性。这个方法在一些合成和竞争基准上取得了实际的改进。

    

    LLMs在代码理解能力上的展示引发了一个问题：它们是否可以用于自动程序验证，这是一个通常需要高级抽象推理的任务，对于验证工具来说是具有挑战性的。我们提出了一种将LLMs的能力和自动推理器结合起来进行自动程序验证的通用方法。我们正式描述了这种方法论，将其作为推导规则的集合进行论证其完备性。我们将计算机推理形成为一个完备的自动验证过程，这在一组合成和竞争基准上带来了实际的改进。

    The demonstrated code-understanding capability of LLMs raises the question of whether they can be used for automated program verification, a task that often demands high-level abstract reasoning about program properties, which is challenging for verification tools. We propose a general methodology to combine the power of LLMs and automated reasoners for automated program verification. We formally describe this methodology as a set of derivation rules and prove its soundness. We instantiate the calculus as a sound automated verification procedure, which led to practical improvements on a set of synthetic and competition benchmarks.
    
[^122]: 通用图随机特征

    Universal Graph Random Features. (arXiv:2310.04859v1 [stat.ML])

    [http://arxiv.org/abs/2310.04859](http://arxiv.org/abs/2310.04859)

    本文提出了一种新的准蒙特卡罗机制，称为排斥随机游走，通过改进图的采样，提高了统计估计器的集中度。该机制在估计图内核、PageRank向量和图形浓度等方面展示了有效性。

    

    我们提出了一种新颖的准蒙特卡罗机制，称为排斥随机游走，以改进基于图的采样。通过在相互作用集合的轨迹之间引入相关性，使它们的边际转移概率保持不变，我们能够更高效地探索图形，提高统计估计器的集中度，同时保持它们的无偏性。该机制可以轻松地实现。我们展示了在估计图内核、PageRank向量和图形浓度等各种情况下，排斥随机游走的有效性。我们提供了详细的实验评估和鲁棒的理论保证。据我们所知，排斥随机游走是第一个在图上相关步行者方向进行严格研究的准蒙特卡罗方案，为这个令人兴奋的新兴领域带来了新的研究。

    We present a novel quasi-Monte Carlo mechanism to improve graph-based sampling, coined repelling random walks. By inducing correlations between the trajectories of an interacting ensemble such that their marginal transition probabilities are unmodified, we are able to explore the graph more efficiently, improving the concentration of statistical estimators whilst leaving them unbiased. The mechanism has a trivial drop-in implementation. We showcase the effectiveness of repelling random walks in a range of settings including estimation of graph kernels, the PageRank vector and graphlet concentrations. We provide detailed experimental evaluation and robust theoretical guarantees. To our knowledge, repelling random walks constitute the first rigorously studied quasi-Monte Carlo scheme correlating the directions of walkers on a graph, inviting new research in this exciting nascent domain.
    
[^123]: 从Shapley Value的角度重新思考Integrated Gradients的基线选择

    Rethink Baseline of Integrated Gradients from the Perspective of Shapley Value. (arXiv:2310.04821v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.04821](http://arxiv.org/abs/2310.04821)

    该论文从Shapley Value的角度重新思考了Integrated Gradients的基线选择，并提出了一种新的基线构建方法叫做Shapley Integrated Gradients (SIG)。在GridWorl上的模拟实验表明，SIG能够生成有意义和无偏的解释模型预测。

    

    许多方法已经尝试通过将深度神经网络（DNNs）的预测归因于其输入特征来解释DNN。其中一个研究充分的归因方法是Integrated Gradients（IG）。具体而言，选择IG的基线是在不同情景下生成有意义和无偏解释模型预测的关键考虑因素。然而，目前利用单一基线的做法未能实现这个愿望，因此需要多个基线。幸运的是，IG与奥曼—夏普利（Aumann-Shapley）价值之间的内在联系形成了一种独特的视角，重新思考了基线的设计。在某些假设下，我们在理论上分析出一组基线与Shapley Value中的联盟相对应。因此，我们提出了一种新的基线构建方法，称为Shapley Integrated Gradients（SIG），通过比例抽样来搜索一组基线，以部分模拟Shapley Value的计算路径。在GridWorl上进行了模拟实验。

    Numerous approaches have attempted to interpret deep neural networks (DNNs) by attributing the prediction of DNN to its input features. One of the well-studied attribution methods is Integrated Gradients (IG). Specifically, the choice of baselines for IG is a critical consideration for generating meaningful and unbiased explanations for model predictions in different scenarios. However, current practice of exploiting a single baseline fails to fulfill this ambition, thus demanding multiple baselines. Fortunately, the inherent connection between IG and Aumann-Shapley Value forms a unique perspective to rethink the design of baselines. Under certain hypothesis, we theoretically analyse that a set of baseline aligns with the coalitions in Shapley Value. Thus, we propose a novel baseline construction method called Shapley Integrated Gradients (SIG) that searches for a set of baselines by proportional sampling to partly simulate the computation path of Shapley Value. Simulations on GridWorl
    
[^124]: 在线损坏用户检测和后悔最小化

    Online Corrupted User Detection and Regret Minimization. (arXiv:2310.04768v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.04768](http://arxiv.org/abs/2310.04768)

    本文介绍了一种名为LOCUD的在线学习问题，旨在从被损坏的用户行为中学习和利用未知的用户关系以加速学习，并在在线环境中识别损坏用户。

    

    在真实世界的在线网络系统中，多个用户通常按顺序进入系统。对于点击欺诈和虚假评论等应用，某些用户可能恶意执行损坏行为以欺骗系统。因此，设计高效的在线学习算法来从潜在的损坏用户行为中稳健地学习，并在在线方式下准确识别损坏用户是至关重要的。现有的工作提出了对抗性损坏鲁棒的强盗算法。然而，这些算法是为单个用户设计的，不能利用多个用户之间的隐式社会关系进行更高效的学习。此外，它们中没有考虑在多用户情景下如何在线检测损坏用户。在本文中，我们提出了一个重要的在线学习问题，即LOCUD，从被打断的行为中学习并利用未知的用户关系以加快学习，并在在线环境中识别损坏用户。

    In real-world online web systems, multiple users usually arrive sequentially into the system. For applications like click fraud and fake reviews, some users can maliciously perform corrupted (disrupted) behaviors to trick the system. Therefore, it is crucial to design efficient online learning algorithms to robustly learn from potentially corrupted user behaviors and accurately identify the corrupted users in an online manner. Existing works propose bandit algorithms robust to adversarial corruption. However, these algorithms are designed for a single user, and cannot leverage the implicit social relations among multiple users for more efficient learning. Moreover, none of them consider how to detect corrupted users online in the multiple-user scenario. In this paper, we present an important online learning problem named LOCUD to learn and utilize unknown user relations from disrupted behaviors to speed up learning, and identify the corrupted users in an online setting. To robustly lea
    
[^125]: DiffNAS: 通过引导更好的结构来启动扩散模型

    DiffNAS: Bootstrapping Diffusion Models by Prompting for Better Architectures. (arXiv:2310.04750v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.04750](http://arxiv.org/abs/2310.04750)

    本文提出了一种名为DiffNAS的基础模型搜索方法，通过引导更好的结构来启动扩散模型，以提高合成性能。通过利用GPT-4作为超网，辅以搜索内存和RFID代理，以及快速收敛训练策略，搜索效率提高了2倍，达到了2.82的性能提升0.37。

    

    最近，扩散模型在合成数据上表现出了显著的性能。在选择扩散路径之后，像UNet这样的基础模型作为去噪自编码器，主要预测需要逐步消除的噪声。因此，采用与预期预算相一致的模型以促进优良的合成性能至关重要。在本文中，我们精心分析了扩散模型，并设计了一种名为"DiffNAS"的基础模型搜索方法。具体而言，我们利用GPT-4作为超网来加速搜索，辅以搜索内存以增强结果。此外，我们采用RFID作为代理，快速对GPT-4产生的实验结果进行排序。我们还采用了快速收敛训练策略来提高搜索效率。严格的实验验证了我们的算法在基于GPT的情境下可以将搜索效率提高2倍，同时取得了2.82的性能，改善了0.37。

    Diffusion models have recently exhibited remarkable performance on synthetic data. After a diffusion path is selected, a base model, such as UNet, operates as a denoising autoencoder, primarily predicting noises that need to be eliminated step by step. Consequently, it is crucial to employ a model that aligns with the expected budgets to facilitate superior synthetic performance. In this paper, we meticulously analyze the diffusion model and engineer a base model search approach, denoted "DiffNAS". Specifically, we leverage GPT-4 as a supernet to expedite the search, supplemented with a search memory to enhance the results. Moreover, we employ RFID as a proxy to promptly rank the experimental outcomes produced by GPT-4. We also adopt a rapid-convergence training strategy to boost search efficiency. Rigorous experimentation corroborates that our algorithm can augment the search efficiency by 2 times under GPT-based scenarios, while also attaining a performance of 2.82 with 0.37 improvem
    
[^126]: 参数高效的多任务模型融合与部分线性化

    Parameter Efficient Multi-task Model Fusion with Partial Linearization. (arXiv:2310.04742v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.04742](http://arxiv.org/abs/2310.04742)

    本文提出了一种参数高效的多任务模型融合方法，通过部分线性化适配器模块，并应用任务算法，实现了对大型预训练模型在多个下游任务上的高效微调，从而提高了多任务模型融合的效果和效率。

    

    大型预训练模型在机器学习中取得了重大进展，并成为基础组件。模型融合方法，如任务算法，已被证明具有强大的可扩展性，可以将来自不同任务的微调权重合并到一个多任务模型中。然而，对于在多个下游任务上高效微调大型预训练模型仍然具有挑战性，导致多任务模型融合效率低下。在这项工作中，我们提出了一种改进多任务融合的新方法，适用于像LoRA微调这样的参数高效微调技术。具体而言，我们的方法仅部分线性化适配器模块，并在线性化的适配器上应用任务算法。这样一来，我们可以利用模型融合优势来进行线性化微调，同时保持微调和推理的效率。我们证明，我们的部分线性化技术使多个任务更有效地融合到单个模型中，表现更好。

    Large pre-trained models have enabled significant advances in machine learning and served as foundation components. Model fusion methods, such as task arithmetic, have been proven to be powerful and scalable to incorporate fine-tuned weights from different tasks into a multi-task model. However, efficiently fine-tuning large pre-trained models on multiple downstream tasks remains challenging, leading to inefficient multi-task model fusion. In this work, we propose a novel method to improve multi-task fusion for parameter-efficient fine-tuning techniques like LoRA fine-tuning. Specifically, our approach partially linearizes only the adapter modules and applies task arithmetic over the linearized adapters. This allows us to leverage the the advantages of model fusion over linearized fine-tuning, while still performing fine-tuning and inference efficiently. We demonstrate that our partial linearization technique enables a more effective fusion of multiple tasks into a single model, outper
    
[^127]: 在持续学习中平衡稳定性和可塑性：激活变化的读出分解（RDAC）框架。

    Balancing stability and plasticity in continual learning: the readout-decomposition of activation change (RDAC) framework. (arXiv:2310.04741v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.04741](http://arxiv.org/abs/2310.04741)

    本文介绍了一个名为RDAC的框架，该框架解剖了持续学习中稳定性和可塑性之间的平衡，并详细分析了几种常用算法在处理任务时的稳定性和可塑性权衡。

    

    持续学习算法旨在获取新知识的同时保留先前的信息。然而，稳定性和可塑性之间的平衡仍然是一个中心挑战。本文介绍了一个框架，对这种平衡进行了解剖，提供了关于持续学习算法的宝贵见解。激活变化的读出分解（RDAC）框架首先解决了稳定性和可塑性困境及其与灾难性遗忘的关系。它将学习诱导的激活变化与先前读出范围内的稳定性程度和零空间的变化与可塑性程度相关联。在处理分裂CIFAR-110任务的深度非线性网络中，该框架阐明了常用正则化算法（SI、EWC和LwF）以及重放算法（GEM和数据重放）的稳定性和可塑性的权衡。

    Continual learning (CL) algorithms strive to acquire new knowledge while preserving prior information. However, this stability-plasticity trade-off remains a central challenge. This paper introduces a framework that dissects this trade-off, offering valuable insights into CL algorithms. The Readout-Decomposition of Activation Change (RDAC) framework first addresses the stability-plasticity dilemma and its relation to catastrophic forgetting. It relates learning-induced activation changes in the range of prior readouts to the degree of stability and changes in the null space to the degree of plasticity. In deep non-linear networks tackling split-CIFAR-110 tasks, the framework clarifies the stability-plasticity trade-offs of the popular regularization algorithms Synaptic intelligence (SI), Elastic-weight consolidation (EWC), and learning without Forgetting (LwF), and replay-based algorithms Gradient episodic memory (GEM), and data replay. GEM and data replay preserved stability and plast
    
[^128]: 离线模仿学习与变分逆向推理

    Offline Imitation Learning with Variational Counterfactual Reasoning. (arXiv:2310.04706v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.04706](http://arxiv.org/abs/2310.04706)

    该论文提出了一个名为OILCA的框架，利用可识别的变分自动编码器生成"对抗性"样本，以解决离线模仿学习中数据稀缺、环境变化等问题。

    

    在离线模仿学习中，智能体旨在学习一种最优的专家行为策略，而不需要额外的在线环境交互。然而，在许多真实场景中，例如机器人操作中，离线数据集是从没有奖励的次优行为中收集来的。由于专家数据稀缺，智能体通常只能简单地记住贫乏的轨迹，并且容易受到环境变化的影响，缺乏对新环境的泛化能力。为了有效地消除会对智能体造成偏差并阻碍泛化的伪特征，我们提出了一个名为OILCA的框架，即离线模仿学习与对抗数据增强。具体来说，我们利用可识别的变分自动编码器生成"对抗性"样本。我们从理论上分析了对抗性识别和泛化的改善。

    In offline Imitation Learning (IL), an agent aims to learn an optimal expert behavior policy without additional online environment interactions. However, in many real-world scenarios, such as robotics manipulation, the offline dataset is collected from suboptimal behaviors without rewards. Due to the scarce expert data, the agents usually suffer from simply memorizing poor trajectories and are vulnerable to the variations in the environments, lacking the capability of generalizing to new environments. To effectively remove spurious features that would otherwise bias the agent and hinder generalization, we propose a framework named \underline{O}ffline \underline{I}mitation \underline{L}earning with \underline{C}ounterfactual data \underline{A}ugmentation (OILCA). In particular, we leverage the identifiable variational autoencoder to generate \textit{counterfactual} samples. We theoretically analyze the counterfactual identification and the improvement of generalization. Moreover, we con
    
[^129]: 增强鲁棒性的带对抗特征抑制的提升建模

    Robustness-enhanced Uplift Modeling with Adversarial Feature Desensitization. (arXiv:2310.04693v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.04693](http://arxiv.org/abs/2310.04693)

    本文提出了一种增强鲁棒性的提升建模框架RUAD，并通过特征选择和对抗特征抑制两个定制模块更有效地解决了提升模型的特征敏感性问题。

    

    提升建模在在线营销中展示了非常有希望的结果。然而，大多数现有的工作在一些实际应用中容易受到鲁棒性挑战的影响。本文首先对上述现象给出了一个可能的解释。我们使用不同的真实世界数据集验证了在线营销中存在特征敏感性问题，一些关键特征的扰动会严重影响提升模型的性能，甚至导致相反的趋势。为了解决上述问题，我们提出了一种新颖的通过对抗特征抑制增强鲁棒性的提升建模框架（RUAD）。具体来说，我们的RUAD通过两个定制模块更有效地减轻提升模型的特征敏感性，包括一个具有联合多标签建模的特征选择模块，以从输入特征中识别一个关键子集，以及一个采用对抗训练和软插值操作的对抗特征抑制模块。

    Uplift modeling has shown very promising results in online marketing. However, most existing works are prone to the robustness challenge in some practical applications. In this paper, we first present a possible explanation for the above phenomenon. We verify that there is a feature sensitivity problem in online marketing using different real-world datasets, where the perturbation of some key features will seriously affect the performance of the uplift model and even cause the opposite trend. To solve the above problem, we propose a novel robustness-enhanced uplift modeling framework with adversarial feature desensitization (RUAD). Specifically, our RUAD can more effectively alleviate the feature sensitivity of the uplift model through two customized modules, including a feature selection module with joint multi-label modeling to identify a key subset from the input features and an adversarial feature desensitization module using adversarial training and soft interpolation operations t
    
[^130]: LauraGPT：使用GPT进行听、关注、理解和再生音频的研究

    LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT. (arXiv:2310.04673v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2310.04673](http://arxiv.org/abs/2310.04673)

    LauraGPT是一个统一的GPT模型，用于音频识别、理解和生成，具有广泛的应用范围，包括自动语音识别、语音翻译、文本到语音合成、机器翻译等任务。

    

    生成式预训练变换器（GPT）模型在各种自然语言处理任务中取得了显著的性能。然而，将类似的框架应用于音频任务的研究有限。以前提出的用于音频任务的大型语言模型要么缺乏充分的定量评估，要么局限于识别和理解音频内容的任务，要么明显不及现有的最先进模型（SOTA）。本文中，我们提出了LauraGPT，一个用于音频识别、理解和生成的统一GPT模型。LauraGPT是一个通用的语言模型，可以处理音频和文本输入，并在任意模式下生成输出。它可以进行与内容、语义、语音学和音频信号分析相关的各种任务。其中一些值得注意的任务包括自动语音识别、语音到文本翻译、文本到语音合成、机器翻译、语音增强、自动音频捕获等。

    Generative Pre-trained Transformer (GPT) models have achieved remarkable performance on various natural language processing tasks. However, there has been limited research on applying similar frameworks to audio tasks. Previously proposed large language models for audio tasks either lack sufficient quantitative evaluations, or are limited to tasks for recognizing and understanding audio content, or significantly underperform existing state-of-the-art (SOTA) models. In this paper, we propose LauraGPT, a unified GPT model for audio recognition, understanding, and generation. LauraGPT is a versatile language model that can process both audio and text inputs and generate outputs in either modalities. It can perform a wide range of tasks related to content, semantics, paralinguistics, and audio-signal analysis. Some of its noteworthy tasks include automatic speech recognition, speech-to-text translation, text-to-speech synthesis, machine translation, speech enhancement, automated audio capt
    
[^131]: 基于Transformer的神经网络替代方法用于变尺寸地图中的链接级路径损耗预测

    Transformer-Based Neural Surrogate for Link-Level Path Loss Prediction from Variable-Sized Maps. (arXiv:2310.04570v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.04570](http://arxiv.org/abs/2310.04570)

    本研究提出了一种基于Transformer的神经网络架构，可以从变尺寸地图和稀疏测量中预测链接级属性，并且在泛化性能方面表现良好。

    

    估计发射机-接收机位置的路径损耗对于网络规划和切换等许多用例至关重要。机器学习已成为基于地图数据预测无线信道属性的常用工具。在本工作中，我们提出了一种基于Transformer的神经网络架构，可以从不同尺寸的地图和稀疏测量中预测链接级属性。地图包含建筑物和植被的信息。Transformer模型关注与路径损耗预测相关的区域，因此可以有效地适应不同尺寸的地图。此外，我们的方法适用于连续的发射机和接收机坐标，无需离散化处理。在实验中，我们展示了所提出的模型能够从稀疏的训练数据中高效地学习到主导的路径损耗，并在新颖地图上进行了良好的泛化。

    Estimating path loss for a transmitter-receiver location is key to many use-cases including network planning and handover. Machine learning has become a popular tool to predict wireless channel properties based on map data. In this work, we present a transformer-based neural network architecture that enables predicting link-level properties from maps of various dimensions and from sparse measurements. The map contains information about buildings and foliage. The transformer model attends to the regions that are relevant for path loss prediction and, therefore, scales efficiently to maps of different size. Further, our approach works with continuous transmitter and receiver coordinates without relying on discretization. In experiments, we show that the proposed model is able to efficiently learn dominant path losses from sparse training data and generalizes well when tested on novel maps.
    
[^132]: 魔法词是什么？LLM提示的控制理论研究

    What's the Magic Word? A Control Theory of LLM Prompting. (arXiv:2310.04444v1 [cs.CL])

    [http://arxiv.org/abs/2310.04444](http://arxiv.org/abs/2310.04444)

    本论文将提示工程形式化为LLM上的最优控制问题，研究了给定token序列时是否存在一种最优提示能够准确预测最终的token，并提出了控制理论中的指标来描述LLM的可操纵性。

    

    提示工程在LLM的部署中是有效和重要的，但在数学上理解不足。在这里，我们将提示工程形式化为LLM上的最优控制问题，其中提示被认为是调节LLM输出分布的控制变量。在这个框架内，我们提出一个简单的问题：给定一个token序列，是否总存在一个我们可以添加的提示，使得LLM能够准确预测最终的token？我们将这样的最优提示称为魔法词，因为添加提示会导致LLM输出正确的答案。如果存在魔法词，我们能否找到它们？如果可以，它们的特性是什么？我们提供了将控制理论应用于自注意力头的分析分析，证明了其权重矩阵的奇异值函数为可控制性的上界。我们借鉴控制理论来提出了一种叫做$k-\epsilon$可控制性的指标，用于描述LLM的可操纵性。

    Prompt engineering is effective and important in the deployment of LLMs but is poorly understood mathematically. Here, we formalize prompt engineering as an optimal control problem on LLMs -- where the prompt is considered a control variable for modulating the output distribution of the LLM. Within this framework, we ask a simple question: given a sequence of tokens, does there always exist a prompt we can prepend that will steer the LLM toward accurately predicting the final token? We call such an optimal prompt the magic word since prepending the prompt causes the LLM to output the correct answer. If magic words exist, can we find them? If so, what are their properties? We offer analytic analysis on the controllability of the self-attention head where we prove a bound on controllability as a function of the singular values of its weight matrices. We take inspiration from control theory to propose a metric called $k-\epsilon$ controllability to characterize LLM steerability. We comput
    
[^133]: 用约束强化学习对抗奖励模型过度优化

    Confronting Reward Model Overoptimization with Constrained RLHF. (arXiv:2310.04373v1 [cs.LG])

    [http://arxiv.org/abs/2310.04373](http://arxiv.org/abs/2310.04373)

    本研究首次研究了组合奖励模型中的过度优化问题，发现组成奖励模型之间的相关性对问题的解决方式有重要影响。我们提出了一种使用约束强化学习来解决这个问题的方法。

    

    大型语言模型通常通过优化适应人类反馈的奖励模型来与人类偏好保持一致。然而，人类偏好是多方面的，越来越常见的做法是从几个简单的奖励模型中派生出奖励，每个模型捕捉语言质量的不同方面。然而，当组合这些组成的奖励模型时，适当地加权变得困难。更加困难的是，由于任何奖励模型只是人类评价的代理，这一过程容易受到过度优化的影响，即超过某一点后，获得更高奖励与更差的人类评价相关。本文通过对组合奖励模型中过度优化进行研究，首次展示了组成奖励模型之间的相关性对这些点的位置有显著影响。然后，我们介绍了一种使用约束强化学习来解决这个问题的方法。

    Large language models are typically aligned with human preferences by optimizing $\textit{reward models}$ (RMs) fitted to human feedback. However, human preferences are multi-faceted, and it is increasingly common to derive reward from a composition of simpler reward models which each capture a different aspect of language quality. This itself presents a challenge, as it is difficult to appropriately weight these component RMs when combining them. Compounding this difficulty, because any RM is only a proxy for human evaluation, this process is vulnerable to $\textit{overoptimization}$, wherein past a certain point, accumulating higher reward is associated with worse human ratings. In this paper, we perform, to our knowledge, the first study on overoptimization in composite RMs, showing that correlation between component RMs has a significant effect on the locations of these points. We then introduce an approach to solve this issue using constrained reinforcement learning as a means of 
    
[^134]: 在概率测度空间中加速优化

    Accelerating optimization over the space of probability measures. (arXiv:2310.04006v1 [math.OC])

    [http://arxiv.org/abs/2310.04006](http://arxiv.org/abs/2310.04006)

    本研究研究了在概率测度空间中加速优化的问题，提出了一种类似于欧几里得空间中基于矩方法的哈密顿流方法，并证明了其可以达到任意高阶的收敛速度。

    

    梯度优化方法的加速是一个非常实用和理论上有意义的问题，特别是在机器学习应用中。大多数研究都集中在欧几里得空间的优化上，但考虑到在许多机器学习问题中需要在概率测度空间上进行优化，研究这种情况下的加速梯度方法是很有意义的。为此，我们引入了一种类似于欧几里得空间中基于矩方法的哈密顿流方法。我们证明了基于这种方法的算法可以达到任意高阶的收敛速度。数值实例证明了我们的论断。

    Acceleration of gradient-based optimization methods is an issue of significant practical and theoretical interest, particularly in machine learning applications. Most research has focused on optimization over Euclidean spaces, but given the need to optimize over spaces of probability measures in many machine learning problems, it is of interest to investigate accelerated gradient methods in this context too. To this end, we introduce a Hamiltonian-flow approach that is analogous to moment-based approaches in Euclidean space. We demonstrate that algorithms based on this approach can achieve convergence rates of arbitrarily high order. Numerical examples illustrate our claim.
    
[^135]: 使用二维卷积的多任务学习在时间序列数据中的应用

    Multitask Learning for Time Series Data\\with 2D Convolution. (arXiv:2310.03925v1 [cs.LG])

    [http://arxiv.org/abs/2310.03925](http://arxiv.org/abs/2310.03925)

    该论文研究了将多任务学习（MTL）应用于时间序列分类（TSC）问题，并发现将最先进的一维卷积模型与MTL集成时性能下降。为了解决这个问题，提出了一种基于二维卷积的新设计。

    

    多任务学习（MTL）旨在开发一个统一的模型，可以同时处理一组密切相关的任务。通过在多个任务上优化模型，MTL在泛化能力方面通常优于非MTL模型。尽管MTL在计算机视觉、自然语言处理和推荐系统等领域得到了广泛研究，但在时间序列数据中的应用却受到了限制。在本文中，我们研究了将MTL应用于时间序列分类（TSC）问题。然而，当将最先进的基于一维卷积的TSC模型与MTL集成时，TSC模型的性能实际上会下降。通过将一维卷积模型与动态时间规整（DTW）距离函数进行比较，可以看出低下的结果是由于一维卷积层的有限表达能力造成的。为了克服这一挑战，我们提出了一种基于二维卷积的新设计。

    Multitask learning (MTL) aims to develop a unified model that can handle a set of closely related tasks simultaneously. By optimizing the model across multiple tasks, MTL generally surpasses its non-MTL counterparts in terms of generalizability. Although MTL has been extensively researched in various domains such as computer vision, natural language processing, and recommendation systems, its application to time series data has received limited attention. In this paper, we investigate the application of MTL to the time series classification (TSC) problem. However, when we integrate the state-of-the-art 1D convolution-based TSC model with MTL, the performance of the TSC model actually deteriorates. By comparing the 1D convolution-based models with the Dynamic Time Warping (DTW) distance function, it appears that the underwhelming results stem from the limited expressive power of the 1D convolutional layers. To overcome this challenge, we propose a novel design for a 2D convolution-based
    
[^136]: 脑机接口中的相位同步成分自组织

    Phase Synchrony Component Self-Organization in Brain Computer Interface. (arXiv:2310.03748v1 [eess.SP])

    [http://arxiv.org/abs/2310.03748](http://arxiv.org/abs/2310.03748)

    本文提出了相位同步成分自组织的概念，利用深度学习端到端网络自动化脑机接口中的预处理和通道选择，从而提高了分析功能性脑连接和识别脑活动的效果。

    

    相位同步信息在分析功能性脑连接和识别脑活动中起着关键作用。目前广泛采用的特征提取流程由预处理、选择脑电采集通道和相位锁定值（PLV）计算组成，在运动想象分类方面取得了成功。然而，该流程是手动的且依赖专家知识，限制了它在不同应用场景中的便利性和适应性。此外，大多数研究都采用了一般的与数据无关的空间滤波器来抑制噪声，阻碍了更重要的相位同步现象的探索。为了解决这些问题，我们提出了相位同步成分自组织的概念，它能够自适应地学习数据相关的空间滤波器，从而自动化预处理和通道选择程序。基于这个概念，我们开发了第一个深度学习端到端网络，直接提取

    Phase synchrony information plays a crucial role in analyzing functional brain connectivity and identifying brain activities. A widely adopted feature extraction pipeline, composed of preprocessing, selection of EEG acquisition channels, and phase locking value (PLV) calculation, has achieved success in motor imagery classification (MI). However, this pipeline is manual and reliant on expert knowledge, limiting its convenience and adaptability to different application scenarios. Moreover, most studies have employed mediocre data-independent spatial filters to suppress noise, impeding the exploration of more significant phase synchronization phenomena. To address the issues, we propose the concept of phase synchrony component self-organization, which enables the adaptive learning of data-dependent spatial filters for automating both the preprocessing and channel selection procedures. Based on this concept, the first deep learning end-to-end network is developed, which directly extracts 
    
[^137]: GPT-MolBERTa：用于分子性质预测的GPT分子特征语言模型

    GPT-MolBERTa: GPT Molecular Features Language Model for molecular property prediction. (arXiv:2310.03030v1 [physics.chem-ph])

    [http://arxiv.org/abs/2310.03030](http://arxiv.org/abs/2310.03030)

    GPT-MolBERTa是一种用于分子性质预测的自监督大型语言模型，通过使用分子的详细文本描述来学习分子的表示，实验表明其在各种分子属性基准上表现良好，并在回归任务中接近最先进的性能。

    

    随着Transformer架构的出现及其对文本数据的强大理解能力，基于文本描述预测分子属性的新领域已经开启。尽管SMILES是最常见的表示形式，但它们缺乏健壮性、丰富信息和规范性，限制了它们成为可推广表示的有效性。在这里，我们提出了GPT-MolBERTa，一种自监督大型语言模型（LLM），它使用分子的详细文本描述来预测其性质。使用ChatGPT收集了326000个分子的基于文本的描述，并用于训练LLM来学习分子的表示。为了预测下游任务的性能，细调阶段使用了BERT和RoBERTa模型。实验证明，GPT-MolBERTa在各种分子属性基准上表现良好，并在回归任务中接近最先进的性能。此外，对注意力进行了进一步分析。

    With the emergence of Transformer architectures and their powerful understanding of textual data, a new horizon has opened up to predict the molecular properties based on text description. While SMILES are the most common form of representation, they are lacking robustness, rich information and canonicity, which limit their effectiveness in becoming generalizable representations. Here, we present GPT-MolBERTa, a self-supervised large language model (LLM) which uses detailed textual descriptions of molecules to predict their properties. A text based description of 326000 molecules were collected using ChatGPT and used to train LLM to learn the representation of molecules. To predict the properties for the downstream tasks, both BERT and RoBERTa models were used in the finetuning stage. Experiments show that GPT-MolBERTa performs well on various molecule property benchmarks, and approaching state of the art performance in regression tasks. Additionally, further analysis of the attention 
    
[^138]: IBCL：连续学习中零样本模型生成用于任务权衡

    IBCL: Zero-shot Model Generation for Task Trade-offs in Continual Learning. (arXiv:2310.02995v1 [cs.LG])

    [http://arxiv.org/abs/2310.02995](http://arxiv.org/abs/2310.02995)

    IBCL提出了一种用于连续学习中任务权衡的零样本模型生成方法，通过更新知识库并利用模型参数分布的凸包形式，实现不同任务性能之间的权衡偏好。

    

    类似于通用的多任务学习，连续学习具有多目标优化的特性，因此面临着不同任务性能之间的权衡。也就是说，为了优化当前任务分布，可能需要在一些先前的任务上牺牲性能。这意味着在不同时间点存在多个帕累托最优的模型，每个模型都解决了不同的任务性能权衡问题。研究人员讨论了如何训练特定的模型来解决特定的权衡偏好。然而，现有的算法需要与偏好数量成比例的训练开销，当存在多个甚至是无限多个偏好时，这是一个巨大的负担。作为响应，我们提出了Imprecise Bayesian Continual Learning (IBCL)。在新任务出现时，IBCL(1)通过模型参数分布的凸包形式更新知识库，(2)获得了特定模型，以实现零样本的任务权衡偏好。

    Like generic multi-task learning, continual learning has the nature of multi-objective optimization, and therefore faces a trade-off between the performance of different tasks. That is, to optimize for the current task distribution, it may need to compromise performance on some previous tasks. This means that there exist multiple models that are Pareto-optimal at different times, each addressing a distinct task performance trade-off. Researchers have discussed how to train particular models to address specific trade-off preferences. However, existing algorithms require training overheads proportional to the number of preferences -- a large burden when there are multiple, possibly infinitely many, preferences. As a response, we propose Imprecise Bayesian Continual Learning (IBCL). Upon a new task, IBCL (1) updates a knowledge base in the form of a convex hull of model parameter distributions and (2) obtains particular models to address task trade-off preferences with zero-shot. That is,
    
[^139]: 在用户模型错误的情况下的在线聚类强化学习

    Online Clustering of Bandits with Misspecified User Models. (arXiv:2310.02717v1 [cs.LG])

    [http://arxiv.org/abs/2310.02717](http://arxiv.org/abs/2310.02717)

    本文介绍了在用户模型错误的情况下的聚类强化学习问题，并提出了两个鲁棒的聚类强化学习算法，以解决用户模型偏差的挑战。

    

    上下文线性强化学习是一个重要的在线学习问题，在每轮中，给定臂的特征，学习代理选择一个臂来最大化长期的累积奖励。聚类强化学习是一系列工作，利用用户偏好的协同效应，并在经典的线性强化学习算法上取得了显著的改进。然而，现有的聚类强化学习算法需要正确规定线性用户模型，当这个关键假设不成立时，可能会失败。如何为在用户模型错误的实际情况下设计鲁棒的聚类强化学习算法仍然是一个开放的问题。在本文中，我们首次提出了在用户模型错误的情况下的聚类强化学习问题，其中用户模型中的期望奖励可能有偏差，不是完美的线性模型。我们设计了两个鲁棒的聚类强化学习算法RCLUMB和RSCLUMB（分别用动态图和集合表示学习到的聚类结构）。

    The contextual linear bandit is an important online learning problem where given arm features, a learning agent selects an arm at each round to maximize the cumulative rewards in the long run. A line of works, called the clustering of bandits (CB), utilize the collaborative effect over user preferences and have shown significant improvements over classic linear bandit algorithms. However, existing CB algorithms require well-specified linear user models and can fail when this critical assumption does not hold. Whether robust CB algorithms can be designed for more practical scenarios with misspecified user models remains an open problem. In this paper, we are the first to present the important problem of clustering of bandits with misspecified user models (CBMUM), where the expected rewards in user models can be perturbed away from perfect linear models. We devise two robust CB algorithms, RCLUMB and RSCLUMB (representing the learned clustering structure with dynamic graph and sets, resp
    
[^140]: 强化学习基础：朝向具有基础先验辅助的具身通用智能体

    Foundation Reinforcement Learning: towards Embodied Generalist Agents with Foundation Prior Assistance. (arXiv:2310.02635v1 [cs.RO])

    [http://arxiv.org/abs/2310.02635](http://arxiv.org/abs/2310.02635)

    本研究提出了一种基于具身基础先验的基础强化学习框架，通过加速训练过程来提高样本效率。

    

    最近人们已经表明，从互联网规模的数据中进行大规模预训练是构建通用模型的关键，正如在NLP中所见。为了构建具身通用智能体，我们和许多其他研究者假设这种基础先验也是不可或缺的组成部分。然而，目前尚不清楚如何以适当的具体形式表示这些具身基础先验，以及它们应该如何在下游任务中使用。在本文中，我们提出了一组直观有效的具身先验，包括基础策略、价值和成功奖励。所提出的先验是基于目标条件的MDP。为了验证其有效性，我们实例化了一个由这些先验辅助的演员-评论家方法，称之为基础演员-评论家（FAC）。我们将我们的框架命名为基础强化学习（FRL），因为它完全依赖于具身基础先验来进行探索、学习和强化。FRL的好处有三个。(1)样本效率高。通过基础先验加速训练过程，减少样本使用量。

    Recently, people have shown that large-scale pre-training from internet-scale data is the key to building generalist models, as witnessed in NLP. To build embodied generalist agents, we and many other researchers hypothesize that such foundation prior is also an indispensable component. However, it is unclear what is the proper concrete form to represent those embodied foundation priors and how they should be used in the downstream task. In this paper, we propose an intuitive and effective set of embodied priors that consist of foundation policy, value, and success reward. The proposed priors are based on the goal-conditioned MDP. To verify their effectiveness, we instantiate an actor-critic method assisted by the priors, called Foundation Actor-Critic (FAC). We name our framework as Foundation Reinforcement Learning (FRL), since it completely relies on embodied foundation priors to explore, learn and reinforce. The benefits of FRL are threefold. (1) Sample efficient. With foundation p
    
[^141]: 关于随机梯度下降中多层蒙特卡洛的并行复杂性

    On the Parallel Complexity of Multilevel Monte Carlo in Stocahstic Gradient Descent. (arXiv:2310.02402v1 [cs.LG])

    [http://arxiv.org/abs/2310.02402](http://arxiv.org/abs/2310.02402)

    本文提出了一种延迟MLMC梯度估计器，通过重复利用之前步骤中计算过的梯度分量，大大降低了MLMC的并行复杂性，并在数值实验中证明了其在随机梯度下降中具有更好的并行复杂性。

    

    在用于顺序模拟（如神经随机微分方程）的随机梯度下降（SGD）中，多层蒙特卡洛（MLMC）方法已被证明在理论计算复杂性方面优于朴素的蒙特卡洛方法。然而在实践中，MLMC在现代GPU等大规模并行计算平台上的可扩展性较差，因为其并行复杂性与朴素蒙特卡洛方法相当。为了解决这个问题，我们提出了延迟MLMC梯度估计器，通过重复利用之前步骤中计算过的梯度分量，大大降低MLMC的并行复杂性。所提出的估计器在每次迭代中能够证明降低平均并行复杂性，但代价是稍差的收敛速率。在我们的数值实验中，我们使用深度对冲的示例来证明与标准MLMC在SGD中相比，我们的方法具有更好的并行复杂性。

    In the stochastic gradient descent (SGD) for sequential simulations such as the neural stochastic differential equations, the Multilevel Monte Carlo (MLMC) method is known to offer better theoretical computational complexity compared to the naive Monte Carlo approach. However, in practice, MLMC scales poorly on massively parallel computing platforms such as modern GPUs, because of its large parallel complexity which is equivalent to that of the naive Monte Carlo method. To cope with this issue, we propose the delayed MLMC gradient estimator that drastically reduces the parallel complexity of MLMC by recycling previously computed gradient components from earlier steps of SGD. The proposed estimator provably reduces the average parallel complexity per iteration at the cost of a slightly worse per-iteration convergence rate. In our numerical experiments, we use an example of deep hedging to demonstrate the superior parallel complexity of our method compared to the standard MLMC in SGD.
    
[^142]: 表示工程化：AI透明化的自上而下方法

    Representation Engineering: A Top-Down Approach to AI Transparency. (arXiv:2310.01405v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.01405](http://arxiv.org/abs/2310.01405)

    这项研究介绍了一种名为表示工程化（RepE）的自上而下方法，通过借鉴认知神经科学的见解，提供了一种增强AI系统透明性的解决方案。该方法将集群级别的表示放在分析的核心，为监测和操纵深度神经网络中的高级认知现象提供了新的方法，并展示了在解决与安全相关的问题上的潜力。

    

    本文中，我们确定并描述了表示工程化（RepE）这一新兴领域，这是一种通过借鉴认知神经科学的见解来增强AI系统透明性的方法。RepE将集群级别的表示放在分析的核心，而不是神经元或电路，为我们提供了监测和操纵深度神经网络（DNNs）中高级认知现象的新方法。我们提供了RepE技术的基准和初步分析，显示它们提供了简单而有效的解决方案，用于改善我们对大型语言模型的理解和控制。我们展示了这些方法如何在包括诚实性、无害性、追求权力等一系列与安全相关的问题上发挥作用，展示了自上而下透明性研究的潜力。我们希望这项工作能够促进RepE的进一步探索，并推动AI系统的透明性和安全性的进步。

    In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.
    
[^143]: 基于窗口的模型平均改善异构联邦学习的泛化能力

    Window-based Model Averaging Improves Generalization in Heterogeneous Federated Learning. (arXiv:2310.01366v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.01366](http://arxiv.org/abs/2310.01366)

    基于窗口的模型平均方法(WIMA)通过聚合不同轮次的全局模型，有效捕获多个用户的知识，减少偏差，提高了异构联邦学习的泛化能力和稳定性，无需额外的通信或客户端计算开销。

    

    联邦学习旨在从分布式用户中学习一个全局模型，并保护他们的隐私。然而，当数据分布异构时，学习过程变得嘈杂、不稳定，并且倾向于最后一次观察到的客户端数据，从而减慢收敛速度。为了解决这些问题并提高全局模型的鲁棒性和泛化能力，我们提出了基于窗口的模型平均方法（WIMA）。WIMA使用基于窗口的方法来聚合不同轮次的全局模型，有效地捕获多个用户的知识并减少最后几个用户的偏差。通过采用窗口视图来处理轮次，WIMA可以从训练的初始阶段就应用。重要的是，我们的方法不引入额外的通信或客户端计算开销。我们的实验表明，WIMA对分布偏移和不好的客户端采样具有鲁棒性，结果呈现出更平滑和更稳定的学习趋势。此外，WIMA可以轻松地进行扩展和集成到现有的联邦学习框架中。

    Federated Learning (FL) aims to learn a global model from distributed users while protecting their privacy. However, when data are distributed heterogeneously the learning process becomes noisy, unstable, and biased towards the last seen clients' data, slowing down convergence. To address these issues and improve the robustness and generalization capabilities of the global model, we propose WIMA (Window-based Model Averaging). WIMA aggregates global models from different rounds using a window-based approach, effectively capturing knowledge from multiple users and reducing the bias from the last ones. By adopting a windowed view on the rounds, WIMA can be applied from the initial stages of training. Importantly, our method introduces no additional communication or client-side computation overhead. Our experiments demonstrate the robustness of WIMA against distribution shifts and bad client sampling, resulting in smoother and more stable learning trends. Additionally, WIMA can be easily 
    
[^144]: 使用深度生成模型的基于集成方法的井下特性表征

    Subsurface Characterization using Ensemble-based Approaches with Deep Generative Models. (arXiv:2310.00839v1 [cs.LG])

    [http://arxiv.org/abs/2310.00839](http://arxiv.org/abs/2310.00839)

    本文提出了一种使用深度生成模型和集成方法的井下特性表征方法。通过结合WGAN-GP和ES-MDA技术，实现了准确且高效的K场估计。这种方法在几个井下实例中得到了验证，并展示了未知K字段的主要特征。

    

    估计如水力传导率（K）等空间分布属性是井下特性表征中的重大挑战。然而，由于计算成本高和稀疏数据集的预测精度低，逆向建模在不适定的高维应用中受限。本文将Wasserstein生成对抗网络与梯度惩罚（WGAN-GP）和基于集成的多元数据同化（ES-MDA）技术相结合，实现了准确且高效的井下特性表征。WGAN-GP通过训练从低维潜变量空间生成高维K场，ES-MDA通过同化可用测量结果来更新潜变量。利用几个井下实例评估了所提出方法的准确性和效率，以及未知K字段的主要特征。

    Estimating spatially distributed properties such as hydraulic conductivity (K) from available sparse measurements is a great challenge in subsurface characterization. However, the use of inverse modeling is limited for ill-posed, high-dimensional applications due to computational costs and poor prediction accuracy with sparse datasets. In this paper, we combine Wasserstein Generative Adversarial Network with Gradient Penalty (WGAN-GP), a deep generative model that can accurately capture complex subsurface structure, and Ensemble Smoother with Multiple Data Assimilation (ES-MDA), an ensemble-based inversion method, for accurate and accelerated subsurface characterization. WGAN-GP is trained to generate high-dimensional K fields from a low-dimensional latent space and ES-MDA then updates the latent variables by assimilating available measurements. Several subsurface examples are used to evaluate the accuracy and efficiency of the proposed method and the main features of the unknown K fie
    
[^145]: SELF：基于语言驱动的大型语言模型自主进化

    SELF: Language-Driven Self-Evolution for Large Language Model. (arXiv:2310.00533v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.00533](http://arxiv.org/abs/2310.00533)

    SELF提出了一种基于语言驱动的创新方法，允许大型语言模型（LLM）不断自我进化，并通过语言反馈作为评估工具来改进模型的响应能力和训练稳定性。

    

    大型语言模型（LLM）展示了在多个领域中的卓越适应能力。然而，实现人类水平的学习和推动自主人工智能的关键——模型自主进化的路径仍然未知。我们引入了一种创新的方法，名为"SELF"（带有语言反馈的自主进化）。这种方法使LLM能够不断地自我进化。此外，SELF利用语言反馈作为一种多功能、全面的评估工具，精确定位响应改进的领域，并提高自主进化训练的稳定性。SELF首先进行元技能学习，专注于自我反馈和自我精炼。这些元技能是关键，引导模型在自制数据的持续训练周期中进行后续的自我进化，从而增强其内在能力。在给定无标签指令的情况下，SELF使模型具备了能够...

    Large Language Models (LLMs) have showcased remarkable versatility across diverse domains. However, the pathway toward autonomous model development, a cornerstone for achieving human-level learning and advancing autonomous AI, remains largely uncharted. We introduce an innovative approach, termed "SELF" (Self-Evolution with Language Feedback). This methodology empowers LLMs to undergo continual self-evolution. Furthermore, SELF employs language-based feedback as a versatile and comprehensive evaluative tool, pinpointing areas for response refinement and bolstering the stability of self-evolutionary training. Initiating with meta-skill learning, SELF acquires foundational meta-skills with a focus on self-feedback and self-refinement. These meta-skills are critical, guiding the model's subsequent self-evolution through a cycle of perpetual training with self-curated data, thereby enhancing its intrinsic abilities. Given unlabeled instructions, SELF equips the model with the capability to
    
[^146]: 提炼归纳偏差：超越模型压缩的知识蒸馏

    Distilling Inductive Bias: Knowledge Distillation Beyond Model Compression. (arXiv:2310.00369v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.00369](http://arxiv.org/abs/2310.00369)

    本文提出了一种超越模型压缩的知识蒸馏方法，通过从轻量级教师模型中提取归纳偏差，使Vision Transformers (ViTs) 的应用成为可能。这种方法包括使用一组不同架构的教师模型来指导学生Transformer，从而有效提高学生的性能。

    

    随着计算机视觉的快速发展，Vision Transformers (ViTs) 提供了在视觉和文本领域中实现统一信息处理的诱人前景。但是由于ViTs缺乏固有的归纳偏差，它们需要大量的训练数据。为了使它们的应用实际可行，我们引入了一种创新的基于集成的蒸馏方法，从轻量级的教师模型中提取归纳偏差。以前的系统仅依靠基于卷积的教学方法。然而，这种方法将一组具有不同架构倾向的轻量级教师模型（例如卷积和非线性卷积）同时用于指导学生Transformer。由于这些独特的归纳偏差，教师模型可以从各种存储数据集中获得广泛的知识，从而提高学生的性能。我们提出的框架还涉及预先计算和存储logits，从根本上实现了非归一化的状态匹配。

    With the rapid development of computer vision, Vision Transformers (ViTs) offer the tantalizing prospect of unified information processing across visual and textual domains. But due to the lack of inherent inductive biases in ViTs, they require enormous amount of data for training. To make their applications practical, we introduce an innovative ensemble-based distillation approach distilling inductive bias from complementary lightweight teacher models. Prior systems relied solely on convolution-based teaching. However, this method incorporates an ensemble of light teachers with different architectural tendencies, such as convolution and involution, to instruct the student transformer jointly. Because of these unique inductive biases, instructors can accumulate a wide range of knowledge, even from readily identifiable stored datasets, which leads to enhanced student performance. Our proposed framework also involves precomputing and storing logits in advance, essentially the unnormalize
    
[^147]: SpatialRank: 基于时空数据的城市事件排名与NDCG优化

    SpatialRank: Urban Event Ranking with NDCG Optimization on Spatiotemporal Data. (arXiv:2310.00270v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.00270](http://arxiv.org/abs/2310.00270)

    这篇论文提出了一种名为SpatialRank的新颖空间事件排名方法，通过基于时空数据的NDCG优化来解决城市事件排名问题。

    

    城市事件排名问题旨在预测未来事件（如交通事故和犯罪事件）的风险最高的前k个地点。这个问题对公共安全和城市管理非常重要，尤其是在资源有限的情况下。然而，由于地点之间复杂而动态的时空相关性，空间中城市事件的不均匀分布，以及正确对具有相似特征的附近地点进行排名的困难，这个问题很具挑战性。前人的研究主要旨在准确预测所有地点的实际风险得分或事件计数。由于预测错误，由此得到的排名通常质量较低。学习排序方法直接优化诸如标准化折扣累积增益（NDCG）之类的指标，但不能处理地点之间存在的时空自相关性。在本文中，我们通过提出一种名为SpatialRank的新颖空间事件排名方法来弥合这一差距。

    The problem of urban event ranking aims at predicting the top-k most risky locations of future events such as traffic accidents and crimes. This problem is of fundamental importance to public safety and urban administration especially when limited resources are available. The problem is, however, challenging due to complex and dynamic spatio-temporal correlations between locations, uneven distribution of urban events in space, and the difficulty to correctly rank nearby locations with similar features. Prior works on event forecasting mostly aim at accurately predicting the actual risk score or counts of events for all the locations. Rankings obtained as such usually have low quality due to prediction errors. Learning-to-rank methods directly optimize measures such as Normalized Discounted Cumulative Gain (NDCG), but cannot handle the spatiotemporal autocorrelation existing among locations. In this paper, we bridge the gap by proposing a novel spatial event ranking approach named Spati
    
[^148]: 两两邻近策略优化: 利用相对反馈进行LLM对齐

    Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment. (arXiv:2310.00212v1 [cs.LG])

    [http://arxiv.org/abs/2310.00212](http://arxiv.org/abs/2310.00212)

    该论文提出了一种新的强化学习框架，使用相对反馈来调整大型语言模型（LLMs）的行为，解决了现有方法在优化比较损失训练的奖励时存在的限制。同时，还提出了一种新的基于轨迹的策略梯度算法（PPPO），用于更有效地进行算法设计和函数逼近。

    

    大型语言模型（LLMs）通过在大型语料库上预先训练来获取广泛的世界知识。然而，由于接触到低质量数据，LLMs可能表现出与人类价值不一致的有害行为。引导LLMs朝着有益行为方向发展的主导方法涉及使用人类反馈的强化学习（RLHF），其中Proximal Policy Optimization（PPO）是默认的RL优化器。尽管其有效性，但PPO在优化基于比较损失训练的奖励时存在局限性。主要问题是，由于需要校准奖励尺度，PPO对于包含相同偏好信息的等价奖励函数不具备不变性。此外，与基于轨迹的优化相比，PPO对于基于令牌的更新的需求引入了函数逼近和算法设计方面的复杂性。本文提出了一种新的框架，基于相对反馈的强化学习，以及一种新颖的基于轨迹的策略梯度算法，Pairwise Proximal Policy Optimization（PPPO），用于解决上述问题。

    Large Language Models (LLMs) can acquire extensive world knowledge through pre-training on large corpora. However, due to exposure to low-quality data, LLMs may exhibit harmful behavior without aligning with human values. The dominant approach for steering LLMs towards beneficial behavior involves Reinforcement Learning with Human Feedback (RLHF), with Proximal Policy Optimization (PPO) serving as the default RL optimizer. Despite its effectiveness, PPO has limitations when optimizing rewards trained from comparison-based loss. Primarily, PPO is not invariant to equivalent reward functions containing identical preference information due to the need to calibrate the reward scale. Additionally, PPO's necessity for token-wise updates introduces complexity in both function approximation and algorithm design compared to trajectory-wise optimization. This paper proposes a new framework, reinforcement learning with relative feedback, and a novel trajectory-wise policy gradient algorithm, Pair
    
[^149]: ABScribe: 使用大型语言模型在人工智能与人类共同写作任务中快速探索多种写作变化

    ABScribe: Rapid Exploration of Multiple Writing Variations in Human-AI Co-Writing Tasks using Large Language Models. (arXiv:2310.00117v1 [cs.HC])

    [http://arxiv.org/abs/2310.00117](http://arxiv.org/abs/2310.00117)

    ABScribe是一种界面，支持在人工智能与人类共同写作任务中快速探索多种写作变化。用户可以使用大型语言模型提示快速生成多个变体，这些变体以可重用的按钮形式呈现，并且可以通过上下文工具栏进行快速的就地比较。

    

    通过重新书写文本来探索替代想法是写作过程的关键。最先进的大型语言模型（LLM）可以简化写作变化生成的过程。然而，当前的界面存在同时考虑多种变化的挑战：在不覆盖文本的情况下创建新的版本可能很困难，而按顺序粘贴它们可能会使文档变得杂乱，增加工作量，并打断作者的流程。为了解决这个问题，我们提出了ABScribe，一种支持在人工智能与人类共同写作任务中快速且结构化地探索写作变化的界面。通过ABScribe，用户可以使用LLM提示快速产生多个变体，这些变体会自动转换成可重用的按钮形式。变体在文本段落中被存储在相邻位置，通过在上下文工具栏上的鼠标悬停交互进行快速的就地比较。我们对12名撰写人员进行的用户研究表明，ABScribe能显著减轻任务负荷（d = 1.20, p < 0.001），提高用户的认知程度。

    Exploring alternative ideas by rewriting text is integral to the writing process. State-of-the-art large language models (LLMs) can simplify writing variation generation. However, current interfaces pose challenges for simultaneous consideration of multiple variations: creating new versions without overwriting text can be difficult, and pasting them sequentially can clutter documents, increasing workload and disrupting writers' flow. To tackle this, we present ABScribe, an interface that supports rapid, yet visually structured, exploration of writing variations in human-AI co-writing tasks. With ABScribe, users can swiftly produce multiple variations using LLM prompts, which are auto-converted into reusable buttons. Variations are stored adjacently within text segments for rapid in-place comparisons using mouse-over interactions on a context toolbar. Our user study with 12 writers shows that ABScribe significantly reduces task workload (d = 1.20, p < 0.001), enhances user perceptions o
    
[^150]: 用大型语言模型进行生成式语音识别错误校正

    Generative Speech Recognition Error Correction with Large Language Models. (arXiv:2309.15649v1 [cs.CL])

    [http://arxiv.org/abs/2309.15649](http://arxiv.org/abs/2309.15649)

    本研究探讨了大型语言模型（LLMs）作为ASR后处理器的能力，通过重新评分和错误校正来提高系统性能。通过使用指令提示和任务激活提示方法，结合上下文学习和微调技术，我们展示了LLMs的泛化能力和有效性。

    

    我们研究了大型语言模型（LLM）作为ASR后处理器的能力，用于重新评分和错误校正。我们的重点是使用指令提示让LLMs执行这些任务而无需微调，我们评估了不同的提示方案，包括零-shot和少-shot的上下文学习，以及一种新颖的任务激活提示（TAP）方法，结合指令和演示。通过在两个领域之外的任务（ATIS和WSJ）上使用预先训练的第一次扫描系统和重新评分输出，我们证明仅通过冻结LLMs的上下文学习进行重新评分可以达到与领域调优的LMs重新评分相竞争的结果。通过将提示技术与微调相结合，我们实现了低于N-best Oracle水平的错误率，展示了LLMs的泛化能力。

    We explore the ability of large language models (LLMs) to act as ASR post-processors that perform rescoring and error correction. Our focus is on instruction prompting to let LLMs perform these task without fine-tuning, for which we evaluate different prompting schemes, both zero- and few-shot in-context learning, and a novel task-activating prompting (TAP) method that combines instruction and demonstration. Using a pre-trained first-pass system and rescoring output on two out-of-domain tasks (ATIS and WSJ), we show that rescoring only by in-context learning with frozen LLMs achieves results that are competitive with rescoring by domain-tuned LMs. By combining prompting techniques with fine-tuning we achieve error rates below the N-best oracle level, showcasing the generalization power of the LLMs.
    
[^151]: MAPTree: 用贝叶斯决策树击败“最优”决策树

    MAPTree: Beating "Optimal" Decision Trees with Bayesian Decision Trees. (arXiv:2309.15312v1 [cs.LG])

    [http://arxiv.org/abs/2309.15312](http://arxiv.org/abs/2309.15312)

    MAPTree是一种通过贝叶斯方法对决策树进行归纳的算法，通过AND/OR搜索实现最大后验树的恢复。在实验中，MAPTree在多个数据集上表现出更好的性能，并且能够以更小的树来实现可比较的性能。在合成数据和实际场景中，MAPTree还展示出更强的抗噪声能力和更好的泛化能力。

    

    决策树仍然是当今最流行的机器学习模型之一，主要是因为其开箱即用的性能和可解释性。在这项工作中，我们通过对树上的后验分布进行最大后验推理，提出了一种贝叶斯决策树归纳的方法。我们首先展示了决策树的最大后验推理与AND/OR搜索之间的关联。利用这一关联，我们提出了一种称为MAPTree的AND/OR搜索算法，能够恢复出最大后验树。最后，我们通过在合成数据和实际世界场景中展示最大后验树的经验性能。在16个实际数据集上，MAPTree要么优于基准线，要么在性能相当的情况下具有更小的树。在一个合成数据集上，MAPTree表现出比现有方法更强的抗噪声能力和更好的泛化能力。最后，MAPTree比其他方法更快地恢复出最大后验树。

    Decision trees remain one of the most popular machine learning models today, largely due to their out-of-the-box performance and interpretability. In this work, we present a Bayesian approach to decision tree induction via maximum a posteriori inference of a posterior distribution over trees. We first demonstrate a connection between maximum a posteriori inference of decision trees and AND/OR search. Using this connection, we propose an AND/OR search algorithm, dubbed MAPTree, which is able to recover the maximum a posteriori tree. Lastly, we demonstrate the empirical performance of the maximum a posteriori tree both on synthetic data and in real world settings. On 16 real world datasets, MAPTree either outperforms baselines or demonstrates comparable performance but with much smaller trees. On a synthetic dataset, MAPTree also demonstrates greater robustness to noise and better generalization than existing approaches. Finally, MAPTree recovers the maxiumum a posteriori tree faster tha
    
[^152]: 大规模语言模型重评分的低秩适应技术在参数高效的语音识别中的应用

    Low-rank Adaptation of Large Language Model Rescoring for Parameter-Efficient Speech Recognition. (arXiv:2309.15223v1 [cs.CL])

    [http://arxiv.org/abs/2309.15223](http://arxiv.org/abs/2309.15223)

    这篇论文介绍了一种基于低秩适应技术的神经语言建模系统，用于语音识别的输出重评分。通过使用低秩分解方法和优化插入矩阵，该系统能够以更高效的方式将BERT模型适应到新领域，大大减少了训练时间。

    

    我们提出了一种基于低秩适应（LoRA）的神经语言建模系统，用于语音识别输出重评分。尽管预训练的语言模型（如BERT）在第二次重评分中表现出优越的性能，但将预训练阶段扩展和将预训练模型适应到特定领域的高计算成本限制了它们在重评分中的实际应用。我们提出了一种基于低秩分解的方法，仅使用预训练参数的一小部分（0.08%）来训练重评分的BERT模型并将其适应到新领域。这些插入的矩阵通过相关性正则化损失和判别性训练目标进行优化。所提出的低秩适应Rescore-BERT（LoRB）体系结构在LibriSpeech和内部数据集上评估，训练时间减少了5.4至3.6倍。

    We propose a neural language modeling system based on low-rank adaptation (LoRA) for speech recognition output rescoring. Although pretrained language models (LMs) like BERT have shown superior performance in second-pass rescoring, the high computational cost of scaling up the pretraining stage and adapting the pretrained models to specific domains limit their practical use in rescoring. Here we present a method based on low-rank decomposition to train a rescoring BERT model and adapt it to new domains using only a fraction (0.08%) of the pretrained parameters. These inserted matrices are optimized through a discriminative training objective along with a correlation-based regularization loss. The proposed low-rank adaptation Rescore-BERT (LoRB) architecture is evaluated on LibriSpeech and internal datasets with decreased training times by factors between 5.4 and 3.6.
    
[^153]: 递归超网络在元强化学习中表现出惊人的强大性能

    Recurrent Hypernetworks are Surprisingly Strong in Meta-RL. (arXiv:2309.14970v1 [cs.LG])

    [http://arxiv.org/abs/2309.14970](http://arxiv.org/abs/2309.14970)

    递归超网络和循环神经网络在元强化学习中的端到端学习表现出惊人的强大性能，相比于现有专门方法更为简单但效果更好。

    

    深度强化学习在实际应用时因样本效率低而不易部署。元强化学习通过学习在元训练时利用相关任务的分布来实现少样本学习，直接解决了这个样本效率问题。最近的研究表明，与专门的元强化学习方法相比，与一个通用的序列模型（如循环神经网络）结合的端到端学习是一个令人惊讶的强基准。然而，这样的观点由于有限的支持证据而引起了争议，特别是在之前的研究中确立了完全相反的观点。在本文中，我们进行了实证研究。虽然我们同样发现循环网络可以达到强大的性能，但我们证明了超网络的使用对于发挥循环基线的潜力至关重要。令人惊讶的是，与超网络相结合时，这种远比现有专门方法简单的循环基准实际上能取得更好的表现。

    Deep reinforcement learning (RL) is notoriously impractical to deploy due to sample inefficiency. Meta-RL directly addresses this sample inefficiency by learning to perform few-shot learning when a distribution of related tasks is available for meta-training. While many specialized meta-RL methods have been proposed, recent work suggests that end-to-end learning in conjunction with an off-the-shelf sequential model, such as a recurrent network, is a surprisingly strong baseline. However, such claims have been controversial due to limited supporting evidence, particularly in the face of prior work establishing precisely the opposite. In this paper, we conduct an empirical investigation. While we likewise find that a recurrent network can achieve strong performance, we demonstrate that the use of hypernetworks is crucial to maximizing their potential. Surprisingly, when combined with hypernetworks, the recurrent baselines that are far simpler than existing specialized methods actually ac
    
[^154]: 通过充分因素和必要因素的概率进行不变学习

    Invariant Learning via Probability of Sufficient and Necessary Causes. (arXiv:2309.12559v1 [cs.LG])

    [http://arxiv.org/abs/2309.12559](http://arxiv.org/abs/2309.12559)

    本研究通过引入充分因素和必要因素的概率（PNS）来改善在未知测试分布上的泛化问题，以解决现有方法主要关注因果性的不变性属性而忽视充分性和必要性条件的问题。

    

    在野外学习中，对于未知的、与训练分布不同的测试分布，外部分布（OOD）泛化是不可或缺的。最近从因果性引发的方法在实现OOD泛化方面显示出了巨大的潜力。然而，现有方法主要关注因果性的不变性属性，而在很大程度上忽视了充分性和必要性条件的属性。换句话说，一个必要但不充分的原因（特征）对于分布转换是不变的，但可能没有所需的准确度。相反，一个充分但不必要的原因（特征）倾向于很好地适应特定数据，但可能存在适应新领域的风险。为了捕捉充分和必要因素的信息，我们采用了经典概念——充分和必要因素的概率（PNS），它指示了一个因素是必要和充分原因的概率。为了将PNS与OOD泛化联系起来，我们提出了一种方法

    Out-of-distribution (OOD) generalization is indispensable for learning models in the wild, where testing distribution typically unknown and different from the training. Recent methods derived from causality have shown great potential in achieving OOD generalization. However, existing methods mainly focus on the invariance property of causes, while largely overlooking the property of \textit{sufficiency} and \textit{necessity} conditions. Namely, a necessary but insufficient cause (feature) is invariant to distribution shift, yet it may not have required accuracy. By contrast, a sufficient yet unnecessary cause (feature) tends to fit specific data well but may have a risk of adapting to a new domain. To capture the information of sufficient and necessary causes, we employ a classical concept, the probability of sufficiency and necessary causes (PNS), which indicates the probability of whether one is the necessary and sufficient cause. To associate PNS with OOD generalization, we propose
    
[^155]: 环境偏向特征排序用于鲁棒性的新颖性检测

    Environment-biased Feature Ranking for Novelty Detection Robustness. (arXiv:2309.12301v1 [cs.LG])

    [http://arxiv.org/abs/2309.12301](http://arxiv.org/abs/2309.12301)

    本文提出了一种环境偏向特征排序的方法，用于鲁棒性的新颖性检测。通过计算特征的环境之间分布方差进行评分，并通过去除高分特征来改善性能。这种方法在真实和合成基准数据上均能提高性能。

    

    我们解决了鲁棒性新颖性检测的问题，在该问题中，我们旨在检测语义内容方面的新颖性，同时对其他无关因素的变化具有不变性。具体来说，我们在具有多个环境的设置中操作，确定与环境更相关而不是任务相关内容的特征集合。因此，我们提出了一种方法，该方法从预训练的嵌入和多环境设置开始，成功根据其环境关注度对特征进行排序。首先，我们基于环境之间的特征分布方差计算每个特征的得分。接下来，我们证明通过舍弃得分较高的特征，我们可以去除虚假的相关性，并在正态协方差和子种群转移的情况下提高整体性能，无论是对于真实的还是对于我们为此任务引入的合成基准数据。

    We tackle the problem of robust novelty detection, where we aim to detect novelties in terms of semantic content while being invariant to changes in other, irrelevant factors. Specifically, we operate in a setup with multiple environments, where we determine the set of features that are associated more with the environments, rather than to the content relevant for the task. Thus, we propose a method that starts with a pretrained embedding and a multi-env setup and manages to rank the features based on their environment-focus. First, we compute a per-feature score based on the feature distribution variance between envs. Next, we show that by dropping the highly scored ones, we manage to remove spurious correlations and improve the overall performance by up to 6%, both in covariance and sub-population shift cases, both for a real and a synthetic benchmark, that we introduce for this task.
    
[^156]: 通过学习表示和影响函数，我们能从对抗样本中获得什么信息

    What Learned Representations and Influence Functions Can Tell Us About Adversarial Examples. (arXiv:2309.10916v1 [cs.LG])

    [http://arxiv.org/abs/2309.10916](http://arxiv.org/abs/2309.10916)

    本文将图像处理领域中的对抗子空间技术应用于自然语言处理，提出了基于最近邻和影响函数的检测器，并通过使用影响函数揭示了自然语言处理中的对抗样本子空间与图像处理中的子空间的关系和任务差异。

    

    对抗样本是通过微小扰动来欺骗深度神经网络的，起初在图像处理领域进行研究，最近在自然语言处理领域也开始关注。尽管在自然语言处理中检测对抗样本的方法主要依赖于输入扰动的搜索，但图像处理领域已经发展出一系列技术来表征学习表示中的对抗子空间。本文将这两种方法应用于自然语言处理，一种基于最近邻和影响函数，一种基于马氏距离。特别是前者相比几个强基准产生了最先进的检测器；此外，对影响函数的新颖使用揭示了自然语言处理中的对抗样本子空间与图像处理中的子空间的关系，并展示了它们根据不同自然语言处理任务的差异。

    Adversarial examples, deliberately crafted using small perturbations to fool deep neural networks, were first studied in image processing and more recently in NLP. While approaches to detecting adversarial examples in NLP have largely relied on search over input perturbations, image processing has seen a range of techniques that aim to characterise adversarial subspaces over the learned representations.  In this paper, we adapt two such approaches to NLP, one based on nearest neighbors and influence functions and one on Mahalanobis distances. The former in particular produces a state-of-the-art detector when compared against several strong baselines; moreover, the novel use of influence functions provides insight into how the nature of adversarial example subspaces in NLP relate to those in image processing, and also how they differ depending on the kind of NLP task.
    
[^157]: PoSE: 通过位置跳跃式训练提高LLMs对于上下文窗口的有效拓展

    PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training. (arXiv:2309.10400v1 [cs.CL])

    [http://arxiv.org/abs/2309.10400](http://arxiv.org/abs/2309.10400)

    本文介绍了一种名为PoSE的训练方法，通过在训练过程中使用固定的上下文窗口和操纵位置索引来适应极长的上下文窗口，实验证明这种方法大大减小了内存和时间开销，对性能影响较小，成功将LLaMA模型扩展到了128k个标记。

    

    本文介绍了一种名为Positional Skip-wise (PoSE)训练的方法，用于将大型语言模型（LLMs）适应于极长的上下文窗口。PoSE通过在训练过程中使用固定的上下文窗口和操纵位置索引来模拟长输入，将训练长度与目标上下文窗口大小分离。具体而言，我们从长输入序列中选择若干短块，并引入不同的跳跃偏置项来修改每个块的位置索引。这些跳跃偏置项以及每个块的长度在每个训练样本中都会变化，使得模型能够适应目标上下文窗口中的所有位置，而无需对完整长度的输入进行训练。实验证明，与对完整长度进行微调相比，PoSE大大减小了内存和时间开销，对性能影响较小。利用这一优势，我们成功将LLaMA模型扩展到了128k个标记。此外，我们经验证实，PoSE与

    In this paper, we introduce Positional Skip-wisE (PoSE) training for efficient adaptation of large language models~(LLMs) to extremely long context windows. PoSE decouples train length from target context window size by simulating long inputs using a fixed context window with manipulated position indices during training. Concretely, we select several short chunks from a long input sequence, and introduce distinct skipping bias terms to modify the position indices of each chunk. These bias terms, along with the length of each chunk, are altered for each training example, allowing the model to adapt to all positions within the target context window without training on full length inputs. Experiments show that, compared with fine-tuning on the full length, PoSE greatly reduces memory and time overhead with minimal impact on performance. Leveraging this advantage, we have successfully extended the LLaMA model to 128k tokens. Furthermore, we empirically confirm that PoSE is compatible with 
    
[^158]: 用梯度逼近降低对抗训练成本

    Reducing Adversarial Training Cost with Gradient Approximation. (arXiv:2309.09464v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.09464](http://arxiv.org/abs/2309.09464)

    本文提出了一种新的对抗训练方法——梯度逼近对抗训练(GAAT)，通过泰勒级数的部分和来近似对抗损失，并近似梯度，以降低建立鲁棒模型的成本。

    

    深度学习模型在多个领域取得了最先进的性能，但它们对于经过巧妙但微小扰动的输入非常脆弱，这被称为对抗样本(Adversarial Examples, AEs)。在许多提高模型对抗样本鲁棒性的策略中，基于投影梯度下降(Projected Gradient Descent, PGD)的对抗训练方法是最有效的之一。然而，由于损失函数的最大化使得生成足够强烈的对抗样本需要巨大的计算开销，对于使用更大更复杂的模型，通常的PGD对抗训练方法有时不切实际。本文提出对抗损失可以通过泰勒级数的部分和来近似，并近似对抗损失的梯度，进而提出一种新的高效的对抗训练方法——梯度逼近对抗训练(GAAT)，以降低建立鲁棒模型的成本。此外，进行了大量实验验证了我们的方法的有效性。

    Deep learning models have achieved state-of-the-art performances in various domains, while they are vulnerable to the inputs with well-crafted but small perturbations, which are named after adversarial examples (AEs). Among many strategies to improve the model robustness against AEs, Projected Gradient Descent (PGD) based adversarial training is one of the most effective methods. Unfortunately, the prohibitive computational overhead of generating strong enough AEs, due to the maximization of the loss function, sometimes makes the regular PGD adversarial training impractical when using larger and more complicated models. In this paper, we propose that the adversarial loss can be approximated by the partial sum of Taylor series. Furthermore, we approximate the gradient of adversarial loss and propose a new and efficient adversarial training method, adversarial training with gradient approximation (GAAT), to reduce the cost of building up robust models. Additionally, extensive experiments
    
[^159]: 基于启发式迭代优化的形态学感知一致性计算（MACCHIatO）的研究

    Morphologically-Aware Consensus Computation via Heuristics-based IterATive Optimization (MACCHIatO). (arXiv:2309.08066v1 [cs.CV])

    [http://arxiv.org/abs/2309.08066](http://arxiv.org/abs/2309.08066)

    本文提出了一种基于启发式迭代优化的方法，利用精心选择的距离的Fréchet均值构建二进制或概率一致性分割。与STAPLE方法相比，该方法不受图像背景大小和先验选择的影响。

    

    从多个二进制或概率掩模中提取一致性分割是解决各种任务的重要方法，如解析标注者间的差异性或多个神经网络输出的融合。本文首先证明了STAPLE算法的输出受到图像背景大小和先验选择的影响。然后我们提出了一种新的方法，基于精心选择的距离的Fréchet均值构建二进制或概率一致性分割，使其完全独立于图像背景大小。我们提供了一种启发式方法来优化此准则，从而使一个体素的类别完全由它与不同掩模的体素级距离、它所属的连通组件和分割它的标注者组决定。我们在多个数据集上与STAPLE方法进行了广泛比较。

    The extraction of consensus segmentations from several binary or probabilistic masks is important to solve various tasks such as the analysis of inter-rater variability or the fusion of several neural network outputs. One of the most widely used methods to obtain such a consensus segmentation is the STAPLE algorithm. In this paper, we first demonstrate that the output of that algorithm is heavily impacted by the background size of images and the choice of the prior. We then propose a new method to construct a binary or a probabilistic consensus segmentation based on the Fr\'{e}chet means of carefully chosen distances which makes it totally independent of the image background size. We provide a heuristic approach to optimize this criterion such that a voxel's class is fully determined by its voxel-wise distance to the different masks, the connected component it belongs to and the group of raters who segmented it. We compared extensively our method on several datasets with the STAPLE met
    
[^160]: 纯粹的消息传递可以估计共同邻居进行链路预测

    Pure Message Passing Can Estimate Common Neighbor for Link Prediction. (arXiv:2309.00976v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.00976](http://arxiv.org/abs/2309.00976)

    这篇论文提出了一种纯粹的消息传递方法，用于估计共同邻居进行链路预测。该方法通过利用输入向量的正交性来捕捉联合结构特征，提出了一种新的链路预测模型MPLP，该模型利用准正交向量估计链路级结构特征，同时保留了节点级复杂性。

    

    消息传递神经网络（MPNN）已成为图表示学习中的事实标准。然而，在链路预测方面，它们往往表现不佳，被简单的启发式算法如共同邻居（CN）所超越。这种差异源于一个根本限制：尽管MPNN在节点级表示方面表现出色，但在编码链路预测中至关重要的联合结构特征（如CN）方面则遇到困难。为了弥合这一差距，我们认为通过利用输入向量的正交性，纯粹的消息传递确实可以捕捉到联合结构特征。具体而言，我们研究了MPNN在近似CN启发式算法方面的能力。基于我们的发现，我们引入了一种新的链路预测模型——消息传递链路预测器（MPLP）。MPLP利用准正交向量估计链路级结构特征，同时保留节点级复杂性。此外，我们的方法表明利用消息传递捕捉结构特征能够改善链路预测性能。

    Message Passing Neural Networks (MPNNs) have emerged as the {\em de facto} standard in graph representation learning. However, when it comes to link prediction, they often struggle, surpassed by simple heuristics such as Common Neighbor (CN). This discrepancy stems from a fundamental limitation: while MPNNs excel in node-level representation, they stumble with encoding the joint structural features essential to link prediction, like CN. To bridge this gap, we posit that, by harnessing the orthogonality of input vectors, pure message-passing can indeed capture joint structural features. Specifically, we study the proficiency of MPNNs in approximating CN heuristics. Based on our findings, we introduce the Message Passing Link Predictor (MPLP), a novel link prediction model. MPLP taps into quasi-orthogonal vectors to estimate link-level structural features, all while preserving the node-level complexities. Moreover, our approach demonstrates that leveraging message-passing to capture stru
    
[^161]: 大型视觉语言模型中幻觉的评估与分析

    Evaluation and Analysis of Hallucination in Large Vision-Language Models. (arXiv:2308.15126v1 [cs.LG])

    [http://arxiv.org/abs/2308.15126](http://arxiv.org/abs/2308.15126)

    本文提出了基于大型语言模型的幻觉评估框架HaELM，可以评估大型视觉语言模型中的幻觉问题，并分析了导致幻觉的因素，并提出了缓解幻觉问题的建议。

    

    最近，大型视觉语言模型（LVLMs）取得了显著的成功。然而，LVLMs仍然存在幻觉问题，这限制了在许多场景中的实用性。幻觉指的是LVLMs响应中不存在于视觉输入中的信息，这可能导致重大后果的潜在风险。目前对LVLMs中的幻觉评估的研究工作有限。在本文中，我们提出了基于大型语言模型（LLM）的幻觉评估框架HaELM。HaELM的性能近似于ChatGPT的95%，并具有低成本、可复现、保护隐私和本地部署等额外优势。利用HaELM，我们评估了当前LVLMs中的幻觉。此外，我们分析了导致LVLMs中幻觉的因素，并提出了缓解幻觉问题的有用建议。

    Large Vision-Language Models (LVLMs) have recently achieved remarkable success. However, LVLMs are still plagued by the hallucination problem, which limits the practicality in many scenarios. Hallucination refers to the information of LVLMs' responses that does not exist in the visual input, which poses potential risks of substantial consequences. There has been limited work studying hallucination evaluation in LVLMs. In this paper, we propose Hallucination Evaluation based on Large Language Models (HaELM), an LLM-based hallucination evaluation framework. HaELM achieves an approximate 95% performance comparable to ChatGPT and has additional advantages including low cost, reproducibility, privacy preservation and local deployment. Leveraging the HaELM, we evaluate the hallucination in current LVLMs. Furthermore, we analyze the factors contributing to hallucination in LVLMs and offer helpful suggestions to mitigate the hallucination problem. Our training data and human annotation halluci
    
[^162]: 通过Mixup增强的元学习方法实现蛋白质模拟器的高效微调

    Mixup-Augmented Meta-Learning for Sample-Efficient Fine-Tuning of Protein Simulators. (arXiv:2308.15116v1 [cs.LG])

    [http://arxiv.org/abs/2308.15116](http://arxiv.org/abs/2308.15116)

    本文通过Mixup增强的元学习方法实现了对蛋白质模拟器的高效微调，可以在有限的训练数据下泛化到未见过的场景，并提供了一种通用的模拟连续动态条件的方法。

    

    分子动力学模拟已经成为研究生物分子的基本工具。与此同时，我们希望在分子能够波动的各种条件下对一组粒子进行模拟。本文中，我们将软提示学习方法应用于分子动力学任务并进行了适应性探索。我们的模型可以在有限的训练数据下非常好地泛化到未见过的和超出分布的场景。虽然我们的工作以温度为测试案例，但我们的方法的多功能性使其可以通过任何连续的动态条件（如压力和体积）进行有效模拟。我们的框架有两个阶段：1）使用数据混合技术进行预训练，增强分子结构数据和温度提示，然后通过逐渐增加比例的方式应用课程学习方法。2）基于元学习的微调框架提高了微调过程的样本效率，并为软提示微调提供更好的表现。

    Molecular dynamics simulations have emerged as a fundamental instrument for studying biomolecules. At the same time, it is desirable to perform simulations of a collection of particles under various conditions in which the molecules can fluctuate. In this paper, we explore and adapt the soft prompt-based learning method to molecular dynamics tasks. Our model can remarkably generalize to unseen and out-of-distribution scenarios with limited training data. While our work focuses on temperature as a test case, the versatility of our approach allows for efficient simulation through any continuous dynamic conditions, such as pressure and volumes. Our framework has two stages: 1) Pre-trains with data mixing technique, augments molecular structure data and temperature prompts, then applies a curriculum learning method by increasing the ratio of them smoothly. 2) Meta-learning-based fine-tuning framework improves sample-efficiency of fine-tuning process and gives the soft prompt-tuning better 
    
[^163]: 授权临床医生并民主化数据科学：大型语言模型自动化临床研究的机器学习。 (arXiv:2308.14120v2 [cs.LG] 更新版)

    Empowering Clinicians and Democratizing Data Science: Large Language Models Automate Machine Learning for Clinical Studies. (arXiv:2308.14120v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.14120](http://arxiv.org/abs/2308.14120)

    chatGPT ADA是一种能够自主开发临床研究所需的最先进的机器学习模型的大型语言模型，可将高级分析工具民主化，使非数据科学家的临床医生能够轻松应用于医学领域。

    

    机器学习（ML）开发者（如数据科学家）和从业者（如临床医生）之间存在知识差距，阻碍了ML在临床数据分析中的充分利用。我们研究了chatGPT Advanced Data Analysis（ADA），即GPT-4的扩展，来弥合这一差距并高效执行ML分析的潜力。我们向chatGPT ADA提供了各种医学专业的大型试验的真实临床数据和研究详细信息，没有给出具体指导。ChatGPT ADA基于原始研究的训练数据自主开发了最先进的ML模型，用于预测临床结果，如癌症发展、癌症进展、疾病并发症或致病基因序列等生物标志物。令人惊讶的是，这些ML模型与其已发表的对应物相匹配甚至表现更好。我们得出结论，chatGPT ADA为民主化医学中的ML提供了一个有前景的途径，使非ML专家能够获得先进的分析工具并推动广泛应用。

    A knowledge gap persists between Machine Learning (ML) developers (e.g., data scientists) and practitioners (e.g., clinicians), hampering the full utilization of ML for clinical data analysis. We investigated the potential of the chatGPT Advanced Data Analysis (ADA), an extension of GPT-4, to bridge this gap and perform ML analyses efficiently. Real-world clinical datasets and study details from large trials across various medical specialties were presented to chatGPT ADA without specific guidance. ChatGPT ADA autonomously developed state-of-the-art ML models based on the original study's training data to predict clinical outcomes such as cancer development, cancer progression, disease complications, or biomarkers such as pathogenic gene sequences. Strikingly, these ML models matched or outperformed their published counterparts. We conclude that chatGPT ADA offers a promising avenue to democratize ML in medicine, making advanced analytics accessible to non-ML experts and promoting broa
    
[^164]: 使用带轻量级注意机制的迁移ResNet增强乳腺癌分类

    Enhancing Breast Cancer Classification Using Transfer ResNet with Lightweight Attention Mechanism. (arXiv:2308.13150v1 [eess.IV])

    [http://arxiv.org/abs/2308.13150](http://arxiv.org/abs/2308.13150)

    本文介绍了一种使用ResNet模型和轻量级注意机制框架的图像分类方法，通过优化特征表示、增强分类能力和改善特征可辨别性，在乳腺癌分类任务上显示出卓越性能和潜在应用前景。

    

    深度学习模型通过学习原始像素数据中的复杂特征层次结构，彻底改变了图像分类。本文介绍了一种基于ResNet模型的图像分类方法，并引入了轻量级注意机制框架来提高性能。该框架优化特征表示，增强分类能力，改善特征可辨别性。我们在Breakhis数据集上验证了算法的有效性，在许多方面显示出卓越的性能。我们的方法不仅在传统模型方面表现优越，在当代视觉变换器等最新方法上也显示出优势。在诸如精度、准确度、召回率、F1分数和G-means等指标上取得了显著改进，同时在收敛时间方面表现良好。这些结果增强了算法的性能，巩固了其在实际图像分类任务中的应用前景。

    Deep learning models have revolutionized image classification by learning complex feature hierarchies in raw pixel data. This paper introduces an image classification method based on the ResNet model, and introduces a lightweight attention mechanism framework to improve performance. The framework optimizes feature representation, enhances classification capabilities, and improves feature discriminativeness. We verified the effectiveness of the algorithm on the Breakhis dataset, showing its superior performance in many aspects. Not only in terms of conventional models, our method also shows advantages on state-of-the-art methods such as contemporary visual transformers. Significant improvements have been achieved in metrics such as precision, accuracy, recall, F1-score, and G-means, while also performing well in terms of convergence time. These results strengthen the performance of the algorithm and solidify its application prospects in practical image classification tasks. Keywords: Re
    
[^165]: 使用多条件扩散模型进行音频生成

    Audio Generation with Multiple Conditional Diffusion Model. (arXiv:2308.11940v1 [cs.SD])

    [http://arxiv.org/abs/2308.11940](http://arxiv.org/abs/2308.11940)

    本论文提出了一种使用多条件扩散模型进行音频生成的方法。通过引入内容和风格等额外条件，增强了现有模型的可控性。这种方法可以精确控制生成音频的时间顺序、音高和能量。由于缺乏合适的数据集和评估指标，作者整合了现有数据集并进行了实验验证。

    

    基于文本的音频生成模型有其局限性，因为它们无法包含音频中的所有信息，仅依靠文本会导致受控性受限。为了解决这个问题，我们提出了一种新颖的模型，通过引入额外的条件（包括内容（时间戳）和风格（音高曲线和能量曲线））作为文本的补充，增强了现有预训练文本到音频模型的可控性。这种方法实现了对生成音频的时间顺序、音高和能量的精细控制。为了保持生成的多样性，我们使用一个可训练的控制条件编码器，该编码器由一个大型语言模型增强，并使用一个可训练的融合网络来编码和融合额外的条件，同时保持预训练文本到音频模型的权重不变。由于缺乏合适的数据集和评估指标，我们将现有数据集整合为一个新的数据集，包括音频和相应的条件，并使用一个可训练的控制条件编码器，该编码器由一个大型语言模型增强，并使用一个可训练的融合网络来编码和融合额外的条件，同时保持预训练文本到音频模型的权重不变。

    Text-based audio generation models have limitations as they cannot encompass all the information in audio, leading to restricted controllability when relying solely on text. To address this issue, we propose a novel model that enhances the controllability of existing pre-trained text-to-audio models by incorporating additional conditions including content (timestamp) and style (pitch contour and energy contour) as supplements to the text. This approach achieves fine-grained control over the temporal order, pitch, and energy of generated audio. To preserve the diversity of generation, we employ a trainable control condition encoder that is enhanced by a large language model and a trainable Fusion-Net to encode and fuse the additional conditions while keeping the weights of the pre-trained text-to-audio model frozen. Due to the lack of suitable datasets and evaluation metrics, we consolidate existing datasets into a new dataset comprising the audio and corresponding conditions and use a 
    
[^166]: 联合回声消除和噪声抑制的超级双路径压缩

    Ultra Dual-Path Compression For Joint Echo Cancellation And Noise Suppression. (arXiv:2308.11053v1 [eess.AS])

    [http://arxiv.org/abs/2308.11053](http://arxiv.org/abs/2308.11053)

    本文提出了一种超级双路径压缩方法用于联合回声消除和噪声抑制，通过时间频率双路径压缩实现广泛的压缩比，同时降低计算成本，且在固定压缩比下能够进一步改善性能。

    

    回声消除和噪声抑制对于全双工通信至关重要，然而大多数现有的神经网络在计算成本高和模型复杂度调整上不灵活。在本文中，我们引入时间频率双路径压缩，实现广泛的压缩比，同时降低计算成本。具体来说，在频率压缩方面，使用可训练滤波器代替手动设计的滤波器进行维度缩减。在时间压缩方面，仅使用帧跳预测会导致性能大幅下降，但通过具有完整序列建模的后处理网络可以缓解这个问题。我们发现，在固定压缩比的情况下，同时使用时间和频率方法进行双路径压缩将进一步改善性能，并且几乎不改变模型大小，压缩比覆盖范围从4x到32x。此外，所提出的模型与快速FullSubNet和DeepFilterNet相比具有竞争力的性能。可以访问演示页面。

    Echo cancellation and noise reduction are essential for full-duplex communication, yet most existing neural networks have high computational costs and are inflexible in tuning model complexity. In this paper, we introduce time-frequency dual-path compression to achieve a wide range of compression ratios on computational cost. Specifically, for frequency compression, trainable filters are used to replace manually designed filters for dimension reduction. For time compression, only using frame skipped prediction causes large performance degradation, which can be alleviated by a post-processing network with full sequence modeling. We have found that under fixed compression ratios, dual-path compression combining both the time and frequency methods will give further performance improvement, covering compression ratios from 4x to 32x with little model size change. Moreover, the proposed models show competitive performance compared with fast FullSubNet and DeepFilterNet. A demo page can be f
    
[^167]: 张量压缩的反向传播免费训练（物理信息）的神经网络

    Tensor-Compressed Back-Propagation-Free Training for (Physics-Informed) Neural Networks. (arXiv:2308.09858v1 [cs.LG])

    [http://arxiv.org/abs/2308.09858](http://arxiv.org/abs/2308.09858)

    本文提出了一个完全无需反向传播的神经网络训练框架，并通过张量压缩的方差约减方法和混合梯度评估方法改进了优化和效率。同时，还扩展了框架用于物理信息的神经网络的估计。

    

    反向传播（BP）被广泛用于神经网络训练中计算梯度。然而，由于缺乏硬件和软件资源来支持自动微分，在边缘设备上实现BP是困难的。这大大增加了设备上训练加速器的设计复杂性和上市时间。本文提出了一个完全无需BP的框架，只需要前向传播就可以训练实际的神经网络。我们的技术贡献有三个方面。首先，我们提出了一种张量压缩的方差约减方法，极大提高了零阶（ZO）优化的可扩展性，使其能够处理大于以前ZO方法能力的网络尺寸。其次，我们提出了一种混合梯度评估方法，提高了ZO训练的效率。最后，我们通过提出一种稀疏格方法来扩展我们的BP-free训练框架，用于物理信息的神经网络（PINNs）的估计。

    Backward propagation (BP) is widely used to compute the gradients in neural network training. However, it is hard to implement BP on edge devices due to the lack of hardware and software resources to support automatic differentiation. This has tremendously increased the design complexity and time-to-market of on-device training accelerators. This paper presents a completely BP-free framework that only requires forward propagation to train realistic neural networks. Our technical contributions are three-fold. Firstly, we present a tensor-compressed variance reduction approach to greatly improve the scalability of zeroth-order (ZO) optimization, making it feasible to handle a network size that is beyond the capability of previous ZO approaches. Secondly, we present a hybrid gradient evaluation approach to improve the efficiency of ZO training. Finally, we extend our BP-free training framework to physics-informed neural networks (PINNs) by proposing a sparse-grid approach to estimate the 
    
[^168]: 图结构残差：一种诊断的学习方法

    Graph Structural Residuals: A Learning Approach to Diagnosis. (arXiv:2308.06961v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2308.06961](http://arxiv.org/abs/2308.06961)

    本文提出了一种新颖的框架，将模型诊断的概念与深度图结构学习相结合，通过数据学习系统的底层结构并提供动态观察。研究通过重新定义系统表示、观察和故障的构建，引入自监督图结构学习模型以及在耦合振荡器系统上的实验，展示了数据驱动的诊断方法的潜力。

    

    传统的基于模型的诊断依赖于构建明确的系统模型，这个过程可能费时且需要专业知识。本文提出了一种新颖的框架，将模型诊断的概念与深度图结构学习相结合。这种数据驱动的方法利用数据学习系统的底层结构，并提供由两个不同的图邻接矩阵表示的动态观察。我们的工作通过三个主要贡献实现了图结构学习与模型诊断的无缝集成：(i)重新定义系统表示、观察和故障的构建、(ii)引入两种不同版本的自监督图结构学习模型架构、(iii)通过耦合振荡器系统的实验展示了我们数据驱动的诊断方法的潜力。

    Traditional model-based diagnosis relies on constructing explicit system models, a process that can be laborious and expertise-demanding. In this paper, we propose a novel framework that combines concepts of model-based diagnosis with deep graph structure learning. This data-driven approach leverages data to learn the system's underlying structure and provide dynamic observations, represented by two distinct graph adjacency matrices. Our work facilitates a seamless integration of graph structure learning with model-based diagnosis by making three main contributions: (i) redefining the constructs of system representation, observations, and faults (ii) introducing two distinct versions of a self-supervised graph structure learning model architecture and (iii) demonstrating the potential of our data-driven diagnostic method through experiments on a system of coupled oscillators.
    
[^169]: 分支潜在神经算子

    Branched Latent Neural Operators. (arXiv:2308.02599v1 [cs.LG])

    [http://arxiv.org/abs/2308.02599](http://arxiv.org/abs/2308.02599)

    Branched Latent Neural Operators (BLNOs) are introduced to learn input-output maps for encoding complex physical processes. BLNOs utilize interpretable latent outputs to enhance learned dynamics and overcome the curse of dimensionality, while also showing excellent generalization properties with small training datasets and short training times. The partial connection structure reduces the number of tunable parameters. BLNOs are proven effective in a challenging biophysically detailed test case.

    

    我们介绍了分支潜在神经算子（BLNOs）来学习编码复杂物理过程的输入-输出映射。BLNO由一个简单紧凑的前馈部分连接神经网络定义，该网络在结构上将不同固有角色的输入进行解离，例如将微分方程的时间变量与模型参数分离，并将它们转化为感兴趣的通用领域。BLNO利用可解释的潜在输出增强了学习到的动态，并通过在单个处理器上使用小的训练数据集和短的训练时间展示了出色的泛化性能。实际上，它们的泛化误差在测试阶段采用的离散化方式相同的情况下保持可比性。此外，部分连接在全连接结构的基础上显著减少了可调参数的数量。我们展示了BLNO在涉及生物物理细节的具有挑战性的测试案例中的能力。

    We introduce Branched Latent Neural Operators (BLNOs) to learn input-output maps encoding complex physical processes. A BLNO is defined by a simple and compact feedforward partially-connected neural network that structurally disentangles inputs with different intrinsic roles, such as the time variable from model parameters of a differential equation, while transferring them into a generic field of interest. BLNOs leverage interpretable latent outputs to enhance the learned dynamics and break the curse of dimensionality by showing excellent generalization properties with small training datasets and short training times on a single processor. Indeed, their generalization error remains comparable regardless of the adopted discretization during the testing phase. Moreover, the partial connections, in place of a fully-connected structure, significantly reduce the number of tunable parameters. We show the capabilities of BLNOs in a challenging test case involving biophysically detailed elect
    
[^170]: 统计估计中的分布偏移: Wasserstein扰动与极小极大理论

    Statistical Estimation Under Distribution Shift: Wasserstein Perturbations and Minimax Theory. (arXiv:2308.01853v1 [stat.ML])

    [http://arxiv.org/abs/2308.01853](http://arxiv.org/abs/2308.01853)

    这篇论文研究了统计估计中的分布偏移问题，主要关注Wasserstein分布偏移，提出了联合分布偏移概念，并分析了几个统计问题的解决方法。论文发现了最优的极小极大风险和最不利的扰动，并证明了样本均值和最小二乘估计量的优越性。

    

    分布偏移是现代统计学习中的一个严重问题，因为它们可以将数据的特性从真实情况中系统地改变。我们专注于Wasserstein分布偏移，其中每个数据点可能会发生轻微扰动，而不是Huber污染模型，其中一部分观测值是异常值。我们提出并研究了超出独立扰动的偏移，探索了联合分布偏移，其中每个观测点的扰动可以协调进行。我们分析了几个重要的统计问题，包括位置估计、线性回归和非参数密度估计。在均值估计和线性回归的预测误差方差下，我们找到了精确的极小极大风险、最不利的扰动，并证明了样本均值和最小二乘估计量分别是最优的。这适用于独立和联合偏移，但最不利的扰动和极小极大风险是不同的。

    Distribution shifts are a serious concern in modern statistical learning as they can systematically change the properties of the data away from the truth. We focus on Wasserstein distribution shifts, where every data point may undergo a slight perturbation, as opposed to the Huber contamination model where a fraction of observations are outliers. We formulate and study shifts beyond independent perturbations, exploring Joint Distribution Shifts, where the per-observation perturbations can be coordinated. We analyze several important statistical problems, including location estimation, linear regression, and non-parametric density estimation. Under a squared loss for mean estimation and prediction error in linear regression, we find the exact minimax risk, a least favorable perturbation, and show that the sample mean and least squares estimators are respectively optimal. This holds for both independent and joint shifts, but the least favorable perturbations and minimax risks differ. For
    
[^171]: 核化归一化流

    Kernelised Normalising Flows. (arXiv:2307.14839v1 [stat.ML])

    [http://arxiv.org/abs/2307.14839](http://arxiv.org/abs/2307.14839)

    本文提出了一种新颖的核化归一化流范式，称为Ferumal流，它将核函数集成到归一化流的框架中。相对于基于神经网络的流，核化流可以在低数据环境中产生竞争力或优越的结果，同时保持参数效率。

    

    归一化流是以其可逆的架构而被描述的生成模型。然而，可逆性要求对其表达能力施加限制，需要大量的参数和创新的架构设计来达到满意的结果。虽然基于流的模型主要依赖于基于神经网络的转换来实现表达能力，但替代的转换方法却受到了有限的关注。在这项工作中，我们提出了一种新颖的核化归一化流范式，称为Ferumal流，它将核函数集成到框架中。我们的结果表明，相比于基于神经网络的流，核化流可以产生有竞争力或优越的结果，同时保持参数效率。核化流在低数据环境中表现出色，可以在数据稀缺的应用中进行灵活的非参数密度估计。

    Normalising Flows are generative models characterised by their invertible architecture. However, the requirement of invertibility imposes constraints on their expressiveness, necessitating a large number of parameters and innovative architectural designs to achieve satisfactory outcomes. Whilst flow-based models predominantly rely on neural-network-based transformations for expressive designs, alternative transformation methods have received limited attention. In this work, we present Ferumal flow, a novel kernelised normalising flow paradigm that integrates kernels into the framework. Our results demonstrate that a kernelised flow can yield competitive or superior results compared to neural network-based flows whilst maintaining parameter efficiency. Kernelised flows excel especially in the low-data regime, enabling flexible non-parametric density estimation in applications with sparse data availability.
    
[^172]: 固定积分神经网络

    Fixed Integral Neural Networks. (arXiv:2307.14439v1 [cs.LG])

    [http://arxiv.org/abs/2307.14439](http://arxiv.org/abs/2307.14439)

    本文介绍了一种方法来通过神经网络表示学习函数的解析积分，具有计算精确积分的能力，并能将约束直接应用于积分，而且还介绍了将学习函数约束为正的方法以及相关应用。

    

    将学习函数通过神经网络进行积分是非常有用的，但是这种积分通常是通过数值方法来计算的，因为解析计算积分过程复杂，尤其是对于神经网络这样的学习函数。本文介绍了一种表示学习函数 $f$ 解析积分的方法。这允许精确计算神经网络的积分，并且通过将约束直接应用于积分来对约束神经网络进行参数化。关键的是，我们还介绍了一种将 $f$ 约束为正的方法，这是许多应用（例如概率分布、距离度量等）所必需的条件。最后，我们介绍了几个可以利用我们的固定积分神经网络（FINN）的应用领域。

    It is often useful to perform integration over learned functions represented by neural networks. However, this integration is usually performed numerically, as analytical integration over learned functions (especially neural networks) is generally viewed as intractable. In this work, we present a method for representing the analytical integral of a learned function $f$. This allows the exact integral of a neural network to be computed, and enables constrained neural networks to be parametrised by applying constraints directly to the integral. Crucially, we also introduce a method to constrain $f$ to be positive, a necessary condition for many applications (e.g. probability distributions, distance metrics, etc). Finally, we introduce several applications where our fixed-integral neural network (FINN) can be utilised.
    
[^173]: 关于注意力网络学习动态的研究

    On the learning Dynamics of Attention Networks. (arXiv:2307.13421v1 [cs.LG])

    [http://arxiv.org/abs/2307.13421](http://arxiv.org/abs/2307.13421)

    本研究分析了软注意力、硬注意力和潜变量边际似然（LVML）注意力三种注意力模型的学习动态，发现了它们在所选择的片段聚合方式上的显著差异，并解释了分类模型在梯度下降下的演化对最终结果的影响。

    

    注意力模型通常通过优化三个标准损失函数之一来学习，分别称为软注意力、硬注意力和潜变量边际似然（LVML）注意力。这三种范式都是为了达到相同的目标，即找到两个模型：一个“焦点”模型，用于“选择”输入中的正确“片段”，和一个“分类”模型，用于将选定的片段处理成目标标签。然而，它们在所选择的片段聚合方式上存在显著差异，导致了不同的动态和最终结果。我们观察到使用这些范式学习的模型具有独特的特征，并将其解释为在焦点模型固定时，分类模型在梯度下降下的演化所致。我们还在一个简单的设置中分析了这些范式，并推导出梯度流下参数轨迹的闭式表达式。在软注意力损失下，焦点模型在初始化阶段快速改善。

    Attention models are typically learned by optimizing one of three standard loss functions that are variously called -- soft attention, hard attention, and latent variable marginal likelihood (LVML) attention. All three paradigms are motivated by the same goal of finding two models -- a `focus' model that `selects' the right \textit{segment} of the input and a `classification' model that processes the selected segment into the target label. However, they differ significantly in the way the selected segments are aggregated, resulting in distinct dynamics and final results. We observe a unique signature of models learned using these paradigms and explain this as a consequence of the evolution of the classification model under gradient descent when the focus model is fixed. We also analyze these paradigms in a simple setting and derive closed-form expressions for the parameter trajectory under gradient flow. With the soft attention loss, the focus model improves quickly at initialization a
    
[^174]: 棋盘上的棋子价值。 (arXiv:2307.05330v1 [cs.AI])

    The Value of Chess Squares. (arXiv:2307.05330v1 [cs.AI])

    [http://arxiv.org/abs/2307.05330](http://arxiv.org/abs/2307.05330)

    本研究通过引入边际估值对国际象棋棋盘上的棋子和棋盘进行评价，提供了关于马、象和兵的有价值的见解。

    

    我们的研究的主要目标是评估棋盘上棋子的价值，并确定棋子在棋盘上的摆放位置。随着国际象棋人工智能的出现，我们能够准确评估国际象棋局面的价值。传统方法对棋子赋予固定的价值$(\symking=\infty, \symqueen=9, \symrook=5, \symbishop=3, \symknight=3, \sympawn=1)$。我们通过引入棋子和棋盘方面的边际估值来改进这种分析。我们通过研究马和象的位置，并提供有关兵的价值的宝贵见解来演示我们的方法。值得注意的是，尼姆佐维奇是倡导兵的结构和价值的先驱之一。最后，我们提出了未来研究的潜在方向。

    Valuing chess squares and determining the placement of pieces on the board are the main objectives of our study. With the emergence of chess AI, it has become possible to accurately assess the worth of positions in a game of chess. The conventional approach assigns fixed values to pieces $(\symking=\infty, \symqueen=9, \symrook=5, \symbishop=3, \symknight=3, \sympawn=1)$. We enhance this analysis by introducing marginal valuations for both pieces and squares. We demonstrate our method by examining the positioning of Knights and Bishops, and also provide valuable insights into the valuation of pawns. Notably, Nimzowitsch was among the pioneers in advocating for the significance of Pawn structure and valuation. Finally, we conclude by suggesting potential avenues for future research.
    
[^175]: 使用扩散模型进行蒸馏和重播的增量学习

    Class-Incremental Learning using Diffusion Model for Distillation and Replay. (arXiv:2306.17560v1 [cs.LG])

    [http://arxiv.org/abs/2306.17560](http://arxiv.org/abs/2306.17560)

    本文提出了一种使用预训练的扩散模型作为增量学习的附加数据源的方法，通过生成属于先前遇到的图像所属类别的合成样本，并在蒸馏损失和分类损失中使用这些样本，进一步提高了模型的性能。

    

    增量学习旨在以增量的方式学习新类别，而不会忘记先前学习的类别。多个研究表明，增量模型可以利用附加数据来帮助减轻灾难性遗忘。本文提出了使用预训练的稳定扩散模型作为增量学习的附加数据源。与依赖于外部、通常是无标签的真实图像数据集的竞争方法相比，我们的方法可以生成属于先前遇到的图像所属类别的合成样本。这使我们不仅可以在蒸馏损失中使用这些附加数据样本，还可以在分类损失中进行重播。在CIFAR100、ImageNet-Subset和ImageNet等竞争基准上的实验表明，这种新方法可以进一步提高模型的性能。

    Class-incremental learning aims to learn new classes in an incremental fashion without forgetting the previously learned ones. Several research works have shown how additional data can be used by incremental models to help mitigate catastrophic forgetting. In this work, following the recent breakthrough in text-to-image generative models and their wide distribution, we propose the use of a pretrained Stable Diffusion model as a source of additional data for class-incremental learning. Compared to competitive methods that rely on external, often unlabeled, datasets of real images, our approach can generate synthetic samples belonging to the same classes as the previously encountered images. This allows us to use those additional data samples not only in the distillation loss but also for replay in the classification loss. Experiments on the competitive benchmarks CIFAR100, ImageNet-Subset, and ImageNet demonstrate how this new approach can be used to further improve the performance of s
    
[^176]: ELM神经元：一种高效且表达力强的皮层神经元模型可以解决长时间跨度任务

    The ELM Neuron: an Efficient and Expressive Cortical Neuron Model Can Solve Long-Horizon Tasks. (arXiv:2306.16922v1 [cs.NE])

    [http://arxiv.org/abs/2306.16922](http://arxiv.org/abs/2306.16922)

    ELM神经元是一种高效且表达力强的皮层神经元模型，它只需要8K个参数就能准确模拟复杂的计算任务。

    

    传统的大规模神经科学模型和机器学习利用简化的个体神经元模型，依靠集体活动和适当调整的连接来执行复杂的计算。然而，每个生物皮层神经元本质上都是一个复杂的计算设备，最近的一项研究证实了这一点，该研究中，需要一个具有数百万个参数的深度人工神经网络来复制详细生物物理模型的输入输出关系。我们对这些多个参数的必要性提出了质疑，并引入了表达力强的泄漏存储器（ELM）神经元，这是一种受生物启发的计算模型，具有高计算表达力，同时也非常高效。值得注意的是，我们的ELM神经元仅需要8,000个可训练参数就能准确匹配前述的输入输出关系。我们发现，准确的模型需要多个类似于存储器的隐藏状态和复杂的非线性突触整合。

    Traditional large-scale neuroscience models and machine learning utilize simplified models of individual neurons, relying on collective activity and properly adjusted connections to perform complex computations. However, each biological cortical neuron is inherently a sophisticated computational device, as corroborated in a recent study where it took a deep artificial neural network with millions of parameters to replicate the input-output relationship of a detailed biophysical model of a cortical pyramidal neuron. We question the necessity for these many parameters and introduce the Expressive Leaky Memory (ELM) neuron, a biologically inspired, computationally expressive, yet efficient model of a cortical neuron. Remarkably, our ELM neuron requires only 8K trainable parameters to match the aforementioned input-output relationship accurately. We find that an accurate model necessitates multiple memory-like hidden states and intricate nonlinear synaptic integration. To assess the comput
    
[^177]: 稀疏模型汤：通过模型平均改进修剪的方法

    Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging. (arXiv:2306.16788v1 [cs.LG])

    [http://arxiv.org/abs/2306.16788](http://arxiv.org/abs/2306.16788)

    本研究通过将多个经过迭代幅度剪枝的模型进行平均，解决了同时利用稀疏性和参数平均的问题，并显著提升了泛化性能。

    

    神经网络可以通过剪枝显著压缩，从而得到稀疏模型，这些模型需要更少的存储和浮点运算，同时保持预测性能。模型汤（Wortsman等人，2022年）通过将多个模型的参数平均成一个单一模型来改善泛化和超出分布性能，而不增加推理时间。然而，识别处于相同损失区域的模型以同时利用稀疏性和参数平均是具有挑战性的，因为对任意稀疏模型进行平均会降低整体稀疏度，原因是不同的稀疏连接性。在这项工作中，我们通过展示在迭代幅度剪枝（IMP）的单次重新训练阶段中探索不同的超参数配置（例如批次排序或权重衰减）产生的模型适合进行平均，并且通过设计共享相同的稀疏连接性来解决这些挑战。平均这些模型显著提升了泛化性能。

    Neural networks can be significantly compressed by pruning, leading to sparse models requiring considerably less storage and floating-point operations while maintaining predictive performance. Model soups (Wortsman et al., 2022) improve generalization and out-of-distribution performance by averaging the parameters of multiple models into a single one without increased inference time. However, identifying models in the same loss basin to leverage both sparsity and parameter averaging is challenging, as averaging arbitrary sparse models reduces the overall sparsity due to differing sparse connectivities. In this work, we address these challenges by demonstrating that exploring a single retraining phase of Iterative Magnitude Pruning (IMP) with varying hyperparameter configurations, such as batch ordering or weight decay, produces models that are suitable for averaging and share the same sparse connectivity by design. Averaging these models significantly enhances generalization performanc
    
[^178]: RL$^3$:通过RL内部的RL$^2$提升元强化学习方法

    RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$. (arXiv:2306.15909v1 [cs.LG])

    [http://arxiv.org/abs/2306.15909](http://arxiv.org/abs/2306.15909)

    RL$^3$是一种原则性混合方法，通过将传统强化学习学到的任务特定动作值作为元强化学习神经网络的输入，提高了元强化学习的性能。

    

    元强化学习（meta-RL）方法，如RL$^2$，已经成为学习针对给定任务分布的数据高效的强化学习算法的有希望的方法。然而，这些强化学习算法在长期任务和超出分布任务方面存在困难，因为它们依赖于递归神经网络来处理经验序列，而不是将它们总结为一般的强化学习组件，例如价值函数。此外，即使是transformers在训练和推理成本变得禁止之前也对它们可以有效推理的历史长度有实际限制。相比之下，传统的强化学习算法在数据效率方面不足，因为它们没有利用领域知识，但随着更多数据的可用性，它们会收敛到最优策略。在本文中，我们提出了RL$^3$，一种组合了传统强化学习和元强化学习的原则性混合方法，通过将通过传统强化学习学习到的特定任务动作值作为元强化学习神经网络的一个输入。

    Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as promising approaches for learning data-efficient RL algorithms tailored to a given task distribution. However, these RL algorithms struggle with long-horizon tasks and out-of-distribution tasks since they rely on recurrent neural networks to process the sequence of experiences instead of summarizing them into general RL components such as value functions. Moreover, even transformers have a practical limit to the length of histories they can efficiently reason about before training and inference costs become prohibitive. In contrast, traditional RL algorithms are data-inefficient since they do not leverage domain knowledge, but they do converge to an optimal policy as more data becomes available. In this paper, we propose RL$^3$, a principled hybrid approach that combines traditional RL and meta-RL by incorporating task-specific action-values learned through traditional RL as an input to the meta-RL neural netw
    
[^179]: 图神经网络的广义$f$-均值聚合

    Generalised $f$-Mean Aggregation for Graph Neural Networks. (arXiv:2306.13826v1 [cs.LG])

    [http://arxiv.org/abs/2306.13826](http://arxiv.org/abs/2306.13826)

    本文提出了一个广义聚合算子，GenAgg，它包括所有标准聚合器的函数空间。实验结果表明，GenAgg能够表示标准聚合器。

    

    图神经网络(GNN)架构由其更新和聚合模块的实现方式定义。许多工作都集中在新型参数化更新模块的方法上，而聚合模块相对较少受到关注。由于聚合函数很难参数化，目前大多数方法选择“标准聚合器”，如$\mathrm{mean}$、$\mathrm{sum}$或$\mathrm{max}$。尽管这种选择通常没有任何理由，但已经表明聚合器的选择对性能有重大影响，最佳聚合器的选择取决于问题。由于聚合是一种有损操作，选择最适合的聚合器以最小化信息丢失至关重要。本文提出了GenAgg，一种广义聚合运算符，它参数化了一个包括所有标准聚合器的函数空间。在我们的实验中，我们展示了GenAgg能够表示标准聚合器。

    Graph Neural Network (GNN) architectures are defined by their implementations of update and aggregation modules. While many works focus on new ways to parametrise the update modules, the aggregation modules receive comparatively little attention. Because it is difficult to parametrise aggregation functions, currently most methods select a "standard aggregator" such as $\mathrm{mean}$, $\mathrm{sum}$, or $\mathrm{max}$. While this selection is often made without any reasoning, it has been shown that the choice in aggregator has a significant impact on performance, and the best choice in aggregator is problem-dependent. Since aggregation is a lossy operation, it is crucial to select the most appropriate aggregator in order to minimise information loss. In this paper, we present GenAgg, a generalised aggregation operator, which parametrises a function space that includes all standard aggregators. In our experiments, we show that GenAgg is able to represent the standard aggregators with mu
    
[^180]: 人工智能灾难性风险综述

    An Overview of Catastrophic AI Risks. (arXiv:2306.12001v1 [cs.CY])

    [http://arxiv.org/abs/2306.12001](http://arxiv.org/abs/2306.12001)

    本文综述了人工智能灾难性风险的四个主要来源，包括恶意使用、人工智能竞赛、组织风险和流氓人工智能。

    

    人工智能的快速发展引起了专家、政策制定者和世界各国领导人对越来越先进的人工智能系统可能带来灾难性风险的担忧。虽然已经有很多风险被单独详细介绍过，但迫切需要系统地讨论和说明潜在危险，以更好地支持减轻这些风险的努力。本文概述了人工智能灾难性风险的主要来源，我们将其分为四个类别：恶意使用，即个人或团体有意使用人工智能造成伤害；人工智能竞赛，即竞争环境促使行动者部署不安全的人工智能或放弃控制权交给人工智能；组织风险，突出人为和复杂系统如何增加灾难性事故发生的可能性；以及流氓人工智能，描述了控制比人类智能更高的代理程序困难的固有难题。对于每个风险类别，我们描述了具体的危害。

    Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards,
    
[^181]: 通过边界的角度理解对比学习

    Understanding Contrastive Learning Through the Lens of Margins. (arXiv:2306.11526v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.11526](http://arxiv.org/abs/2306.11526)

    这项研究提出了一种新的视角来理解边界在对比学习中的作用，并从梯度分析的角度分析了边界如何影响对比学习的效果。

    

    对比学习及其变体是一种在不同领域中高效的自监督学习方法。对比学习使用余弦相似度来度量表示之间的距离，并利用交叉熵进行表示学习。在基于余弦相似度的表示学习框架中，边界在增强人脸和说话人识别任务方面起着重要作用。有趣的是，尽管对比学习和边界依赖于相同的相似度度量和目标函数，但对比学习并没有积极采用边界。此外，只有基于决策边界的解释才被用来解释边界在对比学习中的作用。在这项工作中，我们提出了一个新的角度来理解边界在梯度分析上的作用。基于这个新的角度，我们分析了边界如何影响对比学习的梯度，并将其效果分解成更基本的层次。

    Contrastive learning, along with its variations, has been a highly effective self-supervised learning method across diverse domains. Contrastive learning measures the distance between representations using cosine similarity and uses cross-entropy for representation learning. Within the same framework of cosine-similarity-based representation learning, margins have played a significant role in enhancing face and speaker recognition tasks. Interestingly, despite the shared reliance on the same similarity metrics and objective functions, contrastive learning has not actively adopted margins. Furthermore, decision-boundary-based explanations are the only ones that have been used to explain the effect of margins in contrastive learning. In this work, we propose a new perspective to understand the role of margins based on gradient analysis. Based on the new perspective, we analyze how margins affect gradients of contrastive learning and separate the effect into more elemental levels. We sepa
    
[^182]: 自上而下的机器学习用于粗粒化蛋白质力场

    Top-down machine learning of coarse-grained protein force-fields. (arXiv:2306.11375v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2306.11375](http://arxiv.org/abs/2306.11375)

    通过分子动力学模拟和可微分轨迹重加权训练神经网络势能，实现了自上而下的粗粒化蛋白质力场建模，仅需蛋白质的天然构象即可展示其外推能力。

    

    开发准确和高效的蛋白质粗粒化表征对于理解它们的折叠、功能和在长时间尺度下的相互作用至关重要。我们的方法包括通过分子动力学模拟蛋白质，并利用得到的轨迹通过可微分轨迹重加权来训练神经网络势能。令人惊讶的是，该方法仅需要蛋白质的天然构象，消除了从广泛模拟或内存密集型端到端可微分模拟导出标记数据的需求。一旦训练完成，模型可以用于并行分子动力学模拟，并对训练分布内外的蛋白质进行折叠事件采样，展示其外推能力。通过应用马尔可夫状态模型，可以预测模拟蛋白质的与天然构象相似的构象。由于其理论可转移性和仅使用蛋白质的天然构象的能力，可以在不同尺度上应用该方法。

    Developing accurate and efficient coarse-grained representations of proteins is crucial for understanding their folding, function, and interactions over extended timescales. Our methodology involves simulating proteins with molecular dynamics and utilizing the resulting trajectories to train a neural network potential through differentiable trajectory reweighting. Remarkably, this method requires only the native conformation of proteins, eliminating the need for labeled data derived from extensive simulations or memory-intensive end-to-end differentiable simulations. Once trained, the model can be employed to run parallel molecular dynamics simulations and sample folding events for proteins both within and beyond the training distribution, showcasing its extrapolation capabilities. By applying Markov State Models, native-like conformations of the simulated proteins can be predicted from the coarse-grained simulations. Owing to its theoretical transferability and ability to use solely e
    
[^183]: 基于深度误差因素与总体高分子信号估计的磁共振波谱定量技术

    Magnetic Resonance Spectroscopy Quantification Aided by Deep Estimations of Imperfection Factors and Overall Macromolecular Signal. (arXiv:2306.09681v1 [physics.med-ph])

    [http://arxiv.org/abs/2306.09681](http://arxiv.org/abs/2306.09681)

    本论文通过深度学习方法提高了磁共振波谱技术中代谢物信号定量精度和稳定性。

    

    磁共振波谱（MRS）是一种非侵入性的生物医学检测技术，由于代谢物信号的严重重叠、非理想采集条件引起的信号失真以及与强背景信号包括高分子信号的干扰，使用质子MRS精确量化代谢物仍然存在挑战。在本研究中，将深度学习引入到MRS量化中，通过引入正则化项、非理想采集条件的误差因素以及设计几种经验先验如诸如代谢物和高分子的基集，进一步提高了方法的精度和稳定性。

    Magnetic Resonance Spectroscopy (MRS) is an important non-invasive technique for in vivo biomedical detection. However, it is still challenging to accurately quantify metabolites with proton MRS due to three problems: Serious overlaps of metabolite signals, signal distortions due to non-ideal acquisition conditions and interference with strong background signals including macromolecule signals. The most popular software, LCModel, adopts the non-linear least square to quantify metabolites and addresses these problems by introducing regularization terms, imperfection factors of non-ideal acquisition conditions, and designing several empirical priors such as basissets of both metabolites and macromolecules. However, solving such a large non-linear quantitative problem is complicated. Moreover, when the signal-to-noise ratio of an input MRS signal is low, the solution may have a large deviation. In this work, deep learning is introduced to reduce the complexity of solving this overall quan
    
[^184]: 利用纵向胸部X光和报告预填充放射学报告

    Utilizing Longitudinal Chest X-Rays and Reports to Pre-Fill Radiology Reports. (arXiv:2306.08749v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.08749](http://arxiv.org/abs/2306.08749)

    本文提出了利用纵向胸部X光和报告来预填充放射学报告的方法，以减轻报告错误。通过利用纵向多模态数据和基于transformer的模型，可以更好地捕获包含多模态纵向就诊记录的信息。

    

    尽管使用语音识别软件可以缩短放射学报告的完成时间，但持续的沟通错误会对放射学报告的解释产生重大影响。预填充放射学报告在减轻报告错误方面具有潜力，尽管文献中存在生成医学报告的努力，但对于利用MIMIC-CXR数据集中的患者就诊记录的纵向特性缺乏方法。为了解决这一差距，我们提出使用纵向多模态数据，即以前的患者就诊CXR、当前就诊CXR和以前的就诊报告，来预先填充当前患者就诊报告的“发现”部分。我们首先从MIMIC-CXR数据集中收集了26,625名患者的纵向就诊信息，并创建了一个名为Longitudinal-MIMIC的新数据集。利用这个新数据集，我们训练了一个基于transformer的模型，以捕获包含多模态纵向就诊记录的信息。

    Despite the reduction in turn-around times in radiology reports with the use of speech recognition software, persistent communication errors can significantly impact the interpretation of the radiology report. Pre-filling a radiology report holds promise in mitigating reporting errors, and despite efforts in the literature to generate medical reports, there exists a lack of approaches that exploit the longitudinal nature of patient visit records in the MIMIC-CXR dataset. To address this gap, we propose to use longitudinal multi-modal data, i.e., previous patient visit CXR, current visit CXR, and previous visit report, to pre-fill the 'findings' section of a current patient visit report. We first gathered the longitudinal visit information for 26,625 patients from the MIMIC-CXR dataset and created a new dataset called Longitudinal-MIMIC. With this new dataset, a transformer-based model was trained to capture the information from longitudinal patient visit records containing multi-modal 
    
[^185]: Strokes2Surface：从四维建筑设计素描中恢复曲线网络

    Strokes2Surface: Recovering Curve Networks From 4D Architectural Design Sketches. (arXiv:2306.07220v2 [cs.GR] UPDATED)

    [http://arxiv.org/abs/2306.07220](http://arxiv.org/abs/2306.07220)

    本文介绍了Strokes2Surface，它可从建筑师的笔画中恢复出曲线网络，对于建筑设计中的概念设计和数字建模之间的桥梁具有重要意义。

    

    本文介绍了一个离线几何重建管道Strokes2Surface，它是基于4D Sketching Interface，MR.Sketch的目标是面向建筑设计的。该管道从设计师绘制的笔画中恢复曲线网络，因此在建筑设计的概念设计和数字建模阶段之间建立了桥梁。我们的管道的输入包括3D笔画的折线顶点及其相应的时间戳（作为第四个维度），以及额外的几何和笔触相关的记录属性。基于素描合并和基于素描建模方法的启发，我们的管道利用这些数据并组合三个机器学习（ML）模型；一个分类器和两个聚类模型。特别是，根据建筑设计素描中设计师通常采用的实践观察，我们解决了一个二元分类问题，以识别一笔画是描绘边界和边缘还是用于填充所需建筑物的封闭区域和表面。

    We present Strokes2Surface, an offline geometry-reconstruction pipeline built upon a 4D Sketching Interface, MR.Sketch, targeted at architectural design. The pipeline recovers a curve network from designer-drawn strokes, thus bridging between concept design and digital modeling stages in architectural design. The input to our pipeline consists of 3D strokes' polyline vertices and their corresponding timestamps (as of the fourth dimension), along with additional geometric and stylus-related recorded properties. Inspired by sketch consolidation and sketch-based modeling methods, our pipeline leverages such data and combines three Machine Learning (ML) models; a classifier and two clustering models. In particular, based on observations of practices designers typically employ in architectural design sketches, we solve a binary classification problem to recognize whether a stroke depicts a boundary and edge or is used to fill in the enclosing areas and faces of the intended architectural ob
    
[^186]: 变分不平衡回归(Variational Imbalanced Regression)

    Variational Imbalanced Regression. (arXiv:2306.06599v1 [cs.LG])

    [http://arxiv.org/abs/2306.06599](http://arxiv.org/abs/2306.06599)

    本文提出的变分不平衡回归（VIR）模型通过引入Probabilistic Reweighting方法，可以在不平衡回归方面表现良好，并自然产生合理的不确定性估计。

    

    当标签分布不平衡时，现有的回归模型往往在准确性和不确定性估计方面表现不佳。本文提出了一种概率深度学习模型——变分不平衡回归（VIR），它不仅在不平衡回归方面表现出色，而且自然地产生合理的不确定性估计。与典型的变分自编码器假设I.I.D.表示（数据点的表示不直接受其他数据点的影响）不同，我们的VIR借用具有类似回归标签的数据来计算潜在表示的变分分布；此外，不同于产生点估计的确定性回归模型， VIR预测整个正态反-伽玛分布并调节相关联的共轭分布，对不平衡数据施加概率重新加权，从而提供更好的不确定性估计。在几个真实世界的数据集上进行了实验。

    Existing regression models tend to fall short in both accuracy and uncertainty estimation when the label distribution is imbalanced. In this paper, we propose a probabilistic deep learning model, dubbed variational imbalanced regression (VIR), which not only performs well in imbalanced regression but naturally produces reasonable uncertainty estimation as a byproduct. Different from typical variational autoencoders assuming I.I.D. representations (a data point's representation is not directly affected by other data points), our VIR borrows data with similar regression labels to compute the latent representation's variational distribution; furthermore, different from deterministic regression models producing point estimates, VIR predicts the entire normal-inverse-gamma distributions and modulates the associated conjugate distributions to impose probabilistic reweighting on the imbalanced data, thereby providing better uncertainty estimation. Experiments in several real-world datasets sh
    
[^187]: DiffusionShield：一种针对生成式扩散模型的版权保护数字水印方法

    DiffusionShield: A Watermark for Copyright Protection against Generative Diffusion Models. (arXiv:2306.04642v1 [cs.CR])

    [http://arxiv.org/abs/2306.04642](http://arxiv.org/abs/2306.04642)

    本文提出了一种针对生成式扩散模型的版权保护数字水印方法DiffusionShield，保护图像免受侵权，简单易学，难以检测，可广泛应用于各行各业。

    

    近来，生成式扩散模型(GDMs)在学习和生成图像方面展示了其卓越的能力。大量的GDMs社区自然而然地出现，并促进了GDMs在各个领域的多样化应用。然而，这种无限制的扩散引起了有关版权保护的严重关注。例如，艺术家(包括画家和摄影师)越来越担心GDMs可以毫不费力地复制他们独特的创意作品而无需授权。为了应对这些挑战，我们介绍了一种针对GDMs量身定制的新型水印方案——DiffusionShield。通过将所有权信息编码成一个不可察觉的水印并将其注入图像中，DiffusionShield保护图像免受GDMs侵权。它的水印可以被GDMs轻松地学习并重现在它们生成的图像中。通过从生成的图像中检测水印，可以揭露版权侵权行为。

    Recently, Generative Diffusion Models (GDMs) have showcased their remarkable capabilities in learning and generating images. A large community of GDMs has naturally emerged, further promoting the diversified applications of GDMs in various fields. However, this unrestricted proliferation has raised serious concerns about copyright protection. For example, artists including painters and photographers are becoming increasingly concerned that GDMs could effortlessly replicate their unique creative works without authorization. In response to these challenges, we introduce a novel watermarking scheme, DiffusionShield, tailored for GDMs. DiffusionShield protects images from copyright infringement by GDMs through encoding the ownership information into an imperceptible watermark and injecting it into the images. Its watermark can be easily learned by GDMs and will be reproduced in their generated images. By detecting the watermark from generated images, copyright infringement can be exposed w
    
[^188]: L-C2ST: 基于本地诊断实现模拟推断中后验近似

    L-C2ST: Local Diagnostics for Posterior Approximations in Simulation-Based Inference. (arXiv:2306.03580v1 [stat.ML])

    [http://arxiv.org/abs/2306.03580](http://arxiv.org/abs/2306.03580)

    本文提出了一种名为 L-C2ST 的基于本地诊断实现模拟推断中后验近似的新方法，其可以在任何给定的观测下本地评估后验估计器，有效地解决了目前评估后验估计器限制解决方法的问题。

    

    最近许多模拟推断（SBI）的工作都依赖于深度生成模型来近似复杂、高维度的后验分布。然而，评估这些近似是否可信仍是一个挑战。大多数方法仅在观测空间期望下评估后验估计器。这限制了它们的可解释性，并不能足够地确定哪些观测结果可以信任这些近似或应该改进。我们基于著名的分类器两样本检验 (C2ST)，引入 L-C2ST，一个新方法，允许在任何给定的观测下本地评估后验估计器。它提供有理论基础和易于解释的，如图示诊断。与 C2ST 不同的是，L-C2ST 不需要访问真实后验的样本。对于基于归一化流的后验估计器，L-C2ST 可以专门提供更好的统计功率，同时计算效率更高。

    Many recent works in simulation-based inference (SBI) rely on deep generative models to approximate complex, high-dimensional posterior distributions. However, evaluating whether or not these approximations can be trusted remains a challenge. Most approaches evaluate the posterior estimator only in expectation over the observation space. This limits their interpretability and is not sufficient to identify for which observations the approximation can be trusted or should be improved. Building upon the well-known classifier two-sample test (C2ST), we introduce L-C2ST, a new method that allows for a local evaluation of the posterior estimator at any given observation. It offers theoretically grounded and easy to interpret - e.g. graphical - diagnostics, and unlike C2ST, does not require access to samples from the true posterior. In the case of normalizing flow-based posterior estimators, L-C2ST can be specialized to offer better statistical power, while being computationally more efficien
    
[^189]: 基于图形的方法与特定分布距离相结合的对抗攻击检测

    Graph-based methods coupled with specific distributional distances for adversarial attack detection. (arXiv:2306.00042v1 [cs.LG])

    [http://arxiv.org/abs/2306.00042](http://arxiv.org/abs/2306.00042)

    本论文提出了利用图形的方法结合特定分布距离来检测对抗性攻击，通过研究神经网络的图结构，介绍了用于预测和解释对抗性攻击的特定测量方法，这有助于研究对抗性攻击的内在机理。

    

    人工神经网络容易被精心扰动的输入所欺骗，导致严重的误分类。这些“对抗性”攻击已成为广泛研究的焦点。同样，对抗攻击的检测和防御也有大量研究。我们从图的角度介绍了一种新的方法来检测和解释对抗性攻击。对于图像，无论是良性的还是对抗性的，我们研究了神经网络的架构如何引入一个相关的图形。我们研究了这个图形，并引入了用于预测和解释对抗性攻击的特定测量方法。我们展示了基于图形的方法有助于研究对抗性攻击的内在机理。

    Artificial neural networks are prone to being fooled by carefully perturbed inputs which cause an egregious misclassification. These \textit{adversarial} attacks have been the focus of extensive research. Likewise, there has been an abundance of research in ways to detect and defend against them. We introduce a novel approach of detection and interpretation of adversarial attacks from a graph perspective. For an image, benign or adversarial, we study how a neural network's architecture can induce an associated graph. We study this graph and introduce specific measures used to predict and interpret adversarial attacks. We show that graphs-based approaches help to investigate the inner workings of adversarial attacks.
    
[^190]: In-Context Learning学习了什么以及如何学习？贝叶斯模型平均、参数化和泛化。

    What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization. (arXiv:2305.19420v1 [stat.ML])

    [http://arxiv.org/abs/2305.19420](http://arxiv.org/abs/2305.19420)

    本文对In-Context Learning进行了全面的研究，通过贝叶斯模型平均算法来隐式地实现ICL估计量，并采用在线学习的角度来分析ICL性能，建立后悔界限，并通过关注机制近似参数化。

    

    本文通过回答几个开放性问题，对In-Context Learning（ICL）进行了全面的研究：(a)在语言模型中学习的是什么类型的ICL估计量？(b)适合评估ICL的性能度量是什么，并且错误率是多少？(c)Transformer架构如何实现ICL？为了回答(a)，我们采取了贝叶斯观点，并证明ICL隐含实现了贝叶斯模型平均算法。我们证明了这个贝叶斯模型平均算法可以通过关注机制近似参数化。(b)从在线学习的角度分析ICL性能，建立一个后悔界限 $\mathcal{O}(1/T)$，其中$T$是ICL输入序列长度。(c)除了在关注中编码的贝叶斯模型平均算法，我们还表明，在涉及期间，学习模型和名义模型之间的总变分距离被一个近似误差和一个泛化误差之和所界定。

    In this paper, we conduct a comprehensive study of In-Context Learning (ICL) by addressing several open questions: (a) What type of ICL estimator is learned within language models? (b) What are suitable performance metrics to evaluate ICL accurately and what are the error rates? (c) How does the transformer architecture enable ICL? To answer (a), we take a Bayesian view and demonstrate that ICL implicitly implements the Bayesian model averaging algorithm. This Bayesian model averaging algorithm is proven to be approximately parameterized by the attention mechanism. For (b), we analyze the ICL performance from an online learning perspective and establish a regret bound $\mathcal{O}(1/T)$, where $T$ is the ICL input sequence length. To address (c), in addition to the encoded Bayesian model averaging algorithm in attention, we show that during pertaining, the total variation distance between the learned model and the nominal model is bounded by a sum of an approximation error and a genera
    
[^191]: MiniSUPERB:轻量级自监督语音模型基准测试

    MiniSUPERB: Lightweight Benchmark for Self-supervised Speech Models. (arXiv:2305.19011v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2305.19011](http://arxiv.org/abs/2305.19011)

    这篇论文提出了一个名为MiniSUPERB的轻量级基准测试，可以有效地评估自监督语音模型的性能，并在计算成本上比SUPERB更低。同时，该论文还研究了在少样本情况下评估SSL语音模型的性能变化。

    

    SUPERB被提出用于评估自监督学习（SSL）语音模型在各种任务上的泛化能力。然而，由于大型数据集和多样化任务，它导致了高计算成本。在本文中，我们引入了MiniSUPERB，一个轻量级基准测试，它以明显更低的计算成本有效地评估SSL语音模型并且结果可与SUPERB相比。我们精选代表性任务，采样数据集，并离线提取模型表示。我们的方法与SUPERB Paper和SUPERB Challenge分别达到0.954和0.982的斯皮尔曼等级相关系数。此外，我们在乘-累积操作（MACs）方面减少了97％的计算成本。此外，我们在少样本情况下评估SSL语音模型，并观察到其性能有显著变化。据我们所知，这是第一项研究同时考虑模型本身的计算成本和在基准测试上评估的成本。

    SUPERB was proposed to evaluate the generalizability of self-supervised learning (SSL) speech models across various tasks. However, it incurs high computational costs due to the large datasets and diverse tasks. In this paper, we introduce MiniSUPERB, a lightweight benchmark that efficiently evaluates SSL speech models with comparable results to SUPERB but lower computational costs significantly. We carefully select representative tasks, sample datasets, and extract model representations offline. Our approach achieves a Spearman's rank correlation of 0.954 and 0.982 with SUPERB Paper and SUPERB Challenge, respectively. Additionally, we reduce the computational cost by 97% in terms of Multiply-ACcumulate operations (MACs). Furthermore, we evaluate SSL speech models in few-shot scenarios and observe significant variations in their performance. To our knowledge, this is the first study to examine both the computational cost of the model itself and the cost of evaluating it on a benchmark.
    
[^192]: 扩散模型是多任务强化学习的有效规划器和数据合成器

    Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning. (arXiv:2305.18459v1 [cs.LG])

    [http://arxiv.org/abs/2305.18459](http://arxiv.org/abs/2305.18459)

    本文研究了单一扩散模型在建模多个任务的策略训练的大规模离线数据时的有效性。该方法名为Multi-Task Diffusion Model (MTDiff)，结合了Transformer骨干和提示学习，能够在如此复杂的多任务环境下取得相当不错的性能。

    

    扩散模型在视觉和NLP领域中展现出了高度表现力的生成能力。最近的研究表明，在强化学习领域中，扩散模型也能够有效地建模离线数据集中的复杂策略或轨迹。然而，这些研究仅限于单任务设置，没有考虑多任务的情况。本文旨在研究单一扩散模型在建模用于多个任务策略训练的大规模离线数据时的有效性，具有多样化和多模态的数据分布。具体而言，我们提出了Multi-Task Diffusion Model（MTDiff），这是一种基于扩散的方法，结合Transformer骨干和提示学习，用于多任务离线设置中的生成规划和数据合成。MTDiff利用大量可用于多任务数据中的知识，并在任务之间执行隐式知识共享以进行虚拟规划。

    Diffusion models have demonstrated highly-expressive generative capabilities in vision and NLP. Recent studies in reinforcement learning (RL) have shown that diffusion models are also powerful in modeling complex policies or trajectories in offline datasets. However, these works have been limited to single-task settings where a generalist agent capable of addressing multi-task predicaments is absent. In this paper, we aim to investigate the effectiveness of a single diffusion model in modeling large-scale multi-task offline data, which can be challenging due to diverse and multimodal data distribution. Specifically, we propose Multi-Task Diffusion Model (\textsc{MTDiff}), a diffusion-based method that incorporates Transformer backbones and prompt learning for generative planning and data synthesis in multi-task offline settings. \textsc{MTDiff} leverages vast amounts of knowledge available in multi-task data and performs implicit knowledge sharing among tasks. For generative planning, 
    
[^193]: 零样本任务偏好的不精确贝叶斯继续学习

    Zero-shot Task Preference Addressing Enabled by Imprecise Bayesian Continual Learning. (arXiv:2305.14782v1 [cs.LG])

    [http://arxiv.org/abs/2305.14782](http://arxiv.org/abs/2305.14782)

    提出了零样本任务偏好的不精确贝叶斯继续学习（IBCL）算法，该算法更新模型参数分布凸壳形式的知识库，并使用零样本获取模型以满足不同的偏好，使得在具有大量任务偏好的情况下更加可扩展。

    

    类似于通用的多任务学习，继续学习也具有多目标优化的特性，因此需要在不同任务的性能之间进行平衡。也就是说，为了优化当前任务分布，可能需要在一些任务上牺牲性能以提高其他任务的性能。这意味着存在多个模型，每个模型在不同的时间都是最优的，每个模型都能够解决不同的任务-性能权衡问题。研究人员已经讨论如何训练特定的模型以满足交易偏好。然而，现有的算法需要额外的采样开销-在存在多个，可能是无限数量的偏好时会产生很大的负担。因此，我们提出了不精确贝叶斯继续学习（IBCL）。一旦有新任务，IBCL会（1）更新一个以模型参数分布凸壳形式存在的知识库，（2）并使用零样本获取特定模型以满足不同的偏好。也就是说，IBCL不需要任何额外的数据就能为一个特定的任务偏好生成新的模型，使得在具有大量任务偏好的情况下更加可扩展。

    Like generic multi-task learning, continual learning has the nature of multi-objective optimization, and therefore faces a trade-off between the performance of different tasks. That is, to optimize for the current task distribution, it may need to compromise performance on some tasks to improve on others. This means there exist multiple models that are each optimal at different times, each addressing a distinct task-performance trade-off. Researchers have discussed how to train particular models to address specific preferences on these trade-offs. However, existing algorithms require additional sample overheads -- a large burden when there are multiple, possibly infinitely many, preferences. As a response, we propose Imprecise Bayesian Continual Learning (IBCL). Upon a new task, IBCL (1) updates a knowledge base in the form of a convex hull of model parameter distributions and (2) obtains particular models to address preferences with zero-shot. That is, IBCL does not require any additi
    
[^194]: 通过伪神经切线核代理模型提供深度神经网络的鲁棒性解释

    Robust Explanations for Deep Neural Networks via Pseudo Neural Tangent Kernel Surrogate Models. (arXiv:2305.14585v1 [cs.LG])

    [http://arxiv.org/abs/2305.14585](http://arxiv.org/abs/2305.14585)

    本研究通过建立一个规范化的伪神经切线核，证明了它能够更好地与神经网络决策函数相关，比基于嵌入和影响的替代品更有效，并且从它创建的归因会更准确地选择被扰动的训练数据，从而证明了核线性模型是跨多个数据领域并有效的替代模型。

    

    最近，通过数据归属任务，解释型AI的进步之一是通过解释示例策略实现的。然而，用于将决策归因于训练数据的特征空间，尚未相互比较，以确定它们是否形成神经网络(NN)的真正代理模型。在这里，我们通过两种方式证明了线性特征空间对神经网络的有效性：(1)我们建立了一个规范化的伪神经切线核(pNTK)，它在计算机视觉和大语言模型架构中与神经网络决策函数更相关，比基于嵌入和影响的替代品更为有效；(2)我们展示了从规范化pNTK创建的归因比这些替代品更准确地选择被扰动的训练数据。基于这些观察结果，我们得出结论，核线性模型是跨多个数据领域并有效的替代模型。

    One of the ways recent progress has been made on explainable AI has been via explain-by-example strategies, specifically, through data attribution tasks. The feature spaces used to attribute decisions to training data, however, have not been compared against one another as to whether they form a truly representative surrogate model of the neural network (NN). Here, we demonstrate the efficacy of surrogate linear feature spaces to neural networks through two means: (1) we establish that a normalized psuedo neural tangent kernel (pNTK) is more correlated to the neural network decision functions than embedding based and influence based alternatives in both computer vision and large language model architectures; (2) we show that the attributions created from the normalized pNTK more accurately select perturbed training data in a data poisoning attribution task than these alternatives. Based on these observations, we conclude that kernel linear models are effective surrogate models across m
    
[^195]: Sophia：一种用于语言模型预训练的可扩展的随机二阶优化器

    Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training. (arXiv:2305.14342v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.14342](http://arxiv.org/abs/2305.14342)

    Sophia是一种用于语言模型预训练的可扩展的二阶优化算法，使用对角Hessian作为预调节器，并进行元素级别的裁剪控制更新大小。

    

    鉴于语言模型预训练的巨大成本，优化算法的微小改进将会大大降低训练的时间和成本。Adam及其变种一直是最先进的，而更复杂的二阶（基于Hessian的）优化器往往会带来太多的每步开销。在这篇论文中，我们提出了Sophia，一种简单可扩展的二阶优化器，它使用轻量级估计的对角Hessian作为预调节器。更新步骤是梯度的移动平均值除以估计Hessian的移动平均值，然后进行元素级别的裁剪。裁剪控制了最坏情况下的更新大小，并控制了Hessian在轨迹上的非凸性和快速变化的负面影响。Sophia只在每几次迭代中估计对角Hessian，这几乎没有平均每步的时间和内存开销。在使用GPT m进行语言建模时，

    Given the massive cost of language model pre-training, a non-trivial improvement of the optimization algorithm would lead to a material reduction on the time and cost of training. Adam and its variants have been state-of-the-art for years, and more sophisticated second-order (Hessian-based) optimizers often incur too much per-step overhead. In this paper, we propose Sophia, Second-order Clipped Stochastic Optimization, a simple scalable second-order optimizer that uses a light-weight estimate of the diagonal Hessian as the pre-conditioner. The update is the moving average of the gradients divided by the moving average of the estimated Hessian, followed by element-wise clipping. The clipping controls the worst-case update size and tames the negative impact of non-convexity and rapid change of Hessian along the trajectory. Sophia only estimates the diagonal Hessian every handful of iterations, which has negligible average per-step time and memory overhead. On language modeling with GPT m
    
[^196]: 超越共享词汇：增加多语言机器翻译中的表示词语相似性

    Beyond Shared Vocabulary: Increasing Representational Word Similarities across Languages for Multilingual Machine Translation. (arXiv:2305.14189v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14189](http://arxiv.org/abs/2305.14189)

    本文提出了一种超越共享词汇的方法，通过定义词级信息传输路径和使用图网络来融合跨语言的词嵌入，实现了在多语言机器翻译中提高相似含义词的对齐性和BLEU分数的一致提升。此方法只需要少量额外参数且计算成本增加有限，并且推理时间与基线相同。

    

    在多语言神经机器翻译(MNMT)中，使用共享的词汇是常见的做法。除了简单的设计外，共享标记在积极的知识转移中起着重要的作用，假设共享标记在不同语言中指的是相似的含义。然而，当词汇的重叠较小时，尤其是由于不同的书写系统，转移被限制。在本文中，我们通过词等价类定义了词级信息传输路径，并依赖图网络来融合跨语言的词嵌入。我们的实验证明了我们方法的优势：1) 具有相似含义的词的嵌入在不同语言中更好地对齐，2) 我们的方法在高和低资源MNMT方面实现了一致的BLEU提升达2.3个点，3) 需要少于1.0%的额外可训练参数，并且计算成本的增加有限，而推理时间与基线相同。

    Using a vocabulary that is shared across languages is common practice in Multilingual Neural Machine Translation (MNMT). In addition to its simple design, shared tokens play an important role in positive knowledge transfer, assuming that shared tokens refer to similar meanings across languages. However, when word overlap is small, especially due to different writing systems, transfer is inhibited. In this paper, we define word-level information transfer pathways via word equivalence classes and rely on graph networks to fuse word embeddings across languages. Our experiments demonstrate the advantages of our approach: 1) embeddings of words with similar meanings are better aligned across languages, 2) our method achieves consistent BLEU improvements of up to 2.3 points for high- and low-resource MNMT, and 3) less than 1.0\% additional trainable parameters are required with a limited increase in computational costs, while inference time remains identical to the baseline. We release the c
    
[^197]: 压缩，然后提示：使用可转移提示来改善LLM推理的准确性和效率平衡

    Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt. (arXiv:2305.11186v1 [cs.CL])

    [http://arxiv.org/abs/2305.11186](http://arxiv.org/abs/2305.11186)

    本文提出了使用可转移提示来优化压缩的LLMs的准确性和效率的平衡问题。该方法通过选择精度更高的提示显著提高了压缩的LLM在特定查询方面的生成质量，并实现了4倍推理时间加速。

    

    大型语言模型（LLMs）具有数十亿的参数，表现出在各种自然语言处理（NLP）任务中的卓越性能。然而，它们在推理过程中会带来显着的计算挑战，尤其是在常见的硬件上部署（例如单个GPU）时。因此，通过压缩来最小化LLM推理的延迟，即减少计算和内存需求，变得至关重要。但是，此过程必然引发效率和精度之间的平衡，因为压缩的LLMs通常会经历预测精度下降。在这项研究中，我们提出了一个创新的视角：为了优化这种平衡，压缩的LLMs需要一种不同于原始模型的独特输入格式。我们的研究结果表明，通过选择具有精度的提示，可以显著改善压缩的LLM在特定查询方面的生成质量。基于这一发现，我们提出了一个可转移提示的方法，该方法训练模型预测有效提示。实证上，我们的方法可以在各种语言任务上生成高质量的输出，并实现了4倍速度的推理时间加速，同时保持竞争性的准确性。

    Large Language Models (LLMs), armed with billions of parameters, exhibit exceptional performance across a wide range of Natural Language Processing (NLP) tasks. However, they present a significant computational challenge during inference, especially when deploying on common hardware such as single GPUs. As such, minimizing the latency of LLM inference by curtailing computational and memory requirements, though achieved through compression, becomes critically important. However, this process inevitably instigates a trade-off between efficiency and accuracy, as compressed LLMs typically experience a reduction in predictive precision. In this research, we introduce an innovative perspective: to optimize this trade-off, compressed LLMs require a unique input format that varies from that of the original models. Our findings indicate that the generation quality in a compressed LLM can be markedly improved for specific queries by selecting prompts with precision. Capitalizing on this insight,
    
[^198]: Clifford群等变神经网络

    Clifford Group Equivariant Neural Networks. (arXiv:2305.11141v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.11141](http://arxiv.org/abs/2305.11141)

    我们引入了Clifford群等变神经网络，它可以构建O(n)和E(n)等变模型。该方法通过调整Clifford群的定义以及保持向量空间和乘法结构的作用来实现多个有利属性。

    

    我们引入了Clifford群等变神经网络：一种构建O(n)和E(n)等变模型的新方法。我们确定并研究了Clifford群，它是Clifford代数中的一个子群，其定义经过调整以实现多个有利属性。主要地，该群的作用形成了一个正交自同构，扩展到整个Clifford代数，同时尊重多矢分级。这导致了对应于多矢分解的多个非等价子表示。此外，我们证明该作用不仅尊重Clifford代数的向量空间结构，还尊重其乘法结构，即几何乘积。这些发现意味着我们可以得到在任意维的内积空间中优雅地推广的表达层。我们特别展示了从一个sin

    We introduce Clifford Group Equivariant Neural Networks: a novel approach for constructing $\mathrm{O}(n)$- and $\mathrm{E}(n)$-equivariant models. We identify and study the $\textit{Clifford group}$, a subgroup inside the Clifford algebra whose definition we adjust to achieve several favorable properties. Primarily, the group's action forms an orthogonal automorphism that extends beyond the typical vector space to the entire Clifford algebra while respecting the multivector grading. This leads to several non-equivalent subrepresentations corresponding to the multivector decomposition. Furthermore, we prove that the action respects not just the vector space structure of the Clifford algebra but also its multiplicative structure, i.e., the geometric product. These findings imply that every polynomial in multivectors, An advantage worth mentioning is that we obtain expressive layers that can elegantly generalize to inner-product spaces of any dimension. We demonstrate, notably from a sin
    
[^199]: MoMo: 动量模型的自适应学习率

    MoMo: Momentum Models for Adaptive Learning Rates. (arXiv:2305.07583v1 [cs.LG])

    [http://arxiv.org/abs/2305.07583](http://arxiv.org/abs/2305.07583)

    本文提出了新的自适应学习率，可与任何动量方法一起使用，通过构建损失函数模型并使用下限截断，以及即时估计未知下限，来近似最小化该模型以计算下一步，实验表明，相较于SGDM和Adam，该方法在精度和超参数调优的鲁棒性方面有所提高。

    

    本文提出了新的自适应学习率，可与任何动量方法一起使用。为了展示我们的新学习率，我们开发了MoMo和MoMo-Adam，它们是具有动量（SGDM）的SGD和Adam方法与我们的新自适应学习率一起使用。我们的MoMo方法是通过基于模型的随机优化来激发的，其中我们使用每次迭代采样的批量损失和梯度的动量估计来构建损失函数模型。我们的模型还利用了已知损失函数下限的截断方法。实际上，大多数损失都被下限为零。然后，在每次迭代中，我们近似最小化此模型以计算下一步。对于具有未知下限的损失，我们开发了新的即时下限估计，这些估计用于我们的模型中。数值实验表明，我们的MoMo方法在MNIST、CIFAR10、CIFAR100和Imagenet32等数据集的图像分类训练中，相较于SGDM和Adam，在精度和超参数调优的鲁棒性方面都有所提高。

    We present new adaptive learning rates that can be used with any momentum method. To showcase our new learning rates we develop MoMo and MoMo-Adam, which are SGD with momentum (SGDM) and Adam together with our new adaptive learning rates. Our MoMo methods are motivated through model-based stochastic optimization, wherein we use momentum estimates of the batch losses and gradients sampled at each iteration to build a model of the loss function. Our model also makes use of any known lower bound of the loss function by using truncation. Indeed most losses are bounded below by zero. We then approximately minimize this model at each iteration to compute the next step. For losses with unknown lower bounds, we develop new on-the-fly estimates of the lower bound that we use in our model. Numerical experiments show that our MoMo methods improve over SGDM and Adam in terms of accuracy and robustness to hyperparameter tuning for training image classifiers on MNIST, CIFAR10, CIFAR100, Imagenet32, 
    
[^200]: 基于Segment Anything Model (SAM)增强伪标签的弱监督语义分割

    Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation. (arXiv:2305.05803v1 [cs.CV])

    [http://arxiv.org/abs/2305.05803](http://arxiv.org/abs/2305.05803)

    本论文提出了一种通过利用Segment Anything Model (SAM)增强Class Activation Maps (CAM)生成高质量伪标签的方法来解决弱监督语义分割中CAM的局部激活和虚假激活的限制问题。

    

    仅使用图像级别的监督的弱监督语义分割(WSSS)由于其与像素级注释相比的低成本而越来越受到关注。大多数现有方法依赖于类激活图(CAM)生成像素级的伪标签进行监督训练。然而，众所周知，CAM经常遭受局部激活的限制-只激活最具区分性的部分，而不是整个对象区域和虚假的激活-不必要地激活物体周围的背景。本研究引入了一种简单而有效的方法来解决这些限制，即利用最近发布的Segment Anything Model (SAM)增强CAM生成高质量的伪标签。SAM是一个分割基础模型，展示了将图像分割成段落的强零-shot能力，但缺乏这些区域的语义标签。为了解决这个问题，我们采用特定类别的伪标签作为信号来选择m。

    Weakly Supervised Semantic Segmentation (WSSS) with only image-level supervision has garnered increasing attention due to its low annotation cost compared to pixel-level annotation. Most existing methods rely on Class Activation Maps (CAM) to generate pixel-level pseudo labels for supervised training. However, it is well known that CAM often suffers from partial activation -- activating the most discriminative part instead of the entire object area, and false activation -- unnecessarily activating the background around the object. In this study, we introduce a simple yet effective approach to address these limitations by harnessing the recently released Segment Anything Model (SAM) to generate higher-quality pseudo labels with CAM. SAM is a segmentation foundation model that demonstrates strong zero-shot ability in partitioning images into segments but lacks semantic labels for these regions. To circumvent this, we employ pseudo labels for a specific class as the signal to select the m
    
[^201]: 在联邦学习中平衡隐私与安全：FedGT的群体测试框架

    Balancing Privacy and Security in Federated Learning with FedGT: A Group Testing Framework. (arXiv:2305.05506v1 [cs.LG])

    [http://arxiv.org/abs/2305.05506](http://arxiv.org/abs/2305.05506)

    该论文提出了FedGT框架，通过群体测试的方法在联邦学习中识别并删除恶意客户，从而平衡了隐私和安全，保护数据隐私并提高了识别恶意客户的能力。

    

    我们提出FedGT，一个新颖的框架，用于在联邦学习中识别恶意客户并进行安全聚合。受到群体测试的启发，该框架利用重叠的客户组来检测恶意客户的存在，并通过译码操作识别它们。然后，将这些被识别的客户从模型的训练中删除，并在其余客户之间执行训练。FedGT在隐私和安全之间取得平衡，允许改进识别能力同时仍保护数据隐私。具体而言，服务器学习每个组中客户的聚合模型。通过对MNIST和CIFAR-10数据集进行大量实验，证明了FedGT的有效性，展示了其识别恶意客户的能力，具有低误检和虚警概率，产生高模型效用。

    We propose FedGT, a novel framework for identifying malicious clients in federated learning with secure aggregation. Inspired by group testing, the framework leverages overlapping groups of clients to detect the presence of malicious clients in the groups and to identify them via a decoding operation. The identified clients are then removed from the training of the model, which is performed over the remaining clients. FedGT strikes a balance between privacy and security, allowing for improved identification capabilities while still preserving data privacy. Specifically, the server learns the aggregated model of the clients in each group. The effectiveness of FedGT is demonstrated through extensive experiments on the MNIST and CIFAR-10 datasets, showing its ability to identify malicious clients with low misdetection and false alarm probabilities, resulting in high model utility.
    
[^202]: Tiny-PPG: 用于边缘设备上实时检测光电容积脉搏图信号中运动伪影的轻量级深度神经网络

    Tiny-PPG: A Lightweight Deep Neural Network for Real-Time Detection of Motion Artifacts in Photoplethysmogram Signals on Edge Devices. (arXiv:2305.03308v1 [eess.SP])

    [http://arxiv.org/abs/2305.03308](http://arxiv.org/abs/2305.03308)

    Tiny-PPG是一个用于边缘设备上实时检测PPG信号中运动伪影的轻量级深度神经网络，通过使用深度可分离卷积和空洞空间金字塔池化模块，以及通道注意机制，能够在平衡检测精度和速度的情况下实现最先进的检测效果，并可以在现实世界中部署，实现基于物联网的可穿戴设备和智能健康设备上的准确实时PPG伪影检测。

    

    尽管光电容积脉搏图（PPG）信号已经被广泛应用于基于物联网的可穿戴设备和智能健康设备中进行心血管健康监护，但在现实世界中，PPG信号很容易受到运动伪影的污染。本研究提出了一种名为“Tiny-PPG”的轻量级深度神经网络，用于在物联网边缘设备上准确实时地分割PPG伪影。该模型在公开数据集PPG DaLiA上进行了训练和测试，该数据集使用手表式设备（Empatica E4）对15名受试者在各种日常活动中的PPG信号，具有不同长度和形态的复杂伪影。该模型结构、训练方法和损失函数特别设计，以平衡检测精度和速度，适用于资源受限的嵌入式设备上进行实时PPG伪影检测。为了优化多尺度特征表示的模型大小和能力，该模型采用深度可分离卷积和空洞空间金字塔池化模块。此外，还提出了一个通道注意机制，以跨通道学习区分性和信息丰富的PPG信号特征。实验结果表明，Tiny-PPG在检测精度和计算效率方面均实现了最先进的性能，并可在现实世界中部署，实现基于物联网的可穿戴设备和智能健康设备上的准确实时PPG伪影检测。

    Photoplethysmogram (PPG) signals are easily contaminated by motion artifacts in real-world settings, despite their widespread use in Internet-of-Things (IoT) based wearable and smart health devices for cardiovascular health monitoring. This study proposed a lightweight deep neural network, called Tiny-PPG, for accurate and real-time PPG artifact segmentation on IoT edge devices. The model was trained and tested on a public dataset, PPG DaLiA, which featured complex artifacts with diverse lengths and morphologies during various daily activities of 15 subjects using a watch-type device (Empatica E4). The model structure, training method and loss function were specifically designed to balance detection accuracy and speed for real-time PPG artifact detection in resource-constrained embedded devices. To optimize the model size and capability in multi-scale feature representation, the model employed deep separable convolution and atrous spatial pyramid pooling modules, respectively. Addition
    
[^203]: 基于LTL规范的样本有效无模型强化学习与优化保证

    Sample Efficient Model-free Reinforcement Learning from LTL Specifications with Optimality Guarantees. (arXiv:2305.01381v1 [cs.LG])

    [http://arxiv.org/abs/2305.01381](http://arxiv.org/abs/2305.01381)

    本文提出了一种基于LTL规范的无模型强化学习方法，该方法结合乘积MDP、奖励结构和折扣机制有效地学习并优化未知随机系统最大化满足LTL规范的概率的最优策略。

    

    线性时间逻辑（LTL）广泛用于指定系统策略的高级目标，自主系统学习相对于这样的规范的最优策略是非常理想的。 但是，从LTL规范中学习最优策略并不轻松。我们提出了一种无模型强化学习（RL）方法，该方法可以有效地学习未知随机系统的最优策略，其中使用马尔可夫决策过程（MDP）进行建模。我们提出了一种新颖且更通用的乘积MDP、奖励结构和折扣机制，当与现成的无模型RL算法结合使用时，能够高效地学习最大化给定LTL规范满足概率的最优策略，并提供了更好的有关选择RL中关键参数以保证最优性的理论结果。为了直接评估学习策略，我们采用概率模型检查器PRISM来计算LTL规范的满足概率。

    Linear Temporal Logic (LTL) is widely used to specify high-level objectives for system policies, and it is highly desirable for autonomous systems to learn the optimal policy with respect to such specifications. However, learning the optimal policy from LTL specifications is not trivial. We present a model-free Reinforcement Learning (RL) approach that efficiently learns an optimal policy for an unknown stochastic system, modelled using Markov Decision Processes (MDPs). We propose a novel and more general product MDP, reward structure and discounting mechanism that, when applied in conjunction with off-the-shelf model-free RL algorithms, efficiently learn the optimal policy that maximizes the probability of satisfying a given LTL specification with optimality guarantees. We also provide improved theoretical results on choosing the key parameters in RL to ensure optimality. To directly evaluate the learned policy, we adopt probabilistic model checker PRISM to compute the probability of 
    
[^204]: 通过贝叶斯主动学习实现自校正贝叶斯优化

    Self-Correcting Bayesian Optimization through Bayesian Active Learning. (arXiv:2304.11005v1 [cs.LG])

    [http://arxiv.org/abs/2304.11005](http://arxiv.org/abs/2304.11005)

    该论文提出了SAL和SCoreBO两种方法，用于提高高斯过程模型的超参数选择和贝叶斯优化的表现。

    

    高斯过程已成为贝叶斯优化和主动学习中的首选模型。然而，高斯过程的完全发挥需要巧妙选择超参数，而在文献中很少有关于找到正确超参数的努力。我们演示了选择好的超参数对于高斯过程的影响，并提出了两个明确优先考虑此目标的收购函数。统计距离主动学习（SAL）考虑后验样本的平均不一致性，由统计距离测量。结果显示，在许多测试函数上，它胜过了贝叶斯主动学习的最新结果。然后，我们引入了自校正贝叶斯优化（SCoreBO），它将SAL扩展到同时执行贝叶斯优化和主动超参数学习。相比传统BO，SCoreBO以改进的速度学习模型超参数，同时在最新的贝叶斯优化搜索中取得更好的表现。

    Gaussian processes are cemented as the model of choice in Bayesian optimization and active learning. Yet, they are severely dependent on cleverly chosen hyperparameters to reach their full potential, and little effort is devoted to finding the right hyperparameters in the literature. We demonstrate the impact of selecting good hyperparameters for GPs and present two acquisition functions that explicitly prioritize this goal. Statistical distance-based Active Learning (SAL) considers the average disagreement among samples from the posterior, as measured by a statistical distance. It is shown to outperform the state-of-the-art in Bayesian active learning on a number of test functions. We then introduce Self-Correcting Bayesian Optimization (SCoreBO), which extends SAL to perform Bayesian optimization and active hyperparameter learning simultaneously. SCoreBO learns the model hyperparameters at improved rates compared to vanilla BO, while outperforming the latest Bayesian optimization met
    
[^205]: 基于注意力机制的两阶段脑肿瘤MR图像分割方法

    Two-stage MR Image Segmentation Method for Brain Tumors based on Attention Mechanism. (arXiv:2304.08072v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2304.08072](http://arxiv.org/abs/2304.08072)

    该论文提出了一个基于循环一致生成对抗网络的两阶段脑肿瘤MR图像分割方法，通过引入坐标注意力和空间注意力模块来优化生成器性能，以提高生成和分割性能。

    

    多模态磁共振成像可以揭示人体组织的不同模式，对于临床诊断至关重要。然而，由于成本、噪声和手动标记的限制，获得多样性和可靠的多模态MR图像仍然是一个挑战。为了获得更好的生成和分割性能，提出了一个基于循环一致生成对抗网络（CycleGAN）的协同空间注意力生成对抗网络（CASP-GAN）。引入坐标注意力（CA）模块和空间注意力（SA）模块来优化生成器的性能。这两个模块可以充分利用捕获到的位置信息，准确定位感兴趣区域，并增强生成器模型的网络结构。能够提取结构信息和详细信息。

    Multimodal magnetic resonance imaging (MRI) can reveal different patterns of human tissue and is crucial for clinical diagnosis. However, limited by cost, noise and manual labeling, obtaining diverse and reliable multimodal MR images remains a challenge. For the same lesion, different MRI manifestations have great differences in background information, coarse positioning and fine structure. In order to obtain better generation and segmentation performance, a coordination-spatial attention generation adversarial network (CASP-GAN) based on the cycle-consistent generative adversarial network (CycleGAN) is proposed. The performance of the generator is optimized by introducing the Coordinate Attention (CA) module and the Spatial Attention (SA) module. The two modules can make full use of the captured location information, accurately locating the interested region, and enhancing the generator model network structure. The ability to extract the structure information and the detailed informat
    
[^206]: MC-ViViT: 多分支分类器-ViViT用于使用面部视频检测老年人轻度认知障碍

    MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults using Facial Videos. (arXiv:2304.05292v1 [cs.CV])

    [http://arxiv.org/abs/2304.05292](http://arxiv.org/abs/2304.05292)

    本文提出了一种新的深度机器学习模型MC-ViViT，用于通过分析面部特征检测老年人轻度认知障碍。通过MC模块和结合损失函数来解决数据集样本不平衡问题，提高了算法的性能。

    

    深度机器学习模型包括卷积神经网络(CNN)已成功地应用于使用医学图像、问卷和视频检测轻度认知障碍(MCI)。本文提出了一种新的多分支分类器-视频视觉变换器(MC-ViViT)模型，通过分析面部特征区分MCI和正常认知。数据来自I-CONECT，一个旨在通过提供频繁视频聊天来改善认知功能的行为干预试验。MC-ViViT在一个分支中提取视频的时空特征，并通过MC模块增强表示。由于I-CONECT数据集中的样本不平衡问题（包含难易和正负样本），这使MC-ViViT的性能受到影响。我们提出了一种Hard-Easy和Positive-Negative样本的损失函数（HP Loss）来结合对比度调节损失Focal loss和AD-CORRE loss来解决不平衡问题。我们在I-CONECT数据集上的实验结果显示出该算法的有效性。

    Deep machine learning models including Convolutional Neural Networks (CNN) have been successful in the detection of Mild Cognitive Impairment (MCI) using medical images, questionnaires, and videos. This paper proposes a novel Multi-branch Classifier-Video Vision Transformer (MC-ViViT) model to distinguish MCI from those with normal cognition by analyzing facial features. The data comes from the I-CONECT, a behavioral intervention trial aimed at improving cognitive function by providing frequent video chats. MC-ViViT extracts spatiotemporal features of videos in one branch and augments representations by the MC module. The I-CONECT dataset is challenging as the dataset is imbalanced containing Hard-Easy and Positive-Negative samples, which impedes the performance of MC-ViViT. We propose a loss function for Hard-Easy and Positive-Negative Samples (HP Loss) by combining Focal loss and AD-CORRE loss to address the imbalanced problem. Our experimental results on the I-CONECT dataset show th
    
[^207]: 双重不确定性自训练用于半监督医学图像分割

    Self-training with dual uncertainty for semi-supervised medical image segmentation. (arXiv:2304.04441v1 [cs.CV])

    [http://arxiv.org/abs/2304.04441](http://arxiv.org/abs/2304.04441)

    该论文介绍了在半监督医学图像分割中如何通过双重不确定性的自训练方式来提高分割精度。

    

    在半监督医学图像分割领域中，标记数据短缺是一个根本性问题。如何有效地从未标记的图像中学习图像特征以提高分割精度是这一领域的主要研究方向。传统的自训练方法可以通过为迭代训练生成伪标签部分解决标记数据不足的问题。然而，在训练过程中由模型不确定性产生的噪声直接影响了分割结果。因此，我们在自训练框架中增加了样本层面和像素层面的不确定性以稳定训练过程。具体来说，我们在预训练期间保存了模型的几个时刻，并使用它们在未标记样本上的预测之间的差异作为该样本的样本层面不确定性估计。然后，我们逐渐添加从易到难的未标记样本进行训练。同时，我们添加了一个带有不同上采样方法的解码器。

    In the field of semi-supervised medical image segmentation, the shortage of labeled data is the fundamental problem. How to effectively learn image features from unlabeled images to improve segmentation accuracy is the main research direction in this field. Traditional self-training methods can partially solve the problem of insufficient labeled data by generating pseudo labels for iterative training. However, noise generated due to the model's uncertainty during training directly affects the segmentation results. Therefore, we added sample-level and pixel-level uncertainty to stabilize the training process based on the self-training framework. Specifically, we saved several moments of the model during pre-training, and used the difference between their predictions on unlabeled samples as the sample-level uncertainty estimate for that sample. Then, we gradually add unlabeled samples from easy to hard during training. At the same time, we added a decoder with different upsampling method
    
[^208]: StepMix: 一个用于外部变量广义混合模型的伪似然估计的Python包

    StepMix: A Python Package for Pseudo-Likelihood Estimation of Generalized Mixture Models with External Variables. (arXiv:2304.03853v1 [stat.ME])

    [http://arxiv.org/abs/2304.03853](http://arxiv.org/abs/2304.03853)

    StepMix是一个用于外部变量广义混合模型的伪似然估计的Python包，提供了单步和逐步估计方法，帮助从业人员进行模型估计、选择和解释。

    

    StepMix是一个用于广义有限混合模型(潜在剖面和潜在类分析)与外部变量(协变量和远程结果)的伪似然估计(单步、两步和三步方法)的开源软件包。在许多社会科学的应用中，主要目标不仅是将个体聚类成潜在类别，还包括使用这些类别来开发更复杂的统计模型。这些模型通常分为一个将潜在类别与观察指标相关联的测量模型和一个将协变量和结果变量与潜在类别相关联的结构模型。测量和结构模型可以使用所谓的一步法共同估计，也可以使用逐步方法逐步估计，对于从业人员来说，这些方法在估计潜在类别的可解释性方面具有显著优势。除了一步法，StepMix还实现了文献中提出的最重要的逐步估计方法，提供了用户友好的界面，方便模型的估计、选择和解释。

    StepMix is an open-source software package for the pseudo-likelihood estimation (one-, two- and three-step approaches) of generalized finite mixture models (latent profile and latent class analysis) with external variables (covariates and distal outcomes). In many applications in social sciences, the main objective is not only to cluster individuals into latent classes, but also to use these classes to develop more complex statistical models. These models generally divide into a measurement model that relates the latent classes to observed indicators, and a structural model that relates covariates and outcome variables to the latent classes. The measurement and structural models can be estimated jointly using the so-called one-step approach or sequentially using stepwise methods, which present significant advantages for practitioners regarding the interpretability of the estimated latent classes. In addition to the one-step approach, StepMix implements the most important stepwise estim
    
[^209]: 适应网格细化的群体强化学习

    Swarm Reinforcement Learning For Adaptive Mesh Refinement. (arXiv:2304.00818v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2304.00818](http://arxiv.org/abs/2304.00818)

    这项研究提出了一种适应网格细化的群体强化学习方法，通过将网格建模为一组简单协作的代理，并利用消息传递网络在相邻网格元素之间传播信息，以解决传统方法在复杂模拟中的应用限制。该方法被证实可以学习可靠、可扩展的网格细化策略。

    

    有限元方法是工程学中一种重要的技术，自适应网格细化（AMR）通过动态细化网格区域，在计算速度和模拟精度之间取得有利的平衡。传统的AMR方法依赖于特定任务的启发式规则或昂贵的误差估计器，限制了其在复杂模拟中的应用。最近的学习型AMR方法试图解决这些问题，但目前只能应用于简单的示例。我们将AMR表述为一种新颖的自适应群体马尔可夫决策过程，其中网格被建模为一组简单协作的代理，可以分裂为多个新代理。这个框架可以使用空间奖励公式化简分配问题，并结合消息传递网络在相邻网格元素之间传播信息。我们通过实验证明了我们的方法——自适应群体网格细化（ASMR）的有效性，显示它可以学习可靠、可扩展的网格细化策略。

    The Finite Element Method, an important technique in engineering, is aided by Adaptive Mesh Refinement (AMR), which dynamically refines mesh regions to allow for a favorable trade-off between computational speed and simulation accuracy. Classical methods for AMR depend on task-specific heuristics or expensive error estimators, hindering their use for complex simulations. Recent learned AMR methods tackle these problems, but so far scale only to simple toy examples. We formulate AMR as a novel Adaptive Swarm Markov Decision Process in which a mesh is modeled as a system of simple collaborating agents that may split into multiple new agents. This framework allows for a spatial reward formulation that simplifies the credit assignment problem, which we combine with Message Passing Networks to propagate information between neighboring mesh elements. We experimentally validate the effectiveness of our approach, Adaptive Swarm Mesh Refinement (ASMR), showing that it learns reliable, scalable,
    
[^210]: Reflexion：具有动态记忆和自我反思的自主智能体

    Reflexion: an autonomous agent with dynamic memory and self-reflection. (arXiv:2303.11366v1 [cs.AI])

    [http://arxiv.org/abs/2303.11366](http://arxiv.org/abs/2303.11366)

    本文提出 Reflexion 方法，给智能体赋予了动态记忆和自我反思能力，以增强其任务特定的行动选择能力。

    

    最近决策大型语言模型（LLM）代理的发展在各种基准测试中展现出卓越的性能。然而，这些最先进的方法通常需要内部模型微调、外部模型微调或在定义的状态空间上进行策略优化。由于高质量训练数据的稀缺性或缺乏良好定义的状态空间，实现这些方法可能会具有挑战性。此外，这些代理没有人类决策过程固有的某些品质，特别是从错误中学习的能力。通过反思，人类可以通过试错过程高效地解决新的问题。在最近的研究基础上，我们提出 Reflexion，一种将动态记忆和自我反思能力赋予智能体的方法，以增强其现有的推理轨迹和任务特定的行动选择能力。为了实现完全自动化，我们介绍了一种简单而有效的方法。

    Recent advancements in decision-making large language model (LLM) agents have demonstrated impressive performance across various benchmarks. However, these state-of-the-art approaches typically necessitate internal model fine-tuning, external model fine-tuning, or policy optimization over a defined state space. Implementing these methods can prove challenging due to the scarcity of high-quality training data or the lack of well-defined state space. Moreover, these agents do not possess certain qualities inherent to human decision-making processes, specifically the ability to learn from mistakes. Self-reflection allows humans to efficiently solve novel problems through a process of trial and error. Building on recent research, we propose Reflexion, an approach that endows an agent with dynamic memory and self-reflection capabilities to enhance its existing reasoning trace and task-specific action choice abilities. To achieve full automation, we introduce a straightforward yet effective 
    
[^211]: 通过 Numerai 数据科学竞赛案例，理解时间表格和多变量时间序列的模型复杂度

    Understanding Model Complexity for temporal tabular and multi-variate time series, case study with Numerai data science tournament. (arXiv:2303.07925v1 [cs.LG])

    [http://arxiv.org/abs/2303.07925](http://arxiv.org/abs/2303.07925)

    本文采用 Numerai 数据科学竞赛的数据，探究了多变量时间序列建模中不同特征工程和降维方法的应用；提出了一种新的集成方法，用于高维时间序列建模，该方法在通用性、鲁棒性和效率上优于一些深度学习模型。

    

    本文探究了在多变量时间序列建模中使用不同特征工程和降维方法的应用。利用从 Numerai 数据竞赛创建的特征目标交叉相关时间序列数据集，我们证明在过度参数化的情况下，不同特征工程方法的性能与预测会收敛到可由再生核希尔伯特空间刻画的相同平衡态。我们提出了一种新的集成方法，该方法结合了不同的随机非线性变换，随后采用岭回归模型进行高维时间序列建模。与一些常用的用于序列建模的深度学习模型（如 LSTM 和 transformer）相比，我们的方法更加鲁棒（在不同的随机种子下具有较低的模型方差，且对架构的选择不太敏感），并且更有效率。我们方法的另一个优势在于模型的简单性，因为没有必要使用复杂的深度学习框架。

    In this paper, we explore the use of different feature engineering and dimensionality reduction methods in multi-variate time-series modelling. Using a feature-target cross correlation time series dataset created from Numerai tournament, we demonstrate under over-parameterised regime, both the performance and predictions from different feature engineering methods converge to the same equilibrium, which can be characterised by the reproducing kernel Hilbert space. We suggest a new Ensemble method, which combines different random non-linear transforms followed by ridge regression for modelling high dimensional time-series. Compared to some commonly used deep learning models for sequence modelling, such as LSTM and transformers, our method is more robust (lower model variance over different random seeds and less sensitive to the choice of architecture) and more efficient. An additional advantage of our method is model simplicity as there is no need to use sophisticated deep learning frame
    
[^212]: RACCER: 面向可达和确证的强化学习可追溯解释

    RACCER: Towards Reachable and Certain Counterfactual Explanations for Reinforcement Learning. (arXiv:2303.04475v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.04475](http://arxiv.org/abs/2303.04475)

    RACCER是第一个针对强化学习智能体行为生成对抗事实解释的专用方法，通过使用一组针对强化学习的特定对抗事实属性，保证易于实现且具有高概率预期结果的对抗事实。

    

    尽管强化学习算法在许多任务上取得了成功，但其对神经网络的依赖使其行为难以理解和信任。对抗事实解释是一种人性化的解释，为用户提供了关于如何改变模型输入以达到预期输出的可行建议。然而，当前在强化学习中生成对抗事实的方法忽略了强化学习任务的随机性和顺序性，可能产生难以获得或无法实现预期结果的对抗事实。在这项工作中，我们提出了RACCER，这是一种针对强化学习智能体行为生成对抗事实解释的首个专用方法。我们首先提出并实现了一组针对强化学习的特定对抗事实属性，确保易于实现且具有高概率预期结果的对抗事实。我们使用启发式树搜索代理执行轨迹，以找到最合适的对抗事实。

    While reinforcement learning (RL) algorithms have been successfully applied to numerous tasks, their reliance on neural networks makes their behavior difficult to understand and trust. Counterfactual explanations are human-friendly explanations that offer users actionable advice on how to alter the model inputs to achieve the desired output from a black-box system. However, current approaches to generating counterfactuals in RL ignore the stochastic and sequential nature of RL tasks and can produce counterfactuals that are difficult to obtain or do not deliver the desired outcome. In this work, we propose RACCER, the first RL-specific approach to generating counterfactual explanations for the behavior of RL agents. We first propose and implement a set of RL-specific counterfactual properties that ensure easily reachable counterfactuals with highly probable desired outcomes. We use a heuristic tree search of the agent's execution trajectories to find the most suitable counterfactuals ba
    
[^213]: 过度参数化会导致梯度下降在学习单个神经元时的收敛速度指数级减慢

    Over-Parameterization Exponentially Slows Down Gradient Descent for Learning a Single Neuron. (arXiv:2302.10034v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10034](http://arxiv.org/abs/2302.10034)

    通过研究学习单个神经元的过度参数化设置，本研究发现梯度下降算法的收敛速度会以指数级减慢，是首个给出该问题全局收敛结果的研究。通过证明上下界，我们精确刻画出了收敛速度，并指出了过度参数化对于收敛速度的影响。

    

    我们重新审视了在具有ReLU激活函数和方形损失的高斯输入下学习单个神经元的问题。我们特别关注学生网络具有n≥2个神经元的过度参数化设置。我们证明了随机初始化的梯度下降算法以O(T^-3)的速度全局收敛。这是对于该问题的第一个超过精确参数化设置(n=1)的全局收敛结果，其中梯度下降算法呈现出exp(-Ω(T))的速度。令人惊讶的是，我们进一步证明了在过度参数化设置中，随机初始化的梯度流算法的下界是Ω(T^-3)。这两个下界共同给出了收敛速度的精确刻画，并首次暗示了过度参数化会使收敛速度指数级减慢。为了证明全局收敛，我们需要处理梯度下降动力学中学生神经元之间的相互作用，这在精确参数化设置中不存在。

    We revisit the problem of learning a single neuron with ReLU activation under Gaussian input with square loss. We particularly focus on the over-parameterization setting where the student network has $n\ge 2$ neurons. We prove the global convergence of randomly initialized gradient descent with a $O\left(T^{-3}\right)$ rate. This is the first global convergence result for this problem beyond the exact-parameterization setting ($n=1$) in which the gradient descent enjoys an $\exp(-\Omega(T))$ rate. Perhaps surprisingly, we further present an $\Omega\left(T^{-3}\right)$ lower bound for randomly initialized gradient flow in the over-parameterization setting. These two bounds jointly give an exact characterization of the convergence rate and imply, for the first time, that over-parameterization can exponentially slow down the convergence rate. To prove the global convergence, we need to tackle the interactions among student neurons in the gradient descent dynamics, which are not present in
    
[^214]: AliasNet: 加速相编码MRI的别名伪影抑制网络

    AliasNet: Alias Artefact Suppression Network for Accelerated Phase-Encode MRI. (arXiv:2302.08861v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2302.08861](http://arxiv.org/abs/2302.08861)

    本论文提出了AliasNet，一个别名伪影抑制网络，解决了在相编码MRI中由于硬件限制导致的别名伪影问题。通过开发两种解耦技术，实现了对具有出色1D不相干特性的伪影信号进行明确的1D正则化。该方法还结合了1D和2D重建技术，充分利用了稀疏性模型和多方向特点。

    

    稀疏重建是MRI中的重要方面，有助于减少采集时间并提高时空分辨率。目前流行的方法主要基于压缩感知技术（CS），它依赖于对k空间的随机采样来产生不相干的（噪声样的）伪影。由于硬件限制，1D笛卡尔相编码的欠采样方案在2D CS-MRI中很受欢迎。然而，1D欠采样限制了测量之间的2D不相干性，产生结构化的别名伪影（幽灵），在假设2D稀疏模型时可能难以去除。重建算法通常针对这些与方向相关的伪影采用不考虑方向的2D正则化方法。我们发现相编码伪影可以分离为连续的1D信号，因此开发了两种解耦技术，使得能够进行明确的1D正则化并利用出色的1D不相干特性。我们还推导出一个结合了1D+2D重建技术的方法，充分利用了稀疏性模型和多方向特点。

    Sparse reconstruction is an important aspect of MRI, helping to reduce acquisition time and improve spatial-temporal resolution. Popular methods are based mostly on compressed sensing (CS), which relies on the random sampling of k-space to produce incoherent (noise-like) artefacts. Due to hardware constraints, 1D Cartesian phase-encode under-sampling schemes are popular for 2D CS-MRI. However, 1D under-sampling limits 2D incoherence between measurements, yielding structured aliasing artefacts (ghosts) that may be difficult to remove assuming a 2D sparsity model. Reconstruction algorithms typically deploy direction-insensitive 2D regularisation for these direction-associated artefacts. Recognising that phase-encode artefacts can be separated into contiguous 1D signals, we develop two decoupling techniques that enable explicit 1D regularisation and leverage the excellent 1D incoherence characteristics. We also derive a combined 1D + 2D reconstruction technique that takes advantage of spa
    
[^215]: 《超越批处理二元分类的量子学习理论》

    Quantum Learning Theory Beyond Batch Binary Classification. (arXiv:2302.07409v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07409](http://arxiv.org/abs/2302.07409)

    这篇论文通过研究量子学习理论拓展了批处理多类学习、在线布尔学习和在线多类学习，并提出了第一个具有量子示例的在线学习模型。

    

    Arunachalam和de Wolf（2018）证明了在可实现和糊涂设置下，量子批处理学习布尔函数的样本复杂性与相应的经典样本复杂性具有相同的形式和数量级。在本文中，我们将这个明显令人惊讶的结果推广到了批处理多类学习、在线布尔学习和在线多类学习。对于我们的在线学习结果，我们首先考虑了Dawid和Tewari（2022）经典模型的自适应对手变体。然后，我们引入了第一个（据我们所知）具有量子示例的在线学习模型。

    Arunachalam and de Wolf (2018) showed that the sample complexity of quantum batch learning of boolean functions, in the realizable and agnostic settings, has the same form and order as the corresponding classical sample complexities. In this paper, we extend this, ostensibly surprising, message to batch multiclass learning, online boolean learning, and online multiclass learning. For our online learning results, we first consider an adaptive adversary variant of the classical model of Dawid and Tewari (2022). Then, we introduce the first (to the best of our knowledge) model of online learning with quantum examples.
    
[^216]: 强大的层级增强学习中的知识传输

    Robust Knowledge Transfer in Tiered Reinforcement Learning. (arXiv:2302.05534v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05534](http://arxiv.org/abs/2302.05534)

    本文研究了层级增强学习中的知识传输，提出了一种新颖的在线学习算法，在没有先验知识的任务相似性的情况下实现强大的知识传输。

    

    本文研究了层级增强学习设置，这是一个并行传输学习框架，在这个框架中，目标是将知识从低层（源）任务传输到高层（目标）任务，以减少后者的探索风险，同时并行解决这两个任务。与先前的工作不同，我们不假设低层和高层任务共享相同的动态或奖励函数，并且专注于在没有先验知识的任务相似性的情况下实现强大的知识传输。我们确定了一个称为“最优值支配”的自然而必要的条件，适用于我们的目标。在这个条件下，我们提出了一种新颖的在线学习算法，使得对于高层任务，在部分状态上可以实现恒定的遗憾，这取决于任务相似性，并在两个任务不相似时保持接近最优遗憾；而对于低层任务，它可以在不做出牺牲的情况下保持接近最优。此外，我们进一步研究了具有多个低层任务的情况。

    In this paper, we study the Tiered Reinforcement Learning setting, a parallel transfer learning framework, where the goal is to transfer knowledge from the low-tier (source) task to the high-tier (target) task to reduce the exploration risk of the latter while solving the two tasks in parallel. Unlike previous work, we do not assume the low-tier and high-tier tasks share the same dynamics or reward functions, and focus on robust knowledge transfer without prior knowledge on the task similarity. We identify a natural and necessary condition called the ``Optimal Value Dominance'' for our objective. Under this condition, we propose novel online learning algorithms such that, for the high-tier task, it can achieve constant regret on partial states depending on the task similarity and retain near-optimal regret when the two tasks are dissimilar, while for the low-tier task, it can keep near-optimal without making sacrifice. Moreover, we further study the setting with multiple low-tier tasks
    
[^217]: 通过特征凸卷积神经网络实现非对称的可信鲁棒性

    Asymmetric Certified Robustness via Feature-Convex Neural Networks. (arXiv:2302.01961v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01961](http://arxiv.org/abs/2302.01961)

    本文提出了一种新颖的特征凸卷积神经网络架构，将ICNN与Lipschitz特征映射结合，实现了对抗鲁棒性，并证明了对于具有一个“敏感”类的不对称二元分类设置，可以计算出确定性的认证鲁棒半径。

    

    最近的研究引入了输入凸卷积神经网络(ICNNs)作为具有有利的训练、推理和泛化特性的学习模型，与其凸结构相关。在本文中，我们提出了一种新颖的特征凸卷积神经网络架构，将ICNN与Lipschitz特征映射组合起来，以实现对抗鲁棒性。我们考虑具有一个“敏感”类的不对称二元分类设置，并为这个类证明了确定性的、封闭形式的和易于计算的任意$\ell_p$范数的认证鲁棒半径。我们通过表征它们的决策区域几何、将ICNN回归的通用逼近定理扩展到分类设定，并证明了在足够高维度下，这些模型完美地拟合甚至无结构均匀分布数据的概率下界，从理论上证明了使用这些模型的合理性。在Malimg恶意软件分类和MNIST子集上的实验中，

    Recent works have introduced input-convex neural networks (ICNNs) as learning models with advantageous training, inference, and generalization properties linked to their convex structure. In this paper, we propose a novel feature-convex neural network architecture as the composition of an ICNN with a Lipschitz feature map in order to achieve adversarial robustness. We consider the asymmetric binary classification setting with one "sensitive" class, and for this class we prove deterministic, closed-form, and easily-computable certified robust radii for arbitrary $\ell_p$-norms. We theoretically justify the use of these models by characterizing their decision region geometry, extending the universal approximation theorem for ICNN regression to the classification setting, and proving a lower bound on the probability that such models perfectly fit even unstructured uniformly distributed data in sufficiently high dimensions. Experiments on Malimg malware classification and subsets of MNIST,
    
[^218]: 用引导想象扩充小规模数据集

    Expanding Small-Scale Datasets with Guided Imagination. (arXiv:2211.13976v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.13976](http://arxiv.org/abs/2211.13976)

    本论文提出了一个引导想象框架(GIF)，通过利用DALL-E2和Stable Diffusion (SD)等生成模型，从种子数据中扩充小规模数据集。该框架通过在先验模型的语义空间中优化种子数据潜在特征来创建逼真的图像，并引入了类别保持和样本多样性的标准来指导想象过程。

    

    DNN的功效在很大程度上取决于训练数据的数量和质量。然而，大规模收集和标注数据通常费时费力。为了解决这个问题，我们探索了一项名为数据集扩充的新任务，旨在通过自动创建新的标记样本来扩充一个小规模的可用数据集。为此，我们提出了一个引导想象框架(GIF)，利用DALL-E2和Stable Diffusion (SD)等尖端生成模型的力量，从输入的种子数据中“想象”并创建信息丰富的新数据。具体而言，GIF通过在先验模型的语义有意义的空间中优化种子数据的潜在特征来进行数据的想象，从而创建具有新内容的照片般逼真的图像。为了引导想象朝着创建用于模型训练的信息丰富样本的方向发展，我们引入了两个关键标准，即类别保持信息提升和样本多样性促进。这些标准被证明有效地提高了生成图像的质量和多样性。

    The power of DNNs relies heavily on the quantity and quality of training data. However, collecting and annotating data on a large scale is often expensive and time-consuming. To address this issue, we explore a new task, termed dataset expansion, aimed at expanding a ready-to-use small dataset by automatically creating new labeled samples. To this end, we present a Guided Imagination Framework (GIF) that leverages cutting-edge generative models like DALL-E2 and Stable Diffusion (SD) to "imagine" and create informative new data from the input seed data. Specifically, GIF conducts data imagination by optimizing the latent features of the seed data in the semantically meaningful space of the prior model, resulting in the creation of photo-realistic images with new content. To guide the imagination towards creating informative samples for model training, we introduce two key criteria, i.e., class-maintained information boosting and sample diversity promotion. These criteria are verified to
    
[^219]: 如何使用 SGD 调整视觉模型

    How to Fine-Tune Vision Models with SGD. (arXiv:2211.09359v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.09359](http://arxiv.org/abs/2211.09359)

    SGD 和 AdamW 是用于调整视觉模型的两种常见优化器，当细调梯度在嵌入层中较大时，AdamW 的性能优于 SGD。我们提出了冻结嵌入层的简单修正方法，使得 SGD 表现略好于 AdamW 并且使用更少的内存。

    

    SGD 和 AdamW 是计算机视觉中用于调整大型神经网络的两种最常用的优化器。当这两种方法表现相同时，SGD 更受青睐，因为它比 AdamW 使用更少的内存（具有动量时，每个参数占用 12 字节，而不具有动量时每个参数占用 8 字节），而 AdamW 则需要更多的内存（每个参数占用 16 字节）。然而，在一系列下游任务中，尤其是在具有分布偏移的任务中，我们发现在现代的 Vision Transformer 和 ConvNeXt 模型上，使用 AdamW 进行微调的表现明显优于使用 SGD。我们发现，在微调梯度在第一个“嵌入”层中比其余模型的梯度大得多时，SGD 和 AdamW 之间存在很大的性能差距。我们的分析提出了一个简单的修正方法，可以在不同数据集和模型上始终保持一致：冻结嵌入层（参数的不到 1%），这样 SGD（具有或不具有动量）的表现略优于 AdamW，并且使用更少的内存（例如，在 ViT-L 上，SGD 使用的 GPU 内存较少 33%）。我们的观点导致了最新的技术进展。

    SGD and AdamW are the two most used optimizers for fine-tuning large neural networks in computer vision. When the two methods perform the same, SGD is preferable because it uses less memory (12 bytes/parameter with momentum and 8 bytes/parameter without) than AdamW (16 bytes/parameter). However, on a suite of downstream tasks, especially those with distribution shifts, we find that fine-tuning with AdamW performs substantially better than SGD on modern Vision Transformer and ConvNeXt models. We find that large gaps in performance between SGD and AdamW occur when the fine-tuning gradients in the first "embedding" layer are much larger than in the rest of the model. Our analysis suggests an easy fix that works consistently across datasets and models: freezing the embedding layer (less than 1% of the parameters) leads to SGD with or without momentum performing slightly better than AdamW while using less memory (e.g., on ViT-L, SGD uses 33% less GPU memory). Our insights result in state-of
    
[^220]: 两个模型胜过一个：联邦学习对于谷歌GBoard下一个单词预测并不私密

    Two Models are Better than One: Federated Learning Is Not Private For Google GBoard Next Word Prediction. (arXiv:2210.16947v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.16947](http://arxiv.org/abs/2210.16947)

    本文介绍了对联邦学习用于训练自然语言文本模型的新攻击，展示了对谷歌GBoard应用中下一个单词预测模型的攻击效果，揭示了用户输入的单词和句子可以被高准确度地恢复，引发了隐私担忧。

    

    在本文中，我们介绍了针对联邦学习用于训练自然语言文本模型的新攻击。我们展示了对谷歌GBoard应用中使用的下一个单词预测模型的攻击效果，谷歌GBoard是一款广泛使用的移动键盘应用，早期采用了联邦学习用于生产。我们证明，在各种条件下，可以高准确度地恢复用户在移动设备上输入的单词，例如发送短信时，而诸如使用小批量和添加本地噪音等对策无效。我们还展示了可以高度准确地重建单词顺序（从而重建实际输入的句子）。这引发了明显的隐私担忧，特别是考虑到GBoard正在使用。

    In this paper we present new attacks against federated learning when used to train natural language text models. We illustrate the effectiveness of the attacks against the next word prediction model used in Google's GBoard app, a widely used mobile keyboard app that has been an early adopter of federated learning for production use. We demonstrate that the words a user types on their mobile handset, e.g. when sending text messages, can be recovered with high accuracy under a wide range of conditions and that counter-measures such a use of mini-batches and adding local noise are ineffective. We also show that the word order (and so the actual sentences typed) can be reconstructed with high fidelity. This raises obvious privacy concerns, particularly since GBoard is in production use.
    
[^221]: 带有HaarPooling消息传递的图网络喷注标记算法

    A jet tagging algorithm of graph network with HaarPooling message passing. (arXiv:2210.13869v4 [hep-ex] UPDATED)

    [http://arxiv.org/abs/2210.13869](http://arxiv.org/abs/2210.13869)

    本论文介绍了一种将图神经网络与HaarPooling操作相结合的方法，称为HMPNet，用于高能物理中的夸克胶子标记。实验结果表明，适当选择HaarPooling的信息可以提高夸克胶子标记的准确性。

    

    最近，图神经网络（GNNs）的方法已经被应用于解决高能物理（HEP）中的问题，并且在使用图表示的喷注事件的夸克胶子标记中展现出了巨大潜力。在本文中，我们引入了一种GNNs与HaarPooling操作相结合的方法来分析事件，称之为HaarPooling消息传递神经网络（HMPNet）。在HMPNet中，HaarPooling不仅提取了图的特征，还嵌入了通过k-means对不同粒子特征进行聚类得到的附加信息。我们从五个不同的特征构建了HaarPooling：绝对能量$\log E$，横向动量$\log p_T$，相对坐标$(\Delta\eta,\Delta\phi)$，混合特征$(\log E, \log p_T)$和$(\log E, \log p_T, \Delta\eta,\Delta\phi)$。结果表明，适当选择HaarPooling的信息可以提高夸克胶子标记的准确性，将$\log P_T$的额外信息添加到HMPNet中优于其他所有方法。

    Recently methods of graph neural networks (GNNs) have been applied to solving the problems in high energy physics (HEP) and have shown its great potential for quark-gluon tagging with graph representation of jet events. In this paper, we introduce an approach of GNNs combined with a HaarPooling operation to analyze the events, called HaarPooling Message Passing neural network (HMPNet). In HMPNet, HaarPooling not only extracts the features of graph, but embeds additional information obtained by clustering of k-means of different particle features. We construct Haarpooling from five different features: absolute energy $\log E$, transverse momentum $\log p_T$, relative coordinates $(\Delta\eta,\Delta\phi)$, the mixed ones $(\log E, \log p_T)$ and $(\log E, \log p_T, \Delta\eta,\Delta\phi)$. The results show that an appropriate selection of information for HaarPooling enhances the accuracy of quark-gluon tagging, as adding extra information of $\log P_T$ to the HMPNet outperforms all the o
    
[^222]: 灵活的基于注意力的多策略融合用于高效深度强化学习的研究

    Flexible Attention-Based Multi-Policy Fusion for Efficient Deep Reinforcement Learning. (arXiv:2210.03729v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.03729](http://arxiv.org/abs/2210.03729)

    该论文提出了一种知识引导的强化学习方法（KGRL），通过融合多个知识策略并利用注意力机制实现了灵活的知识重新排列。这种方法可以提高强化学习代理的样本效率和泛化能力。

    

    强化学习 (RL) 代理长期以来一直致力于接近人类学习的效率。人类是优秀的观察者，可以通过聚合来自各种来源的外部知识（包括他人在尝试任务时的观察）来学习。之前在RL中的研究已经将外部知识策略结合到代理中，以帮助其提高样本效率。然而，执行任意组合和替换这些策略仍然是非平凡的，这是泛化和可转移性的重要特征。在这项工作中，我们提出了知识引导的RL(KGRL)，这是一种融合多个知识策略并旨在实现人类学习效率和灵活性的RL范式。我们为KGRL提出了一种新的演员架构，即知识包容性注意网络(KIAN)，该网络通过基于嵌入的注意力行动预测实现了自由的知识重新排列。KIAN还解决了熵不平衡的问题，这是在最大熵KGRL中出现的问题，阻碍了代理的高效表现。

    Reinforcement learning (RL) agents have long sought to approach the efficiency of human learning. Humans are great observers who can learn by aggregating external knowledge from various sources, including observations from others' policies of attempting a task. Prior studies in RL have incorporated external knowledge policies to help agents improve sample efficiency. However, it remains non-trivial to perform arbitrary combinations and replacements of those policies, an essential feature for generalization and transferability. In this work, we present Knowledge-Grounded RL (KGRL), an RL paradigm fusing multiple knowledge policies and aiming for human-like efficiency and flexibility. We propose a new actor architecture for KGRL, Knowledge-Inclusive Attention Network (KIAN), which allows free knowledge rearrangement due to embedding-based attentive action prediction. KIAN also addresses entropy imbalance, a problem arising in maximum entropy KGRL that hinders an agent from efficiently ex
    
[^223]: Rank-N-Contrast:为回归问题学习连续表示的方法

    Rank-N-Contrast: Learning Continuous Representations for Regression. (arXiv:2210.01189v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01189](http://arxiv.org/abs/2210.01189)

    Rank-N-Contrast是一种学习连续表示的回归框架，通过对样本在目标空间中的排名进行比较来提高性能，并在计算机视觉、人机交互和医疗保健等领域的回归任务中取得了最先进的结果。

    

    深度回归模型通常在端到端的方式下学习，没有明确强调回归感知的表示。因此，学习到的表示表现出分散性，无法捕捉样本顺序的连续性，导致广泛的回归任务中产生次优结果。为了填补这个空白，我们提出了Rank-N-Contrast（RNC），一种通过对样本在目标空间中的排名进行比较来学习回归的连续表示的框架。我们从理论和实证上证明了RNC可以保证所学表示的顺序与目标顺序相符，不仅性能更好，而且鲁棒性、效率和泛化能力都得到了显著提高。在计算机视觉、人机交互和医疗保健等领域使用五个真实世界的回归数据集进行的大量实验验证了RNC的最先进性能，突显了其内在的创新贡献。

    Deep regression models typically learn in an end-to-end fashion without explicitly emphasizing a regression-aware representation. Consequently, the learned representations exhibit fragmentation and fail to capture the continuous nature of sample orders, inducing suboptimal results across a wide range of regression tasks. To fill the gap, we propose Rank-N-Contrast (RNC), a framework that learns continuous representations for regression by contrasting samples against each other based on their rankings in the target space. We demonstrate, theoretically and empirically, that RNC guarantees the desired order of learned representations in accordance with the target orders, enjoying not only better performance but also significantly improved robustness, efficiency, and generalization. Extensive experiments using five real-world regression datasets that span computer vision, human-computer interaction, and healthcare verify that RNC achieves state-of-the-art performance, highlighting its intr
    
[^224]: 朝着可信的神经程序合成之路

    Toward Trustworthy Neural Program Synthesis. (arXiv:2210.00848v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2210.00848](http://arxiv.org/abs/2210.00848)

    我们提出了一种能够估计大型语言模型中采样程序正确性概率的简单方法，通过采样候选程序和候选谓词来预测程序的正确性，并推断出关于生成代码行为解释的有用谓词。

    

    我们提出了一种估计从大型语言模型中采样的程序正确性概率的方法。给定一个自然语言描述的编程问题，我们的方法既采样候选程序，又采样候选谓词来指定程序的行为。这样可以学习一个能够生成良好校准程序正确性概率预测的模型。我们的系统还推断出生成代码行为解释中有用的谓词，并且在人类实验中，人们更偏好这些谓词而不是原始语言模型的输出。我们的方法简单、易于实现，并且保持了最先进的生成准确性结果。

    We develop an approach to estimate the probability that a program sampled from a large language model is correct. Given a natural language description of a programming problem, our method samples both candidate programs as well as candidate predicates specifying how the program should behave. This allows learning a model that forms a well-calibrated probabilistic prediction of program correctness. Our system also infers which predicates are useful to explain the behavior of the generated code, and humans preferred these in a human study over raw language model outputs. Our method is simple, easy to implement, and maintains state of the art generation accuracy results.
    
[^225]: 从时间序列中学习领域特定的因果发现

    Learning domain-specific causal discovery from time series. (arXiv:2209.05598v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.05598](http://arxiv.org/abs/2209.05598)

    本研究旨在通过数据驱动的方法提高时间序列中领域特定的因果发现。实验证明，该方法在多个领域上明显优于人类设计的通用因果发现方法。

    

    因果发现是神经科学、医学和机器学习中的重要问题。因果发现的技术包括随机实验和基于格兰杰因果性、条件独立性、结构方程以及评分方法等算法，然而这些方法只在人类设计者做出强假设时才准确。本研究探讨了是否可以通过数据驱动的方法提高时间序列中的领域特定因果发现。我们的研究结果表明，该方法在MOS 6502微处理器、NetSim fMRI数据集和Dream3基因数据集上显著优于人类设计的领域通用因果发现方法，如互信息、VAR-LiNGAM和格兰杰因果性。我们认为，在可行的情况下，这种方法是一种有效的改进因果发现的方法。

    Causal discovery (CD) from time-varying data is important in neuroscience, medicine, and machine learning. Techniques for CD encompass randomized experiments, which are generally unbiased but expensive, and algorithms such as Granger causality, conditional-independence-based, structural-equation-based, and score-based methods that are only accurate under strong assumptions made by human designers. However, as demonstrated in other areas of machine learning, human expertise is often not entirely accurate and tends to be outperformed in domains with abundant data. In this study, we examine whether we can enhance domain-specific causal discovery for time series using a data-driven approach. Our findings indicate that this procedure significantly outperforms human-designed, domain-agnostic causal discovery methods, such as Mutual Information, VAR-LiNGAM, and Granger Causality on the MOS 6502 microprocessor, the NetSim fMRI dataset, and the Dream3 gene dataset. We argue that, when feasible,
    
[^226]: 在高维度中利用神经网络逼近连续函数，并应用于逆问题

    Neural Network Approximation of Continuous Functions in High Dimensions with Applications to Inverse Problems. (arXiv:2208.13305v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2208.13305](http://arxiv.org/abs/2208.13305)

    该论文提出了一种通用的方法，用于解决神经网络在高维空间中逼近连续函数的问题，并对理论与实践之间的差距进行了缩小。该方法基于Johnson-Lindenstrauss嵌入的观察，通过将高维集合嵌入到低维空间中，使得较小的神经网络可以有效逼近高维连续函数。

    

    在过去的十年中，神经网络在各个领域中的逆问题中取得了显著的成功，这推动了它们在医学影像到地震分析等领域的采用。然而，这些逆问题的高维度使得现有理论无法解释在实践中为什么使用看似较小的网络也能得到良好的效果。为了缩小理论与实践之间的差距，我们提供了一种通用的方法来界定神经网络逼近高维集合上的H\"older（或均匀）连续函数所需的复杂度。这种方法基于一个观察：给定一个高维集合$S\subset\mathbb{R}^D$，如果存在一个Johnson-Lindenstrauss嵌入$A\in\mathbb{R}^{d\times D}$，其中$d$较小，将$S$嵌入到一个低维立方体$[-M,M]^d$中，那么对于任何H\"o

    The remarkable successes of neural networks in a huge variety of inverse problems have fueled their adoption in disciplines ranging from medical imaging to seismic analysis over the past decade. However, the high dimensionality of such inverse problems has simultaneously left current theory, which predicts that networks should scale exponentially in the dimension of the problem, unable to explain why the seemingly small networks used in these settings work as well as they do in practice. To reduce this gap between theory and practice, we provide a general method for bounding the complexity required for a neural network to approximate a H\"older (or uniformly) continuous function defined on a high-dimensional set with a low-complexity structure. The approach is based on the observation that the existence of a Johnson-Lindenstrauss embedding $A\in\mathbb{R}^{d\times D}$ of a given high-dimensional set $S\subset\mathbb{R}^D$ into a low dimensional cube $[-M,M]^d$ implies that for any H\"o
    
[^227]: 多个随机特征模型中的多次下降现象

    Multiple Descent in the Multiple Random Feature Model. (arXiv:2208.09897v3 [math.ST] UPDATED)

    [http://arxiv.org/abs/2208.09897](http://arxiv.org/abs/2208.09897)

    本文研究了过度参数化学习中的双下降现象，并在多组分预测模型中进一步探究了多次下降现象。通过理论计算和实验证实，随机特征模型的风险曲线可以呈现出三次下降。

    

    最近的研究表明，过度参数化学习中存在双下降现象。尽管这一现象已经得到了研究，但在理论上尚未完全理解。本文研究了多组分预测模型中的多次下降现象。首先考虑一个“双随机特征模型” (DRFM)，它连接了两种类型的随机特征，并研究了DRFM在岭回归中实现的过度风险。我们计算了高维框架下，当训练样本量、数据维度和随机特征维度趋向于无穷大时，过度风险的精确极限。基于这个计算，我们进一步从理论上证明了DRFM的风险曲线可能呈现出三次下降。然后我们进行了深入的实验研究，以验证我们的理论。最后，我们将研究扩展到“多随机特征模型” (MRFM)，并展示了MRFMs集成K种类型的情况。

    Recent works have demonstrated a double descent phenomenon in over-parameterized learning. Although this phenomenon has been investigated by recent works, it has not been fully understood in theory. In this paper, we investigate the multiple descent phenomenon in a class of multi-component prediction models. We first consider a ''double random feature model'' (DRFM) concatenating two types of random features, and study the excess risk achieved by the DRFM in ridge regression. We calculate the precise limit of the excess risk under the high dimensional framework where the training sample size, the dimension of data, and the dimension of random features tend to infinity proportionally. Based on the calculation, we further theoretically demonstrate that the risk curves of DRFMs can exhibit triple descent. We then provide a thorough experimental study to verify our theory. At last, we extend our study to the ''multiple random feature model'' (MRFM), and show that MRFMs ensembling $K$ types
    
[^228]: 预测聚合的样本复杂度

    Sample Complexity of Forecast Aggregation. (arXiv:2207.13126v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.13126](http://arxiv.org/abs/2207.13126)

    本文研究了一种预测聚合模型，考虑了贝叶斯方法应用于专家命中后的信号汇聚。论文中提供了对于该问题的样本复杂度，表明该复杂度至少为 $\tilde \Omega(m^{n-2} / \varepsilon)$，其中 m 是每个专家信号空间的大小。

    

    我们考虑了一种贝叶斯预测聚合模型，有 n 个专家根据未知二元事件的私有信号报告事件的后验信念给负责人，随后该负责人将报告聚合为对该事件的单个预测。专家的信号和事件的结果服从一个联合分布，该分布对于负责人是未知的，但负责人可以从分布中得到 i.i.d.“样本”，其中每个样本都是由专家的报告（不是信号）和事件的实现组成的元组。使用这些样本，负责人旨在找到一个 $\varepsilon$-最优聚合器，其中最优性是以聚合预测与事件实现之间的预期平方距离来衡量的。我们证明，对于任意离散分布，这个问题的样本复杂度至少是 $\tilde \Omega(m^{n-2} / \varepsilon)$，其中 m 是每个专家信号空间的大小。

    We consider a Bayesian forecast aggregation model where $n$ experts, after observing private signals about an unknown binary event, report their posterior beliefs about the event to a principal, who then aggregates the reports into a single prediction for the event. The signals of the experts and the outcome of the event follow a joint distribution that is unknown to the principal, but the principal has access to i.i.d. "samples" from the distribution, where each sample is a tuple of the experts' reports (not signals) and the realization of the event. Using these samples, the principal aims to find an $\varepsilon$-approximately optimal aggregator, where optimality is measured in terms of the expected squared distance between the aggregated prediction and the realization of the event. We show that the sample complexity of this problem is at least $\tilde \Omega(m^{n-2} / \varepsilon)$ for arbitrary discrete distributions, where $m$ is the size of each expert's signal space. This sample
    
[^229]: 鲁棒飞行控制的神经移动视界估计

    Neural Moving Horizon Estimation for Robust Flight Control. (arXiv:2206.10397v10 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2206.10397](http://arxiv.org/abs/2206.10397)

    本文提出了一种神经移动视界估计器（NeuroMHE），它可以自动调整神经网络建模的关键参数，并适应不同的飞行场景。通过推导出与加权矩阵相关的解析梯度，我们实现了将该估计器作为可学习层嵌入神经网络进行高效学习。此外，我们还开发了一种基于模型的策略梯度算法，以从四旋翼轨迹跟踪误差中直接训练NeuroMHE，而不需要地面真实干扰数据。

    

    估计和应对干扰对于四旋翼飞行控制至关重要。现有的估计器通常需要对特定飞行场景进行大量调整，或者经过广泛的地面真实干扰数据训练，才能实现令人满意的性能。在本文中，我们提出了一种神经移动视界估计器（NeuroMHE），它可以自动调整由神经网络建模的关键参数，并适应不同的飞行场景。我们通过推导与加权矩阵相关的MHE估计的解析梯度，实现了将MHE作为可学习层嵌入神经网络以实现高效学习的无缝融合。有趣的是，我们证明可以使用递归形式的卡尔曼滤波器高效地计算出梯度。此外，我们还开发了一种基于模型的策略梯度算法，以从四旋翼轨迹跟踪误差中直接训练NeuroMHE，而不需要地面真实干扰数据。

    Estimating and reacting to disturbances is crucial for robust flight control of quadrotors. Existing estimators typically require significant tuning for a specific flight scenario or training with extensive ground-truth disturbance data to achieve satisfactory performance. In this paper, we propose a neural moving horizon estimator (NeuroMHE) that can automatically tune the key parameters modeled by a neural network and adapt to different flight scenarios. We achieve this by deriving the analytical gradients of the MHE estimates with respect to the weighting matrices, which enables a seamless embedding of the MHE as a learnable layer into neural networks for highly effective learning. Interestingly, we show that the gradients can be computed efficiently using a Kalman filter in a recursive form. Moreover, we develop a model-based policy gradient algorithm to train NeuroMHE directly from the quadrotor trajectory tracking error without needing the ground-truth disturbance data. The effec
    
[^230]: 学习调节随机权重：受神经调节启发的神经网络用于高效的连续学习

    Learning to Modulate Random Weights: Neuromodulation-inspired Neural Networks For Efficient Continual Learning. (arXiv:2204.04297v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.04297](http://arxiv.org/abs/2204.04297)

    本论文提出了一种受生物神经调节启发的神经网络架构，通过学习调节随机权重的活动来解决连续学习中的灾难性遗忘问题，并提供了新的学习表示解释方法。

    

    现有的连续学习方法主要解决了灾难性遗忘问题，利用正则化方法、回放缓冲区和任务特定的组件。然而，实际连续学习解决方案必须不仅考虑灾难性遗忘的度量指标，还要考虑计算效率和运行时间。在这里，我们引入了一种受生物神经系统中的神经调节启发的新型神经网络架构，以经济高效地解决灾难性遗忘问题，并为解释学习表示提供了新的途径。神经调节是一种生物机制，在机器学习中受到了有限的关注；它以实时动态控制和微调突触动力学来跟踪不同行为背景的需求。受此启发，我们提出的架构学习每个任务环境下的相对较小的一组参数，这些参数对转换输入的不变的、随机化的权重活动进行\emph{神经调节}。

    Existing Continual Learning (CL) approaches have focused on addressing catastrophic forgetting by leveraging regularization methods, replay buffers, and task-specific components. However, realistic CL solutions must be shaped not only by metrics of catastrophic forgetting but also by computational efficiency and running time. Here, we introduce a novel neural network architecture inspired by neuromodulation in biological nervous systems to economically and efficiently address catastrophic forgetting and provide new avenues for interpreting learned representations. Neuromodulation is a biological mechanism that has received limited attention in machine learning; it dynamically controls and fine tunes synaptic dynamics in real time to track the demands of different behavioral contexts. Inspired by this, our proposed architecture learns a relatively small set of parameters per task context that \emph{neuromodulates} the activity of unchanging, randomized weights that transform the input. 
    
[^231]: 无遗憾学习与无界损失：对数池化的情况

    No-Regret Learning with Unbounded Losses: The Case of Logarithmic Pooling. (arXiv:2202.11219v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.11219](http://arxiv.org/abs/2202.11219)

    本文研究了在线对抗环境中，使用对数池化方法学习专家权重的问题。我们提出了一种基于在线镜像下降算法的方法，以达到无遗憾保证。

    

    在每个时间步长$T$中，$m$个专家报告了关于$n$个结果的概率分布；我们希望学习一种聚合这些预测的方法，以达到无遗憾保证。我们关注一种被称为对数池化的基本和实用的聚合方法——log odds 的加权平均，它在某种意义上是一种最优的池化方法选择，如果我们希望最小化 log 损失（作为我们的损失函数）。我们考虑在线对抗环境中学习最佳参数集（即专家权重）的问题。我们假设（必要条件下），对抗选择的结果和预测是一致的，也就是说专家报告了经过校准的预测。施加这个约束条件创建了一个（据我们所知）新颖的半对抗设置，其中对手保留了大量的灵活性。在这个设置下，我们提出了一种基于在线镜像下降的算法，以一种学习专家权重的方式，实现了$O(\s

    For each of $T$ time steps, $m$ experts report probability distributions over $n$ outcomes; we wish to learn to aggregate these forecasts in a way that attains a no-regret guarantee. We focus on the fundamental and practical aggregation method known as logarithmic pooling -- a weighted average of log odds -- which is in a certain sense the optimal choice of pooling method if one is interested in minimizing log loss (as we take to be our loss function). We consider the problem of learning the best set of parameters (i.e. expert weights) in an online adversarial setting. We assume (by necessity) that the adversarial choices of outcomes and forecasts are consistent, in the sense that experts report calibrated forecasts. Imposing this constraint creates a (to our knowledge) novel semi-adversarial setting in which the adversary retains a large amount of flexibility. In this setting, we present an algorithm based on online mirror descent that learns expert weights in a way that attains $O(\s
    
[^232]: 基于数据驱动的带期望约束的极小极大优化

    Data-Driven Minimax Optimization with Expectation Constraints. (arXiv:2202.07868v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.07868](http://arxiv.org/abs/2202.07868)

    本论文研究了基于数据驱动的带期望约束的极小极大优化问题，并提出了一类高效的算法来解决这一问题。依据实验结果，该算法在大规模实际应用中具有实际效率。

    

    近年来，对于包括著名的随机梯度下降方法在内的数据驱动优化方法的关注度显著增加，但很少有人研究数据驱动约束，因为这些硬性约束定义的可行集上的投影计算挑战巨大。本文关注于非光滑凸凹随机极小极大框架，并将数据驱动约束形式化为期望约束。极小极大期望约束问题涵盖了广泛的实际应用，包括二人零和博弈和数据驱动鲁棒优化。我们提出了一类高效的原始对偶算法来解决极小极大期望约束问题，并证明了我们的算法以$\mathcal{O}(\frac{1}{\sqrt{N}})$的最优速率收敛。我们通过在大规模实际应用上进行数值实验来证明我们算法的实际效率。

    Attention to data-driven optimization approaches, including the well-known stochastic gradient descent method, has grown significantly over recent decades, but data-driven constraints have rarely been studied, because of the computational challenges of projections onto the feasible set defined by these hard constraints. In this paper, we focus on the non-smooth convex-concave stochastic minimax regime and formulate the data-driven constraints as expectation constraints. The minimax expectation constrained problem subsumes a broad class of real-world applications, including two-player zero-sum game and data-driven robust optimization. We propose a class of efficient primal-dual algorithms to tackle the minimax expectation-constrained problem, and show that our algorithms converge at the optimal rate of $\mathcal{O}(\frac{1}{\sqrt{N}})$. We demonstrate the practical efficiency of our algorithms by conducting numerical experiments on large-scale real-world applications.
    
[^233]: 多共识分散加速梯度下降

    Multi-consensus Decentralized Accelerated Gradient Descent. (arXiv:2005.00797v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2005.00797](http://arxiv.org/abs/2005.00797)

    本文提出了一种新颖的多共识分散加速梯度下降算法，实现了最优计算复杂度和接近最优的通信复杂度。算法的线性收敛仅取决于全局目标的强凸性，并不要求局部函数是凸函数。实证研究表明该方法在机器学习应用中超越其他方法。

    

    本文考虑了分散凸优化问题，在大规模机器学习、传感器网络和控制理论等领域有广泛应用。我们提出了一种新颖的算法，实现了最优计算复杂度和接近最优的通信复杂度。我们的理论结果对于一个开放问题给出了肯定的答案，即是否存在一种算法可以实现与全局条件数相关而不是局部条件数相关的通信复杂度的(lower bound)相匹配。此外，我们的算法的线性收敛仅取决于全局目标的强凸性，并不要求局部函数是凸函数。我们的方法设计依赖于Nesterov 加速、多共识和梯度追踪等众所周知的技术的创新整合。实证研究显示了我们的方法在机器学习应用中的优越性能。

    This paper considers the decentralized convex optimization problem, which has a wide range of applications in large-scale machine learning, sensor networks, and control theory. We propose novel algorithms that achieve optimal computation complexity and near optimal communication complexity. Our theoretical results give affirmative answers to the open problem on whether there exists an algorithm that can achieve a communication complexity (nearly) matching the lower bound depending on the global condition number instead of the local one. Furthermore, the linear convergence of our algorithms only depends on the strong convexity of global objective and it does \emph{not} require the local functions to be convex. The design of our methods relies on a novel integration of well-known techniques including Nesterov's acceleration, multi-consensus and gradient-tracking. Empirical studies show the outperformance of our methods for machine learning applications.
    
[^234]: 基于贝叶斯优化的动态子目标导向的探索

    Dynamic Subgoal-based Exploration via Bayesian Optimization. (arXiv:1910.09143v4 [math.OC] UPDATED)

    [http://arxiv.org/abs/1910.09143](http://arxiv.org/abs/1910.09143)

    本论文提出了一种基于贝叶斯优化的成本感知探索方法，能够针对稀疏奖励导航环境中的复杂任务，高效地搜索动态子目标的探索策略。

    

    在稀疏奖励导航环境中，通过昂贵和有限的交互中进行强化学习是具有挑战性的，需要有效的探索策略。针对需要现实世界训练的复杂导航任务（当廉价模拟器不可用时），我们考虑一个面临未知环境分布的代理，并且必须决定一种探索策略。在从相同环境分布中选择的测试环境中评估之前，它可以利用一系列训练环境来改进其策略。大多数现有方法关注固定的探索策略，而将探索视为元优化问题的少数方法往往忽视了对成本高效的探索的需求。我们提出了一种成本感知的贝叶斯优化方法，可以高效地搜索一类基于动态子目标的探索策略。该算法调整多个杠杆--子目标的位置，每个episode的长度以及nu的值。

    Reinforcement learning in sparse-reward navigation environments with expensive and limited interactions is challenging and poses a need for effective exploration. Motivated by complex navigation tasks that require real-world training (when cheap simulators are not available), we consider an agent that faces an unknown distribution of environments and must decide on an exploration strategy. It may leverage a series of training environments to improve its policy before it is evaluated in a test environment drawn from the same environment distribution. Most existing approaches focus on fixed exploration strategies, while the few that view exploration as a meta-optimization problem tend to ignore the need for cost-efficient exploration. We propose a cost-aware Bayesian optimization approach that efficiently searches over a class of dynamic subgoal-based exploration strategies. The algorithm adjusts a variety of levers -- the locations of the subgoals, the length of each episode, and the nu
    

